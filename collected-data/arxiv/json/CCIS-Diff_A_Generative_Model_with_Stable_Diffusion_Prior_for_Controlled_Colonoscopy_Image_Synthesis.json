{
    "title": "CCIS-DIFF:AGENERATIVEMODELWITHSTABLEDIFFUSIONPRIORFOR",
    "abstract": "Colonoscopy is crucial for identifying adenomatous polyps and preventing colorectal cancer. However, developing ro- bustmodelsforpolypdetectionischallengingbythelimited sizeandaccessibilityofexistingcolonoscopydatasets.While Mask ArSDM ControlNet CCIS-DIFF (Ours) previous efforts have attempted to synthesize colonoscopy Fig.1. IncontrasttoArSDM[2]andControlNet[3],CCIS- images, current methods suffer from instability and insuffi- DIFFcanutilizenotonlymaskbutalsotextualdescriptionto cientdatadiversity. Moreover,theseapproacheslackprecise generatehigh-fidelity,text-consistentcolonoscopyimages. control over the generation process, resulting in images that fail to meet clinical quality standards. To address these datasets, makingitdifficulttohavesufficientrobustnessand challenges, we propose CCIS-DIFF, a Controlled genera- generalizationthatmeetreal-worldclinicaldemands. tive model for high-quality Colonoscopy Image Synthesis Toaddressthisproblem,previousmethods[4,5,2,6]pri- based on a Diffusion architecture. Our method offers pre- marilyreliedongenerativeadversarialnetworksordiffusion cise control over both the spatial attributes (polyp location models to synthesize more colonoscopy images. Although and shape) and clinical characteristics of polyps that align these efforts aim to address the data scarcity problem, they with clinical descriptions. Specifically, we introduce a blur struggletogenerateasufficientlydiverseandhigh-qualityim- mask weighting strategy to seamlessly blend synthesized age,andthegenerationprocesslacksadequatecontrol,lead- polyps with the colonic mucosa, and a text-aware attention ingtoimagesthatfailtomeetclinicalrequirementsforprac- mechanism to guide the generated images to reflect clini- tical use. As illustrated in Fig. 1, ArSDM [2] only utilizes cal characteristics. Notably, to achieve this, we construct a themasktosynthesizethecolonoscopyimageandthegener- newmulti-modalcolonoscopydatasetthatintegratesimages, atedimageisofpoorqualityandcontainsnoise. Meanwhile, mask annotations, and corresponding clinical text descrip- large-scaletext-to-image(T2I)diffusionmodelssuchasSta- tions. Experimental results demonstrate that our method bleDiffusion[7]andDALL¬∑E[8]havedemonstratedremark- generateshigh-quality,diversecolonoscopyimageswithfine ablecapabilitiesingeneratingimagesfromvariousprompts. controloverbothspatialconstraintsandclinicalconsistency, This raises an important question: Can colonoscopy images offering valuable support for downstream segmentation and begeneratedinacontrolledmannerusingapre-trainedlarge- diagnostictasks. scaleT2Imodel?Inresponse,wepresentaninnovativegener- ative method to synthesize high-quality colonoscopy images Index Terms‚Äî Colonoscopy Image Synthesis, Con- inacontrolledmanner. Themaincontributionsofthispaper trolledSynthesis,StableDiffusion canbesummarizedasfollows: ‚Ä¢ Weproposeanovelgenerativemodel,namedCCIS-DIFF,",
    "body": "CCIS-DIFF:AGENERATIVEMODELWITHSTABLEDIFFUSIONPRIORFOR\nCONTROLLEDCOLONOSCOPYIMAGESYNTHESIS\nYifanXie1,4,JinggeWang2,3,TaoFeng2,FeiMa4‚àó,YangLi2,3‚àó\n1SchoolofSoftwareEngineering,Xi‚ÄôanJiaotongUniversity,Xi‚Äôan,China\n2TsinghuaShenzhenInternationalGraduateSchool,TsinghuaUniversity,Shenzhen,China\n3ShenzhenKeyLaboratoryofUbiquitousDataEnabling,Shenzhen,China\n4GuangdongLaboratoryofArtificialIntelligenceandDigitalEconomy(SZ),Shenzhen,China\nABSTRACT + Text:The polyp is pinkish, with a smooth texture against a background of darker mucosa with some yellowish debris.\nColonoscopy is crucial for identifying adenomatous polyps\nand preventing colorectal cancer. However, developing ro-\nbustmodelsforpolypdetectionischallengingbythelimited\nsizeandaccessibilityofexistingcolonoscopydatasets.While Mask ArSDM ControlNet CCIS-DIFF (Ours)\nprevious efforts have attempted to synthesize colonoscopy Fig.1. IncontrasttoArSDM[2]andControlNet[3],CCIS-\nimages, current methods suffer from instability and insuffi- DIFFcanutilizenotonlymaskbutalsotextualdescriptionto\ncientdatadiversity. Moreover,theseapproacheslackprecise generatehigh-fidelity,text-consistentcolonoscopyimages.\ncontrol over the generation process, resulting in images that\nfail to meet clinical quality standards. To address these datasets, makingitdifficulttohavesufficientrobustnessand\nchallenges, we propose CCIS-DIFF, a Controlled genera- generalizationthatmeetreal-worldclinicaldemands.\ntive model for high-quality Colonoscopy Image Synthesis Toaddressthisproblem,previousmethods[4,5,2,6]pri-\nbased on a Diffusion architecture. Our method offers pre- marilyreliedongenerativeadversarialnetworksordiffusion\ncise control over both the spatial attributes (polyp location models to synthesize more colonoscopy images. Although\nand shape) and clinical characteristics of polyps that align these efforts aim to address the data scarcity problem, they\nwith clinical descriptions. Specifically, we introduce a blur struggletogenerateasufficientlydiverseandhigh-qualityim-\nmask weighting strategy to seamlessly blend synthesized age,andthegenerationprocesslacksadequatecontrol,lead-\npolyps with the colonic mucosa, and a text-aware attention ingtoimagesthatfailtomeetclinicalrequirementsforprac-\nmechanism to guide the generated images to reflect clini- tical use. As illustrated in Fig. 1, ArSDM [2] only utilizes\ncal characteristics. Notably, to achieve this, we construct a themasktosynthesizethecolonoscopyimageandthegener-\nnewmulti-modalcolonoscopydatasetthatintegratesimages, atedimageisofpoorqualityandcontainsnoise. Meanwhile,\nmask annotations, and corresponding clinical text descrip- large-scaletext-to-image(T2I)diffusionmodelssuchasSta-\ntions. Experimental results demonstrate that our method bleDiffusion[7]andDALL¬∑E[8]havedemonstratedremark-\ngenerateshigh-quality,diversecolonoscopyimageswithfine ablecapabilitiesingeneratingimagesfromvariousprompts.\ncontroloverbothspatialconstraintsandclinicalconsistency, This raises an important question: Can colonoscopy images\noffering valuable support for downstream segmentation and begeneratedinacontrolledmannerusingapre-trainedlarge-\ndiagnostictasks. scaleT2Imodel?Inresponse,wepresentaninnovativegener-\native method to synthesize high-quality colonoscopy images\nIndex Terms‚Äî Colonoscopy Image Synthesis, Con-\ninacontrolledmanner. Themaincontributionsofthispaper\ntrolledSynthesis,StableDiffusion\ncanbesummarizedasfollows:\n‚Ä¢ Weproposeanovelgenerativemodel,namedCCIS-DIFF,\n1. INTRODUCTION which offers fine control over both the spatial attributes\nandclinicalcharacteristicsofpolyps,enablingmoreclin-\nColonoscopy is an essential tool for detecting adenomatous icallyconsistentimagesynthesisforpracticaluse.\npolyps and reducing rectal cancer mortality rates [1, ?, ?]. ‚Ä¢ We introduce the blur mask weighting strategy to en-\nHowever, training models for automatic polyp detection is sure seamless integration of synthesized polyps with the\nchallenging due to the small scale of available colonoscopy colonicmucosa,alongwithatext-awareattentionmecha-\nnismthatincorporatestextualinformationintothegener-\n‚àóCorrespondingemail:mafei@gml.ac.cn,yangli@sz.tsinghua.edu.cn ationprocess,toenablecustomizationofpolypimages.\n4202\nvoN\n91\n]VC.sc[\n1v89121.1142:viXra\n(a) üî• Trainable Modules ‚ùÑ Frozen Modules ‚äïElement Addition\nNoise ResNet Self Score\nEmbedding Block Attention Analysis\nAttention Map\nSharing Q, K Text Cross\nEmbedding Attention\nUpdated\nAttention Map\nBlur Mask Cross ResNet\n‚ùÑ üî• Embedding Attention Block x2\n‚ùÑ Output\nVAE Noise ‚äï Weighting\nEncoder\nLLM Agent\n‚ùÑText üî• Mask üî• Mask\nThe polyp is Encoder Encoder Encoder\npink, smooth-\ntextured, and Image Mask The polyp is pink, slightly swollen, round, smooth- set against a Given a colonoscopy image and a textured, and slightly\nbackground of Gaussian Blur corresponding polyp segmentation swollen, set against a\nmucosal tissue mask, please describe the polyps background of mucosal\nyellw owit ih s hso dm ebe ris. i sn w t oe lr lem ns d o ef g c Iro nel e po ur a, n t s d Th ea b xp a te c, k t ge rx ot uu nr de, . yti es ls lu oe w w ishit h d es bo rm ise (. b)\nFig.2. (a)TheoverallarchitectureofCCIS-DIFF.TheparametersofthefrozenmodulesrefertoStableDiffusionv1.5[7]. The\ntrainablemodulesincorporateatext-awareattentionmechanismtoaddresstheissueofneglectingtextprompts. Additionally,\nweintroduceablurmaskweightingstrategytoensureseamlessintegrationofsynthesizedpolypswiththecolonicmucosa. (b)\nTheconstructionpipelineofthemulti-modalcolonoscopydataset.\n‚Ä¢ Wedesignthefirstmulti-modalcolonoscopydatasetthat color, shape, texture, and swelling. As a result, we develop\nuniquely combines colonoscopy images, segmentation a multi-modal colonoscopy dataset consisting of triplets of\nmasks, and clinical text descriptions, providing precise colonoscopy images, mask images, and their corresponding\nalignment between visual data and clinical information textualdescriptionswhichprovidesthenecessaryfoundation\nforimprovedtrainingandevaluation. forgeneratingimageswithimprovedrealismandvariability.\n2. METHOD 2.2. CCIS-DIFFArchitecture\n2.1. Multi-ModalColonoscopyDataset 2.2.1. Overview\nAkeychallengeforcurrentdiffusionmodelsincolonoscopy TheoverviewarchitectureofourCCIS-DIFFispresentedin\nimage synthesis [2, 6] is the absence of a dataset with con- Fig. 2 (a). Using our multi-modal colonoscopy dataset, we\nsistentspatialcharacteristicsofpolypsandtheircorrespond- provide the original colonoscopy image I, the mask image\ning clinical textual descriptions. Existing datasets, such as M, and the corresponding text description T. Each of these\nEndoScene [9], CVC-ClinicDB [10], and Kvasir [11], pri- componentspassesthroughitsrespectiveencoder,notingthat\nmarilycontainimage-maskpairsthatfocusonpolypregions. the text encoder is frozen. Additionally, we develop a blur\nHowever, these datasets do not include detailed clinical text masktoensurethatthegeneratedpolypintegratesseamlessly\ndescriptions, limiting their usefulness for training models with the background, the detailed description will be illus-\nthataimtogenerateclinicallyaccurateanddiversesynthetic tratedinSec. 2.2.2.\nimages. To address this limitation, we construct a novel BasedonControlNet[3],weadoptthetrainablediffusion\nmulti-modal colonoscopy dataset that integrates three essen- branchandimplementthezeroconvolutionstrategytoprotect\ntial components: colonoscopy images, segmentation masks, this branch by eliminating random noise as gradients in the\nandclinicaltextdescriptions. Suchadatasetisvitalforfine- initial training steps. This structure, when applied to large\ntuning pretrained diffusion models, allowing them to adapt models like Stable Diffusion [7], enables the frozen param-\neffectivelytocontrolledcolonoscopyimagesynthesis. eterstopreservetheintegrityoftheproduction-readymodel\nAnoverviewofourdatasetconstructionprocessisshown that has been trained on billions of images. Meanwhile,\nin Fig. 2 (b). To generate accurate captions that accurately the trainable diffusion branch leverages this large-scale pre-\nreflect both spatial constraints and relevant clinical charac- trained model to establish a robust backbone for managing\nteristics, we construct the open-source LLaMA [12] large multi-modal input conditions. Furthermore, to address the\nlanguage model (LLM) agent. This agent takes a textual issueofneglectingtextpromptandeffectivelyincorporating\nprompt and an image-mask colonoscopy pair to create cap- textual information into the generation process, we incor-\ntions for both the foreground and background. To increase porate a text-aware attention mechanism (Sec. 2.2.3) in the\ntextualdiversity,theLLMincorporatesdifferentaspectslike trainablediffusionbranch.\nelpmaS\negamI\nlanigirO\ntxeT\nsemitN\nksaM\nksaM\nrulB\n2.2.2. BlurMaskWeightingStrategy\nTable 1. The quantitative results of the colonoscopy image\nThe purpose of the blur mask weighting strategy is to en- synthesis. Theboldfaceindicatesthebestperformance.\nsurethatthegeneratedpolypisseamlesslyintegratedwiththe\nMethod FID‚Üì CLIP-score‚Üë CLIP-image‚Üë\nbackground.Toachievethis,weapplyaGaussianbluropera-\nControlNet 87.32 30.78 87.09\ntionœÉtothemaskimageM,softeningthetransitionbetween\nUni-ControlNet 92.74 30.57 86.46\nthe polyp mask region and the background. We utilize two\nOurs 71.73 31.96 88.70\nseparatemaskencoderstoextractfeaturesfromthemaskand\nthe blurred mask, each with non-shared parameters. Subse-\nquently,aweightingmatrixM isintroducedtobalancethe 2.2.4. TrainingandInference\nw\ntwo mask branches, and M is learned using a three-layer\nw\nDuringthetrainingphase, thediffusionmodelsstartwithan\nMLP. Thus the blur mask embedding F can be constructed\nb\noriginalimageI andprogressivelyaddnoisetocreateanoisy\nas:\nembedding I , where t indicates the number of times noise\nF =M ‚äôE (œÉ(M)), (1) t\nb w m\nisadded. Themodelslearnanetworkœµ topredictthenoise\nŒ∏\nwhere‚äôistheHadamardproductandE m denotesthemask addedtothenoisyimageI t,basedonasetofconditionsthat\nencoder. include the time step t, a text prompt T, and a mask image\nM. TheoveralllossfunctionLfortheentirediffusionmodel\n2.2.3. Text-AwareAttentionMechanism isrepresentedas:\nInourexperiments,weobservedthatexistingmethods,such (cid:104) (cid:105)\nL=E ‚à•œµ‚àíœµ (I ,t,T,M))‚à•2 . (4)\nas ControlNet [3], often overlook the text prompt and rely I,t,T,M,œµ‚àºN(0,1) Œ∏ t 2\nmore heavily on the mask image. We hypothesize that this\nAdditionally, we randomly replace half of the text prompts\nvisual dominance over the text prompt arises from the text-\nwithemptystrings.Thisstrategyenhancesthemodel‚Äôsability\nfreenatureoftheself-attentionlayers. Toaddressthisissue,\ntodirectlyrecognizethesemanticsoftheinputmaskimage,\nwe propose a text-aware attention mechanism that leverages\nservingasasubstituteforthetextprompt.\nthe cross-attention matrix to regulate the output of the self-\nDuring the inference phase, the diffusion models auto-\nattention.\nGiven the noise embedding F , it is first passed through matically generate noise and produce the final colonoscopy\nn\nimages based on a text prompt and a mask image. Simul-\na ResNet block, as referenced in ControlNet [3]. Following\nthis, we obtain the attention input tensor F ‚àà R(h√ów)√ód, taneously,weutilizeClassifier-FreeGuidance(CFG)[13]to\na\nenhancethesamplingprocess.\nwhich is processed through projection layers to derive the\nqueries Q , keys K , and values V , as well as the attention\ns s s\nmap M\ns\n= Q ‚àösK sT ‚àà Rhw√óhw. To reduce the strong influ- 3. EXPERIMENTANDRESULTS\nd\nenceofthemaskimage,weadjusttheattentionscoresofthe\nprompttext. Specifically,webeginbyconstructingthecross- 3.1. ImplementationDetails\nattentionsimilaritymatrix:\nAll methods were implemented in PyTorch on a single\n‚àö\nM =SoftMax(Q KT/ d), (2) NVIDIA A100 40G GPU. We utilized the pre-trained Sta-\nc c c\nble Diffusion v1.5 [7] model, following ControlNet [3], to\nwhereQ c ‚àà R(h√ów)√ód,K c ‚àà Rl√ód representthequeryand replicateitsUNetencoderasthetrainablecopyencoder. The\nkey tensors from their corresponding cross-attention layers, batchsizewasconfiguredto4, andthelearningratewasset\nand l denotes the number of tokens in the text prompt. For to 1e-5. For inference, we used a default CFG scale of 7.0.\neach pixel j, we define its similarity to the text prompt by TheDDIMsamplerwasemployed,using20stepstosample\nsummingitssimilarityscoreswiththeembeddingindexedby eachimage.\nM . Wethenapplyaclampoperationtonormalizethescores\nc\ns withintherange[0,1]:\nj\n3.2. BaselineAlgorithmsandEvaluationMetrics\ns =Norm(Sum (M )). (3)\nj j c\nForthecolonoscopyimagesynthesistask, wecomparedour\nBycalculatingthescoresforeachpixel,wecanobtainthe method with ControlNet [3] and Uni-ControlNet [14], all of\nfinal text-aware map S. Subsequently, we utilize S to com- whichweretrainedonourmulti-modalcolonoscopydataset.\npute the updated attention map and refine the self-attention Following [3], we assessed performance using FID, CLIP-\nprocess. score,andCLIP-image(measuringthesimilaritybetweenthe\nFinally,theoutputofthecross-attentionmechanisminter- generatedimageandthereferenceimage).\nactsoncemorewiththeblurmaskembedding,generatingthe Forthedownstreampolypsegmentationtask,weutilized\nfinaloutputembeddingthroughaResNetblock. PraNet [1] and Polyp-PVT [15] as baseline segmentation\nText-1 Text-2 Text-3\nThe polyp is pale pink, smooth, and slightly swollen, T teh xe t up ro el y ap g ais in p si tn ak i bs ah c, ks gli rg oh ut nly d s ow f o dl al re kn e, rw mit uh c a o ss am wo io tt hh Th se ig p no il fy icp a nis t r se wd od li ls eh n i , n c oc no tl ro ar s, ta inn gd aa gp ap ie na sr ts ts hm e o do at rh k ea rn d\nwith a background of normal mucosal folds. some yellowish debris. colon background.\nControlNet Uni-ControlNet Ours ControlNet Uni-ControlNet Ours ControlNet Uni-ControlNet Ours\nFig.3. Qualitativecomparisonofcolonoscopyimagesynthesis.\nText\nTable2. Userstudy. Theratingscalerangesfrom1to5,with The polyp is red in color, with a rough and textured surface,\nmoderately swollen, and situated against the pinkish background\nhighernumbersindicatingbetterperformance. of the colon with some reflective spots and yellow mucus.\nMethod ImageFidelity MaskAccuracy TextAccuracy\nControlNet 3.852 3.962 3.918\nUni-ControlNet 2.904 2.966 3.392\nOurs 4.266 4.466 4.518\nTable 3. Comparisons of different settings applied on two\npolyp segmentation baselines [1, 15] across three public w/o BMWS w/o TAAM Ours\nFig.4. Visualizationoftheablationexperiments.\ndatasets[9,10,11].\nEndoScene CVC-ClinicDB Kvasir criterion are presented in Table 2. Our method outperforms\nSettings mDice mIoU mDice mIoU mDice mIoU theothermethodsinallaspects.\nPraNet 86.1 79.1 90.8 86.1 89.2 84.0 Tofurtherverifytheeffectivenessofoursyntheticimages,\nPraNet+ControlNet 85.4 77.7 90.1 85.1 90.6 85.2 weevaluatedourmethodagainststate-of-the-artmethodsfor\nPraNet+Uni-ControlNet 85.9 78.2 89.5 85.1 87.9 82.5\nthe polyp segmentation task. We generated the same num-\nPraNet+Ours 88.5 81.0 92.8 88.0 92.8 87.1\nberofsamplesasthediffusiontrainingsetusingtheoriginal\nPVT 88.2 81.2 92.8 87.6 91.5 86.8\nPVT+ControlNet 88.3 81.4 92.7 88.2 90.3 84.8 masks,whichwethencombinedtocreateanewdownstream\nPVT+Uni-ControlNet 85.8 79.6 89.3 83.8 86.5 80.7 trainingdataset.TheexperimentalresultspresentedinTable3\nPVT+Ours 89.3 82.1 93.5 88.4 92.4 87.0\nhighlighttheeffectivenessofourmethodintrainingimproved\ndownstreammodelsthatachievesuperiorperformance.\nmodels with their default settings, and assessed their perfor-\nWe also conduct ablation visualization experiments to\nmanceusingmeanDice(mDice)andmeanIntersectionover\ndemonstrate the effectiveness of the Blur Mask Weighting\nUnion(mIoU)metrics.\nStrategy(BMWS)andtheText-AwareAttentionMechanism\n(TAAM).TheresultsareillustratedinFig. 4. Thecombina-\n3.3. Results\ntionofBMWSandTAAMsignificantlyenhancesthequality\nofthesynthesizedimagesandtextalignment.\nThe quantitative results of the colonoscopy image synthesis\nare illustrated in Table 1. Our method outperforms others\nin both visual quality and text alignment. Furthermore, we 4. CONCLUSION\npresentacomparisonofourmethodwithothersinFig.3.The\nresultsshowthatthepolypregiongeneratedbyourmethodis In this paper, we present CCIS-DIFF, a generative model\nmore consistent with the mask image, and the overall infor- that leverages a Stable Diffusion prior for the controlled\nmationalignsmorecloselywiththetextprompt. colonoscopy image synthesis. We begin by developing a\nToconductamorecomprehensiveevaluation, weimple- blur mask weighting strategy to seamlessly integrate the\nmentedauserstudyquestionnaire. Weselected30groupsof generated polyp with the colonic mucosa, along with a text-\nimages for comparison and invited 15 clinical experts from aware attention mechanism to address the issue of neglect-\ntheFirstAffiliatedHospitalofSunYat-senUniversitytopar- ing text prompt. Additionally, we introduce a multi-modal\nticipate in the survey. Participants were required to rate the colonoscopy dataset created by large language models. Ex-\nsynthesized images based on three criteria: image fidelity, tensive experiments across various settings demonstrate the\nmaskaccuracy,andtextaccuracy.Theaveragescoresforeach superiorperformanceofCCIS-DIFF.\n1-ksaM\n2-ksaM\n3-ksaM\n1-ksaM\n2-ksaM\n5. COMPLIANCEWITHETHICALSTANDARDS ings of the IEEE/CVF conference on computer vision\nandpatternrecognition,2022,pp.10684‚Äì10695.\nThis research study was conducted retrospectively using hu-\nman subject data made available in open access. Ethical ap- [8] AdityaRamesh,PrafullaDhariwal,AlexNichol,Casey\nprovalwasnotrequiredasconfirmedbythelicenseattached Chu, and Mark Chen, ‚ÄúHierarchical text-conditional\nwiththeopenaccessdata. image generation with clip latents,‚Äù arXiv preprint\narXiv:2204.06125,vol.1,no.2,pp.3,2022.\n6. REFERENCES [9] David Va¬¥zquez, Jorge Bernal, F Javier Sa¬¥nchez, Glo-\nria Ferna¬¥ndez-Esparrach, Antonio M Lo¬¥pez, Adri-\n[1] Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen, ana Romero, Michal Drozdzal, and Aaron Courville,\nHuazhuFu,JianbingShen,andLingShao,‚ÄúPranet:Par- ‚ÄúA benchmark for endoluminal scene segmentation of\nallelreverseattentionnetworkforpolypsegmentation,‚Äù colonoscopy images,‚Äù Journal of healthcare engineer-\nin International conference on medical image comput- ing,vol.2017,no.1,pp.4037190,2017.\ningandcomputer-assistedintervention.Springer,2020,\npp.263‚Äì273. [10] Jorge Bernal, F Javier Sa¬¥nchez, Gloria Ferna¬¥ndez-\nEsparrach, Debora Gil, Cristina Rodr¬¥ƒ±guez, and Fer-\n[2] Yuhao Du, Yuncheng Jiang, Shuangyi Tan, Xusheng nando VilarinÀúo, ‚ÄúWm-dova maps for accurate polyp\nWu,QiDou,ZhenLi,GuanbinLi,andXiangWan,‚ÄúAr- highlighting in colonoscopy: Validation vs. saliency\nsdm: colonoscopy images synthesis with adaptive re- mapsfromphysicians,‚Äù Computerizedmedicalimaging\nfinement semantic diffusion models,‚Äù in International andgraphics,vol.43,pp.99‚Äì111,2015.\nconferenceonmedicalimagecomputingandcomputer-\nassistedintervention.Springer,2023,pp.339‚Äì349. [11] Debesh Jha, Pia H Smedsrud, Michael A Riegler, PaÀöl\nHalvorsen, Thomas De Lange, Dag Johansen, and\n[3] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala, HaÀövard D Johansen, ‚ÄúKvasir-seg: A segmented polyp\n‚ÄúAdding conditional control to text-to-image diffusion dataset,‚Äù in MultiMedia modeling: 26th international\nmodels,‚Äù inProceedingsoftheIEEE/CVFInternational conference, MMM 2020, Daejeon, South Korea, Jan-\nConferenceonComputerVision,2023,pp.3836‚Äì3847. uary5‚Äì8,2020,proceedings,partII26.Springer,2020,\npp.451‚Äì462.\n[4] Yuhui Ma, Yonghuai Liu, Jun Cheng, Yalin Zheng,\nMorteza Ghahremani, Honghan Chen, Jiang Liu, and [12] HugoTouvron, ThibautLavril, GautierIzacard, Xavier\nYitian Zhao, ‚ÄúCycle structure and illumination con- Martinet, Marie-Anne Lachaux, Timothe¬¥e Lacroix,\nstrained gan for medical image enhancement,‚Äù in Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal\nMedical Image Computing and Computer Assisted Azhar,etal.,‚ÄúLlama:Openandefficientfoundationlan-\nIntervention‚ÄìMICCAI2020:23rdInternationalConfer- guagemodels,‚ÄùarXivpreprintarXiv:2302.13971,2023.\nence,Lima,Peru,October4‚Äì8,2020,Proceedings,Part\n[13] Jonathan Ho and Tim Salimans, ‚ÄúClassifier-free diffu-\nII23.Springer,2020,pp.667‚Äì677.\nsionguidance,‚ÄùarXivpreprintarXiv:2207.12598,2022.\n[5] JiaboXu,SaeedAnwar,NickBarnes,FlorianGrimpen,\n[14] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jian-\nOlivier Salvado, Stuart Anderson, and Mohammad Ali\nmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K\nArmin, ‚ÄúOfgan: Realistic rendition of synthetic\nWong, ‚ÄúUni-controlnet: All-in-one control to text-to-\ncolonoscopyvideos,‚Äù inMedicalImageComputingand\nimagediffusionmodels,‚Äù AdvancesinNeuralInforma-\nComputer Assisted Intervention‚ÄìMICCAI 2020: 23rd\ntionProcessingSystems,vol.36,2024.\nInternational Conference, Lima, Peru, October 4‚Äì8,\n2020,Proceedings,PartIII23.Springer,2020,pp.732‚Äì\n[15] Bo Dong, Wenhai Wang, Deng-Ping Fan, Jinpeng Li,\n741.\nHuazhuFu,andLingShao, ‚ÄúPolyp-pvt: Polypsegmen-\ntationwithpyramidvisiontransformers,‚ÄùarXivpreprint\n[6] Minjae Jeong, Hyuna Cho, Sungyoon Jung, and\narXiv:2108.06932,2021.\nWon Hwa Kim, ‚ÄúUncertainty-aware diffusion-based\nadversarial attack for realistic colonoscopy image syn-\nthesis,‚Äù in International Conference on Medical Im-\nage Computing and Computer-Assisted Intervention.\nSpringer,2024,pp.647‚Äì658.\n[7] RobinRombach,AndreasBlattmann,DominikLorenz,\nPatrickEsser,andBjo¬®rnOmmer, ‚ÄúHigh-resolutionim-\nagesynthesiswithlatentdiffusionmodels,‚Äù inProceed-",
    "pdf_filename": "CCIS-Diff_A_Generative_Model_with_Stable_Diffusion_Prior_for_Controlled_Colonoscopy_Image_Synthesis.pdf"
}