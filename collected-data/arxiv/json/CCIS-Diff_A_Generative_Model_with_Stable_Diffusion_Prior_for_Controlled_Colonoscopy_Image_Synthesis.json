{
    "title": "CCIS-Diff A Generative Model with Stable Diffusion Prior for Controlled Colonoscopy Image Synthesis",
    "context": "Colonoscopy is crucial for identifying adenomatous polyps and preventing colorectal cancer. However, developing ro- bust models for polyp detection is challenging by the limited size and accessibility of existing colonoscopy datasets. While previous efforts have attempted to synthesize colonoscopy images, current methods suffer from instability and insuffi- cient data diversity. Moreover, these approaches lack precise control over the generation process, resulting in images that fail to meet clinical quality standards. To address these challenges, we propose CCIS-DIFF, a Controlled genera- tive model for high-quality Colonoscopy Image Synthesis based on a Diffusion architecture. Our method offers pre- cise control over both the spatial attributes (polyp location and shape) and clinical characteristics of polyps that align with clinical descriptions. Specifically, we introduce a blur mask weighting strategy to seamlessly blend synthesized polyps with the colonic mucosa, and a text-aware attention mechanism to guide the generated images to reflect clini- cal characteristics. Notably, to achieve this, we construct a new multi-modal colonoscopy dataset that integrates images, mask annotations, and corresponding clinical text descrip- tions. Experimental results demonstrate that our method generates high-quality, diverse colonoscopy images with fine control over both spatial constraints and clinical consistency, offering valuable support for downstream segmentation and diagnostic tasks. Index Terms‚Äî Colonoscopy Image Synthesis, Con- trolled Synthesis, Stable Diffusion Colonoscopy is an essential tool for detecting adenomatous polyps and reducing rectal cancer mortality rates [1, ?, ?]. However, training models for automatic polyp detection is challenging due to the small scale of available colonoscopy ‚àóCorresponding email: mafei@gml.ac.cn, yangli@sz.tsinghua.edu.cn Mask + Text: The polyp is pinkish, with a smooth texture against a background of darker mucosa with some yellowish debris. ArSDM ControlNet CCIS-DIFF (Ours) Fig. 1. In contrast to ArSDM [2] and ControlNet [3], CCIS- DIFF can utilize not only mask but also textual description to generate high-fidelity, text-consistent colonoscopy images. datasets, making it difficult to have sufficient robustness and generalization that meet real-world clinical demands. To address this problem, previous methods [4, 5, 2, 6] pri- marily relied on generative adversarial networks or diffusion models to synthesize more colonoscopy images. Although these efforts aim to address the data scarcity problem, they struggle to generate a sufficiently diverse and high-quality im- age, and the generation process lacks adequate control, lead- ing to images that fail to meet clinical requirements for prac- tical use. As illustrated in Fig. 1, ArSDM [2] only utilizes the mask to synthesize the colonoscopy image and the gener- ated image is of poor quality and contains noise. Meanwhile, large-scale text-to-image (T2I) diffusion models such as Sta- ble Diffusion [7] and DALL¬∑E [8] have demonstrated remark- able capabilities in generating images from various prompts. This raises an important question: Can colonoscopy images be generated in a controlled manner using a pre-trained large- scale T2I model? In response, we present an innovative gener- ative method to synthesize high-quality colonoscopy images in a controlled manner. The main contributions of this paper can be summarized as follows: ‚Ä¢ We propose a novel generative model, named CCIS-DIFF, which offers fine control over both the spatial attributes and clinical characteristics of polyps, enabling more clin- ically consistent image synthesis for practical use. ‚Ä¢ We introduce the blur mask weighting strategy to en- sure seamless integration of synthesized polyps with the colonic mucosa, along with a text-aware attention mecha- nism that incorporates textual information into the gener- ation process, to enable customization of polyp images. arXiv:2411.12198v1  [cs.CV]  19 Nov 2024",
    "body": "CCIS-DIFF: A GENERATIVE MODEL WITH STABLE DIFFUSION PRIOR FOR\nCONTROLLED COLONOSCOPY IMAGE SYNTHESIS\nYifan Xie1,4, Jingge Wang2,3, Tao Feng2, Fei Ma4‚àó, Yang Li2,3‚àó\n1School of Software Engineering, Xi‚Äôan Jiaotong University, Xi‚Äôan, China\n2Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China\n3Shenzhen Key Laboratory of Ubiquitous Data Enabling, Shenzhen, China\n4Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen, China\nABSTRACT\nColonoscopy is crucial for identifying adenomatous polyps\nand preventing colorectal cancer. However, developing ro-\nbust models for polyp detection is challenging by the limited\nsize and accessibility of existing colonoscopy datasets. While\nprevious efforts have attempted to synthesize colonoscopy\nimages, current methods suffer from instability and insuffi-\ncient data diversity. Moreover, these approaches lack precise\ncontrol over the generation process, resulting in images that\nfail to meet clinical quality standards.\nTo address these\nchallenges, we propose CCIS-DIFF, a Controlled genera-\ntive model for high-quality Colonoscopy Image Synthesis\nbased on a Diffusion architecture. Our method offers pre-\ncise control over both the spatial attributes (polyp location\nand shape) and clinical characteristics of polyps that align\nwith clinical descriptions. Specifically, we introduce a blur\nmask weighting strategy to seamlessly blend synthesized\npolyps with the colonic mucosa, and a text-aware attention\nmechanism to guide the generated images to reflect clini-\ncal characteristics. Notably, to achieve this, we construct a\nnew multi-modal colonoscopy dataset that integrates images,\nmask annotations, and corresponding clinical text descrip-\ntions.\nExperimental results demonstrate that our method\ngenerates high-quality, diverse colonoscopy images with fine\ncontrol over both spatial constraints and clinical consistency,\noffering valuable support for downstream segmentation and\ndiagnostic tasks.\nIndex Terms‚Äî Colonoscopy Image Synthesis, Con-\ntrolled Synthesis, Stable Diffusion\n1. INTRODUCTION\nColonoscopy is an essential tool for detecting adenomatous\npolyps and reducing rectal cancer mortality rates [1, ?, ?].\nHowever, training models for automatic polyp detection is\nchallenging due to the small scale of available colonoscopy\n‚àóCorresponding email: mafei@gml.ac.cn, yangli@sz.tsinghua.edu.cn\nMask\n+ Text: The polyp is pinkish, with a smooth texture against a background of darker mucosa with some yellowish debris.\nArSDM\nControlNet\nCCIS-DIFF (Ours)\nFig. 1. In contrast to ArSDM [2] and ControlNet [3], CCIS-\nDIFF can utilize not only mask but also textual description to\ngenerate high-fidelity, text-consistent colonoscopy images.\ndatasets, making it difficult to have sufficient robustness and\ngeneralization that meet real-world clinical demands.\nTo address this problem, previous methods [4, 5, 2, 6] pri-\nmarily relied on generative adversarial networks or diffusion\nmodels to synthesize more colonoscopy images. Although\nthese efforts aim to address the data scarcity problem, they\nstruggle to generate a sufficiently diverse and high-quality im-\nage, and the generation process lacks adequate control, lead-\ning to images that fail to meet clinical requirements for prac-\ntical use. As illustrated in Fig. 1, ArSDM [2] only utilizes\nthe mask to synthesize the colonoscopy image and the gener-\nated image is of poor quality and contains noise. Meanwhile,\nlarge-scale text-to-image (T2I) diffusion models such as Sta-\nble Diffusion [7] and DALL¬∑E [8] have demonstrated remark-\nable capabilities in generating images from various prompts.\nThis raises an important question: Can colonoscopy images\nbe generated in a controlled manner using a pre-trained large-\nscale T2I model? In response, we present an innovative gener-\native method to synthesize high-quality colonoscopy images\nin a controlled manner. The main contributions of this paper\ncan be summarized as follows:\n‚Ä¢ We propose a novel generative model, named CCIS-DIFF,\nwhich offers fine control over both the spatial attributes\nand clinical characteristics of polyps, enabling more clin-\nically consistent image synthesis for practical use.\n‚Ä¢ We introduce the blur mask weighting strategy to en-\nsure seamless integration of synthesized polyps with the\ncolonic mucosa, along with a text-aware attention mecha-\nnism that incorporates textual information into the gener-\nation process, to enable customization of polyp images.\narXiv:2411.12198v1  [cs.CV]  19 Nov 2024\n\n‚ùÑ\nOriginal Image\nSample\nThe polyp is \npink, smooth-\ntextured, and \nslightly swollen, \nset against a \nbackground of \nmucosal tissue \nwith some \nyellowish debris.\nText\nüî•\nMask\nGaussian Blur\nBlur Mask\nAttention Map\nSharing Q, K\nUpdated\nAttention Map\nCross\nAttention\nTrainable Modules\n‚ùÑ\nüî•\nN times\nMask\nEncoder\nVAE\nEncoder\nText \nEncoder\n‚ùÑ\n‚äï\nüî•\nMask\nEncoder\nüî•\nWeighting\nResNet\nBlock\nx2\nSelf\nAttention\nScore\nAnalysis\nNoise\nEmbedding\nText\nEmbedding\nBlur Mask\nEmbedding\nCross\nAttention\nResNet\nBlock\nOutput\nNoise\nFrozen Modules\n‚äïElement Addition\n‚ùÑ\nInput Text\nLLM Agent\nImage\nMask\nGiven a colonoscopy image and a \ncorresponding polyp segmentation \nmask, please describe the polyps \nin terms of color, shape, texture, \nswollen degree and background.\nThe polyp is pink, \nround, smooth-\ntextured, and slightly \nswollen, set against a \nbackground of mucosal \ntissue with some \nyellowish debris.\n(a)\n(b)\nFig. 2. (a) The overall architecture of CCIS-DIFF. The parameters of the frozen modules refer to Stable Diffusion v1.5 [7]. The\ntrainable modules incorporate a text-aware attention mechanism to address the issue of neglecting text prompts. Additionally,\nwe introduce a blur mask weighting strategy to ensure seamless integration of synthesized polyps with the colonic mucosa. (b)\nThe construction pipeline of the multi-modal colonoscopy dataset.\n‚Ä¢ We design the first multi-modal colonoscopy dataset that\nuniquely combines colonoscopy images, segmentation\nmasks, and clinical text descriptions, providing precise\nalignment between visual data and clinical information\nfor improved training and evaluation.\n2. METHOD\n2.1. Multi-Modal Colonoscopy Dataset\nA key challenge for current diffusion models in colonoscopy\nimage synthesis [2, 6] is the absence of a dataset with con-\nsistent spatial characteristics of polyps and their correspond-\ning clinical textual descriptions. Existing datasets, such as\nEndoScene [9], CVC-ClinicDB [10], and Kvasir [11], pri-\nmarily contain image-mask pairs that focus on polyp regions.\nHowever, these datasets do not include detailed clinical text\ndescriptions, limiting their usefulness for training models\nthat aim to generate clinically accurate and diverse synthetic\nimages.\nTo address this limitation, we construct a novel\nmulti-modal colonoscopy dataset that integrates three essen-\ntial components: colonoscopy images, segmentation masks,\nand clinical text descriptions. Such a dataset is vital for fine-\ntuning pretrained diffusion models, allowing them to adapt\neffectively to controlled colonoscopy image synthesis.\nAn overview of our dataset construction process is shown\nin Fig. 2 (b). To generate accurate captions that accurately\nreflect both spatial constraints and relevant clinical charac-\nteristics, we construct the open-source LLaMA [12] large\nlanguage model (LLM) agent.\nThis agent takes a textual\nprompt and an image-mask colonoscopy pair to create cap-\ntions for both the foreground and background. To increase\ntextual diversity, the LLM incorporates different aspects like\ncolor, shape, texture, and swelling. As a result, we develop\na multi-modal colonoscopy dataset consisting of triplets of\ncolonoscopy images, mask images, and their corresponding\ntextual descriptions which provides the necessary foundation\nfor generating images with improved realism and variability.\n2.2. CCIS-DIFF Architecture\n2.2.1. Overview\nThe overview architecture of our CCIS-DIFF is presented in\nFig. 2 (a). Using our multi-modal colonoscopy dataset, we\nprovide the original colonoscopy image I, the mask image\nM, and the corresponding text description T. Each of these\ncomponents passes through its respective encoder, noting that\nthe text encoder is frozen. Additionally, we develop a blur\nmask to ensure that the generated polyp integrates seamlessly\nwith the background, the detailed description will be illus-\ntrated in Sec. 2.2.2.\nBased on ControlNet [3], we adopt the trainable diffusion\nbranch and implement the zero convolution strategy to protect\nthis branch by eliminating random noise as gradients in the\ninitial training steps. This structure, when applied to large\nmodels like Stable Diffusion [7], enables the frozen param-\neters to preserve the integrity of the production-ready model\nthat has been trained on billions of images.\nMeanwhile,\nthe trainable diffusion branch leverages this large-scale pre-\ntrained model to establish a robust backbone for managing\nmulti-modal input conditions. Furthermore, to address the\nissue of neglecting text prompt and effectively incorporating\ntextual information into the generation process, we incor-\nporate a text-aware attention mechanism (Sec. 2.2.3) in the\ntrainable diffusion branch.\n\n2.2.2. Blur Mask Weighting Strategy\nThe purpose of the blur mask weighting strategy is to en-\nsure that the generated polyp is seamlessly integrated with the\nbackground. To achieve this, we apply a Gaussian blur opera-\ntion œÉ to the mask image M, softening the transition between\nthe polyp mask region and the background. We utilize two\nseparate mask encoders to extract features from the mask and\nthe blurred mask, each with non-shared parameters. Subse-\nquently, a weighting matrix Mw is introduced to balance the\ntwo mask branches, and Mw is learned using a three-layer\nMLP. Thus the blur mask embedding Fb can be constructed\nas:\nFb = Mw ‚äôEm(œÉ(M)),\n(1)\nwhere ‚äôis the Hadamard product and Em denotes the mask\nencoder.\n2.2.3. Text-Aware Attention Mechanism\nIn our experiments, we observed that existing methods, such\nas ControlNet [3], often overlook the text prompt and rely\nmore heavily on the mask image. We hypothesize that this\nvisual dominance over the text prompt arises from the text-\nfree nature of the self-attention layers. To address this issue,\nwe propose a text-aware attention mechanism that leverages\nthe cross-attention matrix to regulate the output of the self-\nattention.\nGiven the noise embedding Fn, it is first passed through\na ResNet block, as referenced in ControlNet [3]. Following\nthis, we obtain the attention input tensor Fa ‚ààR(h√ów)√ód,\nwhich is processed through projection layers to derive the\nqueries Qs, keys Ks, and values Vs, as well as the attention\nmap Ms = QsKT\ns\n‚àö\nd\n‚ààRhw√óhw. To reduce the strong influ-\nence of the mask image, we adjust the attention scores of the\nprompt text. Specifically, we begin by constructing the cross-\nattention similarity matrix:\nMc = SoftMax(QcKT\nc /\n‚àö\nd),\n(2)\nwhere Qc ‚ààR(h√ów)√ód, Kc ‚ààRl√ód represent the query and\nkey tensors from their corresponding cross-attention layers,\nand l denotes the number of tokens in the text prompt. For\neach pixel j, we define its similarity to the text prompt by\nsumming its similarity scores with the embedding indexed by\nMc. We then apply a clamp operation to normalize the scores\nsj within the range [0, 1]:\nsj = Norm(Sumj(Mc)).\n(3)\nBy calculating the scores for each pixel, we can obtain the\nfinal text-aware map S. Subsequently, we utilize S to com-\npute the updated attention map and refine the self-attention\nprocess.\nFinally, the output of the cross-attention mechanism inter-\nacts once more with the blur mask embedding, generating the\nfinal output embedding through a ResNet block.\nTable 1. The quantitative results of the colonoscopy image\nsynthesis. The boldface indicates the best performance.\nMethod\nFID ‚Üì\nCLIP-score ‚Üë\nCLIP-image ‚Üë\nControlNet\n87.32\n30.78\n87.09\nUni-ControlNet\n92.74\n30.57\n86.46\nOurs\n71.73\n31.96\n88.70\n2.2.4. Training and Inference\nDuring the training phase, the diffusion models start with an\noriginal image I and progressively add noise to create a noisy\nembedding It, where t indicates the number of times noise\nis added. The models learn a network œµŒ∏ to predict the noise\nadded to the noisy image It, based on a set of conditions that\ninclude the time step t, a text prompt T, and a mask image\nM. The overall loss function L for the entire diffusion model\nis represented as:\nL = EI,t,T,M,œµ‚àºN (0,1)\nh\n‚à•œµ ‚àíœµŒ∏(It, t, T, M))‚à•2\n2\ni\n.\n(4)\nAdditionally, we randomly replace half of the text prompts\nwith empty strings. This strategy enhances the model‚Äôs ability\nto directly recognize the semantics of the input mask image,\nserving as a substitute for the text prompt.\nDuring the inference phase, the diffusion models auto-\nmatically generate noise and produce the final colonoscopy\nimages based on a text prompt and a mask image. Simul-\ntaneously, we utilize Classifier-Free Guidance (CFG) [13] to\nenhance the sampling process.\n3. EXPERIMENT AND RESULTS\n3.1. Implementation Details\nAll methods were implemented in PyTorch on a single\nNVIDIA A100 40G GPU. We utilized the pre-trained Sta-\nble Diffusion v1.5 [7] model, following ControlNet [3], to\nreplicate its UNet encoder as the trainable copy encoder. The\nbatch size was configured to 4, and the learning rate was set\nto 1e-5. For inference, we used a default CFG scale of 7.0.\nThe DDIM sampler was employed, using 20 steps to sample\neach image.\n3.2. Baseline Algorithms and Evaluation Metrics\nFor the colonoscopy image synthesis task, we compared our\nmethod with ControlNet [3] and Uni-ControlNet [14], all of\nwhich were trained on our multi-modal colonoscopy dataset.\nFollowing [3], we assessed performance using FID, CLIP-\nscore, and CLIP-image (measuring the similarity between the\ngenerated image and the reference image).\nFor the downstream polyp segmentation task, we utilized\nPraNet [1] and Polyp-PVT [15] as baseline segmentation\n\nThe polyp is pale pink, smooth, and slightly swollen, \nwith a background of normal mucosal folds.\nMask-1\nMask-2\nMask-3\nText-1\nControlNet\nOurs\nUni-ControlNet\nThe polyp is pinkish, slightly swollen, with a smooth \ntexture against a background of darker mucosa with \nsome yellowish debris.\nText-2\nControlNet\nUni-ControlNet\nOurs\nThe polyp is reddish in color, and appears smooth and \nsignificant swollen, contrasting against the darker \ncolon background.\nText-3\nControlNet\nUni-ControlNet\nOurs\nFig. 3. Qualitative comparison of colonoscopy image synthesis.\nTable 2. User study. The rating scale ranges from 1 to 5, with\nhigher numbers indicating better performance.\nMethod\nImage Fidelity\nMask Accuracy\nText Accuracy\nControlNet\n3.852\n3.962\n3.918\nUni-ControlNet\n2.904\n2.966\n3.392\nOurs\n4.266\n4.466\n4.518\nTable 3. Comparisons of different settings applied on two\npolyp segmentation baselines [1, 15] across three public\ndatasets [9, 10, 11].\nEndoScene\nCVC-ClinicDB\nKvasir\nSettings\nmDice\nmIoU\nmDice\nmIoU\nmDice\nmIoU\nPraNet\n86.1\n79.1\n90.8\n86.1\n89.2\n84.0\nPraNet + ControlNet\n85.4\n77.7\n90.1\n85.1\n90.6\n85.2\nPraNet + Uni-ControlNet\n85.9\n78.2\n89.5\n85.1\n87.9\n82.5\nPraNet + Ours\n88.5\n81.0\n92.8\n88.0\n92.8\n87.1\nPVT\n88.2\n81.2\n92.8\n87.6\n91.5\n86.8\nPVT + ControlNet\n88.3\n81.4\n92.7\n88.2\n90.3\n84.8\nPVT + Uni-ControlNet\n85.8\n79.6\n89.3\n83.8\n86.5\n80.7\nPVT + Ours\n89.3\n82.1\n93.5\n88.4\n92.4\n87.0\nmodels with their default settings, and assessed their perfor-\nmance using mean Dice (mDice) and mean Intersection over\nUnion (mIoU) metrics.\n3.3. Results\nThe quantitative results of the colonoscopy image synthesis\nare illustrated in Table 1. Our method outperforms others\nin both visual quality and text alignment. Furthermore, we\npresent a comparison of our method with others in Fig. 3. The\nresults show that the polyp region generated by our method is\nmore consistent with the mask image, and the overall infor-\nmation aligns more closely with the text prompt.\nTo conduct a more comprehensive evaluation, we imple-\nmented a user study questionnaire. We selected 30 groups of\nimages for comparison and invited 15 clinical experts from\nthe First Affiliated Hospital of Sun Yat-sen University to par-\nticipate in the survey. Participants were required to rate the\nsynthesized images based on three criteria: image fidelity,\nmask accuracy, and text accuracy. The average scores for each\nThe polyp is red in color, with a rough and textured surface, \nmoderately swollen, and situated against the pinkish background \nof the colon with some reflective spots and yellow mucus.\nText\nMask-1\nMask-2\nOurs\nw/o TAAM\nw/o BMWS\nFig. 4. Visualization of the ablation experiments.\ncriterion are presented in Table 2. Our method outperforms\nthe other methods in all aspects.\nTo further verify the effectiveness of our synthetic images,\nwe evaluated our method against state-of-the-art methods for\nthe polyp segmentation task. We generated the same num-\nber of samples as the diffusion training set using the original\nmasks, which we then combined to create a new downstream\ntraining dataset. The experimental results presented in Table 3\nhighlight the effectiveness of our method in training improved\ndownstream models that achieve superior performance.\nWe also conduct ablation visualization experiments to\ndemonstrate the effectiveness of the Blur Mask Weighting\nStrategy (BMWS) and the Text-Aware Attention Mechanism\n(TAAM). The results are illustrated in Fig. 4. The combina-\ntion of BMWS and TAAM significantly enhances the quality\nof the synthesized images and text alignment.\n4. CONCLUSION\nIn this paper, we present CCIS-DIFF, a generative model\nthat leverages a Stable Diffusion prior for the controlled\ncolonoscopy image synthesis.\nWe begin by developing a\nblur mask weighting strategy to seamlessly integrate the\ngenerated polyp with the colonic mucosa, along with a text-\naware attention mechanism to address the issue of neglect-\ning text prompt. Additionally, we introduce a multi-modal\ncolonoscopy dataset created by large language models. Ex-\ntensive experiments across various settings demonstrate the\nsuperior performance of CCIS-DIFF.\n\n5. COMPLIANCE WITH ETHICAL STANDARDS\nThis research study was conducted retrospectively using hu-\nman subject data made available in open access. Ethical ap-\nproval was not required as confirmed by the license attached\nwith the open access data.\n6. REFERENCES\n[1] Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen,\nHuazhu Fu, Jianbing Shen, and Ling Shao, ‚ÄúPranet: Par-\nallel reverse attention network for polyp segmentation,‚Äù\nin International conference on medical image comput-\ning and computer-assisted intervention. Springer, 2020,\npp. 263‚Äì273.\n[2] Yuhao Du, Yuncheng Jiang, Shuangyi Tan, Xusheng\nWu, Qi Dou, Zhen Li, Guanbin Li, and Xiang Wan, ‚ÄúAr-\nsdm: colonoscopy images synthesis with adaptive re-\nfinement semantic diffusion models,‚Äù in International\nconference on medical image computing and computer-\nassisted intervention. Springer, 2023, pp. 339‚Äì349.\n[3] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala,\n‚ÄúAdding conditional control to text-to-image diffusion\nmodels,‚Äù in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2023, pp. 3836‚Äì3847.\n[4] Yuhui Ma, Yonghuai Liu, Jun Cheng, Yalin Zheng,\nMorteza Ghahremani, Honghan Chen, Jiang Liu, and\nYitian Zhao,\n‚ÄúCycle structure and illumination con-\nstrained gan for medical image enhancement,‚Äù\nin\nMedical Image Computing and Computer Assisted\nIntervention‚ÄìMICCAI 2020: 23rd International Confer-\nence, Lima, Peru, October 4‚Äì8, 2020, Proceedings, Part\nII 23. Springer, 2020, pp. 667‚Äì677.\n[5] Jiabo Xu, Saeed Anwar, Nick Barnes, Florian Grimpen,\nOlivier Salvado, Stuart Anderson, and Mohammad Ali\nArmin,\n‚ÄúOfgan:\nRealistic rendition of synthetic\ncolonoscopy videos,‚Äù in Medical Image Computing and\nComputer Assisted Intervention‚ÄìMICCAI 2020: 23rd\nInternational Conference, Lima, Peru, October 4‚Äì8,\n2020, Proceedings, Part III 23. Springer, 2020, pp. 732‚Äì\n741.\n[6] Minjae Jeong,\nHyuna Cho,\nSungyoon Jung,\nand\nWon Hwa Kim,\n‚ÄúUncertainty-aware diffusion-based\nadversarial attack for realistic colonoscopy image syn-\nthesis,‚Äù\nin International Conference on Medical Im-\nage Computing and Computer-Assisted Intervention.\nSpringer, 2024, pp. 647‚Äì658.\n[7] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¬®orn Ommer, ‚ÄúHigh-resolution im-\nage synthesis with latent diffusion models,‚Äù in Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition, 2022, pp. 10684‚Äì10695.\n[8] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen,\n‚ÄúHierarchical text-conditional\nimage generation with clip latents,‚Äù\narXiv preprint\narXiv:2204.06125, vol. 1, no. 2, pp. 3, 2022.\n[9] David V¬¥azquez, Jorge Bernal, F Javier S¬¥anchez, Glo-\nria Fern¬¥andez-Esparrach, Antonio M L¬¥opez, Adri-\nana Romero, Michal Drozdzal, and Aaron Courville,\n‚ÄúA benchmark for endoluminal scene segmentation of\ncolonoscopy images,‚Äù Journal of healthcare engineer-\ning, vol. 2017, no. 1, pp. 4037190, 2017.\n[10] Jorge Bernal, F Javier S¬¥anchez, Gloria Fern¬¥andez-\nEsparrach, Debora Gil, Cristina Rodr¬¥ƒ±guez, and Fer-\nnando VilariÀúno,\n‚ÄúWm-dova maps for accurate polyp\nhighlighting in colonoscopy:\nValidation vs. saliency\nmaps from physicians,‚Äù Computerized medical imaging\nand graphics, vol. 43, pp. 99‚Äì111, 2015.\n[11] Debesh Jha, Pia H Smedsrud, Michael A Riegler, PÀöal\nHalvorsen, Thomas De Lange, Dag Johansen, and\nHÀöavard D Johansen, ‚ÄúKvasir-seg: A segmented polyp\ndataset,‚Äù in MultiMedia modeling: 26th international\nconference, MMM 2020, Daejeon, South Korea, Jan-\nuary 5‚Äì8, 2020, proceedings, part II 26. Springer, 2020,\npp. 451‚Äì462.\n[12] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth¬¥ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al., ‚ÄúLlama: Open and efficient foundation lan-\nguage models,‚Äù arXiv preprint arXiv:2302.13971, 2023.\n[13] Jonathan Ho and Tim Salimans, ‚ÄúClassifier-free diffu-\nsion guidance,‚Äù arXiv preprint arXiv:2207.12598, 2022.\n[14] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jian-\nmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K\nWong, ‚ÄúUni-controlnet: All-in-one control to text-to-\nimage diffusion models,‚Äù Advances in Neural Informa-\ntion Processing Systems, vol. 36, 2024.\n[15] Bo Dong, Wenhai Wang, Deng-Ping Fan, Jinpeng Li,\nHuazhu Fu, and Ling Shao, ‚ÄúPolyp-pvt: Polyp segmen-\ntation with pyramid vision transformers,‚Äù arXiv preprint\narXiv:2108.06932, 2021.",
    "pdf_filename": "CCIS-Diff_A_Generative_Model_with_Stable_Diffusion_Prior_for_Controlled_Colonoscopy_Image_Synthesis.pdf"
}