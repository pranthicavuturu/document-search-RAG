{
    "title": "Wavelets Are All You Need for Autoregressive Image Generation",
    "context": "two main ingredients. The ﬁrst is wavelet image coding, which allows to tokenize the visual details of an image from coarse to ﬁne details by ordering the information starting with the most signiﬁcant bits of the most signiﬁcant wavelet coeﬃcients. The second is a variant of a language transformer whose architecture is re-designed and optimized for token sequences in this ‘wavelet language’. The transformer learns the signiﬁcant statistical correlations within a token sequence, which are the manifestations of well-known correlations between the wavelet subbands at various resolutions. We show experimental results with conditioning on the generation process. The generation of high-resolution visual information is certainly one of the most remarkable achieve- ments of modern-age artiﬁcial intelligence. One of the prominent methods is diﬀusion-based models [7, 12, 23, 25, 27]. In essence, diﬀusion models attempt to learn inversions of ill-posed operators, such as additive Gaussian noise, blurring, etc., so an image may be generated from random noisy or blurry seeds. Another line of research is designing autoregressive models, that apply the architecture of powerful Large Language Models (LLMs) [21, 31, 33]. These autoregressive methods [8, 24] convert the image pixel representation to a series of visual tokens and then apply generative language techniques. In this paper, we reﬁne this line of research and provide a mathematically robust approach to the autoregressive image generation process. To this end, we reach out to a classic technique in image processing, speciﬁcally, wavelet image coding [26, 29, 30]. Wavelets [4, 6, 17] are one of the main tools of modern approximation theory for nonlinear, adaptive approximation. The various wavelet transforms provide the means to transform an image into a representation that captures the essence of the visual information in a sparse way. Typically, the signiﬁcant wavelet coeﬃcients are a small fraction of the coeﬃcients and represent important edge and texture information, while the insigniﬁcant coeﬃcients with small absolute values are associated with smooth regions of the image. The goal of wavelet image compression methods, such as JPEG2000, is then to eﬃciently store the information of only the signiﬁcant coeﬃcients. In fact, the underlying method of the popular JPEG image compression algorithm [37], invented in the 80s, contains many elements of wavelet coding, where a local Discrete Cosine Transform, a precursor of wavelets, is used. However, in this paper, we leverage the progressive wavelet compression technique, a more advanced form of image compression. It creates a bit-stream where every bit corresponds to the next most important piece of visual information. Since we are generating images rather than decoding them from a compressed ﬁle, there is no need to create actual binary bit-streams, and using a ‘wavelet language’ of a limited number of tokens is suﬃcient. Thus, our new approach to autoregressive image generation is based on two main ingredients. The ﬁrst is progressive wavelet image coding, which allows to tokenize the visual information of an image from coarse to ﬁne details. This can achieved using as few as 6 tokens, by ordering the information starting with the most signiﬁcant bits of the most signiﬁcant wavelet coeﬃcients. The second ingredient is a variant of an NLP decoder-only transformer [21, 31, 33] whose architecture was re-designed and optimized for token sequences in this ‘wavelet language’ The transformer learns the signiﬁcant statistical correlations within a token sequence, which are the manifestations of well-known correlations between the wavelet subbands at various resolutions [1, 16, 18]. During inference, this allows the generation of visually meaningful images from an initial random seed generated from sampled from the distribution of the scaling function coeﬃcients at the lowest resolution. 1",
    "body": "arXiv:2406.19997v2  [cs.LG]  19 Nov 2024\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE\nGENERATION\nWAEL MATTAR, IDAN LEVY, NIR SHARON AND SHAI DEKEL\nAbstract. In this paper, we take a new approach to autoregressive image generation that is based on\ntwo main ingredients. The ﬁrst is wavelet image coding, which allows to tokenize the visual details of an\nimage from coarse to ﬁne details by ordering the information starting with the most signiﬁcant bits of the\nmost signiﬁcant wavelet coeﬃcients. The second is a variant of a language transformer whose architecture\nis re-designed and optimized for token sequences in this ‘wavelet language’. The transformer learns the\nsigniﬁcant statistical correlations within a token sequence, which are the manifestations of well-known\ncorrelations between the wavelet subbands at various resolutions.\nWe show experimental results with\nconditioning on the generation process.\n1. Introduction\nThe generation of high-resolution visual information is certainly one of the most remarkable achieve-\nments of modern-age artiﬁcial intelligence.\nOne of the prominent methods is diﬀusion-based models\n[7, 12, 23, 25, 27]. In essence, diﬀusion models attempt to learn inversions of ill-posed operators, such as\nadditive Gaussian noise, blurring, etc., so an image may be generated from random noisy or blurry seeds.\nAnother line of research is designing autoregressive models, that apply the architecture of powerful\nLarge Language Models (LLMs) [21, 31, 33]. These autoregressive methods [8, 24] convert the image\npixel representation to a series of visual tokens and then apply generative language techniques.\nIn this paper, we reﬁne this line of research and provide a mathematically robust approach to the\nautoregressive image generation process.\nTo this end, we reach out to a classic technique in image\nprocessing, speciﬁcally, wavelet image coding [26, 29, 30]. Wavelets [4, 6, 17] are one of the main tools\nof modern approximation theory for nonlinear, adaptive approximation. The various wavelet transforms\nprovide the means to transform an image into a representation that captures the essence of the visual\ninformation in a sparse way. Typically, the signiﬁcant wavelet coeﬃcients are a small fraction of the\ncoeﬃcients and represent important edge and texture information, while the insigniﬁcant coeﬃcients\nwith small absolute values are associated with smooth regions of the image. The goal of wavelet image\ncompression methods, such as JPEG2000, is then to eﬃciently store the information of only the signiﬁcant\ncoeﬃcients.\nIn fact, the underlying method of the popular JPEG image compression algorithm [37],\ninvented in the 80s, contains many elements of wavelet coding, where a local Discrete Cosine Transform,\na precursor of wavelets, is used. However, in this paper, we leverage the progressive wavelet compression\ntechnique, a more advanced form of image compression. It creates a bit-stream where every bit corresponds\nto the next most important piece of visual information.\nSince we are generating images rather than\ndecoding them from a compressed ﬁle, there is no need to create actual binary bit-streams, and using a\n‘wavelet language’ of a limited number of tokens is suﬃcient.\nThus, our new approach to autoregressive image generation is based on two main ingredients. The ﬁrst\nis progressive wavelet image coding, which allows to tokenize the visual information of an image from\ncoarse to ﬁne details. This can achieved using as few as 6 tokens, by ordering the information starting with\nthe most signiﬁcant bits of the most signiﬁcant wavelet coeﬃcients. The second ingredient is a variant of\nan NLP decoder-only transformer [21, 31, 33] whose architecture was re-designed and optimized for token\nsequences in this ‘wavelet language’ The transformer learns the signiﬁcant statistical correlations within\na token sequence, which are the manifestations of well-known correlations between the wavelet subbands\nat various resolutions [1, 16, 18].\nDuring inference, this allows the generation of visually meaningful\nimages from an initial random seed generated from sampled from the distribution of the scaling function\ncoeﬃcients at the lowest resolution.\n1\n\n2\nWAEL MATTAR, IDAN LEVY, NIR SHARON AND SHAI DEKEL\nUsing the wavelet autoregressive approach, where the ‘wavelet language’ contains only a few tokens,\nprovides many attractive features. The length of the token sequences during training or inference can be\nﬂexible, where longer sequences imply more detailed or higher-resolution images. Guiding the generative\nprocess using a class aﬃliation or text prompting is easily achieved by concatenating the corresponding\nvector representations to the tokens’ vector representation of low dimension. Stochastic control using\nsimple transformer inference techniques, allows to create from one textual prompt a diversity of corre-\nsponding images. Furthermore, since each token is associated with the local support in the image domain\nof the corresponding wavelet, one can switch the guidance during the generative process to allow diﬀerent\nprompting for diﬀerent regions.\nOur paper is organized as follows. We begin with a review of related work in Section 2. In Section 3,\nwe review wavelet image coding and explain how one may extract from the classical theory the ability to\ntokenize the visual information of images. In Section 4, we focus on components of language transforms\nthat we redesign to serve our special wavelet language. We further provide several methods that can be\nused to direct the generation process under certain conditions: class label and/or textual prompt. In\nSection 5, we provide experimental results. Finally, in Section 6, we discuss possible future applications\nof our method, such as multi-modality generation and compositions of blobs.\n2. Related work\nIn this section, we ﬁrst review the current state of the art in image generation. We then review some\nmethods that apply wavelets as a frequency decomposition backbone for various aspects of style transfer,\nacceleration, and optimization of existing image generation methods, etc.\nCurrently, many commercial solutions apply diﬀusion-based models [7, 12, 19, 23, 25, 27]. In essence,\ndiﬀusion models learn inversions of ill-posed operators, such as additive Gaussian noise, blurring, etc.,\nso images may be generated from random noisy or blurry seeds. One then enforces various conditions\non images created through the time steps of the inversion process so that the ﬁnal generated image may\ncorrespond to a given text prompt.\nRecently, there is renewed interest in autoregressive methods with the hope that they will outperform\nthe diﬀusion models. The methods of VQGAN [8] and DALL-E [24] along with [34, 35] utilize a visual\ntokenizer to discretize images into grids of 2D tokens, which are then ﬂattened to a 1D sequence for\nautoregressive learning, mirroring the process of sequential language modeling. For example, in [24] a\ndiscrete variational autoencoder is trained to compress each 256 × 256 RGB image into a 32 × 32 grid\nof image tokens, where each such token can assume 8192 possible values. This creates a relatively short\ncontext sequence of 1024 tokens, but with a vocabulary of 8192 word tokens. The TiTok method, recently\nintroduced in [39], shows how to combine Vision Transformers with the Vector-Quantization method to\narrive at an autoregressive method that may use only 32 tokens. In comparison, our method may use\nonly 6-7 tokens for any image resolution and any level of ﬁne detail generation.\nIn contrast to typical raster-scan methods, where single tokens are sequentially predicted, the method\nof [32] provides an autoregressive learning algorithm based on predicting the image’s next-scale, or next-\nresolution.\nSome methods, such as [36, 40], use wavelets as means for frequency decomposition representations\nfor image inpainting, style transfer, and generative adversarial network methods. Some works propose\nto use wavelets as part of diﬀusion methods [11, 20] to speed up the diﬀusion approach by applying the\ndenoising process in the wavelet regime.\nTo the best of our knowledge, this is the ﬁrst time wavelets are being used as the basis for autoregressive\nimage generation.\n3. Elements of Wavelet Image Coding\nIn this section, we review some elements of wavelet image coding [26, 29, 30] that we use for our\ngenerative method. Essentially, we are interested in the process that takes an image in its raw pixel form\nas input and generates a sequence of tokens that capture its visual details. The structure of the sequence\nfrom coarse to ﬁne details is achieved by ordering the information starting with the most signiﬁcant bits\nof the most signiﬁcant wavelet coeﬃcients. En par with wavelet coding, we also have a goal to create\n\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION\n3\ntoken sequences that are as short as possible. This creates shorter contexts for the transformer decoder\nand improves its performance. As we shall see (Subsection 3.2.3) it is quite easy to convert sequences of\nfew wavelet tokens to shorter sequences at the tradeoﬀof using a larger vocabulary of tokens.\n3.1. Wavelet Transforms. A univariate wavelet system [4, 17] is a family of real functions in L2(R) of\nthe form\n\b\nψj,k : (j, k) ∈Z2\t\nbuilt by dilating and translating a unique mother wavelet function ψ\nψj,k(x) := 2−j/2ψ(2−jx −k),\nwhere the mother wavelet typically has compact support (or fast decay) and has r vanishing moments\n(3.1)\nZ\nR\nxkψ(x)dx = 0,\nk = 0, 1 . . . , r −1.\nWavelet systems can be constructed to serve a basis of L2(R). To facilitate applications, one then also\nconstructs a dual ˜ψ of ψ, where ⟨ψj,k, ˜ψj′,k′⟩= δj,j′δk,k′, so that for each f ∈L2(R),\nf =\nX\nj,k\n⟨f, ˜ψj,k⟩ψj,k.\nFor special choices of ψ , the set {ψj,k} forms an orthonormal basis for L2(R) and then, ψ = ˜ψ.\nUsually, one starts the construction of a wavelet system from a Multi-Resolution Analysis (MRA)\ngenerated by a scaling function ϕ ∈L2 (R) that satisﬁes a two-scale equation\nϕ =\nX\nk\nakϕ (2 · −k).\nOne then sets\nVj = span\nn\nϕj,k := 2−j/2ϕ\n\u00002−j · −k\n\u0001\n: k ∈Z\no\n,\nj ∈Z,\nwhich implies (under certain mild conditions)\n....V2 ⊂V1 ⊂V0 ⊂V−1 ⊂V−2...,\n∩Vj = {0} ,\n∪jVj = L2(R).\nAgain, to facilitate applications, one may also construct a dual ˜ϕ of ϕ, where ⟨ϕ0,k, ˜ϕ0,k′⟩= δk,k′, so that\nfor each f ∈Vj,\nf =\nX\nk\n⟨f, ˜ϕj,k⟩ϕj,k.\nEquipped with the MRA, one then proceeds to construct the wavelet ψ such that Wj := span {ψj,k : k ∈Z}\nwith Vj+1 + Wj+1 = Vj. A classic example for an orthonormal MRA and wavelet system where ϕ = ˜ϕ\nand ψ = ˜ψ, are the Haar scaling function and Haar wavelet\nϕ (x) :=\n\u001a\n1,\nx ∈[0, 1] ,\n0,\nelse.\nψ (x) :=\n\n\n\n1\nx ∈\n\u0002\n0, 1\n2\n\u0001\n,\n−1\nx ∈\n\u0002 1\n2, 1\n\u0003\n,\n0\nelse.\nThe bivariate Haar system (see below) is a good choice when working with piecewise constant images,\nsuch as the MNIST handwritten digits [5]. For some of our experiments, we use a famous wavelet system\nfrom the Cohen Daubechies Feauveau (CDF) family of wavelets [4], which is sometimes termed bior4.4\n(r = 4 in (3.1)) or [9,7] in the signal processing community (the supports of the scaling functions and\nwavelets, as well as the lengths of the associated ﬁlters, are 9 and 7). The generating functions of the\nbior4.4 are depicted in Figure 1.\nThe wavelet model can be easily generalized to any dimension, via a tensor product of the wavelet and\nthe scaling functions. Assume that the univariate dual scaling functions ϕ, ˜ϕ and dual wavelets ψ, ˜ψ, are\ngiven. Then, a wavelet bivariate basis is constructed using three types of basic wavelets\nψ1(x1, x2) := ϕ(x1)ψ(x2),\nψ2(x1, x2) := ψ(x1)ϕ(x2),\nψ3(x1, x2) := ψ(x1)ψ(x2),\n˜ψ1(x1, x2) := ˜ϕ(x1) ˜ψ(x2),\n˜ψ2(x1, x2) := ˜ψ(x1) ˜ϕ(x2),\n˜ψ3(x1, x2) := ˜ψ(x1) ˜ψ(x2).\n\n4\nWAEL MATTAR, IDAN LEVY, NIR SHARON AND SHAI DEKEL\nFigure 1. The CDF [9,7] wavelet system (ﬁgure reproduced from [3]).\nThe bivariate wavelet transform of f ∈L2(R2), in terms of the bivariate wavelet tensor basis\nψe\nj,k := 2−jψe(2−j · −k),\n˜ψe\nj,k := 2−j ˜ψe(2−j · −k),\ne = 1, 2, 3, j ∈Z, k ∈Z2,\nis then\nf =\nX\ne=1,2,3,j∈Z,k∈Z2\n⟨f, ˜ψe\nj,k⟩ψe\nj,k.\nThe bivariate wavelet decomposition can thus be interpreted as a signal decomposition in a set of three\nspatially oriented frequency subbands: LH(e = 1) detects horizontal edges; HL (e = 2) detects vertical\nedges and HH (e = 3) detects diagonal edges.\nUnder the assumption that ψ and ˜ψ are compactly supported (or have fast decay), a wavelet coeﬃcient\n⟨f, ˜ψe\nj,k⟩at a scale j represents the information about the function in the spatial region of radius ∼2j\nin the neighborhood of 2jk, k ∈Z2. At the next ﬁner scale j −1, the information about this region is\nrepresented by four wavelet coeﬃcients, which are described as the children of ⟨f, ˜ψe\nj,k⟩. This leads to a\nnatural tree structure organized in a quad tree structure of each of the three subband types as shown in\nFigure 2. As j decreases, the child coeﬃcients add ﬁner and ﬁner details into the spatial regions occupied\nby their ancestors.\nIn image processing, one uses the Discrete Wavelet Transform (DWT). It works by initially assuming\nthat the image pixels {fk = fk1,k2}M\nk1,k2=1 are good approximants of the projections on the shifts of the\ndual scaling function with the underlying function f (see [17, Section 7.3.1] for a detailed justiﬁcation)\nfk ≈⟨f, ˜ϕ0,k⟩.\nWith these coeﬃcients as input, one uses the DWT to compute coeﬃcients down to some predeﬁned\nlow-resolution m. For simplicity, we may assume that M = 2m and that we use the DWT to compute\n(3.2)\n{⟨f, ˜ϕm,k⟩},\n{⟨f, ˜ψe\nj,k⟩},\n1 ≤j ≤m,\ne = 1, 2, 3.\nWavelet representations are considered very eﬃcient for image compression [26, 29, 30].\nThe edge\ninformation typically constitutes a small portion of a typical image, while the dual wavelet coeﬃcients have\na large absolute value only if edges intersect the support of the corresponding dual wavelets. Consequently,\nthe image can be approximated well using a few signiﬁcant wavelet coeﬃcients. A clear statistical structure\nalso follows: large/small values of wavelet coeﬃcients tend to propagate through the scales of the quadtrees\ndepicted in Figure 2. As an example, a sparse wavelet representation of a 512 × 512 ﬁshing boat image\nand a compressed version of it are shown in Figure 3, where the compression algorithm JPEG2000 is\n\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION\n5\nFigure 2. Wavelet coeﬃcient tree structure across the subbands (MRA decomposition).\nbased on the sparse representation. The Figure clearly depicts that the signiﬁcant wavelet coeﬃcients\n(coeﬃcients with relatively large absolute values) are located on strong edges of the image.\n(a) Fishing boat image.\n(b) 15267 signiﬁcant coeﬃcients.\n(c) Compressed image 1:17.\nFigure 3. Image compression based on sparse wavelet approximation.\n3.2. Embedded Wavelet Tokenization. The sparse wavelet representation (3.2) of an image provides\nthe perfect infrastructure for the generation of embedded coding representations [26, 29, 30]. Embedded\ncoding is similar in spirit to binary ﬁnite precision representations of real numbers, where the “encoding”\ncan cease at any time and provide the “best” approximation of the real number achievable within the\nframework of the binary digit representation. Similarly, the embedded coder can cease at any time and\nprovide the “best” representation of an image achievable within its framework. Embedded coding streams\ncan be generated from wavelet representations by ordering the information on the wavelet representation\nstarting with the most signiﬁcant bits of the most signiﬁcant coeﬃcients. That is, the coeﬃcients with\nthe largest absolute value. In image coding applications the goal is to generate a compressed bit stream\nof ‘0’ and ‘1’s. This can be eﬃciently achieved by using information theoretical tools such as arithmetic\ncoding. Here, our goal is somewhat diﬀerent where we aim to create an eﬃcient tokenization method\nconforming to the following two objectives:\n(i) Ensuring statistically frequent structural patterns - The existence of common patterns within\nthe token sequence allows the language models to learn them as contexts when they attempt\nto generate the most probable next token in the context of the previous tokens. The wavelet\ntokenization processes described below provide that by creating token sequences that are ordered\n\n6\nWAEL MATTAR, IDAN LEVY, NIR SHARON AND SHAI DEKEL\nbased on coeﬃcient absolute value and then resolution.\nIt is known [26, 29], that there are\nstrong correlations between insigniﬁcant coeﬃcients with their ‘ancestors’ at lower resolutions\n(see Subsection 3.2.2).\n(ii) Trade-oﬀbetween sequence length versus number of tokens - When token sequences become very\nlong, the LLMs need to deal with longer contexts, which can be challenging. At the same time, a\ndataset of possibly shorter sequences that are based on a large vocabulary of tokens can also be\nchallenging for diﬀerent reasons. As we shall see, the method of Subsection 3.2.1 uses 7 tokens\nand may create long sequences. The method of 3.2.2 is a somewhat more advanced and manages\nto both reduce the number of tokens by 1 and at the same time reduce the sequence lengths\nsigniﬁcantly as the dimensions of the images increase. In Subsection 3.2.3 we review standard\nmethods that allow to control this tradeoﬀby merging frequent sub-sequences into new tokens.\nFirst, for simplicity of notation, using (3.2), denote for I = (i1, i2), 1 ≤i1, i2 ≤2\nαI := ⟨f, ˜ϕm,I⟩.\nWe also map the coeﬃcients {⟨f, ˜ψe\nj,k⟩}, 1 ≤j ≤m, e = 1, 2, 3,\nαI ←⟨f, ˜ψe\nj,k⟩,\nbased on their location\nI = (i1, i2),\ni1 ≥3 ∨i2 ≥3,\ni1 ≤M ∧i2 ≤M,\nin the coeﬃcient matrix. We note in passing that one may assume that the low resolution scaling function\ncoeﬃcients from (3.2) are known during training and are randomly sampled from some distribution during\nimage generation and therefore need not be part of the tokenization.\n3.2.1. Encoding a wavelet representation into a token sequence. We now show how to process the numeric\nrepresentations of the coeﬃcients, from most signiﬁcant to least signiﬁcant and ‘encode’ them into a\nrelatively compact series of tokens. The representation using the series of tokens should be invertible. That\nis, one should be able to convert (e.g. ‘decode’) the token sequence back to the wavelet representation.\nTo this end, assuming the image pixels are normalized to the range [0, 1], one can show that for an\nimage of dyadic dimension [M, M] = [2m, 2m], after m −1 iterations of the bivariate DWT\n(3.3)\nmax\nI\n|αI| ≤2m−1.\nAssuming for simplicity that all images of a given dataset have the same dyadic dimensions [M, M],\nthen this bound holds for all of their wavelet representations. Our ﬁrst option is to initialize a threshold\nT = 2m−2 and begin scanning the wavelet coeﬃcients of the image, in a predetermined order (see below)\nfor signiﬁcance, with the goal of reporting only those coeﬃcients for which the following holds\nT ≤|αI| < 2T.\nOur second option, is to compute separately for each image in the dataset\n(3.4)\n˜m := ⌈log2 max\nI\n|αI|⌉,\nand then initialize for the speciﬁc image T = 2 ˜m−1. In this scenario, we store and use the parameter ˜m\nfor each image in the training set along with its sequence of tokens.\nWe also maintain a matrix of approximated wavelet coeﬃcients {˜αI} which we initialize with zeros.\nOnce we complete the processing at a given bit plane, we update T ←T/2 and repeat the process. At\neach bit-plane we report the signiﬁcant coeﬃcients that were just uncovered in this bit-plane using a\ntoken ‘NowSigniﬁcantNeg’ if the coeﬃcient is negative or a token ‘NowSigniﬁcantPos’ if it is positive.\nAt the time of uncovering, we modify the approximation of the coeﬃcient ˜αI to have the absolute value\n3T/2, with the reported sign. Next, we add a token to represent the coeﬃcient’s next signiﬁcant bit,\n‘NextAccuracy0’ if the coeﬃcient satisﬁes |αI| ≤|˜αI| or the token ‘NextAccuracy1’ if |αI| > |˜αI|. The\napproximation ˜αI is updated accordingly by subtracting or adding T/4 (depending on the sign of the\ncoeﬃcient and the accuracy bit type).\nLet us demonstrate with an example. Assume T=16 and αI = −17.45. Therefore, the coeﬃcient is\nﬁrst uncovered in the current bit plane. When we arrive at the index I, we report a ‘NowSigniﬁcantNeg’\n\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION\n7\ntoken for this coeﬃcient, providing it with a temporary approximation ˜αI = −24, which lies in the middle\nof the segment [−T, −2T] = [−16, −32]. Next, since in fact |αI| ≤24, we report a token ‘NextAccuracy0’\nto represent the coeﬃcient’s next signiﬁcant bit, providing an updated approximation ˜αI = −20, which\nlies in the middle of the segment [−T, −3T/2] = [−16, −24], leading to a better approximation of the\nground truth value.\nIn case a coeﬃcient has been uncovered in any of the previous bit-planes and is already known to be\nsigniﬁcant, we only add one of the tokens ‘NextAccuracy0’ if |αI| ≤|˜αI| or ‘NextAccuracy1’ if |αI| > |˜αI|.\nWe then update the approximation ˜αI by subtracting or adding T/4 (depending on the sign of the\ncoeﬃcient and the accuracy bit type).\nAssuming the bit-plane scanning order of the coeﬃcients is ﬁxed, one then only needs to add the token\n‘Insigniﬁcant’ to provide a valid invertible tokenization process. One simply scans the coeﬃcients in the\nﬁxed order and uses their true known value to test and apply one of three possibilities:\n(i) |αI| ≥2T: The coeﬃcient has already been reported as signiﬁcant in a previous bit-plane. There-\nfore one reports the token ‘NextAccuracy0’ or ‘NextAccuracy1’ depending on the test |αI| < |˜αI|.\n(ii) T ≤|αI| < 2T: First report the token ‘NowSigniﬁcantNeg’ or ‘NowSigniﬁcantPos’ depending on\nthe sign and then report the token ‘NextAccuracy0’ or ‘NextAccuracy1’.\n(iii) |αI| < T: report ‘Insigniﬁcant’.\nThe process described above, although completely suﬃcient for invertible tokenization, potentially\ncreates long sequences.\nSpeciﬁcally, it does not take into consideration the local correlations among\n‘neighboring’ insigniﬁcant wavelet coeﬃcients. Due to the sparsity property of the wavelet transform,\nduring the scanning process, many of the ‘Insigniﬁcant’ coeﬃcients form local groups. Moreover, there are\ncorrelations between local groups of insigniﬁcant coeﬃcients of the same subband type across resolutions\nin the manner of the quad-tree structure of Figure 2. Image compression algorithms such as the EZW\n[29] or SPIHT [26], are based on statistical zero tree models that try to capture these correlations across\nresolutions (see the Zero-Tree method in the next subsection). As we shall later see, for image generation,\nwe actually rely on the powerful capabilities of the transformer models to learn correlation patterns of the\n‘wavelet language’ of the given dataset. However, we do ‘ease the burden’ oﬀthe transformers signiﬁcantly\nby utilizing the structure of the groups of insigniﬁcant coeﬃcients to reduce the size of the token sequences,\nthereby creating shorter contexts.\nTo this end, we add two additional tokens for groups of insigniﬁcant coeﬃcients: ‘Group4x4’ and\n‘Group2x2’ and modify the scanning process to visit the coeﬃcients based on groups of 4 × 4. The ﬁrst\ntoken is used in locations where the scan is at an index (4l1, 4l2), for some integers l1, l2. If at the current\nbit plane, all the 16 coeﬃcients with indices I = (i1, i2), 4l1 ≤i1 ≤4(l1 + 1), 4l2 ≤i2 ≤4(l2 + 1), are still\ninsigniﬁcant, we issue the token ‘Group4x4’ and the tokenization process continues to the next group of\n4 × 4 coeﬃcients. However, if any of the coeﬃcients of the 4 × 4 group becomes signiﬁcant in the current\nbit-plane, the group breaks down to 4 groups of 2 × 2. If a group of 2 × 2 is still composed of insigniﬁcant\ncoeﬃcients at the current bit-plane, we add a token ‘Group2x2’. If a group of 2×2 breaks down, then each\ncoeﬃcient from the group is reported individually as being ‘Insigniﬁcant’ or one of ‘NowSigniﬁcantNeg’,\n‘NowSigniﬁcantPos’. The scanning process keeps track of which groups broke up, so that only necessary\nand informative tokens are generated. We summarize the seven tokens and their roles below\n(i) ‘Group4x4’ – At the index (4l1, 4l2), the group of 16 coeﬃcients {αI}, 4l1 ≤i1 ≤4l1 + 4, 4l2 ≤\ni2 ≤4l2 + 4, are still insigniﬁcant, |αI| < T.\n(ii) ‘Group2x2’ – At the index (2l1, 2l2), the group of 4 coeﬃcients {αI}, 2l1 ≤i1 ≤2l1 + 2, 2l2 ≤\ni1 ≤2l2 + 2, are still insigniﬁcant, |αI| < T.\n(iii) ’NowSigniﬁcantNeg’, ’NowSigniﬁcantPos’ – At the current location I, the coeﬃcient satisﬁes T ≤\n|αI| < 2T.\nIf the coeﬃcient was part of a group of insigniﬁcant coeﬃcients at the previous\nbit-plane, the group is now automatically dissolved.\n(iv) ‘Insigniﬁcant’ – At the current location I, the coeﬃcient is still insigniﬁcant and satisﬁes |αI| < T.\nIf the coeﬃcient was part of a group of insigniﬁcant coeﬃcients at the previous bit-plane, the group\nis now automatically dissolved.\n\n8\nWAEL MATTAR, IDAN LEVY, NIR SHARON AND SHAI DEKEL\nv) ‘NextAccuracy0’, ‘NextAccuracy1’ – At the current location I, the coeﬃcient has already been\nreported to be signiﬁcant since it satisﬁes |αI| ≥T. Here, we improve the accuracy of its approx-\nimation using one of these tokens, depending on the test |αI| < |˜αI|.\nThe bit-plane scan is carried out in two nested loops; the outer loop proceeds from low resolution to\nhigh resolution, each time traversing the three types of wavelet subbands. The inner loop traverses the\n4 × 4 blocks. Figure 4 illustrates the outer and inner scanning patterns.\n(a) Outer subband scanning order.\n(b) Inner scanning order of 4 × 4\nblocks.\nFigure 4. A sketch illustrating the outer and inner scanning orders.\nFigure 5 exempliﬁes the tokenization algorithm of an image from the MNIST dataset [5]. The image\nwas padded with zeros to be of dimensions M × M = 32 × 32, with m = 5. The bottom row of the\nﬁgure shows the tokens and their locations on the wavelet image for the ﬁrst three bit planes. To make\nthe process clearer, we explicitly write the resulted sequence of tokens for the ﬁrst bit plane shown in\nFigure 5(d).\n{‘Insigniﬁcant’, ‘Insigniﬁcant’, ‘NowSigniﬁcantPos’, ‘Insigniﬁcant’,\n‘Insigniﬁcant’, ‘Insigniﬁcant’, ‘NowSigniﬁcantNeg’, ‘Insigniﬁcant’,\n‘Group2x2’, ‘Group2x2’,\n‘Insigniﬁcant’, ‘Insigniﬁcant’, ‘Insigniﬁcant’, ‘NowSigniﬁcantNeg’,\n‘Group2x2’,‘Group2x2’, ‘Group2x2’,\n‘Group4x4’, . . . , ‘Group4x4’}\nThe token sequences of the second and third bit-planes follow the same scanning pattern. Eventually,\nthe three sequences are concatenated in the natural order to form the ﬁnal sequence which describes the\nthree bit planes wavelet image appearing in Figure 5(c).\nThere is a very important hyper-parameter which is the choice of the smallest threshold at the ﬁnal\nbit-plane. Through this hyper-parameter, the wavelet representation provides us with a very robust and\nstable trade-oﬀof ﬁne detail generation and length of token sequences. Choosing a ﬁnal threshold provides\nvery consistent control over visual quality relating to: “Visually Lossless”, “High”, “Medium”, “Low”,\netc. This is en par with the quality settings in digital cameras, which in turn lead to a selection of the\ncorresponding quantization tables of the JPEG algorithm generating the compressed images.\n3.2.2. Zero-tree tokenization. The Zero-Tree method is an alternative tokenization method that is aligned\nwith [29] and provides shorter sequences, especially as image size increases.\nThe zero-tree approach\nleverages on the correlations of insigniﬁcant coeﬃcients across resolutions.\nStatistically, if a wavelet\ncoeﬃcient at some resolution is insigniﬁcant, then with very high probability (around 90% for real life\nimages) its descendants at the same subband and higher resolutions will also be insigniﬁcant. With the\nzero-tree tokenization method the scanning visits coeﬃcients from low to high resolution and the tokens\n\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION\n9\n(a) 32 × 32 padded MNIST image.\n(b) Wavelet transform.\n(c) Signiﬁcant coeﬃcients after 3 bit-\nplanes.\n(d) Wavelet domain tokenization -\nﬁrst bit plane.\n(e) Wavelet domain tokenization -\nsecond bit plane.\n(f) Wavelet domain tokenization -\nthird bit plane.\nFigure 5. Depiction of the tokenization process. On the top left and middle, a 32 × 32\npadded MNIST image and its wavelet transform. On the top right, the wavelet approxi-\nmation generated by the ﬁrst three bit-planes. The bottom row illustrates the tokens and\ntheir locations on the 32 × 32 grid, where, ‘NowSigniﬁcantNeg’ and ‘NowSigniﬁcantPos’\ntokens are annotated with orange “−” and blue “+” and signs respectively. The tokens\n‘NextAcurracy0’ and ‘NextAccuracy1’ are marked with green down and red up triangles.\nThe purple dots represent ‘Insigniﬁcant’ coeﬃcients and the brown and pink squares rep-\nresent the ‘Group2x2’ and ‘Group4x4’ zero block tokens.\n‘Group2x2’ and ‘Group4x4’ are replaced with a single ‘zero-tree’ token. If the token is reported at a\ncertain location in the scan, then it is understood that the coeﬃcient at this location as well as all its\ndescendants are still insigniﬁcant at the current bit-plane. The descendants of a coeﬃcient at location\nI = (i1, i2),\ni1 ≥3 ∨i2 ≥3,\ni1 ≤M/2 ∧i2 ≤M/2,\nare its children\n{(2i1, 2i2), (2i1, 2i2 + 1), (2i1 + 1, 2i2), (2i1 + 1, 2i2 + 1)}\nand then recursively their children. Once a coeﬃcient is reported as a ‘zero tree’ coeﬃcient, it is understood\nthat all of its descendants are still insigniﬁcant and the scanning skips them. For the FashionMNIST\ndataset (see examples below) the mean token sequence length is 1822.5 for the zero blocks tokenization\nmethod and 1601.7 for the zero tree method, although the latter uses 6 tokens instead of 7.\n\n10\nWAEL MATTAR, IDAN LEVY, NIR SHARON AND SHAI DEKEL\n3.2.3. The trade-oﬀof vocabulary size and sequence lengths. It is quite standard in the ﬁeld of autore-\ngressive methods to control the tradeoﬀbetween the token vocabulary size and the dataset’s mean token\nsequence length in an attempt to ﬁnd the optimal conﬁguration for given computational resources and\nmodel architecture. Since the wavelet method uses a relatively very small number of tokens (language\nmodels typically support a vocabulary of tens of thousands of tokens) and creates relatively long sequences,\nit is relevant in scenarios where the computational resources do not allow the use of long token sequences.\nOne of the simplest methods is Byte Pair Encoding (BPE) [10]. BPE is a subword tokenization technique\ncommonly used in natural language processing, which iteratively merges the most frequent pairs of tokens\ninto new tokens, thereby creating a more compact representation of the data. We used HuggingFace’s\ntokenizers library [14] to generate Figure 6\nFigure 6. The trade-oﬀbetween vocabulary size and token sequence length for the fash-\nionMNIST dataset.\n3.2.4. Decoding the token sequence into an approximate wavelet representation. The tokenization process\ndescribed in the previous subsections can be easily inverted back to an approximate wavelet representation.\nMoreover, any initial sub-sequence can be inverted to provide a possibly coarser approximation.\nWe\ninitialize a matrix of size M × M of the approximated wavelet coeﬃcients {˜αI} with zeros and begin\nthe scanning process with the ﬁrst bit-plane. Based on (3.3) or (3.4), we know how to initialize the ﬁrst\nbit-plane with the initial threshold T = 2m−2 or T = 2 ˜m−1. We then process the token sequence and\nupdate the approximated coeﬃcients using the corresponding ‘signiﬁcant’ and ‘bit accuracy’ tokens. If for\nany given reason, the sequence of tokens terminates, we have the best possible approximated coeﬃcients\n{˜αI} from which we can obtain an approximated image by applying the inverse DWT. Our decoding\nprocess relies on the assumption that the token sequence is valid.\nFor example, a ‘Group4x4’ token\ncannot appear while the decoder scan position is at a location of indices not divisible by 4. It is obvious\nhow to achieve this in the context of image coding. However, during an image generation process, this\nneeds to be enforced using the conditional next-token inference described in Subsection 4.6.\n4. The Generative Wavelet Transformer\nAssume that for a certain dataset of images, we have established the translation of the visual information\nof each image to a sequence of tokens encapsulating the visual information from coarse to ﬁne details as\nexplained in Subsection 3.2. We assume that within the sequences, distinct patterns and relations exist\nbetween the tokens. For example, the wavelet coeﬃcients {⟨f, ˜ψe\nj,k⟩} of wavelets {ψe\nj,k} whose support\nintersects with a certain portion of an edge of the image, will be signiﬁcant and aligned across scales in\na tree-like structure as per Figures 2 and 3(b). At the same time, coeﬃcients of wavelets whose support\nintersects with a smooth area of the image will be insigniﬁcant and they appear in local groups. As\nexplained, they also have a tree structure across scales, that can be captured explicitly by a ‘zero tree’\ntoken. This leads to the intuition that the powerful transformers created over the last few years [21] are\n\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION\n11\nable to learn the patterns of the ‘wavelet language’ and to generate them from some random seeds during\ninference.\nIn this section, we describe how we modiﬁed the architecture of the DistilGPT2 transformer model [28]\nto optimize it to align with the wavelet-based image generation method. This obviously requires training\nthe modiﬁed model from scratch. We found it useful to use the code from HuggingFace [13] as a starting\npoint.\n4.1. Token vector representation. Typically, in the standard scenarios of spoken languages, trans-\nformers apply a ‘pre-processing’ learnable transform to tokens to convert them to vector representations.\nThe idea is that similar words should be converted to vectors with some proximity, which intuitively\nserve as better input for the transformer’s neural network.\nHowever, with the method of Subsection\n3.2.1, our wavelet dictionary includes only 7 tokens that have very distinctive and diﬀerent roles. There-\nfore, the simple transformation of the tokens to the one-shot encoding of the standard basis of dimen-\nsion 7 is probably a better, if not optimal choice. Thus, the initial vector representation of a token is:\n‘Group4x4’ →(1, 0, 0, 0, 0, 0, 0), ‘Group2x2’ →(0, 1, 0, 0, 0, 0, 0), etc. Therefore, in our ‘wavelet’ trans-\nformers the ‘token →vector’ learnable transformation is removed.\n4.2. Initial bit-plane threshold. Recall that we have two options: to use a uniform initial bit-plane\nthreshold for all images in the dataset derived from (3.3), or to use an adaptive initial threshold for each\nimage of the training set using (3.4). In the latter case, we need to inform the transformer, per image,\nwhich initial threshold the token sequence is associated with. We do this as follows: assume a given\ndataset has l possible values for ˜m in (3.4) (e.g., l = 4 for the MNIST dataset, see Figure 7). Then, we\nconcatenate a one-shot encoding of dimension l of the initial threshold parameter of the given image to\neach vector representation of each token.\nFor image generation, one may sample randomly from the distribution of l possible initial thresholds.\nIn the case that the image generation is conditioned on a certain class (see Subsection 4.4), one can sample\nfrom the conditional distribution of the possible thresholds of the speciﬁc class.\nFigure 7. Distribution of log2 of the initial thresholds for the 70,000 MNIST images with\nthe Haar wavelet transform.\n4.3. Positional encoding. In classic transformer architectures [21], one adds the positional encoding\nvp(t) of the position t to the token’s vector representation ve(x(t)). Learnt positional embedding applied\na learned transform t →vp(t). Some transformers use hard-coded mapping of the position. Assuming the\nvector embedding dimension is de and the maximum length of a sequence is lmax, then\nvp(t)(2i −1) = sin(t/l2i/de\nmax ),\nvp(t)(2i) = cos(t/l2i/de\nmax ),\n1 ≤i ≤de/2.\nIn our scenario of the wavelet language, the position of a token in a sequence is (bp, I), where bp is the\nenumeration of the bit-plane and I = (i1, i2) is the index of the current coeﬃcient αI in the scan order.\n\n12\nWAEL MATTAR, IDAN LEVY, NIR SHARON AND SHAI DEKEL\nTherefore, we concatenate to the vector representation of a token from Subsection 4.1, a vector component\nof dimension 3 with the location of the token (bp, i1, i2).\n4.4. Generative guidance. It is obviously critical for any image generation method to allow guidance\nof the generative process by placing a condition on the class type of the generated image or a text prompt\nthat describes it. Some image generation models apply a joint embedding space for text and images for\nthis purpose. One such method is to used a pretrained model such as CLIP [22] that maps text and\nimages to a joint embedding space. The CLIP contains an image encoder f and a caption encoder g,\nthat during training over pairs of images with captions {(x, c)}, optimizes a contrastive cross-entropy loss\nthat encourages high dot-products ⟨f(x), g(c)⟩in the joint embedding space. Thus, any image generation\nmethod, can use the vector embedding of the given text prompt c to guide the generative process by\nconditioning the image embedding f(x) to be highly correlated with the embedding of the textual prompt.\nIn our case, since we converted the problem of image generation to a ‘wavelet-language’ generation,\nwe can apply ‘text’-type prompting methods.\nHaving access to a joint embedding text-image space\nallows us to train using the vector representation of the image training set. Then, at image generation,\nwe use the vector representation of the given text prompt to guide the generative process. There are\nvery simple ways of using these vector representations. We choose to concatenate them to the vector\nrepresentation of each token and its position (as explained above). For example, as shown in Section 5,\nfor the image datasets MNIST or FashionMNIST with 10 classes, it is easy to concatenate a vector of\nlength 10 representing the class of the image. In the case where we wish to guide the generative process\nusing a textual prompt, we may concatenate the CLIP vector embedding [22] of the textual prompt to\neach token vector representation. As we discuss in Subsection 6.2, we hope this approach to guiding the\ngenerative process can be generalized to composition of blobs [38], where a given guiding vector of a blob\nis used only at positions of the scan where the support of the corresponding wavelet intersects the blob.\n4.5. Initialization of the generative process. Since the guidance of the generative process (Subsection\n4.4) is applied through the concatenation of vector representations to each token vector representation, in\nsome cases, the initialization becomes a minor issue. For example, when training on MNIST and generating\ndigits, one can get away with a simple random choice from the subset: ‘Insigniﬁcant’, ‘NowSigniﬁcantNeg’\nor ‘NowSigniﬁantPos’ for the ﬁrst token and from there the transformer will generate a valid token sequence\nwhich is converted to an adequate image of a digit from the pre-selected class.\nA more robust method is as follows. Suppose we wish to generate a handwritten digit from a certain\ndigit class. Let {fs}s∈S be the subset of MNIST images from that speciﬁc digit class and let\n(4.1)\n{⟨fs, ˜ϕm,k⟩},\ns ∈S,\nk = (k1, k2),\n1 ≤k1, k2 ≤2,\nbe the subset of low-resolution coeﬃcients of these images deﬁned by (3.2). Let N(v, Σ) be the fourth-\ndimensional normal distribution, approximated by the subset (4.1). We then sample from N(v, Σ), a\nrandom group of four low-resolution coeﬃcients. Now, the token representation of these coeﬃcients can\nserve as a basis for a robust initialization of the generative process of the required digit. In the case where\nthe guidance is provided by a vector representation of some text-prompt, one can create the normal\ndistribution using a subset of K-nearest neighbors in the image vector representation space.\nOnce some random seeding allows us to initialize the token sequence, we may introduce as much\ndiversity as required using the methods of Subsection 4.7 so that even using the same seed may generate\nvarious images corresponding to the given guidance.\n4.6. Conditional next token inference. In Greedy generative mode, using the method of Subsection\n3.2.1, the next selected token x(t), 1 ≤x(t) ≤7, at location t, is the token for which the transformer\nassigns the highest probability from (p1(t), ..., p7(t)). As described in Subsection 4.7 below, there are\nvarious alternative methods to control the output of the transformer. However, since each generative\ntoken inference step is a statistical event, it may occur that the next predicted token is not valid at the\ncurrent position of the wavelet bit-plane scan. To overcome this, we apply conditional probability to\nensure any selected token satisﬁes the conditions below relating to the context and the current position\nin the scan.\n\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION\n13\n(i) ‘Group4x4’ - The scan is at an index (4l1, 4l2) and the group has not yet dissolved.\n(ii) ‘Group2x2’ - The scan is at an index (2l1, 2l2) and the group has not yet dissolved.\n(iii) ’NowSigniﬁcantNeg’, ’NowSigniﬁcantPos’ - At the current location I, the coeﬃcient αI is still\ninsigniﬁcant, possibly as part of a group of insigniﬁcant coeﬃcients.\n(iv) ‘Insigniﬁcant’ - At the current location I, the coeﬃcient αI is still insigniﬁcant, possibly as part\nof a group of insigniﬁcant coeﬃcients.\nv) ‘NextAccuracy0’, ‘NextAccuracy1’ - At the current location I, the coeﬃcient αI has already been\nreported to be signiﬁcant.\n4.7. Controlling the degree of generative diversity during inference. Since we are applying a\nlanguage transformer model we may use various simple stochastic mechanisms to control the generative\nprocess during inference and allow a diversity of possible images to be generated from a single prompt.\nSome of the available stochastic methods are: Beam search with multinomial sampling, Top-k and Top-p.\nIn our experiments, we tested the latter two:\n(i) Top-k sampling - The Top-k inference method [9] ﬁlters the k most likely next words ﬁrst and then\nsamples from the probability mass that is redistributed among only those k next words. GPT2\nadopted this sampling scheme, which was one of the reasons for its success in story generation.\nIn Figure 9 below, we see a diversity of sandals generated by guiding the model with the vector\nrepresentation of the corresponding FashionMNIST ‘sandal’ class and using the Top-2 method.\nWe see that using k = 2 is suﬃcient to move the generative process from a deterministic process\nto a suﬃciently diverse stochastic process, yet with output that ﬁts the class description.\n(ii) Top-p sampling- In Top-p sampling or nucleus sampling, the selection pool for the next token is\ndetermined by the cumulative probability of the most probable tokens. Setting a threshold p, the\nmodel includes just enough of the most probable tokens so that their combined probability reaches\nor exceeds this threshold. Again, the distribution mass is redistributed among these tokens and\nthen the next token is sampled using this distribution. In Figure 8 we see diﬀerent examples of\nthe digits ‘3’ and ‘8’ generated using the Top-0.6 method.\n5. Experimental results\nWe conducted experiments on the MNIST and FashionMNIST datasets. Here are some details:\n• The images in both datasets were padded with zeros to M × M = 32 × 32, where M = 2m, m = 5\nand normalized to have values within [0, 1].\n• We used the Haar wavelet basis for the MNIST images and the bior4.4 wavelet basis for the\nFashionMNIST.\n• The images were tokenized with a ﬁnal threshold of T = 2−3 for MNIST and T = 2−4 for\nFashionMNIST.\n• The maximal token sequence lengths were 1742 for MNIST and 3098 for FashionMNIST.\n• We trained two separate distillgpt2 models from scratch on the two datasets. As for the training\nconﬁgurations, both training sessions had batch size 4, learning rate 0.0004, and weight decay\n0.01.\n• Models were trained on an NVIDIA A100 GPU with 80GB; MNIST occupied around 22GB while\nFashionMNIST occupied 61GB. Both models were trained for a few days.\nResults with diﬀerent controlling methods appear below in Figures 8 and 9.\n\n14\nWAEL MATTAR, IDAN LEVY, NIR SHARON AND SHAI DEKEL\nFigure 8. Digits generated with Top-p = 0.6 along with a depiction of the generated\nwavelet coeﬃcients.\nFigure 9. Sandals generated with Top-k = 2 along with a depiction of the generated\nwavelet coeﬃcients.\nMore generated images for diﬀerent classes of MNIST and FashionMNIST appear in the following\nﬁgures.\nFigure 10. More MNIST results.\n\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION\n15\nFigure 11. More FashionMNIST results.\n6. Discussion and future work\nIn this paper, we introduced a novel method for image generation that is based on elements of wavelet\nimage coding and NLP transformers. Unfortunately, our research group does not have access to suﬃcient\ncomputational resources at the moment, so this work serves as a ﬁrst modest proof of concept. Indeed,\nthe wavelet representation is a powerful tool in image processing that can serve as a basis for many image\ngeneration functionalities. Here, we list some directions that we will consider for future work.\n6.1. Generation of color images at high resolution and with ﬁne details. In our experiments,\nwe only generated small grayscale images.\nWe provide here some details on how the method can be\ngeneralized:\n(i) Color images - For color images (or even spectral images), we may adopt a well-known paradigm\nfrom image compression. For improved performance, one may transform input images in the RGB\ncolor space to the Y CbCr color space. The Y component is the luminance component, essentially\nthe image’s grayscale part. The other two components, Cb and Cr, capture the color information\nof the image. Typically, the luminance component carries most of the visual information, and\nthus also, its encoding is usually the signiﬁcant part of an encoded image. In image coding, one\nusually encodes separately each of the three channels. Our method can then be generalized to\ncolor images by applying the DWT and the tokenization process separately to each color channel.\n(ii) Generating ﬁne details - Using our wavelet model, ﬁner details are captured at higher bit-planes.\nThe choice of the ﬁnal threshold of the ﬁnal bit-plane provides excellent and very consistent control\nover the amount of detail one wishes to generate. This quantization technique is at the heart of\nthe JPEG algorithms and translates to very speciﬁc modes in digital cameras that can be set to:\n“Visually Lossless”, “High”, “Medium”, etc. This exact form of control also applies to wavelets\nbut, unfortunately, is not the default mode of operation in JPEG2000. Obviously, to generate\nﬁner details, one needs to train the transformer on longer token sequences, again requiring more\ncomputational resources.\n6.2. Support for generation of compositions of blobs. In many cases, one wishes to apply ﬁne-\ngrained control of compositional text-to-image generation, where certain locations in the image, marked\nperhaps with bounding boxes or ellipses, receive diﬀerent textual descriptions [38]. One possible method\nto accomplish this using the wavelet generative approach is to apply the transformer in evaluation mode\nand apply the vector representation of the blob’s textual prompt as described in Subsection 4.4 whenever\nthe bit-plane scan is at indices of wavelet coeﬃcients whose support intersects the blob.\n6.3. Multi-modal generation. The ability to represent an image’s visual information as a sequence of\ntokens presents an attractive possibility of merging the wavelet-based tokens with other language tokens\nto create a uniﬁed multi-modal transformer.\nFunding\nN. Sharon is partially supported by the NSF-BSF award 2019752. W. Mattar is partially supported\nby The Nehemia Levtzion Scholarship for Outstanding Doctoral Students from the Periphery (2023). N.\nSharon and W. Mattar are partially supported by the DFG award 514588180.\n\n16\nWAEL MATTAR, IDAN LEVY, NIR SHARON AND SHAI DEKEL\nReferences\n[1] R. W. Buccigrossi and E. P. Simoncelli, Image compression via joint statistical characterization in\nthe wavelet domain, IEEE transactions on Image processing 8 (1999), 1688-1071.\n[2] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S.\nBrahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-Ros, M. Pellat,\nK. Robinson, D. Valter, S. Narang, G. Mishra, A. Yu, V. Zhao, Y. Huang, A. Dai, H. Yu, S. Petrov, E.\nH. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le and J. Wei, Scaling Instruction-Finetuned\nLanguage Models, Journal of Machine Learning Research 25 (2024), 1-153.\n[3] C. Dana and F. V´aclav, DISCRETE CDF 9 / 7 WAVELET TRANSFORM FOR FINITE-LENGTH\nSIGNALS, https://api.semanticscholar.org/CorpusID:208013335, 2011.\n[4] I. Daubechies, Ten Lectures on Wavelets, SIAM, 1992.\n[5] L. Deng, The mnist database of handwritten digit images for machine learning research, IEEE signal\nprocessing magazine 29 (2012), 141-142.\n[6] R. DeVore, Nonlinear approximation, Acta Numerica 7 (1998), 51-150.\n[7] P. Dhariwal and A. Nichol, Diﬀusion models beat gans on image synthesis, Advances in neural infor-\nmation processing systems 34 2021, 8780-8794.\n[8] P. Esser, R. Rombach and B. Ommer, Taming transformers for high-resolution image synthesis,\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition 2021, 12873-\n12883.\n[9] A. Fan, M. Lewis and Y. Dauphin, Hierarchical Neural Story Generation, Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) 2018,\n889-898.\n[10] P. Gage, A new algorithm for data compression The C Users journal archive 12 (1994), 23-38.\n[11] F. Guth, S. Coste, V. De Bortoli and S. Mallat, Wavelet score-based generative modeling, Advances\nin Neural Information Processing Systems 35 (2022), 478-491.\n[12] J. Ho, A. Jain and P. Abbeel, Denoising Diﬀusion Probabilistic Models, Advances in Neural Infor-\nmation Processing Systems 33 (2020), 6840-6851.\n[13] HuggingFace DistilGPT2, https://huggingface.co/distilbert/distilgpt2, 2019.\n[14] HuggingFace Tokenizer https://huggingface.co/docs/tokenizers/index.\n[15] L. Jiang, B. Dai, W. Wu and C. C. Loy, Focal Frequency Loss for Image Reconstruction and Synthesis,\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) 2021, 13919-\n13929.\n[16] M. M. Kivanc, I. Kozintsev, K. Ramchandran and P. Moulin, Low-complexity image denoising based\non statistical modeling of wavelet coeﬃcients, IEEE Signal Processing Letters 6 (1999), 300-303.\n[17] S. Mallat, A Wavelet tour of signal processing, the sparse way, Academic Press, 2009.\n[18] K. M. Mihcak, I. Kozintsev and K. Ramchandran, Spatially adaptive statistical modeling of wavelet\nimage coeﬃcients and its application to denoising, proceedings of 1999 IEEE International Conference\non Acoustics, Speech, and Signal Processing ICASSP99 6 (1999), 3253-3256.\n[19] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever and M. Chen,\nGLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diﬀusion Models,\nProceedings of the 39th International Conference on Machine Learning 162 (2022), 16784-16804.\n[20] H. Phung, Q. Dao and A. Tran, Wavelet Diﬀusion Models are fast and scalable Image Generators,\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023, 10199-10208.\n[21] M. Phuong and M. Hutter, Formal Algorithms for Transformers, https://arxiv.org/abs/2207.09238,\n2022.\n[22] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh,G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark and others, Learning transferable visual models from natural language supervision, Interna-\ntional conference on machine learning (2021), 8748-8763.\n[23] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu andM. Chen, Hierarchical text-conditional image gener-\nation with clip latents, arXiv, 2022.\n[24] A. Ramesh,M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen and I. Sutskever, Zero-shot\ntext-to-image generation, International conference on machine learning 2021, 8821-8831.\n\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION\n17\n[25] R. Rombach, A. Blattmann D. Lorenz, P. Esser and B. Ommer, High-resolution image synthesis with\nlatent diﬀusion models, Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition 2022, 10684-10695.\n[26] A. Said and W. Pearlman, A new, fast, and eﬃcient image codec based on set partitioning in hierar-\nchical trees, IEEE Transactions on Circuits and Systems for Video Technology 6 (1996), 243-250.\n[27] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, K. Ghasemipour, L. Gontijo, Raphael,\nA. Karagol, Burcu, Salimans, Tim and others, Photorealistic text-to-image diﬀusion models with deep\nlanguage understanding, Advances in neural information processing systems 35 (2002), 36479-36494.\n[28] V. Sanh, DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, Proceedings\nof Thirty-third Conference on Neural Information Processing Systems (NIPS2019).\n[29] J. Shapiro, Embedded image coding using zerotrees of wavelet coeﬃcients, IEEE Transactions in signal\nprocessing 41 1993, 3445-3462.\n[30] D. Taubman and M. Marcellin, JPEG2000: Image Compression Fundamentals, Standards and Prac-\ntice, 2nd edition, Springer, 2002.\n[31] Y. Tay, M. Dehghani, D. Bahri and D. Metzler, Eﬃcient transformers: A survey, ACM Computing\nSurveys 55 (2022), 1-28.\n[32] K. Tian, Y. Jiang, Z. Yuan, B. Peng, and L. Wang, Visual Autoregressive Modeling: Scalable Image\nGeneration via Next-Scale Prediction, arXiv 2024.\n[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, L. Kaiser and I. Polosukhin,\nAttention is all you need, Advances in neural information processing systems 30 (2017).\n[34] X. Wang,W. Wang, Y. Cao, C. Shen and T. Huang, Images speak in images: A generalist painter\nfor in-context visual learning, Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition 2023, 6830-6839.\n[35] D. Lee, C. Kim, S. Kim, M. Cho and W. Wook, Autoregressive image generation using residual\nquantization, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n2022, 11523-11532.\n[36] Q. Zhu,X. Li, J. Sun and H. Bai, WDIG: a wavelet domain image generation framework based on\nfrequency domain optimization, EURASIP Journal on Advances in Signal Processing 2023, 66.\n[37] G. K. Wallace, The JPEG still picture compression standard, IEEE Transactions on Consumer Elec-\ntronics 38 (1992), xviii-xxxiv.\n[38] N. Weili, L. Sifei, M. Morteza, L. Chao, E. Benjamin and V. Arash, Compositional Text-to-Image\nGeneration with Dense Blob Representations, arXiv 2024.\n[39] Q. Yu, M. Weber, X. Deng, X. Shen, D. Cremers and L. Chen, An image is worth 32 tokens for\nreconstruction and generation, NeurIPS 2024, to appear.\n[40] Y. Yu, F. Zhan, S. Lu, J. Pan, F. Ma, X. Xie and C. Miao, Chunyan, Waveﬁll: A wavelet-based\ngeneration network for image inpainting, Proceedings of the IEEE/CVF international conference on\ncomputer vision 2021, 14114-14123.",
    "pdf_filename": "Wavelets_Are_All_You_Need_for_Autoregressive_Image_Generation.pdf"
}