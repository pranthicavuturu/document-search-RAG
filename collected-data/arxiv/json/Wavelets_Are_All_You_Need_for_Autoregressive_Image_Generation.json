{
    "title": "WAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE",
    "abstract": "two main ingredients. The first is wavelet image coding, which allows to tokenize the visual details of an image from coarse tofinedetails byordering theinformation starting with themost significant bitsof the most significant wavelet coefficients. Thesecond is avariant of alanguage transformer whosearchitecture is re-designed and optimized for token sequences in this ‘wavelet language’. The transformer learns the significant statistical correlations within a token sequence, which are the manifestations of well-known correlations between the wavelet subbands at various resolutions. We show experimental results with conditioning on the generation process.",
    "body": "WAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE\nGENERATION\nWAEL MATTAR,IDANLEVY,NIR SHARONANDSHAIDEKEL\nAbstract. In this paper, we take a new approach to autoregressive image generation that is based on\ntwo main ingredients. The first is wavelet image coding, which allows to tokenize the visual details of an\nimage from coarse tofinedetails byordering theinformation starting with themost significant bitsof the\nmost significant wavelet coefficients. Thesecond is avariant of alanguage transformer whosearchitecture\nis re-designed and optimized for token sequences in this ‘wavelet language’. The transformer learns the\nsignificant statistical correlations within a token sequence, which are the manifestations of well-known\ncorrelations between the wavelet subbands at various resolutions. We show experimental results with\nconditioning on the generation process.\n1. Introduction\nThe generation of high-resolution visual information is certainly one of the most remarkable achieve-\nments of modern-age artificial intelligence. One of the prominent methods is diffusion-based models\n[7, 12, 23, 25, 27]. In essence, diffusion models attempt to learn inversions of ill-posed operators, such as\nadditive Gaussian noise, blurring, etc., so an image may be generated from random noisy or blurry seeds.\nAnother line of research is designing autoregressive models, that apply the architecture of powerful\nLarge Language Models (LLMs) [21, 31, 33]. These autoregressive methods [8, 24] convert the image\npixel representation to a series of visual tokens and then apply generative language techniques.\nIn this paper, we refine this line of research and provide a mathematically robust approach to the\nautoregressive image generation process. To this end, we reach out to a classic technique in image\nprocessing, specifically, wavelet image coding [26, 29, 30]. Wavelets [4, 6, 17] are one of the main tools\nof modern approximation theory for nonlinear, adaptive approximation. The various wavelet transforms\nprovide the means to transform an image into a representation that captures the essence of the visual\ninformation in a sparse way. Typically, the significant wavelet coefficients are a small fraction of the\ncoefficients and represent important edge and texture information, while the insignificant coefficients\nwith small absolute values are associated with smooth regions of the image. The goal of wavelet image\ncompressionmethods,suchasJPEG2000, isthentoefficiently storetheinformationofonlythesignificant\ncoefficients. In fact, the underlying method of the popular JPEG image compression algorithm [37],\ninvented in the 80s, contains many elements of wavelet coding, where a local Discrete Cosine Transform,\na precursor of wavelets, is used. However, in this paper, we leverage the progressive wavelet compression\ntechnique,amoreadvancedformofimagecompression. Itcreatesabit-streamwhereeverybitcorresponds\nto the next most important piece of visual information. Since we are generating images rather than\ndecoding them from a compressed file, there is no need to create actual binary bit-streams, and using a\n‘wavelet language’ of a limited number of tokens is sufficient.\nThus, our new approach to autoregressive image generation is based on two main ingredients. The first\nis progressive wavelet image coding, which allows to tokenize the visual information of an image from\ncoarsetofinedetails. Thiscanachieved usingasfewas6tokens, byorderingtheinformationstartingwith\nthe most significant bits of the most significant wavelet coefficients. The second ingredient is a variant of\nan NLP decoder-only transformer [21, 31, 33] whose architecture was re-designed and optimized for token\nsequences in this ‘wavelet language’ The transformer learns the significant statistical correlations within\na token sequence, which are the manifestations of well-known correlations between the wavelet subbands\nat various resolutions [1, 16, 18]. During inference, this allows the generation of visually meaningful\nimages from an initial random seed generated from sampled from the distribution of the scaling function\ncoefficients at the lowest resolution.\n1\n4202\nvoN\n91\n]GL.sc[\n2v79991.6042:viXra\n2 WAELMATTAR,IDANLEVY,NIRSHARONANDSHAIDEKEL\nUsing the wavelet autoregressive approach, where the ‘wavelet language’ contains only a few tokens,\nprovides many attractive features. The length of the token sequences during training or inference can be\nflexible, where longer sequences imply more detailed or higher-resolution images. Guiding the generative\nprocess using a class affiliation or text prompting is easily achieved by concatenating the corresponding\nvector representations to the tokens’ vector representation of low dimension. Stochastic control using\nsimple transformer inference techniques, allows to create from one textual prompt a diversity of corre-\nsponding images. Furthermore, since each token is associated with the local supportin the image domain\nof the correspondingwavelet, one can switch the guidance during the generative process to allow different\nprompting for different regions.\nOur paper is organized as follows. We begin with a review of related work in Section 2. In Section 3,\nwe review wavelet image coding and explain how one may extract from the classical theory the ability to\ntokenize the visual information of images. In Section 4, we focus on components of language transforms\nthat we redesign to serve our special wavelet language. We further provide several methods that can be\nused to direct the generation process under certain conditions: class label and/or textual prompt. In\nSection 5, we provide experimental results. Finally, in Section 6, we discuss possible future applications\nof our method, such as multi-modality generation and compositions of blobs.\n2. Related work\nIn this section, we first review the current state of the art in image generation. We then review some\nmethods that apply wavelets as a frequency decomposition backbone for various aspects of style transfer,\nacceleration, and optimization of existing image generation methods, etc.\nCurrently, many commercial solutions apply diffusion-based models [7, 12, 19, 23, 25, 27]. In essence,\ndiffusion models learn inversions of ill-posed operators, such as additive Gaussian noise, blurring, etc.,\nso images may be generated from random noisy or blurry seeds. One then enforces various conditions\non images created through the time steps of the inversion process so that the final generated image may\ncorrespond to a given text prompt.\nRecently, there is renewed interest in autoregressive methods with the hope that they will outperform\nthe diffusion models. The methods of VQGAN [8] and DALL-E [24] along with [34, 35] utilize a visual\ntokenizer to discretize images into grids of 2D tokens, which are then flattened to a 1D sequence for\nautoregressive learning, mirroring the process of sequential language modeling. For example, in [24] a\ndiscrete variational autoencoder is trained to compress each 256×256 RGB image into a 32×32 grid\nof image tokens, where each such token can assume 8192 possible values. This creates a relatively short\ncontext sequence of 1024 tokens, butwith a vocabulary of 8192 word tokens. TheTiTok method, recently\nintroduced in [39], shows how to combine Vision Transformers with the Vector-Quantization method to\narrive at an autoregressive method that may use only 32 tokens. In comparison, our method may use\nonly 6-7 tokens for any image resolution and any level of fine detail generation.\nIn contrast to typical raster-scan methods, where single tokens are sequentially predicted, the method\nof [32] provides an autoregressive learning algorithm based on predicting the image’s next-scale, or next-\nresolution.\nSome methods, such as [36, 40], use wavelets as means for frequency decomposition representations\nfor image inpainting, style transfer, and generative adversarial network methods. Some works propose\nto use wavelets as part of diffusion methods [11, 20] to speed up the diffusion approach by applying the\ndenoising process in the wavelet regime.\nTothebestofourknowledge, thisisthefirsttimewavelets arebeingusedasthebasisforautoregressive\nimage generation.\n3. Elements of Wavelet Image Coding\nIn this section, we review some elements of wavelet image coding [26, 29, 30] that we use for our\ngenerative method. Essentially, we are interested in the process that takes an image in its raw pixel form\nas input and generates a sequence of tokens that capture its visual details. The structure of the sequence\nfrom coarse to fine details is achieved by ordering the information starting with the most significant bits\nof the most significant wavelet coefficients. En par with wavelet coding, we also have a goal to create\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION 3\ntoken sequences that are as short as possible. This creates shorter contexts for the transformer decoder\nand improves its performance. As we shall see (Subsection 3.2.3) it is quite easy to convert sequences of\nfew wavelet tokens to shorter sequences at the tradeoff of using a larger vocabulary of tokens.\n3.1. Wavelet Transforms. A univariate wavelet system [4, 17] is a family of real functions in L (R) of\n2\nthe form ψ : (j,k) ∈ Z2 built by dilating and translating a unique mother wavelet function ψ\nj,k\n(cid:8) (cid:9)\nψ (x) :=\n2−j/2ψ(2−jx−k),\nj,k\nwhere the mother wavelet typically has compact support (or fast decay) and has r vanishing moments\n(3.1) xkψ(x)dx = 0, k = 0,1...,r−1.\nZR\nWavelet systems can be constructed to serve a basis of L (R). To facilitate applications, one then also\n2\nconstructs a dual ψ˜ of ψ, where hψ j,k,ψ˜ j′,k′i = δ j,j′δ k,k′, so that for each f ∈ L 2(R),\nf = hf,ψ˜ iψ .\nj,k j,k\nXj,k\nFor special choices of ψ , the set {ψ } forms an orthonormal basis for L (R) and then, ψ = ψ˜.\nj,k 2\nUsually, one starts the construction of a wavelet system from a Multi-Resolution Analysis (MRA)\ngenerated by a scaling function ϕ ∈ L (R) that satisfies a two-scale equation\n2\nϕ= a ϕ(2·−k).\nk\nXk\nOne then sets\nV = span ϕ := 2−j/2ϕ 2−j ·−k :k ∈ Z , j ∈ Z,\nj j,k\nn o\n(cid:0) (cid:1)\nwhich implies (under certain mild conditions)\n....V\n2\n⊂ V\n1\n⊂ V\n0\n⊂ V−1 ⊂ V−2..., ∩V\nj\n= {0}, ∪ jV\nj\n= L 2(R).\nAgain, to facilitate applications, one may also construct a dual ϕ˜ of ϕ, where hϕ 0,k,ϕ˜ 0,k′i = δ k,k′, so that\nfor each f ∈ V ,\nj\nf = hf,ϕ˜ iϕ .\nj,k j,k\nXk\nEquippedwiththeMRA,onethenproceedstoconstructthewaveletψsuchthatW := span{ψ :k ∈Z}\nj j,k\nwith V +W = V . A classic example for an orthonormal MRA and wavelet system where ϕ = ϕ˜\nj+1 j+1 j\nand ψ = ψ˜, are the Haar scaling function and Haar wavelet\n1 x ∈ 0, 1 ,\n1, x ∈ [0,1], 2\nϕ(x) := ψ(x) :=  −1 x ∈(cid:2)1,1(cid:1),\n(cid:26) 0, else.  0 else.(cid:2)2 (cid:3)\nThe bivariate Haar system (see below) is a good choice when working with piecewise constant images,\nsuch as the MNIST handwritten digits [5]. For some of our experiments, we use a famous wavelet system\nfrom the Cohen Daubechies Feauveau (CDF) family of wavelets [4], which is sometimes termed bior4.4\n(r = 4 in (3.1)) or [9,7] in the signal processing community (the supports of the scaling functions and\nwavelets, as well as the lengths of the associated filters, are 9 and 7). The generating functions of the\nbior4.4 are depicted in Figure 1.\nThe wavelet model can be easily generalized to any dimension, via a tensor product of the wavelet and\nthe scaling functions. Assume that the univariate dual scaling functions ϕ,ϕ˜ and dual wavelets ψ,ψ˜, are\ngiven. Then, a wavelet bivariate basis is constructed using three types of basic wavelets\nψ1(x ,x ) := ϕ(x )ψ(x ), ψ2(x ,x ) := ψ(x )ϕ(x ), ψ3(x ,x ):= ψ(x )ψ(x ),\n1 2 1 2 1 2 1 2 1 2 1 2\nψ˜1(x ,x ) := ϕ˜(x )ψ˜(x ), ψ˜2(x ,x ) := ψ˜(x )ϕ˜(x ), ψ˜3(x ,x ):= ψ˜(x )ψ˜(x ).\n1 2 1 2 1 2 1 2 1 2 1 2\n4 WAELMATTAR,IDANLEVY,NIRSHARONANDSHAIDEKEL\nFigure 1. The CDF [9,7] wavelet system (figure reproduced from [3]).\nThe bivariate wavelet transform of f ∈ L (R2), in terms of the bivariate wavelet tensor basis\n2\nψe := 2−jψe(2−j ·−k), ψ˜e := 2−jψ˜e(2−j ·−k), e= 1,2,3,j ∈ Z,k ∈ Z2,\nj,k j,k\nis then\nf = hf,ψ˜e iψe .\nj,k j,k\ne=1,2,3X ,j∈Z,k∈Z2\nThe bivariate wavelet decomposition can thus be interpreted as a signal decomposition in a set of three\nspatially oriented frequency subbands: LH(e = 1) detects horizontal edges; HL (e = 2) detects vertical\nedges and HH (e = 3) detects diagonal edges.\nUnder the assumption that ψ and ψ˜are compactly supported(or have fast decay), a wavelet coefficient\nhf,ψ˜e i at a scale j represents the information about the function in the spatial region of radius ∼ 2j\nj,k\nin the neighborhood of 2jk, k ∈ Z2. At the next finer scale j −1, the information about this region is\nrepresented by four wavelet coefficients, which are described as the children of hf,ψ˜e i. This leads to a\nj,k\nnatural tree structure organized in a quad tree structure of each of the three subband types as shown in\nFigure 2. As j decreases, the child coefficients add finer and finer details into the spatial regions occupied\nby their ancestors.\nIn image processing, one uses the Discrete Wavelet Transform (DWT). It works by initially assuming\nthat the image pixels {f = f }M are good approximants of the projections on the shifts of the\nk k1,k2 k1,k2=1\ndual scaling function with the underlying function f (see [17, Section 7.3.1] for a detailed justification)\nf ≈ hf,ϕ˜ i.\nk 0,k\nWith these coefficients as input, one uses the DWT to compute coefficients down to some predefined\nlow-resolution m. For simplicity, we may assume that M = 2m and that we use the DWT to compute\n(3.2) {hf,ϕ˜ i}, {hf,ψ˜e i}, 1 ≤j ≤ m, e = 1,2,3.\nm,k j,k\nWavelet representations are considered very efficient for image compression [26, 29, 30]. The edge\ninformationtypicallyconstitutesasmallportionofatypicalimage,whilethedualwaveletcoefficientshave\nalargeabsolutevalueonlyifedgesintersectthesupportofthecorrespondingdualwavelets. Consequently,\ntheimagecanbeapproximatedwellusingafewsignificantwaveletcoefficients. Aclearstatisticalstructure\nalsofollows: large/smallvaluesofwaveletcoefficientstendtopropagatethroughthescalesofthequadtrees\ndepicted in Figure 2. As an example, a sparse wavelet representation of a 512×512 fishing boat image\nand a compressed version of it are shown in Figure 3, where the compression algorithm JPEG2000 is\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION 5\nFigure 2. Wavelet coefficient tree structure across the subbands (MRA decomposition).\nbased on the sparse representation. The Figure clearly depicts that the significant wavelet coefficients\n(coefficients with relatively large absolute values) are located on strong edges of the image.\n(a) Fishing boat image. (b) 15267 significant coefficients. (c) Compressed image 1:17.\nFigure 3. Image compression based on sparse wavelet approximation.\n3.2. Embedded Wavelet Tokenization. The sparse wavelet representation (3.2) of an image provides\nthe perfect infrastructure for the generation of embedded coding representations [26, 29, 30]. Embedded\ncoding is similar in spirit to binary finite precision representations of real numbers, where the “encoding”\ncan cease at any time and provide the “best” approximation of the real number achievable within the\nframework of the binary digit representation. Similarly, the embedded coder can cease at any time and\nprovidethe“best”representation of an image achievable withinits framework. Embeddedcodingstreams\ncan be generated from wavelet representations by ordering the information on the wavelet representation\nstarting with the most significant bits of the most significant coefficients. That is, the coefficients with\nthe largest absolute value. In image coding applications the goal is to generate a compressed bit stream\nof ‘0’ and ‘1’s. This can be efficiently achieved by using information theoretical tools such as arithmetic\ncoding. Here, our goal is somewhat different where we aim to create an efficient tokenization method\nconforming to the following two objectives:\n(i) Ensuring statistically frequent structural patterns - The existence of common patterns within\nthe token sequence allows the language models to learn them as contexts when they attempt\nto generate the most probable next token in the context of the previous tokens. The wavelet\ntokenization processes described below provide that by creating token sequences that are ordered\n6 WAELMATTAR,IDANLEVY,NIRSHARONANDSHAIDEKEL\nbased on coefficient absolute value and then resolution. It is known [26, 29], that there are\nstrong correlations between insignificant coefficients with their ‘ancestors’ at lower resolutions\n(see Subsection 3.2.2).\n(ii) Trade-off between sequence length versus number of tokens - When token sequences become very\nlong, the LLMs need to deal with longer contexts, which can be challenging. At the same time, a\ndataset of possibly shorter sequences that are based on a large vocabulary of tokens can also be\nchallenging for different reasons. As we shall see, the method of Subsection 3.2.1 uses 7 tokens\nand may create long sequences. The method of 3.2.2 is a somewhat more advanced and manages\nto both reduce the number of tokens by 1 and at the same time reduce the sequence lengths\nsignificantly as the dimensions of the images increase. In Subsection 3.2.3 we review standard\nmethods that allow to control this tradeoff by merging frequent sub-sequences into new tokens.\nFirst, for simplicity of notation, using (3.2), denote for I = (i ,i ), 1 ≤ i ,i ≤ 2\n1 2 1 2\nα := hf,ϕ˜ i.\nI m,I\nWe also map the coefficients {hf,ψ˜e i}, 1≤ j ≤ m, e= 1,2,3,\nj,k\nα ← hf,ψ˜e i,\nI j,k\nbased on their location\nI = (i ,i ), i ≥ 3∨i ≥ 3, i ≤ M ∧i ≤ M,\n1 2 1 2 1 2\nin the coefficient matrix. We note in passingthat onemay assumethat the low resolution scaling function\ncoefficients from(3.2)areknownduringtrainingandarerandomlysampledfromsomedistributionduring\nimage generation and therefore need not be part of the tokenization.\n3.2.1. Encoding a wavelet representation into a token sequence. We now show how to process the numeric\nrepresentations of the coefficients, from most significant to least significant and ‘encode’ them into a\nrelativelycompactseriesoftokens. Therepresentationusingtheseriesoftokensshouldbeinvertible. That\nis, one should be able to convert (e.g. ‘decode’) the token sequence back to the wavelet representation.\nTo this end, assuming the image pixels are normalized to the range [0,1], one can show that for an\nimage of dyadic dimension [M,M] = [2m,2m], after m−1 iterations of the bivariate DWT\n(3.3) max|α |≤\n2m−1.\nI\nI\nAssuming for simplicity that all images of a given dataset have the same dyadic dimensions [M,M],\nthen this bound holds for all of their wavelet representations. Our first option is to initialize a threshold\nT =\n2m−2\nand begin scanning the wavelet coefficients of the image, in a predetermined order (see below)\nfor significance, with the goal of reporting only those coefficients for which the following holds\nT ≤ |α | < 2T.\nI\nOur second option, is to compute separately for each image in the dataset\n(3.4) m˜ := ⌈log max|α |⌉,\n2 I\nI\nand then initialize for the specific image T =\n2m˜−1.\nIn this scenario, we store and use the parameter m˜\nfor each image in the training set along with its sequence of tokens.\nWe also maintain a matrix of approximated wavelet coefficients {α˜ } which we initialize with zeros.\nI\nOnce we complete the processing at a given bit plane, we update T ← T/2 and repeat the process. At\neach bit-plane we report the significant coefficients that were just uncovered in this bit-plane using a\ntoken ‘NowSignificantNeg’ if the coefficient is negative or a token ‘NowSignificantPos’ if it is positive.\nAt the time of uncovering, we modify the approximation of the coefficient α˜ to have the absolute value\nI\n3T/2, with the reported sign. Next, we add a token to represent the coefficient’s next significant bit,\n‘NextAccuracy0’ if the coefficient satisfies |α | ≤ |α˜ | or the token ‘NextAccuracy1’ if |α | > |α˜ |. The\nI I I I\napproximation α˜ is updated accordingly by subtracting or adding T/4 (depending on the sign of the\nI\ncoefficient and the accuracy bit type).\nLet us demonstrate with an example. Assume T=16 and α = −17.45. Therefore, the coefficient is\nI\nfirst uncovered in the current bit plane. When we arrive at the index I, we report a ‘NowSignificantNeg’\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION 7\ntoken for this coefficient, providing it with a temporary approximation α˜ = −24, which lies in the middle\nI\nof the segment [−T,−2T] = [−16,−32]. Next, since in fact |α | ≤ 24, we report a token ‘NextAccuracy0’\nI\nto represent the coefficient’s next significant bit, providing an updated approximation α˜ = −20, which\nI\nlies in the middle of the segment [−T,−3T/2] = [−16,−24], leading to a better approximation of the\nground truth value.\nIn case a coefficient has been uncovered in any of the previous bit-planes and is already known to be\nsignificant, we only add oneof the tokens ‘NextAccuracy0’ if |α | ≤ |α˜ | or ‘NextAccuracy1’ if |α | > |α˜ |.\nI I I I\nWe then update the approximation α˜ by subtracting or adding T/4 (depending on the sign of the\nI\ncoefficient and the accuracy bit type).\nAssuming the bit-plane scanning order of the coefficients is fixed, one then only needs to add the token\n‘Insignificant’ to provide a valid invertible tokenization process. One simply scans the coefficients in the\nfixed order and uses their true known value to test and apply one of three possibilities:\n(i) |α |≥ 2T: The coefficient has already been reported as significant in a previous bit-plane. There-\nI\nfore one reports the token ‘NextAccuracy0’ or ‘NextAccuracy1’ depending on the test |α | < |α˜ |.\nI I\n(ii) T ≤ |α | < 2T: First report the token ‘NowSignificantNeg’ or ‘NowSignificantPos’ depending on\nI\nthe sign and then report the token ‘NextAccuracy0’ or ‘NextAccuracy1’.\n(iii) |α |< T: report ‘Insignificant’.\nI\nThe process described above, although completely sufficient for invertible tokenization, potentially\ncreates long sequences. Specifically, it does not take into consideration the local correlations among\n‘neighboring’ insignificant wavelet coefficients. Due to the sparsity property of the wavelet transform,\nduringthescanningprocess, manyof the‘Insignificant’coefficients formlocalgroups. Moreover, thereare\ncorrelations between local groups of insignificant coefficients of the same subband type across resolutions\nin the manner of the quad-tree structure of Figure 2. Image compression algorithms such as the EZW\n[29] or SPIHT [26], are based on statistical zero tree models that try to capture these correlations across\nresolutions (see the Zero-Tree method in the next subsection). As we shall later see, for image generation,\nwe actually rely on the powerfulcapabilities of the transformer models to learn correlation patterns of the\n‘wavelet language’ofthegiven dataset. However, wedo‘easetheburden’offthetransformerssignificantly\nbyutilizingthestructureofthegroupsofinsignificantcoefficientstoreducethesizeofthetokensequences,\nthereby creating shorter contexts.\nTo this end, we add two additional tokens for groups of insignificant coefficients: ‘Group4x4’ and\n‘Group2x2’ and modify the scanning process to visit the coefficients based on groups of 4×4. The first\ntoken is used in locations where the scan is at an index (4l ,4l ), for some integers l ,l . If at the current\n1 2 1 2\nbit plane, all the 16 coefficients with indices I = (i ,i ), 4l ≤ i ≤ 4(l +1),4l ≤ i ≤4(l +1), are still\n1 2 1 1 1 2 2 2\ninsignificant, we issue the token ‘Group4x4’ and the tokenization process continues to the next group of\n4×4 coefficients. However, if any of the coefficients of the 4×4 group becomes significant in the current\nbit-plane, the group breaks down to 4 groups of 2×2. If a group of 2×2 is still composed of insignificant\ncoefficients atthecurrentbit-plane, weaddatoken ‘Group2x2’. Ifagroupof2×2 breaksdown,theneach\ncoefficient from the group is reported individually as being ‘Insignificant’ or one of ‘NowSignificantNeg’,\n‘NowSignificantPos’. The scanning process keeps track of which groups broke up, so that only necessary\nand informative tokens are generated. We summarize the seven tokens and their roles below\n(i) ‘Group4x4’ – At the index (4l ,4l ), the group of 16 coefficients {α }, 4l ≤ i ≤ 4l +4, 4l ≤\n1 2 I 1 1 1 2\ni ≤ 4l +4, are still insignificant, |α | <T.\n2 2 I\n(ii) ‘Group2x2’ – At the index (2l ,2l ), the group of 4 coefficients {α }, 2l ≤ i ≤ 2l +2, 2l ≤\n1 2 I 1 1 1 2\ni ≤ 2l +2, are still insignificant, |α | <T.\n1 2 I\n(iii) ’NowSignificantNeg’, ’NowSignificantPos’ – At the current location I, the coefficient satisfies T ≤\n|α | < 2T. If the coefficient was part of a group of insignificant coefficients at the previous\nI\nbit-plane, the group is now automatically dissolved.\n(iv) ‘Insignificant’ – At thecurrentlocation I, the coefficient is still insignificant andsatisfies |α | < T.\nI\nIfthecoefficientwaspartofagroupofinsignificantcoefficientsatthepreviousbit-plane,thegroup\nis now automatically dissolved.\n8 WAELMATTAR,IDANLEVY,NIRSHARONANDSHAIDEKEL\nv) ‘NextAccuracy0’, ‘NextAccuracy1’ – At the current location I, the coefficient has already been\nreported to be significant since it satisfies |α |≥ T. Here, we improve the accuracy of its approx-\nI\nimation using one of these tokens, depending on the test |α | < |α˜ |.\nI I\nThe bit-plane scan is carried out in two nested loops; the outer loop proceeds from low resolution to\nhigh resolution, each time traversing the three types of wavelet subbands. The inner loop traverses the\n4×4 blocks. Figure 4 illustrates the outer and inner scanning patterns.\n(a) Outersubband scanning order. (b) Inner scanning order of 4 × 4\nblocks.\nFigure 4. A sketch illustrating the outer and inner scanning orders.\nFigure 5 exemplifies the tokenization algorithm of an image from the MNIST dataset [5]. The image\nwas padded with zeros to be of dimensions M × M = 32 × 32, with m = 5. The bottom row of the\nfigure shows the tokens and their locations on the wavelet image for the first three bit planes. To make\nthe process clearer, we explicitly write the resulted sequence of tokens for the first bit plane shown in\nFigure 5(d).\n{‘Insignificant’, ‘Insignificant’, ‘NowSignificantPos’, ‘Insignificant’,\n‘Insignificant’, ‘Insignificant’, ‘NowSignificantNeg’, ‘Insignificant’,\n‘Group2x2’, ‘Group2x2’,\n‘Insignificant’, ‘Insignificant’, ‘Insignificant’, ‘NowSignificantNeg’,\n‘Group2x2’,‘Group2x2’, ‘Group2x2’,\n‘Group4x4’,...,‘Group4x4’}\nThe token sequences of the second and third bit-planes follow the same scanning pattern. Eventually,\nthe three sequences are concatenated in the natural order to form the final sequence which describes the\nthree bit planes wavelet image appearing in Figure 5(c).\nThere is a very important hyper-parameter which is the choice of the smallest threshold at the final\nbit-plane. Through this hyper-parameter, the wavelet representation provides us with a very robust and\nstabletrade-offoffinedetailgeneration andlengthoftokensequences. Choosingafinalthresholdprovides\nvery consistent control over visual quality relating to: “Visually Lossless”, “High”, “Medium”, “Low”,\netc. This is en par with the quality settings in digital cameras, which in turn lead to a selection of the\ncorresponding quantization tables of the JPEG algorithm generating the compressed images.\n3.2.2. Zero-tree tokenization. TheZero-Tree methodis an alternative tokenization methodthat is aligned\nwith [29] and provides shorter sequences, especially as image size increases. The zero-tree approach\nleverages on the correlations of insignificant coefficients across resolutions. Statistically, if a wavelet\ncoefficient at some resolution is insignificant, then with very high probability (around 90% for real life\nimages) its descendants at the same subband and higher resolutions will also be insignificant. With the\nzero-tree tokenization method the scanning visits coefficients from low to high resolution and the tokens\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION 9\n(a) 32×32 paddedMNIST image. (b) Wavelet transform. (c) Significant coefficients after 3 bit-\nplanes.\n(d) Wavelet domain tokenization - (e) Wavelet domain tokenization - (f) Wavelet domain tokenization -\nfirst bit plane. second bit plane. third bit plane.\nFigure 5. Depiction of the tokenization process. On the top left and middle, a 32×32\npadded MNIST image and its wavelet transform. On the top right, the wavelet approxi-\nmation generated by the first three bit-planes. The bottom row illustrates the tokens and\ntheir locations on the 32×32 grid, where, ‘NowSignificantNeg’ and ‘NowSignificantPos’\ntokens are annotated with orange “−” and blue “+” and signs respectively. The tokens\n‘NextAcurracy0’ and ‘NextAccuracy1’ are marked with green down and red up triangles.\nThe purple dots represent ‘Insignificant’ coefficients and the brown and pink squares rep-\nresent the ‘Group2x2’ and ‘Group4x4’ zero block tokens.\n‘Group2x2’ and ‘Group4x4’ are replaced with a single ‘zero-tree’ token. If the token is reported at a\ncertain location in the scan, then it is understood that the coefficient at this location as well as all its\ndescendants are still insignificant at the current bit-plane. The descendants of a coefficient at location\nI = (i ,i ), i ≥ 3∨i ≥ 3, i ≤ M/2∧i ≤ M/2,\n1 2 1 2 1 2\nare its children\n{(2i ,2i ),(2i ,2i +1),(2i +1,2i ),(2i +1,2i +1)}\n1 2 1 2 1 2 1 2\nandthenrecursivelytheirchildren. Onceacoefficientisreportedasa‘zerotree’coefficient,itisunderstood\nthat all of its descendants are still insignificant and the scanning skips them. For the FashionMNIST\ndataset (see examples below) the mean token sequence length is 1822.5 for the zero blocks tokenization\nmethod and 1601.7 for the zero tree method, although the latter uses 6 tokens instead of 7.\n10 WAELMATTAR,IDANLEVY,NIRSHARONANDSHAIDEKEL\n3.2.3. The trade-off of vocabulary size and sequence lengths. It is quite standard in the field of autore-\ngressive methods to control the tradeoff between the token vocabulary size and the dataset’s mean token\nsequence length in an attempt to find the optimal configuration for given computational resources and\nmodel architecture. Since the wavelet method uses a relatively very small number of tokens (language\nmodelstypicallysupportavocabularyoftensofthousandsoftokens)andcreatesrelativelylongsequences,\nit is relevant in scenarios where the computational resources do not allow the use of long token sequences.\nOne of the simplest methods is Byte Pair Encoding (BPE) [10]. BPE is a subword tokenization technique\ncommonly used in natural language processing, which iteratively merges themost frequent pairs of tokens\ninto new tokens, thereby creating a more compact representation of the data. We used HuggingFace’s\ntokenizers library [14] to generate Figure 6\nFigure 6. The trade-off between vocabulary size and token sequence length for the fash-\nionMNIST dataset.\n3.2.4. Decoding the token sequence into an approximate wavelet representation. The tokenization process\ndescribedintheprevioussubsectionscanbeeasilyinvertedbacktoanapproximatewaveletrepresentation.\nMoreover, any initial sub-sequence can be inverted to provide a possibly coarser approximation. We\ninitialize a matrix of size M × M of the approximated wavelet coefficients {α˜ } with zeros and begin\nI\nthe scanning process with the first bit-plane. Based on (3.3) or (3.4), we know how to initialize the first\nbit-plane with the initial threshold T =\n2m−2\nor T =\n2m˜−1.\nWe then process the token sequence and\nupdatetheapproximated coefficients usingthecorresponding‘significant’ and‘bitaccuracy’ tokens. If for\nany given reason, the sequence of tokens terminates, we have the best possible approximated coefficients\n{α˜ } from which we can obtain an approximated image by applying the inverse DWT. Our decoding\nI\nprocess relies on the assumption that the token sequence is valid. For example, a ‘Group4x4’ token\ncannot appear while the decoder scan position is at a location of indices not divisible by 4. It is obvious\nhow to achieve this in the context of image coding. However, during an image generation process, this\nneeds to be enforced using the conditional next-token inference described in Subsection 4.6.\n4. The Generative Wavelet Transformer\nAssumethatforacertaindatasetofimages,wehaveestablishedthetranslationofthevisualinformation\nof each image to a sequence of tokens encapsulating the visual information from coarse to fine details as\nexplained in Subsection 3.2. We assume that within the sequences, distinct patterns and relations exist\nbetween the tokens. For example, the wavelet coefficients {hf,ψ˜e i} of wavelets {ψe } whose support\nj,k j,k\nintersects with a certain portion of an edge of the image, will be significant and aligned across scales in\na tree-like structure as per Figures 2 and 3(b). At the same time, coefficients of wavelets whose support\nintersects with a smooth area of the image will be insignificant and they appear in local groups. As\nexplained, they also have a tree structure across scales, that can be captured explicitly by a ‘zero tree’\ntoken. This leads to the intuition that the powerful transformers created over the last few years [21] are\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION 11\nable to learn the patterns of the ‘wavelet language’ and to generate them from some random seeds during\ninference.\nIn this section, we describe how we modified the architecture of the DistilGPT2 transformer model [28]\nto optimize it to align with the wavelet-based image generation method. This obviously requires training\nthe modified model from scratch. We found it useful to use the code from HuggingFace [13] as a starting\npoint.\n4.1. Token vector representation. Typically, in the standard scenarios of spoken languages, trans-\nformers apply a ‘pre-processing’ learnable transform to tokens to convert them to vector representations.\nThe idea is that similar words should be converted to vectors with some proximity, which intuitively\nserve as better input for the transformer’s neural network. However, with the method of Subsection\n3.2.1, our wavelet dictionary includes only 7 tokens that have very distinctive and different roles. There-\nfore, the simple transformation of the tokens to the one-shot encoding of the standard basis of dimen-\nsion 7 is probably a better, if not optimal choice. Thus, the initial vector representation of a token is:\n‘Group4x4’ → (1,0,0,0,0,0,0), ‘Group2x2’ → (0,1,0,0,0,0,0), etc. Therefore, in our ‘wavelet’ trans-\nformers the ‘token → vector’ learnable transformation is removed.\n4.2. Initial bit-plane threshold. Recall that we have two options: to use a uniform initial bit-plane\nthreshold for all images in the dataset derived from (3.3), or to use an adaptive initial threshold for each\nimage of the training set using (3.4). In the latter case, we need to inform the transformer, per image,\nwhich initial threshold the token sequence is associated with. We do this as follows: assume a given\ndataset has l possible values for m˜ in (3.4) (e.g., l = 4 for the MNIST dataset, see Figure 7). Then, we\nconcatenate a one-shot encoding of dimension l of the initial threshold parameter of the given image to\neach vector representation of each token.\nFor image generation, one may sample randomly from the distribution of l possible initial thresholds.\nInthecasethattheimagegeneration isconditionedonacertainclass (seeSubsection4.4),onecansample\nfrom the conditional distribution of the possible thresholds of the specific class.\nFigure 7. Distribution of log of the initial thresholds for the 70,000 MNIST images with\n2\nthe Haar wavelet transform.\n4.3. Positional encoding. In classic transformer architectures [21], one adds the positional encoding\nv (t) of the position t to the token’s vector representation v (x(t)). Learnt positional embedding applied\np e\na learned transform t → v (t). Some transformers use hard-coded mapping of the position. Assuming the\np\nvector embedding dimension is d and the maximum length of a sequence is l , then\ne max\nv (t)(2i−1) = sin(t/l2i/de), v (t)(2i) = cos(t/l2i/de), 1 ≤ i≤ d /2.\np max p max e\nIn our scenario of the wavelet language, the position of a token in a sequence is (bp,I), where bp is the\nenumeration of the bit-plane and I = (i ,i ) is the index of the current coefficient α in the scan order.\n1 2 I\n12 WAELMATTAR,IDANLEVY,NIRSHARONANDSHAIDEKEL\nTherefore,weconcatenate tothevector representationofatoken fromSubsection4.1,avector component\nof dimension 3 with the location of the token (bp,i ,i ).\n1 2\n4.4. Generative guidance. It is obviously critical for any image generation method to allow guidance\nof the generative process by placing a condition on the class type of the generated image or a text prompt\nthat describes it. Some image generation models apply a joint embedding space for text and images for\nthis purpose. One such method is to used a pretrained model such as CLIP [22] that maps text and\nimages to a joint embedding space. The CLIP contains an image encoder f and a caption encoder g,\nthat during training over pairs of images with captions {(x,c)}, optimizes a contrastive cross-entropy loss\nthat encourages high dot-products hf(x),g(c)i in the joint embedding space. Thus, any image generation\nmethod, can use the vector embedding of the given text prompt c to guide the generative process by\nconditioningtheimageembeddingf(x)tobehighlycorrelated withtheembeddingofthetextualprompt.\nIn our case, since we converted the problem of image generation to a ‘wavelet-language’ generation,\nwe can apply ‘text’-type prompting methods. Having access to a joint embedding text-image space\nallows us to train using the vector representation of the image training set. Then, at image generation,\nwe use the vector representation of the given text prompt to guide the generative process. There are\nvery simple ways of using these vector representations. We choose to concatenate them to the vector\nrepresentation of each token and its position (as explained above). For example, as shown in Section 5,\nfor the image datasets MNIST or FashionMNIST with 10 classes, it is easy to concatenate a vector of\nlength 10 representing the class of the image. In the case where we wish to guide the generative process\nusing a textual prompt, we may concatenate the CLIP vector embedding [22] of the textual prompt to\neach token vector representation. As we discuss in Subsection 6.2, we hope this approach to guiding the\ngenerative process can be generalized to composition of blobs [38], where a given guiding vector of a blob\nis used only at positions of the scan where the support of the corresponding wavelet intersects the blob.\n4.5. Initializationof thegenerativeprocess. Sincetheguidanceofthegenerativeprocess(Subsection\n4.4) is applied through the concatenation of vector representations to each token vector representation, in\nsomecases,theinitializationbecomesaminorissue. Forexample,whentrainingonMNISTandgenerating\ndigits, onecan get away with asimplerandomchoice from thesubset: ‘Insignificant’, ‘NowSignificantNeg’\nor‘NowSignifiantPos’forthefirsttokenandfromtherethetransformerwillgenerateavalidtokensequence\nwhich is converted to an adequate image of a digit from the pre-selected class.\nA more robust method is as follows. Suppose we wish to generate a handwritten digit from a certain\ndigit class. Let {f s} s∈S be the subset of MNIST images from that specific digit class and let\n(4.1) {hf ,ϕ˜ i}, s ∈ S, k = (k ,k ), 1 ≤ k ,k ≤ 2,\ns m,k 1 2 1 2\nbe the subset of low-resolution coefficients of these images defined by (3.2). Let N(v,Σ) be the fourth-\ndimensional normal distribution, approximated by the subset (4.1). We then sample from N(v,Σ), a\nrandom group of four low-resolution coefficients. Now, the token representation of these coefficients can\nserve as a basis for a robustinitialization of the generative process of the required digit. In the case where\nthe guidance is provided by a vector representation of some text-prompt, one can create the normal\ndistribution using a subset of K-nearest neighbors in the image vector representation space.\nOnce some random seeding allows us to initialize the token sequence, we may introduce as much\ndiversity as required using the methods of Subsection 4.7 so that even using the same seed may generate\nvarious images corresponding to the given guidance.\n4.6. Conditional next token inference. In Greedy generative mode, using the method of Subsection\n3.2.1, the next selected token x(t), 1 ≤ x(t) ≤ 7, at location t, is the token for which the transformer\nassigns the highest probability from (p (t),...,p (t)). As described in Subsection 4.7 below, there are\n1 7\nvarious alternative methods to control the output of the transformer. However, since each generative\ntoken inference step is a statistical event, it may occur that the next predicted token is not valid at the\ncurrent position of the wavelet bit-plane scan. To overcome this, we apply conditional probability to\nensure any selected token satisfies the conditions below relating to the context and the current position\nin the scan.\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION 13\n(i) ‘Group4x4’ - The scan is at an index (4l ,4l ) and the group has not yet dissolved.\n1 2\n(ii) ‘Group2x2’ - The scan is at an index (2l ,2l ) and the group has not yet dissolved.\n1 2\n(iii) ’NowSignificantNeg’, ’NowSignificantPos’ - At the current location I, the coefficient α is still\nI\ninsignificant, possibly as part of a group of insignificant coefficients.\n(iv) ‘Insignificant’ - At the current location I, the coefficient α is still insignificant, possibly as part\nI\nof a group of insignificant coefficients.\nv) ‘NextAccuracy0’, ‘NextAccuracy1’ - At the current location I, the coefficient α has already been\nI\nreported to be significant.\n4.7. Controlling the degree of generative diversity during inference. Since we are applying a\nlanguage transformer model we may use various simple stochastic mechanisms to control the generative\nprocess during inference and allow a diversity of possible images to be generated from a single prompt.\nSome of the available stochastic methods are: Beam search with multinomial sampling, Top-k and Top-p.\nIn our experiments, we tested the latter two:\n(i) Top-k sampling-TheTop-k inferencemethod[9]filtersthek mostlikely nextwordsfirstandthen\nsamples from the probability mass that is redistributed among only those k next words. GPT2\nadopted this sampling scheme, which was one of the reasons for its success in story generation.\nIn Figure 9 below, we see a diversity of sandals generated by guiding the model with the vector\nrepresentation of the corresponding FashionMNIST ‘sandal’ class and using the Top-2 method.\nWe see that using k = 2 is sufficient to move the generative process from a deterministic process\nto a sufficiently diverse stochastic process, yet with output that fits the class description.\n(ii) Top-p sampling- In Top-p sampling or nucleus sampling, the selection pool for the next token is\ndetermined by the cumulative probability of the most probable tokens. Setting a threshold p, the\nmodelincludesjustenoughof themostprobabletokens sothattheir combined probabilityreaches\nor exceeds this threshold. Again, the distribution mass is redistributed among these tokens and\nthen the next token is sampled using this distribution. In Figure 8 we see different examples of\nthe digits ‘3’ and ‘8’ generated using the Top-0.6 method.\n5. Experimental results\nWe conducted experiments on the MNIST and FashionMNIST datasets. Here are some details:\n• The images in both datasets were padded with zeros to M×M = 32×32, where M = 2m, m = 5\nand normalized to have values within [0,1].\n• We used the Haar wavelet basis for the MNIST images and the bior4.4 wavelet basis for the\nFashionMNIST.\n• The images were tokenized with a final threshold of T =\n2−3\nfor MNIST and T =\n2−4\nfor\nFashionMNIST.\n• The maximal token sequence lengths were 1742 for MNIST and 3098 for FashionMNIST.\n• We trained two separate distillgpt2 models from scratch on the two datasets. As for the training\nconfigurations, both training sessions had batch size 4, learning rate 0.0004, and weight decay\n0.01.\n• Models were trained on an NVIDIA A100 GPU with 80GB; MNIST occupied around 22GB while\nFashionMNIST occupied 61GB. Both models were trained for a few days.\nResults with different controlling methods appear below in Figures 8 and 9.\n14 WAELMATTAR,IDANLEVY,NIRSHARONANDSHAIDEKEL\nFigure 8. Digits generated with Top-p = 0.6 along with a depiction of the generated\nwavelet coefficients.\nFigure 9. Sandals generated with Top-k = 2 along with a depiction of the generated\nwavelet coefficients.\nMore generated images for different classes of MNIST and FashionMNIST appear in the following\nfigures.\nFigure 10. More MNIST results.\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION 15\nFigure 11. More FashionMNIST results.\n6. Discussion and future work\nIn this paper, we introduced a novel method for image generation that is based on elements of wavelet\nimage coding and NLP transformers. Unfortunately, our research group does not have access to sufficient\ncomputational resources at the moment, so this work serves as a first modest proof of concept. Indeed,\nthe wavelet representation is a powerful tool in image processing that can serve as a basis for many image\ngeneration functionalities. Here, we list some directions that we will consider for future work.\n6.1. Generation of color images at high resolution and with fine details. In our experiments,\nwe only generated small grayscale images. We provide here some details on how the method can be\ngeneralized:\n(i) Color images - For color images (or even spectral images), we may adopt a well-known paradigm\nfromimagecompression. Forimproved performance,onemaytransforminputimages intheRGB\ncolor space to the YCbCr color space. The Y component is the luminance component, essentially\nthe image’s grayscale part. The other two components, Cb and Cr, capture the color information\nof the image. Typically, the luminance component carries most of the visual information, and\nthus also, its encoding is usually the significant part of an encoded image. In image coding, one\nusually encodes separately each of the three channels. Our method can then be generalized to\ncolor images by applying the DWT and the tokenization process separately to each color channel.\n(ii) Generating fine details - Using our wavelet model, finer details are captured at higher bit-planes.\nThechoiceofthefinalthresholdofthefinalbit-planeprovidesexcellentandveryconsistentcontrol\nover the amount of detail one wishes to generate. This quantization technique is at the heart of\nthe JPEG algorithms and translates to very specific modes in digital cameras that can be set to:\n“Visually Lossless”, “High”, “Medium”, etc. This exact form of control also applies to wavelets\nbut, unfortunately, is not the default mode of operation in JPEG2000. Obviously, to generate\nfiner details, one needs to train the transformer on longer token sequences, again requiring more\ncomputational resources.\n6.2. Support for generation of compositions of blobs. In many cases, one wishes to apply fine-\ngrained control of compositional text-to-image generation, where certain locations in the image, marked\nperhaps with bounding boxes or ellipses, receive different textual descriptions [38]. One possible method\nto accomplish this using the wavelet generative approach is to apply the transformer in evaluation mode\nand apply the vector representation of the blob’s textual prompt as described in Subsection 4.4 whenever\nthe bit-plane scan is at indices of wavelet coefficients whose support intersects the blob.\n6.3. Multi-modal generation. The ability to represent an image’s visual information as a sequence of\ntokens presents an attractive possibility of merging the wavelet-based tokens with other language tokens\nto create a unified multi-modal transformer.\nFunding\nN. Sharon is partially supported by the NSF-BSF award 2019752. W. Mattar is partially supported\nby The Nehemia Levtzion Scholarship for Outstanding Doctoral Students from the Periphery (2023). N.\nSharon and W. Mattar are partially supported by the DFG award 514588180.\n16 WAELMATTAR,IDANLEVY,NIRSHARONANDSHAIDEKEL\nReferences\n[1] R. W. Buccigrossi and E. P. Simoncelli, Image compression via joint statistical characterization in\nthe wavelet domain, IEEE transactions on Image processing 8 (1999), 1688-1071.\n[2] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S.\nBrahma, A.Webson, S.S.Gu,Z.Dai, M.Suzgun,X.Chen,A.Chowdhery,A.Castro-Ros, M.Pellat,\nK.Robinson,D.Valter, S.Narang,G.Mishra,A.Yu,V.Zhao,Y.Huang,A.Dai, H.Yu,S.Petrov,E.\nH. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le and J. Wei, Scaling Instruction-Finetuned\nLanguage Models, Journal of Machine Learning Research 25 (2024), 1-153.\n[3] C. Dana and F. V´aclav, DISCRETE CDF 9 / 7 WAVELET TRANSFORM FOR FINITE-LENGTH\nSIGNALS, https://api.semanticscholar.org/CorpusID:208013335, 2011.\n[4] I. Daubechies, Ten Lectures on Wavelets, SIAM, 1992.\n[5] L. Deng, The mnist database of handwritten digit images for machine learning research, IEEE signal\nprocessing magazine 29 (2012), 141-142.\n[6] R. DeVore, Nonlinear approximation, Acta Numerica 7 (1998), 51-150.\n[7] P. Dhariwal and A. Nichol, Diffusion models beat gans on image synthesis, Advances in neural infor-\nmation processing systems 34 2021, 8780-8794.\n[8] P. Esser, R. Rombach and B. Ommer, Taming transformers for high-resolution image synthesis,\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition 2021, 12873-\n12883.\n[9] A. Fan, M. Lewis and Y. Dauphin, Hierarchical Neural Story Generation, Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) 2018,\n889-898.\n[10] P. Gage, A new algorithm for data compression The C Users journal archive 12 (1994), 23-38.\n[11] F. Guth, S. Coste, V. De Bortoli and S. Mallat, Wavelet score-based generative modeling, Advances\nin Neural Information Processing Systems 35 (2022), 478-491.\n[12] J. Ho, A. Jain and P. Abbeel, Denoising Diffusion Probabilistic Models, Advances in Neural Infor-\nmation Processing Systems 33 (2020), 6840-6851.\n[13] HuggingFace DistilGPT2, https://huggingface.co/distilbert/distilgpt2, 2019.\n[14] HuggingFace Tokenizer https://huggingface.co/docs/tokenizers/index.\n[15] L.Jiang,B.Dai,W.WuandC.C.Loy,Focal Frequency Loss for Image Reconstruction and Synthesis,\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) 2021, 13919-\n13929.\n[16] M. M. Kivanc, I. Kozintsev, K. Ramchandran and P. Moulin, Low-complexity image denoising based\non statistical modeling of wavelet coefficients, IEEE Signal Processing Letters 6 (1999), 300-303.\n[17] S. Mallat, A Wavelet tour of signal processing, the sparse way, Academic Press, 2009.\n[18] K. M. Mihcak, I. Kozintsev and K. Ramchandran, Spatially adaptive statistical modeling of wavelet\nimage coefficientsand itsapplication todenoising,proceedingsof1999IEEEInternationalConference\non Acoustics, Speech, and Signal Processing ICASSP99 6 (1999), 3253-3256.\n[19] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever and M. Chen,\nGLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,\nProceedings of the 39th International Conference on Machine Learning 162 (2022), 16784-16804.\n[20] H. Phung, Q. Dao and A. Tran, Wavelet Diffusion Models are fast and scalable Image Generators,\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023, 10199-10208.\n[21] M. Phuong and M. Hutter, Formal Algorithms for Transformers, https://arxiv.org/abs/2207.09238,\n2022.\n[22] A. Radford, J.W. Kim, C. Hallacy, A.Ramesh,G. Goh, S.Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark and others, Learning transferable visual models from natural language supervision, Interna-\ntional conference on machine learning (2021), 8748-8763.\n[23] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu andM. Chen, Hierarchical text-conditional image gener-\nation with clip latents, arXiv, 2022.\n[24] A. Ramesh,M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen and I. Sutskever, Zero-shot\ntext-to-image generation, International conference on machine learning 2021, 8821-8831.\nWAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION 17\n[25] R. Rombach, A. Blattmann D. Lorenz, P. Esser and B. Ommer, High-resolution image synthesis with\nlatent diffusion models, Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition 2022, 10684-10695.\n[26] A. Said and W. Pearlman, A new, fast, and efficient image codec based on set partitioning in hierar-\nchical trees, IEEE Transactions on Circuits and Systems for Video Technology 6 (1996), 243-250.\n[27] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, K. Ghasemipour, L. Gontijo, Raphael,\nA.Karagol, Burcu,Salimans, Timandothers, Photorealistic text-to-image diffusion models with deep\nlanguage understanding, Advances in neural information processing systems 35 (2002), 36479-36494.\n[28] V. Sanh, DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, Proceedings\nof Thirty-third Conference on Neural Information Processing Systems (NIPS2019).\n[29] J.Shapiro,Embedded image coding using zerotrees of wavelet coefficients, IEEETransactionsinsignal\nprocessing 41 1993, 3445-3462.\n[30] D. Taubman and M. Marcellin, JPEG2000: Image Compression Fundamentals, Standards and Prac-\ntice, 2nd edition, Springer, 2002.\n[31] Y. Tay, M. Dehghani, D. Bahri and D. Metzler, Efficient transformers: A survey, ACM Computing\nSurveys 55 (2022), 1-28.\n[32] K. Tian, Y. Jiang, Z. Yuan, B. Peng, and L. Wang, Visual Autoregressive Modeling: Scalable Image\nGeneration via Next-Scale Prediction, arXiv 2024.\n[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, L. Kaiser and I. Polosukhin,\nAttention is all you need, Advances in neural information processing systems 30 (2017).\n[34] X. Wang,W. Wang, Y. Cao, C. Shen and T. Huang, Images speak in images: A generalist painter\nfor in-context visual learning, Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition 2023, 6830-6839.\n[35] D. Lee, C. Kim, S. Kim, M. Cho and W. Wook, Autoregressive image generation using residual\nquantization,ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition\n2022, 11523-11532.\n[36] Q. Zhu,X. Li, J. Sun and H. Bai, WDIG: a wavelet domain image generation framework based on\nfrequency domain optimization, EURASIP Journal on Advances in Signal Processing 2023, 66.\n[37] G. K. Wallace, The JPEG still picture compression standard, IEEE Transactions on Consumer Elec-\ntronics 38 (1992), xviii-xxxiv.\n[38] N. Weili, L. Sifei, M. Morteza, L. Chao, E. Benjamin and V. Arash, Compositional Text-to-Image\nGeneration with Dense Blob Representations, arXiv 2024.\n[39] Q. Yu, M. Weber, X. Deng, X. Shen, D. Cremers and L. Chen, An image is worth 32 tokens for\nreconstruction and generation, NeurIPS 2024, to appear.\n[40] Y. Yu, F. Zhan, S. Lu, J. Pan, F. Ma, X. Xie and C. Miao, Chunyan, Wavefill: A wavelet-based\ngeneration network for image inpainting, Proceedings of the IEEE/CVF international conference on\ncomputer vision 2021, 14114-14123.",
    "pdf_filename": "Wavelets_Are_All_You_Need_for_Autoregressive_Image_Generation.pdf"
}