{
    "title": "SkillTree Explainable Skill-Based Deep Reinforcement Learning for Long-Horizon Control Tasks",
    "context": "Deep reinforcement learning (DRL) has achieved remark- able success in various domains, yet its reliance on neural networks results in a lack of transparency, which limits its practical applications in safety-critical and human-agent in- teraction domains. Decision trees, known for their notable ex- plainability, have emerged as a promising alternative to neu- ral networks. However, decision trees often struggle in long- horizon continuous control tasks with high-dimensional ob- servation space due to their limited expressiveness. To ad- dress this challenge, we propose SkillTree, a novel hierar- chical framework that reduces the complex continuous action space of challenging control tasks into discrete skill space. By integrating the differentiable decision tree within the high- level policy, SkillTree generates diecrete skill embeddings that guide low-level policy execution. Furthermore, through distillation, we obtain a simplified decision tree model that improves performance while further reducing complexity. Experiment results validate SkillTree’s effectiveness across various robotic manipulation tasks, providing clear skill-level insights into the decision-making process. The proposed ap- proach not only achieves performance comparable to neu- ral network based methods in complex long-horizon control tasks but also significantly enhances the transparency and ex- plainability of the decision-making process. Deep Reinforcement learning (DRL) has been shown as a powerful framework for tackling complex decision-making tasks, achieving remarkable success in various domains such as games (Mnih et al. 2015; BAAI 2023), robotic manip- ulation (Akkaya et al. 2019; Jitosho et al. 2023), and vi- sual navigation (Kulh´anek, Derner, and Babuˇska 2021). De- spite these advancements, the black-box nature of neural networks poses significant challenges in understanding and trusting their decision making processes. This lack of trans- parency is particularly concerning in safety-sensitive and human-agent interaction applications, where understanding the rationale behind decisions is essential (Hickling et al. 2023). For example, in the autonomous driving domain, only *Corresponding author. AAAI 2025 Conference Submission if human users can understand the driving policies can they trustfully use them in their cars. Explainable reinforcement learning (XRL) (Heuillet, Couthouis, and D´ıaz-Rodr´ıguez 2021; Hickling et al. 2023; Milani et al. 2024) aims to enhance the transparency and ex- plainability of DRL models. XRL methods can be broadly categorized into inherently explainable models, where ex- plainability is induced in the model training process, and post-hoc explanations, where models are explained after training. Existing works have shown that using a decision tree (DT) (Costa and Pedreira 2023) as the underlying model is effective (Frosst and Hinton 2017; Bastani, Pu, and Solar- Lezama 2018; Ding et al. 2020; Vasi´c et al. 2022). In a DT, observations are input at the root node, and decisions are de- termined by conditional branches leading to the leaf nodes, which provide the final outputs. This simple structure gives DTs high explainability, offering clear and straightforward decision paths. Unlike post-hoc methods that rely on ex- ternal algorithms to interpret black-box models, DT-based methods embed transparency directly within the model. The DT-based XRL paradigm not only enhances clarity but also simplifies debugging and validation, as the rationale behind each decision is explicitly encoded within the tree. Despite their advantages, DT-based methods are not suit- able for the following challenging task settings. (a) Long- horizon tasks: The temporally-extended decision processes require large and complex trees, which are difficult to opti- mize (Liu et al. 2021). (b) High-dimensional state spaces: DTs often lack sufficient representational ability to effec- tively manage high-dimensional state spaces, leading to sub- optimal performance in these environments (Bastani, Pu, and Solar-Lezama 2018; Ding et al. 2020). (c) Continuous action spaces: The limited number of leaf nodes constrains DTs’ ability to encode continuous control policies optimally, particularly in complex robotic tasks. These limitations re- strict the applicability of DT-based RL methods in complex environments, highlighting the need for approaches that can manage high-dimensional spaces and long-horizon tasks. To address the limitations of traditional DTs in han- dling long-horizon, high-dimensional, and continuous con- trol tasks, we propose a novel framework called SkillTree. SkillTree introduces the concept of skills, which represent arXiv:2411.12173v1  [cs.LG]  19 Nov 2024",
    "body": "SkillTree: Explainable Skill-Based Deep Reinforcement Learning\nfor Long-Horizon Control Tasks\nYongyan Wen1, Siyuan Li1*, Rongchang Zuo1, Lei Yuan2, 3, 4, Hangyu Mao5, Peng Liu1\n1Faculty of Computing, Harbin Institute of Technology\n2National Key Laboratory of Novel Software Technology, Nanjing University\n3School of Artificial Intelligence, Nanjing University\n4Polixir Technologies, 5SenseTime Research\n23B903027@stu.hit.edu.cn, siyuanli@hit.edu.cn, 2021110788@stu.hit.edu.cn, yuanl@lamda.nju.edu.cn,\nmaohangyu@sensetime.com, pengliu@hit.edu.cn\nAbstract\nDeep reinforcement learning (DRL) has achieved remark-\nable success in various domains, yet its reliance on neural\nnetworks results in a lack of transparency, which limits its\npractical applications in safety-critical and human-agent in-\nteraction domains. Decision trees, known for their notable ex-\nplainability, have emerged as a promising alternative to neu-\nral networks. However, decision trees often struggle in long-\nhorizon continuous control tasks with high-dimensional ob-\nservation space due to their limited expressiveness. To ad-\ndress this challenge, we propose SkillTree, a novel hierar-\nchical framework that reduces the complex continuous action\nspace of challenging control tasks into discrete skill space. By\nintegrating the differentiable decision tree within the high-\nlevel policy, SkillTree generates diecrete skill embeddings\nthat guide low-level policy execution. Furthermore, through\ndistillation, we obtain a simplified decision tree model that\nimproves performance while further reducing complexity.\nExperiment results validate SkillTree’s effectiveness across\nvarious robotic manipulation tasks, providing clear skill-level\ninsights into the decision-making process. The proposed ap-\nproach not only achieves performance comparable to neu-\nral network based methods in complex long-horizon control\ntasks but also significantly enhances the transparency and ex-\nplainability of the decision-making process.\nIntroduction\nDeep Reinforcement learning (DRL) has been shown as a\npowerful framework for tackling complex decision-making\ntasks, achieving remarkable success in various domains such\nas games (Mnih et al. 2015; BAAI 2023), robotic manip-\nulation (Akkaya et al. 2019; Jitosho et al. 2023), and vi-\nsual navigation (Kulh´anek, Derner, and Babuˇska 2021). De-\nspite these advancements, the black-box nature of neural\nnetworks poses significant challenges in understanding and\ntrusting their decision making processes. This lack of trans-\nparency is particularly concerning in safety-sensitive and\nhuman-agent interaction applications, where understanding\nthe rationale behind decisions is essential (Hickling et al.\n2023). For example, in the autonomous driving domain, only\n*Corresponding author.\nAAAI 2025 Conference Submission\nif human users can understand the driving policies can they\ntrustfully use them in their cars.\nExplainable reinforcement learning (XRL) (Heuillet,\nCouthouis, and D´ıaz-Rodr´ıguez 2021; Hickling et al. 2023;\nMilani et al. 2024) aims to enhance the transparency and ex-\nplainability of DRL models. XRL methods can be broadly\ncategorized into inherently explainable models, where ex-\nplainability is induced in the model training process, and\npost-hoc explanations, where models are explained after\ntraining. Existing works have shown that using a decision\ntree (DT) (Costa and Pedreira 2023) as the underlying model\nis effective (Frosst and Hinton 2017; Bastani, Pu, and Solar-\nLezama 2018; Ding et al. 2020; Vasi´c et al. 2022). In a DT,\nobservations are input at the root node, and decisions are de-\ntermined by conditional branches leading to the leaf nodes,\nwhich provide the final outputs. This simple structure gives\nDTs high explainability, offering clear and straightforward\ndecision paths. Unlike post-hoc methods that rely on ex-\nternal algorithms to interpret black-box models, DT-based\nmethods embed transparency directly within the model. The\nDT-based XRL paradigm not only enhances clarity but also\nsimplifies debugging and validation, as the rationale behind\neach decision is explicitly encoded within the tree.\nDespite their advantages, DT-based methods are not suit-\nable for the following challenging task settings. (a) Long-\nhorizon tasks: The temporally-extended decision processes\nrequire large and complex trees, which are difficult to opti-\nmize (Liu et al. 2021). (b) High-dimensional state spaces:\nDTs often lack sufficient representational ability to effec-\ntively manage high-dimensional state spaces, leading to sub-\noptimal performance in these environments (Bastani, Pu,\nand Solar-Lezama 2018; Ding et al. 2020). (c) Continuous\naction spaces: The limited number of leaf nodes constrains\nDTs’ ability to encode continuous control policies optimally,\nparticularly in complex robotic tasks. These limitations re-\nstrict the applicability of DT-based RL methods in complex\nenvironments, highlighting the need for approaches that can\nmanage high-dimensional spaces and long-horizon tasks.\nTo address the limitations of traditional DTs in han-\ndling long-horizon, high-dimensional, and continuous con-\ntrol tasks, we propose a novel framework called SkillTree.\nSkillTree introduces the concept of skills, which represent\narXiv:2411.12173v1  [cs.LG]  19 Nov 2024\n\ntemporal abstractions of low-level actions in the context of\nRL. These skills simplify the decision-making process by\nbreaking down long trajectories into manageable segments,\nenabling the agent to plan at a higher level, thereby reduc-\ning the complexity associated with extended tasks. However,\nthe challenge of skill representation arises, as skills are of-\nten represented in continuous spaces, leading to difficulties\nin control and explainability. To overcome this, our frame-\nwork leverages hierarchical structures and discrete skill rep-\nresentation learning. By regularizing the skill space into dis-\ncrete units, we simplify policy learning for skill selection\nand enhance the explainability of the learned skills. Specif-\nically, we employ decision trees to index and execute these\ndiscrete skills, combining the inherent explainability of deci-\nsion trees with the flexibility of skill-based representations.\nThis approach provides a robust solution for managing com-\nplex, long-horizon decision tasks while offering clear, skill-\nlevel insights into the decision-making process. We summa-\nrize the main contributions of this paper as follows:\n1. We propose SkillTree, a novel hierarchical, skill-based\nmethod for explainable reinforcement learning. To the\nbest of our knowledge, it marks the first successful appli-\ncation of DT-based explainable method in long-horizon\ncontinuous control tasks.\n2. We introduce a method for discrete skill representation\nlearning, which effectively reduces the skill space and\nimproves the efficiency of skill-based policy learning.\n3. Experiment results across a variety of robotic manipu-\nlation tasks demonstrate that our method provides skill-\nlevel explanations while achieving performance compa-\nrable to neural network based approaches.\nRelated Work\nExplainable RL\nRecently, explainability approaches in DRL have been\nbroadly categorized into intrinsic and post-hoc explanations,\nbased on the method of their generation. We briefly dis-\ncuss these two classes of methods here. For a more detailed\ntaxonomy and discussion of XRL, please refer to the fol-\nlowing surveys: (Heuillet, Couthouis, and D´ıaz-Rodr´ıguez\n2021; Hickling et al. 2023; Milani et al. 2024).\nIntrinsic explanation methods are designed with explain-\nability as a fundamental aspect, often incorporating simpler,\nmore transparent models such as decision trees (Liu et al.\n2019; Silva et al. 2020; Ding et al. 2020), linear models (Ra-\njeswaran et al. 2017; Molnar, Casalicchio, and Bischl 2020;\nWabartha and Pineau 2023) or symbolic rules (Lyu et al.\n2019; Ma et al. 2021). By ensuring that the model itself\nis explainable, intrinsic methods offer real-time, straightfor-\nward explanations of the agent’s decisions. For instance, us-\ning a decision tree as the policy model enables immediate\nand clear understanding of the decision-making process.\nPost-hoc methods apply explainability techniques after\nthe RL model has been trained. These approaches do not\nalter the original model but instead seek to explain the\ndecisions of more complex, often black-box models. Typ-\nical post-hoc methods include feature attribution meth-\nods like saliency maps (Greydanus et al. 2018; Anderson\net al. 2019; Olson et al. 2021) and Shapley value (Zhang\net al. 2021; Heuillet, Couthouis, and D´ıaz-Rodr´ıguez 2022),\nwhich highlight the most influential input features. Model\ndistillation approximates the black-box model, either lo-\ncally or globally, with explainable models like decision trees\n(Bastani, Pu, and Solar-Lezama 2018; Coppens et al. 2019;\nBewley and Lawry 2021; Orfanos and Lelis 2023). Counter-\nfactual explanations generate alternative scenarios by mod-\nifying input features to observe changes in the model’s out-\nput, offering a way to understand the sensitivity of deci-\nsions (Madumal et al. 2020; Olson et al. 2021). Some studies\nhave explored identifying critical states crucial for achieving\nthe final reward from a state-reward perspective (Guo et al.\n2021; Cheng et al. 2024). Additionally, researchers have an-\nalyzed the impact of different training samples on policy\ntraining outcomes (Deshmukh et al. 2023). Lastly, rule ex-\ntraction methods create human-readable rules that approx-\nimate the model’s behavior and enhance the transparency\nof the decision-making process (Hein, Udluft, and Runkler\n2018; Landajuela et al. 2021).\nDecision Tree\nDecision trees are widely used for their explainability and\nsimplicity, often serving as function approximators in rein-\nforcement learning. Classical decision tree algorithms like\nCART (Loh 2011) and C4.5 (Quinlan 1993) produce ex-\nplainable surrogate policies but are limited in expressive-\nness and impractical for integration into DRL models. Ap-\nproaches such as VIPER (Bastani, Pu, and Solar-Lezama\n2018) attempt to distill neural network policies into veri-\nfiable decision tree by imitation learning. Recent advance-\nments include rule-based node divisions (Dhebar and Deb\n2020) and parametric differentiable decision tree, such as\nsoft decision tree (Frosst and Hinton 2017) and differen-\ntiable decision tree for approximating Q-function or pol-\nicy (Silva et al. 2020). While these methods improve ex-\npressiveness, they are generally constrained to simpler, low-\ndimensional environments. By contrast, our approach ad-\ndresses the complexity of high-dimensional continuous con-\ntrol tasks by converting the action space into a skill space,\nthereby reducing complexity and providing a more effective\nand explainable solution for challenging tasks.\nSkills in Reinforcement Learning\nRecent works have increasingly focused on learning and\nutilizing skills to improve agent efficiency and generaliza-\ntion (Shu, Xiong, and Socher 2018; Hausman et al. 2018;\nShankar and Gupta 2020; Lynch et al. 2020). Skills can be\nextracted from the offline dataset (Shiarlis et al. 2018; Kipf\net al. 2019; Pertsch, Lee, and Lim 2021; Pertsch et al. 2021;\nShi, Lim, and Lee 2023) or manually defined (Lee, Yang,\nand Lim 2019; Dalal, Pathak, and Salakhutdinov 2021;\nBAAI 2023). Typically, these skills are represented as high-\ndimensional, continuous latent variables, which limits their\nexplainability. In contrast, we propose extracting a discrete\nskill representation from a task-agnostic dataset, making it\nsuitable for DT-based policy learning.\n\nPreliminary\nReinforcement Learning\nIn this paper, we are concerned with a finite Markov decision\nprocess (Sutton and Barto 2018), which can be represented\nas a tuple M = (S, A, R, P, p0, γ, H). Where S is the state\nspace, A is the action space, S×A 7→R is the reward space,\nP : S ×A 7→∆(S) is the transfer function, p0 : ∆(S) is the\ninitial state distribution, γ ∈(0, 1) is the discount factor, and\nH is the episode length. The learning objective is to find the\noptimal policy π maximizing the expected discounted return\nmax J (π) = max\nπ\nEs0∼p0,(s0,a0,...,sH)∼π\n\"H−1\nX\nt=0\nγtR(st, at)\n#\n.\n(1)\nSoft Decision Tree\nThe differentiable decision tree (Frosst and Hinton 2017) is\na special type of decision tree that differs from the tradi-\ntional ones like CART (Loh 2011) by using probabilistic de-\ncision boundaries instead of deterministic boundaries. This\nmodification increases the flexibility of model by allowing\nfor smoother transitions at each decision node, as shown in\nFigure 1. The differentiable soft decision tree is a complete\nbinary tree in which each inner node can be represented by a\nlearnable weight ωi\nj and bias ϕi\nj. Here, i and j represent the\nlayer index and the node index of that layer, respectively. By\nusing the sigmoid function σ(·), the probability σ(ωx + ϕ)\nrepresents the transfer probability from that node to the left\nsubtree, and 1 −σ(ωx + ϕ) gives the transfer probability\nto the right subtree. This process allows the decision tree to\nprovide a “soft” decision at each node for the current input.\nConsider a decision tree with depth d. The nodes in the\ntree are denoted by nu, where u = 2i −1 + j denotes the\nnode index, and the decision path P can be represented as\nP = arg max\n{u}\nd−1\nY\ni=0\n2i\nY\nj=0\npi→i+1\n⌊j\n2⌋→j\n(2)\nwhere {u} denotes nodes on the decision path, and pi→i+1\n⌊j\n2 ⌋→j\ndenotes the probability of moving from node n2i+⌊j/2⌋to\nnode n2i+1+j, the probability is calculated as\npi→i+1\n⌊j\n2⌋→j =\n\n\n\n\n\n\n\nσ\n\u0012\nωi\n⌊j\n2⌋x + ϕi\n⌊j\n2⌋\n\u0013\nif j\nmod 2 = 0,\n1 −σ\n\u0012\nωi\n⌊j\n2⌋x + ϕi\n⌊j\n2⌋\n\u0013\notherwise.\n(3)\nWith Equation 2 and 3, the tree can be traversed down to\nthe leaf nodes. Leaf nodes are represented by parameter vec-\ntors w2d×K, where K is the number of output categories.\nThe output of the leaf node k is a categorical distribution\ngiven by softmax(wk), which is independent of the input.\nThe learnable parameters of both the leaf nodes and the in-\nner nodes can be optimized using existing DRL algorithms,\nmaking the soft decision tree suitable as an explainable al-\nternative model for DRL policy.\nFigure 1: Comparison of the soft decision tree (left) and the\nhard decision tree (right).\nExplainable RL with SkillTree\nOur goal is to leverage discrete skill representations to\nlearn a skill decision tree, enabling skill-level explainabil-\nity. Overall, our approach is divided into three stages: (1)\nextracting the discrete skill embeddings from the offline\ndataset, (2) training an explainable DT-based skill policy by\nRL for downstream long-horizon tasks, and (3) distilling the\ntrained policy into a simplified decision tree.\nLearning Discrete Skill Embeddings\nOur goal is to obtain a fixed number of skill embeddings\nthat contain a temporal abstraction of a sequence of ac-\ntions. Skills represent action sequences of useful behav-\niors and are represented by D-dimensional vectors. We as-\nsume an existing task-agnostic dataset D = {τi}, τi =\n{s0, a0, . . . , sHi, aHi}, consists of d trajectories of varying\nlengths, where each trajectory includes states st, . . . , sHi\nand corresponding actions at, . . . , aHi. To regularize the\nskill embeddings, instead of learning within the continuous\nskill space, we employ a skill table Z = {e1, e2, . . . , eK}\n(i.e., codebook) that contains D-dimensional vectors, each\nrepresenting a learnable skill embedding.\nTo learn reusable skills from offline datasets and guide ex-\nploration efficiently, we modified VQ-VAE (Van Den Oord,\nVinyals et al. 2017) to learn skill representations and skill\nprior, as illustrated in Figure 2. First, a demonstration trajec-\ntory is randomly divided into state-action segments of fixed\nlength h during the training. Each segment contains h ob-\nservation states st,h = st, st+1, . . . , st+h−1 and the corre-\nsponding actions at,h = at, at+1, . . . , at+h−1. The input\nto the encoder qϕ is st,h and at,h. The output of the en-\ncoder is a D-dimensional embedding ze. Next, we select the\nembedding from the codebook that is nearest to ze and its\nindex k = arg min\nj\n∥ze −ej∥2, obtaining the regularized\nembedding zq = ek. Finally, the low-level actual action at\nis obtained through the state-conditioned decoder πl(st, zq),\nwhich serves as the low-level policy.\nIn addition, following the approach of (Pertsch, Lee, and\nLim 2021), we introduce a skill prior to guide the high-\nlevel policy. The skill prior p(kt|st) predicts the skill dis-\ntribution given the first state of the sequence in the offline\ndata. This enhances exploration efficiency during high-level\npolicy training by avoiding random skill sampling during\nwarming up. Here, st represents the first state of the se-\nquence. The skill prior is a K-categorical distribution that\nmatches the output of encoder. The objective of the skill\n\nHigh-level Policy\nLow-level Policy\nCodebook\nDecoder\nEncoder\nSkill Prior\nDiscrete Skill Embedding Learning\nEnvironment\nDecision Tree\nSkill Index\nSkill Embedding\nDownstream High-Level Policy Learning\nReward\nNext State\nAction\nFigure 2: Discrete skill embedding learning and downstream high-level DT policy learning. After completing the skill learning,\nwe freeze the decoder and skill prior, and then proceed to finetune the codebook during the high-level policy learning.\nprior is to fit the output distribution of encoder conditioned\nsolely on the first state st. Since the encoder can only pro-\nduce a fixed skill index k, the skill prior aims to match a one-\nhot distribution of that index, aligning its predictions with\nthe encoder. The overall training objective is\nL =\nh−1\nX\nt=0\n∥πl (st, zq) −at∥2\n2\n+ ∥sg[ze] −e∥2\n2 + β∥ze −sg[e]∥2\n2\n−\nK\nX\nk=1\nqϕ(z = k|st,h, at,h) log p(kt = k|st),\n(4)\nwhere sg[·] is the stopgradient operator, p(kt = k|st) is the\nprobability that kt is category k and β is the hyperparam-\neter of the commitment loss term. Categorical distribution\nqϕ(z = k|st,h, at,h) probabilities are defined as one-hot as\nqϕ(z = k|st,h, at,h) =\n(1\nfor k = arg min\nj\n∥ze −ej∥2,\n0\notherwise,\n(5)\nwhere ze is the output of the encoder.\nDownstream RL Policy Learning with Skill Prior\nTo reuse the learning skill embeddings and improve explo-\nration efficiency in RL training, we utilize the skill prior\nto regularize the high-level policy in skill space. After pre-\ntraining the low-level policy and skill prior, the low-level\npolicy πl(st, zt) and the skill prior p(kt|st) are fixed. The\nhigh-level policy πh(kt|st) is executed every h-steps, and\nthe output is the categorical distribution of the skill index kt.\nTo obtain the actual skill embedding, we sample an index kt\nfrom the distribution and then query the codebook with kt,\ni.e., zt = Z[kt]. In each single time step, the low-level pol-\nicy predicts the output action conditioned on the state and\nskill embedding. The low-level policy executes h-steps and\nthe state changes to st+h. We use P+(st+h|st, zt) to denote\nthe state transition after h steps (see line 3-6 in Algorithm 1).\nNext, in order to improve the training efficiency by utilizing\nthe prior information obtained from previous skill training,\nwe add an additional prior term as the learning objective:\nJ(θ) = Eπh\n\"H−1\nX\nt=0\nγtr+(st, zt) −αDKL (πh(kt|st)∥p(kt|st))\n#\n,\n(6)\nwhere r+(st, zt) = Pt+h\ni=t R(si, zi) is the total sum of re-\nwards, θ is the trainable parameter of policy, α is the hy-\nperparameter of divergence term. To prevent the policy from\noverfitting the prior, following (Pertsch, Lee, and Lim 2021),\nα can be automatically tuned by defining a target divergence\nδ (see Algorithm 1 line 15).\nTypically, the high-level policy is implemented as a neu-\nral network, which is a black box and lacks transparency.\nIn order to achieve an explainable decision making process,\nwe implement the high-level policy πh(kt|st) using a differ-\nentiable soft decision tree instead, which can be optimized\nusing existing backpropagation algorithm. Furthermore, the\nstructure of the skill prior is identical to that of the high-\nlevel policy, both of which are implemented using soft de-\ncision trees. To improve learning efficiency, the parameters\nof the skill prior are used to initialize the policy during the\ninitialization of training. We modify SAC (Haarnoja et al.\n2018) algorithm to learn the objective 6 and the process is\nsummarized in Algorithm 1. See Appendix for more details.\n\nAlgorithm 1: SkillTree RL Training\nInputs: codebook Z, target divergence δ, learning rates\nλθ, λψ, λα, target update rate τ, DT skill prior p(kt|st) and\ndiscount factor γ.\n1: Initialize replay buffer B, d-depth high-level DT policy\nπθ(kt|st), critic Qψ(st, zt), target network Q ¯\nψ(st, zt)\n2: for each iteration do\n3:\nfor every h environment steps do\n4:\nkt ∼πθ(kt|st)\n5:\nzt = Z[kt]\n6:\nst′ ∼P+(st+h|st, zt)\n7:\nB ←B ∪{st, zt, r+\nt , st′}\n8:\nend for\n9:\nfor each gradient step do\n10:\nkt′ ∼πθ(kt′|st′)\n11:\nzt′ = Z[k′\nt]\n12:\n¯Q = r+\nt +γ\n\u0002\nQ ¯\nψ(st′, zt′) + αDKL(πθ(kt′|st′)∥p(kt′|st′))\n\u0003\n13:\nθ ←θ−λθ∇θ\n\u0002\nQ ¯\nψ(st, zt) −DKL(πθ(kt|st)∥p(kt|st))\n\u0003\n14:\nψ ←ψ −λψ∇ψ\nh\n1\n2\n\u0000Qψ(st|zt) −¯Q\n\u00012i\n15:\nα ←α−λα∇α [α (DKL(πθ(kt|st)∥p(kt|st)) −δ)]\n16:\n¯ψ ←τψ + (1 −τ) ¯ψ\n17:\nend for\n18: end for\n19: return policy πθ(kt|st), finetuned codebook Z\nDistilling the Decision Tree\nAfter downstream RL training, we obtain a soft decision\ntree with skill-level explainability, where each decision node\nprobabilistically selects child nodes until a skill choice is\nmade. Generally, models with fewer parameters are easier to\nunderstand and accept. To further simplify the model struc-\nture, we distill the soft decision tree into a more straight-\nforward hard decision tree through discretization. Previous\nmethods select the feature with the highest weight at each\nnode but it often obviously degrades tree performance (Ding\net al. 2020). By framing the problem as an imitation learning\ntask, we leverage the learned high-level policy as an expert\npolicy. We sample trajectories from the environment to gen-\nerate the dataset. Given the simplified skill space, we em-\nploy a low-depth CART to classify state-skill index pairs.\nThis approach effectively manages the complexity of the tree\nwhile preserving performance, ensuring that the model re-\nmains both explainable and efficient.\nExperiments\nIn this section, we evaluate SkillTree’s effectiveness in\nachieving a balance between explainability and perfor-\nmance. We demonstrate that our method not only provides\nclear, skill-level explanations but also achieves performance\non par with neural network based approaches.\nTasks Setup\nTo evaluate the performance of our method in long-horizon\nsparse reward tasks, we chose the Franka Kitchen control\ntasks in D4RL (Fu et al. 2020), robotic arm manipulation\n(a) Kitchen MKBL\n(b) Kitchen MLSH\n(c) CALVIN\n(d) Office\nFigure 3: Four long-horizon sparse reward tasks to evaluate.\n(a) The robotic arm has to finish four subtasks in the cor-\nrect order, i.e., Microwave - Kettle - Bottom Burner - Light\n(MKBL). (b) Similar to (a), but with different subtasks: Mi-\ncrowave - Ligt - Slide Cabinet - Hinge Cabinet (MLSH). (c)\nFinish subtasks in the correct order, i.e., Open Drawer - Turn\non Lightbulb - Move Slider Left - Turn on LED. (d) In the of-\nfice cleaning task, the robotic arm needs to pick up objects\nand place them in corresponding containers in sequence.\ntask in CALVIN (Mees et al. 2022) and office cleaning task\n(Pertsch et al. 2021), as illustrated in Figure 3.\nKitchen\nThe Franka Kitchen environment is a robotic arm\ncontrolled environment based on the mujoco implementa-\ntion. The environment contains a 7-DOF robotic arm that is\nable to interact with other objects in the kitchen, such as a\nkettle and other objects. A total of 7 subtasks are included.\nThe observation is 60-dimensional and contains information\nabout joint positions as well as task parameters, and the ac-\ntion space is 9-dimensional with episode length of 280. The\noffset dataset contains 601 trajectories, and each trajectory\ncompletes a variety of tasks that may not be in the same\norder, but no more than 3 subtasks. We set two sets of dif-\nferent subtasks: MKBL and MLSH, as shown in Figure 3(a)\nand 3(b). The MLSH task is more difficult to learn due to the\nvery low frequency of subtask transitions in the dataset.\nCALVIN\nIn CALVIN, a 7-DOF Franka Emika Panda\nrobotic arm needs to be controlled to open a drawer, light\na light bulb, move a slider to the left and light an LED in\nsequence. the observation space is 21-dimensional and con-\ntains both robotic arm states and object states with episode\nlength of 360 steps. The dataset contains 1,239 trajectories\n\nFigure 4: Downstream task learning curves of both our method and baselines. Averaged over 5 independent runs.\nTable 1: Average completed subtasks (ACS) and the number of leaf nodes (Leaf) across methods and domains.\nMethod\nKitchen MKBL\nKitchen MLSH\nCALVIN\nOffice\nACS\nLeaf\nACS\nLeaf\nACS\nLeaf\nACS\nLeaf\nSPiRL\n3.08 ± 0.46\nN/A\n2.82 ± 1.08\nN/A\n2.77 ± 1.46\nN/A\n1.48 ± 0.96\nN/A\nVQ-SPiRL\n2.97 ± 0.41\nN/A\n2.04 ± 0.85\nN/A\n2.10 ± 0.81\nN/A\n1.69 ± 0.64\nN/A\nBC+SAC\n0.00 ± 0.00\nN/A\n0.00 ± 0.00\nN/A\n0.01 ± 0.10\nN/A\n0.00 ± 0.00\nN/A\nCART\n2.11 ± 0.92\n987\n1.16 ± 0.78\n981\n1.02 ± 0.20\n998\n0.00 ± 0.00\n949\nSkillTree\n2.90 ± 0.42\n64\n2.58 ± 0.80\n64\n1.90 ± 0.90\n64\n2.00 ± 0.00\n64\nSkillTree (Distilling)\n3.06 ± 0.65\n64\n2.52 ± 0.82\n64\n2.42 ± 0.49\n64\n2.00 ± 0.00\n64\nSkillTree (DC + Distilling)\n3.25 ± 0.70\n64\n2.62 ± 0.76\n64\n3.00 ± 0.00\n64\n2.00 ± 0.00\n64\nand a total of 34 subtasks. Since it contains a large number of\nsubtasks, the average frequency of transitions between any\nsubtasks in the dataset is very low, requiring more precise\nskill selection, which poses a greater challenge during the\nlearning of the high-level policy.\nOffice\nFor the Office environment, a 5-DOF WidowX\nrobotic arm needs to be controlled to pick up multiple ob-\njects and place them in their corresponding containers with\nepisode length of 350. We utilized a dataset collected by\n(Pertsch et al. 2021), which includes 2,331 trajectories. The\nstate space is 97-dimensional and the action space is 8-\ndimensional. Due to the freedom of object manipulation in\nthis environment, the task is more challenging to complete.\nBaselines\nIn the experiments, we compare our proposed method\nagainst several representative baselines to demonstrate its\neffectiveness and competitive performance with additional\nskill-level explainability.\n• SPiRL (Pertsch, Lee, and Lim 2021): Learns continuous\nskills embeddings and a skill prior, and guides a high-\nlevel policy using the learned prior. It serves as a compet-\nitive skill-based RL algorithm but lacks explainability.\n• VQ-SPiRL: Discretizes the skills in SPiRL but uses neu-\nral networks as the high-level policy model structure.\n• Behavioral Cloning + Soft Actor-Critic (BC+SAC):\nTrains a supervised behavioral cloning policy from the\ndemonstration dataset and finetunes it on the downstream\ntask using Soft Actor-Critic (Haarnoja et al. 2018). It\nserves as a general algorithm without leveraging skills.\n• CART (Loh 2011): Imitation learning using CART with\ntrained SPiRL agent as the teacher.\nFor SkillTree, we set the depth of the tree to 6 in all do-\nmains. The size of codebook K is 16 for Kitchen and Office,\nand 8 for CALVIN. For CART, we sample 1,000 trajectories\non each domain to train and set the maximum depth to 10.\nResults\nFrom the learning curves shown in Figure 4, it is evident that\nour method demonstrates comparable learning efficiency\nand asymptotic performance in all domains. This high-\nlights the effectiveness of our discrete skill representation\nin handling complex, long-horizon tasks. BC+SAC failed\nto complete any subtasks because of the sparse reward.\nIn the CALVIN and Office domain, our method exhibits a\nmarked improvement in early learning efficiency compared\nto SPiRL and performs comparably to VQ-SPiRL. This ad-\nvantage underscores the benefit of discrete skills in enhanc-\ning exploration efficiency, allowing our approach to quickly\nidentify and leverage useful skills for target task. Through\ncomparing to VQ-SPiRL, we validate that the decision tree\ncan maintain performance while reducing model complexity\nin simplified discrete skill space.\nIn Table 1, we evaluate the additional SkillTree after dis-\ntilling on sampled dataset. Specifically, we use two sam-\npling methods. One is to sample the trajectories directly with\nSkillTree (distilling, D), and the other is to clean the trajec-\ntories and keep only those with higher rewards (data clean-\ning + distilling, DC+D). In data cleaning, we select the tra-\njectories completed subtasks greater than 2 for Kitchen and\nCALVIN, and greater than 1 for Office. In each environment,\nwe sample 1000 trajectories. To control the complexity, we\n\nqpos_obj_10@0.01\nqpos_obj_2@-1.29\nqpos_5@2.06\nqpos_5@1.31\nqpos_4@0.09\nqpos_obj_3@-0.00\nqpos_obj_2@-1.19\n ≤\n >\nFigure 5: Visualization of the SkillTree (DC+D) with depth\n3. qpos and qpos obj mean the position of the robotic\narm and objects, respectively. n denotes the number of state-\nskill pairs in the divided decision set.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nmicrowave\nkettle\nbottom burner\ntop burner\nlight switch\nslide cabinet\nhinge cabinet\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 6: We fix the skill output of high-level policy and\nevaluate the focus on subtasks of each skill in Kitchen MKBL\ntask. We set the number of skills K to 16, as shown in the\nx-axis. The color means the success rate of each subtask.\nset the depth of the tree to 6 in all domains. We observe that\nafter data cleaning and distillation, the performance of Skill-\nTree can be further enhanced. In most of the domains, Skill-\nTree DC+D achieves best performance than other baselines.\nThis improvement reflects the effectiveness of our method\nin optimizing the skill representations, allowing it to adapt\nwell even in complex scenarios. Moreover, the distillation\nprocess with data cleaning not only simplifies the model but\nalso ensures that it retains critical decision making capabil-\nities, leading to better performance across different tasks.\nIn contrast, CART has worse performance despite having\ndeeper layers and a much larger number of leaf nodes than\nSkillTree. Huge number of leaf nodes (∼1000) suggests that\nit is difficult to understand intuitively.\nExplainability\nIn Figure 5, we visualize the final decision tree obtained\nafter distillation for the Kitchen MKBL task. This depth of\ndecision tree is only three but can complete an average of\n3.03 subtasks. The very few parameters make the decision-\nmaking process clear and easy to understand. Each node\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\nTime Step (× 10)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nSkill Index\nmicrowave\nkettle\nbottom burner\nlight switch\nFigure 7: Skill index output visualization of an episode in\nKitchen MKBL task, which finished all 4 subtasks.\nmakes splits based on one of the observed features. In the\n60-dimensional observations of the Kitchen domain, the last\n30 dimensions are task parameters that are independent of\nthe state and thus do not influence decision making. This\nproperty is reflected in the learned tree structure because the\ndecision nodes use only the features in the first 30 dimen-\nsions (robot arm position and object position) for splitting.\nTo analyze the effects of each skill, we also evaluated the\neffectiveness of different skills, as shown in Figure 6. We\nfixed the skill output individually and evaluated the comple-\ntion of subtasks. Each skill is capable of completing different\nsubtasks with varying focus. SkillTree selects the appropri-\nate skill based on the observed state, which are then executed\nby the low-level policy. Figure 7 illustrates the sequence of\nskill indices chosen in one episode, during which four sub-\ntasks were completed. It can be seen that the same skill was\nrepeatedly selected at consecutive time steps to complete\nspecific subtasks, indicating the reuse of particular skills.\nMoreover, repeated selection of the same skill indicates a\nrelatively long-term and stable skill execution process. By\nisolating and testing each skill, we can clearly see which\nskills are most effective for particular subtasks. This makes\nit easier to understand the role and functionality of each skill\nwithin the decision making process. See Appendix for more\nexperiment results as well as visualizations.\nConclusions and Limitations\nWe proposed a novel hierarchical skill-based explainable re-\ninforcement learning framework designed to tackle the chal-\nlenges of long-hoziron decision making in continuous con-\ntrol tasks. By integrating discrete skill representations and\nDT, our method offers a robust solution that enhances ex-\nplainability without sacrificing performance. The proposed\napproach effectively regularizes the skill space, simplifies\npolicy learning, and leverages the transparency of the DT.\nExperimental results demonstrate that our method not only\nachieves competitive performance compared to neural net-\nwork based approaches but also provides valuable skill-level\nexplanations. This work represents a important step towards\nmore transparent and explainable RL, paving the way for\ntheir broader application in complex real-world scenarios\nwhere understanding the decision making process is essen-\ntial. Despite the promising results demonstrated, several lim-\nitations remain. The low-level policy in SkillTree still lacks\nexplainability due to its reliance on neural networks, which\n\nlimits full transparency in the decision-making process. Ad-\nditionally, the current approach assumes an offline, task-\nagnostic dataset for skill learning, which may not always\nbe available in practical scenarios. Future work could fo-\ncus on developing methods to enhance the explainability of\nlow-level policies, possibly by integrating explainable mod-\nels directly into the low-level decision-making process.\nReferences\nAkkaya, I.; Andrychowicz, M.; Chociej, M.; Litwin, M.;\nMcGrew, B.; Petron, A.; Paino, A.; Plappert, M.; Powell,\nG.; Ribas, R.; et al. 2019. Solving rubik’s cube with a robot\nhand. arXiv preprint arXiv:1910.07113.\nAnderson, A.; Dodge, J.; Sadarangani, A.; Juozapaitis, Z.;\nNewman, E.; Irvine, J.; Chattopadhyay, S.; Fern, A.; and\nBurnett, M. 2019.\nExplaining reinforcement learning to\nmere mortals: an empirical study.\nIn Proceedings of the\n28th International Joint Conference on Artificial Intelli-\ngence, 1328–1334.\nBAAI, P. 2023. Plan4mc: Skill reinforcement learning and\nplanning for open-world minecraft tasks.\narXiv preprint\narXiv:2303.16563.\nBastani, O.; Pu, Y.; and Solar-Lezama, A. 2018. Verifiable\nreinforcement learning via policy extraction. In Advances in\nNeural Information Processing Systems, volume 31.\nBewley, T.; and Lawry, J. 2021. Tripletree: A versatile in-\nterpretable representation of black box agents and their en-\nvironments. In Proceedings of the AAAI Conference on Ar-\ntificial Intelligence, volume 35, 11415–11422.\nCheng, Z.; Wu, X.; Yu, J.; Sun, W.; Guo, W.; and Xing, X.\n2024. Statemask: Explaining deep reinforcement learning\nthrough state mask. In Advances in Neural Information Pro-\ncessing Systems, volume 36.\nCoppens, Y.; Efthymiadis, K.; Lenaerts, T.; Now´e, A.;\nMiller, T.; Weber, R.; and Magazzeni, D. 2019. Distilling\ndeep reinforcement learning policies in soft decision trees.\nIn Proceedings of the IJCAI 2019 workshop on explainable\nartificial intelligence, 1–6.\nCosta, V. G.; and Pedreira, C. E. 2023. Recent advances\nin decision trees: An updated survey. Artificial Intelligence\nReview, 56(5): 4765–4800.\nDalal, M.; Pathak, D.; and Salakhutdinov, R. R. 2021. Ac-\ncelerating robotic reinforcement learning via parameterized\naction primitives. In Advances in Neural Information Pro-\ncessing Systems, volume 34, 21847–21859.\nDeshmukh, S. V.; Dasgupta, A.; Krishnamurthy, B.; Jiang,\nN.; Agarwal, C.; Theocharous, G.; and Subramanian, J.\n2023. Explaining RL Decisions with Trajectories. In In-\nternational Conference on Learning Representations.\nDhebar, Y.; and Deb, K. 2020. Interpretable rule discovery\nthrough bilevel optimization of split-rules of nonlinear deci-\nsion trees for classification problems. IEEE Transactions on\nCybernetics, 51(11): 5573–5584.\nDing, Z.; Hernandez-Leal, P.; Ding, G. W.; Li, C.;\nand Huang, R. 2020.\nCdt: Cascading decision trees\nfor explainable reinforcement learning.\narXiv preprint\narXiv:2011.07553.\nFrosst, N.; and Hinton, G. 2017. Distilling a neural network\ninto a soft decision tree. arXiv preprint arXiv:1711.09784.\nFu, J.; Kumar, A.; Nachum, O.; Tucker, G.; and Levine, S.\n2020.\nD4rl: Datasets for deep data-driven reinforcement\nlearning. arXiv preprint arXiv:2004.07219.\nGreydanus, S.; Koul, A.; Dodge, J.; and Fern, A. 2018. Vi-\nsualizing and understanding atari agents. In International\nconference on machine learning, 1792–1801. PMLR.\nGuo, W.; Wu, X.; Khan, U.; and Xing, X. 2021.\nEdge:\nExplaining deep reinforcement learning policies.\nIn Ad-\nvances in Neural Information Processing Systems, vol-\nume 34, 12222–12236.\nHaarnoja, T.; Zhou, A.; Abbeel, P.; and Levine, S. 2018.\nSoft Actor-Critic: Off-Policy Maximum Entropy Deep Re-\ninforcement Learning with a Stochastic Actor. In Dy, J.; and\nKrause, A., eds., Proceedings of the 35th International Con-\nference on Machine Learning, volume 80 of Proceedings of\nMachine Learning Research, 1861–1870. PMLR.\nHausman, K.; Springenberg, J. T.; Wang, Z.; Heess, N.; and\nRiedmiller, M. 2018.\nLearning an embedding space for\ntransferable robot skills.\nIn International Conference on\nLearning Representations.\nHein, D.; Udluft, S.; and Runkler, T. A. 2018. Interpretable\npolicies for reinforcement learning by genetic programming.\nEngineering Applications of Artificial Intelligence, 76: 158–\n169.\nHeuillet, A.; Couthouis, F.; and D´ıaz-Rodr´ıguez, N. 2021.\nExplainability in deep reinforcement learning. Knowledge-\nBased Systems, 214: 106685.\nHeuillet, A.; Couthouis, F.; and D´ıaz-Rodr´ıguez, N. 2022.\nCollective explainable AI: Explaining cooperative strategies\nand agent contribution in multiagent reinforcement learning\nwith shapley values. IEEE Computational Intelligence Mag-\nazine, 17(1): 59–71.\nHickling, T.; Zenati, A.; Aouf, N.; and Spencer, P. 2023. Ex-\nplainability in deep reinforcement learning: A review into\ncurrent methods and applications. ACM Computing Surveys,\n56(5): 1–35.\nJitosho, R.; Lum, T. G. W.; Okamura, A.; and Liu, K. 2023.\nReinforcement Learning Enables Real-Time Planning and\nControl of Agile Maneuvers for Soft Robot Arms. In Tan, J.;\nToussaint, M.; and Darvish, K., eds., Proceedings of The 7th\nConference on Robot Learning, volume 229 of Proceedings\nof Machine Learning Research, 1131–1153. PMLR.\nKipf, T.; Li, Y.; Dai, H.; Zambaldi, V.; Sanchez-Gonzalez,\nA.; Grefenstette, E.; Kohli, P.; and Battaglia, P. 2019. Com-\npile: Compositional imitation learning and execution. In In-\nternational Conference on Machine Learning, 3418–3428.\nPMLR.\nKulh´anek, J.; Derner, E.; and Babuˇska, R. 2021. Visual nav-\nigation in real-world indoor environments using end-to-end\ndeep reinforcement learning. IEEE Robotics and Automa-\ntion Letters, 6(3): 4345–4352.\nLandajuela, M.; Petersen, B. K.; Kim, S.; Santiago, C. P.;\nGlatt, R.; Mundhenk, N.; Pettit, J. F.; and Faissol, D.\n\n2021. Discovering symbolic policies with deep reinforce-\nment learning.\nIn Meila, M.; and Zhang, T., eds., Pro-\nceedings of the 38th International Conference on Machine\nLearning, volume 139 of Proceedings of Machine Learning\nResearch, 5979–5989. PMLR.\nLee, Y.; Yang, J.; and Lim, J. J. 2019. Learning to coordi-\nnate manipulation skills via skill behavior diversification. In\nInternational conference on learning representations.\nLiu, G.; Schulte, O.; Zhu, W.; and Li, Q. 2019. Toward in-\nterpretable deep reinforcement learning with linear model\nu-trees.\nIn Machine Learning and Knowledge Discovery\nin Databases: European Conference, ECML PKDD 2018,\nDublin, Ireland, September 10–14, 2018, Proceedings, Part\nII 18, 414–429. Springer.\nLiu, G.; Sun, X.; Schulte, O.; and Poupart, P. 2021. Learn-\ning tree interpretation from object representation for deep\nreinforcement learning. In Advances in Neural Information\nProcessing Systems, volume 34, 19622–19636.\nLoh, W.-Y. 2011. Classification and regression trees. Wiley\ninterdisciplinary reviews: data mining and knowledge dis-\ncovery, 1(1): 14–23.\nLynch, C.; Khansari, M.; Xiao, T.; Kumar, V.; Tompson, J.;\nLevine, S.; and Sermanet, P. 2020. Learning latent plans\nfrom play.\nIn Conference on robot learning, 1113–1132.\nPMLR.\nLyu, D.; Yang, F.; Liu, B.; and Gustafson, S. 2019. SDRL:\ninterpretable and data-efficient deep reinforcement learning\nleveraging symbolic planning. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 33, 2970–\n2977.\nMa, Z.; Zhuang, Y.; Weng, P.; Zhuo, H. H.; Li, D.; Liu,\nW.; and Hao, J. 2021.\nLearning symbolic rules for in-\nterpretable deep reinforcement learning.\narXiv preprint\narXiv:2103.08228.\nMadumal, P.; Miller, T.; Sonenberg, L.; and Vetere, F. 2020.\nExplainable reinforcement learning through a causal lens.\nIn Proceedings of the AAAI conference on artificial intelli-\ngence, volume 34, 2493–2500.\nMees, O.; Hermann, L.; Rosete-Beas, E.; and Burgard, W.\n2022. Calvin: A benchmark for language-conditioned policy\nlearning for long-horizon robot manipulation tasks. IEEE\nRobotics and Automation Letters, 7(3): 7327–7334.\nMilani, S.; Topin, N.; Veloso, M.; and Fang, F. 2024. Ex-\nplainable reinforcement learning: A survey and comparative\nreview. ACM Computing Surveys, 56(7): 1–36.\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-\nness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidje-\nland, A. K.; Ostrovski, G.; et al. 2015. Human-level control\nthrough deep reinforcement learning.\nnature, 518(7540):\n529–533.\nMolnar, C.; Casalicchio, G.; and Bischl, B. 2020.\nInter-\npretable machine learning–a brief history, state-of-the-art\nand challenges. In Joint European conference on machine\nlearning and knowledge discovery in databases, 417–431.\nSpringer.\nOlson, M. L.; Khanna, R.; Neal, L.; Li, F.; and Wong, W.-K.\n2021. Counterfactual state explanations for reinforcement\nlearning agents via generative deep learning. Artificial Intel-\nligence, 295: 103455.\nOrfanos, S.; and Lelis, L. H. 2023.\nSynthesizing pro-\ngrammatic policies with actor-critic algorithms and relu net-\nworks. arXiv preprint arXiv:2308.02729.\nPertsch, K.; Lee, Y.; and Lim, J. 2021. Accelerating rein-\nforcement learning with learned skill priors. In Conference\non robot learning, 188–204. PMLR.\nPertsch, K.; Lee, Y.; Wu, Y.; and Lim, J. J. 2021.\nDemonstration-Guided\nReinforcement\nLearning\nwith\nLearned Skills.\nIn 5th Annual Conference on Robot\nLearning.\nQuinlan, J. R. 1993. C4. 5: Programs for Machine Learning.\nRajeswaran, A.; Lowrey, K.; Todorov, E. V.; and Kakade,\nS. M. 2017. Towards generalization and simplicity in contin-\nuous control. In Advances in neural information processing\nsystems, volume 30.\nShankar, T.; and Gupta, A. 2020. Learning robot skills with\ntemporal variational inference. In International Conference\non Machine Learning, 8624–8633. PMLR.\nShi, L. X.; Lim, J. J.; and Lee, Y. 2023. Skill-based Model-\nbased Reinforcement Learning.\nIn Conference on Robot\nLearning, 2262–2272. PMLR.\nShiarlis, K.; Wulfmeier, M.; Salter, S.; Whiteson, S.; and\nPosner, I. 2018. Taco: Learning task decomposition via tem-\nporal alignment for control. In International Conference on\nMachine Learning, 4654–4663. PMLR.\nShu, T.; Xiong, C.; and Socher, R. 2018. Hierarchical and\nInterpretable Skill Acquisition in Multi-task Reinforcement\nLearning. In International Conference on Learning Repre-\nsentations.\nSilva, A.; Gombolay, M.; Killian, T.; Jimenez, I.; and Son,\nS.-H. 2020. Optimization methods for interpretable differ-\nentiable decision trees applied to reinforcement learning. In\nInternational conference on artificial intelligence and statis-\ntics, 1855–1865. PMLR.\nSutton, R. S.; and Barto, A. G. 2018. Reinforcement Learn-\ning: An Introduction. MIT press.\nVan Den Oord, A.; Vinyals, O.; et al. 2017. Neural discrete\nrepresentation learning. In Advances in Neural Information\nProcessing Systems, volume 30.\nVasi´c, M.; Petrovi´c, A.; Wang, K.; Nikoli´c, M.; Singh, R.;\nand Khurshid, S. 2022. Mo¨ET: Mixture of Expert Trees and\nits application to verifiable reinforcement learning. Neural\nNetworks, 151: 34–47.\nWabartha, M.; and Pineau, J. 2023.\nPiecewise Linear\nParametrization of Policies: Towards Interpretable Deep\nReinforcement Learning.\nIn International Conference on\nLearning Representations.\nZhang, K.; Zhang, J.; Xu, P.-D.; Gao, T.; and Gao, D. W.\n2021. Explainable AI in deep reinforcement learning models\nfor power system emergency control. IEEE Transactions on\nComputational Social Systems, 9(2): 419–427.",
    "pdf_filename": "SkillTree_Explainable_Skill-Based_Deep_Reinforcement_Learning_for_Long-Horizon_Control_Tasks.pdf"
}