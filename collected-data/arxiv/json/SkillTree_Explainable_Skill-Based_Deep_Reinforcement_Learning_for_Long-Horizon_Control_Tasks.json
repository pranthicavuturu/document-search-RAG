{
    "title": "SkillTree: Explainable Skill-Based Deep Reinforcement Learning",
    "abstract": "trustfullyusethemintheircars. Deep reinforcement learning (DRL) has achieved remark- Explainable reinforcement learning (XRL) (Heuillet, able success in various domains, yet its reliance on neural Couthouis,andD´ıaz-Rodr´ıguez2021;Hicklingetal.2023; networks results in a lack of transparency, which limits its Milanietal.2024)aimstoenhancethetransparencyandex- practical applications in safety-critical and human-agent in- teractiondomains.Decisiontrees,knownfortheirnotableex- plainability of DRL models. XRL methods can be broadly plainability,haveemergedasapromisingalternativetoneu- categorized into inherently explainable models, where ex- ralnetworks.However,decisiontreesoftenstruggleinlong- plainability is induced in the model training process, and horizoncontinuouscontroltaskswithhigh-dimensionalob- post-hoc explanations, where models are explained after servation space due to their limited expressiveness. To ad- training. Existing works have shown that using a decision dress this challenge, we propose SkillTree, a novel hierar- tree(DT)(CostaandPedreira2023)astheunderlyingmodel chicalframeworkthatreducesthecomplexcontinuousaction iseffective(FrosstandHinton2017;Bastani,Pu,andSolar- spaceofchallengingcontroltasksintodiscreteskillspace.By Lezama2018;Dingetal.2020;Vasic´ etal.2022).InaDT, integrating the differentiable decision tree within the high- observationsareinputattherootnode,anddecisionsarede- level policy, SkillTree generates diecrete skill embeddings terminedbyconditionalbranchesleadingtotheleafnodes, thatguidelow-levelpolicyexecution.Furthermore,through distillation, we obtain a simplified decision tree model that whichprovidethefinaloutputs.Thissimplestructuregives improves performance while further reducing complexity. DTs high explainability, offering clear and straightforward Experiment results validate SkillTree’s effectiveness across decision paths. Unlike post-hoc methods that rely on ex- variousroboticmanipulationtasks,providingclearskill-level ternal algorithms to interpret black-box models, DT-based insightsintothedecision-makingprocess.Theproposedap- methodsembedtransparencydirectlywithinthemodel.The proach not only achieves performance comparable to neu- DT-basedXRLparadigmnotonlyenhancesclaritybutalso ralnetworkbasedmethodsincomplexlong-horizoncontrol simplifiesdebuggingandvalidation,astherationalebehind tasksbutalsosignificantlyenhancesthetransparencyandex- eachdecisionisexplicitlyencodedwithinthetree. plainabilityofthedecision-makingprocess. Despitetheiradvantages,DT-basedmethodsarenotsuit- able for the following challenging task settings. (a) Long- Introduction horizontasks:Thetemporally-extendeddecisionprocesses requirelargeandcomplextrees,whicharedifficulttoopti- Deep Reinforcement learning (DRL) has been shown as a mize(Liuetal.2021).(b)High-dimensionalstatespaces: powerful framework for tackling complex decision-making DTs often lack sufficient representational ability to effec- tasks,achievingremarkablesuccessinvariousdomainssuch tivelymanagehigh-dimensionalstatespaces,leadingtosub- as games (Mnih et al. 2015; BAAI 2023), robotic manip- optimal performance in these environments (Bastani, Pu, ulation (Akkaya et al. 2019; Jitosho et al. 2023), and vi- andSolar-Lezama2018;Dingetal.2020).(c)Continuous sualnavigation(Kulha´nek,Derner,andBabusˇka2021).De- actionspaces:Thelimitednumberofleafnodesconstrains spite these advancements, the black-box nature of neural DTs’abilitytoencodecontinuouscontrolpoliciesoptimally, networksposessignificantchallengesinunderstandingand particularly in complex robotic tasks. These limitations re- trustingtheirdecisionmakingprocesses.Thislackoftrans- stricttheapplicabilityofDT-basedRLmethodsincomplex parency is particularly concerning in safety-sensitive and environments,highlightingtheneedforapproachesthatcan human-agent interaction applications, where understanding managehigh-dimensionalspacesandlong-horizontasks. the rationale behind decisions is essential (Hickling et al. To address the limitations of traditional DTs in han- 2023).Forexample,intheautonomousdrivingdomain,only dling long-horizon, high-dimensional, and continuous con- *Correspondingauthor. trol tasks, we propose a novel framework called SkillTree. AAAI2025ConferenceSubmission SkillTree introduces the concept of skills, which represent 4202 voN 91 ]GL.sc[ 1v37121.1142:viXra",
    "body": "SkillTree: Explainable Skill-Based Deep Reinforcement Learning\nfor Long-Horizon Control Tasks\nYongyanWen1,SiyuanLi1*,RongchangZuo1,LeiYuan2,3,4,HangyuMao5,PengLiu1\n1FacultyofComputing,HarbinInstituteofTechnology\n2NationalKeyLaboratoryofNovelSoftwareTechnology,NanjingUniversity\n3SchoolofArtificialIntelligence,NanjingUniversity\n4PolixirTechnologies,5SenseTimeResearch\n23B903027@stu.hit.edu.cn,siyuanli@hit.edu.cn,2021110788@stu.hit.edu.cn,yuanl@lamda.nju.edu.cn,\nmaohangyu@sensetime.com,pengliu@hit.edu.cn\nAbstract ifhumanuserscanunderstandthedrivingpoliciescanthey\ntrustfullyusethemintheircars.\nDeep reinforcement learning (DRL) has achieved remark-\nExplainable reinforcement learning (XRL) (Heuillet,\nable success in various domains, yet its reliance on neural\nCouthouis,andD´ıaz-Rodr´ıguez2021;Hicklingetal.2023;\nnetworks results in a lack of transparency, which limits its\nMilanietal.2024)aimstoenhancethetransparencyandex-\npractical applications in safety-critical and human-agent in-\nteractiondomains.Decisiontrees,knownfortheirnotableex- plainability of DRL models. XRL methods can be broadly\nplainability,haveemergedasapromisingalternativetoneu- categorized into inherently explainable models, where ex-\nralnetworks.However,decisiontreesoftenstruggleinlong- plainability is induced in the model training process, and\nhorizoncontinuouscontroltaskswithhigh-dimensionalob- post-hoc explanations, where models are explained after\nservation space due to their limited expressiveness. To ad- training. Existing works have shown that using a decision\ndress this challenge, we propose SkillTree, a novel hierar- tree(DT)(CostaandPedreira2023)astheunderlyingmodel\nchicalframeworkthatreducesthecomplexcontinuousaction\niseffective(FrosstandHinton2017;Bastani,Pu,andSolar-\nspaceofchallengingcontroltasksintodiscreteskillspace.By\nLezama2018;Dingetal.2020;Vasic´ etal.2022).InaDT,\nintegrating the differentiable decision tree within the high-\nobservationsareinputattherootnode,anddecisionsarede-\nlevel policy, SkillTree generates diecrete skill embeddings\nterminedbyconditionalbranchesleadingtotheleafnodes, thatguidelow-levelpolicyexecution.Furthermore,through\ndistillation, we obtain a simplified decision tree model that whichprovidethefinaloutputs.Thissimplestructuregives\nimproves performance while further reducing complexity. DTs high explainability, offering clear and straightforward\nExperiment results validate SkillTree’s effectiveness across decision paths. Unlike post-hoc methods that rely on ex-\nvariousroboticmanipulationtasks,providingclearskill-level ternal algorithms to interpret black-box models, DT-based\ninsightsintothedecision-makingprocess.Theproposedap- methodsembedtransparencydirectlywithinthemodel.The\nproach not only achieves performance comparable to neu- DT-basedXRLparadigmnotonlyenhancesclaritybutalso\nralnetworkbasedmethodsincomplexlong-horizoncontrol\nsimplifiesdebuggingandvalidation,astherationalebehind\ntasksbutalsosignificantlyenhancesthetransparencyandex-\neachdecisionisexplicitlyencodedwithinthetree.\nplainabilityofthedecision-makingprocess.\nDespitetheiradvantages,DT-basedmethodsarenotsuit-\nable for the following challenging task settings. (a) Long-\nIntroduction horizontasks:Thetemporally-extendeddecisionprocesses\nrequirelargeandcomplextrees,whicharedifficulttoopti-\nDeep Reinforcement learning (DRL) has been shown as a\nmize(Liuetal.2021).(b)High-dimensionalstatespaces:\npowerful framework for tackling complex decision-making\nDTs often lack sufficient representational ability to effec-\ntasks,achievingremarkablesuccessinvariousdomainssuch\ntivelymanagehigh-dimensionalstatespaces,leadingtosub-\nas games (Mnih et al. 2015; BAAI 2023), robotic manip-\noptimal performance in these environments (Bastani, Pu,\nulation (Akkaya et al. 2019; Jitosho et al. 2023), and vi-\nandSolar-Lezama2018;Dingetal.2020).(c)Continuous\nsualnavigation(Kulha´nek,Derner,andBabusˇka2021).De-\nactionspaces:Thelimitednumberofleafnodesconstrains\nspite these advancements, the black-box nature of neural\nDTs’abilitytoencodecontinuouscontrolpoliciesoptimally,\nnetworksposessignificantchallengesinunderstandingand\nparticularly in complex robotic tasks. These limitations re-\ntrustingtheirdecisionmakingprocesses.Thislackoftrans-\nstricttheapplicabilityofDT-basedRLmethodsincomplex\nparency is particularly concerning in safety-sensitive and\nenvironments,highlightingtheneedforapproachesthatcan\nhuman-agent interaction applications, where understanding\nmanagehigh-dimensionalspacesandlong-horizontasks.\nthe rationale behind decisions is essential (Hickling et al.\nTo address the limitations of traditional DTs in han-\n2023).Forexample,intheautonomousdrivingdomain,only\ndling long-horizon, high-dimensional, and continuous con-\n*Correspondingauthor. trol tasks, we propose a novel framework called SkillTree.\nAAAI2025ConferenceSubmission SkillTree introduces the concept of skills, which represent\n4202\nvoN\n91\n]GL.sc[\n1v37121.1142:viXra\ntemporal abstractions of low-level actions in the context of et al. 2019; Olson et al. 2021) and Shapley value (Zhang\nRL. These skills simplify the decision-making process by etal.2021;Heuillet,Couthouis,andD´ıaz-Rodr´ıguez2022),\nbreakingdownlongtrajectoriesintomanageablesegments, which highlight the most influential input features. Model\nenabling the agent to plan at a higher level, thereby reduc- distillation approximates the black-box model, either lo-\ningthecomplexityassociatedwithextendedtasks.However, callyorglobally,withexplainablemodelslikedecisiontrees\nthe challenge of skill representation arises, as skills are of- (Bastani,Pu,andSolar-Lezama2018;Coppensetal.2019;\ntenrepresentedincontinuousspaces,leadingtodifficulties BewleyandLawry2021;OrfanosandLelis2023).Counter-\nin control and explainability. To overcome this, our frame- factualexplanationsgeneratealternativescenariosbymod-\nworkleverageshierarchicalstructuresanddiscreteskillrep- ifyinginputfeaturestoobservechangesinthemodel’sout-\nresentationlearning.Byregularizingtheskillspaceintodis- put, offering a way to understand the sensitivity of deci-\ncrete units, we simplify policy learning for skill selection sions(Madumaletal.2020;Olsonetal.2021).Somestudies\nandenhancetheexplainabilityofthelearnedskills.Specif- haveexploredidentifyingcriticalstatescrucialforachieving\nically,weemploydecisiontreestoindexandexecutethese thefinalrewardfromastate-rewardperspective(Guoetal.\ndiscreteskills,combiningtheinherentexplainabilityofdeci- 2021;Chengetal.2024).Additionally,researchershavean-\nsiontreeswiththeflexibilityofskill-basedrepresentations. alyzed the impact of different training samples on policy\nThisapproachprovidesarobustsolutionformanagingcom- training outcomes (Deshmukh et al. 2023). Lastly, rule ex-\nplex,long-horizondecisiontaskswhileofferingclear,skill- traction methods create human-readable rules that approx-\nlevelinsightsintothedecision-makingprocess.Wesumma- imate the model’s behavior and enhance the transparency\nrizethemaincontributionsofthispaperasfollows: of the decision-making process (Hein, Udluft, and Runkler\n1. We propose SkillTree, a novel hierarchical, skill-based 2018;Landajuelaetal.2021).\nmethod for explainable reinforcement learning. To the\nbestofourknowledge,itmarksthefirstsuccessfulappli- DecisionTree\ncation of DT-based explainable method in long-horizon\ncontinuouscontroltasks. Decision trees are widely used for their explainability and\nsimplicity, often serving as function approximators in rein-\n2. We introduce a method for discrete skill representation\nforcement learning. Classical decision tree algorithms like\nlearning, which effectively reduces the skill space and\nCART (Loh 2011) and C4.5 (Quinlan 1993) produce ex-\nimprovestheefficiencyofskill-basedpolicylearning.\nplainable surrogate policies but are limited in expressive-\n3. Experiment results across a variety of robotic manipu-\nness and impractical for integration into DRL models. Ap-\nlation tasks demonstrate that our method provides skill-\nproaches such as VIPER (Bastani, Pu, and Solar-Lezama\nlevel explanations while achieving performance compa-\n2018) attempt to distill neural network policies into veri-\nrabletoneuralnetworkbasedapproaches.\nfiable decision tree by imitation learning. Recent advance-\nments include rule-based node divisions (Dhebar and Deb\nRelatedWork\n2020) and parametric differentiable decision tree, such as\nExplainableRL soft decision tree (Frosst and Hinton 2017) and differen-\nRecently, explainability approaches in DRL have been tiable decision tree for approximating Q-function or pol-\nbroadlycategorizedintointrinsicandpost-hocexplanations, icy (Silva et al. 2020). While these methods improve ex-\nbased on the method of their generation. We briefly dis- pressiveness,theyaregenerallyconstrainedtosimpler,low-\ncussthesetwoclassesofmethodshere.Foramoredetailed dimensional environments. By contrast, our approach ad-\ntaxonomy and discussion of XRL, please refer to the fol- dressesthecomplexityofhigh-dimensionalcontinuouscon-\nlowing surveys: (Heuillet, Couthouis, and D´ıaz-Rodr´ıguez trol tasks by converting the action space into a skill space,\n2021;Hicklingetal.2023;Milanietal.2024). therebyreducingcomplexityandprovidingamoreeffective\nIntrinsicexplanationmethodsaredesignedwithexplain- andexplainablesolutionforchallengingtasks.\nabilityasafundamentalaspect,oftenincorporatingsimpler,\nmore transparent models such as decision trees (Liu et al. SkillsinReinforcementLearning\n2019;Silvaetal.2020;Dingetal.2020),linearmodels(Ra-\njeswaranetal.2017;Molnar,Casalicchio,andBischl2020; Recent works have increasingly focused on learning and\nWabartha and Pineau 2023) or symbolic rules (Lyu et al. utilizing skills to improve agent efficiency and generaliza-\n2019; Ma et al. 2021). By ensuring that the model itself tion (Shu, Xiong, and Socher 2018; Hausman et al. 2018;\nisexplainable,intrinsicmethodsofferreal-time,straightfor- Shankar and Gupta 2020; Lynch et al. 2020). Skills can be\nwardexplanationsoftheagent’sdecisions.Forinstance,us- extractedfromtheofflinedataset(Shiarlisetal.2018;Kipf\ning a decision tree as the policy model enables immediate etal.2019;Pertsch,Lee,andLim2021;Pertschetal.2021;\nandclearunderstandingofthedecision-makingprocess. Shi, Lim, and Lee 2023) or manually defined (Lee, Yang,\nPost-hoc methods apply explainability techniques after and Lim 2019; Dalal, Pathak, and Salakhutdinov 2021;\nthe RL model has been trained. These approaches do not BAAI2023).Typically,theseskillsarerepresentedashigh-\nalter the original model but instead seek to explain the dimensional,continuouslatentvariables,whichlimitstheir\ndecisions of more complex, often black-box models. Typ- explainability. In contrast, we propose extracting a discrete\nical post-hoc methods include feature attribution meth- skill representation from a task-agnostic dataset, making it\nods like saliency maps (Greydanus et al. 2018; Anderson suitableforDT-basedpolicylearning.\nPreliminary\nReinforcementLearning\nInthispaper,weareconcernedwithafiniteMarkovdecision\nprocess(SuttonandBarto2018),whichcanberepresented\nasatupleM =(S,A,R,P,p ,γ,H).WhereS isthestate\n0\nspace,Aistheactionspace,S×A(cid:55)→Ristherewardspace,\nP :S×A(cid:55)→∆(S)isthetransferfunction,p :∆(S)isthe\n0\ninitialstatedistribution,γ ∈(0,1)isthediscountfactor,and\nFigure1:Comparisonofthesoftdecisiontree(left)andthe\nH istheepisodelength.Thelearningobjectiveistofindthe\nharddecisiontree(right).\noptimalpolicyπmaximizingtheexpecteddiscountedreturn\n(cid:34)H−1 (cid:35)\n(cid:88) ExplainableRLwithSkillTree\nmaxJ(π)=maxE γtR(s ,a ) .\nπ\ns0∼p0,(s0,a0,...,sH)∼π t t\nOur goal is to leverage discrete skill representations to\nt=0\n(1) learn a skill decision tree, enabling skill-level explainabil-\nity. Overall, our approach is divided into three stages: (1)\nSoftDecisionTree extracting the discrete skill embeddings from the offline\ndataset,(2)traininganexplainableDT-basedskillpolicyby\nThedifferentiabledecisiontree(FrosstandHinton2017)is\nRLfordownstreamlong-horizontasks,and(3)distillingthe\na special type of decision tree that differs from the tradi-\ntrainedpolicyintoasimplifieddecisiontree.\ntionaloneslikeCART(Loh2011)byusingprobabilisticde-\ncision boundaries instead of deterministic boundaries. This\nLearningDiscreteSkillEmbeddings\nmodification increases the flexibility of model by allowing\nforsmoothertransitionsateachdecisionnode,asshownin Our goal is to obtain a fixed number of skill embeddings\nFigure1.Thedifferentiablesoftdecisiontreeisacomplete that contain a temporal abstraction of a sequence of ac-\nbinarytreeinwhicheachinnernodecanberepresentedbya tions. Skills represent action sequences of useful behav-\nlearnableweightωi andbiasϕi.Here,iandj representthe iors and are represented by D-dimensional vectors. We as-\nj j\nlayerindexandthenodeindexofthatlayer,respectively.By sume an existing task-agnostic dataset D = {τ i},τ i =\nusingthesigmoidfunctionσ(·),theprobabilityσ(ωx+ϕ) {s 0,a 0,...,s Hi,a Hi}, consists of d trajectories of varying\nrepresentsthetransferprobabilityfromthatnodetotheleft lengths, where each trajectory includes states s t,...,s Hi\nsubtree, and 1 − σ(ωx + ϕ) gives the transfer probability and corresponding actions a t,...,a Hi. To regularize the\ntotherightsubtree.Thisprocessallowsthedecisiontreeto skillembeddings,insteadoflearningwithinthecontinuous\nprovidea“soft”decisionateachnodeforthecurrentinput. skill space, we employ a skill table Z = {e 1,e 2,...,e K}\nConsider a decision tree with depth d. The nodes in the (i.e., codebook) that contains D-dimensional vectors, each\ntree are denoted by n , where u = 2i −1+j denotes the representingalearnableskillembedding.\nu\nnodeindex,andthedecisionpathP canberepresentedas Tolearnreusableskillsfromofflinedatasetsandguideex-\nplorationefficiently,wemodifiedVQ-VAE(VanDenOord,\nd−1 2i Vinyals et al. 2017) to learn skill representations and skill\n(cid:89)(cid:89)\nP =argmax pi→i+1 (2) prior,asillustratedinFigure2.First,ademonstrationtrajec-\n⌊j⌋→j\n{u} i=0j=0 2 toryisrandomlydividedintostate-actionsegmentsoffixed\nlength h during the training. Each segment contains h ob-\nwhere{u}denotesnodesonthedecisionpath,andpi→i+1 servation states s = s ,s ,...,s and the corre-\n⌊j⌋→j t,h t t+1 t+h−1\ndenotes the probability of moving from node n 2i+⌊j/2\n2⌋\nto sponding actions a t,h = a t,a t+1,...,a t+h−1. The input\nnoden 2i+1+j,theprobabilityiscalculatedas to the encoder q ϕ is s t,h and a t,h. The output of the en-\ncoderisaD-dimensionalembeddingz .Next,weselectthe\ne\n (cid:18) (cid:19) embedding from the codebook that is nearest to z and its\npi→i+1\n=σ ω ⌊i (cid:18)2j⌋x+ϕi\n⌊ 2j⌋\n(cid:19)\nifj mod 2=0,\nindex k = arg jmin∥z e − e j∥ 2, obtaining the\nrege\nularized\n⌊ 2j⌋→j\n1−σ ω ⌊i j⌋x+ϕi\n⌊j⌋\notherwise. embedding z q = e k. Finally, the low-level actual action a t\n2 2 isobtainedthroughthestate-conditioneddecoderπ l(s t,z q),\n(3) whichservesasthelow-levelpolicy.\nWithEquation2and3,thetreecanbetraverseddownto Inaddition,followingtheapproachof(Pertsch,Lee,and\ntheleafnodes.Leafnodesarerepresentedbyparametervec- Lim 2021), we introduce a skill prior to guide the high-\ntors w2d×K, where K is the number of output categories. level policy. The skill prior p(k |s ) predicts the skill dis-\nt t\nThe output of the leaf node k is a categorical distribution tribution given the first state of the sequence in the offline\ngiven by softmax(w ), which is independent of the input. data.Thisenhancesexplorationefficiencyduringhigh-level\nk\nThelearnableparametersofboththeleafnodesandthein- policy training by avoiding random skill sampling during\nnernodescanbeoptimizedusingexistingDRLalgorithms, warming up. Here, s represents the first state of the se-\nt\nmakingthesoftdecisiontreesuitableasanexplainableal- quence. The skill prior is a K-categorical distribution that\nternativemodelforDRLpolicy. matches the output of encoder. The objective of the skill\nNext State\nDecision Tree\nHigh-level Policy Reward\nEncoder\nSkill Prior\nEnvironment\nSkill Index\nCodebook\nDecoder Low-level Policy\nSkill Embedding Action\nDiscrete Skill Embedding Learning Downstream High-Level Policy Learning\nFigure2:Discreteskillembeddinglearninganddownstreamhigh-levelDTpolicylearning.Aftercompletingtheskilllearning,\nwefreezethedecoderandskillprior,andthenproceedtofinetunethecodebookduringthehigh-levelpolicylearning.\nprioristofittheoutputdistributionofencoderconditioned i.e.,z = Z[k ].Ineachsingletimestep,thelow-levelpol-\nt t\nsolely on the first state s . Since the encoder can only pro- icy predicts the output action conditioned on the state and\nt\nduceafixedskillindexk,theskillprioraimstomatchaone- skillembedding.Thelow-levelpolicyexecutesh-stepsand\nhot distribution of that index, aligning its predictions with thestatechangestos t+h.WeuseP+(s t+h|s t,z t)todenote\ntheencoder.Theoveralltrainingobjectiveis\nthestatetransitionafterhsteps(seeline3-6inAlgorithm1).\nNext,inordertoimprovethetrainingefficiencybyutilizing\nh−1 the prior information obtained from previous skill training,\n(cid:88)\nL= ∥π (s ,z )−a ∥2 weaddanadditionalpriortermasthelearningobjective:\nl t q t 2\nt=0\n+∥sg[z ]−e∥2+β∥z −sg[e]∥2\ne 2 e 2\n−(cid:88)K\nq ϕ(z =k|s t,h,a t,h)logp(k\nt\n=k|s t), (4) J(θ)=E\nπh(cid:34)H (cid:88)−1\nγtr+(s t,z t)−αD KL(π h(k t|s t)∥p(k t|s\nt))(cid:35)\n,\nt=0\nk=1 (6)\nwheresg[·]isthestopgradientoperator,p(k = k|s )isthe wherer+(s ,z ) = (cid:80)t+hR(s ,z )isthetotalsumofre-\nt t t t i=t i i\nprobability that k is category k and β is the hyperparam- wards, θ is the trainable parameter of policy, α is the hy-\nt\neter of the commitment loss term. Categorical distribution perparameterofdivergenceterm.Topreventthepolicyfrom\nq (z =k|s ,a )probabilitiesaredefinedasone-hotas overfittingtheprior,following(Pertsch,Lee,andLim2021),\nϕ t,h t,h\nαcanbeautomaticallytunedbydefiningatargetdivergence\n(cid:40)\n1 fork =argmin∥z e−e j∥ 2, δ(seeAlgorithm1line15).\nq (z =k|s ,a )= j\nϕ t,h t,h\n0 otherwise,\nTypically,thehigh-levelpolicyisimplementedasaneu-\n(5)\nral network, which is a black box and lacks transparency.\nwherez istheoutputoftheencoder.\ne Inordertoachieveanexplainabledecisionmakingprocess,\nweimplementthehigh-levelpolicyπ (k |s )usingadiffer-\nDownstreamRLPolicyLearningwithSkillPrior h t t\nentiable soft decision tree instead, which can be optimized\nTo reuse the learning skill embeddings and improve explo- usingexistingbackpropagationalgorithm.Furthermore,the\nration efficiency in RL training, we utilize the skill prior\nstructure of the skill prior is identical to that of the high-\nto regularize the high-level policy in skill space. After pre-\nlevel policy, both of which are implemented using soft de-\ntraining the low-level policy and skill prior, the low-level\ncisiontrees.Toimprovelearningefficiency,theparameters\npolicy π (s ,z ) and the skill prior p(k |s ) are fixed. The\nl t t t t\nhigh-level policy π (k |s ) is executed every h-steps, and of the skill prior are used to initialize the policy during the\nh t t\ntheoutputisthecategoricaldistributionoftheskillindexk . initialization of training. We modify SAC (Haarnoja et al.\nt\nToobtaintheactualskillembedding,wesampleanindexk 2018) algorithm to learn the objective 6 and the process is\nt\nfrom the distribution and then query the codebook with k , summarizedinAlgorithm1.SeeAppendixformoredetails.\nt\nAlgorithm1:SkillTreeRLTraining\nInputs: codebook Z, target divergence δ, learning rates\nλ ,λ ,λ ,targetupdaterateτ,DTskillpriorp(k |s )and\nθ ψ α t t\ndiscountfactorγ.\n1: InitializereplaybufferB,d-depthhigh-levelDTpolicy\nπ θ(k t|s t),criticQ ψ(s t,z t),targetnetworkQ ψ¯(s t,z t)\n2: foreachiterationdo\n3: foreveryhenvironmentstepsdo\n4: k t ∼π θ(k t|s t)\n5: z t =Z[k t]\n6: s t′ ∼P+(s t+h|s t,z t) (a) KitchenMKBL (b) KitchenMLSH\n7: B ←B∪{s t,z t,r t+,s t′}\n8: endfor\n9: foreachgradientstepdo\n10: k t′ ∼π θ(k t′|s t′)\n11: z t′ =Z[k t′]\n12: Q¯ =r t++γ(cid:2) Q (cid:2)ψ¯(s t′,z t′)+αD KL(π θ(k t′|s t′)∥p(k t′|s\nt\n(cid:3)′))(cid:3)\n13: θ←θ−λ θ∇\nθ\nQ ψ¯(s t,z t)−D KL(π θ(k t|s t)∥p(k t|s t))\n14: ψ ←ψ−λ ψ∇\nψ(cid:104) 21(cid:0)\nQ ψ(s t|z\nt)−Q¯(cid:1)2(cid:105)\n15: α←α−λ α∇ α[α(D KL(π θ(k t|s t)∥p(k t|s t))−δ)]\n16:\nψ¯←τψ+(1−τ)ψ¯\n17: endfor (c) CALVIN (d) Office\n18: endfor\n19: returnpolicyπ θ(k t|s t),finetunedcodebookZ Figure3:Fourlong-horizonsparserewardtaskstoevaluate.\n(a) The robotic arm has to finish four subtasks in the cor-\nrectorder,i.e.,Microwave-Kettle-BottomBurner-Light\nDistillingtheDecisionTree (MKBL).(b)Similarto(a),butwithdifferentsubtasks:Mi-\ncrowave-Ligt-SlideCabinet-HingeCabinet(MLSH).(c)\nAfter downstream RL training, we obtain a soft decision\nFinishsubtasksinthecorrectorder,i.e.,OpenDrawer-Turn\ntreewithskill-levelexplainability,whereeachdecisionnode\nonLightbulb-MoveSliderLeft-TurnonLED.(d)Intheof-\nprobabilistically selects child nodes until a skill choice is\nfice cleaning task, the robotic arm needs to pick up objects\nmade.Generally,modelswithfewerparametersareeasierto\nandplacethemincorrespondingcontainersinsequence.\nunderstandandaccept.Tofurthersimplifythemodelstruc-\nture, we distill the soft decision tree into a more straight-\nforward hard decision tree through discretization. Previous\ntaskinCALVIN(Meesetal.2022)andofficecleaningtask\nmethods select the feature with the highest weight at each\n(Pertschetal.2021),asillustratedinFigure3.\nnodebutitoftenobviouslydegradestreeperformance(Ding\netal.2020).Byframingtheproblemasanimitationlearning\nKitchen TheFrankaKitchenenvironmentisaroboticarm\ntask,weleveragethelearnedhigh-levelpolicyasanexpert\ncontrolled environment based on the mujoco implementa-\npolicy.Wesampletrajectoriesfromtheenvironmenttogen-\ntion.Theenvironmentcontainsa7-DOFroboticarmthatis\nerate the dataset. Given the simplified skill space, we em-\nable to interact with other objects in the kitchen, such as a\nploy a low-depth CART to classify state-skill index pairs.\nkettleandotherobjects.Atotalof7subtasksareincluded.\nThisapproacheffectivelymanagesthecomplexityofthetree\nTheobservationis60-dimensionalandcontainsinformation\nwhile preserving performance, ensuring that the model re-\naboutjointpositionsaswellastaskparameters,andtheac-\nmainsbothexplainableandefficient.\ntionspaceis9-dimensionalwithepisodelengthof280.The\noffset dataset contains 601 trajectories, and each trajectory\nExperiments completes a variety of tasks that may not be in the same\norder, but no more than 3 subtasks. We set two sets of dif-\nIn this section, we evaluate SkillTree’s effectiveness in\nferentsubtasks:MKBLandMLSH,asshowninFigure3(a)\nachieving a balance between explainability and perfor-\nand3(b).TheMLSHtaskismoredifficulttolearnduetothe\nmance. We demonstrate that our method not only provides\nverylowfrequencyofsubtasktransitionsinthedataset.\nclear,skill-levelexplanationsbutalsoachievesperformance\nonparwithneuralnetworkbasedapproaches.\nCALVIN In CALVIN, a 7-DOF Franka Emika Panda\nrobotic arm needs to be controlled to open a drawer, light\nTasksSetup\na light bulb, move a slider to the left and light an LED in\nToevaluatetheperformanceofourmethodinlong-horizon sequence.theobservationspaceis21-dimensionalandcon-\nsparse reward tasks, we chose the Franka Kitchen control tains both robotic arm states and object states with episode\ntasks in D4RL (Fu et al. 2020), robotic arm manipulation length of 360 steps. The dataset contains 1,239 trajectories\nFigure4:Downstreamtasklearningcurvesofbothourmethodandbaselines.Averagedover5independentruns.\nTable1:Averagecompletedsubtasks(ACS)andthenumberofleafnodes(Leaf)acrossmethodsanddomains.\nKitchenMKBL KitchenMLSH CALVIN Office\nMethod\nACS Leaf ACS Leaf ACS Leaf ACS Leaf\nSPiRL 3.08±0.46 N/A 2.82±1.08 N/A 2.77±1.46 N/A 1.48±0.96 N/A\nVQ-SPiRL 2.97±0.41 N/A 2.04±0.85 N/A 2.10±0.81 N/A 1.69±0.64 N/A\nBC+SAC 0.00±0.00 N/A 0.00±0.00 N/A 0.01±0.10 N/A 0.00±0.00 N/A\nCART 2.11±0.92 987 1.16±0.78 981 1.02±0.20 998 0.00±0.00 949\nSkillTree 2.90±0.42 64 2.58±0.80 64 1.90±0.90 64 2.00±0.00 64\nSkillTree(Distilling) 3.06±0.65 64 2.52±0.82 64 2.42±0.49 64 2.00±0.00 64\nSkillTree(DC+Distilling) 3.25±0.70 64 2.62±0.76 64 3.00±0.00 64 2.00±0.00 64\nandatotalof34subtasks.Sinceitcontainsalargenumberof • CART(Loh2011):ImitationlearningusingCARTwith\nsubtasks, the average frequency of transitions between any trainedSPiRLagentastheteacher.\nsubtasks in the dataset is very low, requiring more precise\nFor SkillTree, we set the depth of the tree to 6 in all do-\nskill selection, which poses a greater challenge during the\nmains.ThesizeofcodebookK is16forKitchenandOffice,\nlearningofthehigh-levelpolicy.\nand8forCALVIN.ForCART,wesample1,000trajectories\nOffice For the Office environment, a 5-DOF WidowX oneachdomaintotrainandsetthemaximumdepthto10.\nrobotic arm needs to be controlled to pick up multiple ob-\njectsandplacethemintheircorrespondingcontainerswith Results\nepisode length of 350. We utilized a dataset collected by FromthelearningcurvesshowninFigure4,itisevidentthat\n(Pertschetal.2021),whichincludes2,331trajectories.The our method demonstrates comparable learning efficiency\nstate space is 97-dimensional and the action space is 8- and asymptotic performance in all domains. This high-\ndimensional. Due to the freedom of object manipulation in lights the effectiveness of our discrete skill representation\nthisenvironment,thetaskismorechallengingtocomplete. in handling complex, long-horizon tasks. BC+SAC failed\nto complete any subtasks because of the sparse reward.\nBaselines In the CALVIN and Office domain, our method exhibits a\nmarkedimprovementinearlylearningefficiencycompared\nIn the experiments, we compare our proposed method\ntoSPiRLandperformscomparablytoVQ-SPiRL.Thisad-\nagainst several representative baselines to demonstrate its\nvantageunderscoresthebenefitofdiscreteskillsinenhanc-\neffectiveness and competitive performance with additional\ningexplorationefficiency,allowingourapproachtoquickly\nskill-levelexplainability.\nidentify and leverage useful skills for target task. Through\n• SPiRL(Pertsch,Lee,andLim2021):Learnscontinuous comparingtoVQ-SPiRL,wevalidatethatthedecisiontree\nskills embeddings and a skill prior, and guides a high- canmaintainperformancewhilereducingmodelcomplexity\nlevelpolicyusingthelearnedprior.Itservesasacompet- insimplifieddiscreteskillspace.\nitiveskill-basedRLalgorithmbutlacksexplainability. InTable1,weevaluatetheadditionalSkillTreeafterdis-\ntilling on sampled dataset. Specifically, we use two sam-\n• VQ-SPiRL:DiscretizestheskillsinSPiRLbutusesneu-\nplingmethods.Oneistosamplethetrajectoriesdirectlywith\nralnetworksasthehigh-levelpolicymodelstructure.\nSkillTree(distilling,D),andtheotheristocleanthetrajec-\n• Behavioral Cloning + Soft Actor-Critic (BC+SAC): toriesandkeeponlythosewithhigherrewards(dataclean-\nTrains a supervised behavioral cloning policy from the ing+distilling,DC+D).Indatacleaning,weselectthetra-\ndemonstrationdatasetandfinetunesitonthedownstream jectoriescompletedsubtasksgreaterthan2forKitchenand\ntask using Soft Actor-Critic (Haarnoja et al. 2018). It CALVIN,andgreaterthan1forOffice.Ineachenvironment,\nservesasageneralalgorithmwithoutleveragingskills. wesample1000trajectories.Tocontrolthecomplexity,we\nmicrowave kettle bottom burner light switch\n0\n1\n2\n3\n4\n5\n6\n7\nqpos_obj_10@0.01 8 9\n10\n11\n12\n13\nqpos_obj_2@-1.29 14\n≤ qpos_5@2.06 15\n0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627\nqpos_obj_2@-1.19 Time Step (× 10)\n>\nFigure 7: Skill index output visualization of an episode in\nqpos_5@1.31\nKitchenMKBLtask,whichfinishedall4subtasks.\nqpos_4@0.09\nqpos_obj_3@-0.00\nmakes splits based on one of the observed features. In the\n60-dimensionalobservationsoftheKitchendomain,thelast\n30 dimensions are task parameters that are independent of\nthe state and thus do not influence decision making. This\nFigure5:VisualizationoftheSkillTree(DC+D)withdepth propertyisreflectedinthelearnedtreestructurebecausethe\n3. qpos and qpos obj mean the position of the robotic decision nodes use only the features in the first 30 dimen-\narmandobjects,respectively.ndenotesthenumberofstate- sions(robotarmpositionandobjectposition)forsplitting.\nskillpairsinthedivideddecisionset. Toanalyzetheeffectsofeachskill,wealsoevaluatedthe\neffectiveness of different skills, as shown in Figure 6. We\nfixedtheskilloutputindividuallyandevaluatedthecomple-\n1.0\nmicrowave tionofsubtasks.Eachskilliscapableofcompletingdifferent\nkettle 0.8 subtaskswithvaryingfocus.SkillTreeselectstheappropri-\nbottom burner 0.6 ateskillbasedontheobservedstate,whicharethenexecuted\ntop burner bythelow-levelpolicy.Figure7illustratesthesequenceof\n0.4\nlight switch\nskillindiceschoseninoneepisode,duringwhichfoursub-\nslide cabinet 0.2 taskswerecompleted.Itcanbeseenthatthesameskillwas\nhinge cabinet\n0.0 repeatedly selected at consecutive time steps to complete\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 specific subtasks, indicating the reuse of particular skills.\nFigure 6: We fix the skill output of high-level policy and Moreover, repeated selection of the same skill indicates a\nevaluatethefocusonsubtasksofeachskillinKitchenMKBL relatively long-term and stable skill execution process. By\ntask. We set the number of skills K to 16, as shown in the isolating and testing each skill, we can clearly see which\nx-axis.Thecolormeansthesuccessrateofeachsubtask. skillsaremosteffectiveforparticularsubtasks.Thismakes\niteasiertounderstandtheroleandfunctionalityofeachskill\nwithinthedecisionmakingprocess.SeeAppendixformore\nsetthedepthofthetreeto6inalldomains.Weobservethat experimentresultsaswellasvisualizations.\nafterdatacleaninganddistillation,theperformanceofSkill-\nTreecanbefurtherenhanced.Inmostofthedomains,Skill- ConclusionsandLimitations\nTreeDC+Dachievesbestperformancethanotherbaselines.\nWeproposedanovelhierarchicalskill-basedexplainablere-\nThis improvement reflects the effectiveness of our method\ninforcementlearningframeworkdesignedtotacklethechal-\nin optimizing the skill representations, allowing it to adapt\nlengesoflong-hoziron decisionmakingincontinuous con-\nwell even in complex scenarios. Moreover, the distillation\ntrol tasks. By integrating discrete skill representations and\nprocesswithdatacleaningnotonlysimplifiesthemodelbut\nDT, our method offers a robust solution that enhances ex-\nalsoensuresthatitretainscriticaldecisionmakingcapabil-\nplainability without sacrificing performance. The proposed\nities, leading to better performance across different tasks.\napproach effectively regularizes the skill space, simplifies\nIn contrast, CART has worse performance despite having\npolicy learning, and leverages the transparency of the DT.\ndeeper layers and a much larger number of leaf nodes than\nExperimental results demonstrate that our method not only\nSkillTree.Hugenumberofleafnodes(∼1000)suggeststhat\nachieves competitive performance compared to neural net-\nitisdifficulttounderstandintuitively.\nworkbasedapproachesbutalsoprovidesvaluableskill-level\nexplanations.Thisworkrepresentsaimportantsteptowards\nExplainability\nmore transparent and explainable RL, paving the way for\nIn Figure 5, we visualize the final decision tree obtained their broader application in complex real-world scenarios\nafter distillation for the Kitchen MKBL task. This depth of where understanding the decisionmaking processis essen-\ndecision tree is only three but can complete an average of tial.Despitethepromisingresultsdemonstrated,severallim-\n3.03subtasks.Theveryfewparametersmakethedecision- itationsremain.Thelow-levelpolicyinSkillTreestilllacks\nmaking process clear and easy to understand. Each node explainabilityduetoitsrelianceonneuralnetworks,which\nxednI\nllikS\nlimitsfulltransparencyinthedecision-makingprocess.Ad- Frosst,N.;andHinton,G.2017. Distillinganeuralnetwork\nditionally, the current approach assumes an offline, task- intoasoftdecisiontree. arXivpreprintarXiv:1711.09784.\nagnostic dataset for skill learning, which may not always\nFu, J.; Kumar, A.; Nachum, O.; Tucker, G.; and Levine, S.\nbe available in practical scenarios. Future work could fo-\n2020. D4rl: Datasets for deep data-driven reinforcement\ncusondevelopingmethodstoenhancetheexplainabilityof\nlearning. arXivpreprintarXiv:2004.07219.\nlow-levelpolicies,possiblybyintegratingexplainablemod-\nGreydanus,S.;Koul,A.;Dodge,J.;andFern,A.2018. Vi-\nelsdirectlyintothelow-leveldecision-makingprocess.\nsualizing and understanding atari agents. In International\nReferences conferenceonmachinelearning,1792–1801.PMLR.\nGuo, W.; Wu, X.; Khan, U.; and Xing, X. 2021. Edge:\nAkkaya, I.; Andrychowicz, M.; Chociej, M.; Litwin, M.;\nExplaining deep reinforcement learning policies. In Ad-\nMcGrew, B.; Petron, A.; Paino, A.; Plappert, M.; Powell,\nvances in Neural Information Processing Systems, vol-\nG.;Ribas,R.;etal.2019. Solvingrubik’scubewitharobot\nume34,12222–12236.\nhand. arXivpreprintarXiv:1910.07113.\nAnderson, A.; Dodge, J.; Sadarangani, A.; Juozapaitis, Z.; Haarnoja, T.; Zhou, A.; Abbeel, P.; and Levine, S. 2018.\nNewman, E.; Irvine, J.; Chattopadhyay, S.; Fern, A.; and Soft Actor-Critic: Off-Policy Maximum Entropy Deep Re-\nBurnett, M. 2019. Explaining reinforcement learning to inforcementLearningwithaStochasticActor. InDy,J.;and\nmere mortals: an empirical study. In Proceedings of the Krause,A.,eds.,Proceedingsofthe35thInternationalCon-\n28th International Joint Conference on Artificial Intelli- ferenceonMachineLearning,volume80ofProceedingsof\ngence,1328–1334. MachineLearningResearch,1861–1870.PMLR.\nBAAI, P. 2023. Plan4mc: Skill reinforcement learning and Hausman,K.;Springenberg,J.T.;Wang,Z.;Heess,N.;and\nplanning for open-world minecraft tasks. arXiv preprint Riedmiller, M. 2018. Learning an embedding space for\narXiv:2303.16563. transferable robot skills. In International Conference on\nBastani,O.;Pu,Y.;andSolar-Lezama,A.2018. Verifiable LearningRepresentations.\nreinforcementlearningviapolicyextraction. InAdvancesin Hein,D.;Udluft,S.;andRunkler,T.A.2018. Interpretable\nNeuralInformationProcessingSystems,volume31. policiesforreinforcementlearningbygeneticprogramming.\nBewley, T.; and Lawry, J. 2021. Tripletree: A versatile in- EngineeringApplicationsofArtificialIntelligence,76:158–\nterpretablerepresentationofblackboxagentsandtheiren- 169.\nvironments. InProceedingsoftheAAAIConferenceonAr-\nHeuillet, A.; Couthouis, F.; and D´ıaz-Rodr´ıguez, N. 2021.\ntificialIntelligence,volume35,11415–11422.\nExplainabilityindeepreinforcementlearning. Knowledge-\nCheng,Z.;Wu,X.;Yu,J.;Sun,W.;Guo,W.;andXing,X. BasedSystems,214:106685.\n2024. Statemask: Explaining deep reinforcement learning\nHeuillet, A.; Couthouis, F.; and D´ıaz-Rodr´ıguez, N. 2022.\nthroughstatemask. InAdvancesinNeuralInformationPro-\nCollectiveexplainableAI:Explainingcooperativestrategies\ncessingSystems,volume36.\nandagentcontributioninmultiagentreinforcementlearning\nCoppens, Y.; Efthymiadis, K.; Lenaerts, T.; Nowe´, A.; withshapleyvalues.IEEEComputationalIntelligenceMag-\nMiller, T.; Weber, R.; and Magazzeni, D. 2019. Distilling azine,17(1):59–71.\ndeep reinforcement learning policies in soft decision trees.\nHickling,T.;Zenati,A.;Aouf,N.;andSpencer,P.2023.Ex-\nInProceedingsoftheIJCAI2019workshoponexplainable\nplainability in deep reinforcement learning: A review into\nartificialintelligence,1–6.\ncurrentmethodsandapplications.ACMComputingSurveys,\nCosta, V. G.; and Pedreira, C. E. 2023. Recent advances\n56(5):1–35.\nindecisiontrees:Anupdatedsurvey. ArtificialIntelligence\nReview,56(5):4765–4800. Jitosho,R.;Lum,T.G.W.;Okamura,A.;andLiu,K.2023.\nReinforcement Learning Enables Real-Time Planning and\nDalal,M.;Pathak,D.;andSalakhutdinov,R.R.2021. Ac-\nControlofAgileManeuversforSoftRobotArms.InTan,J.;\nceleratingroboticreinforcementlearningviaparameterized\nToussaint,M.;andDarvish,K.,eds.,ProceedingsofThe7th\naction primitives. In Advances in Neural Information Pro-\nConferenceonRobotLearning,volume229ofProceedings\ncessingSystems,volume34,21847–21859.\nofMachineLearningResearch,1131–1153.PMLR.\nDeshmukh, S. V.; Dasgupta, A.; Krishnamurthy, B.; Jiang,\nKipf, T.; Li, Y.; Dai, H.; Zambaldi, V.; Sanchez-Gonzalez,\nN.; Agarwal, C.; Theocharous, G.; and Subramanian, J.\n2023. Explaining RL Decisions with Trajectories. In In- A.;Grefenstette,E.;Kohli,P.;andBattaglia,P.2019. Com-\nternationalConferenceonLearningRepresentations. pile:Compositionalimitationlearningandexecution. InIn-\nternational Conference on Machine Learning, 3418–3428.\nDhebar,Y.;andDeb,K.2020. Interpretablerulediscovery\nPMLR.\nthroughbileveloptimizationofsplit-rulesofnonlineardeci-\nsiontreesforclassificationproblems. IEEETransactionson Kulha´nek,J.;Derner,E.;andBabusˇka,R.2021. Visualnav-\nCybernetics,51(11):5573–5584. igationinreal-worldindoorenvironmentsusingend-to-end\ndeep reinforcement learning. IEEE Robotics and Automa-\nDing, Z.; Hernandez-Leal, P.; Ding, G. W.; Li, C.;\ntionLetters,6(3):4345–4352.\nand Huang, R. 2020. Cdt: Cascading decision trees\nfor explainable reinforcement learning. arXiv preprint Landajuela, M.; Petersen, B. K.; Kim, S.; Santiago, C. P.;\narXiv:2011.07553. Glatt, R.; Mundhenk, N.; Pettit, J. F.; and Faissol, D.\n2021. Discovering symbolic policies with deep reinforce- Olson,M.L.;Khanna,R.;Neal,L.;Li,F.;andWong,W.-K.\nment learning. In Meila, M.; and Zhang, T., eds., Pro- 2021. Counterfactual state explanations for reinforcement\nceedings of the 38th International Conference on Machine learningagentsviagenerativedeeplearning.ArtificialIntel-\nLearning,volume139ofProceedingsofMachineLearning ligence,295:103455.\nResearch,5979–5989.PMLR. Orfanos, S.; and Lelis, L. H. 2023. Synthesizing pro-\nLee, Y.; Yang, J.; and Lim, J. J. 2019. Learning to coordi- grammaticpolicieswithactor-criticalgorithmsandrelunet-\nnatemanipulationskillsviaskillbehaviordiversification. In works. arXivpreprintarXiv:2308.02729.\nInternationalconferenceonlearningrepresentations. Pertsch, K.; Lee, Y.; and Lim, J. 2021. Accelerating rein-\nLiu,G.;Schulte,O.;Zhu,W.;andLi,Q.2019. Towardin- forcementlearningwithlearnedskillpriors. InConference\nterpretable deep reinforcement learning with linear model onrobotlearning,188–204.PMLR.\nu-trees. In Machine Learning and Knowledge Discovery Pertsch, K.; Lee, Y.; Wu, Y.; and Lim, J. J. 2021.\nin Databases: European Conference, ECML PKDD 2018, Demonstration-Guided Reinforcement Learning with\nDublin,Ireland,September10–14,2018,Proceedings,Part Learned Skills. In 5th Annual Conference on Robot\nII18,414–429.Springer. Learning.\nLiu,G.;Sun,X.;Schulte,O.;andPoupart,P.2021. Learn- Quinlan,J.R.1993. C4.5:ProgramsforMachineLearning.\ning tree interpretation from object representation for deep Rajeswaran, A.; Lowrey, K.; Todorov, E. V.; and Kakade,\nreinforcementlearning. InAdvancesinNeuralInformation S.M.2017.Towardsgeneralizationandsimplicityincontin-\nProcessingSystems,volume34,19622–19636. uouscontrol. InAdvancesinneuralinformationprocessing\nLoh,W.-Y.2011. Classificationandregressiontrees. Wiley systems,volume30.\ninterdisciplinary reviews: data mining and knowledge dis- Shankar,T.;andGupta,A.2020. Learningrobotskillswith\ncovery,1(1):14–23. temporalvariationalinference. InInternationalConference\nonMachineLearning,8624–8633.PMLR.\nLynch,C.;Khansari,M.;Xiao,T.;Kumar,V.;Tompson,J.;\nLevine, S.; and Sermanet, P. 2020. Learning latent plans Shi,L.X.;Lim,J.J.;andLee,Y.2023. Skill-basedModel-\nfrom play. In Conference on robot learning, 1113–1132. based Reinforcement Learning. In Conference on Robot\nPMLR. Learning,2262–2272.PMLR.\nShiarlis, K.; Wulfmeier, M.; Salter, S.; Whiteson, S.; and\nLyu,D.;Yang,F.;Liu,B.;andGustafson,S.2019. SDRL:\nPosner,I.2018.Taco:Learningtaskdecompositionviatem-\ninterpretableanddata-efficientdeepreinforcementlearning\nleveraging symbolic planning. In Proceedings of the AAAI poralalignmentforcontrol. InInternationalConferenceon\nConference on Artificial Intelligence, volume 33, 2970–\nMachineLearning,4654–4663.PMLR.\n2977. Shu, T.; Xiong, C.; and Socher, R. 2018. Hierarchical and\nInterpretableSkillAcquisitioninMulti-taskReinforcement\nMa, Z.; Zhuang, Y.; Weng, P.; Zhuo, H. H.; Li, D.; Liu,\nLearning. In International Conference on Learning Repre-\nW.; and Hao, J. 2021. Learning symbolic rules for in-\nsentations.\nterpretable deep reinforcement learning. arXiv preprint\narXiv:2103.08228. Silva, A.; Gombolay, M.; Killian, T.; Jimenez, I.; and Son,\nS.-H. 2020. Optimization methods for interpretable differ-\nMadumal,P.;Miller,T.;Sonenberg,L.;andVetere,F.2020.\nentiabledecisiontreesappliedtoreinforcementlearning. In\nExplainable reinforcement learning through a causal lens.\nInternationalconferenceonartificialintelligenceandstatis-\nIn Proceedings of the AAAI conference on artificial intelli-\ntics,1855–1865.PMLR.\ngence,volume34,2493–2500.\nSutton,R.S.;andBarto,A.G.2018. ReinforcementLearn-\nMees, O.; Hermann, L.; Rosete-Beas, E.; and Burgard, W.\ning:AnIntroduction. MITpress.\n2022.Calvin:Abenchmarkforlanguage-conditionedpolicy\nVanDenOord,A.;Vinyals,O.;etal.2017. Neuraldiscrete\nlearning for long-horizon robot manipulation tasks. IEEE\nrepresentationlearning. InAdvancesinNeuralInformation\nRoboticsandAutomationLetters,7(3):7327–7334.\nProcessingSystems,volume30.\nMilani, S.; Topin, N.; Veloso, M.; and Fang, F. 2024. Ex-\nVasic´, M.; Petrovic´, A.; Wang, K.; Nikolic´, M.; Singh, R.;\nplainablereinforcementlearning:Asurveyandcomparative andKhurshid,S.2022. MoE¨T:MixtureofExpertTreesand\nreview. ACMComputingSurveys,56(7):1–36.\nits application to verifiable reinforcement learning. Neural\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve- Networks,151:34–47.\nness,J.;Bellemare,M.G.;Graves,A.;Riedmiller,M.;Fidje-\nWabartha, M.; and Pineau, J. 2023. Piecewise Linear\nland,A.K.;Ostrovski,G.;etal.2015. Human-levelcontrol\nParametrization of Policies: Towards Interpretable Deep\nthrough deep reinforcement learning. nature, 518(7540):\nReinforcement Learning. In International Conference on\n529–533.\nLearningRepresentations.\nMolnar, C.; Casalicchio, G.; and Bischl, B. 2020. Inter- Zhang, K.; Zhang, J.; Xu, P.-D.; Gao, T.; and Gao, D. W.\npretable machine learning–a brief history, state-of-the-art 2021.ExplainableAIindeepreinforcementlearningmodels\nand challenges. In Joint European conference on machine forpowersystememergencycontrol. IEEETransactionson\nlearning and knowledge discovery in databases, 417–431. ComputationalSocialSystems,9(2):419–427.\nSpringer.",
    "pdf_filename": "SkillTree_Explainable_Skill-Based_Deep_Reinforcement_Learning_for_Long-Horizon_Control_Tasks.pdf"
}