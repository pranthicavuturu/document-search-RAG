{
    "title": "Distill the Best, Ignore the Rest: Improving Dataset Distillation with",
    "abstract": "Datasetdistillationhasgainedsignificantinterestinre- Dataset Pre-Selects centyears,yetexistingapproachestypicallydistillfromthe Distillation relevant Samples entiredataset,potentiallyincludingnon-beneficialsamples. relevant Weintroduceanovel“PruneFirst,DistillAfter”framework samples thatsystematicallyprunesdatasetsvialoss-basedsampling irrelevant Ours Pre-Selects samples relevant Samples priortodistillation. Byleveragingpruningbeforeclassical distillation techniques and generative priors, we create a Dataset representative core-setthat leads toenhancedgeneraliza- Pruning Distillation tion for unseen architectures - a significant challenge of currentdistillationmethods. Morespecifically,ourproposed Full Distilled Core-Set frameworksignificantlyboostsdistilledquality, achieving Dataset Dataset uptoa5.2percentagepointsaccuracyincreaseevenwith Figure1.Comparisonofclassicalandourproposed“PruneFirst, substantialdatasetpruning,i.e.,removing80%oftheorigi- DistillAfter”pipeline:Traditionaldatasetdistillationoperateson naldatasetpriortodistillation. Overall,ourexperimental thefulldataset,whichincludesbothrelevantandirrelevantsamples. resultshighlighttheadvantagesofoureasy-sampleprioriti- OurPrune-Distillapproachpre-selectsacore-setbypruningloss- zationandcross-architecturerobustness,pavingthewayfor value-basedirrelevantsamples,focusingdistillationonthemost moreeffectiveandhigh-qualitydatasetdistillation. informativesubset,resultinginamorerefineddistilleddataset. 1.Introduction tencyacrossarangeofunseenarchitectures. Large-scaledatasetsarecrucialfortraininghigh-quality Inspiredbypreviousworkondatasetpruningforvarious machinelearningmodelsacrossvariousapplications[10,20]. computervisiontasks[1,4,7,15,16],weexploretheinterplay However,thesheervolumeofdatabringssignificantcom- betweendatasetpruninganddatasetdistillation,proposing putationalandstoragechallenges,makingefficientdataset anovelapproachthatsystematicallyprunessamplesprior distillationmethodshighlydesirable[2,5]. Datasetdistilla- todistillation. AsshowninFigure1, ourmethodenables tionaimstocompresstheselargedatasetsintosmaller,syn- thecreationofcompactyethighlyrepresentativecore-sets, theticsubsetswhilepreservingtrainingquality,yetexisting a subset of the original dataset, through targeted pruning techniquesoftenfallshortinachievingcross-architecturero- thatenhanceperformanceandstabilityacrossdiverseand bustness[17]. Classifiersgenerallyperformbestwhentheir unseen architectures. To realize this, we introduce a loss- architecture matches the one used during distillation, but value-basedsamplingstrategythatleveragesapre-trained performancedegradessignificantlywhentrainedonother classifier model to rank data samples by their “classifica- architectures[3,17]. Thischallengeiscompoundedbythe tiondifficulty”,helpingtocapturethekeycharacteristicsof retention of noisy samples, which dilutes the core repre- eachclass. Thisanalysiscombinestwosamplingstrategies: sentationalvalueofdistilleddata. Addressingtheseissues ascending(startingwithsimplersamples)anddescending requiresnotonlycompactandrepresentativedatabutalsoa (starting with complex samples), allowing us to examine selectivesamplingapproachtoenhanceperformanceconsis- theirrespectiveeffectsondistillationquality. Asaresult,we 1 4202 voN 81 ]VC.sc[ 1v51121.1142:viXra",
    "body": "Distill the Best, Ignore the Rest: Improving Dataset Distillation with\nLoss-Value-Based Pruning\nBrianB.Moser1,2,FedericoRaue1,TobiasC.Nauen1,2,StanislavFrolov1,2,AndreasDengel1,2\n1GermanResearchCenterforArtificialIntelligence\n2UniversityofKaiserslautern-Landau\nfirst.last@dfki.de\nAbstract Classical Distillation Pipeline\nDatasetdistillationhasgainedsignificantinterestinre- Dataset Pre-Selects\ncentyears,yetexistingapproachestypicallydistillfromthe Distillation relevant Samples\nentiredataset,potentiallyincludingnon-beneficialsamples. relevant\nWeintroduceanovel“PruneFirst,DistillAfter”framework samples\nthatsystematicallyprunesdatasetsvialoss-basedsampling irrelevant Ours Pre-Selects\nsamples relevant Samples\npriortodistillation. Byleveragingpruningbeforeclassical\ndistillation techniques and generative priors, we create a\nDataset\nrepresentative core-setthat leads toenhancedgeneraliza- Pruning\nDistillation\ntion for unseen architectures - a significant challenge of\ncurrentdistillationmethods. Morespecifically,ourproposed Full Distilled\nCore-Set\nframeworksignificantlyboostsdistilledquality, achieving Dataset Dataset\nuptoa5.2percentagepointsaccuracyincreaseevenwith\nFigure1.Comparisonofclassicalandourproposed“PruneFirst,\nsubstantialdatasetpruning,i.e.,removing80%oftheorigi-\nDistillAfter”pipeline:Traditionaldatasetdistillationoperateson\nnaldatasetpriortodistillation. Overall,ourexperimental\nthefulldataset,whichincludesbothrelevantandirrelevantsamples.\nresultshighlighttheadvantagesofoureasy-sampleprioriti-\nOurPrune-Distillapproachpre-selectsacore-setbypruningloss-\nzationandcross-architecturerobustness,pavingthewayfor\nvalue-basedirrelevantsamples,focusingdistillationonthemost\nmoreeffectiveandhigh-qualitydatasetdistillation.\ninformativesubset,resultinginamorerefineddistilleddataset.\n1.Introduction\ntencyacrossarangeofunseenarchitectures.\nLarge-scaledatasetsarecrucialfortraininghigh-quality Inspiredbypreviousworkondatasetpruningforvarious\nmachinelearningmodelsacrossvariousapplications[10,20]. computervisiontasks[1,4,7,15,16],weexploretheinterplay\nHowever,thesheervolumeofdatabringssignificantcom- betweendatasetpruninganddatasetdistillation,proposing\nputationalandstoragechallenges,makingefficientdataset anovelapproachthatsystematicallyprunessamplesprior\ndistillationmethodshighlydesirable[2,5]. Datasetdistilla- todistillation. AsshowninFigure1, ourmethodenables\ntionaimstocompresstheselargedatasetsintosmaller,syn- thecreationofcompactyethighlyrepresentativecore-sets,\ntheticsubsetswhilepreservingtrainingquality,yetexisting a subset of the original dataset, through targeted pruning\ntechniquesoftenfallshortinachievingcross-architecturero- thatenhanceperformanceandstabilityacrossdiverseand\nbustness[17]. Classifiersgenerallyperformbestwhentheir unseen architectures. To realize this, we introduce a loss-\narchitecture matches the one used during distillation, but value-basedsamplingstrategythatleveragesapre-trained\nperformancedegradessignificantlywhentrainedonother classifier model to rank data samples by their “classifica-\narchitectures[3,17]. Thischallengeiscompoundedbythe tiondifficulty”,helpingtocapturethekeycharacteristicsof\nretention of noisy samples, which dilutes the core repre- eachclass. Thisanalysiscombinestwosamplingstrategies:\nsentationalvalueofdistilleddata. Addressingtheseissues ascending(startingwithsimplersamples)anddescending\nrequiresnotonlycompactandrepresentativedatabutalsoa (starting with complex samples), allowing us to examine\nselectivesamplingapproachtoenhanceperformanceconsis- theirrespectiveeffectsondistillationquality. Asaresult,we\n1\n4202\nvoN\n81\n]VC.sc[\n1v51121.1142:viXra\nfindthatfocusingexclusivelyonsimplersamplesyieldsa Dataset Condensation (DC) aligns gradients by mini-\nsignificantdistillationimprovement. mizingthedifferencebetweenthegradientsonthesynthetic\nWebuildonrecentadvancementsindatasetdistillation andrealdatasets. Thisisexpressedas\nthatincorporategenerativepriors,exploitingbothStyleGAN-\nXL[3,21]andmodifieddiffusionmodels[17,20]toverify\nL (S,T)=1−\n∇ θℓS(θ)·∇ θℓT(θ)\n. (2)\nourobservations. Throughextensiveexperimentsonsubsets DC ∥∇ θℓS(θ)∥∥∇ θℓT(θ)∥\nofImageNet,wesystematicallyevaluatetheeffectofpruning Distribution Matching (DM) enforces similar feature\nanddistillationstrategiesacrossdiversearchitectures. Asa representationsforrealandsyntheticdatabyaligningthe\nresult,weachievesignificantperformanceboosts,sometimes featuredistributionacrossclasses:\nby up to ca. 17% or 5.2 p.p., on distilled datasets even\n(cid:13) (cid:13)2\nwithsubstantialpruningfactors(i.e.,removing80%ofthe L (S,T)=(cid:88)(cid:13) (cid:13) 1 (cid:88) ψ(x)− 1 (cid:88) ψ(s)(cid:13) (cid:13) , (3)\noriginaldatasetpriortodistillation). DM (cid:13) (cid:13)|T c| |S c| (cid:13) (cid:13)\nc x∈Tc s∈Sc\nOurcontributionscanbesummarizedasfollows:\nwhere T and S represent real and synthetic samples for\nc c\n• APrune-First,Distill-AfterFramework: Weproposea eachclassc.\nnovelapproachthatintegratesdatasetpruningasapre- MatchingTrainingTrajectories(MTT)minimizesthe\nprocessingstepfordatasetdistillation,whichimproves distancebetweenparametertrajectoriesofnetworkstrained\nthequalityofthedistilleddataset. onrealandsyntheticdata. Usingseveralmodelinstances,\nMTT saves the training path of model parameters θ∗T at\n• Loss-Value-Based Sampling Strategy: We introduce t0\nregularintervals,calledexperttrajectories. Fordistillation,\na sampling mechanism based on loss values from a itinitializesanetwork,θˆ withN steps,onthesynthetic\npre-trainedclassifier,guidingtheselectionofthemost t+N\ndata,trackingitstrajectory,andminimizesthedistancefrom\ninformativesamplesandenablingclass-balancedcore-\ntherealdatatrajectory,θ∗ withM steps:\nsetcreation. t+M\n∥θˆ −θ∗ ∥2\n• AnalysisofPruningStrategies: Wecompareascending L (S,T)= t+N t+M . (4)\nMTT ∥θ∗−θ∗ ∥2\nanddescendingsamplingmethodstodeterminetheir t t+M\nimpactondistillationquality,revealingthebenefitsof\n2.2.DatasetDistillationwithGenerativePrior\nprioritizingsimplersamplesinthecore-set.\nIndatasetdistillationwithagenerativeprior,apre-trained\n• ExtensiveCross-ArchitectureEvaluation: Wevalidate\ngenerativemodel isused to synthesizelatent codesrather\nourapproachonmultipleImageNetsubsetsanddiverse\nthanrawpixelvalues[3]. Incorporatinggenerativepriors\narchitectures,demonstratingtherobustnessandflexibil-\nintodatasetdistillationoffersseveraladvantages,primarily\nityofourprunedanddistilleddatasetsacrossvarious\nbycompressinginformativefeaturesintoamorestructured\nmodelarchitectures.\nlatentspace. Thistransformationenablesgreaterflexibility,\naslatentcodesareeasiertomanipulateandadaptcompared\n2.Background\nto pixel-level data. Specifically, let D : RM×h×w×d →\nRM×H×W×C denotethegenerativemodel,whereh·w ≪\nThissectionintroducesthekeyconceptsofdatasetdistil-\nH ·W. Thistransformsthedistillationobjectiveasfollows:\nlationalongsidetheirfusionwithgenerativepriors.\n2.1.DatasetDistillation Z∗ =argminL(D(Z),T), (5)\nZ\nLet T = (X real,Y real) represent a real dataset with where L is the distillation objective defined by the used\nX real ∈ RN×H×W×C, where N denotes the total num- algorithm(seesubsection2.1). GLaD[3],oneoftheinitial\nber of samples. The objective of dataset distillation is to methodstoleveragegenerativepriors,employsapre-trained\ncompressT intoasmallersyntheticsetS =(X syn,Y syn), StyleGAN-XL[21]. LD3M[17]extendsGLaDbyreplacing\nwithX syn ∈RM×H×W×C,whereM =C·IPC,C repre- the StyleGAN-XL with a modified diffusion model, i.e.,\nsentsthenumberofclasses,andIPC theimagesperclass. StableDiffusion[20],tailoredfordatasetdistillation.\nOurgoalistoachieveM ≪N byoptimizing\n2.3.Core-setSelectioninDistilledDatasets\nS∗ =argminL(S,T), (1)\nS Combining distilled datasets with core-set approaches\nwhere L represents the distillation objective, which is de- hasalreadybeenproposedforsmall-scaledatasets,suchas\nfinedbytheapplieddatasetdistillationmethod,e.g.,Dataset CIFAR100andTinyImageNet. Heetal. [11]suggestedap-\nCondensation(DC)[25],DistributionMatching(DM)[24], plyingcore-setselectiondirectlytothedistilleddatasetusing\nandMatchingTrainingTrajectories(MTT)[2]. twospecificrules. Thefirstruleselectsimagesthatareeasy\n2\nAlgorithm1Loss-Value-BasedSampling\ntraditional\nInput: realdatasetT = (X ,Y ),pre-trainedclassi- Apply Dataset\nreal real\nDistillation\nfierM ,lossfunctionL,modem(“easy”or“hard”),and\nθ\nours Algorithm\npruningratior.\n1: //initialization\nClassifier\n2: LossValues=dict()\n3: for eachuniquelabely∈SET(Y real)do\n4: LossValues[y]=list()\n5: DataPairs[y]=list()\n6: endfor\nScores Distilled\n7:\nsort and Core- Dataset\n8: //calculatelosses select top r Set\n9: for eachsamplepair(x i,y i)∈T do\n10: LossValues[y i].append(L(M θ(x i),y i))\nFigure2. Comparisonoftraditionaldatasetdistillationwithour\n11: DataPairs[y i].append((x i,y i))\nproposed“PruneFirst,DistillAfter”pipeline.Ratherthandistilling\n12: endfor thefulldataset,wefirstuseaclassifiertorankandselectthetop-r\n13: samplestocreateacore-set,whichisthendistilledtoproducea\n14: //derivecore-set moreexpressiveandarchitecture-robustdistilleddataset.\n15: π r =list()\n16: for eachuniquelabely∈SET(Y real)do\n17: if(m==“easy”)then\n18: indices=argsort(LossValues[y],“asc”) datasetpruningasacrucialpre-processingstepforeffective\n19: else datasetdistillation. Inthissection,weintroducetheconcept\n20: indices=argsort(LossValues[y],“desc”) ofcore-setsandpresentourmethodforconstructingopti-\n21: endif mizedcore-setsfordatasetdistillation,asshowninFigure2\n22: SortedPairs=DataPairs[indices] andoutlinedinAlgorithm1.\n23: n=int(r·len(SortedPairs))\n3.1.Core-Sets\n24: PrunedPairs=SortedPairs[:n]\n25: π r =cat(π r,PrunedPairs) Let T = (X real,Y real) represent a dataset of size N T,\n26: endfor wherex ∈ X denotesthei-thimagesampleandy ∈\ni real i\n27: Y itslabel. Acore-setπ ⊂T isdefinedasasubsetof\nreal r\n28: Return: pruneddatasetπ r sizeN πr ≈r·N T,withr ∈(0,1)determiningitsrelative\nsize.\nThegoalistoconstructacore-setthatimprovesthequal-\ntoclassifybasedonthelogit-basedpredictionerror,while ityofdistilleddatasetsbyfocusingonkeysamples,achiev-\nthesecondruleaimstobalancetheclassdistributionusing ingthespecifiedsubsetsizethroughasamplingstrategythat\nRademachercomplexity. Inadditiontothiswork,Xuetal. satisfiesthecondition\n[23]introducedanalternativeapproachthatcombinescore-\n(cid:88)\nsetselectionbasedonempiricallossbeforethedistillation N πr = 1 πr(x i)≈r·N T, (6)\nprocesswithdynamicpruningduringthedistillationitself. (xi,yi)∈T\nThe main difference between our work is that our model\nwhere 1 : T → {0,1} serves as an indicator function,\nisevaluatedonalargerdataset,specificallyseveralsubsets πr\nmarkingmembershipofelementsinthecore-setπ within\nofImageNet,andthecombinationofcore-setselectionand r\nthe larger dataset T. In other words, the core-set π acts\ndistillationprocessleveragingpre-trainedgenerativemodels. r\nbetweentheoriginaldatasetandthesyntheticdataset,repre-\nMoreover,weproposetoderivecore-setspriortodistillation.\nsentingonlyafractionoftheoriginaldatasettothedataset\ndistillationmethods. Theconcreterealizationofthecore-\n3.Methodology\nset depends on the sampling mechanism that defines the\nindicatorfunction1 .\nTheoverarchinggoalofdatasetdistillationistoreduce πr\nthesizeofadatasetbylearningstrongandrepresentative\n3.2.Loss-Value-basedSampling\nsyntheticsamples. Yet,findingthemostrepresentativeand\ninformativefeaturesalsoinvolvesthecomplementarytask In image classification, one tries to approximate y ≈\ni\nofremovingnoisyandpoorinformation. Thus,wepropose M (x ),whereM isaclassifierwithparametersθ (i.e.,\nθ i θ\n3\nCNN models like AlexNet, VGG, ResNet, etc.). Tradi- theirqualityacrosspreviouslyunseennetworkarchitectures.\ntionally, we try to optimize M by minimizing the loss Toevaluateeachsyntheticdatasetwithaparticulararchitec-\nθ\nL(M (x ),y )betweenthepredictedlabelsandtheactual ture,anewnetworkistrainedfromscratchonthedistilled\nθ i i\nlabels. However,onecanalsousethelossofatrainedmodel datasetandthentestedonrealimagesinthetestset. This\nM asametricfor“classificationdifficulty”. Buildingupon processisrepeatedfivetimes,andwereportthemeantest\nθ\nthisideaandgivenr ∈(0,1),wecanderiveacore-setwith accuracy with a confidence interval of plus or minus one\nstandarddeviation.\n(cid:88)\nπeasy = argmin L(M (x ),y ), (7)\nr θ i i\n|TT ′|′ ≈⊂ rT ·N,\nT\n(xi,yi)∈T′ 4.3.NetworkArchitectures\nFollowing Cazenavette et al. [3] and previous work in\nwheresampleswithhighlossvaluesareremovedfirst. Vice\ndatasetdistillation[5,17–19],weuseConvNettodistillthe\nversa,wecandefinetheoppositeextremecasebyremoving\n128×128 and 256×256 datasets, respectively [9]. For\nthelowestlossvaluesfirst,with\nevaluatingunseenarchitectures,weemployAlexNet[14],\nπhard = argmax (cid:88) L(M (x ),y ). (8) VGG-11[22],ResNet-18[10],andaVisionTransformer[8].\nr θ i i\n|TT ′|′ ≈⊂ rT ·N,\nT\n(xi,yi)∈T′ 4.4.GenerativePriors\nInsummary,forπeasy,low-loss-valuesamples-oftenrep- Weincorporatetwotypesofgenerativepriors: GLaD[3],\nr\nresenting“easier”instances-areprioritized. Assuch,they which leverages a StyleGAN-XL [21] model, and LD3M\ncanhelpthemodelestablishcoreclasscharacteristicswith- [17], which uses a modified Stable Diffusion [20] model\nout excessive complexity, ensuring a distilled dataset that adaptedspecificallyfordatasetdistillation.\ncapturesessentialfeatures.Ontheotherhand,forπhard,high-\nr 4.5.PruningDetails\nloss-valuesamples-oftencomplex,high-variationinstances\n-areincludedfirst. Thesesamples,richinnuanceandfea- We applied a pre-trained ResNet-18 model [10] with\nturediversity,areusefulforlearningfinedistinctionswithin cross-entropy loss as the scoring mechanism for our pro-\na class. However, introducing such variability early risks posedloss-value-basedsampling.\naddingunnecessarycomplexity,whichmayhindereffective\ndistillationbyoverwhelmingthecorerepresentation. 5.Results\n4.ExperimentalSetup Thissectionevaluatestheimpactofourpruningapproach\nacross several ImageNet subsets and distillation methods,\nWefollowCazenavetteetal.[3]andevaluatethecross- whereweusedπeasy (r = 0.2forDMandr = 0.6forDC\nr\narchitectureperformancewithgenerativepriors(GLaDand andMTT).Next,wepresentouranalysisofπeasyandπhard,\nr r\nLD3M),forIPC=1(MTT,DC,DM)andIPC=10(DC,DM) fromwhichwederivetheoptimalsettingsandvaluesforr.\nwithimagesize128×128aswellasanevaluationwithDC\nand image size 256×256 for IPC=1. In all experiments, 5.1.DistillationResults\nwemaintainconsistenthyperparameterstoguaranteeafair\nThissectiondemonstratestheimpactofourpruningap-\ncomparison. OurcodecanbefoundonGitHub1.\nproachontheperformanceofdatasetdistillationacrossmul-\n4.1.Datasets tipleImageNetsubsetsandarchitectures. Thepruningset-\ntings(60%forDC,MTT,and20%forDM)werederived\nWeassessclassifieraccuracyonsyntheticimagesfrom\nfromtheanalysisprovidedinthenextsubsection5.2. Over-\nvarious10-classsubsetsofImageNet-1k[6]. Thesubsets\nall,theproposedloss-basedpruningconsistentlyimproves\nincludeImageNet-Birds, ImageNet-Fruits, andImageNet-\nperformanceacrossvariousdistillationalgorithms.\nCats,asdefinedinpriorwork[2],aswellastwowidelyused\nIPC=1,128×128.AsshowninTable1,applyingpruning\nsubsets,ImageNetteandImageWoof[12]. Additionally,we\nyieldssignificantaccuracygainsacrossnearlyallsubsets,\nusesubsetsbasedonResNet-50performanceonImageNet\nwithparticularlystrongimprovementsfortheDCandDM\n[3]: ImageNet-Acontainsthetop10classes, followedby\nalgorithmsforIPC=1.Specifically,pruningpriortoDMwith\nImageNet-B,ImageNet-C,ImageNet-D,andImageNet-E.\nGLaDimprovesaccuracyby+5.2percentagepointsonthe\n4.2.EvaluationProtocol ImageNet-AandBsubsetsandby+3.9percentagepoints\non ImageNet-C. Similarly, DM with LD3M sees notable\nWebeginbydistillingsyntheticdatasetsusingthechosen boosts,particularlyonImageNet-A(+3.0points)andIma-\nalgorithms(i.e.,DC,DM,andMTT),followedbyassessing\ngeNette(+2.8points). Overall, GLaDdemonstratesmore\n1https://github.com/Brian-Moser/prune_and_ pronouncedaccuracyimprovementsthanLD3M,emphasiz-\ndistill ingtheeffectivenessofpruningandgeneralizabilityacross\n4\nDistillationSpace Algorithm ImNet-A ImNet-B ImNet-C ImNet-D ImNet-E ImNette ImWoof ImNet-Birds ImNet-Fruits ImNet-Cats\nMTT 33.4±1.5 34.0±3.4 31.4±3.4 27.7±2.7 24.9±1.8 24.1±1.8 16.0±1.2 25.5±3.0 18.3±2.3 18.7±1.5\npixelspace DC 38.7±4.2 38.7±1.0 33.3±1.9 26.4±1.1 27.4±0.9 28.2±1.4 17.4±1.2 28.5±1.4 20.4±1.5 19.8±0.9\nDM 27.2±1.2 24.4±1.1 23.0±1.4 18.4±0.7 17.7±0.9 20.6±0.7 14.5±0.9 17.8±0.8 14.5±1.1 14.0±1.1\nMTT 39.9±1.2 39.4±1.3 34.9±1.1 30.4±1.5 29.0±1.1 30.4±1.5 17.1±1.1 28.2±1.1 21.1±1.2 19.6±1.2\nGLaD DC 41.8±1.7 42.1±1.2 35.8±1.4 28.0±0.8 29.3±1.3 31.0±1.6 17.8±1.1 29.1±1.0 22.3±1.6 21.2±1.4\nDM 31.6±1.4 31.3±3.9 26.9±1.2 21.5±1.0 20.4±0.8 21.9±1.1 15.2±0.9 18.2±1.0 20.4±1.6 16.1±0.7\nMTT 40.4±1.6 42.4±1.3 35.4±1.1 30.5±1.3 30.0±1.1 30.5±1.0 20.3±1.3 27.7±1.6 21.8±1.1 21.2±0.8\n+0.5+0.4 +3.0+0.0 +0.6+0.0 +0.1-0.2 +1.0+0.0 +0.1-0.5 +3.2+0.2 -0.5-0.5 +0.7-0.1 +0.6-0.4\nGLaD DC 42.8±1.9 44.6±1.0 37.6±2.0 29.5±1.7 32.8±1.0 33.6±1.0 19.3±1.2 30.9±0.9 23.3±1.1 22.4±0.8\n(pruned) +1.0+0.5 +2.5-0.2 +2.2+0.9 +1.5+0.9 +3.5-0.3 +2.6-0.6 +1.5-0.1 +1.8-0.1 +1.0-0.5 +1.2-0.6\nDM 36.8±1.6 36.5±1.6 30.8±1.2 23.1±1.4 22.8±2.0 24.9±0.8 16.1±0.9 21.4±0.9 22.7±1.1 16.3±1.0\n+5.2+0.2 +5.2-2.3 +3.9+0.0 +1.6+0.4 +2.4+1.2 +3.0-0.3 +0.9+0.0 +3.2-0.1 +2.3-0.5 +0.2+0.3\nMTT 40.9±1.1 41.6±1.7 34.1±1.7 31.5±1.2 30.1±1.3 32.0±1.3 19.9±1.2 30.4±1.5 21.4±1.1 22.1±1.0\nLD3M DC 42.3±1.3 42.0±1.1 37.1±1.8 29.7±1.3 31.4±1.1 32.9±1.2 18.9±0.6 30.2±1.4 22.6±1.3 21.7±0.8\nDM 35.8±1.1 34.1±1.0 30.3±1.2 24.7±1.0 24.5±0.9 26.8±1.7 18.1±0.7 23.0±1.8 24.5±1.9 17.0±1.1\nMTT 41.2±1.5 42.4±1.2 36.0±1.4 31.7±1.4 29.9±0.9 33.8±1.6 19.9±1.2 29.5±1.1 23.3±1.2 22.3±0.9\n+0.3+0.5 +0.8-0.5 +1.9-0.3 0.2+0.2 -0.2-0.4 +1.8+0.3 +0.0+0.0 -0.9-0.4 +1.9+0.1 +0.2-0.1\nLD3M DC 42.8±1.8 44.0±1.4 37.8±1.1 29.7±0.9 33.1±1.3 34.4±1.3 19.4±0.9 31.6±1.3 24.0±1.0 22.5±1.1\n(pruned) +0.5+0.5 +2.0+0.3 +0.7-0.7 +0.0-0.4 +1.7+0.2 +1.5+0.1 +0.5+0.3 +1.4-0.1 +1.4-0.3 +0.8+0.3\nDM 38.8±1.2 34.9±1.4 29.7±1.6 26.0±1.6 27.2±0.9 29.6±1.3 18.4±1.0 24.5±1.3 24.5±1.4 17.3±1.2\n+3.0+0.1 +0.8+0.4 -0.5+0.4 +1.3+0.6 +2.7+0.0 +2.8-0.4 +0.3+0.3 +1.5-0.5 +0.0-0.5 +0.3+0.1\nTable1.Cross-architectureperformanceonImageNet(128×128)subsetswith1ImagePerClass(IPC).Distillationmethods(MTT,DC,\nDM)wereevaluatedusingAlexNet,VGG11,ResNet18,andViTasunseenarchitectures,withperformanceaveragedoveractualvalidation\nsets.Generativepriors,GLaDandLD3M,weretestedonbothfulldatasetsandprunedcore-sets,showingrelativechangesinperformance\n(greenforimprovements,redfordeclines).Notably,pruningalmostalwaysledtoperformanceimprovementsacrossvariousmethodsand\nsubsets,highlightingthebenefitofcore-setselectionindatasetdistillation.\nGLaD LD3M\nFigure3.IPC=1classimagesfromImageNette(tench,Englishspringer,cassetteplayer,chainsaw,church,Frenchhorn,garbagetruck,gas\npump,golfball,parachute)usingMTTwithGLaD(left)andLD3M(right).Thetoprowshowsimagesdistilledfromthefulldataset,while\nthebottomrowdisplaysdistilledimagesafterpruning(r=0.6).\ndiversedatasetsandunseenarchitectures. Withpruning,it ble2,yieldingupto3.3percentagepointsimprovementsin\nisevenpossibletoboostGLaDbeyondtheperformanceof ImageNet-BforLD3MwithDMand2.2percentagepoints\nLD3M;forexample,ontheImageWoofsubsetwithMTT, forGLaDwithDC.\npruned GLaD achieves a slight improvement of +0.4 per- VisualsforIPC=10,128×128. Wepresentthedistilled\ncentagepointsoverprunedLD3M. imagesforIPC=10inFigure4. Beyondshowcasingvisual\nVisualsforIPC=1,128×128. InFigure3,thedistilled differences,theseresultshighlightanadditionaladvantageof\nclassimagesareshownforGLaDandLD3M,respectively ourpruningmethod: typically,initialimagesarerandomly\n(ImageNette,MTT).Thevisualizationshighlightthechanges selected from each respective class, which can introduce\nindistilledimageswhenapplyingpruningtoselectacoreset. irrelevantorconfusingfeatures. Byapplyingpruning,we\nInbothcases,GLaDandLD3M,theimagesdistilledfrom increase the likelihood of capturing clear, representative\nthecore-set(bottomrow)exhibitamorerefineddepiction images of the actual class. For example, two images pre-\nofclass-specificfeaturescomparedtothefulldataset(top dominantlydisplayhumansarechosenfromthe“Echidna”\nrow). Overall, the changes in distilled images are more class. Ourapproachprunedthesetwoexampleimagesout\npronouncedinGLaD,whichalignswiththeobservationof asapre-trainedclassifierassignedahighlosstothem.\ngreateraccuracyimprovementslistedinTable1. IPC=1, 256×256. In our 256×256 experiments with\nIPC=10, 128×128. The improved quality of distilled IPC=1,weappliedpruningtoLD3Mandobservedasubstan-\ndatasets,achievedthroughcore-setpruning,extendstoset- tialimprovementincross-architectureperformanceacross\ntingswithIPC=10ataresolutionof128×128,asseeninTa- variousinitializationsettings(ImageNet,FFHQ,andRan-\n5\nDistillationSpace Algorithm All ImNet-A ImNet-B ImNet-C ImNet-D ImNet-E\nDC 42.3±3.5 52.3±0.7 45.1±8.3 40.1±7.6 36.1±0.4 38.1±0.4\npixelspace\nDM 44.4±0.5 52.6±0.4 50.6±0.5 47.5±0.7 35.4±0.4 36.0±0.5\nDC 45.9±1.0 53.1±1.4 50.1±0.6 48.9±1.1 38.9±1.0 38.4±0.7\nGLaD\nDM 45.8±0.6 52.8±1.0 51.3±0.6 49.7±0.4 36.4±0.4 38.6±0.7\nDC 47.2±1.0 54.7±1.1 52.3±0.8 48.5±1.3 39.7±0.8 40.6±1.0\nGLaD +1.3+0.0 +1.6-0.3 +2.2+0.2 -0.4+0.2 +0.8-0.2 +2.2+0.3\n(pruned) DM 46.8±1.2 55.3±1.3 51.7±1.0 49.1±0.8 38.5±1.1 39.6±1.6\n+1.0+0.6 +2.5+0.3 +0.4+0.4 -0.6+0.4 +1.9+0.7 +1.0+0.9\nDC 47.1±1.2 55.2±1.0 51.8±1.4 49.9±1.3 39.5±1.0 39.0±1.3\nLD3M\nDM 47.3±2.1 57.0±1.3 52.3±1.1 48.2±4.9 39.5±1.5 39.4±1.8\nDC 47.8±1.1 55.4±1.3 53.5±1.0 49.5±1.0 40.1±1.3 40.6±0.9\nLD3M +0.7-0.1 +0.2+0.3 +1.7-0.4 -0.4-0.3 +0.6+0.3 +1.6-0.4\n(pruned) DM 48.7±1.3 58.0±1.1 55.6±1.2 48.3±1.5 39.7±1.2 41.8±1.3\n+1.4-0.8 +1.0-0.2 +3.3+0.1 +0.1-3.4 +0.2-0.3 +2.4-0.5\nTable2.Cross-architectureperformancewithIPC=10onthesubsetsImageNetAtoE.\nFigure4. IPC=10classimagesfromImageNet-B(“Echidna”,DM).Ourpruningmethodalsorefinestheinitializationprocess(randomly\nselectedimage),asevidencedbytheabsenceofhumansinthebottomrow.\ndom). AsseeninTable3,applyingtheLD3Mmodelson 5.2.PruningAnalysis\nprunedcore-setsconsistentlyoutperformedtheirunpruned\ncounterpartsacrossImageNetsubsets,underscoringtheef- Prior to the experiments presented so far, we analyzed\nfectivenessofloss-basedpruningindistillation.Forinstance, theimpactofpruningsettingsonImageNette,IPC=1across\ntheprunedLD3MmodelinitializedwithImageNetachieved the three distillation methods with LD3M: DC, DM, and\nan average improvement of up to +1.1 percentage points MTT. We evaluated relative dataset sizes in two ranges:\non the ImageNet-C subset and +1.5 on ImageNet-E. The fine-grainedsizesfrom1%to9%,andbroadersizesfrom\nrandominitializationshowedsimilartrends,withgainsof 10%to100%inincrementsof10%. Foreachrelativesize\nupto+1.7percentagepointsonImageNet-B.Thus,wecan r ∈ (0,100), we apply the same relative size class-wise,\nconcludethatpruningalsoimprovesdistillationqualityfor maintainingarepresentativesubsetthatpreservesclassdi-\nhigherresolutions(i.e.,256×256). versityandminimizesbias.\nVisualsforIPC=1,256×256. InFigure5,wepresent TheresultsofourfirstinvestigationareshowninFigure6\ndistilledimagesofasingleclassfromImageNet-B(Lorikeet) (moredetailedTablesintheappendix). Itshowstheperfor-\nforIPC=1,256×256usingtheLD3Mmodelwithdifferent manceofDC,DM,andMTTmethodsaspruningisapplied\ninitializations: ImageNet,FFHQ,andRandom. Thevisual instepsfrom10%to100%(πeasy). Thegeneraltrendacross\nr\nresultsillustratethatdistilledimagesgeneratedfrompruned allmethodsshowsthathigherpruningrates(lowerpercent-\ncore-sets offer clearer and more distinct class representa- ages)leadtofluctuationintheperformancecomparedtothe\ntionsbyremovingclass-unspecificdetails,suchastheblue trainingonthefulldataset. Moreover, itshowsthatMTT\nbackgroundwhencomparedtothefulldataset. andDMcanperformbetterthantheentiredatasetforvarious\n6\nlluf\ndenurp\nDistillationSpace Initialization All ImNet-A ImNet-B ImNet-C ImNet-D ImNet-E\npixelspace - 29.5±3.1 38.3±4.7 32.8±4.1 27.6±3.3 25.5±1.2 23.5±2.4\nImageNet 34.4±2.6 37.4±5.5 41.5±1.2 35.7±4.0 27.9±1.0 29.3±1.2\nGLaD Random 34.5±1.6 39.3±2.0 40.3±1.7 35.0±1.7 27.9±1.4 29.8±1.4\nFFHQ 34.0±2.1 38.3±5.2 40.2±1.1 34.9±1.1 27.2±0.9 29.4±2.1\nImageNet 36.3±1.6 42.1±2.2 42.1±1.5 35.7±1.7 30.5±1.4 30.9±1.2\nLD3M Random 36.5±1.6 42.0±2.0 41.9±1.7 37.1±1.4 30.5±1.5 31.1±1.4\nFFHQ 36.3±1.5 42.0±1.6 41.9±1.6 36.5±2.2 30.5±0.9 30.6±1.1\nImageNet 37.3±1.5 42.6±1.9 43.2±1.5 37.6±1.6 30.8±1.2 32.4±1.2\n+1.0-0.1 +0.5-0.3 +1.1-0.7 +1.1-0.6 +0.3+0.3 +1.5+0.0\nLD3M Random 37.4±1.3 43.2±1.5 43.6±1.7 37.5±1.3 30.5±1.1 32.4±0.8\n(pruned) +0.9-0.3 +1.2-0.5 +1.7+0.0 +0.4-0.1 +0.0-0.4 +1.3-0.6\nFFHQ 37.3±1.3 42.0±1.7 43.6±1.2 37.5±1.7 31.0±1.1 32.6±1.0\n+1.0-0.2 +0.0+0.1 +1.7-0.4 +1.0-0.5 +0.5-0.2 +2.0-0.1\nTable3.256×256distilledHRimagesusingtheDCdistillationalgorithmandIPC=1.Forbothscenarios(LD3MandGLaD),weevaluate\npre-trainedgeneratorsonImageNet[6],FFHQ[13],andrandomlyinitialized.\nReference Image\n(from orig. data)\nImageNet FFHQ Random\nFigure5.Example256×256imagesofadistilledclass(ImageNet-B:Lorikeet)withdifferentlyinitializedLD3M(prunedvs.fulldataset).\nThevariousinitializations,i.e.,whichdatasetwasusedfortrainingthegenerators,aredenotedatthebottom.\nrelativesizes, especiallyaroundthemid-sizelevels(60%- tionbutwithdescendingpruning(πhard). Theplotclearly\nr\n70%). Surprisingly,DMseemstoimprovetheperformance showsthatdescendingpruningconsistentlyleadstodeclin-\nasthedatasetsizedecreasessignificantly(seepeakat20%). ingdatasetqualityforsmallerdatasetsizes,highlightingthe\nThus, we took a deeper look at more fine-grained dataset importanceofeasysamples,probablynoisy-freesamples,\nsizes,specificallyfrom1%to9%,whichresultsareshown fordatasetdistillation.\ninFigure7. Theresultshighlightthatwhileallmethodsexperience\nHere,allmethodsshowasharperdeclineinaccuracyas accuracydegradationwithincreasedpruning,theyalsoshow\nthedatasetsizeisreduced,especiallyforDCandMTT.Yet, variouspruningrangesthatleadtodatasetqualitythatmain-\nwith only 3% of the dataset, DM can maintain the same tains,andinsomecasesevenimproves,performancewith\ndatasetqualitycomparedtotheentiredataset. Lastly,Fig- reduceddatasetsizes,particularlyforDM.Conversely,de-\nure8providesthesamecomparisonasourfirstinvestiga- scendingpruningconsistentlyreducesdatasetquality,em-\n7\nM3DL\n)denurp(\nM3DL\n36 phasizing the critical role of easier samples in achieving\nDC\noptimaldistillationoutcomes.\nDM\n34 MTT Overall, the optimal dataset size varies by distillation\nmethod and dataset. Relative sizes between 60-80% tend\n32 toyieldthebestperformanceforDCandMTT,whileDM\nachievesstrongresultswithdatasetsizescloserto20%(the\n30\nvalues used in the experiments presented in the previous\nsection). Thistrendsuggeststhateachdistillationmethod\n28\nbenefitsfromauniquebalancebetweensamplediversityand\ndatacompactness,whichpruning(πeasy)helpstoachieveby\n26 r\nfocusingonthosecontributingthemosttomodellearning.\n10 20 30 40 50 60 70 80 90 100\nDataset Size [%]\n6.Limitations&FutureWork\nFigure6.PerformanceevaluationofDC,DM,andMTTmethods\non ImageNette with IPC=1. The results are shown with π reasy, Onelimitationofourapproachisdeterminingtheoptimal\nhorizontallinesdenoting100%. pruningfactorrbeforestartingthedistillationprocess. This\npre-computationsteprequiresapreliminaryanalysistoiden-\ntifytheidealamountofdatatoretaininthecore-set,which\ncanaddcomputationaloverhead. Thervaluemayvaryde-\nDC\nDM pendingonthedataset,architecture,anddistillationmethod\n32\nMTT used,necessitatingfine-tuningforeachnewscenario.\n30 Future work should develop dynamic methods for de-\nterminingr,suchasadaptivepruningstrategiesthatadjust\n28 basedonthespecificcharacteristicsofthedatasetandtar-\ngetarchitecture. Suchadvancementswouldstreamlinethe\n26\npruningprocessandmakeitmoreapplicabletoabroader\nrange of datasets and architectures, further improving the\n24\ngeneralizabilityofdistilleddatasets.\n1 2 3 4 5 6 7 8 9\nDataset Size [%] 7.Conclusion\nFigure7.PerformanceevaluationofDC,DM,andMTTmethods\nWeintroducedanovel“PruneFirst,DistillAfter”frame-\nonImageNettewithIPC=1withfine-grainedpruning(1-9%)and\nworkthatcombinesloss-value-basedsamplingandpruning\n100%forπeasy(horizontallines).\nr tooptimizedistillationquality,enhancingtheperformance\nonunseenarchitectures. Weeffectivelyaddresscriticallim-\nitationsinexistingdatasetdistillationapproaches,suchas\n34 DC cross-architecture generalization and data redundancy, by\nDM emphasizingtheimportanceof“easier”samples,i.e.,sam-\n32 MTT\nples with low loss values, for dataset distillation. Experi-\n30 mentalresultsonvariousImageNetsubsetsdemonstratethat\nevensubstantialpruning(e.g.,uptoremoving80%ofthe\n28\noriginaldataset)priortodistillationmaintainsor, mostof\n26\nthetime,improvesdistillationquality,withaccuracygains\n24 reachingupto5.2percentagepointsonvarioussubsets.This\nrobustperformanceacrossdiversearchitecturesandpruning\n22\nfactorsshowcasethescalabilityandgeneralizabilityofour\n10 20 30 40 50 60 70 80 90 100 framework,markingasignificantadvancementinthefield\nDataset Size [%]\nofdatasetdistillation.\nFigure8.PerformanceevaluationofDC,DM,andMTTmethods\non ImageNette with IPC=1. The results are shown with πhard, Acknowledgements\nr\nhorizontallinesdenoting100%.\nThis work was supported by the BMBF projects Sus-\ntainML(Grant101070408),Albatross(Grant01IW24002)\nandbyCarlZeissFoundationthroughtheSustainableEm-\nbeddedAIproject(P2021-02-009).\n8\n]%[\nycaruccA\n]%[\nycaruccA\n]%[\nycaruccA\nReferences ConferenceonArtificialNeuralNetworks, pages351–363.\nSpringer,2024. 1\n[1] SharatAgarwal,HimanshuArora,SaketAnand,andChetan\n[17] BrianBMoser,FedericoRaue,SebastianPalacio,Stanislav\nArora. Contextualdiversityforactivelearning. InComputer\nFrolov,andAndreasDengel. Latentdatasetdistillationwith\nVision–ECCV2020: 16thEuropeanConference, Glasgow,\ndiffusionmodels. arXivpreprintarXiv:2403.03881,2024. 1,\nUK,August23–28,2020,Proceedings,PartXVI16,pages\n2,4\n137–153.Springer,2020. 1\n[18] TimothyNguyen,ZhourongChen,andJaehoonLee. Dataset\n[2] George Cazenavette, Tongzhou Wang, Antonio Torralba,\nmeta-learningfromkernelridge-regression. arXivpreprint\nAlexei A Efros, and Jun-Yan Zhu. Dataset distillation by\narXiv:2011.00050,2020. 4\nmatchingtrainingtrajectories. InCVPR,2022. 1,2,4\n[19] TimothyNguyen,RomanNovak,LechaoXiao,andJaehoon\n[3] George Cazenavette, Tongzhou Wang, Antonio Torralba,\nLee. Datasetdistillationwithinfinitelywideconvolutional\nAlexeiAEfros,andJun-YanZhu. Generalizingdatasetdistil-\nnetworks. NeurIPS,2021. 4\nlationviadeepgenerativeprior. InCVPR,pages3739–3748,\n[20] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\n2023. 1,2,4\nPatrick Esser, and Bjo¨rn Ommer. High-resolution image\n[4] CodyColeman,ChristopherYeh,StephenMussmann,Baha-\nsynthesiswithlatentdiffusionmodels. InCVPR,2022. 1,2,\nranMirzasoleiman,PeterBailis,PercyLiang,JureLeskovec,\n4\nandMateiZaharia. Selectionviaproxy: Efficientdatase-\n[21] AxelSauer,KatjaSchwarz,andAndreasGeiger. Stylegan-\nlectionfordeeplearning. arXivpreprintarXiv:1906.11829,\nxl: Scalingstylegantolargediversedatasets. InACMSIG-\n2019. 1\nGRAPH2022conferenceproceedings,pages1–10,2022. 2,\n[5] JustinCui,RuochenWang,SiSi,andCho-JuiHsieh. Scaling\n4\nupdatasetdistillationtoimagenet-1kwithconstantmemory.\n[22] KarenSimonyanandAndrewZisserman. Verydeepconvo-\nInICML,2023. 1,4\nlutionalnetworksforlarge-scaleimagerecognition. ICLR,\n[6] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLi\n2014. 4\nFei-Fei. Imagenet:Alarge-scalehierarchicalimagedatabase.\n[23] YueXu, Yong-LuLi, KaitongCui, ZiyuWang, CewuLu,\nInCVPR,2009. 4,7\nYu-WingTai,andChi-KeungTang. Distillgoldfrommassive\n[7] QingtangDing,ZhengyuLiang,LongguangWang,Yingqian\nores:Bi-leveldatapruningtowardsefficientdatasetdistilla-\nWang,andJungangYang. Notallpatchesareequal:Hierar- tion. InProceedingsoftheEuropeanConferenceonCom-\nchicaldatasetcondensationforsingleimagesuper-resolution. puterVision(ECCV),2024. 3\nIEEESignalProcessingLetters,2023. 1\n[24] BoZhaoandHakanBilen. Datasetcondensationwithdistri-\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, butionmatching. InWACV,2023. 2\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\n[25] BoZhao,KondaReddyMopuri,andHakanBilen. Dataset\nMostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-\ncondensationwithgradientmatching. ICLR,2020. 2\nvainGelly,etal. Animageisworth16x16words:Transform-\nersforimagerecognitionatscale. ICLR,2020. 4\n[9] SpyrosGidarisandNikosKomodakis. Dynamicfew-shot\nvisuallearningwithoutforgetting. InCVPR,2018. 4\n[10] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.\nDeepresiduallearningforimagerecognition.InCVPR,2016.\n1,4\n[11] Yang He, Lingao Xiao, and Tianyi Joey Zhou. You Only\nCondenseOnce: Tworulesforpruningcondenseddatasets.\nInProceedingsoftheAdvancesinNeuralInformationPro-\ncessingSystems(NeurIPS),2023. 2\n[12] Jeremy Howard. A smaller subset of 10 easily classified\nclasses from imagenet, and a little more french. URL\nhttps://github.com/fastai/imagenette,2019. 4\n[13] TeroKarras, SamuliLaine, andTimoAila. Astyle-based\ngeneratorarchitectureforgenerativeadversarialnetworks. In\nCVPR,pages4401–4410,2019. 7\n[14] AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. Im-\nagenetclassificationwithdeepconvolutionalneuralnetworks.\nNeurIPS,25,2012. 4\n[15] BrianMoser,FedericoRaue,Jo¨rnHees,andAndreasDengel.\nLessismore: Proxydatasetsinnasapproaches. InCVPR,\npages1953–1961,2022. 1\n[16] BrianBMoser,FedericoRaue,andAndreasDengel. Astudy\nindatasetpruningforimagesuper-resolution.InInternational\n9",
    "pdf_filename": "Distill_the_Best,_Ignore_the_Rest_Improving_Dataset_Distillation_with_Loss-Value-Based_Pruning.pdf"
}