{
    "title": "Distill the Best, Ignore the Rest Improving Dataset Distillation with Loss-Value-Based Pruning",
    "context": "Dataset distillation has gained significant interest in re- cent years, yet existing approaches typically distill from the entire dataset, potentially including non-beneficial samples. We introduce a novel “Prune First, Distill After” framework that systematically prunes datasets via loss-based sampling prior to distillation. By leveraging pruning before classical distillation techniques and generative priors, we create a representative core-set that leads to enhanced generaliza- tion for unseen architectures - a significant challenge of current distillation methods. More specifically, our proposed framework significantly boosts distilled quality, achieving up to a 5.2 percentage points accuracy increase even with substantial dataset pruning, i.e., removing 80% of the origi- nal dataset prior to distillation. Overall, our experimental results highlight the advantages of our easy-sample prioriti- zation and cross-architecture robustness, paving the way for more effective and high-quality dataset distillation. Large-scale datasets are crucial for training high-quality machine learning models across various applications [10,20]. However, the sheer volume of data brings significant com- putational and storage challenges, making efficient dataset distillation methods highly desirable [2,5]. Dataset distilla- tion aims to compress these large datasets into smaller, syn- thetic subsets while preserving training quality, yet existing techniques often fall short in achieving cross-architecture ro- bustness [17]. Classifiers generally perform best when their architecture matches the one used during distillation, but performance degrades significantly when trained on other architectures [3,17]. This challenge is compounded by the retention of noisy samples, which dilutes the core repre- sentational value of distilled data. Addressing these issues requires not only compact and representative data but also a selective sampling approach to enhance performance consis- Pruning Dataset Distillation Dataset Distillation Ours Classical Distillation Pipeline relevant samples irrelevant samples Pre-Selects relevant Samples Pre-Selects relevant Samples Full Dataset Core-Set Distilled Dataset Figure 1. Comparison of classical and our proposed “Prune First, Distill After” pipeline: Traditional dataset distillation operates on the full dataset, which includes both relevant and irrelevant samples. Our Prune-Distill approach pre-selects a core-set by pruning loss- value-based irrelevant samples, focusing distillation on the most informative subset, resulting in a more refined distilled dataset. tency across a range of unseen architectures. Inspired by previous work on dataset pruning for various computer vision tasks [1,4,7,15,16], we explore the interplay between dataset pruning and dataset distillation, proposing a novel approach that systematically prunes samples prior to distillation. As shown in Figure 1, our method enables the creation of compact yet highly representative core-sets, a subset of the original dataset, through targeted pruning that enhance performance and stability across diverse and unseen architectures. To realize this, we introduce a loss- value-based sampling strategy that leverages a pre-trained classifier model to rank data samples by their “classifica- tion difficulty”, helping to capture the key characteristics of each class. This analysis combines two sampling strategies: ascending (starting with simpler samples) and descending (starting with complex samples), allowing us to examine their respective effects on distillation quality. As a result, we 1 arXiv:2411.12115v1  [cs.CV]  18 Nov 2024",
    "body": "Distill the Best, Ignore the Rest: Improving Dataset Distillation with\nLoss-Value-Based Pruning\nBrian B. Moser1,2, Federico Raue1, Tobias C. Nauen1,2, Stanislav Frolov1,2, Andreas Dengel1,2\n1German Research Center for Artificial Intelligence\n2University of Kaiserslautern-Landau\nfirst.last@dfki.de\nAbstract\nDataset distillation has gained significant interest in re-\ncent years, yet existing approaches typically distill from the\nentire dataset, potentially including non-beneficial samples.\nWe introduce a novel “Prune First, Distill After” framework\nthat systematically prunes datasets via loss-based sampling\nprior to distillation. By leveraging pruning before classical\ndistillation techniques and generative priors, we create a\nrepresentative core-set that leads to enhanced generaliza-\ntion for unseen architectures - a significant challenge of\ncurrent distillation methods. More specifically, our proposed\nframework significantly boosts distilled quality, achieving\nup to a 5.2 percentage points accuracy increase even with\nsubstantial dataset pruning, i.e., removing 80% of the origi-\nnal dataset prior to distillation. Overall, our experimental\nresults highlight the advantages of our easy-sample prioriti-\nzation and cross-architecture robustness, paving the way for\nmore effective and high-quality dataset distillation.\n1. Introduction\nLarge-scale datasets are crucial for training high-quality\nmachine learning models across various applications [10,20].\nHowever, the sheer volume of data brings significant com-\nputational and storage challenges, making efficient dataset\ndistillation methods highly desirable [2,5]. Dataset distilla-\ntion aims to compress these large datasets into smaller, syn-\nthetic subsets while preserving training quality, yet existing\ntechniques often fall short in achieving cross-architecture ro-\nbustness [17]. Classifiers generally perform best when their\narchitecture matches the one used during distillation, but\nperformance degrades significantly when trained on other\narchitectures [3,17]. This challenge is compounded by the\nretention of noisy samples, which dilutes the core repre-\nsentational value of distilled data. Addressing these issues\nrequires not only compact and representative data but also a\nselective sampling approach to enhance performance consis-\nPruning\nDataset\nDistillation\nDataset\nDistillation\nOurs\nClassical Distillation Pipeline\nrelevant\nsamples\nirrelevant\nsamples\nPre-Selects\nrelevant Samples\nPre-Selects\nrelevant Samples\nFull\nDataset\nCore-Set\nDistilled\nDataset\nFigure 1. Comparison of classical and our proposed “Prune First,\nDistill After” pipeline: Traditional dataset distillation operates on\nthe full dataset, which includes both relevant and irrelevant samples.\nOur Prune-Distill approach pre-selects a core-set by pruning loss-\nvalue-based irrelevant samples, focusing distillation on the most\ninformative subset, resulting in a more refined distilled dataset.\ntency across a range of unseen architectures.\nInspired by previous work on dataset pruning for various\ncomputer vision tasks [1,4,7,15,16], we explore the interplay\nbetween dataset pruning and dataset distillation, proposing\na novel approach that systematically prunes samples prior\nto distillation. As shown in Figure 1, our method enables\nthe creation of compact yet highly representative core-sets,\na subset of the original dataset, through targeted pruning\nthat enhance performance and stability across diverse and\nunseen architectures. To realize this, we introduce a loss-\nvalue-based sampling strategy that leverages a pre-trained\nclassifier model to rank data samples by their “classifica-\ntion difficulty”, helping to capture the key characteristics of\neach class. This analysis combines two sampling strategies:\nascending (starting with simpler samples) and descending\n(starting with complex samples), allowing us to examine\ntheir respective effects on distillation quality. As a result, we\n1\narXiv:2411.12115v1  [cs.CV]  18 Nov 2024\n\nfind that focusing exclusively on simpler samples yields a\nsignificant distillation improvement.\nWe build on recent advancements in dataset distillation\nthat incorporate generative priors, exploiting both StyleGAN-\nXL [3,21] and modified diffusion models [17,20] to verify\nour observations. Through extensive experiments on subsets\nof ImageNet, we systematically evaluate the effect of pruning\nand distillation strategies across diverse architectures. As a\nresult, we achieve significant performance boosts, sometimes\nby up to ca. 17% or 5.2 p.p., on distilled datasets even\nwith substantial pruning factors (i.e., removing 80% of the\noriginal dataset prior to distillation).\nOur contributions can be summarized as follows:\n• A Prune-First, Distill-After Framework: We propose a\nnovel approach that integrates dataset pruning as a pre-\nprocessing step for dataset distillation, which improves\nthe quality of the distilled dataset.\n• Loss-Value-Based Sampling Strategy: We introduce\na sampling mechanism based on loss values from a\npre-trained classifier, guiding the selection of the most\ninformative samples and enabling class-balanced core-\nset creation.\n• Analysis of Pruning Strategies: We compare ascending\nand descending sampling methods to determine their\nimpact on distillation quality, revealing the benefits of\nprioritizing simpler samples in the core-set.\n• Extensive Cross-Architecture Evaluation: We validate\nour approach on multiple ImageNet subsets and diverse\narchitectures, demonstrating the robustness and flexibil-\nity of our pruned and distilled datasets across various\nmodel architectures.\n2. Background\nThis section introduces the key concepts of dataset distil-\nlation alongside their fusion with generative priors.\n2.1. Dataset Distillation\nLet T\n= (Xreal, Yreal) represent a real dataset with\nXreal ∈RN×H×W ×C, where N denotes the total num-\nber of samples. The objective of dataset distillation is to\ncompress T into a smaller synthetic set S = (Xsyn, Ysyn),\nwith Xsyn ∈RM×H×W ×C, where M = C · IPC, C repre-\nsents the number of classes, and IPC the images per class.\nOur goal is to achieve M ≪N by optimizing\nS∗= arg min\nS L(S, T ),\n(1)\nwhere L represents the distillation objective, which is de-\nfined by the applied dataset distillation method, e.g., Dataset\nCondensation (DC) [25], Distribution Matching (DM) [24],\nand Matching Training Trajectories (MTT) [2].\nDataset Condensation (DC) aligns gradients by mini-\nmizing the difference between the gradients on the synthetic\nand real datasets. This is expressed as\nLDC(S, T ) = 1 −\n∇θℓS(θ) · ∇θℓT (θ)\n∥∇θℓS(θ)∥∥∇θℓT (θ)∥.\n(2)\nDistribution Matching (DM) enforces similar feature\nrepresentations for real and synthetic data by aligning the\nfeature distribution across classes:\nLDM(S, T ) =\nX\nc\n\r\r\r\r\r\n1\n|Tc|\nX\nx∈Tc\nψ(x) −\n1\n|Sc|\nX\ns∈Sc\nψ(s)\n\r\r\r\r\r\n2\n, (3)\nwhere Tc and Sc represent real and synthetic samples for\neach class c.\nMatching Training Trajectories (MTT) minimizes the\ndistance between parameter trajectories of networks trained\non real and synthetic data. Using several model instances,\nMTT saves the training path of model parameters θ∗\nt\nT\n0 at\nregular intervals, called expert trajectories. For distillation,\nit initializes a network, ˆθt+N with N steps, on the synthetic\ndata, tracking its trajectory, and minimizes the distance from\nthe real data trajectory, θ∗\nt+M with M steps:\nLMT T (S, T ) = ∥ˆθt+N −θ∗\nt+M∥2\n∥θ∗\nt −θ∗\nt+M∥2 .\n(4)\n2.2. Dataset Distillation with Generative Prior\nIn dataset distillation with a generative prior, a pre-trained\ngenerative model is used to synthesize latent codes rather\nthan raw pixel values [3]. Incorporating generative priors\ninto dataset distillation offers several advantages, primarily\nby compressing informative features into a more structured\nlatent space. This transformation enables greater flexibility,\nas latent codes are easier to manipulate and adapt compared\nto pixel-level data. Specifically, let D : RM×h×w×d →\nRM×H×W ×C denote the generative model, where h · w ≪\nH · W. This transforms the distillation objective as follows:\nZ∗= arg min\nZ L(D(Z), T ),\n(5)\nwhere L is the distillation objective defined by the used\nalgorithm (see subsection 2.1). GLaD [3], one of the initial\nmethods to leverage generative priors, employs a pre-trained\nStyleGAN-XL [21]. LD3M [17] extends GLaD by replacing\nthe StyleGAN-XL with a modified diffusion model, i.e.,\nStable Diffusion [20], tailored for dataset distillation.\n2.3. Core-set Selection in Distilled Datasets\nCombining distilled datasets with core-set approaches\nhas already been proposed for small-scale datasets, such as\nCIFAR100 and Tiny ImageNet. He et al. [11] suggested ap-\nplying core-set selection directly to the distilled dataset using\ntwo specific rules. The first rule selects images that are easy\n2\n\nAlgorithm 1 Loss-Value-Based Sampling\nInput: real dataset T = (Xreal, Yreal), pre-trained classi-\nfier Mθ, loss function L, mode m (“easy” or “hard”), and\npruning ratio r.\n1: // initialization\n2: LossValues = dict()\n3: for each unique label y ∈SET(Yreal) do\n4:\nLossValues[y] = list()\n5:\nDataPairs[y] = list()\n6: end for\n7:\n8: // calculate losses\n9: for each sample pair (xi, yi) ∈T do\n10:\nLossValues[yi].append(L(Mθ(xi), yi))\n11:\nDataPairs[yi].append((xi, yi))\n12: end for\n13:\n14: // derive core-set\n15: πr = list()\n16: for each unique label y ∈SET(Yreal) do\n17:\nif (m == “easy”) then\n18:\nindices = argsort (LossValues[y], “asc”)\n19:\nelse\n20:\nindices = argsort (LossValues[y], “desc”)\n21:\nend if\n22:\nSortedPairs = DataPairs[indices]\n23:\nn = int(r · len(SortedPairs))\n24:\nPrunedPairs = SortedPairs[:n]\n25:\nπr = cat (πr, PrunedPairs)\n26: end for\n27:\n28: Return: pruned dataset πr\nto classify based on the logit-based prediction error, while\nthe second rule aims to balance the class distribution using\nRademacher complexity. In addition to this work, Xu et al.\n[23] introduced an alternative approach that combines core-\nset selection based on empirical loss before the distillation\nprocess with dynamic pruning during the distillation itself.\nThe main difference between our work is that our model\nis evaluated on a larger dataset, specifically several subsets\nof ImageNet, and the combination of core-set selection and\ndistillation process leveraging pre-trained generative models.\nMoreover, we propose to derive core-sets prior to distillation.\n3. Methodology\nThe overarching goal of dataset distillation is to reduce\nthe size of a dataset by learning strong and representative\nsynthetic samples. Yet, finding the most representative and\ninformative features also involves the complementary task\nof removing noisy and poor information. Thus, we propose\nScores\nClassifier\nApply Dataset\nDistillation\nAlgorithm\ntraditional\nCore-\nSet\nDistilled\nDataset\nours\nsort and\nselect top r\nFigure 2. Comparison of traditional dataset distillation with our\nproposed “Prune First, Distill After” pipeline. Rather than distilling\nthe full dataset, we first use a classifier to rank and select the top-r\nsamples to create a core-set, which is then distilled to produce a\nmore expressive and architecture-robust distilled dataset.\ndataset pruning as a crucial pre-processing step for effective\ndataset distillation. In this section, we introduce the concept\nof core-sets and present our method for constructing opti-\nmized core-sets for dataset distillation, as shown in Figure 2\nand outlined in Algorithm 1.\n3.1. Core-Sets\nLet T = (Xreal, Yreal) represent a dataset of size NT ,\nwhere xi ∈Xreal denotes the i-th image sample and yi ∈\nYreal its label. A core-set πr ⊂T is defined as a subset of\nsize Nπr ≈r · NT , with r ∈(0, 1) determining its relative\nsize.\nThe goal is to construct a core-set that improves the qual-\nity of distilled datasets by focusing on key samples, achiev-\ning the specified subset size through a sampling strategy that\nsatisfies the condition\nNπr =\nX\n(xi,yi)∈T\n1πr (xi) ≈r · NT ,\n(6)\nwhere 1πr : T →{0, 1} serves as an indicator function,\nmarking membership of elements in the core-set πr within\nthe larger dataset T . In other words, the core-set πr acts\nbetween the original dataset and the synthetic dataset, repre-\nsenting only a fraction of the original dataset to the dataset\ndistillation methods. The concrete realization of the core-\nset depends on the sampling mechanism that defines the\nindicator function 1πr.\n3.2. Loss-Value-based Sampling\nIn image classification, one tries to approximate yi ≈\nMθ (xi), where Mθ is a classifier with parameters θ (i.e.,\n3\n\nCNN models like AlexNet, VGG, ResNet, etc.). Tradi-\ntionally, we try to optimize Mθ by minimizing the loss\nL(Mθ (xi) , yi) between the predicted labels and the actual\nlabels. However, one can also use the loss of a trained model\nMθ as a metric for “classification difficulty”. Building upon\nthis idea and given r ∈(0, 1), we can derive a core-set with\nπeasy\nr\n= arg min\nT ′⊂T ,\n|T ′|≈r·NT\nX\n(xi,yi)∈T ′\nL (Mθ (xi) , yi) ,\n(7)\nwhere samples with high loss values are removed first. Vice\nversa, we can define the opposite extreme case by removing\nthe lowest loss values first, with\nπhard\nr\n= arg max\nT ′⊂T ,\n|T ′|≈r·NT\nX\n(xi,yi)∈T ′\nL (Mθ (xi) , yi) .\n(8)\nIn summary, for πeasy\nr\n, low-loss-value samples - often rep-\nresenting “easier” instances - are prioritized. As such, they\ncan help the model establish core class characteristics with-\nout excessive complexity, ensuring a distilled dataset that\ncaptures essential features. On the other hand, for πhard\nr\n, high-\nloss-value samples - often complex, high-variation instances\n- are included first. These samples, rich in nuance and fea-\nture diversity, are useful for learning fine distinctions within\na class. However, introducing such variability early risks\nadding unnecessary complexity, which may hinder effective\ndistillation by overwhelming the core representation.\n4. Experimental Setup\nWe follow Cazenavette et al. [3] and evaluate the cross-\narchitecture performance with generative priors (GLaD and\nLD3M), for IPC=1 (MTT, DC, DM) and IPC=10 (DC, DM)\nwith image size 128 × 128 as well as an evaluation with DC\nand image size 256 × 256 for IPC=1. In all experiments,\nwe maintain consistent hyperparameters to guarantee a fair\ncomparison. Our code can be found on GitHub1.\n4.1. Datasets\nWe assess classifier accuracy on synthetic images from\nvarious 10-class subsets of ImageNet-1k [6]. The subsets\ninclude ImageNet-Birds, ImageNet-Fruits, and ImageNet-\nCats, as defined in prior work [2], as well as two widely used\nsubsets, ImageNette and ImageWoof [12]. Additionally, we\nuse subsets based on ResNet-50 performance on ImageNet\n[3]: ImageNet-A contains the top 10 classes, followed by\nImageNet-B, ImageNet-C, ImageNet-D, and ImageNet-E.\n4.2. Evaluation Protocol\nWe begin by distilling synthetic datasets using the chosen\nalgorithms (i.e., DC, DM, and MTT), followed by assessing\n1https : / / github . com / Brian - Moser / prune _ and _\ndistill\ntheir quality across previously unseen network architectures.\nTo evaluate each synthetic dataset with a particular architec-\nture, a new network is trained from scratch on the distilled\ndataset and then tested on real images in the test set. This\nprocess is repeated five times, and we report the mean test\naccuracy with a confidence interval of plus or minus one\nstandard deviation.\n4.3. Network Architectures\nFollowing Cazenavette et al. [3] and previous work in\ndataset distillation [5,17–19], we use ConvNet to distill the\n128 × 128 and 256 × 256 datasets, respectively [9]. For\nevaluating unseen architectures, we employ AlexNet [14],\nVGG-11 [22], ResNet-18 [10], and a Vision Transformer [8].\n4.4. Generative Priors\nWe incorporate two types of generative priors: GLaD [3],\nwhich leverages a StyleGAN-XL [21] model, and LD3M\n[17], which uses a modified Stable Diffusion [20] model\nadapted specifically for dataset distillation.\n4.5. Pruning Details\nWe applied a pre-trained ResNet-18 model [10] with\ncross-entropy loss as the scoring mechanism for our pro-\nposed loss-value-based sampling.\n5. Results\nThis section evaluates the impact of our pruning approach\nacross several ImageNet subsets and distillation methods,\nwhere we used πeasy\nr\n(r = 0.2 for DM and r = 0.6 for DC\nand MTT). Next, we present our analysis of πeasy\nr\nand πhard\nr\n,\nfrom which we derive the optimal settings and values for r.\n5.1. Distillation Results\nThis section demonstrates the impact of our pruning ap-\nproach on the performance of dataset distillation across mul-\ntiple ImageNet subsets and architectures. The pruning set-\ntings (60% for DC, MTT, and 20% for DM) were derived\nfrom the analysis provided in the next subsection 5.2. Over-\nall, the proposed loss-based pruning consistently improves\nperformance across various distillation algorithms.\nIPC=1, 128×128. As shown in Table 1, applying pruning\nyields significant accuracy gains across nearly all subsets,\nwith particularly strong improvements for the DC and DM\nalgorithms for IPC=1. Specifically, pruning prior to DM with\nGLaD improves accuracy by +5.2 percentage points on the\nImageNet-A and B subsets and by +3.9 percentage points\non ImageNet-C. Similarly, DM with LD3M sees notable\nboosts, particularly on ImageNet-A (+3.0 points) and Ima-\ngeNette (+2.8 points). Overall, GLaD demonstrates more\npronounced accuracy improvements than LD3M, emphasiz-\ning the effectiveness of pruning and generalizability across\n4\n\nDistillation Space\nAlgorithm\nImNet-A\nImNet-B\nImNet-C\nImNet-D\nImNet-E\nImNette\nImWoof\nImNet-Birds\nImNet-Fruits\nImNet-Cats\nMTT\n33.4±1.5\n34.0±3.4\n31.4±3.4\n27.7±2.7\n24.9±1.8\n24.1±1.8\n16.0±1.2\n25.5±3.0\n18.3±2.3\n18.7±1.5\npixel space\nDC\n38.7±4.2\n38.7±1.0\n33.3±1.9\n26.4±1.1\n27.4±0.9\n28.2±1.4\n17.4±1.2\n28.5±1.4\n20.4±1.5\n19.8±0.9\nDM\n27.2±1.2\n24.4±1.1\n23.0±1.4\n18.4±0.7\n17.7±0.9\n20.6±0.7\n14.5±0.9\n17.8±0.8\n14.5±1.1\n14.0±1.1\nMTT\n39.9±1.2\n39.4±1.3\n34.9±1.1\n30.4±1.5\n29.0±1.1\n30.4±1.5\n17.1±1.1\n28.2±1.1\n21.1±1.2\n19.6±1.2\nGLaD\nDC\n41.8±1.7\n42.1±1.2\n35.8±1.4\n28.0±0.8\n29.3±1.3\n31.0±1.6\n17.8±1.1\n29.1±1.0\n22.3±1.6\n21.2±1.4\nDM\n31.6±1.4\n31.3±3.9\n26.9±1.2\n21.5±1.0\n20.4±0.8\n21.9±1.1\n15.2±0.9\n18.2±1.0\n20.4±1.6\n16.1±0.7\nMTT\n40.4±1.6\n42.4±1.3\n35.4±1.1\n30.5±1.3\n30.0±1.1\n30.5±1.0\n20.3±1.3\n27.7±1.6\n21.8±1.1\n21.2±0.8\n+0.5+0.4\n+3.0+0.0\n+0.6+0.0\n+0.1-0.2\n+1.0+0.0\n+0.1-0.5\n+3.2+0.2\n-0.5-0.5\n+0.7-0.1\n+0.6-0.4\nGLaD\nDC\n42.8±1.9\n44.6±1.0\n37.6±2.0\n29.5±1.7\n32.8±1.0\n33.6±1.0\n19.3±1.2\n30.9±0.9\n23.3±1.1\n22.4±0.8\n(pruned)\n+1.0+0.5\n+2.5-0.2\n+2.2+0.9\n+1.5+0.9\n+3.5-0.3\n+2.6-0.6\n+1.5-0.1\n+1.8-0.1\n+1.0-0.5\n+1.2-0.6\nDM\n36.8±1.6\n36.5±1.6\n30.8±1.2\n23.1±1.4\n22.8±2.0\n24.9±0.8\n16.1±0.9\n21.4±0.9\n22.7±1.1\n16.3±1.0\n+5.2+0.2\n+5.2-2.3\n+3.9+0.0\n+1.6+0.4\n+2.4+1.2\n+3.0-0.3\n+0.9+0.0\n+3.2-0.1\n+2.3-0.5\n+0.2+0.3\nMTT\n40.9±1.1\n41.6±1.7\n34.1±1.7\n31.5±1.2\n30.1±1.3\n32.0±1.3\n19.9±1.2\n30.4±1.5\n21.4±1.1\n22.1±1.0\nLD3M\nDC\n42.3±1.3\n42.0±1.1\n37.1±1.8\n29.7±1.3\n31.4±1.1\n32.9±1.2\n18.9±0.6\n30.2±1.4\n22.6±1.3\n21.7±0.8\nDM\n35.8±1.1\n34.1±1.0\n30.3±1.2\n24.7±1.0\n24.5±0.9\n26.8±1.7\n18.1±0.7\n23.0±1.8\n24.5±1.9\n17.0±1.1\nMTT\n41.2±1.5\n42.4±1.2\n36.0±1.4\n31.7±1.4\n29.9±0.9\n33.8±1.6\n19.9±1.2\n29.5±1.1\n23.3±1.2\n22.3±0.9\n+0.3+0.5\n+0.8-0.5\n+1.9-0.3\n0.2+0.2\n-0.2-0.4\n+1.8+0.3\n+0.0+0.0\n-0.9-0.4\n+1.9+0.1\n+0.2-0.1\nLD3M\nDC\n42.8±1.8\n44.0±1.4\n37.8±1.1\n29.7±0.9\n33.1±1.3\n34.4±1.3\n19.4±0.9\n31.6±1.3\n24.0±1.0\n22.5±1.1\n(pruned)\n+0.5+0.5\n+2.0+0.3\n+0.7-0.7\n+0.0-0.4\n+1.7+0.2\n+1.5+0.1\n+0.5+0.3\n+1.4-0.1\n+1.4-0.3\n+0.8+0.3\nDM\n38.8±1.2\n34.9±1.4\n29.7±1.6\n26.0±1.6\n27.2±0.9\n29.6±1.3\n18.4±1.0\n24.5±1.3\n24.5±1.4\n17.3±1.2\n+3.0+0.1\n+0.8+0.4\n-0.5+0.4\n+1.3+0.6\n+2.7+0.0\n+2.8-0.4\n+0.3+0.3\n+1.5-0.5\n+0.0-0.5\n+0.3+0.1\nTable 1. Cross-architecture performance on ImageNet (128×128) subsets with 1 Image Per Class (IPC). Distillation methods (MTT, DC,\nDM) were evaluated using AlexNet, VGG11, ResNet18, and ViT as unseen architectures, with performance averaged over actual validation\nsets. Generative priors, GLaD and LD3M, were tested on both full datasets and pruned core-sets, showing relative changes in performance\n(green for improvements, red for declines). Notably, pruning almost always led to performance improvements across various methods and\nsubsets, highlighting the benefit of core-set selection in dataset distillation.\nGLaD\nLD3M\nFigure 3. IPC=1 class images from ImageNette (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas\npump, golf ball, parachute) using MTT with GLaD (left) and LD3M (right). The top row shows images distilled from the full dataset, while\nthe bottom row displays distilled images after pruning (r = 0.6).\ndiverse datasets and unseen architectures. With pruning, it\nis even possible to boost GLaD beyond the performance of\nLD3M; for example, on the ImageWoof subset with MTT,\npruned GLaD achieves a slight improvement of +0.4 per-\ncentage points over pruned LD3M.\nVisuals for IPC=1, 128×128. In Figure 3, the distilled\nclass images are shown for GLaD and LD3M, respectively\n(ImageNette, MTT). The visualizations highlight the changes\nin distilled images when applying pruning to select a core set.\nIn both cases, GLaD and LD3M, the images distilled from\nthe core-set (bottom row) exhibit a more refined depiction\nof class-specific features compared to the full dataset (top\nrow). Overall, the changes in distilled images are more\npronounced in GLaD, which aligns with the observation of\ngreater accuracy improvements listed in Table 1.\nIPC=10, 128×128. The improved quality of distilled\ndatasets, achieved through core-set pruning, extends to set-\ntings with IPC=10 at a resolution of 128×128, as seen in Ta-\nble 2, yielding up to 3.3 percentage points improvements in\nImageNet-B for LD3M with DM and 2.2 percentage points\nfor GLaD with DC.\nVisuals for IPC=10, 128×128. We present the distilled\nimages for IPC=10 in Figure 4. Beyond showcasing visual\ndifferences, these results highlight an additional advantage of\nour pruning method: typically, initial images are randomly\nselected from each respective class, which can introduce\nirrelevant or confusing features. By applying pruning, we\nincrease the likelihood of capturing clear, representative\nimages of the actual class. For example, two images pre-\ndominantly display humans are chosen from the “Echidna”\nclass. Our approach pruned these two example images out\nas a pre-trained classifier assigned a high loss to them.\nIPC=1, 256×256. In our 256×256 experiments with\nIPC=1, we applied pruning to LD3M and observed a substan-\ntial improvement in cross-architecture performance across\nvarious initialization settings (ImageNet, FFHQ, and Ran-\n5\n\nDistillation Space\nAlgorithm\nAll\nImNet-A\nImNet-B\nImNet-C\nImNet-D\nImNet-E\nDC\n42.3±3.5\n52.3±0.7\n45.1±8.3\n40.1±7.6\n36.1±0.4\n38.1±0.4\npixel space\nDM\n44.4±0.5\n52.6±0.4\n50.6±0.5\n47.5±0.7\n35.4±0.4\n36.0±0.5\nDC\n45.9±1.0\n53.1±1.4\n50.1±0.6\n48.9±1.1\n38.9±1.0\n38.4±0.7\nGLaD\nDM\n45.8±0.6\n52.8±1.0\n51.3±0.6\n49.7±0.4\n36.4±0.4\n38.6±0.7\nDC\n47.2±1.0\n54.7±1.1\n52.3±0.8\n48.5±1.3\n39.7±0.8\n40.6±1.0\nGLaD\n+1.3+0.0\n+1.6-0.3\n+2.2+0.2\n-0.4+0.2\n+0.8-0.2\n+2.2+0.3\n(pruned)\nDM\n46.8±1.2\n55.3±1.3\n51.7±1.0\n49.1±0.8\n38.5±1.1\n39.6±1.6\n+1.0+0.6\n+2.5+0.3\n+0.4+0.4\n-0.6+0.4\n+1.9+0.7\n+1.0+0.9\nDC\n47.1±1.2\n55.2±1.0\n51.8±1.4\n49.9±1.3\n39.5±1.0\n39.0±1.3\nLD3M\nDM\n47.3±2.1\n57.0±1.3\n52.3±1.1\n48.2±4.9\n39.5±1.5\n39.4±1.8\nDC\n47.8±1.1\n55.4±1.3\n53.5±1.0\n49.5±1.0\n40.1±1.3\n40.6±0.9\nLD3M\n+0.7-0.1\n+0.2+0.3\n+1.7-0.4\n-0.4-0.3\n+0.6+0.3\n+1.6-0.4\n(pruned)\nDM\n48.7±1.3\n58.0±1.1\n55.6±1.2\n48.3±1.5\n39.7±1.2\n41.8±1.3\n+1.4-0.8\n+1.0-0.2\n+3.3+0.1\n+0.1-3.4\n+0.2-0.3\n+2.4-0.5\nTable 2. Cross-architecture performance with IPC=10 on the subsets ImageNet A to E.\npruned\nfull\nFigure 4. IPC=10 class images from ImageNet-B (“Echidna”, DM). Our pruning method also refines the initialization process (randomly\nselected image), as evidenced by the absence of humans in the bottom row.\ndom). As seen in Table 3, applying the LD3M models on\npruned core-sets consistently outperformed their unpruned\ncounterparts across ImageNet subsets, underscoring the ef-\nfectiveness of loss-based pruning in distillation. For instance,\nthe pruned LD3M model initialized with ImageNet achieved\nan average improvement of up to +1.1 percentage points\non the ImageNet-C subset and +1.5 on ImageNet-E. The\nrandom initialization showed similar trends, with gains of\nup to +1.7 percentage points on ImageNet-B. Thus, we can\nconclude that pruning also improves distillation quality for\nhigher resolutions (i.e., 256×256).\nVisuals for IPC=1, 256×256. In Figure 5, we present\ndistilled images of a single class from ImageNet-B (Lorikeet)\nfor IPC=1, 256×256 using the LD3M model with different\ninitializations: ImageNet, FFHQ, and Random. The visual\nresults illustrate that distilled images generated from pruned\ncore-sets offer clearer and more distinct class representa-\ntions by removing class-unspecific details, such as the blue\nbackground when compared to the full dataset.\n5.2. Pruning Analysis\nPrior to the experiments presented so far, we analyzed\nthe impact of pruning settings on ImageNette, IPC=1 across\nthe three distillation methods with LD3M: DC, DM, and\nMTT. We evaluated relative dataset sizes in two ranges:\nfine-grained sizes from 1% to 9%, and broader sizes from\n10% to 100% in increments of 10%. For each relative size\nr ∈(0, 100), we apply the same relative size class-wise,\nmaintaining a representative subset that preserves class di-\nversity and minimizes bias.\nThe results of our first investigation are shown in Figure 6\n(more detailed Tables in the appendix). It shows the perfor-\nmance of DC, DM, and MTT methods as pruning is applied\nin steps from 10% to 100% (πeasy\nr\n). The general trend across\nall methods shows that higher pruning rates (lower percent-\nages) lead to fluctuation in the performance compared to the\ntraining on the full dataset. Moreover, it shows that MTT\nand DM can perform better than the entire dataset for various\n6\n\nDistillation Space\nInitialization\nAll\nImNet-A\nImNet-B\nImNet-C\nImNet-D\nImNet-E\npixel space\n-\n29.5±3.1\n38.3±4.7\n32.8±4.1\n27.6±3.3\n25.5±1.2\n23.5±2.4\nImageNet\n34.4±2.6\n37.4±5.5\n41.5±1.2\n35.7±4.0\n27.9±1.0\n29.3±1.2\nRandom\n34.5±1.6\n39.3±2.0\n40.3±1.7\n35.0±1.7\n27.9±1.4\n29.8±1.4\nGLaD\nFFHQ\n34.0±2.1\n38.3±5.2\n40.2±1.1\n34.9±1.1\n27.2±0.9\n29.4±2.1\nImageNet\n36.3±1.6\n42.1±2.2\n42.1±1.5\n35.7±1.7\n30.5±1.4\n30.9±1.2\nRandom\n36.5±1.6\n42.0±2.0\n41.9±1.7\n37.1±1.4\n30.5±1.5\n31.1±1.4\nLD3M\nFFHQ\n36.3±1.5\n42.0±1.6\n41.9±1.6\n36.5±2.2\n30.5±0.9\n30.6±1.1\nImageNet\n37.3±1.5\n42.6±1.9\n43.2±1.5\n37.6±1.6\n30.8±1.2\n32.4±1.2\n+1.0-0.1\n+0.5-0.3\n+1.1-0.7\n+1.1-0.6\n+0.3+0.3\n+1.5+0.0\nLD3M\nRandom\n37.4±1.3\n43.2±1.5\n43.6±1.7\n37.5±1.3\n30.5±1.1\n32.4±0.8\n(pruned)\n+0.9-0.3\n+1.2-0.5\n+1.7+0.0\n+0.4-0.1\n+0.0-0.4\n+1.3-0.6\nFFHQ\n37.3±1.3\n42.0±1.7\n43.6±1.2\n37.5±1.7\n31.0±1.1\n32.6±1.0\n+1.0-0.2\n+0.0+0.1\n+1.7-0.4\n+1.0-0.5\n+0.5-0.2\n+2.0-0.1\nTable 3. 256×256 distilled HR images using the DC distillation algorithm and IPC=1. For both scenarios (LD3M and GLaD), we evaluate\npre-trained generators on ImageNet [6], FFHQ [13], and randomly initialized.\nImageNet\nFFHQ\nRandom\nLD3M (pruned)\nLD3M\nReference Image\n(from orig. data)\nFigure 5. Example 256 × 256 images of a distilled class (ImageNet-B: Lorikeet) with differently initialized LD3M (pruned vs. full dataset).\nThe various initializations, i.e., which dataset was used for training the generators, are denoted at the bottom.\nrelative sizes, especially around the mid-size levels (60%-\n70%). Surprisingly, DM seems to improve the performance\nas the dataset size decreases significantly (see peak at 20%).\nThus, we took a deeper look at more fine-grained dataset\nsizes, specifically from 1% to 9%, which results are shown\nin Figure 7.\nHere, all methods show a sharper decline in accuracy as\nthe dataset size is reduced, especially for DC and MTT. Yet,\nwith only 3% of the dataset, DM can maintain the same\ndataset quality compared to the entire dataset. Lastly, Fig-\nure 8 provides the same comparison as our first investiga-\ntion but with descending pruning (πhard\nr\n). The plot clearly\nshows that descending pruning consistently leads to declin-\ning dataset quality for smaller dataset sizes, highlighting the\nimportance of easy samples, probably noisy-free samples,\nfor dataset distillation.\nThe results highlight that while all methods experience\naccuracy degradation with increased pruning, they also show\nvarious pruning ranges that lead to dataset quality that main-\ntains, and in some cases even improves, performance with\nreduced dataset sizes, particularly for DM. Conversely, de-\nscending pruning consistently reduces dataset quality, em-\n7\n\n100\n90\n80\n70\n60\n50\n40\n30\n20\n10\nDataset Size [%]\n26\n28\n30\n32\n34\n36\nAccuracy [%]\nDC\nDM\nMTT\nFigure 6. Performance evaluation of DC, DM, and MTT methods\non ImageNette with IPC=1. The results are shown with πeasy\nr\n,\nhorizontal lines denoting 100%.\n9\n8\n7\n6\n5\n4\n3\n2\n1\nDataset Size [%]\n24\n26\n28\n30\n32\nAccuracy [%]\nDC\nDM\nMTT\nFigure 7. Performance evaluation of DC, DM, and MTT methods\non ImageNette with IPC=1 with fine-grained pruning (1-9%) and\n100% for πeasy\nr\n(horizontal lines).\n100\n90\n80\n70\n60\n50\n40\n30\n20\n10\nDataset Size [%]\n22\n24\n26\n28\n30\n32\n34\nAccuracy [%]\nDC\nDM\nMTT\nFigure 8. Performance evaluation of DC, DM, and MTT methods\non ImageNette with IPC=1. The results are shown with πhard\nr\n,\nhorizontal lines denoting 100%.\nphasizing the critical role of easier samples in achieving\noptimal distillation outcomes.\nOverall, the optimal dataset size varies by distillation\nmethod and dataset. Relative sizes between 60-80% tend\nto yield the best performance for DC and MTT, while DM\nachieves strong results with dataset sizes closer to 20% (the\nvalues used in the experiments presented in the previous\nsection). This trend suggests that each distillation method\nbenefits from a unique balance between sample diversity and\ndata compactness, which pruning (πeasy\nr\n) helps to achieve by\nfocusing on those contributing the most to model learning.\n6. Limitations & Future Work\nOne limitation of our approach is determining the optimal\npruning factor r before starting the distillation process. This\npre-computation step requires a preliminary analysis to iden-\ntify the ideal amount of data to retain in the core-set, which\ncan add computational overhead. The r value may vary de-\npending on the dataset, architecture, and distillation method\nused, necessitating fine-tuning for each new scenario.\nFuture work should develop dynamic methods for de-\ntermining r, such as adaptive pruning strategies that adjust\nbased on the specific characteristics of the dataset and tar-\nget architecture. Such advancements would streamline the\npruning process and make it more applicable to a broader\nrange of datasets and architectures, further improving the\ngeneralizability of distilled datasets.\n7. Conclusion\nWe introduced a novel “Prune First, Distill After” frame-\nwork that combines loss-value-based sampling and pruning\nto optimize distillation quality, enhancing the performance\non unseen architectures. We effectively address critical lim-\nitations in existing dataset distillation approaches, such as\ncross-architecture generalization and data redundancy, by\nemphasizing the importance of “easier” samples, i.e., sam-\nples with low loss values, for dataset distillation. Experi-\nmental results on various ImageNet subsets demonstrate that\neven substantial pruning (e.g., up to removing 80% of the\noriginal dataset) prior to distillation maintains or, most of\nthe time, improves distillation quality, with accuracy gains\nreaching up to 5.2 percentage points on various subsets. This\nrobust performance across diverse architectures and pruning\nfactors showcase the scalability and generalizability of our\nframework, marking a significant advancement in the field\nof dataset distillation.\nAcknowledgements\nThis work was supported by the BMBF projects Sus-\ntainML (Grant 101070408), Albatross (Grant 01IW24002)\nand by Carl Zeiss Foundation through the Sustainable Em-\nbedded AI project (P2021-02-009).\n8\n\nReferences\n[1] Sharat Agarwal, Himanshu Arora, Saket Anand, and Chetan\nArora. Contextual diversity for active learning. In Computer\nVision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XVI 16, pages\n137–153. Springer, 2020. 1\n[2] George Cazenavette, Tongzhou Wang, Antonio Torralba,\nAlexei A Efros, and Jun-Yan Zhu. Dataset distillation by\nmatching training trajectories. In CVPR, 2022. 1, 2, 4\n[3] George Cazenavette, Tongzhou Wang, Antonio Torralba,\nAlexei A Efros, and Jun-Yan Zhu. Generalizing dataset distil-\nlation via deep generative prior. In CVPR, pages 3739–3748,\n2023. 1, 2, 4\n[4] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baha-\nran Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec,\nand Matei Zaharia. Selection via proxy: Efficient data se-\nlection for deep learning. arXiv preprint arXiv:1906.11829,\n2019. 1\n[5] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scaling\nup dataset distillation to imagenet-1k with constant memory.\nIn ICML, 2023. 1, 4\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn CVPR, 2009. 4, 7\n[7] Qingtang Ding, Zhengyu Liang, Longguang Wang, Yingqian\nWang, and Jungang Yang. Not all patches are equal: Hierar-\nchical dataset condensation for single image super-resolution.\nIEEE Signal Processing Letters, 2023. 1\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Transform-\ners for image recognition at scale. ICLR, 2020. 4\n[9] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\nvisual learning without forgetting. In CVPR, 2018. 4\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR, 2016.\n1, 4\n[11] Yang He, Lingao Xiao, and Tianyi Joey Zhou. You Only\nCondense Once: Two rules for pruning condensed datasets.\nIn Proceedings of the Advances in Neural Information Pro-\ncessing Systems (NeurIPS), 2023. 2\n[12] Jeremy Howard. A smaller subset of 10 easily classified\nclasses from imagenet, and a little more french.\nURL\nhttps://github. com/fastai/imagenette, 2019. 4\n[13] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nCVPR, pages 4401–4410, 2019. 7\n[14] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-\nagenet classification with deep convolutional neural networks.\nNeurIPS, 25, 2012. 4\n[15] Brian Moser, Federico Raue, J¨orn Hees, and Andreas Dengel.\nLess is more: Proxy datasets in nas approaches. In CVPR,\npages 1953–1961, 2022. 1\n[16] Brian B Moser, Federico Raue, and Andreas Dengel. A study\nin dataset pruning for image super-resolution. In International\nConference on Artificial Neural Networks, pages 351–363.\nSpringer, 2024. 1\n[17] Brian B Moser, Federico Raue, Sebastian Palacio, Stanislav\nFrolov, and Andreas Dengel. Latent dataset distillation with\ndiffusion models. arXiv preprint arXiv:2403.03881, 2024. 1,\n2, 4\n[18] Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset\nmeta-learning from kernel ridge-regression. arXiv preprint\narXiv:2011.00050, 2020. 4\n[19] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon\nLee. Dataset distillation with infinitely wide convolutional\nnetworks. NeurIPS, 2021. 4\n[20] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022. 1, 2,\n4\n[21] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-\nxl: Scaling stylegan to large diverse datasets. In ACM SIG-\nGRAPH 2022 conference proceedings, pages 1–10, 2022. 2,\n4\n[22] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. ICLR,\n2014. 4\n[23] Yue Xu, Yong-Lu Li, Kaitong Cui, Ziyu Wang, Cewu Lu,\nYu-Wing Tai, and Chi-Keung Tang. Distill gold from massive\nores: Bi-level data pruning towards efficient dataset distilla-\ntion. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), 2024. 3\n[24] Bo Zhao and Hakan Bilen. Dataset condensation with distri-\nbution matching. In WACV, 2023. 2\n[25] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset\ncondensation with gradient matching. ICLR, 2020. 2\n9",
    "pdf_filename": "Distill_the_Best,_Ignore_the_Rest_Improving_Dataset_Distillation_with_Loss-Value-Based_Pruning.pdf"
}