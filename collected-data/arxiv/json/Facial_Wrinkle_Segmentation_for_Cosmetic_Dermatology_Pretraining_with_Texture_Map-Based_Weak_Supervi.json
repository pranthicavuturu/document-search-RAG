{
    "title": "Facial Wrinkle Segmentation for Cosmetic Dermatology Pretraining with Texture Map-Based Weak Supervi",
    "context": "matology. Precise manual segmentation of facial wrinkles is challeng- ing and time-consuming, with inherent subjectivity leading to inconsis- tent results among graders. To address this issue, we propose two solu- tions. First, we build and release the first public facial wrinkle dataset, ‘FFHQ-Wrinkle’, an extension of the NVIDIA FFHQ dataset. It in- cludes 1,000 images with human labels and 50,000 images with auto- matically generated weak labels. This dataset could serve as a foun- dation for the research community to develop advanced wrinkle detec- tion algorithms. Second, we introduce a simple training strategy uti- lizing texture maps, applicable to various segmentation models, to de- tect wrinkles across the face. Our two-stage training strategy first pre- train models on a large dataset with weak labels (N=50k), or masked texture maps generated through computer vision techniques, without human intervention. We then finetune the models using human-labeled data (N=1k), which consists of manually labeled wrinkle masks. The net- work takes as input a combination of RGB and masked texture map of the image, comprising four channels, in finetuning. We effectively com- bine labels from multiple annotators to minimize subjectivity in man- ual labeling. Our strategies demonstrate improved segmentation perfor- mance in facial wrinkle segmentation both quantitatively and visually compared to existing pretraining methods. The dataset is available at https://github.com/labhai/ffhq-wrinkle-dataset. Keywords: Facial wrinkle segmentation · Weakly supervised learning · Texture map pretraining · Transfer learning 1 With the growing interest in dermatological diseases and skin aesthetics, pre- dicting facial wrinkles is becoming increasingly significant. Facial wrinkles serve ∗Corresponding authors arXiv:2408.10060v4  [cs.CV]  19 Nov 2024",
    "body": "Facial Wrinkle Segmentation for Cosmetic\nDermatology: Pretraining with Texture\nMap-Based Weak Supervision\nJunho Moon1[0009−0004−3522−6357], Haejun Chung∗1[0000−0001−8959−237X], and\nIkbeom Jang∗2[0000−0002−6901−983X]\n1 Hanyang University, Seoul 04763, Republic of Korea\n{jhmoon6807, haejun}@hanyang.ac.kr\n2 Hankuk University of Foreign Studies, Yongin 17035, Republic of Korea\nijang@hufs.ac.kr\nAbstract. Facial wrinkle detection plays a crucial role in cosmetic der-\nmatology. Precise manual segmentation of facial wrinkles is challeng-\ning and time-consuming, with inherent subjectivity leading to inconsis-\ntent results among graders. To address this issue, we propose two solu-\ntions. First, we build and release the first public facial wrinkle dataset,\n‘FFHQ-Wrinkle’, an extension of the NVIDIA FFHQ dataset. It in-\ncludes 1,000 images with human labels and 50,000 images with auto-\nmatically generated weak labels. This dataset could serve as a foun-\ndation for the research community to develop advanced wrinkle detec-\ntion algorithms. Second, we introduce a simple training strategy uti-\nlizing texture maps, applicable to various segmentation models, to de-\ntect wrinkles across the face. Our two-stage training strategy first pre-\ntrain models on a large dataset with weak labels (N=50k), or masked\ntexture maps generated through computer vision techniques, without\nhuman intervention. We then finetune the models using human-labeled\ndata (N=1k), which consists of manually labeled wrinkle masks. The net-\nwork takes as input a combination of RGB and masked texture map of\nthe image, comprising four channels, in finetuning. We effectively com-\nbine labels from multiple annotators to minimize subjectivity in man-\nual labeling. Our strategies demonstrate improved segmentation perfor-\nmance in facial wrinkle segmentation both quantitatively and visually\ncompared to existing pretraining methods. The dataset is available at\nhttps://github.com/labhai/ffhq-wrinkle-dataset.\nKeywords: Facial wrinkle segmentation · Weakly supervised learning ·\nTexture map pretraining · Transfer learning\n1\nIntroduction\nWith the growing interest in dermatological diseases and skin aesthetics, pre-\ndicting facial wrinkles is becoming increasingly significant. Facial wrinkles serve\n∗Corresponding authors\narXiv:2408.10060v4  [cs.CV]  19 Nov 2024\n\n2\nJ. Moon et al.\nFig. 1. Two-stage training for facial wrinkle segmentation. (a) Weakly supervised pre-\ntraining stage: the model learns to extract masked texture maps from RGB face images.\n(b) Supervised finetuning stage: the model refines its ability to extract facial wrinkles\nfrom RGB-masked face images and masked texture maps. The model parameters are\ninitialized with the weights from the weakly supervised pretraining stage.\nas critical indicators of aging [2,19,20], and are essential for evaluating skin\nconditions [29,13], diagnosing dermatological disorders [30], and planning pre-\ntreatment protocols for skin management [1,27]. Nevertheless, the manual de-\ntection of facial wrinkles poses considerable challenges. Accurate detection and\nanalysis of facial wrinkles necessitate a high level of expertise, typically available\nonly through well-trained professionals such as dermatologists. This process is\ntime-consuming and entails substantial costs due to the extensive time and effort\nrequired by the experts.\nRecently, numerous studies have focused on the automatic segmentation of fa-\ncial wrinkles through the application of deep learning techniques [25,26,14,15,4,34].\nNevertheless, these deep learning-based approaches are notably data-intensive.\nDue to the intricate distribution of facial wrinkles across the face, analyzing ex-\n\nFacial Wrinkle Segmentation\n3\ntensive collections of images can be exceedingly resource-intensive if each wrinkle\nmust be individually evaluated. Furthermore, the manual analysis procedure is\nfraught with subjectivity. The assessments of individual experts can differ signif-\nicantly based on their experience, level of training, and personal biases, thereby\ncomplicating the consistency and reproducibility of the analysis results.\nTo address these challenges, we propose a two-stage training strategy, as illus-\ntrated in Fig. 1. This approach utilizes computer vision techniques, specifically\nfilters, to generate many weakly labeled wrinkle masks (N=50,000) without hu-\nman intervention for weakly supervised pretraining. A smaller set of accurately\nlabeled wrinkle masks (N=1,000) is employed for supervised finetuning. This\nmethod significantly decreases the time and cost associated with manual wrin-\nkle labeling, providing substantial advantages over traditional methodologies.\nTo ensure the development of a generalized and robust model, we conducted\nexperiments using a dataset comprising images captured from various angles,\nlighting conditions, races, ages, and skin conditions. We quantitatively analyzed\nthe challenges associated with consistent manual wrinkle labeling across such\na diverse dataset and integrated data labeled by multiple annotators to reduce\nsubjectivity during the finetuning stage. No public dataset exists for full-face\nwrinkle segmentation, although there are a few private datasets. To address this\ngap, we have made our dataset publicly accessible to enhance the reproducibility\nand reliability of our results. This initiative aims to reduce the manual labeling\ncosts for future research and serve as a benchmark dataset.\n2\nRelated works\n2.1\nDeep learning-based facial wrinkle segmentation\nDeep learning-based methods for facial wrinkle segmentation aim to enable neu-\nral network models to learn the features necessary for accurate wrinkle detection\nautonomously. Kim et al. [14] introduced a semi-automatic labeling strategy to\nenhance performance by extracting texture maps from face images and combin-\ning them with roughly labeled wrinkle masks, utilizing a U-Net architecture [23]\nfor segmentation. In a subsequent study [15], they further improved segmentation\naccuracy by implementing a weighted deep supervision technique, which employs\na weighted wrinkle map to more precisely calculate the loss for the downsampled\ndecoder, outperforming traditional deep supervision methods. Yang et al. [34]\ndeveloped Striped WriNet, which integrates a Striped Attention Module com-\nposed of Multi-Scale Striped Attention and Global Striped Attention within a\nU-shaped network. This approach applies an attention mechanism across multi-\nple scales, effectively segmenting both coarse and fine wrinkles.\n2.2\nWeakly supervised learning\nWeakly supervised learning is a methodology that trains models using incomplete\nor inaccurate labeled data instead of fully labeled data in situations where strong\n\n4\nJ. Moon et al.\nFig. 2. Training Dataset. (a) High-resolution face images. (b) Masked texture maps\nextracted from face images, which include information about facial features. (c) Reliable\nmanual wrinkle masks created by combining the results of multiple annotators.\nsupervision information is lacking [36]. Xu et al. [33] proposed CAMEL, a weakly\nsupervised learning framework that uses a MIL-based label expansion technique\nto divide images into grid-shaped instances and automatically generate instance-\nlevel labels, enabling histopathology image segmentation with only image-level\nlabels. Shen et al. [11] trained a deep learning model using only scribbles on\nwhole tumors and healthy brain tissue, along with global labels for the presence\nof each substructure, to segment all sub-regions of brain tumors.\n3\nDataset\n3.1\nDataset specifications\nThe first public facial wrinkle dataset, ‘FFHQ-Wrinkle’, comprises pairs of face\nimages and their corresponding wrinkle masks. We focused on wrinkle labels\nwhile utilizing the existing face image dataset FFHQ (Flickr-Faces-HQ) [12],\nwhich contains 70,000 high-resolution (1024x1024) face images captured under\nvarious angles and lighting conditions. The dataset we provide consists of one\nset of manually labeled wrinkle masks (N=1,000) and one set of ‘weak’ wrinkle\nmasks, or masked texture maps, generated without human labor (N=50,000).\n\nFacial Wrinkle Segmentation\n5\nTable 1. Demographic attributes of the dataset. The ‘Human-labeled’ data represents\nthe 1,000 face images manually labeled by human annotators and the ‘Weakly-labeled’\ndata refers to the 50,000 images labeled without human intervention.\nDataset\nHuman-labeled\nWeakly-labeled\nSample size\n1000\n50000\nAge\n0-9 / 10-19 / 20-29 /\n66 / 68 / 233 /\n7030 / 4448 / 13804 /\n30-39 / 40-49 / 50-69 / 70+\n246 / 186 / 161 / 40\n10960 / 6931 / 5550 / 1277\nSex\nMale / Female\n471 / 529\n26929 / 23071\nRace/\nEthnicity\nWhite / Asian / Latino Hispanic /\n587 / 210 / 67 /\n29728 / 11121 / 3895 /\nBlack / Middle Eastern / Indian\n81 / 37 / 18 /\n2383 / 2053 / 820\nWe selected 50,000 images from the FFHQ dataset, specifically image IDs 00000\nto 49999. We used these 50,000 face images to create the weakly labeled wrin-\nkles and randomly sampled 1,000 images from these to create the ground truth\nwrinkles. The methods for generating weakly labeled wrinkles and ground truth\nwrinkles are discussed in Section 4.2. Table 1 summarizes estimated demographic\ninformation of the dataset–i.e. age, race, and sex. The age and sex data were\nsourced from the FFHQ-Aging [22] dataset, where at least three annotators\nlabeled each image. The race/ethnicity attribute was obtained through facial\nattribute analysis using the DeepFace‡ framework. Hence, the demographic in-\nformation may include errors. As illustrated in Fig. 2, the dataset consists of\nindividuals of varying ages, sex, and race/ethnicity, featuring a range of skin\nconditions such as freckles, acne, and pigmentation. This diversity makes the\ndataset particularly suitable for training models to handle the wide array of skin\nconditions encountered in clinical settings. The dataset is publicly available at\nhttps://github.com/labhai/ffhq-wrinkle-dataset.\n3.2\nGround truth wrinkle annotation\nFor ground truth wrinkles, we manually annotated the face images. The an-\nnotation process involved three annotators with extensive experience in image\nprocessing and analysis. Wrinkles can be categorized into two types—dynamic\nwrinkles and static wrinkles [31]. Dynamic wrinkles are formed by facial muscles\nand appear with expressions but disappear when the face is at rest. Static (per-\nmanent) wrinkles are visible even when the face is at rest and result from the\nrepeated formation of dynamic wrinkles over time. We annotated both types of\nwrinkles without distinguishing between them. Given the subjectivity inherent in\nwrinkle data, a consistent standard for wrinkle assessment was established prior\nto the commencement of labeling. The annotators conducted three synchroniza-\ntion sessions to minimize inter-rater variability. The annotation primarily tar-\ngeted the forehead, crow’s feet, and nasolabial folds, encompassing the overall\nfacial area. Due to the high resolution and diversity of the dataset—comprising\nvarious races, skin conditions, backgrounds, and angles—achieving consistent\nlabeling results proved challenging, even with established standards for wrinkle\n‡https://github.com/serengil/deepface\n\n6\nJ. Moon et al.\nFig. 3. Ambiguity in wrinkle evaluation. The labeling results from three annotators\nfor the same image are different.\nTable 2. Inter-rater agreement of manual wrinkle annotation. The Jaccard similarity\nindex and Pearson correlation coefficient between different annotators are analyzed.\nMetric\nAnnotators A&B\nAnnotators B&C\nAnnotators A&C\nAverage\nJaccard similarity index\n0.2631\n0.2962\n0.3182\n0.2925\nPearson correlation coefficient\n0.4167\n0.4559\n0.4928\n0.4551\nassessment, as illustrated in Fig. 3. Consequently, as demonstrated in Table 2,\nthe inter-rater agreement was low, underscoring the highly subjective nature of\nwrinkle assessments.\n4\nMethod\n4.1\nModel architecture\nWe evaluated our proposed method using the U-Net [23] and Swin UNETR [9]\narchitectures, with U-Net serving as the base model for ablation studies and\nadditional experiments. As depicted in Fig. 1, the U-Net model features a stan-\ndard architecture comprising four encoder blocks and four decoder blocks. The\nSwin UNETR model employs an encoder with a window size of 16 and patches\nof size 4x4, projecting the input patch into a 48-dimensional embedding space.\nThis model includes four encoder blocks, each consisting of two successive Swin\nTransformer blocks [16], and four decoder blocks.\n4.2\nTraining strategy\nWe train the segmentation model using a substantial number of masked texture\nmaps in a weakly supervised manner, followed by finetuning with a smaller set\nof reliably manually labeled wrinkle masks in a supervised manner. This train-\ning strategy, which involves finetuning the weights of a pretrained model that\nextracts facial textures using human-labeled wrinkle data, significantly enhances\nthe model’s capability to detect facial wrinkles. The overall training pipeline is\nillustrated in Fig. 1.\n\nFacial Wrinkle Segmentation\n7\nWeakly supervised pretraining stage In the pretraining stage, we utilized\nweakly labeled wrinkle data automatically extracted through computer vision\ntechniques without human intervention as the ground truth. Fig. 4 illustrates\nthe pipeline for generating weakly labeled wrinkles for the weakly supervised\npretraining stage. Utilizing Equation (1), we extracted the texture map [14]\nfrom the face image through a Gaussian kernel-based filter.\nT(x, y) = (1 −\nI(x, y)\n1 + IG(σ)(x, y)) × 255\n(1)\nwhere G represents the Gaussian kernel, σ denotes its standard deviation, IG(σ)\nis the Gaussian filtered image, and (x, y) are the pixel coordinates in the image.\nFollowing the methodology in [14], we set the Gaussian kernel’s standard devia-\ntion to 5 and its size to 21x21 for texture map extraction. The extracted texture\nmap contains detailed information about the contours, curves, and skin textures\nof the face image. However, as the texture map includes numerous false positives\nfrom the background, we employ a BiSeNet [35] architecture-based facial parsing\ndeep learning model§ to mask non-facial regions, resulting in the final masked\ntexture map used as ground truth. We avoid converting the masked texture map\ninto a binary mask due to the variability in the size, shape, and depth of wrin-\nkles, which makes determining an appropriate threshold challenging. Fig. 2-(b)\nshows the masked texture map used as the final ground truth in the weakly\nsupervised pretraining stage.\nIn the weakly supervised pretraining stage, the model takes a 3-channel RGB\nface image as input and outputs a 1-channel masked texture map (Fig. 1-(a)).\nWe use mean squared error (MSE) loss [21] to optimize the model, calculated as\nshown in equation (2).\nMSE(ˆy, y) = 1\nn\nn\nX\ni=1\n( ˆyi −yi)2\n(2)\nwhere ˆyi and yi are the model output and the masked texture map, respectively.\nSupervised finetuning stage For the ground truth in the finetuning stage,\nwe utilized human-labeled wrinkle data generated as described in Section 3.2.\nFig. 5 illustrates the pipeline of the ground truth generation of the wrinkle mask.\nTo produce a reliable ground truth wrinkle mask, we used majority voting to\nretain only the pixels that were labeled by at least two groups, thereby reducing\nvariability among the annotators. Fig. 2-(c) displays the manual wrinkle mask\nused as the final ground truth in the supervised finetuning stage. As model\ninputs, we use masked face images, where non-facial regions were masked using\na facial-parsing model. Additionally, we included masked texture maps, which\nwere used as ground truth in the pretraining stage, as auxiliary inputs.\nIn the supervised finetuning stage, the model takes as input a 3-channel RGB\nface image with only the facial regions and a 1-channel masked texture map.\n§https://github.com/zllrunning/face-parsing.PyTorch\n\n8\nJ. Moon et al.\nFig. 4. Weakly labeled wrinkle generation pipeline. After extracting the texture map\nfrom the face image, we mask the non-facial regions to generate a masked texture map\ncontaining information on facial features. This masked texture map is then used as a\nweakly labeled wrinkle.\nIt then produces a 2-channel output indicating the presence of wrinkles and\nbackground. This stage begins with the model parameters from the pretraining\nstage, where the model was weakly supervised to extract masked texture maps\nfrom face images. Using transfer learning, we refine the model by adjusting\nits weights with manually labeled wrinkle masks. This process enhances the\nmodel’s ability to detect facial wrinkles by building on the general facial texture\nextraction skills developed during pretraining. We optimize the model using soft\nDice loss [5], as shown in equation (3).\nDL(p, g) = 1 −1\nC\nC\nX\nc=1\n2 PN\ni=1 pi,cgi,c\nPN\ni=1 pi,c + PN\ni=1 gi,c\n(3)\nwhere C is the total number of classes, N is the total number of pixels, pi,c\nrepresents the predicted probability for pixel i belonging to class c, and gi,c\nrepresents the ground truth label for pixel i belonging to class c, respectively.\n5\nExperiments\n5.1\nImplementation details\nIn both the weakly supervised pretraining and supervised finetuning stages, we\nutilize the original 1024x1024 image-label pairs as inputs without resizing. The\nAdamW optimizer [18] is employed, configured with a weight decay of 0.05, β1\nset to 0.9, and β2 set to 0.999. We also implement the SGDR scheduler [17]. To\nmaintain dataset diversity, we randomly apply various augmentations, including\nhorizontal flipping, scaling, affine transformation, elastic transformation, grid\ndistortion, and optical distortion during training. The dataset is partitioned\ninto 80% for training, 10% for validation, and 10% for testing.\n\nFacial Wrinkle Segmentation\n9\nFig. 5. Ground truth wrinkle generation pipeline. We combine data labeled by multiple\nannotators through majority voting to create a reliable ground truth wrinkle.\nWeakly supervised pretraining stage In the weakly supervised pretraining\nstage, the model is trained for 300 epochs. The SGDR scheduler starts with an\ninitial period of 100 epochs, with the learning rate beginning at a maximum of\n0.001 and decaying to 0 over the period. At the end of each period, the length\nof the next period doubles that of the previous one. The batch size is 26 for\nU-Net and 22 for Swin UNETR. All pretraining processes were performed on an\nNVIDIA A100 Tensor Core GPU.\nSupervised finetuning stage In the supervised finetuning stage, the U-Net\nmodel is finetuned for 150 epochs, while the Swin UNETR model is finetuned for\n300 epochs. The batch size is 14 for both models. The SGDR scheduler’s initial\nperiod length is set to 50 epochs for U-Net and 100 epochs for Swin UNETR.\nThe learning rate starts at a maximum of 0.0001 and decreases to 0 within each\nperiod. At the end of each period, the length of the next period doubles that of\nthe previous one, with the maximum learning rate set to 90% of the last period’s\nmaximum. All finetuning processes are performed on RTX A6000 and RTX 6000\nAda GPUs.\n5.2\nEvaluation metrics\nTo evaluate the performance of the final finetuned model in wrinkle segmenta-\ntion, we use the Jaccard Similarity Index (JSI), F1-score, and Accuracy (Acc).\nThe Jaccard Similarity Index measures the overlap between the predicted\nwrinkle regions and the ground truth regions, defined as follows:\nJSI = |A ∩B|\n|A ∪B|\n(4)\n\n10\nJ. Moon et al.\nwhere A is the predicted segmentation, and B is the actual label.\nThe F1-score is the harmonic mean of precision and recall, while accuracy\nmeasures the proportion of correctly predicted pixels out of the total pixels.\nThey are defined as follows:\nPrecision =\nTP\nTP + FP\n(5)\nRecall =\nTP\nTP + FN\n(6)\nF1-score = 2 × Precision × Recall\nPrecision + Recall\n(7)\nAcc =\nTP + TN\nTP + TN + FP + FN\n(8)\nwhere TP is the number of true positives, FP is the number of false positives,\nFN is the number of false negatives, and TN is the number of true negatives.\n5.3\nResults\nTo evaluate the performance of our proposed method, we first compare it with\nthe latest methods: the semi-automatic labeling and weighted deep supervision\nmethod [15], and the Striped WriNet method [34]. Because the primary contri-\nbution of this work is the pretraining strategy, we also compare it with other\npretraining techniques. They include using ImageNet pretrained models and self-\nsupervised learning methods. For the ImageNet pretrained models, we replace\nthe encoder part of the U-shape architecture with models pretrained on the\nImageNet-1K dataset [24]; specifically, we use ResNet-50 [10] for U-Net and\nSwin-T [16] for Swin UNETR. For the self-supervised learning methods, we use\ndenoising self-supervised learning [3] for pretraining U-Net, setting the Gaussian\ndistribution’s standard deviation to 0.2, and masked image prediction [32] for\npretraining Swin UNETR, using 32x32 masked patches and a 60% masking ra-\ntio. All training hyperparameters follow those specified in Section 5.1. To assess\nperformance in scenarios with very limited labeled data, we train our model on\nthe full training set (100%, N=800) and on a randomly sampled subset (5%,\nN=40).\nThe proposed method outperforms the latest wrinkle segmentation meth-\nods and the ones using the same model architectures with different pertaining\nmethods. The performance gap is much larger in data-limited situations—i.e.,\nfine-tuned on 5% of the manually-labeled data. Table 3 shows quantitative com-\nparisons of wrinkle segmentation performance for each method using U-Net and\nSwin UNETR architectures. Our method consistently achieves the highest per-\nformance across both datasets and architectures. Fig. 6 presents a qualitative\ncomparison of our method with denoising pretraining using U-Net, which is the\nnext best performing method in experiments using 100% of the data.\n\nFacial Wrinkle Segmentation\n11\nTable 3. Quantitative comparisons of facial wrinkle segmentation performance. Our\nmethod is compared against two latest wrinkle segmentation methods, models trained\nwithout pretraining, and models using different pretraining strategies. These pretrain-\ning techniques include masked image prediction, denoising, and pretraining encoders\nusing the ImageNet-1K dataset.\nMethod\n100% (N=800)\n5% (N=40)\nnparams\nJSI\nF1-score\nAcc\nJSI\nF1-score\nAcc\nSemi automatic labeling + WDS [15]\n0.4552 0.6256 0.9954 0.3384 0.5057 0.9928 17.269M\nStriped WriNet [34]\n0.4665 0.6294 0.9956 0.2382 0.3761 0.9903 6.223M\nSwin UNETR\nwith pretraining\nNo pretraining\n0.4220 0.5858 0.9949 0.2545 0.3944 0.9932 25.153M\nImageNet-1K [24]\n(Swin-T [16])\n0.4385 0.6028 0.9952 0.2877 0.4351 0.9939 100.56M\nMasked image modeling [32] 0.4450 0.6079 0.9954 0.2963 0.4452 0.9937 25.153M\nTexture map (ours)\n0.4643 0.6271 0.9953 0.3416 0.4970\n0.9944 25.155M\nU-Net\nwith pretraining\nNo pretraining\n0.4638 0.6278 0.9955 0.3021 0.4551 0.9918 17.263M\nImageNet-1K [24]\n(ResNet-50 [10])\n0.4664 0.6296 0.9955 0.3428 0.5018 0.9934 32.521M\nDenoising [3]\n0.4709 0.6339 0.9955 0.2840 0.4338 0.9898 17.263M\nTexture map (ours)\n0.4831\n0.6442\n0.9957 0.3512\n0.5116 0.9929 17.264M\nTable 4. Ablation study of the effectiveness of adding a masked texture map as an\nadditional model input. We conduct experiments using U-Net. The segmentation per-\nformance improves when using the masked texture map as an additional input during\nfinetuning after texture map training.\nMethod\nModel input\n100% (N=800)\n5% (N=40)\nJSI\nF1-score\nAcc\nJSI\nF1-score\nAcc\nNo pretraining\nRGB (3-ch)\n0.4638 0.6278 0.9955 0.3021 0.4551 0.9918\nRGB+Texture (4-ch)\n0.4606 0.6221 0.9954 0.3208 0.4743 0.9924\nTexture map\npretraining\nRGB (3-ch)\n0.4796 0.6422\n0.9957 0.3442 0.5051 0.9919\nRGB+Texture (4-ch, ours) 0.4831\n0.6442\n0.9957 0.3512\n0.5116\n0.9929\n5.4\nAblation study\nIncorporating the masked texture map as an additional input during the fine-\ntuning stage led to significant improvements in wrinkle segmentation, demon-\nstrating the effectiveness of our approach. Table 4 presents quantitative compar-\nisons using the U-Net architecture to assess the benefits of including a 1-channel\nmasked texture map as an additional input during finetuning. We compare our\npretraining method (Texture map pretraining) with a conventional approach\n(No pretraining), which is trained solely on manually labeled data, both with\n(RGB+Texture) and without (RGB) the additional masked texture map input.\n6\nDiscussion\nOur approach achieves state-of-the-art performance when compared to two pub-\nlicly released models specifically designed for wrinkle segmentation, in addition\n\n12\nJ. Moon et al.\nFig. 6. Qualitative comparison against the denoising pretraining method. The blue\nboxes highlight areas with significant visual differences. (a) Face image. (b) Ground\ntruth wrinkle. (c) Predicted wrinkles from a model using self-supervised learning with\ndenoising pretraining, followed by finetuning with a manual wrinkle mask. (d) Predicted\nwrinkles from our model, trained with weak supervision using a masked texture map\nand then finetuned with a manual wrinkle mask.\nto outperforming ImageNet pretrained models and self-supervised learning meth-\nods. We demonstrate that our two-stage training strategy significantly enhances\nwrinkle segmentation efficiency. Furthermore, our approach shows the potential\nto achieve high performance with limited data, which could enhance scalability\nand flexibility in clinical settings. By using a large amount of weakly labeled\ndata obtained automatically through filters for weakly supervised training and\nthen finetuning with a small amount of reliable manually labeled data, we sig-\nnificantly reduce the time and cost required for manual labeling while improving\nthe segmentation performance of facial wrinkles. To minimize subjectivity in the\nmanual labeling process, we effectively combine data labeled by multiple an-\nnotators, resulting in more reliable training data. Additionally, to enhance the\nreproducibility of our research and reduce the manual labeling costs for sub-\nsequent studies, we release the dataset publicly available, which can also serve\nas a benchmark dataset for future research. The performance improvement of\nfacial wrinkle segmentation through transfer learning has not been conducted\nin previous research, indicating that our approach can be efficiently integrated\ninto various tasks related to facial wrinkle detection and segmentation tasks.\nAdditionally, since this research falls under the broader category of thin ob-\nject detection tasks, it is expected to be widely applicable to studies requiring\nsegmentation of thin objects (e.g., fundus imaging, vascular imaging).\n\nFacial Wrinkle Segmentation\n13\nFig. 7. Example of a false wrinkle detection. (a) Face image. (b) Masked face image\nused as the model input during the finetuning stage. (c) Visualization of the model’s\npredicted segmentation after the finetuning stage.\nAccording to our experimental results, the performance of the Swin UNETR,\na hybrid transformer-CNN architecture, is lower compared to the standard CNN-\nbased U-Net. In our case, the dataset used for finetuning is relatively small,\nmaking it insufficient to generalize transformer models, which primarily perform\nwell in data-intensive environments due to their low inductive bias [6]. Espe-\ncially in the case of wrinkles, the relationship between adjacent pixels (skin)\nplays a crucial role in their assessment. Therefore, the CNN-based standard\nU-Net, which excels at capturing local information, tends to outperform the\nSwin UNETR, which includes transformer blocks specialized in capturing global\ncontext through multi-head attention mechanisms. Nevertheless, our experimen-\ntal results show that the performance of Swin UNETR progressively improves\nthrough our method, suggesting that with more data and longer pretraining,\nthere is significant potential for performance enhancement. Note that accuracy\nis very high in all experiments since wrinkles occupy a very small proportion of\nthe face and most of the predictions are background pixels.\nHowever, our approach has limitations. As shown in Fig. 7, objects similar to\nwrinkles, such as hair or fingers covering the face, are mistakenly recognized as\nwrinkles in the images. This results in false positives during the wrinkle segmen-\ntation process. To address this issue, upcoming studies will focus on developing\ntechniques that can accurately segment facial regions and precisely distinguish\nbetween wrinkle and non-wrinkle areas to reduce false positives. Also, there may\nbe benefits to including the type of wrinkle (e.g., static vs. dynamic wrinkle)\nto each wrinkle in the facial image because treatment strategies often differ by\nthe type in clinics [28,8,7]. Despite majority voting, the subjectivity in wrinkle\nannotation remains a challenge. Moving forward, we plan to collaborate with\ndermatologists for wrinkle annotation and explore techniques such as soft label-\ning to improve the reliability and trustworthiness of ground truth wrinkles.\n7\nConclusion\nWe propose a two-stage learning strategy for facial wrinkle segmentation that\nleverages transfer learning from facial texture feature extraction. Specifically, the\n\n14\nJ. Moon et al.\nmodel is pretrained using automatically generated weak wrinkle labels (masked\ntexture maps) to learn general facial features such as contours and skin tex-\nture. The model is then finetuned with a smaller set of manually labeled wrinkle\ndata to enhance segmentation performance. This method demonstrates both\nqualitatively and quantitatively superior results, achieving state-of-the-art per-\nformance. Consequently, it significantly reduces the time and cost of manual\nwrinkle labeling, offering potential benefits in cosmetic dermatology. Addition-\nally, the pretraining method’s architecture-independent nature suggests its broad\napplicability to various segmentation models, making it valuable not only in fa-\ncial wrinkle segmentation but also in other areas requiring the segmentation\nof thin objects where manual labeling is costly. To support ongoing research\nand reproducibility, we have made the FFHQ-Wrinkle dataset—the first pub-\nlicly available dataset of its kind—accessible to the research community. This\ndataset comprises 1,000 manually labeled wrinkle images and 50,000 weakly la-\nbeled images. By sharing this dataset, we aim to facilitate the development of\nmore advanced wrinkle detection models and promote further advancements in\nthis field.\nAcknowledgements The authors appreciate Dr. Ik Jun Moon, a dermatolo-\ngist at Asan Medical Center, for sharing invaluable insights and feedback from\na dermatological perspective. This work was supported by the National Re-\nsearch Foundation of Korea (NRF) grants funded by the Ministry of Science\nand ICT (MSIT) (RS-2024-00455720 & RS-2024-00338048), the National In-\nstitute of Health(NIH) research project (2024ER040700), the National Super-\ncomputing Center with supercomputing resources including technical support\n(KSC-2024-CRE-0021), Hankuk University of Foreign Studies Research Fund of\n2024, the artificial intelligence semiconductor support program to nurture the\nbest talents (IITP(2024)-RS-2023-00253914) grant funded by the Korea govern-\nment, and the Culture, Sports and Tourism R&D Program through the Korea\nCreative Content Agency grant funded by the Ministry of Culture, Sports and\nTourism in 2024(RS-2024-00332210).\nReferences\n1. Allemann, I.B., Baumann, L.: Hyaluronic acid gel (juvéderm™) preparations in the\ntreatment of facial wrinkles and folds. Clinical interventions in aging 3(4), 629–634\n(2008)\n2. Aznar-Casanova, J., Torro-Alves, N., Fukusima, S.: How much older do you get\nwhen a wrinkle appears on your face? modifying age estimates by number of wrin-\nkles. Aging, Neuropsychology, and Cognition 17(4), 406–421 (2010)\n3. Brempong, E.A., Kornblith, S., Chen, T., Parmar, N., Minderer, M., Norouzi,\nM.: Denoising pretraining for semantic segmentation. In: Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition. pp. 4175–4186\n(2022)\n4. Chen, J., He, M., Cai, W.: Facial wrinkle detection with multiscale spatial feature\nfusion based on image enhancement and asff-seunet. Electronics 12(24),\n4897\n(2023)\n\nFacial Wrinkle Segmentation\n15\n5. Crum, W.R., Camara, O., Hill, D.L.: Generalized overlap measures for evaluation\nand validation in medical image analysis. IEEE transactions on medical imaging\n25(11), 1451–1461 (2006)\n6. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\n7. Gao, L., Song, W., Qian, L., Zhang, J., Li, K., Yang, J., Wang, G.: Clinical efficacy\nof different therapeutic modes of co2 fractional laser for treatment of static peri-\nocular wrinkles in asian skin. Journal of Cosmetic Dermatology 21(3), 1045–1050\n(2022)\n8. Goldman, A., et al.: Hyaluronic acid dermal fillers: Safety and efficacy for the\ntreatment of wrinkles, aging skin, body sculpturing and medical conditions. Clinical\nMedicine Reviews in Therapeutics 3 (2011)\n9. Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H.R., Xu, D.: Swin unetr:\nSwin transformers for semantic segmentation of brain tumors in mri images. In:\nInternational MICCAI Brainlesion Workshop. pp. 272–284. Springer (2021)\n10. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 770–778 (2016)\n11. Ji, Z., Shen, Y., Ma, C., Gao, M.: Scribble-based hierarchical weakly supervised\nlearning for brain tumor segmentation. In: Medical Image Computing and Com-\nputer Assisted Intervention–MICCAI 2019: 22nd International Conference, Shen-\nzhen, China, October 13–17, 2019, Proceedings, Part III 22. pp. 175–183. Springer\n(2019)\n12. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative\nadversarial networks. In: Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition. pp. 4401–4410 (2019)\n13. Kim, K., Choi, Y.H., Hwang, E.: Wrinkle feature-based skin age estimation scheme.\nIn: 2009 IEEE International Conference on Multimedia and Expo. pp. 1222–1225.\nIEEE (2009)\n14. Kim, S., Yoon, H., Lee, J., Yoo, S.: Semi-automatic labeling and training strat-\negy for deep learning-based facial wrinkle detection. In: 2022 IEEE 35th inter-\nnational symposium on computer-based medical systems (CBMS). pp. 383–388.\nIEEE (2022)\n15. Kim, S., Yoon, H., Lee, J., Yoo, S.: Facial wrinkle segmentation using weighted\ndeep supervision and semi-automatic labeling. Artificial Intelligence in Medicine\n145, 102679 (2023)\n16. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\ntransformer: Hierarchical vision transformer using shifted windows. In: Proceedings\nof the IEEE/CVF international conference on computer vision. pp. 10012–10022\n(2021)\n17. Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts.\narXiv preprint arXiv:1608.03983 (2016)\n18. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 (2017)\n19. Luu, K., Dai Bui, T., Suen, C.Y., Ricanek, K.: Combined local and holistic facial\nfeatures for age-determination. In: 2010 11th International Conference on Control\nAutomation Robotics & Vision. pp. 900–904. IEEE (2010)\n20. Ng, C.C., Yap, M.H., Cheng, Y.T., Hsu, G.S.: Hybrid ageing patterns for face age\nestimation. Image and Vision Computing 69, 92–102 (2018)\n\n16\nJ. Moon et al.\n21. Nix, D.A., Weigend, A.S.: Estimating the mean and variance of the target proba-\nbility distribution. In: Proceedings of 1994 ieee international conference on neural\nnetworks (ICNN’94). vol. 1, pp. 55–60. IEEE (1994)\n22. Or-El, R., Sengupta, S., Fried, O., Shechtman, E., Kemelmacher-Shlizerman, I.:\nLifespan age transformation synthesis. In: Computer Vision–ECCV 2020: 16th Eu-\nropean Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI 16.\npp. 739–755. Springer (2020)\n23. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\nical image segmentation. In: Medical image computing and computer-assisted\nintervention–MICCAI 2015: 18th international conference, Munich, Germany, Oc-\ntober 5-9, 2015, proceedings, part III 18. pp. 234–241. Springer (2015)\n24. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,\nKarpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog-\nnition challenge. International journal of computer vision 115, 211–252 (2015)\n25. Sabina, U., Whangbo, T.K.: Edge-based effective active appearance model for real-\ntime wrinkle detection. Skin Research and Technology 27(3), 444–452 (2021)\n26. Sabina, U., Whangbo, T.K.: Nasolabial wrinkle segmentation based on nested con-\nvolutional neural network. In: 2021 International Conference on Information and\nCommunication Technology Convergence (ICTC). pp. 483–485. IEEE (2021)\n27. Satriyasa, B.K.: Botulinum toxin (botox) a for reducing the appearance of facial\nwrinkles: a literature review of clinical use and pharmacological aspect. Clinical,\ncosmetic and investigational dermatology pp. 223–228 (2019)\n28. Small, R.: Botulinum toxin injection for facial wrinkles. American family physician\n90(3), 168–175 (2014)\n29. Warren, R., Gartstein, V., Kligman, A.M., Montagna, W., Allendorf, R.A., Ridder,\nG.M.: Age, sunlight, and facial skin: a histologic and quantitative study. Journal\nof the American Academy of Dermatology 25(5), 751–760 (1991)\n30. Wilder-Smith, E.P.: Stimulated skin wrinkling as an indicator of limb sympathetic\nfunction. Clinical Neurophysiology 126(1), 10–16 (2015)\n31. Wu, Y., Kalra, P., Thalmann, N.M.: Simulation of static and dynamic wrinkles of\nskin. In: Proceedings Computer Animation’96. pp. 90–97. IEEE (1996)\n32. Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q., Hu, H.: Simmim: A\nsimple framework for masked image modeling. In: Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition. pp. 9653–9663 (2022)\n33. Xu, G., Song, Z., Sun, Z., Ku, C., Yang, Z., Liu, C., Wang, S., Ma, J., Xu, W.:\nCamel: A weakly supervised learning framework for histopathology image segmen-\ntation. In: Proceedings of the IEEE/CVF International Conference on computer\nvision. pp. 10682–10691 (2019)\n34. Yang, M.Y., Shen, Q.L., Xu, D.T., Sun, X.L., Wu, Q.B.: Striped wrinet: Auto-\nmatic wrinkle segmentation based on striped attention module. Biomedical Signal\nProcessing and Control 90, 105817 (2024)\n35. Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N.: Bisenet: Bilateral segmenta-\ntion network for real-time semantic segmentation. In: Proceedings of the European\nconference on computer vision (ECCV). pp. 325–341 (2018)\n36. Zhou, Z.H.: A brief introduction to weakly supervised learning. National science\nreview 5(1), 44–53 (2018)",
    "pdf_filename": "Facial_Wrinkle_Segmentation_for_Cosmetic_Dermatology_Pretraining_with_Texture_Map-Based_Weak_Supervi.pdf"
}