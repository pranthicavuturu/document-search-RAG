{
    "title": "Facial Wrinkle Segmentation for Cosmetic",
    "abstract": "matology. Precise manual segmentation of facial wrinkles is challeng- ing and time-consuming, with inherent subjectivity leading to inconsis- tent results among graders. To address this issue, we propose two solu- tions. First, we build and release the first public facial wrinkle dataset, ‘FFHQ-Wrinkle’, an extension of the NVIDIA FFHQ dataset. It in- cludes 1,000 images with human labels and 50,000 images with auto- matically generated weak labels. This dataset could serve as a foun- dation for the research community to develop advanced wrinkle detec- tion algorithms. Second, we introduce a simple training strategy uti- lizing texture maps, applicable to various segmentation models, to de- tect wrinkles across the face. Our two-stage training strategy first pre- train models on a large dataset with weak labels (N=50k), or masked texture maps generated through computer vision techniques, without human intervention. We then finetune the models using human-labeled data(N=1k),whichconsistsofmanuallylabeledwrinklemasks.Thenet- work takes as input a combination of RGB and masked texture map of the image, comprising four channels, in finetuning. We effectively com- bine labels from multiple annotators to minimize subjectivity in man- ual labeling. Our strategies demonstrate improved segmentation perfor- mance in facial wrinkle segmentation both quantitatively and visually compared to existing pretraining methods. The dataset is available at https://github.com/labhai/ffhq-wrinkle-dataset.",
    "body": "Facial Wrinkle Segmentation for Cosmetic\nDermatology: Pretraining with Texture\nMap-Based Weak Supervision\nJunho Moon1[0009−0004−3522−6357], Haejun Chung∗1[0000−0001−8959−237X], and\nIkbeom Jang∗2[0000−0002−6901−983X]\n1 Hanyang University, Seoul 04763, Republic of Korea\n{jhmoon6807, haejun}@hanyang.ac.kr\n2 Hankuk University of Foreign Studies, Yongin 17035, Republic of Korea\nijang@hufs.ac.kr\nAbstract. Facialwrinkledetectionplaysacrucialroleincosmeticder-\nmatology. Precise manual segmentation of facial wrinkles is challeng-\ning and time-consuming, with inherent subjectivity leading to inconsis-\ntent results among graders. To address this issue, we propose two solu-\ntions. First, we build and release the first public facial wrinkle dataset,\n‘FFHQ-Wrinkle’, an extension of the NVIDIA FFHQ dataset. It in-\ncludes 1,000 images with human labels and 50,000 images with auto-\nmatically generated weak labels. This dataset could serve as a foun-\ndation for the research community to develop advanced wrinkle detec-\ntion algorithms. Second, we introduce a simple training strategy uti-\nlizing texture maps, applicable to various segmentation models, to de-\ntect wrinkles across the face. Our two-stage training strategy first pre-\ntrain models on a large dataset with weak labels (N=50k), or masked\ntexture maps generated through computer vision techniques, without\nhuman intervention. We then finetune the models using human-labeled\ndata(N=1k),whichconsistsofmanuallylabeledwrinklemasks.Thenet-\nwork takes as input a combination of RGB and masked texture map of\nthe image, comprising four channels, in finetuning. We effectively com-\nbine labels from multiple annotators to minimize subjectivity in man-\nual labeling. Our strategies demonstrate improved segmentation perfor-\nmance in facial wrinkle segmentation both quantitatively and visually\ncompared to existing pretraining methods. The dataset is available at\nhttps://github.com/labhai/ffhq-wrinkle-dataset.\nKeywords: Facialwrinklesegmentation·Weaklysupervisedlearning·\nTexture map pretraining · Transfer learning\n1 Introduction\nWith the growing interest in dermatological diseases and skin aesthetics, pre-\ndicting facial wrinkles is becoming increasingly significant. Facial wrinkles serve\n∗Corresponding authors\n4202\nvoN\n91\n]VC.sc[\n4v06001.8042:viXra\n2 J. Moon et al.\nFig.1.Two-stagetrainingforfacialwrinklesegmentation.(a)Weaklysupervisedpre-\ntrainingstage:themodellearnstoextractmaskedtexturemapsfromRGBfaceimages.\n(b) Supervised finetuning stage: the model refines its ability to extract facial wrinkles\nfrom RGB-masked face images and masked texture maps. The model parameters are\ninitialized with the weights from the weakly supervised pretraining stage.\nas critical indicators of aging [2,19,20], and are essential for evaluating skin\nconditions [29,13], diagnosing dermatological disorders [30], and planning pre-\ntreatment protocols for skin management [1,27]. Nevertheless, the manual de-\ntection of facial wrinkles poses considerable challenges. Accurate detection and\nanalysisoffacialwrinklesnecessitateahighlevelofexpertise,typicallyavailable\nonly through well-trained professionals such as dermatologists. This process is\ntime-consumingandentailssubstantialcostsduetotheextensivetimeandeffort\nrequired by the experts.\nRecently,numerousstudieshavefocusedontheautomaticsegmentationoffa-\ncialwrinklesthroughtheapplicationofdeeplearningtechniques[25,26,14,15,4,34].\nNevertheless, these deep learning-based approaches are notably data-intensive.\nDue to the intricate distribution of facial wrinkles across the face, analyzing ex-\nFacial Wrinkle Segmentation 3\ntensivecollectionsofimagescanbeexceedinglyresource-intensiveifeachwrinkle\nmust be individually evaluated. Furthermore, the manual analysis procedure is\nfraughtwithsubjectivity.Theassessmentsofindividualexpertscandiffersignif-\nicantly based on their experience, level of training, and personal biases, thereby\ncomplicating the consistency and reproducibility of the analysis results.\nToaddressthesechallenges,weproposeatwo-stagetrainingstrategy,asillus-\ntrated in Fig. 1. This approach utilizes computer vision techniques, specifically\nfilters, to generate many weakly labeled wrinkle masks (N=50,000) without hu-\nman intervention for weakly supervised pretraining. A smaller set of accurately\nlabeled wrinkle masks (N=1,000) is employed for supervised finetuning. This\nmethod significantly decreases the time and cost associated with manual wrin-\nkle labeling, providing substantial advantages over traditional methodologies.\nTo ensure the development of a generalized and robust model, we conducted\nexperiments using a dataset comprising images captured from various angles,\nlighting conditions, races, ages, and skin conditions. We quantitatively analyzed\nthe challenges associated with consistent manual wrinkle labeling across such\na diverse dataset and integrated data labeled by multiple annotators to reduce\nsubjectivity during the finetuning stage. No public dataset exists for full-face\nwrinklesegmentation,althoughthereareafewprivatedatasets.Toaddressthis\ngap,wehavemadeourdatasetpubliclyaccessibletoenhancethereproducibility\nand reliability of our results. This initiative aims to reduce the manual labeling\ncosts for future research and serve as a benchmark dataset.\n2 Related works\n2.1 Deep learning-based facial wrinkle segmentation\nDeeplearning-basedmethodsforfacialwrinklesegmentationaimtoenableneu-\nralnetworkmodelstolearnthefeaturesnecessaryforaccuratewrinkledetection\nautonomously. Kim et al. [14] introduced a semi-automatic labeling strategy to\nenhance performance by extracting texture maps from face images and combin-\ningthemwithroughlylabeledwrinklemasks,utilizingaU-Netarchitecture[23]\nforsegmentation.Inasubsequentstudy[15],theyfurtherimprovedsegmentation\naccuracybyimplementingaweighteddeepsupervisiontechnique,whichemploys\naweightedwrinklemaptomorepreciselycalculatethelossforthedownsampled\ndecoder, outperforming traditional deep supervision methods. Yang et al. [34]\ndeveloped Striped WriNet, which integrates a Striped Attention Module com-\nposed of Multi-Scale Striped Attention and Global Striped Attention within a\nU-shaped network. This approach applies an attention mechanism across multi-\nple scales, effectively segmenting both coarse and fine wrinkles.\n2.2 Weakly supervised learning\nWeaklysupervisedlearningisamethodologythattrainsmodelsusingincomplete\norinaccuratelabeleddatainsteadoffullylabeleddatainsituationswherestrong\n4 J. Moon et al.\nFig.2. Training Dataset. (a) High-resolution face images. (b) Masked texture maps\nextractedfromfaceimages,whichincludeinformationaboutfacialfeatures.(c)Reliable\nmanual wrinkle masks created by combining the results of multiple annotators.\nsupervisioninformationislacking[36].Xuetal.[33]proposedCAMEL,aweakly\nsupervisedlearningframeworkthatusesaMIL-basedlabelexpansiontechnique\ntodivideimagesintogrid-shapedinstancesandautomaticallygenerateinstance-\nlevel labels, enabling histopathology image segmentation with only image-level\nlabels. Shen et al. [11] trained a deep learning model using only scribbles on\nwhole tumors and healthy brain tissue, along with global labels for the presence\nof each substructure, to segment all sub-regions of brain tumors.\n3 Dataset\n3.1 Dataset specifications\nThe first public facial wrinkle dataset, ‘FFHQ-Wrinkle’, comprises pairs of face\nimages and their corresponding wrinkle masks. We focused on wrinkle labels\nwhile utilizing the existing face image dataset FFHQ (Flickr-Faces-HQ) [12],\nwhich contains 70,000 high-resolution (1024x1024) face images captured under\nvarious angles and lighting conditions. The dataset we provide consists of one\nset of manually labeled wrinkle masks (N=1,000) and one set of ‘weak’ wrinkle\nmasks, or masked texture maps, generated without human labor (N=50,000).\nFacial Wrinkle Segmentation 5\nTable 1.Demographicattributesofthedataset.The‘Human-labeled’datarepresents\nthe1,000faceimagesmanuallylabeledbyhumanannotatorsandthe‘Weakly-labeled’\ndata refers to the 50,000 images labeled without human intervention.\nDataset Human-labeled Weakly-labeled\nSamplesize 1000 50000\n0-9/10-19/20-29/ 66/68/233/ 7030/4448/13804/\nAge\n30-39/40-49/50-69/70+ 246/186/161/40 10960/6931/5550/1277\nSex Male/Female 471/529 26929/23071\nRace/ White/Asian/LatinoHispanic/ 587/210/67/ 29728/11121/3895/\nEthnicity Black/MiddleEastern/Indian 81/37/18/ 2383/2053/820\nWeselected50,000imagesfromtheFFHQdataset,specificallyimageIDs00000\nto 49999. We used these 50,000 face images to create the weakly labeled wrin-\nkles and randomly sampled 1,000 images from these to create the ground truth\nwrinkles. The methods for generating weakly labeled wrinkles and ground truth\nwrinklesarediscussedinSection4.2.Table1summarizesestimateddemographic\ninformation of the dataset–i.e. age, race, and sex. The age and sex data were\nsourced from the FFHQ-Aging [22] dataset, where at least three annotators\nlabeled each image. The race/ethnicity attribute was obtained through facial\nattribute analysis using the DeepFace‡ framework. Hence, the demographic in-\nformation may include errors. As illustrated in Fig. 2, the dataset consists of\nindividuals of varying ages, sex, and race/ethnicity, featuring a range of skin\nconditions such as freckles, acne, and pigmentation. This diversity makes the\ndatasetparticularlysuitablefortrainingmodelstohandlethewidearrayofskin\nconditions encountered in clinical settings. The dataset is publicly available at\nhttps://github.com/labhai/ffhq-wrinkle-dataset.\n3.2 Ground truth wrinkle annotation\nFor ground truth wrinkles, we manually annotated the face images. The an-\nnotation process involved three annotators with extensive experience in image\nprocessing and analysis. Wrinkles can be categorized into two types—dynamic\nwrinklesandstaticwrinkles[31].Dynamicwrinklesareformedbyfacialmuscles\nand appear with expressions but disappear when the face is at rest. Static (per-\nmanent) wrinkles are visible even when the face is at rest and result from the\nrepeated formation of dynamic wrinkles over time. We annotated both types of\nwrinkleswithoutdistinguishingbetweenthem.Giventhesubjectivityinherentin\nwrinkle data, a consistent standard for wrinkle assessment was established prior\nto the commencement of labeling. The annotators conducted three synchroniza-\ntion sessions to minimize inter-rater variability. The annotation primarily tar-\ngeted the forehead, crow’s feet, and nasolabial folds, encompassing the overall\nfacial area. Due to the high resolution and diversity of the dataset—comprising\nvarious races, skin conditions, backgrounds, and angles—achieving consistent\nlabeling results proved challenging, even with established standards for wrinkle\n‡https://github.com/serengil/deepface\n6 J. Moon et al.\nFig.3. Ambiguity in wrinkle evaluation. The labeling results from three annotators\nfor the same image are different.\nTable 2. Inter-rater agreement of manual wrinkle annotation. The Jaccard similarity\nindex and Pearson correlation coefficient between different annotators are analyzed.\nMetric AnnotatorsA&B AnnotatorsB&C AnnotatorsA&C Average\nJaccardsimilarityindex 0.2631 0.2962 0.3182 0.2925\nPearsoncorrelationcoefficient 0.4167 0.4559 0.4928 0.4551\nassessment, as illustrated in Fig. 3. Consequently, as demonstrated in Table 2,\nthe inter-rater agreement was low, underscoring the highly subjective nature of\nwrinkle assessments.\n4 Method\n4.1 Model architecture\nWe evaluated our proposed method using the U-Net [23] and Swin UNETR [9]\narchitectures, with U-Net serving as the base model for ablation studies and\nadditional experiments. As depicted in Fig. 1, the U-Net model features a stan-\ndard architecture comprising four encoder blocks and four decoder blocks. The\nSwin UNETR model employs an encoder with a window size of 16 and patches\nof size 4x4, projecting the input patch into a 48-dimensional embedding space.\nThis model includes four encoder blocks, each consisting of two successive Swin\nTransformer blocks [16], and four decoder blocks.\n4.2 Training strategy\nWe train the segmentation model using a substantial number of masked texture\nmaps in a weakly supervised manner, followed by finetuning with a smaller set\nof reliably manually labeled wrinkle masks in a supervised manner. This train-\ning strategy, which involves finetuning the weights of a pretrained model that\nextractsfacialtexturesusinghuman-labeledwrinkledata,significantlyenhances\nthe model’s capability to detect facial wrinkles. The overall training pipeline is\nillustrated in Fig. 1.\nFacial Wrinkle Segmentation 7\nWeakly supervised pretraining stage In the pretraining stage, we utilized\nweakly labeled wrinkle data automatically extracted through computer vision\ntechniques without human intervention as the ground truth. Fig. 4 illustrates\nthe pipeline for generating weakly labeled wrinkles for the weakly supervised\npretraining stage. Utilizing Equation (1), we extracted the texture map [14]\nfrom the face image through a Gaussian kernel-based filter.\nI(x,y)\nT(x,y)=(1− )×255 (1)\n1+I (x,y)\nG(σ)\nwhere G represents the Gaussian kernel, σ denotes its standard deviation, I\nG(σ)\nis the Gaussian filtered image, and (x,y) are the pixel coordinates in the image.\nFollowing the methodology in [14], we set the Gaussian kernel’s standard devia-\ntionto5anditssizeto21x21fortexturemapextraction.Theextractedtexture\nmapcontainsdetailedinformationaboutthecontours,curves,andskintextures\nofthefaceimage.However,asthetexturemapincludesnumerousfalsepositives\nfromthebackground,weemployaBiSeNet[35]architecture-basedfacialparsing\ndeep learning model§ to mask non-facial regions, resulting in the final masked\ntexturemapusedasgroundtruth.Weavoidconvertingthemaskedtexturemap\ninto a binary mask due to the variability in the size, shape, and depth of wrin-\nkles, which makes determining an appropriate threshold challenging. Fig. 2-(b)\nshows the masked texture map used as the final ground truth in the weakly\nsupervised pretraining stage.\nIntheweaklysupervisedpretrainingstage,themodeltakesa3-channelRGB\nface image as input and outputs a 1-channel masked texture map (Fig. 1-(a)).\nWeusemeansquarederror(MSE)loss[21]tooptimizethemodel,calculatedas\nshown in equation (2).\nn\nMSE(yˆ,y)= 1 (cid:88) (yˆ −y )2 (2)\nn i i\ni=1\nwhereyˆ andy arethemodeloutputandthemaskedtexturemap,respectively.\ni i\nSupervised finetuning stage For the ground truth in the finetuning stage,\nwe utilized human-labeled wrinkle data generated as described in Section 3.2.\nFig.5illustratesthepipelineofthegroundtruthgenerationofthewrinklemask.\nTo produce a reliable ground truth wrinkle mask, we used majority voting to\nretainonlythepixelsthatwerelabeledbyatleasttwogroups,therebyreducing\nvariability among the annotators. Fig. 2-(c) displays the manual wrinkle mask\nused as the final ground truth in the supervised finetuning stage. As model\ninputs, we use masked face images, where non-facial regions were masked using\na facial-parsing model. Additionally, we included masked texture maps, which\nwere used as ground truth in the pretraining stage, as auxiliary inputs.\nInthesupervisedfinetuningstage,themodeltakesasinputa3-channelRGB\nface image with only the facial regions and a 1-channel masked texture map.\n§https://github.com/zllrunning/face-parsing.PyTorch\n8 J. Moon et al.\nFig.4. Weakly labeled wrinkle generation pipeline. After extracting the texture map\nfromthefaceimage,wemaskthenon-facialregionstogenerateamaskedtexturemap\ncontaining information on facial features. This masked texture map is then used as a\nweakly labeled wrinkle.\nIt then produces a 2-channel output indicating the presence of wrinkles and\nbackground. This stage begins with the model parameters from the pretraining\nstage, where the model was weakly supervised to extract masked texture maps\nfrom face images. Using transfer learning, we refine the model by adjusting\nits weights with manually labeled wrinkle masks. This process enhances the\nmodel’sabilitytodetectfacialwrinklesbybuildingonthegeneralfacialtexture\nextractionskillsdevelopedduringpretraining.Weoptimizethemodelusingsoft\nDice loss [5], as shown in equation (3).\nDL(p,g)=1−\n1\n(cid:88)C 2(cid:80)N\ni=1p i,cg\ni,c (3)\nC (cid:80)N p +(cid:80)N g\nc=1 i=1 i,c i=1 i,c\nwhere C is the total number of classes, N is the total number of pixels, p\ni,c\nrepresents the predicted probability for pixel i belonging to class c, and g\ni,c\nrepresents the ground truth label for pixel i belonging to class c, respectively.\n5 Experiments\n5.1 Implementation details\nIn both the weakly supervised pretraining and supervised finetuning stages, we\nutilize the original 1024x1024 image-label pairs as inputs without resizing. The\nAdamW optimizer [18] is employed, configured with a weight decay of 0.05, β\n1\nset to 0.9, and β set to 0.999. We also implement the SGDR scheduler [17]. To\n2\nmaintaindatasetdiversity,werandomlyapplyvariousaugmentations,including\nhorizontal flipping, scaling, affine transformation, elastic transformation, grid\ndistortion, and optical distortion during training. The dataset is partitioned\ninto 80% for training, 10% for validation, and 10% for testing.\nFacial Wrinkle Segmentation 9\nFig.5.Groundtruthwrinklegenerationpipeline.Wecombinedatalabeledbymultiple\nannotators through majority voting to create a reliable ground truth wrinkle.\nWeakly supervised pretraining stage In the weakly supervised pretraining\nstage, the model is trained for 300 epochs. The SGDR scheduler starts with an\ninitial period of 100 epochs, with the learning rate beginning at a maximum of\n0.001 and decaying to 0 over the period. At the end of each period, the length\nof the next period doubles that of the previous one. The batch size is 26 for\nU-Netand22forSwinUNETR.Allpretrainingprocesseswereperformedonan\nNVIDIA A100 Tensor Core GPU.\nSupervised finetuning stage In the supervised finetuning stage, the U-Net\nmodelisfinetunedfor150epochs,whiletheSwinUNETRmodelisfinetunedfor\n300 epochs. The batch size is 14 for both models. The SGDR scheduler’s initial\nperiod length is set to 50 epochs for U-Net and 100 epochs for Swin UNETR.\nThe learning rate starts at a maximum of 0.0001 and decreases to 0 within each\nperiod. At the end of each period, the length of the next period doubles that of\nthepreviousone,withthemaximumlearningratesetto90%ofthelastperiod’s\nmaximum.AllfinetuningprocessesareperformedonRTXA6000andRTX6000\nAda GPUs.\n5.2 Evaluation metrics\nTo evaluate the performance of the final finetuned model in wrinkle segmenta-\ntion, we use the Jaccard Similarity Index (JSI), F1-score, and Accuracy (Acc).\nThe Jaccard Similarity Index measures the overlap between the predicted\nwrinkle regions and the ground truth regions, defined as follows:\n|A∩B|\nJSI= (4)\n|A∪B|\n10 J. Moon et al.\nwhere A is the predicted segmentation, and B is the actual label.\nThe F1-score is the harmonic mean of precision and recall, while accuracy\nmeasures the proportion of correctly predicted pixels out of the total pixels.\nThey are defined as follows:\nTP\nPrecision= (5)\nTP +FP\nTP\nRecall= (6)\nTP +FN\nPrecision×Recall\nF1-score=2× (7)\nPrecision+Recall\nTP +TN\nAcc= (8)\nTP +TN +FP +FN\nwhere TP is the number of true positives, FP is the number of false positives,\nFN is the number of false negatives, and TN is the number of true negatives.\n5.3 Results\nTo evaluate the performance of our proposed method, we first compare it with\nthe latest methods: the semi-automatic labeling and weighted deep supervision\nmethod [15], and the Striped WriNet method [34]. Because the primary contri-\nbution of this work is the pretraining strategy, we also compare it with other\npretrainingtechniques.TheyincludeusingImageNetpretrainedmodelsandself-\nsupervised learning methods. For the ImageNet pretrained models, we replace\nthe encoder part of the U-shape architecture with models pretrained on the\nImageNet-1K dataset [24]; specifically, we use ResNet-50 [10] for U-Net and\nSwin-T [16] for Swin UNETR. For the self-supervised learning methods, we use\ndenoisingself-supervisedlearning[3]forpretrainingU-Net,settingtheGaussian\ndistribution’s standard deviation to 0.2, and masked image prediction [32] for\npretraining Swin UNETR, using 32x32 masked patches and a 60% masking ra-\ntio. All training hyperparameters follow those specified in Section 5.1. To assess\nperformance in scenarios with very limited labeled data, we train our model on\nthe full training set (100%, N=800) and on a randomly sampled subset (5%,\nN=40).\nThe proposed method outperforms the latest wrinkle segmentation meth-\nods and the ones using the same model architectures with different pertaining\nmethods. The performance gap is much larger in data-limited situations—i.e.,\nfine-tunedon5%ofthemanually-labeleddata.Table3showsquantitativecom-\nparisons ofwrinkle segmentationperformance for each method using U-Net and\nSwin UNETR architectures. Our method consistently achieves the highest per-\nformance across both datasets and architectures. Fig. 6 presents a qualitative\ncomparison of our method with denoising pretraining using U-Net, which is the\nnext best performing method in experiments using 100% of the data.\nFacial Wrinkle Segmentation 11\nTable 3. Quantitative comparisons of facial wrinkle segmentation performance. Our\nmethodiscomparedagainsttwolatestwrinklesegmentationmethods,modelstrained\nwithoutpretraining,andmodelsusingdifferentpretrainingstrategies.Thesepretrain-\ning techniques include masked image prediction, denoising, and pretraining encoders\nusing the ImageNet-1K dataset.\n100%(N=800) 5%(N=40)\nMethod\nJSI F1-score Acc JSI F1-score Acc\nnparams\nSemiautomaticlabeling+WDS[15] 0.4552 0.6256 0.99540.3384 0.5057 0.992817.269M\nStripedWriNet[34] 0.4665 0.6294 0.99560.2382 0.3761 0.9903 6.223M\nNopretraining 0.4220 0.5858 0.99490.2545 0.3944 0.993225.153M\nSwinUNETR ImageNet-1K[24]\n0.4385 0.6028 0.99520.2877 0.4351 0.9939100.56M\nwithpretraining (Swin-T[16])\nMaskedimagemodeling[32]0.4450 0.6079 0.99540.2963 0.4452 0.993725.153M\nTexturemap(ours) 0.4643 0.6271 0.99530.3416 0.4970 0.994425.155M\nNopretraining 0.4638 0.6278 0.99550.3021 0.4551 0.991817.263M\nU-Net ImageNet-1K[24]\n0.4664 0.6296 0.99550.3428 0.5018 0.993432.521M\nwithpretraining (ResNet-50[10])\nDenoising[3] 0.4709 0.6339 0.99550.2840 0.4338 0.989817.263M\nTexturemap(ours) 0.4831 0.6442 0.99570.3512 0.5116 0.992917.264M\nTable 4. Ablation study of the effectiveness of adding a masked texture map as an\nadditional model input. We conduct experiments using U-Net. The segmentation per-\nformance improves when using the masked texture map as an additional input during\nfinetuning after texture map training.\n100%(N=800) 5%(N=40)\nMethod Modelinput\nJSI F1-score Acc JSI F1-score Acc\nRGB(3-ch) 0.4638 0.6278 0.9955 0.3021 0.4551 0.9918\nNopretraining\nRGB+Texture(4-ch) 0.4606 0.6221 0.9954 0.3208 0.4743 0.9924\nTexturemap RGB(3-ch) 0.4796 0.6422 0.9957 0.3442 0.5051 0.9919\npretraining RGB+Texture(4-ch,ours) 0.4831 0.6442 0.9957 0.3512 0.5116 0.9929\n5.4 Ablation study\nIncorporating the masked texture map as an additional input during the fine-\ntuning stage led to significant improvements in wrinkle segmentation, demon-\nstratingtheeffectivenessofourapproach.Table4presentsquantitativecompar-\nisonsusingtheU-Netarchitecturetoassessthebenefitsofincludinga1-channel\nmasked texture map as an additional input during finetuning. We compare our\npretraining method (Texture map pretraining) with a conventional approach\n(No pretraining), which is trained solely on manually labeled data, both with\n(RGB+Texture) and without (RGB) the additional masked texture map input.\n6 Discussion\nOurapproachachievesstate-of-the-artperformancewhencomparedtotwopub-\nlicly released models specifically designed for wrinkle segmentation, in addition\n12 J. Moon et al.\nFig.6. Qualitative comparison against the denoising pretraining method. The blue\nboxes highlight areas with significant visual differences. (a) Face image. (b) Ground\ntruthwrinkle.(c)Predictedwrinklesfromamodelusingself-supervisedlearningwith\ndenoisingpretraining,followedbyfinetuningwithamanualwrinklemask.(d)Predicted\nwrinkles from our model, trained with weak supervision using a masked texture map\nand then finetuned with a manual wrinkle mask.\ntooutperformingImageNetpretrainedmodelsandself-supervisedlearningmeth-\nods.Wedemonstratethatourtwo-stagetrainingstrategysignificantlyenhances\nwrinkle segmentation efficiency. Furthermore, our approach shows the potential\nto achieve high performance with limited data, which could enhance scalability\nand flexibility in clinical settings. By using a large amount of weakly labeled\ndata obtained automatically through filters for weakly supervised training and\nthen finetuning with a small amount of reliable manually labeled data, we sig-\nnificantlyreducethetimeandcostrequiredformanuallabelingwhileimproving\nthesegmentationperformanceoffacialwrinkles.Tominimizesubjectivityinthe\nmanual labeling process, we effectively combine data labeled by multiple an-\nnotators, resulting in more reliable training data. Additionally, to enhance the\nreproducibility of our research and reduce the manual labeling costs for sub-\nsequent studies, we release the dataset publicly available, which can also serve\nas a benchmark dataset for future research. The performance improvement of\nfacial wrinkle segmentation through transfer learning has not been conducted\nin previous research, indicating that our approach can be efficiently integrated\ninto various tasks related to facial wrinkle detection and segmentation tasks.\nAdditionally, since this research falls under the broader category of thin ob-\nject detection tasks, it is expected to be widely applicable to studies requiring\nsegmentation of thin objects (e.g., fundus imaging, vascular imaging).\nFacial Wrinkle Segmentation 13\nFig.7. Example of a false wrinkle detection. (a) Face image. (b) Masked face image\nused as the model input during the finetuning stage. (c) Visualization of the model’s\npredicted segmentation after the finetuning stage.\nAccordingtoourexperimentalresults,theperformanceoftheSwinUNETR,\nahybridtransformer-CNNarchitecture,islowercomparedtothestandardCNN-\nbased U-Net. In our case, the dataset used for finetuning is relatively small,\nmakingitinsufficienttogeneralizetransformermodels,whichprimarilyperform\nwell in data-intensive environments due to their low inductive bias [6]. Espe-\ncially in the case of wrinkles, the relationship between adjacent pixels (skin)\nplays a crucial role in their assessment. Therefore, the CNN-based standard\nU-Net, which excels at capturing local information, tends to outperform the\nSwinUNETR,whichincludestransformerblocksspecializedincapturingglobal\ncontextthroughmulti-headattentionmechanisms.Nevertheless,ourexperimen-\ntal results show that the performance of Swin UNETR progressively improves\nthrough our method, suggesting that with more data and longer pretraining,\nthere is significant potential for performance enhancement. Note that accuracy\nis very high in all experiments since wrinkles occupy a very small proportion of\nthe face and most of the predictions are background pixels.\nHowever,ourapproachhaslimitations.AsshowninFig.7,objectssimilarto\nwrinkles, such as hair or fingers covering the face, are mistakenly recognized as\nwrinklesintheimages.Thisresultsinfalsepositivesduringthewrinklesegmen-\ntation process. To address this issue, upcoming studies will focus on developing\ntechniques that can accurately segment facial regions and precisely distinguish\nbetweenwrinkleandnon-wrinkleareastoreducefalsepositives.Also,theremay\nbe benefits to including the type of wrinkle (e.g., static vs. dynamic wrinkle)\nto each wrinkle in the facial image because treatment strategies often differ by\nthe type in clinics [28,8,7]. Despite majority voting, the subjectivity in wrinkle\nannotation remains a challenge. Moving forward, we plan to collaborate with\ndermatologists for wrinkle annotation and explore techniques such as soft label-\ning to improve the reliability and trustworthiness of ground truth wrinkles.\n7 Conclusion\nWe propose a two-stage learning strategy for facial wrinkle segmentation that\nleveragestransferlearningfromfacialtexturefeatureextraction.Specifically,the\n14 J. Moon et al.\nmodel is pretrained using automatically generated weak wrinkle labels (masked\ntexture maps) to learn general facial features such as contours and skin tex-\nture.Themodelisthenfinetunedwithasmallersetofmanuallylabeledwrinkle\ndata to enhance segmentation performance. This method demonstrates both\nqualitatively and quantitatively superior results, achieving state-of-the-art per-\nformance. Consequently, it significantly reduces the time and cost of manual\nwrinkle labeling, offering potential benefits in cosmetic dermatology. Addition-\nally,thepretrainingmethod’sarchitecture-independentnaturesuggestsitsbroad\napplicability to various segmentation models, making it valuable not only in fa-\ncial wrinkle segmentation but also in other areas requiring the segmentation\nof thin objects where manual labeling is costly. To support ongoing research\nand reproducibility, we have made the FFHQ-Wrinkle dataset—the first pub-\nlicly available dataset of its kind—accessible to the research community. This\ndataset comprises 1,000 manually labeled wrinkle images and 50,000 weakly la-\nbeled images. By sharing this dataset, we aim to facilitate the development of\nmore advanced wrinkle detection models and promote further advancements in\nthis field.\nAcknowledgements The authors appreciate Dr. Ik Jun Moon, a dermatolo-\ngist at Asan Medical Center, for sharing invaluable insights and feedback from\na dermatological perspective. This work was supported by the National Re-\nsearch Foundation of Korea (NRF) grants funded by the Ministry of Science\nand ICT (MSIT) (RS-2024-00455720 & RS-2024-00338048), the National In-\nstitute of Health(NIH) research project (2024ER040700), the National Super-\ncomputing Center with supercomputing resources including technical support\n(KSC-2024-CRE-0021), Hankuk University of Foreign Studies Research Fund of\n2024, the artificial intelligence semiconductor support program to nurture the\nbest talents (IITP(2024)-RS-2023-00253914) grant funded by the Korea govern-\nment, and the Culture, Sports and Tourism R&D Program through the Korea\nCreative Content Agency grant funded by the Ministry of Culture, Sports and\nTourism in 2024(RS-2024-00332210).\nReferences\n1. Allemann,I.B.,Baumann,L.:Hyaluronicacidgel(juvéderm™)preparationsinthe\ntreatmentoffacialwrinklesandfolds.Clinicalinterventionsinaging3(4),629–634\n(2008)\n2. Aznar-Casanova, J., Torro-Alves, N., Fukusima, S.: How much older do you get\nwhenawrinkleappearsonyourface?modifyingageestimatesbynumberofwrin-\nkles. Aging, Neuropsychology, and Cognition 17(4), 406–421 (2010)\n3. Brempong, E.A., Kornblith, S., Chen, T., Parmar, N., Minderer, M., Norouzi,\nM.: Denoising pretraining for semantic segmentation. In: Proceedings of the\nIEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.4175–4186\n(2022)\n4. Chen,J.,He,M.,Cai,W.:Facialwrinkledetectionwithmultiscalespatialfeature\nfusion based on image enhancement and asff-seunet. Electronics 12(24), 4897\n(2023)\nFacial Wrinkle Segmentation 15\n5. Crum, W.R., Camara, O., Hill, D.L.: Generalized overlap measures for evaluation\nand validation in medical image analysis. IEEE transactions on medical imaging\n25(11), 1451–1461 (2006)\n6. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\n7. Gao,L.,Song,W.,Qian,L.,Zhang,J.,Li,K.,Yang,J.,Wang,G.:Clinicalefficacy\nof different therapeutic modes of co2 fractional laser for treatment of static peri-\nocular wrinkles in asian skin. Journal of Cosmetic Dermatology 21(3), 1045–1050\n(2022)\n8. Goldman, A., et al.: Hyaluronic acid dermal fillers: Safety and efficacy for the\ntreatmentofwrinkles,agingskin,bodysculpturingandmedicalconditions.Clinical\nMedicine Reviews in Therapeutics 3 (2011)\n9. Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H.R., Xu, D.: Swin unetr:\nSwin transformers for semantic segmentation of brain tumors in mri images. In:\nInternational MICCAI Brainlesion Workshop. pp. 272–284. Springer (2021)\n10. He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimagerecognition.In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 770–778 (2016)\n11. Ji, Z., Shen, Y., Ma, C., Gao, M.: Scribble-based hierarchical weakly supervised\nlearning for brain tumor segmentation. In: Medical Image Computing and Com-\nputer Assisted Intervention–MICCAI 2019: 22nd International Conference, Shen-\nzhen,China,October13–17,2019,Proceedings,PartIII22.pp.175–183.Springer\n(2019)\n12. Karras,T.,Laine,S.,Aila,T.:Astyle-basedgeneratorarchitectureforgenerative\nadversarial networks. In: Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition. pp. 4401–4410 (2019)\n13. Kim,K.,Choi,Y.H.,Hwang,E.:Wrinklefeature-basedskinageestimationscheme.\nIn:2009IEEEInternationalConferenceonMultimediaandExpo.pp.1222–1225.\nIEEE (2009)\n14. Kim, S., Yoon, H., Lee, J., Yoo, S.: Semi-automatic labeling and training strat-\negy for deep learning-based facial wrinkle detection. In: 2022 IEEE 35th inter-\nnational symposium on computer-based medical systems (CBMS). pp. 383–388.\nIEEE (2022)\n15. Kim, S., Yoon, H., Lee, J., Yoo, S.: Facial wrinkle segmentation using weighted\ndeep supervision and semi-automatic labeling. Artificial Intelligence in Medicine\n145, 102679 (2023)\n16. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\ntransformer:Hierarchicalvisiontransformerusingshiftedwindows.In:Proceedings\nof the IEEE/CVF international conference on computer vision. pp. 10012–10022\n(2021)\n17. Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts.\narXiv preprint arXiv:1608.03983 (2016)\n18. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 (2017)\n19. Luu, K., Dai Bui, T., Suen, C.Y., Ricanek, K.: Combined local and holistic facial\nfeatures for age-determination. In: 2010 11th International Conference on Control\nAutomation Robotics & Vision. pp. 900–904. IEEE (2010)\n20. Ng,C.C.,Yap,M.H.,Cheng,Y.T.,Hsu,G.S.:Hybridageingpatternsforfaceage\nestimation. Image and Vision Computing 69, 92–102 (2018)\n16 J. Moon et al.\n21. Nix, D.A., Weigend, A.S.: Estimating the mean and variance of the target proba-\nbility distribution. In: Proceedings of 1994 ieee international conference on neural\nnetworks (ICNN’94). vol. 1, pp. 55–60. IEEE (1994)\n22. Or-El, R., Sengupta, S., Fried, O., Shechtman, E., Kemelmacher-Shlizerman, I.:\nLifespanagetransformationsynthesis.In:ComputerVision–ECCV2020:16thEu-\nropean Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI 16.\npp. 739–755. Springer (2020)\n23. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomed-\nical image segmentation. In: Medical image computing and computer-assisted\nintervention–MICCAI2015:18thinternationalconference,Munich,Germany,Oc-\ntober 5-9, 2015, proceedings, part III 18. pp. 234–241. Springer (2015)\n24. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,\nKarpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog-\nnition challenge. International journal of computer vision 115, 211–252 (2015)\n25. Sabina,U.,Whangbo,T.K.:Edge-basedeffectiveactiveappearancemodelforreal-\ntime wrinkle detection. Skin Research and Technology 27(3), 444–452 (2021)\n26. Sabina,U.,Whangbo,T.K.:Nasolabialwrinklesegmentationbasedonnestedcon-\nvolutional neural network. In: 2021 International Conference on Information and\nCommunication Technology Convergence (ICTC). pp. 483–485. IEEE (2021)\n27. Satriyasa, B.K.: Botulinum toxin (botox) a for reducing the appearance of facial\nwrinkles: a literature review of clinical use and pharmacological aspect. Clinical,\ncosmetic and investigational dermatology pp. 223–228 (2019)\n28. Small,R.:Botulinumtoxininjectionforfacialwrinkles.Americanfamilyphysician\n90(3), 168–175 (2014)\n29. Warren,R.,Gartstein,V.,Kligman,A.M.,Montagna,W.,Allendorf,R.A.,Ridder,\nG.M.: Age, sunlight, and facial skin: a histologic and quantitative study. Journal\nof the American Academy of Dermatology 25(5), 751–760 (1991)\n30. Wilder-Smith,E.P.:Stimulatedskinwrinklingasanindicatoroflimbsympathetic\nfunction. Clinical Neurophysiology 126(1), 10–16 (2015)\n31. Wu,Y.,Kalra,P.,Thalmann,N.M.:Simulationofstaticanddynamicwrinklesof\nskin. In: Proceedings Computer Animation’96. pp. 90–97. IEEE (1996)\n32. Xie,Z.,Zhang,Z.,Cao,Y.,Lin,Y.,Bao,J.,Yao,Z.,Dai,Q.,Hu,H.:Simmim:A\nsimple framework for masked image modeling. In: Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition. pp. 9653–9663 (2022)\n33. Xu, G., Song, Z., Sun, Z., Ku, C., Yang, Z., Liu, C., Wang, S., Ma, J., Xu, W.:\nCamel:Aweaklysupervisedlearningframeworkforhistopathologyimagesegmen-\ntation. In: Proceedings of the IEEE/CVF International Conference on computer\nvision. pp. 10682–10691 (2019)\n34. Yang, M.Y., Shen, Q.L., Xu, D.T., Sun, X.L., Wu, Q.B.: Striped wrinet: Auto-\nmaticwrinklesegmentationbasedonstripedattentionmodule.BiomedicalSignal\nProcessing and Control 90, 105817 (2024)\n35. Yu,C.,Wang,J.,Peng,C.,Gao,C.,Yu,G.,Sang,N.:Bisenet:Bilateralsegmenta-\ntionnetworkforreal-timesemanticsegmentation.In:ProceedingsoftheEuropean\nconference on computer vision (ECCV). pp. 325–341 (2018)\n36. Zhou, Z.H.: A brief introduction to weakly supervised learning. National science\nreview 5(1), 44–53 (2018)",
    "pdf_filename": "Facial_Wrinkle_Segmentation_for_Cosmetic_Dermatology_Pretraining_with_Texture_Map-Based_Weak_Supervi.pdf"
}