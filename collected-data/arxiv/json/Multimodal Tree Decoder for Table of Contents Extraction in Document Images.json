{
    "title": "Multimodal Tree Decoder for Table of Contents Extraction in Document Images",
    "context": "headings of different levels in documents to better understand the outline of the contents, which can be widely used for document understanding and information retrieval. Existing works often use hand-crafted features and predeﬁned rule-based functions to detect headings and resolve the hierarchical relationship between headings. Both the benchmark and research based on deep learning are still limited. Accordingly, in this paper, we ﬁrst introduce a standard dataset, HierDoc, including image samples from 650 documents of scientiﬁc papers with their content labels. Then we propose a novel end-to-end model by using the multimodal tree decoder (MTD) for ToC as a benchmark for HierDoc. The MTD model is mainly composed of three parts, namely encoder, classiﬁer, and decoder. The encoder fuses the multimodality features of vision, text, and layout information for each entity of the document. Then the classiﬁer recognizes and selects the heading entities. Next, to parse the hierarchical relationship between the heading entities, a tree-structured decoder is designed. To evaluate the performance, both the metric of tree-edit-distance similarity (TEDS) and F1- Measure are adopted. Finally, our MTD approach achieves an average TEDS of 87.2% and an average F1-Measure of 88.1% on the test set of HierDoc. The code and dataset will be released at: https://github.com/Pengfei-Hu/MTD. A huge amount of documents have been accumulated with digitization and OCR engines. However, most of them contain text with limited structural information. For example, the table of contents (ToC), which plays an important role in document understanding and information retrieval, is often missing. The task of ToC extraction is to restore the structure of the document and to recognize the hierarchy of sections. As shown in Fig. 1, the output of ToC extraction is a tree of headings with different levels. As for the recent studies, we can roughly divide them into two categories. The ﬁrst one assumes the presence of ToC pages [1] [2]. They ﬁrst detect ToC pages, then analyze them for ToC entries. However, there are quite a few documents without ToC pages [3]. Therefore, others extract ToC from the whole document [4] [5]. They usually utilize hand-crafted features and strong indicators to detect headings, which will be hierarchically ordered according to predeﬁned rule-based functions. Besides, some research explores hybrid approaches * Corresponding Author [6]. They ﬁrst consider whether the document contains ToC pages, then apply one of the methods above. In general, existing approaches depend greatly on strong indicators and predeﬁned rule-based functions. They can perform well on application-dependent and domain-speciﬁc datasets. However, a large amount of task-speciﬁc knowledge and human-designed rules are needed, which does not extend to other types of documents. Recently, deep learning based methods have achieved great success in many ﬁelds related to documents. For example, [7] proposes the LayoutLM method for document image understanding, which is inspired by BERT [8]. This method uses image features and position to pre-train the model and performs well on downstream tasks. Lately, ViBERTgrid [9] is proposed for key information extraction from documents. These demonstrate the powerful ability of deep learning based methods to deal with document-related problems. It is worth noting that the existing datasets are not suitable for deep learning based methods. The ISRI dataset [10] and the Medical Article Records Groundtruth (MARG) dataset [11] only contain bi-level images and similarly simple layouts, predominantly of journal articles. Some datasets [12] [13] [14] are available in several ICDAR challenges, which contain complex layouts of newspapers, books, and technical articles. However, their annotations lack the heading category. Anno- tations with the heading category are provided by PubLayNet [15]. However, there is no information about the heading depth. [16] collects 71 French documents and 72 English documents in PDF format from the ﬁnancial domain. Its structure extraction ground truth is still not aligned with the text lines in the document. To overcome the lack of data for the research of ToC extraction, we collect a dataset, namely Hierarchical academic Document (HierDoc). It contains 650 academic English docu- ments in various ﬁelds from ArXiv1. With LaTeX source code, we generate its ToC ground truth using regular expressions. We also provide annotations for each text line. In this paper, the text line is denoted as the entity. The ToC is aligned with entities for training. For this dataset, 350 and 300 documents 1https://arxiv.org/ arXiv:2212.02896v1  [cs.CV]  6 Dec 2022",
    "body": "Multimodal Tree Decoder for Table of Contents\nExtraction in Document Images\nPengfei Hu1, Zhenrong Zhang1, Jianshu Zhang2, Jun Du1,*, Jiajia Wu2\n1National Engineering Research Center of Speech and Language Information Processing\nUniversity of Science and Technology of China, Hefei, Anhui, P. R. China\n2iFLYTEK Research\nEmail: hudeyouxiang@mail.ustc.edu.cn, zzr666@mail.ustc.edu.cn\njszhang6@iﬂytek.com, jundu@ustc.edu.cn, jjwu@iﬂytek.com\nAbstract—Table of contents (ToC) extraction aims to extract\nheadings of different levels in documents to better understand the\noutline of the contents, which can be widely used for document\nunderstanding and information retrieval. Existing works often\nuse hand-crafted features and predeﬁned rule-based functions\nto detect headings and resolve the hierarchical relationship\nbetween headings. Both the benchmark and research based on\ndeep learning are still limited. Accordingly, in this paper, we\nﬁrst introduce a standard dataset, HierDoc, including image\nsamples from 650 documents of scientiﬁc papers with their\ncontent labels. Then we propose a novel end-to-end model\nby using the multimodal tree decoder (MTD) for ToC as a\nbenchmark for HierDoc. The MTD model is mainly composed\nof three parts, namely encoder, classiﬁer, and decoder. The\nencoder fuses the multimodality features of vision, text, and\nlayout information for each entity of the document. Then the\nclassiﬁer recognizes and selects the heading entities. Next, to\nparse the hierarchical relationship between the heading entities, a\ntree-structured decoder is designed. To evaluate the performance,\nboth the metric of tree-edit-distance similarity (TEDS) and F1-\nMeasure are adopted. Finally, our MTD approach achieves an\naverage TEDS of 87.2% and an average F1-Measure of 88.1%\non the test set of HierDoc. The code and dataset will be released\nat: https://github.com/Pengfei-Hu/MTD.\nI. INTRODUCTION\nA huge amount of documents have been accumulated with\ndigitization and OCR engines. However, most of them contain\ntext with limited structural information. For example, the\ntable of contents (ToC), which plays an important role in\ndocument understanding and information retrieval, is often\nmissing. The task of ToC extraction is to restore the structure\nof the document and to recognize the hierarchy of sections.\nAs shown in Fig. 1, the output of ToC extraction is a tree of\nheadings with different levels.\nAs for the recent studies, we can roughly divide them into\ntwo categories. The ﬁrst one assumes the presence of ToC\npages [1] [2]. They ﬁrst detect ToC pages, then analyze them\nfor ToC entries. However, there are quite a few documents\nwithout ToC pages [3]. Therefore, others extract ToC from\nthe whole document [4] [5]. They usually utilize hand-crafted\nfeatures and strong indicators to detect headings, which will\nbe hierarchically ordered according to predeﬁned rule-based\nfunctions. Besides, some research explores hybrid approaches\n* Corresponding Author\n[6]. They ﬁrst consider whether the document contains ToC\npages, then apply one of the methods above.\nIn general, existing approaches depend greatly on strong\nindicators and predeﬁned rule-based functions. They can\nperform well on application-dependent and domain-speciﬁc\ndatasets. However, a large amount of task-speciﬁc knowledge\nand human-designed rules are needed, which does not extend\nto other types of documents. Recently, deep learning based\nmethods have achieved great success in many ﬁelds related to\ndocuments. For example, [7] proposes the LayoutLM method\nfor document image understanding, which is inspired by BERT\n[8]. This method uses image features and position to pre-train\nthe model and performs well on downstream tasks. Lately,\nViBERTgrid [9] is proposed for key information extraction\nfrom documents. These demonstrate the powerful ability of\ndeep learning based methods to deal with document-related\nproblems.\nIt is worth noting that the existing datasets are not suitable\nfor deep learning based methods. The ISRI dataset [10] and the\nMedical Article Records Groundtruth (MARG) dataset [11]\nonly contain bi-level images and similarly simple layouts,\npredominantly of journal articles. Some datasets [12] [13]\n[14] are available in several ICDAR challenges, which contain\ncomplex layouts of newspapers, books, and technical articles.\nHowever, their annotations lack the heading category. Anno-\ntations with the heading category are provided by PubLayNet\n[15]. However, there is no information about the heading\ndepth. [16] collects 71 French documents and 72 English\ndocuments in PDF format from the ﬁnancial domain. Its\nstructure extraction ground truth is still not aligned with the\ntext lines in the document.\nTo overcome the lack of data for the research of ToC\nextraction, we collect a dataset, namely Hierarchical academic\nDocument (HierDoc). It contains 650 academic English docu-\nments in various ﬁelds from ArXiv1. With LaTeX source code,\nwe generate its ToC ground truth using regular expressions.\nWe also provide annotations for each text line. In this paper,\nthe text line is denoted as the entity. The ToC is aligned with\nentities for training. For this dataset, 350 and 300 documents\n1https://arxiv.org/\narXiv:2212.02896v1  [cs.CV]  6 Dec 2022\n\nare used for training and testing individually. More details of\nthe dataset are described in Section II.\nWith HierDoc, it is possible to take advantage of deep neural\nnetworks for the ToC extraction task. We further propose\nMultimodal Tree Decoder (MTD), an end-to-end model which\ndetects heading entities and produces the ToC by parsing the\nhierarchy of them. To the best of our knowledge, this is the\nﬁrst deep learning based method for ToC extraction. Different\nfrom previous works, MTD offers the following advantages,\n(1) it can process documents in images or PDF format, and\n(2) no human-designed rules are used. It indicates that the\nMTD can generalize well across documents from different\ndomains and handle different forms of user input. We adopt\nthe text line as the basic input element, denoted as entity.\nMTD mainly has three components: encoder, classiﬁer and\ndecoder. Firstly, the encoder as a feature extractor embeds\nvision, plain text, and layout of the entity into a feature vector.\nThen the classiﬁer detects heading entities and feeds them\nto the decoder. Finally, the decoder predicts the relationships\nbetween the heading entities one by one. More speciﬁcally,\nthe attention mechanism built into the decoder locates the\nreference entity of a heading entity, and the relationship\nbetween the two is predicted afterward. The output of the MTD\ncan be transferred to the ToC simply, as shown in Fig. 1. We\nutilize both the Tree-Edit-Distance-based Similarity (TEDS)\n[17] metric and the F1-Measure to evaluate the performance\nof our model. MTD achieves an average TEDS of 87.2% and\nan average F1-Measure of 88.1% on the test set of HierDoc.\nThe ablation studies prove the effectiveness of each module\nof MTD.\nThe main contributions of this paper are as follows:\n• We introduce a standard dataset HierDoc for ToC extrac-\ntion, which contains 650 documents of scientiﬁc papers\nfrom various ﬁelds.\n• We propose a novel end-to-end model, Multimodal Tree\nDecoder (MTD) for ToC extraction. We demonstrate that\nfusing visual, textual, and layout features boosts model\nperformance.\n• By predicting the relationships between entities, the hier-\narchy of headings can be sequentially parsed by the tree\ndecoder in MTD.\n• We achieve the results with an average TEDS of 87.2%\nand an average F1-Measure of 88.1% on the test set\nof HierDoc, which provides a competitive baseline for\nsubsequent research.\nII. DATASET\nAs discussed in Section I, existing datasets [12]–[16] are\nnot suitable for deep learning based methods. To overcome\nthe lack of training data, we release a dataset HierDoc\nwhich contains 350 and 300 document images for training\nand testing, respectively. As illustrated in Fig. 2, HierDoc\nprovides two kinds of annotations, document-level annotations\nand entity-level annotations. The document-level annotation\nserves as the target of ToC extraction, which is used in the\ntest phase. Entity-level annotations are created for training.\nTITLE\nIntroduction\nMethod\nEfficient Batch\nConclusion\nComputation\nOverall \nArchitecture\nTable of Contents\n1               Introduction\n2               Method\n2.1            Overall Architecture\n2.2            Efficient Batch Com-\n3               Conclusion\nsibling\nidentity\nparent\nentity\nheading\nputation\nFig. 1: The ToC can be represented as a tree structure, and the\noutput of MTD can be easily converted to ToC. The heading\n“Efﬁcient Batch Computation” is split into two entities in the\ndocument due to the limited length of the text line.\nFor each entity in the document, we generate a quadruple,\n(content, position, heading, id). content is the textual con-\ntent of the entity. position is deﬁned by (x0, y0, x1, y1), where\n(x0, y0) corresponds to the position of the upper left in the\nbounding box, and (x1, y1) represents the position of the lower\nright. heading is a boolean variable indicating whether the\nentity is a heading (or part of a heading, see Fig. 2 (b)).\nid represents the hierarchical position in the ToC tree. For\nexample, for entity t with content “How do CR ﬂuxes vary\nwith Galactic...” in the Fig. 2 (a), id is “2.2”, which indicates\nthat t is the second child of its parent with content “Science\ncase” and id “2”. And t has one child with the content\n“Mechanical structure” and id “2.2.1”.\nA. Document Collection\nWe download 650 English scientiﬁc documents in PDF\nformat and corresponding LaTeX source codes from arXiv,\ncovering 8 ﬁelds including physics, mathematics, computer\nscience, quantitative biology, etc. Each document is free to\ndistribute, remix, and adapt under the Creative Commons\nAttribution 4.0 user license2.\nB. Label Generation\nThe document-level annotation, as shown in Fig. 2, can be\nparsed using regular expressions with LaTeX source code. It\nis used as the target during the test phase. We also generate\na quadruple (content, position, heading, id) for each entity.\nWe parse documents in PDF format using pdfplumber3 to\nobtain the text contents with positions for each entity. After\nthat, the entities are matched with document-level annotations\nto obtain heading and id according to the Word Error\nRate (WER). To facilitate model processing, the entities on\neach page are organized from top-to-bottom and left-to-right.\nOverall, HierDoc provides document images, entity-level\nannotations, and document-level annotations.\n2https://creativecommons.org/licenses\n3https://github.com/jsvine/pdfplumber\n\nTITLE\nIntroduction\nScience case\nWhat are the CR \nenergy \ndistributions...\nHow do CR \nfluxes vary with \nGalactic...\nWhere are the \nlow-energy CRs \nand how do...\nMechanical \nstructure\nTable of Contents\n1                \nIntroduction\n2                \nScience case\n2.1           \nWhat are the CR \nenergy \ndistributions...\n2.2           \nHow do CR fluxes \nvary with \nGalactic...\n2.2.1           Mechanical structure\n(a) ducoment-level annotations\n𝑥0, 𝑦0, 𝑥1, 𝑦1\n𝑝𝑒𝑛𝑒𝑡𝑟𝑎𝑡𝑒𝑑𝑒𝑛𝑠𝑒𝑐𝑙𝑜𝑢𝑑𝑠?\n𝑇𝑟𝑢𝑒\n2.2.3\n𝑥0, 𝑦0, 𝑥1, 𝑦1\n𝑇ℎ𝑒𝑔𝑎𝑚𝑚𝑎−𝑟𝑎𝑔𝑠… …\n𝐹𝑎𝑙𝑠𝑒\n𝑁𝑈𝐿𝐿\n(b) entity-level annotations\nFig. 2: HierDoc contains two kinds of annotations. Document-\nlevel annotations serve as targets of ToC extraction. Entity-\nlevel\nannotations\nprovide\na\nquadruple\nfor\neach\nentity,\n(content, position, heading, id). heading is a boolean vari-\nable indicating whether an entity is a heading and id identiﬁes\nthe hierarchical position of the entity in the ToC tree.\nIII. THE PROPOSED APPROACH\nThe overall pipeline of MTD is shown in Fig. 3. MTD\nconsists of three components: encoder, classiﬁer and decoder.\nThe vision module, text module, and layout module are ﬁrst\napplied to the input document to extract features of each\nentity. Then the gated unit in the encoder is used to obtain\nthe multimodality features. The following classiﬁer detects\nthe heading entities and feeds them to the decoder. Finally,\nthe decoder parses the hierarchy of headings to produce the\nToC. In the following subsections, we will ﬁrst formalize our\nmethod and then elaborate on the components of MTD.\nA. Formalization\nGiven a document, we ﬁrst obtain the layout and plain\ntext of entities by OCR engines or PDF Parsers, as discussed\nin Section II. Then, the encoder extracts the features of all\nentities. Next, the classiﬁer divides them into two categories,\nheading entities or normal entities. A heading entity is one of\nthe headings of the document, while a normal entity is not.\nFinally, starting from an empty tree with a single root node,\nall heading entities are iterated in the sequence of how they\nappear within the document to generate the ToC tree.\nMore speciﬁcally, there may be one of three relationships\nbetween two heading entities, namely parent, sibling and iden-\ntity. As shown in Fig. 1, Introduction, Method and Conclusion\nare chapters in TITLE. Intuitively, the relationship between\nIntroduction and TITLE is parent. For chapter Method, we\npredict the relationship sibling between it and Introduction.\nIt can avoid predicting the relationship between two entities\nthat are far apart in the document, since the TITLE may have\nmany children. We design the relationship identity because the\nheading may be divided into several entities due to the limited\nlength of text lines. So far, there is one critical issue that\nremains addressed. For the current entity, which entity should\nbe used to predict the relationship with it? The answer is the\nreference entity. The reference entity is the entity immediately\npreceding the current entity within the document, between\nwhich one of the relationships described above exists. Finally,\nafter obtaining the reference entity and relationship for each\nheading entity, we can generate the ToC tree simply.\nB. Encoder\nDifferent from previous works [4]–[6], the encoder requires\nno hand-crafted features to extract features of each entity.\nFirstly, with the entity-level annotations, the encoder extracts\nthe visual features f v\nt , the textual features f s\nt and position\nfeatures f p\nt of each entity. Then, before being fed to the next\nmodule, f v\nt , f s\nt , and f p\nt are fused into ft. ft ∈Rd, f v\nt ∈Rd,\nf s\nt ∈Rd, f p\nt ∈R8, t ∈[1, N], and N is the number of entities\nin the document.\n1) Vision Module:\nThe vision module takes document\nimages as input. It uses FPN [18] to aggregate feature maps\nfrom ResNet-34 [19], then pools a ﬁxed-size feature map ˆf v\nt\nwith the RoIAlign [20] for each entity. In order to integrate\nwith features from other domains, the 2-dimensional feature\nmap ˆf v\nt is ﬂatten to produce visual features f v\nt . The ResNet-\n34 and FPN are pretrained on 1000 scientiﬁc papers with a\ntext detection task [21].\n2) Text Module: Recent studies [7] [22] show that textual\nfeatures play an extremely important role in documents. Fol-\nlowing [23], the BERT is used to extract the textual features\nf s\nt . To make the extracted semantic features more suitable\nfor our network, two linear transformations with a RELU\nactivation are added following the BERT. It is worth noting\nthat both BERT and ResNet-34+FPN do not update their\nparameters during the training phase to save GPU memory\nand improve training speed.\n3) Layout Module: The layout module generates f p\nt as the\nfollowing:\nf p\nt = (xlt\nt\nW , ylt\nt\nH , xrb\nt\nW , yrb\nt\nH , wt\n¯w , ht\n¯h , ylt\nt −yrb\nt−1\n¯h\n, ylt\nt+1 −yrb\nt\n¯h\n)\nwt, ht, (xlt\nt , ylt\nt , xrb\nt , yrb\nt ) correspond to the width, the height,\nand the coordinate position of the bounding box of each\nentity. W, H denote the width and height of the whole page.\nWe normalize ht with ¯h, which is the average height of all\n\nVision\nModule\nText\nModule\nLayout\nModule\nEncoder\nClassifier\nBlocking gradient flow\nPostprocess\nAttention\nMechanism\nRelation\nDecoder\nTitle\nIntorduction\nOur Method\nNetwork\nLoss Functi-\nDecoder\nHeading entity\nNormal entity\nReference\nSibling\nIdentity\non\nTitle\nIntorduction\nOur Method\nNetwork\nLoss Functi-\non\nParent\nOCR/\nPDF parser\nGated \nUnit\nFig. 3: Architecture of the MTD for ToC extraction.\nbounding boxes. We believe that ¯h is a better measure to\nnormalize h than the commonly used H, because it reﬂects\nthe difference in spatial height between entities better.\n4) Gated Unit: Inspired by [24] [25], we introduce some\ntrainable weights to balance the contribution of f v\nt , f s\nt , and\nf p\nt . The fused feature ft is produced as follows:\n(\nzt = σ (Wz · [f v\nt , f s\nt , f p\nt ])\nft = zt ∗f v\nt + (1 −zt) ∗f s\nt + Ezf p\nt\nWhere Wz, Ez are trainable weight matrices, and σ is the\nsigmoid function, Wz ∈Rd×(2d+8), Ez ∈Rd×8. The fused\nfeatures ft are the ﬁnal output of the encoder.\nC. Classiﬁer\nThe classiﬁer divides all entities into two categories, i.e.,\nheading entities and normal entities, as discussed in Section\nIII-A.\nBefore classiﬁcation, Bidirectional Gated Recurrent Unit\n(BiGRU) [26] is used to capture global information:\ngt = BiGRU(ft, gt−1, gt+1)\nwhere gt is the hidden state. Then we apply a fully connected\nlayer and a softmax activation to classify each entity:\nxt = softmax(Wcgt + bc)\nwhere xt represents the classiﬁcation score, xt ∈R2, Wc ∈\nR2×d, bc ∈R2. Considering there is an imbalance between\nheading entities and normal entities, we deﬁne the classiﬁca-\ntion loss as follows:\nLcls = 1\nN\nX\nt≤N\nFL(xt, pt)\nwhere pt is the class label of the entity and FL is the focal loss\nproposed in [27] to deal with the problem of data imbalance\nexisting between various classes.\nD. Decoder\nThe decoder takes detected heading entities as input and\npredicts the relationship between them to parse the heading\nentities into a tree structure. The features of heading entities\ncan be denoted as ˆ\nM, where ˆ\nM ∈RC×d, and C is the number\nof the heading entities. So far, the features of each heading\nentity are still independent of each other. Therefore, we intro-\nduce the transformer [28] to capture long-range dependencies\non heading entities. We take the features ˆ\nM as query, key and\nvalue, which are required by the transformer. The output of\nthe transformer as the ﬁnal features M has a global receptive\nﬁeld.\nInspired by the successful applications of attention mech-\nanism [29] [23], we build the attention mechanism into the\ndecoder. To ﬁnd the reference entity, we compute the predic-\ntion of the current hidden state ˆhs from previous context vector\ncs−1 and its hidden state hs−1 with a GRU:\nˆhs = GRU(cs−1, hs−1)\nThen we employ an attention mechanism with ˆhs as the\nquery and the heading entity features M as both key and value:\nes = Attn(M, ˆhs)\ncs =\nes\n∥es∥1\nM\nwhere ∥· ∥1 is the vector 1-norm. We deﬁne Attn function as\nfollows:\nD = Q ∗\ns−1\nX\nl=1\nel\nˆesi = vT tanh(Whˆhs + Wmmi + Wddi)\n\nesi = activate(esi, es)\nin which\nactivate(esi, es) =\n( 1\nif i = arg max\nj\n(esj)\n0\notherwise\nwhere ∗denotes a convolution layer, Ps−1\nl=1 el denotes the sum\nof the past determined reference entity, ˆes,i denotes the output\nenergy, di denotes the element of D, which is used to keep\ntrack of past alignment information. It is worth noting that\nthe attention mechanism is completed on the features of each\nheading entity. Considering that there is only one reference\nentity for each heading entity, we use activate to obtain the\nattention probability instead of softmax, which means that the\nweight of the reference entity is set to 1 while other entities\nremain 0.\nWith the context vector cs, we compute the current hidden\nstate:\nhs = GRU(cs, ˆhs)\nThe training loss of locating the reference entity is deﬁned\nas:\nLref = 1\nM\nX\ns≤M\nFL( ˆes, ys)\nwhere loss function FL has been deﬁned in Section III-C and\nys denotes the ground truth of the reference entity for time\nstep s. ysi is 1 if ith heading entity is the reference entity of\nthe current heading entity, otherwise 0.\nThe relationship between the current entity and its reference\nentity is predicted through an FFN [28]:\nrs = FFN([cs, hs])\nFFN(x) = W2 max (0, W1x + b1) + b2\nwhere FFN is actually two linear transformations with a ReLU\nactivation in between. rs ∈R3, W1 ∈Rdin×2d, b1 ∈Rdin,\nW2 ∈R3×din, b2 ∈R3.\nThe loss of predicting relationships is as follows:\nLre = 1\nM\nX\ns≤M\nFL(rs, qs)\nwhere qs denotes the ground truth of the relation between the\ncurrent heading entity and its reference entity at time step s.\nIV. EXPERIMENTS\nA. Metric\nIn this paper, we use both the F1-Measure and Tree-Edit-\nDistance-based Similarity (TEDS) metric [17] to evaluate the\nperformance of our model for ToC extraction.\nTo use the F1-Measure, the relationships among the heading\nentities and corresponding reference entities need to be ex-\ntracted. Then F1-Measure measures the percentage of correctly\nextracted pairs of heading entities, where heading entities and\nthe relationships between them are the same as the ground\ntruth.\nWhile using the TEDS metric, we need to present ToC as\na tree structure. The tree-edit distance [30] is used to measure\nthe similarity between two trees. TEDS is computed as:\nTEDS (Ta, Tb) = 1 −EditDist (Ta, Tb)\nmax (|Ta| , |Tb|)\nwhere Ta and Tb are ToC trees. EditDist represents the tree-\nedit distance, and |T| is the number of nodes in T.\nB. Implementation Details\nIn the vision module, ResNet-34+FPN is pre-trained on\n1000 scientiﬁc documents for a text detection task [21]. The\npool size of RoIAlign is set to 3 × 3. The BERT used in\nthe text module is available on GitHub4. The channel number\nof visual, textual, and fused features is 128. The number of\nstacks for the transformer in the decoder is set to 3. And the\nhidden state dimension in the classiﬁer and decoder is both\n128, too.\nThe training objective of our model is to minimize the\nfunction:\nO = α1Lcls + α2Lref + α3Lre\nwhere α1, α2 and α3 are set to 1. Random scale is adopted\nduring training. Both BERT and ResNet-34+FPN do not\nupdate their parameters to save GPU memory. We employ\nthe Adam algorithm [31] for optimization, with the following\nhyper parameters: β1= 0.9 and β2 = 0.999. We set the\nlearning rate using the cosine annealing schedule [32]. The\nminimum learning rate and the initial learning rate are set to\n1e-6 and 5e-4, respectively. The batch size varies between 1\nand 4 according to the number of pages of the document.\nAll experiments are implemented with a single Tesla V100\nGPU with 32GB RAM Memory. The whole framework is\nimplemented using PyTorch.\nC. Ablation Study\n1) The Effectiveness of Each Modality: We perform abla-\ntion studies to evaluate the effectiveness of each modality in\nthis section. The Heading Detecting in tables represents the\ntask of detecting heading entities, and the metric we use is\nF1-Measure. As shown in Table I, when any of the visual,\ntextual, or layout modalities is removed, the performance of\nthe model drops. This suggests that multimodal features play\nan important role in ToC extraction. Note that the parameters\nof our visual and textual modules are frozen during training.\nTherefore, they need to be properly pre-trained as discussed\nin Section IV-B.\n2) The Effectiveness of Feature Fusion Strategies:\nWe\nintroduce a novel feature fusion strategy in Section III-B4. In\nthis section, we conduct experiments to compare it with three\ncommon feature fusion strategies, including dot, concatenate,\nand add. Table II shows that the Gated Unit outperforms other\nstrategies. Thus, the Gated Unit is utilized in our approach.\n4https://github.com/huggingface/transformers\n\nTABLE I: The Effectiveness of Each Modality.\nMethod\nHeading Detecting\nToC Extraction\nP\nR\nF1\nTEDS\nF1\nw/o Text\n88.6\n83.0\n85.7\n63.7\n62.5\nw/o Layout\n95.0\n89.2\n92.0\n80.4\n79.6\nw/o Vision\n97.5\n93.7\n95.5\n86.5\n87.7\nMTD\n96.2\n96.0\n96.1\n87.2\n88.1\nTABLE II: The Effectiveness of Feature Fusion Strategies.\nMethod\nHeading Detecting\nToC Extraction\nP\nR\nF1\nTEDS\nF1\nDot\n96.1\n93.9\n95.0\n84.6\n84.9\nConcat\n96.8\n93.8\n95.3\n84.6\n85.3\nAdd\n97.0\n94.0\n95.5\n85.7\n86.3\nGated Unit\n96.2\n96.0\n96.1\n87.2\n88.1\n3) The Effectiveness of Decoding Methods.: To verify the\nperformance of the decoder in MTD, we replace it with a C-\nclass classiﬁer, which predicts the depth of headings. C is set\nto 5 in HierDoc. The transformer unit is reserved for a fair\ncomparison.\nTABLE III: The Effectiveness of Decoding Methods.\nMethod\nToC Extraction (TEDS)\nC-class Classiﬁer\n72.1\nDecoder\n87.2\nAs described in Table III, the C-class Classiﬁer leads to\ndegradation of performance to a certain extent. Intuitively, the\nDecoder in MTD is more suitable for parsing the hierarchy\nof entities into a tree structure. In addition, the Decoder\ncan generate a tree of any depth by deﬁning three types of\nrelationships, namely parent, sibling, and identity, while the\nC-class classiﬁer has to restrict the tree to a depth of C.\nV. CONCLUSION\nIn this paper, we release a dataset HierDoc for deep learning\nbased methods and further propose the MTD for ToC extrac-\ntion. HierDoc contains 650 document images and annotations\nin both the document-level and entity-level. The document-\nlevel annotations serve as the target of ToC extraction in the\ntest phase, while the entity-level annotations are generated\nfor training. MTD mainly consists of three parts, i.e., en-\ncoder, classiﬁer, and decoder. The encoder extracts features\nof entities, then the classiﬁer selects the heading entities\nand the decoder parses the hierarchy of heading entities. We\ndemonstrate that the use of multimodality features of vision,\ntext, and layout in the encoder boosts model performance. The\ngated unit used in the encoder also outperforms other feature\nfusion strategies. The decoder builds the ToC tree by parsing\nthe hierarchy of heading entities. The decoder outperforms\nthe vanilla C-class classiﬁer by a large margin, and it can\ngenerate a tree of any depth. MTD achieves an average TEDS\nof 87.2% and an average F1-Measure of 88.1% on the test set\nof HierDoc. We hope that our MTD could serve as a strong\nbaseline for deep learning based ToC extraction in the future.\nREFERENCES\n[1] M. El-Haj, P. Alves, P. Rayson, M. Walker, and S. Young, “Retrieving,\nclassifying and analysing narrative commentary in unstructured (glossy)\nannual reports published as pdf ﬁles,” Accounting and Business Re-\nsearch, vol. 50, no. 1, pp. 6–34, 2020.\n[2] A. Doucet, M. Coustaty et al., “Enhancing table of contents extraction\nby system aggregation,” in 2017 14th IAPR international conference on\ndocument analysis and recognition (ICDAR), vol. 1.\nIEEE, 2017, pp.\n242–247.\n[3] A. Doucet, G. Kazai, B. Dresevic, A. Uzelac, B. Radakovic, and\nN. Todic, “Setting up a competition framework for the evaluation\nof structure extraction from ocr-ed books,” International Journal on\nDocument Analysis & Recognition, vol. 14, no. 1, pp. 45–52, 2011.\n[4] A. A. M. Gopinath, S. Wilson, and N. Sadeh, “Supervised and unsuper-\nvised methods for robust separation of section titles and prose text in\nweb documents,” in Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, 2018, pp. 850–855.\n[5] A. M. Namboodiri and A. K. Jain, “Document structure and layout\nanalysis,” in Digital Document Processing.\nSpringer, 2007, pp. 29–48.\n[6] S. Tuarob, P. Mitra, and C. L. Giles, “A hybrid approach to discover\nsemantic hierarchical sections in scholarly documents,” in 2015 13th in-\nternational conference on document analysis and recognition (ICDAR).\nIEEE, 2015, pp. 1081–1085.\n[7] Y. Xu, M. Li, L. Cui, S. Huang, F. Wei, and M. Zhou, “Layoutlm:\nPre-training of text and layout for document image understanding,” in\nProceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, 2020, pp. 1192–1200.\n[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[9] W. Lin, Q. Gao, L. Sun, Z. Zhong, K. Hu, Q. Ren, and Q. Huo,\n“Vibertgrid: A jointly trained multi-modal 2d document representa-\ntion for key information extraction from documents,” arXiv preprint\narXiv:2105.11672, 2021.\n[10] T. A. Nartker, S. V. Rice, and S. E. Lumos, “Software tools and\ntest data for research and testing of page-reading ocr systems,” in\nDocument Recognition and Retrieval XII, 16-20 January 2005, San Jose,\nCalifornia, USA, Proceedings, 2005.\n[11] G.\nThoma,\n“The\nnational\nlibrary\nof\nmedicine,”\nin\nhttp://marg.nlm.nih.gov/, Bethesda, USA, 2005.\n[12] C. Clausner, C. Papadopoulos, S. Pletschacher, and A. Antonacopoulos,\n“The enp image and ground truth dataset of historical newspapers,”\nin 2015 13th International Conference on Document Analysis and\nRecognition (ICDAR), 2015, pp. 931–935.\n[13] C. Clausner, A. Antonacopoulos, and S. Pletschacher, “Icdar2017 com-\npetition on recognition of documents with complex layouts - rdcl2017,”\nin 2017 14th IAPR International Conference on Document Analysis and\nRecognition (ICDAR), vol. 01, 2017, pp. 1404–1410.\n[14] A. Antonacopoulos, D. Bridson, C. Papadopoulos, and S. Pletschacher,\n“A realistic dataset for performance evaluation of document layout\nanalysis,” in 2009 10th International Conference on Document Analysis\nand Recognition.\nIEEE, 2009, pp. 296–300.\n[15] X. Zhong, J. Tang, and A. Jimeno Yepes, “Publaynet: Largest dataset\never for document layout analysis,” in 2019 International Conference on\nDocument Analysis and Recognition (ICDAR), 2019, pp. 1015–1022.\n[16] N.-I. Bentabet, R. Juge, I. El Maarouf, V. Mouilleron, D. Valsamou-\nStanislawski, and M. El-Haj, “The ﬁnancial document structure ex-\ntraction shared task (ﬁntoc 2020),” in Proceedings of the 1st Joint\nWorkshop on Financial Narrative Processing and MultiLing Financial\nSummarisation, 2020, pp. 13–22.\n[17] X. Zhong, E. ShaﬁeiBavani, and A. Jimeno Yepes, “Image-based table\nrecognition: data, model, and evaluation,” in Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part XXI 16.\nSpringer, 2020, pp. 564–580.\n[18] T.-Y. Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie,\n“Feature pyramid networks for object detection,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition, 2017, pp.\n2117–2125.\n\n[19] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770–778.\n[20] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” in\nProceedings of the IEEE international conference on computer vision,\n2017, pp. 2961–2969.\n[21] Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-\nstage object detection,” in 2019 IEEE/CVF International Conference on\nComputer Vision (ICCV), 2020.\n[22] Z. Wang, Y. Xu, L. Cui, J. Shang, and F. Wei, “Layoutreader: Pre-\ntraining of text and layout for reading order detection,” arXiv preprint\narXiv:2108.11591, 2021.\n[23] Z. Zhang, J. Zhang, and J. Du, “Split, embed and merge: An accurate\ntable structure recognizer,” arXiv preprint arXiv:2107.05214, 2021.\n[24] J. Arevalo, T. Solorio, M. Montes-y G´omez, and F. A. Gonz´alez,\n“Gated multimodal units for information fusion,” arXiv preprint\narXiv:1702.01992, 2017.\n[25] D. Yu, X. Li, C. Zhang, T. Liu, J. Han, J. Liu, and E. Ding, “Towards\naccurate scene text recognition with semantic reasoning networks,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 12 113–12 122.\n[26] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of\ngated recurrent neural networks on sequence modeling,” arXiv preprint\narXiv:1412.3555, 2014.\n[27] T. Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, “Focal loss\nfor dense object detection,” in 2017 IEEE International Conference on\nComputer Vision (ICCV), 2017.\n[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems, 2017, pp. 5998–6008.\n[29] J. Zhang, J. Du, S. Zhang, D. Liu, Y. Hu, J. Hu, S. Wei, and\nL. Dai, “Watch, attend and parse: An end-to-end neural network based\napproach to handwritten mathematical expression recognition,” Pattern\nRecognition, vol. 71, pp. 196–206, 2017.\n[30] M. Pawlik and N. Augsten, “Tree edit distance: Robust and memory-\nefﬁcient,” Information Systems, vol. 56, pp. 157–173, 2016.\n[31] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nComputer Science, 2014.\n[32] I. Loshchilov and F. Hutter, “Sgdr: Stochastic gradient descent with\nwarm restarts,” arXiv preprint arXiv:1608.03983, 2016.",
    "pdf_filename": "Multimodal Tree Decoder for Table of Contents Extraction in Document Images.pdf"
}