{
    "title": "Whisper Finetuning on Nepali Language",
    "abstract": "Despite the growing advancements in Automatic Speech Recognition (ASR) models, the development of robust models for underrepresented languages, such as Nepali, remains a challenge. This research focuses on making an exhaustive and generalized dataset followed by fine-tuning OpenAI’s Whisper models of different sizes to improve transcription (speech- to-text) accuracy for the Nepali language. We leverage publicly available ASR datasets and self-recorded custom datasets with a diverse range of accents, dialects, and speaking styles further enriched through augmentation. Our experimental results demonstrate that fine-tuning Whisper models on our curated custom dataset substantially reduces the Word Error Rate (WER) across all model sizes attributed to larger data variations in terms of speaker’s age, gender, and sentiment, acoustic environment, dialect, denser audio segments (15-30 seconds) that are more compatible with Whisper’s input, and manual curation of audios and transcriptions. Notably, our approach outperforms Whisper’s baseline models trained on Fleur’s dataset, achieving WER reductions of up to 36.2% on the small and 23.8% on medium models. Furthermore, we show that data augmentation plays a significant role in enhancing model robustness. Our approach underlines the importance of dataset quality, variation, and augmentation in the adaptation of state-of-the-art models to underrepresented languages for developing accurate ASR systems. 1 Introduction Automatic Speech Recognition (ASR) systems have experienced remarkable advancements in recent years, driven largely by the development of large-scale, pre-trained models such as OpenAI’s Whisper [1]. These models, trained on extensive multilingual datasets, have demonstrated impressive performance across a wide range of languages, supporting appli- cations in voice assistants, automated transcription, and accessibility tools for the hearing impaired. However, many low-resource languages, including Nepali, Hindi, and Albanian, continue to face challenges in achieving high transcription accuracy. These challenges stem from the limited availability of high-quality training data and the linguistic complexities ∗These authors contributed equally to this work. †Corresponding authors 1",
    "body": "Whisper Finetuning on Nepali Language\nSanjay Rijal ∗†1, Shital Adhikari ∗2, Manish Dahal ∗3, Manish Awale4, and\nVaghawan Ojha †5\n1sanjay.rijal@ekbana.info, rijalsanjay42@gmail.com\n2shital.adhikari@ekbana.info\n3manish.dahal@ekbana.info\n4manish.awale@ekbana.info\n5vaghawan.ojha@ekbana.net\nE.K. Solutions Pvt. Ltd., Lalitpur, Nepal\nAbstract\nDespite the growing advancements in Automatic Speech Recognition (ASR) models, the\ndevelopment of robust models for underrepresented languages, such as Nepali, remains a\nchallenge. This research focuses on making an exhaustive and generalized dataset followed\nby fine-tuning OpenAI’s Whisper models of different sizes to improve transcription (speech-\nto-text) accuracy for the Nepali language.\nWe leverage publicly available ASR datasets\nand self-recorded custom datasets with a diverse range of accents, dialects, and speaking\nstyles further enriched through augmentation. Our experimental results demonstrate that\nfine-tuning Whisper models on our curated custom dataset substantially reduces the Word\nError Rate (WER) across all model sizes attributed to larger data variations in terms of\nspeaker’s age, gender, and sentiment, acoustic environment, dialect, denser audio segments\n(15-30 seconds) that are more compatible with Whisper’s input, and manual curation of\naudios and transcriptions. Notably, our approach outperforms Whisper’s baseline models\ntrained on Fleur’s dataset, achieving WER reductions of up to 36.2% on the small and 23.8%\non medium models. Furthermore, we show that data augmentation plays a significant role\nin enhancing model robustness. Our approach underlines the importance of dataset quality,\nvariation, and augmentation in the adaptation of state-of-the-art models to underrepresented\nlanguages for developing accurate ASR systems.\n1\nIntroduction\nAutomatic Speech Recognition (ASR) systems have experienced remarkable advancements\nin recent years, driven largely by the development of large-scale, pre-trained models such\nas OpenAI’s Whisper [1]. These models, trained on extensive multilingual datasets, have\ndemonstrated impressive performance across a wide range of languages, supporting appli-\ncations in voice assistants, automated transcription, and accessibility tools for the hearing\nimpaired. However, many low-resource languages, including Nepali, Hindi, and Albanian,\ncontinue to face challenges in achieving high transcription accuracy. These challenges stem\nfrom the limited availability of high-quality training data and the linguistic complexities\n∗These authors contributed equally to this work.\n†Corresponding authors\n1\n\nof these languages, such as their rich morphology, multiple dialects, and unique phonetic\nstructures.\nPrior works, including Whisper’s original implementation, have trained models using datasets\nlike Fleurs [12]. Fleurs provide only 10.38 hours of Nepali speech data with relatively short\naudio segments (2 to 10 seconds) and minimal variation in acoustic environments. As a\nresult, the Word Error Rates (WER) for Nepali transcriptions in these models remain sig-\nnificantly higher compared to widely spoken languages. The ASR systems pre-trained in a\nsupervised way across many datasets or domains are robust and can better generalize than\nmodels trained on a single source [2, 4, 3]. This is possible by combining the high-quality\ndatasets as much as possible. OpenAI’s Whisper [1] tried mitigating this by scaling weakly\nsupervised speech recognition to the order of 680,000 hours of labeled data covering more\nthan 96 languages. However, for low-resourced languages such as Nepali language, language-\nspecific finetuning incorporating sentiments, accents, pronunciation, acoustic environment,\ngender, and age proves to perform better, especially with long audio segments [5, 6, 7].\nIn recent years, several studies [5, 8, 9] have focused on fine-tuning ASR models for low-\nresource languages, yielding significant improvements in transcription accuracy.\nNotable\nworks include the Gram Vaani [5] and Vistaar [8] projects, which focused on languages\nlike Hindi, Marathi, and Gujarati. These studies have demonstrated the eﬀicacy of using\ndomain-specific datasets and fine-tuning techniques to reduce WER. Gram Vaani, a social\nenterprise in rural India providing voice-based interactions for call center automation, orga-\nnized an ASR challenge in 2022 to improve speech recognition for agricultural and healthcare\nadvisory systems. The study employed both traditional time-delay neural network-hidden\nMarkov models (TDNN-HMM) and fully neural end-to-end (E2E) models, showing remark-\nable improvements in WER, between 30.1% to 37.3%, across different models. Similarly,\nPatel et al. [11] showed a better performance of 30.3% WER using the E2E Conformer\nmodel, surpassing the baseline of 34.8% set during the Gram Vaani challenge. Vistaar [8]\nalso provides benchmark datasets for 59 Indian languages, facilitating comparative studies\nin diverse acoustic and linguistic environments. These initiatives emphasize the importance\nof creating rich, domain-specific datasets that reflect the linguistic diversity of the target\nlanguages. Moreover, the application of advanced models like Whisper [1] and wav2vec [10]\nin these studies has underscored the value of large-scale pre-training followed by language-\nspecific fine-tuning.\nBuilding on these advancements, our work focuses on fine-tuning Whisper models for Nepali\nASR which can also be implemented in other low-resourced languages. We leverage a diverse\nand extensive dataset that includes publicly available speech corpora such as Google Fleurs\n[12], Mozilla Common Voice [13], and OpenSLR [14, 15], along with a custom dataset built\nfrom self-recorded audios. The custom corpus encompasses a wide variety of audio environ-\nments, speaker demographics, and speech styles, significantly expanding the diversity and\nvolume of data available for training. The fine-tuned small model on the custom dataset\nshows a significant improvement of 68.5% compared to other ASR datasets. Moreover, we\nimplement data augmentation techniques, to further enhance the model’s robustness. By\nfine-tuning the Whisper models on this curated dataset, we significantly reduce WER across\nmultiple model sizes including tiny (68.5%), base (70.2%), small (36.2%), and medium\n(23.8%). Our approach demonstrates the critical role that dataset quality, variation, and\naugmentation play in improving ASR performance for underrepresented languages.\n2\n\n2\nDataset\nThe dataset used in this work consists of publicly available ASR datasets such as Google\nFleurs [12], Mozilla Common Voice [13], and OpenSLR datasets SLR43 [14] and SLR143\n[15]. Additionally, we also prepared a custom dataset from a diverse and extensive pool\nof self-recordings and publicly available sources, representing a wide range of speakers and\ntopic. As shown in Table 1, the cumulative dataset contains a total of 33.97 hours of raw\naudio, distributed across:\nTable 1: Raw audio data lengths on different datasets.\nDataset\nSize (Hrs)\nFleurs [12]\n10.38\nCommon Voice [13]\n1.28\nSLR43 [14]\n2.82\nSLR143 [15]\n1.25\nCustom Dataset\n18.24\nThe data includes speech from various environments, ranging from clean to noisy conditions,\nensuring that the transcriptions are accurate despite the background noise. This diverse\ndataset is critical for training robust speech recognition models that generalize well across\ndifferent acoustic settings.\n2.1\nCustom Dataset Preparation\nThe custom dataset primarily consists of read speech and lecture-style recordings, sourced\nfrom publicly available sources and self-recorded audio. The self-recorded portion of the\ndataset includes readings from online news articles, academic texts, and other sources. The\ndataset is diverse in terms of speaker demographics, containing data from various age groups,\nbackgrounds, sentiments, and genders, as detailed in Table 2. Moreover, as shown in Figure\n1, our custom dataset introduces significantly larger vocabularies compared to other open-\nsource datasets used here, allowing the model to generalize across a broader range of speech.\nWe discuss this improved generalization in detail in subsection 4.1.\nFigure 1: Distribution of unique words on custom dataset compared against other open-\nsource datasets. Here unique words don’t include articles, conjunctions, prepositions, and\nexclamations.\n3\n\nTable 2: Metadata of custom dataset\nMetadata\nRanges\nGender\nMale, Female, Unknown\nSpeaker Age\n25-60\nBackground\nClean, White Noise, Crowded\nSentiment\nHappy, Sad, Normal, Angry\nBefore merging the custom dataset’s raw audio to the corpus, we perform a few preprocessing\nto ensure consistency in data and metadata and removal of silence and unrecognized audio\nsegments.\nWe use Audacity[17], an audio editing tool to segment audio into chunks of\n30 seconds and remove artifacts like silence, and unrecognized audio segments. We first\nremove the silences > 1 second and then manually remove the unrecognized or corrupted\naudio segments. Transcriptions were either sourced from publicly available documents of the\ncorresponding audios or generated manually where required. For documents that contain\ntranscription errors, we apply manual corrections to improve the alignment with the audio,\nensuring high-quality training data. After this audio pre-processing, the size of the dataset\nwas reduced to 13.58 hours.\nTo increase the variability and robustness of the dataset, we employ data augmentation\ntechniques using torchaudio [16]. Specifically, we added 8000Hz white noise to the seg-\nmented audio, increasing the data volume while introducing minor distortions that mimic\nreal-world noisy environments. This augmentation process expanded the total size of the\ncustom dataset to 27.17 hours, and when combined with the other datasets, the total size\nof our corpus reached 42.9 hours.\n2.2\nTrain and Evaluation Dataset\nWe conducted a series of experiments for each dataset, fine-tuning Whisper’s pre-trained\nmodels across multiple configurations.\nThe training data was split into 80% for train-\ning and 20% for evaluation. The same data partitioning strategy was applied across all\ndatasets to ensure a fair comparison between our models and Whisper’s baseline models,\ntiny, base, small, and medium. In addition, we fine-tune the above models on combined\ndatasets: a) Fleurs, and Common Voice, b) Fleurs, Common Voice, and SLR (SLR43 +\nSLR143) c) Fleurs, Common Voice, SLR, and custom i.e. all_combined, d) augmented i.e.\nall_combined and its augmentation.\nWe shuffle the training and evaluation datasets separately to reduce the correlation between\nthem, which minimizes the risk of overfitting and allows the model to generalize better to\nunseen data. Moreover, for a fair and better generalization, we use evaluation data as 30%\nthe size of the shuffled training dataset for individual corpus. This is important because if\nwe split the evaluation data only from a specific dataset then it won’t generalize with the\nsame WER accuracy on other datasets. For example, in the case of Fleurs if both training\nand evaluation are samples of the Fleurs dataset, then it will perform poorly on the rest of\nthe dataset, especially when the audio segments are more than 10 seconds.\nAs shown in Figures 3 and 5, the average WER of the training data decreases progressively\nas the model learns to handle the diverse linguistic characteristics present in the Nepali\nlanguage, while the validation data consistently demonstrate improved performance as well.\n4\n\nFigure 2: Fine-tuning pipeline\n3\nFine-tuning Pipeline\nOur overall pipeline is shown in Figure 2. The pipeline consists of four main stages: data\npreparation, dataset processing, model training, and inference. As explained in subsection\n2.1, our custom dataset requires a few audio pre-processing which includes audio chunking,\nsilence removal, and unrecognized audio and transcriptions filtering.\nAfter forming the\ncustom dataset we merge it with other open-source datasets and all the transcripts into a\nsingle metadata.csv file.\nAll datasets are loaded in audiofolder format, followed by preprocessing to resample audio\nclips to a uniform 16kHz frequency.\nThe pipeline then performs feature extraction and\ntokenization to prepare the data for model training. The samples are then filtered by length\nand label suitability to ensure consistency across the dataset.\nFor model training, we employ the Whisper architecture [1] by fully training its base model\nrather than using a pre-trained model. The Whisper model is initialized with specific training\nconfigurations, and a train-test split is defined as described in subsection 2.2. During train-\ning, the pipeline utilizes Whisper’s architecture, loss functions, and optimizers enhancing the\nconvergence. Checkpoints are then saved periodically to allow resumption and model im-\nprovement tracking. Finally, the model is used for inference with an audio input to generate\ntranscriptions.\n4\nExperiments\nWe evaluate our fine-tuned Whisper models on various datasets, both independently and in\ncombination (all_combined). To assess the impact of data augmentation, we also compare\nperformance metrics, specifically loss and word error rate (WER) as primary metrics, before\n5\n\nand after augmentation on all_combined. Additionally, we perform a comparative analysis\nwith the Whisper models presented in OpenAI’s original paper, which used the Fleurs dataset\n[12] for the Nepali language.\nAll the experiments are performed on an Intel i9-10900 CPU @ 3.70GHz paired with 64GB\nDDR4 RAM and a 24GB NVIDIA GeForce RTX 3090 @ 33MHz GPU.\n4.1\nComparison on Different Datasets\nWe individually compare the results of fine-tuned small models across different individual\nand combined corpora. We initially fine-tuned the models on individual datasets as outlined\nin Table 1. However, given the relatively small size of these datasets, some models displayed\nsigns of overfitting after a certain number of epochs, despite adjustments to hyperparameters,\nleading to suboptimal results. The overfitting can be attributed to the limited diversity and\nvolume of the Nepali language datasets shown in Table 1, which proved insuﬀicient for\nrobust deep-learning training. To mitigate this issue, we earlystop the training process at\n1500 epoch and the results are shown in Figure 3. Following early stopping, all datasets\nshow a decreasing trend in loss and WER as shown in Figures 3 and 4, with the custom\ndataset demonstrating a marked improvement in WER across all the models compared to the\nother datasets. This enhancement is due to the custom dataset’s greater comprehensiveness,\nencompassing a wider range of metadata and a larger vocabulary.\nFigure 3: Comparison of WER on different individual datasets fine-tuned on tiny, base,\nsmall, and medium models. WER on our custom dataset is lower than other datasets across\nall the models, and more significant on larger models.\nFigure 4: Comparison of training and evaluation loss on individual datasets fine-tuned on\ntiny, base, small, and medium models.\nConsidering the overfitting issue on limited individual datasets, we combine these datasets on\na cumulative basis into a single corpus as explained in section 2.2 and perform a comparative\nanalysis. By training the models on this combined dataset for 4000 epochs, we achieve better\n6\n\nTable 3: Transcription prediction comparison on small fine-tuned model across different\ndatasets. Ground Truth refers to the original transcriptions from the dataset used for bench-\nmarking.\nDatasets\nPredictions\nGround\nTruth\nदेशभरप˃श्चमीवायुरस्थानीयवायुकोआंʺशकप्रभावरहेकोछ।मौसमपूवार्नुमान\nमहाशाखाकाअनुसारसोमबारदेशकापहाडीभूभागमाआंʺशकबदलीरहीबाँकɃ भूभागमा\nआंʺशकबदलीदेǺखमुख्यतःसफारहनेछ।कोशी, बागमती, गण्डकɃ, कणार्लीर\nसुदूरप˃श्चमप्रदेशकापहाडीभूभागकाएक-दुईस्थानमाहल्कावषार्कोसाथैउच्चपहाडी\nतथािहमालीभूभागकाएक-दुईस्थानमाहल्कािहमपातकोपिनसम्भावनारहेकोछ।\nFleurs\nदेशबरप˃श्चमीवायुरइस्थानीवाईकोआȥम्सकप्रवावरहेकोछमौसोमपूरुवानुमा\nशाखाकानुसारस्रोमबारदेशकोपाडीभूबागमाआȥम्सकबदǺलरैबाँकɃ भूबागमा\nआȥम्सकबदǺलदेǺखमौसोमुखखेतयासफारअिनछकोशीबागमतीगण्डकɃ खणार्ली\nरसुधुपʺशमप्रदेशकापाडीभूबागकाएकदुईस्तानमाहल्कावषार्कोसाथैउच्चपाडी\nतथाइिहमालीभूबागकाएकदुईस्तानमाहल्कािहमपादकोपिनसम्भावनारहेकोछ।\nCommon Voice\nदेशबरपʺशमीभाइुरेस्थानीभाइकोआȥम्सकप्रभाबरहेकोछमौसमपुरुभानुमानमा\nसाखाकानुसारशुहुम्भारदेशकोपाडीभोबागमाआȥम्सकबढालीरैवाकɃ भोबागमा\nअ◌ाȥम्सकबढालीदेǺखमौसममुख्खेठयासफारिनछकोशीबागम˃तगगण्डर्कɃ\nखणालीरसुदुपʺशमप्रद्यचकापाडीभोबागकाएक्दौस्थानमाहल्कावष्वार्कोसाथै\nउचपाडीतथाथाइिहमालीभोबागकाएक्दुस्थानमाहल्कािहम्पादकोपिनसम्भाबना\nरहेकोछ।\nSLR\nदेशबरप˃श्चमीबायुरेष्ठानीबाइकोआȥम्शकप्रबावरहेकोछमौसमपूरुबानुमा\nशाखाकानुसारस्रोमबारदेशकोपाडीभूबागमाआȥम्शकबदǺलरैबाँकɃ भूबागमा\nआȥम्शकबदǺलदेǺखमौसममुख्खेतयासभारिनछ्छकोशीबागȥम्तगण्रकɃ खणाǺलर\nसुदुपʺशमप्रदेशकापाडीभूबागकाएक्दुस्थानमाहल्कावषार्कोसाथैउचपाडी\nतथाइिहमालीभूबागकाएक्दुस्थानमाहल्कािहम्पादकोपिनसम्बाभनारहेकोछ।\nCustom\nDataset\nदेशबरप्र˃श्वमीभागीयरस्थानीबाँकोआंʹसकप्रभावरहेकोछमौसमपूवार्नुमा\nआशाखाकाअनुसारस्रोम्बारदेशकोपारीभूभागमाआंʹसकबदǺलरैबाँकɃ भूभागमा\nआंʹसकबदǺलदेǺखमौसम्मुखखेथयासफाराअिनछकोसीबागमतीगण्रकɃ खणार्Ǻलर\nसुद्दुपस्थीमप्रदेशकापारीभूभागकाएकदुईस्थानमाहल्कावषार्कोसाथैउच्चपारी\nतथािहमालीभूभागकाएकदुईस्थानमाहल्कािहम्पादकोपिनसम्भावनारहेकोछ।\nFleurs+Common\nVoice\nदेशबरप˃श्चमीभाग्युरइस्तानीभागीकोआȥम्शकप्रभावरहेकोछमौसमपूरुबानुमा\nऔंमाशाखाकानुसारश्रोमबारदेशकोपाडीभौभागमाआȥम्शकबदǺलरैबाँकɃ भौभागमा\nआȥम्शकबदǺलदेǺखमौसम्मुखखेथयासफारहिनछकोशीबाग्म˃तगण्रकɃ खणालीर\nसुद्दुपʺशमप्रदेशकापाडीभौभागकाएक्दुस्तानमाहल्कावषार्कोसाथैउच्चपाडीतथाथै\nिहमालीभौभागकाएक्दुस्तानमाहल्कािहमपादकोपिनसम्पाररहेकोछ।\nFleurs+Common\nVoice+SLR\nदेशबरप˃श्चमीबायुरइस्तानीबाईकोआȥम्शकप्रभावरहेकोछमौसमपूरुवानुमानमा\nशाखाकाअनुसारस्रोमबारदेशकोपाडीभूभागमाआȥम्शकबदǺलरैबाँकɃ भूभागमा\nआȥम्शकबदǺलदेǺखमौसममुख्यतयासफारिनछकोशीभागम˃तगण्रकɃ खणालीर\nसुदुपʺशमप्रदेशकापाडीभूभागकाएकदुईस्तानमाहल्कावषार्कोसाथैउच्चपाडी\nतथाइिहमालीभूभागकाएकदुईस्तानमाहल्कािहम्पादकोपिनसम्भावनारहेकोछ।\nall_combined\nदेशबरप˃श्चमीबािहँरस्थानीबाहीकोआȥम्शकप्रभावरहेकोछमौसमपूवार्नुमानमा\nआशाखाकाअनुसारस्रोमबारदेशकोपारीभूबागमाआȥम्शकबदǺलरहीबाँकɃ भूबागमा\nआȥम्शकबदǺलदेǺखमौसमभूखेथयासफारअिनछकोसीबागमातीगण्डकɃ खणार्ली\nरसुदुपʺशमप्रदेशकापारीभूबागकाएकदुईस्थानमाहल्कावषार्कोसाथैउच्चपारी\nतथाईिहमालीभूबागकाएकदुईस्थानमाहल्कािहम्पादकोपिनसम्भावनारहेकोछ।\n7\n\nFigure 5:\nComparison of WER on cumulative datasets with Fleurs as the baseline.\nall_combined outperforms others which becomes more significant as the model complexity\nincreases.\ngeneralization without encountering overfitting, as shown by the significant drop in WER in\nFigure 6.\nAs illustrated in Figures 3 and 5, the custom dataset’s effectiveness in reducing WER is com-\nparable to the cumulative impact of the Fleurs, Common Voice, and SLR datasets, under-\nscoring the dataset’s exhaustiveness and eﬀiciency. Furthermore, adding data from Fleurs,\ninitially used by Whisper, resulted in a progressively smoother WER trend. Notably, the\ncomprehensive combination of all datasets, represented as all_combined, achieved a signif-\nicant WER reduction of approximately 56%. To further assess prediction accuracy across\nindividual and combined corpora, we present a comparative table, Table 3 comparing the\npredictions from each dataset with the ground truth. This table underscores the custom\ndataset’s prediction accuracy and demonstrates the datasets’ combined effectiveness, as re-\nflected in the corresponding WER plots. Additionally, as seen in Table 3, a comparison of\nmodels trained on open-source datasets versus the custom dataset shows that models trained\non open-source datasets struggle with audio inputs exceeding 15 seconds, producing random\npredictions beyond a certain audio length. In contrast, our custom dataset better aligns\nwith Whisper’s input requirements, resulting in improved and more accurate predictions on\nlonger audio segments.\nTable 4: WER comparison on models fine-tuned with different datasets for 1500 epochs.\nDatasets\nModels\ntiny\nbase\nsmall\nmedium\nFleurs\n90.34\n91.01\n80.7\n73.92\nCommon Voice\n104.04\n108.92\n87.9\n79.85\nSLR\n95.07\n99.38\n81.0\n71.77\nCustom Dataset\n84.89\n91.37\n68.5\n55.47\nFleurs+Common Voice\n92.58\n92.24\n85.0\n–\nFleurs+Common Voice+SLR\n86.82\n84.65\n67.0\n–\n4.2\nAugmentation Results\nFollowing the combination of datasets, we applied data augmentation techniques to further\nenhance model performance. Specifically, we introduce an 8000Hz white noise to the raw\naudio using torchaudio [16]. This augmentation is done on all_combined dataset only for\nthe audios whose resampled noise lengths are less than the original audio length. Although\nvery simple, introducing white noise to the raw audio data not only increased the volume\nof the dataset but also improved the model’s performance, as reflected by the decrease in\n8\n\nWER in Figure 6. This highlights the role of data augmentation as a critical step in model\nfine-tuning, especially when working with limited data. While the observed WER reduction\nis modest ( 4%), we anticipate that more sophisticated augmentation methods could yield\nmore substantial improvements. However, as data augmentation is not the primary focus\nof our study, we only demonstrate a basic augmentation method to show how even simple\ntechniques can improve model performance.\nGiven that we have already compared WER and predictions across individual and combined\ndatasets, we restrict our evaluation of augmentation results to the small fine-tuned model\non the combined dataset (all_combined). The observed improvements are consistent across\nother datasets as well, as evidenced by Figure 7, which illustrates training on the Fleurs\ndataset with augmentation up to 7,000 epochs without overfitting. Predictions from models\ntrained on all_combined and augmented datasets are presented in Table 5, demonstrat-\ning subtle yet meaningful improvements in prediction accuracy for the model trained with\naugmented data.\nTable 5: Impact of data augmentation on prediction (with small fine-tuned model).\nDatasets\nPredictions\nGround\nTruth\nिवदेशीमुद्रासʹञ्च˃तकोलाभउठाउनसमेतसरकारअसफल।चरमकुपोषणप्रभािवतिवश्वका\nकȼरब२०लाखबालबाǺलकामृत्युकोजोǺखममारहेकोयूिनसेफकोचेतावनीउपचारात्मकतयारी\nभोजनखȼरदकालािगआʻथकसहयोगकोआह्वान।\nall_combined\nिवदेशीमुद्रासंȥस्थ˃तकोलाभउठाउनसमेतसरकारअसफतजरमकोपोषणप्रभािवत\nिवश्वकाकȼरब२०लाखबालबाǺलकाब्रत्युकोचोǺखमारहेकोयुिनस्टेफकोचेतावनी\nउपजारात्मकतयारीभ्रजनखȼरदकालािगआʻथकसहयोगवावान।\naugmented\nिवदेशीमुद्रासंस्कृ˃तकोलाभउठाउनसमेतसरकारअवसफतजरमकोपोषर्णप्रभािवत\nिवश्वकाकȼरब२०लाखबालबालीकाब्रत्युकोजोǺखम्बारहेकोयुिनससेफकोचेतावनी\nउपजारात्मकतयारीभ्रजनखȼरदकालािगआʻथकसहयोगवावहान।\nGround\nTruth\nरसाफमिहलाच्याȥम्पयनʹसपकोसातौंसंस्करणभोǺलदेǺखकाठमाडौंमाउद्घाटनखेलमाभारत\nरपािकस्तानबीचप्र˃तस्पधार्बǺलयोटोलीनेपाललाईइ˃तहासरच्नेमौका।\nall_combined रसाथमिहलाच्याȥम्पयȥन्सपकोसातौँसंस्करणभोǺलदेǺखकाख्मर्डौंमाउद्घाटनखेलमा\nभारतरपािकस्तानिबचप्र˃तस्पदार्बǺलयोटोलीनेपाललाईइ˃तहासरसनेमौका।\naugmented\nरसाथमिहलाच्याȥम्पयȥन्सपकोसातौंसंस्करणभोǺलदेǺखकाटर्माडौंमाउद्घाटनखेलमा\nभारतरपािकस्तानिबचप्र˃तस्पदार्बǺलयोटोलीनेपाललाईइ˃तहासरस्नेमौका।\n4.3\nComparison with Whisper\nAlongside our custom dataset, we perform a comparative analysis on the benchmark dataset,\nFleurs [12] originally used by Whisper for a comprehensive and fair evaluation. We finetune\nthe tiny, base, small, and medium whisper models. However, because of the GPU limi-\ntation, we couldn’t fully train the large-v1 and large-v2 models. Due to an overfitting\nissue as explained in subsection 4.1, we use augmented datasets for training the models.\nTable 6 compares WER on various models between the whisper and our approach. We also\nshow a progressive graphical comparison of WERs on different fine-tuned models in Figure\n7. Our fine-tuned models show a significantly improved WER on all the trained models for\nthe Nepali language. This improvement is most evident in the small and medium models,\n9\n\nFigure 6: Comparison of WER on all_combined and augmented datasets. Here augmented\nrefers to all_combined+custom augmented.\nwhere the WER was reduced from 69.5 to 36.2 and 54.4 to 23.8 respectively, representing a\nsubstantial performance gain.\nFigure 7: WER on Fleurs dataset [12] fine-tuned on tiny, base, small, and medium whisper\nmodels.\nTable 6: WER comparison between OpenAI’s whisper [1] small and our fine-tuned models\non Fleurs datasets.\nModels\nWhisper\nFleurs\ntiny\n101.8\n68.5\nbase\n102.4\n70.2\nsmall\n69.5\n36.2\nmedium\n54.4\n23.8\nAs seen in the prediction comparison Table 7, although the ground truth is in Devanagari\nscript, the transcriptions generated by Whisper’s tiny and base are in Latin script, which\nexplains the WER > 100 in these models. However, our fine-tuned models across all the\nmodel sizes generate predictions in Devanagari scripts while still incorporating the numbers\nand punctuations. As evident from Tables 6 and 7, our finetuned models generate better\npredictions than Whisper’s original models with significantly lower WER.\n10\n\nTable 7: Transcription prediction comparison between\nOpenAI’s Whisper [1] and our fine-tuned models on\nFleurs dataset.\nModel\nWhisper\nOurs (Fleurs)\nGround\nTruth\nरसाफमिहलाच्याȥम्पयनʹसपकोसातौंसंस्करणभोǺलदेǺखकाठमाडौंमाउद्घाटनखेलमा\nभारतरपािकस्तानबीचप्र˃तस्पधार्बǺलयोटोलीनेपाललाईइ˃तहासरच्नेमौका।\ntiny\nRassa, you’ve got to understand,\nyou’ve got to understand. Rassa,\nyou’ve got to understand. Rassa,\nyou’ve got to understand. Rassa,\nyou’ve got to understand.\nरस्वाथमिहलाच्यापेȥन्सटकोसातो\nसम्सकरणबोलीदेǺखकाथ्मन्डोबाउत्घातन\nकेलमाभारत्रपाकस्तालिवषप्र˃तस्पदार्\nबǺलयोटोलीनेपाललाइ˃तहासरोफ्नेमौका\nbase\nRaasad Mahila Champin Sip ko\nSaatom Samskarant Boli Dhe Ki\nKart Mandoma. Uddhgatant\nKheelma Bhaar Atra Pakistan Bis\nPratis Pratis Pradha. Balyotoli\nNeparla Itihas Rox Ne Mau Ka.\nरसाथमिहलाच्याप्प्यान्पेȥन्सपकोसातौं\nसम्सकरणबोलीदेǺखकातबन्डौमाउत्द्गातन\nखेलमाभारतपािकस्थानिवजप्र˃तȥस्थश\nबǺलयोटोलीनेपारलाईइ˃तहासरस्रोस्थ्धै\nमौखुनेमौगका।\nsmall\nरसाथमहलाचामिपȥन्सपकोसाथो\nसम्सकरनभोलीदेकɃ काध्मन्दोमाउद्गाटन\nकेल्माभारद्रपािकस्तानभीज्प्र˃तस्पदार्\nबǺलयोतोलीिनपाललाईइ˃तहासरोसने\nमाउका\nरसाथमिहलाच्याȥम्पयनʹसपकोसातौँ\nसम्स्करणभोलीदेǺखकाटर्मण्डोमाउत्गाटन\nखेलमाभारतरपािकस्तानबीचप्र˃तस्पदार्\nबǺलयोटोलीनेपाललाईइ˃तहासरस्नेमौका।\nmedium\nरसापमिहलाचाȥम्पनʹसपकोसातो\nसम्सकरनबोलीदेखीकाट्मनडोंमाउध्गातन\nखेलमाभारतरपािकस्तानबीजपर˃तस्पदार्\nबल्योतोलीिनपारलाइ˃तहासरौसनेमोवका\nरसाथमिहलाच्याȥम्पयनʹसपकोसातौँ\nसंस्करणबोǺलदेǺखकाटर्मन्डोउमाउद्घातन\nखेलमाभारतरपािकस्तानबीचप्र˃तस्पदार्\nबǺलयोटोलीनेपारलाईइ˃तहासरोस्नेमौका।\nGround\nTruth\nप्र˃तकूलमौसमलेउडानठप्पहुँदामन्थलीमा३िदनदेǺखअलपत्रपयर्टकलाईसेनाकोजहा-\nजबाटलुक्लापुयार्इनेिबपीराजमागर्मासाँझ६देǺखिबहान४बजेसम्मसवारीचलाउनरोक\n।\ntiny\npratikul mousam le udan thapa\nhuda muntali matindin deki\nalapatra podyata klasena\nkudzahazbatun lu klapuryaini dpi\nrazmargama saa jatshaudi ki\nbihanat saar bodis sama savali\nsaalaun arho\nव्र˃तकोलमौसम्लेउदानथप्पहुँदाबन्थली\nमािकȥन्धनदेǺखअलपत्रपȼरयतकले\nसेनाकोजहासबाटलुकलापूयार्इनीिदिप\nराजमागर्मासाजछौदेकɃ िवहालचार\nवʹजससम्बसबािटछलाउनरो।\nbase\nPratikul Mausam le Udan\nThappahuda Manthali Matin Dindi\nki alapatra Pariyataklaisena\nKuzhaha Zbata Nukhla Puriayini\nDeepi Rajmargama Saadach\nSaadakhi Bhihana Chhar Bhajis\nSamasavari Salau Na Roo\nप्र˃तकूलमौसमलेउडानठप्वपपहुँदा\nमन्थलीमातीनिदनिदनदेǺखअलपत्र\nपȼरयोटकलाईसेनाकोजहाजबाजबाटलुक्ला\nपूयार्इनेिदपीराजमाजर्मागर्मासाजछछौदेǺख\nिबहालाचारबजेस्सम्मसवारीचलाउनरोू।\nsmall\nब्र˃तक्यलमुस्मलेउडानथबरूदामन्त्तलीमा\nतींिदनदेखीआलपत्रपयर्टकलेसेनाको\nजाहाजबातलुक्लापुयार्ईनेिदपीराजमरगमा\nसाजज़चोदेखीभीहानाचारबजेसमस्वारी\nचलाओनरोग\nर˃तकुलमौसमलेउडानठपहुँदामन्थलीमा\nतीनिदनदेǺखअलपत्रपयर्टकलाईसेनाको\nजहाजबाटलुकलापुर्‍याइनेिविपराजमागर्मा\nसाझ्जतछदेǺखिबहानचारबजेसम्बसवानी\nचलाउनरोग।\n11\n\nmedium\nप्र˃तकुलमौसमलेउडानथपपूदामंथलीमा\n˃तनिदनदेिकअलपत्रपयर्तक्लेसेनाको\nजहाजबातलुकलापुयार्इिनभीपीराजमागर्मा\nसाज़देिकʺभहानचारबजेसमसवारी\nचलाउनरोक\nप्र˃तकूलमौसमलेउडानठपहुँदामन्थलीमा\nतीनिदनदेǺखअलपत्रपयर्टकलाईसेनाको\nजहाजबाटलुकलापुयार्इनेबीपीराजमागर्मा\nसाझत6 देǺखिबहाल4 बजेसम्मसवारी\nचलाउनरोग।\n5\nDiscussion and Conclusion\nIn this study, we focus on creating an exhaustive dataset and fine-tuning OpenAI’s Whis-\nper models for the task of Nepali language transcription, addressing key challenges such as\ndataset limitations and model overfitting. Our results demonstrate that dataset and vocab-\nulary size, inclusivity, variability, model input compatibility, and augmentation strategies\nplay a critical role in improving the model’s transcription performance, as evaluated through\nWER.\nThe differences in performance between models trained on other open-source ASR datasets\nand our custom dataset are notable. The custom dataset provides the model with a more\ncomprehensive representation of the linguistic variations in the Nepali language in terms of\ndialect, speaker accents, and environments compared to the Fleurs dataset used in Whis-\nper’s original paper and other open-source datasets compared here. This diversity allows\nthe model to capture more nuanced features of the Nepali language, leading to better gen-\neralization. Comparing quantitatively, the fine-tuned models on the custom dataset show a\nsignificant improvement across all individual, combined, and augmented datasets. Combin-\ning the individual corpus and data augmentation further enhanced the model’s performance.\nWhile we used relatively simple augmentation, it demonstrates that even minor augmenta-\ntion techniques can significantly enhance transcription accuracy in low-resource languages\nlike Nepali.\nWhen comparing our fine-tuned models with OpenAI’s Whisper models, the results show\nthat our models significantly outperform the original Whisper models across all evaluated\nsizes—tiny, base, small, and medium. The progressive improvement across model sizes\nhighlights the effectiveness of fine-tuning for domain-specific tasks, even with limited com-\nputing resources. While Whisper’s training data, Fleurs consists of relatively short audio\nclips (2s-10s), our dataset contains longer and denser audio clips ranging from 5s to 30s.\nThis wide range of clips is more compatible with Whisper’s input, enabling the model to\nbetter capture contextual information over extended sequences.\nIn conclusion, our work shows the importance of dataset and their effectiveness on increasing\nthe accuracy of speech-to-text model such as Whisper. Future work could explore more\nadvanced augmentation techniques, fine-tuning larger Whisper models, and implementing\nsimilar fine-tuning approaches on other limited-resourced languages, potentially leading to\nfurther improvements in transcription accuracy.\n12\n\nReferences\n[1] Radford, A., Kim, J., Xu, T., Brockman, G., McLeavey, C. & Sutskever, I. Robust\nspeech recognition via large-scale weak supervision. International Conference On Ma-\nchine Learning. pp. 28492-28518 (2023)\n[2] Narayanan, A., Misra, A., Sim, K., Pundak, G., Tripathi, A., Elfeky, M., Haghani, P.,\nStrohman, T. & Bacchiani, M. Toward domain-invariant speech recognition via large\nscale training. 2018 IEEE Spoken Language Technology Workshop (SLT). pp. 441-447\n(2018)\n[3] Hsu, W., Sriram, A., Baevski, A., Likhomanenko, T., Xu, Q., Pratap, V., Kahn, J.,\nLee, A., Collobert, R., Synnaeve, G. & Others Robust wav2vec 2.0: Analyzing domain\nshift in self-supervised pre-training. ArXiv Preprint ArXiv:2104.01027. (2021)\n[4] Chen, G., Chai, S., Wang, G., Du, J., Zhang, W., Weng, C., Su, D., Povey, D., Trmal,\nJ., Zhang, J. & Others Gigaspeech: An evolving, multi-domain asr corpus with 10,000\nhours of transcribed audio. ArXiv Preprint ArXiv:2106.06909. (2021)\n[5] Bhanushali, A., Bridgman, G., Deekshitha, G., Ghosh, P., Kumar, P., Kumar, S.,\nKolladath, A., Ravi, N., Seth, A., Singh, A. & Others Gram Vaani ASR Challenge\non spontaneous telephone speech recordings in regional variations of Hindi. Proceedings\nOf The Annual Conference Of The International Speech Communication Association,\nINTERSPEECH. 2022 pp. 3548-3552 (2022)\n[6] Sailor, H., Patil, A. & Patil, H. Advances in Low Resource ASR: A Deep Learning\nPerspective.. SLTU. pp. 15-19 (2018)\n[7] Parikh, A., Bosch, L., Heuvel, H. & Tejedor-Garcı́a, C. Comparing Modular and End-\nTo-End Approaches in ASR for Well-Resourced and Low-Resourced Languages. Proceed-\nings Of The 6th International Conference On Natural Language And Speech Processing\n(ICNLSP 2023). pp. 266-273 (2023)\n[8] Bhogale, K., Sundaresan, S., Raman, A., Javed, T., Khapra, M. & Kumar, P. Vis-\ntaar: Diverse Benchmarks and Training Sets for Indian Language ASR. ArXiv Preprint\nArXiv:2305.15386. (2023)\n[9] Mainzinger, J. Fine-Tuning ASR Models for Very Low-Resource Languages: A Study\non Mvskoke. (University of Washington,2024)\n[10] Baevski, A., Zhou, Y., Mohamed, A. & Auli, M. wav2vec 2.0: A framework for self-\nsupervised learning of speech representations. Advances In Neural Information Process-\ning Systems. 33 pp. 12449-12460 (2020)\n[11] Patel, T. & Scharenborg, O. Using cross-model learnings for the Gram Vaani ASR\nChallenge 2022.. INTERSPEECH. pp. 4880-4884 (2022)\n[12] Conneau, A., Ma, M., Khanuja, S., Zhang, Y., Axelrod, V., Dalmia, S., Riesa, J., Rivera,\nC. & Bapna, A. FLEURS: Few-shot Learning Evaluation of Universal Representations\nof Speech. (2022)\n[13] Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R.,\nSaunders, L., Tyers, F. & Weber, G. Common Voice: A Massively-Multilingual Speech\nCorpus. (2020)\n13\n\n[14] Sodimana, K., Pipatsrisawat, K., Ha, L., Jansche, M., Kjartansson, O., Silva, P. &\nSarin, S. A Step-by-Step Process for Building TTS Voices Using Open Source Data and\nFramework for Bangla, Javanese, Khmer, Nepali, Sinhala, and Sundanese. Proc. The\n6th Intl. Workshop On Spoken Language Technologies For Under-Resourced Languages\n(SLTU). pp. 66-70 (2018,8), http://dx.doi.org/10.21437/SLTU.2018-14\n[15] Khadka, S., G.C., R., Paudel, P., Shah, R. & Joshi, B. Nepali Text-to-Speech Synthesis\nusing Tacotron2 for Melspectrogram Generation. SIGUL 2023, 2nd Annual Meeting Of\nThe Special Interest Group On Under-resourced Languages: A Satellite Workshop Of\nInterspeech 2023. (2023)\n[16] Yang, Y., Hira, M., Ni, Z., Astafurov, A., Chen, C., Puhrsch, C., Pollack, D., Genzel, D.,\nGreenberg, D., Yang, E. & Others Torchaudio: Building blocks for audio and speech\nprocessing. ICASSP 2022-2022 IEEE International Conference On Acoustics, Speech\nAnd Signal Processing (ICASSP). pp. 6982-6986 (2022)\n[17] Audacity audacity. (GitHub Repository,2019), https://github.com/audacity/audacity\n14",
    "pdf_filename": "Whisper_Finetuning_on_Nepali_Language.pdf"
}