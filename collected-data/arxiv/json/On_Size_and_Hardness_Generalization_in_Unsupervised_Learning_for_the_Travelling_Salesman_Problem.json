{
    "title": "On Size and Hardness Generalization in Unsupervised Learning for the",
    "abstract": "asksthefollowingquestion: Givenalistofcitiesandthe WestudythegeneralizationcapabilityofUnsuper- distancesbetweeneachpairofcities,whatistheshortest visedLearninginsolvingtheTravellingSalesman possibleroutethatvisitseachcityexactlyonceandreturns Problem(TSP).WeuseaGraphNeuralNetwork totheorigincity? Avarietyofmethodshavebeendevel- (GNN)trainedwithasurrogatelossfunctionto opedtosolveTSP,includingtheLin-Kernighan-Helsgaun generate an embedding for each node. We use (LKH) heuristics, which is known for their effectiveness these embeddings to construct a heat map that inapproximatingsolutions(Helsgaun,2000),andtheCon- indicatesthelikelihoodofeachedgebeingpartof cordesolver,whichguaranteesoptimalityofthesolutions. theoptimalroute. Wethenapplylocalsearchto TheapplicationofMLforTSPhasprimarilyfocusedonSu- generateourfinalpredictions. Ourinvestigation pervisedLearning(SL)andReinforcementLearning(RL). exploreshowdifferenttraininginstancesizes,em- However,SLmethodsencounterthechallengeofexpensive beddingdimensions,anddistributionsinfluence annotations,whileRLmethodsstrugglewithsparsereward theoutcomesofUnsupervisedLearningmethods. problems. Ourresultsshowthattrainingwithlargerinstance sizesandincreasingembeddingdimensionscan Recently,(Minetal.,2024)proposesanewapproachnamed buildamoreeffectiverepresentation,enhancing UTSPthatemploysUnsupervisedLearning(UL)tobuild themodel’sabilitytosolveTSP.Furthermore,in a data-driven heuristics for the TSP. This unsupervised evaluatinggeneralizationacrossdifferentdistribu- methoddoesnotdependonanylabelleddatasetandgener- tions,wefirstdeterminethehardnessofvarious atesaheatmapinanon-autoregressivemanner,offeringa distributionsandexplorehowdifferenthardnesses distinctalternativetotraditionalSLandRLmodels. affectthefinalresults. Ourfindingssuggestthat While the UL heuristics offer a promising approach, the models trained on harder instances exhibit bet- challengeofgeneralizingacrossvaryingsizesanddistribu- ter generalization capabilities, highlighting the tionsremainssignificant. Inparticular,themodelpresented importance of selecting appropriate training in- in (Min et al., 2024) requires retraining to adapt to new stancesinsolvingTSPusingUnsupervisedLearn- sizes, indicating that a model trained on one size cannot ing. effectivelygeneralizetodifferentsizes. Thispaperexploresthegeneralizationcapabilitiesofunsu- 1.Introduction pervisedheuristicsfortheTSP.Ourfindingsindicatethat theULmodelisabletogeneralizeacrossdifferentproblem ThegoalofmachinelearningforCombinatorialOptimiza- sizes. Regardingthegeneralizationbehaviorofdifferentdis- tion(CO)istoenhanceorsurpasshandcraftedheuristics. tributions,basedonthehardnessresultsby(Gent&Walsh, Recently, there has been an increasing trend in applying 1996),werelatedifferentdistributionstodistinctlevelsof Machine Learning (ML) to tackle CO problems (Bengio hardnesses. Thisallowsustoinvestigatetheimpactofthe et al., 2021). Different from manually crafted heuristics, trainingdata’shardnessonthemodel’sperformance. machinelearningapproachesharnessthepowerofdatato Ourprimarycontributionsareoutlinedasfollows: Wepro- uncoverpatternsinCOproblems. pose a novel approach for enabling a TSP model, once TheEuclideanTravellingSalesmanProblem(TSP)isoneof trained,togeneralizeeffectivelyacrossdifferentproblem sizes. Weshowthattrainingwithlargerproblemsizescan 1Department of Computer Science, Cornell University, enhancemodelperformance. Furthermore,weinvestigate Ithaca 14850, USA. Correspondence to: Yimeng Min <min@cs.cornell.edu>. theimpactofvariousembeddingdimensionsonTSPper- formance, finding that larger embedding dimensions can Preprint 1 4202 voN 91 ]IA.sc[ 2v21202.3042:viXra",
    "body": "On Size and Hardness Generalization in Unsupervised Learning for the\nTravelling Salesman Problem\nYimemgMin1 CarlaP.Gomes1\nAbstract themostfamousandintensivelystudiedCOproblems. TSP\nasksthefollowingquestion: Givenalistofcitiesandthe\nWestudythegeneralizationcapabilityofUnsuper-\ndistancesbetweeneachpairofcities,whatistheshortest\nvisedLearninginsolvingtheTravellingSalesman\npossibleroutethatvisitseachcityexactlyonceandreturns\nProblem(TSP).WeuseaGraphNeuralNetwork\ntotheorigincity? Avarietyofmethodshavebeendevel-\n(GNN)trainedwithasurrogatelossfunctionto\nopedtosolveTSP,includingtheLin-Kernighan-Helsgaun\ngenerate an embedding for each node. We use\n(LKH) heuristics, which is known for their effectiveness\nthese embeddings to construct a heat map that\ninapproximatingsolutions(Helsgaun,2000),andtheCon-\nindicatesthelikelihoodofeachedgebeingpartof\ncordesolver,whichguaranteesoptimalityofthesolutions.\ntheoptimalroute. Wethenapplylocalsearchto\nTheapplicationofMLforTSPhasprimarilyfocusedonSu-\ngenerateourfinalpredictions. Ourinvestigation\npervisedLearning(SL)andReinforcementLearning(RL).\nexploreshowdifferenttraininginstancesizes,em-\nHowever,SLmethodsencounterthechallengeofexpensive\nbeddingdimensions,anddistributionsinfluence\nannotations,whileRLmethodsstrugglewithsparsereward\ntheoutcomesofUnsupervisedLearningmethods.\nproblems.\nOurresultsshowthattrainingwithlargerinstance\nsizesandincreasingembeddingdimensionscan Recently,(Minetal.,2024)proposesanewapproachnamed\nbuildamoreeffectiverepresentation,enhancing UTSPthatemploysUnsupervisedLearning(UL)tobuild\nthemodel’sabilitytosolveTSP.Furthermore,in a data-driven heuristics for the TSP. This unsupervised\nevaluatinggeneralizationacrossdifferentdistribu- methoddoesnotdependonanylabelleddatasetandgener-\ntions,wefirstdeterminethehardnessofvarious atesaheatmapinanon-autoregressivemanner,offeringa\ndistributionsandexplorehowdifferenthardnesses distinctalternativetotraditionalSLandRLmodels.\naffectthefinalresults. Ourfindingssuggestthat\nWhile the UL heuristics offer a promising approach, the\nmodels trained on harder instances exhibit bet-\nchallengeofgeneralizingacrossvaryingsizesanddistribu-\nter generalization capabilities, highlighting the\ntionsremainssignificant. Inparticular,themodelpresented\nimportance of selecting appropriate training in-\nin (Min et al., 2024) requires retraining to adapt to new\nstancesinsolvingTSPusingUnsupervisedLearn-\nsizes, indicating that a model trained on one size cannot\ning.\neffectivelygeneralizetodifferentsizes.\nThispaperexploresthegeneralizationcapabilitiesofunsu-\n1.Introduction pervisedheuristicsfortheTSP.Ourfindingsindicatethat\ntheULmodelisabletogeneralizeacrossdifferentproblem\nThegoalofmachinelearningforCombinatorialOptimiza- sizes. Regardingthegeneralizationbehaviorofdifferentdis-\ntion(CO)istoenhanceorsurpasshandcraftedheuristics. tributions,basedonthehardnessresultsby(Gent&Walsh,\nRecently, there has been an increasing trend in applying 1996),werelatedifferentdistributionstodistinctlevelsof\nMachine Learning (ML) to tackle CO problems (Bengio hardnesses. Thisallowsustoinvestigatetheimpactofthe\net al., 2021). Different from manually crafted heuristics, trainingdata’shardnessonthemodel’sperformance.\nmachinelearningapproachesharnessthepowerofdatato\nOurprimarycontributionsareoutlinedasfollows: Wepro-\nuncoverpatternsinCOproblems.\npose a novel approach for enabling a TSP model, once\nTheEuclideanTravellingSalesmanProblem(TSP)isoneof trained,togeneralizeeffectivelyacrossdifferentproblem\nsizes. Weshowthattrainingwithlargerproblemsizescan\n1Department of Computer Science, Cornell University,\nenhancemodelperformance. Furthermore,weinvestigate\nIthaca 14850, USA. Correspondence to: Yimeng Min\n<min@cs.cornell.edu>. theimpactofvariousembeddingdimensionsonTSPper-\nformance, finding that larger embedding dimensions can\nPreprint\n1\n4202\nvoN\n91\n]IA.sc[\n2v21202.3042:viXra\nOnSizeandHardnessGeneralizationinUnsupervisedLearningfortheTravellingSalesmanProblem\nbuildmoreeffectiverepresentationstoguidethesearchpro- (Xinetal.,2021)trainsaSparseGraphNetworkusingSLto\ncess. Additionally, we explore how the model performs evaluateedgescores,whicharethenintegratedwiththeLin-\nwhentrainedondatasetsofvaryingdistributions. Ourfind- Kernighan-Helsgaun(LKH)algorithmtoguideitssearch\ningsindicatethatmodelstrainedonharderinstancesexhibit process. (Fuetal.,2021)usesaGNNtolearnfromsolved\nbetterperformance,whichunderscorestheimportanceof optimal solutions. The model is trained on a small-scale\ntraininginstances’distributionwithintheframeworkofUL instances,whichcouldbeusedtobuildlargerheatmaps.\nforsolvingCOproblemsliketheTSP.\nHowever, In SL, the generation of optimal solutions for\nWhile recent research papers explored using data-driven trainingistime-consuming.Findingoptimalornear-optimal\ntechniquesforCOproblems,mosthavefocusedonSLor solutionsforlargeTSPinstancesrequiressignificantcom-\nRL.Veryfewhaveexaminedthegeneralizationbehaviours, putationalresourcesandsophisticatedalgorithms.\nparticularlyhowtrainingdata(differentdistributionsofTSP\nIn other words, an ideal model should circumvent these\ninstances) influences final model performance (Bi et al.,\nissues,avoidingthesparserewardprobleminRLandnot\n2022). Ourworkaddressesthisgap,offeringinsightsinto\nrelyingonlabelledoptimalsolutuonsinSL.Addressingthis,\nthesignificanceoftrainingdataselectionanditsdirectim-\narecentapproachby(Minetal.,2024)usesunsupervised\npactontheeffectivenessofMLmodelsforCOtasks. This\nlearning(UL)andtrainsaGNNusingasurrogateloss. The\nexplorationcontributestounderstandingMLmodelsinCO\nmodel generates heat maps through a non-autoregressive\nandprovidespracticalguidelinesforimprovingmodelgen-\nprocess, without relying on labelled optimal solutions or\neralizationandperformanceinsolvingTSP.\nrequiring the agents to explore better solutions, thereby\ncircumventingtheneedforexpensiveannotationandmiti-\n2.Relatedworks gatingthesparserewardproblem.\n2.1.RLforTSP Thispaperisstructuredasfollows: Section3introducesthe\nbackgroundofULforTSP.Section4presentsamethodfor\nThegoalofusingRLforCOistotrainanagentcapable\ngeneralizingacrossvariousproblemsizes. Section5investi-\nof either maximizing or minimizing the expected sum of\ngatesthegeneralizationbehaviorw.r.t. differentembedding\nfuture rewards, known as the return. For a given policy,\ndimensionsandtrainingsizes. Finally,Section6explores\nthe expected return from a current state is defined as the\nthegeneralizationacrossdifferentdistributionsthroughthe\nvaluefunction. InthecontextofTSP,RLtypicallyfocuses\nlensofinstancehardness.\nonminimizingthelengthofthepredictedroute(Yeetal.,\n2024;Zhouetal.,2023;Chenetal.,2024;Maetal.,2024).\n3.ULforTSP\nForexample,(Kooletal.,2019)proposesamodelbasedon\nattentionlayersandtrainsthemodelusingRLusingadeter-\nLet’srevisitthedefinitionoftheTSP.Essentially,theTSP\nministicgreedyrollout.(Belloetal.,2016)trainsarecurrent\ncanbereinterpretedasidentifyingtheshortestHamiltonian\nneuralnetworktopredictpermutationsofcitycoordinates\nCyclethatencompassesallthecities. InULforTSP,the\nandoptimizestheparameterswithapolicygradientmethod\nauthorsfirstreformulatetheTSPintotwoconstraints: the\nusingthenegativetourlengthasarewardsignal.\nshortest path constraint and the Hamiltonian Cycle con-\nHowever, as the size of the TSP increases, the rewards straint. Subsequently, they construct a proxy for each of\nbecomeincreasinglysparse,necessitatinglongexploration theseconstraints(Minetal.,2024).\nsteps before the agent achieves a positive return. So the\nInUTSP,givenncitiesandtheircoordinates(x ,y )∈R2,\ni i\nRLsettingischallengingasitonlylearnsoncetheagent,\nUTSP first uses GNN to generate a soft indicator matrix\nrandomly or through more sophisticated strategies, finds\nT ∈ Rn×n and use T to build the heat map H ∈ Rn×n.\na better solution. Additionally, within RL, the learning\nRow i of H represents the probability distribution of di-\nprocessishardtoconverge,andtheprocessmaybecome\nrected edges originating from city i, while column j cor-\ntrapped in local minima, as discussed in (Bengio et al.,\nresponds to the probability distribution of directed edges\n2021).\nterminatingincityj. Thisheatmapissubsequentlyused\ntodirectalocalsearchalgorithm. Asmentioned,thegoal\n2.2.SLForTSP\nof UTSP is to construct a proxy for two constraints. For\nInSL,themodelistrainedwithadatasetincludinginput the shortest constraint, the authors optimize the distance\ncoordinatesalongsidetheircorrespondingoptimalTSPso- term: ⟨D,H⟩ =\n(cid:80)n i=1(cid:80)n\nj=1D i,jH i,j, where⟨·,·⟩isthe\nlutions. Theobjectiveistoidentifyafunctionthatpredicts\nFrobeniusinnerproduct,D∈Rn×nisthedistancematrix\noutputs for any given input coordinates, aiming for these andD ij isthedistancebetweencityiandcityj. Toaddress\npredictionstoapproximatetheoptimalsolutions(Lietal., theHamiltonianCycleconstraint,theauthorsintroducethe\n2024; Sun & Yang, 2024; Fu et al., 2021). For example, T → H transformation, which is designed to implicitly\n2\nOnSizeandHardnessGeneralizationinUnsupervisedLearningfortheTravellingSalesmanProblem\nencodethisconstraint.\n3.1.UnderstandingT→Htransformation\nT→Htransformationisdefinedas:\nH=TVTT, (1)\nwhere\n \n0 1 0 0 ··· 0 0 0\n0 0 1 0 ··· 0 0 0\n \n0 0 0 1 ··· 0 0 0\nV= \n\n. .\n.\n. .\n.\n. .\n.\n... ... . .\n.\n. .\n.\n. . . \n\n\n\n0 0 0 0\n...\n1 0\n0\n\nFigure1.IllustrationofTandthecorrespondingH.p 1[1]=p 2[3]\n  =p 3[2]=p 4[5]=p 5[4]=1.Thismeansthereisacorresponding\n 0 0 0 0 ··· 0 1 0\n\nHamiltonianCycle:1→3→2→5→4→1.\n0 0 0 0 ··· 0 0 1\n1 0 0 0 ··· 0 0 0\nHere,theRow-wiseconstraintencouragesTtobehavelike\nistheshiftmatrix,whereV ∈ Rn×n. WecaninterpretV\nadoublystochasticmatrix,thusservingasasoftrelaxation\nasrepresentingaHamiltoniancyclethatfollowsthepath\nof a permutation matrix (Min & Gomes, 2023). The No\n1 → 2 → 3 → ··· → n → 1, while T serves as an ap-\nself-loopstermdiscouragesselfloopsinH,whereλ isthe\n2\nproximationofageneralpermutationmatrix. Giventhatour\ndistanceofself-loop,theMinimizetheDistancetermactsas\ninitialheatmapVrepresentsaHamiltoniancycle,andcon-\naproxyforminimizingthedistanceofaHamiltonianCycle.\nsideringthatboththeHamiltoniancycleconstraintholdsand\nthenodeorderingisequivariantunderpermutationopera- Although UTSP offers a promising unsupervised way to\ntions,theHamiltoniancycleconstraintisimplicitlyencoded learn the heat maps, a notable limitation of the model is\ninthisframework. Formoredetails,wereferthereaderto its lack of generalization. Specifically, a model trained\n(Min&Gomes,2023). onTSPinstanceswithncitiescannotbeappliedtoother\ninstances,suchasinstanceswithn+1orn−1cities. This\nWecanalsowriteT→Htransformationas:\nlimitationarisesduetoThavingafixeddimensionofRn×n.\nn−1 Consequently,themodel’sarchitectureisinherentlytiedto\n(cid:88)\nH= p tpT t+1+p npT 1, (2) thesizeofthetraininginstances,restrictingitsadaptability\nt=1 toTSPinstancesofvaryingcitycounts.\nwherep ∈Rn×1isthet columnofT,T=[p |p |...|p ].\nt th 1 2 n\nEquation2providesanotherwayofunderstandingtheT→\n4.SizeGeneralization\nH transformation. The elements in H are defined using\ntwo nearest columns in T. As shown in Figure 1, p = Recall the understanding of T → H transformation in\n1\n[1,0,0,0,0]T andp =[0,0,1,0,0]T. Sincethenon-zero Equation 2. We can interpret that the GNN generates a\n2\nelement in p is located at the first position and the non- n-dimensionalembeddingforeachcity. Inourgeneralized\n1\nzero element in p is at the third position, it indicates a model, givenTSPinstanceswithdifferentsizes, foreach\n2\ndirected edge from node 1 to node 3 in the heat map H. nodeintheseinstances,theGNNoutputsanembeddingof\nThisisdepictedasthepurpleedgeinFigure1. Similarly, dimensionm. Followingthis,aSoftmaxactivationfunction\nthepresenceofanon-zeroelementatthesecondpositionin isappliedtoeachcolumnoftheembeddingmatrix,resulting\np impliesthatthereisadirectededgefromnode3tonode\ninthegenerationofT∈Rn×m.\n3\n2intheheatmapH,representedbytheyellowedge.\nWethenbuildHusing1:\n3.2.TrainingUTSP m−1\n(cid:88)\nH= p pT +p pT, (4)\nt t+1 m 1\nInUTSP,theauthortrainthemodelusingthefollowingloss\nt=1\nLis:\nwherep ∈Rn×1isthet columnofT. Equation4canbe\nn n n n n t th\nλ\n(cid:88) ((cid:88)\nT −1)2+λ\n(cid:88)\nH\n+(cid:88)(cid:88)\nD H .\nreformulatedanalogouslytoEquation1withV∈Rm×m.\n1 i,j 2 i,i i,j i,j\ni=1 j=1 i=1 i=1j=1 1Itisimportanttoobservethatwhenm ̸= n,Hisnotdou-\n(cid:124) Row-wis(cid:123) e(cid:122) constraint (cid:125) N(cid:124) ose(cid:123) lf(cid:122) -loo(cid:125) ps (cid:124) Minimize(cid:123) th(cid:122) edistance(cid:125) blystochastic. WealsotriedeitherreplacingTwith(cid:112) mnTor\n(3) substitutingHwith nH,bothofwhichyieldsimilaroutcomes.\nm\n3\nOnSizeandHardnessGeneralizationinUnsupervisedLearningfortheTravellingSalesmanProblem\nFigure2. Traininghistoryofdifferentm(embeddingdimension)onTSP-2000.\nFigure3. Traininghistoryofdifferentn(instancesize)withsameembeddingdimensionm=1500.\nInpractice,wetrainourmodelunderthelossL: TheTSP-2000,1000,and400trainingdatasetsarecreated\nbyrandomlydistributingpointsona2Dplane,subjecttoa\nn n n n\nλ\n(cid:8)(cid:88) (1−(cid:88)\nH\n)2+(cid:88) (1−(cid:88)\nH\n)2(cid:9) uniformdistribution. ForTSP-200andTSP-400,wetrain\n1 i,j i,j themodelfor300epochs,whileforTSP-1000,wetrainthe\nj=1 i=1 i=1 j=1\nmodelfor600epochs. Eachofthesedatasetsconsistsof\n(cid:124) (cid:123)(cid:122) (cid:125)\nRowandcolumn-wiseconstraint (5) 5,000traininginstances.\nn n\n(cid:88)(cid:88)\n+ D H . We train our model on one NVIDIA A100 Graphics Pro-\ni,j i,j\ni=1j=1 cessingUnit,usingthesameGraphNeuralNetwork(GNN)\n(cid:124) (cid:123)(cid:122) (cid:125) architectureasdescribedin(Minetal.,2024). Themodel\nMinimizethedistance\nistrainedonTSPinstancesofsizes400,1000,and2000,\nBylettingtheGNNtooutputanm-dimensionalembedding usingaconfigurationoftwohiddenlayers,witheachlayer\nforeachcity,themodelachievesgeneralizationacrossdif- comprising128hiddenunits. Thehyperparameterλ , as\n1\nferentinstances. Thismeansthat,throughEquation2,the specified in Equation 4, is set to 100. Our test instances\nheat map H will consistently match the size of the input aretakenfrom(Fuetal.,2021). Here,theperformancegap\ncities(n×n). iscalculatedusingthe l−lopt,wherel representstheTSP\nlopt\nlengthgeneratedbyourmodelandl denotestheoptimal\nopt\n5.Experiment length. We run the search algorithm on Intel Xeon Gold\n6326.\nHere,weexploretheimpactofthegeneralizedmodelon\ndifferent problem sizes. Specifically, we study TSP with Inourapproach,consistentwiththeexistingUTSPframe-\n200,500,and1000cities,eachsizeisevaluatedusing128 work,weemploythesamesearchmethodology.Theprocess\ntestinstances. beginswiththegenerationoftheheatmapH,fromwhich\nweextractthetopM largestvaluesineachrow. Thisextrac-\nDifferentfrompreviouslyUTSPsetting,ournewmethodol- tionleadstotheformationofanewheatmap,denotedasH˜.\nogyinvolvestrainingmodelsonlargerdatasetsandtesting WecomputeH′ =H˜+H˜T tosymmetrizethisupdatedheat\nthem on smaller ones. Specifically, we train a model on\nmap.H′isthenusedtoguidethesearchprocess.Wefurther\naTSP-2000datasetwithm = 1500andtestitonaTSP-\ncalculatetheoverlapbetweennon-zeroedgesinH′andthe\n1000dataset; anothermodelistrainedonTSP-1000with\noptimal solutions, where a higher overlap ratio indicates\nm = 800 and tested on TSP-500; and finally, a model\nthatH′ moreeffectivelycoverstheoptimalsolution. For\ntrained on TSP-400 with m = 320 is tested on TSP-200.\n4\nOnSizeandHardnessGeneralizationinUnsupervisedLearningfortheTravellingSalesmanProblem\nTable1.Resultsofourgeneralizablemodel+LocalSearchw.r.t.existingbaselines,testedon128instanceswithn=200,500and1000.\nTSP200 TSP500 TSP1000\nMethod Type\nLength Gap(%) Time Length Gap(%) Time Length Gap(%) Time\nConcorde Solver 10.7191 0.0000 3.44m 16.5458 0.0000 37.66m 23.1182 0.0000 6.65h\nGurobi Solver 10.7036 -0.1446 40.49m 16.5171 -0.1733 45.63h - - -\nLKH3 Heuristic 10.7195 0.0040 2.01m 16.5463 0.0029 11.41m 23.1190 0.0036 38.09m\nGAT(Deudonetal.,2018) RL,S 13.1746 22.9079 4.84m 28.6291 73.0293 20.18m 50.3018 117.5860 37.07m\nGAT(Kooletal.,2019) RL,BS 11.3769 6.1364 5.77m 19.5283 18.0257 21.99m 29.9048 29.2359 1.64h\nGCN(Joshietal.,2019) SL,G 17.0141 58.7272 59.11s 29.7173 79.6063 6.67m 48.6151 110.2900 28.52m\nSL+RL 20.62s+ 31.17s+ 43.94s+\nAtt-GCRN(Fuetal.,2021) 10.7358 0.1563 16.7471 1.2169 23.5153 1.7179\nMCTS 1.33m 3.33m 6.68m\n4.83s+ 7.28s+ 0.23m+\nUTSP(Minetal.,2024) UL,Search 10.7289 0.0918 16.6846 0.8394 23.3903 1.1770\n1.11m 1.54m 3.51m\n4.94s+ 5.66s+ 0.24m+\nOurModel UL,Search 10.7251 0.0558 16.6820 0.8229 23.3867 1.1616\n1.11m 1.54m 3.51m\nmoredetailedinformation,wereferto (Minetal.,2024). shown in Figure 2. We calculate the overlap ratios and\nsearchperformanceusingmodelswithdifferentembedding\nOurresultsareshowninTable1,inthecaseofTSP-200,our\ndimensions,theresultsareshowninTable2,3. Ourfind-\nmodelachievesagapof0.0558%,whentacklingTSP-500,\ningsindicatethatanincreaseintheembeddingdimension\nthemodelcontinuestodemonstrateitsrobustness,witha\ncontributes to higher overlap ratios and enhanced search\ngap of 0.8229%. The performance in both TSP-200 and\nperformance. Forinstance,theoverlapratioimprovesfrom\nTSP-500suggeststhatourmodel’sapproachtoguidingthe\n82.70% to 94.93% when the embedding dimension m is\nlocalsearchiseffectiveacrossvariousscalesoftheTSP.\nincreasedfrom500to1500,basedontheheatmapswith\nWhen the model is applied to the largest tested instance top5elementsfromeachrow. Correspondingly,thesearch\nsize,TSP-1000,itachievesagapof1.1616%. whichisthe performancealsoimproves,withthegapdecreasingfrom\nminimumoneamongallthemethods. Moreimportantly,it 2.0746%to1.4145%. Thishighlightsthesignificanceof\nunderscoresthemodel’sgeneralizationtoscaleandmaintain embeddingdimensioninincreasingmodelefficacy.Alarger\nalevelofefficiencyinlarge-scaleTSPinstances.Ourresults embeddingdimensioncanbetteridentifyoptimalornear-\nacrossallthreeinstancesizesillustratethatthemodeltrained optimalsolutionsandnarrowthegap.\nusingEquation5isabletogeneralizeacrossinstancesof\ndifferentsizesandeffectivelyenhancesthesearchprocess.\nTable3.Overlapratiosandthesearchresultson128TSP-1000\n5.1.ImpactofVaryingmonTrainingPerformance instancesin(Fuetal.,2021)usingdifferentembeddingdimension\nm.Weselecttop20elementsfromeachrowintheheatmaps.\nAsmentionedinEquation4,mrepresentstheembeddingdi-\nmensionofeachnode.Inthisstudy,weinvestigatetheeffect m OVERLAPRATIO(%) PERFORMANCEGAP(%)\noftheembeddingdimensionmonthemodel’sperformance.\n500 99.99 1.1995±0.1849\nSpecifically,wetrainmodelsonTSP-2000instanceswith\n1000 100.00 1.1608±0.1844\nvaryingembeddingdimensions: m=500,1000,and1500. 1500 100.00 1.1616±0.1743\nWethenevaluatethesemodelsonTSP-1000instancesto\nassesstheirperformance.\nSpecifically,itisnoteworthythatwhenselectingthetop20\nelements from each row, both m = 1000 and m = 1500\nTable2.Overlapratiosandthesearchresultson128TSP-1000\nachievea100.00%overlapratio, whereasm = 500does\ninstancesin(Fuetal.,2021)usingdifferentembeddingdimension\nnotcoveralltheoptimalsolutions,resultinginalargergap.\nm.Weselecttop5elementsfromeachrowintheheatmaps.\nFurthermore,weobservethatm=1000exhibitsmarginally\nbetterperformancecomparedtom=1500. Thissuggests\nm OVERLAPRATIO(%) PERFORMANCEGAP(%)\nthatbeyondacertainthreshold,increasingtheembedding\n500 82.70 2.0746±0.5457\ndimensionyieldsdiminishingreturnsintermsofcovering\n1000 93.75 1.4832±0.2305\n1500 94.93 1.4145±0.2005 optimal solutions. It also implies that there might be an\noptimalrangefortheembeddingdimension, indicatinga\nneedforcarefulconsiderationinthechoiceofmtooptimize\nThetrainingcurvesfordifferentembeddingdimensionsare modelperformance.\n5\nOnSizeandHardnessGeneralizationinUnsupervisedLearningfortheTravellingSalesmanProblem\nFigure4. Expansion Figure5. Explosion Figure6. Implosion Figure7. Uniform\n5.2.ImpactofVaryingnonTrainingPerformance highlighttheimportanceofselectinglargertraininginstance\nsizestoenhancemodelperformanceandefficiency.\nOurmodelcangeneralizeacrossdifferentsizes,meaning\nthat training on one size can effectively translate to per-\n6.HardnessGeneralization\nformanceonanother,previouslyunseensize. Herewein-\nvestigatehowvaryingthetrainingsizeimpactsthemodel’s\nPreviousstudiessuggestthatULcangeneralizeacrossdif-\nperformance.WetrainthemodelusingTSP-400,TSP-1000,\nferentsizes,guidethesearchandreducethesearchspace,\nandTSP-2000instances,allwiththesameembeddingdi-\nHere,wedelveintohowUL’scapabilitytoreducethesearch\nmensionm = 1500. Thetrainingresultsareillustratedin\nspaceisinfluencedbydifferentdistributions. Specifically,\nFigure3.\nweexploretherelationshipbetweendifferentdistributions\nWethentesthowdifferenttraininginstances’sizescanaf- andtheefficiencyofusingULforsolvingtheTSP.\nfecttheoverlapratioandtheperformance. Theresultsare\nHowever,buildingaconnectionbetweenvariousdistribu-\nshowninTable4,5. Wenotethattrainingwithlargerin-\ntionsandtheefficacyofULinreducingthesearchspace\nstancesenhancessearchperformanceunderbothtop5and\npresentssignificantchallenges. Toaddressthis,wefirstfo-\ntop20conditions. Specifically,whenselectingthetop5ele-\ncusoncorrelatingdifferentdistributionswiththeirhardness\nmentsfromeachrow,theperformancegapimprovesfrom\nlevels.\n3.0762%to1.4145%. Similarly,whenchoosingthetop20\nelementsfromeachrow,thegapshowsamarkedimprove-\nment,decreasingfrom1.1885%to1.1616%. Ourresults Phase transition A phase transition refers to a change\nin the solvability of NP-hard problems. When some pa-\nrametersoftheproblemisvaried,forexample,thedensity\nTable4.Overlapratiosandthesearchresultson128TSP-1000 of constraints in a Boolean satisfiability problem (SAT)\ninstances instances in (Fu et al., 2021) with m = 1500 using problem(Mitchelletal.,1992), theproblemundergoesa\ntraininginstanceswithdifferentsizes. Weselecttop5elements transition from being almost solvable to unsolvable. To\nfromeachrowintheheatmaps.Thefirstcolumndenotesdifferent\nbespecific, ThephasetransitioninSATreferstoasharp\ntrainingsizes.\nchangeinthesolvabilityoftheseproblems,dependingon\ntheratioofthenumberofclausestothenumberofvariables\nn OVERLAPRATIO(%) PERFORMANCEGAP(%)\nintheformula.Whentheratioislow(fewclausesrelativeto\n400 68.83 3.0762±1.3141 variables),mostinstancesoftheproblemareeasytosolve.\n1000 93.48 1.5563±0.2345 Thisisbecausetherearefewerconstraints,makingitmore\n2000 94.93 1.4145±0.2005\nlikelytofindasatisfyingassignment. Conversely,whenthis\nratioishigh(manyclausesrelativetovariables),theprob-\nlembecomesover-constrained,andmostinstancesarealso\neasytosolvebecausetheyarealmostcertainlyunsatisfiable.\nTable5.Overlapratiosandthesearchresultson128TSP-1000 Themostinterestingpartoccursatacertaincriticalratio,\ninstancesin(Fuetal.,2021)withm = 1500usingtrainingin-\ntypicallyaround4.3for3-SATproblems. Atthisratio,the\nstanceswithdifferentsizes.Weselecttop20elementsfromeach\nproblemsundergoaphasetransitionandbecomeextremely\nrowintheheatmaps.Thefirstcolumndenotesdifferenttraining\nhardtosolve.Inotherwords,theproblemsaremostdifficult\nsizes.\naroundthephasetransitionpoint (Monassonetal.,1999).\nn OVERLAPRATIO(%) PERFORMANCEGAP(%) Phasetransitionsprovidesapowerfulframeworktostudy\n400 99.96 1.1885±0.1927 the properties of NP-hard problems. However, the exact\n1000 100.00 1.1763±0.1743 natureandlocationofthesetransitionscanbedifficultto\n2000 100.00 1.1616±0.1743 determine andmay depend intricatelyon the structureof\n6\nOnSizeandHardnessGeneralizationinUnsupervisedLearningfortheTravellingSalesmanProblem\nthedifferentproblems. ForTSP,(Gent&Walsh,1996)sug- concept,weexaminehowthesedistributionsinfluencethe\n√\ngestusingtheparameterτ =l / nA,whereAdenotes capacityofULtoefficientlyreducethesearchspaceand\nopt\nthe area covered by the TSP instance, l represents the guidethesearch.\nopt\nlengthoftheoptimalsolution,andnisthenumberofcities.\nThis approach is based on the observation that there is a\nrapid transition in solvability around a fixed value of the\nparameter,specificallyatapproximatelyT =0.78.\nc\nFigure9.ThetrainingcurvesforTSP-400withm=320across\nfourdifferentdistributionsareshown;themodelisthentestedon\nFigure8.TSPphasetransitionandtheτ valuesfordifferentdistri- 128TSP-200instances.\nbutions.\nHere we study four different distributions and see how it\ncaneffectthesearchspacereduction,anillustrationofthese\nfourdistributionisshowninFigure4∼7. Asmentioned\nearlier,aroundthephasetransitionpoint,theproblemsof-\ntenexhibitsthegreatestcomputationalcomplexity(Hard).\nFigure 8 illustrates the scheme of phase transition in the\nTSP.Thex-axisistheτ value,whilethey-axiscorresponds\nto the level of hardness. The point at which τ equals the\ncritical threshold T = 0.78 marks the peak of difficulty,\nc\nexhibiting the highest hardness, we refer more details to\n(Gent&Walsh,1996).\nFurthermore,wepresenttheτ valuesforfourdifferentdis-\ntributions,whereeachτ iscomputedasanaveragefrom100\ninstances,eachwithasizeof200,500and1000,detailedin\nTable6andFigure8. Figure10.ThetrainingcurvesforTSP-1000withm=800across\nfourdifferentdistributionsareshown;themodelisthentestedon\n128TSP-500instances.\n√\nTable6.τ =l / nAofExpansion,Explosion,Implosion,and\nopt\nUniformfordifferentsizes. Wefirsttrainthemodelsusing4differentdistributionswith\nthesameparametersinSection4. Wecalculatetheoverlap\nSIZE EXPANSION EXPLOSION IMPLOSION UNIFORM\nratioofthesemodelsforTSP-200,500,and1000. Thetrain-\n1000 0.4838 0.5629 0.7237 0.7460 ingresultsareshowninFigure9,10and11.Weobservethat\n500 0.5114 0.5905 0.7338 0.7515\nmodelstrainedwithharderinstancesconsistentlyexhibita\n200 0.5796 0.6337 0.7539 0.7745\nlowerloss. Specifically,thelosscurvesformodelstrained\nusingtheUniformdistributionconsistentlyshowthelowest\nAsshowninFigure8,theUniformdistributionisclosestto loss,whilethosetrainedwithExpansionandExplosiondis-\nthephasetransitionpointT . Thisindicatesahighestlevel tributionsdemonstratehigherlosses. Thissuggeststhatthe\nc\nofhardness. Consequently,intermsoftransitioningfrom hardnessleveloftraininginstancesplaysasignificantrole\nhardtoeasy,theorderisobservedasfollows: Uniform≈ intheeffectivenessofthemodeltraining,directlyimpacting\nImplosion>Explosion>Expansion. Followinguponthis thelossmetrics. Itisimportanttonotethatthroughoutour\n7\nOnSizeandHardnessGeneralizationinUnsupervisedLearningfortheTravellingSalesmanProblem\nFigure11.ThetrainingcurvesonTSP-2000withm=1500acrossfourdifferentdistributionsareshown;themodelisthentestedon128\nTSP-1000instances.\nTable7.Overlap ratios and the search results on 128 TSP-200 Table9.Overlapratiosandthesearchresultson128TSP-1000\ninstances in (Fu et al., 2021) using different distributions. We instances in (Fu et al., 2021) using different distributions. We\nselecttop5elementsfromeachrowintheheatmaps. selecttop5elementsfromeachrowintheheatmaps.\nDATASET OVERLAPRATIO(%) PERFORMANCEGAP(%) DATASET OVERLAPRATIO(%) PERFORMANCEGAP(%)\nUNIFORM 95.64 0.0883±0.0885 UNIFORM 94.93 1.4145±0.2005\nIMPLOSION 95.50 0.0876±0.0920 IMPLOSION 94.71 1.4060±0.2078\nEXPLOSION 95.29 0.0979±0.0907 EXPLOSION 93.86 1.5274±0.2632\nEXPANSION 94.00 0.1131±0.0973 EXPANSION 93.38 1.5777±0.2735\nTable8.Overlap ratios and the search results on 128 TSP-500 Table10.Overlapratiosandthesearchresultson128TSP-1000\ninstances in (Fu et al., 2021) using different distributions. We instances in (Fu et al., 2021) using different distributions. We\nselecttop5elementsfromeachrowintheheatmaps. selecttop20elementsfromeachrowintheheatmaps.\nDATASET OVERLAPRATIO(%) PERFORMANCEGAP(%) DATASET OVERLAPRATIO(%) PERFORMANCEGAP(%)\nUNIFORM 95.47 0.9311±0.1638 UNIFORM 100.00 1.1616±0.1743\nIMPLOSION 95.40 0.9394±0.1732 IMPLOSION 100.00 1.1844±0.1572\nEXPLOSION 94.99 0.9410±0.1764 EXPLOSION 100.00 1.1937±0.1764\nEXPANSION 94.03 1.0137±0.1800 EXPANSION 100.00 1.1797±0.2025\nlow overlap ratios and larger performance gaps. This in-\ntrainingprocess,allotherhyperparametersettingsremained\ndicates that models trained on simpler distributions may\nconstant. Therefore,theobservedvariationsinlosscanbe\nstruggle to generalize effectively to more challenging in-\nattributedsolelytothedifferencesintrainingdistributions.\nstancesoftheproblem. Theloweroverlapratiossuggest\nWethenevaluatehowdifferentdistributionscanaffectthe thatthesolutionsgeneratedbythesemodelsarelessaligned\nsearch results. We pick the top 5 element each row and withtheoptimalsolutions,andthelargerperformancegaps\nbuildtheheatmaps. Theoverlapratioandthesearchresults highlightasignificantdisparityineffectivenesswhenthese\nare shown in Table 7, 8 and 9. When training on easier modelsareappliedtothetestTSPinstances. Trainingon\ndistributionssuchasExplosionandExpansion,weobserve harderdistributions,suchasUniform,yieldshigheroverlap\n8\nOnSizeandHardnessGeneralizationinUnsupervisedLearningfortheTravellingSalesmanProblem\nratiosandimprovedsearchperformance. Thisindicatesthat Futuresprogram;theNationalScienceFoundation(NSF)\nmodels trained on harder distributions can build a better andtheNationalInstituteofFoodandAgriculture(NIFA);\nrepresentationofthesearchspace,whichenablesthesearch theAirForceOfficeofScientificResearch(AFOSR);the\nto perform more effectively. It is also observed that the DepartmentofEnergy; andtheToyotaResearchInstitute\nplateausduringtrainingaremorepronouncedwhentrain- (TRI).\ning on harder instances, suggesting that the optimization\nlandscapebecomesmorecomplexwhenthehardnesslevel\nReferences\nincreases.\nBello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio,\nWe evaluate the model’s performance on TSP-1000 in-\nS. Neuralcombinatorialoptimizationwithreinforcement\nstancesbyutilizingthetop20elementsfromeachrowfor\nlearning. arXivpreprintarXiv:1611.09940,2016.\neachdistribution,asdetailedinTable10.Weobservethatby\nselectingthetop20elements,H′isabletocover100.00% Bengio,Y.,Lodi,A.,andProuvost,A. Machinelearning\nof the optimal solutions. Overall, the performance gaps for combinatorial optimization: a methodological tour\nacrossthedistributionsaresimilar,withtrainingonuniform d’horizon. EuropeanJournalofOperationalResearch,\ndistributionscontinuingtoexhibitthelowestperformance 290(2):405–421,2021.\ngap.\nBi,J.,Ma,Y.,Wang,J.,Cao,Z.,Chen,J.,Sun,Y.,andChee,\nY.M. Learninggeneralizablemodelsforvehiclerouting\n7.Conclusion\nproblemsviaknowledgedistillation. AdvancesinNeural\nInformationProcessingSystems,35:31226–31238,2022.\nThis work introduces a new methodology that allows a\ntrained,unsupervisedTSPmodeltogeneralizeacrossdif-\nChen,J.,Wang,J.,Zhang,Z.,Cao,Z.,Ye,T.,andChen,S.\nferentproblemsizes. Ourresultsdemonstratethattraining\nEfficientmetaneuralheuristicformulti-objectivecom-\nonlargerprobleminstancescanimproveperformancecom-\nbinatorialoptimization. AdvancesinNeuralInformation\nparedtotrainingwithsmallerinstances. Additionally,we\nProcessingSystems,36,2024.\ndelveintotheinfluenceofembeddingdimensionsonTSP\nresults,showingthatlargerembeddingdimensionsareim- Deudon,M.,Cournut,P.,Lacoste,A.,Adulyasak,Y.,and\nportantinconstructingmoreeffectiverepresentationsthat Rousseau,L.-M. Learningheuristicsforthetspbypolicy\nguide the search process more efficiently. Moreover, we gradient. InInternationalConferenceontheIntegration\ninvestigatethemodel’sperformanceusingtrainingdatasets ofConstraintProgramming,ArtificialIntelligence,and\nwithdifferentlevelsofhardnesses.Weshowthattrainingon OperationsResearch,pp.170–181.Springer,2018.\nharderinstancescanimprovemodelperformance,empha-\nFu,Z.-H.,Qiu,K.-B.,andZha,H. Generalizeasmallpre-\nsizingtheimportanceofselectingtraininginstanceswith\ntrainedmodeltoarbitrarilylargetspinstances. Proceed-\nappropriate difficulty levels. We train our models on dif-\ningsoftheAAAIConferenceonArtificialIntelligence,35\nferentTSPdistributionstounderstandtheirimpactonthe\n(8):7474–7482,2021.\neffectiveness of UL models. Our study indicates a clear\nrelationshipbetweentheinherenthardnessofdistribution Gent,I.P.andWalsh,T. Thetspphasetransition. Artificial\nandthemodel’scapacitytogeneralizeandeffectivelysolve Intelligence,88(1-2):349–358,1996.\nTSPinstances. Toourknowledge,thisisthefirststudyto\nsystematicallyinvestigateanddemonstratethisconnection. Helsgaun, K. An effective implementation of the lin–\nkernighantravelingsalesmanheuristic. Europeanjournal\nOurresultshighlighttherelationshipbetweenthecharacter-\nofoperationalresearch,126(1):106–130,2000.\nisticsoftraininginstances(sizeandhardness),embedding\ndimensions, and model performance in UL, particularly Joshi, C. K., Laurent, T., and Bresson, X. An efficient\nwhenaddressingCOproblemssuchastheTSP.Weantici- graphconvolutionalnetworktechniqueforthetravelling\npatethatthesefindings—emphasizingthebenefitsoftrain- salesman problem. arXiv preprint arXiv:1906.01227,\ningonlarger,harderinstanceswithincreasedembedding 2019.\ndimensions—caninspirefurtherresearchintheapplication\nKool,W.,vanHoof,H.,andWelling,M. Attention,learn\nofUnsupervisedLearningtoCombinatorialOptimization\ntosolveroutingproblems! InInternationalConference\ntasks.\non Learning Representations, 2019. URL https://\nopenreview.net/forum?id=ByxBFsRqYm.\n8.Acknowledgement\nLi, Y., Guo, J., Wang, R., and Yan, J. From distribution\nThisprojectispartiallysupportedbytheEricandWendy\nlearningintrainingtogradientsearchintestingforcom-\nSchmidtAIinSciencePostdoctoralFellowship,aSchmidt\nbinatorialoptimization. volume36,2024.\n9\nOnSizeandHardnessGeneralizationinUnsupervisedLearningfortheTravellingSalesmanProblem\nMa,Y.,Cao,Z.,andChee,Y.M.Learningtosearchfeasible\nandinfeasibleregionsofroutingproblemswithflexible\nneuralk-opt. AdvancesinNeuralInformationProcessing\nSystems,36,2024.\nMin, Y. and Gomes, C. Unsupervised learning permuta-\ntionsfortspusinggumbel-sinkhornoperator. InNeurIPS\n2023WorkshopOptimalTransportandMachineLearn-\ning,2023.\nMin,Y.,Bai,Y.,andGomes,C.P. Unsupervisedlearning\nforsolvingthetravellingsalesmanproblem. Advancesin\nNeuralInformationProcessingSystems,36,2024.\nMitchell,D.,Selman,B.,Levesque,H.,etal. Hardandeasy\ndistributions of sat problems. In Aaai, volume 92, pp.\n459–465,1992.\nMonasson, R., Zecchina, R., Kirkpatrick, S., Selman, B.,\nandTroyansky,L. Determiningcomputationalcomplex-\nityfromcharacteristic‘phasetransitions’. Nature,400\n(6740):133–137,1999.\nSun,Z.andYang,Y.Difusco:Graph-baseddiffusionsolvers\nforcombinatorialoptimization. AdvancesinNeuralIn-\nformationProcessingSystems,36,2024.\nXin,L.,Song,W.,Cao,Z.,andZhang,J. Neurolkh: Com-\nbiningdeeplearningmodelwithlin-kernighan-helsgaun\nheuristicforsolvingthetravelingsalesmanproblem. Ad-\nvances in Neural Information Processing Systems, 34:\n7472–7483,2021.\nYe,H.,Wang,J.,Cao,Z.,Liang,H.,andLi,Y. Deepaco:\nNeural-enhancedantsystemsforcombinatorialoptimiza-\ntion. volume36,2024.\nZhou,J.,Wu,Y.,Song,W.,Cao,Z.,andZhang,J. Towards\nomni-generalizable neural methods for vehicle routing\nproblems.InInternationalConferenceonMachineLearn-\ning,pp.42769–42789.PMLR,2023.\n10",
    "pdf_filename": "On_Size_and_Hardness_Generalization_in_Unsupervised_Learning_for_the_Travelling_Salesman_Problem.pdf"
}