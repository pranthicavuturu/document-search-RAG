{
    "title": "On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem",
    "abstract": "We study the generalization capability of Unsuper- vised Learning in solving the Travelling Salesman Problem (TSP). We use a Graph Neural Network (GNN) trained with a surrogate loss function to generate an embedding for each node. We use these embeddings to construct a heat map that indicates the likelihood of each edge being part of the optimal route. We then apply local search to generate our final predictions. Our investigation explores how different training instance sizes, em- bedding dimensions, and distributions influence the outcomes of Unsupervised Learning methods. Our results show that training with larger instance sizes and increasing embedding dimensions can build a more effective representation, enhancing the model’s ability to solve TSP. Furthermore, in evaluating generalization across different distribu- tions, we first determine the hardness of various distributions and explore how different hardnesses affect the final results. Our findings suggest that models trained on harder instances exhibit bet- ter generalization capabilities, highlighting the importance of selecting appropriate training in- stances in solving TSP using Unsupervised Learn- ing.",
    "body": "On Size and Hardness Generalization in Unsupervised Learning for the\nTravelling Salesman Problem\nYimemg Min 1 Carla P. Gomes 1\nAbstract\nWe study the generalization capability of Unsuper-\nvised Learning in solving the Travelling Salesman\nProblem (TSP). We use a Graph Neural Network\n(GNN) trained with a surrogate loss function to\ngenerate an embedding for each node. We use\nthese embeddings to construct a heat map that\nindicates the likelihood of each edge being part of\nthe optimal route. We then apply local search to\ngenerate our final predictions. Our investigation\nexplores how different training instance sizes, em-\nbedding dimensions, and distributions influence\nthe outcomes of Unsupervised Learning methods.\nOur results show that training with larger instance\nsizes and increasing embedding dimensions can\nbuild a more effective representation, enhancing\nthe model’s ability to solve TSP. Furthermore, in\nevaluating generalization across different distribu-\ntions, we first determine the hardness of various\ndistributions and explore how different hardnesses\naffect the final results. Our findings suggest that\nmodels trained on harder instances exhibit bet-\nter generalization capabilities, highlighting the\nimportance of selecting appropriate training in-\nstances in solving TSP using Unsupervised Learn-\ning.\n1. Introduction\nThe goal of machine learning for Combinatorial Optimiza-\ntion (CO) is to enhance or surpass handcrafted heuristics.\nRecently, there has been an increasing trend in applying\nMachine Learning (ML) to tackle CO problems (Bengio\net al., 2021). Different from manually crafted heuristics,\nmachine learning approaches harness the power of data to\nuncover patterns in CO problems.\nThe Euclidean Travelling Salesman Problem (TSP) is one of\n1Department of Computer Science, Cornell University,\nIthaca\n14850,\nUSA.\nCorrespondence\nto:\nYimeng\nMin\n<min@cs.cornell.edu>.\nPreprint\nthe most famous and intensively studied CO problems. TSP\nasks the following question: Given a list of cities and the\ndistances between each pair of cities, what is the shortest\npossible route that visits each city exactly once and returns\nto the origin city? A variety of methods have been devel-\noped to solve TSP, including the Lin-Kernighan-Helsgaun\n(LKH) heuristics, which is known for their effectiveness\nin approximating solutions (Helsgaun, 2000), and the Con-\ncorde solver, which guarantees optimality of the solutions.\nThe application of ML for TSP has primarily focused on Su-\npervised Learning (SL) and Reinforcement Learning (RL).\nHowever, SL methods encounter the challenge of expensive\nannotations, while RL methods struggle with sparse reward\nproblems.\nRecently, (Min et al., 2024) proposes a new approach named\nUTSP that employs Unsupervised Learning (UL) to build\na data-driven heuristics for the TSP. This unsupervised\nmethod does not depend on any labelled dataset and gener-\nates a heatmap in a non-autoregressive manner, offering a\ndistinct alternative to traditional SL and RL models.\nWhile the UL heuristics offer a promising approach, the\nchallenge of generalizing across varying sizes and distribu-\ntions remains significant. In particular, the model presented\nin (Min et al., 2024) requires retraining to adapt to new\nsizes, indicating that a model trained on one size cannot\neffectively generalize to different sizes.\nThis paper explores the generalization capabilities of unsu-\npervised heuristics for the TSP. Our findings indicate that\nthe UL model is able to generalize across different problem\nsizes. Regarding the generalization behavior of different dis-\ntributions, based on the hardness results by (Gent & Walsh,\n1996), we relate different distributions to distinct levels of\nhardnesses. This allows us to investigate the impact of the\ntraining data’s hardness on the model’s performance.\nOur primary contributions are outlined as follows: We pro-\npose a novel approach for enabling a TSP model, once\ntrained, to generalize effectively across different problem\nsizes. We show that training with larger problem sizes can\nenhance model performance. Furthermore, we investigate\nthe impact of various embedding dimensions on TSP per-\nformance, finding that larger embedding dimensions can\n1\narXiv:2403.20212v2  [cs.AI]  19 Nov 2024\n\nOn Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem\nbuild more effective representations to guide the search pro-\ncess. Additionally, we explore how the model performs\nwhen trained on datasets of varying distributions. Our find-\nings indicate that models trained on harder instances exhibit\nbetter performance, which underscores the importance of\ntraining instances’ distribution within the framework of UL\nfor solving CO problems like the TSP.\nWhile recent research papers explored using data-driven\ntechniques for CO problems, most have focused on SL or\nRL. Very few have examined the generalization behaviours,\nparticularly how training data (different distributions of TSP\ninstances) influences final model performance (Bi et al.,\n2022). Our work addresses this gap, offering insights into\nthe significance of training data selection and its direct im-\npact on the effectiveness of ML models for CO tasks. This\nexploration contributes to understanding ML models in CO\nand provides practical guidelines for improving model gen-\neralization and performance in solving TSP.\n2. Related works\n2.1. RL for TSP\nThe goal of using RL for CO is to train an agent capable\nof either maximizing or minimizing the expected sum of\nfuture rewards, known as the return. For a given policy,\nthe expected return from a current state is defined as the\nvalue function. In the context of TSP, RL typically focuses\non minimizing the length of the predicted route (Ye et al.,\n2024; Zhou et al., 2023; Chen et al., 2024; Ma et al., 2024).\nFor example, (Kool et al., 2019) proposes a model based on\nattention layers and trains the model using RL using a deter-\nministic greedy rollout. (Bello et al., 2016) trains a recurrent\nneural network to predict permutations of city coordinates\nand optimizes the parameters with a policy gradient method\nusing the negative tour length as a reward signal.\nHowever, as the size of the TSP increases, the rewards\nbecome increasingly sparse, necessitating long exploration\nsteps before the agent achieves a positive return. So the\nRL setting is challenging as it only learns once the agent,\nrandomly or through more sophisticated strategies, finds\na better solution. Additionally, within RL, the learning\nprocess is hard to converge, and the process may become\ntrapped in local minima, as discussed in (Bengio et al.,\n2021).\n2.2. SL For TSP\nIn SL, the model is trained with a dataset including input\ncoordinates alongside their corresponding optimal TSP so-\nlutions. The objective is to identify a function that predicts\noutputs for any given input coordinates, aiming for these\npredictions to approximate the optimal solutions (Li et al.,\n2024; Sun & Yang, 2024; Fu et al., 2021). For example,\n(Xin et al., 2021) trains a Sparse Graph Network using SL to\nevaluate edge scores, which are then integrated with the Lin-\nKernighan-Helsgaun (LKH) algorithm to guide its search\nprocess. (Fu et al., 2021) uses a GNN to learn from solved\noptimal solutions. The model is trained on a small-scale\ninstances, which could be used to build larger heat maps.\nHowever, In SL, the generation of optimal solutions for\ntraining is time-consuming. Finding optimal or near-optimal\nsolutions for large TSP instances requires significant com-\nputational resources and sophisticated algorithms.\nIn other words, an ideal model should circumvent these\nissues, avoiding the sparse reward problem in RL and not\nrelying on labelled optimal solutuons in SL. Addressing this,\na recent approach by (Min et al., 2024) uses unsupervised\nlearning (UL) and trains a GNN using a surrogate loss. The\nmodel generates heat maps through a non-autoregressive\nprocess, without relying on labelled optimal solutions or\nrequiring the agents to explore better solutions, thereby\ncircumventing the need for expensive annotation and miti-\ngating the sparse reward problem.\nThis paper is structured as follows: Section 3 introduces the\nbackground of UL for TSP. Section 4 presents a method for\ngeneralizing across various problem sizes. Section 5 investi-\ngates the generalization behavior w.r.t. different embedding\ndimensions and training sizes. Finally, Section 6 explores\nthe generalization across different distributions through the\nlens of instance hardness.\n3. UL for TSP\nLet’s revisit the definition of the TSP. Essentially, the TSP\ncan be reinterpreted as identifying the shortest Hamiltonian\nCycle that encompasses all the cities. In UL for TSP, the\nauthors first reformulate the TSP into two constraints: the\nshortest path constraint and the Hamiltonian Cycle con-\nstraint. Subsequently, they construct a proxy for each of\nthese constraints (Min et al., 2024).\nIn UTSP, given n cities and their coordinates (xi, yi) ∈R2,\nUTSP first uses GNN to generate a soft indicator matrix\nT ∈Rn×n and use T to build the heat map H ∈Rn×n.\nRow i of H represents the probability distribution of di-\nrected edges originating from city i, while column j cor-\nresponds to the probability distribution of directed edges\nterminating in city j. This heat map is subsequently used\nto direct a local search algorithm. As mentioned, the goal\nof UTSP is to construct a proxy for two constraints. For\nthe shortest constraint, the authors optimize the distance\nterm: ⟨D, H⟩= Pn\ni=1\nPn\nj=1 Di,jHi,j, where ⟨·, ·⟩is the\nFrobenius inner product, D ∈Rn×n is the distance matrix\nand Dij is the distance between city i and city j. To address\nthe Hamiltonian Cycle constraint, the authors introduce the\nT →H transformation, which is designed to implicitly\n2\n\nOn Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem\nencode this constraint.\n3.1. Understanding T →H transformation\nT →H transformation is defined as:\nH = TVTT ,\n(1)\nwhere\nV =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n0\n0\n· · ·\n0\n0\n0\n0\n0\n1\n0\n· · ·\n0\n0\n0\n0\n0\n0\n1\n· · ·\n0\n0\n0\n...\n...\n...\n...\n...\n...\n...\n...\n0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n· · ·\n0\n1\n0\n0\n0\n0\n0\n· · ·\n0\n0\n1\n1\n0\n0\n0\n· · ·\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nis the shift matrix, where V ∈Rn×n. We can interpret V\nas representing a Hamiltonian cycle that follows the path\n1 →2 →3 →· · · →n →1, while T serves as an ap-\nproximation of a general permutation matrix. Given that our\ninitial heat map V represents a Hamiltonian cycle, and con-\nsidering that both the Hamiltonian cycle constraint holds and\nthe node ordering is equivariant under permutation opera-\ntions, the Hamiltonian cycle constraint is implicitly encoded\nin this framework. For more details, we refer the reader to\n(Min & Gomes, 2023).\nWe can also write T →H transformation as:\nH =\nn−1\nX\nt=1\nptpT\nt+1 + pnpT\n1 ,\n(2)\nwhere pt ∈Rn×1 is the tth column of T, T = [p1|p2|...|pn].\nEquation 2 provides another way of understanding the T →\nH transformation. The elements in H are defined using\ntwo nearest columns in T. As shown in Figure 1, p1 =\n[1, 0, 0, 0, 0]T and p2 = [0, 0, 1, 0, 0]T . Since the non-zero\nelement in p1 is located at the first position and the non-\nzero element in p2 is at the third position, it indicates a\ndirected edge from node 1 to node 3 in the heat map H.\nThis is depicted as the purple edge in Figure 1. Similarly,\nthe presence of a non-zero element at the second position in\np3 implies that there is a directed edge from node 3 to node\n2 in the heat map H, represented by the yellow edge.\n3.2. Training UTSP\nIn UTSP, the author train the model using the following loss\nL is:\nλ1\nn\nX\ni=1\n(\nn\nX\nj=1\nTi,j −1)2\n|\n{z\n}\nRow-wise constraint\n+λ2\nn\nX\ni=1\nHi,i\n| {z }\nNo self-loops\n+\nn\nX\ni=1\nn\nX\nj=1\nDi,jHi,j\n|\n{z\n}\nMinimize the distance\n.\n(3)\nFigure 1. Illustration of T and the corresponding H. p1[1] = p2[3]\n= p3[2] = p4[5] = p5[4] = 1. This means there is a corresponding\nHamiltonian Cycle: 1 →3 →2 →5 →4 →1.\nHere, the Row-wise constraint encourages T to behave like\na doubly stochastic matrix, thus serving as a soft relaxation\nof a permutation matrix (Min & Gomes, 2023). The No\nself-loops term discourages self loops in H, where λ2 is the\ndistance of self-loop, the Minimize the Distance term acts as\na proxy for minimizing the distance of a Hamiltonian Cycle.\nAlthough UTSP offers a promising unsupervised way to\nlearn the heat maps, a notable limitation of the model is\nits lack of generalization. Specifically, a model trained\non TSP instances with n cities cannot be applied to other\ninstances, such as instances with n + 1 or n −1 cities. This\nlimitation arises due to T having a fixed dimension of Rn×n.\nConsequently, the model’s architecture is inherently tied to\nthe size of the training instances, restricting its adaptability\nto TSP instances of varying city counts.\n4. Size Generalization\nRecall the understanding of T →H transformation in\nEquation 2. We can interpret that the GNN generates a\nn-dimensional embedding for each city. In our generalized\nmodel, given TSP instances with different sizes, for each\nnode in these instances, the GNN outputs an embedding of\ndimension m. Following this, a Softmax activation function\nis applied to each column of the embedding matrix, resulting\nin the generation of T ∈Rn×m.\nWe then build H using1:\nH =\nm−1\nX\nt=1\nptpT\nt+1 + pmpT\n1 ,\n(4)\nwhere pt ∈Rn×1 is the tth column of T. Equation 4 can be\nreformulated analogously to Equation 1 with V ∈Rm×m.\n1It is important to observe that when m ̸= n, H is not dou-\nbly stochastic. We also tried either replacing T with p n\nmT or\nsubstituting H with n\nmH, both of which yield similar outcomes.\n3\n\nOn Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem\nFigure 2. Training history of different m (embedding dimension) on TSP-2000.\nFigure 3. Training history of different n (instance size) with same embedding dimension m = 1500.\nIn practice, we train our model under the loss L:\nλ1\n\b n\nX\nj=1\n(1 −\nn\nX\ni=1\nHi,j)2 +\nn\nX\ni=1\n(1 −\nn\nX\nj=1\nHi,j)2\t\n|\n{z\n}\nRow and column-wise constraint\n+\nn\nX\ni=1\nn\nX\nj=1\nDi,jHi,j\n|\n{z\n}\nMinimize the distance\n.\n(5)\nBy letting the GNN to output an m-dimensional embedding\nfor each city, the model achieves generalization across dif-\nferent instances. This means that, through Equation 2, the\nheat map H will consistently match the size of the input\ncities (n × n ).\n5. Experiment\nHere, we explore the impact of the generalized model on\ndifferent problem sizes. Specifically, we study TSP with\n200, 500, and 1000 cities, each size is evaluated using 128\ntest instances.\nDifferent from previously UTSP setting, our new methodol-\nogy involves training models on larger datasets and testing\nthem on smaller ones. Specifically, we train a model on\na TSP-2000 dataset with m = 1500 and test it on a TSP-\n1000 dataset; another model is trained on TSP-1000 with\nm = 800 and tested on TSP-500; and finally, a model\ntrained on TSP-400 with m = 320 is tested on TSP-200.\nThe TSP-2000, 1000, and 400 training datasets are created\nby randomly distributing points on a 2D plane, subject to a\nuniform distribution. For TSP-200 and TSP-400, we train\nthe model for 300 epochs, while for TSP-1000, we train the\nmodel for 600 epochs. Each of these datasets consists of\n5,000 training instances.\nWe train our model on one NVIDIA A100 Graphics Pro-\ncessing Unit, using the same Graph Neural Network (GNN)\narchitecture as described in (Min et al., 2024). The model\nis trained on TSP instances of sizes 400, 1000, and 2000,\nusing a configuration of two hidden layers, with each layer\ncomprising 128 hidden units. The hyperparameter λ1, as\nspecified in Equation 4, is set to 100. Our test instances\nare taken from (Fu et al., 2021). Here, the performance gap\nis calculated using the l−lopt\nlopt , where l represents the TSP\nlength generated by our model and lopt denotes the optimal\nlength. We run the search algorithm on Intel Xeon Gold\n6326.\nIn our approach, consistent with the existing UTSP frame-\nwork, we employ the same search methodology. The process\nbegins with the generation of the heat map H, from which\nwe extract the top M largest values in each row. This extrac-\ntion leads to the formation of a new heat map, denoted as ˜H.\nWe compute H′ = ˜H + ˜HT to symmetrize this updated heat\nmap. H′ is then used to guide the search process. We further\ncalculate the overlap between non-zero edges in H′ and the\noptimal solutions, where a higher overlap ratio indicates\nthat H′ more effectively covers the optimal solution. For\n4\n\nOn Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem\nTable 1. Results of our generalizable model + Local Search w.r.t. existing baselines, tested on 128 instances with n = 200, 500 and 1000.\nMethod\nType\nTSP200\nTSP500\nTSP1000\nLength\nGap (%) Time\nLength\nGap (%) Time\nLength\nGap (%)\nTime\nConcorde\nSolver\n10.7191 0.0000\n3.44m\n16.5458 0.0000\n37.66m\n23.1182 0.0000\n6.65h\nGurobi\nSolver\n10.7036 -0.1446\n40.49m\n16.5171 -0.1733\n45.63h\n-\n-\n-\nLKH3\nHeuristic\n10.7195 0.0040\n2.01m\n16.5463 0.0029\n11.41m\n23.1190 0.0036\n38.09m\nGAT (Deudon et al., 2018) RL, S\n13.1746 22.9079\n4.84m\n28.6291 73.0293\n20.18m\n50.3018 117.5860\n37.07m\nGAT (Kool et al., 2019)\nRL, BS\n11.3769 6.1364\n5.77m\n19.5283 18.0257\n21.99m\n29.9048 29.2359\n1.64h\nGCN (Joshi et al., 2019)\nSL, G\n17.0141 58.7272\n59.11s\n29.7173 79.6063\n6.67m\n48.6151 110.2900\n28.52m\nAtt-GCRN(Fu et al., 2021) SL+RL\nMCTS\n10.7358 0.1563\n20.62s + 16.7471 1.2169\n31.17s + 23.5153 1.7179\n43.94s +\n1.33m\n3.33m\n6.68m\nUTSP (Min et al., 2024)\nUL, Search 10.7289 0.0918\n4.83s +\n16.6846 0.8394\n7.28s +\n23.3903 1.1770\n0.23m+\n1.11m\n1.54m\n3.51m\nOur Model\nUL, Search 10.7251 0.0558\n4.94s +\n16.6820 0.8229\n5.66s +\n23.3867 1.1616\n0.24m+\n1.11m\n1.54m\n3.51m\nmore detailed information, we refer to (Min et al., 2024).\nOur results are shown in Table 1, in the case of TSP-200, our\nmodel achieves a gap of 0.0558 %, when tackling TSP-500,\nthe model continues to demonstrate its robustness, with a\ngap of 0.8229%. The performance in both TSP-200 and\nTSP-500 suggests that our model’s approach to guiding the\nlocal search is effective across various scales of the TSP.\nWhen the model is applied to the largest tested instance\nsize, TSP-1000, it achieves a gap of 1.1616%. which is the\nminimum one among all the methods. More importantly, it\nunderscores the model’s generalization to scale and maintain\na level of efficiency in large-scale TSP instances. Our results\nacross all three instance sizes illustrate that the model trained\nusing Equation 5 is able to generalize across instances of\ndifferent sizes and effectively enhances the search process.\n5.1. Impact of Varying m on Training Performance\nAs mentioned in Equation 4, m represents the embedding di-\nmension of each node. In this study, we investigate the effect\nof the embedding dimension m on the model’s performance.\nSpecifically, we train models on TSP-2000 instances with\nvarying embedding dimensions: m = 500, 1000, and 1500.\nWe then evaluate these models on TSP-1000 instances to\nassess their performance.\nTable 2. Overlap ratios and the search results on 128 TSP-1000\ninstances in (Fu et al., 2021) using different embedding dimension\nm. We select top 5 elements from each row in the heat maps.\nm\nOVERLAP RATIO(%) PERFORMANCE GAP(%)\n500\n82.70\n2.0746 ± 0.5457\n1000\n93.75\n1.4832 ± 0.2305\n1500\n94.93\n1.4145 ± 0.2005\nThe training curves for different embedding dimensions are\nshown in Figure 2. We calculate the overlap ratios and\nsearch performance using models with different embedding\ndimensions, the results are shown in Table 2, 3. Our find-\nings indicate that an increase in the embedding dimension\ncontributes to higher overlap ratios and enhanced search\nperformance. For instance, the overlap ratio improves from\n82.70% to 94.93% when the embedding dimension m is\nincreased from 500 to 1500, based on the heat maps with\ntop 5 elements from each row. Correspondingly, the search\nperformance also improves, with the gap decreasing from\n2.0746% to 1.4145%. This highlights the significance of\nembedding dimension in increasing model efficacy. A larger\nembedding dimension can better identify optimal or near-\noptimal solutions and narrow the gap.\nTable 3. Overlap ratios and the search results on 128 TSP-1000\ninstances in (Fu et al., 2021) using different embedding dimension\nm. We select top 20 elements from each row in the heat maps.\nm\nOVERLAP RATIO(%) PERFORMANCE GAP(%)\n500\n99.99\n1.1995 ± 0.1849\n1000\n100.00\n1.1608 ± 0.1844\n1500\n100.00\n1.1616 ± 0.1743\nSpecifically, it is noteworthy that when selecting the top 20\nelements from each row, both m = 1000 and m = 1500\nachieve a 100.00% overlap ratio, whereas m = 500 does\nnot cover all the optimal solutions, resulting in a larger gap.\nFurthermore, we observe that m = 1000 exhibits marginally\nbetter performance compared to m = 1500. This suggests\nthat beyond a certain threshold, increasing the embedding\ndimension yields diminishing returns in terms of covering\noptimal solutions. It also implies that there might be an\noptimal range for the embedding dimension, indicating a\nneed for careful consideration in the choice of m to optimize\nmodel performance.\n5\n\nOn Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem\nFigure 4. Expansion\nFigure 5. Explosion\nFigure 6. Implosion\nFigure 7. Uniform\n5.2. Impact of Varying n on Training Performance\nOur model can generalize across different sizes, meaning\nthat training on one size can effectively translate to per-\nformance on another, previously unseen size. Here we in-\nvestigate how varying the training size impacts the model’s\nperformance. We train the model using TSP-400, TSP-1000,\nand TSP-2000 instances, all with the same embedding di-\nmension m = 1500. The training results are illustrated in\nFigure 3.\nWe then test how different training instances’ sizes can af-\nfect the overlap ratio and the performance. The results are\nshown in Table 4, 5. We note that training with larger in-\nstances enhances search performance under both top 5 and\ntop 20 conditions. Specifically, when selecting the top 5 ele-\nments from each row, the performance gap improves from\n3.0762% to 1.4145%. Similarly, when choosing the top 20\nelements from each row, the gap shows a marked improve-\nment, decreasing from 1.1885% to 1.1616%.\nOur results\nTable 4. Overlap ratios and the search results on 128 TSP-1000\ninstances instances in (Fu et al., 2021) with m = 1500 using\ntraining instances with different sizes. We select top 5 elements\nfrom each row in the heat maps. The first column denotes different\ntraining sizes.\nn\nOVERLAP RATIO(%) PERFORMANCE GAP(%)\n400\n68.83\n3.0762 ± 1.3141\n1000\n93.48\n1.5563 ± 0.2345\n2000\n94.93\n1.4145 ± 0.2005\nTable 5. Overlap ratios and the search results on 128 TSP-1000\ninstances in (Fu et al., 2021) with m = 1500 using training in-\nstances with different sizes. We select top 20 elements from each\nrow in the heat maps. The first column denotes different training\nsizes.\nn\nOVERLAP RATIO(%) PERFORMANCE GAP(%)\n400\n99.96\n1.1885 ± 0.1927\n1000\n100.00\n1.1763 ± 0.1743\n2000\n100.00\n1.1616 ± 0.1743\nhighlight the importance of selecting larger training instance\nsizes to enhance model performance and efficiency.\n6. Hardness Generalization\nPrevious studies suggest that UL can generalize across dif-\nferent sizes, guide the search and reduce the search space,\nHere, we delve into how UL’s capability to reduce the search\nspace is influenced by different distributions. Specifically,\nwe explore the relationship between different distributions\nand the efficiency of using UL for solving the TSP.\nHowever, building a connection between various distribu-\ntions and the efficacy of UL in reducing the search space\npresents significant challenges. To address this, we first fo-\ncus on correlating different distributions with their hardness\nlevels.\nPhase transition\nA phase transition refers to a change\nin the solvability of NP-hard problems. When some pa-\nrameters of the problem is varied, for example, the density\nof constraints in a Boolean satisfiability problem (SAT)\nproblem (Mitchell et al., 1992), the problem undergoes a\ntransition from being almost solvable to unsolvable. To\nbe specific, The phase transition in SAT refers to a sharp\nchange in the solvability of these problems, depending on\nthe ratio of the number of clauses to the number of variables\nin the formula. When the ratio is low (few clauses relative to\nvariables), most instances of the problem are easy to solve.\nThis is because there are fewer constraints, making it more\nlikely to find a satisfying assignment. Conversely, when this\nratio is high (many clauses relative to variables), the prob-\nlem becomes over-constrained, and most instances are also\neasy to solve because they are almost certainly unsatisfiable.\nThe most interesting part occurs at a certain critical ratio,\ntypically around 4.3 for 3-SAT problems. At this ratio, the\nproblems undergo a phase transition and become extremely\nhard to solve. In other words, the problems are most difficult\naround the phase transition point (Monasson et al., 1999).\nPhase transitions provides a powerful framework to study\nthe properties of NP-hard problems. However, the exact\nnature and location of these transitions can be difficult to\ndetermine and may depend intricately on the structure of\n6\n\nOn Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem\nthe different problems. For TSP, (Gent & Walsh, 1996) sug-\ngest using the parameter τ = lopt/\n√\nnA, where A denotes\nthe area covered by the TSP instance, lopt represents the\nlength of the optimal solution, and n is the number of cities.\nThis approach is based on the observation that there is a\nrapid transition in solvability around a fixed value of the\nparameter, specifically at approximately Tc = 0.78.\nFigure 8. TSP phase transition and the τ values for different distri-\nbutions.\nHere we study four different distributions and see how it\ncan effect the search space reduction, an illustration of these\nfour distribution is shown in Figure 4 ∼7. As mentioned\nearlier, around the phase transition point, the problems of-\nten exhibits the greatest computational complexity (Hard).\nFigure 8 illustrates the scheme of phase transition in the\nTSP. The x-axis is the τ value, while the y-axis corresponds\nto the level of hardness. The point at which τ equals the\ncritical threshold Tc = 0.78 marks the peak of difficulty,\nexhibiting the highest hardness, we refer more details to\n(Gent & Walsh, 1996).\nFurthermore, we present the τ values for four different dis-\ntributions, where each τ is computed as an average from 100\ninstances, each with a size of 200, 500 and 1000, detailed in\nTable 6 and Figure 8.\nTable 6. τ = lopt/\n√\nnA of Expansion, Explosion, Implosion, and\nUniform for different sizes.\nSIZE\nEXPANSION EXPLOSION IMPLOSION UNIFORM\n1000\n0.4838\n0.5629\n0.7237\n0.7460\n500\n0.5114\n0.5905\n0.7338\n0.7515\n200\n0.5796\n0.6337\n0.7539\n0.7745\nAs shown in Figure 8, the Uniform distribution is closest to\nthe phase transition point Tc. This indicates a highest level\nof hardness. Consequently, in terms of transitioning from\nhard to easy, the order is observed as follows: Uniform ≈\nImplosion > Explosion > Expansion. Following upon this\nconcept, we examine how these distributions influence the\ncapacity of UL to efficiently reduce the search space and\nguide the search.\nFigure 9. The training curves for TSP-400 with m = 320 across\nfour different distributions are shown; the model is then tested on\n128 TSP-200 instances.\nFigure 10. The training curves for TSP-1000 with m = 800 across\nfour different distributions are shown; the model is then tested on\n128 TSP-500 instances.\nWe first train the models using 4 different distributions with\nthe same parameters in Section 4. We calculate the overlap\nratio of these models for TSP-200, 500, and 1000. The train-\ning results are shown in Figure 9, 10 and 11. We observe that\nmodels trained with harder instances consistently exhibit a\nlower loss. Specifically, the loss curves for models trained\nusing the Uniform distribution consistently show the lowest\nloss, while those trained with Expansion and Explosion dis-\ntributions demonstrate higher losses. This suggests that the\nhardness level of training instances plays a significant role\nin the effectiveness of the model training, directly impacting\nthe loss metrics. It is important to note that throughout our\n7\n\nOn Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem\nFigure 11. The training curves on TSP-2000 with m = 1500 across four different distributions are shown; the model is then tested on 128\nTSP-1000 instances.\nTable 7. Overlap ratios and the search results on 128 TSP-200\ninstances in (Fu et al., 2021) using different distributions. We\nselect top 5 elements from each row in the heat maps.\nDATASET\nOVERLAP RATIO(%) PERFORMANCE GAP(%)\nUNIFORM\n95.64\n0.0883± 0.0885\nIMPLOSION\n95.50\n0.0876± 0.0920\nEXPLOSION\n95.29\n0.0979± 0.0907\nEXPANSION\n94.00\n0.1131± 0.0973\nTable 8. Overlap ratios and the search results on 128 TSP-500\ninstances in (Fu et al., 2021) using different distributions. We\nselect top 5 elements from each row in the heat maps.\nDATASET\nOVERLAP RATIO(%) PERFORMANCE GAP(%)\nUNIFORM\n95.47\n0.9311± 0.1638\nIMPLOSION\n95.40\n0.9394± 0.1732\nEXPLOSION\n94.99\n0.9410± 0.1764\nEXPANSION\n94.03\n1.0137± 0.1800\ntraining process, all other hyperparameter settings remained\nconstant. Therefore, the observed variations in loss can be\nattributed solely to the differences in training distributions.\nWe then evaluate how different distributions can affect the\nsearch results. We pick the top 5 element each row and\nbuild the heat maps. The overlap ratio and the search results\nare shown in Table 7, 8 and 9. When training on easier\ndistributions such as Explosion and Expansion, we observe\nTable 9. Overlap ratios and the search results on 128 TSP-1000\ninstances in (Fu et al., 2021) using different distributions. We\nselect top 5 elements from each row in the heat maps.\nDATASET\nOVERLAP RATIO(%) PERFORMANCE GAP(%)\nUNIFORM\n94.93\n1.4145 ± 0.2005\nIMPLOSION\n94.71\n1.4060 ± 0.2078\nEXPLOSION\n93.86\n1.5274 ± 0.2632\nEXPANSION\n93.38\n1.5777 ± 0.2735\nTable 10. Overlap ratios and the search results on 128 TSP-1000\ninstances in (Fu et al., 2021) using different distributions. We\nselect top 20 elements from each row in the heat maps.\nDATASET\nOVERLAP RATIO(%) PERFORMANCE GAP(%)\nUNIFORM\n100.00\n1.1616± 0.1743\nIMPLOSION\n100.00\n1.1844± 0.1572\nEXPLOSION\n100.00\n1.1937± 0.1764\nEXPANSION\n100.00\n1.1797± 0.2025\nlow overlap ratios and larger performance gaps. This in-\ndicates that models trained on simpler distributions may\nstruggle to generalize effectively to more challenging in-\nstances of the problem. The lower overlap ratios suggest\nthat the solutions generated by these models are less aligned\nwith the optimal solutions, and the larger performance gaps\nhighlight a significant disparity in effectiveness when these\nmodels are applied to the test TSP instances. Training on\nharder distributions, such as Uniform, yields higher overlap\n8\n\nOn Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem\nratios and improved search performance. This indicates that\nmodels trained on harder distributions can build a better\nrepresentation of the search space, which enables the search\nto perform more effectively. It is also observed that the\nplateaus during training are more pronounced when train-\ning on harder instances, suggesting that the optimization\nlandscape becomes more complex when the hardness level\nincreases.\nWe evaluate the model’s performance on TSP-1000 in-\nstances by utilizing the top 20 elements from each row for\neach distribution, as detailed in Table 10. We observe that by\nselecting the top 20 elements, H′ is able to cover 100.00%\nof the optimal solutions. Overall, the performance gaps\nacross the distributions are similar, with training on uniform\ndistributions continuing to exhibit the lowest performance\ngap.\n7. Conclusion\nThis work introduces a new methodology that allows a\ntrained, unsupervised TSP model to generalize across dif-\nferent problem sizes. Our results demonstrate that training\non larger problem instances can improve performance com-\npared to training with smaller instances. Additionally, we\ndelve into the influence of embedding dimensions on TSP\nresults, showing that larger embedding dimensions are im-\nportant in constructing more effective representations that\nguide the search process more efficiently. Moreover, we\ninvestigate the model’s performance using training datasets\nwith different levels of hardnesses. We show that training on\nharder instances can improve model performance, empha-\nsizing the importance of selecting training instances with\nappropriate difficulty levels. We train our models on dif-\nferent TSP distributions to understand their impact on the\neffectiveness of UL models. Our study indicates a clear\nrelationship between the inherent hardness of distribution\nand the model’s capacity to generalize and effectively solve\nTSP instances. To our knowledge, this is the first study to\nsystematically investigate and demonstrate this connection.\nOur results highlight the relationship between the character-\nistics of training instances (size and hardness), embedding\ndimensions, and model performance in UL, particularly\nwhen addressing CO problems such as the TSP. We antici-\npate that these findings — emphasizing the benefits of train-\ning on larger, harder instances with increased embedding\ndimensions — can inspire further research in the application\nof Unsupervised Learning to Combinatorial Optimization\ntasks.\n8. Acknowledgement\nThis project is partially supported by the Eric and Wendy\nSchmidt AI in Science Postdoctoral Fellowship, a Schmidt\nFutures program; the National Science Foundation (NSF)\nand the National Institute of Food and Agriculture (NIFA);\nthe Air Force Office of Scientific Research (AFOSR); the\nDepartment of Energy; and the Toyota Research Institute\n(TRI).\nReferences\nBello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio,\nS. Neural combinatorial optimization with reinforcement\nlearning. arXiv preprint arXiv:1611.09940, 2016.\nBengio, Y., Lodi, A., and Prouvost, A. Machine learning\nfor combinatorial optimization: a methodological tour\nd’horizon. European Journal of Operational Research,\n290(2):405–421, 2021.\nBi, J., Ma, Y., Wang, J., Cao, Z., Chen, J., Sun, Y., and Chee,\nY. M. Learning generalizable models for vehicle routing\nproblems via knowledge distillation. Advances in Neural\nInformation Processing Systems, 35:31226–31238, 2022.\nChen, J., Wang, J., Zhang, Z., Cao, Z., Ye, T., and Chen, S.\nEfficient meta neural heuristic for multi-objective com-\nbinatorial optimization. Advances in Neural Information\nProcessing Systems, 36, 2024.\nDeudon, M., Cournut, P., Lacoste, A., Adulyasak, Y., and\nRousseau, L.-M. Learning heuristics for the tsp by policy\ngradient. In International Conference on the Integration\nof Constraint Programming, Artificial Intelligence, and\nOperations Research, pp. 170–181. Springer, 2018.\nFu, Z.-H., Qiu, K.-B., and Zha, H. Generalize a small pre-\ntrained model to arbitrarily large tsp instances. Proceed-\nings of the AAAI Conference on Artificial Intelligence, 35\n(8):7474–7482, 2021.\nGent, I. P. and Walsh, T. The tsp phase transition. Artificial\nIntelligence, 88(1-2):349–358, 1996.\nHelsgaun, K.\nAn effective implementation of the lin–\nkernighan traveling salesman heuristic. European journal\nof operational research, 126(1):106–130, 2000.\nJoshi, C. K., Laurent, T., and Bresson, X. An efficient\ngraph convolutional network technique for the travelling\nsalesman problem. arXiv preprint arXiv:1906.01227,\n2019.\nKool, W., van Hoof, H., and Welling, M. Attention, learn\nto solve routing problems! In International Conference\non Learning Representations, 2019. URL https://\nopenreview.net/forum?id=ByxBFsRqYm.\nLi, Y., Guo, J., Wang, R., and Yan, J. From distribution\nlearning in training to gradient search in testing for com-\nbinatorial optimization. volume 36, 2024.\n9\n\nOn Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem\nMa, Y., Cao, Z., and Chee, Y. M. Learning to search feasible\nand infeasible regions of routing problems with flexible\nneural k-opt. Advances in Neural Information Processing\nSystems, 36, 2024.\nMin, Y. and Gomes, C. Unsupervised learning permuta-\ntions for tsp using gumbel-sinkhorn operator. In NeurIPS\n2023 Workshop Optimal Transport and Machine Learn-\ning, 2023.\nMin, Y., Bai, Y., and Gomes, C. P. Unsupervised learning\nfor solving the travelling salesman problem. Advances in\nNeural Information Processing Systems, 36, 2024.\nMitchell, D., Selman, B., Levesque, H., et al. Hard and easy\ndistributions of sat problems. In Aaai, volume 92, pp.\n459–465, 1992.\nMonasson, R., Zecchina, R., Kirkpatrick, S., Selman, B.,\nand Troyansky, L. Determining computational complex-\nity from characteristic ‘phase transitions’. Nature, 400\n(6740):133–137, 1999.\nSun, Z. and Yang, Y. Difusco: Graph-based diffusion solvers\nfor combinatorial optimization. Advances in Neural In-\nformation Processing Systems, 36, 2024.\nXin, L., Song, W., Cao, Z., and Zhang, J. Neurolkh: Com-\nbining deep learning model with lin-kernighan-helsgaun\nheuristic for solving the traveling salesman problem. Ad-\nvances in Neural Information Processing Systems, 34:\n7472–7483, 2021.\nYe, H., Wang, J., Cao, Z., Liang, H., and Li, Y. Deepaco:\nNeural-enhanced ant systems for combinatorial optimiza-\ntion. volume 36, 2024.\nZhou, J., Wu, Y., Song, W., Cao, Z., and Zhang, J. Towards\nomni-generalizable neural methods for vehicle routing\nproblems. In International Conference on Machine Learn-\ning, pp. 42769–42789. PMLR, 2023.\n10",
    "pdf_filename": "On_Size_and_Hardness_Generalization_in_Unsupervised_Learning_for_the_Travelling_Salesman_Problem.pdf"
}