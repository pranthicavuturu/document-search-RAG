{
    "title": "SpikingNeRF Making Bio-inspired Neural Networks See through the Real World",
    "context": "In this paper, we propose SpikingNeRF, which aligns the tem- poral dimension of spiking neural networks (SNNs) with the radiance rays, to seamlessly accommodate SNNs to the re- construction of neural radiance fields (NeRF). Thus, the com- putation turns into a spike-based, multiplication-free man- ner, reducing energy consumption and making high-quality 3D rendering, for the first time, accessible to neuromorphic hardware. In SpikingNeRF, each sampled point on the ray is matched to a particular time step and represented in a hy- brid manner where the voxel grids are maintained as well. Based on the voxel grids, sampled points are determined whether to be masked out for faster training and inference. However, this masking operation also incurs irregular tem- poral length, making it intractable for hardware processors, e.g., GPUs, to conduct parallel training. To address this prob- lem, we develop the temporal padding strategy to tackle the masked samples to maintain regular temporal length, i.e., reg- ular tensors, and further propose the temporal condensing strategy to form a denser data structure for hardware-friendly computation. Experiments on various datasets demonstrate that our method can reduce energy consumption by an aver- age of 70.79% and obtain comparable synthesis quality with the ANN baseline. Verification on the neuromorphic hard- ware accelerator also shows that SpikingNeRF can further benefit from neuromorphic computing over the ANN base- lines on energy efficiency. Codes and the appendix are in https://github.com/Ikarosy/SpikingNeRF-of-CASIA. Spiking neural networks (SNNs) are considered the third generation of neural networks, and their bionic modeling encourages much research attention to explore the prospec- tive bio-plausible intelligence that features multitasking and extreme energy efficiency as the human brain does (Maass 1997; Roy, Jaiswal, and Panda 2019). While much dedica- tion has been devoted to SNN research, the gap between the expectation of SNN boosting a wider range of intelli- gent tasks and the fact of artificial neural networks (ANNs) dominating deep learning in the majority of tasks still exists. Recently, more research interests have been invested in narrowing the gap and have promoted notable milestones in various tasks, including image classification (Zhou et al. *Equal contribution. †Corresponding author. Synthetic-NSVF BlendedMVS Synthetic-NeRF NSVF NeRF Mip-NeRF DIVeR TensoRF SpiNeRF-T DVGO SpiNeRF-D Figure 1: Comparisons of our SpikingNeRF with other NeRF-based works in synthesis quality and model rendering energy. Different colors represent different works, and our SpikingNeRF with two different frameworks are denoted in red and violet, respectively. A detailed notation explana- tion is specified in the Experiments section. Different testing datasets are denoted by different shapes. 2022), object detection (Zhang et al. 2022), graph prediction (Zhu et al. 2022b), natural language processing (Zhu, Zhao, and Eshraghian 2023), etc. Besides multi-task supporting, SNN research is also thriving in performance lifting and en- ergy efficiency exploration at the same time. However, we have not yet witnessed the establishment of SNN in the real 3D reconstruction task with advanced performance. Let alone enabling the high-quality real 3D rendering on neuromorphic hardwares, e.g., Loihi (Davies et al. 2018), PTB (Lee, Zhang, and Li 2022), and SpikeSim (Moitra et al. 2023), where neuromorphic computing can essentially acquire low-energy consumption. Meanwhile, high-quality real 3D reconstruction, specifically for NeRF (Mildenhall et al. 2021), has suffered huge computation overhead during rendering, consuming a significant magni- tude of energy (Garbin et al. 2021). Naturally, this raises a question: could bio-inspired spiking neural networks recon- struct the real 3D scene with advanced quality at low en- ergy consumption? This paper investigates the rendering of neural radiance fields with a spiking approach to answer the question. We propose SpikingNeRF to reconstruct volumetric scene arXiv:2309.10987v4  [cs.NE]  19 Nov 2024",
    "body": "SpikingNeRF: Making Bio-inspired Neural Networks See through the Real World\nXingting Yao1,2*, Qinghao Hu1∗, Fei Zhou3, Tielong Liu1,2,\nZitao Mo1, Zeyu Zhu1,2, Zhengyang Zhuge1, Jian Cheng1†\n1Institute of Automation, Chinese Academy of Sciences\n2School of Future Technology, University of Chinese Academy of Sciences\n3China Electric Power Research Institute Co., Ltd\n{yaoxingting2020, huqinghao2014, jian.cheng}@ia.ac.cn,\nAbstract\nIn this paper, we propose SpikingNeRF, which aligns the tem-\nporal dimension of spiking neural networks (SNNs) with the\nradiance rays, to seamlessly accommodate SNNs to the re-\nconstruction of neural radiance fields (NeRF). Thus, the com-\nputation turns into a spike-based, multiplication-free man-\nner, reducing energy consumption and making high-quality\n3D rendering, for the first time, accessible to neuromorphic\nhardware. In SpikingNeRF, each sampled point on the ray is\nmatched to a particular time step and represented in a hy-\nbrid manner where the voxel grids are maintained as well.\nBased on the voxel grids, sampled points are determined\nwhether to be masked out for faster training and inference.\nHowever, this masking operation also incurs irregular tem-\nporal length, making it intractable for hardware processors,\ne.g., GPUs, to conduct parallel training. To address this prob-\nlem, we develop the temporal padding strategy to tackle the\nmasked samples to maintain regular temporal length, i.e., reg-\nular tensors, and further propose the temporal condensing\nstrategy to form a denser data structure for hardware-friendly\ncomputation. Experiments on various datasets demonstrate\nthat our method can reduce energy consumption by an aver-\nage of 70.79% and obtain comparable synthesis quality with\nthe ANN baseline. Verification on the neuromorphic hard-\nware accelerator also shows that SpikingNeRF can further\nbenefit from neuromorphic computing over the ANN base-\nlines on energy efficiency. Codes and the appendix are in\nhttps://github.com/Ikarosy/SpikingNeRF-of-CASIA.\nIntroduction\nSpiking neural networks (SNNs) are considered the third\ngeneration of neural networks, and their bionic modeling\nencourages much research attention to explore the prospec-\ntive bio-plausible intelligence that features multitasking and\nextreme energy efficiency as the human brain does (Maass\n1997; Roy, Jaiswal, and Panda 2019). While much dedica-\ntion has been devoted to SNN research, the gap between\nthe expectation of SNN boosting a wider range of intelli-\ngent tasks and the fact of artificial neural networks (ANNs)\ndominating deep learning in the majority of tasks still exists.\nRecently, more research interests have been invested in\nnarrowing the gap and have promoted notable milestones\nin various tasks, including image classification (Zhou et al.\n*Equal contribution.\n†Corresponding author.\nSynthetic-NSVF\nBlendedMVS\nSynthetic-NeRF\nNSVF\nNeRF\nMip-NeRF\nDIVeR\nTensoRF\nSpiNeRF-T\nDVGO\nSpiNeRF-D\nFigure 1: Comparisons of our SpikingNeRF with other\nNeRF-based works in synthesis quality and model rendering\nenergy. Different colors represent different works, and our\nSpikingNeRF with two different frameworks are denoted\nin red and violet, respectively. A detailed notation explana-\ntion is specified in the Experiments section. Different testing\ndatasets are denoted by different shapes.\n2022), object detection (Zhang et al. 2022), graph prediction\n(Zhu et al. 2022b), natural language processing (Zhu, Zhao,\nand Eshraghian 2023), etc. Besides multi-task supporting,\nSNN research is also thriving in performance lifting and en-\nergy efficiency exploration at the same time.\nHowever, we have not yet witnessed the establishment\nof SNN in the real 3D reconstruction task with advanced\nperformance. Let alone enabling the high-quality real 3D\nrendering on neuromorphic hardwares, e.g., Loihi (Davies\net al. 2018), PTB (Lee, Zhang, and Li 2022), and SpikeSim\n(Moitra et al. 2023), where neuromorphic computing can\nessentially acquire low-energy consumption. Meanwhile,\nhigh-quality real 3D reconstruction, specifically for NeRF\n(Mildenhall et al. 2021), has suffered huge computation\noverhead during rendering, consuming a significant magni-\ntude of energy (Garbin et al. 2021). Naturally, this raises a\nquestion: could bio-inspired spiking neural networks recon-\nstruct the real 3D scene with advanced quality at low en-\nergy consumption? This paper investigates the rendering of\nneural radiance fields with a spiking approach to answer the\nquestion.\nWe propose SpikingNeRF to reconstruct volumetric scene\narXiv:2309.10987v4  [cs.NE]  19 Nov 2024\n\nrepresentations of neural radiance fields. For fast synthe-\nsis, voxel grids methods (Hedman et al. 2021; Liu et al.\n2022; Sun, Sun, and Chen 2022) are considered to explicitly\nstore the volumetric parameters. For efficient computation,\nthe spiking multilayer perceptron (sMLP) is utilized to im-\nplicitly yield volumetric information in an addition-only and\nspike-driven approach. With such an explicit-and-implicit\nhybrid, fast and energy-efficient neural radiance rendering\nbecomes feasible.\nInspired by the imaging process of the primate fovea\nin the retina that accumulates the intensity flow over time\nto stimulate the photoreceptor cell (Masland 2012; W¨assle\n2004), we take one step forward to associate the accumu-\nlation process of rendering with the temporal accumulation\nprocess of SNNs, which ultimately stimulates the spiking\nneurons to fire. Concretely, we align the radiance ray with\nthe temporal dimension of the sMLP, and individually match\neach sampled point on the ray to a time step during render-\ning. Thus, the geometric consecutiveness of the ray is trans-\nformed into the temporal continuity of the SNN. As a re-\nsult, SpikingNeRF seizes the nature of both worlds to make\nthe NeRF rendering in a spiking manner, and brings about a\nnovel and effective data encoding approach for SNN-based\n3D rendering. Different from other SNN-based image recon-\nstructions (Zhu et al. 2022a; Mei et al. 2023), which focus\non the event-based gray 2D reconstruction, we are the first\nto explore the reconstruction of the real RGB world with\nSNNs.\nMoreover, since the number of sampled points on differ-\nent rays always varies, the temporal lengths of different rays\nbecome irregular. Consequently, the querying for the volu-\nmetric information can hardly be parallelized during render-\ning, severely hindering the training process on graphics pro-\ncessing units (GPUs). To solve this issue, we first investigate\nthe temporal padding (TP) method to attain the regular tem-\nporal length in a querying batch, i.e., a regular-shaped tensor,\nthus ensuring parallelism and GPU training feasible. Fur-\nthermore, we propose the temporal condensing-and-padding\n(TCP), to fully constrain the tensor size and condense the\ndata distribution, which is hardware-friendly to neuromor-\nphic hardware accelerators and GPUs. Our thorough exper-\nimentation proves that TCP can maintain both the energy\nmerits of SNNs and the high quality of NeRF rendering as\nshown in Fig. 1.\nTo sum up, our main contributions are as follows:\n• We propose SpikingNeRF that aligns the temporal di-\nmension of SNNs with the radiance rays of NeRF, ex-\nploiting the temporal characteristics of SNNs. To the best\nof our knowledge, this is the first work to accommodate\nSNNs to reconstructing 3D scenes, making high-quality\n3D rendering feasible on the neuromorphic hardware.\n• We propose TP and TCP to solve the irregular tempo-\nral lengths, ensuring the training and inference paral-\nlelism on GPUs. TCP can also further keep SpikingNeRF\nhardware-friendly to the neuromorphic hardware.\n• Our experiments demonstrate the effectiveness of Spik-\ningNeRF on four mainstream tasks, achieving advanced\nenergy efficiency as shown in Fig. 1. For another specific\nexample, SpikingNeRF-D can achieve 72.95% energy re-\nduction with a 0.33 PSNR drop on Tanks&Temples.\nIn order to avoid misunderstanding, we additionally cite\n(Liao et al. 2023) which shares the same title as ours. Their\nwork, based on the ANN implementation, essentially builds\na non-linear and non-spike function named B-FIF to post-\nprocess the particular density-related output of the original\nANN-based NeRF. Overall, they do not use SNNs, do not\naim at the neuromorphic hardware, focus on geometric re-\nconstruction, and report only on the Chamfer metric. So, it\nis rational to deem that (Liao et al. 2023) is irrelevant and\nour work is impossible to conduct quantitative comparisons\nwith theirs. Different from the above work, we are the first\nto explore the rendering of real RGB with SNNs and benefit\nNeRF rendering from neuromorphic computing.\nPreliminaries\nNeural radiance field. To reconstruct the scene for the\ngiven view, NeRF (Mildenhall et al. 2021) first utilizes an\nMLP, which takes in the location coordinates p ∈R3 and\nthe view direction v ∈R2 and yields the density σ ∈R and\nthe color c ∈R3, to implicitly maintain continuous volu-\nmetric representations:\ne, σ = MLPθ(p),\n(1)\nc = MLPγ(e, v),\n(2)\nwhere θ and γ denote the parameters of the separate two\nparts of the MLP, and e is the embedded features. Next,\nNeRF renders the pixel of the expected scene by casting a\nray r from the camera origin point to the direction of the\npixel, then sampling K points along the ray. Through query-\ning the MLP as in Eq. (1-2) K times, K color values and K\ndensity values can be retrieved. Finally, following the prin-\nciples of the discrete volume rendering proposed in (Max\n1995), the expected pixel RGB ˆC(r) can be rendered:\nα = 1 −exp(−σiδi),\nwi =\ni−1\nY\nj=1\n(1 −αi),\n(3)\nˆC(r) ≈\nK\nX\ni=1\nwiαici,\n(4)\nwhere ci and σi denotes the color and the density values of\nthe i-th point respectively, and δi is the distance between the\nadjacent point i and i + 1.\nAfter rendering all the pixels, the expected scene is recon-\nstructed. With the ground-truth pixel color C(r), the param-\neters of the MLP can be trained end-to-end by minimizing\nthe MSE loss:\nL =\n1\n|R|\nX\nr∈R\n∥ˆC(r) −C(r)∥2\n2,\n(5)\nwhere R is the mini-batch containing the sampled rays.\nHybrid volumetric representation. The number of sam-\npled points K in Eq. 4 is usually big, leading to the heavy\nMLP querying burden as displayed in Eq. (1-2). To allevi-\nate this problem, voxel grid representation is utilized to con-\ntain the volumetric parameters directly, e.g., the embedded\n\nfeature e and the density σ in Eq. 1, as the values of the\nvoxel grid. Thus, querying the MLP in Eq. 1 is substituted\nto querying the voxel grids and operating the interpolation,\nwhich is much easier:\nσ = act(interp(p, Vσ)),\n(6)\ne = interp(p, Vf),\n(7)\nwhere Vσ and Vf are the voxel grids related to the volu-\nmetric density and features, respectively. “interp” denotes\nthe interpolation operation, and “act” refers to the activation\nfunction, e.g., ReLU or the shifted softplus (Barron et al.\n2021).\nFurthermore, those irrelevant points with low density or\nunimportant points with low weight can be masked through\npredefined thresholds λ, then Eq. 4 turns into:\nA ≜{i : wi > λ1, αi > λ2},\n(8)\nˆC(r) ≈\nX\ni∈A\nwiαici.\n(9)\nThus, the queries of the MLP for sampled points in Eq. 2\nare significantly reduced. With such computational benefits,\nhybrid volumetric representation is prevalent in neural radi-\nance rendering (Sun, Sun, and Chen 2022; Chen et al. 2022).\nSpiking neuron. The spiking neuron is the most fundamen-\ntal unit in spiking neural networks, which essentially differs\nSNNs from ANNs. The modeling of spiking neurons com-\nmonly adopts the leaky integrate-and-fire (LIF) model:\nUt = Vt−1 + 1\nτ (Xt −Vt−1 + Vreset),\n(10)\nSt = H(Ut −Vth),\n(11)\nVt = Ut ⊙(1 −St) + VresetSt.\n(12)\nHere, we follow the renowned SpikingJelly (Fang et al.\n2023) to implement the LIF neurons. ⊙denotes the\nHadamard product. Ut is the intermediate membrane poten-\ntial at time-step t and can be updated through Eq. 10, where\nVt−1 is the actual membrane potential at time-step t −1\nand Xt denotes the input vector at time-step t, e.g., the ac-\ntivation vector from the previous layer in MLPs. The output\nspike vector St is given by the Heaviside step function H(·)\nin Eq. 11, indicating that a spike is fired when the membrane\npotential exceeds the potential threshold Vth. Dependent on\nwhether the spike is fired at time-step t, the membrane po-\ntential Vt is set to Ut or the reset potential Vreset through\nEq. 12.\nSince the Heaviside step function H(·) is not differen-\ntiable, the surrogate gradient method is utilized to solve this\nissue, which is defined as :\ndH(x)\ndx\n=\n1\n1 + exp(−αx),\n(13)\nwhere α is a predefined hyperparameter. Thus, spiking neu-\nral networks can be optimized end-to-end.\nMethodology\nData encoding\nIn this subsection, we explore two naive data encoding ap-\nproaches for converting the input data to SNN-tailored for-\nmats, i.e., direct-encoding and Poisson-encoding. Both of\nTemporal Dim.\n#2  Poisson Generator\n. . . . . .\nANN\nSNN\nInput Data\n#1  Duplication for  times\nT\nor \nMean\nVoting\nFigure 2: Conventional data encoding schemes. For direct-\nencoding, only the operation #1 is necessary that it dupli-\ncates the input data T times to fit the length of the temporal\ndimension. For Poisson-encoding, both operation #1 and #2\nare utilized to generate the input spike train. The “Mean” or\n“Voting” operation is able to decode the SNN output.\nthem are proven to perform well in the direct learning of\nSNNs (Fang et al. 2021; Han, Srinivasan, and Roy 2020;\nShrestha and Orchard 2018; Cheng et al. 2020).\nAs described in Preliminaries, spiking neurons receive\ndata with an additional dimension called the temporal di-\nmension, which is indexed by the time-step t in Eq. (10-12).\nConsequently, original ANN-formatted data need to be en-\ncoded to fill the temporal dimension as illustrated in Fig.\n2. In the direct-encoding scheme, the original data is dupli-\ncated T times to fill the temporal dimension, where T rep-\nresents the total length of the temporal dimension. As for\nthe Poisson-encoding scheme, besides the duplication op-\neration, it perceives the input value as the probability and\ngenerates a spike according to the probability at each time\nstep. Additionally, a decoding method is entailed for the\nsubsequent operations of rendering, and the mean (Li et al.\n2021) and the voting (Fang et al. 2021) decoding operations\nare commonly considered. We employ the former approach\nsince the latter one is designed for classification tasks (Diehl\nand Cook 2015; Wu et al. 2019).\nThus, with the above two encoding methods, we build two\nnaive versions of SpikingNeRF and are able to conduct ex-\nperiments on various datasets to verify the feasibility.\nTime-ray alignment (TRA)\nThis subsection further explores the potential of accommo-\ndating the SNN to the NeRF rendering process in a more\nnatural and novel way, where we attempt to retain the real-\nvalued input data as direct-encoding does, but do not fill the\ntemporal dimension with the duplication-based approach.\nWe first consider the MLP querying process in the ANN\nphilosophy. For an expected scene to reconstruct, the vol-\numetric parameters of sampled points, e.g., e and v in Eq.\n2, are packed as the input data with the shape of [batch, ce]\nor [batch, cv], where batch represents the sample index and\nc is the channel index of the volumetric parameters. Thus,\nthe MLP can query these data and output the corresponding\ncolor information in parallel. However, from the geometric\n\n3D voxel grids\nVoxel grid based masking\nDensity grids\nσ\nw\nα\n< λ1\n< λ2\nMask\nSpiking MLP\nσ\nrg\nb Render\nt = 1\nt = 2\nt = K\n. . . . . .\nV1\nTemporal dimension \nof  the SNN\nRay with  the \ncolor and the \ndensity values\nK\n∑\ni=1\nwiαici\nRay with  the \nsampled points\nAligned\n(a)\n(b)\nRay after the \nmasking operation\n4\n2\n6\n2\n2\n6\n5\n4\n6\n5\n8\n7\n6\n4\n5\n0\n0\n0\n0\n0\n9\n2\n1\n0\n2\n5\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n Ray1\n Ray2\n Ray3\n Ray4\n Ray5\n T1\n T2\n T3\n T4  T5\n T6  T7\n T8\n Ray2\n Ray3\n Ray4\n Ray5\n4\n2\n6\n0\n2\n2\n5\n0\n0\n0\n0\n5\n6\n4\n0\n6\n4\n0\n0\n8\n9\n2\n1\n2\n0\n5\n0\n0\n0\n0\n1\n5\n6\n0\n0\n1\n0\n0\n7\n0\n Ray1\n T1\n T2\n T3\n T4  T5\n T6  T7\n T8\n(c)\nV2\nFigure 3: Overview of the proposed SpikingNeRF. (a) The rendering process of SpikingNeRF. The whole 3D volumetric\nparameters are stored in the voxel grids. The irrelevant or unimportant samples are masked before the sMLP querying. The\nexpected scenes are rendered with the volumetric information yielded by the sMLP. (b) Alignment between the temporal\ndimension and the ray. The sMLP queries each sampled point step-by-step to yield the volumetric information. (c) Proposed\ntemporal padding (left) and temporal condensing-and-padding (right) methods. For simplification, the channel length of the\nvolumetric parameters is set to 1.\nview, the input data should be packed as [ray, pts, c], where\nray is the ray index and the pts is the index of the sampled\npoints.\nObviously, the ANN-based MLP querying process can\nnot reflect such geometric relations between the ray and the\nsampled points. Then, we consider the computation modal-\nity of SNNs. As illustrated in Eq. (10-12), SNNs naturally\nentail the temporal dimension to process the sequential sig-\nnals. This means a spiking MLP naturally accepts the input\ndata with the shape of [batch, time, c], where time is the\ntemporal index. Therefore, we can reshape the volumetric\nparameters back to [ray, pts, c], and intuitively match each\nsample along the ray to the corresponding time step:\nInputMLP := [batch, c]\n⇒[ray, pts, c]\n⇒[batch, time, c] := InputsMLP ,\n(14)\nwhich is also illustrated in 3(b). Such an alignment does not\nrequire any input data pre-process such as duplication (Zhou\net al. 2022) or Poisson generation (Garg, Chowdhury, and\nRoy 2021) as prior arts commonly do.\nTemporal condensing-and-padding (TCP)\nThe masking operation on sampled points, as illustrated\nin Preliminaries, makes the time-ray alignment intractable.\nAlthough such masking operation improves the rendering\nspeed and quality by curtailing the computation cost of re-\ndundant samples, it also causes the number of queried sam-\nples on different rays to be irregular, which indicates the re-\nshape operation of Eq. 14, i.e., shaping into a tensor, is un-\nfeasible on GPUs after the masking operation.\nTo ensure computation parallelism on GPUs, we first pro-\npose to retain the indices of those masked samples but dis-\ncard their values. As illustrated in Fig. 3(c) Left, we arrange\nboth unmasked and masked samples sequentially to the cor-\nresponding ray-indexed vector, and pad zeros to the vacant\ntensor elements. Such that, a regular-shaped input tensor is\nbuilt, making GPU training feasible. We refer to this simple\napproach as the temporal padding (TP) method.\nHowever, TP does not handle those masked samples ef-\nfectively because those padded zeros will still get involved\nin the following computation and cause the membrane po-\ntential of sMLP to decay, implicitly affecting the outcomes\nof the unmasked samples in the posterior segment of the\nray. Even for a sophisticated hardware accelerator that can\nskip those zeros, the sparse data structure still causes com-\nputation inefficiency such as imbalanced workload (Zhang\net al. 2020). To solve this issue, we design the temporal\ncondensing-and-padding (TCP) scheme, which is illustrated\nin Fig. 3(c) Right. Different from TP, TCP completely dis-\ncards the parameters and indices of the masked samples, and\nadjacently rearranges the unmasked sampled points to the\ncorresponding ray vector. For the vacant tensor elements,\nzeros are filled as TP does. Consequently, valid data is con-\ndensed to the left side of the tensor. Notably, the ray di-\nmension can be sorted according to the valid data number to\nfurther increase the density. As a result, TCP has fully elim-\n\nAlgorithm 1: Overall algorithm of the DVGO-based Spik-\ningNeRF (SpikingNeRF-D) in the rendering process.\nInput: The density and the feature voxel grids Vσ and Vf,\nthe spiking MLP sMLP(·), the view direction of the\ncamera v, the rays from the camera origin to the di-\nrections of N pixels of the expected scene R{N} =\n{r1, r2, ..., rN}, the number of the sampled points per\nray M, the ground-truth RGB C = {C1, C2, ..., CN} of\nthe expected N pixels .\nOutput: The expected RGB ˆC = { ˆC1, ˆC2, ..., ˆCN} of the\nexpected N pixels, the training loss L.\n1: The coordinates of sampled points P{N×M}\n=\n{p1,1, p1,2, ..., pN,M} ←Sample(R).\n2: α{N×M}, w{N×M} ←Weigh(P, Vσ) as in Eq. 6 and\nEq. 3.\n3: Filtered coordinates P ′ ←Mask(P, α, w) as in Eq. 8.\n4: InputMLP\n←\nExtractFeatures(P ′, Vf, v) as de-\nscribed in Eq. 7 and Eq. 2.\n5: The temporal length T ←The maximum point number\namong the batched rays.\n6: InputsMLP ←The TP or TCP transformation on\nInputMLP as described in Eq. 14 and Sec. TCP.\n7: The RGB values c{N,T } ←sMLP(InputsMLP )\n8: ˆC ←Accumulate(P ′, α, w, c) as in Eq. 9.\n9: L ←MSE(C, ˆC) as in Eq. 5\nNote: Dependent on the specific NeRF framework, the func-\ntions, e.g., Sample(·), Mask(·), may be different.\ninated the impact of the masked samples and made Spik-\ningNeRF more hardware-friendly.\nAlthough such data condensing operation can incur extra\noverhead on hardware, a regular and condensed data struc-\nture commonly brings far more benefits to efficiency, cover-\ning the cost (Zhang et al. 2020). Such benefits not only cater\nfor DNN hardware accelerators and GPUs, but also apply\nto neuromorphic hardware, as proposed and proved in PTB\n(Lee, Zhang, and Li 2022) and STELLAR (Mao et al. 2024).\nTherefore, we choose TCP as our mainly proposed method.\nOverall algorithm\nThis section summarizes the overall algorithm of SpikingN-\neRF based on the DVGO (Sun, Sun, and Chen 2022) frame-\nwork. And, the pseudo code is given in Algorithm 1.\nAs illustrated in Fig. 3(a), SpikingNeRF first establishes\nthe voxel grids filled with learnable volumetric parameters.\nIn the case of the DVGO implementation, two groups of\nvoxel grids are built as the input of Algorithm 1, which are\nthe density and the feature voxel grids. Given an expected\nscene with N pixels to render, Step 1 is to sample M points\nalong each ray shot from the camera origin to the direction of\neach pixel. With the N × M sampled points, Step 2 queries\nthe density grids to compute the weight coefficients, and\nStep 3 uses these coefficients to mask out those irrelevant\npoints. Then, Step 4 queries the feature grids for the filtered\npoints and returns each point’s volumetric parameters. Step\n5 and 6 prepare the volumetric parameters into a receivable\ndata format for sMLP with TP or TCP. Step 7 and 8 com-\npute the RGB values for the expected scene. If a backward\nprocess is required, Step 9 calculates the MSE loss between\nthe expected and the ground-truth scenes.\nNotably, the proposed methods, functioning in a plug-\nin way, can be applied to other NeRF frameworks, e.g.,\nthe SoTA TensoRF (Chen et al. 2022), and is also orthog-\nonal and applicable to those efficient NeRFs such as Fast-\nNeRF(Garbin et al. 2021) and KiloNeRF(Reiser et al. 2021).\nExperiments\nIn this section, we demonstrate the effectiveness of our pro-\nposed SpikingNeRF. A) We first build SpikingNeRF on the\nvoxel-grid based DVGO framework (Sun, Sun, and Chen\n2022), and compare the proposed TRA encoding with the\nnaive data encodings. B) We extend SpikingNeRF to the\nTensoRF framework (Chen et al. 2022) to show the flexi-\nbility of our method, and compare SpikingNeRF with the\noriginal DVGO and TensoRF along with other NeRF-based\nworks in both rendering quality and energy cost. C) We\nevaluate SpikingNeRF on the neuromorphic accelerator\nSpikeSim (Moitra et al. 2023) and SATA (Yin et al. 2022)\nand GPU to compare the hardware friendliness of the pro-\nposed TCP over TP, meanwhile showcase the energy advan-\ntage on neuromorphic accelerators over ANNs. D) we fur-\nther discuss the alignment direction issue and extensively\ncompare the merits of our edge-friendly spiking approach\nwith the classic quantization method. Note that, for text\nsaving, we defer the results of unbounded inward-facing\nand forward-facing datasets and all visualizations to the ap-\npendix, and the results of SATA evaluation is also deferred\nalong with the detailed hardware descriptions. For clarity,\nwe term the DVGO-based SpikingNeRF as SpikingNeRF-\nD and the TensoRF-based as SpikingNeRF-T. If not stated\notherwise, TCP is utilized by default.\nExperimental settings\nWe conduct experiments mainly on the four inward-facing\ndatasets, including Synthetic-NeRF(Mildenhall et al. 2021),\nSynthetic-NSVF(Liu et al. 2020), BlendedMVS(Yao et al.\n2020), and Tanks&Temples(Knapitsch et al. 2017). We refer\nto DVGO as the ANN counterpart to SpikingNeRF-D. We re-\nfer to TensoRF as the ANN counterpart to SpikingNeRF-T.\nIn terms of the energy computation, we follow the prior arts\n(Zhou et al. 2022; Horowitz 2014) to estimate the theoretical\nrendering energy cost in most of our experiments except for\nthose in Tab. 4 whose results are produced by the neuromor-\nphic accelerator SpikeSim. Very detailed implementation\nhyper-parameters, the experiment fairness declaration,\nand thorough SpikeSim evaluation details are all speci-\nfied in the appendix.\nComparisons and ablations\nComparisons with the conventional data encodings. As\ndescribed in Data encoding, we propose two naive ver-\nsions of SpikingNeRF-D that adopt two different conven-\ntional data encoding schemes: direct-encoding and Poisson-\nencoding. On the one hand, Poisson-encoding, severely los-\ning the feature information and producing at most 24.83\n\nTable 1: Comparisons with direct-encoding under different time step settings.\nMetric\nPSNR↑\nSSIM↑\nEnergy(mJ) ↓\nPSNR↑\nSSIM↑\nEnergy(mJ) ↓\nPSNR↑\nSSIM↑\nEnergy(mJ) ↓\nDirect-Encoding\nTimeStep=1\nTimeStep=2\nTimeStep=4\nSynthetic-NeRF\n31.22\n0.947\n113.03\n31.51\n0.951\n212.20\n31.55\n0.951\n436.32\nSynthetic-NSVF\n34.17\n0.969\n53.73\n34.49\n0.971\n104.05\n34.56\n0.971\n217.86\nTRA\nDynamic Time Step\nSynthetic-NeRF\n31.34\n0.949\n110.80\n31.59\n0.951\n185.78\n31.64\n0.952\n308.84\nSynthetic-NSVF\n34.33\n0.970\n56.69\n34.63\n0.972\n98.39\n34.57\n0.972\n165.17\nTRA denotes the proposed time-ray alignment with TCP.\nTable 2: Comparisons with direct-encoding on the same sampling density levels.\nDensity Level\n1 (Base)\n2\n4\nMetric\nPSNR↑\nSSIM↑\nEnergy(mJ) ↓\nPSNR↑\nSSIM↑\nEnergy(mJ) ↓\nPSNR↑\nSSIM↑\nEnergy(mJ) ↓\nDirect-Encoding\nSynthetic-NeRF\n31.22\n0.947\n113.03\n31.40\n0.949\n192.81\n31.46\n0.950\n337.98\nSynthetic-NSVF\n34.17\n0.969\n53.73\n34.45\n0.970\n94.58\n34.56\n0.971\n168.10\nTime-ray Alignment with TCP\nSynthetic-NeRF\n31.34\n0.949\n110.80\n31.59\n0.951\n185.78\n31.64\n0.952\n308.84\nSynthetic-NSVF\n34.33\n0.970\n56.69\n34.63\n0.972\n98.39\n34.57\n0.972\n165.17\nPSNR among all time-step settings and datasets, achieves\nfar-from-acceptable synthesis quality, which indicates it\ndoes not work at all. The corresponding quantitative and\nqualitative results of this ineffective scheme are deferred to\nthe appendix. On the other hand, as listed in Tab. 1, direct-\nencoding obtains good synthesis performance with only one\ntime-step, and can achieve higher PSNR as the time step in-\ncreases. Inheriting the good startup of direct-encoding, our\nproposed TRA shows better energy efficiency and render-\ning ability over direct-encoding. In Tab. 1, we change the\nTRA’s time step by adjusting the default sampling density to\ncompare with Direct-Encoding (DE) of different time steps\nsince it is unfeasible to explicitly set TRA’s time step due\nto its dynamic temporal length. Tab. 1 shows that TRA has\nbetter rendering quality under the same energy levels. We\nalso compare TRA with DE under the same sampling den-\nsities in Tab. 2 for fairness, and the outcome still holds. The\nappendix contains the full statistics of Tab. 1 and Tab. 2 for\neach specific scene, which also show TRA consistently out-\nperforms these conventional encodings. Conclusively, TRA\nexploits SNN’s temporal characteristics in 3D rendering and\nproves simple and effective.\nQuantitative comparisons with the ANN counterparts\nand other NeRFs. As shown in Tab. 3, our SpikingNeRF-\nD with TCP can achieve a 70.79% energy saving with\na 0.53 PSNR drop on average over the ANN counter-\npart. Such a trade-off between synthesis quality and en-\nergy cost is reasonable because a significant part of in-\nference is conducted with the addition operations in the\nsMLP of SpikingNeRF-D rather than the multiplication op-\nerations in the original DVGO. On the one hand, compared\nwith those methods, e.g., NeRF(Mildenhall et al. 2021),\nMip-NeRF(Barron et al. 2021), JaxNeRF(Deng, Barron, and\nSrinivasan 2020), that do not perform the masking operation,\nSpikingNeRF-D can reach orders of magnitude lower en-\nergy consumption. On the other hand, compared with those\nmethods, e.g., NSVF(Liu et al. 2020), DIVeR(Wu et al.\n2022), DVGO(Sun, Sun, and Chen 2022), TensoRF(Chen\net al. 2022), that significantly exploit the masking operation,\nSpikingNeRF-D can still obtain better energy efficiency and\ncomparable synthesis quality. Even compared with Kilo-\nNeRF(Reiser et al. 2021) that is aiming at fast rendering\n(which takes days to train), SpikingNeRF-D (that takes min-\nutes to train) still performs better. Furthermore, following\n(Alyamkin et al. 2018; Lee, Yang, and Fan 2023), we adopt\nthe analogous PSNR/Energy to further estimate the energy\nefficiency of SpikingNeRF and the ANN baselines. As listed\nin Tab. 3, SpikingNeRF-D achieves superior energy effi-\nciency among these competitors. Given Fig. 1, the supe-\nriority of our SpikingNeRF in energy efficiency is vivid.\nMoreover, SpikingNeRF-T also reduces energy consump-\ntion by 62.80% with a 0.69 PSNR drop on average. Except\nfor Tanks&Temples, SpikingNeRF-T outperforms DVGO in\nboth PSNR and energy cost. Notably, SpikingNeRF-T only\nuses two FC layers as TensoRF does. One layer is for en-\ncoding data with full precision, the other for spiking with\nbinary computation, leading to only half of the computa-\ntion burden being tackled with the addition operations. This\nexplains why SpikingNeRF-T performs slightly worse than\nSpikingNeRF-D in terms of energy reduction ratio. In con-\nclusion, these results demonstrate the effectiveness of our\nproposed SpikingNeRF in improving energy efficiency.\nQualitative comparisons. Due to text limitation, we defer\npiles of visualizations of SpikingNeRF-D rendering results\nto the appendix. Basically, SpikingNeRF-D shares the anal-\nogous issues with the ANN counterpart on texture distortion.\nAdvantages of temporal condensing on the hardware. To\ndemonstrate the advantages of the proposed temporal con-\ndensing on hardware accelerators as described in Sec. TCP,\nwe evaluate SpikingNeRF-D with TCP and TP on SpikeSim\nusing the SpikeFlow architecture. For one thing, as listed\nin Tab. 4, TCP consistently outperforms TP in both infer-\nence latency and energy overhead by a significant margin\nover the two datasets. Specifically, The gap between TCP\n\nTable 3: Comparisons with the ANN counterpart and other NeRF-based methods.\nDataset\nSynthetic-NeRF\nSynthetic-NSVF\nBlendedMVS\nTanksTemples\nMetric\nPSNR↑\nSSIM↑\nEnergy↓\n(mJ)\nP/E↑\nPSNR↑\nSSIM↑\nEnergy↓\n(mJ)\nP/E↑\nPSNR↑\nSSIM↑\nEnergy↓\n(mJ)\nP/E↑\nPSNR↑\nSSIM↑\nEnergy↓\n(mJ)\nP/E↑\nNeRF\n31.01\n0.947\n4.5e5\n6.9e-5\n30.81\n0.952\n4.5e5\n6.8e-5\n24.15\n0.828\n3.1e5\n7.8e-5\n25.78\n0.864\n1.4e6\n1.8e-5\nMip-NeRF\n33.09\n0.961\n4.5e5\n7.4e-5\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nJaxNeRF\n31.65\n0.952\n4.5e5\n7.0e-5\n-\n-\n-\n-\n-\n-\n-\n-\n27.94\n0.904\n1.4e6\n2.0e-5\nNSVF\n31.74\n0.953\n16427\n1.9e-3\n35.13\n0.979\n8864\n4.0e-3\n26.90\n0.898\n15149\n1.8e-3\n28.40\n0.900\n101443\n2.8e-4\nDIVeR\n32.32\n0.960\n343.96\n0.094\n-\n-\n-\n-\n27.25\n0.910\n548.65\n0.050\n28.18\n0.912\n1930.67\n0.015\nKiloNeRF\n31.00\n0.95\n185.12\n0.167\n33.37\n0.97\n99.89\n0.334\n27.39\n0.92\n170.71\n0.160\n28.41\n0.91\n723.79\n0.039\nDVGO*\n31.98\n0.957\n374.72\n0.085\n35.12\n0.976\n187.85\n0.187\n28.15\n0.922\n320.66\n0.088\n28.42\n0.912\n2147.86\n0.012\nTensoRF*\n33.14\n0.963\n641.17\n0.052\n36.74\n0.982\n465.09\n0.079\n-\n-\n-\n-\n28.50\n0.920\n2790.03\n0.010\nSpikingNeRF-D w/ TP\n31.34\n0.949\n111.59\n0.281\n34.34\n0.970\n57.57\n0.596\n27.80\n0.912\n97.38\n0.285\n28.00\n0.892\n483.48\n0.057\nSpikingNeRF-D w/ TCP\n31.34\n0.949\n110.80\n0.283\n34.34\n0.970\n56.69\n0.606\n27.80\n0.912\n96.37\n0.288\n28.09\n0.896\n581.04\n0.048\nSpikingNeRF-T w/ TCP\n32.45\n0.956\n240.81\n0.134\n35.76\n0.978\n149.98\n0.238\n-\n-\n-\n-\n28.09\n0.904\n1165.90\n0.024\n* denotes an ANN counterpart implemented by the official codes.\nP/E abbreviates the “PSNR/Energy”.\nTable 4: Comparisons between TCP and TP on SpikeSim.\nDataset\nSynthetic-NeRF\nSynthetic-NSVF\nSpikingNeRF-D\nw/ TCP\nw/ TP\nw/ TCP\nw/ TP\nLatency(s)↓\n26.12\n222.22\n13.37\n164.61\nEnergy+(mJ)↓\n65.78\n559.45\n33.68\n414.37\n+ denotes the energy result particularly produced by SpikeSim.\nTable 5: Comparisons with temporal flip.\nDataset\nSynthetic-NeRF\nSynthetic-NSVF\nSpikingNeRF-D\nw/o TF\nw/ TF\nw/o TF\nw/ TF\nPSNR↑\n31.34\n31.25\n34.34\n34.15\nSSIM↑\n0.949\n0.947\n0.970\n0.967\nEnergy (mJ)↓\n110.80\n116.91\n56.69\n61.08\nTF denotes temporal flip.\nand TP is about an order of magnitude in both inference\nspeed and energy cost. These results indicate TCP is sim-\nple but also effective at the inference stage. The same con-\nclusion can also be drawn from the SATA (Yin et al. 2022)\nevaluation as shown in the appendix, which means TCP can\nbenefit the sparsity-aware (event-driven) hardware as well.\nFor the other, comparing the results on SpikeSim (65nm\ntechnology, Tab. 4) with those on 45nm technology general\nhardware (Tab. 3) further demonstrates that the proposed\nSpikingNeRF-D can substantially benefit from its neuro-\nmorphic computing nature on the neuromorphic hardware,\nachieving higher energy-efficiency over ANN baselines. Ad-\nditionally, the temporal condensing will not harm the render-\ning quality at all as shown in Tab. 3.\nIn the SpikeSim evaluation, the temporal condensing oper-\nation is done off-chip. This causes on-chip computation can\nfully benefit from the dense data, thus accounting for the\nhuge performance gap between TCP and TP. But note that\ndue to the pipeline mechanism, the latency of such off-chip\noperation can be easily covered. To evaluate the substan-\ntial overhead and merits that temporal condensing can ac-\ntually bring, we showcase the training and inference time on\nSynthetic-NeRF on single A100 GPU.\n0.2\n0.0\n0.4\n0.6\n0.8\n1.0\nA100 GPU time consumption\nTrain Mins\nInfer. Secs\n61.28\n1.22\n16.74\n0.44\nIn the above figure, the orange bar is the time consumption\nof TP, while the blue one is that of TCP. Even roughly real-\nizing the temporal condensing with PyTorch can still bring\nsignificant time-saving at the rendering stage on GPU.\nDiscussion of the alignment direction. The radiance accu-\nmulation originally has no direction but SNN has, so it’s nec-\nessary to discuss the time-ray alignment direction. We pro-\nTable 6: Comparisons with Quantized NeRF (qNeRF).\nDataset\nSynthetic-NeRF\nSynthetic-NSVF\nMetric\nPSNR↑\nEnergy↓\nPSNR↑\nEnergy↓\nqNeRF\n31.24\n167.67\n34.13\n78.54\nours\n31.34\n110.80\n34.33\n56.69\npose temporal flip to empirically decide the alignment direc-\ntion since the querying direction of sMLP along the camera\nray will affect the inference outcome. Tab. 5 lists the exper-\nimental results of SpikingNeRF-D with and without tempo-\nral flip, i.e., with the consistent and the opposite directions.\nDistinctly, keeping the direction of the temporal dimension\nconsistent with that of the camera ray outperforms the op-\nposite case on the two datasets in synthesis performance and\nenergy efficiency. Therefore, the consistent alignment direc-\ntion is important in SpikingNeRF.\nExtensive comparisons with quantized ANN baselines.\nTo further demonstrate the merits of SpikingNeRF com-\npared to the quantized NeRF version, we quantize the ac-\ntivation of DVGO to spike-bit, i.e., 1-bit, with the renowned\nLSQ (Esser et al. 2020), and compare it with SpikingNeRF-\nD in Tab. 6. The results show that SpikingNeRF-D outper-\nforms the quantized ANN version on the two datasets in both\nsynthesis quality and energy consumption, indicating SNNs\ndo have an advantage over ANNs in scenario of ultra-low-\nenergy computation.\nConclusion\nThis paper proposes SpikingNeRF that accommodates the\nspiking neural network to reconstructing real 3D scenes for\nthe first time, improving energy efficiency. TRA is devel-\noped to encode sampled points, seamlessly combining the\ntemporal characteristic of SNNs with the radiance ray. TCP\nis further proposed to improve hardware friendliness. Thor-\nough experiments are conducted to prove the effectiveness.\n\nReferences\nAlyamkin, S.; Ardi, M.; Brighton, A.; Berg, A. C.; Chen,\nY.; Cheng, H.-P.; Chen, B.; Fan, Z.; Feng, C.; Fu, B.; et al.\n2018. 2018 low-power image recognition challenge. arXiv\npreprint arXiv:1810.01732.\nBarron, J. T.; Mildenhall, B.; Tancik, M.; Hedman, P.;\nMartin-Brualla, R.; and Srinivasan, P. P. 2021. Mip-nerf:\nA multiscale representation for anti-aliasing neural radiance\nfields. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, 5855–5864.\nChen, A.; Xu, Z.; Geiger, A.; Yu, J.; and Su, H. 2022. Ten-\nsorf: Tensorial radiance fields. In European Conference on\nComputer Vision, 333–350. Springer.\nCheng, X.; Hao, Y.; Xu, J.; and Xu, B. 2020. LISNN: Im-\nproving Spiking Neural Networks with Lateral Interactions\nfor Robust Object Recognition. In IJCAI, 1519–1525.\nDavies, M.; Srinivasa, N.; Lin, T.-H.; Chinya, G.; Cao, Y.;\nChoday, S. H.; Dimou, G.; Joshi, P.; Imam, N.; Jain, S.; et al.\n2018. Loihi: A neuromorphic manycore processor with on-\nchip learning. Ieee Micro, 38(1): 82–99.\nDeng, B.; Barron, J. T.; and Srinivasan, P. P. 2020.\nJaxNeRF:\nan\nefficient\nJAX\nimplementation\nof\nNeRF.\nURL http://github. com/googleresearch/google-\nresearch/tree/master/jaxnerf.\nDiehl, P. U.; and Cook, M. 2015. Unsupervised learning\nof digit recognition using spike-timing-dependent plasticity.\nFrontiers in computational neuroscience, 9: 99.\nEsser, S. K.; McKinstry, J. L.; Bablani, D.; Appuswamy, R.;\nand Modha, D. S. 2020. Learned Step Size Quantization. In\nInternational Conference on Learning Representations.\nFang, W.; Chen, Y.; Ding, J.; Yu, Z.; Masquelier, T.; Chen,\nD.; Huang, L.; Zhou, H.; Li, G.; and Tian, Y. 2023. Spiking-\nJelly: An open-source machine learning infrastructure plat-\nform for spike-based intelligence. Science Advances, 9(40):\neadi1480.\nFang, W.; Yu, Z.; Chen, Y.; Masquelier, T.; Huang, T.; and\nTian, Y. 2021. Incorporating learnable membrane time con-\nstant to enhance learning of spiking neural networks.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 2661–2671.\nGarbin, S. J.; Kowalski, M.; Johnson, M.; Shotton, J.; and\nValentin, J. 2021. Fastnerf: High-fidelity neural rendering\nat 200fps. In Proceedings of the IEEE/CVF international\nconference on computer vision, 14346–14355.\nGarg, I.; Chowdhury, S. S.; and Roy, K. 2021. Dct-snn: Us-\ning dct to distribute spatial information over time for low-\nlatency spiking neural networks.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\n4671–4680.\nHan, B.; Srinivasan, G.; and Roy, K. 2020. Rmp-snn: Resid-\nual membrane potential neuron for enabling deeper high-\naccuracy and low-latency spiking neural network. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, 13558–13567.\nHedman, P.; Srinivasan, P. P.; Mildenhall, B.; Barron, J. T.;\nand Debevec, P. 2021. Baking neural radiance fields for real-\ntime view synthesis. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, 5875–5884.\nHorowitz, M. 2014. 1.1 computing’s energy problem (and\nwhat we can do about it). In 2014 IEEE international solid-\nstate circuits conference digest of technical papers (ISSCC),\n10–14. IEEE.\nKnapitsch, A.; Park, J.; Zhou, Q.-Y.; and Koltun, V. 2017.\nTanks and temples: Benchmarking large-scale scene recon-\nstruction. ACM Transactions on Graphics (ToG), 36(4): 1–\n13.\nLee, J.-J.; Zhang, W.; and Li, P. 2022. Parallel time batch-\ning: Systolic-array acceleration of sparse spiking neural\ncomputation.\nIn 2022 IEEE International Symposium on\nHigh-Performance Computer Architecture (HPCA), 317–\n330. IEEE.\nLee, Y.; Yang, L.; and Fan, D. 2023.\nMf-nerf: Mem-\nory efficient nerf with mixed-feature hash table.\nArXiv,\nabs/2304.12587, 2: 3.\nLi, Y.; Guo, Y.; Zhang, S.; Deng, S.; Hai, Y.; and Gu, S.\n2021.\nDifferentiable Spike: Rethinking Gradient-Descent\nfor Training Spiking Neural Networks. Advances in Neu-\nral Information Processing Systems, 34.\nLiao, Z.; Zheng, Q.; Liu, Y.; and Pan, G. 2023.\nSpiking\nNeRF: Representing the Real-World Geometry by a Discon-\ntinuous Representation. arXiv preprint arXiv:2311.09077.\nLiu, L.; Gu, J.; Zaw Lin, K.; Chua, T.-S.; and Theobalt, C.\n2020. Neural sparse voxel fields. Advances in Neural Infor-\nmation Processing Systems, 33: 15651–15663.\nLiu, Y.; Peng, S.; Liu, L.; Wang, Q.; Wang, P.; Theobalt,\nC.; Zhou, X.; and Wang, W. 2022.\nNeural rays for\nocclusion-aware image-based rendering. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 7824–7833.\nMaass, W. 1997. Networks of spiking neurons: the third gen-\neration of neural network models. Neural networks, 10(9):\n1659–1671.\nMao, R.; Tang, L.; Yuan, X.; Liu, Y.; and Zhou, J. 2024. Stel-\nlar: Energy-Efficient and Low-Latency SNN Algorithm and\nHardware Co-Design with Spatiotemporal Computation. In\n2024 IEEE International Symposium on High-Performance\nComputer Architecture (HPCA), 172–185. IEEE.\nMasland, R. H. 2012.\nThe neuronal organization of the\nretina. Neuron, 76(2): 266–280.\nMax, N. 1995. Optical models for direct volume rendering.\nIEEE Transactions on Visualization and Computer Graph-\nics, 1(2): 99–108.\nMei, H.; Wang, Z.; Yang, X.; Wei, X.; and Delbruck,\nT. 2023.\nDeep polarization reconstruction with PDAVIS\nevents.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 22149–22158.\nMildenhall, B.; Srinivasan, P. P.; Tancik, M.; Barron, J. T.;\nRamamoorthi, R.; and Ng, R. 2021.\nNerf: Representing\nscenes as neural radiance fields for view synthesis. Com-\nmunications of the ACM, 65(1): 99–106.\n\nMoitra, A.; Bhattacharjee, A.; Kuang, R.; Krishnan, G.; Cao,\nY.; and Panda, P. 2023. SpikeSim: An end-to-end Compute-\nin-Memory Hardware Evaluation Tool for Benchmarking\nSpiking Neural Networks. IEEE Transactions on Computer-\nAided Design of Integrated Circuits and Systems, 1–1.\nReiser, C.; Peng, S.; Liao, Y.; and Geiger, A. 2021. Kilo-\nnerf: Speeding up neural radiance fields with thousands of\ntiny mlps. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 14335–14345.\nRoy, K.; Jaiswal, A.; and Panda, P. 2019. Towards spike-\nbased machine intelligence with neuromorphic computing.\nNature, 575(7784): 607–617.\nShrestha, S. B.; and Orchard, G. 2018. Slayer: Spike layer\nerror reassignment in time. Advances in neural information\nprocessing systems, 31.\nSun, C.; Sun, M.; and Chen, H.-T. 2022. Direct voxel grid\noptimization: Super-fast convergence for radiance fields re-\nconstruction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 5459–5469.\nW¨assle, H. 2004.\nParallel processing in the mammalian\nretina. Nature Reviews Neuroscience, 5(10): 747–757.\nWu, L.; Lee, J. Y.; Bhattad, A.; Wang, Y.-X.; and Forsyth, D.\n2022. Diver: Real-time and accurate neural radiance fields\nwith deterministic integration for volume rendering. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 16200–16209.\nWu, Y.; Deng, L.; Li, G.; Zhu, J.; Xie, Y.; and Shi, L. 2019.\nDirect training for spiking neural networks: Faster, larger,\nbetter. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 33, 1311–1318.\nYao, Y.; Luo, Z.; Li, S.; Zhang, J.; Ren, Y.; Zhou, L.; Fang,\nT.; and Quan, L. 2020. Blendedmvs: A large-scale dataset\nfor generalized multi-view stereo networks. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, 1790–1799.\nYin, R.; Moitra, A.; Bhattacharjee, A.; Kim, Y.; and Panda,\nP. 2022. Sata: Sparsity-aware training accelerator for spik-\ning neural networks. IEEE Transactions on Computer-Aided\nDesign of Integrated Circuits and Systems, 42(6): 1926–\n1938.\nZhang, J.; Dong, B.; Zhang, H.; Ding, J.; Heide, F.; Yin, B.;\nand Yang, X. 2022. Spiking transformers for event-based\nsingle object tracking. In Proceedings of the IEEE/CVF con-\nference on Computer Vision and Pattern Recognition, 8801–\n8810.\nZhang, Z.; Wang, H.; Han, S.; and Dally, W. J. 2020. Sparch:\nEfficient architecture for sparse matrix multiplication.\nIn\n2020 IEEE International Symposium on High Performance\nComputer Architecture (HPCA), 261–274. IEEE.\nZhou, Z.; Zhu, Y.; He, C.; Wang, Y.; Yan, S.; Tian, Y.; and\nYuan, L. 2022. Spikformer: When spiking neural network\nmeets transformer. arXiv preprint arXiv:2209.15425.\nZhu, L.; Wang, X.; Chang, Y.; Li, J.; Huang, T.; and Tian,\nY. 2022a. Event-based video reconstruction via potential-\nassisted spiking neural network.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 3594–3604.\nZhu, R.-J.; Zhao, Q.; and Eshraghian, J. K. 2023. Spikegpt:\nGenerative pre-trained language model with spiking neural\nnetworks. arXiv preprint arXiv:2302.13939.\nZhu, Z.; Peng, J.; Li, J.; Chen, L.; Yu, Q.; and Luo, S.\n2022b.\nSpiking graph convolutional networks.\narXiv\npreprint arXiv:2205.02767.",
    "pdf_filename": "SpikingNeRF_Making_Bio-inspired_Neural_Networks_See_through_the_Real_World.pdf"
}