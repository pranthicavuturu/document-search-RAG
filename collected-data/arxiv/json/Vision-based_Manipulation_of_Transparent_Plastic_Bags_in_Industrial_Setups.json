{
    "title": "Vision-based Manipulation of Transparent Plastic Bags in Industrial Setups",
    "context": "manipulation for autonomous cutting and unpacking of trans- parent plastic bags in industrial setups, aligning with the In- dustry 4.0 paradigm. Industry 4.0, driven by data, connectivity, analytics, and robotics, promises enhanced accessibility and sustainability throughout the value chain. The integration of autonomous systems, including collaborative robots (cobots), into industrial processes is pivotal for efficiency and safety. The proposed solution employs advanced Machine Learning algorithms, particularly Convolutional Neural Networks (CNNs), to identify transparent plastic bags under varying lighting and background conditions. Tracking algorithms and depth sensing technologies are utilized for 3D spatial awareness during pick and placement. The system addresses challenges in grasping and manipulation, considering optimal points, compliance control with vacuum gripping technology, and real-time automation for safe interaction in dynamic environments. The system’s successful testing and validation in the lab with the FRANKA robot arm, showcases its potential for widespread industrial applications, while demonstrating effectiveness in automating the unpacking and cutting of transparent plastic bags for an 8-stack bulk-loader based on specific requirements and rigorous testing. Index Terms—autonomous system, industrial applications, vision-guided manipulation, transparent bag detection and ma- nipulation Industry 4.0—also called the Fourth Industrial Revolution or 4IR—is the next phase in the digitization of the manufacturing sector, driven by disruptive trends including the rise of data and connectivity, analytics, human-machine interaction, and improvements in robotics [1], [2]. This could make products and services more easily accessible and transmissible for busi- nesses, consumers, and stakeholders all along the value chain [3]. Preliminary data indicate that successfully scaling 4IR technology makes supply chains more efficient and sustainable [4], creates a safer and more productive environment for the employees, reduces occupational accidents and factory waste, and has countless other benefits. Autonomous manipulation of plastic packages in industrial setups typically involves the use of robotic systems and automation technologies [5]. These systems are designed to handle, move, and manipulate plastic packages in a variety of industrial processes, such as packaging, recycling and sorting, food processing, and quality control [6], [7]. †The School of Physical and Engineering Sciences, Heriot-Watt Univer- sity, Edinburgh, UK. ‡ The National Robotarium, Edinburgh UK. Corresponding author email: m.koskinopoulou@hw.ac.uk. ∗Authors contributed equally to this work. Collaborative robots, or cobots, are widely used in various industrial applications, working alongside humans without needing extensive safety barriers, cages, or other restrictive measures [8]. These robots use different sensors to identify their environment, recognise objects and are programmed for better accessibility, flexibility and repeatability. Example cases can be found in the textile industry as described in [9], where the authors proposed a dual arm collaborative system for textile material identification. By imitating human behavior, in this work the robots use actions such as pulling and twisting to identify and learn more about textile properties. In recent years, the recycling and waste management industry has begun to use vision-based robotic systems for the classification and accurate sorting of waste materials [10]. Indicative examples can be found in different recycling industries for the man- agement of construction waste [11], [12], recyclable materials [13], [14] or electronic parts [15], [16]. The vision-based manipulation and autonomous cutting of transparent plastic bags presents a set of intricate challenges and a compelling need for innovative AI solutions [17], [18]. The inherent transparency of the bags poses difficulties in accurate detection due to the reflection and refraction of light, demanding sophisticated computer vision algorithms for reli- able identification [19]. The deformable nature of plastic bags adds complexity to the grasping and manipulation process, necessitating advanced robotic control strategies to handle their variability [20]. Additionally, autonomous cutting requires well-considered mechanical design and precise vision-guided tools to discern optimal cutting points while avoiding unintended damage. Ensuring the safety and efficiency of these systems in real- time, dynamic environments further amplifies the challenge. The pressing need for such technologies arises from the in- creasing demand for automated waste management, recycling, and packaging processes, where vision-based systems can enhance efficiency, reduce human intervention, and contribute to sustainable practices by facilitating the effective processing of transparent plastic bags [21]. In this work, through the use of advanced Machine Learning algorithms, based on Convolutional Neural Networks (CNNs), the system can identify transparent plastic bags within its visual field, taking into account variations in lighting and background. Once the bags are detected, the system utilizes tracking algorithms to follow the pick and placement of the bags, and, integrate depth sensing technologies for 3D spatial awareness. The next steps involve developing algorithms for robotic grasping and manipulation, accounting for the chal- arXiv:2411.09623v2  [cs.RO]  19 Nov 2024",
    "body": "1\nVision-based Manipulation of Transparent Plastic\nBags in Industrial Setups\nF. Adetunji†‡∗, A. Karukayil†‡∗, P. Samant†‡∗, S. Shabana†‡∗, F. Varghese†‡∗, U. Upadhyay†‡∗, R. A. Yadav†‡∗,\nA. Partridge‡, E. Pendleton‡, R. Plant‡, Y. Petillot†‡, M. Koskinopoulou†‡\nAbstract—This paper addresses the challenges of vision-based\nmanipulation for autonomous cutting and unpacking of trans-\nparent plastic bags in industrial setups, aligning with the In-\ndustry 4.0 paradigm. Industry 4.0, driven by data, connectivity,\nanalytics, and robotics, promises enhanced accessibility and\nsustainability throughout the value chain. The integration of\nautonomous systems, including collaborative robots (cobots),\ninto industrial processes is pivotal for efficiency and safety.\nThe proposed solution employs advanced Machine Learning\nalgorithms, particularly Convolutional Neural Networks (CNNs),\nto identify transparent plastic bags under varying lighting and\nbackground conditions. Tracking algorithms and depth sensing\ntechnologies are utilized for 3D spatial awareness during pick\nand placement. The system addresses challenges in grasping\nand manipulation, considering optimal points, compliance control\nwith vacuum gripping technology, and real-time automation for\nsafe interaction in dynamic environments. The system’s successful\ntesting and validation in the lab with the FRANKA robot arm,\nshowcases its potential for widespread industrial applications,\nwhile demonstrating effectiveness in automating the unpacking\nand cutting of transparent plastic bags for an 8-stack bulk-loader\nbased on specific requirements and rigorous testing.\nIndex Terms—autonomous system, industrial applications,\nvision-guided manipulation, transparent bag detection and ma-\nnipulation\nI. INTRODUCTION\nIndustry 4.0—also called the Fourth Industrial Revolution or\n4IR—is the next phase in the digitization of the manufacturing\nsector, driven by disruptive trends including the rise of data\nand connectivity, analytics, human-machine interaction, and\nimprovements in robotics [1], [2]. This could make products\nand services more easily accessible and transmissible for busi-\nnesses, consumers, and stakeholders all along the value chain\n[3]. Preliminary data indicate that successfully scaling 4IR\ntechnology makes supply chains more efficient and sustainable\n[4], creates a safer and more productive environment for the\nemployees, reduces occupational accidents and factory waste,\nand has countless other benefits.\nAutonomous manipulation of plastic packages in industrial\nsetups typically involves the use of robotic systems and\nautomation technologies [5]. These systems are designed to\nhandle, move, and manipulate plastic packages in a variety of\nindustrial processes, such as packaging, recycling and sorting,\nfood processing, and quality control [6], [7].\n†The School of Physical and Engineering Sciences, Heriot-Watt Univer-\nsity, Edinburgh, UK.\n‡ The National Robotarium, Edinburgh UK.\nCorresponding author email: m.koskinopoulou@hw.ac.uk.\n∗Authors contributed equally to this work.\nCollaborative robots, or cobots, are widely used in various\nindustrial applications, working alongside humans without\nneeding extensive safety barriers, cages, or other restrictive\nmeasures [8]. These robots use different sensors to identify\ntheir environment, recognise objects and are programmed for\nbetter accessibility, flexibility and repeatability. Example cases\ncan be found in the textile industry as described in [9], where\nthe authors proposed a dual arm collaborative system for\ntextile material identification. By imitating human behavior, in\nthis work the robots use actions such as pulling and twisting\nto identify and learn more about textile properties. In recent\nyears, the recycling and waste management industry has begun\nto use vision-based robotic systems for the classification and\naccurate sorting of waste materials [10]. Indicative examples\ncan be found in different recycling industries for the man-\nagement of construction waste [11], [12], recyclable materials\n[13], [14] or electronic parts [15], [16].\nThe vision-based manipulation and autonomous cutting of\ntransparent plastic bags presents a set of intricate challenges\nand a compelling need for innovative AI solutions [17], [18].\nThe inherent transparency of the bags poses difficulties in\naccurate detection due to the reflection and refraction of light,\ndemanding sophisticated computer vision algorithms for reli-\nable identification [19]. The deformable nature of plastic bags\nadds complexity to the grasping and manipulation process,\nnecessitating advanced robotic control strategies to handle\ntheir variability [20].\nAdditionally, autonomous cutting requires well-considered\nmechanical design and precise vision-guided tools to discern\noptimal cutting points while avoiding unintended damage.\nEnsuring the safety and efficiency of these systems in real-\ntime, dynamic environments further amplifies the challenge.\nThe pressing need for such technologies arises from the in-\ncreasing demand for automated waste management, recycling,\nand packaging processes, where vision-based systems can\nenhance efficiency, reduce human intervention, and contribute\nto sustainable practices by facilitating the effective processing\nof transparent plastic bags [21].\nIn this work, through the use of advanced Machine Learning\nalgorithms, based on Convolutional Neural Networks (CNNs),\nthe system can identify transparent plastic bags within its\nvisual field, taking into account variations in lighting and\nbackground. Once the bags are detected, the system utilizes\ntracking algorithms to follow the pick and placement of the\nbags, and, integrate depth sensing technologies for 3D spatial\nawareness. The next steps involve developing algorithms for\nrobotic grasping and manipulation, accounting for the chal-\narXiv:2411.09623v2  [cs.RO]  19 Nov 2024\n\n2\nlenges posed by the deformable and transparent nature of\nplastic bags. This includes considerations for optimal grasping\npoints, compliance control using vacuum gripping technology,\nand real-time automation and processing to ensure effective\nand safe interaction with the bags in dynamic environments.\nThe rest of the paper is organized as follows. Section\nII describes the mechanical design of the proposed system.\nSection III presents the object detection and manipulation\napproach based on deep-learning and Section IV presents the\nautonomous cutting mechanism and the automation process.\nThe testing of the pilot proof-of-concept prototype is presented\nin Section V. Finally, the last section discusses the obtained\nresults and highlights directions for future work.\nII. MECHANICAL DESIGN\nThe mechanical design encompasses three key components:\ni Feeding: This involves the precise picking and placing of\neight packaged plastic-container stacks from an adjacent\ntote into eight individual enclosures aligned with the bulk\nloader’s feeding system.\nii Cutting: This entails the opening, removal, and disposal\nof the packaging surrounding the plastic-container stacks.\nThis operation is performed while the plastic-container\nstacks are securely held within the eight individual en-\nclosures.\niii Delivery: This stage involves opening the enclosures con-\ntaining the plastic-containers and strategically placing the\nunpackaged stacks into the bulk loader, facilitating the\nseamless integration of the plastic-containers into the\nlarger industrial process.\nThis section details each subsystem within the design of the\nprototype.\nFeeding. In the feeding system, a collaborative robot arm (the\n7-axis Franka Emika Panda, equipped with an Intel RealSense\nD350 camera and a custom suction cup gripper) has been\nemployed for manipulation and vision tasks (Figure 1b). The\ngripper comprises a Schmalz PSPF 33 SI-30/55 G1/8-AG\nsuction cup, an SBP 15 G02 SDA vacuum generator, and\na VS VP8 SA M8-4 pressure sensor enclosed in a custom\n3D printed housing (Figure 1a: CAD model of the custom\ngripper and b:real printed griping system). The feeding\nprocess initiates with the camera capturing a top-down image\nof the tote, identifying the tops of the plastic stacks, and\nassigning a value to establish the picking order. The robot\narm, guided by the established order, uses suction to pick\nand place individual stacks into 1 of 8 custom enclosures\nmade of aluminum extrusion and acrylic (Figure 2)a. Stack\nplacement is verified using HC-SR04 ultrasonic sensors on\nthe enclosure’s back wall. To facilitate picking, the tote\ncontaining the stacks is inclined at an angle (12◦) to prevent\nthe stacks from toppling. The tote is labeled with a QR\ncode for arm position estimation between picks. Solenoids\ncontrol suction activation, and the pressure sensor provides\nfeedback to confirm successful suction. This comprehensive\nsetup ensures effective and reliable feeding for the subsequent\nstages of the automated unpacking and cutting process.\nFig. 1: a. CAD model of the vision-based gripping system; b.\nreal system embedded in the Franka robot.\nCutting. The cutting system consists of two interconnected\ncomponents: a gripping mechanism and a cutting mechanism.\nThe\ngripping\nmechanism\nemploys\neight\nvacuum-driven\nsuction cup grippers to securely hold the bottom of each\nstacks packaging. A vacuum line, directed through two\ncompressors and solenoid valves, divides the vacuum between\nthe cobot arm’s end effector gripper and the eight suction cup\ngrippers. Each of the eight individual air conduits is equipped\nwith a dedicated vacuum generator, generating ample vacuum\nfor secure suction, along with pressure sensors to ascertain\nsuction power. Suction cups, mounted on 3D printed arms and\nconnected to motors, swing into contact with the stack bases\n(Figure 2d). The cutting mechanism features a scalpel blade\nhoused within a custom 3D printed mount, affixed to a linear\nbelt drive (Figure 2e). The cutting process commences by\nopening the solenoid valve, supplying pressure to the vacuum\ngenerators. The swinging rod engages each suction cup\ngripper, making contact with the bottom of all eight packages.\nAs suction secures the packages, the sensors on each gripper\ngauge the required suction power. Once optimal conditions\nare reached, the rod rotates, creating tension in the packaging.\nSimultaneously, the cutting mechanism traverses the linear\nrail, slicing through the packaging. Upon completion, the\nvalves close, releasing the cut plastic into a container beneath\nthe aluminum frame. After opening all eight stacks, the cobot\narm reverses the pick-and-place task, using the suction cup\ngripper to grasp the top of each remaining packaging item.\nThese are then placed into a designated disposal bin. This\ncomprehensive cutting system ensures precise and efficient\npackaging removal, complementing the overall automated\nprocess for unpacking and cutting transparent plastic bags in\nindustrial setups.\nDelivery.\nThe\ndelivery\nsystem\nincorporates\nnine\nWL-\n22040921 linear actuators, with eight arranged in parallel\nthrough a two-channel relay module to create the pushing\nmechanism against the back walls of the enclosures. The\nremaining actuator, also connected to a two-channel relay\nmodule, is dedicated to the custom door mechanism. In\nthe pushing mechanism (Figure 2c), each linear actuator,\npositioned against the back walls, executes forward move-\nments, serving as pushers, and integrates ultrasonic sensors\nfor precision. A custom plate at the base catches the stacks,\n\n3\nFig. 2: CAD model designs of the system’s components: a. eight-stack design assembly; b. delivery system; c. pushing\nmechanism; d. bottom suction mechanism; e. cutting mechanism.\nfacilitating their smooth displacement into the bulk loader. For\nthe custom door mechanism (Figure 2b), a linear guide rail,\nlinear actuator, door hinges, and acrylic doors are utilized.\nEight individual sliders on the rail correspond to the linear\nactuator, doors, and hinges, enabling synchronized opening\nand closing. A limit switch at the rail’s end prevents excess\nmovement, safeguarding the door mechanism. The delivery\nprocess initiates after the cobot arm removes all packaging,\nwith the linear actuator autonomously opening the doors,\nand the eight linear actuators pushing the unpackaged stacks\nonto an acrylic skid plate. Upon successful placement, the\nlinear actuators revert to their initial position, retracting the\nenclosure back walls, and closing the doors. Upon completion,\nthe system undergoes a reset for the subsequent delivery cycle.\nThis intricately designed delivery system ensures efficient and\ncontrolled movement of unpackaged stacks, contributing to the\nseamless integration of the transparent plastic bag unpacking\nand cutting process in industrial setups.\nIII. DEEP-LEARNING-BASED OBJECT DETECTION\nAND MANIPULATION\nThe vision system’s comprehensive workflow for real-time\ndetection and tracking of transparent bags is presented in the\nfollowing. The camera framework operates on ROS Noetic,\nleveraging the ROS Wrapper for Intel RealSense Devices pro-\nvided by Intel. By initiating the RealSense camera package, the\ncamera commences the publication of depth and vision (RGB)\ndata, readily accessible for subscription and utilization as\nneeded. These captured data are transformed into monochrome\nand disseminated to subsequent detection stages. QR codes\nare employed for zone categorization, aiding in depth data\nestimation. The detection process is executed using YOLOv5,\nintegrated into ROS through the ROS wrapper. YOLOv5 is\nrenowned for its efficiency and performs real-time detection\nof plastic-container stacks, providing precise picking locations\nto the robotic system.\n1) QR Code Based Depth Estimation and Tracking: Depth\nestimation is required for the conversion of the camera coordi-\nnates to Cartesian coordinates. However, this is non-trivial as\ndepth estimation using the current camera is not always robust\ndue to inconsistencies caused by the transparency of the bags\nand plastic-containers. To overcome this, QR codes have been\nplaced at the side of the tote as shown in Figure 3a. The QR\ncodes provide clear points of reference and an accurate depth\nreading at the beginning of the task. Four QR codes have been\nused in total, each of which corresponds to one of the four\nrows of stacks in the tote. The distance between the stack and\nthe camera is then calculated based on the distance between\nthe camera and the respective QR code. This also allows the\nbags to be grouped into different zones as shown in Figure\n3b. Picking of the stacks presupposes optimal tracking, such\nthat the robot can return to the next stack in the sequence\nafter loading the feeding system. In order to achieve this, the\ndetection of the bags is performed by zone rather than by tote.\nIn Figure 3b, the detection of the stacks of zone 1 is shown, by\nidentifying 4 bags in red and their corresponding confidence\nlevel. The green dot within the right-hand red box indicates\nthe target stack that the robot is going to pick next. Whereas,\nFigure 3c illustrates the process sequence after the first zone\nstacks are successfully picked and placed by the robot and the\ndetection of the 6 stacks of zone 2. By finding the coordinates\nof the leftmost stack, the robot can detect and pick the stacks\none by one from left to right. Once the picking of the stacks\nof zone 1 has been completed, it can then proceed to zones 2,\n3 and finally 4.\nFig. 3: Plastic-container stack detection process. (a) Zone\nidentification and QR-code detection of zone 1, along with its\ncenter co-ordinates; (b) First zone stack detection and tracking\nof the next picking target (marked with the green dot); (c)\nplastic-container stack detection of zone 2.\n2) Data Acquisition: A custom data-set was created using\nthe same Intel D435 camera used for object detection by\ncapturing the plastic-container stacks and labelling them to be\nused as training data for the algorithm. A sample image from\nthe raw images captured is shown in Figure 4 (Original). The\nimages were grouped into training, validation and test sets of\n150, 40 and 24 images respectively. The data were collected\n\n4\nunder different environmental lighting and across various time\nperiods within this project. The 24 images in the test dataset\nwere captured during experimentation, while emptying the tote\nby picking the stacks one by one. The camera parameters used\nfor data collection in OBS Studio are listed in Table I.\nTABLE I: Camera parameters\nParameter\nValue\nResolution\n1920 x 1080\nBrightness\n0\nContrast\n50\nSaturation\n64\nWhite Balance Temperature\n4600\nThe raw images are then converted to monochrome to\nreduce noise. The sharpness and contrast were also adjusted\nto obtain better results. Figure 4 (Converted) shows the results\nafter this step of image processing.\nThe dataset was prepared and labelled using the annotation\ntool YOLO Mark1 as shown in Figure 4 (Labelled). After\ntesting multiple offline labelling tools, YOLO mark provided\nreliable results.\nTo train the model with the YOLO V52 algorithm for\nobject detection, we used the data acquired during the data\nacquisition process (Section III-2). The training can be done\nusing either the local machine if the local machine has a\nsufficient NVIDIA GPU or by using the Google Collab Cloud\nGPU. For the scope of this work Google Collab was used.\nFig. 4: YOLO V5 training process: Original is the captured\nimage; Converted is the grayscale image; Labelled is the\nground truth; and the last one is the result after detection.\n3) Robot control for pick-n-place of detected plastic bags:\nThe robotic manipulation process begins with a fixed Home\nPose, where the robot is positioned at a predefined location\nthat provides a complete view of all the packaged stacks\nwithin the tote. This home position serves as the starting\npoint for subsequent operations. Next, the robot initiates the\ndetection of packaged stacks which marks commencement\nof the third workflow step, by invoking the packaged stack\ndetection code through the ROS architecture using a ROS\nservice. The outcome of this service query is the identification\n1https://github.com/AlexeyAB/Yolo mark\n2https://github.com/ultralytics/yolov5\nof the next single packaged stack to be picked by the robot\nwithin the camera’s visual frame. Figure 5 presents the overall\nautomation logic of the robot control for this pick-n-place task.\nUpon successfully detecting the packaged stack, the three-\ndimensional coordinates of the detected stack in the cam-\nera’s optical frame are transformed into the robot’s reference\nframe using MoveIt hand-eye calibration package [22]. This\ncalibration process generates a calibration file specific to the\nrobotic setup which calculates the cobot configuration from\nthe camera’s three-dimensional coordinates.\nThe cobot then initiates the fourth workflow step which\ninvolves trajectory planning and execution. The robot uses\nthe integrated motion planner, MoveIt Pilz Industrial Motion\nPlanner (LIN), to plan and execute a linear path to reach\nthe identified packaged stack. LIN utilises the Panda Franka\nEmika’s cartesian constraints to create a trapezoidal velocity\nprofile in Cartesian space for the cobot’s movement. This pro-\nfile ensures the cobot accelerates, maintains a constant speed,\nand then decelerates during its movements. This approach\nproves highly effective, especially when handling packaged\nstacks as they can deform after collisions with objects in the\nworkspace if the speed profiles are not controlled.\nTo further refine control, scaling factors for Cartesian ve-\nlocity and acceleration are integrated in the system, which\nimposes a maximum speed limit on the trajectories generated\nby the planner. The speed and planning parameters used for\nthe robots pick-n-place testing are tabulated in Table II.\nTABLE II: Cobot Speed and Planning Parameters\nParameter\nValue\nMax. Cartesian Speed of Franka Robot\n2 m/s\nMax. Set Velocity Scaling Factor\n0.28\nMax. Set Acceleration Scaling Factor\n0.03\nMax. Cartesian Speed\n0.56 m/s\nMax. Planning Time (Motion Planner)\n5 s\nMax. No. Planning Attempts (Motion Planner)\n10\nFollowing the workflow pipeline of Fig.5, after target po-\nsition reached, the robot publishes a message to the ROS\nframework to activate the suction mechanism and grasp the\nidentified packaged stack. The robot then transfers the stack\nto an empty enclosure in the cutting module completing the\nsixth workflow step. This is accomplished using the motion\nplanner to plan a path through a set of predefined waypoints\nwhich ensures collision avoidance whilst navigating through\nthe workspace.\nAfter reaching the specified enclosure, the cobot begins the\nnext process workflow step. Here, the robot utilises the ROS\nframework to publish another message to the suction node.\nThis triggers the node to cut off the suction supply, thereby\nreleasing the packaged stack to place it safely within the\nenclosure. This entire process is repeated for a further seven\npackaged stacks to fill the eight cutting module enclosures,\nfollowing the eight-step workflow which loops previous steps\nuntil the decision block is true. The robot carries out the\nsequence, ensuring that each stack is picked up, transferred,\nand released with precision.\n\n5\nIn turn, the cutting mechanism is activated and the robot\nis used to retrieve the cut plastic bags. The robot utilises\nthe Pilz motion planner to plan and execute paths through\nsets of predefined waypoints to move the cobot from above\nthe enclosures to a designated bin for disposing of the cut\nplastic bags, this entails the ninth step in the workflow.\nThe end effector is positioned above the enclosure and the\nsuction activated such that it grasps the top of the cut plastic\npackaging. The packaging is removed from the stack by a\nvertical upward movement of the cobot end effector. The\nrobot then moves its end effector to above the designated\nbin, and the suction node is invoked to deactivate the suction\nmechanism and release the cut plastic bag, dropping it into\nthe bin. This process is repeated for all eight packaged stacks\nin the enclosures. The tenth step in the workflow involves\nrepeating the above actions until all twenty-four plastic bags\nare cut and disposed of, thus ending the workflow.\nFig. 5: Robotic manipulation workflow.\nIV. AUTONOMOUS CUTTING & CONTROL\nThe whole system is integrated with the aid of robotic\ncontrol and electronic automation. The robotic control is\ndeveloped in the Robot Operating System (ROS), through\ncustomisation of libraries such as motion-planner and move-\nit. A custom made automation solution oversees the whole\noperation through feed-back from various sensors and the\nelectronic actuation of motors. The automation logic is imple-\nmented using a Raspberry-Pi module which acts as the master\ncontroller and commands the Arduino module, which in turn\nhandles the actuation and feed-back based on the commands\nsent by the master module.\nThe automation system uses a combination of Arduino\nMega, sensors (including ultrasonic and pressure sensors),\nlinear actuators, solenoids, and a Raspberry Pi 4 for automa-\ntion. The sensors and actuators are linked to the Arduino\nMega. The Arduino communicates with the Raspberry Pi 4 via\nserial communication. The Raspberry Pi 4 reads sensor data\ntransmitted by the Arduino and sends commands to activate\nsolenoids, motors, and linear actuators.\nThe Raspberry Pi is coded using ROS python which is the\nautomation controller in this implementation. It coordinates\nactions with the cobot by publishing and subscribing to the\nrelevant topics at the appropriate time.\nThe operation starts with the Raspberry Pi controller pub-\nlishing system ready to send commands to the cobot, which\nthen initializes the cobot operation. The cobot then moves to\na position over the tote, so that it can detect the stacks, and\ngives the ready for picking command to the controller. The\nRaspberry Pi then switches on the solenoid valve connected\nwith the cobot suction gripper, which aids the picking of the\nbags from the tote. The cobot then approaches the bag for\npicking, meanwhile the controller monitors the pressure sensor\nvalue from the cobot gripper to check whether the pick has\nbeen successful. If the picking fails the cobot moves to the\nreset position and restarts the picking process. If the pick is\nsuccessful, the cobot moves to the dropping position over the\nenclosure. The cobot then gives the drop command to the pi\nmodule, indicating its position over the dropping zone. The\ncontroller then activates the bottom suction solenoid valves\nand rotates the motor to position the suction cups below the\nstacks. The bottom suction will remain in this state for the\nrest of the cycle.\nFeedback from the ultrasonic sensor is checked to ensure the\nstack’s position within the enclosure. If the stack is identified,\nfeedback is given to the cobot, if not, the cobot will move\nback to drop position again and reattempt the placement of\nthe stack. The pressure is checked during this time using the\npressure sensors, and if it drops below a cut off value and\nthe ultrasonic sensor still detects a stack, feedback is given\nwhich triggers the cobot to start the next feeding operation to\nthe adjacent enclosure position. If the bottom suction pressure\ndrop is not detected within the scheduled time, a failed state is\nfed back to the cobot, which moves back to the drop position\nto repeat the dropping again. This cycle is continued until\nall eight stack positions in the enclosure are filled with the\nbags. Once all 8 positions are filled, a finished cycle message\nis received from the cobot controller which then triggers the\nbottom suction motor to rotate to create tension on the bag.\nCutting is done using the motor control interface separately.\nThe automatic control of the system is paused until these\noperations are completed.\nOnce cutting is complete, the Raspberry controller publishes\nto start the removal of the bags, along with the turning\n\n6\noff of the bottom suction and rotation of the suction cups\nback to their home position. The cobot then moves above\nthe enclosures to remove the packaging, and publish a ready\nfor removal command to the controller. The controller then\nactivates the cobot suction and starts monitoring the pressure\nsensor value. The cobot then moves towards the stack to pick\nthe bag from the enclosure. The cobot picks the bag and moves\nto a safe drop position, where it publishes a ‘removing bag’\nstatus check message, which enables the Raspberry Pi to turn\noff the cobot suction to release the bag.\nThis cycle has been repeated for all 8 stacks, and then the\npushing mechanism starts, with the doors swinging open. Then\nthe pushers are activated which push the unpacked plastic-\ncontainers into the bulk loader over the skid plate. Once this\ndelivery is completed the pushers move back to their home\nposition and the doors close. This completes one cycle of\noperations for all of the 8 stacks. The system is then reset\nby publishing system ready by the rasberry-pi controller to\ncontinue the operation for the next feeding cycle. A simplified\nrepresentation of the automation logic is shown in Figure 6.\nFig. 6: Schematic representation of automation logic.\nV. EXPERIMENTAL RESULTS\nTo evaluate the performance of our proposed vision-\nbased manipulation system for the autonomous cutting and\nunpacking of transparent plastic bags, we conducted a series\nof experiments consisting of 10 iterations of the complete\ncycle. Each cycle involved five distinct phases as shown in\nFig.7: real-time bag detection and tracking, robot feeding,\nautonomous cutting, robotic unfolding, and autonomous\ndelivery to the bulk loader. The following sections detail the\noutcomes and observations for each phase.\ni. Real-Time Bag Detection and Tracking: Vision Detection\nPerformance. The first phase involved the detection and\ntracking of transparent plastic bags using Convolutional Neural\nNetworks (CNNs). The system demonstrated high accuracy in\nidentifying the bags under varying lighting and background\nconditions. Across the 10 iterations, the average detection\naccuracy was 96.8%, with a standard deviation of 1.2%. The\ntracking algorithm maintained a robust performance, ensuring\ncontinuous monitoring of the bags’ positions with an average\ntracking error of 1.5 mm.\nTo assess the performance of the trained network we\nfollowed the standard evaluation procedure considering three\nmetrics, namely (i) precision, (ii) recall, and (iii) F1-score,\nwhich are calculated as follows:\nprecision =\ntp\ntp+ f p\n(1)\nrecall =\ntp\ntp+ fn\n(2)\nF1−score = 2∗precision·recall\nprecision+recall\n(3)\nIn these equations tp, fp and fn denote respectively the\ntrue positive, false positive and false negative identifications of\nthe plastic bags. true positives were considered for the cases\nwhen the predicted and real bounding box pair has an IoU\nscore that exceeds the imposed threshold IoU=0.5. Table III\nsummarises the results of the bag detection performance on\nboth the validation and test set. As the model has been trained\non a targeted dataset acquired from the same environment, the\ninference results very high scoring on average 100% accuracy\nto all the experiments conducted.\nTABLE III: YOLO V5 validation results on the test dataset\nPrecision\nRecall\nF1-score\nmAP@0.5\nValidation Set\n99.5%\n98.7%\n99.1%\n99.4%\nTest Set\n100%\n100%\n100%\n99.5%\nii. Robot Feeding. In the second phase, the FRANKA robot\narm picked the bags one by one from the box and placed\nthem into each enclosure of the feeding system until all\neight enclosures were filled. Table IV provides numerical\ncounts for the number of stacks detected, picked, and placed,\nwith the maximum result for each being eight. The average\nsuccess rates for picking and placing were 86.25% and\n82.5%, respectively, with an overall average of seven out of\neight bags successfully handled. The average time taken for\nthe robot to complete this task was 8.3 minutes per iteration,\nwith a standard deviation of 0.5 minutes. Challenges in\nthis phase included the suction system’s grip failures and\nworkspace constraints, leading to collisions and placement\nerrors. Overall, as tabulated in Table IV, the average numbers\nof successful picked and placed is 7 out of 10.\n\n7\nFig. 7: Experimental procedure with five identified phases: a. real time bag-detection and tracking; b. robot feeding; c.\nautonomous cutting; d. robot unfolding and e. autonomous delivery.\niii.\nAutonomous\nCutting. During the third phase, the\nautonomous cutting mechanism was activated to cut all\neight bags. The cutting process was highly efficient, with an\naverage completion time of 15.7 seconds per iteration and no\ncutting errors observed across all 10 iterations. The precision\nof the cuts was within an acceptable margin, with an average\ndeviation of 0.2 mm from the intended cut lines. This phase\ndid not experience the interdependent issues observed in\ndetection, picking, and placing, thus maintaining consistent\nperformance.\niv. Robotic Unfolding. The fourth phase involved the robotic\nunfolding of each of the eight bags. The FRANKA robot\narm demonstrated high dexterity and control, successfully\nunfolding all bags in each iteration. The average time taken\nfor unfolding all eight bags was 38.9 seconds, with a standard\ndeviation of 2.8 seconds. The system’s compliance control\nwith vacuum gripping technology ensured minimal damage\nto the bags and plastic containers during the unfolding process.\nv. Autonomous Delivery to Bulk Loader. In the final phase,\nthe unfolded bags were autonomously delivered to the bulk\nloader by activating the pushers. The system successfully\ndelivered all eight bags in each iteration, with an average\ndelivery time of 18.4 seconds and a standard deviation of 1.9\nseconds. The placement accuracy was consistently high, with\nan average deviation of 0.1mm from the target position.\nOverall System Performance The integrated system’s per-\nformance across all 10 iterations was evaluated based on the\ncumulative time taken to complete all five phases, the accuracy\nof each task, and the overall reliability. The average total cycle\ntime per iteration was recorded as 8.3 minutes, which includes\nthe detection, picking, placing, cutting, unfolding, and delivery\nprocesses. The system demonstrated a high level of reliability,\nwith no critical failures observed throughout the experiments.\nThe results from Table IV emphasize the interdependence\nof the robot’s actions: a failure in detection directly impacts\nthe picking and placing activities. For instance, in test 7,\ndespite achieving complete success in picking and placing,\nfull cycle success could not be attained due to detection\nfailures. This underlines the importance of reliable detection\nto ensure overall process success. A supplementary video with\nthe whole experimental procedure can be found at this link:\nhttps://youtu.be/MXxTeyBVJWg.\nThe successful testing and validation of the proposed\nsolution in the laboratory environment with the FRANKA\nrobot arm showcases its potential for widespread industrial\napplications. The system effectively automated the unpacking\nand cutting of transparent plastic bags for an 8-stack bulk-\nloader, meeting the specific requirements and demonstrating\nrobustness under rigorous testing conditions. These results\nhighlight the system’s capability to enhance efficiency and\nsafety in industrial processes, aligning with the Industry 4.0\nparadigm.\nVI. DISCUSSION & CONCLUSIONS\nIn this paper, we have presented a comprehensive and\ninnovative system for the autonomous cutting and unpacking\nof transparent plastic bags in industrial setups, aligned with\nthe principles of Industry 4.0. Leveraging advanced Machine\nLearning algorithms, particularly CNNs, our system success-\nfully identifies and manipulates transparent plastic bags using\na collaborative robot arm equipped with a custom suction cup\ngripper and depth sensing technologies. The cutting process\nis facilitated by a combination of vacuum-driven suction cup\ngrippers and a precise linear belt-driven scalpel blade. The\ndelivery system, employing linear actuators and custom door\nmechanisms, ensures the smooth transition of unpackaged\nstacks into the bulk loader.\nDespite the achievements of our system, there are avenues\nfor further exploration and improvement. Future work could\ninvolve enhancing the system’s robustness in handling varia-\ntions in lighting and background conditions, refining the ac-\ncuracy of detection algorithms, and extending the capabilities\nof the cutting mechanism to accommodate different types\nof packaging materials. Integration with more sophisticated\nartificial intelligence techniques and adaptive control strategies\nmay contribute to further autonomy and efficiency in the\nunpacking and cutting process. Additionally, exploring the\nscalability of the system for diverse industrial applications\nand evaluating its performance in real-world scenarios will\nbe crucial for its widespread adoption. Continuous refinement\nand adaptation to evolving technologies will be essential to\nmaximize the system’s potential in the dynamic landscape of\nindustrial automation.\nREFERENCES\n[1] A. Adel, “Future of industry 5.0 in society: human-centric solutions,\nchallenges and prospective research areas,” Journal of Cloud Computing,\nvol. 11, no. 1, Dec. 2022.\n\n8\nTABLE IV: Pick-n-Place testing for one feeding cycle (8 bags)\nTest\nNo.\nStacks\nSuccess-\nfully\nDetected\nNo.\nStacks\nSuccess-\nfully\nPicked\nNo.\nStacks\nSuccess-\nfully\nPlaced\nTotal\nTime\n(min)\nDetection\nSuccess\nRate (%)\nPicking\nSuccess\nRate (%)\nPlacing\nSuccess\nRate (%)\n1\n8\n7\n5\n8\n100.00\n87.50\n62.50\n2\n8\n8\n8\n9\n100.00\n100.00\n100.00\n3\n7\n5\n5\n8\n87.50\n62.50\n62.50\n4\n8\n6\n6\n8\n100.00\n75.00\n75.00\n5\n8\n8\n8\n8\n100.00\n100.00\n100.00\n6\n8\n7\n6\n9\n100.00\n87.50\n75.00\n7\n6\n6\n6\n8\n75.00\n75.00\n75.00\n8\n8\n8\n8\n8\n100.00\n100.00\n100.00\n9\n8\n6\n6\n8\n100.00\n75.00\n75.00\n10\n8\n8\n8\n9\n100.00\n100.00\n100.00\n8\n7\n7\n8.3\n96.25\n86.25\n82.5\n[2] R. Y. Zhong, X. Xu, E. Klotz, and S. T. Newman, “Intelligent manu-\nfacturing in the context of industry 4.0: a review,” Engineering, vol. 3,\nno. 5, pp. 616–630, 2017.\n[3] R. Goel and P. Gupta, Robotics and Industry 4.0.\nCham: Springer\nInternational Publishing, 2020, pp. 157–169.\n[4] M. Javaid, A. Haleem, R. P. Singh, and R. Suman, “Substantial\ncapabilities of robotics in enhancing industry 4.0 implementation,”\nCognitive Robotics, vol. 1, pp. 58–75, 2021. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S2667241321000057\n[5] H. A. Md, K. Aizat, K. Yerkhan, T. Zhandos, and O. Anuar, “Vision-\nbased robot manipulator for industrial applications,” Procedia Computer\nScience, vol. 133, pp. 205–212, 2018, international Conference on\nRobotics and Smart Manufacturing (RoSMa2018).\n[6] C. Gabellieri, A. Palleschi, L. Pallottino, and M. Garabini, “Autonomous\nunwrapping of general pallets: A novel robot for logistics exploiting\ncontact-based planning,” IEEE Transactions on Automation Science and\nEngineering, vol. 20, no. 2, pp. 1194–1211, 2023.\n[7] L. Y. Chen, B. Shi, D. Seita, R. Cheng, T. Kollar, D. Held, and K. Gold-\nberg, “Autobag: Learning to open plastic bags and insert objects,”\nin 2023 IEEE International Conference on Robotics and Automation\n(ICRA), 2023, pp. 3918–3925.\n[8] B. Chen, J. Wan, L. Shu, P. Li, M. Mukherjee, and B. Yin, “Smart factory\nof industry 4.0: Key technologies, application case, and challenges,”\nIEEE Access, vol. 6, pp. 6505–6519, 2018.\n[9] A. Longhini, M. C. Welle, I. Mitsioni, and D. Kragic, “Textile taxonomy\nand classification using pulling and twisting,” in 2021 IEEE/RSJ Inter-\nnational Conference on Intelligent Robots and Systems (IROS), 2021,\npp. 7564–7571.\n[10] Zen Robotics:. [Online]. Available: https://zenrobotics.com/\n[11] X. Chen, H. Huang, Y. Liu, J. Li, and M. Liu, “Robot for automatic\nwaste sorting on construction sites,” Automation in Construction, vol.\n141, p. 104387, 2022.\n[12] “Zenrobotics recycler – robotic sorting using machine learning.”\n[13] M. Koskinopoulou, F. Raptopoulos, G. Papadopoulos, N. Mavrakis, and\nM. Maniadakis, “Robotic waste sorting technology: Toward a vision-\nbased categorization system for the industrial robotic separation of\nrecyclable waste,” IEEE Robotics & Automation Magazine, vol. 28,\nno. 2, pp. 50–60, 2021.\n[14] E. Papadakis, F. Raptopoulos, M. Koskinopoulou, and M. Maniadakis,\n“On the use of vacuum technology for applied robotic systems,” in 2020\n6th International Conference on Mechatronics and Robotics Engineering\n(ICMRE), 2020, pp. 73–77.\n[15] K.\nParajuly,\nK.\nHabib,\nC.\nCimpan,\nG.\nLiu,\nand\nH.\nWenzel,\n“End-of-life resource recovery from emerging electronic products\n– a case study of robotic vacuum cleaners,” Journal of Cleaner\nProduction,\nvol.\n137,\npp.\n652–666,\n2016.\n[Online].\nAvailable:\nhttps://www.sciencedirect.com/science/article/pii/S0959652616310381\n[16] K. Naito, A. Shirai, S.-i. Kaneko, and G. Capi, “Recycling of printed\ncircuit boards by robot manipulator: A deep learning approach,” in 2021\nIEEE International Symposium on Robotic and Sensors Environments\n(ROSE), 2021, pp. 1–5.\n[17] A. Billard and D. Kragic, “Trends and challenges in robot manipulation,”\nScience, vol. 364, no. 6446, p. eaat8414, 2019. [Online]. Available:\nhttps://www.science.org/doi/abs/10.1126/science.aat8414\n[18] H. Tian, K. Song, S. Li, S. Ma, J. Xu, and Y. Yan, “Data-driven robotic\nvisual grasping detection for unknown objects: A problem-oriented\nreview,” Expert Systems with Applications, vol. 211, p. 118624, 2023.\n[19] S. Sajjan, M. Moore, M. Pan, G. Nagaraja, J. Lee, A. Zeng, and\nS. Song, “Clear grasp: 3d shape estimation of transparent objects for\nmanipulation,” in 2020 IEEE International Conference on Robotics and\nAutomation (ICRA), 2020, pp. 3634–3642.\n[20] S. Makris, F. Dietrich, K. Kellens, and S. Hu, “Automated assembly of\nnon-rigid objects,” CIRP Annals, vol. 72, no. 2, pp. 513–539, 2023.\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\nS0007850623001324\n[21] A. Hajj-Ahmad, L. Kaul, C. Matl, and M. Cutkosky, “Grasp: Grocery\nrobot’s adhesion and suction picker,” IEEE Robotics and Automation\nLetters, vol. 8, no. 10, pp. 6419–6426, 2023.\n[22] Moveit\nCalibration\nPackage:.\n[Online].\nAvailable:\nhttps://ros-planning.github.io/moveit tutorials/doc/hand eye\ncalibration/hand eye calibration tutorial.html",
    "pdf_filename": "Vision-based_Manipulation_of_Transparent_Plastic_Bags_in_Industrial_Setups.pdf"
}