{
    "title": "1",
    "abstract": "manipulation for autonomous cutting and unpacking of trans- industrial applications, working alongside humans without parent plastic bags in industrial setups, aligning with the In- needing extensive safety barriers, cages, or other restrictive dustry 4.0 paradigm. Industry 4.0, driven by data, connectivity, measures [8]. These robots use different sensors to identify analytics, and robotics, promises enhanced accessibility and sustainability throughout the value chain. The integration of their environment, recognise objects and are programmed for autonomous systems, including collaborative robots (cobots), betteraccessibility,flexibilityandrepeatability.Examplecases into industrial processes is pivotal for efficiency and safety. can be found in the textile industry as described in [9], where The proposed solution employs advanced Machine Learning the authors proposed a dual arm collaborative system for algorithms,particularlyConvolutionalNeuralNetworks(CNNs), textilematerialidentification.Byimitatinghumanbehavior,in to identify transparent plastic bags under varying lighting and background conditions. Tracking algorithms and depth sensing this work the robots use actions such as pulling and twisting technologies are utilized for 3D spatial awareness during pick to identify and learn more about textile properties. In recent and placement. The system addresses challenges in grasping years,therecyclingandwastemanagementindustryhasbegun andmanipulation,consideringoptimalpoints,compliancecontrol to use vision-based robotic systems for the classification and with vacuum gripping technology, and real-time automation for accurate sorting of waste materials [10]. Indicative examples safeinteractionindynamicenvironments.Thesystem’ssuccessful testing and validation in the lab with the FRANKA robot arm, can be found in different recycling industries for the man- showcases its potential for widespread industrial applications, agement of construction waste [11], [12], recyclable materials while demonstrating effectiveness in automating the unpacking [13], [14] or electronic parts [15], [16]. andcuttingoftransparentplasticbagsforan8-stackbulk-loader The vision-based manipulation and autonomous cutting of based on specific requirements and rigorous testing. transparent plastic bags presents a set of intricate challenges Index Terms—autonomous system, industrial applications, and a compelling need for innovative AI solutions [17], [18]. vision-guided manipulation, transparent bag detection and ma- The inherent transparency of the bags poses difficulties in nipulation accurate detection due to the reflection and refraction of light, demanding sophisticated computer vision algorithms for reli- I. INTRODUCTION able identification [19]. The deformable nature of plastic bags adds complexity to the grasping and manipulation process, Industry4.0—alsocalledtheFourthIndustrialRevolutionor necessitating advanced robotic control strategies to handle 4IR—isthenextphaseinthedigitizationofthemanufacturing their variability [20]. sector, driven by disruptive trends including the rise of data Additionally, autonomous cutting requires well-considered and connectivity, analytics, human-machine interaction, and mechanical design and precise vision-guided tools to discern improvements in robotics [1], [2]. This could make products optimal cutting points while avoiding unintended damage. andservicesmoreeasilyaccessibleandtransmissibleforbusi- Ensuring the safety and efficiency of these systems in real- nesses, consumers, and stakeholders all along the value chain time, dynamic environments further amplifies the challenge. [3]. Preliminary data indicate that successfully scaling 4IR The pressing need for such technologies arises from the in- technologymakessupplychainsmoreefficientandsustainable creasingdemandforautomatedwastemanagement,recycling, [4], creates a safer and more productive environment for the and packaging processes, where vision-based systems can employees, reduces occupational accidents and factory waste, enhance efficiency, reduce human intervention, and contribute and has countless other benefits. to sustainable practices by facilitating the effective processing Autonomous manipulation of plastic packages in industrial of transparent plastic bags [21]. setups typically involves the use of robotic systems and Inthiswork,throughtheuseofadvancedMachineLearning automation technologies [5]. These systems are designed to algorithms,basedonConvolutionalNeuralNetworks(CNNs), handle, move, and manipulate plastic packages in a variety of the system can identify transparent plastic bags within its industrial processes, such as packaging, recycling and sorting, visual field, taking into account variations in lighting and food processing, and quality control [6], [7]. background. Once the bags are detected, the system utilizes †TheSchoolofPhysicalandEngineeringSciences,Heriot-WattUniver- tracking algorithms to follow the pick and placement of the sity,Edinburgh,UK. bags, and, integrate depth sensing technologies for 3D spatial ‡ TheNationalRobotarium,EdinburghUK. awareness. The next steps involve developing algorithms for Correspondingauthoremail:m.koskinopoulou@hw.ac.uk. ∗ Authorscontributedequallytothiswork. robotic grasping and manipulation, accounting for the chal- 4202 voN 91 ]OR.sc[ 2v32690.1142:viXra",
    "body": "1\nVision-based Manipulation of Transparent Plastic\nBags in Industrial Setups\nF. Adetunji†‡∗, A. Karukayil†‡∗, P. Samant†‡∗, S. Shabana†‡∗, F. Varghese†‡∗, U. Upadhyay†‡∗, R. A. Yadav†‡∗,\nA. Partridge‡, E. Pendleton‡, R. Plant‡, Y. Petillot†‡, M. Koskinopoulou†‡\nAbstract—This paper addresses the challenges of vision-based Collaborative robots, or cobots, are widely used in various\nmanipulation for autonomous cutting and unpacking of trans- industrial applications, working alongside humans without\nparent plastic bags in industrial setups, aligning with the In-\nneeding extensive safety barriers, cages, or other restrictive\ndustry 4.0 paradigm. Industry 4.0, driven by data, connectivity,\nmeasures [8]. These robots use different sensors to identify\nanalytics, and robotics, promises enhanced accessibility and\nsustainability throughout the value chain. The integration of their environment, recognise objects and are programmed for\nautonomous systems, including collaborative robots (cobots), betteraccessibility,flexibilityandrepeatability.Examplecases\ninto industrial processes is pivotal for efficiency and safety. can be found in the textile industry as described in [9], where\nThe proposed solution employs advanced Machine Learning\nthe authors proposed a dual arm collaborative system for\nalgorithms,particularlyConvolutionalNeuralNetworks(CNNs),\ntextilematerialidentification.Byimitatinghumanbehavior,in\nto identify transparent plastic bags under varying lighting and\nbackground conditions. Tracking algorithms and depth sensing this work the robots use actions such as pulling and twisting\ntechnologies are utilized for 3D spatial awareness during pick to identify and learn more about textile properties. In recent\nand placement. The system addresses challenges in grasping years,therecyclingandwastemanagementindustryhasbegun\nandmanipulation,consideringoptimalpoints,compliancecontrol\nto use vision-based robotic systems for the classification and\nwith vacuum gripping technology, and real-time automation for\naccurate sorting of waste materials [10]. Indicative examples\nsafeinteractionindynamicenvironments.Thesystem’ssuccessful\ntesting and validation in the lab with the FRANKA robot arm, can be found in different recycling industries for the man-\nshowcases its potential for widespread industrial applications, agement of construction waste [11], [12], recyclable materials\nwhile demonstrating effectiveness in automating the unpacking [13], [14] or electronic parts [15], [16].\nandcuttingoftransparentplasticbagsforan8-stackbulk-loader\nThe vision-based manipulation and autonomous cutting of\nbased on specific requirements and rigorous testing.\ntransparent plastic bags presents a set of intricate challenges\nIndex Terms—autonomous system, industrial applications, and a compelling need for innovative AI solutions [17], [18].\nvision-guided manipulation, transparent bag detection and ma-\nThe inherent transparency of the bags poses difficulties in\nnipulation\naccurate detection due to the reflection and refraction of light,\ndemanding sophisticated computer vision algorithms for reli-\nI. INTRODUCTION able identification [19]. The deformable nature of plastic bags\nadds complexity to the grasping and manipulation process,\nIndustry4.0—alsocalledtheFourthIndustrialRevolutionor\nnecessitating advanced robotic control strategies to handle\n4IR—isthenextphaseinthedigitizationofthemanufacturing\ntheir variability [20].\nsector, driven by disruptive trends including the rise of data\nAdditionally, autonomous cutting requires well-considered\nand connectivity, analytics, human-machine interaction, and\nmechanical design and precise vision-guided tools to discern\nimprovements in robotics [1], [2]. This could make products\noptimal cutting points while avoiding unintended damage.\nandservicesmoreeasilyaccessibleandtransmissibleforbusi-\nEnsuring the safety and efficiency of these systems in real-\nnesses, consumers, and stakeholders all along the value chain\ntime, dynamic environments further amplifies the challenge.\n[3]. Preliminary data indicate that successfully scaling 4IR\nThe pressing need for such technologies arises from the in-\ntechnologymakessupplychainsmoreefficientandsustainable\ncreasingdemandforautomatedwastemanagement,recycling,\n[4], creates a safer and more productive environment for the\nand packaging processes, where vision-based systems can\nemployees, reduces occupational accidents and factory waste,\nenhance efficiency, reduce human intervention, and contribute\nand has countless other benefits.\nto sustainable practices by facilitating the effective processing\nAutonomous manipulation of plastic packages in industrial\nof transparent plastic bags [21].\nsetups typically involves the use of robotic systems and\nInthiswork,throughtheuseofadvancedMachineLearning\nautomation technologies [5]. These systems are designed to\nalgorithms,basedonConvolutionalNeuralNetworks(CNNs),\nhandle, move, and manipulate plastic packages in a variety of\nthe system can identify transparent plastic bags within its\nindustrial processes, such as packaging, recycling and sorting,\nvisual field, taking into account variations in lighting and\nfood processing, and quality control [6], [7].\nbackground. Once the bags are detected, the system utilizes\n†TheSchoolofPhysicalandEngineeringSciences,Heriot-WattUniver- tracking algorithms to follow the pick and placement of the\nsity,Edinburgh,UK. bags, and, integrate depth sensing technologies for 3D spatial\n‡ TheNationalRobotarium,EdinburghUK.\nawareness. The next steps involve developing algorithms for\nCorrespondingauthoremail:m.koskinopoulou@hw.ac.uk.\n∗ Authorscontributedequallytothiswork. robotic grasping and manipulation, accounting for the chal-\n4202\nvoN\n91\n]OR.sc[\n2v32690.1142:viXra\n2\nlenges posed by the deformable and transparent nature of\nplasticbags.Thisincludesconsiderationsforoptimalgrasping\npoints,compliancecontrolusingvacuumgrippingtechnology,\nand real-time automation and processing to ensure effective\nand safe interaction with the bags in dynamic environments.\nThe rest of the paper is organized as follows. Section\nII describes the mechanical design of the proposed system.\nSection III presents the object detection and manipulation\napproach based on deep-learning and Section IV presents the\nautonomous cutting mechanism and the automation process.\nThetestingofthepilotproof-of-conceptprototypeispresented\nFig. 1: a. CAD model of the vision-based gripping system; b.\nin Section V. Finally, the last section discusses the obtained\nreal system embedded in the Franka robot.\nresults and highlights directions for future work.\nII. MECHANICALDESIGN\nThemechanicaldesignencompassesthreekeycomponents: Cutting. The cutting system consists of two interconnected\ni Feeding: This involves the precise picking and placing of components: a gripping mechanism and a cutting mechanism.\neight packaged plastic-container stacks from an adjacent The gripping mechanism employs eight vacuum-driven\ntote into eight individual enclosures aligned with the bulk suction cup grippers to securely hold the bottom of each\nloader’s feeding system. stacks packaging. A vacuum line, directed through two\nii Cutting: This entails the opening, removal, and disposal compressorsandsolenoidvalves,dividesthevacuumbetween\nof the packaging surrounding the plastic-container stacks. the cobot arm’s end effector gripper and the eight suction cup\nThis operation is performed while the plastic-container grippers. Each of the eight individual air conduits is equipped\nstacks are securely held within the eight individual en- with a dedicated vacuum generator, generating ample vacuum\nclosures. for secure suction, along with pressure sensors to ascertain\niii Delivery: This stage involves opening the enclosures con- suctionpower.Suctioncups,mountedon3Dprintedarmsand\ntaining the plastic-containers and strategically placing the connected to motors, swing into contact with the stack bases\nunpackaged stacks into the bulk loader, facilitating the (Figure 2d). The cutting mechanism features a scalpel blade\nseamless integration of the plastic-containers into the housed within a custom 3D printed mount, affixed to a linear\nlarger industrial process. belt drive (Figure 2e). The cutting process commences by\nThis section details each subsystem within the design of the opening the solenoid valve, supplying pressure to the vacuum\nprototype. generators. The swinging rod engages each suction cup\ngripper,makingcontactwiththebottomofalleightpackages.\nFeeding.Inthefeedingsystem,acollaborativerobotarm(the As suction secures the packages, the sensors on each gripper\n7-axisFrankaEmikaPanda,equippedwithanIntelRealSense gauge the required suction power. Once optimal conditions\nD350 camera and a custom suction cup gripper) has been arereached,therodrotates,creatingtensioninthepackaging.\nemployed for manipulation and vision tasks (Figure 1b). The Simultaneously, the cutting mechanism traverses the linear\ngripper comprises a Schmalz PSPF 33 SI-30/55 G1/8-AG rail, slicing through the packaging. Upon completion, the\nsuction cup, an SBP 15 G02 SDA vacuum generator, and valves close, releasing the cut plastic into a container beneath\na VS VP8 SA M8-4 pressure sensor enclosed in a custom the aluminum frame. After opening all eight stacks, the cobot\n3D printed housing (Figure 1a: CAD model of the custom arm reverses the pick-and-place task, using the suction cup\ngripper and b:real printed griping system). The feeding gripper to grasp the top of each remaining packaging item.\nprocess initiates with the camera capturing a top-down image These are then placed into a designated disposal bin. This\nof the tote, identifying the tops of the plastic stacks, and comprehensive cutting system ensures precise and efficient\nassigning a value to establish the picking order. The robot packaging removal, complementing the overall automated\narm, guided by the established order, uses suction to pick process for unpacking and cutting transparent plastic bags in\nand place individual stacks into 1 of 8 custom enclosures industrial setups.\nmade of aluminum extrusion and acrylic (Figure 2)a. Stack\nplacement is verified using HC-SR04 ultrasonic sensors on Delivery. The delivery system incorporates nine WL-\nthe enclosure’s back wall. To facilitate picking, the tote 22040921 linear actuators, with eight arranged in parallel\ncontaining the stacks is inclined at an angle (12◦) to prevent through a two-channel relay module to create the pushing\nthe stacks from toppling. The tote is labeled with a QR mechanism against the back walls of the enclosures. The\ncode for arm position estimation between picks. Solenoids remaining actuator, also connected to a two-channel relay\ncontrol suction activation, and the pressure sensor provides module, is dedicated to the custom door mechanism. In\nfeedback to confirm successful suction. This comprehensive the pushing mechanism (Figure 2c), each linear actuator,\nsetupensureseffectiveandreliablefeedingforthesubsequent positioned against the back walls, executes forward move-\nstages of the automated unpacking and cutting process. ments, serving as pushers, and integrates ultrasonic sensors\nfor precision. A custom plate at the base catches the stacks,\n3\nFig. 2: CAD model designs of the system’s components: a. eight-stack design assembly; b. delivery system; c. pushing\nmechanism; d. bottom suction mechanism; e. cutting mechanism.\nfacilitatingtheirsmoothdisplacementintothebulkloader.For placed at the side of the tote as shown in Figure 3a. The QR\nthe custom door mechanism (Figure 2b), a linear guide rail, codes provide clear points of reference and an accurate depth\nlinear actuator, door hinges, and acrylic doors are utilized. readingatthebeginningofthetask.FourQRcodeshavebeen\nEight individual sliders on the rail correspond to the linear used in total, each of which corresponds to one of the four\nactuator, doors, and hinges, enabling synchronized opening rows of stacks in the tote. The distance between the stack and\nand closing. A limit switch at the rail’s end prevents excess the camera is then calculated based on the distance between\nmovement, safeguarding the door mechanism. The delivery the camera and the respective QR code. This also allows the\nprocess initiates after the cobot arm removes all packaging, bags to be grouped into different zones as shown in Figure\nwith the linear actuator autonomously opening the doors, 3b. Picking of the stacks presupposes optimal tracking, such\nand the eight linear actuators pushing the unpackaged stacks that the robot can return to the next stack in the sequence\nonto an acrylic skid plate. Upon successful placement, the after loading the feeding system. In order to achieve this, the\nlinear actuators revert to their initial position, retracting the detectionofthebagsisperformedbyzoneratherthanbytote.\nenclosurebackwalls,andclosingthedoors.Uponcompletion, InFigure3b,thedetectionofthestacksofzone1isshown,by\nthesystemundergoesaresetforthesubsequentdeliverycycle. identifying 4 bags in red and their corresponding confidence\nThisintricatelydesigneddeliverysystemensuresefficientand level. The green dot within the right-hand red box indicates\ncontrolledmovementofunpackagedstacks,contributingtothe the target stack that the robot is going to pick next. Whereas,\nseamless integration of the transparent plastic bag unpacking Figure 3c illustrates the process sequence after the first zone\nand cutting process in industrial setups. stacksaresuccessfullypickedandplacedbytherobotandthe\ndetectionofthe6stacksofzone2.Byfindingthecoordinates\nof the leftmost stack, the robot can detect and pick the stacks\nIII. DEEP-LEARNING-BASEDOBJECTDETECTION\none by one from left to right. Once the picking of the stacks\nANDMANIPULATION\nof zone 1 has been completed, it can then proceed to zones 2,\nThe vision system’s comprehensive workflow for real-time 3 and finally 4.\ndetection and tracking of transparent bags is presented in the\nfollowing. The camera framework operates on ROS Noetic,\nleveragingtheROSWrapperforIntelRealSenseDevicespro-\nvidedbyIntel.ByinitiatingtheRealSensecamerapackage,the\ncameracommencesthepublicationofdepthandvision(RGB)\ndata, readily accessible for subscription and utilization as\nneeded.Thesecaptureddataaretransformedintomonochrome\nand disseminated to subsequent detection stages. QR codes Fig. 3: Plastic-container stack detection process. (a) Zone\nare employed for zone categorization, aiding in depth data identification and QR-code detection of zone 1, along with its\nestimation. The detection process is executed using YOLOv5, centerco-ordinates;(b)Firstzonestackdetectionandtracking\nintegrated into ROS through the ROS wrapper. YOLOv5 is of the next picking target (marked with the green dot); (c)\nrenowned for its efficiency and performs real-time detection plastic-container stack detection of zone 2.\nofplastic-containerstacks,providingprecisepickinglocations\nto the robotic system. 2) Data Acquisition: A custom data-set was created using\n1) QR Code Based Depth Estimation and Tracking: Depth the same Intel D435 camera used for object detection by\nestimationisrequiredfortheconversionofthecameracoordi- capturing the plastic-container stacks and labelling them to be\nnates to Cartesian coordinates. However, this is non-trivial as used as training data for the algorithm. A sample image from\ndepthestimationusingthecurrentcameraisnotalwaysrobust the raw images captured is shown in Figure 4 (Original). The\ndue to inconsistencies caused by the transparency of the bags images were grouped into training, validation and test sets of\nandplastic-containers.Toovercomethis,QRcodeshavebeen 150, 40 and 24 images respectively. The data were collected\n4\nunderdifferentenvironmentallightingandacrossvarioustime of the next single packaged stack to be picked by the robot\nperiods within this project. The 24 images in the test dataset withinthecamera’svisualframe.Figure5presentstheoverall\nwerecapturedduringexperimentation,whileemptyingthetote automationlogicoftherobotcontrolforthispick-n-placetask.\nbypickingthestacksonebyone.Thecameraparametersused Upon successfully detecting the packaged stack, the three-\nfor data collection in OBS Studio are listed in Table I. dimensional coordinates of the detected stack in the cam-\nera’s optical frame are transformed into the robot’s reference\nTABLE I: Camera parameters\nframe using MoveIt hand-eye calibration package [22]. This\nParameter Value calibration process generates a calibration file specific to the\nResolution 1920x1080\nrobotic setup which calculates the cobot configuration from\nBrightness 0\nContrast 50 the camera’s three-dimensional coordinates.\nSaturation 64 The cobot then initiates the fourth workflow step which\nWhiteBalanceTemperature 4600\ninvolves trajectory planning and execution. The robot uses\nthe integrated motion planner, MoveIt Pilz Industrial Motion\nThe raw images are then converted to monochrome to\nPlanner (LIN), to plan and execute a linear path to reach\nreduce noise. The sharpness and contrast were also adjusted\nthe identified packaged stack. LIN utilises the Panda Franka\ntoobtainbetterresults.Figure4(Converted)showstheresults\nEmika’s cartesian constraints to create a trapezoidal velocity\nafter this step of image processing.\nprofileinCartesianspaceforthecobot’smovement.Thispro-\nThe dataset was prepared and labelled using the annotation\nfile ensures the cobot accelerates, maintains a constant speed,\ntool YOLO Mark1 as shown in Figure 4 (Labelled). After\nand then decelerates during its movements. This approach\ntesting multiple offline labelling tools, YOLO mark provided\nproves highly effective, especially when handling packaged\nreliable results.\nstacks as they can deform after collisions with objects in the\nTo train the model with the YOLO V52 algorithm for\nworkspace if the speed profiles are not controlled.\nobject detection, we used the data acquired during the data\nTo further refine control, scaling factors for Cartesian ve-\nacquisition process (Section III-2). The training can be done\nlocity and acceleration are integrated in the system, which\nusing either the local machine if the local machine has a\nimposes a maximum speed limit on the trajectories generated\nsufficientNVIDIAGPUorbyusingtheGoogleCollabCloud\nby the planner. The speed and planning parameters used for\nGPU. For the scope of this work Google Collab was used.\nthe robots pick-n-place testing are tabulated in Table II.\nTABLE II: Cobot Speed and Planning Parameters\nParameter Value\nMax.CartesianSpeedofFrankaRobot 2m/s\nMax.SetVelocityScalingFactor 0.28\nMax.SetAccelerationScalingFactor 0.03\nMax.CartesianSpeed 0.56m/s\nMax.PlanningTime(MotionPlanner) 5s\nMax.No.PlanningAttempts(MotionPlanner) 10\nFollowing the workflow pipeline of Fig.5, after target po-\nsition reached, the robot publishes a message to the ROS\nframework to activate the suction mechanism and grasp the\nidentified packaged stack. The robot then transfers the stack\nFig. 4: YOLO V5 training process: Original is the captured\nto an empty enclosure in the cutting module completing the\nimage; Converted is the grayscale image; Labelled is the\nsixth workflow step. This is accomplished using the motion\nground truth; and the last one is the result after detection.\nplanner to plan a path through a set of predefined waypoints\nwhich ensures collision avoidance whilst navigating through\n3) Robot control for pick-n-place of detected plastic bags:\nthe workspace.\nThe robotic manipulation process begins with a fixed Home\nAfter reaching the specified enclosure, the cobot begins the\nPose, where the robot is positioned at a predefined location\nnext process workflow step. Here, the robot utilises the ROS\nthat provides a complete view of all the packaged stacks\nframework to publish another message to the suction node.\nwithin the tote. This home position serves as the starting\nThis triggers the node to cut off the suction supply, thereby\npoint for subsequent operations. Next, the robot initiates the\nreleasing the packaged stack to place it safely within the\ndetection of packaged stacks which marks commencement\nenclosure. This entire process is repeated for a further seven\nof the third workflow step, by invoking the packaged stack\npackaged stacks to fill the eight cutting module enclosures,\ndetection code through the ROS architecture using a ROS\nfollowing the eight-step workflow which loops previous steps\nservice.Theoutcomeofthisservicequeryistheidentification\nuntil the decision block is true. The robot carries out the\n1https://github.com/AlexeyAB/Yolo mark sequence, ensuring that each stack is picked up, transferred,\n2https://github.com/ultralytics/yolov5 and released with precision.\n5\nIn turn, the cutting mechanism is activated and the robot operation through feed-back from various sensors and the\nis used to retrieve the cut plastic bags. The robot utilises electronicactuationofmotors.Theautomationlogicisimple-\nthe Pilz motion planner to plan and execute paths through mentedusingaRaspberry-Pimodulewhichactsasthemaster\nsets of predefined waypoints to move the cobot from above controller and commands the Arduino module, which in turn\nthe enclosures to a designated bin for disposing of the cut handles the actuation and feed-back based on the commands\nplastic bags, this entails the ninth step in the workflow. sent by the master module.\nThe end effector is positioned above the enclosure and the The automation system uses a combination of Arduino\nsuction activated such that it grasps the top of the cut plastic Mega, sensors (including ultrasonic and pressure sensors),\npackaging. The packaging is removed from the stack by a linear actuators, solenoids, and a Raspberry Pi 4 for automa-\nvertical upward movement of the cobot end effector. The tion. The sensors and actuators are linked to the Arduino\nrobot then moves its end effector to above the designated Mega.TheArduinocommunicateswiththeRaspberryPi4via\nbin, and the suction node is invoked to deactivate the suction serial communication. The Raspberry Pi 4 reads sensor data\nmechanism and release the cut plastic bag, dropping it into transmitted by the Arduino and sends commands to activate\nthe bin. This process is repeated for all eight packaged stacks solenoids, motors, and linear actuators.\nin the enclosures. The tenth step in the workflow involves The Raspberry Pi is coded using ROS python which is the\nrepeating the above actions until all twenty-four plastic bags automation controller in this implementation. It coordinates\nare cut and disposed of, thus ending the workflow. actions with the cobot by publishing and subscribing to the\nrelevant topics at the appropriate time.\nThe operation starts with the Raspberry Pi controller pub-\nlishing system ready to send commands to the cobot, which\nthen initializes the cobot operation. The cobot then moves to\na position over the tote, so that it can detect the stacks, and\ngives the ready for picking command to the controller. The\nRaspberry Pi then switches on the solenoid valve connected\nwith the cobot suction gripper, which aids the picking of the\nbags from the tote. The cobot then approaches the bag for\npicking,meanwhilethecontrollermonitorsthepressuresensor\nvalue from the cobot gripper to check whether the pick has\nbeen successful. If the picking fails the cobot moves to the\nreset position and restarts the picking process. If the pick is\nsuccessful, the cobot moves to the dropping position over the\nenclosure. The cobot then gives the drop command to the pi\nmodule, indicating its position over the dropping zone. The\ncontroller then activates the bottom suction solenoid valves\nand rotates the motor to position the suction cups below the\nstacks. The bottom suction will remain in this state for the\nrest of the cycle.\nFeedbackfromtheultrasonicsensorischeckedtoensurethe\nstack’s position within the enclosure. If the stack is identified,\nfeedback is given to the cobot, if not, the cobot will move\nback to drop position again and reattempt the placement of\nthe stack. The pressure is checked during this time using the\npressure sensors, and if it drops below a cut off value and\nthe ultrasonic sensor still detects a stack, feedback is given\nwhich triggers the cobot to start the next feeding operation to\ntheadjacentenclosureposition.Ifthebottomsuctionpressure\ndropisnotdetectedwithinthescheduledtime,afailedstateis\nfed back to the cobot, which moves back to the drop position\nto repeat the dropping again. This cycle is continued until\nFig. 5: Robotic manipulation workflow. all eight stack positions in the enclosure are filled with the\nbags. Once all 8 positions are filled, a finished cycle message\nis received from the cobot controller which then triggers the\nIV. AUTONOMOUSCUTTING&CONTROL bottom suction motor to rotate to create tension on the bag.\nThe whole system is integrated with the aid of robotic Cutting is done using the motor control interface separately.\ncontrol and electronic automation. The robotic control is The automatic control of the system is paused until these\ndeveloped in the Robot Operating System (ROS), through operations are completed.\ncustomisation of libraries such as motion-planner and move- Oncecuttingiscomplete,theRaspberrycontrollerpublishes\nit. A custom made automation solution oversees the whole to start the removal of the bags, along with the turning\n6\noff of the bottom suction and rotation of the suction cups outcomes and observations for each phase.\nback to their home position. The cobot then moves above\nthe enclosures to remove the packaging, and publish a ready i.Real-TimeBagDetectionandTracking:VisionDetection\nfor removal command to the controller. The controller then Performance. The first phase involved the detection and\nactivates the cobot suction and starts monitoring the pressure trackingoftransparentplasticbagsusingConvolutionalNeural\nsensor value. The cobot then moves towards the stack to pick Networks(CNNs).Thesystemdemonstratedhighaccuracyin\nthebagfromtheenclosure.Thecobotpicksthebagandmoves identifying the bags under varying lighting and background\nto a safe drop position, where it publishes a ‘removing bag’ conditions. Across the 10 iterations, the average detection\nstatus check message, which enables the Raspberry Pi to turn accuracy was 96.8%, with a standard deviation of 1.2%. The\noff the cobot suction to release the bag. tracking algorithm maintained a robust performance, ensuring\nThis cycle has been repeated for all 8 stacks, and then the continuous monitoring of the bags’ positions with an average\npushingmechanismstarts,withthedoorsswingingopen.Then tracking error of 1.5 mm.\nthe pushers are activated which push the unpacked plastic- To assess the performance of the trained network we\ncontainers into the bulk loader over the skid plate. Once this followed the standard evaluation procedure considering three\ndelivery is completed the pushers move back to their home metrics, namely (i) precision, (ii) recall, and (iii) F1-score,\nposition and the doors close. This completes one cycle of which are calculated as follows:\noperations for all of the 8 stacks. The system is then reset\ntp\nby publishing system ready by the rasberry-pi controller to precision= (1)\ntp+fp\ncontinuetheoperationforthenextfeedingcycle.Asimplified\nrepresentation of the automation logic is shown in Figure 6. tp\nrecall= (2)\ntp+fn\nprecision·recall\nF1−score=2∗ (3)\nprecision+recall\nIn these equations tp, fp and fn denote respectively the\ntruepositive,falsepositiveandfalsenegativeidentificationsof\nthe plastic bags. true positives were considered for the cases\nwhen the predicted and real bounding box pair has an IoU\nscore that exceeds the imposed threshold IoU=0.5. Table III\nsummarises the results of the bag detection performance on\nboththevalidationandtestset.Asthemodelhasbeentrained\nonatargeteddatasetacquiredfromthesameenvironment,the\ninference results very high scoring on average 100% accuracy\nto all the experiments conducted.\nTABLE III: YOLO V5 validation results on the test dataset\nPrecision Recall F1-score mAP@0.5\nValidationSet\n99.5% 98.7% 99.1% 99.4%\nTestSet\n100% 100% 100% 99.5%\nii. Robot Feeding. In the second phase, the FRANKA robot\narm picked the bags one by one from the box and placed\nthem into each enclosure of the feeding system until all\neight enclosures were filled. Table IV provides numerical\nFig. 6: Schematic representation of automation logic.\ncounts for the number of stacks detected, picked, and placed,\nwith the maximum result for each being eight. The average\nsuccess rates for picking and placing were 86.25% and\nV. EXPERIMENTALRESULTS\n82.5%, respectively, with an overall average of seven out of\nTo evaluate the performance of our proposed vision- eight bags successfully handled. The average time taken for\nbased manipulation system for the autonomous cutting and the robot to complete this task was 8.3 minutes per iteration,\nunpacking of transparent plastic bags, we conducted a series with a standard deviation of 0.5 minutes. Challenges in\nof experiments consisting of 10 iterations of the complete this phase included the suction system’s grip failures and\ncycle. Each cycle involved five distinct phases as shown in workspace constraints, leading to collisions and placement\nFig.7: real-time bag detection and tracking, robot feeding, errors. Overall, as tabulated in Table IV, the average numbers\nautonomous cutting, robotic unfolding, and autonomous of successful picked and placed is 7 out of 10.\ndelivery to the bulk loader. The following sections detail the\n7\nFig. 7: Experimental procedure with five identified phases: a. real time bag-detection and tracking; b. robot feeding; c.\nautonomous cutting; d. robot unfolding and e. autonomous delivery.\niii. Autonomous Cutting. During the third phase, the solution in the laboratory environment with the FRANKA\nautonomous cutting mechanism was activated to cut all robot arm showcases its potential for widespread industrial\neight bags. The cutting process was highly efficient, with an applications. The system effectively automated the unpacking\naverage completion time of 15.7 seconds per iteration and no and cutting of transparent plastic bags for an 8-stack bulk-\ncutting errors observed across all 10 iterations. The precision loader, meeting the specific requirements and demonstrating\nof the cuts was within an acceptable margin, with an average robustness under rigorous testing conditions. These results\ndeviation of 0.2 mm from the intended cut lines. This phase highlight the system’s capability to enhance efficiency and\ndid not experience the interdependent issues observed in safety in industrial processes, aligning with the Industry 4.0\ndetection, picking, and placing, thus maintaining consistent paradigm.\nperformance.\nVI. DISCUSSION&CONCLUSIONS\niv. Robotic Unfolding. The fourth phase involved the robotic\nunfolding of each of the eight bags. The FRANKA robot In this paper, we have presented a comprehensive and\narm demonstrated high dexterity and control, successfully innovative system for the autonomous cutting and unpacking\nunfolding all bags in each iteration. The average time taken of transparent plastic bags in industrial setups, aligned with\nforunfoldingalleightbagswas38.9seconds,withastandard the principles of Industry 4.0. Leveraging advanced Machine\ndeviation of 2.8 seconds. The system’s compliance control Learning algorithms, particularly CNNs, our system success-\nwith vacuum gripping technology ensured minimal damage fully identifies and manipulates transparent plastic bags using\ntothebagsandplasticcontainersduringtheunfoldingprocess. a collaborative robot arm equipped with a custom suction cup\ngripper and depth sensing technologies. The cutting process\nv. Autonomous Delivery to Bulk Loader. In the final phase, is facilitated by a combination of vacuum-driven suction cup\nthe unfolded bags were autonomously delivered to the bulk grippers and a precise linear belt-driven scalpel blade. The\nloader by activating the pushers. The system successfully delivery system, employing linear actuators and custom door\ndelivered all eight bags in each iteration, with an average mechanisms, ensures the smooth transition of unpackaged\ndelivery time of 18.4 seconds and a standard deviation of 1.9 stacks into the bulk loader.\nseconds. The placement accuracy was consistently high, with Despite the achievements of our system, there are avenues\nan average deviation of 0.1mm from the target position. for further exploration and improvement. Future work could\ninvolve enhancing the system’s robustness in handling varia-\nOverall System Performance The integrated system’s per- tions in lighting and background conditions, refining the ac-\ncuracy of detection algorithms, and extending the capabilities\nformance across all 10 iterations was evaluated based on the\nof the cutting mechanism to accommodate different types\ncumulativetimetakentocompleteallfivephases,theaccuracy\nof packaging materials. Integration with more sophisticated\nofeachtask,andtheoverallreliability.Theaveragetotalcycle\nartificialintelligencetechniquesandadaptivecontrolstrategies\ntimeperiterationwasrecordedas8.3minutes,whichincludes\nmay contribute to further autonomy and efficiency in the\nthedetection,picking,placing,cutting,unfolding,anddelivery\nunpacking and cutting process. Additionally, exploring the\nprocesses.Thesystemdemonstratedahighlevelofreliability,\nscalability of the system for diverse industrial applications\nwith no critical failures observed throughout the experiments.\nand evaluating its performance in real-world scenarios will\nThe results from Table IV emphasize the interdependence\nbe crucial for its widespread adoption. Continuous refinement\nof the robot’s actions: a failure in detection directly impacts\nand adaptation to evolving technologies will be essential to\nthe picking and placing activities. For instance, in test 7,\nmaximize the system’s potential in the dynamic landscape of\ndespite achieving complete success in picking and placing,\nindustrial automation.\nfull cycle success could not be attained due to detection\nfailures. This underlines the importance of reliable detection\ntoensureoverallprocesssuccess.Asupplementaryvideowith REFERENCES\nthe whole experimental procedure can be found at this link:\n[1] A. Adel, “Future of industry 5.0 in society: human-centric solutions,\nhttps://youtu.be/MXxTeyBVJWg.\nchallengesandprospectiveresearchareas,”JournalofCloudComputing,\nThe successful testing and validation of the proposed vol.11,no.1,Dec.2022.\n8\nTABLE IV: Pick-n-Place testing for one feeding cycle (8 bags)\nTest No. No. No. Total Detection Picking Placing\nStacks Stacks Stacks Time Success Success Success\nSuccess- Success- Success- (min) Rate(%) Rate(%) Rate(%)\nfully fully fully\nDetected Picked Placed\n1 8 7 5 8 100.00 87.50 62.50\n2 8 8 8 9 100.00 100.00 100.00\n3 7 5 5 8 87.50 62.50 62.50\n4 8 6 6 8 100.00 75.00 75.00\n5 8 8 8 8 100.00 100.00 100.00\n6 8 7 6 9 100.00 87.50 75.00\n7 6 6 6 8 75.00 75.00 75.00\n8 8 8 8 8 100.00 100.00 100.00\n9 8 6 6 8 100.00 75.00 75.00\n10 8 8 8 9 100.00 100.00 100.00\n8 7 7 8.3 96.25 86.25 82.5\n[2] R. Y. Zhong, X. Xu, E. Klotz, and S. T. Newman, “Intelligent manu- [18] H.Tian,K.Song,S.Li,S.Ma,J.Xu,andY.Yan,“Data-drivenrobotic\nfacturinginthecontextofindustry4.0:areview,”Engineering,vol.3, visual grasping detection for unknown objects: A problem-oriented\nno.5,pp.616–630,2017. review,”ExpertSystemswithApplications,vol.211,p.118624,2023.\n[3] R. Goel and P. Gupta, Robotics and Industry 4.0. Cham: Springer [19] S. Sajjan, M. Moore, M. Pan, G. Nagaraja, J. Lee, A. Zeng, and\nInternationalPublishing,2020,pp.157–169. S. Song, “Clear grasp: 3d shape estimation of transparent objects for\n[4] M. Javaid, A. Haleem, R. P. Singh, and R. Suman, “Substantial manipulation,”in2020IEEEInternationalConferenceonRoboticsand\ncapabilities of robotics in enhancing industry 4.0 implementation,” Automation(ICRA),2020,pp.3634–3642.\nCognitive Robotics, vol. 1, pp. 58–75, 2021. [Online]. Available: [20] S.Makris,F.Dietrich,K.Kellens,andS.Hu,“Automatedassemblyof\nhttps://www.sciencedirect.com/science/article/pii/S2667241321000057 non-rigid objects,” CIRP Annals, vol. 72, no. 2, pp. 513–539, 2023.\n[5] H. A. Md, K. Aizat, K. Yerkhan, T. Zhandos, and O. Anuar, “Vision- [Online]. Available: https://www.sciencedirect.com/science/article/pii/\nbasedrobotmanipulatorforindustrialapplications,”ProcediaComputer S0007850623001324\nScience, vol. 133, pp. 205–212, 2018, international Conference on [21] A. Hajj-Ahmad, L. Kaul, C. Matl, and M. Cutkosky, “Grasp: Grocery\nRoboticsandSmartManufacturing(RoSMa2018). robot’s adhesion and suction picker,” IEEE Robotics and Automation\nLetters,vol.8,no.10,pp.6419–6426,2023.\n[6] C.Gabellieri,A.Palleschi,L.Pallottino,andM.Garabini,“Autonomous\n[22] Moveit Calibration Package:. [Online]. Available:\nunwrapping of general pallets: A novel robot for logistics exploiting\nhttps://ros-planning.github.io/moveit tutorials/doc/hand eye\ncontact-basedplanning,”IEEETransactionsonAutomationScienceand\ncalibration/hand eye calibration tutorial.html\nEngineering,vol.20,no.2,pp.1194–1211,2023.\n[7] L.Y.Chen,B.Shi,D.Seita,R.Cheng,T.Kollar,D.Held,andK.Gold-\nberg, “Autobag: Learning to open plastic bags and insert objects,”\nin 2023 IEEE International Conference on Robotics and Automation\n(ICRA),2023,pp.3918–3925.\n[8] B.Chen,J.Wan,L.Shu,P.Li,M.Mukherjee,andB.Yin,“Smartfactory\nof industry 4.0: Key technologies, application case, and challenges,”\nIEEEAccess,vol.6,pp.6505–6519,2018.\n[9] A.Longhini,M.C.Welle,I.Mitsioni,andD.Kragic,“Textiletaxonomy\nandclassificationusingpullingandtwisting,”in2021IEEE/RSJInter-\nnational Conference on Intelligent Robots and Systems (IROS), 2021,\npp.7564–7571.\n[10] ZenRobotics:.[Online].Available:https://zenrobotics.com/\n[11] X. Chen, H. Huang, Y. Liu, J. Li, and M. Liu, “Robot for automatic\nwaste sorting on construction sites,” Automation in Construction, vol.\n141,p.104387,2022.\n[12] “Zenroboticsrecycler–roboticsortingusingmachinelearning.”\n[13] M.Koskinopoulou,F.Raptopoulos,G.Papadopoulos,N.Mavrakis,and\nM. Maniadakis, “Robotic waste sorting technology: Toward a vision-\nbased categorization system for the industrial robotic separation of\nrecyclable waste,” IEEE Robotics & Automation Magazine, vol. 28,\nno.2,pp.50–60,2021.\n[14] E.Papadakis,F.Raptopoulos,M.Koskinopoulou,andM.Maniadakis,\n“Ontheuseofvacuumtechnologyforappliedroboticsystems,”in2020\n6thInternationalConferenceonMechatronicsandRoboticsEngineering\n(ICMRE),2020,pp.73–77.\n[15] K. Parajuly, K. Habib, C. Cimpan, G. Liu, and H. Wenzel,\n“End-of-life resource recovery from emerging electronic products\n– a case study of robotic vacuum cleaners,” Journal of Cleaner\nProduction, vol. 137, pp. 652–666, 2016. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S0959652616310381\n[16] K. Naito, A. Shirai, S.-i. Kaneko, and G. Capi, “Recycling of printed\ncircuitboardsbyrobotmanipulator:Adeeplearningapproach,”in2021\nIEEE International Symposium on Robotic and Sensors Environments\n(ROSE),2021,pp.1–5.\n[17] A.BillardandD.Kragic,“Trendsandchallengesinrobotmanipulation,”\nScience, vol. 364, no. 6446, p. eaat8414, 2019. [Online]. Available:\nhttps://www.science.org/doi/abs/10.1126/science.aat8414",
    "pdf_filename": "Vision-based_Manipulation_of_Transparent_Plastic_Bags_in_Industrial_Setups.pdf"
}