{
    "title": "Designing Multi-layered Runtime Guardrails for Foundation Model Based Agents Swiss Cheese Model for",
    "context": "tionizing application development across various domains. How- ever, their rapidly growing capabilities and autonomy have raised significant concerns about AI safety. Researchers are exploring better ways to design guardrails to ensure that the runtime behavior of FM-based agents remains within specific bound- aries. Nevertheless, designing effective runtime guardrails is challenging due to the agents’ autonomous and non-deterministic behavior. The involvement of multiple pipeline stages and agent artifacts, such as goals, plans, tools, at runtime further com- plicates these issues. Addressing these challenges at runtime requires multi-layered guardrails that operate effectively at various levels of the agent architecture. Thus, in this paper, we present a comprehensive taxonomy of runtime guardrails for FM-based agents to identify the key quality attributes for guardrails and design dimensions based on the results of a systematic literature review. Inspired by the Swiss Cheese Model, we also propose a reference architecture for designing multi- layered runtime guardrails for FM-based agents, which includes three dimensions: quality attributes, pipelines, and artifacts. The proposed taxonomy and reference architecture provide concrete and robust guidance for researchers and practitioners to build AI-safety-by-design from a software architecture perspective. Index Terms—Foundation Model, Large Language Models (LLM), Agent, Guardrails, Swiss Cheese Model, Responsible AI, AI Safety, Software Architecture, Taxonomy A Foundation Model (FM) is a large-scale machine learning model pre-trained on massive amounts of data using self- supervision at scale. These models are highly versatile and can adapt to a wide range of downstream tasks [1]. The term ‘foundation’ reflects their role as the fundamental base upon which many specialized models/systems are built. However, it is important to recognize that FM-based systems exhibit inherent limitations, particularly when handling complex tasks. Users are often required to provide detailed instructions, which can lead to inefficiencies and is prone to error. An FM-based agent is an autonomous system that is capable of perceiving context, reasoning, planning, and executing workflows by interacting with FMs, external tools, knowledge bases, and other agents to achieve human goals [3]. There has been extensive interest in FM-based agent development recently due to their huge potential to enhance productivity across various domains. However, their autonomous and non- deterministic behavior introduce substantial concerns regard- ing AI safety [2, 78], such as generating harmful or offensive content, producing dangerous or unintended outcomes, spread- ing disinformation and misinformation, etc [77]. To address these challenges, effective runtime guardrails are key to ensure that agents behave in a safe and responsible man- ner [2]. In this context, guardrails are mechanisms integrated into the agent’s architecture to safeguard its behavior during runtime, preventing undesirable or unsafe behaviors [78]. There have been some initial efforts on runtime guardrails such as input filtering [1, 9], output modification [10, 11], adaptive fail-safes [12, 13], real-time monitoring and detection [14–17], and continuous output validation [18–20]. However, the existing guardrail approaches primarily ad- dress functional correctness, often overlooking quality at- tributes of FM-based agents, such as customizability and interpretability. Most importantly, these approaches mainly focus on individual single-layered guardrails that are narrowly applied to specific agent artifacts, such as prompts or FM outputs, which are insufficient to manage the inherent auton- omy and non-deterministic nature of FM-agents. If any single guardrail fails, the associated risks may bypass it, potentially impacting the final results of the FM-based agent. Therefore, in this paper, we first present a comprehensive taxonomy to categorize runtime guardrails from a software architecture perspective, based on the results of a systematic literature review. The taxonomy comprises two primary cate- gories: quality attributes and design options. Inspired by Swiss Cheese Model [76], we also propose novel reference architec- ture for designing multi-layered guardrails of FM-based agents which include three dimensions: quality attributes, pipelines, and artifacts. Each guardrail layer can be designed to protect specific quality attributes (such as privacy and security), spe- cific pipeline stages (such as prompts, intermediate results and final results), as well as agent artifacts (such as goals, plans, and tools). While each layer may have its own weaknesses (i.e. holes in the Swiss Cheese Model), the combined layers create a a robust defense against failures. This reference architecture provides concrete guidance for researchers and practitioners, enabling AI-safety-by-design from a software architecture perspective. arXiv:2408.02205v3  [cs.SE]  19 Nov 2024",
    "body": "Designing Multi-layered Runtime Guardrails for\nFoundation Model Based Agents: Swiss Cheese\nModel for AI Safety by Design\nMd Shamsujjoha, Qinghua Lu, Dehai Zhao, Liming Zhu\nCSIRO’s Data61, Australia\nEmail: {md.shamsujjoha, qinghua.lu, dehai.zhao, liming.zhu}@data61.csiro.au\nAbstract—Foundation Model (FM)-based agents are revolu-\ntionizing application development across various domains. How-\never, their rapidly growing capabilities and autonomy have raised\nsignificant concerns about AI safety. Researchers are exploring\nbetter ways to design guardrails to ensure that the runtime\nbehavior of FM-based agents remains within specific bound-\naries. Nevertheless, designing effective runtime guardrails is\nchallenging due to the agents’ autonomous and non-deterministic\nbehavior. The involvement of multiple pipeline stages and agent\nartifacts, such as goals, plans, tools, at runtime further com-\nplicates these issues. Addressing these challenges at runtime\nrequires multi-layered guardrails that operate effectively at\nvarious levels of the agent architecture. Thus, in this paper,\nwe present a comprehensive taxonomy of runtime guardrails\nfor FM-based agents to identify the key quality attributes for\nguardrails and design dimensions based on the results of a\nsystematic literature review. Inspired by the Swiss Cheese Model,\nwe also propose a reference architecture for designing multi-\nlayered runtime guardrails for FM-based agents, which includes\nthree dimensions: quality attributes, pipelines, and artifacts. The\nproposed taxonomy and reference architecture provide concrete\nand robust guidance for researchers and practitioners to build\nAI-safety-by-design from a software architecture perspective.\nIndex Terms—Foundation Model, Large Language Models\n(LLM), Agent, Guardrails, Swiss Cheese Model, Responsible AI,\nAI Safety, Software Architecture, Taxonomy\nI. INTRODUCTION\nA Foundation Model (FM) is a large-scale machine learning\nmodel pre-trained on massive amounts of data using self-\nsupervision at scale. These models are highly versatile and\ncan adapt to a wide range of downstream tasks [1]. The term\n‘foundation’ reflects their role as the fundamental base upon\nwhich many specialized models/systems are built. However,\nit is important to recognize that FM-based systems exhibit\ninherent limitations, particularly when handling complex tasks.\nUsers are often required to provide detailed instructions, which\ncan lead to inefficiencies and is prone to error.\nAn FM-based agent is an autonomous system that is capable\nof perceiving context, reasoning, planning, and executing\nworkflows by interacting with FMs, external tools, knowledge\nbases, and other agents to achieve human goals [3]. There\nhas been extensive interest in FM-based agent development\nrecently due to their huge potential to enhance productivity\nacross various domains. However, their autonomous and non-\ndeterministic behavior introduce substantial concerns regard-\ning AI safety [2, 78], such as generating harmful or offensive\ncontent, producing dangerous or unintended outcomes, spread-\ning disinformation and misinformation, etc [77].\nTo address these challenges, effective runtime guardrails are\nkey to ensure that agents behave in a safe and responsible man-\nner [2]. In this context, guardrails are mechanisms integrated\ninto the agent’s architecture to safeguard its behavior during\nruntime, preventing undesirable or unsafe behaviors [78].\nThere have been some initial efforts on runtime guardrails such\nas input filtering [1, 9], output modification [10, 11], adaptive\nfail-safes [12, 13], real-time monitoring and detection [14–17],\nand continuous output validation [18–20].\nHowever, the existing guardrail approaches primarily ad-\ndress functional correctness, often overlooking quality at-\ntributes of FM-based agents, such as customizability and\ninterpretability. Most importantly, these approaches mainly\nfocus on individual single-layered guardrails that are narrowly\napplied to specific agent artifacts, such as prompts or FM\noutputs, which are insufficient to manage the inherent auton-\nomy and non-deterministic nature of FM-agents. If any single\nguardrail fails, the associated risks may bypass it, potentially\nimpacting the final results of the FM-based agent.\nTherefore, in this paper, we first present a comprehensive\ntaxonomy to categorize runtime guardrails from a software\narchitecture perspective, based on the results of a systematic\nliterature review. The taxonomy comprises two primary cate-\ngories: quality attributes and design options. Inspired by Swiss\nCheese Model [76], we also propose novel reference architec-\nture for designing multi-layered guardrails of FM-based agents\nwhich include three dimensions: quality attributes, pipelines,\nand artifacts. Each guardrail layer can be designed to protect\nspecific quality attributes (such as privacy and security), spe-\ncific pipeline stages (such as prompts, intermediate results and\nfinal results), as well as agent artifacts (such as goals, plans,\nand tools). While each layer may have its own weaknesses\n(i.e. holes in the Swiss Cheese Model), the combined layers\ncreate a a robust defense against failures. This reference\narchitecture provides concrete guidance for researchers and\npractitioners, enabling AI-safety-by-design from a software\narchitecture perspective.\narXiv:2408.02205v3  [cs.SE]  19 Nov 2024\n\nThe rest of the paper is organized as follows. Section II\ndiscuss the related works and background study required\nto understand the proposed works. The research methods\nemployed in this study are described in Section III, including\na brief discussion of the research protocol used for systematic\nliterature review. The proposed taxonomy of guardrails is\npresented in Section IV,developed based on the results of a\nsystematic literature review. The taxonomy is organized into\nguardrails quality attributes and design options from different\nperspectives. Section V proposes the reference architecture\nfor multi-layered runtime guardrails for FM-based agents.\nSection VI identifies and summarizes the primary threats that\ncould impact the validity of this study. Finally, Section VII\nconcludes the paper and outlines directions for future work.\nII. BACKGROUND AND RELATED WORK\nFMs have significantly advanced current agent development\nand emphasize the need to safeguard their behavior [3, 20].\nIn this context, guardrails for FM-based agents have been\nexplored; however, there is a lack of comprehensive studies\nthat provide a thorough understanding of guardrails for FM-\nbased agents. This paper aims to fill this gap. In the following\nsections, we present key background information and related\nwork.\nA. Recent State-of-the-Art Works on Foundation Models and\nFM-Based Agents\nIn 2021, Bommasani et al. [1] provided a comprehensive\ndiscussion on FMs, illustrating key elements, relationships,\nopportunities, and associated risks. While their focus was\non FMs in general, they highlighted the potential for these\nmodels to serve as the foundation for more complex systems,\nincluding FM-based agents. Zhou et al. [20] reviewed research\nadvancements, challenges, and opportunities for pre-trained\nmodels in text, image, graph, and data modalities. They\nalso discussed the integration of FMs into systems such as\nagents. Both works offer excellent insights into future research\ndirections to address open problems and associated risks in\nFM-based agents.\nRecently, Lu et al. developed a taxonomy of FM-based\nsystems focusing on their pre-training, adaptation, architec-\ntural design, and responsible-AI-by-design [27]. The taxon-\nomy aids software architects and developers in evaluating\nand integrating FMs into complex agent systems. The authors\nthen highlighted considerations for responsible AI and safety\nattributes. Several other works [2, 8, 30, 31, 36], also empha-\nsize the importance of responsible AI and safety practices for\nFM-based agents. In [28, 29], the authors explored the risks\nassociated with deploying LLM-based agents and evaluated\ncurrent approaches for mitigating these risks through model\nalignment, respectively. In 2024, a reference architecture for\ndesigning responsible and safe FM-based agents is proposed\nin [3]. The authors demonstrated that the unique characteristics\nof FM-based agents—such as their autonomous operation,\nnon-deterministic behavior, and continuous evolution—pose\nsignificant challenges in ensuring responsible AI and AI safety.\nB. Existing Guardrails Approaches and Tools for FM-Based\nAgents\nThere exist several frameworks and tools for designing\nguardrails [9, 53, 55, 62, 67]. These works explored model\nalignment during design time to ensure that the FM’s outputs\nalign with defined goals. Pre-training and adaptation strategies\nplay a significant role in mitigating risks in FM-based agents.\nOur focus, however, is on runtime guardrails that monitor and\ncontrol the agent’s behavior during operation. These guardrails\nare essential for addressing emergent issues that arise during\nagent interactions within dynamic environments [1, 31].\nSome initial efforts have been made toward runtime\nguardrails. NeMo Guardrails [16] provides programmable\nguardrails to ensure that agents operate within safe param-\neters by monitoring inputs and outputs. OpenAI’s Moderation\nAPI [35] monitors and filters harmful content generated by\nagents to protect user interactions. The GuardAgent frame-\nwork [36] utilizes an agent to oversee and safeguard other\nagents. It demonstrates strong generalization and low oper-\national overhead by dynamically generating guardrail code.\nWe found that continuous validation ensures outputs from\nFM-based agents adhere to predefined ethical standards and\nguidelines. Techniques such as auditing agents through multi-\nlayered approaches [18, 37] are used to check for biases and\nensure ethical compliance.\nRecently, Bengio et. al. [31] demonstrate that adaptive\nfail-safes characteristics of guardrails intervene automatically\nwhen an FM-based agent exhibits potentially harmful behav-\nior. These fail-safes are designed to modify or halt outputs that\ncould lead to undesirable consequences. Similarly, dynamic\naccess controls adjust access permissions in real time based on\nthe context of data usage to protect sensitive information and\nensure it is accessible under appropriate circumstances [38].\nDue to the dynamic and adaptive nature of FM-based agents,\ndesigning effective runtime guardrails poses several additional\nchallenges [2, 8] e.g., scalability of guardrail mechanisms,\nthe need for real-time monitoring, and the complexity of\ninterpreting agent behaviors in diverse contexts. The authors\nin [77] propose a framework for evaluating AI systems, which\nis applicable to FM-based agents. It includes harmonized\nterminology, a taxonomy of key evaluation elements, and\na mapping of the AI lifecycle to stakeholders for ethical\nand accountable deployment. Despite these efforts, no frame-\nwork currently provides comprehensive guidance on designing\nmulti-layered runtime guardrails for FM-based agents, which\nwe explore in this paper based on SLR.\nIII. METHODOLOGY\nThis study focuses on two primary concepts: (i) foun-\ndation model-based agents and (ii) runtime guardrails. We\nadopted the Petticrew and Roberts approach [39] to define\nthe Population, Interventions, Comparison, Outcomes, and\nContext (PICOC), within which the intervention in this study\nis delivered. The PICOC for this study is shown in Table I.\nUsing these PICOC components and following Kitchenham’s\nguidelines [40], we develop the protocol for this study.\n\nMulti-layered Runtime Guardrails \nTaxonomy – Qualities & design options\nArchitecture -- Swiss cheese model\nProtocol\nDevelop PICOC, Define scientific databases \nand resources for search, Formulate keyword & \nthe search string, Define qualitative & \nquantitative checklists, Specify inclusion & \nexclusion criterion, Define reference \nmanagement process, Define data analysis \ntechiniques\nExecute Keyword Based \nAutomatic Search \nACM, IEEE Xpolre, Springer, ScienceDirect, \n& Google Scholar -- Returns 1733 papers \nStudy Filtering (Vetting Process)\nRemove Duplicate & editorials (1625 papers)\nApply exclusion criteria  (101 papers)\nApply inclusion criteria  (21 papers)\nGrouping selected 21 papers\nManual Search and Snowballing \nReturns 189 papers, Applying inclusion & \nexclusion criteria removes 174 papers\n(15 additional papers for inclusion in the list)\n1st Research Question (RQ1)\nKey qualities for designing runtime \nguardrails for FM-based agents\n3rd Research Question (RQ3)\nDesigning multi-layered runtime guardrails \nArchitecture to address FM-based \nagents’ challenges\nData Extraction, \nSynthesis and Analysis\nExecute review\nDevelop taxonomy\nProposed architecture\nFinal Report\nBidirectional iteration & rectification\n2nd Research Question (RQ3)\nAvailable guardrails design options\nActions, targets & scopes \nStrategies e.g., rules & autonomy\nModalities & techniques\nCross Check & Merge\n32 papers\nFigure 1. Methodology\nTable I\nPICOC FOR THIS STUDY\nPopulation\nStudies and researches focus on multi-layered runtime\nguardrails within foundation model-based agents.\nIntervention\nDevelopment, optimization, and evaluation of multilayer\nruntime guardrails in foundation model-based agents,\nfocusing on key quality attributes and design strategies\nsimilar to the Swiss Cheese Model structure.\nComparison\nComparative analysis of approaches to design multi-\nlayered runtime guardrails in FM-based agents.\nOutcomes\nTaxonomy of multi-layered runtime guardrails for foun-\ndation model-based agents.\nContext\nInclude: Empirical and theoretical studies on the com-\nponents, design and evaluation of guardrails in founda-\ntion model-based agents.\nExclude: Studies beyond the scope of foundation model\nbased agents, non-English literature, and those not con-\nsidering guardrails.\nA. Research Scope and Protocol Development\nThe high-level research approach for this study is shown\nin Figure 1. Initially, we determined the research scope and\ndeveloped a protocol following Kitchenham’s guidelines [40,\n41]. The protocol guided the entire study by defining relevant\nscientific databases and resources, formulating keywords and\nsearch strings, outlining qualitative and quantitative checklists,\nand specify criteria for study inclusion and exclusion.\nB. Research Questions\nWhen formulating our Research Questions (RQ), we\nwanted to ensure that they were broad enough to capture\nthe diverse aspects of multi-layered runtime guardrails while\nbeing specific enough to provide actionable insights. We\ncaptured these aspects through the following three RQs:\nRQ1: What are essential qualities for designing runtime\nguardrails in FM-based agents?\nOur first research question studies the key qualities for\ndesigning multi-layered runtime guardrails in FM-based\nagents. Section IV-A elaborate on how this research question\nis addressed.\nRQ2: What are the design options for runtime guardrails\nin FM-based agents?\nOur second research question investigates guardrails design\noptions in FM-based agents from different perspectives,\nincluding action, target, scope, rule, autonomy, modalities,\nand underlying techniques. Section IV-B outlines our approach\nto addressing this research question.\nRQ3: How can we design runtime guardrails to address\nthe unique challenges of FM-based agents?\nOur third research question explores how to address the\n\nTable II\nCONSOLIDATED CONCEPTS AND SEARCH TERMS\nMain Terms\nSupportive Search Terms\nConcept 1 (Co1):\nFoundation\nModel\nbased agents\nFoundation Models, Foundation Model based\nagents, Large Language Model, Generative\nAI, Artificial General Intelligence, Transformer\nModels, Self-supervised Learning, Pretrained\nModels, Language Models, Conversational AI.\nConcept 2 (Co2):\nRuntime Guardrails\nGuardrails, guardian, responsible AI, safe, risk,\ntrustworthy, protect, detect, monitor, verify, val-\nidate, evaluate, benchmark, design.\nchallenges arising from the autonomous and deterministic\nnature of FM-based agents. Specially, we examine how to\nadapt the Swiss Cheese Model to safeguard the behaviors of\nFM-based agents by implementing multi-layered guardrails\nacross various agent artifacts. Section V presents the proposed\narchitecture and discusses our strategies for addressing this\nresearch question.\nC. Search String Formulation\nRelevant primary studies for this SLR were identified based\non the RQs defined in Section III-B. With the assistance of the\nPICOC approach (shown in Table I), our search terms were\ndivided into two primary concepts, as shown in Table II. These\nconcepts helped us to set a well-formulated search string.\nWe also used synonyms, abbreviations, and alternative\nspellings of search terms to increase the number of relevant\nresearch papers. We used truncation and wildcard operators\nto save time and effort in finding these alternative keywords.\nMoreover, different supplementary key terms or phrases dis-\ncovered during search iterations were added to our search\nstring to enhance our search strategy. Our supposition is that\nthey will collect all relevant articles that contains guardrails for\nFM-based agents. When constructing the final search query,\nthe identified keywords, their alternatives and related terms\nwere linked with Boolean AND (&&), OR (∥) and NOT (¬)\noperators as follows as follows:\n[{(C11∥C12∥...∥C1n)AND(C21∥C22∥...∥C2n)\nNOT(UC1∥UC2∥...∥UCn)]\n(1)\nwhere C11,12,...,1n, and C21,22,...,2n ε Co1 and Co2 of Ta-\nble II, respectively; and UC1, UC2, . . ., UCn refers the\nExclude Context defined earlier in PICOC (Table I).\nD. Selection of Papers: Inclusion and Exclusion Criterion\nTable III and Table IV present the Inclusion Criteria (IC)\nand Exclusion Criteria (EC) that have been used to iden-\ntify the studies for this SLR, respectively. We found that\na considerable amount of work on guardrails exists in gray\nliterature; however, we excluded them as they often lack\npeer review and a rigorous validation process. While some\nsources [42] argue that gray literature is an important resource\nfor systematic literature reviews (SLRs), such literature can\nbe misleading and introduce biases and inconsistencies in the\nreview process [43]. We prioritized peer-reviewed sources in\nthis study to ensure scientific reliability and credibility, as per\nKitchenham et al. guidelines [40, 41].\nTable III\nINCLUSION CRITERIA\nID\nDetail Criterion\nIC1\nFull text of conference papers, journal articles, industry reports,\nand book chapters that are relevant to the defined main concepts:\nFoundation model based agents and guardrails.\nIC2\nPapers written in English that include references.\nIC3\nStudies that specifically address the design and development\nof guardrails in foundation model-based agents. This includes\ntheoretical frameworks, empirical research and case studies.\nIC4\nPapers available in an electronic format, such as PDF, DOC,\nDOCX, HTML, and PS etc.\nTable IV\nEXCLUSION CRITERIA\nID\nDetail Criterion\nEC1\nWork-in-progress proposals, keynote addresses, secondary stud-\nies, and vision papers without concrete relation to guardrails.\nEC2\nDiscussion papers and opinion pieces that do not provide em-\npirical evidence or concrete solutions related to guardrails in\nfoundation model-based agents.\nEC3\nShort communications less than two pages, and studies that do\nnot offer substantial information for analysis.\nEC4\nStudies focusing solely on AI or similar technologies without\ndirect relevance to guardrails.\nEC5\nResearch lacking a clear connection to the design and develop-\nment of guardrails in the context of foundation models.\nEC6\nDuplicate publications or earlier versions of studies that have been\nsuperseded by extended journal versions.\nEC7\nNon-original research, commentary, editorial pieces, and non-\nempirical discussions papers.\nEC8\nStudies inaccessible due to copyright or database restrictions.\nE. Study Search and Filtering Process\nOur filtration process is further detailed in Figure 2. Initially\nwe ran the formatted query on four major databases that\nreturned 1,733 research papers. We then applied filtering\nand classified the studies found according to the guidelines\npresented in [40, 41]. In our initial filtration process, we\nremoved 108 papers due to being duplicated articles, editorial\nor key notes. After reading the title, abstract, conclusion and\nskimming through the introduction, methodology and results,\nwe applied our exclusion criterion defined in Table IV, and\n1524 further papers were removed. During the third step of\nfiltration, we applied inclusion criteria and removed 80 papers\nas these studies did not meet ICs shown in Table IV. In\nparallel, we did a manual search and found 189 papers that\nmeet our key concepts defined in Table II but not contain any\nunwanted content (UC). After applying ICs and ECs, 15 out\nof 189 papers were selected. Finally, we did a cross-check and\nended up with 32 papers (shown in Appendix A).\nF. Data Extraction and Quality Assessment\nWe used a semi-automated process [44] for data extraction\nfrom the selected studies to answer our RQs. Key qualita-\ntive information extracted from each selected study includes\nguardrails definitions, motivations, reported key quality at-\ntributes, and design options. We also extracted several relevant\npieces of information to understand the context and consider-\nations in designing and evaluating runtime guardrails.\n\nAuto Search: 1733 \nPapers\nACM digital library, \nIEEE Xpolre, Springer \nlink, ScienceDirect, and \nGoogle Scholar\nRemaining \nNo. of Papers (1625)\nACM digital library, \nIEEE Xpolre, Springer \nlink, ScienceDirect, and \nGoogle Scholar\nRemove\nDuplicate &\nEditorial \nNotes\nSnowballing and Manual \nSearch: 189 Papers \n(Co1 AND Co2 NOT UC )\nRemaining \nNo. of Papers (101)\nACM digital library, \nIEEE Xpolre, Springer \nlink, ScienceDirect, and \nGoogle Scholar\nApply \nExclusion \nCriteria\nRemaining \nNo. of Papers (21)\nACM digital library, \nIEEE Xpolre, Springer \nlink, ScienceDirect, and \nGoogle Scholar\nApply Inclusion Criteria\nRemaining No. of \nPapers (15)\nApply \nInclusion & \nExclusion \nCriterion\nFinally Selected  : 32 Papers\nACM digital library, IEEE Xpolre, Springer link, ScienceDirect, and Google Scholar\nCross Check & Merge\nFigure 2. Study Selection Process for this SLR\nWe then evaluated each study based on the following five\nQuality Assessment Criteria (QAC) on a scale from 1 (Very\nPoor) to 5 (Excellent). If a study’s average score was less than\n2, it was excluded from further analysis. Otherwise, we used\nthe qualitative information to decide this. The QAC used for\nthis study are 1:\n❖Relevance to guardrails for FM-based agents.\n❖Clear methodology for guardrail design.\n❖Adequate data collection, analysis, and evaluation of\nguardrail effectiveness at different layers of the agent\narchitecture.\n❖Discussion of challenges in designing guardrails for au-\ntonomous and non-deterministic behaviors in agents.\n❖Practical applicability of findings for guardrails in FM-\nbased agents.\nIV. TAXONOMY OF GUARDRAILS FOR FM-BASED AGENTS\nFigure 3 presents the proposed taxonomy of runtime\nguardrails for FM-based agents, developed based on the results\nof a systematic literature review. The taxonomy is organized\ninto external and internal quality attributes, and design options\nfrom different perspectives.\nA. Quality Attributes of Guardrails\nWe examine the key quality attributes that should be con-\nsidered when designing runtime guardrails. Below, we discuss\nthese attributes in detail.\n1) Accuracy: Accuracy in FM-based agents is crucial,\nparticularly in mitigating issues such as hallucinations, mis-\ninformation, and disinformation [45]. Hallucinations occur\n1QAC score for each selected study is presented in Appendix B\nwhen models generate information that is factually incor-\nrect. Such inaccuracies can mislead users and damage the\ncredibility of the agent [10]. Misinformation refers to the\nunintentional spread of false information, while disinformation\ninvolves the deliberate dissemination of falsehoods to receive\nusers [20]. For example, OpenAI uses guardrails to clearly\nlabel AI-generated content to prevent deepfakes and misin-\nformation [46]. One such case has been reported to prevent\nmisleading voters in last US elections [47, 48].\n2) Efficiency: Efficiency is crucial in FM-based agents, as\nusers expect fast, efficient responses [24]. Without guardrails,\nagents risk engaging in resource-intensive tasks that slow down\nresponse times [54]. By dynamically managing resources\nacross multiple layers, these guardrails prevent inefficiencies,\nsuch as endless loops, and filter irrelevant inputs, ensuring\nthat agents focus on processing meaningful data [3, 16, 64].\nAdditionally, FM-based agents can incur significant costs due\nto errors, inefficiencies, or non-compliance with regulations.\nWithout proper guardrails, agents might generate outputs that\nlead to financial losses, legal penalties, or damage to their\nreputation [12, 26]. For example, an agent that provides\nincorrect financial advice could result in monetary losses for\nusers and potential lawsuits against the provider.\n3) Privacy: Privacy in FM-based agents poses risks due\nto handling sensitive data, where data leakage might ex-\npose personal information [1, 12]. This leakage can occur\nthrough direct responses or statistical inferences, or inadvertent\nrevelations through model outputs. In April-May 2023, a\nnotable incident involved Samsung employees leaking propri-\netary information into ChatGPT, leading to Samsung banning\nChatGPT [50].\n\nTaxonomy\nof\nguardrails\nQuality\nattributes\nActions\nDesign\noptions\nPrivacy\nSecurity\nAccuracy\nSafety\nFairness\nCompliance\nGeneralizability\nCustomizability\nAdaptability\nTraceability\nPortability\nInteroperability\nInterpretability\nTargets\nUnderlying\nmodels\nBlock\nRetry\nFall back\nHuman intervention\nFilter\nModify\nPrompts\nTools\nKnowledge bases\nPlans\nReasoning\nContext\nMemory\nRule-based models\nHybrid models\nFoundation models\nNarrow models\nIndustry\nOrganizations\nTeams\nPriority-enabled \nContext-dependent\nUniform \nModality\nSources\nNegotiable\nSingle modal\nMultimodal\nOther agents\nGoals\nRules\nFMs\nFinal results\nIntermediate results\nValidate\nDefer\nIsolate\nEvaluate\nRedundancy\nParallel calls\nEfficiency\nUsers\nFlag\nPipeline\nArtefacts\nWorkflow\nFigure 3. Taxonomy of multi-layered runtime guardrails for FM-based agents.\n4) Security: Security in FM-based agents involves protect-\ning them from malicious activities that could compromise their\nintegrity and functionality [6, 14, 19]. For example, an FM-\nbased agent could be targeted by hackers to manipulate data,\nproducing incorrect or harmful outputs that affect decision-\nmaking processes [51]. An incident reported in [52] described\nhow malicious users manipulated Microsoft’s Tay chatbot to\nproduce inappropriate (offensive) content, leading to its shut-\ndown. FM-based agents are also vulnerable to hacks that may\nbreach data confidentiality [53]. Even with authorized access,\nthere is a risk of data misuse by third-party providers [54].\nMoreover, FM-based agents are prone to adversarial attacks,\nwhere specially designed queries extract sensitive information.\nGuardrails mitigate these risks by detecting and responding\nto real-time threats across various operational layers, safe-\nguarding agent integrity [10, 19], confidentiality [49, 55, 56],\navailability [11, 18, 26, 53, 57] and performance [1, 51].\n5) Safety: FM-based agents face significant safety issues,\nparticularly in generating harmful or misleading outputs.\nThese issues can arise when models produce content that\nis inappropriate, offensive, or incorrect [3]. These issues are\ncritical in contexts where FM-based agents handle critical data\nlike medical diagnosis or self-driving cars, where inaccurate\noutputs could have severe consequences [32]. Additionally,\nthere is a risk of generating questionable content, which can\ndamage the credibility and acceptance of the agent [53].\n6) Fairness: FM-based agents can face bias and discrim-\nination in model outputs. These biases can emerge from the\ntraining data, model algorithms, or deployment context [2, 61].\nFor instance, an agent used in recruitment for screening CVs\nmight inadvertently favor candidates from certain demograph-\nics, cultures, and languages [8, 26], affecting credibility.\n7) Compliance: Compliance in FM-based agents involves\nadhering to legal and regulatory standards [16, 20]. These\nissues are critical because non-compliance can lead to legal\npenalties, reputational damage, and loss of user trust. Runtime\nguardrails reduce these risks by ensuring alignment with\ndata protection regulations, industry standards, and guidelines\nthrough continuous monitoring at multiple levels [26, 54].\nAdditionally, these guardrails assist in automating compliance\nchecks. They ensure that all aspects of the FM-based agent’s\noperations align with the necessary legal and regulatory frame-\nworks [36, 62], and better support internal audits and exter-\nnal regulatory reviews [12]. For example, FM-based agents\nmay unintentionally facilitate unauthorized use of generated\ncontent, making it vulnerable to duplication or improper\ndistribution [10, 26, 55]. Guardrails operating in real time help\nmitigate these risks by detecting and restricting unauthorized\naccess, ensuring better copyright protection [49]. Techniques\nsuch as watermarking, fingerprinting, and labeling are applied\nacross different layers to ensure the ownership and compliance\nwith licensing laws [1, 10].\n8) Generalizability: Generalizability in guardrails for FM-\nbased agents refers to their ability to function effectively in\nreal-time across multiple layers and diverse scenarios without\nprior configurations [63]. Such guardrails ensure that protec-\ntive measures are not overly specific to a single use case\nbut can adapt to various contexts and still perform reliably\nacross layers. The agents’ ability to handle diverse linguistic,\ncultural, and operational contexts is essential to provide robust\nprotection, resilience, and reliability and is ensured by the\ngeneralizability attribute [1, 12]. Guardrails that can extend\ntheir applicability to new domains without significant reconfig-\nuration or degradation in performance, even during unexpected\ninputs or data types, are essential [15, 64].\n\n9) Customizability: Customizable guardrails provide tai-\nlored protection that meets specific requirements and sup-\nports diverse operational needs in FM-based agents [1, 65].\nThe multi-layered runtime approach allows for customization\nat different layers to enable fine-grained control over the\nagent’s behavior during execution, such as adjustments and\nconfigurations that align with particular operational goals, data\ncharacteristics, and regulatory environments. For example, a\ncustomer service chatbot can enable priorities for different\nguardrails and adjust data handling based on the user’s location\nand ensuring compliance with regulation.\n10) Adaptability: Adaptability in guardrails is known as\ntheir capability to adjust and remain effective under varying\nconditions and data landscapes as context evolves [24, 26].\nThis attribute ensures robust and continuous protection by dy-\nnamically responding to changes in input data, usage patterns,\nand emerging threats without manual reconfiguration [15]. For\nexample, a customer service chatbot can automatically update\nits guardrails to detect and block new offensive terms during\ninteractions. This includes incorporating new knowledge and\nadvancements in threat detection techniques [1, 54].\n11) Traceability: The traceability attribute of guardrails\ntracks and records the origins, processes, and decision paths,\nsuch as input and output of FMs, external tools, etc. [27].\nIt involves maintaining detailed logs and records that can be\naudited to understand how decisions are made. For example,\nin a customer service chatbot, traceability ensures that every\nrecommendation can be traced back to the data sources and al-\ngorithms used. This provides a clear audit trail for transparency\nand accountability. Traceability also aids in identifying the root\ncauses of issues to enable timely and accurate troubleshooting\nand improvement [26], and helps in maintaining user trust\nand meeting regulatory requirements [10, 16]. Comprehensive\ndocumentation of data sources and model modifications also\nbetter support effective auditing and compliance checking [12].\n12) Portability:\nPortability in guardrails for FM-based\nagents refers to the ability of these protective measures to\nbe easily adapted and applied across different FM-based\nagents [27]. Multiple layer runtime guardrails allow individual\nlayers to be transferred and integrated into different agents\nwith minimal adjustments in real time. This includes ensuring\nthat they function consistently across various FM architec-\ntures and environments, thereby maintaining their effectiveness\nand integrity regardless of the underlying technologies [26].\nFor example, the same guardrail can be applied for content\nmoderation in both a customer service chatbot and a social\nmedia platform, regardless of their underlying technology. The\nbenefits of designing portable guardrails include compatibility\nacross multiple programming languages and frameworks fa-\ncilitate their integration into diverse technological stacks [49].\nThese capabilities ensure that the guardrails remain effective\nand operational as the agent evolves or migrates to new\nenvironments. Portable guardrails also support seamless up-\ndates and improve scalability to maintain high standards of\nsecurity and compliance while adapting to new technological\nadvancements within agents [16].\n13) Interoperability: Interoperable guardrails work seam-\nlessly across differing agents, technologies and interface effec-\ntively with various components and services within different\nagents [27]. They ensure that security, privacy, and compliance\nprotocols can be applied consistently, even in heterogeneous\nenvironments that utilize varied software and hardware compo-\nnents, or diverse technological ecosystems [16, 67]. Guardrails\nthat interface with various APIs and data formats also en-\nable smooth communication and operation across different\nagents [26]. For example, they enable a customer service\ncopilot and internal support system to share data securely\nand consistently. This promotes cohesive and unified security\nmanagement, reducing the complexity of maintaining multiple\ndisparate protective measures [1], and better support collabo-\nrative efforts and data sharing [49].\n14) Interpretability: Interpretability refers to the clarity\nand transparency with which guardrails and protective mea-\nsures operate. Interpretability allows better inspection and\nunderstanding of each layer’s function during execution. This\nallows users and stakeholders to understand how decisions\nare made and actions are taken by models. Thus increasing\ntrust and accountability [10, 68]. For example, a chatbot\nin healthcare, can explain why certain advice is given or\nrestricted. Transparent guardrails better facilitate auditing and\ncompliance [18]. They also help users to understand that\nactions taken by guardrails can be clearly understood and\nverified [55]. This is essential for identifying and correcting\nerrors, as well as for ensuring that the agent’s operations align\nwith ethical and regulatory standards.\nB. Design Options of Guardrails\nThis section presents a structured taxonomy for designing\nguardrails, focusing on identifying various design alternatives.\n1) Actions: Guardrail actions are crucial for addressing the\nspecific needs of FM-based agent artifacts. We have identified\nthe following guardrail actions that can be applied to FM-\nbased agents:\n• Block: The block action prevents specific inputs (such as\nuser prompts) or outputs (such as content generated by\nFMs) from being processed or sent by various compo-\nnents (such as FMs and tools) in FM-based agents [54].\nFor example, the block action can reject the user prompts\ncontaining harmful instructions, thus preventing unde-\nsired outcomes.\n• Filter: The filter action involves scanning and removing\nundesired or irrelevant content from the inputs or outputs\nof different components in FM-based agents [69, 70]. For\ninstance, a filter may remove any personal data contained\nin the user prompts or the output generated by FMs.\n• Flag: The flag action is used to mark specific inputs,\noutputs, operations within FM-based agents [16]. For\nexample, unusual transactions requested by the FM-based\nagent can be flagged for human review to ensure they\ncomply with organizational policies [1, 30].\n\n• Modify: The modify action allows for the adjustment of\ninputs or outputs of various components in FM-based\nagents to meet specific requirements or standards [9]. For\nexample, the user prompts can be modified by adding\nmore context and examples, making it easier for the FM\nto accurately interpret the user’s intentions and provide\nmore relevant responses.\n• Validate: The validate action checks agent artifacts\nagainst predefined criteria to ensure they meet specified\nrequirements or standards [26, 70]. For example, the plan\ngenerated by FM-based agents should be validated, e.g.,\nthrough external verifier [79], to ensure it is compliant\nwith regulatory policies.\n• Parallel calls: The parallel calls action can send multiple\nrequests to the agent/component to improve responsive-\nness, e.g., a user can send a prompt to the agent or an\nexternal service multiple times at the same time and select\nthe better response [16, 53].\n• Retry: The retry action involves attempting a request\nagain after an initial failure or unsatisfactory result [13].\n• Fall back: When one step in the workflow cannot be\nexecuted successfully, the fall back action redirect to the\nprevious step and state [13, 16, 71].\n• Human intervention: The human intervention action\nrequires humans to review and approve specific outputs or\ndecisions [16, 53, 55]. For example, responses involving\nsensitive medical advice might be flagged for human\napproval before being communicated to users.\n• Defer: The defer action postpones the processing of\na request or task until specific conditions are met or\nadditional information is available [72].\n• Isolate: The isolate action involves segregating a specific\nentity (e.g., user) or component to prevent interaction\nwith the agent [19, 57, 60]. For example, an agent\nmight isolate a compromised narrow AI model suspected\nof being poisoned with malicious data in a sandbox\nenvironment, preventing potential harm to the agent.\n• Redundancy: The redundancy action involves imple-\nmenting backup processes or components to ensure con-\ntinuity and reliability in case of failures [16, 26]. For\nexample, two sensors can be deployed to detect context\ninformation for an agent.\n• Evaluate: The evaluate action involves assessing the\nresults [1]. For instance, an agent might ask another agent\nto evaluate its intermediate or final results.\n2) Targets: Guardrail actions can be applied to various\ntargets across multi-layers, including both pipelines and ar-\ntifacts. Some guardrails are applied the the entire pipeline\n(including prompts, intermediate results, and final results),\nwhile others are focused on specific artifacts (covering goals,\ncontext, reasoning, plans, memory, tools, knowledge bases,\nother agents, FMs). Table V provides an overview of agent\ntargets and corresponding guardrail actions.\nTable V\nA MAPPING OF AGENT TARGETS TO GUARDRAIL ACTIONS\nType\nTargets\nGuardrail Actions\nPipeline\nPrompts\nBlock, filter, flag, modify, parallel calls, retry,\ndefer, evaluate\nIntermediate\nresults\nFlag, human intervention, evaluate\nFinal results\nBlock, filter, flag, modify, retry, fall back, hu-\nman intervention, evaluate\nArtifacts\nGoals\nValidate, block, flag, modify, human interven-\ntion, defer\nContext\nBlock, filter, flag, modify, evaluate\nMemory\nBlock, filter, flag, modify, retry, human inter-\nvention, isolate, evaluate\nReasoning\nFlag, modify, validate, human intervention\nPlans\nBlock, flag, modify, validate, retry, fall back,\nhuman intervention, defer\nWorkflows\nValidate, parallel calls, retry, fall back, human\nintervention, defer, evaluate\nTools\nBlock, parallel calls, retry, fall back, human\nintervention, defer, evaluate\nKnowledge\nbases\nBlock, filter, flag, modify, retry, isolate, evalu-\nate, redundancy\nOther agents\nBlock, flag, parallel calls, retry, fall back, human\nintervention, defer, isolate, evaluate\nFMs\nBlock, filter, flag, modify, parallel calls, retry,\nfall back, human intervention, isolate, evaluate,\nredundancy\n• Prompts: Prompts are the initial user inputs or queries.\nGuardrails on prompts help ensure that user prompts are\nrelevant, appropriate, formatted correctly, and easier for\nFMs to understand [37, 56, 70].\n• Intermediate Results: Intermediate results are the out-\nputs generated at various stages during the workflow\ngeneration of agents, before reaching the final outputs.\nBy monitoring intermediate results, guardrails can detect\nanomalies or inaccuracies before they propagate to the\nfinal results.\n• Final Results: Final results are the end outputs generated\nby agents, which are delivered to users or downstream\nsystems. Guardrails ensure that the final results meet user\nexpectations and comply with regulations and standards.\n• Goals: Ensuring that agents’ goals align with human\nvalues and do not deviate from the human’s intended\ngoals [16, 49].\n• Context: Monitoring the context that agents collect to\nensure it is relevant information and appropriate [36].\n• Memory: Managing the agents’ memory to retain rele-\nvant data and discard outdated or irrelevant information,\nwhile also preventing memory poisoning [36, 64].\n• Reasoning:\nChecking\nwhether\nthe\nreasoning\nis\nsound [30].\n• Plans: Ensuring the generated plans align with human\ngoals [30, 54].\n• Workflows: Managing the exceptions happened during\nruntime workflow execution [80].\n• Tools: Overseeing the proper use of tools by agents,\nincluding implementing access controls, restricting tool\ncapabilities, and detect potential vulnerabilities [36, 49].\n\n• Knowledge Bases: Guardrails enforce stringent monitor-\ning and validation of external knowledge bases, partic-\nularly in retrieval augmented generation scenarios [17].\nFor example, they can prevent the retrieval of sensitive\nbusiness data [73].\n• Other Agents: Managing interactions between agents to\nensure collaboration, prevent conflicts, and mitigate risks\nassociated with malicious behaviors [30, 49].\n• FMs: Guardrails ensures the outputs generated by FMs\nare relevant, appropriate and safe. Also, guardrails over-\nsee the utilization of FMs, preventing misuse and ensur-\ning their application under appropriate conditions [1, 20].\n3) Rules: Guardrails rules can be configured in different\nways: including uniform rules, priority-enabled rules, context-\ndependent rules, and negotiable rules. A uniform strategy\napplies the same set of guardrails consistently across all sce-\nnarios, ensuring simplicity and uniformity [67]. It is particu-\nlarly effective in environments with stable and well-understood\nrisks. It largely reduces the complexity of managing diverse\nguardrails [55]. A priority-enabled strategy prioritizes certain\nguardrails based on the criticality and sensitivity of operations\nor data. Context-dependent strategies adjust the implementa-\ntion of guardrails based on the system’s specific operational\ncontext. This allows for dynamic adjustments to guardrails in\nresponse to changing conditions, user needs, and operational\nenvironments [49]. The negotiability of guardrails, categorized\ninto hard and soft, defines the level of flexibility in enforcing\nrules. Soft guardrails allow adjustments based on context and\nsituational demands, providing a balance between protection\nand operational flexibility [49]. In contrast, hard guardrails are\nrigid and non-negotiable, ensuring adherence to critical legal,\nethical, or safety standards [12, 32].\n4) Sources: The source of guardrails in FM-based agents\nranges from industry regulations and standards to individual\npreferences. Industry-level regulations and standards provide\nthe broader regulatory framework within which FM-based\nagents must operate. Guardrails designed to comply with these\nregulations guarantee that the system adheres to industry best\npractices and legal requirements [16]. They facilitate simpler\nauditing and certification processes, ensuring the agent remains\ncompliant with evolving regulatory landscapes.\nAt the organizational level, guardrails align with internal\npolicies and procedures governing the operation and use of\nFM-based agents. This includes compliance with corporate\ngovernance, data protection policies, and ethical guidelines\nestablished by the organization [12]. Guardrails also ensure\nconsistency and accountability across different departments\nand functions within the organization.\nTeam-level constraints focus on the technical and opera-\ntional limitations defined by the development team. Guardrails\nat this level ensure that the agent functions efficiently within\nthese constraints, such as computational and memory limits,\nwhile maintaining robustness and reliability [26]. They also\nensure that the agent’s operations do not exceed predefined\nthresholds that could lead to performance degradation or\nsecurity vulnerabilities.\nFrom the user perspective, guardrails can reflect individ-\nual preferences and requirements. This involves adjusting\nthe agent’s behavior based on user-defined settings to align\noutputs with both user expectations and ethical considera-\ntions. Incorporating user preferences into guardrails provides a\npersonalized experience while maintaining safety and compli-\nance [55, 69]. Such guardrails ensure that the system respects\nuser autonomy and produces outputs that are relevant and\nacceptable.\n5) Modality: The modality of guardrails refers to the types\nof data and interactions they manage. Guardrails can be de-\nsigned for single modal or multimodal systems. Single modal\nsystems operate with one type of data input or output, such\nas text, image, or audio. For instance, in text-based agents,\nguardrails focus on addressing issues like offensive language,\nmisinformation, and data privacy [49]. In image-based agents,\nthey may involve techniques for detecting explicit content or\nensuring image quality standards [26].\nMultimodal guardrails address the combined risks of han-\ndling multiple data types. They synchronize protections across\ndifferent data types, ensuring comprehensive security and\ncompliance [55]. For example, a system that generates text\nbased on image inputs must ensure accurate and ethical\nrepresentation of the image content. This requires advanced\ncross-modal analysis and validation techniques to ensure the\nsystem operates reliably and ethically across all data types it\nhandles [53].\n6) Underlying\nmodels:\nThe underlying techniques of\nguardrails include rule-based, hybrid, and machine learning\nmodels, with each representing a distinct design option to meet\nspecific requirements [3, 27]. Rule-based models utilize prede-\nfined rules to monitor and control FM-based agents behavior.\nThese models implement strict and deterministic guidelines\nthat the agent must follow to ensure compliance with regula-\ntory requirements for data access and processing [49]. They\nare particularly effective in environments where operational\nparameters are well-defined and stable. Rule-based models can\nbe updated and are somewhat flexible. However, they may still\nstruggle with unexpected scenarios, such as detecting novel\nAI-generated content that falls outside predefined rules. This\nreliance on static rules can limit their adaptability, and regular\nupdates are needed [16, 71].\nIn contrast, machine learning models dynamically adapt and\nimprove guardrails based on new data and scenarios. These\nmodels can also learn from historical data and identify patterns\nthat indicate potential risks or compliance issues [64]. Machine\nlearning models can be further classified into narrow models\nand FMs. Narrow models are specialized systems designed\nfor specific tasks or domains. They require targeted guardrails\nto address domain-specific risks and compliance needs [15].\nFMs are large, general-purpose models that serve as the\nbackbone for multiple applications and tasks. These models\nnecessitate comprehensive and scalable guardrails to handle\na wide range of risks and compliance issues across different\napplications [26]. Nevertheless, they can be computationally\nintensive and require substantial data for training.\n\n Reasoning & planning\nContext engine\nWorkflow \nexecution\nMemory\nExternal\nenvironment\nAgent\nAgentOps infrastructure (continuous monitoring and logging)\nAgents\nTools\nUser\nKnowledge\nbases\nContext\nGoal\nOptimised\nprompt\nWorkflow\nResult\nMulti-layered runtime guardrails\nContinuous\nlearning\nPrivacy guardrails\nfor prompt\n... ... ...\n... ... ...\n... ... ...\nFairness guardrails\nfor final results\nSafety guardrails\nfor goal\nSwiss Cheese Model for AI Safety\nRisks\nPipelines\nQuality\nattributes\nArtifacts\nFigure 4. Reference architecture for multi-layered guardrails of FM-based agents.\nHybrid models integrate rule-based approaches with the\nadaptability of machine learning models to respond to new\nthreats and evolving data patterns [53]. For instance, Khorram-\nrouz et al.[59] demonstrate the use of the PaLM 2 framework\nto process user input and dynamically implement rule-based\ndecisions. This framework tests the system’s limits by itera-\ntively generating toxic content to evaluate PaLM 2’s safety\nguardrails. However, integrating hybrid models can increase\nsystem complexity and create additional challenges [53].\nV. REFERENCE ARCHITECTURE FOR DESIGNING\nMULTI-LAYERED RUNTIME GUARDRAILS OF AGENTS\nFigure 4 shows the proposed reference architecture for\nmulti-layered runtime guardrails of FM-based agents, which\nconsists of four key parts: (i) external environment, (ii) agent\ncomponents, (iii) built-in multi-layered runtime guardrails, and\n(iv) AgentOps infrastructure.\nA. External Environment:\nThe external environment refers to all entities interacting\nwith the agent, including users, other agents, external tools,\nand knowledge bases. Users provide goals and contextual\ninputs that shape the agent’s objectives. To achieve user\ngoals, the agent may utilize context detected in the external\nenvironment and interact with other agents, specialized tools,\nand extensive knowledge bases to perform complex tasks.\nB. Agent Components\nWithin the agent, there are four primary components: the\ncontext engine, reasoning and planning, workflow execution,\nand memory.\n• Context Engine: The context engine processes multi-\nmodal context data from the external environment to\nenrich the user prompt, helping FMs better understand\nuser goals. A prompt may contain elements such as goals\nand context. Instead of waiting for users’ instructions,\nthe agent can also proactively make suggestions based\non the context it detects, such as screen recordings,\nmouse clicks, eye tracking data, gestures, and document\nannotations [3].\n• Reasoning and Planning: After receiving optimized\nprompts, the reasoning and planning component pro-\ncesses the prompt to determine the most effective way\nof achieving the specified goal. This process may involve\nadopting reasoning patterns, such as the chain-of-thought\npattern [81], which structures the agent’s thinking into\nsequential, logical steps that align with the agent’s ob-\njectives. A detailed plan is then formulated to outline\neach step required to accomplish the goal. This includes\nselecting the appropriate tools, knowledge bases, and\nagents to carry out each action. The memory component\nmay be integrated to allow the agent to recall previously\ngathered experience and knowledge to refine the plan.\n• Workflow Execution: The workflow execution compo-\nnent is responsible for executing the sequence of ac-\ntions outlined by the reasoning and planning component.\nThis component directly interacts with external tools,\nknowledge bases, and other agents to complete tasks and\ngenerate outputs aligned with the user’s goals. The results\nare returned to the external environment and stored in the\nagent’s memory for future reference.\n• Memory: The memory component in this architecture\nstores relevant information from prior interactions, plans,\nand results. This accumulated knowledge supports con-\ntinuous learning, enabling the agent to refine its strategies\nand improve capabilities and skills over time, thereby\nimproving accuracy and minimizing repeated errors.\n\nC. Multi-layered Runtime Guardrails\nBuilding on the Swiss Cheese Model, we design multi-\nlayered runtime guardrails for FM-based agents, structured\naround the dimensions of quality attributes, pipelines, and\nartifacts specified in the taxonomy. In this architecture, each\n‘cheese slice’ represents a protective layer within the agent\nsystem, addressing quality attributes, pipeline stages, and/or\nspecific artifacts, such as a layer about privacy guardrails for\nprompts or security guardrails for tools. While each layer\ncontains holes (i.e., potential gaps or weaknesses), where risks\nmight slip through, the holes are positioned differently across\nlayers. Gaps in one layer are often covered by another; thus,\neven if one layer fails, another can catch and mitigate the issue.\nFrom the perspective of quality attributes (discussed in\nSection IV-A), guardrails can be designed to ensure accuracy,\nefficiency, privacy, security, safety, fairness, compliance. From\nthe pipelines perspective, guardrails can be applied at multiple\nstages: the user prompts, intermediate results during workflow\nexecutions, and final results generated by the agent.\n• Guardrails\nfor\nprompts:\nAnalyse\nincoming\nuser\nprompts to detect and manage sensitive information,\nharmful content, misinformation, disinformation, discrim-\ninatory language, ensuring the prompt aligns with safety\nand ethical standards [82].\n• Guardrails for intermediate results: Apply at each step\nof the workflow to verify that intermediate results are\naccurate, safe, and responsible, safeguarding the integrity\nof the process before the final results are produced.\n• Guardrails for final results: Check that the agent’s final\noutputs are align with the user goals and governance\nrequirements, such as AI safety standard requirements.\nMoreover, from the artifacts perspective, guardrails can be\nenforced on each agent artifact including goals, context, mem-\nory, reasoning, plans, tools, knowledge bases, other agents, and\nFMs. These guardrails ensure that each artifact is within safe\nand responsible boundaries.\n• Guardrails for goals: Ensure that the goals are achiev-\nable, within the agent’s scope, and aligned with gover-\nnance requirements, including regulatory standards and\norganizational policies, avoiding goals that may lead to\nharmful outcomes and potential misuse [85].\n• Guardrails for context: Validate contextual information\nto ensure it is relevant, accurate, and free from sensitive\nor misleading information.\n• Guardrails for memory: Ensure that stored past expe-\nrience is relevant, accurate, and free from any malicious\nor misleading content, preventing memory poisoning [83]\nand retaining only useful data for future interactions.\n• Guardrails for reasoning: Check the agent’s reasoning\nprocesses to prevent logical errors and ensure the reason-\ning steps are safe, responsible, and aligned with the user\nintent.\n• Guardrails for plans: Assess the feasibility, safety, and\ncompliance of the plans generated by the agent, ensuring\nthat each step in the workflow is responsible and does\nnot introduce unnecessary risk. The plan can be made by\nexternal verifiers, i.e., external planning tools [79].\n• Guardrails for workflows: Handle the exceptions that\narise during the workflow executions by implementing\nmechanisms like force-failing a step or retrying a tool\ncall [80]\n• Guardrails for external tools: Analyse the quality (e.g.\nvulnerability [86]) of the external tools to ensure that only\napproved and safe tools are invoked by the agent.\n• Guardrails for knowledge bases: Verify that the infor-\nmation retrieved from knowledge bases is relevant and\nethical (e.g., without any PII information).\n• Guardrails for other agents: Ensure the selected agents\nhave a reliable and safe operational history.\n• Guardrails for FMs: Enforce boundaries on the FM’s\nnon-deterministic outputs, applying modifications or flags\nas needed.\nD. AgentOps\nAgentOps provides a comprehensive infrastructure designed\nto enable observability [84] for FM-based agents by continu-\nously monitoring and recording runtime data. This infrastruc-\nture captures a wide range of data elements, from pipeline\nexecution details and agent artifacts to the specific guardrails\napplied to the pipeline and artifacts. All these data need to be\nkept as evidence with metadata such as FM version and the\ntimestamp. The data collected by the AgentOps infrastructure\ncan also feed into multi-layered guardrails to activate the\nrelevant guardrails as needed.\nVI. THREATS TO VALIDITY\nOur study is subject to standard literature search and se-\nlection bias threats. We addressed these threats by searching\nthe most commonly used databases in the IT and software\nengineering domains. We revised our search strings several\ntimes during the automatic search to maximize the number of\nrelevant articles matching two key concepts: ‘guardrails’ and\n‘FM-based agents’. We also kept our search string generic\nto search through the titles, abstracts, keywords, and full text\nof articles to cover the maximum number of relevant papers.\nWe then conducted a manual search on Google Scholar to\ncomplement the automatic search using a snowballing strategy.\nFurthermore, predefined review protocols with detailed inclu-\nsion and exclusion criteria helped us reduce bias in selecting\nprimary studies. We applied several quality assessment criteria\nto estimate the quality of the selected primary studies. Even\nthough the proposed criteria were not too strict, applying them\nled to several initially selected papers being excluded. To\nmitigate the risk of missing important data from the primary\nstudies, we reinstated the excluded papers that were closely\nrelated to the primary studies.\nMoreover, our definitions and categorizations may not cap-\nture all relevant aspects of guardrails in FM-based agents.\nTo mitigate this threat, we validated the taxonomy through\nextensive literature review and expert feedback. However, this\nintroduces a risk of producing biased results that address only\n\nexpert needs, as the people involved in the feedback process\nhave extensive experience in the AI and software engineering\ndomains. Our review protocols helped us to reduce such bias.\nWe prepared a guardrails taxonomy and conducted a com-\nparative analysis of its components to help the reader better\nunderstand their design and evaluation. We critically examined\nthe strength and consistency of relationships in the selected\nstudies to develop a reliable taxonomy and reference archi-\ntecture for designing built-in multi-layered runtime guardrails.\nFinally, we draw conclusions. Nonetheless, the generalizability\nof guardrails to different contexts and types in FM-based\nagents remains a potential limitation. Specific adaptations\nmight be necessary for certain systems, such as those used\nin healthcare or financial organizations.\nVII. CONCLUSION AND FUTURE WORK\nTo advance the understanding of runtime guardrail design\nin FM-based agents, this paper presents a comprehensive\ntaxonomy of guardrail design based on the results of an\nSLR. Our taxonomy categorizes guardrails based on their\nessential quality attributes and key design dimensions, includ-\ning guardrail actions and targets, employed rules, guardrail\nsources, modality, and underlying models. Building on this\ntaxonomy, we propose a novel Swiss Cheese Model for\nAI safety - a reference architecture for designing built-in,\nmulti-layered guardrails in FM-based agents, which includes\nthree dimensions: quality attributes, pipelines, and artifacts.\nIn the future, we plan to develop guardrail services for a\nscientific agent platform, implementing the proposed reference\narchitecture and integrating various design options outlined in\nthe taxonomy.\nAPPENDIX A: LIST OF SELECTED STUDIES\n[SS1] M. Pawagi, V. Kumar, Guardrails: Automated sugges-\ntions for clarifying ambiguous purpose statements, in:\nProceedings of the 16th Annual ACM India Compute\nConference, COMPUTE ’23, Association for Comput-\ning Machinery, New York, NY, USA, 2023, p. 55–60.\ndoi:10.1145/3627217.3627234.\n[SS2] A.\nKhorramrouz,\nS.\nDutta,\nA.\nDutta,\nA.\nR.\nKhudaBukhsh,\nDown\nthe\ntoxicity\nrabbit\nhole:\nInvestigating\npalm\n2\nguardrails,\narXiv\npreprint\narXiv:2309.06415\n(2023).\ndoi:https:\n//doi.org/10.48550/arXiv.2309.06415.\n[SS3] Y.\nDong,\nR.\nMu,\nG.\nJin,\nY.\nQi,\nJ.\nHu,\nX. Zhao, J. Meng, W. Ruan, X. Huang, Building\nguardrails\nfor\nlarge\nlanguage\nmodels,\narXiv\npreprint\narXiv:2402.01822\n(2024).\ndoi:https:\n//doi.org/10.48550/arXiv.2402.01822.\n[SS4] N. Mangaokar, A. Hooda, J. Choi, S. Chandrashekaran,\nK. Fawaz, S. Jha, A. Prakash, Prp: Propagating universal\nperturbations to attack large language model guard-rails,\narXiv preprint arXiv:2402.15911 (2024). doi:https:\n//doi.org/10.48550/arXiv.2402.15911.\n[SS5] M. Anderljung, J. Barnhart, J. Leung, A. Korinek,\nC. O’Keefe, J. Whittlestone, S. Avin, M. Brundage,\nJ. Bullock, D. Cass-Beggs, et al., Frontier ai regulation:\nManaging emerging risks to public safety, arXiv preprint\narXiv:2307.03718 (2023). doi:https://doi.org/\n10.48550/arXiv.2307.03718.\n[SS6] M. Liffiton, B. E. Sheese, J. Savelka, P. Denny, Code-\nhelp: Using large language models with guardrails for\nscalable support in programming classes, in: Proceedings\nof the 23rd Koli Calling International Conference on\nComputing Education Research, Koli Calling ’23, Asso-\nciation for Computing Machinery, New York, NY, USA,\n2024, pp. 1–11. doi:10.1145/3631802.3631830.\n[SS7] Z.\nZhang,\nY.\nLu,\nJ.\nMa,\nD.\nZhang,\nR.\nLi,\nP. Ke, H. Sun, L. Sha, Z. Sui, H. Wang, et al.,\nShieldlm:\nEmpowering\nllms\nas\naligned,\ncustomiz-\nable and explainable safety detectors, arXiv preprint\narXiv:2402.16444 (2024). doi:https://doi.org/\n10.48550/arXiv.2402.16444.\n[SS8] T. Rebedea, R. Dinu, M. N. Sreedhar, C. Parisien, J. Co-\nhen, NeMo guardrails: A toolkit for controllable and safe\nLLM applications with programmable rails, in: Y. Feng,\nE. Lefever (Eds.), Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing:\nSystem Demonstrations, Association for Computational\nLinguistics, Singapore, 2023, pp. 431–445. doi:10.\n18653/v1/2023.emnlp-demo.40.\n[SS9] Y. Wang, L. Singh, Adding guardrails to advanced\nchatbots,\narXiv\npreprint\narXiv:2306.07500\n(2023).\ndoi:https://doi.org/10.48550/arXiv.\n2306.07500.\n[SS10] M. Shanahan, Talking about large language models,\nCommun. ACM 67 (2) (2024) 68–79. doi:10.1145/\n3624724.\n[SS11] W. Du, Q. Li, J. Zhou, X. Ding, X. Wang, Z. Zhou,\nJ. Liu, Finguard: A multimodal aigc guardrail in financial\nscenarios, in: Proceedings of the 5th ACM International\nConference on Multimedia in Asia, MMAsia ’23, Asso-\nciation for Computing Machinery, New York, NY, USA,\n2024, pp. 1–3. doi:10.1145/3595916.3626351.\n[SS12] A. Wei, N. Haghtalab, J. Steinhardt, Jailbroken: How\ndoes llm safety training fail?, Advances in Neural In-\nformation Processing Systems 36 (2024).\ndoi:10.\n48550/arXiv.2307.02483.\n[SS13] J. Zhao, K. Chen, X. Yuan, Y. Qi, W. Zhang,\nN. Yu, Silent guardian: Protecting text from malicious\nexploitation by large language models, arXiv preprint\narXiv:2312.09669 (2023). doi:https://doi.org/\n10.48550/arXiv.2312.09669.\n[SS14] X. Qi, Y. Zeng, T. Xie, P.-Y. Chen, R. Jia, P. Mit-\ntal, P. Henderson, Fine-tuning aligned language models\ncompromises safety, even when users do not intend to!,\narXiv preprint arXiv:2310.03693 (2023). doi:https:\n//doi.org/10.48550/arXiv.2310.03693.\n[SS15] J. M¨okander, J. Schuett, H. R. Kirk, L. Floridi, Au-\nditing large language models: a three-layered approach,\n\nAI and Ethics (2023) 1–31doi:https://doi.org/\n10.1007/s43681-023-00289-2.\n[SS16] S.\nBanerjee,\nS.\nLayek,\nR.\nHazra,\nA.\nMukher-\njee,\nHow\n(un)\nethical\nare\ninstruction-centric\nre-\nsponses\nof\nllms?\nunveiling\nthe\nvulnerabilities\nof\nsafety guardrails to harmful queries, arXiv preprint\narXiv:2402.15302 (2024). doi:https://doi.org/\n10.48550/arXiv.2402.15302.\n[SS17] S.\nEe—Researcher,\nZ.\nW.\nDirector,\nAdapting\ncybersecurity frameworks to manage frontier AI risks,\nInstitute for AI Policy and Strategy (IAPS) (2023).\nURL\nhttps://static1.squarespace.com/\nstatic/64edf8e7f2b10d716b5ba0e1/t/\n6528c5c7f912f74fbd03fc34/1697170896984/Adapting+\ncybersecurity+frameworks+to+manage+frontier+AI+\nrisks.pdf\n[SS18] P.\nRai,\nS.\nSood,\nV.\nK.\nMadisetti,\nA.\nBahga,\nGuardian: A multi-tiered defense architecture for thwart-\ning prompt injection attacks on llms, Journal of Soft-\nware\nEngineering\nand\nApplications\n17\n(1)\n(2024)\n43–68. doi:https://doi.org/10.4236/jsea.\n2024.171003.\n[SS19] A. Kumar, S. Singh, S. V. Murty, S. Ragupathy, The\nethics of interaction: Mitigating security threats in llms,\narXiv preprint arXiv:2401.12273 (2024). doi:https:\n//doi.org/10.48550/arXiv.2401.12273.\n[SS20] X. Shen, Z. Chen, M. Backes, Y. Shen, Y. Zhang,\n”do anything now”: Characterizing and evaluating in-\nthe-wild jailbreak prompts on large language models,\narXiv preprint arXiv:2308.03825 (2023). doi:https:\n//doi.org/10.48550/arXiv.2308.03825.\n[SS21] A.\nKumar,\nC.\nAgarwal,\nS.\nSrinivas,\nS.\nFeizi,\nH. Lakkaraju, Certifying llm safety against adversarial\nprompting, arXiv preprint arXiv:2309.02705 (2023).\ndoi:https://doi.org/10.48550/arXiv.\n2309.02705.\n[SS22] Y. Zeng, H. Lin, J. Zhang, D. Yang, R. Jia,\nW.\nShi,\nHow\njohnny\ncan\npersuade\nllms\nto\njailbreak them: Rethinking persuasion to challenge\nai\nsafety\nby\nhumanizing\nllms,\narXiv\npreprint\narXiv:2401.06373\n(2024).\ndoi:https:\n//doi.org/10.48550/arXiv.2401.06373.\n[SS23] B. Wei, K. Huang, Y. Huang, T. Xie, X. Qi,\nM.\nXia,\nP.\nMittal,\nM.\nWang,\nP.\nHenderson,\nAssessing\nthe\nbrittleness\nof\nsafety\nalignment\nvia\npruning\nand\nlow-rank\nmodifications,\narXiv\npreprint\narXiv:2402.05162\n(2024).\ndoi:https:\n//doi.org/10.48550/arXiv.2402.05162.\n[SS24] S. Goyal, M. Hira, S. Mishra, S. Goyal, A. Goel,\nN. Dadu, D. Kirushikesh, S. Mehta, N. Madaan, Llm-\nguard: Guarding against unsafe llm behavior, in: Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nVol. 38(21), 2024, pp. 23790–23792.\ndoi:https:\n//doi.org/10.1609/aaai.v38i21.30566.\n[SS25] R. R. Llaca, V. Leskoschek, V. C. Paiva, C. Lup˘au,\nP. Lippmann, J. Yang, Student-teacher prompting for\nred teaming to improve guardrails, in: Proceedings of\nthe ART of Safety: Workshop on Adversarial testing\nand Red-Teaming for generative AI, 2023, pp. 11–\n23.\ndoi:http://dx.doi.org/10.18653/v1/\n2023.artofsafety-1.2.\n[SS26] Z. Wang, F. Yang, L. Wang, P. Zhao, H. Wang,\nL. Chen, Q. Lin, K.-F. Wong, SELF-GUARD: Empower\nthe LLM to safeguard itself, in: K. Duh, H. Gomez,\nS. Bethard (Eds.), Proceedings of the 2024 Conference of\nthe North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies\n(Volume 1: Long Papers), Association for Computational\nLinguistics, Mexico City, Mexico, 2024, pp. 1648–1668.\nURL https://aclanthology.org/2024.naacl-long.92\n[SS27] B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang,\nC. Xu, Z. Xiong, R. Dutta, R. Schaeffer, et al., De-\ncodingtrust: A comprehensive assessment of trustworthi-\nness in gpt models, in: Advances in Neural Information\nProcessing Systems, 2023, pp. 1–110.\ndoi:https:\n//doi.org/10.48550/arXiv.2306.11698.\n[SS28] R. Bommasani, D. A. Hudson, E. Adeli, R. B. Altman,\nS. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosse-\nlut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card,\nR. Castellon, N. S. Chatterji, A. S. Chen, K. Creel,\nJ. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya,\nE. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh,\nL. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. D.\nGoodman, S. Grossman, N. Guha, T. Hashimoto, P. Hen-\nderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang,\nT. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti,\nG. Keeling, F. Khani, O. Khattab, P. W. Koh, M. S. Krass,\nR. Krishna, R. Kuditipudi, et al., On the opportunities\nand risks of foundation models, CoRR abs/2108.07258\n(2021). arXiv:2108.07258, doi:https://doi.\norg/10.48550/arXiv.2108.07258.\n[SS29] L. Weidinger, M. Rauh, N. Marchal, A. Manzini,\nL. A. Hendricks, J. Mateos-Garcia, S. Bergman, J. Kay,\nC. Griffin, B. Bariach, et al., Sociotechnical safety\nevaluation of generative ai systems, arXiv preprint\narXiv:2310.11986 (2023). doi:https://doi.org/\n10.48550/arXiv.2310.11986.\n[SS30] D. Kang, D. Raghavan, P. Bailis, M. Zaharia, Model\nassertions for monitoring and improving ml models,\nProceedings of Machine Learning and Systems 2 (2020)\n481–496.\ndoi:https://doi.org/10.48550/\narXiv.2003.01668.\n[SS31] Z. Yuan, Z. Xiong, Y. Zeng, N. Yu, R. Jia, D. Song,\nB. Li, Rigorllm: Resilient guardrails for large lan-\nguage models against undesired content, arXiv preprint\narXiv:2403.13031 (2024). doi:https://doi.org/\n10.48550/arXiv.2403.13031.\n[SS32] Z. Chu, Y. Wang, L. Li, Z. Wang, Z. Qin, K. Ren, A\ncausal explainable guardrails for large language models,\narXiv preprint arXiv:2405.04160 (2024). doi:https:\n//doi.org/10.48550/arXiv.2405.04160\n\nAPPENDIX B: QAC SCORES FOR THE SELECTED STUDIES\nSelected Study\n(SS) No.\nQAC1\nQAC2\nQAC3\nQAC4\nQAC5\nSS-1\n2\n2\n2\n2\n2\nSS-2\n4\n5\n5\n5\n4\nSS-3\n4\n3\n2\n3\n2\nSS-4\n3\n3\n3\n3\n4\nSS-5\n2\n4\n3\n3\n5\nSS-6\n1\n5\n4\n3\n5\nSS-7\n5\n4\n4\n4\n3\nSS-8\n5\n4\n4\n5\n3\nSS-9\n0\n1\n2\n4\n3\nSS-10\n0\n2\n2\n3\n4\nSS-11\n3\n3\n1\n3\n2\nSS-12\n3\n2\n3\n3\n2\nSS-13\n4\n3\n3\n4\n3\nSS-14\n5\n4\n4\n5\n4\nSS-15\n5\n4\n3\n4\n5\nSS-16\n5\n4\n4\n5\n4\nSS-17\n5\n4\n3\n5\n4\nSS-18\n5\n4\n4\n5\n5\nSS-19\n4\n3\n2\n4\n3\nSS-20\n5\n4\n4\n5\n4\nSS-21\n5\n4\n4\n4\n5\nSS-22\n5\n5\n4\n5\n4\nSS-23\n5\n4\n4\n5\n4\nSS-24\n5\n4\n2\n2\n3\nSS-25\n5\n4\n4\n4\n5\nSS-26\n5\n4\n4\n5\n4\nSS-27\n5\n4\n5\n5\n4\nSS-28\n5\n4\n4\n3\n4\nSS-29\n4\n4\n4\n3\n4\nSS-30\n5\n4\n4\n4\n5\nSS-31\n5\n5\n4\n4\n5\nSS-32\n5\n4\n4\n4\n5\nREFERENCES\n[1] R. Bommasani, D. A. Hudson, E. Adeli, and et al.,\n“On the opportunities and risks of foundation models,”\nCoRR, vol. abs/2108.07258, 2021. Link: https://doi.org/\n10.48550/arXiv.2108.07258\n[2] Q. Lu, L. Zhu, J. Whittle, and X. Xu, Responsible\nAI:\nBest\npractices\nfor\ncreating\ntrustworthy\nAI\nsystems,\n1st\ned.\nAddison-Wesley\nProfessional,\n2023.\nLink:\nhttps://www.pearson.com/en-us/subject-\ncatalog/p/responsible-ai-best-practices-for-creating-\ntrustworthy-ai-systems/P200000010211/9780138073886\n[3] Q. Lu, L. Zhu, X. Xu, Z. Xing, S. Harrer, and J. Whittle,\n“Towards responsible generative AI: a reference archi-\ntecture for designing foundation model based agents,” in\n21st International Conference on Software Architecture\nCompanion.\nIEEE, 2024, pp. 119–126.\n[4] N. Maslej et al., “AI index report 2024,” Stanford\nInstitute\nfor\nHuman-Centered\nArtificial\nIntelligence\n(HAI), Tech. Rep., 2024. Link: https://aiindex.stanford.\nedu/report/2024\n[5] G. V. R. Team, “Artificial intelligence market size,\nshare, growth report 2030,” Grand View Research, Tech.\nRep., 2024. Link: https://www.grandviewresearch.com/\nindustry-analysis/artificial-intelligence-ai-market\n[6] R. Bommasani and P. Liang, “Reflections on foundation\nmodels,” 2021, Last accessed on Jun.-2024. Link: https:\n//hai.stanford.edu/news/reflections-foundation-models\n[7] S. Uspenskyi, “Large language model statistics and\nnumbers\n(2024),”\n2024,\nLast\naccessed\non\nJun.-\n2024.\nLink:\nhttps://springsapps.com/knowledge/large-\nlanguage-model-statistics-and-numbers-2024\n[8] L. Wang et al., “A survey on large language model\nbased\nautonomous\nagents,”\nFrontiers\nof\nComputer\nScience,\nvol.\n18,\nno.\n6,\npp.\n1–26,\n2024.\nLink:\nhttps://doi.org/10.1007/s11704-024-40231-1\n[9] Y. Wang and L. Singh, “Adding guardrails to advanced\nchatbots,” arXiv preprint arXiv:2306.07500, 2023. Link:\nhttps://doi.org/10.48550/arXiv.2306.07500\n[10] B.\nWang,\nW.\nChen,\nH.\nPei,\nC.\nXie,\nM.\nKang,\nC. Zhang, C. Xu, Z. Xiong, R. Dutta, R. Schaeffer\net al., “DecodingTrust: A comprehensive assessment of\ntrustworthiness in GPT models,” in Advances in Neural\nInformation Processing Systems, 2023, pp. 1–110. Link:\nhttps://doi.org/10.48550/arXiv.2306.11698\n[11] B. Wei, K. Huang, Y. Huang, T. Xie, X. Qi, M. Xia,\nP. Mittal, M. Wang, and P. Henderson, “Assessing the\nbrittleness of safety alignment via pruning and low-rank\nmodifications,” arXiv preprint arXiv:2402.05162, 2024.\nLink: https://doi.org/10.48550/arXiv.2402.05162\n[12] M. Anderljung et al., “Frontier AI regulation: Managing\nemerging\nrisks\nto\npublic\nsafety,”\narXiv\npreprint\narXiv:2307.03718, 2023. Link: https://doi.org/10.48550/\narXiv.2307.03718\n[13] A. Wei, N. Haghtalab, and J. Steinhardt, “Jailbroken:\nHow does LLM safety training fail?” Advances in\nNeural Information Processing Systems, vol. 36, 2024.\nLink: https://doi.org/10.48550/arXiv.2307.02483\n[14] A.\nGubkin,\n“Understanding\nwhy\nai\nguardrails\nare\nnecessary: Ensuring ethical and responsible ai use,”\n2024, Last accessed on Jul.-2024. Link: https://www.\naporia.com/learn/ai-guardrails/\n[15] Y. Dong, R. Mu, G. Jin, Y. Qi, J. Hu, X. Zhao, J. Meng,\nW. Ruan, and X. Huang, “Building guardrails for large\nlanguage models,” arXiv preprint arXiv:2402.01822,\n2024. Link: https://doi.org/10.48550/arXiv.2402.01822\n[16] T. Rebedea, R. Dinu, M. N. Sreedhar, C. Parisien, and\nJ. Cohen, “NeMo guardrails: A toolkit for controllable\nand safe LLM applications with programmable rails,”\nin Proceedings of the 2023 Conference on Empirical\nMethods\nin\nNatural\nLanguage\nProcessing:\nSystem\nDemonstrations, Singapore, Dec. 2023, pp. 431–445.\nLink: https://aclanthology.org/2023.emnlp-demo.40\n[17] D. Kang, D. Raghavan, P. Bailis, and M. Zaharia, “Model\nassertions for monitoring and improving ml models,”\nProceedings of Machine Learning and Systems, vol. 2,\npp. 481–496, 2020. Link: https://doi.org/10.48550/arXiv.\n2003.01668\n[18] J. M¨okander, J. Schuett, H. R. Kirk, and L. Floridi,\n“Auditing\nlarge\nlanguage\nmodels:\nA\nthree-layered\napproach,”\nAI\nand\nEthics,\npp.\n1–31,\n2023.\nLink:\n\nhttps://doi.org/10.1007/s43681-023-00289-2\n[19] A. Kumar, S. Singh, S. V. Murty, and S. Ragupathy,\n“The ethics of interaction: Mitigating security threats in\nLLMs,” arXiv preprint arXiv:2401.12273, 2024. Link:\nhttps://doi.org/10.48550/arXiv.2401.12273\n[20] C. Zhou et al., “A comprehensive survey on pretrained\nfoundation models: A history from BERT to ChatGPT,”\narXiv preprint arXiv:2302.09419, 2023. Link: https:\n//doi.org/10.48550/arXiv.2302.09419\n[21] M. Zaharia, O. Khattab, L. Chen, J. Q. Davis, H. Miller,\nC. Potts, J. Zou, M. Carbin, J. Frankle, N. Rao,\nand A. Ghodsi, “The shift from models to compound\nAI systems,” Berkeley Artificial Intelligence Research\n(BAIR), Tech. Rep., 2024. Link: https://bair.berkeley.\nedu/blog/2024/02/18/compound-ai-systems/\n[22] IBM, “What are large language models (LLMs)?” 2024,\nLast accessed on Jun.-2024. Link: https://www.ibm.com/\ntopics/large-language-models\n[23] ——,\n“What\nare\nfoundation\nmodels?”\n2024,\nLast\naccessed on Jun.-2024. Link: https://research.ibm.com/\nblog/what-are-foundation-models\n[24] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu,\nH. Chen, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang,\nY. Chang, P. S. Yu, Q. Yang, and X. Xie, “A survey\non evaluation of large language models,” ACM Trans.\nIntell. Syst. Technol., vol. 15, no. 3, mar 2024. Link:\nhttps://doi.org/10.1145/3641289\n[25] X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu,\nY. Yao, A. Zhang, L. Zhang, W. Han, M. Huang, Q. Jin,\nY. Lan, Y. Liu, Z. Liu, Z. Lu, X. Qiu, R. Song, J. Tang,\nJ.-R. Wen, J. Yuan, W. X. Zhao, and J. Zhu, “Pre-trained\nmodels: Past, present and future,” AI Open, vol. 2, pp.\n225–250, 2021. Link: https://www.sciencedirect.com/\nscience/article/pii/S2666651021000231\n[26] S.\nEe\nand\nZ.\nWilliams,\n“Adapting\ncybersecurity\nframeworks to manage frontier AI risks,” Institute for AI\nPolicy and Strategy (IAPS), 2023. Link: https://static1.\nsquarespace.com/static/64edf8e7f2b10d716b5ba0e1/t/\n6528c5c7f912f74fbd03fc34/1697170896984/Adapting+\ncybersecurity+frameworks+to+manage+frontier+AI+\nrisks.pdf\n[27] Q. Lu, L. Zhu, X. Xu, Y. Liu, Z. Xing, and J. Whittle, “A\ntaxonomy of foundation model based systems through\nthe lens of software architecture,” in Proceedings of\nthe IEEE/ACM 3rd International Conference on AI\nEngineering-Software Engineering for AI, 2024, pp.\n1–6. Link: https://doi.org/10.48550/arXiv.2305.05352\n[28] X. Tang, Q. Jin, K. Zhu, T. Yuan, Y. Zhang, W. Zhou,\nM. Qu, Y. Zhao, J. Tang, Z. Zhang et al., “Prioritizing\nsafeguarding over autonomy: Risks of LLM agents for\nscience,” arXiv preprint arXiv:2402.04247, 2024. Link:\nhttps://doi.org/10.48550/arXiv.2402.04247\n[29] S.\nG.\nAyyamperumal\nand\nL.\nGe,\n“Current\nstate\nof\nLLM\nrisks\nand\nAI\nguardrails,”\narXiv\npreprint\narXiv:2406.12934, 2024. Link: https://doi.org/10.48550/\narXiv.2406.12934\n[30] Q. Lu, L. Zhu, X. Xu, Z. Xing, S. Harrer, and\nJ. Whittle, “Building the future of responsible AI:\nA reference architecture for designing large language\nmodel\nbased\nagents,”\narXiv\ne-prints,\n2023.\nLink:\nhttps://doi.org/10.48550/arXiv.2311.13148\n[31] Y. Bengio et al., “Managing extreme AI risks amid\nrapid\nprogress,”\nScience,\nvol.\n384,\nno.\n6698,\npp.\n842–845, 2024. Link: https://www.science.org/doi/abs/\n10.1126/science.adn0117\n[32] L. Weidinger, M. Rauh, N. Marchal, A. Manzini,\nL. A. Hendricks, J. Mateos-Garcia, S. Bergman, J. Kay,\nC. Griffin, B. Bariach et al., “Sociotechnical safety\nevaluation of generative ai systems,” arXiv preprint\narXiv:2310.11986, 2023. Link: https://doi.org/10.48550/\narXiv.2310.11986\n[33] A. E. Hassan, D. Lin, G. K. Rajbahadur, K. Gallaba,\nF. R. Cogo, B. Chen, H. Zhang, K. Thangarajah, G. A.\nOliva, J. Lin et al., “Rethinking software engineering\nin the era of foundation models: A curated catalogue\nof\nchallenges\nin\nthe\ndevelopment\nof\ntrustworthy\nfmware,” arXiv preprint arXiv:2402.15943, 2024. Link:\nhttps://doi.org/10.48550/arXiv.2402.15943\n[34] M. Mylrea and N. Robinson, “Artificial intelligence\n(ai) trust framework and maturity model: Applying\nan entropy lens to improve security, privacy, and\nethical ai,” Entropy, vol. 25, no. 10, 2023. Link:\nhttps://www.mdpi.com/1099-4300/25/10/1429\n[35] OpenAI,\n“OpenAI’s\nmoderation\nAPI,”\n2024,\nLast\naccessed on Jul.-2024. Link: https://platform.openai.\ncom/docs/guides/moderation/overview\n[36] Z. Xiang et al., “GuardAgent: Safeguard llm agents\nby a guard agent via knowledge-enabled reasoning,”\narXiv preprint arXiv:2406.09187, 2024. Link: https:\n//doi.org/10.48550/arXiv.2406.09187\n[37] P. Rai, S. Sood, V. K. Madisetti, and A. Bahga,\n“Guardian:\nA\nmulti-tiered\ndefense\narchitecture\nfor\nthwarting prompt injection attacks on LLMs,” Journal\nof Software Engineering and Applications, vol. 17,\nno. 1, pp. 43–68, 2024. Link: https://doi.org/10.4236/\njsea.2024.171003\n[38] T. Bi, G. Yu, Q. Lu, X. Xu, and N. Van Beest, “The\nprivacy pillar - A conceptual framework for foundation\nmodel-based systems,” arXiv preprint arXiv:2311.06998,\n2023. Link: https://doi.org/10.48550/arXiv.2311.06998\n[39] M. Petticrew and H. Roberts, Systematic reviews in the\nsocial sciences: A practical guide.\nJohn Wiley & Sons,\n2008. Link: https://doi.org/10.1002/9780470754887\n[40] B. A. Kitchenham, S. Charters, and Other Keele Staffs,\n“Guidelines for performing systematic literature reviews\nin software engineering (version 2.3),” Keele University\nand\nDurham\nUniversity\nJoint\nReport,\nTech.\nRep.,\n2007.\nLink:\nhttps://www.elsevier.com/\ndata/promis\nmisc/525444systematicreviewsguide.pdf\n[41] B. Kitchenham, “Procedures for performing systematic\nreviews,” Keele University, UK, Tech. Rep. 2004, 2004.\nLink: http://artemisa.unicauca.edu.co/∼ecaldon/docs/spi/\n\nkitchenham 2004.pdf\n[42] A. Paez, “Gray literature: An important resource in sys-\ntematic reviews,” Journal of Evidence-Based Medicine,\nvol. 10, no. 3, pp. 233–240, 2017.\n[43] K. Godin, J. Stapleton, S. I. Kirkpatrick, R. M. Han-\nning, and S. T. Leatherdale, “Applying systematic review\nsearch methods to the grey literature: a case study exam-\nining guidelines for school-based breakfast programs in\ncanada,” Systematic reviews, vol. 4, pp. 1–10, 2015.\n[44] L. Schmidt, A. Finnerty Mutlu, R. Elmore, B. Olorisade,\nJ. Thomas, and J. Higgins, “Data extraction methods\nfor systematic review (semi)automation: Update of a\nliving systematic review,” F1000Research, vol. 10, no.\n401, 2023. Link: https://doi.org/10.12688/f1000research.\n51117.2\n[45] V. Rawte, A. Sheth, and A. Das, “A survey of\nhallucination in large foundation models,” arXiv preprint\narXiv:2309.05922, 2023. Link: https://doi.org/10.48550/\narXiv.2309.05922\n[46] OpenAI, “OpenAI safety update,” 2024. Link: https:\n//openai.com/index/openai-safety-update/\n[47] S. Torkington, “These are the 3 biggest emerging risks\nthe world is facing,” World Economic Forum, Tech.\nRep.,\n2024.\nLink:\nhttps://www.weforum.org/agenda/\n2024/01/ai-disinformation-global-risks/\n[48] C.\nHutton,\n“Silicon\nvalley\nself-regulates\nfor\nAI\nmisinformation in 2024 elections while government\nlags,”\n2024.\nLink:\nhttps://www.washingtonexaminer.\ncom/news/2803412/silicon-valley-self-regulates-ai-\nmisinformation-in-2024-government-lags/\n[49] S. Goyal, M. Hira, S. Mishra, S. Goyal, A. Goel,\nN. Dadu, D. Kirushikesh, S. Mehta, and N. Madaan,\n“LLMGuard: guarding against unsafe LLM behavior,”\nin Proceedings of the AAAI Conference on Artificial\nIntelligence, vol. 38(21), 2024, pp. 23 790–23 792. Link:\nhttps://doi.org/10.1609/aaai.v38i21.30566\n[50] S. Ray, “Samsung bans ChatGPT among employees after\nsensitive code leak,” 2023, News article, Last accessed\non\nJul.-2024.\nLink:\nhttps://www.forbes.com/sites/\nsiladityaray/2023/05/02/samsung-bans-chatgpt-and-\nother-chatbots-for-employees-after-sensitive-code-leak/\n[51] W. Du, Q. Li, J. Zhou, X. Ding, X. Wang, Z. Zhou,\nand J. Liu, “Finguard: A multimodal AIGC guardrail\nin financial scenarios,” in Proceedings of the 5th ACM\nInternational Conference on Multimedia in Asia, Taiwan,\n2024, pp. 1–3. Link: https://doi.org/10.1145/3595916.\n3626351\n[52] T. Zemˇc´ık, “Failure of chatbot Tay was evil, ugliness\nand\nuselessness\nin\nits\nnature\nor\ndo\nwe\njudge\nit through cognitive shortcuts and biases?” AI &\nSOCIETY, vol. 36, pp. 361–367, 2021. Link: https:\n//doi.org/10.1007/s00146-020-01053-4\n[53] Z.\nYuan,\nZ.\nXiong,\nY.\nZeng,\nN.\nYu,\nR.\nJia,\nD. Song, and B. Li, “RigorLLM: Resilient guardrails\nfor large language models against undesired content,”\narXiv preprint arXiv:2403.13031, 2024. Link: https:\n//doi.org/10.48550/arXiv.2403.13031\n[54] S. Banerjee, S. Layek, R. Hazra, and A. Mukherjee,\n“How (un) ethical are instruction-centric responses of\nLLMs? unveiling the vulnerabilities of safety guardrails\nto harmful queries,” arXiv preprint arXiv:2402.15302,\n2024. Link: https://doi.org/10.48550/arXiv.2402.15302\n[55] J.\nZhao,\nK.\nChen,\nX.\nYuan,\nY.\nQi,\nW.\nZhang,\nand N. Yu, “Silent guardian: Protecting text from\nmalicious\nexploitation\nby\nlarge\nlanguage\nmodels,”\narXiv preprint arXiv:2312.09669, 2023. Link: https:\n//doi.org/10.48550/arXiv.2312.09669\n[56] X. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang,\n“”do anything now”: Characterizing and evaluating in-\nthe-wild jailbreak prompts on large language models,”\narXiv preprint arXiv:2308.03825, 2023. Link: https:\n//doi.org/10.48550/arXiv.2308.03825\n[57] M. Liffiton, B. E. Sheese, J. Savelka, and P. Denny,\n“Codehelp: Using large language models with guardrails\nfor\nscalable\nsupport\nin\nprogramming\nclasses,”\nin\nProceedings of the 23rd Koli Calling International\nConference on Computing Education Research, Finland,\n2024, pp. 1–11. Link: https://doi.org/10.1145/3631802.\n3631830\n[58] Z. Wang, F. Yang, L. Wang, P. Zhao, H. Wang, L. Chen,\nQ. Lin, and K.-F. Wong, “SELF-GUARD: Empower\nthe LLM to safeguard itself,” in Proceedings of the\n2024 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, Mexico, 2024, pp. 1648–1668.\nLink: https://aclanthology.org/2024.naacl-long.92\n[59] A. Khorramrouz, S. Dutta, A. Dutta, and A. R. Khud-\naBukhsh, “Down the toxicity rabbit hole: Investigating\nPaLM 2 guardrails,” arXiv preprint arXiv:2309.06415,\n2023. Link: https://doi.org/10.48550/arXiv.2309.06415\n[60] N. Mangaokar, A. Hooda, J. Choi, S. Chandrashekaran,\nK. Fawaz, S. Jha, and A. Prakash, “PRP: Propagating\nuniversal perturbations to attack large language model\nguard-rails,” arXiv preprint arXiv:2402.15911, 2024.\nLink: https://doi.org/10.48550/arXiv.2402.15911\n[61] P. Gajane and M. Pechenizkiy, “On formalizing fairness\nin prediction with machine learning,” arXiv preprint\narXiv:1710.03184, 2017. Link: https://doi.org/10.48550/\narXiv.1710.03184\n[62] R. R. Llaca, V. Leskoschek, V. C. Paiva, C. Lup˘au,\nP. Lippmann, and J. Yang, “Student-teacher prompting\nfor red teaming to improve guardrails,” in Proceedings\nof the ART of Safety: Workshop on Adversarial testing\nand Red-Teaming for generative AI, 2023, pp. 11–23.\nLink: http://dx.doi.org/10.18653/v1/2023.artofsafety-1.2\n[63] W.\nLi\net\nal.,\n“Segment\nanything\nmodel\ncan\nnot\nsegment anything: Assessing AI foundation model’s\ngeneralizability in permafrost mapping,” Remote Sensing,\nvol. 16, no. 5, p. 797, 2024. Link: https://doi.org/10.\n48550/arXiv.2401.08787\n[64] Z. Zhang, Y. Lu, J. Ma, D. Zhang, R. Li, P. Ke, H. Sun,\nL. Sha, Z. Sui, H. Wang et al., “ShieldLm: Empowering\n\nLLMs as aligned, customizable and explainable safety\ndetectors,” arXiv preprint arXiv:2402.16444, 2024. Link:\nhttps://doi.org/10.48550/arXiv.2402.16444\n[65] H.-I.\nKim,\nK.\nYun,\nJ.-S.\nYun,\nand\nY.\nBae,\n“Customizing\nsegmentation\nfoundation\nmodel\nvia\nprompt\nlearning\nfor\ninstance\nsegmentation,”\narXiv\npreprint\narXiv:2403.09199,\n2024.\nLink:\nhttps:\n//doi.org/10.48550/arXiv.2403.09199\n[66] R. Y. Wong, A. Chong, and R. C. Aspegren, “Privacy\nlegislation as business risks: How GDPR and CCPA\nare represented in technology companies’ investment\nrisk disclosures,” Proceedings of the ACM on Human-\nComputer Interaction, vol. 7, no. CSCW1, pp. 1–26,\n2023. Link: https://doi.org/10.1145/3579515\n[67] Z. Chu, Y. Wang, L. Li, Z. Wang, Z. Qin, and K. Ren,\n“A causal explainable guardrails for large language\nmodels,” arXiv preprint arXiv:2405.04160, 2024. Link:\nhttps://doi.org/10.48550/arXiv.2405.04160\n[68] M. Shanahan, “Talking about large language models,”\nCommun. ACM, vol. 67, no. 2, p. 68–79, jan 2024.\nLink: https://doi.org/10.1145/3624724\n[69] Y. Zeng, H. Lin, J. Zhang, D. Yang, R. Jia, and\nW. Shi, “How johnny can persuade LLMs to jailbreak\nthem: Rethinking persuasion to challenge ai safety by\nhumanizing LLMs,” arXiv preprint arXiv:2401.06373,\n2024. Link: https://doi.org/10.48550/arXiv.2401.06373\n[70] A. Kumar, C. Agarwal, S. Srinivas, S. Feizi, and\nH. Lakkaraju, “Certifying LLM safety against adversarial\nprompting,” arXiv preprint arXiv:2309.02705, 2023.\nLink: https://doi.org/10.48550/arXiv.2309.02705\n[71] D. Dalrymple et al., “Towards guaranteed safe AI:\nA framework for ensuring robust and reliable AI\nsystems,” arXiv preprint arXiv:2405.06624, 2024. Link:\nhttps://doi.org/10.48550/arXiv.2405.06624\n[72] M. Pawagi and V. Kumar, “Guardrails: Automated\nsuggestions\nfor\nclarifying\nambiguous\npurpose\nstatements,” in Proceedings of the 16th Annual ACM\nIndia Compute Conference, 2023, p. 55–60. Link:\nhttps://doi.org/10.1145/3627217.3627234\n[73] W. Zou, R. Geng, B. Wang, and J. Jia, “PoisonedRAG:\nKnowledge poisoning attacks to retrieval-augmented\ngeneration of large language models,” arXiv preprint\narXiv:2402.07867, 2024. Link: https://doi.org/10.48550/\narXiv.2402.07867\n[74] J.\nHua\nand\nP.\nWang,\n“Security\nvulnerabilities\nin\nfacebook\ndata\nbreach,”\nin\nInternational\nConference\non\nInformation\nTechnology-New\nGenerations.\nSpringer,\n2024,\npp.\n159–166.\nLink: https://doi.org/10.1007/978-3-031-56599-1 22\n[75] J.\nLarouzee\nand\nJ.-C.\nLe\nCoze,\n“Good\nand\nbad\nreasons:\nThe\nswiss\ncheese\nmodel\nand\nits\ncritics,” Safety Science, vol. 126, p. 104660, 2020.\nLink:\nhttps://www.sciencedirect.com/science/article/pii/\nS0925753520300576\n[76] T. Shabani, S. Jerie, and T. Shabani, “A comprehensive\nreview of the swiss cheese model in risk management,”\nSafety in Extreme Environments, vol. 6, no. 1, pp. 43–57,\n2024.\n[77] B. Xia, Q. Lu, L. Zhu, and Z. Xing, “Towards AI safety:\nA taxonomy for AI system evaluation,” arXiv preprint\narXiv:2404.05388, 2024.\n[78] L. Bass, Q. Lu, I. Weber, and L. Zhu, Engineering AI\nSystems: Architecture and DevOps Essentials. Addison-\nWesley, 2025.\n[79] K. Valmeekam, K. Stechly, and S. Kambhampati, “Llms\nstill can’t plan; can lrms? a preliminary evaluation of ope-\nnai’s o1 on planbench,” arXiv preprint arXiv:2409.13373,\n2024.\n[80] Q. Lu, X. Xu, L. Bass, L. Zhu, and W. Zhang, “A\ntail-tolerant cloud api wrapper,” IEEE Software, vol. 32,\nno. 1, pp. 76–82, 2015.\n[81] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia,\nE. Chi, Q. V. Le, D. Zhou et al., “Chain-of-thought\nprompting elicits reasoning in large language models,”\nAdvances in neural information processing systems,\nvol. 35, pp. 24 824–24 837, 2022.\n[82] Y. Liu, G. Deng, Y. Li, K. Wang, Z. Wang, X. Wang,\nT. Zhang, Y. Liu, H. Wang, Y. Zheng et al., “Prompt in-\njection attack against llm-integrated applications,” arXiv\npreprint arXiv:2306.05499, 2023.\n[83] Z. Chen, Z. Xiang, C. Xiao, D. Song, and B. Li, “Agent-\npoison: Red-teaming llm agents via poisoning memory\nor knowledge bases,” arXiv preprint arXiv:2407.12784,\n2024.\n[84] L. Dong, Q. Lu, and L. Zhu, “A taxonomy of agentops\nfor enabling observability of foundation model based\nagents,” arXiv preprint arXiv:2411.05285, 2024.\n[85] B. Yohsua, P. Daniel, B. Tamay, B. Rishi, C. Stephen,\nC. Yejin, G. Danielle, H. Hoda, K. Leila, L. Shayne,\nM. Vasilios, M. Mantas, N. Kwan Yee, O. Chinasa T.,\nR. Deborah, S. Theodora, T. Florian, and M. Soren,\n“International\nScientific\nReport\non\nthe\nSafety\nof\nAdvanced AI,” Department for Science, Innovation\nand Technology, Tech. Rep., May 2024. Link: https:\n//hal.science/hal-04612963\n[86] J. Sun, Z. Xing, X. Xia, Q. Lu, X. Xu, and L. Zhu,\n“Aspect-level information discrepancies across heteroge-\nneous vulnerability reports: Severity, types and detection\nmethods,” ACM Transactions on Software Engineering\nand Methodology, vol. 33, no. 2, pp. 1–38, 2023.",
    "pdf_filename": "Designing_Multi-layered_Runtime_Guardrails_for_Foundation_Model_Based_Agents_Swiss_Cheese_Model_for_.pdf"
}