{
    "title": "ShiftAddLLM Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization",
    "context": "Large language models (LLMs) have shown impressive performance on language tasks but face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, resulting in high memory demands and latency bottlenecks. Shift-and-add reparameterization offers a promising solution by replacing costly multiplications with hardware-friendly primitives in both the attention and multi-layer perceptron (MLP) layers of an LLM. However, current reparameterization techniques require training from scratch or full parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs. To address this, we propose accelerating pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbed ShiftAddLLM. Specifically, we quantize each weight matrix into binary matrices paired with group-wise scaling factors. The associated multiplications are reparameterized into (1) shifts between activations and scaling factors and (2) queries and adds according to the binary matrices. To reduce accuracy loss, we present a multi-objective optimization method to minimize both weight and output activation reparameterization errors. Additionally, based on varying sensitivity across layers to reparameterization, we develop an automated bit allocation strategy to further reduce memory usage and latency. Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity reductions of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3- and 2-bit precision, respectively, and more than 80% memory and energy reductions over the original LLMs. Codes and models are available at https://github.com/GATECH-EIC/ ShiftAddLLM. 1 Pretrained LLMs have demonstrated state-of-the-art performance in language understanding and generation tasks [46, 47, 59, 3, 74, 57, 58, 2]. However, deploying these LLMs incurs significant hardware demands, including high latency, memory, and energy consumption, especially on edge or cloud GPU devices. The primary bottlenecks are their immense parameter sizes and the associated multiplication operations. For instance, GPT-3, with 175 billion parameters, requires 350GB of memory in FP16 format [38] and performs 1015 floating-point operations (FLOPs) for a single forward pass [19]. Previous efforts to improve LLM efficiency have focused on pruning [40, 55, 20, 24, 44], quantization [63, 38, 18, 48], and attention optimization [12, 71, 67]. However, these methods still rely on costly multiplication operations in both the attention and MLP layers. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2406.05981v4  [cs.LG]  18 Nov 2024",
    "body": "ShiftAddLLM: Accelerating Pretrained LLMs via\nPost-Training Multiplication-Less Reparameterization\nHaoran You†, Yipin Guo†, Yichao Fu†, Wei Zhou†, Huihong Shi†, Xiaofan Zhang∗\nSouvik Kundu⋄, Amir Yazdanbakhsh‡, Yingyan (Celine) Lin†\n†Georgia Institute of Technology\n⋄Intel Labs\n∗Google\n‡Google DeepMind\n†{haoran.you, celine.lin}@gatech.edu, eic-lab@groups.gatech.edu\n⋄souvikk.kundu@intel.com, ∗‡{xiaofanz, ayazdan}@google.com\nAbstract\nLarge language models (LLMs) have shown impressive performance on language\ntasks but face challenges when deployed on resource-constrained devices due to\ntheir extensive parameters and reliance on dense multiplications, resulting in high\nmemory demands and latency bottlenecks. Shift-and-add reparameterization offers\na promising solution by replacing costly multiplications with hardware-friendly\nprimitives in both the attention and multi-layer perceptron (MLP) layers of an LLM.\nHowever, current reparameterization techniques require training from scratch or\nfull parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs.\nTo address this, we propose accelerating pretrained LLMs through post-training\nshift-and-add reparameterization, creating efficient multiplication-free models,\ndubbed ShiftAddLLM. Specifically, we quantize each weight matrix into binary\nmatrices paired with group-wise scaling factors. The associated multiplications\nare reparameterized into (1) shifts between activations and scaling factors and (2)\nqueries and adds according to the binary matrices. To reduce accuracy loss, we\npresent a multi-objective optimization method to minimize both weight and output\nactivation reparameterization errors. Additionally, based on varying sensitivity\nacross layers to reparameterization, we develop an automated bit allocation strategy\nto further reduce memory usage and latency. Experiments on five LLM families\nand eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving\naverage perplexity reductions of 5.6 and 22.7 points at comparable or lower latency\ncompared to the most competitive quantized LLMs at 3- and 2-bit precision,\nrespectively, and more than 80% memory and energy reductions over the original\nLLMs. Codes and models are available at https://github.com/GATECH-EIC/\nShiftAddLLM.\n1\nIntroduction\nPretrained LLMs have demonstrated state-of-the-art performance in language understanding and\ngeneration tasks [46, 47, 59, 3, 74, 57, 58, 2]. However, deploying these LLMs incurs significant\nhardware demands, including high latency, memory, and energy consumption, especially on edge or\ncloud GPU devices. The primary bottlenecks are their immense parameter sizes and the associated\nmultiplication operations. For instance, GPT-3, with 175 billion parameters, requires 350GB of\nmemory in FP16 format [38] and performs 1015 floating-point operations (FLOPs) for a single forward\npass [19]. Previous efforts to improve LLM efficiency have focused on pruning [40, 55, 20, 24, 44],\nquantization [63, 38, 18, 48], and attention optimization [12, 71, 67]. However, these methods still\nrely on costly multiplication operations in both the attention and MLP layers.\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2406.05981v4  [cs.LG]  18 Nov 2024\n\nTable 1: Hardware cost under 45nm CMOS [27, 69, 23, 50, 5].\nOPs\nMultiplication\nAdd\nShift\nLUTs\n(8-bit Query)\nFP32\nFP16\nINT32\nINT8\nFP32\nFP16\nINT32\nINT8\nINT32\nINT16\nINT8\nEnergy (pJ)\n3.7\n0.9\n3.1\n0.2\n1.1\n0.4\n0.1\n0.03\n0.13\n0.057\n0.024\n0.37 (8 OPs)\nArea (µm2)\n7700\n1640\n3495\n282\n4184\n1360\n137\n36\n157\n73\n34\n787 (8 OPs)\n* Note that 1 LUT corresponds to 8 operations, as each bit in queries is from a weight element.\nWe identify a promising yet unexplored opportunity for improving LLM efficiency: reparameterizing\ntheir extensive multiplications with more cost-effective hardware substitutes, such as bitwise shifts\nand adds. Inspired by practices in computer architecture and digital signal processing, replacing\nmultiplications with bitwise shifts and adds [66, 22] can offer up to 3.1/0.1 = 31× energy and\n3495/137 ≈26× area reductions (see Tab. 1). This hardware-inspired approach can lead to efficient\nand fast implementations, as shown by previous research on ShiftAddNet [69, 70, 72]. Unlike\nprevious techniques that require training from scratch or extensive fine-tuning, we propose a new\nmethod to integrate the shift-and-add concept into LLMs through post-training optimization.\nTo design multiplication-less LLMs, we need to address three key challenges: First, how can\nwe effectively reparameterize pretrained LLMs with shifts and adds in a post-training manner?\nPrevious reparameterization techniques [69, 72] can result in nontrivial quantization errors, requiring\nfine-tuning or retraining to avoid accuracy drops. We aim to develop a ready-to-use post-training\nreparameterization method for LLMs. Second, how can we mitigate the accuracy drop from shift-\nand-add reparameterization? Approximating original multiplications with lower-bit shifts and adds\ntypically reduces model accuracy. Most studies resort to fine-tuning or increasing model sizes,\ncomplicating LLM deployment. We hypothesize that optimizing both weight and activation errors can\nminimize overall reparameterization error, aligning with recent activation-aware weight quantization\nmethods in LLMs. Third, how can we handle varying sensitivities to reparameterization across\ndifferent layers and blocks in LLMs? An automated strategy to determine the optimal number of\nbits for reparameterized weights in each layer is needed. More vulnerable layers should have higher-\nbit representations, while less sensitive layers can use lower-bit representations. This ensures no\nbottlenecked layers due to aggressive reparameterization and maximizes redundancy exploitation. To\nthe best of our knowledge, this is the first attempt to address these three challenges for multiplication-\nless LLMs through post-training reparameterization. Our contributions are summarized as follows:\n• We propose accelerating pretrained LLMs via a post-training bitwise shift-and-add reparameter-\nization, resulting in efficient multiplication-less LLMs, dubbed ShiftAddLLM. All weights are\nquantized into binary matrices paired with group-wise scaling factors; the associated multiplications\nare reparameterized into shift-and-add operations.\n• To mitigate accuracy loss, we present a multi-objective optimization method aligning and optimizing\nboth weight and output activation objectives, minimizing overall reparameterization error, and\nachieving lower perplexity and better task accuracy.\n• We introduce a mixed and automated bit allocation strategy that determines the optimal number of\nbits for reparameterized weights per layer, based on their vulnerability to compression. Susceptible\nlayers receive higher-bit representations, while less sensitive ones get lower-bit representations.\nOur extensive results across five LLMs and eight tasks consistently show the superior accuracy and\nefficiency trade-offs achieved by ShiftAddLLM, with average perplexity reductions of 5.6 and 22.7 at\ncomparable or even lower latency compared to the most competitive quantized LLMs at three and\ntwo bits, respectively, and more than 80% memory and energy reductions over the original LLMs.\n2\nRelated Works\nLLM Quantization. Significant efforts have been made to quantize LLMs, including quantization-\naware training (QAT) [39, 52] and post-training quantization (PTQ) [18, 38, 63, 15]. QAT requires\ncalibrated data and significant retraining resources, whereas PTQ is more dominant due to it lower\ncomputational and time overhead. There are two prevalent PTQ strategies for LLMs: (1) uniform\nquantization of both weights and activations [63, 15, 68], often limited to 8 bits (W8A8) as lower bit\nrepresentations can significantly reduce accuracy; and (2) lower bit weight-only quantization [18, 48,\n14, 28, 6], which quantizes LLM weights to lower bits while keeping activations in a FP16 format.\n2\n\nThis approach alleviates memory bottlenecks associated with the vast parameters of LLMs. For\ninstance, GPTQ [18] uses gradient-based weight quantization and develops INT3/4 kernels to reduce\ndata movements, and LUT-GEMM [48] eliminates the dequantization and uses custom LUT-based\nCUDA kernels to reduce memory and computation costs. In contrast, ShiftAddLLM is the first\nto employ the shift-and-add idea for reparameterizing pre-trained LLMs. This reparameterization\nreduces bit usage for weights and replaces costly multiplications with hardware-friendly primitives,\nfurther reducing energy, latency, and memory.\nMultiplication-less Models. The efficient model community has focused on reducing or replacing\nmultiplications. In CNNs, binary networks [10, 32] binarize weights and activations, while shift-based\nnetworks use spatial shifts [62] or bitwise shifts [16] to substitute for multiplications. AdderNet [7,\n65, 61] replaces multiplications with additions, albeit with a small accuracy drop. ShiftAddNet [69]\nreparameterizes CNNs with cascaded shift and add layers. These techniques have been adapted to\nTransformers. BiLLM [28] introduces binary LLMs, while [54] and [60] extend the addition or shift\nconcepts to the attention mechanisms, respectively. ShiftAddViT [72] reparameterizes pretrained\nVision Transformers (ViTs) with shifts and adds. Contemporary work MatMul-free LM [76] leverages\nadditive operators and Hadamard products for multiplication-free language model training, relying\non FPGAs for speedups. Compared to closely related works like ShiftAddNet [69] and MatMul-free\nLM [76], which requires training from scratch, and ShiftAddViT [72], which demands extensive\nparameter fine-tuning, ShiftAddLLM applies the shift-and-add concept to pre-trained LLMs without\nadditional training or fine-tuning. We also use a multi-objective optimization and automated bit\nallocation strategy to further improve accuracy or reduce GPU latency, energy, and memory usage.\n3\nPreliminaries\nAlgorithm 1 Alternating Multi-bit BCQ [64]\n1: Input: Full-precision weight w ∈Rn,\nbit-width q, alternating cycles T\n2: Output: α∗\ni , b∗\ni ∈{−1, 1}m×n\n3: Function MULTI-BIT BCQ(w, q, T)\n4:\n{αi, bi}q\ni=1 ←GREEDY(w)\n5:\nfor t ←1 to T do\n6:\n{αi}q\ni=1 ←LS(B, w)\n7:\n{bi}q\ni=1 ←BS(α1, . . . , αq, w)\n8:\nend for\n9: end Function\nBinary-coding Quantization (BCQ). BCQ [64]\nquantizes each weight tensor in an L-layer LLM\nw ∈Rm×n into q bits using a linear combination\nof binary matrices {bi}q\ni=1 and corresponding scal-\ning factors {αi}q\ni=1, where bi ∈{−1, 1}m×n. The\nweights are then approximated by wq = Pq\ni=1 αibi\nas a result of minimizing the quantization error, i.e.,\narg minαi,bi ∥w −Pq\ni=1 αibi∥2 to obtain the opti-\nmal α∗\ni , b∗\ni . If q is 1, then the problem collapses to\nbinary quantization, which has an analytical solution:\nb∗= sign(w), α∗= w⊤b∗/n. For multi-bit quan-\ntization, we resort to greedy and alternating meth-\nods [64, 30, 33], as shown in Alg. 1. Initially, we use\nthe greedy method [21] to initialize αi, bi, where the\ni-th bit quantization is performed by minimizing the\nresidual r from the (i −1)-th bit:\nmin\nαi,bi ∥ri−1 −αibi∥2,\nwhere\nri−1 = w −\ni−1\nX\nj=1\nαjbj,\n1 < i ≤q.\n(1)\nWe then obtain the initialized αi, bi sequentially as bi = sign(ri) and αi = r⊤\ni bi/n (Line 4). Next,\nwe perform alternating optimization to further minimize the quantization error. Specifically, {αi}q\ni=1\ncan be iteratively refined using ordinary least squares (LS) [21] as [α1, ..., αq] = ((B⊤B)−1B⊤w)⊤,\nwhere B = [b1, ..., bq] ∈{−1, 1}m×n×q (Line 6). The binary codes {bi}q\ni=1 can then be iteratively\nrecalibrated using a binary search (BS) given the refined {αi}q\ni=1 (Line 7) [64].\nSuch BCQ can support both uniform and non-uniform quantization formats by adjusting the scaling\nfactors and biases accordingly [48]. Our ShiftAddLLM is built on top of BCQ but further replaces all\nassociated multiplications with lower-cost hardware substitutes (e.g., shifts, adds, and LUT queries).\nWe optimize not only the weight quantization error but also the output activation error, thereby\nachieving lower quantization bits along with savings in energy, memory, and computational costs.\nShift and Add Primitives. Direct hardware implementation of multiplications is often inefficient.\nUsing shift and add operations as “shortcuts” provides a more efficient alternative. Shifts, which are\n3\n\n...\n...\n...\n(d) Construct LUTs and Query&Add\n3.14\n-12.57\n-22\n=\nFP16 Shift Using Multi.\nFP16 Shift Using UINT16 Add\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n1\n0\n1\n0\n0\n1\n0\n0\n1\n0\n0\n0\n+\nSign\nExponent\nMantissa\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n0\n0\n(c) FP16 Shift\nKey Value\n0\n255\n...\n...\nLUT 1\nKey Value\n0\n255\n...\n...\nLUT 2\nKey Value\n0\n255\n...\n...\nLUT  \nQuery\n...\n...\n...\n...\n+\n+ +\n=\n...\n+\n+ +\n=\n...\n+\n+ +\n=\n...\n(b) Ours ShiftAddLLM\nOutput Act. O\n...\nAdd\n+\n+ +\n=\n...\nShifted \nAct.\n...\nConstruct\nLUTs\nb'00111111(d'63)b'00101010(d'42)\nb'01111011(d'123)\nBinary Weight  \n...\n...\n...\n...\n: {-1   0, +1   1}\n2\n1\n3\n(a) Previous Weight-\nonly Quantization\nDe-Quant\nMatMul. (FP16)\nX (FP16)\nW   (   4bit)\nO (FP16)\nW   (   4bit)\nShiftAddLLM\nO (FP16)\nX (FP16)\nBinary \nWeight   \nScaling \nFactor\nInput Act.\nX\nFP16 Shift\nConstruct\nLUTs\nQuery\n&Add\n8-bit Key\nShifted \nAct.\nOutput Act. O\nFigure 1: Illustration of our proposed post-training reparameterization for ShiftAddLLM.\nequivalent to multiplying by powers of two, offer a non-uniform quantization solution and can result\nin significant savings. For example, we tested matrix multiplication from one MLP layer of OPT-66B\nbetween weight W ∈R9216×36884 and activation A ∈R1×9216 using FP16 MACs and our 3-bit\nShiftAddLLM. Energy consumption was 80.36J vs. 9.77J, achieving 87.8% savings with our method.\nBoth primitives have inspired many innovations in efficient model innovations [7, 16, 69, 72].\n4\nThe Proposed ShiftAddLLM Framework\nOverview. We introduce our ShiftAddLLM as follows: First, we describe the reparameterization of\npretrained LLMs through a post-training shift-and-add approach in Sec. 4.1. Second, to enhance\naccuracy, we introduce a multi-objective optimization method that accounts for both weight quantiza-\ntion error and output activation error, detailed in Sec. 4.2. Third, to improve efficiency, we explore a\nmixed and automated bit allocation strategy, illustrated in Sec. 4.3.\n4.1\nShiftAddLLM: Post-training Reparameterization of LLMs with Shift and Add Primitives\nPost-training Reparameterization of LLMs. To avoid the need for fine-tuning after reparameteriza-\ntion, our method closely mimics the original multiplications used in LLMs. Previous methods, such as\nweight-only quantization techniques [18], employ gradient-based or activation-aware uniform quan-\ntization to fit the pretrained weight distribution better, thereby achieving lower quantization errors.\nHowever, these methods often lack direct hardware support and require on-the-fly dequantization\nto FP16 for multiplication with activations, as depicted in Fig. 1 (a). In contrast, our ShiftAddLLM\nuses the BCQ format, supporting non-uniform quantization with customized CUDA kernels [48, 29],\nbypassing the need for dequantization, as illustrated in Fig. 1 (b). In particular, our method employs\nthe Alg. 1 to quantize pretrained weights into binary matrices {bi}q\ni=1 and scaling factors {αi}q\ni=1.\nNote that during the alternating optimization cycles, we further quantize all scaling factors to powers\nof two (PoT) [37], as described by the equation:\nαk = POT (rk−1) = POT(α −\nk−1\nX\nj=0\nαj),\nwhere\nPOT(α) = sign(α) · 2P,\n1 ≤k ≤K.\n(2)\nThis additive PoT method adopts a greedy strategy to enhance the representational capacity of PoT,\nusing K scaling factors, where the k-th PoT minimizes the residual r of the (k −1)-th PoT. Each\nPoT effectively quantizes the scaling factor α into sign(α) · 2P, where sign(α) indicates sign flips,\nP = round(log2(abs(α))), and 2P denotes a bitwise shift to the left (P > 0) or right (P < 0).\nAfter the above reparameterization, we can then replace the associated multiplication between weights\nand activations into two steps: (1) Bitwise shifts between activations and scaling factors. Note that\nthe activation is still in the FP16 format, and the multiplication between a floating-point number and\na positive or negative PoT integer can be efficiently implemented by an integer addition instruction\non existing hardware following DenseShift [36], as also illustrated in Fig. 1 (c); (2) Queries and\n4\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n=\nWeight\nInput Act.\nOutput Act.\n...\n+\n+\n+\n...\nOutput Act.\n...\n...\n(b) Activation Objective\n...\n...\n...\n...\n...\n...\nWeight\n...\nt=1:\nQuant. Error\n...\nt=2:\n...\nt=n:\n...\n...\n...\n...\n...\n...\n...\n...\n...\n=\nWeight\nInput Act.\nOutput Act.\n...\n...\nOutput Act.\n...\n...\n...\n...\n...\n...\n...\nWeight\n...\n...\nt=1:\nQuant. Error\n...\nt=2:\n...\nt=n:\n...\n...\n+\n...\n+\n+\n...\n...\n+\n+\n...\n...\n+\n+\n...\n...\n...\n...\n...\n...\n=\n(a) Weight Objective\n(c) Ours Multi-Objective Optimization\nFigure 2: Illustration of our proposed multi-objective optimization framework.\nadds intermediate shifted activations with the binary matrices. To implement this efficiently and\nreduce redundant additions or accumulations, as shown in Fig. 1 (d), we pre-compute 256 (= 28)\npossible values for every eight elements in the shifted activations to construct LUTs. Here every\neight grouped binary weights form an 8-bit key. Suppose the shifted activation is an n-dimensional\nvector. In that case, we will get n/8 LUTs, where the grouped binary weights are used as keys, and the\nprecomputed partial sums are stored as values. This allows us to handle the multiplication between\nthe binary matrix bi and the shifted activations as queries to the LUTs. We then add all the partial\nsums to obtain the final output activations in FP16 format. Such LUTs are well supported by existing\nGPU kernels [48, 29]. The reparameterization can be applied to all weights in pretrained LLMs in a\npost-training manner, replacing costly multiplications with efficient hardware operations.\nTakeaway. ShiftAddLLM presents a novel multiplication-less approach that leverages non-uniform\nquantization via BCQ and additive PoT. This methodology enhances the representation capacity for\noutlier weights and activations of large magnitude compared to uniform quantization. Moreover,\nadditive PoT effectively resolves the issue of limited quantization resolution for non-outlier weights\nand activations. Overall, it allows the quantization levels to better align with the data distribution.\n4.2\nShiftAddLLM: Multi-objective Optimization\nMotivating Analysis on Previous LLM Quantization Objectives. We examine previous weight-\nonly quantization methods to understand the causes of large quantization error and accuracy drop.\nThese methods typically use either a weight or activation objective to minimize quantization error.\nSpecifically, the “weight objective” (see Fig. 2 (a)) aims to minimize the weight quantization error,\ni.e., ∥W −Wq∥2, and adopts scaling factors for each row of quantized weights. However, this does\nnot optimize output activation error, as each weight element is multiplied by a unique input activation\nbefore summing to produce the output. Varying input activations, especially outliers [63, 38], rescale\nthe weight quantization error differently, causing significant divergence in the output activation.\nFor example, LUT-GEMM [48] adopts this weight objective. On the other hand, the “activation\nobjective” (see Fig. 2 (b)) minimizes the output activation error, i.e., ∥WX −WqX∥, by quantizing\none column of weights at a time and continuously updating the remaining unquantized weights to\ncompensate for the quantization error incurred by quantizing a single weight column. However, the\nfixed scaling factors may not adequately accommodate the weights adjusted afterward. OPTQ [18]\nemploys this activation objective.\nOur Multi-Objective Optimization. To further mitigate accuracy drop after reparameterization (see\nSec. 4.1), we introduce a multi-objective optimization framework that combines weight and activation\nobjectives using column-wise scaling factors. This framework effectively reduces quantization error\nfor both weights and activations, thereby improving the accuracy of ShiftAddLLM.\nAs shown in Fig. 2 (c), using column-wise scaling factors overcomes the limitations of the previous\nweight objective [48] by eliminating the impact of varying input activations on quantized weights.\n5\n\nEach scaling factor corresponds to a constant activation value. Additionally, scaling factors for\nsubsequent columns are updated gradually after compensating for the corresponding column’s\nweights, ensuring a better fit than the previous activation objective [18].\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n33.2 (✅)\n9.6 (✅) 44.1 (❌)\n9.9 (✅) 33.5 (✅)\nPPL (  ) Lat. (ms)\nRow-wise\nColumn-wise\nBlock-wise\nOPT-30B w/\n(a) Block-wise Scaling Factors\n(b) Design Comparisons\n16.3 (❌)\nFigure 3: (a) the block-wise scaling factors and (b) the compari-\nson among different designs on OPT-30B [74].\nAccuracy vs.\nLatency Trade-\noffs.\nThe column-wise scaling\nfactor design significantly boosts\naccuracy after reparameterization.\nHowever, it does not fully lever-\nage BCQ [48, 29], which process\neight elements per row of weights\nin parallel as LUT keys, resulting\nin latency overhead for models\nwith ≥30B parameters. For exam-\nple, testing on the OPT-30B [74]\nmodel and WikiText-2 dataset [41] showed (16.3 −9.6) = 6.7 perplexity reduction but with a\n(44.1−33.2)/44.1 ≈24.7% latency overhead, as shown in Fig. 3 (b).\nTo address this, we propose a block-wise scaling factor design that groups 8 columns and 1/8 of the\noriginal rows to share a scaling factor, ensuring compatibility with the BCQ kernel and achieving\nlatency reductions, as shown in Fig. 3 (a). We refer to ShiftAddLLM with column-wise scaling\nfactors as “Ours (Acc.)” for high accuracy optimization, and with block-wise scaling factors as “Ours\n(Lat.)” for optimized accuracy-latency trade-off.\nTakeaway. Our multi-objective optimization approach integrates both weight and activation ob-\njectives, reducing weight quantization error in an activation-aware manner and output activation\nerror reduction in a weight-aware manner. This synergy, achieved through a simple column-wise or\nblock-wise design, significantly boosts the accuracy of weight-only quantization. This aligns with the\nprinciples of previous activation-aware weight quantization methods [38].\n4.3\nShiftAddLLM: Mixed and Automated Bit Allocation\n1\n3\n5\n7\n9 11 13 15 17 19 21 23\nBlocks\n10\n3\n10\n2\n10\n1\nQuant. Error per Param.\n2 bits\n3 bits\n4 bits\nK\nV\nQ Out. FC1 FC2\nLinear Layers in a Block\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n2 bits\n3 bits\n4 bits\nFigure 4: Sensitivity analysis on OPT-1.3B [74].\nSensitivity Analysis. We analyze the sen-\nsitivity of different layers and blocks in\nLLMs to shift-and-add reparameterization.\nAs shown in Fig. 4, later blocks incur\nmore quantization or reparameterization\nerrors.\nWithin each block, Query/Key\n(Q/K) layers are generally more sensitive\nto reparameterization than other linear lay-\ners. This diverse sensitivity motivates us\nto explore mixed bit allocations for LLM\nreparameterization and develop strategies\nto automatically determine the optimal bit\nallocations given the average bit budgets.\n0\n50\n100\n150\n200\nRank w.r.t. Criteria\n0\n50\n100\n150\n200\nRank w.r.t. Reparam. Error\nKendall  = 0.905\nFigure 5: Rank comparisons.\nCriteria and Automated Bit Allocation. To develop the bit alloca-\ntion scheme, we propose criteria to estimate the importance of linear\nweights and formulate the bit allocation as an integer programming\nproblem. For weight Wi from the i-th layer of an LLM, the criterion\nCi is defined as follows:\nCi = ∥IS∥F · STD(IS)2,\nwhere\nIS = Wi/diag(cholesky((XiXT\ni )−1)),\n(3)\nwhere the importance score (IS) is inspired by Optimal Brain Com-\npression [25, 17, 18] and is correlated to the increase in the quadratic\nreconstruction error ∥WX −WqX∥2 after reparameterizing the\nweights, i.e., IS ↑, error increases ↓. The F-norm of IS indicates the\noverall importance of Wi, while the standard deviation (STD) high-\nlights the reparameterization difficulty for outliers. Considering both\nfactors, we achieve a more effective evaluation metric proportional\n6\n\nTable 2: Perplexity comparisons of the OPT models on WikiText-2. Note that we set the group size\nof all methods as the length of rows following the setting of OPTQ [18] for a fair comparison.\nOPT (PPL ↓)\nBits\n125M\n350M\n1.3B\n2.7B\n6.7B\n13B\n30B\n66B\nFP16\n16\n27.65\n22.00\n14.62\n12.47\n10.86\n10.13\n9.56\n9.34\nOPTQ [18]\n3\n53.85\n33.79\n20.97\n16.88\n14.86\n11.61\n10.27\n14.16\nLUT-GEMM [48]\n3\n60.00\n42.32\n49.10\n17.55\n17.44\n12.50\n139.90\n100.33\nAWQ [38]\n3\n54.75\n35416.00\n24.60\n39.01\n16.47\n16.53\n31.01\n5622.00\nOurs (Acc.)\n3\n31.29\n24.24\n21.53\n13.68\n11.18\n10.39\n9.63\n9.43\nOPTQ [18]\n2\n2467.50\n10433.30\n4737.05\n6294.68\n442.63\n126.09\n71.70\n20.91\nLUT-GEMM [48]\n2\n4844.32\n2042.90\n3851.50\n616.30\n17455.52\n4963.27\n7727.27\n6246.00\nAWQ [38]\n2\n3514.61\n18313.24\n9472.81\n22857.70\n8168.30\n5014.92\n7780.96\n103843.84\nQuIP [6]\n2\n92.84\n146.15\n27.90\n30.02\n16.30\n12.34\n11.48\n10.92\nOurs (Acc.)\n2\n51.15\n40.24\n29.03\n20.78\n13.78\n12.17\n10.67\n10.33\nto the actual reparameterization error. As shown in Fig. 5, the rankings derived from our defined\ncriteria and the actual reparameterization error are highly correlated, with a Kendall τ of 0.905. To\nrefine the criteria by incorporating the bit-width, we use least squares polynomial fits to estimate each\nbit’s corresponding reparameterization error as Ci,b.\nGiven the criteria, we can formulate the automated bit allocation as an integer programming problem:\narg min\nβi,b\nL\nX\ni\nX\nb\nβi,b · Ci,b,\ns.t.\nX\nb\nβi,b = 1,\nL\nX\ni\nX\nb\nβi,b · b ≤B · L,\n(4)\nwhere L is the number of layers in the target LLM, b ∈{2, 3, 4} denotes the available bit widths, and\nβi,b ∈{0, 1} is the one-hot indicator for the i-th layer to determine the assigned bits, e.g., {0, 1, 0}\nmeans 3 bits. The objective is to minimize the summed criteria C of all layers under the given average\nbit budget B per layer. The final βi,b represents the assigned bits for the i-th layer in the target LLM.\nTakeaway. Using mixed bits instead of static ones can improve the accuracy-efficiency tradeoffs by\nadapting the varying sensitivities across layers, e.g., Q/K linear layers exhibit higher sensitivity to\nreparameterization; Our adopted criteria provide a quick estimation of the reparameterization error.\n5\nExperiments\n5.1\nExperiment Settings\nModels. We consider five representative SOTA LLM families, including OPT [74], LLaMA-1/2/3 [58,\n2], Gemma [42], Mistral [31], and Bloom [49]. Tasks and Datasets. We evaluate all five LLMs\non the commonly adopted language modeling task using the WikiText-2 [41] dataset for perplexity\nmeasurement. Additionally, we extend the evaluation of the two largest models, OPT-66B and\nLLaMA-2-70B, to eight downstream tasks for zero-shot accuracy evaluation. These tasks include\nARC (Challenge/Easy) [4], BoolQ [9], Copa [1], PIQA [56], RTE [11], StoryCloze [43], and\nMMLU [26]. Baselines. We consider four SOTA LLM quantization methods: OPTQ [18], LUT-\nGEMM [48], QuIP [6], and AWQ [38]. Evaluation Metrics. We evaluate ShiftAddLLM and the\nbaselines using both accuracy and efficiency metrics. For accuracy, we evaluate perplexity on the\nWikiText-2 dataset and zero-shot accuracy on eight downstream tasks. For efficiency, we measure the\nlatency on a single A100-80GB GPU (PCIe) [45] and estimate the energy costs using an Eyeriss-like\nhardware accelerator [8, 75], which calculates not only computational but also data movement energy\n(within 18% of the differences with Eyeriss’s chip measurement results as claimed).\n5.2\nShiftAddLLM over SOTA LLM Quantization Baselines\nResults on OPT Models. To evaluate the effectiveness of our ShiftAddLLM, we compare against\nfour SOTA LLM quantization baselines: OPTQ [18], LUT-GEMM [48], QuIP [6], and AWQ [38].\nUsing the OPT model family [74] and the WikiText-2 dataset [41], we assess perplexity, GPU\nlatency, and energy costs. As shown in Tab. 2, Ours (Acc.) consistently outperforms all baselines,\nachieving an average perplexity reduction of 5.63/38.47/5136.13 compared to OPTQ, LUT-GEMM,\n7\n\n15\n20\n25\nLatency (ms)\n10\n15\n20\n25\n30\nPerplexity\n(a) OPT (3-bit)\n10\n20\n30\n40\n50\nLatency (ms)\n101\n102\n103\n104\n105\n(b) OPT (2-bit)\n25\n50\n75\nLatency (ms)\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n(c) LLaMA-2 (3-bit)\n50\n75\nLatency (ms)\n101\n102\n103\n104\n105\n106\n(d) LLaMA-3 (2-bit)\n18\n20\nLatency (ms)\n20\n25\n30\n35\n40\n45\n(e) Gemma (3-bit)\nOPTQ\nLUT-GEMM\nAWQ\nOurs (Lat. )\nFigure 6: Accuracy-latency tradeoff comparisons on the OPT, LLaMA-2/3, and Gemma models.\nand AWQ, respectively, at 3 bits. At 2 bits, where most baselines fail with significantly high perplexity,\nour method maintains low perplexity, and achieves an average 22.74 perplexity reduction over the\nmost competitive QuIP. Also, as shown in Fig. 6 (a & b), Ours (Lat.) consistently achieves better\naccuracy-latency tradeoffs, with a perplexity reduction of 0.91∼103830.45 at comparable latency\nor 6.5%∼60.1% latency reductions and 26.0%∼44.7% energy savings at similar or even lower\nperplexity. Complete quantitative data on accuracy, latency, and energy is provided in Appendix A.\nTable 3: Perplexity comparisons of the LLaMA models on\nWikiText-2. The group size is set to 128 following [48, 38].\nLLaMA (PPL ↓)\nBits LLaMA-1\nLLaMA-2\nLLaMA-3\n7B\n7B\n13B\n70B\n8B\n70B\nFP16\n16\n5.68\n5.47\n4.88\n3.32\n6.14\n2.86\nOPTQ [18]\n3\n8.81\n6.43\n5.48\n3.88\n13.69\n4.91\nLUT-GEMM [48]\n3\n7.18\n7.02\n5.89\n4.01\n11.10\n5.92\nAWQ [38]\n3\n6.35\n6.24\n5.32\n3.74\n8.15\n4.69\nOurs (Acc.)\n3\n6.04\n5.89\n5.16\n3.64\n7.20\n4.35\nOPTQ [18]\n2\n68.60\n19.92\n12.75\n6.82\n398.0\n26.65\nLUT-GEMM [48]\n2\n303.00\n2242.0 2791.0\n136.4\n19096\n3121\nAWQ [38]\n2\n2.6e5\n2.2e5\n1.2e5\n7.2e4\n1.7e6\n1.7e6\nOurs (Acc.)\n2\n7.98\n8.51\n6.77\n4.72\n12.07\n7.51\nResults on LLaMA Models. We\nfurther evaluate ShiftAddLLM on\nLLaMA models [57, 58, 2] due to\ntheir superior performance among\nopen-source LLMs.\nAs shown\nin Tab.\n3, Ours (Acc.)\nconsis-\ntently outperforms all baselines,\nachieving an average perplexity\nreduction of 1.82/1.47/0.29 and\n80.87/4606.98/678658.74\ncom-\npared to OPTQ, LUT-GEMM, and\nAWQ at 3 and 2 bits, respectively.\nEvaluating Ours (Lat.) with both\naccuracy\nand\nlatency\nmetrics\nas shown in Fig.\n6 (c & d),\nOurs (Lat.)\ndemonstrates better\naccuracy-latency tradeoffs. It achieves 1.1∼1719987.6 perplexity reduction at comparable latency\nor 19.9%∼65.0% latency reductions and 28.4%∼89.9% energy savings at similar or even lower\nperplexity. Complete quantitative data on accuracy, latency, and energy are provided in Appendix B.\nTable 4: Results on Gemma/Mistral/Bloom models.\nPPL (↓)\nBits Gemma-2B Mistral-7B Bloom-3B Bloom-7B\nFP16\n16\n13.88\n5.25\n13.48\n11.37\nOPTQ\n3\n26.08\n7.27\n17.40\n13.47\nLUT-GEMM\n3\n44.36\n22.36\n21.03\n17.29\nOurs (Acc.)\n3\n14.96\n5.60\n14.10\n11.71\nResults on Gemma/Mistral/Bloom\nModels.\nWe also evaluate Shif-\ntAddLLM on Gemma [42], Mis-\ntral [31], and Bloom [49] models,\nwhich are among the most popular\nopen-source LLMs and Mixture-of-\nExpert (MoE) models.\nAs shown\nin Tab. 4, Ours (Acc.) achieves per-\nplexity reductions of 11.12/29.4 for\nGemma-2B, 1.67/16.76 for Mistral-\n7B, and 3.30/6.93 and 1.76/5.58 for BLOOM-3B/7B, respectively, compared to OPTQ and LUT-\nGEMM. As shown in Fig. 6 (e), Ours (Lat.) shows better accuracy-latency tradeoffs, e.g., achieving\n9.56 perplexity reduction and 11% latency reductions over the OTPQ baseline on Gemma models.\nThese results on five LLM families consistently validate the effectiveness of our ShiftAddLLM.\nZero-shot Downstream Tasks. We extend our evaluation to zero-shot downstream datasets for a\nmore comprehensive assessment. As shown in Tab. 5, Ours (Acc.) consistently improves performance\nover previous SOTA baselines, achieving an average accuracy gain of 13.37/13.19 and 2.55/2.39\nover OPTQ and LUT-GEMM baselines at 3 bits when evaluated on OPT-66B and LLaMA-2-70B,\n8\n\nTable 5: Accuracy comparisons on seven downstream tasks for OPT-66B and LLaMA-2-70B.\nModels\nMethods\nBits\nARC_C\nARC_E\nCopa\nBoolQ\nPIQA\nStorycloze\nRTE\nMMLU\nMean\nOPT-66B\nFloating Point\n16\n37.20\n71.25\n86\n69.82\n78.67\n77.47\n60.65\n25.89±0.37\n63.37\nOPTQ [18]\n3\n24.66\n48.86\n70\n52.05\n64.47\n67.09\n53.07\n23.98±0.36\n50.52\nLUT-GEMM [48]\n3\n24.15\n51.85\n81\n53.52\n61.97\n60.60\n48.74\n23.73±0.36\n50.70\nOurs (Acc.)\n3\n35.24\n70.88\n87\n72.45\n77.64\n77.15\n63.18 27.56±0.38\n63.89\nLLaMA-2-70B\nFloating Point\n16\n49.57\n76.14\n90\n82.57\n80.79\n78.61\n68.23\n65.24±0.37\n72.89\nOPTQ [18]\n3\n45.82\n76.34\n90\n81.74\n79.71\n77.34\n67.51\n60.14±0.36\n72.33\nLUT-GEMM [48]\n3\n47.70\n76.42\n89\n80.31\n80.20\n77.78\n68.59\n-\n-\nOurs (Acc.)\n3\n48.38\n77.06\n93\n84.25\n80.47\n78.49\n75.09 62.33±0.38\n74.88\nTable 6: Perplexity and latency results of our mixed bit allocation.\nMethods\nBits\nPPL (↓)\nLatency (ms)\n125M\n350M\n1.3B\n2.7B\n6.7B\n13B\n125M\n350M 1.3B\n2.7B\n6.7B\n13B\nOurs (Lat.)\n2\n712.55 445.78 40.28\n50.95\n18.56\n14.76\n6.3\n12.4\n12.3\n16.9\n16.9\n20.9\nOurs (Mixed)\n2.2\n435.84\n279.19 27.37\n31.97\n17.99\n13.79\n6.3\n12.6\n12.5\n16.8\n16.7\n21.0\nrespectively. These experiments demonstrate that our method not only reduces perplexity but also\nimproves downstream task accuracy.\nGPU Memory Savings. Our ShiftAddLLM also reduces GPU memory usage. For OPT-66B, our\nmethod saves 81% and 87% memory costs over FP16 at 3 (23GB vs. 122GB) and 2 bits (16GB\nvs. 122GB), respectively. For LLaMA-2-70B, it saves 80% and 87% memory costs at 3 (25GB vs.\n128GB) and 2 bits (17GB vs. 128GB), respectively.\nResults of Mixed Bit Allocation. We evaluate our mixed bit allocation strategy (see Sec. 4.3) and\ncompare Ours (Mixed) with Ours (Lat.). As shown in Tab. 6, Ours (Mixed) further reduces the\nperplexity by an average of 79.45 for OPT model families under comparable or even less latency. We\nprovide more results in Appendix F to validate the effectiveness of our mixed bit allocation strategy.\n5.3\nAblation Studies of ShiftAddLLM\n1 3 5 7 9 11131517192123\nBlocks\n1.6\n1.8\n2.0\n2.2\n2.4\nAverage Bits\nK\nV\nQ Out.FC1FC2\nLinear Layers\n1.75\n2.00\n2.25\n2.50\nFigure 7: Visualizing the average bit allocation.\nVisualization of Mixed Bit Allocation. We\nvisualize the bit allocations after applying our\nautomated bit allocation strategy with an aver-\nage bit budget of 2.2 (Fig. 7). The allocation\npattern correlates with the sensitivity to reparam-\neterization identified in Sec. 4.3 and shown in\nFig. 4. For instance, later blocks, which expe-\nrience more quantization or reparameterization\nerrors, receive more bits. The K linear layers\nand the first MLP (FC1) in each block are also\nallocated higher bits. This visualization con-\nfirms that our strategy effectively adjusts bits\naccording to reparameterization errors.\nTable 7: Performance breakdown analysis.\nOPT w/ Sec.\nBits\nPPL\nLatency (ms)\n6.7B\n13B\n6.7B\n13B\n4.1\n2\n6.4e4 1.5e4\n16.5\n20.1\n4.1&4.2\n2\n18.56 14.76\n16.9\n20.9\n4.1&4.2& 4.3\n2.2\n17.99 13.79\n16.7\n21.0\nPerformance and Energy Breakdown. To examine\nthe contribution of each proposed technique, we con-\nducted ablation studies on OPT-6.7B/13B models. As\nshown in Tab. 7, the vanilla ShiftAddLLM (Sec. 4.1)\nsuffers from a significant perplexity increase with 2-\nbit reparameterization. Our multi-objective optimiza-\ntion (Sec. 4.2) reduces perplexity by an average of\n3.9e4, and the mixed bit allocation strategy (Sec. 4.3)\nfurther reduces perplexity by 0.77, maintaining com-\nparable latency. These experiments validate the ef-\nfectiveness of each component in ShiftAddLLM. In addition, profiling the two largest models on\nan Eyeriss accelerator illustrates the energy breakdown of the original LLMs and ShiftAddLLMs.\n9\n\n10\n1\n10\n2\nOurs\nFP16\n83.1% Saving\nEnergy Breakdown for OPT-66B\n10\n1\n10\n2\nOurs\nFP16\n87.4% Saving\nEnergy Breakdown for LLaMa-2-70B\nFP Shift\nLUTs\nFP Add\nFP Mult\nOthers\nFigure 8: Energy breakdown for OPT-66B and LLaMA-70B models using an Eyeriss accelerator.\nAs shown in Fig. 8, ShiftAddLLM reduces energy consumption by 87.2% for OPT-66B and 86.0%\nfor LLaMa-2-70B, with shift-and-add leading to 89.7% and 89.9% energy reduction compared to\noriginal multiplications.\n5.4\nDiscussion on Limitation\nWe demonstrated the accuracy and efficiency of post-training shift-and-add reparameterization of\nLLMs using multi-objective optimization and automated bit allocation, addressing the challenge of\nefficient LLM serving. However, achieving GPU speedup relied on BCQ kernels and the compatible\nOurs (Lat.) with a block-wise scaling factor design. While Ours (Acc.) with a column-wise design\ndelivers high accuracy, we lack the fast CUDA kernel required for similar speedups.\n5.5\nDiscussion on Technique Applicability Beyond LLMs\nWe acknowledge that the idea of shift-and-add reparameterization is general and can be extended\nto other smaller models like CNNs [69] or ViTs [72]. Meanwhile, this work’s implementation is\nspecifically dedicated to large-scale LLMs: It is the first instance of applying the shift-and-add\ntechnique at the scale of LLMs with billions of parameters. While many ideas perform well with\nmodels having millions of parameters, they often fail to scale effectively. Unlike previous methods\nthat require additional training and do not yield good results for large-scale LLMs, our approach\nis uniquely tailored for LLMs. We incorporate “post-training” reparameterization and carefully\ndesigned scaling factor patterns, enabling multi-objective optimization for LLMs and ensuring\nsuperior performance compared to prior quantization methods.\n6\nConclusion\nWe propose accelerating pretrained LLMs through post-training shift-and-add reparameterization,\ncreating efficient multiplication-free models. Our method reparameterizes weight matrices into\nbinary matrices with group-wise scaling factors, transforming multiplications into shifts and adds. To\nmitigate accuracy loss, we introduce a multi-objective optimization strategy that minimizes weight\nand activation reparameterization errors. Additionally, we develop an automated bit allocation strategy\nbased on layer sensitivity to further improve the accuracy-efficiency tradeoff. Extensive results across\nvarious LLM families and tasks validate the effectiveness of ShiftAddLLM. This work opens a new\nperspective on designing efficient LLM serving systems through post-training optimization.\nAcknowledgments and Disclosure of Funding\nThis work is supported by the National Science Foundation (NSF) RTML program (Award number:\n1937592) and the CoCoSys, one of the seven centers in JUMP 2.0, a Semiconductor Research\nCorporation (SRC) program sponsored by DARPA. We extend our gratitude towards Mitchelle\nRasquinha, and Robert Hundt for reviewing the paper and providing insightful feedback. We also\nthank the extended team at Google DeepMind who enabled and supported this research direction.\nReferences\n[1] Ardavan Afshar, Ioakeim Perros, Evangelos E Papalexakis, et al. COPA: Constrained PARAFAC2 for\nsparse & large datasets. In CIKM, 2018.\n[2] Meta AI. LLaMA 3. https://github.com/meta-llama/llama3, 2024.\n[3] Rohan Anil, Sebastian Borgeaud, et al. Gemini: A Family of Highly Capable Multimodal Models. arXiv\npreprint arXiv:2312.11805, 2023.\n10\n\n[4] Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, et al. A Systematic Classification of Knowledge,\nReasoning, and Context within the ARC Dataset. arXiv preprint arXiv:1806.00358, 2018.\n[5] Diogo Brito, Taimur G Rabuske, Jorge R Fernandes, et al. Quaternary logic lookup table in standard\nCMOS. TVLSI, 2014.\n[6] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, et al. QuIP: 2-Bit Quantization of Large Language Models\nWith Guarantees. NeurIPS, 2024.\n[7] Hanting Chen, Yunhe Wang, Chunjing Xu, et al. AdderNet: Do We Really Need Multiplications in Deep\nLearning? In CVPR, 2020.\n[8] Yu-Hsin Chen, Tushar Krishna, Joel S Emer, et al. Eyeriss: An Energy-efficient Reconfigurable Accelerator\nfor Deep Convolutional Neural Networks. JSSCC, 2016.\n[9] Christopher Clark, Kenton Lee, Ming-Wei Chang, et al. BoolQ: Exploring the Surprising Difficulty of\nNatural Yes/No Questions. arXiv preprint arXiv:1905.10044, 2019.\n[10] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, et al. Binarized Neural Networks: Training Deep\nNeural Networks with Weights and Activations Constrained to +1 or -1. arXiv preprint arXiv:1602.02830,\n2016.\n[11] Ido Dagan, Dan Roth, Fabio Zanzotto, et al. Recognizing Textual Entailment: Models and Applications.\nSpringer Nature, 2022.\n[12] Tri Dao, Dan Fu, Stefano Ermon, et al. FlashAttention: Fast and Memory-Efficient Exact Attention with\nIO-Awareness. NeurIPS, 2022.\n[13] Bita Darvish Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna\nVinogradsky, Sarah Massengill, Lita Yang, Ray Bittner, et al. Pushing the limits of narrow precision\ninferencing at cloud scale with microsoft floating point. Advances in neural information processing\nsystems, 33:10271–10281, 2020.\n[14] Tim Dettmers and Luke Zettlemoyer. The Case for 4-bit Precision: k-bit Inference Scaling Laws. In ICML,\n2023.\n[15] Tim Dettmers, Mike Lewis, Younes Belkada, et al. GPT3.int8(): 8-bit Matrix Multiplication for Trans-\nformers at Scale. NeurIPS, 2022.\n[16] Mostafa Elhoushi, Zihao Chen, Farhan Shafiq, et al. DeepShift: Towards Multiplication-Less Neural\nNetworks. In CVPR, 2021.\n[17] Elias Frantar and Dan Alistarh. Optimal Brain Compression: A Framework for Accurate Post-Training\nQuantization and Pruning. NeurIPS, 2022.\n[18] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, et al. OPTQ: Accurate Quantization for Generative\nPre-trained Transformers. In ICLR, 2022.\n[19] Amir Gholami, Zhewei Yao, Sehoon Kim, et al. AI and Memory Wall. IEEE Micro Journal, 2024.\n[20] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, et al. The Unreasonable Ineffectiveness of the\nDeeper Layers. arXiv preprint arXiv:2403.17887, 2024.\n[21] Yiwen Guo, Anbang Yao, Hao Zhao, et al. Network Sketching: Exploiting Binary Structure in Deep CNNs.\nIn CVPR, 2017.\n[22] Bah-Hwee Gwee, Joseph S Chang, Yiqiong Shi, et al. A Low-Voltage Micropower Asynchronous Multiplier\nWith Shift–Add Multiplication Approach. IEEE Transactions on Circuits and Systems I: Regular Papers,\n2008.\n[23] Song Han, Xingyu Liu, Huizi Mao, et al. EIE: Efficient Inference Engine on Compressed Deep Neural\nNetwork. ACM SIGARCH Computer Architecture News, 2016.\n[24] Simla Burcu Harma, Ayan Chakraborty, Elizaveta Kostenok, Danila Mishin, Dongho Ha, Babak Falsafi,\nMartin Jaggi, Ming Liu, Yunho Oh, Suvinay Subramanian, and Amir Yazdanbakhsh. Effective Interplay\nbetween Sparsity and Quantization: From Theory to Practice. arXiv preprint arXiv:2405.20935, 2024.\n[25] Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal Brain Surgeon and General Network Pruning.\nIn IEEE international conference on neural networks, 1993.\n11\n\n[26] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n[27] Mark Horowitz. 1.1 Computing’s Energy Problem (and what we can do about it). In ISSCC, 2014.\n[28] Wei Huang, Yangdong Liu, Haotong Qin, et al. BiLLM: Pushing the Limit of Post-Training Quantization\nfor LLMs. arXiv preprint arXiv:2402.04291, 2024.\n[29] Yongkweon Jeon, Baeseong Park, Se Jung Kwon, et al. BiQGEMM: Matrix Multiplication with Lookup\nTable For Binary-Coding-based Quantized DNNs. In SC, 2020.\n[30] Yongkweon Jeon, Chungman Lee, Eulrang Cho, et al. Mr.BiQ: Post-Training Non-Uniform Quantization\nbased on Minimizing the Reconstruction Error. In CVPR, 2022.\n[31] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, et al. Mistral 7B. arXiv preprint arXiv:2310.06825,\n2023.\n[32] Felix Juefei-Xu, Vishnu Naresh Boddeti, and Marios Savvides. Local Binary Convolutional Neural\nNetworks. In CVPR, 2017.\n[33] Se Jung Kwon, Dongsoo Lee, Yongkweon Jeon, et al. Post-Training Weighted Quantization of Neural\nNetworks for Language Models. https://openreview.net/forum?id=2Id6XxTjz7c, 2021.\n[34] Jung Hyun Lee, Jeonghoon Kim, Se Jung Kwon, and Dongsoo Lee. Flexround: Learnable rounding based\non element-wise division for post-training quantization. In International Conference on Machine Learning,\npages 18913–18939. PMLR, 2023.\n[35] Jung Hyun Lee, Jeonghoon Kim, June Yong Yang, Se Jung Kwon, Eunho Yang, Kang Min Yoo, and\nDongsoo Lee. Lrq: Optimizing post-training quantization for large language models by learning low-rank\nweight-scaling matrices. arXiv preprint arXiv:2407.11534, 2024.\n[36] Xinlin Li, Bang Liu, Rui Heng Yang, et al. DenseShift: Towards Accurate and Efficient Low-Bit Power-of-\nTwo Quantization. In ICCV, 2023.\n[37] Yuhang Li, Xin Dong, and Wei Wang. Additive Powers-of-Two Quantization: An Efficient Non-uniform\nDiscretization for Neural Networks. arXiv preprint arXiv:1909.13144, 2019.\n[38] Ji Lin, Jiaming Tang, Haotian Tang, et al.\nAWQ: Activation-aware Weight Quantization for LLM\nCompression and Acceleration. arXiv preprint arXiv:2306.00978, 2023.\n[39] Zechun Liu, Barlas Oguz, Changsheng Zhao, et al. LLM-QAT: Data-free Quantization Aware Training for\nLarge Language Models. arXiv preprint arXiv:2305.17888, 2023.\n[40] Xinyin Ma, Gongfan Fang, and Xinchao Wang. LLM-Pruner: On the Structural Pruning of Large Language\nModels. NeurIPS, 2023.\n[41] Stephen Merity, Caiming Xiong, James Bradbury, et al. Pointer Sentinel Mixture Models. In ICLR, 2017.\n[42] Thomas Mesnard, Cassidy Hardin, et al. Gemma: Open Models Based on Gemini Research and Technology.\narXiv preprint arXiv:2403.08295, 2024.\n[43] Nasrin Mostafazadeh, Michael Roth, Annie Louis, et al. LSDSem 2017 Shared Task: The Story Cloze\nTest. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level\nSemantics, 2017.\n[44] Mohammad Mozaffari, Amir Yazdanbakhsh, Zhao Zhang, and Maryam Mehri Dahnavi. SLoPe: Double-\nPruned Sparse Plus Lazy Low-rank Adapter Pretraining of LLMs. arXiv preprint arXiv:2405.16325,\n2024.\n[45] NVIDIA\nCorporation.\nNVIDIA\nA100\nTensor\nCore\nGPU.\nhttps://\nwww.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/\nnvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf, 2020. Datasheet.\n[46] OpenAI. ChatGPT: Language Model for Dialogue Generation. https://www.openai.com/chatgpt/,\n2023. Website.\n[47] OpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023.\n12\n\n[48] Gunho Park, Baeseong Park, Minsub Kim, et al.\nLUT-GEMM: Quantized Matrix Multiplication\nbased on LUTs for Efficient Inference in Large-Scale Generative Language Models. arXiv preprint\narXiv:2206.09557, 2022.\n[49] Teven Le Scao, Angela Fan, Christopher Akiki, et al. BLOOM: A 176B-Parameter Open-Access Multilin-\ngual Language Model. arXiv preprint arXiv:2211.05100, 2022.\n[50] Olivier Sentieys. Approximate Computing for DNN. In CSW 2021-HiPEAC Computing Systems Week,\n2021.\n[51] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng\nGao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language\nmodels. arXiv preprint arXiv:2308.13137, 2023.\n[52] Xuan Shen, Zhenglun Kong, Changdi Yang, et al. EdgeQAT: Entropy and Distribution Guided Quantization-\nAware Training for the Acceleration of Lightweight LLMs on the Edge. arXiv preprint arXiv:2402.10787,\n2024.\n[53] Huihong Shi, Haoran You, Yang Zhao, Zhongfeng Wang, and Yingyan Lin. Nasa: Neural architecture\nsearch and acceleration for hardware inspired hybrid networks. In Proceedings of the 41st IEEE/ACM\nInternational Conference on Computer-Aided Design, pages 1–9, 2022.\n[54] Han Shu, Jiahao Wang, Hanting Chen, et al. Adder Attention for Vision Transformer. NeurIPS, 2021.\n[55] Mingjie Sun, Zhuang Liu, Anna Bair, et al. A Simple and Effective Pruning Approach for Large Language\nModels. arXiv preprint arXiv:2306.11695, 2023.\n[56] Sandeep Tata and Jignesh M Patel. PiQA: An Algebra for Querying Protein Data Sets. In SSDBM, 2003.\n[57] Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al. LLaMA: Open and Efficient Foundation Language\nModels. arXiv preprint arXiv:2302.13971, 2023.\n[58] Hugo Touvron, Louis Martin, Kevin Stone, et al. Llama 2: Open Foundation and Fine-Tuned Chat Models.\narXiv preprint arXiv:2307.09288, 2023.\n[59] Ethan Waisberg, Joshua Ong, Mouayad Masalkhi, et al. Google’s AI chatbot “Bard”: A Side-by-Side\nComparison with ChatGPT and its Utilization in Ophthalmology. Eye, 2023.\n[60] Guangting Wang, Yucheng Zhao, Chuanxin Tang, et al. When Shift Operation Meets Vision Transformer:\nAn Extremely Simple Alternative to Attention Mechanism. In AAAI, 2022.\n[61] Yunhe Wang, Mingqiang Huang, Kai Han, et al. AdderNet and its Minimalist Hardware Design for\nEnergy-Efficient Artificial Intelligence. arXiv preprint arXiv:2101.10015, 2021.\n[62] Bichen Wu, Alvin Wan, Xiangyu Yue, et al. Shift: A Zero FLOP, Zero Parameter Alternative to Spatial\nConvolutions. In CVPR, 2018.\n[63] Guangxuan Xiao, Ji Lin, Mickael Seznec, et al. SmoothQuant: Accurate and Efficient Post-Training\nQuantization for Large Language Models. In ICML, 2023.\n[64] Chen Xu, Jianqiang Yao, Zhouchen Lin, et al. Alternating Multi-bit Quantization for Recurrent Neural\nNetworks. arXiv preprint arXiv:1802.00150, 2018.\n[65] Yixing Xu, Chang Xu, Xinghao Chen, et al. Kernel Based Progressive Distillation for Adder Neural\nNetworks. In NeurIPS, 2020.\n[66] Ping Xue and Bede Liu. Adaptive Equalizer Based on a Power-Of-Two-Quantized-LMF Algorithm. IEEE\ntransactions on acoustics, speech, and signal processing, 1986.\n[67] Songlin Yang, Bailin Wang, Yikang Shen, et al. Gated Linear Attention Transformers with Hardware-\nEfficient Training. arXiv preprint arXiv:2312.06635, 2023.\n[68] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, et al. ZeroQuant: Efficient and Affordable Post-\nTraining Quantization for Large-Scale Transformers. NeurIPS, 2022.\n[69] Haoran You, Xiaohan Chen, Yongan Zhang, et al. ShiftAddNet: A Hardware-Inspired Deep Network.\nNeurIPS, 2020.\n[70] Haoran You, Baopu Li, Shi Huihong, et al. ShiftAddNAS: Hardware-Inspired Search for More Accurate\nand Efficient Neural Networks. In ICLR, 2022.\n13\n\n[71] Haoran You, Yichao Fu, Zheng Wang, et al. When Linear Attention Meets Autoregressive Decoding:\nTowards More Effective and Efficient Linearized Large Language Models. In ICML, 2024.\n[72] Haoran You, Huihong Shi, Yipin Guo, et al. ShiftAddViT: Mixture of multiplication primitives towards\nefficient vision transformer. NeurIPS, 2024.\n[73] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu,\nYong Jae Lee, Yan Yan, et al. Llm inference unveiled: Survey and roofline model insights. arXiv preprint\narXiv:2402.16363, 2024.\n[74] Susan Zhang, Stephen Roller, Naman Goyal, et al. OPT: Open Pre-trained Transformer Language Models.\narXiv preprint arXiv:2205.01068, 2022.\n[75] Yang Zhao, Chaojian Li, Yue Wang, et al. DNN-Chip Predictor: An Analytical Performance Predictor for\nDNN Accelerators with Various Dataflows and Hardware Architectures. In ICASSP, 2020.\n[76] Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, and\nJason K Eshraghian. Scalable matmul-free language modeling. arXiv preprint arXiv:2406.02528, 2024.\n14\n\nA\nComplete Accuracy & Latency & Energy Data for OPT Models\nWe supply the complete quantitative accuracy, latency, and energy data measured on the OPT model\nfamily in Tab. 8, 9, and 10, respectively.\nTable 8: Perplexity comparisons of the OPT models on WikiText-2. Note that we set the group size\nof all methods as the number of columns following the setting of OPTQ [18] for a fair comparison.\nOPT (PPL ↓)\nBits\n125M\n350M\n1.3B\n2.7B\n6.7B\n13B\n30B\n66B\nFP16\n16\n27.65\n22.00\n14.62\n12.47\n10.86\n10.13\n9.56\n9.34\nOPTQ [18]\n3\n53.85\n33.79\n20.97\n16.88\n14.86\n11.61\n10.27\n14.16\nLUT-GEMM [48]\n3\n60.00\n42.32\n49.10\n17.55\n17.44\n12.50\n139.90\n100.33\nAWQ [38]\n3\n54.75\n35416.00\n24.60\n39.01\n16.47\n16.53\n31.01\n5622.00\nOurs (Lat.)\n3\n56.96\n28.72\n19.69\n15.28\n11.80\n10.70\n9.89\n9.62\nOPTQ [18]\n2\n2467.50\n10433.30\n4737.05\n6294.68\n442.63\n126.09\n71.70\n20.91\nLUT-GEMM [48]\n2\n4844.32\n2042.90\n3851.50\n616.30\n17455.52\n4963.27\n7727.27\n6246.00\nAWQ [38]\n2\n3514.61\n18313.24\n9472.81\n22857.70\n8168.30\n5014.92\n7780.96\n103843.84\nOurs (Lat.)\n2\n712.55\n445.78\n40.28\n50.95\n18.56\n14.76\n12.55\n12.20\nOurs (Mixed)\n2.2\n435.84\n279.19\n27.37\n31.97\n17.99\n13.79\n11.62\n11.17\nTable 9: A100 GPU latency comparisons on the OPT model family.\nOPT Latency (ms)\nBits\n125M\n350M\n1.3B\n2.7B\n6.7B\n13B\n30B\n66B\nFP16\n16\n7.8\n15.1\n16.7\n20.9\n22.2\n29.5\n51.7\nOOM\nOPTQ [18]\n3\n8.3\n15.9\n15.0\n21.5\n21.1\n26.4\n30.1\n51.5\nLUT-GEMM [48]\n3\n6.3\n11.7\n12.6\n15.5\n17.0\n19.5\n23.7\n39.5\nAWQ [38]\n3\n6.2\n12.1\n12.3\n16.3\n16.3\n20.0\n24.5\n40.9\nOurs (Lat.)\n3\n6.4\n13.3\n12.6\n16.6\n16.9\n20.8\n30.7\n54.1\nOPTQ [18]\n2\n8.2\n16.1\n15.9\n19.7\n19.9\n24.7\n31.5\n50.4\nLUT-GEMM [48]\n2\n6.2\n11.8\n11.8\n15.7\n15.7\n19.8\n23.6\n33.2\nAWQ [38]\n2\n6.2\n12.1\n12.3\n16.3\n16.3\n20.0\n24.5\n40.9\nOurs (Lat.)\n2\n6.3\n12.4\n12.3\n16.9\n16.9\n20.9\n25.4\n42.9\nOurs (Mixed)\n2.2\n6.3\n12.6\n12.5\n16.8\n16.7\n21.0\n27.1\n45.7\n* Note that we use AWQ’s open-sourced INT4 kernel for measuring its latency.\nTable 10: Energy comparisons on the OPT model family.\nOPT Energy (J)\nBits\n125M\n350M\n1.3B\n2.7B\n6.7B\n13B\n30B\n66B\nFP16\n16\n29.26\n83.72\n310.33\n625.80\n1573.41\n3036.99\n7088.39\n15539.87\nOPTQ [18]\n3\n13.77\n28.63\n90.12\n167.37\n399.28\n745.37\n1695.45\n3658.17\nLUT-GEMM [48]\n3\n12.83\n25.19\n75.73\n137.08\n321.54\n592.31\n1332.95\n2858.87\nOurs (Lat.)\n3\n11.68\n21.13\n59.59\n103.53\n235.45\n424.58\n938.98\n1990.17\nOPTQ [18]\n2\n12.58\n24.42\n73.27\n132.30\n309.50\n570.06\n1283.04\n2749.32\nLUT-GEMM [48]\n2\n12.48\n23.96\n70.90\n127.07\n295.77\n542.28\n1215.36\n2599.77\nOurs (Lat.)\n2\n11.41\n20.17\n55.80\n95.67\n215.27\n385.31\n846.54\n1786.62\nOurs (Mixed)\n2.2\n11.45\n20.33\n56.43\n96.98\n218.64\n391.86\n861.95\n1820.55\n15\n\nB\nComplete Accuracy & Latency & Energy Data for LLaMA Models\nWe supply the complete quantitative accuracy, latency, and energy data measured on the LLaMA\nmodel family in Tab. 11, 12, and 13, respectively.\nTable 11: Perplexity comparisons of the LLaMA models on WikiText-2.\nLLaMA (PPL ↓)\nBits\nLLaMA-2\nLLaMA-3\n7B\n13B\n70B\n8B\n70B\nFP16\n16\n5.47\n4.88\n3.32\n6.14\n2.86\nOPTQ [18]\n3\n6.43\n5.48\n3.88\n13.69\n4.91\nLUT-GEMM [48]\n3\n7.02\n5.89\n4.01\n11.10\n5.92\nAWQ [38]\n3\n6.24\n5.32\n3.74\n8.15\n4.69\nOurs (Lat.)\n3\n6.04\n5.33\n3.72\n7.71\n4.66\nOPTQ [18]\n2\n19.92\n12.75\n6.82\n398.0\n26.65\nLUT-GEMM [48]\n2\n2242.0\n2791.0\n136.4\n19096\n3121\nAWQ [38]\n2\n2.22e5\n1.22e5\n7.24e4\n1.71e6\n1.72e6\nOurs (Lat.)\n2\n9.58\n12.57\n5.71\n34.4\n12.4\n* Note that the group size is set to 128 following [48, 38].\nTable 12: A100 GPU latency comparisons of the LLaMA models.\nLLaMA Latency (ms) Bits\nLLaMA-2\nLLaMA-3\n7B\n13B\n70B\n8B\n70B\nFP16\n16\n32.6\n43.1\nOOM\n38.8\nOOM\nOPTQ [18]\n3\n31.1\n42.2\n81.9\n36.2\n90.7\nLUT-GEMM [48]\n3\n27.4\n34.7\n72.6\n31.7\n77.5\nAWQ [38]\n3\n25.4\n31.8\n68.0\n28.5\n67.7\nOurs (Lat.)\n3\n26.7\n33.8\n70.9\n31.4\n72.9\nOPTQ [18]\n2\n34.2\n38.8\n82.5\n36.8\n91.2\nLUT-GEMM [48]\n2\n27.5\n33.3\n71.0\n31.7\n77.2\nAWQ [38]\n2\n25.4\n31.8\n68.0\n28.5\n67.7\nOurs (Lat.)\n2\n27.7\n33.9\n72.1\n31.9\n78.3\nOurs (Mixed)\n2.2\n27.2\n34.3\n75.1\n30.1\n76.4\nTable 13: Energy comparisons of the LLaMA models.\nLLaMA Energy (J)\nBits\nLLaMA-2\nLLaMA-3\n7B\n13B\n70B\n8B\n70B\nFP16\n16\n1563.44\n3040.26\n18482.5\n1776.05\n16445.98\nOPTQ [18]\n3\n383.40\n728.98\n4297.33\n504.07\n3972.72\nLUT-GEMM [48]\n3\n305.06\n574.71\n3349.01\n419.64\n3139.34\nOurs (Lat.)\n3\n218.59\n405.53\n2309.87\n326.47\n2225.71\nOPTQ [18]\n2\n293.15\n552.20\n3212.56\n406.81\n3018.87\nLUT-GEMM [48]\n2\n279.20\n524.16\n3037.94\n391.74\n2865.81\nOurs (Lat.)\n2\n198.33\n365.85\n2065.90\n304.59\n2011.15\nOurs (Mixed)\n2.2\n201.69\n372.40\n2099.53\n306.69\n2066.64\n16\n\nC\nAblation Studies on Multi-Objective Optimization\nWe conduct ablation studies on different optimization objectives. As shown in Tab. 14, our multi-\nobjective optimization demonstrates superior performance in both column-wise and block-wise\nscaling factor formats. It achieves average perplexity reductions of 123.25, 2.22, and 403.18 compared\nto the weight-only objective, activation-only objective, and the vanilla combination of both weight and\nactivation objectives, respectively. These experiments validate the effectiveness of our multi-objective\noptimization approach.\nTable 14: Ablation studies on various optimization objectives.\nOPT PPL\n13B\n30B\n66B\nWei. Obj.\n13.8 222.6\n163.2\nAct. Obj.\n11.7\n10.5\n14.3\nWei. + Act.\n45.0\n16.3\n1178.1\nOurs (Col.-wise) 10.4\n9.6\n9.4\nOurs (Blk.-wise) 10.8\n9.9\n9.6\nD\nImpact of Batch Sizes on Throughput\nTo investigate the impact of batch sizes on the achievable throughput, we have further tested the\nthroughput of our CUDA kernels and end-to-end models with increased batch sizes, as demonstrated\nin Fig. 9. Our ShiftAddLLM still outperforms all three baselines at a batch size of 8 in terms of\naccuracy-efficiency trade-offs, achieving on average 3.37×/2.55×/1.39× throughput improvements\ncompared to OPTQ, AWQ, and LUT-GEMM at similar or much better accuracy.\nFigure 9: (a-b): Accuracy-throughput tradeoff comparisons among ShiftAddLLM, OPTQ, LUT-\nGEMM, and AWQ at a batch size of 8. (c) Kernel throughput evaluation under batch sizes of 1, 2, 4,\nand 8. (d) LLaMA-2-70B end-to-end model throughput evaluation under batch sizes of 1, 2, 4, and 8.\n(e) OPT-66B end-to-end model throughput evaluation under batch sizes of 1, 2, 4, and 8.\nPreviously, we assumed a batch size of one for mobile applications where only one user is using\nthe LLM. This assumption also stems from the sequential nature of LLMs during generation, i.e.,\ngenerating one token at a time based on all previously generated contexts. The assumption of a batch\nsize of 1 is also used in previous literature, such as AWQ, OPTQ, and LUT-GEMM, to measure the\nlatency or throughput for LLM serving.\n17\n\nE\nBenchmark with More Recent Baselines\nWe further compare our ShiftAddLLM with recent LLM quantization baselines FlexRound [34]\nand OmniQuant [51] on OPT and LLaMA models. As shown in Tabs. 15 & 16, our ShiftAddLLM\nconsistently shows better accuracy-efficiency trade-offs, achieving an average of 0.15 (4-bit) / 0.39\n(3-bit) and 0.30 (4-bit) / 0.52 (3-bit) perplexity reduction, as compared to FlexRound and OmniQuant,\nrespectively. Note that the baseline results are directly obtained from the original paper and follow-up\nwork LRQ [35]. In addition, we tested OmniQuant at 2 bits ourselves and found it fails for OPT\nmodels, whereas ours performs well for OPT models and also achieves an average 1.96 perplexity\nreduction than OmniQuant on LLaMA at 2 bits.\nTable 15: Perplexity comparisons between ShiftAddLLM and OmniQuant using OPT models and\nLLaMA models on WikiText-2. The group size is set as the length of rows for OPT models and 128\nfor LLaMA models following baselines.\nMethod\nBits\nOPT\nLLaMA-2\n125M\n350M\n1.3B\n2.7B\n6.7B\n13B\n30B\n66B\n7B\n13B\n70B\nOmniQuant [51]\n4\n29.45\n23.19\n15.04\n12.76 11.03 10.30\n9.65\n-\n5.58\n4.95\n-\nOurs (Acc.)\n4\n28.72\n21.59\n14.98\n12.65 10.95 10.20\n9.63\n-\n5.58\n4.96\n-\nOmniQuant [51]\n3\n35.66\n28.2\n16.68\n13.8\n11.65 10.87 10.00\n9.83\n6.03\n5.28 3.78\nOurs (Acc.)\n3\n31.29\n24.24\n21.53\n13.68 11.18 10.39\n9.63\n9.43\n5.89\n5.16 3.64\nOmniQuant [51]\n2\n311.39\n186.9\n484.51 1.1e6 9.6e5 3.6e4 9.3e3 5.2e3 11.06 8.26 6.55\nOurs (Acc.)\n2\n51.15\n40.24\n29.03\n20.78 13.78 12.17 10.67 10.33\n8.51\n6.77 4.72\nTable 16: Perplexity comparisons between ShiftAddLLM and FlexRound. The group size of\nFlexRound is set as the length of rows following the paper.\nMethod\nBits\nLLaMA-2\n7B\n13B\n70B\nFlexRound [34]\n4\n5.83 5.01\n-\nOurs (Acc.)\n4\n5.58 4.96\n-\nFlexRound [34]\n3\n6.34 5.59 3.92\nOurs (Acc.)\n3\n5.89 5.16 3.64\nF\nMore Results for Mixed Bit Allocation\nTo validate the effectiveness and applicability of our automated bit allocation across different LLM\nmodels, we evaluated and compared Ours (Mixed) with Ours (Lat.). The results are shown in Tab. 17.\nOurs (Mixed) further reduces perplexity by an average of 96.86, 3.23, and 2.63 for OPT, LLaMA,\nand Gemma models, respectively, under comparable or even less latency. This set of experiments\nfurther validates the applicability of our automated bit allocation strategy to different LLMs.\nTable 17: Perplexity and correlation results of our mixed bit allocation.\nMethods\nBits\nOPT\nLLaMA\nGemma\n125M\n1.3B\n13B\n2-7B\n2-13B\n3-8B\n2B\nCorrelation (τ)\n0.910\n0.905 0.915 0.931\n0.929\n0.897\n-\nOurs (Lat.)\n2\n712.55 40.28 14.76\n9.58\n12.57\n34.40 16.52 (3 bits)\nOurs (Mixed)\n2.2\n435.84 27.37 13.79\n8.97\n8.16\n29.72\n13.89\nIn addition, we want to clarify that, for each model, we search for the optimal bit allocation with\nnegligible overhead (e.g., 1% 10% of the reparameterization time). For example, it takes 0.5 seconds\nfor searching versus 72 seconds for reparameterizing OPT-125M with a single bit configuration, and 1\nminute for searching versus 13 minutes for reparameterizing OPT-13B with a single bit configuration.\nThis is achieved by leveraging the proposed proxy criteria (as shown in Sec. 4.3), instead of searching\naccording to the reparameterization errors, which is time-consuming and requires running models at\n18\n\neach bit. Using the proxy criteria, the bit allocation candidate rankings are highly correlated with the\nrankings obtained using actual reparameterization errors, with a Kendall τ of 0.910/0.905/0.915 for\nOPT-125M/1.3B/13B and 0.931/0.929/0.897 for LLaMA-7B/13B/8B, respectively.\nG\n4-Bit Results and Explanation for Using Lower Bit Widths\nWe further provide the 4-bit results in Tab. 18. These results show that ShiftAddLLM consistently\noutperforms the baselines at 4 bits, achieving average perplexity reductions of 0.90/1.32/1.00 and\n0.44/0.22/0.02 as compared to OPTQ/LUT-GEMM/AWQ, using OPT models and LLaMA models,\nrespectively.\nTable 18: Perplexity comparisons of the OPT models and LLaMA models with 4-bit quantization on\nWikiText-2. We set the group size as the length of rows for OPT models and 128 for LLaMA models\nfollowing baselines for fair comparisons.\nMethod\nBits\nOPT\nLLaMA\n125M\n350M\n1.3B\n2.7B\n6.7B\n13B\n30B\n1-7B 2-7B 2-13B 3-8B\nOPTQ [18]\n4\n31.12\n24.24\n15.47 12.87 11.39 10.31 9.63\n6.22\n5.69\n4.98\n7.63\nLUT-GEMM [48]\n4\n31.93\n24.09\n16.15 13.34 12.09 10.40 9.99\n5.94\n5.78\n5.06\n6.85\nAWQ [38]\n4\n31.66\n7.4e3 (outlier) 15.22 13.19 11.23\n-\n-\n5.78\n5.60\n4.97\n-\nOurs (Acc.)\n4\n28.72\n21.59\n14.98 12.65 10.95 10.20 9.63\n5.76\n5.58\n4.96\n6.46\nWe previously considered lower-bit quantization because we aim to push the accuracy-efficiency\nboundary to lower bits with minimal accuracy compromise. This is meaningful for large-scale LLMs,\nwhere even at 3 bits, they remain memory-bound. As analyzed using the Roofline model shown\nin Figure 5 of [73], for Nvidia A6000 GPUs, the turning point from memory-bound to compute-\nbound is 200 arithmetic intensity (OPs/bytes). For LLaMA-7B models, all the operators in the\ndecode/generation phase have around or less than 1 arithmetic intensity, as shown in Table 1 of [73].\nEven at 4 bits, the arithmetic intensity is approximately 1 ÷ 3 × 32 = 8 (same ops but 4/32 fewer\nmemory accesses), which is far less than the turning point of 200 and thus remains memory-bound,\nlet alone larger models like LLaMA-70B or beyond. Reducing from 4 bits to 2 bits can help increase\nthe arithmetic intensity and thus the theoretically maximum performance by 2x, from 6144G OPS\nto 12288G OPS. If memory is not a bottleneck for much smaller cases or prefill stages, higher bits\ncan be used for better accuracy. Our goal is to offer an additional option and trade-off for large,\nmemory-bound cases, without forcing the exclusive use of 2 bits.\nH\nComparison with MSFP\nMSFP [13] is an important prior work that employs a shared exponent across a group of elements and\nshifts the mantissa accordingly, mimicking multiplication by powers of two. In contrast, we clarify\nthat our approach differs from MSFP in two key aspects:\n1. Nature of Approach: MSFP uses shared exponents but relies on various shifted mantissa\nto represent the weights; without this, all weights would collapse to the same value. In\ncontrast, we do not use shared exponents for scaling factors and eliminate the need for\nmantissa. In particular, each scaling factor is represented as a distinct power-of-two integer\n(equivalent to the exponents in floating-point numbers, completely removing the mantissa\nbits). In this way, the multiplication between a floating-point activation and a power-of-two\ninteger scaling factor can be simplified to adding the corresponding integer to the exponent\nbit of the floating-point activation, as described in Fig. 1 (c). In addition, rather than\nsharing the exponents, the entire scaling factor in ShiftAddLLM is shared across groups of\nbinary weights in a column/block-wise manner, as illustrated in Fig. 3 (a) and detailed in\nSec. 4.2, carefully designed to optimize both weight quantization and output activation errors\nwithout conflicts. Hence, there are clear differences between the MSFP datatype and our\nquantization scheme. In fact, our method is orthogonal to MSFP and can be combined with it\nby representing input activations in MSFP for more aggressive performance improvements.\n2. Determining Shared Exponents or Scaling Factors: The method for determining shared\nexponents in MSFP or shared scaling factors in our quantization scheme is different. MSFP\n19\n\nselects the maximum exponent to share across the bounding-box size, i.e., the number of\nelements sharing one exponent [13], which is simpler in implementation yet might not be as\nadaptive. In contrast, in our ShiftAddLLM, the reparameterized binary weights and scaling\nfactors result from multi-objective optimization. This optimization adaptively designs\nscaling factor patterns to avoid conflicts between optimizing weight errors and optimizing\noutput activation errors.\nFinally, in terms of the performance outcomes, MSFP at 4 bits (1-bit sign and 3-bit mantissa) already\nsuffers from large quantization errors, as evidenced by the significant KL divergence shown in Fig.\n3 of [13]. In contrast, our ShiftAddLLM at 3 or 4 bits can still achieve comparable accuracy to FP\nbaselines. To directly compare ShiftAddLLM with MSFP, we conducted additional experiments to\ncompare (1) quantization errors and (2) KL divergence using both methods against their floating-\npoint counterparts. We randomly selected ten weight matrices from OPT-350M, quantizing or\nreparameterizing them using both methods. The results, as summarized in Tab. 19, indicate that\nShiftAddLLM consistently outperforms MSFP, achieving lower KL divergence by 0.0065, 0.0271,\nand 0.0952, and reducing quantization errors by 1707.3, 3251.1, and 5862.0 at 4-bit, 3-bit, and 2-bit\nquantization, respectively.\nTable 19: Comparison between MSFP and ShiftAddLLM with varying bits on KL Divergence and\nQuantization Error.\nMethods\nBits Avg. KL Divergence Avg. Quant. Error\nMSFP (bounding-box size = 128)\n4\n0.0117\n4129.1\nShiftAddLLM (group size = 128)\n4\n0.0052\n2421.8\nMSFP (bounding-box size = 128)\n3\n0.0434\n7859.9\nShiftAddLLM (group size = 128)\n3\n0.0163\n4608.8\nMSFP (bounding-box size = 128)\n2\n0.1485\n14355.7\nShiftAddLLM (group size = 128)\n2\n0.0533\n8493.7\nI\nAdditional Clarifications on Eyeriss\nAs emphasized in Sec. 5, our primary focus is on GPU acceleration, specifically through the develop-\nment of dedicated CUDA kernel support. It is worth noting that, we intentionally did not delve into\nspecific ASIC designs in the main manuscript, which were referenced only to demonstrate potential\nenergy savings.\nTo clarify the Eyeriss in estimating the energy costs, Eyeriss [8] is a well-known energy-efficient\nreconfigurable accelerator architecture designed for deep convolutional neural networks (CNNs). It\noptimizes both dataflow and memory access to reduce energy consumption during neural network\nprocessing. In our work, we adapt the Eyeriss architecture by modifying its MAC (Multiply-\nAccumulate) array, a key component responsible for performing heavy arithmetic computations in\nCNNs. Instead of using traditional MAC units across the array, we replace selected units with shift,\nadd, and lookup table (LUT) operations, aligning with our proposed ShiftAddLLM approach. This\nmodification significantly reduces both the area and power requirements, with savings ranging from\n26% to 89% in different configurations. We refer readers to Fig. 4 of NASA [53], which visually\ndemonstrates the design principles of the overall architecture, and illustrates how replacing traditional\nMAC units with shift and add operations leads to significant reductions in both area and energy\nconsumption. By adapting these principles, we enhance Eyeriss to better align with the computational\nneeds of both LLMs and ShiftAddLLMs while maintaining power and area efficiency.\n20",
    "pdf_filename": "ShiftAddLLM_Accelerating_Pretrained_LLMs_via_Post-Training_Multiplication-Less_Reparameterization.pdf"
}