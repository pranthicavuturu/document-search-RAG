{
    "title": "ShiftAddLLM: Accelerating Pretrained LLMs via",
    "abstract": "Largelanguagemodels(LLMs)haveshownimpressiveperformanceonlanguage tasksbutfacechallengeswhendeployedonresource-constraineddevicesdueto theirextensiveparametersandrelianceondensemultiplications,resultinginhigh memorydemandsandlatencybottlenecks. Shift-and-addreparameterizationoffers apromisingsolutionbyreplacingcostlymultiplicationswithhardware-friendly primitivesinboththeattentionandmulti-layerperceptron(MLP)layersofanLLM. However,currentreparameterizationtechniquesrequiretrainingfromscratchor fullparameterfine-tuningtorestoreaccuracy,whichisresource-intensiveforLLMs. Toaddressthis,weproposeacceleratingpretrainedLLMsthroughpost-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbedShiftAddLLM.Specifically,wequantizeeachweightmatrixintobinary matricespairedwithgroup-wisescalingfactors. Theassociatedmultiplications arereparameterizedinto(1)shiftsbetweenactivationsandscalingfactorsand(2) queriesandaddsaccordingtothebinarymatrices. Toreduceaccuracyloss,we presentamulti-objectiveoptimizationmethodtominimizebothweightandoutput activation reparameterization errors. Additionally, based on varying sensitivity acrosslayerstoreparameterization,wedevelopanautomatedbitallocationstrategy tofurtherreducememoryusageandlatency. ExperimentsonfiveLLMfamilies andeighttasksconsistentlyvalidatetheeffectivenessofShiftAddLLM,achieving averageperplexityreductionsof5.6and22.7pointsatcomparableorlowerlatency compared to the most competitive quantized LLMs at 3- and 2-bit precision, respectively,andmorethan80%memoryandenergyreductionsovertheoriginal LLMs. Codesandmodelsareavailableathttps://github.com/GATECH-EIC/ ShiftAddLLM. 1 Introduction PretrainedLLMshavedemonstratedstate-of-the-artperformanceinlanguageunderstandingand generationtasks[46,47,59,3,74,57,58,2]. However,deployingtheseLLMsincurssignificant hardwaredemands,includinghighlatency,memory,andenergyconsumption,especiallyonedgeor cloudGPUdevices. Theprimarybottlenecksaretheirimmenseparametersizesandtheassociated multiplication operations. For instance, GPT-3, with 175 billion parameters, requires 350GB of memoryinFP16format[38]andperforms1015floating-pointoperations(FLOPs)forasingleforward pass[19]. PreviouseffortstoimproveLLMefficiencyhavefocusedonpruning[40,55,20,24,44], quantization[63,38,18,48],andattentionoptimization[12,71,67]. However,thesemethodsstill relyoncostlymultiplicationoperationsinboththeattentionandMLPlayers. 38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024). 4202 voN 81 ]GL.sc[ 4v18950.6042:viXra",
    "body": "ShiftAddLLM: Accelerating Pretrained LLMs via\nPost-Training Multiplication-Less Reparameterization\nHaoranYou†,YipinGuo†,YichaoFu†,WeiZhou†,HuihongShi†,XiaofanZhang∗\nSouvikKundu⋄,AmirYazdanbakhsh‡,Yingyan(Celine)Lin†\n†GeorgiaInstituteofTechnology ⋄IntelLabs ∗Google ‡GoogleDeepMind\n†{haoran.you,celine.lin}@gatech.edu,eic-lab@groups.gatech.edu\n⋄souvikk.kundu@intel.com,∗‡{xiaofanz,ayazdan}@google.com\nAbstract\nLargelanguagemodels(LLMs)haveshownimpressiveperformanceonlanguage\ntasksbutfacechallengeswhendeployedonresource-constraineddevicesdueto\ntheirextensiveparametersandrelianceondensemultiplications,resultinginhigh\nmemorydemandsandlatencybottlenecks. Shift-and-addreparameterizationoffers\napromisingsolutionbyreplacingcostlymultiplicationswithhardware-friendly\nprimitivesinboththeattentionandmulti-layerperceptron(MLP)layersofanLLM.\nHowever,currentreparameterizationtechniquesrequiretrainingfromscratchor\nfullparameterfine-tuningtorestoreaccuracy,whichisresource-intensiveforLLMs.\nToaddressthis,weproposeacceleratingpretrainedLLMsthroughpost-training\nshift-and-add reparameterization, creating efficient multiplication-free models,\ndubbedShiftAddLLM.Specifically,wequantizeeachweightmatrixintobinary\nmatricespairedwithgroup-wisescalingfactors. Theassociatedmultiplications\narereparameterizedinto(1)shiftsbetweenactivationsandscalingfactorsand(2)\nqueriesandaddsaccordingtothebinarymatrices. Toreduceaccuracyloss,we\npresentamulti-objectiveoptimizationmethodtominimizebothweightandoutput\nactivation reparameterization errors. Additionally, based on varying sensitivity\nacrosslayerstoreparameterization,wedevelopanautomatedbitallocationstrategy\ntofurtherreducememoryusageandlatency. ExperimentsonfiveLLMfamilies\nandeighttasksconsistentlyvalidatetheeffectivenessofShiftAddLLM,achieving\naverageperplexityreductionsof5.6and22.7pointsatcomparableorlowerlatency\ncompared to the most competitive quantized LLMs at 3- and 2-bit precision,\nrespectively,andmorethan80%memoryandenergyreductionsovertheoriginal\nLLMs. Codesandmodelsareavailableathttps://github.com/GATECH-EIC/\nShiftAddLLM.\n1 Introduction\nPretrainedLLMshavedemonstratedstate-of-the-artperformanceinlanguageunderstandingand\ngenerationtasks[46,47,59,3,74,57,58,2]. However,deployingtheseLLMsincurssignificant\nhardwaredemands,includinghighlatency,memory,andenergyconsumption,especiallyonedgeor\ncloudGPUdevices. Theprimarybottlenecksaretheirimmenseparametersizesandtheassociated\nmultiplication operations. For instance, GPT-3, with 175 billion parameters, requires 350GB of\nmemoryinFP16format[38]andperforms1015floating-pointoperations(FLOPs)forasingleforward\npass[19]. PreviouseffortstoimproveLLMefficiencyhavefocusedonpruning[40,55,20,24,44],\nquantization[63,38,18,48],andattentionoptimization[12,71,67]. However,thesemethodsstill\nrelyoncostlymultiplicationoperationsinboththeattentionandMLPlayers.\n38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).\n4202\nvoN\n81\n]GL.sc[\n4v18950.6042:viXra\nTable1: Hardwarecostunder45nmCMOS[27,69,23,50,5].\nMultiplication Add Shift LUTs\nOPs\nFP32 FP16 INT32 INT8 FP32 FP16 INT32 INT8 INT32 INT16 INT8 (8-bitQuery)\nEnergy(pJ) 3.7 0.9 3.1 0.2 1.1 0.4 0.1 0.03 0.13 0.057 0.024 0.37(8OPs)\nArea(µm2) 7700 1640 3495 282 4184 1360 137 36 157 73 34 787(8OPs)\n*Notethat1LUTcorrespondsto8operations,aseachbitinqueriesisfromaweightelement.\nWeidentifyapromisingyetunexploredopportunityforimprovingLLMefficiency: reparameterizing\ntheirextensivemultiplicationswithmorecost-effectivehardwaresubstitutes,suchasbitwiseshifts\nandadds. Inspiredbypracticesincomputerarchitectureanddigitalsignalprocessing, replacing\nmultiplications with bitwise shifts and adds [66, 22] can offer up to 3.1/0.1 = 31× energy and\n3495/137≈26×areareductions(seeTab. 1). Thishardware-inspiredapproachcanleadtoefficient\nand fast implementations, as shown by previous research on ShiftAddNet [69, 70, 72]. Unlike\nprevioustechniquesthatrequiretrainingfromscratchorextensivefine-tuning,weproposeanew\nmethodtointegratetheshift-and-addconceptintoLLMsthroughpost-trainingoptimization.\nTo design multiplication-less LLMs, we need to address three key challenges: First, how can\nwe effectively reparameterize pretrained LLMs with shifts and adds in a post-training manner?\nPreviousreparameterizationtechniques[69,72]canresultinnontrivialquantizationerrors,requiring\nfine-tuningorretrainingtoavoidaccuracydrops. Weaimtodevelopaready-to-usepost-training\nreparameterizationmethodforLLMs. Second,howcanwemitigatetheaccuracydropfromshift-\nand-addreparameterization? Approximatingoriginalmultiplicationswithlower-bitshiftsandadds\ntypically reduces model accuracy. Most studies resort to fine-tuning or increasing model sizes,\ncomplicatingLLMdeployment.Wehypothesizethatoptimizingbothweightandactivationerrorscan\nminimizeoverallreparameterizationerror,aligningwithrecentactivation-awareweightquantization\nmethods in LLMs. Third, how can we handle varying sensitivities to reparameterization across\ndifferentlayersandblocksinLLMs? Anautomatedstrategytodeterminetheoptimalnumberof\nbitsforreparameterizedweightsineachlayerisneeded. Morevulnerablelayersshouldhavehigher-\nbitrepresentations, whilelesssensitivelayerscanuselower-bitrepresentations. Thisensuresno\nbottleneckedlayersduetoaggressivereparameterizationandmaximizesredundancyexploitation. To\nthebestofourknowledge,thisisthefirstattempttoaddressthesethreechallengesformultiplication-\nlessLLMsthroughpost-trainingreparameterization. Ourcontributionsaresummarizedasfollows:\n• WeproposeacceleratingpretrainedLLMsviaapost-trainingbitwiseshift-and-addreparameter-\nization,resultinginefficientmultiplication-lessLLMs,dubbedShiftAddLLM.Allweightsare\nquantizedintobinarymatricespairedwithgroup-wisescalingfactors;theassociatedmultiplications\narereparameterizedintoshift-and-addoperations.\n• Tomitigateaccuracyloss,wepresentamulti-objectiveoptimizationmethodaligningandoptimizing\nboth weight and output activation objectives, minimizing overall reparameterization error, and\nachievinglowerperplexityandbettertaskaccuracy.\n• Weintroduceamixedandautomatedbitallocationstrategythatdeterminestheoptimalnumberof\nbitsforreparameterizedweightsperlayer,basedontheirvulnerabilitytocompression. Susceptible\nlayersreceivehigher-bitrepresentations,whilelesssensitiveonesgetlower-bitrepresentations.\nOurextensiveresultsacrossfiveLLMsandeighttasksconsistentlyshowthesuperioraccuracyand\nefficiencytrade-offsachievedbyShiftAddLLM,withaverageperplexityreductionsof5.6and22.7at\ncomparableorevenlowerlatencycomparedtothemostcompetitivequantizedLLMsatthreeand\ntwobits,respectively,andmorethan80%memoryandenergyreductionsovertheoriginalLLMs.\n2 RelatedWorks\nLLMQuantization. SignificanteffortshavebeenmadetoquantizeLLMs,includingquantization-\nawaretraining(QAT)[39,52]andpost-trainingquantization(PTQ)[18,38,63,15]. QATrequires\ncalibrateddataandsignificantretrainingresources,whereasPTQismoredominantduetoitlower\ncomputationalandtimeoverhead. TherearetwoprevalentPTQstrategiesforLLMs: (1)uniform\nquantizationofbothweightsandactivations[63,15,68],oftenlimitedto8bits(W8A8)aslowerbit\nrepresentationscansignificantlyreduceaccuracy;and(2)lowerbitweight-onlyquantization[18,48,\n14,28,6],whichquantizesLLMweightstolowerbitswhilekeepingactivationsinaFP16format.\n2\nThis approach alleviates memory bottlenecks associated with the vast parameters of LLMs. For\ninstance,GPTQ[18]usesgradient-basedweightquantizationanddevelopsINT3/4kernelstoreduce\ndatamovements,andLUT-GEMM[48]eliminatesthedequantizationandusescustomLUT-based\nCUDA kernels to reduce memory and computation costs. In contrast, ShiftAddLLM is the first\ntoemploytheshift-and-addideaforreparameterizingpre-trainedLLMs. Thisreparameterization\nreducesbitusageforweightsandreplacescostlymultiplicationswithhardware-friendlyprimitives,\nfurtherreducingenergy,latency,andmemory.\nMultiplication-lessModels. Theefficientmodelcommunityhasfocusedonreducingorreplacing\nmultiplications.InCNNs,binarynetworks[10,32]binarizeweightsandactivations,whileshift-based\nnetworksusespatialshifts[62]orbitwiseshifts[16]tosubstituteformultiplications. AdderNet[7,\n65,61]replacesmultiplicationswithadditions,albeitwithasmallaccuracydrop. ShiftAddNet[69]\nreparameterizesCNNswithcascadedshiftandaddlayers. Thesetechniqueshavebeenadaptedto\nTransformers. BiLLM[28]introducesbinaryLLMs,while[54]and[60]extendtheadditionorshift\nconceptstotheattentionmechanisms,respectively. ShiftAddViT[72]reparameterizespretrained\nVisionTransformers(ViTs)withshiftsandadds.ContemporaryworkMatMul-freeLM[76]leverages\nadditiveoperatorsandHadamardproductsformultiplication-freelanguagemodeltraining,relying\nonFPGAsforspeedups. ComparedtocloselyrelatedworkslikeShiftAddNet[69]andMatMul-free\nLM[76], whichrequirestrainingfromscratch, andShiftAddViT[72], whichdemandsextensive\nparameterfine-tuning,ShiftAddLLMappliestheshift-and-addconcepttopre-trainedLLMswithout\nadditional training or fine-tuning. We also use a multi-objective optimization and automated bit\nallocationstrategytofurtherimproveaccuracyorreduceGPUlatency,energy,andmemoryusage.\n3 Preliminaries\nBinary-coding Quantization (BCQ). BCQ [64]\nquantizes each weight tensor in an L-layer LLM\nw ∈ Rm×n into q bits using a linear combination\nofbinarymatrices{b }q andcorrespondingscal- Algorithm1AlternatingMulti-bitBCQ[64]\ningfactors{α }q ,wi hi e= re1 b ∈ {−1,1}m×n. The 1: Input: Full-precision weight w ∈ Rn,\nweightsarethei ni a= p1 proximatedi byw =(cid:80)q α b bit-widthq,alternatingcyclesT\nq i=1 i i\nasaresultofminimizingthequantizationerror,i.e., 2: Output: α∗,b∗ ∈{−1,1}m×n\ni i\nargmin ∥w−(cid:80)q α b ∥2 toobtaintheopti- 3: FunctionMULTI-BITBCQ(w,q,T)\nmalα∗,α bi ∗,b .i Ifq is1,ti h= e1 nti hei problemcollapsesto 4: {α i,b i}q i=1 ← GREEDY(w)\nbinaryi quai ntization,whichhasananalyticalsolution: 5: fort←1toT do\nb∗ = sign(w),α∗ = w⊤b∗/n.Formulti-bitquan- 6: {α i}q i=1 ← LS(B,w)\ntization, we resort to greedy and alternating meth- 7: {b i}q i=1 ← BS(α 1,...,α q,w)\nods[64,30,33],asshowninAlg. 1. Initially,weuse 8: endfor\nthegreedymethod[21]toinitializeα ,b ,wherethe 9: endFunction\ni i\ni-thbitquantizationisperformedbyminimizingthe\nresidualrfromthe(i−1)-thbit:\ni−1\n(cid:88)\nmin∥r −α b ∥2, where r =w− α b , 1<i≤q. (1)\ni−1 i i i−1 j j\nαi,bi\nj=1\nWethenobtaintheinitializedα ,b sequentiallyasb =sign(r )andα =r⊤b /n(Line4). Next,\ni i i i i i i\nweperformalternatingoptimizationtofurtherminimizethequantizationerror. Specifically,{α }q\ni i=1\ncanbeiterativelyrefinedusingordinaryleastsquares(LS)[21]as[α ,...,α ]=((B⊤B)−1B⊤w)⊤,\n1 q\nwhereB=[b ,...,b ]∈{−1,1}m×n×q (Line6). Thebinarycodes{b }q canthenbeiteratively\n1 q i i=1\nrecalibratedusingabinarysearch(BS)giventherefined{α }q (Line7)[64].\ni i=1\nSuchBCQcansupportbothuniformandnon-uniformquantizationformatsbyadjustingthescaling\nfactorsandbiasesaccordingly[48]. OurShiftAddLLMisbuiltontopofBCQbutfurtherreplacesall\nassociatedmultiplicationswithlower-costhardwaresubstitutes(e.g.,shifts,adds,andLUTqueries).\nWe optimize not only the weight quantization error but also the output activation error, thereby\nachievinglowerquantizationbitsalongwithsavingsinenergy,memory,andcomputationalcosts.\nShiftandAddPrimitives. Directhardwareimplementationofmultiplicationsisofteninefficient.\nUsingshiftandaddoperationsas“shortcuts”providesamoreefficientalternative. Shifts,whichare\n3\n(a) Previous Weight- (c) FP16 Shift\nonly Quantization Scaling Input Act. FP16 Shift Using Multi. FP16 Shift Using UINT16 Add\nW ( 4bit) X (FP16) Factor X 3.14 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0\n+\n-22 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\nDe-Quant FP16 Shift -12.57 = 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0\nSign Exponent Mantissa\nMatMul. (FP16) 8-bit Key Shifted\nAct. (d) Construct LUTs and Query&Add\nO (FP16) Co Ln Us Ttr suct\nConstruct\nKL eyU VT\na\n1\nlue\nKL eyU VT\na\n2\nlue\nKL eU yT\nV alue Add\n+ +...+ =\n(b) Ours ShiftAddLLM Binary LUTs 0 0 ... 0 + +...+ = W ( 4bit) X (FP16) Weight 1 255 255 255 3 + +...+ = Query\n&Add 2 Query\nShiftAddLLM b'00111111(d'63)b'00101010(d'42)...b'01111011(d'123)\nShifted\n.. ... ...\n.\n+ +...+ =\nO (FP16) Output Act. O Act. Binary Weight : {-1 0, +1 1} Output Act. O\nFigure1: Illustrationofourproposedpost-trainingreparameterizationforShiftAddLLM.\nequivalenttomultiplyingbypowersoftwo,offeranon-uniformquantizationsolutionandcanresult\ninsignificantsavings. Forexample,wetestedmatrixmultiplicationfromoneMLPlayerofOPT-66B\nbetweenweightW ∈ R9216×36884 andactivationA ∈ R1×9216 usingFP16MACsandour3-bit\nShiftAddLLM.Energyconsumptionwas80.36Jvs. 9.77J,achieving87.8%savingswithourmethod.\nBothprimitiveshaveinspiredmanyinnovationsinefficientmodelinnovations[7,16,69,72].\n4 TheProposedShiftAddLLMFramework\nOverview. WeintroduceourShiftAddLLMasfollows: First,wedescribethereparameterizationof\npretrainedLLMsthroughapost-trainingshift-and-addapproachinSec. 4.1. Second,toenhance\naccuracy,weintroduceamulti-objectiveoptimizationmethodthataccountsforbothweightquantiza-\ntionerrorandoutputactivationerror,detailedinSec. 4.2. Third,toimproveefficiency,weexplorea\nmixedandautomatedbitallocationstrategy,illustratedinSec. 4.3.\n4.1 ShiftAddLLM:Post-trainingReparameterizationofLLMswithShiftandAddPrimitives\nPost-trainingReparameterizationofLLMs. Toavoidtheneedforfine-tuningafterreparameteriza-\ntion,ourmethodcloselymimicstheoriginalmultiplicationsusedinLLMs.Previousmethods,suchas\nweight-onlyquantizationtechniques[18],employgradient-basedoractivation-awareuniformquan-\ntizationtofitthepretrainedweightdistributionbetter,therebyachievinglowerquantizationerrors.\nHowever,thesemethodsoftenlackdirecthardwaresupportandrequireon-the-flydequantization\ntoFP16formultiplicationwithactivations,asdepictedinFig. 1(a). Incontrast,ourShiftAddLLM\nusestheBCQformat,supportingnon-uniformquantizationwithcustomizedCUDAkernels[48,29],\nbypassingtheneedfordequantization,asillustratedinFig. 1(b). Inparticular,ourmethodemploys\ntheAlg. 1toquantizepretrainedweightsintobinarymatrices{b }q andscalingfactors{α }q .\ni i=1 i i=1\nNotethatduringthealternatingoptimizationcycles,wefurtherquantizeallscalingfactorstopowers\noftwo(PoT)[37],asdescribedbytheequation:\nk−1\n(cid:88)\nα = POT(r )= POT(α− α ), where POT(α)=sign(α)·2P, 1≤k ≤K. (2)\nk k−1 j\nj=0\nThisadditivePoTmethodadoptsagreedystrategytoenhancetherepresentationalcapacityofPoT,\nusingK scalingfactors,wherethek-thPoTminimizestheresidualrofthe(k−1)-thPoT.Each\nPoTeffectivelyquantizesthescalingfactorαintosign(α)·2P,wheresign(α)indicatessignflips,\nP=round(log (abs(α))),and2Pdenotesabitwiseshifttotheleft(P>0)orright(P<0).\n2\nAftertheabovereparameterization,wecanthenreplacetheassociatedmultiplicationbetweenweights\nandactivationsintotwosteps: (1)Bitwiseshiftsbetweenactivationsandscalingfactors. Notethat\ntheactivationisstillintheFP16format,andthemultiplicationbetweenafloating-pointnumberand\napositiveornegativePoTintegercanbeefficientlyimplementedbyanintegeradditioninstruction\nonexistinghardwarefollowingDenseShift[36], asalsoillustratedinFig. 1(c); (2)Queriesand\n4\n...\n... ...\n...\n...\n...\n... ... ...\n...\n... ...\n...\n...\n(a) Weight Objective (c) Ours Multi-Objective Optimization\n... ... ...+ +...\n+ ...\n... ...+ +...\n= + ... = =\n... ... + ...\n... ...+ +...\nWeight\nInput Act. Output Act. Weight Input Act. Output Act.\n+\n(b) Activation Objective\n...\n... Quant. Error ... Quant. Error\n... t=1: ... ... t=1: ...\n.. .. .. t=2: ... .. .. .. t=2: ...\n... Output Act. Weight t=n: ... Output Act. Weight t=n: ...\nFigure2: Illustrationofourproposedmulti-objectiveoptimizationframework.\nadds intermediateshifted activations with the binary matrices. To implement thisefficiently and\nreduceredundantadditionsoraccumulations,asshowninFig. 1(d),wepre-compute256(= 28)\npossiblevaluesforeveryeightelementsintheshiftedactivationstoconstructLUTs. Hereevery\neightgroupedbinaryweightsforman8-bitkey. Supposetheshiftedactivationisann-dimensional\nvector. Inthatcase,wewillgetn/8LUTs,wherethegroupedbinaryweightsareusedaskeys,andthe\nprecomputedpartialsumsarestoredasvalues. Thisallowsustohandlethemultiplicationbetween\nthebinarymatrixb andtheshiftedactivationsasqueriestotheLUTs. Wethenaddallthepartial\ni\nsumstoobtainthefinaloutputactivationsinFP16format. SuchLUTsarewellsupportedbyexisting\nGPUkernels[48,29]. ThereparameterizationcanbeappliedtoallweightsinpretrainedLLMsina\npost-trainingmanner,replacingcostlymultiplicationswithefficienthardwareoperations.\nTakeaway. ShiftAddLLMpresentsanovelmultiplication-lessapproachthatleveragesnon-uniform\nquantizationviaBCQandadditivePoT.Thismethodologyenhancestherepresentationcapacityfor\noutlierweightsandactivationsoflargemagnitudecomparedtouniformquantization. Moreover,\nadditivePoTeffectivelyresolvestheissueoflimitedquantizationresolutionfornon-outlierweights\nandactivations. Overall,itallowsthequantizationlevelstobetteralignwiththedatadistribution.\n4.2 ShiftAddLLM:Multi-objectiveOptimization\nMotivatingAnalysisonPreviousLLMQuantizationObjectives. Weexaminepreviousweight-\nonlyquantizationmethodstounderstandthecausesoflargequantizationerrorandaccuracydrop.\nThesemethodstypicallyuseeitheraweightoractivationobjectivetominimizequantizationerror.\nSpecifically,the“weightobjective”(seeFig. 2(a))aimstominimizetheweightquantizationerror,\ni.e.,∥W−W ∥2,andadoptsscalingfactorsforeachrowofquantizedweights. However,thisdoes\nq\nnotoptimizeoutputactivationerror,aseachweightelementismultipliedbyauniqueinputactivation\nbeforesummingtoproducetheoutput. Varyinginputactivations,especiallyoutliers[63,38],rescale\nthe weight quantization error differently, causing significant divergence in the output activation.\nForexample, LUT-GEMM[48]adoptsthisweightobjective. Ontheotherhand, the“activation\nobjective”(seeFig. 2(b))minimizestheoutputactivationerror,i.e.,∥WX−W X∥,byquantizing\nq\nonecolumnofweightsatatimeandcontinuouslyupdatingtheremainingunquantizedweightsto\ncompensateforthequantizationerrorincurredbyquantizingasingleweightcolumn. However,the\nfixedscalingfactorsmaynotadequatelyaccommodatetheweightsadjustedafterward. OPTQ[18]\nemploysthisactivationobjective.\nOurMulti-ObjectiveOptimization. Tofurthermitigateaccuracydropafterreparameterization(see\nSec.4.1),weintroduceamulti-objectiveoptimizationframeworkthatcombinesweightandactivation\nobjectivesusingcolumn-wisescalingfactors. Thisframeworkeffectivelyreducesquantizationerror\nforbothweightsandactivations,therebyimprovingtheaccuracyofShiftAddLLM.\nAsshowninFig. 2(c),usingcolumn-wisescalingfactorsovercomesthelimitationsoftheprevious\nweightobjective[48]byeliminatingtheimpactofvaryinginputactivationsonquantizedweights.\n5\n...\n...\n...\n...\n...\n... ...\n...\n...\n... ...\n...\n... ...\n...\n...\n...\n...\n...\n...\n...\n... ...\n...\n... ...\nEach scaling factor corresponds to a constant activation value. Additionally, scaling factors for\nsubsequent columns are updated gradually after compensating for the corresponding column’s\nweights,ensuringabetterfitthanthepreviousactivationobjective[18].\nAccuracy vs. Latency Trade-\noffs. The column-wise scaling (a) Block-wise Scaling Factors (b) Design Comparisons\nf aa cc ct uo rr acd yes ai fg tn ers ri eg pn aifi rac man et tl ey rib zo ato is ot ns\n.\n.. ... ...\n.\n.. .... ..\n.\n. .... ...\n.\nRO oP wT- -3 w0 iB\nse\nw/ 1P 6P .3L\n(\n❌( ) )L 3a 3t .2. ( (m ✅s)\n)\nHowever, it does not fully lever- ... ... ...\nageBCQ[48,29],whichprocess . .. ... . ..... . ..... Column-wise 9.6 (✅)44.1 (❌)\neightelementsperrowofweights . .. .. . . .. .. . .. .. .. Block-wise 9.9 (✅)33.5 (✅)\ninparallelasLUTkeys,resulting Figure3: (a)theblock-wisescalingfactorsand(b)thecompari-\nin latency overhead for models sonamongdifferentdesignsonOPT-30B[74].\nwith≥30Bparameters. Forexam-\nple,testingontheOPT-30B[74]\nmodel and WikiText-2 dataset [41] showed (16.3 − 9.6) = 6.7 perplexity reduction but with a\n(44.1−33.2)/44.1≈24.7%latencyoverhead,asshowninFig.3(b).\nToaddressthis,weproposeablock-wisescalingfactordesignthatgroups8columnsand1/8ofthe\noriginalrowstoshareascalingfactor,ensuringcompatibilitywiththeBCQkernelandachieving\nlatency reductions, as shown in Fig. 3 (a). We refer to ShiftAddLLM with column-wise scaling\nfactorsas“Ours(Acc.)” forhighaccuracyoptimization,andwithblock-wisescalingfactorsas“Ours\n(Lat.)” foroptimizedaccuracy-latencytrade-off.\nTakeaway. Our multi-objective optimization approach integrates both weight and activation ob-\njectives, reducing weight quantization error in an activation-aware manner and output activation\nerrorreductioninaweight-awaremanner. Thissynergy,achievedthroughasimplecolumn-wiseor\nblock-wisedesign,significantlybooststheaccuracyofweight-onlyquantization. Thisalignswiththe\nprinciplesofpreviousactivation-awareweightquantizationmethods[38].\n4.3 ShiftAddLLM:MixedandAutomatedBitAllocation\nSensitivityAnalysis. Weanalyzethesen-\nsitivity of different layers and blocks in\nLLMstoshift-and-addreparameterization. 2 bits 2 bits\nAs shown in Fig. 4, later blocks incur 101 3 bits 0.5 3 bits\n4 bits 0.4 4 bits morequantizationorreparameterization\nerrors. Within each block, Query/Key 0.3\n102\n(Q/K)layersaregenerallymoresensitive 0.2\ntoreparameterizationthanotherlinearlay- 0.1\ners. Thisdiversesensitivitymotivatesus 103 0.0\n1 3 5 7 9 11131517192123 K V Q Out.FC1FC2\ntoexploremixedbitallocationsforLLM\nBlocks Linear Layers in a Block\nreparameterizationanddevelopstrategies\ntoautomaticallydeterminetheoptimalbit Figure4: SensitivityanalysisonOPT-1.3B[74].\nallocationsgiventheaveragebitbudgets.\nCriteriaandAutomatedBitAllocation. Todevelopthebitalloca-\ntionscheme,weproposecriteriatoestimatetheimportanceoflinear\nweightsandformulatethebitallocationasanintegerprogramming Kendall = 0.905\n200\nproblem.ForweightW fromthei-thlayerofanLLM,thecriterion\ni\nC isdefinedasfollows:\ni 150\nC =∥IS∥ ·STD(IS)2, where\ni F\n(3) 100\nIS=W /diag(cholesky((X XT)−1)), i i i\n50\nwheretheimportancescore(IS)isinspiredbyOptimalBrainCom-\npression[25,17,18]andiscorrelatedtotheincreaseinthequadratic\n0\nreconstruction error ∥WX−W X∥2 after reparameterizing the 0 50 100 150 200\nq\nweights,i.e.,IS↑,errorincreases↓. TheF-normofISindicatesthe Rank w.r.t. Criteria\noverallimportanceofW i,whilethestandarddeviation(STD)high- Figure5: Rankcomparisons.\nlightsthereparameterizationdifficultyforoutliers. Consideringboth\nfactors,weachieveamoreeffectiveevaluationmetricproportional\n6\n...\n...\n...\n.maraP\nrep\nrorrE\n.tnauQ\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\nrorrE\n.marapeR\n.t.r.w\nknaR\nTable2: PerplexitycomparisonsoftheOPTmodelsonWikiText-2. Notethatwesetthegroupsize\nofallmethodsasthelengthofrowsfollowingthesettingofOPTQ[18]forafaircomparison.\nOPT(PPL↓) Bits 125M 350M 1.3B 2.7B 6.7B 13B 30B 66B\nFP16 16 27.65 22.00 14.62 12.47 10.86 10.13 9.56 9.34\nOPTQ[18] 3 53.85 33.79 20.97 16.88 14.86 11.61 10.27 14.16\nLUT-GEMM[48] 3 60.00 42.32 49.10 17.55 17.44 12.50 139.90 100.33\nAWQ[38] 3 54.75 35416.00 24.60 39.01 16.47 16.53 31.01 5622.00\nOurs(Acc.) 3 31.29 24.24 21.53 13.68 11.18 10.39 9.63 9.43\nOPTQ[18] 2 2467.50 10433.30 4737.05 6294.68 442.63 126.09 71.70 20.91\nLUT-GEMM[48] 2 4844.32 2042.90 3851.50 616.30 17455.52 4963.27 7727.27 6246.00\nAWQ[38] 2 3514.61 18313.24 9472.81 22857.70 8168.30 5014.92 7780.96 103843.84\nQuIP[6] 2 92.84 146.15 27.90 30.02 16.30 12.34 11.48 10.92\nOurs(Acc.) 2 51.15 40.24 29.03 20.78 13.78 12.17 10.67 10.33\ntotheactualreparameterizationerror. AsshowninFig. 5,therankingsderivedfromourdefined\ncriteriaandtheactualreparameterizationerrorarehighlycorrelated,withaKendallτ of0.905. To\nrefinethecriteriabyincorporatingthebit-width,weuseleastsquarespolynomialfitstoestimateeach\nbit’scorrespondingreparameterizationerrorasC .\ni,b\nGiventhecriteria,wecanformulatetheautomatedbitallocationasanintegerprogrammingproblem:\nL L\n(cid:88)(cid:88) (cid:88) (cid:88)(cid:88)\nargmin β ·C , s.t. β =1, β ·b≤B·L, (4)\ni,b i,b i,b i,b\nβi,b\ni b b i b\nwhereListhenumberoflayersinthetargetLLM,b∈{2,3,4}denotestheavailablebitwidths,and\nβ ∈{0,1}istheone-hotindicatorforthei-thlayertodeterminetheassignedbits,e.g.,{0,1,0}\ni,b\nmeans3bits. TheobjectiveistominimizethesummedcriteriaCofalllayersunderthegivenaverage\nbitbudgetBperlayer. Thefinalβ representstheassignedbitsforthei-thlayerinthetargetLLM.\ni,b\nTakeaway. Usingmixedbitsinsteadofstaticonescanimprovetheaccuracy-efficiencytradeoffsby\nadaptingthevaryingsensitivitiesacrosslayers,e.g.,Q/Klinearlayersexhibithighersensitivityto\nreparameterization;Ouradoptedcriteriaprovideaquickestimationofthereparameterizationerror.\n5 Experiments\n5.1 ExperimentSettings\nModels. WeconsiderfiverepresentativeSOTALLMfamilies,includingOPT[74],LLaMA-1/2/3[58,\n2], Gemma [42], Mistral [31], and Bloom [49]. Tasks and Datasets. We evaluate all five LLMs\nonthecommonlyadoptedlanguagemodelingtaskusingtheWikiText-2[41]datasetforperplexity\nmeasurement. Additionally, we extend the evaluation of the two largest models, OPT-66B and\nLLaMA-2-70B,toeightdownstreamtasksforzero-shotaccuracyevaluation. Thesetasksinclude\nARC (Challenge/Easy) [4], BoolQ [9], Copa [1], PIQA [56], RTE [11], StoryCloze [43], and\nMMLU[26]. Baselines. WeconsiderfourSOTALLMquantizationmethods: OPTQ[18],LUT-\nGEMM[48],QuIP[6],andAWQ[38]. EvaluationMetrics. WeevaluateShiftAddLLMandthe\nbaselinesusingbothaccuracyandefficiencymetrics. Foraccuracy,weevaluateperplexityonthe\nWikiText-2datasetandzero-shotaccuracyoneightdownstreamtasks. Forefficiency,wemeasurethe\nlatencyonasingleA100-80GBGPU(PCIe)[45]andestimatetheenergycostsusinganEyeriss-like\nhardwareaccelerator[8,75],whichcalculatesnotonlycomputationalbutalsodatamovementenergy\n(within18%ofthedifferenceswithEyeriss’schipmeasurementresultsasclaimed).\n5.2 ShiftAddLLMoverSOTALLMQuantizationBaselines\nResultsonOPTModels. ToevaluatetheeffectivenessofourShiftAddLLM,wecompareagainst\nfourSOTALLMquantizationbaselines: OPTQ[18],LUT-GEMM[48],QuIP[6],andAWQ[38].\nUsing the OPT model family [74] and the WikiText-2 dataset [41], we assess perplexity, GPU\nlatency,andenergycosts. AsshowninTab.2,Ours(Acc.) consistentlyoutperformsallbaselines,\nachievinganaverageperplexityreductionof5.63/38.47/5136.13comparedtoOPTQ,LUT-GEMM,\n7\nOPTQ LUT-GEMM AWQ Ours (Lat.)\n(a) OPT (3-bit) (b) OPT (2-bit) (c) LLaMA-2 (3-bit) (d) LLaMA-3 (2-bit) (e) Gemma (3-bit)\n105 6.5 106 45\n30 6.0 40 104 105\n25 5.5 104 35\n103 5.0 30\n20 103\n15\n102 4.5\n102\n25\n4.0 20\n10 101 101\n15 20 25 10 20 30 40 50 25 50 75 50 75 18 20\nLatency (ms) Latency (ms) Latency (ms) Latency (ms) Latency (ms)\nFigure6: Accuracy-latencytradeoffcomparisonsontheOPT,LLaMA-2/3,andGemmamodels.\nandAWQ,respectively,at3bits.At2bits,wheremostbaselinesfailwithsignificantlyhighperplexity,\nourmethodmaintainslowperplexity,andachievesanaverage22.74perplexityreductionoverthe\nmostcompetitiveQuIP.Also,asshowninFig. 6(a&b),Ours(Lat.) consistentlyachievesbetter\naccuracy-latencytradeoffs,withaperplexityreductionof0.91∼103830.45atcomparablelatency\nor 6.5%∼60.1% latency reductions and 26.0%∼44.7% energy savings at similar or even lower\nperplexity. Completequantitativedataonaccuracy,latency,andenergyisprovidedinAppendixA.\nResultsonLLaMAModels. We\nfurtherevaluateShiftAddLLMon Table 3: Perplexity comparisons of the LLaMA models on\nLLaMAmodels[57,58,2]dueto WikiText-2. Thegroupsizeissetto128following[48,38].\ntheirsuperiorperformanceamong\nopen-source LLMs. As shown LLaMA-1 LLaMA-2 LLaMA-3\nLLaMA(PPL↓) Bits\nin Tab. 3, Ours (Acc.) consis- 7B 7B 13B 70B 8B 70B\ntently outperforms all baselines, FP16 16 5.68 5.47 4.88 3.32 6.14 2.86\nachieving an average perplexity OPTQ[18] 3 8.81 6.43 5.48 3.88 13.69 4.91\nreduction of 1.82/1.47/0.29 and LUT-GEMM[48] 3 7.18 7.02 5.89 4.01 11.10 5.92\n80.87/4606.98/678658.74 com- AWQ[38] 3 6.35 6.24 5.32 3.74 8.15 4.69\nOurs(Acc.) 3 6.04 5.89 5.16 3.64 7.20 4.35\nparedtoOPTQ,LUT-GEMM,and\nOPTQ[18] 2 68.60 19.92 12.75 6.82 398.0 26.65\nAWQat3and2bits,respectively.\nLUT-GEMM[48] 2 303.00 2242.0 2791.0 136.4 19096 3121\nEvaluatingOurs(Lat.) withboth\nAWQ[38] 2 2.6e5 2.2e5 1.2e5 7.2e4 1.7e6 1.7e6\naccuracy and latency metrics\nOurs(Acc.) 2 7.98 8.51 6.77 4.72 12.07 7.51\nas shown in Fig. 6 (c & d),\nOurs (Lat.) demonstrates better\naccuracy-latencytradeoffs. Itachieves1.1∼1719987.6perplexityreductionatcomparablelatency\nor 19.9%∼65.0% latency reductions and 28.4%∼89.9% energy savings at similar or even lower\nperplexity. Completequantitativedataonaccuracy,latency,andenergyareprovidedinAppendixB.\nResultsonGemma/Mistral/Bloom\nModels. We also evaluate Shif- Table4: ResultsonGemma/Mistral/Bloommodels.\ntAddLLM on Gemma [42], Mis-\ntral [31], and Bloom [49] models, PPL(↓) Bits Gemma-2B Mistral-7B Bloom-3B Bloom-7B\nwhich are among the most popular FP16 16 13.88 5.25 13.48 11.37\nopen-sourceLLMsandMixture-of- OPTQ 3 26.08 7.27 17.40 13.47\nExpert (MoE) models. As shown LUT-GEMM 3 44.36 22.36 21.03 17.29\ninTab.4,Ours(Acc.) achievesper- Ours(Acc.) 3 14.96 5.60 14.10 11.71\nplexityreductionsof11.12/29.4for\nGemma-2B,1.67/16.76forMistral-\n7B,and3.30/6.93and1.76/5.58forBLOOM-3B/7B,respectively,comparedtoOPTQandLUT-\nGEMM.AsshowninFig. 6(e),Ours(Lat.) showsbetteraccuracy-latencytradeoffs,e.g.,achieving\n9.56perplexityreductionand11%latencyreductionsovertheOTPQbaselineonGemmamodels.\nTheseresultsonfiveLLMfamiliesconsistentlyvalidatetheeffectivenessofourShiftAddLLM.\nZero-shotDownstreamTasks. Weextendourevaluationtozero-shotdownstreamdatasetsfora\nmorecomprehensiveassessment.AsshowninTab.5,Ours(Acc.)consistentlyimprovesperformance\noverpreviousSOTAbaselines,achievinganaverageaccuracygainof13.37/13.19and2.55/2.39\noverOPTQandLUT-GEMMbaselinesat3bitswhenevaluatedonOPT-66BandLLaMA-2-70B,\n8\nytixelpreP\nTable5: AccuracycomparisonsonsevendownstreamtasksforOPT-66BandLLaMA-2-70B.\nModels Methods Bits ARC_C ARC_E Copa BoolQ PIQA Storycloze RTE MMLU Mean\nFloatingPoint 16 37.20 71.25 86 69.82 78.67 77.47 60.65 25.89±0.37 63.37\nOPT-66B OPTQ[18] 3 24.66 48.86 70 52.05 64.47 67.09 53.07 23.98±0.36 50.52\nLUT-GEMM[48] 3 24.15 51.85 81 53.52 61.97 60.60 48.74 23.73±0.36 50.70\nOurs(Acc.) 3 35.24 70.88 87 72.45 77.64 77.15 63.18 27.56±0.38 63.89\nFloatingPoint 16 49.57 76.14 90 82.57 80.79 78.61 68.23 65.24±0.37 72.89\nLLaMA-2-70B OPTQ[18] 3 45.82 76.34 90 81.74 79.71 77.34 67.51 60.14±0.36 72.33\nLUT-GEMM[48] 3 47.70 76.42 89 80.31 80.20 77.78 68.59 - -\nOurs(Acc.) 3 48.38 77.06 93 84.25 80.47 78.49 75.09 62.33±0.38 74.88\nTable6: Perplexityandlatencyresultsofourmixedbitallocation.\nPPL(↓) Latency(ms)\nMethods Bits\n125M 350M 1.3B 2.7B 6.7B 13B 125M 350M 1.3B 2.7B 6.7B 13B\nOurs(Lat.) 2 712.55 445.78 40.28 50.95 18.56 14.76 6.3 12.4 12.3 16.9 16.9 20.9\nOurs(Mixed) 2.2 435.84 279.19 27.37 31.97 17.99 13.79 6.3 12.6 12.5 16.8 16.7 21.0\nrespectively. Theseexperimentsdemonstratethatourmethodnotonlyreducesperplexitybutalso\nimprovesdownstreamtaskaccuracy.\nGPUMemorySavings. OurShiftAddLLMalsoreducesGPUmemoryusage. ForOPT-66B,our\nmethodsaves81%and87%memorycostsoverFP16at3(23GBvs. 122GB)and2bits(16GB\nvs. 122GB),respectively. ForLLaMA-2-70B,itsaves80%and87%memorycostsat3(25GBvs.\n128GB)and2bits(17GBvs. 128GB),respectively.\nResultsofMixedBitAllocation. Weevaluateourmixedbitallocationstrategy(seeSec. 4.3)and\ncompare Ours (Mixed) with Ours (Lat.). As shown in Tab. 6, Ours (Mixed) further reduces the\nperplexitybyanaverageof79.45forOPTmodelfamiliesundercomparableorevenlesslatency. We\nprovidemoreresultsinAppendixFtovalidatetheeffectivenessofourmixedbitallocationstrategy.\n5.3 AblationStudiesofShiftAddLLM\nVisualization of Mixed Bit Allocation. We\nvisualizethebitallocationsafterapplyingour\nautomatedbitallocationstrategywithanaver- 2.4 2.50\nage bit budget of 2.2 (Fig. 7). The allocation\n2.2 2.25\npatterncorrelateswiththesensitivitytoreparam-\n2.0\neterizationidentifiedinSec. 4.3andshownin 2.00\n1.8\nFig. 4. Forinstance,laterblocks,whichexpe- 1.75\n1.6\nriencemorequantizationorreparameterization 1 3 5 7 911131517192123 K V Q Out.FC1FC2\nerrors, receive more bits. The K linear layers Blocks Linear Layers\nandthefirstMLP(FC1)ineachblockarealso\nFigure7: Visualizingtheaveragebitallocation.\nallocated higher bits. This visualization con-\nfirms that our strategy effectively adjusts bits\naccordingtoreparameterizationerrors.\nPerformanceandEnergyBreakdown. Toexamine\nthecontributionofeachproposedtechnique,wecon- Table7: Performancebreakdownanalysis.\nductedablationstudiesonOPT-6.7B/13Bmodels.As\nshowninTab. 7,thevanillaShiftAddLLM(Sec.4.1) PPL Latency(ms)\nOPTw/Sec. Bits\nsuffersfromasignificantperplexityincreasewith2- 6.7B 13B 6.7B 13B\nbitreparameterization. Ourmulti-objectiveoptimiza- 4.1 2 6.4e4 1.5e4 16.5 20.1\ntion(Sec.4.2)reducesperplexitybyanaverageof 4.1&4.2 2 18.56 14.76 16.9 20.9\n3.9e4,andthemixedbitallocationstrategy(Sec.4.3) 4.1&4.2&4.3 2.2 17.99 13.79 16.7 21.0\nfurtherreducesperplexityby0.77,maintainingcom-\nparablelatency. Theseexperimentsvalidatetheef-\nfectivenessofeachcomponentinShiftAddLLM.Inaddition,profilingthetwolargestmodelson\nanEyerissacceleratorillustratestheenergybreakdownoftheoriginalLLMsandShiftAddLLMs.\n9\nstiB\negarevA\nEnergy Breakdown for OPT-66B Energy Breakdown for LLaMa-2-70B FP Shift\nFP16 FP16 LUTs\nFP Add\nOurs 83.1% Saving Ours 87.4% Saving FP Mult\n101 102 101 102 Others\nFigure8: EnergybreakdownforOPT-66BandLLaMA-70BmodelsusinganEyerissaccelerator.\nAsshowninFig. 8,ShiftAddLLMreducesenergyconsumptionby87.2%forOPT-66Band86.0%\nforLLaMa-2-70B,withshift-and-addleadingto89.7%and89.9%energyreductioncomparedto\noriginalmultiplications.\n5.4 DiscussiononLimitation\nWedemonstratedtheaccuracyandefficiencyofpost-trainingshift-and-addreparameterizationof\nLLMsusingmulti-objectiveoptimizationandautomatedbitallocation,addressingthechallengeof\nefficientLLMserving. However,achievingGPUspeedupreliedonBCQkernelsandthecompatible\nOurs(Lat.) withablock-wisescalingfactordesign. WhileOurs(Acc.) withacolumn-wisedesign\ndelivershighaccuracy,welackthefastCUDAkernelrequiredforsimilarspeedups.\n5.5 DiscussiononTechniqueApplicabilityBeyondLLMs\nWeacknowledgethattheideaofshift-and-addreparameterizationisgeneralandcanbeextended\ntoothersmallermodelslikeCNNs[69]orViTs[72]. Meanwhile,thiswork’simplementationis\nspecifically dedicated to large-scale LLMs: It is the first instance of applying the shift-and-add\ntechniqueatthescaleofLLMswithbillionsofparameters. Whilemanyideasperformwellwith\nmodelshavingmillionsofparameters,theyoftenfailtoscaleeffectively. Unlikepreviousmethods\nthatrequireadditionaltraininganddonotyieldgoodresultsforlarge-scaleLLMs,ourapproach\nis uniquely tailored for LLMs. We incorporate “post-training” reparameterization and carefully\ndesigned scaling factor patterns, enabling multi-objective optimization for LLMs and ensuring\nsuperiorperformancecomparedtopriorquantizationmethods.\n6 Conclusion\nWeproposeacceleratingpretrainedLLMsthroughpost-trainingshift-and-addreparameterization,\ncreating efficient multiplication-free models. Our method reparameterizes weight matrices into\nbinarymatriceswithgroup-wisescalingfactors,transformingmultiplicationsintoshiftsandadds. To\nmitigateaccuracyloss,weintroduceamulti-objectiveoptimizationstrategythatminimizesweight\nandactivationreparameterizationerrors.Additionally,wedevelopanautomatedbitallocationstrategy\nbasedonlayersensitivitytofurtherimprovetheaccuracy-efficiencytradeoff. Extensiveresultsacross\nvariousLLMfamiliesandtasksvalidatetheeffectivenessofShiftAddLLM.Thisworkopensanew\nperspectiveondesigningefficientLLMservingsystemsthroughpost-trainingoptimization.\nAcknowledgmentsandDisclosureofFunding\nThisworkissupportedbytheNationalScienceFoundation(NSF)RTMLprogram(Awardnumber:\n1937592) and the CoCoSys, one of the seven centers in JUMP 2.0, a Semiconductor Research\nCorporation (SRC) program sponsored by DARPA. We extend our gratitude towards Mitchelle\nRasquinha,andRobertHundtforreviewingthepaperandprovidinginsightfulfeedback. Wealso\nthanktheextendedteamatGoogleDeepMindwhoenabledandsupportedthisresearchdirection.\nReferences\n[1] ArdavanAfshar,IoakeimPerros,EvangelosEPapalexakis,etal. COPA:ConstrainedPARAFAC2for\nsparse&largedatasets. InCIKM,2018.\n[2] MetaAI. LLaMA3. https://github.com/meta-llama/llama3,2024.\n[3] RohanAnil,SebastianBorgeaud,etal. Gemini:AFamilyofHighlyCapableMultimodalModels. arXiv\npreprintarXiv:2312.11805,2023.\n10\n[4] MichaelBoratko,HarshitPadigela,DivyendraMikkilineni,etal.ASystematicClassificationofKnowledge,\nReasoning,andContextwithintheARCDataset. arXivpreprintarXiv:1806.00358,2018.\n[5] DiogoBrito,TaimurGRabuske,JorgeRFernandes,etal. Quaternarylogiclookuptableinstandard\nCMOS. TVLSI,2014.\n[6] JerryChee,YaohuiCai,VolodymyrKuleshov,etal. QuIP:2-BitQuantizationofLargeLanguageModels\nWithGuarantees. NeurIPS,2024.\n[7] HantingChen,YunheWang,ChunjingXu,etal. AdderNet:DoWeReallyNeedMultiplicationsinDeep\nLearning? InCVPR,2020.\n[8] Yu-HsinChen,TusharKrishna,JoelSEmer,etal.Eyeriss:AnEnergy-efficientReconfigurableAccelerator\nforDeepConvolutionalNeuralNetworks. JSSCC,2016.\n[9] ChristopherClark,KentonLee,Ming-WeiChang,etal. BoolQ:ExploringtheSurprisingDifficultyof\nNaturalYes/NoQuestions. arXivpreprintarXiv:1905.10044,2019.\n[10] MatthieuCourbariaux,ItayHubara,DanielSoudry,etal. BinarizedNeuralNetworks: TrainingDeep\nNeuralNetworkswithWeightsandActivationsConstrainedto+1or-1. arXivpreprintarXiv:1602.02830,\n2016.\n[11] IdoDagan,DanRoth,FabioZanzotto,etal. RecognizingTextualEntailment:ModelsandApplications.\nSpringerNature,2022.\n[12] TriDao,DanFu,StefanoErmon,etal. FlashAttention:FastandMemory-EfficientExactAttentionwith\nIO-Awareness. NeurIPS,2022.\n[13] Bita Darvish Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna\nVinogradsky, SarahMassengill, LitaYang, RayBittner, etal. Pushingthelimitsofnarrowprecision\ninferencing at cloud scale with microsoft floating point. Advances in neural information processing\nsystems,33:10271–10281,2020.\n[14] TimDettmersandLukeZettlemoyer.TheCasefor4-bitPrecision:k-bitInferenceScalingLaws.InICML,\n2023.\n[15] TimDettmers,MikeLewis,YounesBelkada,etal. GPT3.int8(): 8-bitMatrixMultiplicationforTrans-\nformersatScale. NeurIPS,2022.\n[16] MostafaElhoushi,ZihaoChen,FarhanShafiq,etal. DeepShift: TowardsMultiplication-LessNeural\nNetworks. InCVPR,2021.\n[17] EliasFrantarandDanAlistarh. OptimalBrainCompression:AFrameworkforAccuratePost-Training\nQuantizationandPruning. NeurIPS,2022.\n[18] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, et al. OPTQ: Accurate Quantization for Generative\nPre-trainedTransformers. InICLR,2022.\n[19] AmirGholami,ZheweiYao,SehoonKim,etal. AIandMemoryWall. IEEEMicroJournal,2024.\n[20] AndreyGromov,KushalTirumala,HassanShapourian,etal. TheUnreasonableIneffectivenessofthe\nDeeperLayers. arXivpreprintarXiv:2403.17887,2024.\n[21] YiwenGuo,AnbangYao,HaoZhao,etal.NetworkSketching:ExploitingBinaryStructureinDeepCNNs.\nInCVPR,2017.\n[22] Bah-HweeGwee,JosephSChang,YiqiongShi,etal.ALow-VoltageMicropowerAsynchronousMultiplier\nWithShift–AddMultiplicationApproach. IEEETransactionsonCircuitsandSystemsI:RegularPapers,\n2008.\n[23] SongHan,XingyuLiu,HuiziMao,etal. EIE:EfficientInferenceEngineonCompressedDeepNeural\nNetwork. ACMSIGARCHComputerArchitectureNews,2016.\n[24] SimlaBurcuHarma,AyanChakraborty,ElizavetaKostenok,DanilaMishin,DonghoHa,BabakFalsafi,\nMartinJaggi,MingLiu,YunhoOh,SuvinaySubramanian,andAmirYazdanbakhsh. EffectiveInterplay\nbetweenSparsityandQuantization:FromTheorytoPractice. arXivpreprintarXiv:2405.20935,2024.\n[25] BabakHassibi,DavidGStork,andGregoryJWolff.OptimalBrainSurgeonandGeneralNetworkPruning.\nInIEEEinternationalconferenceonneuralnetworks,1993.\n11\n[26] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuringmassivemultitasklanguageunderstanding. arXivpreprintarXiv:2009.03300,2020.\n[27] MarkHorowitz. 1.1Computing’sEnergyProblem(andwhatwecandoaboutit). InISSCC,2014.\n[28] WeiHuang,YangdongLiu,HaotongQin,etal. BiLLM:PushingtheLimitofPost-TrainingQuantization\nforLLMs. arXivpreprintarXiv:2402.04291,2024.\n[29] YongkweonJeon,BaeseongPark,SeJungKwon,etal. BiQGEMM:MatrixMultiplicationwithLookup\nTableForBinary-Coding-basedQuantizedDNNs. InSC,2020.\n[30] YongkweonJeon,ChungmanLee,EulrangCho,etal. Mr.BiQ:Post-TrainingNon-UniformQuantization\nbasedonMinimizingtheReconstructionError. InCVPR,2022.\n[31] AlbertQJiang,AlexandreSablayrolles,ArthurMensch,etal.Mistral7B.arXivpreprintarXiv:2310.06825,\n2023.\n[32] Felix Juefei-Xu, Vishnu Naresh Boddeti, and Marios Savvides. Local Binary Convolutional Neural\nNetworks. InCVPR,2017.\n[33] SeJungKwon,DongsooLee,YongkweonJeon,etal. Post-TrainingWeightedQuantizationofNeural\nNetworksforLanguageModels. https://openreview.net/forum?id=2Id6XxTjz7c,2021.\n[34] JungHyunLee,JeonghoonKim,SeJungKwon,andDongsooLee. Flexround:Learnableroundingbased\nonelement-wisedivisionforpost-trainingquantization.InInternationalConferenceonMachineLearning,\npages18913–18939.PMLR,2023.\n[35] JungHyunLee,JeonghoonKim,JuneYongYang,SeJungKwon,EunhoYang,KangMinYoo,and\nDongsooLee. Lrq:Optimizingpost-trainingquantizationforlargelanguagemodelsbylearninglow-rank\nweight-scalingmatrices. arXivpreprintarXiv:2407.11534,2024.\n[36] XinlinLi,BangLiu,RuiHengYang,etal. DenseShift:TowardsAccurateandEfficientLow-BitPower-of-\nTwoQuantization. InICCV,2023.\n[37] YuhangLi,XinDong,andWeiWang. AdditivePowers-of-TwoQuantization:AnEfficientNon-uniform\nDiscretizationforNeuralNetworks. arXivpreprintarXiv:1909.13144,2019.\n[38] Ji Lin, Jiaming Tang, Haotian Tang, et al. AWQ: Activation-aware Weight Quantization for LLM\nCompressionandAcceleration. arXivpreprintarXiv:2306.00978,2023.\n[39] ZechunLiu,BarlasOguz,ChangshengZhao,etal. LLM-QAT:Data-freeQuantizationAwareTrainingfor\nLargeLanguageModels. arXivpreprintarXiv:2305.17888,2023.\n[40] XinyinMa,GongfanFang,andXinchaoWang.LLM-Pruner:OntheStructuralPruningofLargeLanguage\nModels. NeurIPS,2023.\n[41] StephenMerity,CaimingXiong,JamesBradbury,etal. PointerSentinelMixtureModels. InICLR,2017.\n[42] ThomasMesnard,CassidyHardin,etal.Gemma:OpenModelsBasedonGeminiResearchandTechnology.\narXivpreprintarXiv:2403.08295,2024.\n[43] NasrinMostafazadeh,MichaelRoth,AnnieLouis,etal. LSDSem2017SharedTask: TheStoryCloze\nTest. InProceedingsofthe2ndWorkshoponLinkingModelsofLexical,SententialandDiscourse-level\nSemantics,2017.\n[44] MohammadMozaffari,AmirYazdanbakhsh,ZhaoZhang,andMaryamMehriDahnavi. SLoPe:Double-\nPruned Sparse Plus Lazy Low-rank Adapter Pretraining of LLMs. arXiv preprint arXiv:2405.16325,\n2024.\n[45] NVIDIA Corporation. NVIDIA A100 Tensor Core GPU. https://\nwww.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/\nnvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf,2020. Datasheet.\n[46] OpenAI. ChatGPT:LanguageModelforDialogueGeneration. https://www.openai.com/chatgpt/,\n2023. Website.\n[47] OpenAI. GPT-4TechnicalReport. arXivpreprintarXiv:2303.08774,2023.\n12\n[48] Gunho Park, Baeseong Park, Minsub Kim, et al. LUT-GEMM: Quantized Matrix Multiplication\nbased on LUTs for Efficient Inference in Large-Scale Generative Language Models. arXiv preprint\narXiv:2206.09557,2022.\n[49] TevenLeScao,AngelaFan,ChristopherAkiki,etal. BLOOM:A176B-ParameterOpen-AccessMultilin-\ngualLanguageModel. arXivpreprintarXiv:2211.05100,2022.\n[50] OlivierSentieys. ApproximateComputingforDNN. InCSW2021-HiPEACComputingSystemsWeek,\n2021.\n[51] WenqiShao,MengzhaoChen,ZhaoyangZhang,PengXu,LiruiZhao,ZhiqianLi,KaipengZhang,Peng\nGao,YuQiao,andPingLuo. Omniquant:Omnidirectionallycalibratedquantizationforlargelanguage\nmodels. arXivpreprintarXiv:2308.13137,2023.\n[52] XuanShen,ZhenglunKong,ChangdiYang,etal.EdgeQAT:EntropyandDistributionGuidedQuantization-\nAwareTrainingfortheAccelerationofLightweightLLMsontheEdge. arXivpreprintarXiv:2402.10787,\n2024.\n[53] HuihongShi,HaoranYou,YangZhao,ZhongfengWang,andYingyanLin. Nasa: Neuralarchitecture\nsearchandaccelerationforhardwareinspiredhybridnetworks. InProceedingsofthe41stIEEE/ACM\nInternationalConferenceonComputer-AidedDesign,pages1–9,2022.\n[54] HanShu,JiahaoWang,HantingChen,etal. AdderAttentionforVisionTransformer. NeurIPS,2021.\n[55] MingjieSun,ZhuangLiu,AnnaBair,etal. ASimpleandEffectivePruningApproachforLargeLanguage\nModels. arXivpreprintarXiv:2306.11695,2023.\n[56] SandeepTataandJigneshMPatel. PiQA:AnAlgebraforQueryingProteinDataSets. InSSDBM,2003.\n[57] HugoTouvron,ThibautLavril,GautierIzacard,etal. LLaMA:OpenandEfficientFoundationLanguage\nModels. arXivpreprintarXiv:2302.13971,2023.\n[58] HugoTouvron,LouisMartin,KevinStone,etal. Llama2:OpenFoundationandFine-TunedChatModels.\narXivpreprintarXiv:2307.09288,2023.\n[59] EthanWaisberg,JoshuaOng,MouayadMasalkhi,etal. Google’sAIchatbot“Bard”: ASide-by-Side\nComparisonwithChatGPTanditsUtilizationinOphthalmology. Eye,2023.\n[60] GuangtingWang,YuchengZhao,ChuanxinTang,etal. WhenShiftOperationMeetsVisionTransformer:\nAnExtremelySimpleAlternativetoAttentionMechanism. InAAAI,2022.\n[61] Yunhe Wang, Mingqiang Huang, Kai Han, et al. AdderNet and its Minimalist Hardware Design for\nEnergy-EfficientArtificialIntelligence. arXivpreprintarXiv:2101.10015,2021.\n[62] BichenWu,AlvinWan,XiangyuYue,etal. Shift:AZeroFLOP,ZeroParameterAlternativetoSpatial\nConvolutions. InCVPR,2018.\n[63] GuangxuanXiao, JiLin, MickaelSeznec, etal. SmoothQuant: AccurateandEfficientPost-Training\nQuantizationforLargeLanguageModels. InICML,2023.\n[64] ChenXu,JianqiangYao,ZhouchenLin,etal. AlternatingMulti-bitQuantizationforRecurrentNeural\nNetworks. arXivpreprintarXiv:1802.00150,2018.\n[65] YixingXu, ChangXu, XinghaoChen, etal. KernelBasedProgressiveDistillationforAdderNeural\nNetworks. InNeurIPS,2020.\n[66] PingXueandBedeLiu. AdaptiveEqualizerBasedonaPower-Of-Two-Quantized-LMFAlgorithm. IEEE\ntransactionsonacoustics,speech,andsignalprocessing,1986.\n[67] SonglinYang,BailinWang,YikangShen,etal. GatedLinearAttentionTransformerswithHardware-\nEfficientTraining. arXivpreprintarXiv:2312.06635,2023.\n[68] ZheweiYao,RezaYazdaniAminabadi,MinjiaZhang,etal. ZeroQuant:EfficientandAffordablePost-\nTrainingQuantizationforLarge-ScaleTransformers. NeurIPS,2022.\n[69] HaoranYou,XiaohanChen,YonganZhang,etal. ShiftAddNet: AHardware-InspiredDeepNetwork.\nNeurIPS,2020.\n[70] HaoranYou,BaopuLi,ShiHuihong,etal. ShiftAddNAS:Hardware-InspiredSearchforMoreAccurate\nandEfficientNeuralNetworks. InICLR,2022.\n13\n[71] HaoranYou,YichaoFu,ZhengWang,etal. WhenLinearAttentionMeetsAutoregressiveDecoding:\nTowardsMoreEffectiveandEfficientLinearizedLargeLanguageModels. InICML,2024.\n[72] HaoranYou,HuihongShi,YipinGuo,etal. ShiftAddViT:Mixtureofmultiplicationprimitivestowards\nefficientvisiontransformer. NeurIPS,2024.\n[73] ZhihangYuan,YuzhangShang,YangZhou,ZhenDong,ChenhaoXue,BingzheWu,ZhikaiLi,QingyiGu,\nYongJaeLee,YanYan,etal. Llminferenceunveiled:Surveyandrooflinemodelinsights. arXivpreprint\narXiv:2402.16363,2024.\n[74] SusanZhang,StephenRoller,NamanGoyal,etal. OPT:OpenPre-trainedTransformerLanguageModels.\narXivpreprintarXiv:2205.01068,2022.\n[75] YangZhao,ChaojianLi,YueWang,etal. DNN-ChipPredictor:AnAnalyticalPerformancePredictorfor\nDNNAcceleratorswithVariousDataflowsandHardwareArchitectures. InICASSP,2020.\n[76] Rui-JieZhu,YuZhang,EthanSifferman,TylerSheaves,YiqiaoWang,DustinRichmond,PengZhou,and\nJasonKEshraghian. Scalablematmul-freelanguagemodeling. arXivpreprintarXiv:2406.02528,2024.\n14\nA CompleteAccuracy&Latency&EnergyDataforOPTModels\nWesupplythecompletequantitativeaccuracy,latency,andenergydatameasuredontheOPTmodel\nfamilyinTab.8,9,and10,respectively.\nTable8: PerplexitycomparisonsoftheOPTmodelsonWikiText-2. Notethatwesetthegroupsize\nofallmethodsasthenumberofcolumnsfollowingthesettingofOPTQ[18]forafaircomparison.\nOPT(PPL↓) Bits 125M 350M 1.3B 2.7B 6.7B 13B 30B 66B\nFP16 16 27.65 22.00 14.62 12.47 10.86 10.13 9.56 9.34\nOPTQ[18] 3 53.85 33.79 20.97 16.88 14.86 11.61 10.27 14.16\nLUT-GEMM[48] 3 60.00 42.32 49.10 17.55 17.44 12.50 139.90 100.33\nAWQ[38] 3 54.75 35416.00 24.60 39.01 16.47 16.53 31.01 5622.00\nOurs(Lat.) 3 56.96 28.72 19.69 15.28 11.80 10.70 9.89 9.62\nOPTQ[18] 2 2467.50 10433.30 4737.05 6294.68 442.63 126.09 71.70 20.91\nLUT-GEMM[48] 2 4844.32 2042.90 3851.50 616.30 17455.52 4963.27 7727.27 6246.00\nAWQ[38] 2 3514.61 18313.24 9472.81 22857.70 8168.30 5014.92 7780.96 103843.84\nOurs(Lat.) 2 712.55 445.78 40.28 50.95 18.56 14.76 12.55 12.20\nOurs(Mixed) 2.2 435.84 279.19 27.37 31.97 17.99 13.79 11.62 11.17\nTable9: A100GPUlatencycomparisonsontheOPTmodelfamily.\nOPTLatency(ms) Bits 125M 350M 1.3B 2.7B 6.7B 13B 30B 66B\nFP16 16 7.8 15.1 16.7 20.9 22.2 29.5 51.7 OOM\nOPTQ[18] 3 8.3 15.9 15.0 21.5 21.1 26.4 30.1 51.5\nLUT-GEMM[48] 3 6.3 11.7 12.6 15.5 17.0 19.5 23.7 39.5\nAWQ[38] 3 6.2 12.1 12.3 16.3 16.3 20.0 24.5 40.9\nOurs(Lat.) 3 6.4 13.3 12.6 16.6 16.9 20.8 30.7 54.1\nOPTQ[18] 2 8.2 16.1 15.9 19.7 19.9 24.7 31.5 50.4\nLUT-GEMM[48] 2 6.2 11.8 11.8 15.7 15.7 19.8 23.6 33.2\nAWQ[38] 2 6.2 12.1 12.3 16.3 16.3 20.0 24.5 40.9\nOurs(Lat.) 2 6.3 12.4 12.3 16.9 16.9 20.9 25.4 42.9\nOurs(Mixed) 2.2 6.3 12.6 12.5 16.8 16.7 21.0 27.1 45.7\n*NotethatweuseAWQ’sopen-sourcedINT4kernelformeasuringitslatency.\nTable10: EnergycomparisonsontheOPTmodelfamily.\nOPTEnergy(J) Bits 125M 350M 1.3B 2.7B 6.7B 13B 30B 66B\nFP16 16 29.26 83.72 310.33 625.80 1573.41 3036.99 7088.39 15539.87\nOPTQ[18] 3 13.77 28.63 90.12 167.37 399.28 745.37 1695.45 3658.17\nLUT-GEMM[48] 3 12.83 25.19 75.73 137.08 321.54 592.31 1332.95 2858.87\nOurs(Lat.) 3 11.68 21.13 59.59 103.53 235.45 424.58 938.98 1990.17\nOPTQ[18] 2 12.58 24.42 73.27 132.30 309.50 570.06 1283.04 2749.32\nLUT-GEMM[48] 2 12.48 23.96 70.90 127.07 295.77 542.28 1215.36 2599.77\nOurs(Lat.) 2 11.41 20.17 55.80 95.67 215.27 385.31 846.54 1786.62\nOurs(Mixed) 2.2 11.45 20.33 56.43 96.98 218.64 391.86 861.95 1820.55\n15\nB CompleteAccuracy&Latency&EnergyDataforLLaMAModels\nWesupplythecompletequantitativeaccuracy,latency,andenergydatameasuredontheLLaMA\nmodelfamilyinTab.11,12,and13,respectively.\nTable11: PerplexitycomparisonsoftheLLaMAmodelsonWikiText-2.\nLLaMA-2 LLaMA-3\nLLaMA(PPL↓) Bits\n7B 13B 70B 8B 70B\nFP16 16 5.47 4.88 3.32 6.14 2.86\nOPTQ[18] 3 6.43 5.48 3.88 13.69 4.91\nLUT-GEMM[48] 3 7.02 5.89 4.01 11.10 5.92\nAWQ[38] 3 6.24 5.32 3.74 8.15 4.69\nOurs(Lat.) 3 6.04 5.33 3.72 7.71 4.66\nOPTQ[18] 2 19.92 12.75 6.82 398.0 26.65\nLUT-GEMM[48] 2 2242.0 2791.0 136.4 19096 3121\nAWQ[38] 2 2.22e5 1.22e5 7.24e4 1.71e6 1.72e6\nOurs(Lat.) 2 9.58 12.57 5.71 34.4 12.4\n*Notethatthegroupsizeissetto128following[48,38].\nTable12: A100GPUlatencycomparisonsoftheLLaMAmodels.\nLLaMA-2 LLaMA-3\nLLaMALatency(ms) Bits\n7B 13B 70B 8B 70B\nFP16 16 32.6 43.1 OOM 38.8 OOM\nOPTQ[18] 3 31.1 42.2 81.9 36.2 90.7\nLUT-GEMM[48] 3 27.4 34.7 72.6 31.7 77.5\nAWQ[38] 3 25.4 31.8 68.0 28.5 67.7\nOurs(Lat.) 3 26.7 33.8 70.9 31.4 72.9\nOPTQ[18] 2 34.2 38.8 82.5 36.8 91.2\nLUT-GEMM[48] 2 27.5 33.3 71.0 31.7 77.2\nAWQ[38] 2 25.4 31.8 68.0 28.5 67.7\nOurs(Lat.) 2 27.7 33.9 72.1 31.9 78.3\nOurs(Mixed) 2.2 27.2 34.3 75.1 30.1 76.4\nTable13: EnergycomparisonsoftheLLaMAmodels.\nLLaMA-2 LLaMA-3\nLLaMAEnergy(J) Bits\n7B 13B 70B 8B 70B\nFP16 16 1563.44 3040.26 18482.5 1776.05 16445.98\nOPTQ[18] 3 383.40 728.98 4297.33 504.07 3972.72\nLUT-GEMM[48] 3 305.06 574.71 3349.01 419.64 3139.34\nOurs(Lat.) 3 218.59 405.53 2309.87 326.47 2225.71\nOPTQ[18] 2 293.15 552.20 3212.56 406.81 3018.87\nLUT-GEMM[48] 2 279.20 524.16 3037.94 391.74 2865.81\nOurs(Lat.) 2 198.33 365.85 2065.90 304.59 2011.15\nOurs(Mixed) 2.2 201.69 372.40 2099.53 306.69 2066.64\n16\nC AblationStudiesonMulti-ObjectiveOptimization\nWeconductablationstudiesondifferentoptimizationobjectives. AsshowninTab.14,ourmulti-\nobjective optimization demonstrates superior performance in both column-wise and block-wise\nscalingfactorformats.Itachievesaverageperplexityreductionsof123.25,2.22,and403.18compared\ntotheweight-onlyobjective,activation-onlyobjective,andthevanillacombinationofbothweightand\nactivationobjectives,respectively. Theseexperimentsvalidatetheeffectivenessofourmulti-objective\noptimizationapproach.\nTable14: Ablationstudiesonvariousoptimizationobjectives.\nOPTPPL 13B 30B 66B\nWei.Obj. 13.8 222.6 163.2\nAct.Obj. 11.7 10.5 14.3\nWei.+Act. 45.0 16.3 1178.1\nOurs(Col.-wise) 10.4 9.6 9.4\nOurs(Blk.-wise) 10.8 9.9 9.6\nD ImpactofBatchSizesonThroughput\nToinvestigatetheimpactofbatchsizesontheachievablethroughput, wehavefurthertestedthe\nthroughputofourCUDAkernelsandend-to-endmodelswithincreasedbatchsizes,asdemonstrated\ninFig.9. OurShiftAddLLMstilloutperformsallthreebaselinesatabatchsizeof8intermsof\naccuracy-efficiencytrade-offs,achievingonaverage3.37×/2.55×/1.39×throughputimprovements\ncomparedtoOPTQ,AWQ,andLUT-GEMMatsimilarormuchbetteraccuracy.\nFigure 9: (a-b): Accuracy-throughput tradeoff comparisons among ShiftAddLLM, OPTQ, LUT-\nGEMM,andAWQatabatchsizeof8. (c)Kernelthroughputevaluationunderbatchsizesof1,2,4,\nand8. (d)LLaMA-2-70Bend-to-endmodelthroughputevaluationunderbatchsizesof1,2,4,and8.\n(e)OPT-66Bend-to-endmodelthroughputevaluationunderbatchsizesof1,2,4,and8.\nPreviously,weassumedabatchsizeofoneformobileapplicationswhereonlyoneuserisusing\ntheLLM.ThisassumptionalsostemsfromthesequentialnatureofLLMsduringgeneration,i.e.,\ngeneratingonetokenatatimebasedonallpreviouslygeneratedcontexts. Theassumptionofabatch\nsizeof1isalsousedinpreviousliterature,suchasAWQ,OPTQ,andLUT-GEMM,tomeasurethe\nlatencyorthroughputforLLMserving.\n17\nE BenchmarkwithMoreRecentBaselines\nWe further compare our ShiftAddLLM with recent LLM quantization baselines FlexRound [34]\nandOmniQuant[51]onOPTandLLaMAmodels. AsshowninTabs.15&16,ourShiftAddLLM\nconsistentlyshowsbetteraccuracy-efficiencytrade-offs,achievinganaverageof0.15(4-bit)/0.39\n(3-bit)and0.30(4-bit)/0.52(3-bit)perplexityreduction,ascomparedtoFlexRoundandOmniQuant,\nrespectively. Notethatthebaselineresultsaredirectlyobtainedfromtheoriginalpaperandfollow-up\nworkLRQ[35]. Inaddition,wetestedOmniQuantat2bitsourselvesandfounditfailsforOPT\nmodels,whereasoursperformswellforOPTmodelsandalsoachievesanaverage1.96perplexity\nreductionthanOmniQuantonLLaMAat2bits.\nTable15: PerplexitycomparisonsbetweenShiftAddLLMandOmniQuantusingOPTmodelsand\nLLaMAmodelsonWikiText-2. ThegroupsizeissetasthelengthofrowsforOPTmodelsand128\nforLLaMAmodelsfollowingbaselines.\nOPT LLaMA-2\nMethod Bits\n125M 350M 1.3B 2.7B 6.7B 13B 30B 66B 7B 13B 70B\nOmniQuant[51] 4 29.45 23.19 15.04 12.76 11.03 10.30 9.65 - 5.58 4.95 -\nOurs(Acc.) 4 28.72 21.59 14.98 12.65 10.95 10.20 9.63 - 5.58 4.96 -\nOmniQuant[51] 3 35.66 28.2 16.68 13.8 11.65 10.87 10.00 9.83 6.03 5.28 3.78\nOurs(Acc.) 3 31.29 24.24 21.53 13.68 11.18 10.39 9.63 9.43 5.89 5.16 3.64\nOmniQuant[51] 2 311.39 186.9 484.51 1.1e6 9.6e5 3.6e4 9.3e3 5.2e3 11.06 8.26 6.55\nOurs(Acc.) 2 51.15 40.24 29.03 20.78 13.78 12.17 10.67 10.33 8.51 6.77 4.72\nTable 16: Perplexity comparisons between ShiftAddLLM and FlexRound. The group size of\nFlexRoundissetasthelengthofrowsfollowingthepaper.\nLLaMA-2\nMethod Bits\n7B 13B 70B\nFlexRound[34] 4 5.83 5.01 -\nOurs(Acc.) 4 5.58 4.96 -\nFlexRound[34] 3 6.34 5.59 3.92\nOurs(Acc.) 3 5.89 5.16 3.64\nF MoreResultsforMixedBitAllocation\nTovalidatetheeffectivenessandapplicabilityofourautomatedbitallocationacrossdifferentLLM\nmodels,weevaluatedandcomparedOurs(Mixed)withOurs(Lat.). TheresultsareshowninTab.17.\nOurs(Mixed)furtherreducesperplexitybyanaverageof96.86,3.23,and2.63forOPT,LLaMA,\nandGemmamodels,respectively,undercomparableorevenlesslatency. Thissetofexperiments\nfurthervalidatestheapplicabilityofourautomatedbitallocationstrategytodifferentLLMs.\nTable17: Perplexityandcorrelationresultsofourmixedbitallocation.\nOPT LLaMA Gemma\nMethods Bits\n125M 1.3B 13B 2-7B 2-13B 3-8B 2B\nCorrelation(τ) 0.910 0.905 0.915 0.931 0.929 0.897 -\nOurs(Lat.) 2 712.55 40.28 14.76 9.58 12.57 34.40 16.52(3bits)\nOurs(Mixed) 2.2 435.84 27.37 13.79 8.97 8.16 29.72 13.89\nInaddition,wewanttoclarifythat,foreachmodel,wesearchfortheoptimalbitallocationwith\nnegligibleoverhead(e.g.,1%10%ofthereparameterizationtime). Forexample,ittakes0.5seconds\nforsearchingversus72secondsforreparameterizingOPT-125Mwithasinglebitconfiguration,and1\nminuteforsearchingversus13minutesforreparameterizingOPT-13Bwithasinglebitconfiguration.\nThisisachievedbyleveragingtheproposedproxycriteria(asshowninSec.4.3),insteadofsearching\naccordingtothereparameterizationerrors,whichistime-consumingandrequiresrunningmodelsat\n18\neachbit. Usingtheproxycriteria,thebitallocationcandidaterankingsarehighlycorrelatedwiththe\nrankingsobtainedusingactualreparameterizationerrors,withaKendallτ of0.910/0.905/0.915for\nOPT-125M/1.3B/13Band0.931/0.929/0.897forLLaMA-7B/13B/8B,respectively.\nG 4-BitResultsandExplanationforUsingLowerBitWidths\nWefurtherprovidethe4-bitresultsinTab.18. TheseresultsshowthatShiftAddLLMconsistently\noutperformsthebaselinesat4bits,achievingaverageperplexityreductionsof0.90/1.32/1.00and\n0.44/0.22/0.02ascomparedtoOPTQ/LUT-GEMM/AWQ,usingOPTmodelsandLLaMAmodels,\nrespectively.\nTable18: PerplexitycomparisonsoftheOPTmodelsandLLaMAmodelswith4-bitquantizationon\nWikiText-2. WesetthegroupsizeasthelengthofrowsforOPTmodelsand128forLLaMAmodels\nfollowingbaselinesforfaircomparisons.\nOPT LLaMA\nMethod Bits\n125M 350M 1.3B 2.7B 6.7B 13B 30B 1-7B 2-7B 2-13B 3-8B\nOPTQ[18] 4 31.12 24.24 15.47 12.87 11.39 10.31 9.63 6.22 5.69 4.98 7.63\nLUT-GEMM[48] 4 31.93 24.09 16.15 13.34 12.09 10.40 9.99 5.94 5.78 5.06 6.85\nAWQ[38] 4 31.66 7.4e3(outlier) 15.22 13.19 11.23 - - 5.78 5.60 4.97 -\nOurs(Acc.) 4 28.72 21.59 14.98 12.65 10.95 10.20 9.63 5.76 5.58 4.96 6.46\nWepreviouslyconsideredlower-bitquantizationbecauseweaimtopushtheaccuracy-efficiency\nboundarytolowerbitswithminimalaccuracycompromise. Thisismeaningfulforlarge-scaleLLMs,\nwhere even at3 bits, they remain memory-bound. Asanalyzed usingthe Rooflinemodel shown\ninFigure5of[73], forNvidiaA6000GPUs, theturningpointfrommemory-boundtocompute-\nbound is 200 arithmetic intensity (OPs/bytes). For LLaMA-7B models, all the operators in the\ndecode/generationphasehavearoundorlessthan1arithmeticintensity,asshowninTable1of[73].\nEvenat4bits,thearithmeticintensityisapproximately1÷3×32=8(sameopsbut4/32fewer\nmemoryaccesses),whichisfarlessthantheturningpointof200andthusremainsmemory-bound,\nletalonelargermodelslikeLLaMA-70Borbeyond. Reducingfrom4bitsto2bitscanhelpincrease\nthearithmeticintensityandthusthetheoreticallymaximumperformanceby2x,from6144GOPS\nto12288GOPS.Ifmemoryisnotabottleneckformuchsmallercasesorprefillstages,higherbits\ncan be used for better accuracy. Our goal is to offer an additional option and trade-off for large,\nmemory-boundcases,withoutforcingtheexclusiveuseof2bits.\nH ComparisonwithMSFP\nMSFP[13]isanimportantpriorworkthatemploysasharedexponentacrossagroupofelementsand\nshiftsthemantissaaccordingly,mimickingmultiplicationbypowersoftwo. Incontrast,weclarify\nthatourapproachdiffersfromMSFPintwokeyaspects:\n1. NatureofApproach: MSFPusessharedexponentsbutreliesonvariousshiftedmantissa\nto represent the weights; without this, all weights would collapse to the same value. In\ncontrast, we do not use shared exponents for scaling factors and eliminate the need for\nmantissa. Inparticular,eachscalingfactorisrepresentedasadistinctpower-of-twointeger\n(equivalenttotheexponentsinfloating-pointnumbers,completelyremovingthemantissa\nbits). Inthisway,themultiplicationbetweenafloating-pointactivationandapower-of-two\nintegerscalingfactorcanbesimplifiedtoaddingthecorrespondingintegertotheexponent\nbit of the floating-point activation, as described in Fig. 1 (c). In addition, rather than\nsharingtheexponents,theentirescalingfactorinShiftAddLLMissharedacrossgroupsof\nbinaryweightsinacolumn/block-wisemanner,asillustratedinFig.3(a)anddetailedin\nSec.4.2,carefullydesignedtooptimizebothweightquantizationandoutputactivationerrors\nwithoutconflicts. Hence,therearecleardifferencesbetweentheMSFPdatatypeandour\nquantizationscheme.Infact,ourmethodisorthogonaltoMSFPandcanbecombinedwithit\nbyrepresentinginputactivationsinMSFPformoreaggressiveperformanceimprovements.\n2. DeterminingSharedExponentsorScalingFactors: Themethodfordeterminingshared\nexponentsinMSFPorsharedscalingfactorsinourquantizationschemeisdifferent. MSFP\n19\nselectsthemaximumexponenttoshareacrossthebounding-boxsize,i.e.,thenumberof\nelementssharingoneexponent[13],whichissimplerinimplementationyetmightnotbeas\nadaptive. Incontrast,inourShiftAddLLM,thereparameterizedbinaryweightsandscaling\nfactors result from multi-objective optimization. This optimization adaptively designs\nscalingfactorpatternstoavoidconflictsbetweenoptimizingweighterrorsandoptimizing\noutputactivationerrors.\nFinally,intermsoftheperformanceoutcomes,MSFPat4bits(1-bitsignand3-bitmantissa)already\nsuffersfromlargequantizationerrors,asevidencedbythesignificantKLdivergenceshowninFig.\n3of[13]. Incontrast,ourShiftAddLLMat3or4bitscanstillachievecomparableaccuracytoFP\nbaselines. TodirectlycompareShiftAddLLMwithMSFP,weconductedadditionalexperimentsto\ncompare(1)quantizationerrorsand(2)KLdivergenceusingbothmethodsagainsttheirfloating-\npoint counterparts. We randomly selected ten weight matrices from OPT-350M, quantizing or\nreparameterizing them using both methods. The results, as summarized in Tab. 19, indicate that\nShiftAddLLMconsistentlyoutperformsMSFP,achievinglowerKLdivergenceby0.0065,0.0271,\nand0.0952,andreducingquantizationerrorsby1707.3,3251.1,and5862.0at4-bit,3-bit,and2-bit\nquantization,respectively.\nTable19: ComparisonbetweenMSFPandShiftAddLLMwithvaryingbitsonKLDivergenceand\nQuantizationError.\nMethods Bits Avg.KLDivergence Avg.Quant.Error\nMSFP(bounding-boxsize=128) 4 0.0117 4129.1\nShiftAddLLM(groupsize=128) 4 0.0052 2421.8\nMSFP(bounding-boxsize=128) 3 0.0434 7859.9\nShiftAddLLM(groupsize=128) 3 0.0163 4608.8\nMSFP(bounding-boxsize=128) 2 0.1485 14355.7\nShiftAddLLM(groupsize=128) 2 0.0533 8493.7\nI AdditionalClarificationsonEyeriss\nAsemphasizedinSec.5,ourprimaryfocusisonGPUacceleration,specificallythroughthedevelop-\nmentofdedicatedCUDAkernelsupport. Itisworthnotingthat,weintentionallydidnotdelveinto\nspecificASICdesignsinthemainmanuscript,whichwerereferencedonlytodemonstratepotential\nenergysavings.\nToclarifytheEyerissinestimatingtheenergycosts,Eyeriss[8]isawell-knownenergy-efficient\nreconfigurableacceleratorarchitecturedesignedfordeepconvolutionalneuralnetworks(CNNs). It\noptimizesbothdataflowandmemoryaccesstoreduceenergyconsumptionduringneuralnetwork\nprocessing. In our work, we adapt the Eyeriss architecture by modifying its MAC (Multiply-\nAccumulate)array,akeycomponentresponsibleforperformingheavyarithmeticcomputationsin\nCNNs. InsteadofusingtraditionalMACunitsacrossthearray,wereplaceselectedunitswithshift,\nadd,andlookuptable(LUT)operations,aligningwithourproposedShiftAddLLMapproach. This\nmodificationsignificantlyreducesboththeareaandpowerrequirements,withsavingsrangingfrom\n26%to89%indifferentconfigurations. WereferreaderstoFig. 4ofNASA[53],whichvisually\ndemonstratesthedesignprinciplesoftheoverallarchitecture,andillustrateshowreplacingtraditional\nMAC units with shift and add operations leads to significant reductions in both area and energy\nconsumption. Byadaptingtheseprinciples,weenhanceEyerisstobetteralignwiththecomputational\nneedsofbothLLMsandShiftAddLLMswhilemaintainingpowerandareaefficiency.\n20",
    "pdf_filename": "ShiftAddLLM_Accelerating_Pretrained_LLMs_via_Post-Training_Multiplication-Less_Reparameterization.pdf"
}