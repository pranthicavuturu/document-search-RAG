{
    "title": "Signaling and Social Learning in Swarms of Robots",
    "context": "This paper investigates the role of communication in improving coordination within robot swarms, focusing on a paradigm where learning and execution occur simultaneously in a decentralized manner. We highlight the role communication can play in addressing the credit assignment problem (individual contribution to the overall performance), and how it can be influenced by it. We propose a taxonomy of existing and future works on from low-level lossless compression with raw signal extraction and processing to high-level lossy compression with structured communication models. The paper reviews current research from evolutionary robotics, multi- agent (deep) reinforcement learning, language models, and biophysics models to outline the challenges and opportunities of communication in a collective of robots that continuously learn from one another through local message exchanges, illustrating a form of social learning. Keywords signaling, communication, social learning, swarm robotics, decentralized learning and execution, cooperation, evolutionary dynamics, multi-robot systems 1Universit´e Paris Cit´e, CNRS, LIED UMR 8236, F-75006 Paris, France 2ECE Paris, France 3Sorbonne Universit´e, CNRS, ISIR, F-75005 Paris, France 4Sorbonne Universit´e, CNRS, IBPS, Laboratoire Jean Perrin, F-75005 Paris, France 5Department of Information Sciences, Ochanomizu University, Tokyo, Japan 6LIMMS (IRL2820)/CNRS-IIS, University of Tokyo, Tokyo, Japan *Corresponding author: nicolas.bredeche@sorbonne-universite.fr Contents 1 1 2 Dynamics of Decentralized Learning and Execution3 3 A Taxonomy of Signaling Methods in Swarm Robotics 4 4 Current Trends in Signaling for Swarm Robotics 6 4.1 Low Degree of Information Selection . . . . . . . . . 6 4.2 High Degree of Information Selection . . . . . . . . 8 5 Conclusion 10 References 10",
    "body": "Arxiv Preprint\nNov 18th, 2024\nSignaling and Social Learning in Swarms of Robots\nLeo Cazenille1, Maxime Toquebiau2,3, Nicolas Lobato-Dauzier4, Alessia Loi3, Loona Macabre3,\nNathana¨el Aubert-Kato5, Anthony Genot6, Nicolas Bredeche3*\nAbstract\nThis paper investigates the role of communication in improving coordination within robot swarms, focusing on\na paradigm where learning and execution occur simultaneously in a decentralized manner. We highlight the\nrole communication can play in addressing the credit assignment problem (individual contribution to the overall\nperformance), and how it can be influenced by it. We propose a taxonomy of existing and future works on\ncommunication, focusing on information selection and physical abstraction as principal axes for classification:\nfrom low-level lossless compression with raw signal extraction and processing to high-level lossy compression\nwith structured communication models. The paper reviews current research from evolutionary robotics, multi-\nagent (deep) reinforcement learning, language models, and biophysics models to outline the challenges and\nopportunities of communication in a collective of robots that continuously learn from one another through local\nmessage exchanges, illustrating a form of social learning.\nKeywords\nsignaling, communication, social learning, swarm robotics, decentralized learning and execution, cooperation,\nevolutionary dynamics, multi-robot systems\n1Universit´e Paris Cit´e, CNRS, LIED UMR 8236, F-75006 Paris, France\n2ECE Paris, France\n3Sorbonne Universit´e, CNRS, ISIR, F-75005 Paris, France\n4Sorbonne Universit´e, CNRS, IBPS, Laboratoire Jean Perrin, F-75005 Paris, France\n5Department of Information Sciences, Ochanomizu University, Tokyo, Japan\n6LIMMS (IRL2820)/CNRS-IIS, University of Tokyo, Tokyo, Japan\n*Corresponding author: nicolas.bredeche@sorbonne-universite.fr\nContents\n1\nIntroduction\n1\n2\nDynamics of Decentralized Learning and Execution3\n3\nA Taxonomy of Signaling Methods in Swarm Robotics\n4\n4\nCurrent Trends in Signaling for Swarm Robotics\n6\n4.1 Low Degree of Information Selection . . . . . . . . . 6\n4.2 High Degree of Information Selection . . . . . . . . 8\n5\nConclusion\n10\nReferences\n10\n1. Introduction\nA general and well-accepted definition of swarm robotics high-\nlights the deployment of a possibly large collective of robots\neach with limited computation and communication capabili-\nties working together as a result of multiple local interactions\nto achieve a common cause [1, 2, 3, 4, 5, 6]. It is important\nto note that “limited” does not mean “simple”: a hypotheti-\ncal collective of idealized self-aware language-capable robots\ncould still be considered a swarm if decentralized coordina-\ntion is required due to the inherent delay in communication,\neven if the environment is static. The limited capabilities of\neach robot are to be understood as a relative property that puts\ninto relation two conceptual levels: (1) at the individual level,\nthe capabilities of one individual component of the swarm,\nwhich encompass both its physical (sensors and actuators) and\nalgorithmic (memory and computing power) capabilities and\n(2) at the global level, the swarm complexity in terms of its\nsize and spatial configuration, which define the possibilities\nof interactions between its components. While the hardware\ncapabilities of the robots limit the goals that can be achieved,\nthe limitation in software capabilities is the key factor. When-\never memory or computation is lacking at the individual level,\ncollective action requires decentralized coordination, as each\nrobot can only sense and act in its immediate surroundings.\nIn addition, it is important to consider the time component of\ncomputational complexity, which depends on either or both a\ntime-constrained task and an inherently dynamic environment.\nThis implies that the swarm’s response time should be short\nenough for its actions to be relevant.\nIn this paper, we do not impose limitations over the actual\ncapabilities of the robots or on the swarm structure (e.g., a het-\nerogeneous swarm of unconventional robots is possible) and\nallow for different interpretations of what cooperation means\n(e.g., from just avoiding each other to displaying complex co-\narXiv:2411.11616v2  [cs.RO]  19 Nov 2024\n\nSignaling and Social Learning in Swarms of Robots — 2/17\nFigure 1. (A) A swarm of robots is deployed in an unknown environment. Robots must learn together to solve a task. Robots\ninteract locally with nearby robots and physical elements. (B) The decision-making process of a focal robot is based on cues\nfrom the physical world and signals from the social world. (C) Diagram of the communication and control policies for a robot,\ndistinguishing between signals for local interactions and cues from the broader environment. The pink box denotes the policy\nof the robot which gets information from observations (i.e., cues and signals) and produces actions (i.e., effectors and\ncommunication channels). There are two sub-policies for each process, though in practice a single general policy may be used\n(e.g., a single artificial neural network), or multiple policies, either ad hoc or subject to learning.\nordinated strategies). This is covered by this slightly different\nand more accurate definition: swarm robotics involves de-\nploying robotic agents that coordinate in a decentralized\nmanner to achieve a common goal, with each robot limited\nto sensing and acting within its immediate environment1.\nThat definition opens new venues for thinking about the future\nof the field, including bridges toward other fields with similar\nconcerns, as we will see later.\nThe design of efficient individual policies within a swarm\nof robots usually relies either on carefully crafting (possibly\nbio-inspired) behavioral rules or on using learning and/or evo-\nlutionary optimization algorithms. The robots’ policies, which\nare generally similar across a given swarm, do not change af-\nter deployment. While this approach is enough in many cases,\n1We consider swarms of self-interested robotic agents as off-topic from\nthe present study\nit is a limitation whenever the target environment is unknown\nbefore deployment or changes over time. This is why an im-\nportant effort, originally stemming from evolutionary robotics,\nhas been made since the advent of the 21st century to develop\ndecentralized online evolutionary learning algorithms. This\nfamily of algorithms aims at enabling a robot swarm to adapt\ncontinuously while already deployed in the real world, as\nillustrated in Figure 1-A, and has been referred to as either\nembodied evolutionary (EE) robotics [7] or social learning\nfor swarm robotics (SLSR) [8, 9]. They have achieved re-\nmarkable success in terms of the number of implementations\non real robots when compared to other fields working with\nlearning multi-robot systems (see [10] for a review).\nIn this paper, we posit that the class of problems addressed\nwhen using such social learning or embodied evolution al-\ngorithms is covered by the umbrella term of decentralized\nlearning and execution (DLE), which designates a paradigm\n\nSignaling and Social Learning in Swarms of Robots — 3/17\nthat will be more familiar to the wider reinforcement learn-\ning community [11, 12]. This contrasts with the widely used\n“design then deploy” paradigm, which includes (1) prior hand\ndesign, (2) offline evolutionary robotics, and (3) multi-agent\nreinforcement learning under the centralized training and de-\ncentralized execution paradigm.\nWe focus here on how communication within a swarm\nof robots can be used to improve coordination under the\nDLE paradigm. Communication can obviously be used by\neach robot in the swarm to enable information sharing and/or\nsynchronized behavioral response [3, 4]. However, communi-\ncation can also play a role in the very nature of the learning\nprocess as all computation regarding learning under the DLE\nparadigm is performed on the field, without any human or\ncentral computer involved. A straight-forward form of com-\nmunication in this context is exemplified by the crude control\nparameter sharing at work in embodied evolutionary algo-\nrithms [13, 7, 14, 9], where (either all or a sub-part of) the\nneural weights of Artificial Neural Networks are sent from one\nrobot to its neighbors, possibly attached with a self-assessment\nof its performance from local observations (details in [10]).\nIn Section 2, we start by exploring how working in the\nDLE paradigm raises unique challenges, whether commu-\nnication among robots is enabled or not. We expose how\nusing DLE can lead to counter-intuitive consequences due\nto learning in a decentralized fashion, in particular regarding\nunwanted and counter-productive competition among robots.\nIn Section 3, we propose a taxonomy to characterize existing\nand future works on communication. Section 4 offers a re-\nview of existing works that draw from several very different\ndomains such as biophysics, evolutionary robotics, language\nevolution, multi-agent deep reinforcement learning, and lan-\nguage models – to provide an overview of current and future\ndirections. We propose a classification of communication\nmeans along the axes of information selection and physical\nabstraction, ranging from raw information directly available\nin the environment in Sec. 4.1 (e.g., transfer of heat or matter,\neither as raw information or as mathematical abstractions) up\nto high-level language-based communication in Sec. 4.2 (e.g.,\nemergent or human-like syntax and grammar). Finally, the\nlast Section summarizes important ideas explored earlier and\nprovides comments and considerations for the future.\n2. Dynamics of Decentralized Learning\nand Execution\nAs stated in the Introduction, natural evolution and social\nlearning are good examples of processes working under the\nDLE paradigm. Individuals compete with one another to\ngain a selective advantage. Combined with random variations\nand inheritable traits, the traits of successful individuals will\nbecome more frequent over time. Of course, there is a stark\ncontrast between natural systems and swarm robotics systems:\nwe engineer the robot swarm to address a particular problem\ndefined before deployment which may require coordination to\nbe addressed (foraging, exploration, patrolling, transporting,\nconstruction, or monitoring to give a few examples [15]).\nWhile the desired outcome may be relatively easy to define,\nthe challenge is to endow each robot with the capability to\nassess how much it contributes to solving the task, i.e., self-\nassessing the robot’s contribution to the global welfare of the\ncollective, which is itself determined by how efficiently the\ntask is solved.\nIn a collective, devising the contribution of each individ-\nual is referred to as the credit assignment problem, which is\nwell known in the multi-agent and cooperative game theory\ncommunities [16, 17]. If a complete alignment of the indi-\nvidual’s interest with the global welfare of the collective is\npossible, the best actions from the robot’s viewpoint will also\nbe the best for the collective. In a setup where individual\npolicies are learned, this corresponds to converging towards\na Nash Equilibrium that is also a social optimum, meaning\nnone of the robots has the incentive to deviate from its current\nbehavioral strategy as it is already the best the robot can do\nreward-wise (see [18, 19, 20] for theoretical considerations in\ndistributed robotic systems, and [21] for a practical example\nwith evolutionary learning in a swarm of robots where there is\na mismatch between evolutionary stable strategies and social\noptimal strategies).\nA direct way to make individual interests coincide with\nthat of the team would be to provide each individual with a\nmeasure of their contribution to the global performance. How-\never, estimating the marginal contribution of each robot\nto the performance of the collective is intractable in the\ngeneral case. Even in an idealistic setting, when a scenario\ncan be replayed an indefinite number of times and robots can\nbe removed or added at will, computation time for estimating\nthe marginal contributions for each individual grows expo-\nnentially with the population size as all subsets of individuals\nmust be considered [22, 23, 24]. It is also interesting to note\nthat the more classic reinforcement learning methods using\ncentralized learning do not yield optimal results, as the robots’\nmarginal contributions are often partially or badly estimated\neven by the centralized critic used in multi-agent (deep) rein-\nforcement learning [12]. One efficient simplifying hypothesis\nused in the field of evolutionary collective robotics is to con-\nsider a swarm of clones [25, 26], turning what originally looks\nlike a collective decision-making problem into an optimiza-\ntion problem as a single control parameter set is used for the\nentire swarm and optimized in a centralized fashion. This\nmethod is however not applicable under the DLE paradigm as\nit requires a centralized coordinator for learning.\nApproximation methods to estimate on the fly the marginal\ncontribution of robots in a collective exist, of course, and trade\ntractability against a lack of optimality or assume simplifying\nhypotheses on the class of problems to be addressed (see in\nparticular [27, 28, 29]). A straightforward method is for the\nhuman supervisor to define a priori an explicit evaluation\nfunction embedded in each robot whose goal is to evaluate\nlocally the performance of said robot. This is the case with\nmost works in embodied evolution and social learning in\n\nSignaling and Social Learning in Swarms of Robots — 4/17\nswarm robotics, where each robot computes an estimate of its\nperformance based solely on directly available information\nand self-assessment [10]. This is also the case in cooperative\nmulti-agent learning whenever each agent is an independent\nlearner, i.e., considering others as part of a nonstationary\nenvironment [30]. In both cases, the global performance\nwill depend on the ability of the human engineer to design a\nfunction that provides a reliable estimate of the performance\nof a robot, aligning local motivation with the desired global\noutcome. Obviously, this can quickly become challenging as\nthe task and/or the environment grow in complexity — e.g.,\nforaging in a field without obstacles can be very different\nfrom foraging in a complex environment where the division\nof labor offers a significant advantage.\nUnfortunately, the slightest misalignment between the\nindividual’s interests and that of the collective can lead to a\nsuboptimal group-wise performance. In that case, the whole\nswarm will eventually converge towards a Nash Equilibrium\nthat does not guarantee social optimality. This is explained by\nthe nature of the evolutionary dynamics at work behind social\nlearning in a swarm: elements that play a part in the robots’\nbehavioral strategies are competing among themselves to in-\nvade the population of robots. If the metric used to compare\nthose elements is aligned (resp. not aligned) with the global\ntask, then competition will end up with individual strategies\nthat are optimal (resp. sub-optimal) w.r.t. the task. This can be\nexplained by using the famous selfish gene metaphor popular-\nized by Richard Dawkins [31]: robots are merely vehicles for\ncompeting units (e.g., genes or group of genes, neural network\nparameters, symbols from an emerging language, elements of\nan artificial culture, etc.) facing selective pressure.\nSuch evolutionary dynamics can then have a direct impact\non the long-term behavioral strategies of neighboring robots,\nwith sometimes surprising outcomes such as mutualistic co-\noperation (i.e., cooperation that benefits each involved party)\nand altruistic behavior (i.e., cooperation that involves a net\nloss at the individual level, but which indirectly benefits the\nsurvival of related individuals) [32]. In particular, each in-\ndividual’s strategy is shaped by its inclusive fitness that\ncaptures both its ability to survive and its ability to help\nrelated individuals (a relation that is generally, but not al-\nways, defined at the genotypic level) [33]. Kin selection, the\nprocess by which an individual favors their relatives, is also\nrelevant for the development of cultural adaptation [34] and\nlanguage [35]. This has been shown previously to also be the\ncase with social learning algorithms for swarm robotics [36]:\nrobots can lose part of their survival chances to help robots\nwith whom they share information.\nFigure 2 puts together the two concepts just discussed: (1)\nthe stronger the alignment between the individual’s interest\nand the group’s welfare, the better the performance w.r.t. to\nthe user-defined objective (x-axis) and (2) inclusive fitness,\nwhich shows the degree to which an individual’s interest is\naligned with that of its relatives (y-axis). In this Figure, we\noppose two extreme configurations, one in which individuals\nin the swarm are in confrontation with conflicting interests,\nand another one in which individuals’ interests are aligned\nand individuals cooperate to maximizing the social welfare,\nwhether this incurs an individual cost or not. Obviously, the\nlevel of cooperation for solving the user-defined task will be\nmaximal if alignment is complete and may decrease otherwise\ndepending on the task at hand. Much less obvious is the\ninfluence of inclusive fitness, where an individual cost may be\npaid for the benefit of the whole. The intuition can be given\nby looking at the example of eusocial colonies (e.g., ants and\ntermites) where the fitness of one individual is vastly defined\nby that of its superorganism. In that case, individual actions\nthat benefit the group will be performed, even if they are\ndetrimental to the individual (see [37] for a study of the impact\nof inclusive fitness in evolutionary collective robotics). To\nsome extent, a high level of inclusive fitness can compensate\nfor a misalignment between the individual’s interest and that\nof its conspecifics. In this Figure, we formulate this relation\nas the degree to which the Nash Equilibrium of the evolving\npopulation will converge to the socially optimal outcome with\nrespect to the user-defined task.\nWe now turn back our attention to communication in a\nswarm of robots and the implication of previous considera-\ntions on it. Communication can be used to endow each robot\nwith the ability to locally estimate its contribution in an online\nfashion, as shown by recent works in the field of coopera-\ntive multi-agent reinforcement learning that proposed using\ncommunication between robots to locally aggregate the data\navailable on the performance of the swarm as a whole [38, 39].\nIn that way, communication can be used to gather data on a\nmacroscopic scale so that more information is available to\neach individual regarding the performance of the whole, and\npossibly to provide an individual’s ability to measure its con-\ntribution. Although this does not solve the credit assignment\nproblem, communication can help to perform counterfactual\nreasoning to simulate hypothetical scenarios in the absence of\nthe focal robot [40].\nUnfortunately, communication also suffers from a possi-\nble misalignment between the Nash Equilibrium and socially\noptimal strategies, especially if it evolves (e.g., emergent sig-\nnaling or language). In case of misalignment, environmental\ncontingencies and competitive pressure among individuals\ncan lead to sub-optimal communication strategies, as evolv-\ning communication undergoes the same pressures as learning\nthe action policy, resulting in robots developing sub-optimal\ncommunication efficiency to gain a competitive advantage\nagainst competitors [41]. In turn, evolving communication\nmay benefit from robots with a higher degree of inclusive\nfitness and/or a shared interest between individuals [42].\n3. A Taxonomy of Signaling Methods in\nSwarm Robotics\nFirst, let us start by narrowing the scope regarding the nature\nof communication we are interested in by distinguishing cues\nfrom signals. Cues provide information to the focal individual,\n\nSignaling and Social Learning in Swarms of Robots — 5/17\nFigure 2. Alignment of Nash Equilibrium with Social\nWelfare with respect to the degree of inclusive fitness and the\ndegree of shared interest among robots. The X-axis shows\nhow aligned the individual’s interest (e.g., its local fitness\nfunction) is with that of the group, which is uniquely defined\nby its ability to optimally solve the task. The Y-axis shows\nthe level of inclusive fitness experienced by each individual\nin the population (e.g., due to kin recognition, environmental\nviscosity, etc.). The four text boxes on the graph provide\nexamples using the well-known theoretical games of\nPrisoner’s Dilemma (PD, a competitive game where players\nshould defect) and Stag Hunt (SH, a coordination game\nwhere players should cooperate), and two extremes regarding\nhow inclusive is an individual’s fitness in a population\n(unrelated individuals working for their own sake vs. a\npopulation of clones working for the collective).\nextracted from the environment through direct observations\n(e.g., the relative alignment of nearby conspecifics [43, 44])\nor identification of body markers (e.g., a conspecific’s phe-\nnotypic trait). They do not require an identified interlocutor\nand, if another individual is involved, they are not produced\nintentionally. Signals involve an emitter and at least one re-\nceiver. They are produced intentionally by the emitter through\none or several available modalities (auditory, visual, olfactory,\netc.), and can vary greatly in complexity, from the production\nof a chemical compound to human language. The interested\nreader can refer to [45] for a comprehensive introduction to\ncues and signaling in nature.\nFigure 1-B and -C provides an illustration from a robot\nswarm perspective. Each robot may experience both cues,\nobserved in the physical world, and signals, originating from\nother robots and received through dedicated channels such\nas short-range proximity communication devices (e.g., in-\nfrared, visible light, radio, etc.). We explicitly limit our scope\nto the moment when information from the signal is readily\navailable to the robot, leaving any pre-processing transparent\n(signals can be initially extracted from another modality such\nas speech and sign language, as is the case in robot-human\ncommunication [46]).\nCommunication strategies in swarm robotics cover both\nstigmergic communication and direct communication. Stig-\nmergic communication works by leaving a trace in the en-\nvironment [47, 48], such as a virtual pheromone trail for\nother robots to consider [49, 50]). Direct communication\ninvolves explicit exchanges of information among robots, ei-\nther through pre-defined or emergent signaling strategies. In\nparticular, emergent communication strategies evolve natu-\nrally from the interactions and the optimization processes at\nwork within the swarm, enabling robots to converge towards\nefficient adaptive behaviors without centralized control.\nIn addition to whether signaling strategies are learned or\npre-defined, the nature of the signals can vary greatly tak-\ning, for instance, discrete and continuous forms. Low-level\ncommunication methods often mimic natural processes like\ndiffusion, reaction, and advection, enabling robots to share in-\nformation about their local environment. High-level methods\ninvolve more abstract forms of communication, such as emer-\ngent or structured language models, allowing for sophisticated\ninteractions and decision-making.\nSignaling also necessarily incurs some form of restriction\nover the nature and the amount of information that will be\nshared, driven by the necessity to transfer relevant information\nonly. This process may be lossless (e.g., suppressing redun-\ndant information, compressing information without loss, or\nchanging the way information is represented) or lossy (e.g., ig-\nnoring irrelevant information, compression with loss). In prac-\ntice, as the complexity of the environment increases, so does\nthe need for sharing only what is relevant for the task at hand\n(e.g., selection attention in humans [51], or methods used to\navoid the curse of dimensionality in machine learning [52]).\nWe propose two axes for classification using the degree\nof information selection and the degree of physical abstrac-\ntion. On the one hand, information selection aims at reducing\nthe quantity of information shared by loosing information\nthat is not deemed relevant. On the other hand, physical ab-\nstraction aims at changing the way information is represented\nwithout loss of information in order to reveal what is already\npresent. This is illustrated in Figure 3. The left part of the\nfigure provides an analogy with algebra to provide an intu-\nition using a mathematical metaphor. The right part maps\nwell-known approaches used in swarm and collective robotics,\nwhich will be explored further in the later Sections.\nIn the region considering a low degree of both information\nselection and physical abstraction, communication processes\nare closely tied to raw physical phenomena, such as reac-\ntion, diffusion, and advection. These methods mimic natural\nprocesses to transfer information, focusing on detailed, low-\nlevel interactions. Increasing the level of physical abstraction\n(x-axis) enables the extraction of hidden but highly relevant\ninformation from raw information such as using spectrum\nanalysis or Fourier transforms, e.g., to capture geometric in-\n\nSignaling and Social Learning in Swarms of Robots — 6/17\nFigure 3. Signaling methods can be projected in a two-dimensional plane using information selection and physical abstraction\nas main components. Left: An algebraic analogy for information selection and physical abstraction in communication\nprocesses. Changing the degree of information selection can be done through operations like restriction to a subspace\n(projection with or without loss). Changing the level of physical abstraction can be done via a change of basis, such as\ntransforming a complex matrix into a simpler diagonal form, illustrating how information can be simplified and structured.\nRight: Different approaches to communication in robotics, mapped by information selection and physical abstraction.\nLow-level methods include biophysics-inspired processes, while high-level methods involve language models and emergent\nlanguages.\nformation of collective spatial configurations [53]. Similarly,\na second axis (y-axis) explores how increasing the degree of\ninformation selection can extract relevant information in dif-\nferent forms. For example, sharing parameters of an artificial\nneural network controller that maps sensory inputs to motor\noutputs, as is common in adaptive swarm robotics [10], can\nbe seen as a highly compressed (and biased) instance of a\nreaction-diffusion process. Finally, the extreme of both axes\npoints towards signaling strategies with high degrees of in-\nformation selection and physical abstraction, where we can\nfind, e.g., the use of a communication apparatus that is based\non a large language model, enabling human-level structured\nperception and signaling.\n4. Current Trends in Signaling for Swarm\nRobotics\nIn this Section, we provide a review of relevant methods from\nswarm robotics as well as other domains, to reveal what the fu-\nture states of signaling could be, considering both ad hoc and\nemerging signaling methods. The Section follows the struc-\nture provided earlier: we first describe signaling methods with\na low degree of information selection in Sub-section 4.1, then\nmove up to those with a high degree of information selection\nin Sub-section 4.2. In each Sub-section, strategies with differ-\nent degrees of physical abstraction are described, also draw-\ning from domains beyond that of swarm robotics. We make\nsignificant room for signaling methods used in multi-agent\nreinforcement learning as well as in the currently popular\ndomain of large language models (LLMs). As mentioned in\nthe Introduction, we stress that while existing swarm robotics\nhardware is still technically limited, this state of affairs may\nchange in the near future. As a consequence, we expect that\ncollective systems that can be identified under the umbrella of\nrobot swarms will feature embedded computation capabilities\npowerful enough to run, and possibly train in real-time, LLMs\n(e.g., LLMs can already run on limited hardware [54]).\n4.1 Low Degree of Information Selection\nSummary: In this section, we investigate communication\nin multi-agent systems as information exchanges with mini-\nmal simplification of the baseline observable data from local\nagents. This involves two types of signaling schemes. Scheme\n(1) involves signals that reflect local observations directly.\nBiological examples include social insect communication and\nautoinducers exchanges in bacteria. In swarm robotics, these\nprinciples are applied through algorithms mimicking biologi-\ncal behaviors through local interaction and communication\nrules. Scheme (2) includes signals with a high level of physical\nabstraction and structure, such as the use of Fourier trans-\nforms and wavelets to analyze and share periodic patterns and\nmulti-scale features in data. In swarm robotics, agents might\nshare with immediate neighbors their computed gradients or\nneural network weights, or perform Fourier transforms or\neigenspectrum analysis to understand and communicate the\nunderlying structure of complex data.\nMulti-agent communication with low information selec-\ntion (lower part of Figure 3) involves the direct and explicit\nexchange of observable information from local agents [55].\nThe signals are transmitted in a form that retains most of the\noriginal observations, without any extensive selection mech-\n\nSignaling and Social Learning in Swarms of Robots — 7/17\nanism removing parts of the baseline observation data. In\nFigure 1C, communication with low information selection\ninvolves minimal loss of information between the cues from\nthe environment and their packaging into signals sent to other\nagents. This approach is relevant either 1) in cases where\nthe observations already have low dimensionality, 2) in cases\nwhere most of the observations contribute to the collective dy-\nnamics of the group, or 3) in cases where knowing in advance\nwhich parts of the observations are useful to communicate is\ndifficult to achieve.\n(1) Low physical abstraction: In the case with both low\ninformation selection and low physical abstraction (lower left\nquadrant in Figure 3), signals represent direct and tangible\ninformation about the environment, with minimal transfor-\nmations from the observations of local agents. This type of\nsignaling is exemplified by the following biological systems:\nautoinducers exchanges among bacteria [56], auditory and tac-\ntile signals in Drosophila [57], chemical alarms released from\ncertain fish species to alert conspecifics of the presence of a\npredator [58], the bioluminescence mechanisms of fireflies\nfor mate attraction [59], electric signals in certain fish [60],\nor birds songs to attract mates [61] or to signal aggressive\nintent [62].\nSwarm robotics algorithms deployed on small robots or\nwith self-organization capabilities also fit in this quadrant,\nbecause they rely on simple ad hoc signaling rules based\ndirectly on local states and observations, without significant\nloss of information or transformations. For instance, in [42]\na signaling behavior is optimized so that robots emit specific\nsignals when they are close to an object or zone of interest. In\n[63] robots share all their local sensory information with their\nneighbors during a predator-prey task. The relative position of\neach robot or site of interest is locally broadcasted in [64, 65].\nIn [66], robots can probabilistically broadcast information\nfrom one to another to assess the dynamics of information\npropagation.\nMulti-agent systems inspired by physical dynamics can\nalso be classified in this category: e.g., reaction-diffusion [67],\nchemical oscillations [68] and morphogenesis [69] can be seen\nas multi-agent systems where agents are spatial discretization\npoints and global dynamics emerge from local interactions\n(communication without information loss). Diffusion is a fun-\ndamental physical process where particles spread from areas\nof higher density to areas of lower density. In multi-agent\nsystems, diffusion can serve as a means of communication.\nFor example, chemical signaling in cells relies on molecu-\nlar diffusion to guide movement, growth, and specialization.\nReaction-diffusion systems involve the creation, transforma-\ntion, or destruction of diffusive elements through local inter-\nactions to create complex patterns. In multi-agent systems,\nreaction-diffusion can explain how agents interact with their\nenvironment and each other via chemical signals [70].\nMoreover, making robots out of molecules allows the cre-\nation of massive swarms of millions of robots. In the last\ndecades, researchers have used artificial DNA as comput-\ning and building blocks to develop molecular robotics [71].\nSuch robots can take the form of DNA origami that self-\nassemble into complex 3D nanostructures, able to connect to\neach other or change configuration depending on biochemical\ncues [72, 73, 74, 75, 76, 77, 78, 79]. Simpler structures can\nalso be programmed to move on tracks [80] and sort cargoes\nat the nanoscale [81]. Coating beads with DNA allows to\ncreate micro-robots with higher computing capabilities, with\nreaction-diffusion serving to form both controllers and sig-\nnals [82, 83, 84]. Another emerging field is controllable active\nmatter, where self-propelled agents process chemical signals\nlocally, leading to self-organization [85, 86, 87, 88, 89, 90, 91].\nA final example is the Turing model of morphogenesis, which\nexplains how patterns like animal stripes and spots emerge\nfrom the interaction of diffusing chemicals, inspiring a swarm\nrobotics implementation where local communication mimics\na reaction-diffusion system to achieve shape formation [92].\n(2) High physical abstraction: The lower right quad-\nrant of Figure 3 represents communication methods involving\nabstract and structured information, often detached from di-\nrect physical processes, with minimal information loss from\nobservations.\nThis includes methods like broadcasting gradients where\nagents locally exchange mathematical abstractions rather than\ndirect physical signals. Gradients represent the internal state\nof each agent’s model, rather than a direct physical quantity.\nFor instance, gradient propagation can compute a geodesic\ndistance to a source robot by incrementally communicating\nvalues through neighboring agents [93]. Gradient broadcast-\ning can occur through microscopic rules derived from local\nobservations [94, 95, 93], or via multi-agent reinforcement\nlearning where the gradients of the loss function are broad-\ncasted from agents to agents [96]. Having differentiation\ncapabilities, i.e., access to the gradient of local states and/or\nmessages, allows the training process to directly use this in-\nformation (e.g., via gradient descent algorithms), accelerating\nconvergence.\nEigenspectrum analysis [97] also fits this quadrant, exam-\nining eigenvalues and eigenvectors to reveal the underlying\nstructure of data. Eigenspectrum analysis is widely used in\na variety of fields, ranging from signal processing and ma-\nchine learning to network analysis. This process can involve\nsimilar dynamics as those obtained in the lower left quadrant\n– however, it will also use mathematical tools to change the\nrepresentation of information without loss of information. For\ninstance, in [53] a swarm of Kilobot robots estimates, in a\ndecentralized way, the eigenspectrum of the communication\ngraph between robots. Such properties are then used to reach a\nglobal consensus on the shape of the swarm, achieving arena\nshape recognition. This process is achieved by relying on\na physics-inspired communication scheme based on the dif-\nfusion of heat across the swarm and mathematical tools to\nlocally extract the second eigenvalue λ2 of the graph Lapla-\ncian, a direct fingerprint of the arena shape containing the\nswarm.\n\nSignaling and Social Learning in Swarms of Robots — 8/17\nFourier transforms [98] and wavelets are other examples\nof abstract tools. Fourier transforms convert signals between\nthe time and frequency domains, enabling agents to analyze\nand share information about periodic patterns. Wavelets de-\ncompose data into different scales, allowing agents to com-\nmunicate detailed features of a signal, from broad trends to\nfine details. While Fourier transforms and wavelets are not\nyet used to process signals in swarm robotics settings, their\ncapabilities to work with more abstract representations may al-\nlow a new class of communication schemes – e.g., to perform\ndistributed spectral analysis as a result of communication, as\nin [53].\n4.2 High Degree of Information Selection\nSummary: In this section, we explore decentralized communi-\ncation as viewed from the prisms of the information bottleneck,\nlanguage evolution, and multi-agent reinforcement learning in\nsituated environments. Reinforcement learning approaches to\nemergent communication are examined, highlighting both ben-\nefits and challenges. We emphasize the opportunities provided\nby LLMs for advancing communication in swarm robotics,\nnoting their strengths in generating human-like language and\nreasoning, and challenges such as biases, hallucinations, em-\nbodiment, and efficient deployment on robots. Overall, we\npresent a range of approaches, with different degrees of phys-\nical abstraction, that enable decentralized agents to learn\ncommunication.\nIn realistic decentralized environments, all observed infor-\nmation is not relevant to transmit to partners. Thus, a higher\ndegree of information selection (upper part of Figure 3) is re-\nquired to allow efficient transmission of the relevant informa-\ntion. This task can be decomposed into two complementary\nsub-tasks: (1) selecting relevant information and (2) transmit-\nting this information. The selection task involves extracting\nparts of the observed information that are relevant to other\nagents. The transmission task requires coding this information\nso other agents understand it while ensuring that bandwidth\nconstraints are respected. Both tasks are highly interconnected.\nThe selected information requires adequate means of coding\nto be transmitted without (or with minimal) loss. The trans-\nmission means, in turn, influence the information selection by\ndictating what information can be transmitted efficiently [99].\nThis is a form of information bottleneck, where agents need\nto generate a compressed mapping of their observations, that\ncontains as much information as possible related to the task at\nhand [100]. As communication comes necessarily at a cost,\nlanguages operate a trade-off between meaning and compres-\nsion [101, 102], maximizing expressiveness while minimizing\ncommunication costs.\nStudying language games shows how languages emerge\nfrom this information bottleneck, and from various ecological\nconstraints.\nIn his seminal work, Luc Steels [103] demon-\nstrated that having a dynamic population of embodied agents,\nwhose reasoning is unknown to one another, motivates the\nemergence of a shared compositional language. In the it-\nerated learning framework [104, 105], the emphasis is put\non a transmission bottleneck that occurs when language is\ntransmitted between successive generations of agents, driving\nlanguages to adopt simple and compositional structures. Fol-\nlowing works have shown that emergent languages are also\nshaped by environmental [106, 107] and physiological [108]\nconstraints. These experiments highlight the different require-\nments for languages to originate in populations of independent\nagents, and demonstrate the emergence of efficient naming\nand grammatical conventions [109, 110, 102, 111].\nHowever, these language games still heavily simplify the\ncontext of communication interactions, by making the agents,\ntheir observations, and their actions, solely defined by the com-\nmunication game. Previous works have classified this kind of\nsetting as non-situated [112], as opposed to situated agents\nthat have a localized existence and can physically interact\nwith their surroundings. In situated environments, communi-\ncation is one of many interfacing processes. It can be used for\ncommunicating not only about observations, but also about\nintents, or even about task-agnostic and abstract concepts. It\nmay involve non-cooperative agents. It might not even be re-\nquired at all times. In such realistic settings, choosing which\ninformation is relevant to communicate is a much more com-\nplex task that involves reasoning about the current state of the\nenvironment, the agent’s objective, and the current knowledge\nand reasoning of other agents. In that sense, learning to com-\nmunicate is inherently a multi-agent problem of learning how\nto behave in a dynamic, partially observable environment.\nRecently, research in multi-agent reinforcement learning\nhas tackled such situated environments, where performance\ndepends on a combination of physical and communication be-\nhavior [113]. In this context, multi-agent systems learn, often\nwith centralized training and decentralized execution, to gen-\nerate messages that participate in maximizing future returns.\nHere, messages are continuous vectors generated by neural\nnetworks inside the agents’ system. This makes communica-\ntion a differentiable sub-step of the action selection process,\nwhich can be learned fully end-to-end as a tool for maximizing\nreturns [114, 115, 116, 117]. Because the message generation\nis differentiable, gradients can flow between agents. Thus,\nmessages are explicitely trained to help other agents maximize\ntheir rewards. This approach has been extended in various\nways for more targeted information sharing [118, 119, 120]\nor to limit bandwidth usage [121, 122, 123, 124]. Similar ap-\nproaches have been developed using discrete symbols for com-\nmunication [125, 126, 127, 128, 129]. In those, agents have to\nreach a consensus on the meaning of each symbol through trial\nand error. The compression constraint depends both on the\nsize of the vocabulary and the size of the sequences. Previous\nworks have shown that imposing constraints on both of these\nattributes induces emergent languages to develop common\ncharacteristics of natural languages such as compositional-\nity [130, 131] and abbreviation of frequent words [132].\nHowever, learning emergent communication through this\ntask-oriented reinforcement learning process has many im-\n\nSignaling and Social Learning in Swarms of Robots — 9/17\nportant limitations. As already mentioned, it often requires a\ncentralized learning algorithm to allow reinforcement learning\ntools to reliably converge to adequate solutions. As with all\ngradient-based learning methods, it acts as a black box that\nlacks practical ways of interpreting and measuring its effi-\nciency [133, 134]. More importantly, differentiable emergent\ncommunication gives no guarantee of learning to communi-\ncate about concepts from the environment. Rather, guided\nby return maximization, agents converge to a consensus that\nmay seem random to the human eye [135]. It lacks ways of\nanchoring its concepts in environmental, task-agnostic modal-\nities. This is akin to the problem of symbol grounding [136].\nHaving emergent communication grounded in meanings from\nthe environment would allow decentralized agents to learn\nto communicate about concepts that are shared with other\nagents, making learning easier and communication more effi-\ncient [103, 137, 106].\nFollowing this idea, grounding approaches have been\nexplored in multi-agent reinforcement learning.\nBy link-\ning communication with visual data [138, 139], natural lan-\nguage [140, 141, 142, 143], or both [144, 145], agents learn\nto generate messages using task-agnostic concepts. In other\nwords, they learn to use concepts dictated by external modal-\nities to transmit information efficiently, instead of search-\ning for a consensus on their own, starting from scratch and\nguided only by rewards. Agents may acquire ”grounded”\nknowledge through a variety of techniques: pre-training on a\nsupervised task [140, 144, 145, 146], alternating between su-\npervision and self-play [147, 146], optimizing the supervised\nand RL objective at the same time [145, 139, 142, 143, 148],\nor constructing additional rewards based on supervised mod-\nels [144]. The nature of the subsidiary tasks depends on\nthe desired type of grounding. An autoencoding task can\nbe added to ensure agents communicate about their observa-\ntions [139, 148]. To ground communication in natural lan-\nguage, agents can be shown examples of human-generated\nsentences [140, 138, 142, 143] or learn to generate similar\noutputs as pre-trained language models [141, 145]. A chal-\nlenge when learning to use natural language is to avoid lan-\nguage drift [145], requiring constant supervision to prevent\nRL agents from forgetting the intended use of the given lan-\nguage [144, 145, 146]. Natural language offers an efficient\nsolution to the information bottleneck problem while allowing\neffortless interpretation and teaming with unknown agents\n(human or artificial).\nWhen using natural language, an obvious solution is to\nturn to LLMs. In addition to being extremely good for gen-\nerating human-like sentences, they can also be grounded in\nvisual and behavioral modalities [149]. Their context window\ncan be exploited in various ways to insert factual informa-\ntion or state objectives to achieve and particular behaviors\nto adopt [150]. This is thanks to two important aspects of\ntraining the LLMs. First, the language-modeling pre-training\nphase shows the model of how humans formulate their rea-\nsoning in natural language. Second, the explicit instruction-\nfollowing task optimized with reinforcement learning from\nhuman feedback [151] trains the LLM to pay close atten-\ntion to what has been requested and how it should be an-\nswered. Consequently, LLMs can be used as a basis for\nmodeling interacting agents [150, 149, 152]. Such agent-\nbased LLMs are given information about the environment,\nthe task, their identity, and their role in the environment, all\ninside an initial prompt. Following this initialization, they\nobserve and act in the environment through visual, textual,\nand physical inputs and outputs [150, 152]. Thanks to their\nreasoning and conversing skills, LLM agents can discuss their\nknowledge and intents with partners before selecting an ac-\ntion [153]. This can even be pushed further with personas\nassigned to each LLM agent, allowing a large diversity of\ndifferent behaviors and offering the advantages of collective\nreasoning [154, 155, 156, 157, 158].\nLLMs offer a nice playground for multi-agent interactions.\nThey efficiently emulate human reasoning and communication.\nTheir built-in interactivity provides a great tool for interpre-\ntation [159] and human-agent interactions [153, 160, 161].\nHowever, several issues with LLMs remain and need address-\ning. First, embodying an LLM is a challenge requiring links\nto be made between language and environmental modalities\n(visual and behavioral). The current development of multi-\nmodal LLMs is a step towards solving this challenge [149].\nBut, these approaches often require a costly fine-tuning phase\nto adapt the model to its new modalities. A subsequent prob-\nlem is the deployment of LLM-based agents on small robotic\nplatforms, which requires engineering work to adapt to the\nconstraints of such platforms. This is especially true for decen-\ntralized robots that must be self-sufficient and are often limited\nin memory and computing power. Furthermore, we need ways\nof countering the intrinsic biases present in human-generated\ndata that LLMs inevitably reproduce [162]. Lastly, the prob-\nlem of hallucinations remains an important obstacle. LLMs\nare known for inventing information and being reluctant to\nadmit when they are wrong [163]. This can lead to issues\nranging from deception to breaking the simulation, which\nrequires more work on methods for detecting, measuring, and\navoiding these hallucinations. While these issues can, and will\ncertainly be addressed, this reminds us that other solutions\nusing smaller models also work and might be preferable in\nmany situations.\nTo conclude, we see that many approaches exist for teach-\ning decentralized agents to communicate about high-dimensional\nenvironmental features. They rely on languages that select in-\nformation to communicate more efficiently. These languages\nabstract physical elements of the world by grounding sym-\nbols in environmental features, allowing the establishment\nof conventions on how information should be transmitted.\nDifferent degrees of physical abstraction may serve differ-\nent purposes. A group of agents specialized in a single task\nmay be content with low physically-abstracted differentiable\nemergent communication learned from task reward. On the\nother hand, established concepts and grammatical rules pro-\n\nSignaling and Social Learning in Swarms of Robots — 10/17\nvide the tools to generalize acquired knowledge, compose new\nideas from fundamental language blocks, and communicate\nwith unknown partners. Thus, higher physical abstraction,\nfound in natural languages, is better fit to handle more general\nsettings.\n5. Conclusion\nWe explored how communication through signaling can be\ncrucial in enhancing coordination within robot swarms operat-\ning under the Decentralized Learning and Execution paradigm.\nWe proposed a structured framework to classify existing and\nfuture signaling methods, covering a wide range of informa-\ntion selection levels and physical abstractions. Throughout\nthe paper, we advocate that swarm robotics with distributed\nonline learning capabilities offer unique challenges, for which\ncommunication can play a positive role, but to which commu-\nnication is also subject. The key messages of our paper are\nsummarized hereafter.\nA path towards complex communication strategies.\nEarlier works in swarm robotics were closely inspired by so-\ncial insects. The current state-of-the-art in swarm robotics\nnow shows a great variety of applications and robotics setups,\nincluding dense to sparse swarms with homogeneous or het-\nerogeneous robots. To account for the fast-paced advances in\nhardware and software, it is important to keep in mind that\nswarm robotics is about the relation between microscopic in-\nteractions and macroscopic organization, which remains valid\neven if powerful computation and signaling capabilities are\navailable. A practical consequence we envision is the advent\nof robots using large language models, composing a society\nof embodied agents with human-like signaling capabilities\nthat are still bound by environmental contingencies (e.g., local\ncommunication only, complex physical interactions). Beyond\nthe anticipated gains in performance, incorporating human\nlanguage-like capabilities can offer valuable benefits with re-\nspect to explainability and human-robot interaction through\nthe use of a shared language.\nDecentralized learning and adaptive dynamics in Swarm\nRobotics present a unique challenge. Addressing the prob-\nlem of distributed credit assignment is a well-known challenge\nin multi-agent systems. However, conducting learning in a\ndecentralized and online fashion adds another layer of com-\nplexity, especially when policy parameters hop from one robot\nto the next. A consequence is that nearby robots can share sim-\nilar parameters, which can indirectly cause either altruistic or\ncompetitive behaviors depending on the degree of relation be-\ntween individuals (see Section 2). Differing from natural sys-\ntems where the population may grow, the fixed size of a robot\nswarm impacts where competition occurs: robots are mere\nresources for which policy parameters are competing, rather\nthan the opposite. Exploring the long-term adaptive dynamics\nof behavioral strategies (in which signaling is included) in\ndynamic and unpredictable environments will be critical for\ndeveloping adaptive and resilient swarm systems. This opens\nup an exciting avenue, requiring an interdisciplinary research\neffort, integrating expertise from fields such as evolutionary\ngame theory [164], collective decision-making [16], evolu-\ntionary dynamics [165], sociophysics [166, 167], physics of\nactive matter [168], evolutionary computation [169] and ma-\nchine learning [170]. We conclude with a list of take-home\nmessages, targeting the three communities we believe will be\nat the center of this coming revolution:\n• Researchers in Swarm Robotics: simple robots are\nnot inherently ”simple”. What matters is the emergence\nof complex behaviors from microscopic interactions.\nWhether you work with large or small robots, dense\nor sparse populations, or few or many robots, all are\nwelcome under the broad aim of continual learning in\nswarm systems.\n• Researchers in Machine Learning: this is all about\nembodiment. Swarm robotics introduces a unique cat-\negory of machine learning problems with elements of\n”social” learning across physically embodied agents.\nAnchoring language models in physical systems brings\nnew challenges and capabilities in distributed, online\nlearning.\n• Researchers in Complex Systems: swarm robotics\nprovides a controllable model for exploring active mat-\nter, sociophysics models, reaction-diffusion, and dif-\nfusion biophysics processes. Swarm robotics offers\nan experimental platform for addressing fundamental\nquestions about adaptive collective systems.\nWe believe that decentralized learning and execution will\ninevitably become more prominent in swarm robotics, with\nsignaling playing a fundamental role. We intend for this\npaper to serve as a milestone in shaping the future of this\nfield by providing a framework to understand the complexities\nand potentials of swarm robotics, where local interactions\ndrive continuously learning embodied agents equipped with\ncomplex signaling mechanisms.\nAcknowledgment\nThis work was supported by the SSR project funded by the\nAgence Nationale pour la Recherche under Grant No ANR-\n24-CE33-7791.\nReferences\n[1] G Beni and J Wang. Swarm intelligence in cellular\nrobotic systems. In NATO ASI. 1993.\n[2] Gregory Dudek, Michael Jenkin, Evangelos Milios, and\nDavid Wilkes. A taxonomy for swarm robots. In Pro-\nceedings of 1993 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS’93), volume 1,\npages 441–447. IEEE, 1993.\n\nSignaling and Social Learning in Swarms of Robots — 11/17\n[3] Manuele Brambilla, Eliseo Ferrante, Mauro Birattari,\nand Marco Dorigo. Swarm robotics : A review from\nthe swarm engineering perspective. Swarm Intelligence,\n7(1):1–41, 2013.\n[4] Heiko Hamann. Swarm Robotics - A Formal Approach.\nSpringer, 2018.\n[5] M Dorigo, G Theraulaz, and V Trianni. Reflections on\nthe future of swarm robotics. Science Robotics, 2020.\n[6] D Floreano and H Lipson. From individual robots to\nrobot societies, 2021.\n[7] RA Watson, SG Ficici, and JB Pollack. Embodied evo-\nlution: Distributing an evolutionary algorithm in a pop-\nulation of robots. Robotics and Autonomous Systems,\n2002.\n[8] Jacqueline Heinerman, Massimiliano Rango, and Agos-\nton Endre Eiben. Evolution, individual learning, and\nsocial learning in a swarm of real robots. In 2015 IEEE\nsymposium series on computational intelligence, pages\n1055–1062. IEEE, 2015.\n[9] N Bredeche and N Fontbonne. Social learning in swarm\nrobotics. Philosophical Transactions of the Royal Soci-\nety B, 2022.\n[10] N Bredeche, E Haasdijk, and A Prieto. Embodied evo-\nlution in collective robotics: A review. Frontiers in\nRobotics and AI, 2018.\n[11] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang,\nand Tamer Basar. Fully decentralized multi-agent rein-\nforcement learning with networked agents. In Interna-\ntional Conference on Machine Learning, pages 5872–\n5881. PMLR, 2018.\n[12] Xueguang Lyu, Yuchen Xiao, Brett Daley, and Christo-\npher Amato. Contrasting centralized and decentralized\ncritics in multi-agent reinforcement learning.\narXiv\npreprint arXiv:2102.04402, 2021.\n[13] Sevan G Ficici, Richard A Watson, and Jordan B Pol-\nlack. Embodied evolution: A response to challenges\nin evolutionary robotics. In Proceedings of the eighth\nEuropean workshop on learning robots, pages 14–22.\nCiteseer, 1999.\n[14] Nicolas\nBredeche\nand\nJean-Marc\nMontanier.\nEnvironment-driven embodied evolution in a population\nof autonomous agents. In International Conference on\nParallel Problem Solving from Nature, pages 290–299.\nSpringer, 2010.\n[15] Melanie Schranz, Martina Umlauft, Micha Sende, and\nWilfried Elmenreich. Swarm robotic behaviors and cur-\nrent applications. Frontiers in Robotics and AI, 7:36,\n2020.\n[16] Noam Nisan, Tim Roughgarden, ´Eva Tardos, and Vi-\njay V. Vazirani. Algorithmic Game Theory. Cambridge\nUniversity Press, New York, NY, USA, 2007.\n[17] Afshin Oroojlooy and Davood Hajinezhad. A review\nof cooperative multi-agent deep reinforcement learning.\nApplied Intelligence, 53(11):13677–13722, 2023.\n[18] David H Wolpert and Kagan Tumer. An introduction to\ncollective intelligence. arXiv preprint cs/9908014, 1999.\n[19] Peter Stone, Gal Kaminka, Sarit Kraus, and Jeffrey\nRosenschein. Ad hoc autonomous agent teams: Collab-\noration without pre-coordination. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 24,\npages 1504–1509, 2010.\n[20] Jason R Marden and Jeff S Shamma.\nGame theory\nand control. Annual review of control, robotics, and\nautonomous systems, 1(1):105–134, 2018.\n[21] P Ecoffet, N Bredeche, and JB Andr´e. Nothing better to\ndo? environment quality and the evolution of coopera-\ntion by partner choice. Journal of Theoretical Biology,\n2021.\n[22] Lloyd S Shapley. A Value for n-person Games. Annals\nof Mathematical Studies, 28:307–317, 1953.\n[23] Yoav Shoham and Kevin Leyton-Brown. Multiagent\nsystems: Algorithmic, game-theoretic, and logical foun-\ndations. Cambridge University Press, 2008.\n[24] Michael Wooldridge.\nAn introduction to multiagent\nsystems. John wiley & sons, 2009.\n[25] Vito Trianni and Marco Dorigo. Self-organisation and\ncommunication in groups of simulated and physical\nrobots. Biological Cybernetics, 95(3):213–231, 2006.\n[26] Markus Waibel, Laurent Keller, and Dario Floreano.\nGenetic team composition and level of selection in the\nevolution of cooperation. IEEE transactions on Evolu-\ntionary Computation, 13(3):648–660, 2009.\n[27] D. Wolpert, Kagan Tumer, and K. Swanson. Optimal\nwonderful life utility functions in multi-agent systems.\n2000.\n[28] Patrick Kolpaczki, Viktor Bengs, Maximilian Muschalik,\nand Eyke H¨ullermeier. Approximating the shapley value\nwithout marginal contributions. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 38,\npages 13246–13255, 2024.\n[29] Jianrui Wang, Yitian Hong, Jiali Wang, Jiapeng Xu,\nYang Tang, Qing-Long Han, and J¨urgen Kurths. Co-\noperative and competitive multi-agent systems: From\noptimization to games. IEEE/CAA Journal of Automat-\nica Sinica, 9(5):763–783, 2022.\n[30] Sven Gronauer and Klaus Diepold. Multi-agent deep\nreinforcement learning: a survey. Artificial Intelligence\nReview, 55(2):895–943, 2022.\n[31] Richard Dawkins. The selfish gene. Oxford university\npress, 2016.\n[32] Stuart A West, Ashleigh S Griffin, and Andy Gard-\nner. Social semantics: altruism, cooperation, mutualism,\n\nSignaling and Social Learning in Swarms of Robots — 12/17\nstrong reciprocity and group selection. Journal of evolu-\ntionary biology, 20(2):415–432, 2007.\n[33] William D Hamilton. The genetical evolution of social\nbehaviour. ii. Journal of theoretical biology, 7(1):17–52,\n1964.\n[34] Peter J Richerson and Robert Boyd. Not by genes alone:\nHow culture transformed human evolution. University\nof Chicago press, 2008.\n[35] Eric Alden Smith. Communication and collective ac-\ntion: language and the evolution of human cooperation.\nEvolution and human behavior, 31(4):231–245, 2010.\n[36] Jean-Marc Montanier and Nicolas Bredeche. Surviving\nthe tragedy of commons: emergence of altruism in a\npopulation of evolving autonomous agents. In European\nconference on artificial life, 2011.\n[37] Markus Waibel, Dario Floreano, and Laurent Keller. A\nquantitative test of hamilton’s rule for the evolution of\naltruism. PLoS biology, 9(5):e1000615, 2011.\n[38] Kaiqing Zhang, Zhuoran Yang, and Tamer Bas¸ar. De-\ncentralized multi-agent reinforcement learning with net-\nworked agents: Recent advances. Frontiers of Informa-\ntion Technology & Electronic Engineering, 22(6):802–\n814, 2021.\n[39] Matthieu Zimmer, Claire Glanois, Umer Siddique, and\nPaul Weng. Learning fair policies in decentralized coop-\nerative multi-agent reinforcement learning. In Interna-\ntional Conference on Machine Learning, pages 12967–\n12978. PMLR, 2021.\n[40] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras,\nNantas Nardelli, and Shimon Whiteson. Counterfactual\nmulti-agent policy gradients. In Proceedings of the AAAI\nconference on artificial intelligence, volume 32, 2018.\n[41] Steffen Wischmann, Dario Floreano, and Laurent Keller.\nHistorical contingency affects signaling strategies and\ncompetitive abilities in evolving populations of simu-\nlated robots. Proceedings of the National Academy of\nSciences, 109(3):864–868, 2012.\n[42] Dario Floreano, Sara Mitri, St´ephane Magnenat, and\nLaurent Keller. Evolutionary conditions for the emer-\ngence of communication in robots. Current biology,\n17(6):514–519, 2007.\n[43] CW Reynolds. Flocks, herds and schools: A distributed\nbehavioral model. In SIGGRAPH, 1987.\n[44] T Vicsek, A Czir´ok, E Ben-Jacob, I Cohen, and\nO Shochet. Novel type of phase transition in a system\nof self-driven particles. Physical review letters, 1995.\n[45] John Maynard Smith and David Harper. Animal signals.\nOxford University Press, 2003.\n[46] Rianne van den Berghe, Josje Verhagen, Ora Oudgenoeg-\nPaz, Sanne van der Ven, and Paul Leseman. Social robots\nfor language learning: A review. Review of Educational\nResearch, 89(2):259–295, 2019.\n[47] Eric Bonabeau, Marco Dorigo, and Guy Theraulaz. In-\nspiration for optimization from social insect behaviour.\nNature, 406(6791):39–42, 2000.\n[48] C Detrain and JL Deneubourg.\nCollective decision-\nmaking and foraging patterns in ants and honeybees.\nAdvances in insect physiology, 2008.\n[49] Eric Bonabeau, Marco Dorigo, and Guy Theraulaz.\nSwarm intelligence: from natural to artificial systems.\nOxford university press, 1999.\n[50] Alexandre Campo, ´Alvaro Guti´errez, Shervin Nouyan,\nCarlo Pinciroli, Valentin Longchamp, Simon Garnier,\nand Marco Dorigo. Artificial pheromone for path selec-\ntion by a foraging swarm of robots. Biological cybernet-\nics, 103:339–352, 2010.\n[51] Marisa Carrasco. Visual attention: The past 25 years.\nVision research, 51(13):1484–1525, 2011.\n[52] Richard Bellman.\nDynamic programming.\nscience,\n153(3731):34–37, 1966.\n[53] L Cazenille, N Lobato-Dauzier, A Loi, M Ito, O Mar-\nchal, N Aubert-Kato, N Bredeche, and AJ Genot. Hear-\ning the shape of an arena with spectral swarm robotics.\narXiv:2403.17147, 2024.\n[54] Georgi Gerganov.\nllama.cpp.\nhttps://github.\ncom/ggerganov/llama.cpp, 2023.\nAccessed:\n2024-05-30.\n[55] Peng Ji, Jiachen Ye, Yu Mu, Wei Lin, Yang Tian, Chit-\ntaranjan Hens, Matjaˇz Perc, Yang Tang, Jie Sun, and\nJ¨urgen Kurths. Signal propagation in complex networks.\nPhysics reports, 1017:1–96, 2023.\n[56] Michiko E Taga and Bonnie L Bassler. Chemical com-\nmunication among bacteria. Proceedings of the National\nAcademy of Sciences, 100(suppl 2):14549–14554, 2003.\n[57] Kelly M LaRue, Jan Clemens, Gordon J Berman, and\nMala Murthy. Acoustic duetting in drosophila virilis\nrelies on the integration of auditory and tactile signals.\nElife, 4:e07277, 2015.\n[58] Kevin R Bairos-Novak, Maud CO Ferrari, and Douglas P\nChivers. A novel alarm signal in aquatic prey: familiar\nminnows coordinate group defences against predators\nthrough chemical disturbance cues. Journal of Animal\nEcology, 88(9):1281–1290, 2019.\n[59] Simone M Marques and Joaquim CG Esteves da Silva.\nFirefly bioluminescence: a mechanistic approach of lu-\nciferase catalyzed reactions. IUBMB life, 61(1):6–17,\n2009.\n[60] Carl D Hopkins. Electric communication in fish. Ameri-\ncan Scientist, 62(4):426–437, 1974.\n[61] Dag Eriksson and Lars Wallin. Male bird song attracts\nfemales—a field experiment. Behavioral Ecology and\nSociobiology, 19:297–299, 1986.\n\nSignaling and Social Learning in Swarms of Robots — 13/17\n[62] William A Searcy, Rindy C Anderson, and Stephen Now-\nicki. Bird song as a signal of aggressive intent. Behav-\nioral Ecology and Sociobiology, 60:234–241, 2006.\n[63] Tiago Rodrigues, Miguel Duarte, Margarida Figueir´o,\nVasco Costa, Sancho Moura Oliveira, and Anders Lyhne\nChristensen. Overcoming limited onboard sensing in\nswarm robotics through local communication. In Trans-\nactions on Computational Collective Intelligence XX,\npages 201–223. Springer, 2015.\n[64] Mohamed S Talamali, Arindam Saha, James AR Mar-\nshall, and Andreagiovanni Reina. When less is more:\nRobot swarms adapt better to changes with constrained\ncommunication.\nScience Robotics, 6(56):eabf1416,\n2021.\n[65] KN McGuire, Christophe De Wagter, Karl Tuyls,\nHJ Kappen, and Guido CHE de Croon. Minimal nav-\nigation solution for a swarm of tiny flying robots to\nexplore an unknown environment. Science Robotics,\n4(35):eaaw9710, 2019.\n[66] Imane Hafnaoui, Gabriela Nicolescu, and Giovanni Bel-\ntrame. Timing information propagation in interactive\nnetworks. Scientific Reports, 9(1):4442, 2019.\n[67] John Crank.\nThe mathematics of diffusion.\nOxford\nuniversity press, 1979.\n[68] Niall Shanks.\nModeling biological systems:\nthe\nbelousov–zhabotinsky reaction. Foundations of Chem-\nistry, 3(1):33–53, 2001.\n[69] AM Turing. The chemical basis of morphogenesis. Bul-\nletin of mathematical biology, 52(1-2):153–197, 1990.\n[70] Tamio Arai, Eiichi Yoshida, and Jun Ota. Information\ndiffusion by local communication of multiple mobile\nrobots. In Proceedings of IEEE Systems Man and Cy-\nbernetics Conference-SMC, volume 4, pages 535–540.\nIEEE, 1993.\n[71] Satoshi Murata. Molecular Robotics: An Introduction.\nSpringer, 2022.\n[72] Sami Nummelin, Boxuan Shen, Petteri Piskunen, Qing\nLiu, Mauri A Kostiainen, and Veikko Linko. Robotic\nDNA nanostructures. ACS Synthetic Biology, 9(8):1923–\n1940, 2020.\n[73] Shawn M Douglas, Ido Bachelet, and George M Church.\nA logic-gated nanorobot for targeted transport of molec-\nular payloads. Science, 335(6070):831–834, 2012.\n[74] Emanuela Torelli, Monica Marini, Sabrina Palmano,\nLuca Piantanida, Cesare Polano, Alice Scarpellini,\nMarco Lazzarino, and Giuseppe Firrao. A DNA origami\nnanorobot controlled by nucleic acid hybridization.\nSmall, 10(14):2918–2926, 2014.\n[75] Akinori Kuzuya and Yuichi Ohya. Nanomechanical\nmolecular devices made of DNA origami. Accounts of\nchemical research, 47(6):1742–1749, 2014.\n[76] Yaniv Amir, Almogit Abu-Horowitz, and Ido Bachelet.\nFolding and characterization of a bio-responsive robot\nfrom DNA origami. JoVE (Journal of Visualized Experi-\nments), (106):e51272, 2015.\n[77] Gal A Kaminka, Rachel Spokoini-Stern, Yaniv Amir,\nNoa Agmon, and Ido Bachelet. Molecular robots obey-\ning asimov’s three laws of robotics.\nArtificial life,\n23(3):343–350, 2017.\n[78] Jasleen Kaur Daljit Singh, Minh Tri Luu, Ali Abbas, and\nShelley FJ Wickham. Switchable DNA-origami nanos-\ntructures that respond to their environment and their\napplications. Biophysical reviews, 10(5):1283–1293,\n2018.\n[79] Suping Li, Qiao Jiang, Shaoli Liu, Yinlong Zhang, Yan-\nhua Tian, Chen Song, Jing Wang, Yiguo Zou, Gregory J\nAnderson, Jing-Yan Han, et al. A DNA nanorobot func-\ntions as a cancer therapeutic in response to a molecular\ntrigger in vivo. Nature biotechnology, 36(3):258–264,\n2018.\n[80] Shelley FJ Wickham, Jonathan Bath, Yousuke Katsuda,\nMasayuki Endo, Kumi Hidaka, Hiroshi Sugiyama, and\nAndrew J Turberfield. A DNA-based molecular motor\nthat can navigate a network of tracks. Nature nanotech-\nnology, 7(3):169–173, 2012.\n[81] Anupama J Thubagere, Wei Li, Robert F Johnson,\nZibo Chen, Shayan Doroudi, Yae Lim Lee, Gre-\ngory Izatt, Sarah Wittman, Niranjan Srinivas, Damien\nWoods, et al. A cargo-sorting DNA robot. Science,\n357(6356):eaan6558, 2017.\n[82] Guillaume Gines, AS Zadorin, J-C Galas, Teruo Fujii,\nA Estevez-Torres, and Y Rondelez. Microscopic agents\nprogrammed by DNA circuits. Nature nanotechnology,\n12(4):351–359, 2017.\n[83] Nathanael Aubert-Kato, Charles Fosseprez, Guillaume\nGines, Ibuki Kawamata, Huy Dinh, Leo Cazenille, An-\ndre Estevez-Tores, Masami Hagiya, Yannick Rondelez,\nand Nicolas Bredeche. Evolutionary optimization of\nself-assembly in a swarm of bio-micro-robots. In Pro-\nceedings of the Genetic and Evolutionary Computation\nConference, pages 59–66, 2017.\n[84] Leo Cazenille, Nicolas Bredeche, and Nathanael Aubert-\nKato. Exploring self-assembling behaviors in a swarm\nof bio-micro-robots using surrogate-assisted map-elites.\nIn 2019 IEEE Symposium Series on Computational In-\ntelligence (SSCI), pages 238–246. IEEE, 2019.\n[85] Alexander Ziepke, Ivan Maryshev, Igor S Aranson, and\nErwin Frey. Multi-scale organization in communicat-\ning active matter. Nature communications, 13(1):6727,\n2022.\n[86] Yibin Wang, Hui Chen, Leiming Xie, Jinbo Liu,\nLi Zhang, and Jiangfan Yu. Swarm autonomy: From\nagent functionalization to machine intelligence.\nAd-\nvanced Materials, page 2312956, 2024.\n\nSignaling and Social Learning in Swarms of Robots — 14/17\n[87] Jens Grauer, Fabian Jan Schwarzendahl, Hartmut L¨owen,\nand Benno Liebchen.\nOptimizing collective behav-\nior of communicating active particles with machine\nlearning. Machine Learning: Science and Technology,\n5(1):015014, 2024.\n[88] Franc¸ois A Lavergne, Hugo Wendehenne, Tobias\nB¨auerle, and Clemens Bechinger.\nGroup formation\nand cohesion of active particles with visual perception–\ndependent motility. Science, 364(6435):70–74, 2019.\n[89] Jakia Jannat Keya, Ryuhei Suzuki, Arif Md Rashedul\nKabir, Daisuke Inoue, Hiroyuki Asanuma, Kazuki Sada,\nHenry Hess, Akinori Kuzuya, and Akira Kakugo. DNA-\nassisted swarm control in a biomolecular motor system.\nNature communications, 9(1):453, 2018.\n[90] Mousumi Akter, JJ Keya, K Kayano, AMR Kabir,\nDaisuke Inoue, Henry Hess, K Sada, Akinori Kuzuya,\nHiroyuki Asanuma, and Akira Kakugo. Cooperative\ncargo transportation by a swarm of molecular machines.\nScience Robotics, 7(65):eabm0677, 2022.\n[91] Nathanael Aubert-Kato, Geoff Nitschke, Ibuki Kawa-\nmata, and Akira Kakugo. Collective cargo transport and\nsorting with molecular swarms. In Artificial Life Confer-\nence Proceedings 35, volume 2023, page 90. MIT Press\nOne Rogers Street, Cambridge, MA 02142-1209, USA\njournals-info ..., 2023.\n[92] I Slavkov, D Carrillo-Zapata, N Carranza, X Diego,\nF Jansson, J Kaandorp, S Hauert, and J Sharpe. Morpho-\ngenesis in robot swarms. Science Robotics, 2018.\n[93] M Rubenstein, A Cornejo, and R Nagpal. Programmable\nself-assembly in a thousand-robot swarm. Science, 2014.\n[94] M Gauci, ME Ortiz, M Rubenstein, and R Nagpal. Er-\nror cascades in collective behavior: a case study of the\ngradient algorithm on 1000 physical agents. In Proceed-\nings of the 16th Conference on Autonomous Agents and\nMultiAgent Systems, pages 1404–1412, 2017.\n[95] H Wang and M Rubenstein. A fast, accurate, and scal-\nable probabilistic sample-based approach for counting\nswarm size. In 2020 IEEE International Conference\non Robotics and Automation (ICRA), pages 7180–7185.\nIEEE, 2020.\n[96] Lucian Busoniu, Robert Babuska, and Bart De Schut-\nter. A comprehensive survey of multiagent reinforce-\nment learning. IEEE Transactions on Systems, Man,\nand Cybernetics, Part C (Applications and Reviews),\n38(2):156–172, 2008.\n[97] Yousef Saad. Numerical methods for large eigenvalue\nproblems: revised edition. SIAM, 2011.\n[98] Alan V Oppenheim. Discrete-time signal processing.\nPearson Education India, 1999.\n[99] Xenia Ohmer, Michael Marino, Michael Franke, and\nPeter K¨onig. Mutual influence between language and\nperception in multi-agent communication games. PLOS\nComputational Biology, 18(10):1–28, 10 2022.\n[100] Naftali Tishby, Fernando C. Pereira, and William Bialek.\nThe information bottleneck method. In Proceedings of\nthe 37-th Annual Allerton Conference on Communica-\ntion, Control and Computing, pages 368–377, 1999.\n[101] Simon Kirby, Monica Tamariz, Hannah Cornish, and\nKenny Smith. Compression and communication in the\ncultural evolution of linguistic structure.\nCognition,\n141:87–102, August 2015.\n[102] Noga Zaslavsky, Charles Kemp, Terry Regier, and Naf-\ntali Tishby. Efficient compression in color naming and\nits evolution. Proceedings of the National Academy of\nSciences, 115(31):7937–7942, July 2018.\n[103] Luc Steels. The Talking Heads Experiment. Volume I.\nWords and Meanings. 1999.\n[104] S. Kirby. Spontaneous evolution of linguistic structure-\nan iterated learning model of the emergence of regularity\nand irregularity. IEEE Transactions on Evolutionary\nComputation, 5(2):102–110, April 2001.\n[105] Kenny Smith, Simon Kirby, and Henry Brighton. Iter-\nated learning: A framework for the emergence of lan-\nguage. Artificial Life, 9(4):371–386, October 2003.\n[106] Paul Vogt. The emergence of compositional structures\nin perceptually grounded language games. Artificial\nIntelligence, 167(1–2):206–242, September 2005.\n[107] Andrew Perfors and Daniel J. Navarro. Language evolu-\ntion can be shaped by the structure of the world. Cogni-\ntive Science, 38(4):775–793, January 2014.\n[108] Morten H. Christiansen and Nick Chater. Language as\nshaped by the brain. Behavioral and Brain Sciences,\n31(5):489–509, October 2008.\n[109] Peter Wellens, Martin Loetzsch, and Luc Steels. Flexible\nword meaning in embodied agents. Connection Science,\n20(2–3):173–191, September 2008.\n[110] Katrien Beuls and Luc Steels. Agent-based models of\nstrategies for the emergence and evolution of grammati-\ncal agreement. PLoS ONE, 8(3):e58960, March 2013.\n[111] J´erˆome Botoko Ekila. Emergence of linguistic conven-\ntions in multi-agent systems through situated commu-\nnicative interactions. In Proceedings of the 23rd Inter-\nnational Conference on Autonomous Agents and Multia-\ngent Systems, AAMAS ’24, page 2725–2727, Richland,\nSC, 2024. International Foundation for Autonomous\nAgents and Multiagent Systems.\n[112] Kyle Wagner, James A. Reggia, Juan Uriagereka, and\nGerald S. Wilkinson. Progress in the simulation of emer-\ngent communication and language. Adaptive Behavior,\n11(1):37–69, 2003.\n[113] Changxi Zhu, Mehdi Dastani, and Shihan Wang. A\nsurvey of multi-agent deep reinforcement learning with\n\nSignaling and Social Learning in Swarms of Robots — 15/17\ncommunication. Autonomous Agents and Multi-Agent\nSystems, 38(1), January 2024.\n[114] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus.\nLearning multiagent communication with backpropaga-\ntion. In Advances in Neural Information Processing\nSystems, pp. 2244–2252, 2016.\n[115] Jakob N. Foerster, Yannis M. Assael, Nando de Freitas,\nand Shimon Whiteson. Learning to communicate with\ndeep multi-agent reinforcement learning. In Proceedings\nof the 30th International Conference on Neural Infor-\nmation Processing Systems, NIPS’16, page 2145–2153,\nRed Hook, NY, USA, 2016. Curran Associates Inc.\n[116] Peng Peng, Ying Wen, Yaodong Yang, Quan Yuan,\nZhenkun Tang, Haitao Long, and Jun Wang. Multi-\nagent bidirectionally-coordinated nets: Emergence of\nhuman-level coordination in learning to play starcraft\ncombat games. 2017.\n[117] Yutong Wang and Guillaume Sartoretti. Fcmnet: Full\ncommunication memory net for team-level cooperation\nin multi-agent systems. In Proceedings of the 21st Inter-\nnational Conference on Autonomous Agents and Multia-\ngent Systems, AAMAS ’22, page 1355–1363, Richland,\nSC, 2022. International Foundation for Autonomous\nAgents and Multiagent Systems.\n[118] Yedid Hoshen. Vain: Attentional multi-agent predic-\ntive modeling. In Proceedings of the 31st International\nConference on Neural Information Processing Systems,\nNIPS’17, page 2698–2708, Red Hook, NY, USA, 2017.\nCurran Associates Inc.\n[119] Jiechuan Jiang and Zongqing Lu. Learning attentional\ncommunication for multi-agent cooperation. In S. Ben-\ngio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-\nBianchi, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems, volume 31. Curran As-\nsociates, Inc., 2018.\n[120] Abhishek Das, Th´eophile Gervet, Joshua Romoff, Dhruv\nBatra, Devi Parikh, Mike Rabbat, and Joelle Pineau.\nTarMAC: Targeted multi-agent communication. In Ka-\nmalika Chaudhuri and Ruslan Salakhutdinov, editors,\nProceedings of the 36th International Conference on Ma-\nchine Learning, volume 97 of Proceedings of Machine\nLearning Research, pages 1538–1546. PMLR, 09–15\nJun 2019.\n[121] Amanpreet\nSingh,\nTushar\nJain,\nand\nSainbayar\nSukhbaatar. Learning when to communicate at scale\nin multiagent cooperative and competitive tasks. In In-\nternational Conference on Learning Representations,\n2019.\n[122] Sai Qian Zhang, Qi Zhang, and Jieyu Lin. Efficient\ncommunication in multi-agent reinforcement learning\nvia variance based control. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett,\neditors, Advances in Neural Information Processing Sys-\ntems, volume 32. Curran Associates, Inc., 2019.\n[123] Rundong Wang, Xu He, Runsheng Yu, Wei Qiu, Bo An,\nand Zinovi Rabinovich. Learning efficient multi-agent\ncommunication: An information bottleneck approach.\nIn Hal Daum´e III and Aarti Singh, editors, Proceedings\nof the 37th International Conference on Machine Learn-\ning, volume 119 of Proceedings of Machine Learning\nResearch, pages 9908–9918. PMLR, 13–18 Jul 2020.\n[124] Shuai Han, Mehdi Dastani, and Shihan Wang. Model-\nbased sparse communication in multi-agent reinforce-\nment learning. In Proceedings of the 2023 International\nConference on Autonomous Agents and Multiagent Sys-\ntems, AAMAS ’23, page 439–447, Richland, SC, 2023.\nInternational Foundation for Autonomous Agents and\nMultiagent Systems.\n[125] Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z\nLeibo, Karl Tuyls, and Stephen Clark. Emergent com-\nmunication through negotiation. In International Con-\nference on Learning Representations, 2018.\n[126] Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls,\nand Stephen Clark. Emergence of linguistic communi-\ncation from referential games with symbolic and pixel\ninput. In International Conference on Learning Repre-\nsentations, 2018.\n[127] Natasha Jaques, Angeliki Lazaridou, Edward Hughes,\nCaglar Gulcehre, Pedro A. Ortega, DJ Strouse, Joel Z.\nLeibo, and Nando de Freitas. Social influence as intrinsic\nmotivation for multi-agent deep reinforcement learning.\nIn Proceedings of the 36th International Conference on\nMachine Learning, pages 3040–3049, 2019.\n[128] Daewoo Kim, Sangwoo Moon, David Hostallero,\nWan Ju Kang, Taeyoung Lee, Kyunghwan Son, and Yung\nYi. Learning to schedule communication in multi-agent\nreinforcement learning. In International Conference on\nLearning Representations, 2019.\n[129] Mathieu Rita, Corentin Tallec, Paul Michel, Jean-\nBastien Grill, Olivier Pietquin, Emmanuel Dupoux, and\nFlorian Strub. Emergent communication: Generalization\nand overfitting in lewis games. In S. Koyejo, S. Mo-\nhamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\neditors, Advances in Neural Information Processing Sys-\ntems, volume 35, pages 1389–1404. Curran Associates,\nInc., 2022.\n[130] Igor Mordatch and Pieter Abbeel.\nEmergence of\ngrounded compositional language in multi-agent popula-\ntions. In Sheila A. McIlraith and Kilian Q. Weinberger,\neditors, Proceedings of the Thirty-Second AAAI Confer-\nence on Artificial Intelligence, pages 1495–1502. AAAI\nPress, 2018.\n[131] Mathieu Rita, Florian Strub, Jean-Bastien Grill, Olivier\nPietquin, and Emmanuel Dupoux. On the role of pop-\nulation heterogeneity in emergent communication. In\n\nSignaling and Social Learning in Swarms of Robots — 16/17\nInternational Conference on Learning Representations,\n2022.\n[132] Rahma Chaabouni, Eugene Kharitonov, Emmanuel\nDupoux, and Marco Baroni. Anti-efficient encoding in\nemergent communication. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett,\neditors, Advances in Neural Information Processing Sys-\ntems, volume 32. Curran Associates, Inc., 2019.\n[133] Ryan Lowe, Jakob Foerster, Y-Lan Boureau, Joelle\nPineau, and Yann Dauphin.\nOn the pitfalls of mea-\nsuring emergent communication. In Proceedings of the\n18th International Conference on Autonomous Agents\nand MultiAgent Systems, AAMAS ’19, page 693–701,\nRichland, SC, 2019. International Foundation for Au-\ntonomous Agents and Multiagent Systems.\n[134] Angeliki Lazaridou and Marco Baroni. Emergent multi-\nagent communication in the deep learning era. June\n2020.\n[135] Diane Bouchacourt and Marco Baroni. How agents see\nthings: On visual representations in an emergent lan-\nguage game. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing,\npages 981–985. Association for Computational Linguis-\ntics, 2018.\n[136] Stevan Harnad. The symbol grounding problem. Physica\nD: Nonlinear Phenomena, 42(1):335–346, 1990.\n[137] Paul Vogt. The physical symbol grounding problem.\nCognitive Systems Research, 3(3):429–457, September\n2002.\n[138] Jason Lee, Kyunghyun Cho, Jason Weston, and Douwe\nKiela. Emergent translation in multi-agent communica-\ntion. In International Conference on Learning Represen-\ntations, 2018.\n[139] Toru Lin, Minyoung Huh, Chris Stauffer, Ser-Nam Lim,\nand Phillip Isola. Learning to ground multi-agent com-\nmunication with autoencoders. In Advances in Neural\nInformation Processing Systems, 2021.\n[140] Abhishek Das, Satwik Kottur, Jose M. F. Moura, Stefan\nLee, and Dhruv Batra. Learning cooperative visual dia-\nlog agents with deep reinforcement learning. In Proceed-\nings of the IEEE International Conference on Computer\nVision (ICCV), 2017.\n[141] Serhii Havrylov and Ivan Titov. Emergence of language\nwith multi-agent games: Learning to communicate with\nsequences of symbols. In Proceedings of the 31st Inter-\nnational Conference on Neural Information Processing\nSystems, NIPS’17, page 2146–2156, Red Hook, NY,\nUSA, 2017. Curran Associates Inc.\n[142] Abhinav Gupta, Marc Lanctot, and Angeliki Lazari-\ndou.\nDynamic population-based meta-learning for\nmulti-agent communication with natural language. In\nA. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman\nVaughan, editors, Advances in Neural Information Pro-\ncessing Systems, 2021.\n[143] Mycal Tucker, Huao Li, Siddharth Agrawal, Dana\nHughes, Katia P. Sycara, Michael Lewis, and Julie Shah.\nEmergent discrete communication in semantic spaces.\nIn A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wort-\nman Vaughan, editors, Advances in Neural Information\nProcessing Systems, 2021.\n[144] Jason Lee, Kyunghyun Cho, and Douwe Kiela. Counter-\ning language drift via visual grounding. In Proceedings\nof the 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-\nIJCNLP), pages 4385–4395, Hong Kong, China, 2019.\nAssociation for Computational Linguistics.\n[145] Angeliki Lazaridou, Anna Potapenko, and Olivier Tiele-\nman. Multi-agent communication meets natural lan-\nguage: Synergies between functional and structural lan-\nguage learning. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7663–7674, Online, 2020. Association for Com-\nputational Linguistics.\n[146] Ryan Lowe, Abhinav Gupta, Jakob Foerster, Douwe\nKiela, and Joelle Pineau. On the interaction between su-\npervision and self-play in emergent communication. In\nInternational Conference on Learning Representations,\n2020.\n[147] Angeliki Lazaridou, Alexander Peysakhovich, and\nMarco Baroni. Multi-agent cooperation and the emer-\ngence of (natural) language. In International Conference\non Learning Representations, 2017.\n[148] Seth Karten, Mycal Tucker, Siva Kailas, and Katia\nSycara. Towards true lossless sparse communication\nin multi-agent systems. In ICRA 2023, 2023.\n[149] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu,\nWenlong Huang, Yevgen Chebotar, Pierre Sermanet,\nDaniel Duckworth, Sergey Levine, Vincent Vanhoucke,\nKarol Hausman, Marc Toussaint, Klaus Greff, Andy\nZeng, Igor Mordatch, and Pete Florence. Palm-e: An\nembodied multimodal language model. In PMLR, ed-\nitor, Proceedings of the 40th International Conference\non Machine Learning, volume 202, 2023.\n[150] Thomas Carta, Cl´ement Romac, Thomas Wolf, Syl-\nvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer.\nGrounding Large Language Models in Interactive En-\nvironments with Online Reinforcement Learning. In\nProceedings of Machine Learning Research, volume\n202. PMLR, 2023.\n[151] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,\nShane Legg, and Dario Amodei. Deep reinforcement\nlearning from human preferences. In I. Guyon, U. Von\n\nSignaling and Social Learning in Swarms of Robots — 17/17\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems, volume 30. Curran As-\nsociates, Inc., 2017.\n[152] Feiyu Zhu and Reid Simmons. Bootstrapping cognitive\nagents with a large language model. Proceedings of the\nAAAI Conference on Artificial Intelligence, 38(1):655–\n663, 2024.\n[153] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong\nZhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu,\nand Chuang Gan. Building cooperative embodied agents\nmodularly with large language models. In International\nConference on Learning Representations, 2024.\n[154] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,\nBeibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,\nShaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah,\nRyen W White, Doug Burger, and Chi Wang. Auto-\ngen: Enabling next-gen llm applications via multi-agent\nconversation. 2023.\n[155] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii\nKhizbullin, and Bernard Ghanem.\nCamel: Commu-\nnicative agents for ”mind” exploration of large language\nmodel society. In A. Oh, T. Neumann, A. Globerson,\nK. Saenko, M. Hardt, and S. Levine, editors, Advances\nin Neural Information Processing Systems, volume 36,\npages 51991–52008. Curran Associates, Inc., 2023.\n[156] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S. Bern-\nstein. Generative agents: Interactive simulacra of hu-\nman behavior. In Proceedings of the 36th Annual ACM\nSymposium on User Interface Software and Technology,\nUIST ’23, New York, NY, USA, 2023. Association for\nComputing Machinery.\n[157] Alexander Sasha Vezhnevets, John P. Agapiou, Avia\nAharon, Ron Ziv, Jayd Matyas, Edgar A. Du´e˜nez-\nGuzm´an, William A. Cunningham, Simon Osindero,\nDanny Karmon, and Joel Z. Leibo. Generative agent-\nbased modeling with actions grounded in physical, so-\ncial, or digital space using concordia. 2023.\n[158] J´er´emy Perez, Corentin L´eger, Marcela Ovando-Tellez,\nChris Foulon, Joan Dussauld, Pierre-Yves Oudeyer, and\nCl´ement Moulin-Frier. Cultural evolution in populations\nof large language models. 2024.\n[159] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and\nDenny Zhou. Chain-of-thought prompting elicits reason-\ning in large language models. In S. Koyejo, S. Mohamed,\nA. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,\nAdvances in Neural Information Processing Systems, vol-\nume 35, pages 24824–24837. Curran Associates, Inc.,\n2022.\n[160] Jijia Liu, Chao Yu, Jiaxuan Gao, Yuqing Xie, Qingmin\nLiao, Yi Wu, and Yu Wang. Llm-powered hierarchical\nlanguage agent for real-time human-ai coordination. In\nInternational Conference on Autonomous Agents and\nMultiagent Systems, 2024.\n[161] William Hunt, Toby Godfrey, and Mohammad D.\nSoorati. Conversational language models for human-\nin-the-loop multi-robot coordination. In Demonstration\nat International Conference on Autonomous Agents and\nMulti-Agent Systems, 2024.\n[162] Alberto Acerbi and Joseph M. Stubbersfield. Large lan-\nguage models show human-like content biases in trans-\nmission chain experiments. Proceedings of the National\nAcademy of Sciences, 120(44), 2023.\n[163] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto,\nand Pascale Fung. Survey of hallucination in natural lan-\nguage generation. ACM Computing Surveys, 55(12):1–\n38, 2023.\n[164] Drew Fudenberg and David Levine. Learning in games.\nEuropean economic review, 42(3-5):631–639, 1998.\n[165] Martin A Nowak. Evolutionary dynamics: exploring the\nequations of life. Harvard university press, 2006.\n[166] Parongama Sen and Bikas K Chakrabarti. Sociophysics:\nan introduction. OUP Oxford, 2014.\n[167] Serge Galam, Yuval Gefen, and Yonathan Shapir. Socio-\nphysics: A new approach of sociological collective be-\nhaviour. i. mean-behaviour description of a strike. Jour-\nnal of Mathematical Sociology, 9(1):1–13, 1982.\n[168] Julien Tailleur, Gerhard Gompper, M Cristina Marchetti,\nJulia M Yeomans, and Christophe Salomon.\nActive\nMatter and Nonequilibrium Statistical Physics: Lecture\nNotes of the Les Houches Summer School: Volume 112,\nSeptember 2018, volume 112. Oxford University Press,\n2022.\n[169] Agoston E Eiben and James E Smith. Introduction to\nevolutionary computing. Springer, 2015.\n[170] Richard S Sutton and Andrew G. Barto. Reinforcement\nlearning: An introduction. A Bradford Book, 2018.",
    "pdf_filename": "Signaling_and_Social_Learning_in_Swarms_of_Robots.pdf"
}