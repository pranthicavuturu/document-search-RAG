{
    "title": "A Layered Architecture for Developing and Enhancing",
    "abstract": "SignificanteffortshasbeenmadetoexpandtheuseofLargeLanguageModels(LLMs)beyond basiclanguagetasks. WhilethegeneralizabilityandversatilityofLLMshaveenabledwidespread adoption, evolving demands in application development often exceed their native capabilities. Meetingthesedemandsmayinvolveadiversesetofmethods,suchasenhancingcreativitythrough eitherinferencetemperatureadjustmentsorcreativity-provokingprompts. Selectingtherightap- proach is critical, as different methods lead to trade-offs in engineering complexity, scalability, andoperationalcosts. ThispaperintroducesalayeredarchitecturethatorganizesLLMsoftware system development into distinct layers, each characterized by specific attributes. By aligning capabilities with these layers, the framework encourages the systematic implementation of capa- bilities in effective and efficient ways that ultimately supports desired functionalities and quali- ties. Through practical case studies, we illustrate the utility of the framework. This work offers developers actionable insights for selecting suitable technologies in LLM-based software system development, promoting robustness and scalability.",
    "body": "A Layered Architecture for Developing and Enhancing\nCapabilities in Large Language Model-based Software Systems\nDawen Zhang1∗, Xiwei Xu1, Chen Wang1, Zhenchang Xing1, and Robert Mao2\n1CSIRO’s Data61, 2ArcBlock\n∗David.Zhang@data61.csiro.au\nAbstract\nSignificanteffortshasbeenmadetoexpandtheuseofLargeLanguageModels(LLMs)beyond\nbasiclanguagetasks. WhilethegeneralizabilityandversatilityofLLMshaveenabledwidespread\nadoption, evolving demands in application development often exceed their native capabilities.\nMeetingthesedemandsmayinvolveadiversesetofmethods,suchasenhancingcreativitythrough\neitherinferencetemperatureadjustmentsorcreativity-provokingprompts. Selectingtherightap-\nproach is critical, as different methods lead to trade-offs in engineering complexity, scalability,\nandoperationalcosts. ThispaperintroducesalayeredarchitecturethatorganizesLLMsoftware\nsystem development into distinct layers, each characterized by specific attributes. By aligning\ncapabilities with these layers, the framework encourages the systematic implementation of capa-\nbilities in effective and efficient ways that ultimately supports desired functionalities and quali-\nties. Through practical case studies, we illustrate the utility of the framework. This work offers\ndevelopers actionable insights for selecting suitable technologies in LLM-based software system\ndevelopment, promoting robustness and scalability.\nKeywords: Artificial intelligence, large language model, software architecture, agent\n1 Introduction\nThe use of Large Language Models (LLMs) has expanded beyond traditional language-related tasks\nsuch as translation and question answering. This widespread adoption is largely driven by the gen-\neralizability and versatility of LLMs, which stem from being trained on vast amounts of diverse data\nsourced from the internet and human annotations, allowing them to capture patterns across many\ndomains. Furthermore, LLMs use text as a flexible input/output interface, which makes interact-\ning with them intuitive and adaptable to various contexts. Combined with advances in techniques\nthat improve their ability to follow instructions and align with specific needs, LLMs have been in-\ncreasingly applied to a variety of domain applications requiring flexibility and scalability, including\nsoftware development [1–3], process automation [4,5], financial analysis [6,7], manufacturing [8,9],\neducation [10,11], and scientific research [12–14].\nHowever,despitetheirremarkablestrengths,LLMshaveclearlimitations. Forinstance,evenwhen\ntrained on vast datasets, they often struggle with domain-specific knowledge and lack the specialized\nexpertiseneededforcertaintasks[15]. Additionally,LLMsarepronetogeneratingplausible-sounding\nbut factually incorrect outputs [16], commonly referred to as “hallucination.” Since LLMs are pri-\nmarily trained on the NLP task of next word prediction based on statistical probabilities [17], they\nmay be less reliable and efficient than simpler but specialized tools for tasks that require a deeper\nunderstanding or internal task representation [18]. Furthermore, due to their language processing\nnature, they also interact with the external world exclusively through the interface of text, limiting\neffective access to the diverse interfaces of external tools or systems [19].\n1\n4202\nvoN\n91\n]ES.sc[\n1v75321.1142:viXra\nAs LLMs are applied to more complex and varied tasks, new requirements continue to emerge.\nThese novel demands often go beyond the initial capabilities of the models, poses significant chal-\nlenges during the development of LLM-based applications. One of the most critical challenges lies\nin identifying the appropriate methods for implementing specific capabilities within LLMs [20,21].\nCapabilities such as incorporating domain knowledge, enforcing specific constraints, or adhering to\nparticularstylesarenotoff-the-shelffeaturessupportedbyLLMs,butcanbeachievedthroughvarious\napproaches. Incorporatingdomainknowledgeisaparticularlytypicalexample,whichcanbeachieved\nthroughtechniquessuchasLLMfine-tuningorRetrievalAugmentedGeneration(RAG).Eachmethod\ncomeswithitsowntrade-offsandcharacteristics, requiringcarefuldesignconsideration. Importantly,\nchoosing the wrong method for implementing a capability can lead to oversophisticated, inefficient,\nor ineffective solutions, and it may result in systems that are unreliable, fail to meet requirements, or\nunderperform in production. Thus, selecting the right approaches for developing these capabilities is\nnot merely a matter of preference but an architectural decision that can have a profound impact on\nthe success of the application.\nIn this paper, we propose a layered approach for implementing capabilities in LLM-based applica-\ntions by mapping them to the layers and components with corresponding attributes, shown in Fig 1.\nByaligningeachcapabilitywiththerelevantcomponentsatthecorrectlayers,thisapproachreinforces\nessential software engineering principles, and ensures that capabilities are effectively implemented.\nLayer Component Attribute Capability\nDependency Way to Implement\nR Inu cn oti rm pe o rS at ta iote ns Exception Handling So Wp oh ri ks ft li oc wat sed Domain\nKnowledge\nEnP gr ino em ep rit ng EM ne gc ih na en ei rs inm g Tooling Orchestration Pro Mc ae ns as g a en md e S nt tate In Ete nr va ic roti no mn ew ni tth IntA enmP dpr elo difb i Rca ab et si il o pit n oy no sf es\nStructured\nApplication Layer Output\nExternal Information\nWatermarking\nInference Layer Efficiency GM ea nc er ro a- tl ie ov ne l C T oo nk te ron l GM ei nc ero ra-l te iov ne l C T oo nk te rn ol\nHallucination\nAvoidance\nData Model Architecture Training Scale & Efficiency Obje &c t Aiv le igs n, mBe eh na tvior BK on uo nw dle ad rig ee s Creativity\nModel Layer\nLayers Attributes Capabilities\nFigure 1: the conceptual architecture of the approach, including Components, Layers, Attributes,\nCapabilities, and their mappings\nWe illustrate the utility of this approach with several real-world example cases, showing how\nthe right alignment between capabilities and implementation methods can lead to the appropriate\nsolutions. Through these examples, we demonstrate how developers can use this approach to make\ninformed design decisions.\nThis paper offers actionable insights into selecting the appropriate approaches for implementing\ncapabilities,empoweringdeveloperstobuildeffectiveandrobustsolutionsforLLM-basedapplications.\nThe remainder of this paper is organized as follows. Section 2 explains the motivating scenario\nof this study, while Section 3 presents the layered approach, its underlying principles, and details of\n2\neach layer. Section 4 demonstrates the approach with various example cases, followed by a review of\nthe existing literature related to this work in Section 5. Section 6 concludes the paper.\n2 Motivating Scenarios\nIn this section, we present two motivating scenarios in which LLM-based application developers are\nlikelytoencounterthechallengesaddressedbyourapproach. Whileestablishedsolutionsmayexistfor\nthesescenarios,weaimtohighlightthelackofastructureddesignprocessandconceptualframework.\nThese hurdles, though partially addressed in some cases, continue to create gaps in other emerging\nscenarios that require careful design considerations.\n2.1 Application Scenario: Integrating Additional Data\nA key scenario in developing LLM-based applications is deciding how to integrate additional data,\nsuch as domain-specific knowledge. In general, this can be achieved through two main approaches:\nintegratingtheinformationasparameterizedknowledgeorintegratingitasnon-parameterizedknowl-\nedge. Parameterized knowledge involves embedding the information directly into the model through\ntraining or fine-tuning, allowing the model to internalize the information. Non-parameterized knowl-\nedge supplies external information dynamically, typically through prompts, during inference, or post-\ngeneration, and is commonly referred to as Retrieval Augmented Generation (RAG) [15].\nWhen determining between parameterized knowledge and non-parameterized knowledge, while\ntherearevariousfactorsthatneedtobeconsidered,theprimarydifferencebetweenthesetwomethods\niswhethertheinformationisparameterized. Inthecaseofparameterizedknowledge,themodellearns\nthis information into its internal representations, making it readily available during inference. This\nbrings a number of characteristics, for instance:\n• Minimal latency - As the information is already learned into parameters of LLMs, using these\nknowledge does not trigger retrieval from separate sources, and does not need information to\nbe explicitly contained and processed as part of the input. This introduces minimal latency at\ninference time.\n• Internal representations of information - Information is learned into the model’s parameters,\nallowing it to form and store complex associations, relations and patterns within its internal\nrepresentation. This integration enables the model to utilize the knowledge cohesively, drawing\nconnections across tokens and adapting responses based on these embedded information.\n• Resource-intensive at training - Integrating knowledge through training or fine-tuning can be\nresource-intensive, requiring significant labour, computational and time investment upfront.\nIn contrast, non-parameterized knowledge allows for flexible information integration at the infer-\nence stage by embedding relevant external information as context within the input provided to the\nlarge language model. This approach brings its own set of characteristics, such as:\n• Flexibility - Integrating information through non-parameterized approaches allows knowledge\nto be continuously added or updated independently of specific models and to be seamlessly\ntransferred and utilized by any model without being tightly coupled to one.\n• Additional input overheads - To supply external information, non-parameterized methods pro-\nvide additional context to models within the input prompt, which consumes token space and\nmay introduce computational overheads at inference.\n• Surface utilization of information - Since the information is provided solely through prompts as\nexternal context and is not embedded within the model’s parameters, it is processed based on\n3\nthe model’s existing learned representations [22]. The model uses this information at a surface\nlevel, interpreting it in light of pre-existing associations.\nThese characteristics make each fitted to different sets of application scenarios of different nature.\nFor example, if the developers want to develop the multilingual capability of the system by supplying\ndata related to a new language to the model, parameterized method is the optimal choice, because\nlanguage understanding demands the model to learn the representations that capture the language’s\nsemantic features and complex linguistic structures. Relying solely on in-context learning cannot\nestablish the statistical relations and deep associations within the language, nor does it form robust\ninternal representations, making it unlikely to produce coherent and sensible outputs.\n2.2 Application Scenario: Constraining Output Structure\nAnothertypicalscenarioisconstrainingtheoutputstructureofthelanguagemodel. Therearealready\nrelatively mature solutions, such as structured output based on constrained decoding, widely offered\nby commercial LLM vendors and the open-source community. However, these solutions were not\navailable at the initial onset of demand, but instead, they went through an evolution process.\nAsLLMsgainedpopularity,usersbegantousethemnotonlyaschatbotsbutalsotoexploretheir\npotential as software components for executing tasks. These LLM-based components are connected\nwithothercomponentsprimarilythroughtheirtext-basedinterfaces. However,sincemajorityofnon-\nLLM-basedcomponentsmaynotbecapableofprocessingnaturallanguageinputs,itbecomesessential\nfor LLMs to generate structured outputs that can be parsed and processed by other components.\nHowever,LLMsacquiretheirgeneralistcapabilitiesduringpre-trainingbylearningtherepresentations\nof vast unstructured textual knowledge from diverse sources in a probabilistic manner, which makes\nthem natively incapable to produce stable structured outputs.\nIn order to connect LLMs with other components, developers attempted the challenge from var-\nious perspectives. Initially, the most intuitive method of trial and error was leveraged. Such at-\ntempt has led to sophisticated parsers in companion with carefully crafted prompts, but a struc-\ntured output from LLMs was still not guaranteed. For instance, in the widely used LLM applica-\ntion framework LangChain, several structured output parsers, including StructuredOutputParser,\nListOutputParser and CommaSeparatedListOutputParser, were introduced1, to parse the outputs\ngenerated by the LLMs. The LLMs were expected to generate outputs based on text prompts that\nspecify the desired output formats. A while after that, a new commit2 introduced additional trial-\nand-error-based output parsers, including RetryOutputParser, RetryWithErrorOutputParser and\nOutputFixingParser, to complement the structured output parsers. Since then, sophisticated retry,\nreflection, andfixingmechanismshavebeenintegratedintothecodebase, expandingitfrominitial80\nlines of code (LoC) to over a thousand LoC.\nFrom a language model training perspective, generating outputs in specific formats has long been\na traditional focus in the field. Fine-tuning has been an effective method for enforcing specific output\nformatsinLLMs[23]byadaptingthemodelswiththedatasetsthatexemplifythedesiredstructures.\nAs the demand for structured outputs increased, fine-tuning was offered by the commercial LLM\nvendors and the open source community explicitly as a means to reliable structured outputs, such\nas in OpenAI’s GPT-3.5 Turbo fine-tuning API release. 3. However, one significant challenge in this\nscenario is retaining the generalist capabilities of LLMs from the risk of catastrophic forgetting [24].\nFurthermore, maintaining the flexibility of not requiring exhaustive fine-tuning across all possible\noutput formats makes it more complex.\n1Github Commit langchain-ai/langchain@df6c33d: https://github.com/langchain-ai/langchain/commit/df6c3\n3d4b31826650f5a04a5f9c352508bb4dd9d\n2Github Commit langchain-ai/langchain@ce5d97b: https://github.com/langchain-ai/langchain/commit/ce5d9\n7bcb3e263f6aa69da6c334e35e20bf4db11\n3OpenAIGPT-3.5Turbofine-tuningandAPIupdates(accessed1November2024): https://openai.com/index/g\npt-3-5-turbo-fine-tuning-and-api-updates/\n4\nAnother crucial stage where the output format can be enforced is the inference time. From a text\ngenerationperspective,theLLMdecodingprocessselectsasequenceoftokensbasedontheprobability\ndistribution for each subsequent token. By interpreting the output format schema as context-free\ngrammars [25], the candidate token space can be constrained through logits manipulation, assigning\nthe lowest probabilities to tokens that would violate the specified format [26], a technique known as\nconstrained decoding. Since the probability of generated tokens is directly controlled by the formal\nrepresentationoftheexpectedoutputformat,thisapproachensuresthatonlyoutputsstrictlyaligned\nwiththespecifiedformataregenerated. Thismethodhasbeenintegratedintocommercialmodelssuch\nasClaude34 andGPT-4o5,andopen-sourcelibrarieslikeHuggingFaceTransformers6 andllama.cpp7.\nThe core implementation of this method used in the HuggingFace Transformers library takes around\n200 LoC8.\nAt the current stage of this evolution, LLM-based applications utilize a combination of three\ntechnologies, with inference-time constrained decoding positioned at the center for consistency and\nreliability. Trainingandfine-tuning,alongwithformat-instructiveprompts,assistthemodelinbetter\nunderstandingtheschemas, ensuringthattheoutputsarebothsyntacticallycorrectandsemantically\nmeaningful. Additionally, the retry and fixing parsers provides an extra layer of assurance that the\noutputs are of expected formats.\n2.3 Motivation and Scope\nFrom these two scenarios, we observe that implementing a specific capability within LLM-based ap-\nplications can lead to various technological paths, with the suitability of each technology dependent\non the specific requirements of the scenario. Identifying these technological paths may require sig-\nnificant engineering efforts, and incur time and resources cost on it. Moreover, these technological\npathsmayoperateatdifferentlayers. Forexample,intheconstrainingoutputstructurescenario,fine-\ntuning targets the knowledge learned into the model parameters, while constrained decoding focuses\nonthetokenselectionprocess,andprompt-basedapproachesaddressnon-parameterizedinputstothe\nmodel. Although these different approaches can be employed to implement the same capability, they\nmay occur at distinct decoupled layers of a LLM-based application, leading to significantly different\nexpectations, outcomes, and implications for developers. Consequently, it is essential for developers\nto possess a systematic understanding of the software architecture of LLM-based applications and to\nreinforce software engineering principles to make informed architectural decisions.\nTherefore, the motivation behind this work is to propose an approach which serves to map antici-\npated capabilities to a potential technology stack for implementation, even in the absence of specific\nexisting or known solutions.\n3 The Approach\n3.1 Glossary\nTo enhance clarity for the reader, we define two key terms used in this approach.\n4Claude3ModelFamilyAnnouncement(accessed1November2024): https://www.anthropic.com/news/claude-3\n-family\n5IntroducingStructuredOutputsintheAPI(accessed1November2024): https://openai.com/index/introduci\nng-structured-outputs-in-the-api/\n6TextGenerationInferenceGuidance(accessed1November2024): https://huggingface.co/docs/text-generat\nion-inference/en/conceptual/guidance\n7llama.cppGBNFGuide(accessed1November2024): https://github.com/ggerganov/llama.cpp/tree/master/g\nrammars\n8Github Commit dottxt-ai/outlines@18aaba1:https://github.com/dottxt-ai/outlines/blob/18aaba1/outlines\n/processors/structured.py\n5\nCapability “Capability” is a widely used term in LLM research, but there lacks the explicit and\nrigorous conception of the term [27]. In this study, we define it as “the ability to effectively perform\na category of tasks or exhibit a specific type of behavior,” which is consistent with current LLM\nstudies [5,27–30] as well as its philosophical and anthropological roots.\nAttribute “Attribute”inthisapproachisdefinedas“inherentlybeingresponsibleforcertainquality,\nfeature, or operation of the system,” refined from its dictionary definition of “a quality or feature\nregarded as a characteristic or inherent part of someone or something.”\n3.2 Principles of the Approach\n“It is the mapping of a system’s functionality onto software structures that determines the architec-\nture’s support for qualities.”\n— Software Architecture in Practice [31]\nWe extend this principle to the software architecture for LLM-based application by mapping\ncapabilities onto the appropriate layers and components with corresponding attributes.\nLayers and components are defined following the principle of separation of concerns. We present\nthe layers, components and their attributes with additional properties in Section 3.3. Using this ar-\nchitecture,weillustratetheprocessofhowtomapdesiredcapabilitiesontotheappropriatestructures\nin Section 3.4.\n3.3 Layers\nInspiredbylayeredsoftwarearchitecturessuchasn-tierarchitecture[32]andlayeredInternetprotocol\nstack[33],webreakanLLM-basedapplicationintolayersbasedonthenatureofrelevantdevelopment\nactivities, including Model Layer, Inference Layer, and Application Layer. For each layer in the\narchitecture,weexaminethenatureofthecomponentsandtheirrolesinsupportingspecificattributes.\nThe overall structure is shown in Fig 1.\n3.3.1 Model Layer\nThe Model Layer involves the foundational aspects of building an LLM, from the data collection to\nthe architecture design and training process. The output of this layer is the large language model,\nwhich is a machine learning model typically built upon transformer or a variant architecture trained\nusing data on natural language processing (NLP) tasks such as causal language modeling (next-token\nprediction). Through this process, the model learns a statistical representation of language\nfrom the data, and embeds in its parameters. We divide the Model Layer into three core\ncomponents, i.e. Data, Model Architecture, and Training.\nData Datausedfortrainingandfine-tuningLLMsgoesthroughaprocessofselection,collection,and\npreprocessing, serving as the source from which LLMs learn the representations. In state-of-the-art\n(SoTA) LLMs, training data is typically collected from diverse sources, including publicly available\ninternet content, inputs from human data workers, knowledge obtained from domain experts, user\ninteractions and feedback, and distilled data from the output of LLMs [34–36].\nThequalityandscopeofthetrainingdatadirectlydeterminewhatinformationisembeddedwithin\nthe internal representation of LLMs and thus subsequently set the boundaries of model’s knowledge\nand capabilities. Beyond learning factual knowledge from the training data, LLMs also capture the\npatterns, relationships, and biases present in the training data, all of which contribute to shaping the\nmodel’sbehaviorsandresponses. Moreover, theseboundaries, behaviors, andresponsesarenotfixed,\nas LLMs can be further adapted and refined through continual pre-training or fine-tuning, where\nadditional data is used to update the model’s internal representations.\n6\nModel Architecture The Model Architecture defines the structure and underlying mechanisms\nof LLMs. The vast majority of LLMs are built on the transformer architecture, which leverage a\nself-attention mechanism that allows the model to capture long-range dependencies and contextual\nrelationships between tokens in a sequence [37]. This makes transformers effective for a wide range of\nNLP tasks, from translation and summarization to text generation.\nThe capability of model is largely tied to the scale of model, often measured by the number\nof parameters. The scale of model is a critical factor that may lead to the emergence of model\nabilities [28]. Larger models have shown emergent abilities that are absent in smaller models, such\nas few-shot learning. These abilities are likely a result of the higher number of parameters leading to\nincreased capacity to capture complex patterns and relationships from the training data [22].\nHowever, this comes with trade-offs in terms of computational cost and resource requirements.\nLarger models demand significant computational resources at training and inference, including more\nperformant computing units and larger memory, impacting the latency and cost of the model. To\naddress these challenges, some LLMs utilize a Mixture of Experts (MoE) architecture [38,39], which\noffersabalancebetweenthescaleandefficiency. MoEmodelsconsistofmultipleexpertsub-networks,\nwith only a subset of experts activated for each input, reducing the computational overhead of the\ninference. Incontrast,smallermodels,thoughlesscapable,arefasterandmoreefficient,makingthem\nmore feasible to be used for real-time applications or deployed on edge devices.\nTraining Training is the process where model learns from the data and embeds what it learns into\nparameters. While the dominant approach remains next-token prediction, various training objectives\nand fine-tuning techniques have been developed to address specific needs and enhance performance\nacross different applications.\nNext-token prediction is a widely adopted training objective for LLMs, where the model learns\nto predict the next token in a sequence by estimating the probability distribution of tokens based on\nthe preceding context. This task, also known as causal language modeling, aligns well with autore-\ngressive models including GPT [40], allowing model to generate fluent and coherent text. Another\ntext generation task is masked language modeling, adopted by BERT [41], trains model to fill in the\nrandom blanks within a sequence of tokens. This bidirectional context, where the model makes use\nof both preceding and following tokens, enables its ability to understand the full sentence structure\nand semantic relationships but at the same time also makes it less fit for sequential text generation.\nIf certain information or patterns are absent in the pre-training dataset, the model can be fine-\ntunedontask-specificdatatorefineitsknowledgebaseandbehavior. AsthedemandforusingLLMs\nfor specific tasks has increased [42], methods like instruction tuning, direct preference optimization\n(DPO), and reinforcement learning from human feedback (RLHF) have become popular to align\nmodelswithhumanpreferencesandimprovetheirabilitytofollowinstructions[43–45]. Thesemodels\nare fine-tuned on crafted dataset or reward functions so that the patterns and human preferences\nare captured into parameters of models. These fune-tuning methods can also be used for specialized\npurposes, such as setting rules for LLMs [46] or enabling models to autonomously call tools [47]. The\nobjective of next-token prediction can be further adapted by combining with techniques like zero-\nshot learning, chain-of-thought (CoT) [48], Monte-Carlo tree search (MCTS) [49] and Reinforcement\nLearning. For instance, Quiet-STaR [50] enhances reasoning by guiding models to “think before\nspeaking”usingintermediaterationaletokensbeforepredictingnexttokentoimprovecapabilitiesfor\nreasoning tasks.\nOpen-sourceLLMsaswellascommercialLLMsprovidetheinterfacesforfine-tuning. Forexample,\nboth OpenAI9 and Anthropic10 provide fine-tuning for their most advanced LLMs to application\ndevelopers. HuggingFace’s transformer library also supports both full fine-tuning and parameter\nefficient fine-tuning (PEFT) [51] over all available models. This provide application developers the\naccess to the model layer to customize the model based on their requirements.\n9Fine-tuningnowavailableforGPT-4o: https://openai.com/index/gpt-4o-fine-tuning/\n10Fine-tuneClaude3HaikuinAmazonBedrock: https://www.anthropic.com/news/fine-tune-claude-3-haiku\n7\nSummary The Model Layer is responsible for shaping the internal representations of LLMs. The\nData component defines the boundaries of the model’s knowledge and capabilities, response patterns,\nand relationships from the training data into the model’s parameters. The Model Architecture com-\nponent determines the structural design, which influence both the scale and efficiency of the model\nand subsequently the emergence of abilities. Finally, the Training component dictates model’s text\ngeneration mechanisms, refining model behavior and incorporating additional knowledge, alignment\nand capabilities through training objectives and fine-tuning techniques. These attributes with their\ncorresponding components and developer access levels are shown in Table 1.\nDev Access (Com- Dev Access (Open\nAttribute Component\nmercial) Source)\nKnowledge Boundaries Data & Training Fine-tuning Full Customization\nModel Architec-\nScale & Efficiency Pre-set Options Full Customization\nture\nObjectives, Behavior &\nData & Training Fine-tuning Full Customization\nAlignment\nTable 1: The attributes determined by the Model Layer with corresponding components and levels of\ndeveloper access.\n3.3.2 Inference Layer\nTheInferenceLayerutilizesthemodel’slearnedrepresentationstogenerateresponses,aprocessknown\nas decoding. In transformer architecture, the attention mechanism is involved to determine the most\nrelevant context when forming the probability distribution of the next token [37]. In each decoding\nstep, the model produces a probability distribution over all tokens. This distribution is generated\nby applying the softmax function to the logits derived from the model’s final hidden state, and the\nresulting probability distribution serves as the basis for selecting the next token in the sequence.\nThe key objective in inference layer is deciding how tokens are chosen from the logits. The\nna¨ıvewayistoselectthetokenwiththehighestprobability,knownasgreedysampling. Whileefficient,\ngreedy sampling makes the generated sequence too rigid and deterministic. To make the output\ndiverse, methods like top-k [52] and top-p [53] samplings consider a subset of candidate tokens based\non their ranks or cumulative probabilities. Beam search-based decoding uses a heuristic approach of\nsampling multiple candidate sequences and select the tokens based on the overall probability of the\nsequence[54]. Appliedonthelogitsbeforeproduceprobabilitydistributionwithsoftmax,temperature\nscalingmanipulatethelogitsbyscalingitwithatemperaturevalue, sothatlowertemperaturesmake\ntheprobabilitydistributionsharper,leadingtomoredeterministicoutputs,whilehighertemperatures\nflatten the distribution, making the selection more stochastic. These strategies represent macro-level\napproaches to token selection.\nAs the inference layer controls the overall process of token generation, it can also exercise micro-\nlevel control, managing the generation in a fine-grained manner. This involves influencing not just\nthe global token selection strategies, but also making more crafted adjustments during the decoding\nprocess to steer the output. As decoding involves processing the logits, logits manipulation makes\nit possible to directly boost or suppress certain tokens, enforce specific semantic patterns across the\nsequence, or promote or eliminate the generation of particular tokens or sequences. This also enables\nconstrained decoding [26], where the probability of tokens violating predefined constraints is set to\nzero, ensuring that the generated output strictly adheres to the desired structure or format.\nInadditiontocontrollingthetokensgenerated,aprimaryconcernintheInferenceLayerishowto\ngenerate tokens efficiently. Efficient decoding focuses on minimizing latency, reducing computational\noverhead, and optimizing resource usage while maintaining the quality of generated outputs. One\n8\nway to achieve that is speculative decoding, which uses a smaller assistant model alongside the main\nLLMduringinference[55–57]. Theassistantmodelquicklydraftspreliminarycandidatetokens,which\nthe primary model can then accept or correct based on the logits from a single forward pass. Other\nmethodsmayinvolvelowerlevelcustomization,includingpagingtheK-Vcache[58],efficientattention\nmechanism [59], precision reduction [60], and parallelism [61].\nOpen-sourcecommunitysupportawiderangeofInferenceLayerfeaturesthroughvariouslibraries,\nincluding HuggingFace Text Generation Inference11 and llama.cpp12. These tools offer developers\nextensive customization over inference settings, covering macro-level strategies, micro-level controls,\nand efficiency optimizations. In contrast, commercial LLMs typically provide limited customization\noptions for the Inference Layer. Inference efficiency is centrally managed by the vendors, which is\na reasonable encapsulation for simplified interfacing and user experience. The exception is prompt\ncaching, which incurs lower cost when using repeated prefix sequences in their queries. For macro-\nlevel control, vendors usually provide basic parameter adjustments for temperature, top-k, and top-\np sampling, but more advanced sampling methods often remain black-boxed. At the micro-level,\ncustomization is primarily restricted to structured output options via format schemas, offering basic\ncontrol over output format while lacking support for more fine-grained token-level control.\nSummary The Inference Layer is responsible for the text generation procedure of LLMs. Macro-\nlevel strategies steer the global configurations of output, while micro-level controls craft the details of\nthe generated sequence in fine-grained manner. The inference efficiency aspect optimizes latency of\ntheinference,throughapproachessuchasassistedgeneration,KVcacheoperations,efficientattention\nmechanisms, precisionreductionand parallelism. These perspectives withrespectivedeveloper access\nlevels are shown in Table 2.\nDeveloper Access (Open\nAttribute Developer Access (Commercial)\nSource)\nMacro-level Token\nTemperature, Top-K & Top-P Full Customization\nGeneration Control\nMicro-level Token\nStructured Output Full Customization\nGeneration Control\nEfficiency Prompt Caching Full Customization\nTable 2: The attributes determined by the Inference Layer with their levels of developer access.\n3.3.3 Application Layer\nTheApplicationLayertranslatesLLM’stextgenerationpowerintofunctionalities,bridgingthegap\nbetween the raw input/output text generation paradigm of LLMs and fully functional\napplications. From a conceptual viewpoint, the Application Layer centers on four key components:\nPrompt Engineering, Mechanism Engineering, Tooling, and Orchestration. While additional com-\nponents exist in LLM-based applications, such as user interface (UI) and application hosting, these\naspectscloselyalignwithtraditionalsoftwaredevelopmentpracticesandarenotuniquetoLLM-based\napplications.\nPrompt Engineering Prompt Engineering focuses on designing prompts that guide the LLM to\nproduce the desired outputs. The practice of Prompt Engineering is rooted in the foundations es-\ntablished by the Model Layer, where the model learns its internal representations from training data\nand is often fine-tuned to follow instructions. However, the probabilistic nature of LLMs remains\n11TextGenerationInference: https://huggingface.co/docs/text-generation-inference/\n12llama.cpp-LLMInferenceinC/C++: https://github.com/ggerganov/llama.cpp\n9\nunchanged. The generated responses are inherently influenced by the learned statistical patterns and\nthe underlying probability distributions. The core principle of Prompt Engineering is to steer the\nmodel’s probability distribution, amplifying the likelihood of generating the intended response [62].\nOneeffectivestrategyinPromptEngineeringistoreducetheambiguityofthequery. Vaguequery\ndescriptionstendtoproduceinconsistentoutputs,astheyarelesslikelytoalignwiththespecificpat-\nterns the model has learned during training. By providing clear and explicit instructions, the model’s\nfocus can be narrowed, guiding it toward a more concentrated area of its learned representation.\nAnother effective strategy is to supply additional guiding tokens directly within the prompt or to\ntrigger the model to generate them before providing the final response. This helps steer the probabil-\nity distribution towards the desired outcome by establishing a clearer context or guiding the model’s\nreasoning process [22]. The approach leverages patterns learned during training, where the model\nhas encountered sequences involving intermediate reasoning steps, explanations, and structured pro-\ncesses. Sincetheinferenceprocessconsidersallpreviouslygeneratedtokens,thismethodenhancesthe\nlikelihood of producing structurally coherent, contextually relevant, and logically consistent outputs,\nbetter aligning with specific task requirements.\nExamples of these methods include zero-shot learning methods such as role playing [63], step-\nby-step prompting [64], and explain-then-answer prompting [65], and few-shot learning methods such\nas the vanilla few-shot prompting [66] and chain-of-thought prompting [48]. Additionally, prompt\noptimization can be achieved through manual or automated tuning methods [62].\nMechanism Engineering Mechanism Engineering [5] involves integrating LLM’s text generation\ninto modular mechanisms, that employ structured approaches that guide the module’s outputs to-\nwards achieving broader objectives or solving complex problems. Mechanism Engineering designs\nabstract processes that leverage the model’s generative abilities in a systematic way. These mech-\nanisms typically consist of multiple individual text generation processes, each serving orthogonal or\ncomplementary objectives aimed at reaching a specific goal. Different generation processes are often\nconnected through symbolic or rule-based programs, represented using certain structures, such as\ndirected acyclic graph (DAG).\nWhileLLMsgeneratetokensinanautoregressiveandlinearmanner,thisgenerationprocessinher-\nently lacks explicit representations for iteration, recursion, branching, and conditionals. Mechanism\nEngineeringaddressesthislimitationbyprovidinganexternalrepresentationthathelpsmaintainstate\nacross different stages of text generation. By integrating symbolic structures and logical controls, it\nenables the LLM to incorporate external information or runtime state, and perform sophisticated\nworkflows.\nRetrieval-augmented generation (RAG) [15] inject external knowledge into the prompt to provide\nadditional context for knowledge-intensive tasks. The mechanism has evolved by allowing LLM to\nactively query external information, such as Agentic RAG [67] and Retrieval-Interleaved Generation\n(RIG) [68].\nThe trial-and-error method involves repeatedly incorporating error information into the input to\nthe LLM, enabling it to improve its responses until the task is successfully accomplished. [5]. The\nReActframework[69]implementsaninterleavedreasoningandactingmechanism,allowingthemodel\nto incrementally execute tasks by alternating between thought processes and actions. Reflexion [70]\nintroduces more sophisticated mechanisms that enable the model to iteratively work on a task, incor-\nporating information from both its trajectory and experience to enhance performance over time.\nSelf-consistency [71] and tree of thoughts (ToT) [72] both adopt multi-path methods to explore\nthe solution space of the task. In self-consistency, multiple paths are generated for a given prompt,\nand the final solution is determined by the voting mechanism, which selects the most consistent or\nfrequent answer among the generated responses from all paths. In contrast, Tree of Thoughts (ToT)\nemploys the LLM to evaluate intermediate steps along the path.\n10\nTooling Without tools, LLMs are only generators of text, lacking the capacity to perform automa-\ntion tasks, validate and iteratively refine proposed solutions, or access external information. Since\nLLMs can only interact with the environment via text-based input and output, they require tools to\nbe integrated and adapted for use through natural language. The LLM itself cannot directly interact\nwith tool interfaces, so it instead relies on intermediary components that interpret the generated text\nto invoke external tools.\nInpassivetoolcalling,suchasinvanillaRetrieval-AugmentedGeneration(RAG)[15],thedecision\ntocallatool(i.e.,retrieverforthecaseofRAG)ismadeexternally,notbytheLLMitself. Theresult\nof the tool call is then supplied back to the LLM as part of the prompt context. This approach does\nnotrequiretheLLMtobeawareoftheexistenceoftoolsorunderstandtheinterfacesforusingthem.\nIn active tool calling, the LLM is given a degree of agency, allowing it to decide when and which\ntoolstoinvokebasedonthecontextofthetask. Themodelcangeneratestructuredtext(e.g.,JSON)\nthat follows specific conventions designed to trigger tool calls directly, or it can describe the tool\ninvocation in natural language. In scenarios where the LLM uses unstructured natural language to\nspecify the tool call, the responsibility for interpreting and executing the tool request is delegated\nto a specifically fine-tuned LLM, for instance the ToolFormer [47] and DataGemma [68]. This setup\nallows the primary LLM to maintain flexibility in how it interacts with tools while offloading the\nresponsibility of explicitly conducting tool calls to a specialized component.\nOrchestration The orchestration component manages the chaining of LLMs, integrates tools into\nthe application, and maintains the states and overall process of the application.\nLLM chaining involves linking together multiple LLM calls to form a cohesive process, tackling\ntasks in a divide-and-conquer manner. Each call builds upon the responses from previous ones,\nallowing the system to break down complex tasks into smaller, manageable sub-tasks. This enables\nthe application to handle multi-step problems effectively.\nTheorchestrationcomponentalsodetermineswhichtoolsareincludedintheapplication,howthey\nare utilized, and how they are shared across multiple LLM calls. Different tools may have distinct\nexecution behaviors, such as asynchronous processing or requiring specific input formats, which the\norchestration component must accommodate.\nStatesoftheapplicationaswellasbothshort-termandlong-termmemoryaremaintainedaspart\nof orchestration. It tracks session data, user configurations, previous interactions, and the sequence\nof LLM calls, ensuring that context is preserved throughout the entire task execution. Additionally,\nthe orchestration component handles errors and exceptions, particularly when the LLM generates\nunexpected outputs. It includes mechanisms to detect issues, trigger corrective actions, or adjust the\nprocess, ensuring a smooth and consistent execution.\nSummary The Application Layer is responsible for leveraging the text generation power to build-\ningfunctionalapplications. ThePromptEngineeringcomponentfocusesoncraftingeffectiveprompts\nthatamplifytheprobabilityofgeneratingdesiredresponses. TheMechanismEngineeringcomponent\nenables sophisticated workflow operations and supports the incorporation of runtime state, allowing\nfor advanced processes beyond simple text generation. The Tooling component extend the abilities\nof LLMs by integrating external tools, enabling the model to perform automation tasks, validate\nsolutions, and access external information. Finally, the Orchestration component manages the co-\nordination of the LLM’s text generation, external tools, and user inputs, ensuring a smooth flow\nof information and the robust execution of workflows. These attributes with their corresponding\ncomponents are shown in Table 3.\n3.3.4 Intra-layer and Inter-layer Dependencies\nThe layers and components of the framework are not isolated. There are dependencies across and\nwithin layers, such that achieving the desired capability may require support from other components\n11\nAttribute Component\nProbability Amplification of Intended Re-\nPrompt Engineering\nsponses\nRuntime States Incorporation Mechanism Engineering\nSophisticated Workflows Mechanism Engineering\nExternal Information Mechanism Engineering & Tooling\nInteraction with Environment Mechanism Engineering & Tooling\nException Handling Mechanism Engineering & Orchestration\nProcess and State Management Orchestration\nTable 3: The attributes determined by the Application Layer with corresponding components.\nwithin the same layer, i.e., Intra-layer Dependencies, or from components across different layers, i.e.,\nInter-layer Dependencies.\nIntra-layer Dependencies In the Model Layer, components often depend on one another to work\neffectively. For instance, certain fine-tuning methods rely heavily on the availability and format of\nspecific data. To perform instruction tuning, the data must be curated in the form of instruction-\noutput pairs. Without this structured data format, the fine-tuning process cannot align the model\neffectively tune the model to follow instructions.\nIn the Application Layer, Mechanism Engineering has dependencies on both Prompt Engineering\nandToolingcomponents. MechanismEngineeringoftenrequirescarefullycraftedpromptstoguidethe\nLLM towards desired responses. Additionally, certain mechanisms rely on the integration of external\ntools to complete the task.\nInter-layer Dependencies The Inference Layer often relies on components from the Model Layer\nto achieve optimal performance and efficiency. For example, efficient inference methods, such as\nrunning the model in reduced precision, may depend on specific training methods like quantization-\naware training conducted in the Model Layer.\nThe Application Layer depends heavily on both the Model and Inference Layers. For instance,\nPromptEngineeringisinfluencedbythemodelarchitecture,particularlythescaleofthemodel,which\ndetermines its ability to perform tasks like few-shot learning. Additionally, the presence of specific\nrepresentationslearnedfromdataiscrucial,aseffectivepromptsrelyonamplifyingthemodel’sprob-\nabilitydistributiontoguideresponsestowarddesiredoutputs. Theeffectivenessofcertainpromptsor\nmechanisms, especially those relying on instruction-following, often requires instruction tuning at the\nModel Layer to align the model’s behavior. Moreover, the transferability of prompts across different\nmodels depends on characteristics of models from the Model Layer [62].\nIn Application Layer, to build tools effectively, especially when employing active and direct tool\ncalling, the support for structured output generation in the Inference Layer is beneficial, as it helps\nthe LLM produce responses in the correct format required by the tool interfaces.\n3.4 Mapping Capabilities onto Layers\nThe Capability Mapping aims at aligning specific system capabilities with the attributes and compo-\nnents across the layers of the architecture. We will use the capability of generating JSON output as\nan example to illustrate this process.\nAttributes Identification The first step in implementing a capability is to identify its relevant\nattributes across the layers. As demonstrated above, different layers are responsible for distinct at-\ntributesofLLM-basedapplications. Eachcapabilitymaycorrespondtooneormorelayers,depending\n12\nonitscharacteristicsandrequirements. Additionally,someattributesmaybecomedepreciatinginne-\ncessity due to the presence of other attributes.\nThe capability of generating JSON output has several key attributes, shown in Table 4.\nAttribute Layer Explanation\nHaving the knowledge of JSON syntax and structure\nKnowledge Boundaries Model Layer intherepresentationofmodelishelpfulforgenerating\ncompliant output\nObjectives, Behavior & Having model to follow explicit instructions improves\nModel Layer\nAlignment the ability to generate output in a specific format\nFine-grained control at the time of token generation\nMicro-level Token Gen-\nInferenceLayer can ensure that the output adheres to the JSON for-\neration Control\nmat\nProbability Amplifica- Usingclearandexplicitpromptexamplescanincrease\ntion of Intended Re- App Layer the likelihood of generating the response in desired\nsponses structure\nIfthemodel’sinternalknowledgedoesnotsufficiently\nExternal Information App Layer cover JSON structure, external information can be\nsupplied to the LLM\nInteraction with Envi- Integrating external tools can validate the response\nApp Layer\nronment format and provide feedback for correction\nError detection and correction mechanisms handle\nException Handling App Layer cases where LLM fail to generate desired output for-\nmat\nTable4: AttributesrelatedtothecapabilityofgeneratingJSONoutput. Lightgrayrowsareidentified\nas depreciating.\nUpon reviewing these attributes, we identify several depreciating instances. The knowledge about\nJSON syntax within Knowledge Boundaries (Model Layer) may reduce the necessity for External\nInformation(ApplicationLayer)ifmodel’strainingdataalreadyincludessufficientexamplesofJSON\nstructures. Moreover, Micro-level Token Generation Control (Inference Layer) can effectively enforce\nJSON format, making extensive Exception Handling (Application Layer) less critical. In light of\nthis, both can be implemented in lightweight manner. By resolving depreciating instances, we avoid\nredundancy and oversophistication while maintaining effectiveness.\nSolution Architecture Based on the identified attributes, we can design a solution architecture\nthat aligns solutions with the appropriate components in each layer. It is important to account for\nIntra-layer and Inter-layer Dependencies. A simplified description for the JSON generation example\nis shown in Table 5.\nAccess Resolution In cases where certain components in the architecture are gated or restricted,\nalternative solutions need to be considered.\nIn the JSON output generation example, if the Inference Layer is gated, developers can consider\nutilizing open-source alternatives. Alternatively, developers can compensate the gated components\nby enabling fallback mechanisms, such as the exception handling previously considered depreciating,\nwhich can be enabled to validate and correct the output format with mechanisms such as trial-and-\nerror.\nHowever, developers must also consider the cost-effectiveness of implementing such sophisticated\nand depreciating workarounds, especially if there is a strong likelihood that these capabilities will be\n13\nLayer Component Solution\nInclude examples of instructions and JSON documents\nData during fine-tuning to help model learn the instructions,\nModel\nsyntax and patterns\nLayer\nModel Architecture Ensure model selected is capable of few-shot learning\nApply instruction tuning or fine-tuning on JSON genera-\nTraining\ntion tasks\nInference Manipulate logits to prevent the generation of format-violating tokens at\nLayer each decoding step\nEnsure the clarity of prompt, and provide few-shot exam-\nPrompt Engineering\nples for generation\nApp\nMechanism Engineering Implement lightweight exception handling mechanism\nLayer\nTooling Integrate JSON parser\nOrchestration Ensurethattheworkflowcandetectandhandleexceptions\nTable 5: The solution at each layer for implementing the capability of generating JSON output.\ncentrallyprovidedbyvendorsinfutureupdates,drivenbyhighdemand. Inthesecases,itisimportant\nfor developers to evaluate their own priorities to decide whether to implement these solutions or wait\nfor vendor support, based on their demands and resource constraints.\nEvaluation While the capability mapping provides a useful framework for guiding the implemen-\ntation, it cannot replace the need for thorough evaluation of the solutions. It is crucial to conduct\ncomprehensive testing, including ablation studies and continuous monitoring, to assess the effective-\nness of each component. By dynamically adjusting the sophistication of each component in response\nto evaluation feedback, developers can ensure both the effectiveness and efficiency of the implemen-\ntation.\n4 Use Case Evaluation\nInadditiontotheJSONoutputgenerationexample,weevaluatetheusefulnessoftheframeworkwith\na variety of use cases.\nCreativity Creativity refers to the capability of generating original ideas. While LLMs cannot\ntruly generalize beyond the data they have been trained on, they can produce responses that appear\ncreative by leveraging different aspects across the layers, as demonstrated in Table 6.\nDepend on the type of creativity needed, different solutions across layers can be selected or com-\nbinedfordesiredeffect. Forexample,MicrosoftCopilotusesthetemperatureparametertocontrolthe\nlevel of creativity in responses13. In contrast, for more specialized applications like scientific research\nagents, sophisticated workflows are employed for controlled creativity in creative tasks [73]. However,\nsuch complex workflows may be overly resource-intensive for general applications, where users only\nrequire a degree of semantic variation, rather than exhaustive creative exploration.\nCall Caching Call Caching refers to the capability of reusing results from previous LLM calls to\nreducecomputationalcostsandoverheads. Thesystemcansolutionstoachieveit,asshowninTable7.\nLLMvendorsincludingOpenAIandAnthropicintegratepromptcachingatInferenceLayer,which\nsignificantly reduces inference costs for bulk queries with same long prefix.\n13MicrosoftCopilotforTechnicalLeaders: https://learn.microsoft.com/copilot/tutorials/learn-microsoft-c\nopilot?tutorial-step=1\n14\nAttribute Layer Explanation\nAbroadanddiversetrainingdatasetequipstheLLM\nwithavastpoolofknowledge,enablingittogenerate\nKnowledge Boundaries Model Layer ideas that may be previously unseen by users. The\nmodel can also create new combinations by linking\nconcepts or knowledge from different domains.\nLess control and alignment over human preferences\nObjectives, Behavior, &\nModel Layer may lead to more stochastic and unconventional re-\nAlignment\nsponses.\nMacro-level Token Gen- Decoding strategies such as temperature adjustment\nInferenceLayer\neration Control and top-k may lead to less deterministic output.\nProbability Ampli- Crafted prompts reinforce the model to align re-\nfication of Intended App Layer sponses with the forms of creativity it has learned\nResponses during training.\nWorkflowsandmechanismsthatmimichumanbrain-\nSophisticatedWorkflows App Layer stormingandideaselectionprocesstogeneratesensi-\nble ideas.\nTable 6: The solution at each layer for provoking creativity.\nAttribute Layer Explanation\nBy storing and reusing attention states for repeated long\nEfficiency InferenceLayer prefix sequences, the system can skip recomputing these\nprefix sequences.\nHash-basedlookupforstoringandretrievingpreviousinput\nProcess and State\nApp Layer and output pairs can avoid the same query being recom-\nManagement\nputed.\nTable 7: The solution at each layer for call caching.\nLangChainsupportscachingfortoolcallsinApplicationLayer,whichisbeneficialwhenLLMsare\nwrapped behind these tools, as it prevents redundant computations. Similarly, AIGNE14 provides an\nabstraction where components, including LLMs, are treated as blocklets, which are callable by users\nor other blocklets. Caching interactions between blocklets helps avoid repetitive nested invocations.\nThese Application Layer caching mechanisms may not be effective in scenarios where the prompts\nvary across requests, but they can be useful in cases involving high-computational components like\nLLMs, where the same request is repeated multiple times within a period. In such cases, caching can\nminimize cost and overhead.\nLong Context ChatGPTwasonlyaccepting4096tokensattheinitialreleasedandwasinsufficient\nfor advanced usage such as codebase analysis. A larger context window enables the model to capture\nrich context or handle long documents. The solution at each layer is shown in Table 8.\nWhile the Model Layer solutions are native and robust for implementation, it may be unavailable\nfor commercial models if long context is not supported. Solutions at the Application Layer, such as\nprompt compression, offer a flexible workaround, but they typically result in higher inference costs,\nand may be less robust and more prone to information loss.\n14AIGNE:https://www.aigne.io/\n15\nAttribute Layer Explanation\nEmploy specialized attention mechanisms like Efficient Atten-\nScale & Effi-\nModel Layer tion [74] and extended positional encodings such as RoPE [75]\nciency\nto handle long input sequences.\nShortening the prompt through prompt compression mecha-\nSophisticated\nApp Layer nismsatApplicationLayersuchasLLMLingua[76]allowmore\nWorkflows\ncontent to fit within the model’s context window.\nTable 8: The solution at each layer for long context.\n5 Related Work\nSeveral studies have examined the design and architectural considerations of LLM-based systems.\nZhou et al. [77] present a decision model for foundation model-based agents, analyzing trade-offs\nbetween various options for each type of architectural element. Lu et al. [78] propose a layered\nreference architecture for foundation model-based systems, emphasizing a pattern-oriented approach\nthat focuses on component orchestration. Liu et al. [79] compile a comprehensive design patterns\ncatalogue for foundation model-based agents.\nIn specific areas, Shamsujjoha et al. [80] provide a taxonomy focused on runtime guardrails for\nLLM agents, exploring a set of guardrail options at each level of system. Wang et al. [5] introduce\nthe concept of Mechanism Engineering, surveying relevant methods from the perspective of design\nmechanismsandapplicationsofautonomousagent. Liuetal.[62]offeranextensivesurveyonvarious\npromptingstrategiesforLLMsaswellastheirrelevantenablingmethodsrequiredduringpre-training\nand inference. Welleck et al. [81] explore inference-time algorithms, highlighting various objectives\nof decoding strategies. Zhang et al. [43] provide a detailed survey on instruction tuning, discussing\naspects including dataset preparation, tuning methods, and efficient tuning techniques.\nWhile these works provide valuable insights into specific aspects, problems, or a particular layer\nof LLM-based applications, they address isolated components without a unified perspective. In con-\ntrast, our work fills this gap by offering a holistic layered approach that systematically examines the\ncharacteristicsofeachlayerwithinLLM-basedapplicationsandguidethedevelopmentofcapabilities\nacross these layers.\n6 Conclusion\nThis paper presents a layered approach for implementing capabilities in LLM-based applications,\ndecouplingthesystemintothreedistinctlayers: ModelLayer,InferenceLayer,andApplicationLayer.\nEach layer is assigned specific attributes based on the characteristics of layers and their components.\nBy aligning capabilities with relevant attributes across layers, this approach enables developers to\nsystematically identify implementation strategies.\nThe proposed framework was evaluated against several typical use cases, demonstrating its use-\nfulness and feasibility in guiding the design of LLM-based applications. Our work bridges the gap\nbetween high-level capability requirements and the underlying architectural components, offering a\nholistic strategy accommodating the development of capabilities in LLM-based applications.\nReferences\n[1] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, “Code-\ngen: An open large language model for code with multi-turn program synthesis,” arXiv preprint\narXiv:2203.13474, 2022.\n16\n[2] X.Hou,Y.Zhao,Y.Liu,Z.Yang,K.Wang,L.Li,X.Luo,D.Lo,J.Grundy,andH.Wang,“Large\nlanguage models for software engineering: A systematic literature review,” ACM Transactions\non Software Engineering and Methodology, 2023.\n[3] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,\nQ. Le et al., “Program synthesis with large language models,” arXiv preprint arXiv:2108.07732,\n2021.\n[4] G. Kim, P. Baldi, and S. McAleer, “Language models can solve computer tasks,” Advances in\nNeural Information Processing Systems, vol. 36, 2024.\n[5] L.Wang,C.Ma,X.Feng,Z.Zhang,H.Yang,J.Zhang,Z.Chen,J.Tang,X.Chen,Y.Linet al.,\n“A survey on large language model based autonomous agents,” Frontiers of Computer Science,\nvol. 18, no. 6, p. 186345, 2024.\n[6] Y. Nie, Y. Kong, X. Dong, J. M. Mulvey, H. V. Poor, Q. Wen, and S. Zohren, “A survey of large\nlanguage models for financial applications: Progress, prospects and challenges,” arXiv preprint\narXiv:2406.11903, 2024.\n[7] S.Wu,O.Irsoy,S.Lu,V.Dabravolski,M.Dredze,S.Gehrmann,P.Kambadur,D.Rosenberg,and\nG.Mann,“Bloomberggpt: Alargelanguagemodelforfinance,”arXivpreprintarXiv:2303.17564,\n2023.\n[8] X. Wang, N. Anwer, Y. Dai, and A. Liu, “Chatgpt for design, manufacturing, and education,”\nProcedia CIRP, vol. 119, pp. 7–14, 2023.\n[9] H. Fan, X. Liu, J. Y. H. Fuh, W. F. Lu, and B. Li, “Embodied intelligence in manufactur-\ning: leveraging large language models for autonomous industrial robotics,” Journal of Intelligent\nManufacturing, pp. 1–17, 2024.\n[10] Y. Dai, A. Liu, and C. P. Lim, “Reconceptualizing chatgpt and generative ai as a student-driven\ninnovation in higher education,” Procedia CIRP, vol. 119, pp. 84–90, 2023.\n[11] S. Sarsa, P. Denny, A. Hellas, and J. Leinonen, “Automatic generation of programming exercises\nandcodeexplanationsusinglargelanguagemodels,”inProceedings of the 2022 ACM Conference\non International Computing Education Research-Volume 1, 2022, pp. 27–43.\n[12] D.A.Boiko,R.MacKnight,B.Kline,andG.Gomes,“Autonomouschemicalresearchwithlarge\nlanguage models,” Nature, vol. 624, no. 7992, pp. 570–578, 2023.\n[13] A.J.Thirunavukarasu,D.S.J.Ting,K.Elangovan,L.Gutierrez,T.F.Tan, andD.S.W.Ting,\n“Large language models in medicine,” Nature medicine, vol. 29, no. 8, pp. 1930–1940, 2023.\n[14] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P. Kumar, E. Dupont, F. J. Ruiz,\nJ. S. Ellenberg, P. Wang, O. Fawzi et al., “Mathematical discoveries from program search with\nlarge language models,” Nature, vol. 625, no. 7995, pp. 468–475, 2024.\n[15] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Ku¨ttler, M. Lewis, W.-t.\nYih, T. Rockt¨aschel et al., “Retrieval-augmented generation for knowledge-intensive nlp tasks,”\nAdvances in Neural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.\n[16] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen et al.,\n“Siren’ssongintheaiocean: asurveyonhallucinationinlargelanguagemodels,”arXiv preprint\narXiv:2309.01219, 2023.\n[17] A. Radford, “Improving language understanding by generative pre-training,” 2018.\n17\n[18] L.Zhou,W.Schellaert,F.Mart´ınez-Plumed,Y.Moros-Daval,C.Ferri,andJ.Hern´andez-Orallo,\n“Larger and more instructable language models become less reliable,” Nature, pp. 1–8, 2024.\n[19] J. Yang, C. E. Jimenez, A. Wettig, K. Lieret, S. Yao, K. Narasimhan, and O. Press, “Swe-\nagent: Agent-computer interfaces enable automated software engineering,” arXiv preprint\narXiv:2405.15793, 2024.\n[20] X. Li, S. Chan, X. Zhu, Y. Pei, Z. Ma, X. Liu, and S. Shah, “Are chatgpt and gpt-4 general-\npurpose solvers for financial text analytics? a study on several typical tasks,” in Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track,\n2023, pp. 408–422.\n[21] X. Tang, Y. Zong, J. Phang, Y. Zhao, W. Zhou, A. Cohan, and M. Gerstein, “Struc-bench:\nAre large language models really good at generating complex structured data?” arXiv preprint\narXiv:2309.08963, 2023.\n[22] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma, “An explanation of in-context learning as\nimplicit bayesian inference,” arXiv preprint arXiv:2111.02080, 2021.\n[23] A. Dunn, J. Dagdelen, N. Walker, S. Lee, A. S. Rosen, G. Ceder, K. Persson, and A. Jain,\n“Structured information extraction from complex scientific text with fine-tuned large language\nmodels,” arXiv preprint arXiv:2212.05238, 2022.\n[24] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,\nJ.Quan,T.Ramalho,A.Grabska-Barwinskaetal.,“Overcomingcatastrophicforgettinginneural\nnetworks,”Proceedings of the national academy of sciences,vol.114,no.13,pp.3521–3526,2017.\n[25] R. J. Parikh, “On context-free languages,” Journal of the ACM (JACM), vol. 13, no. 4, pp.\n570–581, 1966.\n[26] C.HokampandQ.Liu,“Lexicallyconstraineddecodingforsequencegenerationusinggridbeam\nsearch,” arXiv preprint arXiv:1704.07138, 2017.\n[27] U. Anwar, A. Saparov, J. Rando, D. Paleka, M. Turpin, P. Hase, E. S. Lubana, E. Jenner,\nS. Casper, O. Sourbut, B. L. Edelman, Z. Zhang, M. Gu¨nther, A. Korinek, J. Hernandez-Orallo,\nL. Hammond, E. J. Bigelow, A. Pan, L. Langosco, T. Korbak, H. C. Zhang, R. Zhong,\nS. O. hEigeartaigh, G. Recchia, G. Corsi, A. Chan, M. Anderljung, L. Edwards, A. Petrov,\nC. S. de Witt, S. R. Motwani, Y. Bengio, D. Chen, P. Torr, S. Albanie, T. Maharaj,\nJ. N. Foerster, F. Tram`er, H. He, A. Kasirzadeh, Y. Choi, and D. Krueger, “Foundational\nchallenges in assuring alignment and safety of large language models,” Transactions on\nMachine Learning Research, 2024, survey Certification, Expert Certification. [Online]. Available:\nhttps://openreview.net/forum?id=oVTkOs8Pka\n[28] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma,\nD. Zhou, D. Metzler et al., “Emergent abilities of large language models,” Transactions on Ma-\nchine Learning Research, 2022.\n[29] T. Shevlane, S. Farquhar, B. Garfinkel, M. Phuong, J. Whittlestone, J. Leung, D. Kokotajlo,\nN. Marchal, M. Anderljung, N. Kolt et al., “Model evaluation for extreme risks,” arXiv preprint\narXiv:2305.15324, 2023.\n[30] P. Zhou, A. Madaan, S. P. Potharaju, A. Gupta, K. R. McKee, A. Holtzman, J. Pujara, X. Ren,\nS. Mishra, A. Nematzadeh et al., “How far are large language models from agents with theory-\nof-mind?” arXiv preprint arXiv:2310.03051, 2023.\n[31] L.Bass,P.Clements,andR.Kazman,Software Architecture in Practice. Addison-Wesley,2012.\n18\n[32] M. Richards, Software architecture patterns. O’Reilly Media, Incorporated 1005 Gravenstein\nHighway North, Sebastopol, CA ..., 2015, vol. 4.\n[33] C. J. Kale and T. J. Socolofsky, “TCP/IP tutorial,” RFC 1180, Jan. 1991. [Online]. Available:\nhttps://www.rfc-editor.org/info/rfc1180\n[34] M. Khan and A. Hanna, “The subjects and stages of ai dataset development: A framework for\ndataset accountability,” Ohio St. Tech. LJ, vol. 19, p. 171, 2022.\n[35] S.Biswas, M. Wardat, andH. Rajan, “Theart and practiceof data sciencepipelines: Acompre-\nhensive study of data science pipelines in theory, in-the-small, and in-the-large,” in Proceedings\nof the 44th International Conference on Software Engineering, 2022, pp. 2091–2103.\n[36] C.-Y. Hsieh, C.-L. Li, C.-k. Yeh, H. Nakhost, Y. Fujii, A. Ratner, R. Krishna, C.-Y. Lee, and\nT.Pfister, “Distillingstep-by-step! outperforminglargerlanguagemodelswithlesstrainingdata\nand smaller model sizes,” in Findings of the Association for Computational Linguistics: ACL\n2023, 2023, pp. 8003–8017.\n[37] A. Vaswani, “Attention is all you need,” Advances in Neural Information Processing Systems,\n2017.\n[38] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, “Outrageously\nlargeneuralnetworks: Thesparsely-gatedmixture-of-expertslayer,” inInternational Conference\non Learning Representations, 2016.\n[39] Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Zhao, A. M. Dai, Q. V. Le, J. Laudon et al.,\n“Mixture-of-experts with expert choice routing,” Advances in Neural Information Processing\nSystems, vol. 35, pp. 7103–7114, 2022.\n[40] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding\nby generative pre-training.”\n[41] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional\ntransformers for language understanding,” in Proceedings of the 2019 Conference of the North\nAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnolo-\ngies, Volume 1 (Long and Short Papers), 2019, pp. 4171–4186.\n[42] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., “Language models are\nunsupervised multitask learners,” OpenAI blog, vol. 1, no. 8, p. 9, 2019.\n[43] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu, T. Zhang, F. Wu et al.,\n“Instructiontuningforlargelanguagemodels: Asurvey,”arXivpreprintarXiv:2308.10792,2023.\n[44] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, “Direct preference\noptimization: Yourlanguagemodelissecretlyarewardmodel,” Advances in Neural Information\nProcessing Systems, vol. 36, 2024.\n[45] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK.Slama,A.Rayet al.,“Traininglanguagemodelstofollowinstructionswithhumanfeedback,”\nAdvances in neural information processing systems, vol. 35, pp. 27730–27744, 2022.\n[46] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-\nseini, C. McKinnon et al., “Constitutional ai: Harmlessness from ai feedback,” arXiv preprint\narXiv:2212.08073, 2022.\n[47] T.Schick, J.Dwivedi-Yu, R.Dess`ı, R.Raileanu, M.Lomeli, E.Hambro, L.Zettlemoyer, N.Can-\ncedda, and T. Scialom, “Toolformer: Language models can teach themselves to use tools,” Ad-\nvances in Neural Information Processing Systems, vol. 36, 2024.\n19\n[48] J.Wei,X.Wang,D.Schuurmans,M.Bosma,F.Xia,E.Chi,Q.V.Le,D.Zhouet al.,“Chain-of-\nthought prompting elicits reasoning in large language models,” Advances in neural information\nprocessing systems, vol. 35, pp. 24824–24837, 2022.\n[49] J.-B.Grill,F.Altch´e,Y.Tang,T.Hubert,M.Valko,I.Antonoglou,andR.Munos,“Monte-carlo\ntreesearchasregularizedpolicyoptimization,”inInternationalConferenceonMachineLearning.\nPMLR, 2020, pp. 3769–3778.\n[50] E. Zelikman, G. R. Harik, Y. Shao, V. Jayasiri, N. Haber, and N. Goodman, “Quiet-STar:\nLanguage models can teach themselves to think before speaking,” in First Conference on\nLanguage Modeling, 2024. [Online]. Available: https://openreview.net/forum?id=oRXPiSOGH9\n[51] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “Lora:\nLow-rank adaptation of large language models,” arXiv preprint arXiv:2106.09685, 2021.\n[52] A. Fan, M. Lewis, and Y. Dauphin, “Hierarchical neural story generation,” in Proceedings of the\n56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\n2018, pp. 889–898.\n[53] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, “The curious case of neural text degen-\neration,” in International Conference on Learning Representations, 2019.\n[54] M. Freitag and Y. Al-Onaizan, “Beam search strategies for neural machine translation,” ACL\n2017, p. 56, 2017.\n[55] Y. Leviathan, M. Kalman, and Y. Matias, “Fast inference from transformers via speculative\ndecoding,” in International Conference on Machine Learning. PMLR, 2023, pp. 19274–19286.\n[56] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, and J. Jumper, “Accelerating large\nlanguage model decoding with speculative sampling,” arXiv preprint arXiv:2302.01318, 2023.\n[57] Y.Li, F.Wei, C.Zhang, andH.Zhang, “Eagle: Speculativesamplingrequiresrethinkingfeature\nuncertainty,” arXiv preprint arXiv:2401.15077, 2024.\n[58] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Sto-\nica, “Efficient memory management for large language model serving with pagedattention,” in\nProceedings of the 29th Symposium on Operating Systems Principles, 2023, pp. 611–626.\n[59] T.Dao,D.Fu,S.Ermon,A.Rudra,andC.R´e,“Flashattention: Fastandmemory-efficientexact\nattention with io-awareness,” Advances in Neural Information Processing Systems, vol. 35, pp.\n16344–16359, 2022.\n[60] T.Kumar,Z.Ankner,B.F.Spector, B.Bordelon,N.Muennighoff,M.Paul, C.Pehlevan,C.R´e,\nand A. Raghunathan, “Scaling laws for precision,” arXiv preprint arXiv:2411.04330, 2024.\n[61] R. Y. Aminabadi, S. Rajbhandari, A. A. Awan, C. Li, D. Li, E. Zheng, O. Ruwase, S. Smith,\nM.Zhang,J.Rasleyetal.,“Deepspeed-inference: enablingefficientinferenceoftransformermod-\nelsatunprecedentedscale,”inSC22: InternationalConferenceforHighPerformanceComputing,\nNetworking, Storage and Analysis. IEEE, 2022, pp. 1–15.\n[62] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing,” ACM Computing\nSurveys, vol. 55, no. 9, pp. 1–35, 2023.\n[63] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar, J. Spencer-Smith, and\nD. C. Schmidt, “A prompt pattern catalog to enhance prompt engineering with chatgpt,” arXiv\npreprint arXiv:2302.11382, 2023.\n20\n[64] T.Kojima,S.S.Gu,M.Reid,Y.Matsuo,andY.Iwasawa,“Largelanguagemodelsarezero-shot\nreasoners,” Advances in neural information processing systems, vol. 35, pp. 22199–22213, 2022.\n[65] V. Shwartz, P. West, R. Le Bras, C. Bhagavatula, and Y. Choi, “Unsupervised commonsense\nquestion answering with self-talk,” in Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), 2020, pp. 4615–4629.\n[66] T. B. Brown, “Language models are few-shot learners,” arXiv preprint arXiv:2005.14165, 2020.\n[67] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang, J. Callan, and G. Neubig,\n“Active retrieval augmented generation,” arXiv preprint arXiv:2305.06983, 2023.\n[68] P. Radhakrishnan, J. Chen, B. Xu, P. Ramaswami, H. Pho, A. Olmos, J. Manyika, and\nR. Guha, “Knowing when to ask–bridging large language models and data,” arXiv preprint\narXiv:2409.13741, 2024.\n[69] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao, “React: Synergizing\nreasoningandactinginlanguagemodels,”inThe Eleventh International Conference on Learning\nRepresentations, 2023.\n[70] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Reflexion: Language agents\nwithverbalreinforcementlearning,”AdvancesinNeuralInformationProcessingSystems,vol.36,\n2024.\n[71] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou,\n“Self-consistency improves chain of thought reasoning in language models,” arXiv preprint\narXiv:2203.11171, 2022.\n[72] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan, “Tree of thoughts:\nDeliberate problem solving with large language models,” Advances in Neural Information Pro-\ncessing Systems, vol. 36, 2024.\n[73] Q.Huang,J.Vora,P.Liang,andJ.Leskovec,“Benchmarkinglargelanguagemodelsasairesearch\nagents,” in NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.\n[74] Z. Shen, M. Zhang, H. Zhao, S. Yi, and H. Li, “Efficient attention: Attention with linear com-\nplexities,”inProceedingsoftheIEEE/CVFwinterconferenceonapplicationsofcomputervision,\n2021, pp. 3531–3539.\n[75] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, “Roformer: Enhanced transformer with\nrotary position embedding,” Neurocomputing, vol. 568, p. 127063, 2024.\n[76] H. Jiang, Q. Wu, C.-Y. Lin, Y. Yang, and L. Qiu, “Llmlingua: Compressing prompts for accel-\nerated inference of large language models,” in Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, 2023, pp. 13358–13376.\n[77] J. Zhou, Q. Lu, J. Chen, L. Zhu, X. Xu, Z. Xing, and S. Harrer, “A taxonomy of architec-\nture options for foundation model-based agents: Analysis and decision model,” arXiv preprint\narXiv:2408.02920, 2024.\n[78] Q.Lu,L.Zhu,X.Xu,Z.Xing,andJ.Whittle,“Towardsresponsibleaiintheeraofgenerativeai:\nA reference architecture for designing foundation model based systems,” IEEE Software, 2024.\n[79] Y. Liu, S. K. Lo, Q. Lu, L. Zhu, D. Zhao, X. Xu, S. Harrer, and J. Whittle, “Agent design\npattern catalogue: A collection of architectural patterns for foundation model based agents,”\narXiv preprint arXiv:2405.10467, 2024.\n21\n[80] M. Shamsujjoha, Q. Lu, D. Zhao, and L. Zhu, “A taxonomy of multi-layered runtime guardrails\nfor designing foundation model-based agents: Swiss cheese model for ai safety by design,” arXiv\npreprint arXiv:2408.02205, 2024.\n[81] S. Welleck, A. Bertsch, M. Finlayson, H. Schoelkopf, A. Xie, G. Neubig, I. Kulikov, and Z. Har-\nchaoui, “From decoding to meta-generation: Inference-time algorithms for large language mod-\nels,” arXiv preprint arXiv:2406.16838, 2024.\n22",
    "pdf_filename": "A_Layered_Architecture_for_Developing_and_Enhancing_Capabilities_in_Large_Language_Model-based_Softw.pdf"
}