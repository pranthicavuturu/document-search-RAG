{
    "title": "A Layered Architecture for Developing and Enhancing Capabilities in Large Language Model-based Softw",
    "abstract": "Significant efforts has been made to expand the use of Large Language Models (LLMs) beyond basic language tasks. While the generalizability and versatility of LLMs have enabled widespread adoption, evolving demands in application development often exceed their native capabilities. Meeting these demands may involve a diverse set of methods, such as enhancing creativity through either inference temperature adjustments or creativity-provoking prompts. Selecting the right ap- proach is critical, as different methods lead to trade-offs in engineering complexity, scalability, and operational costs. This paper introduces a layered architecture that organizes LLM software system development into distinct layers, each characterized by specific attributes. By aligning capabilities with these layers, the framework encourages the systematic implementation of capa- bilities in effective and efficient ways that ultimately supports desired functionalities and quali- ties. Through practical case studies, we illustrate the utility of the framework. This work offers developers actionable insights for selecting suitable technologies in LLM-based software system development, promoting robustness and scalability.",
    "body": "A Layered Architecture for Developing and Enhancing\nCapabilities in Large Language Model-based Software Systems\nDawen Zhang1∗, Xiwei Xu1, Chen Wang1, Zhenchang Xing1, and Robert Mao2\n1CSIRO’s Data61, 2ArcBlock\n∗David.Zhang@data61.csiro.au\nAbstract\nSignificant efforts has been made to expand the use of Large Language Models (LLMs) beyond\nbasic language tasks. While the generalizability and versatility of LLMs have enabled widespread\nadoption, evolving demands in application development often exceed their native capabilities.\nMeeting these demands may involve a diverse set of methods, such as enhancing creativity through\neither inference temperature adjustments or creativity-provoking prompts. Selecting the right ap-\nproach is critical, as different methods lead to trade-offs in engineering complexity, scalability,\nand operational costs. This paper introduces a layered architecture that organizes LLM software\nsystem development into distinct layers, each characterized by specific attributes. By aligning\ncapabilities with these layers, the framework encourages the systematic implementation of capa-\nbilities in effective and efficient ways that ultimately supports desired functionalities and quali-\nties. Through practical case studies, we illustrate the utility of the framework. This work offers\ndevelopers actionable insights for selecting suitable technologies in LLM-based software system\ndevelopment, promoting robustness and scalability.\nKeywords: Artificial intelligence, large language model, software architecture, agent\n1\nIntroduction\nThe use of Large Language Models (LLMs) has expanded beyond traditional language-related tasks\nsuch as translation and question answering. This widespread adoption is largely driven by the gen-\neralizability and versatility of LLMs, which stem from being trained on vast amounts of diverse data\nsourced from the internet and human annotations, allowing them to capture patterns across many\ndomains. Furthermore, LLMs use text as a flexible input/output interface, which makes interact-\ning with them intuitive and adaptable to various contexts. Combined with advances in techniques\nthat improve their ability to follow instructions and align with specific needs, LLMs have been in-\ncreasingly applied to a variety of domain applications requiring flexibility and scalability, including\nsoftware development [1–3], process automation [4, 5], financial analysis [6, 7], manufacturing [8, 9],\neducation [10,11], and scientific research [12–14].\nHowever, despite their remarkable strengths, LLMs have clear limitations. For instance, even when\ntrained on vast datasets, they often struggle with domain-specific knowledge and lack the specialized\nexpertise needed for certain tasks [15]. Additionally, LLMs are prone to generating plausible-sounding\nbut factually incorrect outputs [16], commonly referred to as “hallucination.” Since LLMs are pri-\nmarily trained on the NLP task of next word prediction based on statistical probabilities [17], they\nmay be less reliable and efficient than simpler but specialized tools for tasks that require a deeper\nunderstanding or internal task representation [18]. Furthermore, due to their language processing\nnature, they also interact with the external world exclusively through the interface of text, limiting\neffective access to the diverse interfaces of external tools or systems [19].\n1\narXiv:2411.12357v1  [cs.SE]  19 Nov 2024\n\nAs LLMs are applied to more complex and varied tasks, new requirements continue to emerge.\nThese novel demands often go beyond the initial capabilities of the models, poses significant chal-\nlenges during the development of LLM-based applications. One of the most critical challenges lies\nin identifying the appropriate methods for implementing specific capabilities within LLMs [20, 21].\nCapabilities such as incorporating domain knowledge, enforcing specific constraints, or adhering to\nparticular styles are not off-the-shelf features supported by LLMs, but can be achieved through various\napproaches. Incorporating domain knowledge is a particularly typical example, which can be achieved\nthrough techniques such as LLM fine-tuning or Retrieval Augmented Generation (RAG). Each method\ncomes with its own trade-offs and characteristics, requiring careful design consideration. Importantly,\nchoosing the wrong method for implementing a capability can lead to oversophisticated, inefficient,\nor ineffective solutions, and it may result in systems that are unreliable, fail to meet requirements, or\nunderperform in production. Thus, selecting the right approaches for developing these capabilities is\nnot merely a matter of preference but an architectural decision that can have a profound impact on\nthe success of the application.\nIn this paper, we propose a layered approach for implementing capabilities in LLM-based applica-\ntions by mapping them to the layers and components with corresponding attributes, shown in Fig 1.\nBy aligning each capability with the relevant components at the correct layers, this approach reinforces\nessential software engineering principles, and ensures that capabilities are effectively implemented.\nModel Layer\nData\nModel Architecture\nTraining\nInference Layer\nApplication Layer\nPrompt\nEngineering\nMechanism\nEngineering\nOrchestration\nTooling\nProbability\nAmplification of\nIntended Responses\nRuntime States\nIncorporation\nSophisticated\nWorkflows\nExternal Information\nInteraction with\nEnvironment\nException Handling\nProcess and State\nManagement\nMacro-level Token\nGeneration Control\nEfficiency\nKnowledge\nBoundaries\nScale & Efficiency\nObjectives, Behavior\n& Alignment\nStructured\nOutput\nDomain\nKnowledge\nWatermarking\nHallucination\nAvoidance\nCreativity\nMicro-level Token\nGeneration Control\nLayers\nAttributes\nCapabilities\nLayer\nComponent\nAttribute\nCapability\nDependency\nWay to Implement\nFigure 1: the conceptual architecture of the approach, including Components, Layers, Attributes,\nCapabilities, and their mappings\nWe illustrate the utility of this approach with several real-world example cases, showing how\nthe right alignment between capabilities and implementation methods can lead to the appropriate\nsolutions. Through these examples, we demonstrate how developers can use this approach to make\ninformed design decisions.\nThis paper offers actionable insights into selecting the appropriate approaches for implementing\ncapabilities, empowering developers to build effective and robust solutions for LLM-based applications.\nThe remainder of this paper is organized as follows. Section 2 explains the motivating scenario\nof this study, while Section 3 presents the layered approach, its underlying principles, and details of\n2\n\neach layer. Section 4 demonstrates the approach with various example cases, followed by a review of\nthe existing literature related to this work in Section 5. Section 6 concludes the paper.\n2\nMotivating Scenarios\nIn this section, we present two motivating scenarios in which LLM-based application developers are\nlikely to encounter the challenges addressed by our approach. While established solutions may exist for\nthese scenarios, we aim to highlight the lack of a structured design process and conceptual framework.\nThese hurdles, though partially addressed in some cases, continue to create gaps in other emerging\nscenarios that require careful design considerations.\n2.1\nApplication Scenario: Integrating Additional Data\nA key scenario in developing LLM-based applications is deciding how to integrate additional data,\nsuch as domain-specific knowledge. In general, this can be achieved through two main approaches:\nintegrating the information as parameterized knowledge or integrating it as non-parameterized knowl-\nedge. Parameterized knowledge involves embedding the information directly into the model through\ntraining or fine-tuning, allowing the model to internalize the information. Non-parameterized knowl-\nedge supplies external information dynamically, typically through prompts, during inference, or post-\ngeneration, and is commonly referred to as Retrieval Augmented Generation (RAG) [15].\nWhen determining between parameterized knowledge and non-parameterized knowledge, while\nthere are various factors that need to be considered, the primary difference between these two methods\nis whether the information is parameterized. In the case of parameterized knowledge, the model learns\nthis information into its internal representations, making it readily available during inference. This\nbrings a number of characteristics, for instance:\n• Minimal latency - As the information is already learned into parameters of LLMs, using these\nknowledge does not trigger retrieval from separate sources, and does not need information to\nbe explicitly contained and processed as part of the input. This introduces minimal latency at\ninference time.\n• Internal representations of information - Information is learned into the model’s parameters,\nallowing it to form and store complex associations, relations and patterns within its internal\nrepresentation. This integration enables the model to utilize the knowledge cohesively, drawing\nconnections across tokens and adapting responses based on these embedded information.\n• Resource-intensive at training - Integrating knowledge through training or fine-tuning can be\nresource-intensive, requiring significant labour, computational and time investment upfront.\nIn contrast, non-parameterized knowledge allows for flexible information integration at the infer-\nence stage by embedding relevant external information as context within the input provided to the\nlarge language model. This approach brings its own set of characteristics, such as:\n• Flexibility - Integrating information through non-parameterized approaches allows knowledge\nto be continuously added or updated independently of specific models and to be seamlessly\ntransferred and utilized by any model without being tightly coupled to one.\n• Additional input overheads - To supply external information, non-parameterized methods pro-\nvide additional context to models within the input prompt, which consumes token space and\nmay introduce computational overheads at inference.\n• Surface utilization of information - Since the information is provided solely through prompts as\nexternal context and is not embedded within the model’s parameters, it is processed based on\n3\n\nthe model’s existing learned representations [22]. The model uses this information at a surface\nlevel, interpreting it in light of pre-existing associations.\nThese characteristics make each fitted to different sets of application scenarios of different nature.\nFor example, if the developers want to develop the multilingual capability of the system by supplying\ndata related to a new language to the model, parameterized method is the optimal choice, because\nlanguage understanding demands the model to learn the representations that capture the language’s\nsemantic features and complex linguistic structures.\nRelying solely on in-context learning cannot\nestablish the statistical relations and deep associations within the language, nor does it form robust\ninternal representations, making it unlikely to produce coherent and sensible outputs.\n2.2\nApplication Scenario: Constraining Output Structure\nAnother typical scenario is constraining the output structure of the language model. There are already\nrelatively mature solutions, such as structured output based on constrained decoding, widely offered\nby commercial LLM vendors and the open-source community.\nHowever, these solutions were not\navailable at the initial onset of demand, but instead, they went through an evolution process.\nAs LLMs gained popularity, users began to use them not only as chatbots but also to explore their\npotential as software components for executing tasks. These LLM-based components are connected\nwith other components primarily through their text-based interfaces. However, since majority of non-\nLLM-based components may not be capable of processing natural language inputs, it becomes essential\nfor LLMs to generate structured outputs that can be parsed and processed by other components.\nHowever, LLMs acquire their generalist capabilities during pre-training by learning the representations\nof vast unstructured textual knowledge from diverse sources in a probabilistic manner, which makes\nthem natively incapable to produce stable structured outputs.\nIn order to connect LLMs with other components, developers attempted the challenge from var-\nious perspectives. Initially, the most intuitive method of trial and error was leveraged. Such at-\ntempt has led to sophisticated parsers in companion with carefully crafted prompts, but a struc-\ntured output from LLMs was still not guaranteed. For instance, in the widely used LLM applica-\ntion framework LangChain, several structured output parsers, including StructuredOutputParser,\nListOutputParser and CommaSeparatedListOutputParser, were introduced1, to parse the outputs\ngenerated by the LLMs. The LLMs were expected to generate outputs based on text prompts that\nspecify the desired output formats. A while after that, a new commit2 introduced additional trial-\nand-error-based output parsers, including RetryOutputParser, RetryWithErrorOutputParser and\nOutputFixingParser, to complement the structured output parsers. Since then, sophisticated retry,\nreflection, and fixing mechanisms have been integrated into the codebase, expanding it from initial 80\nlines of code (LoC) to over a thousand LoC.\nFrom a language model training perspective, generating outputs in specific formats has long been\na traditional focus in the field. Fine-tuning has been an effective method for enforcing specific output\nformats in LLMs [23] by adapting the models with the datasets that exemplify the desired structures.\nAs the demand for structured outputs increased, fine-tuning was offered by the commercial LLM\nvendors and the open source community explicitly as a means to reliable structured outputs, such\nas in OpenAI’s GPT-3.5 Turbo fine-tuning API release. 3. However, one significant challenge in this\nscenario is retaining the generalist capabilities of LLMs from the risk of catastrophic forgetting [24].\nFurthermore, maintaining the flexibility of not requiring exhaustive fine-tuning across all possible\noutput formats makes it more complex.\n1Github Commit langchain-ai/langchain@df6c33d: https://github.com/langchain-ai/langchain/commit/df6c3\n3d4b31826650f5a04a5f9c352508bb4dd9d\n2Github Commit langchain-ai/langchain@ce5d97b: https://github.com/langchain-ai/langchain/commit/ce5d9\n7bcb3e263f6aa69da6c334e35e20bf4db11\n3OpenAI GPT-3.5 Turbo fine-tuning and API updates (accessed 1 November 2024): https://openai.com/index/g\npt-3-5-turbo-fine-tuning-and-api-updates/\n4\n\nAnother crucial stage where the output format can be enforced is the inference time. From a text\ngeneration perspective, the LLM decoding process selects a sequence of tokens based on the probability\ndistribution for each subsequent token. By interpreting the output format schema as context-free\ngrammars [25], the candidate token space can be constrained through logits manipulation, assigning\nthe lowest probabilities to tokens that would violate the specified format [26], a technique known as\nconstrained decoding. Since the probability of generated tokens is directly controlled by the formal\nrepresentation of the expected output format, this approach ensures that only outputs strictly aligned\nwith the specified format are generated. This method has been integrated into commercial models such\nas Claude 34 and GPT-4o5, and open-source libraries like HuggingFace Transformers6 and llama.cpp7.\nThe core implementation of this method used in the HuggingFace Transformers library takes around\n200 LoC8.\nAt the current stage of this evolution, LLM-based applications utilize a combination of three\ntechnologies, with inference-time constrained decoding positioned at the center for consistency and\nreliability. Training and fine-tuning, along with format-instructive prompts, assist the model in better\nunderstanding the schemas, ensuring that the outputs are both syntactically correct and semantically\nmeaningful. Additionally, the retry and fixing parsers provides an extra layer of assurance that the\noutputs are of expected formats.\n2.3\nMotivation and Scope\nFrom these two scenarios, we observe that implementing a specific capability within LLM-based ap-\nplications can lead to various technological paths, with the suitability of each technology dependent\non the specific requirements of the scenario. Identifying these technological paths may require sig-\nnificant engineering efforts, and incur time and resources cost on it. Moreover, these technological\npaths may operate at different layers. For example, in the constraining output structure scenario, fine-\ntuning targets the knowledge learned into the model parameters, while constrained decoding focuses\non the token selection process, and prompt-based approaches address non-parameterized inputs to the\nmodel. Although these different approaches can be employed to implement the same capability, they\nmay occur at distinct decoupled layers of a LLM-based application, leading to significantly different\nexpectations, outcomes, and implications for developers. Consequently, it is essential for developers\nto possess a systematic understanding of the software architecture of LLM-based applications and to\nreinforce software engineering principles to make informed architectural decisions.\nTherefore, the motivation behind this work is to propose an approach which serves to map antici-\npated capabilities to a potential technology stack for implementation, even in the absence of specific\nexisting or known solutions.\n3\nThe Approach\n3.1\nGlossary\nTo enhance clarity for the reader, we define two key terms used in this approach.\n4Claude 3 Model Family Announcement (accessed 1 November 2024): https://www.anthropic.com/news/claude-3\n-family\n5Introducing Structured Outputs in the API (accessed 1 November 2024): https://openai.com/index/introduci\nng-structured-outputs-in-the-api/\n6Text Generation Inference Guidance (accessed 1 November 2024): https://huggingface.co/docs/text-generat\nion-inference/en/conceptual/guidance\n7llama.cpp GBNF Guide (accessed 1 November 2024): https://github.com/ggerganov/llama.cpp/tree/master/g\nrammars\n8Github Commit dottxt-ai/outlines@18aaba1:https://github.com/dottxt-ai/outlines/blob/18aaba1/outlines\n/processors/structured.py\n5\n\nCapability\n“Capability” is a widely used term in LLM research, but there lacks the explicit and\nrigorous conception of the term [27]. In this study, we define it as “the ability to effectively perform\na category of tasks or exhibit a specific type of behavior,” which is consistent with current LLM\nstudies [5,27–30] as well as its philosophical and anthropological roots.\nAttribute\n“Attribute” in this approach is defined as “inherently being responsible for certain quality,\nfeature, or operation of the system,” refined from its dictionary definition of “a quality or feature\nregarded as a characteristic or inherent part of someone or something.”\n3.2\nPrinciples of the Approach\n“It is the mapping of a system’s functionality onto software structures that determines the architec-\nture’s support for qualities.”\n— Software Architecture in Practice [31]\nWe extend this principle to the software architecture for LLM-based application by mapping\ncapabilities onto the appropriate layers and components with corresponding attributes.\nLayers and components are defined following the principle of separation of concerns. We present\nthe layers, components and their attributes with additional properties in Section 3.3. Using this ar-\nchitecture, we illustrate the process of how to map desired capabilities onto the appropriate structures\nin Section 3.4.\n3.3\nLayers\nInspired by layered software architectures such as n-tier architecture [32] and layered Internet protocol\nstack [33], we break an LLM-based application into layers based on the nature of relevant development\nactivities, including Model Layer, Inference Layer, and Application Layer.\nFor each layer in the\narchitecture, we examine the nature of the components and their roles in supporting specific attributes.\nThe overall structure is shown in Fig 1.\n3.3.1\nModel Layer\nThe Model Layer involves the foundational aspects of building an LLM, from the data collection to\nthe architecture design and training process. The output of this layer is the large language model,\nwhich is a machine learning model typically built upon transformer or a variant architecture trained\nusing data on natural language processing (NLP) tasks such as causal language modeling (next-token\nprediction). Through this process, the model learns a statistical representation of language\nfrom the data, and embeds in its parameters. We divide the Model Layer into three core\ncomponents, i.e. Data, Model Architecture, and Training.\nData\nData used for training and fine-tuning LLMs goes through a process of selection, collection, and\npreprocessing, serving as the source from which LLMs learn the representations. In state-of-the-art\n(SoTA) LLMs, training data is typically collected from diverse sources, including publicly available\ninternet content, inputs from human data workers, knowledge obtained from domain experts, user\ninteractions and feedback, and distilled data from the output of LLMs [34–36].\nThe quality and scope of the training data directly determine what information is embedded within\nthe internal representation of LLMs and thus subsequently set the boundaries of model’s knowledge\nand capabilities. Beyond learning factual knowledge from the training data, LLMs also capture the\npatterns, relationships, and biases present in the training data, all of which contribute to shaping the\nmodel’s behaviors and responses. Moreover, these boundaries, behaviors, and responses are not fixed,\nas LLMs can be further adapted and refined through continual pre-training or fine-tuning, where\nadditional data is used to update the model’s internal representations.\n6\n\nModel Architecture\nThe Model Architecture defines the structure and underlying mechanisms\nof LLMs. The vast majority of LLMs are built on the transformer architecture, which leverage a\nself-attention mechanism that allows the model to capture long-range dependencies and contextual\nrelationships between tokens in a sequence [37]. This makes transformers effective for a wide range of\nNLP tasks, from translation and summarization to text generation.\nThe capability of model is largely tied to the scale of model, often measured by the number\nof parameters.\nThe scale of model is a critical factor that may lead to the emergence of model\nabilities [28]. Larger models have shown emergent abilities that are absent in smaller models, such\nas few-shot learning. These abilities are likely a result of the higher number of parameters leading to\nincreased capacity to capture complex patterns and relationships from the training data [22].\nHowever, this comes with trade-offs in terms of computational cost and resource requirements.\nLarger models demand significant computational resources at training and inference, including more\nperformant computing units and larger memory, impacting the latency and cost of the model. To\naddress these challenges, some LLMs utilize a Mixture of Experts (MoE) architecture [38,39], which\noffers a balance between the scale and efficiency. MoE models consist of multiple expert sub-networks,\nwith only a subset of experts activated for each input, reducing the computational overhead of the\ninference. In contrast, smaller models, though less capable, are faster and more efficient, making them\nmore feasible to be used for real-time applications or deployed on edge devices.\nTraining\nTraining is the process where model learns from the data and embeds what it learns into\nparameters. While the dominant approach remains next-token prediction, various training objectives\nand fine-tuning techniques have been developed to address specific needs and enhance performance\nacross different applications.\nNext-token prediction is a widely adopted training objective for LLMs, where the model learns\nto predict the next token in a sequence by estimating the probability distribution of tokens based on\nthe preceding context. This task, also known as causal language modeling, aligns well with autore-\ngressive models including GPT [40], allowing model to generate fluent and coherent text. Another\ntext generation task is masked language modeling, adopted by BERT [41], trains model to fill in the\nrandom blanks within a sequence of tokens. This bidirectional context, where the model makes use\nof both preceding and following tokens, enables its ability to understand the full sentence structure\nand semantic relationships but at the same time also makes it less fit for sequential text generation.\nIf certain information or patterns are absent in the pre-training dataset, the model can be fine-\ntuned on task-specific data to refine its knowledge base and behavior. As the demand for using LLMs\nfor specific tasks has increased [42], methods like instruction tuning, direct preference optimization\n(DPO), and reinforcement learning from human feedback (RLHF) have become popular to align\nmodels with human preferences and improve their ability to follow instructions [43–45]. These models\nare fine-tuned on crafted dataset or reward functions so that the patterns and human preferences\nare captured into parameters of models. These fune-tuning methods can also be used for specialized\npurposes, such as setting rules for LLMs [46] or enabling models to autonomously call tools [47]. The\nobjective of next-token prediction can be further adapted by combining with techniques like zero-\nshot learning, chain-of-thought (CoT) [48], Monte-Carlo tree search (MCTS) [49] and Reinforcement\nLearning.\nFor instance, Quiet-STaR [50] enhances reasoning by guiding models to “think before\nspeaking” using intermediate rationale tokens before predicting next token to improve capabilities for\nreasoning tasks.\nOpen-source LLMs as well as commercial LLMs provide the interfaces for fine-tuning. For example,\nboth OpenAI9 and Anthropic10 provide fine-tuning for their most advanced LLMs to application\ndevelopers.\nHuggingFace’s transformer library also supports both full fine-tuning and parameter\nefficient fine-tuning (PEFT) [51] over all available models. This provide application developers the\naccess to the model layer to customize the model based on their requirements.\n9Fine-tuning now available for GPT-4o: https://openai.com/index/gpt-4o-fine-tuning/\n10Fine-tune Claude 3 Haiku in Amazon Bedrock: https://www.anthropic.com/news/fine-tune-claude-3-haiku\n7\n\nSummary\nThe Model Layer is responsible for shaping the internal representations of LLMs. The\nData component defines the boundaries of the model’s knowledge and capabilities, response patterns,\nand relationships from the training data into the model’s parameters. The Model Architecture com-\nponent determines the structural design, which influence both the scale and efficiency of the model\nand subsequently the emergence of abilities. Finally, the Training component dictates model’s text\ngeneration mechanisms, refining model behavior and incorporating additional knowledge, alignment\nand capabilities through training objectives and fine-tuning techniques. These attributes with their\ncorresponding components and developer access levels are shown in Table 1.\nAttribute\nComponent\nDev\nAccess\n(Com-\nmercial)\nDev\nAccess\n(Open\nSource)\nKnowledge Boundaries\nData & Training\nFine-tuning\nFull Customization\nScale & Efficiency\nModel\nArchitec-\nture\nPre-set Options\nFull Customization\nObjectives, Behavior &\nAlignment\nData & Training\nFine-tuning\nFull Customization\nTable 1: The attributes determined by the Model Layer with corresponding components and levels of\ndeveloper access.\n3.3.2\nInference Layer\nThe Inference Layer utilizes the model’s learned representations to generate responses, a process known\nas decoding. In transformer architecture, the attention mechanism is involved to determine the most\nrelevant context when forming the probability distribution of the next token [37]. In each decoding\nstep, the model produces a probability distribution over all tokens. This distribution is generated\nby applying the softmax function to the logits derived from the model’s final hidden state, and the\nresulting probability distribution serves as the basis for selecting the next token in the sequence.\nThe key objective in inference layer is deciding how tokens are chosen from the logits. The\nna¨ıve way is to select the token with the highest probability, known as greedy sampling. While efficient,\ngreedy sampling makes the generated sequence too rigid and deterministic.\nTo make the output\ndiverse, methods like top-k [52] and top-p [53] samplings consider a subset of candidate tokens based\non their ranks or cumulative probabilities. Beam search-based decoding uses a heuristic approach of\nsampling multiple candidate sequences and select the tokens based on the overall probability of the\nsequence [54]. Applied on the logits before produce probability distribution with softmax, temperature\nscaling manipulate the logits by scaling it with a temperature value, so that lower temperatures make\nthe probability distribution sharper, leading to more deterministic outputs, while higher temperatures\nflatten the distribution, making the selection more stochastic. These strategies represent macro-level\napproaches to token selection.\nAs the inference layer controls the overall process of token generation, it can also exercise micro-\nlevel control, managing the generation in a fine-grained manner. This involves influencing not just\nthe global token selection strategies, but also making more crafted adjustments during the decoding\nprocess to steer the output. As decoding involves processing the logits, logits manipulation makes\nit possible to directly boost or suppress certain tokens, enforce specific semantic patterns across the\nsequence, or promote or eliminate the generation of particular tokens or sequences. This also enables\nconstrained decoding [26], where the probability of tokens violating predefined constraints is set to\nzero, ensuring that the generated output strictly adheres to the desired structure or format.\nIn addition to controlling the tokens generated, a primary concern in the Inference Layer is how to\ngenerate tokens efficiently. Efficient decoding focuses on minimizing latency, reducing computational\noverhead, and optimizing resource usage while maintaining the quality of generated outputs. One\n8\n\nway to achieve that is speculative decoding, which uses a smaller assistant model alongside the main\nLLM during inference [55–57]. The assistant model quickly drafts preliminary candidate tokens, which\nthe primary model can then accept or correct based on the logits from a single forward pass. Other\nmethods may involve lower level customization, including paging the K-V cache [58], efficient attention\nmechanism [59], precision reduction [60], and parallelism [61].\nOpen-source community support a wide range of Inference Layer features through various libraries,\nincluding HuggingFace Text Generation Inference11 and llama.cpp12. These tools offer developers\nextensive customization over inference settings, covering macro-level strategies, micro-level controls,\nand efficiency optimizations. In contrast, commercial LLMs typically provide limited customization\noptions for the Inference Layer. Inference efficiency is centrally managed by the vendors, which is\na reasonable encapsulation for simplified interfacing and user experience. The exception is prompt\ncaching, which incurs lower cost when using repeated prefix sequences in their queries. For macro-\nlevel control, vendors usually provide basic parameter adjustments for temperature, top-k, and top-\np sampling, but more advanced sampling methods often remain black-boxed.\nAt the micro-level,\ncustomization is primarily restricted to structured output options via format schemas, offering basic\ncontrol over output format while lacking support for more fine-grained token-level control.\nSummary\nThe Inference Layer is responsible for the text generation procedure of LLMs. Macro-\nlevel strategies steer the global configurations of output, while micro-level controls craft the details of\nthe generated sequence in fine-grained manner. The inference efficiency aspect optimizes latency of\nthe inference, through approaches such as assisted generation, KV cache operations, efficient attention\nmechanisms, precision reduction and parallelism. These perspectives with respective developer access\nlevels are shown in Table 2.\nAttribute\nDeveloper Access (Commercial)\nDeveloper Access (Open\nSource)\nMacro-level\nToken\nGeneration Control\nTemperature, Top-K & Top-P\nFull Customization\nMicro-level\nToken\nGeneration Control\nStructured Output\nFull Customization\nEfficiency\nPrompt Caching\nFull Customization\nTable 2: The attributes determined by the Inference Layer with their levels of developer access.\n3.3.3\nApplication Layer\nThe Application Layer translates LLM’s text generation power into functionalities, bridging the gap\nbetween the raw input/output text generation paradigm of LLMs and fully functional\napplications. From a conceptual viewpoint, the Application Layer centers on four key components:\nPrompt Engineering, Mechanism Engineering, Tooling, and Orchestration.\nWhile additional com-\nponents exist in LLM-based applications, such as user interface (UI) and application hosting, these\naspects closely align with traditional software development practices and are not unique to LLM-based\napplications.\nPrompt Engineering\nPrompt Engineering focuses on designing prompts that guide the LLM to\nproduce the desired outputs. The practice of Prompt Engineering is rooted in the foundations es-\ntablished by the Model Layer, where the model learns its internal representations from training data\nand is often fine-tuned to follow instructions. However, the probabilistic nature of LLMs remains\n11Text Generation Inference: https://huggingface.co/docs/text-generation-inference/\n12llama.cpp - LLM Inference in C/C++: https://github.com/ggerganov/llama.cpp\n9\n\nunchanged. The generated responses are inherently influenced by the learned statistical patterns and\nthe underlying probability distributions. The core principle of Prompt Engineering is to steer the\nmodel’s probability distribution, amplifying the likelihood of generating the intended response [62].\nOne effective strategy in Prompt Engineering is to reduce the ambiguity of the query. Vague query\ndescriptions tend to produce inconsistent outputs, as they are less likely to align with the specific pat-\nterns the model has learned during training. By providing clear and explicit instructions, the model’s\nfocus can be narrowed, guiding it toward a more concentrated area of its learned representation.\nAnother effective strategy is to supply additional guiding tokens directly within the prompt or to\ntrigger the model to generate them before providing the final response. This helps steer the probabil-\nity distribution towards the desired outcome by establishing a clearer context or guiding the model’s\nreasoning process [22]. The approach leverages patterns learned during training, where the model\nhas encountered sequences involving intermediate reasoning steps, explanations, and structured pro-\ncesses. Since the inference process considers all previously generated tokens, this method enhances the\nlikelihood of producing structurally coherent, contextually relevant, and logically consistent outputs,\nbetter aligning with specific task requirements.\nExamples of these methods include zero-shot learning methods such as role playing [63], step-\nby-step prompting [64], and explain-then-answer prompting [65], and few-shot learning methods such\nas the vanilla few-shot prompting [66] and chain-of-thought prompting [48]. Additionally, prompt\noptimization can be achieved through manual or automated tuning methods [62].\nMechanism Engineering\nMechanism Engineering [5] involves integrating LLM’s text generation\ninto modular mechanisms, that employ structured approaches that guide the module’s outputs to-\nwards achieving broader objectives or solving complex problems.\nMechanism Engineering designs\nabstract processes that leverage the model’s generative abilities in a systematic way. These mech-\nanisms typically consist of multiple individual text generation processes, each serving orthogonal or\ncomplementary objectives aimed at reaching a specific goal. Different generation processes are often\nconnected through symbolic or rule-based programs, represented using certain structures, such as\ndirected acyclic graph (DAG).\nWhile LLMs generate tokens in an autoregressive and linear manner, this generation process inher-\nently lacks explicit representations for iteration, recursion, branching, and conditionals. Mechanism\nEngineering addresses this limitation by providing an external representation that helps maintain state\nacross different stages of text generation. By integrating symbolic structures and logical controls, it\nenables the LLM to incorporate external information or runtime state, and perform sophisticated\nworkflows.\nRetrieval-augmented generation (RAG) [15] inject external knowledge into the prompt to provide\nadditional context for knowledge-intensive tasks. The mechanism has evolved by allowing LLM to\nactively query external information, such as Agentic RAG [67] and Retrieval-Interleaved Generation\n(RIG) [68].\nThe trial-and-error method involves repeatedly incorporating error information into the input to\nthe LLM, enabling it to improve its responses until the task is successfully accomplished. [5]. The\nReAct framework [69] implements an interleaved reasoning and acting mechanism, allowing the model\nto incrementally execute tasks by alternating between thought processes and actions. Reflexion [70]\nintroduces more sophisticated mechanisms that enable the model to iteratively work on a task, incor-\nporating information from both its trajectory and experience to enhance performance over time.\nSelf-consistency [71] and tree of thoughts (ToT) [72] both adopt multi-path methods to explore\nthe solution space of the task. In self-consistency, multiple paths are generated for a given prompt,\nand the final solution is determined by the voting mechanism, which selects the most consistent or\nfrequent answer among the generated responses from all paths. In contrast, Tree of Thoughts (ToT)\nemploys the LLM to evaluate intermediate steps along the path.\n10\n\nTooling\nWithout tools, LLMs are only generators of text, lacking the capacity to perform automa-\ntion tasks, validate and iteratively refine proposed solutions, or access external information. Since\nLLMs can only interact with the environment via text-based input and output, they require tools to\nbe integrated and adapted for use through natural language. The LLM itself cannot directly interact\nwith tool interfaces, so it instead relies on intermediary components that interpret the generated text\nto invoke external tools.\nIn passive tool calling, such as in vanilla Retrieval-Augmented Generation (RAG) [15], the decision\nto call a tool (i.e., retriever for the case of RAG) is made externally, not by the LLM itself. The result\nof the tool call is then supplied back to the LLM as part of the prompt context. This approach does\nnot require the LLM to be aware of the existence of tools or understand the interfaces for using them.\nIn active tool calling, the LLM is given a degree of agency, allowing it to decide when and which\ntools to invoke based on the context of the task. The model can generate structured text (e.g., JSON)\nthat follows specific conventions designed to trigger tool calls directly, or it can describe the tool\ninvocation in natural language. In scenarios where the LLM uses unstructured natural language to\nspecify the tool call, the responsibility for interpreting and executing the tool request is delegated\nto a specifically fine-tuned LLM, for instance the ToolFormer [47] and DataGemma [68]. This setup\nallows the primary LLM to maintain flexibility in how it interacts with tools while offloading the\nresponsibility of explicitly conducting tool calls to a specialized component.\nOrchestration\nThe orchestration component manages the chaining of LLMs, integrates tools into\nthe application, and maintains the states and overall process of the application.\nLLM chaining involves linking together multiple LLM calls to form a cohesive process, tackling\ntasks in a divide-and-conquer manner.\nEach call builds upon the responses from previous ones,\nallowing the system to break down complex tasks into smaller, manageable sub-tasks. This enables\nthe application to handle multi-step problems effectively.\nThe orchestration component also determines which tools are included in the application, how they\nare utilized, and how they are shared across multiple LLM calls. Different tools may have distinct\nexecution behaviors, such as asynchronous processing or requiring specific input formats, which the\norchestration component must accommodate.\nStates of the application as well as both short-term and long-term memory are maintained as part\nof orchestration. It tracks session data, user configurations, previous interactions, and the sequence\nof LLM calls, ensuring that context is preserved throughout the entire task execution. Additionally,\nthe orchestration component handles errors and exceptions, particularly when the LLM generates\nunexpected outputs. It includes mechanisms to detect issues, trigger corrective actions, or adjust the\nprocess, ensuring a smooth and consistent execution.\nSummary\nThe Application Layer is responsible for leveraging the text generation power to build-\ning functional applications. The Prompt Engineering component focuses on crafting effective prompts\nthat amplify the probability of generating desired responses. The Mechanism Engineering component\nenables sophisticated workflow operations and supports the incorporation of runtime state, allowing\nfor advanced processes beyond simple text generation. The Tooling component extend the abilities\nof LLMs by integrating external tools, enabling the model to perform automation tasks, validate\nsolutions, and access external information. Finally, the Orchestration component manages the co-\nordination of the LLM’s text generation, external tools, and user inputs, ensuring a smooth flow\nof information and the robust execution of workflows.\nThese attributes with their corresponding\ncomponents are shown in Table 3.\n3.3.4\nIntra-layer and Inter-layer Dependencies\nThe layers and components of the framework are not isolated. There are dependencies across and\nwithin layers, such that achieving the desired capability may require support from other components\n11\n\nAttribute\nComponent\nProbability\nAmplification\nof\nIntended\nRe-\nsponses\nPrompt Engineering\nRuntime States Incorporation\nMechanism Engineering\nSophisticated Workflows\nMechanism Engineering\nExternal Information\nMechanism Engineering & Tooling\nInteraction with Environment\nMechanism Engineering & Tooling\nException Handling\nMechanism Engineering & Orchestration\nProcess and State Management\nOrchestration\nTable 3: The attributes determined by the Application Layer with corresponding components.\nwithin the same layer, i.e., Intra-layer Dependencies, or from components across different layers, i.e.,\nInter-layer Dependencies.\nIntra-layer Dependencies\nIn the Model Layer, components often depend on one another to work\neffectively. For instance, certain fine-tuning methods rely heavily on the availability and format of\nspecific data. To perform instruction tuning, the data must be curated in the form of instruction-\noutput pairs. Without this structured data format, the fine-tuning process cannot align the model\neffectively tune the model to follow instructions.\nIn the Application Layer, Mechanism Engineering has dependencies on both Prompt Engineering\nand Tooling components. Mechanism Engineering often requires carefully crafted prompts to guide the\nLLM towards desired responses. Additionally, certain mechanisms rely on the integration of external\ntools to complete the task.\nInter-layer Dependencies\nThe Inference Layer often relies on components from the Model Layer\nto achieve optimal performance and efficiency.\nFor example, efficient inference methods, such as\nrunning the model in reduced precision, may depend on specific training methods like quantization-\naware training conducted in the Model Layer.\nThe Application Layer depends heavily on both the Model and Inference Layers. For instance,\nPrompt Engineering is influenced by the model architecture, particularly the scale of the model, which\ndetermines its ability to perform tasks like few-shot learning. Additionally, the presence of specific\nrepresentations learned from data is crucial, as effective prompts rely on amplifying the model’s prob-\nability distribution to guide responses toward desired outputs. The effectiveness of certain prompts or\nmechanisms, especially those relying on instruction-following, often requires instruction tuning at the\nModel Layer to align the model’s behavior. Moreover, the transferability of prompts across different\nmodels depends on characteristics of models from the Model Layer [62].\nIn Application Layer, to build tools effectively, especially when employing active and direct tool\ncalling, the support for structured output generation in the Inference Layer is beneficial, as it helps\nthe LLM produce responses in the correct format required by the tool interfaces.\n3.4\nMapping Capabilities onto Layers\nThe Capability Mapping aims at aligning specific system capabilities with the attributes and compo-\nnents across the layers of the architecture. We will use the capability of generating JSON output as\nan example to illustrate this process.\nAttributes Identification\nThe first step in implementing a capability is to identify its relevant\nattributes across the layers. As demonstrated above, different layers are responsible for distinct at-\ntributes of LLM-based applications. Each capability may correspond to one or more layers, depending\n12\n\non its characteristics and requirements. Additionally, some attributes may become depreciating in ne-\ncessity due to the presence of other attributes.\nThe capability of generating JSON output has several key attributes, shown in Table 4.\nAttribute\nLayer\nExplanation\nKnowledge Boundaries\nModel Layer\nHaving the knowledge of JSON syntax and structure\nin the representation of model is helpful for generating\ncompliant output\nObjectives, Behavior &\nAlignment\nModel Layer\nHaving model to follow explicit instructions improves\nthe ability to generate output in a specific format\nMicro-level Token Gen-\neration Control\nInference Layer\nFine-grained control at the time of token generation\ncan ensure that the output adheres to the JSON for-\nmat\nProbability\nAmplifica-\ntion\nof\nIntended\nRe-\nsponses\nApp Layer\nUsing clear and explicit prompt examples can increase\nthe likelihood of generating the response in desired\nstructure\nExternal Information\nApp Layer\nIf the model’s internal knowledge does not sufficiently\ncover JSON structure, external information can be\nsupplied to the LLM\nInteraction with Envi-\nronment\nApp Layer\nIntegrating external tools can validate the response\nformat and provide feedback for correction\nException Handling\nApp Layer\nError detection and correction mechanisms handle\ncases where LLM fail to generate desired output for-\nmat\nTable 4: Attributes related to the capability of generating JSON output. Light gray rows are identified\nas depreciating.\nUpon reviewing these attributes, we identify several depreciating instances. The knowledge about\nJSON syntax within Knowledge Boundaries (Model Layer) may reduce the necessity for External\nInformation (Application Layer) if model’s training data already includes sufficient examples of JSON\nstructures. Moreover, Micro-level Token Generation Control (Inference Layer) can effectively enforce\nJSON format, making extensive Exception Handling (Application Layer) less critical.\nIn light of\nthis, both can be implemented in lightweight manner. By resolving depreciating instances, we avoid\nredundancy and oversophistication while maintaining effectiveness.\nSolution Architecture\nBased on the identified attributes, we can design a solution architecture\nthat aligns solutions with the appropriate components in each layer. It is important to account for\nIntra-layer and Inter-layer Dependencies. A simplified description for the JSON generation example\nis shown in Table 5.\nAccess Resolution\nIn cases where certain components in the architecture are gated or restricted,\nalternative solutions need to be considered.\nIn the JSON output generation example, if the Inference Layer is gated, developers can consider\nutilizing open-source alternatives. Alternatively, developers can compensate the gated components\nby enabling fallback mechanisms, such as the exception handling previously considered depreciating,\nwhich can be enabled to validate and correct the output format with mechanisms such as trial-and-\nerror.\nHowever, developers must also consider the cost-effectiveness of implementing such sophisticated\nand depreciating workarounds, especially if there is a strong likelihood that these capabilities will be\n13\n\nLayer\nComponent\nSolution\nModel\nLayer\nData\nInclude examples of instructions and JSON documents\nduring fine-tuning to help model learn the instructions,\nsyntax and patterns\nModel Architecture\nEnsure model selected is capable of few-shot learning\nTraining\nApply instruction tuning or fine-tuning on JSON genera-\ntion tasks\nInference\nLayer\nManipulate logits to prevent the generation of format-violating tokens at\neach decoding step\nApp\nLayer\nPrompt Engineering\nEnsure the clarity of prompt, and provide few-shot exam-\nples for generation\nMechanism Engineering\nImplement lightweight exception handling mechanism\nTooling\nIntegrate JSON parser\nOrchestration\nEnsure that the workflow can detect and handle exceptions\nTable 5: The solution at each layer for implementing the capability of generating JSON output.\ncentrally provided by vendors in future updates, driven by high demand. In these cases, it is important\nfor developers to evaluate their own priorities to decide whether to implement these solutions or wait\nfor vendor support, based on their demands and resource constraints.\nEvaluation\nWhile the capability mapping provides a useful framework for guiding the implemen-\ntation, it cannot replace the need for thorough evaluation of the solutions. It is crucial to conduct\ncomprehensive testing, including ablation studies and continuous monitoring, to assess the effective-\nness of each component. By dynamically adjusting the sophistication of each component in response\nto evaluation feedback, developers can ensure both the effectiveness and efficiency of the implemen-\ntation.\n4\nUse Case Evaluation\nIn addition to the JSON output generation example, we evaluate the usefulness of the framework with\na variety of use cases.\nCreativity\nCreativity refers to the capability of generating original ideas.\nWhile LLMs cannot\ntruly generalize beyond the data they have been trained on, they can produce responses that appear\ncreative by leveraging different aspects across the layers, as demonstrated in Table 6.\nDepend on the type of creativity needed, different solutions across layers can be selected or com-\nbined for desired effect. For example, Microsoft Copilot uses the temperature parameter to control the\nlevel of creativity in responses13. In contrast, for more specialized applications like scientific research\nagents, sophisticated workflows are employed for controlled creativity in creative tasks [73]. However,\nsuch complex workflows may be overly resource-intensive for general applications, where users only\nrequire a degree of semantic variation, rather than exhaustive creative exploration.\nCall Caching\nCall Caching refers to the capability of reusing results from previous LLM calls to\nreduce computational costs and overheads. The system can solutions to achieve it, as shown in Table 7.\nLLM vendors including OpenAI and Anthropic integrate prompt caching at Inference Layer, which\nsignificantly reduces inference costs for bulk queries with same long prefix.\n13Microsoft Copilot for Technical Leaders: https://learn.microsoft.com/copilot/tutorials/learn-microsoft-c\nopilot?tutorial-step=1\n14\n\nAttribute\nLayer\nExplanation\nKnowledge Boundaries\nModel Layer\nA broad and diverse training dataset equips the LLM\nwith a vast pool of knowledge, enabling it to generate\nideas that may be previously unseen by users. The\nmodel can also create new combinations by linking\nconcepts or knowledge from different domains.\nObjectives, Behavior, &\nAlignment\nModel Layer\nLess control and alignment over human preferences\nmay lead to more stochastic and unconventional re-\nsponses.\nMacro-level Token Gen-\neration Control\nInference Layer\nDecoding strategies such as temperature adjustment\nand top-k may lead to less deterministic output.\nProbability\nAmpli-\nfication\nof\nIntended\nResponses\nApp Layer\nCrafted prompts reinforce the model to align re-\nsponses with the forms of creativity it has learned\nduring training.\nSophisticated Workflows\nApp Layer\nWorkflows and mechanisms that mimic human brain-\nstorming and idea selection process to generate sensi-\nble ideas.\nTable 6: The solution at each layer for provoking creativity.\nAttribute\nLayer\nExplanation\nEfficiency\nInference Layer\nBy storing and reusing attention states for repeated long\nprefix sequences, the system can skip recomputing these\nprefix sequences.\nProcess and State\nManagement\nApp Layer\nHash-based lookup for storing and retrieving previous input\nand output pairs can avoid the same query being recom-\nputed.\nTable 7: The solution at each layer for call caching.\nLangChain supports caching for tool calls in Application Layer, which is beneficial when LLMs are\nwrapped behind these tools, as it prevents redundant computations. Similarly, AIGNE14 provides an\nabstraction where components, including LLMs, are treated as blocklets, which are callable by users\nor other blocklets. Caching interactions between blocklets helps avoid repetitive nested invocations.\nThese Application Layer caching mechanisms may not be effective in scenarios where the prompts\nvary across requests, but they can be useful in cases involving high-computational components like\nLLMs, where the same request is repeated multiple times within a period. In such cases, caching can\nminimize cost and overhead.\nLong Context\nChatGPT was only accepting 4096 tokens at the initial released and was insufficient\nfor advanced usage such as codebase analysis. A larger context window enables the model to capture\nrich context or handle long documents. The solution at each layer is shown in Table 8.\nWhile the Model Layer solutions are native and robust for implementation, it may be unavailable\nfor commercial models if long context is not supported. Solutions at the Application Layer, such as\nprompt compression, offer a flexible workaround, but they typically result in higher inference costs,\nand may be less robust and more prone to information loss.\n14AIGNE: https://www.aigne.io/\n15\n\nAttribute\nLayer\nExplanation\nScale\n&\nEffi-\nciency\nModel Layer\nEmploy specialized attention mechanisms like Efficient Atten-\ntion [74] and extended positional encodings such as RoPE [75]\nto handle long input sequences.\nSophisticated\nWorkflows\nApp Layer\nShortening the prompt through prompt compression mecha-\nnisms at Application Layer such as LLMLingua [76] allow more\ncontent to fit within the model’s context window.\nTable 8: The solution at each layer for long context.\n5\nRelated Work\nSeveral studies have examined the design and architectural considerations of LLM-based systems.\nZhou et al. [77] present a decision model for foundation model-based agents, analyzing trade-offs\nbetween various options for each type of architectural element.\nLu et al. [78] propose a layered\nreference architecture for foundation model-based systems, emphasizing a pattern-oriented approach\nthat focuses on component orchestration. Liu et al. [79] compile a comprehensive design patterns\ncatalogue for foundation model-based agents.\nIn specific areas, Shamsujjoha et al. [80] provide a taxonomy focused on runtime guardrails for\nLLM agents, exploring a set of guardrail options at each level of system. Wang et al. [5] introduce\nthe concept of Mechanism Engineering, surveying relevant methods from the perspective of design\nmechanisms and applications of autonomous agent. Liu et al. [62] offer an extensive survey on various\nprompting strategies for LLMs as well as their relevant enabling methods required during pre-training\nand inference. Welleck et al. [81] explore inference-time algorithms, highlighting various objectives\nof decoding strategies. Zhang et al. [43] provide a detailed survey on instruction tuning, discussing\naspects including dataset preparation, tuning methods, and efficient tuning techniques.\nWhile these works provide valuable insights into specific aspects, problems, or a particular layer\nof LLM-based applications, they address isolated components without a unified perspective. In con-\ntrast, our work fills this gap by offering a holistic layered approach that systematically examines the\ncharacteristics of each layer within LLM-based applications and guide the development of capabilities\nacross these layers.\n6\nConclusion\nThis paper presents a layered approach for implementing capabilities in LLM-based applications,\ndecoupling the system into three distinct layers: Model Layer, Inference Layer, and Application Layer.\nEach layer is assigned specific attributes based on the characteristics of layers and their components.\nBy aligning capabilities with relevant attributes across layers, this approach enables developers to\nsystematically identify implementation strategies.\nThe proposed framework was evaluated against several typical use cases, demonstrating its use-\nfulness and feasibility in guiding the design of LLM-based applications. Our work bridges the gap\nbetween high-level capability requirements and the underlying architectural components, offering a\nholistic strategy accommodating the development of capabilities in LLM-based applications.\nReferences\n[1] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, “Code-\ngen: An open large language model for code with multi-turn program synthesis,” arXiv preprint\narXiv:2203.13474, 2022.\n16\n\n[2] X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo, D. Lo, J. Grundy, and H. Wang, “Large\nlanguage models for software engineering: A systematic literature review,” ACM Transactions\non Software Engineering and Methodology, 2023.\n[3] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,\nQ. Le et al., “Program synthesis with large language models,” arXiv preprint arXiv:2108.07732,\n2021.\n[4] G. Kim, P. Baldi, and S. McAleer, “Language models can solve computer tasks,” Advances in\nNeural Information Processing Systems, vol. 36, 2024.\n[5] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin et al.,\n“A survey on large language model based autonomous agents,” Frontiers of Computer Science,\nvol. 18, no. 6, p. 186345, 2024.\n[6] Y. Nie, Y. Kong, X. Dong, J. M. Mulvey, H. V. Poor, Q. Wen, and S. Zohren, “A survey of large\nlanguage models for financial applications: Progress, prospects and challenges,” arXiv preprint\narXiv:2406.11903, 2024.\n[7] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, and\nG. Mann, “Bloomberggpt: A large language model for finance,” arXiv preprint arXiv:2303.17564,\n2023.\n[8] X. Wang, N. Anwer, Y. Dai, and A. Liu, “Chatgpt for design, manufacturing, and education,”\nProcedia CIRP, vol. 119, pp. 7–14, 2023.\n[9] H. Fan, X. Liu, J. Y. H. Fuh, W. F. Lu, and B. Li, “Embodied intelligence in manufactur-\ning: leveraging large language models for autonomous industrial robotics,” Journal of Intelligent\nManufacturing, pp. 1–17, 2024.\n[10] Y. Dai, A. Liu, and C. P. Lim, “Reconceptualizing chatgpt and generative ai as a student-driven\ninnovation in higher education,” Procedia CIRP, vol. 119, pp. 84–90, 2023.\n[11] S. Sarsa, P. Denny, A. Hellas, and J. Leinonen, “Automatic generation of programming exercises\nand code explanations using large language models,” in Proceedings of the 2022 ACM Conference\non International Computing Education Research-Volume 1, 2022, pp. 27–43.\n[12] D. A. Boiko, R. MacKnight, B. Kline, and G. Gomes, “Autonomous chemical research with large\nlanguage models,” Nature, vol. 624, no. 7992, pp. 570–578, 2023.\n[13] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting,\n“Large language models in medicine,” Nature medicine, vol. 29, no. 8, pp. 1930–1940, 2023.\n[14] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P. Kumar, E. Dupont, F. J. Ruiz,\nJ. S. Ellenberg, P. Wang, O. Fawzi et al., “Mathematical discoveries from program search with\nlarge language models,” Nature, vol. 625, no. 7995, pp. 468–475, 2024.\n[15] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K¨uttler, M. Lewis, W.-t.\nYih, T. Rockt¨aschel et al., “Retrieval-augmented generation for knowledge-intensive nlp tasks,”\nAdvances in Neural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.\n[16] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen et al.,\n“Siren’s song in the ai ocean: a survey on hallucination in large language models,” arXiv preprint\narXiv:2309.01219, 2023.\n[17] A. Radford, “Improving language understanding by generative pre-training,” 2018.\n17\n\n[18] L. Zhou, W. Schellaert, F. Mart´ınez-Plumed, Y. Moros-Daval, C. Ferri, and J. Hern´andez-Orallo,\n“Larger and more instructable language models become less reliable,” Nature, pp. 1–8, 2024.\n[19] J. Yang, C. E. Jimenez, A. Wettig, K. Lieret, S. Yao, K. Narasimhan, and O. Press, “Swe-\nagent:\nAgent-computer interfaces enable automated software engineering,” arXiv preprint\narXiv:2405.15793, 2024.\n[20] X. Li, S. Chan, X. Zhu, Y. Pei, Z. Ma, X. Liu, and S. Shah, “Are chatgpt and gpt-4 general-\npurpose solvers for financial text analytics? a study on several typical tasks,” in Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track,\n2023, pp. 408–422.\n[21] X. Tang, Y. Zong, J. Phang, Y. Zhao, W. Zhou, A. Cohan, and M. Gerstein, “Struc-bench:\nAre large language models really good at generating complex structured data?” arXiv preprint\narXiv:2309.08963, 2023.\n[22] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma, “An explanation of in-context learning as\nimplicit bayesian inference,” arXiv preprint arXiv:2111.02080, 2021.\n[23] A. Dunn, J. Dagdelen, N. Walker, S. Lee, A. S. Rosen, G. Ceder, K. Persson, and A. Jain,\n“Structured information extraction from complex scientific text with fine-tuned large language\nmodels,” arXiv preprint arXiv:2212.05238, 2022.\n[24] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,\nJ. Quan, T. Ramalho, A. Grabska-Barwinska et al., “Overcoming catastrophic forgetting in neural\nnetworks,” Proceedings of the national academy of sciences, vol. 114, no. 13, pp. 3521–3526, 2017.\n[25] R. J. Parikh, “On context-free languages,” Journal of the ACM (JACM), vol. 13, no. 4, pp.\n570–581, 1966.\n[26] C. Hokamp and Q. Liu, “Lexically constrained decoding for sequence generation using grid beam\nsearch,” arXiv preprint arXiv:1704.07138, 2017.\n[27] U. Anwar, A. Saparov, J. Rando, D. Paleka, M. Turpin, P. Hase, E. S. Lubana, E. Jenner,\nS. Casper, O. Sourbut, B. L. Edelman, Z. Zhang, M. G¨unther, A. Korinek, J. Hernandez-Orallo,\nL. Hammond, E. J. Bigelow, A. Pan, L. Langosco, T. Korbak, H. C. Zhang, R. Zhong,\nS. O. hEigeartaigh, G. Recchia, G. Corsi, A. Chan, M. Anderljung, L. Edwards, A. Petrov,\nC. S. de Witt, S. R. Motwani, Y. Bengio, D. Chen, P. Torr, S. Albanie, T. Maharaj,\nJ. N. Foerster, F. Tram`er, H. He, A. Kasirzadeh, Y. Choi, and D. Krueger, “Foundational\nchallenges in assuring alignment and safety of large language models,” Transactions on\nMachine Learning Research, 2024, survey Certification, Expert Certification. [Online]. Available:\nhttps://openreview.net/forum?id=oVTkOs8Pka\n[28] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma,\nD. Zhou, D. Metzler et al., “Emergent abilities of large language models,” Transactions on Ma-\nchine Learning Research, 2022.\n[29] T. Shevlane, S. Farquhar, B. Garfinkel, M. Phuong, J. Whittlestone, J. Leung, D. Kokotajlo,\nN. Marchal, M. Anderljung, N. Kolt et al., “Model evaluation for extreme risks,” arXiv preprint\narXiv:2305.15324, 2023.\n[30] P. Zhou, A. Madaan, S. P. Potharaju, A. Gupta, K. R. McKee, A. Holtzman, J. Pujara, X. Ren,\nS. Mishra, A. Nematzadeh et al., “How far are large language models from agents with theory-\nof-mind?” arXiv preprint arXiv:2310.03051, 2023.\n[31] L. Bass, P. Clements, and R. Kazman, Software Architecture in Practice.\nAddison-Wesley, 2012.\n18\n\n[32] M. Richards, Software architecture patterns.\nO’Reilly Media, Incorporated 1005 Gravenstein\nHighway North, Sebastopol, CA . . . , 2015, vol. 4.\n[33] C. J. Kale and T. J. Socolofsky, “TCP/IP tutorial,” RFC 1180, Jan. 1991. [Online]. Available:\nhttps://www.rfc-editor.org/info/rfc1180\n[34] M. Khan and A. Hanna, “The subjects and stages of ai dataset development: A framework for\ndataset accountability,” Ohio St. Tech. LJ, vol. 19, p. 171, 2022.\n[35] S. Biswas, M. Wardat, and H. Rajan, “The art and practice of data science pipelines: A compre-\nhensive study of data science pipelines in theory, in-the-small, and in-the-large,” in Proceedings\nof the 44th International Conference on Software Engineering, 2022, pp. 2091–2103.\n[36] C.-Y. Hsieh, C.-L. Li, C.-k. Yeh, H. Nakhost, Y. Fujii, A. Ratner, R. Krishna, C.-Y. Lee, and\nT. Pfister, “Distilling step-by-step! outperforming larger language models with less training data\nand smaller model sizes,” in Findings of the Association for Computational Linguistics: ACL\n2023, 2023, pp. 8003–8017.\n[37] A. Vaswani, “Attention is all you need,” Advances in Neural Information Processing Systems,\n2017.\n[38] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, “Outrageously\nlarge neural networks: The sparsely-gated mixture-of-experts layer,” in International Conference\non Learning Representations, 2016.\n[39] Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Zhao, A. M. Dai, Q. V. Le, J. Laudon et al.,\n“Mixture-of-experts with expert choice routing,” Advances in Neural Information Processing\nSystems, vol. 35, pp. 7103–7114, 2022.\n[40] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding\nby generative pre-training.”\n[41] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional\ntransformers for language understanding,” in Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 2019, pp. 4171–4186.\n[42] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., “Language models are\nunsupervised multitask learners,” OpenAI blog, vol. 1, no. 8, p. 9, 2019.\n[43] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu, T. Zhang, F. Wu et al.,\n“Instruction tuning for large language models: A survey,” arXiv preprint arXiv:2308.10792, 2023.\n[44] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, “Direct preference\noptimization: Your language model is secretly a reward model,” Advances in Neural Information\nProcessing Systems, vol. 36, 2024.\n[45] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray et al., “Training language models to follow instructions with human feedback,”\nAdvances in neural information processing systems, vol. 35, pp. 27 730–27 744, 2022.\n[46] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-\nseini, C. McKinnon et al., “Constitutional ai: Harmlessness from ai feedback,” arXiv preprint\narXiv:2212.08073, 2022.\n[47] T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Can-\ncedda, and T. Scialom, “Toolformer: Language models can teach themselves to use tools,” Ad-\nvances in Neural Information Processing Systems, vol. 36, 2024.\n19\n\n[48] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., “Chain-of-\nthought prompting elicits reasoning in large language models,” Advances in neural information\nprocessing systems, vol. 35, pp. 24 824–24 837, 2022.\n[49] J.-B. Grill, F. Altch´e, Y. Tang, T. Hubert, M. Valko, I. Antonoglou, and R. Munos, “Monte-carlo\ntree search as regularized policy optimization,” in International Conference on Machine Learning.\nPMLR, 2020, pp. 3769–3778.\n[50] E. Zelikman, G. R. Harik, Y. Shao, V. Jayasiri, N. Haber, and N. Goodman, “Quiet-STar:\nLanguage models can teach themselves to think before speaking,” in First Conference on\nLanguage Modeling, 2024. [Online]. Available: https://openreview.net/forum?id=oRXPiSOGH9\n[51] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “Lora:\nLow-rank adaptation of large language models,” arXiv preprint arXiv:2106.09685, 2021.\n[52] A. Fan, M. Lewis, and Y. Dauphin, “Hierarchical neural story generation,” in Proceedings of the\n56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\n2018, pp. 889–898.\n[53] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, “The curious case of neural text degen-\neration,” in International Conference on Learning Representations, 2019.\n[54] M. Freitag and Y. Al-Onaizan, “Beam search strategies for neural machine translation,” ACL\n2017, p. 56, 2017.\n[55] Y. Leviathan, M. Kalman, and Y. Matias, “Fast inference from transformers via speculative\ndecoding,” in International Conference on Machine Learning.\nPMLR, 2023, pp. 19 274–19 286.\n[56] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, and J. Jumper, “Accelerating large\nlanguage model decoding with speculative sampling,” arXiv preprint arXiv:2302.01318, 2023.\n[57] Y. Li, F. Wei, C. Zhang, and H. Zhang, “Eagle: Speculative sampling requires rethinking feature\nuncertainty,” arXiv preprint arXiv:2401.15077, 2024.\n[58] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Sto-\nica, “Efficient memory management for large language model serving with pagedattention,” in\nProceedings of the 29th Symposium on Operating Systems Principles, 2023, pp. 611–626.\n[59] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R´e, “Flashattention: Fast and memory-efficient exact\nattention with io-awareness,” Advances in Neural Information Processing Systems, vol. 35, pp.\n16 344–16 359, 2022.\n[60] T. Kumar, Z. Ankner, B. F. Spector, B. Bordelon, N. Muennighoff, M. Paul, C. Pehlevan, C. R´e,\nand A. Raghunathan, “Scaling laws for precision,” arXiv preprint arXiv:2411.04330, 2024.\n[61] R. Y. Aminabadi, S. Rajbhandari, A. A. Awan, C. Li, D. Li, E. Zheng, O. Ruwase, S. Smith,\nM. Zhang, J. Rasley et al., “Deepspeed-inference: enabling efficient inference of transformer mod-\nels at unprecedented scale,” in SC22: International Conference for High Performance Computing,\nNetworking, Storage and Analysis.\nIEEE, 2022, pp. 1–15.\n[62] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing,” ACM Computing\nSurveys, vol. 55, no. 9, pp. 1–35, 2023.\n[63] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar, J. Spencer-Smith, and\nD. C. Schmidt, “A prompt pattern catalog to enhance prompt engineering with chatgpt,” arXiv\npreprint arXiv:2302.11382, 2023.\n20\n\n[64] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large language models are zero-shot\nreasoners,” Advances in neural information processing systems, vol. 35, pp. 22 199–22 213, 2022.\n[65] V. Shwartz, P. West, R. Le Bras, C. Bhagavatula, and Y. Choi, “Unsupervised commonsense\nquestion answering with self-talk,” in Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), 2020, pp. 4615–4629.\n[66] T. B. Brown, “Language models are few-shot learners,” arXiv preprint arXiv:2005.14165, 2020.\n[67] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang, J. Callan, and G. Neubig,\n“Active retrieval augmented generation,” arXiv preprint arXiv:2305.06983, 2023.\n[68] P. Radhakrishnan, J. Chen, B. Xu, P. Ramaswami, H. Pho, A. Olmos, J. Manyika, and\nR. Guha, “Knowing when to ask–bridging large language models and data,” arXiv preprint\narXiv:2409.13741, 2024.\n[69] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao, “React: Synergizing\nreasoning and acting in language models,” in The Eleventh International Conference on Learning\nRepresentations, 2023.\n[70] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Reflexion: Language agents\nwith verbal reinforcement learning,” Advances in Neural Information Processing Systems, vol. 36,\n2024.\n[71] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou,\n“Self-consistency improves chain of thought reasoning in language models,” arXiv preprint\narXiv:2203.11171, 2022.\n[72] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan, “Tree of thoughts:\nDeliberate problem solving with large language models,” Advances in Neural Information Pro-\ncessing Systems, vol. 36, 2024.\n[73] Q. Huang, J. Vora, P. Liang, and J. Leskovec, “Benchmarking large language models as ai research\nagents,” in NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.\n[74] Z. Shen, M. Zhang, H. Zhao, S. Yi, and H. Li, “Efficient attention: Attention with linear com-\nplexities,” in Proceedings of the IEEE/CVF winter conference on applications of computer vision,\n2021, pp. 3531–3539.\n[75] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, “Roformer: Enhanced transformer with\nrotary position embedding,” Neurocomputing, vol. 568, p. 127063, 2024.\n[76] H. Jiang, Q. Wu, C.-Y. Lin, Y. Yang, and L. Qiu, “Llmlingua: Compressing prompts for accel-\nerated inference of large language models,” in Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, 2023, pp. 13 358–13 376.\n[77] J. Zhou, Q. Lu, J. Chen, L. Zhu, X. Xu, Z. Xing, and S. Harrer, “A taxonomy of architec-\nture options for foundation model-based agents: Analysis and decision model,” arXiv preprint\narXiv:2408.02920, 2024.\n[78] Q. Lu, L. Zhu, X. Xu, Z. Xing, and J. Whittle, “Towards responsible ai in the era of generative ai:\nA reference architecture for designing foundation model based systems,” IEEE Software, 2024.\n[79] Y. Liu, S. K. Lo, Q. Lu, L. Zhu, D. Zhao, X. Xu, S. Harrer, and J. Whittle, “Agent design\npattern catalogue: A collection of architectural patterns for foundation model based agents,”\narXiv preprint arXiv:2405.10467, 2024.\n21\n\n[80] M. Shamsujjoha, Q. Lu, D. Zhao, and L. Zhu, “A taxonomy of multi-layered runtime guardrails\nfor designing foundation model-based agents: Swiss cheese model for ai safety by design,” arXiv\npreprint arXiv:2408.02205, 2024.\n[81] S. Welleck, A. Bertsch, M. Finlayson, H. Schoelkopf, A. Xie, G. Neubig, I. Kulikov, and Z. Har-\nchaoui, “From decoding to meta-generation: Inference-time algorithms for large language mod-\nels,” arXiv preprint arXiv:2406.16838, 2024.\n22",
    "pdf_filename": "A_Layered_Architecture_for_Developing_and_Enhancing_Capabilities_in_Large_Language_Model-based_Softw.pdf"
}