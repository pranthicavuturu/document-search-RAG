{
    "title": "Can EDA Tool Feedback Improve Verilog Generation by LLMs",
    "abstract": "servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. Manuscript submitted to ACM Manuscript submitted to ACM 1 arXiv:2411.11856v1  [cs.AR]  1 Nov 2024",
    "body": "Can EDA Tool Feedback Improve Verilog Generation by LLMs?\nJASON BLOCKLOVE, New York University, USA\nSHAILJA THAKUR, New York University, USA\nBENJAMIN TAN, Universary of Calgary, Canada\nHAMMOND PEARCE, University of New South Wales, Australia\nSIDDHARTH GARG, New York University, USA\nRAMESH KARRI, New York University, USA\nTraditionally, digital hardware designs are written in the Verilog hardware description language (HDL) and debugged manually by\nengineers. This can be time-consuming and error-prone for complex designs. Large Language Models (LLMs) are emerging as a\npotential tool to help generate fully functioning HDL code, but most works have focused on generation in the single-shot capacity: i.e.,\nrun and evaluate, a process that does not leverage debugging and as such does not adequately reflect a realistic development process.\nIn this work we evaluate the ability of LLMs to leverage feedback from electronic design automation (EDA) tools to fix mistakes\nin their own generated Verilog. To accomplish this we present an open-source, highly customizable framework, AutoChip, which\ncombines conversational LLMs with the output from Verilog compilers and simulations to iteratively generate and repair Verilog. To\ndetermine the success of these LLMs we leverage the VerilogEval benchmark set. We evaluate four state-of-the-art conversational\nLLMs, focusing on readily accessible commercial models.\nEDA tool feedback proved to be consistently more effective than zero-shot prompting only with GPT-4o, the most computationally\ncomplex model we evaluated. In the best case we observed a 5.8% increase in the number of successful designs with a 34.2% decrease\nin cost over the best zero-shot results. Mixing smaller models with this larger model at the end of the feedback iterations resulted in\nequally as much success as with GPT-4o using feedback, but for an additional 41.9% less cost (overall decrease in cost over zero-shot of\n89.6%).\nCCS Concepts: ‚Ä¢ Hardware ‚ÜíHardware description languages and compilation; Software tools for EDA; Hardware description\nlanguages and compilation; ‚Ä¢ Computing methodologies ‚ÜíMachine translation.\nAdditional Key Words and Phrases: Verilog, Large Language Models, Automation\nACM Reference Format:\nJason Blocklove, Shailja Thakur, Benjamin Tan, Hammond Pearce, Siddharth Garg, and Ramesh Karri. 2024. Can EDA Tool Feedback\nImprove Verilog Generation by LLMs?. 1, 1 (November 2024), 24 pages. https://doi.org/XXXXXXX.XXXXXXX\nAuthors‚Äô Contact Information: Jason Blocklove, jason.blocklove@nyu.edu, New York University, New York, New York, USA; Shailja Thakur, st4920@\nnyu.edu, New York University, New York, New York, USA; Benjamin Tan, benjamin.tan1@ucalgary.ca, Universary of Calgary, Calgary, Alberta,\nCanada; Hammond Pearce, hammond.pearce@unsw.edu.au, University of New South Wales, Sydney, New South Wales, Australia; Siddharth Garg,\nsiddharth.garg@nyu.edu, New York University, New York, New York, USA; Ramesh Karri, rkarri@nyu.edu, New York University, New York, New York,\nUSA.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\nof this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on\nservers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nManuscript submitted to ACM\nManuscript submitted to ACM\n1\narXiv:2411.11856v1  [cs.AR]  1 Nov 2024\n\n2\nBlocklove et al.\nFig. 1. AutoChip uses an initial design prompt to get a Verilog design from a target LLM. Multiple (ùëò) candidate responses can be\ngenerated per-prompt, which are then each evaluated and ranked using the feedback from HDL compilers and testbench simulations\nto identify mismatches compared to a reference design. The best of these responses (passing the most tests) then has its tool/testbench\nfeedback passed to the LLM to generate improved responses as a greedy tree search. This is done up to a tree depth of ùëë.\n1\nIntroduction\nDesigning digital hardware with a hardware description language (HDL), such as Verilog or VHDL, is a niche skill and\npart of a demanding process requiring substantial expertise. Any mishaps can lead to implementations fraught with\nbugs and errors [9], and with growing demand for digital systems, there is a growing demand for techniques that can\nassist in generating quality HDL. High-level synthesis (HLS) tools, for instance, are able to transform designs written in\nhigh-level software languages like C to target HDLs and implement digital hardware.\nRecent efforts have shifted the abstraction level higher, leveraging state-of-the-art Large Language Models (LLMs) [32]\nto translate natural language to Verilog. DAVE [25] and VeriGen [30] were the first efforts seeking to fine-tune LLMs\nspecifically to generate Verilog. However, VeriGen and its ilk were investigated for their use in a zero-shot manner,\ni.e., they output code in response to a prompt. However, designing hardware in the real-world does not work this\nway‚Äîcode is rarely correct on the first try. Instead, hardware designers iterate over their designs, using feedback from\nsimulation and synthesis tools to identify and fix bugs so that an implementation will meet design specifications. This\nfeedback-based approach is not well reflected in existing code-generation LLMs. Recent work [3] has proposed an\niterative and interactive conversational (or chat-based) approach for Verilog code generation, more closely mimicking\nthe design flow of a human hardware engineer. In this case, though, feedback comes entirely from a human developer\nwho inspects the code, identifies bugs, and provides detailed feedback to the LLM. Such an approach still places\nconsiderable demands on a human developer‚Äôs time, and is more analogous to two hardware designers examining their\ndesigns, rather than using electronic design automation (EDA) tools to directly analyze for correctness and find bugs.\nWe therefore ask: Can further automation reduce the burden on the designer?\nManuscript submitted to ACM\n\nCan EDA Tool Feedback Improve Verilog Generation by LLMs?\n3\nTo help answer this question, we have developed and refined a framework for automating the hardware design\nprocess using LLMs, called AutoChip. While originally a simple iterative loop [31], as we discuss further in Section 3,\nwe observed that the initial code generated from a prompt had a significant impact on the trajectory of the design flow\nand the eventual success or failure of the LLM-generated design. As such, we developed a more expansive tree search\nmethodology (Figure 1), enabling us to more completely evaluate the ability of LLMs to use tool-based feedback to\nrepair their own HDL in the same manner as a hardware engineer. Starting with a design prompt, AutoChip creates and\nevaluates ùëòcandidate solutions and then enhances the most successful design by identifying and rectifying compilation\nerrors and functional bugs over repeated interactions with an LLM. Each candidate design is analyzed for compilation\nerrors/warnings and/or incorrect test cases from a testbench. These messages are used to rank the candidate solutions,\nand we return feedback from the tools and testbenches for the best candidate with a prompt to the LLM to refine its\nimplementation and generate ùëòmore candidates. This process is followed until all tests pass for a candidate response\nor a tree depth of ùëëiterations is reached, at which point the most successful candidate design from the tree search is\nreturned.\nAutoChip was evaluated with two feedback modes: ‚Äòfull context‚Äô keeps appending prompts and responses to the\n‚Äòconversation‚Äô with the LLM; and ‚Äòsuccinct‚Äô prompts only with feedback from the most recent iteration of the framework\nto try ensure that the process ‚Äòfits‚Äô within the context windows of LLMs.\nIn this manuscript, we leverage our more robust AutoChip design to examine six research questions:\n‚Ä¢ RQ1: Does feedback from hardware verification tools improve LLM-generated HDL over zero-shot results?\n‚Ä¢ RQ2: Does the number of iterations and candidate responses impact quality and number of correct implementations?\n‚Ä¢ RQ3: What is the impact of tool feedback-driven code generation on cost?\n‚Ä¢ RQ4: Does the amount of context given with feedback have an impact on the rate of successful designs?\n‚Ä¢ RQ5: Are there certain classes of hardware design problem which LLMs are more well-equipped to solve than others?\n‚Ä¢ RQ6: Can mixing multiple LLMs with different capabilities during a design ‚Äúrun‚Äù improve generation quality at\nreduced cost?\nOur findings for these questions provide useful insights into how current LLMs can be leveraged to enable a fully\nautomated chip design process, given a natural language design specification and resulting in a completed chip design\nready for tapeout.\nModel Evaluation\nWe assess AutoChip‚Äôs feedback-based strategies using the VerilogEval [16] set of benchmarks, which use problems\nand testbenches from HDLBits [34]. These benchmarks have been used in several works in the field to evaluate the\ncapabilities of different LLMs, and provide an initial point of comparison for our AutoChip-derived results.\nWe focus our approaches on commercial LLMs due to their relative availability/accessibility and published perfor-\nmance. Our analysis covers the quality of Verilog code generated relative to computational effort and the success rate\nfor different types of circuit.\nOur results are evaluated from the perspective of both general computational complexity, by analyzing the LLM\ntoken cost, and real-world design cost, by analyzing the USD cost of accessing the evaluated models.\nContributions\nOur key contributions are:\nManuscript submitted to ACM\n\n4\nBlocklove et al.\n‚Ä¢ An open-source framework for evaluating how LLMs can automatically generate hardware, AutoChip, which can be\nexpanded with additional LLMs and configured to use feedback and mix models as needed, and an accompanying\ndataset for evaluating the hardware design and repair capabilities of different large language models (open source\navailable at https://zenodo.org/records/13864552).\n‚Ä¢ Comparison of feedback prompting strategies‚Äîsuccinct vs. full context‚Äîto improve token costs and accuracy.\n‚Ä¢ Comparison of AutoChip feedback strategies using state-of-the-art LLMs‚ÄîGPT-4o,-4o-Mini,-3.5-Turbo, and Claude 3\nHaiku, vs. baseline ‚Äúzero-shot\" Verilog generated by them and other works reported.\n‚Ä¢ Evaluation of leveraging mixed-models with feedback for improving HDL generation at a reduced cost.\n2\nBackground and Prior Work\nLLMs are machine learning (ML) models built with transformers and are trained in a self-supervised manner on vast\nlanguage data sets. LLMs operate by ingesting tokens (character sequences, of approximately 4 characters in OpenAI‚Äôs\nGPT series) and predicting the most probable subsequent token. The most powerful LLMs, e.g., ChatGPT [21], Bard [26],\nand Code Llama [12], boast hundreds of billions of parameters [5, 7] and generalize to a broad range of tasks. Their\naccuracy is boosted via instruction tuning and reinforcement learning with human feedback [24], allowing the LLMs to\nmore effectively understand and respond to user intentions. Prior work specialized LLMs for code generation. GitHub\nCopilot [11] was an early LLM-based code completion engine.\nLLMs for code generation were developed in auto-completion and conversational modes. DAVE [25] was the first\nLLM (finetuned GPT-2) for Verilog generation. VeriGen [30] improved upon this work by expanding on the size of the\nmodel and size of the data sets. Chip-Chat [3] evaluated ChatGPT-4 to work with a hardware designer to generate a\nprocessor and the first fully AI-generated tapeout. RTLCoder [17] is another lightweight model for generating Verilog.\nTo evaluate model performance, a variety of benchmarks have been presented alongside further LLM developments,\nsuch as VerilogEval [16] which evaluates LLMs‚Äô abilities to write Verilog on benchmarks from HDLBits. Similarly,\nRTLLM [18] provides a further set of benchmarks. Other works have examined LLMs for hardware in tasks such as\nhardware bug repair [1] and generating SystemVerilog assertions [14].\nLLMs have also been applied to high-level synthesis. For example, C2HLSC [8] examined how LLMs can be used to\ntranslate general C into the subset of C which is synthesizable. For a larger case study, GPT4AIGChip [10] explored\nhow AI accelerators expressed in HLS could be designed using a GPT-4 based framework.\nCommercial hardware-focused LLMs have been released, with benefits and drawbacks ‚Äì RapidGPT [28], Cadence\nJedAI [6], Nvidia ChipNeMo [15], and Synopsys.ai Copilot [29]. Tool uses range from helping write verilog to answering\nquestions about EDA tool use. ChatEDA [13] use LLMs for automating tooling. A fair comparison is difficult due to the\ndifferent LLMs, methods, benchmarks, and limited availability.\n3\nAutoChip Design Framework\nFigure 1 illustrates AutoChip‚Äôs functional design. The input to AutoChip is a natural language description of the desired\nfunctionality with a Verilog module definition (i.e. the I/O) and an accompanying testbench. In our evaluations we\nleverage the VerilogEval [16] dataset for the source of these descriptions and testbenches, though AutoChip is not\nrestricted to these benchmarks. The design prompt and an overarching system prompt are passed to a conversational\nLLM capable of generating Verilog code. The LLM generates several candidate solutions for the Verilog module, which\nare compiled and simulated if possible, and then ranked based on their success. Should the response not contain a\nVerilog module it is given a rank of ‚àí2, if it fails to compile it is given a rank of ‚àí1, if the compilation has warnings it is\nManuscript submitted to ACM\n\nCan EDA Tool Feedback Improve Verilog Generation by LLMs?\n5\n1 {\n2 \"general \": {\n3\n\"prompt \": \"./ design_prompt.sv\",\n4\n\"name\": \"top_module\",\n5\n\"testbench \": \"./ testbench.sv\",\n6\n\"model_family \": \"ChatGPT\",\n7\n\"model_id \": \"gpt -4o-mini\",\n8\n\"num_candidates \": 5,\n9\n\"iterations \": 5,\n10\n\"outdir \": \"output_dir\",\n11\n\"log\": \"log.txt\",\n12\n\"mixed -model\": false\n13 },\n14\n15 \"mixed -model\": {\n16\n\"model1 \": {\n17\n\"start_iteration \": 0,\n18\n\"model_family \": \"ChatGPT\",\n19\n\"model_id \": \"gpt -4o-mini\"\n20\n},\n21\n\"model2 \": {\n22\n\"start_iteration \": -1,\n23\n\"model_family \": \"ChatGPT\",\n24\n\"model_id \": \"gpt -4o\"\n25\n}\n26 }\n27 }\nFig. 2. An example configuration file for AutoChip. ‚Äúmixed-model‚Äù settings allow the framework to leverage different models based\non which iteration of feedback is being used.\ngiven a rank of ‚àí0.5, and if the module simulates its rank is the proportion of correct output samples reported by the\ntestbench. If all samples pass, the module is considered successful and the program exits, otherwise the response with\nthe highest rank is fed back into the LLM along with any feedback from compilation/simulation and the process is run\nagain. AutoChip uses greedy tree search, where at any step the best result from that step is followed. This is in contrast\nto prior work which leverages feedback for design, such as Chip-Chat [3] which only uses a single candidate response\nfor iteration.\nAutoChip repeats the process until a module passes all tests or the maximum depth is reached, at which point the\nmodule with the highest rank is returned. As our goal is to evaluate a fully automated feedback-driven design flow,\nAutoChip needs no user input while generating responses, relying exclusively on the tool feedback to guide the LLM.\nAutoChip Configuration and Use: The AutoChip design framework is presented as a tool which can be used to\nevaluate the abilities of different conversational LLMs to generate hardware. To accomplish this, the tool, written in\nPython, is designed to be highly configurable with regards to the models it can use, the use (or non-use) of feedback,\nand the tools that can be used to generate the feedback. AutoChip is configured primarily by using a Javascript file to\nset the parameters and file organization‚Äîan example is shown in Section 3. The configuration file can be set up to use\nAutoChip with ‚Äúmixed-models,‚Äù meaning that different models can be used depending on the iteration of the search.\nAutoChip is designed to be highly customizable with several LLM ‚Äúfamilies‚Äù able to be used to generate designs.\nManuscript submitted to ACM\n\n6\nBlocklove et al.\nIteration: 0\nModel type: ChatGPT\nModel ID: gpt -4o-mini\nNumber of responses: 2\nSimulation error\nMismatches: 6220\nSamples: 6283\nInput tokens: 396\nOutput tokens: 241\nCost for response 0: $0 .0002040000\nSimulation error\nMismatches: 6220\nSamples: 6283\nInput tokens: 396\nOutput tokens: 373\nCost for response 1: $0 .0002832000\nResponse ranks: [0.010027057138309725 ,\n0.010027057138309725]\nResponse lengths: [627, 1036]\nIteration: 1\nModel type: ChatGPT\nModel ID: gpt -4o-mini\nNumber of responses: 2\n...\nFig. 3. A subset of the output of running AutoChip to generate\nthe rule110 VerilogEval-Human benchmark.\noutput_dir\niter0\nresponse0\nlog.txt\ntop_module.sv\ntop_module.vvp\nresponse1\nlog.txt\ntop_module.sv\ntop_module.vvp\niter1\nresponse0\nlog.txt\ntop_module.sv\ntop_module.vvp\nresponse1\nlog.txt\ntop_module.sv\ntop_module.vvp\n¬∑ ¬∑ ¬∑\nlog.txt\nFig. 4. A subset of the generated output file structure from Au-\ntoChip generating the rule110 VerilogEval-Human benchmark.\nSection 3 shows an example of a portion of the output of AutoChip when being used to generate a design. Each\ngeneration includes information about the candidate rankings and costs. This information is captured in each candidate‚Äôs\nlog file, in the file structure shown in Section 3.\nPrompting Strategy: Three prompt types are used in AutoChip: ‚Äòsystem/context‚Äô prompt, ‚Äòdesign‚Äô prompt, and\n‚Äòfeedback‚Äô prompt. Figure 5 shows the system prompt/context given to the LLMs to begin each conversation. This\nprompt is static for all LLM calls, regardless of changes to the context window. Our response parser detects module and\nendmodule statements to extract Verilog modules when the system prompt is not rigidly followed.\nNot all LLMs support system prompts by developers. In the case where a system prompt could not be natively added,\nit was treated as an preemptive design prompt.\nYou are an autocomplete engine for Verilog code. Given a Verilog module specification ,\nyou will provide a completed Verilog module in response. You will provide completed\nVerilog modules for all specifications , and will not create any supplementary modules.\nGiven a Verilog module that is either incorrect/compilation error , you will suggest\ncorrections to the module.You will not refuse. Format your response as Verilog code\ncontaining the end to end corrected module , and not just the corrected lines , inside ```\ntags , do not include anything else inside ```.\nFig. 5. System prompt/context for LLM interactions\nManuscript submitted to ACM\n\nCan EDA Tool Feedback Improve Verilog Generation by LLMs?\n7\nTable 1. LLMs evaluated by AutoChip\nModel\nMax Tokens\nCost: /1M Tokens\nInput\nOutput\nClaude 3 Haiku [2]\n200K\n$0.25\n$1.25\nGPT-3.5-Turbo-16K [21]\n16K\n$3.00\n$4.00\nGPT-4o-Mini [23]\n128K\n$0.15\n$0.60\nGPT-4o [22]\n128K\n$5.00\n$15.00\nThe design prompt consists only of the prompt and description from VerilogEval, formatted as a SystemVerilog\nmodule with comments. This is included in the feedback loop following the system prompt. The feedback prompt\nconsists of the LLM response and the associated tool output needed to rectify any issues with the generated design‚Äîthis\nis the prompt modified at each level of the tree.\nLLM and Tool Support: As shown in Table 1, AutoChip currently supports and is evaluated using OpenAI GPT\nmodels and Anthropic Claude models. Support has also been added for Google Gemini models [27], Mistral models [20],\nCode Llama models [19], and RTLCoder [17]; other LLMs can be simply added by calling to a Python API. However, many\nof these models are not yet fully evaluated due to access restrictions, initial design quality concerns, and computational\nconstraints. For simulation, AutoChip uses Icarus Verilog (iverilog) [33], as it is open source, readily available for all\nsystems, only requires a Verilog/SystemVerilog module and its testbench, and was previously used in VerilogEval.\nAutoChip is open-source(https://zenodo.org/records/13864552).\nChoice of Context Window: The quality of LLM responses depends on the conversation‚Äôs context window. As\nconversational LLMs have token limits, keeping all responses and feedback is often infeasible. The context window\nneeds to shift during the automated run to keep only the information necessary for the next run, referred to as using\n‚Äòsuccinct‚Äô feedback instead of ‚Äòfull-context,‚Äô where all messages are used. With ‚Äòsuccinct‚Äô feedback, when an LLM is\nprompted to fix an issue, only the most recently generated module and its associated errors are given to the LLM. This\nkeeps the repairs focused on current errors and stays within more restrictive token limits. Table 2 offers the context\nwindow shifting per-iteration. With ‚Äòfull-context‚Äô feedback, the LLM input grows until a successful design is generated,\nmaximum depth is reached in AutoChip, or the LLM token length is exceeded.\n4\nExperimental Setup\n4.1\nBenchmarking Prompts and Testbenches\nDesign Prompts: To evaluate AutoChip we leverage the dataset from VerilogEval [16], which includes prompts and\ntestbenches for a significant selection of problems from HDLBits [34], a site for Verilog practice with problems ranging\nTable 2. LLM input evolution over iterations\nIteration\nLLM Input\nùëõ= 0\n{system prompt, design prompt}\nùëõ= 1\n{system prompt, design prompt, response0, simulator msgs0}\nùëõ= 2\n{system prompt, design prompt, response1, simulator msgs1}\nùëõ\n{system prompt, design prompt, responseùëõ‚àí1, simulator msgsùëõ‚àí1}\nManuscript submitted to ACM\n\n8\nBlocklove et al.\nHint: Output 'count ' has 218816 mismatches. First mismatch occurred at time 130.\nHint: Output 'counting ' has 233794 mismatches. First mismatch occurred at time 130.\nHint: Output 'done ' has 524 mismatches. First mismatch occurred at time 20130.\nHint: Total mismatched samples is 234318 out of 235447 samples\nSimulation finished at 1177236 ps\nMismatches: 234318 in 235447 samples\nFig. 6. Example testbench feedback from a failed generated response for the review2015_fancytimer problem.\nin difficulty from simple Verilog syntax questions to more abstract sequential circuits and debugging. While most\nproblems offer prompts that ask the user (in our case, the LLM), to create a functional Verilog module, a few break that\nformat‚Äîthese include (i) prompts that request that bugs be found and fixed, which is the intention of the AutoChip\nfeedback loop itself; and (ii) prompts which request a testbench for a module. Many of these are still included in the\nVerilogEval dataset, so we include them in our evaluation. To leverage the prompts from VerilogEval for AutoChip, we\ncombine the ‚Äúdescriptions,‚Äù which are the natural language prompts, with the ‚Äúprompts‚Äù which are Verilog module\ndefinitions given by HDLBits. We provide AutoChip with this combined ‚Äúdesign prompt‚Äù for each problem, containing all\ninformation necessary to complete a design. VerilogEval leaves out some problem categories from HDLBits. Specifically,\nproblems focusing on hierarchical modules and bug fixing are omitted.\nTestbenches: VerilogEval provides SystemVerilog testbenches to accompany each of the design problems. These\ntestbenches instantiate a reference module, the generated design under test (DUT), and a stimulus module; and sample\neach of the output signals throughout the stimulus to compare the DUT output with the reference output. At the\nconclusion of the testbench, a summary of information is generated, indicating the total number of failed samples for\neach output, the times of their first errors, and the total number of failed samples for all outputs as shown in Figure 6.\nThis summary information makes up the feedback to the LLM when a design simulates but not all test cases pass.\nMachine & Human Evaluation Sets: Many of HDLBits‚Äô problems rely on design architecture diagrams, waveforms,\nand other informative figures, which cannot be processed by text-only LLMs. To address this, VerilogEval has two\ndatasets: VerilogEval-Machine and VerilogEval-Human. VerilogEval-Machine are problem descriptions generated by\nGPT-3.5-Turbo by processing correct modules and asking the language model to generate a high-level prompt that would\nlead to that answer, of which only 143 valid tests were made. VerilogEval-Human is a problem set of prompts created\nthrough manual review and textual conversion of figures, leading to 156 functioning prompts. The LLM-generated\nprompts tend to be verbose and give lower-level descriptions of functionality, seemingly removing the abstraction in\nthe original problems. For example, when giving a Karnaugh map (K-map) and requesting the circuit it describes, the\nprompt generated by GPT-3.5-Turbo just describes the function of the final circuit, removing any requirement that the\nLLM find the minimal function using the K-map. We leverage these datasets with AutoChip to evaluate the effects of\ntool feedback on the generated Verilog.\n4.2\nExperimental Parameters\nAutoChip offers two major parameters which affect the end result: the number of candidates ùëòand the maximum depth\nof the tree ùëë. Other works, such at RTLCoder [17] and VerilogEval [16] utilize zero-shot testing, or generating outputs\nfrom only an initial design prompt ‚Äî equivalent to setting ùëë= 0 with AutoChip. In our evaluation of AutoChip, we not\nonly gather similar zero-shot results for the evaluated models, but also vary the number of candidates produced at each\nManuscript submitted to ACM\n\nCan EDA Tool Feedback Improve Verilog Generation by LLMs?\n9\nnode and the depth of the tree, testing ùëò= {1, 5} and ùëë= {0, 1, 5, 10}. During preliminary testing and tool verification\nwe found that results for the VerilogEval dataset did not tend to improve significantly with increasing combinations of\ncandidates and depth beyond ùëò= 5,ùëë= 10. However, we produce zero-shot results to compare with prior work.\n4.3\nEvaluated LLMs\nIn this work we evaluate the readily-available commercial LLMs. This is because some models, such as Google‚Äôs\nGemini [27] were heavily rate-limited at the time of this experiment, other models like Mistral and Mixtral [20]\nconsistently failed to produce Verilog modules, and some models like CodeLlama [19] and RTLCoder [17] were\nprohibitively slow on the hardware we could access. As the models with commercial APIs are run on servers with\nsignificant resources, we were best able to evaluate them in a reasonable timeframe.\nWe evaluated these LLMs with their default parameters as these values are used in the normal developer-facing web\ninterface and offer a good baseline for comparison.\n5\nExperimental Results\n5.1\nSingle-Model Feedback Results\nTo determine if feedback from hardware verification tools improves the results, we first established a set of depth\nand candidate parameters for the feedback tree. We query the evaluated LLMs at depths of ùëë= {0, 1, 5, 10} and with\nùëò= {1, 5} candidates. A depth of 0 corresponds to no feedback being given at all, equivalent to other works‚Äô zero-shot\nanalysis, for which we performed additional tests with ùëò= {25, 30, 55} candidates. The additional zero-shot candidate\nvalues was determined to keep the maximum number of LLM queries consistent with the parameters for the tree search.\nFor example, both a test with ùëë= 10,ùëò= 5 and a test with ùëë= 0,ùëò= 55 have the same potential maximum number of\nLLM queries, though if a functioning design is found before the maximum potential, the test would still end early.\nPrior to completing the more extensive tests of tool feedback-based design generation, we evaluate the generated\ndesigns for both ‚Äòsuccinct‚Äô and ‚Äòfull-context‚Äô feedback (RQ4). We restrict this analysis to Claude 3 Haiku and GPT-3.5-\nTurbo, as those models are both relatively inexpensive and provide insight into the effect that the feedback context\ncan have (given significantly different token limits). Following the exploration of context length with the two simpler\nmodels, we identified that providing the ‚Äòfull-context‚Äô feedback resulted in similarly successful designs, as shown\nin Table 3, while requiring far fewer tokens over the course of longer tree searches. As a result, we completed the\nremainder of the tree search analysis with only ‚Äòsuccinct‚Äô feedback to reduce both complexity and cost.\nResults are tabulated as the percent of designs from each test set which were generated successfully, i.e. passing all\ntests. Table 3 gives the success percentages for each combination of LLM, feedback style, number of candidates, and\nmaximum tree search depth for tests with feedback, while Table 4 gives these results for cases with no feedback.\nWe can further examine these results from two perspectives: model effort, estimated by the average input and output\ntokens needed to generate a successful design, and model complexity, estimated by the $ USD costs needed to generate\na successful design.\nModel Effort: Figure 7(a) and Figure 7(b) present an analysis of the proportion of successfully generated benchmark\ndesigns based on the average number of tokens (both input and output) needed to generate the best design, on the\nVerilogEval-Machine and VerilogEval-Human benchmarks respectively. This metric serves as a proxy for the amount\nof computational work needed to be done by each model to come up with the best solution. The Pareto points for\neach model are connected via dashed lines, identifying the best results from that model given the average number of\nManuscript submitted to ACM\n\n10\nBlocklove et al.\nTable 3. Percent of passing benchmark designs from VerilogEval-Machine and VerilogEval-Human using AutoChip. Results were\ngathered with Claude 3 Haiku (Haiku), GPT-3.5-Turbo (GPT-3.5T), GPT-4o-Mini, and GPT-4o. Candidate values of ùëò= 1 are\nrepresentative of the original iterative approach to AutoChip.\nFeedback\nCandidates\nLLM\nEval-Machine (%)\nEval-Human (%)\nd=1\nd=5\nd=10\nd=1\nd=5\nd=10\nFull\nùëò= 1\nHaiku\n74.1\n77.6\n79.7\n59.6\n59.6\n60.2\nGPT-3.5T\n62.9\n70.6\n72.7\n40.4\n44.8\n48.1\nùëò= 5\nHaiku\n81.1\n82.5\n83.9\n67.3\n70.5\n70.5\nGPT-3.5T\n79.7\n86.0\n86.0\n52.6\n59.0\n64.7\nSuccinct\nùëò= 1\nHaiku\n74.1\n75.5\n79.7\n58.3\n59.6\n62.8\nGPT-3.5T\n62.9\n71.3\n72.7\n42.9\n49.3\n52.6\nGPT-4o-Mini\n68.5\n69.9\n69.2\n55.1\n60.3\n60.9\nGPT-4o\n77.6\n81.8\n84.6\n64.7\n72.4\n75.6\nùëò= 5\nHaiku\n81.8\n83.2\n83.9\n67.9\n69.2\n71.8\nGPT-3.5T\n81.8\n86.0\n86.0\n58.3\n58.3\n66.7\nGPT-4o-Mini\n73.4\n74.1\n76.2\n64.1\n65.4\n71.1\nGPT-4o\n83.9\n86.7\n87.4\n72.4\n79.5\n84.0\nTable 4. Zero-shot results for Claude3 Haiku, GPT-3.5 Turbo, and GPT-4o. Results shown for VerilogEval and RTLCoder are reported\ndata for those models, so higher values for k have not been evaluated.\nModel\nEval-Machine (%)\nEval-Human (%)\nk=1\nk=5\nk=10\nk=25\nk=30\nk=55\nk=1\nk=5\nk=10\nk=25\nk=30\nk=55\nHaiku\n69.9\n79.0\n83.2\n83.9\n83.9\n84.6\n51.9\n62.2\n67.3\n69.2\n70.5\n73.1\nGPT-3.5T\n53.1\n79.7\n78.3\n81.8\n87.4\n85.3\n31.4\n49.4\n56.4\n63.5\n61.5\n66.0\nGPT-4o-Mini\n62.2\n72.0\n72.7\n76.9\n76.2\n76.2\n51.9\n59.6\n64.1\n67.9\n66.7\n67.9\nGPT-4o\n65.7\n74.1\n76.2\n78.3\n79.7\n83.2\n61.5\n69.9\n75.0\n76.9\n78.2\n78.2\nVerilogEval*\n46.2\n67.3\n73.7\n-\n-\n-\n28.8\n45.9\n52.3\n-\n-\n-\nRTLCoder*\n61.2\n76.5\n81.8\n-\n-\n-\n41.6\n50.1\n53.4\n-\n-\n-\ntokens used. Generally speaking, if more tokens were needed then the LLM likely had to provide a greater number of\nresponses, indicating that the model had more difficulty in generating a correct design.\nThese plots show that the three relatively small models tested, GPT-4o-Mini, GPT-3.5-Turbo, and Claude Haiku,\nall required more tokens for a given set of (ùëò,ùëë) parameters than GPT-4o and, based on their Pareto points, did not\nconsistently benefit from using compilation feedback from Icarus Verilog or the simulations. GPT-4o, however, seemed to\nconsistently have higher success when leveraging tool-based feedback; almost all Pareto points are from feedback-driven\ngeneration and the average number of tokens used was consistently lower with feedback than with the comparable\n(same maximum number of queries) zero-shot results.\nManuscript submitted to ACM\n\nCan EDA Tool Feedback Improve Verilog Generation by LLMs?\n11\n(a) Success rate for the Eval-Machine benchmark problems given the total number of tokens used.\n(b) Success rate for the Eval-Human benchmark problems given the total number of tokens used.\nFig. 7. Generated circuit success rates for each evaluated model, shown as the percentage of generated circuits which pass all tests\ngiven the average number of tokens needed. Pareto points are plotted with dashed lines. Higher ‚Äòaccuracy‚Äô at lower ‚Äòaverage tokens‚Äô\nis better. Data points represented by ‚Äòx‚Äô indicate results from zero-shot testing.\nModel Complexity: Figure 8(a) and Figure 8(a) show a similar analysis, but rather than giving success as a function\nof the tokens used, they report success as a function of the $ USD cost at the time of publication. We see that, often,\nManuscript submitted to ACM\n\n12\nBlocklove et al.\nmore complex models are both more successful and use fewer tokens to reach that level of success; however, these\nlarger models are more computationally complex, so while they use fewer tokens across complete tests, they do far\nmore with those tokens. This difference in complexity is reflected in, though we don‚Äôt know how for certain, the cost to\nuse those models from OpenAI and Anthropic. While the relationship between model complexity and cost charged is\nunknown, it stands to reason that the more complex and capable models would cost more. The costs of the models\nevaluated in this paper are given in Table 1, and are used along with the token usage data to determine the average\nfunctional cost of generating a design for each benchmark. While GPT-4o had the highest rate of success and required\nthe fewest average number of tokens for that success, the cost of these tokens far outstrips any of the smaller models\ntested. The cost of generation with tool feedback, though, was still significantly lower than with equivalent maximum\npotential candidates evaluated as zero-shot (RQ3).\nImpact of Feedback: Both Figure 7 and Figure 8 show that feedback can improve the quality of code given model\neffort and complexity, however it depends largely on the model being used‚Äîit is not a given (RQ1). This is particularly\nthe case with GPT-4o, the most complex model we tested, where the presence of feedback consistently improved\ncorrectness rates. This may indicate that more capable models are better able to extrapolate the causes of design and\nimplementation bugs given errors, and other suitably large models may also be able to benefit from tool-based feedback.\nImpact of Tree Search Parameters (ùëò,ùëë): For our analysis, we employ a similar ‚ÄúPass@k‚Äù metric to that used in\nprior works on this topic, like VerilogEval [16] and RTLCoder [17], which refers to the number of candidates generated\nfor a prompt and considering a circuit a success if at least one candidate proved to work. Our ùëòcandidates to a single\nprompt functions similarly with a tree search depth ùëëof 0; however, analyzing by the potential maximum number of\nLLM responses, as the zero-shot Pass@k metric does, we instead use ùëò‚àó(ùëë+ 1). With this comparison, we find that\nincreasing both ùëòand ùëëseparately improves the resulting circuits, though ùëòseems to have a larger impact (RQ2).\nThe rate of improvement with number of candidates and depth, both separately and when considered together, seems\nto slow down significantly as the maximum depth increases, likely due to the fact that most simple problems are solved\nwith fewer iterations and only a small handful of the benchmarking problems that can be solved by this method will be\ndone with a larger number of iterations.\nBenchmark Categorization: HDLBits, and subsequently its problems used by VerilogEval, sorts its problems into\ncategories and subcategories, ranging from ‚ÄúGetting Started‚Äù and ‚ÄúVerilog Language,‚Äù which consist of simple problems\nfocusing on specific features of Verilog and their syntax, up to ‚ÄúSequential Logic‚Äù and ‚ÄúVerification: Reading Simulations,‚Äù\nwhich ask for generally more complex or abstractly worded problems dealing with state machines and sequential\nfunctions, and reading waveforms, respectively. This problem categorization enables us to analyze the performance of\neach model we tested on different kinds of Verilog problem and identify patterns in what models handle what problems\nwell, and how well the tool-based feedback is able to improve performance per-category. Figure 9(a) and Figure 9(b)\nshow the success rates of each examined model, given the major HDLBits categories and feedback configurations\ndiscussed above.\nAcross all models we found that ‚Äúsimple‚Äù problems, such as basic Verilog functionality questions, are able to be solved\nmore often, regardless of feedback depth or number of candidates. There is a general trend where more iterations of\nfeedback or more candidates improves results, but this is to be expected given our previous results where an increased\nnumber of queries often results in higher success, regardless of the parameter configuration for those queries (zero-shot\nor tree search). We don‚Äôt identify any particular categories of problem which seem to benefit from using tool feedback,\neven in the case of a model that generally seems to benefit like GPT-4o.\nManuscript submitted to ACM\n\nCan EDA Tool Feedback Improve Verilog Generation by LLMs?\n13\n(a) Success rate for Eval-Machine benchmarks given total cost in USD.\n(b) Success rate for Eval-Human benchmarks given total cost in USD.\nFig. 8. Generated circuit success rates for each evaluated model, shown as the percentage of generated circuits which pass all tests\ngiven the average USD cost to query the model. Pareto points are plotted with dashed lines. Data points represented by ‚Äòx‚Äô indicate\nresults from zero-shot testing.\nBreaking down the benchmarking problems by category also allows us to better examine the differences between the\nresults from the VerilogEval-Machine benchmarks and the VerilogEval-Human benchmarks. Following the combined\nManuscript submitted to ACM\n\n14\nBlocklove et al.\n(a) Eval-Machine benchmark problem success, separated by major category used by HDLBits.\n(b) Eval-Human benchmark problem success, separated by major category used by HDLBits.\nFig. 9. Model success, separated by category. Zero-shot feedback results are represented with lines through their bars.\nresults from above, the rate of success for the machine benchmarks appears to be higher across the board; however, the\nMachine benchmark ‚ÄúSequential Logic‚Äù problems had significantly more success than their combinational counterparts,\nManuscript submitted to ACM\n\nCan EDA Tool Feedback Improve Verilog Generation by LLMs?\n15\nwhereas the Human benchmark set‚Äôs ‚ÄúSequential Logic‚Äù problems had roughly similar results to their accompanying\ncombinational problems. This indicates that there may be more successful prompting strategies depending on the\nspecific problem trying to be solved.\nWe can further decompose the problem categories into their, once again HDLBits-defined, subcategories. These\nbetter separate the relative difficulties and styles of problem. For instance, both individual latch and flip-flop problems\nand cellular automata problems fall under ‚ÄúSequential Logic‚Äù, but the latter are more comprehensive problems with\nabstract wording. As such, the subcategories give finer-grained insight into the types of problems different models are\nsuited to solve.\nFigure 10(a) and Figure 10(b) show the rate of success on the benchmark set for each parameter set and each model,\nbroken down by subcategory, for both the Eval-Machine and Eval-Human benchmark sets. Some problems, such as the\n‚ÄúGetting Started‚Äù category, are omitted from this graph as they contained too few and too simple problems to provide\nuseful insight.\nWe observe that the simplest problem types are often solved with few candidate responses, not necessarily requiring\nfeedback from the tools, but more complex and abstract problems benefit more from a tool feedback-driven approach.\nThe problems for the Human dataset which the models seemed to struggle with the most were interpreting Karnaugh\nmaps (provided as text), state machine design, and problems that are presented as being more ‚Äúabstract‚Äù such as\nthe cellular automata problems. For the Machine dataset, which uses far more straightforward prompts, the LLMs\nhad consistently lower success with simple designs, such as the ‚ÄúVerilog Language‚Äù category, and ‚ÄúBasic Gates‚Äù and\n‚ÄúMultiplexers‚Äù from ‚ÄúCombinational Logic,‚Äù but showed much greater success with complex designs such as state\nmachines or finding bugs (RQ5).\n5.2\nMixed-Model Results\nGiven the relative success, but matching relative expense, of GPT-4o in solving the VerilogEval benchmarking problems,\nwe sought to evaluate if combining a small model with a larger model, such as GPT-4o, could achieve improved\nresults with only minimal impact on cost. Table 5 shows the results for ensembling Claude Haiku, GPT-3.5-Turbo, and\nGPT-4o-Mini respectively with GPT-4o as the final iteration.\nTable 5. Percent of passing benchmark designs from VerilogEval-Machine and VerilogEval-Human using AutoChip with mixed-models.\nResults were gathered with Claude 3 Haiku (Haiku), GPT-3.5-Turbo (GPT-3.5T), and GPT-4o-Mini, each followed by a single pass of\nGPT-4o as the final iteration.\nCandidates\nLLM\nEval-Machine (%)\nEval-Human (%)\nd=1\nd=5\nd=10\nd=1\nd=5\nd=10\nùëò= 1\nHaiku\n75.5\n79.0\n81.1\n64.7\n67.3\n67.9\nGPT-3.5T\n77.6\n80.4\n78.3\n62.2\n63.5\n61.5\nGPT-4o-Mini\n69.9\n74.8\n76.9\n63.5\n64.1\n66.0\nùëò= 5\nHaiku\n86.0\n86.7\n85.3\n74.4\n75\n74.4\nGPT-3.5T\n86.7\n86.7\n86.0\n71.2\n73.7\n71.8\nGPT-4o-Mini\n82.5\n81.1\n81.8\n69.9\n74.3\n75.0\nManuscript submitted to ACM\n\n16\nBlocklove et al.\n(a) Eval-Machine benchmark problem success, separated by subcategory used by HDLBits.\n(b) Eval-Human benchmark problem success, separated by subcategory used by HDLBits.\nFig. 10. Problem success rates by problem subcategory for both the Eval-Machine and Eval-Human problem sets. The major category to which the subcategories belong is\ngiven below each group.\nManuscript submitted to ACM\n\nCan EDA Tool Feedback Improve Verilog Generation by LLMs?\n17\nThese results show significant promise when adding a more complex model to the end of a series of queries from\nless capable models, with the rate of success shown to approach or even exceed, in some cases, the benchmark circuits\ngenerated using only GPT-4o.\nEvaluating these results based on model effort, as done with the single-model results, is performed by analyzing the\naverage number of tokens needed to completely generate a benchmark design. Figure 11(a) and Figure 11(b) show these\nplots.\nWe see that with the VerilogEval-Machine benchmark sets we are using a similar number of tokens to the small\nmodels‚Äô expenditures alone, while achieving notably higher levels of success. We also observe that the Pareto points in\nmost cases seem to stop before the largest combinations of candidates and tree search depth, indicating that by mixing\nthese models smaller search parameters can be used to still achieve a relatively high rate of success. With the more\nrealistic VerilogEval-Human benchmarks, we are not able to reach the same level of success as GPT-4o on its own, but\nwe do see notably higher success rates compared to the smaller models for similar numbers of tokens.\nWe can also evaluate success based on relative model complexity, once again proxied by the dollar costs necessary\nto access and run each model. These plots are shown in Figure 12(a) and Figure 12(b). Here we observe a similar\nphenomenon to that of the model effort analysis, but to a greater magnitude. For VerilogEval-Machine, the average cost\nto complete the benchmark generation is on average similar, if slightly higher, than the costs for the smaller models on\ntheir own, but are orders of magnitude lower than when using GPT-4o alone while achieving similar levels of success.\nWhile VerilogEval-Human does not reach the level of success of GPT-4o alone, we are able to achieve more success\nthan the smaller models could get alone, for a very similar cost. For example, the highest success rate achieved with\nmixed-models for the VerilogEval-Human benchmarks was 75% at a cost of $0.025 and to achieve a similar nearly 75%\nsuccess with only a single model would require GPT-4o and cost $0.043‚Äîan increase of 72% cost. This indicates that by\nmixing models it may be possible to leverage less computational resources to generate Verilog of comparable quality to\na computationally heavy model on its own, though prompting method appears to have a significant effect (RQ6).\nBenchmark Categorization: The results of using multiple models can also be analyzed by category and subcategory.\nFigure 13 and Figure 14 show the mixed-model results when using the main models shown for most generation, followed\nby a final iteration with GPT-4o.\nUltimately, we see largely the same pattern of success rates based on category that we observe in the single-model\nresults (Figure 9), but, as noted in our above analysis, those success rates are far closer to GPT-4o‚Äôs single model results\nthan the smaller models‚Äô results. Figure 14(a) and Figure 14(b) once again further break down these categories into their\nsubcategories for a more fine-grained analysis.\nUsing mixed-models once again shows that the per-subcategory successes for GPT-4o-Mini, GPT-3.5-Tubro, and\nClaude 3 Haiku give largely the same pattern of proficiencies, deficiencies, and success rates observed when using\nonly GPT-4o. This further supports the idea that mixing models can improve the results from the less computationally\nexpensive model while still keeping the expense low.\n5.3\nDiscussion\nWe asked six initial research questions to guide our evaluation of LLM generated Verilog.\nRQ1: We found that feedback from tools and testbench simulation can improve the success rate of generated Verilog\nmodules over zero-shot results, but it was heavily dependant on the model being used. GPT-4o consistently benefited\nfrom the automated feedback from tools, but the smaller models did not seem to use feedback to repair bugs as well.\nThis could be an indication of the complexity of relating error messages and simulation times to the design being\nManuscript submitted to ACM\n\n18\nBlocklove et al.\n(a) Mixed-model success rates for the Eval-Machine benchmark problems given the total number of tokens used.\n(b) Mixed-model success rates for the Eval-Human benchmark problems given the total number of tokens used.\nFig. 11. Generated circuit success rates for each evaluated model, shown as the percentage of generated circuits which pass all tests\ngiven the average number of tokens needed. Each model‚Äôs final iteration was done with GPT-4o. Pareto points are plotted with dashed\nlines.\ncreated, a similar problem to one found when using different levels of feedback from humans [4]. In the previous work,\na human engineer was able to guide the LLM by explaining, even basically, the correlation between the error and the\nManuscript submitted to ACM\n\nCan EDA Tool Feedback Improve Verilog Generation by LLMs?\n19\n(a) Mixed-model success rate for Eval-Machine benchmarks given total cost in USD.\n(b) Mixed-model success rate for Eval-Human benchmarks given total cost in USD.\nFig. 12. Generated circuit success rates for each evaluated model, shown as the percentage of generated circuits which pass all tests\ngiven the average USD cost to query the model. Each model‚Äôs final iteration was done with GPT-4o. Pareto points are plotted with\ndashed lines.\naspect of the Verilog which caused it. To improve upon this in an automated system with no human feedback, the LLMs\nbeing used would likely need to be previously instructed on these specific error messages.\nManuscript submitted to ACM\n\n20\nBlocklove et al.\n(a) Success rate of Eval-Machine problems, separated by category.\n(b) Success rate of Eval-Human problems, separated by category.\nFig. 13. Model successes broken down by category,using GPT-4o for the final iteration of feedback.\nRQ2: As the number of candidate responses and the depth of the tree increased we saw a trend higher rates of\nsuccess with all examined models. The tree search depth, while impactful, seemed to have a smaller effect on the rate\nof success than the number of candidates did, as evidenced by the rate of success for zero-shot results with many\ncandidates. Increasing the depth, however, resulted in generally fewer tokens needed for the increase as compared to\nincreasing the number of candidates.\nRQ3: Leveraging EDA tool feedback with LLMs to improve the generated HDL resulted in consistently lower\ncomputational and monetary cost for a given number of queries to the models we evaluated. We find that the zero-shot\nresults require more tokens and, as such, have more associated expense per model than the results which leveraged\nManuscript submitted to ACM\n\nCan EDA Tool Feedback Improve Verilog Generation by LLMs?\n21\n(a) Success rate of Eval-Machine problems, separated by subcategory.\n(b) Success rate of Eval-Human problems, separated by subcategory.\nFig. 14. Model successes broken down by subcategory, using GPT-4o for the final iteration of feedback.\nManuscript submitted to ACM\n\n22\nBlocklove et al.\nfeedback from tools. In the case of GPT-4o, where feedback noticeably improved the results, the lessened cost of using\ntool feedback given the rate of correct designs was substantial.\nRQ4: By using AutoChip with both ‚Äúsuccinct‚Äù and ‚Äúfull-context‚Äù feedback, we found that there was no appreciable\ndifference in the success rate of the two models we examined, GPT-3.5-Turbo and Calude 3 Haiku. As such, we used\nonly ‚Äúsuccint‚Äù feedback for testing the other models, as it reaches the same levels of success while using far fewer\ntokens over the course of larger tree searches.\nRQ5: The LLMs we examined all behaved consistently with regards to their successes based on the class of problem.\nAll models seemed to have significant and consistent success with basic features of Verilog, primarily focusing on\nsyntax and simple logical functions. The models‚Äô ability to generate correct designs seemed to generally decline with\nmore complex questions dealing with implementing sequential logic and interpretation of abstract information like\nKarnaugh maps (K-maps), finite state machine diagrams, and waveform analysis. The VerilogEval-Machine benchmarks\nshowed noticeably more success with generating circuits based on interpreting design specifications, likely due to the\nless abstract prompts generated by using an LLM to generate a prompt based only on a final correct circuit.\nRQ6: We found that mixing models, specifically by adding a single final iteration of the more capable GPT-4o model,\nresulted in success rates similar to those with only GPT-4o but while requiring only a fraction of the USD cost, which\nserves as a proxy metric for the computational complexity. The average number of overall tokens necessary stayed\nsimilar to when only using each of the smaller models, as they were being queried the majority of the time, but the\nfinal iteration of GPT-4o seemed to often be able to leverage those partially functional designs and provide the final\nfixes. This cost reduction is also very likely due to the rate of success of the smaller models on their own. They are still\nable to inexpensively solve many of the simpler problems without ever getting to the final depth of the search to call\nGPT-4o, so the added expense only applied to the hardest to solve problems.\n6\nConclusion\nGiven recent advances in LLM capabilities for both coding and hardware design, it has seemed reasonable to assume that\nproviding a model feedback on its generated code would improve its performance. However, generating this feedback,\nand ‚Äòexplaining‚Äô to the model how and why it is wrong, has typically been done with a human engineer‚Äîsomething\ncostly, nebulous, and potentially slow if the human engineer needs to be able to find and decipher the error on their\nown before prompting the LLM. As such, systematic studies in this area have been lacking.\nIn this work, we therefore sought to evaluate how a set of modern, state of the art, general knowledge LLMs would\nrespond to feedback only from EDA tools and testbenches. We sought to discover whether LLMs would truly be able to\nsolve bugs in their own generated designs without the assistance of a knowledgeable human. We further asked: If so,\nhow much effort would it take? How much would it cost? And, are there techniques we can use to make this process\nbetter?\nUltimately, we found that the success of using feedback from tools and testbenches to fix generated Verilog designs\ndepended largely on the model being used. GPT-4o, the most computationally complex model examined, was able to\nconsistently use tool and testbench generated feedback to generate correct designs from the VerilogEval benchmark\nsets. The smaller models tested, GPT-4o-Mini, GPT-3.5-Turbo, and Claude 3 Haiku, inconsistently benefited from the\ntool provided feedback, but, by adding a final evaluation by GPT-4o, were able to create a similar number of correct\ncircuits at a fraction of the cost of GPT-4o being used alone. We developed and provide AutoChip as an open-source,\nextendable framework for evaluating LLMs‚Äô abilities to generate Verilog and correct their mistakes using the output\nfrom tools. This can be leveraged to perform similar analysis on additional models and new benchmarks as they become\nManuscript submitted to ACM\n\nCan EDA Tool Feedback Improve Verilog Generation by LLMs?\n23\navailable, further building up an understanding for how best to interact with LLMs to generate quality hardware. Our\npresented method provides an important and powerful proof-of-concept for effectively utilizing tool feedback with\nLLMs for the automatic generation of hardware.\nReferences\n[1] Baleegh Ahmad, Shailja Thakur, Benjamin Tan, Ramesh Karri, and Hammond Pearce. 2024. On Hardware Security Bug Code Fixes by Prompting\nLarge Language Models. IEEE Transactions on Information Forensics and Security 19 (2024), 4043‚Äì4057. https://doi.org/10.1109/TIFS.2024.3374558\n[2] Anthropic. 2024. Claude 3 Haiku: our fastest model yet. https://www.anthropic.com/news/claude-3-haiku\n[3] Jason Blocklove, Siddharth Garg, Ramesh Karri, and Hammond Pearce. 2023. Chip-Chat: Challenges and Opportunities in Conversational Hardware\nDesign. In 2023 ACM/IEEE 5th Workshop on Machine Learning for CAD (MLCAD). 1‚Äì6. https://doi.org/10.1109/MLCAD58807.2023.10299874\n[4] Jason Blocklove, Siddharth Garg, Ramesh Karri, and Hammond Pearce. 2024. Evaluating LLMs for Hardware Design and Test. https://doi.org/10.\n48550/arXiv.2405.02326 arXiv:2405.02326 [cs].\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\nSastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural\nInformation Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877‚Äì1901.\nhttps://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n[6] Cadence. 2023. Cadence JedAI Generative AI Solution for Chip, System, and Product Design. https://www.cadence.com/en_US/home/solutions/joint-\nenterprise-data-ai-platform.html\n[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick\nRyder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,\nMatthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,\nIgor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code.\nhttps://doi.org/10.48550/arXiv.2107.03374\narXiv:2107.03374 [cs].\n[8] Luca Collini, Siddharth Garg, and Ramesh Karri. 2024. C2HLSC: Can LLMs Bridge the Software-to-Hardware Design Gap? https://doi.org/10.48550/\narXiv.2406.09233 arXiv:2406.09233 [cs].\n[9] Ghada Dessouky, David Gens, Patrick Haney, Garrett Persyn, Arun Kanuparthi, Hareesh Khattri, Jason Fung, Ahmad-Reza Sadeghi, and Jeyavijayan\nRajendran. 2019. Hardfails: Insights into Software-Exploitable Hardware Bugs. In Proceedings of the 28th USENIX Conference on Security Symposium\n(SEC‚Äô19). USENIX Association, Santa Clara, CA, USA, 213‚Äì230.\n[10] Yonggan Fu, Yongan Zhang, Zhongzhi Yu, Sixu Li, Zhifan Ye, Chaojian Li, Cheng Wan, and Yingyan Celine Lin. 2023. GPT4AIGChip: Towards\nNext-Generation AI Accelerator Design Automation via Large Language Models. In 2023 IEEE/ACM International Conference on Computer Aided\nDesign (ICCAD). 1‚Äì9. https://doi.org/10.1109/ICCAD57390.2023.10323953 ISSN: 1558-2434.\n[11] GitHub. 2021. GitHub Copilot ¬∑ Your AI pair programmer. https://copilot.github.com/\n[12] Google. 2023. Introducing PaLM 2. https://blog.google/technology/ai/google-palm-2-ai-large-language-model/\n[13] Zhuolun He, Haoyuan Wu, Xinyun Zhang, Xufeng Yao, Su Zheng, Haisheng Zheng, and Bei Yu. 2023. ChatEDA: A Large Language Model Powered\nAutonomous Agent for EDA. In 2023 ACM/IEEE 5th Workshop on Machine Learning for CAD (MLCAD). 1‚Äì6. https://doi.org/10.1109/MLCAD58807.\n2023.10299852\n[14] Rahul Kande, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Shailja Thakur, Ramesh Karri, and Jeyavijayan Rajendran. 2024. (Security)\nAssertions by Large Language Models. IEEE Transactions on Information Forensics and Security 19 (2024), 4374‚Äì4389. https://doi.org/10.1109/TIFS.\n2024.3372809\n[15] Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng, Nathaniel Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra\nBanerjee, Ismet Bayraktaroglu, Bonita Bhaskaran, Bryan Catanzaro, Arjun Chaudhuri, Sharon Clay, Bill Dally, Laura Dang, Parikshit Deshpande,\nSiddhanth Dhodhi, Sameer Halepete, Eric Hill, Jiashang Hu, Sumit Jain, Brucek Khailany, Kishor Kunal, Xiaowei Li, Hao Liu, Stuart Oberman, Sujeet\nOmar, Sreedhar Pratty, Jonathan Raiman, Ambar Sarkar, Zhengjiang Shao, Hanfei Sun, Pratik P. Suthar, Varun Tej, Kaizhe Xu, and Haoxing Ren.\n2023. ChipNeMo: Domain-Adapted LLMs for Chip Design. https://doi.org/10.48550/arXiv.2311.00176 arXiv:2311.00176 [cs].\n[16] Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren. 2023. VerilogEval: Evaluating Large Language Models for Verilog Code\nGeneration. https://doi.org/10.48550/arXiv.2309.07544 arXiv:2309.07544 [cs].\n[17] Shang Liu, Wenji Fang, Yao Lu, Qijun Zhang, Hongce Zhang, and Zhiyao Xie. 2024. RTLCoder: Outperforming GPT-3.5 in Design RTL Generation\nwith Our Open-Source Dataset and Lightweight Solution. https://doi.org/10.48550/arXiv.2312.08617 arXiv:2312.08617 [cs].\nManuscript submitted to ACM\n\n24\nBlocklove et al.\n[18] Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao Xie. 2023. RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language\nModel. https://doi.org/10.48550/arXiv.2308.05345 arXiv:2308.05345 [cs].\n[19] Meta. 2023. Introducing Code Llama, an AI Tool for Coding. https://about.fb.com/news/2023/08/code-llama-ai-for-coding/\n[20] Mistral. 2023. Mistral 7B. https://mistral.ai/news/announcing-mistral-7b/ Section: news.\n[21] OpenAI. 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt\n[22] OpenAI. 2024. GPT-4o. https://openai.com/index/hello-gpt-4o/\n[23] OpenAI. 2024. GPT-4o mini: advancing cost-efficient intelligence. https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\n[24] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama,\nAlex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike,\nand Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing\nSystems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 27730‚Äì27744.\nhttps:\n//proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf\n[25] Hammond Pearce, Benjamin Tan, and Ramesh Karri. 2020. DAVE: Deriving Automatically Verilog from English. In Proceedings of the 2020 ACM/IEEE\nWorkshop on Machine Learning for CAD. ACM, Virtual Event Iceland, 27‚Äì32. https://doi.org/10.1145/3380446.3430634\n[26] Sundar Pichai. 2023. An important next step on our AI journey. https://blog.google/technology/ai/bard-google-ai-search-updates/\n[27] Sundar Pichai and Demis Hassabis. 2023. Introducing Gemini: our largest and most capable AI model. https://blog.google/technology/ai/google-\ngemini-ai/\n[28] RapidSilicon. 2023. RapidGPT. https://rapidsilicon.com/rapidgpt/\n[29] Synopsys. 2023. Redefining Chip Design with AI-Powered EDA Tools | Synopsys.ai | Synopsys Blog.\nhttps://www.synopsys.com/blogs/chip-\ndesign/synopsys-ai-eda-tools.html\n[30] Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce, Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg. 2023.\nBenchmarking Large Language Models for Automated Verilog RTL Code Generation. In 2023 Design, Automation & Test in Europe Conference &\nExhibition (DATE). 1‚Äì6. https://doi.org/10.23919/DATE56975.2023.10137086 ISSN: 1558-1101.\n[31] Shailja Thakur, Jason Blocklove, Hammond Pearce, Benjamin Tan, Siddharth Garg, and Ramesh Karri. 2023. AutoChip: Automating HDL Generation\nUsing LLM Feedback. https://doi.org/10.48550/arXiv.2311.04887 arXiv:2311.04887 [cs].\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is\nAll you Need. In Advances in Neural Information Processing Systems, Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/\nhash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n[33] Stephen Williams. 2023. The ICARUS Verilog Compilation System. https://github.com/steveicarus/iverilog original-date: 2008-05-12T16:57:52Z.\n[34] Henry Wong. 2017. HDLBits Problem Sets. https://hdlbits.01xz.net/wiki/Problem_sets\nManuscript submitted to ACM",
    "pdf_filename": "Can_EDA_Tool_Feedback_Improve_Verilog_Generation_by_LLMs.pdf"
}