{
    "title": "Heuristic-Free Multi-Teacher Learning",
    "abstract": "We introduce Teacher2Task, a novel framework for multi-teacher learning that eliminates the need for manual aggregation heuristics. Existing multi-teacher methods typically rely on such heuristics to combine predictions from multiple teachers, often resulting in sub-optimal aggregated labels and the propagation of aggregation errors. Teacher2Task addresses these limitations by introducing teacher-specific input tokens and reformulating the training process. Instead of relying on aggregated labels, the framework transforms the training data – con- sisting of ground truth labels and annotations from N teachers – into N+1 distinct tasks: N auxiliary tasks that predict the labeling styles of the N individual teach- ers, and one primary task that focuses on the ground truth labels. This approach, drawing upon principles from multiple learning paradigms, demonstrates strong empirical results across a range of architectures, modalities, and tasks. 1 Introduction Since AlexNet [1], a decade of ML development has yielded a wealth of capable \"teachers\". Humans as teachers, though expensive, provide near-perfect accuracy annotation. Large Language Models (LLMs) offer excellent zero-shot capabilities, generating high-quality \"silver\" data for many tasks. Domain-specific foundational models serve as specialized teachers within their domains. An ideal learning framework would enable ML models to learn effectively from all useful data sources, con- sidering their strengths and weaknesses, unlocking the benefits of both accuracy and scalability. However, effectively leveraging multiple teachers remains an open challenge. Conflicting annota- tions from humans, LLMs, and domain-specific models can be difficult to reconcile, e.g., various teachers give conflict annotation for the same input samples. Also, directly aggregating predictions from LLMs and machine learning (ML) models as final labels can be problematic due to the in- herent noise in individual predictions, which can propagate and amplify in the inaccuracies after aggregation. Existing multiple-teacher learning approaches typically leverage the aggregated output of an ensem- ble of teachers [2] [3]. Most use a simple weighted average of teacher predictions, often with fixed or uniform weights [4][5]. More sophisticated approaches explore manually tuned weights [6] or learn instance-specific teacher importance weights [7] [8]. Alternatively, some methods focus on selecting the \"best\" teacher for each instance, using strategies ranging from random selection [4] to reinforcement learning-based dynamic selection [9]. Specialized approaches, such as assigning teachers to distinct language pairs in multilingual neural machine translation [10], represent specific cases of domain-based teacher selection. However, a common limitation is the reliance on pre- defined heuristics for teacher aggregation or selection, where these heuristics treat the aggregated ∗Correspondence to huythong@google.com arXiv:2411.12724v1  [cs.LG]  19 Nov 2024",
    "body": "Heuristic-Free Multi-Teacher Learning\nHuy Thong Nguyen∗\nGoogle\nEn-Hung Chu\nGoogle\nLenord Melvix\nGoogle\nJazon Jiao\nGoogle\nChunglin Wen\nGoogle\nBenjamin Louie\nGoogle\nAbstract\nWe introduce Teacher2Task, a novel framework for multi-teacher learning that\neliminates the need for manual aggregation heuristics.\nExisting multi-teacher\nmethods typically rely on such heuristics to combine predictions from multiple\nteachers, often resulting in sub-optimal aggregated labels and the propagation\nof aggregation errors. Teacher2Task addresses these limitations by introducing\nteacher-specific input tokens and reformulating the training process. Instead of\nrelying on aggregated labels, the framework transforms the training data – con-\nsisting of ground truth labels and annotations from N teachers – into N+1 distinct\ntasks: N auxiliary tasks that predict the labeling styles of the N individual teach-\ners, and one primary task that focuses on the ground truth labels. This approach,\ndrawing upon principles from multiple learning paradigms, demonstrates strong\nempirical results across a range of architectures, modalities, and tasks.\n1\nIntroduction\nSince AlexNet [1], a decade of ML development has yielded a wealth of capable \"teachers\". Humans\nas teachers, though expensive, provide near-perfect accuracy annotation. Large Language Models\n(LLMs) offer excellent zero-shot capabilities, generating high-quality \"silver\" data for many tasks.\nDomain-specific foundational models serve as specialized teachers within their domains. An ideal\nlearning framework would enable ML models to learn effectively from all useful data sources, con-\nsidering their strengths and weaknesses, unlocking the benefits of both accuracy and scalability.\nHowever, effectively leveraging multiple teachers remains an open challenge. Conflicting annota-\ntions from humans, LLMs, and domain-specific models can be difficult to reconcile, e.g., various\nteachers give conflict annotation for the same input samples. Also, directly aggregating predictions\nfrom LLMs and machine learning (ML) models as final labels can be problematic due to the in-\nherent noise in individual predictions, which can propagate and amplify in the inaccuracies after\naggregation.\nExisting multiple-teacher learning approaches typically leverage the aggregated output of an ensem-\nble of teachers [2] [3]. Most use a simple weighted average of teacher predictions, often with fixed\nor uniform weights [4][5]. More sophisticated approaches explore manually tuned weights [6] or\nlearn instance-specific teacher importance weights [7] [8]. Alternatively, some methods focus on\nselecting the \"best\" teacher for each instance, using strategies ranging from random selection [4]\nto reinforcement learning-based dynamic selection [9]. Specialized approaches, such as assigning\nteachers to distinct language pairs in multilingual neural machine translation [10], represent specific\ncases of domain-based teacher selection. However, a common limitation is the reliance on pre-\ndefined heuristics for teacher aggregation or selection, where these heuristics treat the aggregated\n∗Correspondence to huythong@google.com\narXiv:2411.12724v1  [cs.LG]  19 Nov 2024\n\nFigure 1: Conceptual diagram of (a) Conventional Ensemble with an heuristic to combine multiple\npredictions (b) Our proposed Heuristic-Free Multi-Teacher learning method.\nteacher output, often noisy or sub-optimal, as the ground truth for student training. This limitation\nmotivates our exploration of heuristic-free multi-teacher learning.\nThis work introduces Teacher2Task, a novel multi-teacher learning method that departs from the\nconventional heuristic-based approaches.\nOur proposed method explicitly incorporates teacher-\nspecific tokens into the input, allowing the model to internally differentiate between individual\nteacher labeling styles. For each teacher, we introduce an auxiliary task: predicting the teacher’s\nconfidence score across the entire input distribution. Given N teachers and ground truth labels, we\nconstruct N+1 training tasks: N auxiliary tasks focused on predicting each teacher’s confidence\nscores, and one primary task focused on learning the ground truth. By jointly learning from both the\nground truth and the diverse predictions of multiple teachers, the student model learns a more robust\nand nuanced understanding of the data distribution, effectively interpolating between the ground\ntruth and diverse teacher perspectives.\nThe proposed approach offers several key advantages. First, it is highly label-efficient, as each\nteacher prediction serves as an additional training sample. Second, by explicitly encoding teacher\nidentities within the input, the method eliminates the need for manual aggregation or selection\nheuristics. Finally, it mitigates the impact of potential label inaccuracies by treating teacher con-\nfidence scores as data for auxiliary tasks rather than as absolute ground truth. Experiments across\nvarious modalities and architectures demonstrate that Teacher2Task consistently benefits from the\ninclusion of more teachers, showcasing improved performance and robustness.\nOn another perspective, we extend distillation [11] from one teacher to multiple, with a relatively\nstraightforward path to scaling to \"almost infinity\" teachers. Deep Learning has thrived on scal-\ning rules, notably data-scaling and model-scaling. In a sense, we’re introducing teacher-scaling,\npotentially opening new avenues for innovation.\n2\nProposed Heuristic-Free Multi-Teacher Learning\n2.1\nMulti-Teacher Transformation\nTraditional ensemble methods [2][3] rely on aggregating predictions from multiple teacher models\nfor a given input (see Fig. 1a). However, this approach suffers from several drawbacks:(1) Imperfect\nPredictions: Individual teacher predictions and the aggregated result can both be inaccurate. (2)\nHeuristic Aggregation: Combining annotations often relies on manual and sub-optimal heuristics,\nand (3) Low Efficiency: Annotating a single sample requires running inference on all N teachers.\nProposed Method: To address these limitations, we propose Teacher2Task (Fig.1b). Instead of\ndirectly aggregating predictions, we transform the problem by incorporating teacher identity and\npredicted class as inputs to a model that predicts the teacher’s confidence score.\nMulti-Teacher Input = Teacher Identity + Original Input + Predicted Output Class\nMulti-Teacher Output = Confidence Score\n2\n\nThe proposed algorithm, though simple, introduces several key features that address inherent chal-\nlenges in existing multi-teacher learning approaches.\nIndividualized Teacher Tasks: For each input sample annotated by a teacher, we add special\nteacher tokens to the input and train the model to predict that teacher’s confidence score. This\nallows seamless integration of new teachers – each new teacher simply introduces a new auxiliary\ntask:\nTask for a new teacher: Predict the teacher’s confidence score for each input\nacross the entire input distribution.\nResolving Annotation Conflicts: Traditional multi-teacher learning often relies on heuristics like\nweighted aggregation or teacher selection to resolve conflicting annotations from multiple teachers\non the same input. Our algorithm circumvents this issue. By appending a unique teacher-specific\ntoken to each input, the model learns to differentiate between teachers and their individual labeling\nstyles, implicitly resolving conflicts.\nMitigating Label Noise: Another challenge in multi-teacher learning is the potential for noisy or\ninaccurate labels, both from individual teachers and from aggregated predictions. Existing methods\nfrequently use aggregated results as pseudo-labels for the student, propagating these inaccuracies.\nOur framework, however, treats teacher predictions as targets for auxiliary confidence prediction\ntasks. First, because all neurons in ML models are fixed, there exists an absolute mathematical\nformulate that transforms <inputs, output class> to <confident score>. Second, the true, human-\nannotated ground truth labels remain the primary learning objective. This distinction, enabled by\nthe unique teacher tokens, allows the model to learn from both the ground truth and the diverse\nperspectives of multiple teachers, improving its ability to predict with human-level confidence.\nImproved Label Efficiency: Our approach also offers significant gains in label efficiency. While\naggregation methods require multiple predictions per training sample, our method generates a multi-\nteacher training sample from each individual teacher’s prediction, reducing computational overhead.\n2.2\nConceptual Illustration\nFigure 2: Conceptual illustration for our proposed Multi-Teacher Learning. Our algorithm defines\nN + 1 learning tasks: N auxiliary tasks focused on predicting each teacher’s confidence scores, and\none primary task focused on learning the ground truth..\nFig.2 provides a conceptual illustration. Assuming a limited set of ground truth training data (black\nx symbols) and a need for broad generalization, we consider a scenario with N available teachers.\nOur algorithm defines N + 1 learning tasks: predicting the confidence scores of each teacher (N\ntasks for N teachers) and learning from the ground truth labels (one task).\nWith sufficient training data per teacher, the model learns to approximate each teacher’s perfor-\nmance characteristics (e.g., precision-recall curves). Even a less accurate teacher (e.g., Teacher 2)\nprovides valuable training data for its auxiliary prediction task. As Teacher2Task does not treat those\nless accurate labels as a part of the conventional aggregated labels, the method does not propagate\ninaccurate pseudo-labels for the main ground-truth learning task.\n3\n\nFigure 3: Examples of extracting Teacher2Task samples from (a) LLMs (b) classification models.\nFigure 4: Various model architectures that the proposed algorithm supports (a) Encoder-only (b)\nDual-Encoders (c ) (Multi-head) Classification.\nBecause ground truth data is limited, the model leverages its learned understanding of the more accu-\nrate teachers, combined with the high-quality ground truth, to potentially surpass the performance of\nany individual teacher. Our experimental results, showing in the Experiments section, demonstrate\nthis ability to interpolate towards an upper performance bound.\n2.3\nConstructing Multi-Teacher Training Samples\nWe demonstrate the construction of Teacher2Task training samples using Generative LLMs, multi-\nlabel classification models, and human annotators.\nGenerative LLMs: Large language models (LLMs), prompted with specific instructions, can serve\nas distinct teachers in our Multi-Teacher Learning framework. Open-ended prompts like \"What\nis the class of input?\" yield free-text predictions with associated confidence scores. Conversely,\nprompts like \"Is input semantically relevant to class name?\" pre-define the output class, focusing on\nthe \"Yes\" confidence score. Since each unique prompt effectively creates a new \"teacher\" from an\nLLM, we prefer to predefine a fixed prompt for each LLM, consistently generating Multi-Teacher\nLearning samples (see Fig. 3(a)) from each LLM inference.\nClassification Models: For each class, we use the input, teacher name, and the model’s predicted\nprobability for that class. We can generate multiple Multi-Teacher Learning samples from a single\ninference of multi-class models (See Fig. 3(b)).\nHuman Annotation: Human annotators provide relevance scores for input-class pairs. Averaging\nmultiple annotations yields the final score.\n2.4\nModel Training\nOur proposed multi-teacher learning supports various architectures (see Fig. 4), including: Encoder-\nOnly - The teacher identity can be appended directly to the input, Dual-Encoder - Teacher informa-\n4\n\ntion can be integrated into either the input or class topic, (Multi-Head) Classification - Adding\nteacher identity to the input distinguishes label sources.\nWe generally utilize MSE loss on the confidence score for robust training across architectures, but\nBinary Cross Entropy Loss also works well. At inference, Teacher2Task allows us to predict the con-\nfidence score that any trained teacher would assign to an input-class pair. To maximize performance,\nwe typically default to the most accurate teacher, often human annotators.\n3\nExperiments\nWe experiment on the open-vocabulary classification tasks for image and video understanding.\nOur goal is NOT to compare with public benchmarks, rather we’d like to demonstrate that our\nTeacher2Task algorithm effectively integrates knowledge from diverse sources to surpass that of\noriginal teachers.\n3.1\nTeachers\nOur framework’s flexibility allows for learning from diverse teacher types. To maximize knowledge\nacquisition from various label sources, we utilize four main teacher categories.\nLarge Language Models (LLMs): We employ PaLI [12][13] and Gemini [14] and follow Section\n2.3 to generate multi-teacher samples for image-topic pairs. We use slightly modified prompts for\nPaLI and Gemini:\nPaLI: Is “topic” the primary focus of this image?\nGemini: Answer strictly with YES/NO. Does this image provide visual evidence\nfor the topic “topic”?\nDomain-Specific Models: The domain-specific models excel within specific domains, providing\npotentially high-quality knowledge on a subset of the OpenVocab distribution. Compared to often\nexpensive LLMs, these models offer a cost-effective alternative to generate training samples and add\nknowledge on a smaller domain distribution.\nHumans: We utilize high-quality human annotations as a key source of ground truth for our model.\nSelf-Training Teachers: We leverage self-training [15] [16] as a teacher type within our framework.\nThis allows the model to iteratively learn from its own predictions on unlabeled data.\n3.2\nExperiment Setup\nModel Architectures: We utilize the compact T5/mT5 [17][18] architecture for our encoders. Im-\nages are directly converted to embedding, while videos are transformed into sequences of frame-\nlevel embedding before processing.\nDatasets: Our human-annotated dataset consists of approximately 2M image/video samples, labeled\nby trained operators to indicate the presence of specific visual evidence for given topics (e.g. evening\nwith loved ones, ancient civilizations, sriracha loaded fries, ninja warrior party, ...). We generates\n200M PaLI-labeled, 20M Gemini-labeled, and 300M domain-specific ML-labeled samples.\nAdding Teacher’s Identities: Teacher2Task leverages teacher identities as input features, enabling\nthe model to learn the distinct prediction patterns of each teacher for a given input-output pair. For\ntext-based models like T5/mT5 [17][18], we prepend the teacher’s identifier to the input text (e.g.,\nPaLI: input text or Gemini: input text). For non-textual models, such as a ResNet [19] for image\nclassification, a one-hot vector representing the teacher’s identity can be appended to the input.\nEvaluation: To assess open-vocabulary generalization, we perform a topic-split evaluation to ensure\nthat the majority of topics present in the evaluation set are unseen during training, offering a more\nchallenging and realistic evaluation compared to conventional random splits.\nTraining: We use a learning rate of 1e−3 with a batch size of 65k throughout our experiments.\nMetrics: We focus on precision and recall as key performance indicators, employing PR-AUC\n(Precision-Recall Area Under the Curve) as our primary evaluation metric.\n5\n\nFigure 5: Precision-Recall curves comparison among PaLI, Gemini, and our Multi-Teacher Learn-\ning model. At higher precision levels, our model outperforms Gemini due to its access to human\nannotations. At lower precision levels, it leverages the strengths of both PaLI and Gemini, achieving\nan outer bound on their respective PR curves.\n3.3\nResults\nThis section reports experiment results of a dual-encoder configuration with a 64-dimensional em-\nbedding space, a popular choice for large-scale tagging and high-traffic retrieval systems.\n3.3.1\nImage Classification\nOur Teacher2Task model learns image knowledge from five primary teachers: Human annotations,\nPaLI [13], Gemini [14], and 2 domain-specific models. Utilizing the specified prompts, zero-shot\nPaLI achieves 79.1% PR-AUC, while Gemini reaches 82.2% PR-AUC. Gemini, the larger model,\noutperforms PaLI, particularly in higher precision regions.\nOur Multi-Teacher Learning model, only 150M parameters, surpasses even its best ML teacher\n(Gemini) by achieving 84% PR-AUC. This highlights the benefits of combining diverse knowledge\nsources, even when some are imperfect.\nAnalyzing the PR curve reveals insightful trends (See Fig.5). At higher precision levels, our model\noutperforms Gemini due to its access to human annotations. At lower precision levels, it leverages\nthe strengths of both PaLI and Gemini, achieving an outer bound on their respective PR curves. This\nempirical results well match the conceptual illustration analyzed in Section 2.2.\n3.3.2\nVideo Classification\nTable 1: Video experiment results versus the number of teachers. Clear metric wins when scaling\nmore teachers to model training.\nTeachers\n# Teachers\nPR-AUC\nBaseline: 1 Video Teacher (Human)\n1\n75.6%\n1 Video Teacher (Human) + 5 Image Teachers\n6\n78.1%\n2 Video Teachers (Human + Self-Training) + 5 Image Teachers\n7\n80.0%\nOur baseline for video open-vocabulary classification, trained solely on human-annotated video-\ntopic pairs, achieves a PR-AUC of 75.6%. We demonstrate the effectiveness of our Teacher2Task\nalgorithm by scaling the number of teachers in two scenarios, both yielding metric improvements\n(see Table 1).\nImage-Teacher Learning:\nLeveraging cross-domain knowledge transfer, we incorporate five\nimage-based teachers (humans, LLMs and domain-specific image models), treating images as\n6\n\nsingle-frame videos. Combining these with a human-annotated video teacher within our proposed\nalgorithm increases the PR-AUC from 75.6% to 78.1%.\nSelf-Training Integration: Further expanding the number of useful teachers, we integrate a self-\ntraining teacher [15] [16] that iteratively generates pseudo-labels on unlabeled data. This expanded\nlearning, consisting of the original human-annotated video teacher, five image teachers, and the self-\ntraining teacher, further boosts the PR-AUC from 78.1% to 80%. Unlike conventional self-training,\nour approach benefits from the diverse expertise of multiple teachers and explicitly distinguishes\nself-training samples from ground truth labels through unique teacher identity tokens. Also, we\nobserve consistent performance gains with each self-training iteration.\n3.4\nAblation Study\nNext, we run ablation study of the algorithm with various embedding sizes, model architectures, and\nmodel sizes.\n3.4.1\nEmbedding Sizes\nThe PR-AUC increases for both image and video benchmarks when increasing the embedding size,\nas expected because larger embedding sizes imply better representation capabilities (see Table 2)\nTable 2: Image & video experiment results versus embedding sizes.\nEmbedding Size\nImage PR-AUC\nVideo PR-AUC\n16\n81.2%\n78.2%\n32\n83.1%\n79.5%\n64\n84.0%\n80.0%\n128\n84.6%\n80.3%\n256\n85.2%\n80.5%\n3.4.2\nModel Architectures\nWhen changing model architectures from dual-encoder to encoder-only, we see the encoder-only\nconfiguration slightly outperforms the dual-encoder configuration, explained by the encoder-only\ncan be viewed as dual-encoder with approaching infinite embedding size (see Table 4).\nTable 3: Comparison of results among model architectures.\nModel Architecture\nEmbedding Size\nImage PR-AUC\nVideo PR-AUC\nDual-Encoder\n64\n84.0%\n80.0%\nEncoder-Only\n–\n85.8%\n81.6%\n3.4.3\nModel Sizes\nVariations in model size result in minimal changes in performance. We attribute the algorithm’s\nstable performance across model sizes to its distillation-like approach, which enables smaller student\nmodels to achieve comparable results to larger teacher models (see Table 4).\nTable 4: Image & video experiment results among various model sizes.\nModel Size\nEmbedding Size\nImage PR-AUC\nVideo PR-AUC\n150M\n64\n84.0%\n80.0%\n300M\n64\n84.1%\n80.1%\n150M\n128\n84.6%\n80.3%\n300M\n128\n84.7%\n80.4%\n7\n\n4\nDiscussion\n4.1\nComparison to common ML algorithms\nThis section positions our Teacher2Task algorithm within the broader landscape of Deep Learning\nmethodologies, highlighting its advantages and connections to existing techniques.\nDistillation: While distillation [11] methods typically learn from a single, often stronger, teacher,\nTeacher2Task aggregates knowledge from multiple sources, including human annotations and many\ndiverse models. Our approach offers a scalable path to integrating knowledge from an \"almost\ninfinite\" number of teachers.\nEnsemble Methods: Ensemble methods [2][3] often suffer from limitations such as reliance on\nmanual aggregation heuristics, suboptimal aggregation strategies, and low annotation efficiency.\nTeacher2Task addresses these challenges by directly learning a unified model from multiple teachers\nwithin a principled framework.\nSelf-Training: Self-training [15] [16], a semi-supervised technique, iteratively trains teacher-\nstudent models on labeled and pseudo-labeled data. However, it can be susceptible to confirmation\nbias if pseudo-labels are inaccurate [20]. Teacher2Task mitigates this risk by separating out the\nsource of annotations in the inputs, so that self-training can be an additional teacher in our frame-\nwork.\nPretraining: While Self-Supervised Learning [21] [22] has been widely adopted for pretraining\nwith massive unlabeled datasets, our heuristic-free multi-teacher learning offers a compelling al-\nternative. From readily available LLMs, domain-specific models, and running those on unlabeled\ndata, we can generate billions or even trillions of multi-teacher labeled samples for effective pre-\ntraining, maximizing knowledge transfer by enabling the pretrained model to inherit insights from\nall its teachers.\n4.2\nComparison to Multi-Teacher algorithms\nThis section compares our proposed method to other multi-teacher approaches, dividing into three\nmajor categories: Weighted Aggregation, Teacher Selection, or Domain Separation approaches.\nWeighted Aggregation: While uniform weights for each teacher is the most common practice\n[4][5], research has explored more sophisticated weighting approaches, such as manually tuning\nweights [6] or learning instance-level teacher importance weights [7]. However, even advanced\nweighted averaging methods suffer from drawbacks: reduced annotation efficiency (requiring mul-\ntiple teacher labels per aggregated label), increased computational overhead for label aggregation,\nand potential imperfections in the heuristically aggregated labels.\nTeacher Selection: [4] randomly select a teacher for each mini-batch, while [9] employ reinforce-\nment learning for dynamic teacher selection. This can be considered a special case of weighted\naveraging, where one teacher’s weight is set to 1 and the others to 0. However, this approach still\nsuffers from increased computational overhead, potential for suboptimal teacher selection, and the\ninherent limitation of treating teacher predictions as ground truth.\nDomain Separation: [10] employ multi-teachers for multilingual neural machine translation train-\ning, where each teacher is assigned to a distinct language pair. The problem is a special case, where\nall teacher domains are rigidly separated by language pairs, removing the need for teacher’s aggre-\ngation or selection. In cases of potential annotation conflicts between teachers, the method might\nrequire further heuristics for domain selection.\nOur approach addresses the challenges of existing multi-teacher methods. By treating each teacher’s\nprediction as a training sample, we maximize annotation efficiency by leveraging all teacher labels.\nFurthermore, incorporating teacher identities as input features and re-framing the task as predicting\nindividual teacher labeling styles, we remove the need for weight aggregation, teacher selection, or\ndomain separation. By not viewing teacher confidence scores as ground-truth labels, our algorithm\neliminates the problem of imperfect aggregation heuristics.\n8\n\n5\nConclusion\nTeacher2Task offers a unified and scalable approach that leverages the strengths of multiple learning\nparadigms. By utilizing confidence scores from a potentially vast number of teachers, it extends the\nconcept of distillation while eliminating the need for explicit aggregation heuristics. The approach\nfacilitates the generation of massive training datasets from unlabeled data, proving particularly ef-\nfective for training compact, yet highly knowledgeable, student models that inherit the collective\nknowledge of all original teachers.\nReferences\n[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep\nconvolutional neural networks,” in Advances in Neural Information Processing Systems,\nF. Pereira, C. Burges, L. Bottou, and K. Weinberger, Eds., vol. 25.\nCurran Associates,\nInc., 2012. [Online]. Available:\nhttps://proceedings.neurips.cc/paper_files/paper/2012/file/\nc399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n[2] Z.-H. Zhou, Ensemble Methods: Foundations and Algorithms, 1st ed. Chapman & Hall/CRC,\n2012.\n[3] T. G. Dietterich, “Ensemble methods in machine learning,” in Proceedings of the First In-\nternational Workshop on Multiple Classifier Systems, ser. MCS ’00.\nBerlin, Heidelberg:\nSpringer-Verlag, 2000, p. 1–15.\n[4] T. Fukuda, M. Suzuki, G. Kurata, S. Thomas, J. Cui, and B. Ramabhadran, “Efficient\nknowledge distillation from an ensemble of teachers,” in Interspeech, 2017. [Online].\nAvailable: https://api.semanticscholar.org/CorpusID:30258763\n[5] M.-C. Wu, C.-T. Chiu, and K.-H. Wu, “Multi-teacher knowledge distillation for compressed\nvideo action recognition on deep neural networks,” in ICASSP 2019 - 2019 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 2202–2206.\n[6] Y. Chebotar and A. Waters, “Distilling knowledge from ensembles of neural networks for\nspeech recognition,” in Interspeech, 2016. [Online]. Available: https://api.semanticscholar.\norg/CorpusID:18195425\n[7] Y. Liu, W. Zhang, and J. Wang, “Adaptive multi-teacher multi-level knowledge distillation,”\nCoRR, vol. abs/2103.04062, 2021. [Online]. Available: https://arxiv.org/abs/2103.04062\n[8] C. Pham, T. Hoang, and T.-T. Do, “Collaborative multi-teacher knowledge distillation for\nlearning low bit-width deep neural networks,” in 2023 IEEE/CVF Winter Conference on Ap-\nplications of Computer Vision (WACV), 2023, pp. 6424–6432.\n[9] F. Yuan, L. Shou, J. Pei, W. Lin, M. Gong, Y. Fu, and D. Jiang, “Reinforced multi-teacher\nselection for knowledge distillation,” CoRR, vol. abs/2012.06048, 2020. [Online]. Available:\nhttps://arxiv.org/abs/2012.06048\n[10] X. Tan, Y. Ren, D. He, T. Qin, Z. Zhao, and T.-Y. Liu, “Multilingual neural machine translation\nwith knowledge distillation,” 2019. [Online]. Available: https://arxiv.org/abs/1902.10461\n[11] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” 2015.\n[Online]. Available: https://arxiv.org/abs/1503.02531\n[12] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman,\nA. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong,\nH. Akbari, G. Mishra, L. Xue, A. V. Thapliyal, J. Bradbury, W. Kuo, M. Seyedhosseini,\nC. Jia, B. K. Ayan, C. R. Ruiz, A. P. Steiner, A. Angelova, X. Zhai, N. Houlsby,\nand R. Soricut, “PaLI: A jointly-scaled multilingual language-image model,” in The\nEleventh International Conference on Learning Representations, 2023. [Online]. Available:\nhttps://openreview.net/forum?id=mWVoBz4W0u\n[13] e. a. Xi Chen, “Pali-x: On scaling up a multilingual vision and language model,” 2023.\n[Online]. Available: https://arxiv.org/abs/2305.18565\n9\n\n[14] G. Team, “Gemini:\nA family of highly capable multimodal models,” 2024. [Online].\nAvailable: https://arxiv.org/abs/2312.11805\n[15] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, “Self-training with noisy student improves ima-\ngenet classification,” in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2020, pp. 10 684–10 695.\n[16] H. Pham, Z. Dai, Q. Xie, and Q. V. Le, “Meta pseudo labels,” in 2021 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2021, pp. 11 552–11 563.\n[17] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu, “Exploring the limits of transfer learning with a unified text-to-text transformer,” J. Mach.\nLearn. Res., vol. 21, no. 1, jan 2020.\n[18] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and\nC. Raffel, “mt5:\nA massively multilingual pre-trained text-to-text transformer,” 2021.\n[Online]. Available: https://arxiv.org/abs/2010.11934\n[19] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in 2016\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770–778.\n[20] E. Arazo, D. Ortego, P. Albert, N. E. O’Connor, and K. McGuinness, “Pseudo-labeling\nand confirmation bias in deep semi-supervised learning,”\n2020. [Online]. Available:\nhttps://arxiv.org/abs/1908.02983\n[21] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional\ntransformers for language understanding,” arXiv preprint arXiv:1810.04805, 2018.\n[22] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learn-\ning of visual representations,” in International conference on machine learning. PMLR, 2020,\npp. 1597–1607.\n10",
    "pdf_filename": "Heuristic-Free_Multi-Teacher_Learning.pdf"
}