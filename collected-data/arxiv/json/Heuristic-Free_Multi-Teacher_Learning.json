{
    "title": "Heuristic-Free Multi-Teacher Learning",
    "abstract": "We introduce Teacher2Task, a novel framework for multi-teacher learning that eliminates the need for manual aggregation heuristics. Existing multi-teacher methods typically rely on such heuristics to combine predictions from multiple teachers, often resulting in sub-optimal aggregated labels and the propagation of aggregation errors. Teacher2Task addresses these limitations by introducing teacher-specific input tokens and reformulating the training process. Instead of relying on aggregated labels, the framework transforms the training data – con- sistingofgroundtruthlabelsandannotationsfromN teachers–intoN+1distinct tasks: N auxiliarytasksthatpredictthelabelingstylesoftheN individualteach- ers,andoneprimarytaskthatfocusesonthegroundtruthlabels. Thisapproach, drawing upon principles from multiple learning paradigms, demonstrates strong empiricalresultsacrossarangeofarchitectures,modalities,andtasks. 1 Introduction SinceAlexNet[1],adecadeofMLdevelopmenthasyieldedawealthofcapable\"teachers\".Humans as teachers, though expensive, provide near-perfect accuracy annotation. Large Language Models (LLMs)offerexcellentzero-shotcapabilities,generatinghigh-quality\"silver\"dataformanytasks. Domain-specificfoundationalmodelsserveasspecializedteacherswithintheirdomains. Anideal learningframeworkwouldenableMLmodelstolearneffectivelyfromallusefuldatasources,con- sideringtheirstrengthsandweaknesses,unlockingthebenefitsofbothaccuracyandscalability. However, effectively leveraging multiple teachers remains an open challenge. Conflicting annota- tions from humans, LLMs, and domain-specific models can be difficult to reconcile, e.g., various teachersgiveconflictannotationforthesameinputsamples. Also,directlyaggregatingpredictions from LLMs and machine learning (ML) models as final labels can be problematic due to the in- herent noise in individual predictions, which can propagate and amplify in the inaccuracies after aggregation. Existingmultiple-teacherlearningapproachestypicallyleveragetheaggregatedoutputofanensem- bleofteachers[2][3]. Mostuseasimpleweightedaverageofteacherpredictions,oftenwithfixed or uniform weights [4][5]. More sophisticated approaches explore manually tuned weights [6] or learn instance-specific teacher importance weights [7] [8]. Alternatively, some methods focus on selecting the \"best\" teacher for each instance, using strategies ranging from random selection [4] to reinforcement learning-based dynamic selection [9]. Specialized approaches, such as assigning teacherstodistinctlanguagepairsinmultilingualneuralmachinetranslation[10],representspecific cases of domain-based teacher selection. However, a common limitation is the reliance on pre- defined heuristics for teacher aggregation or selection, where these heuristics treat the aggregated ∗Correspondencetohuythong@google.com 4202 voN 91 ]GL.sc[ 1v42721.1142:viXra",
    "body": "Heuristic-Free Multi-Teacher Learning\nHuyThongNguyen∗ En-HungChu LenordMelvix JazonJiao ChunglinWen\nGoogle Google Google Google Google\nBenjaminLouie\nGoogle\nAbstract\nWe introduce Teacher2Task, a novel framework for multi-teacher learning that\neliminates the need for manual aggregation heuristics. Existing multi-teacher\nmethods typically rely on such heuristics to combine predictions from multiple\nteachers, often resulting in sub-optimal aggregated labels and the propagation\nof aggregation errors. Teacher2Task addresses these limitations by introducing\nteacher-specific input tokens and reformulating the training process. Instead of\nrelying on aggregated labels, the framework transforms the training data – con-\nsistingofgroundtruthlabelsandannotationsfromN teachers–intoN+1distinct\ntasks: N auxiliarytasksthatpredictthelabelingstylesoftheN individualteach-\ners,andoneprimarytaskthatfocusesonthegroundtruthlabels. Thisapproach,\ndrawing upon principles from multiple learning paradigms, demonstrates strong\nempiricalresultsacrossarangeofarchitectures,modalities,andtasks.\n1 Introduction\nSinceAlexNet[1],adecadeofMLdevelopmenthasyieldedawealthofcapable\"teachers\".Humans\nas teachers, though expensive, provide near-perfect accuracy annotation. Large Language Models\n(LLMs)offerexcellentzero-shotcapabilities,generatinghigh-quality\"silver\"dataformanytasks.\nDomain-specificfoundationalmodelsserveasspecializedteacherswithintheirdomains. Anideal\nlearningframeworkwouldenableMLmodelstolearneffectivelyfromallusefuldatasources,con-\nsideringtheirstrengthsandweaknesses,unlockingthebenefitsofbothaccuracyandscalability.\nHowever, effectively leveraging multiple teachers remains an open challenge. Conflicting annota-\ntions from humans, LLMs, and domain-specific models can be difficult to reconcile, e.g., various\nteachersgiveconflictannotationforthesameinputsamples. Also,directlyaggregatingpredictions\nfrom LLMs and machine learning (ML) models as final labels can be problematic due to the in-\nherent noise in individual predictions, which can propagate and amplify in the inaccuracies after\naggregation.\nExistingmultiple-teacherlearningapproachestypicallyleveragetheaggregatedoutputofanensem-\nbleofteachers[2][3]. Mostuseasimpleweightedaverageofteacherpredictions,oftenwithfixed\nor uniform weights [4][5]. More sophisticated approaches explore manually tuned weights [6] or\nlearn instance-specific teacher importance weights [7] [8]. Alternatively, some methods focus on\nselecting the \"best\" teacher for each instance, using strategies ranging from random selection [4]\nto reinforcement learning-based dynamic selection [9]. Specialized approaches, such as assigning\nteacherstodistinctlanguagepairsinmultilingualneuralmachinetranslation[10],representspecific\ncases of domain-based teacher selection. However, a common limitation is the reliance on pre-\ndefined heuristics for teacher aggregation or selection, where these heuristics treat the aggregated\n∗Correspondencetohuythong@google.com\n4202\nvoN\n91\n]GL.sc[\n1v42721.1142:viXra\nFigure1: Conceptualdiagramof(a)ConventionalEnsemblewithanheuristictocombinemultiple\npredictions(b)OurproposedHeuristic-FreeMulti-Teacherlearningmethod.\nteacheroutput,oftennoisyorsub-optimal,asthegroundtruthforstudenttraining. Thislimitation\nmotivatesourexplorationofheuristic-freemulti-teacherlearning.\nThis work introduces Teacher2Task, a novel multi-teacher learning method that departs from the\nconventional heuristic-based approaches. Our proposed method explicitly incorporates teacher-\nspecific tokens into the input, allowing the model to internally differentiate between individual\nteacher labeling styles. For each teacher, we introduce an auxiliary task: predicting the teacher’s\nconfidencescoreacrosstheentireinputdistribution. GivenN teachersandgroundtruthlabels,we\nconstruct N+1 training tasks: N auxiliary tasks focused on predicting each teacher’s confidence\nscores,andoneprimarytaskfocusedonlearningthegroundtruth. Byjointlylearningfromboththe\ngroundtruthandthediversepredictionsofmultipleteachers,thestudentmodellearnsamorerobust\nand nuanced understanding of the data distribution, effectively interpolating between the ground\ntruthanddiverseteacherperspectives.\nThe proposed approach offers several key advantages. First, it is highly label-efficient, as each\nteacher prediction serves as an additional training sample. Second, by explicitly encoding teacher\nidentities within the input, the method eliminates the need for manual aggregation or selection\nheuristics. Finally, it mitigates the impact of potential label inaccuracies by treating teacher con-\nfidence scores as data for auxiliary tasks rather than as absolute ground truth. Experiments across\nvarious modalities and architectures demonstrate that Teacher2Task consistently benefits from the\ninclusionofmoreteachers,showcasingimprovedperformanceandrobustness.\nOn another perspective, we extend distillation [11] from one teacher to multiple, with a relatively\nstraightforward path to scaling to \"almost infinity\" teachers. Deep Learning has thrived on scal-\ning rules, notably data-scaling and model-scaling. In a sense, we’re introducing teacher-scaling,\npotentiallyopeningnewavenuesforinnovation.\n2 ProposedHeuristic-FreeMulti-TeacherLearning\n2.1 Multi-TeacherTransformation\nTraditionalensemblemethods[2][3]relyonaggregatingpredictionsfrommultipleteachermodels\nforagiveninput(seeFig.1a).However,thisapproachsuffersfromseveraldrawbacks:(1)Imperfect\nPredictions: Individual teacher predictions and the aggregated result can both be inaccurate. (2)\nHeuristic Aggregation: Combining annotations often relies on manual and sub-optimal heuristics,\nand(3)LowEfficiency: AnnotatingasinglesamplerequiresrunninginferenceonallNteachers.\nProposed Method: To address these limitations, we propose Teacher2Task (Fig.1b). Instead of\ndirectly aggregating predictions, we transform the problem by incorporating teacher identity and\npredictedclassasinputstoamodelthatpredictstheteacher’sconfidencescore.\nMulti-TeacherInput=TeacherIdentity+OriginalInput+PredictedOutputClass\nMulti-TeacherOutput=ConfidenceScore\n2\nTheproposedalgorithm,thoughsimple,introducesseveralkeyfeaturesthataddressinherentchal-\nlengesinexistingmulti-teacherlearningapproaches.\nIndividualized Teacher Tasks: For each input sample annotated by a teacher, we add special\nteacher tokens to the input and train the model to predict that teacher’s confidence score. This\nallowsseamlessintegrationofnewteachers–eachnewteachersimplyintroducesanewauxiliary\ntask:\nTask for a new teacher: Predict the teacher’s confidence score for each input\nacrosstheentireinputdistribution.\nResolving Annotation Conflicts: Traditionalmulti-teacher learning often relies onheuristics like\nweightedaggregationorteacherselectiontoresolveconflictingannotationsfrommultipleteachers\non the same input. Our algorithm circumvents this issue. By appending a unique teacher-specific\ntokentoeachinput,themodellearnstodifferentiatebetweenteachersandtheirindividuallabeling\nstyles,implicitlyresolvingconflicts.\nMitigating Label Noise: Another challenge in multi-teacher learning is the potential for noisy or\ninaccuratelabels,bothfromindividualteachersandfromaggregatedpredictions. Existingmethods\nfrequently use aggregated results as pseudo-labels for the student, propagating these inaccuracies.\nOur framework, however, treats teacher predictions as targets for auxiliary confidence prediction\ntasks. First, because all neurons in ML models are fixed, there exists an absolute mathematical\nformulate that transforms <inputs, output class> to <confident score>. Second, the true, human-\nannotated ground truth labels remain the primary learning objective. This distinction, enabled by\nthe unique teacher tokens, allows the model to learn from both the ground truth and the diverse\nperspectivesofmultipleteachers,improvingitsabilitytopredictwithhuman-levelconfidence.\nImproved Label Efficiency: Our approach also offers significant gains in label efficiency. While\naggregationmethodsrequiremultiplepredictionspertrainingsample,ourmethodgeneratesamulti-\nteachertrainingsamplefromeachindividualteacher’sprediction,reducingcomputationaloverhead.\n2.2 ConceptualIllustration\nFigure2: ConceptualillustrationforourproposedMulti-TeacherLearning. Ouralgorithmdefines\nN+1learningtasks: N auxiliarytasksfocusedonpredictingeachteacher’sconfidencescores,and\noneprimarytaskfocusedonlearningthegroundtruth..\nFig.2providesaconceptualillustration. Assumingalimitedsetofgroundtruthtrainingdata(black\nxsymbols)andaneedforbroadgeneralization, weconsiderascenariowithNavailableteachers.\nOur algorithm defines N + 1 learning tasks: predicting the confidence scores of each teacher (N\ntasksforNteachers)andlearningfromthegroundtruthlabels(onetask).\nWith sufficient training data per teacher, the model learns to approximate each teacher’s perfor-\nmancecharacteristics(e.g., precision-recallcurves). Evenalessaccurateteacher(e.g., Teacher2)\nprovidesvaluabletrainingdataforitsauxiliarypredictiontask.AsTeacher2Taskdoesnottreatthose\nlessaccuratelabelsasapartoftheconventionalaggregatedlabels,themethoddoesnotpropagate\ninaccuratepseudo-labelsforthemainground-truthlearningtask.\n3\nFigure3: ExamplesofextractingTeacher2Tasksamplesfrom(a)LLMs(b)classificationmodels.\nFigure 4: Various model architectures that the proposed algorithm supports (a) Encoder-only (b)\nDual-Encoders(c)(Multi-head)Classification.\nBecausegroundtruthdataislimited,themodelleveragesitslearnedunderstandingofthemoreaccu-\nrateteachers,combinedwiththehigh-qualitygroundtruth,topotentiallysurpasstheperformanceof\nanyindividualteacher. Ourexperimentalresults,showingintheExperimentssection,demonstrate\nthisabilitytointerpolatetowardsanupperperformancebound.\n2.3 ConstructingMulti-TeacherTrainingSamples\nWedemonstratetheconstructionofTeacher2TasktrainingsamplesusingGenerativeLLMs,multi-\nlabelclassificationmodels,andhumanannotators.\nGenerativeLLMs: Largelanguagemodels(LLMs),promptedwithspecificinstructions,canserve\nas distinct teachers in our Multi-Teacher Learning framework. Open-ended prompts like \"What\nis the class of input?\" yield free-text predictions with associated confidence scores. Conversely,\npromptslike\"Isinputsemanticallyrelevanttoclassname?\"pre-definetheoutputclass,focusingon\nthe\"Yes\"confidencescore. Sinceeachuniqueprompteffectivelycreatesanew\"teacher\"froman\nLLM, we prefer to predefine a fixed prompt for each LLM, consistently generating Multi-Teacher\nLearningsamples(seeFig. 3(a))fromeachLLMinference.\nClassificationModels: Foreachclass, weusetheinput, teachername, andthemodel’spredicted\nprobabilityforthatclass. WecangeneratemultipleMulti-TeacherLearningsamplesfromasingle\ninferenceofmulti-classmodels(SeeFig. 3(b)).\nHumanAnnotation: Humanannotatorsproviderelevancescoresforinput-classpairs. Averaging\nmultipleannotationsyieldsthefinalscore.\n2.4 ModelTraining\nOurproposedmulti-teacherlearningsupportsvariousarchitectures(seeFig.4),including:Encoder-\nOnly-Theteacheridentitycanbeappendeddirectlytotheinput,Dual-Encoder-Teacherinforma-\n4\ntion can be integrated into either the input or class topic, (Multi-Head) Classification - Adding\nteacheridentitytotheinputdistinguisheslabelsources.\nWegenerallyutilizeMSElossontheconfidencescoreforrobusttrainingacrossarchitectures,but\nBinaryCrossEntropyLossalsoworkswell.Atinference,Teacher2Taskallowsustopredictthecon-\nfidencescorethatanytrainedteacherwouldassigntoaninput-classpair.Tomaximizeperformance,\nwetypicallydefaulttothemostaccurateteacher,oftenhumanannotators.\n3 Experiments\nWe experiment on the open-vocabulary classification tasks for image and video understanding.\nOur goal is NOT to compare with public benchmarks, rather we’d like to demonstrate that our\nTeacher2Task algorithm effectively integrates knowledge from diverse sources to surpass that of\noriginalteachers.\n3.1 Teachers\nOurframework’sflexibilityallowsforlearningfromdiverseteachertypes. Tomaximizeknowledge\nacquisitionfromvariouslabelsources,weutilizefourmainteachercategories.\nLargeLanguageModels(LLMs): WeemployPaLI[12][13]andGemini[14]andfollowSection\n2.3togeneratemulti-teachersamplesforimage-topicpairs. Weuseslightlymodifiedpromptsfor\nPaLIandGemini:\nPaLI:Is“topic”theprimaryfocusofthisimage?\nGemini: AnswerstrictlywithYES/NO.Doesthisimageprovidevisualevidence\nforthetopic“topic”?\nDomain-Specific Models: The domain-specific models excel within specific domains, providing\npotentiallyhigh-qualityknowledgeonasubsetoftheOpenVocabdistribution. Comparedtooften\nexpensiveLLMs,thesemodelsofferacost-effectivealternativetogeneratetrainingsamplesandadd\nknowledgeonasmallerdomaindistribution.\nHumans: Weutilizehigh-qualityhumanannotationsasakeysourceofgroundtruthforourmodel.\nSelf-TrainingTeachers:Weleverageself-training[15][16]asateachertypewithinourframework.\nThisallowsthemodeltoiterativelylearnfromitsownpredictionsonunlabeleddata.\n3.2 ExperimentSetup\nModelArchitectures: WeutilizethecompactT5/mT5[17][18]architectureforourencoders. Im-\nages are directly converted to embedding, while videos are transformed into sequences of frame-\nlevelembeddingbeforeprocessing.\nDatasets:Ourhuman-annotateddatasetconsistsofapproximately2Mimage/videosamples,labeled\nbytrainedoperatorstoindicatethepresenceofspecificvisualevidenceforgiventopics(e.g.evening\nwith loved ones, ancient civilizations, sriracha loaded fries, ninja warrior party, ...). We generates\n200MPaLI-labeled,20MGemini-labeled,and300Mdomain-specificML-labeledsamples.\nAddingTeacher’sIdentities: Teacher2Taskleveragesteacheridentitiesasinputfeatures,enabling\nthemodeltolearnthedistinctpredictionpatternsofeachteacherforagiveninput-outputpair. For\ntext-basedmodelslikeT5/mT5[17][18],weprependtheteacher’sidentifiertotheinputtext(e.g.,\nPaLI: input text or Gemini: input text). For non-textual models, such as a ResNet [19] for image\nclassification,aone-hotvectorrepresentingtheteacher’sidentitycanbeappendedtotheinput.\nEvaluation:Toassessopen-vocabularygeneralization,weperformatopic-splitevaluationtoensure\nthatthemajorityoftopicspresentintheevaluationsetareunseenduringtraining,offeringamore\nchallengingandrealisticevaluationcomparedtoconventionalrandomsplits.\nTraining: Weusealearningrateof1e−3withabatchsizeof65kthroughoutourexperiments.\nMetrics: We focus on precision and recall as key performance indicators, employing PR-AUC\n(Precision-RecallAreaUndertheCurve)asourprimaryevaluationmetric.\n5\nFigure5: Precision-RecallcurvescomparisonamongPaLI,Gemini,andourMulti-TeacherLearn-\ning model. At higher precision levels, our model outperforms Gemini due to its access to human\nannotations.Atlowerprecisionlevels,itleveragesthestrengthsofbothPaLIandGemini,achieving\nanouterboundontheirrespectivePRcurves.\n3.3 Results\nThissectionreportsexperimentresultsofadual-encoderconfigurationwitha64-dimensionalem-\nbeddingspace,apopularchoiceforlarge-scaletaggingandhigh-trafficretrievalsystems.\n3.3.1 ImageClassification\nOurTeacher2Taskmodellearnsimageknowledgefromfiveprimaryteachers: Humanannotations,\nPaLI [13], Gemini [14], and 2 domain-specific models. Utilizing the specified prompts, zero-shot\nPaLI achieves 79.1% PR-AUC, while Gemini reaches 82.2% PR-AUC. Gemini, the larger model,\noutperformsPaLI,particularlyinhigherprecisionregions.\nOur Multi-Teacher Learning model, only 150M parameters, surpasses even its best ML teacher\n(Gemini)byachieving84%PR-AUC.Thishighlightsthebenefitsofcombiningdiverseknowledge\nsources,evenwhensomeareimperfect.\nAnalyzingthePRcurverevealsinsightfultrends(SeeFig.5). Athigherprecisionlevels,ourmodel\noutperformsGeminiduetoitsaccesstohumanannotations. Atlowerprecisionlevels,itleverages\nthestrengthsofbothPaLIandGemini,achievinganouterboundontheirrespectivePRcurves.This\nempiricalresultswellmatchtheconceptualillustrationanalyzedinSection2.2.\n3.3.2 VideoClassification\nTable 1: Video experiment results versus the number of teachers. Clear metric wins when scaling\nmoreteacherstomodeltraining.\nTeachers #Teachers PR-AUC\nBaseline: 1VideoTeacher(Human) 1 75.6%\n1VideoTeacher(Human)+5ImageTeachers 6 78.1%\n2VideoTeachers(Human+Self-Training)+5ImageTeachers 7 80.0%\nOur baseline for video open-vocabulary classification, trained solely on human-annotated video-\ntopic pairs, achieves a PR-AUC of 75.6%. We demonstrate the effectiveness of our Teacher2Task\nalgorithm by scaling the number of teachers in two scenarios, both yielding metric improvements\n(seeTable1).\nImage-Teacher Learning: Leveraging cross-domain knowledge transfer, we incorporate five\nimage-based teachers (humans, LLMs and domain-specific image models), treating images as\n6\nsingle-framevideos. Combiningthesewithahuman-annotatedvideoteacherwithinourproposed\nalgorithmincreasesthePR-AUCfrom75.6%to78.1%.\nSelf-Training Integration: Further expanding the number of useful teachers, we integrate a self-\ntrainingteacher[15][16]thatiterativelygeneratespseudo-labelsonunlabeleddata. Thisexpanded\nlearning,consistingoftheoriginalhuman-annotatedvideoteacher,fiveimageteachers,andtheself-\ntrainingteacher,furtherbooststhePR-AUCfrom78.1%to80%. Unlikeconventionalself-training,\nour approach benefits from the diverse expertise of multiple teachers and explicitly distinguishes\nself-training samples from ground truth labels through unique teacher identity tokens. Also, we\nobserveconsistentperformancegainswitheachself-trainingiteration.\n3.4 AblationStudy\nNext,werunablationstudyofthealgorithmwithvariousembeddingsizes,modelarchitectures,and\nmodelsizes.\n3.4.1 EmbeddingSizes\nThePR-AUCincreasesforbothimageandvideobenchmarkswhenincreasingtheembeddingsize,\nasexpectedbecauselargerembeddingsizesimplybetterrepresentationcapabilities(seeTable2)\nTable2: Image&videoexperimentresultsversusembeddingsizes.\nEmbeddingSize ImagePR-AUC VideoPR-AUC\n16 81.2% 78.2%\n32 83.1% 79.5%\n64 84.0% 80.0%\n128 84.6% 80.3%\n256 85.2% 80.5%\n3.4.2 ModelArchitectures\nWhen changing model architectures from dual-encoder to encoder-only, we see the encoder-only\nconfiguration slightly outperforms the dual-encoder configuration, explained by the encoder-only\ncanbeviewedasdual-encoderwithapproachinginfiniteembeddingsize(seeTable4).\nTable3: Comparisonofresultsamongmodelarchitectures.\nModelArchitecture EmbeddingSize ImagePR-AUC VideoPR-AUC\nDual-Encoder 64 84.0% 80.0%\nEncoder-Only – 85.8% 81.6%\n3.4.3 ModelSizes\nVariations in model size result in minimal changes in performance. We attribute the algorithm’s\nstableperformanceacrossmodelsizestoitsdistillation-likeapproach,whichenablessmallerstudent\nmodelstoachievecomparableresultstolargerteachermodels(seeTable4).\nTable4: Image&videoexperimentresultsamongvariousmodelsizes.\nModelSize EmbeddingSize ImagePR-AUC VideoPR-AUC\n150M 64 84.0% 80.0%\n300M 64 84.1% 80.1%\n150M 128 84.6% 80.3%\n300M 128 84.7% 80.4%\n7\n4 Discussion\n4.1 ComparisontocommonMLalgorithms\nThissectionpositionsourTeacher2TaskalgorithmwithinthebroaderlandscapeofDeepLearning\nmethodologies,highlightingitsadvantagesandconnectionstoexistingtechniques.\nDistillation: While distillation [11] methods typically learn from a single, often stronger, teacher,\nTeacher2Taskaggregatesknowledgefrommultiplesources,includinghumanannotationsandmany\ndiverse models. Our approach offers a scalable path to integrating knowledge from an \"almost\ninfinite\"numberofteachers.\nEnsemble Methods: Ensemble methods [2][3] often suffer from limitations such as reliance on\nmanual aggregation heuristics, suboptimal aggregation strategies, and low annotation efficiency.\nTeacher2Taskaddressesthesechallengesbydirectlylearningaunifiedmodelfrommultipleteachers\nwithinaprincipledframework.\nSelf-Training: Self-training [15] [16], a semi-supervised technique, iteratively trains teacher-\nstudentmodelsonlabeledandpseudo-labeleddata. However,itcanbesusceptibletoconfirmation\nbias if pseudo-labels are inaccurate [20]. Teacher2Task mitigates this risk by separating out the\nsourceofannotationsintheinputs, sothatself-trainingcanbeanadditionalteacherinourframe-\nwork.\nPretraining: While Self-Supervised Learning [21] [22] has been widely adopted for pretraining\nwith massive unlabeled datasets, our heuristic-free multi-teacher learning offers a compelling al-\nternative. From readily available LLMs, domain-specific models, and running those on unlabeled\ndata, we can generate billions or even trillions of multi-teacher labeled samples for effective pre-\ntraining, maximizingknowledgetransferbyenablingthepretrainedmodeltoinheritinsightsfrom\nallitsteachers.\n4.2 ComparisontoMulti-Teacheralgorithms\nThissectioncomparesourproposedmethodtoothermulti-teacherapproaches, dividingintothree\nmajorcategories: WeightedAggregation,TeacherSelection,orDomainSeparationapproaches.\nWeighted Aggregation: While uniform weights for each teacher is the most common practice\n[4][5], research has explored more sophisticated weighting approaches, such as manually tuning\nweights [6] or learning instance-level teacher importance weights [7]. However, even advanced\nweightedaveragingmethodssufferfromdrawbacks: reducedannotationefficiency(requiringmul-\ntiple teacher labels per aggregated label), increased computational overhead for label aggregation,\nandpotentialimperfectionsintheheuristicallyaggregatedlabels.\nTeacherSelection: [4]randomlyselectateacherforeachmini-batch,while[9]employreinforce-\nment learning for dynamic teacher selection. This can be considered a special case of weighted\naveraging, where oneteacher’s weightisset to1 andtheothers to0. However, thisapproach still\nsuffersfromincreasedcomputationaloverhead, potentialforsuboptimalteacherselection, andthe\ninherentlimitationoftreatingteacherpredictionsasgroundtruth.\nDomainSeparation: [10]employmulti-teachersformultilingualneuralmachinetranslationtrain-\ning,whereeachteacherisassignedtoadistinctlanguagepair. Theproblemisaspecialcase,where\nallteacherdomainsarerigidlyseparatedbylanguagepairs,removingtheneedforteacher’saggre-\ngation or selection. In cases of potential annotation conflicts between teachers, the method might\nrequirefurtherheuristicsfordomainselection.\nOurapproachaddressesthechallengesofexistingmulti-teachermethods.Bytreatingeachteacher’s\npredictionasatrainingsample,wemaximizeannotationefficiencybyleveragingallteacherlabels.\nFurthermore,incorporatingteacheridentitiesasinputfeaturesandre-framingthetaskaspredicting\nindividualteacherlabelingstyles,weremovetheneedforweightaggregation,teacherselection,or\ndomainseparation. Bynotviewingteacherconfidencescoresasground-truthlabels,ouralgorithm\neliminatestheproblemofimperfectaggregationheuristics.\n8\n5 Conclusion\nTeacher2Taskoffersaunifiedandscalableapproachthatleveragesthestrengthsofmultiplelearning\nparadigms. Byutilizingconfidencescoresfromapotentiallyvastnumberofteachers,itextendsthe\nconceptofdistillationwhileeliminatingtheneedforexplicitaggregationheuristics. Theapproach\nfacilitatesthegenerationofmassivetrainingdatasetsfromunlabeleddata, provingparticularlyef-\nfective for training compact, yet highly knowledgeable, student models that inherit the collective\nknowledgeofalloriginalteachers.\nReferences\n[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep\nconvolutional neural networks,” in Advances in Neural Information Processing Systems,\nF. Pereira, C. Burges, L. Bottou, and K. Weinberger, Eds., vol. 25. Curran Associates,\nInc., 2012. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2012/file/\nc399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n[2] Z.-H.Zhou,EnsembleMethods:FoundationsandAlgorithms,1sted. Chapman&Hall/CRC,\n2012.\n[3] T. G. Dietterich, “Ensemble methods in machine learning,” in Proceedings of the First In-\nternational Workshop on Multiple Classifier Systems, ser. MCS ’00. Berlin, Heidelberg:\nSpringer-Verlag,2000,p.1–15.\n[4] T. Fukuda, M. Suzuki, G. Kurata, S. Thomas, J. Cui, and B. Ramabhadran, “Efficient\nknowledge distillation from an ensemble of teachers,” in Interspeech, 2017. [Online].\nAvailable: https://api.semanticscholar.org/CorpusID:30258763\n[5] M.-C. Wu, C.-T. Chiu, and K.-H. Wu, “Multi-teacher knowledge distillation for compressed\nvideoactionrecognitionondeepneuralnetworks,”inICASSP2019-2019IEEEInternational\nConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),2019,pp.2202–2206.\n[6] Y. Chebotar and A. Waters, “Distilling knowledge from ensembles of neural networks for\nspeech recognition,” in Interspeech, 2016. [Online]. Available: https://api.semanticscholar.\norg/CorpusID:18195425\n[7] Y. Liu, W. Zhang, and J. Wang, “Adaptive multi-teacher multi-level knowledge distillation,”\nCoRR,vol.abs/2103.04062,2021.[Online].Available: https://arxiv.org/abs/2103.04062\n[8] C. Pham, T. Hoang, and T.-T. Do, “Collaborative multi-teacher knowledge distillation for\nlearning low bit-width deep neural networks,” in 2023 IEEE/CVF Winter Conference on Ap-\nplicationsofComputerVision(WACV),2023,pp.6424–6432.\n[9] F. Yuan, L. Shou, J. Pei, W. Lin, M. Gong, Y. Fu, and D. Jiang, “Reinforced multi-teacher\nselection for knowledge distillation,” CoRR, vol. abs/2012.06048, 2020. [Online]. Available:\nhttps://arxiv.org/abs/2012.06048\n[10] X.Tan,Y.Ren,D.He,T.Qin,Z.Zhao,andT.-Y.Liu,“Multilingualneuralmachinetranslation\nwithknowledgedistillation,”2019.[Online].Available: https://arxiv.org/abs/1902.10461\n[11] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” 2015.\n[Online].Available: https://arxiv.org/abs/1503.02531\n[12] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman,\nA. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong,\nH. Akbari, G. Mishra, L. Xue, A. V. Thapliyal, J. Bradbury, W. Kuo, M. Seyedhosseini,\nC. Jia, B. K. Ayan, C. R. Ruiz, A. P. Steiner, A. Angelova, X. Zhai, N. Houlsby,\nand R. Soricut, “PaLI: A jointly-scaled multilingual language-image model,” in The\nEleventh International Conference on Learning Representations, 2023. [Online]. Available:\nhttps://openreview.net/forum?id=mWVoBz4W0u\n[13] e. a. Xi Chen, “Pali-x: On scaling up a multilingual vision and language model,” 2023.\n[Online].Available: https://arxiv.org/abs/2305.18565\n9\n[14] G. Team, “Gemini: A family of highly capable multimodal models,” 2024. [Online].\nAvailable: https://arxiv.org/abs/2312.11805\n[15] Q.Xie,M.-T.Luong,E.Hovy,andQ.V.Le,“Self-trainingwithnoisystudentimprovesima-\ngenetclassification,”in2020IEEE/CVFConferenceonComputerVisionandPatternRecog-\nnition(CVPR),2020,pp.10684–10695.\n[16] H.Pham,Z.Dai,Q.Xie,andQ.V.Le,“Metapseudolabels,”in2021IEEE/CVFConference\nonComputerVisionandPatternRecognition(CVPR),2021,pp.11552–11563.\n[17] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu,“Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer,”J.Mach.\nLearn.Res.,vol.21,no.1,jan2020.\n[18] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and\nC. Raffel, “mt5: A massively multilingual pre-trained text-to-text transformer,” 2021.\n[Online].Available: https://arxiv.org/abs/2010.11934\n[19] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimagerecognition,”in2016\nIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2016,pp.770–778.\n[20] E. Arazo, D. Ortego, P. Albert, N. E. O’Connor, and K. McGuinness, “Pseudo-labeling\nand confirmation bias in deep semi-supervised learning,” 2020. [Online]. Available:\nhttps://arxiv.org/abs/1908.02983\n[21] J.Devlin, M.-W.Chang, K.Lee, andK.Toutanova, “Bert: Pre-trainingofdeepbidirectional\ntransformersforlanguageunderstanding,”arXivpreprintarXiv:1810.04805,2018.\n[22] T.Chen,S.Kornblith,M.Norouzi,andG.Hinton,“Asimpleframeworkforcontrastivelearn-\ningofvisualrepresentations,”inInternationalconferenceonmachinelearning. PMLR,2020,\npp.1597–1607.\n10",
    "pdf_filename": "Heuristic-Free_Multi-Teacher_Learning.pdf"
}