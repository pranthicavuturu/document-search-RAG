{
    "title": "CATCH Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMs",
    "abstract": "Large Vision-Language Model (LVLM) systems have demonstrated impressive vision-language reasoning capabilities but suffer from pervasive and severe hal- lucination issues, posing significant risks in critical domains such as healthcare and autonomous systems. Despite previous efforts to mitigate hallucinations, a persistent issue remains: visual defect from vision-language misalignment, cre- ating a bottleneck in visual processing capacity. To address this challenge, we develop Complementary Adaptive Token-level Contrastive Decoding to Miti- gate Hallucinations in LVLMs (CATCH), based on the Information Bottleneck theory. CATCH introduces Complementary Visual Decoupling (CVD) for visual information separation, Non-Visual Screening (NVS) for hallucination detec- tion, and Adaptive Token-level Contrastive Decoding (ATCD) for hallucination mitigation. CATCH addresses issues related to visual defects that cause dimin- ished fine-grained feature perception and cumulative hallucinations in open-ended 1 arXiv:2411.12713v1  [cs.CV]  19 Nov 2024",
    "body": "CATCH: Complementary Adaptive\nToken-level Contrastive Decoding to Mitigate\nHallucinations in LVLMs\nZhehan Kan1,2, Ce Zhang3, Zihan Liao4, Yapeng Tian4,\nWenming Yang1,2*, Junyuan Xiao1, Xu Li5, Dongmei Jiang2,\nYaowei Wang2, Qingmin Liao1,2\n1*Tsinghua University.\n2Pengcheng Laboratory.\n3Carnegie Mellon University.\n4University of California, Berkeley.\n4The University of Texas at Dallas.\n5Chang’an University.\n*Corresponding author(s). E-mail(s): yang.wenming@sz.tsinghua.edu.cn;\nContributing authors: kzh24@mails.tsinghua.edu.cn;\ncezhang@cs.cmu.edu; zihanliao@berkeley.edu; yapeng.tian@utdallas.edu;\nxiao-jy24@mails.tsinghua.edu.cn; 2021902007@chd.edu.cn;\njiangdm@pcl.ac.cn; wangyw@pcl.ac.cn; liaoqm@tsinghua.edu.cn;\nAbstract\nLarge Vision-Language Model (LVLM) systems have demonstrated impressive\nvision-language reasoning capabilities but suffer from pervasive and severe hal-\nlucination issues, posing significant risks in critical domains such as healthcare\nand autonomous systems. Despite previous efforts to mitigate hallucinations, a\npersistent issue remains: visual defect from vision-language misalignment, cre-\nating a bottleneck in visual processing capacity. To address this challenge, we\ndevelop Complementary Adaptive Token-level Contrastive Decoding to Miti-\ngate Hallucinations in LVLMs (CATCH), based on the Information Bottleneck\ntheory. CATCH introduces Complementary Visual Decoupling (CVD) for visual\ninformation separation, Non-Visual Screening (NVS) for hallucination detec-\ntion, and Adaptive Token-level Contrastive Decoding (ATCD) for hallucination\nmitigation. CATCH addresses issues related to visual defects that cause dimin-\nished fine-grained feature perception and cumulative hallucinations in open-ended\n1\narXiv:2411.12713v1  [cs.CV]  19 Nov 2024\n\nscenarios. It is applicable to various visual question-answering tasks without\nrequiring any specific data or prior knowledge, and generalizes robustly to new\ntasks without additional training, opening new possibilities for advancing LVLM\nin various challenging applications.\nKeywords: Large Vision-Language Models, Hallucinations, Constrastive Decoding\n1 Introduction\nLarge Vision-Language Models (LVLMs) have achieved significant advancements in\nareas such as visual question answering [1] and embodied intelligence [2], owing to their\nremarkable potential to integrate and interpret both visual and linguistic information.\nHowever, hallucinations in LVLMs, referring to the generation of textual content that is\ninconsistent with the visual input, remain a pervasive issue. Therefore, substantial risks\nare posed, particularly in high-stakes domains such as healthcare [3] and autonomous\nsystems [4], where erroneous decisions potentially lead to severe consequence.\nHallucinations in LVLMs primarily arises from an excessive dependence on training\ndata, limited real-world comprehension, and a over-reliance on linguistic information\ndue to the Large Language Model (LLM)-centric architecture of the existing models\nin vision-language reasoning [5]. Despite the prevalence of hallucination issues across\nall existing LVLMs, research dedicated to mitigating this problem remains scarce.\nOne of the primary contributors to hallucination is the quality of data. Recent efforts\nhave focused on addressing this issue by introducing negative data [6], counterfac-\ntual data [7], and reducing noise and errors within existing datasets [8, 9]. Given the\nimbalance in vision-language reasoning introduced by the LLM-centric architecture,\nsome approaches have sought to enhance the model’s visual reasoning capabilities\nby increasing resolution [10–13] or incorporating more advanced vision encoders [14–\n16]. Furthermore, several methods have optimized decoding strategies. For instance,\nHALC [17] employs Grounding DINO to resample images, thereby enhancing LVLMs’\nperceptual sensitivity to fine-grained targets. Other approaches, such as VCD [18]\nand M3ID [19], introduce contrastive decoding, which involves comparing the origi-\nnal image with either a noise-added version or text-only input, aiming to reduce the\nmodels’ vulnerability to language priors.\nPrevious research typically attributes the causes of hallucinations to two primary\nfactors: (1) statistical bias, unbalanced object distribution and biased object corre-\nlations in textual information, (2) language bias, overlook visual evidence and overly\nexploit language priors for decision-making [18]. However, we observe that hallucina-\ntions in LVLMs arise from an inability to comprehensively process visual information,\nrendering them unaccounted for by the two factors mentioned above. We first illus-\ntrate this with the example as shown in Fig. 1, we decouple visual input to seven levels\nby utilizing the Segment Anything Model (SAM) [20] to segment the original visual\ninput with different numbers of target objects. The LVLM has generated the ongoing\nresponse, “One person on the left side is holding a ...”. Initially, it hallucinates by pre-\ndicting “phone” instead of “sandwich”. As irrelevant visual features are reduced, the\n2\n\nFig. 1: Analysis of how varying decoupling levels affect ground-truth token\nprobability. We utilize SAM to segment the original visual input into seven levels.\nThe horizontal axis represents the number of segmented objects selected utilizing SAM,\nand the vertical axis represents the token probability. As irrelevant visual features\nunrelated to the target reduce, the probability of the hallucinated token “phone”\ndecreases, while the probability of the ground-truth token “sandwich” increases.\nprobability of generating the ground-truth token “sandwich” increases, while the prob-\nability of the hallucinated token “phone” decreases. Once irrelevant visual features\nare minimized, the probability of the ground-truth “sandwich” significantly surpasses\nthat of the hallucinated “phone”.\nThis phenomenon indicates that LVLMs fail to perform precise vision-language\nreasoning on an entire image when extraneous information (e.g., segmented portions)\nis present. We define the aforementioned phenomenon as visual defect. The visual\ndefect primarily arises from an overload of visual information exceeding the model’s\nvisual reasoning capacity, causing disruption and uncertainty that bias reasoning\ntoward linguistic information. In addition, visual defects worsen in open-ended sce-\nnarios as bias toward linguistic information propagates and accumulates with each\ntoken generation step, akin to noise in chaotic systems, making it increasingly diffi-\ncult to retain critical visual information and thereby impeding precise reasoning. We\nposit that the visual defect arises from the alignment between visual and linguistic\nfeature spaces. Specifically, visual features possess higher dimensionality than textual\nfeatures, and mapping this higher-dimensional space to a lower-dimensional aligned\nvision-language latent space introduces an information bottleneck, compressing\nfeatures and impeding full information propagation. This issue is intrinsic to vision-\nlanguage tasks, where alignment between visual and linguistic information is necessary.\nTherefore, we believe that simply optimizing high-quality data or strengthening the\nvisual encoder is insufficient to resolve the visual defect.\nWe develop a method for detecting and mitigating hallucinations arising from\nthe visual defect by introducing Complementary Adaptive Token-level Contrastive\nDecoding to Mitigate Hallucinations in LVLMs (CATCH). To separate extraneous\ninformation and create a stable decoupled visual representation at each generation\nstep, we propose Complementary Visual Decoupling (CVD). As illustrated in Fig. 2(a),\nbefore the visual input v is fed into the LVLM, we utilize SAM to segment it into two\n3\n\nVisual Input:\nTextual Input:\n“Please describe this image in detail.”\nOngoing Response:\n“The image depicts a group of people \nwalking along a road centered around a \nconcrete median, with three individuals \nvisible in the scene. One person on the \nleft side is holding a __”\nLVLM\nSAM\nɌ\nNONE\nɐɄ\nɐȺ\nɐɈ\n(a) CVD\nphone\nsandwich\norange\nphone\nsandwich\norange\nphone\nsandwich\norange\n풍풐품ȿɊɉ(ɏɊ| Ɏ, ɐȺ)\n풍풐품ȿɊɉ(ɏɊ| Ɏ, ɐɈ)\n풍풐품ȿɊɉ(ɏɊ| Ɏ, ɐɄ)\nJS Divergence\nJS Divergence\n푱ȯȠɈɄ\n푱ȯȠȺɄ\n>\n√\n(b) NVS\n푱ȯȠɈɄ\n푱ȯȠȺɄ\n<\nJS Divergence\nJS Divergence\n푱ȯȠɈɄ\n푱ȯȠ풐Ʉ\n>\n√\n(c) ATCD\n푱ȯȠɈɄ\n푱ȯȠ풐Ʉ\n<\nphone\nsandwich\norange\n풍풐품ȿɊɉ(ɏɊ| Ɏ, Ɍ)\nHallucination\nphone\nphone\nsandwich\norange\nsandwich √\n×\nFig. 2: LVLMs may generate responses that include hallucinations (e.g., “One person\non the left side is holding a phone”, where “sandwich” is hallucinated as “phone”.\nFirst, the CVD method leverages SAM to decouple the original input image v into the\ndual image zd and the residual image zr, and introduces a non-visual input zn. These\nfour inputs are then passed into the LVLM to generate their corresponding output\ndistributions: logitso, logitsd, logitsr and logitsn. The Jensen-Shannon Divergence\n(JSD) is computed between them to obtain JSDon, JSDmn, and JSDcn. The NVS\nmethod compares JSDmn and JSDcn, and the input with the greater distance is\nselected as the decoupled image (e.g., zr). Next, ATCD selects the decoding strategy\nby comparing JSDcn and JSDon, if JSDcn is greater, the decoupled image output\ndistribution is employed to contrastively subtract the original distribution. Conversely,\nif JSDon is greater, the output distribution from the decoupled image is leveraged\nto contrastively enhance the weighted original distribution. Effectively correcting the\nhallucinated token (e.g., “phone” is successfully corrected to “sandwich”). Notably,\nthis process is dynamically performed at each token generation step.\ncomplementary parts: the dual image zd and the residual image zr. Leveraging their\ncomplementary nature, CVD divides the entire set of visual features into two simplified\nparts. At each generation step, the least important visual features for the next token\nare dynamically highlighted in one part and obscured in the other, designating the\ndecoupled image z as the highlighted one that preserves key visual features while\nremoving extraneous details.\nThe key question is how to identify the correct decoupled image z within the\nset {zd, zr}. As illustrated in Fig. 5, 1,000 instances are randomly selected from the\nMSCOCO [21] dataset, with key visual features masked to create a masked image and\n4\n\na complementary exposed image. We introduce a non-visual input zn, containing no\nvisual information, generates responses based solely on the textual prompt and gener-\nated text tokens. The Jensen-Shannon Divergence (JSD) of the output distributions\nbetween the non-visual input and both the masked image and the exposed image is\nthen calculated, denoted as JSDmn and JSDen, respectively. From the single-sample\n(3a) and extensive statistical analysis (3b), we observe that: (1) the output distri-\nbution of the masked image is nearly identical to that of the non-visual input, with\nJSDen significantly greater than JSDmn, (2) the output probability of the ground-\ntruth token from the exposed image is much higher than that of the visual input\n(e.g., the token “green”). The above observation indicates that when key visual fea-\ntures relevant to the current token are obscured, the generation process relies almost\nentirely on linguistic priors. Furthermore, the exposed image demonstrates that the\nvisual decoupling method (CVD) effectively increases visual information density and\nreduces uncertainty associated with linguistic knowledge.\nBased on the aforementioned analyses, we propose Non-Visual Screening (NVS), as\nshown in Fig. 2(b). we introduce the non-visual input zn alongside the dual image zd\nand residual image zr, which are fed into the LVLM to generate corresponding output\ndistributions. We then calculate the distance between the output distributions from\nthe non-visual input and the dual image, denoted as the dual-to-non distance, and the\ndistance between the output distributions from the non-visual input and the residual\nimage, denoted as the residual-to-non distance. The visual input corresponding to the\ngreater value between the dual-to-non distance and the residual-to-non distance is\nidentified as the decoupled image.\nWe consider two scenarios: (1) Hallucination Existence: when the distance\nbetween the output distributions from the non-visual input and the decoupled image,\ndenoted as the decoupled-to-non distance is greater than the distance between the out-\nput distributions from the non-visual input and the original visual input, denoted as\nthe original-to-non distance, it suggests that the output distribution from the original\nvisual input is closer to that of the non-visual input. This indicates that the orig-\ninal visual input contains redundant information, leading to visual uncertainty and\nthe amplification of language priors, ultimately causing hallucinations. (2) Diversity\nInsufficient: when the decoupled-to-non distance is smaller than original-to-non dis-\ntance, it suggests that the output distribution from the decoupled image is closer to\nthat of the non-visual input, indicating an insufficient global receptive field or the pres-\nence of abstract concepts in the next token. Consequently, the decoupled image lacks\ndiversity, leading to cumulative hallucinations, which occur when reasoning becomes\ndominated by language priors, causing the probability of hallucinations to increase as\nthe sequence lengthens in open-ended generation scenarios.\nBased on these two scenarios, we propose Adaptive Token-level Contrastive Decod-\ning (ATCD) as shown in Fig. 2(c) to contrastively mitigate hallucinations and enhance\ndiversity at each generation step. When the first scenario occurs, the output distri-\nbution from the decoupled image is employed to contrastively subtract the original\ndistribution, which contains hallucinated concepts. When the second scenario occurs,\nthe output distribution from the decoupled image is leveraged to contrastively enhance\n5\n\nthe weighted original distribution, thereby improving the diversity of generation and\npreventing cumulative hallucinations.\n(a) In the single-sample analysis, the output distribution of the masked image is nearly\nidentical to that of the non-visual input, while the output probability of the ground-truth\ntoken from the exposed image is significantly higher compared to that of the original visual\ninput (e.g., the ground-truth token “green”).\n(b) In the extensive statistical analysis, JSDen is significantly greater than JSDmn.\nFig. 3: We randomly selected 1,000 instances from the MSCOCO [21] dataset, masking\nkey visual features to create a masked image and a exposed image and then calculated\nJSDen between the exposed image and the non-visual input, JSDmn between the\nmasked image and the non-visual input. We conducted a single-sample experiment as\nshown in (3a) and performed an extensive statistical analysis as presented in (3b).\n6\n\nCATCH can be seamlessly integrated into various LVLMs and applied across\nvarious visual question-answering scenarios without additional training or expert inter-\nvention. Our method is evaluated with LLaVA-1.5 [12] and InstructBLIP [22] as\nbaselines, both utilizing Vicuna 7B [23] as their language decoder. The evaluation is\nperformed on the public hallucination assessment datasets POPE [24], CHAIR [25],\nand MME [26]. On the POPE dataset, our method improved Accuracy and F1 score\nby up to 8.07 and 5.98 points, respectively, compared to the baselines. On the MME\ndataset, across the four subtasks—Existence, Count, Position, and Color—our method\noutperformed the baselines by 16%, demonstrating CATCH’s significant enhancement\nin the perception of various feature types. Additionally, on the CHAIR dataset, our\nmethod achieved a 45.8% improvement over the baselines, indicating that CATCH\neffectively mitigates cumulative hallucinations in open-ended generation scenarios.\n2 Results\nResults on POPE. As shown in Table 1, we evaluate our CATCH method on the\nPOPE dataset [24]. POPE assesses hallucinations as a binary classification task by\nasking yes/no questions about object presence (e.g., “Is there a dog in the image?”).\nThis benchmark aggregates data from three sources: MSCOCO [21], A-OKVQA [27],\nand GQA [28], and includes three subsets: random, popular, and adversarial, which\naddress object prevalence and co-occurrence patterns. Each sampling setting uses 500\nimages per dataset, with 6 questions per image, resulting in a total of 27,000 query-\nanswer pairs derived from the development sets. Evaluation is based on four key\nmetrics: Accuracy, Precision, Recall, and F1 score.\nWe observed that our CATCH method outperforms the current best by a significant\nmargin. Compared to the baseline, it improves Accuracy and F1 score by up to 8.07\nand 5.98 points, respectively, demonstrating its effectiveness in mitigating hallucinated\nconcepts present in the original distribution.\nIn addition, it is worth noting that while all methods exhibit a clear performance\ndecline from the random to the popular setting, with a further drop in the adver-\nsarial setting, CATCH demonstrates superior performance in these more challenging\nscenarios. It improves accuracy by 6.65, 8.21, and 8.36 points over the baseline in the\nrandom, popular, and adversarial settings, respectively. These results reveal that more\nchallenging tasks amplify the visual defects in LVLMs, whereas CATCH effectively\nreduces the density of extraneous visual information, preventing reliance on language\npriors. We also observed that, compared to Recall, CATCH achieved a more signif-\nicant improvement in Precision, which can be attributed to its lower ”yes” response\nratio compared to the baseline. This suggests that CATCH is more conservative and\nstable when handling uncertain responses.\n7\n\nTable 1: Results on POPE benchmark. The best results are bolded, and the\nsecond-best are underlined.\nSetup\nMethod\nLLaVA 1.5\nInstructBLIP\nAcc. ↑\nPrec. ↑\nRec. ↑\nF1 ↑\nAcc. ↑\nPrec. ↑\nRec. ↑\nF1 ↑\nRandom\nbase\n84.13\n82.86\n86.07\n84.43\n82.80\n82.24\n83.67\n82.95\nVCD\n85.37\n83.14\n88.73\n85.84\n83.93\n84.42\n82.67\n83.73\nM3ID\n86.00\n85.11\n87.27\n86.18\n84.37\n84.62\n84.00\n84.31\nRITUAL\n88.87\n89.23\n88.40\n88.81\n88.83\n90.48\n86.80\n88.60\nCATCH\n90.43\n93.04\n87.40\n90.13\n90.17\n92.28\n87.67\n89.91\nMS-COCO\nPopular\nbase\n80.87\n78.23\n85.53\n81.72\n75.80\n72.74\n82.53\n77.33\nVCD\n81.10\n77.78\n87.07\n82.16\n77.73\n75.43\n82.27\n78.70\nM3ID\n82.83\n79.62\n88.27\n83.72\n77.30\n74.10\n83.93\n78.71\nRITUAL\n85.83\n84.17\n88.27\n86.17\n81.97\n78.90\n87.27\n82.87\nCATCH\n87.07\n90.12\n83.27\n86.56\n83.70\n81.22\n87.67\n84.32\nAdversarial\nbase\n76.23\n71.75\n86.53\n78.45\n75.40\n71.60\n84.20\n77.39\nVCD\n75.60\n70.78\n87.20\n78.14\n76.80\n73.62\n83.53\n78.26\nM3ID\n77.70\n73.23\n87.33\n79.66\n76.03\n72.48\n83.93\n77.79\nRITUAL\n78.80\n74.43\n87.73\n80.54\n78.73\n74.57\n87.20\n80.39\nCATCH\n83.17\n83.10\n83.27\n83.18\n79.90\n75.82\n87.80\n81.37\nRandom\nbase\n81.73\n76.53\n91.53\n83.36\n81.13\n78.03\n86.67\n82.12\nVCD\n81.83\n75.74\n93.67\n83.76\n82.00\n79.38\n86.47\n82.77\nM3ID\n83.57\n77.86\n93.80\n85.09\n82.33\n77.81\n90.47\n83.66\nRITUAL\n85.17\n79.79\n94.20\n86.40\n87.13\n83.92\n91.87\n87.71\nCATCH\n89.63\n88.83\n90.67\n89.74\n89.43\n86.22\n93.87\n89.88\nA-OKVQA\nPopular\nbase\n76.67\n70.51\n91.67\n79.71\n75.67\n70.97\n86.87\n78.12\nVCD\n74.70\n68.12\n92.87\n78.59\n76.50\n71.69\n87.60\n78.85\nM3ID\n76.80\n70.20\n93.13\n80.06\n75.60\n70.40\n88.33\n78.36\nRITUAL\n78.83\n71.99\n94.40\n81.68\n78.73\n72.83\n91.67\n81.17\nCATCH\n84.63\n80.90\n90.67\n85.51\n80.90\n74.54\n93.87\n83.09\nAdversarial\nbase\n67.40\n61.78\n91.27\n73.68\n68.00\n63.08\n86.80\n73.06\nVCD\n67.43\n61.48\n93.33\n74.13\n70.67\n65.24\n88.47\n75.10\nM3ID\n68.10\n61.99\n93.60\n74.58\n69.57\n64.21\n88.40\n74.39\nRITUAL\n68.57\n62.26\n94.27\n74.99\n70.27\n64.15\n91.87\n75.55\nCATCH\n75.47\n69.53\n90.67\n78.70\n71.90\n65.22\n93.87\n76.96\nRandom\nbase\n81.23\n75.42\n92.67\n83.16\n79.93\n76.73\n85.93\n81.07\nVCD\n81.50\n74.78\n95.07\n83.71\n81.83\n79.03\n86.67\n82.67\nM3ID\n82.83\n76.64\n94.47\n84.62\n80.57\n76.77\n87.67\n81.85\nRITUAL\n86.10\n80.30\n95.67\n87.31\n84.87\n82.52\n88.47\n85.39\nCATCH\n89.97\n88.80\n91.47\n90.11\n86.63\n84.11\n90.33\n87.11\nGQA\nPopular\nbase\n72.50\n65.85\n93.47\n77.27\n72.73\n68.14\n85.40\n75.80\nVCD\n71.57\n64.72\n94.80\n76.93\n73.67\n68.82\n86.53\n76.67\nM3ID\n72.83\n66.04\n94.00\n77.58\n74.57\n69.45\n87.73\n77.53\nRITUAL\n74.80\n67.50\n95.67\n79.15\n74.50\n69.17\n88.40\n77.61\nCATCH\n82.97\n78.18\n91.47\n84.30\n76.93\n71.24\n90.33\n79.66\nAdversarial\nbase\n67.63\n61.68\n93.13\n74.21\n69.57\n64.80\n85.67\n73.79\nVCD\n67.47\n61.38\n94.20\n74.33\n69.43\n64.76\n85.27\n73.61\nM3ID\n68.13\n61.88\n94.47\n74.78\n68.90\n64.06\n86.13\n73.47\nRITUAL\n68.23\n61.75\n95.80\n75.10\n70.17\n64.76\n88.47\n74.78\nCATCH\n77.70\n71.72\n91.47\n80.40\n71.40\n65.52\n90.33\n75.95\n8\n\n(a) Results on MME Hallucination benchmark. For the LLaVA and InstructBLIP\nbaselines, we evaluate the existence, count, position, and color subsets. We randomly selected\nfive different seeds and used the average as the final result.\n(b) Results on CHAIR benchmark. Lower CHAIRS and CHAIRI scores indicate better\nperformance.\nFig. 4: Results on MME Hallucination (4a) and CHAIR benchmark (4b).\nResults on MME Hallucination. MME Hallucination [26] is a comprehensive\nbenchmark designed for evaluating LVLMs, comprising four subsets: existence, count,\nposition, and color. Each subset contains 30 images and 60 questions, with two ques-\ntions per image. Similar to POPE, these questions are structured as binary yes/no\nqueries, and performance is measured based on binary accuracy.\nMME Hallucination is more challenging than POPE, as it includes attribute-level\nhallucinations related to position and color, in addition to the existence and count\ndimensions. As shown in Fig. 4a, the first row presents the evaluation results with\nLLaVa as the baseline, while the second row shows the results with InstructBLIP as\nthe baseline. The first four columns represent performance on the existence, count,\nposition, and color subsets, respectively, while the final column shows the total score\n9\n\nacross all four subsets. CATCH demonstrates significant improvements in total score,\nwith increases of 16% and 13.4% for LLaVa and InstructBLIP baselines, respectively.\nNotably, when comparing the results with LLaVa as the baseline to other LVLMs on\nthe MME leaderboard, the positive impact of CATCH is comparable to upgrading\nLLaVa 7B to LLaVa 13B, GPT-4V, LLaVa 1.6 34B, and Qwen-VL-Plus across the\nexistence, count, position, and color dimensions.\nResults on CHAIR. CHAIR\n[25] uses ground-truth captions and object\nannotations to evaluate hallucinations in LVLMs by calculating the proportion of\nhallucinated objects in generated captions relative to actual objects. This evalu-\nation is based on two metrics: CHAIRS =\n|{hallucinated objects}|\n|{all mentioned objects}|,\nCHAIRI =\n|{captions with hallucinated objects}|\n|{all captions}|\n. CHAIRS represents assessments at the sentence\nlevel, measuring the proportion of hallucinated sentences relative to all sentences, while\nCHAIRI measures hallucinations at the object instance level, indicating the propor-\ntion of hallucinated objects relative to all generated objects. Lower scores indicate\nfewer hallucinations. We randomly selected 500 images from the COCO validation\nset and conducted image captioning using the prompt, ”Please describe this image\nin detail.” Fig. 4b shows the evaluation results on the CHAIR dataset. For LLaVA,\nCATCH achieves scores of 14.2 on CHAIRS and 4.7 on CHAIRI, representing sig-\nnificant improvements over the baseline scores of 26.2 and 9.3, with gains of 45.8%\nand 49.5%, respectively. Similarly, when using InstructBLIP as the baseline, CATCH\nachieves scores of 21.2 on CHAIRS and 7.1 on CHAIRI, compared to baseline scores\nof 28.6 and 10.3, showing improvements of 25.9% and 31.1%.\nCATCH significantly prevent cumulative hallucinations. As shown in Fig. 5a, we\ncalculated the JS divergence between the output distributions from the original visual\ninput and non-visual input with the baseline, as well as between the decoupled visual\ninput and the non-visual input with CATCH during generation. We observed that from\nthe 39th token onward, the output distributions from the original visual input and the\nnon-visual input become almost identical. This indicates the onset of cumulative hallu-\ncinations, where the LVLM output relies solely on language priors, sharply increasing\nthe likelihood of hallucinations, such as “mouse” and “cell phone”. In contrast, the\noutput distribution from the decoupled image maintained a greater divergence from\nthat of the non-visual input throughout the generation process, with cumulative hal-\nlucinations occurring only after the 101st token. This suggests that: (1) the decoupled\nimage contains less visual uncertainty due to the separation of extraneous informa-\ntion compared to the original visual input, and (2) in the earlier generation steps,\nCATCH mitigates hallucinated concepts and enhances diversity. For further analysis,\nas showm in Fig. 5b, we randomly sampled 1,000 examples and plotted the distribu-\ntion of the points at which cumulative hallucinations occur. The results indicate that,\nwith the baseline, cumulative hallucinations for the original visual input mostly occur\nfrom 40% of the sequence length, whereas for the decoupled image with CATCH, they\noccur mostly from 80% of the sequence length.\n10\n\n(a) The first row presents an example where CATCH eliminates hallucinations such as\n“mouse” and “phone” that were generated by the baseline. The second row shows the corre-\nsponding cumulative hallucinations analysis, indicating that the baseline model experiences\ncumulative hallucinations as early as the 39th token, whereas CATCH delays the occurrence\nof cumulative hallucinations until the 101st token.\n(b) We provide extensive statistical analysis across 1,000 samples, demonstrating that the\nbaseline model experiences cumulative hallucinations for most samples by 40% of the sentence\nlength, whereas CATCH effectively delays cumulative hallucinations in a substantial portion\nof samples until approximately 80% of the sentence length.\nFig. 5: An analysis of cumulative hallucinations includes a single-sample experiment,\nas shown in (5a), and an extensive statistical analysis, as presented in (5b)\n11\n\n3 Methods\n3.1 Formulation of LVLMs\nWe consider an LVLM parameterized by θ. The model receives a textual query x and\na visual input v, where v provides contextual visual information to aid the model in\ngenerating a relevant response y to the query. Initially, the raw image v is processed\nthrough a vision encoder to extract visual features. These features are then mapped\ninto the input space of the large language model through a vision-language alignment\nmodule (e.g., Q-Former [29], linear projection [12]), generating visual tokens. Subse-\nquently, these visual tokens, together with textual tokens obtained through embedding\nthe query, are fed into the large language model to auto-regressively generate the\nresponse. Mathematically, this process can be formulated as follows:\nyt ∼pθ (yt | v, x, y<t) ∝exp logitθ (yt | v, x, y<t) ,\n(1)\nwhere yt denotes the token generated at time step t, and y<t represents the sequence\nof tokens generated up to time step t −1. During the decoding phase in LVLMs,\nhallucinations arise when the generated probability distribution deviates from the\nfactual information provided by the visual input v. To mitigate hallucinations arising\nfrom visual defect, we propose the Complementary Visual Decouplin (CVD) for visual\ninformation separation, Non-Visual Screening (NVS) for hallucination detection, and\nAdaptive Token-level Contrastive Decoding (ATCD) for hallucination mitigation.\n3.2 Complementary Visual Decoupling\nThe visual defect fundamentally arises from the unbalanced alignment in vision-\nlanguage multimodal integration, leading to a visual information bottleneck. Inspired\nby the information bottleneck principle, which addresses this issue by removing irrele-\nvant information and preserving only information pertinent to the current prediction to\ncreate a more stable representation, we propose the Complementary Visual Decoupling\n(CVD) method to preserve essential visual information while removing extraneous\ndetails.\nAssuming we have raw information v and label y, the core idea of the information\nbottleneck principle is to map the observation of v to a robust representation z, which\nretains the essential characteristics needed for predicting y while simultaneously min-\nimizing redundant information. Theoretically, treating v, z, and y as three random\nvariables, the optimization objective is to maximize the mutual information between\nz and y, while minimizing the mutual information between z and v. This optimization\nobjective can be defined as:\nmin[I(v; z) −I(z; y)].\n(2)\nIn this paper, the raw information v represents the visual input, and the label y\nrepresents the accurate response. We employ the Segment Anything Model (SAM)\nto optimize this objective. Specifically, as illustrated in Fig. 2, a pre-trained SAM is\n12\n\nutilized to segment all objects:\n{O1, O2, ..., ON, B} = SAM(v),\n(3)\nwhere O denotes the objects and B represents the background. Using the area of the\ntarget region as the confidence score, we select the top M objects as the exposed\nportion and mask the remaining objects and background to obtain the dual image zd.\nSimilarly, we mask the top M objects and treat the remaining objects and background\nas the target, generating a residual image zr:\nzd = (\nM\nX\ni\nOi) ⊙v\nzr = (1 −\nM\nX\ni\nOi) ⊙v.\n(4)\nThrough CVD, the original visual information is decoupled into two simplified com-\nponents: the dual image zd and the residual image zr, leveraging their complementary\nnature.\n3.3 Non-Visual Screening\nAt each generation step, key visual features for the next token are dynamically\nemphasized in one part while being obscured in the other. The decoupled image z\nis designated as the one that retains the highlighted key visual features, effectively\neliminating extraneous details. We observed that when critical visual information rel-\nevant to the next token is obscured, the output distribution becomes dominated by\nlanguage priors. Thus, to identify the correct decoupled image z between the dual\nimage and the residual image, we introduce a non-visual input zn, containing only\nthe textual prompt, without any visual information, to serve as an assistant. We then\ncalculate the Jensen-Shannon Divergence (JSD) as the distance between the output\ndistributions from the non-visual input and the dual image as d(zd, zn), and between\nthe non-visual input and the residual image as d(zr, zn):\nd(zd, zn) = DJS ( pθ( yt | zd, x, y<t) ∥pθ( yt | zn, x, y<t)) ,\nd(zr, zn) = DJS ( pθ( yt | zr, x, y<t) ∥pθ( yt | zn, x, y<t)) ,\n(5)\nwhere DJS(P||Q) = 1\n2DKL(P||M)+ 1\n2DKL(Q||M), M = 1\n2(P +Q), and DKL(P||Q) =\nP\ni P(i) log P (i)\nQ(i). The visual input corresponding to the greater distance is selected as\nthe decoupled image z, formulated as:\nz =\n(\nzd,\nif d(zd, zn) ≥d( zr, zn);\nzr,\nif d(zd, zn) < d( zr, zn).\n(6)\n3.4 Adaptive Token-level Contrastive Decoding\nWe first calculate the distance between the output distributions from the non-visual\ninput and the decoupled image, denoted as d(z, zn), and the distance between the\n13\n\noutput distributions from the original visual input and the non-visual input, denoted\nas d(v, zn):\nd(z, zn) = DJS ( pθ( yt | z, x, y<t) ∥pθ( yt | zn, x, y<t)) ,\nd(v, zn) = DJS ( pθ( yt | v, x, y<t) ∥pθ( yt | zn, x, y<t)) ,\n(7)\nWe consider two scenarios: (1) Hallucination Existence: When d(z, zn) is greater\nthan d(v, zn), we conclude that hallucinations are present in the original output dis-\ntribution. Therefore, we use the output distribution from the decoupled image to\ncontrastively subtract the original distribution. (2) Diversity Insufficient: When\nd(z, zn) is less than d(v, zn), we consider there to be a risk of cumulative hallu-\ncinations. In this case, we use the output distribution from the decoupled image\nto contrastively enhance the weighted original distribution, thereby improving the\ndiversity of generation, formulated as:\nyt ∼pθ( yt | z, v, x) =\n\n\n\n\n\n\n\n\n\nsoftmax [α · logitθ( y | z, x) −logitθ( y | v, x)] ,\nif d(zd, zn) ≥d(v, zn);\nsoftmax [β · logitθ( y | v, x) + logitθ( y | zd, x)] ,\nif d(zd, zn) < d(v, zn).\n(8)\nHere, the hyperparameters α and β represent the amplification factors. The final\ngenerated token yt is sampled from pθ.\n3.5 Hardware and Implementtation Details\nAll experiments in this paper were conducted on an NVIDIA RTX 3090 24GB GPU.\nFor SAM, we utilized the pre-trained ViT-H SAM model. The decoding process was\nconfigured with α = 1.2 and β = 3 by default. The number of objects in CVD was set\nto be N ∗0.05 by default.\n4 Discussion\nIn this paper, we reveal that hallucinations in LVLMs primarily stem from the visual\nprocessing bottleneck caused by vision-language misalignment, which we term the\nvisual defect. Unlike previously identified issues of language bias and statistical bias,\nthe visual defect is challenging to resolve through high-quality data, chain-of-thought\nprompting, or fine-tuning, due to limitations in the pre-trained visual architecture.\nBased on the Information Bottleneck theory, we introduce Complementary Visual\nDecoupling (CVD) for visual information decoupling, incorporate a non-visual input\nfor Non-Visual Screening (NVS) to detect hallucinations, and propose Adaptive Token-\nlevel Contrastive Decoding (ATCD) for hallucination mitigation through contrastive\ndecoding.\nCATCH demonstrates impressive performance and robustness in perceiving fine-\ngrained features and mitigating cumulative hallucinations in open-ended scenarios, as\nevidenced by the results on the CHAIR, POPE, and MME datasets across different\n14\n\nbaselines. One exciting aspect of CATCH is that it requires no additional prior knowl-\nedge, data, or training, making it efficiently applicable to various LVLMs. We hope\nthat introducing the concept of visual defects opens new research avenues for study-\ning hallucinations and that our complementary contrastive decoding method inspires\nnew questions and approaches in the pretraining and fine-tuning of LVLMs.\nReferences\n[1] Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh,\nD.: Vqa: Visual question answering. In: Proceedings of the IEEE International\nConference on Computer Vision, pp. 2425–2433 (2015)\n[2] Gupta, A., Savarese, S., Ganguli, S., Fei-Fei, L.: Embodied intelligence via\nlearning and evolution. Nature communications 12(1), 5721 (2021)\n[3] Chen, J., Ouyang, R., Gao, A., Chen, S., Chen, G.H., Wang, X., Zhang, R.,\nCai, Z., Ji, K., Yu, G., et al.: Huatuogpt-vision, towards injecting medical visual\nknowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280 (2024)\n[4] Cui, C., Ma, Y., Cao, X., Ye, W., Zhou, Y., Liang, K., Chen, J., Lu, J., Yang, Z.,\nLiao, K.-D., et al.: A survey on multimodal large language models for autonomous\ndriving. In: Proceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pp. 958–979 (2024)\n[5] Bai, Z., Wang, P., Xiao, T., He, T., Han, Z., Zhang, Z., Shou, M.Z.: Hallucination\nof multimodal large language models: A survey. arXiv preprint arXiv:2404.18930\n(2024)\n[6] Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K.,\nMensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model\nfor few-shot learning. Advances in neural information processing systems 35,\n23716–23736 (2022)\n[7] Yu, Q., Li, J., Wei, L., Pang, L., Ye, W., Qin, B., Tang, S., Tian, Q., Zhuang,\nY.: Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 12944–12953 (2024)\n[8] Wang, L., He, J., Li, S., Liu, N., Lim, E.-P.: Mitigating fine-grained halluci-\nnation by fine-tuning large vision-language models with caption rewrites. In:\nInternational Conference on Multimedia Modeling, pp. 32–45 (2024). Springer\n[9] Yue, Z., Zhang, L., Jin, Q.: Less is more: Mitigating multimodal hallucination\nfrom an eos decision perspective. arXiv preprint arXiv:2402.14545 (2024)\n[10] Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q.,\nZhu, X., Lu, L., et al.: Internvl: Scaling up vision foundation models and aligning\n15\n\nfor generic visual-linguistic tasks. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 24185–24198 (2024)\n[11] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 26296–26306 (2024)\n[12] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. Advances in neural\ninformation processing systems 36 (2024)\n[13] Zhai, B., Yang, S., Xu, C., Shen, S., Keutzer, K., Li, M.: Halle-switch: Controlling\nobject hallucination in large vision language models. arXiv e-prints, 2310 (2023)\n[14] He, X., Wei, L., Xie, L., Tian, Q.: Incorporating visual experts to resolve\nthe information loss in multimodal large language models. arXiv preprint\narXiv:2401.03105 (2024)\n[15] Jain, J., Yang, J., Shi, H.: Vcoder: Versatile vision encoders for multimodal large\nlanguage models. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 27992–28002 (2024)\n[16] Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., Xie, S.: Eyes wide shut? exploring\nthe visual shortcomings of multimodal llms. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 9568–9578 (2024)\n[17] Chen, Z., Zhao, Z., Luo, H., Yao, H., Li, B., Zhou, J.: Halc: Object hallucination\nreduction via adaptive focal-contrast decoding. arXiv preprint arXiv:2403.00425\n(2024)\n[18] Leng, S., Zhang, H., Chen, G., Li, X., Lu, S., Miao, C., Bing, L.: Mitigating\nobject hallucinations in large vision-language models through visual contrastive\ndecoding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 13872–13882 (2024)\n[19] Favero, A., Zancato, L., Trager, M., Choudhary, S., Perera, P., Achille, A., Swami-\nnathan, A., Soatto, S.: Multi-modal hallucination control by visual information\ngrounding. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 14303–14312 (2024)\n[20] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,\nWhitehead, S., Berg, A.C., Lo, W.-Y., et al.: Segment anything. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pp. 4015–4026\n(2023)\n[21] Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,\nZitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision–\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12,\n16\n\n2014, Proceedings, Part V 13, pp. 740–755 (2014). Springer\n[22] Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P.,\nHoi, S.: InstructBLIP: Towards General-purpose Vision-Language Models with\nInstruction Tuning (2023). https://arxiv.org/abs/2305.06500\n[23] Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang,\nS., Zhuang, Y., Gonzalez, J.E., et al.: Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April\n2023) 2(3), 6 (2023)\n[24] Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.-R.: Evaluating object\nhallucination in large vision-language models. arXiv preprint arXiv:2305.10355\n(2023)\n[25] Rohrbach, A., Hendricks, L.A., Burns, K., Darrell, T., Saenko, K.: Object\nhallucination in image captioning. arXiv preprint arXiv:1809.02156 (2018)\n[26] Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li,\nK., Sun, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal\nlarge language models. arXiv preprint arXiv:2306.13394 (2023)\n[27] Schwenk, D., Khandelwal, A., Clark, C., Marino, K., Mottaghi, R.: A-okvqa: A\nbenchmark for visual question answering using world knowledge. In: European\nConference on Computer Vision, pp. 146–162 (2022). Springer\n[28] Hudson, D.A., Manning, C.D.: Gqa: A new dataset for real-world visual rea-\nsoning and compositional question answering. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 6700–6709 (2019)\n[29] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. In: International\nConference on Machine Learning, pp. 19730–19742 (2023). PMLR\n17",
    "pdf_filename": "CATCH_Complementary_Adaptive_Token-level_Contrastive_Decoding_to_Mitigate_Hallucinations_in_LVLMs.pdf"
}