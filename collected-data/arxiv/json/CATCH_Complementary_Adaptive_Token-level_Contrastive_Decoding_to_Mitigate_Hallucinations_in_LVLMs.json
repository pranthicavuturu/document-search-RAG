{
    "title": "CATCH: Complementary Adaptive",
    "abstract": "Large Vision-Language Model (LVLM) systems have demonstrated impressive vision-language reasoning capabilities but suffer from pervasive and severe hal- lucination issues, posing significant risks in critical domains such as healthcare and autonomous systems. Despite previous efforts to mitigate hallucinations, a persistent issue remains: visual defect from vision-language misalignment, cre- ating a bottleneck in visual processing capacity. To address this challenge, we develop Complementary Adaptive Token-level Contrastive Decoding to Miti- gateHallucinationsinLVLMs(CATCH),basedontheInformationBottleneck theory. CATCH introduces Complementary Visual Decoupling (CVD) for visual information separation, Non-Visual Screening (NVS) for hallucination detec- tion, and Adaptive Token-level Contrastive Decoding (ATCD) for hallucination mitigation. CATCH addresses issues related to visual defects that cause dimin- ishedfine-grainedfeatureperceptionandcumulativehallucinationsinopen-ended 1 4202 voN 91 ]VC.sc[ 1v31721.1142:viXra",
    "body": "CATCH: Complementary Adaptive\nToken-level Contrastive Decoding to Mitigate\nHallucinations in LVLMs\nZhehan Kan1,2, Ce Zhang3, Zihan Liao4, Yapeng Tian4,\nWenming Yang1,2*, Junyuan Xiao1, Xu Li5, Dongmei Jiang2,\nYaowei Wang2, Qingmin Liao1,2\n1*Tsinghua University.\n2Pengcheng Laboratory.\n3Carnegie Mellon University.\n4University of California, Berkeley.\n4The University of Texas at Dallas.\n5Chang’an University.\n*Corresponding author(s). E-mail(s): yang.wenming@sz.tsinghua.edu.cn;\nContributing authors: kzh24@mails.tsinghua.edu.cn;\ncezhang@cs.cmu.edu; zihanliao@berkeley.edu; yapeng.tian@utdallas.edu;\nxiao-jy24@mails.tsinghua.edu.cn; 2021902007@chd.edu.cn;\njiangdm@pcl.ac.cn; wangyw@pcl.ac.cn; liaoqm@tsinghua.edu.cn;\nAbstract\nLarge Vision-Language Model (LVLM) systems have demonstrated impressive\nvision-language reasoning capabilities but suffer from pervasive and severe hal-\nlucination issues, posing significant risks in critical domains such as healthcare\nand autonomous systems. Despite previous efforts to mitigate hallucinations, a\npersistent issue remains: visual defect from vision-language misalignment, cre-\nating a bottleneck in visual processing capacity. To address this challenge, we\ndevelop Complementary Adaptive Token-level Contrastive Decoding to Miti-\ngateHallucinationsinLVLMs(CATCH),basedontheInformationBottleneck\ntheory. CATCH introduces Complementary Visual Decoupling (CVD) for visual\ninformation separation, Non-Visual Screening (NVS) for hallucination detec-\ntion, and Adaptive Token-level Contrastive Decoding (ATCD) for hallucination\nmitigation. CATCH addresses issues related to visual defects that cause dimin-\nishedfine-grainedfeatureperceptionandcumulativehallucinationsinopen-ended\n1\n4202\nvoN\n91\n]VC.sc[\n1v31721.1142:viXra\nscenarios. It is applicable to various visual question-answering tasks without\nrequiring any specific data or prior knowledge, and generalizes robustly to new\ntaskswithoutadditionaltraining,openingnewpossibilitiesforadvancingLVLM\nin various challenging applications.\nKeywords:LargeVision-LanguageModels,Hallucinations,ConstrastiveDecoding\n1 Introduction\nLarge Vision-Language Models (LVLMs) have achieved significant advancements in\nareassuchasvisualquestionanswering[1]andembodiedintelligence[2],owingtotheir\nremarkablepotentialtointegrateandinterpretbothvisualandlinguisticinformation.\nHowever,hallucinationsinLVLMs,referringtothegenerationoftextualcontentthatis\ninconsistentwiththevisualinput,remainapervasiveissue.Therefore,substantialrisks\nare posed, particularly in high-stakes domains such as healthcare [3] and autonomous\nsystems [4], where erroneous decisions potentially lead to severe consequence.\nHallucinationsinLVLMsprimarilyarisesfromanexcessivedependenceontraining\ndata, limited real-world comprehension, and a over-reliance on linguistic information\ndue to the Large Language Model (LLM)-centric architecture of the existing models\nin vision-language reasoning [5]. Despite the prevalence of hallucination issues across\nall existing LVLMs, research dedicated to mitigating this problem remains scarce.\nOne of the primary contributors to hallucination is the quality of data. Recent efforts\nhave focused on addressing this issue by introducing negative data [6], counterfac-\ntual data [7], and reducing noise and errors within existing datasets [8, 9]. Given the\nimbalance in vision-language reasoning introduced by the LLM-centric architecture,\nsome approaches have sought to enhance the model’s visual reasoning capabilities\nby increasing resolution [10–13] or incorporating more advanced vision encoders [14–\n16]. Furthermore, several methods have optimized decoding strategies. For instance,\nHALC[17]employsGroundingDINOtoresampleimages,therebyenhancingLVLMs’\nperceptual sensitivity to fine-grained targets. Other approaches, such as VCD [18]\nand M3ID [19], introduce contrastive decoding, which involves comparing the origi-\nnal image with either a noise-added version or text-only input, aiming to reduce the\nmodels’ vulnerability to language priors.\nPrevious research typically attributes the causes of hallucinations to two primary\nfactors: (1) statistical bias, unbalanced object distribution and biased object corre-\nlationsintextualinformation,(2)language bias,overlookvisualevidenceandoverly\nexploit language priors for decision-making [18]. However, we observe that hallucina-\ntionsinLVLMsarisefromaninabilitytocomprehensivelyprocessvisualinformation,\nrendering them unaccounted for by the two factors mentioned above. We first illus-\ntratethiswiththeexampleasshowninFig.1,wedecouplevisualinputtosevenlevels\nby utilizing the Segment Anything Model (SAM) [20] to segment the original visual\ninput with different numbers of target objects. The LVLM has generated the ongoing\nresponse,“One person on the left side is holding a ...”.Initially,ithallucinatesbypre-\ndicting “phone” instead of “sandwich”. As irrelevant visual features are reduced, the\n2\nFig. 1:Analysis of how varying decoupling levels affect ground-truth token\nprobability. We utilize SAM to segment the original visual input into seven levels.\nThehorizontalaxisrepresentsthenumberofsegmentedobjectsselectedutilizingSAM,\nand the vertical axis represents the token probability. As irrelevant visual features\nunrelated to the target reduce, the probability of the hallucinated token “phone”\ndecreases, while the probability of the ground-truth token “sandwich” increases.\nprobabilityofgeneratingtheground-truthtoken“sandwich” increases,whiletheprob-\nability of the hallucinated token “phone” decreases. Once irrelevant visual features\nare minimized, the probability of the ground-truth “sandwich” significantly surpasses\nthat of the hallucinated “phone”.\nThis phenomenon indicates that LVLMs fail to perform precise vision-language\nreasoning on an entire image when extraneous information (e.g., segmented portions)\nis present. We define the aforementioned phenomenon as visual defect. The visual\ndefect primarily arises from an overload of visual information exceeding the model’s\nvisual reasoning capacity, causing disruption and uncertainty that bias reasoning\ntoward linguistic information. In addition, visual defects worsen in open-ended sce-\nnarios as bias toward linguistic information propagates and accumulates with each\ntoken generation step, akin to noise in chaotic systems, making it increasingly diffi-\ncult to retain critical visual information and thereby impeding precise reasoning. We\nposit that the visual defect arises from the alignment between visual and linguistic\nfeature spaces. Specifically, visual features possess higher dimensionality than textual\nfeatures, and mapping this higher-dimensional space to a lower-dimensional aligned\nvision-language latent space introduces an information bottleneck, compressing\nfeatures and impeding full information propagation. This issue is intrinsic to vision-\nlanguagetasks,wherealignmentbetweenvisualandlinguisticinformationisnecessary.\nTherefore, we believe that simply optimizing high-quality data or strengthening the\nvisual encoder is insufficient to resolve the visual defect.\nWe develop a method for detecting and mitigating hallucinations arising from\nthe visual defect by introducing Complementary Adaptive Token-level Contrastive\nDecoding to Mitigate Hallucinations in LVLMs (CATCH). To separate extraneous\ninformation and create a stable decoupled visual representation at each generation\nstep,weproposeComplementaryVisualDecoupling(CVD).AsillustratedinFig.2(a),\nbefore the visual input v is fed into the LVLM, we utilize SAM to segment it into two\n3\nphone Hallucination\n(a) CVD sandwich\nVisual Input: 풍풐품\u0000\u0000\u0000(�\u0000| �, �) orange\nphone\nsandwich\n� �� 풍풐품\u0000\u0000\u0000(�\u0000| �, ��) orange\nphone\nsandwich\nNONE 풍풐품\u0000\u0000\u0000(�\u0000| �, ��) orange\nLVLM phone\nsandwich\n풍풐품\u0000\u0000\u0000(�\u0000| �, ��) orange\nSAM (b) NVS\n> √\n푱�\u0000�� 푱�\u0000�� ×\nJS Divergence phone\nsandwich √\n<\nJS Divergence 푱�\u0000�� 푱�\u0000��\n�� ��\nTextual Input:\n(c) ATCD\n“Please describe this image in detail.” >\n푱�\u0000�� 푱�\u0000풐� √\nJS Divergence phone\nOngoing Response:\nsandwich\n“The image depicts a group of people\nwalking along a road centered around a < orange\nconcrete median, with three individuals JS Divergence 푱�\u0000�� 푱�\u0000풐�\nvisible in the scene. One person on the\nleft side is holding a __”\nFig. 2:LVLMsmaygenerateresponsesthatincludehallucinations(e.g.,“One person\non the left side is holding a phone”, where “sandwich” is hallucinated as “phone”.\nFirst,theCVDmethodleveragesSAMtodecoupletheoriginalinputimagev intothe\ndual image z and the residual image z , and introduces a non-visual input z . These\nd r n\nfour inputs are then passed into the LVLM to generate their corresponding output\ndistributions: logits , logits , logits and logits . The Jensen-Shannon Divergence\no d r n\n(JSD) is computed between them to obtain JSD , JSD , and JSD . The NVS\non mn cn\nmethod compares JSD and JSD , and the input with the greater distance is\nmn cn\nselected as the decoupled image (e.g., z ). Next, ATCD selects the decoding strategy\nr\nby comparing JSD and JSD , if JSD is greater, the decoupled image output\ncn on cn\ndistributionisemployedtocontrastivelysubtracttheoriginaldistribution.Conversely,\nif JSD is greater, the output distribution from the decoupled image is leveraged\non\nto contrastively enhance the weighted original distribution. Effectively correcting the\nhallucinated token (e.g., “phone” is successfully corrected to “sandwich”). Notably,\nthis process is dynamically performed at each token generation step.\ncomplementary parts: the dual image z and the residual image z . Leveraging their\nd r\ncomplementarynature,CVDdividestheentiresetofvisualfeaturesintotwosimplified\nparts. At each generation step, the least important visual features for the next token\nare dynamically highlighted in one part and obscured in the other, designating the\ndecoupled image z as the highlighted one that preserves key visual features while\nremoving extraneous details.\nThe key question is how to identify the correct decoupled image z within the\nset {z , z }. As illustrated in Fig. 5, 1,000 instances are randomly selected from the\nd r\nMSCOCO[21]dataset,withkeyvisualfeaturesmaskedtocreateamaskedimageand\n4\na complementary exposed image. We introduce a non-visual input z , containing no\nn\nvisualinformation,generatesresponsesbasedsolelyonthetextualpromptandgener-\nated text tokens. The Jensen-Shannon Divergence (JSD) of the output distributions\nbetween the non-visual input and both the masked image and the exposed image is\nthen calculated, denoted as JSD and JSD , respectively. From the single-sample\nmn en\n(3a) and extensive statistical analysis (3b), we observe that: (1) the output distri-\nbution of the masked image is nearly identical to that of the non-visual input, with\nJSD significantly greater than JSD , (2) the output probability of the ground-\nen mn\ntruth token from the exposed image is much higher than that of the visual input\n(e.g., the token “green”). The above observation indicates that when key visual fea-\ntures relevant to the current token are obscured, the generation process relies almost\nentirely on linguistic priors. Furthermore, the exposed image demonstrates that the\nvisual decoupling method (CVD) effectively increases visual information density and\nreduces uncertainty associated with linguistic knowledge.\nBasedontheaforementionedanalyses,weproposeNon-VisualScreening(NVS),as\nshown in Fig. 2(b). we introduce the non-visual input z alongside the dual image z\nn d\nandresidualimagez ,whicharefedintotheLVLMtogeneratecorrespondingoutput\nr\ndistributions. We then calculate the distance between the output distributions from\nthenon-visualinputandthedualimage,denotedasthedual-to-non distance,andthe\ndistance between the output distributions from the non-visual input and the residual\nimage, denotedas the residual-to-non distance.The visual input correspondingto the\ngreater value between the dual-to-non distance and the residual-to-non distance is\nidentified as the decoupled image.\nWe consider two scenarios: (1) Hallucination Existence: when the distance\nbetween the output distributions from the non-visual input and the decoupled image,\ndenotedasthedecoupled-to-nondistance isgreaterthanthedistancebetweentheout-\nput distributions from the non-visual input and the original visual input, denoted as\ntheoriginal-to-non distance,itsuggeststhattheoutputdistributionfromtheoriginal\nvisual input is closer to that of the non-visual input. This indicates that the orig-\ninal visual input contains redundant information, leading to visual uncertainty and\nthe amplification of language priors, ultimately causing hallucinations. (2) Diversity\nInsufficient: when the decoupled-to-non distance is smaller than original-to-non dis-\ntance, it suggests that the output distribution from the decoupled image is closer to\nthatofthenon-visualinput,indicatinganinsufficientglobalreceptivefieldorthepres-\nence of abstract concepts in the next token. Consequently, the decoupled image lacks\ndiversity, leading to cumulative hallucinations, which occur when reasoning becomes\ndominated by language priors, causing the probability of hallucinations to increase as\nthe sequence lengthens in open-ended generation scenarios.\nBasedonthesetwoscenarios,weproposeAdaptiveToken-levelContrastiveDecod-\ning(ATCD)asshowninFig.2(c)tocontrastivelymitigatehallucinationsandenhance\ndiversity at each generation step. When the first scenario occurs, the output distri-\nbution from the decoupled image is employed to contrastively subtract the original\ndistribution, which contains hallucinated concepts. When the second scenario occurs,\ntheoutputdistributionfromthedecoupledimageisleveragedtocontrastivelyenhance\n5\nthe weighted original distribution, thereby improving the diversity of generation and\npreventing cumulative hallucinations.\n(a) In the single-sample analysis, the output distribution of the masked image is nearly\nidentical to that of the non-visual input, while the output probability of the ground-truth\ntoken from the exposed image is significantly higher compared to that of the original visual\ninput (e.g., the ground-truth token “green”).\n(b) In the extensive statistical analysis, JSDen is significantly greater than JSDmn.\nFig.3:Werandomlyselected1,000instancesfromtheMSCOCO[21]dataset,masking\nkeyvisualfeaturestocreateamaskedimageandaexposedimageandthencalculated\nJSD between the exposed image and the non-visual input, JSD between the\nen mn\nmasked image and the non-visual input. We conducted a single-sample experiment as\nshown in (3a) and performed an extensive statistical analysis as presented in (3b).\n6\nCATCH can be seamlessly integrated into various LVLMs and applied across\nvariousvisualquestion-answeringscenarioswithoutadditionaltrainingorexpertinter-\nvention. Our method is evaluated with LLaVA-1.5 [12] and InstructBLIP [22] as\nbaselines, both utilizing Vicuna 7B [23] as their language decoder. The evaluation is\nperformed on the public hallucination assessment datasets POPE [24], CHAIR [25],\nand MME [26]. On the POPE dataset, our method improved Accuracy and F1 score\nby up to 8.07 and 5.98 points, respectively, compared to the baselines. On the MME\ndataset,acrossthefoursubtasks—Existence,Count,Position,andColor—ourmethod\noutperformedthebaselinesby16%,demonstratingCATCH’ssignificantenhancement\nin the perception of various feature types. Additionally, on the CHAIR dataset, our\nmethod achieved a 45.8% improvement over the baselines, indicating that CATCH\neffectively mitigates cumulative hallucinations in open-ended generation scenarios.\n2 Results\nResults on POPE. As shown in Table 1, we evaluate our CATCH method on the\nPOPE dataset [24]. POPE assesses hallucinations as a binary classification task by\nasking yes/no questions about object presence (e.g., “Is there a dog in the image?”).\nThis benchmark aggregates data from three sources: MSCOCO [21], A-OKVQA [27],\nand GQA [28], and includes three subsets: random, popular, and adversarial, which\naddress object prevalence and co-occurrence patterns. Each sampling setting uses 500\nimages per dataset, with 6 questions per image, resulting in a total of 27,000 query-\nanswer pairs derived from the development sets. Evaluation is based on four key\nmetrics: Accuracy, Precision, Recall, and F1 score.\nWeobservedthatourCATCHmethodoutperformsthecurrentbestbyasignificant\nmargin. Compared to the baseline, it improves Accuracy and F1 score by up to 8.07\nand5.98points,respectively,demonstratingitseffectivenessinmitigatinghallucinated\nconcepts present in the original distribution.\nIn addition, it is worth noting that while all methods exhibit a clear performance\ndecline from the random to the popular setting, with a further drop in the adver-\nsarial setting, CATCH demonstrates superior performance in these more challenging\nscenarios. It improves accuracy by 6.65, 8.21, and 8.36 points over the baseline in the\nrandom,popular,andadversarialsettings,respectively.Theseresultsrevealthatmore\nchallenging tasks amplify the visual defects in LVLMs, whereas CATCH effectively\nreduces the density of extraneous visual information, preventing reliance on language\npriors. We also observed that, compared to Recall, CATCH achieved a more signif-\nicant improvement in Precision, which can be attributed to its lower ”yes” response\nratio compared to the baseline. This suggests that CATCH is more conservative and\nstable when handling uncertain responses.\n7\nTable 1: Results on POPE benchmark. The best results are bolded, and the\nsecond-best are underlined.\nLLaVA 1.5 InstructBLIP\nSetup Method\nAcc.↑ Prec.↑ Rec.↑ F1↑ Acc.↑ Prec.↑ Rec.↑ F1↑\nbase 84.13 82.86 86.07 84.43 82.80 82.24 83.67 82.95\nVCD 85.37 83.14 88.73 85.84 83.93 84.42 82.67 83.73\nRandom M3ID 86.00 85.11 87.27 86.18 84.37 84.62 84.00 84.31\nRITUAL 88.87 89.23 88.40 88.81 88.83 90.48 86.80 88.60\nCATCH 90.43 93.04 87.40 90.13 90.17 92.28 87.67 89.91\nbase 80.87 78.23 85.53 81.72 75.80 72.74 82.53 77.33\nVCD 81.10 77.78 87.07 82.16 77.73 75.43 82.27 78.70\nPopular M3ID 82.83 79.62 88.27 83.72 77.30 74.10 83.93 78.71\nRITUAL 85.83 84.17 88.27 86.17 81.97 78.90 87.27 82.87\nCATCH 87.07 90.12 83.27 86.56 83.70 81.22 87.67 84.32\nbase 76.23 71.75 86.53 78.45 75.40 71.60 84.20 77.39\nVCD 75.60 70.78 87.20 78.14 76.80 73.62 83.53 78.26\nAdversarial M3ID 77.70 73.23 87.33 79.66 76.03 72.48 83.93 77.79\nRITUAL 78.80 74.43 87.73 80.54 78.73 74.57 87.20 80.39\nCATCH 83.17 83.10 83.27 83.18 79.90 75.82 87.80 81.37\nbase 81.73 76.53 91.53 83.36 81.13 78.03 86.67 82.12\nVCD 81.83 75.74 93.67 83.76 82.00 79.38 86.47 82.77\nRandom M3ID 83.57 77.86 93.80 85.09 82.33 77.81 90.47 83.66\nRITUAL 85.17 79.79 94.20 86.40 87.13 83.92 91.87 87.71\nCATCH 89.63 88.83 90.67 89.74 89.43 86.22 93.87 89.88\nbase 76.67 70.51 91.67 79.71 75.67 70.97 86.87 78.12\nVCD 74.70 68.12 92.87 78.59 76.50 71.69 87.60 78.85\nPopular M3ID 76.80 70.20 93.13 80.06 75.60 70.40 88.33 78.36\nRITUAL 78.83 71.99 94.40 81.68 78.73 72.83 91.67 81.17\nCATCH 84.63 80.90 90.67 85.51 80.90 74.54 93.87 83.09\nbase 67.40 61.78 91.27 73.68 68.00 63.08 86.80 73.06\nVCD 67.43 61.48 93.33 74.13 70.67 65.24 88.47 75.10\nAdversarial M3ID 68.10 61.99 93.60 74.58 69.57 64.21 88.40 74.39\nRITUAL 68.57 62.26 94.27 74.99 70.27 64.15 91.87 75.55\nCATCH 75.47 69.53 90.67 78.70 71.90 65.22 93.87 76.96\nbase 81.23 75.42 92.67 83.16 79.93 76.73 85.93 81.07\nVCD 81.50 74.78 95.07 83.71 81.83 79.03 86.67 82.67\nRandom M3ID 82.83 76.64 94.47 84.62 80.57 76.77 87.67 81.85\nRITUAL 86.10 80.30 95.67 87.31 84.87 82.52 88.47 85.39\nCATCH 89.97 88.80 91.47 90.11 86.63 84.11 90.33 87.11\nbase 72.50 65.85 93.47 77.27 72.73 68.14 85.40 75.80\nVCD 71.57 64.72 94.80 76.93 73.67 68.82 86.53 76.67\nPopular M3ID 72.83 66.04 94.00 77.58 74.57 69.45 87.73 77.53\nRITUAL 74.80 67.50 95.67 79.15 74.50 69.17 88.40 77.61\nCATCH 82.97 78.18 91.47 84.30 76.93 71.24 90.33 79.66\nbase 67.63 61.68 93.13 74.21 69.57 64.80 85.67 73.79\nVCD 67.47 61.38 94.20 74.33 69.43 64.76 85.27 73.61\nAdversarial M3ID 68.13 61.88 94.47 74.78 68.90 64.06 86.13 73.47\nRITUAL 68.23 61.75 95.80 75.10 70.17 64.76 88.47 74.78\nCATCH 77.70 71.72 91.47 80.40 71.40 65.52 90.33 75.95\n8\nOCOC-SM\nAQVKO-A\nAQG\n(a) Results on MME Hallucination benchmark. For the LLaVA and InstructBLIP\nbaselines,weevaluatetheexistence,count,position,andcolorsubsets.Werandomlyselected\nfive different seeds and used the average as the final result.\n(b)ResultsonCHAIRbenchmark.LowerCHAIR andCHAIR scoresindicatebetter\nS I\nperformance.\nFig. 4: Results on MME Hallucination (4a) and CHAIR benchmark (4b).\nResults on MME Hallucination. MME Hallucination [26] is a comprehensive\nbenchmark designed for evaluating LVLMs, comprising four subsets: existence, count,\nposition, and color. Each subset contains 30 images and 60 questions, with two ques-\ntions per image. Similar to POPE, these questions are structured as binary yes/no\nqueries, and performance is measured based on binary accuracy.\nMME Hallucination is more challenging than POPE, as it includes attribute-level\nhallucinations related to position and color, in addition to the existence and count\ndimensions. As shown in Fig. 4a, the first row presents the evaluation results with\nLLaVa as the baseline, while the second row shows the results with InstructBLIP as\nthe baseline. The first four columns represent performance on the existence, count,\nposition, and color subsets, respectively, while the final column shows the total score\n9\nacross all four subsets. CATCH demonstrates significant improvements in total score,\nwith increases of 16% and 13.4% for LLaVa and InstructBLIP baselines, respectively.\nNotably, when comparing the results with LLaVa as the baseline to other LVLMs on\nthe MME leaderboard, the positive impact of CATCH is comparable to upgrading\nLLaVa 7B to LLaVa 13B, GPT-4V, LLaVa 1.6 34B, and Qwen-VL-Plus across the\nexistence, count, position, and color dimensions.\nResults on CHAIR. CHAIR [25] uses ground-truth captions and object\nannotations to evaluate hallucinations in LVLMs by calculating the proportion of\nhallucinated objects in generated captions relative to actual objects. This evalu-\nation is based on two metrics: CHAIR = |{hallucinatedobjects}| , CHAIR =\nS |{allmentionedobjects}| I\n|{captionswithhallucinatedobjects}|. CHAIR represents assessments at the sentence\n|{allcaptions}| S\nlevel,measuringtheproportionofhallucinatedsentencesrelativetoallsentences,while\nCHAIR measures hallucinations at the object instance level, indicating the propor-\nI\ntion of hallucinated objects relative to all generated objects. Lower scores indicate\nfewer hallucinations. We randomly selected 500 images from the COCO validation\nset and conducted image captioning using the prompt, ”Please describe this image\nin detail.” Fig. 4b shows the evaluation results on the CHAIR dataset. For LLaVA,\nCATCH achieves scores of 14.2 on CHAIR and 4.7 on CHAIR , representing sig-\nS I\nnificant improvements over the baseline scores of 26.2 and 9.3, with gains of 45.8%\nand 49.5%, respectively. Similarly, when using InstructBLIP as the baseline, CATCH\nachievesscoresof21.2onCHAIR and7.1onCHAIR ,comparedtobaselinescores\nS I\nof 28.6 and 10.3, showing improvements of 25.9% and 31.1%.\nCATCH significantly prevent cumulative hallucinations. As shown in Fig. 5a, we\ncalculatedtheJSdivergencebetweentheoutputdistributionsfromtheoriginalvisual\ninput and non-visual input with the baseline, as well as between the decoupled visual\ninputandthenon-visualinputwithCATCHduringgeneration.Weobservedthatfrom\nthe39thtokenonward,theoutputdistributionsfromtheoriginalvisualinputandthe\nnon-visualinputbecomealmostidentical.Thisindicatestheonsetofcumulativehallu-\ncinations, where the LVLM output relies solely on language priors, sharply increasing\nthe likelihood of hallucinations, such as “mouse” and “cell phone”. In contrast, the\noutput distribution from the decoupled image maintained a greater divergence from\nthat of the non-visual input throughout the generation process, with cumulative hal-\nlucinationsoccurringonlyafterthe101sttoken.Thissuggeststhat:(1)thedecoupled\nimage contains less visual uncertainty due to the separation of extraneous informa-\ntion compared to the original visual input, and (2) in the earlier generation steps,\nCATCH mitigates hallucinated concepts and enhances diversity. For further analysis,\nas showm in Fig. 5b, we randomly sampled 1,000 examples and plotted the distribu-\ntionof thepointsat whichcumulativehallucinationsoccur. Theresultsindicate that,\nwiththebaseline,cumulativehallucinationsfortheoriginalvisualinputmostlyoccur\nfrom40%ofthesequencelength,whereasforthedecoupledimagewithCATCH,they\noccur mostly from 80% of the sequence length.\n10\n(a) The first row presents an example where CATCH eliminates hallucinations such as\n“mouse” and“phone” thatweregeneratedbythebaseline.Thesecondrowshowsthecorre-\nsponding cumulative hallucinations analysis, indicating that the baseline model experiences\ncumulativehallucinationsasearlyasthe39thtoken,whereasCATCHdelaystheoccurrence\nof cumulative hallucinations until the 101st token.\n(b) We provide extensive statistical analysis across 1,000 samples, demonstrating that the\nbaselinemodelexperiencescumulativehallucinationsformostsamplesby40%ofthesentence\nlength,whereasCATCHeffectivelydelayscumulativehallucinationsinasubstantialportion\nof samples until approximately 80% of the sentence length.\nFig. 5: An analysis of cumulative hallucinations includes a single-sample experiment,\nas shown in (5a), and an extensive statistical analysis, as presented in (5b)\n11\n3 Methods\n3.1 Formulation of LVLMs\nWe consider an LVLM parameterized by θ. The model receives a textual query x and\na visual input v, where v provides contextual visual information to aid the model in\ngenerating a relevant response y to the query. Initially, the raw image v is processed\nthrough a vision encoder to extract visual features. These features are then mapped\ninto the input space of the large language model through a vision-language alignment\nmodule (e.g., Q-Former [29], linear projection [12]), generating visual tokens. Subse-\nquently,thesevisualtokens,togetherwithtextualtokensobtainedthroughembedding\nthe query, are fed into the large language model to auto-regressively generate the\nresponse. Mathematically, this process can be formulated as follows:\ny ∼p (y |v,x,y )∝explogit (y |v,x,y ), (1)\nt θ t <t θ t <t\nwhere y denotes the token generated at time step t, and y represents the sequence\nt <t\nof tokens generated up to time step t − 1. During the decoding phase in LVLMs,\nhallucinations arise when the generated probability distribution deviates from the\nfactual information provided by the visual input v. To mitigate hallucinations arising\nfromvisualdefect,weproposetheComplementaryVisualDecouplin(CVD)forvisual\ninformation separation, Non-Visual Screening (NVS) for hallucination detection, and\nAdaptive Token-level Contrastive Decoding (ATCD) for hallucination mitigation.\n3.2 Complementary Visual Decoupling\nThe visual defect fundamentally arises from the unbalanced alignment in vision-\nlanguage multimodal integration, leading to a visual information bottleneck. Inspired\nbytheinformationbottleneckprinciple,whichaddressesthisissuebyremovingirrele-\nvantinformationandpreservingonlyinformationpertinenttothecurrentpredictionto\ncreateamorestablerepresentation,weproposetheComplementaryVisualDecoupling\n(CVD) method to preserve essential visual information while removing extraneous\ndetails.\nAssuming we have raw information v and label y, the core idea of the information\nbottleneckprincipleistomaptheobservationofv toarobustrepresentationz,which\nretains the essential characteristics needed for predicting y while simultaneously min-\nimizing redundant information. Theoretically, treating v, z, and y as three random\nvariables, the optimization objective is to maximize the mutual information between\nz andy,whileminimizingthemutualinformationbetweenz andv.Thisoptimization\nobjective can be defined as:\nmin[I(v;z)−I(z;y)]. (2)\nIn this paper, the raw information v represents the visual input, and the label y\nrepresents the accurate response. We employ the Segment Anything Model (SAM)\nto optimize this objective. Specifically, as illustrated in Fig. 2, a pre-trained SAM is\n12\nutilized to segment all objects:\n{O ,O ,...,O ,B}=SAM(v), (3)\n1 2 N\nwhere O denotes the objects and B represents the background. Using the area of the\ntarget region as the confidence score, we select the top M objects as the exposed\nportion and mask the remaining objects and background to obtain the dual image z .\nd\nSimilarly,wemaskthetopM objectsandtreattheremainingobjectsandbackground\nas the target, generating a residual image z :\nr\nM M\n(cid:88) (cid:88)\nz =( O )⊙v z =(1− O )⊙v. (4)\nd i r i\ni i\nThrough CVD, the original visual information is decoupled into two simplified com-\nponents:thedualimagez andtheresidualimagez ,leveragingtheircomplementary\nd r\nnature.\n3.3 Non-Visual Screening\nAt each generation step, key visual features for the next token are dynamically\nemphasized in one part while being obscured in the other. The decoupled image z\nis designated as the one that retains the highlighted key visual features, effectively\neliminating extraneous details. We observed that when critical visual information rel-\nevant to the next token is obscured, the output distribution becomes dominated by\nlanguage priors. Thus, to identify the correct decoupled image z between the dual\nimage and the residual image, we introduce a non-visual input z , containing only\nn\nthe textual prompt, without any visual information, to serve as an assistant. We then\ncalculate the Jensen-Shannon Divergence (JSD) as the distance between the output\ndistributions from the non-visual input and the dual image as d(z ,z ), and between\nd n\nthe non-visual input and the residual image as d(z ,z ):\nr n\nd(z , z )=D (p (y |z , x, y )∥p (y |z , x, y )),\nd n JS θ t d <t θ t n <t\nd(z , z )=D (p (y |z , x, y )∥p (y |z , x, y )), (5)\nr n JS θ t r <t θ t n <t\nwhereD (P||Q)= 1D (P||M)+1D (Q||M),M = 1(P+Q),andD (P||Q)=\nJS 2 KL 2 KL 2 KL\n(cid:80) P(i)log P(i). The visual input corresponding to the greater distance is selected as\ni Q(i)\nthe decoupled image z, formulated as:\n(cid:40)\nz , if d(z , z ) ≥ d(z , z );\nz = d d n r n (6)\nz , if d(z , z ) < d(z , z ).\nr d n r n\n3.4 Adaptive Token-level Contrastive Decoding\nWe first calculate the distance between the output distributions from the non-visual\ninput and the decoupled image, denoted as d(z,z ), and the distance between the\nn\n13\noutput distributions from the original visual input and the non-visual input, denoted\nas d(v,z ):\nn\nd(z, z )=D (p (y |z, x, y )∥p (y |z , x, y )),\nn JS θ t <t θ t n <t\nd(v, z )=D (p (y |v, x, y )∥p (y |z , x, y )), (7)\nn JS θ t <t θ t n <t\nWe consider two scenarios: (1) Hallucination Existence: When d(z,z ) is greater\nn\nthan d(v,z ), we conclude that hallucinations are present in the original output dis-\nn\ntribution. Therefore, we use the output distribution from the decoupled image to\ncontrastively subtract the original distribution. (2) Diversity Insufficient: When\nd(z,z ) is less than d(v,z ), we consider there to be a risk of cumulative hallu-\nn n\ncinations. In this case, we use the output distribution from the decoupled image\nto contrastively enhance the weighted original distribution, thereby improving the\ndiversity of generation, formulated as:\n\nsoftmax[α·logit (y|z, x)−logit (y|v, x)],\n θ\nif d(z , z\n)θ\n≥d(v, z );\ny ∼p (y |z, v, x)= d n n (8)\nt θ t\nsoftmax[β·logit θ(y|v, ifx d) (+\nz\nl ,o zgit )θ( <y d| (z\nvd\n,, zx) )] .,\nd n n\nHere, the hyperparameters α and β represent the amplification factors. The final\ngenerated token y is sampled from p .\nt θ\n3.5 Hardware and Implementtation Details\nAll experiments in this paper were conducted on an NVIDIA RTX 3090 24GB GPU.\nFor SAM, we utilized the pre-trained ViT-H SAM model. The decoding process was\nconfigured with α=1.2 and β =3 by default. The number of objects in CVD was set\nto be N ∗0.05 by default.\n4 Discussion\nIn this paper, we reveal that hallucinations in LVLMs primarily stem from the visual\nprocessing bottleneck caused by vision-language misalignment, which we term the\nvisual defect. Unlike previously identified issues of language bias and statistical bias,\nthe visual defect is challenging to resolve through high-quality data, chain-of-thought\nprompting, or fine-tuning, due to limitations in the pre-trained visual architecture.\nBased on the Information Bottleneck theory, we introduce Complementary Visual\nDecoupling (CVD) for visual information decoupling, incorporate a non-visual input\nforNon-VisualScreening(NVS)todetecthallucinations,andproposeAdaptiveToken-\nlevel Contrastive Decoding (ATCD) for hallucination mitigation through contrastive\ndecoding.\nCATCH demonstrates impressive performance and robustness in perceiving fine-\ngrained features and mitigating cumulative hallucinations in open-ended scenarios, as\nevidenced by the results on the CHAIR, POPE, and MME datasets across different\n14\nbaselines.OneexcitingaspectofCATCHisthatitrequiresnoadditionalpriorknowl-\nedge, data, or training, making it efficiently applicable to various LVLMs. We hope\nthat introducing the concept of visual defects opens new research avenues for study-\ning hallucinations and that our complementary contrastive decoding method inspires\nnew questions and approaches in the pretraining and fine-tuning of LVLMs.\nReferences\n[1] Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh,\nD.: Vqa: Visual question answering. In: Proceedings of the IEEE International\nConference on Computer Vision, pp. 2425–2433 (2015)\n[2] Gupta, A., Savarese, S., Ganguli, S., Fei-Fei, L.: Embodied intelligence via\nlearning and evolution. Nature communications 12(1), 5721 (2021)\n[3] Chen, J., Ouyang, R., Gao, A., Chen, S., Chen, G.H., Wang, X., Zhang, R.,\nCai, Z., Ji, K., Yu, G., et al.: Huatuogpt-vision, towards injecting medical visual\nknowledgeintomultimodalllmsatscale.arXivpreprintarXiv:2406.19280(2024)\n[4] Cui,C.,Ma,Y.,Cao,X.,Ye,W.,Zhou,Y.,Liang,K.,Chen,J.,Lu,J.,Yang,Z.,\nLiao,K.-D.,etal.:Asurveyonmultimodallargelanguagemodelsforautonomous\ndriving.In:ProceedingsoftheIEEE/CVFWinterConferenceonApplicationsof\nComputer Vision, pp. 958–979 (2024)\n[5] Bai,Z.,Wang,P.,Xiao,T.,He,T.,Han,Z.,Zhang,Z.,Shou,M.Z.:Hallucination\nof multimodal large language models: A survey. arXiv preprint arXiv:2404.18930\n(2024)\n[6] Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K.,\nMensch,A.,Millican,K.,Reynolds,M.,et al.:Flamingo:avisuallanguagemodel\nfor few-shot learning. Advances in neural information processing systems 35,\n23716–23736 (2022)\n[7] Yu, Q., Li, J., Wei, L., Pang, L., Ye, W., Qin, B., Tang, S., Tian, Q., Zhuang,\nY.: Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 12944–12953 (2024)\n[8] Wang, L., He, J., Li, S., Liu, N., Lim, E.-P.: Mitigating fine-grained halluci-\nnation by fine-tuning large vision-language models with caption rewrites. In:\nInternational Conference on Multimedia Modeling, pp. 32–45 (2024). Springer\n[9] Yue, Z., Zhang, L., Jin, Q.: Less is more: Mitigating multimodal hallucination\nfrom an eos decision perspective. arXiv preprint arXiv:2402.14545 (2024)\n[10] Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q.,\nZhu,X.,Lu,L.,et al.:Internvl:Scalingupvisionfoundationmodelsandaligning\n15\nfor generic visual-linguistic tasks. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 24185–24198 (2024)\n[11] Liu,H.,Li,C.,Li,Y.,Lee,Y.J.:Improvedbaselineswithvisualinstructiontuning.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 26296–26306 (2024)\n[12] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. Advances in neural\ninformation processing systems 36 (2024)\n[13] Zhai,B.,Yang,S.,Xu,C.,Shen,S.,Keutzer,K.,Li,M.:Halle-switch:Controlling\nobject hallucination in large vision language models. arXiv e-prints, 2310 (2023)\n[14] He, X., Wei, L., Xie, L., Tian, Q.: Incorporating visual experts to resolve\nthe information loss in multimodal large language models. arXiv preprint\narXiv:2401.03105 (2024)\n[15] Jain, J., Yang, J., Shi, H.: Vcoder: Versatile vision encoders for multimodal large\nlanguage models. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 27992–28002 (2024)\n[16] Tong,S.,Liu,Z.,Zhai,Y.,Ma,Y.,LeCun,Y.,Xie,S.:Eyeswideshut?exploring\nthe visual shortcomings of multimodal llms. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 9568–9578 (2024)\n[17] Chen, Z., Zhao, Z., Luo, H., Yao, H., Li, B., Zhou, J.: Halc: Object hallucination\nreduction via adaptive focal-contrast decoding. arXiv preprint arXiv:2403.00425\n(2024)\n[18] Leng, S., Zhang, H., Chen, G., Li, X., Lu, S., Miao, C., Bing, L.: Mitigating\nobject hallucinations in large vision-language models through visual contrastive\ndecoding.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand\nPattern Recognition, pp. 13872–13882 (2024)\n[19] Favero,A.,Zancato,L.,Trager,M.,Choudhary,S.,Perera,P.,Achille,A.,Swami-\nnathan, A., Soatto, S.: Multi-modal hallucination control by visual information\ngrounding. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 14303–14312 (2024)\n[20] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,\nWhitehead, S., Berg, A.C., Lo, W.-Y., et al.: Segment anything. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pp. 4015–4026\n(2023)\n[21] Lin,T.-Y.,Maire,M.,Belongie,S.,Hays,J.,Perona,P.,Ramanan,D.,Doll´ar,P.,\nZitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision–\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12,\n16\n2014, Proceedings, Part V 13, pp. 740–755 (2014). Springer\n[22] Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P.,\nHoi, S.: InstructBLIP: Towards General-purpose Vision-Language Models with\nInstruction Tuning (2023). https://arxiv.org/abs/2305.06500\n[23] Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang,\nS.,Zhuang,Y.,Gonzalez,J.E.,et al.:Vicuna:Anopen-sourcechatbotimpressing\ngpt-4with90%*chatgptquality.Seehttps://vicuna.lmsys.org(accessed14April\n2023) 2(3), 6 (2023)\n[24] Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.-R.: Evaluating object\nhallucination in large vision-language models. arXiv preprint arXiv:2305.10355\n(2023)\n[25] Rohrbach, A., Hendricks, L.A., Burns, K., Darrell, T., Saenko, K.: Object\nhallucination in image captioning. arXiv preprint arXiv:1809.02156 (2018)\n[26] Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li,\nK., Sun, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal\nlarge language models. arXiv preprint arXiv:2306.13394 (2023)\n[27] Schwenk, D., Khandelwal, A., Clark, C., Marino, K., Mottaghi, R.: A-okvqa: A\nbenchmark for visual question answering using world knowledge. In: European\nConference on Computer Vision, pp. 146–162 (2022). Springer\n[28] Hudson, D.A., Manning, C.D.: Gqa: A new dataset for real-world visual rea-\nsoning and compositional question answering. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 6700–6709 (2019)\n[29] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. In: International\nConference on Machine Learning, pp. 19730–19742 (2023). PMLR\n17",
    "pdf_filename": "CATCH_Complementary_Adaptive_Token-level_Contrastive_Decoding_to_Mitigate_Hallucinations_in_LVLMs.pdf"
}