{
    "title": "SNN-Based Online Learning of Concepts and Action Laws in an Open World",
    "context": "We present the architecture of a fully autonomous, bio-inspired cognitive agent built around a spiking neural network (SNN) implementing the agent’s semantic memory. The agent explores its universe and learns concepts of objects/situations and of its own actions in a one-shot manner. While object/situation concepts are unary, action concepts are triples made up of an initial situation, a motor activity, and an outcome. They embody the agent’s knowledge of its universe’s actions laws. Both kinds of concepts have different degrees of generality. To make decisions the agent queries its semantic memory for the expected outcomes of envisaged actions and chooses the action to take on the basis of these predictions. Our experiments show that the agent handles new situations by appealing to previously learned general concepts and rapidly modifies its concepts to adapt to environment changes. 1. The ability of a cognitive agent to act adequately in a given environment depends on its ability to predict how performing a given activity might affect its cur- rent situation, that is, it depends on its knowledge of its environment’s action laws. How artificial agents can acquire these laws and how these should be up- dated if their environment changes has proven a dif- ficult question. In the case where the intended envi- ronment is open—that is, where the agent’s designer cannot foresee all the situations the agent might en- counter in the future—, providing a suitable set of ac- tion laws to the agent “by hand” is unfeasible. The only viable solution is that the agent continuously learns the relevant laws from experience, just as nat- ural agents (humans and animals) do. Crucially, this learning process should allow for generalization over disparate experiences, so that the agent is able to be- have appropriately in new situations. It should also accommodate environment changes. The present paper intends to show how this could be done. Its main thrust is that natural agents’ abil- ity to perform well in our open and changing world relies on the fact that they store their knowledge in the form of concepts, which can have various de- grees of generality. Presumably, they first form con- cepts about the encountered objects and situations, and then use these as building blocks for relational concepts, among which are concepts of actions sup- porting their knowledge of their environment’s action laws. We suggest that artificial agents could do just the same, relying on some artificial neural network to learn and store concepts, and then querying it to make predictions about the outcome of envisaged actions. To test this idea, we here build an artificial hybrid agent with a SNN at its core. We make this agent live in a very simple virtual world, composed of rooms which may be, or not, accessible (hence, knowable) to it. At first, the agent is confined to one single room and learns by itself how to act in it according to its own interests. Then at some point a door opens to a new room, containing some never encountered before objects and situations. Yet, although these are new to the agent, some general laws are preserved from one room to the other. We show that having learned these laws in the first room allows the agent to act by and large properly in the second one, as soon as it enters it. We also show that relying on neurally implemented concepts allows the agent to rapidly update its knowl- edge and adapt to environment changes. The paper is organized as follows. In Section 2 we discuss related work, and in Section 3 we present the agent and its universe. Section 4 clarifies the notions of concepts and actions laws we use, while Section 5 describes the neural network and its functioning. Sec- tion 6 describes the agent’s general functioning and Section 7 presents the results. Finally, Section 8 con- cludes and discusses future possible developments of the framework. 2. Related Works Our research problem is autonomous online learning, generalization and updating of concepts and actions laws in an open universe. The intended application is reasoning and planning for autonomous robots. To our knowledge, no existing approach addresses this problem in all its dimensions, even though these are investigated in separate research fields. 1 arXiv:2411.12308v1  [cs.AI]  19 Nov 2024",
    "body": "SNN-Based Online Learning of Concepts and Action Laws\nin an Open World\nChristel Grimaud, Dominique Longin, Andreas Herzig\nUniversit´e de Toulouse, CNRS, Toulouse INP, UT3, IRIT, France\nAbstract\nWe present the architecture of a fully autonomous, bio-inspired cognitive agent built around a spiking neural\nnetwork (SNN) implementing the agent’s semantic memory. The agent explores its universe and learns concepts\nof objects/situations and of its own actions in a one-shot manner. While object/situation concepts are unary, action\nconcepts are triples made up of an initial situation, a motor activity, and an outcome. They embody the agent’s\nknowledge of its universe’s actions laws. Both kinds of concepts have different degrees of generality. To make\ndecisions the agent queries its semantic memory for the expected outcomes of envisaged actions and chooses the\naction to take on the basis of these predictions. Our experiments show that the agent handles new situations by\nappealing to previously learned general concepts and rapidly modifies its concepts to adapt to environment changes.\n1.\nIntroduction\nThe ability of a cognitive agent to act adequately in\na given environment depends on its ability to predict\nhow performing a given activity might affect its cur-\nrent situation, that is, it depends on its knowledge of\nits environment’s action laws. How artificial agents\ncan acquire these laws and how these should be up-\ndated if their environment changes has proven a dif-\nficult question. In the case where the intended envi-\nronment is open—that is, where the agent’s designer\ncannot foresee all the situations the agent might en-\ncounter in the future—, providing a suitable set of ac-\ntion laws to the agent “by hand” is unfeasible. The\nonly viable solution is that the agent continuously\nlearns the relevant laws from experience, just as nat-\nural agents (humans and animals) do. Crucially, this\nlearning process should allow for generalization over\ndisparate experiences, so that the agent is able to be-\nhave appropriately in new situations. It should also\naccommodate environment changes.\nThe present paper intends to show how this could\nbe done. Its main thrust is that natural agents’ abil-\nity to perform well in our open and changing world\nrelies on the fact that they store their knowledge in\nthe form of concepts, which can have various de-\ngrees of generality. Presumably, they first form con-\ncepts about the encountered objects and situations,\nand then use these as building blocks for relational\nconcepts, among which are concepts of actions sup-\nporting their knowledge of their environment’s action\nlaws. We suggest that artificial agents could do just\nthe same, relying on some artificial neural network to\nlearn and store concepts, and then querying it to make\npredictions about the outcome of envisaged actions.\nTo test this idea, we here build an artificial hybrid\nagent with a SNN at its core. We make this agent live\nin a very simple virtual world, composed of rooms\nwhich may be, or not, accessible (hence, knowable)\nto it. At first, the agent is confined to one single room\nand learns by itself how to act in it according to its\nown interests. Then at some point a door opens to a\nnew room, containing some never encountered before\nobjects and situations. Yet, although these are new to\nthe agent, some general laws are preserved from one\nroom to the other. We show that having learned these\nlaws in the first room allows the agent to act by and\nlarge properly in the second one, as soon as it enters\nit. We also show that relying on neurally implemented\nconcepts allows the agent to rapidly update its knowl-\nedge and adapt to environment changes.\nThe paper is organized as follows. In Section 2 we\ndiscuss related work, and in Section 3 we present the\nagent and its universe. Section 4 clarifies the notions\nof concepts and actions laws we use, while Section 5\ndescribes the neural network and its functioning. Sec-\ntion 6 describes the agent’s general functioning and\nSection 7 presents the results. Finally, Section 8 con-\ncludes and discusses future possible developments of\nthe framework.\n2.\nRelated Works\nOur research problem is autonomous online learning,\ngeneralization and updating of concepts and actions\nlaws in an open universe. The intended application\nis reasoning and planning for autonomous robots. To\nour knowledge, no existing approach addresses this\nproblem in all its dimensions, even though these are\ninvestigated in separate research fields.\n1\narXiv:2411.12308v1  [cs.AI]  19 Nov 2024\n\nContinual Learning tackles the problem of life-\nlong knowledge acquisition Wang et al. (2024);\nLesort et al. (2020). Its main challenge is to avoid\ncatastrophic loss of previous knowledge when acquir-\ning new knowledge; a secondary research axis is one-\nshot/few-shots learning, i.e., the ability to learn online\nfrom one or few examples Wang et al. (2020). How-\never, current approaches mostly consider the learn-\ning of tasks (mostly image classification/recognition\ntasks, but also some more complex tasks such as\nplaying games Kirkpatrick et al. (2017)), not of con-\ncepts nor action laws. Furthermore, most of them\nrely on supervised learning and/or labelled training\ndata, which is unsuitable for open world autonomous\nagents.\nConcept Learning has mainly been studied in\nview of explainability Gupta and Narayanan (2024),\nmostly of classification models (e.g., Koh et al.\n(2020)) but also of decision making in the context\nof reinforcement learning Das, Chernova, and Kim\n(2023); Zabounidis et al. (2023). For this reason,\nmany proposals are dedicated to learning a human-\npredefined set of concepts using some annotated data.\nIn the field of Image Classification some approaches\ndeal with the extraction of concepts from data Wang,\nLee, and Qi (2022); Ghorbani et al. (2019); Hase et al.\n(2019), but in these approaches concepts are extracted\nfrom labelled classes of images, which is, again, un-\nsuitable for open world autonomous agents. Further-\nmore, the vast majority of proposed methods disre-\ngard the hierarchical organization of concepts from\nparticular to more general, and they generally do not\naddress one-shot learning, online revision or updating\nof concepts.\nAction Learning has been studied from various\nperspectives. In Dynamic Epistemic Logic, Bolander\nand Gierasimczuk (2018) proposed a method to learn\nan action model through successive observations of\ntransitions between states. However this method does\nnot achieve generalization nor accommodate environ-\nment changes and only considers the universally ap-\nplicable actions (i.e., actions that can be executed\nin every logically possible state), a condition real-\nworld actions rarely satisfy. In the field of Planning,\nBonet, Frances, and Geffner (2019) showed how to\nlearn abstract actions from a few carefully chosen in-\nstances of some general planning problem. However\nsaid instances come with their own set of ground ac-\ntions which must be known beforehand, hence this ap-\nproach cannot be used in open worlds, where an agent\nneeds to incrementally learn from sequential observa-\ntions.\nReinforcement Learning (RL): Our work could\nbe related to model-based approaches of RL Moer-\nland et al. (2023), but differs from them in two impor-\ntant aspects. First, we are only interested in learning\na model of the environment, not in learning policies.\nDespite their undeniable successes, RL approaches\nstruggle to adapt to environment changes and to re-\nvise learned policies Kirk et al. (2023); Farebrother,\nMachado, and Bowling (2018). We believe that an\nagent that would be able to learn online a model of\nthe environment and to dynamically use it to make\ndecisions would be able to quickly adapt its behav-\nior. Second, the learning process we propose does not\ndepend on the existence of rewards, which makes it\nsuitable for contexts where rewards are scarce.\n3.\nThe Agent and its Universe\nThe agent’s universe is built over a grid of boxes,\nwhich we (not the agent) identify using an orthonor-\nmal coordinate system (see Figure 1).\nFigure 1. The agent’s accessible world. A: in the first\nphase, Room 1 only; B: after opening the door, Rooms\n1 and 2.\nEach box represents a particular location in the\nagent’s universe and possesses a particular set of fea-\ntures the agent is able to perceive, drawn from the\nset LF = {OK, KO, NorthWall, EastWall, SouthWall,\nWestWall, Cold, Sound, #0, #1, ..., #24}. Although\nthe agent’s universe is not finite, at any time point\nwe only consider the boxes to which it has access,\nthe set of which is always finite. For example, the\nbox with coordinates (−2, −2) has the feature set\nLF (−2,−2) = {OK, SouthWall, WestWall, Cold, #0},\nwhile the box with coordinates (5, 0) has the feature\nset LF (5,0) = {OK} (“#n” is to be taken as a par-\nticular name for a box hence a feature, not all boxes\nneed to have one). Two boxes with the same fea-\nture set are indistinguishable for the agent. Boxes’\nfeature sets may change over time, reflecting envi-\nronment changes. The rooms are made out of boxes,\nand delimited with impassable walls. Opening a door\namounts to removing the wall features from the con-\ncerned boxes’ feature sets (as in Figure 1.B).\nThe agent is composed of a set of sensors, a per-\nceptual system, a semantic memory, a decision sys-\ntem, a motor system and a set of actuators (see Fig-\nure 2). Sensors collect data from the external world\nand feed it to the perceptual system, which performs\n2\n\nFigure 2. Schema of the agent\nfeature/object recognition. Since we want the agent\nto learn by itself, we need this process to rely on un-\nsupervised learning with unlabelled data. Neural net-\nworks doing this already exist Thiele, Bichler, and\nDupret (2018); Kheradpisheh et al. (2017), so we sim-\nply suppose that the agent’s perceptual system oper-\nates as intended and provides the semantic memory\nwith the appropriate inputs, namely, the features of\nthe agent’s current location. Furthermore, we suppose\nthat the agent’s observations are always correct and\ncomplete.\nSemantic memory forms concepts by binding to-\ngether the perceived features, and stores them for fur-\nther retrieval. Its modeling is the main focus of this\npaper. The decision system is the other important part:\nit queries the semantic memory to predict the outcome\nof possible actions, and decides which one to take on\nthe basis of these predictions. This decision is then\nsent to the motor system, which activates the actua-\ntors to perform the corresponding motor activity. In-\nformation from the actuators is sent back to semantic\nmemory through proprioception, allowing the agent\nto memorize the motor-related features of the realized\nactions.\nThe agent’s possible actions consist in steps from\none box to another adjacent box, in any of the eight\ndirections. Formally, an action is a triple composed of\na depart location, a motor activity, and an outcome.\nBy “motor activity” we mean the fact that the agent’s\nactuators are activated so as to make it move to the im-\nmediate next box in the selected direction. The set of\nmotor activities’ features the agent is able to perceive\nby proprioception is the set MF = {N, NE, E, SE, S,\nSW, W, NW, Diag., Orth.}, where the first eight are\nspecific to each particular direction, while Diag. and\nOrth. are common features shared by all motor ac-\ntivities yielding diagonal/orthogonal moves. In cases\nwhere there is a wall at the edge of the depart box in\nthe selected direction, the agent bumps into it and re-\nmains at the same place. We then say that the action’s\noutcome is a failure. Otherwise, the action’s outcome\nis the agent’s new location.\nAs for the locations’ features, we suppose that KO\ncorresponds to some unpleasant stimulus the agent\nspontaneously wants to avoid, and OK to the absence\nof such a stimulus, while the others convey some in-\ndifferent information.\n4.\nConcepts and Action Laws\nThe agent is able to form two kinds of concepts. First,\nconcepts of “things”, in the broad sense. These bind\ntogether co-occurrent features, and can be seen as\nsome sort of conjunction in which conjuncts have dif-\nferent “weights”, reflecting the fact that some features\nare more important than others in a concept’s defi-\nnition Freund (2008). They are used to store knowl-\nedge about locations and more generally any object,\nso we call them object concepts. The second kind\nis relational concepts. These take other concepts as\nelements, and bind them together into tuples. Con-\ncepts of actions are of this kind: they bind together\nthe agent’s concepts of a depart location, a performed\nmotor activity, and a subsequent outcome, in the order\nin which they were experienced.\nWe say that an object concept X is general, as op-\nposed to particular, if there is another concept Y such\nthat the set of features composing X is a strict subset\nof the set of features composing Y. Y is then said to\nbe more particular than X. We say that an action con-\ncept is general if the object concept of its initial situ-\nation is general or its motor activity component only\ncontains Diag. or Orth.. We understand the generality\nof concepts relative to the set of concepts the agent\npossesses at some point, so no concept is general or\nparticular in itself.\nFor example, when visiting the box (0, 0) the agent\nmay form the particular object concept [OK,#12],\nwhich is a memory of an OK place with name\n#12, and only applies to this particular box in its\naccessible universe. If it then moves North-East\nand arrives at box (1, 1), it can form the particular\nobject concept [OK,#18], and also the particular\naction concept [[OK,#12],[NE,Diag],[OK,#18]]\nwhich corresponds to the memory of being in an OK\nplace with name #12 and then moving North-East\nto arrive at another OK place with name #18. Yet,\nafter visiting a number of locations having the feature\nOK in common, the agent may also form the general\nobject concept [OK]. Furthermore, it is a general rule\nin its accessible universe that moving North-East\nfrom an OK location always leads to another OK\nlocation, except for when there is a wall at the North\nor East edge of the depart box. Therefore, after having\nexperienced a number of North-East moves from\nvarious OK locations, the agent may form general\naction concepts such as [[OK],[NE,Diag],[OK]]\nand\n[[OK,NorthWall][NE,Diag],[Failure]].\nSuch general concepts capture the general (non-\nmonotonic) action laws of the agent’s universe,\nand are the ones it shall rely on to behave in never\nencountered situations.\n3\n\n5.\nImplementing the Agent’s Semantic\nMemory in the Neural Network\nSpiking Neural Networks (SNNs) are well suited for\nautonomous learning in open universes, as they allow\nfor Spike Time Dependent Plasticity (STDP), a fam-\nily of biologically plausible learning rules which can\nachieve unsupervised online learning from unlabelled\ndata Thiele, Bichler, and Dupret (2018). They are also\nknown for being energy-efficient, which is interesting\nfor autonomous robots.\nWe take inspiration in the JAST learning rule\nThorpe et al. (2019); Thorpe (2023), which is a sim-\nplified version of STDP where the sum of the affer-\nent connections weights on any given neuron remains\nconstant through learning. However, contrary to JAST\nwe do not use binary weights but natural numbers.\nMoreover, we do not freeze neurons after learning, so\nas to allow updating.\nThe Network’s Architecture\nThe network is composed of an interface, which com-\nmunicates with the agent’s other components, and a\nbody of hidden neurons which is itself divided into\ntwo layers (see Figure 3).\nThe first layer learns object concepts and the sec-\nond learns action concepts. For this reason we call\ntheir neurons, respectively, object concept neurons\n(O-neurons for short) and action concept neurons (A-\nneurons). This architecture draws on neuroanatomi-\ncal studies according to which concepts are repre-\nsented in the brain by hierarchically organized con-\ncept neurons, each receiving information from some\nlower neurons and sending reciprocal connections to\nthese same neurons so that it can reactivate them for\ninformation retrieval Quiroga (2012); Bausch et al.\n(2021); Shimamura (2010). For simplicity we do not\nmodel these reciprocal connections as such, but in-\nstead we allow for information to flow in both direc-\ntions along the same connections: from interface neu-\nrons to O-neurons and then to A-neurons for learning\nand querying, and the other way round for retrieving\ninformation. A key point is that interface neurons are\nboth input and output neurons, depending on the com-\nputational phase.\nInterface neurons (I-neurons for short) mainly sup-\nport the representation of features, be it of the visited\nlocations or of the agent’s own motor activities. An\nadditional neuron acts as a failure detector, specifi-\ncally firing when the agent bumps into a wall and re-\nmains at the same place. All of them have their labels\nfixed from the start.\nThe first layer of hidden neurons is composed\nof 100 integrate and fire neurons, with a differen-\ntiated dynamics depending on whether their input\nFigure 3. Schema of the SNN. O-neuron #1 supports\nthe concept [#0, Cold, SouthWall, WestWall, OK],\nand O-neuron #2 the concept [#5, SouthWall, KO].\nA-neuron #1 supports the concept\n[[#0, Cold,\nSouthWall, WestWall, OK], [E, Orth], [#5, South-\nWall, KO]], and A-neuron #3 the concept [[OK],\n[Diag], [OK]].\nsource is I-neurons or A-neurons. O-neurons learn co-\noccurrences of perceived features.\nThe second layer is composed of 400 compart-\nment neurons with three separate input compartments.\nThe first compartment receives connections from O-\nneurons, the second one from motor activities I-\nneurons, and the third one from O-neurons and Fail-\nure neuron. Inputs received at each compartment are\nunable to trigger a spike by themselves, but the first\nand second compartments make their next compart-\nment ready to receive and transmit inputs for a certain\namount of time. In this manner, inputs can only be\nefficient if they occur in the correct order, so that A-\nneurons encode sequences of inputs. The use of com-\npartment neurons to learn sequences was suggested in\nCui, Ahmad, and Hawkins (2016); Hawkins and Sub-\nutai (2016).\nThe Network’s functionning\nWe briefly describe the network’s functioning. A\nmore detailed account is provided in Supplementary\nMaterial.\n4\n\nO-neurons’ Learning\nEach time the agent observes its current location,\ninformation from the perceptual system is sent to\nthe network’s interface, inducing the firing of the I-\nneurons that encode the location’s features. This in\nturn triggers the firing of a number of O-neurons. If\nthis number reaches some fixed target number, the\nnetwork directly proceeds to make them learn. Other-\nwise it looks for additional O-neurons by “boosting”\ntheir input. In practice, boosting consists in multiply-\ning the input received by each non-firing O-neuron o\nby a factor bo, which is an increasing function of the\nnumber of steps performed since o’s last spike. This\nprocedure favors the firing of O-neurons that have\nbeen inactive for a long time, which are then re-used\nfor learning.\nThe learning process depends on the accuracy of\nthe agent’s knowledge about its current location. To\nassess it, the (pre-boosting) firing O-neurons send a\nbackward input to I-neurons, and the resulting set of\nfiring I-neurons (the retrieved information) is com-\npared with the initial input (the current observation).\nIf all the observed features can be retrieved from ac-\ntive O-neurons, then all inactive I-synapses (if any) on\nthe learning O-neurons are deleted and replaced with\nsynapses from input neurons. This procedure tends to\nreinforce O-neurons’ connections with I-neurons en-\ncoding well shared features at the expense of connec-\ntions with I-neurons encoding more specific features,\nfostering the learning of general concepts. If, on the\ncontrary, not all the observed features can be retrieved\nfrom the firing O-neurons, then we pick one learning\nO-neuron with maximal number of steps since its last\nlearning and replace all its synapses, active or inac-\ntive, with synapses from input neurons. This neuron\nthus learns the particular situation with all its features.\nThe learning process is similar to the first case for the\nother learning neurons.\nQuerying the network\nTo query the neural network, the decision system\nfirst sends an input to the I-neurons that encode the\nfeatures of the initial situation. Their firing brings a\nnumber of O-neurons to fire, sending an input to A-\nneurons’ first compartment. Then, given an envisaged\nmotor activity m, the decision system sends an in-\nput to the I-neurons that encode m’s features. These\nfire, and send an input to A-neurons’ second compart-\nment. A-neurons’ spiking threshold is then gradually\nlowered until a target number of them fire, sending a\nbackward input to O-neurons through their third com-\npartment’s connections. The O-neurons that fire in re-\nsponse to that input in turn send a backward input\nto I-neurons, the firing of which is the network’s re-\nsponse to the query. The value of A-neurons’ spiking\nthreshold at the moment a given I-neuron spikes de-\ntermines the agent’s confidence in the feature’s pre-\ndiction: the higher its value, the higher the confi-\ndence. So, formally, the querying process returns a\nset Pm = {(f1, c1), ...(fn, cn)}, where fi is a fea-\nture and ci the degree of confidence the agent has in\nits prediction.\nA-neurons’ Learning\nAt each step, O-neurons responding to the depart lo-\ncation and I-neurons responding to the performed mo-\ntor activity send inputs to, respectively, A-neurons’\nfirst and second compartments. A-neurons reaching\na certain threshold are selected for learning. If their\nnumber reaches some fixed target number, the net-\nwork directly proceeds to make them learn, otherwise\nit looks for additional A-neurons by boosting their in-\nput in a way similar to the one used for O-neurons.\nFor each learning A-neuron a we compute a learn-\ning rate LRa, which is an increasing function of the\nnumber of steps performed since the neuron’s last\nlearning. The idea is that seldom used neurons tend to\nencode more particular concepts than often used neu-\nrons, and should thus be able to learn more rapidly to\nretain more features from a given situation.\nThe learning process depends on the accuracy of\nthe agent’s predictions relative to the action’s out-\ncome (see Section 6). If these are correct (i.e., all\nthe expected features are actually present) and com-\nplete (i.e., all the actually present features were ex-\npected), then the learning process replaces all in-\nactive synapses in a’s first two compartments with\nsynapses from their respective input neurons, but only\nmin(LRa, i) in the third one, where i is the number of\ninactive synapses. If they are not correct, then all inac-\ntive synapses plus some active ones are replaced in a’s\nfirst two compartments, with max(LRa, i) the num-\nber of synapses replaced in each one. The learning\nprocess for the third compartment is the same as in the\nfirst case. If the agent’s predictions are not complete,\nthen the number of replaced synapses is max(LRa, i)\nin all three compartments. This differentiated learn-\ning process aims at promoting generalization when\npredictions are correct and complete while allowing\na few neurons to specialize when they are not.\n6.\nFunctioning of the Agent\nSuppose the agent is at some depart location and ob-\nserves it: information from its perceptual system trig-\ngers the firing of the I-neurons encoding the location’s\nfeatures and is transmitted from there to the decision\nsystem, which decides to make a step.\nThe agent’s choice of a motor activity depends\non whether it wants to exploit its current knowledge\nabout the environment, or to explore it to improve\n5\n\nits knowledge. The exploration/exploitation dilemma\nis a well-known problem in online learning Watkins\n(1989); Sutton and Barto (2018), and changing envi-\nronments make it even more difficult. We therefore do\nnot try to reach an optimal solution here, but instead\nwe simply make the agent’s decision system choose\nat random, with equal probability, between an Explo-\nration and an Exploitation mode.\nThis choice being made, for each motor activity m\nout of the eight possible the decision system queries\nthe semantic memory for the outcome the action hav-\ning the current location for initial situation and m for\nmotor activity. It then rates each of them for its suit-\nability, by building the sets S (for “Suitable”), US\n(“Unsuitable”) and UD (“Undecided”):\n• S = {(m, c) | (OK, c) ∈Pm}\n• US = {(m, c) | (KO, c) ∈Pm or (Failure, c) ∈\nPm}\n• UD = {m | ∄c s.t. (m, c) ∈S ∪US}\nThe decision system then chooses a motor activ-\nity depending on the selected mode. In Exploration\nmode, the agent is willing to take risks and chooses\nan action with the most uncertain outcome possible:\nif UD ̸= ∅, it picks one from UD, otherwise it goes\nfor one with the least c in S ∪US. In Exploitation\nmode, by contrast, the agent just wants to land on an\nOK box and to avoid KO boxes and failure as much as\npossible. So, if S ̸= ∅, it chooses one with the great-\nest c. Otherwise, if UD ̸= ∅, it picks one from UD.\nIf both S and UD are empty, it chooses one with the\nleast c in US.\nThe decision system then transmits its decision to\nthe motor system to perform the selected motor ac-\ntivity. The I-neurons that encode its features are ac-\ntivated by proprioception and send an input to A-\nneurons’ second compartment.\nWe simulate the agent’s move by computing its\narrival location and its features. If the agent bumps\ninto a wall, the Failure neuron fires and sends an in-\nput to A-neurons’ third compartment, and the agent\ndirectly learns the action (see A-neurons’ learning\nabove). Otherwise, it first learns relevant object con-\ncepts about its new location (see O-neurons’ learning\nabove), and then the action. After that, the agent is\nready for the next step.\n7.\nResults\nTo test the agent’s learning abilities, we placed it at lo-\ncation (0, 0) and prompted it to perform a succession\nof series of steps, each complete sequence of series of\nsteps being called a trial and consisting in 65536 steps\nin total. The results we present here are averaged over\n50 trials.\nA first group of tests was carried out with the door\nkept closed all along, so the agent had no access to\nthe second room. First, we tested its ability to learn an\naction over one single experience (one-shot learning).\nTo do so, after each step we asked it to redo the predic-\ntion that led to the just realized action, and compared\nthis new prediction with the action’s actual outcome\n(seeTable 1). We call a prediction Correct and Com-\nplete (CC) if the predicted features are exactly those\nof the arrival location. The table’s first line shows\nthe mean percentage of steps leading to a CC post-\nlearning prediction, for each series of steps. The sec-\nond line (MF for “Missed Features”) shows the aver-\nage percentage of features occurrences that the agent\nfailed to predict after learning. The third line (PE for\n“Predictions Errors”) shows the average percentage\nof wrongly predicted features occurrences. These re-\nsults show a good performance at immediate recall\nafter learning.\nTable 1. Mean post-learning predictions percentages\nover 50 trials. CC: Correct and Complete, MF: Miss-\ning Features, PE: Prediction Errors.\nNb of\nSteps\n1\n2\n4\n8\n16\n32\n64\n128\n256\nCC\n100.0 100.0 100.0 98.5 98.0 97.0 96.2 93.4 88.9\nMF\n0.0\n0.0\n0.0\n0.6\n0.8\n1.2\n2.0\n3.0\n4.8\nPE\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.3\nNb of\nSteps\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\nCC\n91.4\n93.5\n95.1\n95.2 95.9 96.5 96.8\n96.7\nMF\n3.5\n2.6\n1.9\n1.9\n1.6\n1.4\n1.3\n1.3\nPE\n0.3\n0.2\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\nTo test whether the acquired knowledge was re-\ntained in the long run, after each series of steps we\nfroze the simulation, deactivated learning and placed\nthe agent successively in each location of each room.\nThere, we asked it for its predictions for each of\nthe eight possible motor activities and compared its\nanswers with the actions’ actual outcomes. Tables\n2 and 3 show each feature’s mean Hit Rate (that\nis, its chances of being predicted when effectively\npresent), and Correctness (its chances of being effec-\ntively present when predicted)1 for each room. For\nlack of space we only show the results for some se-\nries. Values for the first room (white lines) show that\nlearned actions are indeed recalled long after having\nbeen performed. Values for the second room (grey\nlines) show that despite never having been in this\nroom (since we kept the door closed) the agent is\nable to correctly predict OK and KO features and to a\nlesser extent Failure —and this, even though locations\n1Hit Rate is also known as True Positive Rate, Recall or Sen-\nsitivity, while Correctness is also known as Precision or Positive\nPredictive Value – see for example Kohavi and Provost (1998) for\ndefinitions. Here we multiplied the obtained figures by 100 to get\npercentages.\n6\n\nTable 2. Predictions’ Hit Rates after n steps, door\nclosed.\nNb of\nSteps\nRoom\nOK\nKO\nFail.\nWall\nCold\nSound\nBox\nName\n1\n1\n13.4\n11.9\n0.0\n0.0\n0.0\n0.0\n1.4\n2\n14.0\n11.6\n0.0\n0.0\n0.0\n0.0\n0.0\n8\n1\n32.6\n39.1\n28.6\n14.0\n9.1\n0.0\n4.1\n2\n28.3\n35.7\n28.2\n11.1\n0.0\n0.0\n0.0\n64\n1\n77.5\n77.7\n45.3\n35.0\n26.0\n0.0\n22.4\n2\n66.2\n69.3\n38.8\n22.8\n0.0\n0.0\n0.0\n512\n1\n94.3\n92.2\n70.5\n64.5\n57.2\n0.0\n50.9\n2\n84.5\n84.6\n40.2\n23.4\n0.0\n0.0\n0.0\n8192\n1\n97.3\n94.7\n83.0\n79.6\n72.5\n0.0\n72.6\n2\n91.7\n87.2\n52.0\n26.0\n0.0\n0.0\n0.0\n32768\n1\n96.5\n94.1\n85.7\n80.7\n81.5\n0.0\n75.0\n2\n89.5\n84.6\n52.2\n23.8\n0.0\n0.0\n0.0\nfrom the second room have different sets of features,\nincluding for some of them a new feature, Sound. The\npoor performance at wall prediction is due to the lack\nof general rules of the universe regarding the pres-\nence of walls in adjacent boxes: the agent uses partic-\nular concepts to predict them in the first room hence\nit is helpless in the second room. The mixed result for\nfailure prediction comes from a competition between\ngeneral action concepts, the control of which needs to\nbe improved.\nTable 3. Predictions’ Correctness after n steps, door\nclosed.\nNb of\nSteps\nRoom\nOK\nKO\nFail.\nWall\nCold\nSound\nBox\nName\n1\n1\n18.0\n17.5\n0.0\n0.0\n0.0\n0.0\n4.0\n2\n20.0\n18.4\n0.0\n0.0\n0.0\n0.0\n0.0\n8\n1\n45.2\n42.1\n26.6\n18.8\n5.1\n0.0\n5.2\n2\n43.4\n40.6\n20.4\n11.5\n0.0\n0.0\n0.0\n64\n1\n74.8\n74.5\n58.7\n34.0\n28.6\n0.0\n22.7\n2\n72.0\n72.8\n41.8\n18.9\n0.0\n0.0\n0.0\n512\n1\n90.5\n87.2\n83.9\n69.5\n72.8\n0.0\n68.6\n2\n83.4\n83.1\n47.4\n25.9\n0.0\n0.0\n0.0\n8192\n1\n93.0\n92.2\n94.4\n87.7\n95.2\n0.0\n84.3\n2\n85.5\n86.5\n66.2\n36.3\n0.0\n0.0\n0.0\n32768\n1\n94.2\n93.0\n93.4\n89.9\n94.0\n0.0\n87.1\n2\n85.3\n86.2\n60.7\n38.1\n0.0\n0.0\n0.0\nWe also tested the agent’s ability to use its knowl-\nedge to make appropriate decisions. Each time it\nchose, during the simulation, to exploit its knowledge,\nwe recorded the chosen action’s outcome. Figure 4.A\nshows the percentages of OK, KO and Failure out-\ncomes thus obtained in each series of steps. We kept\ntrack of the visited locations, in order to check that the\nagent was not looping indefinitely on the same boxes:\nin fact, all boxes kept being visited, be it very rarely,\nat any point of the trials, due to the Exploration mode.\nFinally, we tested whether the agent would be able\nto use the knowledge acquired in the first room to act\njudiciously in the second room. To do so, at the end\nof each series of steps we asked it to chose a move\nfor each of the second room’s type of locations (we\nsay that two locations are of the same type if they\nhave exactly the same features). Figure 4.B shows the\npercentages of OK, KO and Failure obtained in this\nmanner. These results reflect the agent’s performance\nat making predictions about the second room’s loca-\ntions: it successfully predicts OK and KO boxes, but\nhas more difficulties predicting failure.\nIn a second group of tests, we kept the same setup,\nbut opened the door at the 2048th step. The agent\nspontaneously went in the second room, and spent a\nvariable but significant amount of time in it (36.6% of\nsteps on average, standard deviation = 10.7).\nTables 4 and 5 show the features’s Hit Rates and\nCorrectness from the moment the door was opened.\nThese results show that the agent was able to learn\nnew concepts involving the Sound feature. The seem-\ningly low Hit Rates for the feature are explained by\nthe lack of observable cues in boxes at the direct south\nof boxes with sound, which prevents the agent from\nbeing able to predict it in 40% of the cases. These re-\nsults also show that changing the agent’s environment\ndoes not lead to significant loss of previously acquired\nknowledge. What happens is that the network accom-\nmodates the new concepts by recruiting neurons that\nhave not been used for a long time to encode them\n(remember the boosting of inputs sums in the learning\nprocesses of O- and A-neurons). As a result, the agent\ntends to forget the details of particular objects and ac-\ntions such as boxes’ names or the presence of Cold\nand walls (see the moderate drop of these features’\nHit Rates), and more generally to forget old and un-\nused concepts. But general concepts, which are used\non a regular basis, are preserved, at least as long as\nthey keep being used.\nAccordingly, the agent’s ability to use its knowl-\nedge to make appropriate decisions in the first room\nis not impacted by the door being opened. In fact, the\nbar chart of OK, KO and Failure outcomes obtained in\nthis second run of tests showed no visible difference\nwith the one obtained with the door closed and shown\nin Figure 4.A.\nAs regards computing time, it takes about 10 sec-\nFigure 4. Actions’ mean outcomes with door closed;\nGreen: OK, Red: KO, Blue: Failure, Grey: no data\n7\n\nTable 4. Predictions’ Hit Rates after n steps, door\nopened at 2048th step.\nNb of\nSteps\nRoom\nOK\nKO\nFail.\nWall\nCold\nSound\nBox\nName\n2048\n1\n95.8\n94.2\n78.5\n74.1\n69.2\n0.0\n64.7\n2\n89.3\n86.2\n45.7\n24.2\n0.0\n0.0\n0.0\n4096\n1\n97.9\n96.5\n75.6\n62.7\n56.6\n0.0\n53.1\n2\n93.7\n92.9\n57.4\n34.2\n0.0\n35.0\n5.1\n8192\n1\n96.8\n96.9\n81.1\n66.4\n58.9\n0.0\n55.6\n2\n93.7\n93.6\n64.0\n36.5\n0.0\n34.2\n4.2\n16384\n1\n96.8\n95.4\n82.5\n68.5\n67.4\n0.0\n60.0\n2\n92.4\n91.3\n66.0\n33.9\n0.0\n32.1\n5.3\n32768\n1\n96.9\n97.2\n85.6\n67.8\n59.1\n0.0\n57.0\n2\n93.2\n92.5\n75.5\n39.7\n0.0\n38.4\n7.6\n65536\n1\n96.9\n94.4\n88.8\n68.4\n59.2\n0.0\n60.5\n2\n93.6\n90.7\n80.5\n40.5\n0.0\n39.6\n7.1\nTable 5. Predictions’ Correctness after n steps – door\nopened at 2048th step.\nNb of\nSteps\nRoom\nOK\nKO\nFail.\nWall\nCold\nSound\nBox\nName\n2048\n1\n92.9\n89.5\n91.5\n82.7\n91.0\n0.0\n78.9\n2\n85.2\n83.7\n58.0\n36.6\n0.0\n0.0\n0.0\n4096\n1\n94.6\n90.7\n93.0\n75.1\n91.8\n0.0\n88.1\n2\n90.9\n87.8\n75.9\n44.7\n0.0\n64.0\n1.2\n8192\n1\n94.9\n93.2\n93.3\n77.7\n94.3\n0.0\n87.9\n2\n91.1\n89.2\n79.7\n44.8\n0.0\n61.5\n1.4\n16384\n1\n95.6\n93.0\n91.7\n81.5\n96.0\n0.0\n90.3\n2\n91.3\n89.2\n75.9\n47.3\n0.0\n66.2\n1.1\n32768\n1\n96.2\n94.5\n94.1\n78.3\n95.2\n0.0\n91.0\n2\n93.2\n91.5\n79.9\n50.8\n0.0\n64.0\n2.5\n65536\n1\n96.2\n95.9\n90.6\n82.1\n94.6\n0.0\n94.7\n2\n93.6\n92.8\n79.8\n51.7\n0.0\n65.4\n2.2\nonds on a conventional computer for the agent to run\n2048 steps while keeping track of all the test data. We\nmade no attempt to optimize the computing time, as\nit seems less critical in the case of online learning of\nautonomous agents which can learn while physically\nperforming their actions.\nFinally, we highlight that the choice of a localist\nrepresentation for the neural network makes it easy to\nread, since the concept encoded by each neuron can\nbe known simply by looking at its connections.\n8.\nConclusion and Future Developments\nIn this paper we have designed and implemented a\nfully autonomous agent that learns action laws online\nand accommodates environment changes. This agent\nrelies on general concepts to handle new situations\nand dynamically adjusts its concepts to its current en-\nvironment. This makes it well suited for open worlds:\nif a new door were to open to a third room with new\nobjects and laws, it would learn them just as it did for\nthe second room. Of course, this would come at the\ncost of the forgetting of its least used concepts, but\nthese are precisely the ones it needs the less. In fact,\nthe agent’s ability to selectively forget ensures that it\nwill always be able to learn about new environments,\nby replacing old unused concepts by new useful ones.\nFurther work remains to be done to endow the agent\nwith planning abilities. Notably, a notion of applica-\nble action law would be needed. Intuitively, it seems\nthat an action law represented by an action concept\n[x, y, z] should be deemed applicable in a situation\ns if s satisfies all the features in x and z ̸= Fail-\nure. Furthermore, to comply with open world require-\nments the agent would need to be able to build its own\nset of possible situations (states) online. The set of its\nobject concepts could probably be used to this end. A\ncost function should also be added, and the decision\nsystem should be augmented so as to handle goals.\nAdditionally, a number of other improvements\nwould be desirable in order to allow the agent to live\nin more realistic environments. A first one would be to\nimplement negation in the network, so that the agent\nwould be able to represent the fact that a given ob-\nject does not have a given feature. We believe that this\ncould be done by the means of neural inhibition, but\nthe appropriate learning rules remain to be found. An-\nother essential improvement would be to have the net-\nwork use incomplete information as input for learn-\ning and querying, and to make the agent able to query\nits semantic memory for object properties given some\npartial input. It seems to us that this would bring the\nagent to draw non-monotonic inferences in the spirit\nof Grimaud (2016). It would also be useful to allow\nthe agent to distinguish between objects and their lo-\ncations, since actions can modify one, the other or\nboth. Biological brains achieve this by using two sep-\narate pathways to process the “what” and the “where”\ncomponents of observations before reunifying them,\nand this could be an inspiration source. Lastly, a com-\npletely different line of research would be to inves-\ntigate how the agent should decide between Explo-\nration and Exploitation modes in an open world.\nReferences\nBausch, M.; Niediek, J.; Reber, T. P.; Mackay, S.;\nBostr¨om, J.; Elger, C. E.; and Mormann, F. 2021.\nConcept neurons in the human medial temporal\nlobe flexibly represent abstract relations between\nconcepts. Nature communications, 12(1): 6164.\nBolander, T.; and Gierasimczuk, N. 2018. Learning\nto act: qualitative learning of deterministic action\nmodels. Journal of Logic and Computation, 28(2):\n337–365.\nBonet, B.; Frances, G.; and Geffner, H. 2019. Learn-\ning features and abstract actions for computing\ngeneralized plans. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 33, 2703–\n2710.\n8\n\nCui, Y.; Ahmad, S.; and Hawkins, J. 2016.\nCon-\ntinuous online sequence learning with an unsuper-\nvised neural network model. Neural computation,\n28(11): 2474–2504.\nDas,\nD.;\nChernova,\nS.;\nand\nKim,\nB.\n2023.\nState2explanation: Concept-based explanations to\nbenefit agent learning and user understanding. Ad-\nvances in Neural Information Processing Systems,\n36: 67156–67182.\nFarebrother, J.; Machado, M. C.; and Bowling, M.\n2018. Generalization and regularization in DQN.\narXiv preprint arXiv:1810.00123.\nFreund, M. 2008. On the notion of concept I. Artifi-\ncial Intelligence, 152(1): 105–137.\nGhorbani, A.; Wexler, J.; Zou, J. Y.; and Kim, B.\n2019. Towards automatic concept-based explana-\ntions. Advances in neural information processing\nsystems, 32.\nGrimaud, C. 2016.\nModelling reasoning processes\nin natural agents: a partial-worlds-based logical\nframework for elemental non-monotonic infer-\nences and learning.\nJournal of Applied Non-\nClassical Logics, 26(4): 251–285.\nGupta, A.; and Narayanan, P. 2024.\nA survey on\nConcept-based Approaches For Model Improve-\nment. arXiv preprint arXiv:2403.14566.\nHase, P.; Chen, C.; Li, O.; and Rudin, C. 2019. Inter-\npretable image recognition with hierarchical pro-\ntotypes. In Proceedings of the AAAI Conference\non Human Computation and Crowdsourcing, vol-\nume 7, 32–40.\nHawkins, J.; and Subutai, A. 2016.\nWhy Neurons\nHave Thousands of Synapses, a Theory of Se-\nquence Memory in Neocortex. Frontiers in Neural\nCicuits, 10.\nKheradpisheh, S. R.; Ganjtabesh, M.; Thorpe, S. J.;\nand Masquelier, T. 2017. STDP-based spiking deep\nconvolutional neural networks for object recogni-\ntion. Neural Networks, 99: 56–67.\nKirk,\nR.;\nZhang,\nA.;\nGrefenstette,\nE.;\nand\nRockt¨aschel, T. 2023.\nA survey of zero-shot\ngeneralisation in deep reinforcement learning.\nJournal of Artificial Intelligence Research, 76:\n201–264.\nKirkpatrick, J.; Pascanu, R.; Rabinowitz, N.; Veness,\nJ.; Desjardins, G.; Rusu, A. A.; Milan, K.; Quan, J.;\nRamalho, T.; Grabska-Barwinska, A.; et al. 2017.\nOvercoming catastrophic forgetting in neural net-\nworks. Proceedings of the national academy of sci-\nences, 114(13): 3521–3526.\nKoh, P. W.; Nguyen, T.; Tang, Y. S.; Mussmann, S.;\nPierson, E.; Kim, B.; and Liang, P. 2020. Concept\nbottleneck models. In International conference on\nmachine learning, 5338–5348. PMLR.\nKohavi, R.; and Provost, F. 1998. Glossary of Terms.\nLesort, T.; Lomonaco, V.; Stoian, A.; Maltoni, D.; Fil-\nliat, D.; and D´ıaz-Rodr´ıguez, N. 2020. Continual\nlearning for robotics: Definition, framework, learn-\ning strategies, opportunities and challenges. Infor-\nmation fusion, 58: 52–68.\nMoerland, T. M.; Broekens, J.; Plaat, A.; Jonker,\nC. M.; et al. 2023.\nModel-based reinforcement\nlearning: A survey. Foundations and Trends® in\nMachine Learning, 16(1): 1–118.\nQuiroga, R. Q. 2012.\nConcept cells: the building\nblocks of declarative memory functions.\nNature\nReviews Neuroscience, 13: 587–597.\nShimamura, A. P. 2010. Hierarchical relational bind-\ning in the medial temporal lobe: the strong get\nstronger. Hippocampus, 20(11): 1206–1216.\nSutton, R. S.; and Barto, A. G. 2018. Reinforcement\nlearning: An introduction. MIT press.\nThiele, J.; Bichler, O.; and Dupret, A. 2018. Event-\nbased, timescale invariant unsupervised online\ndeep learning with STDP. Front. Comput. Neu-\nrosci. 12, 46 (2018).\nThorpe, S. 2023. Timing, Spikes, and the Brain. In\nTime and Science: Volume 2: Life Sciences, 207–\n236. World Scientific Publishing Europe Ldt.\nThorpe, S.; Masquelier, T.; Martin, J.; Yousefzadeh,\nA. R.; and Linares-Barranco, B. 2019.\nMethod,\ndigital electronic circuit and system for unsuper-\nvised detection of repeating patterns in a series of\nevents. Patent US20190286944A1.\nWang, A.; Lee, W.-N.; and Qi, X. 2022. Hint: Hier-\narchical neuron concept explainer. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 10254–10264.\nWang, L.; Zhang, X.; Su, H.; and Zhu, J. 2024. A\ncomprehensive survey of continual learning: the-\nory, method and application.\nIEEE Transactions\non Pattern Analysis and Machine Intelligence.\nWang, Y.; Yao, Q.; Kwok, J. T.; and Ni, L. M. 2020.\nGeneralizing from a few examples: A survey on\nfew-shot learning. ACM computing surveys (csur),\n53(3): 1–34.\nWatkins, C. J. C. H. 1989. Learning from delayed\nrewards.\n9\n\nZabounidis, R.; Campbell, J.; Stepputtis, S.; Hughes,\nD.; and Sycara, K. P. 2023.\nConcept learning\nfor interpretable multi-agent reinforcement learn-\ning. In Conference on Robot Learning, 1828–1837.\nPMLR.\n10",
    "pdf_filename": "SNN-Based_Online_Learning_of_Concepts_and_Action_Laws_in_an_Open_World.pdf"
}