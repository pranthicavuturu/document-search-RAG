{
    "title": "SNN-Based Online Learning of Concepts and Action Laws",
    "abstract": "We present the architecture of a fully autonomous, bio-inspired cognitive agent built around a spiking neural network (SNN) implementing the agent’s semantic memory. The agent explores its universe and learns concepts ofobjects/situationsandofitsownactionsinaone-shotmanner.Whileobject/situationconceptsareunary,action concepts are triples made up of an initial situation, a motor activity, and an outcome. They embody the agent’s knowledge of its universe’s actions laws. Both kinds of concepts have different degrees of generality. To make decisionstheagentqueriesitssemanticmemoryfortheexpectedoutcomesofenvisagedactionsandchoosesthe action to take on the basis of these predictions. Our experiments show that the agent handles new situations by appealingtopreviouslylearnedgeneralconceptsandrapidlymodifiesitsconceptstoadapttoenvironmentchanges.",
    "body": "SNN-Based Online Learning of Concepts and Action Laws\nin an Open World\nChristelGrimaud,DominiqueLongin,AndreasHerzig\nUniversite´ deToulouse,CNRS,ToulouseINP,UT3,IRIT,France\nAbstract\nWe present the architecture of a fully autonomous, bio-inspired cognitive agent built around a spiking neural\nnetwork (SNN) implementing the agent’s semantic memory. The agent explores its universe and learns concepts\nofobjects/situationsandofitsownactionsinaone-shotmanner.Whileobject/situationconceptsareunary,action\nconcepts are triples made up of an initial situation, a motor activity, and an outcome. They embody the agent’s\nknowledge of its universe’s actions laws. Both kinds of concepts have different degrees of generality. To make\ndecisionstheagentqueriesitssemanticmemoryfortheexpectedoutcomesofenvisagedactionsandchoosesthe\naction to take on the basis of these predictions. Our experiments show that the agent handles new situations by\nappealingtopreviouslylearnedgeneralconceptsandrapidlymodifiesitsconceptstoadapttoenvironmentchanges.\n1. Introduction To test this idea, we here build an artificial hybrid\nagentwithaSNNatitscore.Wemakethisagentlive\nin a very simple virtual world, composed of rooms\nThe ability of a cognitive agent to act adequately in\nwhich may be, or not, accessible (hence, knowable)\nagivenenvironmentdependsonitsabilitytopredict\ntoit.Atfirst,theagentisconfinedtoonesingleroom\nhow performing a given activity might affect its cur-\nand learns by itself how to act in it according to its rentsituation,thatis,itdependsonitsknowledgeof\nown interests. Then at some point a door opens to a\nits environment’s action laws. How artificial agents\nnewroom,containingsomeneverencounteredbefore\ncan acquire these laws and how these should be up-\nobjectsandsituations.Yet,althoughthesearenewto\ndated if their environment changes has proven a dif-\ntheagent,somegenerallawsarepreservedfromone\nficult question. In the case where the intended envi-\nroomtotheother.Weshowthathavinglearnedthese\nronmentisopen—thatis,wheretheagent’sdesigner\nlaws in the first room allows the agent to act by and\ncannot foresee all the situations the agent might en-\nlargeproperlyinthesecondone,assoonasitenters\ncounterinthefuture—,providingasuitablesetofac-\nit.Wealsoshowthatrelyingonneurallyimplemented\ntion laws to the agent “by hand” is unfeasible. The\nconceptsallowstheagenttorapidlyupdateitsknowl-\nonly viable solution is that the agent continuously\nedgeandadapttoenvironmentchanges.\nlearnstherelevantlawsfromexperience,justasnat-\nThepaperisorganizedasfollows.InSection2we\nural agents (humans and animals) do. Crucially, this\ndiscussrelatedwork,andinSection3wepresentthe\nlearningprocessshouldallowforgeneralizationover\nagentanditsuniverse.Section4clarifiesthenotions\ndisparateexperiences,sothattheagentisabletobe-\nofconceptsandactionslawsweuse,whileSection5\nhave appropriately in new situations. It should also\ndescribestheneuralnetworkanditsfunctioning.Sec-\naccommodateenvironmentchanges.\ntion 6 describes the agent’s general functioning and\nThe present paper intends to show how this could\nSection7presentstheresults.Finally,Section8con-\nbe done. Its main thrust is that natural agents’ abil-\ncludesanddiscussesfuturepossibledevelopmentsof\nity to perform well in our open and changing world\ntheframework.\nrelies on the fact that they store their knowledge in\nthe form of concepts, which can have various de-\ngrees of generality. Presumably, they first form con- 2. RelatedWorks\ncepts about the encountered objects and situations,\nand then use these as building blocks for relational Ourresearchproblemisautonomousonlinelearning,\nconcepts, among which are concepts of actions sup- generalization and updating of concepts and actions\nportingtheirknowledgeoftheirenvironment’saction laws in an open universe. The intended application\nlaws. We suggest that artificial agents could do just isreasoningandplanningforautonomousrobots.To\nthesame,relyingonsomeartificialneuralnetworkto our knowledge, no existing approach addresses this\nlearnandstoreconcepts,andthenqueryingittomake problem in all its dimensions, even though these are\npredictionsabouttheoutcomeofenvisagedactions. investigatedinseparateresearchfields.\n1\n4202\nvoN\n91\n]IA.sc[\n1v80321.1142:viXra\nContinual Learning tackles the problem of life- Despite their undeniable successes, RL approaches\nlong knowledge acquisition Wang et al. (2024); struggle to adapt to environment changes and to re-\nLesort et al. (2020). Its main challenge is to avoid vise learned policies Kirk et al. (2023); Farebrother,\ncatastrophiclossofpreviousknowledgewhenacquir- Machado, and Bowling (2018). We believe that an\ningnewknowledge;asecondaryresearchaxisisone- agent that would be able to learn online a model of\nshot/few-shotslearning,i.e.,theabilitytolearnonline the environment and to dynamically use it to make\nfromone orfew examplesWanget al.(2020). How- decisions would be able to quickly adapt its behav-\never, current approaches mostly consider the learn- ior.Second,thelearningprocessweproposedoesnot\ning of tasks (mostly image classification/recognition depend on the existence of rewards, which makes it\ntasks, but also some more complex tasks such as suitableforcontextswhererewardsarescarce.\nplayinggamesKirkpatricketal.(2017)),notofcon-\ncepts nor action laws. Furthermore, most of them\n3. TheAgentanditsUniverse\nrely on supervised learning and/or labelled training\ndata,whichisunsuitableforopenworldautonomous The agent’s universe is built over a grid of boxes,\nagents. which we (not the agent) identify using an orthonor-\nConcept Learning has mainly been studied in malcoordinatesystem(seeFigure1).\nview of explainability Gupta and Narayanan (2024),\nmostly of classification models (e.g., Koh et al.\n(2020)) but also of decision making in the context\nof reinforcement learning Das, Chernova, and Kim\n(2023); Zabounidis et al. (2023). For this reason,\nmany proposals are dedicated to learning a human-\npredefinedsetofconceptsusingsomeannotateddata.\nIn the field of Image Classification some approaches\ndealwiththeextractionofconceptsfromdataWang,\nLee,andQi(2022);Ghorbanietal.(2019);Haseetal.\n(2019),butintheseapproachesconceptsareextracted\nfrom labelled classes of images, which is, again, un-\nFigure1.Theagent’saccessibleworld.A:inthefirst\nsuitable for open world autonomous agents. Further-\nphase,Room1only;B:afteropeningthedoor,Rooms\nmore, the vast majority of proposed methods disre-\n1and2.\ngard the hierarchical organization of concepts from\nparticulartomoregeneral,andtheygenerallydonot\nEach box represents a particular location in the\naddressone-shotlearning,onlinerevisionorupdating\nagent’suniverseandpossessesaparticularsetoffea-\nofconcepts.\ntures the agent is able to perceive, drawn from the\nAction Learning has been studied from various\nsetLF ={OK,KO,NorthWall,EastWall,SouthWall,\nperspectives.InDynamicEpistemicLogic,Bolander\nWestWall, Cold, Sound, #0, #1, ..., #24}. Although\nandGierasimczuk(2018)proposedamethodtolearn\nthe agent’s universe is not finite, at any time point\nan action model through successive observations of\nwe only consider the boxes to which it has access,\ntransitionsbetweenstates.Howeverthismethoddoes\nthe set of which is always finite. For example, the\nnotachievegeneralizationnoraccommodateenviron-\nbox with coordinates (−2,−2) has the feature set\nment changes and only considers the universally ap-\nLF = {OK,SouthWall,WestWall,Cold,#0},\nplicable actions (i.e., actions that can be executed (−2,−2)\nwhile the box with coordinates (5,0) has the feature\nin every logically possible state), a condition real-\nset LF = {OK} (“#n” is to be taken as a par-\nworld actions rarely satisfy. In the field of Planning, (5,0)\nticular name for a box hence a feature, not all boxes\nBonet, Frances, and Geffner (2019) showed how to\nneed to have one). Two boxes with the same fea-\nlearnabstractactionsfromafewcarefullychosenin-\nture set are indistinguishable for the agent. Boxes’\nstances of some general planning problem. However\nfeature sets may change over time, reflecting envi-\nsaidinstancescomewiththeirownsetofgroundac-\nronment changes. Therooms are made out of boxes,\ntionswhichmustbeknownbeforehand,hencethisap-\nanddelimitedwithimpassablewalls.Openingadoor\nproachcannotbeusedinopenworlds,whereanagent\namountstoremovingthewallfeaturesfromthecon-\nneedstoincrementallylearnfromsequentialobserva-\ncernedboxes’featuresets(asinFigure1.B).\ntions.\nReinforcement Learning (RL): Our work could The agent is composed of a set of sensors, a per-\nbe related to model-based approaches of RL Moer- ceptual system, a semantic memory, a decision sys-\nlandetal.(2023),butdiffersfromthemintwoimpor- tem, a motor system and a set of actuators (see Fig-\ntant aspects. First, we are only interested in learning ure 2). Sensors collect data from the external world\namodeloftheenvironment,notinlearningpolicies. andfeedittotheperceptualsystem,whichperforms\n2\n4. ConceptsandActionLaws\nTheagentisabletoformtwokindsofconcepts.First,\nconcepts of “things”, in the broad sense. These bind\ntogether co-occurrent features, and can be seen as\nsomesortofconjunctioninwhichconjunctshavedif-\nFigure2.Schemaoftheagent ferent“weights”,reflectingthefactthatsomefeatures\nare more important than others in a concept’s defi-\nnition Freund (2008). They are used to store knowl-\nfeature/object recognition. Since we want the agent\nedge about locations and more generally any object,\ntolearnbyitself,weneedthisprocesstorelyonun-\nso we call them object concepts. The second kind\nsupervisedlearningwithunlabelleddata.Neuralnet-\nis relational concepts. These take other concepts as\nworks doing this already exist Thiele, Bichler, and\nelements, and bind them together into tuples. Con-\nDupret(2018);Kheradpishehetal.(2017),sowesim-\ncepts of actions are of this kind: they bind together\nply suppose that the agent’s perceptual system oper-\ntheagent’sconceptsofadepartlocation,aperformed\nates as intended and provides the semantic memory\nmotoractivity,andasubsequentoutcome,intheorder\nwith the appropriate inputs, namely, the features of\ninwhichtheywereexperienced.\ntheagent’scurrentlocation.Furthermore,wesuppose\nthat the agent’s observations are always correct and\nWesaythatanobjectconceptXisgeneral,asop-\ncomplete.\nposedtoparticular,ifthereisanotherconceptYsuch\nSemantic memory forms concepts by binding to- thatthesetoffeaturescomposingXisastrictsubset\ngethertheperceivedfeatures,andstoresthemforfur- of the set of features composing Y. Y is then said to\nther retrieval. Its modeling is the main focus of this bemoreparticularthanX.Wesaythatanactioncon-\npaper.Thedecisionsystemistheotherimportantpart: ceptisgeneraliftheobjectconceptofitsinitialsitu-\nitqueriesthesemanticmemorytopredicttheoutcome ationisgeneraloritsmotoractivitycomponentonly\nofpossibleactions,anddecideswhichonetotakeon containsDiag.orOrth..Weunderstandthegenerality\nthe basis of these predictions. This decision is then of concepts relative to the set of concepts the agent\nsent to the motor system, which activates the actua- possesses at some point, so no concept is general or\ntors to perform the corresponding motor activity. In- particularinitself.\nformationfromtheactuatorsissentbacktosemantic\nmemory through proprioception, allowing the agent Forexample,whenvisitingthebox(0,0)theagent\ntomemorizethemotor-relatedfeaturesoftherealized may form the particular object concept [OK,#12],\nactions. which is a memory of an OK place with name\nThe agent’s possible actions consist in steps from #12, and only applies to this particular box in its\none box to another adjacent box, in any of the eight accessible universe. If it then moves North-East\ndirections.Formally,anactionisatriplecomposedof and arrives at box (1,1), it can form the particular\na depart location, a motor activity, and an outcome. object concept [OK,#18], and also the particular\nBy“motoractivity”wemeanthefactthattheagent’s action concept [[OK,#12],[NE,Diag],[OK,#18]]\nactuatorsareactivatedsoastomakeitmovetotheim- whichcorrespondstothememoryofbeinginanOK\nmediatenextboxintheselecteddirection.Thesetof place with name #12 and then moving North-East\nmotoractivities’featurestheagentisabletoperceive to arrive at another OK place with name #18. Yet,\nbyproprioceptionisthesetMF ={N,NE,E,SE,S, aftervisitinganumberoflocationshavingthefeature\nSW, W, NW, Diag., Orth.}, where the first eight are OK incommon,theagentmayalsoformthegeneral\nspecific to each particular direction, while Diag. and objectconcept[OK].Furthermore,itisageneralrule\nOrth. are common features shared by all motor ac- in its accessible universe that moving North-East\ntivities yielding diagonal/orthogonal moves. In cases from an OK location always leads to another OK\nwherethereisawallattheedgeofthedepartboxin location,exceptforwhenthereisawallattheNorth\ntheselecteddirection,theagentbumpsintoitandre- orEastedgeofthedepartbox.Therefore,afterhaving\nmainsatthesameplace.Wethensaythattheaction’s experienced a number of North-East moves from\noutcomeisafailure.Otherwise,theaction’soutcome various OK locations, the agent may form general\nistheagent’snewlocation. action concepts such as [[OK],[NE,Diag],[OK]]\nAsforthelocations’features,wesupposethatKO and [[OK,NorthWall][NE,Diag],[Failure]].\ncorresponds to some unpleasant stimulus the agent Such general concepts capture the general (non-\nspontaneouslywantstoavoid,andOK totheabsence monotonic) action laws of the agent’s universe,\nofsuchastimulus,whiletheothersconveysomein- and are the ones it shall rely on to behave in never\ndifferentinformation. encounteredsituations.\n3\n5. Implementing the Agent’s Semantic\nMemoryintheNeuralNetwork\nSpikingNeuralNetworks(SNNs)arewellsuitedfor\nautonomouslearninginopenuniverses,astheyallow\nforSpikeTimeDependentPlasticity(STDP),afam-\nilyofbiologicallyplausiblelearningruleswhichcan\nachieveunsupervisedonlinelearningfromunlabelled\ndataThiele,Bichler,andDupret(2018).Theyarealso\nknownforbeingenergy-efficient,whichisinteresting\nforautonomousrobots.\nWe take inspiration in the JAST learning rule\nThorpeetal.(2019);Thorpe(2023),whichisasim-\nplified version of STDP where the sum of the affer-\nentconnectionsweightsonanygivenneuronremains\nconstantthroughlearning.However,contrarytoJAST\nwe do not use binary weights but natural numbers.\nMoreover,wedonotfreezeneuronsafterlearning,so\nastoallowupdating.\nTheNetwork’sArchitecture\nThenetworkiscomposedofaninterface,whichcom-\nmunicates with the agent’s other components, and a Figure3.SchemaoftheSNN.O-neuron#1supports\nbody of hidden neurons which is itself divided into theconcept[#0,Cold,SouthWall,WestWall,OK],\ntwolayers(seeFigure3). and O-neuron #2 the concept [#5,SouthWall,KO].\nA-neuron #1 supports the concept [[#0,Cold,\nThe first layer learns object concepts and the sec-\nSouthWall,WestWall,OK],[E,Orth],[#5,South-\nond learns action concepts. For this reason we call\ntheir neurons, respectively, object concept neurons Wall,KO]], and A-neuron #3 the concept [[OK],\n(O-neuronsforshort)andactionconceptneurons(A-\n[Diag],[OK]].\nneurons). This architecture draws on neuroanatomi-\ncal studies according to which concepts are repre-\nsented in the brain by hierarchically organized con- sourceisI-neuronsorA-neurons.O-neuronslearnco-\ncept neurons, each receiving information from some occurrencesofperceivedfeatures.\nlower neurons and sending reciprocal connections to The second layer is composed of 400 compart-\nthese same neurons so that it can reactivate them for mentneuronswiththreeseparateinputcompartments.\ninformation retrieval Quiroga (2012); Bausch et al. The first compartment receives connections from O-\n(2021);Shimamura(2010).Forsimplicitywedonot neurons, the second one from motor activities I-\nmodel these reciprocal connections as such, but in- neurons, and the third one from O-neurons and Fail-\nsteadweallowforinformationtoflowinbothdirec- ure neuron. Inputs received at each compartment are\ntionsalongthesameconnections:frominterfaceneu- unable to trigger a spike by themselves, but the first\nronstoO-neuronsandthentoA-neuronsforlearning and second compartments make their next compart-\nandquerying,andtheotherwayroundforretrieving mentreadytoreceiveandtransmitinputsforacertain\ninformation.Akeypointisthatinterfaceneuronsare amount of time. In this manner, inputs can only be\nbothinputandoutputneurons,dependingonthecom- efficientiftheyoccurinthecorrectorder,sothatA-\nputationalphase. neuronsencodesequencesofinputs.Theuseofcom-\nInterfaceneurons(I-neuronsforshort)mainlysup- partmentneuronstolearnsequenceswassuggestedin\nporttherepresentationoffeatures,beitofthevisited Cui,Ahmad,andHawkins(2016);HawkinsandSub-\nlocations or of the agent’s own motor activities. An utai(2016).\nadditional neuron acts as a failure detector, specifi-\ncallyfiringwhentheagentbumpsintoawallandre-\nmainsatthesameplace.Allofthemhavetheirlabels TheNetwork’sfunctionning\nfixedfromthestart.\nThe first layer of hidden neurons is composed We briefly describe the network’s functioning. A\nof 100 integrate and fire neurons, with a differen- more detailed account is provided in Supplementary\ntiated dynamics depending on whether their input Material.\n4\nO-neurons’Learning threshold at the moment a given I-neuron spikes de-\ntermines the agent’s confidence in the feature’s pre-\nEach time the agent observes its current location,\ndiction: the higher its value, the higher the confi-\ninformation from the perceptual system is sent to\ndence. So, formally, the querying process returns a\nthe network’s interface, inducing the firing of the I-\nset P = {(f ,c ),...(f ,c )}, where f is a fea-\nm 1 1 n n i\nneurons that encode the location’s features. This in\nture and c the degree of confidence the agent has in\ni\nturn triggers the firing of a number of O-neurons. If\nitsprediction.\nthis number reaches some fixed target number, the\nnetworkdirectlyproceedstomakethemlearn.Other-\nA-neurons’Learning\nwiseitlooksforadditionalO-neuronsby“boosting”\ntheirinput.Inpractice,boostingconsistsinmultiply- Ateachstep,O-neuronsrespondingtothedepartlo-\ningtheinputreceivedbyeachnon-firingO-neurono cationandI-neuronsrespondingtotheperformedmo-\nbyafactorb o,whichisanincreasingfunctionofthe tor activity send inputs to, respectively, A-neurons’\nnumber of steps performed since o’s last spike. This first and second compartments. A-neurons reaching\nprocedure favors the firing of O-neurons that have a certain threshold are selected for learning. If their\nbeeninactiveforalongtime,whicharethenre-used number reaches some fixed target number, the net-\nforlearning. workdirectlyproceedstomakethemlearn,otherwise\nThe learning process depends on the accuracy of itlooksforadditionalA-neuronsbyboostingtheirin-\nthe agent’s knowledge about its current location. To putinawaysimilartotheoneusedforO-neurons.\nassess it, the (pre-boosting) firing O-neurons send a ForeachlearningA-neuronawecomputealearn-\nbackward input to I-neurons, and the resulting set of ing rate LR , which is an increasing function of the\na\nfiring I-neurons (the retrieved information) is com- number of steps performed since the neuron’s last\npared with the initial input (the current observation). learning.Theideaisthatseldomusedneuronstendto\nIfalltheobservedfeaturescanberetrievedfromac- encodemoreparticularconceptsthanoftenusedneu-\ntiveO-neurons,thenallinactiveI-synapses(ifany)on rons,andshouldthusbeabletolearnmorerapidlyto\nthelearningO-neuronsaredeletedandreplacedwith retainmorefeaturesfromagivensituation.\nsynapsesfrominputneurons.Thisproceduretendsto The learning process depends on the accuracy of\nreinforce O-neurons’ connections with I-neurons en- the agent’s predictions relative to the action’s out-\ncodingwellsharedfeaturesattheexpenseofconnec- come (see Section 6). If these are correct (i.e., all\ntionswithI-neuronsencodingmorespecificfeatures, the expected features are actually present) and com-\nfostering the learning of general concepts. If, on the plete (i.e., all the actually present features were ex-\ncontrary,notalltheobservedfeaturescanberetrieved pected), then the learning process replaces all in-\nfromthefiringO-neurons,thenwepickonelearning active synapses in a’s first two compartments with\nO-neuronwithmaximalnumberofstepssinceitslast synapsesfromtheirrespectiveinputneurons,butonly\nlearning and replace all its synapses, active or inac- min(LR ,i)inthethirdone,whereiisthenumberof\na\ntive, with synapses from input neurons. This neuron inactivesynapses.Iftheyarenotcorrect,thenallinac-\nthuslearnstheparticularsituationwithallitsfeatures. tivesynapsesplussomeactiveonesarereplacedina’s\nThelearningprocessissimilartothefirstcaseforthe first two compartments, with max(LR ,i) the num-\na\notherlearningneurons. ber of synapses replaced in each one. The learning\nprocessforthethirdcompartmentisthesameasinthe\nfirstcase.Iftheagent’spredictionsarenotcomplete,\nQueryingthenetwork\nthenthenumberofreplacedsynapsesismax(LR ,i)\na\nin all three compartments. This differentiated learn-\nTo query the neural network, the decision system\ning process aims at promoting generalization when\nfirst sends an input to the I-neurons that encode the\npredictions are correct and complete while allowing\nfeatures of the initial situation. Their firing brings a\nafewneuronstospecializewhentheyarenot.\nnumber of O-neurons to fire, sending an input to A-\nneurons’firstcompartment.Then,givenanenvisaged\nmotor activity m, the decision system sends an in- 6. FunctioningoftheAgent\nput to the I-neurons that encode m’s features. These\nfire,andsendaninputtoA-neurons’secondcompart- Supposetheagentisatsomedepartlocationandob-\nment.A-neurons’spikingthresholdisthengradually servesit:informationfromitsperceptualsystemtrig-\nlowereduntilatargetnumberofthemfire,sendinga gersthefiringoftheI-neuronsencodingthelocation’s\nbackwardinputtoO-neuronsthroughtheirthirdcom- featuresandistransmittedfromtheretothedecision\npartment’sconnections.TheO-neuronsthatfireinre- system,whichdecidestomakeastep.\nsponse to that input in turn send a backward input The agent’s choice of a motor activity depends\nto I-neurons, the firing of which is the network’s re- on whether it wants to exploit its current knowledge\nsponsetothequery.ThevalueofA-neurons’spiking about the environment, or to explore it to improve\n5\nitsknowledge.Theexploration/exploitationdilemma thesecondroom.First,wetesteditsabilitytolearnan\nis a well-known problem in online learning Watkins actionoveronesingleexperience(one-shotlearning).\n(1989);SuttonandBarto(2018),andchangingenvi- Todoso,aftereachstepweaskedittoredothepredic-\nronmentsmakeitevenmoredifficult.Wethereforedo tionthatledtothejustrealizedaction,andcompared\nnot try to reach an optimal solution here, but instead this new prediction with the action’s actual outcome\nwe simply make the agent’s decision system choose (seeTable1).WecallapredictionCorrectandCom-\natrandom,withequalprobability,betweenanExplo- plete (CC) if the predicted features are exactly those\nrationandanExploitationmode. of the arrival location. The table’s first line shows\nThischoicebeingmade,foreachmotoractivitym the mean percentage of steps leading to a CC post-\nout of the eight possible the decision system queries learningprediction,foreachseriesofsteps.Thesec-\nthesemanticmemoryfortheoutcometheactionhav- ondline(MFfor“MissedFeatures”)showstheaver-\ningthecurrentlocationforinitialsituationandmfor agepercentageoffeaturesoccurrencesthattheagent\nmotor activity.It thenrates eachof them forits suit- failedtopredictafterlearning.Thethirdline(PEfor\nability, by building the sets S (for “Suitable”), US “Predictions Errors”) shows the average percentage\n(“Unsuitable”)andUD (“Undecided”): ofwronglypredictedfeaturesoccurrences.Thesere-\n• S ={(m,c)|(OK,c)∈P } sults show a good performance at immediate recall\nm\n• US = {(m,c) | (KO,c) ∈ P or(Failure,c) ∈ afterlearning.\nm\nP }\nm\n• UD ={m|∄cs.t.(m,c)∈S ∪US} Table 1. Mean post-learning predictions percentages\nThe decision system then chooses a motor activ- over50trials.CC:CorrectandComplete,MF:Miss-\nity depending on the selected mode. In Exploration ingFeatures,PE:PredictionErrors.\nmode, the agent is willing to take risks and chooses\nan action with the most uncertain outcome possible:\nifUD ̸= ∅,itpicksonefromUD,otherwiseitgoes\nfor one with the least c in S ∪ US. In Exploitation\nCC 100.0 100.0 100.0 98.5 98.0 97.0 96.2 93.4 88.9\nmode,bycontrast,theagentjustwantstolandonan MF 0.0 0.0 0.0 0.6 0.8 1.2 2.0 3.0 4.8\nOKboxandtoavoidKOboxesandfailureasmuchas PE 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3\npossible.So,ifS ̸= ∅,itchoosesonewiththegreat-\nest c. Otherwise, if UD ̸= ∅, it picks one from UD.\nIfbothS andUD areempty,itchoosesonewiththe\nCC 91.4 93.5 95.1 95.2 95.9 96.5 96.8 96.7\nleastcinUS. MF 3.5 2.6 1.9 1.9 1.6 1.4 1.3 1.3\nThe decision system then transmits its decision to PE 0.3 0.2 0.1 0.1 0.1 0.1 0.1 0.1\nthe motor system to perform the selected motor ac-\ntivity. The I-neurons that encode its features are ac- To test whether the acquired knowledge was re-\ntivated by proprioception and send an input to A- tained in the long run, after each series of steps we\nneurons’secondcompartment. frozethesimulation,deactivatedlearningandplaced\nWe simulate the agent’s move by computing its theagentsuccessivelyineachlocationofeachroom.\narrival location and its features. If the agent bumps There, we asked it for its predictions for each of\ninto a wall, the Failure neuron fires and sends an in- the eight possible motor activities and compared its\nput to A-neurons’ third compartment, and the agent answers with the actions’ actual outcomes. Tables\ndirectly learns the action (see A-neurons’ learning 2 and 3 show each feature’s mean Hit Rate (that\nabove). Otherwise, it first learns relevant object con- is, its chances of being predicted when effectively\nceptsaboutitsnewlocation(seeO-neurons’learning present),andCorrectness(itschancesofbeingeffec-\nabove), and then the action. After that, the agent is tively present when predicted)1 for each room. For\nreadyforthenextstep. lack of space we only show the results for some se-\nries.Valuesforthefirstroom(whitelines)showthat\nlearned actions are indeed recalled long after having\n7. Results\nbeen performed. Values for the second room (grey\nlines) show that despite never having been in this\nTotesttheagent’slearningabilities,weplaceditatlo-\nroom (since we kept the door closed) the agent is\ncation(0,0)andpromptedittoperformasuccession\nabletocorrectlypredictOK andKOfeaturesandtoa\nofseriesofsteps,eachcompletesequenceofseriesof\nlesserextentFailure—andthis,eventhoughlocations\nstepsbeingcalledatrialandconsistingin65536steps\nintotal.Theresultswepresenthereareaveragedover 1HitRateisalsoknownasTruePositiveRate,RecallorSen-\n50trials. sitivity,whileCorrectnessisalsoknownasPrecisionorPositive\nPredictiveValue–seeforexampleKohaviandProvost(1998)for\nAfirstgroupoftestswascarriedoutwiththedoor\ndefinitions.Herewemultipliedtheobtainedfiguresby100toget\nkept closed all along, so the agent had no access to percentages.\n6\nfobN\nfobN\nspetS\nspetS\n1\n215\n2\n4201\n4\n8402\n8\n6904\n61\n2918\n23\n48361\n46\n86723\n821\n63556\n652\nTable 2. Predictions’ Hit Rates after n steps, door for each of the second room’s type of locations (we\nclosed. say that two locations are of the same type if they\nhaveexactlythesamefeatures).Figure4.Bshowsthe\npercentages of OK, KO and Failure obtained in this manner.Theseresultsreflecttheagent’sperformance\n1 13.4 11.9 0.0 0.0 0.0 0.0 1.4 at making predictions about the second room’s loca-\n1\n2 14.0 11.6 0.0 0.0 0.0 0.0 0.0 tions:itsuccessfullypredictsOKandKOboxes,but\n1 32.6 39.1 28.6 14.0 9.1 0.0 4.1\n8 hasmoredifficultiespredictingfailure.\n2 28.3 35.7 28.2 11.1 0.0 0.0 0.0\n1 77.5 77.7 45.3 35.0 26.0 0.0 22.4 Inasecondgroupoftests,wekeptthesamesetup,\n64\n2 66.2 69.3 38.8 22.8 0.0 0.0 0.0 but opened the door at the 2048th step. The agent\n1 94.3 92.2 70.5 64.5 57.2 0.0 50.9\n512 spontaneously went in the second room, and spent a\n2 84.5 84.6 40.2 23.4 0.0 0.0 0.0\n1 97.3 94.7 83.0 79.6 72.5 0.0 72.6 variablebutsignificantamountoftimeinit(36.6%of\n8192\n2 91.7 87.2 52.0 26.0 0.0 0.0 0.0 stepsonaverage,standarddeviation=10.7).\n1 96.5 94.1 85.7 80.7 81.5 0.0 75.0\n32768 Tables 4 and 5 show the features’s Hit Rates and\n2 89.5 84.6 52.2 23.8 0.0 0.0 0.0\nCorrectness from the moment the door was opened.\nThese results show that the agent was able to learn\nnewconceptsinvolvingtheSoundfeature.Theseem-\nfromthesecondroomhavedifferentsetsoffeatures,\nincludingforsomeofthemanewfeature,Sound.The ingly low Hit Rates for the feature are explained by\nthelackofobservablecuesinboxesatthedirectsouth\npoorperformanceatwallpredictionisduetothelack\nof boxes with sound, which prevents the agent from\nof general rules of the universe regarding the pres-\nbeingabletopredictitin40%ofthecases.Thesere-\nenceofwallsinadjacentboxes:theagentusespartic-\nsultsalsoshowthatchangingtheagent’senvironment\nular concepts to predict them in the first room hence\ndoesnotleadtosignificantlossofpreviouslyacquired\nitishelplessinthesecondroom.Themixedresultfor\nknowledge.Whathappensisthatthenetworkaccom-\nfailurepredictioncomesfromacompetitionbetween\nmodatesthenewconceptsbyrecruitingneuronsthat\ngeneralactionconcepts,thecontrolofwhichneedsto\nhave not been used for a long time to encode them\nbeimproved.\n(remembertheboostingofinputssumsinthelearning\nTable3.Predictions’Correctnessafternsteps,door processesofO-andA-neurons).Asaresult,theagent\nclosed. tendstoforgetthedetailsofparticularobjectsandac-\ntions such as boxes’ names or the presence of Cold\nand walls (see the moderate drop of these features’\nHit Rates), and more generally to forget old and un-\nused concepts. But general concepts, which are used\n1 18.0 17.5 0.0 0.0 0.0 0.0 4.0\n1 on a regular basis, are preserved, at least as long as\n2 20.0 18.4 0.0 0.0 0.0 0.0 0.0\n1 45.2 42.1 26.6 18.8 5.1 0.0 5.2 theykeepbeingused.\n8\n2 43.4 40.6 20.4 11.5 0.0 0.0 0.0 Accordingly, the agent’s ability to use its knowl-\n1 74.8 74.5 58.7 34.0 28.6 0.0 22.7\n64 edge to make appropriate decisions in the first room\n2 72.0 72.8 41.8 18.9 0.0 0.0 0.0\n1 90.5 87.2 83.9 69.5 72.8 0.0 68.6 isnotimpactedbythedoorbeingopened.Infact,the\n512\n2 83.4 83.1 47.4 25.9 0.0 0.0 0.0 barchartofOK,KOandFailureoutcomesobtainedin\n1 93.0 92.2 94.4 87.7 95.2 0.0 84.3\n8192 this second run of tests showed no visible difference\n2 85.5 86.5 66.2 36.3 0.0 0.0 0.0\n1 94.2 93.0 93.4 89.9 94.0 0.0 87.1 withtheoneobtainedwiththedoorclosedandshown\n32768\n2 85.3 86.2 60.7 38.1 0.0 0.0 0.0 inFigure4.A.\nAs regards computing time, it takes about 10 sec-\nWealsotestedtheagent’sabilitytouseitsknowl-\nedge to make appropriate decisions. Each time it\nchose,duringthesimulation,toexploititsknowledge,\nwerecordedthechosenaction’soutcome.Figure4.A\nshows the percentages of OK, KO and Failure out-\ncomesthusobtainedineachseriesofsteps.Wekept\ntrackofthevisitedlocations,inordertocheckthatthe\nagentwasnotloopingindefinitelyonthesameboxes:\ninfact,allboxeskeptbeingvisited,beitveryrarely,\natanypointofthetrials,duetotheExplorationmode.\nFinally,wetestedwhethertheagentwouldbeable\ntousetheknowledgeacquiredinthefirstroomtoact\nFigure4.Actions’meanoutcomeswithdoorclosed;\njudiciously in the second room. To do so, at the end\nGreen:OK,Red:KO,Blue:Failure,Grey:nodata\nof each series of steps we asked it to chose a move\n7\nfobN\nfobN\nspetS\nspetS\nmooR\nmooR\nKO\nKO\nOK\nOK\n.liaF\n.liaF\nllaW\nllaW\ndloC\ndloC\ndnuoS\ndnuoS\nxoB\nxoB\nemaN\nemaN\nTable 4. Predictions’ Hit Rates after n steps, door willalwaysbeabletolearnaboutnewenvironments,\nopenedat2048thstep.\nbyreplacingoldunusedconceptsbynewusefulones.\nFurtherworkremainstobedonetoendowtheagent\nwithplanningabilities.Notably,anotionofapplica- ble action law would be needed. Intuitively, it seems\n1 95.8 94.2 78.5 74.1 69.2 0.0 64.7 that an action law represented by an action concept\n2048\n2 89.3 86.2 45.7 24.2 0.0 0.0 0.0 [x,y,z]shouldbedeemedapplicableinasituation\n1 97.9 96.5 75.6 62.7 56.6 0.0 53.1\n4096 s if s satisfies all the features in x and z ̸= Fail-\n2 93.7 92.9 57.4 34.2 0.0 35.0 5.1\n1 96.8 96.9 81.1 66.4 58.9 0.0 55.6 ure.Furthermore,tocomplywithopenworldrequire-\n8192\n2 93.7 93.6 64.0 36.5 0.0 34.2 4.2 mentstheagentwouldneedtobeabletobuilditsown\n1 96.8 95.4 82.5 68.5 67.4 0.0 60.0\n16384 setofpossiblesituations(states)online.Thesetofits\n2 92.4 91.3 66.0 33.9 0.0 32.1 5.3\n1 96.9 97.2 85.6 67.8 59.1 0.0 57.0 objectconceptscouldprobablybeusedtothisend.A\n32768\n2 93.2 92.5 75.5 39.7 0.0 38.4 7.6 cost function should also be added, and the decision\n1 96.9 94.4 88.8 68.4 59.2 0.0 60.5\n65536 systemshouldbeaugmentedsoastohandlegoals.\n2 93.6 90.7 80.5 40.5 0.0 39.6 7.1\nAdditionally, a number of other improvements\nTable5.Predictions’Correctnessafternsteps–door wouldbedesirableinordertoallowtheagenttolive\nopenedat2048thstep. inmorerealisticenvironments.Afirstonewouldbeto\nimplementnegationinthenetwork,sothattheagent\nwould be able to represent the fact that a given ob-\njectdoesnothaveagivenfeature.Webelievethatthis\n1 92.9 89.5 91.5 82.7 91.0 0.0 78.9 could be done by the means of neural inhibition, but\n2048\n2 85.2 83.7 58.0 36.6 0.0 0.0 0.0 theappropriatelearningrulesremaintobefound.An-\n4096 1 94.6 90.7 93.0 75.1 91.8 0.0 88.1 otheressentialimprovementwouldbetohavethenet-\n2 90.9 87.8 75.9 44.7 0.0 64.0 1.2\n1 94.9 93.2 93.3 77.7 94.3 0.0 87.9 work use incomplete information as input for learn-\n8192\n2 91.1 89.2 79.7 44.8 0.0 61.5 1.4 ingandquerying,andtomaketheagentabletoquery\n16384 1 95.6 93.0 91.7 81.5 96.0 0.0 90.3 itssemanticmemoryforobjectpropertiesgivensome\n2 91.3 89.2 75.9 47.3 0.0 66.2 1.1\n1 96.2 94.5 94.1 78.3 95.2 0.0 91.0 partialinput.Itseemstousthatthiswouldbringthe\n32768\n2 93.2 91.5 79.9 50.8 0.0 64.0 2.5 agent to draw non-monotonic inferences in the spirit\n65536 1 96.2 95.9 90.6 82.1 94.6 0.0 94.7 of Grimaud (2016). It would also be useful to allow\n2 93.6 92.8 79.8 51.7 0.0 65.4 2.2\ntheagenttodistinguishbetweenobjectsandtheirlo-\ncations, since actions can modify one, the other or\nboth.Biologicalbrainsachievethisbyusingtwosep-\nondsonaconventionalcomputerfortheagenttorun\naratepathwaystoprocessthe“what”andthe“where”\n2048stepswhilekeepingtrackofallthetestdata.We\ncomponents of observations before reunifying them,\nmade no attempt to optimize the computing time, as\nandthiscouldbeaninspirationsource.Lastly,acom-\nitseemslesscriticalinthecaseofonlinelearningof\npletely different line of research would be to inves-\nautonomousagentswhichcanlearnwhilephysically\ntigate how the agent should decide between Explo-\nperformingtheiractions.\nrationandExploitationmodesinanopenworld.\nFinally, we highlight that the choice of a localist\nrepresentationfortheneuralnetworkmakesiteasyto\nread, since the concept encoded by each neuron can References\nbeknownsimplybylookingatitsconnections.\nBausch, M.; Niediek, J.; Reber, T. P.; Mackay, S.;\nBostro¨m, J.; Elger, C. E.; and Mormann, F. 2021.\n8. ConclusionandFutureDevelopments\nConcept neurons in the human medial temporal\nlobe flexibly represent abstract relations between\nIn this paper we have designed and implemented a\nconcepts. Naturecommunications,12(1):6164.\nfullyautonomousagentthatlearnsactionlawsonline\nand accommodates environment changes. This agent Bolander, T.; and Gierasimczuk, N. 2018. Learning\nrelies on general concepts to handle new situations to act: qualitative learning of deterministic action\nanddynamicallyadjustsitsconceptstoitscurrenten- models. JournalofLogicandComputation,28(2):\nvironment.Thismakesitwellsuitedforopenworlds: 337–365.\nifanewdoorweretoopentoathirdroomwithnew\nobjectsandlaws,itwouldlearnthemjustasitdidfor Bonet,B.;Frances,G.;andGeffner,H.2019. Learn-\nthe second room. Of course, this would come at the ing features and abstract actions for computing\ncost of the forgetting of its least used concepts, but generalizedplans.InProceedingsoftheAAAICon-\nthese are precisely the ones it needs the less. In fact, ferenceonArtificialIntelligence,volume33,2703–\ntheagent’sabilitytoselectivelyforgetensuresthatit 2710.\n8\nfobN\nfobN\nspetS\nspetS\nmooR\nmooR\nKO\nKO\nOK\nOK\n.liaF\n.liaF\nllaW\nllaW\ndloC\ndloC\ndnuoS\ndnuoS\nxoB\nxoB\nemaN\nemaN\nCui, Y.; Ahmad, S.; and Hawkins, J. 2016. Con- Koh, P. W.; Nguyen, T.; Tang, Y. S.; Mussmann, S.;\ntinuousonlinesequencelearningwithanunsuper- Pierson,E.;Kim,B.;andLiang,P.2020. Concept\nvisedneuralnetworkmodel. Neuralcomputation, bottleneckmodels. InInternationalconferenceon\n28(11):2474–2504. machinelearning,5338–5348.PMLR.\nDas, D.; Chernova, S.; and Kim, B. 2023. Kohavi,R.;andProvost,F.1998. GlossaryofTerms.\nState2explanation: Concept-based explanations to\nbenefitagentlearninganduserunderstanding. Ad- Lesort,T.;Lomonaco,V.;Stoian,A.;Maltoni,D.;Fil-\nvancesinNeuralInformationProcessingSystems, liat, D.; and D´ıaz-Rodr´ıguez, N. 2020. Continual\n36:67156–67182. learningforrobotics:Definition,framework,learn-\ningstrategies,opportunitiesandchallenges. Infor-\nFarebrother, J.; Machado, M. C.; and Bowling, M. mationfusion,58:52–68.\n2018. Generalization and regularization in DQN.\narXivpreprintarXiv:1810.00123. Moerland, T. M.; Broekens, J.; Plaat, A.; Jonker,\nC. M.; et al. 2023. Model-based reinforcement\nFreund,M.2008. OnthenotionofconceptI. Artifi- learning: A survey. Foundations and Trends® in\ncialIntelligence,152(1):105–137. MachineLearning,16(1):1–118.\nGhorbani, A.; Wexler, J.; Zou, J. Y.; and Kim, B. Quiroga, R. Q. 2012. Concept cells: the building\n2019. Towards automatic concept-based explana- blocks of declarative memory functions. Nature\ntions. Advances in neural information processing ReviewsNeuroscience,13:587–597.\nsystems,32.\nShimamura,A.P.2010. Hierarchicalrelationalbind-\nGrimaud, C. 2016. Modelling reasoning processes\ning in the medial temporal lobe: the strong get\nin natural agents: a partial-worlds-based logical\nstronger. Hippocampus,20(11):1206–1216.\nframework for elemental non-monotonic infer-\nences and learning. Journal of Applied Non- Sutton,R.S.;andBarto,A.G.2018. Reinforcement\nClassicalLogics,26(4):251–285. learning:Anintroduction. MITpress.\nGupta, A.; and Narayanan, P. 2024. A survey on Thiele, J.; Bichler, O.; and Dupret, A. 2018. Event-\nConcept-based Approaches For Model Improve- based, timescale invariant unsupervised online\nment. arXivpreprintarXiv:2403.14566. deep learning with STDP. Front. Comput. Neu-\nrosci.12,46(2018).\nHase,P.;Chen,C.;Li,O.;andRudin,C.2019. Inter-\npretable image recognition with hierarchical pro- Thorpe, S. 2023. Timing, Spikes, and the Brain. In\ntotypes. In Proceedings of the AAAI Conference Time and Science: Volume 2: Life Sciences, 207–\non Human Computation and Crowdsourcing, vol- 236.WorldScientificPublishingEuropeLdt.\nume7,32–40.\nThorpe, S.; Masquelier, T.; Martin, J.; Yousefzadeh,\nHawkins, J.; and Subutai, A. 2016. Why Neurons A. R.; and Linares-Barranco, B. 2019. Method,\nHave Thousands of Synapses, a Theory of Se- digital electronic circuit and system for unsuper-\nquenceMemoryinNeocortex. FrontiersinNeural vised detection of repeating patterns in a series of\nCicuits,10. events. PatentUS20190286944A1.\nKheradpisheh, S. R.; Ganjtabesh, M.; Thorpe, S. J.; Wang, A.; Lee, W.-N.; and Qi, X. 2022. Hint: Hier-\nandMasquelier,T.2017.STDP-basedspikingdeep archicalneuronconceptexplainer. InProceedings\nconvolutional neural networks for object recogni- of the IEEE/CVF Conference on Computer Vision\ntion. NeuralNetworks,99:56–67. andPatternRecognition,10254–10264.\nKirk, R.; Zhang, A.; Grefenstette, E.; and Wang, L.; Zhang, X.; Su, H.; and Zhu, J. 2024. A\nRockta¨schel, T. 2023. A survey of zero-shot comprehensive survey of continual learning: the-\ngeneralisation in deep reinforcement learning. ory, method and application. IEEE Transactions\nJournal of Artificial Intelligence Research, 76: onPatternAnalysisandMachineIntelligence.\n201–264.\nWang,Y.;Yao,Q.;Kwok,J.T.;andNi,L.M.2020.\nKirkpatrick,J.;Pascanu,R.;Rabinowitz,N.;Veness,\nGeneralizing from a few examples: A survey on\nJ.;Desjardins,G.;Rusu,A.A.;Milan,K.;Quan,J.;\nfew-shotlearning. ACMcomputingsurveys(csur),\nRamalho, T.; Grabska-Barwinska, A.; et al. 2017.\n53(3):1–34.\nOvercoming catastrophic forgetting in neural net-\nworks.Proceedingsofthenationalacademyofsci- Watkins, C. J. C. H. 1989. Learning from delayed\nences,114(13):3521–3526. rewards.\n9\nZabounidis,R.;Campbell,J.;Stepputtis,S.;Hughes,\nD.; and Sycara, K. P. 2023. Concept learning\nfor interpretable multi-agent reinforcement learn-\ning.InConferenceonRobotLearning,1828–1837.\nPMLR.\n10",
    "pdf_filename": "SNN-Based_Online_Learning_of_Concepts_and_Action_Laws_in_an_Open_World.pdf"
}