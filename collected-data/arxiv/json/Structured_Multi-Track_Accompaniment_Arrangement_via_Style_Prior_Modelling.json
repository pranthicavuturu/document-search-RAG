{
    "title": "Structured Multi-Track Accompaniment Arrangement",
    "abstract": "In the realm of music AI, arranging rich and structured multi-track accompani- mentsfromasimpleleadsheetpresentssignificantchallenges. Suchchallenges includemaintainingtrackcohesion,ensuringlong-termcoherence,andoptimizing computationalefficiency. Inthispaper,weintroduceanovelsystemthatleverages priormodellingoverdisentangledstylefactorstoaddressthesechallenges. Our method presents a two-stage process: initially, a piano arrangement is derived fromtheleadsheetbyretrievingpianotexturestyles;subsequently,amulti-track orchestration is generated by infusing orchestral function styles into the piano arrangement. Ourkeydesignistheuseofvectorquantizationandauniquemulti- streamTransformertomodelthelong-termflowoftheorchestrationstyle,which enablesflexible,controllable,andstructuredmusicgeneration. Experimentsshow thatbyfactorizingthearrangementtaskintointerpretablesub-stages,ourapproach enhancesgenerativecapacitywhileimprovingefficiency. Additionally,oursystem supportsavarietyofmusicgenresandprovidesstylecontrolatdifferentcompo- sitionhierarchies. Wefurthershowthatoursystemachievessuperiorcoherence, structure,andoverallarrangementqualitycomparedtoexistingbaselines. 1 Introduction Representationlearningtechniqueshaveenablednewpossibilitiesforcontrollablegenerativemod- elling. Bylearningimplicitstylerepresentations,whichareoftenhardtoexplicitlylabel(e.g.,timbre ofmusic audio[21], texture ofmusiccomposition[39], andartistic style inpaintings[20]), new musicandartworkscanbecreatedviastyletransferandlatentspacesampling. Theselearnedstyle factorscanalsoserveasexternalcontrolsfordownstreamgenerativemodels,includingTransform- ers [18, 36] and diffusion models [42]. HowNever, applying style factors to long-term sequence generationremainsachallengingtask. Existingapproachesrelyonstyletemplatesspecifiedmanually orbyheuristicrules[36,42,51],whichareimpracticalforlong-termgeneration. Moreover,when structuralconstraintsareimposed,misalignedstylefactorscanresultinincoherentoutputs. Toaddressthesechallenges,weaimtodevelopanovelsequencegenerationframeworkleveraginga globalstyleplanner,orprior,whichmodelstheconditionaldistributionofstylefactorsgiventhe modelinputâ€™scontentfactors. Bothstyleandcontentfactorsaresequencesofcompact,structurally aligned latent codes over a disentangled representation space. By infusing the style back to the content,wecanrecovertheobservationaltargetwithgloballycoherentstylepatterns. Inthispaper,westudystylepriormodellingthroughthetaskofmulti-trackaccompanimentarrange- ment,atypicalscenarioforlong-termconditionalsequencegeneration. Weassumetheinputofa 38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024). 4202 voN 91 ]DS.sc[ 3v43361.0132:viXra",
    "body": "Structured Multi-Track Accompaniment Arrangement\nvia Style Prior Modelling\nJingweiZhao1,3 GusXia4,5 ZiyuWang5,4 YeWang2,1,3\n1InstituteofDataScience,NUS 2SchoolofComputing,NUS\n3IntegrativeSciencesandEngineeringProgramme,NUSGraduateSchool\n4MachineLearningDepartment,MBZUAI 5ComputerScienceDepartment,NYUShanghai\njzhao@u.nus.edu gus.xia@mbzuai.ac.ae\nziyu.wang@nyu.edu wangye@comp.nus.edu.sg\nAbstract\nIn the realm of music AI, arranging rich and structured multi-track accompani-\nmentsfromasimpleleadsheetpresentssignificantchallenges. Suchchallenges\nincludemaintainingtrackcohesion,ensuringlong-termcoherence,andoptimizing\ncomputationalefficiency. Inthispaper,weintroduceanovelsystemthatleverages\npriormodellingoverdisentangledstylefactorstoaddressthesechallenges. Our\nmethod presents a two-stage process: initially, a piano arrangement is derived\nfromtheleadsheetbyretrievingpianotexturestyles;subsequently,amulti-track\norchestration is generated by infusing orchestral function styles into the piano\narrangement. Ourkeydesignistheuseofvectorquantizationandauniquemulti-\nstreamTransformertomodelthelong-termflowoftheorchestrationstyle,which\nenablesflexible,controllable,andstructuredmusicgeneration. Experimentsshow\nthatbyfactorizingthearrangementtaskintointerpretablesub-stages,ourapproach\nenhancesgenerativecapacitywhileimprovingefficiency. Additionally,oursystem\nsupportsavarietyofmusicgenresandprovidesstylecontrolatdifferentcompo-\nsitionhierarchies. Wefurthershowthatoursystemachievessuperiorcoherence,\nstructure,andoverallarrangementqualitycomparedtoexistingbaselines.\n1 Introduction\nRepresentationlearningtechniqueshaveenablednewpossibilitiesforcontrollablegenerativemod-\nelling. Bylearningimplicitstylerepresentations,whichareoftenhardtoexplicitlylabel(e.g.,timbre\nofmusic audio[21], texture ofmusiccomposition[39], andartistic style inpaintings[20]), new\nmusicandartworkscanbecreatedviastyletransferandlatentspacesampling. Theselearnedstyle\nfactorscanalsoserveasexternalcontrolsfordownstreamgenerativemodels,includingTransform-\ners [18, 36] and diffusion models [42]. HowNever, applying style factors to long-term sequence\ngenerationremainsachallengingtask. Existingapproachesrelyonstyletemplatesspecifiedmanually\norbyheuristicrules[36,42,51],whichareimpracticalforlong-termgeneration. Moreover,when\nstructuralconstraintsareimposed,misalignedstylefactorscanresultinincoherentoutputs.\nToaddressthesechallenges,weaimtodevelopanovelsequencegenerationframeworkleveraginga\nglobalstyleplanner,orprior,whichmodelstheconditionaldistributionofstylefactorsgiventhe\nmodelinputâ€™scontentfactors. Bothstyleandcontentfactorsaresequencesofcompact,structurally\naligned latent codes over a disentangled representation space. By infusing the style back to the\ncontent,wecanrecovertheobservationaltargetwithgloballycoherentstylepatterns.\nInthispaper,westudystylepriormodellingthroughthetaskofmulti-trackaccompanimentarrange-\nment,atypicalscenarioforlong-termconditionalsequencegeneration. Weassumetheinputofa\n38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).\n4202\nvoN\n91\n]DS.sc[\n3v43361.0132:viXra\npianoaccompanimentscore,whichtypicallycarriesaverse-chorusstructure. Ourtargetistogenerate\ncorrespondingmulti-trackarrangementsfeaturingbandorchestration. Westartbydisentanglinga\nbandscoreattimetintopianoreductionc (contentfactor)andorchestralfunctionsk (stylefactors\nt t\nforindividualtracksk = 1,2,Â·Â·Â· ,K). Ontopofthis,wemodeltheprioroffindingappropriate\nfunctionstoorchestrateagivenpianoscore,orformallyp(s1:K | c ). Tomodeldependencies\n1:T 1:T\nonbothtime(T)andtrack(K)directions,wedevelopamulti-streamTransformerwithinterleaved\ntime-wiseandtrack-wiselayers. Thetrack-wiselayerallowsforflexiblecontroloverthechoice\nof instruments and the number of tracks, while the time-wise layer ensures structural alignment\nthroughcross-attentiontothepianoreduction. Decodingtheinferreds1:K withc ,wecanaddress\n1:T 1:T\naccompanimentarrangementinaflexiblemulti-trackformwithextendedwhole-songstructure.\nExperimentsshowthatourmethodoutperformsexistingsequentialtokenpredictionapproachesand\nprovidesbettermulti-trackcohesion,structuralcoherence,andcomputationalefficiency. Additionally,\ncomparedtoexistingdesignsofmulti-streamlanguagemodels,ourmodelhandlesflexiblestream\ncombinationsmoreeffectivelywithenhancedgenerativecapacity.\nTosummarize,ourcontributionsinthispaperarethree-folded:\nâ€¢ Weproposestylepriormodelling,ahierarchicalgenerativemethodologyaddressing\nbothlong-termstructure(viastylepriorathighlevel)andfine-grainedcondition/control(via\nrepresentationdisentanglementatlowlevel). Ourapproachmovesbeyondthelimitation\nofmanualspecificationofstylefactors,providingaflexible,efficient,andself-supervised\nsolutionforlong-termsequencepredictionandgenerationtasks.\nâ€¢ Weproposeanovellayerinterleavingarchitectureformulti-streamlanguagemodelling.\nIn our case, it models parallel music tracks with a flexible track number, controllable\ninstruments,andmanageablecomputation. Toourknowledge,itisthefirstmulti-stream\nlanguagemodelwithtractablegeneralizationtoflexiblestreamcombinations.\nâ€¢ Integratingourpreviousstudyonpianotexturestyletransfer[39,50],wepresentacomplete\nmusicautomationsystemarranginganinputleadsheet(abasicmusicformwithmelody\nandchordonly)viapianoaccompanimenttomulti-trackarrangement. Theentiresystemis\ninterpretableattwocompositionhierarchies: 1)pianotextureand2)orchestralfunction,\nanddemonstratesstate-of-the-artarrangementperformanceforvariedgenresofmusic.1\n2 RelatedWorks\nInthissection,weoverviewthreetopicsrelatedtoourstudy. Section2.1reviewsexistingstudieson\nrepresentationdisentanglement.Section2.2summarizespriormodellingmethodsinmusicgeneration.\nSection2.3reviewsthecurrentprogresswiththetaskofaccompanimentarrangement.\n2.1 Content-StyleDisentanglementviaRepresentationLearning\nRepresentationdisentanglementisapopulartechniqueindeepgenerativemodelling[3,16,48,49].\nInthemusicdomain,thisapproachhasprovenvaluablebylearningcompositionalfactorsrelatedto\nmusicstyleandcontent. Bymanipulatingthesefactorsthroughinterpolation[32],swapping[39],\nandpriorsampling[46],itprovidesaself-supervisedandcontrollablepathwayforvariousmusic\nautomationtasks. Recentworksleveragedisentangledstylefactorsascontrolsignalsforlong-term\nmusicgeneration[36,42]. However,theseapproachestypicallytreatstylerepresentationsasfixed\ncondition sequences during training, requiring manual specification or additional algorithms for\ncontrolduringinference. Incontrast,wemodelthepriorofthestyletoapplyconditionalonthegiven\nmusiccontent,whichisamoregeneralizedandflexibleapproach.\n2.2 MusicGenerationwithLatentPrior\nInsequencegenerationtasks(e.g.,musicandaudio),learningapriorsampleroveracompact,latent\nrepresentationspaceisoftenmoreefficientandeffective.Jukebox[7]modelsthelatentcodesencoded\nby VQ-VAEs [34] as music priors, which can further reconstruct minutes of music audio. More\nrecently,MusicLM[2]andMusicGen[4]learnmulti-modalpriorsforgeneratingmusicfromtext\n1Demoandmoreresources:https://zhaojw1998.github.io/structured-arrangement/\n2\nTable1: Summaryofthedatarepresentationsappliedinthispaper. Weusenotation[a..b]todenote\ntheintegerinterval{x|aâ‰¤xâ‰¤b,xâˆˆZ}includingbothendpoints.\nMulti-TrackArrangement PianoReduction OrchestralFunction\nDataRepresentation xâˆˆ[0..32]TÃ—KÃ—32Ã—128 pn[x]âˆˆ[0..32]TÃ—32Ã—128 fn[x]âˆˆ[0,1]TÃ—KÃ—32\nLatentDimension zâˆˆRTÃ—256 câˆˆRTÃ—256 sâˆˆ[0..127]8TÃ—K\nprompts. Whilepriormodellingfacilitateslong-termgeneration,thelatentcodesintheseworksare\nnotinterpretable,thuslackingaprecisecontrolbymusiccontent-basedsignals(e.g.,musicstructure).\nSuchcontrolsareessentialforconditionalgenerationtasks,includingaccompanimentarrangement.\nInthispaper,wemodelastylepriorconditionalonthedisentangledmusiccontent,whichallowsfor\nstructuredlong-termmusicgeneration,enhancingbothinterpretabilityandcontrollability.\n2.3 AccompanimentArrangement\nAccompanimentarrangementaimstocomposetheaccompanimentpartgivenaleadsheet,whichisa\ndifficultconditionalgenerationtaskinvolvingstructuralconstraints. Existingmethodsmainlytraina\nconditionallanguagemodelbasedonsequentialnote-leveltokenization[14,15,30,33],whichoften\nsuffer from slow inference speed, truncated structural context, and/or simplified instrumentation.\nRecentattemptswithdiffusionmodelsshowhighersamplequalitywithfasterinference[23,26,27],\nbut still consider limited instruments or tracks. AccoMontage [47, 50] maintains a whole-song\nstructurebymanipulatinghigh-levelcompositionfactors,butislimitedtopianoarrangementalone.\nOurpaperpresentsatwo-stageapproach: fromleadsheettopianoaccompaniment,andfrompiano\nto multi-track, both leveraging prior modelling of high-level style factors. This approach offers\nmodularity[11]andenableshigh-qualitywhole-songandmulti-trackaccompanimentarrangement.\n3 Method\nWedevelopamodelthattakesapianoreductionasinputandoutputsanorchestratedmulti-track\narrangement. Usinganautoencoder,wedisentangleamulti-trackmusicscoreintoitspianoreduction\n(contentfactor)andorchestralfunction(stylefactor). Wethendesignapriormodeltoinferorchestral\nfunctionsgiventhepianoreduction. Theautoencoderoperatesatsegmentlevel,whilethepriormodel\nworksonthewholesong. Theentiremodelcanoperateasanorchestratormoduleinacomplete\narrangementsystem. Inthissection,weintroduceourdatarepresentationinSection3.1,autoencoder\nframeworkinSection3.2,andpriormodeldesigninSection3.3.\n3.1 DataRepresentation\nWesummarizeourdatarepresentationsinTable1. LetxbeaK-trackarrangementscore. Wesplit\nitintoT segmentsandrepresentxk â€”eachsegmenttrackâ€”asamatrixofshapeP Ã—N. Here\nt\nP = 128 represents 128 MIDI pitches and N is the time dimension of a segment. This matrix\nrepresentation aligns with the modified piano roll in [39], where each non-zero entry (p,n) > 0\nindicatesanoteonsetanditsvalueindicatesthenoteduration. Inthispaper, weprimarilyfocus\nonmusicpiecesin4/4timesignaturewith1/4-beatresolution. Durationvaluesrangefrom1(for\nsixteenthnotes)to32(fordoublewholenotes). Weconsider1segment=8beats(2bars)andderive\nN =32,whichisaproperscaleforlearningmusiccontent/stylerepresentations[37,39â€“41,46].\nThepianoreductionofxisnotatedaspn[x]. ItisapproximatedbydownmixingallK tracksintoa\nsingle-trackmixturesimilarto[8]. Whenconcurringnotesarefoundacrosstracks,wekeeptheone\nwiththelargestduration(i.e.,track-wisemaximum). Segment-wise,pn[x] isalsoaP Ã—N matrix.\nt\nItpreservestheoverallmusiccontentwhilediscardingthemulti-trackform.\nTheorchestralfunctionofxisnotatedasfn[x]. Itdescribestherhythmandgroovingpatterns[45]of\neachsegmenttrack,whichservesastheâ€œskeletonâ€ofamulti-trackform. Formally,\nfn[x]k =colsum(1 )/max_sum, (1)\nt {xk>0}\nt\nwhere indicator function 1 counts each note onset position as 1; colsum(Â·) sums up the pitch\n{Â·}\ndimension,derivinga1Ã—N time-seriesfeature;max_sum=14isfornormalization.Theorchestral\n3\nTrack Decoder\nTrack Axis ğ³ğ³ğ‘¡ğ‘¡1 ğ³ğ³ğ‘¡ğ‘¡2 ğ³ğ³ğ‘¡ğ‘¡ğ¾ğ¾ VQ Codebook\nDect ğ±ğ±ï¿½ğ’•ğ’•ğŸğŸ (Track SD ee pc at ratorğ±ğ± ï¿½)ğ’•ğ’•ğŸğŸ Dect ğ±ğ±ï¿½ğ’•ğ’•ğ‘²ğ‘² (128x16)\nâ‹¯\nFn. Enc. Fn. Dec. Sep\nfn[ğ±ğ±]ğ‘¡ğ‘¡ğ‘˜ğ‘˜ fï¿½n[ğ±ğ±]ğ‘¡ğ‘¡ğ‘˜ğ‘˜\npnP[iğ±ğ±an]ğ‘¡ğ‘¡o EEncnocdp er Encf Decf Encf Decf Encf Decf Function Encoder Function Decoder\nFigure1:Thğœğœeğ‘¡ğ‘¡autoencodğ¬ğ¬eğ‘¡ğ‘¡1 rarchitecture.ğ¬ğ¬Iğ‘¡ğ‘¡2 tlearnâ‹¯scontentrepğ¬ğ¬ğ‘¡ğ‘¡ğ¾ğ¾\nresentationc tfrompianoreduction,style\nrepresentationss1:K fromorchestralfunction,andleveragesbothtoreconstructindividualtracks.\nt\nfunction fn[x] essentially describes the form, or layout, of multi-track music x. It indicates the\nrhythmicintensityofparalleltracksandinformswheretoputmorenotesandwheretokeepsilent.\n3.2 Autoencoder\nOurautoencoderconsistsoftwocomponentsasshowninFigure1. AVQ-VAEsubmodule(right\nofFigure1)encodesorchestralfunctionfn[x]k. AVAEmodule(leftofFigure1)encodespiano\nt\nreductionpn[x] andreconstructsindividualtracksx1:K leveragingthecuesfromfn[x]1:K. During\nt t t\ntraining,bothinputspn[x]andfn[x]aredeterministictransformsfromtheoutputxandtheentire\nmodelisself-supervised.Weseesimilartechniquesforrepresentationdisentanglementin[39,41,46].\nTheVQ-VAEconsistsofFunctionEncoderEncf andDecoderDecf. EncoderEncf containsa1-D 2\nconvolutionallayerfollowedbyavectorquantizationblock. OurintuitionforapplyingaVQ-VAEis\nthatorchestralfunctioncommonlyconsistsofrhythmpatterns(suchassyncopation,arpeggio,etc.)\nthatcannaturallybecategorizedasdiscretevariables. Inourcase,eachsegmenttrackisencoded\ninto8discreteembeddingsona1-beatscale,indicatingtheflowoforchestrationstyle. Formally,\nsk :={sk}8t =Encf(fn[x]k), k =1,2,Â·Â·Â· ,K, (2)\nt Ï„ Ï„=8tâˆ’7 t\nwheresk isthelatentorchestralfunctioncodeforthek-thtrackattheÏ„-thbeat. Weencodefn[x]k at\nÏ„ t\nafiner1-beatscale(insteadofsegment)topreservefine-grainedrhythmicdetails. Thenewscaleis\nre-indexedbyÏ„ =1,2,Â·Â·Â· ,8T. Wecollectivelydenoteeach8-codegroupingassk forconciseness.\nt\nTheVAEconsistsofPianoEncoderEncp,TrackSeparatorSep,andTrackDecoderDect. Encoder\nEncp learnscontentrepresentationc frompianoreductionpn[x] . Herec isacontinuousrepre-\nt t t\nsentation(withoutvectorquantization)thatcapturesmorenuancedmusiccontent. DecoderDect\nreconstructsindividualtracksxk fromtrackrepresentationzk. Notably,z1:K arerecoveredfromc\nt t t t\nusingtheorchestralfunctioncuesfroms1:K. Formally,\nt\nz1,z2,Â·Â·Â· ,zK =Sep(s1,s2,Â·Â·Â· ,sK |c ), (3)\nt t t t t t t\nwhereTrackSeparatorSepisaTransformerencoder. Inthisprocess,eachsk queriesc torecover\nt t\nthecorrespondingtrack(k),whiletheyalsoattendtoeachothertomaintainthedependencyamong\nparalleltracks. Learnableinstrumentembeddings[51]areaddedtoeachtrackbasedonitsinstrument\nclass. WeprovidedetailsoftheautoencoderarchitectureinAppendixA.1.\n3.3 StylePriorModelling\nTheVQ-VAEinSection3.2deriveslatentcodess1:K fororchestralfunctionasamulti-streamtime\n1:8T\nseries. Herek =1,2,Â·Â·Â·K isthestream(track)indexandÏ„ =1,2,Â·Â·Â· ,8T isthetime(beat)index.\nThepurposeofstylepriormodellingistoinferorchestralfunctiongivenpianoreductionsothatthe\nformercanbeleveragedtoorchestratethelatterintomulti-trackmusic. Wedesignourpriormodel\nasshowninFigure2. Itisanencoder-decoderframeworkthatmodelsparalleltracks/streamsof\norchestralfunctioncodesconditionalonthepianoreduction.\nThedecodermodule(rightofFigure2)hasalternatelayersofTrackEncoderandAuto-Regressive\nDecoder. TrackEncoderisastandardTransformerencoderlayer[35]anditaggregatesinter-track\ninformationalongthetrackaxis. Auto-RegressiveDecoderisaTransformerdecoderlayer(with\nself-attentionandcross-attention)anditpredictsnext-steporchestralfunctioncodesonthetimeaxis.\n4\nâ‹¯ â‹¯ â‹¯\nâ‹¯\nConv1d\nâ‹¯\nFC FC\nCross-Attention\nğ¾ğ¾ ğ¾ğ¾ ğ¾ğ¾ ğ¾ğ¾\nğ‘ ğ‘ 1 ğ‘ ğ‘ 2 ğ‘ ğ‘ 3 ğ‘ ğ‘ ğœğœ+1\nContext Encoder 1\nğ‘ ğ‘ 1 â‹¯\n+ 1\nAuto-Regressive Deâ‹¯coder\nğ‘ ğ‘ ğœğœ+1\n+\nâ‹¯\nSinusoidal Positional Eğœğœn1c:ğ‘‡ğ‘‡oding\nRelative Positional Embedding +\nMusic Timing Condition\nInstrument Embedding\nGaussian Noise\nTime Axis\nğ‘ ğ‘ ğœğœğ¾ğ¾\n+\n1 1 1\nFigure2: Thepriormodelarchitecture. Theoverallsaorschitecğ‘ ğ‘ 1tureisğ‘ ğ‘ a2ne â‹¯ncoder-değ‘ ğ‘ cğœğœoderTransformer,\nwhilethedecodermoduleisinterleavedwithorthogonaltime-wiseandtrack-wiselayers.\nByorthogonallystackingtwotypesoflayers,wecanmodeltrack-wiseandtime-wisedependen-\nciessimultaneouslywithamanageablecomputationalcost. Comparedtosequentialtokenization\nmethodsinpreviousstudies[9,30,36],ourmethodbringsdownthecomplexityfromO(N2T2)to\nO(max(N,T)2). Moreover,wesupportaflexiblemulti-trackform(N beingvariable)withadiverse\ninstrumentationoption. Weaddinstrumentembedding[51]andrelativepositionalembedding[15]to\nthetrackaxis,where34instrumentclasses[25]aresupported. Weaddmusictimingcondition[7]to\nthetimeaxis,whichencodesthepositionsinatrainingexcerptasfractionsofthecompletesong,\nhelpingthemodelcapturetheoverallstructureofasong.\nTheencodermodule(leftofFigure2)ofourpriormodelisastandardTransformerencoder,which\ntakespianoreductionc asglobalcontext.Itisconnectedtothedecodermoduleviacross-attention\n1:T\nandmaintainstheglobalphrasestructure. Duringtraining, bothc ands1:K arederivedfrom\n1:T 1:8T\nthe same multi-track piece and the entire model is self-supervised. Let p be the distribution of\nÎ¸\norchestralfunctioncodesfittedbyourpriormodelÎ¸,thetrainingobjectiveisthemeanofnegative\nlog-likelihoodofnext-stepcodeprediction:\nK\n1 (cid:88)\nL(Î¸)=âˆ’ logp (sk |s1:K,c ). (4)\nK Î¸ Ï„ <Ï„ 1:T\nk=1\nWeprovidemoreimplementationdetailsofthepriormodelinAppendixA.2. Wenotethatthereisa\npotentialdomainshiftfromourapproximatedpianoreductiontorealpianoarrangements. Toprevent\noverfitting,weuseaGaussiannoiseÏµtoblurc whilepreservingitshigh-levelstructure. During\n1:T\ntraining,Ïµiscombinedwithc usingaweightedsummationwithnoiseweightÎ³ rangingfrom0\n1:T\nto1. Itencouragesapartialunconditionalgenerationcapability. Atinferencetime,Î³ isaparameter\nthatcanbalancecreativitywithfaithfulness. AnexperimentonÎ³ iscoveredinAppendixC.\n4 Whole-SongMulti-TrackAccompanimentArrangement\nWefinalizeacompletemusicautomationsystembyapplyingstylepriormodellingattwocascaded\nstages. AsshowninFigure3,ourautoencoderandorchestralfunctionprioroperateonStage2for\npianotomulti-trackarrangement. OnStage1,weadoptourpreviousstudy,apianotextureprior[50]\nStage 1\nLead Sheet\nPiano Texture Prior External Control\nStage 2\nPiano Arrangement\nOrchestral Function Prior\nPiano Texture Style\nA Complete Multi-Track\nOrchestral Function Style Multi-Track Arrangement\nAccompaniment Arrangement System\nFigure3: Acompleteaccompanimentarrangementsystembasedoncascadedpriormodelling. The\nfirst stage models piano texture style given lead sheet while the second stage models orchestral\nfunctionstylegivenpiano. Besidesmodularity,thesystemofferscontrolonbothcompositionlevels.\n5\n13\nCounterpoint Relation between Tracks\nLong-Term Phrase Coherence over Extended Context\n41\nHarmonic/Melodic Division between Two Guitars\nMetrical Division between String and Brass\nFigure4: ArrangementforCanYouFeeltheLoveTonight,apopsonginatotalof60bars. Weshow\ntwochoruspartsfrombar13to41. Weusereddottedboxestoshowcoherenceinlong-termstructure.\nWeusecolouredblockstoshownaturalnessandcohesioninmulti-trackarrangement.\nontopofchord/texturerepresentationlearning[39],forleadsheettopianoarrangement.Givenalead\nsheet,thefirststagegeneratesapianoaccompaniment,establishingtheroughwhole-songstructure.\nOursystemthenorchestratesthepianoaccompanimentintoacompletemulti-trackarrangementwith\nbandinstrumentation. Thistwo-stageapproachmirrorsmusiciansâ€™creativeworkflow[1]andallows\nforcontrolatbothcompositionlevels. Inparticular,weprovidethreecontroloptions:\n1. TextureSelection: TofilterpianotexturesonStage1bymetadataandstatisticalfeatures.\n2. Instrumentation: TocustomizethetracknumberandchoiceofinstrumentsonStage2.\n3. OrchestralPrompt:Toprompttheorchestrationprocesswithanorchestralfunctiontemplate.\nWeshowcaseanarrangementexamplebythecompletesysteminFigure4. Thesysteminputisa\nleadsheetshownbytheMelstaff. Thefinaloutputistheaccompanimentinthereststaves. Notably,\ntheleadsheetconsistsof60barsinanstructureofi4A8B8B8x4A8B8B8O4(usingnotationsby[5]).\nHere,i4,x4,andO4eachdenotea4-barintro,interlude,andoutro. A8andB8representan8-bar\nverseandchorus,respectively. Figure4showsthearrangementresultforthefirstandthirdchoruses,\nspanningfrombar13to41. Weleveragecontroloption2tocustomizetheinstrumentationascelesta,\nacousticguitars(2),electricpianos(2),acousticpiano,violin,brass,andelectricbassinatotalof\nK =9tracks. ThecompletearrangementscoreisincludedinAppendixE.\nInFigure4,weobservesomemulti-trackarrangementpatternsthatarecommoninpractice. Purple\nblockshighlightacounterpointrelationbetweenguitartrackA.G.1andelectricpianotrackE.P.1.\n6\nGreenblocksshowtwoguitartrackswithcomplementaryorchestralfunctions: onemelodic(A.G.1)\nand theother harmonic (A.G.2). Yellow blocks illustratethe metrical division between the string\n(Vlns.)andthebrass(Br.)sections,withstringsonthedownbeatandbrassontheoffbeat. These\npatternsdemonstrateanaturalandcohesivemulti-trackarrangementbyoursystem. Moreover,we\nseeconsistentaccompanimentpatternsechoinginbothchoruspartsthatspanover30bars(shownby\nreddottedboxes),whilethelatteraddsapianoarpeggiotrack(Pno.)toenhancethemusicalflow.\nThisdemonstratesstructuredwhole-songarrangementoverextendedmusiccontexts.\n5 Experiment\nInthissection,weevaluatetheperformanceofourmulti-trackaccompanimentarrangementsystem.\nGiventhatexistingmethodsprimarilyfocusonleadsheettomulti-trackarrangement,weensurea\nfaircomparisonbyusingthetwo-stageapproachdiscussedinSection4. InSection5.1,wepresent\nthe datasets used and the training details of our model. In Section 5.2, we describe the baseline\nmodelsusedforcomparison. Ourevaluationisdividedintotwoparts: objectiveevaluation,detailed\nin Section 5.3, and subjective evaluation, covered in Section 5.4. For the single-stage piano to\nmulti-track(Stage2)andleadsheettopiano(Stage1)arrangementtasks,weperformadditional\ncomparisonswithvariousablationarchitecturesinSection5.5and5.6,respectively.\n5.1 DatasetsandTrainingDetails\nWe use two datasets to train the autoencoder and the style prior, respectively. The autoencoder\nistrainedwithSlahk2100[25],whichcontains2Kcuratedmulti-trackpieceswith34instrument\nclassesinabalanceddistribution. Wediscardthedrumtrackandclipeachpieceinto2-barsegments\nwith1-barhopsize. Weusetheofficialtrainingsplitandaugmenttrainingsamplesbytransposing\nto all 12 keys. The autoencoder comprises 19M learnable parameters and is trained with batch\nsize64for30epochsonanRTXA5000GPUwith24GBmemory. WeuseAdamoptimizer[19]\nwithalearningratefrom1e-3exponentiallydecayedto1e-5. Weuseexponentialmovingaverage\n(EMA)[29]andrandomrestart[7]toupdatethecodebookwithcommitmentratioÎ² =0.25.\nWeuseLakhMIDIDataset(LMD)[28]totrainthepriormodel. Itcontains170kmusicpiecesandis\nabenchmarkdatasetfortrainingmusicgenerativemodels. Wecollect2/4and4/4pieces(110kafter\nprocessing)andrandomlysplitLMDatsonglevelintotraining(95%)andvalidation(5%)sets. We\nfurtherclipeachpieceinto32-bartrainingexcerpts(i.e.,T =16atmaximum)witha4-barhopsize.\nOurpriormodelhas30Mparametersandistrainedwithbatchsize16for10epochs(600Kiterations)\nontwoRTXA5000GPUs. WeapplyAdamWoptimizer[22]withalearningrateof1e-4,scheduled\nbya1k-steplinearwarm-upfollowedbyasinglecycleofcosinedecaytoafinalrateof1e-6.\nFor model inference and testing, we consider two additional datasets: Nottingham [10] and\nWikiMT[44]. Bothdatasetscontainleadsheets(inABCnotation)thatarenotseenduringtrainingor\nvalidation. Moreover,theycoverdiversemusicgenresincludingfolk,pop,andjazz. Whenarranging\napiece,weleveragecontroloption2tosetuptheinstrumentation. Withoutlossofgenerality,this\ncontrolchoiceisrandomlysampledfromSlakh2100validation/testsets. Toarrangemusiclonger\nthanthepriormodelâ€™scontextlength(32bars), weusewindowedsampling[7], wherewemove\naheadourcontextwindowby4barsandcontinuesamplingbasedontheprevious28bars. Weapply\nnucleussampling[13]withtopprobabilityp=0.05andtemperaturet=6.\n5.2 BaselineModels\nWecompareoursystemwiththreeexistingmethods:PopMAG[30],AnticipatoryMusicTransformer\n(AMT)[33],andGETMusic[23]. PopMAGandGETMusicgeneratemulti-trackaccompaniments\nfrom an input lead sheet based on a Transformer and a diffusion model, respectively. AMT is\nTransformer-based and it continues the accompaniment part from an input melody with starting\naccompanimentprompt. Weprovidedetailedconfigurationsinthefollowing.\nPopMAGisanencoder-decoderarchitecturebasedonTransformer-XL[6]. Itrepresentsmulti-track\nmusicbysequentialnote-leveltokenizationandisfullysupervised. Theencodertakesaleadsheetas\ninputandthedecodergeneratesmulti-trackaccompanimentauto-regressively. Sincethemodelisnot\nopensource,WereproduceitonLMDwithleadsheetsextractedby[24](melody)and[17](chord).\n7\nTable2: Objectiveevaluationresultsforleadsheettomulti-track arrangement(Section5.3). All\nentriesareoftheformmeanÂ±sems,wheresisaletter. Differentletterswithinacolumnindicate\nsignificantdifferences(p<0.05/6)basedonWilcoxonsignedranktestwithBonferronicorrection.\nModel ChordAcc. â†‘ DOAâ†‘ Structureâ†‘ Latencyâ†“\nOurs 0.564Â±0.014a 0.300Â±0.005a 1.519Â±0.030a 0.461Â±0.005b\nAMT[33] 0.446Â±0.013bc 0.294Â±0.006a 1.094Â±0.009c 6.320Â±0.212d\nGETMusic[23] 0.423Â±0.012c 0.225Â±0.007c 1.243Â±0.017b 0.450Â±0.002a\nPopMAG[30] 0.470Â±0.013b 0.270Â±0.007b 1.086Â±0.008c 0.638Â±0.013c\nGround-Truth - 0.333Â±0.009 1.980Â±0.019 -\nAnticipatoryMusicTransformer(AMT)isadecoder-onlyTransformerarchitecturewithnote-level\ntokenization. Itintroducesanâ€œanticipationâ€method,whereconditionaltokens(melodyandstarting\nprompt)andgenerativetokens(accompanimentcontinuation)areinterleavedtotrainaconditional\ngenerative model. Since our testing dataset does not provide ground-truth accompaniment, the\nstarting prompt is given by the generation result (first 2 bars) of our system. We use the official\nimplementationoftheAMTmodel,2whichisalsotrainedonLMD.\nGETMusic represents multi-track music as an image-like matrix resembling score arrangement,\nbased on which a denoising diffusion probabilistic model is trained with a mask reconstruction\nobjective. Givenaleadsheet,itsupportsgenerating5accompanimenttracksusingpiano,guitar,\nstring,bass,anddrumortheirsubsets. Inourexperiment,wegenerateall5accompanimenttracks.\nWeusetheofficialimplementationoftheGETMusicmodel,3whichistrainedoninternaldata.\n5.3 ObjectiveEvaluationonMulti-TrackArrangement\nWeintroducefourmetricstoevaluatemulti-trackarrangementperformance: chordaccuracy[23,30],\ndegreeofarrangement(DOA),structureawareness[42],andinferencelatency[30]. Amongthem,\nchordaccuracymeasuresthemulti-trackharmonythatreflectsthefitnessoftheaccompanimenttothe\nleadsheet;DOAmeasuresinter-tracktonaldiversitythatreflectsthecreativityoftheinstrumentation.\nBothmetricsdemonstratemusiccohesionatlocalscales. Ontheotherhand,structureawareness\nmeasuresphrase-levelcontentsimilaritythatreflectslong-termstructuralcoherenceofthewhole\nsong. Finally, we use inference latency (in second/bar) to evaluate computational efficiency of\neach method. The detailed computation of each metric is provided in Appendix B. In Table 2,\nwecomputeground-truthDOAusing1000randompiecesfromLMD.Wecomputeground-truth\nstructureawarenessusing857piecesin4/4fromPOP909Dataset[38].\nWerandomlysample50piecesin4/4timesignaturefromNottinghamandWikiMTrespectively(100\npiecesintotal)toconductexperiment.Thelengthofeachpiecerangesfrom16to32bars.Werunour\nmethodandbaselinemodelsateachpiecein3independentrounds,deriving300setsofmulti-track\narrangementsamples. InTable2,wereporttheevaluationresultswithmeanvalue,standarderror\nofmean(sem),andstatisticalsignificancecomputedbyWilcoxonsignedranktest[43]. Wefind\nsignificantdifferencesbetweenourmethodandallbaselines(p-valuep<0.05/6,usingBonferroni\ncorrection). Inparticular,ourmethodoutperformsinchordaccuracy,structureawareness,andDOA,\nindicatingthecapabilityofarrangingharmonious,structured,andcreativeaccompaniments. The\ndiffusionbaselineoutperformsininferencelatencyasitappliesonly100diffusionsteps.Ourmethodâ€™s\nefficiencyisonparwithit,whilebeing10timesfasterthanvanillanote-levelauto-regression.\n5.4 SubjectiveEvaluationonMulti-TrackArrangement\nWealsoconductadouble-blindonlinesurveytotestmusicquality.Oursurveyconsistsof5evaluation\nsets,eachcontaininganinputleadsheetfollowedby4arrangementsamplesbyourmethodandeach\nbaseline. Eachsampleis24-32barslongandissynthesizedtoaudioat90BPM(~1minuteper\nsample). Boththesetorderandthesampleorderineachsetarerandomized. Werequestparticipants\nto listen to 2 sets and evaluate the musical quality based on a 5-point Likert scale from 1 to 5.\n2https://github.com/jthickstun/anticipation\n3https://github.com/microsoft/muzic/tree/main/getmusic\n8\nTable3: Objectiveevaluationresultsforpianotomulti-trackarrangement(Section5.5). Allentries\nareoftheformmeanÂ±sems,wheresisaletter. Differentletterswithinacolumnindicatesignificant\ndifferences(p<0.05/6)basedonWilcoxonsignedranktestwithBonferronicorrection.\nPrior Faithfulness(stats.) â†‘ Faithfulness(latent)â†‘ DOAâ†‘ NLLâ†“\nOurs 0.945Â±0.001a 0.215Â±0.005a 0.308Â±0.005a 0.411Â±0.004\nParallel 0.937Â±0.002b 0.153Â±0.003b 0.233Â±0.005c 0.960Â±0.010\nDelay 0.915Â±0.004c 0.133Â±0.003c 0.207Â±0.005d 1.024Â±0.006\nRandom 0.913Â±0.003c 0.113Â±0.003d 0.262Â±0.005b -\n\u00002\u0000X\u0000U\u0000V \u0000$\u00000\u00007 \u0000*\u0000(\u00007\u00000\u0000X\u0000V\u0000L\u0000F \u00003\u0000R\u0000S\u00000\u0000$\u0000* \u00002\u0000X\u0000U\u0000V \u00003\u0000D\u0000U\u0000D\u0000O\u0000O\u0000H\u0000O \u0000'\u0000H\u0000O\u0000D\u0000\\ \u00005\u0000D\u0000Q\u0000G\u0000R\u0000P\n\u0000\u0017\u0000\u0011\u0000\u0013 \u0000\u0017\u0000\u0011\u0000\u0013\n\u0000\u0016\u0000\u0011\u0000\u0013 \u0000\u0016\u0000\u0011\u0000\u0013\n\u0000\u0015\u0000\u0011\u0000\u0013 \u0000\u0015\u0000\u0011\u0000\u0013\n\u0000\u0014\u0000\u0011\u0000\u0013 \u0000\u0014\u0000\u0011\u0000\u0013\n\u0000&\u0000R\u0000K\u0000H\u0000U\u0000H\u0000Q\u0000F\u0000\\ \u00006\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u00001\u0000D\u0000W\u0000X\u0000U\u0000D\u0000O\u0000Q\u0000H\u0000V\u0000V\u0000&\u0000U\u0000H\u0000D\u0000W\u0000L\u0000Y\u0000L\u0000W\u0000\\\u00000\u0000X\u0000V\u0000L\u0000F\u0000D\u0000O\u0000L\u0000W\u0000\\ \u0000,\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000P\u0000H\u0000Q\u0000W\u0000D\u0000W\u0000L\u0000R\u0000Q \u00006\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H \u0000&\u0000U\u0000H\u0000D\u0000W\u0000L\u0000Y\u0000L\u0000W\u0000\\ \u00000\u0000X\u0000V\u0000L\u0000F\u0000D\u0000O\u0000L\u0000W\u0000\\\nFigure5: Subjectiveevaluationresultsonlead Figure6: Subjectiveevaluationresultsonpiano\nsheettomulti-trackarrangement(Section5.4). tomulti-trackarrangement(Section5.5).\nTheevaluationisbasedon5criteria: 1)HarmonyandTextureCoherency,2)Long-TermStructure,\n3)Naturalness,4)Creativity,and5)OverallMusicality.\nAtotalof23participants(8femaleand15male)withdiversemusicalbackgroundshavecompleted\noursurvey,withanaveragecompletiontimeof22minutes. Themeanratingsandstandarderrors,\ncomputedbywithin-subject(repeated-measures)ANOVA[31],arepresentedinFigure5. Significant\ndifferencesareobservedacrossallcriteria(p-valuep<0.05). Notably,ourmethodoutperformsall\nbaselinesoneachcriterion,aligningwiththeresultsfromtheobjectiveevaluation.\n5.5 AblationStudyonStylePriorArchitecture\nWenowvalidateourdesignwiththepriormodelbyexclusivelyevaluatingonthepianotomulti-track\narrangementtask. Ourpriormodelisbasedonauniquelayerinterleavingdesign,whichenables\nmulti-streamtimeseriesmodellingwithexplicitstream-wiseattention. Wecompareitwithtwoother\nmulti-streamarchitectures: 1)Parallel: summingupparallelcodeembeddingsforjointlanguage\nmodelling[18],and2)Delay: leveraginga1-stepdelaycodeinterleavingtocatchimplicitstream-\nwisedependency[4]. BothParallelandDelayaretrainedunderthesamesetupasourmodel. We\nadditionallyintroduce3)Random: anaivepriorbasedonrandomtemplateretrieval. Thetemplates\naresampledevery2barswithsharedinstrumentationfromthevalidation/testsetsofSlakh2100.\nWe introduce two metrics to evaluate piano to multi-track arrangement: faithfulness and degree\nofarrangement(DOA).Faithfulnessmeasuresifthegeneratedarrangementfaithfullyreflectsthe\noriginalcontentfromthepiano.Itcomputesthesimilaritybetweeni)theinputpiano,andii)thepiano\nreductionofthegeneratedmulti-trackarrangement. Inourcase,wecomputecosinesimilarityover\ntwofeatures: astatistical(stats.) pitchclasshistogram[45]andalatenttexturerepresentation[39],\nwhichemphasizetonalandrhythmicsimilarity,respectively. DOAmeasuresthecreativityasdefined\ninSection5.3. WealsoreporttheNLLlossforourmodel,Parallel,andDelay.\nWeconductexperimentsusingthetestsetofPOP909[39],whichconsistsof88pianoarrangement\npieces. Inourexperiment,weusethefirstsectionofeachpiece,whichcontains2to4complete\nphrasestotallyspanning24to32bars. Weusecontroloption3topromptourmodel,Parallel,and\nDelaywiththesame2-barorchestralfunctiontemplate(sampledfromSlakh2100)andseehowit\nisdeveloped. Wereportmeanvalue,standarderrorofmean(sem),andstatisticalsignificancein\nTable3andfindsignificantdifferencesinbothfaithfulnessandDOA.Wealsoconductasubjective\nevaluation in the same setup as Section 5.4, with the results presented in Figure 6. Here we\nconsideranadditionalcriterion,Instrumentation,whichreflectsthewell-formednessofthemulti-\n9\nTable4: Ablationstudyonalternativeleadsheettopianoarrangement(i.e.,Stage1)modules. Here\nweinvestigatetheimpactofStage1totheentiretwo-stagesystem. Evaluationresultsarebasedon\nthefinalmulti-trackarrangementusingrespectiveStage1modules.\nTwo-StageSystem ChordAcc. â†‘ Structureâ†‘ DOAâ†‘\nOurs(Stage1+Stage2) 0.564Â±0.014 1.519Â±0.030 0.300Â±0.005\nWhole-Song-Gen[42]+Stage2 0.509Â±0.015 1.121Â±0.006 0.277Â±0.006\nTable5: EvaluationresultsforleadsheettopianoarrangementexclusivelyonStage1.\nPianoArr. Module ChordAcc. â†‘ Structureâ†‘\nOurs(PianoTexturePrior) 0.540Â±0.016 1.983Â±0.147\nWhole-Song-Gen[42] 0.430Â±0.020 1.153Â±0.180\ntrackarrangement. Significantdifferencesareobservedacrossallcriteria(p-valuep<0.05). Overall,\nParallelandDelaybothfallshortinperformancebecausetheyassumeapresetstreamcombination,\nwhile in our setting, both track numbers and choices of instruments are flexible. By explicitly\nmodellingstream-wiseattention,ourlayerinterleavingdesignfitswelltothatgeneralizedscenario.\n5.6 AblationStudyonPianoArrangement\nNowwevalidateourchoicefortheleadsheettopianoarrangementmoduleonthefirststageof\nourtwo-stagesystem. OurchoiceisapianotexturepriorascoveredinSection4. Weconductan\nablationstudybyreplacingitwiththeWhole-Song-Genmodel[42],which,toourknowledge,isthe\nonlyexistingalternativethatcanhandleawhole-songstructure. Theablationstudyisconductedin\nthesamesetupasSection5.3. InTable4,wereportchordaccuracy,structureawareness,andDOA\nregardingthefinalmulti-trackarrangementresults. Wefurthercompareourpianotexturepriorwith\nWhole-Song-Genexclusivelyonthepianoaccompanimentarrangementtask. InTable5,wereport\nchordaccuracyandstructureawarenessregardingpianoarrangementforbothmodels. Significant\ndifferences(p-valuep<0.05)arefoundinallmetricsbasedonWilcoxonsignedranktest.\nBycomparingTable4andTable5,wecanseethatahigher-qualitypianoarrangementgenerally\nencouragesamoremusicalandcreativefinalmulti-trackarrangementresult. Specifically,thepiano\narrangementonStage1laysthegroundworkfor(atleast)chordprogressionandphrasestructurefor\nStage2,bothofwhichareimportantforcapturingthelong-termstructureinwhole-songmulti-track\narrangement. Moreover,weseethatourpianotextureprioroutperformsexistingalternativesand\nguaranteesadecentpianoquality,thusbeingthebestchoiceforoursystem.\n6 Conclusion\nTosumup,wecontributeamusicautomationsystemformulti-trackaccompanimentarrangement.\nThemainnoveltyliesinourproposedstylepriormodelling,agenericmethodologyforstructured\nsequencegenerationwithfine-grainedcontrol. Bymodellingthepriorofdisentangledstylefactors\ngivencontent,webuildacascadedarrangementprocess: fromleadsheettopianotexturestyle,and\nthenfrompianotoorchestralfunctionstyle.Oursystemfirstgeneratesapianoaccompanimentfroma\nleadsheet,establishingtheroughwhole-songstructure. Itthenorchestratesthepianoaccompaniment\nintoacompletemulti-trackarrangementwithbandinstrumentation. Extensiveexperimentsshowthat\noursystemgeneratesstructured,creative,andnaturalmulti-trackarrangementswithstate-of-the-art\nquality. Atahigherlevel,weelaborateourmethodologyasinterpretablemodularrepresentation\nlearning, which leverages finely disentangled and manipulable music representations to tackle\ncomplextaskswithacompositionalhierarchy. Wehopeourresearchbringsnewperspectivesto\nbroaderdomainsofmusiccreation,sequencedatamodelling,andrepresentationlearning.\nReferences\n[1] SamuelAdlerandPeterHesterman. Thestudyoforchestration,volume2. WWNortonNew\nYork,NY,1989.\n10\n[2] AndreaAgostinelli,TimoIDenk,ZalÃ¡nBorsos,JesseEngel,MauroVerzetti,AntoineCaillon,\nQingqingHuang,ArenJansen,AdamRoberts,MarcoTagliasacchi,etal. Musiclm: Generating\nmusicfromtext. arXivpreprintarXiv:2301.11325,2023.\n[3] CarolineChan,ShiryGinosar,TinghuiZhou,andAlexeiA.Efros. Everybodydancenow. In\n2019IEEE/CVFInternationalConferenceonComputerVision,pages5932â€“5941,2019.\n[4] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi,\nandAlexandreDÃ©fossez. Simpleandcontrollablemusicgeneration. InAdvancesinneural\ninformationprocessingsystems,volume36,2023.\n[5] Shuqi Dai, Huan Zhang, and Roger B Dannenberg. Automatic analysis and influence of\nhierarchical structure on melody, rhythm and harmony in popular music. arXiv preprint\narXiv:2010.07518,2020.\n[6] ZihangDai,ZhilinYang,YimingYang,JaimeG.Carbonell,QuocVietLe,andRuslanSalakhut-\ndinov.Transformer-xl:Attentivelanguagemodelsbeyondafixed-lengthcontext.InProceedings\nofthe57thConferenceoftheAssociationforComputationalLinguistics,pages2978â€“2988,\n2019.\n[7] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya\nSutskever. Jukebox: Agenerativemodelformusic. arXivpreprintarXiv:2005.00341,2020.\n[8] Hao-WenDong,ChrisDonahue,TaylorBerg-Kirkpatrick,andJulianJ.McAuley. Towards\nautomatic instrumentation by learning to separate parts in symbolic multitrack music. In\nProceedings of the 22nd International Society for Music Information Retrieval Conference,\npages159â€“166,2021.\n[9] Hao-Wen Dong, Ke Chen, Shlomo Dubnov, Julian McAuley, and Taylor Berg-Kirkpatrick.\nMultitrackmusictransformer. InInternationalConferenceonAcoustics,SpeechandSignal\nProcessing,pages1â€“5.IEEE,2023.\n[10] Eric Foxley. Nottingham database. [EB/OL], 2011. https://ifdo.ca/~seymour/\nnottingham/nottingham.htmlAccessedMay17,2023.\n[11] CurtisHawthorne,AndriyStasyuk,AdamRoberts,IanSimon,Cheng-ZhiAnnaHuang,Sander\nDieleman, ErichElsen, JesseH.Engel, andDouglasEck. Enablingfactorizedpianomusic\nmodeling and generation with the MAESTRO dataset. In 7th International Conference on\nLearningRepresentations,2019.\n[12] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415,2016.\n[13] AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejinChoi. Thecuriouscaseofneural\ntextdegeneration. In8thInternationalConferenceonLearningRepresentations,2020.\n[14] Wen-YiHsiao,Jen-YuLiu,Yin-ChengYeh,andYi-HsuanYang. Compoundwordtransformer:\nLearningtocomposefull-songmusicoverdynamicdirectedhypergraphs. InProceedingsofthe\nAAAIConferenceonArtificialIntelligence,volume35,pages178â€“186,2021.\n[15] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne,\nNoamShazeer,AndrewM.Dai,MatthewD.Hoffman,MonicaDinculescu,andDouglasEck.\nMusictransformer: Generatingmusicwithlong-termstructure. In7thInternationalConference\nonLearningRepresentations,2019.\n[16] RongjieHuang,YiRen,JinglinLiu,ChenyeCui,andZhouZhao. Generspeech: Towardsstyle\ntransferforgeneralizableout-of-domaintext-to-speech. InAdvancesinneuralinformation\nprocessingsystems,volume35,2022.\n[17] JunyanJiang,KeChen,WeiLi,andGusXia. Large-vocabularychordtranscriptionviachord\nstructuredecomposition. InProceedingsofthe20thInternationalSocietyforMusicInformation\nRetrievalConference,pages644â€“651,2019.\n11\n[18] EugeneKharitonov,AnnLee,AdamPolyak,YossiAdi,JadeCopet,KushalLakhotia,TuAnh\nNguyen,MorganeRiviÃ¨re,AbdelrahmanMohamed,EmmanuelDupoux,andWei-NingHsu.\nText-freeprosody-awaregenerativespoken languagemodeling. InProceedingsofthe60th\nAnnualMeetingoftheAssociationforComputationalLinguistics,pages8666â€“8681,2022.\n[19] DiederikPKingmaandJimmyBa. Adam:Amethodforstochasticoptimization. arXivpreprint\narXiv:1412.6980,2014.\n[20] DmytroKotovenko,ArtsiomSanakoyeu,SabineLang,andBjÃ¶rnOmmer. Contentandstyle\ndisentanglement for artistic style transfer. In 2019 IEEE/CVF International Conference on\nComputerVision,pages4421â€“4430.IEEE,2019.\n[21] LiweiLin,GusXia,QiuqiangKong,andJunyanJiang. Aunifiedmodelforzero-shotmusic\nsourceseparation,transcriptionandsynthesis. InProceedingsofthe22ndInternationalSociety\nforMusicInformationRetrievalConference,pages381â€“388,2021.\n[22] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. In6thInternational\nConferenceonLearningRepresentations,2018.\n[23] Ang Lv, Xu Tan, Peiling Lu, Wei Ye, Shikun Zhang, Jiang Bian, and Rui Yan. Getmusic:\nGenerating any music tracks with a unified representation and diffusion framework. arXiv\npreprintarXiv:2305.10841,2023.\n[24] Xichu Ma, Xiao Liu, Bowen Zhang, and Ye Wang. Robust melody track identification in\nsymbolic music. In Proceedings of the 23rd International Society for Music Information\nRetrievalConference,pages842â€“849,2022.\n[25] EthanManilow,GordonWichern,PremSeetharaman,andJonathanLeRoux. Cuttingmusic\nsourceseparationsomeslakh: Adatasettostudytheimpactoftrainingdataqualityandquantity.\nIn2019IEEEWorkshoponApplicationsofSignalProcessingtoAudioandAcoustics,pages\n45â€“49,2019.\n[26] Lejun Min, Junyan Jiang, Gus Xia, and Jingwei Zhao. Polyffusion: A diffusion model for\npolyphonicscoregenerationwithinternalandexternalcontrols. InProceedingsofthe24th\nInternationalSocietyforMusicInformationRetrievalConference,pages231â€“238,2023.\n[27] MatthiasPlasser,SilvanPeter,andGerhardWidmer. Discretediffusionprobabilisticmodelsfor\nsymbolicmusicgeneration. InProceedingsoftheThirty-SecondInternationalJointConference\nonArtificialIntelligence,pages5842â€“5850,2023.\n[28] ColinRaffel. Learning-BasedMethodsforComparingSequences,withApplicationstoAudio-\nto-MIDIAlignmentandMatching. PhDthesis,ColumbiaUniversity,USA,2016.\n[29] AliRazavi,AaronVandenOord,andOriolVinyals. Generatingdiversehigh-fidelityimages\nwithvq-vae-2. InAdvancesinneuralinformationprocessingsystems,volume32,2019.\n[30] YiRen, JinzhengHe, XuTan, TaoQin, ZhouZhao, andTie-YanLiu. Popmag: Popmusic\naccompaniment generation. In Proceedings of the 28th ACM International Conference on\nMultimedia,pages1198â€“1206,2020.\n[31] HenryScheffe. Theanalysisofvariance,volume72. JohnWiley&Sons,1999.\n[32] HaoHaoTanandDorienHerremans. Musicfadernets: Controllablemusicgenerationbasedon\nhigh-levelfeaturesvialow-levelfeaturemodelling. InProceedingsofthe21stInternational\nSocietyforMusicInformationRetrievalConference,pages109â€“116,2020.\n[33] JohnThickstun,DavidLeoWrightHall,ChrisDonahue,andPercyLiang. Anticipatorymusic\ntransformer. TransactionsonMachineLearningResearch,2024.\n[34] AaronVanDenOord,OriolVinyals,etal. Neuraldiscreterepresentationlearning. InAdvances\ninneuralinformationprocessingsystems,volume30,2017.\n[35] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,\nLukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.InAdvancesinneuralinformation\nprocessingsystems,volume30,2017.\n12\n[36] DimitrivonRÃ¼tte,LucaBiggio,YannicKilcher,andThomasHoffman. Figaro: Generating\nsymbolicmusicwithfine-grainedartisticcontrol. In11thInternationalConferenceonLearning\nRepresentations,2023.\n[37] ZiyuWangandGusXia. Musebert: Pre-trainingmusicrepresentationformusicunderstanding\nand controllable generation. In Proceedings of the 22nd International Society for Music\nInformationRetrievalConference,pages722â€“729,2021.\n[38] ZiyuWang,KeChen,JunyanJiang,YiyiZhang,MaoranXu,ShuqiDai,andGusXia. POP909:\nApop-songdatasetformusicarrangementgeneration. InProceedingsofthe21stInternational\nSocietyforMusicInformationRetrievalConference,pages38â€“45,2020.\n[39] ZiyuWang,DingsuWang,YixiaoZhang,andGusXia. Learninginterpretablerepresentation\nforcontrollablepolyphonicmusicgeneration. InProceedingsofthe21stInternationalSociety\nforMusicInformationRetrievalConference,pages662â€“669,2020.\n[40] ZiyuWang,YiyiZhang,YixiaoZhang,JunyanJiang,RuihanYang,GusXia,andJunboZhao.\nPIANOTREEVAE:Structuredrepresentationlearningforpolyphonicmusic. InProceedings\nofthe21stInternationalSocietyforMusicInformationRetrievalConference,pages368â€“375,\n2020.\n[41] ZiyuWang,DejingXu,GusXia,andYingShan. Audio-to-symbolicarrangementviacross-\nmodalmusicrepresentationlearning. InInternationalConferenceonAcoustics,Speechand\nSignalProcessing,pages181â€“185.IEEE,2022.\n[42] ZiyuWang,LejunMin,andGusXia. Whole-songhierarchicalgenerationofsymbolicmusic\nusingcascadeddiffusionmodels.In12thInternationalConferenceonLearningRepresentations,\n2024.\n[43] FrankWilcoxon. Individualcomparisonsbyrankingmethods. InBreakthroughsinstatistics:\nMethodologyanddistribution,pages196â€“202.Springer,1992.\n[44] ShangdaWu,DingyaoYu,XuTan,andMaosongSun. Clamp: Contrastivelanguage-music\npre-trainingforcross-modalsymbolicmusicinformationretrieval. InProceedingsofthe24th\nInternationalSocietyforMusicInformationRetrievalConference,pages157â€“165,2023.\n[45] Shih-Lun Wu and Yi-Hsuan Yang. The jazz transformer on the front line: Exploring the\nshortcomingsofai-composedmusicthroughquantitativemeasures. InProceedingsofthe21st\nInternationalSocietyforMusicInformationRetrievalConference,pages142â€“149,2020.\n[46] RuihanYang,DingsuWang,ZiyuWang,TianyaoChen,JunyanJiang,andGusXia.Deepmusic\nanalogyvialatentrepresentationdisentanglement. InProceedingsofthe20thInternational\nSocietyforMusicInformationRetrievalConference,pages596â€“603,2019.\n[47] LiYi,HaochenHu,JingweiZhao,andGusXia. Accomontage2: Acompleteharmonization\nandaccompanimentarrangementsystem. InProceedingsofthe23rdInternationalSocietyfor\nMusicInformationRetrievalConference,pages248â€“255,2022.\n[48] WenjieYin,HangYin,KimBaraka,DanicaKragic,andMÃ¥rtenBjÃ¶rkman. Dancestyletransfer\nwithcross-modaltransformer. InIEEE/CVFWinterConferenceonApplicationsofComputer\nVision,pages5047â€“5056,2023.\n[49] SiyangYuan,PengyuCheng,RuiyiZhang,WeituoHao,ZheGan,andLawrenceCarin. Improv-\ningzero-shotvoicestyletransferviadisentangledrepresentationlearning. In9thInternational\nConferenceonLearningRepresentations,2021.\n[50] JingweiZhaoandGusXia. Accomontage: Accompanimentarrangementviaphraseselection\nand style transfer. In Proceedings of the 22nd International Society for Music Information\nRetrievalConference,pages833â€“840,2021.\n[51] JingweiZhao,GusXia,andYeWang. Q&A:Query-basedrepresentationlearningformulti-\ntracksymbolicmusicre-arrangement. InProceedingsoftheThirty-SecondInternationalJoint\nConferenceonArtificialIntelligence,pages5878â€“5886,2023.\n13\nA ImplementationDetails\nA.1 Autoencoder\nTheautoencoderconsistsofaVQ-VAEsubmoduleandanoverarchingVAE.TheencoderoftheVQ-\nVAEconsistsofa1-Dconvolutionallayerofkernalsize4,stride4,and16outputchannels,followed\nbyavectorquantizationblockwithcodebooksize128. Thedecodertakestheconcatenatedlatent\ncodesandleveragestwofully-connectedlayers(shape128Ã—256and256Ã—32)forreconstruction. In\ntheoverarchingVAE,PianoEncoderandTrackDecoderareadaptedfromPianoTreeVAE[40]. The\nencoderfirstappliesapitch-wisebi-directionalGRUtosummarizeconcurrentnotesattimestepn\nandthenappliesatime-wiseGRUtoencodethefullrepresentation. Thedecodermirrorstheencoder\nstructurewithtime-andpitch-wiseuni-directionalGRUstoreconstructindividualtracks. Weuse\nhiddensize256inasinglelayerforpitchGRUsand512fortimeGRUs. TheTrackSeparatorisa\n2-layerTransformerencoderwith8attentionheads,0.1dropoutratio,andGELUactivation[12]. The\nhiddendimensionsofself-attentiond andfeed-forwardlayersd are512and1024,respectively.\nmodel ff\nTheautoencoderistrainedwithjointreconstructionlossfororchestralfunction(MSE)andindividual\ntracks(crossentropy). TheVQ-VAEisadditionallyregularizedwithlatentlossandcommitmentloss\nwithcommitmentratioÎ² =0.25. TheVAEisregularizedwithKLlossoverallcontinuousfactors\n(c andz1:K)basedonKLannealing[41]witharatioexponentiallyincreasingfrom0to0.5.\nt t\nA.2 PriorModel\nThepriormodelconsistsofa12-layerContextEncoderanda12-layerAuto-RegressiveDecoder.\nThelatterisinterleavedwithanother12track-wiseTrackEncoderlayers. Foreachlayer,weapply\n8 attention heads, 0.1 dropout ratio, and GELU activation. We apply layer normalization before\nself-attention (i.e., norm first). The hidden dimensions of self-attention d and feed-forward\nmodel\nlayers d are 256 and 1024, respectively. We apply relative positional embedding [15] to Track\nff\nEncodersothattwotracksinitializedwithidenticalinstrumentscanstillgeneratedifferentcontent.\nOurpriormodelistrainedonthelatentcodesc ands1:K inferredbyawell-trainedautoencoder\n1:T 1:8T\nonLMD.Fordiscretecodes,wetakethecodebookindicesandlearnanewembedding.\nB ObjectiveEvaluationMetrics\nB.1 DegreeofArrangement\nInmulti-trackarrangement,paralleltrackstypicallyplayauniqueroletoeachotherintheoverall\narrangement. Weareinterestedincapturingthediversityandcreativityinherentintheseroles.\nToachievethis,weconsiderthepitchclasshistogram[45]asaprobabilitydistributionP. LetP\nt,k\nbethedistributionofthet-thbarintrackk,andPpn bethatofthet-thbarinthepianoreduction.\nt\nRecallthatinthispaperweapproximatethepianoreductionofamulti-trackpiecebydownmixingall\ntracks. BothP andPpnare12-Dvectors,describingtonalityofindividualtracksandtheoverall\nt,k t\narrangement,respectively. WecomputetheKLdivergenceofeachtracktothepianoreduction:\nT\n1 (cid:88)\nd = KL(P âˆ¥Ppn), (5)\nk T t,k t\nt=1\nwhereT isthetotalnumberofbars.\nInterpreting d in terms of KL divergence, we see it as the â€œexcess surpriseâ€ from the overall\nk\narrangement(pianoreduction)whentrackkisplayedinisolation. Alarged indicatesthattrackk\nk\npossessesauniquequality,suchasabasstrackplayingtherootoracounter-melodytrackfocusing\nontensions. Conversely, asmalld suggeststhattrackk servesasafoundationalelementinthe\nk\narrangement,suchasstringpaddingthatestablishestheharmonicfoundation.\nIfalld valuesaresmall,itimplieshomogeneityacrosstracksandthusalowdegreeofarrangement.\nk\nConversely,ifalld valuesarehigh,itsuggestsacompositiondominatedbycounterpoints,ascenario\nk\nless common in pop music. A well-orchestrated piece typically exhibits a diverse range of d\nk\nvalues,encompassingbothfoundationalanduniquedecorativetracks. Wethusdefinethedegreeof\n14\narrangementDOAasthestandarddeviationofd fork =1,2,Â·Â·Â· ,K acrossalltracks:\nk\n(cid:115)\n(cid:80)K (d âˆ’d)2\nDOA= k=1 k , (6)\nK\nwheredisthemean. K isthetotalnumberoftracks. Toestablishareferencepoint,wecalculatethe\nground-truthDOA=0.333basedon1000randomlyselectedpiecesfromtheLMDdataset. Within\nthiscontext,ahigherDOAsignifiesamorecreativearrangement.\nB.2 StrctureAwareness\nWeintroduceInter-phraseLatentSimilarity(ILS)from[42]tomeasurethestructuralawarenessof\nlong-termarrangement. ILScalculatesthecontentsimilarityamongsame-typephrases(e.g.,chorus)\nversusthewholesong. Itleveragespre-traineddisentangledVAEsthatencodemusicnotesintolatent\nrepresentationsandthencomparecosinesimilaritiesinthelatentspace. Inourcase,wecompute\nILSoverthepianoreductionofageneratedarrangementsinceitcontainstheoverallcontent. We\napplythetextureVAE[39]andobtainalatenttexturerepresentationctxt forevery2-barsegment.\nt\nForodd-numberedphrases,werepeatitsfinalbarandpadittotheendofthephrase. Supposethere\nareM differenttypesofphrasesinonepieceandletI bethesetofsegmentindicesinthetype-m\nm\nphrase,ILSisdefinedastheratiobetweensame-typephrasesimilarityandglobalaveragesimilarity:\n((cid:80)M (cid:80) cos(ctxt,ctxt))/((cid:80)M |I |2âˆ’|I |)\nILS= m=1 iÌ¸=jâˆˆIm i j m=1 m m , (7)\n(cid:80) cos(ctxt,ctxt)/(T2âˆ’T)\n1â‰¤iÌ¸=jâ‰¤T i j\nwhere|Â·|isthecardinalityofaset. T isthenumberof2-barsegments. WhenapplyingILS,we\nuse [5] to automatically lable the phrase structure of a piece. To establish a reference point, we\ncalculatetheground-truthILS = 1.980basedonthePOP909dataset(withphraseannotationby\nhuman). Withinthiscontext,ahigherILSsignifiessaliencywithlong-termphrase-levelstructure.\nB.3 ChordAccuracy\nWeintroducechordaccuracyfrom[30]tomeasureifthechordsofthegeneratedarrangementmatch\ntheconditionalchordsequenceintheleadsheet. Itreflectstheharmonicityofthegeneratedmusic\nandisdefinedasfollows:\n1 N (cid:88)chord\nCA= 1 , (8)\nN\nchord\n{Ci=CË† i}\ni=1\nwhereN isthenumberofchordsinapiece;C isthei-thchordinthe(ground-truth)leadsheet;\nchord i\nandCË† isthealignedchordinthegeneratedarrangement.\ni\nTheoriginalformulationin[30]considerschordaccuracyforindividualtracks. Givenoursystemâ€™s\ncapabilitytoaccommodateavariablecombinationoftracks,weoptforabroaderevaluationforthe\noverallarrangement. Inourcase,weextractthechordsequenceofageneratedarrangementwith[17]\nandcompareitinrootandqualitywithground-truthat1-beatgranularity,whichismorerigorous.\nB.4 OrchestrationFaithfulness\nWemeasurethefaithfulnessoforchestrationbythesimilaritybetweeni)theinputpianoandii)the\npianoreductionofthegeneratedmulti-trackarrangement. Leteinandepnbevectorfeaturesderived\nt t\nfromthet-thsegmentoftheinputandthereduction,respectively. OrchestrationfaithfulnessOFis\ndefinedasfollows:\nT\n1 (cid:88)\nOF= cos(ein,epn), (9)\nT t t\nt=1\nwherecos(Â·,Â·)iscosinesimilarity. T isthenumberofsegments.\nInourwork,weselecttwooptionsforvectorfeaturee. Oneisastatisticalpitchclasshistogram[45],\nwhich is a 12-D vector describing pitch class distribution. The other is a latent 256-D texture\nrepresentationlearnedbyapre-trainedVAE[39]. Bothfeaturesaregeneraldescriptorsofthemusical\ncontentwithrespectivefocusontonalharmonyandrhythmicgrooves.\n15\nC ExperimentonNoiseWeightÎ³\nContinuing from Section 3.3, we compare different Î³ values and see their impact to the model\nperformance. Whenapplyingourmodeltopianotomulti-trackarrangement,Î³ balancestheforce\nofanoisyfactoraddedtothepiano,whichencouragesapartialunconditionalgenerationcapability.\nTheexperimentalsettingsarethesameasSection5.5. Weevaluatetheresultsbasedonfaithfulness\nandDOA.InTable6,wereportmeanvalue,standarderrorofmean(sem),andstatisticalsignificance\ncomputedbyWilcoxonsignedranktest. ByvaryingtheÎ³ value,weobserveacontrollablebalance\nbetweenfaithfulnessandcreativity. Specifically,alargerÎ³ encouragescreativity(higherDOA)atthe\ncostoffaithfulness. Ifnotmentionedotherwise,weuseÎ³ =0.25forexperimentsinthispaper.\nTable6: ObjectiveevaluationresultsontheimpactofnoiseweightÎ³ inAppendixC.\nNoiseWeightÎ³ Faithfulness(stats.) â†‘ Faithfulness(latent)â†‘ DOAâ†‘\nÎ³ =0 0.946Â±0.001a 0.228Â±0.005a 0.300Â±0.005c\nÎ³ =0.25 0.945Â±0.001ab 0.215Â±0.005b 0.308Â±0.005bc\nÎ³ =0.5 0.944Â±0.001b 0.187Â±0.004c 0.320Â±0.006ab\nÎ³ =1 0.936Â±0.002c 0.127Â±0.003d 0.325Â±0.007a\nD OnlineSurveySpecifics\nWedistributeoursurveyviaSurveyMonkey.4 Oursurveyconsistsof5samplesetsforboththelead\nsheettomulti-trackandthepianotomulti-trackarrangementtasks(10setsintotal). Eachsampleis\n24-32barslongandissynthesizedtoaudioat90BPMusingBandLab5withthedefaultsoundfont.\nEachparticipantlistensto2sets(inrandomorder)andthemeantimespentis22minutes. Figure7\nshowsthesamplepagesofoursurveywithinstructionstotheparticipants.\n(a)Leadsheettomulti-trackarrangement. (b)Pianotomulti-trackarrangement.\nFigure7: Screenshotsofsurveypagesandinstructionsofouronlinesurvey.\n4https://www.surveymonkey.com\n5https://www.bandlab.com/\n16\nE ExampleonStructuredArrangement\nWedemonstrateanexampleofaccompanimentarrangementbyourproposedsystem. Theinputlead\nsheetisacompletepopsongshowninSectionE.1. Oursystemfirstarrangesapianoaccompaniment\nfor the whole song, which is shown in Section E.2. The piano score is then orchestrated into a\nmulti-trackarrangementwithcustomizedinstrumentation,whichisshowninSectionE.3.\nE.1 LeadSheet\nWeuseoursystemtoarrangeforCanYouFeeltheLoveTonight,apopsongbyEltonJohn. Asshown\nin Figure 8, the entire song is 60 bars long and it presents a structure of i4A8B8B8x4A8B8B8O4,\nwherei,x,O,A,andBeachrefertointro,interlude,outro,verse,andchorus.\nî²¥\ni4 = 65\nBâ™­ F/A Eâ™­/G Bâ™­/F Eâ™­ Bâ™­/D F/A Bâ™­ Cm7 Bâ™­/D\nî‰  î‚„\nî‰  î“¤ î“¤ î“¤ î“¤ î“¤ î“¤ î“¤ î“¤\nî î‚„\nA8\n5 Eâ™­ Bâ™­/D Eâ™­ Bâ™­/D Eâ™­ Bâ™­/D Cm7 F/A\nî î‰ î‰  î„ î„ î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤î‡§ î“¥ î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤ î“¥ î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤ î“¥\n9 Eâ™­ Bâ™­/D Eâ™­ Bâ™­/D Eâ™­ Gm Aâ™­ F\nî B8î‰ î‰  î‚¤ î‚¤î‚¤ î‚¤î‚¤î‡§ î‚¤ î‚¤î‡§ î“¥ î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤ î“¥ î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“¦î‡§ î‚¤î‰‚\n13 Bâ™­ F/A Gm Eâ™­ Bâ™­ Eâ™­ C7/E F\nî î‰ î‰  î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î“¥ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¤\n17 Eâ™­ Bâ™­/D Gm Eâ™­ Cm7 Bâ™­/D Cm/Eâ™­ C/E F\nBî8î‰ î‰  î“¥ î‚¤î‚¤î‚¤ î‚¤ î‚¤\nî‚¤î‚¤\nî‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤\nî‚¤\nî“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¥ î“¦î‡§ î‚¤î‰‚\n21 Bâ™­ F/A Gm Eâ™­ Bâ™­ Eâ™­ C7/E F\nî‰ î‰  î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¥ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¤\nî î‚¤ î‚¤ î‚¤ î‚¤\n25 Eâ™­ Bâ™­/D Gm Eâ™­ Cm Bâ™­/D Cm/Eâ™­ F7sus Eâ™­/Bâ™­ Bâ™­\nxî4î‰ î‰  î“¥ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚£ î“¤\n29 1.Bâ™­ F/A Eâ™­/G Bâ™­/F Eâ™­ Bâ™­/D F/A Bâ™­ Cm7 Bâ™­/D\nî‰ \nî î‰  î“¤ î“¤ î“¤ î“¤ î“¤ î“¤ î“¤ î“¤ î„ î„\nO4\n33 2.Eâ™­ Bâ™­/D Gm Eâ™­ Cm Bâ™­/D Cm/Eâ™­ F7sus Eâ™­/Bâ™­ Bâ™­\nî î‰ î‰  î“¥ î‚¤î‚¤ Fî‚¤ igî‚¤ ureî‚¤ 8:î‚¤Lî‚¤eadî‚¤ sheî‚¤ etî‚¤ forî‚¤ poî‚¤ pî‚¤ sî‚¤ongî‚¤î‡§ Caî‚¤nYî‚¤ ouFeeî‚¤ ltheLî‚¤ oveToî‚¤ nigî‚¤ hî‡§ t. î‚£ î“¤\n17\nE.2 PianoArrangement\nThepianoarrangementresultisshownfromFigure9toFigure10.Itroughlyestablishesawhole-song\nstructureandlaysthegroundworkforbandorchestrationatthenextstage. Demoaudioforthepiano\nscoreisavailableathttps://zhaojw1998.github.io/structured-arrangement/.\nî²¥\ni4 = 65\nBâ™­ F/A Eâ™­/G Bâ™­/F Eâ™­ Bâ™­/D F/A Bâ™­ Cm7 Bâ™­/D\nMelody\nî î‰ î‰  î‚„ î‚„ î“¤ î“¤ î“¤ î“¤ î“¤ î“¤ î“¤ î“¤\nPiano îî‰ î‰ î‚„ î‚„ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‡§î‡§ î‚¤ î‚¤î‚¤î‚¤ î‡§î‡§ î‚¤î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‚¤î‚¤ î‚¤î‡§ î‚¤î‚¤î‚¤î“§ î‡§î‡§î‡§î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤î‚¤ î‚¤î‡§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‡§î‡§ î‚¤î‚¤î‡§ î‚¤î‚¤ î‡§ î‚¤î“¥ î‚¤ î‚¤\nî‚¤\nî¢ î‰ î‚„ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤\nA8î‰  î‚„ î‚¢ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î‚¢\n5 Eâ™­ Bâ™­/D Eâ™­ î‚¢ Bâ™­/D Eâ™­ Bâ™­/D î‰ Cm7 F/A\nMel.\nî€€î î‰ î‰  î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¥\nPno. îî‰ î‰  î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤î“¥ î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤\n9î¢ î‰ î‰  î‚¤î‚¤î‚¤\nî‚£\nEâ™­\nî‚¤î‚¤ î‚¤ î‚£î‚¤î‚¤\nBâ™­/D\nî‚¤ î‚¤ î‚£î‚£\nEâ™­\nî‚£\nî‚£\nBâ™­/D\nEî‚¤î‚¤\nî‚¤ î‚£\nâ™­\nî‚¤î‚¤ Gî‚¤ î‚£î‚¤î‚¤\nm\nî‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤\nî‚£\nAâ™­\nî‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤î‚¤\nî‚£\nF\nî‚¤ î‚¤î‚¤î‚¤\nMel.\nî€€ î î‰ î‰  î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î‚¤î‡§ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¦î‡§ î‚¤î‰‚\nî‰  î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤\nPno. îî‰  î‚¤ î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤\n1î¢ 3Bî‰ 8î‰  î‚£î‚¤î‚¤î‚¤ Bâ™­ î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚£F/A î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤ Gmî‚£î‚£ Eâ™­î‚£ î‚£ î‚¤î‚¤ î‚¤ î‚£ Bâ™­ î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î‚£ Eâ™­ î‚¤î‚¤ î‚¤ Cî‚¤ 7/Eî‚¤ î‰  î‰ î‚¤î‚¤î‚¤ î‚£ F î‚¤î‚¤ î‚¤î‚¤ î‚£î“¦ î‚¤î‰€ î‚¤\nMel.\nî€€ î î‰ î‰  î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î“¥ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¤\nPno. îî‰ î‰  î“£ î“¥\nî‚£î‚£î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î‚¤\nî“¥\nî‚£î‚£ î‚¤î‚¤\nî“¦\nî‚¤ î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î‚¤\n1î¢ 7 î‰ î‰  î‚£î“¦ Eâ™­ î‚¤ î‚¤ î‚¤î‚¤ î‚¤ Bî‚£î“¦ â™­/D î‚¤ î‚¤î‚¤ î‚¤ î‚£î“¦ Gmî‚¤ î‚¤ î‚¤ Eî‚£î“¦ â™­ î‚¤ î‚¤ î‚¤ î‚£î“¦ Cm7î‚¤ î‚¤ Bâ™­/D î‚¤ Cî‚£î“¦ m/Eâ™­î‚¤ C/î‚¤ E î‚¤ Fî‚£î“¦ î‚¤ î‚¤ î‚¤ î‚£î“¦ î‚¤ î‚¤ î‚¤\nMel.\nî€€ î î‰ î‰  î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¥ î“¦î‡§ î‚¤î‰‚\nî‰ \nPno. îî‰  î‚¤ î‚£î‚£ î‚¤î‚¤ î“¥ î‚£î‚£ î‚¤ î“¥ î‚£ î‚¤ î“¦ î‚¤ î‚£ î‚£î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤\nî‰€\n2î¢ 1Bî‰ î‰ 8 î‚£î“¦\nBâ™­\nî‚¤ î‚¤ î‚¤ î‚£î“¦\nF/A\nî‚¤ î‚¤ î‚¤ Gmî‚£î“¦ î‚¤ î‚¤ î‚¤ î‚£î“¦\nEâ™­\nî‚¤ î‚¤ î‚¤ Bî‚£î“¦\nâ™­\nî‚¤ î‚¤\nî‚¤\nî‚£î“¦\nEâ™­\nî‚¤ î‚¤ C7/Eî‚¤ î‚£î“¦\nF\nî‚¤ î‚¤ î‚¤ î‚£î“¦ î‚¤ î‚¤ î‚¤ î‚¤\nMel.\nî î‰ î‰  î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¥ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¤\nî€€\nî‰ \nPno. îî‰  î‚¤ î‚£ î‚£ î‚¤ î“¦ î‚¤ î‚¤ î‚£î‚£ î‚¤ î‚¤ î‚¤ î“¥ î‚£î‚£ î‚¤ î‚¤ î“¦ î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î‚¤\n2î¢ 5î‰ î‰  î‚£î“¦ Eâ™­ î‚¤ î‚¤ î‚¤ Bî‚£î“¦ â™­/Dî‚¤ î‚¤ î‚¤ î‚£î“¦ Gmî‚¤ î‚¤ î‚¤ Eî‚£î“¦ â™­ î‚¤ î‚¤ î‚¤ î‚£î“¦ Cmî‚¤ î‚¤ Bâ™­/D î‚¤ Cî‚£î“¦ m/Eâ™­î‚¤ F7suî‚¤ s î‚¤ Eâ™­/Bî‚£î“¦ â™­ î‚¤ î‚¤ î‚¤ Bâ™­ î‚£î“¦ î‚¤ î‚¤ î‚¤\nMel.\nî€€ î î‰ î‰  î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚£ î“¤\nî‰ \nPno. îî‰  î‚¤ î‚£î‚£ î‚¤î‚¤ î“¥ î‚¤î‚¤î‡§î‡§ î‚¤ î‚¤î‚¤ î‚¤ î‚¤ î‚£ î‚¤î‚¤ î“¥ î‚¤î‚¤î‡§î‡§ î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‚¤ î‚¤\nî¢ î‰ î‰  î‚£î“¦ î‚¤ î‚¤ î‚¤ î‚£î“¦ î‚¤ î‚¤ î‚¤ î‚£î“¦ î‚¤ î‚¤ î‚¤ î‚£î“¦ î‚¤ î‚¤ î‚¤ î‚£î“¦ î‚¤ î‚¤\nî‚¤\nî‚£î“¥ î‚¤ î‚¤ î‚£î“¦ î‚¤ î‚¤ î‚¤ î‚£î“¦ î‚¤î‰€ î‚¤ î‚¤ î‚¤ î‚¤\nFigure9: Pianoarrangementscore(page1).\nî€€\n18\nx4\n29 Bâ™­ F/A Eâ™­/G Bâ™­/F Eâ™­ Bâ™­/D F/A Bâ™­ Cm7 Bâ™­/D\nMel.\nî î‰ î‰  î“¤ î“¤ î“¤ î“¤ î“¤ î“¤ î“¤ î“¤\nPno. îî‰ î‰  î‚£î‚£ î“¥ î“§î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤ î“¥ î“§î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤ î“¦î‡§ î‰ƒî‚¤î‚¤î‚¤î‚¤î‚¤ î‚£ î“¤\nî¢ Aî‰ î‰ \n8\nî“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤î“§ î‚¤î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§ î‚£î‚¤î‚¤î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î“§ î‚£î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î“§ î‚£î‚¤î‚¤ î‚¤ î‚¤î‚¤ î“¦\nî‚£\nî‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤\n33 î‚£Eâ™­ Bâ™­î‚£/D Eâ™­ Bâ™­î‚£/D Eâ™­ Bâ™­/D Cm7 F/Aî‚£\nMel.\nî€€ î î‰ î‰  î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î‚¤î‡§ î“¥ î‚¤ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î“¥ î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“¥\nPno. îî‰ î‰  î‚¤ î‚¤ î“£ î‚¤ î‚¤ î“¥î‡§ î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚¤ î“£ î‚¤ î‚¤ î“§î‚¤ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤î“§î‚¤ î‚¤î‚¤î‚¤î‚¤î‡§ î‚¤\nî¢ î‰ î‰  î“§ î‚¤î‚¤î‚¤ î‚¤ î‚¤î“§ î‚£î‚¤î‚¤î‚¤ î‚¤ î‚¤ î“§ î‚£î‚¤î‚¤î‚¤ î‚¤ î‚¤î“§ î‚£î‚¤î‚¤î‚¤ î‚¤ î‚¤ î“§ î‚¤î‚¤î‚¤ î‚¤ î‚¤î“§ î‚£î‚¤î‚¤î‚¤ î‚¤ î‚¤ î“§ î‚£ î‚¤î‚¤î‚¤ î“§î‚¤î‚¤ î‚¤ î‚¤\n37 î‚£ Eâ™­ Bâ™­/D Eâ™­ Bâ™­/D Eî‚£ â™­ Gm Aâ™­ î‚£ F\nMel.\nî€€ î î‰ î‰  î‚¤ î‚¤î‚¤î‚¤î‚¤î‡§ î‚¤î‚¤î‡§ î“¥ î‚¤ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î“¥ î‚¤ î‚¤î‚¤î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‡§ î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“¦î‡§ î‚¤î‰‚\nPno. îî‰ î‰  î‚¤ î‚¤î“£ î‚¤ î‚¤ î‚¤ î‚¤î“£ î‚¤î‚¤ î‚¤ î‚¤î‚¤ î‚¤î‚¤î“£ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‰  î‚¤î‚¤î“£ î‚¤\n4î¢ 1Bî‰ 8î‰  î“§\nî‚£\nBâ™­î‚¤î‚¤î‚¤ î‚¤ î‚¤ Fî“§\nî‚£\n/Aî‚¤î‚¤î‚¤ î‚¤ î‚¤ Gmî“§ î‚¤î‚¤î‚¤ î“§ î‚¤î‚¤î‚¤ Eâ™­î“§ î‚£î‚¤î‚¤ î‚¤ î‚¤ î‚¤ Bâ™­î“§\nî‚¤î‚¤\nî‚¤î“§ î‚¤î‚¤ î‚¤ Eî“§\nî‚¤\nâ™­î‚¤ î‚¤ î“§\nî‚¤\nC7î‚¤\n/E\nî‚¤ î‰ î“§\nî‚¤\nFî‰ î‚¤ î‚¤î“§ î‚¤î‚¤ î‚¤ î‰¡î“§ î‚£î‚¤î‰¡î‚¤î‚¤ î‚¤î‚¤\nMel.\nî€€ î î‰ î‰  î‚¤î‡§ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î“¥ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¤\nî‰ \nPno. îî‰  î‚¢ î“¤ î“¦ î‚¤ î‚¤ î‚¤ î‚£ î“¤ î“¤ î“¦ î‚¤î‚¤î‚¤ î‚¤\n4î¢ 5î‰ î‰  î“§ î‚£Eâ™­î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î“§ î‚£Bî‚¤ â™­/Dî‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î“§ î‚£î‚¤Gmî‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤î“§ î‚£ Eî‚¤ â™­î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§ î‚£Cî‚¤ m7î‚¤î‚¤î‚¤ Bâ™­î‚¤ /Dî‚¤î‚¤î“§ î‚£ Cmî‚¤ /Eî‚¤\nâ™­\nî‚¤î‚¤ C/Eî‚¤î‚¤î‚¤ Fî“§ î‚£î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î“§ î‚£î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤\nMel.\nî€€ î î‰ î‰  î“¥ î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¥ î“¦î‡§ î‚¤î‰‚\nPno. îî‰ î‰  î‚£ î‚¤ î“¤ î‚¤ î“£ î‚£ î‚¤ î“¤ î‚¤ î“¤ î‚¤î‚¤ î‚¤î‚¤î‚¤î‚¤î‚¤ î‚¤\n4î¢ 9Bî‰ î‰ 8 î“§ î‚£ Bâ™­î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î“§ î‚£F/Aî‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î“§ î‚£Gî‚¤mî‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§ î‚£Eî‚¤â™­ î‚¤î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§ î‚£Bî‚¤ â™­î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§ î‚£ Eî‚¤ â™­î‚¤î‚¤ î‚¤ C7î‚¤ /Eî‚¤ î“§ î‚£î‚¤ Fî‚¤î‚¤î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î‚¤ î“§ î‚£î‚¤î‚¤î‚¤î“§î‚¤î‚¤î‚¤\nMel.\nî€€ î î‰ î‰  î‚¤î‡§ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î‚¤î‚¤ î“¥ î“¥ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¤\nî‰ \nPno. îî‰ \nî‚£\nî“¤ î“¤ î“¦\nî‚¤î‚¤î‚¤ î‚¤ î‚£\nî“¤ î“¤ î“¦\nî‚¤î‚¤î‚¤ î‚¤\n5î¢ 3 î‰ î‰  î“§ î‚£Eâ™­î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î“§ î‚£Bâ™­î‰¡ /Dî‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ Gî“§ î‚£mî‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤î“§ î‚£ Eâ™­î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§ î‚£Cmî‚¤î‚¤î‚¤î‚¤ Bî‚¤ â™­/Dî‚¤î‚¤î“§ î‚£ Cî‚¤ m/î‚¤ Eâ™­î‚¤î‚¤ Fî‚¤ 7suî‚¤ sî‚¤ î“§ î‚£Eî‚¤ â™­/Bî‚¤ â™­î‚¤î‚¤ î‚¤î‚¤î‚¤î“§ î‚£î‚¤ Bâ™­î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤\nMel.\nî€€ î î‰ î‰  î“¥ î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‡§ î‚£ î“¤\nPno. îî‰ î‰ \nî‚£ î‚¤\nî“¤\nî‚¤\nî“£\nî‚¤\nî“£\nî‚¤\nî“¦\nî‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤\n5î¢ 7Oî‰ î‰ 4 î“§ î‚£ Eâ™­î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î“§ î‚£Bâ™­î‚¤ /Dî‚¤î‚¤ î‚¤î‚¤ î‚¤ î“§ î‚£Gî‚¤mî‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤î“§ î‚£Eî‚¤ â™­î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§ î‚£î‚¤ Cmî‚¤î‚¤ Bî‚¤ â™­/Dî‚¤î‚¤ Cî“§ î‚£ m/î‚¤ Eâ™­î‚¤î‚¤ F7sî‚¤ usî‚¤î‚¤ Eâ™­/Bî“§ î‚£ â™­î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ Bâ™­î‚¤î“¦ î‚£ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤\nMel.\nî€€ î î‰ î‰  î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚£ î“¤\nPno. îî‰ î‰  î“£ î“£ î“£\nî‚¤\nî“£\nî‚¤\nî¢ î‰ î‰  î“§ î‚£î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î“§ î‚£î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§ î‚£î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î“§ î‚£î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§ î‚£î‚¤î‚¤ î‚¤ î‚¤î‡§ î“§ î‚£î‚¤î‚¤î‚¤î‚¤ î“§ î‚£î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î“§ î‚£î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤\nFigure10: Pianoarrangementscore(page2,lastpage).\nî€€\n19\nE.3 Multi-TrackArrangement\nThemulti-trackarrangementisshownfromFigure11toFigure15. Wecustomizetheinstrumentation\nascelesta,acousticguitars(2),electricpianos(2),acousticpiano,violin,brass,andelectricbassinato-\ntalofK =9tracks. Wecanseethatthestructureoftheaccompanimentfollowstheleadsheet. Demo\naudioisavailableathttps://zhaojw1998.github.io/structured-arrangement/. Morede-\ntailedanalysisonthisarrangementdemoiscoveredinSection4.\ni4 î²¥ = 65\nBâ™­ F/A Eâ™­/G Bâ™­/F Eâ™­ Bâ™­/D F/A Bâ™­ Cm7 Bâ™­/D\nMelody\nî î‰ î‰  î‚„ î‚„ î“¤ î“¤ î“¤ î“¤ î“¤ î“¤ î“¤ î“¤\nCelesta\nîî‰ î‰ î‚„ î‚„ î‚¤ î“¦ î‚¤î‚¤î‚¤ î‚¤î‚¤ î“¦ î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“¦ î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“£\nAcoustic Guitar 1\nî’î‰ î‰ î‚„\nî‚„\nî“£ î“£ î“£ î“¦î‡§ î‚¤î‚¤î‚¤î‚¤î‚¤î‚¤ î‚¤\nî‰ƒ\nAcoustic Guitar 2\nî’î‰ î‰ î‚„\nî‚„\nî“£ î“£ î“£ î“£\nPiano îî‰ î‰ î‚„ î‚„ î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‡§î‡§ î‚¤ î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‡§î‡§ î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‡§ î‚¤î‚¤î‚¤ î‚¤\nî¢ î‰ î‰ î‚„ î‚„ î‚¤ î‚¤î‡§ î‚¤î‚¤î‡§ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î‚¤ î‚¤î‚¤ î‚¤î‰€ î‚¤ î‚¤î‰€î‚¤ î‚¤ î‚¤î‡§ î‚¤î‚¤ î“§î‚¤î‡§ î‚¤\nî‰\nElectric Piano 1\nî€€\nîî‰ î‰ î‚„\nî‚„\nî“£ î“£ î“£ î“£\nElectric Piano 2 îî‰ î‰ î‚„ î‚„ î“£ î“£ î“£ î“£\nî¢ î‰ î‚„ î“£ î“£ î“£ î“£\nî‰  î‚„\nViolins\nî€€\nîî‰ î‰ î‚„\nî‚„\nî“£ î“£ î“£\nî‚¤ î‚¤ î‚£\nBrass\nîœî‰ î‰ î‚„ î‚„ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤\nElectric Bass î‚¤ î‚¤ î‚¤ î‚¤\nî¢ î‰ î‚„ î“£ î“£ î“£ î“£\nî‰  î‚„\nA8\n5 Eâ™­ Bâ™­/D Eâ™­ Bâ™­/D Eâ™­ Bâ™­/D Cm7 F/A\nMel.\nî î‰ î‰  î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¥\nA.G. 2\nî’î‰ î‰  î‚¤î‚¤ î‚¤ î“¥ î‚¤î‚¤ î‚¤ î“¦ î‚¤î‚¤ î‚¤î‚¤ î“¦ î‚¤î‚¤î‚£ î‚¤ î‚¤î‚¤ î“¥ î‚¤î‚¤ î‚¤ î“¦ î‚¤î‚¤ î‚¤ î‚¤î‚¤ î“¦ î‚¤î‚¤î‚¤ î‚¤î‚¤ î“¥\nE.P. 1\nî‰ \nîî‰ \nî‚¤\nî“¥\nî‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤\nî“¥ î“¥ î“¦\nî‚¤î‚¤ î‚£ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤\nî“¥ î“¥ î“¦\nî‚¤\nî‚¤\nî‰ \nE.P. 2 îî‰ \nî‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î‚¤\nî¢ î‰ î‰  î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤\nî‚¤\nî‚¤\nî‚¤\nî‚¤ î‚¤î‚¤ î‚£î‚¤ î‚¤î‚¤ î‚£î‚¤ î“¥ î‚¤ î“¥\nî‚¤ î‚£\nVlns.\nî‰ \nî€€ îî‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£\nBr.\nîœî‰ î‰  î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤\nî‚¤î‚¤\nî‚¤\nî‚¤î‚¤\nî‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤\nî‚¤\nî‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤\nE.B.\nî¢ î‰ î‰  î‚£ î‚£ Figure11:î‚£ Multi-tracî‚£karrangemî‚£ entscore(î‚£page1). î‚£ î‚£\n20\n9 Eâ™­ Bâ™­/D Eâ™­ Bâ™­/D Eâ™­ Gm Aâ™­ F\nMel.\nî î‰ î‰  î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î‚¤î‡§ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¦î‡§ î‚¤î‰‚\nA.G. 1\nî‚¤\nî’î‰ î‰  î“£ î“£ î“£ î“¤ î‚¤ î‚¤\nA.G. 2 î‚¤\nî’î‰ î‰  î“¥ î‚¤î‚¤î‚¤î‚¤ î‚¤ î‚¤î‡§î‡§ î‚¤ î‚¤ î‚¤î‚¤ î‚¤î‡§ î‡§î‡§ î‚¤î‚¤ î‚£ î“¤ î‚¤ î‚¤ î“¦ î‚¤î‚¤ î‰  î‚¤î‚¤î‚¤ î“¥ î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤\nî‚¤ î‚¤\nPno. îî‰ î‰  î“£ î“£ î“£ î“¤ î‚¤ î‚¤\nî¢ î‰  î“£ î“£ î“£ î“£\nî‰ \nE.P. 1\nî‰ \nî€€\nîî‰ \nî‚£ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤\nî“¥ î“¤\nî‚£ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‰ î‚¤\nî“¥ î“¤\nE.P. 2 î î¢î‰  î‰ î‰  î‰  î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤ î‚¤ î‚£î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤ î‰  î‰ î‚£î‚¤ î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤î‚¤ î“¤ î“¥\nî‚¤\nVlns.\nî‰ \nî€€ îî‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‰ î‚£ î‚£\nBr.\nE.B. îœî‰ î‰  î‚¤î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‰  î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤\nî¢ î‰ î‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‰ î‚£ î‚£\nB8\n13 Bâ™­ F/A Gm Eâ™­ Bâ™­ Eâ™­ C7/E F\nMel.\nî î‰ î‰  î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î“¥ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¤\nA.G. 1\nî‰ \nî’î‰  î‚£ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¥ î“¦ î‚¤ î‚¤ î‚£ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¤\nA.G. 2\nî’î‰ î‰  î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤ î‚¤ î“¦ î‚¤ î‚¤ î‚¤ î‚¤ î“¥ î‚¤ î‚¤î‚¤ î“¥ î‚¤ î‚¤î‚¤ î“¦ î‚¤ î‚¤ î‚¤ î“¥ î‚¤ î‚¤î‚¤ î“¥\nî‚¤\nE.P. 1\nîî‰ î‰  î“¥ î“¥ î“¦ î“¥ î“¥î‡§\nVlns. î‚£ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤\nîî‰ î‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£\nBr.\nîœî‰ î‰  î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤\nî‚¤\nî“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤\nE.B.\nî¢ î‰ î‰  î‚¤î‡§ î‚¤î‰€ î‚¤î‡§ î‚¤î‰€ î‚¤î‡§ î‚¤î‰€ î‚¤î‰€ î‚¤î‡§ î‚¤î‡§ î‚¤î‰€ î‚£ î‚¤î‡§ î‰î‚¤ î‚£\n17 Eâ™­ Bâ™­/D Gm Eâ™­ Cm7 Bâ™­/D Cm/Eâ™­ C/E F\nMel.\nî î‰ î‰  î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¥ î“¦î‡§ î‚¤î‰‚\nA.G. 1\nî’î‰ î‰  î‚£ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£î‡§ î‚¤ î‚¤ î‚¤ î‚£ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£î‡§ î“¦ î‚¤ î‚¤\nA.G. 2\nî’î‰ î‰  î‚¤ î‚¤î‚¤ î“¥ î‚¤ î‚¤î‚¤ î“¥ î‚¤î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤ î‚¤î‚¤ î“¥ î‚¤ î‚¤î‚¤ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î“¥\nE.P. 1\nî‰ \nVlns.\nîî‰ \nî‚£ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤\nî“¥ î“¥ î“¦\nî‚¤ î‚¤ î‚£ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤\nî“¥ î“¤\nîî‰ î‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£\nBr.\nîœî‰ î‰  î“¥ î‚¤î‚¤ î‚¤ î“¥ î‚¤ î‚¤î‚¤ î“¥ î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤\nE.B.\nî¢ î‰ î‰  î‚¤î‡§ î‚¤î‰€ î‚£ Figure12:î‚¤ Multî“¦ i-tî‚¤rî‰€ acî‚£karrangemeî‚¤ nî‡§ tscoî‚¤rî‰€ eî‚£ (page2). î‚¤î‡§ î‰î‚¤ î‚£\n21\nB8\n21 Bâ™­ F/A Gm Eâ™­ Bâ™­ Eâ™­ C7/E F\nMel.\nî î‰ î‰  î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¥ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¤\nA.G. 1\nî’î‰ î‰  î‚¤ î‚¤ î‚¤î“¥ î“¥î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‰€ î‡§ î“¦î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¦ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¤\nA.G. 2 î‚¤ î‰\nVlns.\nî’î‰ î‰  î‚¤î‚¤î‚¤ î“¦\nî‚¤ î‚¤\nî‚¤ î‚¤ î“¥ î‚¤î‚¤ î‚¤ î“¦ î‚¤ î‚¤ î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¦ î‚¤ î‚¤ î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥\nîî‰ î‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£\nBr.\nîœî‰ î‰  î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤ î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤\nE.B.\nî¢ î‰ î‰  î‚¤î‡§ î‚¤î‰€ î‚£ î‚¤ î“¦ î‚¤î‰€ î‚£ î‚¤î‡§ î‚¤î‰€ î‚£ î‚¤î‡§ î‚¤ î‰ î‚£\n25 Eâ™­ Bâ™­/D Gm Eâ™­ Cm Bâ™­/D Cm/Eâ™­ F7sus Eâ™­/Bâ™­ Bâ™­\nMel.\nî î‰ î‰  î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚£ î“¤\nA.G. 1\nî’î‰ î‰  î‚£ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£î‡§ î‚¤ î‚¤ î‚¤ î‚£ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¥ î“¤\nA.G. 2\nî’î‰ î‰  î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“¦\nî‚¤\nî‚¤\nî‚¤\nî‚¤ î‚¤ î‚¤\nVlns.\nîî‰ î‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£\nBr.\nîœî‰ î‰  î“¥ î‚¤î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤ î‚¤î‚¤ î‚¤î‚¤î‚¤ î“¥\nE.B.\nî¢ î‰ î‰  î‚¤î‡§ î‚¤î‰€ î‚£ î‚¤ î“¦ î‚¤î‰€ î‚£ î‚¤î‡§ î‚¤ î‰ î‚£ î‚¤ î“¦ î‚¤ î‰ î‚£\nx4\n29 Bâ™­ F/A Eâ™­/G Bâ™­/F Eâ™­ Bâ™­/D F/A Bâ™­ Cm7 Bâ™­/D\nMel.\nî î‰ î‰  î“¤ î“¤ î“¤ î“¤ î“¤ î“¤ î“¤ î“¤\nA.G. 1 î‰  î‚£ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚£î‡§ î‚¤î‚¤ î‚£ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¢\nî’î‰  î“¦\nA.G. 2\nî’î‰ î‰  î“§\nî‚¤î‚¤î‚¤î‚¤\nî“§\nî‚£î‚¤î‚¤î‚¤î‚¤î‚¤î‚¤\nî‚¤ î“§\nî‚£î‚¤î‚¤î‚¤î‚¤\nî“§\nî‚¤î‚¤î‚¤î‚¤\nî‚¤î‚¤î‚¤ î“§\nî‚£î‚¤î‚¤î‚¤î‚¤\nî“§\nî‚¤î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤\nî“§\nî‚£î‚¤î‚¤î‚¤î‚¤\nî“§\nî‚¤î‚¤î‚¤î‚¤\nî‚£ î‚£ î‚£ î‚£\nPno. îî‰ î‰  î“£ î“£ î“£ î“£\nî‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤\nî‚¤ î‚¤î‚¤\nî¢ î‰ î‰  î“§ î‚¤î‚¤ î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î“§ î‚¤î‚¤ î“§ î‚¤î‚¤ î‚¤î‚¤ î‚¤ î“§ î‚¤î‚¤ î“§ î‚¤ î‚¤î‚¤î‚¤ î“§ î‚¤î‚¤ î“¤\nî‚£ î‚¤ î‚£ î‚¤ î‚£ î‚¤ î‚£\nî‚£ î‚£ î‚£\nE.P. 2\nî€€\nîî‰ î‰  î“§ î‚¤î‚¤î‚¤ î“§ î‚¤î‚¤î‚¤\nî‚¤î‚¤\nî“£\nî‚¤î‚¤ î‚¤î‚¤\nî“£\nî‚¤î‚¤\nî“£\nî‚¤î‚¤\nî¢ î‰ î‰  î‚¤ î‚¤ î“§ î‚¤î‚¤ î“§î‚¤î‚¤ î“§ î‚¤î‚¤ î“§î‚¤î‚¤ î“§ î‚¤î‚¤î‚¤î‚¤ î“§î‚¤î‚¤\nî‚£ î‚£ î‚£ î‚£\nE.B. î‚£ î‚£ î‚£ î‚£\nî‚£ î‚£ î‚£\nî€€ î¢ î‰ î‰  î‚£ î‚£ î‚£ î‚£ î‚£\nA8\n33 Eâ™­ Bâ™­/D Eâ™­ Bâ™­/D Eâ™­ Bâ™­/D Cm7 F/A\nMel.\nî î‰ î‰  î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤î‚¤î‡§ î“¥ î‚¤ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î“¥ î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î“¥\nPno. îî‰ î‰  î“£ î“¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤ î“¤ î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤î‚¤î‚¤î‚¤\nî‚¤\nî‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤î‚¤î‚¤ î‚¤\nî¢ î‰ î‰  î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î“§ î‚£î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§ î‚£î‚¤î‚¤ î“§ î‚£î‚¤ î“§î‚¤î‚¤ î“§ î‚£î‚¤î‚¤ î“¥ î“§ î‚£î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î‚¤ î“§ î‚¤î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤\nî‚£ î‚£ î‚£\nE.B.\nî¢ î‰ î‰  î‚£ î‚£ Figure13:î‚£ Multi-traî‚£ckarrangemî‚£ entscorî‚£e(page3). î‚£ î‚£\nî€€\n22\n37 Eâ™­ Bâ™­/D Eâ™­ Bâ™­/D Eâ™­ Gm Aâ™­ F\nMel.\nî î‰ î‰  î‚¤ î‚¤î‚¤î‚¤î‚¤î‡§ î‚¤î‚¤î‡§ î“¥ î‚¤ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î“¥ î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î“¦î‡§ î‚¤î‰‚\nCel.\nî‰ \nîî‰  î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚£ î‚£ î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚£ î‚£\nPno.\nîî‰ î‰  î“£ î“£ î“£ î“£\nî¢ î‰  î“§\nî‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤î“§î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§î‚¤î‚¤ î‚¤î“§î‚¤î‚¤ î‚¤î“§î‚¤î‚¤î‚¤î“§î‚¤î‚¤ î‚¤ î“§î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î“§î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§î‰ î‚¤î‚¤î‚¤î‚¤î‚¤î‚¤î‚¤ î“§î‚¤î‚¤î‰¡î‚¤î‚¤\nî‰  î‚£î‚¤ î‚£ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î‚£ î‰ î‚£ î‚£\nE.B.\nî¢ î‰  î‚£ î‚£ î‚£\nî€€ î‰  î‚£ î‚£ î‚£ î‰ î‚£ î‚£\nB8\n41 Bâ™­ F/A Gm Eâ™­ Bâ™­ Eâ™­ C7/E F\nMel.\nî î‰ î‰  î‚¤î‡§ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‡§ î‚¤ î“¥ î“¥ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¤\nA.G. 1\nî’î‰ î‰  î‚£ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚£î‡§ î‚¤ î‚¤î‚¤ î‚£ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚£î‡§ î‚¤ î‚¤î‚¤\nA.G. 2\nî’î‰ î‰  î‚¤î‚¤\nî‚¤\nî“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤\nî‚¤\nî“¥ î‚¤î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤î‚¤î“¥\nPno.\nîî‰ î‰  î“£ î“£ î“£ î“£\nVlns.\nî¢ î‰ î‰  î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤\nî€€\nîî‰ î‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£\nBr.\nîœî‰ î‰  î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤\nE.B.\nî¢ î‰  î‚£ î‚£\nî‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£\n45 Eâ™­ Bâ™­/D Gm Eâ™­ Cm7 Bâ™­/D Cm/Eâ™­ C/E F\nMel.\nî î‰ î‰  î“¥ î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‚¤ î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¥ î“¦î‡§ î‚¤î‰‚\nA.G. 1\nî’î‰ î‰  î‚¤ î‚¤î‚¤î‚¤ î“¥ î“¦ î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤î‰€ î‡§ î“§î“¥ î“¦ î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î“¦ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚£ î“¤\nî‰\nA.G. 2\nî’î‰ î‰  î‚¤î‚¤ î‚¤ î“¦ î‚¤î‚¤î‚¤ î‚¤ î“¥ î‚¤î‚¤ î‚¤ î“¦ î‚¤î‚¤î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¦ î‚¤î‚¤ î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥\nPno.\nîî‰ î‰  î“£ î“£ î“£ î“£\nî¢ î‰ î‰  î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤\nVlns. î‚£ î‚£\nî€€ îî‰ î‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£\nBr.\nîœî‰ î‰  î“¥ î‚¤ î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤\nE.B.\nî¢ î‰ î‰  î‚£ î‚£ Figure14:î‚£Multi-tracî‚£karrangemî‚£ entscore(î‚£\npage4).\nî‚£ î‚£\n23\nB8\n49 Bâ™­ F/A Gm Eâ™­ Bâ™­ Eâ™­ C7/E F\nMel.\nî î‰ î‰  î‚¤î‡§ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î‚¤î‚¤ î“¥ î“¥ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚£ î“¤\nA.G. 1\nî’î‰ î‰  î‚£ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚£î‡§ î“¦ î‚¤î‚¤ î‚£ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î“¥ î“¤\nA.G. 2\nî’î‰ î‰  î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤î‚¤î“¥\nPno. îî‰ î‰  î“£ î“£ î“£ î“£\nVlns.\nî¢ î‰ î‰  î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‰¡î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‰¡î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤\nî€€ îî‰ î‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£\nBr.\nîœî‰ î‰  î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤\nî‚¤\nî‚¤î‚¤î‚¤\nE.B.\nî¢ î‰  î‚£ î‚£\nî‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£\n53 Eâ™­ Bâ™­/D Gm Eâ™­ Cm Bâ™­/D Cm/Eâ™­ F7sus Eâ™­/Bâ™­ Bâ™­\nMel.\nî î‰ î‰  î“¥ î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‡§ î‚£ î“¤\nA.G. 1\nî’î‰ î‰  î‚¤ î‚¤î‚¤î‚¤ î“¥ î“¦ î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î“¥ î“¦ î‚¤î‚¤ î‚¤ î‚¤î‚¤î‚¤ î“¦ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤ î“¥ î“¤\nî‰\nA.G. 2\nî’î‰ î‰  î‚¤î‚¤î‚¤ î“¦ î‚¤î‚¤î‚¤ î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¦ î‚¤î‚¤î‚¤ î‚¤ î“¥ î‚¤ î‚¤î‚¤ î“¥ î‚¤ î‚¤î‚¤ î“¦ î‚¤î‚¤ î‚¤ î‚¤ î“¥ î“¤\nPno.\nîî‰ î‰  î“£ î“£ î“£ î“£\nî¢ î‰ î‰  î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤ î‚¤ î‚£î“§ î‚¤î‚¤ î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤\nVlns. î‚£\nî€€ îî‰ î‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£\nBr.\nîœî‰ î‰  î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î“¥ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚¤î‚¤\nE.B.\nî¢ î‰ î‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£\nO4\n57 Eâ™­ Bâ™­/D Gm Eâ™­ Cm Bâ™­/D Cm/Eâ™­ F7sus Eâ™­/Bâ™­ Bâ™­\nMel.\nî î‰ î‰  î“¥ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‡§ î‚£ î“¤\nA.G. 2\nî’î‰ î‰  î‚£î“§î‚¤î‚¤î‚¤î‚¤ î“§î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤\nî‚¤\nî‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚£î“§î‚¤î‚¤î‚¤î‚¤î‚¤ î‚¤î‚¤ î“£ î“£\nî‚£\nE.P. 1\nî‰ \nîî‰ \nî‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤ î‚¤ î‚¤\nî“¥ î“¥ î“¦\nî‚¤î‚¤ î‚¤ î‚¤ î‚¤ î‚¤î‚¤î‚¤ î‚¤ î‚£\nî“¤\nE.P. 2 îî‰ î‰ \nî‚¤î‚¤\nî“£\nî‚¤î‚¤\nî“£\nî‚¤î‚¤\nî“£\nî‚¤î‚¤\nî“£\nî¢ î‰ î‰  î‚£î“§î‚¤î‚¤ î“§î‚¤î‚¤ î“§ î‚¤î‚¤î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î‚£î“§î‚¤î‚¤ î“§î‚¤î‚¤ î‚£î“§ î‚¤î‚¤î‚¤î‚¤ î“§î‚¤î‚¤ î‚¤\nVlns. î‚£ î‚£ î‚£ î‚£\nî€€ îî‰ î‰  î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£ î‚£\nE.B.\nî¢ î‰ î‰  î‚£ Figuî‚£ re15: Multi-î‚£trackarranî‚£gementscoreî‚£ (page5,î‚£ lastpage). î‚£ î‚£\n24\nF Limitation\nWeproposeatwo-stagesystemforwhole-song,multi-trackaccompanimentarrangement. Inthe\ncontextofthispaper,weacknowledgethatourcurrentsystemexclusivelysupportstonaltracksin\nquadruplemeterswhiledisregardingtriplemeters,tripletnotes,anddrums. However,weperceive\nthis as a technical limitation rather than a scientific challenge. We also acknowledge that our\ncurrentsystemprimarilyemphasizesthecompositionlevel,therebyomittingthemodellingofMIDI\nvelocity,dynamictiming,andMIDIcontrolmessages. Consequently,thegeneratedresultsdonot\nencompassperformanceMIDIandmaylackexpressivequalities. Nevertheless,webelievethatour\ncomposition-centricworkservesasasolidandvitalfoundationforfurtheradvancementsinthose\nspecificareas,thusfacilitatingthedevelopmentofenhancedtechniquesandfeatures. Asapioneering\nwork,oursystemistheforemostaccomplishmentinsolvingwhole-songmulti-trackaccompaniment\narrangement,characterizedbyflexiblecontrollabilityontracknumberandchoiceofinstruments.\nG BroaderImpacts\nOurmulti-trackaccompanimentarrangementsystem,whichincorporatesstyletogenerateaccom-\npaniment,isdesignedtoenhanceoriginalityandcreativity. Itservesasaplatformforhuman-AI\nco-creation,wheretheuserprovidescontent-basedmaterial(inourcase,leadsheet)thatremains\nfundamentallyoriginal,whiletheAIagentinfusesstyle,enrichestheform,andenhancescreativity.\nOursystemthereforeempowersmusicianstoexplorenewmusicalideasandexpandtheircreative\nboundaries. Thisapproachalsoallowsforrapidmock-upwithdifferentstylesandarrangements,\nfosteringanenvironmentwhereinnovationandartisticexpressioncanthrive.\nHowever,weacknowledgetheneedtoaddresspotentialrisks. Theaccessibilityofoursystemmay\ninadvertently lead to excessive reliance on automation, potentially impeding the development of\nfundamentalskillsamongmusicians. Additionally,widespreadadoptionofthesystemmaycontribute\ntothehomogenizationofmusic,threateningthedistinctivenessandindividualitythatarecrucialto\nartisticexpression. WerecognizethatourdatasetspredominantlyfeaturescontemporaryWestern\nmusic,whichintroducesaculturalbiasthatcouldlimitthediversityofgeneratedcompositions.\n25",
    "pdf_filename": "Structured_Multi-Track_Accompaniment_Arrangement_via_Style_Prior_Modelling.pdf"
}