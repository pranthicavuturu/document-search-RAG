{
    "title": "Structured Multi-Track Accompaniment Arrangement via Style Prior Modelling",
    "abstract": "In the realm of music AI, arranging rich and structured multi-track accompani- ments from a simple lead sheet presents significant challenges. Such challenges include maintaining track cohesion, ensuring long-term coherence, and optimizing computational efficiency. In this paper, we introduce a novel system that leverages prior modelling over disentangled style factors to address these challenges. Our method presents a two-stage process: initially, a piano arrangement is derived from the lead sheet by retrieving piano texture styles; subsequently, a multi-track orchestration is generated by infusing orchestral function styles into the piano arrangement. Our key design is the use of vector quantization and a unique multi- stream Transformer to model the long-term flow of the orchestration style, which enables flexible, controllable, and structured music generation. Experiments show that by factorizing the arrangement task into interpretable sub-stages, our approach enhances generative capacity while improving efficiency. Additionally, our system supports a variety of music genres and provides style control at different compo- sition hierarchies. We further show that our system achieves superior coherence, structure, and overall arrangement quality compared to existing baselines. 1 Introduction Representation learning techniques have enabled new possibilities for controllable generative mod- elling. By learning implicit style representations, which are often hard to explicitly label (e.g., timbre of music audio [21], texture of music composition [39], and artistic style in paintings [20]), new music and artworks can be created via style transfer and latent space sampling. These learned style factors can also serve as external controls for downstream generative models, including Transform- ers [18, 36] and diffusion models [42]. HowNever, applying style factors to long-term sequence generation remains a challenging task. Existing approaches rely on style templates specified manually or by heuristic rules [36, 42, 51], which are impractical for long-term generation. Moreover, when structural constraints are imposed, misaligned style factors can result in incoherent outputs. To address these challenges, we aim to develop a novel sequence generation framework leveraging a global style planner, or prior, which models the conditional distribution of style factors given the model input’s content factors. Both style and content factors are sequences of compact, structurally aligned latent codes over a disentangled representation space. By infusing the style back to the content, we can recover the observational target with globally coherent style patterns. In this paper, we study style prior modelling through the task of multi-track accompaniment arrange- ment, a typical scenario for long-term conditional sequence generation. We assume the input of a 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2310.16334v3  [cs.SD]  19 Nov 2024",
    "body": "Structured Multi-Track Accompaniment Arrangement\nvia Style Prior Modelling\nJingwei Zhao1,3\nGus Xia4,5\nZiyu Wang5,4\nYe Wang2,1,3\n1Institute of Data Science, NUS\n2School of Computing, NUS\n3Integrative Sciences and Engineering Programme, NUS Graduate School\n4Machine Learning Department, MBZUAI\n5Computer Science Department, NYU Shanghai\njzhao@u.nus.edu\ngus.xia@mbzuai.ac.ae\nziyu.wang@nyu.edu\nwangye@comp.nus.edu.sg\nAbstract\nIn the realm of music AI, arranging rich and structured multi-track accompani-\nments from a simple lead sheet presents significant challenges. Such challenges\ninclude maintaining track cohesion, ensuring long-term coherence, and optimizing\ncomputational efficiency. In this paper, we introduce a novel system that leverages\nprior modelling over disentangled style factors to address these challenges. Our\nmethod presents a two-stage process: initially, a piano arrangement is derived\nfrom the lead sheet by retrieving piano texture styles; subsequently, a multi-track\norchestration is generated by infusing orchestral function styles into the piano\narrangement. Our key design is the use of vector quantization and a unique multi-\nstream Transformer to model the long-term flow of the orchestration style, which\nenables flexible, controllable, and structured music generation. Experiments show\nthat by factorizing the arrangement task into interpretable sub-stages, our approach\nenhances generative capacity while improving efficiency. Additionally, our system\nsupports a variety of music genres and provides style control at different compo-\nsition hierarchies. We further show that our system achieves superior coherence,\nstructure, and overall arrangement quality compared to existing baselines.\n1\nIntroduction\nRepresentation learning techniques have enabled new possibilities for controllable generative mod-\nelling. By learning implicit style representations, which are often hard to explicitly label (e.g., timbre\nof music audio [21], texture of music composition [39], and artistic style in paintings [20]), new\nmusic and artworks can be created via style transfer and latent space sampling. These learned style\nfactors can also serve as external controls for downstream generative models, including Transform-\ners [18, 36] and diffusion models [42]. HowNever, applying style factors to long-term sequence\ngeneration remains a challenging task. Existing approaches rely on style templates specified manually\nor by heuristic rules [36, 42, 51], which are impractical for long-term generation. Moreover, when\nstructural constraints are imposed, misaligned style factors can result in incoherent outputs.\nTo address these challenges, we aim to develop a novel sequence generation framework leveraging a\nglobal style planner, or prior, which models the conditional distribution of style factors given the\nmodel input’s content factors. Both style and content factors are sequences of compact, structurally\naligned latent codes over a disentangled representation space. By infusing the style back to the\ncontent, we can recover the observational target with globally coherent style patterns.\nIn this paper, we study style prior modelling through the task of multi-track accompaniment arrange-\nment, a typical scenario for long-term conditional sequence generation. We assume the input of a\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2310.16334v3  [cs.SD]  19 Nov 2024\n\npiano accompaniment score, which typically carries a verse-chorus structure. Our target is to generate\ncorresponding multi-track arrangements featuring band orchestration. We start by disentangling a\nband score at time t into piano reduction ct (content factor) and orchestral function sk\nt (style factors\nfor individual tracks k = 1, 2, · · · , K). On top of this, we model the prior of finding appropriate\nfunctions to orchestrate a given piano score, or formally p(s1:K\n1:T | c1:T ). To model dependencies\non both time (T) and track (K) directions, we develop a multi-stream Transformer with interleaved\ntime-wise and track-wise layers. The track-wise layer allows for flexible control over the choice\nof instruments and the number of tracks, while the time-wise layer ensures structural alignment\nthrough cross-attention to the piano reduction. Decoding the inferred s1:K\n1:T with c1:T , we can address\naccompaniment arrangement in a flexible multi-track form with extended whole-song structure.\nExperiments show that our method outperforms existing sequential token prediction approaches and\nprovides better multi-track cohesion, structural coherence, and computational efficiency. Additionally,\ncompared to existing designs of multi-stream language models, our model handles flexible stream\ncombinations more effectively with enhanced generative capacity.\nTo summarize, our contributions in this paper are three-folded:\n• We propose style prior modelling, a hierarchical generative methodology addressing\nboth long-term structure (via style prior at high level) and fine-grained condition/control (via\nrepresentation disentanglement at low level). Our approach moves beyond the limitation\nof manual specification of style factors, providing a flexible, efficient, and self-supervised\nsolution for long-term sequence prediction and generation tasks.\n• We propose a novel layer interleaving architecture for multi-stream language modelling.\nIn our case, it models parallel music tracks with a flexible track number, controllable\ninstruments, and manageable computation. To our knowledge, it is the first multi-stream\nlanguage model with tractable generalization to flexible stream combinations.\n• Integrating our previous study on piano texture style transfer [39, 50], we present a complete\nmusic automation system arranging an input lead sheet (a basic music form with melody\nand chord only) via piano accompaniment to multi-track arrangement. The entire system is\ninterpretable at two composition hierarchies: 1) piano texture and 2) orchestral function,\nand demonstrates state-of-the-art arrangement performance for varied genres of music.1\n2\nRelated Works\nIn this section, we overview three topics related to our study. Section 2.1 reviews existing studies on\nrepresentation disentanglement. Section 2.2 summarizes prior modelling methods in music generation.\nSection 2.3 reviews the current progress with the task of accompaniment arrangement.\n2.1\nContent-Style Disentanglement via Representation Learning\nRepresentation disentanglement is a popular technique in deep generative modelling [3, 16, 48, 49].\nIn the music domain, this approach has proven valuable by learning compositional factors related to\nmusic style and content. By manipulating these factors through interpolation [32], swapping [39],\nand prior sampling [46], it provides a self-supervised and controllable pathway for various music\nautomation tasks. Recent works leverage disentangled style factors as control signals for long-term\nmusic generation [36, 42]. However, these approaches typically treat style representations as fixed\ncondition sequences during training, requiring manual specification or additional algorithms for\ncontrol during inference. In contrast, we model the prior of the style to apply conditional on the given\nmusic content, which is a more generalized and flexible approach.\n2.2\nMusic Generation with Latent Prior\nIn sequence generation tasks (e.g., music and audio), learning a prior sampler over a compact, latent\nrepresentation space is often more efficient and effective. Jukebox [7] models the latent codes encoded\nby VQ-VAEs [34] as music priors, which can further reconstruct minutes of music audio. More\nrecently, MusicLM [2] and MusicGen [4] learn multi-modal priors for generating music from text\n1Demo and more resources: https://zhaojw1998.github.io/structured-arrangement/\n2\n\nTable 1: Summary of the data representations applied in this paper. We use notation [a..b] to denote\nthe integer interval {x | a ≤x ≤b, x ∈Z} including both endpoints.\nMulti-Track Arrangement\nPiano Reduction\nOrchestral Function\nData Representation\nx ∈[0..32]T ×K×32×128\npn[x] ∈[0..32]T ×32×128\nfn[x] ∈[0, 1]T ×K×32\nLatent Dimension\nz ∈RT ×256\nc ∈RT ×256\ns ∈[0..127]8T ×K\nprompts. While prior modelling facilitates long-term generation, the latent codes in these works are\nnot interpretable, thus lacking a precise control by music content-based signals (e.g., music structure).\nSuch controls are essential for conditional generation tasks, including accompaniment arrangement.\nIn this paper, we model a style prior conditional on the disentangled music content, which allows for\nstructured long-term music generation, enhancing both interpretability and controllability.\n2.3\nAccompaniment Arrangement\nAccompaniment arrangement aims to compose the accompaniment part given a lead sheet, which is a\ndifficult conditional generation task involving structural constraints. Existing methods mainly train a\nconditional language model based on sequential note-level tokenization [14, 15, 30, 33], which often\nsuffer from slow inference speed, truncated structural context, and/or simplified instrumentation.\nRecent attempts with diffusion models show higher sample quality with faster inference [23, 26, 27],\nbut still consider limited instruments or tracks. AccoMontage [47, 50] maintains a whole-song\nstructure by manipulating high-level composition factors, but is limited to piano arrangement alone.\nOur paper presents a two-stage approach: from lead sheet to piano accompaniment, and from piano\nto multi-track, both leveraging prior modelling of high-level style factors. This approach offers\nmodularity [11] and enables high-quality whole-song and multi-track accompaniment arrangement.\n3\nMethod\nWe develop a model that takes a piano reduction as input and outputs an orchestrated multi-track\narrangement. Using an autoencoder, we disentangle a multi-track music score into its piano reduction\n(content factor) and orchestral function (style factor). We then design a prior model to infer orchestral\nfunctions given the piano reduction. The autoencoder operates at segment level, while the prior model\nworks on the whole song. The entire model can operate as an orchestrator module in a complete\narrangement system. In this section, we introduce our data representation in Section 3.1, autoencoder\nframework in Section 3.2, and prior model design in Section 3.3.\n3.1\nData Representation\nWe summarize our data representations in Table 1. Let x be a K-track arrangement score. We split\nit into T segments and represent xk\nt — each segment track — as a matrix of shape P × N. Here\nP = 128 represents 128 MIDI pitches and N is the time dimension of a segment. This matrix\nrepresentation aligns with the modified piano roll in [39], where each non-zero entry (p, n) > 0\nindicates a note onset and its value indicates the note duration. In this paper, we primarily focus\non music pieces in 4/4 time signature with 1/4-beat resolution. Duration values range from 1 (for\nsixteenth notes) to 32 (for double whole notes). We consider 1 segment = 8 beats (2 bars) and derive\nN = 32, which is a proper scale for learning music content/style representations [37, 39–41, 46].\nThe piano reduction of x is notated as pn[x]. It is approximated by downmixing all K tracks into a\nsingle-track mixture similar to [8]. When concurring notes are found across tracks, we keep the one\nwith the largest duration (i.e., track-wise maximum). Segment-wise, pn[x]t is also a P × N matrix.\nIt preserves the overall music content while discarding the multi-track form.\nThe orchestral function of x is notated as fn[x]. It describes the rhythm and grooving patterns [45] of\neach segment track, which serves as the “skeleton” of a multi-track form. Formally,\nfn[x]k\nt = colsum(1{xk\nt >0})/max_sum,\n(1)\nwhere indicator function 1{·} counts each note onset position as 1; colsum(·) sums up the pitch\ndimension, deriving a 1×N time-series feature; max_sum = 14 is for normalization. The orchestral\n3\n\n2\npn[𝐱𝐱]𝑡𝑡\nConv1d\nFC\n⋯\n⋯\nFC\n⋯\n⋯\n(128x16)\nො𝐱𝐱𝒕𝒕\n𝟏𝟏\nො𝐱𝐱𝒕𝒕\n𝟐𝟐\n⋯\nො𝐱𝐱𝒕𝒕\n𝑲𝑲\n𝐜𝐜𝑡𝑡\n𝐳𝐳𝑡𝑡\n1\n𝐳𝐳𝑡𝑡\n2\n𝐳𝐳𝑡𝑡\n𝐾𝐾\nTrack Axis\n𝐬𝐬𝑡𝑡\n1\n𝐬𝐬𝑡𝑡\n2\n𝐬𝐬𝑡𝑡\n𝐾𝐾\n⋯\n⋯\nfn[𝐱𝐱]𝑡𝑡\n𝑘𝑘\n෡fn[𝐱𝐱]𝑡𝑡\n𝑘𝑘\nSep (Track Separator )\nTrack Decoder\nPiano Encoder\nFn. Enc.\nVQ Codebook\nEncp\nEncf\nEncf\nEncf\nDecf\nDecf\nDecf\nDect\nDect\nDect\nFn. Dec.\nFunction Encoder\nFunction Decoder\nFigure 1: The autoencoder architecture. It learns content representation ct from piano reduction, style\nrepresentations s1:K\nt\nfrom orchestral function, and leverages both to reconstruct individual tracks.\nfunction fn[x] essentially describes the form, or layout, of multi-track music x. It indicates the\nrhythmic intensity of parallel tracks and informs where to put more notes and where to keep silent.\n3.2\nAutoencoder\nOur autoencoder consists of two components as shown in Figure 1. A VQ-VAE submodule (right\nof Figure 1) encodes orchestral function fn[x]k\nt . A VAE module (left of Figure 1) encodes piano\nreduction pn[x]t and reconstructs individual tracks x1:K\nt\nleveraging the cues from fn[x]1:K\nt\n. During\ntraining, both inputs pn[x] and fn[x] are deterministic transforms from the output x and the entire\nmodel is self-supervised. We see similar techniques for representation disentanglement in [39, 41, 46].\nThe VQ-VAE consists of Function Encoder Encf and Decoder Decf. Encoder Encf contains a 1-D\nconvolutional layer followed by a vector quantization block. Our intuition for applying a VQ-VAE is\nthat orchestral function commonly consists of rhythm patterns (such as syncopation, arpeggio, etc.)\nthat can naturally be categorized as discrete variables. In our case, each segment track is encoded\ninto 8 discrete embeddings on a 1-beat scale, indicating the flow of orchestration style. Formally,\nsk\nt := {sk\nτ}8t\nτ=8t−7 = Encf(fn[x]k\nt ), k = 1, 2, · · · , K,\n(2)\nwhere sk\nτ is the latent orchestral function code for the k-th track at the τ-th beat. We encode fn[x]k\nt at\na finer 1-beat scale (instead of segment) to preserve fine-grained rhythmic details. The new scale is\nre-indexed by τ = 1, 2, · · · , 8T. We collectively denote each 8-code grouping as sk\nt for conciseness.\nThe VAE consists of Piano Encoder Encp, Track Separator Sep, and Track Decoder Dect. Encoder\nEncp learns content representation ct from piano reduction pn[x]t. Here ct is a continuous repre-\nsentation (without vector quantization) that captures more nuanced music content. Decoder Dect\nreconstructs individual tracks xk\nt from track representation zk\nt . Notably, z1:K\nt\nare recovered from ct\nusing the orchestral function cues from s1:K\nt\n. Formally,\nz1\nt, z2\nt, · · · , zK\nt = Sep(s1\nt, s2\nt, · · · , sK\nt | ct),\n(3)\nwhere Track Separator Sep is a Transformer encoder. In this process, each sk\nt queries ct to recover\nthe corresponding track (k), while they also attend to each other to maintain the dependency among\nparallel tracks. Learnable instrument embeddings [51] are added to each track based on its instrument\nclass. We provide details of the autoencoder architecture in Appendix A.1.\n3.3\nStyle Prior Modelling\nThe VQ-VAE in Section 3.2 derives latent codes s1:K\n1:8T for orchestral function as a multi-stream time\nseries. Here k = 1, 2, · · · K is the stream (track) index and τ = 1, 2, · · · , 8T is the time (beat) index.\nThe purpose of style prior modelling is to infer orchestral function given piano reduction so that the\nformer can be leveraged to orchestrate the latter into multi-track music. We design our prior model\nas shown in Figure 2. It is an encoder-decoder framework that models parallel tracks/streams of\norchestral function codes conditional on the piano reduction.\nThe decoder module (right of Figure 2) has alternate layers of Track Encoder and Auto-Regressive\nDecoder. Track Encoder is a standard Transformer encoder layer [35] and it aggregates inter-track\ninformation along the track axis. Auto-Regressive Decoder is a Transformer decoder layer (with\nself-attention and cross-attention) and it predicts next-step orchestral function codes on the time axis.\n4\n\nContext Encoder\n𝐜𝐜1:𝑇𝑇\n𝑠𝑠𝜏𝜏1\n⋯\n⋯\n⋯\n𝑠𝑠𝜏𝜏𝐾𝐾\n𝑠𝑠2\n1\n𝑠𝑠1\n1\nsos\n𝑠𝑠1\n1\n𝑠𝑠1\n𝐾𝐾\n𝑠𝑠2\n𝐾𝐾\n𝑠𝑠3\n𝐾𝐾\n𝑠𝑠𝜏𝜏+1\n𝐾𝐾\nTime Axis\n+\n+\n+\n+\nCross-Attention\n⋯\n𝑠𝑠𝜏𝜏+1\n1\nAuto-Regressive Decoder\nSinusoidal Positional Encoding\nRelative Positional Embedding\nMusic Timing Condition\nInstrument Embedding\nGaussian Noise\nFigure 2: The prior model architecture. The overall architecture is an encoder-decoder Transformer,\nwhile the decoder module is interleaved with orthogonal time-wise and track-wise layers.\nBy orthogonally stacking two types of layers, we can model track-wise and time-wise dependen-\ncies simultaneously with a manageable computational cost. Compared to sequential tokenization\nmethods in previous studies [9, 30, 36], our method brings down the complexity from O(N 2T 2) to\nO(max(N, T)2). Moreover, we support a flexible multi-track form (N being variable) with a diverse\ninstrumentation option. We add instrument embedding [51] and relative positional embedding [15] to\nthe track axis, where 34 instrument classes [25] are supported. We add music timing condition [7] to\nthe time axis, which encodes the positions in a training excerpt as fractions of the complete song,\nhelping the model capture the overall structure of a song.\nThe encoder module (left of Figure 2) of our prior model is a standard Transformer encoder, which\ntakes piano reduction c1:T as global context. It is connected to the decoder module via cross-attention\nand maintains the global phrase structure. During training, both c1:T and s1:K\n1:8T are derived from\nthe same multi-track piece and the entire model is self-supervised. Let pθ be the distribution of\norchestral function codes fitted by our prior model θ, the training objective is the mean of negative\nlog-likelihood of next-step code prediction:\nL(θ) = −1\nK\nK\nX\nk=1\nlog pθ(sk\nτ | s1:K\n<τ , c1:T ).\n(4)\nWe provide more implementation details of the prior model in Appendix A.2. We note that there is a\npotential domain shift from our approximated piano reduction to real piano arrangements. To prevent\noverfitting, we use a Gaussian noise ϵ to blur c1:T while preserving its high-level structure. During\ntraining, ϵ is combined with c1:T using a weighted summation with noise weight γ ranging from 0\nto 1. It encourages a partial unconditional generation capability. At inference time, γ is a parameter\nthat can balance creativity with faithfulness. An experiment on γ is covered in Appendix C.\n4\nWhole-Song Multi-Track Accompaniment Arrangement\nWe finalize a complete music automation system by applying style prior modelling at two cascaded\nstages. As shown in Figure 3, our autoencoder and orchestral function prior operate on Stage 2 for\npiano to multi-track arrangement. On Stage 1, we adopt our previous study, a piano texture prior [50]\nPiano Texture Prior\nOrchestral Function Prior\nLead Sheet\nPiano Arrangement\nA Complete Multi-Track \nAccompaniment Arrangement System\nPiano Texture Style\nOrchestral Function Style\nExternal Control\nMulti-Track Arrangement\nStage 1\nStage 2\nFigure 3: A complete accompaniment arrangement system based on cascaded prior modelling. The\nfirst stage models piano texture style given lead sheet while the second stage models orchestral\nfunction style given piano. Besides modularity, the system offers control on both composition levels.\n5\n\nCounterpoint Relation between Tracks\nMetrical Division between String and Brass\nHarmonic/Melodic Division between Two Guitars\nLong-Term Phrase Coherence over Extended Context\n41\n13\nFigure 4: Arrangement for Can You Feel the Love Tonight, a pop song in a total of 60 bars. We show\ntwo chorus parts from bar 13 to 41. We use red dotted boxes to show coherence in long-term structure.\nWe use coloured blocks to show naturalness and cohesion in multi-track arrangement.\non top of chord/texture representation learning [39], for lead sheet to piano arrangement. Given a lead\nsheet, the first stage generates a piano accompaniment, establishing the rough whole-song structure.\nOur system then orchestrates the piano accompaniment into a complete multi-track arrangement with\nband instrumentation. This two-stage approach mirrors musicians’ creative workflow [1] and allows\nfor control at both composition levels. In particular, we provide three control options:\n1. Texture Selection: To filter piano textures on Stage 1 by metadata and statistical features.\n2. Instrumentation: To customize the track number and choice of instruments on Stage 2.\n3. Orchestral Prompt: To prompt the orchestration process with an orchestral function template.\nWe showcase an arrangement example by the complete system in Figure 4. The system input is a\nlead sheet shown by the Mel staff. The final output is the accompaniment in the rest staves. Notably,\nthe lead sheet consists of 60 bars in an structure of i4A8B8B8x4A8B8B8O4 (using notations by [5]).\nHere, i4, x4, and O4 each denote a 4-bar intro, interlude, and outro. A8 and B8 represent an 8-bar\nverse and chorus, respectively. Figure 4 shows the arrangement result for the first and third choruses,\nspanning from bar 13 to 41. We leverage control option 2 to customize the instrumentation as celesta,\nacoustic guitars (2), electric pianos (2), acoustic piano, violin, brass, and electric bass in a total of\nK = 9 tracks. The complete arrangement score is included in Appendix E.\nIn Figure 4, we observe some multi-track arrangement patterns that are common in practice. Purple\nblocks highlight a counterpoint relation between guitar track A.G.1 and electric piano track E.P.1.\n6\n\nGreen blocks show two guitar tracks with complementary orchestral functions: one melodic (A.G.1)\nand the other harmonic (A.G.2). Yellow blocks illustrate the metrical division between the string\n(Vlns.) and the brass (Br.) sections, with strings on the downbeat and brass on the offbeat. These\npatterns demonstrate a natural and cohesive multi-track arrangement by our system. Moreover, we\nsee consistent accompaniment patterns echoing in both chorus parts that span over 30 bars (shown by\nred dotted boxes), while the latter adds a piano arpeggio track (Pno.) to enhance the musical flow.\nThis demonstrates structured whole-song arrangement over extended music contexts.\n5\nExperiment\nIn this section, we evaluate the performance of our multi-track accompaniment arrangement system.\nGiven that existing methods primarily focus on lead sheet to multi-track arrangement, we ensure a\nfair comparison by using the two-stage approach discussed in Section 4. In Section 5.1, we present\nthe datasets used and the training details of our model. In Section 5.2, we describe the baseline\nmodels used for comparison. Our evaluation is divided into two parts: objective evaluation, detailed\nin Section 5.3, and subjective evaluation, covered in Section 5.4. For the single-stage piano to\nmulti-track (Stage 2) and lead sheet to piano (Stage 1) arrangement tasks, we perform additional\ncomparisons with various ablation architectures in Section 5.5 and 5.6, respectively.\n5.1\nDatasets and Training Details\nWe use two datasets to train the autoencoder and the style prior, respectively. The autoencoder\nis trained with Slahk2100 [25], which contains 2K curated multi-track pieces with 34 instrument\nclasses in a balanced distribution. We discard the drum track and clip each piece into 2-bar segments\nwith 1-bar hop size. We use the official training split and augment training samples by transposing\nto all 12 keys. The autoencoder comprises 19M learnable parameters and is trained with batch\nsize 64 for 30 epochs on an RTX A5000 GPU with 24GB memory. We use Adam optimizer [19]\nwith a learning rate from 1e-3 exponentially decayed to 1e-5. We use exponential moving average\n(EMA) [29] and random restart [7] to update the codebook with commitment ratio β = 0.25.\nWe use Lakh MIDI Dataset (LMD) [28] to train the prior model. It contains 170k music pieces and is\na benchmark dataset for training music generative models. We collect 2/4 and 4/4 pieces (110k after\nprocessing) and randomly split LMD at song level into training (95%) and validation (5%) sets. We\nfurther clip each piece into 32-bar training excerpts (i.e., T = 16 at maximum) with a 4-bar hop size.\nOur prior model has 30M parameters and is trained with batch size 16 for 10 epochs (600K iterations)\non two RTX A5000 GPUs. We apply AdamW optimizer [22] with a learning rate of 1e-4, scheduled\nby a 1k-step linear warm-up followed by a single cycle of cosine decay to a final rate of 1e-6.\nFor model inference and testing, we consider two additional datasets: Nottingham [10] and\nWikiMT [44]. Both datasets contain lead sheets (in ABC notation) that are not seen during training or\nvalidation. Moreover, they cover diverse music genres including folk, pop, and jazz. When arranging\na piece, we leverage control option 2 to set up the instrumentation. Without loss of generality, this\ncontrol choice is randomly sampled from Slakh2100 validation/test sets. To arrange music longer\nthan the prior model’s context length (32 bars), we use windowed sampling [7], where we move\nahead our context window by 4 bars and continue sampling based on the previous 28 bars. We apply\nnucleus sampling [13] with top probability p = 0.05 and temperature t = 6.\n5.2\nBaseline Models\nWe compare our system with three existing methods: PopMAG [30], Anticipatory Music Transformer\n(AMT) [33], and GETMusic [23]. PopMAG and GETMusic generate multi-track accompaniments\nfrom an input lead sheet based on a Transformer and a diffusion model, respectively. AMT is\nTransformer-based and it continues the accompaniment part from an input melody with starting\naccompaniment prompt. We provide detailed configurations in the following.\nPopMAG is an encoder-decoder architecture based on Transformer-XL [6]. It represents multi-track\nmusic by sequential note-level tokenization and is fully supervised. The encoder takes a lead sheet as\ninput and the decoder generates multi-track accompaniment auto-regressively. Since the model is not\nopen source, We reproduce it on LMD with lead sheets extracted by [24] (melody) and [17] (chord).\n7\n\nTable 2: Objective evaluation results for lead sheet to multi-track arrangement (Section 5.3). All\nentries are of the form mean ± sems, where s is a letter. Different letters within a column indicate\nsignificant differences (p < 0.05/6) based on Wilcoxon signed rank test with Bonferroni correction.\nModel\nChord Acc. ↑\nDOA ↑\nStructure ↑\nLatency ↓\nOurs\n0.564 ± 0.014a\n0.300 ± 0.005a\n1.519 ± 0.030a\n0.461 ± 0.005b\nAMT [33]\n0.446 ± 0.013bc\n0.294 ± 0.006a\n1.094 ± 0.009c\n6.320 ± 0.212d\nGETMusic [23]\n0.423 ± 0.012c\n0.225 ± 0.007c\n1.243 ± 0.017b\n0.450 ± 0.002a\nPopMAG [30]\n0.470 ± 0.013b\n0.270 ± 0.007b\n1.086 ± 0.008c\n0.638 ± 0.013c\nGround-Truth\n-\n0.333 ± 0.009\n1.980 ± 0.019\n-\nAnticipatory Music Transformer (AMT) is a decoder-only Transformer architecture with note-level\ntokenization. It introduces an “anticipation” method, where conditional tokens (melody and starting\nprompt) and generative tokens (accompaniment continuation) are interleaved to train a conditional\ngenerative model. Since our testing dataset does not provide ground-truth accompaniment, the\nstarting prompt is given by the generation result (first 2 bars) of our system. We use the official\nimplementation of the AMT model,2 which is also trained on LMD.\nGETMusic represents multi-track music as an image-like matrix resembling score arrangement,\nbased on which a denoising diffusion probabilistic model is trained with a mask reconstruction\nobjective. Given a lead sheet, it supports generating 5 accompaniment tracks using piano, guitar,\nstring, bass, and drum or their subsets. In our experiment, we generate all 5 accompaniment tracks.\nWe use the official implementation of the GETMusic model,3 which is trained on internal data.\n5.3\nObjective Evaluation on Multi-Track Arrangement\nWe introduce four metrics to evaluate multi-track arrangement performance: chord accuracy [23, 30],\ndegree of arrangement (DOA), structure awareness [42], and inference latency [30]. Among them,\nchord accuracy measures the multi-track harmony that reflects the fitness of the accompaniment to the\nlead sheet; DOA measures inter-track tonal diversity that reflects the creativity of the instrumentation.\nBoth metrics demonstrate music cohesion at local scales. On the other hand, structure awareness\nmeasures phrase-level content similarity that reflects long-term structural coherence of the whole\nsong. Finally, we use inference latency (in second/bar) to evaluate computational efficiency of\neach method. The detailed computation of each metric is provided in Appendix B. In Table 2,\nwe compute ground-truth DOA using 1000 random pieces from LMD. We compute ground-truth\nstructure awareness using 857 pieces in 4/4 from POP909 Dataset [38].\nWe randomly sample 50 pieces in 4/4 time signature from Nottingham and WikiMT respectively (100\npieces in total) to conduct experiment. The length of each piece ranges from 16 to 32 bars. We run our\nmethod and baseline models at each piece in 3 independent rounds, deriving 300 sets of multi-track\narrangement samples. In Table 2, we report the evaluation results with mean value, standard error\nof mean (sem), and statistical significance computed by Wilcoxon signed rank test [43]. We find\nsignificant differences between our method and all baselines (p-value p < 0.05/6, using Bonferroni\ncorrection). In particular, our method outperforms in chord accuracy, structure awareness, and DOA,\nindicating the capability of arranging harmonious, structured, and creative accompaniments. The\ndiffusion baseline outperforms in inference latency as it applies only 100 diffusion steps. Our method’s\nefficiency is on par with it, while being 10 times faster than vanilla note-level auto-regression.\n5.4\nSubjective Evaluation on Multi-Track Arrangement\nWe also conduct a double-blind online survey to test music quality. Our survey consists of 5 evaluation\nsets, each containing an input lead sheet followed by 4 arrangement samples by our method and each\nbaseline. Each sample is 24-32 bars long and is synthesized to audio at 90 BPM (~1 minute per\nsample). Both the set order and the sample order in each set are randomized. We request participants\nto listen to 2 sets and evaluate the musical quality based on a 5-point Likert scale from 1 to 5.\n2https://github.com/jthickstun/anticipation\n3https://github.com/microsoft/muzic/tree/main/getmusic\n8\n\nTable 3: Objective evaluation results for piano to multi-track arrangement (Section 5.5). All entries\nare of the form mean±sems, where s is a letter. Different letters within a column indicate significant\ndifferences (p < 0.05/6) based on Wilcoxon signed rank test with Bonferroni correction.\nPrior\nFaithfulness (stats.) ↑\nFaithfulness (latent) ↑\nDOA ↑\nNLL ↓\nOurs\n0.945 ± 0.001a\n0.215 ± 0.005a\n0.308 ± 0.005a\n0.411 ± 0.004\nParallel\n0.937 ± 0.002b\n0.153 ± 0.003b\n0.233 ± 0.005c\n0.960 ± 0.010\nDelay\n0.915 ± 0.004c\n0.133 ± 0.003c\n0.207 ± 0.005d\n1.024 ± 0.006\nRandom\n0.913 ± 0.003c\n0.113 ± 0.003d\n0.262 ± 0.005b\n-\nCoherency Structure NaturalnessCreativity Musicality\n1.0\n2.0\n3.0\n4.0\nOurs\nAMT\nGETMusic\nPopMAG\nFigure 5: Subjective evaluation results on lead\nsheet to multi-track arrangement (Section 5.4).\nInstrumentation Structure\nCreativity\nMusicality\n1.0\n2.0\n3.0\n4.0\nOurs\nParallel\nDelay\nRandom\nFigure 6: Subjective evaluation results on piano\nto multi-track arrangement (Section 5.5).\nThe evaluation is based on 5 criteria: 1) Harmony and Texture Coherency, 2) Long-Term Structure,\n3) Naturalness, 4) Creativity, and 5) Overall Musicality.\nA total of 23 participants (8 female and 15 male) with diverse musical backgrounds have completed\nour survey, with an average completion time of 22 minutes. The mean ratings and standard errors,\ncomputed by within-subject (repeated-measures) ANOVA [31], are presented in Figure 5. Significant\ndifferences are observed across all criteria (p-value p < 0.05). Notably, our method outperforms all\nbaselines on each criterion, aligning with the results from the objective evaluation.\n5.5\nAblation Study on Style Prior Architecture\nWe now validate our design with the prior model by exclusively evaluating on the piano to multi-track\narrangement task. Our prior model is based on a unique layer interleaving design, which enables\nmulti-stream time series modelling with explicit stream-wise attention. We compare it with two other\nmulti-stream architectures: 1) Parallel: summing up parallel code embeddings for joint language\nmodelling [18], and 2) Delay: leveraging a 1-step delay code interleaving to catch implicit stream-\nwise dependency [4]. Both Parallel and Delay are trained under the same setup as our model. We\nadditionally introduce 3) Random: a naive prior based on random template retrieval. The templates\nare sampled every 2 bars with shared instrumentation from the validation/test sets of Slakh2100.\nWe introduce two metrics to evaluate piano to multi-track arrangement: faithfulness and degree\nof arrangement (DOA). Faithfulness measures if the generated arrangement faithfully reflects the\noriginal content from the piano. It computes the similarity between i) the input piano, and ii) the piano\nreduction of the generated multi-track arrangement. In our case, we compute cosine similarity over\ntwo features: a statistical (stats.) pitch class histogram [45] and a latent texture representation [39],\nwhich emphasize tonal and rhythmic similarity, respectively. DOA measures the creativity as defined\nin Section 5.3. We also report the NLL loss for our model, Parallel, and Delay.\nWe conduct experiments using the test set of POP909 [39], which consists of 88 piano arrangement\npieces. In our experiment, we use the first section of each piece, which contains 2 to 4 complete\nphrases totally spanning 24 to 32 bars. We use control option 3 to prompt our model, Parallel, and\nDelay with the same 2-bar orchestral function template (sampled from Slakh2100) and see how it\nis developed. We report mean value, standard error of mean (sem), and statistical significance in\nTable 3 and find significant differences in both faithfulness and DOA. We also conduct a subjective\nevaluation in the same setup as Section 5.4, with the results presented in Figure 6. Here we\nconsider an additional criterion, Instrumentation, which reflects the well-formedness of the multi-\n9\n\nTable 4: Ablation study on alternative lead sheet to piano arrangement (i.e., Stage 1) modules. Here\nwe investigate the impact of Stage 1 to the entire two-stage system. Evaluation results are based on\nthe final multi-track arrangement using respective Stage 1 modules.\nTwo-Stage System\nChord Acc. ↑\nStructure ↑\nDOA ↑\nOurs (Stage 1 + Stage 2)\n0.564 ± 0.014\n1.519 ± 0.030\n0.300 ± 0.005\nWhole-Song-Gen [42] + Stage 2\n0.509 ± 0.015\n1.121 ± 0.006\n0.277 ± 0.006\nTable 5: Evaluation results for lead sheet to piano arrangement exclusively on Stage 1.\nPiano Arr. Module\nChord Acc. ↑\nStructure ↑\nOurs (Piano Texture Prior)\n0.540 ± 0.016\n1.983 ± 0.147\nWhole-Song-Gen [42]\n0.430 ± 0.020\n1.153 ± 0.180\ntrack arrangement. Significant differences are observed across all criteria (p-value p < 0.05). Overall,\nParallel and Delay both fall short in performance because they assume a preset stream combination,\nwhile in our setting, both track numbers and choices of instruments are flexible. By explicitly\nmodelling stream-wise attention, our layer interleaving design fits well to that generalized scenario.\n5.6\nAblation Study on Piano Arrangement\nNow we validate our choice for the lead sheet to piano arrangement module on the first stage of\nour two-stage system. Our choice is a piano texture prior as covered in Section 4. We conduct an\nablation study by replacing it with the Whole-Song-Gen model [42], which, to our knowledge, is the\nonly existing alternative that can handle a whole-song structure. The ablation study is conducted in\nthe same setup as Section 5.3. In Table 4, we report chord accuracy, structure awareness, and DOA\nregarding the final multi-track arrangement results. We further compare our piano texture prior with\nWhole-Song-Gen exclusively on the piano accompaniment arrangement task. In Table 5, we report\nchord accuracy and structure awareness regarding piano arrangement for both models. Significant\ndifferences (p-value p < 0.05) are found in all metrics based on Wilcoxon signed rank test.\nBy comparing Table 4 and Table 5, we can see that a higher-quality piano arrangement generally\nencourages a more musical and creative final multi-track arrangement result. Specifically, the piano\narrangement on Stage 1 lays the groundwork for (at least) chord progression and phrase structure for\nStage 2, both of which are important for capturing the long-term structure in whole-song multi-track\narrangement. Moreover, we see that our piano texture prior outperforms existing alternatives and\nguarantees a decent piano quality, thus being the best choice for our system.\n6\nConclusion\nTo sum up, we contribute a music automation system for multi-track accompaniment arrangement.\nThe main novelty lies in our proposed style prior modelling, a generic methodology for structured\nsequence generation with fine-grained control. By modelling the prior of disentangled style factors\ngiven content, we build a cascaded arrangement process: from lead sheet to piano texture style, and\nthen from piano to orchestral function style. Our system first generates a piano accompaniment from a\nlead sheet, establishing the rough whole-song structure. It then orchestrates the piano accompaniment\ninto a complete multi-track arrangement with band instrumentation. Extensive experiments show that\nour system generates structured, creative, and natural multi-track arrangements with state-of-the-art\nquality. At a higher level, we elaborate our methodology as interpretable modular representation\nlearning, which leverages finely disentangled and manipulable music representations to tackle\ncomplex tasks with a compositional hierarchy. We hope our research brings new perspectives to\nbroader domains of music creation, sequence data modelling, and representation learning.\nReferences\n[1] Samuel Adler and Peter Hesterman. The study of orchestration, volume 2. WW Norton New\nYork, NY, 1989.\n10\n\n[2] Andrea Agostinelli, Timo I Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon,\nQingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: Generating\nmusic from text. arXiv preprint arXiv:2301.11325, 2023.\n[3] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A. Efros. Everybody dance now. In\n2019 IEEE/CVF International Conference on Computer Vision, pages 5932–5941, 2019.\n[4] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi,\nand Alexandre Défossez. Simple and controllable music generation. In Advances in neural\ninformation processing systems, volume 36, 2023.\n[5] Shuqi Dai, Huan Zhang, and Roger B Dannenberg. Automatic analysis and influence of\nhierarchical structure on melody, rhythm and harmony in popular music. arXiv preprint\narXiv:2010.07518, 2020.\n[6] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhut-\ndinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings\nof the 57th Conference of the Association for Computational Linguistics, pages 2978–2988,\n2019.\n[7] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya\nSutskever. Jukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020.\n[8] Hao-Wen Dong, Chris Donahue, Taylor Berg-Kirkpatrick, and Julian J. McAuley. Towards\nautomatic instrumentation by learning to separate parts in symbolic multitrack music. In\nProceedings of the 22nd International Society for Music Information Retrieval Conference,\npages 159–166, 2021.\n[9] Hao-Wen Dong, Ke Chen, Shlomo Dubnov, Julian McAuley, and Taylor Berg-Kirkpatrick.\nMultitrack music transformer. In International Conference on Acoustics, Speech and Signal\nProcessing, pages 1–5. IEEE, 2023.\n[10] Eric Foxley.\nNottingham database.\n[EB/OL], 2011.\nhttps://ifdo.ca/~seymour/\nnottingham/nottingham.html Accessed May 17, 2023.\n[11] Curtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian Simon, Cheng-Zhi Anna Huang, Sander\nDieleman, Erich Elsen, Jesse H. Engel, and Douglas Eck. Enabling factorized piano music\nmodeling and generation with the MAESTRO dataset. In 7th International Conference on\nLearning Representations, 2019.\n[12] Dan Hendrycks and Kevin Gimpel.\nGaussian error linear units (gelus).\narXiv preprint\narXiv:1606.08415, 2016.\n[13] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural\ntext degeneration. In 8th International Conference on Learning Representations, 2020.\n[14] Wen-Yi Hsiao, Jen-Yu Liu, Yin-Cheng Yeh, and Yi-Hsuan Yang. Compound word transformer:\nLearning to compose full-song music over dynamic directed hypergraphs. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 35, pages 178–186, 2021.\n[15] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne,\nNoam Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck.\nMusic transformer: Generating music with long-term structure. In 7th International Conference\non Learning Representations, 2019.\n[16] Rongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. Generspeech: Towards style\ntransfer for generalizable out-of-domain text-to-speech. In Advances in neural information\nprocessing systems, volume 35, 2022.\n[17] Junyan Jiang, Ke Chen, Wei Li, and Gus Xia. Large-vocabulary chord transcription via chord\nstructure decomposition. In Proceedings of the 20th International Society for Music Information\nRetrieval Conference, pages 644–651, 2019.\n11\n\n[18] Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu Anh\nNguyen, Morgane Rivière, Abdelrahman Mohamed, Emmanuel Dupoux, and Wei-Ning Hsu.\nText-free prosody-aware generative spoken language modeling. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics, pages 8666–8681, 2022.\n[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[20] Dmytro Kotovenko, Artsiom Sanakoyeu, Sabine Lang, and Björn Ommer. Content and style\ndisentanglement for artistic style transfer. In 2019 IEEE/CVF International Conference on\nComputer Vision, pages 4421–4430. IEEE, 2019.\n[21] Liwei Lin, Gus Xia, Qiuqiang Kong, and Junyan Jiang. A unified model for zero-shot music\nsource separation, transcription and synthesis. In Proceedings of the 22nd International Society\nfor Music Information Retrieval Conference, pages 381–388, 2021.\n[22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 6th International\nConference on Learning Representations, 2018.\n[23] Ang Lv, Xu Tan, Peiling Lu, Wei Ye, Shikun Zhang, Jiang Bian, and Rui Yan. Getmusic:\nGenerating any music tracks with a unified representation and diffusion framework. arXiv\npreprint arXiv:2305.10841, 2023.\n[24] Xichu Ma, Xiao Liu, Bowen Zhang, and Ye Wang. Robust melody track identification in\nsymbolic music. In Proceedings of the 23rd International Society for Music Information\nRetrieval Conference, pages 842–849, 2022.\n[25] Ethan Manilow, Gordon Wichern, Prem Seetharaman, and Jonathan Le Roux. Cutting music\nsource separation some slakh: A dataset to study the impact of training data quality and quantity.\nIn 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pages\n45–49, 2019.\n[26] Lejun Min, Junyan Jiang, Gus Xia, and Jingwei Zhao. Polyffusion: A diffusion model for\npolyphonic score generation with internal and external controls. In Proceedings of the 24th\nInternational Society for Music Information Retrieval Conference, pages 231–238, 2023.\n[27] Matthias Plasser, Silvan Peter, and Gerhard Widmer. Discrete diffusion probabilistic models for\nsymbolic music generation. In Proceedings of the Thirty-Second International Joint Conference\non Artificial Intelligence, pages 5842–5850, 2023.\n[28] Colin Raffel. Learning-Based Methods for Comparing Sequences, with Applications to Audio-\nto-MIDI Alignment and Matching. PhD thesis, Columbia University, USA, 2016.\n[29] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images\nwith vq-vae-2. In Advances in neural information processing systems, volume 32, 2019.\n[30] Yi Ren, Jinzheng He, Xu Tan, Tao Qin, Zhou Zhao, and Tie-Yan Liu. Popmag: Pop music\naccompaniment generation. In Proceedings of the 28th ACM International Conference on\nMultimedia, pages 1198–1206, 2020.\n[31] Henry Scheffe. The analysis of variance, volume 72. John Wiley & Sons, 1999.\n[32] Hao Hao Tan and Dorien Herremans. Music fadernets: Controllable music generation based on\nhigh-level features via low-level feature modelling. In Proceedings of the 21st International\nSociety for Music Information Retrieval Conference, pages 109–116, 2020.\n[33] John Thickstun, David Leo Wright Hall, Chris Donahue, and Percy Liang. Anticipatory music\ntransformer. Transactions on Machine Learning Research, 2024.\n[34] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances\nin neural information processing systems, volume 30, 2017.\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, volume 30, 2017.\n12\n\n[36] Dimitri von Rütte, Luca Biggio, Yannic Kilcher, and Thomas Hoffman. Figaro: Generating\nsymbolic music with fine-grained artistic control. In 11th International Conference on Learning\nRepresentations, 2023.\n[37] Ziyu Wang and Gus Xia. Musebert: Pre-training music representation for music understanding\nand controllable generation. In Proceedings of the 22nd International Society for Music\nInformation Retrieval Conference, pages 722–729, 2021.\n[38] Ziyu Wang, Ke Chen, Junyan Jiang, Yiyi Zhang, Maoran Xu, Shuqi Dai, and Gus Xia. POP909:\nA pop-song dataset for music arrangement generation. In Proceedings of the 21st International\nSociety for Music Information Retrieval Conference, pages 38–45, 2020.\n[39] Ziyu Wang, Dingsu Wang, Yixiao Zhang, and Gus Xia. Learning interpretable representation\nfor controllable polyphonic music generation. In Proceedings of the 21st International Society\nfor Music Information Retrieval Conference, pages 662–669, 2020.\n[40] Ziyu Wang, Yiyi Zhang, Yixiao Zhang, Junyan Jiang, Ruihan Yang, Gus Xia, and Junbo Zhao.\nPIANOTREE VAE: Structured representation learning for polyphonic music. In Proceedings\nof the 21st International Society for Music Information Retrieval Conference, pages 368–375,\n2020.\n[41] Ziyu Wang, Dejing Xu, Gus Xia, and Ying Shan. Audio-to-symbolic arrangement via cross-\nmodal music representation learning. In International Conference on Acoustics, Speech and\nSignal Processing, pages 181–185. IEEE, 2022.\n[42] Ziyu Wang, Lejun Min, and Gus Xia. Whole-song hierarchical generation of symbolic music\nusing cascaded diffusion models. In 12th International Conference on Learning Representations,\n2024.\n[43] Frank Wilcoxon. Individual comparisons by ranking methods. In Breakthroughs in statistics:\nMethodology and distribution, pages 196–202. Springer, 1992.\n[44] Shangda Wu, Dingyao Yu, Xu Tan, and Maosong Sun. Clamp: Contrastive language-music\npre-training for cross-modal symbolic music information retrieval. In Proceedings of the 24th\nInternational Society for Music Information Retrieval Conference, pages 157–165, 2023.\n[45] Shih-Lun Wu and Yi-Hsuan Yang. The jazz transformer on the front line: Exploring the\nshortcomings of ai-composed music through quantitative measures. In Proceedings of the 21st\nInternational Society for Music Information Retrieval Conference, pages 142–149, 2020.\n[46] Ruihan Yang, Dingsu Wang, Ziyu Wang, Tianyao Chen, Junyan Jiang, and Gus Xia. Deep music\nanalogy via latent representation disentanglement. In Proceedings of the 20th International\nSociety for Music Information Retrieval Conference, pages 596–603, 2019.\n[47] Li Yi, Haochen Hu, Jingwei Zhao, and Gus Xia. Accomontage2: A complete harmonization\nand accompaniment arrangement system. In Proceedings of the 23rd International Society for\nMusic Information Retrieval Conference, pages 248–255, 2022.\n[48] Wenjie Yin, Hang Yin, Kim Baraka, Danica Kragic, and Mårten Björkman. Dance style transfer\nwith cross-modal transformer. In IEEE/CVF Winter Conference on Applications of Computer\nVision, pages 5047–5056, 2023.\n[49] Siyang Yuan, Pengyu Cheng, Ruiyi Zhang, Weituo Hao, Zhe Gan, and Lawrence Carin. Improv-\ning zero-shot voice style transfer via disentangled representation learning. In 9th International\nConference on Learning Representations, 2021.\n[50] Jingwei Zhao and Gus Xia. Accomontage: Accompaniment arrangement via phrase selection\nand style transfer. In Proceedings of the 22nd International Society for Music Information\nRetrieval Conference, pages 833–840, 2021.\n[51] Jingwei Zhao, Gus Xia, and Ye Wang. Q&A: Query-based representation learning for multi-\ntrack symbolic music re-arrangement. In Proceedings of the Thirty-Second International Joint\nConference on Artificial Intelligence, pages 5878–5886, 2023.\n13\n\nA\nImplementation Details\nA.1\nAutoencoder\nThe autoencoder consists of a VQ-VAE submodule and an overarching VAE. The encoder of the VQ-\nVAE consists of a 1-D convolutional layer of kernal size 4, stride 4, and 16 output channels, followed\nby a vector quantization block with codebook size 128. The decoder takes the concatenated latent\ncodes and leverages two fully-connected layers (shape 128×256 and 256×32) for reconstruction. In\nthe overarching VAE, Piano Encoder and Track Decoder are adapted from PianoTree VAE [40]. The\nencoder first applies a pitch-wise bi-directional GRU to summarize concurrent notes at time step n\nand then applies a time-wise GRU to encode the full representation. The decoder mirrors the encoder\nstructure with time- and pitch-wise uni-directional GRUs to reconstruct individual tracks. We use\nhidden size 256 in a single layer for pitch GRUs and 512 for time GRUs. The Track Separator is a\n2-layer Transformer encoder with 8 attention heads, 0.1 dropout ratio, and GELU activation [12]. The\nhidden dimensions of self-attention dmodel and feed-forward layers dff are 512 and 1024, respectively.\nThe autoencoder is trained with joint reconstruction loss for orchestral function (MSE) and individual\ntracks (cross entropy). The VQ-VAE is additionally regularized with latent loss and commitment loss\nwith commitment ratio β = 0.25. The VAE is regularized with KL loss over all continuous factors\n(ct and z1:K\nt\n) based on KL annealing [41] with a ratio exponentially increasing from 0 to 0.5.\nA.2\nPrior Model\nThe prior model consists of a 12-layer Context Encoder and a 12-layer Auto-Regressive Decoder.\nThe latter is interleaved with another 12 track-wise Track Encoder layers. For each layer, we apply\n8 attention heads, 0.1 dropout ratio, and GELU activation. We apply layer normalization before\nself-attention (i.e., norm first). The hidden dimensions of self-attention dmodel and feed-forward\nlayers dff are 256 and 1024, respectively. We apply relative positional embedding [15] to Track\nEncoder so that two tracks initialized with identical instruments can still generate different content.\nOur prior model is trained on the latent codes c1:T and s1:K\n1:8T inferred by a well-trained autoencoder\non LMD. For discrete code s, we take the codebook indices and learn a new embedding.\nB\nObjective Evaluation Metrics\nB.1\nDegree of Arrangement\nIn multi-track arrangement, parallel tracks typically play a unique role to each other in the overall\narrangement. We are interested in capturing the diversity and creativity inherent in these roles.\nTo achieve this, we consider the pitch class histogram [45] as a probability distribution P. Let Pt,k\nbe the distribution of the t-th bar in track k, and P pn\nt\nbe that of the t-th bar in the piano reduction.\nRecall that in this paper we approximate the piano reduction of a multi-track piece by downmixing all\ntracks. Both Pt,k and P pn\nt\nare 12-D vectors, describing tonality of individual tracks and the overall\narrangement, respectively. We compute the KL divergence of each track to the piano reduction:\ndk = 1\nT\nT\nX\nt=1\nKL(Pt,k ∥P pn\nt ),\n(5)\nwhere T is the total number of bars.\nInterpreting dk in terms of KL divergence, we see it as the “excess surprise” from the overall\narrangement (piano reduction) when track k is played in isolation. A large dk indicates that track k\npossesses a unique quality, such as a bass track playing the root or a counter-melody track focusing\non tensions. Conversely, a small dk suggests that track k serves as a foundational element in the\narrangement, such as string padding that establishes the harmonic foundation.\nIf all dk values are small, it implies homogeneity across tracks and thus a low degree of arrangement.\nConversely, if all dk values are high, it suggests a composition dominated by counterpoints, a scenario\nless common in pop music. A well-orchestrated piece typically exhibits a diverse range of dk\nvalues, encompassing both foundational and unique decorative tracks. We thus define the degree of\n14\n\narrangement DOA as the standard deviation of dk for k = 1, 2, · · · , K across all tracks:\nDOA =\nsPK\nk=1(dk −d)2\nK\n,\n(6)\nwhere d is the mean. K is the total number of tracks. To establish a reference point, we calculate the\nground-truth DOA = 0.333 based on 1000 randomly selected pieces from the LMD dataset. Within\nthis context, a higher DOA signifies a more creative arrangement.\nB.2\nStrcture Awareness\nWe introduce Inter-phrase Latent Similarity (ILS) from [42] to measure the structural awareness of\nlong-term arrangement. ILS calculates the content similarity among same-type phrases (e.g., chorus)\nversus the whole song. It leverages pre-trained disentangled VAEs that encode music notes into latent\nrepresentations and then compare cosine similarities in the latent space. In our case, we compute\nILS over the piano reduction of a generated arrangement since it contains the overall content. We\napply the texture VAE [39] and obtain a latent texture representation ctxt\nt\nfor every 2-bar segment.\nFor odd-numbered phrases, we repeat its final bar and pad it to the end of the phrase. Suppose there\nare M different types of phrases in one piece and let Im be the set of segment indices in the type-m\nphrase, ILS is defined as the ratio between same-type phrase similarity and global average similarity:\nILS =\n(PM\nm=1\nP\ni̸=j∈Im cos(ctxt\ni\n, ctxt\nj ))/(PM\nm=1 |Im|2 −|Im|)\nP\n1≤i̸=j≤T cos(ctxt\ni\n, ctxt\nj )/(T 2 −T)\n,\n(7)\nwhere | · | is the cardinality of a set. T is the number of 2-bar segments. When applying ILS, we\nuse [5] to automatically lable the phrase structure of a piece. To establish a reference point, we\ncalculate the ground-truth ILS = 1.980 based on the POP909 dataset (with phrase annotation by\nhuman). Within this context, a higher ILS signifies saliency with long-term phrase-level structure.\nB.3\nChord Accuracy\nWe introduce chord accuracy from [30] to measure if the chords of the generated arrangement match\nthe conditional chord sequence in the lead sheet. It reflects the harmonicity of the generated music\nand is defined as follows:\nCA =\n1\nNchord\nNchord\nX\ni=1\n1{Ci= ˆ\nCi},\n(8)\nwhere Nchord is the number of chords in a piece; Ci is the i-th chord in the (ground-truth) lead sheet;\nand ˆCi is the aligned chord in the generated arrangement.\nThe original formulation in [30] considers chord accuracy for individual tracks. Given our system’s\ncapability to accommodate a variable combination of tracks, we opt for a broader evaluation for the\noverall arrangement. In our case, we extract the chord sequence of a generated arrangement with [17]\nand compare it in root and quality with ground-truth at 1-beat granularity, which is more rigorous.\nB.4\nOrchestration Faithfulness\nWe measure the faithfulness of orchestration by the similarity between i) the input piano and ii) the\npiano reduction of the generated multi-track arrangement. Let ein\nt and epn\nt\nbe vector features derived\nfrom the t-th segment of the input and the reduction, respectively. Orchestration faithfulness OF is\ndefined as follows:\nOF = 1\nT\nT\nX\nt=1\ncos(ein\nt , epn\nt ),\n(9)\nwhere cos(·, ·) is cosine similarity. T is the number of segments.\nIn our work, we select two options for vector feature e. One is a statistical pitch class histogram [45],\nwhich is a 12-D vector describing pitch class distribution. The other is a latent 256-D texture\nrepresentation learned by a pre-trained VAE [39]. Both features are general descriptors of the musical\ncontent with respective focus on tonal harmony and rhythmic grooves.\n15\n\nC\nExperiment on Noise Weight γ\nContinuing from Section 3.3, we compare different γ values and see their impact to the model\nperformance. When applying our model to piano to multi-track arrangement, γ balances the force\nof a noisy factor added to the piano, which encourages a partial unconditional generation capability.\nThe experimental settings are the same as Section 5.5. We evaluate the results based on faithfulness\nand DOA. In Table 6, we report mean value, standard error of mean (sem), and statistical significance\ncomputed by Wilcoxon signed rank test. By varying the γ value, we observe a controllable balance\nbetween faithfulness and creativity. Specifically, a larger γ encourages creativity (higher DOA) at the\ncost of faithfulness. If not mentioned otherwise, we use γ = 0.25 for experiments in this paper.\nTable 6: Objective evaluation results on the impact of noise weight γ in Appendix C.\nNoise Weight γ\nFaithfulness (stats.) ↑\nFaithfulness (latent) ↑\nDOA ↑\nγ = 0\n0.946 ± 0.001a\n0.228 ± 0.005a\n0.300 ± 0.005c\nγ = 0.25\n0.945 ± 0.001ab\n0.215 ± 0.005b\n0.308 ± 0.005bc\nγ = 0.5\n0.944 ± 0.001b\n0.187 ± 0.004c\n0.320 ± 0.006ab\nγ = 1\n0.936 ± 0.002c\n0.127 ± 0.003d\n0.325 ± 0.007a\nD\nOnline Survey Specifics\nWe distribute our survey via SurveyMonkey.4 Our survey consists of 5 sample sets for both the lead\nsheet to multi-track and the piano to multi-track arrangement tasks (10 sets in total). Each sample is\n24-32 bars long and is synthesized to audio at 90 BPM using BandLab5 with the default soundfont.\nEach participant listens to 2 sets (in random order) and the mean time spent is 22 minutes. Figure 7\nshows the sample pages of our survey with instructions to the participants.\n(a) Lead sheet to multi-track arrangement.\n(b) Piano to multi-track arrangement.\nFigure 7: Screenshots of survey pages and instructions of our online survey.\n4https://www.surveymonkey.com\n5https://www.bandlab.com/\n16\n\nE\nExample on Structured Arrangement\nWe demonstrate an example of accompaniment arrangement by our proposed system. The input lead\nsheet is a complete pop song shown in Section E.1. Our system first arranges a piano accompaniment\nfor the whole song, which is shown in Section E.2. The piano score is then orchestrated into a\nmulti-track arrangement with customized instrumentation, which is shown in Section E.3.\nE.1\nLead Sheet\nWe use our system to arrange for Can You Feel the Love Tonight, a pop song by Elton John. As shown\nin Figure 8, the entire song is 60 bars long and it presents a structure of i4A8B8B8x4A8B8B8O4,\nwhere i, x, O, A, and B each refer to intro, interlude, outro, verse, and chorus.\n25\n33\n5\n9\n29\n21\n17\n13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n = 65\nB8\nB8\nO4\nx4\ni4\nA8\nE♭\nCm\nGm\nB♭/D\nB♭/D\nB♭/D\nCm7\nB♭\nB♭\nCm/E♭\nE♭\nF7sus\nE♭\nF/A\nB♭/D\nB♭/F\nB♭\nE♭\nB♭/D\nE♭/B♭\nCm/E♭\nB♭\nF\nC/E\nE♭\nE♭\nE♭\nB♭/D\nB♭\nB♭/D\nB♭/D\nF/A\nC7/E\nGm\nF/A\nF\nF\nE♭\nGm\nC7/E\nGm\nF\nCm\nB♭/D\nE♭\nF/A\nB♭/D\nE♭\nE♭/G\nE♭\nE♭\nB♭/D\nA♭\nGm\nCm/E♭\nB♭\nCm7\nF7sus\nE♭\nE♭/B♭\nGm\nF/A\nB♭\nCm7\nB♭/F\nE♭\nE♭\nB♭/D\nE♭/G\nE♭\nB♭/D\nF/A\nCm7\nB♭\nB♭\nB♭\nF/A\nE♭\nB♭/D\nE♭\nB♭/D\n1.\n2.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Lead sheet for pop song Can You Feel the Love Tonight.\n17\n\nE.2\nPiano Arrangement\nThe piano arrangement result is shown from Figure 9 to Figure 10. It roughly establishes a whole-song\nstructure and lays the groundwork for band orchestration at the next stage. Demo audio for the piano\nscore is available at https://zhaojw1998.github.io/structured-arrangement/.\n25\n17\n13\n9\n21\n5\nMel.\nMelody\nMel.\nMel.\nMel.\nMel.\nMel.\nPno.\nPno.\nPno.\nPno.\nPno.\nPno.\nPiano\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n = 65\nB8\ni4\nA8\nB8\nE♭\nCm/E♭\nE♭\nB♭/D\nB♭\nB♭/D\nGm\nGm\nE♭\nE♭/G\nE♭\nB♭\nB♭/D\nCm7\nB♭\nF/A\nB♭/D\nE♭\nC7/E\nB♭/D\nB♭/D\nE♭\nB♭/D\nE♭\nCm/E♭\nF/A\nCm7\nB♭\nF/A\nGm\nB♭\nE♭\nF\nF\nB♭\nE♭\nE♭\nE♭\nF/A\nGm\nB♭/D\nC/E\nF\nE♭\nB♭/D\nC7/E\nB♭/D\nCm7\nB♭\nB♭/D\nE♭\nB♭/F\nF\nCm\nE♭\nF7sus\nA♭\nE♭/B♭\nF/A\nGm\nE♭\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Piano arrangement score (page 1).\n18\n\n29\n45\n33\n37\n53\n49\n41\n57\nMel.\nMel.\nMel.\nMel.\nMel.\nMel.\nMel.\nMel.\nPno.\nPno.\nPno.\nPno.\nPno.\nPno.\nPno.\nPno.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA8\nB8\nO4\nB8\nx4\nCm\nGm\nE♭\nB♭\nF\nB♭/D\nGm\nB♭\nF\nB♭/D\nC/E\nB♭/D\nGm\nE♭\nCm7\nGm\nE♭\nE♭\nB♭/D\nB♭/D\nGm\nF/A\nCm/E♭\nB♭\nF\nF/A\nE♭/B♭\nB♭/D\nE♭\nB♭/D\nE♭\nF7sus\nB♭\nB♭/D\nCm/E♭\nB♭/F\nE♭/G\nF/A\nE♭\nB♭/D\nE♭\nA♭\nCm\nB♭\nE♭\nE♭\nF/A\nE♭\nB♭\nF\nC7/E\nCm7\nB♭/D\nF/A\nB♭/D\nCm7\nE♭\nE♭\nE♭\nE♭\nE♭/B♭\nB♭/D\nC7/E\nE♭\nB♭\nF7sus\nGm\nB♭\nCm/E♭\nB♭/D\nE♭\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Piano arrangement score (page 2, last page).\n19\n\nE.3\nMulti-Track Arrangement\nThe multi-track arrangement is shown from Figure 11 to Figure 15. We customize the instrumentation\nas celesta, acoustic guitars (2), electric pianos (2), acoustic piano, violin, brass, and electric bass in a to-\ntal of K = 9 tracks. We can see that the structure of the accompaniment follows the lead sheet. Demo\naudio is available at https://zhaojw1998.github.io/structured-arrangement/. More de-\ntailed analysis on this arrangement demo is covered in Section 4.\n5\nMel.\nMelody\nCelesta\nAcoustic Guitar 1\nAcoustic Guitar 2\nA.G. 2\nPiano\nElectric Piano 1\nE.P. 1\nElectric Piano 2\nE.P. 2\nVlns.\nViolins\nBr.\nBrass\nE.B.\nElectric Bass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n = 65\nA8\ni4\nE♭/G\nF/A\nF/A\nB♭/D\nB♭\nB♭\nCm7\nCm7\nB♭/D\nB♭/D\nE♭\nE♭\nB♭/D\nB♭/D\nE♭\nF/A\nB♭/F\nE♭\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: Multi-track arrangement score (page 1).\n20\n\n17\n13\n9\nMel.\nMel.\nMel.\nA.G. 1\nA.G. 1\nA.G. 1\nA.G. 2\nA.G. 2\nA.G. 2\nPno.\nE.P. 1\nE.P. 1\nE.P. 1\nE.P. 2\nVlns.\nVlns.\nVlns.\nBr.\nBr.\nBr.\nE.B.\nE.B.\nE.B.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB8\nGm\nF\nE♭\nGm\nB♭/D\nA♭\nC/E\nE♭\nC7/E\nB♭\nCm/E♭\nB♭/D\nF\nB♭/D\nE♭\nCm7\nE♭\nB♭\nE♭\nF/A\nE♭\nGm\nF\nE♭\nB♭/D\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: Multi-track arrangement score (page 2).\n21\n\n33\n21\n25\n29\nMel.\nMel.\nMel.\nMel.\nA.G. 1\nA.G. 1\nA.G. 1\nA.G. 2\nA.G. 2\nA.G. 2\nPno.\nPno.\nE.P. 2\nVlns.\nVlns.\nBr.\nBr.\nE.B.\nE.B.\nE.B.\nE.B.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx4\nA8\nB8\nB♭/D\nB♭/D\nE♭\nF/A\nE♭\nCm\nB♭/D\nE♭\nE♭\nE♭/B♭\nB♭\nGm\nE♭\nF/A\nB♭\nB♭/F\nB♭\nB♭\nCm/E♭\nCm7\nB♭/D\nE♭\nB♭/D\nF/A\nF/A\nB♭\nF7sus\nB♭/D\nB♭/D\nCm7\nE♭\nE♭\nF\nE♭/G\nC7/E\nGm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Multi-track arrangement score (page 3).\n22\n\n37\n41\n45\nMel.\nMel.\nMel.\nCel.\nA.G. 1\nA.G. 1\nA.G. 2\nA.G. 2\nPno.\nPno.\nPno.\nVlns.\nVlns.\nBr.\nBr.\nE.B.\nE.B.\nE.B.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB8\nE♭\nCm7\nCm/E♭\nB♭/D\nB♭/D\nA♭\nF/A\nE♭\nGm\nB♭/D\nB♭\nC7/E\nC/E\nF\nE♭\nF\nE♭\nB♭/D\nE♭\nE♭\nGm\nB♭\nGm\nE♭\nF\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14: Multi-track arrangement score (page 4).\n23\n\n49\n57\n53\nMel.\nMel.\nMel.\nA.G. 1\nA.G. 1\nA.G. 2\nA.G. 2\nA.G. 2\nPno.\nPno.\nE.P. 1\nE.P. 2\nVlns.\nVlns.\nVlns.\nBr.\nBr.\nE.B.\nE.B.\nE.B.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nO4\nB8\nCm/E♭\nE♭\nF7sus\nE♭\nE♭/B♭\nE♭\nCm/E♭\nGm\nE♭\nGm\nF/A\nB♭\nB♭\nE♭\nC7/E\nF\nCm\nB♭\nGm\nE♭\nB♭\nB♭/D\nCm\nE♭/B♭\nB♭/D\nF7sus\nB♭/D\nB♭/D\n\n\n\n\n\n\n\n\n\nFigure 15: Multi-track arrangement score (page 5, last page).\n24\n\nF\nLimitation\nWe propose a two-stage system for whole-song, multi-track accompaniment arrangement. In the\ncontext of this paper, we acknowledge that our current system exclusively supports tonal tracks in\nquadruple meters while disregarding triple meters, triplet notes, and drums. However, we perceive\nthis as a technical limitation rather than a scientific challenge. We also acknowledge that our\ncurrent system primarily emphasizes the composition level, thereby omitting the modelling of MIDI\nvelocity, dynamic timing, and MIDI control messages. Consequently, the generated results do not\nencompass performance MIDI and may lack expressive qualities. Nevertheless, we believe that our\ncomposition-centric work serves as a solid and vital foundation for further advancements in those\nspecific areas, thus facilitating the development of enhanced techniques and features. As a pioneering\nwork, our system is the foremost accomplishment in solving whole-song multi-track accompaniment\narrangement, characterized by flexible controllability on track number and choice of instruments.\nG\nBroader Impacts\nOur multi-track accompaniment arrangement system, which incorporates style to generate accom-\npaniment, is designed to enhance originality and creativity. It serves as a platform for human-AI\nco-creation, where the user provides content-based material (in our case, lead sheet) that remains\nfundamentally original, while the AI agent infuses style, enriches the form, and enhances creativity.\nOur system therefore empowers musicians to explore new musical ideas and expand their creative\nboundaries. This approach also allows for rapid mock-up with different styles and arrangements,\nfostering an environment where innovation and artistic expression can thrive.\nHowever, we acknowledge the need to address potential risks. The accessibility of our system may\ninadvertently lead to excessive reliance on automation, potentially impeding the development of\nfundamental skills among musicians. Additionally, widespread adoption of the system may contribute\nto the homogenization of music, threatening the distinctiveness and individuality that are crucial to\nartistic expression. We recognize that our datasets predominantly features contemporary Western\nmusic, which introduces a cultural bias that could limit the diversity of generated compositions.\n25",
    "pdf_filename": "Structured_Multi-Track_Accompaniment_Arrangement_via_Style_Prior_Modelling.pdf"
}