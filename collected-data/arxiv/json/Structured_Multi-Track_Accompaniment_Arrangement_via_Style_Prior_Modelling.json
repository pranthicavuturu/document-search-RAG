{
    "title": "Structured Multi-Track Accompaniment Arrangement",
    "abstract": "In the realm of music AI, arranging rich and structured multi-track accompani- mentsfromasimpleleadsheetpresentssignificantchallenges. Suchchallenges includemaintainingtrackcohesion,ensuringlong-termcoherence,andoptimizing computationalefficiency. Inthispaper,weintroduceanovelsystemthatleverages priormodellingoverdisentangledstylefactorstoaddressthesechallenges. Our method presents a two-stage process: initially, a piano arrangement is derived fromtheleadsheetbyretrievingpianotexturestyles;subsequently,amulti-track orchestration is generated by infusing orchestral function styles into the piano arrangement. Ourkeydesignistheuseofvectorquantizationandauniquemulti- streamTransformertomodelthelong-termflowoftheorchestrationstyle,which enablesflexible,controllable,andstructuredmusicgeneration. Experimentsshow thatbyfactorizingthearrangementtaskintointerpretablesub-stages,ourapproach enhancesgenerativecapacitywhileimprovingefficiency. Additionally,oursystem supportsavarietyofmusicgenresandprovidesstylecontrolatdifferentcompo- sitionhierarchies. Wefurthershowthatoursystemachievessuperiorcoherence, structure,andoverallarrangementqualitycomparedtoexistingbaselines. 1 Introduction Representationlearningtechniqueshaveenablednewpossibilitiesforcontrollablegenerativemod- elling. Bylearningimplicitstylerepresentations,whichareoftenhardtoexplicitlylabel(e.g.,timbre ofmusic audio[21], texture ofmusiccomposition[39], andartistic style inpaintings[20]), new musicandartworkscanbecreatedviastyletransferandlatentspacesampling. Theselearnedstyle factorscanalsoserveasexternalcontrolsfordownstreamgenerativemodels,includingTransform- ers [18, 36] and diffusion models [42]. HowNever, applying style factors to long-term sequence generationremainsachallengingtask. Existingapproachesrelyonstyletemplatesspecifiedmanually orbyheuristicrules[36,42,51],whichareimpracticalforlong-termgeneration. Moreover,when structuralconstraintsareimposed,misalignedstylefactorscanresultinincoherentoutputs. Toaddressthesechallenges,weaimtodevelopanovelsequencegenerationframeworkleveraginga globalstyleplanner,orprior,whichmodelstheconditionaldistributionofstylefactorsgiventhe modelinput’scontentfactors. Bothstyleandcontentfactorsaresequencesofcompact,structurally aligned latent codes over a disentangled representation space. By infusing the style back to the content,wecanrecovertheobservationaltargetwithgloballycoherentstylepatterns. Inthispaper,westudystylepriormodellingthroughthetaskofmulti-trackaccompanimentarrange- ment,atypicalscenarioforlong-termconditionalsequencegeneration. Weassumetheinputofa 38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024). 4202 voN 91 ]DS.sc[ 3v43361.0132:viXra",
    "body": "Structured Multi-Track Accompaniment Arrangement\nvia Style Prior Modelling\nJingweiZhao1,3 GusXia4,5 ZiyuWang5,4 YeWang2,1,3\n1InstituteofDataScience,NUS 2SchoolofComputing,NUS\n3IntegrativeSciencesandEngineeringProgramme,NUSGraduateSchool\n4MachineLearningDepartment,MBZUAI 5ComputerScienceDepartment,NYUShanghai\njzhao@u.nus.edu gus.xia@mbzuai.ac.ae\nziyu.wang@nyu.edu wangye@comp.nus.edu.sg\nAbstract\nIn the realm of music AI, arranging rich and structured multi-track accompani-\nmentsfromasimpleleadsheetpresentssignificantchallenges. Suchchallenges\nincludemaintainingtrackcohesion,ensuringlong-termcoherence,andoptimizing\ncomputationalefficiency. Inthispaper,weintroduceanovelsystemthatleverages\npriormodellingoverdisentangledstylefactorstoaddressthesechallenges. Our\nmethod presents a two-stage process: initially, a piano arrangement is derived\nfromtheleadsheetbyretrievingpianotexturestyles;subsequently,amulti-track\norchestration is generated by infusing orchestral function styles into the piano\narrangement. Ourkeydesignistheuseofvectorquantizationandauniquemulti-\nstreamTransformertomodelthelong-termflowoftheorchestrationstyle,which\nenablesflexible,controllable,andstructuredmusicgeneration. Experimentsshow\nthatbyfactorizingthearrangementtaskintointerpretablesub-stages,ourapproach\nenhancesgenerativecapacitywhileimprovingefficiency. Additionally,oursystem\nsupportsavarietyofmusicgenresandprovidesstylecontrolatdifferentcompo-\nsitionhierarchies. Wefurthershowthatoursystemachievessuperiorcoherence,\nstructure,andoverallarrangementqualitycomparedtoexistingbaselines.\n1 Introduction\nRepresentationlearningtechniqueshaveenablednewpossibilitiesforcontrollablegenerativemod-\nelling. Bylearningimplicitstylerepresentations,whichareoftenhardtoexplicitlylabel(e.g.,timbre\nofmusic audio[21], texture ofmusiccomposition[39], andartistic style inpaintings[20]), new\nmusicandartworkscanbecreatedviastyletransferandlatentspacesampling. Theselearnedstyle\nfactorscanalsoserveasexternalcontrolsfordownstreamgenerativemodels,includingTransform-\ners [18, 36] and diffusion models [42]. HowNever, applying style factors to long-term sequence\ngenerationremainsachallengingtask. Existingapproachesrelyonstyletemplatesspecifiedmanually\norbyheuristicrules[36,42,51],whichareimpracticalforlong-termgeneration. Moreover,when\nstructuralconstraintsareimposed,misalignedstylefactorscanresultinincoherentoutputs.\nToaddressthesechallenges,weaimtodevelopanovelsequencegenerationframeworkleveraginga\nglobalstyleplanner,orprior,whichmodelstheconditionaldistributionofstylefactorsgiventhe\nmodelinput’scontentfactors. Bothstyleandcontentfactorsaresequencesofcompact,structurally\naligned latent codes over a disentangled representation space. By infusing the style back to the\ncontent,wecanrecovertheobservationaltargetwithgloballycoherentstylepatterns.\nInthispaper,westudystylepriormodellingthroughthetaskofmulti-trackaccompanimentarrange-\nment,atypicalscenarioforlong-termconditionalsequencegeneration. Weassumetheinputofa\n38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).\n4202\nvoN\n91\n]DS.sc[\n3v43361.0132:viXra\npianoaccompanimentscore,whichtypicallycarriesaverse-chorusstructure. Ourtargetistogenerate\ncorrespondingmulti-trackarrangementsfeaturingbandorchestration. Westartbydisentanglinga\nbandscoreattimetintopianoreductionc (contentfactor)andorchestralfunctionsk (stylefactors\nt t\nforindividualtracksk = 1,2,··· ,K). Ontopofthis,wemodeltheprioroffindingappropriate\nfunctionstoorchestrateagivenpianoscore,orformallyp(s1:K | c ). Tomodeldependencies\n1:T 1:T\nonbothtime(T)andtrack(K)directions,wedevelopamulti-streamTransformerwithinterleaved\ntime-wiseandtrack-wiselayers. Thetrack-wiselayerallowsforflexiblecontroloverthechoice\nof instruments and the number of tracks, while the time-wise layer ensures structural alignment\nthroughcross-attentiontothepianoreduction. Decodingtheinferreds1:K withc ,wecanaddress\n1:T 1:T\naccompanimentarrangementinaflexiblemulti-trackformwithextendedwhole-songstructure.\nExperimentsshowthatourmethodoutperformsexistingsequentialtokenpredictionapproachesand\nprovidesbettermulti-trackcohesion,structuralcoherence,andcomputationalefficiency. Additionally,\ncomparedtoexistingdesignsofmulti-streamlanguagemodels,ourmodelhandlesflexiblestream\ncombinationsmoreeffectivelywithenhancedgenerativecapacity.\nTosummarize,ourcontributionsinthispaperarethree-folded:\n• Weproposestylepriormodelling,ahierarchicalgenerativemethodologyaddressing\nbothlong-termstructure(viastylepriorathighlevel)andfine-grainedcondition/control(via\nrepresentationdisentanglementatlowlevel). Ourapproachmovesbeyondthelimitation\nofmanualspecificationofstylefactors,providingaflexible,efficient,andself-supervised\nsolutionforlong-termsequencepredictionandgenerationtasks.\n• Weproposeanovellayerinterleavingarchitectureformulti-streamlanguagemodelling.\nIn our case, it models parallel music tracks with a flexible track number, controllable\ninstruments,andmanageablecomputation. Toourknowledge,itisthefirstmulti-stream\nlanguagemodelwithtractablegeneralizationtoflexiblestreamcombinations.\n• Integratingourpreviousstudyonpianotexturestyletransfer[39,50],wepresentacomplete\nmusicautomationsystemarranginganinputleadsheet(abasicmusicformwithmelody\nandchordonly)viapianoaccompanimenttomulti-trackarrangement. Theentiresystemis\ninterpretableattwocompositionhierarchies: 1)pianotextureand2)orchestralfunction,\nanddemonstratesstate-of-the-artarrangementperformanceforvariedgenresofmusic.1\n2 RelatedWorks\nInthissection,weoverviewthreetopicsrelatedtoourstudy. Section2.1reviewsexistingstudieson\nrepresentationdisentanglement.Section2.2summarizespriormodellingmethodsinmusicgeneration.\nSection2.3reviewsthecurrentprogresswiththetaskofaccompanimentarrangement.\n2.1 Content-StyleDisentanglementviaRepresentationLearning\nRepresentationdisentanglementisapopulartechniqueindeepgenerativemodelling[3,16,48,49].\nInthemusicdomain,thisapproachhasprovenvaluablebylearningcompositionalfactorsrelatedto\nmusicstyleandcontent. Bymanipulatingthesefactorsthroughinterpolation[32],swapping[39],\nandpriorsampling[46],itprovidesaself-supervisedandcontrollablepathwayforvariousmusic\nautomationtasks. Recentworksleveragedisentangledstylefactorsascontrolsignalsforlong-term\nmusicgeneration[36,42]. However,theseapproachestypicallytreatstylerepresentationsasfixed\ncondition sequences during training, requiring manual specification or additional algorithms for\ncontrolduringinference. Incontrast,wemodelthepriorofthestyletoapplyconditionalonthegiven\nmusiccontent,whichisamoregeneralizedandflexibleapproach.\n2.2 MusicGenerationwithLatentPrior\nInsequencegenerationtasks(e.g.,musicandaudio),learningapriorsampleroveracompact,latent\nrepresentationspaceisoftenmoreefficientandeffective.Jukebox[7]modelsthelatentcodesencoded\nby VQ-VAEs [34] as music priors, which can further reconstruct minutes of music audio. More\nrecently,MusicLM[2]andMusicGen[4]learnmulti-modalpriorsforgeneratingmusicfromtext\n1Demoandmoreresources:https://zhaojw1998.github.io/structured-arrangement/\n2\nTable1: Summaryofthedatarepresentationsappliedinthispaper. Weusenotation[a..b]todenote\ntheintegerinterval{x|a≤x≤b,x∈Z}includingbothendpoints.\nMulti-TrackArrangement PianoReduction OrchestralFunction\nDataRepresentation x∈[0..32]T×K×32×128 pn[x]∈[0..32]T×32×128 fn[x]∈[0,1]T×K×32\nLatentDimension z∈RT×256 c∈RT×256 s∈[0..127]8T×K\nprompts. Whilepriormodellingfacilitateslong-termgeneration,thelatentcodesintheseworksare\nnotinterpretable,thuslackingaprecisecontrolbymusiccontent-basedsignals(e.g.,musicstructure).\nSuchcontrolsareessentialforconditionalgenerationtasks,includingaccompanimentarrangement.\nInthispaper,wemodelastylepriorconditionalonthedisentangledmusiccontent,whichallowsfor\nstructuredlong-termmusicgeneration,enhancingbothinterpretabilityandcontrollability.\n2.3 AccompanimentArrangement\nAccompanimentarrangementaimstocomposetheaccompanimentpartgivenaleadsheet,whichisa\ndifficultconditionalgenerationtaskinvolvingstructuralconstraints. Existingmethodsmainlytraina\nconditionallanguagemodelbasedonsequentialnote-leveltokenization[14,15,30,33],whichoften\nsuffer from slow inference speed, truncated structural context, and/or simplified instrumentation.\nRecentattemptswithdiffusionmodelsshowhighersamplequalitywithfasterinference[23,26,27],\nbut still consider limited instruments or tracks. AccoMontage [47, 50] maintains a whole-song\nstructurebymanipulatinghigh-levelcompositionfactors,butislimitedtopianoarrangementalone.\nOurpaperpresentsatwo-stageapproach: fromleadsheettopianoaccompaniment,andfrompiano\nto multi-track, both leveraging prior modelling of high-level style factors. This approach offers\nmodularity[11]andenableshigh-qualitywhole-songandmulti-trackaccompanimentarrangement.\n3 Method\nWedevelopamodelthattakesapianoreductionasinputandoutputsanorchestratedmulti-track\narrangement. Usinganautoencoder,wedisentangleamulti-trackmusicscoreintoitspianoreduction\n(contentfactor)andorchestralfunction(stylefactor). Wethendesignapriormodeltoinferorchestral\nfunctionsgiventhepianoreduction. Theautoencoderoperatesatsegmentlevel,whilethepriormodel\nworksonthewholesong. Theentiremodelcanoperateasanorchestratormoduleinacomplete\narrangementsystem. Inthissection,weintroduceourdatarepresentationinSection3.1,autoencoder\nframeworkinSection3.2,andpriormodeldesigninSection3.3.\n3.1 DataRepresentation\nWesummarizeourdatarepresentationsinTable1. LetxbeaK-trackarrangementscore. Wesplit\nitintoT segmentsandrepresentxk —eachsegmenttrack—asamatrixofshapeP ×N. Here\nt\nP = 128 represents 128 MIDI pitches and N is the time dimension of a segment. This matrix\nrepresentation aligns with the modified piano roll in [39], where each non-zero entry (p,n) > 0\nindicatesanoteonsetanditsvalueindicatesthenoteduration. Inthispaper, weprimarilyfocus\nonmusicpiecesin4/4timesignaturewith1/4-beatresolution. Durationvaluesrangefrom1(for\nsixteenthnotes)to32(fordoublewholenotes). Weconsider1segment=8beats(2bars)andderive\nN =32,whichisaproperscaleforlearningmusiccontent/stylerepresentations[37,39–41,46].\nThepianoreductionofxisnotatedaspn[x]. ItisapproximatedbydownmixingallK tracksintoa\nsingle-trackmixturesimilarto[8]. Whenconcurringnotesarefoundacrosstracks,wekeeptheone\nwiththelargestduration(i.e.,track-wisemaximum). Segment-wise,pn[x] isalsoaP ×N matrix.\nt\nItpreservestheoverallmusiccontentwhilediscardingthemulti-trackform.\nTheorchestralfunctionofxisnotatedasfn[x]. Itdescribestherhythmandgroovingpatterns[45]of\neachsegmenttrack,whichservesasthe“skeleton”ofamulti-trackform. Formally,\nfn[x]k =colsum(1 )/max_sum, (1)\nt {xk>0}\nt\nwhere indicator function 1 counts each note onset position as 1; colsum(·) sums up the pitch\n{·}\ndimension,derivinga1×N time-seriesfeature;max_sum=14isfornormalization.Theorchestral\n3\nTrack Decoder\nTrack Axis 𝐳𝐳𝑡𝑡1 𝐳𝐳𝑡𝑡2 𝐳𝐳𝑡𝑡𝐾𝐾 VQ Codebook\nDect 𝐱𝐱�𝒕𝒕𝟏𝟏 (Track SD ee pc at rator𝐱𝐱 �)𝒕𝒕𝟐𝟐 Dect 𝐱𝐱�𝒕𝒕𝑲𝑲 (128x16)\n⋯\nFn. Enc. Fn. Dec. Sep\nfn[𝐱𝐱]𝑡𝑡𝑘𝑘 f�n[𝐱𝐱]𝑡𝑡𝑘𝑘\npnP[i𝐱𝐱an]𝑡𝑡o EEncnocdp er Encf Decf Encf Decf Encf Decf Function Encoder Function Decoder\nFigure1:Th𝐜𝐜e𝑡𝑡autoencod𝐬𝐬e𝑡𝑡1 rarchitecture.𝐬𝐬I𝑡𝑡2 tlearn⋯scontentrep𝐬𝐬𝑡𝑡𝐾𝐾\nresentationc tfrompianoreduction,style\nrepresentationss1:K fromorchestralfunction,andleveragesbothtoreconstructindividualtracks.\nt\nfunction fn[x] essentially describes the form, or layout, of multi-track music x. It indicates the\nrhythmicintensityofparalleltracksandinformswheretoputmorenotesandwheretokeepsilent.\n3.2 Autoencoder\nOurautoencoderconsistsoftwocomponentsasshowninFigure1. AVQ-VAEsubmodule(right\nofFigure1)encodesorchestralfunctionfn[x]k. AVAEmodule(leftofFigure1)encodespiano\nt\nreductionpn[x] andreconstructsindividualtracksx1:K leveragingthecuesfromfn[x]1:K. During\nt t t\ntraining,bothinputspn[x]andfn[x]aredeterministictransformsfromtheoutputxandtheentire\nmodelisself-supervised.Weseesimilartechniquesforrepresentationdisentanglementin[39,41,46].\nTheVQ-VAEconsistsofFunctionEncoderEncf andDecoderDecf. EncoderEncf containsa1-D 2\nconvolutionallayerfollowedbyavectorquantizationblock. OurintuitionforapplyingaVQ-VAEis\nthatorchestralfunctioncommonlyconsistsofrhythmpatterns(suchassyncopation,arpeggio,etc.)\nthatcannaturallybecategorizedasdiscretevariables. Inourcase,eachsegmenttrackisencoded\ninto8discreteembeddingsona1-beatscale,indicatingtheflowoforchestrationstyle. Formally,\nsk :={sk}8t =Encf(fn[x]k), k =1,2,··· ,K, (2)\nt τ τ=8t−7 t\nwheresk isthelatentorchestralfunctioncodeforthek-thtrackattheτ-thbeat. Weencodefn[x]k at\nτ t\nafiner1-beatscale(insteadofsegment)topreservefine-grainedrhythmicdetails. Thenewscaleis\nre-indexedbyτ =1,2,··· ,8T. Wecollectivelydenoteeach8-codegroupingassk forconciseness.\nt\nTheVAEconsistsofPianoEncoderEncp,TrackSeparatorSep,andTrackDecoderDect. Encoder\nEncp learnscontentrepresentationc frompianoreductionpn[x] . Herec isacontinuousrepre-\nt t t\nsentation(withoutvectorquantization)thatcapturesmorenuancedmusiccontent. DecoderDect\nreconstructsindividualtracksxk fromtrackrepresentationzk. Notably,z1:K arerecoveredfromc\nt t t t\nusingtheorchestralfunctioncuesfroms1:K. Formally,\nt\nz1,z2,··· ,zK =Sep(s1,s2,··· ,sK |c ), (3)\nt t t t t t t\nwhereTrackSeparatorSepisaTransformerencoder. Inthisprocess,eachsk queriesc torecover\nt t\nthecorrespondingtrack(k),whiletheyalsoattendtoeachothertomaintainthedependencyamong\nparalleltracks. Learnableinstrumentembeddings[51]areaddedtoeachtrackbasedonitsinstrument\nclass. WeprovidedetailsoftheautoencoderarchitectureinAppendixA.1.\n3.3 StylePriorModelling\nTheVQ-VAEinSection3.2deriveslatentcodess1:K fororchestralfunctionasamulti-streamtime\n1:8T\nseries. Herek =1,2,···K isthestream(track)indexandτ =1,2,··· ,8T isthetime(beat)index.\nThepurposeofstylepriormodellingistoinferorchestralfunctiongivenpianoreductionsothatthe\nformercanbeleveragedtoorchestratethelatterintomulti-trackmusic. Wedesignourpriormodel\nasshowninFigure2. Itisanencoder-decoderframeworkthatmodelsparalleltracks/streamsof\norchestralfunctioncodesconditionalonthepianoreduction.\nThedecodermodule(rightofFigure2)hasalternatelayersofTrackEncoderandAuto-Regressive\nDecoder. TrackEncoderisastandardTransformerencoderlayer[35]anditaggregatesinter-track\ninformationalongthetrackaxis. Auto-RegressiveDecoderisaTransformerdecoderlayer(with\nself-attentionandcross-attention)anditpredictsnext-steporchestralfunctioncodesonthetimeaxis.\n4\n⋯ ⋯ ⋯\n⋯\nConv1d\n⋯\nFC FC\nCross-Attention\n𝐾𝐾 𝐾𝐾 𝐾𝐾 𝐾𝐾\n𝑠𝑠1 𝑠𝑠2 𝑠𝑠3 𝑠𝑠𝜏𝜏+1\nContext Encoder 1\n𝑠𝑠1 ⋯\n+ 1\nAuto-Regressive De⋯coder\n𝑠𝑠𝜏𝜏+1\n+\n⋯\nSinusoidal Positional E𝐜𝐜n1c:𝑇𝑇oding\nRelative Positional Embedding +\nMusic Timing Condition\nInstrument Embedding\nGaussian Noise\nTime Axis\n𝑠𝑠𝜏𝜏𝐾𝐾\n+\n1 1 1\nFigure2: Thepriormodelarchitecture. Theoverallsaorschitec𝑠𝑠1tureis𝑠𝑠a2ne ⋯ncoder-de𝑠𝑠c𝜏𝜏oderTransformer,\nwhilethedecodermoduleisinterleavedwithorthogonaltime-wiseandtrack-wiselayers.\nByorthogonallystackingtwotypesoflayers,wecanmodeltrack-wiseandtime-wisedependen-\nciessimultaneouslywithamanageablecomputationalcost. Comparedtosequentialtokenization\nmethodsinpreviousstudies[9,30,36],ourmethodbringsdownthecomplexityfromO(N2T2)to\nO(max(N,T)2). Moreover,wesupportaflexiblemulti-trackform(N beingvariable)withadiverse\ninstrumentationoption. Weaddinstrumentembedding[51]andrelativepositionalembedding[15]to\nthetrackaxis,where34instrumentclasses[25]aresupported. Weaddmusictimingcondition[7]to\nthetimeaxis,whichencodesthepositionsinatrainingexcerptasfractionsofthecompletesong,\nhelpingthemodelcapturetheoverallstructureofasong.\nTheencodermodule(leftofFigure2)ofourpriormodelisastandardTransformerencoder,which\ntakespianoreductionc asglobalcontext.Itisconnectedtothedecodermoduleviacross-attention\n1:T\nandmaintainstheglobalphrasestructure. Duringtraining, bothc ands1:K arederivedfrom\n1:T 1:8T\nthe same multi-track piece and the entire model is self-supervised. Let p be the distribution of\nθ\norchestralfunctioncodesfittedbyourpriormodelθ,thetrainingobjectiveisthemeanofnegative\nlog-likelihoodofnext-stepcodeprediction:\nK\n1 (cid:88)\nL(θ)=− logp (sk |s1:K,c ). (4)\nK θ τ <τ 1:T\nk=1\nWeprovidemoreimplementationdetailsofthepriormodelinAppendixA.2. Wenotethatthereisa\npotentialdomainshiftfromourapproximatedpianoreductiontorealpianoarrangements. Toprevent\noverfitting,weuseaGaussiannoiseϵtoblurc whilepreservingitshigh-levelstructure. During\n1:T\ntraining,ϵiscombinedwithc usingaweightedsummationwithnoiseweightγ rangingfrom0\n1:T\nto1. Itencouragesapartialunconditionalgenerationcapability. Atinferencetime,γ isaparameter\nthatcanbalancecreativitywithfaithfulness. Anexperimentonγ iscoveredinAppendixC.\n4 Whole-SongMulti-TrackAccompanimentArrangement\nWefinalizeacompletemusicautomationsystembyapplyingstylepriormodellingattwocascaded\nstages. AsshowninFigure3,ourautoencoderandorchestralfunctionprioroperateonStage2for\npianotomulti-trackarrangement. OnStage1,weadoptourpreviousstudy,apianotextureprior[50]\nStage 1\nLead Sheet\nPiano Texture Prior External Control\nStage 2\nPiano Arrangement\nOrchestral Function Prior\nPiano Texture Style\nA Complete Multi-Track\nOrchestral Function Style Multi-Track Arrangement\nAccompaniment Arrangement System\nFigure3: Acompleteaccompanimentarrangementsystembasedoncascadedpriormodelling. The\nfirst stage models piano texture style given lead sheet while the second stage models orchestral\nfunctionstylegivenpiano. Besidesmodularity,thesystemofferscontrolonbothcompositionlevels.\n5\n13\nCounterpoint Relation between Tracks\nLong-Term Phrase Coherence over Extended Context\n41\nHarmonic/Melodic Division between Two Guitars\nMetrical Division between String and Brass\nFigure4: ArrangementforCanYouFeeltheLoveTonight,apopsonginatotalof60bars. Weshow\ntwochoruspartsfrombar13to41. Weusereddottedboxestoshowcoherenceinlong-termstructure.\nWeusecolouredblockstoshownaturalnessandcohesioninmulti-trackarrangement.\nontopofchord/texturerepresentationlearning[39],forleadsheettopianoarrangement.Givenalead\nsheet,thefirststagegeneratesapianoaccompaniment,establishingtheroughwhole-songstructure.\nOursystemthenorchestratesthepianoaccompanimentintoacompletemulti-trackarrangementwith\nbandinstrumentation. Thistwo-stageapproachmirrorsmusicians’creativeworkflow[1]andallows\nforcontrolatbothcompositionlevels. Inparticular,weprovidethreecontroloptions:\n1. TextureSelection: TofilterpianotexturesonStage1bymetadataandstatisticalfeatures.\n2. Instrumentation: TocustomizethetracknumberandchoiceofinstrumentsonStage2.\n3. OrchestralPrompt:Toprompttheorchestrationprocesswithanorchestralfunctiontemplate.\nWeshowcaseanarrangementexamplebythecompletesysteminFigure4. Thesysteminputisa\nleadsheetshownbytheMelstaff. Thefinaloutputistheaccompanimentinthereststaves. Notably,\ntheleadsheetconsistsof60barsinanstructureofi4A8B8B8x4A8B8B8O4(usingnotationsby[5]).\nHere,i4,x4,andO4eachdenotea4-barintro,interlude,andoutro. A8andB8representan8-bar\nverseandchorus,respectively. Figure4showsthearrangementresultforthefirstandthirdchoruses,\nspanningfrombar13to41. Weleveragecontroloption2tocustomizetheinstrumentationascelesta,\nacousticguitars(2),electricpianos(2),acousticpiano,violin,brass,andelectricbassinatotalof\nK =9tracks. ThecompletearrangementscoreisincludedinAppendixE.\nInFigure4,weobservesomemulti-trackarrangementpatternsthatarecommoninpractice. Purple\nblockshighlightacounterpointrelationbetweenguitartrackA.G.1andelectricpianotrackE.P.1.\n6\nGreenblocksshowtwoguitartrackswithcomplementaryorchestralfunctions: onemelodic(A.G.1)\nand theother harmonic (A.G.2). Yellow blocks illustratethe metrical division between the string\n(Vlns.)andthebrass(Br.)sections,withstringsonthedownbeatandbrassontheoffbeat. These\npatternsdemonstrateanaturalandcohesivemulti-trackarrangementbyoursystem. Moreover,we\nseeconsistentaccompanimentpatternsechoinginbothchoruspartsthatspanover30bars(shownby\nreddottedboxes),whilethelatteraddsapianoarpeggiotrack(Pno.)toenhancethemusicalflow.\nThisdemonstratesstructuredwhole-songarrangementoverextendedmusiccontexts.\n5 Experiment\nInthissection,weevaluatetheperformanceofourmulti-trackaccompanimentarrangementsystem.\nGiventhatexistingmethodsprimarilyfocusonleadsheettomulti-trackarrangement,weensurea\nfaircomparisonbyusingthetwo-stageapproachdiscussedinSection4. InSection5.1,wepresent\nthe datasets used and the training details of our model. In Section 5.2, we describe the baseline\nmodelsusedforcomparison. Ourevaluationisdividedintotwoparts: objectiveevaluation,detailed\nin Section 5.3, and subjective evaluation, covered in Section 5.4. For the single-stage piano to\nmulti-track(Stage2)andleadsheettopiano(Stage1)arrangementtasks,weperformadditional\ncomparisonswithvariousablationarchitecturesinSection5.5and5.6,respectively.\n5.1 DatasetsandTrainingDetails\nWe use two datasets to train the autoencoder and the style prior, respectively. The autoencoder\nistrainedwithSlahk2100[25],whichcontains2Kcuratedmulti-trackpieceswith34instrument\nclassesinabalanceddistribution. Wediscardthedrumtrackandclipeachpieceinto2-barsegments\nwith1-barhopsize. Weusetheofficialtrainingsplitandaugmenttrainingsamplesbytransposing\nto all 12 keys. The autoencoder comprises 19M learnable parameters and is trained with batch\nsize64for30epochsonanRTXA5000GPUwith24GBmemory. WeuseAdamoptimizer[19]\nwithalearningratefrom1e-3exponentiallydecayedto1e-5. Weuseexponentialmovingaverage\n(EMA)[29]andrandomrestart[7]toupdatethecodebookwithcommitmentratioβ =0.25.\nWeuseLakhMIDIDataset(LMD)[28]totrainthepriormodel. Itcontains170kmusicpiecesandis\nabenchmarkdatasetfortrainingmusicgenerativemodels. Wecollect2/4and4/4pieces(110kafter\nprocessing)andrandomlysplitLMDatsonglevelintotraining(95%)andvalidation(5%)sets. We\nfurtherclipeachpieceinto32-bartrainingexcerpts(i.e.,T =16atmaximum)witha4-barhopsize.\nOurpriormodelhas30Mparametersandistrainedwithbatchsize16for10epochs(600Kiterations)\nontwoRTXA5000GPUs. WeapplyAdamWoptimizer[22]withalearningrateof1e-4,scheduled\nbya1k-steplinearwarm-upfollowedbyasinglecycleofcosinedecaytoafinalrateof1e-6.\nFor model inference and testing, we consider two additional datasets: Nottingham [10] and\nWikiMT[44]. Bothdatasetscontainleadsheets(inABCnotation)thatarenotseenduringtrainingor\nvalidation. Moreover,theycoverdiversemusicgenresincludingfolk,pop,andjazz. Whenarranging\napiece,weleveragecontroloption2tosetuptheinstrumentation. Withoutlossofgenerality,this\ncontrolchoiceisrandomlysampledfromSlakh2100validation/testsets. Toarrangemusiclonger\nthanthepriormodel’scontextlength(32bars), weusewindowedsampling[7], wherewemove\naheadourcontextwindowby4barsandcontinuesamplingbasedontheprevious28bars. Weapply\nnucleussampling[13]withtopprobabilityp=0.05andtemperaturet=6.\n5.2 BaselineModels\nWecompareoursystemwiththreeexistingmethods:PopMAG[30],AnticipatoryMusicTransformer\n(AMT)[33],andGETMusic[23]. PopMAGandGETMusicgeneratemulti-trackaccompaniments\nfrom an input lead sheet based on a Transformer and a diffusion model, respectively. AMT is\nTransformer-based and it continues the accompaniment part from an input melody with starting\naccompanimentprompt. Weprovidedetailedconfigurationsinthefollowing.\nPopMAGisanencoder-decoderarchitecturebasedonTransformer-XL[6]. Itrepresentsmulti-track\nmusicbysequentialnote-leveltokenizationandisfullysupervised. Theencodertakesaleadsheetas\ninputandthedecodergeneratesmulti-trackaccompanimentauto-regressively. Sincethemodelisnot\nopensource,WereproduceitonLMDwithleadsheetsextractedby[24](melody)and[17](chord).\n7\nTable2: Objectiveevaluationresultsforleadsheettomulti-track arrangement(Section5.3). All\nentriesareoftheformmean±sems,wheresisaletter. Differentletterswithinacolumnindicate\nsignificantdifferences(p<0.05/6)basedonWilcoxonsignedranktestwithBonferronicorrection.\nModel ChordAcc. ↑ DOA↑ Structure↑ Latency↓\nOurs 0.564±0.014a 0.300±0.005a 1.519±0.030a 0.461±0.005b\nAMT[33] 0.446±0.013bc 0.294±0.006a 1.094±0.009c 6.320±0.212d\nGETMusic[23] 0.423±0.012c 0.225±0.007c 1.243±0.017b 0.450±0.002a\nPopMAG[30] 0.470±0.013b 0.270±0.007b 1.086±0.008c 0.638±0.013c\nGround-Truth - 0.333±0.009 1.980±0.019 -\nAnticipatoryMusicTransformer(AMT)isadecoder-onlyTransformerarchitecturewithnote-level\ntokenization. Itintroducesan“anticipation”method,whereconditionaltokens(melodyandstarting\nprompt)andgenerativetokens(accompanimentcontinuation)areinterleavedtotrainaconditional\ngenerative model. Since our testing dataset does not provide ground-truth accompaniment, the\nstarting prompt is given by the generation result (first 2 bars) of our system. We use the official\nimplementationoftheAMTmodel,2whichisalsotrainedonLMD.\nGETMusic represents multi-track music as an image-like matrix resembling score arrangement,\nbased on which a denoising diffusion probabilistic model is trained with a mask reconstruction\nobjective. Givenaleadsheet,itsupportsgenerating5accompanimenttracksusingpiano,guitar,\nstring,bass,anddrumortheirsubsets. Inourexperiment,wegenerateall5accompanimenttracks.\nWeusetheofficialimplementationoftheGETMusicmodel,3whichistrainedoninternaldata.\n5.3 ObjectiveEvaluationonMulti-TrackArrangement\nWeintroducefourmetricstoevaluatemulti-trackarrangementperformance: chordaccuracy[23,30],\ndegreeofarrangement(DOA),structureawareness[42],andinferencelatency[30]. Amongthem,\nchordaccuracymeasuresthemulti-trackharmonythatreflectsthefitnessoftheaccompanimenttothe\nleadsheet;DOAmeasuresinter-tracktonaldiversitythatreflectsthecreativityoftheinstrumentation.\nBothmetricsdemonstratemusiccohesionatlocalscales. Ontheotherhand,structureawareness\nmeasuresphrase-levelcontentsimilaritythatreflectslong-termstructuralcoherenceofthewhole\nsong. Finally, we use inference latency (in second/bar) to evaluate computational efficiency of\neach method. The detailed computation of each metric is provided in Appendix B. In Table 2,\nwecomputeground-truthDOAusing1000randompiecesfromLMD.Wecomputeground-truth\nstructureawarenessusing857piecesin4/4fromPOP909Dataset[38].\nWerandomlysample50piecesin4/4timesignaturefromNottinghamandWikiMTrespectively(100\npiecesintotal)toconductexperiment.Thelengthofeachpiecerangesfrom16to32bars.Werunour\nmethodandbaselinemodelsateachpiecein3independentrounds,deriving300setsofmulti-track\narrangementsamples. InTable2,wereporttheevaluationresultswithmeanvalue,standarderror\nofmean(sem),andstatisticalsignificancecomputedbyWilcoxonsignedranktest[43]. Wefind\nsignificantdifferencesbetweenourmethodandallbaselines(p-valuep<0.05/6,usingBonferroni\ncorrection). Inparticular,ourmethodoutperformsinchordaccuracy,structureawareness,andDOA,\nindicatingthecapabilityofarrangingharmonious,structured,andcreativeaccompaniments. The\ndiffusionbaselineoutperformsininferencelatencyasitappliesonly100diffusionsteps.Ourmethod’s\nefficiencyisonparwithit,whilebeing10timesfasterthanvanillanote-levelauto-regression.\n5.4 SubjectiveEvaluationonMulti-TrackArrangement\nWealsoconductadouble-blindonlinesurveytotestmusicquality.Oursurveyconsistsof5evaluation\nsets,eachcontaininganinputleadsheetfollowedby4arrangementsamplesbyourmethodandeach\nbaseline. Eachsampleis24-32barslongandissynthesizedtoaudioat90BPM(~1minuteper\nsample). Boththesetorderandthesampleorderineachsetarerandomized. Werequestparticipants\nto listen to 2 sets and evaluate the musical quality based on a 5-point Likert scale from 1 to 5.\n2https://github.com/jthickstun/anticipation\n3https://github.com/microsoft/muzic/tree/main/getmusic\n8\nTable3: Objectiveevaluationresultsforpianotomulti-trackarrangement(Section5.5). Allentries\nareoftheformmean±sems,wheresisaletter. Differentletterswithinacolumnindicatesignificant\ndifferences(p<0.05/6)basedonWilcoxonsignedranktestwithBonferronicorrection.\nPrior Faithfulness(stats.) ↑ Faithfulness(latent)↑ DOA↑ NLL↓\nOurs 0.945±0.001a 0.215±0.005a 0.308±0.005a 0.411±0.004\nParallel 0.937±0.002b 0.153±0.003b 0.233±0.005c 0.960±0.010\nDelay 0.915±0.004c 0.133±0.003c 0.207±0.005d 1.024±0.006\nRandom 0.913±0.003c 0.113±0.003d 0.262±0.005b -\n\u00002\u0000X\u0000U\u0000V \u0000$\u00000\u00007 \u0000*\u0000(\u00007\u00000\u0000X\u0000V\u0000L\u0000F \u00003\u0000R\u0000S\u00000\u0000$\u0000* \u00002\u0000X\u0000U\u0000V \u00003\u0000D\u0000U\u0000D\u0000O\u0000O\u0000H\u0000O \u0000'\u0000H\u0000O\u0000D\u0000\\ \u00005\u0000D\u0000Q\u0000G\u0000R\u0000P\n\u0000\u0017\u0000\u0011\u0000\u0013 \u0000\u0017\u0000\u0011\u0000\u0013\n\u0000\u0016\u0000\u0011\u0000\u0013 \u0000\u0016\u0000\u0011\u0000\u0013\n\u0000\u0015\u0000\u0011\u0000\u0013 \u0000\u0015\u0000\u0011\u0000\u0013\n\u0000\u0014\u0000\u0011\u0000\u0013 \u0000\u0014\u0000\u0011\u0000\u0013\n\u0000&\u0000R\u0000K\u0000H\u0000U\u0000H\u0000Q\u0000F\u0000\\ \u00006\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u00001\u0000D\u0000W\u0000X\u0000U\u0000D\u0000O\u0000Q\u0000H\u0000V\u0000V\u0000&\u0000U\u0000H\u0000D\u0000W\u0000L\u0000Y\u0000L\u0000W\u0000\\\u00000\u0000X\u0000V\u0000L\u0000F\u0000D\u0000O\u0000L\u0000W\u0000\\ \u0000,\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000P\u0000H\u0000Q\u0000W\u0000D\u0000W\u0000L\u0000R\u0000Q \u00006\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H \u0000&\u0000U\u0000H\u0000D\u0000W\u0000L\u0000Y\u0000L\u0000W\u0000\\ \u00000\u0000X\u0000V\u0000L\u0000F\u0000D\u0000O\u0000L\u0000W\u0000\\\nFigure5: Subjectiveevaluationresultsonlead Figure6: Subjectiveevaluationresultsonpiano\nsheettomulti-trackarrangement(Section5.4). tomulti-trackarrangement(Section5.5).\nTheevaluationisbasedon5criteria: 1)HarmonyandTextureCoherency,2)Long-TermStructure,\n3)Naturalness,4)Creativity,and5)OverallMusicality.\nAtotalof23participants(8femaleand15male)withdiversemusicalbackgroundshavecompleted\noursurvey,withanaveragecompletiontimeof22minutes. Themeanratingsandstandarderrors,\ncomputedbywithin-subject(repeated-measures)ANOVA[31],arepresentedinFigure5. Significant\ndifferencesareobservedacrossallcriteria(p-valuep<0.05). Notably,ourmethodoutperformsall\nbaselinesoneachcriterion,aligningwiththeresultsfromtheobjectiveevaluation.\n5.5 AblationStudyonStylePriorArchitecture\nWenowvalidateourdesignwiththepriormodelbyexclusivelyevaluatingonthepianotomulti-track\narrangementtask. Ourpriormodelisbasedonauniquelayerinterleavingdesign,whichenables\nmulti-streamtimeseriesmodellingwithexplicitstream-wiseattention. Wecompareitwithtwoother\nmulti-streamarchitectures: 1)Parallel: summingupparallelcodeembeddingsforjointlanguage\nmodelling[18],and2)Delay: leveraginga1-stepdelaycodeinterleavingtocatchimplicitstream-\nwisedependency[4]. BothParallelandDelayaretrainedunderthesamesetupasourmodel. We\nadditionallyintroduce3)Random: anaivepriorbasedonrandomtemplateretrieval. Thetemplates\naresampledevery2barswithsharedinstrumentationfromthevalidation/testsetsofSlakh2100.\nWe introduce two metrics to evaluate piano to multi-track arrangement: faithfulness and degree\nofarrangement(DOA).Faithfulnessmeasuresifthegeneratedarrangementfaithfullyreflectsthe\noriginalcontentfromthepiano.Itcomputesthesimilaritybetweeni)theinputpiano,andii)thepiano\nreductionofthegeneratedmulti-trackarrangement. Inourcase,wecomputecosinesimilarityover\ntwofeatures: astatistical(stats.) pitchclasshistogram[45]andalatenttexturerepresentation[39],\nwhichemphasizetonalandrhythmicsimilarity,respectively. DOAmeasuresthecreativityasdefined\ninSection5.3. WealsoreporttheNLLlossforourmodel,Parallel,andDelay.\nWeconductexperimentsusingthetestsetofPOP909[39],whichconsistsof88pianoarrangement\npieces. Inourexperiment,weusethefirstsectionofeachpiece,whichcontains2to4complete\nphrasestotallyspanning24to32bars. Weusecontroloption3topromptourmodel,Parallel,and\nDelaywiththesame2-barorchestralfunctiontemplate(sampledfromSlakh2100)andseehowit\nisdeveloped. Wereportmeanvalue,standarderrorofmean(sem),andstatisticalsignificancein\nTable3andfindsignificantdifferencesinbothfaithfulnessandDOA.Wealsoconductasubjective\nevaluation in the same setup as Section 5.4, with the results presented in Figure 6. Here we\nconsideranadditionalcriterion,Instrumentation,whichreflectsthewell-formednessofthemulti-\n9\nTable4: Ablationstudyonalternativeleadsheettopianoarrangement(i.e.,Stage1)modules. Here\nweinvestigatetheimpactofStage1totheentiretwo-stagesystem. Evaluationresultsarebasedon\nthefinalmulti-trackarrangementusingrespectiveStage1modules.\nTwo-StageSystem ChordAcc. ↑ Structure↑ DOA↑\nOurs(Stage1+Stage2) 0.564±0.014 1.519±0.030 0.300±0.005\nWhole-Song-Gen[42]+Stage2 0.509±0.015 1.121±0.006 0.277±0.006\nTable5: EvaluationresultsforleadsheettopianoarrangementexclusivelyonStage1.\nPianoArr. Module ChordAcc. ↑ Structure↑\nOurs(PianoTexturePrior) 0.540±0.016 1.983±0.147\nWhole-Song-Gen[42] 0.430±0.020 1.153±0.180\ntrackarrangement. Significantdifferencesareobservedacrossallcriteria(p-valuep<0.05). Overall,\nParallelandDelaybothfallshortinperformancebecausetheyassumeapresetstreamcombination,\nwhile in our setting, both track numbers and choices of instruments are flexible. By explicitly\nmodellingstream-wiseattention,ourlayerinterleavingdesignfitswelltothatgeneralizedscenario.\n5.6 AblationStudyonPianoArrangement\nNowwevalidateourchoicefortheleadsheettopianoarrangementmoduleonthefirststageof\nourtwo-stagesystem. OurchoiceisapianotexturepriorascoveredinSection4. Weconductan\nablationstudybyreplacingitwiththeWhole-Song-Genmodel[42],which,toourknowledge,isthe\nonlyexistingalternativethatcanhandleawhole-songstructure. Theablationstudyisconductedin\nthesamesetupasSection5.3. InTable4,wereportchordaccuracy,structureawareness,andDOA\nregardingthefinalmulti-trackarrangementresults. Wefurthercompareourpianotexturepriorwith\nWhole-Song-Genexclusivelyonthepianoaccompanimentarrangementtask. InTable5,wereport\nchordaccuracyandstructureawarenessregardingpianoarrangementforbothmodels. Significant\ndifferences(p-valuep<0.05)arefoundinallmetricsbasedonWilcoxonsignedranktest.\nBycomparingTable4andTable5,wecanseethatahigher-qualitypianoarrangementgenerally\nencouragesamoremusicalandcreativefinalmulti-trackarrangementresult. Specifically,thepiano\narrangementonStage1laysthegroundworkfor(atleast)chordprogressionandphrasestructurefor\nStage2,bothofwhichareimportantforcapturingthelong-termstructureinwhole-songmulti-track\narrangement. Moreover,weseethatourpianotextureprioroutperformsexistingalternativesand\nguaranteesadecentpianoquality,thusbeingthebestchoiceforoursystem.\n6 Conclusion\nTosumup,wecontributeamusicautomationsystemformulti-trackaccompanimentarrangement.\nThemainnoveltyliesinourproposedstylepriormodelling,agenericmethodologyforstructured\nsequencegenerationwithfine-grainedcontrol. Bymodellingthepriorofdisentangledstylefactors\ngivencontent,webuildacascadedarrangementprocess: fromleadsheettopianotexturestyle,and\nthenfrompianotoorchestralfunctionstyle.Oursystemfirstgeneratesapianoaccompanimentfroma\nleadsheet,establishingtheroughwhole-songstructure. Itthenorchestratesthepianoaccompaniment\nintoacompletemulti-trackarrangementwithbandinstrumentation. Extensiveexperimentsshowthat\noursystemgeneratesstructured,creative,andnaturalmulti-trackarrangementswithstate-of-the-art\nquality. Atahigherlevel,weelaborateourmethodologyasinterpretablemodularrepresentation\nlearning, which leverages finely disentangled and manipulable music representations to tackle\ncomplextaskswithacompositionalhierarchy. Wehopeourresearchbringsnewperspectivesto\nbroaderdomainsofmusiccreation,sequencedatamodelling,andrepresentationlearning.\nReferences\n[1] SamuelAdlerandPeterHesterman. Thestudyoforchestration,volume2. WWNortonNew\nYork,NY,1989.\n10\n[2] AndreaAgostinelli,TimoIDenk,ZalánBorsos,JesseEngel,MauroVerzetti,AntoineCaillon,\nQingqingHuang,ArenJansen,AdamRoberts,MarcoTagliasacchi,etal. Musiclm: Generating\nmusicfromtext. arXivpreprintarXiv:2301.11325,2023.\n[3] CarolineChan,ShiryGinosar,TinghuiZhou,andAlexeiA.Efros. Everybodydancenow. In\n2019IEEE/CVFInternationalConferenceonComputerVision,pages5932–5941,2019.\n[4] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi,\nandAlexandreDéfossez. Simpleandcontrollablemusicgeneration. InAdvancesinneural\ninformationprocessingsystems,volume36,2023.\n[5] Shuqi Dai, Huan Zhang, and Roger B Dannenberg. Automatic analysis and influence of\nhierarchical structure on melody, rhythm and harmony in popular music. arXiv preprint\narXiv:2010.07518,2020.\n[6] ZihangDai,ZhilinYang,YimingYang,JaimeG.Carbonell,QuocVietLe,andRuslanSalakhut-\ndinov.Transformer-xl:Attentivelanguagemodelsbeyondafixed-lengthcontext.InProceedings\nofthe57thConferenceoftheAssociationforComputationalLinguistics,pages2978–2988,\n2019.\n[7] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya\nSutskever. Jukebox: Agenerativemodelformusic. arXivpreprintarXiv:2005.00341,2020.\n[8] Hao-WenDong,ChrisDonahue,TaylorBerg-Kirkpatrick,andJulianJ.McAuley. Towards\nautomatic instrumentation by learning to separate parts in symbolic multitrack music. In\nProceedings of the 22nd International Society for Music Information Retrieval Conference,\npages159–166,2021.\n[9] Hao-Wen Dong, Ke Chen, Shlomo Dubnov, Julian McAuley, and Taylor Berg-Kirkpatrick.\nMultitrackmusictransformer. InInternationalConferenceonAcoustics,SpeechandSignal\nProcessing,pages1–5.IEEE,2023.\n[10] Eric Foxley. Nottingham database. [EB/OL], 2011. https://ifdo.ca/~seymour/\nnottingham/nottingham.htmlAccessedMay17,2023.\n[11] CurtisHawthorne,AndriyStasyuk,AdamRoberts,IanSimon,Cheng-ZhiAnnaHuang,Sander\nDieleman, ErichElsen, JesseH.Engel, andDouglasEck. Enablingfactorizedpianomusic\nmodeling and generation with the MAESTRO dataset. In 7th International Conference on\nLearningRepresentations,2019.\n[12] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415,2016.\n[13] AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejinChoi. Thecuriouscaseofneural\ntextdegeneration. In8thInternationalConferenceonLearningRepresentations,2020.\n[14] Wen-YiHsiao,Jen-YuLiu,Yin-ChengYeh,andYi-HsuanYang. Compoundwordtransformer:\nLearningtocomposefull-songmusicoverdynamicdirectedhypergraphs. InProceedingsofthe\nAAAIConferenceonArtificialIntelligence,volume35,pages178–186,2021.\n[15] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne,\nNoamShazeer,AndrewM.Dai,MatthewD.Hoffman,MonicaDinculescu,andDouglasEck.\nMusictransformer: Generatingmusicwithlong-termstructure. In7thInternationalConference\nonLearningRepresentations,2019.\n[16] RongjieHuang,YiRen,JinglinLiu,ChenyeCui,andZhouZhao. Generspeech: Towardsstyle\ntransferforgeneralizableout-of-domaintext-to-speech. InAdvancesinneuralinformation\nprocessingsystems,volume35,2022.\n[17] JunyanJiang,KeChen,WeiLi,andGusXia. Large-vocabularychordtranscriptionviachord\nstructuredecomposition. InProceedingsofthe20thInternationalSocietyforMusicInformation\nRetrievalConference,pages644–651,2019.\n11\n[18] EugeneKharitonov,AnnLee,AdamPolyak,YossiAdi,JadeCopet,KushalLakhotia,TuAnh\nNguyen,MorganeRivière,AbdelrahmanMohamed,EmmanuelDupoux,andWei-NingHsu.\nText-freeprosody-awaregenerativespoken languagemodeling. InProceedingsofthe60th\nAnnualMeetingoftheAssociationforComputationalLinguistics,pages8666–8681,2022.\n[19] DiederikPKingmaandJimmyBa. Adam:Amethodforstochasticoptimization. arXivpreprint\narXiv:1412.6980,2014.\n[20] DmytroKotovenko,ArtsiomSanakoyeu,SabineLang,andBjörnOmmer. Contentandstyle\ndisentanglement for artistic style transfer. In 2019 IEEE/CVF International Conference on\nComputerVision,pages4421–4430.IEEE,2019.\n[21] LiweiLin,GusXia,QiuqiangKong,andJunyanJiang. Aunifiedmodelforzero-shotmusic\nsourceseparation,transcriptionandsynthesis. InProceedingsofthe22ndInternationalSociety\nforMusicInformationRetrievalConference,pages381–388,2021.\n[22] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. In6thInternational\nConferenceonLearningRepresentations,2018.\n[23] Ang Lv, Xu Tan, Peiling Lu, Wei Ye, Shikun Zhang, Jiang Bian, and Rui Yan. Getmusic:\nGenerating any music tracks with a unified representation and diffusion framework. arXiv\npreprintarXiv:2305.10841,2023.\n[24] Xichu Ma, Xiao Liu, Bowen Zhang, and Ye Wang. Robust melody track identification in\nsymbolic music. In Proceedings of the 23rd International Society for Music Information\nRetrievalConference,pages842–849,2022.\n[25] EthanManilow,GordonWichern,PremSeetharaman,andJonathanLeRoux. Cuttingmusic\nsourceseparationsomeslakh: Adatasettostudytheimpactoftrainingdataqualityandquantity.\nIn2019IEEEWorkshoponApplicationsofSignalProcessingtoAudioandAcoustics,pages\n45–49,2019.\n[26] Lejun Min, Junyan Jiang, Gus Xia, and Jingwei Zhao. Polyffusion: A diffusion model for\npolyphonicscoregenerationwithinternalandexternalcontrols. InProceedingsofthe24th\nInternationalSocietyforMusicInformationRetrievalConference,pages231–238,2023.\n[27] MatthiasPlasser,SilvanPeter,andGerhardWidmer. Discretediffusionprobabilisticmodelsfor\nsymbolicmusicgeneration. InProceedingsoftheThirty-SecondInternationalJointConference\nonArtificialIntelligence,pages5842–5850,2023.\n[28] ColinRaffel. Learning-BasedMethodsforComparingSequences,withApplicationstoAudio-\nto-MIDIAlignmentandMatching. PhDthesis,ColumbiaUniversity,USA,2016.\n[29] AliRazavi,AaronVandenOord,andOriolVinyals. Generatingdiversehigh-fidelityimages\nwithvq-vae-2. InAdvancesinneuralinformationprocessingsystems,volume32,2019.\n[30] YiRen, JinzhengHe, XuTan, TaoQin, ZhouZhao, andTie-YanLiu. Popmag: Popmusic\naccompaniment generation. In Proceedings of the 28th ACM International Conference on\nMultimedia,pages1198–1206,2020.\n[31] HenryScheffe. Theanalysisofvariance,volume72. JohnWiley&Sons,1999.\n[32] HaoHaoTanandDorienHerremans. Musicfadernets: Controllablemusicgenerationbasedon\nhigh-levelfeaturesvialow-levelfeaturemodelling. InProceedingsofthe21stInternational\nSocietyforMusicInformationRetrievalConference,pages109–116,2020.\n[33] JohnThickstun,DavidLeoWrightHall,ChrisDonahue,andPercyLiang. Anticipatorymusic\ntransformer. TransactionsonMachineLearningResearch,2024.\n[34] AaronVanDenOord,OriolVinyals,etal. Neuraldiscreterepresentationlearning. InAdvances\ninneuralinformationprocessingsystems,volume30,2017.\n[35] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,\nLukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.InAdvancesinneuralinformation\nprocessingsystems,volume30,2017.\n12\n[36] DimitrivonRütte,LucaBiggio,YannicKilcher,andThomasHoffman. Figaro: Generating\nsymbolicmusicwithfine-grainedartisticcontrol. In11thInternationalConferenceonLearning\nRepresentations,2023.\n[37] ZiyuWangandGusXia. Musebert: Pre-trainingmusicrepresentationformusicunderstanding\nand controllable generation. In Proceedings of the 22nd International Society for Music\nInformationRetrievalConference,pages722–729,2021.\n[38] ZiyuWang,KeChen,JunyanJiang,YiyiZhang,MaoranXu,ShuqiDai,andGusXia. POP909:\nApop-songdatasetformusicarrangementgeneration. InProceedingsofthe21stInternational\nSocietyforMusicInformationRetrievalConference,pages38–45,2020.\n[39] ZiyuWang,DingsuWang,YixiaoZhang,andGusXia. Learninginterpretablerepresentation\nforcontrollablepolyphonicmusicgeneration. InProceedingsofthe21stInternationalSociety\nforMusicInformationRetrievalConference,pages662–669,2020.\n[40] ZiyuWang,YiyiZhang,YixiaoZhang,JunyanJiang,RuihanYang,GusXia,andJunboZhao.\nPIANOTREEVAE:Structuredrepresentationlearningforpolyphonicmusic. InProceedings\nofthe21stInternationalSocietyforMusicInformationRetrievalConference,pages368–375,\n2020.\n[41] ZiyuWang,DejingXu,GusXia,andYingShan. Audio-to-symbolicarrangementviacross-\nmodalmusicrepresentationlearning. InInternationalConferenceonAcoustics,Speechand\nSignalProcessing,pages181–185.IEEE,2022.\n[42] ZiyuWang,LejunMin,andGusXia. Whole-songhierarchicalgenerationofsymbolicmusic\nusingcascadeddiffusionmodels.In12thInternationalConferenceonLearningRepresentations,\n2024.\n[43] FrankWilcoxon. Individualcomparisonsbyrankingmethods. InBreakthroughsinstatistics:\nMethodologyanddistribution,pages196–202.Springer,1992.\n[44] ShangdaWu,DingyaoYu,XuTan,andMaosongSun. Clamp: Contrastivelanguage-music\npre-trainingforcross-modalsymbolicmusicinformationretrieval. InProceedingsofthe24th\nInternationalSocietyforMusicInformationRetrievalConference,pages157–165,2023.\n[45] Shih-Lun Wu and Yi-Hsuan Yang. The jazz transformer on the front line: Exploring the\nshortcomingsofai-composedmusicthroughquantitativemeasures. InProceedingsofthe21st\nInternationalSocietyforMusicInformationRetrievalConference,pages142–149,2020.\n[46] RuihanYang,DingsuWang,ZiyuWang,TianyaoChen,JunyanJiang,andGusXia.Deepmusic\nanalogyvialatentrepresentationdisentanglement. InProceedingsofthe20thInternational\nSocietyforMusicInformationRetrievalConference,pages596–603,2019.\n[47] LiYi,HaochenHu,JingweiZhao,andGusXia. Accomontage2: Acompleteharmonization\nandaccompanimentarrangementsystem. InProceedingsofthe23rdInternationalSocietyfor\nMusicInformationRetrievalConference,pages248–255,2022.\n[48] WenjieYin,HangYin,KimBaraka,DanicaKragic,andMårtenBjörkman. Dancestyletransfer\nwithcross-modaltransformer. InIEEE/CVFWinterConferenceonApplicationsofComputer\nVision,pages5047–5056,2023.\n[49] SiyangYuan,PengyuCheng,RuiyiZhang,WeituoHao,ZheGan,andLawrenceCarin. Improv-\ningzero-shotvoicestyletransferviadisentangledrepresentationlearning. In9thInternational\nConferenceonLearningRepresentations,2021.\n[50] JingweiZhaoandGusXia. Accomontage: Accompanimentarrangementviaphraseselection\nand style transfer. In Proceedings of the 22nd International Society for Music Information\nRetrievalConference,pages833–840,2021.\n[51] JingweiZhao,GusXia,andYeWang. Q&A:Query-basedrepresentationlearningformulti-\ntracksymbolicmusicre-arrangement. InProceedingsoftheThirty-SecondInternationalJoint\nConferenceonArtificialIntelligence,pages5878–5886,2023.\n13\nA ImplementationDetails\nA.1 Autoencoder\nTheautoencoderconsistsofaVQ-VAEsubmoduleandanoverarchingVAE.TheencoderoftheVQ-\nVAEconsistsofa1-Dconvolutionallayerofkernalsize4,stride4,and16outputchannels,followed\nbyavectorquantizationblockwithcodebooksize128. Thedecodertakestheconcatenatedlatent\ncodesandleveragestwofully-connectedlayers(shape128×256and256×32)forreconstruction. In\ntheoverarchingVAE,PianoEncoderandTrackDecoderareadaptedfromPianoTreeVAE[40]. The\nencoderfirstappliesapitch-wisebi-directionalGRUtosummarizeconcurrentnotesattimestepn\nandthenappliesatime-wiseGRUtoencodethefullrepresentation. Thedecodermirrorstheencoder\nstructurewithtime-andpitch-wiseuni-directionalGRUstoreconstructindividualtracks. Weuse\nhiddensize256inasinglelayerforpitchGRUsand512fortimeGRUs. TheTrackSeparatorisa\n2-layerTransformerencoderwith8attentionheads,0.1dropoutratio,andGELUactivation[12]. The\nhiddendimensionsofself-attentiond andfeed-forwardlayersd are512and1024,respectively.\nmodel ff\nTheautoencoderistrainedwithjointreconstructionlossfororchestralfunction(MSE)andindividual\ntracks(crossentropy). TheVQ-VAEisadditionallyregularizedwithlatentlossandcommitmentloss\nwithcommitmentratioβ =0.25. TheVAEisregularizedwithKLlossoverallcontinuousfactors\n(c andz1:K)basedonKLannealing[41]witharatioexponentiallyincreasingfrom0to0.5.\nt t\nA.2 PriorModel\nThepriormodelconsistsofa12-layerContextEncoderanda12-layerAuto-RegressiveDecoder.\nThelatterisinterleavedwithanother12track-wiseTrackEncoderlayers. Foreachlayer,weapply\n8 attention heads, 0.1 dropout ratio, and GELU activation. We apply layer normalization before\nself-attention (i.e., norm first). The hidden dimensions of self-attention d and feed-forward\nmodel\nlayers d are 256 and 1024, respectively. We apply relative positional embedding [15] to Track\nff\nEncodersothattwotracksinitializedwithidenticalinstrumentscanstillgeneratedifferentcontent.\nOurpriormodelistrainedonthelatentcodesc ands1:K inferredbyawell-trainedautoencoder\n1:T 1:8T\nonLMD.Fordiscretecodes,wetakethecodebookindicesandlearnanewembedding.\nB ObjectiveEvaluationMetrics\nB.1 DegreeofArrangement\nInmulti-trackarrangement,paralleltrackstypicallyplayauniqueroletoeachotherintheoverall\narrangement. Weareinterestedincapturingthediversityandcreativityinherentintheseroles.\nToachievethis,weconsiderthepitchclasshistogram[45]asaprobabilitydistributionP. LetP\nt,k\nbethedistributionofthet-thbarintrackk,andPpn bethatofthet-thbarinthepianoreduction.\nt\nRecallthatinthispaperweapproximatethepianoreductionofamulti-trackpiecebydownmixingall\ntracks. BothP andPpnare12-Dvectors,describingtonalityofindividualtracksandtheoverall\nt,k t\narrangement,respectively. WecomputetheKLdivergenceofeachtracktothepianoreduction:\nT\n1 (cid:88)\nd = KL(P ∥Ppn), (5)\nk T t,k t\nt=1\nwhereT isthetotalnumberofbars.\nInterpreting d in terms of KL divergence, we see it as the “excess surprise” from the overall\nk\narrangement(pianoreduction)whentrackkisplayedinisolation. Alarged indicatesthattrackk\nk\npossessesauniquequality,suchasabasstrackplayingtherootoracounter-melodytrackfocusing\nontensions. Conversely, asmalld suggeststhattrackk servesasafoundationalelementinthe\nk\narrangement,suchasstringpaddingthatestablishestheharmonicfoundation.\nIfalld valuesaresmall,itimplieshomogeneityacrosstracksandthusalowdegreeofarrangement.\nk\nConversely,ifalld valuesarehigh,itsuggestsacompositiondominatedbycounterpoints,ascenario\nk\nless common in pop music. A well-orchestrated piece typically exhibits a diverse range of d\nk\nvalues,encompassingbothfoundationalanduniquedecorativetracks. Wethusdefinethedegreeof\n14\narrangementDOAasthestandarddeviationofd fork =1,2,··· ,K acrossalltracks:\nk\n(cid:115)\n(cid:80)K (d −d)2\nDOA= k=1 k , (6)\nK\nwheredisthemean. K isthetotalnumberoftracks. Toestablishareferencepoint,wecalculatethe\nground-truthDOA=0.333basedon1000randomlyselectedpiecesfromtheLMDdataset. Within\nthiscontext,ahigherDOAsignifiesamorecreativearrangement.\nB.2 StrctureAwareness\nWeintroduceInter-phraseLatentSimilarity(ILS)from[42]tomeasurethestructuralawarenessof\nlong-termarrangement. ILScalculatesthecontentsimilarityamongsame-typephrases(e.g.,chorus)\nversusthewholesong. Itleveragespre-traineddisentangledVAEsthatencodemusicnotesintolatent\nrepresentationsandthencomparecosinesimilaritiesinthelatentspace. Inourcase,wecompute\nILSoverthepianoreductionofageneratedarrangementsinceitcontainstheoverallcontent. We\napplythetextureVAE[39]andobtainalatenttexturerepresentationctxt forevery2-barsegment.\nt\nForodd-numberedphrases,werepeatitsfinalbarandpadittotheendofthephrase. Supposethere\nareM differenttypesofphrasesinonepieceandletI bethesetofsegmentindicesinthetype-m\nm\nphrase,ILSisdefinedastheratiobetweensame-typephrasesimilarityandglobalaveragesimilarity:\n((cid:80)M (cid:80) cos(ctxt,ctxt))/((cid:80)M |I |2−|I |)\nILS= m=1 i̸=j∈Im i j m=1 m m , (7)\n(cid:80) cos(ctxt,ctxt)/(T2−T)\n1≤i̸=j≤T i j\nwhere|·|isthecardinalityofaset. T isthenumberof2-barsegments. WhenapplyingILS,we\nuse [5] to automatically lable the phrase structure of a piece. To establish a reference point, we\ncalculatetheground-truthILS = 1.980basedonthePOP909dataset(withphraseannotationby\nhuman). Withinthiscontext,ahigherILSsignifiessaliencywithlong-termphrase-levelstructure.\nB.3 ChordAccuracy\nWeintroducechordaccuracyfrom[30]tomeasureifthechordsofthegeneratedarrangementmatch\ntheconditionalchordsequenceintheleadsheet. Itreflectstheharmonicityofthegeneratedmusic\nandisdefinedasfollows:\n1 N (cid:88)chord\nCA= 1 , (8)\nN\nchord\n{Ci=Cˆ i}\ni=1\nwhereN isthenumberofchordsinapiece;C isthei-thchordinthe(ground-truth)leadsheet;\nchord i\nandCˆ isthealignedchordinthegeneratedarrangement.\ni\nTheoriginalformulationin[30]considerschordaccuracyforindividualtracks. Givenoursystem’s\ncapabilitytoaccommodateavariablecombinationoftracks,weoptforabroaderevaluationforthe\noverallarrangement. Inourcase,weextractthechordsequenceofageneratedarrangementwith[17]\nandcompareitinrootandqualitywithground-truthat1-beatgranularity,whichismorerigorous.\nB.4 OrchestrationFaithfulness\nWemeasurethefaithfulnessoforchestrationbythesimilaritybetweeni)theinputpianoandii)the\npianoreductionofthegeneratedmulti-trackarrangement. Leteinandepnbevectorfeaturesderived\nt t\nfromthet-thsegmentoftheinputandthereduction,respectively. OrchestrationfaithfulnessOFis\ndefinedasfollows:\nT\n1 (cid:88)\nOF= cos(ein,epn), (9)\nT t t\nt=1\nwherecos(·,·)iscosinesimilarity. T isthenumberofsegments.\nInourwork,weselecttwooptionsforvectorfeaturee. Oneisastatisticalpitchclasshistogram[45],\nwhich is a 12-D vector describing pitch class distribution. The other is a latent 256-D texture\nrepresentationlearnedbyapre-trainedVAE[39]. Bothfeaturesaregeneraldescriptorsofthemusical\ncontentwithrespectivefocusontonalharmonyandrhythmicgrooves.\n15\nC ExperimentonNoiseWeightγ\nContinuing from Section 3.3, we compare different γ values and see their impact to the model\nperformance. Whenapplyingourmodeltopianotomulti-trackarrangement,γ balancestheforce\nofanoisyfactoraddedtothepiano,whichencouragesapartialunconditionalgenerationcapability.\nTheexperimentalsettingsarethesameasSection5.5. Weevaluatetheresultsbasedonfaithfulness\nandDOA.InTable6,wereportmeanvalue,standarderrorofmean(sem),andstatisticalsignificance\ncomputedbyWilcoxonsignedranktest. Byvaryingtheγ value,weobserveacontrollablebalance\nbetweenfaithfulnessandcreativity. Specifically,alargerγ encouragescreativity(higherDOA)atthe\ncostoffaithfulness. Ifnotmentionedotherwise,weuseγ =0.25forexperimentsinthispaper.\nTable6: Objectiveevaluationresultsontheimpactofnoiseweightγ inAppendixC.\nNoiseWeightγ Faithfulness(stats.) ↑ Faithfulness(latent)↑ DOA↑\nγ =0 0.946±0.001a 0.228±0.005a 0.300±0.005c\nγ =0.25 0.945±0.001ab 0.215±0.005b 0.308±0.005bc\nγ =0.5 0.944±0.001b 0.187±0.004c 0.320±0.006ab\nγ =1 0.936±0.002c 0.127±0.003d 0.325±0.007a\nD OnlineSurveySpecifics\nWedistributeoursurveyviaSurveyMonkey.4 Oursurveyconsistsof5samplesetsforboththelead\nsheettomulti-trackandthepianotomulti-trackarrangementtasks(10setsintotal). Eachsampleis\n24-32barslongandissynthesizedtoaudioat90BPMusingBandLab5withthedefaultsoundfont.\nEachparticipantlistensto2sets(inrandomorder)andthemeantimespentis22minutes. Figure7\nshowsthesamplepagesofoursurveywithinstructionstotheparticipants.\n(a)Leadsheettomulti-trackarrangement. (b)Pianotomulti-trackarrangement.\nFigure7: Screenshotsofsurveypagesandinstructionsofouronlinesurvey.\n4https://www.surveymonkey.com\n5https://www.bandlab.com/\n16\nE ExampleonStructuredArrangement\nWedemonstrateanexampleofaccompanimentarrangementbyourproposedsystem. Theinputlead\nsheetisacompletepopsongshowninSectionE.1. Oursystemfirstarrangesapianoaccompaniment\nfor the whole song, which is shown in Section E.2. The piano score is then orchestrated into a\nmulti-trackarrangementwithcustomizedinstrumentation,whichisshowninSectionE.3.\nE.1 LeadSheet\nWeuseoursystemtoarrangeforCanYouFeeltheLoveTonight,apopsongbyEltonJohn. Asshown\nin Figure 8, the entire song is 60 bars long and it presents a structure of i4A8B8B8x4A8B8B8O4,\nwherei,x,O,A,andBeachrefertointro,interlude,outro,verse,andchorus.\n\ni4 = 65\nB♭ F/A E♭/G B♭/F E♭ B♭/D F/A B♭ Cm7 B♭/D\n \n        \n \nA8\n5 E♭ B♭/D E♭ B♭/D E♭ B♭/D Cm7 F/A\n                          \n9 E♭ B♭/D E♭ B♭/D E♭ Gm A♭ F\n B8                       \n13 B♭ F/A Gm E♭ B♭ E♭ C7/E F\n                     \n17 E♭ B♭/D Gm E♭ Cm7 B♭/D Cm/E♭ C/E F\nB8    \n\n    \n\n         \n21 B♭ F/A Gm E♭ B♭ E♭ C7/E F\n                 \n    \n25 E♭ B♭/D Gm E♭ Cm B♭/D Cm/E♭ F7sus E♭/B♭ B♭\nx4                    \n29 1.B♭ F/A E♭/G B♭/F E♭ B♭/D F/A B♭ Cm7 B♭/D\n\n           \nO4\n33 2.E♭ B♭/D Gm E♭ Cm B♭/D Cm/E♭ F7sus E♭/B♭ B♭\n    F ig ure 8:Lead she et for po p song CanY ouFee ltheL oveTo nig h t.  \n17\nE.2 PianoArrangement\nThepianoarrangementresultisshownfromFigure9toFigure10.Itroughlyestablishesawhole-song\nstructureandlaysthegroundworkforbandorchestrationatthenextstage. Demoaudioforthepiano\nscoreisavailableathttps://zhaojw1998.github.io/structured-arrangement/.\n\ni4 = 65\nB♭ F/A E♭/G B♭/F E♭ B♭/D F/A B♭ Cm7 B♭/D\nMelody\n           \nPiano                                         \n\n        \nA8          \n5 E♭ B♭/D E♭  B♭/D E♭ B♭/D  Cm7 F/A\nMel.\n                                    \nPno.                            \n9  \n\nE♭\n  \nB♭/D\n  \nE♭\n\n\nB♭/D\nE\n \n♭\n G \nm\n   \n\nA♭\n  \n\nF\n \nMel.\n                                     \n       \nPno.                 \n1 3B8  B♭    F/A     Gm E♭     B♭       E♭   C 7/E    F     \nMel.\n                      \nPno.   \n     \n\n \n\n      \n1 7   E♭     B ♭/D     Gm   E ♭     Cm7  B♭/D  C m/E♭ C/ E  F       \nMel.\n                           \n\nPno.                   \n\n2 1B8 \nB♭\n   \nF/A\n   Gm    \nE♭\n   B\n♭\n \n\n\nE♭\n  C7/E \nF\n       \nMel.\n                      \n\n\nPno.                         \n2 5  E♭    B ♭/D    Gm   E ♭     Cm  B♭/D  C m/E♭ F7su s  E♭/B ♭    B♭    \nMel.\n                          \n\nPno.                    \n                    \n\n            \nFigure9: Pianoarrangementscore(page1).\n\n18\nx4\n29 B♭ F/A E♭/G B♭/F E♭ B♭/D F/A B♭ Cm7 B♭/D\nMel.\n         \nPno.              \n A\n8\n                   \n\n    \n33 E♭ B♭/D E♭ B♭/D E♭ B♭/D Cm7 F/A\nMel.\n                  \nPno.                          \n                            \n37  E♭ B♭/D E♭ B♭/D E ♭ Gm A♭  F\nMel.\n                    \nPno.                 \n4 1B8 \n\nB♭   F\n\n/A   Gm    E♭     B♭\n\n   E\n\n♭  \n\nC7\n/E\n \n\nF      \nMel.\n                    \n\nPno.             \n4 5  E♭  B ♭/D    Gm    E ♭   C m7 B♭ /D  Cm /E\n♭\n C/E F    \nMel.\n                      \nPno.              \n4 9B8   B♭  F/A    Gm   E♭    B ♭    E ♭  C7 /E   F   \nMel.\n                 \n\nPno. \n\n  \n  \n  \n \n5 3   E♭  B♭ /D   G m    E♭   Cm B ♭/D  C m/ E♭ F 7su s  E ♭/B ♭   B♭ \nMel.\n                    \nPno. \n \n\n\n\n\n\n\n\n    \n5 7O4   E♭  B♭ /D    Gm   E ♭    Cm B ♭/D C  m/ E♭ F7s us E♭/B  ♭  B♭    \nMel.\n                          \nPno.    \n\n\n\n                      \nFigure10: Pianoarrangementscore(page2,lastpage).\n\n19\nE.3 Multi-TrackArrangement\nThemulti-trackarrangementisshownfromFigure11toFigure15. Wecustomizetheinstrumentation\nascelesta,acousticguitars(2),electricpianos(2),acousticpiano,violin,brass,andelectricbassinato-\ntalofK =9tracks. Wecanseethatthestructureoftheaccompanimentfollowstheleadsheet. Demo\naudioisavailableathttps://zhaojw1998.github.io/structured-arrangement/. Morede-\ntailedanalysisonthisarrangementdemoiscoveredinSection4.\ni4  = 65\nB♭ F/A E♭/G B♭/F E♭ B♭/D F/A B♭ Cm7 B♭/D\nMelody\n           \nCelesta\n                     \nAcoustic Guitar 1\n\n\n     \n\nAcoustic Guitar 2\n\n\n   \nPiano                              \n                     \n\nElectric Piano 1\n\n\n\n   \nElectric Piano 2      \n     \n \nViolins\n\n\n\n  \n  \nBrass\n                     \nElectric Bass    \n     \n \nA8\n5 E♭ B♭/D E♭ B♭/D E♭ B♭/D Cm7 F/A\nMel.\n                                    \nA.G. 2\n                       \nE.P. 1\n\n\n\n\n   \n  \n     \n  \n\n\n\nE.P. 2 \n                   \n        \n\n\n\n       \n \nVlns.\n\n         \nBr.\n   \n\n\n\n  \n\n        \nE.B.\n    Figure11: Multi-trackarrangem entscore(page1).  \n20\n9 E♭ B♭/D E♭ B♭/D E♭ Gm A♭ F\nMel.\n                                    \nA.G. 1\n\n      \nA.G. 2 \n                       \n \nPno.       \n     \n\nE.P. 1\n\n\n\n    \n \n    \n \nE.P. 2                                \n\nVlns.\n\n         \nBr.\nE.B.                          \n         \nB8\n13 B♭ F/A Gm E♭ B♭ E♭ C7/E F\nMel.\n                     \nA.G. 1\n\n                    \nA.G. 2\n                         \n\nE.P. 1\n     \nVlns.                  \n        \nBr.\n          \n\n     \nE.B.\n               \n17 E♭ B♭/D Gm E♭ Cm7 B♭/D Cm/E♭ C/E F\nMel.\n                          \nA.G. 1\n                    \nA.G. 2\n                        \nE.P. 1\n\nVlns.\n\n      \n  \n        \n \n        \nBr.\n                      \nE.B.\n     Figure12: Mult i-tr ackarrangeme n tscor e (page2).   \n21\nB8\n21 B♭ F/A Gm E♭ B♭ E♭ C7/E F\nMel.\n                      \nA.G. 1\n                          \nA.G. 2  \nVlns.\n  \n \n                     \n        \nBr.\n                   \nE.B.\n               \n25 E♭ B♭/D Gm E♭ Cm B♭/D Cm/E♭ F7sus E♭/B♭ B♭\nMel.\n                         \nA.G. 1\n                   \nA.G. 2\n               \n\n\n\n  \nVlns.\n        \nBr.\n                    \nE.B.\n                 \nx4\n29 B♭ F/A E♭/G B♭/F E♭ B♭/D F/A B♭ Cm7 B♭/D\nMel.\n         \nA.G. 1            \n \nA.G. 2\n \n\n\n\n \n\n\n\n \n\n\n \n\n\n\n\n   \nPno.     \n   \n \n                    \n      \n  \nE.P. 2\n\n    \n\n\n \n\n\n\n\n            \n   \nE.B.    \n  \n       \nA8\n33 E♭ B♭/D E♭ B♭/D E♭ B♭/D Cm7 F/A\nMel.\n                     \nPno.                \n\n     \n                    \n  \nE.B.\n    Figure13: Multi-trackarrangem entscore(page3).  \n\n22\n37 E♭ B♭/D E♭ B♭/D E♭ Gm A♭ F\nMel.\n                        \nCel.\n\n              \nPno.\n    \n  \n               \n          \nE.B.\n    \n      \nB8\n41 B♭ F/A Gm E♭ B♭ E♭ C7/E F\nMel.\n                \nA.G. 1\n              \nA.G. 2\n \n\n   \n\n          \nPno.\n    \nVlns.\n                            \n\n        \nBr.\n                   \nE.B.\n   \n      \n45 E♭ B♭/D Gm E♭ Cm7 B♭/D Cm/E♭ C/E F\nMel.\n                    \nA.G. 1\n                   \n\nA.G. 2\n                      \nPno.\n    \n                          \nVlns.  \n         \nBr.\n                   \nE.B.\n    Figure14:Multi-trackarrangem entscore(\npage4).\n \n23\nB8\n49 B♭ F/A Gm E♭ B♭ E♭ C7/E F\nMel.\n                \nA.G. 1\n              \nA.G. 2\n               \nPno.     \nVlns.\n                           \n         \nBr.\n                 \n\n\nE.B.\n   \n      \n53 E♭ B♭/D Gm E♭ Cm B♭/D Cm/E♭ F7sus E♭/B♭ B♭\nMel.\n                  \nA.G. 1\n                   \n\nA.G. 2\n                     \nPno.\n    \n                           \nVlns. \n         \nBr.\n                  \nE.B.\n         \nO4\n57 E♭ B♭/D Gm E♭ Cm B♭/D Cm/E♭ F7sus E♭/B♭ B♭\nMel.\n                         \nA.G. 2\n   \n\n     \n\nE.P. 1\n\n\n      \n  \n      \n\nE.P. 2 \n\n\n\n\n\n\n\n\n             \nVlns.    \n         \nE.B.\n   Figu re15: Multi-trackarrangementscore (page5, lastpage).  \n24\nF Limitation\nWeproposeatwo-stagesystemforwhole-song,multi-trackaccompanimentarrangement. Inthe\ncontextofthispaper,weacknowledgethatourcurrentsystemexclusivelysupportstonaltracksin\nquadruplemeterswhiledisregardingtriplemeters,tripletnotes,anddrums. However,weperceive\nthis as a technical limitation rather than a scientific challenge. We also acknowledge that our\ncurrentsystemprimarilyemphasizesthecompositionlevel,therebyomittingthemodellingofMIDI\nvelocity,dynamictiming,andMIDIcontrolmessages. Consequently,thegeneratedresultsdonot\nencompassperformanceMIDIandmaylackexpressivequalities. Nevertheless,webelievethatour\ncomposition-centricworkservesasasolidandvitalfoundationforfurtheradvancementsinthose\nspecificareas,thusfacilitatingthedevelopmentofenhancedtechniquesandfeatures. Asapioneering\nwork,oursystemistheforemostaccomplishmentinsolvingwhole-songmulti-trackaccompaniment\narrangement,characterizedbyflexiblecontrollabilityontracknumberandchoiceofinstruments.\nG BroaderImpacts\nOurmulti-trackaccompanimentarrangementsystem,whichincorporatesstyletogenerateaccom-\npaniment,isdesignedtoenhanceoriginalityandcreativity. Itservesasaplatformforhuman-AI\nco-creation,wheretheuserprovidescontent-basedmaterial(inourcase,leadsheet)thatremains\nfundamentallyoriginal,whiletheAIagentinfusesstyle,enrichestheform,andenhancescreativity.\nOursystemthereforeempowersmusicianstoexplorenewmusicalideasandexpandtheircreative\nboundaries. Thisapproachalsoallowsforrapidmock-upwithdifferentstylesandarrangements,\nfosteringanenvironmentwhereinnovationandartisticexpressioncanthrive.\nHowever,weacknowledgetheneedtoaddresspotentialrisks. Theaccessibilityofoursystemmay\ninadvertently lead to excessive reliance on automation, potentially impeding the development of\nfundamentalskillsamongmusicians. Additionally,widespreadadoptionofthesystemmaycontribute\ntothehomogenizationofmusic,threateningthedistinctivenessandindividualitythatarecrucialto\nartisticexpression. WerecognizethatourdatasetspredominantlyfeaturescontemporaryWestern\nmusic,whichintroducesaculturalbiasthatcouldlimitthediversityofgeneratedcompositions.\n25",
    "pdf_filename": "Structured_Multi-Track_Accompaniment_Arrangement_via_Style_Prior_Modelling.pdf"
}