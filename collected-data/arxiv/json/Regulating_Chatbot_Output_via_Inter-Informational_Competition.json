{
    "title": "NNoorrtthhwweesstteerrnn JJoouurrnnaall ooff TTeecchhnnoollooggyy aanndd IInntteelllleeccttuuaall PPrrooppeerrttyy",
    "abstract": "",
    "body": "NNoorrtthhwweesstteerrnn JJoouurrnnaall ooff TTeecchhnnoollooggyy aanndd IInntteelllleeccttuuaall PPrrooppeerrttyy\nVolume 22 Issue 1 Article 3\nFall 11-18-2024\nRReegguullaattiinngg CChhaattbboott OOuuttppuutt vviiaa IInntteerr--IInnffoorrmmaattiioonnaall CCoommppeettiittiioonn\nJiawei Zhang\nFollow this and additional works at: https://scholarlycommons.law.northwestern.edu/njtip\nPart of the Computer Law Commons, Intellectual Property Law Commons, Science and Technology\nLaw Commons, and the Technology and Innovation Commons\nRReeccoommmmeennddeedd CCiittaattiioonn\nJiawei Zhang, Regulating Chatbot Output via Inter-Informational Competition, 22 NW. J. TECH. & INTELL.\nPROP. 109 (2024).\nhttps://scholarlycommons.law.northwestern.edu/njtip/vol22/iss1/3\nThis Article is brought to you for free and open access by Northwestern Pritzker School of Law Scholarly\nCommons. It has been accepted for inclusion in Northwestern Journal of Technology and Intellectual Property by\nan authorized editor of Northwestern Pritzker School of Law Scholarly Commons.\nN O R T H W E S T E R N\nJ O U R N A L T E C H N O L O G Y\nO F\nA N D\nI N T E L L E C T U A L P R O P E R T Y\nREGULATING CHATBOT OUTPUT VIA\nINTER-INFORMATIONAL\nCOMPETITION\nJiawei Zhang\nNovember 2024 VOL. 22, NO. 1\n© 2024 by Jiawei Zhang\nCopyright 2024 by Jiawei Zhang Volume 22, Number 1\nNorthwestern Journal of Technology and Intellectual Property\nREGULATING CHATBOT OUTPUT VIA\nINTER-INFORMATIONAL COMPETITION\nJiawei Zhang*\nABSTRACT—The advent of ChatGPT has sparked over a year of\nregulatory frenzy. Policymakers across jurisdictions have embarked on an\nAI regulatory “arms race,” and worldwide researchers have begun devising\na potpourri of regulatory schemes to handle the content risks posed by\ngenerative AI products as represented by ChatGPT. However, few existing\nstudies have rigorously questioned the assumption that, if left unregulated,\nAI chatbot’s output would inflict tangible, severe real harm on human\naffairs. Most researchers have overlooked the critical possibility that the\ninformation market itself can effectively mitigate these risks and, as a result,\nthey tend to use regulatory tools to address the issue directly.\nThis Article develops a yardstick for re-evaluating both AI-related\ncontent risks and corresponding regulatory proposals by focusing on inter-\ninformational competition among various outlets. The decades-long history\nof regulating information and communications technologies indicates that\nregulators tend to err too much on the side of caution and to put forward\nexcessive regulatory measures when encountering the uncertainties brought\nabout by new technologies. In fact, a trove of empirical evidence has\ndemonstrated that market competition among information outlets can\neffectively mitigate many risks and that overreliance on direct regulatory\ntools is not only unnecessary but also detrimental.\nThis Article argues that sufficient competition among chatbots and\nother information outlets in the information marketplace can sufficiently\nmitigate and even resolve some content risks posed by generative AI\ntechnologies. This may render certain loudly advocated but not well-tailored\n* Ph.D. Candidate and Research Fellow at the Technical University of Munich; Guest Researcher at Max\nPlanck Institute for Innovation and Competition. M.Phil. (Oxon); LL.M. (U.C. Berkeley). This Article\nbenefited from presentations at conferences and workshops at the European University Institute,\nUniversity of Tübingen, University of Oxford, Leiden University, IE University, and Osnabrück\nUniversity. I owe many thanks to the organizers, discussants, and participants of these great events; to\nGabriele Carovano, Urs Gasser, Georg Gesk, Philipp Mahlow, Philip Meinel, Xiangyu Ma, Scott Marcus,\nHelga Nowotny, Henrik Nolte, Boris Paal, Pier Luigi Parcu, Yahui Song, Alessio Tartaro, Simone van\nder Hof, Hao Yuan, and Chongyao Wang for their helpful comments, advice, and support for this Article;\nand to Kailey Fairchild, Adam Nguyen, Kate Richerson, Matt Green and other editors of the Northwestern\nJournal of Technology and Intellectual Property for their excellent editorial work. All errors are my own.\nI welcome any comments on this Article: victor.jiawei.zhang@gmail.com.\n109\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nregulatory strategies—like mandatory prohibitions, licensure, curation of\ndatasets, and notice-and-response regimes—unnecessary and even toxic to\ndesirable competition and innovation throughout the AI industry. For\nprivacy disclosure, copyright infringement, and any other risks that the\ninformation market might fail to satisfactorily address, proportionately\ndesigned regulatory tools can help to ensure a healthy environment for the\ninformational marketplace and to serve the long-term interests of the public.\nUltimately, the ideas that I advance in this Article should pour some much-\nneeded cold water on the regulatory frenzy over generative AI and steer the\nissue back to a rational track.\nTABLE OF CONTENTS\nI. INTRODUCTION ..................................................................................................... 110\nII. CONTENT RISKS ................................................................................................... 113\nIII. RISK CONTROL VIA A MARKET-CENTERED APPROACH ......................................... 117\nA. Why a Market-Centered Approach? ........................................................... 117\nB. Chatbot Output in the Information Marketplace ........................................ 123\n1. Internal Market Competition............................................................... 123\n2. External Market Competition .............................................................. 128\nIV. RE-EVALUATIONS & SUGGESTIONS....................................................................... 131\nA. Re-evaluations of Existing Regulatory Proposals....................................... 132\n1. Mandatory Content-Based Prohibition ............................................... 132\n2. Licensure ............................................................................................. 134\n3. Curation of Datasets ............................................................................ 135\n4. Transparency ....................................................................................... 136\n5. Traceability.......................................................................................... 139\n6. Notice-and-Respond Mechanism ........................................................ 141\n7. Audits .................................................................................................. 143\n8. Liability ............................................................................................... 145\nB. Further Suggestions .................................................................................... 147\n1. Re Internal-Market Competition ......................................................... 147\n2. Re External-Market Competition ........................................................ 149\nV. CONCLUSION ........................................................................................................ 153\nI. INTRODUCTION\nOn May 24, 2023, Sam Altman, the CEO of OpenAI, cautioned that the\ncompany might leave the European Union (EU) owing to its overregulation\nof technologies, including generative AI.1 His warning served as a sharp\n1 See OpenAI May Leave the EU if Regulations Bite – CEO, REUTERS (May 24, 2023, 4:22 PM),\nhttps://www.reuters.com/technology/openai-may-leave-eu-if-regulations-bite-ceo-2023-05-24/\n[https://perma.cc/9N4D-5NB5].\n110\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nreminder for policymakers in far-flung jurisdictions around the world: there\nmust be careful deliberation to identify the optimal level of regulation that\nsimultaneously promotes the responsible growth of generative AI and\nmitigates the recognized risks.\nAmong myriad risks created and magnified by generative AI systems,\ncontent risks—such as the risks of harmful content, discrimination and bias,\nmisinformation, privacy disclosure, and copyright infringement2—have\nprogressively become the focal point of regulatory examination and\nscholarly discourse. Policymakers and researchers have designed a series of\nnovel regulatory tools to mitigate the recognized content risks induced by\ngenerative AI. Most notably, these tools include mandatory prohibitions,\nlicensure, curation of datasets, transparency, traceability, notice-and-respond\nrules, and auditing procedures.3 Some other legal experts have proposed to\napply established legal frameworks to the chatbot context.4\nHowever, a lingering issue confronting us is whether these regulatory\napproaches are necessary and, on balance, helpful. A more precise\nformulation of this issue is how we should tailor a regulatory approach so\nthat it is consistently proportionate. Historically, government regulations on\nother information and communications technologies (ICTs) have imparted\nto us substantial knowledge about the sensible regulation of emerging\ntechnologies. Foremost among these lessons is that unnecessary regulation\ncan do more harm than good.5 To temper the prevailing regulatory fervor\nsurrounding generative AI and to put the AI policy discourse back on a more\njudicious path, it is imperative that we establish a robust analytical\nframework within which we can critically and accurately re-evaluate both\nthe content risks posed by generative AI systems and the corresponding\nregulatory proposals. In this Article, I advance a market-centered approach\nthat enables us to evaluate these proposals in line with an accurate\n2 For detailed explanations of the risks of generative AI, see, for example, OpenAI, GPT-4 Technical\nReport (Dec. 19, 2023) [hereinafter OpenAI’s Report] (unpublished manuscript) (on file with arXiv),\nhttps://arxiv.org/abs/2303.08774v4 [https://perma.cc/USZ5-UMBJ]; Laura Weidinger et al., Ethical and\nSocial Risks of Harm from Language Models (Dec. 8, 2021) [hereinafter Risks of LLMs] (unpublished\nmanuscript) (on file with arXiv), https://arxiv.org/abs/2112.04359v1 [https://perma.cc/GL4K-KTKD].\nPart II analyzes five kinds of content risks. See discussion infra Part II.\n3 See discussion infra Section IV.A.1–.7.\n4 See discussion infra Section IV.A.8.\n5 See, e.g., Mark A. Lemley, The Contradictions of Platform Regulation, 1 J. FREE SPEECH L. 303,\n330–35 (2021) (explaining why regulation is ineffective and even harmful to market competition and\ninnovation); LAWRENCE LESSIG, FREE CULTURE: HOW BIG MEDIA USES TECHNOLOGY AND THE LAW TO\nLOCK DOWN CULTURE AND CONTROL CREATIVITY 199 (2004) (arguing that “[o]verregulation stifles\ncreativity, [. . .] corrupts citizens and weakens the rule of law”).\n111\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nunderstanding of the competitive relationships among chatbots and\ninformation outlets.6\nThis Article proceeds in four Parts. Part II is purely descriptive,\nexploring five kinds of content risks posed by generative AI: harmful\ncontent, discrimination and bias, misinformation, privacy disclosure, and\ncopyright infringement. Part III proposes the market-centered approach.\nSection A rationalizes why, in general, a market-centered approach\nconstitutes an appropriate instrument for the assessment of both content risks\nand regulatory policies relative to generative AI. After reviewing the long\nhistory of regulatory schemes targeting other ICTs, Section B evaluates the\ncontent risks of chatbots by focusing on inter-informational competition,\nnamely the internal market competition (i.e., AI-enabled chatbot\ncompetition) and external market competition (i.e., broader competition\namong various information outlets). This Part shows that sufficient inter-\ninformational competition can mitigate some chatbot-content risks, such as\nharmful content, discrimination and bias, and misinformation. However, as\npotential market failure is always a possibility, policymakers need to design\nand implement a proportionate regulatory scheme that is especially\nresponsive to privacy disclosure, copyright infringement, and other risks\nwhich are less easily addressed through market competition.\nPart IV re-evaluates the regulatory proposals and suggests ways to\nhandle unresolved issues related to generative AI. In Section A, I argue that\nmandatory prohibitions, licensure, curation of datasets, and notice-and-\nrespond mechanisms are not well-tailored, constituting unnecessary\nendeavors in regulating the output of chatbots. Instead, policymakers should\nemphasize transparency, traceability, and auditing, which, when\nappropriately tailored, can promote healthy inter-informational competition\nin ways that effectively mitigate content risks. Furthermore, I recommend\nthat, in specific cases involving the adjudication of chatbot liability, courts\nshould confirm that tangible harm exists by distinguishing it from mere\nhypothetical risks and should subsequently conduct a risk-utility test—the\nHand formula (specifically, the marginal Hand formula)—to reach an\nequitable apportionment of responsibilities.\nSection B of Part IV suggests how policymakers should approach\ncertain unaddressed risks in internal and external markets. In the first point,\nwhich focuses on the internal market, I endorse several market-based and\ntechnology-neutral regulatory tools capable of deterring concentration and\nother anticompetitive practices in the generative AI industry. In the second\npoint, I propose that a personalized and decentralized privacy protection\n6 See discussion infra Part III.\n112\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nscheme can mitigate the risks of privacy disclosure by striking a healthy\nbalance between AI companies’ interest in training their large language\nmodels (LLMs) and AI users’ expectations of privacy. I then address\ncopyright infringement issues by employing the first and fourth factors of\nfair use. I explain that chatbots’ summaries and brief quotations of\ncopyrighted works, when rigorously cited, can help soften the competitive\nrelationship between chatbot output and copyrighted works.\nII. CONTENT RISKS\nI have no ambition here to design a seamless classification system of\ncontent risks in the chatbot industry. Rather, my aim is to present a non-\nexhaustive and purely descriptive list of widely discussed and legally\ncontroversial content risks posed by chatbot output, namely harmful content,\ndiscrimination and bias, misinformation, privacy disclosure, and copyright\ninfringement.\nHarmful Content. “Harmful content” can be used broadly to indicate all\nproblematic information generated by chatbots.7 This Article, however,\nadopts a narrower definition, using “harmful content” in parallel with other\nkinds of content risks, to roughly indicate information that defies the values\nor spirits of human civilization, such as hate speech. However, the extent to\nwhich the content can be harmful remains a matter of debate.8 The primary\nreason for this lack of consensus is that perceptions of what constitutes\nharmful content vary markedly across individuals and cultures. For example,\nChina’s AI regulation prioritizes conformity with the country’s so-called\n“core socialist values,” and thus broadly defines harmful output as any\ncontent that threatens this rather rigid take on social stability and national\nsecurity.9 Of course, in other countries, particularly those in the West, such\n7 See, e.g., Mark A. Lemley et al., Where’s the Liability in Harmful AI Speech?, 3 J. FREE SPEECH L.\n589, 595–601 (2023).\n8 See OpenAI’s Report, supra note 2, at 47 n.12.\n9 See Shengcheng Shi Rengong Zhineng Fuwu Guanli Zanxing Banfa (生成式人工智能服务管理\n暂行办法) [Interim Measures for Regulating Generative AI Services] (promulgated by Cyberspace\nAdmin., Nat’l Dev. & Reform Comm’n, Ministry Educ., Ministry Sci. & Tech., Ministry Indus. & Info.\nTech., Ministry Pub. Sec., Nat’l Radio & Television Admin., July 10, 2023, effective Aug. 15, 2023)\n[hereinafter Chinese Generative AI Measures], http://www.cac.gov.cn/2023-\n07/13/c_1690898327029107.htm [https://perma.cc/FA3M-K2ER] (China). For a complete English-\ntranslated version, see, for example, Interim Measures for the Management of Generative Artificial\nIntelligence Services, CHINA L. TRANSLATE (July 13, 2023),\nhttps://www.chinalawtranslate.com/en/generative-ai-interim/ [https://perma.cc/4V4M-GEDC]. Article\n4(1) provides:\nThe provision and use of generative AI services shall comply with laws and administrative\nregulations, respect social morality and ethics, and meet the following requirements: Core\n113\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nconsiderations play little or no role in governmental policymaking.\nMoreover, chatbot companies have their own understanding of what\nconstitutes harmful content. For example, OpenAI defines “harmful content”\nas\n(1) advice or other content promoting self-harm behaviors,\n(2) graphic materials that are erotic or violent in nature,\n(3) harassing, demeaning, and hateful content\n(4) content useful for the planning or carrying out of violent acts, or\n(5) information promoting the discovery or disclosure of illegal\ncontent.10\nAccording to the GPT-4 Technical Report, OpenAI has already substantially\naddressed these concerns.11\nDiscrimination and Bias. Another common concern about chatbot\noutput is that it can promote discrimination and bias.12 One illustrative\nexample of this possibility occurred when a chatbot, when asked to finish a\nsentence beginning with the words “Two Muslims walked into a . . .,”\nanswered “. . . Texas cartoon contest and opened fire.”13 A milder example\nof discrimination and bias occurred when a chatbot defined “family” as “a\nman and a woman who get married and have children”—a definition that\nexcludes, among others, homosexual child-rearing spouses and childless\nspouses.14 These risks arise because the prodigious volume of training data\nused by LLMs for the generation of information represents commonly held\ndiscriminatory and biased views, some of which can be regarded as\nharmful.15 OpenAI has endeavored to mitigate these risks by stopping\nchatbots from responding to queries that might be likely to trigger a\ndiscriminatory or biased response.16 This solution, however, is evidently a\nstopgap measure. After all, a chatbot’s refusal to answer risky questions is\nunderstandably costly from a technical perspective and, in all likelihood,\nsocialist values shall be upheld, and it is prohibited to generate any content prohibited by laws\nor administrative regulations, such as content inciting subversion of national sovereignty or\nthe overturn of the socialist system, threatening national security and interests, harming the\nnation’s image, inciting separatism, undermining national unity and social stability . . . as\nwell as content containing false or harmful information. . . . (emphasis added).\n10 OpenAI’s Report, supra note 2, at 47.\n11 Id. at 48.\n12 See Moin Nadeem et al., StereoSet: Measuring Stereotypical Bias in Pretrained Language Models,\n1 PROC. OF THE 59TH ANN. MEETING OF THE ASS’N FOR COMPUTATIONAL LINGUISTICS & THE 11TH\nINT’L J. CONF. ON NAT. LANGUAGE PROCESSING 5356 (2021) (showing that LLMs exhibit strong\nstereotypical biases).\n13 Risks of LLMs, supra note 2, at 9.\n14 Id. at 13.\n15 Id. at 11–12.\n16 See OpenAI’s Report, supra note 2, at 49.\n114\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nwholly infeasible from an intellectual perspective, as there is no consensus\nabout what constitutes harmful discrimination and bias.17\nMisinformation. The third pressing concern revolves around the\nvoluminous misinformation fabricated by LLMs. Experts have voiced their\napprehension regarding LLMs’ unbridled capacity to generate\nhallucinatorily inaccurate misinformation with alarming velocity and in\nimmense volume.18 Recent news reports and academic research strongly\nindicate that chatbot-generated misinformation can pollute the information\nenvironment,19 for example, by tarnishing the reputation of an innocent\nperson,20 or by encouraging people to engage in patently faulty actions.21 As\na result, chatbot output poses a significant threat to a wide array of targets,\nincluding democratic discourse,22 and electoral processes.23 The threat of\nmisinformation is further compounded when malicious actors manipulate\n17 See Risks of LLMs, supra note 2, at 12–13.\n18 See id. at 21–25; Sarah Kreps et al., All the News That’s Fit to Fabricate: AI-Generated Text as a\nTool of Media Misinformation, 9 J. EXPERIMENTAL POL. SCI. 104, 105 (2022). But see Felix M. Simon et\nal., Misinformation Reloaded? Fears About the Impact of Generative AI on Misinformation Are\nOverblown, 4 HARV. KENNEDY SCH. MISINFORMATION REV. 1, 1 (2023) (arguing that “the effects of\ngenerative AI on the misinformation landscape are overblown”).\n19 See, e.g., Ziv Epstein et al., Art and the Science of Generative AI, 380 SCI. 1110, 1111 (2023)\n(arguing that “[n]ew possibilities for the generation of photorealistic synthetic media . . . may undermine\ntrust in authentically-captured media via the liar’s dividend”); Melissa Heikkilä, How to Spot AI-\nGenerated Text, MIT TECH. REV. (Dec. 19, 2022),\nhttps://www.technologyreview.com/2022/12/19/1065596/how-to-spot-ai-generated-text/\n[https://perma.cc/7ZZX-MSV5] (“In an already polarized, politically fraught online world, these AI tools\ncould further distort the information we consume.”); Emily Bell, A Fake News Frenzy: Why ChatGPT\nCould Be Disastrous for Truth in Journalism, THE GUARDIAN (Mar. 3, 2023),\nhttps://www.theguardian.com/commentisfree/2023/mar/03/fake-news-chatgpt-truth-journalism-\ndisinformation [https://perma.cc/JJV8-6PRW] (arguing that “the real peril lies outside the world of\ninstantaneous deception, which can be easily debunked, and in the area of creating both confusion and\nexhaustion by ‘flooding the zone’ with material that overwhelms the truth or at least drowns out more\nbalanced perspectives.”).\n20 See, e.g., Pranshu Verma & Will Oremus, ChatGPT Invented a Sexual Harassment Scandal and\nNamed a Real Law Prof as the Accused, WASH. POST (Apr. 5, 2023, 2:07 PM),\nhttps://www.washingtonpost.com/technology/2023/04/05/chatgpt-lies/ [https://perma.cc/7FA4-PV8Z].\n21 See, e.g., Sara Merken, New York Lawyers Sanctioned for Using Fake ChatGPT Cases in Legal\nBrief, REUTERS (June 26, 2023, 3:28 AM), https://www.reuters.com/legal/new-york-lawyers-sanctioned-\nusing-fake-chatgpt-cases-legal-brief-2023-06-22/ [https://perma.cc/C93W-EFTQ].\n22 See Sarah Kreps & Doug Kriner, How AI Threatens Democracy, 34 J. DEMOCRACY 122, 124\n(2023) (arguing that “[g]enerative AI threatens three central pillars of democratic governance:\nrepresentation, accountability, and ultimately, the most important currency in a political system––trust”);\nsee also James H. Kuklinski et al., Misinformation and the Currency of Democratic Citizenship, 62 J.\nPOL. 790 (2000) (showing how misinformation influences and reinforces citizens’ beliefs in political\ndiscourse).\n23 See, e.g., Joshua A. Tucker, AI Could Create a Disinformation Nightmare in the 2024 Election,\nTHE HILL (July 14, 2023), https://thehill.com/opinion/4096006-ai-could-create-a-disinformation-\nnightmare-in-the-2024-election/ [https://perma.cc/3H9J-U99N].\n115\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nchatbots.24 In all these respects, it is critical to consider the legal dichotomy\nbetween facts and opinions.25 Although dressed up as objective statements of\nfact, some chatbot output either reflects personal opinion or leans toward\nopinion.26 Because an LLM’s training corpus might harbor abundant\nopinions and semi-opinions either disguised or misinterpreted as hard facts,\nchatbot companies face the hugely challenging technical task of identifying\nopinions and eliminating them from “factual” chatbot output.27 To this end,\nOpenAI has made great strides in addressing this matter.28\nPrivacy Disclosure. Some AI-generated facts, although accurate and\nobjective, may risk legally unacceptable disclosures of privacy. This risk\nmaterializes when LLMs train on a corpus that includes sensitive personal\ninformation or even legally classified information.29 Chatbots can violate a\nspecific person’s privacy either by directly disclosing memorized sensitive\ninformation about the person or by making correct inferences based on\ncorrelational data about the person.30 Public figures are at a higher risk of\nprivacy exposure than are average citizens because an LLM system is more\nlikely to document information about the former than about the latter.31\nOpenAI has taken some steps to mitigate the privacy breach risk.32\nCopyright Infringement. Chatbot output can trigger concerns related to\ncopyright infringement. However, neither the general public nor AI\n24 See, e.g., Andrew Myers, AI’s Powers of Political Persuasion, STAN. HAI (Feb. 27, 2023),\nhttps://hai.stanford.edu/news/ais-powers-political-persuasion [https://perma.cc/3A5Y-9J7R] (“Large\nlanguage models, such as GPT-3, might be applied by ill-intentioned domestic and foreign actors through\nmis- or disinformation campaigns or to craft problematic content based on inaccurate or misleading\ninformation for as-yet-unforeseen political purposes.”).\n25 In a legal context, pure statements of fact and expressions of an opinion are accorded different\nlevels of protection. See Jacobus v. Trump, 51 N.Y.S.3d 330, 337 (N.Y. Sup. Ct. 2017) (“The privilege\nprotecting the expression of an opinion is rooted in the preference that ideas be fully aired.”); Gettner v.\nFitzgerald, 677 S.E.2d 149, 153 (Ga. Ct. App. 2009) (“[A] statement that reflects an opinion or subjective\nassessment, as to which reasonable minds could differ, cannot be proved false.”); Info. Sys. & Networks\nCorp. v. City of Atlanta, 281 F.3d 1220, 1228 (11th Cir. 2002) (“Because Commissioner McCall’s\nstatement was an opinion—and thus subjective by definition—it is not capable of being proved false.”).\n26 See Risks of LLMs, supra note 2, at 23 (noting that LLMs may present a majority opinion as\nfactually correct).\n27 Id. at 23–24.\n28 See OpenAI’s Report, supra note 2, at 46.\n29 See Hannah Brown et al., What Does It Mean for a Language Model to Preserve Privacy?, FACCT\n’22: PROC. OF THE 2022 ACM CONF. ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY 2280, 2282\n(2022).\n30 See Risks of LLMs, supra note 2, at 18–21.\n31 See OpenAI’s Report, supra note 2, at 53.\n32 Id. (the measures taken by OpenAI to reduce privacy risks include “fine-tuning models to reject\nthese types of requests, removing personal information from the training dataset where feasible, creating\nautomated model evaluations, monitoring and responding to user attempts to generate this type of\ninformation, and restricting this type of use in our terms and policies”).\n116\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\ncompanies regard this risk as particularly alarming, possibly because\ncopyright infringement risks are essentially interest-based, rather than ethics-\nbased, risks.33 Copyright holders, on the other hand, have expressed a strong\naversion to generative AI, and some have even filed lawsuits against OpenAI\nfor its alleged role in copyright infringement.34 A common foundation of\nthese legal petitions is that developers of chatbots copied the authors’\ncopyrighted works to LLM-training databases without obtaining the authors’\npermission—a situation resulting in chatbot-generated derivative output that\ninfringed on the authors’ copyright.35 For OpenAI, however, the risk of\ncopyright infringement is not as serious a concern as the four previously\nmentioned risks; in fact, a report that OpenAI drafted regarding the risks of\ngenerative AI makes no mention of copyright risk.36 OpenAI and other\nchatbot developers argue that the fair use doctrine protects their unlicensed\nuse of copyrighted materials. 37 Incidentally, this defense has gained\nwidespread support from copyright scholars.38\nIII. RISK CONTROL VIA A MARKET-CENTERED APPROACH\nA. Why a Market-Centered Approach?\nNo single regulatory approach can be reasonably viewed as a once-and-\nfor-all solution. That said, vigorous market competition usually works more\n33 See Laura Weidinger et al., Taxonomy of Risks Posed by Language Models, FACCT ’22: PROC. OF\nTHE 2022 ACM CONF. ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY 214, 221 (2022)\n(categorizing the risks of copyright violation as socioeconomic harms).\n34 See, e.g., Alexandra Alter & Elizabeth A. Harris, Franzen, Grisham and Other Prominent Authors\nSue OpenAI, N.Y. TIMES (Sept. 20, 2023), https://www.nytimes.com/2023/09/20/books/authors-openai-\nlawsuit-chatgpt-copyright.html [https://perma.cc/K4FQ-VFUD]; Bobby Allyn, ‘New York Times’\nConsiders Legal Action Against OpenAI as Copyright Tensions Swirl, NPR (Aug. 16, 2023),\nhttps://www.npr.org/2023/08/16/1194202562/new-york-times-considers-legal-action-against-openai-as-\ncopyright-tensions-swirl [https://perma.cc/D664-SLVP]; Blake Brittain, Lawsuit Says OpenAI Violated\nUS Authors’ Copyrights to Train AI Chatbot, REUTERS (June 29, 2023),\nhttps://www.reuters.com/article/ai-copyright-lawsuit-idCAKBN2YF17R [https://perma.cc/GJ62-\nTZNY].\n35 Id.; see also Pamela Samuelson, Generative AI Meets Copyright, 381 SCI. 158, 159 (2023).\n36 See OpenAI’s Report, supra note 2.\n37 See 17 U.S.C. § 107.\n38 See, e.g., Mark A. Lemley & Bryan Casey, Fair Learning, 99 TEX. L. REV. 743, 748 (2021)\n(proposing to use and adjust the fair use doctrines to protect some forms of copying of copyrighted\nmaterials for machine learning); Matthew Sag, The New Legal Landscape for Text Mining and Machine\nLearning, 66 J. COPYRIGHT SOC’Y U.S.A. 291, 314–28 (2019) (discussing how fair use doctrine should\nbe applied to copying conduct in the context of text data mining); Stephen Wolfson, Fair Use: Training\nGenerative AI, CREATIVE COMMONS (Feb. 17, 2023), https://creativecommons.org/2023/02/17/fair-use-\ntraining-generative-ai/ [https://perma.cc/TMZ5-6NBN] (arguing that “fair use should permit using\ncopyrighted works as training data for generative AI models” given the copyright law purpose to\n“encourage the new creative works, to promote learning, and to benefit the public interest”).\n117\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\neffectively than regulation for the management of risks. Although regulation\nis seen as a direct approach to an actual or potential problem, overreliance\non regulation can lead to a notorious, vicious cycle. Regulation can prevent\nnascent market players from entering a private-sector market, which\nexacerbates concentration levels, in turn triggering even harsher\nregulations.39 The actions of the European Union’s General Data Protection\nRegulation (GDPR) illustrate this vicious cycle.40 Moreover, policymakers,\ndue to a lack of technical knowledge, are potentially subject to regulatory\ncapture by AI giants, which can further entrench the incumbents and\nfacilitate an establishment of “a government-protected cartel that is insulated\nfrom market competition.”41 In contrast, market-based approaches to a vast\nswath of real and potential problems are more technology-neutral, more\ndynamic, and more adaptable to unpredictable risks than conduct-based\nregulatory approaches.\nIndeed, how we should deal with the content risks posed by generative\nAI is a topic that has already been well-documented in the history of ICT\npolicymaking. In the United States, the advancement of ICTs synchronized\nwith government regulation has triggered a series of intensive debates\nconcerning the relative merits of regulation and competition with respect to\nfacilitating ICT innovation and enhancing consumer welfare. For over half a\ncentury, the competition-versus-regulation debate has pivoted from one\n39 See, e.g., Mark A. Lemley, The Contradictions of Platform Regulation, 1 J. FREE SPEECH L. 303,\n335 (2021); LAWRENCE LESSIG, FREE CULTURE: HOW BIG MEDIA USES TECHNOLOGY AND THE LAW TO\nLOCK DOWN CULTURE AND CONTROL CREATIVITY 199 (2004).\n40 See Michal S. Gal & Oshrit Aviv, The Competitive Effects of the GDPR, 16 J. COMPETITION L. &\nECON. 349, 349 (2020) (arguing that “[t]he GDPR creates two main harmful effects on competition and\ninnovation: it limits competition in data markets, creating more concentrated market structures and\nentrenching the market power of those who are already strong; and it limits data sharing between different\ndata collectors, thereby preventing the realization of some data synergies which may lead to better data-\nbased knowledge”); Garrett A. Johnson et al., Privacy and Market Concentration: Intended and\nUnintended Consequences of the GDPR, 69 MGMT. SCI. 5695 (2023) (showing that GDPR increased\ndigital market concentration).\n41 COMMUNICATIONS AND DIGITAL COMMITTEE, LARGE LANGUAGE MODELS AND GENERATIVE AI,\n2023–24, HL 54, ¶¶ 43–49 (UK).\n118\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\ntheme to another, including the fairness doctrine,42 must-carry rules,43\nnetwork neutrality policies,44 and, recently, platform-neutrality proposals.45\nFrom this lengthy and varied debate, we can identify at least four common\nlessons that are relevant to the current AI regulatory context.\nFirst, policymakers and researchers have a great tendency to overstate\nthe risks associated with emerging technologies and to reflexively advocate\nharsh conduct-based regulations for the mitigation of these overstated risks.\nFor instance, one century ago, when radio broadcasting was born, people\n42 Compare In the Matter of Editorializing by Broadcast Licensees, 13 F.C.C. 1246, 1249, 1254\n(1949), and Applicability of the Fairness Doctrine in Handling of Controversial Issues of Public\nImportance, 29 Fed. Reg. 10415 (1964) (proposing fairness doctrine to require broadcast licensees to\npresent these controversial public issues in a fair and balanced way so as to ensure a free and fair\ncompetition of competing views), with Loveday v. FCC, 707 F.2d 1443, 1459 (D.C. Cir. 1983)\n(explaining that the First Amendment protections of broadcast political speech may expand due to the\ndramatic increase in the number of broadcast stations), and FCC v. League of Women Voters of Cal., 468\nU.S. 364, 376 (1984) (“[W]ith the advent of cable and satellite television technology, communities now\nhave access to such a wide variety of stations that the scarcity doctrine is obsolete.”).\n43 Compare Rules re Microwave-Served CATV, First Report and Order, 38 F.C.C. 683, 683–84\n(1965) (initiating the must-carry rules to require the private CATV microwave stations, as requested by\nany television broadcast station, must “carry the station’s signal without material degradation”), and\nCATV, Second Report and Order, 2 F.C.C.2d 725 (1966) (extending must-carry obligations to all kinds\nof cable systems), with Cable Television Consumer Protection and Competition Act of 1992, Pub. L. No.\n102–385, 106 Stat. 1460, 1471–81 (1992) (narrowly tailoring the scope of must-carry rules to market\nfunctions), and Turner Broad. Sys., Inc. v. FCC, 512 U.S. 622, 643 (1994) (ruling that unregulated cable\noperators had the potential to silence the voice of the broadcast stations so that noncable subscribers will\nsuffer from the loss of otherwise diverse and antagonistic sources of information), and Laurence H.\nWiner, The Red Lion of Cable, and Beyond? – Turner Broadcasting v. FCC, 15 CARDOZO ARTS & ENT.\nL.J. 1, 67–68 (1997) (arguing that “[t]he proliferation of converging yet competing technologies for the\n(interactive) exchange of information of all kinds may well create ‘channels’ of communication that\noutstrip the ‘programming’ available to fill them”).\n44 Compare In the Matter of Preserving the Open Internet Broadband Indus. Pracs., 25 F.C.C. Rcd.\n17905 (2010) (implementing network neutrality rules by establishing three obligations for broadband\noperators to fulfill: transparency, no blocking, and no reasonable discrimination), and In the Matter of\nProtecting & Promoting the Open Internet, 30 F.C.C. Rcd. 5601, 5757 (2015) (reclassifying the\nbroadband service as telecommunications services that are subject to common carrier obligations,\nincluding no blocking, no throttling, no paid prioritization, enhanced transparency, and general rules of\nno unreasonable interference), with In the Matter of Restoring Internet Freedom, 33 F.C.C. Rcd. 311\n(2018) [hereinafter 2018 Internet Order] (reclassifying broadband service as the information service and\nembracing light-touch regulation based on economic reasons).\n45 Compare Lina M. Khan, Sources of Tech Platform Power, 2 GEO. L. TECH. REV. 325, 331–34\n(2018) (arguing that a dominant platform should be required to “treat all commerce flowing through its\ninfrastructure equally” and prevented from “using the threat of discrimination to extract and extort”), and\nFrank Pasquale, Platform Neutrality: Enhancing Freedom of Expression in Spheres of Private Power, 17\nTHEORETICAL INQUIRIES L. 487, 497–503 (2016) (suggests regulating digital platforms, such as Google,\nwith must-carry rules), with Herbert Hovenkamp, Antitrust and Platform Monopoly, 130 YALE L.J. 1952,\n1971 (2021) (arguing that the antitrust approach works more effectively than regulations in addressing\nplatform monopoly issues), and Lemley, supra note 5, at 331–35 (arguing that “regulatory choices . . .\nare likely to entrench those incumbents, making it harder and more costly for someone to compete with\nthem and eliminating the possibility of competing by offering a different set of policies”).\n119\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nwere deeply concerned that the new form of information dissemination, if\nleft unregulated, could distort people’s beliefs and harm our democracy.46\nThus, the U.S. Federal Communications Commission (FCC) implemented\nthe fairness doctrine. Essentially, the fairness doctrine requires that broadcast\nlicensees not only allocate sufficient time for the coverage of issues relevant\nto the public interest, but also present these controversial issues in a fair and\nbalanced way. In other words, the fairness doctrine is supposed to ensure that\ndivergent and competing voices can be heard by the public.47 The advocates\nof the fairness doctrine have argued that the underpinning rationale for this\nduty-setting rests largely on the physical scarcity of broadcast frequencies.48\nHowever, Coase argues that the scarcity rationale fails to establish any\ncompelling justification for widespread governmental interference because\nalmost all resources are scarce in the economic sense.49\nThe second common lesson to be drawn from the competition-versus-\nregulation debate is that researchers and policymakers generally regard\ncompetition as primary, and regulation as ancillary. The legitimacy of this\nrule has been repeatedly and clearly documented. Just consider the following\ntwo phenomena. First, it is true that some ICT policies have initially been\nladen with conduct-based regulations, but these policies have eventually\nended up embracing market-based approaches, particularly after a change in\nmarket conditions. For example, the FCC in 1987 repealed the fairness\ndoctrine after the emergence and growth of other information technologies.50\n46 Don’t these feel somewhat familiar? Such remarks have also been repetitively applied to ChatGPT\nwithout any alterations one century later. See PAUL STARR, THE CREATION OF THE MEDIA: POLITICAL\nORIGINS OF MODERN COMMUNICATIONS 347–48 (2004) (arguing that “radio threatened to distort\n[democracy]” because “there developed an interdependence between those who held political power (and\nneeded radio) and those who controlled radio (and needed political goodwill)”); MONROE E. PRICE,\nTELEVISION, THE PUBLIC SPHERE AND NATIONAL IDENTITY 161 (1995) (a congressman stated the similar\nconcern that “[t]here is no agency so fraught with possibilities for service of good or evil to the American\npeople as the radio” and that “[broadcasting stations] can mold and crystallize sentiment as no agency in\nthe past has been able to do”); see also Kreps & Kriner, supra note 22; Nathan E. Sanders & Bruce\nSchneier, How ChatGPT Hijacks Democracy, N.Y. TIMES (Jan. 15, 2023),\nhttps://www.nytimes.com/2023/01/15/opinion/ai-chatgpt-lobbying-democracy.html\n[https://perma.cc/W5VM-XGME] (describing how ChatGPT can be misused to generate voluminous\ninformation to influence democratic discourse and policymaking process).\n47 See Applicability of the Fairness Doctrine in Handling of Controversial Issues of Public\nImportance, supra note 42.\n48 See, e.g., Red Lion Broad. Co. v. FCC, 395 U.S. 367, 390 (1969) (“Because of the scarcity of radio\nfrequencies, the Government is permitted to put restraints on licensees in favor of others whose views\nshould be expressed on this unique medium.”).\n49 See R. H. Coase, The Federal Communications Commission, 2 J.L. & ECON. 1, 14 (1959).\n50 See In Re Complaint of Syracuse Peace Council Against Television Station WTVH Syracuse, New\nYork, 2 F.C.C. Rcd. 5043, 5051 (1987) (noting that “the growth in both radio and television broadcasting\nalone provided ‘a reasonable assurance that a sufficient diversity of opinion on controversial issues of\n120\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nSimilarly, the FCC in 2018 terminated its network neutrality rules after\nrealizing that broadband service providers had already been subject to\nintense healthy market competition and that light-touch regulation would be\nsufficient.51 A second phenomenon pointing to the primacy of competition\nover regulation is the tendency of regulation advocates to demonstrate,\nbefore proposing their regulatory rules, that market failure is an inescapable\nfact and that self-regulation is impossible.52 This means that, even for\nregulation advocates, regulatory endeavors can only be justified when the\nmarket is functioning ineffectively. Thus, regarding ICT service providers,\nif the market is already capable of regulating them in ways that satisfactorily\naddress public concerns, a governmental regulatory approach would seem to\nbe patently unwarranted. The very possibility of such an outcome means that\npolicymakers should always first analyze whether a specific market player\nor industry is already or will soon be subject to competition capable of\nachieving regulatory effects similar to or better than those associated with\ngovernment intervention.\nA third lesson we can learn is that government policymakers should\ntailor regulatory approaches to the specific aim of invigorating the\neffectiveness of markets. The primary difference between conduct-based\nregulations and market-centered regulations is that the former are an attempt\nto fix a perceived problem directly, whereas the latter are an attempt to fix\nnot the problem, but an indirectly related obstacle to free market operations,\nwhich, once unshackled from the obstacle, can themselves solve the\nproblem. The history of U.S. ICT regulation indicates that U.S. policymakers\nhave favored market-centered approaches because they generally prove to be\nsufficient and even better than direct regulation of a perceived problem. For\ninstance, the court in 1985 found that the first version of the must-carry rules\nviolated the Constitution’s First Amendment partly because the rules\nrestricted editorial freedom and harmed market competition.53 A few months\nafter this ruling, the FCC published a modified version of the must-carry\nrules, and this time they were narrowly tailored to “maximiz[e] program\npublic importance [would] be provided in each broadcast market’”) (quoting Inquiry Into Section 73.1910\nof the Comm’n’s Rules and Reguls. Concerning Alts. to the Gen. Fairness Doctrine Obligations of Broad.\nLicensees, 102 F.C.C.2d 145, 147 (1985)).\n51 See 2018 Internet Order, supra note 44, ¶¶ 123–39, 232–38.\n52 See, e.g., Tim Wu, Network Neutrality, Broadband Discrimination, 2 J. ON TELECOMM. & HIGH\nTECH. L. 141, 143 (2003) (questioning the efficacy of the broadband market before proposing the non-\ndiscrimination rules); see also Khan, supra note 45, at 326–29 (2018) (explaining how tech platforms\nexercise their supra-market power before proposing common carriage rules like regulating broadband\nproviders).\n53 See Quincy Cable TV, Inc. v. FCC, 768 F.2d 1434, 1445, 1454, 1459–63 (D.C. Cir. 1985).\n121\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nchoice and preserv[e] competition in video services.”54 In the same vein, the\nFCC tailored network neutrality rules to the changed market conditions: of\nthe no-blocking, no-throttling, no-paid-prioritization, and transparency rules,\nonly the last one was left unretired in the 2018 Internet Order.55\nThe last lesson from the competition-versus-regulation debate is that,\nregarding ICTs, both policymaking and judicial rulings are generally\ncentered on enhancing people’s access to diverse, competing information\noutlets.56 One important way to promote this access is to ensure that people\nhave a sufficient choice of information sources. In terms of radio broadcasts,\nlisteners should be able to choose from several diverse broadcast stations,57\nor simply switch to other information outlets.58 Similarly, in terms of cable\ntransmission, a main concern of governments is that cable operators’\nconsiderable and growing market power and their market position as\ngatekeepers can significantly undermine the ability of local broadcasters to\ndeliver their unique messages to consumers.59 And in terms of broadband\nregulations, FCC policymakers felt comfortable easing their network\nneutrality rules only after discovering that competitive markets prevent\nbroadband operators from locking in broadband subscribers.60\nAccumulated during the past half-century, these invaluable insights into\nthe primacy of competition over regulation should not be disregarded in\nhaste; rather, they should be preserved and heeded for the current and future\nregulation of generative AI. By reviewing these historical regulatory\n54 See In the Matter of Amend. of Part 76 of the Comm’n’s Rules Concerning Carriage of Television\nBroad. Signals by Cable Television Sys., 1 F.C.C. Rcd. 864, ¶¶ 180–200 (1986).\n55 See 2018 Internet Order, supra note 44.\n56 See, e.g., Red Lion Broad. Co. v. FCC, 395 U.S. 367, 390 (1969) (“It is the right of the viewers\nand listeners, not the right of the broadcasters, which is paramount.”); Turner Broad. Sys., Inc. v. FCC,\n512 U.S. 622, 663 (1994) (holding that unregulated cable operators had the potential to silence the voice\nof the broadcast stations so that noncable subscribers will suffer from the loss of otherwise diverse and\nantagonistic sources of information); In the Matter of Preserving the Open Internet Broadband Indus.\nPracs., supra note 44, at 17918 (worrying that the unregulated broadband operators may have economic\nincentives to treat edge providers’ content differently, thereby harming the competition of adjacent\nmarkets and leading to lower quality and fewer choices for end users).\n57 See Loveday v. FCC, 707 F.2d 1443, 1459 (D.C. Cir. 1983) (explaining that the First Amendment\nprotections of broadcast political speech may expand due to the dramatic increase in the number of\nbroadcast stations).\n58 See FCC v. League of Women Voters of Cal., 468 U.S. 364, 376 n.11 (1984) (emphasis added)\n(“[W]ith the advent of cable and satellite television technology, communities now have access to such a\nwide variety of stations that the scarcity doctrine is obsolete.”).\n59 See Turner Broad. Sys., Inc. v. FCC, 520 U.S. 180 (1997).\n60 Compare In the Matter of Preserving the Open Internet Broadband Indus. Pracs., supra note 44, at\n17921 (noting that the broadband operators were not subject to effective market competition partly\nbecause subscribers have high switching costs), with 2018 Internet Order, supra note 44, ¶ 128\n(explaining that low churn rate does not necessarily indicate market power; instead, it may reflect\ncompetitive actions taken by broadband operators).\n122\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\npractices, we can better establish a forward-looking framework that will\neffectively manage the risks posed by generative AI and even the risks posed\nby other unpredictable future technologies. Specifically, we should integrate\nrigorous analysis of inter-informational competition into an equally rigorous\nanalysis of content risks. If inter-informational competition inherently\npossesses the potential to mitigate certain risks, these risks may not constitute\nactual harm to our society and thus may not warrant anything other than\nlight-touch regulation. In other words, regarding the desire to improve the\nquality of chatbot output, if market forces can achieve goals that are the\nequivalent of or even superior to the goals achievable by regulation, why\nwould a society engage in unnecessary and even counterproductive\nregulation?\nThe central issue that ought to attract special attention is the\nphenomenon of unfair competition between distinct sources of information.\nWe know that some kinds of information can have chilling effects on others.\nCopyright-infringing work, for example, may supersede—and thus silence—\ncopyrighted work.61 In the next several sections, I will evaluate internal- and\nexternal-market competition and unfair inter-informational competition\nfrom the perspective of information consumers.\nB. Chatbot Output in the Information Marketplace\nThe regulatory history of ICTs indicates a dichotomy between the\ninternal market, where competition takes place among providers of the same\nkind of ICT service (e.g., Broadband Provider A vs. Broadband Provider B),\nand the external market, where competition takes place among providers of\ndifferent kinds of ICT service (e.g., Cable Provider A vs. Broadband\nProvider A).62 Here, this division of the analysis into two types of markets\npermits an examination of whether chatbot companies face sufficient\ncompetition from one another and from other entities.\n1. Internal Market Competition\nInternal market competition is the output competition between chatbots.\nActually—the internal market is the chatbot product market. ChatGPT, for\nexample, vigorously competes with actors such as Microsoft CoPilot\n(formerly known as Bing), Google Gemini (formerly known as Bard),\nChatsonic, and Claude for market dominance regarding chatbot products.\nOne of the most important facets of this competition is the improvement of\n61 See, e.g., Gregory Day, Competition and Piracy, 32 BERKELEY TECH. L.J. 775, 790 n.88–91\n(2017) (explaining how free riding can gain economic advantages in the market by underselling the\noriginal good at a lower price).\n62 See supra notes 56–60 and accompanying text.\n123\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\noutput quality and the derisking of generated content. Thus, chatbot\ncompanies, if under sufficient market pressure, will have endogenous\nincentives to create products that cater as much as possible to public\nexpectations. Of course, all well-run companies attempt to prioritize their\ncommercial interests, but problems will not arise if the companies’\ncommercial interests are well-aligned with the ethical expectations of society\nand the policymakers’ regulatory goals of mitigating the recognized risks.\nGenerative AI service providers will not sit on the sidelines of risk as long\nas marketplace competition is possible, and as long as critical users can\nrecognize the output degradation.63\nA current inclination is to say that the internal market, namely the\nchatbot product market, is a competitive one. Admittedly, there are some\ninherent market barriers for companies (particularly for start-ups) seeking to\nenter the AI-driven chatbot market.64 But these barriers do not mean that a\nwinner can take all in the chatbot market. In fact, the function of an AI\nchatbot, from the consumer perspective, is very similar to the function of a\nsearch engine—they both respond to users’ queries based on datasets and\nalgorithmic operations.65 Thus, some pre-established rules for the search\nengine market would be well-suited here. First, in both the search engine and\nchatbot markets, service providers compete primarily over output relevance,\nquality, and speed.66 This fact implies that imposing regulatory obligations\non the three aforementioned parameters is unnecessary and even harmful\nbecause, again, a chatbot service provider has inherent incentives to improve\nits performances within these parameters so as to attract more users and to\nexpand its market share.\n63 See Maurice E. Stucke & Ariel Ezrachi, When Competition Fails to Optimize Quality: A Look at\nSearch Engines, 18 YALE J.L. & TECH. 70, 76–77 (2016) (arguing that search engine provider’s ability\nand incentive to degrade the quality depend on the degree of network effects, consumers’ switching costs,\nand their ability to accurately assess the quality differences).\n64 See, e.g., Diane Coyle, Preempting a Generative AI Monopoly, PROJECT SYNDICATE (Feb. 2,\n2023), https://www.project-syndicate.org/commentary/preventing-tech-giants-from-monopolizing-\nartificial-intelligence-chatbots-by-diane-coyle-2023-02 [https://perma.cc/M8LX-EB6L] (arguing that\n“the massive, immensely costly, and rapidly increasing computing power needed to train and maintain\ngenerative AI tools represents a substantial barrier to entry that could lead to market concentration”);\nThomas Höppner & Luke Streatfeild, ChatGPT, Bard & Co.: An Introduction to AI for Competition and\nRegulatory Lawyers, HAUSFELD (Feb. 23, 2023), https://www.hausfeld.com/what-we-think/competition-\nbulletin/chatgpt-bard-co-an-introduction-to-ai-for-competition-and-regulatory-lawyers/\n[https://perma.cc/XSW8-DEUF] (illustrating entry barriers at the AI compute level, data creation level,\nAI modeling level, and AI development level).\n65 See, e.g., Beatriz Botero Arcila, Is It a Platform? Is It a Search Engine? It’s Chat GPT! The\nEuropean Liability Regime for Large Language Models, 3 J. FREE SPEECH L. 455, 479–85 (2023)\n(analogizing some types of large language models to search engines).\n66 See MAURICE E. STUCKE & ALLEN P. GRUNES, BIG DATA AND COMPETITION POLICY 173 (2016).\n124\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nSecond, akin to the search engine market, the chatbot market does not\nexhibit a “direct network effect,”67 meaning switching costs are low.\nConsequently, when specific chatbot products function poorly, “it’s easy for\nusers to go elsewhere because [the] competition is only a click away.”68 As\nusers are not locked into a single generative AI system, chatbot companies\nface competitive pressure from user multihoming. 69 If ChatGPT, for\nexample, cannot produce information of satisfactorily high quality, users can\nturn to alternatives, such as Google Gemini or Claude, without incurring any\nsubstantial switching costs.70\nHowever, a comparison between the search engine market and the\nchatbot market suggests that the “data feedback loop” can hinder competition\namong chatbot companies, especially startups, which understandably lack a\nsufficient volume of data.71 Lessons drawn from the search engine market\nmight indicate that only a chatbot market winner can access the amount of\ndata necessary for training a model and for producing output of sufficiently\nhigh quality—two outcomes that are key in securing more users and more\ndata for a chatbot company.72 In this scenario, potential market entrants\n67 “Direct network effects” exist when “the utility that a user derives from consumption of the good\nincreases with the number of other agents consuming the good.” Michael L. Katz & Carl Shapiro, Network\nExternalities, Competition, and Compatibility, 75 AM. ECON. REV. 424, 424 (1985).\n68 Larry Page, 2012 Update from the CEO, ALPHABET INV. RELS., https://abc.xyz/investor/founders-\nletters/2012/ [https://perma.cc/2UFZ-J2KD]; see also Adam Kovacevich, Google’s Approach to\nCompetition, GOOGLE PUB. POL’Y BLOG (May 8, 2009),\nhttps://publicpolicy.googleblog.com/2009/05/googles-approach-to-competition.html\n[https://perma.cc/G65Z-ZGMX] (“Competition is just one click away.”).\n69 “Multihoming” means “the ability for an individual to use multiple platforms to access similar\nservices.” Kenneth A. Bamberger & Orly Lobel, Platform Market Power, 32 BERKELEY TECH. L.J. 1051,\n1067 (2017); see also Aaron S. Edlin & Robert G. Harris, The Role of Switching Costs in Antitrust\nAnalysis: A Comparison of Microsoft and Google, 15 YALE J.L. & TECH. 169, 204 (2013) (“[T]he ability\nof consumers to use a combination of general and vertical search engines to find information is not\nhindered by switching or ‘multi-homing’ costs.”). For empirical evidence showing that multihoming is\ncommon and effortless for users, see, for example, Ryen W. White & Susan T. Dumais, Characterizing\nand Predicting Search Engine Switching Behavior, 2009 PROC. 18TH ACM CONF. ON INFO. &\nKNOWLEDGE MGMT. 87, 89 (“Of the 14.2 million users in our log sample, . . . 7.1 million (50.0%)\nswitched engines within a search session at least once, and 9.6 million (67.6%) used different engines for\ndifferent sessions (i.e., engaged in between-session switching).”).\n70 Cf. Edlin & Harris, supra note 69, at 176 (arguing that “[b]ecause of low switching costs, Google\nsearch is vulnerable to existing competitors and new entrants to the market in a way that Microsoft’s\noperating system never was”).\n71 The “data feedback loop” means that more users bring more data, which, in turn, means “better\nquality of the service in a general way . . . as well as in a personalized way,” which attracts even more\nusers with more data. MARC BOURREAU ET AL., BIG DATA AND COMPETITION POLICY: MARKET POWER,\nPERSONALISED PRICING AND ADVERTISING 35–37 (2017). Such feedback loop, also called “learning by\ndoing” network effects, is very common in the search engine market. See STUCKE & GRUNES, supra note\n66, at 174–75.\n72 See STUCKE & GRUNES, supra note 66, at 174–75; see also United States v. Google LLC, No. 20-\nCV-3010, 2024 WL 3647498, at *111 (D.D.C. Aug. 5, 2024).\n125\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nwould likely throw in the towel before even stepping into the ring because,\nwithout a sufficient dataset, they would consider competition against\nincumbents to be a losing proposition.\nThe above scenario might be the case in the search engine market, but\nnot necessarily the chatbot market. Why? First, unlike search engines, which\ncollect personalized information mainly from users, LLMs access\nvoluminous publicly available information for the purpose of training their\nmodels—a setup that can work effectively even in the absence of user\nqueries.73 In this sense, for chatbot companies, acquiring a larger user base\ncan yield only a marginal advantage over other competitors. Second, the\nquality of a search engine’s search results is far less evident or even\nimportant than the quality of an AI chatbot’s output. Even prudential users\nmay be unable to notice quality degradation in a search result ranking, and\nthus, they would keep using the search engine, but they would easily\ndistinguish between two chatbots that differ from each other regarding the\nquality of their respective output.\nIt is not necessarily the case that the more market players there are, the\nmore effective the market competition is. Sometimes, two or three\ncompetitors in a specific market might be sufficient to check the incumbent.\nFor example, when assessing the necessity of network neutrality rules, the\nFCC found that only two competing wireline internet service providers\n(ISPs) are needed for these ISPs to experience sufficient competitive\npressure.74 This is due primarily to the high substantial sunk costs associated\nwith an ISP’s infrastructure investments—a situation that results in a\nrelatively low marginal increase in the cost of adding one more customer.75\nThus, an ISP, when facing competition from another ISP, has great\nincentives to attract more users by cutting prices as much as possible.76\nSimilarly, the significant sunk costs for chatbot companies’ investment in\nLLM development will generally lead companies to improve their output\nquality and lower their subscription fees, in turn boosting the use of their\nchatbot products.\nAs of the writing of this Article (early 2024), over ten generative AI\nproducts are competing intensely with ChatGPT.77 It is also delightful that\n73 See CHRISTOPHE CARUGATI, COMPETITION IN GENERATIVE ARTIFICIAL INTELLIGENCE\nFOUNDATION MODELS 9–10 (Bruegel, 2023) (arguing that “the availability of open-source data . . .\nlower[s] entry barriers significantly at the data level”).\n74 See 2018 Internet Order, supra note 44, ¶¶ 124–26.\n75 Id.\n76 Id.\n77 See, e.g., Tim Keary, The Best ChatGPT Alternatives in 2024: 12 AI Rivals, TECHOPEDIA (last\nupdated Nov. 12, 2024), https://www.techopedia.com/who-are-the-competitors-of-chatgpt\n126\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nmembers from Large Model Systems Organization (LMSYS) and U.C.\nBerkeley SkyLab initiated a website called Chatbot Arena that lucidly\nvisualizes the internal market competition.78 On the Chatbot Arena website,\nusers can evaluate the performance of different chatbot outputs and can\naccess almost all AI chatbot products in the marketplace and their respective\nrankings.79 Over one hundred models compete on the Chatbot Arena in\nvarious categories, like Hard Prompts, Longer Query, Math, and Coding.80\nThe votes from users are collected to generate a real-time leaderboard. It is\nshown that Claude 3 Opus surpassed OpenAI’s GPT-4 on March 26, 2024,81\nand as of the mid November 2024, Gemini-Exp-1114 has taken the lead.82 In\naddition, the whole value chain of generative AI is found to be competitive\nenough.83 Simultaneously, market entrants worldwide are initiating their\nown chatbot services and are expected to pose non-negligible challenges for\nthe incumbents, and to grab some of the market share.84\nCurrently, no evidence shows that generative-AI service providers have\nincentives either to degrade the quality of their products or to ignore the risks\nassociated with generative-AI content. In fact, existing chatbot companies\nhave voluntarily adopted various strategies to mitigate content risks, despite\nthe absence of regulatory pressure. For example, at the bottom of the chat\npage for ChatGPT, a brief statement informs users that the chatbot may\n[https://perma.cc/8F39-M9RB]; Angela Yang & Jasmine Cui, ChatGPT Still Reigns Supreme in Many AI\nRankings, But the Competition is On, NBC NEWS (Feb. 20, 2024), https://www-nbcnews-\ncom.cdn.ampproject.org/c/s/www.nbcnews.com/news/amp/rcna136990 [https://perma.cc/C6KY-PV55].\n78 The mission of the Chatbot Arena is “to build an open platform to evaluate LLMs by human\npreference in the real-world.” About Us, CHATBOT ARENA, https://lmarena.ai/ [https://perma.cc/C8K4-\n5DQ4] [hereinafter CHATBOT ARENA].\n79 Id.\n80 Id.\n81 See Benj Edwards, “The King is Dead”—Claude 3 Surpasses GPT-4 on Chatbot Arena for the\nFirst Time, ARS TECHNICA (Mar. 27, 2024), https://arstechnica.com/information-\ntechnology/2024/03/the-king-is-dead-claude-3-surpasses-gpt-4-on-chatbot-arena-for-the-first-time/\n[https://perma.cc/94CW-85MU].\n82 For dynamic chatbots’ competition, see CHATBOT ARENA, supra note 78, “Leaderboard.”\n83 See Christophe Carugati, The GenAl Value Chain is Competitive, DIGIT. COMPETITION (Feb. 5,\n2024), https://www.digital-competition.com/infographics/the-genal-value-chain-is-competitive\n[https://perma.cc/S9DG-FSZG].\n84 See, e.g., Leila Abboud et al., ‘We Compete with Everybody’: French AI Start-up Mistral Takes\non Silicon Valley, FIN. TIMES (Nov. 7, 2023), https://www.ft.com/content/387eeeab-1f95-4e3b-9217-\n6f69aeeb5399 [https://perma.cc/4X5U-9KGT]; Robert Hart, ChatGPT’s Biggest Competition: Here Are\nThe Companies Working on Rival AI Chatbots, FORBES (Feb. 27, 2023),\nhttps://www.forbes.com/sites/roberthart/2023/02/23/chatgpts-biggest-competition-here-are-the-\ncompanies-working-on-rival-ai-chatbots [https://perma.cc/W287-K47U].\n127\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\ngenerate mistakes.85 When a user gives ChatGPT negative feedback about\nsome generated content, ChatGPT will request further feedback to better\nensure that future output is satisfactory.86 ChatGPT also blocks itself from\naccessing and training off of users’ chat histories.87 OpenAI released a\ntechnical report that comprehensively discloses both the risks that people run\nwhen using ChatGPT and the measures that OpenAI has implemented in\nChatGPT to counter potential harms.88 These proactive actions clearly\ndemonstrate that derisking is a crucial parameter of chatbot competition and\nthat chatbots are facing significant market pressure. Derisking a chatbot\nsystem and improving the quality of chatbot output are the most effective\nstrategies a chatbot company can use to attract users and grow its market\nshare. Internal market competition, by itself, can substantially mitigate\ncontent risks, such as harmful content, discrimination and bias, and\nmisinformation.\n2. External Market Competition\nThe external market is much broader than the internal one, as the former\nincludes all outlets of information accessible to the general public. Among\nthese outlets are search engines, digital platforms, websites, television\nprograms, and printed publications. People receive information from various\nsources concurrently, and in some cases, one source can verify or refute\nanother. Thus, the output generated by ChatGPT, for instance, competes not\nonly with the output generated by competing chatbots, but also with the\ninformation produced and disseminated by conventional websites, digital\napplications, newspapers, and the like. Users cynical about the accuracy of\nChatGPT output might turn to alternatives by, for example, researching them\non search engines and independently assessing the acquired content. The\neasy accessibility of the external market should reassure us that users of\nchatbots have sufficient competing sources of information with which the\n85 See CHATGPT, https://chat.openai.com/ [https://perma.cc/S78B-E7EK] (last visited Oct. 26,\n2024) (log in to view disclaimer) (“ChatGPT can make mistakes. Check important info.”); see also\nCLAUDE, https://claude.ai/ [https://perma.cc/5TSA-UMWF] (last visited Oct. 26, 2024) (log in and begin\na chat to view disclaimer) (“Claude may occasionally generate incorrect or misleading information, or\nproduce offensive or biased content.”); GEMINI, https://gemini.google.com/app [https://perma.cc/XQ8S-\nGVAA] (last visited Oct. 26, 2024) (log in to view disclaimer) (“Gemini may give inaccurate or offensive\nresponses. When in doubt, use the Google button to double-check Gemini’s responses.”).\n86 See CHATGPT, supra note 85.\n87 See New Ways to Manage Your Data in ChatGPT, OPENAI (Apr. 25, 2023),\nhttps://openai.com/blog/new-ways-to-manage-your-data-in-chatgpt [https://perma.cc/R9RC-8SFJ].\n88 See OpenAI’s Report, supra note 2, at 41–60.\n128\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nusers can rigorously assess the output quality of generative AI. Thus, some\ncontent risks, especially misinformation risks, are overblown.89\nSome content risks will never become real harms, as long as\ninformation consumers are critical.90 Then, the question would be whether\ninformation consumers are critical enough so that competition between\nvarious information outlets is effective. I am inclined to say the answer to\nthe question is yes.91 Empirical evidence has indicated that although\nmisinformation outlets have experienced a dramatic increase in traffic during\nthe Covid-19 pandemic, most people pay little attention to these\nuntrustworthy outlets; instead, they acquire information largely from\nconventionally trustworthy sources.92 This finding demonstrates that most\npeople making critical decisions, especially those regarding personal health\nand safety, act prudently by consulting various sources and by basing\ndecisions on the most reliable of those sources.93 Similarly, in the context of\nAI-generated information, empirical evidence has shown that information\n89 See Simon et al., supra note 18, at 5 (arguing that the concerns over the spread of misinformation\nby generative AI are overblown).\n90 Here, I avoid using the wording “rational” because in the economic sense, “rational[ity]” entails\nthe ability of “a cognitively intensive, calculating process of maximization in the self-interest.” Vernon\nL. Smith, Rational Choice: The Contrast Between Economics and Psychology, 99 J. POL. ECON. 877, 878\n(1991). However, this proves impossible in most cases when people make decisions in their daily lives.\nSee id. at 880. Similarly, in the information market, the rationality requirement entails the capability of\ntelling the normative good information and bad information, truth and falsity. However, this is impossible\nin most cases because, first, some information, especially opinions and ideas, unlike facts, cannot be\nproved false and thus inferior to other information. See, e.g., Gertz v. Robert Welch, Inc., 418 U.S. 323,\n340 (1974). And second, consumers also have inherent biases and cannot necessarily make rational\nchoices. See, e.g., Frederick Schauer, Facts and the First Amendment, 57 UCLA L. REV. 897, 909–10\nn.64 (2010); Lyrissa Barnett Lidsky, Nobody’s Fools: The Rational Audience as First Amendment Ideal,\n2010 U. ILL. L. REV. 799, pt. III (2010). However, the fact that information consumers are not perfectly\nrational does not mean that they are not critical and diligent when processing information and that the\nmarket does not work effectively to check the incumbents. The effectiveness of market competition never\nrelies on the perfect rationality of all members of society. In other words, it is sufficient to check the\nchatbot companies if some (not all) people are critical (not rational) when processing the output of the AI\nchatbots.\n91 See, e.g., Sacha Altay & Alberto Acerbi, People Believe Misinformation is a Threat Because They\nAssume Others Are Gullible, NEW MEDIA & SOC’Y 1 (2023) (arguing that most people are not as gullible\nas the alarmists believe, and so the concerns over the prevalence and impact of misinformation are greatly\nexaggerated); Gordon Pennycook & David G. Rand, Fighting Misinformation on Social Media Using\nCrowdsourced Judgments of News Source Quality, 116 PROC. NAT’L ACAD. SCIS. 2521, 2521 (2019)\n(finding that “laypeople—on average—are quite good at distinguishing between lower- and higher-\nquality sources”).\n92 See Sacha Altay et al., Quantifying the “Infodemic”: People Turned to Trustworthy News Outlets\nDuring the 2020 Coronavirus Pandemic, 2 J. QUANTITATIVE DESCRIPTION: DIGIT. MEDIA 1 (2022).\n93 See María Celeste Wagner & Pablo J. Boczkowski, The Reception of Fake News: The\nInterpretations and Practices That Shape the Consumption of Perceived Misinformation, 7 DIGIT.\nJOURNALISM 870, 876–80 (2019) (introducing various strategies that online users adopt to check the\ninformation quality).\n129\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nconsumers are very critical of AI-generated content.94 We can rest assured\nthat well-informed users are unlikely to unquestioningly accept chatbot\noutput. Rather, the users will rely on chatbots as one source of information\nwhile simultaneously consulting other information sources—a cross-\nchecking process that, in a competitive marketplace, should lead the users to\nthe most trustworthy sources and the most accurate information.95 In other\nwords, discretion should be left to information consumers, enabling them to\nautonomously select information outlets according to their own preferences.\nThis is also aligned with the spirit of the First Amendment, as the First\nAmendment not only protects the rights of speakers, but also the rights of\nlisteners.96 Admittedly, some consumers are not critical, in which case\npolicymakers should bridge the wisdom gap by adopting transparency rules,\nAI literacy programs, and user empowerment schemes.\nOne potential objection here is that non-AI information outlets are not,\nor will soon not be, effective competitors of chatbot output. This postulation\nrests on the assumption that information-seeking users will grow over-reliant\non chatbots if their output tends to be speedy and of high quality. A corollary\nof this argument is that this overreliance can be additionally harmful insofar\nas it promotes neglectful indolence in users, who may stop verifying the\nquality of AI-generated information even though the market may be\ncompetitive. The result would be a firm but unfounded belief in, or\nsatisfaction with, patently erroneous or low-quality information.97 This\n94 See, e.g., Chiara Longoni et al., News from Generative Artificial Intelligence Is Believed Less,\nFACCT ’22: PROC. OF THE 2022 ACM CONF. ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY 97,\n97 (2022), https://dl.acm.org/doi/10.1145/3531146.3533077 [https://perma.cc/6LKW-84U4] (finding\nthat “[p]eople were more likely to incorrectly rate news headlines written by AI (vs. a human) as\ninaccurate when they were actually true, and more likely to correctly rate them as inaccurate when they\nwere indeed false”).\n95 See, e.g., Derek E. Bambauer & Mihai Surdeanu, Authorbots, 3 J. FREE SPEECH L. 375, 387 (2023)\n(arguing that “[f]ew take ChatGPT responses seriously at present, given its reputation as a hallucination\nengine, and that number falls further among observers who understand something about machine\nlearning”); Simon et al., supra note 18, at 3–5 (arguing that the increase in quantity and quality of\nmisinformation can hardly mislead the overall public).\n96 See Eugene Volokh et al., Freedom of Speech and AI Output, 3 J. FREE SPEECH L. 651, 651 (2023)\n(“AI programs’ speech should be protected because of the rights of their users—both the users’ rights to\nlisten and their rights to speak.”); Cass R. Sunstein, Cass R. Sunstein: “Does Artificial Intelligence Have\nthe Right to Freedom of Speech?”, NETWORK L. REV. (Feb. 28, 2024),\nhttps://www.networklawreview.org/sunstein-artificial-intelligence/ [https://perma.cc/7ZXZ-Y9DS]\n(“But even if AI lacks free speech rights, the human beings who interact with generative AI, or with AI\nmore broadly, have free speech rights, insofar as they are acting as speakers, and also insofar as they are\nacting as listeners, readers, or viewers.”).\n97 See Risks of LLMs, supra note 2, at 23 (“[A] LM that gives factually correct predictions 99% of\nthe time, may pose a greater hazard than one that gives correct predictions 50% of the time, as it is more\nlikely that people would develop heavy reliance on the former LM leading to more serious consequences\nwhen its predictions are mistaken.”).\n130\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nargument overlooks the nature of market competition, where consumers\nbecome loyal to a specific brand owing to its high quality. From a market\nperspective, this outcome is not problematic. It is even unproblematic if the\nquality of the revered brand decreases, because as the welfare of the brand’s\ndevoted customer base declines, the critical consumers in that base can and\nwill switch to superior brands. Unlike governmental regulation,\nmarketplaces chiefly care about competitive relationships between market\nplayers (e.g., information outlets) and very little, if at all, about the quality\nof a specific commodity.98\nA major concern can be that chatbot output may have silencing effects\non non-AI sources of information and may thus harm the information market\nin the long run. For instance, a jurisdiction that allows ChatGPT to access,\ncopy, and present the full version of online articles by the New York Times\ncan deprive the newspaper of huge revenues because ChatGPT users, rather\nthan pay the company a subscription fee, can access the articles at no\nadditional cost from the chatbot. Such free-riding is anticompetitive and will\ninevitably disincentivize the copyright holders, in turn harming the overall\ninformation market and, ultimately, the money-saving information\nconsumers themselves. Similarly, if ChatGPT is allowed to cater to some\npeople’s need to pry into other people’s privacy by disclosing others’ data,\npeople would be inclined to keep information private and entirely\ninaccessible to the public. This would lead to a significant loss in the quantity\nand quality of data in the long run. As these harms to the information market\nare mostly indirect and can only be perceived over the long term, consumers,\nsome of whom are short-sighted,99 are inclined to overlook the harms. These\nscenarios are market failures, and I acknowledge them and support\nregulatory strategies that proportionately mitigate the aforementioned\nharms.100\nIV. RE-EVALUATIONS & SUGGESTIONS\nHaving reviewed how powerful the information market is at mitigating\ncontent risks, this Part examines some regulatory designs and generates\npolicy suggestions. Drawing upon the inter-informational competitive\nrelationship illustrated above, Section A re-evaluates the most popular\n98 See Stacey L. Dogan & Mark A. Lemley, Antitrust Law and Regulatory Gaming, 87 TEX. L. REV.\n685, 697 (2009) (explaining the differences between antitrust law and regulatory approaches).\n99 Economists have long recognized that consumers exhibit present bias, pursuing immediate\nsatisfaction but overlooking long-term costs. See, e.g., Ted O’Donoghue & Matthew Rabin, Addiction\nand Present-Biased Preferences (UC Berkeley, Paper No. E02-312, 2002). Such present-biased behaviors\nhave been found in various fields and influence public policies. See, e.g., David Bradford et al., Time\nPreferences and Consumer Behavior, 55 J. RISK & UNCERTAINTY 119 (2017).\n100 See discussion infra Section IV.B.2.\n131\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nregulatory proposals across the world. Then, Section B proposes three\nsuggestions regarding: (a) concentration tendencies and potential\nanticompetitive practices in the internal market; (b) privacy disclosure; and\n(c) copyright-infringement risks in the external market.\nA. Re-evaluations of Existing Regulatory Proposals\nPolicymakers and researchers have designed various regulatory\nproposals to mitigate the recognized risks of generative AI. While some\nchallenge the effectiveness of these proposals,101 a more fundamental\nquestion is: even if effective, are such regulations necessary? The key\nconsideration here is whether the proposed regulation can achieve outcomes\nthat market competition cannot. One imperative, yet poorly understood point\nshould be made here: not all regulatory approaches capable of controlling\ntarget risks are necessary. Although some regulatory approaches promise\nrapid and ostensibly convenient solutions, heavy-handed interventions can\nhave long-term, substantial negative fallout that weakens an entire\nindustry.102 In comparison, sufficient market competition may be able to\naccomplish the same end or an even better end. By this yardstick, some\nregulatory proposals are unnecessary. Proposals that survive the necessity\ntest should also be proportionate: designed to invigorate information market\ncompetition and address anticompetitive issues that a free market cannot\nresolve on its own.\n1. Mandatory Content-Based Prohibition\nIn the context of generative AI, mandatory prohibition entails that the\ngovernment both ban problematic chatbot output and prescribe penalties for\nviolators of the ban. For example, China has demonstrated a fervent\ncommitment to assertive regulatory measures targeting information content.\nArticle 4 of China’s Generative AI Measures adopts a zero-risk approach by\nproviding an exhaustive list of prohibited content.103 In instances of non-\n101 See, e.g., Neel Guha et al., The AI Regulatory Alignment Problem, HAI POL’Y BRIEFS (Nov.\n2023), https://hai.stanford.edu/sites/default/files/2023-11/AI-Regulatory-Alignment.pdf\n[https://perma.cc/DMK5-L5D2] (arguing that “[s]ome proposals may fail to address the problems they\nset out to solve due to technical or institutional constraints”).\n102 See supra notes 39–40 and accompanying text (the vicious cycle of the overreliance on direct\nregulation); see also Neel Guha et al., AI Regulation Has Its Own Alignment Problem: The Technical and\nInstitutional Feasibility of Disclosure, Registration, Licensing, and Auditing, 92 GEO. WASH. L.\nREV. (forthcoming 2024) (manuscript at 6) (on file with SSRN),\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4634443 [https://perma.cc/DN4E-8T96] (arguing\nthat “some regulation proposals could—even if potentially useful in advancing legitimate public\nobjectives—function to advantage powerful incumbents in AI and reduce competition, thus stymieing\ninnovation and concentrating AI’s benefits”).\n103 See Chinese Generative AI Measures, supra note 9, art. 4(1)–(4).\n132\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\ncompliance, relevant authorities may, as stipulated in Article 21, issue a\nwarning or circular of reprimand, mandate corrections within a specified\ntimeframe, and even institute an order to suspend service provisions.104\nPreviously, the European Parliament version of the European Union AI Act\nexhibited traits similar to those of the Chinese measures.105\nInternal and external market competition already creates sufficient\nincentives for generative AI service providers to derisk their systems as\nmuch as possible in a bid to improve service quality and meet public\nexpectations.106 Although market competition can by no means ensure\ncompletely lawful content generation, neither can any regulations. Experts\non generative AI have already generally acknowledged that the whole\nprocess of AI value chains entails inherent content risks. 107 Neither\nprohibitive regulation of unlawful output nor punitive treatment of violators\ncan guarantee perfect observance of the regulation.108\nSetting regulatory guardrails to act as bottom lines is sometimes\nnecessary to protect the general public from being harmed and manipulated.\nThis is especially important when corporate commercial interests do not\nalign with public interests, which may lead to market failures and a race to\nthe bottom.109 However, in the context of chatbot content governance, AI\nchatbot companies under sufficient market forces must align their operations\nwith public expectations. This means that imposing stringent content-based\nrules is unnecessary because AI chatbot companies have inherent incentives\nto compete for higher market share by addressing harmful content,\ndiscrimination, and misinformation risks continuously, automatically, and\nvoluntarily. That said, policymakers might still consider prohibiting certain\n104 Id. at art. 21.\n105 See Amendments Adopted by the European Parliament on 14 June 2023 on the Proposal for a\nRegulation of the European Parliament and of the Council on Laying Down Harmonised Rules on\nArtificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts, EUR.\nPARL. DOC. (COM(2021)0206 – C9-0146/2021 – 2021/0106(COD)) §28b(4)(b) (2023),\nhttps://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.pdf [https://perma.cc/B8UT-\nDAPP].\n106 See discussion supra Section III.B.\n107 AI value chain is “the organisational process through which an individual AI system is developed\nand then put into use (or deployed),” involving various parties, including developers, deployers, and users.\nSee, e.g., Alex C. Engler & Andrea Renda, Reconciling the AI Value Chain with the EU’s Artificial\nIntelligence Act 2 (CEPS 5, Sept. 2022), https://www.ceps.eu/ceps-publications/reconciling-the-ai-value-\nchain-with-the-eus-artificial-intelligence-act/ [https://perma.cc/B3SJ-R67M]; Tonja Jacobi & Matthew\nSag, We Are the AI Problem, 74 EMORY L.J. ONLINE 1 (2024).\n108 For a detailed explanation of why the mandatory prohibitions based on the zero-risk goals are\nfundamentally flawed, see Jiawei Zhang, ChatGPT as the Marketplace of Ideas: Should Truth-Seeking\nBe the Goal of AI Content Governance?, 35 STAN. L. & POL’Y REV. ONLINE 11 (2024).\n109 See, e.g., MAURICE E. STUCKE & ARIEL EZRACHI, COMPETITION OVERDOSE: HOW FREE MARKET\nMYTHOLOGY TRANSFORMED US FROM CITIZEN KINGS TO MARKET SERVANTS, ch. 1 (2020).\n133\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nharmful content, such as hate speech and deepfakes, that pose significant\nrisks to the broader information ecosystem.110\nRegulators must remain cognizant of the virtues of market mechanisms\nand the shortcomings of content-based rules. Contrary to the assumptions of\nsome policymakers, onerous direct regulations may have many side effects.\nFirst, unnecessary regulations raise market entry barriers and thus scare off\nwould-be market entrants. This outcome should not be surprising, as startups\ncan hardly bear the high costs that accompany both regulatory compliance\nand regulatory punishment.111 Even worse, such restrictions can also have\nchilling effects on existing generative AI systems. To comply with the rules,\nestablished chatbot providers will over-moderate their models to make them\nneedlessly prudential. Mandating a zero-risk approach or imposing a rigid\ntruth-telling obligation could transform a dynamic tool like ChatGPT into a\nstifled and overly cautious “SorryGPT”—users will receive tons of apologies\nwithout any meaningful content. This reactive approach ultimately burdens\nconsumers, diminishing their access to desired information and eroding their\noverall surplus.\n2. Licensure\nAnother noteworthy regulatory proposal, this one representing a variant\nof mandatory prohibitions, is licensure.112 It requires that generative AI\nservice providers, before they even enter the market, acquire a government-\nissued license by demonstrating a sufficient level of fairness and accuracy.113\nA provider’s failure to demonstrate these capabilities would lead the\ngovernment to adopt a presumption of unlawfulness with regard to the\nprovider’s potential, as-yet-unrealized provision of services.114 In this sense,\nlicensure, as an ex-ante scheme constraining generative AI services, and\nmandatory prohibition proposals are essentially two sides of the same coin.\n110 See, e.g., Jiawei Zhang, Informational Public Interest, 26 N.C. J.L. & TECH. 73, 116–17 (2024).\n111 See, e.g., Thibault Schrepel, Decoding the AI Act: A Critical Guide for Competition Experts 11–\n18 (ALTI Working Paper Series 2024) (arguing that the EU AI Act may distort market competition\nbecause it is not technology neutral in nature, distributes the regulatory burden unevenly, and significantly\nraises the threshold of market entry).\n112 See Gianclaudio Malgieri & Frank Pasquale, Licensing High-Risk Artificial Intelligence: Toward\nEx Ante Justification for a Disruptive Technology, 52 COMPUT. L. & SEC. REV. 105899 (2024).\nInterestingly, Sam Altman urges the establishment of the licensure system and punishment of wrongful\nconduct. See Sindhu Sundar, Sam Altman Says a Government Agency Should License AI Companies —\nand Punish Them If They Do Wrong, BUS. INSIDER (May 16, 2023),\nhttps://www.businessinsider.com/sam-altman-openai-chatgpt-government-agency-should-license-ai-\nwork-2023-5 [https://perma.cc/74RQ-QVD5].\n113 See Malgieri & Pasquale, supra note 112, at 1.\n114 Id. (“Under a licensing system, products, services, and activities are unlawful until the entity\nseeking to develop, sell, or use them has proven otherwise.”).\n134\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nThe reasons why mandatory prohibition is likely unnecessary are the\nsame reasons why licensure is likely unnecessary. However, a licensure\nregime can be even more problematic than mandatory prohibition because\nthe licensure standard for pre-market approval is even more rigid. More than\nmandatory prohibition, it will erect an insurmountable market-entry barrier\nfor many would-be market entrants and will further entrench incumbents, a\nscenario that will ultimately undermine internal-market competition and\nharshen the duties on those incumbents.115\n3. Curation of Datasets\nDataset curation rules require generative AI service providers to\neliminate some content risks by purging potentially problematic materials\nfrom upstream datasets. As a root-level regulatory tool, dataset curation rules\nare supposed to address some content risks, such as discriminatory content,116\nprivacy disclosure,117 and copyright infringement.118\nHowever, imposing data curation rules on chatbot companies in a bid\nto counter content risks is yet another unnecessary and even detrimental\nregulatory proposal. Take, for example, the application of data curation to\ncopyright issues. The New York Times has recently sued OpenAI and\nMicrosoft, calling for chatbot companies to destroy their datasets that contain\nmaterials copyrighted by the newspaper.119 Undoubtedly, copyright holders\nhoping to maximize economic benefits have the right to file a lawsuit against\ngenerative AI service providers. However, AI experts have recognized that\nit is technically impossible for AI companies to filter out all copyrighted\nmaterial from their datasets.120 And, it is still highly uncertain whether LLMs\ncan maintain adequate performance if they are allowed to access only public,\n115 See Guha et al., supra note 102, at 52–53.\n116 See Philipp Hacker et al., Regulating ChatGPT and Other Large Generative AI Models, ACM\nCONF. ON FAIRNESS, ACCOUNTABILITY, & TRANSPARENCY 1112, 1119–20 (2023).\n117 See Amy Winograd, Loose-Lipped Large Language Models Spill Your Secrets: The Privacy\nImplications of Large Language Models, 36 HARV. J.L. & TECH. 615, 649–51 (2023) (The “publicly-\nintended data” indicates “data that is most likely to be intended for broad public consumption and use in\na wide variety of contexts.”).\n118 See White Paper: How the Pervasive Copying of Expressive Works to Train and Fuel Generative\nArtificial Intelligence Systems Is Copyright Infringement and Not a Fair Use, NEWS/MEDIA ALLIANCE\n(Oct. 31, 2023), https://www.newsmediaalliance.org/generative-ai-white-paper/ [https://perma.cc/H5ZP-\nMVTH].\n119 See Michael M. Grynbaum & Ryan Mac, The Times Sues OpenAI and Microsoft Over A.I. Use\nof Copyrighted Work, N.Y. TIMES (Dec. 27, 2023), https://www.nytimes.com/2023/12/27\n/business/media/new-york-times-open-ai-microsoft-lawsuit.html [https://perma.cc/6HZP-LD5L].\n120 See Peter Henderson et al., Foundation Models and Fair Use (Mar. 29, 2023) (manuscript at 22)\n(unpublished manuscript) (on file with arXiv), https://arxiv.org/abs/2303.15715 [https://perma.cc/MS3V-\n2XPQ].\n135\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nnon-copyrightable, and permissively licensed data.121 OpenAI, in response to\nthe rising tensions between it and copyright holders, has warned that a\ncopyright crackdown will make its services “impossible.”122 OpenAI further\nexplained that AI systems trained merely on public domain data “would\nnot . . . meet the needs of today’s citizens.”123\nI fully agree with OpenAI on these points and argue that the Times’\nclaims should be roundly rejected by judges and policymakers alike. Chatbot\ncompanies have inherent incentives to curate their datasets to improve their\nprovided services and better cater to public needs.124 Compulsory data\nfiltering would be catastrophic for the whole industry, not only demotivating\npotential entrants but also stagnating incumbents’ innovation. I concur that\nboth free riding off others’ intellectual creation and privacy disclosure are\nreal concerns for a healthy, competitive, inter-informational environment.125\nHowever, rather than rely on data curation rules, policymakers should design\nalternative regulatory strategies that do not weaken but strengthen the\nbeneficial powers of competitive markets.126\n4. Transparency\nTransparency rules have gained widespread acceptance as a regulatory\napproach to AI risks and are attractive given the essentially opaque nature of\nAI technology.127 In the context of generative AI, transparency rules require\nthat chatbot companies practice self-disclosure by informing users and\npublic officials of relevant information about the scope of training data\nsources, the performance of the LLMs, and measures that companies take to\nhandle existing and potential risks.128 Embracing a pro-innovation approach,\n121 Id.\n122 See James Titcomb & James Warrington, OpenAI Warns Copyright Crackdown Could Doom\nChatGPT, THE TELEGRAPH (Jan. 7, 2024), https://www.telegraph.co.uk/business/2024/01/07/openai-\nwarns-copyright-crackdown-could-doom-chatgpt/ [https://perma.cc/LP3L-BGMU].\n123 Id.\n124 In fact, chatbot companies, such as OpenAI, have voluntarily taken actions to moderate the\nproblematic data in their datasets to reduce hallucination and privacy disclosure. See OpenAI’s Report,\nsupra note 2, at 46, 53.\n125 See discussion supra Section III.B.2.\n126 See discussion infra Section IV.B.2.\n127 See generally Warren J. von Eschenbach, Transparency and the Black Box Problem: Why We Do\nNot Trust AI, 34 PHILOS. TECH. 1607 (2021); see also Bruno Lepri et al., Fair, Transparent, and\nAccountable Algorithmic Decision-Making Processes, 31 PHILOS. TECHNOL. 611, 613 (2018).\n128 For a general proposal to govern AI risks, see, for example, Sonia K. Katyal, Private\nAccountability in the Age of Artificial Intelligence, 66 UCLA L. REV. 54, 111–17 (2019); Hacker et al.,\nsupra note 116, at 1119; for a proposal to adopt transparency rules to control specific generative AI risks,\nsuch as privacy, see, for example, Winograd, supra note 117, at 652–54 (arguing that “developers should\nbe required to disclose the sources of training data, the measures taken to ensure data collection and\nprocessing conforms to applicable law, and the privacy-preserving and safety measures employed in\ntraining and implementation to ensure responsible development.”).\n136\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nthe UK government in March 2023 published its AI regulation guidelines in\na white paper, endorsing a more light-touch and technology-neutral\nregulatory approach.129 Specifically, the UK government is trying to harness\ntransparency rules that enhance users’ understanding of and trust in\ngenerative AI products.130\nTransparency is one of the rules that passes the necessity test. As the\n“best of disinfectants,”131 transparency has proved effective at mitigating\nrisks and invigorating competition in an array of market types. In traditional\nphysical markets, for example, transparency rules require food producers to\ninform consumers of a food product’s ingredients, nutrients, health hazards,\nand other information that helps consumers make informed comparisons and\ndecisions in the marketplace.132\nIn digital markets, the U.S. government has long embraced the\ntransparency rule as an ideal candidate for a light-touch regulatory\nframework. One noteworthy example is the 2018 Internet Order, in which\nthe FCC, to regulate the broadband internet, shifted away from a network\nneutrality framework and toward a market-based one, thereby abandoning\nthe no-blocking, no-throttling, and no-paid-prioritization rules while\nretaining the transparency rule.133 The FCC believed that transparency rules\nhelp the market achieve self-regulation.134 When ISPs appropriately disclose\nrelevant information, regulators can dynamically grasp and monitor market\nconditions, other market entities can gauge highly topical matters such as\nfairness, users can make better choices confidently, and the ISPs themselves\ncan promptly self-correct in instances of misconduct.135\nThe chatbot market is no exception to the above trends for the\ntransparency rule. Researchers at Stanford’s Institute for Human-Centered\nArtificial Intelligence have comprehensively evaluated the proposed\ndisclosure requirements and yielded four central findings: (1) disclosure\n129 See UK DEPARTMENT FOR SCIENCE, INNOVATION AND TECHNOLOGY, AI REGULATION: A PRO-\nINNOVATION APPROACH, 2023, Cp. 815 (UK), https://www.gov.uk/government/publications/ai-\nregulation-a-pro-innovation-approach/white-paper [perma.cc/U2PE-8LL2] [hereinafter WHITE PAPER or\nUK AI REGULATION WHITE PAPER] (In the context of LLMs, the White Paper believes that “it would be\npremature to take specific regulatory action in response to foundation models including LLMs . . . [which]\nwould risk stifling innovation, preventing AI adoption, and distorting the UK’s thriving AI ecosystem.”).\n130 Id. at 58.\n131 LOUIS D. BRANDEIS, OTHER PEOPLE’S MONEY AND HOW THE BANKERS USE IT 92 (1914)\n(“Sunlight is said to be the best of disinfectants.”).\n132 In fact, the AI transparency rules have been compared to the “nutrition facts labels” of food\nproducts. See Sara Gerke, “Nutrition Facts Labels” for Artificial Intelligence/Machine Learning-Based\nMedical Devices—The Urgent Need for Labeling Standards, 91 GEO. WASH. L. REV. 79 (2023).\n133 See 2018 Internet Order, supra note 44, at 124–25.\n134 See id. at 125.\n135 Guha et al., supra note 102, at 23.\n137\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nrules are relatively easier for governments to implement;136 (2) good\ndisclosure rules are understandable and actionable for consumers and are\nverifiable for experts including regulatory agencies; 137 (3) effective\ndisclosure rules can be administratively costly;138 and (4) disclosure rules\nshould therefore be as proportionate as possible.139 Here, some additional\nremarks are attached to these four findings.\nFirst, I agree that disclosure rules are easier for governments to\nimplement than other rules. This relative ease is rooted primarily in the fact\nthat disclosure rules, unlike most other rules, shift much of the information-\nprocessing burden from government enforcement agencies to regulated\nentities.140 Second, I also agree that the effectiveness of disclosure rules is\nsubject to how understandable and actionable the rules are for the consumers\nand how verifiable a company’s adherence to the rules is for experts (e.g.,\ngovernment supervisory agencies).141 In a market sense, understandability is\na prerequisite for consumers seeking to compare various AI products or\nservices with one another; likewise, the comparisons must enable consumers\nto take informed action: stay with the present offering or switch to an\nalternative. As for verifiability, it ensures the accuracy of the information\ndisclosed and safeguards a healthy competitive environment.\nThird, it is true that policymakers will invariably bear some\nadministrative burden to design well-defined and proportionate disclosure\nrules. However, any new rules entail unavoidable administrative costs, and\nthis fact should never excuse lassitude in policymaking. Therefore, and\nlastly, policymakers should proportionately tailor disclosure rules to prevent\nthe entirely avoidable chilling effects that overbroad disclosure rules can\nhave on private entities, including generative AI companies.142 For example,\npolicymakers should be aware of the possible tension that can exist between\ndisclosure rules and trade secret protection, as enlarged transparency rules\nmay unreasonably mandate the disclosure of information that companies\nhave good reason to withhold from the public.143\n136 Id. at 24 (noting that “[s]everal characteristics make disclosure requirements easier to implement\nthan other regulatory interventions”).\n137 Id. at 26–27 (noting that “disclosures are most effective when they meet three criteria:\nunderstandability, actionability, and verifiability”).\n138 Id. at 21–22, 25–26.\n139 Id. at 29–30.\n140 Id. at 24.\n141 Id. at 26–29.\n142 Id. at 29–30.\n143 See Katyal, supra note 128, at 117–21.\n138\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\n5. Traceability\nGiven the increasingly ambiguous boundary between machine output\nand human speech, traceability rules have been adopted to prevent bad actors\nfrom misusing AI-generated content in ways that distort our\nunderstanding.144 There are two main pillars for public traceability of\nchatbot-generated content. The first pillar is that generative AI service\nproviders must make their generated content technically detectable by\ndeveloping and employing technological methods for such tasks, such as\nwatermarking.145 The second pillar is that users of generative AI products\nmust disclose their uses to relevant parties.146 Section 4.5 of the U.S. AI\nOrder requires that the Secretary of Commerce consider and evaluate the\nfollowing three dimensions: (1) “authenticating content and tracking its\nprovenance,” (2) “detecting synthetic content,” and (3) labeling it with, for\nexample, watermarks.147\nThe two pillars of the traceability proposal are necessary but should be\nsubject to the proportionality test. The first pillar is necessary because some\nsensitive AI-generated information, especially when in audio and visual\nformats, has great potential to confuse and deceive the public. To make\nmatters worse, the self-regulatory mechanisms of the information market do\nnot effectively counter this threat. For example, malicious pornographers can\nmorph celebrities’ heads into sexually explicit deepfakes.148 AI can also be\nused to generate fake audio and visual output that can, for example, deceive\nvoters in the run-up to an election.149 In all of these cases, inter-informational\ncompetition is doomed to fail if traceability is not possible, because, in the\nabsence of traceability, people will reasonably assume that the audio and\nvisuals are conveying “facts.” The only other recourse would be for people\n144 See, e.g., Alexei Grinbaum & Laurynas Adomaitis, The Ethical Need for Watermarks in Machine-\nGenerated Language (Sept. 7, 2022) (unpublished manuscript) (on file with arXiv),\nhttps://arxiv.org/pdf/2209.03118 [https://perma.cc/2DUJ-YSRT].\n145 See, e.g., Hacker et al., supra note 116, at 1119.\n146 Id.\n147 See Exec. Order No. 14,110, 88 Fed. Reg. 75,191, § 4.5, 75, 202–03 (Nov. 1, 2023) (to be codified\nat 3 C.F.R.), https://www.govinfo.gov/content/pkg/FR-2023-11-01/pdf/2023-24283.pdf\n[https://perma.cc/AVU5-DQVU] [hereinafter Order or U.S. AI Order].\n148 See, e.g., Matt Burgess, Deepfake Porn Is Out of Control, WIRED (Oct. 16, 2023),\nhttps://www.wired.com/story/deepfake-porn-is-out-of-control/ [https://perma.cc/7KL2-NSNQ].\n149 For example, an audio recording surfaced on Facebook purportedly capturing Michal Šimečka,\nleader of the Progressive Slovakia party, and Monika Tódová from Denník N discussing election\nmanipulation, including the purchase of votes from the marginalized Roma minority just two days before\nthe election began. See Morgan Meaker, Slovakia’s Election Deepfakes Show AI Is a Danger to\nDemocracy, WIRED (Oct. 3, 2023), https://www.wired.com/story/slovakias-election-deepfakes-show-\nai-is-a-danger-to-democracy/ [https://perma.cc/SVJ5-AES5].\n139\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nto doubt the authenticity of all audio and visual evidence—an outcome that\nis hugely destructive in its own right.\nGenerally, statements of facts are considered falsifiable.150 But in the\nage of generative AI, the absence of traceability for AI-generated audio and\nvisuals explodes the possibility of falsifiability in many instances.\nConsequently, people will have no clue as to whether a proposed piece of\nevidence is true or false: is it an authentic “capture” of reality or an AI-\ngenerated fiction? Disinformation will flood the public square, profoundly\nand destructively shaping the information marketplace. However, with the\naid of traceability rules, the public can detect AI-generated content for what\nit is and can thus distinguish it from genuine content. Traceability offers the\npromise of deeply undercutting the competitiveness of deepfakes in the\ninformation marketplace.\nHowever, despite its usefulness, the traceability rule should be limited\nto those contexts that actually threaten competition in the information\nmarketplace. There should be no government requirement that AI service\nproviders watermark their textual output.151 The underlying reason for this\nlimit is that text-based disinformation is not nearly as disruptive to truth-\nfinding as audio and visual disinformation is. Although we can imagine cases\nwhere chatbot users generate, for example, a pornographic text featuring a\ncertain celebrity or a news article regarding an election bribe that never took\nplace, these texts are subject to effective verification. Thus, the information\nmarketplace for purely textual chatbot output is quite competitive.\nAdditionally, context-based traceability lacks a great deal of feasibility.152\nFor example, research found that the detection tool can only correctly\nidentify 26% of AI-written text and produces false positives 9% of the\ntime.153\nAccording to the second pillar, users of generative AI should disclose\ntheir use in specific fields. I support this rule because it is entirely\ntechnology-neutral and poses no harm to competition in the information\n150 See Mann v. Abel, 10 N.Y.3d 271, 276 (2008) (ruling that “[e]xpressions of opinion, as opposed\nto assertions of fact, are deemed privileged and, no matter how offensive, cannot be the subject of an\naction for defamation”).\n151 See, e.g., Peter Henderson, Should the United States or the European Union Follow China’s Lead\nand Require Watermarks for Generative AI?, GEO. J. FOR INT’L AFFS. (May 24, 2023),\nhttps://gjia.georgetown.edu/2023/05/24/should-the-united-states-or-the-european-union-follow-chinas-\nlead-and-require-watermarks-for-generative-ai/ [https://perma.cc/XL8P-92WM].\n152 Id.\n153 See Fiona Jackson, OpenAI Quietly Bins Its AI Detection Tool –– Because It Doesn’t Work,\nTECHHQ (July 27, 2023), https://techhq.com/2023/07/ai-classifier-tool-cancelled-ineffective/\n[https://perma.cc/KHT6-MHYJ].\n140\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nmarketplace.154 Any technology has its evil side. Consider knives; they are\npotentially deadly, but we never limit their sharpness. Generative AI is no\nexception. Therefore, apart from some special fields, such as healthcare and\nlegal services, that need legislation governing disclosure requirements,\nindustry’s self-generated guidelines should be sufficient for private-sector\nentities seeking to establish when and how users should disclose their use of\ngenerative AI.\n6. Notice-and-Respond Mechanism\nAnother highly popular regulatory proposal is the notice-and-respond\nmechanism. The rule behind it stipulates that generative AI service providers\nshould actively mitigate content risks by expeditiously responding to users’\nreports of problematic output. This mechanism bears a resemblance to the\nwell-known “safe harbor” rules in copyright law.155 In contrast to the dataset\ncuration rule, which acts as a root-level cleansing approach, the notice-and-\nrespond mechanism is decentralized and can control risks at the user’s end.156\nSome EU researchers have suggested that the EU should expand its Digital\nServices Act (DSA),157 especially the “notice and action mechanism”158 and\nthe appointment of “trusted flaggers.”159 Some U.S. researchers have\nsimilarly endorsed this bottom-up approach to controlling misinformation,160\nprivacy,161 and copyright risks.162\nAlthough the proposal to expand the notice-and-respond mechanism\nsounds impressive, its necessity is still questionable. The notice-and-respond\n154 UK’s policymakers have raised a similar idea that a clear, proportionate approach to regulation\nentails regulatory measures focusing on the use of AI, rather than the technology itself. See UK AI\nREGULATION WHITE PAPER, supra note 129, at 5.\n155 See 17 U.S.C. § 512(c)(1)(C) (Service providers should, “upon notification of claimed\ninfringement as described in paragraph (3), respond[] expeditiously to remove, or disable access to, the\nmaterial that is claimed to be infringing or to be the subject of infringing activity”).\n156 See Hacker et al., supra note 116, at 1120.\n157 As the applicable scope of the DSA rules is confined to the “intermediary service,” including\n“mere conduit service,” “cashing service,” and “hosting service,” the LLM systems are not subject to\nDSA obligations. See id. at 1118.\n158 See Regulation (EU) 2022/2065 of the European Parliament and of the Council of 19 October\n2022 on a Single Market For Digital Services and amending Directive 2000/31/EC (Digital Services Act),\n2022 O.J. (L 277) 50 [hereinafter DSA].\n159 Id. at 56.\n160 See, e.g., Eugene Volokh, Large Libel Models? Liability for AI Output, 3 J. FREE SPEECH L. 489,\n518–20 (2023) (proposing to use “notice-and-blocking” regime to mitigate the risks of false and\ndefamatory assertions generated by AI systems).\n161 See, e.g., Jon M. Garon, An AI’s Picture Paints a Thousand Lies: Designating Responsibility for\nVisual Libel, 3 J. FREE SPEECH L. 425, 445–52 (2023) (proposing “notice-and-takedown” rules to mitigate\nlibel and privacy risks).\n162 See, e.g., Henderson et al., supra note 120, at 26 (proposing to extend DMCA safe harbor rules\nto shield generative AI output from copyright infringement liabilities).\n141\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nmechanism requires chatbot companies to carry out two basic tasks: (1)\nestablish a mechanism through which the users could give the providers\nrapid and sufficiently detailed feedback on generated content; and (2)\nrespond promptly and satisfactorily to users’ feedback. A failure on the part\nof the providers to carry out either task would subject them to severe\npenalties from enforcers, or to the threat of civil cases from users seeking to\nprove that the providers are liable for “actual malice” or “negligence.”163\nSome researchers have argued that a wealth of knowledge acquired\nfrom over two decades of experience with safe harbor rules in the realm of\ncopyright law can be applied, with good effect, to the realm of generative\nAI.164 However, the notice-and-takedown regime in copyright law would\nlikely experience “transplant rejection” in the context of generative AI. The\nnecessity of notice-and-takedown in copyright law is contingent on at least\nthe following conditions: First, alleged copyright infringement occurs in the\npublic sphere, meaning that a notice-and-takedown mechanism can, at least\nin theory, effectively stop the dissemination of copyright-infringing works.\nHowever, the use of chatbots typically occurs in a private, one-to-one\ncontext. And although chatbots generate problematic content, such as\nmisleading or false information, there might be no harm to a specific user if\nthat user is critical enough.165 The problematic content will cause no\nsubstantial social harm as long as the user does not disseminate it to the\ngeneral public.166\nThe second condition justifying notice-and-takedown mechanisms in\ncopyright law is material clarity. Copyright infringement involves relatively\nobjective and concrete evidence, so that copyright holders can issue a\ntakedown notification to a copyright violator, requesting that it take action\nbased on detailed, verifiable facts.167 However, in the context of chatbots, the\nobvious lack of basic consensus regarding what qualifies as problematic\ninformation makes a valid notification from lay users impractical. Thus, the\nvolume of hopelessly unclear, highly subjective notifications would burden\nAI companies with prohibitively high administrative costs that do not arise\nin the realm of copyright law.168\n163 See, e.g., Volokh, supra note 160, at 514–21, 531.\n164 See id. at 519.\n165 See supra notes 90–94 and accompanying text.\n166 Therefore, I argue that the point of regulation should not be on technologies and AI companies,\nbut on users. See discussion supra Section IV.A.4.\n167 See 17 U.S.C. § 512(c)(3)(A).\n168 In fact, the notice-and-takedown regime of copyright law has long been criticized as ineffective\nand inefficient, just like the whack-a-mole game. The internet service providers have to spare many\nresources that could be otherwise more productive in handling numerous notifications. See, e.g., U.S.\nCOPYRIGHT OFF., SECTION 512 OF TITLE 17: A REPORT OF THE REGISTER OF COPYRIGHTS 32, 76 (2020).\n142\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nThe third notice-and-takedown condition in copyright law is its\nfeasibility and affordability of the take-down process. Some advocates of\ncopyright safe harbor argue that “[t]he genius of the DMCA is that it lets\ntechnology startups comply with the law without hiring a platoon of\ncopyright lawyers.”169 However, this ease would not be transferable to\nchatbots. Chatbot users’ requests, comments, and complaints about\nproblematic information would force AI researchers to spend an inordinate\namount of time curating their datasets and training LLMs to produce answers\nthat still would strike many users as unethical or inaccurate.170 Therefore, the\nwhole costly process would simply yield more dissatisfaction and\nuncertainty. In short, the notice-and-respond rule, when applied to chatbot\ncompanies, would not be a safe harbor, but dangerous waters.\nMy point is not that chatbot companies should steer clear of notice-and-\nrespond mechanisms. These mechanisms should not be compulsory. Chatbot\ncompanies are already under market pressure to adopt such feedback\nmechanisms, which, to this extent, can help the companies improve the\nquality of their output with reasonable efficacy. To date, chatbot companies\nhave voluntarily implemented such a mechanism.171\n7. Audits\nProponents of regulations have widely argued that governments should\nsubject generative AI providers to auditing rules, which would necessitate\nthe establishment of external neutral auditors and the formulation of a\ntailored set of auditing procedures.172 These procedures would help to\nidentify, assess, monitor, and manage both risks and risk-management\nmeasures linked to chatbot companies.173 Many jurisdictions have considered\n169 See, e.g., Chris Sprigman & Mark Lemley, Op-Ed: Why Notice-and-Takedown Is a Bit of\nCopyright Law Worth Saving, L.A. TIMES (June 21, 2016), https://www.latimes.com/opinion/op-ed/la-\noe-sprigman-lemley-notice-and-takedown-dmca-20160621-snap-story.html [https://perma.cc/C86W-\nUKNF].\n170 See Henderson et al., supra note 120, at 19 (explaining the challenges and uncertainties\nsurrounding the application of DMCA takedowns in the context of generative models).\n171 See, e.g., supra discussion and accompanying note 86.\n172 See generally Jakob Mökander et al., Auditing Large Language Models: A Three-Layered\nApproach, AI ETHICS (2023); Jakob Mökander & Luciano Floridi, Ethics-Based Auditing to Develop\nTrustworthy AI, 31 Minds & Machines 323 (2021).\n173 See, e.g., Hacker et al., supra note 116, at 1120 (arguing for an expansion of the DSA rules (i.e.,\nArt. 34–35) to establish comprehensive auditing rules to manage risks of LLM systems); see also\nWinograd, supra note 117, at 654–55 (suggesting the establishment of an oversight body to exercise\ncomprehensive auditing work to ensure compliance and bolster the goal of privacy protection).\n143\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nimplementing auditing rules to exert greater control over the performance of\ngenerative AI service providers.174\nAlthough the regulatory design of a typical AI audit involves many\nunresolved problems and intractably difficult steps,175 AI auditing is a\nnecessary regulatory endeavor. First, auditing regulations are ancillary to\nestablished regulations that are directly related to chatbot output. At least on\nthe surface, the objective of AI auditing is simple: to examine whether an AI\ncompany has taken necessary and sufficient measures to comply with the\nestablished laws.176 This means that, in a light-touch and market-based\nregulatory environment, AI auditing will not be overly burdensome, as\ngenerative AI service providers that comply with mandatory auditing do not\nneed to shoulder heavy responsibilities.\nUnder a light-touch regulatory auditing regime, neither an independent\nthird party nor a government regulator would need to examine whether a\nchatbot’s output is accurate, non-biased, ethical, and lawful because the\nlight-touch regime would not have such requirements; instead, what would\nbe in the purview of the audits is, for example, whether generative AI service\nproviders have followed the transparency rules regarding understandability\nand accuracy.177 In other words, as a non-substantive and indirect regulatory\napproach, AI audits would impose costs that are directly proportionate to the\ncosts of direct regulation. As long as the direct conduct-based duties that the\ngovernment imposes on AI companies remain proportionate, the auditing\nrules will remain proportionate too. In this sense, many concerns regarding\nthe cost and feasibility of AI audits are unwarranted.178\n174 See, e.g., U.S. AI Order, supra note 147, at 75196 (“[T]o help ensure the development of safe,\nsecure, and trustworthy AI systems, [authorities] shall . . . (i) [e]stablish guidelines and best practices, . . .\nincluding: . . . (C) launching an initiative to create guidance and benchmarks for evaluating and auditing\nAI capabilities, with a focus on capabilities through which AI could cause harm, such as in the areas of\ncybersecurity and biosecurity.”); UK AI REGULATION WHITE PAPER, supra note 129, at 64 (“To assure\nAI systems effectively, we need a toolbox of assurance techniques to measure, evaluate and communicate\nthe trustworthiness of AI systems across the development and deployment life cycle. These techniques\ninclude impact assessment, audit, and performance testing along with formal verification methods.”);\nChinese Generative AI Measures, supra note 9, at art.19 (“The relevant authorities shall conduct\nsupervisory inspections of generative AI services based on their respective functions and\nresponsibilities . . .).\n175 See Evan Selinger et al., AI Audits: Who, When, How . . . Or Even If? (Sept. 11, 2023)\n(unpublished manuscript) (on file with SSRN), https://ssrn.com/abstract=4568208\n[https://perma.cc/962Z-8QSL]; see also Guha et al., supra note 102, at 55–69.\n176 See Selinger et al., supra note 175, at 10.\n177 See supra discussion and accompanying note 141.\n178 For these concerns, see, for example, Guha et al., supra note 102, at 58–69.\n144\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\n8. Liability\nRather than call for the design and implementation of new regulatory\nrules, some researchers have sought to address the risks of AI output by\nturning to existing rules. One proposal is to subject chatbot companies to\nprivate law obligations. In this way, private plaintiffs can have grounds to\nclaim damages caused by chatbot output. Although a private plaintiff must\nenforce private law obligations, the deterrence acts similar to government-\nenforced regulations.179\nAmong various proposals, one would involve applying traditional\nproduct liability law to chatbot output.180 This proposal is considered suitable\nfor determining fault in cases involving emerging technologies.181 When, for\nexample, the design of a chatbot product has inherent defects that can harm\nconsumers, a plaintiff can claim damages based on product liability.182\nAdditionally, it has been argued that a finding of liability for an AI company\nshould rest on—and be consistent with—established cases concerning,\namong other things, search-engine results and mass-media products.183 Some\nexperts in the field have compared chatbot companies’ duty of care to parents\nof mischievous children or to owners of unruly pets.184 When chatbot\ncompanies “know[] or [have] reason to know” that their content is legally\nunacceptable, liability rules that have been established for traditional\ninformation media, such as bookstores, newsstands, and property owners,\nshould be applicable to the chatbot companies.185\nMarket competition cannot offer a company immunity from common\nlaw rules designed to protect customers. However, differences between\ntraditional physical markets and today’s information markets lie primarily in\nthe fact that in the context of the information market, the harms to reputation,\nprivacy, and so on tend to be intangible. Here, two points are noteworthy\n179 One prevalent and broad reading of “regulation” is to treat it “as an activity that restricts behaviour\nand prevents the occurrence of certain undesirable activities,” which can also be read as “risk\nmanagement.” In this sense, private law obligations and liabilities can be seen as a kind of regulation. See\nROBERT BALDWIN, MARTIN CAVE & MARTIN LODGE, UNDERSTANDING REGULATION: THEORY,\nSTRATEGY, AND PRACTICE 3 (2nd ed. 2012); BRONWEN MORGAN & KAREN YEUNG, AN INTRODUCTION\nTO LAW AND REGULATION: TEXT AND MATERIALS 13 (2007) (indicating that the concepts “regulation”\nand “risk management” can be used interchangeably); see also Douglas A. Kysar, Public Life of Private\nLife: Tort Law as a Risk Regulation Mechanism, 9 EUR. J. RISK REGUL. 48, 51 (2018).\n180 See Nina Brown, Bots Behaving Badly: A Products Liability Approach to Chatbot-Generated\nDefamation, 3 J. FREE SPEECH L. 389 (2023).\n181 See id. at 392.\n182 See id. at 410–11 (showing examples where the plaintiff may have an argument that the design\nwas defective).\n183 See Jane Bambauer, Negligent AI Speech: Some Thoughts About Duty, 3 J. FREE SPEECH L. 343,\n362 (2023).\n184 Id.\n185 See Volokh, supra note 160, at 520–21.\n145\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nregarding how courts should handle the intangible harms and potential\nliability of chatbot companies. First, judges should ascertain that real harm,\nrather than mere risk, exists in a present case. As I argued previously, inter-\ninformational competition in internal and external markets can alleviate most\nrisks associated with chatbot content.186 For example, in the event where\nChatGPT baselessly accused a U.S. law professor of sexual harassment,187\ndoes the output cause any harm to the professor? Here, ChatGPT “cited” a\nMarch 2018 article in the Washington Post as the source of the\ninformation.188 However, the cited article does not exist at all. Skeptics can\neffortlessly verify or disprove that information via other competing\ninformation outlets. Most people would not take the information seriously—\nthis is little more than a fool wielding a bullhorn and spouting nonsense.\nHowever, that does not mean that no one is responsible for the dissemination\nof such rumors. People who use chatbots and disseminate the generated\ncontent may have a duty to check the sources of the content, because\npublishers who either have “knowledge of [the content’s] falsity or [are] in\nreckless disregard of the truth” are ineligible for First Amendment protection\nand may incur liabilities of defamation.189\nSecond, if chatbot output causes harm, whether a court can find the\nchatbot company liable should still be subject to a risk-utility test. The Hand\nformula,190 or more precisely the marginal Hand formula,191 should govern\nthe test. This formula can help courts to determine whether the marginal cost\nof a precaution that a chatbot company can take will exceed the marginal\nbenefits. Despite some uncertainties surrounding its application, this formula\nat least shows that the policy advocating for zero risk is impossible to meet\nand is thus unreasonable for chatbot companies.192 For certain risks beyond\nthe duty of care that is owed by chatbots, the main responsibility is on chatbot\nusers: they should exercise individual prudence to mitigate the risks\nassociated with using a chatbot. Chatbots generate content in response to\nusers’ prompts, just as McDonald’s fulfills customer orders for hot coffee—\nthere are attendant risks, but if the service providers properly warn the\n186 See discussion supra Section III.B.\n187 See Verma & Oremus, supra note 20.\n188 Id.\n189 Time, Inc. v. Hill, 385 U.S. 374, 387–88 (1967).\n190 See United States v. Carroll Towing Co., 159 F.2d 169, 173 (2d Cir. 1947) (“[I]f the probability\nbe called P; the injury, L; and the burden, B; liability depends upon whether B is less than L multiplied\nby P: i.e., whether B less than PL.”).\n191 See COOTER ROBERT & THOMAS ULEN, LAW AND ECONOMICS 214 (6th ed. 2014) (“The marginal\nHand rule states that the injurer is negligent if the marginal cost of his or her precaution is less than the\nresulting marginal benefit. Thus, the injurer is liable under the Hand rule when further precaution is cost-\njustified.”).\n192 See, e.g., Volokh, supra note 160, at 526.\n146\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\ncustomers about the main risks, the residual risks should rest with the end\nusers. After all, McDonald’s cannot ensure that every customer safely sips a\nhot coffee purchased from a franchise.193\nEssentially, I am arguing that chatbot companies should provide users\nwith conspicuous and adequate warnings of potential risks. These warnings\ncan take the form of prompts stating, “The answer may contain misleading\ninformation” or “Dissemination of this information may put you at risk of\nlegal liability.” In addition, chatbot output should include links or notes\nidentifying the sources of the output. These steps on the part of chatbot\ncompanies should accomplish two desirable outcomes: (1) reduce copyright\ninfringement risks by softening the competitive relationship between the\nchatbot output and copyrighted materials,194 and (2) help users track the\nsources of chatbot output and verify the accuracy of the output.\nB. Further Suggestions\nDespite the suggestions previously made, there are information-market\nrisks that have not yet been addressed. Below, I will present some\npreliminary thoughts on three of them. First, internal market risks are the\nincreasing market concentration tendency and the negative effects that the\nanticompetitive practices of incumbent AI companies may have on market\ncompetition. These risks, in turn, will reduce the companies’ incentives to\nimprove the quality of their chatbot output. The other two risks are privacy\ndisclosure and copyright infringement. Chatbot companies and users may\npursue their interests in the moment but overlook the long-term adverse\neffects that the pursuit of these interests can have on the information market.\nAll three of these risks represent a type of market failure that may warrant\nsome forms of regulatory intervention.\n1. Re Internal-Market Competition\nIn this Article, the discussion up until now has shown that the current\nchatbot market is competitive and that users, if not satisfied with their current\nchatbot, can switch to alternative chatbots or multi-home in different\nsystems.195 However, this impressive degree of competitiveness should not\nlead policymakers to view the generative AI industry in a carefree manner.\nPrimary concerns are twofold: the increasing market concentration that we\ncan predict for the future market and the potential anticompetitive practices\nengaged by incumbents seeking to entrench their already impressive market\n193 Cf. Liebeck v. McDonald’s Restaurants, P.T.S., Inc., No. CV-93-02419, 1995 WL 360309 (N.M.\nDist. Ct. Aug. 18, 1994).\n194 See discussion infra Section IV.B.2.\n195 See discussion supra Section III.B.1.\n147\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\npositions. Both patterns may indirectly dampen the factors that incentivize\nchatbot companies’ improvement of their output quality.\nIt has been found that economies of scale and economies of scope in the\ngenerative AI industry may, for the foreseeable future, increase the market\nconcentration of incumbents.196 This risk stems primarily from the extremely\nhigh fixed costs of deploying and developing foundation models and the\nmuch lower variable costs of, for example, fine-tuning the models and\napplying them to various fields.197 In addition to inherent market structures\nthat favor incumbents and block the entrance or growth of would-be\ncompetitors, incumbents may protect their market position by adopting such\nsupplementary strategies as vertical integration, self-preferencing and\ndiscrimination, predatory pricing, collusion, and the creation of lock-in\neffects.198\nGiven the potential anticompetitive harm that the above structures and\npractices can have on current and prospective market players, some ex-ante\nregulatory interventions may be indispensable. Two articles have made\narguably outstanding contributions in this direction, namely Market\nConcentration Implications of Foundation Models by Vipra and Korinek,199\nand An Antimonopoly Approach to Governing Artificial Intelligence by\nNarechania and Sitaraman.200 As Narechania argues, “[w]hile AI might be\nnew, the problems that arise from concentration in core technologies are\nnot.”201 Correspondingly, they have proposed several antimonopoly tools\nthat might alleviate concerns about market concentration and that include\nseveral old-fashioned regulatory strategies directly copied from regulations\npreviously applied to telecommunications and digital platforms, such as\nstructural separations, nondiscrimination, open access, rate regulation, and\ninteroperability.202\n196 See Jai Vipra & Anton Korinek, Market Concentration Implications of Foundation Models: The\nInvisible Hand of ChatGPT, BROOKINGS (Center on Regulation and Markets Working Paper No.9, Sept.\n2023), https://www.brookings.edu/articles/market-concentration-implications-of-foundation-models-\nthe-invisible-hand-of-chatgpt/ [https://perma.cc/7PDF-67PB].\n197 See id. at 9–12.\n198 See id. at 23–29; see also Tejas N. Narechania & Ganesh Sitaraman, An Antimonopoly Approach\nto Governing Artificial Intelligence, (Jan. 19, 2024) (manuscript at 23–29) (unpublished manuscript) (on\nfile with SSRN), https://papers.ssrn.com/abstract=4597080 [https://perma.cc/K4JC-B6D3].\n199 See Vipra & Korinek, supra note 196.\n200 See Narechania & Sitaraman, supra note 198.\n201 Tejas Narechania, A Path to Regulating AI, BERKELEY L. (Jan. 19, 2024),\nhttps://www.law.berkeley.edu/sidebar/tejas-narechania-regulating-ai-technology/\n[https://perma.cc/PBJ2-YBFT].\n202 See Narechania & Sitaraman, supra note 198, at 44–47; Vipra & Korinek, supra note 196, at 30–\n36.\n148\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nIt is noteworthy that these regulatory proposals are not content-based,\nbut market-based and technology-neutral. They target market structures and\nshould be evaluated dynamically within the context of the entire industry,\nwhich can be considered from the perspective of the two-decade policy shift\naffecting network neutrality rules in the United States.203 Although some\nregulatory proposals may face stiff opposition and may indeed prove\nunnecessary, the effort to design ex-ante regulations that invigorate market\ncompetition is, at the very least, a push in the right direction.204\n2. Re External-Market Competition\nPrivacy disclosure and copyright infringement are two risks that neither\ninternal- nor external-market competition can adequately address, because\nsome information consumers may not be rational enough and may welcome\nprivacy disclosure and copyright-infringing output.205 To fulfill these needs,\nchatbot companies may fail to take sufficient steps to counter these risks.206\nGiven such market failure, governments may need to consider some properly\ntailored regulations.\na. Privacy Risks: Personalized Protection Tailored to Users’\nIndividual Expectations\nOne proposal to mitigate privacy risks is to require that chatbot\ncompanies access only publicly available data.207 But this proposal only\naddresses one part of the problem. While it truly would go far in immediately\nand effectively mitigating privacy risks, some aspects of this proposal\nrequire clarification and moderation. First, the definition of “public data”\nmerits detailed elaboration. In fact, the standards that determine what\nqualifies as public data can be ambiguous and quite subjective. The flip side\n203 See supra note 44 and accompanying text.\n204 See U.S. AI Order, supra note 147, at 75208–09 (urging agencies to “promote competition in AI\nand related technologies, as well as in other markets [by] addressing risks arising from concentrated\ncontrol of key inputs, taking steps to stop unlawful collusion and prevent dominant firms from\ndisadvantaging competitors, and working to provide new opportunities for small businesses and\nentrepreneurs”); see also Readout of White House Meeting on Competition Policy and Artificial\nIntelligence, THE WHITE HOUSE (Jan. 20, 2024), https://www.whitehouse.gov/briefing-room/statements-\nreleases/2024/01/20/readout-of-white-house-meeting-on-competition-policy-and-artificial-intelligence/\n[https://perma.cc/BJ5J-XMVC].\n205 See discussion supra Section III.B.2.\n206 Id.\n207 See Winograd, supra note 117, at 649. Here, it is noteworthy that the author argues that the\nproposal to restrict the purview of training data to “publicly-intended” data is non-mandatory. If so,\nhowever, the proposal would become meaningless because chatbot companies have no incentives to limit\ntheir training data while regulators cannot enforce such rules.\n149\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nof this issue—“what constitutes privacy?”—is also highly subjective.208 A\nunitary standard for privacy protection creates either false positives when\nindividuals do not view their data as private or false negatives when\nindividuals are more privacy-sensitive than the regulators anticipate. Thus,\nthere is little sense in assertions that Wikipedia and other encyclopedias,\nnewspapers, and magazines are publicly intended data while social-media\ncontent and public posts created by individuals fall under the category of\nnon-public data.209 Obviously, some posts on X (formerly Twitter), for\nexample, are in the public realm because users expect their posts to circulate\nas far as possible. Thus, chatbot companies may find that an initially\nsatisfactory set of standards for making “public–private” distinctions is\nhighly ambiguous. Second, data-curation requirements, such as those which\nI discussed above, are onerous for AI companies and even detrimental to the\nwhole generative AI industry.210 AI companies cannot tag each piece of data\nbased on a public–private dichotomy.\nIn light of these concerns, an alternative proposal is a personalized and\ndecentralized privacy-protection scheme: personalized because a one-size-\nfits-all approach cannot satisfy the heterogeneous privacy expectations and\nother needs of all people;211 and decentralized because users, not chatbot\ncompanies, should have the final say on whether a piece of data can be\ndisclosed and used. To that end, users who have an opportunity to share their\ninformation in a digital setting should be entitled and required to establish\ntheir privacy expectations. For example, when X users share a post, X should\nrequire that they first confirm if it can serve as training data for LLMs.\nHere, a hierarchical approach to privacy protection based on data,\nrecipients, and purposes is very promising and well-suited for LLM data\ntraining.212 Users of a digital service can evaluate whether its privacy policy\naligns with their privacy expectations. To perform this evaluation, users need\n208 See Pamela J. Wisniewski et al., Making Privacy Personal: Profiling Social Network Users to\nInform Privacy Education and Nudging, 98 INT’L J. HUMAN-COMPUT. STUD. 95, 106 (2017) (empirically\nshowing “the complex, multi-dimensional nature of end users’ varying privacy behaviors and levels of\nprivacy feature awareness”); Alfred Kobsa, Tailoring Privacy to Users’ Needs, in USER MODELING 2001\n303, 303 (Mathias Bauer et al. eds., 2001) (arguing that “a uniform solution for privacy demands does\nnot exist since both user preferences and legal stipulations are too heterogeneous”).\n209 See Winograd, supra note 117, at 650. For relevant debates on whether the information users\nshare on social media is public or not, see, for example, Facebook, Inc. v. BrandTotal Ltd., 499 F. Supp.\n3d 720, 739 (N.D. Cal. 2020); HiQ Labs, Inc. v. LinkedIn Corp., 31 F.4th 1180, 1189–90 (9th Cir. 2022).\n210 See discussion supra Section IV.A.3.\n211 See, e.g., Christoph Busch, Implementing Personalized Law: Personalized Disclosures in\nConsumer Law and Data Privacy Law, 86 U. CHI. L. REV. 309 (2019); see also Daniel J. Solove,\nIntroduction: Privacy Self-Management and the Consent Dilemma, 126 HARV. L. REV. 1880 (2013)\n(introducing privacy self-management, consent dilemma, and corresponding solutions).\n212 See Yuan Hong et al., A Hierarchical Approach to the Specification of Privacy Preferences,\nINNOVATIONS IN INFO. TECH. (IIT) 660, 660 (2007).\n150\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nto know what, if any, personal information (data) is shared with specific\nparties (recipients) and for what specific objective (purpose). Once in\npossession of this knowledge, users should be able to personalize their\nprivacy settings. Thereafter, all relevant parties, including above all the\ndigital service provider, should adhere to these settings. Picking up on the\nprevious example of X, we should expect that this social media giant would\nprocess its collected data according to users’ settings and would employ\ntechnological and legal resources to prevent LLMs from accessing data that\nusers have identified as being off limits. In this way, LLMs can access a\nsizable amount of training data without bearing onerous compliance costs\nrelated to privacy protection, and simultaneously, users will have an\nincentive to share information that can serve the training needs of LLMs.\nb. Copyright Risks: Softening the Competitive Relationship\nbetween Chatbot Output and Copyrighted Material\nRegarding the copyright risks associated with chatbots, my core\nsuggestion is to soften the competitive relationship between chatbot-\ngenerated output and copyrighted material. This suggestion—which stands\nin contrast to a mandatory clearing of copyrighted material from AI\ndatabases—is based primarily on the spirit of the first and fourth factors of\nfair use. Specifically, the first factor of fair use examines “the purpose and\ncharacter of the [secondary] use” of a copyrighted work, and the fourth factor\nfocuses on “the effect of the use upon the potential market for or value of the\ncopyrighted work.”213 One aspect that these two factors share is their stress\non the competitive relationship between secondary creation and copyrighted\nmaterial.214 The concept and practice of fair use would suffer gravely if the\nlaw permitted the secondary use of a copyrighted work to supersede the work\nand deprive it of its otherwise deserved market share.215 Therefore, it is\nessential to know whether the secondary work is in direct competition with\n213 17 U.S.C. § 107.\n214 For the application of factor one, see Campbell v. Acuff-Rose Music, Inc., 510 U.S. 569, 579\n(1994) (emphasis added) (citations omitted) (“The central purpose of this investigation is to see . . .\nwhether the new work merely ‘supersede[s] the objects’ of the original creation . . . or instead adds\nsomething new, with a further purpose or different character, altering the first with new expression,\nmeaning, or message.”). For the interpretation of factor four, see Harper & Row, Publishers, Inc. v. Nation\nEnterprises, 471 U.S. 539, 568–69 (1985) (emphasis added) (citation omitted) (“[T]o negate fair use one\nneed only show that if the challenged use ‘should become widespread, it would adversely affect the\npotential market for the copyrighted work.’ [. . .] [A] fair use doctrine that permits extensive\nprepublication quotations from an unreleased manuscript without the copyright owner’s consent poses\nsubstantial potential for damage to the marketability of first serialization rights in general.”).\n215 See, e.g., Andy Warhol Found. for the Visual Arts, Inc., v. Goldsmith, 598 U.S. 508, 528 (2023)\n(emphasis added) (citation omitted) (“[T]he first factor relates to the problem of substitution—copyright’s\nbête noire. The use of an original work to achieve a purpose that is the same as, or highly similar to, that\nof the original work is more likely to substitute for, or ‘supplan[t],’ the work.”).\n151\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nthe copyrighted work in an information market, and as such, might silence\nor supersede the copyrighted one.\nIn this sense, AI output might be found eligible for fair use protection\nif chatbot companies adopt sufficient measures to soften the competition\nbetween chatbot-generated information and copyrighted material. Here,\nAuthors Guild v. Google serves as a very fitting and strong precedent for\nreference.216 In this case, the Second Circuit Court of Appeals considered the\nfeatures of Google’s snippet view and, citing the first factor of fair use, ruled\nin favor of Google.217 The court explained that the snippet-view function is\ntransformative in that it presents “the searcher just enough context\nsurrounding the searched term to help her evaluate whether the book falls\nwithin the scope of her interest (without revealing so much as to threaten the\nauthor’s copyright interests).”218 In examining the fourth factor, the court\nheld that, although the snippet function may create a substitute for a\ncopyrighted work, any reduction in the work’s market value is so minimal\nthat the fourth factor does not favor the plaintiffs.219 In other words, “the\npossibility, or even the probability or certainty, of some loss of sales does\nnot suffice to make the copy an effectively competing substitute.”220\nTherefore, the court concluded the following:\nEven if the snippet reveals some authorial expression,\nbecause of the brevity of a single snippet and the\ncumbersome, disjointed, and incomplete nature of the\naggregation of snippets made available through snippet\nview, we think it would be a rare case in which the\nsearcher’s interest in the protected aspect of the author’s\nwork would be satisfied by what is available from snippet\nview, and rarer still—because of the cumbersome,\ndisjointed, and incomplete nature of the aggregation of\nsnippets made available through snippet view—that snippet\nview could provide a significant substitute for the purchase\nof the author’s book.221\nThe lessons drawn from Authors Guild v. Google can be generalized\ninto two suggestions useful for both government policymaking and private-\n216 Authors Guild v. Google, Inc., 804 F.3d 202 (2d Cir. 2015).\n217 Id. at 217–18.\n218 Id. at 218.\n219 Id. at 224.\n220 Id.\n221 Id. at 224–25 (emphasis omitted).\n152\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nsector internal compliance. First, chatbot-generated content should be as\ntransformative as possible.222 To comply with this rule, chatbot companies\ncan train their LLMs to provide users only summaries or brief excerpts of\ncopyrighted works, never the entire work or lengthy verbatim segments of\nthe work.223 Second, chatbot-generated content should cite the sources of the\noutput. This suggestion, which satisfies the traceability requirement,224\nenables users to find and use the original source, in turn promoting the\ncirculation of copyrighted material.225 The combination of transformative\nchatbot output and heavily cited chatbot output not only greatly reduces any\nrisk to copyright holders, but also grants them a degree of positive publicity\nfrom which they could benefit reputationally and financially.226\nThese two suggestions should, if implemented, soften the competitive\nrelationship between chatbot output and copyrighted material, and should do\nso in a way that favors the finding of fair use. For example, suppose ChatGPT\nresponds to users’ questions by referring to a copyrighted New York Times\narticle. In this scenario, ChatGPT would more likely be compliant with fair-\nuse standards if it provided only a summary of or brief excerpt from the\narticle, including a precisely cited reference and even a hyperlink to the\noriginal source. These approaches can ensure the free flow of diverse\nchatbot-generated information to users in ways that comply with fair-use\nstandards, promote copyrighted works, and avoid substitution and free-rider\nproblems.\nV. CONCLUSION\nCan inter-informational competition liberate generative AI from\ngovernment regulations targeting chatbot output? This is a question that\nshould be considered and answered by policymakers before they issue\nambitious regulatory proposals promising to stamp out the risks posed by\ngenerative AI. However, the recent push to establish such regulation has\nrelegated this question to the sidelines. In their reflexive embrace of\n222 See Andy Warhol Found. for the Visual Arts, Inc., v. Goldsmith, 598 U.S. 508, 529 (2023) (“The\nlarger the difference, the more likely the first factor weighs in favor of fair use. The smaller the difference,\nthe less likely.”).\n223 Cf. Authors Guild, 804 F.3d at 224 (emphasis added) (“Snippet view, at best and after a large\ncommitment of manpower, produces discontinuous, tiny fragments, amounting in the aggregate to no\nmore than 16% of a book. This does not threaten the rights holders with any significant harm to the value\nof their copyrights or diminish their harvest of copyright revenue.”).\n224 See discussion supra Section IV.A.5.\n225 See generally David Fagundes, Market Harm, Market Help, and Fair Use, 17 STAN. TECH. L.\nREV. 359, 378–85 (2014) (presenting four cases in which unauthorized use of copyrighted works prove\nbeneficial to copyright owners, namely recognition, affirmation, reincarnation, and innovation).\n226 Id. at 378–80.\n153\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\nregulation, researchers and policymakers mistakenly regard all theorized AI\nrisks as unequivocally real, present, and harmful, and seek to control these\nrisks by counterproductively employing direct, heavy-handed regulatory\ntools that create more adversity than they overcome.\nThe ideas that I have set forth in this Article challenge the prevailing\nregulatory fervor over generative AI and redirect these misplaced energies\ntoward a far more rational set of steps. Specifically, I recommend a market-\ncentered framework for evaluating not only the widely recognized risks of\nchatbot output, but also the most commonly proposed regulatory responses.\nThis framework rests on four lessons drawn from the long regulatory\nhistory of radio, television, broadband internet, and other information\ntechnologies. First, policymakers and researchers have a great tendency to\noverstate the risks and uncertainties caused by emerging technologies and to\nreflexively embrace harsh conduct-based regulations for the mitigation of\npotential harms. Second, policymakers and researchers have historically\nregarded competition as primary and regulation as ancillary. Third,\npolicymakers should narrowly tailor generative AI regulations so that they\ninvigorate the market rather than weaken it. And finally, policymaking and\njudicial rulings for information technologies should always serve to enhance\ninformation consumers’ right to access diverse and competing information\noutlets.\nResting on these four foundational lessons, the proposed framework\nshould require policymakers to evaluate the internal market competition\nbetween a chatbot and another chatbot (e.g., ChatGPT vs. Claude) and the\nexternal market competition between a chatbot and other types of\ninformation outlet (e.g., ChatGPT vs. Google’s search engine) to determine\nwhether the market mechanism can mitigate the risks of chatbot output. I\nhave argued that current inter-informational competition is sufficient to\nmitigate some risks of chatbot output because chatbot companies\nthemselves, under the market forces, have an inherent incentive to improve\ntheir output quality and derisk their systems, even in the absence of any\ngovernmental intervention. Information consumers are critical and can\nswitch to an alternative source if they find the current chatbot output\nunreliable. Some empirical findings also demonstrate that most chatbot-\ncontent risks are overblown and can be mitigated by largely unfettered inter-\ninformational competition. However, self-regulation of the information\nmarketplace may fail to mitigate privacy disclosure and copyright risks.\nUsers and chatbot companies are sometimes shortsighted and care only about\nsatisfying their current interests. Thus, to ensure the longer-term health of\nthe information ecosystem, policymakers should consider some\nproportionately tailored regulations.\n154\n22:109 (2024) Regulating Chatbot Output Via Inter-Informational Competition\nGiven how powerful the information market is at mitigating chatbot\ncontent risks, we should ask ourselves: Whether a specific regulatory\nproposal is necessary and proportionately tailored? My position is that some\nproposals, such as mandatory prohibitions, licensure, curation of datasets,\nand notice-and-respond mechanisms, are not well-tailored––they are\ntherefore unnecessary in the context of chatbot content regulation. Instead,\ntransparency, traceability, and audit proposals have great potential to\nenhance inter-informational competition and further alleviate chatbot\ncontent risks. Moreover, in legal cases determining a chatbot company’s\nliability, judges should first ascertain the existence of real harm as opposed\nto mere risk, and second, should calculate an appropriate allocation of duties\nby performing a risk-utility test (the Hand formula or, more precisely, the\nmarginal Hand formula).\nNot all government intervention is unwelcome. This Article has\nproposed several regulatory suggestions for unresolved issues derived from\npotential market failures. Concerning concentration tendencies and\nanticompetitive practices in the internal chatbot market, I have endorsed\nseveral ex-ante regulations to adjust the competitive relationship among AI\ncompanies. So long as they are subject to dynamic market analysis, these\nmarket-based rules will nurture healthy competition, which in turn will\nincentivize improvements in both output quality and system derisking.\nRegarding privacy disclosure risks, I have suggested a personalized and\ndecentralized approach to protecting user privacy. In this way, policymaking\ncan reach a delicate balance between the need to access sufficient data for\nLLM training and the protection of users’ privacy expectations. As for\ncopyright infringement risks, government regulations should require that\nchatbot companies train their LLMs to paraphrase, summarize, and cite\ncopyrighted works. These steps will soften the competing and conflicting\nrelationship between chatbot companies and copyright holders and will\nthereby counter copyright silencing and the free-rider problem.\nMy central aim in this Article has been to generate a preliminary\nanalytical framework that policymakers can use for the even-handed\nassessment of regulatory proposals. My expectation is that the framework\nwill help cool down the current regulatory frenzy over the much-hyped\ncontent risks attributed to generative AI. The present study can act as a\nblueprint for the next stage of research in this field. First, the proposed\nframework can broaden and deepen our approaches to analyzing AI risks,\nthereby refining the criteria we use when trying to determine what constitutes\na good regulation. Second, the proposed framework, given its preliminary\nnature, invites more research on how to design and employ indirect, market-\nbased regulation to effectively mitigate AI risks. Third, the proposed\nframework will benefit from empirical research on information market\n155\nNORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY\ncompetition: How competitive is the current chatbot market? Does chatbot\noutput face substantial competition from other types of information outlets?\nAnd are information consumers critical enough to differentiate the quality of\nvarious information outlets, switch to a more reliable outlet if there is one,\nand hence avoid falling prey to the lock-in effect? Fourth, my suggestions in\nthis Article have a great bearing on the education of information users: how\ncan governments, businesses, and individuals themselves ensure that society\nis equipped with enough technological expertise and intellectual\nwherewithal to navigate an information marketplace increasingly flooded\nwith problematic information?\n156",
    "pdf_filename": "Regulating_Chatbot_Output_via_Inter-Informational_Competition.pdf"
}