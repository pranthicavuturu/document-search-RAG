{
    "title": "The future of document indexing - GPT and Donut revolutionize table of content processing",
    "context": "Industrial projects rely heavily on lengthy, complex specification documents, making te- dious manual extraction of structured information a major bottleneck. This paper introduces an innovative approach to automate this process, leveraging the capabilities of two cutting- edge AI models: Donut, a model that extracts information directly from scanned documents without OCR, and OpenAI GPT-3.5 Turbo, a robust large language model. The proposed methodology is initiated by acquiring the table of contents (ToCs) from construction specifi- cation documents and subsequently structuring the ToCs text into JSON data. Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5 Turbo reaching 89% in effec- tively organizing the ToCs. This landmark achievement represents a significant leap forward in document indexing, demonstrating the immense potential of AI to automate information extraction tasks across diverse document types, boosting efficiency and liberating critical resources in various industries. Keywords Document AI · Document Classification · Information extraction · Large Language Models · OCR Models · Visual Document Understanding 1 Traversing extensive and intricate documents and locating essential information buried within numerous pages demands considerable time and effort. This inefficiency not only impedes understanding but also results in significant costs associated with manual data entry and extraction. To address this challenge, researchers have delved into the expanding realm of AI, with a particular emphasis on automating the extraction of information from large documents. Documents, whether in traditional written or electronic form, are essential for conveying information in various domains such as technology and business. Electronic documents, including PDF files, Microsoft Word documents, spreadsheets, emails, invoices, and presen- tations, play diverse roles like record-keeping, communication, collaboration, and supporting legal and arXiv:2403.07553v1  [cs.IR]  12 Mar 2024",
    "body": "THE FUTURE OF DOCUMENT INDEXING: GPT AND DONUT\nREVOLUTIONIZE TABLE OF CONTENT PROCESSING\nDegaga Wolde Feyisa\nDreeven Technologies Inc, St-Lambert, QC, Canada\ndegagawolde@gmail.com\nHaylemicheal Berihun\nDreeven Technologies Inc, St-Lambert, QC, Canada\nhaylemicheal.mekonnen@gmail.com\nAmanuel Zewdu\nDreeven Technologies inc.\namanuelzewdu21@gmail.com\nMahsa Najimoghadam\nDreeven Technologies Inc, St-Lambert, QC, Canada\nnaji.mahsa@gmail.com\nMarzieh Zare\nDreeven Technologies Inc, St-Lambert, QC, Canada\nmarzieh.zare@Dreeven.com\nMarch 13, 2024\nABSTRACT\nIndustrial projects rely heavily on lengthy, complex specification documents, making te-\ndious manual extraction of structured information a major bottleneck. This paper introduces\nan innovative approach to automate this process, leveraging the capabilities of two cutting-\nedge AI models: Donut, a model that extracts information directly from scanned documents\nwithout OCR, and OpenAI GPT-3.5 Turbo, a robust large language model. The proposed\nmethodology is initiated by acquiring the table of contents (ToCs) from construction specifi-\ncation documents and subsequently structuring the ToCs text into JSON data. Remarkable\naccuracy is achieved, with Donut reaching 85% and GPT-3.5 Turbo reaching 89% in effec-\ntively organizing the ToCs. This landmark achievement represents a significant leap forward\nin document indexing, demonstrating the immense potential of AI to automate information\nextraction tasks across diverse document types, boosting efficiency and liberating critical\nresources in various industries.\nKeywords Document AI · Document Classification · Information extraction · Large Language Models ·\nOCR Models · Visual Document Understanding\n1\nIntroduction\nTraversing extensive and intricate documents and locating essential information buried within numerous\npages demands considerable time and effort. This inefficiency not only impedes understanding but also\nresults in significant costs associated with manual data entry and extraction. To address this challenge,\nresearchers have delved into the expanding realm of AI, with a particular emphasis on automating the\nextraction of information from large documents. Documents, whether in traditional written or electronic\nform, are essential for conveying information in various domains such as technology and business. Electronic\ndocuments, including PDF files, Microsoft Word documents, spreadsheets, emails, invoices, and presen-\ntations, play diverse roles like record-keeping, communication, collaboration, and supporting legal and\narXiv:2403.07553v1  [cs.IR]  12 Mar 2024\n\nA PREPRINT - MARCH 13, 2024\nfinancial transactions. The PDF format, introduced by Adobe Inc. in the 1990s, is particularly noteworthy\nfor its widespread use in online document distribution, preserving original layout and formatting across\ndevices and software. PDF documents, generated from applications like Microsoft Word and Adobe Acrobat,\ncan encompass text, images, tables, graphs, hyperlinks, annotations, and forms, making them integral to\nmodern document sharing and viewing practices. Additionally, a Table of Contents (ToC) acts as a guide in\ndocuments, facilitating quick navigation to specific sections of interest without the need to scan the entire\ntext. Typically positioned after cover pages, the ToC includes main headings, subheadings, or subsections,\nenhancing document accessibility for readers.\nThe benefits of automating information extraction are multifold. First, it provides a concise overview of the\ndocument’s structure and essential content, acting as a roadmap for efficient navigation. This is particularly\ncritical in technical documents like construction specifications, where specific sections hold information on\ndistinct components like electrical work or fire protection systems [1]. Second, it significantly reduces the\ntime and costs associated with manual data extraction. Imagine the human resource burden of manually\nextracting key information from a multi-page contract compared to an AI system performing the task swiftly\nand accurately. These advantages have spurred considerable research and development in the field, with\nnumerous companies offering visual document understanding tools (e.g., DocuVision, Adobe Experience\nManager) [2, 3, 4, 5], and academic papers exploring various AI-based information extraction approaches\n[1, 6, 7, 8].\nOur work falls within this domain, focusing on parsing large PDF documents and structuring their content for\neffortless navigation. Specifically, we target the extraction of heading numbers, titles, and their subheadings,\naiming to build comprehensive ToCs. This structured information, often stored in formats like JSON files\nor SQL databases, serves as a vital key to indexing the document easily. By providing users access to this\nstructured data through a user-friendly dashboard, we aim to empower them to visually comprehend the\ndocument’s content and navigate its depths with unprecedented ease.\nNavigating complex, multi-page documents can be a difficult task. Often, these documents are divided\ninto distinct sections, each focused on specific topics or types of information. For instance, a construction\nspecification document may contain sections dedicated to plumbing systems, floor openings, or concrete\nspecifications. Efficiently traversing these sections relies heavily on structured information, such as well-\norganized ToCs. This structured information serves as a roadmap, guiding readers to relevant passages and\nfacilitating comprehension.\nRecognizing the importance of structure in document navigation, our work focuses on automatically\nextracting heading numbers, titles, and their corresponding subheadings from lengthy PDF documents.\nThis extracted information forms the foundation of a comprehensive ToC, effectively transforming the\ndocument into a structured data format. We store this structured data in readily accessible formats like\nJSON files or SQL databases, enabling further analysis and visualization. We developed a user-friendly\ndashboard to empower users with effortless exploration of this extracted information. Through this interface,\nusers can readily visualize the document’s content, delve deeper into specific sections, and gain a holistic\nunderstanding of the document’s structure and main themes.\nThe remaining sections of the paper are organized as follows. In Section 2, we explore related works,\ndiscussing their contributions and shortcomings. The methodology employed in this study is outlined in\nSection 3. Our findings and the discussion of results are presented in Section 3.4. The paper concludes with\nSection 4, providing a conclusion, and Section 5, which outlines potential avenues for future work.\n2\nRelated Work\nUnderstanding document images is a critical task but presents significant challenges due to the need\nfor complex functions such as text reading and overall document comprehension. Current methods for\nVisual Document Understanding (VDU) rely on off-the-shelf Optical Character Recognition (OCR) engines\n[9, 10, 11, 12, 13, 14] for text extraction and focus on understanding tasks using OCR outputs. However,\nOCR-based approaches have several drawbacks, including computational costs, inflexibility in handling\ndifferent languages or document types, and error propagation issues. To address these challenges, a few\nOCR-free VDU models have been proposed [15, 16]. In contrast to OCR-based models, OCR-free VDU\nmodels are designed to understand the visual content of documents without relying on traditional OCR\ntechniques. These models use deep learning architectures, such as convolutional neural networks (CNNs)\nand transformers, to extract features from the visual content of documents. Large language models like GPT\n2\n\nA PREPRINT - MARCH 13, 2024\n[17] can extract structured information from raw text. In the following sections, let’s explore in-depth the\nliterature on OCR-based and OCR-free models proposed by various authors for document understanding.\n2.1\nOCR-Based VDU\nLi, et al. [12] proposed an upgraded version of a document analysis system called PP-StructureV2. This new\nversion contains two subsystems: Layout Information Extraction and Key Information Extraction (KIE). The\nfocus of KIE is to extract specific information that users are interested in, and it includes subtasks such as\nSemantic Entity Recognition (SER) and Relation Extraction (RE). The paper also introduces several models\nthat integrate text and layout information to improve the KIE process, including LayoutLM, LayoutLMv2,\nLayoutXLM, and XY-LayoutLM. Overall, the contribution of this paper is an improved document analysis\nsystem with enhanced functionality for extracting key information from unstructured documents.\nPP-StructureV2 is one of the core components of PaddleOCR. Specifically, PP-StructureV2 provides layout\nanalysis and key information extraction capabilities to PaddleOCR. The PaddleOCR team has developed\nan ultra-lightweight OCR system called PP-OCR, which prioritizes accuracy and speed for OCR industry\napplications [18]. PP-OCRv3 [19] is an upgraded version of PP-OCRv2, which incorporates nine optimization\nstrategies for text detection and recognition models. In comparison to PP-OCRv2, PP-OCRv3 demonstrates\nan 11% improvement in English model precision, a 5% improvement in Chinese model precision, and an\naverage recognition accuracy improvement of over 5% for 80 multilingual models, while maintaining a\nsimilar speed.\nIn paper[13, 14], the authors proposed the use of LayoutLM [20], a language model pre-trained on business\ndocuments, along with two new pre-training tasks and a post-processing algorithm to improve information\nextraction performance on expense receipts, invoices, and purchase orders. The proposed method signifi-\ncantly improves extraction performance on both public and private datasets. The authors used OCR (Optical\nCharacter Recognition) to extract textual and positional information from the business documents in the\nBusiness Documents Collection. This extracted information was then used as input for their pre-training\nlanguage models on business documents. The accuracy of OCR is important because it affects the quality\nof data that goes into training these models, which can ultimately impact their performance in extracting\nrelevant information from similar types of documents\n2.2\nOCR-Free VDU\nCurrent document understanding models, such as LayoutLM, will often require OCR processing to extract\nthe text from documents before they can be processed. While OCR can be an effective way to extract text\nfrom documents, it is not without its challenges. OCR accuracy can be impacted by factors such as the\nquality of the original document, the font used, and the clarity of the text. Furthermore, OCR is slow and\ncomputationally intensive which adds another layer of complexity. To overcome these challenges, new\napproaches are needed that can accurately interpret documents without the need for OCR.\nOne popular approach to OCR-free VDU is to use end-to-end models that incorporate large language models,\nsuch as BERT (Bidirectional Encoder Representations from Transformers) [15], GPT(Generative Pre-trained\nTransformer), and their variants [21]. These language models are pre-trained on large amounts of text data,\nenabling them to learn representations that capture the semantic relationships between words and phrases.\nBy incorporating these language models into OCR-free VDU models, they can better understand the context\nand meaning of the visual content of documents, such as invoices, receipts, and forms. This approach has\nshown promising results in tasks such as information extraction, document classification, and question\nanswering.\nThe Donut model [16] is an end-to-end VDU solution that uses an encoder-decoder transformers model\narchitecture, proposed by researchers from Naver CLOVA and recently made available to use with the\nHuggingFace transformers library. It encodes an image (split into patches using a Swim Transformer) into\ntoken vectors it can then decode, or translate, into an output sequence in the form of a data structure (which\ncan then be further parsed into JSON) using the BART decoder model, publicly pre-trained on multilingual\ndatasets.\n2.3\nLarge Language Models\nLarge language models (LLMs), including transformer-based models like BERT [22] or Megatron [23],\nrepresent a subset of AI specifically designed for extracting crucial information from documents. Trained\n3\n\nA PREPRINT - MARCH 13, 2024\non extensive datasets encompassing both text and code, these models excel in identifying and extracting\nvarious entities from text, such as names, addresses, dates, and numbers. This adaptability positions them\nas valuable tools for extracting key information from diverse documents, such as invoices, contracts, and\nmedical records [24].\nLeveraging their capacity to grasp contextual intricacies within text, Language Models (LLMs) demonstrate\nsuperior accuracy compared to conventional methods. The proficiency of LLMs in comprehending context\nenhances the precision of entity identification. Furthermore, these models can be efficiently scaled to handle\nsubstantial volumes of documents, rendering them highly suitable for various applications. Despite their\nadvantages, it is crucial to acknowledge the potential challenges associated with using LLMs for information\nextraction. Biases, factual errors, and limitations in understanding complex information can pose significant\nhurdles [25, 26]. Acknowledging and addressing these challenges is essential for ensuring the responsible\nand accurate deployment of LLMs in information extraction tasks. Recent advancements in LLM-based\ninformation extraction further enhance their utility. Techniques like multi-task learning and integration\nwith additional resources, such as knowledge graphs, contribute to improved performance and expanded\ncapabilities [27, 28, 29, 30]. These developments exemplify the dynamic nature of LLM research and its\ncontinuous evolution.\nFor further insights into challenges and methodologies related to LLM-based document information extrac-\ntion, the work of Yousefi Maragheh et al., [31] can provide valuable perspectives. Additionally, [32] provides\na comprehensive survey of LLM applications in information extraction.\n3\nMethodology\n3.1\nData Preparation\nAny machine learning project requires a huge amount of data. Fortunately, the model used for this project,\nthe Donut model, is pre-trained with a big dataset for both the image classification and information extraction\ntasks. So we fine-tuned it with our annotated image datasets to make it suitable for the two tasks.\nThe dataset used to fine-tune the image classifier model was composed of two classes of images taken from\nthe specification document. The two classes are labeled as \"ToC\", and \"other\". To train the second model,\nwhich extracts heading and subheading information from the table of content pages identified by the first\nmodel, we used annotated \"ToC\" images extracted from varieties of specification documents. The data\nneeded to fine-tune the Donut model should include the document’s image and a JSON file containing\nthe key information from the document. Figure 1 and 2 show how the data is prepared and annotation is\ngenerated for each ToC.\n4\n\nA PREPRINT - MARCH 13, 2024\nFigure 1: For this type of document, the first heading (h) is referred to as the heading, while the second\nheading (sh) is referred to as the subheading. The heading number (hn) comes before the heading title (ht).\nThe same is true for the subheading number (shn) and title (sht).\nFigure 2: For this type of document, the division is referred to as the heading(h), while the sections are\nreferred to as the subheading (sh). The heading number (hn) comes before the division title (ht). The same is\ntrue for the section number (she) and title (sht)\nOur dataset consisted of 200 annotated data samples utilized for both training and testing. We allocated\n90% of the dataset for training purposes and the remaining 10% for testing. The images in the dataset were\nstandardized to a height of 1260 and a width of 960 pixels. This ensured uniform dimensions for each image\nbefore being processed by the models in our study.\nThe dataset for the classification model is a page of the specification document with their respective class(table\nof contents, and other pages). In general, there are two classes: Table of Contents – a table of contents for the\ndocument, mostly found in the first few pages, and other Pages – pages that are not the table of contents.\nIt can include white pages, cover pages, and other pages of the document that are not the main table of\ncontents of the document.\n5\n\nA PREPRINT - MARCH 13, 2024\n3.2\nThe Proposed AI-Based Document Indexing Technique\nIn this section, we will discuss the methodology and process used for structuring a ToCs in a large PDF\ndocument. Figure 3 illustrates the proposed pipeline, which we will describe in detail. To begin with, the\nPDF document is uploaded, and each page is converted into an image. Subsequently, a classifier model is\nemployed to categorize the images into one of two groups: tables of contents, or other pages. The pages\nclassified as tables of contents are then fed into another model, which extracts information in ToCs such as\nheading numbers, heading titles, subheading numbers, and subheading titles from each page. The extracted\ninformation is then structured and saved in a JSON file, which is parsed and displayed in the frontend\napplication for the user to view. Additionally, the JSON file is stored in a database for future reference.\nFigure 3: The proposed approach for retrieving and structuring table of contents of a PDF document.\n3.2.1\nFetching the Table of Contents\nDocument classification is a process of assigning categories or classes to documents to make them easier to\nmanage, search, filter, or analyze. A document in this case is an item of information that has content related\nto some specific category. In a document classification application, an incoming stream or a set of documents\nis compared to a predefined set of rules. When a document matches one or more rules, the application\nperforms some action. In this work classify a page of specification documents into a table of contents and\nother pages. Since we extract section titles and numbers from the table of contents of the documents, this is a\nvery vital step toward the end objective.\nIn the case of the Donut model, the process involves converting the pages into images, which are then input\ninto the model to identify the ToC. Donut base model pre-trained on RVL-CDIP [33, 34] is pulled from\nhugging face and fine-tuned with classification data prepared as described in Section 3.1. On the other hand,\nwhen working with the OpenAI GPT-3.5 Turbo, a different approach is taken. Instead of converting the\npages into images, a prompt provided in Figure 4 is utilized to retrieve the text of the ToC after obtaining\nraw text data from the PDF. This method underscores the versatility of the GPT-3.5 Turbo, showcasing its\ncapability to handle textual information directly without the need for image conversion.\nFigure 4: A prompt to extract ToC text from the raw text of a PDF file.\n3.2.2\nStructuring the Table of Contents in JSON format\nExtracting data from unstructured documents is always a challenge. Previously we used to have rule-based\napproaches to tackle such problems. However, due to the nature of the rule-based mechanism, external\nknowledge sources, and manpower are required. To solve such issues, NLP is always a go-to solution for\neveryone. Deep learning has revolutionized the NLP field and to add to its hugging face has always delivered\n6\n\nA PREPRINT - MARCH 13, 2024\nstate-of-the-art solutions for multiple problems in NLP. We’re going to discuss one of the state-of-the-art\nOCR free visual document understanding called Donut and the OpenAI GPT-3.5 Turbo.\nIn this work, a Donut model pre-trained on the ICDAR-SROIE [35] dataset is used. The model is first pulled\nfrom the hugging face repository and fine-tuned with our dataset. We sued 90% for the training set and 10%\nfor the testing set. Two distinct types of specification documents exist in this context. The corresponding\nimages were then annotated based on the desired JSON outputs. Following the data preparation outlined in\n3.1, the data is supplied to the Donut model for the fine-tuning process.\nFor the OpenAI GPT-3.5 Turbo, we need to enhance the prompt design to leverage the capabilities of few-shot\nlearning. This tex-to-text generation model takes a list of messages as input and generates a model-generated\nmessage as output. Few-shot learning involves providing the model with a small number of examples\nto enable it to understand and generalize from them. In our approach, we have incorporated a few-shot\nlearning scenario by using a specific example in Figure 5. This example serves as a guiding instance for the\nmodel to learn the desired behavior. By presenting the model with an illustrative example, we enable it to\ngrasp the context and structure required for generating the desired output. This promotes a more efficient\nand effective learning process, allowing the model to adapt and generalize from the provided example to\nsimilar instances in the future.\nThe prompt is designed to instruct the model to extract JSON data based on the provided example and\nadhere to the given schema. This way, the few-shot learning paradigm enhances the model’s ability to\nunderstand and respond to diverse prompts by learning from explicit examples, ultimately improving its\nperformance in generating accurate and contextually relevant outputs.\n3.2.3\nAPI integration\nAfter training our models and adding post-processing scripts, we exposed the pipeline using Flask API.\nThese endpoints will be integrated with the front-end dashboard and yield the list of sections and divisions\nin a provided specification document.\n3.2.4\nPresentation – Dashboard\nWe developed a dashboard to showcase our work using Next.js, a React framework that provides additional\nfeatures such as server-side rendering and the generation of static websites. React, a JavaScript library\ntraditionally employed for building web applications rendered in the client’s browser with JavaScript, is at\nthe core of Next.js. Despite its popularity, this approach poses several challenges. These include issues with\nusers who lack access to JavaScript or have disabled it, potential security concerns, significantly prolonged\npage loading times, and adverse effects on the site’s overall search engine optimization. Developers have\nrecognized these problems and seek solutions for a more inclusive and efficient web experience.\n3.3\nEvaluation Metrics\nAssessing the effectiveness of a machine learning model requires the application of suitable evaluation\nmetrics. In our scenario, we employ two distinct models: one designed for classification and another for\norganizing data within specified ToCs. The classification model’s performance is gauged using accuracy.\nMeanwhile, for the second model, tasked with extracting structured data, we assess accuracy by comparing\npredicted key information with the corresponding ground truth JSON data. Exact matches are considered\ncorrect, while discrepancies are deemed missed key information. This approach allows us to compute the\naccuracy of structured data generation.\n3.4\nResult and Discussion\nIn this section, we discuss the results obtained after fine-tuning the model as described in the experimental\nsetup. We evaluated the model using the above metrics and achieved an overall accuracy of 82.2% by\nrunning it on 20 test documents and the results are shown in Figure 4. The accuracy of detecting heading\nnumbers and titles is 90% and 81%, respectively. The accuracy for detecting subheading numbers and titles\nis 88% and 79%, respectively.\n7\n\nA PREPRINT - MARCH 13, 2024\nFigure 5: A prompt to extract key information based on a given example by formatting the output according\nto the provided schema.\n8\n\nA PREPRINT - MARCH 13, 2024\nHN\nHT\nSHN\nSHT\nAvrg\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.92\n0.78\n0.91\n0.76\n0.85\nAccuracy\nFigure 6: Peformance of Donut in detecting Heading number(HN), heading title(HT), subheading num-\nber(SHN), and subheading title(SHT).\nHN\nHT\nSHN\nSHT\nAvrg\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.92\n0.83\n0.88\n0.84\n0.89\nAccuracy\nFigure 7: Peformance of OpenAI GPT-3.5 Turbo in detecting Heading number(HN), heading title(HT),\nsubheading number(SHN), and subheading title(SHT).\n9\n\nA PREPRINT - MARCH 13, 2024\n4\nConclusion\nFailure to accurately structure information from large documents could create massive change orders, cost\noverruns, and schedule delays, which can negatively impact your bottom line. Large language models and\ncomputer vision are great tools to autonomously rearrange and categorize big documents like construction\nspecification documents which may include various information about different technical sectors like\narchitectural and mechanical work. By using an OCR-free document AI tool and OpenAI GPT-3.5 Turbo, we\nachieved an end-to-end tray that serves the list of sections and divisions in any specification document in a\nJSON.\n5\nFuture Work\nTo further improve the accuracy of the document extraction model, several future tasks can be pursued. One\nsuch task is to collect more comprehensive data from a wider range of sources. This data could be used to\nrefine the model’s understanding of patterns. Another potential future task is to explore the use of more\nadvanced machine learning algorithms, such as deep learning or reinforcement learning, to enhance the\nmodel’s predictive capabilities. These algorithms could enable the model to identify more complex patterns\nand relationships in the data and to make more accurate predictions based on these insights. Finally, ongoing\nmonitoring and evaluation of the model’s performance will be critical to ensuring its continued accuracy and\nrelevance over time. This could involve regular updates to the model’s training data and ongoing testing\nand validation to identify any potential weaknesses or areas for improvement. By pursuing these future\ntasks, the ML model can continue to evolve and improve its predictions.\nReferences\n[1] Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, and Sungrae Park. Bros:\nA pre-trained language model focusing on text and layout for better key information extraction from\ndocuments. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10767–10775,\n2022.\n[2] Emad Elwany, Allison Hegel, Marina Shah, Brendan Roof, Genevieve Peaslee, and Quentin Rivet.\nDeeperdive: The unreasonable effectiveness of weak supervision in document understanding a case\nstudy in collaboration with uipath inc. arXiv preprint arXiv:2208.08000, 2022.\n[3] Google. Document ai documentation, 2023. Last accessed 2 May 2023.\n[4] hyperscience. Document ai documentation, 2023. Last accessed 2 May 2023.\n[5] uipath. Document ai documentation, 2023. Last accessed 2 May 2023.\n[6] Filip Grali´nski, Tomasz Stanisławek, Anna Wróblewska, Dawid Lipi´nski, Agnieszka Kaliska, Paulina\nRosalska, Bartosz Topolski, and Przemysław Biecek. Kleister: A novel task for information extraction\ninvolving long documents with complex layout. arXiv preprint arXiv:2003.02356, 2020.\n[7] Tomasz Stanisławek, Filip Grali´nski, Anna Wróblewska, Dawid Lipi´nski, Agnieszka Kaliska, Paulina\nRosalska, Bartosz Topolski, and Przemysław Biecek. Kleister: key information extraction datasets\ninvolving long documents with complex layouts. In Document Analysis and Recognition–ICDAR 2021:\n16th International Conference, Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part I, pages\n564–579. Springer, 2021.\n[8] Weihong Lin, Qifang Gao, Lei Sun, Zhuoyao Zhong, Kai Hu, Qin Ren, and Qiang Huo. Vibertgrid: a\njointly trained multi-modal 2d document representation for key information extraction from documents.\nIn Document Analysis and Recognition–ICDAR 2021: 16th International Conference, Lausanne, Switzerland,\nSeptember 5–10, 2021, Proceedings, Part I 16, pages 548–563. Springer, 2021.\n[9] Cagrı Sayallar, Ahmet Sayar, and Nurcan Babalık. An ocr engine for printed receipt images using deep\nlearning techniques. International Journal of Advanced Computer Science and Applications, 14(2), 2023.\n[10] Jørgen Burchardt. Are searches in ocr-generated archives trustworthy? an analysis of digital newspaper\narchives. Jahrbuch für Wirtschaftsgeschichte/Economic History Yearbook, 64(1):31–54, 2023.\n[11] Aparna Manjunath Akanksh, Manjunath Sudhakar Nayak, Santhanam Nishith, Satish Nitin Pandit,\nShreyas Sunkad, Pratiba Deenadhayalan, and Shobha Gangadhara. Automated invoice data extraction\nusing image processing. IAES International Journal of Artificial Intelligence, 12(2):514, 2023.\n10\n\nA PREPRINT - MARCH 13, 2024\n[12] Chenxia Li, Ruoyu Guo, Jun Zhou, Mengtao An, Yuning Du, Lingfeng Zhu, Yi Liu, Xiaoguang Hu, and\nDianhai Yu. Pp-structurev2: A stronger document analysis system. arXiv preprint arXiv:2210.05391,\n2022.\n[13] Thibault Douzon, Stefan Duffner, Christophe Garcia, and Jérémy Espinas. Improving information\nextraction on business documents with specific pre-training tasks. In Document Analysis Systems: 15th\nIAPR International Workshop, DAS 2022, La Rochelle, France, May 22–25, 2022, Proceedings, pages 111–125.\nSpringer, 2022.\n[14] Felix Krieger, Paul Drews, and Burkhardt Funk. Automated invoice processing: Machine learning-based\ninformation extraction for long tail suppliers. Available at SSRN 4386107.\n[15] Mohamed Dhouib, Ghassen Bettaieb, and Aymen Shabou. Docparser: End-to-end ocr-free information\nextraction from visually rich documents. arXiv preprint arXiv:2304.12484, 2023.\n[16] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok\nHwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding\ntransformer. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27,\n2022, Proceedings, Part XXVIII, pages 498–517. Springer, 2022.\n[17] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774, 2023.\n[18] Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Yang,\nQingqing Dang, et al. Pp-ocr: A practical ultra lightweight ocr system. arXiv preprint arXiv:2009.09941,\n2020.\n[19] Chenxia Li, Weiwei Liu, Ruoyu Guo, Xiaoting Yin, Kaitao Jiang, Yongkun Du, Yuning Du, Lingfeng Zhu,\nBaohua Lai, Xiaoguang Hu, et al. Pp-ocrv3: More attempts for the improvement of ultra lightweight\nocr system. arXiv preprint arXiv:2206.03001, 2022.\n[20] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of\ntext and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, pages 1192–1200, 2020.\n[21] Adnan Ul-Hasan, Syed Saqib Bukhari, Faisal Shafait, and Thomas M Breuel. Ocr-free table of contents\ndetection in urdu books. In 2012 10th IAPR International Workshop on Document Analysis Systems, pages\n404–408. IEEE, 2012.\n[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[23] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint\narXiv:1909.08053, 2019.\n[24] Shreekant Mandvikar. Augmenting intelligent document processing (idp) workflows with contempo-\nrary large language models (llms). International Journal of Computer Trends and Technology, 71(10):80–91,\n2023.\n[25] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232, 2023.\n[26] Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar, Muham-\nmad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, et al. A survey on large language models:\nApplications, challenges, limitations, and practical usage. Authorea Preprints, 2023.\n[27] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023.\n[28] Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, and Xindong Wu. Chatgpt is not enough: Enhancing\nlarge language models with knowledge graphs for fact-aware language modeling. arXiv preprint\narXiv:2306.11489, 2023.\n[29] Cheonsu Jeong. A study on the implementation of generative ai services using an enterprise data-based\nllm application architecture. arXiv preprint arXiv:2309.01105, 2023.\n[30] CS Krishna. Prompt generate train (pgt): A framework for few-shot domain adaptation, alignment, and\nuncertainty calibration of a retriever augmented generation (rag) model for domain specific open book\nquestion-answering. arXiv preprint arXiv:2307.05915, 2023.\n11\n\nA PREPRINT - MARCH 13, 2024\n[31] Reza Yousefi Maragheh, Chenhao Fang, Charan Chand Irugu, Parth Parikh, Jason Cho, Jianpeng Xu,\nSaranyan Sukumar, Malay Patel, Evren Korpeoglu, Sushant Kumar, et al. Llm-take: Theme-aware\nkeyword extraction using large language models. pages 4318–4324, 2023.\n[32] Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng,\nand Enhong Chen. Large language models for generative information extraction: A survey. arXiv\npreprint arXiv:2312.17617, 2023.\n[33] Adam W Harley, Alex Ufkes, and Konstantinos G Derpanis. Evaluation of deep convolutional nets for\ndocument image classification and retrieval. In 2015 13th International Conference on Document Analysis\nand Recognition (ICDAR), pages 991–995. IEEE, 2015.\n[34] Stefan Larson, Gordon Lim, and Kevin Leach. On evaluation of document classifiers using rvl-cdip. In\nProceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics,\npages 2657–2670, 2023.\n[35] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar.\nIcdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference\non Document Analysis and Recognition (ICDAR), pages 1516–1520. IEEE, 2019.\n12",
    "pdf_filename": "The future of document indexing - GPT and Donut revolutionize table of content processing.pdf"
}