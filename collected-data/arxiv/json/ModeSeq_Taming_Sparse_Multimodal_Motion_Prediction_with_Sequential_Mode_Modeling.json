{
    "title": "ModeSeq Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling",
    "context": "Anticipating the multimodality of future events lays the foundation for safe autonomous driving. However, multi- modal motion prediction for traffic agents has been clouded by the lack of multimodal ground truth. Existing works pre- dominantly adopt the winner-take-all training strategy to tackle this challenge, yet still suffer from limited trajectory diversity and misaligned mode confidence. While some ap- proaches address these limitations by generating excessive trajectory candidates, they necessitate a post-processing stage to identify the most representative modes, a process lacking universal principles and compromising trajectory accuracy. We are thus motivated to introduce ModeSeq, a new multimodal prediction paradigm that models modes as sequences. Unlike the common practice of decoding mul- tiple plausible trajectories in one shot, ModeSeq requires motion decoders to infer the next mode step by step, thereby more explicitly capturing the correlation between modes and significantly enhancing the ability to reason about mul- timodality. Leveraging the inductive bias of sequential mode prediction, we also propose the Early-Match-Take- All (EMTA) training strategy to diversify the trajectories further. Without relying on dense mode prediction or rule- based trajectory selection, ModeSeq considerably improves the diversity of multimodal output while attaining satisfac- tory trajectory accuracy, resulting in balanced performance on motion prediction benchmarks. Moreover, ModeSeq nat- urally emerges with the capability of mode extrapolation, which supports forecasting more behavior modes when the future is highly uncertain. Handling the intricate uncertainty presented in the real world is one of the major hurdles in autonomous driving. One aspect of the uncertainty lies in the multimodal behav- ior of traffic agents, i.e., multiple instantiations of an agentâ€™s Scene Embedding 0.5 0.4 0.3 (a) Parallel mode modeling Scene Embedding 0.7 0.3 0.1 (b) Sequential mode modeling Figure 1. A comparison between parallel and sequential mode modeling. While parallel mode modeling (Fig. 1a) decodes mul- timodal trajectories in one shot, our sequential mode modeling (Fig. 1b) reasons about multiple plausible futures step by step, which captures the relationships between modes to avoid produc- ing indistinguishable trajectories and confidence scores. future may be compatible with a given observation of the past. Without characterizing the multimodal distribution of agent motions, autonomous vehicles may fail to interact with the surroundings in a safe and human-like manner. For this reason, advanced decision-making systems demand a motion predictor to forecast several plausible and represen- tative trajectories of critical agents [8, 16]. Although multimodality has long been the central topic studied in motion prediction, this problem has not been fundamentally solved owing to the unavailability of mul- timodal ground truth, i.e., only one possibility is observable in real-world driving data. To struggle with this dilemma, most existing works adopt the winner-take-all (WTA) strat- egy [15] for training [4, 6, 17, 27, 39, 46, 53, 54]. Under this strategy, only the best among all predicted trajectories will receive supervision signals for regression, while all the remaining will be masked in the training loss. Despite being the current standard practice in the research community, the WTA solution has been found to cause mode collapse eas- ily and produce indistinguishable trajectories [24, 34, 45], further confusing the learning of mode scoring [18]. As a remedy, some recent research intends to cover the ground- truth mode by generating a massive number of trajectory candidates [27, 39, 46], from which the most representa- tive ones are heuristically selected based on post-processing methods such as non-maximum suppression (NMS). How- 1 arXiv:2411.11911v1  [cs.LG]  17 Nov 2024",
    "body": "ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode\nModeling\nZikang Zhou1\nHengjian Zhou2\nHaibo Hu1\nZihao Wen1\nJianping Wang1\nYung-Hui Li3\nYu-Kai Huang4\n1City University of Hong Kong\n2Zhejiang University\n3Hon Hai Research Institute\n4Carnegie Mellon University\nAbstract\nAnticipating the multimodality of future events lays the\nfoundation for safe autonomous driving. However, multi-\nmodal motion prediction for traffic agents has been clouded\nby the lack of multimodal ground truth. Existing works pre-\ndominantly adopt the winner-take-all training strategy to\ntackle this challenge, yet still suffer from limited trajectory\ndiversity and misaligned mode confidence. While some ap-\nproaches address these limitations by generating excessive\ntrajectory candidates, they necessitate a post-processing\nstage to identify the most representative modes, a process\nlacking universal principles and compromising trajectory\naccuracy. We are thus motivated to introduce ModeSeq, a\nnew multimodal prediction paradigm that models modes as\nsequences. Unlike the common practice of decoding mul-\ntiple plausible trajectories in one shot, ModeSeq requires\nmotion decoders to infer the next mode step by step, thereby\nmore explicitly capturing the correlation between modes\nand significantly enhancing the ability to reason about mul-\ntimodality.\nLeveraging the inductive bias of sequential\nmode prediction, we also propose the Early-Match-Take-\nAll (EMTA) training strategy to diversify the trajectories\nfurther. Without relying on dense mode prediction or rule-\nbased trajectory selection, ModeSeq considerably improves\nthe diversity of multimodal output while attaining satisfac-\ntory trajectory accuracy, resulting in balanced performance\non motion prediction benchmarks. Moreover, ModeSeq nat-\nurally emerges with the capability of mode extrapolation,\nwhich supports forecasting more behavior modes when the\nfuture is highly uncertain.\n1. Introduction\nHandling the intricate uncertainty presented in the real\nworld is one of the major hurdles in autonomous driving.\nOne aspect of the uncertainty lies in the multimodal behav-\nior of traffic agents, i.e., multiple instantiations of an agentâ€™s\nScene Embedding\n0.5\n0.4\n0.3\n(a) Parallel mode modeling\nScene Embedding\n0.7\n0.3\n0.1\n(b) Sequential mode modeling\nFigure 1. A comparison between parallel and sequential mode\nmodeling. While parallel mode modeling (Fig. 1a) decodes mul-\ntimodal trajectories in one shot, our sequential mode modeling\n(Fig. 1b) reasons about multiple plausible futures step by step,\nwhich captures the relationships between modes to avoid produc-\ning indistinguishable trajectories and confidence scores.\nfuture may be compatible with a given observation of the\npast.\nWithout characterizing the multimodal distribution\nof agent motions, autonomous vehicles may fail to interact\nwith the surroundings in a safe and human-like manner. For\nthis reason, advanced decision-making systems demand a\nmotion predictor to forecast several plausible and represen-\ntative trajectories of critical agents [8, 16].\nAlthough multimodality has long been the central topic\nstudied in motion prediction, this problem has not been\nfundamentally solved owing to the unavailability of mul-\ntimodal ground truth, i.e., only one possibility is observable\nin real-world driving data. To struggle with this dilemma,\nmost existing works adopt the winner-take-all (WTA) strat-\negy [15] for training [4, 6, 17, 27, 39, 46, 53, 54]. Under\nthis strategy, only the best among all predicted trajectories\nwill receive supervision signals for regression, while all the\nremaining will be masked in the training loss. Despite being\nthe current standard practice in the research community, the\nWTA solution has been found to cause mode collapse eas-\nily and produce indistinguishable trajectories [24, 34, 45],\nfurther confusing the learning of mode scoring [18]. As a\nremedy, some recent research intends to cover the ground-\ntruth mode by generating a massive number of trajectory\ncandidates [27, 39, 46], from which the most representa-\ntive ones are heuristically selected based on post-processing\nmethods such as non-maximum suppression (NMS). How-\n1\narXiv:2411.11911v1  [cs.LG]  17 Nov 2024\n\never, such a post-processing step requires carefully tuning\nthe hyperparameters, e.g., the thresholds in NMS. Even if\nthe hyperparameters were well-tuned, they might not fit var-\nious scenarios with diverse road conditions, leading to infe-\nrior generalization. Moreover, performing dense mode pre-\ndiction followed by rule-based post-processing may signif-\nicantly sacrifice trajectory accuracy in practice [39], given\nthat correctly extracting the best trajectories from a large set\nof candidates is non-trivial.\nThe limitations of mainstream methods prompt us to\nseek an end-to-end solution that directly produces a sparse\nset of diverse, high-quality, and representative agent tra-\njectories, eliminating the need for dense mode prediction\nand heuristic mode selection. To begin with, we identify\na commonality of existing multimodal motion predictors,\nthat all trajectory modes are decoded in one shot, which we\ndub parallel mode modeling as depicted in Fig. 1a. De-\nspite its efficiency, this paradigm neglects the relationship\nbetween the predicted modes, hindering models from de-\ncoding diverse multimodal output. For anchor-free methods\nwith parallel mode modeling [27, 28, 46, 53, 54], the dis-\ntinction between the decoded trajectories depends entirely\non the difference in sub-network parameters, which can-\nnot be guaranteed under the unstable WTA training. For\nthis reason, some of these solutions turn to dense mode\nprediction and rely on post-processing steps to diversify\nthe output [27, 46].\nWhile anchor-based approaches of-\nfload the duties of ensuring diversity onto the input an-\nchors [4, 29, 39, 51], determining a sparse set of anchors\nthat can adapt to specific scenarios is challenging, com-\npelling all these approaches to employ excessive anchors\nfor dense mode prediction. Under the paradigm of paral-\nlel mode modeling, producing multimodal trajectories with\nsparse mode prediction faces significant obstacles.\nTo tackle these challenges, this paper explores sequential\nmode modeling (ModeSeq), a completely different path-\nway toward sparse multimodal motion prediction. As illus-\ntrated in Fig. 1b, we attempt to construct a chain of modes\nwhen decoding the future from the scene embeddings, pro-\nducing only one plausible trajectory and the corresponding\nconfidence at each decoding step. Compared with paral-\nlel prediction, our approach puts more effort into captur-\ning the correlation between modes, asking the model to tell\nwhat the next mode should be and how much confidence it\nhas conditioned on the mode embeddings at previous steps.\nBy giving the model a chance to look at the prior modes\nand learning the factorized joint latent space of multiple\nfutures, we tremendously boost the capability of reason-\ning about multimodality and characterizing the full distri-\nbution without the reliance on dense mode prediction, post-\nprocessing tricks, and all manner of anchors. To strengthen\nthe capacity of models under our new paradigm, we develop\nan iterative refinement framework similar to DETR-like de-\ncoders [3, 39, 52, 54], which is powered by reordering the\nmode embeddings in between decoding layers. Moreover,\nleveraging the order of modes in the sequence, we further\npropose the Early-Match-Take-All (EMTA) training strat-\negy, which can encourage the decoder to match the ground\ntruth as early as possible. Meanwhile, our EMTA scheme\nalso enforces the decoder to vacate the duplicated modes to\ncover the missing futures at the cost of negligible degrada-\ntion in trajectory accuracy, thereby achieving better mode\ncoverage and easing the learning of confidence scoring.\nWe validate ModeSeq on the Waymo Open Motion\nDataset [9] and the Argoverse 2 Motion Forecasting\nDataset [49], where we achieve more balanced performance\nin terms of mode coverage, mode scoring, and trajectory\naccuracy, compared with representative motion forecasting\nmethods such as QCNet [54] and MTR [39]. Furthermore,\nour approach naturally emerges with the capability of mode\nextrapolation thanks to sequential mode modeling, which\nenables predicting a dynamic number of modes on demand.\n2. Related Work\nMultimodality has been a dark cloud in the field of mo-\ntion prediction. Early works employ generative models to\nsample multimodal trajectories [11, 13, 14, 32, 44], but\nthey are susceptible to mode collapse. Modern motion pre-\ndictors [4, 6, 17, 27, 39, 46, 53, 54] mostly follow the\nparadigm of multiple choice learning [15], where multiple\ntrajectory modes are produced directly from mixture den-\nsity networks [2]. Due to the lack of multimodal ground\ntruth, these methods adopt the WTA training strategy [15],\nwhich is unstable and fails to deal with mode collapse fun-\ndamentally [24, 34, 45].\nTo mitigate this issue, a line\nof research performs dense mode prediction, i.e., decod-\ning excessive trajectory candidates for better mode cover-\nage [27, 39, 46]. Among these works, some equip the de-\ncoder with anchors [4, 29, 39, 51] to achieve more stable\ntraining. However, dense mode prediction necessitates the\nrule-based selection of the most representative trajectories\nfrom a large set of candidates, risking the precision of tra-\njectories and the generalization ability across a wide range\nof scenarios. This paper provides new insights into multi-\nmodal problems by introducing the paradigm of sequential\nmode modeling and the EMTA training strategy, pursuing\nan end-to-end solution that produces a sparse set of diverse,\nhigh-quality, and representative trajectories directly.\nSequential modeling has found many applications in mo-\ntion prediction and traffic modeling. On the one hand, ap-\nplying sequential modeling to the time dimension results\nin trajectory encoders and decoders based on recurrent net-\nworks [1, 5, 7, 11, 12, 14, 26, 31, 32, 35, 44, 46] or Trans-\nformers [10, 21, 27, 28, 30, 36, 47, 50, 53â€“55], which can\nfacilitate the learning of temporal dynamics.\nIn particu-\nlar, recent advances in motion generation [30, 36, 55] have\n2\n\nshown that factorizing the joint distribution of multi-agent\ntime-series in a social autoregressive manner [1, 32, 44] can\nbetter characterize the evolution of traffic scenarios. On the\nother hand, some works utilize sequential modeling in the\nagent dimension for multi-agent motion prediction [33, 43].\nFor example, M2I [43] uses heuristic methods to label influ-\nencers and reactors from pairs of agents, followed by pre-\ndicting the marginal distribution of the influencers and the\nconditional distribution of the reactors. FJMP [33] extends\nM2I to model the joint distribution of an arbitrary number of\nagents, where the joint future trajectories of agents are fac-\ntorized using a directed acyclic graph. Our work is the first\nattempt that employs sequential prediction in the mode di-\nmension, which enhances the understanding of multimodal\nbehavior by capturing the correlation between modes.\n3. Methodology\n3.1. Problem Formulation\nDenote S as the input of motion prediction models, which\nencompasses the map elements represented as M polygo-\nnal instances and the T-step historical trajectories of A traf-\nfic agents (e.g., vehicles, pedestrians, and cyclists) in the\nscene. The models are tasked with forecasting K plausible\ntrajectory modes per agent of interest, each comprising Ë†T\nwaypoints and an associated confidence score. These tra-\njectories are desired to be representative, reflecting distinct\nbehavior modes of agents and properly measuring the like-\nlihood of each mode via the estimated confidence.\nTypical motion predictors employ the encoder-decoder\narchitecture, where an encoder computes the embedding\nÎ¨ from the scene input, based on which a decoder learns\nthe embeddings {mi,k}kâˆˆ{1,...,K} of the i-th agentâ€™s future\nmodes. Without loss of generality, the following simplifies\nmi,k as mk to discuss the prediction for a single agent,\nwhich can be extended for multiple agents by repeating the\nsame decoding process. Given mk, a prediction head then\noutputs a trajectory Ë†yk = [Ë†y1\nk, . . . , Ë†y Ë†T\nk ] and a confidence\nscore Ë†Ï•k via simple modules such as multi-layer percep-\ntrons (MLPs). The whole pipeline can be summarized as\nï£±\nï£´\nï£²\nï£´\nï£³\nÎ¨ = Encoder (S) ,\n{mk}kâˆˆ{1,...,K} = Decoder (Î¨) ,\nË†yk, Ë†Ï•k = Head (mk) ,\nk âˆˆ{1, . . . , K} .\n(1)\n3.2. Motivation\nPrior works formulate multimodal decoding as a prob-\nlem of set prediction [3, 48]. For instance, most cutting-\nedge methods employ DETR-like decoders [3] to produce\nthe joint embeddings of multiple modes from learnable or\nanchor-based queries that are permutation-equivariant [27,\n39, 46, 54].\nIdeally, the joint mode embeddings should\nbe supervised by the ground-truth multimodal distribution,\nakin to the application of set prediction in object detec-\ntors [3, 20, 25, 56] where each object query receives su-\npervision signals via optimal bipartite matching. However,\nreal-world driving data contains only one instantiation of\nscene evolution, which compels motion prediction solutions\nto adopt the winner-take-all (WTA) matching [15]. As a re-\nsult, only the mode embeddings of the best-predicted tra-\njectories get optimized, which will easily degenerate the\nmodels into learning multiple mode embeddings indepen-\ndently. The inability to jointly optimize all modes explains\nwhy DETR-like motion decoders fail to avoid duplicated\ntrajectories without the use of non-maximum suppression\n(NMS), given that gathering multiple predicted trajectories\naround the most probable regions is expected to achieve\nlower training loss when only one ground-truth future is\npresented during training.\nTo facilitate reasoning about multimodality in the ab-\nsence of multimodal ground truth, our ModeSeq framework\nrequires the decoder to conduct chain-based factorization\non the joint embeddings, which is demonstrated as follows:\nmt = Decoder\n\u0000Î¨, {mk}kâˆˆ{1,...,tâˆ’1}\n\u0001\n, t âˆˆ{1, . . . , K} .\n(2)\nWith such a factorization that converts the unordered set of\nmodes into a sequence, the correlation between modes can\nbe naturally strengthened, as the mode to be decoded de-\npends on the ones that appear previously. Further equipping\nthis framework with appropriate model implementation and\ntraining scheme has the potential to offer better mode cov-\nerage and scoring without severe sacrifice in trajectory ac-\ncuracy, which we introduce in the following sections.\n3.3. Scene Encoding\nSince this work focuses on the decoding of multimodal tra-\njectories, we simply adopt QCNet [54] as the scene en-\ncoder, which is one of the de facto best practices in industry\nand academia due to its symmetric modeling in space and\ntime leveraging relative positional embeddings [37]. This\nencoder, on the one hand, exploits a hierarchical map en-\ncoding module based on map-map self-attention to produce\nthe map embedding of shape [M, D], with D referring to\nthe hidden size. On the other hand, the encoder consists\nof Transformer modules that factorize the space and time\naxes, including temporal self-attention, agent-map cross-\nattention, and agent-agent self-attention. These three types\nof attention are grouped and interleaved twice to yield the\nagent embedding of shape [A, T, D], which constitutes the\nfinal scene embeddings together with the map embedding.\nIn principle, any scene encoding method can fit into our\nModeSeq framework with reasonable efforts.\n3.4. Single-Layer Mode Sequence\nThis section illustrates the detailed structure of a single\nModeSeq layer, which consists of a Memory Transformer\n3\n\nScene Embedding\nA\nT\nM\nContext Transformer\nMemory \nTransformer\nK\nV\nQ\nQ\nK V\nPush\nMLP\n0.2\n[ð‘š!\n(!), ð‘š$\n(!)]\nð‘š%\n(&)\nð‘š%\n(!)\nMode Rearrangement\nModeSeq Layer 1\nModeSeq Layer 2\nScene Embedding\n0.2\n0.4\n0.5\n0.2\n0.5\n0.4\n0.0\n0.0\n0.0\n0.1\n0.3\n0.7\n0.4\n0.5\n0.0\nEMTA Loss\nEMTA Loss\n0.x\nMode Confidence\nMode Embedding\nMap Embedding\nAgent Embedding\nFigure 2. Overview of the ModeSeq framework. Left: We stack multiple ModeSeq layers with mode rearrangement in between to\niteratively refine the multimodal output under the Early-Match-Take-All (EMTA) training strategy. Right: Each ModeSeq layer consists\nof a Memory Transformer module for capturing mode-wise dependencies and a Context Transformer module for retrieving the scene\nembeddings produced by the encoder, operating in a recurrent fashion to decode a sequence of trajectory modes.\nmodule and a Context Transformer module. Stacking mul-\ntiple ModeSeq layers can further improve the performance\nvia iterative refinement, which will be discussed in Sec. 3.5.\nRoutine. We introduce the decoding procedure of the â„“-\nth ModeSeq layer, with the right-hand side of Fig. 2 de-\npicting the first layer (i.e., â„“= 1).\nThe layer is de-\nsigned to recurrently output a sequence of mode embed-\ndings [m(â„“)\n1 , . . . , m(â„“)\nK ], where we slightly complicate the\nnotation of modes with a superscript that identifies the\nlayer index.\nThe input of the layer includes the scene\nembedding Î¨ yielded by the encoder and the mode em-\nbeddings produced by the (â„“âˆ’1)-th decoding layer, i.e.,\n[m(â„“âˆ’1)\n1\n, . . . , m(â„“âˆ’1)\nK\n]. Since the latter does not apply to\nthe first layer, we introduce an embedding e âˆˆRD to serve\nas the output of the â€œ0-th layerâ€, which is randomly initial-\nized at the beginning of training. The same learnable e is\nshared across the K input embeddings of the first layer:\nm(0)\nk\n= e, k âˆˆ{1, . . . , K} .\n(3)\nBefore starting the decoding, we create an empty sequence\nâ„¦(â„“)\n0\n= [ ] with the subscript and superscript indicating the\n0-th decoding step and the â„“-th layer, respectively. This se-\nquence will be used to keep track of the mode embeddings\nproduced at various steps. At the t-th decoding step, we em-\nploy a Memory Transformer module and a Context Trans-\nformer module to update m(â„“âˆ’1)\nt\nto become m(â„“)\nt , lever-\naging the information in the memory bank â„¦(â„“)\ntâˆ’1 and the\nscene embedding Î¨. The output mode embedding m(â„“)\nt\nis then pushed to the end of the sequence â„¦(â„“)\ntâˆ’1 to obtain\nâ„¦(â„“)\nt\n= [m(â„“)\n1 , . . . , m(â„“)\nt ], which will serve as the input at\nthe (t + 1)-th decoding step. After going through K decod-\ning steps, we use a prediction head to transform each of the\nmode embeddings stored in â„¦(â„“)\nK into a specific trajectory\nand a corresponding confidence score via MLPs. The fol-\nlowing paragraphs detail the modules constituting a Mode-\nSeq layer and discuss the differences between our approach\nand other alternatives.\nMemory Transformer. The Memory Transformer takes\ncharge of modeling the sequential dependencies of trajec-\ntory modes. At the t-th decoding step of the â„“-th layer, this\nmodule takes as input â„¦(â„“)\ntâˆ’1 and m(â„“âˆ’1)\nt\n, the memory bank\nfor the current layer and the t-th mode embedding derived\nfrom the last layer. Since we desire the generation of the\nt-th mode embedding m(â„“)\nt\nto be aware of the existence of\nthe preceding modes, we treat m(â„“âˆ’1)\nt\nas the query of the\nTransformer module, which retrieves the memory bank in a\ncross-attention manner:\nË†m(â„“)\nt\n= MemFormer\n\u0010\nquery=m(â„“âˆ’1)\nt\n, key/value=â„¦(â„“)\ntâˆ’1\n\u0011\n.\n(4)\nIn this way, the information in â„¦(â„“)\ntâˆ’1 is assimilated into\nm(â„“âˆ’1)\nt\nto produce Ë†m(â„“)\nt , which is a query feature condi-\ntioned on the modes up to the (t âˆ’1)-th decoding step.\nContext Transformer. To derive scene-compliant modes,\nwe must provide the query feature with a specific scene con-\ntext. To this end, we use the Context Transformer module\nto refine the conditional query Ë†m(â„“)\nt\nwith the scene embed-\ndings output by the encoder. Specifically, the t-th mode em-\nbedding m(â„“)\nt\nis computed by enriching Ë†m(â„“)\nt\nwith Î¨ using\ncross-attention:\nm(â„“)\nt\n= CtxFormer\n\u0010\nquery = Ë†m(â„“)\nt , key/value = Î¨\n\u0011\n.\n(5)\nConsidering the high complexity of performing global at-\ntention, we decompose the Context Transformer into three\nseparate modules in practice, including mode-time cross-\nattention, mode-map cross-attention, and mode-agent cross-\nattention, each of which takes as input only a subset of the\n4\n\nembeddings contained in Î¨. First, the mode-time cross-\nattention fuses the query feature with the historical encod-\ning belonging to the agent of interest, enabling the query to\nadapt to the specific agent. Second, we aggregate the map\ninformation surrounding the agent of interest into the query\nfeature leveraging the mode-map cross-attention, which\ncontributes to the map compliance of the forecasting results.\nFinally, utilizing the mode-agent cross-attention module to\nfuse the neighboring agentsâ€™ latest embeddings promotes\nthe modelâ€™s social awareness. After going through these\nthree modules, the conditional query Ë†m(â„“)\nt\neventually be-\ncomes m(â„“)\nt , which is now context-aware.\nPrediction Head.\nGiven the conditional, context-aware\nmode embedding m(â„“)\nt , we use an MLP head to output the\nt-th trajectory Ë†y(â„“)\nt\nand another to estimate the correspond-\ning confidence score Ë†Ï•(â„“)\nt :\nï£±\nï£´\nï£²\nï£´\nï£³\nË†y(â„“)\nt\n= MLP\n\u0010\nm(â„“)\nt\n\u0011\n,\nË†Ï•(â„“)\nt\n= MLP\n\u0010\nm(â„“)\nt\n\u0011\n.\n(6)\nComparison with DETR-Like Decoders. In contrast to\nmotion decoders [27, 39, 46, 54] inspired by DETR [3],\nwhere the relationships between modes are completely ne-\nglected [27, 46] or weakly modeled by mode-mode self-\nattention [39, 54], modeling modes as a sequence strength-\nens mode-wise relational reasoning thanks to the condi-\ntional dependence in generating multimodal embeddings,\nwhich is beneficial to eliminating duplicated trajectories.\nFurthermore, it is worth noting that DETR-like approaches\ncan only decode a fixed number of modes, as the number of\nlearnable/static anchors cannot be changed once specified at\nthe start of training. By contrast, our ModeSeq framework\nsupports decoding more/less modes at test time, which can\nbe simply achieved by changing the number of decoding\nsteps. This characteristic can be helpful since the degree of\nuncertainty varies by scenario.\nComparison with Typical Recurrent Networks.\nThe\nModeSeq layer can be viewed as a sort of recurrent net-\nwork [5, 12] due to its parameter sharing across all decod-\ning steps, though its core components are modernized with\nTransformers to achieve impressive performance.\nWhile\ntypical recurrent networks compress the memory into a sin-\ngle hidden state, which is often lossy, the Memory Trans-\nformer inside a ModeSeq layer allows for direct access to\nall prior mode embeddings, naturally scaling the capacity\nof the memory as the number of modes grows.\n3.5. Multi-Layer Mode Sequences\nSingle-layer mode sequences may have limited capability\nof learning high-quality mode representations. In particu-\nlar, if the layer happens to produce unrealistic or less likely\nmodes at the first few decoding steps, the learning of the\nlater modes may be unexpectedly disturbed. Inspired by\nDETR [3], we develop an iterative refinement framework\nby stacking multiple ModeSeq layers and applying training\nlosses to the output of each layer. As shown in the left part\nof Fig. 2, all layers except for the first one take as input\nthe mode embeddings output from the last round of decod-\ning, refining the features with the scene context. Crucially,\nwe introduce the operation of mode rearrangement in be-\ntween layers, which corrects the order of the embeddings in\nthe mode sequence to encourage decoding trajectory modes\nwith monotonically decreasing confidence scores.\nMode Rearrangement. Before transitioning from the â„“-\nth to the (â„“+ 1)-th ModeSeq layer, we sort the mode em-\nbeddings stored in the memory bank â„¦(â„“)\nK according to the\ndescending order of the confidence scores predicted from\nthem. The sorted mode embeddings will then be sequen-\ntially input to the (â„“+1)-th ModeSeq layer for recurrent de-\ncoding. Through iterative refinement with mode rearrange-\nment, the trajectories and the order of modes become more\nscene-compliant and more reasonable, respectively.\n3.6. Early-Match-Take-All Training\nThe WTA training strategy [15] is blamed for produc-\ning overlapped trajectories and indistinguishable confidence\nscores [18, 24, 34, 45]. Fortunately, our approach has the\nopportunity to opt for a more advanced training method\nthanks to the paradigm of sequential mode modeling. In\nthis section, we propose the EMTA loss, which leverages\nthe order of modes to define the positive and negative sam-\nples toward better mode coverage and confidence scoring\nwithout significantly sacrificing trajectory accuracy.\nTypical WTA loss optimizes only the trajectory with the\nminimum displacement error with respect to the ground\ntruth. In comparison, our EMTA loss optimizes the matched\ntrajectory decoded at the earliest recurrent step. For exam-\nple, if both the second and the third trajectories match the\nground truth, only the second one will be optimized, regard-\nless of which one has the minimum error. To this end, we\nsearch over the K predictions to acquire the collection of\nmode indices associated with matched trajectories:\nG(â„“) =\nn\nk | k âˆˆ{1, Â· Â· Â· , K} âˆ§1\nn\nIsMatch\n\u0010\nË†y(â„“)\nk , y\n\u0011oo\n,\n(7)\nwhere G(â„“) denotes the set of qualified mode indices in\nthe â„“-th ModeSeq layer, 1{Â·} represents the indicator func-\ntion, and IsMatch(Â·, Â·) defines the criterion for a match\ngiven the ground-truth trajectory y. The implementation\nof IsMatch(Â·, Â·) can be flexible, depending on the trajec-\ntory accuracy demanded by practitioners. For instance, on\nthe Waymo Open Motion Dataset [9], we decide whether a\npredicted trajectory is a match based on the velocity-aware\ndistance thresholds defined in the Miss Rate metric of the\nbenchmark; while on the Argoverse 2 Motion Forecasting\nDataset [49], a matched trajectory is expected to have less\n5\n\nDataset\nMethod\nEnsemble\nLidar\nSoft mAP6 â†‘\nmAP6 â†‘\nMR6 â†“\nminADE6 â†“\nminFDE6 â†“\nVal\nMTR v3 [38]\nÃ—\nâœ“\n-\n0.4593\n0.1175\n0.5791\n1.1809\nMTR++ [40]\nÃ—\nÃ—\n-\n0.4382\n0.1337\n0.6031\n1.2135\nQCNet [54]\nÃ—\nÃ—\n0.4508\n0.4452\n0.1254\n0.5122\n1.0225\nModeSeq (Ours)\nÃ—\nÃ—\n0.4562\n0.4507\n0.1206\n0.5237\n1.0681\nTest\nMTR v3 [38]\nâœ“\nâœ“\n0.4967\n0.4859\n0.1098\n0.5554\n1.1062\nModeSeq (Ours)\nâœ“\nÃ—\n0.4737\n0.4665\n0.1204\n0.5680\n1.1766\nRMP Ensemble [42]\nâœ“\nÃ—\n0.4726\n0.4553\n0.1113\n0.5596\n1.1272\nTable 1. Quantitative results on the 2024 Waymo Open Dataset Motion Prediction Benchmark.\nMethod\nEnsemble b-minFDE6 â†“MR6 â†“minADE6 â†“minFDE6 â†“\nMTR [39]\nâœ“\n1.98\n0.15\n0.73\n1.44\nMTR++ [40]\nâœ“\n1.88\n0.14\n0.71\n1.37\nQCNet [54]\nÃ—\n1.91\n0.16\n0.65\n1.29\nModeSeq (Ours)\nÃ—\n1.87\n0.14\n0.63\n1.26\nTable 2.\nQuantitative results on the 2024 Argoverse 2 Single-\nAgent Motion Forecasting Benchmark.\nthan 2-meter final displacement error. Given G(â„“), we de-\ntermine the unique positive sampleâ€™s index Ë†k(â„“) as follows,\nwith all the remaining modes treated as negative samples:\nË†k(â„“) =\nï£±\nï£²\nï£³\nmin\nkâˆˆG(â„“) k\nif |G(â„“)| > 0 ;\nargmin\nk\nDist\n\u0010\nË†y(â„“)\nk , y\n\u0011\notherwise ,\n(8)\nwhere |Â·| denotes the cardinality of a set, and Dist(Â·, Â·) mea-\nsures the average displacement error between trajectories.\nThis strategy for label assignment encourages the model to\ndecode matched trajectories as early as possible by treating\nthe earliest instead of the best matches as positive samples.\nMeanwhile, it drives the later matches, if any, away from the\nground truth by assigning negative labels to them. On the\nother hand, if none of the predictions match, which com-\nmonly happens at the early stage of training, we will fall\nback to the regular WTA scheme to ease the difficulty in op-\ntimization. Following label assignment, we use the Laplace\nnegative log-likelihood [53, 54] as the regression loss, opti-\nmizing the trajectories of the positive samples. Besides, we\nuse the Binary Focal Loss [19] to optimize the confidence\nscores according to the labels assigned. We also try a vari-\nant of confidence loss, where we introduce the definition of\nignored samples to mask the loss of the modes decoded ear-\nlier than the positive samples, which is shown to be effective\nin the absence of mode rearrangement.\n4. Experiments\n4.1. Experimental Setup\nDatasets. We conduct experiments on the Waymo Open\nMotion Dataset (WOMD) [9] and the Argoverse 2 Mo-\ntion Forecasting Dataset [49].\nThe WOMD contains\n486995/44097/44920 training/validation/testing samples,\nwhere the history of 1.1 seconds is provided as the con-\ntext and the 8-second future trajectories of up to 8 agents\nare required to predict.\nThe Argoverse 2 dataset com-\nprises 199908/24988/24984 samples with 5-second obser-\nvation windows and 6-second prediction horizons for train-\ning/validation/testing.\nMetrics. Following the standard of the benchmarks [9, 49],\nwe constrain models to output at most K = 6 trajecto-\nries. We use Miss Rate (MRK) to measure mode cover-\nage, which counts the fraction of cases in which the model\nfails to produce any trajectories that match the ground truth\nwithin the required thresholds. Built upon the definition of\na match, mAPK and Soft mAPK assess the precision of the\nconfidence scores by computing the P/R curves and aver-\naging the precision values over various confidence thresh-\nolds. To further evaluate trajectory quality, we use mini-\nmum Average Displacement Error (minADEK) and mini-\nmum Final Displacement Error (minFDEK) as indicators,\nwhich calculate the distance between the ground truth and\nthe best-predicted trajectories as an average over the whole\nhorizon and at the final time step, respectively. Besides,\nthe b-minFDEK concerns the joint performance of trajecto-\nries and confidences by summing the minFDEK and Brier\nscores of the best-predicted trajectories.\nImplementation Details. We develop models with a hid-\nden size of 128. The decoder stacks 6 layers for iterative\nrefinement, with each layer executing 6 steps to obtain ex-\nactly 6 modes as required by the benchmarks [9, 49]. On\nthe WOMD [9], we use the AdamW optimizer [23] to train\nmodels for 30 epochs on the training set with a batch size of\n32, a weight decay rate of 0.1, and a dropout rate of 0.1. On\nArgoverse 2 [49], we use a similar training configuration\nexcept that the number of epochs is extended to 64. The\ninitial learning rate is set to 5Ã—10âˆ’4, which is decayed to 0\nat the end of training following the cosine annealing sched-\nule [22]. Unless specified, the ablation studies are based on\nexperiments on the WOMD with 20% of the training data.\n4.2. Comparison with State of the Art\nWe compare our approach with QCNet [54] and the MTR\nseries [38â€“40], which are currently the most effective\nsparse and dense multimodal prediction solutions across\nthe WOMD and the Argoverse 2 dataset. As demonstrated\nin Tab. 1, ModeSeq achieves the best scoring performance\namong the Lidar-free methods on the validation split of the\nWOMD, though it lags behind the mAP6 performance of\nMTR v3 [38], a model that augments the input information\nwith raw sensor data. As a sparse mode predictor, ModeSeq\n6\n\nDecoder\nTraining Strategy\nIgnored Samples\nSoft mAP6 â†‘\nmAP6 â†‘\nMR6 â†“\nminADE6 â†“\nminFDE6 â†“\nDETR w/ Refinement\nWTA\nNone\n0.4096\n0.4050\n0.1536\n0.5660\n1.1716\nOther Matches\n0.4150\n0.4103\n0.1502\n0.5619\n1.1621\nModeSeq (Ours)\nWTA\nNone\n0.4138\n0.4093\n0.1502\n0.5563\n1.1498\nOther Matches\n0.4207\n0.4161\n0.1503\n0.5556\n1.1501\nEMTA\nNone\n0.4231\n0.4196\n0.1457\n0.5700\n1.1851\nOther Matches\n0.4098\n0.4060\n0.1496\n0.5817\n1.2207\nTable 3. Effects of sequential mode modeling and Early-Match-Take-All training on the validation set of the WOMD.\nMode Rearrangement\nIgnored Samples\nSoft mAP6 â†‘\nmAP6 â†‘\nMR6 â†“\nminADE6 â†“\nminFDE6 â†“\nÃ—\nNone\n0.4112\n0.4077\n0.1548\n0.5884\n1.2389\nEarly Mismatches\n0.4141\n0.4109\n0.1489\n0.5749\n1.2066\nâœ“\nNone\n0.4231\n0.4196\n0.1457\n0.5700\n1.1851\nEarly Mismatches\n0.4161\n0.4129\n0.1461\n0.5751\n1.2041\nTable 4. Effects of mode rearrangement on the validation set of the WOMD.\n1\n2\n3\n4\n5\n6\nDecoder Layer\n0.38\n0.39\n0.40\n0.41\n0.42\n0.43\nSoft mAP\n1\n2\n3\n4\n5\n6\nDecoder Layer\n0.14\n0.15\n0.16\n0.17\n0.18\nMR\nFigure 3. The performance after each decoding layer on the vali-\ndation set of the WOMD.\nundoubtedly outperforms MTR++ [40] in terms of MR6,\nminADE6, and minFDE6 by a large margin. Compared with\nQCNet [54], ModeSeq attains better Soft mAP6, mAP6,\nand MR6 at the cost of slight degradation on minADE6 and\nminFDE6, confirming that our approach can improve the\nmode coverage and confidence scoring of sparse predictors\nwithout significant sacrifice in trajectory accuracy. As of\nthe time we submitted the results to the benchmark, the en-\nsemble version of ModeSeq ranked first among Lidar-free\napproaches on the test set of the WOMD. Our approach also\nexhibits promising performance on the Argoverse 2 dataset,\nwhere our ensemble-free model surpasses QCNet and the\nMTR series on all critical metrics as shown in Tab. 2.\n4.3. Ablation Study\nEffects of Sequential Mode Modeling. In Tab. 3, we ex-\namine the effectiveness of sequential mode modeling by\ncomparing ModeSeq with the sparse DETR-like decoder\nenhanced with iterative refinement [3], both employing the\nsame QCNet encoder [54] for fair comparisons. The results\ndemonstrate that ModeSeq outperforms the baseline on all\nmetrics when using the same training strategy. Interestingly,\nignoring the confidence loss of the suboptimal modes that\nmatch the ground truth can improve the performance of both\nmethods under the WTA training. The reason behind this is\nthat treating the other matched modes as negative samples\nwill confuse the optimization process, given that the best\nand the other matches usually have similar mode represen-\ntations while they are assigned as opposite samples.\nEffects of EMTA Training. We also investigate the role\nof EMTA training in Tab. 3. After replacing the WTA loss\nwith our EMTA scheme, the results on Soft mAP6, mAP6,\nand MR6 are considerably improved, which demonstrates\nthe benefits of EMTA training in terms of mode cover-\nage and confidence scoring. On the other hand, the per-\nformance on minADE6 and minFDE6 slightly deteriorates\nsince the EMTA loss has relaxed the requirement for tra-\njectory accuracy, but the degree of deterioration falls within\nan acceptable extent, leading to more balanced performance\ntaken overall. Moreover, contrary to the conclusion drawn\nfrom the WTA baselines, treating other matches as ignored\nsamples is detrimental under the EMTA strategy. This is\nbecause the joint effects of sequential mode decoding and\nEMTA training have broken the symmetry of mode mod-\neling and label assignment, allowing us to assign the other\nmatches as negative samples to drive them away from the\nground truth for covering other likely modes.\nEffects of Iterative Refinement. To understand the effects\nof iterative refinement under our framework, we take the\noutput from different decoding layers for evaluation. As\nshown in Fig. 3, the performance on Soft mAP6 and MR6\nis generally improved as the depth increases, totaling a sub-\nstantial enhancement between the first and the last layer.\nOne of the reasons why iterative refinement works well can\nbe attributed to the operation of mode rearrangement in be-\ntween layers, which we explain in the following.\nEffects of Mode Rearrangement. We study the effects of\nmode rearrangement in Tab. 4. Comparing the first and third\nrows of the table, we can see that reordering the mode em-\nbeddings before further refinement can remarkably promote\nthe forecasting capability. To gain deeper insights into the\nresults, we develop a variant of label assignment, where the\nmodes decoded earlier than the first match are deemed ig-\nnored samples. We found this strategy to outperform the de-\n7\n\n(a) #Mode@Training=3, #Mode@Inference=3\n(b) #Mode@Training=6, #Mode@Inference=6\n(c) #Mode@Training=6, #Mode@Inference=24\nFigure 4. Visualization on the WOMD. The agents in purple are predicted with blue trajectories, with the opacity indicating confidence.\nfault one in the absence of mode rearrangement, while the\nconclusion reverses when we reorder the modes in between\nlayers. This phenomenon can be explained by the fact that\nbad modes may appear in the first few decoding steps of the\nshallow layers, which can negatively impact the learning of\nthe subsequent modes. By manually putting the less confi-\ndent modes to the end of the sequence, we enable the model\nto prioritize the refinement of the more probable trajectories\nin the next layer. Without rearrangement, we have to inten-\ntionally assign monotonically decreasing labels by blocking\nthe training loss of the early mismatches, aiming at implic-\nitly guiding the model to output more confident modes first.\nCapability of Representative Mode Learning.\nWe\ndemonstrate ModeSeqâ€™s ability to produce representative\nmodes in Tab. 5. While training models to decode merely 3\nmodes necessarily leads to worse performance, the 3-mode\nvariant of ModeSeq achieves the same level of performance\non Soft mAP6 and mAP6 compared with the 6-mode model.\nBy comparison, QCNet [54] fails to achieve comparable re-\nsults if only using 3 mode queries during training.\nCapability of Mode Extrapolation.\nWe ask the model\ntrained by generating 6 modes to execute more decoding\nsteps at test time. As depicted in Fig. 5, ModeSeq achieves\nlower prediction error with the increase of the decoded\nmodes, emerging with the capability of mode extrapolation\nthanks to sequential modeling. This characteristic enables\nhandling various degrees of uncertainty across scenarios.\n4.4. Qualitative Results\nThe qualitative results produced by ModeSeq are presented\nin Fig. 4. Figure 4a demonstrates that our model can gener-\nate representative trajectories when being trained to decode\nonly 3 modes. Comparing Fig. 4c with Fig. 4b, we can see\nthat the 6-mode model successfully extrapolates diverse yet\nrealistic modes when executing 24 decoding steps during in-\nference, showcasing the extrapolation ability of ModeSeq.\n5. Conclusion\nThis paper introduces ModeSeq, a modeling framework that\nachieves sparse multimodal motion prediction via sequen-\nModel\n#Mode\nSoft mAP6 â†‘mAP6 â†‘MR6 â†“\nTraining Inference\nQCNet [54]\n3\n3\n0.4214\n0.4163\n0.2007\n6\n6\n0.4508\n0.4452\n0.1254\nModeSeq (Ours)\n3\n3\n0.4509\n0.4479\n0.1967\n6\n6\n0.4562\n0.4507\n0.1206\nTable 5. Capability of generating representative modes with pre-\ncise confidence scores. Models are trained on 100% training data\nand evaluated on the validation split of the WOMD.\n612 24\n48\n96\n#Mode@Inference\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\nminFDE\n612 24\n48\n96\n#Mode@Inference\n0.23\n0.26\n0.29\n0.32\n0.35\n0.38\nMR\nFigure 5. The results of generating more than 6 modes on the\nvalidation set of the WOMD.\ntial mode modeling. The framework comprises a mecha-\nnism of sequential multimodal decoding, an architecture of\niterative refinement with mode rearrangement, and a train-\ning strategy of Early-Match-Take-All label assignment. As\nan alternative to the unordered multimodal decoding and the\nwinner-take-all training strategy, ModeSeq achieves state-\nof-the-art results on motion prediction benchmarks and ex-\nhibits the characteristic of mode extrapolation, creating a\nnew path to solving multimodal problems.\nLimitations. Our approach is still flawed in some respects.\nFirst, the sequential generation of modes is less efficient\nthan one-shot predictions, which necessitates an improve-\nment in efficiency.\nSecond, although our approach sup-\nports multi-agent forecasting in parallel, we only explore\nmarginal multi-agent prediction, lacking validation on joint\nprediction. Future research may involve extending Mode-\nSeq into a joint multi-agent model.\nAcknowledgement.\nThis project is supported by a\ngrant from Hong Kong Research Grant Council un-\nder\nGRF\nproject\n11216323\nand\nCRF\nC1042-23G.\n8\n\nReferences\n[1] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan,\nAlexandre Robicquet, Li Fei-Fei, and Silvio Savarese. So-\ncial lstm: Human trajectory prediction in crowded spaces. In\nCVPR, 2016. 2, 3\n[2] Christopher M Bishop. Mixture density networks. 1994. 2\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 2,\n3, 5, 7\n[4] Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir\nAnguelov. Multipath: Multiple probabilistic anchor trajec-\ntory hypotheses for behavior prediction. In CoRL, 2019. 1,\n2\n[5] Kyunghyun Cho, Bart van Merrienboer, CÂ¸ aglar GÂ¨ulcÂ¸ehre,\nDzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\nYoshua Bengio.\nLearning phrase representations using\nrnn encoder-decoder for statistical machine translation. In\nEMNLP, 2014. 2, 5\n[6] Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou,\nTsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schnei-\nder, and Nemanja Djuric. Multimodal trajectory predictions\nfor autonomous driving using deep convolutional networks.\nIn ICRA, 2019. 1, 2\n[7] Nachiket Deo and Mohan M Trivedi. Convolutional social\npooling for vehicle trajectory prediction. In CVPRW, 2018.\n2\n[8] Wenchao Ding, Lu Zhang, Jing Chen, and Shaojie Shen. Ep-\nsilon: An efficient planning system for automated vehicles in\nhighly interactive environments. T-RO, 2021. 1\n[9] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi\nLiu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp,\nCharles R. Qi, Yin Zhou, Zoey Yang, AurÂ´elien Chouard, Pei\nSun, Jiquan Ngiam, Vijay Vasudevan, Alexander McCauley,\nJonathon Shlens, and Dragomir Anguelov. Large scale in-\nteractive motion forecasting for autonomous driving: The\nwaymo open motion dataset. In ICCV, 2021. 2, 5, 6, 1\n[10] Francesco Giuliari, Irtiza Hasan, Marco Cristani, and Fabio\nGalasso. Transformer networks for trajectory forecasting. In\nICPR, 2020. 2\n[11] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese,\nand Alexandre Alahi. Social gan: Socially acceptable trajec-\ntories with generative adversarial networks. In CVPR, 2018.\n2\n[12] Sepp Hochreiter and JÂ¨urgen Schmidhuber. Long short-term\nmemory. Neural Computation, 1997. 2, 5\n[13] Joey Hong, Benjamin Sapp, and James Philbin. Rules of the\nroad: Predicting driving behavior with a convolutional model\nof semantic interactions. In CVPR, 2019. 2\n[14] Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B\nChoy, Philip HS Torr, and Manmohan Chandraker. Desire:\nDistant future prediction in dynamic scenes with interacting\nagents. In CVPR, 2017. 2\n[15] Stefan Lee, Senthil Purushwalkam Shiva Prakash, Michael\nCogswell, Viresh Ranjan, David Crandall, and Dhruv Batra.\nStochastic multiple choice learning for training diverse deep\nensembles. In NIPS, 2016. 1, 2, 3, 5\n[16] Tong Li, Lu Zhang, Sikang Liu, and Shaojie Shen. Marc:\nMultipolicy and risk-aware contingency planning for au-\ntonomous driving. RA-L, 2023. 1\n[17] Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song\nFeng, and Raquel Urtasun. Learning lane graph representa-\ntions for motion forecasting. In ECCV, 2020. 1, 2\n[18] Longzhong Lin, Xuewu Lin, Tianwei Lin, Lichao Huang,\nRong Xiong, and Yue Wang.\nEda: Evolving and distinct\nanchors for multimodal motion prediction. In AAAI, 2024.\n1, 5\n[19] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr DollÂ´ar. Focal loss for dense object detection. In CVPR,\n2017. 6\n[20] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi,\nHang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic an-\nchor boxes are better queries for detr. In ICLR, 2022. 3\n[21] Yicheng Liu, Jinghuai Zhang, Liangji Fang, Qinhong Jiang,\nand Bolei Zhou. Multimodal motion prediction with stacked\ntransformers. In CVPR, 2021. 2\n[22] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\ndescent with warm restarts. In ICLR, 2017. 6\n[23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019. 6\n[24] Osama Makansi, Eddy Ilg, Ozgun Cicek, and Thomas Brox.\nOvercoming limitations of mixture density networks: A sam-\npling and fitting framework for multimodal future prediction.\nIn CVPR, 2019. 1, 2, 5\n[25] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng,\nHouqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang.\nConditional detr for fast training convergence.\nIn ICCV,\n2021. 3\n[26] Jean Mercat, Thomas Gilles, Nicole El Zoghby, Guil-\nlaume Sandou, Dominique Beauvois, and Guillermo Pita\nGil. Multi-head attention for multi-modal joint vehicle mo-\ntion forecasting. In ICRA, 2020. 2\n[27] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth\nGoel, Khaled S Refaat, and Benjamin Sapp.\nWayformer:\nMotion forecasting via simple & efficient attention networks.\nIn ICRA, 2023. 1, 2, 3, 5\n[28] Jiquan Ngiam, Benjamin Caine, Vijay Vasudevan, Zheng-\ndong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, Rebecca\nRoelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, David\nWeiss, Ben Sapp, Zhifeng Chen, and Jonathon Shlens. Scene\ntransformer: A unified architecture for predicting multiple\nagent trajectories. In ICLR, 2022. 2\n[29] Tung Phan-Minh, Elena Corina Grigore, Freddy A Boulton,\nOscar Beijbom, and Eric M Wolff. Covernet: Multimodal\nbehavior prediction using trajectory sets. In CVPR, 2020. 2\n[30] Jonah Philion, Xue Bin Peng, and Sanja Fidler. Trajeglish:\nTraffic modeling as next-token prediction. In ICLR, 2024. 2\n[31] Nicholas Rhinehart, Kris M Kitani, and Paul Vernaza. R2p2:\nA reparameterized pushforward policy for diverse, precise\ngenerative path forecasting. In ECCV, 2018. 2\n[32] Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and\nSergey Levine. Precog: Prediction conditioned on goals in\nvisual multi-agent settings. In ICCV, 2019. 2, 3\n9\n\n[33] Luke\nRowe,\nMartin\nEthier,\nEli-Henry\nDykhne,\nand\nKrzysztof Czarnecki.\nFjmp: Factorized joint multi-agent\nmotion prediction over learned directed acyclic interaction\ngraphs. In CVPR, 2023. 3\n[34] Christian Rupprecht, Iro Laina, Robert DiPietro, Maximil-\nian Baust, Federico Tombari, Nassir Navab, and Gregory D\nHager. Learning in an uncertain world: Representing am-\nbiguity through multiple hypotheses. In ICCV, 2017. 1, 2,\n5\n[35] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and\nMarco Pavone. Trajectron++: Dynamically-feasible trajec-\ntory forecasting with heterogeneous data. In ECCV, 2020.\n2\n[36] Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou,\nNigamaa Nayakanti, Khaled S Refaat, Rami Al-Rfou, and\nBenjamin Sapp. Motionlm: Multi-agent motion forecasting\nas language modeling. In ICCV, 2023. 2\n[37] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\nSelf-\nattention with relative position representations. In NAACL,\n2018. 3\n[38] Chen Shi, Shaoshuai Shi, and Li Jiang. Mtr v3: 1st place so-\nlution for 2024 waymo open dataset challenge - motion pre-\ndiction. In CVPR 2024 Workshop on Autonomous Driving,\n2024. 6\n[39] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele.\nMotion transformer with global intention localization and lo-\ncal movement refinement. In NeurIPS, 2022. 1, 2, 3, 5, 6\n[40] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele.\nMtr++: Multi-agent motion prediction with symmetric scene\nmodeling and guided intention querying. TPAMI, 2024. 6, 7\n[41] Roman Solovyev, Weimin Wang, and Tatiana Gabruseva.\nWeighted boxes fusion: Ensembling boxes from different ob-\nject detection models. Image and Vision Computing, 2021.\n1\n[42] Jiawei Sun, Jiahui Li, Tingchen Liu, Chengran Yuan, Shuo\nSun, Zefan Huang, Anthony Wong, Keng Peng Tee, and\nMarcelo H Ang Jr. Rmp-yolo: A robust motion predictor\nfor partially observable scenarios even if you only look once.\narXiv preprint arXiv:2409.11696, 2024. 6\n[43] Qiao Sun, Xin Huang, Junru Gu, Brian C Williams, and\nHang Zhao. M2i: From factored marginal trajectory pre-\ndiction to interactive prediction. In CVPR, 2022. 3\n[44] Yichuan Charlie Tang and Ruslan Salakhutdinov. Multiple\nfutures prediction. In NeurIPS, 2019. 2, 3\n[45] Luca Anthony Thiede and Pratik Prabhanjan Brahma. Ana-\nlyzing the variety loss in the context of probabilistic trajec-\ntory prediction. In ICCV, 2019. 1, 2, 5\n[46] Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivas-\ntava, Khaled S Refaat, Nigamaa Nayakanti, Andre Cornman,\nKan Chen, Bertrand Douillard, Chi Pang Lam, Dragomir\nAnguelov, and Benjamin Sapp. Multipath++: Efficient in-\nformation fusion and trajectory aggregation for behavior pre-\ndiction. In ICRA, 2022. 1, 2, 3, 5\n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, 2017. 2\n[48] Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order\nmatters: Sequence to sequence for sets. In ICLR, 2016. 3\n[49] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lam-\nbert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Rat-\nnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes,\nDeva Ramanan, Peter Carr, and James Hays. Argoverse 2:\nNext generation datasets for self-driving perception and fore-\ncasting. In NeurIPS Datasets and Benchmarks, 2021. 2, 5,\n6, 1\n[50] Cunjun Yu, Xiao Ma, Jiawei Ren, Haiyu Zhao, and Shuai Yi.\nSpatio-temporal graph transformer networks for pedestrian\ntrajectory prediction. In ECCV, 2020. 2\n[51] Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Ben Sapp,\nBalakrishnan Varadarajan, Yue Shen, Yi Shen, Yuning Chai,\nCordelia Schmid, Congcong Li, and Dragomir Anguelov.\nTnt: Target-driven trajectory prediction. In CoRL, 2020. 2\n[52] Yang Zhou, Hao Shao, Letian Wang, Steven L Waslander,\nHongsheng Li, and Yu Liu. Smartrefine: A scenario-adaptive\nrefinement framework for efficient motion prediction.\nIn\nCVPR, 2024. 2\n[53] Zikang Zhou, Luyao Ye, Jianping Wang, Kui Wu, and Kejie\nLu. Hivt: Hierarchical vector transformer for multi-agent\nmotion prediction. In CVPR, 2022. 1, 2, 6\n[54] Zikang Zhou, Jianping Wang, Yung-Hui Li, and Yu-Kai\nHuang. Query-centric trajectory prediction. In CVPR, 2023.\n1, 2, 3, 5, 6, 7, 8\n[55] Zikang Zhou, Haibo Hu, Xinhong Chen, Jianping Wang,\nNan Guan, Kui Wu, Yung-Hui Li, Yu-Kai Huang, and\nChun Jason Xue. Behaviorgpt: Smart agent simulation for\nautonomous driving with next-patch prediction. In NeurIPS,\n2024. 2\n[56] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. In ICLR, 2021. 3\n10\n\nModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode\nModeling\nSupplementary Material\n6. Definition of a Match\nThe Argoverse 2 Motion Forecasting Benchmark [49] de-\nsires the predictionsâ€™ displacement error at the 60-th time\nstep to be less than 2 meters. By linearly scaling the 2-meter\nthreshold across time steps, we obtain a distance threshold\nÎ“(t) for each time step t:\nÎ“ (t) = t\n30 .\n(9)\nOur EMTA training loss requires a matched trajectory to\nfall within the given threshold of the ground truth at every\nfuture time step.\nOn the Waymo Open Motion Dataset (WOMD) [9], the\nthresholds are divided into lateral and longitudinal ones,\nwhich are adaptive to the current velocity of the agent of\ninterest. To begin with, the benchmark defines a scaling\nfactor with respect to the velocity v:\nScale (v) =\nï£±\nï£´\nï£´\nï£²\nï£´\nï£´\nï£³\n0.5\nif v < 1.4 ;\n0.5 + 0.5 v âˆ’1.4\n11 âˆ’1.4\nif 1.4 â‰¤v < 11 ;\n1\notherwise .\n(10)\nUtilizing the scaling factor, we define the lateral threshold\nÎ“lat(v, t) as\nÎ“lat (v, t) = Scale (v) Ã—\nï£±\nï£²\nï£³\nt\n30\nif 1 â‰¤t â‰¤30 ;\n0.04t âˆ’0.2\notherwise .\n(11)\nSimilarly, we set the longitudinal threshold Î“lon(v, t) to be\ntwice as large as the lateral one:\nÎ“lon (v, t) = Scale (v) Ã—\nï£±\nï£²\nï£³\nt\n15\nif 1 â‰¤t â‰¤30 ;\n0.08t âˆ’0.4\notherwise .\n(12)\nRegarding the experiments on the WOMD, we demand a\nmatched trajectory to have errors below both the lateral and\nlongitudinal thresholds at every future time step.\n7. Ensemble Method on the WOMD\nInspired by Weighted Boxes Fusion (WBF) [41], we pro-\npose Weighted Trajectory Fusion to aggregate multimodal\ntrajectories produced by multiple models. Our ensemble\nmethod is almost the same as WBF, except we are fus-\ning trajectories according to distance thresholds rather than\nModeSeq (Ours)\nQCNet [54]\n#Mode = 3\n#Mode = 6\n#Mode = 3\n#Mode = 6\nLatency (ms)\n86Â±9\n143Â±10\n63Â±11\n69Â±16\nTable 6. Comparisons on the inference latency averaged over the\nvalidation set of the WOMD.\nbounding boxes according to IOU thresholds. Our ensem-\nble method can improve mAP6/Soft mAP6/MR6 by sac-\nrificing minADE6/minFDE6, which indicates that the per-\nformance on various metrics sometimes disagrees.\nThe\ncritical hyperparameters in Weighted Trajectory Fusion are\nthe distance thresholds used for trajectory clustering. We\nchoose the velocity-aware thresholds defined in Eq. (11)\nand Eq. (12) as the base thresholds. On top of this, we mul-\ntiply the base thresholds with the scaling factors of 1.5, 1.4,\nand 1.4 for vehicles, pedestrians, and cyclists, respectively.\n8. Inference Latency\nAs stated in the main paper, a weakness of our current ap-\nproach lies in the inference latency, which is about twice as\nhigh as that of QCNet [54] if predicting 6 modes accord-\ning to the measurement in Tab. 6. However, in many real-\nworld use cases, the number of modes is refrained from be-\ning more than 3. As shown in Tab. 6, the gap in inference\nlatency between a 3-mode ModeSeq and a 3-mode QCNet\nis much smaller. Given our approachâ€™s capability of produc-\ning representative trajectories with fewer modes, we believe\nsequential mode modeling has the potential to be deployed\non board. Our future work will focus on further reducing\nthe inference cost by improving the architecture or distill-\ning a small model from a large one, which is necessary for\nfacilitating real-world applications of our approach.\n9. More Qualitative Results\nFigure 6 supplements the results in Fig. 4 to demonstrate\nour approachâ€™s ability to produce representative trajectories\nand extrapolate more modes.\n1\n\n(a) #Mode@Training=3, #Mode@Inference=3\n(b) #Mode@Training=6, #Mode@Inference=6\n(c) #Mode@Training=6, #Mode@Inference=24\nFigure 6. Visualization on the WOMD. The agents in purple are predicted with blue trajectories, with the opacity indicating confidence.\n2",
    "pdf_filename": "ModeSeq_Taming_Sparse_Multimodal_Motion_Prediction_with_Sequential_Mode_Modeling.pdf"
}