{
    "title": "ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode",
    "abstract": "0.5 Scene Embedding 0.3 Anticipating the multimodality of future events lays the (a)Parallelmodemodeling foundation for safe autonomous driving. However, multi- modalmotionpredictionfortrafficagentshasbeenclouded 0.7 0.3 0.1 bythelackofmultimodalgroundtruth. Existingworkspre- Scene Embedding dominantly adopt the winner-take-all training strategy to (b)Sequentialmodemodeling tacklethischallenge,yetstillsufferfromlimitedtrajectory Figure 1. A comparison between parallel and sequential mode diversityandmisalignedmodeconfidence. Whilesomeap- modeling. Whileparallelmodemodeling(Fig.1a)decodesmul- proachesaddresstheselimitationsbygeneratingexcessive timodal trajectories in one shot, our sequential mode modeling trajectory candidates, they necessitate a post-processing (Fig. 1b) reasons about multiple plausible futures step by step, whichcapturestherelationshipsbetweenmodestoavoidproduc- stage to identify the most representative modes, a process ingindistinguishabletrajectoriesandconfidencescores. lacking universal principles and compromising trajectory accuracy. We are thus motivated to introduce ModeSeq, a future may be compatible with a given observation of the newmultimodalpredictionparadigmthatmodelsmodesas past. Without characterizing the multimodal distribution sequences. Unlike the common practice of decoding mul- ofagentmotions,autonomousvehiclesmayfailtointeract tiple plausible trajectories in one shot, ModeSeq requires withthesurroundingsinasafeandhuman-likemanner. For motiondecoderstoinferthenextmodestepbystep,thereby this reason, advanced decision-making systems demand a more explicitly capturing the correlation between modes motionpredictortoforecastseveralplausibleandrepresen- andsignificantlyenhancingtheabilitytoreasonaboutmul- tativetrajectoriesofcriticalagents[8,16]. timodality. Leveraging the inductive bias of sequential Although multimodality has long been the central topic mode prediction, we also propose the Early-Match-Take- studied in motion prediction, this problem has not been All (EMTA) training strategy to diversify the trajectories fundamentally solved owing to the unavailability of mul- further. Withoutrelyingondensemodepredictionorrule- timodalgroundtruth,i.e.,onlyonepossibilityisobservable basedtrajectoryselection,ModeSeqconsiderablyimproves in real-world driving data. To struggle with this dilemma, thediversityofmultimodaloutputwhileattainingsatisfac- mostexistingworksadoptthewinner-take-all(WTA)strat- torytrajectoryaccuracy,resultinginbalancedperformance egy [15] for training [4, 6, 17, 27, 39, 46, 53, 54]. Under onmotionpredictionbenchmarks.Moreover,ModeSeqnat- this strategy, only the best among all predicted trajectories urally emerges with the capability of mode extrapolation, willreceivesupervisionsignalsforregression,whileallthe whichsupportsforecastingmorebehaviormodeswhenthe remainingwillbemaskedinthetrainingloss.Despitebeing futureishighlyuncertain. thecurrentstandardpracticeintheresearchcommunity,the WTAsolutionhasbeenfoundtocausemodecollapseeas- ily and produce indistinguishable trajectories [24, 34, 45], 1.Introduction further confusing the learning of mode scoring [18]. As a remedy, somerecentresearchintendstocovertheground- Handling the intricate uncertainty presented in the real truth mode by generating a massive number of trajectory world is one of the major hurdles in autonomous driving. candidates [27, 39, 46], from which the most representa- Oneaspectoftheuncertaintyliesinthemultimodalbehav- tiveonesareheuristicallyselectedbasedonpost-processing ioroftrafficagents,i.e.,multipleinstantiationsofanagent’s methodssuchasnon-maximumsuppression(NMS).How- 1 4202 voN 71 ]GL.sc[ 1v11911.1142:viXra",
    "body": "ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode\nModeling\nZikangZhou1 HengjianZhou2 HaiboHu1 ZihaoWen1\nJianpingWang1 Yung-HuiLi3 Yu-KaiHuang4\n1CityUniversityofHongKong\n2ZhejiangUniversity 3HonHaiResearchInstitute 4CarnegieMellonUniversity\nAbstract 0.4\n0.5\nScene Embedding\n0.3\nAnticipating the multimodality of future events lays the (a)Parallelmodemodeling\nfoundation for safe autonomous driving. However, multi-\nmodalmotionpredictionfortrafficagentshasbeenclouded\n0.7 0.3 0.1\nbythelackofmultimodalgroundtruth. Existingworkspre- Scene Embedding\ndominantly adopt the winner-take-all training strategy to (b)Sequentialmodemodeling\ntacklethischallenge,yetstillsufferfromlimitedtrajectory Figure 1. A comparison between parallel and sequential mode\ndiversityandmisalignedmodeconfidence. Whilesomeap- modeling. Whileparallelmodemodeling(Fig.1a)decodesmul-\nproachesaddresstheselimitationsbygeneratingexcessive timodal trajectories in one shot, our sequential mode modeling\ntrajectory candidates, they necessitate a post-processing (Fig. 1b) reasons about multiple plausible futures step by step,\nwhichcapturestherelationshipsbetweenmodestoavoidproduc-\nstage to identify the most representative modes, a process\ningindistinguishabletrajectoriesandconfidencescores.\nlacking universal principles and compromising trajectory\naccuracy. We are thus motivated to introduce ModeSeq, a future may be compatible with a given observation of the\nnewmultimodalpredictionparadigmthatmodelsmodesas past. Without characterizing the multimodal distribution\nsequences. Unlike the common practice of decoding mul- ofagentmotions,autonomousvehiclesmayfailtointeract\ntiple plausible trajectories in one shot, ModeSeq requires withthesurroundingsinasafeandhuman-likemanner. For\nmotiondecoderstoinferthenextmodestepbystep,thereby this reason, advanced decision-making systems demand a\nmore explicitly capturing the correlation between modes motionpredictortoforecastseveralplausibleandrepresen-\nandsignificantlyenhancingtheabilitytoreasonaboutmul- tativetrajectoriesofcriticalagents[8,16].\ntimodality. Leveraging the inductive bias of sequential Although multimodality has long been the central topic\nmode prediction, we also propose the Early-Match-Take- studied in motion prediction, this problem has not been\nAll (EMTA) training strategy to diversify the trajectories fundamentally solved owing to the unavailability of mul-\nfurther. Withoutrelyingondensemodepredictionorrule- timodalgroundtruth,i.e.,onlyonepossibilityisobservable\nbasedtrajectoryselection,ModeSeqconsiderablyimproves in real-world driving data. To struggle with this dilemma,\nthediversityofmultimodaloutputwhileattainingsatisfac- mostexistingworksadoptthewinner-take-all(WTA)strat-\ntorytrajectoryaccuracy,resultinginbalancedperformance egy [15] for training [4, 6, 17, 27, 39, 46, 53, 54]. Under\nonmotionpredictionbenchmarks.Moreover,ModeSeqnat- this strategy, only the best among all predicted trajectories\nurally emerges with the capability of mode extrapolation, willreceivesupervisionsignalsforregression,whileallthe\nwhichsupportsforecastingmorebehaviormodeswhenthe remainingwillbemaskedinthetrainingloss.Despitebeing\nfutureishighlyuncertain. thecurrentstandardpracticeintheresearchcommunity,the\nWTAsolutionhasbeenfoundtocausemodecollapseeas-\nily and produce indistinguishable trajectories [24, 34, 45],\n1.Introduction further confusing the learning of mode scoring [18]. As a\nremedy, somerecentresearchintendstocovertheground-\nHandling the intricate uncertainty presented in the real truth mode by generating a massive number of trajectory\nworld is one of the major hurdles in autonomous driving. candidates [27, 39, 46], from which the most representa-\nOneaspectoftheuncertaintyliesinthemultimodalbehav- tiveonesareheuristicallyselectedbasedonpost-processing\nioroftrafficagents,i.e.,multipleinstantiationsofanagent’s methodssuchasnon-maximumsuppression(NMS).How-\n1\n4202\nvoN\n71\n]GL.sc[\n1v11911.1142:viXra\never, such a post-processing step requires carefully tuning coders[3,39,52,54], whichispoweredbyreorderingthe\nthe hyperparameters, e.g., the thresholds in NMS. Even if mode embeddings in between decoding layers. Moreover,\nthehyperparameterswerewell-tuned,theymightnotfitvar- leveraging the order of modes in the sequence, we further\niousscenarioswithdiverseroadconditions,leadingtoinfe- propose the Early-Match-Take-All (EMTA) training strat-\nriorgeneralization. Moreover,performingdensemodepre- egy,whichcanencouragethedecodertomatchtheground\ndictionfollowedbyrule-basedpost-processingmaysignif- truth as early as possible. Meanwhile, our EMTA scheme\nicantly sacrifice trajectory accuracy in practice [39], given alsoenforcesthedecodertovacatetheduplicatedmodesto\nthatcorrectlyextractingthebesttrajectoriesfromalargeset coverthemissingfuturesatthecostofnegligibledegrada-\nofcandidatesisnon-trivial. tion in trajectory accuracy, thereby achieving better mode\ncoverageandeasingthelearningofconfidencescoring.\nThe limitations of mainstream methods prompt us to\nWe validate ModeSeq on the Waymo Open Motion\nseekanend-to-endsolutionthatdirectlyproducesasparse\nDataset [9] and the Argoverse 2 Motion Forecasting\nset of diverse, high-quality, and representative agent tra-\nDataset[49],whereweachievemorebalancedperformance\njectories, eliminating the need for dense mode prediction\nin terms of mode coverage, mode scoring, and trajectory\nand heuristic mode selection. To begin with, we identify\naccuracy,comparedwithrepresentativemotionforecasting\na commonality of existing multimodal motion predictors,\nmethodssuchasQCNet[54]andMTR[39]. Furthermore,\nthatalltrajectorymodesaredecodedinoneshot,whichwe\nourapproachnaturallyemergeswiththecapabilityofmode\ndub parallel mode modeling as depicted in Fig. 1a. De-\nextrapolation thanks to sequential mode modeling, which\nspite its efficiency, this paradigm neglects the relationship\nenablespredictingadynamicnumberofmodesondemand.\nbetween the predicted modes, hindering models from de-\ncodingdiversemultimodaloutput.Foranchor-freemethods\n2.RelatedWork\nwith parallel mode modeling [27, 28, 46, 53, 54], the dis-\ntinction between the decoded trajectories depends entirely\nMultimodality has been a dark cloud in the field of mo-\non the difference in sub-network parameters, which can-\ntion prediction. Early works employ generative models to\nnot be guaranteed under the unstable WTA training. For\nsample multimodal trajectories [11, 13, 14, 32, 44], but\nthis reason, some of these solutions turn to dense mode\ntheyaresusceptibletomodecollapse. Modernmotionpre-\nprediction and rely on post-processing steps to diversify\ndictors [4, 6, 17, 27, 39, 46, 53, 54] mostly follow the\nthe output [27, 46]. While anchor-based approaches of-\nparadigm of multiple choice learning [15], where multiple\nfload the duties of ensuring diversity onto the input an-\ntrajectory modes are produced directly from mixture den-\nchors [4, 29, 39, 51], determining a sparse set of anchors\nsity networks [2]. Due to the lack of multimodal ground\nthat can adapt to specific scenarios is challenging, com-\ntruth,thesemethodsadopttheWTAtrainingstrategy[15],\npelling all these approaches to employ excessive anchors\nwhichisunstableandfailstodealwithmodecollapsefun-\nfor dense mode prediction. Under the paradigm of paral-\ndamentally [24, 34, 45]. To mitigate this issue, a line\nlelmodemodeling,producingmultimodaltrajectorieswith\nof research performs dense mode prediction, i.e., decod-\nsparsemodepredictionfacessignificantobstacles.\ning excessive trajectory candidates for better mode cover-\nTotacklethesechallenges,thispaperexploressequential age [27, 39, 46]. Among these works, some equip the de-\nmode modeling (ModeSeq), a completely different path- coder with anchors [4, 29, 39, 51] to achieve more stable\nwaytowardsparsemultimodalmotionprediction. Asillus- training. However, dense mode prediction necessitates the\ntratedinFig.1b,weattempttoconstructachainofmodes rule-based selection of the most representative trajectories\nwhendecodingthefuturefromthesceneembeddings,pro- from a large set of candidates, risking the precision of tra-\nducingonlyoneplausibletrajectoryandthecorresponding jectoriesandthegeneralizationabilityacrossawiderange\nconfidence at each decoding step. Compared with paral- of scenarios. This paper provides new insights into multi-\nlel prediction, our approach puts more effort into captur- modalproblemsbyintroducingtheparadigmofsequential\ningthecorrelationbetweenmodes,askingthemodeltotell mode modeling and the EMTA training strategy, pursuing\nwhatthenextmodeshouldbeandhowmuchconfidenceit anend-to-endsolutionthatproducesasparsesetofdiverse,\nhasconditionedonthemodeembeddingsatprevioussteps. high-quality,andrepresentativetrajectoriesdirectly.\nBy giving the model a chance to look at the prior modes Sequential modeling has found many applications in mo-\nand learning the factorized joint latent space of multiple tion prediction and traffic modeling. On the one hand, ap-\nfutures, we tremendously boost the capability of reason- plying sequential modeling to the time dimension results\ning about multimodality and characterizing the full distri- intrajectoryencodersanddecodersbasedonrecurrentnet-\nbutionwithouttherelianceondensemodeprediction,post- works[1,5,7,11,12,14,26,31,32,35,44,46]orTrans-\nprocessingtricks,andallmannerofanchors. Tostrengthen formers [10, 21, 27, 28, 30, 36, 47, 50, 53–55], which can\nthecapacityofmodelsunderournewparadigm,wedevelop facilitate the learning of temporal dynamics. In particu-\naniterativerefinementframeworksimilartoDETR-likede- lar,recentadvancesinmotiongeneration[30,36,55]have\n2\nshown that factorizing the joint distribution of multi-agent akin to the application of set prediction in object detec-\ntime-seriesinasocialautoregressivemanner[1,32,44]can tors [3, 20, 25, 56] where each object query receives su-\nbettercharacterizetheevolutionoftrafficscenarios. Onthe pervisionsignalsviaoptimalbipartitematching. However,\nother hand, some works utilize sequential modeling in the real-world driving data contains only one instantiation of\nagentdimensionformulti-agentmotionprediction[33,43]. sceneevolution,whichcompelsmotionpredictionsolutions\nForexample,M2I[43]usesheuristicmethodstolabelinflu- toadoptthewinner-take-all(WTA)matching[15]. Asare-\nencers and reactors from pairs of agents, followed by pre- sult, only the mode embeddings of the best-predicted tra-\ndicting the marginal distribution of the influencers and the jectories get optimized, which will easily degenerate the\nconditionaldistributionofthereactors. FJMP[33]extends models into learning multiple mode embeddings indepen-\nM2Itomodelthejointdistributionofanarbitrarynumberof dently. Theinabilitytojointlyoptimizeallmodesexplains\nagents,wherethejointfuturetrajectoriesofagentsarefac- why DETR-like motion decoders fail to avoid duplicated\ntorizedusingadirectedacyclicgraph. Ourworkisthefirst trajectories without the use of non-maximum suppression\nattemptthatemployssequentialpredictioninthemodedi- (NMS),giventhatgatheringmultiplepredictedtrajectories\nmension,whichenhancestheunderstandingofmultimodal around the most probable regions is expected to achieve\nbehaviorbycapturingthecorrelationbetweenmodes. lower training loss when only one ground-truth future is\npresentedduringtraining.\n3.Methodology To facilitate reasoning about multimodality in the ab-\nsenceofmultimodalgroundtruth,ourModeSeqframework\n3.1.ProblemFormulation\nrequires the decoder to conduct chain-based factorization\nDenote S as the input of motion prediction models, which onthejointembeddings,whichisdemonstratedasfollows:\nencompasses the map elements represented as M polygo- (cid:0) (cid:1)\nm =Decoder Ψ, {m } , t∈{1,...,K}.\nt k k∈{1,...,t−1}\nnalinstancesandtheT-stephistoricaltrajectoriesofAtraf-\n(2)\nfic agents (e.g., vehicles, pedestrians, and cyclists) in the\nWithsuchafactorizationthatconvertstheunorderedsetof\nscene. ThemodelsaretaskedwithforecastingK plausible\nmodesintoasequence,thecorrelationbetweenmodescan\ntrajectory modes per agent of interest, each comprising Tˆ\nbe naturally strengthened, as the mode to be decoded de-\nwaypoints and an associated confidence score. These tra-\npendsontheonesthatappearpreviously.Furtherequipping\njectoriesaredesiredtoberepresentative,reflectingdistinct\nthisframeworkwithappropriatemodelimplementationand\nbehaviormodesofagentsandproperlymeasuringthelike-\ntrainingschemehasthepotentialtoofferbettermodecov-\nlihoodofeachmodeviatheestimatedconfidence.\nerage and scoring without severe sacrifice in trajectory ac-\nTypical motion predictors employ the encoder-decoder\ncuracy,whichweintroduceinthefollowingsections.\narchitecture, where an encoder computes the embedding\nΨ from the scene input, based on which a decoder learns 3.3.SceneEncoding\ntheembeddings{m } ofthei-thagent’sfuture\ni,k k∈{1,...,K} Sincethisworkfocusesonthedecodingofmultimodaltra-\nmodes. Withoutlossofgenerality,thefollowingsimplifies\njectories, we simply adopt QCNet [54] as the scene en-\nm as m to discuss the prediction for a single agent,\ni,k k coder,whichisoneofthedefactobestpracticesinindustry\nwhichcanbeextendedformultipleagentsbyrepeatingthe\nand academia due to its symmetric modeling in space and\nsamedecodingprocess. Givenm ,apredictionheadthen\nk time leveraging relative positional embeddings [37]. This\noutputs a trajectory yˆ k = [yˆ k1,...,yˆ kTˆ] and a confidence encoder, on the one hand, exploits a hierarchical map en-\nscore ϕˆ k via simple modules such as multi-layer percep- codingmodulebasedonmap-mapself-attentiontoproduce\ntrons (MLPs). The whole pipeline can be summarized as the map embedding of shape [M,D], with D referring to\n the hidden size. On the other hand, the encoder consists\nΨ=Encoder(S) ,\n of Transformer modules that factorize the space and time\n{m k} k∈{1,...,K} =Decoder(Ψ) , (1) axes, including temporal self-attention, agent-map cross-\nyˆ ,ϕˆ =Head(m ), k ∈{1,...,K}. attention,andagent-agentself-attention. Thesethreetypes\nk k k\nof attention are grouped and interleaved twice to yield the\n3.2.Motivation\nagentembeddingofshape[A,T,D], whichconstitutesthe\nPrior works formulate multimodal decoding as a prob- final scene embeddings together with the map embedding.\nlem of set prediction [3, 48]. For instance, most cutting- In principle, any scene encoding method can fit into our\nedge methods employ DETR-like decoders [3] to produce ModeSeqframeworkwithreasonableefforts.\nthe joint embeddings of multiple modes from learnable or\n3.4.Single-LayerModeSequence\nanchor-based queries that are permutation-equivariant [27,\n39, 46, 54]. Ideally, the joint mode embeddings should This section illustrates the detailed structure of a single\nbesupervisedbytheground-truthmultimodaldistribution, ModeSeq layer, which consists of a Memory Transformer\n3\nEMTA Loss 0.7 0.3 0.1\nPush\nModeSeq Layer 2\n0.5 0.4 0.2 K\n0.5\nV Q\n0.2\nMode Rearrangement 0.4 Context Transformer MLP\n𝑚(!)\nEMTA Loss 0.4 0.5 0.2 [𝑚 !(!),𝑚 $(!)] Q K V %\n0.0 0.x Mode Confidence\nModeSeq Layer 1 𝑚 %(&) Mode Embedding\nM A\nMap Embedding\nAgentEmbedding\nScene Embedding 0.0 0.0 0.0 T\nScene Embedding\nFigure 2. Overview of the ModeSeq framework. Left: We stack multiple ModeSeq layers with mode rearrangement in between to\niterativelyrefinethemultimodaloutputundertheEarly-Match-Take-All(EMTA)trainingstrategy. Right: EachModeSeqlayerconsists\nof a Memory Transformer module for capturing mode-wise dependencies and a Context Transformer module for retrieving the scene\nembeddingsproducedbytheencoder,operatinginarecurrentfashiontodecodeasequenceoftrajectorymodes.\nmoduleandaContextTransformermodule. Stackingmul- lowingparagraphsdetailthemodulesconstitutingaMode-\ntipleModeSeqlayerscanfurtherimprovetheperformance Seqlayeranddiscussthedifferencesbetweenourapproach\nviaiterativerefinement,whichwillbediscussedinSec.3.5. andotheralternatives.\nRoutine. We introduce the decoding procedure of the ℓ- Memory Transformer. The Memory Transformer takes\nth ModeSeq layer, with the right-hand side of Fig. 2 de- charge of modeling the sequential dependencies of trajec-\npicting the first layer (i.e., ℓ = 1). The layer is de- torymodes. Atthet-thdecodingstepoftheℓ-thlayer,this\nsigned to recurrently output a sequence of mode embed- moduletakesasinputΩ(ℓ) andm(ℓ−1),thememorybank\nt−1 t\ndings [m(ℓ),...,m(ℓ)], where we slightly complicate the for the current layer and the t-th mode embedding derived\n1 K\nnotation of modes with a superscript that identifies the from the last layer. Since we desire the generation of the\nlayer index. The input of the layer includes the scene t-thmodeembeddingm(ℓ) tobeawareoftheexistenceof\nt\nembedding Ψ yielded by the encoder and the mode em- the preceding modes, we treat m(ℓ−1) as the query of the\nbeddings produced by the (ℓ − 1)-th decoding layer, i.e., t\nTransformermodule,whichretrievesthememorybankina\n[m( 1ℓ−1),...,m( Kℓ−1)]. Since the latter does not apply to cross-attentionmanner:\nthefirstlayer,weintroduceanembeddinge∈RD toserve\n(cid:16) (cid:17)\nastheoutputofthe“0-thlayer”,whichisrandomlyinitial- mˆ(ℓ) =MemFormer query=m(ℓ−1), key/value=Ω(ℓ) .\nt t t−1\nized at the beginning of training. The same learnable e is\n(4)\nsharedacrosstheK inputembeddingsofthefirstlayer: In this way, the information in Ω(ℓ) is assimilated into\nt−1\nm( k0) =e, k ∈{1,...,K}. (3) m t(ℓ−1) to produce mˆ( tℓ), which is a query feature condi-\nBeforestartingthedecoding,wecreateanemptysequence tionedonthemodesuptothe(t−1)-thdecodingstep.\nΩ(ℓ) = []withthesubscriptandsuperscriptindicatingthe Context Transformer. To derive scene-compliant modes,\n0\n0-thdecodingstepandtheℓ-thlayer,respectively. Thisse- wemustprovidethequeryfeaturewithaspecificscenecon-\nquencewillbeusedtokeeptrackofthemodeembeddings text. To this end, we use the Context Transformer module\nproducedatvarioussteps.Atthet-thdecodingstep,weem- torefinetheconditionalquerymˆ( tℓ) withthesceneembed-\nploy a Memory Transformer module and a Context Trans- dingsoutputbytheencoder.Specifically,thet-thmodeem-\nformer module to update m( tℓ−1) to become m( tℓ), lever- beddingm( tℓ) iscomputedbyenrichingmˆ( tℓ) withΨusing\naging the information in the memory bank Ω(ℓ) and the cross-attention:\nt−1\nscene embedding Ψ. The output mode embedding m( tℓ) m(ℓ) =CtxFormer(cid:16) query=mˆ(ℓ), key/value=Ψ(cid:17) .\nis then pushed to the end of the sequence Ω(ℓ) to obtain t t\nt−1 (5)\nΩ(ℓ) = [m(ℓ),...,m(ℓ)], which will serve as the input at Considering the high complexity of performing global at-\nt 1 t\nthe(t+1)-thdecodingstep. AftergoingthroughK decod- tention, we decompose the Context Transformer into three\ningsteps,weuseapredictionheadtotransformeachofthe separate modules in practice, including mode-time cross-\nmode embeddings stored in Ω(ℓ) into a specific trajectory attention,mode-mapcross-attention,andmode-agentcross-\nK\nand a corresponding confidence score via MLPs. The fol- attention,eachofwhichtakesasinputonlyasubsetofthe\n4\nTransformer\nMemory\nembeddings contained in Ψ. First, the mode-time cross- DETR [3], we develop an iterative refinement framework\nattention fuses the query feature with the historical encod- bystackingmultipleModeSeqlayersandapplyingtraining\ningbelongingtotheagentofinterest,enablingthequeryto lossestotheoutputofeachlayer. Asshownintheleftpart\nadapt to the specific agent. Second, we aggregate the map of Fig. 2, all layers except for the first one take as input\ninformationsurroundingtheagentofinterestintothequery themodeembeddingsoutputfromthelastroundofdecod-\nfeature leveraging the mode-map cross-attention, which ing,refiningthefeatureswiththescenecontext. Crucially,\ncontributestothemapcomplianceoftheforecastingresults. we introduce the operation of mode rearrangement in be-\nFinally,utilizingthemode-agentcross-attentionmoduleto tweenlayers,whichcorrectstheorderoftheembeddingsin\nfuse the neighboring agents’ latest embeddings promotes themodesequencetoencouragedecodingtrajectorymodes\nthe model’s social awareness. After going through these withmonotonicallydecreasingconfidencescores.\nthree modules, the conditional query mˆ(ℓ) eventually be- Mode Rearrangement. Before transitioning from the ℓ-\nt\ncomesm(ℓ),whichisnowcontext-aware. th to the (ℓ+1)-th ModeSeq layer, we sort the mode em-\nPredictiot n Head. Given the conditional, context-aware beddingsstoredinthememorybankΩ(ℓ) accordingtothe\nK\nmodeembeddingm(ℓ),weuseanMLPheadtooutputthe descending order of the confidence scores predicted from\nt\nt-thtrajectoryyˆ(ℓ) andanothertoestimatethecorrespond- them. The sorted mode embeddings will then be sequen-\nt\ningconfidencescoreϕˆ(ℓ): tiallyinputtothe(ℓ+1)-thModeSeqlayerforrecurrentde-\nt coding. Throughiterativerefinementwithmoderearrange-\n (cid:16) (cid:17)\nyˆ t(ℓ) =MLP m( tℓ) , ment,thetrajectoriesandtheorderofmodesbecomemore\n(cid:16) (cid:17) (6) scene-compliantandmorereasonable,respectively.\nϕˆ( tℓ) =MLP m( tℓ) .\n3.6.Early-Match-Take-AllTraining\nComparison with DETR-Like Decoders. In contrast to\nmotion decoders [27, 39, 46, 54] inspired by DETR [3], The WTA training strategy [15] is blamed for produc-\nwheretherelationshipsbetweenmodesarecompletelyne- ingoverlappedtrajectoriesandindistinguishableconfidence\nglected [27, 46] or weakly modeled by mode-mode self- scores [18, 24, 34, 45]. Fortunately, our approach has the\nattention[39,54],modelingmodesasasequencestrength- opportunity to opt for a more advanced training method\nens mode-wise relational reasoning thanks to the condi- thanks to the paradigm of sequential mode modeling. In\ntional dependence in generating multimodal embeddings, this section, we propose the EMTA loss, which leverages\nwhich is beneficial to eliminating duplicated trajectories. theorderofmodestodefinethepositiveandnegativesam-\nFurthermore,itisworthnotingthatDETR-likeapproaches ples toward better mode coverage and confidence scoring\ncanonlydecodeafixednumberofmodes,asthenumberof withoutsignificantlysacrificingtrajectoryaccuracy.\nlearnable/staticanchorscannotbechangedoncespecifiedat TypicalWTAlossoptimizesonlythetrajectorywiththe\nthestartoftraining. Bycontrast,ourModeSeqframework minimum displacement error with respect to the ground\nsupportsdecodingmore/lessmodesattesttime,whichcan truth.Incomparison,ourEMTAlossoptimizesthematched\nbe simply achieved by changing the number of decoding trajectorydecodedattheearliestrecurrentstep. Forexam-\nsteps. Thischaracteristiccanbehelpfulsincethedegreeof ple, if both the second and the third trajectories match the\nuncertaintyvariesbyscenario. groundtruth,onlythesecondonewillbeoptimized,regard-\nComparison with Typical Recurrent Networks. The less of which one has the minimum error. To this end, we\nModeSeq layer can be viewed as a sort of recurrent net- search over the K predictions to acquire the collection of\nwork[5,12]duetoitsparametersharingacrossalldecod- modeindicesassociatedwithmatchedtrajectories:\ningsteps,thoughitscorecomponentsaremodernizedwith\n(cid:110) (cid:110) (cid:16) (cid:17)(cid:111)(cid:111)\nTransformers to achieve impressive performance. While G(ℓ) = k |k ∈{1,··· ,K}∧1 IsMatch yˆ(ℓ), y ,\nk\ntypicalrecurrentnetworkscompressthememoryintoasin- (7)\ngle hidden state, which is often lossy, the Memory Trans- where G(ℓ) denotes the set of qualified mode indices in\nformer inside a ModeSeq layer allows for direct access to theℓ-thModeSeqlayer,1{·}representstheindicatorfunc-\nall prior mode embeddings, naturally scaling the capacity tion, and IsMatch(·, ·) defines the criterion for a match\nofthememoryasthenumberofmodesgrows. given the ground-truth trajectory y. The implementation\nof IsMatch(·, ·) can be flexible, depending on the trajec-\n3.5.Multi-LayerModeSequences\ntory accuracy demanded by practitioners. For instance, on\nSingle-layer mode sequences may have limited capability theWaymoOpenMotionDataset[9],wedecidewhethera\nof learning high-quality mode representations. In particu- predictedtrajectoryisamatchbasedonthevelocity-aware\nlar,ifthelayerhappenstoproduceunrealisticorlesslikely distance thresholds defined in the Miss Rate metric of the\nmodes at the first few decoding steps, the learning of the benchmark; while on the Argoverse 2 Motion Forecasting\nlater modes may be unexpectedly disturbed. Inspired by Dataset [49], a matched trajectory is expected to have less\n5\nDataset Method Ensemble Lidar SoftmAP6↑ mAP6↑ MR6↓ minADE6↓ minFDE6↓\nMTRv3[38] × ✓ - 0.4593 0.1175 0.5791 1.1809\nMTR++[40] × × - 0.4382 0.1337 0.6031 1.2135\nQCNet[54] × × 0.4508 0.4452 0.1254 0.5122 1.0225\nVal\nModeSeq(Ours) × × 0.4562 0.4507 0.1206 0.5237 1.0681\nMTRv3[38] ✓ ✓ 0.4967 0.4859 0.1098 0.5554 1.1062\nTest ModeSeq(Ours) ✓ × 0.4737 0.4665 0.1204 0.5680 1.1766\nRMPEnsemble[42] ✓ × 0.4726 0.4553 0.1113 0.5596 1.1272\nTable1.Quantitativeresultsonthe2024WaymoOpenDatasetMotionPredictionBenchmark.\nMethod Ensembleb-minFDE6↓MR6↓minADE6↓minFDE6↓ vationwindowsand6-secondpredictionhorizonsfortrain-\nMTR[39] ✓ 1.98 0.15 0.73 1.44 ing/validation/testing.\nMTR++[40] ✓ 1.88 0.14 0.71 1.37 Metrics. Followingthestandardofthebenchmarks[9,49],\nQCNet[54] × 1.91 0.16 0.65 1.29 we constrain models to output at most K = 6 trajecto-\nModeSeq(Ours) × 1.87 0.14 0.63 1.26\nries. We use Miss Rate (MR ) to measure mode cover-\nK\nTable 2. Quantitative results on the 2024 Argoverse 2 Single- age,whichcountsthefractionofcasesinwhichthemodel\nAgentMotionForecastingBenchmark. failstoproduceanytrajectoriesthatmatchthegroundtruth\nthan 2-meter final displacement error. Given G(ℓ), we de- withintherequiredthresholds. Builtuponthedefinitionof\nterminetheuniquepositivesample’sindexkˆ(ℓ) asfollows, amatch,mAP andSoftmAP assesstheprecisionofthe\nK K\nwithalltheremainingmodestreatedasnegativesamples: confidence scores by computing the P/R curves and aver-\n min k if|G(ℓ)|>0; aging the precision values over various confidence thresh-\n\nkˆ(ℓ) = k∈G(ℓ) (cid:16) (cid:17) (8) olds. To further evaluate trajectory quality, we use mini-\nargminDist yˆ k(ℓ), y otherwise, mum Average Displacement Error (minADE K) and mini-\nk\nmum Final Displacement Error (minFDE ) as indicators,\nwhere|·|denotesthecardinalityofaset,andDist(·, ·)mea- K\nwhich calculate the distance between the ground truth and\nsures the average displacement error between trajectories.\nthebest-predictedtrajectoriesasanaverageoverthewhole\nThisstrategyforlabelassignmentencouragesthemodelto\nhorizon and at the final time step, respectively. Besides,\ndecodematchedtrajectoriesasearlyaspossiblebytreating\ntheb-minFDE concernsthejointperformanceoftrajecto-\ntheearliestinsteadofthebestmatchesaspositivesamples. K\nries and confidences by summing the minFDE and Brier\nMeanwhile,itdrivesthelatermatches,ifany,awayfromthe K\nscoresofthebest-predictedtrajectories.\nground truth by assigning negative labels to them. On the\nImplementation Details. We develop models with a hid-\nother hand, if none of the predictions match, which com-\nden size of 128. The decoder stacks 6 layers for iterative\nmonly happens at the early stage of training, we will fall\nrefinement, witheachlayerexecuting6stepstoobtainex-\nbacktotheregularWTAschemetoeasethedifficultyinop-\nactly 6 modes as required by the benchmarks [9, 49]. On\ntimization. Followinglabelassignment,weusetheLaplace\ntheWOMD[9],weusetheAdamWoptimizer[23]totrain\nnegativelog-likelihood[53,54]astheregressionloss,opti-\nmodelsfor30epochsonthetrainingsetwithabatchsizeof\nmizingthetrajectoriesofthepositivesamples. Besides,we\n32,aweightdecayrateof0.1,andadropoutrateof0.1. On\nuse the Binary Focal Loss [19] to optimize the confidence\nArgoverse 2 [49], we use a similar training configuration\nscoresaccordingtothelabelsassigned. Wealsotryavari-\nexcept that the number of epochs is extended to 64. The\nantofconfidenceloss,whereweintroducethedefinitionof\ninitiallearningrateissetto5×10−4,whichisdecayedto0\nignoredsamplestomaskthelossofthemodesdecodedear-\nattheendoftrainingfollowingthecosineannealingsched-\nlierthanthepositivesamples,whichisshowntobeeffective\nule[22]. Unlessspecified,theablationstudiesarebasedon\nintheabsenceofmoderearrangement.\nexperimentsontheWOMDwith20%ofthetrainingdata.\n4.Experiments\n4.2.ComparisonwithStateoftheArt\n4.1.ExperimentalSetup\nWe compare our approach with QCNet [54] and the MTR\nDatasets. We conduct experiments on the Waymo Open series [38–40], which are currently the most effective\nMotion Dataset (WOMD) [9] and the Argoverse 2 Mo- sparse and dense multimodal prediction solutions across\ntion Forecasting Dataset [49]. The WOMD contains theWOMDandtheArgoverse2dataset. Asdemonstrated\n486995/44097/44920 training/validation/testing samples, in Tab. 1, ModeSeq achieves the best scoring performance\nwhere the history of 1.1 seconds is provided as the con- amongtheLidar-freemethodsonthevalidationsplitofthe\ntext and the 8-second future trajectories of up to 8 agents WOMD, though it lags behind the mAP performance of\n6\nare required to predict. The Argoverse 2 dataset com- MTRv3[38],amodelthataugmentstheinputinformation\nprises 199908/24988/24984 samples with 5-second obser- withrawsensordata.Asasparsemodepredictor,ModeSeq\n6\nDecoder TrainingStrategy IgnoredSamples SoftmAP6↑ mAP6↑ MR6↓ minADE6↓ minFDE6↓\nNone 0.4096 0.4050 0.1536 0.5660 1.1716\nDETRw/Refinement WTA\nOtherMatches 0.4150 0.4103 0.1502 0.5619 1.1621\nNone 0.4138 0.4093 0.1502 0.5563 1.1498\nWTA\nOtherMatches 0.4207 0.4161 0.1503 0.5556 1.1501\nModeSeq(Ours)\nNone 0.4231 0.4196 0.1457 0.5700 1.1851\nEMTA\nOtherMatches 0.4098 0.4060 0.1496 0.5817 1.2207\nTable3.EffectsofsequentialmodemodelingandEarly-Match-Take-AlltrainingonthevalidationsetoftheWOMD.\nModeRearrangement IgnoredSamples SoftmAP6↑ mAP6↑ MR6↓ minADE6↓ minFDE6↓\nNone 0.4112 0.4077 0.1548 0.5884 1.2389\n×\nEarlyMismatches 0.4141 0.4109 0.1489 0.5749 1.2066\nNone 0.4231 0.4196 0.1457 0.5700 1.1851\n✓\nEarlyMismatches 0.4161 0.4129 0.1461 0.5751 1.2041\nTable4.EffectsofmoderearrangementonthevalidationsetoftheWOMD.\n0.43 0.18 will confuse the optimization process, given that the best\nandtheothermatchesusuallyhavesimilarmoderepresen-\n0.42\n0.17\ntationswhiletheyareassignedasoppositesamples.\n0.41\nEffects of EMTA Training. We also investigate the role\n0.16\n0.40 ofEMTAtraininginTab.3. AfterreplacingtheWTAloss\n0.15 withourEMTAscheme,theresultsonSoftmAP 6,mAP 6,\n0.39\nand MR are considerably improved, which demonstrates\n6\n0.38 0.14 the benefits of EMTA training in terms of mode cover-\n1 2 3 4 5 6 1 2 3 4 5 6\nage and confidence scoring. On the other hand, the per-\nDecoder Layer Decoder Layer\nformance on minADE and minFDE slightly deteriorates\nFigure3. Theperformanceaftereachdecodinglayeronthevali- 6 6\nsince the EMTA loss has relaxed the requirement for tra-\ndationsetoftheWOMD.\njectoryaccuracy,butthedegreeofdeteriorationfallswithin\nundoubtedly outperforms MTR++ [40] in terms of MR ,\n6 anacceptableextent,leadingtomorebalancedperformance\nminADE ,andminFDE byalargemargin.Comparedwith\n6 6 taken overall. Moreover, contraryto the conclusion drawn\nQCNet [54], ModeSeq attains better Soft mAP , mAP ,\n6 6 fromtheWTAbaselines,treatingothermatchesasignored\nandMR atthecostofslightdegradationonminADE and\n6 6 samples is detrimental under the EMTA strategy. This is\nminFDE , confirming that our approach can improve the\n6 because the joint effects of sequential mode decoding and\nmodecoverageandconfidencescoringofsparsepredictors\nEMTA training have broken the symmetry of mode mod-\nwithout significant sacrifice in trajectory accuracy. As of\nelingandlabelassignment, allowingustoassigntheother\nthetimewesubmittedtheresultstothebenchmark,theen-\nmatches as negative samples to drive them away from the\nsemble version of ModeSeq ranked first among Lidar-free\ngroundtruthforcoveringotherlikelymodes.\napproachesonthetestsetoftheWOMD.Ourapproachalso\nEffectsofIterativeRefinement. Tounderstandtheeffects\nexhibitspromisingperformanceontheArgoverse2dataset,\nof iterative refinement under our framework, we take the\nwhere our ensemble-free model surpasses QCNet and the\noutput from different decoding layers for evaluation. As\nMTRseriesonallcriticalmetricsasshowninTab.2.\nshown in Fig. 3, the performance on Soft mAP and MR\n6 6\nisgenerallyimprovedasthedepthincreases,totalingasub-\n4.3.AblationStudy\nstantial enhancement between the first and the last layer.\nEffects of Sequential Mode Modeling. In Tab. 3, we ex- Oneofthereasonswhyiterativerefinementworkswellcan\namine the effectiveness of sequential mode modeling by beattributedtotheoperationofmoderearrangementinbe-\ncomparing ModeSeq with the sparse DETR-like decoder tweenlayers,whichweexplaininthefollowing.\nenhancedwithiterativerefinement[3],bothemployingthe EffectsofModeRearrangement. Westudytheeffectsof\nsameQCNetencoder[54]forfaircomparisons. Theresults moderearrangementinTab.4.Comparingthefirstandthird\ndemonstratethatModeSeqoutperformsthebaselineonall rowsofthetable,wecanseethatreorderingthemodeem-\nmetricswhenusingthesametrainingstrategy.Interestingly, beddingsbeforefurtherrefinementcanremarkablypromote\nignoring the confidence loss of the suboptimal modes that the forecasting capability. To gain deeper insights into the\nmatchthegroundtruthcanimprovetheperformanceofboth results,wedevelopavariantoflabelassignment,wherethe\nmethodsundertheWTAtraining. Thereasonbehindthisis modes decoded earlier than the first match are deemed ig-\nthat treating the other matched modes as negative samples noredsamples.Wefoundthisstrategytooutperformthede-\n7\nPAm\ntfoS\nRM\n(a)#Mode@Training=3,#Mode@Inference=3 (b)#Mode@Training=6,#Mode@Inference=6 (c)#Mode@Training=6,#Mode@Inference=24\nFigure4.VisualizationontheWOMD.Theagentsinpurplearepredictedwithbluetrajectories,withtheopacityindicatingconfidence.\nfault one in the absence of mode rearrangement, while the #Mode\nconclusionreverseswhenwereorderthemodesinbetween\nModel\nTraining Inference\nSoftmAP6↑ mAP6↑ MR6↓\nlayers. Thisphenomenoncanbeexplainedbythefactthat 3 3 0.4214 0.4163 0.2007\nQCNet[54]\nbadmodesmayappearinthefirstfewdecodingstepsofthe 6 6 0.4508 0.4452 0.1254\nshallowlayers,whichcannegativelyimpactthelearningof 3 3 0.4509 0.4479 0.1967\nModeSeq(Ours)\nthesubsequentmodes. Bymanuallyputtingthelessconfi- 6 6 0.4562 0.4507 0.1206\ndentmodestotheendofthesequence,weenablethemodel Table5. Capabilityofgeneratingrepresentativemodeswithpre-\ntoprioritizetherefinementofthemoreprobabletrajectories ciseconfidencescores. Modelsaretrainedon100%trainingdata\ninthenextlayer. Withoutrearrangement,wehavetointen- andevaluatedonthevalidationsplitoftheWOMD.\ntionallyassignmonotonicallydecreasinglabelsbyblocking 2.1 0.38\nthetraininglossoftheearlymismatches,aimingatimplic- 2.0 0.35\nitlyguidingthemodeltooutputmoreconfidentmodesfirst. 1.9\n0.32\nCapability of Representative Mode Learning. We 1.8\ndemonstrate ModeSeq’s ability to produce representative 0.29\n1.7\nmodesinTab.5. Whiletrainingmodelstodecodemerely3\n1.6 0.26\nmodesnecessarilyleadstoworseperformance,the3-mode\nvariantofModeSeqachievesthesamelevelofperformance 1.5 0.23\n612 24 48 96 612 24 48 96\nonSoftmAP andmAP comparedwiththe6-modemodel.\n6 6 #Mode@Inference #Mode@Inference\nBycomparison,QCNet[54]failstoachievecomparablere- Figure 5. The results of generating more than 6 modes on the\nsultsifonlyusing3modequeriesduringtraining. validationsetoftheWOMD.\nCapability of Mode Extrapolation. We ask the model tial mode modeling. The framework comprises a mecha-\ntrained by generating 6 modes to execute more decoding nismofsequentialmultimodaldecoding,anarchitectureof\nstepsattesttime. AsdepictedinFig.5,ModeSeqachieves iterativerefinementwithmode rearrangement, and atrain-\nlower prediction error with the increase of the decoded ingstrategyofEarly-Match-Take-Alllabelassignment. As\nmodes,emergingwiththecapabilityofmodeextrapolation analternativetotheunorderedmultimodaldecodingandthe\nthanks to sequential modeling. This characteristic enables winner-take-all training strategy, ModeSeq achieves state-\nhandlingvariousdegreesofuncertaintyacrossscenarios. of-the-artresultsonmotionpredictionbenchmarksandex-\nhibits the characteristic of mode extrapolation, creating a\n4.4.QualitativeResults\nnewpathtosolvingmultimodalproblems.\nThequalitativeresultsproducedbyModeSeqarepresented Limitations. Ourapproachisstillflawedinsomerespects.\ninFig.4. Figure4ademonstratesthatourmodelcangener- First, the sequential generation of modes is less efficient\naterepresentativetrajectorieswhenbeingtrainedtodecode than one-shot predictions, which necessitates an improve-\nonly3modes. ComparingFig.4cwithFig.4b,wecansee ment in efficiency. Second, although our approach sup-\nthatthe6-modemodelsuccessfullyextrapolatesdiverseyet ports multi-agent forecasting in parallel, we only explore\nrealisticmodeswhenexecuting24decodingstepsduringin- marginalmulti-agentprediction,lackingvalidationonjoint\nference,showcasingtheextrapolationabilityofModeSeq. prediction. Future research may involve extending Mode-\nSeqintoajointmulti-agentmodel.\n5.Conclusion Acknowledgement. This project is supported by a\ngrant from Hong Kong Research Grant Council un-\nThispaperintroducesModeSeq,amodelingframeworkthat der GRF project 11216323 and CRF C1042-23G.\nachieves sparse multimodal motion prediction via sequen-\n8\nEDFnim\nRM\nReferences [16] Tong Li, Lu Zhang, Sikang Liu, and Shaojie Shen. Marc:\nMultipolicy and risk-aware contingency planning for au-\n[1] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan,\ntonomousdriving. RA-L,2023. 1\nAlexandreRobicquet, LiFei-Fei, andSilvioSavarese. So-\n[17] MingLiang,BinYang,RuiHu,YunChen,RenjieLiao,Song\nciallstm:Humantrajectorypredictionincrowdedspaces.In\nFeng,andRaquelUrtasun. Learninglanegraphrepresenta-\nCVPR,2016. 2,3\ntionsformotionforecasting. InECCV,2020. 1,2\n[2] ChristopherMBishop. Mixturedensitynetworks. 1994. 2\n[18] Longzhong Lin, Xuewu Lin, Tianwei Lin, Lichao Huang,\n[3] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas\nRong Xiong, and Yue Wang. Eda: Evolving and distinct\nUsunier,AlexanderKirillov,andSergeyZagoruyko.End-to-\nanchorsformultimodalmotionprediction. InAAAI,2024.\nendobjectdetectionwithtransformers. InECCV,2020. 2,\n1,5\n3,5,7\n[19] Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,and\n[4] YuningChai,BenjaminSapp,MayankBansal,andDragomir\nPiotrDolla´r.Focallossfordenseobjectdetection.InCVPR,\nAnguelov. Multipath: Multipleprobabilisticanchortrajec-\n2017. 6\ntoryhypothesesforbehaviorprediction. InCoRL,2019. 1,\n2 [20] ShilongLiu,FengLi,HaoZhang,XiaoYang,XianbiaoQi,\n[5] Kyunghyun Cho, Bart van Merrienboer, C¸aglar Gu¨lc¸ehre, HangSu,JunZhu,andLeiZhang. Dab-detr: Dynamican-\nDzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and chorboxesarebetterqueriesfordetr. InICLR,2022. 3\nYoshua Bengio. Learning phrase representations using [21] YichengLiu,JinghuaiZhang,LiangjiFang,QinhongJiang,\nrnn encoder-decoder for statistical machine translation. In andBoleiZhou. Multimodalmotionpredictionwithstacked\nEMNLP,2014. 2,5 transformers. InCVPR,2021. 2\n[6] Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou, [22] IlyaLoshchilovandFrankHutter. Sgdr:Stochasticgradient\nTsung-HanLin,ThiNguyen,Tzu-KuoHuang,JeffSchnei- descentwithwarmrestarts. InICLR,2017. 6\nder,andNemanjaDjuric. Multimodaltrajectorypredictions [23] IlyaLoshchilovandFrankHutter. Decoupledweightdecay\nforautonomousdrivingusingdeepconvolutionalnetworks. regularization. InICLR,2019. 6\nInICRA,2019. 1,2\n[24] OsamaMakansi,EddyIlg,OzgunCicek,andThomasBrox.\n[7] NachiketDeoandMohanMTrivedi. Convolutionalsocial Overcominglimitationsofmixturedensitynetworks:Asam-\npoolingforvehicletrajectoryprediction. InCVPRW,2018. plingandfittingframeworkformultimodalfutureprediction.\n2 InCVPR,2019. 1,2,5\n[8] WenchaoDing,LuZhang,JingChen,andShaojieShen.Ep-\n[25] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng,\nsilon:Anefficientplanningsystemforautomatedvehiclesin\nHouqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang.\nhighlyinteractiveenvironments. T-RO,2021. 1\nConditional detr for fast training convergence. In ICCV,\n[9] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi\n2021. 3\nLiu, HangZhao, SabeekPradhan, YuningChai, BenSapp,\n[26] Jean Mercat, Thomas Gilles, Nicole El Zoghby, Guil-\nCharlesR.Qi,YinZhou,ZoeyYang,Aure´lienChouard,Pei\nlaume Sandou, Dominique Beauvois, and Guillermo Pita\nSun,JiquanNgiam,VijayVasudevan,AlexanderMcCauley,\nGil. Multi-headattentionformulti-modaljointvehiclemo-\nJonathon Shlens, and Dragomir Anguelov. Large scale in-\ntionforecasting. InICRA,2020. 2\nteractive motion forecasting for autonomous driving: The\n[27] NigamaaNayakanti,RamiAl-Rfou,AurickZhou,Kratarth\nwaymoopenmotiondataset. InICCV,2021. 2,5,6,1\nGoel, Khaled S Refaat, and Benjamin Sapp. Wayformer:\n[10] FrancescoGiuliari,IrtizaHasan,MarcoCristani,andFabio\nMotionforecastingviasimple&efficientattentionnetworks.\nGalasso.Transformernetworksfortrajectoryforecasting.In\nInICRA,2023. 1,2,3,5\nICPR,2020. 2\n[28] Jiquan Ngiam, Benjamin Caine, Vijay Vasudevan, Zheng-\n[11] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese,\ndongZhang,Hao-TienLewisChiang,JeffreyLing,Rebecca\nandAlexandreAlahi.Socialgan:Sociallyacceptabletrajec-\nRoelofs,AlexBewley,ChenxiLiu,AshishVenugopal,David\ntorieswithgenerativeadversarialnetworks. InCVPR,2018.\nWeiss,BenSapp,ZhifengChen,andJonathonShlens.Scene\n2\ntransformer: A unified architecture for predicting multiple\n[12] SeppHochreiterandJu¨rgenSchmidhuber. Longshort-term\nagenttrajectories. InICLR,2022. 2\nmemory. NeuralComputation,1997. 2,5\n[29] TungPhan-Minh,ElenaCorinaGrigore,FreddyABoulton,\n[13] JoeyHong,BenjaminSapp,andJamesPhilbin. Rulesofthe\nOscar Beijbom, and Eric M Wolff. Covernet: Multimodal\nroad:Predictingdrivingbehaviorwithaconvolutionalmodel\nbehaviorpredictionusingtrajectorysets. InCVPR,2020. 2\nofsemanticinteractions. InCVPR,2019. 2\n[14] NamhoonLee,WongunChoi,PaulVernaza,ChristopherB [30] JonahPhilion,XueBinPeng,andSanjaFidler. Trajeglish:\nChoy,PhilipHSTorr,andManmohanChandraker. Desire: Trafficmodelingasnext-tokenprediction. InICLR,2024. 2\nDistantfuturepredictionindynamicsceneswithinteracting [31] NicholasRhinehart,KrisMKitani,andPaulVernaza.R2p2:\nagents. InCVPR,2017. 2 A reparameterized pushforward policy for diverse, precise\n[15] Stefan Lee, Senthil Purushwalkam Shiva Prakash, Michael generativepathforecasting. InECCV,2018. 2\nCogswell,VireshRanjan,DavidCrandall,andDhruvBatra. [32] Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and\nStochasticmultiplechoicelearningfortrainingdiversedeep SergeyLevine. Precog: Predictionconditionedongoalsin\nensembles. InNIPS,2016. 1,2,3,5 visualmulti-agentsettings. InICCV,2019. 2,3\n9\n[33] Luke Rowe, Martin Ethier, Eli-Henry Dykhne, and [49] BenjaminWilson,WilliamQi,TanmayAgarwal,JohnLam-\nKrzysztof Czarnecki. Fjmp: Factorized joint multi-agent bert,JagjeetSingh,SiddheshKhandelwal,BowenPan,Rat-\nmotion prediction over learned directed acyclic interaction nesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes,\ngraphs. InCVPR,2023. 3 DevaRamanan, PeterCarr, andJamesHays. Argoverse2:\n[34] Christian Rupprecht, Iro Laina, Robert DiPietro, Maximil- Nextgenerationdatasetsforself-drivingperceptionandfore-\nianBaust,FedericoTombari,NassirNavab,andGregoryD casting. InNeurIPSDatasetsandBenchmarks,2021. 2,5,\nHager. Learning in an uncertain world: Representing am- 6,1\nbiguitythroughmultiplehypotheses. InICCV,2017. 1,2, [50] CunjunYu,XiaoMa,JiaweiRen,HaiyuZhao,andShuaiYi.\n5 Spatio-temporal graph transformer networks for pedestrian\n[35] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and trajectoryprediction. InECCV,2020. 2\nMarcoPavone. Trajectron++: Dynamically-feasibletrajec- [51] Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Ben Sapp,\ntory forecasting with heterogeneous data. In ECCV, 2020. BalakrishnanVaradarajan,YueShen,YiShen,YuningChai,\n2 Cordelia Schmid, Congcong Li, and Dragomir Anguelov.\n[36] AriSeff,BrianCera,DianChen,MasonNg,AurickZhou, Tnt:Target-driventrajectoryprediction. InCoRL,2020. 2\nNigamaa Nayakanti, Khaled S Refaat, Rami Al-Rfou, and [52] Yang Zhou, Hao Shao, Letian Wang, Steven L Waslander,\nBenjaminSapp. Motionlm: Multi-agentmotionforecasting HongshengLi,andYuLiu.Smartrefine:Ascenario-adaptive\naslanguagemodeling. InICCV,2023. 2 refinement framework for efficient motion prediction. In\n[37] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self- CVPR,2024. 2\nattentionwithrelativepositionrepresentations. InNAACL, [53] ZikangZhou,LuyaoYe,JianpingWang,KuiWu,andKejie\n2018. 3 Lu. Hivt: Hierarchical vector transformer for multi-agent\n[38] ChenShi,ShaoshuaiShi,andLiJiang.Mtrv3:1stplaceso- motionprediction. InCVPR,2022. 1,2,6\nlutionfor2024waymoopendatasetchallenge-motionpre-\n[54] Zikang Zhou, Jianping Wang, Yung-Hui Li, and Yu-Kai\ndiction. InCVPR2024WorkshoponAutonomousDriving,\nHuang. Query-centrictrajectoryprediction. InCVPR,2023.\n2024. 6\n1,2,3,5,6,7,8\n[39] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele.\n[55] Zikang Zhou, Haibo Hu, Xinhong Chen, Jianping Wang,\nMotiontransformerwithglobalintentionlocalizationandlo-\nNan Guan, Kui Wu, Yung-Hui Li, Yu-Kai Huang, and\ncalmovementrefinement. InNeurIPS,2022. 1,2,3,5,6\nChunJasonXue. Behaviorgpt: Smartagentsimulationfor\n[40] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele.\nautonomousdrivingwithnext-patchprediction. InNeurIPS,\nMtr++:Multi-agentmotionpredictionwithsymmetricscene\n2024. 2\nmodelingandguidedintentionquerying. TPAMI,2024. 6,7\n[56] XizhouZhu,WeijieSu,LeweiLu,BinLi,XiaogangWang,\n[41] Roman Solovyev, Weimin Wang, and Tatiana Gabruseva.\nandJifengDai. Deformabledetr: Deformabletransformers\nWeightedboxesfusion:Ensemblingboxesfromdifferentob-\nforend-to-endobjectdetection. InICLR,2021. 3\njectdetectionmodels. ImageandVisionComputing,2021.\n1\n[42] JiaweiSun, JiahuiLi, TingchenLiu, ChengranYuan, Shuo\nSun, Zefan Huang, Anthony Wong, Keng Peng Tee, and\nMarcelo H Ang Jr. Rmp-yolo: A robust motion predictor\nforpartiallyobservablescenariosevenifyouonlylookonce.\narXivpreprintarXiv:2409.11696,2024. 6\n[43] Qiao Sun, Xin Huang, Junru Gu, Brian C Williams, and\nHang Zhao. M2i: From factored marginal trajectory pre-\ndictiontointeractiveprediction. InCVPR,2022. 3\n[44] YichuanCharlieTangandRuslanSalakhutdinov. Multiple\nfuturesprediction. InNeurIPS,2019. 2,3\n[45] LucaAnthonyThiedeandPratikPrabhanjanBrahma. Ana-\nlyzingthevarietylossinthecontextofprobabilistictrajec-\ntoryprediction. InICCV,2019. 1,2,5\n[46] Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivas-\ntava,KhaledSRefaat,NigamaaNayakanti,AndreCornman,\nKan Chen, Bertrand Douillard, Chi Pang Lam, Dragomir\nAnguelov, and Benjamin Sapp. Multipath++: Efficient in-\nformationfusionandtrajectoryaggregationforbehaviorpre-\ndiction. InICRA,2022. 1,2,3,5\n[47] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-\nreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia\nPolosukhin. Attentionisallyouneed. InNIPS,2017. 2\n[48] OriolVinyals,SamyBengio,andManjunathKudlur. Order\nmatters:Sequencetosequenceforsets. InICLR,2016. 3\n10\nModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode\nModeling\nSupplementary Material\n6.DefinitionofaMatch\nModeSeq(Ours) QCNet[54]\n#Mode=3 #Mode=6 #Mode=3 #Mode=6\nThe Argoverse 2 Motion Forecasting Benchmark [49] de-\nLatency(ms) 86±9 143±10 63±11 69±16\nsires the predictions’ displacement error at the 60-th time\nsteptobelessthan2meters.Bylinearlyscalingthe2-meter\nTable6. Comparisonsontheinferencelatencyaveragedoverthe\nthresholdacrosstimesteps, weobtainadistancethreshold\nvalidationsetoftheWOMD.\nΓ(t)foreachtimestept:\nt\nΓ(t)= . (9) bounding boxes according to IOU thresholds. Our ensem-\n30\nble method can improve mAP /Soft mAP /MR by sac-\n6 6 6\nOur EMTA training loss requires a matched trajectory to rificing minADE 6/minFDE 6, which indicates that the per-\nfall within the given threshold of the ground truth at every formance on various metrics sometimes disagrees. The\nfuturetimestep. criticalhyperparametersinWeightedTrajectoryFusionare\nOntheWaymoOpenMotionDataset(WOMD)[9],the the distance thresholds used for trajectory clustering. We\nthresholds are divided into lateral and longitudinal ones, choose the velocity-aware thresholds defined in Eq. (11)\nwhich are adaptive to the current velocity of the agent of andEq.(12)asthebasethresholds. Ontopofthis,wemul-\ninterest. To begin with, the benchmark defines a scaling tiplythebasethresholdswiththescalingfactorsof1.5,1.4,\nfactorwithrespecttothevelocityv: and1.4forvehicles,pedestrians,andcyclists,respectively.\n0.5 ifv <1.4; 8.InferenceLatency\n\nv−1.4\nScale(v)= 0.5+0.5 if1.4≤v <11; Asstatedinthemainpaper, aweaknessofourcurrentap-\n11−1.4\n proachliesintheinferencelatency,whichisabouttwiceas\n1 otherwise.\nhigh as that of QCNet [54] if predicting 6 modes accord-\n(10)\ning to the measurement in Tab. 6. However, in many real-\nUtilizing the scaling factor, we define the lateral threshold\nworldusecases,thenumberofmodesisrefrainedfrombe-\nΓ (v, t)as\nlat ing more than 3. As shown in Tab. 6, the gap in inference\n t latencybetweena3-modeModeSeqanda3-modeQCNet\n if1≤t≤30;\nismuchsmaller.Givenourapproach’scapabilityofproduc-\nΓ (v, t)=Scale(v)× 30\nlat\n0.04t−0.2 otherwise. ingrepresentativetrajectorieswithfewermodes,webelieve\nsequentialmodemodelinghasthepotentialtobedeployed\n(11)\non board. Our future work will focus on further reducing\nSimilarly,wesetthelongitudinalthresholdΓ (v, t)tobe\nlon\nthe inference cost by improving the architecture or distill-\ntwiceaslargeasthelateralone:\ningasmallmodelfromalargeone,whichisnecessaryfor\n t facilitatingreal-worldapplicationsofourapproach.\n if1≤t≤30;\nΓ (v, t)=Scale(v)× 15\nlon\n0.08t−0.4 otherwise. 9.MoreQualitativeResults\n(12)\nFigure 6 supplements the results in Fig. 4 to demonstrate\nRegarding the experiments on the WOMD, we demand a\nourapproach’sabilitytoproducerepresentativetrajectories\nmatchedtrajectorytohaveerrorsbelowboththelateraland\nandextrapolatemoremodes.\nlongitudinalthresholdsateveryfuturetimestep.\n7.EnsembleMethodontheWOMD\nInspired by Weighted Boxes Fusion (WBF) [41], we pro-\npose Weighted Trajectory Fusion to aggregate multimodal\ntrajectories produced by multiple models. Our ensemble\nmethod is almost the same as WBF, except we are fus-\ningtrajectoriesaccordingtodistancethresholdsratherthan\n1\n(a)#Mode@Training=3,#Mode@Inference=3 (b)#Mode@Training=6,#Mode@Inference=6 (c)#Mode@Training=6,#Mode@Inference=24\nFigure6.VisualizationontheWOMD.Theagentsinpurplearepredictedwithbluetrajectories,withtheopacityindicatingconfidence.\n2",
    "pdf_filename": "ModeSeq_Taming_Sparse_Multimodal_Motion_Prediction_with_Sequential_Mode_Modeling.pdf"
}