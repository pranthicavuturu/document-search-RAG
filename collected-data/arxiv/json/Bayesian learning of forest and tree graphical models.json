{
    "title": "Bayesian learning of forest and tree graphical models",
    "context": "",
    "body": "Bayesian learning of forest  \nand tree graphical models \n \nEdmund Jones \n \nA dissertation submitted to the University of Bristol  \nin accordance with the requirements for award of the  \ndegree of PhD in the Faculty of Science \n \nSchool of Mathematics \nStatistics Group \n \nMarch 2013 \n \n \n \n \n \n \n \n \n \n \n \nCopyright Â© 2013 Edmund Jones. All rights reserved. \nThis is a compact version of my PhD thesis, not the official version that was submitted to the \nUniversity of Bristol. The front matter has been shortened and the page design is different, but \nthe main content is the same. Minor corrections have been made, and these are listed at the end. \nResearch from this thesis has been published in two journal articles: \nE. Jones & V. Didelez (2016), Inequalities on partial correlations in Gaussian graphical models \ncontaining star shapes, Communications in Statistics â€“ Theory and Methods, 45 (20), 5990â€“5996. \nhttps://doi.org/10.1080/03610926.2014.953696 \nE. Jones & V. Didelez (2017), Thinning a triangulation of a Bayesian network or undirected graph \nto create a minimal triangulation, International Journal of Uncertainty, Fuzziness and Knowledge-\nBased Systems, 25 (3), 349â€“366. https://doi.org/10.1142/S0218488517500143 \n\n \n \n \nAbstract \nFrequentist methods for learning Gaussian graphical model structure are unsuccessful \nat identifying hubs when ğ‘›< ğ‘. An alternative is Bayesian structure-learning, in which \nit is common to restrict attention to certain classes of graphs and to explore and \napproximate the posterior distribution by repeatedly moving from one graph to another, \nusing MCMC or other methods such as stochastic shotgun search (SSS). I give two \ncorrected versions of an algorithm for non-decomposable graphs and discuss random \ngraph distributions in depth, in particular as priors in Bayesian structure-learning.  \nThe main topic of the thesis is Bayesian structure-learning with forests or trees. Forest \nand tree graphical models are widely used, and I explain how restricting attention to \nthese graphs can be justified using theorems on random graphs. I describe how to use \nmethods based on the Chowâ€“Liu algorithm and the Matrix Tree Theorem to find the \nMAP forest and certain quantities in the full posterior distribution on trees.  \nI give adapted versions of MCMC and SSS for approximating the posterior distribution \nfor forests and trees, and systems for storing these graphs so that it is easy and efficient \nto choose legal moves to neighbouring forests or trees and update the stored \ninformation. Experiments with the adapted algorithms and simulated datasets show \nthat the system for storing trees so that moves are chosen uniformly at random does \nnot bring much advantage over simpler systems. SSS with trees does well when the true \ngraph is a tree or a sparse graph. Graph priors improve detection of hubs but need large \nranges of probabilities to have much effect. SSS with trees and SSS with forests do \nbetter than SSS with decomposable graphs in certain cases. MCMC on forests often fails \nto mix well and MCMC on trees is much slower than SSS. \n \n \n \n \nAcknowledgements \nI would like to thank my supervisor, Vanessa Didelez, for all her advice and guidance. I \nwould also like to thank staff and students of Bristol University and express my \ngratitude for the funding I have received through the Engineering and Physical Sciences \nResearch Council.  \n \n \n\nContents \n1 \nIntroduction 1 \n \n1.1 \nBackground 1 \n1.2 \nThe subjects of this thesis 1 \n1.3 \nStructure of the thesis 2 \n1.4 \nSummary of main contributions 4 \n1.5 \nThe meanings of ğ‘› and ğ‘ 4 \n2 \nGraphs and graphical models 5 \n2.1 \nGraphs 5 \n2.2 \nGraphical models 7 \n2.3 \nThe covariance and precision matrices for GGMs 9 \n2.4 \nBiomolecular networks 12 \n2.5 \nSupplementary notes: alternative terms and the history of graphical \nmodels 15 \n3 \nStructure-learning for GGMs 17 \n3.1 \nBayesian methods 17 \n3.2 \nFrequentist methods 24 \n4 \nCorrections to an algorithm for recursive thinning 26 \n4.1 \nMaximal prime decomposition and minimal triangulation 26 \n4.2 \nRecursive thinning 27 \n4.3 \nNotation 28 \n4.4 \nThe incorrect algorithm 28 \n4.5 \nHow the incorrect algorithm goes wrong 29 \n4.6 \nA correct algorithm 30 \n4.7 \nA second correct algorithm 31 \n4.8 \nComments on the two correct algorithms 34 \n4.9 \nWhich of the correct algorithms is faster? 34 \n4.10 What is the best algorithm for minimal triangulation? 35 \n5 \nRandom graph distributions 36 \n5.1 \nTwo ways of looking at graph distributions 36 \n5.2 \nErdÅ‘sâ€“RÃ©nyi random graphs 37 \n5.3 \nComplex networks 37 \n5.4 \nFactored distributions 38 \n5.5 \nGraph priors that have been proposed 40 \n5.6 \nGraph priors based on random graph models 42 \n5.7 \nPractical graph prior distributions 44 \n\n6 \nForest and tree graphs and graphical models 47 \n6.1 \nWhy consider forest and tree graphical models? 47 \n6.2 \nThe claim that sparse graphs are locally tree-like 49 \n7 \nThe Chowâ€“Liu algorithm 59 \n7.1 \nFinding the optimal tree 59 \n7.2 \nKruskalâ€™s algorithm 60 \n7.3 \nRelevant developments since Chowâ€“Liu 61 \n7.4 \nFinding the MAP forest 62 \n7.5 \nSupplementary notes 65 \n8 \nMethods for factored distributions on trees 67 \n8.1 \nIntroduction and the Matrix Tree Theorem 67 \n8.2 \nThe normalizing constant for discrete-valued tree graphical models 69 \n8.3 \nThe normalizing constant for GGMs 70 \n8.4 \nGenerating random trees or forests 72 \n8.5 \nSupplementary notes: the history of MTT 72 \n9 \nLocal moves in forests and trees 74 \n9.1 \nPreamble 74 \n9.2 \nStoring forests and trees for local moves 74 \n9.3 \nThe system for storing a forest 78 \n9.4 \nThe system for storing a tree 83 \n9.5 \nSupplementary notes: PrÃ¼fer sequences 93 \n10 Algorithms for exploring the posterior distribution 94 \n10.1 Adaptations of two algorithms 94 \n10.2 Analyzing posterior graph distributions and assessing algorithms 101 \n11 Experiments 107 \n11.1 Facts about star and chain graphs 107 \n11.2 Experiments with systems for storing trees 108 \n11.3 Experiments with non-forests 115 \n11.4 Experiments with MCMC on forests and trees 116 \n11.5 Experiments with methods for trees 122 \n11.6 Experiments with graph prior distributions 123 \n11.7 Experiments with forests, trees, and decomposable graphs 125 \n12 Conclusions 128 \n12.1 Restricting to forests and trees 128 \n12.2 Graph distributions and theoretical results 128 \n12.3 Algorithms for structure-learning with forests or trees 129 \n12.4 Computer experiments 129 \nAppendix I: Graph enumerations 131 \nAppendix II: Glossary of terms related to graphs 133 \nAppendix III: Asymptotic notations 135 \nReferences 136 \n\n \n1 \n1 \nIntroduction \n1.1 \nBackground  \nIn recent years, high-throughput methods and increases in computing power have seen \nhuge increases in the amount of data on DNA and other biomolecules. Much of this data \nis amenable to analysis by statistical methods. One example is the use of probabilistic \ngraphical models to analyze gene regulation networks. The key task is to deduce the \nstructure of the graphical model from the numerical expression values of a set of genes, \nobserved in a set of cells. These values are measured using microarrays.  \nThis task involves two types of â€œsparsityâ€. Firstly, the number of observations is usually \nmuch less than the number of variables (which is the number of nodes). This is the \nissue of â€œğ‘›< ğ‘â€, a major topic in statistics. Secondly, the graph is believed to have few \nedges.  \nAlbieri (2010) considered three frequentist algorithms for learning the structure of \nGaussian graphical models (GGMs) from numerical data with ğ‘›< ğ‘. She used these \nalgorithms on expression values for a set of E. coli genes for which the true graph \nstructure had been deduced by biological experiments, and on several simulated \ndatasets that were generated using known graph structures. She found that when the \ntrue graph contained hubs (nodes that are connected to many other nodes), the \nalgorithms tended to produce graphs in which the hub and all the nodes connected to it \nformed a complete subgraph, making it impossible to tell which node was the hub. Hubs \nare one of the most notable features of gene regulation networks and other real-world \nnetworks, so these results suggest that the frequentist algorithms may be unsatis-\nfactory for learning the structures of these networks.  \n1.2 \nThe subjects of this thesis \nThe main subject of this thesis is Bayesian structure-learning for GGMs in the cases \nwhere attention is restricted to forests or trees. Forests are graphs that contain no \ncycles, and trees are connected forests (see Figure 1.1). Forests and trees are sparse \nand they exclude the possibility of the large complete subgraphs produced by the \nalgorithms in Albieri (2010).  \nOne of the main questions addressed by the thesis is whether it is sensible to restrict \nattention to forests or trees when there are existing methods that work on wider \nclasses of graphs. I have done numerous experiments to answer this question. Another \nis, how should different algorithms for Bayesian structure-learning be evaluated and \n\n1 Introduction \n \n 2 \ncompared? The thesis is also about prior distributions on the graph structure. One way \nto improve the discovery of hubs is to use a prior that gives higher probability to graphs \nthat contain hubs.  \nRestricting to forests or trees and using prior distributions can both be regarded as \nways to overcome the difficulties identified by Albieri (2010). But they also have \nbroader applicability and raise new questions. The thesis is mainly about Gaussian \ngraphical models, though some of the results and algorithms are valid for other types of \ngraphical models.  \n \n \nFigure 1.1. Left to right: a forest, a tree, and a graph that is neither.  \n \n1.3 \nStructure of the thesis \nChapter 2 gives an introduction to graphs and graphical models. Chapter 3 describes \nthe standard Bayesian method for structure-learning of GGMs, in which the number of \nnodes is fixed and every possible graph has a prior and posterior probability. Next is a \nreview of frequentist methods, including the main ones used by Albieri (2010).  \nChapter 4 explains corrections to an algorithm that is used on non-decomposable \ngraphs in Bayesian structure-learning. The purpose of the algorithm is to remove some \nof a set of extra edges to leave a minimal graph that is still triangulated. I present two \ncorrected versions of the algorithm and detailed discussions of how the original \nalgorithm goes wrong and which of the corrected versions is better.  \nThe prior and posterior distributions on the graph structure are random distributions \non the space of graphs with a fixed number of nodes. These random distributions are \ndiscussed in depth in chapter 5. Firstly I present two ways of looking at these \ndistributions, â€œrandom graph modelsâ€ and â€œgraph distributionsâ€. I describe the main \ndistributions that have been studied outside the field of graphical models. I then give \nseveral definitions of what I call â€œfactoredâ€ distributions. These can be used in several of \nthe algorithms for structure-learning that appear in subsequent chapters. (However, \nthey cannot be used as priors that encourage hubs, so I do not use them in my own \nexperiments.) Next I review distributions that have been used as priors in Bayesian \nstructure-learning and discuss the possibility of using graph priors based on random \ngraph models. Finally I present desirable criteria for graph priors and some possible \npriors that fulfil these criteria.  \n\n \n1.3 Structure of the thesis \n \n3 \nChapter 6 is about forest and tree graphical models. I give several reasons why it can be \nsensible to restrict attention to these relatively small classes of graphs and a detailed \nand formal consideration of one of these, the notion that sparse graphs are locally tree-\nlike.  \nChapters 7 and 8 are about fast algorithms for forest and tree graphical model \nstructure-learning. Chapter 7 is about the Chowâ€“Liu algorithm, which finds the \nmaximum-likelihood tree graphical model. Adaptations of this algorithm can be used to \nfind a forest, using penalized likelihood, or to find the most likely graph in Bayesian \nstructure-learning restricted to trees or forests.  \nChapter 8 is about Bayesian structure-learning on trees using methods based on the \n19th-century Matrix Tree Theorem. A previously published paper explained how this \ntheorem can be used to find certain quantities exactly in polynomial time. I show how \nthe method works for GGMs and how it can be used to find certain useful quantities \nsuch as the posterior expected degrees of the nodes or the expected true-positive rate.  \nThe algorithms in chapters 7 and 8 are fast and produce objects that may be useful in \nBayesian structure-learning. But to estimate other quantities and objects, or to produce \nan estimate of the entire posterior distribution, it is necessary to visit large numbers of \nindividual graphs. With 15 or more nodes, the number of possible graphs is so large \nthat it is computationally infeasible to calculate the posterior probabilities of all of them. \nThis is true even when only forests or trees are considered. Instead, there are \nalgorithms that approximate the posterior distribution by exploring the space of \npossible graphs.  \nIn chapter 9, I propose new systems for storing forests and trees so that â€œlocal movesâ€ \nto other forests or trees can be made easily and efficiently. For both types of graph, local \nmoves can be chosen uniformly at random from among all possible moves. Section 10.1 \ndescribes how to adapt two previously published algorithms for Bayesian structure-\nlearning of GGMs so that they can be used on forests and trees. These algorithms are the \nreversible-jump MCMC of Giudici & Green (1999) and the stochastic shotgun search \n(SSS) of Jones et al (2005). Section 10.2 is about how to evaluate and compare \nfrequentist and Bayesian methods for structure-learning. The graph or graphs \nproduced by the algorithm can be compared to the true graph, if that is known.  \nChapter 11 mostly consists of computer experiments to evaluate and compare the \nalgorithms and systems in chapters 8â€“10 and answer the question of whether it is \nsensible to restrict attention to forests or trees. Section 11.1 gives three new facts about \ntwo types of graph, stars and chains, to show that these graphs are extremal in senses \nto do with the numbers of local moves (equivalently, the numbers of neighbouring \ngraphs). For these reasons stars and chains are used in most of the experiments in the \nrest of the chapter.  \nSection 11.2 is about experiments to compare the system for storing trees described in \nsection 9.4 with three alternative systems, using one of my versions of the SSS algo-\nrithm. Section 11.3 has experiments on datasets for which the true graph is not a forest \nbut is sparse and locally tree-like, to see whether restricting attention to trees produces \ngood results in this case.  \n\n1 Introduction \n \n 4 \nSection 11.4 describes experiments with my versions of the reversible-jump MCMC. \nSection 11.5 compares SSS restricted to trees with the methods from chapter 8, which \ncalculate exact posterior quantities.  \nSection 11.6 has experiments with graph prior distributions that are designed to \nencourage hubs. These priors are compared with the uniform distribution, which has \nbeen the most commonly used graph prior in previous research. Finally, section 11.7 \ncompares SSS on trees, SSS on forests, and SSS on decomposable graphs, again to \naddress the question of whether it is sensible to restrict attention to trees or forests.  \nChapter 12 presents discussions and possibilities for future research. Appendix I gives \nthe results of some new graph enumerations, including the number of decomposable \ngraphs with 13 nodes, Appendix II is a glossary of terms from graph theory, and \nAppendix III defines asymptotic notations.  \n1.4 \nSummary of main contributions  \nThe main contributions of this thesis are as follows.  \nâ€¢ \n(Chapter 4) Corrections to an algorithm for recursive thinning, including \nexplanation of how the algorithm goes wrong and two correct algorithms, with proofs.  \nâ€¢ \n(Section 6.2) Rigorous investigation of the notion that sparse graphs are locally \ntree-like.  \nâ€¢ \n(Section 8.2) Explanations of how a previously published algorithm can be used for \nBayesian structure-learning of tree GGMs and for finding the expected posterior \nvalues of certain quantities.  \nâ€¢ \n(Chapter 9) Systems and algorithms for storing forests and trees so that local moves \ncan be made easily and uniformly at random, and numerous propositions related to these.  \nâ€¢ \n(Section 10.1) Modifications of two previously published algorithms for Bayesian \nstructure-learning of GGMs so that they can be used on forests and trees.  \nâ€¢ \n(Chapter 11) Experiments to assess the systems for storing forests and trees, assess \nhow structure-learning with trees performs when the true graph has cycles, \ncompare different graph prior distributions, and compare structure-learning with \ntrees and forests to structure-learning with decomposable graphs.  \n1.5 \nThe meanings of n and p \nIn the ErdÅ‘sâ€“RÃ©nyi random graph model ğº(ğ‘›, ğ‘), ğ‘› is the number of nodes and ğ‘ is the \nprobability of each edge being present. But in multivariate statistics, ğ‘› is usually the \nnumber of data and ğ‘ is the dimension of the problem, which in graphical models is the \nnumber of nodes. Both these systems of notation are very standard in their respective fields.  \nBoth ErdÅ‘sâ€“RÃ©nyi graphs and multivariate statistics arise many times in this thesis, but \nseldom close to each other. So I use standard notation throughout, except briefly in \nsection 11.3. The meanings of ğ‘› and ğ‘ are consistent within individual chapters, but not \nwithin the whole thesis. The meanings are stated when the two letters first appear in \neach chapter and should also be obvious from the context. (Note that ğ‘(â‹…) is also used to \nmean probability density functions, and in chapter 6 the number of data is ğ‘š.) \n\n \n5 \n2 \nGraphs and graphical models \n2.1 \nGraphs \nBasic definitions \nThese definitions are sufficient for this thesis and are not the most general. See also \nAppendix II, which is a glossary of relevant terms.  \nâ€¢ \nA graph ğº is a pair (ğ‘‰, ğ¸), where ğ‘‰ is a finite set of nodes, also known as vertices, \nand ğ¸ is a set of edges.  \nâ€¢ \nIn an undirected graph, the elements of ğ¸ are unordered pairs (ğ‘¢, ğ‘£), where ğ‘¢, ğ‘£âˆˆğ‘‰. \n(Standard practice is to write unordered pairs using braces, as {ğ‘¢, ğ‘£}, but I use \nregular parentheses, like Edwards et al 2010.)  \nâ€¢ \nIn a directed graph, the elements of ğ¸ are ordered pairs (ğ‘¢, ğ‘£), where ğ‘¢, ğ‘£âˆˆğ‘‰.  \nAll graphs considered in this thesis are simple, which means they do not have multiple \nedges or self-loops. In other words, all the elements of ğ¸ are distinct, in directed graphs \nif (ğ‘¢, ğ‘£) âˆˆğ¸ then (ğ‘£, ğ‘¢) âˆ‰ğ¸, and in both types of graph if (ğ‘¢, ğ‘£) âˆˆğ¸ then ğ‘¢â‰ ğ‘£.  \nOf course graphs are usually thought of visually. The nodes are dots, and the edges are \nlines between pairs of dots. A directed edge (ğ‘¢, ğ‘£) is drawn as an arrow from ğ‘¢ to ğ‘£.  \nâ€¢ \nA subgraph of ğº is a graph ğ»= (ğ‘‰â€², ğ¸â€²) where ğ‘‰â€² âŠ†ğ‘‰, ğ¸â€² âŠ†ğ¸, and ğ‘¢, ğ‘£âˆˆğ‘‰â€² for all \n(ğ‘¢, ğ‘£) âˆˆğ¸â€². The notation ğ»âŠ†ğº means that ğ» is a subgraph of ğº.  \nâ€¢ \nAn induced subgraph is a subgraph (ğ‘‰â€², ğ¸â€²) in which ğ‘‰â€² âŠ†ğ‘‰ and ğ¸â€² = {(ğ‘¢, ğ‘£) âˆˆ\nğ¸: ğ‘¢, ğ‘£âˆˆğ‘‰â€²}.  \nâ€¢ \nğ‘‰ğº means the set of nodes in ğº and ğ¸ğº means the set of edges in ğº. If (ğ‘‰â€², ğ¸ğ‘‰â€²) is an \ninduced subgraph then ğ¸ğ‘‰â€² means the set of edges in the subgraph induced by ğ‘‰â€².  \nâ€¢ \nThe set of neighbours of ğ‘£ is ğ‘›ğ‘’(ğ‘£) = {ğ‘¢âˆˆğ‘‰: (ğ‘¢, ğ‘£) âˆˆğ¸ or (ğ‘£, ğ‘¢) âˆˆğ¸}.  \nâ€¢ \nThe degree of ğ‘£ is deg ğ‘£= |ğ‘›ğ‘’(ğ‘£)|.  \nâ€¢ \nThe size of ğº is |ğ¸|.  \nPaths \nâ€¢ \nIn an undirected graph, a path is a sequence of distinct nodes (ğ‘¢1, ğ‘¢2, â€¦ , ğ‘¢ğ‘˜) such \nthat (ğ‘¢1, ğ‘¢2), (ğ‘¢2, ğ‘¢3), â€¦ , (ğ‘¢ğ‘˜âˆ’1, ğ‘¢ğ‘˜) âˆˆğ¸.  \nâ€¢ \nIn a directed graph, a directed path is a sequence of distinct nodes (ğ‘¢1, ğ‘¢2, â€¦ , ğ‘¢ğ‘˜) \nsuch that (ğ‘¢1, ğ‘¢2), (ğ‘¢2, ğ‘¢3), â€¦ , (ğ‘¢ğ‘˜âˆ’1, ğ‘¢ğ‘˜) âˆˆğ¸. It is natural to refer to this as a \ndirected path from ğ‘¢1 to ğ‘¢ğ‘˜. (This is sometimes used as the definition of a â€œpathâ€ in \na directed graphâ€”for example, see Lauritzen 1996, page 6.)  \n\n2 Graphs and graphical models \n \n 6 \nâ€¢ \nIn a directed graph, a path is a sequence of distinct nodes (ğ‘¢1, ğ‘¢2, â€¦ , ğ‘¢ğ‘˜) such that \neither (ğ‘¢1, ğ‘¢2) âˆˆğ¸ or (ğ‘¢2, ğ‘¢1) âˆˆğ¸, either (ğ‘¢2, ğ‘¢3) âˆˆğ¸ or (ğ‘¢3, ğ‘¢2) âˆˆğ¸, â€¦, and either \n(ğ‘¢ğ‘˜âˆ’1, ğ‘¢ğ‘˜) âˆˆğ¸ or (ğ‘¢ğ‘˜, ğ‘¢ğ‘˜âˆ’1) âˆˆğ¸. I may refer to (ğ‘¢1, ğ‘¢2, â€¦ , ğ‘¢ğ‘˜) as a path from ğ‘¢1 to \nğ‘¢ğ‘˜, but â€œfromâ€ and â€œtoâ€ do not imply that the path is directed.  \nâ€¢ \nIn an undirected graph, a cycle is a path (ğ‘¢1, ğ‘¢2, â€¦ , ğ‘¢ğ‘˜) where ğ‘˜â‰¥3 and (ğ‘¢ğ‘˜, ğ‘¢1) âˆˆğ¸. \nâ€¢ \nThe girth of a graph is the length of its shortest cycle, or infinity if it has no cycles.  \nâ€¢ \nFor a graph to be connected means that there is a path between any two nodes.  \nâ€¢ \nSuppose ğ´, ğµ, and ğ¶ are induced subgraphs of ğº with no nodes in common. ğ¶ \nseparates ğ´ and ğµ if any path between a node in ğ´ and a node in ğµ includes a node \nin ğ¶.  \nDefinitions that only apply to directed graphs \nâ€¢ \nIn an edge (ğ‘¢, ğ‘£), ğ‘¢ is called the parent and ğ‘£ is called the child.  \nâ€¢ \nThe set of children of ğ‘£ is ğ‘â„(ğ‘£) = {ğ‘¢âˆˆğ‘‰: (ğ‘£, ğ‘¢) âˆˆğ¸}. \nâ€¢ \nThe set of parents of ğ‘£ is ğ‘ğ‘(ğ‘£) = {ğ‘¢âˆˆğ‘‰: (ğ‘¢, ğ‘£) âˆˆğ¸}.  \nâ€¢ \nThe set of descendants of ğ‘£ is ğ‘‘ğ‘’(ğ‘£) = {ğ‘¢âˆˆğ‘‰: there is a directed path from ğ‘£ to ğ‘¢}.  \nâ€¢ \nThe set of ancestors of ğ‘£ is ğ‘ğ‘›(ğ‘£) = {ğ‘¢âˆˆğ‘‰: there is a directed path from ğ‘¢ to ğ‘£}.  \nClasses of undirected graph \nâ€¢ \nA complete graph is an undirected graph where (ğ‘¢, ğ‘£) âˆˆğ¸ for all ğ‘¢, ğ‘£âˆˆğ‘‰. The \ncomplete graph on ğ‘ nodes is called ğ¾ğ‘.  \nâ€¢ \nA maximal complete subgraph ğ» of ğº is called a clique. (Maximal means there is no \ncomplete subgraph ğ»â€² of ğº such that ğ»âŠ†ğ»â€² and ğ»â‰ ğ»â€².)  \nâ€¢ \nA forest is an undirected graph that has no cycles. In a forest, any two nodes are \nconnected by at most one path.  \nâ€¢ \nA tree is a connected forest. In a tree, any two nodes are connected by exactly one \npath.  \nâ€¢ \nFor a connected graph ğº= (ğ‘‰, ğ¸), a spanning tree of ğº is a tree ğ‘‡= (ğ‘‰, ğ¸â€²) such \nthat ğ¸â€² âŠ†ğ¸.  \nDecomposable graphs \nDecomposable graphs are a class of undirected graphs that is especially important in \ngraphical models. A proper decomposition of ğº is a pair of induced subgraphs \n((ğ´, ğ¸ğ´), (ğµ, ğ¸ğµ))  such that ğ‘‰= ğ´âˆªğµ, ğ´â‰ âˆ…, ğµâ‰ âˆ…, ğ´â‰ ğ‘‰, ğµâ‰ ğ‘‰, the induced \nsubgraph with node-set ğ¶= ğ´âˆ©ğµ is complete, and ğ¶ separates ğ´âˆ–ğ¶ from ğµâˆ–ğ¶. This \nğ¶ is called a separator. It may be possible to decompose ğ´ and ğµ further. Following \nrepeated decomposition, the subgraphs that cannot be decomposed any further are \ncalled the prime components of ğº. If all the prime components are cliques, then the \noriginal graph is said to be decomposable.  \nIf a graph is decomposable, then its cliques can be put in a perfect sequence (see the \ndefinition on pages 14â€“15 of Lauritzen 1996). As well as the list of cliques, a perfect \nsequence also gives a list of separators, which are sets of nodes that each induce a \ncomplete subgraph.  \n\n \n2.2 Graphical models \n \n7 \nThe lists of cliques and separators are used in expressions for the factorized joint \ndensity in graphical models (see section 2.2). The cliques are all distinct, but separators \ncan appear more than once in the list. For this reason the separators will be regarded as \na collection in which an element can appear more than once, rather than a set.  \nThe class of decomposable graphs is the same as the class of chordal graphs, which have \nbeen studied in graph theory. A chordal graph is one in which any cycle of length four or \nmore has a chordâ€”for any cycle (ğ‘¢1, ğ‘¢2, â€¦ , ğ‘¢ğ‘˜) where ğ‘˜â‰¥4 there is an edge (ğ‘¢ğ‘–, ğ‘¢ğ‘—) \nwhere ğ‘–, ğ‘—âˆˆ{1, â€¦ , ğ‘˜} and ğ‘¢ğ‘– and ğ‘¢ğ‘— are not adjacent in the cycle. For a proof that these \ntwo classes of graphs are equivalent, see Proposition 2.5 in Lauritzen (1996), which \nuses â€œweakly decomposableâ€ instead of â€œdecomposableâ€ and â€œtriangulatedâ€ instead of \nâ€œchordalâ€. In chapter 4 I will use the term â€œtriangulatedâ€. Section 2.5 gives other names \nfor this class of graphs.  \nTrees and forests \nChapter 9 will use many times the fact that in a tree there is precisely one path between \nany two nodes. Proposition 2.1 is another simple fact about trees that will be referred to \nin section 7.2.  \nProposition 2.1. Adding one edge to a tree creates a graph that has precisely one cycle.  \nProof. Suppose the tree is ğ‘‡ and the extra edge is ğ‘’= (ğ‘¢, ğ‘£). Being a tree, ğ‘‡ contains no \ncycles. So any cycle in ğ‘‡+ ğ‘’, meaning (ğ‘‰ğ‘‡, ğ¸ğ‘‡âˆª{ğ‘’}), must consist of ğ‘’ and a path from ğ‘¢ \nto ğ‘£ in ğ‘‡. Conversely, any path from ğ‘¢ to ğ‘£ in ğ‘‡ will give rise to a cycle when ğ‘’ is added. \nSince ğ‘‡ is a tree, there is precisely one path from ğ‘¢ to ğ‘£ in ğ‘‡, so ğ‘‡+ ğ‘’ contains precisely \none cycle. ï‚¨  \nFor another proof, see Theorem 2.1(b) in Even (1979).  \nAll trees and forests are decomposable. The cliques are the pairs of nodes that have \nedges between them. In trees, the separators are the nodes that have degree 2 or more. \nIn forests, the separators are the nodes that have degree 2 or more and the empty set. \nSo the cliques all have size 2 and the separators all have size 1 or 0.  \nDefinitions and facts to do with rooted trees are given in â€œFacts about rooted treesâ€, in \nsection 9.2.  \n2.2 \nGraphical models \nConditional independence and graphical models \nğ‘‹ and ğ‘Œ are conditionally independent given ğ‘ if ğ‘( ğ‘¥, ğ‘¦âˆ£âˆ£ğ‘§) = ğ‘( ğ‘¥âˆ£ğ‘§)ğ‘( ğ‘¦âˆ£âˆ£ğ‘§) for all \nğ‘¥ and ğ‘¦ and for all ğ‘§ such that ğ‘(ğ‘§) > 0. In symbols this is written as ğ‘‹ \n ğ‘Œ | ğ‘.  \nA graphical model consists of a graph in conjunction with a multivariate statistical \nmodel or family of models. Each node ğ‘£ of the graph represents a single variable, ğ‘‹ğ‘£. In \nthis thesis, it will be assumed that the joint density of these univariate random variables \nis positive and continuous with respect to a product measure. For more general cases, \nsee chapter 3 of Lauritzen (1996).  \n\n2 Graphs and graphical models \n \n 8 \nThe structure of the graph summarizes relations of conditional independence between \nthe variables. In undirected graphical models, if ğ‘¢, ğ‘£âˆˆğ‘‰ then (ğ‘¢, ğ‘£) âˆ‰ğ¸â‡’ğ‘‹ğ‘¢ \n ğ‘‹ğ‘£ | ğ‘‹ğ‘‰âˆ–{ğ‘¢,ğ‘£}. This is called the pairwise Markov property, and with the assumption \nabove it is equivalent to the local and global Markov properties (Lauritzen 1996, section \n3.2.1). In directed acyclic graphical models, also known as Bayesian networks, ğ‘‹ğ‘£ \n ğ‘‹ğ‘‰âˆ–ğ‘‘ğ‘’(ğ‘£) | ğ‘‹ğ‘ğ‘(ğ‘£) for all ğ‘£âˆˆğ‘‰.  \nGraphical models are used for specifying, analyzing, and interpreting complex relations \nbetween random variables. Much of this thesis is about graphical models where the \ngraph is a forest or a tree. For these graphs, there are simple equivalences between \ndirected and undirected graphical models. To explain these requires three definitions. \nSee section 9.2 for the definitions of â€œrooted treeâ€ and â€œrooted forestâ€; and for two \ngraphical models to be â€œMarkov-equivalentâ€ means that they imply the same \nconditional independence relations. The equivalence between the directed and \nundirected graphical models is that rooted trees or forests are Markov-equivalent to the \nundirected trees or forests formed by removing the direction from each edge.  \nGaussian graphical models \nThis thesis is mainly about Gaussian graphical models (GGMs), which are undirected. \nThese are one of the most widely studied types of graphical model. In a GGM, the \nvariables follow a multivariate normal distribution, ğ‘‹~ğ‘ğ‘(ğœ‡, Î£). One property of this \ndistribution is that ğ‘‹ğ‘– \n ğ‘‹ğ‘— | ğ‘‹ğ‘‰âˆ–{ğ‘–,ğ‘—} â‡”(Î£âˆ’1)ğ‘–ğ‘—= 0. (Here the nodes are identified with \nthe numbers {1, â€¦ , ğ‘}.) This can be seen by writing out the joint density and factorizing \nit, and has been known at least since Wermuth (1976). Using the definition of \nundirected graphical models, it follows that (ğ‘–, ğ‘—) âˆ‰ğ¸â‡’(Î£âˆ’1)ğ‘–ğ‘—= 0. In other words, the \nedges that are absent from the graph correspond to zeroes in the precision matrix ğ¾=\nÎ£âˆ’1 (also known as the concentration matrix).  \nThe object of interest is Î£ rather than ğœ‡, so it is common to set ğœ‡= 0. Data can easily be \ncentred so that ğ‘¥Ì… = 0. Suppose ğ‘‹ is an ğ‘›Ã— ğ‘ matrix that contains ğ‘› observations of a ğ‘-\nvariate Gaussian distribution, and let 1ğ‘› be an ğ‘›Ã— ğ‘› matrix of 1s. The centred matrix is \n(ğ¼ğ‘›âˆ’1ğ‘›/ğ‘›)ğ‘‹.  \nGGMs can be used to model gene regulation networks, as discussed in section 2.4, and \nfinancial objects such as currency values (Carvalho et al 2007) and asset returns \n(Carvalho & Scott 2009). Murray & Ghahramani (2004) state that GGMs are â€œtrivialâ€. \nThis can perhaps be taken to mean that they are simpler than general undirected \ngraphical models.  \nStructure-learning \nOne of the main tasks or problems to do with graphical models is structure-learning. \nThis is the problem of how to infer the graph structure from observations of the \nrandom variables. Another is the problem of inferenceâ€”how to calculate distributions \non certain nodes given observations of other nodes.  \nMaximum-likelihood methods can be used for many statistical problems. But in \ngraphical model structure-learning, the maximum-likelihood graph is always the \ncomplete graph, because this implies no restrictions on the variables. The maximum-\n\n \n2.3 The covariance and precision matrices for GGMs \n \n9 \nlikelihood estimator of the covariance matrix always exists if ğ‘›> ğ‘, but only sometimes \nexists if ğ‘›â‰¤ğ‘ (for details see Lauritzen 1996, section 5.2.1). In microarray experiments \n(see section 2.4), ğ‘›â‰ªğ‘, so maximum-likelihood methods cannot be used.  \nFor GGMs there are a variety of frequentist and Bayesian methods for structure-\nlearning. These are described in chapter 3. The main topic of this thesis is Bayesian \nstructure-learning of forests and trees.  \n2.3 \nThe covariance and precision matrices for GGMs \nPossible partial correlations \nFirstly, to standardize a matrix ğ‘€ means to replace it by ğ·ğ‘€ğ·, where ğ· is the diagonal \nmatrix whose elements are ğ‘‘ğ‘–ğ‘–= ğ‘šğ‘–ğ‘–\nâˆ’1/2. The (ğ‘–, ğ‘—) element of the standardized matrix is \nthus ğ‘šğ‘–ğ‘—/âˆšğ‘šğ‘–ğ‘–ğ‘šğ‘—ğ‘—, so the diagonal elements of the standardized matrix are all 1.  \nIn Gaussian graphical models, not all combinations of partial correlations are possible. \nLet the precision matrix Î£âˆ’1 be ğ¾, and consider the standardized precision matrix ğ¶, \nwhere ğ‘ğ‘–ğ‘—= ğ‘˜ğ‘–ğ‘—/âˆšğ‘˜ğ‘–ğ‘–ğ‘˜ğ‘—ğ‘— and âˆ’1 â‰¤ğ‘ğ‘–ğ‘—â‰¤1. The partial correlation between ğ‘‹ğ‘– and ğ‘‹ğ‘— is \nğ‘Ÿğ‘–ğ‘—= âˆ’ğ‘ğ‘–ğ‘— (for ğ‘–â‰ ğ‘—), so ğ¶ could be called the negative partial correlation matrix. The \nprecision matrix has to be positive-definite, so ğ¶ also has to be positive-definite.  \nSylvesterâ€™s criterion (Gilbert 1991) states that a matrix is positive-definite if and only if \nthe determinants of all its square upper-left submatrices are positive. These \ndeterminants are called the leading principal minors of the matrix. Applying this \ncriterion to ğ¶ gives a set of algebraic inequalities that must be satisfied by the partial \ncorrelations.  \nFor some graphs, these inequalities can be greatly simplified. Consider a graph in which \nnode 1 has edges to all the other nodes, and there are no other edges apart from these. \nThe graph is called a â€œstarâ€ and node 1 is called a â€œhubâ€. In this case, \nğ¶=\n(\n \n \n1\nğ‘12\nğ‘13\nâ‹¯\nğ‘1ğ‘\n ğ‘12\n1\n0\nâ‹¯\n0 \n ğ‘13\n0\n1\nâ‹¯\n0 \nâ‹®\nâ‹®\nâ‹®\nâ‹±\nâ‹® \n ğ‘1p\n0\n0\nâ‹¯\n1 )\n \n  . \nOne of the square upper-left submatrices is the entire matrix. The determinant of this \nbeing positive is equivalent to  \nâˆ‘ğ‘1ğ‘—\n2\nğ‘\nğ‘—=2\n< 1 . \nIf this inequality holds then the other leading principal minors are also positive. So this \ninequality on its own is a necessary and sufficient condition for ğ¶ being positive-\ndefinite and the distribution being valid. The necessary and sufficient condition on the \npartial correlations is obviously just âˆ‘\nğ‘Ÿ1ğ‘—\n2\nğ‘\nğ‘—=2\n< 1 . \n\n2 Graphs and graphical models \n \n 10 \nIt follows, for example, that in a V-shaped graph with three nodes and two edges, at \nleast one of the partial correlations along the edges must have magnitude less than \nâˆš1/2 â‰ˆ0.707. More generally, in a star with ğ‘  â€œraysâ€, there must be at least one partial \ncorrelation on an edge that has magnitude less than âˆš1/ğ‘ .  \nAs a necessary condition, the inequality generalizes to graphs that contain stars as \ninduced subgraphs. This is because the nodes can simply be reordered so that the hub \nis node 1 and the other ğ‘  nodes of the star come next. The above argument applied to \nthe upper-left (ğ‘ + 1) Ã— (ğ‘ + 1) submatrix shows that âˆ‘\nğ‘Ÿ1ğ‘—\n2\nğ‘ +1\nğ‘—=2\n< 1 . For example, in any \ngraph that contains a V-shape, which means any graph that does not consist entirely of \ndisjoint cliques, there must be at least one partial correlation on an edge that has \nmagnitude less than 0.707.  \nAs far as I am aware, these conditions on partial correlations in stars have not \npreviously appeared in published research. The closest thing I have found is assump-\ntions A3 and A4 in Kalisch & BÃ¼hlmann (2007), which are about the numbers of neigh-\nbours of nodes and the magnitudes of the partial correlations in GGMs. These assump-\ntions are also used in Maathuis et al (2009).  \nFor shapes other than stars, it is easy to write down the inequalities that result from \nSylvesterâ€™s criterion, but it is generally not easy to rearrange them into a useful form.  \nPossible standard correlations \nMore widely known than partial correlations, and possibly also of interest, are the \nstandard correlations. These can be found by inverting the standardized precision \nmatrix and standardizing.  \nAs with partial correlations, the conditional independence relations shown by the graph \nimply conditions on the correlations. However, these conditions are not as simple or \nnotable as the ones for partial correlations. For the V-shaped graph on three nodes,  \nğ¶= (\n1\nğ‘12\nğ‘13\n ğ‘12\n1\n0 \n ğ‘13\n0\n1\n), \nwhich means that the upper triangle of the correlation matrix, found by inverting and \nthen standardizing, is  \n(\n  \n \n1\nâˆ’ğ‘12\nâˆš1 âˆ’ğ‘13\n2\nâˆ’ğ‘13\nâˆš1 âˆ’ğ‘12\n2\n1\nğ‘12ğ‘13\nâˆš(1 âˆ’ğ‘12\n2 )(1 âˆ’ğ‘13\n2 )\n \n1\n)\n  \n \n. \nIt can be seen that ğ‘ğ‘œğ‘Ÿğ‘Ÿ(ğ‘‹2, ğ‘‹3) = ğ‘ğ‘œğ‘Ÿğ‘Ÿ(ğ‘‹1, ğ‘‹2)ğ‘ğ‘œğ‘Ÿğ‘Ÿ(ğ‘‹1, ğ‘‹3). The correlation between \nthe two unconnected nodes is the product of the other two correlations.  \nFor arbitrary-sized stars, a standard formula for the inverse of a partitioned matrix can \nbe used to show that  \n\n \n2.3 The covariance and precision matrices for GGMs \n \n11 \nğ‘ğ‘œğ‘Ÿğ‘Ÿ(ğ‘‹1, ğ‘‹ğ‘–) = âˆ’ğ‘1ğ‘–[1 âˆ’ğ‘¡ + ğ‘1ğ‘–\n2 ]\nâˆ’1/2  \nand  ğ‘ğ‘œğ‘Ÿğ‘Ÿ(ğ‘‹ğ‘—, ğ‘‹ğ‘˜) = ğ‘1ğ‘—ğ‘1ğ‘˜[(1 âˆ’ğ‘¡ + ğ‘1ğ‘—\n2 )(1 âˆ’ğ‘¡ + ğ‘1ğ‘˜\n2 )]\nâˆ’1/2   for ğ‘—, ğ‘˜â‰ 1, \nwhere ğ‘¡= âˆ‘\nğ‘1ğ‘š\n2\nğ‘\nğ‘š=2\n. Again ğ‘ğ‘œğ‘Ÿğ‘Ÿ(ğ‘‹ğ‘—, ğ‘‹ğ‘˜) = ğ‘ğ‘œğ‘Ÿğ‘Ÿ(ğ‘‹1, ğ‘‹ğ‘—)ğ‘ğ‘œğ‘Ÿğ‘Ÿ(ğ‘‹1, ğ‘‹ğ‘˜). There is no simple \ngeneralization to graphs that contain stars as induced subgraphs.  \nCreating possible covariance matrices \nChapter 11 is about experiments to evaluate and compare algorithms for Bayesian \nstructure-learning of GGMs. These experiments use simulated datasets that each \ncorrespond to a particular graph. This subsection is about the issues involved in \ncreating these simulated datasets and several ways of doing it.  \nGiven a covariance matrix ğ›´, data from ğ‘ğ‘(0, ğ›´) can easily be generated in R or other \nstatistical packages. But creating a possible Î£ for a given graph is sometimes non-trivial. \nNecessary and sufficient conditions on Î£ are that it be symmetric and positive-definite, \nand that the precision matrix ğ¾= Î£âˆ’1 have zeroes in the positions that correspond to \nabsent edges in the graph. Of course the task of creating a possible covariance matrix is \nequivalent to creating a possible precision matrix or negative partial correlation matrix.  \nNumerous papers describe experiments that must have involved creating covariance \nmatrices for given graphs, but most do not mention how this was done. It seems likely \nthat the authors chose the partial correlations to all be equal and reasonably large, and \nthen made adjustments as necessary to ensure that the matrix was positive-definite. \nThe papers that do mention how it was done mostly describe specific simple matrices. \nMeinshausen & BÃ¼hlmann (2006, page 1448) generated large random graphs whose \nnodes have maximum degree 4, and chose all the partial correlations to be 0.245. They \nstate that absolute values less than 0.25 guarantee that the precision matrix is positive-\ndefinite. For general graphs, no such statement can be made, as shown in â€œPossible \npartial correlationsâ€, above. Guo et al (2011, pages 6â€“7) created precision matrices for \nâ€œchainâ€ graphs, then added extra edges at random. For each extra edge they set the two \ncorresponding elements of the precision matrix to be a random value from \nğ‘ˆğ‘›ğ‘–ğ‘“([âˆ’1, âˆ’0.5] âˆª[0.5,1]).  \nOne sure-fire way to create a possible Î£ is to use the formulas in Appendix A of Roverato \n(2002). This method was used by Castelo & Roverato (2006). It uses the Cholesky \ndecomposition ğ¾= Î¦ğ‘‡Î¦, where Î¦  is an upper-triangular matrix. The diagonal \nelements of Î¦, and the elements that correspond to edges in the graph, can be chosen \nfreely, and Roverato calls these the â€œfreeâ€ elements. The other elements, which he calls \nâ€œfixedâ€, have to be calculated according to Roverato (2002)â€™s equation (10). ğ¾ and ğ›´ can \nthen be calculated from Î¦.  \nFor decomposable graphs, the calculations for fixed elements can be avoided, as in \nAlbieri (2010). If the vertices are ordered according to a perfect vertex elimination \nscheme, then all the fixed elements of Î¦ are zero (Roverato 2002, page 408). A perfect \nvertex elimination scheme is the reverse of a perfect numberingâ€”see Lauritzen (1996, \npage 15).  \n\n2 Graphs and graphical models \n \n 12 \nThe next question is how to choose the free elements of Î¦. For very small graphs it is \npossible to work out explicit formulas for how the elements of Î¦ will affect the \nelements of ğ¾, but for most graphs it is not. It is undesirable to have partial correlations \nthat are very close to zero, since these edges will be difficult to detect. But in most \ngraphs it is impossible for all the partial correlations to have large magnitudeâ€”see \nâ€œPossible partial correlationsâ€ above. The simplest way is to set all the free elements to \nhave the same value, though oneâ€™s first choice might not give a positive-definite matrix \nbecause of hubs or other structures.  \nAn alternative way to create a covariance matrix for a given graph is to first choose any \nsymmetric matrix ğ¾ such that the diagonal elements are positive and the elements \ncorresponding to absent edges are zero. Find the eigenvalues of ğ¾, and if any of these \nare negative, let âˆ’ğœ† be the lowest one. Replace ğ¾ with ğ¾+ ğ›¾ğ¼, for some ğ›¾> ğœ†. This \nensures that all the eigenvalues are positive, so ğ¾ is positive-definite, without \ndisturbing the off-diagonal zeroes or the symmetry. SchÃ¤fer & Strimmer (2005a, pages \n757â€“758) used a similar method, though they added quantities to each of the diagonal \nelements individually. (The R package â€œGeneNetâ€, by SchÃ¤fer et al 2012, contains a \nfunction that performs their method.)  \nHaving created a possible covariance matrix from which to generate simulated data, it is \ncommon to standardize the matrix so that the variances are all 1. This ensures that the \nvariables are all on the same scale.  \nThe datasets used in the experiments in chapter 11 mostly correspond to true graphs \nthat are trees. To create the covariances for these datasets, I started by setting ğ¾ to have \n1â€™s on the diagonal and equal values in all the positions that correspond to edges. I then \ninverted ğ¾ and standardized to create Î£.  \n2.4 \nBiomolecular networks \nModelling biomolecular networks \nThe ultimate intended application of my work on GGMs is gene regulation networks. \nEach gene corresponds to a node, and the numerical value for each node is the \nlogarithm of the expression level of that gene. The networks arise because genes are \ntranscribed to form molecules of mRNA, which are then translated to form proteins, and \nsome of these proteins are transcription factors that promote or inhibit the tran-\nscription of other genes (Pournara & Wernisch 2007).  \nThe idea of using GGMs to model gene regulation networks was first proposed in Fried-\nman et al (2000). This paper used Bayesian networks and the Bayesian structure-\nlearning methods described in Heckerman et al (1995). It contains several significant \nideas, for example that most of the difficulties arise from the number of variables being \nmuch greater than the number of observationsâ€”that is, ğ‘›â‰ªğ‘â€”and the idea of using \nprior biological knowledge about the network structure. GGMs have subsequently been \nused to model gene regulation networks in Castelo & Roverato (2006, 2009), Albieri \n(2010), Edwards et al (2010), and probably many othersâ€”as of February 2013, Google \nScholar says that Friedman et al (2000) has been cited 2285 times.  \n\n \n2.4 Biomolecular networks \n \n13 \nThere are numerous public databases that contain the results of experiments to \nmeasure gene expression levels, for example the National Center for Biotechnology \nInformationâ€™s Gene Expression Omnibus (Barrett et al 2007) and M3D (Faith et al \n2008). These databases usually have ğ‘›â‰ªğ‘.  \nFor details of the preliminary statistical analysis of microarray experiments, including \nexperimental design and how the data is processed and cleaned, see Wit & McClure \n(2004). Sections 6.2.1 and 6.2.2 of this book present arguments for and against the use \nof the multivariate Gaussian distribution to model log gene expression levels.  \nMeasurements of gene expression levels are produced using DNA microarrays. These \nare an example of a high-throughput methodâ€”a method that can quickly produce data \non large numbers of biomolecules. GGMs can also be used to model other large bio-\nmolecular networks. For an overview of this topic see Markowetz & Spang (2007).  \nIt is believed that gene regulation networks and other biomolecular networks tend to \nhave certain properties related to the degrees of the nodes and other features of the \ngraph. These properties are the topic of the next few subsections. Some of the research \nin this area has not been mathematically rigorous. For example, in BarabÃ¡si & Albert \n(1999) the description of the growth of â€œscale-freeâ€ graphs (see below) is not a full \ndefinition of a random process, as pointed out in BollobÃ¡s et al (2001). In this section I \nwill just report the properties without attempting to state or discuss them in a fully \nmathematical way. Some of them are discussed further in chapter 5.  \nHubs \nBiomolecular networks tend to contain a few nodes, called hubs, that are connected to a \nlarge number of other nodes. In gene regulation networks, the biological meaning of a \nhub is that one gene codes for a protein that regulates the expression of many other \ngenes. Hubs are probably the most notable and widely recognized characteristic of bio-\nmolecular networks. For example, BarabÃ¡si & Oltvai (2004) report that the transcrip-\ntional gene regulation networks of E. coli and S. cerevisiae (yeast) contain \ndisproportionately many hubs. In Alterovitz & Ramoni (2006), Figure 1 shows several \nhubs in the E. coli gene regulation network that have very large numbers of neighbours. \nRoyer et al (2008) states that hubs are also a feature of protein networks, and that the \nabundance of hubs can be explained by models of evolution.  \nOther motifs \nCertain small-scale motifs also seem to be common in biomolecular networks. Motifs \nare subgraphs, or induced subgraphs, that appear more often in real networks than in \nâ€œrandomized networksâ€ (Milo et al 2002). Milo et al (2002) set out to find three- and \nfour-node motifs in various well-studied directed networks including the E. coli and S. \ncerevisiae gene regulation networks. They compared these real networks to random \ngraphs where each node had the same number of incoming and outgoing edges as in \nthe real networks, and found that the feed-forward loop and the bi-fanâ€”see Figure \n2.1â€”occur far more frequently in the gene regulation networks. The ğ‘-values for these \nmotifs ranged from 10 to 41.  \n\n2 Graphs and graphical models \n \n 14 \nRoyer et al (2008) states that protein interaction networks tend to have cliques and \nbicliques. A biclique is two sets of nodes where every node in one set is connected to \nevery node in the otherâ€”see Figure 2.1. Alon (2007) shows a directed biclique from \nthe transcription regulation network of E. coli (in his Figure 6).  \n \n \nFigure 2.1. Left to right: a feed-forward loop, a bi-fan, and a biclique. These motifs have been \nfound to be common in real-world networks.  \nSparsity \nIt is widely believed that biomolecular networks are sparse, meaning that they have few \nedges. The precise meaning of this statement is discussed in depth in section 6.2. For \nexample, Leclerc (2008) reports the numbers of nodes and edges in gene networks for \nArabidopsis, Drosophila, and three other widely studied model organisms. All these \nnetworks are sparse. Pournara & Wernisch (2007) state that gene regulation networks \nare sparse because most genes are known to be regulated by a small number of tran-\nscription factors and most transcription factors regulate a small number of genes. \nProtein interaction networks are also believed to be sparse (Spirin & Mirny 2003). \nMany papers simply assert without elaboration that biomolecular networks are \nsparseâ€”for example Wille & BÃ¼hlmann (2006) and Han et al (2007).  \nScale-free networks \nBarabÃ¡si & Oltvai (2004) state that the most striking feature of biomolecular networks, \nas well as social and technological networks, is that they are approximately â€œscale-freeâ€. \nThis term was introduced by BarabÃ¡si & Albert (1999) and means that, over a large \nrange, the degrees of the nodes follow a power law; that is, the probability that a node \nhas degree ğ‘˜ is proportional to ğ‘˜âˆ’ğ›¾. In most cases, 2 < ğ›¾< 3.  \nIn scale-free graphs most nodes have small degrees, but a few have very high degrees. \n(In other words, the power-law distribution is long-tailed.) So for biomolecular \nnetworks both the sparsity and the relative preponderance of hubs could be regarded \nas consequences of being scale-free.  \nJeong et al (2001) reports that the S. cerevisiae protein network follows a power law, \nand Jeong et al (2000) studied metabolic networks of 43 species and found strong \nevidence of power laws. However, BarabÃ¡si & Oltvai (2004) state that in the \ntranscriptional gene regulation networks of E. coli and S. cerevisiae, the degree \ndistributions are mixtures of power laws and exponential distributions.  \n\n \n2.5 Supplementary notes: alternative terms and the history of graphical models \n \n15 \nLog-transformation \nGene expression data needs to be log-transformed before being modelled by the multi-\nvariate Gaussian distribution. But websites, online databases, and papers that present \nthis kind of data do not always state whether this has been done.  \nFor example, Albieri (2010) used a gene expression dataset with 100 nodes and 43 \nobservations that was a subset of the EcoliOxygen dataset in the R package â€œqpgraphâ€ \n(Castelo & Roverato 2009). The EcoliOxygen dataset is reported in Covert et al (2004) \nand available from the National Center for Biotechnology Informationâ€™s Gene \nExpression Omnibus (http://www.ncbi.nlm.nih.gov/geo/, Barrett et al 2007), where it \nis record number GDS680. Without looking at the numerical values themselves, it is not \nobvious whether the values in EcoliOxygen have been log-transformed. Covert et al \n(2004) mentions a ğ‘¡-test on log-transformed data, but that is all.  \nI found a different database of E. coli gene expression data, M3D (http://m3d.bu.edu/, \nFaith et al 2008), where it is stated that the values are log-transformed. I plotted a \nhistogram of all the expression levels from M3D and a histogram of all the EcoliOxygen \ndata. The two distributions looked similar, which suggests that the EcoliOxygen data \nhave been log-transformed.  \n2.5 \nSupplementary notes: alternative terms and the history \nof graphical models \nBooks about graphical models include Pearl (1988), Whittaker (1990), Edwards (1995), \nLauritzen (1996), Cowell et al (2007), and Koller & Friedman (2009). Graphical models \nare also known as probabilistic graphical models or graphical Markov models \n(Wermuth 1998, Wermuth & Cox 2001). Undirected graphical models are sometimes \ncalled Markov random fields, and directed acyclic graphical models are often called \nBayesian networks (Bayes nets for short) or belief networks.  \nFor a brief history of graphical models see Wermuth (1998), in which their origins are \ntraced back to the early twentieth century. Gaussian graphical models originate in \nDempster (1972). But Dempster did not mention graphs or conditional independence. \nWhat he proposed was to simplify the multivariate normal distribution ğ‘ğ‘(ğœ‡, ğ›´) by \nsetting some elements of ğ›´âˆ’1 to zero. Dempster called this â€œcovariance selectionâ€, and \nas a result Gaussian graphical models are also known as covariance selection models. \nThey are occasionally called concentration graph models (Wermuth & Cox 2001).  \nThe task of inferring the graph structure from data is often called â€œstructural learningâ€, \nthough I prefer â€œstructure-learningâ€. It is also referred to as â€œmodel selectionâ€, â€œreverse \nengineeringâ€ (Alon 2003, Castelo & Roverato 2009, Maathuis et al 2010), â€œtopology \ndiscoveryâ€ (Anandkumar et al 2011), and â€œestimation of structureâ€ (Lauritzen 2012). \nFor GGMs it is sometimes called covariance selection. Structural learning contrasts with \nâ€œquantitative learningâ€, which means estimating the numerical parameters of the \nprobability distribution (Giudici 1996).  \nIn the field of graphical models it is common to talk about â€œdecomposableâ€ graphs. In \ngraph theory these are called chordal graphs (Gavril 1974, Diestel 2005, Bondy & Murty \n\n2 Graphs and graphical models \n \n 16 \n2008). Sometimes they are called triangulated graphs (Rose 1970, Rose 1972, Berge \n1973, Lauritzen 1996, Diestel 2005). They have also been called rigid circuit graphs \n(Dirac 1961), perfect elimination graphs (Rose et al 1976), and monotone transitive \ngraphs (Rose 1972).  \nRegarding the terms â€œstarâ€ and â€œhubâ€, it is not ideal to use words that are unrelated in \nthe real world for mathematical objects that are closely related. But â€œhubâ€ often refers \nto the centre of a network, so its use in describing graphs is natural; and it is useful to \nhave the separate word â€œstarâ€ for the hub and the nodes connected to it. Both are \ncommonly usedâ€”â€œhubâ€ in BarabÃ¡si & Oltvai (2004) and Albieri (2010), for example, \nand â€œstarâ€ in Royer et al (2008) and Yuan & Lin (2007).  \n \n\n \n17 \n3 \nStructure-learning for GGMs \n3.1 \nBayesian methods  \nThe standard Bayesian method \nBayesian learning of graphical model structure involves a likelihood, a prior distri-\nbution, some data, and a posterior distribution. The prior and the posterior are both \ndistributions on the set of all graphs with the appropriate number of nodes, or in the \ncase that only a certain subset of graphs is considered, they are distributions on that set \nof graphs. The prior is specified by the user or researcher and the posterior is \ncalculated from the prior and the data.  \nThe most widely used Bayesian method for learning Gaussian graphical model \nstructure requires a prior distribution on Î£ as well as the prior on the graph structure. \nSuppose there are ğ‘ nodes. Let ğ‘¥ be the ğ‘›Ã— ğ‘ matrix (ğ‘› rows, ğ‘ columns) of the ğ‘› \nobserved data:  \nğ‘¥=\n(\n \nğ‘¥1\nğ‘‡\nğ‘¥2\nğ‘‡\nâ‹®\nğ‘¥ğ‘›ğ‘‡)\n , \nand let ğ‘ˆ= ğ‘¥ğ‘‡ğ‘¥. (ğ‘ˆ is usually called ğ‘†, but I use ğ‘† to mean separators.) The likelihood is  \nğ‘( ğ‘¥âˆ£âˆ£ğºğ‘–, Î£ ) = (2ğœ‹)âˆ’ğ‘›ğ‘/2|ğ¾|ğ‘›/2 exp [âˆ’1\n2 âˆ‘ğ‘¥ğ‘–\nğ‘‡ğ¾ğ‘¥ğ‘–\nğ‘–\n] \n= (2ğœ‹)âˆ’ğ‘›ğ‘/2|ğ¾|ğ‘›/2 exp [âˆ’1\n2 tr(ğ¾ğ‘ˆ)] , \nwhere ğ¾= Î£âˆ’1, and the posterior probability of ğºğ‘– being the true graph is  \nğ‘( ğºğ‘–âˆ£âˆ£ğ‘¥) =\nğ‘( ğ‘¥âˆ£âˆ£ğºğ‘–)ğ‘(ğºğ‘–)\nâˆ‘ğ‘( ğ‘¥âˆ£âˆ£ğºğ‘—)ğ‘(ğºğ‘—)\nğ‘—\n. \nThe meaning of ğ‘(â‹…) changes according to its arguments. Of the terms on the right-hand \nside, ğ‘(ğºğ‘–) is the prior probability of ğºğ‘– and ğ‘( ğ‘¥âˆ£âˆ£ğºğ‘–) is the marginal likelihood: \nğ‘( ğ‘¥âˆ£âˆ£ğºğ‘–) = âˆ«\nğ‘( ğ‘¥âˆ£âˆ£ğºğ‘–, Î£ ) ğ‘( Î£ âˆ£âˆ£ğºğ‘–) ğ‘‘Î£\nÎ£âˆ’1âˆˆğ‘€+(ğºğ‘–)\n .  \nHere ğ‘€+(ğºğ‘–) is the set of positive-definite matrices that have zeroes in the positions \nthat correspond to absent edges in ğºğ‘–. So the integral is over all values of Î£ that are \npossible for ğºğ‘–.  \n\n3 Structure-learning for GGMs \n \n 18 \nFor ğ‘( Î£ âˆ£âˆ£ğºğ‘–) it is common to use the generalized hyper inverse Wishart (HIW) distri-\nbution (Dawid & Lauritzen 1993), which is conjugate. This and other priors for Î£ are \ndescribed in the next few subsections. Graph priors are discussed in chapter 5.  \nComplete graphs \nIf the graph is known to be the complete graph, ğ¾ğ‘, then the distribution of ğ‘¥ is just the \nmultivariate Gaussian distribution with no conditional-independence restrictions, and \nthe conjugate prior for Î£ is the inverse Wishart distribution. This is defined as follows.  \nIf ğ‘‹ is an ğ‘šÃ— ğ‘ matrix where each row is an independent sample from the ğ‘-variate \nGaussian distribution with zero mean and covariance matrix ğ‘‰, then the ğ‘Ã— ğ‘ matrix \nğ‘ˆ= ğ‘‹ğ‘‡ğ‘‹ has the Wishart distribution with scale matrix ğ‘‰ and ğ‘š degrees of freedom. I \nwill write this as ğ‘ˆ ~ ğ‘Š(ğ‘š; ğ‘‰). For ğ‘ˆ to be invertible with probability 1, it is necessary \nthat ğ‘šâ‰¥ğ‘. The distribution of Î£ = ğ‘ˆâˆ’1 is then the inverse Wishart distribution with \ninverse scale matrix ğ·= ğ‘‰âˆ’1 and ğ‘š degrees of freedom. I will write this as ğ¼ğ‘Š(ğ›¿, ğ·), \nwhere ğ›¿= ğ‘šâˆ’ğ‘+ 1.  \nThe only restrictions on the parameters for the Wishart distribution are that ğ‘‰ be \npositive-definite and ğ‘š be positive. The only restrictions on the parameters for the \ninverse Wishart distribution are that ğ· be positive-definite and ğ‘šâ‰¥ğ‘, which means \nğ›¿â‰¥1. Obviously under these characterizations ğ‘š and ğ‘ are both positive integers.  \nIf Î£ ~ ğ¼ğ‘Š(ğ›¿, ğ·), then the density of Î£ is \nğ‘(Î£) =\n|ğ·|(ğ›¿+ğ‘âˆ’1)/2 exp [âˆ’1\n2 tr(ğ·Î£âˆ’1)]\n2(ğ›¿+ğ‘âˆ’1)ğ‘/2 |Î£|ğ‘+ğ›¿/2 Î“ğ‘((ğ›¿+ ğ‘âˆ’1)/2) \n=\n|ğ·\n2|\n(ğ›¿+ğ‘âˆ’1)/2\n exp [âˆ’1\n2 tr(ğ·Î£âˆ’1)]\n |Î£|ğ‘+ğ›¿/2 Î“ğ‘((ğ›¿+ ğ‘âˆ’1)/2)\n \n(Giudici & Green 1999, page 787; Roverato 2002, page 396). Here Î“ğ‘ is the multivariate \ngamma function (James 1964), defined by \nÎ“ğ‘(ğ‘) = ğœ‹ğ‘(ğ‘âˆ’1)/4 âˆÎ“[ğ‘+ (1 âˆ’ğ‘—)/2] .\nğ‘\nğ‘—=1\n \nThe â€œnormalizing constantâ€ for the inverse Wishart distribution is the part of the \nformula for the density that does not involve Î£ (Jones et al 2005). There is no problem \nwith terms like |Î£|ğ‘+ğ›¿/2, where the exponent can be non-integer, since Î£ and ğ· are both \npositive-definite and so their determinants are positive.  \nDecomposable graphs \nFor a decomposable graph, the conjugate prior for Î£ is the hyper inverse Wishart (HIW) \ndistribution. This was defined by Dawid & Lauritzen (1993) and also described in detail \nin Giudici & Green (1999).  \n\n \n3.1 Bayesian methods \n \n19 \nFor a given decomposable graph, suppose the cliques have covariances Î£ğ¶ and the prior \non each Î£ğ¶ is ğ¼ğ‘Š(ğ›¿, ğ·ğ¶), where ğ›¿ is some positive number that is the same for all \ncliques. Dawid & Lauritzen (1993) showed that these distributions on the cliques \ninduce a unique hyper Markov distribution on Î£, the covariance for the whole graph. In \nthis distribution, Î£ is constrained so that its inverse has zeroes in the appropriate \nplaces, which means the distribution is Markov on the graph. They called this the hyper \ninverse Wishart distribution and showed that it is conjugate for the family of \nmultivariate Gaussian distributions that are Markov on the graph.  \nTwo issues that arise in specifying the ğ·ğ¶â€™s are hyperconsistency and compatibility. \nHyperconsistency means that the distributions of the clique covariances have to be the \nsame where they overlap, so (ğ·ğ¶1)ğ‘–ğ‘—= (ğ·ğ¶2)ğ‘˜ğ‘™ whenever (ğ‘–, ğ‘—) and (ğ‘˜, ğ‘™) identify the \nsame edge. Compatibility between the distributions on two graphs means that any \nclique that appears in two graphs has the same distribution in both cases. Hyper-\nconsistency is essential but compatibility is merely desirable. Probably the simplest \nway to ensure hyperconsistency and compatibility is to choose a single ğ‘Ã— ğ‘ matrix ğ·, \nand for each graph let each ğ·ğ¶ or ğ·ğ‘† be the appropriate submatrix of ğ·. For full details \nof these issues see Dawid & Lauritzen (1993) or Giudici & Green (1999). For \nincomplete graphs not every element of ğ· is used.  \nI will parameterize the HIW distribution using a ğ‘Ã— ğ‘ matrix ğ· and write it as \nğ»ğ¼ğ‘Šğº(ğ›¿, ğ·). If Î£ ~ ğ»ğ¼ğ‘Šğº(ğ›¿, ğ·) then the density of Î£ is \nğ‘(Î£) =\nâˆğ‘( Î£ğ¶âˆ£âˆ£ğº)\nğ¶\nâˆğ‘( Î£ğ‘†âˆ£âˆ£ğº)\nğ‘†\n, \nwhere Î£ğ¶ ~ ğ¼ğ‘Š(ğ›¿, ğ·ğ¶) and Î£ğ‘† ~ ğ¼ğ‘Š(ğ›¿, ğ·ğ‘†). The product in the numerator is over the set \nof cliques, and the product in the denominator is over the collection of separators. A \nseparator may appear more than once in this collection.  \nFor the HIW prior to be proper, it is sufficient that ğ›¿> 2 (Roverato 2002, page 402; \nJones et al 2005, page 390) and ğ·âˆ’1 âˆˆğ‘€+(ğº) (Atay-Kayis & Massam 2005, page 322). \nIf the prior on Î£ is ğ»ğ¼ğ‘Šğº(ğ›¿, ğ·), then the posterior is ğ»ğ¼ğ‘Šğº(ğ›¿+ ğ‘›, ğ·+ ğ‘ˆ), where ğ‘ˆ=\nğ‘¥ğ‘‡ğ‘¥ is a sufficient statistic for the data ğ‘¥.  \nThe marginal likelihood ğ‘(ğ‘¥âˆ£ğº) can be found explicitly as follows. In the following \nexpressions, |ğ›´| is the determinant of ğ›´ but |ğ¶| is the number of elements in ğ¶:  \nğ‘( ğ‘¥âˆ£ğº) = âˆ«ğ‘( ğ‘¥âˆ£ğº, Î£ )ğ‘( Î£ âˆ£ğº) ğ‘‘Î£ \n= âˆ«\nâˆ(2ğœ‹)âˆ’ğ‘›|ğ¶|/2|Î£ğ¶|âˆ’ğ‘›/2 exp [âˆ’1\n2 ğ‘¡ğ‘Ÿ(ğ‘ˆğ¶Î£ğ¶\nâˆ’1)]\nğ¶\nâˆ(2ğœ‹)âˆ’ğ‘›|ğ‘†|/2|Î£ğ‘†|âˆ’ğ‘›/2 exp [âˆ’1\n2 ğ‘¡ğ‘Ÿ(ğ‘ˆğ‘†Î£ğ‘†\nâˆ’1)]\nğ‘†\n                                                        \n              â‹… \nâˆ \n|ğ·ğ¶\n2 |\nğ›¿+|ğ¶|âˆ’1\n2\nÎ“|ğ¶| (ğ›¿+ |ğ¶| âˆ’1\n2\n)\n  |Î£ğ¶|âˆ’ğ›¿+2|ğ¶|\n2\nexp [âˆ’1\n2 tr(ğ·ğ¶ Î£ğ¶\nâˆ’1)]\nğ¶\nâˆ \n|ğ·ğ‘†\n2 |\nğ›¿+|ğ‘†|âˆ’1\n2\nÎ“|ğ‘†| (ğ›¿+ |ğ‘†| âˆ’1\n2\n)\n  |Î£ğ‘†|âˆ’ğ›¿+2|ğ‘†|\n2\nexp [âˆ’1\n2 tr(ğ·ğ‘† Î£ğ‘†\nâˆ’1)]\nğ‘†\nğ‘‘Î£ \n\n3 Structure-learning for GGMs \n \n 20 \n=\nâˆ|ğ·ğ¶\n2 |\nğ›¿+|ğ¶|âˆ’1\n2\n/ Î“|ğ¶| (ğ›¿+ |ğ¶| âˆ’1\n2\n) \nğ¶\nâˆ|ğ·ğ‘†\n2 |\nğ›¿+|ğ‘†|âˆ’1\n2\n/ Î“|ğ‘†| (ğ›¿+ |ğ‘†| âˆ’1\n2\n) \nğ‘†\nâ‹…(2ğœ‹)âˆ’ğ‘›ğ‘/2 âˆ«\nâˆ |Î£ğ¶|âˆ’ğ›¿+ğ‘›+2|ğ¶|\n2\nexp [âˆ’1\n2 tr({ğ·C + ğ‘ˆğ¶} Î£ğ¶\nâˆ’1)]\nğ¶\nâˆ |Î£ğ‘†|âˆ’ğ›¿+ğ‘›+2|ğ‘†|\n2\nexp [âˆ’1\n2 tr({ğ·S + ğ‘ˆğ‘†} Î£ğ‘†\nâˆ’1)]\nğ‘†\nğ‘‘Î£ . \nIn these integrals the measure ğ‘‘Î£ can be taken to be the product of the Lebesgue \nmeasures on the elements of the incomplete covariance matrix, which contains only the \nelements of Î£ that correspond to edges in the graph (Giudici & Green 1999). The \nexponent of 2ğœ‹ is simplified using âˆ‘|ğ¶|\nğ¶\nâˆ’âˆ‘|ğ‘†|\nğ‘†\n= ğ‘, which follows from the definition \nof a perfect sequence. In the second large expression, the first big fraction is the \nnormalizing constant for the HIW prior density (the part of this density that does not \ninvolve Î£) and the integrand is the HIW posterior density without its normalizing \nconstant. It follows that  \nğ‘( ğ‘¥âˆ£ğº) = (2ğœ‹)âˆ’ğ‘›ğ‘/2\nâˆ\nğ‘˜(ğ¶, ğ›¿, ğ·)\nğ‘˜(ğ¶, ğ›¿+ ğ‘›, ğ·+ ğ‘ˆ) \nğ¶\nâˆ\nğ‘˜(ğ‘†, ğ›¿, ğ·)\nğ‘˜(ğ‘†, ğ›¿+ ğ‘›, ğ·+ ğ‘ˆ)\nğ‘†\n \n , \nwhere ğ‘˜ is the normalizing constant for each clique or separator: \nğ‘˜(ğ¶, ğ›¿, ğ·) =\n|ğ·ğ¶\n2 |\nğ›¿+|ğ¶|âˆ’1\n2\n Î“|ğ¶| (ğ›¿+ |ğ¶| âˆ’1\n2\n)\n . \nOnce the marginal likelihood of a graph has been calculated, it is easy to find its \nunnormalized posterior probability, since ğ‘( ğºâˆ£ğ‘¥) âˆğ‘( ğ‘¥âˆ£ğº)ğ‘(ğº).  \nGeneral graphs \nFor graphs that may or may not be decomposable, the conjugate prior for Î£ is the \ngeneralization of the HIW distribution given by Roverato (2002). This is called the G-\nWishart distribution in Atay-Kayis & Massam (2005), Lenkoski & Dobra (2011), and \nWang & Li (2012).  \nAs in the decomposable case, the density can be written as the product of densities on \nthe prime components divided by the product of densities on the separators (Roverato \n2002, Proposition 2). For the separators and complete prime components, the density \nis the inverse Wishart distribution, as before. For any incomplete prime components, \nthe density is  \nğ‘(Î£ğ‘ƒ\nğ¸) âˆ|Î£ğ‘ƒ|âˆ’ğ›¿âˆ’2\n2  ğ½(Î£ğ‘ƒ\nğ¸) exp [âˆ’1\n2 tr(ğ›´ğ‘ƒ\nâˆ’1ğ·ğ‘ƒ)]. \nHere ğ¸ is the edge-set of the prime component ğ‘ƒ. The reason for writing the density as \na function of Î£ğ‘ƒ\nğ¸, rather than just Î£ğ‘ƒ, is to emphasize that its dimension equals the \nnumber of free (unconstrained) elements in Î£ğ‘ƒ. (In contrast, with cliques and \n\n \n3.1 Bayesian methods \n \n21 \nseparators the dimension of the random variable is |ğ¶|(|ğ¶| + 1)/2, or the same with \nğ‘†â€”the full number of elements in the Cholesky square root.) Some of the non-free \nelements of Î£ğ‘ƒ appear in the expression to the right of the proportional symbol. It \nwould also be possible to just write Î£ğ‘ƒ throughout. The term ğ½(Î£ğ‘ƒ\nğ¸) is the Jacobian for \nthe transformation from ğ¾ğ‘ƒ\nğ¸ to Î£ğ‘ƒ\nğ¸.  \nTo find the marginal likelihood, let ğ‘˜(ğ‘ƒ, ğ›¿, ğ·) be the normalizing constant in the \nexpression for ğ‘(Î£ğ‘ƒ\nğ¸), so that  \nğ‘˜(ğ‘ƒ, ğ›¿, ğ·)âˆ’1 = âˆ«\n|Î£ğ‘ƒ|âˆ’ğ›¿âˆ’2\n2  ğ½(Î£ğ‘ƒ\nğ¸) exp [âˆ’1\n2 tr(ğ›´ğ‘ƒ\nâˆ’1ğ·ğ‘ƒ)] ğ‘‘Î£ğ‘ƒ\nğ¸\nÎ£ğ‘ƒ\nğ¸âˆ£ğ‘ƒ\n. \nThis integral cannot be calculated exactly and is discussed in the next subsection. As \nwith decomposable graphs, the marginal likelihood factorizes according to the decom-\nposition of the graph:  \nğ‘( ğ‘¥âˆ£ğº) = (2ğœ‹)âˆ’ğ‘›ğ‘/2\nâˆ\nğ‘˜(ğ‘ƒ, ğ›¿, ğ·)\nğ‘˜(ğ‘ƒ, ğ›¿+ ğ‘›, ğ·+ ğ‘ˆ) \nğ‘ƒ\nâˆ\nğ‘˜(ğ¶, ğ›¿, ğ·)\nğ‘˜(ğ¶, ğ›¿+ ğ‘›, ğ·+ ğ‘ˆ) \nğ¶\nâˆ\nğ‘˜(ğ‘†, ğ›¿, ğ·)\nğ‘˜(ğ‘†, ğ›¿+ ğ‘›, ğ·+ ğ‘ˆ)\nğ‘†\n \n. \nThe three products are over the incomplete prime components, the cliques, and the \nseparators. The ğ‘˜â€™s in the first product in the numerator are defined by the equation \nwith the integral, and the ğ‘˜â€™s in the other two products are as in the previous subsection.  \nCalculating the normalizing constant for incomplete prime components \nThe problem with the above expression for the marginal likelihood is that ğ‘˜(ğ‘ƒ, ğ›¿, ğ·), \nthe normalizing constant for incomplete prime components, cannot be calculated \nexactly. For calculating it approximately, Roverato (2002) presents a method that uses \nimportance sampling and Atay-Kayis & Massam (2005) give a method that uses simple \nMonte Carlo. Lenkoski & Dobra (2011) use a Laplace method that is quicker but less \naccurate. Moghaddam et al (2009) give two other Laplace-type methods.  \nMoghaddam et al (2009) describe Monte Carlo methods as the â€œgold standardâ€ for this \nproblem. Section 4.2 of Atay-Kayis & Massam (2005) presents their Monte Carlo \nmethod as a step-by-step algorithm. First, change variables from Î£ to ğ¾, and then to Î¦, \nthe Cholesky square root of ğ¾. Then change variables to Î¦ post-multiplied by the \ninverse of the Cholesky square root of ğ·âˆ’1. Next, manipulate this expression into the \nform of a multiple of the expectation of a function with respect to chi-squared and \nunivariate normal random variables (the former corresponding to the diagonal \nelements of the matrix, the latter corresponding to the edges that are present in the \ngraph). Finally, approximate the integral using simple Monte Carlo.  \nRoverato (2002) used the generalized HIW distribution to analyze Fisherâ€™s iris data. \nThis is a well-known set of multivariate data with ğ‘= 4 that was published in Anderson \n(1935) and used in Fisher (1936). I have done the same analysis of this dataset, using \nthe same values of the HIW hyperparameters as Roverato (2002) and the same uniform \ngraph distribution, using Java. Instead of Roveratoâ€™s importance-sampling method, I \nused Atay-Kayis & Massam (2005)â€™s simple Monte Carlo method. The posterior \ndistribution that I found was very close to Roveratoâ€™sâ€”see Figure 3.1. The reasons it \n\n3 Structure-learning for GGMs \n \n 22 \nwas not exactly the same were probably that both methods are random and that \nRoverato (2002) only used 15,000 samples for the importance sampling whereas I used \na billion for the Monte Carlo method. The top graph is a four-cycle, which is of course \nnon-decomposable.  \n \n \n0.14797 \n0.147 \n \n0.13481 \n0.135 \n \n0.10590 \n0.106 \n \n0.10583 \n0.106 \nFigure 3.1. The top four graphs for the iris data. Below each graph is its posterior probability \naccording to my program, with 1 billion iterations of the Monte Carlo method, and according to \nRoverato (2002).  \n \nExploring the posterior distribution  \nFor small ğ‘ it is possible to calculate the posterior probability for every possible graph. \nFor ğ‘ larger than about 10, this is computationally infeasible, because there are too \nmany graphs, even if attention is restricted to only decomposable ones. The solution is \nto somehow explore the space of graphs, moving from one graph to another repeatedly. \nMadigan & Raftery (1994) presented methods for doing this in an ad-hoc way, for both \ndirected and undirected graphical models.  \nGiudici & Green (1999) gave a reversible-jump MCMC algorithm for approximating the \nposterior distributions of Î£ and the graph structure, in the case that attention is \nrestricted to decomposable graphs. The dimension-changing proposals consist of \nadding or deleting a single edge to the graph structure. The posterior graph distribution \nis taken to be the proportion of time spent at each graph. Asymptotically the Markov \nchain gives a sample from the exact true posterior distribution. Brooks et al (2003) give \nan adaptation of this method, and Green & Thomas (2013) give another MCMC algo-\nrithm for the same problem, which stores and manipulates not graphs but junction trees.  \nAs an alternative to MCMC, Jones et al (2005) proposed a â€œstochastic shotgun searchâ€ \nalgorithm for exploring either the space of all possible graphs or the space of all \ndecomposable graphs. At each step, this calculates the unnormalized posterior \nprobability of several neighbouring graphs, and then chooses which one to move to \naccording to a certain distribution based on those unnormalized probabilities.  \nSection 10.1 gives full descriptions of how the algorithms of Giudici & Green (1999) and \nJones et al (2005) can be adapted to the cases where attention is restricted to forests or \ntrees. Chapter 11 is about experiments to assess how well these adapted algorithms do.  \nMoghaddam et al (2009) propose a â€œneighbourhood fusionâ€ method for exploring the \nposterior graph distribution for general GGMs. To do this, for each node use lasso \n\n \n3.1 Bayesian methods \n \n23 \nregression or a similar method to estimate its neighbourhoods of all possible sizes, and \ncalculate a probability for each neighbourhood. Then repeatedly sample from these \npossible neighbourhoods, combine them to create a graph, and calculate its score. \nDobra et al (2011) give an MCMC method for general graphs. For non-decomposable \ngraphs, the method avoids the need to find the posterior normalizing constant of the \nHIW distribution, which is the most time-consuming part of the calculations.  \nAn alternative conjugate prior \nOne possible weakness of the HIW prior is that it only has one scalar parameter (a \nâ€œshapeâ€ parameter). Letac & Massam (2007) define an alternative prior, for decom-\nposable graphs, that has a scalar parameter for each possible clique and separator and \nis thus more flexible. This is a generalization of the HIW distribution and is still \nconjugate. Rajaratnam et al (2008) give a reference prior (in other words, a non-\ninformative priorâ€”see Rajaratnam et al 2008, page 2819, or Gelman et al 2004, page \n61) that is an improper special case of Letac & Massamâ€™s.  \nAn alternative method that just uses a prior for the covariance matrix \nThe rest of this thesis uses the HIW prior on Î£ (though many sections are more general \nand not directly related to GGMs or Î£). But this is not the only Bayesian method for \nlearning GGM structure. Wong et al (2003) give a prior for Î£âˆ’1 that enables its off-\ndiagonal elements to be zero with positive probability. In effect this combines the priors \nfor Î£ and the graphs into a single distribution. The prior is constructed as follows. \nFirstly they write Î£âˆ’1 as ğ‘‡ğ¶ğ‘‡, where ğ¶ is the negative partial correlation matrix and ğ‘‡ is \ndiagonal. ğ‘‡ğ‘–ğ‘–\n2 is given an uninformative gamma prior. For ğ¶ğ‘–ğ‘— they use a hierarchical \nprior: each element is zero (corresponding to the edge being absent) with a certain \nprobability, and then ğ¶ is distributed uniformly in the space of possible values. They \ndescribe a reversible-jump MCMC scheme for generating values of Î£âˆ’1. The proposal \ndistributions are the full conditional distributions of ğ‘‡ğ‘–ğ‘– and ğ¶ğ‘–ğ‘—, both approximated by \nnormal distributions. The distribution for ğ¶ğ‘–ğ‘— is a mixture that uses the indicator \nfunction ğ•€[ğ¶ğ‘–ğ‘—= 0].  \nThis method removes the need for a separate graph prior. It also applies to all graphs in \none go, whereas the HIW distribution described above is a separate distribution on Î£ \nfor every graph. Experiments in Wong et al (2003) suggest that when Î£âˆ’1 is sparse the \nmethod works well compared to the maximum likelihood estimator of Î£ and two \nestimators proposed by Yang & Berger (1994). The comparisons used two loss \nfunctions from the same paper.  \nWith this method it is not possible to use any detailed prior beliefs about the graph \nstructure. The user can only specify a prior distribution for ğœ“, the probability that each \nedge is present. Another possible disadvantage is that it is not possible to calculate \nanything about the posterior distribution exactly. In contrast, with the HIW prior there \nis an explicit formula for the posterior probabilities of decomposable graphs.  \n\n3 Structure-learning for GGMs \n \n 24 \n3.2 \nFrequentist methods  \nPreamble \nThere are also various frequentist methods for GGM structure-learning. These produce \na single graph rather than a distribution over a set of graphs. Albieri (2010) is a review \nand comparison of some of the main frequentist methods. Three of these are described \nbelow. See also Dobra et al (2004), Castelo & Roverato (2006), and chapter 20 of Koller \n& Friedman (2009).  \nSome of the many methods for DAG structure-learning may be suitable for GGMs. See \nfor example chapter 18 of Koller & Friedman (2009) or Gasse et al (2012). There has \nalso been various research on estimating the covariance matrix that makes little or no \nmention of graphs or graphical models, for example Yang & Berger (1994), Liechty et al \n(2004), or Bickel & Levina (2008).  \nThe simple frequentist method \nThis method is described by Albieri (2010) on pages 20â€“21. Find the sample covariance \nmatrix, invert it to find the sample precision matrix, and then standardize (see section \n2.3) to find the sample negative partial correlation matrix. Draw an edge between each \npair of nodes if and only if the magnitude of their sample partial correlation is above a \ncertain threshold. The appropriate threshold can be calculated using the fact that if the \ntrue partial correlation between two nodes is zero, then the sample partial correlation \nfollows a ğ‘¡-distribution (Lauritzen 1996, section 5.2.2; Albieri 2010, section 3.3.3), and \nusing multiple-testing procedures as described by Drton & Perlman (2007).  \nThe problem is that when ğ‘›< ğ‘, the sample covariance matrix is singular and cannot \nnecessarily be inverted. Formerly, the standard methods for graphical model structure-\nlearning were greedy stepwise forward-selection and backward-eliminationâ€”see \nWhittaker (1990, section 8.4) or Edwards (1995, sections 6.1â€“6.2). But these fail to \naccount for multiple testing (Edwards 1995, page 138).  \nThe shrinkage / empirical Bayes method \nThis method was proposed in SchÃ¤fer & Strimmer (2005a,b). To estimate the \ncovariance matrix, they use a linear â€œshrinkageâ€ of the unbiased estimator towards a \ndiagonal estimator in which the variances are not necessarily equal (SchÃ¤fer & \nStrimmer 2005b). This shrinkage estimator is always positive-definite, so it can be \ninverted to find estimators of the precision and partial correlation matrices.  \nThe next step is to test the partial correlations. The distribution of the estimated partial \ncorrelations is claimed to be similar to the exact distribution (Hotelling 1953), which \nappears in the maximum likelihood method. The number of degrees of freedom for this \ndistribution is estimated from the dataâ€”this is the â€œempirical Bayesâ€ step (SchÃ¤fer & \nStrimmer 2005a). This ultimately gives a threshold to which the estimated partial \ncorrelations are compared to decide which edges are present in the graph.  \n\n \n3.2 Frequentist methods \n \n25 \nLasso-type methods \nThe â€œlassoâ€ (Tibshirani 1996) is a method for estimating coefficients in standard linear \nmodels. Let {ğ‘¦ğ‘–} be the observations, {ğ‘¥ğ‘–ğ‘—} be the observed covariates, and {ğ›½ğ‘—} be the \nregression coefficients, and assume that ğ‘¦Ì… = 0. The coefficients are chosen to minimize \nthe residual sum of squares  \nâˆ‘(ğ‘¦ğ‘–âˆ’âˆ‘ğ›½ğ‘—ğ‘¥ğ‘–ğ‘—\nğ‘—\n)\n2\nğ‘–\n \nsubject to â€–ğ›½â€–1 â‰¤ğ‘¡. Here ğ‘¡ is a tuning parameter and â€–ğ›½â€–1 is the ğ¿1 norm of ğ›½, which is \nâˆ‘|ğ›½ğ‘—|\nğ‘—\n. This method often gives coefficients that are exactly zero, which means that the \ncorresponding covariates do not appear in the model.  \nSeveral methods inspired by the lasso have been proposed for GGM structure-learning. \nMeinshausen & BÃ¼hlmann (2006) proposed a â€œneighbourhood selectionâ€ method. For \neach node ğ‘–, do lasso regression with ğ‘‹ğ‘– as the observation and all the other nodes \n ğ‘‹ğ‘‰\\{ğ‘–,ğ‘—} as covariates; the nodes for which the regression coefficients are non-zero are \ntaken to be the estimated neighbourhood of ğ‘– in the graph. To estimate the whole graph \nstructure, the edge (ğ‘–, ğ‘—) is claimed to be present if and only if ğ‘– is in the estimated \nneighbourhood of ğ‘— and vice versaâ€”alternatively, the same thing but with â€œor vice versaâ€.  \nFriedman et al (2007) present a method that gives estimates of the graph structure and \nthe whole of the precision matrix. The idea is to maximize the log-likelihood penalized \nby the ğ¿1 norm of ğ¾,  \nlog|ğ¾| âˆ’tr(ğ‘†ğ¾) âˆ’ğœŒâ€–ğ¾â€–1 , \nover non-negative-definite matrices ğ¾. Here ğ¾= Î£âˆ’1, ğ‘† is the empirical covariance \nmatrix, ğœŒ is a tuning parameter, |ğ¾| is the determinant, and â€–ğ¾â€–1 = âˆ‘|ğ¾ğ‘–ğ‘—|\nğ‘–,ğ‘—\n (this sigma \nmeans a sum). This is equivalent to a minimization problem that resembles a lasso \nproblem as in Tibshirani (1996)â€”see Banerjee et al (2008) for details. Friedman et al \n(2007)â€™s contribution is the â€œgraphical lasso algorithmâ€ for solving the minimization \nproblem. This gives an estimate of Î£ that can be inverted reasonably fast to give an \nestimate of ğ¾. Their experiments suggest that their algorithm is much faster than the \nrival one in Banerjee et al (2008), but the computation time depends greatly on ğ‘.  \nYuan & Lin (2007) set out to maximize the same penalized log-likelihood, except that \nthey omit the diagonal elements of ğ¾ from the penalty. Meinshausen (2008) shows that \nthis method is not consistent for estimating the graph structure. For a certain graph and \ncovariance matrix, it gives the wrong graph structure in the â€œpopulation caseâ€, where \nthe MLE of the covariance equals the true covariance, and with positive probability in \nthe case of finite samples.  \nFinding hubs \nAlbieri (2010) compared the shrinkage / empirical Bayes method, the graphical lasso, \nand the PC algorithm of Kalisch & BÃ¼hlmann (2007), which is for structure-learning of \ndirected acyclic graphical models. She found that none of these algorithms was good at \ndiscovering hubs. Instead of finding hubs, these algorithms found that the hub and all \nthe nodes it is connected to were all connected, making a large complete subgraph.  \n\n \n26 \n4 \nCorrections to an algorithm for \nrecursive thinning \n4.1 \nMaximal prime decomposition and minimal triangulation \nThis chapter presents corrections to a graph-manipulation algorithm known as \nrecursive thinning. First it is necessary to explain minimal triangulation.  \nSection 3.1 described Bayesian structure-learning of GGMs with the generalized hyper \ninverse Wishart (G-Wishart) prior distribution on Î£. In this framework, finding the \nmarginal likelihood of a given graph requires finding its maximal prime decomposition. \nOlesen & Madsen (2002) is about how to find the maximal prime decomposition of a \ndirected graph. The same process works for undirected graphs, except that one step, \nâ€œmoralizationâ€, is omitted.  \nThe first step in finding the maximal prime decomposition is to find a minimal triangu-\nlation, which is defined as follows. Let (ğ‘‰, ğ¸) be a finite undirected graph. A triangu-\nlation of (ğ‘‰, ğ¸) is a set of extra edges ğ‘‡, often called fill edges, such that ğ¸âˆ©ğ‘‡= âˆ… and \n(ğ‘‰, ğ¸âˆªğ‘‡) is triangulated. As stated in section 2.1, triangulated graphs are the same as \ndecomposable or chordal graphs. A minimal triangulation is one such that removing \nany edge makes it no longer a triangulation. (Minimal triangulation is not necessary for \ngraphs that are already decomposable, but I am describing the general process.) \nMinimal triangulations are not the same as minimum triangulations; the latter are \ntriangulations for which there are no triangulations with fewer edges. Finding \nminimum triangulations is NP-hard (as proved in Yannakakis 1981).  \nThere are numerous algorithms to find minimal triangulations. Heggernes (2006) is a \nhistory and survey of these algorithms. She divides them into two main categories, \nbased on two different characterizations of triangulated graphs: (a) they have perfect \nelimination orders, and (b) every minimal separator is a clique. She gives brief \nexplanations of five or so algorithms in each category.  \nThe first algorithms for minimal triangulation were published in 1976. Two of these \ntake time ğ‘‚(ğ‘šğ‘›) = ğ‘‚(ğ‘›3), where ğ‘› is the number of nodes and ğ‘š is the number of \nedges of the untriangulated graph. Algorithms based on the separator-based character-\nization started to appear in the 1990s, and many more algorithms have appeared since \nthen. Heggernes (2006) makes no mention of graphical models or statistics, except for a \ncursory citation of a 1988 paper by Lauritzen and Spiegelhalter. The main applications \nthat she mentions are sparse matrix computations (not in a way that is directly relevant \nto graphical models) and solving systems of sparse linear equations.  \n\n \n4.2 Recursive thinning \n \n27 \nHeggernes (2006) also discusses a third class of ways to create a minimal triangulation: \ncreate a triangulation that is not necessarily minimal, and then remove excess edges to \ncreate a minimal triangulation. In this approach, the usual way to create a triangulation \nis elimination (which she calls Elimination Game). This works as follows. Put the nodes \nin some order, the â€œelimination orderingâ€; for each node in turn, add edges as necessary \nto make all the neighbours of the node be connected to each other, and then remove the \nnode and all its incident edges. The triangulation consists of all the edges that are added \nduring this loop. Heggernes gives four algorithms for finding a minimal triangulation by \nremoving excess extra edges from a triangulation that was created using elimination.  \nThere are many possible ways to choose an elimination ordering. It can even be chosen \nas the algorithm progresses. One popular ordering is â€œminimum degreeâ€, where at each \nstep you choose the remaining node that has the smallest degree (or one of these nodes, \nif there are more than one). This often creates minimal triangulations straight away, but \nnot always.  \nFor a slightly different use of triangulation in statistics or machine-learning, see MeilÇ & \nJordan (1997).  \n4.2 \nRecursive thinning \nThe R package â€œgRbaseâ€ (Dethlefsen & HÃ¸jsgaard 2005) includes a function called mini-\nmalTriang, which performs minimal triangulation. The main argument to this function \nis the graph for which a minimal triangulation is desired. As an optional argument, a \ntriangulation can be supplied; if it is not, then one is created using a function called \ntriangulate. The main body of minimalTriang is an algorithm that removes excess extra \nedges from the triangulation to create a minimal triangulation.  \nThe documentation for minimalTriang cites Olesen & Madsen (2002)â€”this is true as of \nFebruary 2013, when the most recent version of gRbase was version 1.6-7. The relevant \npart of Olesen & Madsen (2002) is 2, and the source of the algorithm is cited as \nKjaerulff (1993). The relevant part of Kjaerulff (1993) is chapter 1, which is the same as \nchapters 1 and 2 of Kjaerulff (1990), so I will just refer to the earlier document.  \nKjaerulff (1990) and Olesen & Madsen (2002) both call the algorithm â€œrecursive \nthinningâ€. Only Kjaerulff (1990)â€™s version of it is recursive, meaning that it calls itself. \nOlesen & Madsen (2002)â€™s version uses a Repeat loop and is not recursive, but it is \nessentially the same. In this chapter I will present non-recursive versions of algorithms, \nbecause I think these are easier to understand.  \nBoth Kjaerulff (1990) and Olesen & Madsen (2002) claim that the algorithm works on \nany triangulation, not just ones created by elimination. However, the algorithm as given \nin these two publications is not correct, even for triangulations created by elimination. \nThis chapter is concerned with correcting the algorithm for recursive thinning.  \nThe next sections present the incorrect recursive thinning algorithm, Algorithm I, and \nthen two corrected versions, Algorithms II and III, and proofs that these are correct. \nThe R function minimalTriang actually performs Algorithm III, not the incorrect \n\n4 Corrections to an algorithm for recursive thinning \n \n 28 \nalgorithm cited in its documentation. Algorithm II is a simplified version of Algorithm \nIII.  \nIt appears that Kjaerulff (1990) is not well known. Heggernes (2006) states that â€œIn \n1996, Blair et al â€¦ posed and solved the problem of making a given triangulation \nminimal by removing edges.â€ The reference is to Blair et al (2001), but this is precisely \nthe problem addressed by Kjaerulff (1990).  \n4.3 \nNotation \nI use a simplified version of the notation in Kjaerulff (1990) and Olesen & Madsen \n(2002). The list below gives my notation, the notation used in these two papers, and the \nvariable names in the R code for minimalTriang, to make it easy to compare the \ndifferent versions of the algorithms.  \nâ€¢ \nThe given graph is (ğ‘‰, ğ¸). \nâ€¢ \nğ‘‡ is the triangulation. Its initial value is the triangulation that is given as input to the \nalgorithm. (This is called ğ‘‡ in Olesen & Madsen 2002 and TT in minimalTriang.)  \nâ€¢ \nğº= (ğ‘‰, ğ¸âˆªğ‘‡) is the triangulated version of the graph.  \nâ€¢ \nIn Algorithm I, ğ‘ˆ is the set of edges that get removed on this iteration of the Repeat \nloop. (This is called ğ‘‡â€² in Kjaerulff 1990 and Olesen & Madsen 2002.)  \nâ€¢ \nIn Algorithm III, ğ‘…âŠ†ğ‘‡ is the set of edges that are candidates for removal. Edges are \nsometimes added to ğ‘…. (This is called ğ‘…â€² in Kjaerulff 1990 and Olesen & Madsen \n2002, and Rn in minimalTriang.)  \nâ€¢ \nIn Algorithm III, ğµ is the set of nodes at the end of edges that have been removed on \nthis iteration of the Repeat loop. (In minimalTriang, exclT is first the set of edges \nthat have been removed on this iteration, and then this set of nodes.)  \nğ‘‡, ğº, ğ‘…, and ğµ all change during the algorithm. ğ‘‡ and ğº always change at the same time, \nso it is always true that ğº= (ğ‘‰, ğ¸âˆªğ‘‡).  \n4.4 \nThe incorrect algorithm \nAlgorithm I is the incorrect algorithm as given in Olesen & Madsen (2002). (Kjaerulffâ€™s \nalgorithm starts with ğº= (ğ‘‰, ğ¸âˆªğ‘‡), whereas Olesen & Madsen start with ğº= (ğ‘‰, ğ¸); I \nthink this is a minor oversight in the latter.)  \n \nAlgorithm I: an incorrect method for recursive thinning \n1. Set ğ‘…= ğ‘‡. \n2. Repeat \n3.  \nSet ğ‘ˆ= {(ğ‘¥, ğ‘¦) âˆˆğ‘…âˆ¶ğº(ğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦)) is complete (in ğº)}. \n4.  \nSet ğ‘‡= ğ‘‡âˆ–ğ‘ˆ (and update ğº, which is (ğ‘‰, ğ¸âˆªğ‘‡)).  \n5.  \nSet ğ‘…= {ğ‘’1 âˆˆğ‘‡âˆ¶âˆƒğ‘’2 âˆˆğ‘ˆ such that ğ‘’1 âˆ©ğ‘’2 â‰ âˆ…}.  \n6. Until ğ‘ˆ= âˆ….  \n7. Return ğ‘‡. \n \n\n \n4.5 How the incorrect algorithm goes wrong \n \n29 \nLine 5 sets ğ‘… to be the set of remaining extra edges that share a node with one or more \nof the edges that was removed on this iteration of the Repeat loop. Line 9 in Algorithm \nIII does the same thing, though it is written differently. (There is no analogous line in \nAlgorithm II.)  \n4.5 \nHow the incorrect algorithm goes wrong \nThe fundamental problem with Algorithm I is that it removes more than one edge at a \ntime, instead of updating the graph after each individual edge-removal and checking \nwhether the other edges can still be removed. Algorithms II and III work correctly \nbecause they update ğ‘‡ and ğº after each individual edge-removal.  \nThe simplest example of a triangulation for which Algorithm I does not work is Figure \n4.1(a), where the solid lines are the edges in ğ¸ and the dashed lines are the edges in ğ‘‡. \nThe algorithm removes both edges at the first step.  \nKjaerulff (1990) and Olesen & Madsen (2002) state that the algorithm works on any \ntriangulation, but they seem to have in mind triangulations produced by elimination. \nThe simplest such triangulation for which it does not work is shown in Figure 4.1(b). \nAny elimination ordering that starts with the node at the bottom would produce this \ntriangulation.  \nThis is not minimum-degree eliminationâ€”the bottom node has the highest degree. \nHowever, the algorithm can also fail on triangulations created by minimum-degree \nelimination. An example graph can be constructed as follows. Start with the five-node \ngraph in Figure 4.1(b). For each node except the bottom one, add a clique of 10 nodes \nthat intersects with the original graph only at that node. The new nodes have degree 9, \nthe bottom node still has degree 4, and the other four nodes now have degree 12. \nMinimum-degree elimination will start with the bottom node and produce the two \nextra edges shown in Figure 4.1(b) (as well as many others), and Algorithm I will fail.  \n \n(a) \n \n \n(b) \n \nFigure 4.1. Two graphs, shown with solid lines, and triangulations of them, shown with dashed \nlines. (a) A triangulation for which Algorithm I does not work. (b) A triangulation produced by \nelimination for which Algorithm I does not work.  \n \n\n4 Corrections to an algorithm for recursive thinning \n \n 30 \nSimilarly, probably any other rule for choosing an elimination ordering will in some \ncases lead to the failure of the algorithm. For example, the algorithm will fail in any \ngraph where the graph in Figure 4.1(b) appears as an induced subgraph and the bottom \nnode comes first in the elimination ordering.  \nIncidentally, although Kjaerulff (1990) makes it clear, using unambiguous English and \nstandard notation, that his algorithm checks all the extra edges on the first run, in one \nexample (on pages 11â€“12) he checks the edges one at a time.  \n4.6 \nA correct algorithm \nAlgorithm II: a correct method for recursive thinning \n1. Put the edges in ğ‘‡ in some arbitrary order.  \n2. Repeat  \n3.  \nFor each edge (ğ‘¥, ğ‘¦) âˆˆğ‘‡ in turn, in order, \n4.  \n \nIf ğº(ğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦)) is complete (in ğº) \n5.  \n \n \nRemove (ğ‘¥, ğ‘¦) (from ğ‘‡ and ğº). \n6. Until â€œno edges were removed this timeâ€.  \n7. Return ğ‘‡. \n \nA preliminary result for proving the correctness of Algorithm II \nAssuming that ğº is triangulated, say that the edge (ğ‘¥, ğ‘¦) âˆˆğ‘‡ is â€œremovableâ€ from the \ncurrent ğ‘‡ if removing it does not make ğº become untriangulated.  \nProposition 4.1. (ğ‘¥, ğ‘¦) âˆˆğ‘‡ is removable if and only if the condition in line 4 of \nAlgorithm II is fulfilled. \nProof. Suppose the condition in line 4 is not fulfilled. There must be nodes ğ‘ and ğ‘ such \nthat {(ğ‘¥, ğ‘), (ğ‘, ğ‘¦), (ğ‘¦, ğ‘), (ğ‘, ğ‘¥)} âŠ†ğ¸âˆªğ‘‡ and (ğ‘, ğ‘) âˆ‰ğ¸âˆªğ‘‡. Removing (ğ‘¥, ğ‘¦)  would \nmake there be a chordless cycle of length four, ğ‘¥â€“ğ‘â€“ğ‘¦â€“ğ‘â€“ğ‘¥, which would mean that ğº \nwould no longer be triangulated. So the condition in line 4 is necessary for (ğ‘¥, ğ‘¦) to be \nremovable.  \nNow suppose the condition in line 4 is fulfilled. Firstly, suppose that removing (ğ‘¥, ğ‘¦) \nmakes there be a chordless cycle of length 5 or more. Then there must have been a \nchordless cycle of length 4 or more before (ğ‘¥, ğ‘¦) was removed. But ğº was triangulated, \nso this is impossible. Secondly, suppose that removing (ğ‘¥, ğ‘¦) causes the appearance of a \nchordless cycle of length 4, say ğ‘¥â€“ğ‘â€“ğ‘¦â€“ğ‘â€“ğ‘¥, where (ğ‘, ğ‘) âˆ‰ğ¸âˆªğ‘‡. This is impossible, \nbecause it contradicts the condition in line 4. So removing (ğ‘¥, ğ‘¦) does not lead to the \nappearance of any chordless cycles of length 4 or more. This shows that the condition in \nline 4 is sufficient for (ğ‘¥, ğ‘¦) to be removable. ï‚¨  \nSo in Algorithm II, the For loop simply checks each edge in ğ‘‡ in turn, and removes the \nedge if it is removable.  \n\n \n4.7 A second correct algorithm \n \n31 \nProof of correctness for Algorithm II \nI will use the word â€œrunâ€ to refer to a single iteration of the Repeat loop. It suffices to \nprove that (a) the final ğ‘‡ is a triangulation, (b) this triangulation is minimal, and (c) the \nalgorithm finishes in finite time.  \n(a) ğº is triangulated to start with, by definition. It is only ever modified by the removal \nof an edge, which happens when the condition in line 4 is fulfilled. ğº remains triangu-\nlated after every such removal, by Proposition 4.1. Therefore ğº is always triangulated \nand ğ‘‡ is always a triangulation.  \n(b) On the final run, the algorithm checks the condition in line 4 for every edge in ğ‘‡, and \nfinds that it is not fulfilled for any of them. By Proposition 4.1, this means that removing \nany of the edges in ğ‘‡ would make ğº become untriangulated. In other words, the \ntriangulation is minimal.  \n(c) Let ğ‘¡ be the initial number of edges in ğ‘‡. On each run except the last, the Repeat loop \nremoves at least one edge. So the largest number of times that the Repeat loop can be \ncarried out is ğ‘¡+ 1, which is finite. On each run, the For loop checks all the remaining \nedges in ğ‘‡. On the ğ‘–th run, the remaining number of edges in ğ‘‡ is at most ğ‘¡âˆ’ğ‘–+ 1. This \nis also finite, so the algorithm is certain to finish in finite time. ï‚¨  \n4.7 \nA second correct algorithm \nAlgorithm III: a second correct method for recursive thinning \n1. Put the edges in ğ‘‡ in some arbitrary order (minimalTriang uses lexicographic order).  \n2. Set ğ‘…= ğ‘‡. \n3. Repeat \n4.  \nSet ğµ= âˆ…. \n5.  \nFor each edge (ğ‘¥, ğ‘¦) âˆˆğ‘… in turn, in order,  \n6.  \n \nIf ğº(ğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦)) is complete (in ğº) \n7.  \n \n \nRemove (ğ‘¥, ğ‘¦) (from ğ‘‡ and ğº) \n8.  \n \n \nAdd ğ‘¥ and ğ‘¦ to ğµ  \n9.  \nSet ğ‘…= {(ğ‘¥, ğ‘¦) âˆˆğ‘‡: ğ‘¥âˆˆğµ or ğ‘¦âˆˆğµ}  \n10. Until ğµ= âˆ….  \n11. Return ğ‘‡. \n \nAn example of how Algorithms II and III are different \nFigure 4.2 shows an example of how Algorithms II and III sometimes do not remove the \nsame edges as each other on every run. On the first run, Algorithm III does not include \nâ‘ â€“â‘¢ in ğ‘…, so on the second run it misses the chance to remove this edge.  \nThe intention of Algorithm I \nIn Algorithm I, the idea of ğ‘… was that you can save time by not checking edges that you \nknow cannot be removed (Kjaerulff 1990). The same concept is used in Algorithm III, \nwhich is a corrected version of Algorithm I. The idea is that on the next run there is no \n\n4 Corrections to an algorithm for recursive thinning \n \n 32 \npoint checking edges for which the sets of neighbours of the two nodes did not change \non this runâ€”because even if you check these edges, they will not be removed.  \nBut this idea is mistaken, because the sets of neighbours sometimes change from one \niteration of the For loop to the next. And if the sets of neighbours change, then it may \nbecome possible to remove the edge. This is illustrated in Figure 4.2. For both \nalgorithms, when the second run starts, the sets of neighbours for â‘ â€“â‘¢ are the same \nas in the first run. But in Algorithm II when the For loop gets round to checking â‘ â€“â‘¢, \nthe neighbours have changed and the edge gets removed.  \nFigure 4.2 also shows that in Algorithm III it is possible for an edge to be excluded from \nğ‘… but later reappear in it and be removed.  \n \n \nFigure 4.2. A graph and a triangulation for which Algorithms II and III do not remove the same \nextra edges on each run. The solid edges are the graph and the dashed edges are the extra edges. \nOn each run the edges are checked are checked in the order â‘ â€“â‘¡, â‘ â€“â‘¢, â‘¡â€“â‘¤. Each \nalgorithm also does one final run, which is not shown, in which no edges are removed. (The \ngraph itself is already triangulated, so the triangulation is pointless, but this is just an example \nfor illustration.)  \n \n\n \n4.7 A second correct algorithm \n \n33 \nProof of correctness for Algorithm III \nAgain I will use â€œrunâ€ to refer to a single iteration of the Repeat loop.  \nAlgorithm III is the same as Algorithm II, except that on each run Algorithm II checks all \nthe edges in ğ‘‡, whereas Algorithm III only checks the edges in ğ‘…, which is always the \nsame as ğ‘‡ or a subset of it. When the For loop finishes, ğµ is the set of nodes such that \nedges incident to them have been removed during the current run. So line 9 has the \neffect of ensuring that if (ğ‘¥, ğ‘¦) âˆˆğ‘‡âˆ–ğ‘… then neither ğ‘›ğ‘’(ğ‘¥) nor ğ‘›ğ‘’(ğ‘¦) changed during the \ncurrent run, which means that ğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦) did not change during the current run. \nThe contrapositive of this is that if ğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦) changed during the current run, then \n(ğ‘¥, ğ‘¦) âˆˆğ‘…. (The converse is not trueâ€”sometimes line 9 puts (ğ‘¥, ğ‘¦) in ğ‘… even though \nğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦) did not change during the current run.)  \nLine 10 in Algorithm III is the same as line 6 in Algorithm II. The equivalence of the \nother parts of the two algorithms is obvious.  \nIt suffices to prove that on the final run of Algorithm III, the edges that are not checked \nare not removable. The proof will work by considering an edge (ğ‘¥, ğ‘¦) that does not get \nchecked on the final run, and looking back through the runs to find the last run where it \nwas checked. When (ğ‘¥, ğ‘¦) was last checked, it was obviously found to be unremovable. \nIt will be shown that after that ğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦) never changed, from which it follows that \n(ğ‘¥, ğ‘¦) is not removable during the final run.  \nSuppose that (ğ‘¥, ğ‘¦) does not get checked on the final run, and say this run was the ğ‘–th. \nJust after line 9 on the (ğ‘–âˆ’1)th run, (ğ‘¥, ğ‘¦) must have been in ğ‘‡âˆ–ğ‘…. This means that \nğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦) did not change during the (ğ‘–âˆ’1)th run.  \nEither (a) (ğ‘¥, ğ‘¦) was checked during the (ğ‘–âˆ’1)th run, or (b) it was not. If (a), then \nğº(ğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦)) must have been found to be incomplete, otherwise (ğ‘¥, ğ‘¦) would have \nbeen removed from ğ‘‡. Since ğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦) did not change during this run, even if (ğ‘¥, ğ‘¦) \nwas checked on the final run then ğº(ğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦)) would still be found to be \nincomplete (since edges are never added to ğº). In other words, during the final run, \n(ğ‘¥, ğ‘¦) is not removable.  \nIf (b), then (ğ‘¥, ğ‘¦) must have been in ğ‘‡âˆ–ğ‘… just after line 9 in the (ğ‘–âˆ’2)th run. This \nmeans that ğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦) did not change during the (ğ‘–âˆ’2)th run. Either (a2) (ğ‘¥, ğ‘¦) \nwas checked during the (ğ‘–âˆ’2)th run, or (b2) it was not. If (a2), then ğº(ğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦)) \nmust have been found to be incomplete during the (ğ‘–âˆ’2)th run. It is now known that \nğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦) did not change during the (ğ‘–âˆ’2)th or (ğ‘–âˆ’1)th runs. It follows that \nğº(ğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦)) is still incomplete after line 9 in the (ğ‘–âˆ’1)th run, so on the final run \n(ğ‘¥, ğ‘¦) is not removable.  \nIf (b2), then (ğ‘¥, ğ‘¦) must have been in ğ‘‡âˆ–ğ‘… just after line 9 in the (ğ‘–âˆ’3)th run. This \nreasoning can be continued backwards through the runs. All the edges were checked on \nthe first run, so eventually this search backwards through the runs is certain to find a \nrun where (ğ‘¥, ğ‘¦) was checked and ğº(ğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦)) was found to be incomplete; and \nğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦) has not changed since this runâ€”if it had changed, (ğ‘¥, ğ‘¦) would have \nbeen put in ğ‘… and checked on the next run. It follows that ğº(ğ‘›ğ‘’(ğ‘¥) âˆ©ğ‘›ğ‘’(ğ‘¦)) is still not \n\n4 Corrections to an algorithm for recursive thinning \n \n 34 \ncomplete after line 9 in the (ğ‘–âˆ’1)th run, so even if (ğ‘¥, ğ‘¦) were checked on the ğ‘–th run it \nwould not be removed. ï‚¨  \n4.8 \nComments on the two correct algorithms \nUnderlying the two proofs is the result, quoted in Heggernes (2006), that â€œif ğºâŠ‚ğ» for \ntwo chordal graphs ğº and ğ» on the same vertex set, then there is a sequence of edges \nthat can be removed from ğ» one by one, such that the resulting graph after each \nremoval is chordal, until we reach ğº.â€ This explains why it is sensible to remove one \nedge at a time.  \nThe edges can be checked in different orders on different runs. The proofs make no \nassumptions about these orders. However, it is natural in writing a computer program \nto make it check the edges in the same order on every run. \nThe R function minimalTriang performs Algorithm III plus various checks. For example, \nit checks whether (ğ‘‰, ğ¸) is triangulated at the start and whether ğº is triangulated at the \nend. The test in line 10 of Algorithm III is actually done before line 9, and if the result is \ntrue then the process breaks out of the Repeat loop.  \n4.9 \nWhich of the correct algorithms is faster? \nIn the example in Figure 4.2, Algorithm II is faster than Algorithm III. Algorithm II can \nalso be faster for a triangulation produced by minimum-degree elimination. An example \ncan be constructed from the original graph in Figure 4.2 (the graph with the four solid \nedges). Add two nodes that are connected to each other and node 5, and add two nodes \nthat are connected to each other and node 2. One possible minimum-degree elimination \nbegins by eliminating nodes 4, 3, and 1 and creates the same three extra edges as in \nFigure 4.2. Algorithms II and III proceed in the same way as in Figure 4.2, and \nAlgorithm II is faster.  \nOn the other hand, Algorithm III is sometimes faster than Algorithm II, even with \ntriangulations produced by minimum-degree elimination ordering. Again consider the \ngraph and the triangulation shown at the top of Figure 4.2, or the ones described in the \nprevious paragraph. But this time suppose the edges are checked in the order 1â€“3, 1â€“2, \n2â€“5. On the first run both algorithms remove 2â€“5, on the second run they both remove \n1â€“2, and on the third run they both remove 1â€“3. But on the second run, Algorithm III \nsaves time by not checking 1â€“3. This makes it faster overall, assuming that creating ğµ \nand ğ‘… does not take any time.  \nKjaerulff (1990) recommends checking the edges in the reverse of the order in which \nthey were added during the elimination-ordering algorithm. It is not clear whether one \nof the two correct algorithms is always faster than the other if this advice is followed.  \nI carried out an experimental comparison of the two correct algorithms in R, using \nsimplified versions of minimalTriang. To test the two algorithms it is necessary to \ncreate triangulations. The R function triangulate uses a version of minimum-degree \nelimination ordering that very often creates a minimal triangulation straight away, so \nthat the recursive thinning algorithm has nothing to do. So I wrote a simpler triangu-\n\n \n4.10 What is the best algorithm for minimal triangulation? \n \n35 \nlation function that does elimination with the nodes in their natural order. The idea was \nthat this would create non-minimal triangulations more often.  \nI created 100,000 random non-decomposable graphs on 30 nodes, by choosing them \nuniformly at random from among all such graphs, with replacement. I then created \ntriangulations of all these graphs and ran the two programs on the graphs and their \ntriangulations. Algorithm II took 767 seconds of CPU time and Algorithm III took 830 \nseconds.  \n4.10 What is the best algorithm for minimal triangulation? \nHeggernes (2006) reports that the fastest known algorithm for minimal triangulation is \nğ‘œ(ğ‘›2.376), where ğ‘› is the number of nodes. This algorithm appears in Heggernes et al \n(2005). The fastest algorithms are rather complicated, and their high asymptotic \nspeeds rely on the detailed manipulations being done in specially fast ways.  \nIf a fast algorithm was wanted, my personal choice would be MCS-M (Berry et al 2004). \nThis has asymptotic speed ğ‘‚(ğ‘šğ‘›), where ğ‘š is the number of edges of the untriangu-\nlated graph. MCS-M is not especially simple, since it requires searching along paths \nwhere the nodes fulfil a certain condition. This is more complicated to program than \nmerely checking whether the neighbours of a node are connected, which is how Algo-\nrithms II and III work. Unless speed is paramount, it seems sensible to use minimum-\ndegree elimination followed by Algorithm II or III to remove excess edges, especially as \nminimum-degree elimination often produces minimal triangulations straight away.  \nR is much slower than general-purpose programming languages, so there would be no \npoint in rewriting minimalTriang to use an algorithm that is theoretically or \nasymptotically superior. If speed was important, it would be more sensible to rewrite \ntriangulate or the main body of minimalTriang in C or Fortran and call these from R, or \nabandon R and use a different programming language.  \n \n \n\n \n36 \n5 \nRandom graph distributions \n5.1 \nTwo ways of looking at graph distributions \nBayesian structure-learning of graphical models involves probability distributions on \nsets of graphs. One of the first steps for the user is to specify a prior distribution that \naccords with their beliefs about which graphs are more or less likely. This chapter is \nabout probability distributions on graphs and in particular about prior distributions in \nBayesian structure-learning.  \nThere are two ways of looking at or defining probability distributions on sets of graphs. \nThe first is that you have a set of graphs and a formula that can be applied to any of \nthese graphs to give its probability, or its unnormalized probability. Obviously the \nprobabilities are all non-negative and sum to 1. This I will call a â€œgraph distributionâ€. \nThe second is a â€œrandom graph modelâ€, which is essentially a random or partly random \nprocedure for constructing a graph.  \nAny probability distribution on a set of graphs can be defined in either of these two \nways. But in practice the two ways of looking at these distributions are different. If you \nare given a graph distribution, it may be difficult to generate a graph from it. Conversely, \nif you are given a random graph model, it may be difficult to calculate the probability of \na given graph.  \nIf you have a graph distribution, MCMC can be used to generate a sample of graphs that \napproximately follow the distribution. The acceptance probability for moving from ğº to \nğºâ€² is  \nmin {1,  ğ‘(ğºâ€²)\nğ‘(ğº)\nğ‘(ğºâ€² â†’ğº)\nğ‘(ğºâ†’ğºâ€²)} , \nwhere ğ‘ is the graph distribution and ğ‘(ğº1 â†’ğº2) is the probability of proposing to \nmove to ğº2 if the current graph is ğº1. In principle, the proposal distribution ğ‘ can be \nchosen arbitrarily as long as the Markov chain is irreducible and aperiodic.  \nWhen dealing with prior distributions for graphical model structure-learning, it is \nnecessary to calculate the probability of a given graph, so it is natural to work with \ngraph distributions rather than random graph models. However, you might also want to \nbe able to generate from the distribution, for example to empirically evaluate whether it \nencourages hubs.  \nGraph distributions have been the subject of some research in the context of graphical \nmodel structure-learning. But random graph models have been the subject of far more \n\n \n5.2 ErdÅ‘sâ€“RÃ©nyi random graphs \n \n37 \nresearch, in other contexts, as described in the next two sections. In this chapter, the \nnumber of nodes in the graph is ğ‘›.  \n5.2 \nErdÅ‘sâ€“RÃ©nyi random graphs  \nThe first random graph models to be studied in depth were ErdÅ‘sâ€“RÃ©nyi graphs. These \ntwo models will be referred to in several sections of this chapter and in section 6.2. The \nfirst ErdÅ‘sâ€“RÃ©nyi model is ğº(ğ‘›, ğ‘), in which there are ğ‘› nodes and each edge appears \nindependently with probability ğ‘, and the second is ğº(ğ‘›, ğ‘€), where there are ğ‘› nodes \nand ğ‘€ edges and all such graphs have equal probability. ğ‘€ and ğ‘ are usually ğ‘€(ğ‘›) and \nğ‘(ğ‘›), functions of ğ‘›. The first ErdÅ‘sâ€“RÃ©nyi model was introduced in Gilbert (1959) and \nthe second was introduced in ErdÅ‘s & RÃ©nyi (1959).  \nErdÅ‘sâ€“RÃ©nyi random graph theory is covered in depth by BollobÃ¡s (2001). It is mainly \nconcerned with approximating the proportion of graphs that have a certain property \nand seeing what happens as ğ‘›â†’âˆ. In many cases, either almost every graph has the \nproperty (in other words, the proportion of graphs with the property tends to 1 as ğ‘›â†’\nâˆ) or almost every graph does not have the property. The preface of BollobÃ¡s (2001) \nsays that the main omission from this book is probably random trees, which are \ncovered in chapter 7 of Moon (1970).  \nThe notations ğº(ğ‘›, ğ‘) and ğº(ğ‘›, ğ‘€) may seem ambiguous, because the second parameter \nhas two possible meanings, but it is uncommon to write specific numbers or formulas \ninside the brackets. Alternative notations include ğ’¢ğ‘, ğ’¢(ğ‘), ğ’¢ğ‘›,ğ‘, and ğ’¢ğ‘›,ğ‘š. ğº(ğ‘›, ğ‘) is \nsometimes called the Bernoulli random graph or the binomial model, and ğº(ğ‘›, ğ‘€) is \nsometimes called the uniform model (Janson et al 2000, page 2). For many questions, \nresults about these two different models are very similar. The two models are in certain \nsenses equivalent, as shown by Theorem 2.2 in BollobÃ¡s (2001) and a stronger result in \nÅuczak (1990).  \nErdÅ‘sâ€“RÃ©nyi graphs are clearly random graph models as defined in section 5.1. \nHowever, given an ErdÅ‘sâ€“RÃ©nyi model, it is also easy to calculate the probability of any \ngiven graph. So they could also be regarded as graph distributions.  \n5.3 \nComplex networks \nâ€œRandom graphsâ€ is sometimes taken to mean ErdÅ‘sâ€“RÃ©nyi random graphs. An example \nof this usage is in Watts & Strogatz (1998). In ErdÅ‘sâ€“RÃ©nyi graphs, the node degrees \nfollow an approximate Poisson distribution, which means that most nodes have similar \ndegrees (BarabÃ¡si & Oltvai 2004, Jeong et al 2000). Since the late 1990s a consensus \nhas emerged that these graphs are usually unsuitable for modelling networks in the real \nworld.  \nRandom graph models that are intended to model real-world networks have come to be \nknown as â€œcomplex networksâ€. Some research on complex networks is not mathe-\nmatically rigorous and instead demonstrates properties by means of experiments on \ncomputer. Examples of this type of research are Watts & Strogatz (1998) and BarabÃ¡si \n& Albert (1999), which have both been highly influential. The range of random graph \n\n5 Random graph distributions \n \n 38 \nmodels or complex networks that have been proposed, studied, and used is discussed in \ndepth in Newman (2003).  \nOne random graph model discussed in Newman (2003) is the â€œscale-freeâ€ graphs of \nBarabÃ¡si & Albert (1999). These are described in section 2.4, about biomolecular \nnetworks.  \nAnother random graph model that is currently the subject of research is the configu-\nration model. This starts with a fixed degree for each vertex. A random graph is \ngenerated by creating the appropriate number of half-edges for each node, and then \njoining the half-edges in pairs uniformly at random. Doing this can lead to self-edges \nand multi-edges, but asymptotically the proportion of these is small (Molloy & Reed \n1995).  \nThe configuration model is used to model social networks or networks of human \ncontact. These networks are then used for modelling the spread of epidemicsâ€”see for \nexample Andersson (1998) or Britton et al (2007, 2011). The configuration model has \nbeen used to prove asymptotic mathematical theorems about graphs that are chosen \nuniformly at random from among all those that have a given degree sequence (Molloy & \nReed 1995).  \nSimilar to the configuration model is the expected-degree model (Chung & Lu 2002a,b, \n2006; Chung et al 2003), in which each node ğ‘£ğ‘– has a weight ğ‘¤ğ‘–, and the edge (ğ‘£ğ‘–, ğ‘£ğ‘—) is \npresent with probability ğ‘¤ğ‘–ğ‘¤ğ‘—/âˆ‘ğ‘¤ğ‘˜, independent of all the other edges. If self-edges are \npermitted then ğ”¼(deg(ğ‘£ğ‘–)) = ğ‘¤ğ‘–. For this model, the probability of a given graph can \neasily be calculated. The expected-degree model is an example of a factored distri-\nbutionâ€”see the next section.  \n5.4 \nFactored distributions \nDefinitions \nThis section is about a certain class of graph distributions that appears in several \ncontexts. I will refer to these as â€œfactoredâ€ distributions (following MeilÄƒ & Jaakkola \n2006). The number of nodes is fixed. A factored distribution on a set of graphs is one \nwhere each edge has a weight, ğ‘¤ğ‘’, and the probability of each graph is proportional to \nthe product of the weights of the edges in that graph. In symbols, \n                                                                      â„™(ğº) âˆâˆğ‘¤ğ‘’\nğ‘’âˆˆğ¸ğº\n ,                                                           (1) \nfor ğºâˆˆğ’¢, where ğ’¢ is the set of graphs under consideration; and â„™(ğº) = 0 for ğºâˆ‰ğ’¢. It \nwill be useful later to write the definition with an equals sign:  \n                                                             â„™(ğº) =\nâˆ\nğ‘¤ğ‘’\nğ‘’âˆˆğ¸ğº\nâˆ‘\nâˆ\nğ‘¤ğ‘’\nğ‘’âˆˆğ¸ğ»\nğ»âˆˆğ’¢\n .                                                       (2) \nLet ğ¸ğ‘ğ‘™ğ‘™ be the set of all (ğ‘›\n2) possible edges. Any distribution where  \n\n \n5.4 Factored distributions \n \n39 \n                                                       â„™(ğº) âˆâˆğ‘ğ‘’\nğ‘’âˆˆğ¸\nâˆ(1 âˆ’ğ‘ğ‘’)\nğ‘’âˆˆğ¸ğ‘ğ‘™ğ‘™âˆ–ğ¸\n ,                                             (3) \nfor some {ğ‘ğ‘’}, is also factored, since this expression can be written as  \nâˆ\nğ‘ğ‘’\n1 âˆ’ğ‘ğ‘’\nğ‘’âˆˆğ¸\n âˆ(1 âˆ’ğ‘ğ‘’) âˆ âˆ\nğ‘ğ‘’\n1 âˆ’ğ‘ğ‘’\nğ‘’âˆˆğ¸\nğ‘’âˆˆğ¸ğ‘ğ‘™ğ‘™\n = âˆğ‘¤ğ‘’\nğ‘’âˆˆğ¸\n , \nwhere ğ‘¤ğ‘’= ğ‘ğ‘’/(1 âˆ’ğ‘ğ‘’) (which is the odds that ğ‘’âˆˆğ¸ in the case described in the next \nsubsection).  \nThe set of all graphs \nLet ğ’¢ğ‘ğ‘™ğ‘™ be the set of all 2(ğ‘›\n2) graphs. If ğ’¢= ğ’¢ğ‘ğ‘™ğ‘™, then factored distributions are ones \nwhere each edge is present or absent with a fixed probability and all these events are \nindependent. Moreover, ğ‘ğ‘’ is the probability that ğ‘’âˆˆğ¸ and ğ‘¤ğ‘’ is the odds of the same \nevent. To see these facts, use definition (3), and note that  \nâˆ‘{âˆğ‘ğ‘’\nğ‘’âˆˆğ¸ğº\n \nâˆ\n(1 âˆ’ğ‘ğ‘’)\nğ‘’âˆˆğ¸ğ‘ğ‘™ğ‘™âˆ–ğ¸ğº\n}\nğºâˆˆğ’¢ğ‘ğ‘™ğ‘™\n= âˆ(ğ‘ğ‘’+ (1 âˆ’ğ‘ğ‘’))\nğ‘’âˆˆğ¸ğ‘ğ‘™ğ‘™\n= 1 . \nIt follows that the proportional-to symbol in definition (3) can be replaced by an equals \nsign: \nâ„™(ğº) = âˆğ‘ğ‘’\nğ‘’âˆˆğ¸\n âˆ(1 âˆ’ğ‘ğ‘’)\nğ‘’âˆ‰ğ¸\n . \nThis is essentially the definition of the presence or absence of each edge being \nindependent.  \nTrees and forests \nIf ğ’¢ is the set of forests or trees, then in a factored distribution the edges are not present \nor absent independently of each other, because the graph is constrained to be a forest \nor tree. This was pointed out by MeilÄƒ & Jaakkola (2006). \nIf ğ’¢ is the set of trees, then the products in the numerator of (2) all have the same \nnumber of terms (namely ğ‘›âˆ’1). So  \nâ„™(ğº) = âˆ\nğ‘¤ğ‘’\n(âˆ‘\nâˆ\nğ‘¤ğ‘’\nğ‘’âˆˆğ¸ğ»\nğ»âˆˆğ’¢\n)\n1/(ğ‘›âˆ’1)\nğ‘’âˆˆğ¸ğº\n . \nThis now has the very simple form â„™(ğº) = âˆ\nğ‘¤ğ‘’\nğ‘’âˆˆğ¸\n. (To convert to this form, replace \neach of the original ğ‘¤ğ‘’â€™s with ğ‘¤ğ‘’/(âˆ‘\nâˆ\nğ‘¤ğ‘’\nğ‘’âˆˆğ¸ğ»\nğ»âˆˆğ’¢\n)1/(ğ‘›âˆ’1).) However, the form with the \nproportional-to symbol is more natural, since this is how factored distributions arise as \nprior distributions that are inferred from expert knowledge, as suggested by Madigan & \nRaftery (1994), and as posterior distributions.  \n\n5 Random graph distributions \n \n 40 \nUses of factored distributions \nFactored distributions are graph distributions, rather than random graph models. \nHowever, in the case that ğ’¢= ğ’¢ğ‘ğ‘™ğ‘™, it is easy to generate from them, since all the edges \nare independent, so they could be regarded as random graph models.  \nThe use of factored distributions as prior distributions for graphical model structure-\nlearning seems to have been first proposed by Madigan & Raftery (1994). They suggest \ngetting an expert to estimate the probability of each edge being present and assuming \nthat the presences of the edges are mutually independent.  \nThe results in MeilÄƒ & Jaakkola (2006) are all about factored distributions on trees. For \nBayesian structure-learning of discrete-valued tree graphical models, they prove that \nunder certain assumptions the graph posterior is a factored distribution. Chapter 8 \ndescribes methods for analyzing factored distributions for trees, based on MeilÄƒ & \nJaakkola (2006), and methods for generating from these distributions. Section 7.4 \ndescribes how the Chowâ€“Liu algorithm can be used if the prior is a factored \ndistribution on trees.  \n5.5 \nGraph priors that have been proposed \nPriors for undirected graphs \nIn Bayesian structure-learning it is necessary to be able to calculate the probability of a \ngiven graph, so in practice prior distributions are invariably defined as graph distri-\nbutions, rather than random graph models.  \nThe simplest type of graph prior distribution is the uniform distribution, where each of \nthe graphs under consideration is equally likely. Priors of this type have been used by \nCooper & Herskovits (1992), Madigan & Raftery (1994), Giudici (1996), Giudici & \nGreen (1999), Roverato (2002), Atay-Kayis & Massam (2005), Dobra et al (2011), \nWang & Li (2012), and others. If you are considering all graphs, or all decomposable \ngraphs, then the uniform distribution gives higher probability to medium graph sizesâ€”\nthe â€œsizeâ€ of a graph is the number of edges it hasâ€”than to small or large sizes (Giudici \n& Green 1999, Jones et al 2005, Carvalho & Scott 2009, Armstrong et al 2009).  \nFor undirected graphical models, several alternatives have been proposed in published \nresearch. One is the â€œsize-based priorâ€ of Armstrong et al (2009). In this distribution, \nnon-decomposable graphs have probability zero, all sizes are equally likely, and all \ndecomposable graphs of the same size are equally likely. They also propose a more \ngeneral hierarchical prior distribution in which the size has a binomial distribution, \nâ„™(size = ğ‘˜) = ((ğ‘›\n2)\nğ‘˜) ğœ“ğ‘˜(1 âˆ’ğœ“)(ğ‘›\n2)âˆ’ğ‘˜  for ğ‘˜= 0,1, â€¦ , (ğ‘›\n2), \nthe binomial parameter ğœ“ has a beta distribution, and again all decomposable graphs of \nthe same size are equally likely. If ğœ“< 0.5 then more probability is given to sparser \ngraphs.  \n\n \n5.5 Graph priors that have been proposed \n \n41 \nPart of the motivation for the size-based prior distribution was the belief that the graph \nis sparse. But the prior fails to reflect this belief in a sensible way, because it does not \ntake account of how many graphs there are that have each size. For example, it often \ngives lower probability to each of the graphs of size (ğ‘›\n2) âˆ’1 than to the complete graph. \nMore probability is assigned to the size (ğ‘›\n2) âˆ’1, but it gets shared out among many \ngraphs. Any sensible â€œsparsity-encouragingâ€ prior would surely give higher probability \nto any graph of size ğ‘˜âˆ’1 than to any graph of size ğ‘˜, especially in the case ğ‘˜= ğ‘›. \nConsequently it does not seem sensible to assign a probability to a size without taking \ninto account how many graphs have that size.  \nDobra et al (2004) and Jones et al (2005) use â„™(ğº) = ğ›½|ğ¸ğº|(1 âˆ’ğ›½)(ğ‘›\n2)âˆ’|ğ¸ğº|, where ğ›½âˆˆ\n[0,1]. When all graphs are being considered, this is the first ErdÅ‘sâ€“RÃ©nyi graph model, \nwhere each edge is present independently with probability ğ›½. When only decomposable \ngraphs are being considered, it is not. Jones et al (2005) call this the Bernoulli prior but \nI will call it the â€œbinomial priorâ€. Carvalho & Scott (2009) say that this prior distribution \nis â€œrapidly becoming the standard,â€ and they use an adaptation of it in which there is a \nhierarchical prior distribution on ğ›½.  \nBornn & Caron (2011) propose a class of priors for decomposable graphs that are \ncalculated using the cliques and separators. The main one they suggest is  \nâ„™(ğº) âˆ\nâˆğ‘(|ğ¶| âˆ’1)!\nğ¶\nâˆğ‘(|ğ‘†| âˆ’1)!\nğ‘†\n . \nThe product in the denominator is not over the collection of separators, as in section \n3.1, but over the collection of non-empty separators. (Their more general prior has \nfunctions ğœ“ğ¶(ğ¶ğ‘—) and ğœ“ğ‘†(ğ‘†ğ‘—) in the numerator and denominator, â€œwith the convention \nthat ğœ“ğ‘†(âˆ…) = 1,â€ but the rest of the paper makes it clear that ğ‘†ğ‘— is never âˆ….) The \nparameters ğ‘ and ğ‘ can be adjusted to encourage or discourage cliques and non-empty \nseparators respectively. The main aim of these priors is to express the belief that the \nnodes should be clustered in cliques, especially non-overlapping cliques, rather than \nspread out in long lines.  \nThomas et al (2008) use undirected graphical models to analyze residue positions in \nproteins. They describe a â€œcontact graph priorâ€, which only permits edges between \npairs of residue that are within a certain physical distance of each other. \nAs mentioned in section 5.4, Madigan & Raftery (1994) proposed factored priors, where \neach edge has a fixed probability of appearing in the graph and the presences of all the \nedges are independent. These can be used for both undirected graphical models and \ndirected acyclic ones. \nPriors for DAGs \nFor directed acyclic graphs, Heckerman et al (1995) assume that the user can express \ntheir prior beliefs in the form of a single graph. They assign prior probabilities to \ngraphs by penalizing them according to the number of edges that are different \ncompared to the userâ€™s graph. Buntine (1991) describes how to convert an expertâ€™s \n\n5 Random graph distributions \n \n 42 \nbeliefs on the probability of each edge into a prior distribution. Heckerman et al \n(1997/1999/2006) use a uniform prior.  \nMukherjee & Speed (2008) propose a more elaborate class of graph priors based on the \nproperties that the graph is believed to have. These priors are of the form  \nâ„™(ğº) âˆexp (ğœ†âˆ‘ğ‘¤ğ‘–ğ‘“ğ‘–(ğº)\nğ‘–\n), \nwhere each ğ‘“ğ‘– is a â€œconcordance functionâ€ that increases as ğº matches the prior belief \nmore closely, and the ğ‘¤ğ‘– are weights. By using several ğ‘“ğ‘–â€™s it is possible to combine \nseveral prior beliefs. They give possible ğ‘“ğ‘–â€™s for several types of prior belief, for example \nthat certain edges are likely to be present or absent, that edges between two groups of \nvertices are unlikely (which they say is common in molecular biology), or that the \nnodes are unlikely to have many edges into them. They also give an ğ‘“ğ‘– for the belief that \nthe degree distribution is likely to be scale-free (see section 2.4). Also given are three \nreferences (19â€“21) that discuss informative priors for biomolecular networks.  \nChapters 4 and 5 of Byrne (2011) are about â€œstructural Markov propertiesâ€ for \ndecomposable undirected graphical models and DAG graphical models. These are \nproperties of graph distributions that are analogous to the standard Markov properties \nand the hyper and meta Markov properties from Dawid & Lauritzen (1993). The basic \nidea is that two components of the graph are conditionally independent given a \nseparating component. Graph distributions that have the structural Markov property \nare conjugate priors in certain situations.  \n5.6 \nGraph priors based on random graph models \nThis section is about what kind of prior distribution should be used for Bayesian \nlearning of graphical model structure. Of course the graph prior should encapsulate the \nresearcherâ€™s prior beliefs about the graph structure.  \nThese beliefs for biomolecular networks were discussed in section 2.4. A good graph \nprior for biomolecular networks might give high probability to sparse graphs, \nencourage structures such as hubs and cliques, or induce an approximate power law on \nthe node degrees. The presence or absence of all of these features except cliques can be \nassessed by looking at a graphâ€™s degree sequence. (To put it mathematically, the \npresence or absence of these features can be expressed as a function of the degree \nsequence.) For example, the question of whether there are any hubs is simply about \nwhether there are any degrees that are much higher than the others. Conversely, if you \nare free to choose the degree sequence then you can choose it so as to encourage or \ndiscourage most of these features. An alternative way to enforce sparsity is to consider \nonly forests or trees.  \nIn any case it seems sensible for the degree sequence to be the main feature of graphs \nthat is used in specifying the prior distribution, or the sole feature that is used. One \npossible random graph model is the configuration model, described in section 5.3. For \nthis, you have to specify the degree of each node. Sparsity could be enforced by simply \nchoosing low degrees, or low total degree. The configuration model might be appro-\n\n \n5.6 Graph priors based on random graph models \n \n43 \npriate if you thought that a specific node was a hub and none of the others were. But it \ncannot be used to express the belief that an unspecified node is a hub, or the belief that \neach node has a small but positive probability of being a hub.  \nUsing the configuration model leads to several complications. It generates not simple \ngraphs but configurations, which may include multiple edges and self-edges. Multiple \nedges would have to be replaced with single edges, and self-edges would have to be \ndiscarded, so the eventual degree of each node might be lower than intended. Chapters \n6â€“11 are about forest and tree graphical models. In the configuration model, if the \ndegrees are suitably low then the graph is likely to be a forest, so you could generate \nforests by choosing low degrees and rejecting any graphs that were not forests. \nHowever, there does not seem to be an easy and practical way to generate trees from \nthe configuration model.  \nIn connection to the configuration model the question arises of whether a given set of \nnumbers is a possible degree-sequence. For this question see Theorem 6.4 in section \n6.2.  \nPerhaps the biggest drawback of the configuration model is that, using the terminology \nfrom section 5.1, it is a random graph model rather than a graph distribution, and \ncalculating the probability of a given graph is difficult. The probability of any given \nconfiguration is 2ğ·/2(ğ·/2)!/ğ·!, where ğ·= âˆ‘\ndeg(ğ‘£)\nğ‘£âˆˆğ‘‰\n. But the probability of a given \ngraph is more complicated, and if you are restricting to forests or trees then it is more \ncomplicated again.  \nThe expected-degree model, also described in section 5.3, has the advantages that it is a \nfactored distribution, the probability of a given graph is easy to calculate (since for each \nedge there is a simple formula for the probability that it is present), and it is more \nflexible than the configuration model in the sense that the degree of each node is not \nspecified exactly. But it is still no use for the scenario where you believe that an \nunspecified node is a hub.  \nThe configuration model or expected-degree model could be adapted by using a \nhierarchical distribution. For example, \nâ„™(ğ‘£ is a hub) = ğœƒ \ndeg(ğ‘£) | (ğ‘£ is a hub)~ ğ‘ƒğ‘œğ‘–ğ‘ ğ‘ ğ‘œğ‘›(ğœ†1) \ndeg(ğ‘£) | (ğ‘£ is not a hub) ~ ğ‘ƒğ‘œğ‘–ğ‘ ğ‘ ğ‘œğ‘›(ğœ†2). \nObviously ğœ†1 > ğœ†2. To generate from this model, you would choose whether each node \nis a hub according to the Bernoulli distribution with parameter ğœƒ, then choose the \ndegree of each node according to the appropriate Poisson distribution, and finally \ngenerate the graph according to the configuration model or the expected-degree model. \nThis still suffers from the drawback that it is complicated and inelegant to calculate the \nprobability of a given graph, since you have to sum over all the 2ğ‘› possibilities of which \nnodes are hubs and then the range of the Poisson distribution, which is the non-\nnegative integers. You could decide as a priori knowledge that there is only one hub, in \nwhich case the first sum would only have ğ‘› terms; or when exploring the graph \nposterior distribution (see section 10.1) you could have a separate â€œmoveâ€ that consists \n\n5 Random graph distributions \n \n 44 \nof deciding anew which nodes were hubs, in other words resampling from the \nğµğ‘’ğ‘Ÿğ‘›ğ‘œğ‘¢ğ‘™ğ‘™ğ‘–(ğœƒ) distribution for each node. But there are still the problems of multiple \nedges and self-edges, and the complications of restricting to forests or trees if that is \nrequired.  \nInstead of having a separate Bernoulli distribution for each node, you could have \nâ„™(ğ‘£ğ‘–1, â€¦ , ğ‘£ğ‘–ğ‘š are hubs, and the other nodes are not)  \n                              = (ğ‘›\nğ‘š)\nâˆ’1\n for all {ğ‘–1, â€¦ , ğ‘–ğ‘š} âŠ†{1, â€¦ , ğ‘›}. \nHere ğ‘š nodes are chosen to be hubs, and these are chosen uniformly at random from all \nthe nodes. With this distribution it would be simpler to calculate the probability of a \ngiven graph, since there are only (ğ‘›\nğ‘š) possibilities to sum over.  \nAnother alternative would be to use a Pareto distribution to choose the degrees. Pareto \ndistributions are long-tailed. Most values are small but there is some chance of getting a \nlarge value.  \n5.7 \nPractical graph prior distributions \nIn this section I propose seven criteria for a graph prior. I explain how the priors in \nsection 5.5 fail to satisfy these criteria and propose one possible prior that does fulfil \nthem. In section 11.6 I will use this prior in experiments to see whether it gives better \nresults than the uniform graph prior.  \nThe seven criteria, listed below, are intended to reflect the beliefs that the graph is \nsparse and some nodes are likely to be hubs, but it is not known which ones. They also \ninclude certain criteria that are useful in practice.  \n1. \nThere has to be an explicit formula for the probability of any given graph. This \nprobability can be unnormalized, because Bayesian structure-learning usually \nproduces unnormalized posterior probabilities anyway.  \n2. \nIt is desirable that there be a computationally efficient method for generating from \nthe prior distribution, as with random graph models. This would enable you to \ncheck whether the prior produces graphs that look right and accord with your \nbeliefs. However, this is not as important as criterion number 1.  \n3. \nThe formula should give higher probabilities to sparse graphs. The precise \nmeaning of this criterion is deliberately not specified, and it is not needed if you \nare restricting attention to trees or forests.  \n4. \nThe formula should give higher probabilities to graphs with hubs, and higher \nprobabilities to graphs with hubs that have larger degrees. The precise meaning of \nthis criterion is deliberately not specified.  \n5. \nThe formula should be a function of the unordered degree sequence, or \nequivalently the sorted degree sequence. This means the prior is symmetric in the \nnodes and the degrees are exchangeable. It corresponds to not knowing which \nnodes are hubs. The reason for this criterion is that, as mentioned in section 5.6, \nthe beliefs that the graph is sparse and some unspecified nodes are likely to be \nhubs can be expressed as beliefs about the unordered degree sequence.  \n\n \n5.7 Practical graph prior distributions \n \n45 \n6. \nIf only trees are under consideration, then it is desirable but not essential for the \nprior to be a factored distribution, because there are various fast methods for \nanalyzing factored distributions on trees.  \n7. \nAll other things being equal, the formula should be simple, because this will make \nit easier to understand and work with. \nNone of the graph prior distributions in section 5.5 satisfy these criteria fully. The size-\nbased prior of Armstrong et al (2009) and the binomial prior of Jones et al (2005) and \nothers do not fulfil criterion 4, about hubsâ€”they would give the same probability to a \ngraph with 100 edges in which all the degrees are below 5 as to a graph with 100 edges \nin which one node had degree 50. Bornn & Caron (2011)â€™s general class of priors might \ninclude ones that fulfil criterion 4, but it is difficult to see how, and such priors would \nnot fulfil criterion 7. The class of priors proposed by Mukherjee & Speed (2008) \ncertainly does contain priors that fulfil criteria 1â€“5, but actually creating one of these \nwould be tantamount to inventing a prior from scratch, because their class of priors is \nso broad.  \nI will describe one possible graph prior distribution, which I will call the â€œhub-\nencouraging priorâ€. This has two parameters, ğœ’âˆˆâ„¤+ and ğœ“âˆˆâ„+. Given a graph, \nsubtract ğœ’ from all the degrees, and retain only the positive ones. The probability of the \ngraph is proportional to the sum of these values plus ğœ“. In symbols,  \nâ„™(ğº) âˆ ğœ“+ âˆ‘max{0, deg(ğ‘£) âˆ’ğœ’}\nğ‘£âˆˆğ‘‰\n  \n           =  ğœ“+\nâˆ‘\n(deg(ğ‘£) âˆ’ğœ’)\nğ‘£: deg(ğ‘£)>ğœ’\n . \nThe idea is that a node is regarded as a hub if and only if its degree is greater than ğœ’. All \ngraphs that have no hubs are equally likely, and any graph that has a hub is more likely \nthan any graph that does not. A hub contributes more if its degree is higher. How much \nhubs affect a graphâ€™s probability also depends on ğœ“â€”the larger ğœ“, the smaller the effect. \nObviously ğœ’ should be a reasonably large positive integer, for example 10, and ğœ“ should \nbe positive so that even graphs with no hubs still have positive probability. The range of \nunnormalized probabilities is [ğœ“, ğœ“+ ğ‘âˆ’1 âˆ’ğœ’], assuming that the graphs under \nconsideration include at least one that has no hubs and at least one in which one node \nhas the maximum possible degree.  \nThis distribution is not easy to generate from, but its simplicity means that it is easy to \ninterpret and understand. For example, if ğœ“= 10 then all graphs in which no node has \ndegree greater than 10 have the same probability. If one wanted to generate graphs \nfrom this distribution and look at them, MCMC could be used to generate from it \napproximately, or a rejection or importance-sampling method could be used to sample \nfrom it exactly, though this might be cumbersome or slow.  \nAs described in chapters 7 and 8, there are numerous useful methods that can be used \nto analyze factored distributions on trees. Unfortunately, priors that fulfil criteria 4â€“5 \ncannot be expressed as factored distributions. Consider all the trees that contain the \nedges (ğ‘£2, ğ‘£3), (ğ‘£2, ğ‘£4), â€¦ , (ğ‘£2, ğ‘£ğ‘›). Only one edge is unspecified, and this edge must \ninclude ğ‘£1. According to criterion 4, the tree with (ğ‘£1, ğ‘£2) ought to have higher \n\n5 Random graph distributions \n \n 46 \nprobability than the tree with (ğ‘£1, ğ‘£3), because ğ‘£2 is already a hub. So in a factored \ndistribution, ğ‘¤(ğ‘£1,ğ‘£2), the factor for (ğ‘£1, ğ‘£2), would have to be higher than ğ‘¤(ğ‘£1,ğ‘£3), the \nfactor for (ğ‘£1, ğ‘£3). But a similar argument about a different set of trees shows that \nğ‘¤(ğ‘£1,ğ‘£3) has to be higher than ğ‘¤(ğ‘£1,ğ‘£2), which is impossible. On the other hand, the belief \nthat a particular set of nodes are hubs can be expressed as a factored distributionâ€”the \nweights on the edges from those nodes should simply be high.  \n \n\n \n47 \n6 \nForest and tree graphs and \ngraphical models \n6.1 \nWhy consider forest and tree graphical models? \nPreamble \nForests are graphs that have no cycles, and trees are connected forests. This definition \nof trees makes it sound as though they are bigger than forests. In graphical models this \nis appropriate, since the set of nodes is usually fixed, and so trees have more edges than \nforests (except for forests that are themselves trees). The alternative and equivalent \ndefinition of forests is that they are graphs whose connected components are all trees. \nIn the literature of machine learning, where a lot of research on graphical models \nappears, forests are sometimes referred to as trees (MeilÄƒ & Jaakkola 2006, Bradley & \nGuestrin 2010, Bach & Jordan 2003).  \nIn Bayesian learning of GGM structure, it is common to restrict attention to decom-\nposable graphs (in other words, to set the prior probability of all non-decomposable \ngraphs to zero), because the marginal likelihoods of these graphs can be calculated \nexactly using the explicit formula from section 3.1. It is also possible to restrict \nattention even further, to forests or trees, by setting the prior probability of all other \ngraphs to zero. All forests and trees are decomposable.  \nForests and trees are very restricted classes of graphs, and no doubt these graphs are \ntoo simple to be realistic models of biological or other networks, as mentioned in \nEdwards et al (2010). But there are several reasons why it might be sensible and \ndesirable to consider only forests or trees. These reasons are the subject of this chapter.  \nComputational tractability \nOne of the main reasons for restricting attention to forests or trees is that they are \nmuch more computationally tractable than general or even decomposable graphs. This \nis essentially because the joint density factorizes in terms of marginal densities on \nnodes and pairs of nodes. Viewing forests or trees as decomposable graphs, the cliques \nare the edges and the separators are the individual nodes. (In unconnected forests, the \nseparators also include the empty set, but this can be ignored because it contributes a \nfactor of 1 to the density and other quantities.) The multiplicity of each non-empty \nseparator is the degree of that node minus one. So the factorization of the joint density \nusing cliques and separators is \n\n6 Forest and tree graphs and graphical models \n \n 48 \nğ‘(ğ‘¥) =\nâˆ\nğ‘(ğ‘¥ğ‘¢, ğ‘¥ğ‘£)\n(ğ‘¢,ğ‘£)âˆˆğ¸\nâˆ\nğ‘(ğ‘¥ğ‘£)\nğ‘£âˆˆğ‘‰\ndeg(ğ‘£)âˆ’1 = âˆğ‘(ğ‘¥ğ‘£)\nğ‘£âˆˆğ‘‰\nâˆ\nğ‘(ğ‘¥ğ‘¢, ğ‘¥ğ‘£)\nğ‘(ğ‘¥ğ‘¢)ğ‘(ğ‘¥ğ‘£)\n(ğ‘¢,ğ‘£)âˆˆğ¸\n , \nand the likelihood given ğ‘š independent and identically distributed observations is \nâˆ\nğ‘(ğ‘¥ğ‘–)\nğ‘š\nğ‘–=1\n.  \nFor graphical-model structure-learning, the algorithm of Chow & Liu (1968), described \nin chapter 7, gives the maximum-likelihood tree in time that is polynomial in the \nnumber of nodes. Let the number of nodes be ğ‘›. The time taken is ğ‘‚(ğ‘›2 log ğ‘›) according \nto Acid et al (1991), Eaton & Murphy (2007), and Meyer et al (2007). But these papers \neither cite no sources for this claim or cite sources that do not make the claim. The only \npublication I have found that actually calculates the asymptotic time is MeilÄƒ (1999), \nwhich proves that it is ğ‘‚(ğ‘›2(ğ‘š+ log ğ‘›)), where ğ‘š is the number of observations. The \nfirst term is for calculating the edge-weights, and the second term is for doing Kruskal's \nalgorithm. Goldberger & Leshem (2009) state that the time taken is ğ‘‚(ğ‘›2) if Primâ€™s \nalgorithm is used.  \nFor general graphs, structure-learning is believed to be computationally intractable. For \nexample, Anandkumar et al (2012) and Tan et al (2010b) both assert that structure-\nlearning of general graphs is NP-hard. However, it is not clear exactly what they mean, \nsince the maximum-likelihood graph is always the complete graph. Both papers cite \nKarger & Srebro (2001), which shows only that finding the maximum-likelihood \ndecomposable graph with bounded clique-size is NP-hard. Anandkumar et al (2012) \nalso cite Bogdanov et al (2008), which is about the case where the node-degrees are \nbounded.  \nThe other major computational task with graphical models is inferenceâ€”finding the \nmarginal distributions on one set of nodes, given data on another set. This is also fast \non trees. For general graphs, inference is done using the junction-tree algorithm \n(Lauritzen & Spiegelhalter 1988), whose running time is exponential in the size of the \nlargest clique. But for trees and forests, the junction-tree algorithm simplifies to the \nsum-product algorithm, also called belief propagation, which takes time proportional to \nthe number of edges (Pearl 1988, sections 4.2â€“4.3).  \nSparsity \nAnother key justification for restricting attention to forests or trees is that biomolecular \nnetworks are sparse and â€œsparse graphs are locally tree-likeâ€. A detailed discussion and \ninvestigation of this notion appears in section 6.2. This is the second type of sparsity \nthat arises in graphical model structure-learning (the first type of sparsity is the \nnumber of variables or nodes being much greater than the number of observations).  \nInformal justifications \nFor structure-learning of biomolecular networks, Edwards et al (2010) express the \nbelief that forests can give some idea of the structure and be useful in several ways. \nFirstly, they suggest that if you select a forest then this can be used as the initial model \nin a search algorithm through a wider space of graphs, for example decomposable \ngraphs. For algorithms that examine decomposable graphs, it is not usually possible to \nstart from graphs produced by procedures such as the graphical lasso of Friedman et al \n\n \n6.2 The claim that sparse graphs are locally tree-like \n \n49 \n(2007), since these are not guaranteed to be decomposable. A suitable forest can be \nfound quickly using the Chowâ€“Liu algorithm, described in section 7.1.  \nEdwards et al (2010) also suggest regarding some of the properties of the selected \nforest as properties of the true graph. For example, you could assume that the true \ngraph has the same connected components as the forestâ€”in graphical models, separate \nconnected components are marginally independent. If there is more than one \ncomponent, this will reduce the dimension of the problem, which is a great advantage \nfor computational efficiency in multivariate statistics. Edwards et al (2010) also claim \nthat analysis of forests could be used to identify hubs, which are one of the most \nimportant features of biomolecular networks.  \nTree and forest graphical models in use \nTree and forest graphical models have been used for a wide variety of applications. \nKundaje et al (2002) uses trees because they can be learnt quickly and are appro-\npriately sparse for time-series gene regulation networks. Costa et al (2008) use tree \nGGMs to model gene expression levels at different stages of cell differentiation. Each \ntree corresponds to a group of genes, each node corresponds to a known stage of cell \ndifferentiation, and the edges are directed forwards in time.  \nIhler et al (2007) gives several examples of how inference on tree graphical models can \nbe used in climate science. Willsky (2002, from page 1399) reviews how tree graphical \nmodels have been used in a very wide variety of fields including oceanography, analysis \nof network traffic, and numerous aspects of image analysis.  \nTree networks appear naturally in biology as phylogenetic trees, which show how \ndifferent species have evolved from each other. Phylogenetics uses genetic information \nto infer this tree structure. Given their evolutionary predecessors, organisms are \ngenetically independent of their predecessorsâ€™ predecessors, so these trees can be \nregarded as probabilistic graphical models, as mentioned for example in Friedman \n(2004). But phylogenetics is a major field of research in its own right, and the models \nused are more elaborate. See chapter 7 of Durbin et al (1998) for an overview of \nlearning phylogenetic trees from genetic data using clustering, bootstrapping, and other \ntechniques from statistics. Incidentally, the full evolutionary tree of life on earth is not a \ntree. Genetic material is not only passed from organisms to their offspring, but is some-\ntimes also transferred laterally, especially between bacteria. This phenomenon was first \nidentified in Freeman (1951).  \n6.2 \nThe claim that sparse graphs are locally tree-like \nPreamble \nThe notion that sparse graphs are locally tree-like is an important justification for \nstudying tree and forest graphical models. If the true graph is locally tree-like, then a \nforest structure might give useful information about small parts of the graph, even if it \nis unlikely to be accurate across large sets of nodes. Anandkumar et al (2011) state that \nsparse graphs are locally tree-like and cite BollobÃ¡s (1985), which is the first edition of \nthe book Random Graphs, about ErdÅ‘sâ€“RÃ©nyi graphs.  \n\n6 Forest and tree graphs and graphical models \n \n 50 \nSpeaking very informally, it seems believable that sparse graphs are locally tree-like, \nsince if there are not many edges then there is not much chance of them being close to \neach other and forming short cycles. One example of an informal statement on this \nquestion appears in Macris (2006), which is a physics paper. The author claims that in \nsparse graphs with ğ‘› nodes the typical size of loops is ğ‘‚(ğ‘›); rephrasing in the caption \nof a figure, he says the loops are of size ğ‘‚(ğ‘›) with high probability. There is no citation \nor comment on how this is known, and there is no formal definition of â€œsparseâ€ or \nâ€œtypicalâ€.  \nInterpretations of â€œsparseâ€ \nAs discussed in section 2.4, it is often claimed that biomolecular networks are sparse, \nmeaning that they have few edges, but a precise definition of â€œsparseâ€ in this context is \nelusive. It may be useful to distinguish â€œdegree-sparsityâ€, where the degrees of the \nnodes are small in some sense, from â€œedge-sparsityâ€, which means only that the total \nnumber of edges is small. But obviously the degrees and the number of edges are \nclosely related. Some definitions refer to the average degree, which is 2|ğ¸|/ğ‘›. Clearly \nsuch definitions could be expressed in terms of the total number of edges.  \nIt is possible to imagine three types of precise definition of sparse graphs. Firstly, \nâ€œsparseâ€ could be defined for a given graph, so that if someone gives you a graph you \ncan examine it and say whether it is sparse or not. For example, you could say that a \nsparse graph is one with at most 2ğ‘› edges (ğ‘› being the number of nodes). With \ngraphical models it is natural to want to say whether a specific graph is sparse, since \none deals with specific graphs that have specific numbers of nodes and edges.  \nSecondly, â€œsparseâ€ could be defined for a random graph model with a fixed number of \nnodes. For example, ğº(ğ‘›, ğ‘) is sparse if ğ‘â‰¤0.2. (For the definition of ğº(ğ‘›, ğ‘) see section \n5.2.)  \nThirdly, sparsity could be defined for a random graph model where ğ‘›â†’âˆ, explicitly or \nimplicitly. This would be an asymptotic definition that only makes assertions about all \nğ‘›> ğ‘, for some unspecified ğ‘, in terms of probabilities. Several definitions of this type \nappear in the literature. In ErdÅ‘sâ€“RÃ©nyi random graph theory and extremal graph \ntheory, â€œsparseâ€ usually means the number of edges is ğ‘‚(ğ‘›) (Diestel 2005, page 163; \nBollobÃ¡s 2001, pages 221 and 303). BollobÃ¡s & Riordan (2011) is about sparse graphs \nthat have Î˜(ğ‘›) edges; it describes these graphs as â€œextremelyâ€ sparse and says they are \nthe sparsest graphs that are interesting to study. Sudakov & VerstraÃ«te (2008) say that \ndense graphs have average degree linear in ğ‘›, which means they have Î˜(ğ‘›2) edges and \npresumably implies that sparse graphs have ğ‘‚(ğ‘›) edges. A different asymptotic \ndefinition is that sparse graphs have average degree close to 2 (BollobÃ¡s & SzemerÃ©di \n2002).  \nThese three types of definition are separate in that none of them imply any of the others. \nThe first is about fixed graphs, the second is about fixed ğ‘›, and the third is about ğ‘›â†’âˆ.  \nFor pure mathematicians, the first two types of definition are rather arbitrary. It is more \nnatural to think of adjectives such as â€œsparseâ€ in terms of asymptotic expressions such \nas ğ‘‚(ğ‘›), and so the third type of definition is the most common. The theory of ErdÅ‘sâ€“\nRÃ©nyi random graphs mostly uses this kind of definition, and the same is true of the \n\n \n6.2 The claim that sparse graphs are locally tree-like \n \n51 \nrigorous results that have been proved about BarabÃ¡si & Albert (1999)â€™s scale-free \ngraphsâ€”see chapter 4 of Durrett (2007). Janson et al (2000, page 2) say that the whole \nof random graph theory is asymptotic in nature. Of course the problem with asymptotic \ndefinitions is that graphs in the real world are finite, and these definitions say nothing \nabout specific finite graphs.  \nHere are three other definitions of graph sparsity from the literature. The first two use \ndegree-sparsity. Firstly, Dobra et al (2004) say a sparse graph is one where each node \nhas only a small number of neighbours compared to the total number of nodes. This is \nconvenient for their purposes but excludes the possibility of hubs, even though the \npaper is about GGMs for gene expression networks. Secondly, Meinshausen & BÃ¼hl-\nmann (2006) use the condition that max\nğ‘£âˆˆğ‘‰deg(ğ‘£) = ğ‘‚(ğ‘›ğœ…) for some ğœ…âˆˆ[0,1), for the \npurpose of proving asymptotic results. This condition means that |ğ¸| =\n1\n2 âˆ‘deg(ğ‘£)\nğ‘£\n can \nbe as large as \nğ‘›\n2 max deg(ğ‘£) = ğ‘‚(ğ‘›1+ğœ…), so it is similar to the asymptotic definitions \nabove but less sparse than |ğ¸| = ğ‘‚(ğ‘›). If ğœ… is small, it would imply the condition of \nDobra et al (2004). Thirdly, Wille & BÃ¼hlmann (2006) state simply that â€œif the number \nof â€¦ edges is much smaller than ğ‘(ğ‘âˆ’1)/2 [they use ğ‘ instead of ğ‘›, so this is the \nmaximum possible number of edges], a graph is generally referred to as being sparse.â€  \nInterpretations of â€œlocally tree-likeâ€ \nThe phrase â€œlocally tree-likeâ€ appears in the literatures of physics, computer science, \nand pure mathematics, as well as statistics, with a wide variety of interpretations. Many \ndefinitions of it are brief, informal, and only given in passing.  \nPerhaps the simplest interpretation of â€œlocally tree-likeâ€ is that it means the graph has \nfew short cycles (of course this is still vague and not a formal mathematical idea). This \nis the interpretation that appears in Anandkumar et al (2011), Forney (2003), Brum-\nmitt et al (2012), and Sly (2010). With this interpretation, the claim that sparse graphs \nare locally tree-like can be justified to some extent using the theorems about ErdÅ‘sâ€“\nRÃ©nyi random graphs that are described in the next section. Using these theorems will \nmean interpreting the vague words â€œfewâ€ and â€œlocallyâ€ to have asymptotic meanings, as \nwith the third type of definition of â€œsparseâ€ in the previous subsection.  \nOne alternative to investigating the property of having few short cycles is to investigate \nthe property of having none. This would mean investigating the girth of random graphs. \nNeither BollobÃ¡s (2001) nor Janson et al (2000), whose preface describes it as an \nupdate of BollobÃ¡s (1985), give any results about girth. But some facts can be deduced \nfrom the results on small cycles. For example, â„™(girth â‰¥5) = â„™(ğ‘‹3 = 0 âˆ©ğ‘‹4 = 0), \nwhere ğ‘‹ğ‘– is the number of cycles of length ğ‘–, and the theorems in the next subsection \ngive information about â„™(ğ‘‹ğ‘–= 0).  \nAs with â€œsparseâ€, it would be possible to make a precise definition of â€œlocally tree-likeâ€ \nfor given graphs or given ğ‘›. For example, the girth of the graph has to be at least 5, or \nğ‘›/4.  \nMost of the other interpretations of â€œlocally tree-likeâ€ refer to the girth. For example, \nMiller (2008) uses a sequence of graphs ğº1, ğº2, â€¦ and takes locally tree-like to mean \nthat ğºğ‘š has girth greater than 2ğ‘š, Coja-Oghlan et al (2009) take it to mean there are \n\n6 Forest and tree graphs and graphical models \n \n 52 \neither no short cycles or the girth is Î©(log ğ‘›) (the relevant sentences are conjectures or \ninformal observations), Chandar (2010) takes it to mean the girth is Î©(log ğ‘›), and \nVontobel (2003) simply says that it means there are â€œno short cyclesâ€. Durrett (2007, \npage 134) says that being locally tree-like is the same thing as having no trianglesâ€”\nwhich has the practical advantage that it is completely precise and applies to actual \nspecific graphs. Karrer & Newman (2010) state that it means â€œall small connected \nsubsets of vertices within the network are trees.â€ Cooper & Frieze (2010) give a formal \ndefinition for whether a node, rather than a graph, is locally tree-like, based on whether \nany cycles appear when you explore up to a certain distance away from the node.  \nSmall cycles in ErdÅ‘sâ€“RÃ©nyi random graphs \nThe most relevant rigorous results are the set of theorems about the numbers of small \ncycles in an ErdÅ‘sâ€“RÃ©nyi random graph. For definitions of ErdÅ‘sâ€“RÃ©nyi random graphs \nand notations, see section 5.2. The present question of the number of small cycles is one \nof the many questions for which results about the two ErdÅ‘sâ€“RÃ©nyi models are very \nsimilar.  \nFollowing BollobÃ¡s (2001), the canonical text in this field, I will use the notation \nğ‘(ğ‘›)~ğ‘(ğ‘›) to mean that lim\nğ‘›â†’âˆ\nğ‘(ğ‘›)\nğ‘(ğ‘›) = 1 (as mentioned in Appendix III). This subsection \ndoes not present new results but rather presents and uses theorems and corollaries \nthat have been proved elsewhere.  \nTheorem 6.1 (Theorem 3a in ErdÅ‘s & RÃ©nyi 1960). In ğº(ğ‘›, ğ‘€), suppose that ğ‘€(ğ‘›) ~ ğ‘ğ‘›, \nwhere ğ‘> 0. Let ğ‘‹ğ‘– be the number of cycles of length ğ‘– in ğº. Then \nâ„™(ğ‘‹ğ‘–= ğ‘—) ~ ğœ†ğ‘—ğ‘’âˆ’ğœ†\nğ‘—!\n , \nwhere ğœ†= (2ğ‘)ğ‘–/2ğ‘–. In other words, ğ‘‹ğ‘– converges in probability to Poisson(ğœ†) as ğ‘›â†’âˆ.  \nTheorem 6.2. With the same assumptions as in Theorem 6.1,  \nâ„™(ğ‘‹ğ‘–= ğ‘—ğ‘– for ğ‘–= 3,4, â€¦ , ğ‘¡) ~ âˆğœ†ğ‘–\nğ‘—ğ‘–ğ‘’âˆ’ğœ†ğ‘–\nğ‘—ğ‘–!\nğ‘¡\nğ‘–=3\n \nwhere ğœ†ğ‘–= (2ğ‘)ğ‘–/2ğ‘–. In other words, the joint distribution of the ğ‘‹ğ‘–â€™s converges in \nprobability to the joint distribution of independent Poisson random variables with the \ngiven means.  \nProof. This follows from Theorem 4 of BollobÃ¡s (1981), using the fact that the auto-\nmorphism group of the cycle of length ğ‘– has size 2ğ‘–.  \nTheorem 6.1 is about a single cycle-length. Theorem 6.2 is just Theorem 6.1 plus the \nadditional result that the numbers of different-sized cycles are asymptotically inde-\npendent.  \nTheorem 6.3 (Corollary 4.9 in BollobÃ¡s 2001, Corollary 9 in BollobÃ¡s 1985). In ğº(ğ‘›, ğ‘), \nsuppose that ğ‘(ğ‘›) ~ ğ‘/ğ‘›, where ğ‘> 0. Let ğ‘‹ğ‘– be the number of cycles of length ğ‘– in ğº. \nThen \n\n \n6.2 The claim that sparse graphs are locally tree-like \n \n53 \nâ„™(ğ‘‹ğ‘–= ğ‘—ğ‘– for ğ‘–= 3,4, â€¦ , ğ‘¡) ~ âˆğœ†ğ‘–\nğ‘—ğ‘–ğ‘’âˆ’ğœ†ğ‘–\nğ‘—ğ‘–!\nğ‘¡\nğ‘–=3\n \nwhere ğœ†ğ‘–= ğ‘ğ‘–/2ğ‘–.  \nObviously Theorem 6.2, about ğº(ğ‘›, ğ‘€), is almost exactly the same as Theorem 6.3, \nabout ğº(ğ‘›, ğ‘). But in the former, the Poisson parameters have a factor of 2 that does not \nappear in the latter. This is to be expected, since in ğº(ğ‘›, ğ‘€) with ğ‘€(ğ‘›) ~ ğ‘ğ‘› the \nexpected number of edges is asymptotically ğ‘ğ‘›, whereas in ğº(ğ‘›, ğ‘) with ğ‘(ğ‘›) ~ ğ‘/ğ‘› the \nexpected number of edges is ğ‘(ğ‘›âˆ’1)/2, or equivalently ğ‘ğ‘›/2; and the two models \ncorrespond most closely when the expected numbers of edges are the same.  \nThe main results in BollobÃ¡s (1981) and section 4.1 of BollobÃ¡s (2001) are more \ngeneral than the ones given above. They state that if a possible subgraph is â€œstrictly \nbalancedâ€, then the number of copies of it that appear in a random graph is asym-\nptotically distributed as a Poisson random variable. â€œStrictly balancedâ€ is a property \npossessed by all cycles, and â€œcopiesâ€ of a subgraph means subgraphs that are iso-\nmorphic to it.  \nThe values of the Poisson parameters for ErdÅ‘sâ€“RÃ©nyi graphs \nThe above results on ErdÅ‘sâ€“RÃ©nyi random graphs use the assumption that ğ‘€(ğ‘›) ~ ğ‘ğ‘› \nor ğ‘(ğ‘›) ~ ğ‘/ğ‘›. The number of possible edges is ~ğ‘›2/2. So both the assumptions imply \nthat the graph is sparse in the sense that the number of edges is ğ‘‚(ğ‘›)â€”as discussed \nabove, this is the meaning of â€œsparseâ€ that is most often used in graph theory. For fixed ğ‘ \nand any given ğ‘–, the Poisson parameters do not depend on ğ‘›. This means that for large ğ‘› \nthey are â€œsmallâ€, in the sense that as ğ‘›â†’âˆ the proportion of cycles of length ğ‘– that \nappear in the graph will tend to zero. (This proportion is the number of cycles of length \nğ‘– that appear in the graph divided by the number of cycles of length ğ‘– in ğ¾ğ‘›.) In this \nsense the results imply that for large ğ‘› sparse graphs have few short cyclesâ€”in other \nwords, they are locally tree-like.  \nA secondary question is whether short cycles are commoner or rarer than long ones. In \nTheorem 6.2, ğœ†ğ‘–= (2ğ‘)ğ‘–/2ğ‘–. For large ğ‘– this is dominated by (2ğ‘)ğ‘–. If ğ‘> 0.5, then long \ncycles are more likely than short ones, but if ğ‘â‰¤0.5, then short cycles are more likely \nthan long ones. However, the latter case corresponds to extremely sparse graphs where \n|ğ¸| â‰¤|ğ‘‰|/2 (at least asymptotically). These have so few edges that long cycles are not \neven possible. In Theorem 6.3, ğœ†ğ‘–= ğ‘ğ‘–/2ğ‘–, so the situation is essentially the same but \nthe boundary between the two cases is ğ‘= 1.  \nThe next question is whether the Poisson parameters ğœ†ğ‘– are actually reasonably small \nfor sparse graphs with realistic ğ‘›. The obvious way to investigate this is to choose ğ‘›, \nand ğ‘ or ğ‘€, and find ğœ†ğ‘– using the equations given above. If ğœ†ğ‘– is small for, say, ğ‘–âˆˆ\n{3,4,5,6}, then it can be said that the graph is locally tree-like. Table 6.1 shows the \nvalues of the Poisson parameters for small ğ‘– with a sparse model from ğº(ğ‘›, ğ‘), a sparse \nmodel from ğº(ğ‘›, ğ‘€), and the random graph model ğº(ğ‘›, ğ‘= 0.5), where all graphs are \nequally likely. Of course the Poisson parameters are also the asymptotic mean numbers \nof cycles.  \n\n6 Forest and tree graphs and graphical models \n \n 54 \nThe first two random graph models have the same Poisson parameters. But the \nparameters for ğº(100,0.5) are much bigger. Based on this it seems reasonable to state \nthat for the sparse random graph models the Poisson parameters are small.  \n \nRandom graph  \nmodel \nPoisson parameters for numbers of cycles of lengths from 3 to 8 \n3 \n4 \n5 \n6 \n7 \n8 \nğº(ğ‘›, ğ‘) with ğ‘= 10/ğ‘› \nExample: ğº(100,0.1) \n167 \n1 250 \n10 000 \n83 333 \n714 286 \n6 250 000 \nğº(ğ‘›, ğ‘€) with ğ‘€= 5ğ‘› \nExample: ğº(100,500) \n167 \n1 250 \n10 000 \n83 333 \n714 286 \n6 250 000 \nğº(100, ğ‘= 0.5)  \n20 833 \n781 250 \n3.1 Ã— 107 \n1.3 Ã— 109 5.6 Ã— 1010 2.4 Ã— 1012 \nTable 6.1. Poisson parameters for the numbers of cycles for two sets of sparse ErdÅ‘sâ€“RÃ©nyi \nrandom graph models and for ğº(100,0.5). These are from Theorems 6.2 and 6.3. (In ğº(100,0.1), \nğº(100,500), and ğº(100,0.5), the parameter ğ‘, defined in Theorems 6.2 and 6.3, was chosen so \nthat the asymptotic conditions were satisfied exactly; so for ğº(ğ‘›, ğ‘) it was chosen to be ğ‘›ğ‘, and \nfor ğº(ğ‘›, ğ‘€) it was chosen to be ğ‘€/ğ‘›.)  \n \nUsing simple Monte Carlo to approximate the numbers of small cycles \nThere is often a tacit assumption that asymptotic results will be approximately true for \nrealistic-sized problems. But graphical models usually have fixed ğ‘›, and for any fixed ğ‘› \nit is quite possible that a given asymptotic result does not hold, even approximately.  \nThe obvious avenue of approach is to look at actual random graphs, identify all their \ncycles, and examine the numbers of cycles of each length. For very small ğ‘› it would be \npossible to generate all the possible graphs for either of the ErdÅ‘sâ€“RÃ©nyi random graph \nmodels. For ğº(ğ‘›, ğ‘), this would involve generating all 2(ğ‘›\n2) possible graphs and \nweighting them according to their probabilities. This is only feasible for ğ‘› up to about 8. \nFor ğº(ğ‘›, ğ‘€), it would only be necessary to generate the ((ğ‘›\n2)\nğ‘€) graphs that are possible \nunder this model, but even for ğ‘›= 16 and ğ‘€= 10 this is 1.2 Ã— 1014, which is very large. \nSo it seems more sensible to look at a sample of randomly generated graphs from \nğº(ğ‘›, ğ‘) or ğº(ğ‘›, ğ‘€). This is a type of simple Monte Carlo method. If the sample is \nreasonably big then the empirical distributions of the numbers of cycles should be \nsimilar to the true distributions.  \nIdentifying all the cycles in a given undirected graph can be done by considering all the \npossible cycles and then checking if they are present. Coming up with a more efficient \nmethod is far from trivial. Much research on this problem appeared in the late 1960s \nand early 1970s. Mateti & Deo (1976) give a summary of all the algorithms that were \nknown at that time, classified into four basic types. The simplest way I have found is to \nuse the algorithm in Paton (1969) to find a â€œfundamental setâ€ of cycles, which is a basis \nof the vector space of all the cycles, and then use the algorithm in Gibbs (1969) to \ngenerate the other cycles. I have not found any single document that gives a complete \naccount of how to find all the cycles in an undirected graph.  \n\n \n6.2 The claim that sparse graphs are locally tree-like \n \n55 \nI wrote a program to generate a simple Monte Carlo sample of random graphs from \nğº(ğ‘›, ğ‘) or ğº(ğ‘›, ğ‘€) and then count all their cycles. Some results are shown in Figure 6.1. \nThe most relevant cycles are the shortest ones, and the bar-charts show that there are \nindeed few of these. The asymptotic means are close to the true values for the very \nshortest cycles.  \nSmall cycles in graphs with given degree sequences \nAs discussed above, claims that sparse graphs are locally tree-like have usually been \nmade with reference to theorems about cycles in ErdÅ‘sâ€“RÃ©nyi random graphs, or with \nreference to nothing. But, as discussed in section 2.4, BarabÃ¡si & Oltvai (2004) and \nother papers have claimed that real networks tend to be â€œscale-freeâ€, meaning that over \na large range the degrees of the nodes follow a power law. This would mean that the \nErdÅ‘sâ€“RÃ©nyi random graph models are not appropriate. It might be more sensible to \nconsider theorems about random graph models that are scale-free or approximately so.  \nThe most relevant results seem to be the ones about cycles in random graphs with given \ndegree sequence. This subsection will present the results but not a detailed investi-\ngation. A degree sequence is a list of ğ‘› non-negative integers. For a graph to have a \ngiven degree sequence means that the degrees of its nodes are the same as the integers \nin this list, in some order. The random graph model is that all graphs with the given \ndegree sequence are equally likely. Random graphs with given degree sequence could \nbe made scale-free by choosing the degree sequence to follow a power law.  \nThe other main feature of biomolecular networks mentioned in section 2.4 was that \nthey contain hubs. Small cycles in graphs that contain hubs could be investigated using \nthe theorems on random graphs with given degree sequence. The degree sequence \nshould simply be chosen to contain a small number of large degrees.  \nIn either case it is necessary to specify a degree sequence. Given a degree sequence, the \ntheorems below can be used to calculate the asymptotic numbers of short cycles. But \nnot just any sequence of integers can be a degree sequence. For a start, they have to be \nin {0, â€¦ , ğ‘›âˆ’1} and their sum has to be even. But this is not sufficientâ€”for example, \n{2,2,0} is not the degree sequence of any graph. Necessary and sufficient conditions are \ngiven by Theorem 6.4.  \nTheorem 6.4 (ErdÅ‘s & Gallai 1960). Let {ğ‘‘1, â€¦ , ğ‘‘ğ‘›} be a non-increasing sequence of \nnon-negative integers. This is the degree sequence of some graph if and only if âˆ‘\nğ‘‘ğ‘–\nğ‘›\nğ‘–=1\n is \neven and âˆ‘\nğ‘‘ğ‘–\nğ‘˜\nğ‘–=1\nâ‰¤ğ‘˜(ğ‘˜âˆ’1) + âˆ‘\nmin{ğ‘˜, ğ‘‘ğ‘–}\nğ‘›\nğ‘–=ğ‘˜+1\n for 1 â‰¤ğ‘˜â‰¤ğ‘›.  \nProof. For a proof in English see Tripathi et al (2010).  \nTheorem 6.5 gives the asymptotic distributions of the numbers of cycles.  \nTheorem 6.5 (Theorem 2 in BollobÃ¡s 1980). Let {ğ‘‘1, â€¦ , ğ‘‘ğ‘›} be a given degree sequence, \nand suppose all graphs with this degree sequence are equally likely. Let ğ‘š= Â½ âˆ‘\nğ‘‘ğ‘–\nğ‘›\nğ‘–=1\n \nbe the number of edges in each of these graphs, and assume that 2ğ‘šâˆ’ğ‘›â†’âˆ as ğ‘›â†’âˆ. \nThen the numbers of cycles ğ‘‹3, â€¦ , ğ‘‹ğ‘˜ are asymptotically independent Poisson random \nvariables with means ğœ†ğ‘–= ğœ†ğ‘–/2ğ‘–, where ğœ†=\n1\nğ‘šâˆ‘\n(ğ‘‘ğ‘–\n2 )\nğ‘›\nğ‘–=1\n.  \n \n\n6 Forest and tree graphs and graphical models \n \n 56 \n(a) ğº(22,0.1) \n \n(b) ğº(30,40) \n \nFigure 6.1. Numbers of cycles in (a) ğº(22,0.1) and (b) ğº(30,40). The â€œactualâ€ means were found \nby generating simple Monte Carlo samples of 5000 graphs and then counting all the cycles. In \n(a) the asymptotic means are the Poisson parameters from Theorem 6.3, with ğ‘= ğ‘›ğ‘; in (b) \nthey are the Poisson parameters from Theorem 6.2, with ğ‘= ğ‘€/ğ‘›. The asymptotic means are \nonly shown for short cycles, since they get very big, and in (b) the largest cycle-lengths are \nomitted altogether.  \n \n\n \n6.2 The claim that sparse graphs are locally tree-like \n \n57 \nWhether these Poisson parameters can be regarded as small depends on the degree \nsequence, or more accurately the sequence of degree sequences, since the theorem is \nabout asymptotic behaviour as ğ‘›â†’âˆ. (Theorem 6.5 was also proved independently as \nCorollary 1 in Wormald 1981. Wormaldâ€™s result is slightly different, being about cycles \nof an arbitrary set of lengths.)  \nA special case of graphs with given degree sequences is regular graphs. Regular graphs \nare ones where every node has the same degree, and ğ‘‘-regular graphs are ones where \nevery node has degree ğ‘‘. All ğ‘‘-regular graphs have ğ‘›ğ‘‘/2 edges, so if ğ‘‘ is fixed they are \nsparse in the sense that the number of edges is ğ‘‚(ğ‘›). Many of the results on regular \ngraphs assume that ğ‘‘â‰¥3. This is not a restrictive assumption, since if ğ‘‘= 2 then the \ngraph is one big loop, and if ğ‘‘= 1 then it consists of nothing but connected components \nof size 2â€”these cases are both trivial.  \nRegular graphs definitely do not have hubs, so they are not likely to be good models for \nbiomolecular networks. In any case, for regular graphs Theorem 6.5 simplifies to give \nthe following result.  \nTheorem 6.6. (Wormald 1981; BollobÃ¡s 1980; BollobÃ¡s 2001, page 56). Assume graphs \nare chosen uniformly at random from the set of ğ‘‘-regular graphs. For fixed ğ‘˜â‰¥3, the \nnumbers of cycles ğ‘‹3, â€¦ , ğ‘‹ğ‘˜ are asymptotically independent Poisson random variables \nwith means ğœ†3, â€¦ ğœ†ğ‘˜, where ğœ†ğ‘–= (ğ‘‘âˆ’1)ğ‘–/2ğ‘–. \nAs with Theorems 6.2 and 6.3, about ErdÅ‘sâ€“RÃ©nyi graphs, these Poisson parameters do \nnot depend on ğ‘›, so as ğ‘›â†’âˆ the proportion of cycles of length ğ‘– that appear in the \ngraph tends to zero. In this sense the graphs have few short cycles and are locally tree-\nlike.  \nBollobÃ¡s (2001, page 84) comments as follows on the similarity between this result for \nregular graphs and the results for ErdÅ‘sâ€“RÃ©nyi random graphs. A random ğ‘‘-regular \ngraph is in many ways similar to either ğº(ğ‘›, ğ‘) with ğ‘(ğ‘›) = ğ‘‘/ğ‘› or ğº(ğ‘›, ğ‘€) with \nğ‘€(ğ‘›) = ğ‘‘ğ‘›/2. In a random ğ‘‘-regular graph the expected number of cycles of length ğ‘– is \n(ğ‘‘âˆ’1)ğ‘–/2ğ‘–, but in the two ErdÅ‘sâ€“RÃ©nyi models it is ğ‘‘ğ‘–/2ğ‘–. This means that short cycles \nare slightly less likely in regular graphs than in ErdÅ‘sâ€“RÃ©nyi random graphs.  \nSlightly stronger than Theorem 6.6 is Theorem 6.7, in which ğ‘‘ is allowed to grow slowly \nas a function of ğ‘›.  \nTheorem 6.7 (Theorem 1 in McKay et al 2004). Assume graphs are chosen uniformly at \nrandom from the set of ğ‘‘-regular graphs. Allow ğ‘‘= ğ‘‘(ğ‘›) and ğ‘”= ğ‘”(ğ‘›) to increase with \nğ‘›, so long as (ğ‘‘âˆ’1)2ğ‘”âˆ’1 = ğ‘œ(ğ‘›). Let {ğ‘1, â€¦ , ğ‘ğ‘˜} be a set of cycle-lengths that is a non-\nempty subset of {3, â€¦ , ğ‘”}. Then ğ‘‹ğ‘1, â€¦ , ğ‘‹ğ‘ğ‘˜, the numbers of cycles of these lengths, are \nasymptotically independent Poisson random variables with means ğœ†ğ‘–= (ğ‘‘âˆ’1)ğ‘ğ‘–/2ğ‘ğ‘–.  \nCorollary 6.8 (Corollary 1 in McKay et al 2004). With the same assumptions as in \nTheorem 6.7, the probability that the girth is greater than ğ‘” is  \nexp (âˆ’âˆ‘(ğ‘‘âˆ’1)ğ‘Ÿ/2ğ‘Ÿ\nğ‘”\nğ‘Ÿ=3\n+ ğ‘œ(1)) . \n\n6 Forest and tree graphs and graphical models \n \n 58 \nIf the restriction to the class of regular graphs is lifted, and instead ğ‘‘ is the maximum \ndegree of the graph, then it might be possible to use one of the above results on \nasymptotic Poisson distributions as a sort of â€œupper boundâ€ and thus show that a \nfurther class of sparse graphs is locally tree-like. Removing edges from a regular graph \ncan only decrease the number of cycles of any given length.  \nSummary \nThe purpose of this section was to consider the notion that sparse graphs are locally \ntree-like. Probably the most natural way to interpret â€œsparseâ€ is asymptotically (with \nğ‘›â†’âˆ), and probably the most natural way to interpret â€œlocally tree-likeâ€ is that there \nare few short cycles, where â€œfewâ€ and â€œshortâ€ are also to be interpreted asymptotically. \nWhen these interpretations are used, theorems about the two ErdÅ‘sâ€“RÃ©nyi random \ngraph models can be used to justify the notion that sparse graphs are locally tree-like.  \nThe first drawback of this is that asymptotic theorems say nothing about real graphs \nwith fixed ğ‘›. The only remedies are to generate actual graphs and count the cycles, as I \ndid above, or to come up with new theorems about the numbers of short cycles in \ngraphs with specific ğ‘›.  \nThe second drawback is that, as mentioned in section 2.4 and 5.3, real networks are not \nwell modelled by ErdÅ‘sâ€“RÃ©nyi graphs. For scale-free graphs or other complex networks \nit may be possible to use the theorems about cycles in graphs with given degree \nsequence, though some of these random graphs are not defined as precisely as ErdÅ‘sâ€“\nRÃ©nyi graphs.  \nSupplementary notes: extremal graph theory \nThe field of extremal graph theory (BollobÃ¡s 1978, Diestel 2005) addresses problems \nthat are somewhat related to the question of being locally tree-like. Extremal graph \ntheory is about â€œall graphsâ€ or â€œno graphsâ€; it is not concerned with â€œmost graphsâ€ or \nrandom graphs. The archetypal question is to find the number of edges, as a function of \nthe number of nodes, such that all graphs with that many edges contain a certain \nsubgraph. The biggest graphs that do not contain the subgraph are called the extremal \ngraphs. For example, TurÃ¡n (1941) found the graph that has the maximum number of \nedges among graphs with ğ‘› nodes that do not contain any copies of ğ¾ğ‘Ÿ. This graph, \nknown as the TurÃ¡n graph, is the extremal graph for this problem.  \nExtremal graph theory is not just about subgraphs but also about other properties. The \nmost relevant property to the question of being locally tree-like is girth. As mentioned \nabove, â€œlocally tree-likeâ€ could be taken to mean that a graph has large girth. BollobÃ¡s & \nSzemerÃ©di (2002) show that the girth of a graph with ğ‘› nodes and ğ‘›+ ğ‘˜ edges is at \nmost 2(ğ‘›+ ğ‘˜)(logğ‘˜+ log log ğ‘˜+ 4)/3ğ‘˜, for ğ‘›â‰¥4 and ğ‘˜â‰¥2. Section III.1 of BollobÃ¡s \n(1978) is about graphs with large minimal degree and large girth.  \nSudakov & VerstraÃ«te (2008) prove various results about the set of lengths of cycles in a \ngraph. For example, given the average degree and the girth of the graph, they give an \nasymptotic lower bound for the size of this set and show that it contains a certain \nnumber of consecutive even integers.  \n \n\n \n59 \n7 \nThe Chowâ€“Liu algorithm  \n7.1 \nFinding the optimal tree \nSuppose you are given a joint distribution on a set of discrete-valued random variables, \nand each variable corresponds to a node. From the distributions on these variables that \nare Markov with respect to a tree on these nodes, you have to find one that is closest to \nthe given distribution in terms of Kullbackâ€“Leibler distance. An elegant algorithm for \nsolving this problem was given in Chow & Liu (1968), a much-cited paper that \nappeared before probabilistic graphical models were widely studied.  \nThe paper also addressed a dual problem. Suppose the true distribution on the \ndiscrete-valued random variables is unknown but observations from it are available. \nFrom the distributions that are Markov with respect to a tree, find one that has \nmaximum likelihood. The algorithm for doing this is essentially the same as the one for \nthe first problem, except that it uses empirical distributions rather than true ones.  \nThe algorithm for the second problem is described below. For both problems it is \npossible for there to be more than one optimal tree, but I will sometimes refer to â€œtheâ€ \noptimal tree as this is easier to read.  \nChow & Liu describe their algorithms using directed graphical models in the shape of \nrooted trees. Each of their arrows points from a dependent variable to the variable it \ndepends on, and towards the rootâ€”the opposite direction to what is now standard. Any \nrooted-tree DAG is Markov-equivalent to an undirected tree, so the algorithms and \nresults can also be stated using undirected graphs.  \nGiven observations from a discrete-valued multivariate distribution, the obvious way of \nfinding the optimal tree and distribution would be to first find the optimal distribution \nfor each possible tree, and then look through all the trees and find the optimal one. It \nturns out that these two steps can be done in one. Let the nodes be {1, â€¦ , ğ‘}, and \nwithout loss of generality let node 1 be the root of the tree. Chow & Liu write the \ndensity as  \nğ‘(ğ‘¥) = ğ‘(ğ‘¥1) âˆğ‘( ğ‘¥ğ‘–âˆ£âˆ£ğ‘¥ğ‘ğ‘(ğ‘–) )\nğ‘\nğ‘–=2\n, \nwhere ğ‘ğ‘(ğ‘–) is the parent of node ğ‘– (they would draw the arrow from ğ‘– to ğ‘ğ‘(ğ‘–)). If \nthere are ğ‘› independent and identically distributed data, the likelihood is âˆ\nğ‘(ğ‘¥ğ‘˜)\nğ‘›\nğ‘˜=1\n. \nChow & Liu (1968) show that maximizing the log-likelihood over all tree distributions \nis equivalent to maximizing  \n\n7 The Chowâ€“Liu algorithm \n \n 60 \nâˆ‘\nğ¼ğ‘¢,ğ‘£\n(ğ‘¢,ğ‘£)âˆˆğ¸\n , \nwhere (ğ‘¢, ğ‘£) is an unordered pair and ğ¼ğ‘¢,ğ‘£ is the empirical mutual information, also \ncalled the sample mutual information (Chow & Liu 1968) or the empirical cross-\nentropy (Lauritzen 2006): \nğ¼ğ‘¢,ğ‘£= âˆ‘ğ‘›(ğ‘¥ğ‘¢, ğ‘¥ğ‘£)\nğ‘›\n log\nğ‘›(ğ‘¥ğ‘¢, ğ‘¥ğ‘£)/ğ‘›\nğ‘›(ğ‘¥ğ‘¢)ğ‘›(ğ‘¥ğ‘£)/ğ‘›2 \nğ‘¥ğ‘¢, ğ‘¥ğ‘£\n. \nHere for example ğ‘›(ğ‘¥ğ‘¢, ğ‘¥ğ‘£) is the number of observations of ğ‘‹ğ‘¢= ğ‘¥ğ‘¢ and ğ‘‹ğ‘£= ğ‘¥ğ‘£, and if \nany of the ğ‘›(â‹…)â€™s is zero then the summand is taken to be zero. Recall that the variables \nare all discrete. ğ¼ğ‘¢,ğ‘£ is always non-negative.  \nAs an aside, note that the elements in the expression for ğ¼ğ‘¢,ğ‘£ are maximum-likelihood \nestimators. For example, ğ‘›(ğ‘¥ğ‘¢, ğ‘¥ğ‘£)/ğ‘› is the MLE of â„™(ğ‘‹ğ‘¢= ğ‘¥ğ‘¢, ğ‘‹ğ‘£= ğ‘¥ğ‘£). This is just the \nobservation that in discrete decomposable graphical models the MLE of each \nprobability on a clique is simply the observed proportion of the data that take that set \nof values.  \nThe first part of the Chowâ€“Liu algorithm is to calculate ğ¼ğ‘¢,ğ‘£ for all possible edges (ğ‘¢, ğ‘£). \nIn the second part of the algorithm, the ğ¼ğ‘¢,ğ‘£â€™s are regarded as weights on the possible \nedges. The task that remains is to find the tree with maximum total weight. This is a \nmaximum-weight spanning tree (MWST) problem. In this case what is sought is a \nmaximum-weight tree that spans the complete graph ğ¾ğ‘.  \nThere are several simple algorithms that can solve MWST problems in polynomial time \nand are thus efficient in high dimensions. Chow & Liu (1968) and most papers based on \nit use Kruskalâ€™s algorithm (Kruskal 1956), which is described in the next section. For \nanother discussion of the Chowâ€“Liu algorithm see Pearl (1988, section 8.2.1). The \nasymptotic time that the algorithm takes is discussed in section 6.1.  \n7.2 \nKruskalâ€™s algorithm \nThe algorithm as used in the Chowâ€“Liu algorithm \nGiven a complete graph on a set of nodes, and real-valued weights on each edge, the \nfollowing algorithm produces a maximum-weight spanning treeâ€”in other words, a \nmaximum-weight tree on the same set of nodes.  \n \nAlgorithm IV: Kruskalâ€™s algorithm \n1. Start with the empty graph on the given set of ğ‘ nodes.  \n2. From among the unused edges whose addition would not lead to the appearance of a \ncycle, add the one with largest weight. (If there are two or more such edges, add any \none of them.)  \n3. Repeat step 2 until you have ğ‘âˆ’1 edges.  \n \n\n \n7.3 Relevant developments since Chowâ€“Liu \n \n61 \nDifferent versions of the algorithm \nAlgorithm IV gives a maximum-weight spanning tree for the complete graph. Kruskalâ€™s \nalgorithm is usually stated in a different form, for finding the minimum-weight \nspanning tree for a given connected graph (Kruskal 1956). Bondy & Murty (2008) say \nthat Kruskalâ€™s algorithm first appeared in BorÅ¯vka (1926a,b), which are in Czech, and \nthat Kruskalâ€™s discovery of it was independent.  \nWhether the total weight has to be minimized or maximized is obviously trivial. To \nminimize instead of maximize, simply replace â€œlargestâ€ with â€œsmallestâ€ in step 2.  \nMoreover, as pointed out by Kruskal (1956), there is no loss of generality in considering \nonly the complete graph. Suppose you want to find a maximum-weight spanning tree \nfor a connected graph ğº= (ğ‘‰, ğ¸) that is not complete. In the complete graph ğ¾|ğ‘‰|, set \nthe weight of each edge ğ‘’ to be its weight in ğº if ğ‘’âˆˆğ¸, or âˆ’âˆ if ğ‘’âˆ‰ğ¸, and do Algorithm \nIV. ğº spans ğ¾|ğ‘‰| and thus contains at least one spanning tree. This means that ğ¾|ğ‘‰| has at \nleast one spanning tree with no infinite-weight edges. It follows that the algorithm will \nnever add an infinite-weight edge and will produce a tree that spans ğº. Any proof of \ncorrectness for Algorithm IV will therefore also suffice as a proof of correctness for the \nmore usual form of Kruskalâ€™s algorithm, and vice versa.  \nThe most well-known alternative to Kruskalâ€™s algorithm is Primâ€™s algorithm, for which a \nfull proof appears in section 2.2 of Even (1979).  \nProofs \nProofs that the usual form of Kruskalâ€™s algorithm is correct can be found in Bondy & \nMurty (1976, page 39) or Aldous & Wilson (2000, pages 190â€“191). Kruskal (1956) \nonly proved that the algorithm is correct when the weights are all distinct. All these \nproofs use the same basic idea of identifying a cycle and then modifying a tree by \nadding an edge and removing oneâ€”see also Theorem 2.3 in Even (1979). They use \nProposition 2.1, which stated that adding an edge to a tree creates a graph with \nprecisely one cycle.  \nKruskal (1956) proved that if all the edge-weights are distinct then the minimum-\nweight spanning tree is unique. (Actually this was the main purpose of the paper; the \nalgorithm was merely a way to prove this fact.) If the edge-weights are not distinct, then \nthe minimum- or maximum-weight spanning tree is not necessarily unique.  \n7.3 \nRelevant developments since Chowâ€“Liu  \nChow & Liuâ€™s method can be adapted to the case of the multivariate Gaussian distri-\nbution. Again it gives either the tree that minimizes the Kullbackâ€“Leibler distance to the \ntrue distribution or the maximum-likelihood tree. As in the discrete case, the weight on \neach edge is the mutual information. This equals âˆ’\n1\n2 log(1 âˆ’ğœŒğ‘’2), where ğœŒğ‘’ is the actual \nor empirical correlation coefficient along edge ğ‘’. Transforming the edge-weights by any \nmonotone-increasing function gives the same result in Kruskalâ€™s algorithm, so it is also \npossible to use just ğœŒğ‘’2 (Goldberger & Leshem 2009). This method for GGMs appears in \nLauritzen (2006), Goldberger & Leshem (2009), and Tan et al (2010a).  \n\n7 The Chowâ€“Liu algorithm \n \n 62 \nEdwards et al (2010) gives two adaptations of Chow & Liuâ€™s method. The first deals \nwith the drawback that even if the true graph is a forest, Chow & Liuâ€™s method will \nalways produce a tree. This is analogous to the facts that in graphical models the \nmaximum-likelihood graph is always the complete graph and in regression problems \nthe maximum-likelihood model always includes all the covariates. They adapt Chow & \nLiuâ€™s method to optimize a penalized likelihood criterion such as AIC or BIC. In general, \nthis produces a forest, not a tree. The formula for the edge-weights is changed by \nsubtracting a certain quantity. This means that the edge-weights can be negative. To \nfind the optimal forest, you remove all the edges whose weights are negative and then \ndo Kruskalâ€™s algorithm on all the connected components. Their second adaptation is an \nextension to mixed graphical models where some nodes are discrete and some are \nGaussian. Obviously GGMs are a special case of these models. The paper also includes \nseveral example applications of the methods. One of these uses the breast cancer data \nfrom the R package â€œgRbaseâ€. The analysis took 18 seconds and located several nodes \nthat seem to be hubs. It is also described in section 7.3 of HÃ¸jsgaard et al (2012). \nThe two algorithms in Edwards et al (2010) have been implemented in the R function \nminForest, in the package â€œgRapHDâ€ (Abreu et al 2010). See â€œFinding the MAP forest in \nRâ€ in section 7.4.  \n7.4 \nFinding the MAP forest \nUsing a uniform graph prior  \nIn Bayesian structure-learning, the graph that has highest posterior probability is called \nthe MAP (maximum a posteriori probability) graph. An adaptation of the Chowâ€“Liu \nalgorithm can be used to find the MAP forest for discrete random variables, assuming \nthat the graph prior is uniform on the set of forests (HÃ¸jsgaard et al 2012, section 7.7). \nThe weight of each edge is taken to be the logarithm of the Bayes factor for the presence \nof that edge, and the version of Kruskalâ€™s algorithm in Edwards et al (2010) is then used \nto find the MAP forest. (Remove the edges that have negative weights, then do Kruskalâ€™s \nalgorithm on the graph that remains.) Forests like this are sometimes called spanning \nforests, where â€œspanningâ€ just means that the forest has the same set of nodes as the \noriginal graph.  \nThis method can be adapted to GGM structure-learning with the hyper inverse Wishart \nprior on the covariance matrix. This adaptation appears in lectures 8 and 9 of Lauritzen \n(2006). Let ğº= (ğ‘‰, ğ¸) and ğ‘= |ğ‘‰|. Assume the graph prior distribution is uniform, so \nğ‘(ğº) is the same for every graph and ğ‘( ğºâˆ£ğ‘¥) âˆğ‘(ğ‘¥âˆ£ğº). The method is best explained \nby rearranging the formula for the marginal likelihood ğ‘(ğ‘¥âˆ£ğº). In section 3.1 I wrote \nthe formula for the marginal likelihood of a decomposable graph as  \nğ‘( ğ‘¥âˆ£ğº) = (2ğœ‹)âˆ’ğ‘›ğ‘/2\nâˆ\nğ‘˜(ğ¶, ğ›¿, ğ·)\nğ‘˜(ğ¶, ğ›¿+ ğ‘›, ğ·+ ğ‘ˆ) \nğ¶\nâˆ\nğ‘˜(ğ‘†, ğ›¿, ğ·)\nğ‘˜(ğ‘†, ğ›¿+ ğ‘›, ğ·+ ğ‘ˆ)\nğ‘†\n \n , \nwhere  \n\n \n7.4 Finding the MAP forest \n \n63 \nğ‘˜(ğ¶, ğ›¿, ğ·) =\n|ğ·ğ¶\n2 |\nğ›¿+|ğ¶|âˆ’1\n2\n Î“|ğ¶| (ğ›¿+ |ğ¶| âˆ’1\n2\n)\n . \nIn the decomposition of a forest the cliques are the edges (strictly, the pairs of nodes \nthat have edges between them) and the isolated nodes (the nodes that have degree \nzero). The separators are a subset of the individual nodes plus, if the forest is not \nconnected, the empty set. The number of times each node appears as a separator is its \ndegree minus one; any empty separators contribute a factor of 1 and can thus be \nignored.  \nSo for forests the marginal likelihood is  \nğ‘( ğ‘¥âˆ£ğº) = (2ğœ‹)âˆ’ğ‘›ğ‘/2\nâˆ\nğ‘˜({ğ‘¢, ğ‘£}, ğ›¿, ğ·)\nğ‘˜({ğ‘¢, ğ‘£}, ğ›¿+ ğ‘›, ğ·+ ğ‘ˆ) \n(ğ‘¢,ğ‘£)âˆˆğ¸\nâˆ\n[\nğ‘˜({ğ‘£}, ğ›¿, ğ·)\nğ‘˜({ğ‘£}, ğ›¿+ ğ‘›, ğ·+ ğ‘ˆ)]\ndeg(ğ‘£)âˆ’1\nğ‘£âˆˆğ‘‰\n \n . \nLet \nğ¾(ğ‘£1, ğ‘£2, â€¦ , ğ‘£ğ‘š) = \nğ‘˜({ğ‘£1, ğ‘£2, â€¦ , ğ‘£ğ‘š}, ğ›¿, ğ·)\nğ‘˜({ğ‘£1, ğ‘£2, â€¦ , ğ‘£ğ‘š}, ğ›¿+ ğ‘›, ğ·+ ğ‘ˆ) . \n(ğ¾ is a â€œvariadicâ€ function that takes any number of arguments.) Then  \nğ‘( ğ‘¥âˆ£ğº) = (2ğœ‹)âˆ’ğ‘›ğ‘/2 âˆ\nğ¾(ğ‘¢, ğ‘£) \n(ğ‘¢,ğ‘£)âˆˆğ¸\nâˆ\nğ¾(ğ‘£)deg(ğ‘£)âˆ’1\nğ‘£âˆˆğ‘‰\n   \n                                   = (2ğœ‹)âˆ’ğ‘›ğ‘/2 âˆğ¾(ğ‘£) \nğ‘£âˆˆğ‘‰\nâˆ\nğ¾(ğ‘¢, ğ‘£)\nğ¾(ğ‘¢)ğ¾(ğ‘£) .\n(ğ‘¢,ğ‘£)âˆˆğ¸\n \nIn this last expression, the first product is over the nodes, which are the same for all \ngraphs. So to choose among forests with different edges it is sufficient to maximize the \nsecond product. This can be done using an adaptation of Chow & Liuâ€™s algorithm. \nSimply let the weight of edge (ğ‘¢, ğ‘£) be log\nğ¾(ğ‘¢,ğ‘£)\nğ¾(ğ‘¢)ğ¾(ğ‘£) , and as in Edwards et al (2010) omit \nall edges that have negative weights.  \nAfter describing how to use penalized likelihoods with the Chowâ€“Liu algorithm, \nEdwards et al (2010) states that Panayidou (2011) â€œfinds the Bayesian MAP tree/forest \nin a similar wayâ€. This probably refers to the method in HÃ¸jsgaard et al (2012) or the \nmethod I have described. (Panayidou 2011 can only be viewed by travelling to Oxford.)  \nThis method for finding the MAP forest is fast, like the standard Chowâ€“Liu algorithm. \nSimilar methods appear in MeilÄƒ-Predoviciu (1999, page 59) and Heckerman et al \n(1995, pages 226â€“227). Lauritzen (2006) poses the question of whether there a \nfeasible algorithm for finding the MAP decomposable graph, and states that the answer \nis probably no since this task is probably NP-complete.  \n\n7 The Chowâ€“Liu algorithm \n \n 64 \nFactored priors \nThe method described in the previous section requires the graph prior distribution to \nbe uniform, so that ğ‘( ğºâˆ£ğ‘¥) âˆğ‘(ğ‘¥âˆ£ğº). But it can also be adapted to work with \nfactored distributions, which were defined in section 5.4. For factored distributions, \neach edge (ğ‘¢, ğ‘£) has associated with it a quantity ğ‘¤ğ‘¢ğ‘£, and the probability of ğº= (ğ‘‰, ğ¸) \nis  \nğ‘(ğº) âˆ\nâˆğ‘¤ğ‘¢ğ‘£\n(ğ‘¢,ğ‘£)âˆˆğ¸\n . \nWith a graph prior distribution of this form, the posterior probability of graph ğº is  \nğ‘( ğºâˆ£ğ‘¥) âˆğ‘( ğ‘¥âˆ£ğº)ğ‘(ğº) \n= (2ğœ‹)âˆ’ğ‘›ğ‘/2 âˆğ¾(ğ‘£) \nğ‘£âˆˆğ‘‰\nâˆ\nğ¾(ğ‘¢, ğ‘£)\nğ¾(ğ‘¢)ğ¾(ğ‘£) \n(ğ‘¢,ğ‘£)âˆˆğ¸\n âˆğ‘¤ğ‘¢ğ‘£ \n(ğ‘¢,ğ‘£)âˆˆğ¸\n. \nThe only parts of this formula that depend on the forest are the second and third \nproducts. So the weight of edge (ğ‘¢, ğ‘£) should be set to  \nlog ( ğ¾(ğ‘¢, ğ‘£)\nğ¾(ğ‘¢)ğ¾(ğ‘£) ğ‘¤ğ‘¢ğ‘£) .  \nKruskalâ€™s algorithm can now be used as before to find the MAP forest. Edges whose \nweights are negative should be removed before doing Kruskalâ€™s algorithm.  \nThis method cannot be adapted to work with general graph prior distributions. In the \ngeneral case the posterior probability of a graph ğº is ğ‘( ğºâˆ£ğ‘¥) âˆğ‘( ğ‘¥âˆ£ğº)ğ‘(ğº), and the \nprior probability of the graph, ğ‘(ğº), cannot be factorized into a contribution from each \nedge. So it is impossible to write ğ‘( ğºâˆ£ğ‘¥) in a way that reduces the problem to that of \nfinding a maximum-weight spanning forest. For example, with the size-based priors of \nArmstrong et al (2009), adding an edge does not have a fixed multiplicative effect on \nthe probability of the graph; the effect depends rather on how many edges there are in \ntotal.  \nFinding the MAP forest in R \nThe MAP forest for GGMs can be found using the function minForest, from the R \npackage â€œgRapHDâ€ (Abreu et al 2010; see also Edwards et al 2010 or HÃ¸jsgaard et al \n2012, section 7.4). You have to write a custom function to calculate the edge-weights \nand then pass this function to minForest as the â€œstatâ€ argument. The only other \narguments you need to provide are the data (an ğ‘›Ã— ğ‘ matrix) and the prior values of \nthe two HIW hyperparameters. I tested this method on the iris data used by Roverato \n(2002) and it gave the right answer.  \nDespite its name, minForest finds the forest that maximizes the given edge-weights. In \nits documentation the Arguments section states that the default value of stat is â€œLRâ€, but \nactually it is â€œBICâ€.  \n\n \n7.5 Supplementary notes \n \n65 \n7.5 \nSupplementary notes  \nMethods for finding the top few trees \nKruskalâ€™s algorithm finds the optimal tree. There are also fast algorithms that can find \nthe top ğ‘˜ trees, for any ğ‘˜âˆˆ{1, â€¦ , ğ‘ğ‘âˆ’2}, or all the spanning trees in order from best to \nworst. See Gabow (1977), Camerini et al (1980), Eppstein (1990), SÃ¶rensen & Janssens \n(2005), or Climaco et al (2008). Cowell (2013) uses the algorithm of SÃ¶rensen & \nJanssens (2005) to find the most likely pedigree charts for a group of animals or \nhumans.  \nOther edge-weights in the Chowâ€“Liu algorithm \nThe algorithm in section 7.1 does not have to be done with these exact weights on the \nedges. Transforming the edge-weights by any monotone increasing function makes no \ndifference. For more on this see Acid et al (1991).  \nImprovements to the Chowâ€“Liu algorithm \nNumerous papers have proposed improvements to the Chowâ€“Liu algorithm and \nadaptations of it. For example, Alcobe (2002) gives an â€œincrementalâ€ method to imple-\nment the Chowâ€“Liu algorithm in the case where data come in one at a time and you \nneed to update the tree after each item of data. His computer experiments suggest that \nit is much faster than running the Chowâ€“Liu algorithm again from scratch each time. \nChoi et al (2011) gives two algorithms for learning â€œlatentâ€ tree graphical models where \nsome variables are unobserved, which is NP-hard. They prove that the algorithms are \nasymptotically consistent and report the results of numerical experiments. Wang \n(2009) is similar.  \nMeilÄƒ (1999) presents a way of speeding up the Chowâ€“Liu algorithm if the data are \nsparse, by comparing some mutual informations without actually calculating them. \nZaffalon & Hutter (2005) is an adaptation of the Chowâ€“Liu algorithm that apparently \ngives results that are more robust to the random variation in the data, by using the \nâ€œimprecise Dirichlet modelâ€ to model the prior uncertainty about the data, which are \ndiscrete-valued.  \nPelleg & Moore (2006) present a way of speeding up the Chowâ€“Liu algorithm for large \ndatasets by maintaining confidence intervals on the edge-weights. Their maximum-\nweight spanning tree algorithm works down from the complete graph to a tree. When \ntwo edges need to be compared but their confidence intervals overlap, they look at \nmore data to shrink one of them and make a decision. Naturally this method does not \nalways find the optimal tree.  \nOther research based on the Chowâ€“Liu algorithm \nGupta et al (2010) is about learning forest graphical models using non-parametric \nkernel density estimates on each node and pair of nodes, using a method based on \nChow & Liuâ€™s. Fleischer et al (2005) consider the NP-hard problem of finding the \nminimum spanning tree where both the edges and the â€œinner nodesâ€ (the nodes that \nare not leaves) have weights.  \n\n7 The Chowâ€“Liu algorithm \n \n 66 \nIn machine learning there has been research on â€œmixture of treesâ€ models (MeilÄƒ & \nJordan 2000), in which the joint density consists of a weighted sum of the densities of \nseveral tree distributions. Mixture-of-trees models can be regarded as having an \nunobserved variable that chooses one of the trees; each separate tree distribution is a \nconditional distribution given that the unobserved variable chose that tree. MeilÄƒ & \nJordan (2000)â€™s method for learning a mixture-of-trees model from data is a \ncombination of the EM algorithm, for the unobserved variable, and Chowâ€“Liu, for each \nof the trees. See also Kollin & Koivisto (2006) or Kumar & Koller (2009).  \nVincent Tan and his collaborators have produced several papers based on the Chowâ€“Liu \nalgorithm. In Tan et al (2010a) they calculate the â€œerror exponentâ€ for the maximum-\nlikelihood estimator of the tree structure. Measuring the difficulty of learning a graph \nby the error exponent, they prove that the star graph (consisting of a hub and its spokes \nonly) is the hardest to learn and the chain graph (with all the nodes in a line) is the \neasiest. See section 11.1 for other senses in which these graphs are extremal. Tan et al \n(2010c) is about hypothesis tests to decide which of two trees or forests a sample \ncomes from, Tan et al (2010b) is about learning two tree graphical models for the \npurpose of classifying future observations into one of two categories, and Tan et al \n(2010d) is about learning forests for discrete graphical models by removing edges from \nthe Chowâ€“Liu tree.  \n \n\n \n67 \n8 \nMethods for factored \ndistributions on trees \n8.1 \nIntroduction and the Matrix Tree Theorem \nThis chapter is about methods for analyzing factored distributions on trees, in \nparticular factored posterior distributions. For a factored distribution on trees, MeilÄƒ & \nJaakkola (2006) showed how to find the normalizing constant and certain other \nquantities in polynomial time, rather than by calculating the unnormalized probabilities \nof all the possible trees and summing them, which would be much slower. (Factored \ndistributions were discussed in section 5.4. The main ideas in the chapter were \nmentioned briefly and without details in two presentations, Lauritzen 2006 and 2012.)  \nMeilÄƒ & Jaakkola (2006) presented their theorems in the context of Bayesian structure-\nlearning for discrete-valued graphical models. Section 8.2 gives a summary of their \nrelevant results and several new examples of questions they can be used to answer. In \nsection 8.3 I show how these methods can be used for GGM structure-learning. Section \n8.4 is a review of methods for generating trees and forests from factored distributions.  \nThe methods in this chapter are based on the Matrix Tree Theorem (MTT), or more \nprecisely a version of it that I will call the Weighted Matrix Tree Theorem (WMTT). \nMTT gives a way to calculate how many spanning trees a given graph has, and WMTT \ngives an explicit way of finding all the spanning trees. Section 8.5 describes the origins \nof these two theorems and includes references to publications that contain proofs of \nthem.  \nMatrix Tree Theorem (MTT). For an undirected graph ğº= (ğ‘‰, ğ¸), where ğ‘‰= {1, â€¦ , ğ‘}, \ndefine the Laplacian matrix ğ¿ by  \nğ¿ğ‘–ğ‘—= {\n âˆ’1  if (ğ‘–, ğ‘—) âˆˆğ¸\n deg(ğ‘–)   if ğ‘–= ğ‘—\n 0  otherwise.   \n \nThe number of spanning trees of ğº equals the absolute value of any minor of ğ¿. (A \nminor of a matrix is the determinant of the matrix formed by removing one row and \none column.)  \nWeighted Matrix Tree Theorem (WMTT). Let ğº be as above. On each edge (ğ‘–, ğ‘—), put an \nindeterminate variable ğ‘¥ğ‘–ğ‘—. Define ğ¿ by  \n\n8 Methods for factored distributions on trees \n \n 68 \nğ¿ğ‘–ğ‘—=\n{\n \n \n \n âˆ’ğ‘¥ğ‘–ğ‘—                   if (ğ‘–, ğ‘—) âˆˆğ¸  \n âˆ‘\nğ‘¥ğ‘–ğ‘˜\nğ‘˜:(ğ‘–,ğ‘˜)âˆˆğ¸\n  if ğ‘–= ğ‘—           \n0                         otherwise.   \n \nLet ğ‘€ be the absolute value of any of the minors of ğ¿, and for a spanning tree ğ‘‡=\n(ğ‘‰, ğ¸ğ‘‡), let â„(ğ‘‡) = âˆ\nğ‘¥ğ‘–ğ‘—\n(ğ‘–,ğ‘—)âˆˆğ¸ğ‘‡\n. Then ğ‘€= âˆ‘â„(ğ‘‡)\nğ‘‡\n, where the sum is over all spanning \ntrees of ğº.  \n \nIn other words, each monomial term in ğ‘€, when it is simplified, corresponds to one \npossible spanning tree. (A monomial is a product of powers of variables.) If ğ‘¥ğ‘–ğ‘—= 1 for \nall (ğ‘–, ğ‘—) âˆˆğ¸ then ğ¿ is just the Laplacian matrix and WMTT reduces to MTT.  \nHere is an example of WMTT. It can also be regarded as an example of MTT, by replacing \nall the ğ‘¥ğ‘–ğ‘—â€™s by 1. The graph is shown on the left in Figure 8.1. The matrix is  \nğ¿= (\nğ‘¥12 + ğ‘¥14\nâˆ’ğ‘¥12\n0\nâˆ’ğ‘¥14\nâˆ’ğ‘¥12\nğ‘¥12 + ğ‘¥23 + ğ‘¥24\nâˆ’ğ‘¥23\nâˆ’ğ‘¥24\n0\nâˆ’ğ‘¥23\nğ‘¥23 + ğ‘¥34\nâˆ’ğ‘¥34\nâˆ’ğ‘¥14\nâˆ’ğ‘¥24\nâˆ’ğ‘¥34\nğ‘¥14 + ğ‘¥24 + ğ‘¥34\n) , \nand the absolute value of any of its minors will simplify to give \nğ‘€= ğ‘¥12ğ‘¥14ğ‘¥23 + ğ‘¥12ğ‘¥14ğ‘¥34 + ğ‘¥12ğ‘¥23ğ‘¥24 + ğ‘¥12ğ‘¥23ğ‘¥34 \n               + ğ‘¥12ğ‘¥24ğ‘¥34 + ğ‘¥14ğ‘¥23ğ‘¥24 + ğ‘¥14ğ‘¥23ğ‘¥34 + ğ‘¥14ğ‘¥24ğ‘¥34. \nFor instance, the first monomial, ğ‘¥12ğ‘¥14ğ‘¥23, corresponds to the spanning tree shown on \nthe right in Figure 8.1. Setting all the ğ‘¥ğ‘–ğ‘—â€™s to 1 gives ğ‘€= 8, which is the number of \nspanning trees of the graph.  \nAs another example, if ğº is ğ¾3, the complete graph on 3 nodes, then ğ‘€ simplifies to \nğ‘¥12ğ‘¥13 + ğ‘¥12ğ‘¥23 + ğ‘¥13ğ‘¥23. The first monomial, ğ‘¥12ğ‘¥13, corresponds to the tree with \nedges (1,2) and (1,3).  \n \n \nFigure 8.1. A graph (left), and the spanning tree of it that corresponds to ğ‘¥12ğ‘¥14ğ‘¥23 in WMTT.  \n \nMany proofs of MTT work by first showing that the absolute values of the minors are all \nequal. They then use the Binetâ€“Cauchy theorem (Lancaster & Tismenetsky 1985, \n\n \n8.2 The normalizing constant for discrete-valued tree graphical models \n \n69 \nsection 2.5) to express one of the minors in terms of determinants of smaller matrices. \nThese determinants are Â±1 if the corresponding subgraph is a spanning tree and 0 \notherwise.  \n8.2 \nThe normalizing constant for discrete-valued tree \ngraphical models \nFor factored distributions on the set of trees, MeilÄƒ & Jaakkola (2006) show how to \ncalculate the normalizing constant in polynomial time, using WMTT. Without their \nmethod this would be impractical, since the obvious way to calculate this quantity \nrequires summing over all possible trees, and the number of possible trees is super-\nexponential.  \nThe main theorem follows from applying WMTT to the complete graph ğ¾ğ‘. Suppose you \nhave a factored distribution on trees, defined as in equation (1) in section 5.4. In WMTT, \nlet each of the indeterminate variables ğ‘¥ğ‘–ğ‘— equal the corresponding edge-factor ğ‘¤ğ‘’=\nğ‘¤(ğ‘–,ğ‘—) from the factored distribution. Now WMTT states that ğ‘€= âˆ‘â„(ğ‘‡)\nğ‘‡\n, where the \nsum is over all the spanning trees of ğº, in other words all the trees. But  \nâ„(ğ‘‡) =\nâˆğ‘¥ğ‘–ğ‘—\n(ğ‘–,ğ‘—)âˆˆğ¸ğ‘‡\n= âˆğ‘¤ğ‘’\nğ‘’âˆˆğ¸ğ‘‡\nâˆâ„™(ğ‘‡). \nThe normalizing constant for the factored distribution is  \nâˆ‘âˆğ‘¤ğ‘’\nğ‘’âˆˆğ¸ğ‘‡\nğ‘‡\n= âˆ‘â„(ğ‘‡)\nğ‘‡\n= ğ‘€, \nwhich can be calculated in polynomial time using standard algorithms for calculating \ndeterminants.  \nMeilÄƒ & Jaakkola (2006) is about Bayesian structure-learning for discrete-valued tree \ngraphical models. Undirected tree graphical models are equivalent to rooted-tree DAG \ngraphical models, and they use both forms. They show that if certain reasonable-\nsounding assumptions about the parameters of the discrete distribution are satisfied, \nthen the prior distributions of these parameters must be a product of Dirichlet distri-\nbutions. It then follows that if the prior distribution on the graph structure is factored, \nthen so is the posterior. (The abstract says the posterior â€œcan be completely determined \nanalytically in polynomial timeâ€, but calculating the entire posterior distribution in \npolynomial time is impossible since the number of trees, and hence the amount of \ninformation in the posterior, is super-exponential.)  \nBeing able to calculate the normalizing constant means that the posterior probability of \nany given tree can be calculated exactly. MeilÄƒ & Jaakkola (2006) also show that under a \nfactored distribution, the expectations of real-valued â€œadditiveâ€ or â€œmultiplicativeâ€ \nfunctions of trees can be calculated quickly by using derivatives of the normalizing \nconstant (expressed as a function of the edge-factors). A function ğ‘“ is additive if it is of \nthe form ğ‘“((ğ‘‰, ğ¸)) = âˆ‘\nğ‘“ğ‘¢ğ‘£\n(ğ‘¢,ğ‘£)âˆˆğ¸\n, where ğ‘“ğ‘¢ğ‘£ are weights on the edges, and multi-\nplicative if it is of the form ğ‘“((ğ‘‰, ğ¸)) = âˆ\nğ‘“ğ‘¢ğ‘£\n(ğ‘¢,ğ‘£)âˆˆğ¸\n.  \n\n8 Methods for factored distributions on trees \n \n 70 \nI will now show that several useful quantities to do with factored posterior distri-\nbutions can be calculated using very simple additive functions. Let ğ‘“= ğ•€(ğ‘–,ğ‘—)âˆˆğ¸. This is \nthe indicator function for the edge (ğ‘–, ğ‘—). This is obviously additive, with  \nğ‘“ğ‘¢ğ‘£= { 1 if {ğ‘¢, ğ‘£} = {ğ‘–, ğ‘—}\n 0 otherwise.           \nThe method of MeilÄƒ & Jaakkola (2006) can therefore be used to calculate ğ”¼(ğ‘“(ğ¸)) in \nthe posterior distribution. This is simply the posterior probability that the edge is in the \ngraph, â„™((ğ‘–, ğ‘—) âˆˆğ¸), which could very easily be a quantity of interest.  \nLet ğ‘ğ‘–ğ‘—= â„™((ğ‘–, ğ‘—) âˆˆğ¸), and suppose that this has been calculated for every possible \nedge. If the data was simulated from a distribution with a known graph structure \n(ğ‘‰, ğ¸ğ‘¡ğ‘Ÿğ‘¢ğ‘’), then the expected number of true-positives, another quantity that might be \nof \ninterest, \nis âˆ‘\nğ‘ğ‘¢ğ‘£\n(ğ‘¢,ğ‘£)âˆˆğ¸ğ‘¡ğ‘Ÿğ‘¢ğ‘’\n. \nThe \nexpected \nnumber \nof \nfalse-positives \nis \nâˆ‘\nğ‘ğ‘¢ğ‘£\n(ğ‘¢,ğ‘£)âˆˆğ¸ğ‘ğ‘™ğ‘™âˆ–ğ¸ğ‘¡ğ‘Ÿğ‘¢ğ‘’\n, and other related quantities can be found in similar ways. (See \nsection 10.2 for more on these quantities.)  \nThe degree of ğ‘– is âˆ‘\nğ•€(ğ‘£,ğ‘–)âˆˆğ¸\nğ‘£â‰ ğ‘–\n, so the expected degree of ğ‘– is ğ”¼(deg(ğ‘–)) = âˆ‘\nğ‘ğ‘£ğ‘–\nğ‘£â‰ ğ‘–\n. \nAlternatively, the expected degree of ğ‘– can be calculated directly, using  \nğ‘“ğ‘¢ğ‘£= { 1  if ğ‘¢= ğ‘– or ğ‘£= ğ‘–\n 0  otherwise.             \nThe value of the corresponding additive function ğ‘“ is deg(ğ‘–).  \nIn these ways several quantities that might be of interest can be expressed in terms of \nthe ğ‘ğ‘–ğ‘—â€™s. One quantity that cannot is the expected maximum degree. Finding this \nrequires quantities like â„™(deg(ğ‘–) = 1). This cannot be calculated using additive or \nmultiplicative functions, because it is the expectation of ğ•€{deg(ğ‘–) = 1}, which is not just \nthe sum or product of fixed weights on the edges.  \nOther limitations of these methods are that they do not work with general prior \ndistributions and they do not work with forestsâ€”essentially because there is no MTT \nfor forests. Another possible drawback is that in Bayesian structure-learning it may be \npreferable to work with only a subset of the possible models, rather than average over \nall of them. Madigan & Raftery (1994), for example, argue that models with much lower \nprobability than the best ones should be discarded completely.  \n8.3 \nThe normalizing constant for GGMs \nHow the methods work for GGMs \nMeilÄƒ & Jaakkola (2006) mention that their results still work with GGMs and Bayesian \nlearning of tree-structure but do not give any details. For GGMs, if the uniform graph \nprior on trees is used, the posterior is \nâ„™( ğºâˆ£ğ‘¥) âˆ(2ğœ‹)âˆ’ğ‘›ğ‘/2 âˆğ¾(ğ‘£) \nğ‘£âˆˆğ‘‰\nâˆ\nğ¾(ğ‘¢, ğ‘£)\nğ¾(ğ‘¢)ğ¾(ğ‘£) \n(ğ‘¢,ğ‘£)âˆˆğ¸\n, \n\n \n8.3 The normalizing constant for GGMs \n \n71 \nwhere ğ‘¥ is the ğ‘›Ã— ğ‘ matrix of observed data and ğ¾ is defined in section 7.4. (The \nexpression for ğ¾ involves ğ‘ˆ= ğ‘¥ğ‘‡ğ‘¥ and the HIW hyperparameters, but ğ¾ is written as a \nfunction of one or two nodes for simplicity.)  \nThis is clearly a factored distribution, with the weight on edge (ğ‘¢, ğ‘£) being \nğ¾(ğ‘¢,ğ‘£)\nğ¾(ğ‘¢)ğ¾(ğ‘£). It \nfollows that all the facts and methods in the previous section can be used. For Bayesian \nstructure-learning on forests, \nğ¾(ğ‘¢,ğ‘£)\nğ¾(ğ‘¢)ğ¾(ğ‘£) can be regarded as the Bayes factor for the \npresence of the edge (ğ‘¢, ğ‘£), as mentioned in section 7.4 and Lauritzen (2006, 2012).  \nIf the graph prior is factored then the results in the previous section still hold. If the \nprior is  \nâ„™(ğº) âˆâˆğ‘¤ğ‘’\nğ‘’âˆˆğ¸\n , \nthen the posterior is  \nâ„™( ğºâˆ£ğ‘¥) âˆ\nâˆğ‘¤(ğ‘¢,ğ‘£)\nğ¾(ğ‘¢, ğ‘£)\nğ¾(ğ‘¢)ğ¾(ğ‘£) \n(ğ‘¢,ğ‘£)âˆˆğ¸\n, \nwhich is clearly a factored distribution.  \nIn the computer experiments in section 11.5, MeilÄƒ & Jaakkola (2006)â€™s methods are \nused for finding the expected true-positive rate under the posterior distribution. The \nlimitations of their methods are the same in the case of GGMs as they are for discrete-\nvalued graphical models.  \nA computer program for GGMs \nI have written a computer program that uses the MTT-based method to find the \nnormalizing constant, the expected degree of each node, and the expected number of \ntrue positives, for GGMs. I used this to produce the results in section 11.5. The input to \nthe program is the data and the prior values of the HIW hyperparameters. But the input \nto the subroutines that actually perform the MTT-based methods is just the symmetric \nmatrix of edge-factors that defines the posterior distribution. So the program could \neasily be adapted to work with any factored prior.  \nFor ğ‘= 30, the normalizing constant is beyond the range of the usual double-precision \nfloating-point numbers that are used by computers. But it can be found if you use \nspecial classes and packages for arbitrary- or high-precision decimals. In the Java \nprogramming language, objects of the BigDecimal class (Oracle 2012) are stored as \nğ‘¥Ã— 10ğ‘¦ where ğ‘¥âˆˆ{0,1, â€¦ }  and ğ‘¦âˆˆ{âˆ’231, âˆ’231 + 1, â€¦ , 231 âˆ’1} . The size of ğ‘¥ is \nlimited only by the size of the Java virtual machine, which in turn is limited by the host \ncomputer. Matrix algebra with BigDecimals can be done using any of the classes that \nimplements the FieldMatrix interface in the Commons Math package (Apache Software \nFoundation 2012). These classes and packages are of a high quality, though arithmetic \nwith them is naturally slower than with the usual floating-point decimals.  \nOn the topic of high-precision arithmetic, Wang & Li (2012) say that their methods may \nneed to calculate quantities such as ğ‘’âˆ’10000, but â€œto our knowledge, [no] current soft-\n\n8 Methods for factored distributions on trees \n \n 72 \nware for Gaussian graphical models has yet supported this level of precision.â€ Perhaps \nmy program for the MTT-based methods is the first. Lauritzen (2006) says certain \nalgorithms for forests do not work well because most of the values are essentially zero. \nBigDecimal might be able to overcome these problems.  \n8.4 \nGenerating random trees or forests \nAs discussed in sections 5.1 and 5.7, one of the main things you might want to do with a \ngraph distribution is generate from it. This section is a review of methods for \ngenerating spanning trees or forests of a given graph according to a uniform or factored \ndistribution. For Bayesian learning of tree graphical models, the given graph would be \nğ¾ğ‘. (Generating from uniform distributions has no purpose in graphical model \nstructure-learning but is closely related to generating from factored distributions.)  \nPropp & Wilson (1998) give a history of the algorithms for generating a spanning tree \nuniformly at random. The first one was by GuÃ©noche (1983) and a faster algorithm \nappears in Colbourn et al (1989). The basic idea is that if you repeatedly choose an \nedge uniformly at random and discard it if it creates a cycle, you will not get the uniform \ndistribution. But if you go through the edges and accept each one according to the \nproportion of spanning trees that contain it, then you will. This proportion can be \ncalculated using MTT. The same idea works for factored distributions, using WMTTâ€”\nsee Kulkarni (1990).  \nA different type of algorithm for generating a spanning tree was discovered by Broder \n(1989) and Aldous (1990). Do a simple random walk on the graph until you have \nvisited every node. For each node apart from the first one, record the edge by which the \nnode was first visited. The set of these edges constitutes a spanning tree chosen uni-\nformly at random.  \nPropp & Wilson (1998) give two algorithms for generating from a uniform or factored \ndistribution. One uses â€œcoupling from the pastâ€, which is a way of generating exactly \nfrom the invariant distribution of an ergodic Markov chain that has a finite number of \nstates. The other uses â€œcycle-poppingâ€.  \nGenerating forests from a factored distribution is much more difficult. Dai (2008) \npresents two sets of algorithms for this problem. (He uses â€œforestsâ€ to mean subgraphs \nof a given graph that contain all its nodes and have no cycles.) The first set uses \ncoupling from the past and the second set uses rejection methods. The main rejection-\ntype algorithm is 8: add an extra node and edges from it to all the other nodes, run one \nof the algorithms for generating a random tree, remove the extra node and the edges \nthat include it, and then accept this forest with a certain probability.  \n8.5 \nSupplementary notes: the history of MTT \nMeilÄƒ & Jaakkola (2006) state that their main theorem was first proved in Jaakkola et al \n(2000) but that they later discovered a similar result, which must have been WMTT, in \nHarary (1967). The idea of using WMTT to find the normalizing constant for random \ntrees appears implicitly in Kulkarni (1990). It was also conceived independently by Koo \n\n \n8.5 Supplementary notes: the history of MTT \n \n73 \net al (2007), Smith & Smith (2007), and McDonald & Satta (2007), in the field of \ncomputational linguistics.  \nMTT and WMTT, or theorems that are essentially equivalent to them, were known in the \n19th century and rediscovered multiple times in the 20th. Moon (1970, page 42) and \nKnuth (1997, pages 583 and 586) give detailed accounts of their origins. Kirchhoff \n(1847) is often credited with MTT or WMTT. His main result is essentially the same as \nWMTT, but it is about the dual problem of finding all the sets of edges that can be \nremoved to leave a tree. A similar version appears in Maxwell (1892, pages 403â€“410). \nThese publications are about electrical circuits and resistances, and some mental \nexertion is needed to interpret them as graph theory. In mathematics, a version of \nWMTT appears in Cayley (1856) and Sylvester (1857). Books that contain proofs of \nWMTT include Moon (1970) and BollobÃ¡s (1998, page 57), and of course MTT follows \nfrom WMTT.  \nThe normalizing constant is called the â€œnormalization constantâ€ in MeilÄƒ & Jaakkola \n(2006). It is also known as the â€œpartition functionâ€, for example in Murray & Ghahra-\nmani (2004).  \n \n\n \n74 \n9 \nLocal moves in forests and trees \n9.1 \nPreamble  \nAlgorithmic graph theory (Even 1979, Gibbons 1985) is mostly about solving problems \nfor given graphs. Typical problems are testing whether a graph is planar or colouring \nthe nodes so that no two adjacent nodes have the same colour. Chapter 6 of Bondy & \nMurty (2008) is called â€œTree-search algorithmsâ€. This includes breadth-first search, \ndepth-first search, and algorithms to find minimum-weight spanning trees, shortest \npaths, and so on.  \nIn contrast, this chapter is about algorithms for storing and manipulating graphs, with a \nview to exploring the posterior graph distribution in graphical model structure-\nlearning. The main algorithms are designed for manipulating graphs by repeatedly \nadding and deleting edges. The main issue is how to store the graph in order to take \nadvantage of the information from the previous step, avoid wasteful repeated searches \nthrough the graph, and enable the information that is stored to be updated in an \nefficient way.  \nIn computer programs there are two common ways to store undirected graphs. The first \nis the adjacency matrix. This is usually a symmetric square matrix of 1s and 0s but can \nalso be regarded and stored as a triangular matrix, or a square or triangular matrix of \nbooleans. The other way is a list of edges. This is regarded as more suitable for sparse \ngraphs, since it uses less memory. Forests and trees can be stored in a different way by \nregarding each component as a rooted tree, with arbitrary root, and storing just the \nparent of each node, or â€œnullâ€ if it is a root.  \n9.2 \nStoring forests and trees for local moves  \nThe purposes of the algorithms \nAs mentioned in section 3.1, in Bayesian analysis of the graph structure it is impossible \nto calculate the posterior probability of all the 2(ğ‘\n2) possible graphs on ğ‘ nodes. For \ndecomposable graphs there are reversible-jump MCMC algorithms for sampling from \nthe posterior distribution of the graph structure and the covariance matrix (Giudici & \nGreen 1999, Green & Thomas 2013). Jones et al (2005) proposed a stochastic search \nalgorithm for moving through the space of all possible graphs and calculating the exact \nposterior probabilities of the graphs that are visited. Restricted versions of these \nalgorithms can be applied to forests, and adapted versions of them can be applied to \ntrees. (Details are given in section 10.1.)  \n\n \n9.2 Storing forests and trees for local moves \n \n75 \nConsider a Bayesian analysis in which attention is restricted to forests. To explore the \nposterior distribution of the graph structure the most obvious, natural, and â€œlocalâ€ type \nof move is to add or delete one edge at a time. For trees, the most obvious type of move \nis to move an edge. (I use the word â€œmoveâ€ with two different meanings. For forests a \nmove means adding or deleting an edge; for trees it means literally moving an edge \nfrom one position to another. I treat forests and trees completely separately, so there \nshould be no ambiguity.)  \nFor forests, it is easy to describe which edges can be added and removed while ensuring \nthat the graph is still a forest. Any existing edge can be removed, and an edge can be \nadded if and only if its two nodes are in different connected components. For trees, it is \nsimilarly easy to describe which moves are possible. First choose an edge, temporarily \nremove it, and identify the two connected components that result; the edge can then be \nput back between any two nodes that are not both in the same connected component. \n(Two alternative ways of making moves in trees are described in the last paragraph of \nthis subsection.)  \nThese conditions are easy to describe verbally, but they are less easy to program or \nwrite in the form of detailed algorithms, and they are time-consuming to carry out. To \nsee whether a particular move is possible it is necessary to identify connected compo-\nnents. Identifying a connected component means doing a breadth-first search, or \npossibly a depth-first search, through the component (Golumbic 1980, pages 37â€“42; \nCormen et al 2009, pages 594â€“612). This means finding all the neighbours of a node at \neach step. Finding several or all of the possible moves from the current graph, which is \nnecessary for the algorithm of Jones et al (2005), would require doing all of this many \ntimes. It is efficient and elegant to be able to choose a move straight away, rather than \nhaving to choose one, test whether it is a legal move, and if not then reject it and repeat. \nAnother issue is to do with how to choose moves randomly for MCMC proposal \ndistributions or other algorithms that explore the graph space. To achieve good mixing, \nit may be desirable to be able to choose a move uniformly at random from among all the \npossible moves.  \nSection 9.3 describes how to store a forest in such a way that it is easy to choose a legal \nmove uniformly at random, and how to update the stored information after a move. \nSection 9.4 describes an analogous system for trees. These systems make it simple to \nprogram graph-search algorithms that choose these moves uniformly at random. They \nare computationally efficient because the update algorithms are â€œlocalâ€â€”they never \nneed to search through all the nodes or all the edges. (On the other hand, the algorithms \nthat store the graph, and check that it is a forest or tree, are not local and do search \nthrough all the nodes. But these usually only need to be done once.)  \nFor exploring the space of decomposable graphs, Thomas & Green (2009a, b) state that \nit is desirable to be able to find a decomposable neighbouring graph straight away, not \nby choosing a random neighbour and then checking whether it is decomposable. (A \nâ€œneighbourâ€ of a graph is a graph formed by making one move from it.) The reason is \nthat for large ğ‘ the former way should be much faster. This is essentially the same as the \nmain reason behind my approach for forests and trees.  \n\n9 Local moves in forests and trees \n \n 76 \nThere are at least two alternative ways of making basic moves on trees. The first is in \nPropp & Wilson (1998, page 196) and is for rooted trees. Choose a node, other than the \nroot, to be the new root; draw an edge from the new root to the old root; and delete the \nedge that goes in to the new root. The second is from Climaco et al (2008). Add an edge, \nidentify the cycle that results, and then remove an edge from the cycle.  \nHow the algorithms are shown  \nIn the following subsections, each algorithm is preceded by an explanation of what it \ndoes and how it works. The algorithms are written in a style that is intended to be easy \nto translate into computer code. The right-hand columns contain verbal descriptions of \nwhat is being done, where this is not completely obvious, and other comments.  \nâ€¢ \nAlgorithms V and VIII, for storing the graph and checking its properties, assume \nthat the graph is supplied in the form of its adjacency matrix, ğ´.  \nâ€¢ \nğ‘‹â†ğ‘Œ means that ğ‘‹ is assigned the value ğ‘Œ. \nâ€¢ \nFor loops, the scope is shown by indentation. \nâ€¢ \nWhereas in directed graphs ğ‘ğ‘(ğ‘£) is usually a set, here it is a single node, because \nall the connected components are rooted trees.  \nNotation and partitions \nThe algorithms are written in pseudo-code or plain English rather than traditional set-\ntheory notation. One reason for this is that they use partitions. A partition of a set ğ‘ is a \nset {ğ‘1, â€¦ , ğ‘ğ‘˜} such that ğ‘ğ‘–âˆ©ğ‘ğ‘—= âˆ… for all ğ‘–â‰ ğ‘— and â‹ƒ\nğ‘ğ‘–\nğ‘˜\nğ‘–=1\n= ğ‘. The ğ‘ğ‘–â€™s are called \nâ€œpartsâ€. Simple operations such as â€œmove ğ‘£ to a new partâ€ are long and difficult to read \nwhen written in set-theory notation.  \nIn programming, probably the most natural way to work with a partition is to store it as \na pair of associative arrays. In one associative array, each key is an object (an element of \nğ‘), and the value associated with this key is the â€œlabelâ€ of the part that the object is in. \nThe labels can be positive integers. In the other associative array, each key is the label of \na part, and the value associated with this key is the set of objects that are in this part. \nQueries of the form â€œwhich part is this object in?â€ and â€œwhich objects does this part \ncontain?â€ can be answered quickly and easily since they each involve just a single look-\nup. When an object is moved from one part to another, both the associative arrays have \nto be updated.  \nFacts about rooted trees \nHere are several simple results about rooted trees that are used by the algorithms. As \nstated in section 2.1, in directed graphs I use â€œpathâ€ to mean â€œundirected pathâ€.  \nDefinition 9.1. A rooted tree is a directed tree in which one node is designated the root \nand the paths from the root to all the other nodes are directed paths. (In other words, \nall the edges point away from the root.)  \nRooted trees can also be defined as directed trees with any of the following three \nproperties. Proofs that the definitions are equivalent are omitted.  \nâ€¢ \nThe root is an ancestor of all the other nodes.  \n\n \n9.2 Storing forests and trees for local moves \n \n77 \nâ€¢ \nThe root has no parents, and all the other nodes have exactly one parent each.  \nâ€¢ \nFor each edge, the node nearer to the root is the parent and the node further from \nthe root is the child (where â€œnearerâ€ and â€œfurtherâ€ refer to the length of the path \nfrom the node to the root).  \nRooted trees are especially easy to deal with in algorithms and computer programs. \nTogether with each node are stored references to its children, and together with each \nnode except the root is stored a reference to its parent. Obviously ğ‘›ğ‘’(ğ‘£) = {ğ‘ğ‘(ğ‘£)} âˆª\nğ‘â„(ğ‘£). It is trivially easy to find the path from ğ‘£ to the rootâ€”this is simply ğ‘£, ğ‘ğ‘(ğ‘£), \nğ‘ğ‘(ğ‘ğ‘(ğ‘£)), â€¦, until you get to the root. It is easy to find all the descendants of ğ‘£, by \nâ€œfanning downâ€ from ğ‘£ to its children, then all their children, and so onâ€”this is done in \nAlgorithms VII and IX.  \nIn Algorithms Vâ€“VII, for forests, each connected component of the graph is regarded as \na rooted tree. In Algorithms VIIIâ€“IX, for trees, the whole graph is regarded as a rooted \ntree. The directions on the edges are just for the purpose of computational convenience. \nThey do not have any meaning in the graphical models.  \nDefinition 9.2. In a directed graph ğº, a reverse-directed path (ğ‘¢, ğ‘¢1, â€¦ , ğ‘¢ğ‘˜, ğ‘£) is a path \nsuch that (ğ‘£, ğ‘¢ğ‘˜, â€¦ , ğ‘¢1, ğ‘¢) is a directed path in ğº.  \nProposition 9.3 defines the â€œyoungest common ancestorâ€ of two nodes in a rooted tree \nand gives some of its properties.  \nProposition 9.3. For any two nodes ğ‘¢ and ğ‘£ in a rooted tree, there is a unique node ğ‘¤ \nthat has the following properties:  \nâ€¢ \nğ‘¢, ğ‘£âˆˆ{ğ‘¤} âˆªğ‘‘ğ‘’(ğ‘¤), and  \nâ€¢ \nall nodes ğ‘¥ such that ğ‘¢, ğ‘£âˆˆ{ğ‘¥} âˆªğ‘‘ğ‘’(ğ‘¥) are on the path between ğ‘¤ and the root.  \nI will call ğ‘¤ the â€œyoungest common ancestorâ€ of ğ‘¢ and ğ‘£. (Note that ğ‘¤ might be equal to \nğ‘¢ or ğ‘£, so it is not necessarily one of their ancestors.) It also has this property:  \nâ€¢ \nğ‘¤ is on the path between ğ‘¢ and ğ‘£.  \nProof. Let ğ‘ƒ be the reverse-directed path from ğ‘¢ to the root, and let ğ‘‹= {ğ‘¥âˆˆğ‘‰: ğ‘¢, ğ‘£âˆˆ\n{ğ‘¥} âˆªğ‘‘ğ‘’(ğ‘¥)}. If ğ‘¥âˆˆğ‘‹ then ğ‘¥âˆˆ{ğ‘¢} âˆªğ‘ğ‘›(ğ‘¢). This means there is a reverse-directed path \nfrom ğ‘¢ to ğ‘¥. Each node has at most one parent, so ğ‘¥ must lie on ğ‘ƒ. Therefore all \nelements of ğ‘‹ are in ğ‘ƒ. (It is not strictly true that â€œğ‘‹âŠ†ğ‘ƒâ€, since ğ‘ƒ is a sequence.)  \nLet ğ‘¤ be the first element of ğ‘ƒ that is in ğ‘‹. Since ğ‘¢, ğ‘£âˆˆ{ğ‘¤} âˆªğ‘‘ğ‘’(ğ‘¤), it follows that \n{ğ‘¢, ğ‘£} âŠ‚ğ‘‘ğ‘’(ğ‘ğ‘(ğ‘¤)) âŠ‚ğ‘‘ğ‘’(ğ‘ğ‘(ğ‘ğ‘(ğ‘¤))) âŠ‚â‹¯âŠ‚ğ‘‘ğ‘’(ğ‘Ÿğ‘œğ‘œğ‘¡). This says that all the sub-\nsequent elements of ğ‘ƒ are also in ğ‘‹. So ğ‘‹ consists of the nodes on the reverse-directed \npath from ğ‘¤ to the root. This shows the existence and uniqueness of the node that has \nthe first two properties.  \nAs for the third property, let the unique path from ğ‘¢ to ğ‘¤ be (ğ‘¢, ğ‘¢1, â€¦ , ğ‘¢ğ‘˜, ğ‘¤) and the \nunique path from ğ‘£ to ğ‘¤ be (ğ‘£, ğ‘£1, â€¦ , ğ‘£ğ‘™, ğ‘¤). None of the ğ‘¢ğ‘–â€™s can be the same as any of \nthe ğ‘£ğ‘—â€™s, because if ğ‘¢ğ‘–= ğ‘£ğ‘— then this node would be ğ‘¤. So the unique path from ğ‘¢ to ğ‘£ is \n(ğ‘¢, ğ‘¢1, â€¦ , ğ‘¢ğ‘˜, ğ‘¤, ğ‘£ğ‘™, â€¦ , ğ‘£1, ğ‘£), and this contains ğ‘¤. ï‚¨ \n\n9 Local moves in forests and trees \n \n 78 \nAn alternative way of proving Proposition 9.3 is by noting that for any three nodes in a \ntree, the three paths between them have exactly one node in common. If the three nodes \nare taken to be ğ‘¢, ğ‘£, and the root, then the node that the paths have in common is ğ‘¤.  \nProposition 9.4. Let ğ‘£ be a node in a rooted tree. If you reverse all the edges on the path \nfrom the root to ğ‘£, the result is a rooted tree with ğ‘£ as its root. \nProof. A tree is rooted at ğ‘Ÿ if and only if, for all ğ‘¢âˆˆğ‘‰, there is a directed path from ğ‘Ÿ to ğ‘¢. \nConsider any node ğ‘¢âˆˆğ‘‰, and let ğ‘¤ be the youngest common ancestor of ğ‘¢ and ğ‘£ \n(where ğ‘£ is the node mentioned in the proposition). All the ancestors of ğ‘£ lie on the \npath from ğ‘Ÿ to ğ‘£, so ğ‘¤ must lie on this path. After the edges are reversed, there is a new \ndirected path from ğ‘£ to ğ‘¤, and the old directed path from ğ‘¤ to ğ‘¢ is still there. So there is \na directed path from ğ‘£ to ğ‘¢, which means the new graph is a rooted tree with root ğ‘£. All \nthese statements still hold if any two or more of ğ‘Ÿ, ğ‘¢, ğ‘£, and ğ‘¤ are equal, or if two \ndifferent pairs of them are equal. ï‚¨  \nProposition 9.5. Suppose ğµ= (ğ‘‰ğµ, ğ¸ğµ) is a rooted tree with root ğ‘, ğ¶= (ğ‘‰ğ¶, ğ¸ğ¶) is a \nrooted tree with root ğ‘, ğ‘‰ğµâˆ©ğ‘‰ğ¶= âˆ…, and ğ‘£âˆˆğ‘‰ğµ. Let ğ· be the graph formed by \ncombining ğµ and ğ¶ and adding the edge (ğ‘£, ğ‘), so ğ·= (ğ‘‰ğµâˆªğ‘‰ğ¶, ğ¸ğµâˆªğ¸ğ¶âˆª(ğ‘£, ğ‘)). Then \nğ· is a rooted tree with root ğ‘.  \nProof. A node ğ‘¢ in ğ· is either in ğ‘‰ğµ or in ğ‘‰ğ¶. If ğ‘¢âˆˆğ‘‰ğµ, then there is obviously a unique \ndirected path from ğ‘ to ğ‘¢ in ğ·, because ğµ is a rooted tree. If ğ‘¢âˆˆğ‘‰ğ¶, then a directed path \nfrom ğ‘ to ğ‘¢ in ğ· can be formed by combining, in order, the directed path from ğ‘ to ğ‘£ \n(which exists because ğµ is a rooted tree), the edge (ğ‘£, ğ‘), and the directed path from ğ‘ \nto ğ‘¢ (which exists because ğ¶ is a rooted tree). ï‚¨ \n9.3 \nThe system for storing a forest  \nThe purpose of the system  \nThis section describes how to store a forest in such a way that it is easy to choose an \nedge-removal or edge-addition move uniformly at random from all the possible moves. \nFor this it is necessary to have available the set of edges that can be added and the set of \nedges that can be removed, so that one of these can be chosen uniformly at random. Any \nedge can be removed. The non-trivial issue is which edges can be added. This requires \nknowing whether two nodes are in the same connected component.  \nEach component of the forest is regarded as a rooted tree. There are three algorithms. \nAlgorithm V is for storing a forest and checking that it is a forest, Algorithm VI is for \nadding an edge, and Algorithm VII is for removing an edge.  \nReversing an edge is not a possible move, because this would violate the condition that \nthe components are rooted (in all cases except components with two nodes). Moreover, \nthe directions are only for computational convenience, so reversing an edge would not \nchange the graphical model.  \n\n \n9.3 The system for storing a forest \n \n79 \nWhat is stored \nâ€¢ \nThe ğ‘ nodes. Each node ğ‘£ stores references to its parent, ğ‘ğ‘(ğ‘£), and its children, \nğ‘â„(ğ‘£). (Some nodes do not have a parent, and some nodes have no children.)  \nâ€¢ \nA partition of the nodes into connected components.  \nâ€¢ \nA partition of the edges into the three parts ğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘”, ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’, and ğ‘›ğ‘œğ‘›ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’.  \nâ€¢ \nThe bit-pattern that constitutes the lower triangle of the adjacency matrix. \nThe bit-pattern is the most compact way to store a graph. The method for updating it is \ntrivial, so this is omitted from the algorithms. The point of storing the bit-pattern is that \nthe user will probably want to keep a record of some or all of the graphs that are visited. \nFor this it is not necessary to have all the detailed information about parents, children, \nand partitions, so just the bit-pattern can be used.  \nTo choose a move uniformly at random from among all the possible moves, simply \nchoose an edge uniformly at random from ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’âˆªğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘”. If the edge is in ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’, \ndo Algorithm VI, and if it is in ğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘”, do Algorithm VII.  \n \nAlgorithm V: store a forest G(V,E), and check that it is a forest \nSet the nodesâ€™ parents and children, create the \npartition of the nodes, and check that it is a forest: \n \n1. ğ‘¢ğ‘›ğ‘‘ğ‘–ğ‘ ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘’ğ‘‘â†ğ‘‰  \n \n2. ğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡â†âˆ…  \nThis will be the set of nodes that have \nbeen discovered but not dealt with.  \n3. Do \nEach iteration of this loop will deal \nwith a new connected component. \n4.   \nMove an arbitrary node ğ‘Ÿğ‘œğ‘œğ‘¡ from \n \nğ‘¢ğ‘›ğ‘‘ğ‘–ğ‘ ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘’ğ‘‘ to ğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡. \n5.  \nğ‘ğ‘(ğ‘Ÿğ‘œğ‘œğ‘¡) â†ğ‘›ğ‘¢ğ‘™ğ‘™  \nThis indicates that ğ‘Ÿğ‘œğ‘œğ‘¡ has no parent. \n6.  \nIn the node partition, create a new part \n \ncalled ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘ƒğ‘ğ‘Ÿğ‘¡, and add ğ‘Ÿğ‘œğ‘œğ‘¡ to it. \n \n7.  \nWhile ğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡â‰ âˆ… \nThis loop does breadth-first search of \nthe component. Each iteration visits \n(deals with) one node, namely \nğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡.  \n8.  \n \nRemove an arbitrary node ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ from \n \n \nğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡. \n9.  \n \nğ‘â„(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) â†ğ‘›ğ‘’(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) âˆ– \n \n \nğ‘ğ‘(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) \nTo find ğ‘›ğ‘’(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡), use the adjacency \nmatrix ğ´. If ğ‘ğ‘(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) is ğ‘›ğ‘¢ğ‘™ğ‘™, \nregard it as âˆ….  \n10.  \n \nIf ğ‘â„(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) âŠˆğ‘¢ğ‘›ğ‘‘ğ‘–ğ‘ ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘’ğ‘‘, it is  \n \n \nnot a forest; exit. \n11.  \n \nFor each node ğ‘â„ğ‘–ğ‘™ğ‘‘âˆˆğ‘â„(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡), \n \n \n12.  \n \n \nğ‘ğ‘(ğ‘â„ğ‘–ğ‘™ğ‘‘) â†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡.  \n \n13.   \n \nMove the nodes in ğ‘â„(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) from \n \n \nğ‘¢ğ‘›ğ‘‘ğ‘–ğ‘ ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘’ğ‘‘ to ğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡. \n \n14.   \n \nAdd the nodes in ğ‘â„(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) to  \n \n \nğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘ƒğ‘ğ‘Ÿğ‘¡. \n \n15. Until ğ‘¢ğ‘›ğ‘‘ğ‘–ğ‘ ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘’ğ‘‘= âˆ…. \n \n\n9 Local moves in forests and trees \n \n 80 \nCreate the edge partition: \n \n16. Create the edge partition, with the three parts \nğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘”, ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’, and ğ‘›ğ‘œğ‘›ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’. \n \n17. For each pair of nodes (ğ‘¢, ğ‘£) \n \n18.   \nif ğ´ğ‘¢ğ‘£= 1, put (ğ‘¢, ğ‘£) in ğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘” \n \n19.   \nelse if ğ‘¢ and ğ‘£ are in the same part of the \n \nnode partition, put (ğ‘¢, ğ‘£) in ğ‘›ğ‘œğ‘›ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’ \n \n20.   \nelse put (ğ‘¢, ğ‘£) in ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’.  \n \n \nAlgorithm VI: add an edge (u,v)  \nSee Figure 9.1, in which ğ‘¢ is â‘¡ and ğ‘£ is â‘ . When the new edge is added, it needs to be \ngiven a direction. Suppose it is directed from ğ‘£ to ğ‘¢. In the original graph before the \nedge is added, let ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘ be the component that contains ğ‘£ and ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ be the \ncomponent that contains ğ‘¢. In Figure 9.1, ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘ is {â‘§â‘ªâ‘ â‘¢â‘¤} and ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ \nis {â‘¨â‘¡â‘¥â‘¦}. When the new edge is added, ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘ and ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ combine to \nform a new component.  \nProposition 9.6. If the edges on the path from ğ‘¢ up to the root of ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ are \nreversed, then the new component will be a rooted tree.  \nProof. By Proposition 9.4, reversing the edges on the path from ğ‘¢ to the root of \nğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ will make ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ be a rooted tree with root ğ‘¢. By Proposition 9.5, if \nyou combine ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘ and ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ and add the directed edge (ğ‘£, ğ‘¢), the result is a \nrooted tree. ï‚¨  \nSo to update the edge-directions, all that is necessary is to reverse the edges from ğ‘¢ up \nto the root of ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘. The only nodes whose parents or children change are ğ‘¢, ğ‘£, \nand the nodes on the path from ğ‘¢ to the root of ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘. The parents and children \nof the nodes on this path are updated in the loop in lines 4â€“11, and the children of ğ‘£ are \nupdated in line 12.  \nThe new edge changes to ğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘”. This edge is changed to ğ‘›ğ‘œğ‘›ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’ in one iteration \nof the nested loops in lines 13â€“15 and then changed to ğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘” in line 16. All the other \npossible edges between the two components change from ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’ to ğ‘›ğ‘œğ‘›ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’. This \nis done in lines 13â€“15. The update of the node partition is obvious and is done in line 17.  \n \n \n\n \n9.3 The system for storing a forest \n \n81 \nFigure 9.1. Algorithm VI, for adding an edge. The new edge can be oriented either way. To \nupdate the parents and children, go up from the new child â‘¡ to the root â‘¨ while reversing the \narrows. To update the node partition, move all the nodes from the part that contains the new \nchild, â‘¡, to the part that contains the new parent, â‘ . The changes are enclosed by the thick line.  \n \nAlgorithm VI: add an edge (u,v) \nCheck that adding the edge does not create a \ncycle: \n \n1. Check that (ğ‘¢, ğ‘£) âˆˆğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’.  \n \nUpdate the nodesâ€™ parents and children: \n \n2. ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡â†ğ‘¢ \nğ‘£ will be the parent of ğ‘¢. \n3. ğ‘ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘ â†ğ‘£ \n \n4. Do \nThis loop goes â€œupâ€ from ğ‘¢ and reverses all \nthe arrows. Each iteration deals with one \nnode, ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡.  \n5.  \nUnless this is the first iteration, \n \nremove ğ‘ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘  from ğ‘â„(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡). \n6.  \nğ‘›ğ‘’ğ‘¥ğ‘¡â†ğ‘ğ‘(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) \n \n7.  \nğ‘ğ‘(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) â†ğ‘ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘  \n \n8.  \nIf ğ‘›ğ‘’ğ‘¥ğ‘¡= ğ‘›ğ‘¢ğ‘™ğ‘™, break from the loop. \nThe former root must have been reached.  \n9.  \nAdd ğ‘›ğ‘’ğ‘¥ğ‘¡ to ğ‘â„(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡). \n \n10.  \nğ‘ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘ â†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ \n \n11.  \nğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡â†ğ‘›ğ‘’ğ‘¥ğ‘¡ \n \n12. Add ğ‘¢ to ğ‘â„(ğ‘£).  \n \nUpdate the edge partition: \n \n13. For each node ğ‘¤ in the same part as ğ‘¢ \n \n14.  \nFor each node ğ‘¥ in the same part as ğ‘£ \n \n15.  \n  \nMove (ğ‘¤, ğ‘¥) from ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’ to  \n \n  \nğ‘›ğ‘œğ‘›ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’. \n \n16. Move (ğ‘¢, ğ‘£) from ğ‘›ğ‘œğ‘›ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’ to ğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘”.   \nUpdate the node partition: \n \n17. Move all the nodes in ğ‘¢â€™s part to ğ‘£â€™s part.  \n \n \nAlgorithm VII: remove an edge (u,v) \nThe first step in removing the edge (ğ‘¢, ğ‘£) is to rename ğ‘¢ and ğ‘£ as ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ and ğ‘â„ğ‘–ğ‘™ğ‘‘, in \nthe appropriate order. See Figure 9.2, in which ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ is â‘§ and ğ‘â„ğ‘–ğ‘™ğ‘‘ is â‘ª. None of the \nedge-directions will change. The only nodes whose parents or children change are \nğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ and ğ‘â„ğ‘–ğ‘™ğ‘‘. This is done in lines 4â€“5. The node partition is identified by â€œfanning \ndownâ€ from ğ‘â„ğ‘–ğ‘™ğ‘‘ to identify its new connected component. This is a breadth-first \nsearch and is done in lines 7â€“11.  \nThe edge that is removed changes from ğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘” to ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’. All the edges between the \ntwo new components change from ğ‘›ğ‘œğ‘›ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’ to ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’. These updates are done in \nlines 12â€“15.  \n\n9 Local moves in forests and trees \n \n 82 \n \n \nFigure 9.2. Algorithm VII, for removing an edge. To update the node partition, fan down from the \nnewly orphaned node, â‘ª, to all its descendants, and move all these nodes to a new part. The \nchanges are enclosed by the thick line.  \n \nAlgorithm VII: remove an edge (u,v)  \nCheck that the edge can be removed: \n \n1. Check that (ğ‘¢, ğ‘£) âˆˆğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘”.  \n \nUpdate the nodesâ€™ parents and children:  \n \n2. If ğ‘ğ‘(ğ‘¢) = ğ‘£,  \n \nğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡â†ğ‘£; ğ‘â„ğ‘–ğ‘™ğ‘‘â†ğ‘¢ \nFind which is the parent and which \nis the child.  \n3. else  \n \nğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡â†ğ‘¢; ğ‘â„ğ‘–ğ‘™ğ‘‘â†ğ‘£ \n4. ğ‘ğ‘(ğ‘â„ğ‘–ğ‘™ğ‘‘) â†ğ‘›ğ‘¢ğ‘™ğ‘™ \n \n5. Remove ğ‘â„ğ‘–ğ‘™ğ‘‘ from ğ‘â„(ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡). \n \nUpdate the node partition:  \n \n6. Move ğ‘â„ğ‘–ğ‘™ğ‘‘ to a new part, ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘¡ \n \n7. ğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡â†{ğ‘â„ğ‘–ğ‘™ğ‘‘} \n \n8. While ğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡â‰ âˆ…  \nThis loop â€œfans downâ€ from ğ‘â„ğ‘–ğ‘™ğ‘‘ to \nfind all its descendants and put \nthem in ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘¡.  \n9.  \nRemove an arbitrary node ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ from \n \nğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡.  \n10.   \nAdd ğ‘â„(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) to ğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡. \n11.   \nMove all of ğ‘â„(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) to ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘¡. \nUpdate the edge partition: \n \n12. Move (ğ‘¢, ğ‘£) from ğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘” to ğ‘›ğ‘œğ‘›ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’. \nThis is temporary.  \n13. For each node ğ‘¢ in ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘¡ \n \n14.   \nFor each node ğ‘£ in the same part as ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ \n \n15.   \n  \nMove (ğ‘¢, ğ‘£) from ğ‘›ğ‘œğ‘›ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’ to ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’ \n \n \n\n \n9.4 The system for storing a tree \n \n83 \n9.4 \nThe system for storing a tree  \nThe purpose of the system \nThis section describes how to store and update a tree in such a way that it is easy to \nselect edge-moves uniformly at random from among all the possible edge-moves. \nAlgorithm VIII is for storing a tree and checking that it is a tree, and Algorithm IX is for \nchoosing an edge-move uniformly at random and then updating the information that is \nstored.  \nChoosing an edge-move uniformly at random \nChoosing an edge-move consists of choosing an edge, removing it, and then choosing \nwhere to reinsert it. If at the initial step you choose the edge uniformly at random from \namong all the existing edges, then not all edge-moves will be equally likely. Consider the \ntree in Figure 9.3. If you choose and remove edge A, then there are 6 places it can be \nreinserted (while making sure the graph is still a tree); so choosing A is the first step in \n6 possible edge-moves. But if you choose and remove edge B, there are 4 Ã— 3 = 12 \nplaces it can be reinserted; choosing B is the first step in 12 possible edge-moves. To \nchoose the edge-move uniformly at random, you need to be twice as likely to choose B \nas to choose ğ´.  \n \n \nFigure 9.3. A tree. To choose an edge-move uniformly at random from among all the possible \nedge-moves, you need to have â„™(choose edge ğµ) = 2â„™(choose edge ğ´). \n \nIn general, to be able to choose an edge-move uniformly at random, it is necessary to \nknow for each edge the sizes of the two connected components that would result from \nremoving that edge. The most convenient way to store this information is by assigning a \nâ€œweightâ€ to each node except the root. The weight of each node is the number of its \ndescendants plus one and will be denoted by ğ‘Š(â‹…). If the edge between ğ‘£ and ğ‘ğ‘(ğ‘£) is \nremoved, then the sizes of the two connected components are ğ‘Š(ğ‘£) and ğ‘âˆ’ğ‘Š(ğ‘£), and \nthe edge can be reinserted in any of ğ‘Š(ğ‘£) Ã— (ğ‘âˆ’ğ‘Š(ğ‘£)) possible places.  \nSo the procedure to choose an edge-move uniformly at random is as follows. Choose a \nnode at random, with the probability of node ğ‘£ being proportional to ğ‘Š(ğ‘£) Ã— (ğ‘âˆ’\nğ‘Š(ğ‘£)). Remove the edge between ğ‘£ and ğ‘ğ‘(ğ‘£). From each of the two connected \ncomponents, choose one node uniformly at random. Finally, reinsert the edge between \nthese two nodes.  \n\n9 Local moves in forests and trees \n \n 84 \nThe most complicated part of this system is the updates of the node-weights after the \nedge-move. This is the main work of Algorithm IX.  \nAfter an edge is removed, the graph consists of two connected components. Hereafter \nthe component that contains the root will be called ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘ and the other component \nwill be called ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘.  \nUsing uniformly chosen edge-moves \nChoosing an edge-move uniformly at random could be useful in a tree version of Giudici \n& Green (1999)â€™s MCMC method or Jones et al (2005)â€™s stochastic search method. In \none form of the latter method, all the neighbouring graphs are analyzed, so there is no \nneed to choose edge-moves uniformly at random. (It would still be convenient to store \nthe tree as a rooted tree, but there is no need for the weights.) In the general form, only \nsome of them are analyzed. It is easy to adapt Algorithm IX to produce not just one but \nany number of edge-moves uniformly at random from among all the possible edge-\nmoves.  \nThe idea of choosing edge-moves uniformly at random is that this may give better \nmixing among the possible graphs. For example, if you remove an edge that includes a \nleaf (a node of degree 1), then that node will definitely still be a leaf after the edge is \nreinserted. Section 11.1 will show that if you choose the edge to move uniformly at \nrandom, rather than choose the edge-move uniformly at random, then leaves are more \nlikely to remain leaves. Section 11.2 presents the results of experiments to see when \nchoosing edge-moves uniformly at random is beneficial.  \nTwo slightly different versions of the system \nWith the system described above, there is positive probability that the edge will be \nreinserted in the same place as it was removed from, so the â€œedge-moveâ€ will consist of \nthe graph staying the same. It is easy to adapt the method and the algorithms to avoid \nthis. When you calculate the probabilities, use ğ‘Š(ğ‘£) Ã— (ğ‘âˆ’ğ‘Š(ğ‘£)) âˆ’1 instead of \nğ‘Š(ğ‘£) Ã— (ğ‘âˆ’ğ‘Š(ğ‘£)), and when deciding where to reinsert the edge, exclude the original \nposition of the edge. (In the computer experiments in chapter 11, I use this adapted \nversion.)  \nIt would also be possible to store the weights on the edges rather than the nodes. The \nweight of each edge would be the number of nodes that are â€œdownstreamâ€ of itâ€”that is, \non the same side of the edge as the child. To convert from node-weights to edge-weights, \nfrom each node ğ‘£ remove ğ‘Š(ğ‘£) and put it on (ğ‘£, ğ‘ğ‘(ğ‘£)) instead. To choose an edge-\nmove uniformly at random, choose edge ğ‘’ with probability proportional to ğ‘Š(ğ‘’) Ã— (ğ‘âˆ’\nğ‘Š(ğ‘’)).  \nWith weights on the edges, there is no need to treat the root as a special case. The \nupdates of the weights in ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘ are also simpler to describe, since there is no need \nto talk about a path to â€œjust beforeâ€ ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ. Overall, using edge-weights is \nmore natural than using node-weights. But the differences are trivial, and with each \nnode you already have to store the parents and children, so in practice it is simpler to \nuse node-weights.  \n\n \n9.4 The system for storing a tree \n \n85 \nWhat is stored \nâ€¢ \nThe ğ‘ nodes. The tree is regarded as a rooted tree. Each node ğ‘£ stores references to \nits parent, ğ‘ğ‘(ğ‘£), and its children, ğ‘â„(ğ‘£). The first node is the root and has no \nparent; some nodes have no children. The root never changes.  \nâ€¢ \nEach node except for the root also stores a weight, which is the number of its \ndescendants plus one.  \nâ€¢ \nThe bit-pattern that constitutes the lower triangle of the adjacency matrix. As with \nforests, the storing and updating of the bit-pattern are omitted from the algorithms.  \nAlgorithm VIII: store a tree, and check that it is a tree \nThe loop in lines 6â€“13 of Algorithm VIII is exactly the same as the inner loop of \nAlgorithm V (lines 7â€“14). It fans down from ğ‘Ÿğ‘œğ‘œğ‘¡ to identify one connected component \nof the graph. This time there are two ways the graph could fail to be a tree. Firstly, it \nmight have cycles, which is tested in line 9. Secondly, this one component might not \ninclude all the nodes, which is tested in line 14. \n \nAlgorithm VIII: store a tree, and check that it is a tree \nSet the nodesâ€™ parents and children and check that it is a \ntree: \n \n1. ğ‘¢ğ‘›ğ‘‘ğ‘–ğ‘ ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘’ğ‘‘â†ğ‘‰ \n \n2. ğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡â†âˆ… \n \n3. Move an arbitrary node ğ‘Ÿğ‘œğ‘œğ‘¡ from ğ‘¢ğ‘›ğ‘‘ğ‘–ğ‘ ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘’ğ‘‘ to \nğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡. \n \n4. ğ‘ğ‘(ğ‘Ÿğ‘œğ‘œğ‘¡) â†ğ‘›ğ‘¢ğ‘™ğ‘™ \n \n5. ğ‘›ğ‘œğ‘‘ğ‘’ğ‘ ğ¹ğ‘œğ‘¢ğ‘›ğ‘‘â†1 \n \n6. While ğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡â‰ âˆ… \nThis loop fans down from ğ‘Ÿğ‘œğ‘œğ‘¡ to \nall its descendants, which hope-\nfully means the entire graph.  \n7.   \nRemove an arbitrary node ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ from ğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡ \n8.   \nğ‘â„(ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡) â†ğ‘›ğ‘’(ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡) âˆ–ğ‘ğ‘(ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡) \nTo find ğ‘›ğ‘’(ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡), use ğ´. \n9.   \nIf ğ‘â„(ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡) âŠˆğ‘¢ğ‘›ğ‘‘ğ‘–ğ‘ ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘’ğ‘‘, it is not a tree; \n \nexit. \nThis tests for cycles.  \n10.   \nFor each node ğ‘â„ğ‘–ğ‘™ğ‘‘âˆˆğ‘â„(ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡) \n \n11.   \n  \nğ‘ğ‘(ğ‘â„ğ‘–ğ‘™ğ‘‘) â†ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ \n \n12.   \nMove ğ‘â„(ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡) from ğ‘¢ğ‘›ğ‘‘ğ‘–ğ‘ ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘’ğ‘‘ to \n \nğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡. \n \n13.   \nğ‘›ğ‘œğ‘‘ğ‘’ğ‘ ğ¹ğ‘œğ‘¢ğ‘›ğ‘‘â†ğ‘›ğ‘œğ‘‘ğ‘’ğ‘ ğ¹ğ‘œğ‘¢ğ‘›ğ‘‘+ |ğ‘â„(ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡)| \n \n14. If ğ‘›ğ‘œğ‘‘ğ‘’ğ‘ ğ¹ğ‘œğ‘¢ğ‘›ğ‘‘â‰ ğ‘, it is not a tree; exit.  \nThis tests whether all the nodes \nhave been found.  \nCalculate the node-weights: \n \n15. ğ‘“ğ‘–ğ‘›ğ‘‘ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡(ğ‘Ÿğ‘œğ‘œğ‘¡) \nThe subroutine ğ‘“ğ‘–ğ‘›ğ‘‘ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ is \nimmediately below. \n16. Discard ğ‘Š(ğ‘Ÿğ‘œğ‘œğ‘¡) \nRecursive subroutine ğ’‡ğ’Šğ’ğ’…ğ‘¾ğ’†ğ’Šğ’ˆğ’‰ğ’•(ğ’—): \n \ni. \nğ‘Š(ğ‘£) â†1 \nThis 1 counts the node itself.  \n\n9 Local moves in forests and trees \n \n 86 \nii. For each node ğ‘â„ğ‘–ğ‘™ğ‘‘âˆˆğ‘â„(ğ‘£) \nThis loop calculates ğ‘Š(ğ‘£) and \nensures that the weights of ğ‘£â€™s \nchildren will be calculated.  \niii.  \nğ‘Š(ğ‘£) â†ğ‘Š(ğ‘£) + ğ‘“ğ‘–ğ‘›ğ‘‘ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡(ğ‘â„ğ‘–ğ‘™ğ‘‘) \niv. Return ğ‘Š(ğ‘£) \n \nNotation for Algorithm IX \nAlgorithm IX chooses and makes an edge-move, and updates the edge-directions and \nnode-weights as necessary. Suppose the edge is moved from (ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘) to \n(ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘). Of course it is possible that ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡= ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ or \nğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘= ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘.  \nAfter (ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘) is removed, the graph has two connected components. Call \nthe component that contains the root ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘ and the other component ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘. \n(The nodes in ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ might actually be â€œolderâ€ on average than the nodes in \nğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘, if the â€œageâ€ of a node is such that each parent is 1 older than its children, but \nthis does not matter.)  \nTo preserve the rootedness of the tree, the direction of the new edge has to be from \nğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘ to ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘. So ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡âˆˆğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘ and ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘âˆˆğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘.  \nFacts used by Algorithm IX \nFigure 9.4 shows a typical edge-move and how the node-weights and edge-directions \nchange. This is intended to give an intuitive understanding of the propositions in this \nsection and how Algorithm IX works.  \nFigure 9.5 shows the eight different possibilities for the relative positions of ğ‘Ÿğ‘œğ‘œğ‘¡, \nğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, and ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ in ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘. For example, if ğ‘Ÿğ‘œğ‘œğ‘¡ is on the path between \nğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ and ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, then ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘ looks like (c) in Figure 9.5; the configuration \nin Figure 9.4 is a special case of (d) in Figure 9.5.  \nThe propositions and proofs below all hold completely generally, whichever of the \npossibilities in Figure 9.5 holds for ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘, and even if ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘= ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘. For \nexample, if ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘= ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘ then Proposition 9.7 simply states that no edge-\ndirections need to be changed.  \nRecall that because the graph is a tree, the path between any two nodes is unique.  \nProposition 9.7. If the edges on the path between ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘ and ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘ are reversed, \nthe graph that results will be a rooted tree whose root is the root of ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘.  \nProof. First note that ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘ must be the root of ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘. This is because \nğ‘ğ‘(ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘) used to be ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, but the edge (ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘) has been \nremoved; so in ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘, ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘ has no parent, which means it must be the root. \nThe proof then follows from Proposition 9.4 and Proposition 9.5 in the same way that \nProposition 9.6 does. Reversing the edges makes ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ into a tree rooted at \nğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘, \nand \nrelinking ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘ and ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ with \nthe \nnew \nedge \n(ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘) then makes a rooted tree whose root is the root of ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘. ï‚¨ \nProposition 9.8. In ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘, the only nodes whose weights can possibly change are the \nones on the path between ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ and ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡.  \n\n \n9.4 The system for storing a tree \n \n87 \nProof. For conciseness I will sometimes regard a path as a set of nodes rather than a \nsequence. The proof will consist of gradually narrowing down the set of nodes whose \nweights can possibly change. Let ğ‘£âˆˆğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘. The weight of ğ‘£ only changes if ğ‘‘ğ‘’(ğ‘£) \nchanges, where ğ‘‘ğ‘’(ğ‘£) = {ğ‘¢âˆˆğ‘‰: there exists a directed path from ğ‘£ to ğ‘¢}. Let ğ‘‘ğ‘’ğ‘œğ‘™ğ‘‘(ğ‘£) \nbe the descendants of ğ‘£ in the old graph, before the edge is moved, and ğ‘‘ğ‘’ğ‘›ğ‘’ğ‘¤(ğ‘£) be its \ndescendants in the new graph, after the edge is moved. Note that ğ‘ğ‘›(ğ‘£) does not change \nwhen the edge is moved, so there is no need for any subscript on it.  \nIt \nis \nsufficient \nto \nconsider \nnodes \nin  ğ¹= {ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡} âˆªğ‘ğ‘›(ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡) âˆª  \n{ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡} âˆªğ‘ğ‘›(ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡), since ğ‘‘ğ‘’ğ‘œğ‘™ğ‘‘(ğ‘£) â‰ ğ‘‘ğ‘’ğ‘›ğ‘’ğ‘¤(ğ‘£) is only possible if ğ‘£âˆˆğ¹. To \nsee this, first note that ğ‘‘ğ‘’ğ‘œğ‘™ğ‘‘(ğ‘£) â‰ ğ‘‘ğ‘’ğ‘›ğ‘’ğ‘¤(ğ‘£) means there exists some ğ‘¢ such that either \nğ‘¢âˆˆğ‘‘ğ‘’ğ‘œğ‘™ğ‘‘(ğ‘£) and ğ‘¢âˆ‰ğ‘‘ğ‘’ğ‘›ğ‘’ğ‘¤(ğ‘£) or ğ‘¢âˆˆğ‘‘ğ‘’ğ‘›ğ‘’ğ‘¤(ğ‘£) and ğ‘¢âˆ‰ğ‘‘ğ‘’ğ‘œğ‘™ğ‘‘(ğ‘£). If the former holds, \nthen the directed path from ğ‘£ to ğ‘¢ in the old graph must include the edge \n(ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘), which implies that ğ‘£âˆˆ{ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡} âˆªğ‘ğ‘›(ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡). If the \nlatter holds, then the directed path from ğ‘£ to ğ‘¢ in the new graph must include \n(ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘) , which implies that ğ‘£âˆˆ{ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡} âˆªğ‘ğ‘›(ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡) . \nCombining these two possibilities shows that ğ‘‘ğ‘’ğ‘œğ‘™ğ‘‘(ğ‘£) can only differ from ğ‘‘ğ‘’ğ‘›ğ‘’ğ‘¤(ğ‘£) if \nğ‘£âˆˆğ¹.  \nHowever, the proposition makes no claim about ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ or ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ themselves. \nSo it is sufficient to consider nodes in ğº= ğ‘ğ‘›(ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡) âˆªğ‘ğ‘›(ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡). Let \nğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ be the youngest common ancestor of ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ and ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡. By \nProposition 9.3, ğ‘ğ‘›(ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡) consists of ğ‘ğ‘›(ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ) and the directed \npath from ğ‘¤ to ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, and ğ‘ğ‘›(ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡) consists of ğ‘ğ‘›(ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ) and \nthe directed path from ğ‘¤ to ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡. So ğº is the union of ğ‘ğ‘›(ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ), the \npath from ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ to ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, and the path from ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ to \nğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡. The union of these two paths is the path from ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ to ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ \n(this holds even if ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ= ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ or ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡). So ğº is the union of \nğ‘ğ‘›(ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ) and the path from ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ to ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡.  \nTo prove the proposition it therefore suffices to check that if ğ‘£âˆˆğ‘ğ‘›(ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ) \nthen ğ‘‘ğ‘’ğ‘œğ‘™ğ‘‘(ğ‘£) = ğ‘‘ğ‘’ğ‘›ğ‘’ğ‘¤(ğ‘£). Suppose ğ‘£âˆˆğ‘ğ‘›(ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ). None of the ancestorâ€“\ndescendant relationships in ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘ change when the edge is moved, so ğ‘‘ğ‘’ğ‘œğ‘™ğ‘‘(ğ‘£) âˆ©\nğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘= ğ‘‘ğ‘’ğ‘›ğ‘’ğ‘¤(ğ‘£) âˆ©ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘.  \nIf ğ‘¢âˆˆğ‘‘ğ‘’ğ‘œğ‘™ğ‘‘(ğ‘£) âˆ©ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘, then in the old graph there must be a directed path \n(ğ‘£, â€¦ , ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ, â€¦ , ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘, â€¦ , ğ‘¢). Here it is possible for any of \nthe ellipses to signify no nodes (for example, if ğ‘£= ğ‘ğ‘(ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ) then the first \nellipsis disappears); it is even possible that ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ= ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ or \nğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘= ğ‘¢, in which case the path â€œcollapsesâ€ in the obvious way; but it is not \npossible that ğ‘£= ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ. By Proposition 9.3 there is a directed path from \nğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ to ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, and by Proposition 9.7 there is a directed path in the \nnew graph from ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘ to ğ‘¢. So in the new graph there is a directed path \n(ğ‘£, â€¦ ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ, â€¦ , ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘, â€¦ , ğ‘¢), in which similar â€œcollapsingsâ€ \nare possible. The existence of this path shows that ğ‘¢âˆˆğ‘‘ğ‘’ğ‘›ğ‘’ğ‘¤(ğ‘£) âˆ©ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘. A \nsimilar argument shows the converse, that if ğ‘¢âˆˆğ‘‘ğ‘’ğ‘›ğ‘’ğ‘¤(ğ‘£) âˆ©ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ then ğ‘¢âˆˆ\nğ‘‘ğ‘’ğ‘œğ‘™ğ‘‘(ğ‘£) âˆ©ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘. \nTherefore ğ‘‘ğ‘’ğ‘œğ‘™ğ‘‘(ğ‘£) âˆ©ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘= ğ‘‘ğ‘’ğ‘›ğ‘’ğ‘¤(ğ‘£) âˆ©ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”- \nğ¶ğ‘œğ‘šğ‘. Putting this together with ğ‘‘ğ‘’ğ‘œğ‘™ğ‘‘(ğ‘£) âˆ©ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘= ğ‘‘ğ‘’ğ‘›ğ‘’ğ‘¤(ğ‘£) âˆ©ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘ shows \nthat ğ‘‘ğ‘’ğ‘œğ‘™ğ‘‘(ğ‘£) = ğ‘‘ğ‘’ğ‘›ğ‘’ğ‘¤(ğ‘£). ï‚¨  \n\n9 Local moves in forests and trees \n \n 88 \nProposition 9.9. In ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘, the only nodes whose weights can possibly change are \nthe ones on the path between ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘ and ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘.  \nProof. The nodes that are not on the path between ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘ and ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘ are all on the \nends of arrows that emanate from nodes on that path, or in tree structures on the ends \nof these arrows. The descendants of these nodes consist entirely of other nodes in these \ntree structures, and these sets of descendants do not change when the edge is moved. ï‚¨  \nFor an illustration of Propositions 9.8 and 9.9, see Figure 9.4, in which ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ is \nthe right part of the two graphs and the nodes that are not on the paths mentioned in \nthe propositions are white.  \nProposition 9.10. The weight of ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘ changes to |ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘|. For the other \nnodes on the path from ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘ to ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘, the weight changes to |ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘| âˆ’ğ‘¥, \nwhere ğ‘¥ is the original weight of the previous node on this path.  \nProof. After the edge-move there is an edge from ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ to ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘. So all the \nother nodes in ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ must be descendants of ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘, and the weight of \nğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘ is therefore |ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘|. Next consider a node ğ‘£ on the path from \nğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘ to ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘ (for example ğ‘£= â¼ in Figure 9.4). After the edge-directions are \nupdated, the edge going into ğ‘£ comes from the previous node on this path (in this case, \nâ“­). So the descendants of ğ‘£ consist of all the nodes in ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ except for ğ‘£ itself \nand the nodes on the other side of this edge. The number of nodes on the other side of \nthe edge is the original weight of the previous node on the path; call this ğ‘¥. So the new \nweight of ğ‘£ is |ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘| âˆ’ğ‘¥. ï‚¨  \nTo describe the updates for the nodes on the path between ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ and ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, \nit is necessary to split this path into two parts. As before, let ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ be the \nyoungest common ancestor of ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ and ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡. (Figure 9.5 shows the eight \npossibilities for the relative positions of these nodes; the arguments hold in all cases.) \nConsider separately the path from ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ to ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ and the path from \nğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ to ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ.  \nDefinition 9.11. If the path between ğ‘¢ and ğ‘£ is (ğ‘¢, ğ‘¢1, â€¦ , ğ‘¢ğ‘˜, ğ‘£), then the path from ğ‘¢ to \nâ€œjust beforeâ€ ğ‘£ is (ğ‘¢, ğ‘¢1, â€¦ , ğ‘¢ğ‘˜). If ğ‘˜= 1 then this is (ğ‘¢, ğ‘¢1), if the path between ğ‘¢ and ğ‘£ \nis just (ğ‘¢, ğ‘£) then it is (ğ‘¢), and if ğ‘¢= ğ‘£ then it is âˆ….  \nProposition 9.12. For the nodes on the path from ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ to just before \nğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ, the weight increases by |ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘|.  \nProof. Consider a node ğ‘£ on this path (for example, ğ‘£= â“® in Figure 9.4). The nodes in \nğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ are not descendants of ğ‘£ before the move, but they are after. So the weight \nof ğ‘£ increases by |ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘|. ï‚¨ \nProposition 9.13. For the nodes on the path from ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ to just before \nğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ, the weight decreases by |ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘|. \nProof. Consider a node ğ‘£ on this path (for example, ğ‘£= â» in Figure 9.4). The nodes in \nğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ are descendants of ğ‘£ before the move, but not after. So the weight of ğ‘£ \ndecreases by |ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘|. ï‚¨ \nProposition 9.14. The weight of ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ does not change.  \n\n \n9.4 The system for storing a tree \n \n89 \nProof. This node has the same descendants before and after the move. ï‚¨ \nAlgorithm IX: choose and make an edge-move \nLine 1 decides which edge to move, lines 3â€“4 update ğ‘ğ‘(ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘) and ğ‘â„(ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡), \nand lines 5â€“12 choose where to move the edge to. Lines 7â€“9 is a breadth-first search \nthat identifies all the descendants of ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘ and puts them in ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘.  \nLines 16â€“26 traverse the path up from ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘ to ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘, updating the node-\nweights as described in Proposition 9.10 and reversing the arrows. The paths from \nğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ and ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ to ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ cannot immediately be identified. \nLines 27â€“32 identify the path from ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ up to the root. Lines 33â€“36 then go up \nfrom ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ to this path, updating the node-weights according to Proposition 9.12. \nLine 37 identifies ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ. Lines 38â€“41 then go up from ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ to just \nbefore ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ, updating the node-weights according to Proposition 9.13.  \nFinally, lines 42â€“43 update ğ‘â„(ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡) and ğ‘â„(ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡).  \n \nAlgorithm IX: choose and make an edge-move \nChoose which edge to move, and remove it: \n \n1. Choose a node ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘ at random; the probability \nof choosing ğ‘£ is proportional to \n ğ‘Š(ğ‘£) Ã— (ğ‘âˆ’ğ‘Š(ğ‘£)).  \nThe edge to be removed will be \n(ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘, ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡).  \n2. ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡â†ğ‘ğ‘(ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘) \n \n3. ğ‘ğ‘(ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘) â†ğ‘›ğ‘¢ğ‘™ğ‘™ \n \n4. Remove ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘ from ğ‘â„(ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡).  \n \nChoose where to reinsert the edge: \n \n5. ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘â†{ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘} \nğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ will be the component \nthat currently contains ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘.  \n6. ğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡â†{ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘} \n7. While ğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡â‰ âˆ… \nThis loop â€œfans downâ€ from \nğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘ and puts all its descen-\ndants in ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘.  \n8.   \nRemove an arbitrary node ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ from  \n      ğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡.  \n9.   \nPut ğ‘â„(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) in ğ‘¤ğ‘ğ‘–ğ‘¡ğ‘™ğ‘–ğ‘ ğ‘¡ and ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘.  \n10. ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘â†ğ‘‰âˆ–ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ \nğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘ is the component that \ncontains ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ and the \nroot.  \n11. Choose a node ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘ uniformly at random from \nğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘.  \n12. Choose a node ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ uniformly at random \nfrom ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘.  \nThe edge will be reinserted at \n(ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘, ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡). \nTraverse the path up from ğ’ğ’†ğ’˜ğ‘ªğ’‰ğ’Šğ’ğ’… to ğ’ğ’ğ’…ğ‘ªğ’‰ğ’Šğ’ğ’…, \nupdating the node-weights and reversing the arrows: \n \n13. ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡â†ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘ \n \n14. ğ‘ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘ â†ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ \n \n15. ğ‘¥â†0 \nIn the loop, ğ‘¥ will be the former \nweight of the previous node.  \n16. Do \n17.   \nğ‘¡ğ‘’ğ‘šğ‘â†ğ‘Š(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) \nğ‘¡ğ‘’ğ‘šğ‘ is temporary and can be \ndiscarded after line 19.  \n18.   \nğ‘Š(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) â†|ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘| âˆ’ğ‘¥ \n\n9 Local moves in forests and trees \n \n 90 \n19.   \nğ‘¥â†ğ‘¡ğ‘’ğ‘šğ‘ \n \n20.   \nğ‘›ğ‘’ğ‘¥ğ‘¡â†ğ‘ğ‘(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) \n \n21.   \nRemove ğ‘ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘  from ğ‘â„(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) \n \n22.   \nğ‘ğ‘(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) â†ğ‘ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘  \n \n23.   \nIf ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡= ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘, break from the loop.  \n \n24.   \nAdd ğ‘›ğ‘’ğ‘¥ğ‘¡ to ğ‘â„(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡). \n \n25.   \nğ‘ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘ â†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ \n \n26.   \nğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡â†ğ‘›ğ‘’ğ‘¥ğ‘¡ \n \nIdentify the path from ğ’ğ’ğ’…ğ‘·ğ’‚ğ’“ğ’†ğ’ğ’• to the root:  \n \n27. ğ‘ğ‘ğ‘¡â„â†âˆ… \nğ‘ğ‘ğ‘¡â„ does not need to be ordered; \nit can just be an ordinary set.  \n28. ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡â†ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ \n29. Do  \n \n30.   \nAdd ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ to ğ‘ğ‘ğ‘¡â„. \n \n31.   \nğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡â†ğ‘ğ‘(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) \n \n32. Until ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡= ğ‘›ğ‘¢ğ‘™ğ‘™ \n \nGo up from ğ’ğ’†ğ’˜ğ‘·ğ’‚ğ’“ğ’†ğ’ğ’• till just before you meet ğ’‘ğ’‚ğ’•ğ’‰, \nupdating the node-weights along the way:  \n \n33. ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡â†ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ \n \n34. While ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡âˆ‰ğ‘ğ‘ğ‘¡â„ \n \n35.   \nğ‘Š(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) â†ğ‘Š(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) + |ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘| \n \n36.   \nğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡â†ğ‘ğ‘(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) \n \n37. ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿâ†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ \n \nGo up from ğ’ğ’ğ’…ğ‘·ğ’‚ğ’“ğ’†ğ’ğ’• to just before \nğ’„ğ’ğ’ğ’ğ’ğ’ğ‘¨ğ’ğ’„ğ’†ğ’”ğ’•ğ’ğ’“ and update the node-weights:  \n \n38. ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡â†ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ \n \n39. While ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡â‰ ğ‘ğ‘œğ‘šğ‘šğ‘œğ‘›ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ \n \n40.   \nğ‘Š(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) â†ğ‘Š(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) âˆ’|ğ‘¦ğ‘œğ‘¢ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘| \n \n41.   \nğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡â†ğ‘ğ‘(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡) \n \nUpdate the children of ğ’ğ’ğ’…ğ‘·ğ’‚ğ’“ğ’†ğ’ğ’• and ğ’ğ’†ğ’˜ğ‘·ğ’‚ğ’“ğ’†ğ’ğ’•:  \n \n42. Remove ğ‘œğ‘™ğ‘‘ğ¶â„ğ‘–ğ‘™ğ‘‘ from ğ‘â„(ğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡).  \n \n43. Add ğ‘›ğ‘’ğ‘¤ğ¶â„ğ‘–ğ‘™ğ‘‘ to ğ‘â„(ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡).  \n \n \nWhen choosing which edge to move, ğ‘Š(ğ‘£) is not used directly; what is used instead is \nğ‘”(ğ‘Š(ğ‘£)) = ğ‘Š(ğ‘£) Ã— (ğ‘âˆ’ğ‘Š(ğ‘£)). So it might seem better to store ğ‘”(ğ‘Š(ğ‘£)) instead of \nğ‘Š(ğ‘£). However, this is not possible, because ğ‘” is not invertible. Specifically, ğ‘Š(ğ‘£) will \nsometimes get updated to ğ‘Š(ğ‘£) + ğ‘¦, and from ğ‘”(ğ‘Š(ğ‘£)) it is not possible to calculate \nğ‘”(ğ‘Š(ğ‘£) + ğ‘¦).  \n \n\n \n9.4 The system for storing a tree \n \n91 \n \nFigure 9.4. A tree before and after an edge is moved according to Algorithm IX. The weights are \nshown next to the nodes; ğ‘Ÿ is the root. The white nodes are unaffected.  \n \n\n9 Local moves in forests and trees \n \n 92 \n \n\n \n9.5 Supplementary notes: PrÃ¼fer sequences \n \n93 \nFigure 9.5 (previous page). The eight possibilities for the relative positions of ğ‘Ÿğ‘œğ‘œğ‘¡ (shown as ğ‘Ÿ), \nğ‘œğ‘™ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡, and ğ‘›ğ‘’ğ‘¤ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ in ğ‘œğ‘™ğ‘‘ğ¶ğ‘œğ‘šğ‘. Where a node has dotted arrows going out of it, this \nmeans there may be any number of edges going out of it, and on the ends of these edges there \nmay be any tree structures. The dashed lines indicate directed paths that may be of any length. \n(All nodes on these paths should also be regarded as having dotted arrows going out of them.)  \n \n9.5 \nSupplementary notes: PrÃ¼fer sequences  \nPrÃ¼fer sequences (PrÃ¼fer 1918, Wu & Chao 2004), also known as PrÃ¼fer codes, are an \nalternative way to store trees. They are sequences of length ğ‘âˆ’2 whose elements are \nthe labels of the nodes (or, equivalently, integers in {1, â€¦ , ğ‘}). There is a one-to-one \ncorrespondence between all the possible trees on ğ‘ nodes and all the possible PrÃ¼fer \nsequences, and there are algorithms for working out the PrÃ¼fer sequence from the tree \nand vice versa. The one-to-one correspondence trivially implies Cayleyâ€™s formula for the \nnumber of trees on ğ‘ labelled nodes, ğ‘ğ‘âˆ’2 (Cayley 1889). Changing one letter in the \nPrÃ¼fer sequence does not correspond to anything so simple as moving one edge in the \ngraph, so it does not seem sensible to use PrÃ¼fer sequences for the present purpose.  \n \n\n \n94 \n10 \nAlgorithms for exploring the \nposterior distribution  \n10.1 Adaptations of two algorithms  \nPreamble \nIn Bayesian structure-learning for GGMs restricted to forests or trees, if there are 15 or \nmore nodes then there are still too many graphs for it to be possible to analyze all of \nthem. Instead the posterior distribution has to be approximated in some way (as \nmentioned in â€œExploring the posterior distributionâ€ in section 3.1). This section \ndescribes two ways of doing this.  \nReversible-jump MCMC for structure-learning \nOne way to approximate the posterior distribution is reversible-jump MCMC, based on \nthe method for decomposable graphs described in Giudici & Green (1999). This and the \nnext two subsections describe how this method can be adapted for forests or trees. \nMuch of this is closely based on section 3.2 of Giudici & Green (1999), and most of the \nnotation is the same. If ğ‘€ is a matrix, then ğ‘€ğ´ means the submatrix of ğ‘€ that consists of \nthe rows and columns indexed by the elements of ğ´.  \nThe standard Metropolisâ€“Hastings algorithm creates a Markov chain whose distri-\nbution converges to a given invariant distribution; values from this Markov chain are \nused as an approximate sample from the distribution. Reversible-jump MCMC is similar, \nbut the dimension of the state-space can change from one step to the next, so a more \ncomplicated formula has to be used for the acceptance probability. Reversible-jump \nMCMC is mostly used for approximating posterior distributions that include models of \nseveral different dimensions.  \nThe formula for the acceptance probability in reversible-jump MCMC is equation (7) in \nGreen (1995). It is somewhat complicated, so I will give the formula for the special case \nof proposing a move to a higher-dimensional variable. Let the variable be ğ‘¦ and the \ndesired invariant distribution be ğœ‹(ğ‘¦), and suppose that the proposed move is from ğ‘¦ to \nğ‘¦â€², which has higher dimension. Sample ğ‘¢ from a distribution with density ğ‘ and let ğ‘¦â€² \n= ğ‘¦â€²(ğ‘¦, ğ‘¢) be an invertible deterministic function. The move is accepted with probability  \nğ›¼(ğ‘¦, ğ‘¦â€²) = min{1, ğœ‹(ğ‘¦â€²)\nğœ‹(ğ‘¦) Ã—\nğ‘Ÿ(ğ‘¦â€²)\nğ‘Ÿ(ğ‘¦)ğ‘(ğ‘¢) Ã— | ğœ•ğ‘¦â€²\nğœ•(ğ‘¦, ğ‘¢)|} . \n \n               â†‘ \nğ‘‡= ratio of  \ntarget densities \n          â†‘ \nğ‘ƒ= proposal \nratio \n       â†‘ \nJacobian \n\n \n10.1 Adaptations of two algorithms \n \n95 \nHere ğ‘Ÿ(ğ‘¦) is the probability of choosing this type of move, starting from ğ‘¦. This formula \nensures that the Markov chain satisfies detailed balance and its distribution converges \nto ğœ‹.  \nGiudici & Green (1999) explain how to use reversible-jump MCMC to create an approxi-\nmate sample from the posterior distribution in Bayesian structure-learning of GGMs, for \ndecomposable graphs, using the HIW prior distribution for Î£. In the next two \nsubsections I describe adaptations of this algorithm for forests and trees, respectively, \nand show how the formulas for the various acceptance probabilities can be derived \nfrom general formulas. These adapted algorithms produce an approximate sample from \nthe posterior distribution of ğ‘¦= (ğº, Î“), where ğº is the graph and Î“ is the incomplete \ncovariance matrix. Î“ only contains the elements that correspond to edges in ğº; the \nother elements are blank. (See Giudici & Green 1999 for why it is convenient to use Î“ \nrather than Î£ or Î£âˆ’1.)  \nThe dimension of Î“ is the same as the number of edges in ğº. In the case of trees, the \ndimension of Î“ always stays the same but the positions of its elements change when ğº \nchanges, so reversible-jump MCMC is still appropriate. The main object of interest is the \nposterior distribution of ğº, which is simply the marginal distribution of ğº.  \nThe MCMC for forests repeatedly performs the following two types of move:  \n(a) add or delete an edge from ğº (this also requires changes to Î“),  \n(b) change all the elements of Î“.  \nThe MCMC for trees is the same except that, instead of adding or deleting an edge from \nğº, it moves an edge from one position to another.  \nGiudici & Green (1999) use a slightly more elaborate algorithm, with a hierarchical \nprior for the HIW parameters ğ›¿ and ğ·. Their variable is ğ‘¦= (ğº, Î“, ğ›¿, ğ·), and they have \ntwo further move-types, for updating ğ›¿ and ğ·.  \nMCMC on forests \nMCMC on forests, which I will call McmcF, is a simplified version of MCMC on \ndecomposable graphs. For move-type (a), the proposal is to update ğ‘¦= (ğº, Î“) to ğ‘¦â€² =\n(ğºâ€², Î“â€²), where ğº= (ğ‘‰, ğ¸) and ğºâ€² = (ğ‘‰, ğ¸â€²). The edge to add or remove, (ğ‘£ğ‘–, ğ‘£ğ‘—), is \nchosen uniformly at random from the edges that can be added or removed.  \nAdding an edge  \nFirst consider the case where the edge is to be added, so that ğ¸â€² = ğ¸âˆª(ğ‘£ğ‘–, ğ‘£ğ‘—). The \nformula for the acceptance probability can be derived from the formula in the previous \nsubsection. Firstly consider the ratio of the target densities, which I will call ğ‘‡ğ‘‘\n+. (Here ğ‘‘ \nmeans â€œdecomposable graphsâ€ and + means â€œadding an edgeâ€; I will use similar \nnotations for other quantities and other types of move.) This is  \nğ‘‡ğ‘‘\n+ = ğœ‹(ğ‘¦â€²)\nğœ‹(ğ‘¦) =\nâ„(Î£ğ‘†)â„(Î£ğ‘†âˆª{ğ‘–,ğ‘—}\nâ€²\n)\nâ„(Î£ğ‘†âˆª{ğ‘–})â„(Î£ğ‘†âˆª{ğ‘—}) , \n\n10 Algorithms for exploring the posterior distribution \n \n 96 \nwhere ğ‘† is a separator on the path between cliques that contain ğ‘– and ğ‘—, and â„(Î£ğ´) =\nğ¼ğ‘Š(Î£ğ´; ğ›¿, ğ·ğ´) Ã— ğ‘(ğ‘¥ğ´, Î£ğ´). Here ğ¼ğ‘Š is the inverse-Wishart density and ğ‘ is the multi-\nvariate Gaussian likelihood.  \nIn a forest, (ğ‘£ğ‘–, ğ‘£ğ‘—) can only be added if ğ‘£ğ‘– and ğ‘£ğ‘— are in different components. It follows \nthat ğ‘†= âˆ… and the ratio simplifies to  \nğ‘‡ğ‘“\n+ =\nâ„(Î£ğ‘–ğ‘—\nâ€² )\nâ„(Î£ğ‘–)â„(Î£ğ‘—) . \nCalculating â„(Î£ğ‘–) and â„(Î£ğ‘—) involves the one-dimensional inverse-Wishart distribution \nğ¼ğ‘Š(ğœ; ğ›¿, ğ·), which is the same as the inverse-gamma distribution with parameters ğ›¿/2 \nand ğ·/2.  \nThe next part of the formula is ğ‘ƒ, the proposal ratio: \nğ‘ƒğ‘“\n+ =\nğ‘Ÿâˆ’(ğ‘¦â€²)\nğ‘Ÿ+(ğ‘¦)ğ‘(ğ‘¢) . \nHere ğ‘Ÿ+(ğ‘¦) is the probability of choosing this particular move, which is  \nğ‘Ÿ+(ğ‘¦) =\n1\n|ğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘”ğº| + |ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’ğº| , \nwhere ğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘”ğº is the number of existing edges in ğº and ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’ğº is the number of \naddable edges in ğº; ğ‘Ÿâˆ’(ğ‘¦â€²) is the probability of choosing the reverse move, from ğºâ€² to ğº. \nTo calculate ğ‘Ÿâˆ’(ğ‘¦â€²), let ğ¼ be the component in ğº that contains ğ‘£ğ‘– and let ğ½ be the compo-\nnent that contains ğ‘£ğ‘—. Compared to ğº, ğºâ€² has one more existing edge and |ğ¼||ğ½| fewer \naddable edges, so  \nğ‘Ÿâˆ’(ğ‘¦â€²) =\n1\n|ğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘”ğº| + 1 + |ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’ğº| âˆ’|ğ¼||ğ½| . \nAs with decomposable graphs, Î“ is updated to Î“â€² by adding a new element in positions \n(ğ‘–, ğ‘—) and (ğ‘—, ğ‘–). The new element is ğ›¾ğ‘–ğ‘—\nâ€² = ğ‘¢, and this is drawn from a zero-mean \nGaussian distribution with variance ğœğº\n2, so  \nğ‘(ğ‘¢) =\n1\nğœğºâˆš2ğœ‹\nexp (âˆ’ğ‘¢2\n2ğœğº\n2) . \nThe variance ğœğº\n2 is chosen by the user. The last part of the formula for the acceptance \nprobability is the Jacobian. As with decomposable graphs, this is 1, because the new \nparameter ğ‘¢ is used with no transformation (Giudici & Green 1999, Green 2003). The \nacceptance probability is therefore just  \nmin{1, ğ‘‡ğ‘“\n+ğ‘ƒğ‘“\n+} . \nIf the graph prior distribution is not uniform, then ğ‘‡ğ‘“\n+ needs to be multiplied by \nğ‘(ğºâ€²)/ğ‘(ğº), where ğ‘(ğº) is the prior probability of ğº. The same is true in the \nsubsequent cases (for ğ‘‡ğ‘“\nâˆ’ and ğ‘‡ğ‘¡).  \n\n \n10.1 Adaptations of two algorithms \n \n97 \nRemoving an edge \nIf the edge is to be removed, then ğ¸â€² = ğ¸âˆ–(ğ‘£ğ‘–, ğ‘£ğ‘—). For decomposable graphs, the ratio \nof the target distributions is  \nğ‘‡ğ‘‘\nâˆ’= ğœ‹(y)\nğœ‹(ğ‘¦â€²) =\nâ„(Î£ğ‘†âˆª{ğ‘–})â„(Î£ğ‘†âˆª{ğ‘—})\nâ„(Î£ğ‘†)â„(Î£ğ‘†âˆª{ğ‘–,ğ‘—}\nâ€²\n)  , \nwhich is just 1/ğ‘‡ğ‘‘\n+. Similarly, for forests the ratio is ğ‘‡ğ‘“\nâˆ’= 1/ğ‘‡ğ‘“\n+.  \nThe proposal ratio is  \nğ‘ƒğ‘“\nâˆ’= ğ‘Ÿ+(ğ‘¦â€²)ğ‘(ğ‘¢)\nğ‘Ÿâˆ’(ğ‘¦)\n . \n(There is a ğ‘ in the numerator and not in the denominator because the dimension is \nbeing decreased; this follows from equation (7) in Green 1995.) Here ğ‘Ÿâˆ’(ğ‘¦) is the \nprobability of choosing this move. This is the same as ğ‘Ÿ+(ğ‘¦), which is given above. To \ncalculate ğ‘Ÿ+(ğ‘¦â€²), let ğ¼ be the component in ğºâ€² that contains ğ‘£ğ‘– and let ğ½ be the \ncomponent that contains ğ‘£ğ‘—. Compared to ğº, ğºâ€² has one less existing edge and |ğ¼||ğ½| \nmore addable edges, so the probability of the reverse move is  \nğ‘Ÿ+(ğ‘¦â€²) =\n1\n|ğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘”ğº| âˆ’1 + |ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ğ‘™ğ‘’ğº| + |ğ¼||ğ½| . \nÎ“ is updated by removing its (ğ‘–, ğ‘—) and (ğ‘—, ğ‘–) elements; ğ‘¢= ğ›¾ğ‘–ğ‘— and ğ‘ is as above. The \nJacobian is 1 and the acceptance probability is  \nmin{1, ğ‘‡ğ‘“\nâˆ’ğ‘ƒğ‘“\nâˆ’} . \nUpdating the incomplete covariance matrix \nMove-type (b), the update of Î“, is exactly as in Giudici & Green (1999). Each element is \nperturbed by adding a zero-mean Gaussian random variable with variance ğœğ‘–ğ‘—\n2. In \nsymbols,  \nğ›¾ğ‘–ğ‘—\nâ€²  ~ ğ‘(ğ›¾ğ‘–ğ‘—, ğœğ‘–ğ‘—\n2). \nHere ğœğ‘–ğ‘— is a single value, chosen by the user, though it could be a different value for \neach pair (ğ‘–, ğ‘—). This is not a dimension-changing move, so the appropriate acceptance \nprobability can be found using the formula for the standard Metropolisâ€“Hastings \nalgorithm. It consists of two factors. The first is the ratio of the target distributions, \nwhich is  \nğ‘‡ğ‘“\nÎ“ = ğ»ğ¼ğ‘Š( Î£â€² âˆ£ğ›¿, ğ·, ğº)ğ‘( ğ‘¥âˆ£Î£â€², ğº)\nğ»ğ¼ğ‘Š( Î£ âˆ£ğ›¿, ğ·, ğº)ğ‘( ğ‘¥âˆ£Î£, ğº)  . \nğ»ğ¼ğ‘Š is the HIW density and ğ‘ is the multivariate Gaussian likelihood. The second \nfactor is the ratio of the proposal distributions. This is 1 since the proposal distribution \nis symmetric. The acceptance probability is therefore min{1, ğ‘‡ğ‘“\nÎ“}.  \n\n10 Algorithms for exploring the posterior distribution \n \n 98 \nMCMC on trees \nI will call this McmcT.  \nUpdating the graph \nUpdating a tree by moving an edge requires removing one element from Î“ and inserting \na different element. The dimension of the parameter space stays the same, but the \nmeaning of the parameters changes. The formula for the acceptance probability can be \nderived from the formula for general reversible-jump MCMC.  \nFirst consider move-type (a). Suppose the proposed update to the graph is to move the \nedge (ğ‘£ğ‘–, ğ‘£ğ‘—) to (ğ‘£ğ‘˜, ğ‘£ğ‘™), so that ğ¸â€² = ğ¸âˆª(ğ‘£ğ‘˜, ğ‘£ğ‘™) âˆ–(ğ‘£ğ‘–, ğ‘£ğ‘—) . The ratio of the target \ndensities is  \nğ‘‡ğ‘¡=\nâ„(Î£ğ‘˜ğ‘™)\nâ„(Î£ğ‘˜)â„(Î£ğ‘™) Ã— â„(Î£ğ‘–)â„(Î£ğ‘—)\nâ„(Î£ğ‘–ğ‘—)\n , \nwhere â„(Î£ğ´) is as for forests.  \nThe proposal ratio is  \nğ‘ƒğ‘¡= ğ‘Ÿ(ğ‘¦â€²)\n ğ‘Ÿ(ğ‘¦) Ã— ğ‘(ğ›¾ğ‘–ğ‘—)\nğ‘(ğ›¾ğ‘˜ğ‘™\nâ€² ) . \nThe factors in this will be explained in turn. Firstly, ğ‘Ÿ(ğ‘¦) is the probability of the current \nmove, which is 1/ğ‘š(ğº), where ğ‘š(ğº) is the number of possible moves from ğº; ğ‘Ÿ(ğ‘¦â€²) is \nthe probability of the reverse move, which is 1/ğ‘š(ğºâ€²). Assume the edge-move is chosen \nuniformly at random from among all the possible edge-moves, as described in section \n9.4, and ğ‘£1 is the root. If the current graph is ğº, then the number of possible moves is  \nğ‘š(ğº) = âˆ‘[ğ‘Š(ğ‘£ğ‘§)(ğ‘âˆ’ğ‘Š(ğ‘£ğ‘§))]\nğ‘\nğ‘§=2\n, \nwhere ğ‘Š(ğ‘£) = |ğ‘‘ğ‘’(ğ‘£)| + 1. The values of ğ‘š(ğº) and ğ‘š(ğºâ€²) can be calculated when they \nare needed. Alternatively, ğ‘š(ğºâ€²) can mostly be calculated from ğ‘š(ğº)â€”most of the \nvalues in the sum for ğ‘š(ğºâ€²) are the same as the values in the sum for ğ‘š(ğº), since the \nonly nodes whose weights change are the ones on two particular paths (see section 9.4). \nAs for forests, the incomplete covariance matrix Î“ is updated by removing ğ›¾ğ‘–ğ‘— and \ninserting ğ›¾ğ‘˜ğ‘™\nâ€² , which is drawn from ğ‘(0, ğœğº\n2), whose density is ğ‘.  \nThe Jacobian is 1. Putting all these together, the acceptance probability is min{1, ğ‘‡ğ‘¡ğ‘ƒğ‘¡} .  \nThe simplest alternative to choosing edge-moves uniformly at random is to choose an \nedge uniformly at random, then remove it, then reinsert it uniformly at random. \nSuppose the components that result from removing the edge are ğ¼ and ğ½. The proba-\nbility of choosing that edge to remove is 1/(ğ‘âˆ’1), and the probability of putting it back \nin any particular position is 1/(|ğ¼||ğ½| âˆ’1), so the probability of any particular move is  \n1\nğ‘âˆ’1 Ã—\n1\n|ğ¼||ğ½| âˆ’1 . \n\n \n10.1 Adaptations of two algorithms \n \n99 \nThe probability of the reverse move is the same, so these two elements cancel out, and \nthe proposal ratio is just ğ‘(ğ›¾ğ‘–ğ‘—)/ğ‘(ğ›¾ğ‘˜ğ‘™\nâ€² ).  \nUpdating the incomplete covariance matrix \nThe update of Î“ is the same as in the case of forests.  \nStochastic shotgun search on forests and trees \nAn alternative to MCMC is the shotgun stochastic search algorithm that appears in \nsection 6 of Jones et al (2005). This algorithm moves around in the space of possible \ngraphs, calculating the unnormalized posterior probability of the graphs that it visits \nand some of their neighbours, and usually moving towards graphs with higher proba-\nbility. It does not involve a Markov chain and it does not give an approximation to the \nposterior distribution of Î£. Below is a version of this algorithm that has been adapted \nfor forests or trees. I call the version for forests SSSF and the version for trees SSST.  \n1. Start with a forest/tree ğº, and calculate and store its unnormalized posterior proba-\nbility.  \n2. Choose ğœ” distinct moves from ğº. (For forests a move consists of adding or removing \nan edge, and for trees it consists of moving an edge. Use the algorithms in section \n9.3 for forests and section 9.4 for trees.)  \n3. Calculate and store the unnormalized posterior probabilities of the ğœ” neighbouring \nforests/trees that result from doing these moves (except in the case of graphs for \nwhich this has previously been done).  \n4. Select one of the ğœ” neighbouring graphs by choosing each with probability propor-\ntional to its unnormalized posterior probability, and set ğº to be this graph.  \n5. Go back to step 2 and repeat many times. (Either stop after a fixed amount of time \nor after a fixed number of iterations.)  \nThe unnormalized posterior distribution is taken to be the values that were calculated \nfor the graphs whose probabilities were calculated, and zero for all other graphs. The \nalgorithm is intended as a simple alternative to MCMC with the possible advantage that \nit always moves to a new graph at every iteration, so it cannot get stuck at a single \ngraph. It simply explores the space of possible graphs, finding their unnormalized \nposterior probabilities, and tends to move towards graphs that have higher proba-\nbilities. It sometimes moves to graphs of lower probability, so it is not just deterministic \ngreedy hill-climbing. Any particular route through all the possible graphs has positive \nprobability, so if run for enough time it will eventually visit all the graphs. In this trivial \nsense it asymptotically gives the true posterior distribution.  \nAs well as the restriction to forests or trees, the above algorithm is different from the \noriginal one in Jones et al (2005) in three other ways. Firstly, in the original algorithm, \nat step 3 only the top ğ‘¥2 neighbouring graphs are retained. Secondly, at step 4 the neigh-\nbouring graph ğºğ‘– is chosen with probability proportional to ğ‘ğ‘–\nğ›¼, where ğ‘ğ‘– is its \nunnormalized posterior probability and ğ›¼ is a positive annealing parameter. (As ğ›¼â†’âˆ \nthe original algorithm becomes deterministic greedy hill-climbing.) Thirdly, at step 5 \nonly a list of the top ğ‘¥3 graphs is stored. Of these three differences, the third is the most \nlikely to be useful, since storing all the graphs takes a lot of memory.  \n\n10 Algorithms for exploring the posterior distribution \n \n \n100 \nThe experiments in sections 7 and 8 of Jones et al (2005) use ğ›¼= 1 and ğ‘¥2 = ğœ”, which \nmake their algorithm similar to the one given above. They also set ğœ” to be the number \nof neighbouring graphs, so the algorithm calculates the unnormalized posterior \nprobabilities of all the neighbouring graphs, not just some of them. (The set of all the \nneighbouring graphs consists of all the graphs that can be made by making a single \nmove from the present graph.) If all the neighbouring graphs are analyzed, then in step \n2 there is no need to choose moves at random, uniformly or otherwise. In the case of \ntrees, this would mean that the node-weights are not needed. \nWith trees and large ğ‘, the number of neighbouring graphs is huge, as shown in Table \n10.1, so if all of them are analyzed it would take a long time to do even one iteration of \nSSST. For this reason I use the version where only some of the neighbouring graphs are \nchecked at each iteration.  \nJones et al (2005) say their algorithm is designed for distributed implementation \n(which means using multiple computers at once), and that â€œdistributed computation is \nessential to the development of search and constructive methods beyond moderate \ndimensions.â€ Scott & Carvalho (2008) imply that using distributed computing is the \nmain purpose of Jones et al (2005)â€™s algorithm. Certainly, step 3 can be parallelized in \nan obvious way. But my programs to implement my versions of their algorithm are \nserial, not parallel, and they give reasonable results in a short amount of time (see the \nexperiments in chapter 11).  \n \nGraph \n \nğ‘= 100 \n \nğ‘= 1000 \n \nstar \nchain \n \nstar \nchain \nNumber of neighbours \n/ possible edge-moves \n \n9 801 \n998 001 \n \n166 650 \n166 666 500 \nTable 10.1. The number of neighbouring graphs (equivalently, the number of possible edge-\nmoves) within the space of trees, for four selected graphs. â€œStarâ€ and â€œchainâ€ are defined in \nsection 11.1 and the values were calculated using Propositions 11.4 and 11.5.  \n \nHow to store decomposable graphs \nIn section 11.7, SSSF and SSST are compared with the stochastic shotgun search algo-\nrithm on decomposable graphs. My programs for these experiments store and mani-\npulate decomposable graphs in basically the same way as Giudici & Green (1999) and \nJones et al (2005)â€”see also Jones et al (2004), which is a longer version. Full details \nare in section 3.1 and the appendix of Giudici & Green (1999) and section 2.1 of Green \n& Thomas (2013). Here I will just give aspects that are specific to my programs.  \nMy programs store decomposable graphs as junction trees and manipulate them by \nadding or removing any edge that can be added or removed, as in Giudici & Green \n(1999). If the proposed edge is not in the graph and the closest two cliques that contain \nthe two nodes are not neighbours in the junction tree, then the junction tree is \nmanipulated to make the two cliques be neighbours, as described in the second-last \nparagraph of Giudici & Green (1999).  \n\n \n10.2 Analyzing posterior graph distributions and assessing algorithms \n \n101 \nUsing junction trees, rather than junction forests, has the advantage that adding an edge \nbetween two separate components does not need to be treated separately, since it is a \nspecial case of the move shown by the downwards arrow in Figure 3(d) of Green & \nThomas (2013). It also means that separators are sometimes empty.  \n10.2 Analyzing posterior graph distributions and assessing \nalgorithms  \nHow frequentist algorithms are evaluated  \nFrequentist algorithms for graphical model structure-learning produce a single graph. \n(See section 3.2.) If the true graph is known, the natural way to measure how well one \nof these algorithms does is to compare the graph produced by the algorithm with the \ntrue graph. There are two scenarios in which you would know the true graph. One is \nthat you used simulated data that was generated from a distribution that corresponds \nto this graph. The other is that the data corresponds to objects that have been analyzed \nusing non-statistical methods, and a supposedly true graph-structure has been deduced \nfrom this analysis. The latter scenario is sometimes the case with networks of gene or \nprotein interactionâ€”see for example Albieri (2010).  \nProbably the simplest ways to measure the success of a frequentist algorithm are the \nnumbers of true-positives, false-positives, false-negatives, and true-negatives. Table \n10.2 shows the meanings of these phrases.  \n \n \n \nTrue graph \n \n \nEdge \nNon-edge \nGraph produced by  \nthe algorithm \nEdge \ntrue-positive \nfalse-positive \nNon-edge \nfalse-negative \ntrue-negative \nTable 10.2. The meanings of â€œtrue-positiveâ€ and related phrases, for a single graph produced by \na frequentist algorithm. For example, a true-positive is an edge that is in both the true graph and \nthe graph produced by the algorithm.  \n \nTrue-positives and the other three quantities are not specific to graphs or graphical \nmodel structure-learning. They can be used with any type of binary classification, for \nexample frequentist statistical hypothesis tests or medical tests to diagnose whether a \nperson has a diseaseâ€”a test is reliable if it seldom gives false-positives or false-\nnegatives.  \nAlso used are several ratios (Albieri 2010, page 50). In the following, ğ‘‡ğ‘ƒ stands for the \nnumber of true-positives, and the other abbreviations are similar:  \n \n \n \nprecision = \nğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ \n \n \n \ntrue-positive rate = recall = sensitivity = \nğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ¹ğ‘ \n \n \n \ntrue-negative rate = specificity = \nğ‘‡ğ‘\nğ‘‡ğ‘+ğ¹ğ‘ƒ \n\n10 Algorithms for exploring the posterior distribution \n \n \n102 \n \n \n \nfalse-positive rate = \nğ¹ğ‘ƒ\nğ‘‡ğ‘+ğ¹ğ‘ƒ \n \n \n \nfalse-negative rate = \nğ¹ğ‘\nğ‘‡ğ‘ƒ+ğ¹ğ‘ \n \n \n \naccuracy = \nğ‘‡ğ‘ƒ+ğ‘‡ğ‘\nğ‘‡ğ‘ƒ+ğ¹ğ‘+ğ¹ğ‘ƒ+ğ‘‡ğ‘ \n \n \n \nerror rate = \nğ¹ğ‘ƒ+ğ¹ğ‘\nğ‘‡ğ‘ƒ+ğ¹ğ‘+ğ¹ğ‘ƒ+ğ‘‡ğ‘ \nFor example, the true-positive rate is the proportion of edges in the true graph that \nwere correctly identified by the algorithm. Probably the most-used rates are the first \nthree. For these, higher values are better.  \nMany frequentist algorithms have a tuning parameter. Varying this and repeating the \nalgorithm gives different values of the rates. An algorithm can be assessed by plotting \nthe precision (on the vertical axis) against the recall, for different values of the tuning \nparameter. Alternatively it can be assessed by plotting the recall against the false-\npositive rateâ€”this is called a receiver operating characteristic (ROC) curve. For \nexamples see Albieri (2010) or Guo et al (2011).  \nSome algorithms for estimating multivariate Gaussian distributions produce an \nestimate of the covariance matrix. This can be assessed using measures such as entropy \nloss and Frobenius loss (Guo et al 2011). Some research on estimating these distri-\nbutions talks only about the covariance matrix and does not mention graphs at all \n(though Dempster 1972, for example, does have elements of the precision matrix set to \nexactly zero).  \nAll these methods can be used either to compare different algorithms or to compare \ndifferent parameters in the same algorithm. For ways of evaluating algorithms for \ndirected acyclic graphical models, see Gasse et al (2012).  \nFrequentist and Bayesian algorithms can also be assessed by using the estimated \ngraphs or covariance matrices they produce to make predictions. These predictions can \nbe compared with reality or with other data that were not used by the algorithm. But \nmaking predictions is a different goal from learning the structure.  \nHow Bayesian methods are evaluated \nAssessing a Bayesian method is more complicated than assessing a frequentist method, \nbecause the former produces a graph distribution rather than a single graph. In this \nsection I will refer to the posterior probability that an edge is present in the graph as \nthe probability of that edge. This is sometimes called an edge-inclusion probability.  \nOne way to evaluate a Bayesian method is to use the MAP graph, and see for example \nhow many true-positives it has. But a single graph is rather simple given the large \namount of information in the posterior distribution. There is another drawback that is \neasiest to explain with an example. It might be the case that there are ten graphs that \nhave high probability, and the 2nd to 9th most likely graphs have many edges in \ncommon with each other but few edges in common with the most likely (MAP) graph. \nIn this case, if you are going to use a single graph, then it would probably be better to \n\n \n10.2 Analyzing posterior graph distributions and assessing algorithms \n \n103 \nuse the 2nd most likely graph, or the graph consisting of all edges whose probabilities \nare above a certain threshold, rather than the MAP graph.  \nAny of the algorithms described in section 10.1 produces an approximation to the graph \nposterior distribution, in the form of a set of graphs and estimates of their posterior \nprobabilities. The MTT-based methods described in chapter 8 do not produce the entire \nposterior distribution itself, or an approximation to it, but they can produce several \nexact quantities that can be used to compare algorithms.  \nAs with frequentist algorithms, there are two scenarios in which the true graph is \nknown. If the true graph is known, the simplest quantities with which to evaluate \nalgorithms are the expected number of true-positives and the expected values of the \nother quantities in Table 10.2.  \nBelow are listed the main objects or types of information that have been used in \nprevious research to summarize estimated graph posterior distributions for undirected \ngraphical models, or to evaluate the algorithms that produced these distributions.  \nâ€¢ \nThe probabilities of all of the edges, in the form of a triangular matrix. The elements \nof the matrix are ğ‘ğ‘–ğ‘—= â„™((ğ‘–, ğ‘—) âˆˆğ¸âˆ£ğ‘¥). See Wang & Li (2012)â€™s example with six \nnodes or Dobra et al (2011)â€™s example with ten nodes. This seems to be the most \ncommonly used information.  \nâ€¢ \nThe probabilities of all the edges, in the form of a diagonally symmetric square grid \nwhere the shade of grey in each little square indicates that edgeâ€™s probability. See \nWong et al (2003), Scott & Carvalho (2008), or Armstrong et al (2009). \nâ€¢ \nThe probabilities of certain specific edges. See Jones et al (2005) or Carvalho & \nScott (2009).  \nâ€¢ \nA graph consisting of all the edges whose probabilities are above a certain \nthreshold. See Wang & Li (2012)â€™s example with 100 nodes, where the threshold is \nÂ½ and they call this object the â€œposterior mean graphâ€, or Armstrong et al (2009)â€™s \nexample with 11 nodes, where the threshold is 70%.  \nâ€¢ \nThe top-ranking (most likely) graph or, less commonly, graphs. See Giudici & Green \n(1999) or Jones et al (2005).  \nâ€¢ \nThe probabilities of the top-ranking graph or, less commonly, graphs. These may be \nnormalized or unnormalized. See Giudici & Green (1999), Jones et al (2005), or \nScott & Carvalho (2008). Scott & Carvalho (2008) judge their search algorithms by \nthe posterior probabilities of the models they findâ€”the higher, the better. For each \nalgorithm they show the top 1000 posterior probabilities on a histogram.  \nâ€¢ \nETPR (expected true-positive rate) and related quantities. Moghaddam et al (2009) \ngive a plot of ğ‘‡ğ‘ƒğ‘… against ğ¹ğ‘ƒğ‘… in which each graph appears as a single point, and \nthe expected values of these rates (under Bayesian model-averaging) is plotted as a \nsingle point in a different colour.  \nThese numbers or graphs are usually then commented on and discussed, and \nconclusions are reached about which algorithm, prior, or parameter is best.  \nMost of the objects listed above only convey separate information about each edge; they \ndo not show which combinations of edges are likely to be present. Consequently they \nfail to reveal certain notable features of the graph distribution. As an extreme and \n\n10 Algorithms for exploring the posterior distribution \n \n \n104 \nunlikely example, if the top few graphs were stars centred at different nodes (see \nDefinition 11.1), then the matrix of edge-probabilities would only show that all the \nedges in those stars were likely. It would not reveal that the graph was almost certainly \na star. Only the MAP graph and the other top-ranking graphs convey any information \nabout the graph as a whole, not just separate edges.  \nSection 2.4 mentioned the importance of hubs or stars. Albieri (2010) found that when \nthe true graph contains a star, frequentist algorithms mistakenly find that these nodes \nform a clique. In evaluating Bayesian methods it would be desirable to be able to notice \nany consistent structural â€œbiasâ€ such as this. The simplest way to judge whether stars \nare correctly identified would be to look at the posterior expected degrees of the hub \nand the nodes it is connected to. (If the entire true graph is a star, then the expected \ndegree of the hub is exactly the same as ğ¸ğ‘‡ğ‘ƒğ‘….) In chapter 11, I evaluate various \nBayesian algorithms and priors using simulated datasets, but my main algorithms only \nconsider forests or trees, so there is no possibility of misidentifying a star as a clique.  \nFriedman et al (2000) use indicator functions for features of the graph. They work with \ndirected graphical models. The first type of features they consider is â€œMarkov relationsâ€, \nabout whether one node is in the Markov blanket of another (which holds if the two \nnodes are connected by an edge or share a child). The other is â€œorder relationsâ€, about \nwhether one node is an ancestor of another.  \nSingle numbers for evaluating Bayesian methods \nIn abbreviations hereafter, â€œEâ€ means â€œexpectedâ€. It is useful to have a small number of \nnumerical quantities to evaluate how well a Bayesian method does, because these are \nordered and easier to interpret than large matrices or graphs. Possible quantities are \nğ¸ğ‘‡ğ‘ƒğ‘…, the three quantities related to it, and the expected degrees of the nodes. The \nalgorithms in section 10.1 are random, so the estimates of ğ¸ğ‘‡ğ‘ƒğ‘… and the other \nquantities will probably vary between different runs.  \nIf the Bayesian analysis is restricted to trees, then there are restrictions on ğ¸ğ‘‡ğ‘ƒ and the \nthree related quantities. In particular, ğ¸ğ‘‡ğ‘ƒ+ ğ¸ğ¹ğ‘ƒ= ğ‘âˆ’1. If the true graph is also a \ntree, ğ¸ğ‘‡ğ‘ƒ+ ğ¸ğ¹ğ‘= ğ‘âˆ’1, so only one of the four quantities is worth looking at. If the \nanalysis is restricted to forests, then it may be useful to look at two values, for example \nğ¸ğ‘‡ğ‘ƒ and ğ¸ğ‘‡ğ‘, but if all the high-ranking graphs are trees then ğ¸ğ‘‡ğ‘ƒ+ ğ¸ğ‘‡ğ‘ will be close \nto ğ‘âˆ’1.  \nIn the case of trees, the MTT-based methods from section 8.2 and 8.3 can be used to find \nğ¸ğ‘‡ğ‘ƒğ‘…. I will use them for these purposes in section 11.5.  \nFormulas for evaluating Bayesian algorithms \nAs examples, I will give formulas for four quantities to do with the posterior graph \ndistribution: (a) the probability of any particular edge, (b) the degree of any particular \nnode, (c) ğ¸ğ‘‡ğ‘ƒ, and (d) ğ¸ğ‘‡ğ‘ƒğ‘….  \nSuppose the true graph is ğº= (ğ‘‰, ğ¸) and the graphs produced by the algorithm are \n{ğºğ‘–= (ğ‘‰, ğ¸ğ‘–)}. In the case of the MTT-based method, this set contains all the possible \ntrees. Let  \n\n \n10.2 Analyzing posterior graph distributions and assessing algorithms \n \n105 \nğ•€ğºğ‘–\n(ğ‘¢,ğ‘£) = { 1 if (ğ‘¢, ğ‘£) âˆˆğ¸ğ‘–\n0 otherwise.   \n(a) The posterior probability of edge (ğ‘¢, ğ‘£) is  \nâˆ‘ğ‘( ğºğ‘–âˆ£âˆ£ğ‘¥)ğ•€ğºğ‘–\n(ğ‘¢,ğ‘£)\nğ‘–\n . \nThis formula uses the normalized posterior probabilities of the graphs, so you have to \nadd all the unnormalized probabilities to find the normalizing constant (except in the \ncase of the MTT-based method, which calculates the normalizing constant without \nusing all the separate unnormalized probabilities).  \n(b) The expected degree of node ğ‘£ is  \nâˆ‘(ğ‘( ğºğ‘–âˆ£âˆ£ğ‘¥) âˆ‘ğ•€ğºğ‘–\n(ğ‘¢,ğ‘£)\nğ‘¢â‰ ğ‘£\n)\nğ‘–\n= âˆ‘âˆ‘ğ‘( ğºğ‘–âˆ£âˆ£ğ‘¥)ğ•€ğºğ‘–\n(ğ‘¢,ğ‘£)\nğ‘¢â‰ ğ‘£\nğ‘–\n. \n(c) The number of true-positives in ğºğ‘– is  \nğ‘‡ğ‘ƒ= âˆ‘ğ•€ğº\n(ğ‘¢,ğ‘£)ğ•€ğºğ‘–\n(ğ‘¢,ğ‘£)\nğ‘¢,ğ‘£âˆˆğ‘‰\n , \nso  \nğ¸ğ‘‡ğ‘ƒ= âˆ‘âˆ‘ğ‘( ğºğ‘–âˆ£âˆ£ğ‘¥)ğ•€ğº\n(ğ‘¢,ğ‘£)ğ•€ğºğ‘–\n(ğ‘¢,ğ‘£)\nğ‘¢,ğ‘£âˆˆğ‘‰\nğ‘–\n \n=\nâˆ‘\nâˆ‘ğ‘( ğºğ‘–âˆ£âˆ£ğ‘¥)ğ•€ğºğ‘–\n(ğ‘¢,ğ‘£)\nğ‘–\n(ğ‘¢,ğ‘£)âˆˆğ¸\n. \nThe second expression shows that ğ¸ğ‘‡ğ‘ƒ is the sum of the posterior probabilities of all \nthe edges in the true graph.  \n(d) For ğºğ‘–, ğ‘‡ğ‘ƒğ‘…= ğ‘‡ğ‘ƒ/|ğ¸ğº|. So ğ¸ğ‘‡ğ‘ƒğ‘… is  \nğ¸ğ‘‡ğ‘ƒğ‘… = âˆ‘âˆ‘ğ‘( ğºğ‘–âˆ£âˆ£ğ‘¥)\nğ•€ğº\n(ğ‘¢,ğ‘£)ğ•€ğºğ‘–\n(ğ‘¢,ğ‘£)\n|ğ¸ğº|\nğ‘¢,ğ‘£âˆˆğ‘‰\nğ‘–\n= ğ¸ğ‘‡ğ‘ƒ\n|ğ¸ğº| , \nwhich lies between 0 and 1.  \nFor algorithms that produce an approximation to the entire posterior distribution, an \nalternative would be to calculate these values by only using the top ğ‘ most likely graphs, \nfor some ğ‘.  \nVisual representations of graph distributions  \nWhat is the best way to visually represent or summarize a graph posterior distribution? \nA single graph is too simple. A triangular or symmetric matrix of edge-probabilities \ncontains more information, but for eight or more nodes it is probably impossible to \nnotice any overall patterns. A square grid, with colours or shades representing the \nvalues in this matrix, works well if the graph has a strong structure, but if the nodes are \n\n10 Algorithms for exploring the posterior distribution \n \n \n106 \nin no particular order and the high-probability graphs have complicated structure then \nit may be hard to take in.  \nAnother possibility is a graph in which the thickness of each edge is proportional to its \nprobability, and only edges whose probabilities are greater than a certain threshold are \nshown. This would be easy to take in, because there is no need to peer at numbers or \ncount which column or row an entry is in. But it still only conveys separate information \nabout each edge.  \nThe best way of showing an entire graph distribution is an animation that consists of \ngraphs generated from it. I realized this when Peter Green said it during a talk in \nNovember 2012. Java programs that can show animations of the MCMC in Green & \nThomas (2013) are normally available from Alun Thomasâ€™s JPSGCS website, at \nhttp://balance.med.utah.edu/wiki/index.php/JPSGCS, though as of February 2013 this \nis not working.  \nSupplementary notes: further details on evaluation of Bayesian methods  \nIt may be useful to give more detail about some of the papers that evaluate Bayesian \nmethods. Dobra et al (2011)â€™s example with ten nodes is about matrix-variate GGMs. As \nwell as the estimated edge probabilities they also give the standard errors of these \nestimated probabilities. One of their methods did very well, giving probability 1 to all \nthe edges in the true graph and less than 0.1 to all the edges that were not. Matrix-\nvariate distributions have two graphs; one of their true graphs had ğ‘= 5 and the other \nwas a loop with ğ‘= 10. Wang & Li (2012) found in one of their examples that all the \nedges in the true graph got probability 1 and all the other edges got probability below \n0.08. The true graph was a loop with ğ‘= 100.  \nCarvalho & Scott (2009) and Wang & Li (2012) evaluate their posterior distributions by \nusing them for prediction in mutual funds (schemes that pool money from many \ninvestors and invest it in stocks or other financial assetsâ€”see U.S. Securities and \nExchange Commission 2010). Moghaddam et al (2009) evaluate posterior distributions \nby using them for prediction, but I cannot understand whether they use all the graphs \nor just one of them.  \nGiudici & Green (1999) also give the expected number of edges under the posterior \ndistribution. Some algorithms produce estimates of the posterior distribution for the \ncovariance matrix. For how these can be assessed see Giudici & Green (1999) or Wong \net al (2003).  \nArmstrong et al (2009) compared their MCMC for GGM structure-learning to one in \nBrooks et al (2003). To do this they gave a Manhattan plot that showed the number of \nedges in the graph at each iteration and the cumulative number of graphs visited at \neach iteration. They also used effective sample sizes. All the methods in this section \n(10.2) can of course just as well be used for directed acyclic graphical models. See for \nexample Altomare et al (2011).  \n \n\n \n107 \n11 \nExperiments \n11.1 Facts about star and chain graphs \nThe subsequent sections of this chapter describe experiments on simulated data. Most \nof the datasets were generated from distributions that correspond to star and chain \ngraphs. The reasons for using these shapes of graph were that they are extremal in \ncertain senses, described by the three propositions in this section, to do with exploring \nthe space of trees by making local moves. To define these two types of graph, which are \nalso trees, let ğ‘‰= {ğ‘£1, â€¦ , ğ‘£ğ‘}.  \nDefinition 11.1. (ğ‘‰, ğ¸) is a star if ğ¸= {{ğ‘£1, ğ‘£ğ‘–}, â€¦ , {ğ‘£ğ‘–âˆ’1, ğ‘£ğ‘–}, {ğ‘£ğ‘–+1, ğ‘£ğ‘–}, â€¦ , {ğ‘£ğ‘, ğ‘£ğ‘–}} for \nsome ğ‘–. (See also section 2.3.)  \nDefinition 11.2. (ğ‘‰, ğ¸) is a chain if the nodes can be relabelled in such a way that ğ¸=\n{{ğ‘£1, ğ‘£2}, {ğ‘£2, ğ‘£3}, â€¦ , {ğ‘£ğ‘âˆ’1, ğ‘£ğ‘}}.  \nThe weight of node ğ‘£ is ğ‘Š(ğ‘£) = 1 + |ğ‘‘ğ‘’(ğ‘£)|. The number of edge-moves that start with \nremoving {ğ‘£, ğ‘ğ‘(ğ‘£)} is ğ‘”(ğ‘Š(ğ‘£)) = ğ‘Š(ğ‘£)(ğ‘âˆ’ğ‘Š(ğ‘£)), and the total number of edge-\nmoves is âˆ‘\nğ‘”(ğ‘Š(ğ‘£))\nğ‘£â‰ ğ‘Ÿğ‘œğ‘œğ‘¡\n. In considering the number of possible edge-moves from \nstars and chains, the root can be chosen arbitrarily from among all the nodes since this \ndoes not affect the number of edge-moves.  \nProposition 11.3. Stars are the only trees where all the nodes are chosen with equal \nprobability in line 1 of Algorithm IX (section 9.4).  \nProof. Suppose there is a tree that contains a path of length 4 and that this treeâ€™s edges \nwould be chosen with equal probability in line 1 of Algorithm IX. Regard the tree as a \nrooted tree with root ğ‘Ÿ at one end of this path. Call the subsequent nodes on the path ğ‘£1, \nğ‘£2, and ğ‘£3â€”see Figure 11.1, in which other nodes are not shown. Now ğ‘‘ğ‘’(ğ‘£1) âŠƒ\nğ‘‘ğ‘’(ğ‘£2) âŠƒğ‘‘ğ‘’(ğ‘£3), so ğ‘Š(ğ‘£1) > ğ‘Š(ğ‘£2) > ğ‘Š(ğ‘£3). These three nodes being chosen with \nequal probability means that ğ‘”(ğ‘Š(ğ‘£1)) = ğ‘”(ğ‘Š(ğ‘£2)) = ğ‘”(ğ‘Š(ğ‘£3)). But ğ‘” is a quadratic \nfunction, so it is impossible for three distinct values of ğ‘¥ to have the same value of ğ‘”(ğ‘¥). \nTherefore no such tree can exist. The only trees that contain no paths of length 4 are \nstars, which completes the proof. ï‚¨  \n \n \nFigure 11.1. A path of length 4.  \n \n\n11 Experiments \n \n \n108 \nProposition 11.4. A star with ğ‘ nodes has (ğ‘âˆ’1)2 possible edge-moves, and this is the \nfewest of any tree with ğ‘ nodes.  \nProof. The value of ğ‘”(ğ‘¤) is minimized at ğ‘¤= 1 or ğ‘âˆ’1, and its minimum value is ğ‘âˆ’1. \nConsider a star with ğ‘ nodes and suppose that its root is the hub (the node at the centre \nof the star). All the other nodes have weight 1, so the total number of possible edge-\nmoves is (ğ‘âˆ’1)2, which is the minimum. ï‚¨  \nProposition 11.5. A chain with ğ‘ nodes has (ğ‘3 âˆ’ğ‘)/6 possible edge-moves, and this is \nthe most of any tree with ğ‘ nodes.  \nProof. Regard the chain graph as a horizontal line, take the leftmost node to be the root, \nand number the nodes from left to right. The weight of the (ğ‘–+ 1)th node is ğ‘âˆ’ğ‘–, so the \ntotal number of possible edge-moves is  \nâˆ‘ğ‘”(ğ‘âˆ’ğ‘–)\nğ‘âˆ’1\nğ‘–=1\n= âˆ‘(ğ‘âˆ’ğ‘–)ğ‘–\nğ‘âˆ’1\nğ‘–=1\n= ğ‘3 âˆ’ğ‘\n6\n . \nTo show that this is the maximum, consider a tree that contains a node of degree 3 or \nmore. Let this node be the root, label its children ğ‘£1, ğ‘£2, ğ‘£3, â€¦, and let ğ‘¤1, ğ‘¤2, ğ‘¤3, â€¦ be \nthese childrenâ€™s weights. Without loss of generality assume that 1 â‰¤ğ‘¤1 â‰¤ğ‘¤2 â‰¤ğ‘¤3 â‰¤\nâ‹¯â‰¤ğ‘âˆ’1. Now ğ‘¤1 + ğ‘¤2 + ğ‘¤3 + â‹¯= ğ‘âˆ’1, because the left-hand side counts all the \nnodes in the graph except the root exactly once. It follows that ğ‘¤1 â‰¤(ğ‘âˆ’1)/3 and ğ‘¤1 +\nğ‘¤2 â‰¤2(ğ‘âˆ’1)/3. Because ğ‘”(ğ‘¥) is a quadratic function with peak at ğ‘¥= ğ‘/2, it must be \nthe case that ğ‘”(ğ‘¤1 + ğ‘¤2) > ğ‘”(ğ‘¤1).  \nThe number of edge-moves that start with the removal of (ğ‘Ÿğ‘œğ‘œğ‘¡, ğ‘£1) is ğ‘”(ğ‘¤1). Create a \nnew tree by deleting the edge (ğ‘Ÿğ‘œğ‘œğ‘¡, ğ‘£2) and replacing it with (ğ‘£1, ğ‘£2). In the new tree \nğ‘Š(ğ‘£1) = ğ‘¤1 + ğ‘¤2, so the number of edge-moves that start with the removal of \n(ğ‘Ÿğ‘œğ‘œğ‘¡, ğ‘£1) is ğ‘”(ğ‘¤1 + ğ‘¤2) > ğ‘”(ğ‘¤1). For all other nodes ğ‘£, ğ‘Š(ğ‘£) and hence ğ‘”(ğ‘Š(ğ‘£)) are \nthe same as in the original tree. So the new tree has more edge-moves than the old one.  \nIt follows that any tree with the largest possible number of edge-moves must have no \nnode of degree 3 or more. The only such trees are chains. ï‚¨  \nThe three propositions still hold if the â€œnon-moveâ€ is excluded, so that ğ‘”(ğ‘Š(ğ‘£)) =\nğ‘Š(ğ‘£)(ğ‘âˆ’ğ‘Š(ğ‘£)) âˆ’1. The only differences are that in Proposition 11.4 the number of \nedge-moves is (ğ‘âˆ’1)(ğ‘âˆ’2) and in Proposition 11.5 it is (ğ‘3 âˆ’7ğ‘)/6 + 1.  \nIn trees with other shapes, individual nodes can be extremal. Let ğ‘’= (ğ‘£, ğ‘ğ‘(ğ‘£)). If ğ‘£ or \nğ‘ğ‘(ğ‘£) has degree 1, then ğ‘”(ğ‘Š(ğ‘£)) has its lowest possible value. If ğ‘’ splits the tree as \nnearly as possible in half, so that ğ‘Š(ğ‘£) âˆˆ{(ğ‘âˆ’1)/2, ğ‘/2, (ğ‘+ 1)/2}, then ğ‘”(ğ‘Š(ğ‘£)) has \nits highest possible value.  \n11.2 Experiments with systems for storing trees  \nDifferent systems for storing trees \nSection 9.4 describes a way of storing trees so that edge-moves can be chosen uniformly \nat random. I will call this System A. To assess System A it is desirable to compare it to \n\n \n11.2 Experiments with systems for storing trees \n \n109 \nother systems for storing trees and choosing edge-moves. Here I describe three other \nsystems. In all four, trees are stored as rooted trees, because this makes it easy to check \nwhether moves are legal. All four systems produce a set of ğœ” distinct edge-moves. The \nsystems will subsequently be compared using SSST, with the non-move excluded.  \nSystem A (weights). Trees are stored and edge-moves are chosen as described in \nsection 9.4.  \nSystem B (unused weights). Edge-weights are stored but not used. To choose ğœ” edge-\nmoves, first create a list ğ¿ that contains ğ‘âˆ’2 copies of each edge. Choose ğœ” edges from \nğ¿ uniformly at random and put these in a list called ğ‘€. These are the edges that are to \nbe removed (and obviously they are not necessarily distinct). For each distinct edge ğ‘’ in \nğ‘€, let ğ‘šğ‘’ be the number of times ğ‘’ appears in ğ‘€, identify the two components that \nresult when you remove ğ‘’, and choose ğ‘šğ‘’ distinct places to reinsert it.  \nSystem C (no weights). No edge-weights are stored. Edge-moves are chosen as in \nSystem B.  \nSystem D (rejection). No edge-weights are stored. To choose ğœ” edge-moves, repeat the \nfollowing as many times as necessary: choose an edge uniformly at random, remove it \nand identify the two components that result, choose where to reinsert it uniformly at \nrandom, and accept this edge-move if and only if it has not already been chosen.  \nSystem D has one drawback. If ğœ” is large relative to the total number of possible edge-\nmoves, then it is likely that many edge-moves will be rejected and choosing ğœ” different \nedge-moves will take a long time.  \nSystems B and C are designed to avoid this drawback. The number of edge-moves that \nstart with removing (ğ‘£, ğ‘ğ‘(ğ‘£)) is ğ‘”(ğ‘Š(ğ‘£)) = ğ‘Š(ğ‘£)(ğ‘âˆ’ğ‘Š(ğ‘£)) âˆ’1 â‰¥ğ‘âˆ’2, so for each \ndistinct edge ğ‘’ in ğ¿ there are at least ğ‘âˆ’2 possible places where it can be reinserted. ğ¿ \ncontains ğ‘âˆ’2 copies of each edge, so however many copies of ğ‘’ are chosen to be put in \nğ‘€, there will certainly be enough possible places for it to be reinserted. There is never \nany need to reject and repeat. Systems B and C will not work if ğœ”> |ğ¿| = (ğ‘âˆ’1)(ğ‘âˆ’\n2), but this does not matter unless you want to find more edge-moves than that.  \nThe only difference between Systems A and B is that System B does not use the weights \n(it does store and update them). So if using the weights, and choosing edge-moves \nuniformly at random, gives some advantage, then this should be evident by comparing \nthe results of experiments that use these two systems.  \nThe only difference between Systems B and C is that System B wastes time storing and \nupdating the node-weights. So System C should always do at least as well as System B. If \nstoring and updating the weights takes little time, then there should be little difference \nbetween Systems B and C.  \nDatasets \nTo compare the various algorithms and ways of storing trees, a large number of \nsimulated datasets were generated. The values of ğ‘ that were used were 30 and 100, \nand the values of ğ‘› (the number of data) were 50 and 500. For each value of ğ‘, two \ncovariance matrices were created, one corresponding to a star and the other corres-\n\n11 Experiments \n \n \n110 \nponding to a chain. The diagonal elements of the covariances were all 1 and the non-\nzero partial correlations were all 0.99/âˆšğ‘âˆ’1; these two conditions, together with the \ngraph, specify the covariances completely. (The reason for using this formula is the \ninequality in section 2.3 about the partial correlations in stars.)  \nThe four covariance matrices and two values of ğ‘› give eight combinations of covariance \nmatrix and ğ‘›, whose descriptions can be seen on the horizontal axes in Figure 11.2. If \njust a single dataset were generated for each of these, then these datasets might be \natypical and the results might fail to show the effects of the different systems for storing \ntrees (and the different shapes of graph and values of ğ‘ and ğ‘›). For this reason, 500 \ndatasets were generated for each combination of covariance matrix and ğ‘›, and the \nalgorithm was run on all of these. The datasets were all generated from zero-mean \nmultivariate Gaussian distributions.  \nStar and chain graphs were used because they are extremal in the senses described in \nsection 11.1. Suppose the true graph is a star. If at a certain point in SSST the current \ngraph is the true graph, then, by Proposition 11.3, System A is equally likely to choose \nany of the edges to move. Systems Bâ€“D always do this. It follows that all four systems \nwill choose edge-moves uniformly at random. If the current graph is not the true graph \nbut something similar to it, as will probably be the case most of the time, then Systems \nBâ€“D will choose edge-moves almost uniformly at random. In contrast, when the true \ngraph is a chain, Systems Bâ€“D will often choose edge-moves with a distribution that is \nfar from uniformâ€”for example, all chains contain nodes that have the lowest and \nhighest possible values of ğ‘”(ğ‘Š(ğ‘£)).  \nExperiments \nThe four systems were compared by using them with SSST, as described in section 10.1, \nand running the algorithm under the same computational conditions and for the same \namount of CPU time, with the same parameters. For the hyper inverse Wishart prior on \nÎ£, the scalar hyperparameter ğ›¿ was 3 and the matrix hyperparameter was ğ¼ğ‘(ğ›¿+ 2) \n(see Jones et al 2005, the erratum listed in the references, and Donnet & Marin 2012). \nThe prior distribution on the graph structure was uniform on trees with ğ‘ vertices. For \nthese experiments ğœ” was chosen to be ğ‘2/20 so that it scaled appropriately with the \nnumber of possible edge-moves; this means ğœ”= 45 in the cases where ğ‘= 30 and ğœ”=\n500 where ğ‘= 100. For each value of ğ‘, all runs were started at a fixed graph that was \ndifferent from either of the graphs that the data were generated from.  \nThe issue of how to evaluate Bayesian structure-learning was discussed in section 10.2. \nEach run of the algorithm was done for 20 seconds, and the following quantities were \nrecorded: \nâ€¢ \nthe number of distinct graphs visited \nâ€¢ \nğ¸ğ‘‡ğ‘ƒğ‘… \nâ€¢ \nthe true-positive rate in the top graph \nâ€¢ \nthe score of the top graph (its unnormalized log posterior probability) \nâ€¢ \nthe sums of the scores of the top 10 graphs.  \n\n \n11.2 Experiments with systems for storing trees \n \n111 \nResults \nThe results are shown in Figure 11.2. Each bar-chart corresponds to one of the \nquantities in the bullet-list above. Each group of four bars corresponds to one \ncombination of covariance matrix and ğ‘›, and within each group each bar corresponds \nto one system for storing trees. For all five bar-charts, larger values are better.  \nEach bar corresponds to 500 runs of the algorithm on the different datasets generated \nfrom the same distribution with the same ğ‘›. The heights of the bars are the median \nvalues and the â€œwhiskersâ€ show the 25% and 75% quartiles.  \nTo assess the algorithm in the cases where the true graph was a star, it is also of interest \nto know the posterior expected degree of the node that was supposed to be the hub at \nthe centre of the star. But this is just ğ¸ğ‘‡ğ‘ƒğ‘… multiplied by ğ‘âˆ’1, so the relative heights of \nthe bars would be the same as in the second bar-chart.  \nMost of the bar-charts show no difference between the four systems for storing trees, or \nonly tiny differences. The number of graphs visited varies somewhat. System A does \nbetter than the other systems on the datasets with ğ‘= 30, ğ‘›= 500, and the true graph \na star. But with four of the sets of 500 datasets it does worse than Systems B or C. \nSystem D does very badly for two of the sets, which is notable as it is probably the most \nobvious and easy to program.  \nIt might be expected that the weights would make more difference for chains than for \nhubs, because of the extremal properties shown in section 11.1. The numbers of graphs \nvisited by System A are indeed more different for chains than for stars. But they are \nlower. However, the interquartile ranges sometimes overlap.  \nOverall System C does slightly better than System B, as expected, though there is a large \noverlap between the ranges. The last two bar-charts are of less interest, firstly because \nthey show no differences between the four systems, and secondly because in these bar-\ncharts it is not legitimate to compare quantities that correspond to different sets of \ndatasets (because scores for different sets of datasets have nothing to do with each \nother).  \nThe differences shown in the bar-charts between the four systems are minor, but the \ndifferences between the eight datasets are major. Unsurprisingly, the expected true-\npositive rates and the true-positive rates in the top graphs are much higher when ğ‘› is \nlarge, and lower when ğ‘ is large. Of the four combinations of ğ‘› and ğ‘, the only one with \nğ‘›< ğ‘ is ğ‘= 100, ğ‘›= 50. This had the worst results in terms of true-positive rates, \nwhich was to be expected. It seems that SSST gives better results with stars than with \nchains. Perhaps stars are easier to approximate by wrong trees than chains.  \nSummarizing results from 500 datasets in a single bar has the disadvantage that you \ncannot compare the four systems for any specific dataset. But in many cases the \nâ€œwhiskersâ€ are close to the tops of the bars, showing that there is not too much \nvariation within each set of 500 datasets.  \n \n\n11 Experiments \n \n \n112 \n \n\n \n11.2 Experiments with systems for storing trees \n \n113 \n \nFigure 11.2 (previous page and this page). Comparison of four different systems for storing \ntrees in SSST. Each bar-chart shows one measure of how well the four systems did on eight sets \nof 500 datasets. The heights of the bars are the median values and the â€œwhiskersâ€ show the 25% \nand 75% quartiles.  \n \nVariation with single datasets \nSSST is a random algorithm, so even with a single dataset the results might vary from \none run to the next. Figure 11.3 shows how the values vary between different runs on \neight particular datasets. Almost all the â€œwhiskersâ€ are very close to the tops of the bars, \nshowing that there is little variation between runs.  \n\n11 Experiments \n \n \n114 \n \nFigure 11.3. The variation between different runs of SSST on eight datasets. These bar-charts \nshow the same things as Figure 11.2 except that each bar corresponds to 500 runs of the \nalgorithm on the same dataset.  \n \n\n \n11.3 Experiments with non-forests \n \n115 \nFalse-positives in chains \nThe measures in Figure 11.2 are mostly lower for chains than for stars. One question \nthat arises is whether there is some pattern to the high-probability graphs that are \nvisited when the true graph is a chain. For example, do they tend to have false-positive \nedges between nodes that are two apart in the true graph?  \nFigure 11.4 shows the expected proportions of false-positives that were of this type, for \nthe chain graphs, as produced by SSST. These proportions are all low, showing that \nthere was not much tendency to find these edges. This is somewhat surprising, since it \nmeans that graphs that link further-apart nodes have higher probabilities or are more \nlikely to be visited. On the other hand, the interquartile ranges are large.  \n \n \nFigure 11.4. Expected proportion of false-positives that link nodes that are two apart in the true \ngraph, for the chain graphs.  \n \n11.3 Experiments with non-forests  \nThe experiments in this section address the question of whether restricting attention to \ntrees gives reasonable results in the case that the true graph is not a tree or forest, but \nis locally tree-like (see section 6.2). I used SSST, with System A, on datasets generated \nfrom graphs that were generated from the second ErdÅ‘sâ€“RÃ©nyi model, where the \nnumber of edges is fixed. In this section I will call this model ğº(ğ‘, ğ‘€), where ğ‘ is the \nnumber of nodes and ğ‘€ is the number of edges. Theorem 6.2 implies that these graphs \nshould have few short cycles and thus be locally tree-like.  \n\n11 Experiments \n \n \n116 \nFor these experiments, generating a dataset consists of randomly generating a graph, \nthen creating a covariance matrix, and finally generating from the multivariate Gaussian \ndistribution. To ensure that the graphs had some cycles, I chose ğ‘€ to be greater than \nğ‘âˆ’1.  \nResults are shown in Figure 11.5, in the light grey bars. The first and second bars in \neach group correspond to stars and chains and are taken from Figure 11.2. These are \nshown for comparison. In the second bar-chart, the white bars show the true values of \nğ¸ğ‘‡ğ‘ƒğ‘… for the datasets that correspond to ErdÅ‘sâ€“RÃ©nyi graphs. These were calculated \nusing the MTT-based methods from chapter 8.  \nBecause the algorithm is restricted to trees, all the graphs in the posterior distribution \nhave ğ‘âˆ’1 edges. But the true graphs have ğ‘€ edges, and ğ‘€> ğ‘âˆ’1. So it is impossible \nfor any graph in the posterior distribution to achieve a true-positive rate greater than \n(ğ‘âˆ’1)/ğ‘€. These maximum achievable true-positive rates are shown by thick lines in \nthe second and third bar-charts.  \nIn several cases the results for the ErdÅ‘sâ€“RÃ©nyi graphs are better than the results for \nthe chains. In all cases they are at least similar. Overall, the values are reasonably high. \nThis provides some evidence that the restriction to trees is acceptable for these locally \ntree-like ErdÅ‘sâ€“RÃ©nyi graphs.  \nSSST gives very similar values of ğ¸ğ‘‡ğ‘ƒğ‘… to MTT for the datasets with ğ‘= 30, ğ‘›= 500 \nand ğ‘= 100, ğ‘›= 50. This means that SSST estimates ğ¸ğ‘‡ğ‘ƒğ‘… very accurately in these \ncases. With the other datasets it somewhat overestimates ğ¸ğ‘‡ğ‘ƒğ‘….  \n11.4 Experiments with MCMC on forests and trees \nAbout the experiments \nThe MCMC algorithms used in this section are McmcF, where only forests are \nconsidered, and McmcT, where only trees are considered. These are described in \nsection 10.1. The datasets were generated in the same way as the ones in section 11.1, \nand the values of ğ‘ and ğ‘› are stated below. McmcF and McmcT have two parameters \nthat can be set, ğœğº and ğœğ‘–ğ‘—. The former is used in the updates to the graph structure and \nthe latter is used in the updates to the covariance matrix.  \nThe findings of this section, in summary, are that McmcF usually fails to mix, and that \nMcmcT mixes but takes much longer than SSST to give useful results. First I will \ndescribe the experiments and then I will discuss the results.  \n \n\n \n11.4 Experiments with MCMC on forests and trees \n \n117 \n \nFigure 11.5. The light grey bars (the third bar in each group) show measures of how successful \nSSST is with non-forest graphs generated from ğº(ğ‘, ğ‘€), the second ErdÅ‘sâ€“RÃ©nyi model. Each \nbar corresponds to 100 datasets, each generated from a covariance that matches a different \ngraph. The darker bars show values from section 11.2, for comparison, and the white bars show \nthe true values of ğ¸ğ‘‡ğ‘ƒğ‘…. The thick lines show maximum achievable values for the experiments \nwith the ErdÅ‘sâ€“RÃ©nyi graphs.  \n \n\n11 Experiments \n \n \n118 \nExperiments with McmcF \nFirst dataset \nThe first set of experiments was done on a dataset with ğ‘= 5 and ğ‘›= 30, generated \nfrom a distribution for which the true graph was a star. McmcF was run with a range of \nvalues of ğœğº (specifically, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, and 5), for 20 million \niterations in each case. In the true distribution on forests, the probabilities of the top \ntwo graphs are 0.39 and 0.08. With ğœğº= 0.05, McmcF got the top graph right but \nestimated its probability to be 0.96. With the other values of ğœğº, McmcF got the top \ngraph wrong and estimated the probabilities of these wrong top graphs to be 0.43 or \nmore. Clearly McmcF failed totally. (In this and the other MCMC experiments there was \nno problem with the updates of Î“. In this case ğœğ‘–ğ‘— was 0.01 and the acceptance-rate for \nupdates of Î“ was between 77% and 80%.)  \nIn all cases the most-visited graph was a tree. To see whether McmcF might be getting \nstuck in local optimums, and what kinds of graphs these local optimums might be, I \ncalculated the scores of all the possible forests, in other words the exact true posterior \ndistribution. The top 125 graphs were treesâ€”all the trees had higher scores than all the \nunconnected forests. The top graph was 1.1 million times more likely than the top \nunconnected forest.  \nSecond dataset \nThe second set of experiments was done on a dataset with ğ‘= 5 and ğ‘›= 10. Again the \ntrue graph was a star. Because ğ‘› is smaller this might be expected to give a less peaked \nposterior distribution and mix better (Friedman & Koller 2003). McmcF was run with \nthe same values of ğœğº as for the first dataset, for 20 million iterations in each case. For \nseven of these experiments it got the top graph wrong. With ğœğº= 5 it got the top graph \nright, and with ğœğº= 0.5 it got the top three graphs right and their probabilities right to \nwithin 8%. This was much better than with the first dataset, though it was surprising \nthat the two values of ğœğº that worked best were so far apart. \nAgain the most-visited graph was a tree in all the experiments. In the exact true \nposterior distribution, the top 125 graphs were again all trees. But this time the top \ngraph was only 13.8 times more likely than the top unconnected forest.  \nThird dataset \nThe third dataset had ğ‘= 5, ğ‘›= 10, and the true graph as in Figure 11.6. The purpose \nof this was to see whether McmcF might mix better when the true graph is not a tree. \nMcmcF was run for ğœğº= 0.1, 0.2, and 0.5, for 20 million iterations in each case. All three \ntimes, it correctly identified the two most likely graphs. Figure 11.7 shows the top few \ngraphs in the true posterior distribution and for McmcF with ğœğº= 0.1 (of the three \nvalues, this gave the highest acceptance rate for graph updates, 16%). McmcF seems to \nhave got stuck in certain trees for longer than it should have.  \n \n\n \n11.4 Experiments with MCMC on forests and trees \n \n119 \n \nFigure 11.6. The true graph, for the third dataset for McmcF.  \n \nTrue  \ndistribution: \n \n \n0.0299 \n0.0284 \n0.0148 \n \n0.0145 \n \nMcmcF: \n \n \n0.0438 \n0.0401 \n0.0206 \n \n0.0205 \nFigure 11.7. For the third dataset used with McmcF, the four most likely forests and their proba-\nbilities according to the true posterior distribution (restricted to forests) and according to \nMcmcF with ğœğº= 0.1.  \n \nOther datasets \nMcmcF works less well with ğ‘> 5. For example, it often spends more than half the time \nat a single graph or has an acceptance rate for graph updates of less than 0.01%. It \nworks well for ğ‘= 4, and gives the correct posterior distribution, but this is no use \nsince exhaustive search of all the possible graphs takes only a few seconds when ğ‘= 4.  \nExperiments with McmcT \nI ran McmcT on datasets like the ones used in section 11.2, with ğ‘= 30 or ğ‘= 100, \nusing a range of values of ğœğº. It failed to mix with the datasets where ğ‘›= 500, but it \nmixed well with the ones where ğ‘›= 50. Setting ğœğ‘–ğ‘—= 0.01 usually seems to give an \nacceptance rate of 40% or 50%, which in MCMC is generally regarded as good. Setting \nğœğº requires more trial and error. If ğœğº is too small or too big, the updates almost never \nget accepted, but if it is chosen appropriately then McmcT mixes, at least with the \ndatasets where ğ‘›= 50.  \nThe question arises of whether McmcT gives a reasonable approximation to the true \nposterior distribution in a reasonable amount of time, or whether it merely mixes well \n\n11 Experiments \n \n \n120 \namong graphs that do not have high scores. I ran McmcT on two datasets with ğ‘= 30 \nand ğ‘›= 50, and recorded several quantities at certain intervals, to see how well it was \nmixing and how long it was taking. I used ğœğº= 0.5. Figure 11.8 shows the number of \ndistinct graphs visited, ğ¸ğ‘‡ğ‘ƒğ‘…, and the true score of the most-visited graph, for these \ntwo experiments. (For McmcT, the true score of the most-visited graph can be used as a \nmeasure of how well the algorithm does. This is analogous to the highest score found in \nSSST.)  \nFor both datasets, the number of distinct graphs visited steadily increased. McmcT \nvisited far more graphs with the chain dataset than with the star dataset, again showing \nthat these two types of graph are greatly different. According to the other two \nquantities, McmcT did somewhat less well than SSST for both graph-shapes. (For the \nSSST results see Figure 11.2.) Taking the chain as an example, with McmcT ğ¸ğ‘‡ğ‘ƒğ‘… \nsettled around 0.144, but with SSST its median was 0.235, and with McmcT the true \nscore of the most-visited graph settled around âˆ’2125, but with SSST the highest score \nfound was âˆ’2107 (which is better).  \nSSST was only run for 20 seconds, but to do 10 million iterations took McmcT 7 hours \nand 13 minutes for the star and 9 hours and 5 minutes for the chain. Figure 11.8 shows \nthat at least 1 million iterations are needed to get a reasonable result. Overall, SSST \nseems to be better in practice than McmcT, firstly because with McmcT it is necessary to \nexperiment to find suitable values of ğœğ‘–ğ‘— and ğœğº, and secondly because, once the main \nalgorithms are underway, SSST gives reasonable results much faster. For these reasons, \nI use SSST in the subsequent sections of this chapter.  \nThe failure of McmcF \nAsymptotically, McmcF produces the true posterior distribution. Given a long enough \ntime, it would produce a good approximation. But for ğ‘> 5 or large ğ‘›, if it is run for a \nreasonable length of time or a reasonable number of iterations, it does not give a good \napproximation of the posterior distribution and sometimes completely fails to mix.  \nThe reason is probably the high peaks that often seem to appear in the posterior distri-\nbutions. When the true graph is a tree, all the high-scoring graphs tend to be trees, and \nthese trees have much higher scores than the highest-scoring unconnected forests. But \nfor McmcF to get from a tree to another tree it has to first visit an unconnected forest. \n(Obviously, this problem does not arise with McmcT.)  \nFor example, with the first dataset discussed above, the unconnected forests all had far \nlower scores, in the area of a million times lower, than the high-ranking trees. This \nshows that the posterior distribution is very peaked and multimodal. There was also a \npeak at the top-ranking graph, which is also the true graph. Its score is 5 times the score \nof the next graph. This peak probably results from ğ‘› being large compared to ğ‘.  \nThe obvious way to adapt McmcF would be to have different types of moves in the \ngraph spaces. For example, you could add or remove two or more edges at a time. The \nsystem for storing forests described in section 9.3 would have to be adapted, but some \nof the ideas would still be useful. Another possible adaptation would be to sometimes \nmove edges, instead of just adding and removing themâ€”though the algorithm might \nend up only visiting trees, in which case you might as well use McmcT.  \n\n \n11.4 Experiments with MCMC on forests and trees \n \n121 \n \n \n \n \n \n \n \n \n \n \n \nFigure 11.8. How three aspects of the estimated posterior distribution change over 10 million \niterations of McmcT on two datasets.  \n \n\n11 Experiments \n \n \n122 \nDifferent types of graph-moves is not the only possibility. Karagiannis & Andrieu \n(2012) describe a method that addresses the wider problem of reversible-jump MCMC \nalgorithms getting stuck. To make a proposal, their algorithm chooses a dimension-\nchanging move and then moves about in the new model-space to find parameters that \nwill give higher acceptance probability. The method has desirable asymptotic \nproperties, and it can be applied to the MCMC method of Giudici & Green (1999), \nthough this application is not addressed in the paper.  \nSimilar observations in other research \nFriedman & Koller (2003) discuss in some depth the issue of MCMC for graphical model \nstructure-learning failing to mix well. Their paper is mainly about directed graphical \nmodels but also covers undirected ones. They say that MCMC on the graph structure is \nslow to mix because the posterior distribution is often peaked, meaning that \nneighbouring graphs have very different scores. Even small changes such as removing \nan edge cause large changes in the posterior probability. If ğ‘› is large then the posterior \nwill be sharply peaked at a single model. This corresponds to what I found with McmcF \nand the first dataset.  \nFriedman & Koller (2003) state that â€œin small domains with a substantial amount of \ndata, it has been shown that the highest scoring model is orders of magnitude more \nlikely than any other.â€ But the source they cite, Heckerman et al (1997), only shows this \nin one specific example.  \nAltomare et al (2011) say it is now recognized that MCMC methods are not efficient for \nthese problems, because of the huge number of possible graphs and the multimodal \nposterior distributions. Scott & Carvalho (2008) make similar comments. Brooks et al \n(2003) discusses the general issue that in reversible-jump MCMC it is difficult to come \nup with proposals that will get accepted a reasonable proportion of the time. They \nsuggest adapting the method of Giudici & Green (1999) by retaining the previous values \nof the elements of the covariance matrix and using them in choosing the proposed new \nvalues.  \n11.5 Experiments with methods for trees \nOf the quantities in section 11.2, the MTT-based method from section 8.3 can only \nproduce ğ¸ğ‘‡ğ‘ƒğ‘…. Figure 11.9 shows ğ¸ğ‘‡ğ‘ƒğ‘… for SSST, run for three different lengths of time, \nand the MTT-based method. These experiments used the same settings as in section \n11.2, and 100 datasets for each group of four bars. The MTT-based method is \ndeterministic and gives exact values, so this is really an assessment of how well SSST \napproximates the true posterior distribution. SSST does reasonably well, though it \noverestimates ğ¸ğ‘‡ğ‘ƒğ‘… in the case of the chain graphs. Perhaps SSST only visits high-\nprobability graphs, but the low-probability graphs have fewer true-positives and still \nmake a noticeable difference to ğ¸ğ‘‡ğ‘ƒğ‘….  \n \n\n \n11.6 Experiments with graph prior distributions \n \n123 \n \nFigure 11.9. Comparison of SSST with the MTT-based method. Each group of four bars \ncorresponds to one combination of covariance matrix and ğ‘›, and 100 datasets.  \n \n11.6 Experiments with graph prior distributions \nTo see whether hub-encouraging graph priors can give better results than the uniform \ngraph prior, in the case when the true graph is a star, I ran SSST with the following four \ngraph priors.  \nâ€¢ \nthe hub-encouraging prior from section 5.7 with ğœ“= 1 and ğœ’= 0.9ğ‘ \nâ€¢ \nthe hub-encouraging prior from section 5.7 with ğœ“= 0.01 and ğœ’= 0.9ğ‘ \nâ€¢ \nthe prior defined by ğ‘(ğº) âˆexp(maxdeg(ğ‘£)) \nâ€¢ \nthe uniform graph prior.  \nSmall values of ğœ“ were used because larger values gave almost no improvement over \nthe uniform prior. The third prior was intended to be strongly hub-encouraging. It gives \nmuch higher probability to graphs that have a single hub, and much higher probability \nto graphs where that hub has higher degree. The four priors were used on the datasets \nfrom section 11.2 for which the true graph was a star, and the results are shown in \nFigure 11.10.  \n \n \n\n11 Experiments \n \n \n124 \n \nFigure 11.10. Experiments to compare hub-encouraging priors with the uniform prior, on four \nsets of datasets. Each bar-chart shows one measure of how well the algorithm did. Each group of \nfour bars shows three hub-encouraging priors and the uniform prior. (As before, each bar shows \nresults with 500 datasets generated from the distribution with the same covariance matrix.)  \n \n\n \n11.7 Experiments with forests, trees, and decomposable graphs \n \n125 \nThe first hub-encouraging prior, with ğœ“= 1  and ğœ’= 0.9ğ‘, gave little or no \nimprovement over the uniform prior, and the second, with ğœ“= 0.01, gave some \nimprovement. Overall the third prior was the best.  \nEvidently, graph priors need to give much greater probability to some graphs than to \nothers if they are to have any effect on the posterior. This is presumably because among \nthe marginal likelihoods of all the graphs, some values are many orders of magnitude \ngreater than others. This suggests that one should look at the range of marginal likeli-\nhoods and then decide a suitable range for the graph prior, but that would go against \nthe fundamental principles of Bayesian inference.  \nAs in section 11.2, the datasets with ğ‘= 100 and ğ‘›= 50 gave the lowest values. But 0.5 \nor 0.6 are still not bad for the quantities in the second and third bar-charts.  \n11.7 Experiments with forests, trees, and decomposable \ngraphs \nThe final set of experiments are a further investigation of whether restricting to trees or \nforests is sensible. I compared SSST and SSSF with one of the original versions of Jones \net al (2005)â€™s stochastic shotgun search algorithm. Jones et al (2005) described \nversions of their algorithm for both decomposable and general graphs, but found that \nsearching general graphs â€œbecomes very challengingâ€ as ğ‘ increases past 15. For this \nreason I used the version that is restricted to decomposable graphs. I will call this SSSD. \n(Details of how my programs stored decomposable graphs are given at the end of \nsection 10.1.)  \nThe same datasets were used as in section 11.2. The algorithms were all run for 60 \nseconds on each dataset. For SSSF the values of ğœ” used in previous sections were too \nbig, because there are often not that many possible moves, so for all the experiments in \nthis section I used ğœ”= ğ‘/2.  \nThe results are shown in Figure 11.11. According to the second and third bar-charts, \nSSST and SSSF did better than SSSD when ğ‘›= 500 and the true graph was a chain, SSSD \ndid best when ğ‘›= 50, and the three algorithms did roughly as well as each other in the \nother cases. Overall these bar-charts provide some further reassurance that the \nrestriction to trees or forests is reasonable.  \nAs shown by the first bar-chart in Figure 11.11, SSST and SSSF visited far more graphs \nin the same amount of time than SSSD. But the three types of graph have very different \nimplementations. It might be said that SSSD was not given enough time to visit a \nreasonable number of graphs. So I repeated the experiment but ran each algorithm for \n500 iterations rather than 60 seconds.  \nThe results are shown in Figure 11.12. On average, SSSD took 14.9 times longer than \nSSST. But SSST still did better than SSSD according to some of the groups of bars and \nnot much worse according to the others. (Obviously it is still not completely fair to \ncompare the numbers of graphs visited by the three algorithms.)  \n \n\n11 Experiments \n \n \n126 \n \nFigure 11.11. Comparison of SSST, SSSF, and SSSD, using the same datasets as in section 11.2. \nEach algorithm was run for 60 seconds.  \n \n\n \n11.7 Experiments with forests, trees, and decomposable graphs \n \n127 \n \nFigure 11.12. Comparison of SSST, SSSF, and SSSD. Each algorithm was run for 500 iterations, to \ngive SSSD a chance to visit a reasonable number of graphs.  \n \n\n \n128 \n12 \nConclusions \n12.1 Restricting to forests and trees \nThe reasons in favour of restricting attention to forests or trees, in Bayesian structure-\nlearning of graphical models, can be summarized as follows. Chapter 6 showed that \nthere has been plenty of theoretical and applied research using forests and trees and \ngave theoretical reasons in favour of them. Chapters 7 and 8 gave fast algorithms that \ncan be used on them. Chapter 11 provided empirical evidence based on several \nexperiments. Firstly, SSST gave good results in terms of ğ¸ğ‘‡ğ‘ƒğ‘… and the other measures, \nespecially for star graphs and even when ğ‘›< ğ‘. Secondly, SSST did reasonably well with \nsparse and locally tree-like graphs that were not forests. Thirdly, SSST did almost as \nwell as SSSD according to the true-positive rates and much better according to the \nnumbers of graphs visited, though these experiments had the drawbacks that the true \ngraphs were trees and the algorithms are not easy to compare because their implemen-\ntations are so different. SSSF also did better than SSSD on some groups of datasets.  \nBayesian structure-learning has no difficulty with ğ‘›< ğ‘ and often gives good results in \nterms of true-positive rates, though naturally it is unlikely to give high ğ¸ğ‘‡ğ‘ƒğ‘… if ğ‘›â‰ªğ‘. \nRestricting to forests or trees automatically overcomes the problem of stars being \nmisidentified as cliques. It would be interesting to compare graphs produced by the \nalgorithms in Albieri (2010) with graphs produced by the Chowâ€“Liu algorithm and the \nadaptations of it in chapter 7.  \nWhich method is best depends on the purpose of the analysis. But overall it seems \nentirely plausible that there are practical circumstances in which it would be preferable \nto do Bayesian structure-learning on forests and trees, rather than on decomposable \ngraphs or all graphs.  \n12.2 Graph distributions and theoretical results \nThe two ways of looking at graph distributions from chapter 5 should be helpful in \nclarifying ideas about graph distributions. Factored distributions are at least \ntheoretically useful because they can be used in algorithms based on the Chowâ€“Liu \nmethod and MTT.  \nNone of the distributions previously used as priors in Bayesian structure-learning is \nsatisfactory for encouraging hubs. The criteria and priors proposed in section 5.7 are \nmore suitable for this purpose.  \n\n \n12.3 Algorithms for structure-learning with forests or trees \n \n129 \nThe theoretical results in section 6.2 showed how the claim that sparse graphs are \nlocally tree-like can be made rigorous and used to justify restricting attention to forests \nor trees. A possible topic for future research is whether similar theoretical or empirical \nresults can be found for other random graph models such as scale-free graphs.  \nChapter 4 gave a proof of correctness for the algorithm for recursive thinning that is \nused in the R package gRbase, which is one of the main constituents of the large-scale \nproject called â€œgRaphical models in Râ€. It also gave a simpler algorithm that is \nsometimes faster.  \n12.3 Algorithms for structure-learning with forests or trees  \nChapter 7 discussed how methods based on the Chowâ€“Liu algorithm can be used with \nGGMs to find the maximum-likelihood tree, the optimal forest using likelihood \npenalized by AIC or BIC, and the MAP forest in Bayesian structure-learning. Chapter 8 \nshowed how the method based on MTT can be used to find certain types of information \nabout the posterior distribution over all trees. All these methods are very fast compared \nto any attempt to approximate the whole posterior distribution, and they all work with \nfactored prior distributions. The drawbacks are that they can only answer certain types \nof questions.  \nChapter 9 presented efficient systems for storing forests and trees so that single-edge \nmoves could easily be chosen uniformly at random and the stored information could \neasily be updated. It might be worth investigating other ways of choosing edge-moves \nin trees, such as adding an edge, identifying the cycle that results, and then removing an \nedge, to see if they perform better.  \n12.4 Computer experiments  \nMany of the experiments in chapter 11 used only four different graphs and two \ndifferent values of ğ‘› (the number of data), so there is obviously plenty of scope for \nmore experiments. For example, the algorithms could be used on data generated from \ndifferent shapes of graphsâ€”perhaps ErdÅ‘sâ€“RÃ©nyi graphs of the first type, scale-free \ngraphs, or specific real-world graphs. It would also be interesting to try them on gene \nexpression data or financial data. Most of the experiments used only trees, so further \nresearch might use forests instead.  \nThe SSS algorithms are designed to be run on parallel or distributed processors. For \ndatasets with much higher ğ‘ it would probably be necessary to use multiple processors. \nIt is certainly advisable to use a fast programming languageâ€”I found that Java is 100 \ntimes faster than R.  \nIn experiments using SSST, the system for storing trees had some effect on how many \ngraphs were visited but almost no effect on ğ¸ğ‘‡ğ‘ƒğ‘… or the other measures. ğ¸ğ‘‡ğ‘ƒğ‘… and the \ntrue-positive rates in the top graphs were generally high, especially for the star graphs. \nEspecially good was the result that ğ¸ğ‘‡ğ‘ƒğ‘… was roughly 0.5 for the datasets with ğ‘= 100, \nğ‘›= 50, and the true graph a star. SSST did reasonably well on datasets generated from \nsparse and locally tree-like graphs that were not forests. This was the first piece of \nempirical evidence that it may be sensible to restrict attention to trees.  \n\n12 Conclusions \n \n \n130 \nMcmcF did not mix well. McmcT mixed well but took far longer than SSST to give \nreasonable results, and required trial-and-error to find suitable values of the para-\nmeters for the proposal distributions.  \nIn the experiments with the MTT-based methods, the approximations of the true \nposterior distributions produced by SSST gave higher values of ğ¸ğ‘‡ğ‘ƒğ‘… than the true \nposterior distribution. This was especially the case for chain graphs. In a sense this is \nevidence that SSST works well, though it does not address the question of whether \nrestricting to trees is sensible.  \nNext were experiments with graph prior distributions that were designed to encourage \nhubs. The priors proposed in chapter 5 had small effects in some cases, but the more \nextreme prior with ğ‘(ğº) âˆexp(max deg(ğ‘£)) was better at identifying the hub when \nğ‘›< ğ‘. If graph priors are intended to encourage hubs then they need to give much \nhigher probability to some graphs than others. Obviously there is a large amount of \nscope for further experiments with graph priors that encourage hubs, scale-free degree \nsequences, or other features that are believed to be common in real-world networks. \nHub-encouraging priors could also be used with other algorithms for GGM structure-\nlearning, such as the MCMC method of Green & Thomas (2013), which works with \njunction trees.  \nLastly, section 11.7 compared SSST, for trees, SSSF, for forests, and SSSD, for \ndecomposable graphs. Given the same amount of time, SSST and SSSF visited far more \ngraphs than SSSD. In terms of ğ¸ğ‘‡ğ‘ƒğ‘… and the true-positive rate in the top graph, SSST \nand SSSF did better on two sets of datasets, SSSD did best on five, and all three did very \nclose to equally well on one. These results gave further evidence that restricting \nattention to trees or forests may be sensible. The class of decomposable graphs is \nbigger but this was outweighed by the computational simplicity of trees or forests. \nFurther research might compare SSST and SSSF with SSSD on data generated from \ngraphs that are not forests but are locally tree-like.  \n \n\n \n131 \nAppendix I: Graph enumerations \nThis appendix presents the results of some enumerations of decomposable graphs. \nThese numbers are not important or meaningful. However, they have never been found \nbefore, as far as I can tell.  \nTable A1 shows the number of decomposable graphs with ğ‘› nodes, for ğ‘› up to 13. The \nnumbers for ğ‘› up to 12 are from Sloane (2011), which is sequence A058862 on a \nwebsite called Online Encyclopedia of Integer Sequences. The number for ğ‘›= 13 does \nnot seem to have appeared anywhere before. I worked it out using a formula on the \nsame webpage and sequence A007134 from the same website. The method for working \nout these numbers is described in Wormald (1985).  \nTable A2 shows the number of decomposable graphs with ğ‘›= 9 nodes, for each \npossible number of edges. The analogous numbers for ğ‘› up to 8 are given in Table 7.1 of \nArmstrong (2005). I found the numbers for ğ‘›= 9 by writing a program that does a \nmaximum cardinality search (Tarjan & Yannakakis 1984) on every possible graph, to \ntest whether it is decomposable. This program took one week to run on an average \ndesktop computer. But a parallelized version running on a high-powered computer, \nwith twelve 3GHz processors, did it in 6.5 hours. To do the same thing for ğ‘›= 10 would \ntake much longer, because there are 1024 times more graphs.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\nAppendix I: Graph enumerations \n \n \n132 \nn \nNumber of decomposable \ngraphs with n nodes \nPercent of graphs that  \nare decomposable \n1 \n1 \n \n100 \n2 \n2 \n \n100 \n3 \n8 \n \n100 \n4 \n61 \n \n95 \n5 \n822 \n \n80 \n6 \n18 154 \n \n55 \n7 \n617 675 \n \n29 \n8 \n30 888 596 \n \n12 \n9 \n2 192 816 760 \n \n 3.2 \n10 \n215 488 096 587 \n \n 0.61 \n11 \n28 791 414 081 916 \n \n 0.080 \n12 \n5 165 908 492 061 926 \n \n 0.0070 \n13 \n1 234 777 416 771 739 141 \n \n 0.00041 \nTable A1. The number of decomposable graphs with ğ‘› nodes, for ğ‘› up to 13. \n \n \ne \n ğ‘›(ğ‘’) \ne \n ğ‘›(ğ‘’) \ne \n ğ‘›(ğ‘’) \ne \n ğ‘›(ğ‘’) \n0 \n1 \n10 \n59194170 \n19 \n170178120 \n28 \n1154547 \n1 \n36 \n11 \n94169376 \n20 \n130062807 \n29 \n430236 \n2 \n630 \n12 \n137060700 \n21 \n92533764 \n30 \n137718 \n3 \n7140 \n13 \n181199340 \n22 \n62171838 \n31 \n37800 \n4 \n58527 \n14 \n216312390 \n23 \n39638592 \n32 \n10080 \n5 \n364140 \n15 \n234891000 \n24 \n23221338 \n33 \n2100 \n6 \n1741530 \n16 \n237142836 \n25 \n12310704 \n34 \n252 \n7 \n6317460 \n17 \n227923920 \n26 \n 5983866 \n35 \n36 \n8 \n16933905 \n18 \n204956724 \n27 \n2699508 \n36 \n1 \n9 \n33969628 \n \n \n \n \n \n \nTable A2. The number of decomposable graphs with 9 nodes, for each possible number of edges. \nThe number of decomposable graphs with 9 nodes and ğ‘’ edges is ğ‘›(ğ‘’). \n \n\n \n133 \nAppendix II: Glossary of terms related \nto graphs \nSee also section 2.1. The definitions refer to a general graph ğº= (ğ‘‰, ğ¸). Vague terms are \nmarked â€œ(Vague.)â€ Some of the vague terms have been given precise definitions in \ncertain contexts, as described in the main text.  \nabsent An edge ğ‘’ is absent if ğ‘’âˆ‰ğ¸. (Most authors use â€œmissingâ€, which I think is worse, \nsince firstly it suggests there is something wrong, and secondly it is not the natural \nopposite of â€œpresentâ€, which is clearly the best word for what it means.)  \nchordal graph See section 2.1.  \nclique A maximal complete subgraph.  \ncomponent / connected component A maximal set of nodes such that there is a path \nbetween any pair of them.  \nconnected A graph is connected if for any two nodes ğ‘¢, ğ‘£âˆˆğ‘‰ there is a path from ğ‘¢ to ğ‘£.  \ncycle A cycle is a path (ğ‘¢1, ğ‘¢2, â€¦ , ğ‘¢ğ‘˜) where ğ‘˜â‰¥3 and (ğ‘¢ğ‘˜, ğ‘¢1) âˆˆğ¸. (In graph theory, \ncycles are also called â€œloopsâ€ or â€œcircuitsâ€â€”see for example Even 1979. In Van Lint & \nWilson 2001, a combinatorics book, they are called â€œpolygonsâ€.)  \ndecomposable graph See section 2.1.  \ndegree The degree of a node is the number of edges that are incident to it.  \ndense (Vague.) This is the opposite or negation of â€œsparseâ€, q.v. In BollobÃ¡s & Riordan \n(2011), dense graphs have Î˜(ğ‘›2) edges.  \ndirected edge An edge (ğ‘¢, ğ‘£) such that (ğ‘£, ğ‘¢) âˆ‰ğ¸. A directed edge (ğ‘¢, ğ‘£) is drawn as an \narrow from ğ‘¢ to ğ‘£.  \ndirected graph A graph in which all the edges are directed. (Pearl 1988, page 232, uses \nâ€œmultiply connected networkâ€ to mean a directed graph that is not necessarily a \nforest.)  \ndirected path \nIn a directed graph, a sequence of nodes ğ‘¢1, ğ‘¢2, â€¦ , ğ‘¢ğ‘˜ such that \n(ğ‘¢1, ğ‘¢2), (ğ‘¢2, ğ‘¢3), â€¦ , (ğ‘¢ğ‘˜âˆ’1, ğ‘¢ğ‘˜) âˆˆğ¸ and ğ‘¢ğ‘–â‰ ğ‘¢ğ‘— for ğ‘–â‰ ğ‘—. \ndistance The distance between two nodes is the number of edges on the shortest path \nbetween them.  \nforest A graph that has no cycles. It can also be defined as a graph whose connected \ncomponents are all trees, q.v. (Pearl 1988 calls directed forests â€œpolytreesâ€ and \nâ€œsingly connected networksâ€.)  \ngirth The girth of a graph is the length of the shortest cycle that it contains, or âˆ if it \nhas no cycles. So a graph is chordal (q.v.) if and only if its girth is either 3 or âˆ.  \nhub (Vague.) A node whose degree is large.  \nincident An edge (ğ‘¢, ğ‘£) is incident to a node ğ‘¤ if ğ‘¤= ğ‘¢ or ğ‘¤= ğ‘£.  \nleaf A node, especially in a tree or forest, whose degree is 1.  \n\nAppendix II: Glossary of terms related to graphs \n \n \n134 \nlength of a path The number of edges on the path.  \nlocally tree-like (Vague.) This has been interpreted in several ways, for example to \nmean that there are few short cycles or that there are none. See section 6.2.  \nmultiple edges More than one edge between the same pair of nodes.  \nneighbour A neighbour of ğ‘£ is a node ğ‘¢ such that (ğ‘¢, ğ‘£) âˆˆğ¸ or (ğ‘£, ğ‘¢) âˆˆğ¸.  \npath See also section 2.1.  \n(a) In an undirected graph, a path is a sequence of nodes ğ‘¢1, ğ‘¢2, â€¦ , ğ‘¢ğ‘˜ such that \n(ğ‘¢1, ğ‘¢2), (ğ‘¢2, ğ‘¢3), â€¦ , (ğ‘¢ğ‘˜âˆ’1, ğ‘¢ğ‘˜) âˆˆğ¸ and ğ‘¢ğ‘–â‰ ğ‘¢ğ‘— for ğ‘–â‰ ğ‘—.  \n(b) In a directed graph, a path is a sequence of nodes ğ‘¢1, ğ‘¢2, â€¦ , ğ‘¢ğ‘˜ such that either \n(ğ‘¢1, ğ‘¢2) âˆˆğ¸ or (ğ‘¢2, ğ‘¢1) âˆˆğ¸, either (ğ‘¢2, ğ‘¢3) âˆˆğ¸ or (ğ‘¢3, ğ‘¢2) âˆˆğ¸, â€¦, either (ğ‘¢ğ‘˜âˆ’1, ğ‘¢ğ‘˜) âˆˆ\nğ¸ or (ğ‘¢ğ‘˜, ğ‘¢ğ‘˜âˆ’1) âˆˆğ¸, and ğ‘¢ğ‘–â‰ ğ‘¢ğ‘— for ğ‘–â‰ ğ‘—. This is my definition, and it is non-\nstandard. (Under standard definitions, a path in a directed graph has to be directed, \nand what I call a path would probably be called an â€œundirected pathâ€.)  \n(c) For there to be a path between ğ´âŠ†ğ‘‰ and ğµâŠ†ğ‘‰ means that there is a path \nbetween some ğ‘¢âˆˆğ´ and some ğ‘£âˆˆğµ.  \npresent An edge ğ‘’ is present if ğ‘’âˆˆğ¸.  \nrooted forest A directed forest in which each component is a rooted tree. (Heckerman \net al 1995, page 226, calls these â€œbranchingsâ€.)  \nrooted tree See Definition 9.1 in section 9.2. A directed tree in which one node is \ndesignated the root, and the paths from the root to all the other nodes are directed \npaths. The text just after Definition 9.1 gives three other equivalent definitions. \n(Heckerman et al 1995, page 226, calls these â€œtree-like networksâ€. Pearl 1988, pages \n143 and 150, uses â€œcausal treeâ€ to mean a rooted tree graphical model.)  \nself-loop An edge from a node to itself (Mateti & Deo 1976, page 90). \nseparate (verb) Suppose ğ´, ğµ, ğ¶âŠ†ğ‘‰. If all paths from ğ´ to ğ¶ pass through ğµ, then ğµ \nseparates ğ´ from ğ¶. See Lauritzen (1996, page 6).  \nsimple A graph is simple if it has no self-loops or multiple edges (Mateti & Deo 1976).  \nsize The size of ğº is |ğ¸|, the number of edges. (This term is used in graph theory and in \nArmstrong et al 2009.) \nspan (verb) A graph ğ» spans a graph ğº= (ğ‘‰, ğ¸), or a set of nodes ğ‘‰, if it is connected \nand the nodes of ğ» are ğ‘‰. See also spanning.  \nspanning A spanning tree of a connected graph (ğ‘‰, ğ¸) is a tree (ğ‘‰, ğ¸â€²) such that ğ¸â€² âŠ†ğ¸. \nThis word is also used loosely in the phrase â€œspanning forestâ€ (Edwards et al 2010, \nLauritzen 2006). A spanning forest of a graph (ğ‘‰, ğ¸) is a forest (ğ‘‰, ğ¸â€²) such that ğ¸â€² âŠ†\nğ¸.  \nsparse (Vague.) A sparse graph is one that has few edges. For concrete definitions see \nsection 6.2.  \ntree A connected graph that has no cycles. It can also be defined as a connected forest. \nIn the machine-learning community, â€œtreeâ€ is sometimes used to mean forest (Bach \n& Jordan 2003, Bradley & Guestrin 2010), in which case trees are referred to as \nâ€œspanning treesâ€. \ntriangulated graph See section 2.1.  \nundirected edge An unordered pair (ğ‘¢, ğ‘£) âˆˆğ¸; alternatively, an ordered pair (ğ‘¢, ğ‘£) such \nthat (ğ‘¢, ğ‘£) âˆˆğ¸ and (ğ‘£, ğ‘¢) âˆˆğ¸.  \nundirected graph A graph in which all the edges are undirected. \nundirected path See path (b).  \n\n \n135 \nAppendix III: Asymptotic notations \nThese are used in chapter 6.  \nâ€¢ \nğ‘“(ğ‘›) = ğ‘‚(ğ‘”(ğ‘›)) means there are some numbers ğ‘˜ and ğ‘ such that |ğ‘“(ğ‘›)| â‰¤\nğ‘˜|ğ‘”(ğ‘›)| for all ğ‘›â‰¥ğ‘.  \nâ€¢ \nğ‘“(ğ‘›) = Î˜(ğ‘”(ğ‘›)) means there are some numbers ğ‘, ğ‘, ğ‘> 0 such that ğ‘ğ‘”(ğ‘›) â‰¤\nğ‘“(ğ‘›) â‰¤ğ‘ğ‘”(ğ‘›) for all ğ‘›â‰¥ğ‘. This implies that ğ‘“(ğ‘›) = ğ‘‚(ğ‘”(ğ‘›)).  \nâ€¢ \nğ‘“(ğ‘›) = ğ‘œ(ğ‘”(ğ‘›)) means that lim\nğ‘›â†’âˆ\nğ‘“(ğ‘›)\nğ‘”(ğ‘›) = 0. This implies that ğ‘“(ğ‘›) = ğ‘‚(ğ‘”(ğ‘›)).  \nâ€¢ \nğ‘“(ğ‘›)~ğ‘”(ğ‘›) means that lim\nğ‘›â†’âˆ\nğ‘“(ğ‘›)\nğ‘”(ğ‘›) = 1. This implies that ğ‘“(ğ‘›) = Î˜(ğ‘”(ğ‘›)). \nâ€¢ \nğ‘“(ğ‘›) = Î©(ğ‘”(ğ‘›)) means there are some numbers ğ‘˜ and ğ‘ such that |ğ‘“(ğ‘›)| â‰¥ğ‘˜|ğ‘”(ğ‘›)| \nfor all ğ‘›â‰¥ğ‘.  \nFor more notations of this type, and their origins, see chapter 3 of Cormen et al (2009).  \n \n\n \n136 \nReferences \nI have not read the publications that are in foreign languages, though I have read English translations \nwhere these are listed.  \n \nAbreu, G.C.G., Edwards, D., & Labouriau, R. (2010), High-dimensional graphical model search with the \ngRapHD R package. Journal of Statistical Software, 37 (1), 1â€“18.  \nAcid, S., De Campos, L.M., GonzÃ¡lez, A., Molina, R., & PÃ©rez de la Blanca, N. (1991), Learning with CASTLE. In \nKruse, R. & Siegel, P. (eds.), Symbolic and Quantitative Approaches to Uncertainty, Springer: Berlin. \n(Pages 97â€“106.)  \nAlbieri, V. (2010), A comparison of procedures for structural learning of biological networks. PhD thesis, \nUniversitÃ  degli Studi di Padova.  \nAlcobe, J.R. (2002), An incremental algorithm for tree-shaped Bayesian network learning. Proceedings of \nthe 15th European Conference on Artificial Intelligence, 350â€“354.  \nAldous, D.J. (1990), The random walk construction of uniform spanning trees and uniform labelled trees. \nSIAM Journal on Discrete Mathematics, 3, 450â€“465.  \nAldous, J.M. & Wilson, R.J. (2000), Graphs and Applications: An Introductory Approach. Springer-Verlag: \nLondon.  \nAlon, U. (2003), Biological networks: the tinkerer as an engineer. Science, 301, 1866â€“1867.  \nAlon, U. (2007), Network motifs: theory and experimental approaches. Nature Reviews: Genetics, 8, 450â€“\n461.  \nAlterovitz, G. & Ramoni, M.F. (2006), Discovering biological guilds through topological abstraction. AMIA \n2006 Symposium Proceedings, 1â€“5.  \nAltomare, D., Consonni, G., & La Rocca, L. (2011), Objective Bayesian search of Gaussian directed acyclic \ngraphical models. Technical report, UniversitÃ  di Pavia.  \nAnandkumar, A., Hassidim, A., & Kelner, J. (2011), Topology discovery of sparse random graphs with few \nparticipants. ACM Sigmetrics 2011: International Conference on Measurement and Modeling of \nComputer Systems.  \nAnandkumar, A., Tan, V.Y.F., Huang, F. & Willsky, A.S. (2012), High-dimensional structure learning of Ising \nmodels: local separation criterion. The Annals of Statistics, 40 (3), 1346â€“1375.  \nAnderson, E. (1935), The irises of the Gaspe peninsula. Bulletin of the American Iris Society, 59, 2â€“5.  \nAndersson, H. (1998), Limit theorems for a random graph epidemic model. The Annals of Applied \nProbability, 8 (4), 1331â€“1349.  \nApache \nSoftware \nFoundation \n(2012), \nFieldMatrix \n(Commons \nMath \n3.1.1 \nAPI). \nhttp://commons.apache.org/math/api-3.1.1/org/apache/commons/math3/linear/FieldMatrix.html. \nAccessed in February 2013.  \nArmstrong, H. (2005), Bayesian estimation of decomposable Gaussian graphical models. PhD thesis, \nUniversity of New South Wales.  \nArmstrong, H., Carter, C. K., Wong, K.F.K., & Kohn, R. (2009), Bayesian covariance matrix estimation using a \nmixture of decomposable graphical models. Statistics and Computing, 19, 303â€“316.  \nAtay-Kayis, A. & Massam, H. (2005), A Monte Carlo method for computing the marginal likelihood in \nnondecomposable Gaussian graphical models. Biometrika, 92 (2), 317â€“335.  \nBach, F.R. & Jordan, M.I. (2003), Beyond independent components: trees and clusters. Journal of Machine \nLearning Research, 4, 1205â€“1233.  \nBanerjee, O., El Ghaoui, L., & Dâ€™Aspremont, A. (2008), Model selection through sparse maximum likelihood \nestimation for multivariate Gaussian or binary data. Journal of Machine Learning Research, 9, 485â€“516.  \nBarabÃ¡si, A.-L. & Albert, R. (1999), Emergence of scaling in random networks. Science, 286, 509â€“512.  \n\n \nReferences \n \n137 \nBarabÃ¡si, A.-L. & Oltvai, Z.N. (2004), Network biology: understanding the cellâ€™s functional organization. \nNature Reviews: Genetics, 5, 101â€“113.  \nBarrett, T., Troup, D.B., Wilhite, S.E., Ledoux, P., Rudnev, D., Evangelista, C., Kim, I.F., Soboleva, A., \nTomashevsky, M., & Edgar, R. (2007), NCBI GEO: mining tens of millions of expression profilesâ€”\ndatabase and tools update. Nucleic Acids Research, 35, D760â€“D765.  \nBerge, C. (1973), Graphs and Hypergraphs. Northâ€“Holland: Amsterdam. (Translated from the French by E. \nMinieka.)  \nBerry, A., Blair, J.R.S., Heggernes, P., & Peyton, B.W. (2004), Maximum cardinality search for computing \nminimal triangulations of graphs. Algorithmica, 39 (4), 287â€“298.  \nBickel, P.J. & Levina, E. (2008), Regularized estimation of large covariance matrices. The Annals of Statistics, \n36 (1), 199â€“227.  \nBlair, J.R.S., Heggernes, P., & Telle, J.A. (2001), A practical algorithm for making ï¬lled graphs minimal. \nTheoretical Computer Science, 250, 125â€“141.  \nBogdanov, A., Mossel, E., & Vadhan, S. (2008), The complexity of distinguishing Markov random ï¬elds. \nApproximation, Randomization and Combinatorial Optimization: Lecture Notes in Computer Science, \n331â€“342.  \nBollobÃ¡s, B. (1978), Extremal Graph Theory. Academic Press: New York / London.  \nBollobÃ¡s, B. (1980), A probabilistic proof of an asymptotic formula for the number of labelled regular \ngraphs. European Journal of Combinatorics, 1, 311â€“316.  \nBollobÃ¡s, B. (1981), Threshold functions for small graphs. Mathematical Proceedings of the Cambridge \nPhilosophical Society, 90 (2), 197â€“206.  \nBollobÃ¡s, B. (1985), Random Graphs, 1st edition. Academic Press: New York / London.  \nBollobÃ¡s, B. (1998), Modern Graph Theory. Springer: New York.  \nBollobÃ¡s, B. (2001), Random Graphs, 2nd edition. Cambridge University Press: Cambridge.  \nBollobÃ¡s, B., Riordan, O., Spencer, J., & TusnÃ¡dy, G. (2001), The degree sequence of a scale-free random \ngraph process. Random Structures & Algorithms, 18 (3), 279â€“290.  \nBollobÃ¡s, B. & SzemerÃ©di, E. (2002), Girth of sparse graphs. Journal of Graph Theory, 39 (3), 194â€“200.  \nBollobÃ¡s, B. & Riordan, O. (2011), Sparse graphs: metrics and random models. Random Structures & \nAlgorithms, 39 (1), 1â€“38.  \nBondy, J.A. & Murty, U.S.R. (1976), Graph Theory with Applications. Macmillan: London.  \nBondy, J.A. & Murty, U.S.R. (2008), Graph Theory. Springer.  \nBornn, L. & Caron, F. (2011), Bayesian clustering in decomposable graphs. Bayesian Analysis, 6 (4), 829â€“\n846.  \nBorÅ¯vka, O. (1926a), O jistÃ©m problÃ©mu minimÃ¡lnÃ­m. PrÃ¡ce MoravskÃ© PÅ™Ã­rodovÄ•deckÃ© SpoleÄnosti Brno, 3, \n37â€“58. (In Czech with German summary.)  \nBorÅ¯vka, O. (1926b), PÅ™Ã­spÄ•vek k Å™eÅ¡enÃ­ otÃ¡zky economickÃ© stavby elektrovodnÃ½ch sÃ­tÃ­. ElektrotechnickÃ½ \nObzor, 15 (10), 153â€“154. (In Czech.)  \nBradley, J.K. & Guestrin, C. (2010), Learning tree conditional random fields. Proceedings of the 27th \nInternational Conference on Machine Learning, 127â€“134.  \nBritton, T., Janson, S., & Martin-LÃ¶f, A. (2007), Graphs with specified degree distributions, simple epidemics, \nand local vaccination strategies. Advances in Applied Probability, 39, 922â€“948.  \nBritton, T., Deijfen, M. & Liljeros, F. (2011), A weighted configuration model and inhomogeneous epidemics. \nJournal of Statistical Physics, 145, 1368â€“1384.  \nBroder, A. (1989), Generating random spanning trees. 30th Annual Symposium on Foundations of \nComputer Science, 442â€“447.  \nBrooks, S.P., Giudici, P., & Roberts, G.O. (2003), Efficient construction of reversible jump Markov chain \nMonte Carlo proposal distributions. Journal of the Royal Statistical Society, Series B (Statistical \nMethodology), 65 (1), 3â€“55.  \nBrummitt, C.D., Dâ€™Souza, R.M., & Leicht, E.A. (2012), Suppressing cascades of load in interdependent \nnetworks. Proceedings of the National Academy of Sciences, 109 (12), E680â€“E689.  \nBuntine, W. (1991), Theory refinement on Bayesian networks. Proceedings of the Seventh Conference \nAnnual Conference on Uncertainty in Artificial Intelligence, 52â€“60.  \nByrne, S. (2011), Hyper and structural Markov laws for graphical models. PhD thesis, University of \nCambridge.  \nCamerini, P.M., Fratta. L., & Maffiolo, F. (1980), The K best spanning arborescences of a network. Networks, \n10, 91â€“110.  \n\nReferences \n \n \n138 \nCarvalho, C.M., Massam, H., & West, M. (2007), Simulation of hyper-inverse Wishart distributions in \ngraphical models. Biometrika, 94 (3), 647â€“659.  \nCarvalho, C.M., Polson, N.G., & Scott, J.G. (2009), Handling sparsity via the horseshoe. Journal of Machine \nLearning Research: Workshop and Conference Proceedings, 5, 73â€“80.  \nCarvalho, C.M. & Scott, J.G. (2009), Objective Bayesian model selection in Gaussian graphical models. \nBiometrika, 96 (3), 497â€“512.  \nCastelo, R. & Roverato, A. (2006), A robust procedure for Gaussian graphical model search from microarray \ndata with p larger than n. Journal of Machine Learning Research, 7, 2621â€“2650.  \nCastelo, R. & Roverato, A. (2009), Reverse engineering molecular regulatory networks from microarray \ndata with qp-graphs. Journal of Computational Biology, 16 (2), 213â€“227.  \nCayley, A. (1856), Note sur une formule pour la reversion des sÃ©ries. Journal fÃ¼r die reine und angewandte \nMathematik / Journal for Pure and Applied Mathematics, 52, 276â€“284. (In French.)  \nCayley, A. (1889), A theorem on trees. Quarterly Journal of Pure and Applied Mathematics, 23, 376â€“378.  \nChandar, V.B. (2010), Sparse graph codes for compression, sensing, and secrecy. PhD thesis, Massachusetts \nInstitute of Technology.  \nChoi, M.J., Tan, V.Y.F., Anandkumar, A., & Willsky, A.S. (2011), Learning latent tree graphical models. \nJournal of Machine Learning Research, 12, 1771â€“1812.  \nChow, C.K. & Liu, C.N. (1968), Approximating discrete probability distributions with dependence trees. \nIEEE Transactions on Information Theory, 14 (3), 462â€“467.  \nChung, F. & Lu, L. (2002a), Connected components in random graphs with given expected degree \nsequences. Annals of Combinatorics, 6, 125â€“145.  \nChung, F. & Lu, L. (2002b), The average distances in random graphs with given expected degrees. \nProceedings of the National Academy of Sciences, 99 (25), 15879â€“15882.  \nChung, F., Lu, L., & Vu, V. (2003), Spectra of random graphs with given expected degrees. Proceedings of the \nNational Academy of Sciences, 100 (11), 6313â€“6318.  \nChung, F. & Lu, L. (2006), The volume of the giant component of a random graph with given expected \ndegrees. SIAM Journal of Discrete Mathematics, 20 (2), 395â€“411.  \nClimaco, J., Pascoal, M., & Gomes da Silva, C. (2008), Some computational improvements on finding the K \nshortest spanning trees. Technical report, Institute of Systems Engineering and Computers (INESC), \nCoimbra.  \nCoja-Oghlan, A., Mossel, E., & Vilenchik, D. (2009), A spectral approach to analysing belief propagation for \n3-colouring. Combinatorics, Probability and Computing, 18, 881â€“912.  \nColbourn, C.J., Day, R.P.J., & Nel, L.D. (1989), Unranking and ranking spanning trees of a graph. Journal of \nAlgorithms, 10, 271â€“286.  \nCooper, C. & Frieze, A. (2010), Random walks with look-ahead in scale-free random graphs. SIAM Journal \non Discrete Mathematics, 24 (3), 1162â€“1176.  \nCooper, G.F. & Herskovits, E. (1992), A Bayesian method for the induction of probabilistic networks from \ndata. Machine Learning, 9, 309â€“347.  \nCormen, T.H., Leiserson, C.E., Rivest, R.L., & Stein, C., (2009), Introduction to Algorithms, 3rd edition. MIT \nPress: Cambridge.  \nCosta, I.G., Roepcke, S., Hafemeister, C., & Schliep, A. (2008), Inferring differentiation pathways from gene \nexpression. Bioinformatics, 24, i156â€“i164.  \nCovert, M.W., Knight, E.M., Reed, J.L., Herrgard, M.J., & Palsson, B.O. (2004), Integrating high-throughput \nand computational data elucidates bacterial networks. Nature, 429, 92â€“96.  \nCowell, R.G., Dawid, A.P., Lauritzen, S.L., & Spiegelhalter, D.J. (2007), Probabilistic Networks and Expert \nSystems: Exact Computational Methods for Bayesian Networks. Springer Science+Business Media: New \nYork.  \nCowell, R.G. (2013), A simple greedy algorithm for reconstructing pedigrees. Theoretical Population \nBiology, 83, 55â€“63.  \nDai, H. (2008), Perfect sampling methods for random forests. Advances in Applied Probability, 40, 897â€“917.  \nDawid, A.P., & Lauritzen, S.L. (1993), Hyper Markov laws in the statistical analysis of decomposable \ngraphical models. The Annals of Statistics, 21 (3), 1272â€“1317. \nDempster, A.P. (1972), Covariance selection. Biometrics, 28, 157â€“175.  \nDethlefsen, C. & HÃ¸jsgaard, S. (2005), A common platform for graphical models in R: the gRbase package. \nJournal of Statistical Software, 14(17), 1â€“12.  \nDiestel, R. (2005), Graph Theory, 3rd edition. Springer: Berlin.  \nDirac, G.A. (1961), On rigid circuit graphs. Abhandlungen Mathematisches Seminar Hamburg, 25, 71â€“76.  \n\n \nReferences \n \n139 \nDobra, A., Hans, C., Jones, B., Nevins, J.R., Yao, G., & West, M. (2004), Sparse graphical models for exploring \ngene expression data. Journal of Multivariate Analysis, 90, 196â€“212.  \nDobra, A., Lenkoski, A., & Rodriguez, A. (2011), Bayesian inference for general Gaussian graphical models \nwith application to multivariate lattice data. Journal of the American Statistical Association, 106 (496), \n1418â€“1433.  \nDonnet, S. & Marin, J.-M. (2012), An empirical Bayes procedure for the selection of Gaussian graphical \nmodels. Statistics and Computing, 22 (5), 1113â€“1123.  \nDrton, M. & Perlman, M.D. (2007), Model selection for Gaussian concentration graphs. Biometrika, 91 (3), \n591â€“602.  \nDurbin, R., Eddy, S., Krogh, A., & Mitchison, G. (1998), Biological Sequence Analysis: Probabilistic Models of \nProteins and Nucleic Acids. Cambridge University Press: Cambridge.  \nDurrett, R. (2007), Random Graph Dynamics. Cambridge University Press: Cambridge.  \nEaton, D. & Murphy, K. (2007), Bayesian structure learning using dynamic programming and MCMC. \nProceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence, 101â€“108.  \nEdwards, D. (1995), Introduction to Graphical Modelling. Springer-Verlag New York.  \nEdwards, D., De Abreu, G.C.G., & Labouriau, R. (2010), Selecting high-dimensional mixed graphical models \nusing minimal AIC or BIC forests. BMC Bioinformatics, 11 (18).  \nEppstein, D. (1990), Finding the k smallest spanning trees. SWAT 90: 2nd Scandinavian Workshop on \nTheory, 38â€“47.  \nErdÅ‘s, P. & Gallai, T. (1960), GrÃ¡fok elÅ‘Ã­rt fokszÃ¡mÃº pontokkal. Matematikai Lapok, 11, 264â€“274. (In \nHungarian. Translation of title: â€œGraphs with given degrees of vertices.â€)  \nErdÅ‘s, P. & RÃ©nyi, A. (1959), On random graphs I. Publicationes Mathematicae Debrecen, 6, 290â€“297.  \nErdÅ‘s, P. & RÃ©nyi, A. (1960), On the evolution of random graphs. Publications of the Mathematical Institute \nof the Hungarian Academy of Sciences, 5, 17â€“61.  \nEven, S. (1979), Graph Algorithms. Pitman: London.  \nFaith, J.J., Driscoll, M.E., Fusaro, V.A., Cosgrove, E.J., Hayate, B., Juhn, F.S., Scheider, S.J., & Gardner, T.S. \n(2008), Many Microbe Microarrays Database: uniformly normalized Affymetrix compendia with \nstructured experimental metadata. Nucleic Acids Research, 36, D866â€“D870.  \nFisher, R.A. (1936), The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7 (2), \n179â€“188.  \nFleischer, R., Ge, Q., Li, J., Tian, S., & Wang, H. (2005), Approximating spanning tree with weighted inner \nnodes. Proceedings of the 6th International Conference on Parallel and Distributed Computing , \nApplications and Technologies, 660â€“664.  \nForney, D. (2003), The sum-product algorithm. Chapter 12 of lecture notes on Principles of Digital \nCommunication II (MIT OpenCourseWare), http://dspace.mit.edu/bitstream/handle/1721.1/36834 \n/6-451Spring-2003/NR/rdonlyres/Electrical-Engineering-and-Computer-Science/6-451Spring-\n2003/2F248E11-3358-4009-B5D8-753FC36DD925/0/chapter12.pdf. Accessed in February 2013.  \nFreeman, V.J. (1951), Studies on the virulence of bacteriophage-infected strains of Corynebacterium \ndiphtheriae. Journal of Bacteriology, 61 (6), 675â€“688.  \nFriedman, N., Linial, M., Nachman, I. & Peâ€™er, D. (2000), Using Bayesian networks to analyze expression \ndata. Journal of Computational Biology, 7 (3/4), 601â€“210.  \nFriedman, N. (2004), Inferring cellular networks using probabilistic graphical models. Science, 303, 799â€“\n805.  \nFriedman, N. & Koller, D. (2003), Being Bayesian about network structure: a Bayesian approach to \nstructure discovery in Bayesian networks. Machine Learning, 50, 95â€“125.  \nFriedman, J., Hastie, T., & Tibshirani, R. (2007), Sparse inverse covariance estimation with the graphical \nlasso. Biostatistics, 9 (3), 432â€“441.  \nGabow, H.N. (1977), Two algorithms for generating weighted spanning trees in order. SIAM Journal on \nComputing, 6 (1), 139â€“150.  \nGasse, M., Aussem, A., & Elghazel, H. (2012), An experimental comparison of hybrid algorithms for \nBayesian network structure learning. In Flach, P., De Bie, T., & Cristianini, N. (eds.), Machine Learning \nand Knowledge Discovery in Databases, Springer: Berlin/Heidelberg. (Pages 58â€“73.)  \nGavril, F. (1974), An algorithm for testing chordality of graphs. Information Processing Letters, 3 (4), 110â€“\n112.  \nGelman, A., Carlin, J.B., Stern, H.S., & Rubin, D.B. (2004), Bayesian Data Analysis, 2nd edition. Chapman & \nHall / CRC: Boca Raton.  \nGibbons, A. (1985), Algorithmic graph theory. Cambridge University Press: Cambridge.  \n\nReferences \n \n \n140 \nGibbs, N.E. (1969), A cycle generation algorithm for finite undirected linear graphs. Journal of the \nAssociation for Computing Machinery, 16 (4), 564â€“568.  \nGilbert, E.N. (1959), Random graphs. The Annals of Mathematical Statistics, 30 (4), 1141â€“1144.  \nGilbert, G.T. (1991), Positive definite matrices and Sylvesterâ€™s criterion. The American Mathematical \nMonthly, 98 (1), 44â€“46.  \nGiudici, P. (1996), Learning in graphical Gaussian models. Bayesian Statistics 5, 621â€“628.  \nGiudici, P. & Green, P.J. (1999), Decomposable graphical Gaussian model determination. Biometrika, 86 (4), \n785â€“801.  \nGoldberger, J. & Leshem, A. (2009), A Gaussian tree approximation for integer least-squares. Advances in \nNeural Information Processing Systems 22, 638â€“645.  \nGolumbic, M.C. (1980), Algorithmic Graph Theory and Perfect Graphs. Academic Press.  \nGreen, P.J. (1995), Reversible jump Markov chain Monte Carlo computation and Bayesian model \ndetermination. Biometrika, 82 (4), 711â€“732.  \nGreen, P.J. (2003), Trans-dimensional Markov chain Monte Carlo. In Green, P.J., Hjort, N.L., & Richardson, S. \n(eds.), Highly Structured Stochastic Systems, Oxford University Press: Oxford. (Pages 179â€“198.)  \nGreen, P.J. & Thomas, A. (2013), Sampling decomposable graphs using a Markov chain on junction trees. \nBiometrika, 100 (1), 1â€“20.  \nGuÃ©noche, A. (1983), Random spanning tree. Journal of Algorithms, 4, 214â€“220. (In French.)  \nGuo, J., Levina, E., Michailidis, G., & Zhu, J. (2011), Joint estimation of multiple graphical models. Biometrika, \n98 (1), 1â€“15.  \nGupta, A., Lafferty, J., Liu, H., Wasserman, L., & Xiu, M. (2010), Forest density estimation. COLT 2010: \nProceedings of the 23rd Annual Conference on Learning Theory, 394â€“406.  \nHan, S., Yoon, Y., & Cho, K.-H. (2007), Inferring biomolecular interaction networks based on convex \noptimization. Computational Biology and Chemistry, 31 (5â€“6), 347â€“354.  \nHarary, F. (1967). Graphs and matrices. SIAM Review, 9 (1), 83â€“90.  \nHeckerman, D., Geiger, D., & Chickering, D.M. (1995), Learning Bayesian networks: the combination of \nknowledge and statistical data. Machine Learning, 20, 197â€“243.  \nHeckerman, D., Meek, C., & Cooper, G. (1997), A Bayesian approach to causal discovery. Technical report \nMSR-TR-97-05, Microsoft Research. (Note: Heckerman et al 1997, 1999, and 2006 are almost \nidentical.)  \nHeckerman, D., Meek, C., & Cooper, G. (1999), A Bayesian approach to causal discovery. In Glymour, C. & \nCooper, G.F. (eds.), Computation, Causation, and Discovery, AAAI Press: Menlo Park / MIT Press: \nCambridge. (Pages 141â€“165.)  \nHeckerman, D., Meek, C., & Cooper, G. (2006), A Bayesian approach to causal discovery. In Holmes, D.E. & \nJain, \nL.C. \n(eds.), \nInnovations \nin \nMachine \nLearning: Theory \nand \nApplications, Springer: \nBerlin/Heidelberg. (Pages 1â€“28.)  \nHeggernes, P. (2006), Minimal triangulations of graphs: a survey. Discrete Mathematics, 306 (3), 297â€“317.  \nHeggernes, P., Telle, J.A., & Villanger, Y. (2005), Computing minimal triangulations in time ğ‘‚(ğ‘›ğ›¼log ğ‘›) =\nğ‘œ(ğ‘›2.376). SIAM Journal on Discrete Mathematics, 19 (4), 900â€“913. \nHÃ¸jsgaard, S., Edwards, D., & Lauritzen, S. (2012), Graphical Models with R. Springer Science+Business \nMedia: New York.  \nHotelling, H. (1953), New light on the correlation coeï¬ƒcient and its transforms. Journal of the Royal \nStatistical Society, Series B (Statistical Methodology), 15 (2), 193â€“232.  \nIhler, A.T., Kirshner, S., Ghil, M., Robertson, A.W., & Smyth, P. (2007), Graphical models for statistical \ninference and data assimilation. Physica D, 230, 72â€“87.  \nJaakkola, T., MeilÄƒ, M., & Jebara, T. (2000), Maximum entropy discrimination. Advances in Neural \nInformation Processing Systems 12, 470â€“476.  \nJames, A.T. (1964), Distributions of matrix variates and latent roots derived from normal samples. Annals \nof Mathematical Statistics, 35 (2), 475â€“501.  \nJanson, S., Åuczak, T., & RuciÅ„ski, A. (2000), Random Graphs. Wiley.  \nJeong, H., Tombor, B., Albert, R., Oltvai, Z.N. & BarabÃ¡si, A.-L. (2000), The large-scale organization of \nmetabolic networks. Nature, 407 (6804), 651â€“654.  \nJeong, H., Mason, S.P., BarabÃ¡si, A.-L., & Oltvai, Z.N. (2001), Lethality and centrality in protein networks. \nNature, 411 (6833), 41.  \nJones, B., Carvalho, C., Dobra, A., Hans, C., Carter, C., & West, M. (2004), Archival version including \nappendices: Experiments in stochastic computation for high-dimensional graphical models. \nhttp://ftp.isds.duke.edu/WorkingPapers/04-01.long.pdf. Accessed in February 2013.  \n\n \nReferences \n \n141 \nJones, B., Carvalho, C., Dobra, A., Hans, C., Carter, C., & West, M. (2005), Experiments in stochastic \ncomputation for high-dimensional graphical models. Statistical Science, 20 (4), 388â€“400. (See also the \nerratum at http://www.massey.ac.nz/~mbjones/research/errata.html, accessed in February 2013.)  \nKalisch, M. & BÃ¼hlmann, P. (2007), Estimating high-dimensional directed acyclic graphs with the PC-\nalgorithm. Journal of Machine Learning Research, 8, 613â€“636.  \nKaragiannis, G. & Andrieu, C. (2012), Annealed importance sampling reversible jump MCMC algorithms. \nSubmitted for publication.  \nKarger, D. & Srebro, N. (2001), Learning Markov networks: maximum bounded tree-width graphs. \nProceedings of the 12th Annual Symposium on Discrete Algorithms, 392â€“401.  \nKarrer, B. & Newman, M.E.J. (2010), Random graphs containing arbitrary distributions of subgraphs. \nPhysical Review E: Statistical, Nonlinear and Soft Matter Physics, 82 (6), 066118.  \nKirchhoff, G. (1847), Ãœber die AuflÃ¶sung der Gleichungen, auf welche man bei der Untersuchung der \nlinearen Verteilung galvanischer StrÃ¶me gefÃ¼hrt wird. Annalen der Physik und Chemie, 72, 497â€“508. \n(In German.) English translation by J.B. Oâ€™Toole: Kirchhoff, G. (1958), On the solution of the equations \nobtained from the investigation of the linear distribution of galvanic currents. IRE Transactions on \nCircuit Theory, 5 (1), 4â€“7.  \nKjaerulff, U. (1990), Triangulation of graphs: algorithms giving small total state space. Technical report \nR90-09, Aalborg University.  \nKjaerulff, U. (1993), Aspects of efficiency improvement in Bayesian networks. PhD thesis, Aalborg \nUniversity.  \nKnuth, D.E. (1997), The Art of Computer Programming, vol. I, 3rd edition. Addison Wesley Longman. \nKoller, D. & Friedman, N. (2009), Probabilistic Graphical Models: Principles and Techniques. MIT Press: \nCambridge.  \nKollin, J. & Koivisto, M. (2006), Bayesian learning with mixtures of trees. Machine Learning: ECML 2006, \n294â€“305.  \nKoo, T., Globerson, A., Carreras, X., & Collins, M. (2007), Structured Prediction Models via the Matrix-Tree \nTheorem. Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language \nProcessing and Computational Natural Language learning, Prague, June 2007, 141â€“150.  \nKruskal, J.B. (1956), On the shortest spanning subtree of a graph and the traveling salesman problem. \nProceedings of the American Mathematical Society, 7 (1), 48â€“50.  \nKulkarni, V.G. (1990), Generating random combinatorial objects. Journal of Algorithms, 11 (2), 185â€“207.  \nKumar, M.P. & Koller, D. (2009), Learning a small mixture of trees. Advances in Neural Information \nProcessing Systems 22, 1051â€“1059.  \nKundaje, A., Jebara, T., Antar, O., & Leslie, C. (2002), Learning regulatory networks from sparsely sampled \ntime series expression data. Technical report, Columbia University.  \nLancaster, P. & Tismenetsky, M. (1985), The Theory of Matrices: With Applications. Academic Press: San \nDiego.  \nLauritzen, S.L. & Spiegelhalter, D.J. (1988), Local computations with probabilities on graphical structures \nand their application to expert systems. Journal of the Royal Statistical Society, Series B (Statistical \nMethodology), 50 (2), 157â€“224.  \nLauritzen, S.L. (1996), Graphical Models. Oxford University Press: Oxford.  \nLauritzen, S.L. (2006), Fundamentals of graphical models (lectures 8 and 9). http://www.stats.ox.ac.uk \n/~steffen/stflour/. Accessed in February 2013.  \nLauritzen, S. (2012), Structure estimation in graphical models (lecture). http://www.stats.ox.ac.uk \n/~steffen/seminars/waldstructure.pdf. Accessed in February 2013.  \nLeclerc, R.D. (2008), Survival of the sparsest: robust gene networks are parsimonious. Molecular Systems \nBiology, 4 (213). \nLenkoski, A. & Dobra, A. (2011), Computational aspects related to inference in Gaussian graphical models \nwith the G-Wishart prior. Journal of Computational and Graphical Statistics, 20 (1), 140â€“157.  \nLetac, G. & Massam, H. (2007), Wishart distributions for decomposable graphs. The Annals of Statistics, 35 \n(3), 1278â€“1323.  \nLiechty, J.C., Liechty, M.W., & MÃ¼ller, P. (2004), Bayesian correlation estimation. Biometrika, 91 (1), 1â€“14.  \nÅuczak, T. (1990), On the equivalence of two basic models of random graphs. Random Graphs â€™87: Based \non Proceedings of the Third International Seminar on Random Graphs and Probabilistic Methods in \nCombinatorics, June 27 to July 3, 1987, PoznaÅ„, Poland, 151â€“157.  \nMaathuis, M.H., Kalisch, M., & BÃ¼hlmann, P. (2009), Estimating high-dimensional intervention effects from \nobservational data. The Annals of Statistics, 37 (6A), 3133â€“3164.  \n\nReferences \n \n \n142 \nMaathuis, M.H., Colombo, D., Kalisch, M., & BÃ¼hlmann, P. (2010), Predicting causal effects in large-scale \nsystems from observational data. Nature Methods, 7 (4), 247â€“248.  \nMacris, N. (2006), Applications of correlation inequalities to low density graphical codes. European Journal \nof Physics B, 50, 51â€“55.  \nMadigan, D. & Raftery, A.E. (1994), Model selection and accounting for model uncertainty in graphical \nmodels using Occamâ€™s window. Journal of the American Statistical Association, 89 (428), 1535â€“1546.  \nMarkowetz, F. & Spang, R. (2007), Inferring cellular networksâ€”a review. BMC Bioinformatics, 8 (suppl. 6), \nS5.  \nMateti, P. & Deo, N. (1976), On algorithms for enumerating all circuits of a graph. SIAM Journal on \nComputing, 5 (1), 90â€“99.  \nMaxwell, J.C. (1892), A Treatise on Electricity and Magnetism, vol. 1, 3rd edition. Oxford University Press: \nLondon. (Reprinted by Dover Publications.)  \nMcDonald, R. & Satta, G. (2007), On the complexity of non-projective data-driven dependency parsing. \nProceedings of the Tenth International Conference on Parsing Technologies, 121â€“132.  \nMcKay, B.D., Wormald, N.C., & Wysocka, B. (2004), Short cycles in random regular graphs. Electronic \nJournal of Combinatorics, 11.  \nMeilÇ, M. & Jordan, M.I. (1997). Triangulation by continuous embedding. AI Memo no. 1605 / CBCL Memo \nno. 146, Massachusetts Institute of Technology.  \nMeilÄƒ, M. (1999), An accelerated Chow and Liu algorithm: fitting tree distributions to high-dimensional \nsparse data. Proceedings of the Sixteenth International Conference on Machine Learning, 249â€“257.  \nMeilÄƒ-Predoviciu, M. (1999), Learning with mixtures of trees. PhD thesis, Massachusetts Institute of \nTechnology.  \nMeilÄƒ, M. & Jordan, M.I. (2000), Learning with mixtures of trees. Journal of Machine Learning Research, 1, \n1â€“48.  \nMeilÄƒ, M. & Jaakkola, T. (2006), Tractable Bayesian learning of tree belief networks. Statistics and \nComputing, 16, 77â€“92.  \nMeinshausen, N. & BÃ¼hlmann, P. (2006), High-dimensional graphs and variable selection with the lasso. \nThe Annals of Statistics, 34 (3), 1436â€“1462.  \nMeinshausen, N. (2008), A note on the lasso for Gaussian graphical model selection. Statistics & Probability \nLetters, 78 (7), 880â€“884.  \nMeyer, P.E., Kontos, K., Lafitte, F., & Bontempi, G. (2007), Information-theoretic inference of large \ntranscriptional regulatory networks. EURASIP Journal on Bioinformatics and Systems Biology.  \nMiller, J.C. (2008), Bounding the size and probability of epidemics on networks. Journal of Applied \nProbability, 45, 498â€“512.  \nMilo, R., Shen-Orr, S., Itzkovitz, S., Kashtan, N., Chklovskii, D., & Alon, U. (2002), Network motifs: simple \nbuilding blocks of complex networks. Science, 298, 824â€“827.  \nMoghaddam, B., Marlin, B.M., Khan, M.E., & Murphy, K.P. (2009), Accelerating Bayesian structural inference \nfor non-decomposable Gaussian graphical models. Advances in Neural Information Processing Systems \n22, 1285â€“1293.  \nMolloy, M. & Reed, B. (1995), A critical point for random graphs with a given degree sequence. Random \nStructures and Algorithms, 6 (2â€“3), 161â€“180.  \nMoon, J.W. (1970), Counting Labelled Trees. Canadian Mathematical Congress.  \nMukherjee, S. & Speed, T.P. (2008), Network inference using informative priors. Proceedings of the \nNational Academy of Sciences, 105 (38), 14313â€“14318.  \nMurray, I. & Ghahramani, Z. (2004), Bayesian learning in undirected graphical models: approximate MCMC \nalgorithms. Proceedings of the Twentieth Annual Conference on Uncertainty in Artificial Intelligence, \n392â€“399.  \nNewman, M.E.J. (2003), The structure and function of complex networks. SIAM Review, 45 (2), 167â€“256.  \nOlesen, K.G. & Madsen, A.L. (2002), Maximal prime subgraph decomposition of Bayesian networks. IEEE \nTransactions on Systems, Man and Cybernetics, Part B: Cybernetics, 32 (1), 21â€“31.  \nOracle (2012), BigDecimal (Java Platform SE 7). http://docs.oracle.com/javase/7/docs/api/java/math \n/BigDecimal.html. Accessed in February 2013.  \nPanayidou, K. (2011), Estimation of tree structure for variable selection. DPhil thesis, University of Oxford.  \nPaton, K. (1969), An algorithm for finding a fundamental set of cycles of a graph. Communications of the \nAssociation for Computing Machinery, 12 (9), 514â€“518.  \nPearl, J. (1988), Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan \nKaufmann: San Francisco.  \n\n \nReferences \n \n143 \nPelleg, D. & Moore, A. (2006), Dependency trees in sub-linear time and bounded memory. The VLDB Journal.  \nPournara, I. & Wernisch, L. (2007), Factor analysis for gene regulatory networks and transcription factor \nactivity profiles. BMC Bioinformatics, 8 (61).  \nPropp, J.G. & Wilson, D.B. (1998), How to get a perfectly random sample from a generic Markov chain and \ngenerate a random spanning tree of a directed graph. Journal of Algorithms, 27, 170â€“217.  \nPrÃ¼fer, H. (1918), Neuer Beweis eines Satzes Ã¼ber Permutationen. Archiv der Mathematik und Physik, 27, \n742â€“744. (In German.)  \nRajaratnam, B., Massam, H., & Carvalho, C.M. (2008), Flexible covariance estimation in graphical Gaussian \nmodels. The Annals of Statistics, 36 (6), 2818â€“2849.  \nRose, D.J. (1970), Triangulated graphs and the elimination process. Journal of Mathematical Analysis and \nits Applications, 32, 597â€“609.  \nRose, D.J. (1972), A graph-theoretic study of the numerical solution of sparse positive definite systems of \nlinear equations. In Read, R.C. (ed.), Graph Theory and Computing, Academic Press: New York. (Pages \n63â€“75.)  \nRose, D.J., Tarjan, R.E., & Lueker, G.S. (1976), Algorithmic aspects of vertex elimination on graphs. SIAM \nJournal on Computing, 5 (2), 266â€“283.  \nRoverato, A. (2002), Hyper inverse Wishart distribution for non-decomposable graphs and its application \nto Bayesian inference for Gaussian graphical models. Scandinavian Journal of Statistics, 29 (3), 391â€“411. \nRoyer, L., Reimann, M., Andreopoulos, B., & Schroeder, M. (2008), Unraveling protein networks with power \ngraph analysis. PLOS Computational Biology, 4 (7).  \nSchÃ¤fer, J. & Strimmer, K. (2005a), An empirical Bayes approach to inferring large-scale gene association \nnetworks. Bioinformatics, 21 (6), 754â€“764.  \nSchÃ¤fer, J. & Strimmer, K. (2005b), A shrinkage approach to large-scale covariance matrix estimation and \nimplications for functional genomics. Statistical Applications in Genetics and Molecular Biology, 4 (1).  \nSchÃ¤fer, J., Opgen-Rhein, R., & Strimmer, K. (2012), GeneNet: modeling and inferring gene networks, R \npackage version 1.2.5. http://CRAN.R-project.org/package=GeneNet. Accessed in February 2013.  \nScott, J.G. & Carvalho, C.M. (2008), Feature-inclusion stochastic search for Gaussian graphical models. \nJournal of Computational and Graphical Statistics, 17 (4), 790â€“808.  \nSloane, N.J.A. (2011), Sequence A058862 in Online Encyclopedia of Integer Sequences. http://oeis.org \n/A058862. Accessed in February 2013.  \nSly, A. (2010), Computational transition at the uniqueness threshold. 51st Annual IEEE Symposium on \nFoundations of Computer Science, 287â€“296.  \nSmith, D.A. & Smith, N.A. (2007), Probabilistic models of nonprojective dependency trees. Proceedings of \nthe 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational \nNatural Language Learning, 132â€“140.  \nSÃ¶rensen, K. & Janssens, G.K. (2005), An algorithm to generate all spanning trees of a graph in order of \nincreasing cost. Pesquisa Operacional, 25 (2), 219â€“229.  \nSpirin, V. & Mirny, L.A. (2003), Protein complexes and functional modules in molecular networks. \nProceedings of the National Academy of Sciences, 100 (21), 12123â€“12128.  \nSudakov, B. & VerstraÃ«te, J. (2008), Cycle lengths in sparse graphs. Combinatorica, 28 (3), 357â€“372.  \nSylvester, J.J. (1857), On the change of systems of independent variables. Quarterly Journal of Pure and \nApplied Mathematics, 1, 42â€“56 and 126â€“134.  \nTan, V.Y.F., Anandkumar, A., & Willsky, A.S. (2010a), Learning Gaussian tree models: analysis of error \nexponents and extremal structures. IEEE Transactions on Signal Processing, 58 (5), 2701â€“2714.  \nTan, V.Y.F., Sanghavi, S., Fisher, J.W., & Willsky, A.S. (2010b), Learning graphical models for hypothesis \ntesting and classification. IEEE Transactions on Signal Processing, 58 (11), 5481â€“5495.  \nTan, V.Y.F., Anandkumar, A., & Willsky, A.S. (2010c), Error exponents for composite hypothesis testing of \nMarkov forest distributions. IEEE International Symposium on Information Theory; Austin, Texas; June \n13â€“18, 2010.  \nTan, V.Y.F., Anandkumar, A., & Willsky, A.S. (2010d), Scaling laws for learning high-dimensional Markov \nforest distributions. Forty-Eighth Annual Allerton Conference on Communication, Control, and \nComputing.  \nTarjan, R.E. & Yannakakis, M. (1984), Simple linear-time algorithms to test chordality of graphs, test \nacyclicity of hypergraphs, and selectively reduce acyclic hypergraphs. SIAM Journal on Computing, 13 \n(3), 566â€“579. \nThomas, A. & Green, P.J. (2009a), Enumerating the decomposable neighbours of a decomposable graph \nunder a simple perturbation scheme. Computational Statistics and Data Analysis, 53 (4), 1232â€“1238.  \n\nReferences \n \n \n144 \nThomas, A. & Green, P.J. (2009b), Enumerating the junction trees of a decomposable graph, Journal of \nComputational and Graphical Statistics, 18 (4), 930â€“940.  \nThomas, J., Ramakrishnan, N., & Bailey-Kellogg, C. (2008), Graphical models of residue coupling in protein \nfamilies. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 5 (2), 183â€“197.  \nTibshirani, R. (1996), Regression shrinkage and selection via the lasso. Journal of the Royal Statistical \nSociety Series, Series B (Statistical Methodology), 58 (1), 267â€“288.  \nTripathi, A., Venugopalan, S., & West, D.B. (2010), A short constructive proof of the ErdÅ‘sâ€“Gallai \ncharacterization of graphic lists. Discrete Mathematics, 310 (4), 843â€“844.  \nTurÃ¡n, P. (1941), Egy grÃ¡felmÃ©leti szÃ©lsÅ‘Ã©rtÃ©kfeladatrÃ³l. Matematiko Fizicki Lapok, 48, 436â€“452. (In \nHungarian.) English translation by G. TurÃ¡n: â€œAn extremal problem in graph theoryâ€, in P. ErdÅ‘s (ed.) \n(1990), Collected papers of Paul TurÃ¡n, vol. I, AkadÃ©miai KiadÃ³: Budapest. (Pages 231â€“251.)  \nU.S. Securities and Exchange Commission (2010), Mutual funds. http://www.sec.gov/answers/mut \nfund.htm. Accessed in February 2013.  \nVan Lint, J.M. & Wilson, R.M. (2001), A Course in Combinatorics, 2nd edition. Cambridge University Press: \nCambridge.  \nVontobel, P.O. (2003), Algebraic coding for iterative decoding. Doctoral thesis (Diss. ETH No. 14961), Swiss \nFederal Institute of Technology, Zurich / ETH Zurich.  \nWang, Y. (2009), Latent tree models for multivariate density estimation: algorithms and applications. PhD \nthesis, Hong Kong University of Science and Technology.  \nWang, H. & Li, S.Z. (2012), Eï¬ƒcient Gaussian graphical model determination under G-Wishart prior \ndistributions. Electronic Journal of Statistics, 6, 168â€“198.  \nWatts, D.J. & Strogatz, S.H. (1998), Collective dynamics of â€œsmall-world networksâ€. Nature, 393, 440â€“442.  \nWermuth, N. (1976), Model search among multiplicative models. Biometrics, 32, 253â€“263.  \nWermuth, N. (1998), Graphical Markov models. In Kotz, S., Read, C.B., & Banks, D.L. (eds.), Encyclopedia of \nStatistical Sciences, update volume 2, Wiley: New York. (Pages 284â€“300.)  \nWermuth, N. & Cox, D.R. (2001). Graphical models: overview. In Baltes, P.B. & Smelser, N.J. (eds.), \nInternational Encyclopedia of the Social and Behavioral Sciences, Elsevier: Amsterdam. (Pages 6379â€“\n6386.)  \nWhittaker, J. (1990), Graphical models in applied multivariate statistics. Wiley: Chichester.  \nWille, A. & BÃ¼hlmann, P. (2006), Low-order conditional independence graphs for inferring genetic \nnetworks. Statistical Applications in Genetics and Molecular Biology, 5 (1).  \nWillsky, A.S. (2002), Multiresolution Markov models for signal and image processing. Proceedings of the \nIEEE, 90 (8), 1396â€“1458.  \nWit, E. & McClure, J. (2004), Statistics for Microarrays: Design, Analysis and Inference. Wiley-Blackwell: \nChichester.  \nWong, F., Carter, C.K., & Kohn, R. (2003), Efficient estimation of covariance selection models. Biometrika, \n90 (4), 809â€“830.  \nWormald, N.C. (1981), The asymptotic distribution of short cycles in random regular graphs. Journal of \nCombinatorial Theory, Series B, 31, 168â€“182.  \nWormald, N.C. (1985), Counting labelled chordal graphs. Graphs and Combinatorics, 1, 193â€“200.  \nWu, B.Y. & Chao, K.-M. (2004), Spanning trees and optimization problems. Chapman & Hall / CRC.  \nYang, R. & Berger, J.O. (1994), Estimation of a covariance matrix using the reference prior. The Annals of \nStatistics, 22 (3), 1195â€“1211.  \nYannakakis, M. (1981), Computing the minimum fill-in is NP-complete. SIAM Journal of Algebraic Discrete \nMethods, 2 (1), 77â€“79.  \nYuan, M. & Lin, Y. (2007), Model selection and estimation in the Gaussian graphical model. Biometrika, 94 \n(1), 19â€“35.  \nZaffalon, M. & Hutter, M. (2005), Robust inference of trees. Annals of Mathematics and Artificial \nIntelligence, 45, 215â€“239.  \n======================================================================================================= \nThe following corrections have been made in this compact version of the thesis.  \nâ€¢ In section 9.4, in Algorithm IX, in the comment to the right of line 17, â€œline 20â€ has been changed to â€œline 19â€.  \nâ€¢ Also in Algorithm IX, this new line has been inserted: â€œ21. Remove previous from ch(current)â€. In the two paragraphs just \nbefore this algorithm, the references to line-numbers in the algorithm have been corrected accordingly.  \nâ€¢ In section 11.1, in the proof of Proposition 11.3, the second g(W(v2)) has been changed to g(W(v3)). \nâ€¢ In section 11.7, â€œto give SSSD a chance to visit a reasonable number of graphsâ€ has been moved from the caption of Figure \n11.11 to the caption of Figure 11.12.  \nSeveral copyediting errors have also been corrected.",
    "pdf_filename": "Bayesian learning of forest and tree graphical models.pdf"
}