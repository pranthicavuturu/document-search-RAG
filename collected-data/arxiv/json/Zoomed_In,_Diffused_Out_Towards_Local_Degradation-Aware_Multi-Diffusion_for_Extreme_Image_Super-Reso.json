{
    "title": "Zoomed In, Diffused Out: Towards Local Degradation-Aware Multi-Diffusion for",
    "abstract": "( ) bis gu gp ep ro tr ht as n Large-scale, pre-trained Text-to-Image (T2I) diffusion 512x512 modelshavegainedsignificantpopularityinimagegener- local awareness ationtasksandhaveshownunexpectedpotentialinimage Super-Resolution (SR). However, most existing T2I diffu- 4x SR prediction (SeeSR + MD) penguins sionmodelsaretrainedwitharesolutionlimitof512×512, making scaling beyond this resolution an unresolved but supports necessary challenge for image SR. In this work, we intro- bigger than 512x512 duceanovelapproachthat,forthefirsttime,enablesthese local modelstogenerate2K,4K,andeven8Kimageswithoutany awareness additionaltraining. OurmethodleveragesMultiDiffusion, which distributes the generation across multiple diffusion 4x SR prediction (Ours) stones pathstoensureglobalcoherenceatlargerscales,andlocal degradation-awarepromptextraction,whichguidestheT2I Figure1. Comparisonof4xSuper-Resolution(SR)predictionsus- ingSeeSR+MultiDiffusion(MD)andourproposedmethod.While modeltoreconstructfinelocalstructuresaccordingtoits MDunlockshigherresolutionsbeyond512×512, ourproposed low-resolutioninput. Theseinnovationsunlockhigherreso- strategyofextractinglocaldegradation-awarepromptsensureslo- lutions,allowingT2Idiffusionmodelstobeappliedtoimage caldetailawareness,improvingfine-grainedstructurerestoration, SRtaskswithoutlimitationonresolution. asdemonstratedinthestonesregions. 1.Introduction SRmodels,suchasSR3,DiffBIR,andSRDiff,havealready Image Super-Resolution (SR) is vital for a wide range achieved impressive results by generating realistic details ofreal-worldapplications,includingsatelliteimaging,med- atthesescalesusingconventionalSRdatasetsandtraining ical diagnostics, and consumer photography, where high- themfromscratch[18,21,37]. resolution outputs (e.g., 2K, 4K, or 8K) are essential for Ontheotherside,thedevelopmentofT2Imodelsacceler- capturingfinedetailsandensuringclarity[1,6,38,39,49]. ates,asexemplifiedbyastonishingapplicationslikeDall-E AlthoughSRmethods,particularlythoseusinglocalopera- andStableDiffusion[4,36]. Drivenbytrainingonincreas- tionslikeCNNs,havemadesignificantprogress,handling inglylargeanddiversedatasets,theypresentagreatresource complexdegradationinLow-Resolution(LR)inputsremains forimageSR[13,15]. Byrepurposingtheirvastgenerative apersistentchallenge[22,31,32]. capabilities,thesemodelscanunlocknewpossibilitiesfor Recently,diffusionmodels,particularlypre-trainedText- High-Resolution(HR)imageenhancement. Inspiredbythis to-Image(T2I)diffusionmodels,haverevolutionizedimage idea,methodslikeStableSR,PASD,andSeeSRhaverepur- generation tasks [11,23,45,50]. Originally designed for posedpre-trainedT2ImodelstoSR,achievingimpressive creativeapplicationsliketext-guidedimagesynthesis,these resultsinlimited-resolutionscenarios[42,46,47]. modelshavedemonstratedstrongpotentialinimageSR,es- Yet,allT2IdiffusionmodelsrepurposedforimageSRare peciallyinhandling4xscalingandbeyond,wherehallucinat- limitedbyaresolutionof512×512,whichisimpracticalfor ingfinedetailsbecomesessential[30,32]. Diffusion-based real-worldapplicationsandamajorlimitation[3,9,12,34]. 1 4202 voN 81 ]VC.sc[ 1v27021.1142:viXra",
    "body": "Zoomed In, Diffused Out: Towards Local Degradation-Aware Multi-Diffusion for\nExtreme Image Super-Resolution\nBrianB.Moser1,2,StanislavFrolov1,2,TobiasC.Nauen1,2,FedericoRaue1,AndreasDengel1,2\n1GermanResearchCenterforArtificialIntelligence\n2UniversityofKaiserslautern-Landau\nfirst.last@dfki.de\nAbstract\n( ) bis gu gp ep ro tr ht as n\nLarge-scale, pre-trained Text-to-Image (T2I) diffusion 512x512\nmodelshavegainedsignificantpopularityinimagegener- local\nawareness\nationtasksandhaveshownunexpectedpotentialinimage\nSuper-Resolution (SR). However, most existing T2I diffu-\n4x SR prediction (SeeSR + MD) penguins\nsionmodelsaretrainedwitharesolutionlimitof512×512,\nmaking scaling beyond this resolution an unresolved but\nsupports\nnecessary challenge for image SR. In this work, we intro- bigger than\n512x512\nduceanovelapproachthat,forthefirsttime,enablesthese\nlocal\nmodelstogenerate2K,4K,andeven8Kimageswithoutany awareness\nadditionaltraining. OurmethodleveragesMultiDiffusion,\nwhich distributes the generation across multiple diffusion 4x SR prediction (Ours) stones\npathstoensureglobalcoherenceatlargerscales,andlocal\ndegradation-awarepromptextraction,whichguidestheT2I Figure1. Comparisonof4xSuper-Resolution(SR)predictionsus-\ningSeeSR+MultiDiffusion(MD)andourproposedmethod.While\nmodeltoreconstructfinelocalstructuresaccordingtoits\nMDunlockshigherresolutionsbeyond512×512, ourproposed\nlow-resolutioninput. Theseinnovationsunlockhigherreso-\nstrategyofextractinglocaldegradation-awarepromptsensureslo-\nlutions,allowingT2Idiffusionmodelstobeappliedtoimage\ncaldetailawareness,improvingfine-grainedstructurerestoration,\nSRtaskswithoutlimitationonresolution.\nasdemonstratedinthestonesregions.\n1.Introduction\nSRmodels,suchasSR3,DiffBIR,andSRDiff,havealready\nImage Super-Resolution (SR) is vital for a wide range achieved impressive results by generating realistic details\nofreal-worldapplications,includingsatelliteimaging,med- atthesescalesusingconventionalSRdatasetsandtraining\nical diagnostics, and consumer photography, where high- themfromscratch[18,21,37].\nresolution outputs (e.g., 2K, 4K, or 8K) are essential for Ontheotherside,thedevelopmentofT2Imodelsacceler-\ncapturingfinedetailsandensuringclarity[1,6,38,39,49]. ates,asexemplifiedbyastonishingapplicationslikeDall-E\nAlthoughSRmethods,particularlythoseusinglocalopera- andStableDiffusion[4,36]. Drivenbytrainingonincreas-\ntionslikeCNNs,havemadesignificantprogress,handling inglylargeanddiversedatasets,theypresentagreatresource\ncomplexdegradationinLow-Resolution(LR)inputsremains forimageSR[13,15]. Byrepurposingtheirvastgenerative\napersistentchallenge[22,31,32]. capabilities,thesemodelscanunlocknewpossibilitiesfor\nRecently,diffusionmodels,particularlypre-trainedText- High-Resolution(HR)imageenhancement. Inspiredbythis\nto-Image(T2I)diffusionmodels,haverevolutionizedimage idea,methodslikeStableSR,PASD,andSeeSRhaverepur-\ngeneration tasks [11,23,45,50]. Originally designed for posedpre-trainedT2ImodelstoSR,achievingimpressive\ncreativeapplicationsliketext-guidedimagesynthesis,these resultsinlimited-resolutionscenarios[42,46,47].\nmodelshavedemonstratedstrongpotentialinimageSR,es- Yet,allT2IdiffusionmodelsrepurposedforimageSRare\npeciallyinhandling4xscalingandbeyond,wherehallucinat- limitedbyaresolutionof512×512,whichisimpracticalfor\ningfinedetailsbecomesessential[30,32]. Diffusion-based real-worldapplicationsandamajorlimitation[3,9,12,34].\n1\n4202\nvoN\n81\n]VC.sc[\n1v27021.1142:viXra\nInthiswork,weintroduceanovelsolutionthat,forthe Incontrast,ourapproachintroducesanovelmethodto\nfirsttime,enablespre-trainedT2Idiffusionmodelstogener- scalediffusionmodelsbeyond512x512withoutretraining,\nateSRimagesatresolutionsof2K,4K,andeven8K,which whichwetermedextremeimageSR(i.e.,2K,4K,and8K).\nwecoinextremeimageSR,withoutanyadditionaltraining.\nOur approach leverages two core components: (1) Multi- 2.2.ExploitingT2IforSR\nDiffusion,whichdistributestheimagegenerationprocess\nText-to-Image (T2I) conditioning has emerged as a\nacrossmultiplediffusionpathstoensureglobalcoherence,\npromisingapproachforenhancingSuper-Resolution(SR)\nand(2)localdegradation-awarepromptextraction, which\ntasksbyleveragingthecapabilitiesofpre-trainedT2Imodels.\nprovidesfine-grainedcontroloverlocalstructures,allowing\nFine-tuningthesemodels,withtheadditionofspecialized\nthe model to effectively restore detailed textures and se-\nencoders suited for SR, allows for integrating textual de-\nmanticconsistency. Byintegratinglocaldegradation-aware\nscriptionsintoimageenhancement. Thisfusionoftextual\npromptextraction,ourmethodensuresconsistentsemantic\ninformation can provide a richer guidance source, poten-\naccuracyacrossallimagescales,settinganewstandardfor\ntially improving theaccuracyand contextual relevance of\nextremeimageSR.\nsynthesizedimagesinSRapplications.\nInsummary,ourkeycontributionsareasfollows:\nFor instance, Wang et al. introduced StableSR, which\n• We introduce MultiDiffusion to image SR, which exploits text guidance by incorporating a time-aware en-\nenables scaling T2I diffusion models (trained for coder trained alongside a frozen Latent Diffusion Model\n512x512) beyond 512x512 by coordinating multiple (LDM) [36,42]. This framework utilizes trainable spatial\ndiffusionpathsforglobalcoherence. feature transform layers to condition the model based on\ninput images. An optional controllable feature wrapping\n• Weproposelocaldegradation-awarepromptextraction,\nmodulefurtherenhancesStableSR’sadaptability,allowing\nwhichenhancestheabilitytopreservelocaldetailsand\nfor fine-grained user control. The design of this module\nstructuralintegrity.\ndraws inspiration from CodeFormer, contributing to Sta-\nbleSR’sflexibilityinaddressingdiverseuserneedsandpref-\n• We demonstrate, for the first time, the capability of\nerences[53].\npre-trained T2I models to generate 2K, 4K, and 8K\nSimilarly,Yangetal. proposedPixel-AwareStableDiffu-\nimageswithouttrainingandsetaninitialbenchmark\nsion(PASD),whichadvancestheconditioningprocessby\nforextremeimageSRalongsideclassicalmethods.\nembeddingtextdescriptionsofLRinputsusingaCLIPtext\n2.RelatedWork encoder[35,47].Thismethodimprovesthemodel’scapacity\ntosynthesizeimageswithgreaterprecisionbyembedding\nThissectionbrieflyreviewsthecurrentfieldofimageSR contextualinformationfromtextualsources,enhancingthe\nandhowT2IisexploitedforimageSR. overallfidelityandrelevanceofthegeneratedimages.\nConcurrently,othermethodslikeSeeSRexploresimilar\n2.1.ImageSR\nT2Iconditioningframeworks,whileXPSRtakesthisconcept\nImageSRhasseensignificantadvancementsthroughthe furtherbymergingdifferentlevelsofsemantictextencod-\ndevelopment of CNNs. Classical CNN-based SR models ings[33,46]. XPSRcombineshigh-levelencodings(image\n(includingvisiontransformers),suchasRRDB,ESRGAN, content)withlow-levelencodings(qualityperception,sharp-\nSwinIR,andHAT,excelinupscalingimagestoanyresolu- ness,noise,andotherLRimagedistortions),furtherrefining\ntionduetotheirlocaloperations[7,19,44]. Theyleverage the SR results through a more nuanced understanding of\nlocalreceptivefields,enablingthemtoprocessandtrainon bothcontentandperceptualquality.\nrelatively small image patches (e.g., 192x192) while gen-\neralizingwelltolargerimages. Thispropertymakesthem 3.Methodology\nhighlyscalableforSRtasksacrossvariousresolutions.\nMorerecently,diffusionmodelshaveemergedaspower- This section introduces our novel approach to extreme\nfulalternativesforSR,particularlyinhandlinghigherscaling imageSRbyleveragingpre-trainedT2Idiffusionmodels.\nfactorssuchas4xandbeyond. ModelslikeSR3,DiffBIR, For the first time, our method enables T2I models repur-\nand SRDiff have demonstrated strong capabilities in hal- posed for SR to achieve resolutions of 2K, 4K, and 8K\nlucinating fine details required at these scales [18,21,37]. without additional training but with local coherence. The\nHowever,thesediffusion-basedSRmodelstypicallyneedto approachisbuiltupontwocoreinnovations: MultiDiffusion,\nbetrainedonthetargetsizeandareoftenlimitedtoresolu- whichensuresglobalcoherenceathighresolutions,andlocal\ntionsof512x512duetoarchitecturalandtrainingconstraints, degradation-awarepromptextraction,whichenhanceslocal\nwhichrestricttheirflexibilityandpracticalitycomparedto detailrecovery. Figure2illustratestheoverallconceptof\nCNN-basedmethodsthatcanbeappliedtoanyresolution. ourmethod.\n2\nSeeSR\nControlled\nT2I\nlimited to 512x512 images & Image Tagging \"beak, penguin, Model\none prompt for entire image Encoder Head stone, rocky\"\nGlobal Degradation-Aware Tag Extraction\nOurs\nControlled\nT2I\nImage Tagging \"beak, penguin, Model\nEncoder Head stone, rocky\"\nControlled\nT2I\nImage Tagging \"sea, stone, Model\nEncoder Head rock formation\"\nLocal Degradation-Aware Tag Extraction\nMultiDiffuser\nFigure2. IllustrationofSeeSR(top)comparedtoourlocaldegradation-awaremethod(bottom). WhileSeeSRislimitedtoafixed\nimagesizeof512×512,ourmethodcantechnicallyupscaletoanyresolutionduetotwocomponents: MultiDiffusion(MD)andlocal\ndegradation-awarepromptextraction.TheMDprocessisappliedtooverlappingtiles.Withoutlocaldegradation-awarepromptextraction,\ntheclassifierguidancegenerateshallucinateddetailsbasedonglobalpromptsthatdescribetheentireimage,leadingtoinconsistenciesin\nlocaltilecontent.Ourapproachincorporateslocaltagextractionand,thereby,providestile-specificprompts,ensuringmoreaccurateand\ncoherentdetailgenerationacrosstheentireimage.\n3.1.MultiDiffusionProcess Since we want to unlock the generation of images\nlarger than 512×512 pixels (i.e., latent codes larger than\nMultiDiffusion(MD)allowsthemodeltogeneratehigh-\n64×64), our goal is the generation latent representa-\nresolutionimagesbydistributingtheimagesynthesisacross\ntions M ,M ,...,M in a new latent space M =\nT T−1 0\nmultiple diffusion paths [3,12,34]. At each stage, the la- RW′×H′×C, withW′ ≥ W,H′ ≥ H, usingthesamepre-\ntent feature map is divided into overlapping tiles, each of\ntrainedmodelΦwithoutanyretrainingorfine-tuning. Tradi-\nwhich undergoes a separate diffusion process. This tech-\ntionally,modelspre-trainedonfixed-sizelatentscannotbe\nnique maintains global coherence by sharing overlapping\ndirectlyappliedtoproducelatentsofarbitrarysizes[8,28].\ninformationbetweenadjacenttiles. Thefinalimageissyn-\nTo address this, MD extends the diffusion process by\nthesizedbymergingtheoutputsfromthesemultiplepaths,\napplying a joint diffusion approach, where multiple over-\nensuringconsistencyinbothglobalstructureandlocaldetail.\nlapping latent windows are merged via averaging. More\nUnliketraditionalT2Idiffusionmethods,whicharelimited\nto512×512pixels,thisprocessenablesresolutionsof2K\nformally,wedefinenmappingsF\ni\n: M ∈ RW′×H′×C →\nL ∈ RW×H×C withi ∈ {1,...,n}, whichmap(orcrop)\nandbeyondwithoutanyretrainingorfine-tuning.\nthelargerlatentspaceMintonlatentrepresentationsofthe\nMore concretely, let Φ : L×Y → L be a pre-trained\nT2ImodelthatoperatesinthelatentspaceL=RW×H×C originalsizeW ×H (i.e.,64×64).\nandconditionspaceY,e.g.,textualprompts. Givenanoisy Thenumberofoverlappinglatentcropsnisdefinedas\nlatentrepresentationL ∼N(0,I)andaconditiony ∈Y, n = W′−W + 1, where ω represents the stride between\nT ω\ntheT2IdiffusionmodelΦproducesasequenceoflatents adjacentcroppingwindows. Withthesemappings,thede-\nstarting from L and gradually denoising it towards the noising process is applied independently to each cropped\nT\ncleanlatentrepresentationL thatcanbedecodedtoaHR latentwindow. Subsequently,thelatentrepresentationsare\n0\napproximation: stitched back to the original size M = RW′×H′×C, i.e.\nM = MultiDiffuser({Li }n ), which averages the\nt−1 t−1 i=1\nL ,L ,...,L suchthat L =Φ(L |y). (1) overlappinglatentregions.\nT T−1 0 t−1 t\n3\n3.2.LocalDegradation-AwarePromptExtraction Algorithm1LocalDegradation-AwareMultiDiffusion\nAsdemonstratedintheintroductoryexampleinFigure1, Require: Φ{pre-trainedT2Idiffusionmodel}\nsolelyapplyingMDoftenresultsinimageswithexcessive φ{pre-trainedpromptextractor}\nhallucinateddetails. Thisoccursbecausetheglobalprompt {F i}n i=1{non-overlappinglatentmappings}\nusedforclassifierguidanceinthediffusionprocessapplies {I i}n i=1{correspondingimagepatches}\nuniformly across all tiles. While the prompt effectively M T ∼N(0,I){noisyinitialization}\ndescribestheglobalstructureoftheimage,itlacksthelocal 1: foreachwindowi=1,...,ndo\ncontentspecificityneededforeachtile. Consequently,the 2: y i ←φ(I i){extractlocalprompts}\nmodelattemptstoreconstructfinedetailsineverytilebased 3: endfor\non global information, leading to over-hallucination and 4: fort=T,T −1,...,0do\ninconsistenciesinlocalstructures. 5: foreachwindowi=1,...,ndo\nMoreformally, lety ∈ Y beaconditiondrawnfroma 6: Li t−1 ←Φ(F i(M t)|y i){denoisewindow}\nprompt extractor. As F is guiding the diffusion model Φ 7: endfor\ni\nwiththesameprompty,i.e.,Φ(Li\nt\n|y)forallpossiblewin- 8: M t−1 ←MultiDiffuser({Li t−1}n i=1)\ndowsi∈{1,...,n},thelargerlatentfeaturemapresulting 9: endfor\nfromthisMDprocesswillhavetheconditioninginformation 10: returnM 0\ny infusedateveryspatialposition. Hence,globalprompts\naloneareinsufficientforreconstructingregion-specificcon-\ntent,astheylackthegranularitytocapturetheuniquelocal bysettingthestrideto32,ensuringabalancebetweencom-\ndetailswithineachpatch. putationalefficiencyandimageconsistency[12]. Notably,\nToensurethecoherenceoflocalstructureanddetails,we we chose not to apply the non-overlapping striding strat-\nproposealocaldegradation-awarepromptextraction,which egyproposedinSpotDiffusion,asitsignificantlydegraded\ndescribes the local degradation patterns in LR inputs (see theSRresultsforLRinputsbyfailingtocapturenecessary\nnextSectionforconcreteextractor). Inmoredetail,ifnis contextualinformationacrosspatches.\nthenumberofmaps(orcrops),weproposetoconditionthe For the local degradation-aware prompt extraction, we\nMD process with a set of n conditions, i.e., {y ,...,y }, applyDAPEonthecorrespondingtilesinimagespace(i.e.,\n1 n\nwhere F uses the condition y . These prompts guide the extractedfrom512×512imagepatches)thatwouldundergo\ni i\ndiffusionmodelinreconstructingdetailedtexturesandstruc- the individual diffusion steps in F i in latent space. This\nturesduringtheSRprocess. Byleveragingtheseprompts, extractionensuresthateachpatchreceiveslocallyrelevant\nourmethodeffectivelyreducesartifactsandnoise,particu- prompts, enhancing fine detail restoration. Interestingly,\nlarlyathighresolutions(e.g.,2K,4K,and8K),andensures thisprocessresultsinaveragingoverlappinglatentpatches\naccuraterestorationoffine-graineddetails. Asaresult,our generatedundervaryingpromptconditions. Thisdynamic\napproach,asoutlinedinAlgorithm1,significantlymitigates useoflocalpromptshelpspreventinconsistenciesbetween\nthecommonissuesofover-hallucinationandvisualartifacts adjacent patches and avoids over-hallucination, which is\nthatoccurwhenrelyingsolelyonglobalpromptguidance. commonwhenusingglobalpromptsinMD[3].\n3.3.ExploitingPre-TrainedPromptExtractors 4.ExperimentalSetup\nOur method is designed to be flexible and compatible\nInthissection,wedetailthedatasets,models,andevalua-\nwithanypre-trainedT2Idiffusionmodelandpromptextrac-\ntionmetricsusedtoassesstheperformanceofourproposed\ntionstrategy. Still,inthiswork,wespecificallybuildupon\nmethod,comparingitagainstbothclassicalanddiffusion-\nthepre-trainedmodelsutilizedbySeeSR[46]. Specifically,\nbased SR models across various extreme-resolution tasks.\nweutilizethepre-trainedStableDiffusionV2.0model,along The code for our experiments can be found on GitHub 1,\nwithits8×compressionVAE,andtheDegradation-Aware\nwhichcomplementstheofficialimplementationofSeeSR.\nPromptExtractor(DAPE),whichitselfisafine-tunedtag-\nstyle prompt extraction model, i.e., RAM [10,36,46,52]. 4.1.Datasets\nThesecomponentshaveproveneffectiveforreal-worldSR\nForourexperiments,weutilizedtheDIV2Kvalidation\ntasks,allowingustoleveragetheirrobustgenerativecapabil-\nsetaswellastheTest4KandTest8Kdatasetsintroducedby\nitiesforhigher-resolutionimagesynthesis.\nClassSR[2,16]. Thesedatasetsprovidearobustbenchmark\nFortheMDprocess,weadoptthestandardconfiguration\nforSRtasks,allowingustoevaluatetheperformanceofour\nof dividing the latent space into 64x64 patches [3]. This\nmethodacrossarangeofhighresolutions(i.e.,2K,4K,and\napproachensuresefficientglobalcoherenceacrosstheim-\n8K),whichwecoinextremeimageSR.\nagewhilemaintainingthefine-grainedstructure. Inspired\nbySpotDiffusion,wereducetheoverlapbetweenpatches 1https://github.com/Brian-Moser/zido\n4\nLR SeeSR + MD Ours HR\nFigure3. Qualitativecomparisonofa2Kimage(899;DIV2KVal)betweenLR,SeeSR+MD(PSNR↑:25.273,SSIM↑:0.769,LPIPS↓:0.130),\nour method (PSNR↑:26.217, SSIM↑:0.794, LPIPS↓:0.103), and HR. In general, we observe that our method reconstructs details in\nbackgroundobjectsbetterthanSeeSR+MD(seelightpatternsinthelowerleftcorner).\nSinceourapproachfullyleveragespre-trainedT2Imodels Additionally, lower scores are expected in our method\nwithoutanyadditionaltraining,wedidnotusetheclassical since the mentioned models are explicitly SR-trained,\nSRtrainingdatasetslikeDIV2K(train)andFlickr2K[2,41]. whereaswerepurposepre-trainedT2ImodelswithoutSR-\nOurfocusonenablingextremeSRatresolutionsfarbeyond training(therebymodifyingSeeSRwithMDasbaseline).\n512×512alsoledustoexcludeconventionalSRevaluation\ndatasets, such as Set5, Set14, BSDS100, Manga109, and 5.Results\nUrban100[5,14,26,27,48]. Thesedatasetsarecommonly\nThis section presents qualitative and quantitative com-\nusedinSRresearchbutarelimitedinresolution,typically\nparisons between our approach and SeeSR modified with\ncappingat512×512,makingthemunsuitablefortestingthe\nMD(leadinginSR-repurposedT2Imethods). Sincenopre-\nfullcapabilitiesofourmethod[3,36,46,47]. Byfocusingon\ntrainedT2Imodelshavebeenpreviouslytestedforextreme\nextremeresolutions,weaimtodemonstratethescalability\nimageSR(resolutionslargerthan512×512), SeeSRwith\nandversatilityofourapproachinsupportingany-resolution\nMDservesasourprimarybaselineforevaluation.\nSRwithpre-trainedmodels,particularlyfor2K,4K,and8K.\nFor the scaling factor, we selected a 4× magnification, 5.1.QualitativeResults\nmeaningthattheHRimagesarefourtimeslargerinspatial\ndimensionsthantheirLRcounterparts. Thisscalingfactoris Toassesstheeffectivenessofourproposedapproachvi-\ncommonindiffusion-basedSRbenchmarksandisgenerally sually,wefocusonchallengingyetvisiblecasesfromthe\nconsideredchallenging[31,32]. DIV2Kvalidationsettohighlighttheimprovementsinfine\ndetailpreservation,localcoherence,andoverallperceptual\n4.2.Models&Metrics quality. As shown in Figure 3, our method demonstrates\nsignificantimprovementsinrecoveringfinedetailsandgen-\nWe compare a variety of standard SR approaches, in- eratingmorecoherentstructurescomparedtoSeeSR+MD.\ncludingregression-basedmethodslikeEDSR,RRDB,and Inthe2Kresolutionimage,ourapproachnotonlypreserves\nCAR,GAN-basedmethodssuchasRankSRGANandES- theglobalstructurebutalsoreconstructsbackgroundobjects,\nRGAN,aswellasthenormalizingflowapproachSRFlow suchasthelightpatternsinthelower-leftcorner,withmore\n[17,20,24,25,40,43,44,51]. Additionally,weincludedif- accuracyandclarity(i.e.,withoutartifacts). Thisisdueto\nfusionmodelsthatareexplicitlytrainedforimageSR,such the local degradation-aware prompt extraction, which en-\nas SR3+YODA, SRDiff, and DiWa, to provide a broader ablesmorepreciselocaldetailreconstruction,avoidingthe\ncomparison[18,29,30,37]. hallucinationofirrelevantglobalfeatures.\nToevaluatethequalityofthereconstructedimages,we InFigure4andFigure5, wefurtherillustratehowour\nuse both pixel-based metrics, namely PSNR and SSIM, methodhandlesthepresenceofbackgroundtexturesbytak-\nas well as the perceptual quality metric LPIPS [31,32]. ingacloserlookatzoomed-inregions. Inthefirstexample\nPixel-based metrics measure structural similarity and re- (Figure4),SeeSR+MDhallucinatesfeather-likepatternsin\nconstructionaccuracy,whileLPIPSprovidesamorehuman- the leaves due to the global prompt guidance, leading to\nperception-alignedevaluationofimagequality,allowingus inconsistentresults(seewavystructuresabovetheleaf). By\nto assess both fidelity and perceptual realism. Note that contrast,ourmethodmaintainslocalcoherenceandrecon-\nclassicalSRapproachestypicallyexhibitmuchhigherpixel- structs the leaves more naturally and accurately. For the\nbasedscoresthandiffusionmodels,asdiffusionmodelsare secondexample(Figure5),wecanobservefur-likepatterns\npronetohallucinateddetails[37]. inthedirtinSeeSR,contrarytoours. Althoughbothmeth-\n5\nFigure 4. Qualitative comparison (886; DIV2K Val) between Figure 5. Qualitative comparison (809; DIV2K Val) between\nSeeSR+MD(PSNR↑:26.252, SSIM↑:0.766, LPIPS↓:0.123)and SeeSR+MD(PSNR↑:25.107, SSIM↑:0.598, LPIPS↓:0.076)and\nourapproach(PSNR↑:28.115,SSIM↑:0.802,LPIPS↓:0.091).The ourapproach(PSNR↑:25.627,SSIM↑:0.621,LPIPS↓:0.069).The\nglobaltagswere“balustrade,bird,blue,fence,green,macaw,par- globaltagswere“animal,break,floor,grass,green,lay,lion,lush,\nrot,perch,pole,rail,sit,stand,yellow”.WhileSeeSR+MDhalluci- man, mane, mouth, relax, tree”. Similarly, SeeSR+MDhalluci-\nnatesbirdpatternsontheleavesinthebackgroundduetoglobal natesfur-likepatternsinthebrowndirt,leadingtoartifactsthat\npromptguidance,ourapproachpreserveslocalcoherencebyrecon- degradethevisualqualityandcontributetoitsinferiorperformance\nstructingleavesmorenaturally.However,althoughthebackground comparedtoourapproach,whichpreservesthenaturaltextureof\niscontent-wiseaccurate,ourmethodintroducesmorefine-grained, thedirtmoreeffectively.Onceagain,ourmethodgeneratesfiner,\nblurry-freedetailsthanthosepresentintheoriginalHRimage. sharperdetailsthatsurpassthelevelfoundintheHRimage.\nods occasionally introduce more fine-grained details than 5.2.QuantitativeResults\nthoseintheoriginalHRimage,ourmethodavoidsartifacts\nandhallucinations,ensuringperceptuallyconsistentresults. Tothoroughlyevaluatetheperformanceofourmethod,\nOverall,ourqualitativeresultsdemonstratethatthelocal we conducted experiments on the DIV2K validation set,\ndegradation-aware prompt extraction significantly outper- as well as the Test4K and Test8K datasets. Our results\nformsSeeSR+MDingeneratinghigh-qualitysuper-resolved arecomparedagainstvariousexistingSRmethods,includ-\nimages, particularly at extreme resolutions. The improve- ingregression-based,GAN-based,anddiffusion-basedap-\nmentsareespeciallynotableinpreservinglocaltexturesand proaches(trainedexplicitlyonSRdata).\navoiding over-hallucination, which is a common issue in As shown in Table 1, our method outperforms SeeSR\nMD-basedmodelsguidedbyglobalprompts. Theaddition equippedwithMDregardingperceptualqualitymetricssuch\noflocaldegradation-awarepromptsnotonlyensuresmore asLPIPSwhileachievingcompetitiveresultsonpixel-based\nconsistenttexturesbutalsoprovidesasharper,artifact-free metrics like PSNR and SSIM. Specifically, for 4x SR on\nreconstructionoffinedetails,makingourapproachparticu- DIV2K, our method achieved a PSNR of 24.34, SSIM of\nlarlyadvantageousforcomplexsceneswhereglobalprompts 0.68,andLPIPSof0.108,representingaclearimprovement\nalonefailtomaintaincoherence. inimagequalitycomparedtoSeeSR+MD.\n6\nRH\nDM\n+\nRSeeS\nsruO\nRH\nDM\n+\nRSeeS\nsruO\nType Methods PSNR↑ SSIM↑ LPIPS↓ Type Methods PSNR↑ SSIM↑ LPIPS↓\nBicubic 26.70 0.77 0.409 Bicubic 31.749 0.843 0.0817\nRegression\nEDSR 28.98 0.83 0.270 RRDB 33.931 0.887 0.0536\nRegression\nRRDB 29.44 0.84 0.253\nGANs ESRGAN 30.609 0.810 0.0348\nCAR 32.82 0.88 -\nDiffusion SeeSR+MD 26.808 0.727 0.0972\nRankSRGAN 26.55 0.75 0.128\nGANs (T2I-trained) Ours 26.871 0.735 0.0920\nESRGAN 26.22 0.75 0.124\nNFs SRFlow 27.09 0.76 0.120 Table2.Resultsof4xSRontheTest4Kdataset.Ourmethodcon-\nDiffusion SR3+YODA 27.24 0.77 0.127 sistentlyoutperformsSeeSR+MD,achievinghigherPSNR,SSIM,\n(SR-trained) SRDiff 27.41 0.79 0.136 andbetterperceptualquality(LPIPS),showcasingitsstrengthin\nDiWa 28.09 0.78 0.104 handlingextreme-resolutionSRtasks.\nDiffusion SeeSR+MD 24.28 0.67 0.110\n(T2I-trained) Ours 24.34 0.68 0.108 Type Methods PSNR↑ SSIM↑ LPIPS↓\nBicubic 26.103 0.750 0.1264\nTable1.Quantitativeresultsfor4xSRontheDIV2Kvalidationset. Regression\nRRDB 28.012 0.817 0.0754\nOurmethodoutperformsSeeSR+MDinbothperceptualquality\nGANs ESRGAN 24.789 0.723 0.0462\n(LPIPS)andpixel-basedmetrics(PSNRandSSIM).Regression-,\nNormalizingFlow-andGAN-basedSRmodelsarealsoincluded Diffusion SeeSR+MD 23.473 0.647 0.0822\nforcomparison,alongsidediffusionmodelstrainedspecificallyfor (T2I-trained) Ours 23.505 0.653 0.0796\nSRtasks(DIV2K+Flickr2K),contrarytoSeeSRandourmethod.\nTable 3. Results of 4x SR on the Test8K dataset. Our method\nsurpassesSeeSR+MDacrossallmetrics-PSNR,SSIM,andLPIPS\nWe further evaluated our method on the challenging -demonstratingitsscalabilityandsuperiorperformanceathigh\nTest4K and Test8K datasets to demonstrate its scalability resolutions.\nto extreme resolutions, a crucial requirement for modern\nSR tasks. As shown in Table 2, our method consistently\nRegression 23.0%\noutperformedSeeSR+MDonthe4xSRtaskfortheTest4K\nSeeSR+MD 25.3%\ndataset,achievingaPSNRof26.871,SSIMof0.735,and\nOurs 53.3%\nLPIPSof0.0920. Similarly,onthemoredemandingTest8K\n0 20 40 60 80 100\ndataset(seeTable3),ourmethodachievedaPSNRof23.505, Fool Rates [%]\nSSIM of 0.653, and LPIPS of 0.0796, clearly surpassing\nFigure 6. DIV2K Val fool rates (higher is better; photo-\nSeeSR+MDacrossbothpixel-basedandperceptualquality\nrealisticsamplesyieldafoolrateof50%). Regression(RRDB),\nmetrics.\nSeeSR+MD,andouroutputsarecomparedagainstgroundtruth.\nIn contrast to SeeSR, our approach not only achieves\nsuperior perceptual results but also excels in producing\nsharper,artifact-freeimageswithconsistentlyhigherPSNR\nandSSIMvalues. Byscalingbeyondthetypical512x512 5.4.PromptAnalysis\nlimit,ourmethodcontributessignificantlytowardsmaking\nWe analyze the textual information added by our local\nT2Ipracticalforreal-worldSRapplications.\ndegradation-awarepromptextractioninDIV2KValbycount-\ninguniquetags. For ourmethodandthesameLRimage,\n5.3.UserStudy\nwecountcommontagsbetweentheMDpathsasone. Thus,\nInspiredbySahariaetal. (SR3[37]),weconducteda2- ahighercountimpliesmoretailoredguidancefordifferent\nalternativeforced-choiceuserstudy. Weasked25subjects imageareas,supportingourhypothesisthatlocalprompts\n“Which of the two images is a better high-quality version contributetobetterrestorationofcomplexstructures.\nof the low-resolution image in the middle?” We selected As shown in Figure 7, our proposed local degradation-\n36 random 2K test images from DIV2K Val (12 for each awarepromptextractionmethodgeneratessignificantlymore\nmethod: RRDB,SeeSR+MD,andours),whichwerecenter- uniquetagsthanSeeSR.Also,ithighlightsthatwegenerate\ncroppedto512×512forbettervisualexamination. awiderrangeoftags,allowingforamoreprecisedescrip-\nTheresultofouruserstudyisshowninFigure6. Our tion. Naturally,increasedtagdiversityimproveslocaldetail\nmethodnotonlysurpassesRRDBandSeeSR+MD,achiev- description across the image and is further evidenced in\ningmorethandoubletheirfoolrates,butitalsoattainsafool Figure 8: Our local degradation-aware prompt extraction\nratecloseto50%,renderingitsoutputsnearlyindistinguish- includesthemajorityoftagsproducedbySeeSRbutalso\nablefromHRground-truthimages. addsdistinctones,asunderscoredbytheredcluster.\n7\n40\nGlobal (SeeSR) Global (SeeSR)\n25\nLocal (Ours) Local (Ours)\nCommon Tags\n20\n20\n15\n10\n0\n5\n0\n20\n10 20 30 40 50 60 70 80\nUnique Tag Counts\nFigure7. ThedistributionofuniquetagcountsforDIV2KVal. 40\n60 40 20 0 20 40 60\nTagsappearingacrossmultipleimagepatchesarecountedasone. t-SNE Dimension 1\nOurmethodgeneratesmoreuniquetagsoverall,withthehighest\nfrequencypeakingat35comparedtoSeeSR,whichpeaksaround Figure8.t-SNEofWord2Vecembeddedtagsbyourmethodand\n10.Theglobaltagextractionspansaround5-20uniquetags,while SeeSR,alongsidecommonuniquetags. Ourlocaltagextraction\nweproduce10-80. includessimilartags(line)andaddsmoreuniquetags(circle).\nThisenhancementinlocalcoherenceisoneoftheprimary\nOurextensiveexperimentsshowthatourmethodsignif-\nreasons our approach outperforms SeeSR in quantitative\nicantlyoutperformsthebaselineSeeSR+MDinbothqual-\nmetrics(suchasLPIPS)andqualitativevisualcomparisons,\nitativeandquantitativemetrics,particularlyinchallenging\nasdetailedinthepreviousresults.\nhigh-resolution scenarios. Enabling any high-resolutions,\nmaintaininglocalcoherence,andavoidingover-hallucination\n6.Limitation&FutureWork\nwhilegeneratingvisuallyaccuratedetailssetsanewbench-\nWhileourproposedlocaldegradation-awarepromptex- markforhigh-resolutionSRusingpre-trainedT2Imodels.\ntractionmethodsignificantlyimprovestheperformanceof\nT2Idiffusionmodelsforany-resolutionimageSR,itstillhas Acknowledgements\nroomforimprovementcomparedtotraditionalSRmethods,\nThis work was supported by the BMBF projects Sus-\nwhichareexplicitlydesignedandtrainedforSR.\ntainML(Grant101070408),Albatross(Grant01IW24002)\nFutureworkcouldexploreahybridapproachthatcom-\nandbyCarlZeissFoundationthroughtheSustainableEm-\nbinesthestrengthsoftraditionalSRmodelswithT2Imod-\nbeddedAIproject(P2021-02-009).\nelstoaddresstheperformancedifferencecomparedtoSR-\ntrained methods like RRDB and ESRGAN. For instance,\nReferences\nusing a GAN or CNN-based model to generate an initial\ncoarsesuper-resolvedimageandrefinementusingadiffu-\n[1] Elisabetta Adami and Carey Jewitt. Social media and the\nsionmodelcouldleadtoevenbetterquality.\nvisual,2016. 1\n[2] EirikurAgustssonandRaduTimofte.Ntire2017challengeon\n7.Conclusion\nsingleimagesuper-resolution:Datasetandstudy. InCVPRW,\npages126–135,2017. 4,5\nThisworkintroducedanovelapproachforextremeim-\n[3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\nage SR by leveraging pre-trained T2I diffusion models.\nMultidiffusion:Fusingdiffusionpathsforcontrolledimage\nThroughourproposedMDprocessandlocaldegradation-\ngeneration. 2023. 1,3,4,5\nawarepromptextraction,weenabledthegenerationofhigh-\n[4] JamesBetker,GabrielGoh,LiJing,TimBrooks,Jianfeng\nquality images at resolutions of 2K, 4K, and 8K without\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce\nanyadditionaltraining. Bydistributingthegenerationpro-\nLee, Yufei Guo, et al. Improving image generation with\ncess across multiple diffusion paths, our method ensures\nbetter captions. Computer Science. https://cdn. openai.\nglobalcoherence, whilelocalpromptextractionenhances com/papers/dall-e-3.pdf,2(3):8,2023. 1\nfine-graineddetailrestoration. Thisaddressesthelimitations\n[5] MarcoBevilacqua,AlineRoumy,ChristineGuillemot,and\noftraditionalT2ImodelsexploitedforimageSR,making Marie Line Alberi-Morel. Low-complexity single-image\nthemapplicabletoabroadrangeoftaskswhereclarityand super-resolutionbasedonnonnegativeneighborembedding.\nprecisionatlargescalesarecritical. 2012. 5\n8\nycneuqerF 2\nnoisnemiD\nENS-t\n[6] SawsenBoudraa, AhlemMelouah, andHayetFaridaMer- Towards blind image restoration with generative diffusion\nouani. Improvingmassdiscriminationinmammogram-cad prior. arXivpreprintarXiv:2308.15070,2023. 1,2\nsystemusingtextureinformationandsuper-resolutionrecon- [22] AnranLiu,YihaoLiu,JinjinGu,YuQiao,andChaoDong.\nstruction. EvolvingSystems,11(4):697–706,2020. 1 Blindimagesuper-resolution: Asurveyandbeyond. IEEE\n[7] XiangyuChen, XintaoWang, JiantaoZhou, YuQiao, and TPAMI,2022. 1\nChaoDong.Activatingmorepixelsinimagesuper-resolution [23] AndreasLugmayr,MartinDanelljan,AndresRomero,Fisher\ntransformer. InCVPR,pages22367–22377,2023. 2 Yu,RaduTimofte,andLucVanGool. Repaint: Inpainting\n[8] YinboChen,OliverWang,RichardZhang,EliShechtman, using denoising diffusion probabilistic models. In CVPR,\nXiaolong Wang, and Michael Gharbi. Image neural field 2022. 1\ndiffusionmodels. InCVPR,pages8007–8017,2024. 3 [24] AndreasLugmayr,MartinDanelljan,LucVanGool,andRadu\n[9] RuoyiDu,DongliangChang,TimothyHospedales,Yi-Zhe Timofte. Srflow: Learningthesuper-resolutionspacewith\nSong,andZhanyuMa. Demofusion: Democratisinghigh- normalizingflow. InECCV,2020. 5\nresolutionimagegenerationwithno$$$. InCVPR,pages [25] Cheng Ma, Yongming Rao, Yean Cheng, Ce Chen, Jiwen\n6159–6168,2024. 1 Lu,andJieZhou. Structure-preservingsuperresolutionwith\n[10] PatrickEsser,RobinRombach,andBjornOmmer. Taming gradientguidance. InCVPR,2020. 5\ntransformersforhigh-resolutionimagesynthesis. InCVPR, [26] David Martin, Charless Fowlkes, Doron Tal, and Jitendra\npages12873–12883,2021. 4 Malik. Adatabaseofhumansegmentednaturalimagesand\n[11] Stanislav Frolov, Tobias Hinz, Federico Raue, Jo¨rn Hees, its application to evaluating segmentation algorithms and\nandAndreasDengel. Adversarialtext-to-imagesynthesis:A measuringecologicalstatistics. InICCV,volume2,pages\nreview. NeuralNetworks,144:187–209,2021. 1 416–423.IEEE,2001. 5\n[12] StanislavFrolov,BrianBMoser,andAndreasDengel. Spotd- [27] YusukeMatsui, KotaIto, YujiAramaki, AzumaFujimoto,\niffusion:Afastapproachforseamlesspanoramageneration Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa.\novertime. arXivpreprintarXiv:2407.15507,2024. 1,3,4 Sketch-basedmangaretrievalusingmanga109dataset. Multi-\n[13] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda mediatoolsandapplications,76:21811–21838,2017. 5\nAskell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Das- [28] KangfuMei,ZhengzhongTu,MauricioDelbracio,Hossein\nsarma,DawnDrain,NelsonElhage,etal. Predictabilityand Talebi,VishalMPatel,andPeymanMilanfar. Biggerisnot\nsurpriseinlargegenerativemodels.In2022ACMConference alwaysbetter:Scalingpropertiesoflatentdiffusionmodels.\nonFairness,Accountability,andTransparency,2022. 1 arXivpreprintarXiv:2404.01367,2024. 3\n[14] Jia-BinHuang,AbhishekSingh,andNarendraAhuja. Single [29] BrianBMoser,StanislavFrolov,FedericoRaue,Sebastian\nimagesuper-resolutionfromtransformedself-exemplars. In Palacio,andAndreasDengel. Yoda:Youonlydiffuseareas.\nCVPR,pages5197–5206,2015. 5 anarea-maskeddiffusionapproachforimagesuper-resolution.\n[15] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B arXivpreprintarXiv:2308.07977,2023. 5\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec [30] BrianBMoser,StanislavFrolov,FedericoRaue,Sebastian\nRadford,JeffreyWu,andDarioAmodei. Scalinglawsfor Palacio,andAndreasDengel. Wavinggoodbyetolow-res:\nneurallanguagemodels. arXivpreprintarXiv:2001.08361, Adiffusion-waveletapproachforimagesuper-resolution. In\n2020. 1 2024 International Joint Conference on Neural Networks\n[16] XiangtaoKong,HengyuanZhao,YuQiao,andChaoDong. (IJCNN),pages1–8.IEEE,2024. 1,5\nClasssr:Ageneralframeworktoacceleratesuper-resolution [31] BrianBMoser,FedericoRaue,StanislavFrolov,Sebastian\nnetworks by data characteristic. In CVPR, pages 12016– Palacio,Jo¨rnHees,andAndreasDengel. Hitchhiker’sguide\n12025,2021. 4 tosuper-resolution:Introductionandrecentadvances. IEEE\n[17] ChristianLedig,LucasTheis,FerencHusza´r,JoseCaballero, TPAMI,45(8):9862–9882,2023. 1,5\nAndrew Cunningham, Alejandro Acosta, Andrew Aitken, [32] Brian B Moser, Arundhati S Shanbhag, Federico Raue,\nAlykhanTejani,JohannesTotz,ZehanWang,etal. Photo- Stanislav Frolov, Sebastian Palacio, and Andreas Dengel.\nrealisticsingleimagesuper-resolutionusingagenerativead- Diffusionmodels,imagesuper-resolution,andeverything:A\nversarialnetwork. InCVPR,2017. 5 survey.IEEETransactionsonNeuralNetworksandLearning\n[18] HaoyingLi,YifanYang,MengChang,ShiqiChen,Huajun Systems,2024. 1,5\nFeng,ZhihaiXu,QiLi,andYuetingChen. Srdiff: Single [33] YunpengQu,KunYuan,KaiZhao,QizhiXie,JinhuaHao,\nimagesuper-resolutionwithdiffusionprobabilisticmodels. MingSun, andChaoZhou. Xpsr: Cross-modalpriorsfor\nInNeurocomputing,2022. 1,2,5 diffusion-basedimagesuper-resolution. arXiv:2403.05049,\n[19] JingyunLiang,JiezhangCao,GuoleiSun,KaiZhang,Luc 2024. 2\nVanGool,andRaduTimofte.Swinir:Imagerestorationusing [34] FabioQuattrini,VittorioPippi,SilviaCascianelli,andRita\nswintransformer. InICCV,pages1833–1844,2021. 2 Cucchiara. Mergingandsplittingdiffusionpathsforsemanti-\n[20] BeeLim,SanghyunSon,HeewonKim,SeungjunNah,and callycoherentpanoramas. arXivpreprintarXiv:2408.15660,\nKyoungMuLee. Enhanceddeepresidualnetworksforsingle 2024. 1,3\nimagesuper-resolution. InCVPRW,pages136–144,2017. 5 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\n[21] XinqiLin,JingwenHe,ZiyanChen,ZhaoyangLyu,BenFei, Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nBoDai,WanliOuyang,YuQiao,andChaoDong. Diffbir: AmandaAskell,PamelaMishkin,JackClark,etal. Learning\n9\ntransferablevisualmodelsfromnaturallanguagesupervision. [51] WenlongZhang,YihaoLiu,ChaoDong,andYuQiao.Ranksr-\npages8748–8763.PMLR,2021. 2 gan:Generativeadversarialnetworkswithrankerforimage\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, super-resolution. InICCV,2019. 5\nPatrick Esser, and Bjo¨rn Ommer. High-resolution image [52] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,\nsynthesiswithlatentdiffusionmodels. InCVPR,2022. 1,2, ZhaochuanLuo,YanchunXie,YuzhuoQin,TongLuo,Yaqian\n4,5 Li,ShilongLiu,etal. Recognizeanything:Astrongimage\n[37] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali- taggingmodel. InCVPR,pages1724–1732,2024. 4\nmans,DavidJFleet,andMohammadNorouzi. Imagesuper- [53] ShangchenZhou,KelvinChan,ChongyiLi,andChenChange\nresolutionviaiterativerefinement. IEEETPAMI,2022. 1,2, Loy. Towardsrobustblindfacerestorationwithcodebook\n5,7 lookuptransformer. NeurIPS,35,2022. 2\n[38] Maria Schreiber. Audiences, aesthetics and affordances\nanalysingpracticesofvisualcommunicationonsocialmedia.\nDigitalCulture&Society,3(2):143–164,2017. 1\n[39] WenzheShi,JoseCaballero,FerencHusza´r,JohannesTotz,\nAndrewPAitken,RobBishop,DanielRueckert,andZehan\nWang. Real-timesingleimageandvideosuper-resolution\nusinganefficientsub-pixelconvolutionalneuralnetwork. In\nCVPR,pages1874–1883,2016. 1\n[40] JaeWoongSoh,GuYongPark,JunhoJo,andNamIkCho.\nNaturalandrealisticsingleimagesuper-resolutionwithex-\nplicitnaturalmanifolddiscrimination. InCVPR,2019. 5\n[41] RaduTimofte,ShuhangGu,JiqingWu,andLucVanGool.\nNtire2018challengeonsingleimagesuper-resolution:Meth-\nodsandresults. InCVPRW,2018. 5\n[42] JianyiWang,ZongshengYue,ShangchenZhou,KelvinCK\nChan,andChenChangeLoy. Exploitingdiffusionpriorfor\nreal-worldimagesuper-resolution. arXivpreprint,2023. 1,2\n[43] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy.\nRecoveringrealistictextureinimagesuper-resolutionbydeep\nspatialfeaturetransform. InCVPR,pages606–615,2018. 5\n[44] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,\nChaoDong,YuQiao,andChenChangeLoy. Esrgan: En-\nhancedsuper-resolutiongenerativeadversarialnetworks. In\nECCV,2018. 2,5\n[45] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale,\nTrungBui,TongYu,ZheLin,YangZhang,andShiyuChang.\nUncoveringthedisentanglementcapabilityintext-to-image\ndiffusionmodels. InCVPR,pages1900–1910,2023. 1\n[46] RongyuanWu,TaoYang,LingchenSun,ZhengqiangZhang,\nShuaiLi,andLeiZhang. Seesr: Towardssemantics-aware\nreal-worldimagesuper-resolution. arXiv:2311.16518,2023.\n1,2,4,5\n[47] TaoYang,RongyuanWu,PeiranRen,XuansongXie,and\nLeiZhang. Pixel-awarestablediffusionforrealisticimage\nsuper-resolutionandpersonalizedstylization. arXivpreprint\narXiv:2308.14469,2023. 1,2,5\n[48] RomanZeyde,MichaelElad,andMatanProtter. Onsingle\nimagescale-upusingsparse-representations. InInternational\nconferenceoncurvesandsurfaces,pages711–730.Springer,\n2010. 5\n[49] Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, Wei Niu,\nYushuWu,TianyunZhang,MalithJayaweera,DavidKaeli,\nBin Ren, et al. Achieving on-mobile real-time super-\nresolutionwithneuralarchitectureandpruningsearch. In\nICCV,pages4821–4831,2021. 1\n[50] LvminZhang,AnyiRao,andManeeshAgrawala. Adding\nconditional control to text-to-image diffusion models. In\nICCV,pages3836–3847,2023. 1\n10",
    "pdf_filename": "Zoomed_In,_Diffused_Out_Towards_Local_Degradation-Aware_Multi-Diffusion_for_Extreme_Image_Super-Reso.pdf"
}