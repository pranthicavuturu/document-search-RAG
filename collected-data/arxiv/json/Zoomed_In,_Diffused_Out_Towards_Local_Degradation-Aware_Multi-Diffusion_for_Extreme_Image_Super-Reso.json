{
    "title": "Zoomed In, Diffused Out Towards Local Degradation-Aware Multi-Diffusion for Extreme Image Super-Reso",
    "abstract": "Large-scale, pre-trained Text-to-Image (T2I) diffusion models have gained significant popularity in image gener- ation tasks and have shown unexpected potential in image Super-Resolution (SR). However, most existing T2I diffu- sion models are trained with a resolution limit of 512×512, making scaling beyond this resolution an unresolved but necessary challenge for image SR. In this work, we intro- duce a novel approach that, for the first time, enables these models to generate 2K, 4K, and even 8K images without any additional training. Our method leverages MultiDiffusion, which distributes the generation across multiple diffusion paths to ensure global coherence at larger scales, and local degradation-aware prompt extraction, which guides the T2I model to reconstruct fine local structures according to its low-resolution input. These innovations unlock higher reso- lutions, allowing T2I diffusion models to be applied to image SR tasks without limitation on resolution.",
    "body": "Zoomed In, Diffused Out: Towards Local Degradation-Aware Multi-Diffusion for\nExtreme Image Super-Resolution\nBrian B. Moser1,2, Stanislav Frolov1,2, Tobias C. Nauen1,2, Federico Raue1, Andreas Dengel1,2\n1German Research Center for Artificial Intelligence\n2University of Kaiserslautern-Landau\nfirst.last@dfki.de\nAbstract\nLarge-scale, pre-trained Text-to-Image (T2I) diffusion\nmodels have gained significant popularity in image gener-\nation tasks and have shown unexpected potential in image\nSuper-Resolution (SR). However, most existing T2I diffu-\nsion models are trained with a resolution limit of 512×512,\nmaking scaling beyond this resolution an unresolved but\nnecessary challenge for image SR. In this work, we intro-\nduce a novel approach that, for the first time, enables these\nmodels to generate 2K, 4K, and even 8K images without any\nadditional training. Our method leverages MultiDiffusion,\nwhich distributes the generation across multiple diffusion\npaths to ensure global coherence at larger scales, and local\ndegradation-aware prompt extraction, which guides the T2I\nmodel to reconstruct fine local structures according to its\nlow-resolution input. These innovations unlock higher reso-\nlutions, allowing T2I diffusion models to be applied to image\nSR tasks without limitation on resolution.\n1. Introduction\nImage Super-Resolution (SR) is vital for a wide range\nof real-world applications, including satellite imaging, med-\nical diagnostics, and consumer photography, where high-\nresolution outputs (e.g., 2K, 4K, or 8K) are essential for\ncapturing fine details and ensuring clarity [1,6,38,39,49].\nAlthough SR methods, particularly those using local opera-\ntions like CNNs, have made significant progress, handling\ncomplex degradation in Low-Resolution (LR) inputs remains\na persistent challenge [22,31,32].\nRecently, diffusion models, particularly pre-trained Text-\nto-Image (T2I) diffusion models, have revolutionized image\ngeneration tasks [11, 23, 45, 50]. Originally designed for\ncreative applications like text-guided image synthesis, these\nmodels have demonstrated strong potential in image SR, es-\npecially in handling 4x scaling and beyond, where hallucinat-\ning fine details becomes essential [30,32]. Diffusion-based\n4x SR prediction (SeeSR + MD)\n4x SR prediction (Ours)\nlocal\nawareness\nsupports\nbigger than\n512x512\nlocal\nawareness\nsupports\nbigger than\n512x512\n(    )\npenguins\nstones\nFigure 1. Comparison of 4x Super-Resolution (SR) predictions us-\ning SeeSR + MultiDiffusion (MD) and our proposed method. While\nMD unlocks higher resolutions beyond 512×512, our proposed\nstrategy of extracting local degradation-aware prompts ensures lo-\ncal detail awareness, improving fine-grained structure restoration,\nas demonstrated in the stones regions.\nSR models, such as SR3, DiffBIR, and SRDiff, have already\nachieved impressive results by generating realistic details\nat these scales using conventional SR datasets and training\nthem from scratch [18,21,37].\nOn the other side, the development of T2I models acceler-\nates, as exemplified by astonishing applications like Dall-E\nand StableDiffusion [4,36]. Driven by training on increas-\ningly large and diverse datasets, they present a great resource\nfor image SR [13,15]. By repurposing their vast generative\ncapabilities, these models can unlock new possibilities for\nHigh-Resolution (HR) image enhancement. Inspired by this\nidea, methods like StableSR, PASD, and SeeSR have repur-\nposed pre-trained T2I models to SR, achieving impressive\nresults in limited-resolution scenarios [42,46,47].\nYet, all T2I diffusion models repurposed for image SR are\nlimited by a resolution of 512×512, which is impractical for\nreal-world applications and a major limitation [3,9,12,34].\n1\narXiv:2411.12072v1  [cs.CV]  18 Nov 2024\n\nIn this work, we introduce a novel solution that, for the\nfirst time, enables pre-trained T2I diffusion models to gener-\nate SR images at resolutions of 2K, 4K, and even 8K, which\nwe coin extreme image SR, without any additional training.\nOur approach leverages two core components: (1) Multi-\nDiffusion, which distributes the image generation process\nacross multiple diffusion paths to ensure global coherence,\nand (2) local degradation-aware prompt extraction, which\nprovides fine-grained control over local structures, allowing\nthe model to effectively restore detailed textures and se-\nmantic consistency. By integrating local degradation-aware\nprompt extraction, our method ensures consistent semantic\naccuracy across all image scales, setting a new standard for\nextreme image SR.\nIn summary, our key contributions are as follows:\n• We introduce MultiDiffusion to image SR, which\nenables scaling T2I diffusion models (trained for\n512x512) beyond 512x512 by coordinating multiple\ndiffusion paths for global coherence.\n• We propose local degradation-aware prompt extraction,\nwhich enhances the ability to preserve local details and\nstructural integrity.\n• We demonstrate, for the first time, the capability of\npre-trained T2I models to generate 2K, 4K, and 8K\nimages without training and set an initial benchmark\nfor extreme image SR alongside classical methods.\n2. Related Work\nThis section briefly reviews the current field of image SR\nand how T2I is exploited for image SR.\n2.1. Image SR\nImage SR has seen significant advancements through the\ndevelopment of CNNs. Classical CNN-based SR models\n(including vision transformers), such as RRDB, ESRGAN,\nSwinIR, and HAT, excel in upscaling images to any resolu-\ntion due to their local operations [7,19,44]. They leverage\nlocal receptive fields, enabling them to process and train on\nrelatively small image patches (e.g., 192x192) while gen-\neralizing well to larger images. This property makes them\nhighly scalable for SR tasks across various resolutions.\nMore recently, diffusion models have emerged as power-\nful alternatives for SR, particularly in handling higher scaling\nfactors such as 4x and beyond. Models like SR3, DiffBIR,\nand SRDiff have demonstrated strong capabilities in hal-\nlucinating fine details required at these scales [18, 21, 37].\nHowever, these diffusion-based SR models typically need to\nbe trained on the target size and are often limited to resolu-\ntions of 512x512 due to architectural and training constraints,\nwhich restrict their flexibility and practicality compared to\nCNN-based methods that can be applied to any resolution.\nIn contrast, our approach introduces a novel method to\nscale diffusion models beyond 512x512 without retraining,\nwhich we termed extreme image SR (i.e., 2K, 4K, and 8K).\n2.2. Exploiting T2I for SR\nText-to-Image (T2I) conditioning has emerged as a\npromising approach for enhancing Super-Resolution (SR)\ntasks by leveraging the capabilities of pre-trained T2I models.\nFine-tuning these models, with the addition of specialized\nencoders suited for SR, allows for integrating textual de-\nscriptions into image enhancement. This fusion of textual\ninformation can provide a richer guidance source, poten-\ntially improving the accuracy and contextual relevance of\nsynthesized images in SR applications.\nFor instance, Wang et al. introduced StableSR, which\nexploits text guidance by incorporating a time-aware en-\ncoder trained alongside a frozen Latent Diffusion Model\n(LDM) [36, 42]. This framework utilizes trainable spatial\nfeature transform layers to condition the model based on\ninput images. An optional controllable feature wrapping\nmodule further enhances StableSR’s adaptability, allowing\nfor fine-grained user control. The design of this module\ndraws inspiration from CodeFormer, contributing to Sta-\nbleSR’s flexibility in addressing diverse user needs and pref-\nerences [53].\nSimilarly, Yang et al. proposed Pixel-Aware Stable Diffu-\nsion (PASD), which advances the conditioning process by\nembedding text descriptions of LR inputs using a CLIP text\nencoder [35,47]. This method improves the model’s capacity\nto synthesize images with greater precision by embedding\ncontextual information from textual sources, enhancing the\noverall fidelity and relevance of the generated images.\nConcurrently, other methods like SeeSR explore similar\nT2I conditioning frameworks, while XPSR takes this concept\nfurther by merging different levels of semantic text encod-\nings [33,46]. XPSR combines high-level encodings (image\ncontent) with low-level encodings (quality perception, sharp-\nness, noise, and other LR image distortions), further refining\nthe SR results through a more nuanced understanding of\nboth content and perceptual quality.\n3. Methodology\nThis section introduces our novel approach to extreme\nimage SR by leveraging pre-trained T2I diffusion models.\nFor the first time, our method enables T2I models repur-\nposed for SR to achieve resolutions of 2K, 4K, and 8K\nwithout additional training but with local coherence. The\napproach is built upon two core innovations: MultiDiffusion,\nwhich ensures global coherence at high resolutions, and local\ndegradation-aware prompt extraction, which enhances local\ndetail recovery. Figure 2 illustrates the overall concept of\nour method.\n2\n\nImage\nEncoder\nTagging\nHead\n\"beak, penguin,\nstone, rocky\"\nControlled\nT2I\nModel\nImage\nEncoder\nTagging\nHead\n\"sea, stone,\nrock formation\"\nControlled\nT2I\nModel\nLocal Degradation-Aware Tag Extraction\nMultiDiffuser\nImage\nEncoder\nTagging\nHead\n\"beak, penguin,\nstone, rocky\"\nControlled\nT2I\nModel\nGlobal Degradation-Aware Tag Extraction\nlimited to 512x512 images &\nOurs\nSeeSR\none prompt for entire image\nFigure 2.\nIllustration of SeeSR (top) compared to our local degradation-aware method (bottom). While SeeSR is limited to a fixed\nimage size of 512×512, our method can technically upscale to any resolution due to two components: MultiDiffusion (MD) and local\ndegradation-aware prompt extraction. The MD process is applied to overlapping tiles. Without local degradation-aware prompt extraction,\nthe classifier guidance generates hallucinated details based on global prompts that describe the entire image, leading to inconsistencies in\nlocal tile content. Our approach incorporates local tag extraction and, thereby, provides tile-specific prompts, ensuring more accurate and\ncoherent detail generation across the entire image.\n3.1. MultiDiffusion Process\nMultiDiffusion (MD) allows the model to generate high-\nresolution images by distributing the image synthesis across\nmultiple diffusion paths [3, 12, 34]. At each stage, the la-\ntent feature map is divided into overlapping tiles, each of\nwhich undergoes a separate diffusion process. This tech-\nnique maintains global coherence by sharing overlapping\ninformation between adjacent tiles. The final image is syn-\nthesized by merging the outputs from these multiple paths,\nensuring consistency in both global structure and local detail.\nUnlike traditional T2I diffusion methods, which are limited\nto 512×512 pixels, this process enables resolutions of 2K\nand beyond without any retraining or fine-tuning.\nMore concretely, let Φ : L × Y →L be a pre-trained\nT2I model that operates in the latent space L = RW ×H×C\nand condition space Y, e.g., textual prompts. Given a noisy\nlatent representation LT ∼N(0, I) and a condition y ∈Y,\nthe T2I diffusion model Φ produces a sequence of latents\nstarting from LT and gradually denoising it towards the\nclean latent representation L0 that can be decoded to a HR\napproximation:\nLT , LT −1, . . . , L0\nsuch that\nLt−1 = Φ(Lt | y).\n(1)\nSince we want to unlock the generation of images\nlarger than 512×512 pixels (i.e., latent codes larger than\n64×64), our goal is the generation latent representa-\ntions MT , MT −1, . . . , M0 in a new latent space M =\nRW ′×H′×C, with W ′ ≥W, H′ ≥H, using the same pre-\ntrained model Φ without any retraining or fine-tuning. Tradi-\ntionally, models pre-trained on fixed-size latents cannot be\ndirectly applied to produce latents of arbitrary sizes [8,28].\nTo address this, MD extends the diffusion process by\napplying a joint diffusion approach, where multiple over-\nlapping latent windows are merged via averaging. More\nformally, we define n mappings Fi : M ∈RW ′×H′×C →\nL ∈RW ×H×C with i ∈{1, . . . , n}, which map (or crop)\nthe larger latent space M into n latent representations of the\noriginal size W × H (i.e., 64×64).\nThe number of overlapping latent crops n is defined as\nn =\nW ′−W\nω\n+ 1, where ω represents the stride between\nadjacent cropping windows. With these mappings, the de-\nnoising process is applied independently to each cropped\nlatent window. Subsequently, the latent representations are\nstitched back to the original size M = RW ′×H′×C, i.e.\nMt−1 = MultiDiffuser({Li\nt−1}n\ni=1), which averages the\noverlapping latent regions.\n3\n\n3.2. Local Degradation-Aware Prompt Extraction\nAs demonstrated in the introductory example in Figure 1,\nsolely applying MD often results in images with excessive\nhallucinated details. This occurs because the global prompt\nused for classifier guidance in the diffusion process applies\nuniformly across all tiles. While the prompt effectively\ndescribes the global structure of the image, it lacks the local\ncontent specificity needed for each tile. Consequently, the\nmodel attempts to reconstruct fine details in every tile based\non global information, leading to over-hallucination and\ninconsistencies in local structures.\nMore formally, let y ∈Y be a condition drawn from a\nprompt extractor. As Fi is guiding the diffusion model Φ\nwith the same prompt y, i.e., Φ(Li\nt | y) for all possible win-\ndows i ∈{1, . . . , n}, the larger latent feature map resulting\nfrom this MD process will have the conditioning information\ny infused at every spatial position. Hence, global prompts\nalone are insufficient for reconstructing region-specific con-\ntent, as they lack the granularity to capture the unique local\ndetails within each patch.\nTo ensure the coherence of local structure and details, we\npropose a local degradation-aware prompt extraction, which\ndescribes the local degradation patterns in LR inputs (see\nnext Section for concrete extractor). In more detail, if n is\nthe number of maps (or crops), we propose to condition the\nMD process with a set of n conditions, i.e., {y1, . . . , yn},\nwhere Fi uses the condition yi. These prompts guide the\ndiffusion model in reconstructing detailed textures and struc-\ntures during the SR process. By leveraging these prompts,\nour method effectively reduces artifacts and noise, particu-\nlarly at high resolutions (e.g., 2K, 4K, and 8K), and ensures\naccurate restoration of fine-grained details. As a result, our\napproach, as outlined in Algorithm 1, significantly mitigates\nthe common issues of over-hallucination and visual artifacts\nthat occur when relying solely on global prompt guidance.\n3.3. Exploiting Pre-Trained Prompt Extractors\nOur method is designed to be flexible and compatible\nwith any pre-trained T2I diffusion model and prompt extrac-\ntion strategy. Still, in this work, we specifically build upon\nthe pre-trained models utilized by SeeSR [46]. Specifically,\nwe utilize the pre-trained StableDiffusion V2.0 model, along\nwith its 8× compression VAE, and the Degradation-Aware\nPrompt Extractor (DAPE), which itself is a fine-tuned tag-\nstyle prompt extraction model, i.e., RAM [10, 36, 46, 52].\nThese components have proven effective for real-world SR\ntasks, allowing us to leverage their robust generative capabil-\nities for higher-resolution image synthesis.\nFor the MD process, we adopt the standard configuration\nof dividing the latent space into 64x64 patches [3]. This\napproach ensures efficient global coherence across the im-\nage while maintaining the fine-grained structure. Inspired\nby SpotDiffusion, we reduce the overlap between patches\nAlgorithm 1 Local Degradation-Aware MultiDiffusion\nRequire: Φ {pre-trained T2I diffusion model}\nφ {pre-trained prompt extractor}\n{Fi}n\ni=1 {non-overlapping latent mappings}\n{Ii}n\ni=1 {corresponding image patches}\nMT ∼N(0, I) {noisy initialization}\n1: for each window i = 1, . . . , n do\n2:\nyi ←φ (Ii) {extract local prompts}\n3: end for\n4: for t = T, T −1, . . . , 0 do\n5:\nfor each window i = 1, . . . , n do\n6:\nLi\nt−1 ←Φ(Fi (Mt) | yi) {denoise window}\n7:\nend for\n8:\nMt−1 ←MultiDiffuser({Li\nt−1}n\ni=1)\n9: end for\n10: return M0\nby setting the stride to 32, ensuring a balance between com-\nputational efficiency and image consistency [12]. Notably,\nwe chose not to apply the non-overlapping striding strat-\negy proposed in SpotDiffusion, as it significantly degraded\nthe SR results for LR inputs by failing to capture necessary\ncontextual information across patches.\nFor the local degradation-aware prompt extraction, we\napply DAPE on the corresponding tiles in image space (i.e.,\nextracted from 512×512 image patches) that would undergo\nthe individual diffusion steps in Fi in latent space. This\nextraction ensures that each patch receives locally relevant\nprompts, enhancing fine detail restoration. Interestingly,\nthis process results in averaging overlapping latent patches\ngenerated under varying prompt conditions. This dynamic\nuse of local prompts helps prevent inconsistencies between\nadjacent patches and avoids over-hallucination, which is\ncommon when using global prompts in MD [3].\n4. Experimental Setup\nIn this section, we detail the datasets, models, and evalua-\ntion metrics used to assess the performance of our proposed\nmethod, comparing it against both classical and diffusion-\nbased SR models across various extreme-resolution tasks.\nThe code for our experiments can be found on GitHub 1,\nwhich complements the official implementation of SeeSR.\n4.1. Datasets\nFor our experiments, we utilized the DIV2K validation\nset as well as the Test4K and Test8K datasets introduced by\nClassSR [2,16]. These datasets provide a robust benchmark\nfor SR tasks, allowing us to evaluate the performance of our\nmethod across a range of high resolutions (i.e., 2K, 4K, and\n8K), which we coin extreme image SR.\n1https://github.com/Brian-Moser/zido\n4\n\nLR\nSeeSR + MD\nOurs\nHR\nFigure 3. Qualitative comparison of a 2K image (899; DIV2K Val) between LR, SeeSR+MD (PSNR↑:25.273, SSIM↑:0.769, LPIPS↓:0.130),\nour method (PSNR↑:26.217, SSIM↑:0.794, LPIPS↓:0.103), and HR. In general, we observe that our method reconstructs details in\nbackground objects better than SeeSR+MD (see light patterns in the lower left corner).\nSince our approach fully leverages pre-trained T2I models\nwithout any additional training, we did not use the classical\nSR training datasets like DIV2K (train) and Flickr2K [2,41].\nOur focus on enabling extreme SR at resolutions far beyond\n512×512 also led us to exclude conventional SR evaluation\ndatasets, such as Set5, Set14, BSDS100, Manga109, and\nUrban100 [5,14,26,27,48]. These datasets are commonly\nused in SR research but are limited in resolution, typically\ncapping at 512×512, making them unsuitable for testing the\nfull capabilities of our method [3,36,46,47]. By focusing on\nextreme resolutions, we aim to demonstrate the scalability\nand versatility of our approach in supporting any-resolution\nSR with pre-trained models, particularly for 2K, 4K, and 8K.\nFor the scaling factor, we selected a 4× magnification,\nmeaning that the HR images are four times larger in spatial\ndimensions than their LR counterparts. This scaling factor is\ncommon in diffusion-based SR benchmarks and is generally\nconsidered challenging [31,32].\n4.2. Models & Metrics\nWe compare a variety of standard SR approaches, in-\ncluding regression-based methods like EDSR, RRDB, and\nCAR, GAN-based methods such as RankSRGAN and ES-\nRGAN, as well as the normalizing flow approach SRFlow\n[17,20,24,25,40,43,44,51]. Additionally, we include dif-\nfusion models that are explicitly trained for image SR, such\nas SR3+YODA, SRDiff, and DiWa, to provide a broader\ncomparison [18,29,30,37].\nTo evaluate the quality of the reconstructed images, we\nuse both pixel-based metrics, namely PSNR and SSIM,\nas well as the perceptual quality metric LPIPS [31, 32].\nPixel-based metrics measure structural similarity and re-\nconstruction accuracy, while LPIPS provides a more human-\nperception-aligned evaluation of image quality, allowing us\nto assess both fidelity and perceptual realism. Note that\nclassical SR approaches typically exhibit much higher pixel-\nbased scores than diffusion models, as diffusion models are\nprone to hallucinated details [37].\nAdditionally, lower scores are expected in our method\nsince the mentioned models are explicitly SR-trained,\nwhereas we repurpose pre-trained T2I models without SR-\ntraining (thereby modifying SeeSR with MD as baseline).\n5. Results\nThis section presents qualitative and quantitative com-\nparisons between our approach and SeeSR modified with\nMD (leading in SR-repurposed T2I methods). Since no pre-\ntrained T2I models have been previously tested for extreme\nimage SR (resolutions larger than 512×512), SeeSR with\nMD serves as our primary baseline for evaluation.\n5.1. Qualitative Results\nTo assess the effectiveness of our proposed approach vi-\nsually, we focus on challenging yet visible cases from the\nDIV2K validation set to highlight the improvements in fine\ndetail preservation, local coherence, and overall perceptual\nquality. As shown in Figure 3, our method demonstrates\nsignificant improvements in recovering fine details and gen-\nerating more coherent structures compared to SeeSR+MD.\nIn the 2K resolution image, our approach not only preserves\nthe global structure but also reconstructs background objects,\nsuch as the light patterns in the lower-left corner, with more\naccuracy and clarity (i.e., without artifacts). This is due to\nthe local degradation-aware prompt extraction, which en-\nables more precise local detail reconstruction, avoiding the\nhallucination of irrelevant global features.\nIn Figure 4 and Figure 5, we further illustrate how our\nmethod handles the presence of background textures by tak-\ning a closer look at zoomed-in regions. In the first example\n(Figure 4), SeeSR+MD hallucinates feather-like patterns in\nthe leaves due to the global prompt guidance, leading to\ninconsistent results (see wavy structures above the leaf). By\ncontrast, our method maintains local coherence and recon-\nstructs the leaves more naturally and accurately. For the\nsecond example (Figure 5), we can observe fur-like patterns\nin the dirt in SeeSR, contrary to ours. Although both meth-\n5\n\nHR\nSeeSR + MD\nOurs\nFigure 4.\nQualitative comparison (886; DIV2K Val) between\nSeeSR+MD (PSNR↑:26.252, SSIM↑:0.766, LPIPS↓:0.123) and\nour approach (PSNR↑:28.115, SSIM↑:0.802, LPIPS↓:0.091). The\nglobal tags were “balustrade, bird, blue, fence, green, macaw, par-\nrot, perch, pole, rail, sit, stand, yellow”. While SeeSR+MD halluci-\nnates bird patterns on the leaves in the background due to global\nprompt guidance, our approach preserves local coherence by recon-\nstructing leaves more naturally. However, although the background\nis content-wise accurate, our method introduces more fine-grained,\nblurry-free details than those present in the original HR image.\nods occasionally introduce more fine-grained details than\nthose in the original HR image, our method avoids artifacts\nand hallucinations, ensuring perceptually consistent results.\nOverall, our qualitative results demonstrate that the local\ndegradation-aware prompt extraction significantly outper-\nforms SeeSR+MD in generating high-quality super-resolved\nimages, particularly at extreme resolutions. The improve-\nments are especially notable in preserving local textures and\navoiding over-hallucination, which is a common issue in\nMD-based models guided by global prompts. The addition\nof local degradation-aware prompts not only ensures more\nconsistent textures but also provides a sharper, artifact-free\nreconstruction of fine details, making our approach particu-\nlarly advantageous for complex scenes where global prompts\nalone fail to maintain coherence.\nHR\nSeeSR + MD\nOurs\nFigure 5.\nQualitative comparison (809; DIV2K Val) between\nSeeSR+MD (PSNR↑:25.107, SSIM↑:0.598, LPIPS↓:0.076) and\nour approach (PSNR↑:25.627, SSIM↑:0.621, LPIPS↓:0.069). The\nglobal tags were “animal, break, floor, grass, green, lay, lion, lush,\nman, mane, mouth, relax, tree”. Similarly, SeeSR+MD halluci-\nnates fur-like patterns in the brown dirt, leading to artifacts that\ndegrade the visual quality and contribute to its inferior performance\ncompared to our approach, which preserves the natural texture of\nthe dirt more effectively. Once again, our method generates finer,\nsharper details that surpass the level found in the HR image.\n5.2. Quantitative Results\nTo thoroughly evaluate the performance of our method,\nwe conducted experiments on the DIV2K validation set,\nas well as the Test4K and Test8K datasets. Our results\nare compared against various existing SR methods, includ-\ning regression-based, GAN-based, and diffusion-based ap-\nproaches (trained explicitly on SR data).\nAs shown in Table 1, our method outperforms SeeSR\nequipped with MD regarding perceptual quality metrics such\nas LPIPS while achieving competitive results on pixel-based\nmetrics like PSNR and SSIM. Specifically, for 4x SR on\nDIV2K, our method achieved a PSNR of 24.34, SSIM of\n0.68, and LPIPS of 0.108, representing a clear improvement\nin image quality compared to SeeSR+MD.\n6\n\nType\nMethods\nPSNR ↑\nSSIM ↑\nLPIPS ↓\nRegression\nBicubic\n26.70\n0.77\n0.409\nEDSR\n28.98\n0.83\n0.270\nRRDB\n29.44\n0.84\n0.253\nCAR\n32.82\n0.88\n-\nGANs\nRankSRGAN\n26.55\n0.75\n0.128\nESRGAN\n26.22\n0.75\n0.124\nNFs\nSRFlow\n27.09\n0.76\n0.120\nDiffusion\nSR3 + YODA\n27.24\n0.77\n0.127\n(SR-trained)\nSRDiff\n27.41\n0.79\n0.136\nDiWa\n28.09\n0.78\n0.104\nDiffusion\nSeeSR + MD\n24.28\n0.67\n0.110\n(T2I-trained)\nOurs\n24.34\n0.68\n0.108\nTable 1. Quantitative results for 4x SR on the DIV2K validation set.\nOur method outperforms SeeSR+MD in both perceptual quality\n(LPIPS) and pixel-based metrics (PSNR and SSIM). Regression-,\nNormalizing Flow- and GAN-based SR models are also included\nfor comparison, alongside diffusion models trained specifically for\nSR tasks (DIV2K+Flickr2K), contrary to SeeSR and our method.\nWe further evaluated our method on the challenging\nTest4K and Test8K datasets to demonstrate its scalability\nto extreme resolutions, a crucial requirement for modern\nSR tasks. As shown in Table 2, our method consistently\noutperformed SeeSR+MD on the 4x SR task for the Test4K\ndataset, achieving a PSNR of 26.871, SSIM of 0.735, and\nLPIPS of 0.0920. Similarly, on the more demanding Test8K\ndataset (see Table 3), our method achieved a PSNR of 23.505,\nSSIM of 0.653, and LPIPS of 0.0796, clearly surpassing\nSeeSR+MD across both pixel-based and perceptual quality\nmetrics.\nIn contrast to SeeSR, our approach not only achieves\nsuperior perceptual results but also excels in producing\nsharper, artifact-free images with consistently higher PSNR\nand SSIM values. By scaling beyond the typical 512x512\nlimit, our method contributes significantly towards making\nT2I practical for real-world SR applications.\n5.3. User Study\nInspired by Saharia et al. (SR3 [37]), we conducted a 2-\nalternative forced-choice user study. We asked 25 subjects\n“Which of the two images is a better high-quality version\nof the low-resolution image in the middle?” We selected\n36 random 2K test images from DIV2K Val (12 for each\nmethod: RRDB, SeeSR+MD, and ours), which were center-\ncropped to 512 × 512 for better visual examination.\nThe result of our user study is shown in Figure 6. Our\nmethod not only surpasses RRDB and SeeSR+MD, achiev-\ning more than double their fool rates, but it also attains a fool\nrate close to 50%, rendering its outputs nearly indistinguish-\nable from HR ground-truth images.\nType\nMethods\nPSNR ↑\nSSIM ↑\nLPIPS ↓\nRegression\nBicubic\n31.749\n0.843\n0.0817\nRRDB\n33.931\n0.887\n0.0536\nGANs\nESRGAN\n30.609\n0.810\n0.0348\nDiffusion\nSeeSR + MD\n26.808\n0.727\n0.0972\n(T2I-trained)\nOurs\n26.871\n0.735\n0.0920\nTable 2. Results of 4x SR on the Test4K dataset. Our method con-\nsistently outperforms SeeSR+MD, achieving higher PSNR, SSIM,\nand better perceptual quality (LPIPS), showcasing its strength in\nhandling extreme-resolution SR tasks.\nType\nMethods\nPSNR ↑\nSSIM ↑\nLPIPS ↓\nRegression\nBicubic\n26.103\n0.750\n0.1264\nRRDB\n28.012\n0.817\n0.0754\nGANs\nESRGAN\n24.789\n0.723\n0.0462\nDiffusion\nSeeSR + MD\n23.473\n0.647\n0.0822\n(T2I-trained)\nOurs\n23.505\n0.653\n0.0796\nTable 3. Results of 4x SR on the Test8K dataset. Our method\nsurpasses SeeSR+MD across all metrics - PSNR, SSIM, and LPIPS\n- demonstrating its scalability and superior performance at high\nresolutions.\n0\n20\n40\n60\n80\n100\nFool Rates [%]\nOurs\nSeeSR+MD\nRegression\n53.3%\n25.3%\n23.0%\nFigure 6.\nDIV2K Val fool rates (higher is better; photo-\nrealistic samples yield a fool rate of 50%). Regression (RRDB),\nSeeSR+MD, and our outputs are compared against ground truth.\n5.4. Prompt Analysis\nWe analyze the textual information added by our local\ndegradation-aware prompt extraction in DIV2K Val by count-\ning unique tags. For our method and the same LR image,\nwe count common tags between the MD paths as one. Thus,\na higher count implies more tailored guidance for different\nimage areas, supporting our hypothesis that local prompts\ncontribute to better restoration of complex structures.\nAs shown in Figure 7, our proposed local degradation-\naware prompt extraction method generates significantly more\nunique tags than SeeSR. Also, it highlights that we generate\na wider range of tags, allowing for a more precise descrip-\ntion. Naturally, increased tag diversity improves local detail\ndescription across the image and is further evidenced in\nFigure 8: Our local degradation-aware prompt extraction\nincludes the majority of tags produced by SeeSR but also\nadds distinct ones, as underscored by the red cluster.\n7\n\n10\n20\n30\n40\n50\n60\n70\n80\nUnique Tag Counts\n0\n5\n10\n15\n20\n25\nFrequency\nGlobal (SeeSR)\nLocal (Ours)\nFigure 7. The distribution of unique tag counts for DIV2K Val.\nTags appearing across multiple image patches are counted as one.\nOur method generates more unique tags overall, with the highest\nfrequency peaking at 35 compared to SeeSR, which peaks around\n10. The global tag extraction spans around 5-20 unique tags, while\nwe produce 10-80.\nThis enhancement in local coherence is one of the primary\nreasons our approach outperforms SeeSR in quantitative\nmetrics (such as LPIPS) and qualitative visual comparisons,\nas detailed in the previous results.\n6. Limitation & Future Work\nWhile our proposed local degradation-aware prompt ex-\ntraction method significantly improves the performance of\nT2I diffusion models for any-resolution image SR, it still has\nroom for improvement compared to traditional SR methods,\nwhich are explicitly designed and trained for SR.\nFuture work could explore a hybrid approach that com-\nbines the strengths of traditional SR models with T2I mod-\nels to address the performance difference compared to SR-\ntrained methods like RRDB and ESRGAN. For instance,\nusing a GAN or CNN-based model to generate an initial\ncoarse super-resolved image and refinement using a diffu-\nsion model could lead to even better quality.\n7. Conclusion\nThis work introduced a novel approach for extreme im-\nage SR by leveraging pre-trained T2I diffusion models.\nThrough our proposed MD process and local degradation-\naware prompt extraction, we enabled the generation of high-\nquality images at resolutions of 2K, 4K, and 8K without\nany additional training. By distributing the generation pro-\ncess across multiple diffusion paths, our method ensures\nglobal coherence, while local prompt extraction enhances\nfine-grained detail restoration. This addresses the limitations\nof traditional T2I models exploited for image SR, making\nthem applicable to a broad range of tasks where clarity and\nprecision at large scales are critical.\n60\n40\n20\n0\n20\n40\n60\nt-SNE Dimension 1\n40\n20\n0\n20\n40\nt-SNE Dimension 2\nGlobal (SeeSR)\nLocal (Ours)\nCommon Tags\nFigure 8. t-SNE of Word2Vec embedded tags by our method and\nSeeSR, alongside common unique tags. Our local tag extraction\nincludes similar tags (line) and adds more unique tags (circle).\nOur extensive experiments show that our method signif-\nicantly outperforms the baseline SeeSR+MD in both qual-\nitative and quantitative metrics, particularly in challenging\nhigh-resolution scenarios. Enabling any high-resolutions,\nmaintaining local coherence, and avoiding over-hallucination\nwhile generating visually accurate details sets a new bench-\nmark for high-resolution SR using pre-trained T2I models.\nAcknowledgements\nThis work was supported by the BMBF projects Sus-\ntainML (Grant 101070408), Albatross (Grant 01IW24002)\nand by Carl Zeiss Foundation through the Sustainable Em-\nbedded AI project (P2021-02-009).\nReferences\n[1] Elisabetta Adami and Carey Jewitt. Social media and the\nvisual, 2016. 1\n[2] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on\nsingle image super-resolution: Dataset and study. In CVPRW,\npages 126–135, 2017. 4, 5\n[3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\nMultidiffusion: Fusing diffusion paths for controlled image\ngeneration. 2023. 1, 3, 4, 5\n[4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce\nLee, Yufei Guo, et al. Improving image generation with\nbetter captions.\nComputer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf, 2(3):8, 2023. 1\n[5] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and\nMarie Line Alberi-Morel.\nLow-complexity single-image\nsuper-resolution based on nonnegative neighbor embedding.\n2012. 5\n8\n\n[6] Sawsen Boudraa, Ahlem Melouah, and Hayet Farida Mer-\nouani. Improving mass discrimination in mammogram-cad\nsystem using texture information and super-resolution recon-\nstruction. Evolving Systems, 11(4):697–706, 2020. 1\n[7] Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and\nChao Dong. Activating more pixels in image super-resolution\ntransformer. In CVPR, pages 22367–22377, 2023. 2\n[8] Yinbo Chen, Oliver Wang, Richard Zhang, Eli Shechtman,\nXiaolong Wang, and Michael Gharbi. Image neural field\ndiffusion models. In CVPR, pages 8007–8017, 2024. 3\n[9] Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe\nSong, and Zhanyu Ma. Demofusion: Democratising high-\nresolution image generation with no $$$. In CVPR, pages\n6159–6168, 2024. 1\n[10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In CVPR,\npages 12873–12883, 2021. 4\n[11] Stanislav Frolov, Tobias Hinz, Federico Raue, J¨orn Hees,\nand Andreas Dengel. Adversarial text-to-image synthesis: A\nreview. Neural Networks, 144:187–209, 2021. 1\n[12] Stanislav Frolov, Brian B Moser, and Andreas Dengel. Spotd-\niffusion: A fast approach for seamless panorama generation\nover time. arXiv preprint arXiv:2407.15507, 2024. 1, 3, 4\n[13] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Das-\nsarma, Dawn Drain, Nelson Elhage, et al. Predictability and\nsurprise in large generative models. In 2022 ACM Conference\non Fairness, Accountability, and Transparency, 2022. 1\n[14] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single\nimage super-resolution from transformed self-exemplars. In\nCVPR, pages 5197–5206, 2015. 5\n[15] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. arXiv preprint arXiv:2001.08361,\n2020. 1\n[16] Xiangtao Kong, Hengyuan Zhao, Yu Qiao, and Chao Dong.\nClasssr: A general framework to accelerate super-resolution\nnetworks by data characteristic. In CVPR, pages 12016–\n12025, 2021. 4\n[17] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero,\nAndrew Cunningham, Alejandro Acosta, Andrew Aitken,\nAlykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-\nrealistic single image super-resolution using a generative ad-\nversarial network. In CVPR, 2017. 5\n[18] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun\nFeng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff: Single\nimage super-resolution with diffusion probabilistic models.\nIn Neurocomputing, 2022. 1, 2, 5\n[19] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc\nVan Gool, and Radu Timofte. Swinir: Image restoration using\nswin transformer. In ICCV, pages 1833–1844, 2021. 2\n[20] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and\nKyoung Mu Lee. Enhanced deep residual networks for single\nimage super-resolution. In CVPRW, pages 136–144, 2017. 5\n[21] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei,\nBo Dai, Wanli Ouyang, Yu Qiao, and Chao Dong. Diffbir:\nTowards blind image restoration with generative diffusion\nprior. arXiv preprint arXiv:2308.15070, 2023. 1, 2\n[22] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.\nBlind image super-resolution: A survey and beyond. IEEE\nTPAMI, 2022. 1\n[23] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models. In CVPR,\n2022. 1\n[24] Andreas Lugmayr, Martin Danelljan, Luc Van Gool, and Radu\nTimofte. Srflow: Learning the super-resolution space with\nnormalizing flow. In ECCV, 2020. 5\n[25] Cheng Ma, Yongming Rao, Yean Cheng, Ce Chen, Jiwen\nLu, and Jie Zhou. Structure-preserving super resolution with\ngradient guidance. In CVPR, 2020. 5\n[26] David Martin, Charless Fowlkes, Doron Tal, and Jitendra\nMalik. A database of human segmented natural images and\nits application to evaluating segmentation algorithms and\nmeasuring ecological statistics. In ICCV, volume 2, pages\n416–423. IEEE, 2001. 5\n[27] Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto,\nToru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa.\nSketch-based manga retrieval using manga109 dataset. Multi-\nmedia tools and applications, 76:21811–21838, 2017. 5\n[28] Kangfu Mei, Zhengzhong Tu, Mauricio Delbracio, Hossein\nTalebi, Vishal M Patel, and Peyman Milanfar. Bigger is not\nalways better: Scaling properties of latent diffusion models.\narXiv preprint arXiv:2404.01367, 2024. 3\n[29] Brian B Moser, Stanislav Frolov, Federico Raue, Sebastian\nPalacio, and Andreas Dengel. Yoda: You only diffuse areas.\nan area-masked diffusion approach for image super-resolution.\narXiv preprint arXiv:2308.07977, 2023. 5\n[30] Brian B Moser, Stanislav Frolov, Federico Raue, Sebastian\nPalacio, and Andreas Dengel. Waving goodbye to low-res:\nA diffusion-wavelet approach for image super-resolution. In\n2024 International Joint Conference on Neural Networks\n(IJCNN), pages 1–8. IEEE, 2024. 1, 5\n[31] Brian B Moser, Federico Raue, Stanislav Frolov, Sebastian\nPalacio, J¨orn Hees, and Andreas Dengel. Hitchhiker’s guide\nto super-resolution: Introduction and recent advances. IEEE\nTPAMI, 45(8):9862–9882, 2023. 1, 5\n[32] Brian B Moser, Arundhati S Shanbhag, Federico Raue,\nStanislav Frolov, Sebastian Palacio, and Andreas Dengel.\nDiffusion models, image super-resolution, and everything: A\nsurvey. IEEE Transactions on Neural Networks and Learning\nSystems, 2024. 1, 5\n[33] Yunpeng Qu, Kun Yuan, Kai Zhao, Qizhi Xie, Jinhua Hao,\nMing Sun, and Chao Zhou. Xpsr: Cross-modal priors for\ndiffusion-based image super-resolution. arXiv:2403.05049,\n2024. 2\n[34] Fabio Quattrini, Vittorio Pippi, Silvia Cascianelli, and Rita\nCucchiara. Merging and splitting diffusion paths for semanti-\ncally coherent panoramas. arXiv preprint arXiv:2408.15660,\n2024. 1, 3\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\n9\n\ntransferable visual models from natural language supervision.\npages 8748–8763. PMLR, 2021. 2\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022. 1, 2,\n4, 5\n[37] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-\nmans, David J Fleet, and Mohammad Norouzi. Image super-\nresolution via iterative refinement. IEEE TPAMI, 2022. 1, 2,\n5, 7\n[38] Maria Schreiber.\nAudiences, aesthetics and affordances\nanalysing practices of visual communication on social media.\nDigital Culture & Society, 3(2):143–164, 2017. 1\n[39] Wenzhe Shi, Jose Caballero, Ferenc Husz´ar, Johannes Totz,\nAndrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan\nWang. Real-time single image and video super-resolution\nusing an efficient sub-pixel convolutional neural network. In\nCVPR, pages 1874–1883, 2016. 1\n[40] Jae Woong Soh, Gu Yong Park, Junho Jo, and Nam Ik Cho.\nNatural and realistic single image super-resolution with ex-\nplicit natural manifold discrimination. In CVPR, 2019. 5\n[41] Radu Timofte, Shuhang Gu, Jiqing Wu, and Luc Van Gool.\nNtire 2018 challenge on single image super-resolution: Meth-\nods and results. In CVPRW, 2018. 5\n[42] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK\nChan, and Chen Change Loy. Exploiting diffusion prior for\nreal-world image super-resolution. arXiv preprint, 2023. 1, 2\n[43] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy.\nRecovering realistic texture in image super-resolution by deep\nspatial feature transform. In CVPR, pages 606–615, 2018. 5\n[44] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,\nChao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En-\nhanced super-resolution generative adversarial networks. In\nECCV, 2018. 2, 5\n[45] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale,\nTrung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang.\nUncovering the disentanglement capability in text-to-image\ndiffusion models. In CVPR, pages 1900–1910, 2023. 1\n[46] Rongyuan Wu, Tao Yang, Lingchen Sun, Zhengqiang Zhang,\nShuai Li, and Lei Zhang. Seesr: Towards semantics-aware\nreal-world image super-resolution. arXiv:2311.16518, 2023.\n1, 2, 4, 5\n[47] Tao Yang, Rongyuan Wu, Peiran Ren, Xuansong Xie, and\nLei Zhang. Pixel-aware stable diffusion for realistic image\nsuper-resolution and personalized stylization. arXiv preprint\narXiv:2308.14469, 2023. 1, 2, 5\n[48] Roman Zeyde, Michael Elad, and Matan Protter. On single\nimage scale-up using sparse-representations. In International\nconference on curves and surfaces, pages 711–730. Springer,\n2010. 5\n[49] Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, Wei Niu,\nYushu Wu, Tianyun Zhang, Malith Jayaweera, David Kaeli,\nBin Ren, et al.\nAchieving on-mobile real-time super-\nresolution with neural architecture and pruning search. In\nICCV, pages 4821–4831, 2021. 1\n[50] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nICCV, pages 3836–3847, 2023. 1\n[51] Wenlong Zhang, Yihao Liu, Chao Dong, and Yu Qiao. Ranksr-\ngan: Generative adversarial networks with ranker for image\nsuper-resolution. In ICCV, 2019. 5\n[52] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,\nZhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian\nLi, Shilong Liu, et al. Recognize anything: A strong image\ntagging model. In CVPR, pages 1724–1732, 2024. 4\n[53] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change\nLoy. Towards robust blind face restoration with codebook\nlookup transformer. NeurIPS, 35, 2022. 2\n10",
    "pdf_filename": "Zoomed_In,_Diffused_Out_Towards_Local_Degradation-Aware_Multi-Diffusion_for_Extreme_Image_Super-Reso.pdf"
}