{
    "title": "Scideator Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination",
    "abstract": "",
    "body": "Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper\nFacet Recombination\nMARISSA RADENSKY, University of Washington, USA\nSIMRA SHAHID∗, Adobe, India\nRAYMOND FOK, University of Washington, USA\nPAO SIANGLIULUE, Allen Institute for AI, USA\nTOM HOPE† and DANIEL S. WELD†, Allen Institute for AI, USA\n† Equal Advisors\nThe scientific ideation process often involves blending salient aspects of existing papers to create new ideas. To see if large language\nmodels (LLMs) can assist this process, we contribute Scideator, a novel mixed-initiative tool for scientific ideation. Starting from\na user-provided set of papers, Scideator extracts key facets — purposes, mechanisms, and evaluations — from these and relevant\npapers, allowing users to explore the idea space by interactively recombining facets to synthesize inventive ideas. Scideator also\nhelps users to gauge idea novelty by searching the literature for potential overlaps and showing automated novelty assessments\nand explanations. To support these tasks, Scideator introduces four LLM-powered retrieval-augmented generation (RAG) modules:\nAnalogous Paper Facet Finder, Faceted Idea Generator, Idea Novelty Checker, and Idea Novelty Iterator. In a within-subjects user\nstudy, 19 computer-science researchers identified significantly more interesting ideas using Scideator compared to a strong baseline\ncombining a scientific search engine with LLM interaction.\n1\nINTRODUCTION\nScientists are continuously brainstorming research ideas on which to work next. A good idea should be relevant to the\nscientist’s interests and novel within the scientific community. Research papers are a major source of inspiration for\nrelevant and novel ideas, as they expose scientists to relevant concepts to re-combine and form new ideas [4, 21, 36].\nHowever, generating relevant and novel scientific ideas by recombining concepts from research papers is difficult\nfor multiple reasons. For one, scientists must wade through an ever-expanding scientific literature to find relevant\nconcepts [2, 19]. Moreover, the phenomenon of fixation biases scientists against considering more diverse concepts\nand concept recombinations for their research; instead, they are predisposed to thinking about a problem in familiar\nterms, which hinders the stimulation of novel ideas [11, 37]. Even if a scientist manages to identify interesting concept\nrecombinations to form potential research ideas, assessing the ideas’ novelty in comparison to the existing literature is\na cumbersome yet critical task.\nBuilding a fully or semi-automated ideation system has been an ambition of researchers for decades, and Scideatorbuilds\non strong prior work from many other researchers, filling a unique niche. We extend a line of work that presents\nsystems for finding analogies between research papers [4, 21, 36], adopting their facet-based framework but using\nmodern large language model (LLM) methods to identify relevant facets and perform facet recombinations. We are\nalso inspired by recent work showing that LLMs have promise to assist ideation in domains outside science, helping\npeople to generate more ideas [6] and more diverse ideas [27, 40]. While some of this LLM-based work employs facet\n∗Made large contribution.\nAuthors’ addresses: Marissa Radensky, radensky@cs.washington.edu, University of Washington, USA; Simra Shahid, Adobe, India; Raymond Fok,\nUniversity of Washington, USA; Pao Siangliulue, Allen Institute for AI, USA; Tom Hope†, tomh@allenai.org; Daniel S. Weld†, danw@allenai.org, Allen\nInstitute for AI, USA\n† Equal Advisors.\n1\narXiv:2409.14634v2  [cs.HC]  18 Nov 2024\n\n2\nRadensky et al.\nFig. 1. The Scideator workflow. 1) The interaction starts with the user providing a set of input papers that they would like to use as a\nstarting point for ideation. 2) The tool responds by retrieving analogous papers to the input papers and extracting facets from the\ninput and analogous papers. These facets are purpose, mechanism, and evaluation. (We exclude evaluation facets above for clarity, as\nthe main logic of the tool is based on the other facets.) 3) The user then selects paper facets as well as adds their own facets for which\nthey want to generate ideas. 4) The tool recombines these selected facets into ideas with one purpose and one mechanism. If the user\ndid not select any facets of a certain type, the tool selects one for them. 5) The user selects an idea to assess for novelty. 6) The tool\nclassifies the idea as \"novel\" or \"not novel\" and provides a short reason as to why. 7) The user reviews the novelty classification and\nadjusts it if they disagree. 8) If the idea is determined to be \"not novel,\" the tool provides suggestions for more novel ideas that replace\none of the initial idea’s facets.\nrecombination [6, 43], it fails to account for scientific ideation’s important requirements– that ideas be grounded in the\nliterature and novel relative to prior work. Yet other work applies LLMs to scientific ideation without reasoning about\nanalogous facets [28]; one problem noted by their participants was the generation of potentially unoriginal ideas. In\ncontrast, our approach to ideation provides two mechanisms to increase the originality of candidate scientific ideas: 1)\nfacet recombination and 2) explicit novelty assessment.\nIn this work, we present Scideator, an LLM-powered tool for scientific ideation that extracts facets (purpose,\nmechanism, and evaluation) from existing papers and helps recombine them in novel research ideas, carefully checking\nthe literature to avoid overlap with prior work. As in prior work, the purpose facet describes the problem being addressed\nby the paper, while the mechanism facet describes the paper’s proposed solution to the problem [4, 21, 36]. We also\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n3\ninclude an evaluation facet, which describes the method to determine if the mechanism successfully addressed the\npurpose. For a given input paper, an analogous paper may utilize a similar purpose but distant mechanism, or a similar\nmechanism but distant purpose. Innovation may also come from using a different type of evaluation.\nScideator takes research papers as input and identifies analogous papers. It then extracts the key facets from the\ninput and analogous papers. Next, the scientist works with the tool to select facets to combine in potentially novel\nresearch ideas. Scideator then scours the scholarly literature to assess the novelty of the proposed ideas and provides\nsuggestions for how to improve ideas deemed not novel. To build Scideator’s novelty checker, we make several\ncontributions. We annotate a small set of previously generated ideas with novelty labels and short “reviews” explaining\nthe novelty decision, and we use these reviews as in-context examples for an LLM. We retrieve papers similar to each\ngenerated idea, and the LLM then reasons about the novelty of the idea vis-a-vis retrieved related work, providing a\ncrisp explanation to the user.\nWe investigate how Scideator impacts scientific ideation through a within-subjects user study with 19 computer-\nscience researchers. Participants generated research ideas twice, once with Scideator and once with a strong baseline\ncombining a scientific search engine with LLM interaction. Our results show that participants found more ideas that\nthey thought were novel and interesting using Scideator; after removing an outlier, this improvement is statistically\nsignificant (Section 5.1.1).\nFurthermore, for the same collection of papers relevant to an idea, our idea novelty checker module improves\nagreement with human novelty ratings by more than 10x in comparison to a recent approach for LLM-based novelty\nratings [30], and leads to approximately 13% higher agreement than another recent approach [41]. We also conduct\nanalysis on different design choices around training LLMs to assess novelty. For example, we show that our approach\nof manually annotating generated ideas with novelty labels and concise reviews leads to better outcomes than other\napproaches that involve only literature retrieval, use peer-review data from OpenReview [35], or approaches that use\nLLM “prompt optimizers” [23, 50]. In Appendix C, we demonstrate the different ways LLMs interpret novelty, and how\nthe subtle changes in prompt design can significantly impact its performance.\nThrough interaction logs, we observe interesting ideation patterns from participants using Scideator. For one, we\nfind that participants spend twice as much time interacting with saved ideas and associated facets as they do unsaved\nideas and associated facets. In addition, junior researchers interacted longer with ideas (and associated facets) that they\nrated as exciting, whereas senior researchers interacted more with unfamiliar ideas. Although all participants were\nmore reluctant to utilize facets that were categorized as distant from their input paper, junior researchers were more\nwilling to save ideas that included more distant facets.\nIn summary, we make the following contributions:\n• Scideator, a novel LLM-powered tool for scientific ideation that\n– identifies and recombines analogous research-paper facets into potentially novel research ideas and\n– uses in-context learning with examples of ideas annotated for novelty to automatically generate a novelty\nevaluation of candidate ideas, allowing users to iteratively improve idea novelty when needed.\n• A within-subjects user study (N=19) demonstrating that Scideator, compared to a baseline of access to the\nsame LLM and a scientific literature search engine, leads to significantly more ideas that participants find novel\nand interesting (Section 5.1.1 and Figure 9).\n• An analysis of common patterns of ideation while using Scideator shows that participants interacted with\nsaved ideas and associated facets two times more than unsaved ideas (Section 5.4 and Figure 18). In addition,\n\n4\nRadensky et al.\njuniors had more interactions with ideas they were excited about and had thought of before the study, while\nseniors engaged more with unfamiliar ideas and showed less variation in the number of interactions for different\nlevels of excitement for an idea (Figure 19). Juniors were also more willing to save ideas with facets more distant\nfrom their input paper, though all participants were more reluctant to use distant facets (Section 5.4.1 and Figure\n17).\n2\nRELATED WORK\n2.1\nDivergent versus Convergent Thinking\nIn ideation, there are two main stages of thinking: divergent and convergent [8, 39]. While engaging in divergent\nthinking, the ideator is not worried about generating the most high-quality ideas. Instead, they aim to produce as many\nideas as possible in an effort to leave no stone unturned in considering potential solutions to their problem. At this stage\nof the ideation process, avoiding fixation on familiar concepts is important [11, 37]. Otherwise, the ideator may miss\nstrong candidate ideas simply because they utilize more distant concepts. In contrast, while engaging in convergent\nthinking, the ideator concentrates on narrowing down their ideas and determining which ideas to pursue. In this work,\nwe focus on divergent thinking. In scientific ideation, this equates to gathering inspiration from many sources and\ncoming up with several potential research ideas. Our tool Scideator also supports evaluating ideas for novelty. While\nidea evaluation is generally aligned with convergent thinking, we utilize this evaluation to help users consider even\nmore ideas when there are not enough existing novel candidate ideas.\n2.2\nConcept Combination and Analogy\nConcept combination and analogy are key methods for creating ideas [15, 22, 46]. Often, concept combination refers to\nthe fusing of two concepts into a new emergent concept. In the rest of this paper, we use the phrase “concept combination”\nmore broadly to refer to the use of multiple concepts in creating a new idea. Related work has investigated how concept\ncombination may be used in LLM-powered tools for ideation. Through CreativeConnect, users can recombine keywords\nto generate a graphic sketch [6], and through Luminate, users can recombine values of various dimensions to generate\ndiverse LLM responses [43]. Prior works have explored combining facets from an input artifact and analogous artifact\nin order to produce a new idea [5, 42]. We build upon these works by developing and evaluating a tool that supports\nexploration of a scientific idea space grounded in the literature.\nOf particular note to scientific ideation is a line of work that describes ideas in terms of two facets: the purpose (i.e.,\nthe problem) and the mechanism (i.e., the proposed solution to the problem). Hope et al. found that this faceted idea\nframework helps identify useful analogies for ideation [16]. If two ideas have similar purposes, then the mechanism of\none idea may apply well to the purpose of the other idea. Similarly, if two ideas have similar mechanisms, then the\npurpose of one may combine well with the mechanism of the other. Subsequently, the framework has been shown to\nfacilitate the creation of analogies between product ideas [16, 18], biological and design ideas [20], research papers [4, 21],\nand research-paper authors [36]. Utilizing this faceted framework, our mixed-initiative tool not only presents research-\npaper facets to recombine but also supports rapid exploration of the design space of potential facet recombinations and\nassociated ideas using the power of LLMs.\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n5\n2.3\nHuman-AI Scientific Ideation\nSeveral prior works have looked into automating scientific ideation [1, 25, 48], but automatic methods are currently\ninsufficient for formulating novel, impactful research ideas [17, 48]. In response, many works have studied the benefits\nof human-AI collaboration on scientific ideation [13, 49]. A number of studies have demonstrated that scientists and AI\nare able to work together to identify inspirational analogies between two scientific papers or paper authors [4, 21, 36].\nWith the rapid advancement of LLMs, recent work has started introducing LLM-powered scientific ideation tools. The\ntool SeeChat X Ideas takes a problem and scientific areas as input and returns a detailed research idea with citations\nand an associated literature review [14]. Liu et al.’s CoQuest helps scientists generate potential research questions in\na depth-first or breadth-first manner, with open-ended textual feedback to the system and a paper graph visualizer\nthat surfaces papers relevant to a generated research question [28]. In an effort to help scientists generate more novel\nand relevant ideas, we build on this prior work with a tool that proposes ideas grounded directly in recombinations of\nrelevant paper facets and supports users in evaluating and improving an idea’s novelty.\n2.3.1\nResearch Idea Novelty Evaluation. Dean et al. determined novelty, relevance, feasibility, and specificity as the\nmost prominent metrics to constitute a “good” idea [10]. We focus on the aspect of novelty that they referred to as\noriginality, which is defined as \"the degree to which the idea is not only rare but is also ingenious, imaginative, or\nsurprising.\" There has been an increase in work on automatic evaluation of research idea novelty [29, 30, 48]. Meanwhile,\nNigam et al. introduced Acceleron, a mixed-initiative, LLM-powered tool that uses an agent-based architecture with\ndistinct personas to assess the novelty of a research proposal relative to similar papers [33, 34]. However, Acceleron’s\nevaluation only involved three researchers. Related work has also emerged regarding automatic paper reviews, which\noften involve assessing the paper’s novelty [9, 26]. Sun et al. presented an LLM-powered tool to support novice peer\nreviewers, which included in-situ knowledge support for novelty evaluation [44]. We build on these works with an\nevaluation of the mixed-initiative use of Scideator’s idea novelty checker.\n3\nSYSTEM\n3.1\nDesign Goals\nWe developed Scideator with two design goals in mind.\n• DG1: Help scientists to generate potential research ideas that are relevant to them.\n• DG2: Help scientists to generate potential research ideas that are novel.\nScientists want to work on research ideas relevant to their interests. To address DG1, our system retrieves facets\nrelevant to the user’s input papers that can be mixed and matched to the user’s liking in order to form research ideas.\nThe user can also add their own facets. In addition, to enhance the expressiveness of the faceted idea framework\nconsisting of purpose and mechanism facets, we introduce the facet of evaluation, or the method to determine whether\nor not the proposed solution solves the problem.\nScientists want to work on research ideas that are novel in order to make meaningful contributions to the scientific\ncommunity. To address DG2, we include features to address functional fixedness and fixation [11, 37]. Prior work\nillustrates how humans are predisposed to thinking about a problem in only one manner – the first manner to which\nthey were exposed. When attempting to brainstorm solutions for a problem, humans are impeded by this predisposition.\nScideator generates facets of varying distance from its input papers in order to encourage exploration of facets outside\nthe user’s filter bubble, and the user can view the scope of facets utilized thus far to encourage more diverse exploration.\n\n6\nRadensky et al.\n(a)\n(b)\nFig. 2. Scideator’s main page. a) Users can select, add, and generate more facets to recombine. b) Users can explore their generated\nideas, manually add their own, and evaluate an idea for novelty by clicking its search icon.\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n7\nThe system also creates analogies that map the purpose and mechanism of one research paper’s idea to the purpose\nand mechanism of another paper’s idea; these analogies are then used to suggest relevant yet novel combinations of a\npurpose from one paper and a mechanism from another paper. An important challenge in finding novel ideas is to\ndetermine whether a given idea is novel compared to a vast pool of existing literature. Our tool includes features for 1)\nevaluating an idea for novelty relative to prior work and 2) finding more novel but still relevant ideas.\n3.2\nWorkflow and Implementation\nFigure 1 provides an overview of the Scideator workflow. The frontend of Scideator was developed using React and\nTypeScript, and the backend with Python.\n3.2.1\nPapers –> Facets. FRONTEND. When a user first opens Scideator (Figure 2), they see a start page where they\nenter a set of paper IDs that they would like to use as a starting point for an ideation session. The tool then presents\npaper facets (Figure 2a) and initial ideas (Figure 2b). The facets are the purpose, mechanism, and evaluation extracted\nfrom each of the input papers as well as retrieved analogous papers. The purpose facet describes the problem being\naddressed by the paper, the mechanism facet describes the paper’s proposed solution to the problem, and the evaluation\nfacet describes the paper’s method to determine if the proposed solution actually solves the problem. The user can\nperuse the facets, which are organized in three columns, one for each facet type. To understand why some facets seem\nmore distant from their input than others, the user can also see if a facet is from their own input (e.g., input paper,\nmanually-added facet), a near analogous paper, or a far analogous paper. The initial facets are ordered by distance, from\ninput to far. If the user is unsure of the meaning of a particular facet, they can hover over the facet’s associated question\nmark to to see its description. They can also delete a facet in which they are not interested if the facet is not currently\nbeing used by an idea and there are at least two facets left in its column. By clicking a facet’s text, the user can filter for\nrelevant ideas below. If the user wants to see which facets in a particular column are currently used in an idea below,\nthey can click the \"Used\" filter button above the column.\nIf the user wants to add their own facet, they can do so by typing it in the text box under the relevant facet column\nand clicking \"Add.\" Furthermore, the user can use the \"generate more facets\" button in two manners. If they want more\nfacets related to a specific query, they can provide the query before clicking the button. If they leave the query blank,\nthey will receive more analogous facets of the tool’s choosing. When facets are added, they are appended to the bottom\nof their respective columns.\nBACKEND. Scideator employs our Analogous Paper Facet Finder module at this step (Figure 3).1 First, in-\nformation for each input paper is retrieved from the Semantic Scholar Academic Graph API [24] using its Semantic\nScholar paper corpus ID (input by the user) and the API call for details about a paper.2 We also use the Semantic Scholar\nRecommendations API call for recommended papers for a single positive paper to retrieve four relevant papers to the\ninput paper– the top-ranked two papers from the \"all-cs\" corpus and the same from the \"recent\" corpus.3 We use both\ncorpora to ensure a balance of generally relevant and newly relevant papers. We refer to these retrieved papers as being\nnear-1 to the input paper. We instruct the LLM gpt-4o-2024-05-13, set to a temperature of zero, to determine the key\nfacets (purpose, mechanism, and evaluation) of the input and near-1 papers based on their titles and abstracts. (This\nLLM and temperature are always the ones in use unless noted otherwise, and whenever we mention using a paper, we\nare referring to its title and abstract.) The LLM is prompted to write the facets as short phrases of no more than seven\n1The prompts to the LLM for the Analogous Facet Finder module may be found in Appendix E.\n2https://api.semanticscholar.org/api-docs/graph#tag/Paper-Data/operation/get_graph_get_paper\n3https://api.semanticscholar.org/api-docs/recommendations#tag/Paper-Recommendations/operation/get_papers_for_paper\n\n8\nRadensky et al.\nFig. 3. The Analogous Paper Facet Finder module. For one input paper, Scideator uses Semantic Scholar’s API to retrieve similar\npapers that will act as analogous papers (near-1). It uses the input and near-1 papers to create a summary of relevant works to the\ninput paper. Next, the tool extracts key facets from the input paper. The LLM then uses the input paper’s purpose and mechanism\nfacets to determine three queries for papers with an analogous purpose and mechanism. The queries are for analogous papers with\nvarying distances from the input paper: same topic (near-2), same subarea (far-1), and different subarea (far-2). Those queries are fed\nto the Semantic Scholar API to retrieve analogous papers. Finally, the facets of all the analogous papers are extracted by the LLM.\nwords. Scideator uses the input and near-1 papers to create a summary of relevant works. This summarization step is\ninspired by CoQuest’s “search and summarize” action for coming up with research questions [28].\nNext, for each input paper, we ask the LLM to come up with three analogies between the purpose and mechanism of\nthe input paper and another purpose and mechanism. More specifically, we request one analogy that uses a purpose\nand mechanism from the same topic within computer science research (near-2), one analogy that uses a purpose and\nmechanism from the same subarea (far-1), and one that uses a purpose and mechanism from a different subarea (far-2).\nWe also prompt the LLM to provide a query to find papers relevant to each analogous purpose and mechanism set.\nWe then use the Semantic Scholar API call for paper relevance search4 to retrieve the top four relevant papers to each\nquery, two from the \"all-cs\" corpus and two from the \"recent\" corpus. For each retrieved paper, the LLM is once again\nemployed to determine its key facets.5\nTo generate more facets without a user-provided query, the tool retrieves 16 more papers per input paper, four from\neach distance category, in the same manner as at the start of the tool’s use. Thus, four more analogous paper sets are\ncreated. The near-2, far-1, and far-2 papers come from new analogies generated by the LLM. To generate more facets\nwith a user-provided query, the tool retrieves four papers relevant to the query in the same manner that it retrieves\npapers relevant to an analogous-paper query. In this case, one more analogous paper set is created. In the same manner\nas before, the LLM is prompted to determine the key facets from each of the gathered papers. Also, if the user manually\nadds their own idea, the LLM is prompted to determine its facets based on the idea text.\n3.2.2\nFacets –> Ideas. FRONTEND. Once the user has gotten a sense of the facets available, they can select facets with\nwhich they want to generate ideas and then click the \"Generate New Ideas\" button (Figure 2a). The user can select as\nmany facets as they want in each column. For a given facet column, if the user does not select any facets, the tool will\n4https://api.semanticscholar.org/api-docs/graph#tag/Paper-Data/operation/get_graph_paper_relevance_search\n5To confirm reasonable consistency in facet distance, the first and second author annotated previously unseen purposes and mechanisms generated by\nthe model for three papers. Both annotators classified the majority of near purposes, near mechanisms, far purposes, and far mechanisms the same as the\nmodel. One annotator demonstrated substantial agreement (Cohen’s Kappa = 0.61) and the other demonstrated moderate agreement (Cohen’s Kappa =\n0.53) with the tool’s classifications.\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n9\nFig. 4. The Faceted Idea Generator module implementing the initial method for idea generation (see Table 1 for a description of the\nvariations on this method). For a given input paper, the module takes multiple inputs from the Analogous Paper Facet Finder module:\nthe input paper and its facets, four analogous paper sets and their facets, and the relevant works’ summary. For each analogous\npaper set, the LLM generates two analogies and corresponding research ideas: one idea combines an input paper purpose and an\nanalogous paper mechanism, and one idea does the opposite. The relevant works’ summary provides the LLM context for what has\nalready been covered in prior research.\nselect facets for them when generating ideas based on the logic described in the backend section below. Otherwise, each\ngenerated idea will use one of the selected facets. Also, for the first two facets selected in the purpose and mechanism\ncolumns, the tool stars two facet suggestions in the other columns without selected facets.\nAs the user explores their generated ideas (Figure 2b), they can save them with the bookmark button or delete them\nwith the trash button. They can get information on which facets were used in an idea in two manners. For one, they\ncan check the color-coded highlighting of the facets within the idea. For additional information on the facets used, they\ncan click the idea box and the facets above will be filtered to show only the three used in the idea. If the user wants to\nadd their own idea, they can do so in the bottom-most idea box, and the tool determines the idea’s facets for them.\nBACKEND. Scideator employs our Faceted Idea Generator module at this step (Figure 4).6 The module is first\nused to create eight ideas per input paper using our Initial idea generation method. Per input paper, this module\ntakes multiple inputs from the Analogous Paper Facet Finder module: the input paper and its facets, four sets of\nanalogous papers and their facets, and the relevant works’ summary. The LLM is instructed to differentiate its ideas\nfrom the existing work described in the relevant works’ summary. Also, it is prompted to generate two analogies and\ncorresponding ideas per analogous paper set. The two analogies are between the input paper’s purpose and mechanism\nand an analogous paper’s purpose and mechanism. The LLM converts one analogy into an idea combining the input\npaper’s purpose and the analogous paper’s mechanism. It converts the other analogy into an idea combining the\ninput paper’s mechanism and the analogous paper’s purpose. Each idea’s evaluation facet, selected by the LLM, is the\nevaluation facet either from the purpose’s associated paper or the mechanism’s associated paper. This whole process is\nrepeated for each of the four paper sets analogous to the given input paper. These four sets are for near-1, near-2, far-1,\nand far-2 distance. For each generated idea, we also prompt the LLM to highlight the spans related to each facet in the\ncorresponding color (green for purpose, yellow for mechanism, and purple for evaluation).\n6The prompts to the LLM for the Faceted Idea Generator module may be found in Appendix F.\n\n10\nRadensky et al.\nMethod\nSelected facet(s)\nInput\nOutput ideas\n<purpose, mechanism, evaluation>\nInitial\nNone\n• Relevant papers’ LLM summary\n• Input papers with facets\n• Analogous papers with facets\n• <input, analogous, either>\n• <analogous, input, either>\nNo-P-no-M\nNo purpose\nNo mechanism\nOptional evaluation\n• [Same inputs as the Initial method]\n• Selected evaluation facet(s)\n• <input, analogous, *>\n• <analogous, input, *>\nP-or-M\nOnly purpose(s)\nor mechanism(s)\nOptional evaluation\n• [Same inputs as the Initial method]\n• Selected purpose or\nmechanism facet(s)\n• Selected evaluation facet(s)\nSelected P/M facet’s distance <= near-1\n• <selected, far-analogous ,*> or\n<far-analogous, selected, *>\nSelected P/M facet’s distance > near -1\n• <selected, near-analogous ,*> or\n<near-analogous, selected, *>\nP-and-M\nBoth purpose(s)\nand mechanism(s)\nOptional evaluation\n• [Same inputs as the Initial method]\n• Selected purpose facet(s)\n• Selected mechanism facet(s)\n• Selected evaluation facet(s)\n• <selected, selected, *>\nTable 1. Faceted idea generation methods. * = one of the selected evaluation facets or an evaluation facet associated with a selected\nfacet. far-analogous = facet with distance greater than near-1. near-analogous = facet with distance less than or equal to near-1.\nTo generate more ideas, the tool first determines if the user selected any purposes or mechanisms (Table 1). If the\nuser did not select any purposes or mechanisms (No-P-no-M), the tool repeats the Initial method, choosing a random\nfour of its analogous paper sets to generate ideas. The only difference from generating the initial ideas is that, if at\nleast one evaluation is selected, the tool is prompted to include one of the selected evaluations in each idea. If the\nuser selected at least one purpose but no mechanism or vice versa (P-or-M), the tool determines if the first selected\npurpose or mechanism that it sees is an input, near-1, near-2, far-1, or far-2 facet. If it is an input or near-1 facet, the\ntool proceeds with the No-P-no-M method except that the ideas combine any selected purpose or mechanism with\na mechanism or purpose of a distance greater than near-1. If the first selected purpose or mechanism that the tool\nsees is more distant than near-1, the tool proceeds with the No-P-no-M method except that the ideas combine any\nselected purpose or mechanism with a mechanism or purpose of a distance less than or equal to near-1. We note that, for\nNo-P-no-M and P-or-M, tool-generated facets (i.e., facets not added manually by the user) are prioritized to be included\nin ideas because they have paper context to provide to the LLM. If at least one purpose and at least one mechanism is\nselected (P-and-M), the tool generates ideas that combine one of the selected purposes, one of the selected mechanisms,\nand one of the selected evaluations (or one of the evaluations associated with a selected purpose or mechanism). For\nthis P-and-M method, the tool generates a different number of ideas depending upon how many facets are selected. If\nmore than two facets are selected from any facet column, then it generates eight ideas, but if this threshold is not met,\nthen the tool generates only two ideas. This is to avoid the nontrivial amount of redundancy that would occur across\neight ideas with only a couple options for each facet type.\nThe suggestions for facets with which to combine a selected facet are generated using a similar process to the P-or-M\nidea generation method. The main difference is that, instead of generating facet-recombination ideas, the tool stops\nat generating the facet recombinations themselves. In this module, the LLM’s temperature is set to 0.75 to make the\nresponses more varied.\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n11\n(a) The novelty checker when an idea is classified as “novel”.\n(b) The novelty checker when an idea is re-classified as “not novel” by the user and new idea suggestions are requested.\nFig. 5. Scideator’s novelty checker.\n\n12\nRadensky et al.\n3.2.3\nIdea –> Novelty Check. FRONTEND. The user can evaluate an idea for novelty by clicking the search button to\nthe left of the idea. This will lead to a modal where the user can review the idea, its associated facets, and 10 related\npapers to the idea (Figure 5a). Below the related papers, the user can read the tool’s novelty evaluation of the idea,\nwhich consists of a classification of \"novel\" or \"not novel\" along with a short reason referencing the related papers. The\nuser can adjust both the rating and reason if they disagree.\nSearch API\nInput Idea\nPapers Related to Input Paper \nfrom Semantic Scholar\nAnalogous Papers \nIdea-Keyword \nPapers\nFrom Previous Steps  (Sections 3.3.1, 3.3.2)\nCandidate Papers\nSTEP 2: Filtering for Most Relevant Papers\nSTEP 1: Identifying a Broad \nCollection of Papers Relevant to Idea\nSTEP 3: Evaluating Novelty of the Idea\nCompare Specter \nEmbeddings \nRankGPT  \nRe-Ranking\nTop N Papers \nSimilar to Idea\nMost Relevant Papers\nNovelty Classification and Reasoning\nInput Idea\nMost Relevant Papers\nKeywords\nExtract \nkeywords \nfrom Idea \nInput Idea\nExpert Labeled Examples \n..\nClassification: Not Novel\nReasoning: The idea is not novel because it closely resembles existing tools like 'Bursting Scientific \nFilter Bubbles'[4], Bridger[9] and ResearchRabbit[3], which also use citation-based suggestions \nand visualizations. The proposed tool's combination of citation networks and author profiles is already \nwell-covered by these systems.\n…\nMost Relevant \nPapers\nIdea\nClassification \nand Reasoning\n..\nInput Idea\nThis idea is not novel \nbecause…\nThis idea is novel \nbecause…\nThis idea is not novel \nbecause…\nFig. 6. The Idea Novelty Checker module follows a three-step process to evaluate idea novelty. First, it constructs a broad collection\nof papers relevant to the idea. Second, it filters this collection using embedding-based similarity, followed by an LLM-based re-ranker,\nRankGPT, to identify the most relevant papers. Finally, it incorporates expert-labeled novel and non-novel examples as in-context\nexamples to guide the LLM in evaluating the idea’s novelty.\nBACKEND. Scideator employs our Idea Novelty Checker module at this step (Figure 6).7 The module leverages\na customized retrieval-augmented LLM to assess an idea’s novelty by comparing it to a set of retrieved most relevant\npapers. In the context of scientific literature, novelty evaluation faces several difficult challenges. First, the space of\npossibly overlapping papers is vast, with hundreds of millions of possible matches. Second, determining the criteria for\njudging novelty is inherently challenging, since novelty can be subjective and defined in multiple ways.\nIn order to solve these challenges, the novelty checker follows three key steps. First, it collects a broad collection of\npapers relevant to the idea. Second, to solve the problem of scale, it leverages prior work using the Semantic Scholar\nAPI’s embedding-based paper similarity search [7] followed by an LLM-based re-ranker [45], narrowing down the\ncandidate space to the most relevant papers. Finally, to address the challenge of subjectivity and complexity of evaluating\nnovelty, it incorporates expert-labeled examples of novel and not novel ideas with reasoning grounded in the collection\nof most relevant papers. These expert-labeled examples of ‘idea’, ‘most relevant papers’, and ‘reasoning’, are used as\nin-context examples to guide the LLM to reason about the novelty of an idea from the most relevant papers. For a given\nidea, the reasoning generated by the novelty checker points to the related papers for similarities and differences, if any,\n7The prompts to the LLM for the Idea Novelty Checker module may be found in Appendix G.\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n13\nand makes the novelty evaluation process more transparent for users when using Scideator (see an example reasoning\nin Step 3 of Figure 6).\nStep 1: Identifying a Broad Collection of Papers Relevant to the Idea. To accurately assess the novelty of an\nidea, it is crucial to compare it against a comprehensive collection of papers that cover the various facets of the idea.\nSimple retrieval methods often overlook important aspects of an idea [31, 32, 47]. To address this, we use a two-step\napproach: first, we reuse input and analogous papers that helped generate the idea. Second, we employ a query-based\nretrieval method, where search queries are generated corresponding to different keywords related to the idea, and\nqueried through the Semantic Scholar Search API [24]. Corresponding to each search query, papers are added to the\ncollection of relevant papers. We prompt the LLM to generate these search queries based on the keywords related to\nthe idea (see Appendix for prompt G.2).\nStep 2: Filtering for Most Relevant Papers: To identify the papers most likely to overlap with the candidate idea,\nwe implement a two-stage re-ranking process that combines embedding-based filtering with an LLM-based re-ranking\napproach. Our re-ranking approach in this part of the system follows standard practices in information retrieval\npipelines that use powerful and costly LLMs as re-rankers [12, 45].\nFirst, we employ SPECTER-2 embeddings [7] to compute the semantic similarity between the idea and each paper in\nour collection of papers from Step 1. We select the top N (N=100) papers with the highest cosine similarity between\ntheir embeddings and the idea embedding. While this embedding-based ranking efficiently narrows down the collection\nof papers, it is limited in its capacity to capture deeper and more contextual relationships between different facets of\nthe idea and the papers, in comparison to powerful state-of-art LLMs [38].\nTo address these limitations, we introduce RankGPT [45], an LLM-based re-ranker, in the second stage to refine the\nranking of these top N papers based on their relevance to the idea. RankGPT goes beyond surface-level similarities,\nusing LLMs to re-rank papers by comparing similarites and differences to the idea. In our experiments with 118 ideas,\nadding the RankGPT step adjusted paper rankings for nearly 80% of the ideas, frequently reshuffling the importance\nof papers compared to the embedding-based method, SPECTER-2. Notably, in 7 cases, entirely new papers that were\npreviously ranked much lower, were introduced in the top 10 most relevant papers. This adjustment in rank becomes\neven more significant when we specifically re-rank papers based on the idea’s \"purpose\" facet. We do this by changing\nthe relevance criterion in the RankGPT prompt, from general relevance to focusing on the main purpose of the idea and\npapers. Purpose-based re-ranking introduced new papers that had been previously overlooked by the embedding-based\nsimilarity method in over half of the ideas (60 cases out of 118 ideas), further highlighting the importance of this second\nstage of re-ranking.\nBy combining the top-k (k=10) papers from both general relevance and purpose-based relevance rankings, we obtain\na balanced set of papers that are aligned both semantically with the idea and with its main purpose. This collection of\nmost relevant papers is used by the novelty checker in the next step to evaluate the idea’s novelty.\nStep 3: Evaluating Novelty of the Idea. In the final step, Scideator leverages LLMs to assess the novelty of the\nidea by comparing it to the collection of most relevant papers identified in Step 2. However, determining the novelty\nof an idea in relation to existing scientific literature is a complex and subjective task. To address this challenge we\nconducted an expert annotation study to collect examples of novel and not novel ideas with reasoning grounded in\nthe most relevant papers for each idea. These labeled ideas were then used as in-context examples to guide the LLM,\nhelping it to reason about novelty in the same way human experts do. In the following sections, we outline the setup\n\n14\nRadensky et al.\nand findings of our study, including experiments that compare various prompts for assessing the novelty of research\nideas. We also examine and compare the prompts used in recently proposed LLM-based idea generation methods.\nExpert-Labeled Examples and Novelty Evaluation. Assessing the novelty of an idea is inherently subjective and\noften challenging. To better understand how novelty can be evaluated, we conducted a two-part annotation study\ninvolving the first and second authors of the paper, who reviewed the novelty of ideas based on the most relevant\npapers from (Step 1 and 2).\nIn the first study, experts evaluated 51 ideas, comprising of 46 ideas generated from Scideator and 5 ideas adapted from\naccepted and rejected papers from OpenReview (ICLR 22, NeurIPS 23). The ideas were categorized as novel, moderately\nnovel, or not novel. The experts had high agreement on idea novelty (Cohen’s Kappa = 0.64). A key observation from\nthis study was that experts sometimes relied on their broader domain knowledge, rather than restricting their judgments\nto the most relevant papers.\nBuilding on this observation, we conducted a second study to minimize the influence of external knowledge. In this\nstudy, experts were instructed to base their idea novelty judgments solely on the provided papers, and we simplified the\ncategories to just two: novel and not novel. The experts evaluated 51 ideas: 34 new ones generated by Scideator and\n17 from the previous study for which experts had relied on their broader domain knowledge. By narrowing the focus to\nrelevant papers alone, we observed fewer disagreements8 and a higher agreement rate (Cohen’s Kappa =0.68).\nAs a result of these two studies, we created a dataset of 67 ideas for which the experts reached consensus—39\ncategorized as novel and 28 as not novel ideas, along with reasoning grounded in the most relevant papers Please refer\nto Section A of the Appendix for sample examples.\nIncorporating Expert Annotations to Guide LLM’s Novelty Evaluation. In addition to the challenges of evaluating\nnovelty, such as subjectivity and complexity, another key challenge is prompt sensitivity. Even minor changes in how\nprompts are phrased can lead to different outcomes and interpretations, making it important to optimize the prompt\ndesign. To address this, we used our expert-annotated dataset of 67 ideas, split into train and test sets (35/32), with\na balanced set of novel and not novel ideas. We then compared various strategies, including Zero-Shot, In-Context\nExamples (using both OpenReview and expert-labeled ideas from the training set), and different prompt optimization\nstrategies such as DSPy[23], TextGRAD[50], and Anthropic’s prompt generator 9.\nIn addition to our manually crafted zero-shot prompt, we utilized Anthropic’s prompt generator to further refine it.\nUsing these prompts, we experimented with two in-context example setups:\n(1) OpenReview Examples: We extracted reviews from ICLR and NeurIPS submissions using the OpenReview\nAPI[35], which contain strengths, presentation, limitations, soundness, weaknesses, questions, confidence, contribu-\ntion, summary, and rating. Since OpenReview abstract submissions contain detailed descriptions of experiments\nand methodologies that differ from our expert-labeled examples, we adapted the style of these submissions to\nmatch our training data. After rigorous filtering, we gathered 8,156 submissions where reviews included details\nabout the novelty of the paper. Since the reviews typically discuss the entire paper, not just the core idea, we\nmanually identified reviews that specifically evaluate the main idea. From these, we collected approximately 20\nexamples and randomly sampled idea and review pairs as in-context examples.\n8Of the 8 instances of disagreement, in 4 cases, either of the two experts missed details from the paper, and in 2 cases, the experts differed in their\nperception of subtle contributions to novelty. For the other 2 cases, no specific comments were provided.\n9https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n15\n(2) Expert-Labeled Examples: In the second setup, we used the expert-labeled data in the training set as in-context\nexamples. We experimented with different numbers of in-context examples of idea and related papers pairs\nalong with their novelty class and review.\nDSPy and TextGRAD are prompt optimization techniques that learn from training and testing examples and optimize\nthe prompt. For this, we used our train/test split from the expert-labeled dataset. Please refer to Section D of the\nAppendix for details about experimental setup for all these approaches.\nModels\nAccuracy\nPrecision\nRecall\nF1\nZero Shot Setting\nZero Shot\n0.68\n0.76\n0.64\n0.65\n+ improved prompt using Anthropic prompt generator\n0.68\n0.70\n0.64\n0.64\nPrompt Optimizers\nDSPy\n- with idea, most relevant papers, class\n0.68\n0.83\n0.62\n0.58\n- with idea, most relevant papers, class, reasoning\n0.66\n0.82\n0.58\n0.52\nTextGRAD\n- with idea, most relevant papers, class\n0.78\n0.76\n0.76\n0.76\nIn-context Setting\nOpen-Review Examples\n- with idea & review (i.e., reasoning)\n0.59\n0.55\n0.51\n0.43\nExpert Labeled Examples\n- with idea, reasoning\n0.75\n0.76\n0.77\n0.75\n- with idea, most relevant papers, class\n0.78\n0.77\n0.76\n0.77\n- with idea, most relevant papers, class, reasoning\n0.81\n0.84\n0.78\n0.79\nTable 2. Our Experimental Results using gpt-4o.\nResults: We observed that using our expert-annotated data as in-context examples significantly improved accuracy\ncompared to both the Zero-Shot settings, DSPY, TextGRAD and the OpenReview in-context example setup (Table 2).10\nAs the reviews in OpenReview do not contain references to papers, we tested our expert-labeled examples with and\nwithout relevant papers for fair comparison. Even when we excluded relevant papers from our expert-labeled examples,\nour approach still outperformed the setting with OpenReview in-context examples.\nWe also experimented with TextGrad and two setups for DSPY, with and without reasoning. Our approach with\nexpert labels consistently outperformed the prompts generated by these prompt optimizing approaches. TextGRAD\ncould not improve from its starting system prompt, but did offer some interesting insights into LLM’s prompt sensitivity.\nNotably, prompts that were concise and specifically discussed how to compare papers for evaluating novelty led to\nsignificantly higher accuracy. In contrast, prompts with extra details, such as publication dates or identifying the\nevolution of a research problem, resulted in decreased performance. These findings highlight that even subtle differences\nin prompt wording can dramatically impact performance, underscoring the importance of careful prompt design in\nLLM-based novelty evaluation. For a detailed analysis of these patterns and insights into optimizing prompts for idea\nnovelty evaluation, we encourage readers to explore Appendix C.\nComparison with Baselines: We also compared our best performing in-context setup with the AI Scientist’s [30]\nidea novelty reviewer (different from its paper reviewer) and AI Researcher’s [41] on the same test set of ideas (Table\n10We used the model “gpt-4o” during the months of August and September 2024.\n\n16\nRadensky et al.\n3). Since both setups require a different style of input idea, we adapted the ideas to match the requirements of each\nsystem (see details in Appendix D). We compare only the prompts to assess novelty of these two approaches with\nScideator, rather than the entire system, because the test set containing the novelty judgements were based on a\nfixed set of the 10 most relevant papers for each idea. Since different retrieval methods could introduce new papers, we\nstandardize the most relevant papers to ensure a fair comparison of the prompts alone.\nOur approach achieved over 10 times more agreement with expert-labeled examples compared to AI Scientist,\nand approximately 13% higher agreement than AI Researcher, further validating the effectiveness of our novelty\nchecker. It is important to note that AI Scientist defaults to \"not novel\" when it fails to reach a conclusion in novelty\nevaluation (18 out of 32 times), which may have impacted its agreement rates. More context about this can be found in\nAppendix D. We also present some qualitative examples in Section B of the Appendix, showcasing how these approaches\nevaluate the novelty of an idea.\nModels\nAccuracy\nPrecision\nRecall\nF1\nCohen Kappa\nAI Scientist [30]\n0.47\n0.55\n0.53\n0.44\n0.05\nAI Researcher [41]\n- GPT-4o\n0.78\n0.81\n0.74\n0.75\n0.52\n- Claude-3-5-sonnet\n0.56\n0.63\n0.61\n0.56\n0.19\nScideator (Ours)\n0.81\n0.84\n0.78\n0.79\n0.59\nTable 3. Comparison with Baselines. Given a common collection of most relevant papers for an idea, we compare the novelty\nevaluation prompts of Scideator , AI Scientist [30] and AI Researcher [41].\nBased on these findings, the novelty checker in Scideator incorporates expert-annotated examples that include\nthe idea, related papers, reasoning, and novelty class (see Appendix G). This Idea Novelty Checker module forms the\nfoundation for the next step, where Scideator enables users to view suggestions to improve an idea’s novelty.\n3.2.4\nNovelty Check –> Novel Idea Suggestions. FRONTEND. If the tool determines that an idea is not novel, or if the\nuser adjusts an idea’s classification to \"not novel\" and refreshes the \"New Idea Suggestions\" section (Figure 5b), the tool\npresents the user with three new idea suggestions. Each suggestion replaces a different facet in the original idea in an\neffort to make the idea more novel relative to the related papers. If the user likes one of these suggestions, they can add\nit to their idea list.\nBACKEND. Scideator employs its Idea Novelty Iterator module at this step (Figure 7).11 The module is used to\nmodify an idea in order to produce a more novel one. The module provides the LLM with an idea, reason for its “not\nnovel” classification, the related papers from which a more novel idea should differentiate, and all the facets available to\nadd to the idea. The LLM is instructed to come up with three more novel idea suggestions such that each suggestion\nreplaces a different facet in the initial idea with another facet of the same type (e.g., purpose). For increased variance in\nresponses, we use a temperature of 0.75 in this module.\n4\nMETHODS\n4.1\nResearch Questions\n• RQ1: Which tool leads to more saved ideas?\n11The prompts to the LLM for the Idea Novelty Iterator module may be found in Appendix H.\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n17\nFig. 7. The Idea Novelty Iterator module. If an idea is classified as “not novel” by the tool or user, the LLM takes as input from\nprevious modules the idea and its facets, the reason for the “not novel” classification, retrieved papers relevant to the idea, and facets\navailable to add to the idea. It uses these inputs to generate three idea suggestions, each replacing a different facet in the original\nidea, that are intended to be novel relative to the original idea’s related papers.\n• RQ2: Which tool leads to higher average confidence in tbe novelty of saved ideas?\n• RQ3: Which tool leads to higher average excitement for saved ideas?\n• RQ4: How do researchers utilize each tool for ideation?\n4.2\nParticipants\nNineteen computer-science researchers (W: 10, M: 8, NB: 1) were recruited through institutional mailing lists and\nacademic social networks to participate in the study.12 We compensated participants with a $50 Amazon gift card.\nThose who participated in a follow-up survey received an additional $10. The participants were from various fields of\ncomputer science, most commonly natural language processing (N=8) and human-computer interaction (N=5). They\nwere at various stages of their research career (undergraduate student: 2, master’s student: 1, engineer with master’s: 1,\nPhD student: 12, postdoc: 2, professor: 1). Generally, the participants interacted with LLMs often (at least once per...\nmonth: 2, week: 5, day: 12).\n4.3\nStudy Design\nThe study was within-subjects and had two conditions. In the treatment-first condition, participants interacted with\nScideator followed by the baseline tool. In the baseline-first condition, the reverse was true. During the treatment\nportion, participants were provided Scideator and their starting paper link. During the baseline portion (Figure 8),\nparticipants were provided their starting paper link, a link to the scientific search engine Semantic Scholar, and a\nbaseline tool.\nThe baseline tool provided the ability to prompt gpt-4o-2024-05-13, the same version of the LLM that we used for\nmost of Scideator’s functionality. We used OpenAI’s Assistants API13 to create a thread for each participant so that\nthe system would remember and contextualize their conversation. We gave the assistant the name \"Scientific Research\nIdeator\" and the instructions \"You are ScientistGPT, an intelligent assistant that helps researchers come up with coherent,\nnovel, and useful research ideas.\" We otherwise left it on its default settings, which includes a temperature of 1.0. We\nalso provided a button for participants to generate a more detailed version of their instructions. We included this\nfunctionality so that participants would have some support in prompt engineering. This button called Anthropic’s\n12We note that 21 participants were actually recruited, but the first two participants were dis-enrolled, due to unclear instructions which caused them to\nfail to understand a baseline feature. Instructions were subsequently clarified before the final 19 participants completed their study sessions.\n13https://platform.openai.com/docs/assistants/quickstart\n\n18\nRadensky et al.\nFig. 8. The baseline included quick access to a scientific search engine and this tool. This tool consists of a gpt-4o chat interface with\ncontextual memory, a button to generate a more detailed version of instructions to the AI, and a place to store ideas.\nprompt generator14 with the model claude-3-5-sonnet-20240620. The output prompt generally contained one to two\nvariable placeholders. We explained to the participants that these placeholder variables could either be filled in or\nremoved from the instructions as they saw fit. Participants could store ideas at the bottom of the tool by manually\ntyping or pasting text and clicking \"Add.\" They were then able to save, expand, or delete the idea in the same manner as\nin Scideator. The idea boxes would not hold more than 1000 characters, in line with our task instructions described in\nSection 4.4.\nWhen filling out their recruitment survey, each participant provided two topics of interest for which they wanted\nto do research ideation. In particular, for each topic, we asked them to provide the following as a starting point for\nideation: a broad purpose, broad mechanism, starting paper, and backup paper in case the Semantic Scholar API did not\nhave the title and abstract for the first one. We dictated that the papers could not be authored by them. We randomized\nwhich of the participant’s ideation topics was used first during the study.\n14https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n19\n4.4\nProcedure\nEach within-subjects study session was 90 minutes, and the sessions were recorded and transcribed using Google\nMeet.15 Participants interacted with Scideator and the baseline tool in randomized order. For each part of the study\n(treatment and baseline), the session coordinator provided the participant with a link to the assigned tool as well as a\nreminder of the broad purpose, broad mechanism, and paper that the participant wanted to use as a starting point for\nideation. The coordinator provided the link for the starting paper as well. In the baseline, the coordinator also supplied\na link to the scientific search engine Semantic Scholar. In the treatment, the participant entered their starting paper’s\nprovided corpus ID in order to load the tool with facets and ideas based on their starting paper. After both the tool and\npaper links were loaded, the coordinator gave the participant a tutorial describing the assigned tool’s features. Next,\nthe coordinator instructed the participant to spend 20 minutes coming up with as many research ideas as possible that\nthey thought were both novel and interesting to think about further, keeping in mind the chosen paper, broad purpose,\nand broad mechanism as a starting point. The coordinator further explained that every research idea that met these\ncriteria should be saved.\nThe instructions noted that we define a research idea as one or more sentences (no more than 1000 characters)\ndescribing a potential research project, and an idea is a statement rather than a question. The instructions also defined\nthe novelty of an idea as “the degree to which the idea is not only rare but is also ingenious, imaginative, or surprising,\"\nwhich is Dean et al.’s definition of idea originality. The coordinator alerted the participant when five minutes remained\nand reminded participants to save all ideas that they thought were novel and interesting to think about further. The\ncoordinator also explained that, if the participant could not generate three ideas that they found interesting and novel,\nthey should still save three ideas to rate at the end of the five minutes. Once 20 minutes had passed, the participant\nrated each idea with respect to how confident they were in its novelty and how excited they were to continue thinking\nabout it. Each rating was on a seven-point Likert-type scale, with 7 being most positive. When they were done rating\nthe ideas, the coordinator engaged them in a semi-structured interview for up to 15 minutes, as time permitted. The\ninterview covered the participant’s idea generation experience and process of deciding whether or not to save an idea.\nFor the treatment, if there was time, the interview also addressed the participant’s thoughts on Scideator’s facet\nbreakdown and novelty checker.\n5\nRESULTS\n5.1\nRQ1: Number of Saved Ideas\n5.1.1\nPrimary Analysis. To answer RQ1, we obtained the number of ideas that participants saved in the treatment\nversus baseline. There was an outlier, violating an assumption of our originally planned paired-samples t-test. Therefore,\nwe conducted a Wilcoxon signed-rank test over the participants’ paired results as well as a paired-samples t-test of the\nresults without the outlier. We find that participants identify more ideas that they think are novel and interesting to\nponder further (i.e., save more ideas) with Scideator (M=7.58, SD=3.66) than with the baseline tool (M=6.37, SD=4.04)\n(Figure 9). This difference is statistically significant when the outlier is removed (Paired Samples t-test, t(17)=-2.85,\np<.05). The difference is not statistically significant when the outlier is included (Wilcoxon Signed-Rank Test, V=126,\np=n.s.).16 As a reminder, the participants were instructed to save all ideas that they found novel and interesting, but if\nthey could not generate any ideas that were interesting, they were still required to save at least three ideas to rate.\n15The study script may be found in the supplementary materials.\n16The R code and data for statistical testing may be found in the supplementary materials.\n\n20\nRadensky et al.\nFig. 9. Participants save more ideas while using Scideator as opposed to the baseline (RQ1). This result is significant if the outlier\n(bottom of right-side plot) is removed.\n5.1.2\nExploratory Analysis: Learning Effect. We observed a strong and interesting learning effect in which the partici-\npants who experienced the treatment first went on to save over 96% more ideas in the baseline than their counterparts\nwho experienced the baseline first (treatment-first: M=8.30, SD=4.74; treatment-second: M=4.22, SD=1.30) (Figure 10).\nThus, Scideator may have helped the participants to get into an appropriate mindset for divergent ideation, in which\none tries to generate as many potential ideas as possible, as opposed to convergent ideation, in which one tries to\ndetermine the single best idea [8, 39]. Out of the 10 participants who saw the treatment first, eight generated multiple\nclear idea lists using the baseline tool. On the other hand, only 3 of 9 participants who experienced the baseline first\ngenerated multiple clear idea lists with the baseline tool. This provides some evidence, though limited, that participants\nwere in a better mindset to generate several ideas after experiencing the treatment. A related discussion comparing the\ntreatment and baseline ideation patterns may be found in Section 5.4.3.\n5.1.3\nExploratory Analysis: Juniors vs Seniors. We split participants into two groups based on seniority, which we\ndefine in terms of the participant’s number of first-author and last-author papers. Juniors (N=10) had <= 3 such papers,\nand seniors (N=9) had >= 5 such papers. We investigated the benefits of Scideator for each group and found that\njuniors (M=1.30, SD=3.65) had a greater increase in number of saved ideas than seniors (M=1.11, SD=3.18) when using\nScideator compared to the baseline (Figure 11). This may be due to more openness to diverse ideas, which is further\ndiscussed in Section 5.4.1 below.\n5.2\nRQ2: Confidence in Idea Novelty\n5.2.1\nPrimary Analysis. To answer RQ2, we calculated the average of each participant’s ratings for confidence in\nthe novelty of their saved ideas. We observed no significant difference between participants’ average confidence in\nthe novelty of their saved ideas in the treatment (M=5.32, SD=0.84) versus baseline (M=5.55, SD=0.68) (Wilcoxon\nSigned-Rank Test, V=112, p=n.s.). Thus, Scideator helped participants to generate more ideas worth saving without\nsignificantly decreasing participants’ average confidence in the novelty of their saved ideas.\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n21\nFig. 10. Participants saved more ideas in the baseline if they completed the treatment first, perhaps due to inspiration from Scideator’s\nfaceted ideation process.\nFig. 11. Junior participants had a greater increase in saved ideas from the treatment compared to the baseline.\n5.2.2\nExploratory Analysis: Novelty Checker. The novelty checker was used 116 times during the study, with participants\nusing it six times on average (SD=3). Only two participants, one junior and one senior, did not use the novelty checker\nat all.\nOn average, we observed that when participants used the novelty checker, they were more likely to save the idea\n(Figure 12 (a)) suggesting the module did not reduce their confidence in the idea’s novelty. Furthermore, seniors used\n\n22\nRadensky et al.\nthe novelty checker more frequently (M=6.4, SD=4.6) than juniors (M=5.8, SD=3) and saved a higher proportion of\nthese ideas (Figure 12 (b)).\n10\n8\n6\n4\n2\n0\n10\n8\n6\n4\n2\n0\n7.0\n6.5\n6.0\n5.5\n5.0\n4.5\n4.0\n3.5\n3.0\nWhen Novelty\nChecker is NotUsed\nWhen Novelty\nChecker is Used\nNumber of Times Novelty Checker is Used\nNumber of Times Novelty Checker is Used\nAverage Confidence in Novelty Rating\nInteractions with Novelty Checker\nInteractions with Novelty Checker by Seniority\nImpact of Novelty Checker on\nUser's Rating of Idea's Novelty\nSubmitted Ideas\nIdeas Not Submitted\nSubmitted Ideas\nIdeas Not Submitted\nJunior\nSenior\n(a)\n(b)\n(c)\nFig. 12. Influence of Idea Novelty Checker on saving an idea and confidence in an idea’s novelty.\nWe also measured the effect of the novelty checker module on participants’ confidence in the novelty of ideas. When\nthe module was used, participants on average were more confident in their judgement of an idea’s novelty compared to\nwhen it was not used (Figure 12 (c)). More specifically, when seniors did not use the novelty checker, their confidence\nin an idea’s novelty was on average slightly lower (M=4.83, SD=1.63) compared to when they used it (M=5.75, SD=1.09).\nHowever, junior participants did not show a significant difference in their confidence with or without the novelty\nchecker.\nIn only 2 out of 76 instances was the novelty checker used, the idea saved, and the confidence rating still low (2, 3). As\nshown on the right side of Figure 13, we also observed that few participants saved ideas with similarly low confidence\n(1, 2, 3) when the novelty checker was not used.\n5.2.3\nExploratory Analysis: Newness of Idea to Participant. While a participant may be confident in the novelty of a\nsaved idea relative to the literature, they may have thought of it before using the ideation tools in the study. Therefore,\nwe conducted a post-study survey on how new the participants’ saved ideas were to them. In particular, we asked\navailable participants to rate their agreement with the statement \"The idea above reminds me of an idea that I thought of\nbefore participating in the study.\" The rating was on a seven-point Likert-type scale with seven indicating the strongest\nagreement. With 13 participant responses, we observed a similar average lack of newness in saved ideas across the two\ntools (treatment: M=4.29, SD=0.75; baseline: M=4.43, SD=1.26). In both the treatment and baseline, participants thus\nseem to have saved ideas that were fairly new to them. While Scideator helped participants to produce more ideas\nthat they thought were novel and interesting to think about further, it did not diminish the average newness of saved\nideas to participants.\nWith respect to Scideator in particular, we found that participants generally felt more confident in an idea’s novelty\nwhen they had previously thought of the idea before (Figure 13). Participants were both familiar with the idea and\nconfident in its novelty in 50 cases, whereas they were unfamiliar yet confident in the idea’s novelty in 33 cases. We\nalso observed that participants frequently used the novelty checker for ideas with which they were already familiar\n(see the bottom left of Figure 13).\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n23\nFig. 13. Participants were more confident in an idea’s novelty when they had used the Idea Novelty Checker and were familiar with\nthe idea before using Scideator . Note: This figure includes data only from participants who completed the post-survey.\n5.3\nRQ3: Excitement about Idea\n5.3.1\nPrimary Analysis. To answer RQ3, we conducted the same analysis as for RQ2 but using participants’ ratings\nfor excitement to continue thinking about their saved ideas. We found that the treatment and baseline led to similar\naverage levels of excitement about saved ideas, which were fairly high (treatment: M=5.34, SD=0.67; baseline: M=5.50,\nSD=0.57) (Wilcoxon Signed-Rank Test, V=107, p=n.s.). Thus, Scideator helped participants to generate more ideas\nworth saving without significantly decreasing participants’ average excitement about their saved ideas.\n5.4\nRQ4: Ideation Patterns\nTo answer RQ4, we analyzed interaction logs of participants’ actions while using each tool.17 We also analyzed\nparticipants’ semi-structured interview responses using inductive thematic analysis [3]. When participants are quoted,\nthey are named in terms of their unique ID number, seniority group, and tool they were using at the time (e.g.,\nP1-senior-treatment).\n5.4.1\nTreatment Ideation: Types of Facets in Saved Ideas. In their interviews, participants by and large found the\nbreakdown of facets into purposes and mechanisms to be useful. Two participants described how the facet breakdown\nhelped them to control the ideas generated, filtering out facets in which they were not interested. P5-junior-treatment\nobserved, “I like the combination of purpose, mechanism, and evaluation. It kind of helped me filter out those facets\nthat are not relevant enough to kind of save me time.” On a related note, P9-junior-treatment explained how the facet\nbreakdown helped them to better understand from where the generated ideas came. They commented, “This tool gives\nus more interpretability because it is exactly telling you that, okay, it is choosing one facet from every column and trying\n17Consenting participants’ logs are provided in the supplementary materials.\n\n24\nRadensky et al.\nFig. 14. Participants more often opted to select their own facets rather than let the LLM choose them.\nFig. 15. Junior and senior researchers had more unique mechanisms than unique purposes in their saved ideas. Seniors had a slightly\nlower percentage of saved ideas with a unique purpose and unique mechanism.\nto generate an idea based on those input facets, instead of just randomly coming up with some idea, which we saw in the\nprevious tool.” A couple participants pointed out how the facet breakdown provides structure to the ideation process.\nP18-senior-treatment noted, “It helped a lot with organizing the ideas because it has this sort of structured way to build\nideas.” These comments on control and structure align with participants’ proclivity to select facets for saved ideas\nthemselves. Participants saved more ideas with purposes and mechanisms selected by themselves rather than the LLM\n(Figure 14). The average proportion of saved ideas with purposes selected by the participant was 0.76 (SD=0.24), and\nthe average proportion of saved ideas with mechanisms selected by the participant was 0.62 (SD=0.35).\nParticipants tended not to find as much use for the evaluation facets in terms of ideation. Some participants decided\non set evaluation methods, while others thought the evaluation should not be considered until later on in the ideation\nprocess. As an example, P4-junior-treatment related, “Evaluation is something which is not very useful... because if you\nhave a purpose and a mechanism, you can find ways to evaluate... Evaluation cannot make ideas novel.” We therefore\nfocus on purpose and mechanism in the rest of this section.\nWe also analyzed what proportion of participants’ saved ideas used unique purposes and mechanisms. We found\nthat participants used fewer unique purposes than mechanisms. For juniors, the proportion of saved ideas with unique\npurposes (M=0.58, SD=0.22) was lower than the proportion of saved ideas with unique mechanisms (M=0.69, SD=0.25).\nFor seniors, the proportion of saved ideas with unique purposes (M=0.52, SD=0.25) was also lower than the proportion\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n25\nFig. 16. Participants used input/near facets more than far facets.\nFig. 17. Junior participants were more willing than seniors to include far mechanisms and near purposes in their saved ideas.\nof saved ideas with unique mechanisms (M=0.64, SD=0.24). On average, the proportion of seniors’ saved ideas with a\nunique purpose or mechanism was slightly lower than that of juniors (Figure 15). Overall, these results indicate that\nparticipants preferred experimenting more with the mechanism rather than the purpose of their ideas.\nIn addition, we investigated if participants utilized input facets (i.e., facets from the input paper, manually-added facets,\nfacets generated based on a user-provided query), near analogous paper facets, or far analogous paper facets in their\nsaved ideas. We observed that participants were reluctant to use far purposes and mechanisms, particularly purposes\n(Figure 16). On average, the proportion of participants’ saved ideas with input purposes was 0.49 (SD=0.32), with near\npurposes was 0.43 (SD=0.30), and with far purposes was 0.08 (SD=0.15). Similarly, the proportion of participants’ saved\nideas with input mechanisms was 0.47 (SD=0.26), with near mechanisms was 0.38 (SD=0.26), and with far mechanisms\nwas 0.15 (SD=0.14). The reluctance towards using far mechanisms was stronger for seniors (M=0.10, SD=0.13) than\njuniors (M=0.19, SD=0.15) (Figure 17). Similarly, seniors were less likely to include near purposes in their saved ideas\n(M=0.31, SD=0.26) than juniors (M=0.54, SD=0.31). In their interviews, some participants indicated that the far facets\nwere too distant from their research or too unfamiliar to evaluate. For instance, P6-senior-treatment noted one factor\n\n26\nRadensky et al.\nthat they took into consideration when deciding on whether or not to save an idea was “if I actually knew some of the\n[components of the idea]... For example, this idea was basically suggesting a Saturn Ring Classifier Module, which I’m not\nentirely familiar with, so I don’t know if it’s novel or not.” Though they saw that the tool classified the idea as novel, they\nfelt uncomfortable saving the idea without their own familiarity with the subject.\nFalse\nTrue\nSubmission Status\n0\n5\n10\n15\n20\n25\nCorrected Path Length\nNumber of Interactions with the Idea vs Submission Status\nJunior\nSenior\nSeniority\n0\n5\n10\n15\n20\n25\nCorrected Path Length\nNumber of Interactions with the Idea\nby Seniority vs Submission Status\nSubmitted\nFalse\nTrue\nFig. 18. Participants interacted more with submitted ideas, with juniors engaging longer than seniors.\n5.4.2\nTreatment Ideation: Path Analysis. Submitted ideas typically involved longer interaction paths (M=6.47, STD=2.29)\ncompared to unsubmitted ideas (M=3.16, STD=2.81). This implies that saving an idea often required more exploration\nand interaction (Figure 18 (a)). Junior participants, in particular, engaged more with saved ideas and their facets (M=6.97,\nSTD=2.44) compared to seniors (M=5.88, STD=1.97) (Figure 18 (b)). Juniors appeared to explore the system more\nthoroughly, interacting with different parts of Scideator like facet selection, novelty checker, etc., while seniors\nreached conclusions faster, spending less time with the ideas that they ultimately saved. Notably, there was an outlier,\nwhich was an idea that was not submitted but still had a long interaction path. In this case, the senior participant used\nthe idea’s facets to generate new ideas, even though it was ultimately not saved.\nFor the ideas that were saved, the number of interactions juniors had with these ideas seemed to correlate with\ntheir excitement levels. As shown on the left side of Figure 19, juniors had longer interactions with ideas before saving\nthem, and subsequently rated these saved ideas with higher excitement (5, 6, 7), suggesting a correlation between their\nlevel of engagement and excitement. In contrast, seniors exhibited less variation in number of interactions based on\nexcitement ratings. As shown on the right side of Figure 19, juniors engage more with ideas they had thought of before\nthe study, while seniors interact more with unfamiliar ideas.\n5.4.3\nComparing Baseline and Treatment Ideation Patterns. While a commonly acknowledged benefit of the treatment\nwas its facet breakdown (see Section 5.4.1), the overarching benefit of the baseline that participants cited was its\nflexibility. A commonly raised advantage of the increased flexibility was that participants could request specific details\nabout how to implement a research idea, alluding to convergent ideation. On the other hand, participants noted that\nScideator felt restrictive when they wanted to take actions such as getting more details on an idea or learning more\nabout a topic. In particular, participants using Scideator expressed concerns about the feasibility of some ideas and\nwanted to better understand how they would work. For example, P15-senior-treatment noted, “A lot of times the tool\nuses these words like ‘character profile analysis’ or something like ‘disparity analysis.’ Saying this is just easier than saying\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n27\n2\n3\n4\n5\n6\n7\nIdea Excitement\n4\n6\n8\n10\n12\n14\nNumber of Interactions with the Idea\nNumber of Interactions with the Idea Vs Idea Excitement\nSeniority\nJunior\nSenior\nNeither Agree\nNor Disagree\n[4]\nThought of Idea\nPrior to Study\n[5, 6, 7]\nUnfamiliar with Idea\n[1, 2, 3]\n4\n6\n8\n10\n12\n14\nNumber of Interactions with the Idea\nNumber of Interactions with the Idea Vs Familiarity with the Idea\nSeniority\nJunior\nSenior\nFig. 19. Junior participants interacted more with ideas they were excited about and had thought of before the study. Senior participants\nengaged more with unfamiliar ideas, and showed less variation in the number of interaction for different idea excitement ratings.\nhow it should be done.... [The tool] didn’t introduce me to any existing stereotypical datasets that I can utilize, existing\nbenchmark datasets that I could actually use.... what does it mean?” Furthermore, some participants noted appreciation\nfor the Scideator ideas that contained more details. When asked what they liked about a saved idea that they shared,\nP12-senior-treatment explained, “Just superficially, you can see that it’s longer.... This one it actually got more specific\nabout what it recommended you do.... It was just a more exciting proposal with more detail.” One participant suggested\nthat a combination of interactions from the treatment and baseline may help to strike a balance between divergent and\nconvergent ideation. P15-senior-baseline reflected, “I wish there was a way to combine both the tools because the facets\nwere very interesting.... ‘do you want just two/three lines of an idea or a detailed experiment designed with more details’... if\nI had that option it would have been really nice.”\n6\nDISCUSSION\nGrounding mixed-initiative scientific ideation in research-paper facets shows promise for supporting\ndivergent ideation. We introduced Scideator, a mixed-initiative, LLM-powered tool for scientific ideation through\nresearch-paper facet recombination. While prior work established the utility of extracting purpose and mechanism\nfacets from research papers for identifying scientific analogies [4, 21, 36], we are the first to apply this framework to a\nhuman-LLM interaction for scientific ideation. Aligned with our goal of supporting divergent ideation [8, 39], results\nfrom a within-subjects study show that participants were able to find more ideas that they thought were both novel and\ninteresting to think about further when using Scideator as opposed to the baseline of access to an LLM and scientific\nsearch engine. Through semi-structured interview responses, we observed that participants appreciated being able to\nmix and match relevant purposes and mechanisms for research idea generation.\nProviding support for mixed-initiative idea novelty assessment may be beneficial for divergent scientific\nideation. Participants often found value in having an idea novelty checker that they could activate for ideas of interest.\nIndeed, participants used the novelty checker an average of 6 times. That said, in their interviews, participants also\nnoted a couple concerns regarding the novelty checker. Two common concerns were that the novelty checker classified\nideas as “novel” too often and that the set of most related papers was sometimes missing important works. Both of\n\n28\nRadensky et al.\nthese concerns may have been reduced if the tool had retrieved more papers for its evaluation of an idea, which it did\nnot in the user study due to latency limitations.\nMore support is needed for generating ideas that are both very new to the scientist and capture the\nscientist’s interest. Scideator helped participants to generate more ideas that they thought were novel and interesting\nto think about further. However, there is room for improvement in terms of helping scientists to come up with ideas\nthat are very new to them yet still relevant to them. After using Scideator, participants described avoiding certain\nfacets because they were not familiar with how to use them, did not know their meaning, or found them to be “too far”\nfrom their research area. They also sometimes mentioned avoiding certain ideas because they did not know how the\nassociated mechanism could be used to achieve the associated purpose. Correspondingly, we observed that participants\ndid not save ideas with far facets as much as ideas with input and near facets. By avoiding unfamiliar and distant facets,\nscientists may miss opportunities for generating ideas that are not only novel but completely new to them, helping\nthem think of more ideas that would never have occurred to them otherwise. While we provide short descriptions of\nfacets in Scideator, this could be taken a step further with in-context question-answering to help users understand\na facet to whatever degree is necessary. To help users recognize the utility of unfamiliar facets and the feasibility of\ngenerated ideas, the tool could allow users to ask questions about how a facet may be used in general or in relation to a\nparticular idea. In addition, the tool could be more transparent and provide users the analogy between two papers that\nit used to generate an idea. This could help users to more easily see the connection between the input/near facets and\nthe far facets.\nScientists may benefit from LLM-augmented support in determining the feasibility of their ideas during\ndivergent ideation. We found that, while Scideator supported divergent ideation by helping participants to generate\nmore research ideas worth saving than the baseline, participants appreciated the baseline’s flexibility for obtaining\nmore details about how to implement a research idea, an advantage more related to convergent ideation. This mirrors\nChoi et al.’s findings in a study on graphic design ideation [6]. They found that participants benefited more from their\ngenerative AI tool for keyword recombination in terms of divergent ideation, but the participants benefited more from\nthe baseline, which included access to chatGPT and a layout diffusion model, in terms of convergent ideation. The goal\nof Scideator was to support divergent ideation, producing many potential research ideas rather than one polished\nresearch proposal. Nevertheless, interview responses indicate that scientists may appreciate support in understanding\nthe feasibility of their potential research ideas in order to get a sense of whether or not an idea is worth saving in the\nfirst place.\n7\nLIMITATIONS AND FUTURE WORK\nThe study has a number of limitations that we should note. First, the participants were only computer-science researchers\nwho are moderately to very familiar with LLMs. Future work may investigate how other scientists might work with a\ntool like Scideator. Second, there were only 19 participants. Given the small sample size, future work may be done to\nvalidate the results observed here. Third, participants had a limited amount of time (20 minutes) to interact with the\ntools. A few participants mentioned that they might have tried some actions with Scideator that they did not get to do\nhad they had more time with it. Future studies may explore how scientists utilize a tool like Scideator over a longer\nperiod of time. Fourth, we compared Scideator to a strong baseline combining interaction with an LLM and scientific\nsearch engine. However, it would be interesting to explore how interaction with Scideator compares to other tools for\nscientific ideation such as CoQuest [28]. Fifth, due to latency constraints, our Idea Novelty Checker module utilized\nonly 10 relevant papers to assess the novelty of an idea during the study. Future work could look into how scientists\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n29\ninteract with the module when it has access to more relevant papers in its novelty evaluation. Sixth, to keep the study\nrelatively simple, we asked participants to provide one paper as input to Scideator. Future work may evaluate how the\ninteraction changes with more input papers. Seventh, in order to allow people to ideate on the topics about which they\nare most excited, we left the selection of the paper used as a starting point for ideation up to the participant, beyond\nthe fact that it was not their own paper. Future work may examine how input paper familiarity and recency impacts\nthe experience and outcome of working with Scideator. Lastly, we observed a learning effect in which participants\ngenerated more ideas worth saving in the baseline if they had gone through the treatment first. Although this indicates\nthat the treatment helped participants get into a mindset effective for divergent idea generation, it is unclear whether\nthis stems from participants adopting parts of the Scideator workflow. Future work should clarify this phenomenon,\nperhaps by comparing Scideator to other types of baselines such as ones with a button to generate several research\nideas without using facet recombination.\n8\nCONCLUSION\nWe present Scideator, a novel mixed-initiative tool for scientific idea generation that takes a set of papers as input and\nextracts key facets (purposes, mechanisms, and evaluations) from the input and retrieved analogous papers. Users can\nexplore different recombinations of these facets, synthesized in succinct ideas. They can also use Scideator to then\nevaluate and iterate on ideas’ novelty. We introduce four LLM-powered retrieval-augmented generation (RAG) modules\nto support Scideator’s workflow: Analogous Paper Facet Finder, Faceted Idea Generator, Idea Novelty Checker, and\nIdea Novelty Iterator. We found that, in comparison to a strong baseline combining interactions with an LLM and a\nscientific search engine, our tool helped computer-science researchers to generate more ideas that they thought were\nboth novel and interesting.\nACKNOWLEDGMENTS\nThis research was supported by the Allen Institute for Artificial Intelligence (AI2). The authors thank the many folks at\nAI2 who provided helpful feedback on this work, including Peter Jansen, Peter Clark, and Ashish Sabharwal. We also\nthank the participants who made this work possible.\nREFERENCES\n[1] Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. 2024. Researchagent: Iterative research idea generation over scientific\nliterature with large language models. arXiv preprint arXiv:2404.07738 (2024).\n[2] Lutz Bornmann and Rüdiger Mutz. 2015. Growth rates of modern science: A bibliometric analysis based on the number of publications and cited\nreferences. Journal of the association for information science and technology 66, 11 (2015), 2215–2222.\n[3] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3, 2 (2006), 77–101.\n[4] Joel Chan, Joseph Chee Chang, Tom Hope, Dafna Shahaf, and Aniket Kittur. 2018. Solvent: A mixed initiative system for finding analogies between\nresearch papers. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 1–21.\n[5] Lydia B Chilton, Savvas Petridis, and Maneesh Agrawala. 2019. VisiBlends: A flexible workflow for visual blends. In Proceedings of the 2019 CHI\nConference on Human Factors in Computing Systems. 1–14.\n[6] DaEun Choi, Sumin Hong, Jeongeon Park, John Joon Young Chung, and Juho Kim. 2024. CreativeConnect: Supporting Reference Recombination for\nGraphic Design Ideation with Generative AI. In Proceedings of the CHI Conference on Human Factors in Computing Systems. 1–25.\n[7] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S. Weld. 2020. SPECTER: Document-level Representation Learning using\nCitation-informed Transformers. ArXiv abs/2004.07180 (2020). https://api.semanticscholar.org/CorpusID:215768677\n[8] Arthur Cropley. 2006. In praise of convergent thinking. Creativity research journal 18, 3 (2006), 391–404.\n[9] Mike D’Arcy, Tom Hope, Larry Birnbaum, and Doug Downey. 2024. Marg: Multi-agent review generation for scientific papers. arXiv preprint\narXiv:2401.04259 (2024).\n\n30\nRadensky et al.\n[10] Douglas L Dean, Jill Hender, Tom Rodgers, and Eric Santanen. 2006. Identifying good ideas: constructs and scales for idea evaluation. Journal of\nAssociation for Information Systems 7, 10 (2006), 646–699.\n[11] Karl Duncker and Lynne S Lees. 1945. On problem-solving. Psychological monographs 58, 5 (1945), i.\n[12] Jingtong Gao, Bo Chen, Xiangyu Zhao, Weiwen Liu, Xiangyang Li, Yichao Wang, Zijian Zhang, Wanyu Wang, Yuyang Ye, Shanru Lin, Huifeng\nGuo, and Ruiming Tang. 2024. LLM-enhanced Reranking in Recommender Systems. ArXiv abs/2406.12433 (2024). https://api.semanticscholar.org/\nCorpusID:270562015\n[13] Hua Guo and David H Laidlaw. 2018. Topic-based exploration and embedded visualizations for research idea generation. IEEE transactions on\nvisualization and computer graphics 26, 3 (2018), 1592–1607.\n[14] Holistic Intelligence for Global Good. [n. d.]. SeeChat x Ideas. https://higg.world/seechat-ideas/.\n[15] Keith J Holyoak and Paul Thagard. 1996. Mental leaps: Analogy in creative thought. MIT press.\n[16] Tom Hope, Joel Chan, Aniket Kittur, and Dafna Shahaf. 2017. Accelerating innovation through analogy mining. In Proceedings of the 23rd ACM\nSIGKDD international conference on knowledge discovery and data mining. 235–243.\n[17] Tom Hope, Doug Downey, Daniel S Weld, Oren Etzioni, and Eric Horvitz. 2023. A computational inflection for scientific discovery. Commun. ACM\n66, 8 (2023), 62–73.\n[18] Tom Hope, Ronen Tamari, Daniel Hershcovich, Hyeonsu B Kang, Joel Chan, Aniket Kittur, and Dafna Shahaf. 2022. Scaling creative inspiration\nwith fine-grained functional aspects of ideas. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. 1–15.\n[19] Arif E Jinha. 2010. Article 50 million: an estimate of the number of scholarly articles in existence. Learned publishing 23, 3 (2010), 258–263.\n[20] Hyeonsu B Kang, David Chuan-En Lin, Nikolas Martelaro, Aniket Kittur, Yan-Ying Chen, and Matthew K Hong. 2024. BioSpark: An End-to-End\nGenerative System for Biological-Analogical Inspirations and Ideation. In Extended Abstracts of the CHI Conference on Human Factors in Computing\nSystems. 1–13.\n[21] Hyeonsu B Kang, Xin Qian, Tom Hope, Dafna Shahaf, Joel Chan, and Aniket Kittur. 2022. Augmenting scientific creativity with an analogical\nsearch engine. ACM Transactions on Computer-Human Interaction 29, 6 (2022), 1–36.\n[22] James C Kaufman and Robert J Sternberg. 2010. The Cambridge handbook of creativity. Cambridge University Press.\n[23] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T.\nJoshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2023. DSPy: Compiling Declarative Language Model Calls into\nSelf-Improving Pipelines. arXiv preprint arXiv:2310.03714 (2023).\n[24] Rodney Michael Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra,\nYoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph\nGorney, David W. Graham, F.Q. Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle\nLo, Jaron Lochner, Kelsey MacMillan, Tyler C. Murray, Christopher Newell, Smita R Rao, Shaurya Rohatgi, Paul Sayre, Zejiang Shen, Amanpreet\nSingh, Luca Soldaini, Shivashankar Subramanian, A. Tanaka, Alex D Wade, Linda M. Wagner, Lucy Lu Wang, Christopher Wilhelm, Caroline\nWu, Jiangjiang Yang, Angele Zamarron, Madeleine van Zuylen, and Daniel S. Weld. 2023. The Semantic Scholar Open Data Platform. ArXiv\nabs/2301.10140 (2023). https://api.semanticscholar.org/CorpusID:256194545\n[25] Dan Lahav, Jon Saad Falcon, Bailey Kuehl, Sophie Johnson, Sravanthi Parasa, Noam Shomron, Duen Horng Chau, Diyi Yang, Eric Horvitz, Daniel S\nWeld, et al. 2022. A search engine for discovery of scientific challenges and directions. In Proceedings of the AAAI Conference on Artificial Intelligence,\nVol. 36. 11982–11990.\n[26] Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Scott Smith, Yian Yin, et al.\n2024. Can large language models provide useful feedback on research papers? A large-scale empirical analysis. NEJM AI 1, 8 (2024), AIoa2400196.\n[27] Gionnieve Lim and Simon T Perrault. 2024. Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models.\narXiv preprint arXiv:2403.12928 (2024).\n[28] Yiren Liu, Si Chen, Haocong Cheng, Mengxia Yu, Xiao Ran, Andrew Mo, Yiliu Tang, and Yun Huang. 2024. How ai processing delays foster\ncreativity: Exploring research question co-creation with an llm-based agent. In Proceedings of the CHI Conference on Human Factors in Computing\nSystems. 1–25.\n[29] Yiren Liu, Mengxia Yu, Meng Jiang, and Yun Huang. 2023. Creative Research Question Generation for Human-Computer Interaction Research.. In\nIUI Workshops. 58–66.\n[30] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The ai scientist: Towards fully automated open-ended\nscientific discovery. arXiv preprint arXiv:2408.06292 (2024).\n[31] Sheshera Mysore, Arman Cohan, and Tom Hope. 2022. Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity.\nIn Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.\n4453–4470.\n[32] Sheshera Mysore, Tim O’Gorman, Andrew McCallum, and Hamed Zamani. [n. d.]. CSFCube–A Test Collection of Computer Science Research\nArticles for Faceted Query by Example. ([n. d.]).\n[33] Harshit Nigam, Manasi Patwardhan, Lovekesh Vig, and Gautam Shroff. 2024. Acceleron: A Tool to Accelerate Research Ideation. arXiv preprint\narXiv:2403.04382 (2024).\n[34] Harshit Nigam, Manasi Patwardhan, Lovekesh Vig, and Gautam Shroff. 2024. An Interactive Co-Pilot for Accelerated Research Ideation. In\nProceedings of the Third Workshop on Bridging Human–Computer Interaction and Natural Language Processing. 60–73.\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n31\n[35] OpenReview. [n. d.]. OpenReview. https://openreview.net/.\n[36] Jason Portenoy, Marissa Radensky, Jevin D West, Eric Horvitz, Daniel S Weld, and Tom Hope. 2022. Bursting scientific filter bubbles: Boosting\ninnovation via novel author discovery. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. 1–13.\n[37] A Terry Purcell and John S Gero. 1996. Design and other types of fixation. Design studies 17, 4 (1996), 363–383.\n[38] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Conference on Empirical Methods\nin Natural Language Processing. https://api.semanticscholar.org/CorpusID:201646309\n[39] Mark A Runco et al. 2010. Divergent thinking, creativity, and ideation. The Cambridge handbook of creativity 413 (2010), 446.\n[40] Orit Shaer, Angelora Cooper, Osnat Mokryn, Andrew L Kun, and Hagit Ben Shoshan. 2024. AI-Augmented Brainwriting: Investigating the use of\nLLMs in group ideation. In Proceedings of the CHI Conference on Human Factors in Computing Systems. 1–17.\n[41] Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. 2024. Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP\nResearchers. arXiv preprint arXiv:2409.04109 (2024).\n[42] Arvind Srinivasan and Joel Chan. 2024. Improving Selection of Analogical Inspirations through Chunking and Recombination. In Proceedings of the\n16th Conference on Creativity & Cognition. 374–397.\n[43] Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun Li, and Haijun Xia. 2024. Luminate: Structured Generation and Exploration of Design Space with\nLarge Language Models for Human-AI Co-Creation. In Proceedings of the CHI Conference on Human Factors in Computing Systems. 1–26.\n[44] Lu Sun, Aaron Chan, Yun Seo Chang, and Steven P Dow. 2024. ReviewFlow: Intelligent Scaffolding to Support Academic Peer Reviewing. In\nProceedings of the 29th International Conference on Intelligent User Interfaces. 120–137.\n[45] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good\nat Search? Investigating Large Language Models as Re-Ranking Agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing. 14918–14937.\n[46] P Thagard. 2012. The cognitive science of science: Explanation, discovery, and conceptual change. The MIT Press.\n[47] Jianyou Wang, Kaicheng Wang, Xiaoyue Wang, Prudhviraj Naidu, Leon Bergen, and Ramamohan Paturi. 2023. DORIS-MAE: scientific document\nretrieval using multi-level aspect-based queries. In Proceedings of the 37th International Conference on Neural Information Processing Systems.\n38404–38419.\n[48] Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. 2023. Scimon: Scientific inspiration machines optimized for novelty. arXiv preprint\narXiv:2305.14259 (2023).\n[49] Hongji Yang, Delin Jing, and Lu Zhang. 2016. Creative Computing: an approach to knowledge combination for creativity?. In 2016 IEEE Symposium\non Service-Oriented System Engineering (SOSE). IEEE, 407–414.\n[50] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. 2024. TextGrad: Automatic\" Differentiation\"\nvia Text. arXiv preprint arXiv:2406.07496 (2024).\nA\nEXPERT-LABELED EXAMPLES\nTable 4 presents expert-labeled examples from our annotation study, which include the idea, the most relevant papers\n(with links to Semantic Scholar), and the reasoning provided by the experts. These experts are the paper’s first two\nauthors. The examples come from the training set of our dataset. In our novelty checker, we add both the titles and\nabstracts of the most relevant papers for each idea.\nTable 4. Expert-labeled examples from annotation study with ideas, relevant papers, and reasoning\nfor novelty evaluation.\nExample 1\nIdea: Develop a natural language processing classifier designed to improve scientific paper revisions by automatically\nidentifying and categorizing reviewer comments that are most likely to lead to substantial and actionable revisions. The system\nwould be trained on a manually-labeled dataset analysis of scientific review comments and the corresponding paper edits,\nleveraging features such as linguistic cues, sentiment, and comment specificity to predict the likelihood of a comment being\nacted upon. This classifier could then be used to prioritize reviewer feedback, helping authors focus on the most impactful\nsuggestions first.\n\n32\nRadensky et al.\nMost Relevant Papers:\n(1) ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews\n(2) Can large language models provide useful feedback on research papers? A large-scale empirical analysis\n(3) A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications\n(4) arXivEdits: Understanding the Human Revision Process in Scientific Writing\n(5) Characterizing Text Revisions to Better Support Collaborative\n(6) Can We Automate Scientific Reviewing?\n(7) DeepReviewer: Collaborative Grammar and Innovation Neural Network for Automatic Paper Review\n(8) Aspect-based Sentiment Analysis of Scientific Reviews\n(9) Aspect-based sentiment analysis of online peer reviews and prediction of paper acceptance results\n(10) ReviVal: Towards Automatically Evaluating the Informativeness of Peer Reviews\nReasoning: The idea is novel because it uniquely focuses on prioritizing reviewer comments for actionable revisions, which\nis not explicitly addressed in ARIES[1] or other related works like ReviVal [10].\nExample 2\nIdea: Develop a systematic review-based framework designed to align LLM evaluation with human preferences,\nensuring that evaluation criteria are continuously refined based on comprehensive reviews of user feedback and emerging\nmodel behaviors. This framework will utilize content analysis of user interactions and feedback to identify patterns and\nareas of improvement. The effectiveness of this framework will be assessed through a qualitative study involving iterative\ncycles of user feedback and criteria refinement.\nMost Relevant Papers:\n(1) EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria\n(2) HUMANELY: HUMAN EVALUATION OF LLM YIELD, USING A NOVEL WEB BASED EVALUATION TOOL\n(3) Evaluation of Code Generation for Simulating Participant Behavior in Experience Sampling Method by Iterative\nIn-Context Learning of a Large Language Model\n(4) Human-Centered Evaluation and Auditing of Language Models\n(5) Aligning Model Evaluations with Human Preferences: Mitigating Token Count Bias in Language Model Assessments\n(6) Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences\n(7) Human-Centered Design Recommendations for LLM-as-a-judge\n(8) CheckEval: Robust Evaluation Framework using Large Language Model via Checklist\n(9) Discovering Language Model Behaviors with Model-Written Evaluations\n(10) Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models\nReasoning: The idea is not novel because it closely resembles existing frameworks like EvalLM[1] and HumanELY[2], which\nalready align LLM evaluations with human preferences using user-defined criteria and human feedback.\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n33\nExample 3\nIdea: Develop a scholarly paper recommendation system that uses contextualized text descriptions to personalize paper\nrecommendations. By analyzing a researcher’s prior publications and saved papers, the system generates detailed contextual\ndescriptions for each recommended paper, comparing them to the user’s body of work. This approach not only highlights the\nrelevance but also provides a deeper understanding of how each recommendation aligns with the user’s research interests. The\neffectiveness of this system can be assessed through a user study, ensuring that researchers find the recommended papers\nmore relevant and easier to triage.\nMost Relevant Papers:\n(1) PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers\n(2) CiteSee: Augmenting Citations in Scientific Papers with Persistent and Personalized Historical Context\n(3) Threddy: An Interactive System for Personalized Thread-based Exploration and Organization of Scientific Literature\n(4) Relatedly: Scaffolding Literature Reviews with Existing Related Work Sections\n(5) CiteRead: Integrating Localized Citation Contexts into Scientific Paper Reading\n(6) DiscipLink: Unfolding Interdisciplinary Information Seeking Process via Human-AI Co-Exploration\n(7) SciLit: A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation\n(8) Rec4LRW - Scientific Paper Recommender System for Literature Review and Writing\n(9) Recommender System for Literature Review and Writing\n(10) Can I have more of these please?: Assisting researchers in finding similar research papers from a seed basket of papers\nReasoning: The idea is not novel because it closely mirrors the approach of PaperWeaver[1], which also uses user-collected\npapers to generate contextualized descriptions for recommendations. Both systems aim to enhance relevance and understanding\nof recommended papers through user-specific contexts.\n\n34\nRadensky et al.\nB\nCOMPARING BASELINES NOVELTY ASSESSMENT\nIn this section, we provide a qualitative comparison of the novelty evaluations conducted by AI Scientist, AI Researcher,\nand Scideator on two research ideas. These qualitative examples are presented in Tables 5 and 6, showcasing how\neach system assesses the novelty of a research idea.\nAs observed from the two tables, Scideator (ours) provides concise reasons for the novelty decision, attributed to\nthe most relevant papers. For instance, in Example 1, it correctly identified the idea as “novel” by referencing existing\nworks and highlighting specific similarities and differences. While AI Researcher also aligns with the ground truth\nlabels, that is considers the ideas to be novel, it differs in its novelty evaluation. Specifically, AI Researcher evaluates\neach idea by comparing it with one paper at a time to determine if the paper should be “cited”, considering the idea\n“not novel” if any paper is citable. In our examples, AI Researcher did not find any paper to be citable for the ideas, even\nthough the papers share similar purposes (See more details in Appendix D). Due to space constraints, we show only the\ninsights from the first paper for each example in the tables.\nIn Table 6, AI Scientist’s novelty judgment aligns with the ground truth. It also brings an interesting approach in\nits reasoning by highlighting what is missed in current papers and providing actionable suggestions in the form of\nqueries to extract more relevant literature. However, we notice that AI Scientist occasionally misinterprets the idea. For\nexample, in Table 5, its prompt overlooks the idea altogether and focuses on the code that is also provided as input (see\nmore details in Appendix D).\nTable 5. Comparison of reasoning provided by different approaches for first example idea.\nExample 1\nIdea: Develop a system that uses a faceted author representation of digital learning resource (DLR) creators based on\ntheir educational materials and inferred teaching personas. This system would aim to support ubiquitous learning by\nhelping learners discover novel educators and materials that offer innovative perspectives. Usability testing of learning\nresources would be conducted to ensure the system enhances the learning experience by balancing relevance and\nnovelty, thus boosting the accessibility and discoverability of diverse educational content.\nMost Relevant Papers:\n(1) Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery\n(2) Bridger: Toward Bursting Scientific Filter Bubbles and Boosting Innovation via Novel Author Discovery\n(3) Novel Algorithmic Recommendation Engine for Diverse Content Discovery\n(4) ComLittee: Literature Discovery with Personal Elected Author Committees\n(5) Explanations in Open User Models for Personalized Information Exploration\n(6) AMiner: Mining Deep Knowledge from Big Scholar Data\n(7) Similar researcher search in academic environments\n(8) VeTo-web: A Recommendation Tool for the Expansion of Sets of Scholars\n(9) From Who You Know to What You Read: Augmenting Scientific Recommendations with Implicit Social\nNetworks\n(10) DiscipLink: Unfolding Interdisciplinary Information Seeking Process via Human-AI Co-Exploration\nExpert-Labeled Class: Novel\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n35\nMethod\nReview\nScideator (Ours)\nThe idea is novel because it introduces the concept of faceted author representation specifically\nfor digital learning resource (DLR) creators, focusing on their educational materials and in-\nferred teaching personas to support ubiquitous learning. This approach is distinct from related\nworks like Bridger[1][2], which focus on scientific author discovery, and ComLittee[4], which\nsupports authorcentric exploration in research. The proposed system aims to enhance the\ndiscoverability of diverse educational content, balancing relevance and novelty, which is not\nexplicitly addressed in the related papers.\nAI Scientist\nTHOUGHT: The proposed research presents an innovative approach to language models. The\nrough goal is to train a model to predict the next character in a sequence in order to be able to\ncapture the semantic meaning of the entire sequence, which can be used in language models.\nThis approach is not uncommon, though, as there are many areas where we must generate\npredictions of the next likely character, which can be used in order to understand the entire\nmeaning of the text. This model aims to improve upon traditional models by locally training\nthe model on the use of characters rather than words. Of particular note is the fact that the\nmodel uses a scaled gradient descent method which changes how the weights are updated, a\nmethod that is very new and, thus, is a key point of innovation. While that is extremely novel,\na model that works on the character level is not. The primary new technique here is the use of\na momentum of the gradient, which is used to improve learning. The model uses a technique\nsimilar to many current models. It is possible that it could be used with different parameters,\nthough, to learn more or to learn in a more effective way than other models. This approach has\nbeen suggested many times over the years, but it is promising to pursue. It is, however, a novel\napproach to the existing technology.\nHowever, we must consider the existing literature and how it might overlap with this propo-\nsition. I will now proceed to search for related publications that may be relevant in order\nto establish a definitive argument regarding the novelty of the proposed model. RESPONSE:\n“‘json\\n\\n \"Query\": \"Character-level language models\"\\n\\n“‘\\n I believe this should identify\nany previous work or existing lines of research that are most similar to the proposed model.\nThis will confirm whether this is actually a novel concept. My primary concern is understanding\nif this is a novel or an existing work, as this is a model that could potentially be used for many\npurposes. There is a good chance that it might have already been proposed. I hope that this\nsearch will bring more clarity to the argument.\nIf decision is not made, default is: not novel.\n\n36\nRadensky et al.\nAI Researcher\nProposed Research Problem and Approach:\nProject Proposal: The research problem is the limited accessibility and discoverability of inno-\nvative educational content due to the lack of effective representation and utilization of diverse\nteaching personas and educational materials of DLR creators. The approach involves developing\na faceted author representation based on creators’ educational materials and inferred teaching\npersonas to enhance the discovery of novel educational content.\nPaper Abstract: The research problem is the isolation of scientific research and information\noverload, which limit awareness and hinder innovation. The approach involves constructing a\nfaceted representation of authors from their papers and inferred personas to balance relevance\nand novelty in discovering new scholars and their work.\nSummary and Differences: - Project Proposal Summary: Enhancing the discovery of digital\nlearning resources by developing a faceted author representation based on educational materials\nand teaching personas. - Paper Abstract Summary: Facilitating the discovery of new scholars and\ntheir work by constructing a faceted author representation from scientific papers and inferred\npersonas to balance relevance and novelty. - Differences: - The project proposal focuses on\ndigital learning resources and educational content, aiming to improve the learning experience.\n- The paper abstract focuses on scientific research and scholars, aiming to boost innovation and\nbridge diverse scientific communities.\nNo\n........ The comparison with other papers has been truncated due to space limitations. All papers in\nthe most relevant set are marked as \"Binary Judgment: No\", which implies it should not be cited\nas similar work to the idea.\nClassification: novel\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n37\nTable 6. Comparison of reasoning provided by different approaches for second example idea.\nExample 2\nIdea: Develop a Co-Creative Interaction Framework for LLM-assisted evaluations to align llm evaluation\nwith human preferences. This framework will map out detailed interaction models between human evaluators and\nLLMs, including turn-taking, communication protocols, and iterative feedback loops. The framework will be evaluated\nthrough qualitative study involving user testing and thematic analysis of evaluator experiences, capturing how well\nthe framework supports alignment with human preferences and identifying areas for iterative improvement.\nMost Relevant Papers:\n(1) EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria\n(2) Human-Centered Design Recommendations for LLM-as-a-judge\n(3) CheckEval: Robust Evaluation Framework using Large Language Model via Checklist\n(4) Human-Centered Evaluation and Auditing of Language Models\n(5) Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models\n(6) Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences\n(7) Evaluation of Code Generation for Simulating Participant Behavior in Experience Sampling Method by Iterative\nIn-Context Learning of a Large Language Model\n(8) Discovering Language Model Behaviors with Model-Written Evaluations\n(9) LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models\n(10) Do LLMs Meet the Needs of Software Tutorial Writers? Opportunities and Design Implications\nExpert-Labeled Class: Novel\nMethod\nReview\nScideator (Ours)\nThe idea is novel because it introduces a CoCreative Interaction Framework specifically\ndesigned to align LLM evaluation with human preferences through detailed interaction models,\nincluding turntaking, communication protocols, and iterative feedback loops. This approach is\ndistinct from related works like EvalLM[1], which focuses on iterative refinement of prompts,\nand EvaluLLM[2], which leverages LLMs as customizable judges but does not map out detailed\ninteraction models. Additionally, CheckEval[3] and Prometheus 2[5] focus on evaluation\nframeworks and opensource evaluators, respectively, without the cocreative interaction aspect.\nThe proposed framework’s emphasis on qualitative studies and thematic analysis to capture\nevaluator experiences further differentiates it from existing methods.\n\n38\nRadensky et al.\nAI Scientist\nTHOUGHT:\\n The proposed idea involves designing a framework for co-creative interaction\nbetween human evaluators and Large Language Models (LLMs), focusing on aspects like turn-\ntaking and communication protocols. It also emphasizes qualitative study and thematic analysis.\nThis is a very specific angle that combines human-computer interaction and AI evaluation.\nWhile several existing papers discuss LLM evaluation frameworks, human-centered design,\nand human involvement in evaluation (e.g., EvalLM, EvaluLLM, and other papers from the\nresults), the specific focus on co-creative interaction frameworks with detailed interaction\nmodels and iterative feedback loops may not have been explored thoroughly.\\n\\n Additionally,\nthe idea looks at aligning evaluations with human preferences which is a nuanced approach.\nTherefore, a more targeted query to ascertain if any existing literature discusses similar co-\ncreative frameworks or detailed interaction models involving human evaluators and LLMs is\nnecessary. The current results do not seem to cover this specific angle in depth.\nDecision made: novel\nRESPONSE:\\n“‘json\\n\\n \"Query\": \"Co-creative interaction framework LLM evaluation\"\\n\\n“‘\nAI Researcher\nProposed Research Problem and Approach:\n- Project Proposal: - Problem: Current LLM evaluation methods often fail to align with human\npreferences. - Approach: Develop a Co-Creative Interaction Framework (CCIF) that includes\ndetailed interaction models between human evaluators and LLMs, facilitating turn-taking,\ncommunication protocols, and iterative feedback loops to better align LLM evaluations with\nhuman preferences. - Paper Abstract: - Problem: Developers must iteratively revise prompts by\nevaluating outputs to diagnose weaknesses, which is effort-intensive and context-specific. -\nApproach: Present EvalLM, an interactive system for iteratively refining prompts by evaluating\nmultiple outputs on user-defined criteria, using an LLM-based evaluator to provide feedback and\nimprove prompts. - Differences: - The project proposal focuses on aligning LLM evaluations with\nhuman preferences through a co-creative interaction framework involving detailed interaction\nmodels. - The paper abstract focuses on refining LLM prompts through an interactive system\nthat evaluates outputs based on user-defined criteria.\nNo\n........ The comparison with other papers has been truncated due to space limitations. All papers in\nthe most relevant set are marked as \"Binary Judgment: No\", which implies it should not be cited\nas similar work to the idea.\nClassification: novel\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n39\nC\nINSIGHTS FROM LLM PROMPT OPTIMIZATION USING TEXTGRAD\nIn our experiments with TextGrad, we investigated how specific prompt instructions influence an LLM’s ability to\nclassify the novelty of an idea. Table 7 presents the accuracy of various prompts optimized with TextGrad on our dataset\n(train=25, validation = 10, test = 32).\nPrompts with both non-zero and zero validation accuracy included various instructions for evaluating the novelty of\nideas, such as assessing the uniqueness of methods and their comparison to existing research. Through this prompt\noptimization process, we observed interesting ways in which LLMs may evaluate novelty, like considering historical\ncontext, frequency of similar studies, comparative analysis with existing works, examining arguments for both novel\nand non-novel perspectives. However, prompts without these specific instructions also influenced accuracy, suggesting\nthe complexity of novelty evaluation with LLMs.\nNotably, some prompts with similar instructions showed different performance on validation data. For example,\nboth prompt 3 (accuracy = 0) and prompt 9 (accuracy = 0.6) includes instructions to evaluate if idea introduces unique\nmethodologies, and how it compares to existing work. However, the difference in their performance suggests that\nsubtle variations in wording and instruction framing can significantly impact the classification performance. It remains\nunclear why certain prompts perform better despite having similar instructions.\nOur analysis highlights the LLM’s sensitivity to prompt design when assessing novelty of an idea. Even minor\nvariations in wording and structure can lead to substantial performance changes, emphasizing the need for careful\nprompt engineering and well-chosen in-context examples to guide the LLM for idea novelty evaluation.\nTable 7. Performance trends of test accuracy across prompts during prompt optimization with\nTextGRAD. Highlighted text shows unique instructions used to evaluate the novelty of ideas. The final test accuracy was 0.78125,\nshowing that none of the optimized prompts (1 to 12) improved over the original.\nPrompt\nNumber\nValidation Ac-\ncuracy\nPrompt Text\nStarting\nPrompt\n0.8\nYou are a classifier. Given a research idea and related papers, classify the idea\nas ‘novel’ or ‘not novel’ Respond in two lines:\nReview: <This is the review you will generate after comparing idea with related\npapers.>\nClassification:<novel or not novel label according to your review/rationale>\n\n40\nRadensky et al.\nPrompt\nNumber\nValidation Ac-\ncuracy\nPrompt Text\n1\n0\nYou are a classifier. Given a research idea and related papers, classify the idea\nas ‘novel’ or ‘not novel’.\nA novel idea introduces a unique, groundbreaking concept or ap-\nproach not previously covered in the literature. A not novel idea\nreiterates\nor\nslightly\nmodifies\nexisting\nresearch.\nConsider\nthe\nhistorical context and frequency of similar studies\nwhen\nmaking\nyour\nclassification. Focus on identifying unique, groundbreaking elements that\ndifferentiate the idea from existing research. Do not rely solely on keywords or\nthe mention of a controlled setting to determine novelty. If the classification is\nambiguous, indicate uncertainty and suggest a human review.\nRespond with only the classification label: ‘novel’ or ‘not novel’.\n2\n0.7\nYou are a classifier. Classify the research idea as ‘novel’ or ‘not novel’ based on\nthe related papers. Provide a brief review and directly state the classification.\nReview: <This is the review you will generate after comparing the idea with\nrelated papers.> Classification: <novel or not novel label according to your\nreview/rationale>\nExample: Review: The idea is unique as it combines adaptive interfaces with AI\nexplanations, which is not covered in the provided papers. Classification: novel.\n3\n0\nYou are a classifier. Given a research idea and related papers, classify the idea\nas ‘novel’ or ’not novel.’\nRespond in two lines:\nReview: Provide a detailed review comparing the idea with related papers.\nInclude specific examples and details from the referenced papers to justify\nyour classification. Highlight both similarities and differences between the\nproposed idea and existing methodologies. Ensure your review is concise and\nprecise, focusing on the main arguments.\nClassification: Based on the review, classify the idea as ‘novel’ or ‘not novel’\naccording to the following criteria:\n- Uniqueness of the approach\n- Originality of the application\n- Novelty of the results\nProvide specific references or evidence from the papers mentioned to support\nyour classification. Use assertive language to clearly convey your classifica-\ntion.\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n41\nPrompt\nNumber\nValidation Ac-\ncuracy\nPrompt Text\n4\n0.7\nYou are a classifier. Classify the research idea as ‘novel’ or ‘not novel’ based on\nthe related papers. Respond concisely:\nReview: <brief review> Classification: <novel or not novel>\n5\n0\nYou are a classifier. Given a research idea and related papers, classify the idea\nas ‘novel’ or ’not novel.’\nRespond in two lines:\n1. **Review**: Provide a detailed review comparing the idea with related papers.\nInclude specific examples and details from the referenced papers to justify your\nclassification. Highlight both similarities and differences between the proposed\nidea and existing methodologies. Ensure your review is concise and precise,\nfocusing on the main arguments.\n2. **Classification**: Based on the review, classify the idea as ‘novel’ or ‘not\nnovel’ according to the following criteria:\n- Uniqueness of the approach\n- Originality of the application\n- Novelty of the results\n- Provide specific references or evidence from the papers mentioned to support\nyour classification.\n- Use assertive language to clearly convey your classification.\n\n42\nRadensky et al.\nPrompt\nNumber\nValidation Ac-\ncuracy\nPrompt Text\n6\n0.6\nYou are a classifier. Given a research idea and related papers, classify the idea\nas ‘novel’ or ‘not novel’.\n- **Contextual Understanding** : Consider whether the idea introduces a new\nmethodology, algorithm, or application that has not been previously discussed\nin the related papers.\n- **Feature Extraction** : Identify unique components of the idea, such as spe-\ncific algorithms, methodologies, or applications, and compare them with those\nmentioned in the related papers.\n-\n**Historical Data** : Evaluate the idea against existing algorithms and\nmethodologies discussed in the related papers to determine if it introduces\na new concept.\n- **Specificity in Language** : Provide a detailed explanation for your classifi-\ncation, mentioning specific similarities or differences between the idea and the\nrelated papers.\n- **Critical Analysis** : Critically assess the originality of the idea by compar-\ning it with existing solutions and discussing any incremental improvements or\nnovel aspects.\n- **Feedback Loop** : Your classifications will be reviewed by experts. Use their\nfeedback to refine your understanding and improve future classifications.\n- **Bias Reduction** : Consider both the possibility that the idea is novel and\nthat it is not novel. Provide evidence for your conclusion.\n- **Explainability** : Justify your classification by explaining why the idea is\nnovel or not novel, referencing specific elements from the related papers.\nRespond in two lines:\nReview: <Provide a concise review in no more than two sentences, based on\nthe comparison of the idea with related papers.> Classification: <novel or not\nnovel label according to your review/rationale>\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n43\nPrompt\nNumber\nValidation Ac-\ncuracy\nPrompt Text\n7\n0.3\nYou are a classifier. Given a research idea and related papers, classify the idea\nas ‘novel’ or ‘not novel’ based on the following criteria:\n1. **Definition of Novelty** : - A ‘novel’ idea introduces a unique, groundbreak-\ning concept, methodology, or significant improvement over existing work. - A\n‘not novel’ idea closely aligns with existing work without significant innovation.\n2. **Contextual Instructions** : - If the idea involves common methodologies\nor well-known techniques, explicitly mention these aspects in your review. -\nConsider the historical context and frequency of similar studies when making\nyour classification.\n3. **Comparative Analysis** : - Compare the proposed idea with existing sys-\ntems or technologies mentioned in the related papers. Highlight similarities to\njustify the classification.\n4. **Evidence and Examples** : - Provide specific examples or evidence from\nthe related papers that demonstrate the lack of novelty.\n5. **Structured Format** : - Use a structured format with sections such as\n’Introduction,’ ’Evaluation Criteria,’ ’Comparative Analysis,’ and ’Conclusion’\nto present your review.\nRespond in two lines:\nReview: <This is the review you will generate after comparing the idea with\nrelated papers.> Classification: <novel or not novel label according to your\nreview/rationale>\n8\n0\nYou are a classifier. Given a research idea and related papers, classify the idea\nas ‘novel’ or ‘not novel’. Respond in two lines:\n1. Review: Provide a detailed review that includes specific references to the\nrelated papers, highlighting similarities and differences. Include direct quotes\nor specific sections from the related papers that support your classification.\n2. Classification: Use precise terminology to classify the idea as ‘novel’ or ‘not\nnovel’ based on your review/rationale. Avoid vague terms and be specific in\nyour justification.\n\n44\nRadensky et al.\nPrompt\nNumber\nValidation Ac-\ncuracy\nPrompt Text\n9\n0.6\nYou are a classifier. Given a research idea and related papers, classify the idea\nas ‘novel’ or ‘not novel’.\nDefinition of Novel: An idea is ‘novel’ if it introduces a new concept, method-\nology, or significant improvement that is not already well-documented in the\nprovided papers.\nCriteria for Evaluation:\n1. Uniqueness of the approach.\n2. Combination of elements.\n3. Presence of similar frameworks in the literature.\nInstructions:\n1. Compare the proposed idea’s methodology, scope, and application with those\ndescribed in the related papers.\n2. Highlight specific aspects of the idea and compare them with the related\npapers.\n3. Provide a detailed review based on the comparison.\n4. Conclude with a classification of ‘novel’ or ‘not novel’ based on this compari-\nson.\nRespond in the following format: Review: <This is the review you will generate\nafter comparing the idea with related papers.> Classification: <novel or not\nnovel label according to your review/rationale>\n10\n0.6\nYou are a classifier. Given a research idea and related papers, classify the idea\nas ‘novel’ or ‘not novel’. A novel idea introduces a unique, groundbreaking\nconcept or approach not previously covered in the literature. A not novel idea\nreiterates or slightly modifies existing research. Consider the historical context\nand frequency of similar studies when making your classification.\nRespond in two lines: Review: <one-sentence review> Classification: <novel or\nnot novel>\nEnsure your response is concise and uses simple language. Avoid unnecessary\ndetails.\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n45\nPrompt\nNumber\nValidation Ac-\ncuracy\nPrompt Text\n11\n0.6\nYou are a classifier. Given a research idea and related papers, classify the\nidea as ‘novel’ or ‘not novel’. Respond with: 1. Review: Provide a concise\nreview in no more than two sentences, comparing the idea with related papers.\nEnsure your review includes a clear rationale for why the idea is classified as\n‘novel’ or ‘not novel’. Avoid using uncertain terms like ’appears’ or ’seems’ .\n2. Classification: Use the term ‘novel’ or ‘not novel’ consistently based on your\nreview.\nExample:\nReview: The proposed idea of developing a human-centric explainable AI system\nis novel because it uniquely combines explainable AI techniques with iterative\nimprovement through human feedback and predictive models. Classification:\nnovel\n12\n0.5\nYou are a classifier. Given a research idea and related papers, classify the idea\nas ‘novel’ or ‘not novel’. Respond in two lines:\nReview: <Provide a detailed review comparing the idea with related papers,\nincluding specific examples and reasons for your classification. Mention existing\ntools or research that cover similar capabilities.> Classification: <novel or not\nnovel label according to your review/rationale. Use the term ‘novel’ consistently\nin both your review and classification. Ensure your response is detailed yet\nconcise, avoiding unnecessary verbosity.>\nD\nEXPERIMENTAL SETUP\nD.1\nExperimental Setup of Different Prompting Approaches\nThe best-performing in-context setting used 15 novel and non-novel examples with seed 100. For the OpenReview\nexamples, the best setup involved 5 idea-review pairs. For DSPy we used 2 bootstrapped examples, and trained both\nDSPy and TextGRAD for 12 prompt iterations.\nD.2\nBaselines\nIn this section, we briefly describe the AI-Scientist and AI-Researcher novelty evaluation system. We compare only the\nnovelty prompts of these two approaches with Scideator, rather than the entire system, because the test set contains\nnovelty judgements based on a fixed set of the 10 most relevant papers for each idea. Since different retrieval methods\ncould introduce new papers, we standardize the paper set to ensure a fair comparison of the prompts alone.\nAI Scientist. AI Scientist [30] consists of both a paper reviewer and an idea novelty evaluator. In our analysis, we focus\nsolely on the idea novelty evaluator. Lu et al. [30]’s idea novelty evaluator operates in an iterative manner by querying\n\n46\nRadensky et al.\nan LLM to determine the novelty of a research idea. At each iteration, the LLM generates a query from the idea which\nin the next iteration is sent to Semantic Scholar to retrieve the most relevant papers for comparison. Based on this\ncomparison, the LLM provides a decision: if the idea is novel, it outputs \"decision made: novel,\" and the process stops,\nlabeling the idea as novel. Alternately, if there is substantial overlap with existing works, the LLM outputs \"decision\nmade: not novel,\" and the idea is marked as not novel. The process continues either until a decision is reached or until\nthe maximum number of iterations (default is 10). In cases where no clear decision is made, the tool defaults to labeling\nthe idea as \"not novel\".\nTo compare the effectiveness of the AI Scientist prompt and ensure that no variability is introduced by the retrieval\nstep in its novelty evaluation, we fixed and provided the most relevant papers. We evaluated the decisions made by\nthe AI Scientist novelty checker in two setups: (i) Single Iteration with Fixed Papers: where we performed only one\niteration with the most relevant papers, and, (ii) Maximum Iterations with Fixed Papers: where we allowed the model\nto continue up to a maximum number of iterations (default is 10), still using the fixed set of most relevant papers. In\nthe first setting, we observed that in 18 out of 32 cases, a decision was not made, defaulting to \"not novel\". For the\nremaining ideas, in 8 out of 32 cases the decision was not novel and in 6 out of 32 cases the decision was novel. In the\nsecond setting, where we allowed the model to iterate until it reached a decision, we found that all but 2 cases were\nconsidered novel, even though the test set includes 13 ideas labeled as \"not novel.\" Notably, those 2 instances were cases\nwhere the model defaulted to \"not novel\" rather than explicitly deciding so. We chose to report the result of the first\nsetting only, as it shows more diversity of \"not novel\" judgements.\nWe adapted our test set ideas to match the style required by AI Scientist’s novelty prompt. We employed a prompt\noptimized using Anthropic’s prompt generator for this style adaptation and also provided in-context examples of ideas\nfrom their GitHub repository.\nIn addition to evaluating the idea, their novelty checker also uses a code file, experiment.py, which links to open-\nsource codes relevant to the domain of the idea. For language-model-based ideas, it references the NanoGPT code18,\nwhile for diffusion-model-based ideas, it links to codes such as tiny-diffusion19. Since all expert-labeled examples in the\ntest and train sets are from the language domain, we use the NanoGPT code as the default experiment.py. We used the\ndefault LLM specified in their setup: \"gpt-4o-2024-08-06\".\nAI Researcher. AI Researcher [41] uses a Swiss-system tournament ranking to compare ideas pairwise, filtering out\nlower-ranked ideas. However, in our evaluation, we do not adopt this ranking system because our goal is to provide\na novelty classification for each idea in the test set. Instead, we compare AI Researcher’s standalone prompt, which\nevaluates each idea individually without tournament-style ranking.\nAI Researcher evaluates the novelty of an idea by comparing it to one paper at a time, unlike AI Scientist and\nScideator. For each comparison, the LLM decides whether the paper should be cited as similar work. If the research\nproblem and approach in the idea and the paper align, the LLM outputs \"Yes,\" recommending that the paper should be\ncited. If they differ, it outputs \"No,\" along with an explanation of the differences. If any comparison results in a \"Yes,\" the\nidea is considered not novel.\nWe adapted the style of our ideas to align with the format required by AI Researcher’s novelty prompt. We leveraged\nthe style adaptation prompt provided by them from their GitHub repository. Following their approach, we present\n18https://github.com/karpathy/nanoGPT, https://github.com/karpathy/nanoGPT/pull/254\n19https://github.com/tanelp/tiny-diffusion, https://github.com/lucidrains/ema-pytorch, https://www.research.autodesk.com/publications/same-stats-\ndifferent-graphs/\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n47\nresults for both \"gpt-4o\" and \"claude-3-5-sonnet-20240620\" in Table 3. In the qualitative examples, we showcase the\nresults of the better-performing model, gpt-4o.\nE\nPROMPTS FOR ANALOGOUS PAPER FACET FINDER\nE.1\nPrompt to extract facets from a paper title/abstract.\ndef promptTextToPurposeMechanism(texts):\nprompt = f\"\"\"\nTEXTS:\"\"\"\nfor t in range(0, len(texts)):\nprompt += f\"\"\"\nText { t+1}\n{texts[t]}\"\"\"\nprompt += f\"\"\"\nINSTRUCTIONS:\nYou are ScientistGPT, an intelligent assistant that helps researchers come up with coherent, novel, and useful research ideas.\nPresent the purpose, mechanism, and evaluation of each text above.\nThe purpose is the problem being addressed (e.g., to assist with writing scientific tweetorials, to answer questions over the scientific\nliterature).\n↩→\nThe mechanism is the proposed method to solve the problem (e.g., interactive system with LLM-based scaffolding, Retrieval-Augmented\nGeneration agent).\n↩→\nThe evaluation is the method used to determine how well the proposed solution solved the problem (e.g., lab user study, science QA\nbenchmarks).\n↩→\nFollow the rules below for generating each facet (purpose, mechanism, and evaluation):\n1. Broad enough so as to generalize to new research ideas but specific enough to be helpful in coming up with research ideas.\n2. Single short phrases only (no more than 7 words). If you cannot write the facet in a short phrase, it is too specific.\n3. No numbers unless they are part of a name (e.g., GPT-4, big 5 personality traits).\n4. No acronyms or abbreviations.\n5. If the text has more than one of the same type of facet, combine them into one.\nExamples of bad vs good purposes:\n- bad (too specific): to generate creative writing activities for third-grade English lessons --> good: to support elementary creative writing\n- bad (too broad): to support healthcare --> good: to provide clinical decision support\n- bad (more than one purpose that are uncombined): to improve engagement between content creators and audience, to decrease negative effects\nof social media --> good: to improve social media creator-audience interaction\n↩→\nExamples of bad vs good mechanisms:\n- bad (too specific, numbers that aren't part of a name, acronym): LLM chain-of-thought from gpt-3.5-turbo trained up to 11/06 with\ntemperature=0.7 --> good: chain-of-thought from large language models\n↩→\n- bad (too broad): recommendation system --> good: collaborative filtering\n- bad (too broad): machine learning --> good: k-nearest neighbors algorithm\n- bad (more than one mechanism that are uncombined): content-based AI explanations, social-based AI explanations --> good: AI explanations\ngenerated with different techniques\n↩→\nExamples of bad vs good evaluations:\n- bad (too specific): between-subjects 4x4 user study with 32 participants --> good: Wizard of Oz user study\n- bad (too broad): questionnaire --> good: NASA-TLX Index\n- bad (too broad): qualitative evaluation --> good: semi-structured interviews\nFollow the rules below for generating definitions of each facet.\n1. Should be up to 2 sentences.\n2. Replace proper nouns with their definitions.\n3. Replace jargon with their definitions.\n4. Write out acronyms.\n5. Should be self-contained. Do NOT include information that is beyond the definition of the facet.\n6. Do NOT reuse the words already in the facet.\nExamples of bad vs good definitions:\n- facet: longitudinal study.\n\n48\nRadensky et al.\nbad: a study that evaluates the tool Toolio over the course of a year --> good: a study that takes place over a long period of time extending\nat least multiple days\n↩→\n- facet: Toolio for creative writing.\nbad: Toolio implements SLM for generating creative writing --> good: A mixed-initiative tool that uses large language models to scaffold the\nprocess of writing creative short stories by implementing the Standard Learning Method\n↩→\n- facet: to help users better understand black-box models.\nbad: to help users better understand AI-Bot-360 --> good: to help users better understand how AI models work when their algorithm cannot be\nfully known\n↩→\nMake sure all information is faithful to the associated text.\nIt is very important that you follow the answer format provided below!\nFORMAT FOR ANSWER:\nText <number>\nPurpose: To <verb> <rest of purpose here>\nPurpose Definition: <purpose definition here>\nMechanism: <noun phrase mechanism here>\nMechanism Definition: <mechanism definition here>\nEvaluation: <noun phrase evaluation here>\nEvaluation Definition: <evaluation definition here>\nANSWER:\n\"\"\"\nreturn prompt\nE.2\nPrompt to retrieve facets from papers associated with an analogous query.\ndef promptFacetsFromQueryPapers(texts, query, type=\"\", query2=\"\", type2=\"\"):\nprompt = f\"\"\"\nTEXTS:\"\"\"\nfor t in range(0, len(texts)):\nprompt += f\"\"\"\nText { t+1}\n{texts[t]}\"\"\"\nprompt += f\"\"\"\nINSTRUCTIONS:\nYou are ScientistGPT, an intelligent assistant that helps researchers come up with coherent, novel, and useful research ideas.\nPresent the purpose, mechanism, and evaluation of each text above.\"\"\"\nif type:\nprompt += f\"\"\"\nThe {type} should be relevant to but not a copy of the following query: {query}.\"\"\"\nelse:\nprompt += f\"\"\"\nThe facets should be relevant to but not a copy of the following query: {query}.\"\"\"\nif type2:\nprompt += f\"\"\"\nThe {type2} should be relevant to but not a copy of the following query: {query2}.\"\"\"\nprompt += f\"\"\"\nThe purpose is the problem being addressed (e.g., to assist with writing scientific tweetorials, to answer questions over the scientific\nliterature).\n↩→\nThe mechanism is the proposed method to solve the problem (e.g., interactive system with LLM-based scaffolding, Retrieval-Augmented\nGeneration agent).\n↩→\nThe evaluation is the method used to determine how well the proposed solution solved the problem (e.g., lab user study, science QA\nbenchmarks).\n↩→\nFollow the rules below for generating each facet (purpose, mechanism, and evaluation):\n1. Broad enough so as to generalize to new research ideas but specific enough to be helpful in coming up with research ideas.\n2. Single short phrases only (no more than 7 words). If you cannot write the facet in a short phrase, it is too specific.\n3. No numbers unless they are part of a name (e.g., GPT-4, big 5 personality traits).\n4. No acronyms or abbreviations.\n5. If the text has more than one of the same type of facet, combine them into one.\"\"\"\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n49\nif type:\nprompt += f\"\"\"\n6. The {type} should be relevant to but not a copy of the following query: {query}.\"\"\"\nelse:\nprompt += f\"\"\"\n6. The facets should be relevant to but not a copy of the following query: {query}.\"\"\"\nif type2:\nprompt += f\"\"\"\n7. The {type2} should be relevant to but not a copy of the following query: {query2}.\"\"\"\nprompt += f\"\"\"\nExamples of bad vs good purposes:\n- bad (too specific): to generate creative writing activities for third-grade English lessons --> good: to support elementary creative writing\n- bad (too broad): to support healthcare --> good: to provide clinical decision support\n- bad (more than one purpose that are uncombined): to improve engagement between content creators and audience, to decrease negative effects\nof social media --> good: to improve social media creator-audience interaction\n↩→\nExamples of bad vs good mechanisms:\n- bad (too specific, numbers that aren't part of a name, acronym): LLM chain-of-thought from gpt-3.5-turbo trained up to 11/06 with\ntemperature=0.7 --> good: chain-of-thought from large language models\n↩→\n- bad (too broad): recommendation system --> good: collaborative filtering\n- bad (too broad): machine learning --> good: k-nearest neighbors algorithm\n- bad (more than one mechanism that are uncombined): content-based AI explanations, social-based AI explanations --> good: AI explanations\ngenerated with different techniques\n↩→\nExamples of bad vs good evaluations:\n- bad (too specific): between-subjects 4x4 user study with 32 participants --> good: Wizard of Oz user study\n- bad (too broad): questionnaire --> good: NASA-TLX Index\n- bad (too broad): qualitative evaluation --> good: semi-structured interviews\nFollow the rules below for generating definitions of each facet.\n1. Should be up to 2 sentences.\n2. Replace proper nouns with their definitions.\n3. Replace jargon with their definitions.\n4. Write out acronyms.\n5. Should be self-contained. Do NOT include information that is beyond the definition of the facet.\n6. Do NOT reuse the words already in the facet.\nExamples of bad vs good definitions:\n- facet: longitudinal study.\nbad: a study that evaluates the tool Toolio over the course of a year --> good: a study that takes place over a long period of time extending\nat least multiple days\n↩→\n- facet: Toolio for creative writing.\nbad: Toolio implements SLM for generating creative writing --> good: A mixed-initiative tool that uses large language models to scaffold the\nprocess of writing creative short stories by implementing the Standard Learning Method\n↩→\n- facet: to help users better understand black-box models.\nbad: to help users better understand AI-Bot-360 --> good: to help users better understand how AI models work when their algorithm cannot be\nfully known\n↩→\nMake sure all information is faithful to the associated text.\nIt is important that you follow the answer format provided below!\"\"\"\nprompt += f\"\"\"\nFORMAT FOR ANSWER:\nText <number>\nPurpose: To <verb> <rest of purpose here>\nPurpose Definition: <purpose definition here>\nMechanism: <noun phrase mechanism here>\nMechanism Definition: <mechanism definition here>\nEvaluation: <noun phrase evaluation here>\nEvaluation Definition: <evaluation definition here>\nANSWER:\n\n50\nRadensky et al.\n\"\"\"\nreturn prompt\nE.3\nPrompt to obtain queries for analogous papers from same topic, same subarea, and different subarea\ncompared to input paper.\ndef promptAnalogyQueries(purpose, mechanism, previousQueries):\nprompt = f\"\"\"\nINITIAL PURPOSE/MECHANISM:\nPurpose: {purpose}\nMechanism: {mechanism}\"\"\"\nif previousQueries:\nprompt += f\"\"\"\nPREVIOUS QUERIES:\"\"\"\nfor q in previousQueries:\nif q != \"start-input\" and q != \"none\":\nprompt += f\"\"\"\n{q}\"\"\"\nprompt += f\"\"\"\nINSTRUCTIONS:\nYou are ScientistGPT, an intelligent assistant that helps researchers come up with coherent, novel, and useful research ideas.\nWhat are some analogous purposes and mechanisms to the initial purpose and mechanism above?\nProvide one analogous purpose/mechanism that is from the same topic of computer science research, one that is from the same subarea but\ndifferent topic of computer science research, and one that is from a different subarea of computer science research.\n↩→\nThe relationship between the initial purpose and mechanism should be very similar to the relationship between the analogous purpose and\nmechanism.\n↩→\nAlso, provide a query (up to 5 words) for finding research papers relevant to each analogous purpose/mechanism.\"\"\"\nif previousQueries:\nprompt += f\"\"\"\nMake sure you come up with new analogous purposes/mechanisms that are NOT covered by the previous queries above.\"\"\"\nprompt += f\"\"\"\nFORMAT FOR ANSWER:\n1. From same topic of computer science research:\nSame Topic: <topic of initial purpose/mechanism here>\nAnalogy: {purpose} is to {mechanism} as <analogous purpose here> is to <analogous mechanism here> because both relationships involve\n<specific common relationship description here>.\n↩→\nPurpose: <analogous purpose here>\nMechanism: <analogous mechanism here>\nQuery for Relevant Research Papers: <query combining analogous purpose and mechanism here>\n2. From same subarea of computer science research, but different topic of computer science research:\nSame Subarea: <subarea of initial purpose/mechanism here>\nAnalogy: {purpose} is to {mechanism} as <analogous purpose here> is to <analogous mechanism here> because both relationships involve\n<specific common relationship description here>.\n↩→\nPurpose: <analogous purpose here>\nMechanism: <analogous mechanism here>\nQuery for Relevant Research Papers: <query combining analogous purpose and mechanism here>\n3. From different subarea of computer science research:\nDifferent Subarea <different subarea from initial purpose/mechanism here>\nAnalogy: {purpose} is to {mechanism} as <analogous purpose here> is to <analogous mechanism here> because both relationships involve\n<specific common relationship description here>.\n↩→\nPurpose: <analogous purpose here>\nMechanism: <analogous mechanism here>\nQuery for Relevant Research Papers: <query combining analogous purpose and mechanism here>\nANSWER:\n\"\"\"\nreturn prompt\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n51\nE.4\nPrompt to shorten a query for papers in the event that it is too long to retrieve 4 relevant papers.\ndef promptShortenQuery(query):\nprompt = f\"\"\"\nQUERY: {query}\nINSTRUCTIONS:\nYou are ScientistGPT, an intelligent assistant that helps researchers come up with coherent, novel, and useful research ideas.\nThe query above is too specific to retrieve any research papers.\nPlease provide a simpler and shorter version of the query to find relevant research papers.\nIf you must lose some meaning when shortening the query, prioritize the most important information.\nANSWER:\n\"\"\"\nreturn prompt\n\\end{verbatim}\n\\subsection{Prompt to summarize input and near-1 analogous papers to obtain related works' summary.}\n\\begin{verbatim}\ndef promptSummarizePapers(facets, papers, corpus_ids):\nprompt = f\"\"\"\nPAPERS:\"\"\"\nindex = 0\nfor t in corpus_ids:\nindex += 1\nprompt += f\"\"\"\nPaper {index}:\nTitle: {papers[t][\"title\"]}\nAbstract: {papers[t][\"abstract\"]}\nPurpose Text: {facets[papers[t][\"purpose\"]][\"text\"]}\nPurpose ID: {papers[t][\"purpose\"]}\nMechanism Text: {facets[papers[t][\"mechanism\"]][\"text\"]}\nMechanism ID: {papers[t][\"mechanism\"]}\nEvaluation Text: {facets[papers[t][\"evaluation\"]][\"text\"]}\nEvaluation ID: {papers[t][\"evaluation\"]}\"\"\"\nprompt += f\"\"\"\nINSTRUCTIONS:\nYou are ScientistGPT, an intelligent assistant that helps researchers come up with coherent, novel, and useful research ideas.\nSummarize the prior work above in around 300 words.\nDo not summarize individual papers one-by-one. Instead, summarize their contributions as a whole.\nThat way, you will know what has already been done in research and will not propose similar ideas.\nInstead, you will come up with novel ideas that build upon the designated papers.\nANSWER:\n\"\"\"\nreturn prompt\nF\nPROMPTS FOR FACETED IDEA GENERATOR\nF.1\nPrompt to obtain ideas using Initial or No-P-no-M method.\ndef promptInitialAnalogyIdeas(\npapers, facets, designated_papers, analogous_papers, query, summary, set_eval=[]\n):\nprompt = f\"\"\"\nSUMMARY OF PRIOR WORK:\n{summary}\nDESIGNATED PAPER:\"\"\"\nindex = 0\nfor t in designated_papers:\nindex += 1\nprompt += f\"\"\"\n\n52\nRadensky et al.\nPaper {index}:\nTitle: {papers[t][\"title\"]}\nAbstract: {papers[t][\"abstract\"]}\nPurpose Text: {facets[papers[t][\"purpose\"]][\"text\"]}\nPurpose ID: {papers[t][\"purpose\"]}\nMechanism Text: {facets[papers[t][\"mechanism\"]][\"text\"]}\nMechanism ID: {papers[t][\"mechanism\"]}\"\"\"\nif not set_eval:\nprompt += f\"\"\"\nEvaluation Text: {facets[papers[t][\"evaluation\"]][\"text\"]}\nEvaluation ID: {papers[t][\"evaluation\"]}\"\"\"\nprompt += f\"\"\"\nANALOGOUS PAPERS RELATED TO {query}:\"\"\"\nindex = 0\nfor t in analogous_papers:\nindex += 1\nprompt += f\"\"\"\nPaper {index}:\nTitle: {papers[t][\"title\"]}\nAbstract: {papers[t][\"abstract\"]}\nPurpose Text: {facets[papers[t][\"purpose\"]][\"text\"]}\nPurpose ID: {papers[t][\"purpose\"]}\nMechanism Text: {facets[papers[t][\"mechanism\"]][\"text\"]}\nMechanism ID: {papers[t][\"mechanism\"]}\"\"\"\nif not set_eval:\nprompt += f\"\"\"\nEvaluation Text: {facets[papers[t][\"evaluation\"]][\"text\"]}\nEvaluation ID: {papers[t][\"evaluation\"]}\"\"\"\nif set_eval:\nprompt += f\"\"\"\nEVALUATION OPTIONS:\"\"\"\nind = 0\nfor eval in set_eval:\nprompt += f\"\"\"\n{ind}. Evaluation Text: {eval[1]}\nEvaluation ID: {eval[2]}\"\"\"\nind += 1\nprompt += f\"\"\"\nINSTRUCTIONS:\nYou are ScientistGPT, an intelligent assistant that helps researchers come up with coherent, novel, useful research ideas.\nA novel research idea is not only rare but also ingenious, imaginative, or surprising.\nA useful research idea applies to the stated problem and is effective at solving the problem.\nFirst, read the summary of prior work above.\nThat way, you will know what has already been done in research and will not propose similar ideas.\nInstead, you will come up with novel ideas that build upon prior work.\nSecond, come up with two novel, useful analogies between the designated paper's purpose/mechanism and an analogous paper's\npurpose/mechanism.\"\"\"\n↩→\nif set_eval:\nprompt += f\"\"\"\nFor one analogy, create a coherent, novel, and useful future work idea that combines the purpose from the analogous paper, the mechanism\nfrom the designated paper, and one of the evaluation options above.\n↩→\nFor the other analogy, create a coherent, novel, and useful future work idea that combines the purpose from the designated paper, the\nmechanism from the analogous paper, and one of the evaluation options above.\"\"\"\n↩→\nelse:\nprompt += f\"\"\"\nFor one analogy, create a coherent, novel, and useful future work idea that combines the purpose from the analogous paper, the mechanism\nfrom the designated paper, and the evaluation from either paper.\n↩→\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n53\nFor the other analogy, create a coherent, novel, and useful future work idea that combines the purpose from the designated paper, the\nmechanism from the analogous paper, and the evaluation from either paper.\"\"\"\n↩→\nprompt += f\"\"\"\nExample Analogy: The purpose \"to make a lighter, more compact solar array\" is to the mechanism \"lighter and more compact hinges and springs\"\nas the purpose \"to create folded structures\" is to the mechanism \"origami techniques\" because both relationships involve utilizing a\ntechnique that reduces the volume of an object in order to create a more compact object.\n↩→\n↩→\nMake the ideas as specific as possible while describing each in 100 to 150 words.\nThe ideas should be self-contained and not require researchers to have read the provided papers. For example, saying \"Use Tool X\" is not\nself-contained because the researcher might not have read the relevant paper to know what Tool X is. Instead, you could say \"Use a tool\nto do Y.\"\n↩→\n↩→\nThe ideas should be novel and should NOT be similar to the ideas already mentioned in the designated papers.\nAlso, bold and highlight the phrases that relate to the facets in each idea.\nThere should be at least one bolded phrase per facet in each idea.\nUse HTML tags to bold the phrases and highlight them in the assigned color: purpose=#F0FFF0, mechanism=#FFFACD, evaluation=#E6E6FA.\nFORMAT FOR IDEA WITH BOLDED/HIGHLIGHTED FACETS:\n<non-facet text> <b style=\"background-color:#FFFACD;\"><mechanism text></b> <non-facet text> <b style=\"background-color:#F0FFF0;\"><purpose\ntext></b> <non-facet text>. <non-facet text> <b style=\"background-color:#E6E6FA;\"><evaluation text></b>.\n↩→\nThe purposes/mechanisms/evaluations IDs should be written VERBATIM from what is provided.\nDo NOT make up facet IDs that are not explicitly given above.\nIt is important that you follow the answer format provided below!\nFORMAT FOR ANSWER:\nAnalogy: The designated purpose <purpose text from designated paper here> is to the designated mechanism <mechanism text from designated\npaper here> as the analagous purpose <purpose text from selected analogous paper here> is to the analogous mechanism <mechanism text\nfrom selected analogous paper here> because both relationships involve <common relationship description here>.\n↩→\n↩→\nWhy Analogy is Novel Compared to Prior Work: <sentence on why analogy is novel here>\nWhy Analogy is Useful: <sentence on why analogy is useful here>\nPurpose Text: <purpose text from selected analogous paper here>\nPurpose ID: <purpose ID from selected analogous paper here>\nMechanism Text: <mechanism text from designated paper here>\nMechanism ID: <mechanism ID from designated paper here>\"\"\"\nif not set_eval:\nprompt += f\"\"\"\nEvaluation Text: <selected evaluation from either paper here>\nEvaluation ID: <selected evaluation ID from either paper here>\"\"\"\nelse:\nprompt += f\"\"\"\nEvaluation Text: <selected evaluation option text here>\nEvaluation ID: <selected evaluation option ID here>\"\"\"\nprompt += f\"\"\"\nFuture Work Idea: <idea with bolded/highlighted facets here>\nAnalogy: The designated purpose <purpose text from designated paper here> is to the designated mechanism <mechanism text from designated\npaper here> as the analagous purpose <purpose text from selected analogous paper here> is to the analogous mechanism <mechanism text\nfrom selected analogous paper here> because both relationships involve <common relationship description here>.\n↩→\n↩→\nWhy Analogy is Novel Compared to Prior Work: <sentence on why analogy is novel here>\nWhy Analogy is Useful: <sentence on why analogy is useful here>\nPurpose Text: <purpose text from designated paper here>\nPurpose ID: <purpose ID from designated paper here>\nMechanism Text: <mechanism text from selected analogous paper here>\nMechanism ID: <mechanism ID from selected analogous paper here>\"\"\"\nif not set_eval:\nprompt += f\"\"\"\nEvaluation Text: <selected evaluation from either paper here>\nEvaluation ID: <selected evaluation ID here>\"\"\"\nelse:\nprompt += f\"\"\"\nEvaluation Text: <selected evaluation option text here>\nEvaluation ID: <selected evaluation option ID here>\"\"\"\nprompt += f\"\"\"\n\n54\nRadensky et al.\nFuture Work Idea: <idea with bolded/highlighted facets here>\nANSWER:\n\"\"\"\nreturn prompt\nF.2\nPrompt to obtain relevant facet recommendations when a facet is selected.\nef promptRelevantFacets(\npapers,\nfacets,\nsummary,\ndesignated_papers,\nanalogous_papers,\nrelevant_purpose,\nset_eval,\nnumber=2,\n):\nprompt = f\"\"\"\nSUMMARY OF PRIOR WORK:\n{summary}\nDESIGNATED PAPERS:\"\"\"\nindex = 0\nfor t in designated_papers:\nindex += 1\nif t in papers:\nprompt += f\"\"\"\nPaper {index}:\"\"\"\nif \"title\" in papers[t]:\nprompt += f\"\"\"\nTitle: {papers[t][\"title\"]}\nAbstract: {papers[t][\"abstract\"]}\"\"\"\nprompt += f\"\"\"\nPurpose Text: {facets[papers[t][\"purpose\"]][\"text\"]}\nPurpose ID: {papers[t][\"purpose\"]}\nMechanism Text: {facets[papers[t][\"mechanism\"]][\"text\"]}\nMechanism ID: {papers[t][\"mechanism\"]}\"\"\"\nprompt += f\"\"\"\nANALOGOUS PAPERS:\"\"\"\nindex = 0\nfor t in analogous_papers:\nindex += 1\nprompt += f\"\"\"\nPaper {index}:\nTitle: {papers[t][\"title\"]}\nAbstract: {papers[t][\"abstract\"]}\nPurpose Text: {facets[papers[t][\"purpose\"]][\"text\"]}\nPurpose ID: {papers[t][\"purpose\"]}\nMechanism Text: {facets[papers[t][\"mechanism\"]][\"text\"]}\nMechanism ID: {papers[t][\"mechanism\"]}\"\"\"\nif not set_eval:\nprompt += f\"\"\"\nEvaluation Text: {facets[papers[t][\"evaluation\"]][\"text\"]}\nEvaluation ID: {papers[t][\"evaluation\"]}\"\"\"\nprompt += f\"\"\"\nINSTRUCTIONS:\nYou are ScientistGPT, an intelligent assistant that helps researchers come up with coherent, novel, and useful research ideas.\nA novel research idea is not only rare but also ingenious, imaginative, or surprising.\nA useful research idea applies to the stated problem and is effective at solving the problem.\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n55\nFirst, read the summary of prior work above.\nThat way, you will know what has already been done in research and will not propose similar ideas.\nInstead, you will come up with novel ideas that build upon prior work.\nSecond, come up with {number} novel, useful analogies between the purpose/mechanism from a designated paper and the purpose/mechanism from\nan analogous paper.\"\"\"\n↩→\nif relevant_purpose:\nif set_eval:\nprompt += f\"\"\"\nFor each analogy, note the purpose from the designated paper and the mechanism from the analogy paper.\"\"\"\nelse:\nprompt += f\"\"\"\nFor each analogy, note the purpose from the designated paper and the mechanism and evaluation from the analogy paper.\"\"\"\nelse:\nif set_eval:\nprompt += f\"\"\"\nFor each analogy, note the mechanism from the designated paper and the purpose from the analogy paper.\"\"\"\nelse:\nprompt += f\"\"\"\nFor each analogy, note the mechanism from the designated paper and the purpose and evaluation from the analogy paper.\"\"\"\nprompt += f\"\"\"\nExample Analogy: The designated purpose \"to make a lighter, more compact solar array\" is to the designated mechanism \"lighter and more\ncompact hinges and springs\" as the selected purpose \"to create folded structures\" is to the selected mechanism \"origami techniques\"\nbecause both relationships involve utilizing a technique that reduces the volume of an object in order to create a more compact object.\n↩→\n↩→\nThe purposes/mechanisms/evaluations IDs should be written VERBATIM from what is provided.\nDo NOT make up facet IDs that are not explicitly given above.\nIt is important that you follow the answer format provided below!\nFORMAT FOR ANSWER:\"\"\"\nif relevant_purpose:\nprompt += f\"\"\"\n[number]. Analogy: The designated purpose <purpose text from selected designated paper here> is to the designated mechanism <mechanism text\nfrom selected designated paper here> as the analagous purpose <purpose text from selected analogous paper here> is to the analagous\nmechanism <mechanism text from selected analogous paper here> because both relationships involve <common relationship description here>.\n↩→\n↩→\nPurpose Text: <purpose text from selected designated paper here>\nPurpose ID: <purpose ID from selected designated paper here>\"\"\"\nelse:\nprompt += f\"\"\"\n[number]. Analogy: The designated purpose <purpose text from selected designated paper here> is to the designated mechanism <mechanism text\nfrom selected designated paper here> as the analagous purpose <purpose text from selected analogous paper here> is to the analagous\nmechanism <mechanism text from selected analogous paper here> because both relationships involve <common relationship description here>.\n↩→\n↩→\nPurpose Text: <purpose text from selected analogous paper here>\nPurpose ID: <purpose ID from selected analogous paper here>\"\"\"\nif not relevant_purpose:\nprompt += f\"\"\"\nMechanism Text: <mechanism text from selected designated paper here>\nMechanism ID: <mechanism ID from selected designated paper here>\"\"\"\nelse:\nprompt += f\"\"\"\nMechanism Text: <mechanism text from selected analogous paper here>\nMechanism ID: <mechanism ID from selected analogous paper here>\"\"\"\nif not set_eval:\nprompt += f\"\"\"\nEvaluation Text: <evaluation text from selected analogous paper here>\nEvaluation ID: <evaluation ID from selected analogous paper here>\"\"\"\nprompt += f\"\"\"\nANSWER:\n\"\"\"\nreturn prompt\n\n56\nRadensky et al.\nF.3\nPrompt to obtain ideas using P-or-M method.\ndef promptFillAnalogyIdeas(\npapers,\nfacets,\nsummary,\ndesignated_papers,\nanalogous_papers,\nrelevant_purposes,\nrelevant_mechanisms,\nset_eval,\nnumber=8,\n):\nprompt = f\"\"\"\nSUMMARY OF PRIOR WORK:\n{summary}\nDESIGNATED PAPERS:\"\"\"\nindex = 0\nfor t in designated_papers:\nindex += 1\nprompt += f\"\"\"\nPaper {index}:\"\"\"\nif t in papers:\nif \"title\" in papers[t]:\nprompt += f\"\"\"\nTitle: {papers[t][\"title\"]}\nAbstract: {papers[t][\"abstract\"]}\"\"\"\nprompt += f\"\"\"\nPurpose Text: {facets[papers[t][\"purpose\"]][\"text\"]}\nPurpose ID: {papers[t][\"purpose\"]}\nMechanism Text: {facets[papers[t][\"mechanism\"]][\"text\"]}\nMechanism ID: {papers[t][\"mechanism\"]}\"\"\"\nif not set_eval:\nprompt += f\"\"\"\nEvaluation Text: {facets[papers[t][\"evaluation\"]][\"text\"]}\nEvaluation ID: {papers[t][\"evaluation\"]}\"\"\"\nprompt += f\"\"\"\nANALOGOUS PAPERS:\"\"\"\nindex = 0\nfor t in analogous_papers:\nindex += 1\nprompt += f\"\"\"\nPaper {index}:\nTitle: {papers[t][\"title\"]}\nAbstract: {papers[t][\"abstract\"]}\nPurpose Text: {facets[papers[t][\"purpose\"]][\"text\"]}\nPurpose ID: {papers[t][\"purpose\"]}\nMechanism Text: {facets[papers[t][\"mechanism\"]][\"text\"]}\nMechanism ID: {papers[t][\"mechanism\"]}\"\"\"\nif not set_eval:\nprompt += f\"\"\"\nEvaluation Text: {facets[papers[t][\"evaluation\"]][\"text\"]}\nEvaluation ID: {papers[t][\"evaluation\"]}\"\"\"\nif set_eval:\nprompt += f\"\"\"\nEVALUATION OPTIONS:\"\"\"\nind = 0\nfor eval in set_eval:\nprompt += f\"\"\"\n{ind}. Evaluation Text: {eval[1]}\nEvaluation ID: {eval[2]}\"\"\"\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n57\nind += 1\nprompt += f\"\"\"\nINSTRUCTIONS:\nYou are ScientistGPT, an intelligent assistant that helps researchers come up with coherent, novel, and useful research ideas.\nA novel research idea is not only rare but also ingenious, imaginative, or surprising.\nA useful research idea applies to the stated problem and is effective at solving the problem.\nFirst, read the summary of prior work above.\nThat way, you will know what has already been done in research and will not propose similar ideas.\nInstead, you will come up with novel ideas that build upon prior work.\nSecond, come up with {number} novel, useful analogies between the purpose/mechanism from a designated paper and the purpose/mechanism from\nan analogous paper.\"\"\"\n↩→\nif relevant_purposes:\nif set_eval:\nprompt += f\"\"\"\nFor each analogy, create a novel, useful future work idea that combines the purpose from a designated paper with the mechanism from an\nanalogous paper.\n↩→\nThe idea should also include one of the evaluation options above.\"\"\"\nelse:\nprompt += f\"\"\"\nFor each analogy, create a novel, useful future work idea that combines the purpose from a designated paper, the mechanism from an analogous\npaper, and the evaluation from one of those two papers.\"\"\"\n↩→\nelse:\nif set_eval:\nprompt += f\"\"\"\nFor each analogy, create a novel, useful future work idea that combines the mechanism from a designated paper with the purpose from an\nanalogous paper.\n↩→\nThe idea should also include one of the evaluation options above.\"\"\"\nelse:\nprompt += f\"\"\"\nFor each analogy, create a novel, useful future work idea that combines the mechanism from a designated paper, the purpose from an analogous\npaper, and the evaluation from one of those two papers.\"\"\"\n↩→\nprompt += f\"\"\"\nExample Analogy: The designated purpose \"to make a lighter, more compact solar array\" is to the designated mechanism \"lighter and more\ncompact hinges and springs\" as the selected purpose \"to create folded structures\" is to the selected mechanism \"origami techniques\"\nbecause both relationships involve utilizing a technique that reduces the volume of an object in order to create a more compact object.\n↩→\n↩→\nMake the ideas as specific as possible while describing each in 100 to 150 words.\nThe ideas should NOT be mentioned in a paper already.\nThe idea should be self-contained and not require researchers to have read the provided papers. For example, saying \"Use Tool X\" is not\nself-contained because the researcher might not have read the relevant paper to know what Tool X is. Instead, you could say \"Use a tool\nto do Y.\"\n↩→\n↩→\nThe idea should be novel and should NOT be similar to the ideas already mentioned in the summary of prior work.\nAlso, bold and highlight the phrases that relate to the facets in the idea.\nThere should be at least one bolded phrase per facet.\nUse HTML tags to bold the phrases and highlight them in the assigned color: purpose=#F0FFF0, mechanism=#FFFACD, evaluation=#E6E6FA.\nEXAMPLE IDEA WITH BOLDED/HIGHLIGHTED FACETS:\nCreate a <b style=\"background-color:#FFFACD;\">Retrieval-Augmented Generation model</b> designed <b style=\"background-color:#F0FFF0;\">to\nanswer questions from full-text scientific articles</b>, thereby improving accuracy and providing source provenance. The agent will be\nevaluated on <b style=\"background-color:#E6E6FA;\">science QA benchmarks</b>.\n↩→\n↩→\nThe purposes/mechanisms/evaluations IDs should be written VERBATIM from what is provided above.\nDo NOT make up facet IDs that are not explicitly given above.\nIt is important that you follow the answer format provided below!\nFORMAT FOR ANSWER:\n[number]. Analogy: The designated purpose <purpose text from selected designated paper here> is to the designated mechanism <mechanism text\nfrom selected designated paper here> as the analagous purpose <purpose text from selected analagous paper here> is to the analogous\nmechanism <mechanism text from selected analagous paper here> because both relationships involve <common relationship description\nhere>.\"\"\"\n↩→\n↩→\n↩→\n\n58\nRadensky et al.\nif relevant_purposes:\nprompt += f\"\"\"\nPurpose Text: <purpose text from selected designated paper here>\nPurpose ID: <purpose ID from selected designated paper here>\nMechanism Text: <mechanism text from selected analogous paper here>\nMechanism ID: <mechanism ID from selected analogous paper here>\"\"\"\nelse:\nprompt += f\"\"\"\nPurpose Text: <purpose text from selected analogous paper here>\nPurpose ID: <purpose ID from selected analogous paper here>\nMechanism Text: <mechanism text from selected designated paper here>\nMechanism ID: <mechanism ID from selected designated paper here>\"\"\"\nif set_eval:\nprompt += f\"\"\"\nEvaluation Text: <selected evaluation option text here>\nEvaluation ID: <selected evaluation option ID here>\n\"\"\"\nelse:\nprompt += f\"\"\"\nEvaluation Text: <evaluation text from same paper as purpose or mechanism here>\nEvaluation ID: <evaluation ID from same paper as purpose or mechanism here>\n\"\"\"\nprompt += f\"\"\"\nFuture Work Idea: <idea with bolded/highlighted selected facets here>\nWhy Idea is Novel Compared to Prior Work: <sentence on why idea is novel here>\nWhy Idea is Useful Compared to Prior Work: <sentence on why idea is useful here>\nANSWER:\n\"\"\"\nreturn prompt\nF.4\nPrompt to obtain ideas using P-and-M method.\ndef promptFacetsToIdeas(summary, papers, facets, number=8):\nprompt = f\"\"\"\nSUMMARY OF PRIOR WORK:\n{summary}\nFACET OPTIONS TO INCLUDE IN IDEAS (AND THEIR CORRESPONDING PAPERS):\"\"\"\nind = 1\nfor key in facets:\nprompt += f\"\"\"\nPaper {ind}\"\"\"\nif key in papers:\nif \"title\" in papers[key]:\nprompt += f\"\"\"\nTitle: {papers[key][\"title\"]}\nAbstract: {papers[key][\"abstract\"]}\"\"\"\nfor facet in facets[key]:\nprompt += f\"\"\"\n{facet[\"type\"]} Text: {facet[\"text\"]}\n{facet[\"type\"]} ID: {facet[\"id\"]}\"\"\"\nind += 1\nprompt += f\"\"\"\nINSTRUCTIONS:\nYou are ScientistGPT, an intelligent assistant that helps researchers come up with coherent, novel, and useful research ideas.\nA novel research idea is not only rare but also ingenious, imaginative, or surprising.\nA useful research idea applies to the stated problem and is effective at solving the problem.\nFirst, read the summary of prior work above.\nThat way, you will know what has already been done in research and will not propose similar ideas.\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n59\nInstead, you will come up with novel ideas that build upon prior work.\nSecond, come up with {number} different future work ideas that utilize one of the purposes, one of the mechanisms, and one of the\nevaluations provided above.\n↩→\nThe ideas should NOT be covered in prior work already.\nThe ideas should be self-contained and not require researchers to have read the provided papers. For example, saying \"Use Tool X\" is not\nself-contained because the researcher might not have read the relevant paper to know what Tool X is. Instead, you could say \"Use a tool\nto do Y.\"\n↩→\n↩→\nBe as specific as possible, but the ideas should be described in 100 to 150 words.\nAlso, bold and highlight the phrases that relate to the facets in the idea.\nThere should be at least one bolded phrase per facet in the idea.\nUse HTML tags to bold the phrases and highlight them in the assigned color: purpose=#F0FFF0, mechanism=#FFFACD, evaluation=#E6E6FA.\nEXAMPLE IDEA WITH BOLDED/HIGHLIGHTED FACETS:\nCreate a <b style=\"background-color:#FFFACD;\">Retrieval-Augmented Generation model</b> designed <b style=\"background-color:#F0FFF0;\">to\nanswer questions from full-text scientific articles</b>, thereby improving accuracy and providing source provenance. The agent will be\nevaluated on <b style=\"background-color:#E6E6FA;\">science QA benchmarks</b>.\n↩→\n↩→\nThe purposes/mechanisms/evaluations IDs should be written VERBATIM from what is provided.\nDo NOT make up facet IDs that are not explicitly given above.\nIt is very important that you follow the answer format provided below!\nFORMAT FOR ANSWER:\n[number].\nPurpose Text: <selected purpose text here>\nPurpose ID: <selected purpose ID here>\nMechanism Text: <selected mechanism text here>\nMechanism ID: <selected mechanism ID here>\nEvaluation Text: <selected evaluation text here>\nEvaluation ID: <selected evaluation ID here>\nFuture Work Idea: <future work idea utilizing facets here>\nWhy Idea is Novel Compared to Prior Work: <sentence on why idea is novel here>\nWhy Idea is Useful Compared to Prior Work: <sentence on why idea is useful here>\nANSWER:\n\"\"\"\nreturn prompt\nG\nPROMPTS FOR IDEA NOVELTY CHECKER\nG.1\nPrompt to assess idea novelty.\ndef promptNoveltyChecker(idea, similar_documents, incontext_part):\nrelevant_papers = []\nfor i, (_, row) in enumerate(similar_documents.iterrows()):\nrelevant_papers.append(\n{\n\"role\": \"user\",\n\"content\": f\"Paper ID [{i}]: Title: {row['title']}. Abstract: {row['abstract']}\",\n}\n)\nprompt = [\n{\n\"role\": \"system\",\n\"content\": \"You are ReviewerGPT, an intelligent assistant that helps researchers evaluate the novelty of their ideas.\",\n},\n{\n\"role\": \"user\",\n\"content\": f\"\"\"You are given some papers similar to the proposed idea. Your task is to evaluate the idea's novelty using the related\npapers only.\n↩→\nTypes of novelty categories:\n\n60\nRadensky et al.\n- Not Novel: The idea closely replicates existing work with minimal or no new contributions.\n- Novel:\n- The idea introduces new concepts or approaches that are not common in existing literature.\n- The idea uniquely combines concepts from existing papers, but this combination does not occur in any related papers.\n- A new application with same approach is also novel.\nInstructions:\n- Use the example reviews below to write a review for the provided idea by comparing it to the related papers.\n- Don't assume any prior knowledge about the idea.\n- When referencing a related paper, then use paper id in the review, mention it in this format: [5]. The paper ID is present\nbetween Paper ID [<paper_id>]: Title.\n↩→\n- After reviewing, classify the idea into one of this category: novel or not novel.\n- Make sure the generated review follows the format in example reviews provided below.\n- The review should be concise - around 60 to 100 words.\n{incontext_part}\nOutput Format:\n- Class: [novel / not novel]\n- Review: The idea is [novel / not novel] different because...\n\"\"\",\n},\n{\"role\": \"assistant\", \"content\": \"Sure, please provide the IDEA.\"},\n{\"role\": \"user\", \"content\": f\"Here is the idea: {idea}\"},\n{\"role\": \"assistant\", \"content\": \"Okay, now provide the related papers.\"},\n]\nprompt.extend(relevant_papers)\nreturn prompt\nG.2\nPrompt to extract keywords from an idea to use as queries for related papers.\ndef get_keywords(input_string):\nprompt = [\n{\n\"role\": \"system\",\n\"content\": \"You are KeywordGPT, an intelligent assistant that can identify relevant keywords for searching documents related to the\nIDEA.\",\n↩→\n},\n{\n\"role\": \"user\",\n\"content\": \"\"\"Your task is to extract keywords from the provided IDEA that can be queried on a search engine like semantic for\nfinding similar research papers, which match in main purpose of the idea.\n↩→\nDon't give vague keywords like machine learning or computer science, but something specific to this IDEA, which will help to\nunderstand the novelty of this IDEA.\n↩→\nPlease provide 3-4 unique keywords without overlapping terms.\nEach keyword should be 2 words or less.\nPlease return a pythonic list of keywords.\n\"\"\",\n},\n{\"role\": \"assistant\", \"content\": \"Sure, please provide the IDEA.\"},\n{\"role\": \"user\", \"content\": input_string},\n]\nreturn prompt\n\nScideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination\n61\nH\nPROMPTS FOR IDEA NOVELTY ITERATOR\nH.1\nPrompt to generate more novel idea suggestions for an idea classified as “not novel.”\ndef promptMoreNovelIdea(idea, papers, facets):\nprompt = f\"\"\"\nINITIAL IDEA:\n{idea[\"text\"]}\nPRIOR WORK FROM WHICH MORE NOVEL IDEA SHOULD DIFFERENTIATE:\"\"\"\nind = 1\nfor t in list(papers.keys()):\nprompt += f\"\"\"\nPaper {ind}\"\"\"\nif \"title\" in papers[t]:\nprompt += f\"\"\"\nTitle: {papers[t][\"title\"]}\nAbstract: {papers[t][\"abstract\"]}\n\"\"\"\nind += 1\nprompt += f\"\"\"\nSUMMARY OF WHY INITIAL IDEA IS NOT NOVEL COMPARED TO PRIOR WORK:\n{idea[\"novelty_review\"]}\nFACETS AVAILABLE TO ADD TO INITIAL IDEA:\n\"\"\"\nfor facet_key in facets:\nfacet = facets[facet_key]\nif facet[\"id\"] not in idea[\"facets\"]:\nprompt += f\"\"\"\n{facet[\"text\"]+\" (ID: \"+facet[\"id\"]+\")\"}\n\"\"\"\nprompt += f\"\"\"\nFACETS AVAILABLE TO REMOVE FROM INITIAL IDEA:\n\"\"\"\nfor facet_id in idea[\"facets\"]:\nprompt += f\"\"\"\n{facets[facet_id][\"text\"]+\" (ID: \"+facets[facet_id][\"id\"]+\")\"}\n\"\"\"\nprompt += f\"\"\"\nINSTRUCTIONS:\nYou are ScientistGPT, an intelligent assistant that helps researchers come up with coherent, novel, and useful research ideas.\nA novel research idea is not only rare but also ingenious, imaginative, or surprising.\nA useful research idea applies to the stated problem and is effective at solving the problem.\nFirst, read the above prior work and summary of why the initial idea is not novel compared to the prior work.\nThat way, you will know what has already been done in research and will not propose similar ideas.\nInstead, you will come up with novel ideas that build upon the prior work.\nSecond, provide 3 options for ideas that are more novel and useful than the initial idea.\nEach more novel/useful idea should utilize one new facet and remove one existing facet from the initial idea.\nThe summary of facets available to add and to remove are above.\nMake each novel/useful idea as specific as possible but describe it in 100 to 150 words.\nThe purposes/mechanisms/evaluations IDs should be written VERBATIM from what is provided.\nDo NOT make up facet IDs that are not explicitly given above.\nIt is very important that you follow the answer format provided below!\nFORMAT FOR ANSWER:\n1. Removed Purpose: <one removed purpose text here>\nRemoved Purpose ID: <removed purpose ID here (e.g., [purpose-string-123456789])>\nAdded Purpose: <one added purpose text here>\nAdded Purpose ID: <added purpose ID here (e.g., [purpose-string-012345678])>\n\n62\nRadensky et al.\nMore Novel Idea: <more novel idea here>\nWhy Idea is Novel: <sentence on why idea is novel here>\nWhy Idea is Useful: <sentence on why idea is useful here>\n2. Removed Mechanism: <one removed mechanism text here>\nRemoved Mechanism ID: <removed mechanism ID here (e.g., [mechanism-string-34523235])>\nAdded Mechanism: <one added mechanism text here>\nAdded Mechanism ID: <added mechanism ID here (e.g., [mechanism-string-634902049])>\nMore Novel Idea: <more novel idea here>\nWhy Idea is More Novel: <sentence on why idea is novel here>\nWhy Idea is Useful: <sentence on why idea is useful here>\n3. Removed Evaluation: <one removed evaluation text here>\nRemoved Evaluation ID: <removed evaluation ID here (e.g., [evaluation-string-123456789])>\nAdded Evaluation: <one added evaluation text here>\nAdded Evaluation ID: <added evaluation ID here (e.g., [evaluation-string-348950684])>\nMore Novel Idea: <more novel idea here>\nWhy Idea is More Novel: <sentence on why idea is novel here>\nWhy Idea is Useful: <sentence on why idea is useful here>\nANSWER:\n\"\"\"\nreturn prompt",
    "pdf_filename": "Scideator_Human-LLM_Scientific_Idea_Generation_Grounded_in_Research-Paper_Facet_Recombination.pdf"
}