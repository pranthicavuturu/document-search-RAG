{
    "title": "Exploring Optimal Transport-Based Multi-Grained Alignments for Text-Molecule Retrieval",
    "abstract": "progress, making the cross-modal text-molecule retrieval task increasingly vital. This task focuses on accurately retrieving molecule structures based on textual descriptions, by effectively aligning textual descriptions and molecules to assist researchers in identifying suitable molecular candidates. However, many existing approaches overlook the details inherent in molecule sub- structures. In this work, we introduce the Optimal TRansport- based Multi-grained Alignments model (ORMA), a novel ap- proach that facilitates multi-grained alignments between textual descriptions and molecules. Our model features a text encoder and a molecule encoder. The text encoder processes textual descriptions to generate both token-level and sentence-level representations, while molecules are modeled as hierarchical heterogeneous graphs, encompassing atom, motif, and molecule nodes to extract representations at these three levels. A key innovation in ORMA is the application of Optimal Transport (OT) to align tokens with motifs, creating multi-token represen- tations that integrate multiple token alignments with their cor- responding motifs. Additionally, we employ contrastive learning to refine cross-modal alignments at three distinct scales: token- atom, multitoken-motif, and sentence-molecule, ensuring that the similarities between correctly matched text-molecule pairs are maximized while those of unmatched pairs are minimized. To our knowledge, this is the first attempt to explore alignments at both the motif and multi-token levels. Experimental results on the ChEBI-20 and PCdes datasets demonstrate that ORMA significantly outperforms existing state-of-the-art (SOTA) models. Specifically, in text-molecule retrieval on ChEBI-20, our model achieves a Hits@1 score of 66.5%, surpassing the SOTA model AMAN by 17.1%. Similarly, in molecule-text retrieval, ORMA secures a Hits@1 score of 61.6%, outperforming AMAN by 15.0%. Index Terms—Text-molecule Retrieval, Multi-grained Repre- sentation Learning, Cross-modal Alignment, Optimal Transport I. INTRODUCTION The rapid advancement of bioinformatics has led to the construction of numerous large-scale molecular databases, such as PubChem [1]. These databases play a crucial role in the discovery and synthesis of new drugs. However, accurately retrieving desired molecules from these databases presents a significant challenge. Therefore, aiming to retrieve molecules based on text queries, cross-modal text-molecule retrieval [2] has become increasingly important. * Equal contribution. † Corresponding author. Water is an oxygen hydride consisting of an oxygen atom that is covalently bonded to two hydrogen atoms. Text Query Hydrogen peroxide is a colorless liquid at room temperature with a bitter taste. Water is an oxygen hydride consisting of an oxygen atom that is covalently bonded to two hydrogen atoms. Text Candidates Retrieve Retrieve Molecule Candidates H!O C!H\"O H!O! H!O Molecule Query Text-Molecule Retrieval Molecule-Text Retrieval Fig. 1. The text-molecule retrieval task is designed to retrieve molecules based on text queries, while molecule-text retrieval task does the opposite. The red box indicates the ground-truth retrieval result. Generally, existing studies mainly focus on the utilization of neural networks to learn representations of textual descriptions and molecules, and then calculating text-molecule similari- ties for retrieval. For example, several studies [3]–[5] resort to pretrained models based on Simplified Molecular Input Line Entry Specification (SMILES) [6] and text sequences. Alternatively, more studies like MoMu [7] and MoleculeSTM [8] represent molecules as 2D topological graphs, and then employ cross-modal contrastive learning to align molecular graphs and textual descriptions within a shared semantic space. Furthermore, AMAN [9] utilizes adversarial learning to effectively bridge these two modalities, achieving state-of- the-art (SOTA) performance. Despite these advancements, most studies overlook the de- tailed structural information essential for understanding molec- ular properties. Molecules are composed of atoms connected by chemical bonds, and their properties can be influenced by their structural motifs. In molecular chemistry, a motif is defined as a specific group of bonded atoms that follows a con- sistent and repeating pattern. Ignoring such detailed structural information seriously limits the precision of retrieval results. The only exception is the recently proposed Atomas [10], which applies clustering algorithms to extract representations at multiple granularities. To address the above issue, we propose a novel text- molecule model with Optimal TRansport-based Multi- grained Alignments (ORMA). Overall, our model contains a SciBERT-based text encoder and a GCN-based molecule arXiv:2411.11875v1  [cs.IR]  4 Nov 2024",
    "body": "Exploring Optimal Transport-Based Multi-Grained\nAlignments for Text-Molecule Retrieval\nZijun Min1*, Bingshuai Liu1*, Liang Zhang1, Jia Song1, Jinsong Su1†, Song He2†, Xiaochen Bo2†\n1School of Informatics, Xiamen University, Xiamen, China\n2Institute of Health Service and Transfusion Medicine, Beijing, China\n{minzijun, bsliu, lzhang, songjia}@stu.xmu.edu.cn, jssu@xmu.edu.cn, {hes1224, boxiaoc}@163.com\nAbstract—The field of bioinformatics has seen significant\nprogress, making the cross-modal text-molecule retrieval task\nincreasingly vital. This task focuses on accurately retrieving\nmolecule structures based on textual descriptions, by effectively\naligning textual descriptions and molecules to assist researchers\nin identifying suitable molecular candidates. However, many\nexisting approaches overlook the details inherent in molecule sub-\nstructures. In this work, we introduce the Optimal TRansport-\nbased Multi-grained Alignments model (ORMA), a novel ap-\nproach that facilitates multi-grained alignments between textual\ndescriptions and molecules. Our model features a text encoder\nand a molecule encoder. The text encoder processes textual\ndescriptions to generate both token-level and sentence-level\nrepresentations, while molecules are modeled as hierarchical\nheterogeneous graphs, encompassing atom, motif, and molecule\nnodes to extract representations at these three levels. A key\ninnovation in ORMA is the application of Optimal Transport\n(OT) to align tokens with motifs, creating multi-token represen-\ntations that integrate multiple token alignments with their cor-\nresponding motifs. Additionally, we employ contrastive learning\nto refine cross-modal alignments at three distinct scales: token-\natom, multitoken-motif, and sentence-molecule, ensuring that the\nsimilarities between correctly matched text-molecule pairs are\nmaximized while those of unmatched pairs are minimized. To\nour knowledge, this is the first attempt to explore alignments\nat both the motif and multi-token levels. Experimental results\non the ChEBI-20 and PCdes datasets demonstrate that ORMA\nsignificantly outperforms existing state-of-the-art (SOTA) models.\nSpecifically, in text-molecule retrieval on ChEBI-20, our model\nachieves a Hits@1 score of 66.5%, surpassing the SOTA model\nAMAN by 17.1%. Similarly, in molecule-text retrieval, ORMA\nsecures a Hits@1 score of 61.6%, outperforming AMAN by\n15.0%.\nIndex Terms—Text-molecule Retrieval, Multi-grained Repre-\nsentation Learning, Cross-modal Alignment, Optimal Transport\nI. INTRODUCTION\nThe rapid advancement of bioinformatics has led to the\nconstruction of numerous large-scale molecular databases,\nsuch as PubChem [1]. These databases play a crucial role in\nthe discovery and synthesis of new drugs. However, accurately\nretrieving desired molecules from these databases presents a\nsignificant challenge. Therefore, aiming to retrieve molecules\nbased on text queries, cross-modal text-molecule retrieval [2]\nhas become increasingly important.\n* Equal contribution.\n† Corresponding author.\nWater is an oxygen hydride consisting of an \noxygen atom that is covalently bonded to \ntwo hydrogen atoms.\nText Query\nHydrogen peroxide is a colorless liquid \nat room temperature with a bitter taste.\nWater is an oxygen hydride consisting \nof an oxygen atom that is covalently \nbonded to two hydrogen atoms.\nText Candidates\nRetrieve\nRetrieve\nMolecule Candidates\nH!O\nC!H\"O\nH!O!\nH!O\nMolecule \nQuery\nText-Molecule Retrieval\nMolecule-Text Retrieval\nFig. 1.\nThe text-molecule retrieval task is designed to retrieve molecules\nbased on text queries, while molecule-text retrieval task does the opposite.\nThe red box indicates the ground-truth retrieval result.\nGenerally, existing studies mainly focus on the utilization of\nneural networks to learn representations of textual descriptions\nand molecules, and then calculating text-molecule similari-\nties for retrieval. For example, several studies [3]–[5] resort\nto pretrained models based on Simplified Molecular Input\nLine Entry Specification (SMILES) [6] and text sequences.\nAlternatively, more studies like MoMu [7] and MoleculeSTM\n[8] represent molecules as 2D topological graphs, and then\nemploy cross-modal contrastive learning to align molecular\ngraphs and textual descriptions within a shared semantic\nspace. Furthermore, AMAN [9] utilizes adversarial learning\nto effectively bridge these two modalities, achieving state-of-\nthe-art (SOTA) performance.\nDespite these advancements, most studies overlook the de-\ntailed structural information essential for understanding molec-\nular properties. Molecules are composed of atoms connected\nby chemical bonds, and their properties can be influenced\nby their structural motifs. In molecular chemistry, a motif is\ndefined as a specific group of bonded atoms that follows a con-\nsistent and repeating pattern. Ignoring such detailed structural\ninformation seriously limits the precision of retrieval results.\nThe only exception is the recently proposed Atomas [10],\nwhich applies clustering algorithms to extract representations\nat multiple granularities.\nTo address the above issue, we propose a novel text-\nmolecule\nmodel\nwith\nOptimal\nTRansport-based\nMulti-\ngrained Alignments (ORMA). Overall, our model contains\na SciBERT-based text encoder and a GCN-based molecule\narXiv:2411.11875v1  [cs.IR]  4 Nov 2024\n\nencoder, to individually learn textual and molecular represen-\ntations at multiple granularities for retrieval.\nConcretely, we utilize the text encoder to process each input\ntextual description, obtaining token and sentence represen-\ntations. Simultaneously, we decompose each input molecule\ninto motifs based on chemical rules, and then represent it as\nan undirected heterogeneous graph. In this graph, each node\nrepresents either an atom, motif, or global molecule node, with\ntwo types of edges modeling the relationships between atom\nand motif, and motif and molecule, respectively. Based on\nthis graph, we employ the molecule encode to encoder the\ninput molecule at multiple levels, obtaining atom, motif, and\nmolecule representations.\nSubsequently, we treat the tokens and motifs as independent\ndistributions and employ optimal transport to achieve their\nalignments. Furthermore, for each motif, we obtain a fused\nrepresentation of its aligned tokens as the multi-token repre-\nsentation.\nWith the previously mentioned cross-modal representations\nat different granularities, we introduce contrastive learning\nlosses in our model training to achieve alignments at three\nlevels: token-atom, multitoken-motif, and sentence-molecule.\nThese losses aim to maximize similarity scores for matched\ntext-molecule pairs while minimizing those for unmatched\npairs, thereby enhancing alignments across different modal-\nities. During inference of text-molecule retrieval, we calculate\nsimilarities at three levels, and combine them to retrieve the\nmolecule with the highest similarity. In molecule-text retrieval\ntask, we perform this process in the opposite direction.\nTo summarize, the main contributions of our work are three-\nfold:\n• We propose ORMA, designed to learn textual and molec-\nular representations at multiple levels. It employs multiple\nlosses to align cross-modal representations at different\nlevels, thereby enhancing cross-modal alignments for\nretrieval.\n• We model the alignments between tokens and motifs\nas an optimal transport problem to learn multi-token\nrepresentations for each motif, facilitating subsequent\nmultitoken-motif alignments. To the best of our knowl-\nedge, our work is the first to explore the multitoken-motif\nalignments in text-molecule retrieval.\n• Experimental results on the ChEBI-20 and PCdes datasets\ndemonstrate that our model achieves significant improve-\nments over the state-of-the-art (SOTA) models. Specif-\nically, for the text-molecule retrieval on ChEBI-20, the\nHits@1 score of our model is 66.5%, outperforming the\nSOTA model–AMAN by 17.1%. Similarly, in the task\nof molecule-text retrieval, our model achieves a Hits@1\nscore of 61.6%, surpassing AMAN by 15.0%.\nII. RELATED WORK\nTo achieve high-quality text-molecule retrieval, most studies\nrepresent molecules as 1D sequences, 2D molecular graphs, or\n3D molecular conformations. In the aspect of 1D molecule\nmodeling, SMILES [6] has been widely used to represent\nmolecular sequences, emerging many typical pre-training\nmodels, such as KV-PLM [3], MolT5 [4], and Text+Chem\nT5 [11]. Meanwhile, some studies switch their attention to\n2D topological graphs, where atoms and chemical bonds are\nconsidered as nodes and edges, respectively. For example,\nstudies like MoMu [7] and MoleculeSTM [8] employ cross-\nmodal contrastive learning to align text and molecular graphs\nin a shared semantic space. Moreover, MolCA [12] intro-\nduces a cross-modal projector, while AMAN [9] and [13]\nemploy adversarial learning to bridge these two modalities\neffectively. As the extension of the above studies, several\nstudies incorporate additional modalities to aid the alignments\nbetween text and molecules, such as MolFM [14] incorporated\nknowledge graphs, and GIT-Mol [15] incorporated images.\nRecently, some studies focus on the spatial information in 3D\nmolecular conformations. For instance, 3D-MoLM [16] adopts\na similar architecture to MolCA [12], using 3D conformations\nto represent molecules.\nDespite these advancements, most methods only focus on\nglobal molecular information, often overlooking detailed in-\nformation at the motif and atom levels. To integrate detailed\ninformation, multi-grained alignments have been explored in\nvarious natural language processing (NLP) [17]–[22] and\ncomputer vision (CV) [23]–[25] tasks, enabling models to\neffectively process complex information. However, in the task\nof text-molecule retrieval, few studies pay attention to this\ntechnique. The only exception is Atomas [10] which applies a\nclustering algorithm to extract features at the atom, fragment,\nand molecule levels.\nAlthough with the same motivation as Atomas, we construct\na hierarchical heterogeneous molecular graph to obtain atom,\nmotif, and molecule representations. More importantly, we\nintroduce optimal transport to fuse the representations of\nmultiple tokens aligned with the same motif. Through con-\ntrastive learning, we align text and molecules at token-atom,\nmultitoken-motif, and sentence-molecule levels. Subsequent\nexperimental results show that our model outperforms Atomas,\nconfirming the effectiveness of our model.\nIII. OUR MODEL\nIn this section, we will provide a detailed description of\nour model. We first present the main architecture of our\nmodel, which mainly contains a text encoder and a molecule\nencoder to learn representations at different granularities. Next,\nwe further introduce Optimal Transport (OT) to generate\nthe fused representation of multiple tokens aligned with the\nsame motifs. To the best of our knowledge, our work is\nthe first attempt to consider representations at the motif and\nmulti-token levels. Finally, we define a training objective that\ninvolves three alignment losses to achieve cross-modal multi-\ngrained alignments.\nA. Main Architecture\nAs shown in Figure 2, our model mainly consists of a\nmolecule encoder and a text encoder. As implemented in\nprevious studies [2], [9], we also employ SciBERT [26]\n\nN-acetyl-L-alanine \nis an N-acetyl-L-\namino acid that is L-\nalanine in which one \nof the hydrogens \nattached to the \nnitrogen is replaced \nby an acetyl group...\nMolecule \nEncoder\nAtom \nRepresentations\nMolecule\nRepresentation\nSentence\nRepresentation\nToken \nRepresentations\nMulti-token \nRepresentations\n[CLS]\nmolecule \nnode\nmotif \nnode\natom \nnode\nFusion\nText\nEncoder\n𝑳𝒕𝒂\nMotif \nRepresentations\n𝑳𝒎𝒎\n𝑳$𝒎\nFig. 2. The illustration of our model. In short, our model comprises a text encoder and a molecule encoder and designs three alignment losses Lta, Lmm, Lsm\nat token-atom, multitoken-motif, and sentence-molecule levels, respectively. On the left side of the figure is a molecular graph, where the nodes at the top\nrepresent atom nodes. In the middle, each node with the solid edge is a motif node, connected to the atom nodes it contains. At the bottom, the node with\nthe dashed edge is the molecule node, connected to all motif nodes.\nto encode the input textual descriptions, obtaining represen-\ntations at both sentence and token levels. Compared with\nother pretraining models, SciBERT outperforms in encoding\nchemical textual descriptions, due to pretraining on a large-\nscale corpus of scientific publications. Given an input textual\ndescription with Nt tokens, we first concatenate a special\n[CLS] token at the beginning of the sequence. Then, we input\nthe sequence into the SciBERT encoder, where the learned\nd-dimensional representation of [CLS] serves as the sentence\nrepresentation hs, and those of remaining tokens are used as\ntoken representations: Ht = [ht\n1, ht\n2, . . . , ht\nNt] ∈RNt×d.\nTo effectively encode the input molecule, we represent it as\nan undirected heterogeneous graph and use a GCN encoder to\nlearn its representations at different granularities. The molecu-\nlar graph contains three types of nodes, constructed as follows:\n(1) Atom nodes. Since each molecule consists of atoms\nconnected by chemical bonds, we include each atom as an\nindividual node, which enables us to fully capture the detailed\nstructural information of the molecule. (2) Motif nodes. In\nmolecular chemistry, a motif is defined as a specific group of\nbonded atoms that follows a consistent and repeating pattern.\nThus, we believe that motifs encode rich implicit semantic\ninformation, which is crucial for understanding the molecular\nproperties described in the text. Following previous study\n[27], we employ the BRICS algorithm [28] with an additional\ndecomposition rule to extract motifs, all of which are also\nincluded as separate motif nodes. (3) Molecule node. We also\ninclude a global molecule node to facilitate the learning of a\ncomprehensive molecule representation. To effectively capture\nthe relationships between nodes at different granularities, we\nconsider two types of edges: (1) Motif-Atom Edges. Each\nmotif node is connected to its constituent atom nodes. (2)\nMolecule-Motif Edges. The molecule node is connected to\nall motif nodes.\nFor example, in the molecular graph shown on the left part\nof Figure 2, each node at the top represents an atom node,\nand each node with the solid edge in the middle is a motif\nnode, connected to the atom nodes it contains. Meanwhile,\nindicated as a node with the dashed edge, the molecule\nnode is connected to all motif nodes. This heterogeneous\ngraph allows for enhancing the information sharing among\natom, motif, and molecule nodes during the neighborhood\naggregation of graph neural networks, thus facilitating multi-\ngrained molecular representation learning.\nBased on the above heterogeneous graph, we employ a\n3-layer Graph Convolutional Network (GCN)\n[29] to learn\nnode representations, which correspond to molecular repre-\nsentations at three granularities: (1) the atom representations\nHa = [ha\n1, ha\n2, . . . , ha\nNa] ∈RNa×d, where Na is denoted as\nthe number of atoms, (2) the motif representations Hm =\n[hm\n1 , hm\n2 , . . . , hm\nNm] ∈RNm×d, with Nm is the number of\nmotif, and (3) the global molecule representation hg ∈Rd.\nB. Optimal Transport-Based Multi-token Fused Representa-\ntion Learning\nAs previously mentioned, a crucial aspect of our model is\nthe consideration of the fused representation of multiple tokens\naligned with the corresponding motif. To this end, we model\nthe alignments between the input tokens and motifs as the\nOptimal Transport (OT) problem, as illustrated in Figure 4.\nAs a typical machine learning problem, OT aims to find the\nscheme that minimizes the transportation cost of transferring\none distribution to another distribution [30]. Concretely, we\nfirst consider the following crucial definitions: (1) We regard\nthe token representations Ht and motif representations Hm\nas two independent distributions. (2) Cij = 1 −cos(ht\ni, hm\nj )\nis the transportation cost from ht\ni to hm\nj , where cos(∗, ∗) is a\ncosine distance function. (3) We define T = {Tij}, 1≤i≤Nt,\n1≤j≤Nm, as an assignment plan, learned to optimize the\nalignments between input tokens and motifs. Solving the\noptimal transport problem is equivalent to addressing a specific\nnetwork-flow problem [31] as follows:\nmin\nNt\nX\ni=1\nNm\nX\nj=1\nTijCij = min Tr(TT\nijCij),\n(1)\n\n.\nToken-Atom Similarity Matrix\nNorm\nUpdated Atom \nRepresentations \nof Sample 𝑘\nSum \nPooling\nToken Representations \nof Sample 𝑞\nAtom Representations of Sample 𝑘\nSum \nPooling\nSimilarity 𝑆!\"\n#$\nℎ!\n\"\nℎ#\n\"\nℎ$\n\"\nℎ%\n\"\nℎ&\n\"\nℎ!\n' ℎ#\n' ℎ$\n' ℎ%\n' ℎ&\n' ℎ(\n' ℎ)\n' ℎ*\n' ℎ+\n'\nℎ!\n',\nℎ#\n',\nℎ$\n',\nℎ%\n',\nℎ&\n',\nFig. 3. The process of token-atom alignments. Within a batch, we update the\natom representations of each sample and then calculate the token-atom level\nsimilarities between samples. Then we use contrastive learning to maximize\nthe similarities between matched text-molecule pairs while minimizing those\nbetween unmatched pairs.\nwhere Tr(TT\nijCij) represents the Frobenius dot-product. The\nexact optimal transport problem generally poses computational\nchallenges due to its complexity. To address this challenge,\nwe introduce the Inexact Proximal Point Method for Optimal\nTransport (IPOT) [32], which approximately solves the opti-\nmal transport problem to obtain an optimal assignment plan.\nTherefore, for the i-th token, we determine its optimal\nalignment motif index as a(i) = arg min\n1≤j≤Nm\nTr(TT\nijCij). Then\nlet Dj={i|a(i)=j,1≤i≤Nt} denote the index set, where the\ncorresponding tokens are aligned to the j-th motif. Sub-\nsequently, we average the representations of tokens occur-\nring in Dj to obtain their fused multi-token representation:\nhp\nj =\n1\n|Dj|I(a(i) = j)ht\ni, where I(·) is the indicator function.\nAnalogously, we can obtain all motif-aligned multi-token\nrepresentations as Hp = [hp\n1, hp\n2, . . . , hp\nNp] ∈RNp×d, where\nNp is the number of multi-tokens.\nC. Model Training and Inference\nThrough the above steps, we obtain the representations of\ninput texts and molecules at different granularities. Further,\nwe define the following training objective:\nL = α · Lta + β · Lmm + (1 −α −β) · Lsm,\n(2)\nwhere Lta, Lmm, Lsm denote the token-atom, multitoken-\nmotif, and sentence-molecule alignment losses, and α, β are\ntwo coefficients balancing the effects of different losses,\nrespectively. In the following, we will detail these three losses.\n1) Token-Atom Alignment Loss Lta: To ensure precise\nalignments between tokens and atoms, we introduce a token-\natom alignment loss based on contrastive learning. Concretely,\nas shown in Figure 3, we compute the token-atom similarity\nmatrix between token representations {ht\ni} and atom repre-\nsentations {ha\nj }. The similarity matrix is then normalized\nby the min-max normalization along the atom dimension.\nSubsequently, we further normalize the matrix along the token\ndimension to obtain the alignment weights, used to unify the\nToken Representations\nIPOT \nIteration\nℎ!\n\" ℎ#\n\" ℎ$\n\" ℎ%\n\" ℎ&\n\"\nCost \nMatrix\nOptimal \nAssignment Plan\nℎ#\n\"\nℎ!\n\"\nℎ%\n\"\nℎ&\n\"\nℎ$\n\"\nFusion\nMulti-token \nRepresentations\nOptimal \nAlignment\nToken \nSelection\nℎ#\n\"\nPooling(ℎ$\n\", ℎ&\n\")\nPooling(ℎ!\n\", ℎ%\n\")\nMotif\nRepresentations\nℎ!\n-\nℎ#\n-\nℎ$\n-\nFig. 4.\nThe process of obtaining multi-token representations through OT.\nConsidering the alignments between token representations and motif repre-\nsentations as an optimal transport problem, we fuse the token representations\ninto multi-token representations corresponding to specific motifs.\ndimensions of token and atom representations. After mul-\ntiplying these alignment weights with atom representations,\nwe update the atom representations as {ha′\ni } by integrating\ntextual information. Finally, we apply sum pooling to reduce\nthe dimensions of both representations, which are then used\nto calculate the final similarity at the token-atom level.\nWithin a batch of size B, we compute similarities {Sta\nqk},\n1≤q≤B, 1≤k≤B, between samples at the token-atom level.\nFollowing this, we define the token-atom alignment loss Lta\nas the average of two Categorical Cross-Entropy (CCE) losses\ncorresponding to text-molecule and molecule-text retrieval\ntasks, respectively:\nLta = −1\n2B\nB\nX\nk=1\nlog\nexp(Sta\nkk)\nPB\nq=1 exp(Sta\nqk)\n−1\n2B\nB\nX\nk=1\nlog\nexp(Sta\nkk)\nPB\nq=1 exp(Sta\nkq)\n.\n(3)\n2) Multitoken-Motif Alignment Loss Lmm: We also employ\ncontrastive learning to achieve alignments between multi-\ntokens and motifs. Specifically, in a similar way, we calculate\nthe similarities {Smm\nqk }, 1≤q≤B, 1≤k≤B, at multi-token and\nmotif level between samples within the same batch. Thus,\nthe multitoken-motif alignment loss Lmm is formulated as\nfollows:\nLmm = −1\n2B\nB\nX\nk=1\nlog\nexp(Smm\nkk )\nPB\nq=1 exp(Smm\nqk )\n−1\n2B\nB\nX\nk=1\nlog\nexp(Smm\nkk )\nPB\nq=1 exp(Smm\nkq )\n.\n(4)\n3) Sentence-Molecule Alignment Loss Lsm:\nSimilar to\nother levels of alignments, we employ contrastive learning to\ncompute the CCE loss at sentence-molecule level. Concretely,\nwe maximize the similarities between matched sentence and\nmolecule pairs, while minimizing those between unmatched\n\nTABLE I\nPERFORMANCE ON THE CHEBI-20 DATASET FOR TEXT-MOLECULE AND MOLECULE-TEXT RETRIEVAL TASKS. THE BOLD PART INDICATES THE BEST\nPERFORMANCE. ORMA ACHIEVES LEADING PERFORMANCE IN BOTH RETRIEVAL TASKS. ↑DENOTES THAT THE HIGHER IS THE BETTER, WHILE ↓\nDENOTES THAT THE LOWER IS THE BETTER. † REPRESENTS OUR REPRODUCED RESULTS OF ATOMAS-BASE AND THE OTHER RESULTS ARE REPORTED IN\nTHE PREVIOUS WORKS.\nModels\nText-Molecule Retrieval\nMolecule-Text Retrieval\nHits@1(↑)\nHits@10(↑)\nMRR(↑)\nMean Rank(↓)\nHits@1(↑)\nHits@10(↑)\nMRR(↑)\nMean Rank(↓)\nMLP-Ensemble [2]\n29.4%\n77.6%\n0.452\n20.78\n-\n-\n-\n-\nGCN-Ensemble [2]\n29.4%\n77.1%\n0.447\n28.77\n-\n-\n-\n-\nAll-Ensemble [2]\n34.4%\n81.1%\n0.499\n20.21\n25.2%\n74.1%\n0.408\n21.77\nMLP+Atten [2]\n22.8%\n68.7%\n0.375\n30.37\n-\n-\n-\n-\nMLP+FPG [33]\n22.6%\n68.6%\n0.374\n30.37\n-\n-\n-\n-\nAMAN [9]\n49.4%\n92.1%\n0.647\n16.01\n46.6%\n91.6%\n0.625\n16.50\nAtomas-base† [10]\n50.1%\n92.1%\n0.653\n14.49\n45.6%\n90.3%\n0.614\n15.12\nORMA (Ours)\n66.5%\n93.9%\n0.772\n18.53\n61.6%\n93.8%\n0.739\n8.10\n(R)-nephthenol\n)O)C\nL-histidinol\nL-histidinol is an amino alcohol that is propanol \nsubstituted by 1H-imidazol-4-yl group at position \n3 and an amino group at position 2 (the 2S \nstereoisomer). It is a member of imidazoles and \nan amino alcohol. It is a conjugate base of a L-\nhistidinol(1+).\n(R)-nephthenol is a cembrane diterpenoid \nobtained by regio- and stereoselective hydration \nof the exocyclic double bond of cembrene C. It \nhas a role as a bacterial metabolite and a coral \nmetabolite. It is a macrocycle, a cembrane \nditerpenoid, a tertiary alcohol and an olefinic \ncompound. It derives from a cembrene C.\nFig. 5. The visualization for the alignments of motifs and multi-tokens. The\nmotifs marked in red and green on the left correspond to the multi-tokens\nhighlighted in red and green on the right, respectively.\npairs, with the sentence-molecule alignment loss Lsm defined\nas follows:\nLsm = −1\n2B\nB\nX\nk=1\nlog\nexp(Ssm\nkk )\nPB\nq=1 exp(Ssm\nqk )\n−1\n2B\nB\nX\nk=1\nlog\nexp(Ssm\nkk )\nPB\nq=1 exp(Ssm\nkq )\n,\n(5)\nwhere {Ssm\nqk }, 1≤q≤B, 1≤k≤B, is denoted as the similarities\nbetween the sentence representation of the q-th sample and the\nmolecule representation of the k-th sample.\nDuring inference for text-molecule retrieval, we calculate\nthe similarities between text queries and all molecule candi-\ndates at the above three levels. Then we assign the coefficients\nα, β to these similarities at different levels to obtain the final\nsimilarity, retrieving the molecule with the highest similarity.\nIn molecule-text retrieval task, we perform this process in the\nopposite direction.\nIV. EXPERIMENTS\nA. Dataset and Evaluation\nOur model is trained and evaluated on two datasets: ChEBI-\n20 dataset [2] and PCdes dataset [3]. The ChEBI-20 dataset\ncontains 33,010 molecules with textual descriptions, divided\ninto training, validation, and test sets in the ratio of 8:1:1.\nFollowing previous studies [2], [9], we evaluate the retrieval\nperformance using Hits@1, Hits@10, mean reciprocal rank\n(MRR), and mean rank. During inference, test samples are\nretrieved from the entire ChEBI-20 dataset. The PCdes dataset\nconsists of 15,000 molecule pairs from PubChem [1], split\nin the ratio of 7:1:2. After excluding 8 molecules whose\nSMILES strings can not be converted into 2D graphs using\nRDKit [34], 14,992 instances remain for our experiments.\nFollowing previous works, retrieval performance is evaluated\nusing Recall at 1/10 (R@1, R@10), MRR, and mean rank.\nPrevious pretrained studies [3], [7], [8], [10], [12], [14] often\nutilize finetuning or directly inference at zero-shot settings\non the PCdes dataset. Since our approach does not involve\npretraining, we train our model from scratch on the PCdes\ntraining set and evaluate its performance on the test set.\nB. Experimental Settings\nDuring model training, we utilize a pretrained SciBERT [26]\nas the text encoder with a maximum text length of 256. We\nemploy a 3-layer GCN as the graph encoder with an output\ndimension of 300. We use the Adam optimizer [35], setting the\nlearning rate of 3e-5 for SciBERT and 1e-4 for the remaining\nparts of our model. Our training spans 60 epochs and uses a\nbatch size of 32. We assign the coefficients for different levels\nin training objective as follows: α = 0.5, β = 0.2.\nC. Baseline Models\na) AMAN [9]: It leverages the SciBERT to encode tex-\ntual descriptions and a Graph Transformer Network (GTN) to\nencode molecules. Besides, it introduces adversarial learning\nand triplet loss to align these modalities.\n\nTABLE II\nRESULTS ON THE PCDES DATASET FOR TEXT-MOLECULE AND MOLECULE-TEXT RETRIEVAL TASKS AT ZERO-SHOT, FINETUNING AND FROM-SCRATCH\nSETTINGS. THE BOLD PARTS INDICATE THE BEST PERFORMANCE. NOTE THAT OUR MODEL, TRAINED FROM SCRATCH, SIGNIFICANTLY OUTPERFORMS\nOTHERS IN ALL METRICS. ↑DENOTES THAT THE HIGHER IS THE BETTER.\nModels\nText-Molecule Retrieval\nMolecule-Text Retrieval\nR@1(↑)\nR@5(↑)\nR@10(↑)\nMRR(↑)\nR@1(↑)\nR@5(↑)\nR@10(↑)\nMRR(↑)\nPretrained Model + Zero-shot\nMoMu [7]\n4.9%\n14.5%\n20.7%\n0.103\n5.1%\n12.8%\n18.9%\n0.099\nMolCA [12]\n35.1%\n62.1%\n69.8%\n0.473\n38.0%\n66.8%\n74.5%\n0.508\nMolFM [14]\n16.1%\n30.7%\n39.5%\n0.236\n13.9%\n28.7%\n36.2%\n0.214\nMoleculeSTM [8]\n35.8%\n-\n-\n-\n39.5%\n-\n-\n-\nAtomas-base [10]\n39.1%\n59.7%\n66.6%\n0.473\n37.9%\n59.2%\n65.6%\n0.478\nAtomas-large [10]\n49.1%\n68.3%\n73.2%\n0.578\n46.2%\n66.0%\n72.3%\n0.555\nPretrained Model + Finetuning\nSciBERT [26]\n16.3%\n33.9%\n42.6%\n0.250\n15.0%\n34.1%\n41.7%\n0.239\nKV-PLM [3]\n18.4%\n37.2%\n45.4%\n0.274\n16.6%\n35.9%\n44.8%\n0.260\nKV-PLM* [3]\n20.6%\n37.9%\n45.7%\n0.292\n19.3%\n37.3%\n45.3%\n0.281\nMoMu [7]\n24.5%\n45.4%\n53.8%\n0.343\n24.9%\n44.9%\n54.3%\n0.345\nMolFM [14]\n29.8%\n50.5%\n58.6%\n0.396\n29.4%\n50.3%\n58.5%\n0.393\nFrom-scratch\nORMA (Ours)\n64.8%\n82.3%\n86.3%\n0.727\n62.1%\n81.4%\n86.3%\n0.710\nTABLE III\nABLATION STUDY ON THE CHEBI-20 DATASET AT THREE LEVELS: TOKEN-ATOM (TA), MULTITOKEN-MOTIF (MM), AND SENTENCE-MOLECULE (SM).\nTHE RESULTS SUGGEST THAT INTEGRATING THREE LEVELS SIGNIFICANTLY IMPROVES PERFORMANCE IN BOTH TEXT-MOLECULE AND MOLECULE-TEXT\nRETRIEVAL TASKS. ↓DENOTES THAT THE LOWER IS THE BETTER.\nLevel\nText-Molecule Retrieval\nMolecule-Text Retrieval\nTA\nMM\nSM\nHits@1(↑)\nHits@10(↑)\nMRR(↑)\nMean Rank(↓)\nHits@1(↑)\nHits@10(↑)\nMRR(↑)\nMean Rank(↓)\n✓\n✓\n✓\n66.5%\n93.9%\n0.772\n18.53\n61.6%\n93.8%\n0.739\n8.1\n✓\n✓\n57.4%\n91.6%\n0.700\n26.74\n38.4%\n89.4%\n0.552\n12.87\n✓\n✓\n61.7%\n92.5%\n0.733\n23.10\n51.9%\n91.3%\n0.661\n11.13\n✓\n✓\n35.6%\n81.8%\n0.511\n41.39\n35.1%\n80.8%\n0.506\n30.95\n✓\n24.1%\n75.3%\n0.401\n42.28\n21.8%\n72.1%\n0.377\n30.86\n✓\n17.0%\n55.0%\n0.291\n219.45\n20.2%\n56.3%\n0.319\n247.27\n✓\n49.6%\n89.1%\n0.636\n37.17\n23.3%\n71.6%\n0.384\n22.64\nb) Atomas [10]: This model is pretrained on large-\nscale text and SMILES data, aligning textual and molecular\nmodalities at three granularities. Atomas utilizes the clustering\napproach to align textual descriptions and molecules at three\ngranularities, while our model adopts optimal transport and\ncontrastive learning to achieve alignments at three levels. It\ninferences at zero-shot settings for evaluating retrieval perfor-\nmance on PCdes. Additionally, on the ChEBI-20 dataset, we\ntrain Atomas from scratch using the same configuration as\nours to evaluate its retrieval capabilities.\nD. Main Results\nWe present experimental results on the ChEBI-20 dataset\nfor both retrieval tasks, as detailed in Table I. In the text-\nmolecule retrieval task, our model significantly outperforms all\nother baseline models, achieving a Hits@1 of 66.5%, Hits@10\nof 93.9%, and an MRR of 0.772. Although our mean rank\nis 18.53 that is not the best, it is competitively close to the\nbest-performing model, Atomas-base, which obtains a slightly\nbetter mean rank of 14.49 but lower Hits@1 and MRR.\nIn the molecule-text retrieval task, our model also demon-\nstrates superior performance, leading in all metrics with a\nHits@1 of 61.6%, Hits@10 of 93.8%, an MRR of 0.739,\nand an impressively low mean rank of 8.10. The experimental\nresults confirm the effectiveness of our model in these retrieval\ntasks, providing valuable insights for further enhancements.\nWe also present experimental results on the PCdes dataset\nfor two retrieval tasks, as detailed in Table II. Notably,\nour model, trained from scratch on this dataset, significantly\noutperforms existing models at both zero-shot and finetuning\nsettings. In the text-molecule retrieval task, our model achieves\nan impressive R@1 of 64.8%, remarkably better than the\nbest zero-shot model (Atomas-large at 49.1%) and finetuning\nmodel (MolFM at 29.8%). It also achieves an R@5 of 82.3%,\nR@10 of 86.3%, and MRR of 0.727. In the molecule-text\nretrieval task, our model demonstrates superior results with an\nR@1 of 62.1%, R@5 of 81.4%, and R@10 of 86.3%, along-\nside MRR of 0.710. This exceptional performance highlights\nthe potential of our model to surpass generalized pretrained\nmodels in both retrieval tasks, confirming its effectiveness.\n\nMalachite green isothiocyanate \nis an organic perchlorate salt. It \nhas a role as a fluorochrome. It \ncontains a malachite green \nisothiocyanate cation.\nMalachite green \nisothiocyanate\nDouble green\nGentian Violet\nPararosaniline(1+) \nMalachite \ngreen cation\nGround Truth\nText Query\nTop-k Candidates\nMalachite green \nisothiocyanate\n(R)-Piperazine-2-\ncarboxylic acid\n(R)-Piperazine-2-\ncarboxylic acid\n(R)-piperidine-3-\ncarboxylic acid\n6-Oxopiperidine-\n2-carboxylic acid\n(S)-Piperazine-2-\ncarboxylic acid\nNipecotic acid\n(R)-piperazine-2-carboxylic acid \nis a piperazine-2-carboxylic acid \nhaving (R)-configuration. It is a \nconjugate acid of a (R)-\npiperazine-2-carboxylate. It is an \nenantiomer of a (S)-piperazine-\n2-carboxylic acid. It is a \ntautomer of a (R)-piperazine-2-\ncarboxylic acid zwitterion.\nFig. 6. Case study of our model in the text-molecule retrieval task. The red box indicates that the molecule with the highest similarity retrieved by our model\nis the ground truth.\nTestosterone acetate is an \nandrostanoid that is the acetate \nderivative of testosterone. It has a \nrole as a human metabolite. It is a \nsterol ester, an androstanoid and a \n3-oxo-Delta(4) steroid. It derives \nfrom a testosterone.\nGround Truth\nMolecule Query\nTop-k Candidates\nTestosterone acetate\nTestosterone acetate is an \nandrostanoid that is the acetate \nderivative of testosterone. It has \na role as a human metabolite. It \nis a sterol ester, an androstanoid \nand a 3-oxo-Delta(4) steroid. It \nderives from a testosterone.\nTestosterone acetate\n17alpha-hydroxyprogesterone is \na 17alpha-hydroxy steroid that \nis the 17alpha-hydroxy \nderivative of progesterone. It \nhas a role as a metabolite, a \nprogestin, a human metabolite \nand a mouse metabolite. It is a \n17alpha-hydroxy steroid…\nHydroxyprogesterone\nProgesterone\nProgesterone is a C21-steroid \nhormone in which a pregnane \nskeleton carries oxo substituents \nat positions 3 and 20 and is \nunsaturated at C(4)-C(5). As a \nhormone, it is involved in the \nfemale menstrual cycle, \npregnancy and embryogenesis \nof humans and other species…\n16-Dehydropregnenolone\n16,17-didehydropregnenolone is \na 3beta-hydroxy steroid that is \npregnenolone with a double \nbond between positions 16 and \n17. It is a 3beta-hydroxy steroid, \na 20-oxo steroid and an enone. \nIt derives from a pregnenolone.\nFig. 7. Case study of our model in the molecule-text retrieval task. The blue box highlights the candidate description retrieved by our model that exactly\ncorresponds to the ground truth.\nE. Ablation Study\nWe conduct an ablation study on the ChEBI-20 dataset for\nboth retrieval tasks, providing insights about the benefits of\nalignments at different levels: token-atom (TA), multitoken-\nmotif (MM), and sentence-molecule (SM). From Table III,\nwe can clearly observe that the use of all three levels leads\nto the best results, confirming the effectiveness of the multi-\nlevel alignments. Conversely, the results are worse when only\ntwo levels of alignments are employed. Also, relying on one\nlevel of alignment alone further diminishes the performance.\nThese results highlight that alignment at each level uniquely\ncontributes to the overall performance.\nF. Case Study\n1) Retrieval Performance: We select several typical sam-\nples from the test set to demonstrate the retrieval capability\nof our model. As illustrated in Figure 6, our model accurately\nretrieves the ground truth molecules as the top-1 candidate for\nboth queries. The molecules retrieved in the top-k candidates\nclosely resemble the text queries, confirming the effective\nalignments between textual descriptions and molecules. As\nshown in Figure 7, our model also correctly identifies the\nground truth description as the top-1 retrieval result for a\ngiven molecule query. Although the textual descriptions of the\ntop-k retrieved candidates vary, their corresponding molecule\nstructures are remarkably similar. This suggests that our model\naccurately grasps the alignments between text and molecules,\nhighlighting its superior performance.\n2) Visualization for the Alignments of Motifs and Multi-\ntokens: We also visualize the fusion of multiple motif-aligned\ntokens through optimal transport theory in Figure 5. On the\nleft side of the figure, atoms marked in the same color serve\nas the motifs extracted by our model. On the right side,\nthe textual descriptions highlighted in the same colors repre-\nsent the identified multi-tokens corresponding to the motifs\nof matching colors on the left. For instance, we use (R)-\nnephthenol and L-histidinol as examples, where the motifs\nextracted by our model match the identified multi-tokens.\nThis comprehensively demonstrates that the optimal transport\nis helpful for the alignments between the multi-tokens and\ncorresponding motifs.\nV. CONCLUSION\nIn this paper, we propose an Optimal TRansport-based\nMulti-grained Alignments model (ORMA), designed to en-\nhance the cross-modal text-molecule retrieval task. By uti-\nlizing SciBERT, we encode textual descriptions into token\nand sentence representations. Simultaneously, we represent\nthe molecule as a heterogeneous graph containing atom,\n\nmotif, and molecule nodes, encoded by our GCN molecule\nencoder. Innovatively, we consider the alignments between\ntokens and motifs as the optimal transport problem, fusing\ntoken representations into motif-aligned multi-token repre-\nsentations. Furthermore, we employ contrastive learning to\nalign textual descriptions and molecules at three levels: token-\natom, multitoken-motif, and sentence-molecule. To the best\nof our knowledge, our work is the first attempt to consider\nrepresentations at the motif and multi-token granularities.\nExperimental results on the ChEBI-20 and PCdes datasets\ndemonstrate the superior retrieval performance of our model\ncompared to previous state-of-the-art models. In the future,\nwe will extend ORMA to integrate more modal data sources,\nsuch as protein structures and cellular images, to further the\napplication of ORMA in more complex biological systems.\nVI. ACKNOWLEDGEMENTS\nThe project was supported by the National Natural Science\nFoundation of China (Nos. 62276219 and 62472370) and\nthe Public Technology Service Platform Project of Xiamen\n(No. 3502Z20231043). We also thank the reviewers for their\ninsightful comments.\nREFERENCES\n[1] S. Kim, P. A. Thiessen, E. E. Bolton, J. Chen, G. Fu, A. Gindulyte,\nL. Han, J. He, S. He, B. A. Shoemaker, et al., “Pubchem substance\nand compound databases,” Nucleic acids research, vol. 44, no. D1,\npp. D1202–D1213, 2016.\n[2] C. Edwards, C. Zhai, and H. Ji, “Text2mol: Cross-modal molecule\nretrieval with natural language queries,” in Proceedings of the 2021\nConference on Empirical Methods in Natural Language Processing,\npp. 595–607, 2021.\n[3] Z. Zeng, Y. Yao, Z. Liu, and M. Sun, “A deep-learning system bridging\nmolecule structure and biomedical text with comprehension comparable\nto human professionals,” Nature communications, vol. 13, no. 1, p. 862,\n2022.\n[4] C. Edwards, T. Lai, K. Ros, G. Honke, K. Cho, and H. Ji,\n“Translation between molecules and natural language,” arXiv preprint\narXiv:2204.11817, 2022.\n[5] Z. Liu, W. Zhang, Y. Xia, L. Wu, S. Xie, T. Qin, M. Zhang, and T.-Y.\nLiu, “Molxpt: Wrapping molecules with text for generative pre-training,”\narXiv preprint arXiv:2305.10688, 2023.\n[6] D. Weininger, “Smiles, a chemical language and information system. 1.\nintroduction to methodology and encoding rules,” Journal of chemical\ninformation and computer sciences, vol. 28, no. 1, pp. 31–36, 1988.\n[7] B. Su, D. Du, Z. Yang, Y. Zhou, J. Li, A. Rao, H. Sun, Z. Lu, and J.-R.\nWen, “A molecular multimodal foundation model associating molecule\ngraphs with natural language,” arXiv preprint arXiv:2209.05481, 2022.\n[8] S. Liu, W. Nie, C. Wang, J. Lu, Z. Qiao, L. Liu, J. Tang, C. Xiao, and\nA. Anandkumar, “Multi-modal molecule structure–text model for text-\nbased retrieval and editing,” Nature Machine Intelligence, vol. 5, no. 12,\npp. 1447–1457, 2023.\n[9] W. Zhao, D. Zhou, B. Cao, K. Zhang, and J. Chen, “Adversarial\nmodality alignment network for cross-modal molecule retrieval,” IEEE\nTransactions on Artificial Intelligence, 2023.\n[10] Y. Zhang, G. Ye, C. Yuan, B. Han, L.-K. Huang, J. Yao, W. Liu,\nand Y. Rong, “Atomas: Hierarchical alignment on molecule-text\nfor unified molecule understanding and generation,” arXiv preprint\narXiv:2404.16880, 2024.\n[11] D. Christofidellis, G. Giannone, J. Born, O. Winther, T. Laino, and\nM. Manica, “Unifying molecular and textual representations via multi-\ntask language modelling,” in International Conference on Machine\nLearning, pp. 6140–6157, PMLR, 2023.\n[12] Z. Liu, S. Li, Y. Luo, H. Fei, Y. Cao, K. Kawaguchi, X. Wang, and T.-\nS. Chua, “Molca: Molecular graph-language modeling with cross-modal\nprojector and uni-modal adapter,” arXiv preprint arXiv:2310.12798,\n2023.\n[13] J. Song, W. Zhuang, Y. Lin, L. Zhang, C. Li, J. Su, S. He, and X. Bo,\n“Towards Cross-Modal Text-Molecule Retrieval with Better Modality\nAlignment,” arXiv e-prints, p. arXiv:2410.23715, Oct. 2024.\n[14] Y. Luo, K. Yang, M. Hong, X. Liu, and Z. Nie, “Molfm: A multimodal\nmolecular foundation model,” arXiv preprint arXiv:2307.09484, 2023.\n[15] P. Liu, Y. Ren, J. Tao, and Z. Ren, “Git-mol: A multi-modal large\nlanguage model for molecular science with graph, image, and text,”\nComputers in Biology and Medicine, vol. 171, p. 108073, 2024.\n[16] S. Li, Z. Liu, Y. Luo, X. Wang, X. He, K. Kawaguchi, T.-S. Chua, and\nQ. Tian, “Towards 3d molecule-text interpretation in language models,”\narXiv preprint arXiv:2401.13923, 2024.\n[17] F. Fan, Y. Feng, and D. Zhao, “Multi-grained attention network for\naspect-level sentiment classification,” in Proceedings of the 2018 con-\nference on empirical methods in natural language processing, pp. 3433–\n3442, 2018.\n[18] B. Zhang, D. Xiong, J. Su, and Y. Qin, “Alignment-supervised bidimen-\nsional attention-based recursive autoencoders for bilingual phrase repre-\nsentation,” IEEE transactions on cybernetics, vol. 50, no. 2, pp. 503–513,\n2018.\n[19] B. Zhang, D. Xiong, and J. Su, “Battrae: Bidimensional attention-based\nrecursive autoencoders for learning bilingual phrase embeddings,” in\nProceedings of the AAAI conference on artificial intelligence, vol. 31,\n2017.\n[20] J. Su, D. Xiong, B. Zhang, Y. Liu, J. Yao, and M. Zhang, “Bilingual cor-\nrespondence recursive autoencoder for statistical machine translation,”\nin Proceedings of the 2015 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 1248–1258, 2015.\n[21] J. Su, B. Zhang, D. Xiong, R. Li, and J. Yin, “Convolution-enhanced\nbilingual recursive neural network for bilingual semantic modeling,” in\nProceedings of COLING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers, pp. 3071–3081, 2016.\n[22] B. Zhang, D. Xiong, J. Su, H. Duan, and M. Zhang, “Bilingual\nautoencoders with global descriptors for modeling parallel sentences,”\nin Proceedings of COLING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers, pp. 2548–2558, 2016.\n[23] Y. Ma, G. Xu, X. Sun, M. Yan, J. Zhang, and R. Ji, “X-clip: End-\nto-end multi-grained contrastive learning for video-text retrieval,” in\nProceedings of the 30th ACM International Conference on Multimedia,\npp. 638–647, 2022.\n[24] P. Jin, H. Li, Z. Cheng, J. Huang, Z. Wang, L. Yuan, C. Liu, and J. Chen,\n“Text-video retrieval with disentangled conceptualization and set-to-set\nalignment,” arXiv preprint arXiv:2305.12218, 2023.\n[25] Z. Wang, Y.-L. Sung, F. Cheng, G. Bertasius, and M. Bansal, “Unified\ncoarse-to-fine alignment for video-text retrieval,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 2816–\n2827, 2023.\n[26] I. Beltagy, K. Lo, and A. Cohan, “Scibert: A pretrained language model\nfor scientific text,” arXiv preprint arXiv:1903.10676, 2019.\n[27] X. Zang, X. Zhao, and B. Tang, “Hierarchical molecular graph self-\nsupervised learning for property prediction,” Communications Chem-\nistry, vol. 6, no. 1, p. 34, 2023.\n[28] J. Degen, C. Wegscheid-Gerlach, A. Zaliani, and M. Rarey, “On the art\nof compiling and using’drug-like’chemical fragment spaces,” ChemMed-\nChem, vol. 3, no. 10, p. 1503, 2008.\n[29] T. N. Kipf and M. Welling, “Semi-supervised classification with graph\nconvolutional networks,” arXiv preprint arXiv:1609.02907, 2016.\n[30] G. Peyr´e, M. Cuturi, et al., “Computational optimal transport: With\napplications to data science,” Foundations and Trends® in Machine\nLearning, vol. 11, no. 5-6, pp. 355–607, 2019.\n[31] G. Luise, A. Rudi, M. Pontil, and C. Ciliberto, “Differential properties\nof sinkhorn approximation for learning with wasserstein distance,”\nAdvances in Neural Information Processing Systems, vol. 31, 2018.\n[32] Y. Xie, X. Wang, R. Wang, and H. Zha, “A fast proximal point method\nfor computing exact wasserstein distance,” in Proceedings of The 35th\nUncertainty in Artificial Intelligence Conference (R. P. Adams and\nV. Gogate, eds.), vol. 115 of Proceedings of Machine Learning Research,\npp. 433–453, PMLR, 22–25 Jul 2020.\n[33] J. Han and J. Pei, “Mining frequent patterns by pattern-growth: method-\nology and implications,” ACM SIGKDD explorations newsletter, vol. 2,\nno. 2, pp. 14–20, 2000.\n[34] RDKit, “Rdkit: Open-source cheminformatics.” https://www.rdkit.org.\n[35] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.",
    "pdf_filename": "Exploring_Optimal_Transport-Based_Multi-Grained_Alignments_for_Text-Molecule_Retrieval.pdf"
}