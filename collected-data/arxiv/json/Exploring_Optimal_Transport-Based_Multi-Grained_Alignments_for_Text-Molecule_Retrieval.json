{
    "title": "Exploring Optimal Transport-Based Multi-Grained",
    "abstract": "progress, making the cross-modal text-molecule retrieval task Text Query Molecule increasingly vital. This task focuses on accurately retrieving Query Water is an oxygen hydride consisting of an molecule structures based on textual descriptions, by effectively oxygen atom that is covalently bonded to H!O aligning textual descriptions and molecules to assist researchers two hydrogen atoms. Retrieve in identifying suitable molecular candidates. However, many Retrieve existingapproachesoverlookthedetailsinherentinmoleculesub- Text Candidates structures. In this work, we introduce the Optimal TRansport- Molecule Candidates Water is an oxygen hydride consisting based Multi-grained Alignments model (ORMA), a novel ap- of an oxygen atom that is covalently bonded to two hydrogen atoms. proach that facilitates multi-grained alignments between textual descriptions and molecules. Our model features a text encoder Hydrogen peroxide is a colorless liquid and a molecule encoder. The text encoder processes textual H!O H!O! C!H\"O at room temperature with a bitter taste. descriptions to generate both token-level and sentence-level representations, while molecules are modeled as hierarchical Fig. 1. The text-molecule retrieval task is designed to retrieve molecules heterogeneous graphs, encompassing atom, motif, and molecule based on text queries, while molecule-text retrieval task does the opposite. Theredboxindicatestheground-truthretrievalresult. nodes to extract representations at these three levels. A key innovation in ORMA is the application of Optimal Transport (OT) to align tokens with motifs, creating multi-token represen- tations that integrate multiple token alignments with their cor- Generally,existingstudiesmainlyfocusontheutilizationof responding motifs. Additionally, we employ contrastive learning neuralnetworkstolearnrepresentationsoftextualdescriptions to refine cross-modal alignments at three distinct scales: token- and molecules, and then calculating text-molecule similari- atom,multitoken-motif,andsentence-molecule,ensuringthatthe ties for retrieval. For example, several studies [3]–[5] resort similarities between correctly matched text-molecule pairs are to pretrained models based on Simplified Molecular Input maximized while those of unmatched pairs are minimized. To our knowledge, this is the first attempt to explore alignments Line Entry Specification (SMILES) [6] and text sequences. at both the motif and multi-token levels. Experimental results Alternatively, more studies like MoMu [7] and MoleculeSTM on the ChEBI-20 and PCdes datasets demonstrate that ORMA [8] represent molecules as 2D topological graphs, and then significantlyoutperformsexistingstate-of-the-art(SOTA)models. employ cross-modal contrastive learning to align molecular Specifically, in text-molecule retrieval on ChEBI-20, our model graphs and textual descriptions within a shared semantic achieves a Hits@1 score of 66.5%, surpassing the SOTA model AMAN by 17.1%. Similarly, in molecule-text retrieval, ORMA space. Furthermore, AMAN [9] utilizes adversarial learning secures a Hits@1 score of 61.6%, outperforming AMAN by to effectively bridge these two modalities, achieving state-of- 15.0%. the-art (SOTA) performance. Index Terms—Text-molecule Retrieval, Multi-grained Repre- Despite these advancements, most studies overlook the de- sentation Learning, Cross-modal Alignment, Optimal Transport tailedstructuralinformationessentialforunderstandingmolec- ular properties. Molecules are composed of atoms connected I. INTRODUCTION by chemical bonds, and their properties can be influenced by their structural motifs. In molecular chemistry, a motif is The rapid advancement of bioinformatics has led to the definedasaspecificgroupofbondedatomsthatfollowsacon- construction of numerous large-scale molecular databases, sistent and repeating pattern. Ignoring such detailed structural such as PubChem [1]. These databases play a crucial role in information seriously limits the precision of retrieval results. thediscoveryandsynthesisofnewdrugs.However,accurately The only exception is the recently proposed Atomas [10], retrieving desired molecules from these databases presents a which applies clustering algorithms to extract representations significant challenge. Therefore, aiming to retrieve molecules at multiple granularities. based on text queries, cross-modal text-molecule retrieval [2] To address the above issue, we propose a novel text- has become increasingly important. molecule model with Optimal TRansport-based Multi- grained Alignments (ORMA). Overall, our model contains *Equalcontribution. †Correspondingauthor. a SciBERT-based text encoder and a GCN-based molecule 4202 voN 4 ]RI.sc[ 1v57811.1142:viXra",
    "body": "Exploring Optimal Transport-Based Multi-Grained\nAlignments for Text-Molecule Retrieval\nZijun Min1*, Bingshuai Liu1*, Liang Zhang1, Jia Song1, Jinsong Su1†, Song He2†, Xiaochen Bo2†\n1School of Informatics, Xiamen University, Xiamen, China\n2Institute of Health Service and Transfusion Medicine, Beijing, China\n{minzijun, bsliu, lzhang, songjia}@stu.xmu.edu.cn, jssu@xmu.edu.cn, {hes1224, boxiaoc}@163.com\nAbstract—The field of bioinformatics has seen significant Text-Molecule Retrieval Molecule-Text Retrieval\nprogress, making the cross-modal text-molecule retrieval task Text Query Molecule\nincreasingly vital. This task focuses on accurately retrieving Query\nWater is an oxygen hydride consisting of an\nmolecule structures based on textual descriptions, by effectively oxygen atom that is covalently bonded to H!O\naligning textual descriptions and molecules to assist researchers two hydrogen atoms.\nRetrieve\nin identifying suitable molecular candidates. However, many\nRetrieve\nexistingapproachesoverlookthedetailsinherentinmoleculesub- Text Candidates\nstructures. In this work, we introduce the Optimal TRansport- Molecule Candidates Water is an oxygen hydride consisting\nbased Multi-grained Alignments model (ORMA), a novel ap- of an oxygen atom that is covalently\nbonded to two hydrogen atoms.\nproach that facilitates multi-grained alignments between textual\ndescriptions and molecules. Our model features a text encoder Hydrogen peroxide is a colorless liquid\nand a molecule encoder. The text encoder processes textual\nH!O H!O! C!H\"O at room temperature with a bitter taste.\ndescriptions to generate both token-level and sentence-level\nrepresentations, while molecules are modeled as hierarchical Fig. 1. The text-molecule retrieval task is designed to retrieve molecules\nheterogeneous graphs, encompassing atom, motif, and molecule based on text queries, while molecule-text retrieval task does the opposite.\nTheredboxindicatestheground-truthretrievalresult.\nnodes to extract representations at these three levels. A key\ninnovation in ORMA is the application of Optimal Transport\n(OT) to align tokens with motifs, creating multi-token represen-\ntations that integrate multiple token alignments with their cor- Generally,existingstudiesmainlyfocusontheutilizationof\nresponding motifs. Additionally, we employ contrastive learning neuralnetworkstolearnrepresentationsoftextualdescriptions\nto refine cross-modal alignments at three distinct scales: token- and molecules, and then calculating text-molecule similari-\natom,multitoken-motif,andsentence-molecule,ensuringthatthe\nties for retrieval. For example, several studies [3]–[5] resort\nsimilarities between correctly matched text-molecule pairs are\nto pretrained models based on Simplified Molecular Input\nmaximized while those of unmatched pairs are minimized. To\nour knowledge, this is the first attempt to explore alignments Line Entry Specification (SMILES) [6] and text sequences.\nat both the motif and multi-token levels. Experimental results Alternatively, more studies like MoMu [7] and MoleculeSTM\non the ChEBI-20 and PCdes datasets demonstrate that ORMA [8] represent molecules as 2D topological graphs, and then\nsignificantlyoutperformsexistingstate-of-the-art(SOTA)models.\nemploy cross-modal contrastive learning to align molecular\nSpecifically, in text-molecule retrieval on ChEBI-20, our model\ngraphs and textual descriptions within a shared semantic\nachieves a Hits@1 score of 66.5%, surpassing the SOTA model\nAMAN by 17.1%. Similarly, in molecule-text retrieval, ORMA space. Furthermore, AMAN [9] utilizes adversarial learning\nsecures a Hits@1 score of 61.6%, outperforming AMAN by to effectively bridge these two modalities, achieving state-of-\n15.0%. the-art (SOTA) performance.\nIndex Terms—Text-molecule Retrieval, Multi-grained Repre-\nDespite these advancements, most studies overlook the de-\nsentation Learning, Cross-modal Alignment, Optimal Transport\ntailedstructuralinformationessentialforunderstandingmolec-\nular properties. Molecules are composed of atoms connected\nI. INTRODUCTION by chemical bonds, and their properties can be influenced\nby their structural motifs. In molecular chemistry, a motif is\nThe rapid advancement of bioinformatics has led to the\ndefinedasaspecificgroupofbondedatomsthatfollowsacon-\nconstruction of numerous large-scale molecular databases,\nsistent and repeating pattern. Ignoring such detailed structural\nsuch as PubChem [1]. These databases play a crucial role in\ninformation seriously limits the precision of retrieval results.\nthediscoveryandsynthesisofnewdrugs.However,accurately\nThe only exception is the recently proposed Atomas [10],\nretrieving desired molecules from these databases presents a\nwhich applies clustering algorithms to extract representations\nsignificant challenge. Therefore, aiming to retrieve molecules\nat multiple granularities.\nbased on text queries, cross-modal text-molecule retrieval [2]\nTo address the above issue, we propose a novel text-\nhas become increasingly important.\nmolecule model with Optimal TRansport-based Multi-\ngrained Alignments (ORMA). Overall, our model contains\n*Equalcontribution.\n†Correspondingauthor. a SciBERT-based text encoder and a GCN-based molecule\n4202\nvoN\n4\n]RI.sc[\n1v57811.1142:viXra\nencoder, to individually learn textual and molecular represen- molecular sequences, emerging many typical pre-training\ntations at multiple granularities for retrieval. models, such as KV-PLM [3], MolT5 [4], and Text+Chem\nConcretely,weutilizethetextencodertoprocesseachinput T5 [11]. Meanwhile, some studies switch their attention to\ntextual description, obtaining token and sentence represen- 2D topological graphs, where atoms and chemical bonds are\ntations. Simultaneously, we decompose each input molecule considered as nodes and edges, respectively. For example,\ninto motifs based on chemical rules, and then represent it as studies like MoMu [7] and MoleculeSTM [8] employ cross-\nan undirected heterogeneous graph. In this graph, each node modal contrastive learning to align text and molecular graphs\nrepresentseitheranatom,motif,orglobalmoleculenode,with in a shared semantic space. Moreover, MolCA [12] intro-\ntwo types of edges modeling the relationships between atom duces a cross-modal projector, while AMAN [9] and [13]\nand motif, and motif and molecule, respectively. Based on employ adversarial learning to bridge these two modalities\nthis graph, we employ the molecule encode to encoder the effectively. As the extension of the above studies, several\ninput molecule at multiple levels, obtaining atom, motif, and studiesincorporateadditionalmodalitiestoaidthealignments\nmolecule representations. betweentextandmolecules,suchasMolFM[14]incorporated\nSubsequently,wetreatthetokensandmotifsasindependent knowledge graphs, and GIT-Mol [15] incorporated images.\ndistributions and employ optimal transport to achieve their Recently, some studies focus on the spatial information in 3D\nalignments. Furthermore, for each motif, we obtain a fused molecularconformations.Forinstance,3D-MoLM[16]adopts\nrepresentation of its aligned tokens as the multi-token repre- asimilararchitecturetoMolCA[12],using3Dconformations\nsentation. to represent molecules.\nWith the previously mentioned cross-modal representations Despite these advancements, most methods only focus on\nat different granularities, we introduce contrastive learning global molecular information, often overlooking detailed in-\nlosses in our model training to achieve alignments at three formation at the motif and atom levels. To integrate detailed\nlevels: token-atom, multitoken-motif, and sentence-molecule. information, multi-grained alignments have been explored in\nThese losses aim to maximize similarity scores for matched various natural language processing (NLP) [17]–[22] and\ntext-molecule pairs while minimizing those for unmatched computer vision (CV) [23]–[25] tasks, enabling models to\npairs, thereby enhancing alignments across different modal- effectively process complex information. However, in the task\nities. During inference of text-molecule retrieval, we calculate of text-molecule retrieval, few studies pay attention to this\nsimilarities at three levels, and combine them to retrieve the technique.TheonlyexceptionisAtomas[10]whichappliesa\nmoleculewiththehighestsimilarity.Inmolecule-textretrieval clustering algorithm to extract features at the atom, fragment,\ntask, we perform this process in the opposite direction. and molecule levels.\nTosummarize,themaincontributionsofourworkarethree- AlthoughwiththesamemotivationasAtomas,weconstruct\nfold: a hierarchical heterogeneous molecular graph to obtain atom,\n• WeproposeORMA,designedtolearntextualandmolec- motif, and molecule representations. More importantly, we\nularrepresentationsatmultiplelevels.Itemploysmultiple introduce optimal transport to fuse the representations of\nlosses to align cross-modal representations at different multiple tokens aligned with the same motif. Through con-\nlevels, thereby enhancing cross-modal alignments for trastive learning, we align text and molecules at token-atom,\nretrieval. multitoken-motif, and sentence-molecule levels. Subsequent\n• We model the alignments between tokens and motifs experimentalresultsshowthatourmodeloutperformsAtomas,\nas an optimal transport problem to learn multi-token confirming the effectiveness of our model.\nrepresentations for each motif, facilitating subsequent\nIII. OURMODEL\nmultitoken-motif alignments. To the best of our knowl-\nedge,ourworkisthefirsttoexplorethemultitoken-motif In this section, we will provide a detailed description of\nalignments in text-molecule retrieval. our model. We first present the main architecture of our\n• ExperimentalresultsontheChEBI-20andPCdesdatasets model, which mainly contains a text encoder and a molecule\ndemonstratethatourmodelachievessignificantimprove- encodertolearnrepresentationsatdifferentgranularities.Next,\nments over the state-of-the-art (SOTA) models. Specif- we further introduce Optimal Transport (OT) to generate\nically, for the text-molecule retrieval on ChEBI-20, the the fused representation of multiple tokens aligned with the\nHits@1 score of our model is 66.5%, outperforming the same motifs. To the best of our knowledge, our work is\nSOTA model–AMAN by 17.1%. Similarly, in the task the first attempt to consider representations at the motif and\nof molecule-text retrieval, our model achieves a Hits@1 multi-token levels. Finally, we define a training objective that\nscore of 61.6%, surpassing AMAN by 15.0%. involves three alignment losses to achieve cross-modal multi-\ngrained alignments.\nII. RELATEDWORK\nA. Main Architecture\nToachievehigh-qualitytext-moleculeretrieval,moststudies\nrepresentmoleculesas1Dsequences,2Dmoleculargraphs,or As shown in Figure 2, our model mainly consists of a\n3D molecular conformations. In the aspect of 1D molecule molecule encoder and a text encoder. As implemented in\nmodeling, SMILES [6] has been widely used to represent previous studies [2], [9], we also employ SciBERT [26]\n𝑳\natom 𝒕𝒂\nnode Atom Token N-acetyl-L-alanine\nRepresentations Representations\nis an N-acetyl-L-\nFusion\namino acid that is L-\nMolecule Text alanine in which one\nmotif 𝑳\nnode Encoder Motif 𝒎𝒎 Multi-token Encoder of the hydrogens\nRepresentations Representations attached to the\nnitrogen is replaced\n[CLS] by an acetyl group...\nmolecule\nnode 𝑳 $𝒎\nMolecule Sentence\nRepresentation Representation\nFig.2. Theillustrationofourmodel.Inshort,ourmodelcomprisesatextencoderandamoleculeencoderanddesignsthreealignmentlossesLta,Lmm,Lsm\nat token-atom, multitoken-motif, and sentence-molecule levels, respectively. On the left side of the figure is a molecular graph, where the nodes at the top\nrepresentatomnodes.Inthemiddle,eachnodewiththesolidedgeisamotifnode,connectedtotheatomnodesitcontains.Atthebottom,thenodewith\nthedashededgeisthemoleculenode,connectedtoallmotifnodes.\nto encode the input textual descriptions, obtaining represen- indicated as a node with the dashed edge, the molecule\ntations at both sentence and token levels. Compared with node is connected to all motif nodes. This heterogeneous\nother pretraining models, SciBERT outperforms in encoding graph allows for enhancing the information sharing among\nchemical textual descriptions, due to pretraining on a large- atom, motif, and molecule nodes during the neighborhood\nscale corpus of scientific publications. Given an input textual aggregation of graph neural networks, thus facilitating multi-\ndescription with N tokens, we first concatenate a special grained molecular representation learning.\nt\n[CLS] token at the beginning of the sequence. Then, we input Based on the above heterogeneous graph, we employ a\nthe sequence into the SciBERT encoder, where the learned 3-layer Graph Convolutional Network (GCN) [29] to learn\nd-dimensional representation of [CLS] serves as the sentence node representations, which correspond to molecular repre-\nrepresentation hs, and those of remaining tokens are used as sentations at three granularities: (1) the atom representations\ntok Te on er fe fp er ce tis ve en lt yat eio nn cs o: deH tt he= in[h pt\n1\nu, th mt 2, o. le. c. u, lh et\nN ,t\nw] e∈ reR pN ret s× ed n.\nt itas\nH thea n= um[h ba\n1\ne, rh oa\n2\nf,. a. t. o, mh sa\nN ,a\n(] 2∈\n)\ntR heNa m× od t, ifw rh ee pr re esN\nena\ntai ts iod ne sno Hte md a =s\na len aru nnd iti sre rc et pe rd esh ee nt te ar to iog ne sne ao tu ds ifg fera rep nh ta gn rd anu us le ara itiG esC .N The enc mo od le er cuto\n-\nm[h om 1 ti, fh ,m 2 an, d.. (. 3, )h tm N hm e] gl∈ obaR lN mm o× led c, ulw ei rth eprN esm eni ts atit oh ne hn gum ∈b Re dr .of\nlargraphcontainsthreetypesofnodes,constructedasfollows:\n(1) Atom nodes. Since each molecule consists of atoms B. Optimal Transport-Based Multi-token Fused Representa-\nconnected by chemical bonds, we include each atom as an tion Learning\nindividualnode,whichenablesustofullycapturethedetailed\nstructural information of the molecule. (2) Motif nodes. In As previously mentioned, a crucial aspect of our model is\ntheconsiderationofthefusedrepresentationofmultipletokens\nmolecular chemistry, a motif is defined as a specific group of\naligned with the corresponding motif. To this end, we model\nbonded atoms that follows a consistent and repeating pattern.\nthe alignments between the input tokens and motifs as the\nThus, we believe that motifs encode rich implicit semantic\nOptimal Transport (OT) problem, as illustrated in Figure 4.\ninformation, which is crucial for understanding the molecular\nAs a typical machine learning problem, OT aims to find the\nproperties described in the text. Following previous study\nscheme that minimizes the transportation cost of transferring\n[27], we employ the BRICS algorithm [28] with an additional\none distribution to another distribution [30]. Concretely, we\ndecomposition rule to extract motifs, all of which are also\nincludedasseparatemotifnodes.(3)Molecule node.Wealso first consider the following crucial definitions: (1) We regard\nthe token representations Ht and motif representations Hm\ninclude a global molecule node to facilitate the learning of a\nas two independent distributions. (2) C = 1−cos(ht,hm)\ncomprehensivemoleculerepresentation.Toeffectivelycapture ij i j\nis the transportation cost from ht to hm, where cos(∗,∗) is a\nthe relationships between nodes at different granularities, we i j\nconsider two types of edges: (1) Motif-Atom Edges. Each cosine distance function. (3) We define T={T ij}, 1≤i≤N t,\n1≤j≤N , as an assignment plan, learned to optimize the\nmotif node is connected to its constituent atom nodes. (2) m\nMolecule-Motif Edges. The molecule node is connected to alignments between input tokens and motifs. Solving the\noptimaltransportproblemisequivalenttoaddressingaspecific\nall motif nodes.\nnetwork-flow problem [31] as follows:\nFor example, in the molecular graph shown on the left part\nof Figure 2, each node at the top represents an atom node,\nand each node with the solid edge in the middle is a motif\nmin(cid:88)Nt (cid:88)Nm\nT C =minTr(TTC ), (1)\nij ij ij ij\nnode, connected to the atom nodes it contains. Meanwhile,\ni=1j=1\nToken Representations\nℎ!\"ℎ#\"ℎ$\"ℎ%\"ℎ&\" Atom Representations of Sample 𝑘 Token Representations\nℎ!’ℎ#’ℎ$’ℎ%’ℎ&’ℎ(’ℎ)’ℎ*’ℎ+’ ℎ!\"ℎ#\"ℎ$\"ℎ%\"ℎ&\" Atom Representations of Sample 𝑘\nRe ℎ ℎ ℎp ! # $- - -reM seo nti tf a Iti to I ePn rs aO tiT\no n\nMC ao ts rt i x SeT lo ek cte in o n ℎ!\" ℎ#\" ℎ$\" ℎ%\" ℎ&\" FuR sieM op nru el sti e- nto t ℎ P Pak o o#\"te i o oon l li in n n g gs ( (ℎ ℎ!$\" \",, ℎℎ %\"&\" )) Token o fR Se ℎ ℎ ℎ ℎ ℎap ! # $ % &\" \" \" \" \"mre ps le en 𝑞tations Norm .\nRU\ne\nop\np\nfd\nr\nSea aste\ne\nmd\nn p\ntA\na\nlett\nℎ ℎ ℎ ℎ ℎi\no\no ! # $ % &’ ’ ’ ’\n’𝑘m\n, , , , ,ns Re ℎ ℎ ℎp ! # $- - -reM seo nti tf a Iti to I ePn rs aO tiT\no n\nMC ao ts rt i x SeT lo ek cte in o n ℎ!\" ℎ#\" ℎ$\" ℎ%\" ℎ&\"FuR sieM op nru el sti e- nto t ℎ P Pak o o#\"te i o oon l li in n n g gs ( (ℎ ℎ!$\" \",, ℎℎ %\"&\" )) Token o fR Se ℎ ℎ ℎ ℎ ℎap ! # $ % &\" \" \" \" \"mre ps le en 𝑞tations\nℎ!’ℎ#’ℎ$’ℎ%’ℎ&’ℎ(’ℎ)’ℎ*’ℎ+’\nNorm . RU e op p fd r Sea aste e md n p tA a lett ℎ ℎ ℎ ℎ ℎi o o ! # $ % &’ ’ ’ ’ ’𝑘m , , , , ,ns\nToken-Atom Similarity Matrix Token-Atom Similarity Matrix\nSum Sum\nSum Sum\nPooling Pooling\nPooling Pooling\nOptimal Maximum Values Similarity 𝑆#$ AssiO gnp mtim ena tl P lan AO lip gt ni mm ea nl t Similarity 𝑆!#$ \"\nAssignment Plan of Each Token !\"\nFig. 4. The process of obtaining multi-token representations through OT.\nFig.3. Theprocessoftoken-atomalignments.Withinabatch,weupdatethe\nConsidering the alignments between token representations and motif repre-\natomrepresentationsofeachsampleandthencalculatethetoken-atomlevel\nsentationsasanoptimaltransportproblem,wefusethetokenrepresentations\nsimilaritiesbetweensamples.Thenweusecontrastivelearningtomaximize\nintomulti-tokenrepresentationscorrespondingtospecificmotifs.\nthesimilaritiesbetweenmatchedtext-moleculepairswhileminimizingthose\nbetweenunmatchedpairs.\ndimensions of token and atom representations. After mul-\nwhere Tr(TTC ) represents the Frobenius dot-product. The tiplying these alignment weights with atom representations,\nij ij\nexactoptimaltransportproblemgenerallyposescomputational we update the atom representations as {ha′} by integrating\ni\nchallenges due to its complexity. To address this challenge, textual information. Finally, we apply sum pooling to reduce\nwe introduce the Inexact Proximal Point Method for Optimal the dimensions of both representations, which are then used\nTransport (IPOT) [32], which approximately solves the opti- to calculate the final similarity at the token-atom level.\nmal transport problem to obtain an optimal assignment plan. Within a batch of size B, we compute similarities {Sta},\nqk\nTherefore, for the i-th token, we determine its optimal 1≤q≤B, 1≤k≤B, between samples at the token-atom level.\nalignment motif index as a(i) = argminTr(TTC ). Then Following this, we define the token-atom alignment loss L\nij ij ta\n1≤j≤Nm astheaverageoftwoCategoricalCross-Entropy(CCE)losses\nlet D ={i|a(i)=j,1≤i≤N } denote the index set, where the\nj t corresponding to text-molecule and molecule-text retrieval\ncorresponding tokens are aligned to the j-th motif. Sub-\ntasks, respectively:\nsequently, we average the representations of tokens occur-\nring in D to obtain their fused multi-token representation:\nhp = 1 j I(a(i) = j)ht, where I(·) is the indicator function. L =− 1 (cid:88)B log exp(St ka k)\nAj nalog|D ouj|\nsly, we can\ni\nobtain all motif-aligned multi-token\nta 2B (cid:80)B exp(Sta)\nk=1 q=1 qk\nr Nepre isse tn ht eat nio un ms ba es\nr\noH fp m= ulti[ -h top 1 k,h enp 2 s, ....,hp Np] ∈ RNp×d, where\n−\n1 (cid:88)B\nlog\nexp(St ka k)\n. (3)\np 2B (cid:80)B exp(Sta)\nk=1 q=1 kq\nC. Model Training and Inference\n2) Multitoken-MotifAlignmentLossL : Wealsoemploy\nmm\nThrough the above steps, we obtain the representations of\ncontrastive learning to achieve alignments between multi-\ninput texts and molecules at different granularities. Further,\ntokens and motifs. Specifically, in a similar way, we calculate\nwe define the following training objective:\nthesimilarities{Smm},1≤q≤B,1≤k≤B,atmulti-tokenand\nqk\nmotif level between samples within the same batch. Thus,\nL=α·L +β·L +(1−α−β)·L , (2)\nta mm sm\nthe multitoken-motif alignment loss L is formulated as\nmm\nwhere L ,L ,L denote the token-atom, multitoken- follows:\nta mm sm\nmotif, and sentence-molecule alignment losses, and α,β are\ntwo coefficients balancing the effects of different losses, 1 (cid:88)B exp(Smm)\nL =− log kk\nrespectively.Inthefollowing,wewilldetailthesethreelosses. mm 2B (cid:80)B exp(Smm)\nk=1 q=1 qk\n1) Token-Atom Alignment Loss L : To ensure precise\nalignments between tokens and\natoms,ta\nwe introduce a token-\n−\n1 (cid:88)B\nlog\nexp(Sm kkm)\n. (4)\natomalignmentlossbasedoncontrastivelearning.Concretely, 2B (cid:80)B exp(Smm)\nk=1 q=1 kq\nas shown in Figure 3, we compute the token-atom similarity\nmatrix between token representations {ht} and atom repre- 3) Sentence-Molecule Alignment Loss L : Similar to\ni sm\nsentations {ha}. The similarity matrix is then normalized other levels of alignments, we employ contrastive learning to\nj\nby the min-max normalization along the atom dimension. compute the CCE loss at sentence-molecule level. Concretely,\nSubsequently,wefurthernormalizethematrixalongthetoken we maximize the similarities between matched sentence and\ndimension to obtain the alignment weights, used to unify the molecule pairs, while minimizing those between unmatched\nTABLEI\nPERFORMANCEONTHECHEBI-20DATASETFORTEXT-MOLECULEANDMOLECULE-TEXTRETRIEVALTASKS.THEBOLDPARTINDICATESTHEBEST\nPERFORMANCE.ORMAACHIEVESLEADINGPERFORMANCEINBOTHRETRIEVALTASKS.↑DENOTESTHATTHEHIGHERISTHEBETTER,WHILE↓\nDENOTESTHATTHELOWERISTHEBETTER.†REPRESENTSOURREPRODUCEDRESULTSOFATOMAS-BASEANDTHEOTHERRESULTSAREREPORTEDIN\nTHEPREVIOUSWORKS.\nText-MoleculeRetrieval Molecule-TextRetrieval\nModels\nHits@1(↑) Hits@10(↑) MRR(↑) MeanRank(↓) Hits@1(↑) Hits@10(↑) MRR(↑) MeanRank(↓)\nMLP-Ensemble[2] 29.4% 77.6% 0.452 20.78 - - - -\nGCN-Ensemble[2] 29.4% 77.1% 0.447 28.77 - - - -\nAll-Ensemble[2] 34.4% 81.1% 0.499 20.21 25.2% 74.1% 0.408 21.77\nMLP+Atten[2] 22.8% 68.7% 0.375 30.37 - - - -\nMLP+FPG[33] 22.6% 68.6% 0.374 30.37 - - - -\nAMAN[9] 49.4% 92.1% 0.647 16.01 46.6% 91.6% 0.625 16.50\nAtomas-base† [10] 50.1% 92.1% 0.653 14.49 45.6% 90.3% 0.614 15.12\nORMA(Ours) 66.5% 93.9% 0.772 18.53 61.6% 93.8% 0.739 8.10\n(R)-nephthenol is a cembrane diterpenoid\nIV. EXPERIMENTS\nobtained by regio-and stereoselective hydration\nA. Dataset and Evaluation\nof the exocyclic double bond of cembrene C. It\nhas a role as a bacterial metabolite and a coral\nOurmodelistrainedandevaluatedontwodatasets:ChEBI-\nmetabolite. It is a macrocycle, a cembrane\n20 dataset [2] and PCdes dataset [3]. The ChEBI-20 dataset\nditerpenoid, a tertiary alcohol and an olefinic\n(R)-nephthenol compound. It derives from a cembrene C. contains 33,010 molecules with textual descriptions, divided\ninto training, validation, and test sets in the ratio of 8:1:1.\nL-histidinol is an amino alcohol that is propanol Following previous studies [2], [9], we evaluate the retrieval\nCC1=CCCC(=CCC(CCC(=CCC1)C)C(C)(C)O)C substituted by 1H-imidazol-4-yl group at position performance using Hits@1, Hits@10, mean reciprocal rank\n3 and an amino group at position 2 (the 2S\n(MRR), and mean rank. During inference, test samples are\nstereoisomer). It is a member of imidazoles and\nretrievedfromtheentireChEBI-20dataset.ThePCdesdataset\nan amino alcohol. It is a conjugate base of a L-\nL-histidinol histidinol(1+). consists of 15,000 molecule pairs from PubChem [1], split\nin the ratio of 7:1:2. After excluding 8 molecules whose\nC1=C(NC=N1)CC(CO)N\nFig.5. Thevisualizationforthealignmentsofmotifsandmulti-tokens.The SMILES strings can not be converted into 2D graphs using\nmotifs marked in red and green on the left correspond to the multi-tokens\nRDKit [34], 14,992 instances remain for our experiments.\nhighlightedinredandgreenontheright,respectively.\nFollowing previous works, retrieval performance is evaluated\nusing Recall at 1/10 (R@1, R@10), MRR, and mean rank.\nPrevious pretrained studies [3], [7], [8], [10], [12], [14] often\npairs, with the sentence-molecule alignment loss L sm defined utilize finetuning or directly inference at zero-shot settings\nas follows: on the PCdes dataset. Since our approach does not involve\npretraining, we train our model from scratch on the PCdes\ntraining set and evaluate its performance on the test set.\n1 (cid:88)B exp(Ssm)\nL =− log kk\nsm 2B (cid:80)B exp(Ssm) B. Experimental Settings\nk=1 q=1 qk\n1 (cid:88)B exp(Ssm) Duringmodeltraining,weutilizeapretrainedSciBERT[26]\n− log kk , (5)\n2B (cid:80)B exp(Ssm) as the text encoder with a maximum text length of 256. We\nk=1 q=1 kq employ a 3-layer GCN as the graph encoder with an output\ndimensionof300.WeusetheAdamoptimizer[35],settingthe\nwhere{Ssm},1≤q≤B,1≤k≤B,isdenotedasthesimilarities learning rate of 3e-5 for SciBERT and 1e-4 for the remaining\nqk\nparts of our model. Our training spans 60 epochs and uses a\nbetweenthesentencerepresentationoftheq-thsampleandthe\nbatchsizeof32.Weassignthecoefficientsfordifferentlevels\nmolecule representation of the k-th sample.\nin training objective as follows: α=0.5,β =0.2.\nDuring inference for text-molecule retrieval, we calculate\nthe similarities between text queries and all molecule candi-\nC. Baseline Models\ndatesattheabovethreelevels.Thenweassignthecoefficients\nα,β to these similarities at different levels to obtain the final a) AMAN [9]: It leverages the SciBERT to encode tex-\nsimilarity, retrieving the molecule with the highest similarity. tual descriptions and a Graph Transformer Network (GTN) to\nIn molecule-text retrieval task, we perform this process in the encode molecules. Besides, it introduces adversarial learning\nopposite direction. and triplet loss to align these modalities.\nTABLEII\nRESULTSONTHEPCDESDATASETFORTEXT-MOLECULEANDMOLECULE-TEXTRETRIEVALTASKSATZERO-SHOT,FINETUNINGANDFROM-SCRATCH\nSETTINGS.THEBOLDPARTSINDICATETHEBESTPERFORMANCE.NOTETHATOURMODEL,TRAINEDFROMSCRATCH,SIGNIFICANTLYOUTPERFORMS\nOTHERSINALLMETRICS.↑DENOTESTHATTHEHIGHERISTHEBETTER.\nText-MoleculeRetrieval Molecule-TextRetrieval\nModels\nR@1(↑) R@5(↑) R@10(↑) MRR(↑) R@1(↑) R@5(↑) R@10(↑) MRR(↑)\nPretrainedModel+Zero-shot\nMoMu[7] 4.9% 14.5% 20.7% 0.103 5.1% 12.8% 18.9% 0.099\nMolCA[12] 35.1% 62.1% 69.8% 0.473 38.0% 66.8% 74.5% 0.508\nMolFM[14] 16.1% 30.7% 39.5% 0.236 13.9% 28.7% 36.2% 0.214\nMoleculeSTM[8] 35.8% - - - 39.5% - - -\nAtomas-base[10] 39.1% 59.7% 66.6% 0.473 37.9% 59.2% 65.6% 0.478\nAtomas-large[10] 49.1% 68.3% 73.2% 0.578 46.2% 66.0% 72.3% 0.555\nPretrainedModel+Finetuning\nSciBERT[26] 16.3% 33.9% 42.6% 0.250 15.0% 34.1% 41.7% 0.239\nKV-PLM[3] 18.4% 37.2% 45.4% 0.274 16.6% 35.9% 44.8% 0.260\nKV-PLM*[3] 20.6% 37.9% 45.7% 0.292 19.3% 37.3% 45.3% 0.281\nMoMu[7] 24.5% 45.4% 53.8% 0.343 24.9% 44.9% 54.3% 0.345\nMolFM[14] 29.8% 50.5% 58.6% 0.396 29.4% 50.3% 58.5% 0.393\nFrom-scratch\nORMA(Ours) 64.8% 82.3% 86.3% 0.727 62.1% 81.4% 86.3% 0.710\nTABLEIII\nABLATIONSTUDYONTHECHEBI-20DATASETATTHREELEVELS:TOKEN-ATOM(TA),MULTITOKEN-MOTIF(MM),ANDSENTENCE-MOLECULE(SM).\nTHERESULTSSUGGESTTHATINTEGRATINGTHREELEVELSSIGNIFICANTLYIMPROVESPERFORMANCEINBOTHTEXT-MOLECULEANDMOLECULE-TEXT\nRETRIEVALTASKS.↓DENOTESTHATTHELOWERISTHEBETTER.\nLevel Text-MoleculeRetrieval Molecule-TextRetrieval\nTA MM SM Hits@1(↑) Hits@10(↑) MRR(↑) MeanRank(↓) Hits@1(↑) Hits@10(↑) MRR(↑) MeanRank(↓)\n✓ ✓ ✓ 66.5% 93.9% 0.772 18.53 61.6% 93.8% 0.739 8.1\n✓ ✓ 57.4% 91.6% 0.700 26.74 38.4% 89.4% 0.552 12.87\n✓ ✓ 61.7% 92.5% 0.733 23.10 51.9% 91.3% 0.661 11.13\n✓ ✓ 35.6% 81.8% 0.511 41.39 35.1% 80.8% 0.506 30.95\n✓ 24.1% 75.3% 0.401 42.28 21.8% 72.1% 0.377 30.86\n✓ 17.0% 55.0% 0.291 219.45 20.2% 56.3% 0.319 247.27\n✓ 49.6% 89.1% 0.636 37.17 23.3% 71.6% 0.384 22.64\nb) Atomas [10]: This model is pretrained on large- In the molecule-text retrieval task, our model also demon-\nscale text and SMILES data, aligning textual and molecular strates superior performance, leading in all metrics with a\nmodalitiesatthreegranularities.Atomasutilizestheclustering Hits@1 of 61.6%, Hits@10 of 93.8%, an MRR of 0.739,\napproach to align textual descriptions and molecules at three and an impressively low mean rank of 8.10. The experimental\ngranularities, while our model adopts optimal transport and resultsconfirmtheeffectivenessofourmodelintheseretrieval\ncontrastive learning to achieve alignments at three levels. It tasks, providing valuable insights for further enhancements.\ninferences at zero-shot settings for evaluating retrieval perfor-\nmance on PCdes. Additionally, on the ChEBI-20 dataset, we\nWe also present experimental results on the PCdes dataset\ntrain Atomas from scratch using the same configuration as\nfor two retrieval tasks, as detailed in Table II. Notably,\nours to evaluate its retrieval capabilities.\nour model, trained from scratch on this dataset, significantly\noutperforms existing models at both zero-shot and finetuning\nD. Main Results settings.Inthetext-moleculeretrievaltask,ourmodelachieves\nan impressive R@1 of 64.8%, remarkably better than the\nWe present experimental results on the ChEBI-20 dataset best zero-shot model (Atomas-large at 49.1%) and finetuning\nfor both retrieval tasks, as detailed in Table I. In the text- model (MolFM at 29.8%). It also achieves an R@5 of 82.3%,\nmoleculeretrievaltask,ourmodelsignificantlyoutperformsall R@10 of 86.3%, and MRR of 0.727. In the molecule-text\notherbaselinemodels,achievingaHits@1of66.5%,Hits@10 retrievaltask,ourmodeldemonstratessuperiorresultswithan\nof 93.9%, and an MRR of 0.772. Although our mean rank R@1 of 62.1%, R@5 of 81.4%, and R@10 of 86.3%, along-\nis 18.53 that is not the best, it is competitively close to the side MRR of 0.710. This exceptional performance highlights\nbest-performingmodel,Atomas-base,whichobtainsaslightly the potential of our model to surpass generalized pretrained\nbetter mean rank of 14.49 but lower Hits@1 and MRR. models in both retrieval tasks, confirming its effectiveness.\nText Query Ground Truth Top-k Candidates\nMalachite green isothiocyanate\nis an organic perchlorate salt. It\nhas a role as a fluorochrome. It\ncontains a malachite green\nisothiocyanate cation.\nMalachite green Malachite green Malachite\nDouble green Gentian Violet Pararosaniline(1+)\nisothiocyanate isothiocyanate green cation\n(R)-piperazine-2-carboxylic acid\nis a piperazine-2-carboxylic acid\nhaving (R)-configuration. It is a\nconjugate acid of a (R)-\npiperazine-2-carboxylate. It is an\nenantiomer of a (S)-piperazine-\n2-carboxylic acid. It is a\ntautomer of a (R)-piperazine-2- (R)-Piperazine-2- (R)-Piperazine-2- (R)-piperidine-3- 6-Oxopiperidine- (S)-Piperazine-2- Nipecotic acid\ncarboxylic acid zwitterion. carboxylic acid carboxylic acid carboxylic acid 2-carboxylic acid carboxylic acid\nFig.6. Casestudyofourmodelinthetext-moleculeretrievaltask.Theredboxindicatesthatthemoleculewiththehighestsimilarityretrievedbyourmodel\nisthegroundtruth.\nMolecule Query Top-k Candidates\nTestosterone acetate is an 17alpha-hydroxyprogesterone is Progesterone is a C21-steroid 16,17-didehydropregnenolone is\nandrostanoid that is the acetate a 17alpha-hydroxy steroid that hormone in which a pregnane a 3beta-hydroxy steroid that is\nderivative of testosterone. It has is the 17alpha-hydroxy skeleton carries oxo substituents pregnenolone with a double\nTestosterone acetate\na role as a human metabolite. It derivative of progesterone. It at positions 3 and 20 and is bond between positions 16 and\nis a sterol ester, an androstanoid has a role as a metabolite, a unsaturated at C(4)-C(5). As a 17. It is a 3beta-hydroxy steroid,\nGround Truth\nand a 3-oxo-Delta(4) steroid. It progestin, a human metabolite hormone, it is involved in the a 20-oxo steroid and an enone.\nTestosterone acetate is an derives from a testosterone. and a mouse metabolite. It is a female menstrual cycle, It derives from a pregnenolone.\nandrostanoid that is the acetate 17alpha-hydroxy steroid… pregnancy and embryogenesis\nderivative of testosterone. It has a of humans and other species…\nrole as a human metabolite. It is a\nsterol ester, an androstanoid and a Testosterone acetate 16-Dehydropregnenolone\n3-oxo-Delta(4) steroid. It derives\nfrom a testosterone. Hydroxyprogesterone Progesterone\nFig. 7. Case study of our model in the molecule-text retrieval task. The blue box highlights the candidate description retrieved by our model that exactly\ncorrespondstothegroundtruth.\nE. Ablation Study top-k retrieved candidates vary, their corresponding molecule\nstructuresareremarkablysimilar.Thissuggeststhatourmodel\nWe conduct an ablation study on the ChEBI-20 dataset for\naccurately grasps the alignments between text and molecules,\nboth retrieval tasks, providing insights about the benefits of\nhighlighting its superior performance.\nalignments at different levels: token-atom (TA), multitoken-\n2) Visualization for the Alignments of Motifs and Multi-\nmotif (MM), and sentence-molecule (SM). From Table III,\ntokens: Wealsovisualizethefusionofmultiplemotif-aligned\nwe can clearly observe that the use of all three levels leads\ntokens through optimal transport theory in Figure 5. On the\nto the best results, confirming the effectiveness of the multi-\nleft side of the figure, atoms marked in the same color serve\nlevel alignments. Conversely, the results are worse when only\nas the motifs extracted by our model. On the right side,\ntwo levels of alignments are employed. Also, relying on one\nthe textual descriptions highlighted in the same colors repre-\nlevel of alignment alone further diminishes the performance.\nsent the identified multi-tokens corresponding to the motifs\nThese results highlight that alignment at each level uniquely\nof matching colors on the left. For instance, we use (R)-\ncontributes to the overall performance.\nnephthenol and L-histidinol as examples, where the motifs\nextracted by our model match the identified multi-tokens.\nF. Case Study\nThis comprehensively demonstrates that the optimal transport\n1) Retrieval Performance: We select several typical sam- is helpful for the alignments between the multi-tokens and\nples from the test set to demonstrate the retrieval capability corresponding motifs.\nof our model. As illustrated in Figure 6, our model accurately\nretrievesthegroundtruthmoleculesasthetop-1candidatefor\nV. CONCLUSION\nboth queries. The molecules retrieved in the top-k candidates In this paper, we propose an Optimal TRansport-based\nclosely resemble the text queries, confirming the effective Multi-grained Alignments model (ORMA), designed to en-\nalignments between textual descriptions and molecules. As hance the cross-modal text-molecule retrieval task. By uti-\nshown in Figure 7, our model also correctly identifies the lizing SciBERT, we encode textual descriptions into token\nground truth description as the top-1 retrieval result for a and sentence representations. Simultaneously, we represent\ngivenmoleculequery.Althoughthetextualdescriptionsofthe the molecule as a heterogeneous graph containing atom,\nmotif, and molecule nodes, encoded by our GCN molecule [13] J.Song,W.Zhuang,Y.Lin,L.Zhang,C.Li,J.Su,S.He,andX.Bo,\nencoder. Innovatively, we consider the alignments between “Towards Cross-Modal Text-Molecule Retrieval with Better Modality\nAlignment,”arXive-prints,p.arXiv:2410.23715,Oct.2024.\ntokens and motifs as the optimal transport problem, fusing\n[14] Y.Luo,K.Yang,M.Hong,X.Liu,andZ.Nie,“Molfm:Amultimodal\ntoken representations into motif-aligned multi-token repre- molecularfoundationmodel,”arXivpreprintarXiv:2307.09484,2023.\nsentations. Furthermore, we employ contrastive learning to [15] P. Liu, Y. Ren, J. Tao, and Z. Ren, “Git-mol: A multi-modal large\nlanguage model for molecular science with graph, image, and text,”\naligntextualdescriptionsandmoleculesatthreelevels:token-\nComputersinBiologyandMedicine,vol.171,p.108073,2024.\natom, multitoken-motif, and sentence-molecule. To the best [16] S.Li,Z.Liu,Y.Luo,X.Wang,X.He,K.Kawaguchi,T.-S.Chua,and\nof our knowledge, our work is the first attempt to consider Q.Tian,“Towards3dmolecule-textinterpretationinlanguagemodels,”\narXivpreprintarXiv:2401.13923,2024.\nrepresentations at the motif and multi-token granularities.\n[17] F. Fan, Y. Feng, and D. Zhao, “Multi-grained attention network for\nExperimental results on the ChEBI-20 and PCdes datasets aspect-level sentiment classification,” in Proceedings of the 2018 con-\ndemonstrate the superior retrieval performance of our model ferenceonempiricalmethodsinnaturallanguageprocessing,pp.3433–\n3442,2018.\ncompared to previous state-of-the-art models. In the future,\n[18] B.Zhang,D.Xiong,J.Su,andY.Qin,“Alignment-supervisedbidimen-\nwe will extend ORMA to integrate more modal data sources, sionalattention-basedrecursiveautoencodersforbilingualphraserepre-\nsuch as protein structures and cellular images, to further the sentation,”IEEEtransactionsoncybernetics,vol.50,no.2,pp.503–513,\n2018.\napplication of ORMA in more complex biological systems.\n[19] B.Zhang,D.Xiong,andJ.Su,“Battrae:Bidimensionalattention-based\nrecursive autoencoders for learning bilingual phrase embeddings,” in\nVI. ACKNOWLEDGEMENTS\nProceedings of the AAAI conference on artificial intelligence, vol. 31,\nThe project was supported by the National Natural Science 2017.\n[20] J.Su,D.Xiong,B.Zhang,Y.Liu,J.Yao,andM.Zhang,“Bilingualcor-\nFoundation of China (Nos. 62276219 and 62472370) and\nrespondence recursive autoencoder for statistical machine translation,”\nthe Public Technology Service Platform Project of Xiamen inProceedingsofthe2015ConferenceonEmpiricalMethodsinNatural\n(No. 3502Z20231043). We also thank the reviewers for their LanguageProcessing,pp.1248–1258,2015.\n[21] J. Su, B. Zhang, D. Xiong, R. Li, and J. Yin, “Convolution-enhanced\ninsightful comments.\nbilingualrecursiveneuralnetworkforbilingualsemanticmodeling,”in\nProceedings of COLING 2016, the 26th International Conference on\nREFERENCES\nComputationalLinguistics:TechnicalPapers,pp.3071–3081,2016.\n[1] S. Kim, P. A. Thiessen, E. E. Bolton, J. Chen, G. Fu, A. Gindulyte, [22] B. Zhang, D. Xiong, J. Su, H. Duan, and M. Zhang, “Bilingual\nL. Han, J. He, S. He, B. A. Shoemaker, et al., “Pubchem substance autoencoders with global descriptors for modeling parallel sentences,”\nand compound databases,” Nucleic acids research, vol. 44, no. D1, inProceedingsofCOLING2016,the26thInternationalConferenceon\npp.D1202–D1213,2016.\nComputationalLinguistics:TechnicalPapers,pp.2548–2558,2016.\n[2] C. Edwards, C. Zhai, and H. Ji, “Text2mol: Cross-modal molecule [23] Y. Ma, G. Xu, X. Sun, M. Yan, J. Zhang, and R. Ji, “X-clip: End-\nretrieval with natural language queries,” in Proceedings of the 2021 to-end multi-grained contrastive learning for video-text retrieval,” in\nConference on Empirical Methods in Natural Language Processing, Proceedingsofthe30thACMInternationalConferenceonMultimedia,\npp.595–607,2021. pp.638–647,2022.\n[3] Z.Zeng,Y.Yao,Z.Liu,andM.Sun,“Adeep-learningsystembridging [24] P.Jin,H.Li,Z.Cheng,J.Huang,Z.Wang,L.Yuan,C.Liu,andJ.Chen,\nmoleculestructureandbiomedicaltextwithcomprehensioncomparable “Text-videoretrievalwithdisentangledconceptualizationandset-to-set\ntohumanprofessionals,”Naturecommunications,vol.13,no.1,p.862, alignment,”arXivpreprintarXiv:2305.12218,2023.\n2022. [25] Z.Wang,Y.-L.Sung,F.Cheng,G.Bertasius,andM.Bansal,“Unified\n[4] C. Edwards, T. Lai, K. Ros, G. Honke, K. Cho, and H. Ji,\ncoarse-to-finealignmentforvideo-textretrieval,”inProceedingsofthe\n“Translation between molecules and natural language,” arXiv preprint IEEE/CVF International Conference on Computer Vision, pp. 2816–\narXiv:2204.11817,2022. 2827,2023.\n[5] Z.Liu,W.Zhang,Y.Xia,L.Wu,S.Xie,T.Qin,M.Zhang,andT.-Y. [26] I.Beltagy,K.Lo,andA.Cohan,“Scibert:Apretrainedlanguagemodel\nLiu,“Molxpt:Wrappingmoleculeswithtextforgenerativepre-training,”\nforscientifictext,”arXivpreprintarXiv:1903.10676,2019.\narXivpreprintarXiv:2305.10688,2023. [27] X. Zang, X. Zhao, and B. Tang, “Hierarchical molecular graph self-\n[6] D.Weininger,“Smiles,achemicallanguageandinformationsystem.1. supervised learning for property prediction,” Communications Chem-\nintroduction to methodology and encoding rules,” Journal of chemical istry,vol.6,no.1,p.34,2023.\ninformationandcomputersciences,vol.28,no.1,pp.31–36,1988. [28] J.Degen,C.Wegscheid-Gerlach,A.Zaliani,andM.Rarey,“Ontheart\n[7] B.Su,D.Du,Z.Yang,Y.Zhou,J.Li,A.Rao,H.Sun,Z.Lu,andJ.-R.\nofcompilingandusing’drug-like’chemicalfragmentspaces,”ChemMed-\nWen,“Amolecularmultimodalfoundationmodelassociatingmolecule\nChem,vol.3,no.10,p.1503,2008.\ngraphswithnaturallanguage,”arXivpreprintarXiv:2209.05481,2022. [29] T.N.KipfandM.Welling,“Semi-supervisedclassificationwithgraph\n[8] S.Liu,W.Nie,C.Wang,J.Lu,Z.Qiao,L.Liu,J.Tang,C.Xiao,and\nconvolutionalnetworks,”arXivpreprintarXiv:1609.02907,2016.\nA. Anandkumar, “Multi-modal molecule structure–text model for text- [30] G. Peyre´, M. Cuturi, et al., “Computational optimal transport: With\nbasedretrievalandediting,”NatureMachineIntelligence,vol.5,no.12, applications to data science,” Foundations and Trends® in Machine\npp.1447–1457,2023.\nLearning,vol.11,no.5-6,pp.355–607,2019.\n[9] W. Zhao, D. Zhou, B. Cao, K. Zhang, and J. Chen, “Adversarial [31] G.Luise,A.Rudi,M.Pontil,andC.Ciliberto,“Differentialproperties\nmodalityalignmentnetworkforcross-modalmoleculeretrieval,”IEEE of sinkhorn approximation for learning with wasserstein distance,”\nTransactionsonArtificialIntelligence,2023. AdvancesinNeuralInformationProcessingSystems,vol.31,2018.\n[10] Y. Zhang, G. Ye, C. Yuan, B. Han, L.-K. Huang, J. Yao, W. Liu, [32] Y.Xie,X.Wang,R.Wang,andH.Zha,“Afastproximalpointmethod\nand Y. Rong, “Atomas: Hierarchical alignment on molecule-text\nforcomputingexactwassersteindistance,”inProceedingsofThe35th\nfor unified molecule understanding and generation,” arXiv preprint Uncertainty in Artificial Intelligence Conference (R. P. Adams and\narXiv:2404.16880,2024. V.Gogate,eds.),vol.115ofProceedingsofMachineLearningResearch,\n[11] D. Christofidellis, G. Giannone, J. Born, O. Winther, T. Laino, and pp.433–453,PMLR,22–25Jul2020.\nM.Manica,“Unifyingmolecularandtextualrepresentationsviamulti- [33] J.HanandJ.Pei,“Miningfrequentpatternsbypattern-growth:method-\ntask language modelling,” in International Conference on Machine ologyandimplications,”ACMSIGKDDexplorationsnewsletter,vol.2,\nLearning,pp.6140–6157,PMLR,2023. no.2,pp.14–20,2000.\n[12] Z.Liu,S.Li,Y.Luo,H.Fei,Y.Cao,K.Kawaguchi,X.Wang,andT.- [34] RDKit,“Rdkit:Open-sourcecheminformatics.” https://www.rdkit.org.\nS.Chua,“Molca:Moleculargraph-languagemodelingwithcross-modal [35] D.P.KingmaandJ.Ba,“Adam:Amethodforstochasticoptimization,”\nprojector and uni-modal adapter,” arXiv preprint arXiv:2310.12798, arXivpreprintarXiv:1412.6980,2014.\n2023.",
    "pdf_filename": "Exploring_Optimal_Transport-Based_Multi-Grained_Alignments_for_Text-Molecule_Retrieval.pdf"
}