{
    "title": "Sufficient Invariant Learning for Distribution Shift",
    "abstract": "Learningrobustmodelsunderdistributionshiftsbetweentrainingandtestdatasets isafundamentalchallengeinmachinelearning. Whilelearninginvariantfeatures acrossenvironmentsis apopular approach, itoftenassumes thatthesefeatures arefullyobservedinbothtrainingandtestsets—aconditionfrequentlyviolated in practice. When models rely on invariant features absent in the test set, their robustnessinnewenvironmentscandeteriorate. Totacklethisproblem, wein- troduceanovellearningprinciplecalledtheSufficientInvariantLearning(SIL) framework,whichfocusesonlearningasufficientsubsetofinvariantfeaturesrather thanrelyingonasinglefeature. Afterdemonstratingthelimitationofexistingin- variantlearningmethods,weproposeanewalgorithm,AdaptiveSharpness-aware GroupDistributionallyRobustOptimization(ASGDRO),tolearndiverseinvariant featuresbyseekingcommonflatminimaacrosstheenvironments. Wetheoretically demonstratethatfindingacommonflatminimaenablesrobustpredictionsbased ondiverseinvariantfeatures. Empiricalevaluationsonmultipledatasets,including our new benchmark, confirm ASGDRO’s robustness against distribution shifts, highlightingthelimitationsofexistingmethods. 1 Introduction Machinelearningmodelstypicallyassumethattrain- ingandtestdataaredrawnfromthesamedistribu- tion. However,inreal-worldscenarios,thisassump- tionisoftenviolatedwheneverthetrainingandtest distributiondiffer,knownasdistributionshifts. In these cases, model performance tends to degrade, highlighting the need to develop models that are Figure1: Leftvisualizestheimagesthatcontain robusttodistributionshiftsforreliableoutcomes. aspuriousfeature,ZNI,andmultipleinvariantfea- tures,Z ,Z ,andZ intrainingenvironment Tail Beak Feet Totrainmodelsrobusttodistributionshift, invari- E . IfthemodelfocusesontheZNI (greenback- tr ant learning focuses on identifying latent features ground),thenitfailstopredictcorrectlyinthetest thatremainconstantacrossenvironments,referred environmentE\\E (Right).Evenifthemodelcap- tr toasinvariantfeatures. Thesefeaturesenablecon- turestheinvariantfeaturesinE ,e.g.,Z ,itstill tr Feet sistentpredictionsacrossenvironmentsbydiscour- failstopredictcorrectlywhentheinvariantfeatures agingmodelsfromrelyingonspuriousfeatures(Ar- arenotpresent(Gray). However,itispossibleto jovskyetal.,2019)–featuresthatarenotpreserved predictcorrectlyifwelearndiverseinvariantfea- acrosschangesinenvironmentsorgroups1. Forex- turessufficiently,Z Feet,Z Tail,andZ Beak. WithSIL (Red),themodelpredictsthelabelusingremaining ample, indomain generalization tasks (Kohet al., invariantfeatures,Z andZ eventhoughZ 2021;GulrajaniandLopez-Paz,2020),thegoalis Tail Beak Feet isnotpresentinthetestenvironmentE\\E . tolearninvariantfeaturesthatconsistentlypredict tr 1Inthispaper,thetermsenvironmentanddomainareusedinterchangeably.Agroupreferstoasubpopulation correspondingtoaparticularlabelwithinaspecificenvironment. 4202 voN 91 ]GL.sc[ 3v33531.0122:viXra",
    "body": "Sufficient Invariant Learning for Distribution Shift\nTaeroKim1 SubeenPark1 SungjunLim1\nYonghanJung2 KrikamolMuandet3 KyungwooSong1\n1YonseiUniversity 2PurdueUniversity 3CISPAHelmholtzCenterforInformationSecurity\nAbstract\nLearningrobustmodelsunderdistributionshiftsbetweentrainingandtestdatasets\nisafundamentalchallengeinmachinelearning. Whilelearninginvariantfeatures\nacrossenvironmentsis apopular approach, itoftenassumes thatthesefeatures\narefullyobservedinbothtrainingandtestsets—aconditionfrequentlyviolated\nin practice. When models rely on invariant features absent in the test set, their\nrobustnessinnewenvironmentscandeteriorate. Totacklethisproblem, wein-\ntroduceanovellearningprinciplecalledtheSufficientInvariantLearning(SIL)\nframework,whichfocusesonlearningasufficientsubsetofinvariantfeaturesrather\nthanrelyingonasinglefeature. Afterdemonstratingthelimitationofexistingin-\nvariantlearningmethods,weproposeanewalgorithm,AdaptiveSharpness-aware\nGroupDistributionallyRobustOptimization(ASGDRO),tolearndiverseinvariant\nfeaturesbyseekingcommonflatminimaacrosstheenvironments. Wetheoretically\ndemonstratethatfindingacommonflatminimaenablesrobustpredictionsbased\nondiverseinvariantfeatures. Empiricalevaluationsonmultipledatasets,including\nour new benchmark, confirm ASGDRO’s robustness against distribution shifts,\nhighlightingthelimitationsofexistingmethods.\n1 Introduction\nMachinelearningmodelstypicallyassumethattrain-\ningandtestdataaredrawnfromthesamedistribu-\ntion. However,inreal-worldscenarios,thisassump-\ntionisoftenviolatedwheneverthetrainingandtest\ndistributiondiffer,knownasdistributionshifts. In\nthese cases, model performance tends to degrade,\nhighlighting the need to develop models that are Figure1: Leftvisualizestheimagesthatcontain\nrobusttodistributionshiftsforreliableoutcomes. aspuriousfeature,ZNI,andmultipleinvariantfea-\ntures,Z ,Z ,andZ intrainingenvironment\nTail Beak Feet\nTotrainmodelsrobusttodistributionshift, invari- E . IfthemodelfocusesontheZNI (greenback-\ntr\nant learning focuses on identifying latent features ground),thenitfailstopredictcorrectlyinthetest\nthatremainconstantacrossenvironments,referred environmentE\\E (Right).Evenifthemodelcap-\ntr\ntoasinvariantfeatures. Thesefeaturesenablecon- turestheinvariantfeaturesinE ,e.g.,Z ,itstill\ntr Feet\nsistentpredictionsacrossenvironmentsbydiscour- failstopredictcorrectlywhentheinvariantfeatures\nagingmodelsfromrelyingonspuriousfeatures(Ar- arenotpresent(Gray). However,itispossibleto\njovskyetal.,2019)–featuresthatarenotpreserved predictcorrectlyifwelearndiverseinvariantfea-\nacrosschangesinenvironmentsorgroups1. Forex- turessufficiently,Z Feet,Z Tail,andZ Beak. WithSIL\n(Red),themodelpredictsthelabelusingremaining\nample, indomain generalization tasks (Kohet al.,\ninvariantfeatures,Z andZ eventhoughZ\n2021;GulrajaniandLopez-Paz,2020),thegoalis Tail Beak Feet\nisnotpresentinthetestenvironmentE\\E .\ntolearninvariantfeaturesthatconsistentlypredict tr\n1Inthispaper,thetermsenvironmentanddomainareusedinterchangeably.Agroupreferstoasubpopulation\ncorrespondingtoaparticularlabelwithinaspecificenvironment.\n4202\nvoN\n91\n]GL.sc[\n3v33531.0122:viXra\nlabelsacrossmultipleenvironments. Assumingthatthelearnedinvariantfeaturespersistinallunseen\nenvironments,theyguaranteethemodel’sgeneralizationperformanceonnewenvironments(Muandet\netal.,2013;Lietal.,2018). Similarly,learningmodelsrobusttosubpopulationshiftsisessentialin\ncasesofsevereimbalancesbetweengroups. Inthisscenario,invariantfeaturesplayacrucialrolein\naddressingthechallengesfacedbyunderrepresentedgroups,whicharedisproportionatelyimpacted\nbystrongspuriouscorrelations(Sagawaetal.,2019;Yaoetal.,2022;Izmailovetal.,2018).\nHowever,learningallpossibleinvariantfeaturesischallenginginpracticebecausemostexisting\ninvariantlearningapproachesfocusoneliminatingspuriouscorrelations,whichcanbeachievedby\nleveragingonlyasubsetoftheinvariantfeaturespresentinthetrainingenvironments. Moreover,\ninvariantfeaturesidentifiedbythemodelmaynotbeobservableinunseenenvironments(Guoetal.,\n2024;Tsymbal,2004). Thisunderscorestheimportanceoflearningasufficientnumberofinvariant\nfeatures, rather than relying on a single invariant feature. To address this, we introduce a novel\napproachcalledSufficientInvariantLearning(SIL),whichfocusesonlearningasufficientsetof\ninvariantfeaturesforimprovedgeneralization. Forexample,considerthescenariodepictedinFigure\n1. Trainingenvironmentsforanimageofabirdmayincludemultipleinvariantfeatures, suchas\nZ ,Z andZ . Ifamodelreliesonasingleinvariantfeature,sayZ ,itmayfailtoclassify\nTail Beak Feet Feet\nanimageofthebirdifthefeatureisunobservable(e.g.,thebird’sfeetarehiddenunderwater). In\ncontrast,ifthemodelusesasufficientlydiversesetofinvariantfeatures(e.g.,allofZ ,Z and\nTail Beak\nZ ),itcanstillclassifytheimagecorrectlyaslongasoneormoreoftheotherinvariantfeatures\nFeet\narepresent. Thishighlightstherobustnessandgeneralizationbenefitsoflearningasufficientnumber\nofinvariantfeatures.\nInthisstudy,wedeveloptheSILframeworkanddemonstratethatleveragingsufficientlydiverse\ninvariantfeaturesthroughSILenhancesmodelrobustness. AsamethodforSIL,weproposeAdaptive\nSharpness-awareGroupDistributionallyRobustOptimization(ASGDRO).WeshowthatASGDRO\nattainsSILbyeffectivelylearningdiverseinvariantfeatureswhilesuccessfullyeliminatingspurious\ncorrelations. Furthermore, we show that the ability of ASGDRO to perform SIL is due to its\nconvergencetoacommonflatminima(Foretetal.,2020)acrossdiverseenvironments. Through\nempiricalevaluationsonatoyexampleandournewlyintroducedSILBenchmarkdataset,weshow\nthatexistinginvariantlearningalgorithmsfallshortincapturingdiverseinvariantfeatures,whereas\nASGDRO successfully achieves SIL. By learning a wide range of invariant features sufficiently,\nASGDROexhibitsrobustgeneralizationperformanceundervariousdistributionshiftscenarios,as\nevidencedbyextensiveexperimentsinvolvingsubpopulationanddomainshifts.\n2 RelatedWorks\n2.1 InvariantLearningforDistributionShift\nThestandardapproachtomoderndeeplearningisEmpiricalRiskMinimization(ERM)(Vapnik,\n1999),whichminimizestheaveragetrainingloss. However,ERMmaynotguaranteerobustnessin\ndistributionshifts. Toimprovethegeneralizationperformanceindistributionshift,GroupDistribu-\ntionallyRobustOptimization(GDRO)minimizestheworstgrouplossforeachiterationtoalleviate\nspurious correlations (Sagawa et al., 2019). Meanwhile, various studies utilize loss gradient for\ninvariantlearning. Forexample,Arjovskyetal.(2019)minimizesthegradientnormofthefixed\nclassifieracrossenvironments. Otherresearchmatchesthelossgradientforeachenvironmenttofind\ninvariantfeatures(Shietal.,2021;Rameetal.,2022a). Furthermore,balancingtherepresentation\nusingselectivesamplingwithmix-upsamples(Yaoetal.,2022)orre-trainingtheclassifierona\nsmallbalancedset(Kirichenkoetal.,2022)showtheeffectivenessoflearningarobustmodel. Some\nstudiesenhancegeneralizationbycombininginvariantlearningalgorithmswithfeatureextractors\nwithrichrepresentations(Zhangetal.,2022;Chenetal.,2024a;ZhangandBottou,2023).\nUndertheassumptionthatinvariantfeaturesinthetrainingenvironmentalsoexistinthetesten-\nvironment, invariant learning theoretically guarantees an optimal predictor (Rojas-Carulla et al.,\n2018). However,wearguethatexistinginvariantlearningalgorithmsdonotlearnsufficientlydiverse\ninvariantfeatures,andtheystillsufferfromsignificantperformancedropsintestenvironmentswhere\nsomeinvariantfeaturesareunobserved(Guoetal.,2024;Tsymbal,2004). Toremedythisproblem,\nweintroducethenovelframework,SIL,andguaranteethegeneralizationabilityfordiverseinvariant\nfeatures. Throughexperimentsonthenewlyproposedbenchmarkinthispaper,aswellasonexisting\nbenchmarksforevaluatingmodelrobustnesstodistributionshifts(GulrajaniandLopez-Paz,2020;\n2\nKohetal.,2021),wedemonstratethatournovelalgorithmdesignedforSILleadstomorerobust\npredictions.\n2.2 FlatnessandGeneralization\nVariousstudiesarguethatfindingflatminimaimprovesgeneralizationperformance(Keskaretal.,\n2016;Neyshaburetal.,2017). Asaresult,manyalgorithmsemergetofindflatminima. Sharpness-\naware Minimization (SAM) (Foret et al., 2020) finds flat minima by minimizing the maximum\ntraininglossofneighborhoodsforthecurrentparameterwithinρradiusballontheparameterspace.\nMoreover,AdaptiveSAM(ASAM)introducesthenormalizationoperatortogetabettercorrelation\nbetweenflatnessandthemodel’sgeneralizationabilitybyavoidingthescalesymmetriesbetween\nthelayers(Kwonetal.,2021). StochasticWeightAveraging(SWA)alsoreachestheflatminimaby\naveragingtheweight(Izmailovetal.,2018). UndertheIIDsetting,theseapproaches(Foretetal.,\n2020;Kwonetal.,2021;Izmailovetal.,2018)successfullydecreasethegeneralizationgap.\nChaetal.(2021)showsthatoptimizingthemodeltowardsflatterminimathroughweightaveraging\nimprovesdomaingeneralizationperformance. However,itisstillnecessarytoverifywhethermodels\noperaterobustlythroughweightaveragingwhenstrongspuriouscorrelationsexist. Indeed,some\nstudiesdemonstratethatweightaveragingmethodsmaystillnotberobustincertainsubpopulation\nshifttasks(Rameetal.,2022b). Zhangetal.(2023)alsoshowsthatflatminimamakemodelsmore\nrobusttonoisepresentininstances. However,ourstudyaimstoshowtheeffectivenessofflatness\ninmoreextremedistributionshiftsettings,suchassubpopulationshiftanddomaingeneralization.\nSpringeretal.(2024)presentsthatwheneasy-to-learnandhard-to-learnfeaturescoexist,models\ntrainedbySAMlearnmorebalancedrepresentations. Thisalignswithourobservations, andwe\naimtoachieveSILbysuccessfullyremovingspuriouscorrelationsandlearningsufficientlydiverse\ninvariantfeaturesbyintroducingtheconstraintsrelatedtoflatnessfortherobustmodelinvarious\ndistributionshiftsettings.\n3 Methodology\n3.1 ProblemSetting\nLetX denotetheinputspace,Y thelabelspace,Z thefeaturespace,andΘtheparameterspace.\nConsider a set of environments E, where, for each environment e ∈ E, there exists a dataset\nDe = {(Xe,Ye)} with the input data Xe ∈ X, corresponding labels Ye ∈ Y, and n as the\nne e\nnumberofdatapointsinenvironmente. WeassumeasetoffeaturesZ =(ZI,ZNI)⊂Z,whereZI\nisasetofinvariantfeaturessatisfyingthefollowinginvarianceconditionandZNIrepresentsasetof\nspuriousfeatureswhosecorrelationwithYevariesdependingontheenvironmente(Arjovskyetal.,\n2019;Creageretal.,2021;Kruegeretal.,2021):\nDefinition1(InvarianceCondition). ZIisasetofinvariantfeaturessatisfying\nE[Ye|ZI]=E[Ye′ |ZI] foralle,e′ ∈E , (1)\ntr\nE ⊂E denotesthesetofenvironmentsintrainingdataset.\ntr\nIn fact, for training environments E , the invariance condition holds for any subset ZˆI ⊆ ZI. In\ntr\nparticular,wedenoteZIasasingletonsetwithi–thelementofZI,wherei∈{1,...,p}andpdenotes\ni\nthenumberofinvariantfeatures. Forexample,inFigure1,consideratasktoclassifytheimagesof\nbirdsintowaterbirdsorlandbirds,ZI ={Z ,Z ,Z }withp=3,ZNI ={Z }and\nBeak Tail Feet Background\nZI ={Z }.\n1 Beak\nSupposeamodelf =h◦gparametrizedbyθ =(θ ,θ )∈Θ,whereg :X →Z isanencoderwith\ng h\nparametersθ andh:Z →Y isaclassifierwithparametersθ . LetRe(θ)=E[ℓ(f(Xe;θ),Ye)]\ng h\ndenotetheriskofamodelf inenvironmente,whereℓdenotesalossfunction. Invariantlearning\nseekstominimizethemaximumriskacrossenvironments,\nminmaxRe(θ),\nθ e∈E\nandtotrainmodelsthathaverobustperformanceandgeneralizationabilityforunseenenvironments\nbylearninginvariantfeatureZI (Arjovskyetal.,2019;Sagawaetal.,2019;Creageretal.,2021;\n3\nKruegeretal.,2021). Inparticular,givenZI,Rojas-Carullaetal.(2018)demonstratethatlearning\noptimalclassifierθ∗,whichisbasedonallinvariantfeaturesinZI,leadstorobustmodelpredictions,\nh\nθ∗ ∈minmaxRe(θ ), (2)\nh h\nθh e∈E\nwhereRe(θ )=[ℓ(h(ZI;θ ),Ye)],assumingthattheinvarianceconditionholdsforalle∈E. In\nh h\nthiswork,however,wefocusonsituationswheretheoptimalclassifierforEquation2isnotunique,\nforinstance, whenanyclassifierthatdependsonasubsetofZˆI remainsoptimal. Toaddressthe\nchallengesinsuchscenarios,weproposeaframeworkcalledSufficientInvariantLearning(SIL).\n3.2 SufficientInvariantLearning\nIntheclassificationtask,weconcentrateonthefactthatanysubsetofZIallowsamodeltominimize\nthemaximumriskacrossenvironmentse ∈ E . InthepreviousexampleofFigure1,themodel’s\ntr\nclassifiermayutilizeonlyZ todistinguishbetweenwaterbirdsandlandbirds,oritmayemploy\nFeet\nallZI simultaneouslyinE . Consequently,aclassifierthatsatisfiesEquation2isnotunique,and\ni tr\nsincetheoptimalencoderisalsonotuniquewhenderivedfromtheoptimalclassifier(Arjovskyetal.,\n2019),Equation2issimilarlynotunique. Todifferentiateamongvariouspredictionmechanismsthat\nleverageinvariantfeaturesinZI,wedefinetheinvariantmechanismasfollows:\nDefinition2(InvariantMechanism). Foranencoderg parameterizedbyθI andaclassifierh\nθ gI g θ hI\nparameterizedbyθI,theinvariantmechanismθI = (θI,θI) ∈ ΘisatupleforasubsetZˆI ⊆ ZI\nh g h\nsatisfyingthefollowings:\n• Condition1: h :ZˆI (cid:55)→Ye, ∀e∈E .\nθI tr\nh\n• Condition2: θI ∈argmin max Re(θ).\nθ e∈Etr\nΘI ⊂Θdenotesacollectionofallinvariantmechanisms.\nSpecifically,wedenotetheinvariantmechanismthatutilizesonlyZIasθI,fori={1,...p}.\ni i\nAsaresult,invariantmechanismsthatfocussolelyonaspecificinvariantfeatureθI mayencounter\ni\nchallengesinproducingrobustpredictionswhenthepartoftheinputcorrespondingtothatfeature\nisdistortedbynoise,absentduetocropping,oroccludedbyenvironmentalfactors. Accordingly,\nthenon-uniquenessoftheinvariantmechanismsuggeststhattrainingtheencoderbasedonclassifier\ninvariance (Arjovsky et al., 2019; Ahuja et al., 2021) or refining it to capture richer information\n(Zhangetal.,2022;Chenetal.,2024a)canbenefitfromadditionalregularizationontheinvariant\nmechanism. This observation also implies that robust optimization methods designed to satisfy\nEquation2overE (Duchietal.,2016;Orenetal.,2019;Sagawaetal.,2019)haveanavenuefor\ntr\nachievingenhancedgeneralizationperformance.\nWearguethattrainingmorerobustmodelsrequiresensuringgeneralizationperformanceacrossa\nsufficientlydiversesetofinvariantfeatures. Toachievethis,weintroduceanovelinvariantlearning\nframework,termedSufficientInvariantLearning(SIL):\nDefinition3(SufficientInvariantLearning). SufficientInvariantLearningreferstoidentifyθSIsuch\nthat\nθSI ∈argminmaxRe(θ),\nθ e∈E\ns.t. θSI ∈argminmax max E[ℓ(h (ZˆI),Ye)].\nh\nθh e∈E ZˆI⊆ZI\nθh\nSILaimstotrainaclassifierthatoperatesrobustlynotonlyacrossallenvironmentsbutalsowhenit\nisprovidedwithonlyasubsetZˆI. SILensuresthatthemodelleveragessufficientlydiverseinvariant\nfeaturesconsistentlyinmakingpredictions, assumingthatamodeltrainedonthetargettaskhas\nalready learned sufficient representations of the invariant features (Kirichenko et al., 2022). The\nmostchallengingaspectofachievingSIListhatprovidingindividuallyinterveneddataoneachZˆIto\nperformSILiscostly,andsituationswheredefiningZˆIindividuallyisambiguousarecommon. To\naddressthis,weproposeASGDRO,anovelmethodmotivatedbytheperspectiveofthelosssurface\nanddemonstratestheeffectivenessoffindingcommonflatminimaforSIL.\n4\nAlgorithm1AdaptiveSharpness-awareGroupDistributionallyRobustOptimization(ASGDRO)\nInput: TrainingdatasetDe ={(Xe,Ye)}fore∈E ,Radiusρ>0,Learningrateη >0,Robuststepsize\ntr tr\nγ >0,Thenumberofenvironments|E |\ntr\n1: Initialization:θ ;λ(0) =1/|E |,e=1,...,|E |;\n0 e tr tr\n2: fort=1,2,3,...do\n3: ComputetraininglossRe(θ );\nt\n4: Computeϵ∗ =ρ Tθ2∇Re(θt) ;\ne ∥Tθ∇Re(θt)∥\n5: Gradientascent:θ∗=θ +ϵ∗;\nt t e\n6: FindlossforeachenvironmentRe(θ∗);\nt\n7: Computeλ˜(t) =λ(t−1)exp(γRe(θ∗))respectively;\ne e t\n8: Updateλ(t) =λ˜(t)/(cid:80) λ˜(t);\ne e e e\n9: ComputeR (θ )=(cid:80) λ(t)Re(θ∗);\nASGDRO t e e t\n10: Compute∇R (θ )=(cid:80) λ(t)∇Re(θ∗);\nASGDRO t e e t\n11: Returntoθ ;\nt\n12: Updatetheparameters:θ =θ −η∇R (θ );\nt+1 t ASGDRO t\n13: endfor\n3.3 ASGDRO:AdaptiveSharpness-awareGroupDistributionallyRobustOptimization\nIntheliteratureonmodelmergingandmulti-tasklearning(Ilharcoetal.,2022;Wortsmanetal.,\n2022;Ainsworthetal.;Raméetal.,2023),itisoftenassumedthatarobustmodelacrossalltaskslies\nwithinthelinearinterpolationofmodelsthatperformwelloneachindividualtask. Inspiredbythis\nobservation,weconsiderθI asamodelthatperformswellonasingletask,andwehypothesizethat\ni\nθSIexistswithinthelinearinterpolationofthesemechanisms. Withoutlossofgenerality,subsetsthat\narenotsingletonscanbeequivalentlyrepresentedasaninterpolationofsingletoninvariantfeatures\nZI. Hence,fortheremainderofthiswork,werestrictourconsiderationtoZIandZI(AppendixA.2).\ni i\nThekeydifferencefrompreviousstudiesisthatweevaluateeachtasksolelyonthesamedataset.\nTherefore,asdiscussedinSection3.2,differentinvariantmechanismsareexpectedtohavesimilar\nrisks,\nRe(θSI)−Re(θI)≈0 foralle∈E . (3)\ni tr\nAchallengeforSIListhatwedonothaveaccesstoinformationaboutθI. However,basedonthe\ni\nobservationinNeyshaburetal.(2020)thatdifferentmodelstrainedfromthesamepre-trainedmodel\nlie in the same loss basin, we assume that models located on the linear path between θSI and θI\ni\nalsoexhibitsimilarrisk. Therefore,θSI shouldguaranteelowriskswithinaballofradiusatleast\nmax ||θI −θSI||,denotedasρ,inEuclideanspace. Introducingaperturbationϵ := θI −θSI,we\ni i e i\nobtainthefollowingconditionfortheriskofθI:\ni\nmax Re(θI)= max Re(θSI+ϵ ). (4)\ni e\ni∈{1,...,p} ||ϵe||≤ρ\nFrom our motivation, ρ is a hyper-parameter adjusting the model class of θI deviated from θSI.\ni\nMoreover,accordingtoDefinition1,allθI shouldexhibitrobustperformanceacrossenvironments\ni\ne ∈ E . Finally, weproposeanovelobjectivefunctionnamedAdaptiveSharpness-awareGroup\ntr\nDistributionallyRobustOptimization(ASGDRO),whichisformulatedasfollows:\nmax max Re(θ+ϵ ). (5)\ne\ne∈Etr||ϵe||≤ρ\nInthefollowingsections,wetheoreticallyshowthatASGDROnotonlylearnsinvariantfeaturesbut\nalsobalancesthelearningofinvariantmechanisms,therebyachievingSIL.Also,wedemonstratethat\nASGDROfindsthecommonflatminimaacrossenvironments,leadingtoSIL.\n3.4 SILandCommonFlatMinima\nWedemonstratethatASGDROtrainsthemodeltoachieveSILbyshowingthatASGDRObalances\ntheuseofinvariantmechanisms.\nTheorem1(SufficientInvariantMechanism). LetθI beaconvexcombinationofθI,whereλisa\nλ i\np-dimensionalvector.Considermean-squarederrorasthelossfunction.Then,givenZI =(1,...,1)\n5\nwith|Z|=p,\nλ∗ =argminmax max Re(θI +ϵ)\nλ\nλ e∈Etr||ϵ||≤ρ\n=argminmax(cid:2) Re(θI)+ρ||λ||·||∇Re(θI)||(cid:3)\n(6)\nλ λ\nλ e∈Etr\n1 1\n=argmin||λ||=( ,..., )\np p\nλ\nwhere||·||denotesL norm.\n2\nRefer to Appendix A.4 for the proof. Theorem 1 states that ASGDRO ensures that even when\ninvariantfeaturescontributeequallytotheoutput,themodeldoesnotfavorasimplesolutionfocusing\nonasingleinvariantfeature. Instead,itlearnsadiverserangeofinvariantmechanisms. Asshownin\nEquation6,thisregularizationeffectarisesthroughthegradientnorm||∇Re(θ)||.\nProposition1(CommonFlatMinima). BytheTaylorexpansion,\nmax max Re(θ+ϵ )≈max[Re(θ)+ρ||∇Re(θ)||]. (7)\ne\ne∈E ||ϵe||≤ρ e∈E\nASGDROleadstoaregularizationofthegradientnorm,Re,||∇Re(θ)||,acrossenvironments,which\ndrivesthemodeltoconvergetocommonflatminima.\nRefertoAppendixA.5forproof. Asdemonstratedin(Zhaoetal.,2022),small||∇Re(θ)||indicates\nflatminima. WealsodemonstratethispropertyempiricallyinFigure5andAppendixA.11Finally,\nwearguethatfindingcommonflatminimaencouragesthemodeltolearnsufficientlydiverseinvariant\nmechanisms. Moreover,thisalignswithexistingstudiesinIIDsettings,whichsuggestthatflatter\nminimaimprovethegeneralizationperformanceofmodels(Foretetal.,2020;Kwonetal.,2021;\nKeskar et al.,2016). Additionally, wedemonstrate in AppendixA.5 thatASGDRO successfully\neliminatesthespuriousfeatureZewhileeffectivelylearningtheinvariantfeature.\n3.5 ImplementationofASGDRO\nFromForetetal.(2020),maximumvalueofinnerterminEquation5isapproximatedwhenϵ =\ne\nρ\n∇Re(θ)\n. However,Kwonetal.(2021)demonstratethatbyintroducingthenormalizationmatrix\n∥∇Re(θ)∥\nT ,whichremovesthescalesymmetrypresentonthelosssurface,thecorrelationbetweenflatness\nθ\nandgeneralizationperformanceisstrengthened. ASGDROalsoadoptsthesameT ,andmodified\nθ\nobjectivefunctionisasfollows:\nR (θ)=maxRe(θ+ϵ∗), (8)\nASGDRO e\ne∈Etr\nwhereϵ∗ =ρ T θ2∇Re(θ) isadversarialperturbationforeachenvironmente.\ne ∥Tθ∇Re(θ)∥\nToaddresstheinstabilityintrainingthatarisesfromtheoptimizationapproachofselectingonly\ntheworstenvironmentateachstep,weadoptanalternativegradient-basedoptimizationalgorithm\ninspiredbyGDRO(Sagawaetal.,2019). WemodifytheobjectivefunctionofASGDROintothe\nformoflinearinterpolationacrossenvironmentsandalsoupdatetheircoefficients:\n(cid:88)\nmaxRe(θ+ϵ∗)= max λ Re(θ+ϵ∗), (9)\ne∈Etr e (cid:80) eλe=1,λe≥0\ne∈Etr\ne e\nwhere λ is the weight imposed on adversarial perturbed loss for each environment. Finally, we\ne\nupdateourmodelparameterfromthecurrentparameterθ asfollows:\nt\n(cid:88)\nθ −η∇R (θ)=θ −η λ(t)∇Re(θ +ϵ∗), (10)\nt ASGDRO t e t e\ne∈Etr\nwhereη denotethelearningrateandλ(t) denotetheweightimposedoneachenvironmentlossat\ne\ntimestept. RefertoAlgorithm1forthedetails.\n6\nFigure2: SufficientInvariantLearningandCommonFlatMinimaIn(a-1)and(a-2),twoaxes,θI andθI,\n1 2\nrepresenttheinvariantdirectionsofparameterscorrespondingtoeachinvariantmechanismrespectively.Thered\ncircleindicatestheareaboundbyρformeasuringflatnessinASGDRO.(b-1)and(b-2)showthatwhenEnv2\nhassharpminimainthedirectionofθI,GDROstillconverges,butASGDROdoesnothaveanyoptimalpoint\n1\nduetothesharpnessofθI.However,in(c-1)and(c-2)whenbothinvariantdirectionsofEnv2aswellasEnv\n1\n1areflat,ASGDROhasanoptimalpointandpreferstoconverge.Thatis,ASGDROlearnsdiverseinvariant\nfeaturessufficiently.\n4 Experiments\n4.1 ToyExmaple\nWedemonstratethroughatoyexamplethattherepresentativeinvariantlearningalgorithmGDRO\nSagawaetal.(2019)failstolearndiverseinvariantmechanisms,whereasASGDROsuccessfully\nachievesSILbyencouragingthemodeltoconvergetothecommonflatminima(Figure2). First,we\nassumethatweknowtwodifferentdirectionscorrespondingtothedifferentinvariantmechanismθI\n1\nandθI,whichlearnsdifferentinvariantfeatures,ZI andZI,respectively. Wedefinethelosssurface\n2 1 2\nofeachenvironmentefollowingaGaussianfunctionwithrespecttoθI andθI:\n1 2\n(cid:18) (cid:19)\n1 1\nG(θ)= exp − (θ−µ)TΣ−1(θ−µ) , (11)\n2π(cid:112) |Σ| 2\n(cid:20) θI(cid:21) (cid:20)\nµ\n(cid:21) (cid:20)\nσ σ\n(cid:21)\nwhere θ= 1 ,µ(e) = 1 ,Σ(e) = 11 12 .\nθ 2I µ 2 σ 21σ 22\nTomakelossesgreaterthan0,wesubtractedG(θ)fromitsmaximumvalue. Asaresult,wedefine\nthelosssurfacecorrespondingtothetwoenvironments,eachwithaminimumvalueof0,asfollows:\nRe=1(θ)=maxG(θ;µ(1),Σ(1))−G(θ;µ(1),Σ(1)) (12)\nθ\nRe=2(θ)=maxG(θ;µ(2),Σ(2))−G(θ;µ(2),Σ(2)) (13)\nθ\nNow,wecreatesharporflatminimainaspecificdirectionbyadjustingthecovariancematrixΣ(e).\nInthisexample,weconsiderafixedsituationwherebothe = 1ande = 2haveflatminimawith\nrespecttoθI. WhenRe=1(θ)alwayshasflatminimainthedirectionofθI,weaimtoobservehow\n2 1\nthelossR correspondingtoeachobjectivefunctionchangesdependingonwhetherθI hassharp\nobj 2\norflatminima(a-1anda-2inFigure2). Theparametersthatweusetogeneratethetoyexamplesare\nasfollows:\n(cid:20) (cid:21) (cid:20) (cid:21)\n−2.0 1.5 0.0\nEnv1(e=1): µ= , FlatΣ= (14)\n0.0 0.0 2.0\n(cid:20) (cid:21) (cid:20) (cid:21)\n2.0 1.5 0.0\nEnv2(e=2): µ= , SharpΣ= (15)\n0.0 0.0 0.05\nWeevaluateeachalgorithmthroughthelosssurfaceineachdirection(secondandthirdcolumnsof\nFigure2). WhenEnv2exhibitssharpnessforθI (firstrowofFigure2),itindicatesthatlearningthe\n1\ninvariantfeaturecorrespondingtoθI mayresultinalargegeneralizationgap(Keskaretal.,2016).\n1\nHowever,GDROdoesnotincorporateregularizationonflatnessandonlyconsidersthelossatthe\ncurrentparameter,allowingconvergencetoasharpsolution. FromTheorem1,itimpliesthelarge\ngradientnorm,andthissituationdoesnotconstitutesuccessfulSIL.Incontrast,ASGDRO,which\ntakesintoaccountthelossinneighboringparameters,avoidssharpregionsforθI (b-1andc-1in\n1\nFigure2).\n7\nFigure 3: Overview of H-CMNIST. There are three features, color and shape (invariant features, ZI =\n{Z ,Z })andboxposition(spuriousfeature,ZNI ={Z }).TheratioofZ isflippedbetweenthetrain\nColor Shape BP BP\nandtestset.Thetestsetconsistsoftwotestbeds,oneforevaluatingwhetherlearninginvariantfeaturesandthe\notherforevaluatingwhetherlearningsufficientlydiverseinvariantfeatures.\nWhenEnv2isflatforθI (secondrowinFigure2),wesaythatthemodelperformsSILifitconverges\n1\nintothecommonflatminimabetweenEnv1andEnv2. However,GDROhasthesamelossatthe\noptimalpointinthissituationasinthepreviouscase,indicatingthatGDROdoesnotspecifically\nregularizethemodeltoperformSIL.Ontheotherhand,ASGDRO,byaccountingforcommonflat\nminima,identifiesanoptimalparameterthatpromoteslearningofdiverseinvariantmechanisms(b-2\nandc-2inFigure2). Asaresult,byconsideringflatness,themodelperformsSILandisexpectedto\nmakerobustpredictionsinunseenenvironmentsbyleveragingmultipleinvariantfeatures.\n4.2 HeterogenousColoredMNIST\nByfindingthecommonflatminima,ASGDRO\nTestBed1 TestBed2\nlearnsdiverseinvariantfeatures.Todemonstrate Spu&Inv Inv Spu&Shape Shape\nthis,weproposeHeterogeneousColoredMNIST ERM 97.11±3.44 98.75±1.19 34.64±9.90 57.41±2.58\n(H-CMNIST),anewdatasetdesignedtoevalu- ASAM 98.57±1.21 98.12±1.74 34.78±8.41 57.07±1.91\nGDRO 99.95±0.07 99.92±0.08 57.53±2.11 61.44±1.03\natewhetherthemodellearnsdiverseinvariant ASGDRO 99.88±0.11 99.83±0.12 66.62±5.61 69.17±6.19\nmechanismssufficiently(Figure3). H-CMNIST Table 1: H-CMNIST Results. TestBed1 evaluates\nevaluate whether the remaining invariant fea- whetherthemodellearnseasyinvariantfeatureZ ,\nColor\nture is additionally learned by the algorithm, andTestBed2evaluatestheabilitytolearnadditional\nassumingthatthemodelhasalreadylearnedone invariantfeatureZ Shape.\ninvariantfeature.\nH-CMNIST includes two invariant features, the color ZI = {Z } and shape of digits ZI =\n1 color 2\n{Z },andonespuriousfeature,thepositionofthebox(BP)ZNI ={Z }. Thatis,eachclasshas\nshape BP\nitsowncolorandshape. UsingBP,weconstructtwoenvironments,TopLeft(Env0)andBottom\nRight(Env1). Itsimplifiesthesituationswherespuriouscorrelationsoccur(Sagawaetal.,2019;\nDeGraveetal.,2021). Specifically,inthetrainingset,95%ofLeftTopBPbelongstoclass0,and\nonly5%belongstoclass1. Incontrast,wecollect95%ofRightBottomBPinclass1,andassigned\nonly5%toclass0. Intestenvironments,thecompositionofBPisflipped. Forexperimentaldetails,\nrefertoAppendixA.6\nTable1showstheresultsofH-CMNIST.H-CMNISTassumesaneasilylearnableinvariantfeature\nZ toevaluatewhetherthemodel,havingalreadylearnedoneinvariantfeature,canlearnadditional\ncolor\ninvariantfeaturesZ . Concretely,TestBed1servesasapreliminarysteptoverifythataneasily\nshape\nlearnableinvariantfeatureisindeedpresent. InTestBed1,theperformanceofallalgorithmsissimilar\nregardlessofthepresenceofthespuriousfeatureZ ,indicatingthatallhavelearnedatleastone\nBP\ninvariantfeature.\nHowever,inTestbed2,withoutZ ,bothERMandASAMshowsignificantperformancediscrep-\ncolor\nanciesdependingonthepresenceofspuriousfeatureZ . ComparedwiththeresultsofTestBed1,\nBP\nERMandASAMonlylearnZ successfully,buttheyfailtocapturetheadditionalinvariantfeature,\ncolor\nZ . Itindicatesthatevenwhenarelativelyeasierinvariantfeatureexists,thespuriousfeature\nshape\ninfluencestherelativelymorechallenginginvariantfeature. AlthoughGDROexhibitsrobustnessto\nspuriouscorrelationscomparedtoERMandASAM,itstillfailstolearnoneoftheinvariantfeatures,\nZ . However,ASGDROmakesrobustpredictionsagainstspuriousfeaturesandmoresuccessful\nshape\nlearningofshapefeaturesinTestBed2,comparedtootherbaselines. ItimpliesthatSILisnecessary\nfortherobustmodelandASGDROoptimizesthemodeltolearnsufficientlydiverseinvariantfeatures\nZI ={Z ,Z }consideringthecommonflatminimaacrossenvironments.\nshape shape\n8\nCMNIST Waterbirds CelebA CivilComments PT–FT Ca Am vge .ly (o %n )17 Civ WilC oro sm t(m %e )nts WF oM rso tW (%) 10A thm pa ez r.o (n %) AR vx gR .(x %1 )\nAvg. Worst Avg. Worst Avg. Worst Avg. Worst\n×–ERM 70.3±6.4 56.0±3.6 32.3±1.3 53.8±0.8 29.9±0.4\nERM‡ 27.8% 0.0% 97.0%63.7% 94.9%47.8% 92.2% 56.0% ×–GDRO 68.4±7.3 70.0±2.0 30.8±0.8 53.3±0.0 23.0±0.3\nASAM 40.5%34.1% 97.4%72.4% 93.7%46.5% 92.3% 58.9% ×–IRM 64.2±8.1 66.3±2.1 30.0±1.4 52.4±0.8 8.2±1.1\nIRM‡ 72.1%70.3% 87.5%75.6% 94.0%77.8% 88.8% 66.3% ERM–ERM 74.3±6.0 55.5±1.8 33.6±1.0 51.1±0.6 30.2±0.1\nIB-IRM‡ 72.2%70.7% 88.5%76.5% 93.6%85.0% 89.1% 65.3% ERM–GDRO 76.1±6.5 69.5±0.2 33.0±0.5 52.0±0.0 30.0±0.1\nV-REx‡ 71.7%70.2% 88.0%73.6% 92.2%86.7% 90.2% 64.9% ERM–IRM 75.7±7.4 68.8±1.0 33.5±1.1 52.0±0.0 30.1±0.1\nCORAL‡ 71.8%69.5% 90.3%79.8% 93.8%76.9% 88.7% 65.6% Bonsai–ERM 74.0±5.3 63.3±3.5 31.9±0.5 48.6±0.6 24.2±0.4\nGDRO‡ 72.3%68.6% 91.8%90.6% 92.1%87.2% 89.9% 70.0% Bonsai–GDRO 72.8±5.4 70.2±1.3 33.1±1.2 42.7±1.1 23.0±0.5\nDomainMix‡ 51.4%48.0% 76.4%53.0% 93.4%65.6% 90.9% 63.6% Bonsai–IRM 73.6±6.2 68.4±2.0 32.5±1.2 47.1±0.6 23.4±0.4\nFish‡ 46.9%35.6% 85.6%64.0% 93.1%61.2% 89.8% 71.1% FeAT–ERM 77.8±2.5 68.1±2.3 33.1±0.8 52.9±0.6 30.7±0.4\nLISA‡ 74.0%73.3% 91.8%89.2% 92.4%89.3% 89.2% 72.6% FeAT–GDRO 80.4±3.3 71.3±0.5 33.6±1.7 52.6±0.6 30.0±0.1\nPDE‡‡ –% –% 92.4%90.3% 92.0%91.0% 86.3% 71.5% FeAT–IRM 78.0±3.1 70.3±1.1 34.0±0.7 52.9±0.6 30.0±0.2\n×–ASGDRO 81.0±3.8 71.8±0.4 35.0±0.3 54.5±0.5 30.5±0.1\nASGDRO 74.8%74.2% 92.3%91.4% 92.1%91.0% 90.2% 71.8%\nTable3: WildsBenchmark. Out-of-distribution\nTable2: SubpopulationShift. ‡denotestheperfor-\ngeneralizationperformancesonwildsbenchmark\nmancereportedfrom(Yaoetal.,2022),and‡‡denotes\nwithrichrepresentation. Theperformancesofthe\ntheperformancereportedfrom(Dengetal.,2024).Avg.\nbaselinemodelsarethereportedresultsfromKoh\ndenotes average accuracy, and Worst denotes worst\ngroupaccuracy.RefertoAppendixA.7forerrorbars et al. (2021) and Chen et al. (2024a). Refer to\nandexperimentaldetails. AppendixA.8forerrorbars.\n4.3 ExperimentalResults\nIn all experiments except for the toy\nMethod PACS VLCS OH TI DN Avg\nexample, instead of calculating ϵ∗ =\ne ERM† 85.5 77.5 66.5 46.1 40.9 63.3\nρ T θ2∇Re(θ) for each environment, we IRM† 83.5 78.6 64.3 47.6 33.9 61.6\n|Tθ∇Re(θ)| GDRO† 84.4 76.7 66.0 43.2 33.3 60.7\nuse a common adversarial perturbation I-Mixup† 84.6 77.4 68.1 47.9 39.2 63.4\nby utilizing the empirical risk R (θ) = MMD† 84.7 77.5 66.4 42.2 23.4 58.8\n1 (cid:80) (cid:80) ℓ(f(Xe;θ),YS e), i.e., S Aa Rg MNe †t† 8 86 5. .3 1 7 77 7. .8 6 6 68 4. .1 8 4 48 5. .6 5 4 30 5. .3 5 6 64 1. .2 7\n|De||Etr| e∈Etr ne\nVREx† 84.9 78.3 66.4 46.4 33.6 61.9\nϵ∗ =ρ T θ2∇RS(θ) . Ineachperformanceta- RSC† 85.2 77.1 65.5 46.6 38.9 62.7\n|Tθ∇RS(θ)| GSAM(Zhuangetal.,2021) 85.9 79.1 69.3 47.0 44.6 65.1\nble,boldfaceandunderlinedtextrepresent RDM(Nguyenetal.,2024) 87.2 78.4 67.3 47.5 43.4 64.8\nthe highest and second-highest accuracy RS-SCM(Chenetal.,2024b) 85.8 77.6 68.8 47.6 42.5 64.4\nLFME(Chenetal.,2024c) 85.0 78.4 69.1 48.3 42.1 64.6\nforeachdataset,respectively.\nDPLCLIP 96.6 79.0 82.7 45.4 59.1 72.6\nDPLCLIP+GDRO 95.9 79.7 83.6 46.0 59.1 72.9\nWeconductexperimentsforsubpopulation\nDPLCLIP+ASGDRO 96.8 80.7 83.7 48.9 59.8 74.0\nshift,CMNIST(Arjovskyetal.,2019),Wa-\nterbirds(Sagawaetal.,2019),CelebA(Liu Table4: DomainBed.Thesymbol†indicatesreportedper-\netal.,2015),andCivilComments(Borkan formanceinGulrajaniandLopez-Paz(2020). RefertoAp-\npendixA.9forerrorbarsandexperimentaldetails.\netal.,2019). Thegoalofthesubpopulation\nshifttaskistoobtainthebetterworstgroupperformancebylearninginvariantfeatures. Different\nfromH-CMNIST,thespuriouscorrelationactsasastrongershortcut. Asaresult,themodelscannot\nlearn any invariant feature easily. Table 2 shows the results of subpopulation shift experiments.\nASAM,whichconsidersflatness,failstoeliminatespuriouscorrelationsandshowslimitedpredic-\ntiveaccuracyontheworstgroup. Ontheotherhand, ASGDROshowsthebestandworstgroup\nperformanceforalldataexceptCivilComments. ForCivilCommentsdata,ASGDROalsoshows\ncomparableperformancewiththebestalgorithmsamongthebaselines. ComparedtoGDRO,the\nprimarydistinctionofASGDROisitsabilitytofindacommonflatminima,whichnotonlyenhances\nrobustnessfortheworstgroupbutalsoreducesthegapbetweenaverageaccuracyandworstgroup\naccuracy. Therefore,Table2providessupportforourclaimthatsufficientlylearningdiverseinvariant\nmechanismsleadstorobustgeneralizationperformance.\nOneapproachtotrainingarobustmodelistoenrichtherepresentationlearningofinvariantfeatures\nZhang et al. (2022); Chen et al. (2024a) rather than training by ERM. This process consists of a\npre-training(PT)stagededicatedtorepresentationlearning,followedbyafine-tuning(FT)stage\nutilizing existing invariant learning algorithms. In Table 3, we compare these algorithms with\nASGDRO,evaluatedontheWildsbenchmarkdataset,whichincludesvarioustypesofdistribution\nshiftscollectedfromreal-worldscenarios. Notably,thesuperiorperformanceofASGDRO,even\ncompared to invariant learning algorithms trained with rich representations during the FT stage,\nsuggeststhatitisimportantnotonlytolearnrichrepresentationsofinvariantfeaturesbutalsoto\nensurethatpredictionsarecomposedusingdiverseinvariantfeaturesbylearningsufficientlydiverse\ninvariantmechanisms.\n9\nFigure4: Grad-CAMASGDROlearnsdiverse Figure5: HessianAnalysisonCelebA.ASGDROfinds\ninvariantfeatures. thecommonflatminimaforallgroups.\nASGDROisamodel-agnosticmethodandiseasilyappliedtovariousalgorithms.WeapplyASGDRO\nwithDPLCLIP(Zhangetal.,2021),whichperformsthepromptlearningfordomaingeneralization.\nWeconductDomainBedbenchmark(GulrajaniandLopez-Paz,2020),whichisthemostcommonly\nusedforevaluatingdomaingeneralizationperformanceunderafairsetting. Table4presentsthat\nASGDROshowsbetterperformanceinalldatasetscomparedtoDPLCLIP.ASGDROalsoachieves\nbetterdomaingeneralizationperformancethanthealgorithmthatcombinedDPLCLIPandGDRO.\n4.4 VisualInterpretationbyGrad-CAM\nWeconductGrad-CAManalysistoverifywhethertheeffectoflearningSILisbeingproperlyapplied\non the ground-truth label (Figure 4). The minority group, land birds on a water background, is\nunderrepresentedbythespuriouscorrelationasithasonlyafewsamples. ERMandASAMuse\nseveralfeaturestopredictthemajoritygroup,landbirdsonalandbackground,butfailtoremove\nspuriouscorrelation. Asaresult, theyalsousethebackgroundfeature. Fortheminoritygroups,\nhowever,onlyasmallpartoftheinvariantfeaturesisobservedtobeusedforprediction. GDRO\nsuccessfully removes spurious correlation regardless of the group but still uses only the part of\ninvariantfeaturesforprediction. Ontheotherhand,ASGDROfocusesonvariousinvariantfeatures\nforpredictionregardlessofthegroup;thatis,itsufficientlyusesdiverseinvariantfeaturesofland\nbirds. Additionally,ASGDROsuccessfullyexcludesspuriousfeaturesintheirprediction. Appendix\nA.10providesadditionalresultsonGrad-CAM.\n4.5 HessianAnalysis\nInFigure5,wereporttheeigenvaluesoftheHessianmatrixtomeasureandcomparetheflatness\nofthemodelYaoetal.(2020). Alowereigenvalueindicatesaflatterminima. ComparedtoGDRO,\nASGDRO exhibits lower eigenvalues across all groups. Furthermore, GDRO shows particularly\nsharperminimainGroup2and3,whichincludeminoritygroups. Incontrast,ASGDROmaintains\nrelativelyuniformeigenvaluesregardlessofthegroup. ThissuggeststhatASGDROindeedfindsa\ncommonflatminima,withtheregularizationforsuchminimaenablingthemodeltomakerobust\npredictions by leveraging diverse invariant mechanisms. Refer to Appendix A.11 for additional\nexperimentalanalysis.\n5 Conclusion\nThis study highlights the significance of SIL, which promotes the learning of diverse invariant\nfeatures. Unliketraditionalinvariantlearning,SILenablesmodelstoleveragethesediverseinvariant\nmechanismsforprediction,ensuringrobustnesseveninenvironmentswheresomeinvariantfeatures\nareunobserved. WealsointroduceASGDRO,thefirstSILalgorithmspecificallydesignedtoidentify\ncommon flat minima across environments. Through both theoretical analysis and experimental\nvalidation,wedemonstratethatASGDROeffectivelylearnsadiverseinvariantmechanismsufficiently\nandfindsacommonflatminima,whichinturnfacilitatesSIL.Wefurthervalidatetheeffectivenessof\nSILbydemonstratingthegeneralizationcapabilitiesofASGDROonournewlydevelopedsynthetic\nSILdataset,H-CMNIST,aswellasonvarioustypesofdistributionshiftbenchmarkdatasets.\n10\nReferences\nMartinArjovsky,LéonBottou,IshaanGulrajani,andDavidLopez-Paz. Invariantriskminimization. arXiv\npreprintarXiv:1907.02893,2019.\nPangWeiKoh,ShioriSagawa,HenrikMarklund,SangMichaelXie,MarvinZhang,AkshayBalsubramani,\nWeihuaHu,MichihiroYasunaga,RichardLanasPhillips,IrenaGao,etal.Wilds:Abenchmarkofin-the-wild\ndistributionshifts. InInternationalConferenceonMachineLearning,pages5637–5664.PMLR,2021.\nIshaanGulrajaniandDavidLopez-Paz.Insearchoflostdomaingeneralization.arXivpreprintarXiv:2007.01434,\n2020.\nKrikamolMuandet,DavidBalduzzi,andBernhardSchölkopf. Domaingeneralizationviainvariantfeature\nrepresentation. InInternationalconferenceonmachinelearning,pages10–18.PMLR,2013.\nYaLi,XinmeiTian,MingmingGong,YajingLiu,TongliangLiu,KunZhang,andDachengTao. Deepdomain\ngeneralizationviaconditionalinvariantadversarialnetworks. InProceedingsoftheEuropeanconferenceon\ncomputervision(ECCV),pages624–639,2018.\nShioriSagawa,PangWeiKoh,TatsunoriBHashimoto,andPercyLiang.Distributionallyrobustneuralnetworks.\nInInternationalConferenceonLearningRepresentations,2019.\nHuaxiuYao,YuWang,SaiLi,LinjunZhang,WeixinLiang,JamesZou,andChelseaFinn. Improvingout-of-\ndistributionrobustnessviaselectiveaugmentation. InInternationalConferenceonMachineLearning,pages\n25407–25437.PMLR,2022.\nPIzmailov,AGWilson,DPodoprikhin,DVetrov,andTGaripov. Averagingweightsleadstowideroptimaand\nbettergeneralization. In34thConferenceonUncertaintyinArtificialIntelligence2018,UAI2018,pages\n876–885,2018.\nSiyuan Guo, Jonas Bernhard Wildberger, and Bernhard Schölkopf. Out-of-variable generalisation for dis-\ncriminativemodels. InTheTwelfthInternationalConferenceonLearningRepresentations,2024. URL\nhttps://openreview.net/forum?id=zwMfg9PfPs.\nAlexeyTsymbal. Theproblemofconceptdrift:definitionsandrelatedwork. ComputerScienceDepartment,\nTrinityCollegeDublin,106(2):58,2004.\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for\nefficientlyimprovinggeneralization. InInternationalConferenceonLearningRepresentations,2020.\nVladimirVapnik. Thenatureofstatisticallearningtheory. Springerscience&businessmedia,1999.\nYugeShi,JeffreySeely,PhilipHSTorr,NSiddharth,AwniHannun,NicolasUsunier,andGabrielSynnaeve.\nGradientmatchingfordomaingeneralization. arXivpreprintarXiv:2104.09937,2021.\nAlexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-\ndistributiongeneralization. InInternationalConferenceonMachineLearning,pages18347–18377.PMLR,\n2022a.\nPolinaKirichenko,PavelIzmailov,andAndrewGordonWilson.Lastlayerre-trainingissufficientforrobustness\ntospuriouscorrelations. arXivpreprintarXiv:2204.02937,2022.\nJianyuZhang,DavidLopez-Paz,andLéonBottou. Richfeatureconstructionfortheoptimization-generalization\ndilemma. InInternationalConferenceonMachineLearning,pages26397–26411.PMLR,2022.\nYongqiang Chen, Wei Huang, Kaiwen Zhou, Yatao Bian, Bo Han, and James Cheng. Understanding and\nimprovingfeaturelearningforout-of-distributiongeneralization. AdvancesinNeuralInformationProcessing\nSystems,36,2024a.\nJianyu Zhang and Léon Bottou. Learning useful representations for shifting tasks and distributions. In\nInternationalConferenceonMachineLearning,pages40830–40850.PMLR,2023.\nMateoRojas-Carulla,BernhardSchölkopf,RichardTurner,andJonasPeters.Invariantmodelsforcausaltransfer\nlearning. JournalofMachineLearningResearch,19(36):1–34,2018.\nNitishShirishKeskar,DheevatsaMudigere,JorgeNocedal,MikhailSmelyanskiy,andPingTakPeterTang. On\nlarge-batchtrainingfordeeplearning:Generalizationgapandsharpminima.arXivpreprintarXiv:1609.04836,\n2016.\n11\nBehnamNeyshabur,SrinadhBhojanapalli,DavidMcAllester,andNatiSrebro. Exploringgeneralizationindeep\nlearning. Advancesinneuralinformationprocessingsystems,30,2017.\nJungminKwon,JeongseopKim,HyunseoPark,andInKwonChoi. Asam:Adaptivesharpness-awareminimiza-\ntionforscale-invariantlearningofdeepneuralnetworks. InInternationalConferenceonMachineLearning,\npages5905–5914.PMLR,2021.\nJunbumCha,SanghyukChun,KyungjaeLee,Han-CheolCho,SeunghyunPark,YunsungLee,andSungrae\nPark. Swad: Domaingeneralizationbyseekingflatminima. AdvancesinNeuralInformationProcessing\nSystems,34:22405–22418,2021.\nAlexandreRame,MatthieuKirchmeyer,ThibaudRahier,AlainRakotomamonjy,PatrickGallinari,andMatthieu\nCord. Diverseweightaveragingforout-of-distributiongeneralization. AdvancesinNeuralInformation\nProcessingSystems,35:10821–10836,2022b.\nXingxuanZhang,RenzheXu,HanYu,YanchengDong,PengfeiTian,andPengCu.Flatness-awareminimization\nfordomaingeneralization. arXivpreprintarXiv:2307.11108,2023.\nJacobMitchellSpringer,VaishnavhNagarajan,andAditiRaghunathan.Sharpness-awareminimizationenhances\nfeaturequalityviabalancedlearning. InTheTwelfthInternationalConferenceonLearningRepresentations,\n2024. URLhttps://openreview.net/forum?id=3xDaj4pRna.\nElliotCreager,Jörn-HenrikJacobsen,andRichardZemel. Environmentinferenceforinvariantlearning. In\nInternationalConferenceonMachineLearning,pages2189–2200.PMLR,2021.\nDavidKrueger,EthanCaballero,Joern-HenrikJacobsen,AmyZhang,JonathanBinas,DinghuaiZhang,Remi\nLePriol,andAaronCourville. Out-of-distributiongeneralizationviariskextrapolation(rex). InInternational\nConferenceonMachineLearning,pages5815–5826.PMLR,2021.\nKartikAhuja, EthanCaballero, DinghuaiZhang, Jean-ChristopheGagnon-Audet, YoshuaBengio, Ioannis\nMitliagkas,andIrinaRish. Invarianceprinciplemeetsinformationbottleneckforout-of-distributiongeneral-\nization. AdvancesinNeuralInformationProcessingSystems,34:3438–3450,2021.\nJohnDuchi,PeterGlynn,andHongseokNamkoong. Statisticsofrobustoptimization:Ageneralizedempirical\nlikelihoodapproach. arXivpreprintarXiv:1610.03425,2016.\nYonatanOren, ShioriSagawa, TatsunoriBHashimoto, andPercyLiang. Distributionallyrobustlanguage\nmodeling. arXivpreprintarXiv:1909.02060,2019.\nGabrielIlharco,MarcoTulioRibeiro,MitchellWortsman,SuchinGururangan,LudwigSchmidt,Hannaneh\nHajishirzi,andAliFarhadi. Editingmodelswithtaskarithmetic. arXivpreprintarXiv:2212.04089,2022.\nMitchellWortsman,GabrielIlharco,JongWookKim,MikeLi,SimonKornblith,RebeccaRoelofs,RaphaelGon-\ntijoLopes,HannanehHajishirzi,AliFarhadi,HongseokNamkoong,etal. Robustfine-tuningofzero-shot\nmodels. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages\n7959–7971,2022.\nSamuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo\npermutationsymmetries,2022. URLhttps://arxiv.org/abs/2209.04836.\nAlexandreRamé,KartikAhuja,JianyuZhang,MatthieuCord,LéonBottou,andDavidLopez-Paz. Model\nratatouille:Recyclingdiversemodelsforout-of-distributiongeneralization. InInternationalConferenceon\nMachineLearning,pages28656–28679.PMLR,2023.\nBehnamNeyshabur,HanieSedghi,andChiyuanZhang.Whatisbeingtransferredintransferlearning? Advances\ninneuralinformationprocessingsystems,33:512–523,2020.\nYangZhao,HaoZhang,andXiuyuanHu. Penalizinggradientnormforefficientlyimprovinggeneralizationin\ndeeplearning. arXivpreprintarXiv:2202.03599,2022.\nAlexJDeGrave,JosephDJanizek,andSu-InLee. Aiforradiographiccovid-19detectionselectsshortcutsover\nsignal. NatureMachineIntelligence,3(7):610–619,2021.\nYihe Deng, Yu Yang, Baharan Mirzasoleiman, and Quanquan Gu. Robust learning with progressive data\nexpansionagainstspuriouscorrelation. Advancesinneuralinformationprocessingsystems,36,2024.\nJuntangZhuang,BoqingGong,LiangzheYuan,YinCui,HartwigAdam,NichaCDvornek,JamessDuncan,\nTingLiu,etal. Surrogategapminimizationimprovessharpness-awaretraining. InInternationalConference\nonLearningRepresentations,2021.\n12\nToanNguyen,KienDo,BaoDuong,andThinNguyen. Domaingeneralisationviariskdistributionmatching. In\nProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision,pages2790–2799,\n2024.\nZiliangChen,YongsenZheng,Zhao-RongLai,QuanlongGuan,andLiangLin. Diagnosingandrectifying\nfakeoodinvariance:Arestructuredcausalapproach. InProceedingsoftheAAAIConferenceonArtificial\nIntelligence,volume38,pages11471–11479,2024b.\nLiangChen,YongZhang,YibingSong,ZhiqiangShen,andLingqiaoLiu. Lfme: Asimpleframeworkfor\nlearningfrommultipleexpertsindomaingeneralization. arXivpreprintarXiv:2410.17020,2024c.\nZiweiLiu,PingLuo,XiaogangWang,andXiaoouTang.Deeplearningfaceattributesinthewild.InProceedings\noftheIEEEinternationalconferenceoncomputervision,pages3730–3738,2015.\nDanielBorkan,LucasDixon,JeffreySorensen,NithumThain,andLucyVasserman. Nuancedmetricsfor\nmeasuringunintendedbiaswithrealdatafortextclassification. InCompanionproceedingsofthe2019world\nwidewebconference,pages491–500,2019.\nXin Zhang, Yusuke Iwasawa, Yutaka Matsuo, and Shixiang Shane Gu. Amortized prompt: Lightweight\nfine-tuningforclipindomaingeneralization. arXivpreprintarXiv:2111.12853,2021.\nZheweiYao,AmirGholami,KurtKeutzer,andMichaelWMahoney. Pyhessian:Neuralnetworksthroughthe\nlensofthehessian. In2020IEEEinternationalconferenceonbigdata(Bigdata),pages581–590.IEEE,\n2020.\nJiaweiDu,HanshuYan,JiashiFeng,JoeyTianyiZhou,LiangliZhen,RickSiowMongGoh,andVincentYF\nTan. Efficient sharpness-aware minimization for improved training of neural networks. arXiv preprint\narXiv:2110.03141,2021.\nJiaweiDu,DaquanZhou,JiashiFeng,VincentYFTan,andJoeyTianyiZhou. Sharpness-awaretrainingforfree.\narXivpreprintarXiv:2205.14083,2022.\nMaksymAndriushchenko, DaraBahri, HosseinMobahi, andNicolasFlammarion. Sharpness-awaremini-\nmizationleadstolow-rankfeatures. AdvancesinNeuralInformationProcessingSystems,36:47032–47051,\n2023.\nKaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecognition. In\nProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages770–778,2016.\nYannLeCun,LéonBottou,YoshuaBengio,andPatrickHaffner. Gradient-basedlearningappliedtodocument\nrecognition. ProceedingsoftheIEEE,86(11):2278–2324,1998.\nCatherineWah,SteveBranson,PeterWelinder,PietroPerona,andSergeBelongie. Thecaltech-ucsdbirds-200-\n2011dataset. 2011.\nBoleiZhou,AgataLapedriza,AdityaKhosla,AudeOliva,andAntonioTorralba. Places:A10millionimage\ndatabaseforscenerecognition. IEEEtransactionsonpatternanalysisandmachineintelligence, 40(6):\n1452–1464,2017.\nEvanZLiu,BehzadHaghgoo,AnnieSChen,AditiRaghunathan,PangWeiKoh,ShioriSagawa,PercyLiang,\nandChelseaFinn. Justtraintwice: Improvinggrouprobustnesswithouttraininggroupinformation. In\nInternationalConferenceonMachineLearning,pages6781–6792.PMLR,2021.\nZongboHan,ZhipengLiang,FanYang,LiuLiu,LanqingLi,YataoBian,PeilinZhao,BingzheWu,Changqing\nZhang,andJianhuaYao. Umix: Improvingimportanceweightingforsubpopulationshiftviauncertainty-\nawaremixup. arXivpreprintarXiv:2209.08928,2022.\nVictorSanh,LysandreDebut,JulienChaumond,andThomasWolf.Distilbert,adistilledversionofbert:smaller,\nfaster,cheaperandlighter. arXivpreprintarXiv:1910.01108,2019.\nIlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. arXivpreprintarXiv:1711.05101,\n2017.\nAdinaWilliams, NikitaNangia, andSamuelRBowman. Abroad-coveragechallengecorpusforsentence\nunderstandingthroughinference. arXivpreprintarXiv:1704.05426,2017.\nChenFang,YeXu,andDanielNRockmore. Unbiasedmetriclearning:Ontheutilizationofmultipledatasets\nandwebimagesforsofteningbias. InProceedingsoftheIEEEInternationalConferenceonComputerVision,\npages1657–1664,2013.\n13\nDaLi, YongxinYang, Yi-ZheSong, andTimothyMHospedales. Deeper, broaderandartierdomaingen-\neralization. InProceedingsoftheIEEEinternationalconferenceoncomputervision,pages5542–5550,\n2017.\nHemanthVenkateswara,JoseEusebio,ShayokChakraborty,andSethuramanPanchanathan. Deephashing\nnetworkforunsuperviseddomainadaptation. InProceedingsoftheIEEEconferenceoncomputervisionand\npatternrecognition,pages5018–5027,2017.\nSaraBeery,GrantVanHorn,andPietroPerona. Recognitioninterraincognita. InProceedingsoftheEuropean\nconferenceoncomputervision(ECCV),pages456–473,2018.\nXingchaoPeng, QinxunBai, XideXia, ZijunHuang, KateSaenko, andBoWang. Momentmatchingfor\nmulti-sourcedomainadaptation. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputer\nvision,pages1406–1415,2019.\nRamprasaathR.Selvaraju,MichaelCogswell,AbhishekDas,RamakrishnaVedantam,DeviParikh,andDhruv\nBatra. Grad-cam:Visualexplanationsfromdeepnetworksviagradient-basedlocalization. In2017IEEE\nInternationalConferenceonComputerVision(ICCV),pages618–626,2017. doi:10.1109/ICCV.2017.74.\n14\nA Appendix: SufficientInvariantLearningforDistributionShift\nA.1 LimitationsandFutureWorks\nASGDRO utilizes adversarial perturbations to find flat minima, similar to SAM. It requires two\nforward and backward passes in a single training iteration, which is one of the persistent issues\nwithSAM-basedalgorithms. However,recentresearchhasbeenactivelyfocusingonimprovingthe\ncomputationalcostofSAM(Duetal.,2021,2022). ThecomputationalcostofASGDROcanalsobe\nimprovedinasimilarcontext,andweconsiderthistobeafuturework.\nToevaluatewhetherthealgorithmeffectivelylearnsdiverseinvariantmechanismssufficientlyand\nperformsrobustpredictions,anewbenchmarkdatasetisnecessary. Unlikeexistinginvariantlearning\nbenchmarksthatonlyrequireasmallnumberofattributes,constructinganSILbenchmarkdemands\nrichattributeannotationstoformmultipleinvariantfeatures. Inthispaper,weattempttovalidate\nSILusingH-CMNIST,butitisasyntheticdatasetbasedonMNIST.Thisimpliestheneedforanew\nbenchmarktovalidateSILonreal-worlddata,whichweleaveitasafuturework.\nA.2 Thesubsetrelationshipofinvariantfeatures\nInDefinition3,h (ZˆI)referstoaclassifierthatreliessolelyonZˆI ⊆ZI. Givenasinglesample,if\nθg\nanyinvariantfeaturewithinZˆIisobserved,weexpectthelossevaluatedbytheclassifiertobevery\nsmall. FortwodifferentsubsetZˆI,ZˆI ⊆ZˆIthatsatisfyZˆI ⊆ZˆI,thefollowinginequalityholds:\na b b a\nP(ZˆI ⊆ZˆI isobservedine∈E)≤P(ZˆI ⊆ZˆI isobservedine∈E).\ni b i a\nwhereP denotestheprobability. NotethatZIalsocanbepartitionedasfollows:\np\nZI = (cid:91) {ZˆI ||ZˆI|=i},\ni=1\nwhere|·|denotesthecardinalityofasetandpthenumberofinvariantfeatures. Itfollowsthat\n(cid:104)\nmax E[ℓ(h (ZˆI),Ye)]=max E[ℓ(h (ZI),Ye)],\nZˆI⊆ZI\nθh θh\nmax E[ℓ(h (ZˆI),Ye)],\nZˆI⊆ZI\nθh\ns.t.|ZˆI|=p−1\n...,\n(cid:105)\nmax E[ℓ(h (ZˆI),Ye)]\nZˆI⊆ZI\nθh\ns.t.|ZˆI|=1\n= max E[ℓ(h (ZˆI),Ye)]\nZˆI⊆ZI\nθh\ns.t.|ZˆI|=1\n= max E[ℓ(h (ZI),Ye)],\nZI⊂ZI\nθh i\ni\nassumingthatobservingadditionalinvariantfeaturesdonotadverselyaffecttheperformanceofthe\ncurrentmodel.\nA.3 ProofofProposition1\nProposition1(CommonFlatMinima). BytheTaylorexpansion,\nmax max Re(θ+ϵ )≈max[Re(θ)+ρ||∇Re(θ)||]. (7)\ne\ne∈E ||ϵe||≤ρ e∈E\nASGDROleadstoaregularizationofthegradientnorm,Re,||∇Re(θ)||,acrossenvironments,which\ndrivesthemodeltoconvergetocommonflatminima.\n15\nProof. RecallthatobjectivefunctionofASGDRO(Equation5)isasfollows:\nmax max Re(θ+ϵ ).\ne\ne∈E ||ϵe||≤ρ\nWeuseE insteadofE ,sincethispropertyofASGDROholdsinanysetofenvironments. AsRe(θ)\ntr\nisindependentofϵ ,itcanbefactoredoutofthemaximizationtermoverϵ asfollows:\ne e\nmax max Re(θ+ϵ )=max[Re(θ)+ max [Re(θ+ϵ )−Re(θ)]]\ne e\ne∈E ||ϵe||≤ρ e∈E ||ϵe||≤ρ\nNotethatweintentionallyaddandsubtractR toreformulatetheexpression,enablingtheseparation\ne\noftermsforcleareranalysis. UsingtheTaylorapproximationexpandeduptothefirst-orderterm,we\nhave:\nmax[Re(θ)+ max [Re(θ+ϵ )−Re(θ)]]≈max[Re(θ)+ max [ϵ ·∇Re(θ)]]\ne e\ne∈E ||ϵe||≤ρ e∈E ||ϵe||≤ρ\n=max[Re(θ)+ϵ∗·∇Re(θ)], (16)\ne\ne∈E\nwhereϵ∗ =ρ ∇Re(θ) . NotethatEquation16holdsbecausethemaximumvalueover||ϵ ||≤ρis\ne ||∇Re(θ)|| e\nachievedwhenϵ and∇Re(θ)arealignedinthesamedirection(Foretetal.,2020). Bysubstituting\ne\nϵ∗,weobtainthefollowingequation:\ne\nmax[Re(θ)+ϵ∗·∇Re(θ)]=max[Re(θ)+ρ||∇Re(θ)||].\ne\ne∈E e∈E\nZhaoetal.(2022)demonstratethatminimizingthegradientnormoftheriskleadstofindingflat\nminima. Equation 17 minimizes both risk and the gradient norm of risk for each environment.\nConsequently, ASGDRO constrains the training process to find a common flat minimum across\nenvironments.\nA.4 ProofofTheorem1\nTheorem1(SufficientInvariantMechanism). LetθI beaconvexcombinationofθI,whereλisa\nλ i\np-dimensionalvector.Considermean-squarederrorasthelossfunction.Then,givenZI =(1,...,1)\nwith|Z|=p,\nλ∗ =argminmax max Re(θI +ϵ)\nλ\nλ e∈Etr||ϵ||≤ρ\n=argminmax(cid:2) Re(θI)+ρ||λ||·||∇Re(θI)||(cid:3)\n(6)\nλ λ\nλ e∈Etr\n1 1\n=argmin||λ||=( ,..., )\np p\nλ\nwhere||·||denotesL norm.\n2\nProof. Inthissetting,weconsiderasingleinputforeachenvironmente.Supposetherearepinvariant\nfeatures,andeveryinvariantfeaturehasthesameactivation:\nZI =(1,...,1),\nwhere |ZI| = p. We assume that all spurious features are completely removed. Thus, Z =\n(ZI,ZNI)=ZI,where|Z|=p. Consequently,theriskforZ isidenticalacrossallenvironmentse:\nRe(θ)=Re′ (θ)=c foranye,e′ ∈E , (17)\ntr\nwherecisaconstant. GivenZI,wefocusonlyontheparametersoftheclassifier,denotedbyθI.\nRecall that the classifier satisfying Equation 2, and Equation 17, is not unique. Define θI as the\ni\nclassifierthatutilizesonlythei-thelementofZI.\nForsimplicity,letθI beacolumnvectorwhereonlythei–thelementisone,andallotherelements\ni\narezero:\nZIθI =ZI =1.\ni i\n16\nFurthermore,theconvexcombinationofθI alsoyieldsanequivalentoutput:\ni\np\n(cid:88)\nZI λ θI =1,\ni i\ni=1\nwhere\n(cid:80)p\nλ = 1 and 0 ≤ λ ≤ 1 for all i ∈ {1,...,p}. We denote the current classifier as\ni=1 i i\nθI :=(cid:80)p λ θI,whereλ=(λ ,...,λ ). FromProposition1,weknow:\nλ i=1 i i 1 p\nmax max Re(θ+ϵ )=max[Re(θ)+ρ||∇ Re(θ)||]. (18)\ne θ\ne∈E ||ϵe||≤ρ e∈Etr\nFor the mean-squared error loss function Re(θ) = 1∥Ye −(cid:80)p θ ∥2, the gradient is given by\n2 i=1 i\n∇Re(θ)=−(Ye−(cid:80)p\nθ )·1,where1isap-dimensionalvectorwhoseelementsareallequalto\ni=1 i\n1. SubstitutingθI intoEquation18,weget:\nλ\nmax max Re(θI +ϵ )=max(cid:2) Re(θI)+ρ∥∇ Re(θI)∥(cid:3) .\nλ e λ θ λ\ne∈E ∥ϵe∥≤ρ e∈Etr\nThissimplifiesto:\nmax(cid:2) Re(θI)+ρ∥−λ⊙∇Re(θI)∥(cid:3) =max(cid:2) Re(θI)+ρ∥λ∥·∥∇Re(θI)∥(cid:3)\n,\nλ λ λ λ\ne∈Etr e∈Etr\nwhere Re(θI) = c for any λ. Since the classifier uses only invariant features, minimizing the\nλ\nadversarialtermreducesto:\nargminmax max Re(θI +ϵ)=argminmax(cid:2) Re(θI)+ρ||λ||·||∇Re(θI)||(cid:3)\nλ λ λ\nλ e∈Etr||ϵ||≤ρ λ e∈Etr\n=argmin||λ||.\nλ\nBytheCauchy-Schwarzinequality:\n(cid:32) p (cid:33)2 p\n(cid:88) (cid:88)\nλ ≤p· λ2 =p·||λ||2.\ni i\ni=1 i=1\nUnderthecondition(cid:80)p λ =1,equalityholdswhenλ = 1 foralli,yielding:\ni=1 i i p\n1 1\nargmin||λ||=( ,..., )\np p\nλ\nA.5 MechanismofASGDROforRemovingSpuriousFeatures\nASGDRO successfully removes spurious features. Inspired by Andriushchenko et al. (2023) we\nreformulatethetwo-layerReLUcasepresentedinthatpapertodemonstratethis.Consideratwo–layer\nReLUnetwork\nf(θ)=⟨θ ,σ(θ x)⟩, (19)\nh g\nwhere θ = (θ ,θ ), θ ∈ Rk×m and θ ∈ Rk. Recall that ASGDRO minimizes the maximum\ng h g h\nsharpnessacrossenvironments:\nmax max Re(θ+ϵ ).\ne\ne∈E ||ϵe||≤ρ\nLete denotetheenvironmentthatattainsthemaximumriskatthecurrentstept.Then,theadversarial\nt\nperturbationisϵ∗ =ρ ∇Ret(θ) (Foretetal.,2020)andtheriskis\net ∥∇Ret(θ)∥\n∇Ret(θ)\nmax Ret(θ+ϵ )=Ret(θ+ρ )\n||ϵet||≤ρ et ∥∇Ret(θ)∥\nUnderthefirst–orderTaylorapproximation,\n(cid:18) ∇Ret(θ) (cid:19)\n∇Ret θ+ρ ≈∇[Ret(θ)+ρ∥∇Ret(θ)∥] (20)\n∥∇Ret(θ)∥\n17\nAndriushchenkoetal.(2023)showsthatundertwo–layerReLUnetwork,theupdateruleforpre-\nactivationofk–thneuronisasfollows:\n(cid:32) (cid:33)\n∥∇f(θ)∥\n⟨θ(k),x⟩(t+1) ≈⟨θ(k),x⟩(t)−ηγ 1+ρ a σ′(⟨θ(k),x⟩)∥x∥2\ng g (cid:112) Ret(θ) k g\n(cid:124) (cid:123)(cid:122) (cid:125)\n(a)\n(cid:112)\nRet(θ)\n−ηρ σ(⟨θ(k),x⟩)∥x∥2,\n∥∇f(θ)∥ g\n(cid:124) (cid:123)(cid:122) (cid:125)\n(b)\nwhereηdenotesthelearningrate,γ =f(θ)−y,i.e. theresidual.\nIn ASGDRO, regularization on the gradient norm has two key effects. First, as seen in term (a),\nthegradientupdatedirectionremainsthesame,butthemodelisupdatedwithalargerlearningrate.\nSecond,interm(b),whenRet(θ)islargeenough,thepre-activationofthek-thneuron,⟨θ g(k),x⟩,\nturns negative. Note that a large Ret implies that highly activated neurons at this point tend to\nencodesignificantinformationfromspuriousfeatures. WhenRet(θ)causesthepre-activationofa\nneurontobecomenegative,thenatureoftheReLUactivationfunctionensuresthattheoutputofthat\nneuronbecomeszero. Thisindicatesthat,underdistributionshifts,regularizationviathecommon\nflatminimainASGDROeffectivelyremovesspuriousfeatures.\nA.6 Heterogeneous-CMNIST(H-CMNIST):ExperimentalDetails\nInH-CMNISTexperiments,weuseResNet18(Heetal.,2016)withSGD.Wealsoconductreweighted\nsamplingwhenthealgorithmsettingcanusetheenvironmentinformation,i.e.,GDRO(Sagawaetal.,\n2019)andASGDRO.IntheH-CMNISTexperiment,wesetthelossofGDROandASGDRObythe\ngroup,notthedomain. Thatis,thereisfourgroups;(Class=0,BP=TopLeft),(Class=0,BP=Bottom\nRight),(Class=1,BP=TopLeft),(Class=1,BP=BottomRight).Forhyperparametertuning,weperform\ngrid search over learning rate, {10−3,10−4}, and L –regularization, {1,10−1,10−3,10−4}. We\n2\nfixthebatchsize,128,andtrainthemodelupto20epochs. ForASAM(Kwonetal.,2021)and\nASGDRO,wesearchthehyperparameterρamong{0.05,0.2,0.5,0.8}. Wefixtherobuststepsize,\nγ,as0.01forGDROandASGDRO.Weevaluatethemodelswiththreerandomseeds.\nA.7 SubpopulationShifts: DatasetsandExperimentalDetails\nDatasetDetails\nInTable2inthemainpaper,weconductourexperimentforsubpopulationshiftswithfivedatasets:\nCMNIST (Arjovsky et al., 2019), Waterbirds (Sagawa et al., 2019), CelebA (Liu et al., 2015),\nCivilComments(Borkanetal.,2019). CMNIST,Waterbirds, andCelebAdatasetscorrespondto\ncomputervisiontasks(Figure6),whileCivilCommentspertaintonaturallanguageprocessingtasks.\nInthissection,wewilldescribeeachdatasetandprovideexperimentaldetails. Toimplementthis,we\nutilizedthecodesprovidedby(Yaoetal.,2022)2.\nColoredMNIST(CMNIST)\nIn the CMNIST dataset provided by (Arjovsky et al., 2019), we perform binary classification to\npredictwhichnumbercorrespondstotheshapeofagivendigit. Specifically,whentheshapeofthe\ndigitcorrespondstoalogitbetween0and4,theclassisassignedas0,andwhenitfallsbetween5\nand9,theclassisassignedas1. However,unliketheoriginalMNISTdataset(LeCunetal.,1998),\nCMNISTintroducescolorasaspuriousfeatureinthetrainingset. Whenthisspuriouscorrelation\nbecomesstrongerthantheinvariantrelationshipbetweentheclassandtheshapeofthedigit,amodel\ntrainedwithoutanyregularizationmaybepronetorelyingonthespuriousfeatureforpredictions.\nWhileArjovskyetal.(2019)constructstwoenvironmentswithdifferentratiosofspuriousfeatures\nin the training set, Yao et al. (2022) uses a single environment to compose the training set. Our\nCMNISTdatasetexperimentfollowsthesamesettingas(Yaoetal.,2022),wherethedatasetconsists\n2https://github.com/huaxiuyao/LISA\n18\nFigure6: CMNIST,Waterbirds,CelebA.Ineachdataset,eachrowrepresentstheclassandeach\ncolumnrepresentsthespuriousfeature. Thenumberswrittenbelowtheimagesrepresenttheratioor\ncountofdatabelongingtoeachgroupinthetrainingdataset,whereeachgroupconsistsof(Class,\nSpuriousFeature)pairs.\noffourgroupswhenconsideringcombinationsof“ShapeofLogit”and“Color”asasinglegroup.\nConcretely,Class0andClass1havesimilarnumbersofdatapoints,butthedistributionofspurious\nfeaturesdiffersbetweenthetwoclasses. Class0consistsof80%redlogitsand20%greenlogits,\nwhileClass1has80%greenlogitsand20%redlogits. Furthermore,withineachclass,25%ofthe\ndataactsaslabelnoise,havingalogitshapethatdoesnotcorrespondtoitsclass. Therefore,the\nspuriousfeature,color,formsastrongercorrelationbetweenclassescomparedtothatoftheinvariant\nfeature,theshapeoflogits.\nThe validation set is constructed with an equal number of instances per group. The worst-group\naccuracy,definedasthelowestaccuracyamongallthegroups,isutilizedtoselectthebestmodel.\nForthetestset,weassumeadistributionofthespuriousfeaturethatisoppositetothetrainingset.\nSpecifically,forClass0,90%ofthedatahasaredcolor,and10%hasagreencolor,whileforClass1,\nitistheopposite. Itisdonetoassesswhetherthemodelreliesonthespuriousfeatureforpredictions.\nWaterbirds\nWaterbirds dataset, constructed by (Sagawa et al., 2019), is designed for the task of determining\nwhether a bird belongs to the Landbird or Waterbird class. It consists of images of birds, from\n(Wahetal.,2011),astheinvariantfeature,whilethespuriousfeatureisthebackground,from(Zhou\netal.,2017),whichcaneitherbeWaterorLandbackground. Indeed,intheWaterbirdsdataset,the\ngroupsareformedbythecombinationof“Bird”and“Background”. Specifically,thebirdimages\ncorrespondingtoeachclassconsistofmorethan10differentspeciesofbirds. Ontheotherhand,\neachbackgroundiscomposedoftwocategoriesobtainedfrom(Zhouetal.,2017). Ascanbeseenin\nFigure6,theLandbirdclasspredominantlyhasimageswithLandbackground,whilethemajorityof\nimagesintheWaterbirdclasshaveWaterbackground. Therefore,thespuriousfeature,background,\nmayindeedformastrongspuriouscorrelationwitheachclass.\nWefollowthesettingofpreviousresearch,(Sagawaetal.,2019;Yaoetal.,2022),forthevalidation\nandtestprocessesaswell. Thebestmodelisselectedbasedonthehighestworst-groupaccuracyon\nthevalidationset. Unlikethetrainingset,thevalidationandtestsetsaredesignedtohaveanequal\nnumberofimagesforeachgroupwithineachclass. Whenreportingtheaverageaccuracyonthe\ntestdatasetusingthebestmodel,wefirstcomputethegroupaccuracyforeachgroupinthetestset.\nThen,wecalculatetheweightedaverageoftheseaccuraciesusingthegroupdistributionfromthe\ntrainingset. Thisapproachisadoptedtomitigatetheuncertaintyinestimatinggroupaccuracies,\nasthenumberofimagesbelongingtotheminoritygroupintheWaterbirddatasetissignificantly\nsmallercomparedtootherdatasets(Sagawaetal.,2019).\nCelebA\nCelebAdatasetby(Liuetal.,2015)isacollectionoffacialimagesofcelebritiesfromaroundthe\nworld. Itincludesattributevaluesassociatedwitheachindividual,suchashaircolorandgender. In\nordertoevaluatetheeffectsofsubpopulationshifts,Sagawaetal.(2019)reformulatedtheCelebA\n19\nCMNIST Waterbirds CelebA CivilComments\nAvg Worst Avg Worst Avg Worst Avg Worst\nERM‡ 27.8±1.9% 0.0±0.0% 97.0±0.2% 63.7±1.9% 94.9±0.2% 47.8±3.7% 92.2±0.1% 56.0±3.6%\nASAM 40.5±0.8% 34.1±1.2% 97.4±0.0% 72.4±0.4% 93.7±0.8% 46.5±10.3% 92.3±0.1% 58.9±1.7%\nIRM‡ 72.1±1.2% 70.3±0.8% 87.5±0.7% 75.6±3.1% 94.0±0.4% 77.8±3.9% 88.8±0.7% 66.3±2.1%\nIB-IRM‡ 72.2±1.3% 70.7±1.2% 88.5±0.6% 76.5±1.2% 93.6±0.3% 85.0±1.8% 89.1±0.3% 65.3±1.5%\nV-REx‡ 71.7±1.2% 70.2±0.9% 88.0±1.0% 73.6±0.2% 92.2±0.1% 86.7±1.0% 90.2±0.3% 64.9±1.2%\nCORAL‡ 71.8±1.7% 69.5±0.9% 90.3±1.1% 79.8±1.8% 93.8±0.3% 76.9±3.6% 88.7±0.5% 65.6±1.3%\nGDRO‡ 72.3±1.2% 68.6±0.8% 91.8±0.3% 90.6±1.1% 92.1±0.4% 87.2±1.6% 89.9±0.5% 70.0±2.0%\nDomainMix‡ 51.4±1.3% 48.0±1.3% 76.4±0.3% 53.0±1.3% 93.4±0.1% 65.6±1.7% 90.9±0.4% 63.6±2.5%\nFish‡ 46.9±1.4% 35.6±1.7% 85.6±0.4% 64.0±0.3% 93.1±0.3% 61.2±2.5% 89.8±0.4% 71.1±0.4%\nLISA‡ 74.0±0.1% 73.3±0.2% 91.8±0.3% 89.2±0.6% 92.4±0.4% 89.3±1.1% 89.2±0.9% 72.6±0.1%\nPDE‡‡ –% –% 92.4±0.8% 90.3±0.3% 92.0±0.6% 91.0±0.4% 86.3±1.7% 71.5±0.5%\nASGDRO 74.8±0.1% 74.2±0.0% 92.3±0.1% 91.4±0.1% 92.1±0.4% 91.0±0.5% 90.2±0.2% 71.8±0.4%\nTable5: SubpopulationShift.‡denotestheperformancereportedfrom(Yaoetal.,2022),and‡‡denotesthe\nperformancereportedfrom(Dengetal.,2024).Avg.denotesaverageaccuracy,andWorstdenotesworstgroup\naccuracy\ndatasettoalignwiththetaskofpredictingwhetherthehaircolorisblondornot. Inthiscase,the\nspuriousfeatureisgender,andthus,thedatasetiscomposedoffourgroupsbasedonthecombinations\nof hair color and gender. It can be observed from Figure 6 that images belonging to Class 0,\ncorrespondingtodarkhairrather,areplentifulregardlessofgender. However,forimagesinClass\n1,whichrepresentblondhair,themajorityofthemaredistributedintheFemalegroup. Therefore,\ngendercanactasaspuriousfeature,andthegoalofthistaskistoobtainamodelthatfocusessolely\non the invariant feature, hair color, rather than the face which may capture the characteristics of\ngender-relatedfeatures.\nThe best model is selected based on the best worst-group accuracy on the validation set. In this\ncase,thevalidationsetandtestsethavethesamedistributionofimagespergroupasthetrainingset.\nTherefore,theaveragetestaccuracyreflectsthisdistributionaccordingly.\nCivilComments\nTheCivilCommentsdataset,(Borkanetal.,2019),isadatasetthatgatherscommentsfromonline\nplatforms and is used for the task of classifying whether a given comment is toxic or not. We\nconducttheexperimentontheCivilCommentsdataset,whichhasbeenreformulatedby(Kohetal.,\n2021). Each comment is labeled to indicate whether it mentions the presence of any word of\nthe8demographicidentities;Black,White,Christian,Muslim,otherreligions,MaleandFemale.\nTherefore,theCivilCommentsdatasetconsistsof16groups,formedbythecombinationoftoxiclabels\nandthepresenceorabsenceofthe8demographicidentitiesineachcomment. Eachdemographic\nidentitycanpotentiallyactasaspuriousfeature. Topreventthis,thegoalofthetaskistotrainthe\nmodeltofocussolelyontheinvariantfeatureoftoxiclabelsandnotrelyondemographicidentities\naspredictivefactors.\nHowever,inreality,unlikeotherdatasets,eachcommentintheCivilCommentsdatasetcanmention\nmorethanonedemographicidentity. Consideringallpossiblecombinationsofdemographicidentities\nforeachcommentandtrainingthemodelonallthesecombinationswouldbeinefficient. Therefore,\nwefollowthelearningapproachproposedby(Kohetal.,2021). Concretely,weonlyconsiderfour\ngroupsbasedonwhetherthecommentmentionstoxicityandwhetheritmentionsthedemographic\nidentityofbeing“Black”,withoutconsideringotherdemographicidentities.Wetrainthemodelusing\nthesefourgroups. However,duringthevalidationandtest,weevaluatethemodel’sperformance\nindividually forall 16groupsand recordthe lowest accuracyamong thegroupaccuracies asthe\nworst-groupaccuracy. TheBestmodelisselectedbasedonthisworst-groupaccuracy.\nExperimentalDetails\nThesearchrangeofthehyperparameterρ,whichdeterminestherangeforexploringtheflatregion,is\nfixedto{0.05,0.2,0.5,0.8,1.0,1.2,1.5}foralldatasets. Weevaluatethemodelacrossthreerandom\nseedsandreporttheaverageperformance. Wesetrobuststepsizeγ,inAlgorithm1ofthemainpaper,\n{0.1,0.01}. Inaddition,weusethesamerangeforadjusted-groupcoefficientC,{0,1,2,3,4,5}\n(Section3.3in(Sagawaetal.,2019)fordetails). InCMNIST,Waterbirds,andCelebAdatasets,we\n20\nutilizeResNet50(Heetal.,2016)models. ThesamehyperparameterrangesareappliedtoASAM\nandASGDRO,andtheotherperformancesforotherbaselinesarereportedperformancesfrom(Liu\netal.,2021;Yaoetal.,2022;Hanetal.,2022). Allexperimentsinthispaperwereconductedusing\nNVIDIARTXA6000with49140MiBofGPUmemoryandGeForceRTX3090with24.00GiBof\nGPUmemory.\nInCMNIST,wehavethesamehyperparametersearchrangeas(Yaoetal.,2022)bydefault: batch\nsize16,learningrate10−3,L –regularization10−4withSGDover300epochs. ForWaterbirds,we\n2\nperform the grid search over the batch size, {16,64}, the learning rate, {10−3,10−4,10−5}, and\nL –regularization,{10−4,10−1,1}.WetrainourmodelwithSGDover300epochs.Wealsoconduct\n2\ngridsearchoverthebatchsize,{16,128},thelearningrate,{10−4,10−5},andL –regularization,\n2\n{10−4,10−2,1}forCelebA,trainingwithSGDover50epochs. Wereferenced(Yaoetal.,2022;\nLiuetal.,2021)forthisrangeofhyperparametersearch. ForCivilComments,weuseDistilBERT\n(Sanhetal.,2019)model. Wefollowthehyperparametersearchrangeprovidedin(Kohetal.,2021).\nForoptimizer,weuseAdamW(LoshchilovandHutter,2017)with10−2forL –regularization. We\n2\nfindtheoptimallearningrateamong{10−6,2×10−6,10−5,2×10−5}. Wetrainupto5epochs\nwithbatchsize16. Thegradientclippingisappliedonlyduringthesecondstep,whichistheactual\nupdatestep,intheSAM-basedalgorithm(Foretetal.,2020).\nA.8 ErrorbarsforWildsBenchmark\nFigure7: StandardDeviationsforWildsBenchmarkDatasets.\nWedemonstratethedifferencesbetweenGDROandASGDROinvariousdistributionshiftscenarios\nthatcouldoccurintherealworld. WildsbenchmarkKohetal.(2021)consistsofdatasetscollected\nfrom the real world. Camelyon17 and RxRx1 are datasets where domain shift is predominant.\nAmazonandFMoWaredatasetswherebothsubpopulationshiftanddomainshiftaresimultaneously\npredominant. Figure7showstheresultsofASGDROandGDROonWildsBenchmark,MetaShift\ndataset,andMulti-NLI(Williamsetal.,2017). ASGDROshowssuperiorperformancesconsistently\ncomparedwithGDRO.Itimpliesthatidentifyingcommonflatminimaacrossenvironmentsenhances\ntherobustnessofmodels.\n21\nA.9 ExperimentalDetailsandErrorbarsforDomainbedwithDPLCLIP\nExperimentalDetailsforDomainBedExperiment\nUsingDomainBedframework(GulrajaniandLopez-Paz,2020),weevaluatedomaingeneralization\nalgorithmsbyrandomlysamplinghyperparametercombinationswithinpredefinedhyperparameter\nsearchrangesforeachalgorithm. Thegoalofdomaingeneralizationistotrainmodelsthatperform\nrobustlyonunseendomains. Consequently,thechoiceofthebestmodelisheavilyinfluencedby\nwhether the validation set used for model selection is sampled from the test domain or the train\ndomains. Toaccountforthis,weprovideresultsforboththetraining-domainvalidationset,which\ndoesnotutilizeinformationfromthetestdomain,andthetest-domainvalidationset,wheremodel\nselectionisperformedusinginformationfromthetestdomain. Thefollowingsubsectionspresentthe\nresultsforeachdataset,consideringbothmodelselectionmethods.\nBycombiningASGDROwiththeexistingsuccessfuldomaingeneralizationapproach,DPLCLIP\n(Zhang et al., 2021)3, we demonstrate the versatility of ASGDRO, as it can easily be integrated\nwith other algorithms. Moreover, our results show that ASGDRO not only improves perfor-\nmance in the context of subpopulation shift but also achieves performance gains in the pres-\nence of domain shift. For experimental details, we set the range of the robust step size γ as\nlambda r: 10**r.uniform(-4, -2)withγ =0.001bydefaultandtheneighborhoodsizeρas\nlambda r: r.choice([0.05, 0.5, 1.0, 5.0]). TheothersettingsarethesameasDPLCLIP\n(Zhang et al., 2021). Following common convention, we conducted 20 hyperparameter searches\nandreportedtheaveragesforthreerandomseeds. Weevaluatedourmodelonthefourdatasetsas\nin the original DPLCLIP paper: VLCS (Fang et al., 2013), PACS (Li et al., 2017), OfficeHome\n(Venkateswaraetal.,2017), andTerraIncognita(Beeryetal.,2018)andDomainNet(Pengetal.,\n2019).\nModelselection: training-domainvalidationset\nVLCS\nAlgorithm C L S V Avg\nDPLCLIP 99.1±0.5 61.1±1.5 72.6±2.6 83.1±2.5 79.0\nDPLCLIPGDRO 99.9±0.0 61.3±2.5 74.4±1.1 83.4±2.6 79.7\nDPLCLIPASGDRO 100.0±0.0 62.7±0.4 74.5±1.4 85.7±0.8 80.7\nPACS\nAlgorithm A C P S Avg\nDPLCLIP 97.6±0.2 98.3±0.3 99.9±0.0 90.5±0.5 96.6\nDPLCLIPGDRO 97.0±0.7 98.2±0.1 99.8±0.1 88.6±1.4 95.9\nDPLCLIPASGDRO 97.7±0.1 98.7±0.1 99.8±0.0 91.0±0.5 96.8\nOfficeHome\nAlgorithm A C P R Avg\nDPLCLIP 80.6±0.8 69.2±0.2 90.1±0.2 91.1±0.0 82.7\nDPLCLIPGDRO 82.3±0.2 70.9±0.1 90.0±0.4 91.1±0.1 83.6\nDPLCLIPASGDRO 82.1±0.4 71.3±0.8 90.3±0.6 91.2±0.3 83.7\nTerraIncognita\nAlgorithm L100 L38 L43 L46 Avg\nDPLCLIP 47.1±1.4 50.1±1.2 41.6±1.9 42.7±0.7 45.4\nDPLCLIPGDRO 49.1±0.9 48.7±2.6 46.3±2.6 39.8±1.4 46.0\nDPLCLIPASGDRO 52.8±0.9 51.5±2.1 49.2±1.2 42.1±0.9 48.9\n3https://github.com/shogi880/DPLCLIP\n22\nDomainNet\nAlgorithm clip info paint quick real sketch Avg\nDPLCLIP 70.9±0.3 51.9±0.3 66.6±0.3 14.6±0.5 84.3±0.2 66.6±0.1 59.1\nDPLCLIPGDRO 71.8±0.4 51.3±0.4 67.0±0.3 15.3±0.2 84.4±0.1 65.0±0.9 59.1\nDPLCLIPASGDRO 71.5±0.5 52.2±0.4 67.5±0.6 16.4±0.2 84.7±0.1 66.5±0.2 59.8\nAverages\nAlgorithm VLCS PACS OfficeHome TerraIncognita DomainNet Avg\nDPLCLIP 79.0±0.7 96.6±0.1 82.7±0.2 45.4±1.0 59.1±0.1 72.6\nDPLCLIPGDRO 79.7±1.3 95.9±0.4 83.6±0.1 46.0±1.0 59.1±0.2 72.9\nDPLCLIPASGDRO 80.7±0.3 96.8±0.2 83.7±0.5 48.9±0.3 59.8±0.2 74.0\nModelselection: test-domainvalidationset(Oracle)\nVLCS\nAlgorithm C L S V Avg\nDPLCLIP 99.8±0.1 69.7±0.6 72.4±1.0 86.2±0.5 82.0\nDPLCLIPGDRO 99.9±0.0 64.9±1.1 79.1±0.5 86.5±0.2 82.6\nDPLCLIPASGDRO 99.8±0.1 67.4±0.9 78.1±0.5 86.9±0.1 83.1\nPACS\nAlgorithm A C P S Avg\nDPLCLIP 97.6±0.1 98.7±0.3 99.8±0.1 91.2±0.3 96.8\nDPLCLIPGDRO 97.4±0.3 98.9±0.2 99.8±0.1 91.9±0.3 97.0\nDPLCLIPASGDRO 97.7±0.2 99.1±0.0 99.9±0.0 91.7±0.3 97.1\nOfficeHome\nAlgorithm A C P R Avg\nDPLCLIP 81.7±0.2 70.9±0.1 90.3±0.3 90.7±0.0 83.4\nDPLCLIPGDRO 81.3±0.8 70.6±0.3 90.5±0.1 90.9±0.3 83.3\nDPLCLIPASGDRO 83.2±0.4 71.7±0.2 91.9±0.1 91.3±0.1 84.5\nTerraIncognita\nAlgorithm L100 L38 L43 L46 Avg\nDPLCLIP 55.9±2.3 58.5±0.3 48.2±0.5 40.9±3.0 50.9\nDPLCLIPGDRO 57.9±1.0 55.3±1.5 49.6±2.0 41.8±1.4 51.2\nDPLCLIPASGDRO 56.2±0.8 54.1±0.3 50.7±0.7 42.1±0.5 50.8\nDomainNet\nAlgorithm clip info paint quick real sketch Avg\nDPLCLIP 72.0±0.5 52.1±0.3 67.3±0.2 16.6±0.2 84.4±0.2 66.8±0.1 59.9\nDPLCLIPGDRO 72.0±0.2 51.7±0.1 67.2±0.4 16.7±0.2 84.5±0.0 66.3±0.1 59.7\nDPLCLIPASGDRO 71.5±0.5 52.8±0.3 68.1±0.3 16.5±0.2 84.9±0.0 67.0±0.1 60.2\n23\nAverages\nAlgorithm VLCS PACS OfficeHome TerraIncognita DomainNet Avg\nDPLCLIP 82.0±0.3 96.8±0.1 83.4±0.1 50.9±0.6 59.9±0.2 74.6\nDPLCLIPGDRO 82.6±0.2 97.0±0.2 83.3±0.2 51.2±1.0 59.7±0.0 74.8\nDPLCLIPASGDRO 83.1±0.2 97.1±0.1 84.5±0.1 50.8±0.3 60.2±0.1 75.1\nA.10 Grad-CAMAnalysis\nInthissection,wepresentadditionalGrad-CAM(Selvarajuetal.,2017)resultsontheWaterbirdsand\nCelebAdatasets. InFigure8and9,thered-colored-namefeaturesrepresentinvariantfeaturesinthe\nrespectivetask,whilethegreen-colored-namefeaturesrepresentspuriousfeatures. IntheGrad-CAM\nimages,thepixelsthateachmodelfocusesontopredicttheground-truthlabelarehighlightedcloser\ntotheredcolorintheimage.\nERM(Vapnik,1999)andASAM(Kwonetal.,2021)areregularization-freealgorithmsthatdonot\nspecificallyencouragemodelstofocusoninvariantfeatures,andthisisreflectedintheGrad-CAM\nresults. Specifically, when observing Group 0 and Group 3 of Waterbirds, which can strongly\nformthecorrelationbetweenclassandspurious,aswellasGroup0,1,and2ofCelebA,inmost\ncases, the results show a strong focus on both spurious and invariant features simultaneously or\nsolelyonspuriousfeatures. Forsomeimages,particularlybetweenCelebAdataset’sGroup0and\n1 where there are no minority groups within a class, there is some degree of focus on invariant\nfeatures. However,theseimagesstillcontainasignificantamountofunnecessarypixelssuchasthe\nbackground. Conversely, inminoritygroupssuchasGroup1and2inWaterbirdsorGroup3in\nCelebA,thereisapredominantfocusoninvariantfeaturestopredicttheground-truthlabel. However,\nthisfocusislimitedtoonlyasubsetoftheoverallinvariantfeaturesandstillincludesomespurious\nfeatures.\nInalgorithmsspecificallydesignedtolearninvariantfeatureslikeGDRO(Sagawaetal.,2019),LISA\n(Yaoetal.,2022),andASGDRO(Ours),theGrad-CAMresultsexhibitdifferentpatternscomparedto\nERMandASAM.Inthemostofresultsforthethreealgorithms,themodelsdemonstrateareasonable\nfocusoninvariantfeatures. ComparedwithERMandASAM,therearesignificantreductionsin\ntheextenttowhichtheyfocusonspuriousfeatures. However,GDROandLISAstillconcentrate\nonlyonapartofinvariantfeatures. Additionally,insomecases,theymayexhibitagreaterfocuson\nspuriousfeaturesthanonthesubsetofinvariantfeatures. Itisalsofrequentlyobservedthattheystill\nheavilyincludespuriousfeaturesorsolelyfocusonspuriousfeatureswhendealingwithmajority\ngroupssuchasGroup1and3inWaterbirdsorGroup0,1,and2inCelebA.Asintheresultsof\nGroup1,and2inWaterbirdsorGroup3inCelebA,weobservethatthemodels’lowabilitytofully\nconcentrateoninvariantfeaturesisaffectedbytheperformanceofmodelsthatstillexhibitafocuson\nspuriousfeatures. Thisobservationhighlightstheimpactofthemodels’performanceontheirability\ntocompletelyfocusoninvariantfeatures.\nIncontrasttootherbaselines,ASGDROdemonstratesastrongerfocusoninvariantfeatures. Asa\nresult,Grad-CAManalysisshowsthatASGDROhasrelativelylargerregionsoffocusoninvariant\nfeaturescomparedtootherbaselines. Simultaneously,itsuccessfullyeliminatesspuriousfeatures\nwhileaccuratelypredictingtheground-truthlabel.Therefore,theseresultsdemonstratethatASGDRO\nhasahighercapacityforcapturingsufficientlydiverseinvariantfeatures,andthischaracteristicis\nreflectedinitsperformance. Thatis,ASGDROpromotesthatthemodelperformsSIL.\n24\nFigure8: Grad-CAMresultsontheWaterbirdsDataset. Thewordshighlightedinredrepresent\ninvariantfeatures: LandbirdandWaterbird. Onthecontrary,thewordshighlightedingreenrepresent\nspuriousfeatures:LandandWaterbackground.IntheTrainingSet,Group1andGroup2areminority\ngroupswithsignificantlyfewerdatasamplescomparedtoothergroups.\n25\nFigure9: Grad-CAMresultsontheCelebADataset. Thefeatureshighlightedinredrepresent\ninvariantwords: DarkHairandBlondHair. Onthecontrary,thewordshighlightedingreenrepresent\nspuriousfeatures:FemaleandMale.IntheTrainingSet,Group3isaminoritygroupwithsignificantly\nfewerdatasamplescomparedtoothergroups.\n26\nA.11 HessianAnalysisforWaterbirdsDataset\nTheLargestEigenvalue TheSecondLargestEigenvalue\nMethod Majority Minority Total Majority Minority Total\nERM 990 4894 2265 166 511 709\nASAM 972 5475 2624 178 524 647\nGDRO 131 447 353 118 346 129\nASGDRO 107 342 279 98 274 105\nTable6: HessianAnalysisonWaterbirds. ASGDROfindsthecommonflatminimaforbothmajorityand\nminoritygroups.\nERMandASAMhavesignificantlysharperminimafortheminoritygroupcomparedtoGDROand\nASGDROduetothespuriouscorrelation,althoughASAMisdesignedtofindflatminima. Compared\nto GDRO and other baselines, ASGDRO achieves the lowest eigenvalue in the first and second\nmaximumeigenvaluesforeverygroup.\n27",
    "pdf_filename": "Sufficient_Invariant_Learning_for_Distribution_Shift.pdf"
}