{
    "title": "TS-ACL A Time Series Analytic Continual Learning Framework for Privacy-Preserving and Class-Incremen",
    "context": "Class-incremental pattern recognition in time series is a significant problem, which aims to learn from continually arriving streaming data examples with incremental classes. A primary challenge in this problem is catastrophic forget- ting, where the incorporation of new data samples causes the models to forget previously learned information. While the replay-based methods achieve promising results by stor- ing historical data to address catastrophic forgetting, they come with the invasion of data privacy. On the other hand, the exemplar-free methods preserve privacy but suffer from significantly decreased accuracy. To address these chal- lenges, we proposed TS-ACL, a novel Time Series Analytic Continual Learning framework for privacy-preserving and class-incremental pattern recognition. Identifying gradi- ent descent as the root of catastrophic forgetting, TS-ACL transforms each update of the model into a gradient-free analytical learning process with a closed-form solution. By leveraging a pre-trained frozen encoder for embedding ex- traction, TS-ACL only needs to recursively update an ana- lytic classifier in a lightweight manner. This way, TS-ACL simultaneously achieves non-forgetting, privacy preserva- tion, and lightweight consumption, making it widely suit- able for various applications, particularly in edge comput- ing scenarios. Extensive experiments on five benchmark datasets confirm the superior and robust performance of TS-ACL compared to existing advanced methods. Code is available at https://github.com/asdasdczxczq/TS-ACL. With significant research attention in recent years, pattern recognition in time series plays a critical role in various *The first three authors contributed equally to this work. †Corresponding authors: tangentheng@gmail.com, hpzhuang@scut.edu.cn. Task 1 Task 2 Task 3 Class 1 Class 2 Class 3 Class 4 Class 5 Class 6 Embedding & Label Memory Matrix Classifier Weights Recursive Update Initial Calculation Continual Data Streams Analytical Classifier Initial Calculation Encoder Network & Randomly-initialized Hidden Layer Classes 1, 2 Classes 1, 2, 3, 4 Classes 1, 2, 3, 4, 5, 6 Recursive Update Recursive Update Recursive Update Recursive Regression-Based Learning Figure 1. Overview of class-incremental pattern recognition in time series, in which our TS-ACL can simultaneously achieve non- forgetting, privacy preservation, and lightweight consumption. applications, such as healthcare diagnostics [32], industrial production [36], and urban computing [13]. Deep Learning (DL) approaches have gained widespread popularity due to their superior performance, and they typically rely on of- fline, static datasets that assume data to be independently and identically distributed (i.i.d.) [10]. Unfortunately, as shown in Figure 1, real-world scenar- ios often involve continual data streams from sensors, re- sulting in an ever-growing volume of time series data with incremental classes, where the i.i.d. assumption no longer holds [23]. Additionally, as new data with previously un- seen classes may emerge over time, the dataset becomes increasingly complex [23]. This dynamic environment re- quires the DL models to continually adapt and learn from new data samples. Unfortunately, this process is often hin- dered by the well-known problem of “catastrophic forget- ting”, in which the models forget previously learned knowl- edge when exposed to new data samples [5, 33]. To mitigate catastrophic forgetting, numerous Class- arXiv:2410.15954v2  [cs.LG]  18 Nov 2024",
    "body": "TS-ACL: A Time Series Analytic Continual Learning Framework for\nPrivacy-Preserving and Class-Incremental Pattern Recognition\nKejia Fan1*, Jiaxu Li1*, Songning Lai2*, Linpu Lv3, Jianheng Tang4†,\nAnfeng Liu1, Houbing Herbert Song5, Yutao Yue2, Huiping Zhuang6†\n1Central South University, 2The Hong Kong University of Science and Technology (Guangzhou),\n3Zhengzhou University, 4Peking University, 5University of Maryland, Baltimore County,\n6South China University of Technology\nAbstract\nClass-incremental pattern recognition in time series is a\nsignificant problem, which aims to learn from continually\narriving streaming data examples with incremental classes.\nA primary challenge in this problem is catastrophic forget-\nting, where the incorporation of new data samples causes\nthe models to forget previously learned information. While\nthe replay-based methods achieve promising results by stor-\ning historical data to address catastrophic forgetting, they\ncome with the invasion of data privacy. On the other hand,\nthe exemplar-free methods preserve privacy but suffer from\nsignificantly decreased accuracy. To address these chal-\nlenges, we proposed TS-ACL, a novel Time Series Analytic\nContinual Learning framework for privacy-preserving and\nclass-incremental pattern recognition.\nIdentifying gradi-\nent descent as the root of catastrophic forgetting, TS-ACL\ntransforms each update of the model into a gradient-free\nanalytical learning process with a closed-form solution. By\nleveraging a pre-trained frozen encoder for embedding ex-\ntraction, TS-ACL only needs to recursively update an ana-\nlytic classifier in a lightweight manner. This way, TS-ACL\nsimultaneously achieves non-forgetting, privacy preserva-\ntion, and lightweight consumption, making it widely suit-\nable for various applications, particularly in edge comput-\ning scenarios.\nExtensive experiments on five benchmark\ndatasets confirm the superior and robust performance of\nTS-ACL compared to existing advanced methods. Code is\navailable at https://github.com/asdasdczxczq/TS-ACL.\n1. Introduction\nWith significant research attention in recent years, pattern\nrecognition in time series plays a critical role in various\n*The first three authors contributed equally to this work.\n†Corresponding\nauthors:\ntangentheng@gmail.com,\nhpzhuang@scut.edu.cn.\nTask 1\nTask 2\nTask 3\nClass 1\nClass 2\nClass 3\nClass 4\nClass 5\nClass 6\nEmbedding \n& Label\nMemory \nMatrix\nClassifier \nWeights\nRecursive Update\nInitial Calculation\nContinual \nData Streams\nAnalytical Classifier\nInitial Calculation\nEncoder Network & Randomly-initialized Hidden Layer\nClasses 1, 2\nClasses 1, 2, 3, 4\nClasses 1, 2, 3, 4, 5, 6\nRecursive Update\nRecursive Update\nRecursive Update\nRecursive Regression-Based Learning\nFigure 1. Overview of class-incremental pattern recognition in\ntime series, in which our TS-ACL can simultaneously achieve non-\nforgetting, privacy preservation, and lightweight consumption.\napplications, such as healthcare diagnostics [32], industrial\nproduction [36], and urban computing [13]. Deep Learning\n(DL) approaches have gained widespread popularity due to\ntheir superior performance, and they typically rely on of-\nfline, static datasets that assume data to be independently\nand identically distributed (i.i.d.) [10].\nUnfortunately, as shown in Figure 1, real-world scenar-\nios often involve continual data streams from sensors, re-\nsulting in an ever-growing volume of time series data with\nincremental classes, where the i.i.d. assumption no longer\nholds [23]. Additionally, as new data with previously un-\nseen classes may emerge over time, the dataset becomes\nincreasingly complex [23]. This dynamic environment re-\nquires the DL models to continually adapt and learn from\nnew data samples. Unfortunately, this process is often hin-\ndered by the well-known problem of “catastrophic forget-\nting”, in which the models forget previously learned knowl-\nedge when exposed to new data samples [5, 33].\nTo mitigate catastrophic forgetting, numerous Class-\narXiv:2410.15954v2  [cs.LG]  18 Nov 2024\n\nIncremental Learning (CIL) methods have been proposed.\nThese methods can be broadly categorized into two types:\nthe replay-based methods [11, 25, 26] and the exemplar-\nfree methods [1, 12, 23]. The replay-based methods store\nhistorical samples or use generative models to produce syn-\nthetic samples, called the exemplar, to enable the model to\nretain previously learned knowledge [25, 26, 28]. While\nthese replay-based methods can achieve impressive perfor-\nmance, they inherently raise significant concerns about pri-\nvacy due to the need to store historical data [39]. More im-\nportantly, in many practical applications, particularly edge\ncomputing scenarios, the limited storage and computational\nresources usually render such methods impractical [17]. On\nthe other hand, the exemplar-free methods typically aim\nto protect learned knowledge by modifying the loss func-\ntion or optimizing the learning process without exemplar\n[12, 33]. Although these methods avoid privacy issues, they\nstill suffer from significantly decreased accuracy with sub-\noptimal performance, limiting their practical utility [40].\nFurthermore, a more fundamental and general limita-\ntion within current CIL methods, whether the replay-based\nor the exemplar-free, lies in their reliance on gradient-\nbased backpropagation techniques. Since gradients com-\nputed from new data can conflict with or even contradict\nthose derived from historical data, current CIL methods fail\nto fully address the problem of catastrophic forgetting. As\na result, updates based on conflicting gradients inevitably\nlead to the erasure of previously acquired knowledge [5].\nTo address these challenges, we introduce a novel Time\nSeries Analytic Continual Learning framework, named TS-\nACL, for privacy-preserving and class-incremental pattern\nrecognition, as shown in Figure 1. Identifying gradient de-\nscent as the root of catastrophic forgetting, TS-ACL trans-\nforms each update of the model into a gradient-free ana-\nlytical learning process with a closed-form solution, Thus,\nTS-ACL completely removes the need for gradient-based\nupdates and fundamentally solves the issue of catastrophic\nforgetting. By leveraging a pre-trained frozen encoder for\nembedding extraction, TS-ACL only needs to recursively\nupdate an analytic classifier for each task in a lightweight\nmanner. This way, TS-ACL simultaneously achieves non-\nforgetting, privacy preservation, and lightweight consump-\ntion. The key contributions of this work are as follows:\n1. We propose a novel TS-ACL framework for privacy-\npreserving and class-incremental pattern recognition. By\nintroducing analytic learning to replace gradient descent,\nTS-ACL achieves insightful non-forgetting, thereby fun-\ndamentally solving the issue of catastrophic forgetting.\n2. With the least squares method for each update, TS-ACL\nachieves lightweight consumption in both computation\nand storage, making it highly suitable for various appli-\ncations, especially in edge computing scenarios.\n3. Through theoretical analysis, we prove that TS-ACL’s\nperformance on incremental tasks is fully equivalent to\njoint training on all tasks, ensuring its remarkable prop-\nerty of “non-forgetting”.\n4. Comprehensive experiments validate the state-of-the-art\nperformance of TS-ACL. Our TS-ACL, as an exemplar-\nfree method, not only far surpasses other exemplar-free\nbaselines, but also demonstrates amazing competitive-\nness even compared to the replay-based ones.\n2. Related Work\n2.1. Pattern Recognition in Time Series\nPattern recognition in time series aims to assign categorical\nlabels to time series data based on its patterns or charac-\nteristics, playing an increasingly important role in numer-\nous domains [19, 32, 36]. In the early stages of research,\nthe focus was primarily on non-DL methods, which mainly\nincluded distance-based methods [14, 29] and ensembling\nmethods [15, 18]. The distance-based methods rely on var-\nious time series distance measurement techniques, with the\nDynamic Time Warping as a representative example [9], us-\ning 1-nearest neighbor classifiers. The ensembling methods\nintegrate multiple individual 1-nearest neighbor classifiers\nwith different distance metrics to achieve improved perfor-\nmance [15, 19]. These non-DL methods typically incur sig-\nnificant computational overhead and struggle with scalabil-\nity when applied to large-scale datasets [8, 19].\nRecently, DL-based methods have been extensively stud-\nied and have achieved encouraging performance [8, 23, 31].\nThese methods are typically trained on offline, static, and\ni.i.d. datasets, with little consideration given to scenarios\ninvolving continual data perception in real-world environ-\nments [23]. In practice, new time series data samples are\ncontinually collected over time, necessitating incremental\nmodel updates [23, 33]. Unfortunately, classical DL-based\nmethods inevitably face the problem of “catastrophic for-\ngetting”, where the models forget previously learned knowl-\nedge when exposed to new data samples [5, 33].\n2.2. Class-incremental Learning\nTo alleviate the problem of catastrophic forgetting, numer-\nous studies on CIL [11, 12, 33] have emerged and can\nbe broadly categorized into two main types: the replay-\nbased methods [25, 25, 26] and the examplar-free methods\n[1, 12, 23]. The replay-based methods store a subset of his-\ntorical samples, called the exemplar, and replay them during\nincremental training to alleviate the problem of catastrophic\nforgetting [26, 28]. Meanwhile, the examplar-free methods\nattempt to preserve prior knowledge by incorporating addi-\ntional terms into the loss function or by explicitly designing\nand manipulating the optimization process [1, 23].\nIn practice, the replay-based methods typically achieve\nbetter performance thanks to the exemplar, but they inher-\n\nPre-training & Coordinating\nRecursive Regression-Based Learning\nTask 2\nTask 3\nTask t-1\nTask t\nSupport \nUpdate\nSupport \nUpdate\nObtained from \nCoordinating\n(c) Recursive Regression-Based Learning: t-th Task\nData of \nTask t\nRHL\nAnalytic Classifier\nPre-trained Encoder\n(c) Recursive Regression-Based Learning: 2-nd Task\nData of \nTask 2\nRHL\nAnalytic Classifier\nPre-trained Encoder\n(b) Regression-Based Coordinate\nData of \nTask 1\nAnalytic Classifier\nRHL\nPre-trained Encoder\n(a) Gradient Descent-Based Training\nFlatten\nData of \nTask 1\nPre-training Encoder\nSoftmax Classifier\nFigure 2. Overview of the proposed TS-ACL framework. In (a), a pre-trained encoder is first obtained on Task 1 through Gradient Descent-\nBased Training. Next, as shown in (b), a Regression-Based Coordinate module with an RHL is introduced before the classification head\nto enhance the embedding dimension, resulting in Ψ1 from Task 1 data. Finally, in (c), a Recursive Regression-Based Learning process is\napplied to achieve non-forgetting across tasks.\nently raise significant concerns about privacy [39]. In many\npractical applications, particularly edge computing scenar-\nios, the limited storage and computational resources usually\nrender such methods impractical [17]. Meanwhile, although\nthe exemplar-free methods can avoid privacy issues, they\ntypically suffer from significantly suboptimal performance\n[33, 40]. More critically, both these methods depend on\ngradient-based updating techniques, which fundamentally\ndo not solve the problem of catastrophic forgetting [5].\n2.3. Analytic Learning\nAnalytic learning, as a distinct technical pathway from\ngradient-based methods, has attracted considerable research\nattention in recent years [6, 30]. Specifically, the main idea\nof analytic learning is to directly compute the parameters of\nneural networks using mathematical methods such as least\nsquares, thereby eliminating the need for gradients [30, 39].\nFor example, the radial basis network [20] use least-squares\nestimation to train parameters after the kernel transforma-\ntion in the first layer.\nThe multilayer analytic learning\n[34] transforms nonlinear network learning into segments,\nand efficiently solves this problem using least-squares tech-\nniques in a single-epoch training process. Recently, several\nanalytic learning-based CIL methods [38–40] have been\nproposed, which utilize least-squares techniques to directly\ncompute the closed-form solutions for neural network in\nCIL scenario. We highlight that the gradient-free nature\nof analytic learning offers a foundational solution to catas-\ntrophic forgetting. Building on this, we propose TS-ACL,\nan analytic learning approach specifically crafted for class-\nincremental pattern recognition in time series, ensuring ab-\nsolute knowledge retention without forgetting.\n3. Method\n3.1. Preliminaries\nLet the model be continually trained for T tasks, where the\ntraining data for each task comes from different classes. Let\nDtrain\nt\n∼{Utrain\nt\n, Vtrain\nt\n} and Dtest\nt\n∼{Utest\nt , Vtest\nt } denote\nthe training and testing datasets at task t (t = 1, . . . , T).\nSpecifically, Ut ∈RNt×c×l (e.g., Nt time series with a\nshape of c×l) and Vtrain\nt\n∈RNt×dyt (with task t containing\ndyt classes) represent the stacked input and label tensors.\nIn the CIL scenario, the classes learned across different\n\ntasks are independent and non-overlapping [24, 33]. For\neach task t, the network learns from Dtrain\nt\nand Dtest\nt , where\nthe classes differ from those in previous tasks. In task t, the\ndata labels Vtrain\nt\nand Vtest\nt\nbelong to a task-specific class\nset Ct, disjoint from other sets Ct′ (for t′ ̸= t). Thus, data\nfrom different tasks exhibit distinct class distributions. For\neach task t, the objective is to use previous parameters Θt−1\nand Dtrain\nt\nto update the model to Θt, ensuring both stabil-\nity (retaining past knowledge) and plasticity (learning new\nknowledge).\nOur proposed TS-ACL, comprising Gradient Descent-\nBased Training, Regression-Based Coordinate, and Recur-\nsive Regression-Based Learning as shown in Figure\n2,\nachieves both stability and plasticity. TS-ACL ensures ab-\nsolute retention of past knowledge, guaranteeing incremen-\ntal learning is equivalent to joint training, where all tasks are\nlearned simultaneously. This provides rigorous assurance of\nhigh accuracy across past and current tasks.\n3.2. Gradient Descent-Based Training\nFirst, we use gradient descent to train a regular classification\nnetwork on the basic training task, which usually includes\nmultiple training epochs. Theoretically, it can be composed\nof any commonly used network structure with a softmax\nclassifier. After training, the output V of the network can\nbe expressed as:\nV = fsoftmax(fflat(fencoder(U, Θencoder))Φ),\n(1)\nwhere Θencoder and Φ are the parameters representing the\nencoder network and the classifier, fencoder(U, Θencoder) rep-\nresents the encoder output, fsoftmax and fflat are the softmax\nfunction and the flattening operator. After training, we save\nand freeze the encoder weights and treat the encoder as a\nembedding exactor.\n3.3. Regression-Based Coordinate\nUpon completing Gradient Descent-Based Training, we\nproceed with the Regression-Based Coordinate step us-\ning the analytical classifier replacing the softmax classifier.\nThis process is carried out by the analytic classifier as the\ncore mechanism, and briefly consists of three sub-steps:\nThe first sub-step involves extracting the embedding ma-\ntrix (denoted as U(encoder)\n1\n) by passing the input tensor Utrain\n1\nthrough the trained encoder network, followed by a flatten-\ning operation, i.e.,\nU(encoder)\n1\n= fflat(fencoder(Utrain\n1\n, Θencoder)),\n(2)\nwhere U(encoder)\n1\n∈RN1×dencoder. Next, instead of directly\nmapping the embeddings to the classification output via a\nsingle classifier layer, we add a Randomly-initialized Hid-\nden Layer (RHL). Specifically, the embedding U(encoder)\n1\nis\nexpanded into U(E)\n1\nas follows:\nU(E)\n1\n= fact(U(encoder)\n1\nΦE),\n(3)\nwhere U(E)\n1\n∈RN1×dE, with dE representing the embedding\nexpansion size (generally dencoder < dE). Here, fact refers to\nthe activation function, and ΦE is the matrix used to expand\nthe encoder-extracted embeddings. The matrix ΦE is initial-\nized by sampling its elements from a normal distribution.\nWhy need RHL? The necessity of the RHL layer lies in\nthe need for a larger parameter space for analytical learn-\ning. By projecting the original embeddings into a higher-\ndimensional space, it enables the model to fully realize its\npotential. This projection allows the model to express more\ncomplex relationships within the data, which are difficult to\ncapture in a lower-dimensional space.\nFinally, the expanded embeddings U(E)\n1\nare mapped to\nthe label matrix Vtrain\n1\nusing a linear regression procedure,\nby solving the following optimization problem:\narg min\nΦ1\n\r\r\rVtrain\n1\n−U(E)\n1 Φ1\n\r\r\r\n2\nF + γ ∥Φ1∥2\nF ,\n(4)\nwhere γ is a regularization parameter. The solution to this\nproblem is given by:\nbΦ1 =\n\u0010\nU(E)⊤\n1\nU(E)\n1\n+ γI\n\u0011−1\nU(E)⊤\n1\nVtrain\n1\n,\n(5)\nwhere the notation ⊤is the transposed operation and the\nnotation −1 represent the inverse operation.\n3.4. Recursive Regression-Based Learning\nAfter completing the Regression-Based Coordinate, we\nproceed with the CIL process using Recursive Regression-\nBased Learning in a recursive and analytical manner. To\ndemonstrate this, without loss of generality, assume we are\ngiven Dtrain\n1\n, . . . , Dtrain\nt−1, and let\nU(E)\n1:t−1 ∈RN1:t−1×dE,\nV1:t−1 ∈RN1:t−1×Pt−1\ni=1 dyi (6)\nrepresent the concatenated activation and label tensors, re-\nspectively, from task 1 to t −1, i.e.,\nU(E)\n1:t−1 =\n\n\nU(E)\n1...\nU(E)\nt−1\n\n,\nV1:t−1 =\n\n\nVtrain\n1\n. . .\n0\n...\n...\n...\n0\n. . .\nVtrain\nt−1\n\n.\n(7)\nHere, N1:t−1 indicates the total number of data samples\nfrom task 1 to t −1. The sparse structure of V1:t−1 arises\nbecause of the mutually exclusive classes across tasks. The\nlearning problem can then be formulated as follows:\narg min\nΦt−1\n\r\r\rV1:t−1 −U(E)\n1:t−1Φt−1\n\r\r\r\n2\nF + γ ∥Φt−1∥2\nF ,\n(8)\n\naccording to 5, at task t −1, we have:\nˆΦt−1 =\n\u0010\nU(E)⊤\n1:t−1U1:t−1 + γI\n\u0011−1\nU(E)⊤\n1:t−1V1:t−1,\n(9)\nwhere ˆΦt−1 ∈RdE×Pt−1\ni=1 dyi , with the column size expand-\ning as t increases. Let\nΨt−1 =\n\u0010\nU(E)⊤\n1:t−1U1:t−1 + γI\n\u0011−1\n(10)\ndenote the aggregated temporal inverse correlation matrix,\nwhich captures the correlation information from both cur-\nrent and past samples. Based on this, out goal is to com-\npute ˆΦt using only ˆΦt−1, Ψt−1, and the current task’s data\nUtrain\nt\n, without involving historical samples such as U1:t−1.\nThe process is formulated in the following theorem.\nTheorem 1. The Φ weights, recursively obtained by\nˆΦt =\nh\nˆΦt−1 −ΨtU(E)\nt\nU(E)⊤\nt\nˆΦt−1\nΨtU(E)⊤\nt\nVtrain\nt\ni\n(11)\nare equivalent to those obtained from 9 for task t. The ma-\ntrix Ψt can also be recursively updated by\nΨt = Ψt−1 −Ψt−1U(E)\nt\n\u0010\nI + U(E)⊤\nt\nΨt−1U(E)\nt\n\u0011−1\nU(E)⊤\nt\nΨt−1.\n(12)\nProof. See the supplementary materials.\nAs stated in Theorem 1, the proposed TS-ACL frame-\nwork provides a recursive update for the Φ weights without\nlosing any historical information. First, the base model is\ntrained on the initial dataset (e.g. to compute ˆΦ1), and the\nCIL process continues using the recursive formulation to\nobtain ˆΦt for t > 1.\nPrivacy-Preserving and High-Performance.\nTS-ACL\nsafeguards privacy in two significant ways: first, by avoid-\ning the use of historical data samples, and second, by en-\nsuring impossible to use reverse engineering to obtain his-\ntorical data samples from the Ψ matrix. As an exemplar-\nfree method, TS-ACL differs from other exemplar-free ap-\nproaches, which often exhibit considerable performance\ngaps compared to replay-based methods. In contrast, TS-\nACL performs on par with or even surpasses replay-based\nmethods, as shown in the following experiments. TS-ACL\nnot only upholds privacy but also achieves high perfor-\nmance, and this dual benefit is particularly valuable in to-\nday’s era of increased data privacy concerns.\n4. Experiment\n4.1. Setup\nDatasets. Following [24] We continued previous research\nby selecting five balanced time series datasets, each con-\ntaining samples with the same length and variables, which\nis shown in Table 1. See supplementary for more details.\nTable 1. Time series dataset details which is sourced from [24].\nDataset\nShape (C × L)\nTrain Size\nTest Size\n# Classes\n# Exp Tasks\nUCI-HAR [7]\n9 × 128\n7352\n2947\n6\n3\nUWave [16]\n3 × 315\n896\n3582\n8\n4\nDSA [2]\n45 × 125\n6840\n2280\n18\n6\nGRABMyo [22]\n28 × 128\n36120\n12040\n16\n5\nWISDM [35]\n3 × 200\n18184\n6062\n18\n6\nCIL Setting. We trained and evaluated on five datasets,\neach divided into n tasks with two distinct classes per task,\nand randomly shuffled class order before partitioning. To\nevaluate the robustness, we used different random seeds to\nconduct five independent experiments and report the aver-\nage results with standard deviation.\nTask Stream. Following [24], we divided tasks into a val-\nidation stream and an experiment stream. In the UCI-HAR\n[7] and UWave datasets [16], both the validation and exper-\nimental streams consist of 3 and 4 tasks, respectively. And\nother datasets include three tasks in the validation stream,\nwith the experimental stream covering the remaining tasks,\nas summarized in Table 1. We initially perform a grid search\non the validation stream to optimize hyperparameters, then\nuse the selected parameters to conduct experiments on the\nexperiment stream.\nComparison Baselines. Following [24], for examplar-free\nmethods, we selected 3 classical baselines: LwF [12], MAS\n[1], and DT2W [23]. For replay-based methods, we chose\n7 up-to-date baselines: GR [28], ER [26], ICARL [25], DER\n[3], ASER [27], CLPOS [10] and FASTICARL [11].\nEvaluation Metrics. To quantitatively assess the perfor-\nmance of CIL methods, we utilize two widely adopted\nmetrics: Average accuracy and Forgetting.\nAverage ac-\ncuracy is calculated by averaging the accuracy of all pre-\nviously encountered tasks, including the current task af-\nter learning the current task t.\nIt is defined as At =\n1\nt\nPt\ni=1 At,i, where At,i represents the accuracy on task\ni after learning task t.\nForgetting is measured to cap-\nture how much performance degrades on previous tasks\nafter learning a new task t.\nIt is computed as Ft\n=\n1\nt−1\nPt−1\ni=1\n\u0000maxj∈{1,...,t−1} Aj,i −At,i\n\u0001\n.\nAt task t, the\nforgetting on task i is defined as the maximum difference\nbetween the highest accuracy previously achieved on task i\nand the accuracy on task i after learning task t.\nSelected encoder. We adopted the same 1D-CNN encoder\nnetwork structure as in [24]. The network comprises four\nconvolutional blocks, each containing a 1D convolutional\nlayer, a normalization layer, a max-pooling layer, and a\ndropout layer. For the GR generator, we chose TimeVAE\n[4], where both the encoder and decoder follow a four-layer\nConv1D and ConvTranspose1D structure.\nImplementation Details. Following [24], all experiments\nwere conducted five times, each with distinct class orders\nand random seeds. For each experimental run, hyperpa-\n\n(a) Batch Norm\n(b) Layer Norm\nFigure 3. The accuracy of all methods on five datasets changes as the tasks progress.\nTable 2. For the best-performing method in the exemplar-free approach, we use bold formatting, and for the best performance across all\nmethods, we underline the result. The average value and standard deviation of each metric are reported based on the results of 5 runs.\n(a) Batch Norm\nDataset\nMetric\nNaive\nOffline\nGR\nER\nDER\nHerding\nASER\nCLOPS\nFastICARL\nLwF\nMAS\nDT2W\nTS-ACL\nUCI-HAR\nAT ↑\n32.00±2.90\n94.94±2.17\n66.66±14.05\n65.46±13.78\n74.41±7.57\n69.58±19.66\n92.36±2.78\n72.87±11.91\n79.69±7.77\n35.96±11.33\n52.34±15.92\n53.23±16.33\n88.41±1.52\nFT ↓\n98.71±1.38\nN.A\n34.51±17.06\n31.77±34.66\n8.37±13.59\n32.01±33.80\n8.58±5.00\n22.83±26.62\n20.53±18.20\n80.78±17.46\n63.40±22.78\n60.17±25.07\n7.23±3.08\nUWave\nAT ↑\n25.36±0.59\n96.61±1.05\n76.20±6.25\n70.28±8.21\n70.88±7.71\n78.47±2.87\n82.74±2.01\n71.04±1.66\n67.77±8.62\n44.67±11.64\n53.80±10.36\n64.44±5.92\n91.89±1.72\nFT ↓\n91.35±18.7\nN.A\n27.43±7.24\n36.91±11.10\n35.01±10.41\n26.00±3.26\n21.21±2.71\n35.47±1.32\n39.33±11.16\n51.46±27.35\n40.56±21.64\n32.66±5.79\n4.05±0.78\nDSA\nAT ↑\n17.04±0.82\n99.65±0.46\n31.51±5.74\n79.75±18.11\n59.19±17.56\n82.42±10.04\n97.26±1.59\n74.10±15.43\n67.28±15.06\n24.82±8.02\n31.82±6.36\n19.56±3.82\n98.33±1.34\nFT ↓\n99.55±0.99\nN.A\n72.92±9.96\n22.57±19.31\n28.55±30.89\n19.45±12.30\n3.18±1.84\n22.83±22.31\n35.47±19.87\n80.72±13.51\n74.85±13.94\n90.27±10.13\n0.94±1.33\nGRABMyo\nAT ↑\n19.44±0.38\n93.63±1.22\n20.59±4.67\n47.03±6.82\n31.38±4.65\n47.14±7.43\n56.50±4.24\n43.75±4.03\n40.55±4.97\n19.22±0.49\n19.04±1.45\n21.34±4.75\n57.06±3.80\nFT ↓\n95.23±2.43\nN.A\n64.87±8.64\n32.37±6.28\n62.89±4.34\n33.24±15.75\n48.36±5.16\n29.66±7.33\n48.50±6.76\n95.0±1.5\n67.09±41.90\n58.33±10.10\n15.54±1.42\nWISDM\nAT ↑\n14.89±1.39\n85.31±1.30\n42.42±4.99\n48.36±16.79\n32.95±5.89\n32.72±6.23\n14.89±2.55\n10.74±2.79\n17.29±6.76\n15.44±1.75\n41.69±7.54\n28.99±9.19\n85.35±2.81\nFT ↓\n90.07±8.6\nN.A\n28.66±11.88\n57.49±19.89\n34.02±13.47\n40.17±9.42\n69.35±34.11\n59.45±18.45\n51.10±30.63\n73.35±10.23\n26.62±12.48\n53.80±28.75\n6.29±0.58\n(b) Layer Norm\nDataset\nMetric\nNaive\nOffline\nGR\nER\nDER\nHerding\nASER\nCLOPS\nFastICARL\nLwF\nMAS\nDT2W\nTS-ACL\nUCI-HAR\nAT ↑\n36.44±10.35\n92.31±0.82\n80.04±7.69\n89.53±2.41\n90.75±1.90\n89.95±2.51\n89.82±1.43\n89.64±1.50\n85.43±3.74\n47.40±14.04\n59.53±15.90\n80.15±6.11\n87.75±2.24\nFT ↓\n92.25±13.25\nN.A\n25.75±15.74\n9.53±6.54\n8.58±5.97\n9.96±7.24\n10.20±5.57\n8.98±4.82\n17.51±8.25\n74.04±22.17\n52.47±27.00\n16.55±9.33\n6.29±2.87\nUWave\nAT ↑\n24.85±0.12\n96.39±0.22\n85.77±3.76\n78.89±4.27\n77.74±6.51\n85.42±1.89\n77.89±5.26\n73.79±3.39\n79.01±0.98\n29.09±5.34\n40.74±9.29\n55.09±9.27\n92.12±1.75\nFT ↓\n98.15±1.4\nN.A\n15.37±4.38\n25.87±5.68\n27.31±7.67\n16.51±1.82\n27.14±6.33\n32.12±2.95\n25.60±2.24\n73.47±25.06\n65.01±15.14\n40.28±18.04\n3.72±1.31\nDSA\nAT ↑\n19.81±4.12\n99.53±0.76\n69.50±7.26\n97.24±1.43\n98.01±0.69\n97.75±1.36\n95.97±6.32\n89.65±5.51\n91.39±6.16\n17.01±4.33\n35.75±6.35\n19.06±4.11\n98.12±1.75\nFT ↓\n96.23±4.95\nN.A\n36.43±8.64\n3.25±1.78\n2.28±0.82\n2.62±1.59\n4.73±7.55\n12.30±6.64\n10.28±7.44\n87.93±15.21\n66.17±14.57\n96.85±4.81\n0.98±1.08\nGRABMyo\nAT ↑\n19.46±0.34\n93.83±0.87\n20.56±1.37\n61.16±3.30\n63.78±3.96\n60.07±3.69\n57.90±4.79\n52.05±5.11\n52.84±3.49\n19.42±0.32\n18.15±1.57\n20.09±7.28\n56.44±3.22\nFT ↓\n94.17±3.34\nN.A\n94.44±3.13\n40.87±3.59\n37.01±3.68\n42.46±4.02\n45.49±5.48\n52.22±5.54\n52.46±4.19\n93.25±6.64\n88.34±7.52\n22.57±9.52\n15.75±1.69\nWISDM\nAT ↑\n14.60±2.25\n88.60±1.94\n24.33±8.12\n66.88±7.36\n67.14±5.37\n69.67±3.98\n51.48±15.85\n44.00±8.11\n44.87±3.67\n13.83±3.51\n19.25±6.66\n15.59±7.92\n83.53±2.87\nFT ↓\n88.47±10.58\nN.A\n85.78±10.00\n33.80±7.90\n33.38±6.67\n30.04±3.98\n53.04±18.68\n60.74±10.12\n52.79±6.37\n83.53±17.51\n74.65±12.06\n37.50±14.48\n7.13±1.69\nrameter tuning was performed through two validation it-\nerations on validation tasks. All models were trained us-\ning the Adam optimizer with a learning rate of 0.001 and a\nbatch size of 64, with 100 epochs for the first task and sub-\nsequent tasks and the learning rate scheduler is treated as\na hyperparameter and is tuned during the validation tasks.\nHowever, TS-ACL diverges from this by bypassing gradi-\nent descent after the initial task, opting instead for an an-\nalytical update method that requires only a single epoch.\nDuring incremental learning, TS-ACL employs a frozen en-\ncoder with an appended RHL. Data preprocessing includes\na non-trainable input normalization layer applied before the\nencoder to standardize inputs on a per-sample basis. Spe-\ncific normalization methods were used for each dataset:\nlayer normalization for UCI-HAR, DSA, and GRABMyo;\ninstance normalization for UWave; and no normalization\nfor WISDM. The encoder’s architecture integrates two nor-\nmalization types: Batch Normalization (BN) and Layer\nNormalization (LN). For replay-based methods, we set the\nmemory storage pool to 5%. For TS-ACL, we configure γ\n\n(a) Batch Norm\n(b) Layer Norm\nFigure 4. The performance of each method is averaged across the five datasets.\n(a) Batch Norm\n(b) Layer Norm\nFigure 5. The accuracy of TS-ACL on five datasets varies with different γ values as tasks progress.\nto {0.01, 0.1, 1, 10, 100} and set the embedding expansion\nsize to 8192. We select the best γ value to present in the\nexperimental results. See supplementary for more details.\n4.2. Main Results\nPlasticity. As demonstrated in Table 2 and Figure 3, TS-\nACL achieves remarkable accuracy across five datasets.\nUnder the exemplar-free approach, TS-ACL consistently\noutperforms the best existing methods.\nIn the BN set-\nting, TS-ACL surpasses the second-best method by 35.18%,\n27.45%, 66.51%, 35.72%, and 43.66% across the five\ndatasets. In the LN setting, TS-ACL surpasses the second-\nbest method by 7.6%, 37.03%, 62.37%, 36.41%, and\n64.28%, respectively. In fact, on most datasets, TS-ACL\noutperforms all methods, including the second-best by\n9.15%, 1.07%, 0.56%, and 37.00% in the BN setting for\nUWave, DSA, GRABMyo, and WSIDM, respectively, and\nby 6.70%, 0.11%, 13.86% in the LN setting for UWave,\nDSA, and WSIDM, respectively. These results underscore\nTS-ACL’s superior performance in terms of plasticity.\nStability.\nAs shown in Table 2, our TS-ACL achieves\nthe lowest forgetting rate across all datasets, under both\nBN and LN conditions.\nSpecifically, under BN, it sur-\npasses the strongest baseline model by 1.35%, 17.16%,\n2.24%, 16.83%, and 20.33% on the five datasets, respec-\ntively. Likewise, under LN, it outperforms the strongest\nbaseline by 2.29%, 12.79%, 1.3%, 6.82%, and 22.91%, re-\nspectively. This superior performance can be attributed to\nthe fact that TS-ACL is mathematically proven to achieve\nresults comparable to joint learning in the context of incre-\nmental learning, which has the remarkable property of non-\nforgetting, as discussed in Section 3.4.\n4.3. Robustness Analyses\nClass Order Robustness. For each dataset and method,\nwe conducted experiments five times, each with a different\nrandom seed, resulting in varying class orders across each\nrun. We then averaged the final task accuracy and standard\ndeviation of each method across five datasets, as illustrated\nin Figure 4 according to Table 2. The results indicate that\nthe TS-ACL achieves the highest average accuracy and the\nlowest standard deviation, Here, we primarily focus on the\nmagnitude of the standard deviation, as it directly reflects\nthe robustness of each method under different Class Order\n\nTable 3. AT and FT on all benchmark datasets with various values of the regularization term γ.\n(a) Batch Norm\nγ\nUCI-HAR (%)\nUWave (%)\nDSA (%)\nGRABMyo (%)\nWISDM (%)\nAT ↑\nFT ↓\nAT ↑\nFT ↓\nAT ↑\nFT ↓\nAT ↑\nFT ↓\nAT ↑\nFT ↓\n0.01\n85.83 ± 2.71\n9.57 ± 2.09\n91.67 ± 1.66\n4.11 ± 1.11\n93.67 ± 8.37\n4.37 ± 8.92\n50.11 ± 4.02\n10.45 ± 1.97\n79.57 ± 9.63\n10.24 ± 7.44\n0.1\n83.53 ± 1.64\n10.16 ± 3.34\n91.89 ± 1.72\n4.05 ± 0.78\n96.50 ± 5.61\n2.75 ± 5.66\n48.43 ± 2.93\n15.76 ± 2.04\n84.86 ± 2.11\n6.82 ± 1.51\n1\n87.15 ± 1.37\n7.70 ± 3.32\n91.68 ± 2.24\n3.96 ± 1.34\n98.26 ± 2.25\n1.33 ± 2.33\n54.91 ± 3.37\n15.43 ± 2.32\n85.35 ± 2.81\n6.29 ± 0.58\n10\n88.20 ± 1.40\n7.29 ± 3.03\n91.35 ± 2.10\n3.98 ± 1.26\n97.89 ± 1.47\n1.38 ± 1.38\n57.06 ± 3.80\n15.54 ± 1.42\n80.89 ± 3.39\n8.45 ± 0.92\n100\n88.41 ± 1.52\n7.23 ± 3.08\n88.21 ± 2.61\n4.45 ± 1.18\n96.54 ± 1.57\n2.28 ± 1.64\n53.14 ± 4.96\n15.70 ± 1.67\n68.92 ± 6.53\n12.81 ± 2.40\n(b) Layer Norm\nγ\nUCI-HAR (%)\nUWave (%)\nDSA (%)\nGRABMyo (%)\nWISDM (%)\nAT ↑\nFT ↓\nAT ↑\nFT ↓\nAT ↑\nFT ↓\nAT ↑\nFT ↓\nAT ↑\nFT ↓\n0.01\n83.38 ± 1.42\n9.50 ± 3.06\n92.05 ± 0.72\n3.45 ± 0.94\n96.49 ± 2.47\n1.88 ± 1.85\n50.82 ± 3.45\n9.44 ± 2.33\n73.75 ± 6.85\n12.05 ± 6.86\n0.1\n84.43 ± 2.77\n8.33 ± 2.80\n91.80 ± 1.02\n3.89 ± 1.75\n98.12 ± 1.07\n0.98 ± 1.08\n49.27 ± 5.34\n12.24 ± 3.37\n82.46 ± 4.76\n8.22 ± 4.17\n1\n87.47 ± 2.56\n7.03 ± 3.39\n92.12 ± 1.75\n3.72 ± 1.31\n98.03 ± 1.14\n1.02 ± 1.16\n52.52 ± 3.37\n14.79 ± 2.48\n83.53 ± 2.87\n7.13 ± 1.69\n10\n87.75 ± 2.24\n6.29 ± 2.87\n91.00 ± 2.52\n4.31 ± 1.78\n96.44 ± 1.82\n1.90 ± 1.48\n56.44 ± 3.22\n15.54 ± 1.42\n78.45 ± 4.16\n9.17 ± 1.58\n100\n87.38 ± 2.08\n7.12 ± 3.63\n88.12 ± 2.65\n4.88 ± 1.37\n93.89 ± 1.81\n3.18 ± 1.62\n55.17 ± 2.73\n15.55 ± 1.32\n69.06 ± 5.28\n12.23 ± 1.39\nsettings. Among all the comparison methods, our approach\nmaintains the smallest standard deviation, indicating that\nour method exhibits the strongest class order robustness.\nWhy TS-ACL has class order robustness? As demon-\nstrated in 3.4, the effectiveness of our TS-ACL in incre-\nmental continual learning is equivalent to that of joint\nlearning. This weight-invariant property ensures that our\nmethod achieves nearly consistent performance across dif-\nferent class order settings, with only minor standard devi-\nation due to the slight differences in encoding parameters\nlearned during the first task. This property guarantees ro-\nbust performance across various class order settings.\nNorm Robustness.\nAs shown in Figure 3 and Table 2,\nfor TS-ACL, the performance difference between BN and\nLN is negligible. In replay-based methods, due to BN’s re-\nliance on batch statistics, is particularly vulnerable to the\nimpact of imbalanced distributions between new and old\nsamples in incremental learning. This imbalance not only\ncontributes to the forgetting of previously learned knowl-\nedge but also hinders the acquisition of new information\n[21]. In contrast, LN through instance normalization, effec-\ntively alleviates these issues, demonstrating superior perfor-\nmance. However, ASER exhibits a little performance gap\nbetween BN and LN, as hypothesized in [24]. ASER se-\nlects a balanced and representative set of memory samples,\nwhich helps maintain unbiased statistics under BN.\nWhy TS-ACL has norm robustness? Our TS-ACL pos-\nsesses norm robustness because it is trained using analyti-\ncal learning, with the encoder functioning as a frozen em-\nbedding extractor that does not require gradient updates or\nbackpropagation during continual learning. Therefore, BN\nand LN fundamentally only modify embedding extraction,\nand the extracted embeddings can be considered to contain\nsimilar representations. As a result, they do not significantly\nimpact the classification. In comparison to exemplar-free\nmethods, the effectiveness of BN or LN depends on the spe-\ncific approach used. The attribute of norm robustness within\nour TS-ACL provides a unique advantage, as it has minimal\nimpact on results, allowing us to avoid repeated experiments\nto find the optimal normalization way.\nRegularization Term Analysis. As shown in Figure 5 and\nTable 3, overall, γ is relatively stable across a broad range\nof values (e.g. {0.01, 0.1, 1, 10, 100}). However, on the\nsecond task of the GRABMyo dataset, a significant perfor-\nmance drop is observed when γ is set to 0.01 and 0.1, which\nis due to overfitting. Additionally, on the Wisdom dataset,\nwhen γ is set to 100, performance declines notably due to\nunderfitting, as the simplicity of the linear regression be-\ncomes apparent [37].\n5. Conclusion\nIn this study, we propose TS-ACL, an analytical continual\nlearning framework for time series class-incremental pat-\ntern recognition that fundamentally addresses the problem\nof catastrophic forgetting through gradient-free recursive\nregression learning, while also achieving privacy preser-\nvation and efficiency.\nExperimental results demonstrate\nthat TS-ACL significantly outperforms existing methods\nacross five benchmark datasets, achieving both stability and\nplasticity, and simultaneously possessing the non-forgetting\nproperty and weight-invariant property, making it a robust\nsolution, particularly in resource-constrained scenarios such\nas edge computing.\n\nReferences\n[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,\nMarcus Rohrbach, and Tinne Tuytelaars.\nMemory aware\nsynapses: Learning what (not) to forget. In Proceedings of\nthe European conference on computer vision (ECCV), pages\n139–154, 2018. 2, 5\n[2] Kerem Altun and Billur Barshan. Human activity recogni-\ntion using inertial/magnetic sensor units. In Human Behavior\nUnderstanding: First International Workshop, HBU 2010,\nIstanbul, Turkey, August 22, 2010. Proceedings 1, pages 38–\n51. Springer, 2010. 5\n[3] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide\nAbati, and Simone Calderara.\nDark experience for gen-\neral continual learning: a strong, simple baseline. Advances\nin neural information processing systems, 33:15920–15930,\n2020. 5\n[4] Abhyuday Desai, Cynthia Freeman, Zuhui Wang, and Ian\nBeaver. Timevae: A variational auto-encoder for multivari-\nate time series generation. arXiv preprint arXiv:2111.08095,\n2021. 5\n[5] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville,\nand Yoshua Bengio.\nAn empirical investigation of catas-\ntrophic forgetting in gradient-based neural networks. arXiv\npreprint arXiv:1312.6211, 2013. 1, 2, 3\n[6] Ping Guo, Michael R Lyu, and NE Mastorakis. Pseudoin-\nverse learning algorithm for feedforward neural networks.\nAdvances in Neural Networks and Applications, 1(321-326),\n2001. 3\n[7] Dewi Pramudi Ismi, Shireen Panchoo, and Murinto Murinto.\nK-means clustering based filter feature selection on high di-\nmensional data. International Journal of Advances in Intel-\nligent Informatics, 2(1):38–45, 2016. 5\n[8] Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon,\nCesare Alippi, Geoffrey I. Webb, Irwin King, and Shirui Pan.\nA survey on graph neural networks for time series: Forecast-\ning, classification, imputation, and anomaly detection. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n2024. 2\n[9] Rohit J Kate. Using dynamic time warping distances as fea-\ntures for improved time series classification. Data mining\nand knowledge discovery, 30:283–312, 2016. 2\n[10] Dani Kiyasseh, Tingting Zhu, and David Clifton. A clini-\ncal deep learning framework for continually learning from\ncardiac signals across diseases, time, modalities, and institu-\ntions. Nature Communications, 12(1):4221, 2021. 1, 5\n[11] Young D Kwon, Jagmohan Chauhan, and Cecilia Mas-\ncolo. Fasticarl: Fast incremental classifier and representation\nlearning with efficient budget allocation in audio sensing ap-\nplications. arXiv preprint arXiv:2106.07268, 2021. 2, 5\n[12] Zhizhong Li and Derek Hoiem. Learning without forgetting.\nIEEE transactions on pattern analysis and machine intelli-\ngence, 40(12):2935–2947, 2017. 2, 5\n[13] Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming\nJin, Dongjin Song, Shirui Pan, and Qingsong Wen. Founda-\ntion models for time series analysis: A tutorial and survey.\nIn Proceedings of the 30th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, pages 6555–6565,\n2024. 1\n[14] Jason Lines and Anthony Bagnall. Time series classification\nwith ensembles of elastic distance measures. Data Mining\nand Knowledge Discovery, 29:565–592, 2015. 2\n[15] Jason Lines, Sarah Taylor, and Anthony Bagnall. Time series\nclassification with hive-cote: The hierarchical vote collective\nof transformation-based ensembles. ACM Transactions on\nKnowledge Discovery from Data (TKDD), 12(5):1–35, 2018.\n2\n[16] Jiayang Liu, Lin Zhong, Jehan Wickramasuriya, and Venu\nVasudevan. uwave: Accelerometer-based personalized ges-\nture recognition and its applications. Pervasive and Mobile\nComputing, 5(6):657–675, 2009. 5\n[17] Siliang Lu, Jingfeng Lu, Kang An, Xiaoxian Wang, and\nQingbo He. Edge computing on iot for machine signal pro-\ncessing and fault diagnosis: A review.\nIEEE Internet of\nThings Journal, 10(13):11093–11116, 2023. 2, 3\n[18] Matthew Middlehurst, James Large, Michael Flynn, Jason\nLines, Aaron Bostrom, and Anthony Bagnall. Hive-cote 2.0:\na new meta ensemble for time series classification. Machine\nLearning, 110(11):3211–3243, 2021. 2\n[19] Navid Mohammadi Foumani, Lynn Miller, Chang Wei Tan,\nGeoffrey I. Webb, Germain Forestier, and Mahsa Salehi.\nDeep learning for time series classification and extrinsic re-\ngression: A current survey. ACM Comput. Surv., 56(9), 2024.\n2\n[20] Jooyoung Park and Irwin W Sandberg. Universal approxi-\nmation using radial-basis-function networks. Neural compu-\ntation, 3(2):246–257, 1991. 3\n[21] Quang Pham, Chenghao Liu, and Steven Hoi. Continual nor-\nmalization: Rethinking batch normalization for online con-\ntinual learning. arXiv preprint arXiv:2203.16102, 2022. 8\n[22] Ashirbad Pradhan, Jiayuan He, and Ning Jiang. Multi-day\ndataset of forearm and wrist electromyogram for hand ges-\nture recognition and biometrics. Scientific data, 9(1):733,\n2022. 5\n[23] Zhongzheng Qiao, Minghui Hu, Xudong Jiang, Ponnuthu-\nrai Nagaratnam Suganthan, and Ramasamy Savitha. Class-\nincremental learning on multivariate time series via shape-\naligned temporal distillation. In ICASSP 2023-2023 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 1–5. IEEE, 2023. 1, 2, 5\n[24] Zhongzheng Qiao, Quang Pham, Zhen Cao, Hoang H Le,\nPonnuthurai N Suganthan, Xudong Jiang, and Ramasamy\nSavitha. Class-incremental learning for time series: Bench-\nmark and evaluation.\narXiv preprint arXiv:2402.12035,\n2024. 4, 5, 8\n[25] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\nSperl, and Christoph H Lampert. icarl: Incremental classifier\nand representation learning. In Proceedings of the IEEE con-\nference on Computer Vision and Pattern Recognition, pages\n2001–2010, 2017. 2, 5\n[26] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lil-\nlicrap, and Gregory Wayne. Experience replay for continual\nlearning.\nAdvances in neural information processing sys-\ntems, 32, 2019. 2, 5\n\n[27] Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott San-\nner, Hyunwoo Kim, and Jongseong Jang.\nOnline class-\nincremental continual learning with adversarial shapley\nvalue. In Proceedings of the AAAI Conference on Artificial\nIntelligence, pages 9630–9638, 2021. 5\n[28] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.\nContinual learning with deep generative replay. Advances in\nneural information processing systems, 30, 2017. 2, 5\n[29] Chang Wei Tan, Franc¸ois Petitjean, and Geoffrey I Webb.\nFastee: fast ensembles of elastic distances for time series\nclassification. Data Mining and Knowledge Discovery, 34\n(1):231–272, 2020. 2\n[30] Jonathan Tapson and Andr´e van Schaik. Learning the pseu-\ndoinverse solution to network weights. Neural Networks, 45:\n94–100, 2013. 3\n[31] Jingyuan Wang, Chen Yang, Xiaohan Jiang, and Junjie Wu.\nWhen: A wavelet-dtw hybrid attention network for hetero-\ngeneous time series analysis.\nIn Proceedings of the 29th\nACM SIGKDD Conference on Knowledge Discovery and\nData Mining, pages 2361–2373, 2023. 2\n[32] Lin Wang, Zheng Yin, Mamta Puppala, Chika F. Ezeana,\nKelvin K. Wong, Tiancheng He, Deepa B. Gotur, and\nStephen T. C. Wong. A time-series feature-based recursive\nclassification model to optimize treatment strategies for im-\nproving outcomes and resource allocations of covid-19 pa-\ntients. IEEE Journal of Biomedical and Health Informatics,\n26(7):3323–3329, 2022. 1, 2\n[33] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A\ncomprehensive survey of continual learning: Theory, method\nand application. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 46(8):5362–5383, 2024. 1, 2, 3, 4\n[34] Xi-Zhao Wang, Tianlun Zhang, and Ran Wang. Noniterative\ndeep learning: Incorporating restricted boltzmann machine\ninto multilayer random weight neural networks. IEEE Trans-\nactions on Systems, Man, and Cybernetics: Systems, 49(7):\n1299–1308, 2019. 3\n[35] Gary M Weiss. Wisdm smartphone and smartwatch activity\nand biometrics dataset. UCI Machine Learning Repository:\nWISDM Smartphone and Smartwatch Activity and Biomet-\nrics Dataset Data Set, 7:133190–133202, 2019. 5\n[36] Wenbiao Yang, Kewen Xia, Zhaocheng Wang, Shurui Fan,\nand Ling Li. Self-attention causal dilated convolutional neu-\nral network for multivariate time series classification and its\napplication.\nEngineering Applications of Artificial Intelli-\ngence, 122:106151, 2023. 1, 2\n[37] Huiping Zhuang, Zhiping Lin, and Kar-Ann Toh. Correlation\nprojection for analytic learning of a classification network.\nNeural Processing Letters, 53:3893–3914, 2021. 8\n[38] Huiping Zhuang, Zhenyu Weng, Hongxin Wei, Renchunzi\nXie, Kar-Ann Toh, and Zhiping Lin. ACIL: Analytic class-\nincremental learning with absolute memorization and pri-\nvacy protection. In Advances in Neural Information Process-\ning Systems, pages 11602–11614. Curran Associates, Inc.,\n2022. 3\n[39] Huiping Zhuang, Zhenyu Weng, Run He, Zhiping Lin, and\nZiqian Zeng.\nGkeal: Gaussian kernel embedded analytic\nlearning for few-shot class incremental task. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 7746–7755, 2023. 2, 3\n[40] Huiping Zhuang, Run He, Kai Tong, Ziqian Zeng, Cen Chen,\nand Zhiping Lin. DS-AL: A dual-stream analytic learning for\nexemplar-free class-incremental learning. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, pages 17237–\n17244, 2024. 2, 3\n\nTS-ACL: A Time Series Analytic Continual Learning Framework for\nPrivacy-Preserving and Class-Incremental Pattern Recognition\nSupplementary Material\nA. Proof of Theorem\nIn this section, we provide a mathematical proof for Theorem 1.\nWe prove Theorem 1 using the solution of joint training on t tasks with ridge regression:\nˆΦt =\n\u0010\nU(E)⊤\n1:t\nU(E)\n1:t + γI\n\u0011−1\nU(E)⊤\n1:t\nV1:t.\n(13)\nBy decoupling the t-th task from previous tasks, ˆΦt can be written as:\nˆΦt =\n\n\nh\nU(E)⊤\n1:t−1\nU(E)⊤\nt\ni\n\n\n\u0010\nU(E)\n1:t−1\n\u0011\n\u0010\nU(E)\nt\n\u0011\n\n+ γI\n\n\n−1 h\nU(E)⊤\n1:t−1\nU(E)⊤\nt\ni \u0014V1:t−1\n0\n0\nVtrain\nt\n\u0015\n=\n\u0010\nU(E)⊤\n1:t−1U(E)\n1:t−1 + γI + U(E)⊤\nt\nU(E)\nt\n\u0011−1 h\nU(E)⊤\n1:t−1V1:t−1\nU(E)⊤\nt\nVtrain\nt\ni\n.\n(14)\nWe introduce the memory matrix as in the following definition:\nΨt =\n\u0010\nU(E)⊤\n1:t\nU(E)\n1:t + γI\n\u0011−1\n,\n(15)\nwhich is the matrix inversion term of Eqn 13.\nNoticing that Ψt−1 =\n\u0010\nU(E)⊤\n1:t−1U(E)\n1:t−1 + γI\n\u0011−1\n, by Woodbury matrix identity where (A + UCV)−1 = A−1 −\nA−1U\n\u0000C−1 + VA−1U\n\u0001−1 VA−1 and treating Ψt−1 as A−1, the memory at the t-th step can be further defined as a\nrecursive solution:\nΨt = Ψt−1 −Ψt−1U(E)⊤\nt\n\u0010\nI + U(E)\nt\nΨt−1U(E)⊤\nt\n\u0011−1\nU(E)\nt\nΨt−1.\n(16)\nThus, the parameter ˆΦt is derived as\nˆΦt =\nh\nΨtU(E)⊤\n1:t−1V1:t−1\nΨtU(E)⊤\nt\nVtrain\nt\ni\n.\n(17)\nDenote the left submatrix ΨtU(E)⊤\n1:t−1V1:t−1 as H. By substituting Eqn 16 into 17,\nH = ˆΦt−1 −Ψt−1U(E)⊤\nt\n\u0010\nI + U(E)\nt\nΨt−1U(E)⊤\nt\n\u0011−1\nU(E)\nt\nˆΦt−1.\n(18)\nBased on the identity (I + P)−1 = I −(I + P)−1 P, it is further derived as:\nH = ˆΦt−1 −ΨtU(E)⊤\nt\nU(E)\nt\nˆΦt−1.\n(19)\nThus,\nˆΦt =\nh\nˆΦt−1 −ΨtU(E)⊤\nt\nU(E)\nt\nˆΦt−1\nΨtU(E)⊤\nt\nVtrain\nt\ni\n.\n(20)\nTheorem 1 is thus proved.\nB. More Implementation Details\nThe learning rate scheduler was optimized as a hyperparameter, alongside the use of early stopping to mitigate overfitting\nand patience values were set at 20 for ER- and GR-based methods and at 5 for other approaches. To facilitate early stopping,\na validation set was created by dividing the training data in a 1:9 ratio, with validation loss monitored. Three learning rate\nscheduling strategies were evaluated: Step10, Step15, and OneCycleLR. In Step10 and Step15, the learning rate decreased\nby a factor of 0.1 at the 10th and 15th epochs, respectively. Dropout rates varied across datasets: 0 for UCI-HAR and UWave,\nand 0.3 for DSA, GRABMyo, and WISDM. For additional information, please refer to our code.\n\nC. Dataset Details\nUCI-HAR dataset includes temporal sequences obtained from smartphone inertial sensors during the execution of six daily\nactivities. Data were collected at a frequency of 50Hz from 30 participants of varying ages. Each input sequence consists of\nnine channels, covering a temporal span of 128 timesteps.\nUWave provides over 4000 samples collected from eight individuals performing eight distinct gesture patterns. The data\nconsists of time series captured along three accelerometer axes, with each sample having a dimensionality of 3 and spanning\n315 timesteps.\nDSA gathers motion sensor data from 19 different sports activities performed by eight volunteers. Each segment serves as\na single sample, featuring data across 45 channels and 125 timesteps. For uniformity among classes, 18 activity classes were\nselected for experimentation.\nGRABMyo is a comprehensive surface electromyography (sEMG) dataset designed for hand gesture recognition. It in-\ncludes signals corresponding to 16 gestures performed by 43 participants across three sessions. Recordings last five seconds,\nsampled from 28 channels at 2048 Hz. For experimentation, data from a single session were used and downsampled to 256\nHz. A non-overlapping sliding window of 0.5 seconds (128 timesteps) was applied to segment the signals into samples. The\ndata were then split into training and testing sets in a 3:1 ratio, ensuring both sets include all subjects to mitigate distribution\nshifts across participants.\nWISDM is a human activity recognition dataset that captures sensor data from 18 activities performed by 51 participants.\nUsing the phone accelerometer modality, samples were generated with a non-overlapping sliding window of 200, representing\n10-second time series collected at a 20Hz frequency. Like GRABMyo, the data were split into training and test sets at a 3:1\nratio, ensuring all participants are represented in both splits.\nD. Algorithm\nAlgorithm 1 The Proposed TS-ACL Framework\nRequire: Total number of tasks T; Training time series datasets {Dtrain\nt\n}T\nt=1; Regularization parameter γ; embedding expan-\nsion size dE\nEnsure: Final model parameters Θencoder and ˆΦT\n1: Gradient Descent-Based Training\n2: Train the network on Dtrain\n1\nusing gradient descent\n3: Save and freeze the encoder weights Θencoder\n4: Regression-Based Coordinate\n5: Extract and expand embeddings using Eq.(2), Eq.(3)\n6: Compute initial classifier weights using Eq.(5)\n7: Compute aggregated temporal inverse correlation matrix using Eq.(10)\n8: Recursive Regression-Based Learning\n9: for t = 2 to T do\n10:\nExtract and expand embeddings using Eq.(2), Eq.(3)\n11:\nUpdate aggregated temporal inverse correlation matrix using Eq.(12)\n12:\nUpdate classifier weights using Eq.(11)\n13: end for\n14: Output: Final model parameters Θencoder and ˆΦT",
    "pdf_filename": "TS-ACL_A_Time_Series_Analytic_Continual_Learning_Framework_for_Privacy-Preserving_and_Class-Incremen.pdf"
}