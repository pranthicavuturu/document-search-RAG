{
    "title": "TS-ACL: A Time Series Analytic Continual Learning Framework for",
    "abstract": "Task 1 Task 2 Task 3 Data Streams Class 1 Class 2 Class 3 Class 4 Class 5 Class 6 Class-incremental pattern recognition in time series is a significant problem, which aims to learn from continually arrivingstreamingdataexampleswithincrementalclasses. Encoder Network & Randomly-initialized Hidden Layer Aprimarychallengeinthisproblemiscatastrophicforget- ting, where the incorporation of new data samples causes Recursive Regression-Based Learning themodelstoforgetpreviouslylearnedinformation. While Em &b Le ad bd ein lg Memory Initial Calculation Recursive Update Recursive Update thereplay-basedmethodsachievepromisingresultsbystor- Matrix ing historical data to address catastrophic forgetting, they Initial Calculation Recursive Update Recursive Update comewiththeinvasionofdataprivacy. Ontheotherhand, Classifier Weights theexemplar-freemethodspreserveprivacybutsufferfrom Classes 1, 2 Classes 1, 2, 3, 4 Classes 1, 2, 3, 4, 5, 6 significantly decreased accuracy. To address these chal- Analytical Classifier lenges,weproposedTS-ACL,anovelTimeSeriesAnalytic Figure 1. Overview of class-incremental pattern recognition in ContinualLearningframeworkforprivacy-preservingand timeseries,inwhichourTS-ACLcansimultaneouslyachievenon- class-incremental pattern recognition. Identifying gradi- forgetting,privacypreservation,andlightweightconsumption. ent descent as the root of catastrophic forgetting, TS-ACL transforms each update of the model into a gradient-free analyticallearningprocesswithaclosed-formsolution. By applications,suchashealthcarediagnostics[32],industrial leveragingapre-trainedfrozenencoderforembeddingex- production[36],andurbancomputing[13]. DeepLearning traction, TS-ACLonlyneedstorecursivelyupdateanana- (DL)approacheshavegainedwidespreadpopularitydueto lytic classifier in a lightweight manner. This way, TS-ACL their superior performance, and they typically rely on of- simultaneously achieves non-forgetting, privacy preserva- fline, static datasets that assume data to be independently tion, and lightweight consumption, making it widely suit- andidenticallydistributed(i.i.d.) [10]. ableforvariousapplications,particularlyinedgecomput- ing scenarios. Extensive experiments on five benchmark Unfortunately, as shownin Figure 1, real-world scenar- datasets confirm the superior and robust performance of ios often involve continual data streams from sensors, re- TS-ACL compared to existing advanced methods. Code is sultinginanever-growingvolumeoftimeseriesdatawith availableathttps://github.com/asdasdczxczq/TS-ACL. incremental classes, where the i.i.d. assumption no longer holds [23]. Additionally, as new data with previously un- seen classes may emerge over time, the dataset becomes 1.Introduction increasingly complex [23]. This dynamic environment re- quires the DL models to continually adapt and learn from With significant research attention in recent years, pattern newdatasamples. Unfortunately,thisprocessisoftenhin- recognition in time series plays a critical role in various dered by the well-known problem of “catastrophic forget- ting”,inwhichthemodelsforgetpreviouslylearnedknowl- *Thefirstthreeauthorscontributedequallytothiswork. edgewhenexposedtonewdatasamples[5,33]. †Corresponding authors: tangentheng@gmail.com, hpzhuang@scut.edu.cn. To mitigate catastrophic forgetting, numerous Class- 4202 voN 81 ]GL.sc[ 2v45951.0142:viXra",
    "body": "TS-ACL: A Time Series Analytic Continual Learning Framework for\nPrivacy-Preserving and Class-Incremental Pattern Recognition\nKejiaFan1*,JiaxuLi1*,SongningLai2*,LinpuLv3,JianhengTang4†,\nAnfengLiu1,HoubingHerbertSong5,YutaoYue2,HuipingZhuang6†\n1CentralSouthUniversity,2TheHongKongUniversityofScienceandTechnology(Guangzhou),\n3ZhengzhouUniversity,4PekingUniversity,5UniversityofMaryland,BaltimoreCounty,\n6SouthChinaUniversityofTechnology\nAbstract Continual\nTask 1 Task 2 Task 3 Data Streams\nClass 1 Class 2 Class 3 Class 4 Class 5 Class 6\nClass-incremental pattern recognition in time series is a\nsignificant problem, which aims to learn from continually\narrivingstreamingdataexampleswithincrementalclasses.\nEncoder Network & Randomly-initialized Hidden Layer\nAprimarychallengeinthisproblemiscatastrophicforget-\nting, where the incorporation of new data samples causes Recursive Regression-Based Learning\nthemodelstoforgetpreviouslylearnedinformation. While Em &b Le ad bd ein lg\nMemory Initial Calculation Recursive Update Recursive Update\nthereplay-basedmethodsachievepromisingresultsbystor- Matrix\ning historical data to address catastrophic forgetting, they Initial Calculation Recursive Update Recursive Update\ncomewiththeinvasionofdataprivacy. Ontheotherhand,\nClassifier\nWeights\ntheexemplar-freemethodspreserveprivacybutsufferfrom\nClasses 1, 2 Classes 1, 2, 3, 4 Classes 1, 2, 3, 4, 5, 6\nsignificantly decreased accuracy. To address these chal-\nAnalytical Classifier\nlenges,weproposedTS-ACL,anovelTimeSeriesAnalytic\nFigure 1. Overview of class-incremental pattern recognition in\nContinualLearningframeworkforprivacy-preservingand\ntimeseries,inwhichourTS-ACLcansimultaneouslyachievenon-\nclass-incremental pattern recognition. Identifying gradi-\nforgetting,privacypreservation,andlightweightconsumption.\nent descent as the root of catastrophic forgetting, TS-ACL\ntransforms each update of the model into a gradient-free\nanalyticallearningprocesswithaclosed-formsolution. By\napplications,suchashealthcarediagnostics[32],industrial\nleveragingapre-trainedfrozenencoderforembeddingex-\nproduction[36],andurbancomputing[13]. DeepLearning\ntraction, TS-ACLonlyneedstorecursivelyupdateanana-\n(DL)approacheshavegainedwidespreadpopularitydueto\nlytic classifier in a lightweight manner. This way, TS-ACL\ntheir superior performance, and they typically rely on of-\nsimultaneously achieves non-forgetting, privacy preserva-\nfline, static datasets that assume data to be independently\ntion, and lightweight consumption, making it widely suit-\nandidenticallydistributed(i.i.d.) [10].\nableforvariousapplications,particularlyinedgecomput-\ning scenarios. Extensive experiments on five benchmark Unfortunately, as shownin Figure 1, real-world scenar-\ndatasets confirm the superior and robust performance of ios often involve continual data streams from sensors, re-\nTS-ACL compared to existing advanced methods. Code is sultinginanever-growingvolumeoftimeseriesdatawith\navailableathttps://github.com/asdasdczxczq/TS-ACL. incremental classes, where the i.i.d. assumption no longer\nholds [23]. Additionally, as new data with previously un-\nseen classes may emerge over time, the dataset becomes\n1.Introduction increasingly complex [23]. This dynamic environment re-\nquires the DL models to continually adapt and learn from\nWith significant research attention in recent years, pattern newdatasamples. Unfortunately,thisprocessisoftenhin-\nrecognition in time series plays a critical role in various dered by the well-known problem of “catastrophic forget-\nting”,inwhichthemodelsforgetpreviouslylearnedknowl-\n*Thefirstthreeauthorscontributedequallytothiswork.\nedgewhenexposedtonewdatasamples[5,33].\n†Corresponding authors: tangentheng@gmail.com,\nhpzhuang@scut.edu.cn. To mitigate catastrophic forgetting, numerous Class-\n4202\nvoN\n81\n]GL.sc[\n2v45951.0142:viXra\nIncremental Learning (CIL) methods have been proposed. performance on incremental tasks is fully equivalent to\nThese methods can be broadly categorized into two types: jointtrainingonalltasks,ensuringitsremarkableprop-\nthe replay-based methods [11, 25, 26] and the exemplar- ertyof“non-forgetting”.\nfree methods [1, 12, 23]. The replay-based methods store 4. Comprehensiveexperimentsvalidatethestate-of-the-art\nhistoricalsamplesorusegenerativemodelstoproducesyn- performanceofTS-ACL.OurTS-ACL,asanexemplar-\ntheticsamples,calledtheexemplar,toenablethemodelto free method, not only far surpasses other exemplar-free\nretain previously learned knowledge [25, 26, 28]. While baselines, but also demonstrates amazing competitive-\nthesereplay-basedmethodscanachieveimpressiveperfor- nessevencomparedtothereplay-basedones.\nmance,theyinherentlyraisesignificantconcernsaboutpri-\nvacyduetotheneedtostorehistoricaldata[39]. Moreim- 2.RelatedWork\nportantly, in many practical applications, particularly edge\n2.1.PatternRecognitioninTimeSeries\ncomputingscenarios,thelimitedstorageandcomputational\nresourcesusuallyrendersuchmethodsimpractical[17].On Patternrecognitionintimeseriesaimstoassigncategorical\nthe other hand, the exemplar-free methods typically aim labels to time series data based on its patterns or charac-\nto protect learned knowledge by modifying the loss func- teristics, playing an increasingly important role in numer-\ntion or optimizing the learning process without exemplar ous domains [19, 32, 36]. In the early stages of research,\n[12,33].Althoughthesemethodsavoidprivacyissues,they thefocuswasprimarilyonnon-DLmethods,whichmainly\nstillsufferfromsignificantlydecreasedaccuracywithsub- included distance-based methods [14, 29] and ensembling\noptimalperformance,limitingtheirpracticalutility[40]. methods[15,18]. Thedistance-basedmethodsrelyonvar-\nFurthermore, a more fundamental and general limita- ioustimeseriesdistancemeasurementtechniques,withthe\ntionwithincurrentCILmethods,whetherthereplay-based DynamicTimeWarpingasarepresentativeexample[9],us-\nor the exemplar-free, lies in their reliance on gradient- ing1-nearestneighborclassifiers. Theensemblingmethods\nbased backpropagation techniques. Since gradients com- integrate multiple individual 1-nearest neighbor classifiers\nputed from new data can conflict with or even contradict withdifferentdistancemetricstoachieveimprovedperfor-\nthosederivedfromhistoricaldata,currentCILmethodsfail mance[15,19]. Thesenon-DLmethodstypicallyincursig-\ntofullyaddresstheproblemofcatastrophicforgetting. As nificantcomputationaloverheadandstrugglewithscalabil-\na result, updates based on conflicting gradients inevitably itywhenappliedtolarge-scaledatasets[8,19].\nleadtotheerasureofpreviouslyacquiredknowledge[5]. Recently,DL-basedmethodshavebeenextensivelystud-\nToaddressthesechallenges,weintroduceanovelTime iedandhaveachievedencouragingperformance[8,23,31].\nSeriesAnalyticContinualLearningframework,namedTS- These methods are typically trained on offline, static, and\nACL, for privacy-preserving and class-incremental pattern i.i.d. datasets, with little consideration given to scenarios\nrecognition,asshowninFigure1. Identifyinggradientde- involving continual data perception in real-world environ-\nscentastherootofcatastrophicforgetting, TS-ACLtrans- ments [23]. In practice, new time series data samples are\nforms each update of the model into a gradient-free ana- continually collected over time, necessitating incremental\nlytical learning process with a closed-form solution, Thus, modelupdates[23,33]. Unfortunately,classicalDL-based\nTS-ACL completely removes the need for gradient-based methods inevitably face the problem of “catastrophic for-\nupdatesandfundamentallysolvestheissueofcatastrophic getting”,wherethemodelsforgetpreviouslylearnedknowl-\nforgetting. By leveraging a pre-trained frozen encoder for edgewhenexposedtonewdatasamples[5,33].\nembedding extraction, TS-ACL only needs to recursively\n2.2.Class-incrementalLearning\nupdate an analytic classifier for each task in a lightweight\nmanner. This way, TS-ACL simultaneously achieves non- Toalleviatetheproblemofcatastrophicforgetting, numer-\nforgetting, privacypreservation, andlightweightconsump- ous studies on CIL [11, 12, 33] have emerged and can\ntion. Thekeycontributionsofthisworkareasfollows: be broadly categorized into two main types: the replay-\n1. We propose a novel TS-ACL framework for privacy- based methods[25,25,26]andtheexamplar-freemethods\npreservingandclass-incrementalpatternrecognition.By [1,12,23].Thereplay-based methodsstoreasubsetofhis-\nintroducinganalyticlearningtoreplacegradientdescent, toricalsamples,calledtheexemplar,andreplaythemduring\nTS-ACLachievesinsightfulnon-forgetting,therebyfun- incrementaltrainingtoalleviatetheproblemofcatastrophic\ndamentallysolvingtheissueofcatastrophicforgetting. forgetting[26,28]. Meanwhile,theexamplar-freemethods\n2. Withtheleastsquaresmethodforeachupdate,TS-ACL attempttopreservepriorknowledgebyincorporatingaddi-\nachieves lightweight consumption in both computation tionaltermsintothelossfunctionorbyexplicitlydesigning\nandstorage,makingithighlysuitableforvariousappli- andmanipulatingtheoptimizationprocess[1,23].\ncations,especiallyinedgecomputingscenarios. In practice, the replay-based methods typically achieve\n3. Through theoretical analysis, we prove that TS-ACL’s better performance thanks to the exemplar, but they inher-\nPre-training & Coordinating\n(a) Gradient Descent-Based Training (b) Regression-Based Coordinate\nData of Pre-training Encoder Softmax Classifier Data of Pre-trained Encoder RHL Analytic Classifier\nTask 1 Task 1\nFlatten\nRecursive Regression-Based Learning\nTask 2 Task 3 Task t-1 Task t\n(c) Recursive Regression-Based Learning: 2-nd Task (c) Recursive Regression-Based Learning: t-th Task\nData of Pre-trained Encoder RHL Analytic Classifier Data of Pre-trained Encoder RHL Analytic Classifier\nTask 2 Task t\nSupport Update Support Update\nObtained from\nCoordinating\nFigure2.OverviewoftheproposedTS-ACLframework.In(a),apre-trainedencoderisfirstobtainedonTask1throughGradientDescent-\nBasedTraining. Next,asshownin(b),aRegression-BasedCoordinatemodulewithanRHLisintroducedbeforetheclassificationhead\ntoenhancetheembeddingdimension,resultinginΨ fromTask1data.Finally,in(c),aRecursiveRegression-BasedLearningprocessis\n1\nappliedtoachievenon-forgettingacrosstasks.\nentlyraisesignificantconcernsaboutprivacy[39]. Inmany niquesinasingle-epochtrainingprocess. Recently,several\npractical applications, particularly edge computing scenar- analytic learning-based CIL methods [38–40] have been\nios,thelimitedstorageandcomputationalresourcesusually proposed,whichutilizeleast-squarestechniquestodirectly\nrendersuchmethodsimpractical[17].Meanwhile,although compute the closed-form solutions for neural network in\nthe exemplar-free methods can avoid privacy issues, they CIL scenario. We highlight that the gradient-free nature\ntypically suffer from significantly suboptimal performance ofanalyticlearningoffersafoundationalsolutiontocatas-\n[33, 40]. More critically, both these methods depend on trophic forgetting. Building on this, we propose TS-ACL,\ngradient-based updating techniques, which fundamentally ananalyticlearningapproachspecificallycraftedforclass-\ndonotsolvetheproblemofcatastrophicforgetting[5]. incrementalpatternrecognitionintimeseries,ensuringab-\nsoluteknowledgeretentionwithoutforgetting.\n2.3.AnalyticLearning\n3.Method\nAnalytic learning, as a distinct technical pathway from\ngradient-basedmethods,hasattractedconsiderableresearch\n3.1.Preliminaries\nattentioninrecentyears[6,30]. Specifically,themainidea\nofanalyticlearningistodirectlycomputetheparametersof LetthemodelbecontinuallytrainedforT tasks,wherethe\nneural networks using mathematical methods such as least trainingdataforeachtaskcomesfromdifferentclasses.Let\nsquares,therebyeliminatingtheneedforgradients[30,39]. Dtrain ∼ {Utrain,Vtrain} and Dtest ∼ {Utest,Vtest} denote\nt t t t t t\nForexample,theradialbasisnetwork[20]useleast-squares the training and testing datasets at task t (t = 1,...,T).\nestimation to train parameters after the kernel transforma- Specifically, U t ∈ RNt×c×l (e.g., N t time series with a\ntion in the first layer. The multilayer analytic learning shapeofc×l)andV ttrain ∈RNt×dyt (withtasktcontaining\n[34] transforms nonlinear network learning into segments, d yt classes)representthestackedinputandlabeltensors.\nandefficientlysolvesthisproblemusingleast-squarestech- IntheCILscenario, theclasseslearnedacrossdifferent\ntasks are independent and non-overlapping [24, 33]. For expandedintoU(E)asfollows:\n1\neachtaskt,thenetworklearnsfromDtrain andDtest,where\nt t\ntheclassesdifferfromthoseinprevioustasks. Intaskt,the U(E) =f (U(encoder)ΦE), (3)\n1 act 1\ndata labels Vtrain and Vtest belong to a task-specific class\nt t\nset C t, disjoint from other sets C\nt′\n(for t′ ̸= t). Thus, data whereU( 1E) ∈RN1×dE,withd Erepresentingtheembedding\nfromdifferenttasksexhibitdistinctclassdistributions. For expansionsize(generallyd encoder <d E). Here,f actrefersto\neachtaskt,theobjectiveistousepreviousparametersΘ theactivationfunction,andΦEisthematrixusedtoexpand\nt−1\nand Dtrain to update the model to Θ , ensuring both stabil- theencoder-extractedembeddings.ThematrixΦEisinitial-\nt t\nity (retaining past knowledge) and plasticity (learning new izedbysamplingitselementsfromanormaldistribution.\nknowledge). WhyneedRHL?ThenecessityoftheRHLlayerliesin\nOur proposed TS-ACL, comprising Gradient Descent- the need for a larger parameter space for analytical learn-\nBased Training, Regression-Based Coordinate, and Recur- ing. By projecting the original embeddings into a higher-\nsive Regression-Based Learning as shown in Figure 2, dimensional space, it enables the model to fully realize its\nachieves both stability and plasticity. TS-ACL ensures ab- potential. Thisprojectionallowsthemodeltoexpressmore\nsoluteretentionofpastknowledge,guaranteeingincremen- complexrelationshipswithinthedata,whicharedifficultto\ntallearningisequivalenttojointtraining,wherealltasksare captureinalower-dimensionalspace.\nlearnedsimultaneously.Thisprovidesrigorousassuranceof Finally, the expanded embeddings U(E) are mapped to\n1\nhighaccuracyacrosspastandcurrenttasks. the label matrix Vtrain using a linear regression procedure,\n1\nbysolvingthefollowingoptimizationproblem:\n3.2.GradientDescent-BasedTraining\n(cid:13) (cid:13)2\nFirst,weusegradientdescenttotrainaregularclassification argmin(cid:13)Vtrain−U(E)Φ (cid:13) +γ∥Φ ∥2 , (4)\n(cid:13) 1 1 1(cid:13) 1 F\nnetwork on the basic training task, which usually includes Φ1 F\nmultipletrainingepochs. Theoretically,itcanbecomposed\nwhereγ isaregularizationparameter. Thesolutiontothis\nof any commonly used network structure with a softmax\nproblemisgivenby:\nclassifier. After training, the output V of the network can\nbeexpressedas: Φ(cid:98)1 =(cid:16) U( 1E)⊤U( 1E)+γI(cid:17)−1 U( 1E)⊤V 1train, (5)\nV=f (f (f (U,Θ ))Φ), (1)\nsoftmax flat encoder encoder where the notation ⊤ is the transposed operation and the\nnotation−1representtheinverseoperation.\nwhere Θ and Φ are the parameters representing the\nencoder\nencodernetworkandtheclassifier,f encoder(U,Θ encoder)rep- 3.4.RecursiveRegression-BasedLearning\nresentstheencoderoutput,f andf arethesoftmax\nsoftmax flat\nAfter completing the Regression-Based Coordinate, we\nfunctionandtheflatteningoperator. Aftertraining,wesave\nproceedwiththeCILprocessusingRecursiveRegression-\nand freeze the encoder weights and treat the encoder as a\nBased Learning in a recursive and analytical manner. To\nembeddingexactor.\ndemonstratethis,withoutlossofgenerality,assumeweare\n3.3.Regression-BasedCoordinate givenDtrain,...,Dtrain,andlet\n1 t−1\nU prp oo cn eedco wm ip thleti tn hg\ne\nRG er gad reie ssn it onD -Bes ac se en dt-B Ca os oe rd dinT ar ta ein sin teg p, uw se\n-\nU( 1E :t)\n−1\n∈RN1:t−1×dE, V\n1:t−1\n∈RN1:t−1×(cid:80)t i=− 11dyi (6)\ningtheanalyticalclassifierreplacingthesoftmaxclassifier. represent the concatenated activation and label tensors, re-\nThis process is carried out by the analytic classifier as the spectively,fromtask1tot−1,i.e.,\ncoremechanism,andbrieflyconsistsofthreesub-steps:\nThefirstsub-stepinvolvesextractingtheembeddingma-  U(E)  Vtrain ... 0 \n1 1\nt tr hi rx o( ud ge hn to ht eed tra as inU ed( 1en ec no cde or) d) eb ry nep ta wss oi rn kg ,fth oe lloin wp eu dtt be yns ao flr aU ttt 1 era nin - U( 1E :t) −1 =   . . .   , V 1:t−1 =  . . . ... . . .  .\ningoperation,i.e.,\nU( t−E)\n1\n0 ... V ttr −ai 1n\n(7)\nU(encoder) =f (f (Utrain,Θ )), (2) Here,N 1:t−1 indicatesthetotalnumberofdatasamples\n1 flat encoder 1 encoder\nfromtask1tot−1. ThesparsestructureofV arises\n1:t−1\nwhere U(encoder) ∈ RN1×dencoder. Next, instead of directly becauseofthemutuallyexclusiveclassesacrosstasks. The\n1 learningproblemcanthenbeformulatedasfollows:\nmapping the embeddings to the classification output via a\nsingle classifier layer, we add a Randomly-initialized Hid- (cid:13) (cid:13)2\ndenLayer(RHL).Specifically,theembeddingU(encoder) is argmin(cid:13) (cid:13)V 1:t−1−U( 1E :t) −1Φ t−1(cid:13) (cid:13) +γ∥Φ t−1∥2 F , (8)\n1 Φt−1 F\naccordingto5,attaskt−1,wehave: Table1.Timeseriesdatasetdetailswhichissourcedfrom[24].\n(cid:16) (cid:17)−1\nΦˆ = U(E)⊤ U +γI U(E)⊤ V , (9) Dataset Shape(C×L) TrainSize TestSize #Classes #ExpTasks\nt−1 1:t−1 1:t−1 1:t−1 1:t−1\nUCI-HAR[7] 9×128 7352 2947 6 3\nUWave[16] 3×315 896 3582 8 4\nwhereΦˆ\nt−1\n∈RdE×(cid:80) it =− 11dyi,withthecolumnsizeexpand- DSA[2] 45×125 6840 2280 18 6\nGRABMyo[22] 28×128 36120 12040 16 5\ningastincreases. Let\nWISDM[35] 3×200 18184 6062 18 6\n(cid:16) (cid:17)−1\nΨ = U(E)⊤ U +γI (10)\nt−1 1:t−1 1:t−1\nCIL Setting. We trained and evaluated on five datasets,\ndenote the aggregated temporal inverse correlation matrix,\neachdividedintontaskswithtwodistinctclassespertask,\nwhich captures the correlation information from both cur-\nand randomly shuffled class order before partitioning. To\nrent and past samples. Based on this, out goal is to com-\nevaluatetherobustness, weuseddifferentrandomseedsto\nputeΦˆ usingonlyΦˆ ,Ψ ,andthecurrenttask’sdata\nt t−1 t−1 conduct five independent experiments and report the aver-\nUtrain,withoutinvolvinghistoricalsamplessuchasU .\nt 1:t−1 ageresultswithstandarddeviation.\nTheprocessisformulatedinthefollowingtheorem.\nTaskStream. Following[24],wedividedtasksintoaval-\nTheorem1. TheΦweights,recursivelyobtainedby idationstreamandanexperimentstream. IntheUCI-HAR\n(cid:104) (cid:105) [7]andUWavedatasets[16],boththevalidationandexper-\nΦˆ t = Φˆ t−1−Ψ tU( tE)U( tE)⊤Φˆ t−1 Ψ tU( tE)⊤V ttrain imentalstreamsconsistof3and4tasks,respectively. And\n(11) other datasets include three tasks in the validation stream,\nareequivalenttothoseobtainedfrom9fortaskt. Thema- withtheexperimentalstreamcoveringtheremainingtasks,\ntrixΨ canalsoberecursivelyupdatedby assummarizedinTable1.Weinitiallyperformagridsearch\nt\nonthevalidationstreamtooptimizehyperparameters,then\n(cid:16) (cid:17)−1\nΨ =Ψ −Ψ U(E) I+U(E)⊤Ψ U(E) U(E)⊤Ψ . use the selected parameters to conduct experiments on the\nt t−1 t−1 t t t−1 t t t−1\n(12) experimentstream.\nComparisonBaselines. Following[24],forexamplar-free\nProof. Seethesupplementarymaterials.\nmethods,weselected3classicalbaselines: LwF[12],MAS\nAs stated in Theorem 1, the proposed TS-ACL frame- [1], and DT2W [23]. For replay-based methods, we chose\nworkprovidesarecursiveupdatefortheΦweightswithout 7up-to-datebaselines:GR[28],ER[26],ICARL[25],DER\nlosing any historical information. First, the base model is [3],ASER[27],CLPOS[10]andFASTICARL[11].\ntrainedontheinitialdataset(e.g. tocomputeΦˆ ), andthe Evaluation Metrics. To quantitatively assess the perfor-\n1\nCIL process continues using the recursive formulation to mance of CIL methods, we utilize two widely adopted\nobtainΦˆ fort>1. metrics: Average accuracy and Forgetting. Average ac-\nt\nPrivacy-Preserving and High-Performance. TS-ACL curacy is calculated by averaging the accuracy of all pre-\nsafeguardsprivacyintwosignificantways: first, byavoid- viously encountered tasks, including the current task af-\ning the use of historical data samples, and second, by en- ter learning the current task t. It is defined as A t =\nsuring impossible to use reverse engineering to obtain his- 1 t (cid:80)t i=1A t,i, where A t,i represents the accuracy on task\ntorical data samples from the Ψ matrix. As an exemplar- i after learning task t. Forgetting is measured to cap-\nfreemethod,TS-ACLdiffersfromotherexemplar-freeap- ture how much performance degrades on previous tasks\nproaches, which often exhibit considerable performance after learning a new task t. It is computed as F t =\ngaps compared to replay-based methods. In contrast, TS- t−1 1(cid:80)t i=− 11(cid:0) max j∈{1,...,t−1}A j,i−A t,i(cid:1) . At task t, the\nACL performs on par with or even surpasses replay-based forgetting on task i is defined as the maximum difference\nmethods, asshowninthefollowingexperiments. TS-ACL betweenthehighestaccuracypreviouslyachievedontaski\nnot only upholds privacy but also achieves high perfor- andtheaccuracyontaskiafterlearningtaskt.\nmance, and this dual benefit is particularly valuable in to- Selectedencoder. Weadoptedthesame1D-CNNencoder\nday’seraofincreaseddataprivacyconcerns. network structure as in [24]. The network comprises four\nconvolutional blocks, each containing a 1D convolutional\n4.Experiment layer, a normalization layer, a max-pooling layer, and a\ndropout layer. For the GR generator, we chose TimeVAE\n4.1.Setup\n[4],whereboththeencoderanddecoderfollowafour-layer\nDatasets. Following [24] We continued previous research Conv1DandConvTranspose1Dstructure.\nby selecting five balanced time series datasets, each con- Implementation Details. Following [24], all experiments\ntaining samples with the same length and variables, which were conducted five times, each with distinct class orders\nisshowninTable1. Seesupplementaryformoredetails. and random seeds. For each experimental run, hyperpa-\n(a)BatchNorm\n(b)LayerNorm\nFigure3.Theaccuracyofallmethodsonfivedatasetschangesasthetasksprogress.\nTable2. Forthebest-performingmethodintheexemplar-freeapproach,weuseboldformatting,andforthebestperformanceacrossall\nmethods,weunderlinetheresult.Theaveragevalueandstandarddeviationofeachmetricarereportedbasedontheresultsof5runs.\n(a)BatchNorm\nDataset Metric Naive Offline GR ER DER Herding ASER CLOPS FastICARL LwF MAS DT2W TS-ACL\nUCI-HAR\nAT↑ 32.00±2.90 94.94±2.17 66.66±14.05 65.46±13.78 74.41±7.57 69.58±19.66 92.36±2.78 72.87±11.91 79.69±7.77 35.96±11.33 52.34±15.92 53.23±16.33 88.41±1.52\nFT↓ 98.71±1.38 N.A 34.51±17.06 31.77±34.66 8.37±13.59 32.01±33.80 8.58±5.00 22.83±26.62 20.53±18.20 80.78±17.46 63.40±22.78 60.17±25.07 7.23±3.08\nUWave\nAT↑ 25.36±0.59 96.61±1.05 76.20±6.25 70.28±8.21 70.88±7.71 78.47±2.87 82.74±2.01 71.04±1.66 67.77±8.62 44.67±11.64 53.80±10.36 64.44±5.92 91.89±1.72\nFT↓ 91.35±18.7 N.A 27.43±7.24 36.91±11.10 35.01±10.41 26.00±3.26 21.21±2.71 35.47±1.32 39.33±11.16 51.46±27.35 40.56±21.64 32.66±5.79 4.05±0.78\nDSA\nAT↑ 17.04±0.82 99.65±0.46 31.51±5.74 79.75±18.11 59.19±17.56 82.42±10.04 97.26±1.59 74.10±15.43 67.28±15.06 24.82±8.02 31.82±6.36 19.56±3.82 98.33±1.34\nFT↓ 99.55±0.99 N.A 72.92±9.96 22.57±19.31 28.55±30.89 19.45±12.30 3.18±1.84 22.83±22.31 35.47±19.87 80.72±13.51 74.85±13.94 90.27±10.13 0.94±1.33\nGRABMyo\nAT↑ 19.44±0.38 93.63±1.22 20.59±4.67 47.03±6.82 31.38±4.65 47.14±7.43 56.50±4.24 43.75±4.03 40.55±4.97 19.22±0.49 19.04±1.45 21.34±4.75 57.06±3.80\nFT↓ 95.23±2.43 N.A 64.87±8.64 32.37±6.28 62.89±4.34 33.24±15.75 48.36±5.16 29.66±7.33 48.50±6.76 95.0±1.5 67.09±41.90 58.33±10.10 15.54±1.42\nWISDM\nAT↑ 14.89±1.39 85.31±1.30 42.42±4.99 48.36±16.79 32.95±5.89 32.72±6.23 14.89±2.55 10.74±2.79 17.29±6.76 15.44±1.75 41.69±7.54 28.99±9.19 85.35±2.81\nFT↓ 90.07±8.6 N.A 28.66±11.88 57.49±19.89 34.02±13.47 40.17±9.42 69.35±34.11 59.45±18.45 51.10±30.63 73.35±10.23 26.62±12.48 53.80±28.75 6.29±0.58\n(b)LayerNorm\nDataset Metric Naive Offline GR ER DER Herding ASER CLOPS FastICARL LwF MAS DT2W TS-ACL\nUCI-HAR\nAT↑ 36.44±10.35 92.31±0.82 80.04±7.69 89.53±2.41 90.75±1.90 89.95±2.51 89.82±1.43 89.64±1.50 85.43±3.74 47.40±14.04 59.53±15.90 80.15±6.11 87.75±2.24\nFT↓ 92.25±13.25 N.A 25.75±15.74 9.53±6.54 8.58±5.97 9.96±7.24 10.20±5.57 8.98±4.82 17.51±8.25 74.04±22.17 52.47±27.00 16.55±9.33 6.29±2.87\nUWave\nAT↑ 24.85±0.12 96.39±0.22 85.77±3.76 78.89±4.27 77.74±6.51 85.42±1.89 77.89±5.26 73.79±3.39 79.01±0.98 29.09±5.34 40.74±9.29 55.09±9.27 92.12±1.75\nFT↓ 98.15±1.4 N.A 15.37±4.38 25.87±5.68 27.31±7.67 16.51±1.82 27.14±6.33 32.12±2.95 25.60±2.24 73.47±25.06 65.01±15.14 40.28±18.04 3.72±1.31\nDSA\nAT↑ 19.81±4.12 99.53±0.76 69.50±7.26 97.24±1.43 98.01±0.69 97.75±1.36 95.97±6.32 89.65±5.51 91.39±6.16 17.01±4.33 35.75±6.35 19.06±4.11 98.12±1.75\nFT↓ 96.23±4.95 N.A 36.43±8.64 3.25±1.78 2.28±0.82 2.62±1.59 4.73±7.55 12.30±6.64 10.28±7.44 87.93±15.21 66.17±14.57 96.85±4.81 0.98±1.08\nGRABMyo\nAT↑ 19.46±0.34 93.83±0.87 20.56±1.37 61.16±3.30 63.78±3.96 60.07±3.69 57.90±4.79 52.05±5.11 52.84±3.49 19.42±0.32 18.15±1.57 20.09±7.28 56.44±3.22\nFT↓ 94.17±3.34 N.A 94.44±3.13 40.87±3.59 37.01±3.68 42.46±4.02 45.49±5.48 52.22±5.54 52.46±4.19 93.25±6.64 88.34±7.52 22.57±9.52 15.75±1.69\nWISDM\nAT↑ 14.60±2.25 88.60±1.94 24.33±8.12 66.88±7.36 67.14±5.37 69.67±3.98 51.48±15.85 44.00±8.11 44.87±3.67 13.83±3.51 19.25±6.66 15.59±7.92 83.53±2.87\nFT↓ 88.47±10.58 N.A 85.78±10.00 33.80±7.90 33.38±6.67 30.04±3.98 53.04±18.68 60.74±10.12 52.79±6.37 83.53±17.51 74.65±12.06 37.50±14.48 7.13±1.69\nrameter tuning was performed through two validation it- coderwithanappendedRHL.Datapreprocessingincludes\nerations on validation tasks. All models were trained us- anon-trainableinputnormalizationlayerappliedbeforethe\ningtheAdamoptimizerwithalearningrateof0.001anda encoder to standardize inputs on a per-sample basis. Spe-\nbatchsizeof64,with100epochsforthefirsttaskandsub- cific normalization methods were used for each dataset:\nsequent tasks and the learning rate scheduler is treated as layer normalization for UCI-HAR, DSA, and GRABMyo;\na hyperparameter and is tuned during the validation tasks. instance normalization for UWave; and no normalization\nHowever, TS-ACL diverges from this by bypassing gradi- forWISDM.Theencoder’sarchitectureintegratestwonor-\nent descent after the initial task, opting instead for an an- malization types: Batch Normalization (BN) and Layer\nalytical update method that requires only a single epoch. Normalization(LN).Forreplay-basedmethods, wesetthe\nDuringincrementallearning,TS-ACLemploysafrozenen- memorystoragepoolto5%. ForTS-ACL,weconfigureγ\n(a)BatchNorm (b)LayerNorm\nFigure4.Theperformanceofeachmethodisaveragedacrossthefivedatasets.\n(a)BatchNorm\n(b)LayerNorm\nFigure5.TheaccuracyofTS-ACLonfivedatasetsvarieswithdifferentγvaluesastasksprogress.\nto{0.01,0.1,1,10,100}andsettheembeddingexpansion BN and LN conditions. Specifically, under BN, it sur-\nsize to 8192. We select the best γ value to present in the passes the strongest baseline model by 1.35%, 17.16%,\nexperimentalresults. Seesupplementaryformoredetails. 2.24%, 16.83%, and 20.33% on the five datasets, respec-\ntively. Likewise, under LN, it outperforms the strongest\n4.2.MainResults\nbaselineby2.29%,12.79%,1.3%,6.82%,and22.91%,re-\nspectively. This superior performance can be attributed to\nPlasticity. As demonstrated in Table 2 and Figure 3, TS-\nthe fact that TS-ACL is mathematically proven to achieve\nACL achieves remarkable accuracy across five datasets.\nresultscomparabletojointlearninginthecontextofincre-\nUnder the exemplar-free approach, TS-ACL consistently\nmentallearning,whichhastheremarkablepropertyofnon-\noutperforms the best existing methods. In the BN set-\nforgetting,asdiscussedinSection3.4.\nting,TS-ACLsurpassesthesecond-bestmethodby35.18%,\n27.45%, 66.51%, 35.72%, and 43.66% across the five\n4.3.RobustnessAnalyses\ndatasets. IntheLNsetting,TS-ACLsurpassesthesecond-\nbest method by 7.6%, 37.03%, 62.37%, 36.41%, and Class Order Robustness. For each dataset and method,\n64.28%, respectively. In fact, on most datasets, TS-ACL weconductedexperimentsfivetimes,eachwithadifferent\noutperforms all methods, including the second-best by random seed, resulting in varying class orders across each\n9.15%, 1.07%, 0.56%, and 37.00% in the BN setting for run. Wethenaveragedthefinaltaskaccuracyandstandard\nUWave, DSA,GRABMyo, andWSIDM,respectively, and deviationofeachmethodacrossfivedatasets,asillustrated\nby 6.70%, 0.11%, 13.86% in the LN setting for UWave, in Figure 4 according to Table 2. The results indicate that\nDSA,andWSIDM,respectively. Theseresultsunderscore theTS-ACLachievesthehighestaverageaccuracyandthe\nTS-ACL’ssuperiorperformanceintermsofplasticity. loweststandarddeviation,Here,weprimarilyfocusonthe\nStability. As shown in Table 2, our TS-ACL achieves magnitude of the standard deviation, as it directly reflects\nthe lowest forgetting rate across all datasets, under both the robustness of each method under different Class Order\nTable3.A andF onallbenchmarkdatasetswithvariousvaluesoftheregularizationtermγ.\nT T\n(a)BatchNorm\nUCI-HAR(%) UWave(%) DSA(%) GRABMyo(%) WISDM(%)\nγ\nAT ↑ FT ↓ AT ↑ FT ↓ AT ↑ FT ↓ AT ↑ FT ↓ AT ↑ FT ↓\n0.01 85.83±2.71 9.57±2.09 91.67±1.66 4.11±1.11 93.67±8.37 4.37±8.92 50.11±4.02 10.45±1.97 79.57±9.63 10.24±7.44\n0.1 83.53±1.64 10.16±3.34 91.89±1.72 4.05±0.78 96.50±5.61 2.75±5.66 48.43±2.93 15.76±2.04 84.86±2.11 6.82±1.51\n1 87.15±1.37 7.70±3.32 91.68±2.24 3.96±1.34 98.26±2.25 1.33±2.33 54.91±3.37 15.43±2.32 85.35±2.81 6.29±0.58\n10 88.20±1.40 7.29±3.03 91.35±2.10 3.98±1.26 97.89±1.47 1.38±1.38 57.06±3.80 15.54±1.42 80.89±3.39 8.45±0.92\n100 88.41±1.52 7.23±3.08 88.21±2.61 4.45±1.18 96.54±1.57 2.28±1.64 53.14±4.96 15.70±1.67 68.92±6.53 12.81±2.40\n(b)LayerNorm\nUCI-HAR(%) UWave(%) DSA(%) GRABMyo(%) WISDM(%)\nγ\nAT ↑ FT ↓ AT ↑ FT ↓ AT ↑ FT ↓ AT ↑ FT ↓ AT ↑ FT ↓\n0.01 83.38±1.42 9.50±3.06 92.05±0.72 3.45±0.94 96.49±2.47 1.88±1.85 50.82±3.45 9.44±2.33 73.75±6.85 12.05±6.86\n0.1 84.43±2.77 8.33±2.80 91.80±1.02 3.89±1.75 98.12±1.07 0.98±1.08 49.27±5.34 12.24±3.37 82.46±4.76 8.22±4.17\n1 87.47±2.56 7.03±3.39 92.12±1.75 3.72±1.31 98.03±1.14 1.02±1.16 52.52±3.37 14.79±2.48 83.53±2.87 7.13±1.69\n10 87.75±2.24 6.29±2.87 91.00±2.52 4.31±1.78 96.44±1.82 1.90±1.48 56.44±3.22 15.54±1.42 78.45±4.16 9.17±1.58\n100 87.38±2.08 7.12±3.63 88.12±2.65 4.88±1.37 93.89±1.81 3.18±1.62 55.17±2.73 15.55±1.32 69.06±5.28 12.23±1.39\nsettings. Amongallthecomparisonmethods,ourapproach andtheextractedembeddingscanbeconsideredtocontain\nmaintains the smallest standard deviation, indicating that similarrepresentations.Asaresult,theydonotsignificantly\nourmethodexhibitsthestrongestclassorderrobustness. impact the classification. In comparison to exemplar-free\nWhy TS-ACL has class order robustness? As demon- methods,theeffectivenessofBNorLNdependsonthespe-\nstrated in 3.4, the effectiveness of our TS-ACL in incre- cificapproachused.Theattributeofnormrobustnesswithin\nmental continual learning is equivalent to that of joint ourTS-ACLprovidesauniqueadvantage,asithasminimal\nlearning. This weight-invariant property ensures that our impactonresults,allowingustoavoidrepeatedexperiments\nmethod achieves nearly consistent performance across dif- tofindtheoptimalnormalizationway.\nferent class order settings, with only minor standard devi- RegularizationTermAnalysis. AsshowninFigure5and\nation due to the slight differences in encoding parameters Table3, overall, γ isrelativelystableacrossabroadrange\nlearned during the first task. This property guarantees ro- of values (e.g. {0.01,0.1,1,10,100}). However, on the\nbustperformanceacrossvariousclassordersettings. secondtaskoftheGRABMyodataset,asignificantperfor-\nNorm Robustness. As shown in Figure 3 and Table 2, mancedropisobservedwhenγissetto0.01and0.1,which\nfor TS-ACL, the performance difference between BN and is due to overfitting. Additionally, on the Wisdom dataset,\nLNisnegligible. Inreplay-basedmethods,duetoBN’sre- when γ is set to 100, performance declines notably due to\nliance on batch statistics, is particularly vulnerable to the underfitting, as the simplicity of the linear regression be-\nimpact of imbalanced distributions between new and old comesapparent[37].\nsamples in incremental learning. This imbalance not only\ncontributes to the forgetting of previously learned knowl- 5.Conclusion\nedge but also hinders the acquisition of new information\n[21]. Incontrast,LNthroughinstancenormalization,effec- In this study, we propose TS-ACL, an analytical continual\ntivelyalleviatestheseissues,demonstratingsuperiorperfor- learning framework for time series class-incremental pat-\nmance. However, ASER exhibits a little performance gap tern recognition that fundamentally addresses the problem\nbetween BN and LN, as hypothesized in [24]. ASER se- of catastrophic forgetting through gradient-free recursive\nlectsabalancedandrepresentativesetofmemorysamples, regression learning, while also achieving privacy preser-\nwhichhelpsmaintainunbiasedstatisticsunderBN. vation and efficiency. Experimental results demonstrate\nWhy TS-ACL has norm robustness? Our TS-ACL pos- that TS-ACL significantly outperforms existing methods\nsesses norm robustness because it is trained using analyti- acrossfivebenchmarkdatasets,achievingbothstabilityand\ncal learning, with the encoder functioning as a frozen em- plasticity,andsimultaneouslypossessingthenon-forgetting\nbeddingextractorthatdoesnotrequiregradientupdatesor property and weight-invariant property, making it a robust\nbackpropagationduringcontinuallearning. Therefore, BN solution,particularlyinresource-constrainedscenariossuch\nand LN fundamentally only modify embedding extraction, asedgecomputing.\nReferences KnowledgeDiscoveryandDataMining, pages6555–6565,\n2024. 1\n[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,\n[14] JasonLinesandAnthonyBagnall. Timeseriesclassification\nMarcus Rohrbach, and Tinne Tuytelaars. Memory aware\nwith ensembles of elastic distance measures. Data Mining\nsynapses: Learningwhat(not)toforget. InProceedingsof\nandKnowledgeDiscovery,29:565–592,2015. 2\ntheEuropeanconferenceoncomputervision(ECCV),pages\n[15] JasonLines,SarahTaylor,andAnthonyBagnall.Timeseries\n139–154,2018. 2,5\nclassificationwithhive-cote:Thehierarchicalvotecollective\n[2] KeremAltunandBillurBarshan. Humanactivityrecogni-\nof transformation-based ensembles. ACM Transactions on\ntionusinginertial/magneticsensorunits.InHumanBehavior\nKnowledgeDiscoveryfromData(TKDD),12(5):1–35,2018.\nUnderstanding: First International Workshop, HBU 2010,\n2\nIstanbul,Turkey,August22,2010.Proceedings1,pages38–\n[16] Jiayang Liu, Lin Zhong, Jehan Wickramasuriya, and Venu\n51.Springer,2010. 5\nVasudevan. uwave: Accelerometer-basedpersonalizedges-\n[3] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide\nturerecognitionanditsapplications. PervasiveandMobile\nAbati, and Simone Calderara. Dark experience for gen-\nComputing,5(6):657–675,2009. 5\neralcontinuallearning: astrong,simplebaseline. Advances\n[17] Siliang Lu, Jingfeng Lu, Kang An, Xiaoxian Wang, and\ninneuralinformationprocessingsystems,33:15920–15930,\nQingboHe. Edgecomputingoniotformachinesignalpro-\n2020. 5\ncessing and fault diagnosis: A review. IEEE Internet of\n[4] Abhyuday Desai, Cynthia Freeman, Zuhui Wang, and Ian\nThingsJournal,10(13):11093–11116,2023. 2,3\nBeaver. Timevae: Avariationalauto-encoderformultivari-\n[18] Matthew Middlehurst, James Large, Michael Flynn, Jason\natetimeseriesgeneration.arXivpreprintarXiv:2111.08095,\nLines,AaronBostrom,andAnthonyBagnall.Hive-cote2.0:\n2021. 5\nanewmetaensemblefortimeseriesclassification. Machine\n[5] IanJGoodfellow,MehdiMirza,DaXiao,AaronCourville,\nLearning,110(11):3211–3243,2021. 2\nand Yoshua Bengio. An empirical investigation of catas-\n[19] NavidMohammadiFoumani,LynnMiller,ChangWeiTan,\ntrophicforgettingingradient-basedneuralnetworks. arXiv\nGeoffrey I. Webb, Germain Forestier, and Mahsa Salehi.\npreprintarXiv:1312.6211,2013. 1,2,3\nDeeplearningfortimeseriesclassificationandextrinsicre-\n[6] Ping Guo, Michael R Lyu, and NE Mastorakis. Pseudoin- gression:Acurrentsurvey.ACMComput.Surv.,56(9),2024.\nverse learning algorithm for feedforward neural networks.\n2\nAdvancesinNeuralNetworksandApplications,1(321-326),\n[20] JooyoungParkandIrwinWSandberg. Universalapproxi-\n2001. 3\nmationusingradial-basis-functionnetworks.Neuralcompu-\n[7] DewiPramudiIsmi,ShireenPanchoo,andMurintoMurinto. tation,3(2):246–257,1991. 3\nK-meansclusteringbasedfilterfeatureselectiononhighdi-\n[21] QuangPham,ChenghaoLiu,andStevenHoi.Continualnor-\nmensionaldata. InternationalJournalofAdvancesinIntel-\nmalization: Rethinkingbatchnormalizationforonlinecon-\nligentInformatics,2(1):38–45,2016. 5\ntinuallearning. arXivpreprintarXiv:2203.16102,2022. 8\n[8] MingJin,HuanYeeKoh,QingsongWen,DanieleZambon,\n[22] AshirbadPradhan, JiayuanHe, andNingJiang. Multi-day\nCesareAlippi,GeoffreyI.Webb,IrwinKing,andShiruiPan.\ndatasetofforearmandwristelectromyogramforhandges-\nAsurveyongraphneuralnetworksfortimeseries:Forecast-\nture recognition and biometrics. Scientific data, 9(1):733,\ning,classification,imputation,andanomalydetection. IEEE\n2022. 5\nTransactionsonPatternAnalysisandMachineIntelligence,\n[23] Zhongzheng Qiao, Minghui Hu, Xudong Jiang, Ponnuthu-\n2024. 2\nraiNagaratnamSuganthan,andRamasamySavitha. Class-\n[9] RohitJKate. Usingdynamictimewarpingdistancesasfea- incremental learning on multivariate time series via shape-\ntures for improved time series classification. Data mining aligned temporal distillation. In ICASSP 2023-2023 IEEE\nandknowledgediscovery,30:283–312,2016. 2 International Conference on Acoustics, Speech and Signal\n[10] Dani Kiyasseh, Tingting Zhu, and David Clifton. A clini- Processing(ICASSP),pages1–5.IEEE,2023. 1,2,5\ncal deep learning framework for continually learning from [24] Zhongzheng Qiao, Quang Pham, Zhen Cao, Hoang H Le,\ncardiacsignalsacrossdiseases,time,modalities,andinstitu- Ponnuthurai N Suganthan, Xudong Jiang, and Ramasamy\ntions. NatureCommunications,12(1):4221,2021. 1,5 Savitha. Class-incrementallearningfortimeseries: Bench-\n[11] Young D Kwon, Jagmohan Chauhan, and Cecilia Mas- mark and evaluation. arXiv preprint arXiv:2402.12035,\ncolo.Fasticarl:Fastincrementalclassifierandrepresentation 2024. 4,5,8\nlearningwithefficientbudgetallocationinaudiosensingap- [25] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\nplications. arXivpreprintarXiv:2106.07268,2021. 2,5 Sperl,andChristophHLampert.icarl:Incrementalclassifier\n[12] ZhizhongLiandDerekHoiem.Learningwithoutforgetting. andrepresentationlearning.InProceedingsoftheIEEEcon-\nIEEEtransactionsonpatternanalysisandmachineintelli- ferenceonComputerVisionandPatternRecognition,pages\ngence,40(12):2935–2947,2017. 2,5 2001–2010,2017. 2,5\n[13] YuxuanLiang,HaominWen,YuqiNie,YushanJiang,Ming [26] DavidRolnick,ArunAhuja,JonathanSchwarz,TimothyLil-\nJin,DongjinSong,ShiruiPan,andQingsongWen. Founda- licrap,andGregoryWayne. Experiencereplayforcontinual\ntionmodelsfortimeseriesanalysis: Atutorialandsurvey. learning. Advances in neural information processing sys-\nIn Proceedings of the 30th ACM SIGKDD Conference on tems,32,2019. 2,5\n[27] Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott San- oftheIEEE/CVFconferenceoncomputervisionandpattern\nner, Hyunwoo Kim, and Jongseong Jang. Online class- recognition,pages7746–7755,2023. 2,3\nincremental continual learning with adversarial shapley [40] HuipingZhuang,RunHe,KaiTong,ZiqianZeng,CenChen,\nvalue. InProceedingsoftheAAAIConferenceonArtificial andZhipingLin.DS-AL:Adual-streamanalyticlearningfor\nIntelligence,pages9630–9638,2021. 5 exemplar-freeclass-incrementallearning. InProceedingsof\n[28] HanulShin,JungKwonLee,JaehongKim,andJiwonKim. theAAAIConferenceonArtificialIntelligence,pages17237–\nContinuallearningwithdeepgenerativereplay. Advancesin 17244,2024. 2,3\nneuralinformationprocessingsystems,30,2017. 2,5\n[29] Chang Wei Tan, Franc¸ois Petitjean, and Geoffrey I Webb.\nFastee: fast ensembles of elastic distances for time series\nclassification. Data Mining and Knowledge Discovery, 34\n(1):231–272,2020. 2\n[30] JonathanTapsonandAndre´vanSchaik. Learningthepseu-\ndoinversesolutiontonetworkweights.NeuralNetworks,45:\n94–100,2013. 3\n[31] JingyuanWang,ChenYang,XiaohanJiang,andJunjieWu.\nWhen: Awavelet-dtwhybridattentionnetworkforhetero-\ngeneous time series analysis. In Proceedings of the 29th\nACM SIGKDD Conference on Knowledge Discovery and\nDataMining,pages2361–2373,2023. 2\n[32] Lin Wang, Zheng Yin, Mamta Puppala, Chika F. Ezeana,\nKelvin K. Wong, Tiancheng He, Deepa B. Gotur, and\nStephenT.C.Wong. Atime-seriesfeature-basedrecursive\nclassificationmodeltooptimizetreatmentstrategiesforim-\nproving outcomes and resource allocations of covid-19 pa-\ntients. IEEEJournalofBiomedicalandHealthInformatics,\n26(7):3323–3329,2022. 1,2\n[33] LiyuanWang, XingxingZhang, HangSu, andJunZhu. A\ncomprehensivesurveyofcontinuallearning:Theory,method\nandapplication. IEEETransactionsonPatternAnalysisand\nMachineIntelligence,46(8):5362–5383,2024. 1,2,3,4\n[34] Xi-ZhaoWang,TianlunZhang,andRanWang.Noniterative\ndeep learning: Incorporating restricted boltzmann machine\nintomultilayerrandomweightneuralnetworks.IEEETrans-\nactionsonSystems,Man,andCybernetics: Systems,49(7):\n1299–1308,2019. 3\n[35] GaryMWeiss. Wisdmsmartphoneandsmartwatchactivity\nandbiometricsdataset. UCIMachineLearningRepository:\nWISDM Smartphone and Smartwatch Activity and Biomet-\nricsDatasetDataSet,7:133190–133202,2019. 5\n[36] WenbiaoYang, KewenXia, ZhaochengWang, ShuruiFan,\nandLingLi. Self-attentioncausaldilatedconvolutionalneu-\nralnetworkformultivariatetimeseriesclassificationandits\napplication. Engineering Applications of Artificial Intelli-\ngence,122:106151,2023. 1,2\n[37] HuipingZhuang,ZhipingLin,andKar-AnnToh.Correlation\nprojection for analytic learning of a classification network.\nNeuralProcessingLetters,53:3893–3914,2021. 8\n[38] Huiping Zhuang, Zhenyu Weng, Hongxin Wei, Renchunzi\nXie,Kar-AnnToh,andZhipingLin. ACIL:Analyticclass-\nincremental learning with absolute memorization and pri-\nvacyprotection.InAdvancesinNeuralInformationProcess-\ning Systems, pages 11602–11614. Curran Associates, Inc.,\n2022. 3\n[39] HuipingZhuang, ZhenyuWeng, RunHe, ZhipingLin, and\nZiqian Zeng. Gkeal: Gaussian kernel embedded analytic\nlearningforfew-shotclassincrementaltask. InProceedings\nTS-ACL: A Time Series Analytic Continual Learning Framework for\nPrivacy-Preserving and Class-Incremental Pattern Recognition\nSupplementary Material\nA.ProofofTheorem\nInthissection,weprovideamathematicalproofforTheorem1.\nWeproveTheorem1usingthesolutionofjointtrainingonttaskswithridgeregression:\n(cid:16) (cid:17)−1\nΦˆ = U(E)⊤U(E)+γI U(E)⊤V . (13)\nt 1:t 1:t 1:t 1:t\nBydecouplingthet-thtaskfromprevioustasks,Φˆ canbewrittenas:\nt\n (cid:16) (cid:17) −1\nΦˆ t =(cid:104) U( 1E :t) −⊤ 1 U( tE)⊤(cid:105)  (cid:16)U U( 1E : ( tt) E− )(cid:17)1 +γI (cid:104) U( 1E :t) −⊤ 1 U( tE)⊤(cid:105)(cid:20) V 1 0:t−1 V0 ttrain(cid:21)\n(14)\n(cid:16) (cid:17)−1(cid:104) (cid:105)\n= U(E)⊤ U(E) +γI+U(E)⊤U(E) U(E)⊤ V U(E)⊤Vtrain .\n1:t−1 1:t−1 t t 1:t−1 1:t−1 t t\nWeintroducethememorymatrixasinthefollowingdefinition:\n(cid:16) (cid:17)−1\nΨ = U(E)⊤U(E)+γI , (15)\nt 1:t 1:t\nwhichisthematrixinversiontermofEqn13.\n(cid:16) (cid:17)−1\nNoticing that Ψ = U(E)⊤ U(E) +γI , by Woodbury matrix identity where (A+UCV)−1 = A−1 −\nt−1 1:t−1 1:t−1\nA−1U(cid:0) C−1+VA−1U(cid:1)−1 VA−1 and treating Ψ as A−1, the memory at the t-th step can be further defined as a\nt−1\nrecursivesolution:\n(cid:16) (cid:17)−1\nΨ =Ψ −Ψ U(E)⊤ I+U(E)Ψ U(E)⊤ U(E)Ψ . (16)\nt t−1 t−1 t t t−1 t t t−1\nThus,theparameterΦˆ isderivedas\nt\n(cid:104) (cid:105)\nΦˆ t = Ψ tU( 1E :t) −⊤ 1V 1:t−1 Ψ tU( tE)⊤V ttrain . (17)\nDenotetheleftsubmatrixΨ U(E)⊤ V asH. BysubstitutingEqn16into17,\nt 1:t−1 1:t−1\n(cid:16) (cid:17)−1\nH=Φˆ −Ψ U(E)⊤ I+U(E)Ψ U(E)⊤ U(E)Φˆ . (18)\nt−1 t−1 t t t−1 t t t−1\nBasedontheidentity(I+P)−1 =I−(I+P)−1P,itisfurtherderivedas:\nH=Φˆ −Ψ U(E)⊤U(E)Φˆ . (19)\nt−1 t t t t−1\nThus,\n(cid:104) (cid:105)\nΦˆ t = Φˆ t−1−Ψ tU( tE)⊤U( tE)Φˆ t−1 Ψ tU( tE)⊤V ttrain . (20)\nTheorem1isthusproved.\nB.MoreImplementationDetails\nThe learning rate scheduler was optimized as a hyperparameter, alongside the use of early stopping to mitigate overfitting\nandpatiencevaluesweresetat20forER-andGR-basedmethodsandat5forotherapproaches. Tofacilitateearlystopping,\navalidationsetwascreatedbydividingthetrainingdataina1:9ratio, withvalidationlossmonitored. Threelearningrate\nschedulingstrategieswereevaluated: Step10, Step15, andOneCycleLR.InStep10andStep15, thelearningratedecreased\nbyafactorof0.1atthe10thand15thepochs,respectively.Dropoutratesvariedacrossdatasets:0forUCI-HARandUWave,\nand0.3forDSA,GRABMyo,andWISDM.Foradditionalinformation,pleaserefertoourcode.\nC.DatasetDetails\nUCI-HARdatasetincludestemporalsequencesobtainedfromsmartphoneinertialsensorsduringtheexecutionofsixdaily\nactivities. Datawerecollectedatafrequencyof50Hzfrom30participantsofvaryingages. Eachinputsequenceconsistsof\nninechannels,coveringatemporalspanof128timesteps.\nUWaveprovidesover4000samplescollectedfromeightindividualsperformingeightdistinctgesturepatterns. Thedata\nconsistsoftimeseriescapturedalongthreeaccelerometeraxes,witheachsamplehavingadimensionalityof3andspanning\n315timesteps.\nDSAgathersmotionsensordatafrom19differentsportsactivitiesperformedbyeightvolunteers. Eachsegmentservesas\nasinglesample,featuringdataacross45channelsand125timesteps. Foruniformityamongclasses,18activityclasseswere\nselectedforexperimentation.\nGRABMyo is a comprehensive surface electromyography (sEMG) dataset designed for hand gesture recognition. It in-\ncludessignalscorrespondingto16gesturesperformedby43participantsacrossthreesessions. Recordingslastfiveseconds,\nsampledfrom28channelsat2048Hz. Forexperimentation,datafromasinglesessionwereusedanddownsampledto256\nHz. Anon-overlappingslidingwindowof0.5seconds(128timesteps)wasappliedtosegmentthesignalsintosamples. The\ndatawerethensplitintotrainingandtestingsetsina3:1ratio,ensuringbothsetsincludeallsubjectstomitigatedistribution\nshiftsacrossparticipants.\nWISDMisahumanactivityrecognitiondatasetthatcapturessensordatafrom18activitiesperformedby51participants.\nUsingthephoneaccelerometermodality,samplesweregeneratedwithanon-overlappingslidingwindowof200,representing\n10-secondtimeseriescollectedata20Hzfrequency. LikeGRABMyo,thedataweresplitintotrainingandtestsetsata3:1\nratio,ensuringallparticipantsarerepresentedinbothsplits.\nD.Algorithm\nAlgorithm1TheProposedTS-ACLFramework\nRequire: TotalnumberoftasksT;Trainingtimeseriesdatasets{Dtrain}T ;Regularizationparameterγ;embeddingexpan-\nt t=1\nsionsized\nE\nEnsure: FinalmodelparametersΘ andΦˆ\nencoder T\n1: GradientDescent-BasedTraining\n2: TrainthenetworkonDtrainusinggradientdescent\n1\n3: SaveandfreezetheencoderweightsΘ encoder\n4: Regression-BasedCoordinate\n5: ExtractandexpandembeddingsusingEq.(2),Eq.(3)\n6: ComputeinitialclassifierweightsusingEq.(5)\n7: ComputeaggregatedtemporalinversecorrelationmatrixusingEq.(10)\n8: RecursiveRegression-BasedLearning\n9: fort=2toT do\n10: ExtractandexpandembeddingsusingEq.(2),Eq.(3)\n11: UpdateaggregatedtemporalinversecorrelationmatrixusingEq.(12)\n12: UpdateclassifierweightsusingEq.(11)\n13: endfor\n14: Output: FinalmodelparametersΘ encoderandΦˆ T",
    "pdf_filename": "TS-ACL_A_Time_Series_Analytic_Continual_Learning_Framework_for_Privacy-Preserving_and_Class-Incremen.pdf"
}