{
    "title": "IDCIA Immunocytochemistry Dataset for Cellular Image Analysis",
    "context": "We present a new annotated microscopic cellular image dataset to improve the effectiveness of machine learning methods for cellular image analysis. Cell counting is an important step in cell analysis. Typically, domain experts manually count cells in a microscopic image. Automated cell counting can potentially eliminate this te- dious, time-consuming process. However, a good, labeled dataset is required for training an accurate machine learning model. Our dataset includes microscopic images of cells, and for each image, the cell count and the location of individual cells. The data were collected as part of an ongoing study investigating the potential of electrical stimulation to modulate stem cell differentiation and possible applications for neural repair. Compared to existing pub- licly available datasets, our dataset has more images of cells stained with more variety of antibodies (protein components of immune responses against invaders) typically used for cell analysis. The experimental results on this dataset indicate that none of the five existing models under this study are able to achieve sufficiently ac- curate count to replace the manual methods. The dataset is available at https://figshare.com/articles/dataset/Dataset/21970604. CCS CONCEPTS ‚Ä¢ Computing methodologies ‚ÜíSupervised learning; ‚Ä¢ Ap- plied computing ‚ÜíSystems biology; Imaging. KEYWORDS Cellular Biology, Machine Learning, Artificial Intelligence, Dataset, Fluorescence Microscopy, Deep learning 1 Cell biology is a sub-discipline of biology where the structure and physiological functioning, and interaction of cells are studied [3]. Cells are examined under a microscope and imaged at a high resolu- tion. In immunocytochemistry (ICC), different antibodies are used ‚àóBoth authors contributed equally. ‚Ä†Abdurahman Ali Mohammed, Wallapak Tavanapong, and Azeez Idris are with the Department of Computer Science at Iowa State University, Ames, IA 50011 USA. ‚Ä°Catherine Fonder and Donald S. Sakaguchi are with the Department of Genetics, De- velopment, and Cell Biology (GDCB), Molecular, Cellular, and Developmental Biology Program (MCDB), and the Neuroscience Program, Iowa State University, Ames, IA 50011 USA. ¬ßNanovaccine Institute, Iowa State University, Ames, IA 50011, USA. ¬∂Surya K. Mallapragada is with the Department of Chemical and Biological Engineer- ing at Iowa State University, Ames, IA 50011 USA. to visualize the presence of particular proteins to identify specific cell types in a given sample. Cell analysis involves a wide range of tasks, such as counting cells and measuring and evaluating cell state (e.g., shape, motility), cell health, and cell growth. Cell biology is closely intertwined with other fields, such as neuroscience, genet- ics, and molecular biology. One fascinating application area of cell biology is research for the potential diagnosis and treatment of dis- eases. The research in this area is full of potential and possibilities that could improve quality of life. Deep Neural Networks (DNNs) have been applied in the analysis of microscopic cell images, including cell counting [26, 35], segmen- tation [1, 10, 11, 23], and detection [9, 12, 34]. Given an input image, cell counting provides the number of cells in the image. In contrast, cell segmentation finds the contours of individual cells, separating them from each other and the background. On the other hand, cell detection localizes a cell by drawing the smallest rectangle around each cell in the input image. The advantages of DNNs over tra- ditional machine learning methods are that DNNs automatically extract important properties (features) of the object of interest and use them to perform the intended task. However, the major draw- back of DNNs is that it requires a large high-quality labeled dataset for accurate predictions. Existing DNN methods for cell counting can be broadly categorized into two groups: detection-based and regression-based categories. The detection-based category undertakes the counting task by first detecting individual cells (contours, bounding boxes, or cen- troids of the cells) in a given image and counting the detected cells to obtain the final cell count [14, 23]. These methods hinge on the availability of the annotated ground truth of the bounding box or a centroid of a cell. The methods are also dependent on the characteristics of the microscopic input images. In particular, detection-based methods fail to offer good performance when there is a high occlusion in the images. The regression-based category [26, 35] predicts the cell count without detecting individual cells. Some of these methods use only the ground truth cell count for each training image for training. Other methods predict a corresponding density map for a given image and obtain the final count from the predicted density map. Our team examines cellular images taken after electrical stimu- lation experiments on stem cells for cell differentiation. Cell differ- entiation is the process in which an unspecialized cell develops and matures to become a specialized cell. Electrical stimulation of stem cells is potentially useful for stem cell therapy in patients with nerve arXiv:2411.08992v2  [eess.IV]  19 Nov 2024",
    "body": "IDCIA: Immunocytochemistry Dataset for Cellular Image Analysis\nAbdurahman Ali Mohammed‚àó‚Ä†\nabdu@iastate.com\nIowa State University\nUSA\nCatherine Fonder‚àó‚Ä°¬ß\ncfonder@iastate.edu\nIowa State University\nUSA\nDonald S. Sakaguchi‚Ä°¬ß\ndssakagu@iastate.edu\nIowa State University\nUSA\nWallapak Tavanapong‚Ä†\ntavanapo@iastate.edu\nIowa State University\nUSA\nSurya K. Mallapragada¬∂¬ß\nsuryakm@iastate.edu\nIowa State University\nUSA\nAzeez Idris‚Ä†\naidris@iastate.edu\nIowa State University\nUSA\nABSTRACT\nWe present a new annotated microscopic cellular image dataset to\nimprove the effectiveness of machine learning methods for cellular\nimage analysis. Cell counting is an important step in cell analysis.\nTypically, domain experts manually count cells in a microscopic\nimage. Automated cell counting can potentially eliminate this te-\ndious, time-consuming process. However, a good, labeled dataset\nis required for training an accurate machine learning model. Our\ndataset includes microscopic images of cells, and for each image,\nthe cell count and the location of individual cells. The data were\ncollected as part of an ongoing study investigating the potential\nof electrical stimulation to modulate stem cell differentiation and\npossible applications for neural repair. Compared to existing pub-\nlicly available datasets, our dataset has more images of cells stained\nwith more variety of antibodies (protein components of immune\nresponses against invaders) typically used for cell analysis. The\nexperimental results on this dataset indicate that none of the five\nexisting models under this study are able to achieve sufficiently ac-\ncurate count to replace the manual methods. The dataset is available\nat https://figshare.com/articles/dataset/Dataset/21970604.\nCCS CONCEPTS\n‚Ä¢ Computing methodologies ‚ÜíSupervised learning; ‚Ä¢ Ap-\nplied computing ‚ÜíSystems biology; Imaging.\nKEYWORDS\nCellular Biology, Machine Learning, Artificial Intelligence, Dataset,\nFluorescence Microscopy, Deep learning\n1\nINTRODUCTION\nCell biology is a sub-discipline of biology where the structure and\nphysiological functioning, and interaction of cells are studied [3].\nCells are examined under a microscope and imaged at a high resolu-\ntion. In immunocytochemistry (ICC), different antibodies are used\n‚àóBoth authors contributed equally.\n‚Ä†Abdurahman Ali Mohammed, Wallapak Tavanapong, and Azeez Idris are with the\nDepartment of Computer Science at Iowa State University, Ames, IA 50011 USA.\n‚Ä°Catherine Fonder and Donald S. Sakaguchi are with the Department of Genetics, De-\nvelopment, and Cell Biology (GDCB), Molecular, Cellular, and Developmental Biology\nProgram (MCDB), and the Neuroscience Program, Iowa State University, Ames, IA\n50011 USA.\n¬ßNanovaccine Institute, Iowa State University, Ames, IA 50011, USA.\n¬∂Surya K. Mallapragada is with the Department of Chemical and Biological Engineer-\ning at Iowa State University, Ames, IA 50011 USA.\nto visualize the presence of particular proteins to identify specific\ncell types in a given sample. Cell analysis involves a wide range\nof tasks, such as counting cells and measuring and evaluating cell\nstate (e.g., shape, motility), cell health, and cell growth. Cell biology\nis closely intertwined with other fields, such as neuroscience, genet-\nics, and molecular biology. One fascinating application area of cell\nbiology is research for the potential diagnosis and treatment of dis-\neases. The research in this area is full of potential and possibilities\nthat could improve quality of life.\nDeep Neural Networks (DNNs) have been applied in the analysis\nof microscopic cell images, including cell counting [26, 35], segmen-\ntation [1, 10, 11, 23], and detection [9, 12, 34]. Given an input image,\ncell counting provides the number of cells in the image. In contrast,\ncell segmentation finds the contours of individual cells, separating\nthem from each other and the background. On the other hand, cell\ndetection localizes a cell by drawing the smallest rectangle around\neach cell in the input image. The advantages of DNNs over tra-\nditional machine learning methods are that DNNs automatically\nextract important properties (features) of the object of interest and\nuse them to perform the intended task. However, the major draw-\nback of DNNs is that it requires a large high-quality labeled dataset\nfor accurate predictions. Existing DNN methods for cell counting\ncan be broadly categorized into two groups: detection-based and\nregression-based categories.\nThe detection-based category undertakes the counting task by\nfirst detecting individual cells (contours, bounding boxes, or cen-\ntroids of the cells) in a given image and counting the detected\ncells to obtain the final cell count [14, 23]. These methods hinge\non the availability of the annotated ground truth of the bounding\nbox or a centroid of a cell. The methods are also dependent on\nthe characteristics of the microscopic input images. In particular,\ndetection-based methods fail to offer good performance when there\nis a high occlusion in the images. The regression-based category\n[26, 35] predicts the cell count without detecting individual cells.\nSome of these methods use only the ground truth cell count for each\ntraining image for training. Other methods predict a corresponding\ndensity map for a given image and obtain the final count from the\npredicted density map.\nOur team examines cellular images taken after electrical stimu-\nlation experiments on stem cells for cell differentiation. Cell differ-\nentiation is the process in which an unspecialized cell develops and\nmatures to become a specialized cell. Electrical stimulation of stem\ncells is potentially useful for stem cell therapy in patients with nerve\narXiv:2411.08992v2  [eess.IV]  19 Nov 2024\n\nMohammed et al.\ninjuries. Cell counting is an important step toward determining an\nappropriate amount of electrical voltage and stimulation duration\nto be applied. To perform the electrical stimulation, cells are placed\non the surface of a scaffold, which are structures providing support\nfor cells to grow within an interdigitated electrode region. Then the\nvoltage is applied to the electrode pads of the scaffold, which are\nstructures providing support for cells to grow. During an electrical\nstimulation experiment, cells exhibit changes in size, shape, and\nenergy requirement [6, 32, 33]. Following electrical stimulation,\nimmunocytochemistry (ICC) is performed to measure the effect of\nthe stimulation on the cells. Different antibodies are used during\nthe ICC process to identify the potential cell types these cells could\nbe differentiating into. A fluorescent microscope is used to examine\nand image the cells. Currently, cell counting and cell analysis are\ndone manually. The challenges for developing accurate automated\ncell counting are a wide range of cells in an image given different\nantibodies, different cell sizes, low contrast, and cell occlusion.\nThe main contributions of this work are as follows.\n(1) An annotated dataset for automated cell counting along\nwith the domain knowledge to use the dataset. The anno-\ntation includes the cell locations as well as the count of\ncells per image. To the best of our knowledge, there is no\nannotated fluorescent microscopic cell image dataset that\ncovers as many staining methods as this dataset.\n(2) Performance comparison of the state-of-the-art regression-\nbased and density map estimation DNN methods. The re-\nsults can be used as baseline results for future improvement.\nThe source code and the trained models are available pub-\nlicly at https://github.com/ISU-NRT-D4/cell-analysis.\nThe rest of the paper is organized as follows. In Section 2, we\nprovide a summary of existing datasets related to cell counting. Sec-\ntion 3 presents our data collection and annotation process and the\ndetails of our new dataset. Section 4 includes applicable scenarios\nto utilize the dataset. Section 5 details the baseline experimental\nresults on the dataset with five DNN models. Finally, we provide a\nconclusion and description of the future work in Section 6.\n2\nEXISTING DATASETS\nSeveral cellular image datasets are available publicly. Some datasets\nare for detection based methods [7, 16, 21, 23, 24]. These datasets,\nconsisting of either contour or smallest bounding box annotations\nof individual cells, are suitable for cell counting tasks. A few datasets\nare specifically intended for cell counting [13, 18, 22, 31]. Table 1\nsummarizes the existing datasets for cell counting for different types\nof cells. Lempitsky and Zisserman provided a synthetic dataset of\nRGB images of bacterial cells from fluorescence microscopy [18].\nThis dataset is widely used for training machine learning models\nfor cell counting. Kainz et al. [13] introduced a dataset of bright-\nfield microscopic bone marrow cell images. These are RGB images\nwith inhomogeneous backgrounds. The dataset described in [22] is\ncomprised of histology RGB images of human embryonic cells. The\nimages in this dataset have noisy backgrounds and a large variance\nin the number of cells in the images. To the best of our knowledge,\nthe proposed dataset is the only dataset that contains images of\nAdult Hippocampal Progenitor Cells with different antibodies for\nstaining.\nTable 1: Datasets for Cell Counting\nDataset\nType of cell\nNo.\nof\nimages\nResolution\nExisting\nSynthetic Bacterial\nCells [18]\nBacteria\n200\n256 x 256\nBone Marrow [13]\nBone Marrow\n40\n60 x 60\nColorectal\nCancer\nCells [31]\nColorectal\nCancer\nCells\n100\n500 x 500\nhESCs [22]\nHuman Embryonic\nStem Cells\n49\n512 x 512\nProposed\nIDCIA\nAdult Hippocampal\nProgenitor Cells\n262\n800 x 600\nCompared to thousands of images in the public datasets for seg-\nmentation and detection of generic objects (e.g., Microsoft COCO\n[20], PASCAL VOC [8], and CityScapes [5]), each of the datasets in\nTable 1 has much fewer images. Moreover, there is no dataset that\nincorporates images from more than five antibody staining meth-\nods, as well as additional information about the antibody used per\nimage. Having more public datasets and ground truth is desirable\nfor automated cell image analysis.\n3\nIDCIA: PROPOSED IMAGE DATASET\nWe describe the data collection process and annotation process and\nthe structure of the dataset.\n3.1\nData Collection\nOur dataset contains images of rat Adult Hippocampal Progenitor\nCells (AHPCs) [15] after electrical stimulation experiments and ICC.\nAHPCs have the potential to differentiate into the three primary cell\ntypes of the central nervous system in vitro: Neurons, Astrocytes,\nand Oligodendrocytes. The experiments were performed in the\nSakaguchi Lab1 at Iowa State University. The cells were generously\ngifted by Dr. Fred H Gage2. The experiments started by placing\n400,000 cells onto scaffolds containing graphene-based interdigi-\ntated electrode circuits [32]. Fig. 1 shows a picture of 3D-printed\npolylactic acid (PLA) scaffolds used in the experiments. PLA is a\nbiocompatible and biodegradable polymer making it an ideal sub-\nstrate for supporting cell growth. The scaffolds were, in turn, placed\ninside 60mm dishes containing cell culture media. During the stim-\nulation, the scaffolds were removed from the dish and covered with\na small volume of media, and the electrode pads of the scaffolds\nwere connected to a power supply with a desired voltage.\nFor this dataset, the cultured cells underwent electrical stimu-\nlation at 125 mV for 10, 15, or 20-minute durations, once a day\nfor a period of 7 days. An additional scaffold was set aside as a\ncontrol and received no stimulation. After the 7-day period, ICC\nwas performed on both the stimulated and non-stimulated samples\nto evaluate various neural differentiation markers. The process oc-\ncurred as follows. The electrode pad regions of the scaffolds were\n1https://faculty.sites.iastate.edu/dssakagu/\n2https://gage.salk.edu/\n\nIDCIA: Immunocytochemistry Dataset for Cellular Image Analysis\nFigure 1: Scaffold for cells undergoing an electrical stimula-\ntion. The red box indicates the cell culture region.\nTable 2: Seven primary antibodies used in the experiments\nAntibody\nCell Type Identification\nDAPI\nCell Nuclei\nTuJ1\nImmature Neurons\nMAP2ab\nMaturing Neurons\nRIP\nOligodendrocytes\nGFAP\nAstrocytes\nNestin\nNeural stem cells\nKi67\nProliferating Cells\ndiscarded, and then each scaffold was cut into six pieces using\nsterile scissors. The cut pieces then underwent a process of ICC\nthat involved a fixation process, repeated rinsing, and incubation\nin primary and secondary antibodies. Primary antibodies bind to\nantigens, whereas fluorophore-conjugated secondary antibodies\nbind to a primary antibody to allow for the indirect detection of the\ntarget protein. Cell nuclei were stained with DAPI. Table 2 shows\nthe seven primary antibodies used during the experiments. Once\nthis process was complete, the pieces were mounted onto micro-\nscope slides for fluorescence imaging with an upright fluorescence\nmicroscope (Nikon Microphot FXA) for visualization. Imaging of\nthe cells was made with a 20x objective, and images were captured\nwith a CCD camera. Fig. 2(A-C) shows pseudo-colorized examples\nof the cells.\nImaging and counting of cells of the microscope images were\nconducted blind. That is, the individuals conducting these processes\nwere not informed which scaffolds underwent which stimulation\ncondition. This was done to limit the amount of bias when collecting\nthe results of the experiments.\nFollowing the imaging of the samples, manual annotation was\nperformed by a group of undergraduate students led by a graduate\nstudent with more than three years of experience in cellular image\nanalysis. The ImageJ [28] Cell Counter tool was used to annotate\na cell by manually placing a dot on each cell in an input image.\nThe tool reports the total number of dots in the image. ImageJ is a\npowerful tool for processing and analyzing scientific images. The\ntool also allows the measurement of various cell properties, such\nas size, shape, and intensity.\nTable 3: Statistics of IDCIA: 800x600 image resolution; range\nof the number of cells per image [0, 581]\nAntibody\nNo. of images\nMean cell count per image\n¬± std\nDAPI\n119\n141.35 ¬± 122.56\nTuJ1\n25\n17.48 ¬± 18.34\nMAP2ab\n24\n49.71 ¬± 35.07\nRIP\n24\n49.542 ¬± 45.65\nGFAP\n23\n3.43 ¬± 5.17\nNestin\n23\n89.35 ¬± 79.70\nKi67\n24\n8.17 ¬± 8.38\nTotal\n262\n83.857¬±104.42\nWe used dot annotations to label the cells for counting purposes.\nFirst, dot annotation enables the accurate marking of individual\ncells. This is to avoid double counting some cells or missing to\ncount other cells for images with a large number of cells or with\ndensely packed or overlapping cells. Second, dot annotation is a\nfast and efficient way to count and identify cells since the exact cell\ncontour or bounding box is not required. It is useful for expanding\nthe dataset in the future. Third, dot annotations can be used to verify\nhow a DNN model arrives at the predicted cell count, which should\nimprove cell biologists‚Äô trust in the model. Finally, dot annotations\nare useful for the development and evaluation of both detection-\nbased and regression-based DNN methods for cell counting.\n3.2\nDataset Structure and Details\nAfter the completion of annotating all the images, we split the\ndataset into three non-overlapping sets: Training, Validation, and\nTesting at the ratio of 60:20:20. To ensure that each of these sets\ncontains a proportional number of samples from each antibody\ntype, we used stratified sampling based on antibody type. We then\nran a program to extract the coordinates of individual dots from\na dot-annotated image by thresholding the color of the dot and\nsaving the coordinates in a csv file.\nTable 3 presents statistics about the IDCIA dataset of 262 images\nwith 84 cells on average per image. Notice that the cells are not\nevenly distributed across antibodies. Images from DAPI staining\nto identify cell nuclei has the most cells. The dataset has a high\nvariance in terms of the number of cells per image. This dataset\nbrings an interesting and challenging problem due to the variability\nintroduced by the use of multiple antibodies in immunolabeling\nexperiments. Each antibody interacts differently with the cells,\nresulting in a wide range of appearances and visual characteristics\nwithin the same sample. This can make it difficult to accurately\nand reliably count the number of cells present. Additionally, the\npresence of multiple antibody labels may also result in overlap\nbetween cells, further complicating the cell counting process.\nThe provided dataset has two directories: images and ground_truth,\nand the readme.md file for the description of the dataset folder. The\nimages directory contains seven sub-directories, one for each of\nthe primary antibodies used in the experiments. Inside each di-\nrectory are grayscale images from fluorescence microscopy. The\nimages were resized to 800x600 pixels. The images in Fig. 2 were\npseudo-colored for better visualization. All the images are named\n\nMohammed et al.\nusing a consistent format that gives information about the antibody,\nobjective, cell type, and field number. Similarly, the ground truth\ndirectory contains seven directories under it. Inside these directo-\nries are .csv files containing coordinates of the dot-annotated cells\nfor each image in the images directory. For the reproducibility of\nexperiments, we provide .csv files that list the names of the images\nin the training, validation, and testing sets.\n4\nUSAGE SCENARIOS OF IDCIA\nThe dot-annotated microscopic cell images and accompanying im-\nmunolabeling and staining information provided in this dataset of-\nfer opportunities for computer scientists to contribute to advancing\ncell biology research. Here, we outline a few potential applications.\nThe dot annotations with the number and location of cells marked\nfor each image are useful for developing effective and interpretable\ncounting methods, as outlined in Section 2. As cell counting is an\nimportant task in cell analysis, desirable automated methods should\nproduce the predicted count within the experts‚Äô acceptable error\nrate of, at most 5% difference from the actual cell count. If auto-\nmated cell counting is quick, accurate, and trustworthy, electrical\nstimulation experiments for stem cell therapy can be accelerated.\nDue to a high variance in the number of cells labeled for each\nstaining antibody, the name of the antibody used may be useful\nfor improving automated cell counting methods. For instance, the\nDAPI staining of cell nuclei identifies many more cells than the\nimmunolabeling with the antibodies. This dataset includes images\nfrom the experiments covering the use of seven primary antibodies\nfor immunolabeling. On the contrary, DNN methods may be devel-\noped to classify images to predict the antibody used for cell labeling.\nWe refer to this problem as antibody classification problem.\nGiven the limited number of datasets for cellular image anal-\nysis and the limited number of images for each dataset, more\nhigh-quality datasets are needed. Our dataset supplements exist-\ning datasets and may be useful for transfer learning that extracts\ninformation from data in one domain and transfers the learned\nknowledge to another domain [17, 29].\nBoth the cell count and the locations of individual cells are useful\nfor developing cell segmentation or cell detection methods based on\nhigh-level labeling (i.e., weak supervision). Dot annotations, which\nare faster to acquire than detailed annotations, can be leveraged\nto obtain more detailed annotations and reduce manual labeling\ntime. Utilizing ground truth information obtained through weak\nsupervision can help in the automated segmentation and detection\nof individual cells. This allows the measurement of cell shape and\norientation. Currently, proprietary software such as MetaXpress\nexists for measuring cell shape and cell orientation. However, the\nsoftware requires that cell culture be done on a smooth surface,\nwhich greatly limits the opportunities for using custom-designed\nscaffolds for electrical stimulations, as shown in Fig. 1.\n4.1\nSuggested Metrics\nSuggested metrics for the cell counting task are Mean Absolute\nError (MAE) [2] and Root Mean Squared Error (RMSE) [2]. MAE\nis the average of the absolute difference between the label ground\ntruth count ùë¶ùëñand the predicted value count ÀÜùë¶ùëñfor all n images\nin a given dataset. RMSE penalizes large errors to a greater extent\ncompared to MAE. See Equations 1-2. We introduce Acceptable\nError Count Percent (ACP) to measure the percentage of images\nwhose predicted count is within a 5% difference from the true count\nby the domain expert, as shown in Equation 3. We use Iverson\nbrackets ‚ü¶.‚üßto denote a function that returns 1 if the condition is\nsatisfied or 0 otherwise. These metrics are calculated below. Due to\nlimited space, we only report MAE and ACP in this paper.\nùëÄùê¥ùê∏=\n\u0012 1\nùëõ\n\u0013 ùëõ\n‚àëÔ∏Å\nùëñ=1\n| ÀÜùë¶ùëñ‚àíùë¶ùëñ|\n(1)\nùëÖùëÄùëÜùê∏=\nv\nt\u0012 1\nùëõ\n\u0013 ùëõ\n‚àëÔ∏Å\nùëñ=1\n( ÀÜùë¶i ‚àíùë¶ùëñ)2\n(2)\nùê¥ùê∂ùëÉ=\n\u0012 1\nùëõ\n\u0013\n‚àó100\nùëõ\n‚àëÔ∏Å\nùëñ=1\n‚ü¶| ÀÜùë¶ùëñ‚àíùë¶ùëñ| ‚â§0.05 ‚àóùë¶ùëñ‚üß\n(3)\nThe lower the values of MAE, the more accurate a model‚Äôs pre-\ndictions are. On the other hand, a higher ACP indicates that more\npredictions are in the acceptable margin. For the antibody classifi-\ncation problem, traditional performance metrics for classification\nproblems such as accuracy, precision, recall, and F1-score can be\nused [2].\n5\nBASELINE EXPERIMENTS FOR CELL\nCOUNTING\nWe evaluated five different models for cell counting using the ID-\nCIA dataset. They are a Convolutional Neural Network (CNN) with\nregression output (CNN Regression), two-crowd counting meth-\nods (CSRNet [19], MCNN [36]), and two cell-counting methods\n(Count-ception [26], FCRN [35]). All these four methods are based\non density map estimation. For CSRNet and Count-ception, we used\nthe source code provided by the original authors of the methods.\nOur CNN Regression model has a pre-trained VGG16 [30] network\nwith a fully connected layer at the end of it, followed by one output\nneuron for the predicted cell count for a given image. We excluded\n[14, 23] from our experiments since they require detailed annota-\ntions for training. All models were implemented in Python using\nthe PyTorch [25] library and trained on NVIDIA Tesla T4 and P2000\nGPUs.\nCount-ception and FCRN were developed for cell counting. Count-\nception is a network of fully convolutional layers without any pool-\ning layer. This is to avoid losing pixel information and to ease the\ncalculation of the receptive field. Given an input image, Count-\nception produces an intermediate count map. Each network inside\nit counts the number of objects in its receptive field. FCRN uses\nCNN to regress a cell‚Äôs spatial density across an image. It first maps\nthe input image to feature maps with dense representation and then\nrecovers the spatial span by bilinear up-sampling. FCRN allows\nprediction for input with an arbitrary size. FCRN-A is a version of\nFCRN that uses small 3 √ó 3 kernels for every convolutional layer,\nand each convolutional layer is followed by a pooling layer.\nCSRNet [19] and MCNN [36] are density-map based models de-\nveloped for counting people in a congested environment. These\nmodels can handle dense crowds, which makes them well-suited for\nhandling cell congestion. In addition, these models were designed\nto be robust to variations in object size and shape, lighting, and\n\nIDCIA: Immunocytochemistry Dataset for Cellular Image Analysis\nFigure 2: Quantification of immature neurons in AHPCs after 7 days in vitro (DIV) of electrical stimulation. Row 1 (A-C) shows\nfluorescence images of AHPCs labeled with an immature neuron marker (TuJ1, red; A and C) and nuclei marker (DAPI, blue; B\nand C) following 15 min. of 125 mV electrical stimulation once a day for 7 days. Row 2 (D-E) shows the dot-annotated images of\nthe TuJ1 (D) and DAPI (E) staining by using the ImageJ Cell Counter tool to put a pink dot on a cell to be counted. Scale bar = 50\n¬µm. Images have been pseudo-colorized for better visualization.\ncontrast conditions. CSRNet is a two-component network with a\nCNN as the first component for feature extraction. The second com-\nponent is a dilated CNN to produce larger reception fields, replacing\npooling operations. MCNN extracts scale-relevant features by using\nfilters with different sizes of receptive fields. The authors proposed\na network of three parallel CNNs with different filter sizes. For an\ninput image, the network averages the predicted density maps of\nthe three CNNs and outputs a final count prediction. To use the\ndot-annotated images for training CSRNet and MCNN, we followed\nthe ground truth generation method in [36] by blurring each dot\nannotation using a Gaussian kernel to produce corresponding den-\nsity maps. Since the generated density maps have a high impact on\nthe performance of the models for cell counting, we used geometry-\nadaptive kernels [19] to accurately generate corresponding density\nmaps for input images. Fig. 3 (left) shows input images. Fig. 3 (right)\nshows the corresponding density maps generated.\nAll models were then trained using an end-to-end stochastic\ngradient descent method [27] and data augmentations per the orig-\ninal authors‚Äô code. The loss function used in training was PyTorch\nL1Loss. The grid search method was conducted to obtain the best\nhyperparameter values on the validation dataset. For each method,\nwe performed five runs on the IDCIA dataset. Each run involved\ntraining the model using the hyperparameter values that give the\nbest MAE on the validation dataset. The average of the MAEs over\nthe five runs was reported for each model.\nFigure 3: Images (left) and generated density maps (right)\nThe optimal learning rate, batch size, and the number of epochs,\nas determined by the grid search method, are given for each count-\ning method. The results are indicated in Table 4. Count-ception\nis the best method giving the lowest MAE. CSRNet and MCNN\nperform comparably despite being originally proposed for crowd-\ncounting tasks. CNN Regression and FCRN-A are the two worst\n\nMohammed et al.\nTable 4: Avg. MAEs of the five methods on the IDCIA test\ndataset\nModel\nLearning rate\nBatch\nSize\nEpochs\nAvg.\nMAE\nCNN Regression\n3.5e-05\n16\n500\n24.15\nCSRNet [19]\n2e-7\n1\n400\n18.64\nMCNN [36]\n1e-7\n1\n1500\n19.57\nCount-ception [4]\n1e-2\n2\n1000\n15.47\nFCRN-A [35]\n1e-2\n16\n1000\n27.49\nTable 5: Comparison of ACP of the five methods\nModel\nACP%\nCNN Regression\n9%\nCSRNet\n17%\nMCNN\n0%\nCountception\n9%\nFCRN_A\n9%\nmethods. FCRN-A gives the worst performance on IDCIA, although\nit was proposed for cell counting. FCRN-A uses small 3 √ó 3 ker-\nnels for every convolutional layer, and each convolutional layer is\nfollowed by a pooling layer.\nAccording to Table 5, the CSRNet model has the highest ACP\nat 17%, which indicates that it has the best performance. On the\nother hand, the MCNN model has the lowest ACP at 0%, indicating\npoor performance in this aspect, even though it has comparable\nperformance under the mean absolute error (MAE) metric. The\nCNN regression, Count-ception, and FCRN-A models have the same\nACP at 9%, indicating similar performance. While MCNN performs\ndecently in terms of MAE, it performs the worst based on ACP,\nwhich is an important criterion for domain experts.\nBreaking down the performance of the models with respect to\nthe different antibody labeling in Table 6, Count-ception performs\nbetter on most of the staining types, while the simple CNN Regres-\nsion model exhibits the best performance in two of the staining\nmethods. CSRNet performs best for the DAPI labeled samples, while\nMCNN performs best for the Ki67 labeled samples. Such perfor-\nmance differences are caused by the different visual appearances\nof cells under different staining antibodies. It indicates the need to\nbuild models that can produce an accurate prediction for a given\nimage under a certain antibody labeling. Count-ception has the\nlowest MAE on the TuJ1, MAP2ab, and Nestin immunolabeled im-\nages. Count-ception is the second best for the rest of the antibody\nlabels. CNN Regression has the lowest MAE on the RIP and GFAP\nlabeled images.\nOverall, our results demonstrate the potential of DNNs for count-\ning cells from microscope images. IDCIA has a high variance in\nterms of cell count for different antibody labels. In our experiments,\nCount-ception undercounts cells in some images but overcounts in\nothers. It undercounts all the DAPI labeled images in the test dataset.\nThis highlights the importance of evaluating the performance of\ndifferent models on different immunolabeling antibodies.\nTable 6: Avg. MAE per antibody labeling on the IDCIA test\ndataset\nStaining\nAvg. MAE by different methods\nCNN Reg.\nCSRNet MCNN\nCount-\nception\nFCRN_A\nDAPI\n25.94\n13.29\n29.84\n15.8\n21.82\nTuJ1\n16.98\n7\n4.66\n3.11\n11.23\nMAP2ab\n37.72\n41.4\n46.59\n25.2\n38.13\nRIP\n14.43\n15.2\n23.42\n16.9\n64.43\nGFAP\n36.13\n75.4\n37.15\n38.59\n58.67\nNestin\n19.05\n15.5\n9.08\n6.69\n8.41\nKi67\n16.32\n7.4\n1.11\n2.18\n7.56\n6\nCONCLUSION AND FUTURE WORK\nIn this paper, we present a new annotated dataset of images of cells\nfrom a fluorescence microscope. The cells were immunolabeled\nusing a panel of cell type-specific antibody markers, and all cell\nnuclei stained using DAPI. The dataset is available for public use\nalong with the source code and the trained models. We present\nthe effectiveness of deep-learning methods for counting on the\ndataset. We found that different existing deep-learning models are\nbest for different antibodies used for labeling. All the methods still\nunderperform when using the ACP metric based on the domain\nexperts, leaving room for improvement. The results of our study\nhighlight the challenges in accurately predicting cell counts. We\nplan to continue to explore different architectures and training\ntechniques in order to increase the performance on the ACP metric.\nFuture work includes the development of a new DNN method that\nis sufficiently accurate and acceptable by domain experts.\nACKNOWLEDGMENTS\nWe thank all the undergraduate students, John Swanson, Gabrielle\nSawin, and Anna Garbe, who participated in the imaging and anno-\ntation of the fluorescence microscopy images. This work is partially\nsupported by the NSF Grant No. 2152117. Findings, opinions, and\nconclusions expressed in this paper do not necessarily reflect the\nview of the funding agency.\nREFERENCES\n[1] Yousef Al-Kofahi, Alla Zaltsman, Robert Graves, Will Marshall, and Mirabela\nRusu. 2018. A deep learning-based algorithm for 2-D cell segmentation in\nmicroscopy images. BMC Bioinformatics 19, 1 (Oct. 2018), 365. https://doi.org/\n10.1186/s12859-018-2375-z\n[2] Christopher M Bishop. 2006.\nPattern recognition and machine learning.\nNew York : Springer, [2006] ¬©2006.\nhttps://search.library.wisc.edu/catalog/\n9910032530902121\n[3] R. A. Bradshaw and P. D. Stahl. 2016. Cell Biology: An Overview. In Refer-\nence Module in Biomedical Sciences. Elsevier. https://doi.org/10.1016/B978-0-12-\n801238-3.99496-0\n[4] Joseph Paul Cohen, Genevi√®ve Boucher, Craig A. Glastonbury, Henry Z. Lo,\nand Yoshua Bengio. 2017. Count-ception: Counting by Fully Convolutional\nRedundant Counting. In 2017 IEEE International Conference on Computer Vision\nWorkshops (ICCVW). 18‚Äì26. https://doi.org/10.1109/ICCVW.2017.9\n[5] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus En-\nzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. 2016.\nThe Cityscapes Dataset for Semantic Urban Scene Understanding.\nhttps:\n//doi.org/10.48550/arXiv.1604.01685\n[6] Suprem R. Das, Metin Uz, Shaowei Ding, Matthew T. Lentner, John A. Hondred,\nAllison A. Cargill, Donald S. Sakaguchi, Surya Mallapragada, and Jonathan C.\nClaussen. 2017.\nElectrical Differentiation of Mesenchymal Stem Cells into\n\nIDCIA: Immunocytochemistry Dataset for Cellular Image Analysis\nSchwann-Cell-Like Phenotypes Using Inkjet-Printed Graphene Circuits. Ad-\nvanced Healthcare Materials 6, 7 (2017), 1601087. https://doi.org/10.1002/adhm.\n201601087\n[7] Christoffer Edlund, Timothy R. Jackson, Nabeel Khalid, Nicola Bevan, Timothy\nDale, Andreas Dengel, Sheraz Ahmed, Johan Trygg, and Rickard Sj√∂gren. 2021.\nLIVECell‚ÄîA large-scale dataset for label-free live cell segmentation. Nature\nMethods 18, 9 (Sept. 2021), 1038‚Äì1045. https://doi.org/10.1038/s41592-021-01249-\n6 Number: 9 Publisher: Nature Publishing Group.\n[8] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. 2010.\nThe Pascal Visual Object Classes (VOC) Challenge. International Journal of\nComputer Vision 88, 2 (June 2010), 303‚Äì338.\n[9] Seiya Fujita and Xian-Hua Han. 2020. Cell Detection and Segmentation in\nMicroscopy Images with Improved Mask R-CNN. In Proceedings of the Asian\nConference on Computer Vision (ACCV) Workshops.\n[10] Ali Ghaznavi, Renata Rychtarikova, Mohammadmehdi Saberioon, and Dalibor\nStys. 2022. Cell segmentation from telecentric bright-field transmitted light\nmicroscopy images using a Residual Attention U-Net: a case study on HeLa line.\nComputers in Biology and Medicine 147 (Aug. 2022), 105805. https://doi.org/10.\n1016/j.compbiomed.2022.105805 arXiv:2203.12290 [cs, eess, q-bio].\n[11] Yuki Hiramatsu, Kazuhiro Hotta, Ayako Imanishi, Michiyuki Matsuda, and Kenta\nTerai. 2018. Cell Image Segmentation by Integrating Multiple CNNs. In 2018\nIEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops\n(CVPRW). 2286‚Äì22866. https://doi.org/10.1109/CVPRW.2018.00296 ISSN: 2160-\n7516.\n[12] Hao Jiang, Sen Li, Weihuang Liu, Hongjin Zheng, Jinghao Liu, and Yang Zhang.\n2020. Geometry-Aware Cell Detection with Deep Learning. mSystems 5, 1\n(2020), 10.1128/msystems.00840‚Äì19. https://doi.org/10.1128/msystems.00840-19\narXiv:https://journals.asm.org/doi/pdf/10.1128/msystems.00840-19\n[13] Philipp Kainz, Martin Urschler, Samuel Schulter, Paul Wohlhart, and Vincent\nLepetit. 2015. You Should Use Regression to Detect Cells. In Medical Image\nComputing and Computer-Assisted Intervention ‚Äì MICCAI 2015 (Lecture Notes\nin Computer Science), Nassir Navab, Joachim Hornegger, William M. Wells, and\nAlejandro F. Frangi (Eds.). Springer International Publishing, Cham, 276‚Äì283.\nhttps://doi.org/10.1007/978-3-319-24574-4_33\n[14] Aisha Khan, Stephen Gould, and Mathieu Salzmann. 2016. Deep Convolutional\nNeural Networks for Human Embryonic Cell Counting. In Computer Vision\n‚Äì ECCV 2016 Workshops (Lecture Notes in Computer Science), Gang Hua and\nHerv√© J√©gou (Eds.). Springer International Publishing, Cham, 339‚Äì348. https:\n//doi.org/10.1007/978-3-319-46604-0_25\n[15] Elizabeth D. Kirby, Akela A. Kuwahara, Reanna L. Messer, and Tony Wyss-\nCoray. 2015. Adult hippocampal neural stem and progenitor cells regulate the\nneurogenic niche by secreting VEGF. Proceedings of the National Academy\nof Sciences of the United States of America 112, 13 (March 2015), 4128‚Äì4133.\nhttps://doi.org/10.1073/pnas.1422448112\n[16] Florian Kromp, Eva Bozsaky, Fikret Rifatbegovic, Lukas Fischer, Magdalena\nAmbros, Maria Berneder, Tamara Weiss, Daria Lazic, Wolfgang D√∂rr, Allan\nHanbury, Klaus Beiske, Peter F. Ambros, Inge M. Ambros, and Sabine Taschner-\nMandl. 2020. An annotated fluorescence image dataset for training nuclear\nsegmentation methods. Scientific Data 7, 1 (Aug. 2020), 262. https://doi.org/10.\n1038/s41597-020-00608-w\n[17] Falko Lavitt, Demi J. Rijlaarsdam, Dennet van der Linden, Ewelina Weglarz-\nTomczak, and Jakub M. Tomczak. 2021. Deep Learning and Transfer Learning\nfor Automatic Cell Counting in Microscope Images of Human Cancer Cell Lines.\nApplied Sciences 11, 11 (Jan. 2021), 4912. https://doi.org/10.3390/app11114912\n[18] Victor Lempitsky and Andrew Zisserman. 2010.\nLearning To Count Ob-\njects in Images. In Advances in Neural Information Processing Systems,\nVol. 23. Curran Associates, Inc.\nhttps://papers.nips.cc/paper/2010/hash/\nfe73f687e5bc5280214e0486b273a5f9-Abstract.html\n[19] Yuhong Li, Xiaofan Zhang, and Deming Chen. 2018. CSRNet: Dilated Convolu-\ntional Neural Networks for Understanding the Highly Congested Scenes. In 2018\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 1091‚Äì1100.\nhttps://doi.org/10.1109/CVPR.2018.00120 ISSN: 2575-7075.\n[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick,\nJames Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll√°r.\n2015. Microsoft COCO: Common Objects in Context. https://doi.org/10.48550/\narXiv.1405.0312\n[21] Vebjorn Ljosa, Katherine L. Sokolnicki, and Anne E. Carpenter. 2012. Annotated\nhigh-throughput microscopy image sets for validation. Nature Methods 9, 7\n(July 2012), 637‚Äì637. https://doi.org/10.1038/nmeth.2083 Number: 7 Publisher:\nNature Publishing Group.\n[22] Kyaw Thu Minn, Yuheng C. Fu, Shenghua He, Sabine Dietmann, Steven C.\nGeorge, Mark A. Anastasio, Samantha A. Morris, and Lilianna Solnica-Krezel.\n2020. High-resolution transcriptional and morphogenetic profiling of cells from\nmicropatterned human ESC gastruloid cultures. eLife 9 (Nov. 2020), e59445.\nhttps://doi.org/10.7554/eLife.59445\n[23] Roberto Morelli, Luca Clissa, Roberto Amici, Matteo Cerri, Timna Hitrec, Marco\nLuppi, Lorenzo Rinaldi, Fabio Squarcio, and Antonio Zoccoli. 2021. Automating\ncell counting in fluorescent microscopy through deep learning with c-ResUnet.\nScientific Reports 11, 1 (Nov. 2021), 22920. https://doi.org/10.1038/s41598-021-\n01929-5 Number: 1 Publisher: Nature Publishing Group.\n[24] Sapun Parekh and Schwendy Mischa. 2022. EVICAN Dataset. https://doi.org/\n10.17617/3.AJBV1S Type: dataset.\n[25] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban\nDesmaison, Andreas K√∂pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\nChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning\nLibrary. https://doi.org/10.48550/arXiv.1912.01703 arXiv:1912.01703 [cs, stat].\n[26] Joseph Paul Cohen, Genevieve Boucher, Craig A. Glastonbury, Henry Z.\nLo, and Yoshua Bengio. 2017.\nCount-ception: Counting by Fully\nConvolutional\nRedundant\nCounting.\n18‚Äì26.\nhttps://openaccess.\nthecvf.com/content_ICCV_2017_workshops/w1/html/Cohen_Count-\nception_Counting_by_ICCV_2017_paper.html\n[27] Sebastian Ruder. 2017. An overview of gradient descent optimization algorithms.\nhttps://doi.org/10.48550/arXiv.1609.04747\n[28] Caroline A. Schneider, Wayne S. Rasband, and Kevin W. Eliceiri. 2012. NIH Image\nto ImageJ: 25 years of image analysis. Nature Methods 9, 7 (July 2012), 671‚Äì675.\nhttps://doi.org/10.1038/nmeth.2089 Number: 7 Publisher: Nature Publishing\nGroup.\n[29] Ling Shao, Fan Zhu, and Xuelong Li. 2015. Transfer learning for visual catego-\nrization: a survey. IEEE transactions on neural networks and learning systems 26,\n5 (May 2015), 1019‚Äì1034. https://doi.org/10.1109/TNNLS.2014.2330900\n[30] Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Net-\nworks for Large-Scale Image Recognition. https://doi.org/10.48550/arXiv.1409.\n1556 arXiv:1409.1556 [cs].\n[31] Korsuk Sirinukunwattana, Shan E Ahmed Raza, Yee-Wah Tsang, David R. J.\nSnead, Ian A. Cree, and Nasir M. Rajpoot. 2016. Locality Sensitive Deep Learning\nfor Detection and Classification of Nuclei in Routine Colon Cancer Histology\nImages. IEEE Transactions on Medical Imaging 35, 5 (May 2016), 1196‚Äì1206.\nhttps://doi.org/10.1109/TMI.2016.2525803\n[32] Metin Uz, Maxsam Donta, Meryem Mededovic, Donald S. Sakaguchi, and\nSurya K. Mallapragada. 2019. Development of Gelatin and Graphene-Based\nNerve Regeneration Conduits Using Three-Dimensional (3D) Printing Strate-\ngies for Electrical Transdifferentiation of Mesenchymal Stem Cells.\nIndus-\ntrial & Engineering Chemistry Research 58, 18 (May 2019), 7421‚Äì7427.\nhttps:\n//doi.org/10.1021/acs.iecr.8b05537\n[33] Metin Uz, John A. Hondred, Maxsam Donta, Juhyung Jung, Emily Kozik, Jonathan\nGreen, Elizabeth J. Sandquist, Donald S. Sakaguchi, Jonathan C. Claussen, and\nSurya Mallapragada. 2020. Determination of Electrical Stimuli Parameters To\nTransdifferentiate Genetically Engineered Mesenchymal Stem Cells into Neu-\nronal or Glial Lineages. Regenerative Engineering and Translational Medicine 6, 1\n(March 2020), 18‚Äì28. https://doi.org/10.1007/s40883-019-00126-1\n[34] Ching-Wei Wang, Sheng-Chuan Huang, Yu-Ching Lee, Yu-Jie Shen, Shwu-Ing\nMeng, and Jeff L. Gaol. 2022. Deep learning for bone marrow cell detection and\nclassification on whole-slide images. Medical Image Analysis 75 (2022), 102270.\nhttps://doi.org/10.1016/j.media.2021.102270\n[35] Weidi Xie, J. Alison Noble, and Andrew Zisserman. 2018. Microscopy cell count-\ning and detection with fully convolutional regression networks. Computer\nMethods in Biomechanics and Biomedical Engineering: Imaging & Visualization 6,\n3 (May 2018), 283‚Äì292. https://doi.org/10.1080/21681163.2016.1149104\n[36] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. 2016.\nSingle-Image Crowd Counting via Multi-Column Convolutional Neural Network.\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n589‚Äì597. https://doi.org/10.1109/CVPR.2016.70 ISSN: 1063-6919.",
    "pdf_filename": "IDCIA_Immunocytochemistry_Dataset_for_Cellular_Image_Analysis.pdf"
}