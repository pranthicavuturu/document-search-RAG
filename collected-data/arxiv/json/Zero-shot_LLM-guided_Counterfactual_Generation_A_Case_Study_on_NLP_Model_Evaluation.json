{
    "title": "Zero-shot LLM-guided Counterfactual Generation A Case Study on NLP Model Evaluation",
    "abstract": "complex, black-box models for solving many natural language processing (NLP) tasks, there is also an increasing necessity of methods to stress-test these models and provide some degree of interpretability or explainability. While counterfactual examples are useful in this regard, automated generation of counterfactuals is a data and resource intensive process. such methods depend on models such as pre-trained language models that are then fine-tuned on auxiliary, often task-specific datasets, that may be infeasible to build in practice, especially for new tasks and data domains. Therefore, in this work we explore the possibility of leveraging large language models (LLMs) for zero- shot counterfactual generation in order to stress-test NLP models. We propose a structured pipeline to facilitate this generation, and we hypothesize that the instruction-following and textual understanding capabilities of recent LLMs can be effectively leveraged for generating high quality counterfactuals in a zero- shot manner, without requiring any training or fine-tuning. Through comprehensive experiments on a variety of propreitary and open-source LLMs, along with various downstream tasks in NLP, we explore the efficacy of LLMs as zero-shot counterfactual generators in evaluating and explaining black-box NLP models. Index Terms—counterfactual generation, model evaluation, explanation, explainability, large language models I. INTRODUCTION Over the last couple of decades, machine learning and natural language processing (NLP) systems have developed massively, especially in terms of the complexity and scale of the models used in different downstream tasks. For example, for most NLP tasks, such as tasks in the GLUE [1] or SuperGLUE [2] benchmarks, the state-of-the-art performance is achieved by large, black-box models such as pre-trained language models (PLM) [3]. Effective use and deployment of such models, especially in high-stakes areas, require careful evaluation, validation and stress-testing. Furthermore, models should also be explainable or interpretable, i.e., decisions made by such black-box models should ideally be accompanied by how and/or why the model reached that decision [4]. While such endeavors are still challenging in the context of black- box models, in this regard, counterfactual examples have been used to perform evaluation, explanation, robustness testing and even improvement of NLP models [5]–[7]. For example, the following two sentences - s1: This movie is brilliant!, s2: This movie is boring. are counterfactual examples for the Fig. 1. Examples of an input sentence and its corresponding counterfactual examples with same or opposite label. input sentence This movie is great. Such minimally perturbed variations of the input text can be used in a variety of settings to evaluate models, to understand whether a model is able to focus on the task-specific features in the input text in order to classify the input text correctly. While several previous works have investigated the applica- bility of human expert annotators to design such counterfactual examples [8], [9], this is not scalable in practice, thereby moti- vating the exploration of automated counterfactual generation methods. Automated counterfactual generation methods such as [5], [6] use pre-trained language models, or mask-filling, or models trained via control codes for the generation. For exam- ple, training a conditional generation model in Polyjuice [5] requires sentence-pair dataset for each control code (such as: negation, quantifier, shuffle, lexical, etc.). Similar methods requiring large amounts of training and/or task-specific data are used in other automated counterfactual generation meth- ods. However, having access to such task-specific training datasets may be infeasible in practice, especially for newly emerging data domains and tasks. Therefore, we are interested in investigating: Is there a way to simplify the counterfactual generation process and perform the generation without any auxiliary data? In order to explore this, in this work, we address a new problem and application setting: zero-shot counterfactual gen- eration for evaluating and explaining NLP models. We tackle this problem by using the power of recent state-of-the-art instruction-tuned large language models (LLMs). While recent arXiv:2405.04793v2  [cs.CL]  19 Nov 2024",
    "body": "Zero-shot LLM-guided Counterfactual Generation:\nA Case Study on NLP Model Evaluation\nAmrita Bhattacharjee\nSchool of Computing and AI\nArizona State University\nTempe, AZ, USA\nabhatt43@asu.edu\nRaha Moraffah\nDepartment of Computer Science\nWorcester Polytechnic Institute\nWorcester, MA, USA\nrmoraffah@wpi.edu\nJoshua Garland\nGlobal Security Initiative\nArizona State University\nTempe, AZ, USA\nJoshua.Garland@asu.edu\nHuan Liu\nSchool of Computing and AI\nArizona State University\nTempe, AZ, USA\nhuanliu@asu.edu\nAbstract—With the development and proliferation of large,\ncomplex, black-box models for solving many natural language\nprocessing (NLP) tasks, there is also an increasing necessity of\nmethods to stress-test these models and provide some degree of\ninterpretability or explainability. While counterfactual examples\nare useful in this regard, automated generation of counterfactuals\nis a data and resource intensive process. such methods depend\non models such as pre-trained language models that are then\nfine-tuned on auxiliary, often task-specific datasets, that may\nbe infeasible to build in practice, especially for new tasks\nand data domains. Therefore, in this work we explore the\npossibility of leveraging large language models (LLMs) for zero-\nshot counterfactual generation in order to stress-test NLP models.\nWe propose a structured pipeline to facilitate this generation,\nand we hypothesize that the instruction-following and textual\nunderstanding capabilities of recent LLMs can be effectively\nleveraged for generating high quality counterfactuals in a zero-\nshot manner, without requiring any training or fine-tuning.\nThrough comprehensive experiments on a variety of propreitary\nand open-source LLMs, along with various downstream tasks in\nNLP, we explore the efficacy of LLMs as zero-shot counterfactual\ngenerators in evaluating and explaining black-box NLP models.\nIndex Terms—counterfactual generation, model evaluation,\nexplanation, explainability, large language models\nI. INTRODUCTION\nOver the last couple of decades, machine learning and\nnatural language processing (NLP) systems have developed\nmassively, especially in terms of the complexity and scale of\nthe models used in different downstream tasks. For example,\nfor most NLP tasks, such as tasks in the GLUE [1] or\nSuperGLUE [2] benchmarks, the state-of-the-art performance\nis achieved by large, black-box models such as pre-trained\nlanguage models (PLM) [3]. Effective use and deployment of\nsuch models, especially in high-stakes areas, require careful\nevaluation, validation and stress-testing. Furthermore, models\nshould also be explainable or interpretable, i.e., decisions made\nby such black-box models should ideally be accompanied by\nhow and/or why the model reached that decision [4]. While\nsuch endeavors are still challenging in the context of black-\nbox models, in this regard, counterfactual examples have been\nused to perform evaluation, explanation, robustness testing\nand even improvement of NLP models [5]–[7]. For example,\nthe following two sentences - s1: This movie is brilliant!,\ns2: This movie is boring. are counterfactual examples for the\nFig. 1. Examples of an input sentence and its corresponding counterfactual\nexamples with same or opposite label.\ninput sentence This movie is great. Such minimally perturbed\nvariations of the input text can be used in a variety of settings\nto evaluate models, to understand whether a model is able to\nfocus on the task-specific features in the input text in order to\nclassify the input text correctly.\nWhile several previous works have investigated the applica-\nbility of human expert annotators to design such counterfactual\nexamples [8], [9], this is not scalable in practice, thereby moti-\nvating the exploration of automated counterfactual generation\nmethods. Automated counterfactual generation methods such\nas [5], [6] use pre-trained language models, or mask-filling, or\nmodels trained via control codes for the generation. For exam-\nple, training a conditional generation model in Polyjuice [5]\nrequires sentence-pair dataset for each control code (such as:\nnegation, quantifier, shuffle, lexical, etc.). Similar methods\nrequiring large amounts of training and/or task-specific data\nare used in other automated counterfactual generation meth-\nods. However, having access to such task-specific training\ndatasets may be infeasible in practice, especially for newly\nemerging data domains and tasks. Therefore, we are interested\nin investigating: Is there a way to simplify the counterfactual\ngeneration process and perform the generation without any\nauxiliary data?\nIn order to explore this, in this work, we address a new\nproblem and application setting: zero-shot counterfactual gen-\neration for evaluating and explaining NLP models. We tackle\nthis problem by using the power of recent state-of-the-art\ninstruction-tuned large language models (LLMs). While recent\narXiv:2405.04793v2  [cs.CL]  19 Nov 2024\n\nwork has started exploring the effectiveness of using LLMs\nfor generating counterfactuals, these works use additional\nguidance, such as, in the form of few-shot exemplars for in-\ncontext learning. Given that gold-standard exemplar samples\ncan be hard to come across for many tasks, alongside LLM\ncontext length issues restricting the number of in-context\nexamples, we explore the possibility of using LLMs in a\nzero-shot manner for generating counterfactuals in order to\nevaluate and explain black-box text classifiers. Given that\nrecent LLMs are trained on massive amounts of text data,\nfollowed by subsequent supervised fine-tuning and alignment\nsteps, empirical evidence suggests that such LLMs can be used\nas pseudo-oracles or general-purpose solvers especially in NLP\ntasks [10]. As an extension, we propose the paradigm of using\nLLMs as zero-shot counterfactual generators for stress-testing\ntext classifier models. To further this exploration, we propose\na pipeline that leverages recent LLMs in order to generate\nplausible, human-interpretable counterfactual examples in a\ncompletely zero-shot manner. Our proposed pipeline requires\nonly the input text along with either the ground truth label or\nthe predicted label from the black-box classifier (depending\non the use-case) and uses a structured, hard-prompting method\nto use off-the-shelf LLMs for generating the counterfactuals,\nwithout any fine-tuning or training with additional data. We\nenvision that automating the task of counterfactual generation\nvia a carefully designed pipeline that leverages LLMs can help\nto reduce costs and make NLP model development, evaluation\nand explanation more streamlined and efficient. We use our\nproposed pipeline to generate counterfactuals in order to\nexplore their effectiveness in (1) explaining and, (2) evaluating\nNLP models for a variety of downstream tasks. Our results\ndemonstrate that, when used in our pipeline, LLMs may be\neffectively used to generate effective zero-shot counterfactuals\nthat can be used for stress-testing text classifiers.\nTo the best of our knowledge, this is the first piece of work\nto tackle the problem of zero-shot counterfactual generation to\nevaluate and explain text classifiers. Overall our contributions\nin this paper are as follows:\n1) We explore a Framework for Instructed Zero-shot Coun-\nterfactual Generation with LanguagE Models, which we\nrefer to as FIZLE for brevity 1.\n2) We investigate and evaluate FIZLE for two important\nuse-cases: explaining and evaluating black-box text clas-\nsification models.\n3) Through experiments on three benchmark datasets, sev-\neral open-source and proprietary LLMs, we investigate\nthe effectiveness of the proposed pipeline compared to\nrecent baselines and discuss implications for future work\nin this direction.\nII. BACKGROUND AND RELATED WORKS\nIn this section we describe some preliminary concepts along\nwith relevant related works.\n1All code, prompts, supplementary materials, etc. are available at https:\n//github.com/AmritaBh/zero-shot-llm-counterfactual.\nFig. 2. Our proposed FIZLE pipeline for zero-shot LLM-guided counterfac-\ntual generation for evaluation and explanation of black-box text classifiers.\na) Counterfactual Examples: According to most defini-\ntions in literature [4], counterfactuals in text are minimally\nedited versions of an original text that can flip the label of a\nclassifier2. Counterfactuals are typically similar to the input\ninstance, and vary from it in a small number of features.\nCounterfactual examples may be used to stress-test trained\nmodels, provide explanations in the form of counterfactual\nexplanations, and also for model improvement via training\nwith counterfactual examples and counterfactually augmented\ndata. While several efforts have been made in the manual\ncreation of counterfactuals [8], [9], this method does not scale\nup and is therefore infeasible in practice for most use-cases.\nAutomated methods for counterfactual generation are therefore\nmore prevalent. Such methods often use language models\ntrained on some control codes for conditional text generation\nin order to generate plausible and diverse counterfactuals [5],\n[6]. In the context of text classification, which is the scope\nof this paper, counterfactual examples can be generated from\nthe input text via token-based substitution methods, masked-\nlanguage modeling, controlled text generation via control\ncodes [11]. Authors in [12] create realistic counterfactuals via\nlanguage modeling using a Counterfactual GAN architecture.\nHowever, all of these methods use either auxiliary models\nand/or training data, for example, to capture the style charac-\nteristics of different control codes. Some recent work has also\n2although, in this paper, we often use the phrase “counterfactual with same\nlabel as input” which effectively refers to a semantically similar re-write\nof the input text, i.e, similar to having undergone a label-preserving data\naugmentation step.\n\nexplored using large language models in few-shot settings for\ncounterfactual generation [13]. Unlike previous work, in this\npaper, we focus on zero-shot counterfactual generation for\nstress-testing text classifiers.\nb) Large Language Models and Applications in NLP:\nLarge Language Models (LLMs) are usually transformer-based\nmodels capable of generating human-like text. Recent exam-\nples of large language models include the GPT family of mod-\nels from OpenAI [14]–[16], the Llama family of models from\nMeta [17]–[19], Gemini from Google\n[20], etc. A general\ntraining recipe for training LLMs include an unsupervised pre-\ntraining step, where the model is trained using a huge corpora\nof text, typically comprising of text from the internet [21],\n[22], followed by one or more supervised fine-tuning steps,\nsuch as instruction-tuning [23], [24]. More recent state-of-the-\nart LLMs such as GPT-3.5 or GPT-4 are further fine-tuned via\nreinforcement learning with human feedback (RLHF) [25], in\norder to ‘align’ such models more with human preferences and\nvalues. During the fine-tuning and RLHF stages LLMs learn to\nfollow instructions for specific tasks and respond in a helpful\nmanner. Instruction-tuning essentially fine-tunes the model\non massive datasets of (instruction, response) pairs, whereby\nLLMs learn to follow instructions in a prompt in order to\nperform tasks. The vast amount of training, both via the pre-\ntraining and the instruction-tuning stages, enables LLMs to\nperform complex tasks [10], perform in-context learning [26],\netc. Recent advancements in LLMs have sparked simultaneous\nexploration and research into the applicability of these LLMs\non a variety of different tasks, such as data labeling [27]–[29],\ntext classification [30], [31], model explanation [7], etc. We\nadd on to this emerging body of work and explore LLM-guided\nzero-shot counterfactual generation for NLP model evaluation.\nc) LLMs for Counterfactual Generation: While the use\nof LLMs for generating counterfactuals is still an emerging\ndirection, some recent works have started exploring the role\nand effectiveness of LLMs in counterfactual generation [32].\nWhile [13] provides a thorough evaluation on how prompting,\nmodel size, task complexity, etc. affect LLM generations of\ncounterfactuals, authors in\n[33] look at how large language\nmodels can be used to generate counterfactual data for training\nsmaller language models. Authors in [7] explore the use of\nLLMs for causal explainability using counterfactuals. Unlike\nprior work in this direction, in this paper we focus on exploring\nwhether LLMs can be used to generate counterfactuals in the\nzero-shot setting, specifically to evaluate and stress test black-\nbox model in a post-training, pre-deployment scenario.\nIII. ZERO-SHOT LLM-GUIDED COUNTERFACTUAL\nGENERATION\nIn this section, we describe our counterfactual generation\nmethodology as shown in Figure 2. Following the causal\nexplanation generation procedure in prior work\n[7], we use\nstate-of-the-art LLMs in an off-the-shelf manner, without any\nfine-tuning. We improve upon prior work\n[7] by expanding\nand broadening their pipeline into a more general framework\nthat can work for tasks other than causal explanation. Note\nthat in this paper, we formulate and evaluate our pipeline on\nthe broad task of text classification, whereas formulations for\nother text tasks can be derived in a similar manner. To facilitate\nthis we explain the following components in our framework:\na) Input Dataset and Other Task-specific Input: The first\ncomponent in our pipeline takes a task dataset as input and\npre-processes it into tuples denoted by (xi, ˆyi), where xi ∈\nX denote a text sample in the input dataset X, and ˆyi ∈\n{0, 1, ..., k} denote the ground truth label of the corresponding\ninput, in a k-class classification problem. Depending on the\nuse-case, we also have black-box access to a text classification\nmodel f(·) whereby we get f(xi) = yi, which is the predicted\nlabel. In this case, we also build tuples of the form (xi, yi)\nfor use in the generation step.\nb) LLM as the Counterfactual Generator: We leverage\nrecent state-of-the-art LLMs as the counterfactual generators.\nGiven that these models have been trained on vast amounts\nof textual data along with extensive instruction tuning, we\nassume that LLMs can learn to modify and perturb text input\nto simulate how human annotators generate counterfactuals for\nspecific tasks [34]. For this, we use both proprietary models\nfrom OpenAI and open-source models from Meta AI, and\nuse carefully crafted instructions and constraint prompts to\ngenerate the counterfactuals. Specifically, we use the following\nproprietary models via the OpenAI API wherever applicable:\n• GPT-3.53: Often referred to as ChatGPT. This is the\nmodel that has been explored in a variety of text applica-\ntions. Specifically we use the gpt-3.5-turbo variant.\n• GPT-44: This is the successor to GPT-3.5 and is known to\nbe more capable. Specifically this model is purported to\nbe able to understand complex instruction better, thereby\nmaking it highly suitable to our task of counterfactual\ngeneration. We use the gpt-4 and gpt-4-turbo\nversions in our experiments.\n• GPT-4o5: This is the most recent and flagship model from\nthe OpenAI GPT family of models. Although this model\nis capable of solving multimodal tasks (including text and\nvisual input and text output), we only use it for text input.\nAccording to OpenAI, this model is capable of solving\ncomplex, multi-step tasks, thereby making it a suitable\ncandidate for our task.\n• GPT-4o-mini6: This is a low-cost, lower-latency and\npossibly smaller7 of the previous GPT-4o model. The\nlightweight nature of this model may be beneficial for\nresearchers looking to use our pipeline for larger datasets.\nAmong the open-source models, we use the following\nmodels from Meta’s Llama family of models:\n• Llama 2 7B: This is the 7 billion parameter version of\nLlama 2 model [18]. We specifically use the ‘chat’ variant\nvia Huggingface8.\n3https://platform.openai.com/docs/models/gpt-3-5-turbo\n4https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4\n5https://platform.openai.com/docs/models/gpt-4o\n6https://platform.openai.com/docs/models/gpt-4o-mini\n7There is no official information regarding size of GPT-4o vs. GPT-4o-mini\n8https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n\n• Llama 2 13B: This is the 13 billion parameter version of\nLlama 2 model [18]. Similar to the previous one, we use\nthe ‘chat’ variant from Huggingface9.\n• Llama 3 8B: This is the 8 billion parameter variant\nof the more recent Llama 3 model [19]. We use the\n‘instruct’ variant of the model from Huggingface10,since\nit is known to be well-suited for following instructions in\nthe prompt.\nDue to resource constraints, we were unable to use larger\nvariants of Llama 2 and Llama 3 models.\nc) Instruction and Constraint Prompt: To generate the\ncounterfactuals in a zero-shot manner using the chosen LLM,\nthe prompt needs to have informative instructions and con-\nstraints to guide the generation. We generate two types of\ncounterfactuals: (1) actual counterfactuals: that is, counter-\nfactuals that have a different label from the original input,\naccording to the definition of counterfactual example. These\nare used in the counterfactual explanation experiments (see\nSection V), and (2) counterfactuals with same label as original:\nThese are used in the contrast set experiments (see Section\nVI). For setting (1), we experiment with two variants of\nthe generation process: (i) naive: Here the LLM is directly\nprompted to generate a counterfactual, and (ii) guided: Here\nwe use a two-step process - first leveraging the LLM to\nidentify the important input features (i.e., words) that result\nin the predicted label, and then prompting the same LLM\nto edit a minimal set of those identified features to generate\nthe counterfactual. For ease of extraction of the generated\ncounterfactuals, we also specify an output constraint that\nallows easy parsing based on a regular expression string match.\nIV. EXPERIMENTAL SETTINGS\nWe use the pipeline described in the previous section\nin order to generate counterfactuals and demonstrate these\nfor stress-testing and explaining black-box text classifiers.\nSpecifically our tasks are: (1) Counterfactual explanations\nfor explaining decisions of black-box text classifiers and (2)\nEvaluating black-box text classification models via contrast\nsets. To facilitate this, here we go over the datasets used\nand the general experimental setup for all our generation and\nevaluation experiments:\nA. Datasets\nIn this work, we focus on two broad categories of lan-\nguage tasks: text classification and natural language inference\n(NLI)11. For text classification we use two datasets: IMDB [35]\nfor sentiment classification and AG News12 for news topic\nclassification. For NLI, we use the SNLI dataset [36], [37].\nThis variety of datasets allows us to evaluate the LLM-\ngenerated counterfactuals over a variety of label situations\nfrom binary to multi-class.\n9https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\n10https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n11NLI here is also treated as a text classification task where the labels for\neach input are simply one of {entailment, neutral, contradiction}.\n12https://huggingface.co/datasets/ag news\nThe IMDB dataset13 consists of a total of 50k highly polar\nmovie reviews from IMDB (Internet Movie Database). Each\ndata instance consists of a text string comprising the review\ntext, and a label, either ‘negative’ or ‘positive’. The AG News\ndataset consists of over 120k news articles, belonging to one\nof four news topics: ‘world’, ‘sports’, ‘business’ and ‘sci-\nence/technology’. The Stanford Natural Language Inference\n(SNLI) dataset14 consists of 570k sentence pairs consisting of\na premise and a hypothesis. Each premise-hypothesis pair is\nlabeled with one of ‘entailment’, ‘contradiction’ or ‘neutral’\nlabels.\nB. Experimental Setup\nAll experiments on open-source models were performed on\ntwo A100 GPUs with a total of 80G memory. For 13B Llama\n2 models, we use 4-bit quantization using the optimal ‘nf4’\ndatatype [38]. For all LLMs, we use top p sampling with p =\n1, temperature t = 0.4 and a repetition penalty of 1.1. We\nuse PyTorch and fine-tuned models hosted on HuggingFace in\nboth Sections V and VI.\nV. COUNTERFACTUAL EXPLANATIONS VIA\nLLM-GENERATED COUNTERFACTUALS\nExplainability is a major challenge in many NLP appli-\ncations such as text classification [39]–[41]. Although re-\ncent models involving pre-trained transformer-based language\nmodels [42] have achieved or even exceeded human-level\nperformance on several tasks [1], [2], most of these models\nare black-box by design and hence are not interpretable.\nSuch models do not offer transparency on why it predicted\na certain label, or even what features in the input resulted\nin the prediction. The ubiquity of these black-box classifiers\nnecessitates the development of explanation frameworks and\ntechniques that provide some degree of understanding into the\ndecision-making function of the model [43]. Counterfactual\nexplanations [4] give an insight into what could have been\ndifferent in the input to change the output label predicted\nby the classifier. Gold-standard counterfactual generation re-\nquires human annotators and is also task-specific [44], [45],\ntherefore making it an extremely expensive and labor-intensive\nendeavor. Therefore, we investigate whether we use zero-shot\nLLM-generated counterfactuals as counterfactual explanations\nfor black-box text classifiers.\nA. Methodology\nTo generate counterfactual explanations for a black-box\ntext classifier f(·) that predicts f(xi) = yi, we use the\ntuple (xi, yi) in the generation step, thereby replacing the\nground truth label in Figure 2 by the model-predicted label,\nsince we aim to explain why the model predicted yi for the\ninput sample xi. In our experiments, we use a DistilBERT\nmodel\n[46] , fine-tuned on the specific task dataset as the\nblack-box model we aim to explain. Note that since our\ncounterfactual generation process is model-agnostic, the same\n13https://huggingface.co/datasets/imdb\n14https://huggingface.co/datasets/snli\n\nFramework Variant\nPrompt Structure\nFIZLEguided\nStep 1: In the task of <task on task-dataset>, a trained black-box classifier correctly predicted\nthe label ‘<yi>’ for the following text. Explain why the model predicted the ‘<yi>’ label by\nidentifying the words in the input that caused the label.\nList ONLY the words as a comma separated list.\\n—\\nText: <xi>\nStep 2: Generate a counterfactual explanation for the original text by ONLY changing a minimal set of the\nwords you identified, so that the label changes from ‘<yi>’ to ‘<ycf>’. Use the following definition of\n‘counterfactual explanation’: “A counterfactual explanation reveals what should have been different in an\ninstance to observe a diverse outcome.” Enclose the generated text within <new> tags.\nFIZLEnaive\nIn the task of <task on task-dataset>, a trained black-box classifier correctly predicted the label ‘<yi>’\nfor the following text. Generate a counterfactual explanation by making minimal changes to the input text,\nso that the label changes from ‘<yi>’ to ‘<ycf>’. Use the following definition of ‘counterfactual explanation’:\n“A counterfactual explanation reveals what should have been different in an instance to observe a diverse\noutcome.” Enclose the generated text within <new> tags.\\n—\\nText: <xi>.\nprocedure can be applied to any black-box classifier in place\nof DistilBERT. Inspired by prior work [7], we develop and\nexperiment with two variants of FIZLE: (1) FIZLEnaive:\nwhich directly generates the counterfactual explanation, and\n(2) FIZLEguided: which first extracts words that may have\nbeen responsible for the predicted label, and then uses those\nselected words to generate a counterfactual explanation, in a\ntwo-step manner. We hypothesize that the two-step generation\nmay result in more effective and better quality counterfactual\nexplanations, due to the additional guidance provided to the\nLLM, analogous to prior work such as Chain of Thought\nprompting [47]. We show the prompts used in both the variants\nin Table V.\nB. Evaluation Metrics\nTo evaluate the goodness of the counterfactual explanations\ngenerated by our zero-shot LLM-guided pipeline, we use a\nvariety of evaluation metrics following prior work\n[5]–[7].\nIdeally, the generated counterfactual explanations should be\nable to flip the label of the classifier, thereby showcasing what\ncould have changed in the input that would flip the label of\nthe classifier. Furthermore, counterfactual explanations should\nalso be minimally edited samples of the input text, i.e., they\nshould be as close as possible to the input sample both in the\ntoken space and the semantic space. To capture and evaluate\nthese criteria, we use the following metrics:\na) Label Flip Score:\nWe use Label Flip Score to\nmeasure the effectiveness of the generated counterfactual\nexplanations. For each input text xi in the test split of\nthe dataset, with correctly predicted label f(xi) = yk, we\nevaluate the corresponding LLM-generated counterfactual xcf\ni\nusing the same black-box classifier f(·) and obtain a label\nfor the counterfactual. For an effective counterfactual, the\nobtained label should be different from the original label yk.\nThen Label Flip Score % (LFS) is computed as:\nLFS = 1\nn\nn\nX\ni=1\n1[f(xi) ̸= f(xcf\ni )] × 100\n(1)\nwhere n is the number of samples in the test set and 1 is\nthe identity function.\nb) Textual Similarity: Counterfactual explanations gener-\nated by the LLMs should ideally be as ‘similar’ to the original\ninput text as possible. To evaluate this similarity, we use two\nmetrics: similarity of the text embeddings using the Universal\nSentence Encoder (USE) [48] in the latent space, and a nor-\nmalized Levenshtein distance [49] to measure word edits in the\ntoken space. The semantic similarity using the embeddings of\nthe original input and the generated counterfactual is computed\nas the inner product of the original and the counterfactual\nembeddings, averaged over the test dataset:\nsimsemantic = 1\nn\nn\nX\ni=1\nEnc(xi) · Enc(xcf\ni )\n(2)\nwhere Enc(·) refers to the Universal Sentence Encoder, n\nis the number of samples in the test set.\nLevenshtein distance [49] between two strings is defined as\nthe minimum number of single character edits that are required\nto convert one string to another. To measure the distance be-\ntween the original input text and the generated counterfactual\nin the token space we use a normalized Levenshtein distance,\nfurther averaged over the test dataset. This is computed as:\nedit dist = 1\nn\nn\nX\ni=1\nlev(xi, xcf\ni )\nmax(|xi|, |xcf\ni |)\n(3)\nwhere |xi| and |xcf\ni | refer to the length of xi and xcf\ni\nrespectively, lev(·, ·) refers to the Levenshtein distance, and\nn is the number of samples in the test set.\nC. Baselines\nSimilar to other counterfactual generation methods [5],\n[6], we compare our proposed FIZLE pipeline with three\nrepresentative baselines from three categories of similar works:\n(i) BAE [50] is a recent adversarial attack method that uses\nmasked language modeling with BERT to perturb the input\ntext by replacing masked words; (ii) CheckList\n[51] is a\nmethod for behavioral testing of NLP models via test cases\ngenerated by template-based methods as well as masked\nlanguage models like RoBERTa; (iii) Polyjuice\n[5] is a\nrecent counterfactual generation method that uses an auxiliary\n\nlanguage model (such as GPT-2) to generate diverse counter-\nfactuals. Note that unlike these baselines, our FIZLE pipeline\ndoes not require any additional dataset or training, thereby\nenabling a completely zero-shot generation.\nD. Results: Effectiveness of Generated Counterfactual Expla-\nnations\nFollowing the experimental setup described above, we eval-\nuate whether generated counterfactuals can be used to explain\nblack-box classifiers for the three datasets, as compared to\nthe baselines. We show these quantitative results in Table I.\nFor each LLM, we evaluate both variants of our framework:\nFIZLEguided and FIZLEnaive. For effective and good quality\ncounterfactual explanations, ideally we would expect high\nvalues of LFS and semantic similarity with low values of edit\ndistance. Overall, we see varied performance of the LLMs and\nthe two variants across the different tasks. Similar to other\ncounterfactual generation works [6], we see an obvious trade-\noff between the Label Flip Score and the semantic similarity.\nThis is intuitive since the more the generated counterfactual\ndeviates from the original input text, higher the chances are\nfor it to be a successful counterfactual for the original input\n(i.e, it would result in a label flip). Among the three baselines,\nwe see CheckList fails completely in generating counterfactual\nexplanations. We see satisfactory performance by BAE, except\nfor the AG News dataset. For Polyjuice, even though the LFS\nscores are high, the unsatisfactory textual similarity scores\nimply that the counterfactuals generated are not good quality\nand deviate from the input text significantly.\nOverall we do not see a clear winner between the two\nvariants: FIZLEguided outperforms FIZLEnaive. However,\nwhile the LFS scores are better in the naive variant over the\nguided, the LLM is unable to preserve the textual similarity\nin comparison to the guided case. This may imply that the\nadditional ‘guidance’ provided by identifying the input words\nbefore the counterfactual explanation generation step enables\nthe generation of counterfactuals closer in semantics to the\noriginal input. GPT-4o and GPT-4o-mini when used in the\nnaive variant of our pipeline, have the best performance for\nzero-shot counterfactual explanation generation, in terms of\nLFS, for IMDB and AG News datasets, respectively. For nat-\nural language inference on the SNLI dataset, we see all LLMs\nstruggle to generate good counterfactual explanations. GPT-\n4o performs well, possibly owing to its instruction and textual\nunderstanding capabilities [10], but the best performance is\nby the Polyjuice baseline. This poor performance of LLMs\nparticularly on the SNLI dataset is further evidence towards\nLLMs struggling with inference and reasoning. This gap in\nthe capabilities of recent LLMs on reasoning tasks has been\nobserved by several recent efforts as well [52], [53].\nLastly, we see the open-source models Llama 2 7B and\n13B struggle to generate zero-shot counterfactual explanations\nwith small number of edits, thus resulting in very high edit\ndistances. The Llama 2 models struggle to keep the generated\ncounterfactuals semantically similar to the original input, im-\nplying they either make too many edits to the input text, or\noutput some unrelated, low-quality text that does not conform\nto the instructions provided in the prompt. However, the newer\nLlama 3 8b model outperforms both these models in most\nsettings.\nVI. EVALUATING MODELS VIA LLM-GENERATED\nCOUNTERFACTUALS\nDeep learning models such as text classification models are\noften trained in a supervised manner using labeled training\nsets, and then evaluated on a hold-out test set. Such train-test\nsplits of data usually arise from the same corpus that has the\nsame or similar sources and annotation guidelines. Therefore,\nin essence, standard evaluation using such hold-out test sets\nmeasure merely the in-distribution performance of the model,\nwhile in reality, the same model may demonstrate sub-par\nperformance on out-of-distribution or in-the-wild test data [8].\nTo alleviate this issue to some degree, approaches such as eval-\nuating using challenge sets or robustness to label-preserving\nperturbations, etc. have been explored by the community. One\nspecific method of stress-testing such models is via contrast\nsets\n[8]. A contrast set C(x) is essentially a sample of\npoints around a data point x, that is close to the local ground\ntruth decision boundary. Samples in C(x) may have same or\ndifferent ground truth label as x. In practice, C(x) can be a set\nof samples that are ‘close’ to x, i.e., have minimal edit distance\nfrom x, yet be ‘challenging’ for a trained model to classify. In\nthe original contrast sets work [8], the authors advocate for an\nevaluation paradigm where dataset authors themselves create\nand release such contrast sets for model evaluation. However,\nwe note that this is highly infeasible in practice, given the cost\nof expert creation of such challenging data points. Therefore,\nautomated methods for designing such challenging evaluation\nsets in the form of contrast sets are highly desirable, albeit at\nthe expense of trading off expert insights. One such automated\nmethod for developing contrast sets to evaluate models is\nthat of counterfactual examples, as demonstrated by previous\nwork [5]. Motivated by the effectiveness of counterfactuals\nas contrast sets in prior work [5], we envision the use of\nLLM-generated contrast sets as well for the same purpose of\nmodel evaluation. Here we describe the methodology for the\ngeneration and evaluation of such contrast sets using LLMs in\na zero-shot manner.\nA. Methodology\nFor generating the contrast sets, we use the same LLMs\nas used in Section V, and prompt each LLM to generate\ncounterfactuals in a zero-shot manner using the input text\nand ground truth label tuple (xi, ˆyi). Unlike [5], we do not\nuse human annotators to label the generated counterfactuals.\nTherefore, differing from [5], in our evaluation, we only focus\non counterfactuals that have the same label as the original\ninput, and use these as contrast sets. We make this choice\nsince the lack of human annotation and lack of step-by-step\nguidance (such as in FIZLEguided) would make it harder to\nvalidate whether the edits performed by the LLM are actually\nlabel flipping or not. Instead, we guide the generation process\n\nTABLE I\nEVALUATION RESULTS OF BOTH VARIANTS OF OUR FIZLE FRAMEWORK IN COMPARISON TO BASELINES: BAE [50], CHECKLIST [51] AND\nPOLYJUICE [5]. WE REPORT THE LABEL FLIP SCORE (LFS), SEMANTIC SIMILARITY (SEM. SIM) AND NORMALIZED LEVENSHTEIN DISTANCE (EDIT\nDIST.). BEST LFS SCORES FOR EACH DATASET ARE IN BOLD, SECOND BEST IS UNDERLINED.\nModel\nIMDB\nAG News\nSNLI\nLFS ↑\nSem. Sim. ↑\nEdit Dist. ↓\nLFS ↑\nSem. Sim. ↑\nEdit Dist. ↓\nLFS ↑\nSem. Sim. ↑\nEdit Dist. ↓\nBAE [50]\n79.6\n0.99\n0.044\n25.0\n0.97\n0.063\n74.4\n0.95\n0.054\nCheckList [51]\n2.6\n0.99\n0.013\n1.6\n0.92\n0.083\n3.0\n0.96\n0.036\nPolyjuice [5]\n96.86\n0.25\n0.884\n72.64\n0.22\n0.749\n95.8\n0.74\n0.367\nGPT-3.5\n(guided)\n78.52\n0.91\n0.126\n30.55\n0.95\n0.084\n32.47\n0.89\n0.102\nGPT-3.5\n(naive)\n59.19\n0.88\n0.236\n49.0\n0.91\n0.325\n56.39\n0.92\n0.182\nGPT-4\n(guided)\n97.2\n0.89\n0.142\n82.39\n0.65\n0.232\n73.6\n0.88\n0.153\nGPT-4\n(naive)\n99.6\n0.87\n0.226\n84.39\n0.65\n0.278\n78.0\n0.88\n0.152\nGPT-4o\n(guided)\n93.56\n0.85\n0.389\n62.5\n0.74\n0.1917\n66.56\n0.92\n0.092\nGPT-4o\n(naive)\n99.57\n0.72\n0.56\n90.12\n0.41\n0.549\n79.39\n0.90\n0.157\nGPT-4o-mini\n(guided)\n98.58\n0.82\n0.413\n66.33\n0.76\n0.173\n47.09\n0.81\n0.185\nGPT-4o-mini\n(naive)\n100.0\n0.63\n0.642\n66.6\n0.59\n0.485\n51.4\n0.83\n0.282\nLlama 2 7B\n(guided)\n76.64\n0.66\n0.546\n51.11\n0.77\n0.244\n36.82\n0.74\n0.304\nLlama 2 7B\n(naive)\n64.7\n0.59\n0.68\n35.25\n0.70\n0.492\n58.33\n0.62\n0.577\nLlama 2 13B\n(guided)\n51.11\n0.70\n0.533\n51.65\n0.77\n0.266\n50.2\n0.67\n0.495\nLlama 2 13B\n(naive)\n66.67\n0.52\n0.715\n37.63\n0.58\n0.606\n59.95\n0.55\n0.621\nLlama 3 8B\n(guided)\n90.95\n0.80\n0.453\n47.48\n0.73\n0.197\n44.89\n0.81\n0.216\nLlama 3 8B\n(naive)\n97.34\n0.71\n0.522\n75.36\n0.71\n0.505\n61.6\n0.71\n0.466\nTABLE II\nPERFORMANCE OF FIZLE-GENERATED COUNTERFACTUALS AS CONTRAST SETS. C.S. ACC. REFERS TO ACCURACY ON THE GENERATED CONTRAST\nSETS, ORIGINAL TEST ACC. IS THE TEST ACCURACY ON THE CORRESPONDING PAIRED ORIGINAL SAMPLES. SEM. SIM. REFERS TO SEMANTIC\nSIMILARITY AS COMPUTED BY EQ. 2, EDIT. DIST. IS THE TOKEN LEVEL DISTANCE AS COMPUTED BY EQ. 3, CONS. (%) IS THE CONSISTENCY AS\nCOMPUTED BY EQ. 4.\nCounterfactual\ngenerator\nIMDB\nSNLI\nAG News\nOriginal\nTest Acc.\nC.s. Acc. ↓\nEdit\nDist. ↓\nSem.\nSim. ↑\nCons.\n%↓\nOriginal\nTest Acc.\nC.s. Acc. ↓\nEdit\nDist. ↓\nSem.\nSim. ↑\nCons.\n%↓\nOriginal\nTest Acc.\nC.s. Acc. ↓\nEdit\nDist. ↓\nSem.\nSim. ↑\nCons.\n%↓\nPolyjuice [5]\n94.3\n84.9\n-\n-\n76.1\n86.5\n72.3\n-\n-\n56.4\n-\n-\n-\n-\n-\nExpert [8]\n96.31\n84.84\n0.136\n0.939\n81.56\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nGPT-3.5\n88.82\n0.162\n0.931\n85.22\n57.42\n0.175\n0.908\n53.53\n93.42\n0.287\n0.883\n92.07\nGPT-4\n93.35\n92.65\n0.157\n0.942\n90.95\n86.25\n73.01\n0.277\n0.841\n68.82\n94.6\n94.0\n0.352\n0.855\n93.0\nGPT-4o\n95.4\n0.133\n0.953\n93.2\n80.61\n0.237\n0.882\n75.15\n95.2\n0.406\n0.829\n92.8\nGPT-4o-mini\n94.0\n0.394\n0.873\n91.0\n74.34\n0.343\n0.842\n68.89\n93.8\n0.452\n0.818\n92.0\nLlama 2 7B\n87.32\n0.559\n0.728\n83.74\n63.88\n0.382\n0.782\n57.87\n92.94\n0.438\n0.808\n91.72\nLlama 2 13B\n84.76\n0.580\n0.710\n82.2\n48.6\n0.476\n0.738\n43.21\n93.5\n0.427\n0.808\n92.03\nLlama 3 8B\n89.78\n0.267\n0.861\n88.03\n72.75\n0.158\n0.923\n67.51\n93.64\n0.229\n0.874\n91.95\n\nvia the instruction in the prompt. We use the following prompt\nto perform the generation:\n‘You are a robustness checker for a machine learning\nalgorithm. In the task of <taski>, the following data\nsample has the ground truth label <ˆyi>. Make minimal\nchanges to the data sample to create a more challenging\ndata point while keeping the ground truth label the same.\nText: <xi>’\nwhere, taski is the description of the task, such as “senti-\nment classification”, xi is the input text, and ˆyi is the ground\ntruth label.\nB. Evaluation Metrics\nFor evaluating the goodness of the generated counterfactuals\nas contrast sets, we compare the accuracy of the target model\nf(·) on both the original test set and the generated contrast\nset. Following [5], we also measure the consistency,\nconsistency = 1\nn\nn\nX\ni=1\n1[f(xi) = ˆyi ∧f(xcs\ni ) = ˆycs\ni ] × 100\n(4)\nwhere xcs\ni is the LLM-generated contrast set for the original\ninput xi, ˆycs\ni\nis the ground truth label for the contrast set\nexample and n is the number of test samples. Consistency\nmeasures the percentage of times when the model correctly\nclassifies both the original and the contrast set example.\nLike the previous set of experiments, we want the generated\ncounterfactuals (or contrast sets) to be as close to the original\ntext input as possible, i.e., the edits should ideally be minimal.\nTherefore, we capture the textual similarity again in the token\nspace via Equation 3 and the latent space via Equation 2.\nC. Baselines\nSince there is not much work on contrast sets, we have a\nlimited set of baselines here. We use the original expert-created\ncontrast sets for the IMDB dataset from the original work [8].\nThis consists of 488 original test data samples, and 488\ncontrast samples created by the dataset experts. Furthermore,\nwe use Polyjuice-generated counterfactuals [5] as contrast sets\nfor comparison.\nD. Results: Effectiveness of Counterfactuals as Contrast Sets\nWe use the same DistilBERT models as in Section V that\nare fine-tuned for each of the 3 tasks (IMDB, SNLI and AG\nNews). We evaluate each of these 3 models on both the original\ntest set and the counterfactual one (i.e., the contrast sets) and\nshow these results in Table II. We obtain the performance\nvalues for Polyjuice-generated contrast sets from the original\npaper [5]. For the ‘Expert’ baseline, the IMDB contrast sets\nare created by human experts in [8]. Unfortunately, there does\nnot exist any expert created contrast set for SNLI and AG\nNews datasets. As evident from the test accuracies on both the\noriginal test set and the counterfactual one, we see a consistent\ndecrease in performance on the generated counterfactuals over\nthe original samples. The performance drop for the AG News\ndataset seems to be the least while interestingly, we see the\nhighest performance drop on contrast sets for the SNLI dataset.\nFurthermore, we see GPT-3.5 and GPT-4 are able to create\ncontrast sets with high degree of semantic similarity and\nlow edit distance, thus being more desirable over Llama 2\ngenerated contrast sets. Interestingly, we see that GPT-4o and\nGPT-4o-mini contrast sets often have better performance than\nthe original test set, which could imply that these contrast sets\nare too ‘easy’, and therefore not functional. Overall, for the\nother LLMs, the drop in accuracy and the consistency values\nseem analogous to similar results in literature (average drop\nin classification accuracy of around 6.8% according to\n[5])\nthat use human-generated contrast sets for evaluation [5], [8].\nEvaluating models with such LLM-generated contrast sets may\nthereby allow the model developer to investigate what type of\nsamples the model is failing on, thereby informing choices\nregarding further robustness training.\nWhile this is promising, we do note the ethical concerns\nsurrounding this: LLM-generated contrast sets may induce\npre-existing biases that can propagate further bias and errors\nthrough evaluation and subsequent model improvement steps.\nOne hybrid way to effectively use LLM-generated contrast sets\nis by broadly identifying the failure models of the model via\nprobing the model using the LLM-generated contrast sets, and\nthen employing human annotators or data creators to hone in\non that specific failure mode to either generate more contrast\nsets or counterfactually augmented training data to fill the\nidentified gap. Such a combined method would greatly reduce\ncosts while still being effective in terms of model evaluation\nand development.\nVII. CASE STUDY: HOW HAS THE PERFORMANCE OF\nLLMS EVOLVED OVER TIME?\nIn order to use LLMs for zero-shot counterfactual genera-\ntion for stress-testing models, one needs to account for the\nfast evolving landscape of LLM development and release.\nThese models, especially proprietary ones, undergo frequent\nmodel updates. Older models are often deprecated, making\nadoption of such solutions challenging without thorough re-\nevaluation of pipelines such as ours. To explore the effect of\nmodel release and deprecation life cycles, we conduct a small\ncase study to evaluate how performance of this method of\ngenerating counterfactuals vary over time. We do this by re-\nporting naive generation results for counterfactual explanations\nfor GPT family of models. Models we use here, from oldest\n(2022) to newest (2024), are: text-davinci-00315 →\nGPT-3.5 →GPT-4 →GPT-4o. We show this comparison\nin Figure 3. Keeping the expected trade-off between LFS and\nsemantic similarity in mind, we see that for a task like NLI,\nnewer models in the GPT family perform better, achieving\nhigh LFS scores while retaining semantic similarity, whereas\nfor AG News, performance is varied: recent models like GPT-\n4 and GPT-4o achieve higher LFS but at the cost of textual\nsimilarity. While a more thorough evaluation is required in\n15https://platform.openai.com/docs/deprecations\n\nFig. 3. Comparison of LFS and semantic similarity (Sem-sim) for generated\ncounterfactual explanations for AG News (top) and SNLI (bottom). LFS %\nscaled to 0-1, higher values for both are better. dv-003 refers to text-davinci-\n003.\norder to make broad claims, however, given these tasks and\nmodels, recent models like GPT-4o seem to be better at\nmore complex tasks such as reasoning and inference, while a\nrelatively older model like GPT-4 may be sufficient for other\ntypes of tasks.\nVIII. CONCLUSION & FUTURE WORK\nIn this paper, we explore the possibility of using LLM-\ngenerated zero-shot counterfactuals for stress-testing black-\nbox text classification models. To this end, we propose a\npipeline, with two variants, for generation of such counterfac-\ntuals in a zero-shot manner. We conduct experiments with a\nvariety of proprietary and open LLMs, and evaluate generated\ncounterfactuals on two broad NLP model development tasks:\n(1) counterfactual explanation of black-box text classifiers,\nand (2) evaluation of black-box text classification models via\ncontrast sets. Our results are promising and we see benefits to\nusing our proposed FIZLE pipeline across the two use-cases\nand three downstream tasks. Our findings suggest the effective\nuse of LLM-generated counterfactuals for explaining black-\nbox NLP models, as well as potentially identifying failure\nmodels of NLP models via evaluation with contrast sets. We\nfurther discuss implications and how hybrid human-and-AI\nmethods may benefit from our exploration, along with looking\nat how model updates over time can affect the quality and\nperformance of generated counterfactuals.\nFuture work may investigate modifications to the generation\npipeline such as plugging in an additional model, perhaps a\nsmall language mode (SLM) to evaluate label of generated\ncontrast sets. More effort can also be put into validating\nthe faithfulness of the generated counterfactual explanation\nand correctness of generated contrast sets. Further exploration\ncould look into categorizing the failure modes of the black-box\nmodels, after being evaluated by these generated counterfac-\ntuals. This information can then be used to collect or generate\nmore data for robustness training of these models. Since\none of the challenges in our method was to ensure that the\ngenerated text is actually a counterfactual, devising ways and\nhuman annotations to ensure more conformity of the generated\ncounterfactual explanations to the definition of ‘counterfactual\nexplanation’ is also be an area that can be improved. Finally,\napart from these two use-cases of counterfactuals, LLM-\ngenerated counterfactuals can also be evaluated in tasks such\nas model training or improvement, uncovering biases in model\npredictions, incorporating fairness into model predictions, etc.\nACKNOWLEDGMENTS\nThis work is supported by the DARPA SemaFor project\n(HR001120C0123). The views, opinions and/or findings ex-\npressed are those of the authors.\nREFERENCES\n[1] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n“Glue: A multi-task benchmark and analysis platform for natural lan-\nguage understanding,” arXiv preprint arXiv:1804.07461, 2018.\n[2] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,\nO. Levy, and S. Bowman, “Superglue: A stickier benchmark for general-\npurpose language understanding systems,” Advances in neural informa-\ntion processing systems, vol. 32, 2019.\n[3] X. Liu, T. Sun, J. He, J. Wu, L. Wu, X. Zhang, H. Jiang, Z. Cao,\nX. Huang, and X. Qiu, “Towards efficient nlp: A standard evaluation\nand a strong baseline,” arXiv preprint arXiv:2110.07038, 2021.\n[4] C. Molnar, Interpretable machine learning.\nLulu. com, 2020.\n[5] T. Wu, M. T. Ribeiro, J. Heer, and D. S. Weld, “Polyjuice: Generating\ncounterfactuals for explaining, evaluating, and improving models,” in\nProceedings of the 59th Annual Meeting of the ACL and the 11th\nIJCNLP (Volume 1: Long Papers), 2021, pp. 6707–6723.\n[6] N. Madaan, I. Padhi, N. Panwar, and D. Saha, “Generate your coun-\nterfactuals: Towards controlled counterfactual generation for text,” in\nProceedings of the AAAI Conference on Artificial Intelligence, vol. 35,\nno. 15, 2021, pp. 13 516–13 524.\n[7] A. Bhattacharjee, R. Moraffah, J. Garland, and H. Liu, “Towards llm-\nguided causal explainability for black-box text classifiers,” in AAAI 2024\nWorkshop on Responsible Language Models, Vancouver, BC, Canada,\n2024.\n[8] M. Gardner, Y. Artzi, V. Basmov, J. Berant, B. Bogin, S. Chen, P. Dasigi,\nD. Dua, Y. Elazar, A. Gottumukkala et al., “Evaluating models’ local\ndecision boundaries via contrast sets,” in Findings of the ACL: EMNLP\n2020, 2020, pp. 1307–1323.\n[9] L. Qin, A. Bosselut, A. Holtzman, C. Bhagavatula, E. Clark, and\nY. Choi, “Counterfactual story reasoning and generation,” in Proceedings\nof EMNLP-IJCNLP, 2019, pp. 5043–5053.\n[10] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Ka-\nmar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg et al., “Sparks of artificial\ngeneral intelligence: Early experiments with gpt-4,” arXiv preprint\narXiv:2303.12712, 2023.\n[11] N. Madaan, S. Bedathur, and D. Saha, “Plug and play counterfactual\ntext generation for model robustness,” arXiv preprint arXiv:2206.10429,\n2022.\n[12] M. Robeer, F. Bex, and A. Feelders, “Generating realistic natural lan-\nguage counterfactuals,” in Findings of the Association for Computational\nLinguistics: EMNLP 2021, 2021, pp. 3611–3625.\n[13] Y. Li, M. Xu, X. Miao, S. Zhou, and T. Qian, “Prompting large\nlanguage models for counterfactual generation: An empirical study,” in\nProceedings of the 2024 Joint International Conference on Computa-\ntional Linguistics, Language Resources and Evaluation (LREC-COLING\n2024), 2024, pp. 13 201–13 221.\n\n[14] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[15] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-\nels are few-shot learners,” Advances in neural information processing\nsystems, vol. 33, pp. 1877–1901, 2020.\n[16] OpenAI, “Gpt-4 technical report,” arXiv, pp. 2303–08 774, 2023.\n[17] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al.,\n“Llama: Open and efficient foundation language models,” arXiv preprint\narXiv:2302.13971, 2023.\n[18] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama\n2: Open foundation and fine-tuned chat models,” arXiv preprint\narXiv:2307.09288, 2023.\n[19] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\nA. Mathur, A. Schelten, A. Yang, A. Fan et al., “The llama 3 herd of\nmodels,” arXiv preprint arXiv:2407.21783, 2024.\n[20] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,\nJ. Schalkwyk, A. M. Dai, A. Hauth et al., “Gemini: a family of highly\ncapable multimodal models,” arXiv preprint arXiv:2312.11805, 2023.\n[21] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli,\nH. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, “The refinedweb\ndataset for falcon llm: Outperforming curated corpora with web data,\nand web data only,” CoRR, 2023.\n[22] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\nJ. Phang, H. He, A. Thite, N. Nabeshima et al., “The pile: An\n800gb dataset of diverse text for language modeling,” arXiv preprint\narXiv:2101.00027, 2020.\n[23] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\nmodels to follow instructions with human feedback,” Advances in Neural\nInformation Processing Systems, vol. 35, pp. 27 730–27 744, 2022.\n[24] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li,\nX. Wang, M. Dehghani, S. Brahma et al., “Scaling instruction-finetuned\nlanguage models,” JMLR, vol. 25, no. 70, pp. 1–53, 2024.\n[25] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei,\n“Deep reinforcement learning from human preferences,” Advances in\nneural information processing systems, vol. 30, 2017.\n[26] Q. Dong, L. Li, D. Dai, C. Zheng, J. Ma, R. Li, H. Xia, J. Xu, Z. Wu,\nB. Chang et al., “A survey on in-context learning,” in Proceedings\nof the 2024 Conference on Empirical Methods in Natural Language\nProcessing, 2024, pp. 1107–1128.\n[27] X. He, Z. Lin, Y. Gong, A. Jin, H. Zhang, C. Lin, J. Jiao, S. M. Yiu,\nN. Duan, W. Chen et al., “Annollm: Making large language models to\nbe better crowdsourced annotators,” arXiv preprint arXiv:2303.16854,\n2023.\n[28] P. Bansal and A. Sharma, “Large language models as annotators:\nEnhancing generalization of nlp models at minimal cost,” arXiv preprint\narXiv:2306.15766, 2023.\n[29] Z. Tan, A. Beigi, S. Wang, R. Guo, A. Bhattacharjee, B. Jiang,\nM. Karami, J. Li, L. Cheng, and H. Liu, “Large language models for\ndata annotation: A survey,” arXiv preprint arXiv:2402.13446, 2024.\n[30] X. Sun, X. Li, J. Li, F. Wu, S. Guo, T. Zhang, and G. Wang, “Text clas-\nsification via large language models,” arXiv preprint arXiv:2305.08377,\n2023.\n[31] A. Bhattacharjee and H. Liu, “Fighting fire with fire: can chatgpt detect\nai-generated text?” ACM SIGKDD Explorations Newsletter, vol. 25,\nno. 2, pp. 14–21, 2024.\n[32] V. B. Nguyen, P. Youssef, J. Schl¨otterer, and C. Seifert, “Llms for\ngenerating and evaluating counterfactuals: A comprehensive study,”\narXiv preprint arXiv:2405.00722, 2024.\n[33] Z. Chen, Q. Gao, A. Bosselut, A. Sabharwal, and K. Richardson, “Disco:\nDistilling counterfactuals with large language models,” arXiv preprint\narXiv:2212.10534, 2022.\n[34] F. Gilardi, M. Alizadeh, and M. Kubli, “Chatgpt outperforms crowd\nworkers for text-annotation tasks,” Proceedings of the National Academy\nof Sciences, vol. 120, no. 30, p. e2305016120, 2023.\n[35] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts,\n“Learning word vectors for sentiment analysis,” in Proceedings of the\n49th Annual Meeting of the Association for Computational Linguistics:\nHuman Language Technologies.\nPortland, Oregon, USA: Association\nfor Computational Linguistics, June 2011, pp. 142–150. [Online].\nAvailable: http://www.aclweb.org/anthology/P11-1015\n[36] B. MacCartney and C. D. Manning, “Modeling semantic containment\nand exclusion in natural language inference,” in Proceedings of the 22nd\nInternational Conference on Computational Linguistics (Coling 2008),\n2008, pp. 521–528.\n[37] S. Bowman, G. Angeli, C. Potts, and C. D. Manning, “A large annotated\ncorpus for learning natural language inference,” in Proceedings of\nEMNLP, 2015, pp. 632–642.\n[38] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora:\nEfficient finetuning of quantized llms,” arXiv preprint arXiv:2305.14314,\n2023.\n[39] M. T. Ribeiro, S. Singh, and C. Guestrin, “” why should i trust you?”\nexplaining the predictions of any classifier,” in Proceedings of the 22nd\nACM SIGKDD international conference on knowledge discovery and\ndata mining, 2016, pp. 1135–1144.\n[40] P. Atanasova, J. G. Simonsen, C. Lioma, and I. Augenstein, “A di-\nagnostic study of explainability techniques for text classification,” in\nProceedings of EMNLP, 2020, pp. 3256–3274.\n[41] O.-M. Camburu, T. Rockt¨aschel, T. Lukasiewicz, and P. Blunsom, “e-\nsnli: Natural language inference with natural language explanations,”\nAdvances in Neural Information Processing Systems, vol. 31, 2018.\n[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, 2017.\n[43] S. Ali, T. Abuhmed, S. El-Sappagh, K. Muhammad, J. M. Alonso-\nMoral, R. Confalonieri, R. Guidotti, J. Del Ser, N. D´ıaz-Rodr´ıguez,\nand F. Herrera, “Explainable artificial intelligence (xai): What we know\nand what is left to attain trustworthy artificial intelligence,” Information\nFusion, vol. 99, p. 101805, 2023.\n[44] D. Kaushik, E. Hovy, and Z. Lipton, “Learning the difference that\nmakes a difference with counterfactually-augmented data,” in ICLR,\n2020. [Online]. Available: https://openreview.net/forum?id=Sklgs0NFvr\n[45] D. Khashabi, T. Khot, and A. Sabharwal, “More bang for your buck:\nNatural perturbation for robust question answering,” in Proceedings of\nEMNLP, 2020, pp. 163–170.\n[46] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled\nversion of bert: smaller, faster, cheaper and lighter,” arXiv preprint\narXiv:1910.01108, 2019.\n[47] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\nD. Zhou et al., “Chain-of-thought prompting elicits reasoning in large\nlanguage models,” Advances in neural information processing systems,\nvol. 35, pp. 24 824–24 837, 2022.\n[48] D. Cer, Y. Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. S. John,\nN. Constant, M. Guajardo-Cespedes, S. Yuan, C. Tar et al., “Universal\nsentence encoder,” arXiv preprint arXiv:1803.11175, 2018.\n[49] V. I. Levenshtein et al., “Binary codes capable of correcting deletions,\ninsertions, and reversals,” in Soviet physics doklady, vol. 10, no. 8.\nSoviet Union, 1966, pp. 707–710.\n[50] S. Garg and G. Ramakrishnan, “Bae: Bert-based adversarial examples\nfor text classification,” in Proceedings of EMNLP, 2020, pp. 6174–6181.\n[51] M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh, “Beyond accuracy:\nBehavioral testing of nlp models with checklist,” in Proceedings of the\n58th Annual Meeting of the ACL, 2020, pp. 4902–4912.\n[52] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\nJ. Aslanides, S. Henderson, R. Ring, S. Young et al., “Scaling language\nmodels: Methods, analysis & insights from training gopher,” arXiv\npreprint arXiv:2112.11446, 2021.\n[53] K. Valmeekam, A. Olmo, S. Sreedharan, and S. Kambhampati, “Large\nlanguage models still can’t plan (a benchmark for llms on planning and\nreasoning about change),” in NeurIPS 2022 FMDM Workshop.",
    "pdf_filename": "Zero-shot_LLM-guided_Counterfactual_Generation_A_Case_Study_on_NLP_Model_Evaluation.pdf"
}