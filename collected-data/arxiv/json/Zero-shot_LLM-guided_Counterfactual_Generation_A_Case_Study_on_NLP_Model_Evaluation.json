{
    "title": "Zero-shot LLM-guided Counterfactual Generation:",
    "abstract": "complex, black-box models for solving many natural language processing (NLP) tasks, there is also an increasing necessity of methods to stress-test these models and provide some degree of interpretability or explainability. While counterfactual examples areusefulinthisregard,automatedgenerationofcounterfactuals is a data and resource intensive process. such methods depend on models such as pre-trained language models that are then fine-tuned on auxiliary, often task-specific datasets, that may be infeasible to build in practice, especially for new tasks and data domains. Therefore, in this work we explore the possibility of leveraging large language models (LLMs) for zero- shotcounterfactualgenerationinordertostress-testNLPmodels. Fig.1. Examplesofaninputsentenceanditscorrespondingcounterfactual We propose a structured pipeline to facilitate this generation, exampleswithsameoroppositelabel. and we hypothesize that the instruction-following and textual understanding capabilities of recent LLMs can be effectively leveraged for generating high quality counterfactuals in a zero- input sentence This movie is great. Such minimally perturbed shot manner, without requiring any training or fine-tuning. Throughcomprehensiveexperimentsonavarietyofpropreitary variations of the input text can be used in a variety of settings and open-source LLMs, along with various downstream tasks in to evaluate models, to understand whether a model is able to NLP,weexploretheefficacyofLLMsaszero-shotcounterfactual focus on the task-specific features in the input text in order to generators in evaluating and explaining black-box NLP models. classify the input text correctly. Index Terms—counterfactual generation, model evaluation, Whileseveralpreviousworkshaveinvestigatedtheapplica- explanation, explainability, large language models bilityofhumanexpertannotatorstodesignsuchcounterfactual examples[8],[9],thisisnotscalableinpractice,therebymoti- I. INTRODUCTION vating the exploration of automated counterfactual generation Over the last couple of decades, machine learning and methods. Automated counterfactual generation methods such natural language processing (NLP) systems have developed as [5],[6]usepre-trainedlanguagemodels,ormask-filling,or massively, especially in terms of the complexity and scale of modelstrainedviacontrolcodesforthegeneration.Forexam- the models used in different downstream tasks. For example, ple, training a conditional generation model in Polyjuice [5] for most NLP tasks, such as tasks in the GLUE [1] or requires sentence-pair dataset for each control code (such as: SuperGLUE [2] benchmarks, the state-of-the-art performance negation, quantifier, shuffle, lexical, etc.). Similar methods is achieved by large, black-box models such as pre-trained requiring large amounts of training and/or task-specific data language models (PLM) [3]. Effective use and deployment of are used in other automated counterfactual generation meth- such models, especially in high-stakes areas, require careful ods. However, having access to such task-specific training evaluation, validation and stress-testing. Furthermore, models datasets may be infeasible in practice, especially for newly shouldalsobeexplainableorinterpretable,i.e.,decisionsmade emergingdatadomainsandtasks.Therefore,weareinterested by such black-box models should ideally be accompanied by in investigating: Is there a way to simplify the counterfactual how and/or why the model reached that decision [4]. While generation process and perform the generation without any such endeavors are still challenging in the context of black- auxiliary data? boxmodels,inthisregard,counterfactualexampleshavebeen In order to explore this, in this work, we address a new used to perform evaluation, explanation, robustness testing problemandapplicationsetting:zero-shotcounterfactualgen- and even improvement of NLP models [5]–[7]. For example, eration for evaluating and explaining NLP models. We tackle the following two sentences - s1: This movie is brilliant!, this problem by using the power of recent state-of-the-art s2: This movie is boring. are counterfactual examples for the instruction-tunedlargelanguagemodels(LLMs).Whilerecent 4202 voN 91 ]LC.sc[ 2v39740.5042:viXra",
    "body": "Zero-shot LLM-guided Counterfactual Generation:\nA Case Study on NLP Model Evaluation\nAmrita Bhattacharjee Raha Moraffah Joshua Garland Huan Liu\nSchool of Computing and AI Department of Computer Science Global Security Initiative School of Computing and AI\nArizona State University Worcester Polytechnic Institute Arizona State University Arizona State University\nTempe, AZ, USA Worcester, MA, USA Tempe, AZ, USA Tempe, AZ, USA\nabhatt43@asu.edu rmoraffah@wpi.edu Joshua.Garland@asu.edu huanliu@asu.edu\nAbstract—With the development and proliferation of large,\ncomplex, black-box models for solving many natural language\nprocessing (NLP) tasks, there is also an increasing necessity of\nmethods to stress-test these models and provide some degree of\ninterpretability or explainability. While counterfactual examples\nareusefulinthisregard,automatedgenerationofcounterfactuals\nis a data and resource intensive process. such methods depend\non models such as pre-trained language models that are then\nfine-tuned on auxiliary, often task-specific datasets, that may\nbe infeasible to build in practice, especially for new tasks\nand data domains. Therefore, in this work we explore the\npossibility of leveraging large language models (LLMs) for zero-\nshotcounterfactualgenerationinordertostress-testNLPmodels.\nFig.1. Examplesofaninputsentenceanditscorrespondingcounterfactual\nWe propose a structured pipeline to facilitate this generation,\nexampleswithsameoroppositelabel.\nand we hypothesize that the instruction-following and textual\nunderstanding capabilities of recent LLMs can be effectively\nleveraged for generating high quality counterfactuals in a zero-\ninput sentence This movie is great. Such minimally perturbed\nshot manner, without requiring any training or fine-tuning.\nThroughcomprehensiveexperimentsonavarietyofpropreitary variations of the input text can be used in a variety of settings\nand open-source LLMs, along with various downstream tasks in to evaluate models, to understand whether a model is able to\nNLP,weexploretheefficacyofLLMsaszero-shotcounterfactual focus on the task-specific features in the input text in order to\ngenerators in evaluating and explaining black-box NLP models.\nclassify the input text correctly.\nIndex Terms—counterfactual generation, model evaluation,\nWhileseveralpreviousworkshaveinvestigatedtheapplica-\nexplanation, explainability, large language models\nbilityofhumanexpertannotatorstodesignsuchcounterfactual\nexamples[8],[9],thisisnotscalableinpractice,therebymoti-\nI. INTRODUCTION\nvating the exploration of automated counterfactual generation\nOver the last couple of decades, machine learning and methods. Automated counterfactual generation methods such\nnatural language processing (NLP) systems have developed as [5],[6]usepre-trainedlanguagemodels,ormask-filling,or\nmassively, especially in terms of the complexity and scale of modelstrainedviacontrolcodesforthegeneration.Forexam-\nthe models used in different downstream tasks. For example, ple, training a conditional generation model in Polyjuice [5]\nfor most NLP tasks, such as tasks in the GLUE [1] or requires sentence-pair dataset for each control code (such as:\nSuperGLUE [2] benchmarks, the state-of-the-art performance negation, quantifier, shuffle, lexical, etc.). Similar methods\nis achieved by large, black-box models such as pre-trained requiring large amounts of training and/or task-specific data\nlanguage models (PLM) [3]. Effective use and deployment of are used in other automated counterfactual generation meth-\nsuch models, especially in high-stakes areas, require careful ods. However, having access to such task-specific training\nevaluation, validation and stress-testing. Furthermore, models datasets may be infeasible in practice, especially for newly\nshouldalsobeexplainableorinterpretable,i.e.,decisionsmade emergingdatadomainsandtasks.Therefore,weareinterested\nby such black-box models should ideally be accompanied by in investigating: Is there a way to simplify the counterfactual\nhow and/or why the model reached that decision [4]. While generation process and perform the generation without any\nsuch endeavors are still challenging in the context of black- auxiliary data?\nboxmodels,inthisregard,counterfactualexampleshavebeen In order to explore this, in this work, we address a new\nused to perform evaluation, explanation, robustness testing problemandapplicationsetting:zero-shotcounterfactualgen-\nand even improvement of NLP models [5]–[7]. For example, eration for evaluating and explaining NLP models. We tackle\nthe following two sentences - s1: This movie is brilliant!, this problem by using the power of recent state-of-the-art\ns2: This movie is boring. are counterfactual examples for the instruction-tunedlargelanguagemodels(LLMs).Whilerecent\n4202\nvoN\n91\n]LC.sc[\n2v39740.5042:viXra\nwork has started exploring the effectiveness of using LLMs\nfor generating counterfactuals, these works use additional\nguidance, such as, in the form of few-shot exemplars for in-\ncontext learning. Given that gold-standard exemplar samples\ncan be hard to come across for many tasks, alongside LLM\ncontext length issues restricting the number of in-context\nexamples, we explore the possibility of using LLMs in a\nzero-shot manner for generating counterfactuals in order to\nevaluate and explain black-box text classifiers. Given that\nrecent LLMs are trained on massive amounts of text data,\nfollowed by subsequent supervised fine-tuning and alignment\nsteps,empiricalevidencesuggeststhatsuchLLMscanbeused\naspseudo-oraclesorgeneral-purposesolversespeciallyinNLP\ntasks[10].Asanextension,weproposetheparadigmofusing\nLLMs as zero-shot counterfactual generators for stress-testing\ntext classifier models. To further this exploration, we propose\na pipeline that leverages recent LLMs in order to generate\nplausible, human-interpretable counterfactual examples in a\ncompletely zero-shot manner. Our proposed pipeline requires\nonly the input text along with either the ground truth label or\nthe predicted label from the black-box classifier (depending\nontheuse-case)andusesastructured,hard-promptingmethod\nto use off-the-shelf LLMs for generating the counterfactuals,\nwithout any fine-tuning or training with additional data. We\nenvision that automating the task of counterfactual generation Fig.2. OurproposedFIZLEpipelineforzero-shotLLM-guidedcounterfac-\ntualgenerationforevaluationandexplanationofblack-boxtextclassifiers.\nviaacarefullydesignedpipelinethatleveragesLLMscanhelp\ntoreducecostsandmakeNLPmodeldevelopment,evaluation\nand explanation more streamlined and efficient. We use our a) Counterfactual Examples: According to most defini-\nproposed pipeline to generate counterfactuals in order to tions in literature [4], counterfactuals in text are minimally\nexploretheireffectivenessin(1)explainingand,(2)evaluating edited versions of an original text that can flip the label of a\nNLP models for a variety of downstream tasks. Our results classifier2. Counterfactuals are typically similar to the input\ndemonstrate that, when used in our pipeline, LLMs may be instance, and vary from it in a small number of features.\neffectivelyusedtogenerateeffectivezero-shotcounterfactuals Counterfactual examples may be used to stress-test trained\nthat can be used for stress-testing text classifiers. models, provide explanations in the form of counterfactual\nTo the best of our knowledge, this is the first piece of work explanations, and also for model improvement via training\ntotackletheproblemofzero-shotcounterfactualgenerationto with counterfactual examples and counterfactually augmented\nevaluate and explain text classifiers. Overall our contributions data. While several efforts have been made in the manual\nin this paper are as follows: creationofcounterfactuals[8],[9],thismethoddoesnotscale\n1) We explore a Framework for Instructed Zero-shot Coun- up and is therefore infeasible in practice for most use-cases.\nterfactual Generation with LanguagE Models, which we Automatedmethodsforcounterfactualgenerationaretherefore\nrefer to as FIZLE for brevity 1. more prevalent. Such methods often use language models\n2) We investigate and evaluate FIZLE for two important trained on some control codes for conditional text generation\nuse-cases: explaining and evaluating black-box text clas- in order to generate plausible and diverse counterfactuals [5],\nsification models. [6]. In the context of text classification, which is the scope\n3) Through experiments on three benchmark datasets, sev- of this paper, counterfactual examples can be generated from\neral open-source and proprietary LLMs, we investigate the input text via token-based substitution methods, masked-\nthe effectiveness of the proposed pipeline compared to language modeling, controlled text generation via control\nrecent baselines and discuss implications for future work codes[11].Authorsin [12]createrealisticcounterfactualsvia\nin this direction. language modeling using a Counterfactual GAN architecture.\nHowever, all of these methods use either auxiliary models\nII. BACKGROUNDANDRELATEDWORKS and/or training data, for example, to capture the style charac-\nteristics of different control codes. Some recent work has also\nInthissectionwedescribesomepreliminaryconceptsalong\nwith relevant related works. 2although,inthispaper,weoftenusethephrase“counterfactualwithsame\nlabel as input” which effectively refers to a semantically similar re-write\n1All code, prompts, supplementary materials, etc. are available at https: of the input text, i.e, similar to having undergone a label-preserving data\n//github.com/AmritaBh/zero-shot-llm-counterfactual. augmentationstep.\nexplored using large language models in few-shot settings for that in this paper, we formulate and evaluate our pipeline on\ncounterfactual generation [13]. Unlike previous work, in this the broad task of text classification, whereas formulations for\npaper, we focus on zero-shot counterfactual generation for othertexttaskscanbederivedinasimilarmanner.Tofacilitate\nstress-testing text classifiers. this we explain the following components in our framework:\nb) Large Language Models and Applications in NLP: a) InputDatasetandOtherTask-specificInput: Thefirst\nLargeLanguageModels(LLMs)areusuallytransformer-based component in our pipeline takes a task dataset as input and\nmodels capable of generating human-like text. Recent exam- pre-processes it into tuples denoted by (x ,yˆ), where x ∈\ni i i\nplesoflargelanguagemodelsincludetheGPTfamilyofmod- X denote a text sample in the input dataset X, and yˆ ∈\ni\nels from OpenAI [14]–[16], the Llama family of models from {0,1,...,k}denotethegroundtruthlabelofthecorresponding\nMeta [17]–[19], Gemini from Google [20], etc. A general input, in a k-class classification problem. Depending on the\ntrainingrecipefortrainingLLMsincludeanunsupervisedpre- use-case,wealsohaveblack-boxaccesstoatextclassification\ntrainingstep,wherethemodelistrainedusingahugecorpora modelf(·)wherebywegetf(x )=y ,whichisthepredicted\ni i\nof text, typically comprising of text from the internet [21], label. In this case, we also build tuples of the form (x ,y )\ni i\n[22], followed by one or more supervised fine-tuning steps, for use in the generation step.\nsuchasinstruction-tuning[23],[24].Morerecentstate-of-the- b) LLM as the Counterfactual Generator: We leverage\nartLLMssuchasGPT-3.5orGPT-4arefurtherfine-tunedvia recent state-of-the-art LLMs as the counterfactual generators.\nreinforcement learning with human feedback (RLHF) [25], in Given that these models have been trained on vast amounts\norderto‘align’suchmodelsmorewithhumanpreferencesand of textual data along with extensive instruction tuning, we\nvalues.Duringthefine-tuningandRLHFstagesLLMslearnto assume that LLMs can learn to modify and perturb text input\nfollow instructions for specific tasks and respond in a helpful tosimulatehowhumanannotatorsgeneratecounterfactualsfor\nmanner. Instruction-tuning essentially fine-tunes the model specific tasks [34]. For this, we use both proprietary models\non massive datasets of (instruction, response) pairs, whereby from OpenAI and open-source models from Meta AI, and\nLLMs learn to follow instructions in a prompt in order to use carefully crafted instructions and constraint prompts to\nperform tasks. The vast amount of training, both via the pre- generatethecounterfactuals.Specifically,weusethefollowing\ntraining and the instruction-tuning stages, enables LLMs to proprietary models via the OpenAI API wherever applicable:\nperform complex tasks [10], perform in-context learning [26], • GPT-3.53: Often referred to as ChatGPT. This is the\netc.RecentadvancementsinLLMshavesparkedsimultaneous modelthathasbeenexploredinavarietyoftextapplica-\nexploration and research into the applicability of these LLMs tions.Specificallyweusethegpt-3.5-turbovariant.\nonavarietyofdifferenttasks,suchasdatalabeling [27]–[29], • GPT-44:ThisisthesuccessortoGPT-3.5andisknownto\ntext classification [30], [31], model explanation [7], etc. We be more capable. Specifically this model is purported to\naddontothisemergingbodyofworkandexploreLLM-guided be able to understand complex instruction better, thereby\nzero-shotcounterfactualgenerationforNLPmodelevaluation. making it highly suitable to our task of counterfactual\nc) LLMs for Counterfactual Generation: While the use generation. We use the gpt-4 and gpt-4-turbo\nof LLMs for generating counterfactuals is still an emerging versions in our experiments.\ndirection, some recent works have started exploring the role • GPT-4o5:Thisisthemostrecentandflagshipmodelfrom\nand effectiveness of LLMs in counterfactual generation [32]. the OpenAI GPT family of models. Although this model\nWhile [13]providesathoroughevaluationonhowprompting, iscapableofsolvingmultimodaltasks(includingtextand\nmodel size, task complexity, etc. affect LLM generations of visualinputandtextoutput),weonlyuseitfortextinput.\ncounterfactuals, authors in [33] look at how large language According to OpenAI, this model is capable of solving\nmodelscanbeusedtogeneratecounterfactualdatafortraining complex, multi-step tasks, thereby making it a suitable\nsmaller language models. Authors in [7] explore the use of candidate for our task.\nLLMs for causal explainability using counterfactuals. Unlike • GPT-4o-mini6: This is a low-cost, lower-latency and\npriorworkinthisdirection,inthispaperwefocusonexploring possibly smaller7 of the previous GPT-4o model. The\nwhether LLMs can be used to generate counterfactuals in the lightweight nature of this model may be beneficial for\nzero-shot setting, specifically to evaluate and stress test black- researcherslookingtouseourpipelineforlargerdatasets.\nbox model in a post-training, pre-deployment scenario. Among the open-source models, we use the following\nmodels from Meta’s Llama family of models:\nIII. ZERO-SHOTLLM-GUIDEDCOUNTERFACTUAL\nGENERATION • Llama 2 7B: This is the 7 billion parameter version of\nLlama2model[18].Wespecificallyusethe‘chat’variant\nIn this section, we describe our counterfactual generation via Huggingface8.\nmethodology as shown in Figure 2. Following the causal\nexplanation generation procedure in prior work [7], we use 3https://platform.openai.com/docs/models/gpt-3-5-turbo\nstate-of-the-art LLMs in an off-the-shelf manner, without any\n4https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4\n5https://platform.openai.com/docs/models/gpt-4o\nfine-tuning. We improve upon prior work [7] by expanding\n6https://platform.openai.com/docs/models/gpt-4o-mini\nand broadening their pipeline into a more general framework 7ThereisnoofficialinformationregardingsizeofGPT-4ovs.GPT-4o-mini\nthat can work for tasks other than causal explanation. Note 8https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n• Llama 2 13B: This is the 13 billion parameter version of The IMDB dataset13 consists of a total of 50k highly polar\nLlama 2 model [18]. Similar to the previous one, we use movie reviews from IMDB (Internet Movie Database). Each\nthe ‘chat’ variant from Huggingface9. data instance consists of a text string comprising the review\n• Llama 3 8B: This is the 8 billion parameter variant text,andalabel,either‘negative’or‘positive’.TheAG News\nof the more recent Llama 3 model [19]. We use the dataset consists of over 120k news articles, belonging to one\n‘instruct’ variant of the model from Huggingface10,since of four news topics: ‘world’, ‘sports’, ‘business’ and ‘sci-\nitisknowntobewell-suitedforfollowinginstructionsin ence/technology’.TheStanfordNaturalLanguageInference\nthe prompt. (SNLI)dataset14 consistsof570ksentencepairsconsistingof\nDue to resource constraints, we were unable to use larger a premise and a hypothesis. Each premise-hypothesis pair is\nvariants of Llama 2 and Llama 3 models. labeled with one of ‘entailment’, ‘contradiction’ or ‘neutral’\nc) Instruction and Constraint Prompt: To generate the labels.\ncounterfactuals in a zero-shot manner using the chosen LLM,\nB. Experimental Setup\nthe prompt needs to have informative instructions and con-\nAll experiments on open-source models were performed on\nstraints to guide the generation. We generate two types of\ntwo A100 GPUs with a total of 80G memory. For 13B Llama\ncounterfactuals: (1) actual counterfactuals: that is, counter-\n2 models, we use 4-bit quantization using the optimal ‘nf4’\nfactuals that have a different label from the original input,\ndatatype[38].ForallLLMs,weusetop psamplingwithp=\naccording to the definition of counterfactual example. These\n1, temperature t = 0.4 and a repetition penalty of 1.1. We\nare used in the counterfactual explanation experiments (see\nusePyTorchandfine-tunedmodelshostedonHuggingFacein\nSectionV),and(2)counterfactualswithsamelabelasoriginal:\nboth Sections V and VI.\nThese are used in the contrast set experiments (see Section\nVI). For setting (1), we experiment with two variants of V. COUNTERFACTUALEXPLANATIONSVIA\nthe generation process: (i) naive: Here the LLM is directly LLM-GENERATEDCOUNTERFACTUALS\nprompted to generate a counterfactual, and (ii) guided: Here\nExplainability is a major challenge in many NLP appli-\nwe use a two-step process - first leveraging the LLM to\ncations such as text classification [39]–[41]. Although re-\nidentify the important input features (i.e., words) that result\ncent models involving pre-trained transformer-based language\nin the predicted label, and then prompting the same LLM\nmodels [42] have achieved or even exceeded human-level\nto edit a minimal set of those identified features to generate\nperformance on several tasks [1], [2], most of these models\nthe counterfactual. For ease of extraction of the generated\nare black-box by design and hence are not interpretable.\ncounterfactuals, we also specify an output constraint that\nSuch models do not offer transparency on why it predicted\nallowseasyparsingbasedonaregularexpressionstringmatch.\na certain label, or even what features in the input resulted\nIV. EXPERIMENTALSETTINGS in the prediction. The ubiquity of these black-box classifiers\nnecessitates the development of explanation frameworks and\nWe use the pipeline described in the previous section\ntechniquesthatprovidesomedegreeofunderstandingintothe\nin order to generate counterfactuals and demonstrate these\ndecision-making function of the model [43]. Counterfactual\nfor stress-testing and explaining black-box text classifiers.\nexplanations [4] give an insight into what could have been\nSpecifically our tasks are: (1) Counterfactual explanations\ndifferent in the input to change the output label predicted\nfor explaining decisions of black-box text classifiers and (2)\nby the classifier. Gold-standard counterfactual generation re-\nEvaluating black-box text classification models via contrast\nquires human annotators and is also task-specific [44], [45],\nsets. To facilitate this, here we go over the datasets used\nthereforemakingitanextremelyexpensiveandlabor-intensive\nand the general experimental setup for all our generation and\nendeavor. Therefore, we investigate whether we use zero-shot\nevaluation experiments:\nLLM-generatedcounterfactualsascounterfactualexplanations\nA. Datasets for black-box text classifiers.\nIn this work, we focus on two broad categories of lan- A. Methodology\nguage tasks: text classification and natural language inference\nTo generate counterfactual explanations for a black-box\n(NLI)11.Fortextclassificationweusetwodatasets:IMDB[35]\ntext classifier f(·) that predicts f(x ) = y , we use the\nfor sentiment classification and AG News12 for news topic i i\ntuple (x ,y ) in the generation step, thereby replacing the\ni i\nclassification. For NLI, we use the SNLI dataset [36], [37].\nground truth label in Figure 2 by the model-predicted label,\nThis variety of datasets allows us to evaluate the LLM-\nsince we aim to explain why the model predicted y for the\ni\ngenerated counterfactuals over a variety of label situations\ninput sample x . In our experiments, we use a DistilBERT\ni\nfrom binary to multi-class.\nmodel [46] , fine-tuned on the specific task dataset as the\nblack-box model we aim to explain. Note that since our\n9https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\n10https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct counterfactual generation process is model-agnostic, the same\n11NLIhereisalsotreatedasatextclassificationtaskwherethelabelsfor\neachinputaresimplyoneof{entailment,neutral,contradiction}. 13https://huggingface.co/datasets/imdb\n12https://huggingface.co/datasets/ag news 14https://huggingface.co/datasets/snli\nFramework Variant Prompt Structure\nStep 1: In the task of <task on task-dataset>, a trained black-box classifier correctly predicted\nthe label ‘<y >’ for the following text. Explain why the model predicted the ‘<y >’ label by\ni i\nFIZLE identifying the words in the input that caused the label.\nguided\nList ONLY the words as a comma separated list.\\n—\\nText: <x >\ni\nStep 2: Generate a counterfactual explanation for the original text by ONLY changing a minimal set of the\nwords you identified, so that the label changes from ‘<y >’ to ‘<y >’. Use the following definition of\ni cf\n‘counterfactual explanation’: “A counterfactual explanation reveals what should have been different in an\ninstance to observe a diverse outcome.” Enclose the generated text within <new> tags.\nIn the task of <task on task-dataset>, a trained black-box classifier correctly predicted the label ‘<y >’\ni\nfor the following text. Generate a counterfactual explanation by making minimal changes to the input text,\nFIZLE so that the label changes from ‘<y >’ to ‘<y >’. Use the following definition of ‘counterfactual explanation’:\nnaive i cf\n“A counterfactual explanation reveals what should have been different in an instance to observe a diverse\noutcome.” Enclose the generated text within <new> tags.\\n—\\nText: <x >.\ni\nprocedure can be applied to any black-box classifier in place b) TextualSimilarity: Counterfactualexplanationsgener-\nof DistilBERT. Inspired by prior work [7], we develop and atedbytheLLMsshouldideallybeas‘similar’totheoriginal\nexperiment with two variants of FIZLE: (1) FIZLE : input text as possible. To evaluate this similarity, we use two\nnaive\nwhich directly generates the counterfactual explanation, and metrics: similarity of the text embeddings using the Universal\n(2) FIZLE : which first extracts words that may have Sentence Encoder (USE) [48] in the latent space, and a nor-\nguided\nbeen responsible for the predicted label, and then uses those malizedLevenshteindistance[49]tomeasurewordeditsinthe\nselected words to generate a counterfactual explanation, in a token space. The semantic similarity using the embeddings of\ntwo-step manner. We hypothesize that the two-step generation theoriginalinputandthegeneratedcounterfactualiscomputed\nmay result in more effective and better quality counterfactual as the inner product of the original and the counterfactual\nexplanations, due to the additional guidance provided to the embeddings, averaged over the test dataset:\nLLM, analogous to prior work such as Chain of Thought\nn\nprompting[47].Weshowthepromptsusedinboththevariants sim = 1 (cid:88) Enc(x )·Enc(xcf) (2)\nin Table V. semantic n i i\ni=1\nB. Evaluation Metrics where Enc(·) refers to the Universal Sentence Encoder, n\nTo evaluate the goodness of the counterfactual explanations is the number of samples in the test set.\ngenerated by our zero-shot LLM-guided pipeline, we use a Levenshtein distance [49] between two strings is defined as\nvariety of evaluation metrics following prior work [5]–[7]. theminimumnumberofsinglecharactereditsthatarerequired\nIdeally, the generated counterfactual explanations should be to convert one string to another. To measure the distance be-\nabletoflipthelabeloftheclassifier,therebyshowcasingwhat tween the original input text and the generated counterfactual\ncould have changed in the input that would flip the label of in the token space we use a normalized Levenshtein distance,\nthe classifier. Furthermore, counterfactual explanations should further averaged over the test dataset. This is computed as:\nalso be minimally edited samples of the input text, i.e., they\nshould be as close as possible to the input sample both in the edit dist=\n1 (cid:88)n lev(x i,xc if)\n(3)\ntoken space and the semantic space. To capture and evaluate n max(|x |,|xcf|)\ni=1 i i\nthese criteria, we use the following metrics:\nwhere |x | and |xcf| refer to the length of x and xcf\na) Label Flip Score: We use Label Flip Score to i i i i\nrespectively, lev(·,·) refers to the Levenshtein distance, and\nmeasure the effectiveness of the generated counterfactual\nn is the number of samples in the test set.\nexplanations. For each input text x in the test split of\ni\nthe dataset, with correctly predicted label f(x ) = y , we\ni k C. Baselines\nevaluatethecorrespondingLLM-generatedcounterfactualxcf\ni Similar to other counterfactual generation methods [5],\nusing the same black-box classifier f(·) and obtain a label\n[6], we compare our proposed FIZLE pipeline with three\nfor the counterfactual. For an effective counterfactual, the\nrepresentativebaselinesfromthreecategoriesofsimilarworks:\nobtained label should be different from the original label y .\nk\n(i) BAE [50] is a recent adversarial attack method that uses\nThen Label Flip Score % (LFS) is computed as:\nmasked language modeling with BERT to perturb the input\n1 (cid:88)n text by replacing masked words; (ii) CheckList [51] is a\nLFS =\nn\n1[f(x i)̸=f(xc if)]×100 (1) method for behavioral testing of NLP models via test cases\ni=1 generated by template-based methods as well as masked\nwhere n is the number of samples in the test set and 1 is language models like RoBERTa; (iii) Polyjuice [5] is a\nthe identity function. recentcounterfactualgenerationmethodthatusesanauxiliary\nlanguage model (such as GPT-2) to generate diverse counter- output some unrelated, low-quality text that does not conform\nfactuals.Notethatunlikethesebaselines,ourFIZLEpipeline totheinstructionsprovidedintheprompt.However,thenewer\ndoes not require any additional dataset or training, thereby Llama 3 8b model outperforms both these models in most\nenabling a completely zero-shot generation. settings.\nD. Results: Effectiveness of Generated Counterfactual Expla- VI. EVALUATINGMODELSVIALLM-GENERATED\nnations COUNTERFACTUALS\nFollowingtheexperimentalsetupdescribedabove,weeval- Deep learning models such as text classification models are\nuate whether generated counterfactuals can be used to explain often trained in a supervised manner using labeled training\nblack-box classifiers for the three datasets, as compared to sets, and then evaluated on a hold-out test set. Such train-test\nthe baselines. We show these quantitative results in Table I. splits of data usually arise from the same corpus that has the\nFor each LLM, we evaluate both variants of our framework: same or similar sources and annotation guidelines. Therefore,\nFIZLE andFIZLE .Foreffectiveandgoodquality in essence, standard evaluation using such hold-out test sets\nguided naive\ncounterfactual explanations, ideally we would expect high measure merely the in-distribution performance of the model,\nvalues of LFS and semantic similarity with low values of edit while in reality, the same model may demonstrate sub-par\ndistance.Overall,weseevariedperformanceoftheLLMsand performanceonout-of-distributionorin-the-wildtestdata[8].\nthe two variants across the different tasks. Similar to other Toalleviatethisissuetosomedegree,approachessuchaseval-\ncounterfactual generation works [6], we see an obvious trade- uating using challenge sets or robustness to label-preserving\noff between the Label Flip Score and the semantic similarity. perturbations,etc.havebeenexploredbythecommunity.One\nThis is intuitive since the more the generated counterfactual specific method of stress-testing such models is via contrast\ndeviates from the original input text, higher the chances are sets [8]. A contrast set C(x) is essentially a sample of\nfor it to be a successful counterfactual for the original input points around a data point x, that is close to the local ground\n(i.e,itwouldresultinalabelflip).Amongthethreebaselines, truth decision boundary. Samples in C(x) may have same or\nweseeCheckListfailscompletelyingeneratingcounterfactual differentgroundtruthlabelasx.Inpractice,C(x)canbeaset\nexplanations.WeseesatisfactoryperformancebyBAE,except ofsamplesthatare‘close’tox,i.e.,haveminimaleditdistance\nfor the AG News dataset. For Polyjuice, even though the LFS fromx,yetbe‘challenging’foratrainedmodeltoclassify.In\nscores are high, the unsatisfactory textual similarity scores theoriginalcontrastsetswork[8],theauthorsadvocateforan\nimply that the counterfactuals generated are not good quality evaluation paradigm where dataset authors themselves create\nand deviate from the input text significantly. and release such contrast sets for model evaluation. However,\nOverall we do not see a clear winner between the two wenotethatthisishighlyinfeasibleinpractice,giventhecost\nvariants: FIZLE outperforms FIZLE . However, of expert creation of such challenging data points. Therefore,\nguided naive\nwhile the LFS scores are better in the naive variant over the automated methods for designing such challenging evaluation\nguided, the LLM is unable to preserve the textual similarity sets in the form of contrast sets are highly desirable, albeit at\nin comparison to the guided case. This may imply that the theexpenseoftradingoffexpertinsights.Onesuchautomated\nadditional ‘guidance’ provided by identifying the input words method for developing contrast sets to evaluate models is\nbefore the counterfactual explanation generation step enables that of counterfactual examples, as demonstrated by previous\nthe generation of counterfactuals closer in semantics to the work [5]. Motivated by the effectiveness of counterfactuals\noriginal input. GPT-4o and GPT-4o-mini when used in the as contrast sets in prior work [5], we envision the use of\nnaive variant of our pipeline, have the best performance for LLM-generated contrast sets as well for the same purpose of\nzero-shot counterfactual explanation generation, in terms of model evaluation. Here we describe the methodology for the\nLFS, for IMDB and AG News datasets, respectively. For nat- generationandevaluationofsuchcontrastsetsusingLLMsin\nurallanguageinferenceontheSNLIdataset,weseeallLLMs a zero-shot manner.\nstruggle to generate good counterfactual explanations. GPT-\nA. Methodology\n4operformswell,possiblyowingtoitsinstructionandtextual\nunderstanding capabilities [10], but the best performance is For generating the contrast sets, we use the same LLMs\nby the Polyjuice baseline. This poor performance of LLMs as used in Section V, and prompt each LLM to generate\nparticularly on the SNLI dataset is further evidence towards counterfactuals in a zero-shot manner using the input text\nLLMs struggling with inference and reasoning. This gap in and ground truth label tuple (x ,yˆ). Unlike [5], we do not\ni i\nthe capabilities of recent LLMs on reasoning tasks has been use human annotators to label the generated counterfactuals.\nobserved by several recent efforts as well [52], [53]. Therefore,differingfrom [5],inourevaluation,weonlyfocus\nLastly, we see the open-source models Llama 2 7B and on counterfactuals that have the same label as the original\n13Bstruggletogeneratezero-shotcounterfactualexplanations input, and use these as contrast sets. We make this choice\nwith small number of edits, thus resulting in very high edit since the lack of human annotation and lack of step-by-step\ndistances. The Llama 2 models struggle to keep the generated guidance (such as in FIZLE ) would make it harder to\nguided\ncounterfactuals semantically similar to the original input, im- validate whether the edits performed by the LLM are actually\nplying they either make too many edits to the input text, or label flipping or not. Instead, we guide the generation process\nTABLEI\nEVALUATIONRESULTSOFBOTHVARIANTSOFOURFIZLEFRAMEWORKINCOMPARISONTOBASELINES:BAE[50],CHECKLIST[51]AND\nPOLYJUICE[5].WEREPORTTHELABELFLIPSCORE(LFS),SEMANTICSIMILARITY(SEM.SIM)ANDNORMALIZEDLEVENSHTEINDISTANCE(EDIT\nDIST.).BESTLFSSCORESFOREACHDATASETAREINBOLD,SECONDBESTISUNDERLINED.\nIMDB AG News SNLI\nModel\nLFS ↑ Sem. Sim. ↑ Edit Dist. ↓ LFS ↑ Sem. Sim. ↑ Edit Dist. ↓ LFS ↑ Sem. Sim. ↑ Edit Dist. ↓\nBAE [50] 79.6 0.99 0.044 25.0 0.97 0.063 74.4 0.95 0.054\nCheckList [51] 2.6 0.99 0.013 1.6 0.92 0.083 3.0 0.96 0.036\nPolyjuice [5] 96.86 0.25 0.884 72.64 0.22 0.749 95.8 0.74 0.367\nGPT-3.5\n78.52 0.91 0.126 30.55 0.95 0.084 32.47 0.89 0.102\n(guided)\nGPT-3.5\n59.19 0.88 0.236 49.0 0.91 0.325 56.39 0.92 0.182\n(naive)\nGPT-4\n97.2 0.89 0.142 82.39 0.65 0.232 73.6 0.88 0.153\n(guided)\nGPT-4\n99.6 0.87 0.226 84.39 0.65 0.278 78.0 0.88 0.152\n(naive)\nGPT-4o\n93.56 0.85 0.389 62.5 0.74 0.1917 66.56 0.92 0.092\n(guided)\nGPT-4o\n99.57 0.72 0.56 90.12 0.41 0.549 79.39 0.90 0.157\n(naive)\nGPT-4o-mini\n98.58 0.82 0.413 66.33 0.76 0.173 47.09 0.81 0.185\n(guided)\nGPT-4o-mini\n100.0 0.63 0.642 66.6 0.59 0.485 51.4 0.83 0.282\n(naive)\nLlama 2 7B\n76.64 0.66 0.546 51.11 0.77 0.244 36.82 0.74 0.304\n(guided)\nLlama 2 7B\n64.7 0.59 0.68 35.25 0.70 0.492 58.33 0.62 0.577\n(naive)\nLlama 2 13B\n51.11 0.70 0.533 51.65 0.77 0.266 50.2 0.67 0.495\n(guided)\nLlama 2 13B\n66.67 0.52 0.715 37.63 0.58 0.606 59.95 0.55 0.621\n(naive)\nLlama 3 8B\n90.95 0.80 0.453 47.48 0.73 0.197 44.89 0.81 0.216\n(guided)\nLlama 3 8B\n97.34 0.71 0.522 75.36 0.71 0.505 61.6 0.71 0.466\n(naive)\nTABLEII\nPERFORMANCEOFFIZLE-GENERATEDCOUNTERFACTUALSASCONTRASTSETS.C.S.ACC.REFERSTOACCURACYONTHEGENERATEDCONTRAST\nSETS,ORIGINALTESTACC.ISTHETESTACCURACYONTHECORRESPONDINGPAIREDORIGINALSAMPLES.SEM.SIM.REFERSTOSEMANTIC\nSIMILARITYASCOMPUTEDBYEQ.2,EDIT.DIST.ISTHETOKENLEVELDISTANCEASCOMPUTEDBYEQ.3,CONS.(%)ISTHECONSISTENCYAS\nCOMPUTEDBYEQ.4.\nCounterfactual IMDB SNLI AGNews\ngenerator\nOriginal Edit Sem. Cons. Original Edit Sem. Cons. Original Edit Sem. Cons.\nC.s.Acc.↓ C.s.Acc.↓ C.s.Acc.↓\nTestAcc. Dist.↓ Sim.↑ %↓ TestAcc. Dist.↓ Sim.↑ %↓ TestAcc. Dist.↓ Sim.↑ %↓\nPolyjuice[5] 94.3 84.9 - - 76.1 86.5 72.3 - - 56.4 - - - - -\nExpert[8] 96.31 84.84 0.136 0.939 81.56 - - - - - - - - - -\nGPT-3.5 88.82 0.162 0.931 85.22 57.42 0.175 0.908 53.53 93.42 0.287 0.883 92.07\nGPT-4 92.65 0.157 0.942 90.95 73.01 0.277 0.841 68.82 94.0 0.352 0.855 93.0\nGPT-4o 95.4 0.133 0.953 93.2 80.61 0.237 0.882 75.15 95.2 0.406 0.829 92.8\n93.35 86.25 94.6\nGPT-4o-mini 94.0 0.394 0.873 91.0 74.34 0.343 0.842 68.89 93.8 0.452 0.818 92.0\nLlama27B 87.32 0.559 0.728 83.74 63.88 0.382 0.782 57.87 92.94 0.438 0.808 91.72\nLlama213B 84.76 0.580 0.710 82.2 48.6 0.476 0.738 43.21 93.5 0.427 0.808 92.03\nLlama38B 89.78 0.267 0.861 88.03 72.75 0.158 0.923 67.51 93.64 0.229 0.874 91.95\nviatheinstructionintheprompt.Weusethefollowingprompt dataset seems to be the least while interestingly, we see the\nto perform the generation: highestperformancedroponcontrastsetsfortheSNLIdataset.\nFurthermore, we see GPT-3.5 and GPT-4 are able to create\n‘You are a robustness checker for a machine learning\ncontrast sets with high degree of semantic similarity and\nalgorithm. In the task of <task >, the following data\ni\nlow edit distance, thus being more desirable over Llama 2\nsample has the ground truth label <yˆ>. Make minimal\ni\ngenerated contrast sets. Interestingly, we see that GPT-4o and\nchanges to the data sample to create a more challenging\nGPT-4o-mini contrast sets often have better performance than\ndata point while keeping the ground truth label the same.\ntheoriginaltestset,whichcouldimplythatthesecontrastsets\nText: <x >’\ni\nare too ‘easy’, and therefore not functional. Overall, for the\nwhere, task is the description of the task, such as “senti-\ni other LLMs, the drop in accuracy and the consistency values\nment classification”, x is the input text, and yˆ is the ground\ni i seem analogous to similar results in literature (average drop\ntruth label.\nin classification accuracy of around 6.8% according to [5])\nthatusehuman-generatedcontrastsetsforevaluation [5],[8].\nB. Evaluation Metrics\nEvaluatingmodelswithsuchLLM-generatedcontrastsetsmay\nForevaluatingthegoodnessofthegeneratedcounterfactuals\nthereby allow the model developer to investigate what type of\nas contrast sets, we compare the accuracy of the target model\nsamples the model is failing on, thereby informing choices\nf(·) on both the original test set and the generated contrast\nregarding further robustness training.\nset. Following [5], we also measure the consistency,\nWhile this is promising, we do note the ethical concerns\nsurrounding this: LLM-generated contrast sets may induce\nn\n1 (cid:88) pre-existing biases that can propagate further bias and errors\nconsistency = 1[f(x )=yˆ ∧f(xcs)=yˆcs]×100\nn i i i i through evaluation and subsequent model improvement steps.\ni=1\n(4) OnehybridwaytoeffectivelyuseLLM-generatedcontrastsets\nwherexcs istheLLM-generatedcontrastsetfortheoriginal is by broadly identifying the failure models of the model via\ni\ninput x , yˆcs is the ground truth label for the contrast set probingthemodelusingtheLLM-generatedcontrastsets,and\ni i\nexample and n is the number of test samples. Consistency then employing human annotators or data creators to hone in\nmeasures the percentage of times when the model correctly on that specific failure mode to either generate more contrast\nclassifies both the original and the contrast set example. sets or counterfactually augmented training data to fill the\nLike the previous set of experiments, we want the generated identified gap.Such a combinedmethod wouldgreatly reduce\ncounterfactuals (or contrast sets) to be as close to the original costs while still being effective in terms of model evaluation\ntextinputaspossible,i.e.,theeditsshouldideallybeminimal. and development.\nTherefore, we capture the textual similarity again in the token\nVII. CASESTUDY:HOWHASTHEPERFORMANCEOF\nspace via Equation 3 and the latent space via Equation 2.\nLLMSEVOLVEDOVERTIME?\nC. Baselines\nIn order to use LLMs for zero-shot counterfactual genera-\nSince there is not much work on contrast sets, we have a tion for stress-testing models, one needs to account for the\nlimitedsetofbaselineshere.Weusetheoriginalexpert-created fast evolving landscape of LLM development and release.\ncontrast sets for the IMDB dataset from the original work [8]. These models, especially proprietary ones, undergo frequent\nThis consists of 488 original test data samples, and 488 model updates. Older models are often deprecated, making\ncontrast samples created by the dataset experts. Furthermore, adoption of such solutions challenging without thorough re-\nweusePolyjuice-generatedcounterfactuals[5]ascontrastsets evaluation of pipelines such as ours. To explore the effect of\nfor comparison. model release and deprecation life cycles, we conduct a small\ncase study to evaluate how performance of this method of\nD. Results: Effectiveness of Counterfactuals as Contrast Sets\ngenerating counterfactuals vary over time. We do this by re-\nWe use the same DistilBERT models as in Section V that portingnaivegenerationresultsforcounterfactualexplanations\nare fine-tuned for each of the 3 tasks (IMDB, SNLI and AG for GPT family of models. Models we use here, from oldest\nNews).Weevaluateeachofthese3modelsonboththeoriginal (2022) to newest (2024), are: text-davinci-00315 →\ntest set and the counterfactual one (i.e., the contrast sets) and GPT-3.5 → GPT-4 → GPT-4o. We show this comparison\nshow these results in Table II. We obtain the performance in Figure 3. Keeping the expected trade-off between LFS and\nvalues for Polyjuice-generated contrast sets from the original semantic similarity in mind, we see that for a task like NLI,\npaper [5]. For the ‘Expert’ baseline, the IMDB contrast sets newer models in the GPT family perform better, achieving\narecreatedbyhumanexpertsin [8].Unfortunately,theredoes high LFS scores while retaining semantic similarity, whereas\nnot exist any expert created contrast set for SNLI and AG forAGNews,performanceisvaried:recentmodelslikeGPT-\nNewsdatasets.Asevidentfromthetestaccuraciesonboththe 4 and GPT-4o achieve higher LFS but at the cost of textual\noriginaltestsetandthecounterfactualone,weseeaconsistent similarity. While a more thorough evaluation is required in\ndecreaseinperformanceonthegeneratedcounterfactualsover\nthe original samples. The performance drop for the AG News 15https://platform.openai.com/docs/deprecations\nand correctness of generated contrast sets. Further exploration\ncouldlookintocategorizingthefailuremodesoftheblack-box\nmodels, after being evaluated by these generated counterfac-\ntuals.Thisinformationcanthenbeusedtocollectorgenerate\nmore data for robustness training of these models. Since\none of the challenges in our method was to ensure that the\ngenerated text is actually a counterfactual, devising ways and\nhumanannotationstoensuremoreconformityofthegenerated\ncounterfactualexplanationstothedefinitionof‘counterfactual\nexplanation’ is also be an area that can be improved. Finally,\napart from these two use-cases of counterfactuals, LLM-\ngenerated counterfactuals can also be evaluated in tasks such\nasmodeltrainingorimprovement,uncoveringbiasesinmodel\npredictions, incorporating fairness into model predictions, etc.\nACKNOWLEDGMENTS\nThis work is supported by the DARPA SemaFor project\n(HR001120C0123). The views, opinions and/or findings ex-\npressed are those of the authors.\nFig.3. ComparisonofLFSandsemanticsimilarity(Sem-sim)forgenerated\nREFERENCES\ncounterfactual explanations for AG News (top) and SNLI (bottom). LFS %\nscaledto0-1,highervaluesforbotharebetter.dv-003referstotext-davinci-\n[1] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n003.\n“Glue: A multi-task benchmark and analysis platform for natural lan-\nguageunderstanding,”arXivpreprintarXiv:1804.07461,2018.\n[2] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,\norder to make broad claims, however, given these tasks and O.Levy,andS.Bowman,“Superglue:Astickierbenchmarkforgeneral-\nmodels, recent models like GPT-4o seem to be better at purposelanguageunderstandingsystems,”Advancesinneuralinforma-\ntionprocessingsystems,vol.32,2019.\nmore complex tasks such as reasoning and inference, while a\n[3] X. Liu, T. Sun, J. He, J. Wu, L. Wu, X. Zhang, H. Jiang, Z. Cao,\nrelatively older model like GPT-4 may be sufficient for other X. Huang, and X. Qiu, “Towards efficient nlp: A standard evaluation\ntypes of tasks. andastrongbaseline,”arXivpreprintarXiv:2110.07038,2021.\n[4] C.Molnar,Interpretablemachinelearning. Lulu.com,2020.\n[5] T.Wu,M.T.Ribeiro,J.Heer,andD.S.Weld,“Polyjuice:Generating\nVIII. CONCLUSION&FUTUREWORK\ncounterfactuals for explaining, evaluating, and improving models,” in\nProceedings of the 59th Annual Meeting of the ACL and the 11th\nIn this paper, we explore the possibility of using LLM-\nIJCNLP(Volume1:LongPapers),2021,pp.6707–6723.\ngenerated zero-shot counterfactuals for stress-testing black-\n[6] N. Madaan, I. Padhi, N. Panwar, and D. Saha, “Generate your coun-\nbox text classification models. To this end, we propose a terfactuals: Towards controlled counterfactual generation for text,” in\npipeline, with two variants, for generation of such counterfac-\nProceedingsoftheAAAIConferenceonArtificialIntelligence,vol.35,\nno.15,2021,pp.13516–13524.\ntuals in a zero-shot manner. We conduct experiments with a\n[7] A. Bhattacharjee, R. Moraffah, J. Garland, and H. Liu, “Towards llm-\nvariety of proprietary and open LLMs, and evaluate generated guidedcausalexplainabilityforblack-boxtextclassifiers,”inAAAI2024\ncounterfactuals on two broad NLP model development tasks: Workshop on Responsible Language Models, Vancouver, BC, Canada,\n2024.\n(1) counterfactual explanation of black-box text classifiers,\n[8] M.Gardner,Y.Artzi,V.Basmov,J.Berant,B.Bogin,S.Chen,P.Dasigi,\nand (2) evaluation of black-box text classification models via D. Dua, Y. Elazar, A. Gottumukkala et al., “Evaluating models’ local\ncontrast sets. Our results are promising and we see benefits to decisionboundariesviacontrastsets,”inFindingsoftheACL:EMNLP\n2020,2020,pp.1307–1323.\nusing our proposed FIZLE pipeline across the two use-cases\n[9] L. Qin, A. Bosselut, A. Holtzman, C. Bhagavatula, E. Clark, and\nandthreedownstreamtasks.Ourfindingssuggesttheeffective Y.Choi,“Counterfactualstoryreasoningandgeneration,”inProceedings\nuse of LLM-generated counterfactuals for explaining black- ofEMNLP-IJCNLP,2019,pp.5043–5053.\n[10] S.Bubeck,V.Chandrasekaran,R.Eldan,J.Gehrke,E.Horvitz,E.Ka-\nbox NLP models, as well as potentially identifying failure\nmar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg et al., “Sparks of artificial\nmodels of NLP models via evaluation with contrast sets. We general intelligence: Early experiments with gpt-4,” arXiv preprint\nfurther discuss implications and how hybrid human-and-AI arXiv:2303.12712,2023.\n[11] N. Madaan, S. Bedathur, and D. Saha, “Plug and play counterfactual\nmethodsmaybenefitfromourexploration,alongwithlooking\ntextgenerationformodelrobustness,”arXivpreprintarXiv:2206.10429,\nat how model updates over time can affect the quality and 2022.\nperformance of generated counterfactuals. [12] M. Robeer, F. Bex, and A. Feelders, “Generating realistic natural lan-\nguagecounterfactuals,”inFindingsoftheAssociationforComputational\nFutureworkmayinvestigatemodificationstothegeneration\nLinguistics:EMNLP2021,2021,pp.3611–3625.\npipeline such as plugging in an additional model, perhaps a [13] Y. Li, M. Xu, X. Miao, S. Zhou, and T. Qian, “Prompting large\nsmall language mode (SLM) to evaluate label of generated languagemodelsforcounterfactualgeneration:Anempiricalstudy,”in\nProceedings of the 2024 Joint International Conference on Computa-\ncontrast sets. More effort can also be put into validating\ntionalLinguistics,LanguageResourcesandEvaluation(LREC-COLING\nthe faithfulness of the generated counterfactual explanation 2024),2024,pp.13201–13221.\n[14] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,I.Sutskeveretal., for Computational Linguistics, June 2011, pp. 142–150. [Online].\n“Language models are unsupervised multitask learners,” OpenAI blog, Available:http://www.aclweb.org/anthology/P11-1015\nvol.1,no.8,p.9,2019. [36] B. MacCartney and C. D. Manning, “Modeling semantic containment\n[15] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal, andexclusioninnaturallanguageinference,”inProceedingsofthe22nd\nA.Neelakantan,P.Shyam,G.Sastry,A.Askelletal.,“Languagemod- International Conference on Computational Linguistics (Coling 2008),\nels are few-shot learners,” Advances in neural information processing 2008,pp.521–528.\nsystems,vol.33,pp.1877–1901,2020. [37] S.Bowman,G.Angeli,C.Potts,andC.D.Manning,“Alargeannotated\n[16] OpenAI,“Gpt-4technicalreport,”arXiv,pp.2303–08774,2023. corpus for learning natural language inference,” in Proceedings of\n[17] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, EMNLP,2015,pp.632–642.\nT. Lacroix, B. Rozie`re, N. Goyal, E. Hambro, F. Azhar et al., [38] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora:\n“Llama:Openandefficientfoundationlanguagemodels,”arXivpreprint Efficientfinetuningofquantizedllms,”arXivpreprintarXiv:2305.14314,\narXiv:2302.13971,2023. 2023.\n[18] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, [39] M.T.Ribeiro,S.Singh,andC.Guestrin,“”whyshoulditrustyou?”\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama explainingthepredictionsofanyclassifier,”inProceedingsofthe22nd\n2: Open foundation and fine-tuned chat models,” arXiv preprint ACM SIGKDD international conference on knowledge discovery and\narXiv:2307.09288,2023. datamining,2016,pp.1135–1144.\n[19] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, [40] P. Atanasova, J. G. Simonsen, C. Lioma, and I. Augenstein, “A di-\nA. Mathur, A. Schelten, A. Yang, A. Fan et al., “The llama 3 herd of agnostic study of explainability techniques for text classification,” in\nmodels,”arXivpreprintarXiv:2407.21783,2024. ProceedingsofEMNLP,2020,pp.3256–3274.\n[41] O.-M. Camburu, T. Rockta¨schel, T. Lukasiewicz, and P. Blunsom, “e-\n[20] G.Team,R.Anil,S.Borgeaud,Y.Wu,J.-B.Alayrac,J.Yu,R.Soricut,\nsnli: Natural language inference with natural language explanations,”\nJ.Schalkwyk,A.M.Dai,A.Hauthetal.,“Gemini:afamilyofhighly\nAdvancesinNeuralInformationProcessingSystems,vol.31,2018.\ncapablemultimodalmodels,”arXivpreprintarXiv:2312.11805,2023.\n[42] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,\n[21] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nH.Alobeidli,B.Pannier,E.Almazrouei,andJ.Launay,“Therefinedweb\nneuralinformationprocessingsystems,vol.30,2017.\ndataset for falcon llm: Outperforming curated corpora with web data,\n[43] S. Ali, T. Abuhmed, S. El-Sappagh, K. Muhammad, J. M. Alonso-\nandwebdataonly,”CoRR,2023.\nMoral, R. Confalonieri, R. Guidotti, J. Del Ser, N. D´ıaz-Rodr´ıguez,\n[22] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\nandF.Herrera,“Explainableartificialintelligence(xai):Whatweknow\nJ. Phang, H. He, A. Thite, N. Nabeshima et al., “The pile: An\nandwhatislefttoattaintrustworthyartificialintelligence,”Information\n800gb dataset of diverse text for language modeling,” arXiv preprint\nFusion,vol.99,p.101805,2023.\narXiv:2101.00027,2020.\n[44] D. Kaushik, E. Hovy, and Z. Lipton, “Learning the difference that\n[23] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nmakes a difference with counterfactually-augmented data,” in ICLR,\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\n2020.[Online].Available:https://openreview.net/forum?id=Sklgs0NFvr\nmodelstofollowinstructionswithhumanfeedback,”AdvancesinNeural\n[45] D. Khashabi, T. Khot, and A. Sabharwal, “More bang for your buck:\nInformationProcessingSystems,vol.35,pp.27730–27744,2022.\nNatural perturbation for robust question answering,” in Proceedings of\n[24] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li,\nEMNLP,2020,pp.163–170.\nX.Wang,M.Dehghani,S.Brahmaetal.,“Scalinginstruction-finetuned\n[46] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled\nlanguagemodels,”JMLR,vol.25,no.70,pp.1–53,2024.\nversion of bert: smaller, faster, cheaper and lighter,” arXiv preprint\n[25] P.F.Christiano,J.Leike,T.Brown,M.Martic,S.Legg,andD.Amodei, arXiv:1910.01108,2019.\n“Deep reinforcement learning from human preferences,” Advances in [47] J.Wei,X.Wang,D.Schuurmans,M.Bosma,F.Xia,E.Chi,Q.V.Le,\nneuralinformationprocessingsystems,vol.30,2017. D. Zhou et al., “Chain-of-thought prompting elicits reasoning in large\n[26] Q.Dong,L.Li,D.Dai,C.Zheng,J.Ma,R.Li,H.Xia,J.Xu,Z.Wu, languagemodels,”Advancesinneuralinformationprocessingsystems,\nB. Chang et al., “A survey on in-context learning,” in Proceedings vol.35,pp.24824–24837,2022.\nof the 2024 Conference on Empirical Methods in Natural Language [48] D. Cer, Y. Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. S. John,\nProcessing,2024,pp.1107–1128. N.Constant,M.Guajardo-Cespedes,S.Yuan,C.Taretal.,“Universal\n[27] X. He, Z. Lin, Y. Gong, A. Jin, H. Zhang, C. Lin, J. Jiao, S. M. Yiu, sentenceencoder,”arXivpreprintarXiv:1803.11175,2018.\nN.Duan,W.Chenetal.,“Annollm:Makinglargelanguagemodelsto [49] V.I.Levenshteinetal.,“Binarycodescapableofcorrectingdeletions,\nbe better crowdsourced annotators,” arXiv preprint arXiv:2303.16854, insertions, and reversals,” in Soviet physics doklady, vol. 10, no. 8.\n2023. SovietUnion,1966,pp.707–710.\n[28] P. Bansal and A. Sharma, “Large language models as annotators: [50] S. Garg and G. Ramakrishnan, “Bae: Bert-based adversarial examples\nEnhancinggeneralizationofnlpmodelsatminimalcost,”arXivpreprint fortextclassification,”inProceedingsofEMNLP,2020,pp.6174–6181.\narXiv:2306.15766,2023. [51] M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh, “Beyond accuracy:\n[29] Z. Tan, A. Beigi, S. Wang, R. Guo, A. Bhattacharjee, B. Jiang, Behavioraltestingofnlpmodelswithchecklist,”inProceedingsofthe\nM. Karami, J. Li, L. Cheng, and H. Liu, “Large language models for 58thAnnualMeetingoftheACL,2020,pp.4902–4912.\ndataannotation:Asurvey,”arXivpreprintarXiv:2402.13446,2024. [52] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\n[30] X.Sun,X.Li,J.Li,F.Wu,S.Guo,T.Zhang,andG.Wang,“Textclas- J.Aslanides,S.Henderson,R.Ring,S.Youngetal.,“Scalinglanguage\nsificationvialargelanguagemodels,”arXivpreprintarXiv:2305.08377, models: Methods, analysis & insights from training gopher,” arXiv\n2023. preprintarXiv:2112.11446,2021.\n[31] A.BhattacharjeeandH.Liu,“Fightingfirewithfire:canchatgptdetect [53] K.Valmeekam,A.Olmo,S.Sreedharan,andS.Kambhampati,“Large\nai-generated text?” ACM SIGKDD Explorations Newsletter, vol. 25, languagemodelsstillcan’tplan(abenchmarkforllmsonplanningand\nno.2,pp.14–21,2024. reasoningaboutchange),”inNeurIPS2022FMDMWorkshop.\n[32] V. B. Nguyen, P. Youssef, J. Schlo¨tterer, and C. Seifert, “Llms for\ngenerating and evaluating counterfactuals: A comprehensive study,”\narXivpreprintarXiv:2405.00722,2024.\n[33] Z.Chen,Q.Gao,A.Bosselut,A.Sabharwal,andK.Richardson,“Disco:\nDistilling counterfactuals with large language models,” arXiv preprint\narXiv:2212.10534,2022.\n[34] F. Gilardi, M. Alizadeh, and M. Kubli, “Chatgpt outperforms crowd\nworkersfortext-annotationtasks,”ProceedingsoftheNationalAcademy\nofSciences,vol.120,no.30,p.e2305016120,2023.\n[35] A.L.Maas,R.E.Daly,P.T.Pham,D.Huang,A.Y.Ng,andC.Potts,\n“Learning word vectors for sentiment analysis,” in Proceedings of the\n49thAnnualMeetingoftheAssociationforComputationalLinguistics:\nHuman Language Technologies. Portland, Oregon, USA: Association",
    "pdf_filename": "Zero-shot_LLM-guided_Counterfactual_Generation_A_Case_Study_on_NLP_Model_Evaluation.pdf"
}