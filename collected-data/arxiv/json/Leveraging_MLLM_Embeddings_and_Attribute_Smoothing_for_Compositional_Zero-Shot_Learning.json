{
    "title": "Leveraging MLLM Embeddings and Attribute Smoothing for",
    "abstract": "unseen attribute-object compositions by leveraging knowl- Compositional zero-shot learning (CZSL) aims to recog- edgeofattributesandobjects(i.e.,primitives)learnedfrom nize novel compositions of attributes and objects learned seencompositions. Specifically,inthetrainingphase,mod- from seen compositions. Previous works disentangle at- elsareprovidedwithimagesandcompositionallabels(e.g., tributeandobjectbyextractingsharedandexclusiveparts ripe orange and peeled apple). During the test- betweenimagepairssharingthesameattribute(object),as ing phase, given an image depicting a novel composition well asaligning themwith pretrained wordembeddings to (e.g., peeled orange), models are assigned to classify improve unseen attribute-object recognition. Despite the theimageintothecorrespondingcategory[44]. significant achievements of existing efforts, they are ham- Prior works [22, 27] focus on mapping the visual fea- pered by three limitations: (1) the efficacy of disentangle- turesandthewordembeddingsofcompositionsintoajoint mentiscompromisedduetotheinfluenceofthebackground space. These methods have poor generalization capabil- and the intricate entanglement of attribute with object in itytounseencompositions,astheyfailtolearnprimitives. the same parts. (2) existing word embeddings fail to cap- Therefore, recentstudies[8,14,38]considervisualdisen- ture complex multimodal semantic information. (3) over- tanglement. Amongthem,someprominentworksdeploya confidence exhibited by existing models in seen composi- tripletofimagestodisentangle:agivenimage(notedasthe tions hinders their generalization to novel compositions. mainimage),andtwosupplementaryimages,eachsharing Beingawareofthese,weproposeanovelframeworknamed eitherthesameattributeorthesameobjectasthemainim- Multimodal Large Language Model (MLLM) embeddings age. Thetripletofimagesistreatedastwoimagepairsfor andatTRibutesmoothIngguiDEddiseNTanglement(TRI- subsequent analysis. These approaches aim to disentangle DENT) for CZSL. First, we leverage feature adaptive ag- attribute and object by analyzing the shared and exclusive gregation modules to mitigate the impact of background, features of the image pair, as well as aligning them with andutilizelearnableconditionmaskstocapturemultigran- wordembeddings(e.g.,GloVe[32]),asshowninFigure1. ularity features for disentanglement. Then, the last hid- Althoughthesepioneerresearchstudieshaveachievedgreat denstatesofMLLMareemployedaswordembeddingsfor progress,theyexhibitthreelimitations: their superior representation capabilities. Moreover, we L1: Disentanglementisimpededduetotheinfluenceof propose attribute smoothing with auxiliary attributes gen- the background and the intricate entanglement of attribute eratedbyLargeLanguageModel(LLM)forseencomposi- with object in the same parts of image. On the one hand, tions, addressing the issue of overconfidence by encourag- models tend to extract the background feature unique to ingthemodeltolearnmoreattributesinonegivencompo- oneimageinthepairasthedisentangledexclusivefeatures. sition. Extensive experiments demonstrate that TRIDENT On the other hand, some existing methods [36, 38] com- achievesstate-of-the-artperformanceonthreebenchmarks. putethesimilarityofimagepairsfordisentanglementatthe spatial level. However, disentangling attribute and object at the spatial level presents significant challenges because theyentangleinthesamespatialfeatures. Takinganimage 1.Introduction ofripe appleasanexample,thespatialregionscorre- sponding to the attribute ”ripe” and the object ”apple” Asforthestudyofcompositionalgeneralizationabilityin- arefullyco-located. herenttohumans,compositionalzero-shotlearning(CZSL) L2: Existing word embeddings lack the depth needed *Correspondingauthor to capture complex multimodal semantic information. To 1 4202 voN 81 ]VC.sc[ 1v48521.1142:viXra",
    "body": "Leveraging MLLM Embeddings and Attribute Smoothing for\nCompositional Zero-Shot Learning\nXudongYan1, SongheFeng1*, YangZhang1, JianYang2, YueguanLin2, HaojunFei2\n1SchoolofComputerScienceandTechnology,BeijingJiaotongUniversity 2QifuTechnology\n{xud yan, shfeng, chefzhang}@bjtu.edu.cn, {feihaojun, yangjian1, linyueguan}-jk@360shuke.com\nAbstract [22, 26, 33] is proposed to enable machines to recognize\nunseen attribute-object compositions by leveraging knowl-\nCompositional zero-shot learning (CZSL) aims to recog- edgeofattributesandobjects(i.e.,primitives)learnedfrom\nnize novel compositions of attributes and objects learned seencompositions. Specifically,inthetrainingphase,mod-\nfrom seen compositions. Previous works disentangle at- elsareprovidedwithimagesandcompositionallabels(e.g.,\ntributeandobjectbyextractingsharedandexclusiveparts ripe orange and peeled apple). During the test-\nbetweenimagepairssharingthesameattribute(object),as ing phase, given an image depicting a novel composition\nwell asaligning themwith pretrained wordembeddings to (e.g., peeled orange), models are assigned to classify\nimprove unseen attribute-object recognition. Despite the theimageintothecorrespondingcategory[44].\nsignificant achievements of existing efforts, they are ham- Prior works [22, 27] focus on mapping the visual fea-\npered by three limitations: (1) the efficacy of disentangle- turesandthewordembeddingsofcompositionsintoajoint\nmentiscompromisedduetotheinfluenceofthebackground space. These methods have poor generalization capabil-\nand the intricate entanglement of attribute with object in itytounseencompositions,astheyfailtolearnprimitives.\nthe same parts. (2) existing word embeddings fail to cap- Therefore, recentstudies[8,14,38]considervisualdisen-\nture complex multimodal semantic information. (3) over- tanglement. Amongthem,someprominentworksdeploya\nconfidence exhibited by existing models in seen composi- tripletofimagestodisentangle:agivenimage(notedasthe\ntions hinders their generalization to novel compositions. mainimage),andtwosupplementaryimages,eachsharing\nBeingawareofthese,weproposeanovelframeworknamed eitherthesameattributeorthesameobjectasthemainim-\nMultimodal Large Language Model (MLLM) embeddings age. Thetripletofimagesistreatedastwoimagepairsfor\nandatTRibutesmoothIngguiDEddiseNTanglement(TRI- subsequent analysis. These approaches aim to disentangle\nDENT) for CZSL. First, we leverage feature adaptive ag- attribute and object by analyzing the shared and exclusive\ngregation modules to mitigate the impact of background, features of the image pair, as well as aligning them with\nandutilizelearnableconditionmaskstocapturemultigran- wordembeddings(e.g.,GloVe[32]),asshowninFigure1.\nularity features for disentanglement. Then, the last hid- Althoughthesepioneerresearchstudieshaveachievedgreat\ndenstatesofMLLMareemployedaswordembeddingsfor progress,theyexhibitthreelimitations:\ntheir superior representation capabilities. Moreover, we L1: Disentanglementisimpededduetotheinfluenceof\npropose attribute smoothing with auxiliary attributes gen- the background and the intricate entanglement of attribute\neratedbyLargeLanguageModel(LLM)forseencomposi- with object in the same parts of image. On the one hand,\ntions, addressing the issue of overconfidence by encourag- models tend to extract the background feature unique to\ningthemodeltolearnmoreattributesinonegivencompo- oneimageinthepairasthedisentangledexclusivefeatures.\nsition. Extensive experiments demonstrate that TRIDENT On the other hand, some existing methods [36, 38] com-\nachievesstate-of-the-artperformanceonthreebenchmarks. putethesimilarityofimagepairsfordisentanglementatthe\nspatial level. However, disentangling attribute and object\nat the spatial level presents significant challenges because\ntheyentangleinthesamespatialfeatures. Takinganimage\n1.Introduction ofripe appleasanexample,thespatialregionscorre-\nsponding to the attribute ”ripe” and the object ”apple”\nAsforthestudyofcompositionalgeneralizationabilityin-\narefullyco-located.\nherenttohumans,compositionalzero-shotlearning(CZSL)\nL2: Existing word embeddings lack the depth needed\n*Correspondingauthor to capture complex multimodal semantic information. To\n1\n4202\nvoN\n81\n]VC.sc[\n1v48521.1142:viXra\nripe ripe\nyellow\nripe apple “ripe” y ree dllow ripe apple “ripe” red\n“apple” apple “apple” apple orange\norange\ncar car\npeeled apple “peeled” peeled apple “peeled”\nExisting Method TRIDENT (Ours)\nFigure1. AgeneralcomparisonbetweentheexistingmethodandourproposedTRIDENT.Notethat,weonlypresenttherepresentation\nlearningofanimagepairsharingtheobjectforbrevity.\nbeginwith, wordembeddings, suchasWord2Vec[21]and based on attribute-object compositions and perform label\nGloVe [32], are grounded in word frequency and contex- smoothingforattributes,i.e.,attributesmoothing.\ntualco-occurrence,ratherthancapturinghigh-levelseman- Insummary,thecontributionsofourworkarethree-fold:\ntic nuances[39]. Moreover, the process of aligning visual 1. Weproposenovelfeatureadaptiveaggregationmod-\nfeatures with word embeddings can be viewed as a form ulestoreducetheimpactofbackground, andutilizelearn-\nofcross-modalmatching;however,thesewordembeddings able condition masks to capture multi-granularity features\nare trained only in a single text modal, failing to capture fordisentanglementinCZSL.\ncross-modalinformationbetweenimagesandtexts. 2. WeemploybothLLMandMLLMtoguideattribute-\nL3: Existing methods display excessive confidence in object disentanglement by generating auxiliary attributes\nseencompositions, impairingtheirabilitytogeneralizeto- and representing word embeddings, respectively. To the\nward novel compositions. Due to the one-hot label used bestofourknowledge,wearethefirsttoleveragebothLLM\nduring training, these approaches are limited by learning andMLLMtoadvancedisentanglementinCZSLtask.\nonlyoneattributeandobject,neglectingthefactthatobjects 3. We conduct extensive experiments to evaluate our\nnaturally exhibit multiple attributes [43]. Consequently, method on three CZSL benchmarks, showing that TRI-\nmodelsexhibitoverconfidenceinthedisentangledground- DENT has achieved state-of-the-art performance. The\ntruthattribute,treatingotherattributesthatcandescribethe\nsourcecodewillbereleasedsoon1.\nobjectasnegativeattributes,whichresultsinthediminished\nperformanceonunseencompositions. 2.RelatedWork\nTo address the aforementioned limitations, we propose Compositionalzero-shotlearning(CZSL).Priorworksin\na novel framework named Multimodal Large Language CZSL can be broadly divided into two main streams. One\nModel (MLLM) embeddings and atTRibute smoothIng main stream is to learn representations of compositions in\nguiDEddiseNTanglement(TRIDENT),whichconsistsof a joint space. SymNet [15] proposes to learn symmetry\nthree major modules: visual feature extraction, attribute- propertyincompositions. Co-CGE[20]leveragesaGraph\nobject disentanglement, and feature alignment. The first ConvolutionalNeuralNetworktolearncompositionalrep-\nmoduleleveragesfeatureadaptiveaggregation(FAA)mod- resentations. The other main stream aims at disentangling\nules to mitigate the impact of background noise, and ex- visual representations of primitives to reduce composition\nploits learnable condition masks to learn multi-granularity learningintoprimitivelearning. SCEN[13]leveragescon-\nfeatures to improve subsequent disentanglement. The sec- trastivelosstoexcavatediscriminativeprototypesofprimi-\nondmoduleaimsatleveragingsharedandexclusiveweights tives. OADis[38]disentanglesprimitivesbyaffinitymod-\nofimagepairstodisentangleattributeandobjectunderthe ules. CANet [42] learns conditional attribute conditioned\ntheparadigmthatapartfromthesharedfeaturesoftheim- ontherecognizedobjectandtheinputimage.\nage pair, each image has its own exclusive features. The More recent works [9, 18, 28] focus on leveraging\nthirdmoduleisintendedtoalignthevisualfeaturesofcom- the encyclopedic knowledge of pretrained vision-language\npositions and disentangled primitives with the last hidden models (VLM), such as Contrastive Language-Image Pre-\nstatesofanMLLM,LLaVAv1.5[16],i.e.,MLLMembed-\ntraining (CLIP) [35] and Context Optimization (CoOp)\ndings. This is inspired by some works [12, 23, 24, 41], [46],toencodeandalignimagesandtexts.\nwhich find that the last hidden states of (M)LLM exhibit\nLarge language model (LLM). LLMs have realized\npowerful representational capabilities in embedding tasks,\nsignificant advancements thanks to the scaling up of train-\nsuchasretrievalandclassification. Moreover,totacklethe\ningdataandtheincreaseinthenumberofparameters.Early\nissue that the ineffective overconfidence of the models re-\nmodels,suchasBERT[6]andGPT-2[34],initiallyexhibit\ngardingground-truthattributehindersthemfromgeneraliz-\nstrongcapabilitiesinunderstandingandgeneratinghuman-\ning to unseen compositions, we exploit a Large Language\nModel(LLM),GPT-3.5[29]togenerateauxiliaryattributes 1https://github.com/xud-yan/Trident\n2\n...... ......\nlustrous\ncrisp,\ncrimson,\nbeige,\nred,\nrefresh,\nLLM\napple\napple\nripe\npeeled\nlike language. Subsequently, GPT-3 [4] and LLaMA [40] tanglementframework(TRIDENT)forCZSL,asshownin\ndemonstrategreatbreakthroughsacrossnumerouslanguage Figure 2. It consists of three major modules: (1) visual\nbenchmarks. Moreover, by performing instruction fine- featureextraction,(2)attribute-objectdisentanglement,and\ntuningonLLM,ChatGPT[30]andVicuna[5,45]areable (3)featurealignment. WenowdetaileachmoduleofTRI-\ntocomprehendandfollowhumaninstructionsbetter. DENTinthissection.\nExpandingonLLM,MultimodalLargeLanguageModel\n3.2.1.VisualFeatureExtraction\n(MLLM) incorporates a pretrained visual encoder for\nAs shown in Figure 2, we denote a given image with the\nvision-language tasks. Flamingo [1] first integrates Vision\nattribute-object composition label (e.g. ripe apple) as\nTransformer (ViT) [7] and LLM by gated cross-attention.\nthe main image xm, and randomly sample an image with\nRecently, LLaVA [17] and LLaVA v1.5 [16] introduce vi-\nthe same attribute xa (i.e., ripe orange), as well as an\nsualinstructiontuningtoenhanceinstructionfollowingca-\nimage sharing the same object xo (i.e., peeled apple)\npability. ThevisualunderstandingpartofLLaVAv1.5con-\ntocompriseatripletimageset. Fortheconvenienceofex-\nsists of a ViT and a multilayer perceptron (MLP) cross-\npression, wesimplyuseximg (whereimg ∈ {m,a,o})to\nmodal connector (CMC). CMC processes visual features\ncollectively denote the images as they are processed using\nbeforethelastlayerofViT,aligningthevisualspaceofViT\nthesamemodule.\nwith the linguistic space of LLM. We choose LLaVA v1.5\nVisualfeatureextractionbackbone. Asmentionedbe-\nasourfoundationalMLLMasithasdemonstratedstate-of-\nfore,sinceLLaVAv1.5isusedasourfundamentalMLLM,\nthe-artperformanceacrossvarioustasks.\nwe directly leverage the visual encoder, ViT, and cross-\nRecently, exploring the powerful language capabilities\nmodal connector (CMC) from the model to extract visual\nof (M)LLM to handle representation tasks (e.g., retrieval)\nfeatures. Specifically, the image ximg is partitioned into\nhas emerged as a prominent research domain. SGPT [23]\nnpatchtokens,whicharesubsequentlyputintoViTalong\nexploits the last hidden states of LLM for the input token\nwith the [CLS] token. Afterward, the output of patch to-\nsequence or a special learnable token to derive represen-\nkensbeforethelastlayerofViTisfedintotheCMCmod-\ntational embeddings. Subsequently, GritLM [24] applies\nule,asimplementedinLLaVAv1.5.Toalignthedimension\nmeanpoolingoverthelasthiddenstatesofLLMtoyieldthe\nofpatchtokens outputbyCMCwiththatof [CLS] token\ntextualembeddings.FROMAGe[12]usesalearnabletoken\nproducedbyViT,thepatchtokensoutputbyCMCareinput\ntorepresentthetextfedintoMLLMforimageretrieval.\nintoalinearlayer.Consequently,weobtainonefeaturevec-\ntorof[CLS]tokenfimg ∈ Rd andapatchfeaturematrix\n3.Approach cls\nofnpatchtokensFimg ∈Rn×d,wheredisthedimension\npatch\n3.1.TaskFormulation ofthefeatures.\nLocalfeaturesextraction. Intuitively, thecomposition\nCompositionalzero-shotlearning(CZSL)aimsatlearninga\n(e.g., ripe apple) only occupies a few parts of the im-\nmodelthatcanrecognizeunseencompositionsofattributes\nage. Sinceeachpatchtokenusuallycorrespondstoonelo-\nandobjectsthatarelearnedfromseencompositions. Given\ncal region of the image, to filter out background noise and\nanattributesetAandanobjectsetO,theattributesandob-\nfocus on related regions, we deploy a set of feature adap-\njectsarecomposedtoformacompositionsetC = A×O.\ntive aggregation (FAA) modules to derive p relevant local\nThecompositionsetC isdividedintotwodisjointsets: the\nfeaturesofximg,whereeachFAAmoduleisformulatedas\nseencompositionsetC andtheunseencompositionsetC ,\ns u follows:\nwhereC s∩C u =∅andC s∪C u =C. Themodelistrained (cid:40) v =agg⊗Fimg\nwith aseen trainingset D tr = {(x s,c s)}, where x s ∈ X s patch (1)\nis an image from the seen image set X corresponding to agg =σ(Conv(Fimg ))\ns patch\ntheseencompositionsetC ,andc ∈ C isthelabelofx .\ns s s s whereConv(·)representsthe1×1convolutionlayer,σ(·)\nFollowing the Generalized CZSL [33], the model is eval- denotes the sigmoid activation function, agg ∈ Rn is the\nuated on a predefined test set D = {(x ,c )}, where\nte te te weightvector,thek-thelementofagg istheweightfork-\nx te ∈X teisanimagefromtheunseenimagesetX tecorre- thpatchfeature.⊗representsmatrixproduct,andv ∈Rdis\nspondingtothecompositionsubsetC ofC,i.e.,C ⊆ C,\nte te thelocalfeatureobtainedbyanFAAmodule. Wevertically\nandc ∈C isthelabelofx . TheaimofCZSLtaskisto\nte te te concatenatethelocalfeaturesproducedbypFAAmodules\nlearnamodelM : X te → C te thatpredictslabelsc te from toobtainthelocalfeaturematrixFimg ∈Rp×d.\nl\nC fortheinputimagesx ∈X .\nte te te Global features extraction. Normally, the ViT output\nof [CLS] token is regarded as containing various global\n3.2.TRIDENT\ninformation of the image, which highly entangles both at-\nAsthemajornovelty,weproposeanovelframeworknamed tribute and object features together[8]. To disperse multi-\nMLLMembeddingsandattributesmoothingguideddisen- granularity global information into different representa-\n3\nPlease give me three adjectives smooth, round, juicy\nthat can describe the visual LLM red, crimson, mature\nfeature of a photo of a/an ... refresh, beige, crisp Attribute\nSmoothing\nFAA1\nattr\n⋮ ×p ⋮\nFAAp\n∙  obj\n: ripe orange CMC C MLP 1 -        ∙ MLP\n... patch tokens MLP 1 -                 ∙ ∙ MLC P\nViT    \n∙\n\n: ripe apple\nC1 ∙\n[CLS] C M ML LP\nP\n11 -- \n\n\n\n\n\n\n\n\n\n\n\n∙∙\n∙\nM ML LCP\nP ⋮ ×q ⋮\n\nCq∙\n\n\n: peeled apple\nImage\northo Embedder  comp Visual Feature\nVisual Feature Extraction Attribute-Object Disentanglement Feature Alignment \nFigure2. TheoverallarchitectureofourproposedTRIDENT.TRIDENTconsistsofthreemajormodules: (a)visualfeatureextraction,\n(b)attribute-objectdisentanglement,and(c)featurealignment.\ntions, q learnable condition masks are applied to fimg to toderivethefinalfeaturerepresentationfimg . Thismod-\ncls comp\nobtainqdifferentglobalrepresentations,whereeachglobal uleisformulatedasfollows:\nrepresentationiscomputedas:\nfimg =Lin (Cat(Avg(Fimg),Avg(Fimg))) (4)\ncomp comp g l\nu=fimg⊙c (2)\ncls wherefimg ∈ R2d denotesthevisualfeaturecorrespond-\ncomp\nwhere u ∈ Rd denotes each global representation. Here ingtothecomposition.Thismoduleisdesignedtolearnthe\nc∈Rdreferstoeachlearnableconditionmaskand⊙isthe visualfeaturesofimagesassociatedwiththeircorrespond-\nelement-wise multiplication. Consequently, we vertically ing composition labels, serving as the primary branch for\nconcatenate q global representations to derive the global inference.\nfeaturematrixF gimg ∈Rq×d. 3.2.2.Attribute-ObjectDisentanglement\nFeatures concatenation. Finally, Fimg and Fimg are\nl g As mentioned before, one of the key challenges for CZSL\nverticallyconcatenatedtoformthevisualfeaturesofximg,\ntask is to disentangle attribute and object from visual fea-\ni.e.,Fimg = [F limg,F gimg] ∈ R(p+q)×d,whichisusedfor tures. To overcome such challenge, we propose a novel\nthefollowingdisentanglementofattributeandobject. weighted disentanglement module to disentangle primi-\nOrthogonal regularization. We ideally want features tives,asillustratedinFigure2. Forbrevity,oneimagepair\nextractedbydifferentmodulescanrepresentdifferentinfor- xmandxafromthetripletimagesetistakenasanexample\nmationoftheimageximg.Tothisend,wefurtherintroduce to elaborate on this module, while another image pair xm\ntheorthogonalregularization,i.e.: andxofollowsthesamearchitecture.\nWeightscomputation. Thefeaturesofxm andxa (i.e.,\nL ortho =\n(cid:88) (||FimgFimgT\n−I|| Fro) (3) Fa and Fo) are vertically concatenated and fed into two\nimg∈{m,a,o} MLP modules to derive their respective weights of shared\nattribute features relative to each other, and subsequently\nwhere I ∈ R(p+q)×(p+q) is the identity matrix. ||·||\nFro utilizethemtocomputetheweightsoftheirownexclusive\nreferstotheFrobeniusnormofthematrix.\nobjectfeaturesasfollows:\nImageembedder. Inspiredby[26],fortheinputimage\nx rei sm pg e, cw tive efi lyr ,st au ns de hA ov rie zr oa ng te aP llo yo cls onA cv ag te( n·) ato en thF egi mmg byan Cd aF tl (im ·,g ·),   w wam mtt2 2ra\na\n= =σ 1( −M wL mP 2m a2a([Fm,Fa]))\nobj attr\nto aggregate both global and local visual information of (5)\nx caim teg nac to er dre fs ep ao tun rd ein pg asto set she thc ro om ugp hos ait li io nn eala rb le al y. eT rh Le in ntheco (n ·)-  w waa at2 2tm mr = =σ 1( −M wL aP 2ma2m([Fm,Fa]))\ncomp obj attr\n4\nEmbedding\nMLP\nPair\nComposed\nThe\nLast\nHidden\nStates\nof MLLM\nattribute\nwords\nobject\nwords\nwhere wm2a,wa2m ∈ Rh demonstrate the weights of the ... well.’, where tisthenumberofauxiliaryattributesand\nattr attr\nsharedattributefeaturesofxmrelativetoxa,andxarelative attribute-objectcomposition(e.g., peeledapple)isfilledin\nto xm, respectively. wm2a and wa2m denote the weights ’...’. Please refer to Appendix A for more details about\nobj obj\nof exclusive object features corresponding to xm and xa, the generation of auxiliary attributes by GPT-3.5. Subse-\nrespectively, which are derived by ”1−shared weights” quently, the generated auxiliary attribute words form a set\nparadigm as beyond the shared features of the image pair A . Therefore,thesetofallwordsY isobtained,including\na\nare the exclusive features of each image. Taking wm2a as attributes,objectsandauxiliaryattributesasfollows:\nattr\nan example, its k-th element refers to the shared attribute\nY =A∪O∪A (8)\nproportionofk-thfeatureofxmrelativetoxa. a\nDisentangled features obtainment. We multiply ele- Obtaining MLLM embeddings for words and com-\nments of each weight by the corresponding features and positions. Each word y ∈ Y is fed into LLaVA v1.5 to\nthen calculate the average. The following takes the pro- get the last hidden states, i.e., LLaVA (·). Please re-\nlhs\ncessofobtainingthesharedattributefeaturesofimagexm fer to Appendix B for more details about the obtainment\nrelativetoxaasanexample: ofthelasthiddenstatesofLLaVAv1.5foraninputword.\nSubsequently,theyarepassedthroughanMLPlayertoget\nh\nfm2a =\n1 (cid:88)\nwm2a Fa (6)\nembeddingsE word(·)ofaligneddimensionwithvisualfea-\nattr h attri i,: tures. And for a composed pair c of attribute a and object\ni=1\no, i.e., c = (a,o), we get the last hidden states of LLaVA\nwhere Fa denotes the i-th row of Fa, i.e., the i-th fea-\ni,: v1.5 for a and o, respectively, which are then horizontally\nture of xa. wm2a refers to the i-th element of wm2a, and\nfm2a ∈ Rd\nisa tt htr ei\nsharedattributefeatureofxm\nrea lt atr\ntiveto\nconcatenatedandfedintoalinearlayerLin co(·)togetthe\nattr composed pair embedding E co(·). The process is formu-\nxa.\nlatedasfollows:\nFortheimagepairofxmandxa,fourpartsareobtained:\nthe shared attribute features of xm relative to xa, and xa E word(y)=MLP wd(LLaVA lhs(y)) (9)\nrelativetoxm,aswellastwoexclusiveobjectfeaturesofthe\nE (c)=Lin (Cat((LLaVA (a),(LLaVA (o)))\nco co lhs lhs\nimagepair,respectively. Thesefourfeaturesaremarkedas\n(10)\nfe ,wheree ∈ {m2a,a2m}andpri ∈ {attr,obj}. Then\npri Wordexpanding. Priorworkscomputecosinesimilar-\nthesharedattributefeatureofxa andxm withoutrelativity\nities of disentangled features and word embeddings only\nisobtainedbyanMLPlayer,whichislessdependentonthe\nwithintherespectivedomainsofattributesorobjects,which\nobject. Theprocessisasfollows:\nresultsinthedisentangledattributesandobjectsstillretain-\nfma =MLP (Cat(fm2a,fa2m)) (7) ingtheinformationofeachother. Toaddresstheproblem,\nattr ma attr attr\nweproposeawordexpandingstrategy,whichcomputesco-\nSimilarly,wedisentangleattributeandobjectforxmand sinesimilaritiesofvisualfeaturesandtheembeddingsofall\nxo andobtainthesamefeaturesasxm andxa: fe ,where words,includingattributesandobjects,andtreatsallwords\npri\ne∈{m2o,o2m}andpri∈{obj,attr}andfmo. excepttheground-truthwordasnegativelabels.\nobj\nAlignment by cross-entropy. Similar to [19], we use\n3.2.3.FeatureAlignment\ncross-entropytoprocessthecosinesimilarityofvisualfea-\nInspired by [24] that leverages the last hidden states as\ntures and word embeddings. Assume that f is the visual\nthe representation embeddings, we consider the last hid-\nembedding and E (wd) is the word embedding for the\nword\nden states of LLaVA v1.5 [16] as our MLLM embeddings\nwordwd∈Y inajointspace. Theclassifierlogitfromf to\nfor words. Moreover, to tackle the problem that the inef-\nE (wd)isdefinedasfollows:\nword\nfectiveoverconfidenceexhibitedbythemodelsintermsof\nthe ground-truth attribute hinders them from generalizing eδ·cos(f,Eword(wd))\nCE(f,wd)= (11)\nto unseen compositions, GPT 3.5 is employed to generate (cid:80) eδ·cos(f,Eword(y))\ny∈Y\nseveralauxiliaryattributesthatdescribeanobjectwithonly\nwhere δ is the temperature variable, and cos(·,·) denotes\noneground-truthattributeandperformlabelsmoothingdur-\ncosinesimilarityfunction.Thuscross-entropywith/without\ningattributealignment. Nowwedetaileachpartoffeature\nlabelsmoothingcanbeuniformlyformulatedasfollows:\nalignment.\nGeneratingauxiliaryattributewordsbyLLM.Since (cid:88)\nH(f,Y)= −zlog(CE(f,y))\nonly attribute text needs to be generated, we leverage a\ny∈Y\nLLM,GPT-3.5,insteadofMLLM,togenerateseveralaux- \n1−α, ifyisgroundtruthlabel (12)\niliaryattributesforeachcomposition. Specifically,thefol- \nlowing prompt is input to LLM: ’Please give me t adjec- with z = α/t, ifyisauxiliarylabel\ntivesthatcandescribethevisualfeatureofaphotoofa/an\n0,\notherwise\n5\nwhereαdenotesthesmoothingfactor,treferstothenum- 4.Experiment\nber of auxiliary labels and z ∈ [0,1] represents the tar-\n4.1.ExperimentSetup\ngetvalueofone-hotorsmoothinglabel. Forcross-entropy\nwithout label smoothing, i.e. with one-hot label H oh, α is Datasets. We evaluate our model on three challenging\nsetto0. Andthecross-entropywithlabelsmoothingisde- CZSL benchmark datasets: MIT-states [11], C-GQA [25],\nnotedasH ls. andVAW-CZSL[38].Wepresenttheintroductionandcom-\nForthedisentangledattributefeaturesofoneimagerel- mondatasplitsofthethreedatasetsinAppendixC.\native to each other, since a single object exhibits multiple Metrics. FollowingthecommongeneralizedCZSLset-\nattributes,weexploitattributesmoothingwithauxiliaryat- ting[33], weevaluateourmodelonseenandunseenpairs\ntributestounderminetheconfidenceintheground-truthat- separately. Basedonthem,acalibrationbiastradesoffbe-\ntributeandlearnmorerelatedattributes. Forthesharedat- tweentheaccuraciesofseenandunseenpairs.Wecalculate\ntribute features without relativity, one-hot label is used to area under the curve AUC (in %) using seen and unseen\ncompute its classification loss. The loss for disentangled classificationaccuraciesatdifferentbiasesintestdata. The\nattributescanbedefinedasfollows: bestseenandunseenaccuraciesSeenandUnseen(in%)\n(cid:88) ofthecurvearealsoreported. Inaddition,wecalculatethe\nL = H (Fe ,Y)+H (Fma,Y)\nattr ls attr oh attr harmonicmeanofseenandunseenclassificationaccuracies\nc∈{m2a,a2m,m2o,o2m} atdifferencebiasesandreportthebestoneHM (in%).\n(13)\nImplementation details. We use the visual encoder of\nConcerning the disentangled object features, we use\nLLaVA v1.5, Vit-large-14-336px as our frozen feature ex-\ncross-entropy with one-hot label to learn the prototype of\ntractor,whoseoutputscontain577tokens(1[CLS]and576\ntheobjectandthelossisasfollows:\npatch tokens) of 1024 dimensions. The cross-modal con-\n(cid:88) nector of LLaVA v1.5 maps the features to the dimension\nL = H (Fe ,Y)+H (Fmo,Y)\nobj oh obj oh obj of 4096, the same as last hidden states of based LLM Vi-\nc∈{m2a,a2m,m2o,o2m}\ncuna v1.5 [45]. Image embedder and the MLP for words\n(14)\nmapthemtothedimensionof1024forfastertraining.TRI-\nWithrespecttothevisualfeatureoftheimagefromim-\nDENT and all baseline models are trained with 128 batch\nage embedder, we calculate the cosine similarity between\nsizefor50epochs. Thenumberofglobalfeaturesissetto\nvisualembeddingandthecomposedpairembeddingofthe\n6, 2, 4 for the three datasets, respectively, and the number\ncorresponding composition label and use one-hot label to\nof local features is twice that of global features. The la-\nalign them. The classification loss for compositions is as\nbelsmoothingfactorissetto0.09, 0.03, 0.03forthethree\nfollows:\ndatasets, respectively. The number of generated auxiliary\nL = (cid:88) H (Fimg ,C ) (15) attributes for each composition is set to 3. Refer to Ap-\ncomp oh comp s\npendixDformoreinformationaboutimplementation.\nimg∈{m,a,o}\nBaselines. WecompareourTRIDENTwithrecentand\n3.3.TrainingandInference prominent approaches in the task of CZSL: SymNet [15],\nCompCos [19], Co-CGE [20], SCEN [13], OADis [38],\nDuring the training phase, the overall loss function is for-\nINV [44], CANet [42], and ProCC [10]. We replace their\nmulatedasfollows:\nbackbone with Vit-large-14-336px and retrain all models\nwith the same epoch for the sake of fairness. In addition,\nL=γ L +γ L +γ L +γ L\northo ortho comp comp attr attr obj obj\nalthough comparing TRIDENT with CLIP-based CZSL\n(16)\nmethods,whichrelyonthedual-towerarchitecture,isvery\nwhere γ , γ , γ , and γ are weighting factors\northo comp attr obj\nunfairduetothesignificantadditionoftrainableparameters\ntobalancetheinfluenceofdifferentlosses.\nandtrainingoverheadforboththetextandvisualencoders,\nFor inference, we use the composition feature space\nwestillchoosethefoundationalCLIPandCoOpmodelsas\ngenerated by the classifier that is obtained by optimizing\nbaselinesfortheirstrongzero-shotclassificationabilities.\nL . Specifically, given an image from test set, the co-\ncomp\nsinesimilaritiesofitsvisualfeatureobtainedbyimageem-\n4.2.ResultsandDiscussion\nbedderandthecomposedpairembeddingsofallcandidate\ncompositions in the test set are computed. The composi- In this section, we compare TRIDENT with state-of-the-\ntionwiththehighestsimilarityistheclasspredictedbythe art methods. As shown in Table 1, TRIDENT surpasses\nmodel. Note that although the disentanglement branches othermodelsbyasubstantialmarginingeneral. ForMIT-\narenotusedforinference,theystillinfluencetheformation States, TRIDENT boosts AUC, HM, and Unseen from\nofthecompositionfeaturespacethroughtheirsharedvisual 13.6%,29.8%,and39.9%ofCANettothenewstate-of-the-\nfeatureextractionmoduledescribedinSection3.2.1. art performance of 14.2%, 30.9%, and 40.0% with 0.6%,\n6\nMIT-States C-GQA VAW-CZSL\nMethod AUC HM Seen Unseen AUC HM Seen Unseen AUC HM Seen Unseen\nSymNet[15] 3.2 13.7 22.7 20.1 1.9 10.8 20.3 11.8 2.8 13.5 20.2 18.0\nCompCos[19] 12.3 28.2 39.0 39.5 5.0 17.7 32.8 19.1 6.5 20.8 30.5 27.4\nCo-CGE[20] 10.3 25.1 41.0 33.1 4.2 15.2 32.9 17.0 6.2 19.7 31.0 26.1\nSCEN[13] 9.8 24.6 35.1 36.5 3.8 15.3 31.5 15.7 5.7 19.2 29.9 24.5\nOADis[38] 13.1 29.0 42.3 27.3 2.3 12.1 23.3 12.8 4.1 16.2 26.0 20.7\nINV[44] 11.5 26.6 28.5 25.0 1.4 7.9 28.6 6.8 2.0 11.1 21.1 11.9\nCANet[42] 13.6 29.8 46.4 39.9 5.7 18.9 34.8 20.5 6.7 21.0 31.2 27.4\nProCC[10] 9.5 28.1 43.1 39.1 3.5 15.1 32.4 15.8 3.6 18.9 26.9 25.5\nCLIP[28] 11.0 26.1 30.2 46.0 1.4 8.6 7.5 25.0 - - - -\nCoOp[28] 13.5 29.8 34.4 47.6 4.4 17.1 20.5 26.8 - - - -\nTRIDENT(Ours) 14.2 30.9 44.5 40.0 8.0 22.6 39.5 24.1 8.3 23.4 33.3 31.1\nTable 1. Comparison with the state-of-the-art results on MIT-States, C-GQA and VAW-CZSL. We compare our TRIDENT with the\nstate-of-the-artmethodsontestAUC,bestseen(Seen),bestunseen(Unseen)andbestharmonicmean(HM)accuraciesonthesethree\ndatasets.Wemeasuretop-1AUConMIT-StatesandC-GQA,andtop-3AUConVAW-CZSL.Bestresultsaredisplayedinboldface,and\nsecondbestresultsareunderlined.\n1.1%, and 0.1% improvement, respectively. Our model Method AUC HM Seen Unseen\nachieves competitive performance on MIT-States bench-\nw/ocondition masks 14.0 30.5 44.2 39.8\nmark,despiteconsiderablelabelnoise[2].However,forthe\nw/oFAAs 13.9 30.4 44.4 39.7\nmorechallengingbenchmarkC-GQA,TRIDENTachieves\nw/oword expanding 14.0 30.1 44.7 39.8\n8.0%, 22.6%, 39.5%, and 24.1% on the metrics of AUC,\nw/oattribute smoothing 13.9 30.5 44.9 39.5\nHM, Seen, and Unseen, providing 2.3%, 3.7%, 4.7%, w/oL +L 13.2 30.1 43.8 38.9\nattr obj\nand 3.6% improvements on the previous state-of-the-art w/oL 14.1 30.7 44.6 39.7\northo\nmodel CANet. For the existing most challenging bench- TRIDENT 14.2 30.9 44.5 40.0\nmark dataset VAW-CZSL, TRIDENT attains performance\nof 8.3%, 23.4%, 23.4%, and 33.3%, surpassing CANet by Table2. AblationstudyresultsonMIT-States. w/ocertain part\n1.6%,2.4%,2.2%,and3.7%intermsofAUC,HM,Seen, denotesthispartisablated.\nand Unseen. The largest improvement is observed in the\nUnseen metric, indicating that attribute smoothing helps ule on MIT-States, as it is the most common used dataset.\nenhance the generalization ability of the model. We ob- TheablationresultsarereportedinTable2.Fromthistable,\nserveTRIDENTperformssignificantlybetterthanCANet wegainthefollowingobservations.\nregarding all metrics on two challenging and low-noise 1)Bothw/ocondition masksmodelandw/oFAAsmodel\nbenchmarkdatasetC-GQAandVAW-CZSL,indicatingthe performworsethanTRIDENT,whichvalidatestheimpor-\nefficacyofourapproach. Thisimprovementarisesfromthe tanceofextractingthemulti-granularityfeaturesandfilter-\nutilizationofMLLMembeddingsandattributesmoothing, ingoutthebackgroundnoise,respectively.\nwhichenhanceattribute-objectdisentanglementandconse- 2)TRIDENTsurpassesw/oword expandingmodeland\nquently facilitate the recognition of unseen compositions w/o attribute smoothing model on the Unseen metric, yet\nwhilemaintainingperformanceonseencompositions. fallsshortofthemontheSeenmetric. Thedifferencebe-\nIn addition, we compare TRIDENT with dual-tower tweenTRIDENTandthew/oword expandingmodelstems\nCLIP and CoOp after fine-tuned for the CZSL task. Since from its more thorough disentanglement, which enhances\ntheyaretrainedonalargeamountofimage-textdata,they therecognitionofunseenimageswhileweakenstheidenti-\npossess zero-shot image classification capabilities, which ficationofseenimages. ThedisparitybetweenTRIDENT\nleadstobetterclassificationresultsforunseenimages. Re- andthew/oattribute smoothingmodelarisesfromattribute\ngardingUnseenmetric,CoOpoutperformsTRIDENTby smoothing,whichdiminishestheconfidenceofthemodelin\n7.6% and 2.7% on MIT-States and C-GQA, respectively. seen compositions, facilitating its generalization to unseen\nHowever, TRIDENT surpasses CoOp by 0.7% and 1.1% compositions. However, the improvement of TRIDENT\non the core metrics of AUC and HM on MIT-States, as overthesetwomodelsonAUC andHM indicatestheef-\nwell as 3.6% and 5.5% on C-GQA, which suggests TRI- fectivenessofwordexpandingandlabelsmoothingstrategy.\nDENTperformsbetterthanCLIPandCoOpinCZSLtask. 3) TRIDENT outperforms w/o L +L model on\nattr obj\nall metrics, confirming that the attribute-object disentan-\n4.3.AblationStudy\nglement module is highly advantageous for generalization\nEffectivenessofeachcomponent. Weablatecertainmod- fromseencompositionstounseencompositions.\nuleofTRIDENTtoevaluatethecontributionofeachmod- 4) w/o L model is inferior to TRIDENT, which\northo\n7\nMIT-States C-GQA VAW-CZSL\nBurnt House Green Leaf White Wave MIT-States MIT-States\nRight B L B BB au u uu rr r rr gn n nn et t tt L B TFH o oi uo rg we iu ld es i re ng GG B B Thl r rr u o ee icr ewe r k nn y n L FLL L e lee e oaaa a f wff f\ner\nS S W GW h h rhh aa a ii yl l tt l l ee o o Ww w LW ai nOWa vev ecae ev ae n OPe rae nle gd e ChO ul rd ch\nNarrow Tail Brown Door Graffitied Graffiti C-GQA C-GQA\nRight F NN o aa l rr d rr oeo d ww M VT o aa lui ll en ytain TB B aer no ig w e Cn u D rD to ao o io nrr F PG a ar d ia nef tf d eit di e G d Gr a rG f afr fit fa i if tf iiti Yo Gu irn lg G Gr re apen e\nShattered Mountain Closed Blinds Blue Graffiti\nVerdant Mountain White Blinds Graffitied Wall\nOld Boat Colorful Pillow Multicolored\nWrong B U BD Or n re ldo ofp l k kaa Be etin en n odt ae S B t d Bh r i oo dW ar ge o t eod R O R O Lae er r ra ad d gn n eg gFP e e li Pol l wo iP F llw lei ooll r wwow er P O B P RT u lr ee r uoaf dd sf wn y hd Bg n By ee B B- ae eBB ra e aere a ra ra rr EWV lA eaW pl- kC hZ i aS nL ng t WVA o BW o- eC d dZS eL n\n(a)image-to-textretrieval. (b)text-to-imageretrieval(successfulcases). (c)text-to-imageretrieval(failurecases).\nFigure 3. Qualitative analysis. (a) Top-5 image-to-text retrieval cases. The first two rows display successful cases, while the last row\npresentsfailurecases. (b)Successfulcasesoftop-5text-to-imageretrieval. (c)Failurecasesoftop-5text-to-imageretrieval. Inallcases,\ntheattributeandobjectofcompositionlabelaremarkedinorangeandblue,respectively. Andthesuccessfulandfailureretrievalresults\naretaggedingreenandred,respectively.\nMethod Varient AUC HM imagelabeledburnt house,wenoticethatthetopfour\npredictionscanbothdescribelogsburningonfireintheim-\nft+w2v 8.2 22.8\nSCEN[13] age. Intermsoftheimagelabeledgreen leaf,another\nLLaVA 10.3 25.1\nlhs\nsuccessful case, the predicted attributes can also describe\nCANet[42]\nft+w2v 12.3 28.4 leaf,whichisthankstoattributesmoothinglearningmore\nLLaVA 12.5 28.3\nlhs attributesforanobject.Forthefailurecases,suchastheim-\nft+w2v 14.0 29.9 age labeled multicolored teddy-bear, the model\nTRIDENT\nLLaVA lhs 14.2 30.9 focusesonthemainorangebearandneglectsthemulticol-\noredbackground,whichisattributedtoFAAmodules.\nTable 3. Impact of word embedding on MIT-States. ft+w2v We then consider text-to-image retrieval. Successful\nmeansthesumofWord2VecandFasttext.LLAVA lhsrepresents casesareshowninFigure3b,whilefailurecasesareshown\nthelasthiddenstatesofLLAVAv1.5.\nin Figure 3c. Given a text composition, we embed it and\nretrieve the top-5 closest images. We observe that the re-\ntrievedimagesofpeeled orangearedefinitelycorrect.\nsuggests the designed orthogonal regularization is helpful\nHowever, theretrievedimagesofgreen grapesareall\ntoguaranteedifferentfeaturesextractdifferentinformation.\nwrong. This is due to the fact that the training images of\nImpact of word embeddings. Our work leverages the\ngreen grapesinC-GQAdatasetarealmostfilledwith\nlasthiddenstatesofLLaVAv1.5(LLaVA )aswordem-\nlhs a single grape, making it difficult for the model to capture\nbeddings, while Word2Vec [21] and Fasttext [3] are the\nthe contour features of a bunch of green grapes. The\nmost common word embeddings for MIT-States in previ-\nimage-to-text and text-to-image retrieval experiments con-\nousworks. InTable3,basedonthreemodels: SCEN[13],\nfirmthatourmodeleffectivelyprojectsvisualfeaturesand\nCANet[42]andTRIDENT,wecomparetheperformance\nwordembeddingsintoaunifiedspace.\nofemployingthelasthiddenstatesofLLaVAv1.5andthe\nsumofWord2VecandFasttext(ft+w2v),respectively. The\n5.Conclusion\nresults indicate that the last hidden states of MLLM cap-\nture more complex multimodal semantic information than In this work, we propose a novel framework termed TRI-\nordinarywordembeddings. DENT to address the challenging CZSL task. First, we\nFordetailsontheimpactofhyperparameters, including leverage feature adaptive aggregation modules to mitigate\nthenumberofvisualfeaturesandthelabelsmoothingfac- the impact of background, and utilize learnable condition\ntor,pleaserefertoAppendixE. masks to capture multi-granularity features for attribute-\nobjectdisentanglement. Inaddition,weexploitthelasthid-\n4.4.QualitativeAnalysis\ndenstatesofMLLMtoreplaceordinarywordembeddings,\nInspiredby[8],weuseTRIDENTtoconductbothimage- astheycancapturecomplexmultimodalsemanticinforma-\nto-text retrieval and text-to-image retrieval experiments on tion. Moreover,weleverageLLMtogenerateauxiliaryat-\nthethreedatasets. Wefirstconsiderimage-to-textretrieval, tributes and perform attribute smoothing to diminish over-\nshown in Figure 3a. The first two rows display success- confidenceofmodelsinseencompositions, whichenables\nfulcases,whilethelastrowpresentsfailurecases. Andthe modelstogeneralizetounseencompositionsbetter. Exten-\ncasesshowninthesethreecolumnsaredrawnfromthethree sive experiments have been conducted on three challeng-\ndatasets, respectively. Given an image, such as the image ing datasets, and the results demonstrate the effectiveness\nofburnt house,weextractitsvisualfeaturesbyimage ofTRIDENT.Inthefuture,weplantoextendourmethod\nembedderandretrievethetop-5closestcomposedpairem- toharnessthepowerfulcapabilitiesofLLMs,MLLMs,and\nbeddingsofcompositions. Forsuccessfulcases,suchasthe CLIPtomoreeffectivelyaddresstheCZSLtask.\n8\nReferences Wang, ZimingLiu, andXiaochengLu. Procc: Progressive\ncross-primitive compatibility for open-world compositional\n[1] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, Antoine\nzero-shotlearning. InProceedingsoftheAAAIConference\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-\nonArtificialIntelligence,pages12689–12697,2024. 6,7\nsch, Katherine Millican, Malcolm Reynolds, Roman Ring,\n[11] PhillipIsola,JosephJLim,andEdwardHAdelson.Discov-\nEliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,\neringstatesandtransformationsinimagecollections.InPro-\nSina Samangooei, Marianne Monteiro, Jacob L Menick,\nceedings of the IEEE Conference on Computer Vision and\nSebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sa-\nPatternRecognition,pages1383–1391,2015. 6,12\nhand Sharifzadeh, Mikoł aj Bin´kowski, Ricardo Barreira,\n[12] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\nOriol Vinyals, Andrew Zisserman, and Kare´n Simonyan.\nGroundinglanguagemodelstoimagesformultimodalinputs\nFlamingo:avisuallanguagemodelforfew-shotlearning. In\nandoutputs. InProceedingsoftheInternationalConference\nAdvancesinNeuralInformationProcessingSystems,pages\nonMachineLearning,pages17283–17300,2023. 2,3\n23716–23736,2022. 3\n[13] Xiangyu Li, Xu Yang, Kun Wei, Cheng Deng, and Muli\n[2] Yuval Atzmon, Felix Kreuk, Uri Shalit, and Gal Chechik.\nYang. Siamesecontrastiveembeddingnetworkforcomposi-\nA causal view of compositional zero-shot recognition. In\ntionalzero-shotlearning. InProceedingsoftheIEEE/CVF\nAdvancesinNeuralInformationProcessingSystems,pages\nConference on Computer Vision and Pattern Recognition,\n1462–1473,2020. 7\npages9326–9335,2022. 2,6,7,8\n[3] Piotr Bojanowski, Edouard Grave, Armand Joulin, and\n[14] Xiangyu Li, Xu Yang, Xi Wang, and Cheng Deng. Agree\nTomasMikolov.Enrichingwordvectorswithsubwordinfor-\ntodisagree: Exploringpartialsemantic consistencyagainst\nmation. TransactionsoftheAssociationforComputational\nvisualdeviationforcompositionalzero-shotlearning. IEEE\nLinguistics,5:135–146,2017. 8\nTransactions on Cognitive and Developmental Systems, 16\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\n(4):1433–1444,2024. 1\nbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-\n[15] Yong-LuLi,YueXu,XiaohanMao,andCewuLu. Symme-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\ntryandgroupinattribute-objectcompositions. InProceed-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\ningsoftheIEEE/CVFConferenceonComputerVisionand\nHenighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nPatternRecognition,page11313–11322,2020. 2,6,7\nJeffreyWu,ClemensWinter,ChrisHesse,MarkChen,Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack [16] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nClark,ChristopherBerner,SamMcCandlish,AlecRadford, Improved baselines with visual instruction tuning. In Pro-\nIlya Sutskever, and Dario Amodei. Language models are ceedingsoftheIEEE/CVFConferenceonComputerVision\nfew-shotlearners. InAdvancesinNeuralInformationPro- andPatternRecognition,pages26296–26306,2024. 2,3,5\ncessingSystems,pages1877–1901,2020. 3 [17] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.\n[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang- Visual instruction tuning. Advances in neural information\nhaoWu,HaoZhang,LianminZheng,SiyuanZhuang,Yong- processingsystems,36,2024. 3\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. [18] XiaochengLu,SongGuo,ZimingLiu,andJingcaiGuo.De-\nXing. Vicuna: An open-source chatbot impressing gpt-4 composedsoftpromptguidedfusionenhancingforcompo-\nwith90%*chatgptquality,2023. 3 sitionalzero-shotlearning. InProceedingsoftheIEEE/CVF\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Conference on Computer Vision and Pattern Recognition,\nToutanova. BERT:Pre-trainingofdeepbidirectionaltrans- pages23560–23569,2023. 2\nformersforlanguageunderstanding. InProceedingsofthe [19] MassimilianoMancini,MuhammadFerjadNaeem,Yongqin\nConferenceoftheNorthAmericanChapteroftheAssocia- Xian, and ZeynepAkata. Openworld compositionalzero-\ntionforComputationalLinguistics:HumanLanguageTech- shotlearning. InProceedingsoftheIEEE/CVFConference\nnologies, Volume 1 (Long and Short Papers), pages 4171– on Computer Vision and Pattern Recognition, pages 5222–\n4186,2019. 2 5230,2021. 5,6,7\n[7] Alexey Dosovitskiy. An image is worth 16x16 words: [20] MassimilianoMancini,MuhammadFerjadNaeem,Yongqin\nTransformersforimagerecognitionatscale. arXivpreprint Xian, and Zeynep Akata. Learning graph embeddings for\narXiv:2010.11929,2020. 3 openworldcompositionalzero-shotlearning. IEEETrans-\n[8] ShaozheHao,KaiHan,andKwan-YeeK.Wong. Learning actionsonPatternAnalysisandMachineIntelligence,46(3):\nattention as disentangler for compositional zero-shot learn- 1545–1560,2022. 2,6,7\ning. InProceedingsoftheIEEE/CVFConferenceonCom- [21] TomasMikolov.Efficientestimationofwordrepresentations\nputerVisionandPatternRecognition, pages15315–15324, invectorspace. arXivpreprintarXiv:1301.3781,2013. 2,8\n2023. 1,3,8 [22] IshanMisra,AbhinavGupta,andMartialHebert. Fromred\n[9] SitengHuang,BiaoGong,YutongFeng,MinZhang,Yiliang winetoredtomato: Compositionwithcontext. InProceed-\nLv,andDonglinWang.Troika:Multi-pathcross-modaltrac- ingsoftheIEEEConferenceonComputerVisionandPattern\ntionforcompositionalzero-shotlearning. InProceedingsof Recognition,pages1160–1169,2017. 1\ntheIEEE/CVFConferenceonComputerVisionandPattern [23] Niklas Muennighoff. Sgpt: Gpt sentence embeddings for\nRecognition,pages24005–24014,2024. 2 semanticsearch. arXivpreprintarXiv:2202.08904,2022. 2,\n[10] FushuoHuo,WenchaoXu,SongGuo,JingcaiGuo,Haozhao 3\n9\n[24] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, [36] Frank Ruis, Gertjan Burghouts, and Doina Bucur. Inde-\nFuruWei,TaoYu,AmanpreetSingh,andDouweKiela.Gen- pendentprototypepropagationforzero-shotcompositional-\nerative representational instruction tuning. arXiv preprint ity. InAdvancesinNeuralInformationProcessingSystems,\narXiv:2402.09906,2024. 2,3,5,12 pages10641–10653,2021. 1\n[25] Muhammad Ferjad Naeem, Yongqin Xian, Federico [37] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija\nTombari, and Zeynep Akata. Learning graph embeddings Jain,SamratMondal,andAmanChadha. Asystematicsur-\nfor compositional zero-shot learning. In Proceedings of veyofpromptengineeringinlargelanguagemodels: Tech-\ntheIEEE/CVFConferenceonComputerVisionandPattern niques and applications. arXiv preprint arXiv:2402.07927,\nRecognition,pages953–962,2021. 6,12 2024. 11\n[26] TusharNagarajanandKristenGrauman. Attributesasoper- [38] NiratSaini,KhoiPham,andAbhinavShrivastava.Disentan-\nators: Factorizingunseenattribute-objectcompositions. In glingvisualembeddingsforattributesandobjects. InPro-\nProceedings of the European Conference on Computer Vi- ceedingsoftheIEEE/CVFConferenceonComputerVision\nsion,pages169–185,2018. 1,4 andPatternRecognition,pages13658–13667,2022. 1,2,6,\n[27] Zhixiong Nan, Yang Liu, Nanning Zheng, and Song-Chun 7,12\nZhu. Recognizingunseenattribute-objectpairwithgenera- [39] Justyna Sarzynska-Wawer, Aleksander Wawer, Aleksan-\ntivemodel. InProceedingsoftheAAAIConferenceonArti- draPawlak, JuliaSzymanowska, IzabelaStefaniak, Michal\nficialIntelligence,pages8811–8818,2019. 1 Jarkiewicz,andLukaszOkruszek. Detectingformalthought\n[28] NihalV.Nayak,PeilinYu,andStephenH.Bach. Learning disorderbydeepcontextualizedwordrepresentations. Psy-\ntocomposesoftpromptsforcompositionalzero-shotlearn- chiatryResearch,304:114135,2021. 2\ning. In International Conference on Learning Representa- [40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\ntions,2023. 2,7 Martinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste\n[29] OpenAI. Gpt-3.5-turboapi,2023. 2 Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\n[30] LongOuyang, JeffreyWu, XuJiang, DiogoAlmeida, Car- Llama: Open and efficient foundation language models.\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini arXivpreprintarXiv:2302.13971,2023. 3\nAgarwal,KatarinaSlama,AlexRay,JohnSchulman,Jacob [41] Bin Wang and C.-C. Jay Kuo. Sbert-wk: A sentence\nHilton,FraserKelton,LukeMiller,MaddieSimens,Amanda embedding method by dissecting bert-based word models.\nAskell, Peter Welinder, Paul F Christiano, Jan Leike, and IEEE/ACM Transactions on Audio, Speech, and Language\nRyanLowe.Traininglanguagemodelstofollowinstructions Processing,28:2146–2157,2020. 2\nwith human feedback. In Advances in Neural Information\n[42] QingshengWang,LingqiaoLiu,ChenchenJing,HaoChen,\nProcessingSystems,pages27730–27744,2022. 3\nGuoqiangLiang,PengWang,andChunhuaShen. Learning\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nconditional attributes for compositional zero-shot learning.\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming In Proceedings of the IEEE/CVF Conference on Computer\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, VisionandPatternRecognition,pages11197–11206,2023.\nAndreasKopf,EdwardYang,ZacharyDeVito,MartinRai-\n2,6,7,8\nson, AlykhanTejani, SasankChilamkurthy, BenoitSteiner,\n[43] Shuo Xu, Sai Wang, Xinyue Hu, Yutian Lin, Bo Du, and\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An\nYuWu. Mac:Abenchmarkformultipleattributescomposi-\nimperativestyle,high-performancedeeplearninglibrary. In\ntionalzero-shotlearning. arXivpreprintarXiv:2406.12757,\nAdvancesinNeuralInformationProcessingSystems,2019.\n2024. 2\n13\n[44] TianZhang,KongmingLiang,RuoyiDu,XianSun,Zhanyu\n[32] Jeffrey Pennington, Richard Socher, and Christopher Man-\nMa,andJunGuo. Learninginvariantvisualrepresentations\nning.GloVe:Globalvectorsforwordrepresentation.InPro-\nforcompositionalzero-shotlearning. InProceedingsofthe\nceedingsoftheConferenceonEmpiricalMethodsinNatural\nEuropeanConferenceonComputerVision,pages339–355,\nLanguageProcessing,pages1532–1543,2014. 1,2\n2022. 1,6,7\n[33] SenthilPurushwalkam,MaximilianNickel,AbhinavGupta,\n[45] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nandMarc’AurelioRanzato. Task-drivenmodularnetworks\nZhuang,ZhanghaoWu,YonghaoZhuang,ZiLin,Zhuohan\nforzero-shotcompositionallearning. InProceedingsofthe\nLi,DachengLi,EricXing,HaoZhang,JosephEGonzalez,\nIEEE/CVF International Conference on Computer Vision,\nandIonStoica. Judgingllm-as-a-judgewithmt-benchand\npages3592–3601,2019. 1,3,6\nchatbotarena. InAdvancesinNeuralInformationProcess-\n[34] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario\ningSystems,pages46595–46623,2023. 3,6\nAmodei,andIlyaSutskever. Languagemodelsareunsuper-\n[46] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiwei\nvisedmultitasklearners,2019. 2\nLiu. Conditionalpromptlearningforvision-languagemod-\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nels. InProceedingsoftheIEEE/CVFConferenceonCom-\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nputerVisionandPatternRecognition, pages16816–16825,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\n2022. 2\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodelsfromnaturallanguagesupervision. InProceedings\noftheInternationalConferenceonMachineLearning,pages\n8748–8763,2021. 2\n10\nAppendix\nUser: Pleasegivemefiveadjectivesthataremost\nlikelytodescribetheinflatedpoolwell.\nA. Auxiliary Attributes Generation by LLM\nandPost-process GPT-3.5: 1.Spacious2.Durable3.Sturdy4.Bright\n5.Relaxing\nAs mentioned before, we leverage GPT-3.5 to generate User: Nowlgiveyouanobjectwithanadjective:\nsomeauxiliaryattributesforattributesmoothing.Theauxil- inflated pool. Please give me five more\niaryattributesaregeneratedbasedonthecontextualcompo- possibleadjectivesthatcandescribethein-\nsitionoftheobjectanditsattribute,suchasripe apple flatedpoolwell.\nor sliced apple. The model takes into account the\nGPT-3.5: 1.Refreshinginflatedpool2.Relaxingin-\nuniquecharacteristicsthatarisefromthecombinationofat-\nflatedpool3. Blueinflatedpool4. Large\ntribute and the object’s context. This ensures that the aux-\ninflatedpool5.Funinflatedpool\niliary attributes accurately capture the nuances of the spe-\nUser: Pleasegivemefivecommonadjectivesthat\ncificcomposition,ratherthangeneralobject-levelattributes.\ncandescribeainflatedpoolwell.\nSincethegenerationabilityofLLMisaffectedbymanyfac-\ntors,inthissection,wefirstexploretheimpactofdifferent GPT-3.5: 1. Large2. Round3. Inviting4. Fun5.\npromptsonLLM-generatedcontent. Thenwestudyonthe Sturdy\ninfluenceofthenumberofauxiliaryattributest.Inaddition,\nUser: Pleasegivemefiveadjectivesthatcande-\nweintroducepost-processingofgeneratedtexttoeliminate scribethevisualfeatureofaphotoofa/an\npotentialnoise. inflatedpoolwell.\nImpact of prompt input to LLM. Users typically in-\nGPT-3.5: 1. Colorful2. Refreshing3. Sparkling4.\nteract with LLMs through prompts to request answers to\nBright5.Relaxing\nquestions, generate text, complete tasks, and more. The\nmodel generates text based on the provided prompt, striv-\nTable 5. Example prompts with the composition inflated\ning to meet the user’s requirements [37]. Therefore, the\npoolinputtoGPT-3.5andthegeneratedtext.\ngooddesignofpromptissignificantforstimulatingknowl-\nedgeofLLMs,whichenablesthemtobetterfollowourin-\nstructions and generate auxiliary attributes with high qual-\nity. Wefirstdesignsomepromptswithdifferentstyle,then\ninput them into GPT-3.5 and observe the quality of gener-\natedattributes. Somepromptexamplesonthecomposition\nUser: Pleasegivemefiveadjectivesthataremost browned beef and ancient building are shown\nlikelytodescribethebrownedbeefwell.\ninTable4andTable5,respectively.\nGPT-3.5: 1. Flavorful2. Juicy3. Savory4. Tender AsshowninTable4,thepromptwithout”thevisualfea-\n5.Rich ture of ...” may cause the model to produce adjectives that\nare not specific but generic, such Savory and Rich. In\nUser: Now l give you an object with an adjec-\ntive: browned beef. Please give me five both Table 4 and Table 5, the prompts starting with ”Now\nmorepossibleadjectivesthatcandescribe Igiveyou...”,comparedtothosestartingwith”Pleasegive\nthebrownedbeefwell. me ...”, result in a weaker instruction following ability of\nthemodel. Therefore,wechoosetheprompt: ”Pleasegive\nGPT-3.5: 1. Juicybeef2. Tenderbeef3. Flavorful\nme five adjectives that can describe the visual feature of a\nbeef4.Savorybeef5.Succulentbeef\nphotoofa/an... well.”\nUser: Pleasegivemefivecommonadjectivesthat Impactofthenumberofauxiliaryattributest. InTa-\ncandescribeabrownedbeefwell.\nble4,weobservethatthegeneratedattributesdescribethe\ncompositionstovaryingdegrees,withlateritemsinthese-\nGPT-3.5: 1. Juicy2. Brown3. Savory4. Tender5.\nquence being less relevant generally. Therefore, we study\nSucculent\nontheinfluenceofthenumberofauxiliaryattributest.\nUser: Pleasegivemefiveadjectivesthatcande-\nTable 7 and Table 8 show the generated text using dif-\nscribethevisualfeatureofaphotoofa/an\nbronedbeefwell. ferent t of compositions large garden and young\ngirl. The results demonstrate that the greater the num-\nGPT-3.5: 1.Juicy2.Glistening3.Crispy4.Sizzling ber,themoregenericadjectiveswithirrelevantinformation\n5.Mouthwatering areincluded,forexample,Captivatingisgeneratedfor\nbothcompositions. Inaddition,withtincreasing,thenoise\nTable 4. Example prompts with the composition browned\nin the generated text due to the uncertainty of the model\nbeefinputtoGPT-3.5andthegeneratedtext.\n11\nComposition Train Validation Test\nDataset |A| |O| |A×O| |C | |X| |C | |C | |X| |C | |C | |X|\ns s u s u\nMIT-States[11] 115 245 28175 1262 30338 300 300 10420 400 400 12995\nC-GQA[25] 413 674 278362 5592 26920 1252 1040 7280 888 923 5098\nVAW-CZSL[38] 440 541 238040 11175 72203 2121 2322 9524 2449 2470 10856\nTable6.Summarystatisticsofthedatasetsusedinourexperiments.\nt the generated text for the composition B. Obtainment of The Last Hidden States of\nlarge garden MLLM\n3 1.Lush2.Vibrant3.Flourishing\nWe input the attribute (object) word into LLaVA v1.5,\nwhich first tokenizes the word into z tokens. These to-\n5 1. Lush2. Expansive3. Vibrant4. Serene\nkens pass through all attention blocks in the MLLM, ul-\n5.Verdant\ntimately generating z embeddings of dimension d after\nm\nthelast block, namedthelast hiddenstates. Subsequently,\n10 1. Lush2. Vibrant3. Expansive4. Serene\nweapplyaveragepoolingtothesezembeddingsofdimen-\n5. Colorful 6. Beautiful 7. Bountiful 8.\nsiond toobtainad -dimensionalembeddingthatrepre-\nCaptivating9.Peaceful10.Tranquil m m\nsentstheattribute. Sincethelasthiddenstatesaredesigned\nto generate the next token rather than for representation,\nTable7. Impactoftonthegeneratedtextwiththecomposition Muennighoffetal.[24]leveragesinstructiontofine-tunethe\nlarge garden. NotethattheinputpromptprovidedtoGPT- model. Therefore,wefine-tunethelasthiddenstateswitha\n3.5isthepreviouslyselectedone,replacingtandthecomposition. lowlearningrateduringthetrainingphaseofTRIDENT.\nIt is important to note that although LLaVA v1.5 may\nt the generated text for the composition\nhaveseencertainimagesduringtraining,themodelisasked\nyoung girl\ntogeneratetextualdescriptionsofimagesinanautoregres-\n3 1.Innocent2.Radiant3.Youthful\nsivemannerduringtraining. Thetextualdescriptionsfocus\nonthemaincontentoftheimage,ratherthanthe”attribute-\n5 1. Youthful2. Innocent3. Vibrant4. Ra- object” label. Therefore, there is no issue of data leakage\ndiant5.Captivating\nwhentrainingTRIDENT.\n10 1.Radiant2.Innocent3.Vibrant4.Capti- C.DataStatictics\nvating5.Playful6.Ethereal7.Alluring8.\nCharming9.Enchanting10.Happpy We evaluate our model on three challenging CZSL bench-\nmark datasets: MIT-states [11], C-GQA [25], and VAW-\nCZSL [38]. MIT-states consists of diverse real-world im-\nTable8. Impactoftonthegeneratedtextwiththecomposition ages labeled by early image search engine technology. C-\nyoung girl.NotethattheinputpromptprovidedtoGPT-3.5is\nGQAandVAW-CZSLaretwomorechallengingbenchmark\nthepreviouslyselectedone,replacingtandthecomposition.\ndatasetsthatconsistofbroadcollectionsofin-the-wildim-\naboutthegivenimagegrows. Theyoung girlmaynot ages. C-GQAhasmoreone-to-onecompositions,whileob-\nbe happy, yet the model fails to find ten words to describe jects in VAW-CZSL share more attributes. Table 6 shows\nher, so it has to guses. Therefore, we set t to 3, this mini- detaileddatastatisticsfollowingthecommondatasplitsof\nmizesthegeneraladjectivesandnoisewhileretaininguse- MIT-States[11], C-GQA[25]andVAW-CZSL[38]. MIT-\nfulinformation. States contains 53753 images, with 115 attributes and 245\nPost-processing of generated text. GPT-3.5 generates objects. Itcomprises1262seencompositionsand300/400\na segment of text, which we need to process into multiple (validation/test) unseen compositions. C-GQA is a natu-\nuseful words by exploiting regular expressions. However, ral image dataset which contains 39298 images, with 413\ntheauxiliaryattributesgeneratedbyLLMmaycontainthe attributesand764objects. Itincludes5,592seencomposi-\nattributeoftheinputcomposition, forexample, generating tions and 1,040/923 (validation/test) unseen compositions.\nancient for ancient building. At this point, we VAW-CZSLisalargerdatasetwhichcontains440attributes\nreusethemodeltogeneratet+1adjectivesforthiscompo- and541objectsfor238040images,anditissplitinto11175\nsitionandselectthreeadjectivesthatarenottheattributeof seen and 2322/2470 unseen compositions for training and\ntheinputcomposition. validation/testing,respectively.\n12\n31.0 23.0\nHM HM\n30.5 22.5\n30.0 22.0\n29.5 21.5\n29.0 21.0\n15.0 AUC 8.0 AUC\n14.5\n7.5\n14.0\n7.0\n13.5\n13.0 6.5\n2 4 6 8 2 4 6 8\nNumber of visual features X (Q=X, P=X/2) Number of visual features X (Q=X, P=X/2)\n(a)MIT-States (b)C-GQA\nFigure4.Impactofthenumberofthevisualfeatureson(a)MIT-Statesand(b)C-GQA.\n31.0 22.8\nHM HM\n30.8 22.6\n30.6\n22.4\n30.4\n22.2\n30.2\n30.0 22.0\n14.4 AUC 8.4 AUC\n14.2 8.2\n14.0 8.0\n13.8 7.8\n13.6 7.6\n13.4 7.4\n0.03 0.06 0.09 0.12 0.15 0.03 0.06 0.09 0.12 0.15\nLabel smoothing factor\n(a)MIT-States (b)C-GQA\nFigure5.Impactofthelabelsmoothingfactoron(a)MIT-Statesand(b)C-GQA.\nD.Implementationdetails withrespecttodifferentnumbersofvisualfeaturesanddif-\nferent label smoothing factors, respectively. Experiments\nWe use NVIDIA PTX 3090 GPU to train all models un-\nexploring the impact of hyperparameters are conducted on\nder the Pytorch framework [31]. Since L leverages\ncomp datasetsMIT-StatesandC-GQA.\nimage features during training, we use a Batch Normal-\nImpactofthenumberofvisualfeatures. Weillustrate\nization, ReLU and 0.3 dropout for Image embedder. We\ntheperformanceofTRIDENTinfluencedbydifferentnum-\ntrainTRIDENTbyAdamoptimizerwithweightdecay5e-\nbersofattributefeaturesinFigure.InFigure4a,theperfor-\n5,learningrates1.5e-6forwordembeddingaswellas2e-4\nmanceofourmodelonMIT-Statesgenerallyimproveswith\nforothermodulesonthreedatasets. Wedecaythelearning\nthe increasing number of visual features, but subsequently\nrate by 10 at epoch 30 and 40. The temperature variable\ndeclines. This trend is reasonable, as a greater number of\nof cosine similarity δ is set to 0.05. For weighting coeffi-\nVisual features contains more useful information, thereby\ncients γ ,γ ,γ ,andγ , we set them to 0.1, 1,\northo comp attr obj\nenhancingtheperformance. However,thenumberofuseful\n0.5,and0.5,respectively.\nfeaturesislimited;thus,anexcessivenumberofvisualfea-\nturesmayintroduceredundancyandnoise,ultimatelyham-\nE.ImpactofHyperparameters\nperingtheperformanceofthemodel.\nToprovidemoreinsightintotheeffectofvisualfeaturesand However,inFigure4b,asthenumberofvisualfeatures\nlabelsmoothing,westudyontheperformanceofTRIDENT increases,theperformanceofthemodelonC-GQAtendsto\n13\nMH\nCUA\nMH\nCUA\nMH\nCUA\nMH\nCUA\ndeclineoverall.Thismaybeattributedtothemodel’sstrong\nexpressivecapabilityinhandlingcompositionreasoning.In\nthelow-noiseC-GQAdataset,optimalperformancecanbe\nachievedusingonlytwofeatures. Increasingthenumberof\nfeatures, however, results in heightened model complexity\nwithouttangiblebenefits, potentiallyimpairinggeneraliza-\ntion to unseen compositions. In contrast, the MIT-States\ndataset exhibits significant noise; thus, while the increase\nofvisualfeaturesmayintroducemorenoise, italsoneces-\nsitates a greater amount of useful information, which can\neffectivelymitigatetheimpactofthenoise.\nImpactofthenumberoflabelsmoothingfactor. The\nlabelsmoothingfactorαmodulatestheextenttowhichthe\nmodel’sconfidenceinseencompositionsisattenuated.Fig-\nure5ashowsthatasαincreases, themodel’sperformance\non MIT-States initially improves before subsequently de-\nclining. Thisisbecauseifalphaistoosmall,labelsmooth-\ning fails to enhance generalization, while if alpha is too\nlarge, it adversely affects the model’s ability to learn the\nrepresentationoftheoriginallabels,resultinginmorelosses\nthan gains. However, as shown in Figure 5b, the model\nachieves the best performance with C-GQA a smaller α.\nThismaybeattributedtothefactthat, comparedtoevery-\nday objects, LLMs are less familiar with in-the-wild ob-\njects, leading to relatively lower quality in the generated\nauxiliary attributes; thus, a smaller smoothing factor can\nmitigatetheimpact.\n14",
    "pdf_filename": "Leveraging_MLLM_Embeddings_and_Attribute_Smoothing_for_Compositional_Zero-Shot_Learning.pdf"
}