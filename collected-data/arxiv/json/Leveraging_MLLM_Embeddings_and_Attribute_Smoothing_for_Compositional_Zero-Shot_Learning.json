{
    "title": "Leveraging MLLM Embeddings and Attribute Smoothing for Compositional Zero-Shot Learning",
    "context": "Compositional zero-shot learning (CZSL) aims to recog- nize novel compositions of attributes and objects learned from seen compositions. Previous works disentangle at- tribute and object by extracting shared and exclusive parts between image pairs sharing the same attribute (object), as well as aligning them with pretrained word embeddings to improve unseen attribute-object recognition. Despite the significant achievements of existing efforts, they are ham- pered by three limitations: (1) the efficacy of disentangle- ment is compromised due to the influence of the background and the intricate entanglement of attribute with object in the same parts. (2) existing word embeddings fail to cap- ture complex multimodal semantic information. (3) over- confidence exhibited by existing models in seen composi- tions hinders their generalization to novel compositions. Being aware of these, we propose a novel framework named Multimodal Large Language Model (MLLM) embeddings and atTRibute smoothIng guiDEd diseNTanglement (TRI- DENT) for CZSL. First, we leverage feature adaptive ag- gregation modules to mitigate the impact of background, and utilize learnable condition masks to capture multigran- ularity features for disentanglement. Then, the last hid- den states of MLLM are employed as word embeddings for their superior representation capabilities. Moreover, we propose attribute smoothing with auxiliary attributes gen- erated by Large Language Model (LLM) for seen composi- tions, addressing the issue of overconfidence by encourag- ing the model to learn more attributes in one given compo- sition. Extensive experiments demonstrate that TRIDENT achieves state-of-the-art performance on three benchmarks. As for the study of compositional generalization ability in- herent to humans, compositional zero-shot learning (CZSL) *Corresponding author [22, 26, 33] is proposed to enable machines to recognize unseen attribute-object compositions by leveraging knowl- edge of attributes and objects (i.e., primitives) learned from seen compositions. Specifically, in the training phase, mod- els are provided with images and compositional labels (e.g., ripe orange and peeled apple). During the test- ing phase, given an image depicting a novel composition (e.g., peeled orange), models are assigned to classify the image into the corresponding category [44]. Prior works [22, 27] focus on mapping the visual fea- tures and the word embeddings of compositions into a joint space. These methods have poor generalization capabil- ity to unseen compositions, as they fail to learn primitives. Therefore, recent studies [8, 14, 38] consider visual disen- tanglement. Among them, some prominent works deploy a triplet of images to disentangle: a given image (noted as the main image), and two supplementary images, each sharing either the same attribute or the same object as the main im- age. The triplet of images is treated as two image pairs for subsequent analysis. These approaches aim to disentangle attribute and object by analyzing the shared and exclusive features of the image pair, as well as aligning them with word embeddings (e.g., GloVe [32]), as shown in Figure 1. Although these pioneer research studies have achieved great progress, they exhibit three limitations: L1: Disentanglement is impeded due to the influence of the background and the intricate entanglement of attribute with object in the same parts of image. On the one hand, models tend to extract the background feature unique to one image in the pair as the disentangled exclusive features. On the other hand, some existing methods [36, 38] com- pute the similarity of image pairs for disentanglement at the spatial level. However, disentangling attribute and object at the spatial level presents significant challenges because they entangle in the same spatial features. Taking an image of ripe apple as an example, the spatial regions corre- sponding to the attribute ”ripe” and the object ”apple” are fully co-located. L2: Existing word embeddings lack the depth needed to capture complex multimodal semantic information. To 1 arXiv:2411.12584v1  [cs.CV]  18 Nov 2024",
    "body": "Leveraging MLLM Embeddings and Attribute Smoothing for\nCompositional Zero-Shot Learning\nXudong Yan1, Songhe Feng1*, Yang Zhang1, Jian Yang2, Yueguan Lin2, Haojun Fei2\n1School of Computer Science and Technology, Beijing Jiaotong University\n2Qifu Technology\n{xud yan, shfeng, chefzhang}@bjtu.edu.cn, {feihaojun, yangjian1, linyueguan}-jk@360shuke.com\nAbstract\nCompositional zero-shot learning (CZSL) aims to recog-\nnize novel compositions of attributes and objects learned\nfrom seen compositions.\nPrevious works disentangle at-\ntribute and object by extracting shared and exclusive parts\nbetween image pairs sharing the same attribute (object), as\nwell as aligning them with pretrained word embeddings to\nimprove unseen attribute-object recognition. Despite the\nsignificant achievements of existing efforts, they are ham-\npered by three limitations: (1) the efficacy of disentangle-\nment is compromised due to the influence of the background\nand the intricate entanglement of attribute with object in\nthe same parts. (2) existing word embeddings fail to cap-\nture complex multimodal semantic information. (3) over-\nconfidence exhibited by existing models in seen composi-\ntions hinders their generalization to novel compositions.\nBeing aware of these, we propose a novel framework named\nMultimodal Large Language Model (MLLM) embeddings\nand atTRibute smoothIng guiDEd diseNTanglement (TRI-\nDENT) for CZSL. First, we leverage feature adaptive ag-\ngregation modules to mitigate the impact of background,\nand utilize learnable condition masks to capture multigran-\nularity features for disentanglement.\nThen, the last hid-\nden states of MLLM are employed as word embeddings for\ntheir superior representation capabilities.\nMoreover, we\npropose attribute smoothing with auxiliary attributes gen-\nerated by Large Language Model (LLM) for seen composi-\ntions, addressing the issue of overconfidence by encourag-\ning the model to learn more attributes in one given compo-\nsition. Extensive experiments demonstrate that TRIDENT\nachieves state-of-the-art performance on three benchmarks.\n1. Introduction\nAs for the study of compositional generalization ability in-\nherent to humans, compositional zero-shot learning (CZSL)\n*Corresponding author\n[22, 26, 33] is proposed to enable machines to recognize\nunseen attribute-object compositions by leveraging knowl-\nedge of attributes and objects (i.e., primitives) learned from\nseen compositions. Specifically, in the training phase, mod-\nels are provided with images and compositional labels (e.g.,\nripe orange and peeled apple). During the test-\ning phase, given an image depicting a novel composition\n(e.g., peeled orange), models are assigned to classify\nthe image into the corresponding category [44].\nPrior works [22, 27] focus on mapping the visual fea-\ntures and the word embeddings of compositions into a joint\nspace.\nThese methods have poor generalization capabil-\nity to unseen compositions, as they fail to learn primitives.\nTherefore, recent studies [8, 14, 38] consider visual disen-\ntanglement. Among them, some prominent works deploy a\ntriplet of images to disentangle: a given image (noted as the\nmain image), and two supplementary images, each sharing\neither the same attribute or the same object as the main im-\nage. The triplet of images is treated as two image pairs for\nsubsequent analysis. These approaches aim to disentangle\nattribute and object by analyzing the shared and exclusive\nfeatures of the image pair, as well as aligning them with\nword embeddings (e.g., GloVe [32]), as shown in Figure 1.\nAlthough these pioneer research studies have achieved great\nprogress, they exhibit three limitations:\nL1: Disentanglement is impeded due to the influence of\nthe background and the intricate entanglement of attribute\nwith object in the same parts of image. On the one hand,\nmodels tend to extract the background feature unique to\none image in the pair as the disentangled exclusive features.\nOn the other hand, some existing methods [36, 38] com-\npute the similarity of image pairs for disentanglement at the\nspatial level. However, disentangling attribute and object\nat the spatial level presents significant challenges because\nthey entangle in the same spatial features. Taking an image\nof ripe apple as an example, the spatial regions corre-\nsponding to the attribute ”ripe” and the object ”apple”\nare fully co-located.\nL2: Existing word embeddings lack the depth needed\nto capture complex multimodal semantic information. To\n1\narXiv:2411.12584v1  [cs.CV]  18 Nov 2024\n\nripe\nyellow\nred\napple\ncar\norange\n......\nripe\nyellow\nred\napple\ncar\norange\n......\nred, \ncrimson, \nlustrous\nrefresh,\nbeige,\ncrisp,\nripe apple\npeeled apple\n“ripe”\n“apple”\n“peeled”\nExisting Method\nTRIDENT (Ours)\nripe apple\npeeled apple\n“ripe”\n“apple”\n“peeled”\nLLM\npeeled \napple\nripe\n apple\nFigure 1. A general comparison between the existing method and our proposed TRIDENT. Note that, we only present the representation\nlearning of an image pair sharing the object for brevity.\nbegin with, word embeddings, such as Word2Vec [21] and\nGloVe [32], are grounded in word frequency and contex-\ntual co-occurrence, rather than capturing high-level seman-\ntic nuances[39]. Moreover, the process of aligning visual\nfeatures with word embeddings can be viewed as a form\nof cross-modal matching; however, these word embeddings\nare trained only in a single text modal, failing to capture\ncross-modal information between images and texts.\nL3: Existing methods display excessive confidence in\nseen compositions, impairing their ability to generalize to-\nward novel compositions. Due to the one-hot label used\nduring training, these approaches are limited by learning\nonly one attribute and object, neglecting the fact that objects\nnaturally exhibit multiple attributes [43].\nConsequently,\nmodels exhibit overconfidence in the disentangled ground-\ntruth attribute, treating other attributes that can describe the\nobject as negative attributes, which results in the diminished\nperformance on unseen compositions.\nTo address the aforementioned limitations, we propose\na novel framework named Multimodal Large Language\nModel (MLLM) embeddings and atTRibute smoothIng\nguiDEd diseNTanglement (TRIDENT), which consists of\nthree major modules: visual feature extraction, attribute-\nobject disentanglement, and feature alignment. The first\nmodule leverages feature adaptive aggregation (FAA) mod-\nules to mitigate the impact of background noise, and ex-\nploits learnable condition masks to learn multi-granularity\nfeatures to improve subsequent disentanglement. The sec-\nond module aims at leveraging shared and exclusive weights\nof image pairs to disentangle attribute and object under the\nthe paradigm that apart from the shared features of the im-\nage pair, each image has its own exclusive features. The\nthird module is intended to align the visual features of com-\npositions and disentangled primitives with the last hidden\nstates of an MLLM, LLaVA v1.5 [16], i.e., MLLM embed-\ndings. This is inspired by some works [12, 23, 24, 41],\nwhich find that the last hidden states of (M)LLM exhibit\npowerful representational capabilities in embedding tasks,\nsuch as retrieval and classification. Moreover, to tackle the\nissue that the ineffective overconfidence of the models re-\ngarding ground-truth attribute hinders them from generaliz-\ning to unseen compositions, we exploit a Large Language\nModel (LLM), GPT-3.5 [29] to generate auxiliary attributes\nbased on attribute-object compositions and perform label\nsmoothing for attributes, i.e., attribute smoothing.\nIn summary, the contributions of our work are three-fold:\n1. We propose novel feature adaptive aggregation mod-\nules to reduce the impact of background, and utilize learn-\nable condition masks to capture multi-granularity features\nfor disentanglement in CZSL.\n2. We employ both LLM and MLLM to guide attribute-\nobject disentanglement by generating auxiliary attributes\nand representing word embeddings, respectively.\nTo the\nbest of our knowledge, we are the first to leverage both LLM\nand MLLM to advance disentanglement in CZSL task.\n3.\nWe conduct extensive experiments to evaluate our\nmethod on three CZSL benchmarks, showing that TRI-\nDENT has achieved state-of-the-art performance.\nThe\nsource code will be released soon 1.\n2. Related Work\nCompositional zero-shot learning (CZSL). Prior works in\nCZSL can be broadly divided into two main streams. One\nmain stream is to learn representations of compositions in\na joint space. SymNet [15] proposes to learn symmetry\nproperty in compositions. Co-CGE [20] leverages a Graph\nConvolutional Neural Network to learn compositional rep-\nresentations. The other main stream aims at disentangling\nvisual representations of primitives to reduce composition\nlearning into primitive learning. SCEN [13] leverages con-\ntrastive loss to excavate discriminative prototypes of primi-\ntives. OADis [38] disentangles primitives by affinity mod-\nules. CANet [42] learns conditional attribute conditioned\non the recognized object and the input image.\nMore recent works [9, 18, 28] focus on leveraging\nthe encyclopedic knowledge of pretrained vision-language\nmodels (VLM), such as Contrastive Language-Image Pre-\ntraining (CLIP) [35] and Context Optimization (CoOp)\n[46], to encode and align images and texts.\nLarge language model (LLM). LLMs have realized\nsignificant advancements thanks to the scaling up of train-\ning data and the increase in the number of parameters. Early\nmodels, such as BERT [6] and GPT-2 [34], initially exhibit\nstrong capabilities in understanding and generating human-\n1https://github.com/xud-yan/Trident\n2\n\nlike language. Subsequently, GPT-3 [4] and LLaMA [40]\ndemonstrate great breakthroughs across numerous language\nbenchmarks.\nMoreover, by performing instruction fine-\ntuning on LLM, ChatGPT [30] and Vicuna [5, 45] are able\nto comprehend and follow human instructions better.\nExpanding on LLM, Multimodal Large Language Model\n(MLLM) incorporates a pretrained visual encoder for\nvision-language tasks. Flamingo [1] first integrates Vision\nTransformer (ViT) [7] and LLM by gated cross-attention.\nRecently, LLaVA [17] and LLaVA v1.5 [16] introduce vi-\nsual instruction tuning to enhance instruction following ca-\npability. The visual understanding part of LLaVA v1.5 con-\nsists of a ViT and a multilayer perceptron (MLP) cross-\nmodal connector (CMC). CMC processes visual features\nbefore the last layer of ViT, aligning the visual space of ViT\nwith the linguistic space of LLM. We choose LLaVA v1.5\nas our foundational MLLM as it has demonstrated state-of-\nthe-art performance across various tasks.\nRecently, exploring the powerful language capabilities\nof (M)LLM to handle representation tasks (e.g., retrieval)\nhas emerged as a prominent research domain. SGPT [23]\nexploits the last hidden states of LLM for the input token\nsequence or a special learnable token to derive represen-\ntational embeddings. Subsequently, GritLM [24] applies\nmean pooling over the last hidden states of LLM to yield the\ntextual embeddings. FROMAGe [12] uses a learnable token\nto represent the text fed into MLLM for image retrieval.\n3. Approach\n3.1. Task Formulation\nCompositional zero-shot learning (CZSL) aims at learning a\nmodel that can recognize unseen compositions of attributes\nand objects that are learned from seen compositions. Given\nan attribute set A and an object set O, the attributes and ob-\njects are composed to form a composition set C = A × O.\nThe composition set C is divided into two disjoint sets: the\nseen composition set Cs and the unseen composition set Cu,\nwhere Cs ∩Cu = ∅and Cs ∪Cu = C. The model is trained\nwith a seen training set Dtr = {(xs, cs)}, where xs ∈Xs\nis an image from the seen image set Xs corresponding to\nthe seen composition set Cs, and cs ∈Cs is the label of xs.\nFollowing the Generalized CZSL [33], the model is eval-\nuated on a predefined test set Dte = {(xte, cte)}, where\nxte ∈Xte is an image from the unseen image set Xte corre-\nsponding to the composition subset Cte of C, i.e., Cte ⊆C,\nand cte ∈Cte is the label of xte. The aim of CZSL task is to\nlearn a model M : Xte →Cte that predicts labels cte from\nCte for the input images xte ∈Xte.\n3.2. TRIDENT\nAs the major novelty, we propose a novel framework named\nMLLM embeddings and attribute smoothing guided disen-\ntanglement framework (TRIDENT) for CZSL, as shown in\nFigure 2. It consists of three major modules: (1) visual\nfeature extraction, (2) attribute-object disentanglement, and\n(3) feature alignment. We now detail each module of TRI-\nDENT in this section.\n3.2.1. Visual Feature Extraction\nAs shown in Figure 2, we denote a given image with the\nattribute-object composition label (e.g. ripe apple) as\nthe main image xm, and randomly sample an image with\nthe same attribute xa (i.e., ripe orange), as well as an\nimage sharing the same object xo (i.e., peeled apple)\nto comprise a triplet image set. For the convenience of ex-\npression, we simply use ximg (where img ∈{m, a, o}) to\ncollectively denote the images as they are processed using\nthe same module.\nVisual feature extraction backbone. As mentioned be-\nfore, since LLaVA v1.5 is used as our fundamental MLLM,\nwe directly leverage the visual encoder, ViT, and cross-\nmodal connector (CMC) from the model to extract visual\nfeatures. Specifically, the image ximg is partitioned into\nn patch tokens, which are subsequently put into ViT along\nwith the [CLS] token. Afterward, the output of patch to-\nkens before the last layer of ViT is fed into the CMC mod-\nule, as implemented in LLaVA v1.5. To align the dimension\nof patch tokens output by CMC with that of [CLS] token\nproduced by ViT, the patch tokens output by CMC are input\ninto a linear layer. Consequently, we obtain one feature vec-\ntor of [CLS] token f img\ncls\n∈Rd and a patch feature matrix\nof n patch tokens F img\npatch ∈Rn×d, where d is the dimension\nof the features.\nLocal features extraction. Intuitively, the composition\n(e.g., ripe apple) only occupies a few parts of the im-\nage. Since each patch token usually corresponds to one lo-\ncal region of the image, to filter out background noise and\nfocus on related regions, we deploy a set of feature adap-\ntive aggregation (FAA) modules to derive p relevant local\nfeatures of ximg, where each FAA module is formulated as\nfollows:\n(\nv = agg ⊗F img\npatch\nagg = σ(Conv(F img\npatch))\n(1)\nwhere Conv(·) represents the 1 × 1 convolution layer, σ(·)\ndenotes the sigmoid activation function, agg ∈Rn is the\nweight vector, the k-th element of agg is the weight for k-\nth patch feature. ⊗represents matrix product, and v ∈Rd is\nthe local feature obtained by an FAA module. We vertically\nconcatenate the local features produced by p FAA modules\nto obtain the local feature matrix F img\nl\n∈Rp×d.\nGlobal features extraction. Normally, the ViT output\nof [CLS] token is regarded as containing various global\ninformation of the image, which highly entangles both at-\ntribute and object features together[8]. To disperse multi-\ngranularity global information into different representa-\n3\n\nFAAp\nMLP\nMLP\nImage \nEmbedder\nPlease give me three adjectives \nthat can describe the visual \nfeature of a photo of a/an ...\nLLM\nsmooth, round, juicy\nred, crimson, mature\nrefresh, beige, crisp\nC\n  : ripe orange\nAttribute\nSmoothing\n�comp\nThe Last Hidden States of MLLM\nMLP\n�attr\nComposed \nPair \nEmbedding\nVisual Feature Extraction\nAttribute-Object Disentanglement\nFeature Alignment\n  : ripe apple\n  : peeled apple\n����\n���\nattribute words\nobject words\n∙\n1 -\n∙\n∙\n∙\n1 -\n�����\n���\n�����\n���\n����\n���\n�����\n���\n∙\n∙\n∙\n∙\n����\n���\n����\n���\n�����\n���\n1 -\n1 -\nMLP\nMLP\nC\nMLP\nC\nMLP\nC\nViT\n[CLS]\n∙\n∙\nC1\nCq\n  ⋮×q  \n⋮×p  ⋮\nFAA1\n�ortho\n��\n��\n�obj\nVisual Feature �\nCMC\n��\n��\n��\n ...\npatch tokens\nMLP\nMLP\n⋮\nFigure 2. The overall architecture of our proposed TRIDENT. TRIDENT consists of three major modules: (a) visual feature extraction,\n(b) attribute-object disentanglement, and (c) feature alignment.\ntions, q learnable condition masks are applied to f img\ncls\nto\nobtain q different global representations, where each global\nrepresentation is computed as:\nu = f img\ncls ⊙c\n(2)\nwhere u ∈Rd denotes each global representation. Here\nc ∈Rd refers to each learnable condition mask and ⊙is the\nelement-wise multiplication. Consequently, we vertically\nconcatenate q global representations to derive the global\nfeature matrix F img\ng\n∈Rq×d.\nFeatures concatenation. Finally, F img\nl\nand F img\ng\nare\nvertically concatenated to form the visual features of ximg,\ni.e., F img = [F img\nl\n, F img\ng\n] ∈R(p+q)×d, which is used for\nthe following disentanglement of attribute and object.\nOrthogonal regularization. We ideally want features\nextracted by different modules can represent different infor-\nmation of the image ximg. To this end, we further introduce\nthe orthogonal regularization, i.e.:\nLortho =\nX\nimg∈{m,a,o}\n(||F imgF imgT −I||F ro)\n(3)\nwhere I ∈R(p+q)×(p+q) is the identity matrix. || · ||F ro\nrefers to the Frobenius norm of the matrix.\nImage embedder. Inspired by [26], for the input image\nximg, we first use AveragePools Avg(·) on F img\ng\nand F img\nl\n,\nrespectively, and horizontally concatenate them by Cat(·, ·)\nto aggregate both global and local visual information of\nximg corresponding to the composition label. Then the con-\ncatenated feature passes through a linear layer Lincomp(·)\nto derive the final feature representation f img\ncomp. This mod-\nule is formulated as follows:\nf img\ncomp = Lincomp(Cat(Avg(F img\ng\n), Avg(F img\nl\n)))\n(4)\nwhere f img\ncomp ∈R2d denotes the visual feature correspond-\ning to the composition. This module is designed to learn the\nvisual features of images associated with their correspond-\ning composition labels, serving as the primary branch for\ninference.\n3.2.2. Attribute-Object Disentanglement\nAs mentioned before, one of the key challenges for CZSL\ntask is to disentangle attribute and object from visual fea-\ntures. To overcome such challenge, we propose a novel\nweighted disentanglement module to disentangle primi-\ntives, as illustrated in Figure 2. For brevity, one image pair\nxm and xa from the triplet image set is taken as an example\nto elaborate on this module, while another image pair xm\nand xo follows the same architecture.\nWeights computation. The features of xm and xa (i.e.,\nF a and F o) are vertically concatenated and fed into two\nMLP modules to derive their respective weights of shared\nattribute features relative to each other, and subsequently\nutilize them to compute the weights of their own exclusive\nobject features as follows:\n\n\n\n\n\n\n\n\n\n\n\nwm2a\nattr = σ(MLPm2a([F m, F a]))\nwm2a\nobj\n= 1 −wm2a\nattr\nwa2m\nattr = σ(MLPa2m([F m, F a]))\nwa2m\nobj\n= 1 −wa2m\nattr\n(5)\n4\n\nwhere wm2a\nattr , wa2m\nattr ∈Rh demonstrate the weights of the\nshared attribute features of xm relative to xa, and xa relative\nto xm, respectively. wm2a\nobj\nand wa2m\nobj\ndenote the weights\nof exclusive object features corresponding to xm and xa,\nrespectively, which are derived by ”1 −shared weights”\nparadigm as beyond the shared features of the image pair\nare the exclusive features of each image. Taking wm2a\nattr as\nan example, its k-th element refers to the shared attribute\nproportion of k-th feature of xm relative to xa.\nDisentangled features obtainment. We multiply ele-\nments of each weight by the corresponding features and\nthen calculate the average. The following takes the pro-\ncess of obtaining the shared attribute features of image xm\nrelative to xa as an example:\nf m2a\nattr = 1\nh\nh\nX\ni=1\nwm2a\nattr i F a\ni,:\n(6)\nwhere F ai,: denotes the i-th row of F a, i.e., the i-th fea-\nture of xa. wm2a\nattr i refers to the i-th element of wm2a\nattr , and\nf m2a\nattr ∈Rd is the shared attribute feature of xm relative to\nxa.\nFor the image pair of xm and xa, four parts are obtained:\nthe shared attribute features of xm relative to xa, and xa\nrelative to xm, as well as two exclusive object features of the\nimage pair, respectively. These four features are marked as\nf e\npri, where e ∈{m2a, a2m} and pri ∈{attr, obj}. Then\nthe shared attribute feature of xa and xm without relativity\nis obtained by an MLP layer, which is less dependent on the\nobject. The process is as follows:\nf ma\nattr = MLPma(Cat(f m2a\nattr , f a2m\nattr ))\n(7)\nSimilarly, we disentangle attribute and object for xm and\nxo and obtain the same features as xm and xa: f e\npri, where\ne ∈{m2o, o2m} and pri ∈{obj, attr} and f mo\nobj .\n3.2.3. Feature Alignment\nInspired by [24] that leverages the last hidden states as\nthe representation embeddings, we consider the last hid-\nden states of LLaVA v1.5 [16] as our MLLM embeddings\nfor words. Moreover, to tackle the problem that the inef-\nfective overconfidence exhibited by the models in terms of\nthe ground-truth attribute hinders them from generalizing\nto unseen compositions, GPT 3.5 is employed to generate\nseveral auxiliary attributes that describe an object with only\none ground-truth attribute and perform label smoothing dur-\ning attribute alignment. Now we detail each part of feature\nalignment.\nGenerating auxiliary attribute words by LLM. Since\nonly attribute text needs to be generated, we leverage a\nLLM, GPT-3.5, instead of MLLM, to generate several aux-\niliary attributes for each composition. Specifically, the fol-\nlowing prompt is input to LLM: ’Please give me t adjec-\ntives that can describe the visual feature of a photo of a/an\n... well.’, where t is the number of auxiliary attributes and\nattribute-object composition (e.g., peeled apple) is filled in\n’...’.\nPlease refer to Appendix A for more details about\nthe generation of auxiliary attributes by GPT-3.5. Subse-\nquently, the generated auxiliary attribute words form a set\nAa. Therefore, the set of all words Y is obtained, including\nattributes, objects and auxiliary attributes as follows:\nY = A ∪O ∪Aa\n(8)\nObtaining MLLM embeddings for words and com-\npositions. Each word y ∈Y is fed into LLaVA v1.5 to\nget the last hidden states, i.e., LLaV Alhs(·). Please re-\nfer to Appendix B for more details about the obtainment\nof the last hidden states of LLaVA v1.5 for an input word.\nSubsequently, they are passed through an MLP layer to get\nembeddings Eword(·) of aligned dimension with visual fea-\ntures. And for a composed pair c of attribute a and object\no, i.e., c = (a, o), we get the last hidden states of LLaVA\nv1.5 for a and o, respectively, which are then horizontally\nconcatenated and fed into a linear layer Linco(·) to get the\ncomposed pair embedding Eco(·). The process is formu-\nlated as follows:\nEword(y) = MLPwd(LLaV Alhs(y))\n(9)\nEco(c) = Linco(Cat((LLaV Alhs(a), (LLaV Alhs(o)))\n(10)\nWord expanding. Prior works compute cosine similar-\nities of disentangled features and word embeddings only\nwithin the respective domains of attributes or objects, which\nresults in the disentangled attributes and objects still retain-\ning the information of each other. To address the problem,\nwe propose a word expanding strategy, which computes co-\nsine similarities of visual features and the embeddings of all\nwords, including attributes and objects, and treats all words\nexcept the ground-truth word as negative labels.\nAlignment by cross-entropy. Similar to [19], we use\ncross-entropy to process the cosine similarity of visual fea-\ntures and word embeddings. Assume that f is the visual\nembedding and Eword(wd) is the word embedding for the\nword wd ∈Y in a joint space. The classifier logit from f to\nEword(wd) is defined as follows:\nCE(f, wd) =\neδ·cos(f,Eword(wd))\nP\ny∈Y eδ·cos(f,Eword(y))\n(11)\nwhere δ is the temperature variable, and cos(·, ·) denotes\ncosine similarity function. Thus cross-entropy with/without\nlabel smoothing can be uniformly formulated as follows:\nH(f, Y) =\nX\ny∈Y\n−z log(CE(f, y))\nwith z =\n\n\n\n\n\n1 −α,\nif y is ground truth label\nα/t,\nif y is auxiliary label\n0,\notherwise\n(12)\n5\n\nwhere α denotes the smoothing factor, t refers to the num-\nber of auxiliary labels and z ∈[0, 1] represents the tar-\nget value of one-hot or smoothing label. For cross-entropy\nwithout label smoothing, i.e. with one-hot label Hoh, α is\nset to 0. And the cross-entropy with label smoothing is de-\nnoted as Hls.\nFor the disentangled attribute features of one image rel-\native to each other, since a single object exhibits multiple\nattributes, we exploit attribute smoothing with auxiliary at-\ntributes to undermine the confidence in the ground-truth at-\ntribute and learn more related attributes. For the shared at-\ntribute features without relativity, one-hot label is used to\ncompute its classification loss. The loss for disentangled\nattributes can be defined as follows:\nLattr =\nX\nc ∈{m2a, a2m, m2o, o2m}\nHls(F e\nattr, Y) + Hoh(F ma\nattr, Y)\n(13)\nConcerning the disentangled object features, we use\ncross-entropy with one-hot label to learn the prototype of\nthe object and the loss is as follows:\nLobj =\nX\nc ∈{m2a, a2m, m2o, o2m}\nHoh(F e\nobj, Y) + Hoh(F mo\nobj , Y)\n(14)\nWith respect to the visual feature of the image from im-\nage embedder, we calculate the cosine similarity between\nvisual embedding and the composed pair embedding of the\ncorresponding composition label and use one-hot label to\nalign them. The classification loss for compositions is as\nfollows:\nLcomp =\nX\nimg∈{m,a,o}\nHoh(F img\ncomp, Cs)\n(15)\n3.3. Training and Inference\nDuring the training phase, the overall loss function is for-\nmulated as follows:\nL = γorthoLortho + γcompLcomp + γattrLattr + γobjLobj\n(16)\nwhere γortho, γcomp, γattr, and γobj are weighting factors\nto balance the influence of different losses.\nFor inference, we use the composition feature space\ngenerated by the classifier that is obtained by optimizing\nLcomp. Specifically, given an image from test set, the co-\nsine similarities of its visual feature obtained by image em-\nbedder and the composed pair embeddings of all candidate\ncompositions in the test set are computed. The composi-\ntion with the highest similarity is the class predicted by the\nmodel. Note that although the disentanglement branches\nare not used for inference, they still influence the formation\nof the composition feature space through their shared visual\nfeature extraction module described in Section 3.2.1.\n4. Experiment\n4.1. Experiment Setup\nDatasets.\nWe evaluate our model on three challenging\nCZSL benchmark datasets: MIT-states [11], C-GQA [25],\nand VAW-CZSL [38]. We present the introduction and com-\nmon data splits of the three datasets in Appendix C.\nMetrics. Following the common generalized CZSL set-\nting [33], we evaluate our model on seen and unseen pairs\nseparately. Based on them, a calibration bias trades off be-\ntween the accuracies of seen and unseen pairs. We calculate\narea under the curve AUC (in %) using seen and unseen\nclassification accuracies at different biases in test data. The\nbest seen and unseen accuracies Seen and Unseen (in %)\nof the curve are also reported. In addition, we calculate the\nharmonic mean of seen and unseen classification accuracies\nat difference biases and report the best one HM (in %).\nImplementation details. We use the visual encoder of\nLLaVA v1.5, Vit-large-14-336px as our frozen feature ex-\ntractor, whose outputs contain 577 tokens (1 [CLS] and 576\npatch tokens) of 1024 dimensions. The cross-modal con-\nnector of LLaVA v1.5 maps the features to the dimension\nof 4096, the same as last hidden states of based LLM Vi-\ncuna v1.5 [45]. Image embedder and the MLP for words\nmap them to the dimension of 1024 for faster training. TRI-\nDENT and all baseline models are trained with 128 batch\nsize for 50 epochs. The number of global features is set to\n6, 2, 4 for the three datasets, respectively, and the number\nof local features is twice that of global features. The la-\nbel smoothing factor is set to 0.09, 0.03, 0.03 for the three\ndatasets, respectively. The number of generated auxiliary\nattributes for each composition is set to 3. Refer to Ap-\npendix D for more information about implementation.\nBaselines. We compare our TRIDENT with recent and\nprominent approaches in the task of CZSL: SymNet [15],\nCompCos [19], Co-CGE [20], SCEN [13], OADis [38],\nINV [44], CANet [42], and ProCC [10]. We replace their\nbackbone with Vit-large-14-336px and retrain all models\nwith the same epoch for the sake of fairness. In addition,\nalthough comparing TRIDENT with CLIP-based CZSL\nmethods, which rely on the dual-tower architecture, is very\nunfair due to the significant addition of trainable parameters\nand training overhead for both the text and visual encoders,\nwe still choose the foundational CLIP and CoOp models as\nbaselines for their strong zero-shot classification abilities.\n4.2. Results and Discussion\nIn this section, we compare TRIDENT with state-of-the-\nart methods. As shown in Table 1, TRIDENT surpasses\nother models by a substantial margin in general. For MIT-\nStates, TRIDENT boosts AUC, HM, and Unseen from\n13.6%, 29.8%, and 39.9% of CANet to the new state-of-the-\nart performance of 14.2%, 30.9%, and 40.0% with 0.6%,\n6\n\nMIT-States\nC-GQA\nVAW-CZSL\nMethod\nAUC\nHM\nSeen\nUnseen\nAUC\nHM\nSeen\nUnseen\nAUC\nHM\nSeen\nUnseen\nSymNet [15]\n3.2\n13.7\n22.7\n20.1\n1.9\n10.8\n20.3\n11.8\n2.8\n13.5\n20.2\n18.0\nCompCos [19]\n12.3\n28.2\n39.0\n39.5\n5.0\n17.7\n32.8\n19.1\n6.5\n20.8\n30.5\n27.4\nCo-CGE [20]\n10.3\n25.1\n41.0\n33.1\n4.2\n15.2\n32.9\n17.0\n6.2\n19.7\n31.0\n26.1\nSCEN [13]\n9.8\n24.6\n35.1\n36.5\n3.8\n15.3\n31.5\n15.7\n5.7\n19.2\n29.9\n24.5\nOADis [38]\n13.1\n29.0\n42.3\n27.3\n2.3\n12.1\n23.3\n12.8\n4.1\n16.2\n26.0\n20.7\nINV [44]\n11.5\n26.6\n28.5\n25.0\n1.4\n7.9\n28.6\n6.8\n2.0\n11.1\n21.1\n11.9\nCANet [42]\n13.6\n29.8\n46.4\n39.9\n5.7\n18.9\n34.8\n20.5\n6.7\n21.0\n31.2\n27.4\nProCC [10]\n9.5\n28.1\n43.1\n39.1\n3.5\n15.1\n32.4\n15.8\n3.6\n18.9\n26.9\n25.5\nCLIP [28]\n11.0\n26.1\n30.2\n46.0\n1.4\n8.6\n7.5\n25.0\n-\n-\n-\n-\nCoOp [28]\n13.5\n29.8\n34.4\n47.6\n4.4\n17.1\n20.5\n26.8\n-\n-\n-\n-\nTRIDENT (Ours)\n14.2\n30.9\n44.5\n40.0\n8.0\n22.6\n39.5\n24.1\n8.3\n23.4\n33.3\n31.1\nTable 1. Comparison with the state-of-the-art results on MIT-States, C-GQA and VAW-CZSL. We compare our TRIDENT with the\nstate-of-the-art methods on test AUC, best seen (Seen), best unseen (Unseen) and best harmonic mean (HM) accuracies on these three\ndatasets. We measure top-1 AUC on MIT-States and C-GQA, and top-3 AUC on VAW-CZSL. Best results are displayed in boldface, and\nsecond best results are underlined.\n1.1%, and 0.1% improvement, respectively.\nOur model\nachieves competitive performance on MIT-States bench-\nmark, despite considerable label noise [2]. However, for the\nmore challenging benchmark C-GQA, TRIDENT achieves\n8.0%, 22.6%, 39.5%, and 24.1% on the metrics of AUC,\nHM, Seen, and Unseen, providing 2.3%, 3.7%, 4.7%,\nand 3.6% improvements on the previous state-of-the-art\nmodel CANet. For the existing most challenging bench-\nmark dataset VAW-CZSL, TRIDENT attains performance\nof 8.3%, 23.4%, 23.4%, and 33.3%, surpassing CANet by\n1.6%, 2.4%, 2.2%, and 3.7% in terms of AUC, HM, Seen,\nand Unseen. The largest improvement is observed in the\nUnseen metric, indicating that attribute smoothing helps\nenhance the generalization ability of the model. We ob-\nserve TRIDENT performs significantly better than CANet\nregarding all metrics on two challenging and low-noise\nbenchmark dataset C-GQA and VAW-CZSL, indicating the\nefficacy of our approach. This improvement arises from the\nutilization of MLLM embeddings and attribute smoothing,\nwhich enhance attribute-object disentanglement and conse-\nquently facilitate the recognition of unseen compositions\nwhile maintaining performance on seen compositions.\nIn addition, we compare TRIDENT with dual-tower\nCLIP and CoOp after fine-tuned for the CZSL task. Since\nthey are trained on a large amount of image-text data, they\npossess zero-shot image classification capabilities, which\nleads to better classification results for unseen images. Re-\ngarding Unseen metric, CoOp outperforms TRIDENT by\n7.6% and 2.7% on MIT-States and C-GQA, respectively.\nHowever, TRIDENT surpasses CoOp by 0.7% and 1.1%\non the core metrics of AUC and HM on MIT-States, as\nwell as 3.6% and 5.5% on C-GQA, which suggests TRI-\nDENT performs better than CLIP and CoOp in CZSL task.\n4.3. Ablation Study\nEffectiveness of each component. We ablate certain mod-\nule of TRIDENT to evaluate the contribution of each mod-\nMethod\nAUC\nHM\nSeen\nUnseen\nw/o condition masks\n14.0\n30.5\n44.2\n39.8\nw/o FAAs\n13.9\n30.4\n44.4\n39.7\nw/o word expanding\n14.0\n30.1\n44.7\n39.8\nw/o attribute smoothing\n13.9\n30.5\n44.9\n39.5\nw/o Lattr + Lobj\n13.2\n30.1\n43.8\n38.9\nw/o Lortho\n14.1\n30.7\n44.6\n39.7\nTRIDENT\n14.2\n30.9\n44.5\n40.0\nTable 2. Ablation study results on MIT-States. w/o certain part\ndenotes this part is ablated.\nule on MIT-States, as it is the most common used dataset.\nThe ablation results are reported in Table 2. From this table,\nwe gain the following observations.\n1) Both w/o condition masks model and w/o FAAs model\nperform worse than TRIDENT, which validates the impor-\ntance of extracting the multi-granularity features and filter-\ning out the background noise, respectively.\n2) TRIDENT surpasses w/o word expanding model and\nw/o attribute smoothing model on the Unseen metric, yet\nfalls short of them on the Seen metric. The difference be-\ntween TRIDENT and the w/o word expanding model stems\nfrom its more thorough disentanglement, which enhances\nthe recognition of unseen images while weakens the identi-\nfication of seen images. The disparity between TRIDENT\nand the w/o attribute smoothing model arises from attribute\nsmoothing, which diminishes the confidence of the model in\nseen compositions, facilitating its generalization to unseen\ncompositions. However, the improvement of TRIDENT\nover these two models on AUC and HM indicates the ef-\nfectiveness of word expanding and label smoothing strategy.\n3) TRIDENT outperforms w/o Lattr + Lobj model on\nall metrics, confirming that the attribute-object disentan-\nglement module is highly advantageous for generalization\nfrom seen compositions to unseen compositions.\n4) w/o Lortho model is inferior to TRIDENT, which\n7\n\n       MIT-States                                                             C-GQA                                                       VAW-CZSL                          \nRight\nRight\nWrong\nOld Boat\nBroken  Shore\nUnpainted  Wood\nDeflated  Boat\nBroken  Bridge\nOld  Boat\nNarrow Tail\nNarrow  Tail\nFolded  Mountain\nNarrow  Valley\nShattered  Mountain\nVerdant  Mountain\nColorful Pillow\nRed  Pillow\nOrange  Pillow\nRed  Flower\nOrange  Flower\nLarge  Pillow\nBrown Door\nBrown  Door\nBeige  Door\nTan  Curtain\nClosed  Blinds\nWhite  Blinds\nMulticolored \nTeddy-Bear\nPuffy Bear\nOrange Bear\nBrown Bear\nPlush Bear\nRed Bear\nGraffitied Graffiti\nGraffitied  Graffiti\nFaded  Graffiti\nPainted  Graffiti\nBlue  Graffiti\nGraffitied  Wall\nBurnt House\nBurnt  House\nBurnt  Log\nLarge  Fire\nBurnt  Building\nBurnt  Tower\nGreen Leaf\nGreen  Leaf\nBlurry  Leaf\nBrown  Leaf\nThick  Leaf\nGreen  Flower\nWhite Wave\nWhite  Wave\nShallow  Wave\nShallow  Ocean\nWhite  Line\nGray  Wave\n(a) image-to-text retrieval.\nMIT-States\nPeeled\nOrange\nC-GQA\nYoung\nGirl\nVAW-CZSL\nWalking\nElephant\n(b) text-to-image retrieval (successful cases).\nMIT-States\nOld\nChurch\nC-GQA\nGreen\nGrape\nVAW-CZSL\nWooden\nBed\n(c) text-to-image retrieval (failure cases).\nFigure 3. Qualitative analysis. (a) Top-5 image-to-text retrieval cases. The first two rows display successful cases, while the last row\npresents failure cases. (b) Successful cases of top-5 text-to-image retrieval. (c) Failure cases of top-5 text-to-image retrieval. In all cases,\nthe attribute and object of composition label are marked in orange and blue, respectively. And the successful and failure retrieval results\nare tagged in green and red, respectively.\nMethod\nV arient\nAUC\nHM\nSCEN [13]\nft+w2v\n8.2\n22.8\nLLaVAlhs\n10.3\n25.1\nCANet [42]\nft+w2v\n12.3\n28.4\nLLaVAlhs\n12.5\n28.3\nTRIDENT\nft+w2v\n14.0\n29.9\nLLaVAlhs\n14.2\n30.9\nTable 3. Impact of word embedding on MIT-States. ft + w2v\nmeans the sum of Word2Vec and Fasttext. LLAV Alhs represents\nthe last hidden states of LLAVA v1.5.\nsuggests the designed orthogonal regularization is helpful\nto guarantee different features extract different information.\nImpact of word embeddings. Our work leverages the\nlast hidden states of LLaVA v1.5 (LLaV Alhs) as word em-\nbeddings, while Word2Vec [21] and Fasttext [3] are the\nmost common word embeddings for MIT-States in previ-\nous works. In Table 3, based on three models: SCEN [13],\nCANet [42] and TRIDENT, we compare the performance\nof employing the last hidden states of LLaVA v1.5 and the\nsum of Word2Vec and Fasttext (ft+w2v), respectively. The\nresults indicate that the last hidden states of MLLM cap-\nture more complex multimodal semantic information than\nordinary word embeddings.\nFor details on the impact of hyperparameters, including\nthe number of visual features and the label smoothing fac-\ntor, please refer to Appendix E.\n4.4. Qualitative Analysis\nInspired by [8], we use TRIDENT to conduct both image-\nto-text retrieval and text-to-image retrieval experiments on\nthe three datasets. We first consider image-to-text retrieval,\nshown in Figure 3a. The first two rows display success-\nful cases, while the last row presents failure cases. And the\ncases shown in these three columns are drawn from the three\ndatasets, respectively. Given an image, such as the image\nof burnt house, we extract its visual features by image\nembedder and retrieve the top-5 closest composed pair em-\nbeddings of compositions. For successful cases, such as the\nimage labeled burnt house, we notice that the top four\npredictions can both describe logs burning on fire in the im-\nage. In terms of the image labeled green leaf, another\nsuccessful case, the predicted attributes can also describe\nleaf, which is thanks to attribute smoothing learning more\nattributes for an object. For the failure cases, such as the im-\nage labeled multicolored teddy-bear, the model\nfocuses on the main orange bear and neglects the multicol-\nored background, which is attributed to FAA modules.\nWe then consider text-to-image retrieval.\nSuccessful\ncases are shown in Figure 3b, while failure cases are shown\nin Figure 3c. Given a text composition, we embed it and\nretrieve the top-5 closest images. We observe that the re-\ntrieved images of peeled orange are definitely correct.\nHowever, the retrieved images of green grapes are all\nwrong. This is due to the fact that the training images of\ngreen grapes in C-GQA dataset are almost filled with\na single grape, making it difficult for the model to capture\nthe contour features of a bunch of green grapes. The\nimage-to-text and text-to-image retrieval experiments con-\nfirm that our model effectively projects visual features and\nword embeddings into a unified space.\n5. Conclusion\nIn this work, we propose a novel framework termed TRI-\nDENT to address the challenging CZSL task. First, we\nleverage feature adaptive aggregation modules to mitigate\nthe impact of background, and utilize learnable condition\nmasks to capture multi-granularity features for attribute-\nobject disentanglement. In addition, we exploit the last hid-\nden states of MLLM to replace ordinary word embeddings,\nas they can capture complex multimodal semantic informa-\ntion. Moreover, we leverage LLM to generate auxiliary at-\ntributes and perform attribute smoothing to diminish over-\nconfidence of models in seen compositions, which enables\nmodels to generalize to unseen compositions better. Exten-\nsive experiments have been conducted on three challeng-\ning datasets, and the results demonstrate the effectiveness\nof TRIDENT. In the future, we plan to extend our method\nto harness the powerful capabilities of LLMs, MLLMs, and\nCLIP to more effectively address the CZSL task.\n8\n\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-\nsch, Katherine Millican, Malcolm Reynolds, Roman Ring,\nEliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,\nSina Samangooei, Marianne Monteiro, Jacob L Menick,\nSebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sa-\nhand Sharifzadeh, Mikoł aj Bi´nkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Kar´en Simonyan.\nFlamingo: a visual language model for few-shot learning. In\nAdvances in Neural Information Processing Systems, pages\n23716–23736, 2022. 3\n[2] Yuval Atzmon, Felix Kreuk, Uri Shalit, and Gal Chechik.\nA causal view of compositional zero-shot recognition.\nIn\nAdvances in Neural Information Processing Systems, pages\n1462–1473, 2020. 7\n[3] Piotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. Enriching word vectors with subword infor-\nmation. Transactions of the Association for Computational\nLinguistics, 5:135–146, 2017. 8\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei.\nLanguage models are\nfew-shot learners. In Advances in Neural Information Pro-\ncessing Systems, pages 1877–1901, 2020. 3\n[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing.\nVicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality, 2023. 3\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\nConference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages 4171–\n4186, 2019. 2\n[7] Alexey Dosovitskiy.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 3\n[8] Shaozhe Hao, Kai Han, and Kwan-Yee K. Wong. Learning\nattention as disentangler for compositional zero-shot learn-\ning. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 15315–15324,\n2023. 1, 3, 8\n[9] Siteng Huang, Biao Gong, Yutong Feng, Min Zhang, Yiliang\nLv, and Donglin Wang. Troika: Multi-path cross-modal trac-\ntion for compositional zero-shot learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 24005–24014, 2024. 2\n[10] Fushuo Huo, Wenchao Xu, Song Guo, Jingcai Guo, Haozhao\nWang, Ziming Liu, and Xiaocheng Lu. Procc: Progressive\ncross-primitive compatibility for open-world compositional\nzero-shot learning. In Proceedings of the AAAI Conference\non Artificial Intelligence, pages 12689–12697, 2024. 6, 7\n[11] Phillip Isola, Joseph J Lim, and Edward H Adelson. Discov-\nering states and transformations in image collections. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 1383–1391, 2015. 6, 12\n[12] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\nGrounding language models to images for multimodal inputs\nand outputs. In Proceedings of the International Conference\non Machine Learning, pages 17283–17300, 2023. 2, 3\n[13] Xiangyu Li, Xu Yang, Kun Wei, Cheng Deng, and Muli\nYang. Siamese contrastive embedding network for composi-\ntional zero-shot learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 9326–9335, 2022. 2, 6, 7, 8\n[14] Xiangyu Li, Xu Yang, Xi Wang, and Cheng Deng. Agree\nto disagree: Exploring partial semantic consistency against\nvisual deviation for compositional zero-shot learning. IEEE\nTransactions on Cognitive and Developmental Systems, 16\n(4):1433–1444, 2024. 1\n[15] Yong-Lu Li, Yue Xu, Xiaohan Mao, and Cewu Lu. Symme-\ntry and group in attribute-object compositions. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, page 11313–11322, 2020. 2, 6, 7\n[16] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 2, 3, 5\n[17] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems, 36, 2024. 3\n[18] Xiaocheng Lu, Song Guo, Ziming Liu, and Jingcai Guo. De-\ncomposed soft prompt guided fusion enhancing for compo-\nsitional zero-shot learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 23560–23569, 2023. 2\n[19] Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin\nXian, and Zeynep Akata. Open world compositional zero-\nshot learning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5222–\n5230, 2021. 5, 6, 7\n[20] Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin\nXian, and Zeynep Akata. Learning graph embeddings for\nopen world compositional zero-shot learning. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 46(3):\n1545–1560, 2022. 2, 6, 7\n[21] Tomas Mikolov. Efficient estimation of word representations\nin vector space. arXiv preprint arXiv:1301.3781, 2013. 2, 8\n[22] Ishan Misra, Abhinav Gupta, and Martial Hebert. From red\nwine to red tomato: Composition with context. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 1160–1169, 2017. 1\n[23] Niklas Muennighoff.\nSgpt: Gpt sentence embeddings for\nsemantic search. arXiv preprint arXiv:2202.08904, 2022. 2,\n3\n9\n\n[24] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang,\nFuru Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Gen-\nerative representational instruction tuning.\narXiv preprint\narXiv:2402.09906, 2024. 2, 3, 5, 12\n[25] Muhammad\nFerjad\nNaeem,\nYongqin\nXian,\nFederico\nTombari, and Zeynep Akata. Learning graph embeddings\nfor compositional zero-shot learning.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 953–962, 2021. 6, 12\n[26] Tushar Nagarajan and Kristen Grauman. Attributes as oper-\nators: Factorizing unseen attribute-object compositions. In\nProceedings of the European Conference on Computer Vi-\nsion, pages 169–185, 2018. 1, 4\n[27] Zhixiong Nan, Yang Liu, Nanning Zheng, and Song-Chun\nZhu. Recognizing unseen attribute-object pair with genera-\ntive model. In Proceedings of the AAAI Conference on Arti-\nficial Intelligence, pages 8811–8818, 2019. 1\n[28] Nihal V. Nayak, Peilin Yu, and Stephen H. Bach. Learning\nto compose soft prompts for compositional zero-shot learn-\ning. In International Conference on Learning Representa-\ntions, 2023. 2, 7\n[29] OpenAI. Gpt-3.5-turbo api, 2023. 2\n[30] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul F Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions\nwith human feedback. In Advances in Neural Information\nProcessing Systems, pages 27730–27744, 2022. 3\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An\nimperative style, high-performance deep learning library. In\nAdvances in Neural Information Processing Systems, 2019.\n13\n[32] Jeffrey Pennington, Richard Socher, and Christopher Man-\nning. GloVe: Global vectors for word representation. In Pro-\nceedings of the Conference on Empirical Methods in Natural\nLanguage Processing, pages 1532–1543, 2014. 1, 2\n[33] Senthil Purushwalkam, Maximilian Nickel, Abhinav Gupta,\nand Marc’Aurelio Ranzato. Task-driven modular networks\nfor zero-shot compositional learning. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 3592–3601, 2019. 1, 3, 6\n[34] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners, 2019. 2\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In Proceedings\nof the International Conference on Machine Learning, pages\n8748–8763, 2021. 2\n[36] Frank Ruis, Gertjan Burghouts, and Doina Bucur.\nInde-\npendent prototype propagation for zero-shot compositional-\nity. In Advances in Neural Information Processing Systems,\npages 10641–10653, 2021. 1\n[37] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija\nJain, Samrat Mondal, and Aman Chadha. A systematic sur-\nvey of prompt engineering in large language models: Tech-\nniques and applications. arXiv preprint arXiv:2402.07927,\n2024. 11\n[38] Nirat Saini, Khoi Pham, and Abhinav Shrivastava. Disentan-\ngling visual embeddings for attributes and objects. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 13658–13667, 2022. 1, 2, 6,\n7, 12\n[39] Justyna Sarzynska-Wawer, Aleksander Wawer, Aleksan-\ndra Pawlak, Julia Szymanowska, Izabela Stefaniak, Michal\nJarkiewicz, and Lukasz Okruszek. Detecting formal thought\ndisorder by deep contextualized word representations. Psy-\nchiatry Research, 304:114135, 2021. 2\n[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 3\n[41] Bin Wang and C.-C. Jay Kuo.\nSbert-wk:\nA sentence\nembedding method by dissecting bert-based word models.\nIEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 28:2146–2157, 2020. 2\n[42] Qingsheng Wang, Lingqiao Liu, Chenchen Jing, Hao Chen,\nGuoqiang Liang, Peng Wang, and Chunhua Shen. Learning\nconditional attributes for compositional zero-shot learning.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11197–11206, 2023.\n2, 6, 7, 8\n[43] Shuo Xu, Sai Wang, Xinyue Hu, Yutian Lin, Bo Du, and\nYu Wu. Mac: A benchmark for multiple attributes composi-\ntional zero-shot learning. arXiv preprint arXiv:2406.12757,\n2024. 2\n[44] Tian Zhang, Kongming Liang, Ruoyi Du, Xian Sun, Zhanyu\nMa, and Jun Guo. Learning invariant visual representations\nfor compositional zero-shot learning. In Proceedings of the\nEuropean Conference on Computer Vision, pages 339–355,\n2022. 1, 6, 7\n[45] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez,\nand Ion Stoica. Judging llm-as-a-judge with mt-bench and\nchatbot arena. In Advances in Neural Information Process-\ning Systems, pages 46595–46623, 2023. 3, 6\n[46] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Conditional prompt learning for vision-language mod-\nels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 16816–16825,\n2022. 2\n10\n\nAppendix\nA. Auxiliary Attributes Generation by LLM\nand Post-process\nAs mentioned before, we leverage GPT-3.5 to generate\nsome auxiliary attributes for attribute smoothing. The auxil-\niary attributes are generated based on the contextual compo-\nsition of the object and its attribute, such as ripe apple\nor sliced apple.\nThe model takes into account the\nunique characteristics that arise from the combination of at-\ntribute and the object’s context. This ensures that the aux-\niliary attributes accurately capture the nuances of the spe-\ncific composition, rather than general object-level attributes.\nSince the generation ability of LLM is affected by many fac-\ntors, in this section, we first explore the impact of different\nprompts on LLM-generated content. Then we study on the\ninfluence of the number of auxiliary attributes t. In addition,\nwe introduce post-processing of generated text to eliminate\npotential noise.\nImpact of prompt input to LLM. Users typically in-\nteract with LLMs through prompts to request answers to\nquestions, generate text, complete tasks, and more. The\nmodel generates text based on the provided prompt, striv-\ning to meet the user’s requirements [37]. Therefore, the\ngood design of prompt is significant for stimulating knowl-\nedge of LLMs, which enables them to better follow our in-\nstructions and generate auxiliary attributes with high qual-\nUser:\nPlease give me five adjectives that are most\nlikely to describe the browned beef well.\nGPT-3.5:\n1. Flavorful 2. Juicy 3. Savory 4. Tender\n5. Rich\nUser:\nNow l give you an object with an adjec-\ntive: browned beef. Please give me five\nmore possible adjectives that can describe\nthe browned beef well.\nGPT-3.5:\n1. Juicy beef 2. Tender beef 3. Flavorful\nbeef 4. Savory beef 5. Succulent beef\nUser:\nPlease give me five common adjectives that\ncan describe a browned beef well.\nGPT-3.5:\n1. Juicy 2. Brown 3. Savory 4. Tender 5.\nSucculent\nUser:\nPlease give me five adjectives that can de-\nscribe the visual feature of a photo of a/an\nbroned beef well.\nGPT-3.5:\n1. Juicy 2. Glistening 3. Crispy 4. Sizzling\n5. Mouthwatering\nTable 4.\nExample prompts with the composition browned\nbeef input to GPT-3.5 and the generated text.\nUser:\nPlease give me five adjectives that are most\nlikely to describe the inflated pool well.\nGPT-3.5:\n1. Spacious 2. Durable 3. Sturdy 4. Bright\n5. Relaxing\nUser:\nNow l give you an object with an adjective:\ninflated pool.\nPlease give me five more\npossible adjectives that can describe the in-\nflated pool well.\nGPT-3.5:\n1. Refreshing inflated pool 2. Relaxing in-\nflated pool 3. Blue inflated pool 4. Large\ninflated pool 5. Fun inflated pool\nUser:\nPlease give me five common adjectives that\ncan describe a inflated pool well.\nGPT-3.5:\n1. Large 2. Round 3. Inviting 4. Fun 5.\nSturdy\nUser:\nPlease give me five adjectives that can de-\nscribe the visual feature of a photo of a/an\ninflated pool well.\nGPT-3.5:\n1. Colorful 2. Refreshing 3. Sparkling 4.\nBright 5. Relaxing\nTable 5.\nExample prompts with the composition inflated\npool input to GPT-3.5 and the generated text.\nity. We first design some prompts with different style, then\ninput them into GPT-3.5 and observe the quality of gener-\nated attributes. Some prompt examples on the composition\nbrowned beef and ancient building are shown\nin Table 4 and Table 5, respectively.\nAs shown in Table 4, the prompt without ”the visual fea-\nture of ...” may cause the model to produce adjectives that\nare not specific but generic, such Savory and Rich. In\nboth Table 4 and Table 5, the prompts starting with ”Now\nI give you...”, compared to those starting with ”Please give\nme ...”, result in a weaker instruction following ability of\nthe model. Therefore, we choose the prompt: ”Please give\nme five adjectives that can describe the visual feature of a\nphoto of a/an ... well.”\nImpact of the number of auxiliary attributes t. In Ta-\nble 4, we observe that the generated attributes describe the\ncompositions to varying degrees, with later items in the se-\nquence being less relevant generally. Therefore, we study\non the influence of the number of auxiliary attributes t.\nTable 7 and Table 8 show the generated text using dif-\nferent t of compositions large garden and young\ngirl. The results demonstrate that the greater the num-\nber, the more generic adjectives with irrelevant information\nare included, for example, Captivating is generated for\nboth compositions. In addition, with t increasing, the noise\nin the generated text due to the uncertainty of the model\n11\n\nComposition\nTrain\nValidation\nTest\nDataset\n|A|\n|O|\n|A × O|\n|Cs|\n|X|\n|Cs|\n|Cu|\n|X|\n|Cs|\n|Cu|\n|X|\nMIT-States [11]\n115\n245\n28175\n1262\n30338\n300\n300\n10420\n400\n400\n12995\nC-GQA [25]\n413\n674\n278362\n5592\n26920\n1252\n1040\n7280\n888\n923\n5098\nVAW-CZSL [38]\n440\n541\n238040\n11175\n72203\n2121\n2322\n9524\n2449\n2470\n10856\nTable 6. Summary statistics of the datasets used in our experiments.\nt\nthe generated text for the composition\nlarge garden\n3\n1. Lush 2. Vibrant 3. Flourishing\n5\n1. Lush 2. Expansive 3. Vibrant 4. Serene\n5. Verdant\n10\n1. Lush 2. Vibrant 3. Expansive 4. Serene\n5. Colorful 6. Beautiful 7. Bountiful 8.\nCaptivating 9. Peaceful 10. Tranquil\nTable 7. Impact of t on the generated text with the composition\nlarge garden. Note that the input prompt provided to GPT-\n3.5 is the previously selected one, replacing t and the composition.\nt\nthe generated text for the composition\nyoung girl\n3\n1. Innocent 2. Radiant 3. Youthful\n5\n1. Youthful 2. Innocent 3. Vibrant 4. Ra-\ndiant 5. Captivating\n10\n1. Radiant 2. Innocent 3. Vibrant 4. Capti-\nvating 5. Playful 6. Ethereal 7. Alluring 8.\nCharming 9. Enchanting 10. Happpy\nTable 8. Impact of t on the generated text with the composition\nyoung girl. Note that the input prompt provided to GPT-3.5 is\nthe previously selected one, replacing t and the composition.\nabout the given image grows. The young girl may not\nbe happy, yet the model fails to find ten words to describe\nher, so it has to guses. Therefore, we set t to 3, this mini-\nmizes the general adjectives and noise while retaining use-\nful information.\nPost-processing of generated text. GPT-3.5 generates\na segment of text, which we need to process into multiple\nuseful words by exploiting regular expressions. However,\nthe auxiliary attributes generated by LLM may contain the\nattribute of the input composition, for example, generating\nancient for ancient building. At this point, we\nreuse the model to generate t + 1 adjectives for this compo-\nsition and select three adjectives that are not the attribute of\nthe input composition.\nB. Obtainment of The Last Hidden States of\nMLLM\nWe input the attribute (object) word into LLaVA v1.5,\nwhich first tokenizes the word into z tokens.\nThese to-\nkens pass through all attention blocks in the MLLM, ul-\ntimately generating z embeddings of dimension dm after\nthe last block, named the last hidden states. Subsequently,\nwe apply average pooling to these z embeddings of dimen-\nsion dm to obtain a dm-dimensional embedding that repre-\nsents the attribute. Since the last hidden states are designed\nto generate the next token rather than for representation,\nMuennighoff et al. [24] leverages instruction to fine-tune the\nmodel. Therefore, we fine-tune the last hidden states with a\nlow learning rate during the training phase of TRIDENT.\nIt is important to note that although LLaVA v1.5 may\nhave seen certain images during training, the model is asked\nto generate textual descriptions of images in an autoregres-\nsive manner during training. The textual descriptions focus\non the main content of the image, rather than the ”attribute-\nobject” label. Therefore, there is no issue of data leakage\nwhen training TRIDENT.\nC. Data Statictics\nWe evaluate our model on three challenging CZSL bench-\nmark datasets: MIT-states [11], C-GQA [25], and VAW-\nCZSL [38]. MIT-states consists of diverse real-world im-\nages labeled by early image search engine technology. C-\nGQA and VAW-CZSL are two more challenging benchmark\ndatasets that consist of broad collections of in-the-wild im-\nages. C-GQA has more one-to-one compositions, while ob-\njects in VAW-CZSL share more attributes. Table 6 shows\ndetailed data statistics following the common data splits of\nMIT-States [11], C-GQA [25] and VAW-CZSL [38]. MIT-\nStates contains 53753 images, with 115 attributes and 245\nobjects. It comprises 1262 seen compositions and 300/400\n(validation/test) unseen compositions. C-GQA is a natu-\nral image dataset which contains 39298 images, with 413\nattributes and 764 objects. It includes 5,592 seen composi-\ntions and 1,040/923 (validation/test) unseen compositions.\nVAW-CZSL is a larger dataset which contains 440 attributes\nand 541 objects for 238040 images, and it is split into 11175\nseen and 2322/2470 unseen compositions for training and\nvalidation/testing, respectively.\n12\n\n29.0\n29.5\n30.0\n30.5\n31.0\nHM\nHM\n2\n4\n6\n8\nNumber of visual features X (Q=X, P=X/2)\n13.0\n13.5\n14.0\n14.5\n15.0\nAUC\nAUC\n(a) MIT-States\n21.0\n21.5\n22.0\n22.5\n23.0\nHM\nHM\n2\n4\n6\n8\nNumber of visual features X (Q=X, P=X/2)\n6.5\n7.0\n7.5\n8.0\nAUC\nAUC\n(b) C-GQA\nFigure 4. Impact of the number of the visual features on (a) MIT-States and (b) C-GQA.\n30.0\n30.2\n30.4\n30.6\n30.8\n31.0\nHM\nHM\n0.03\n0.06\n0.09\n0.12\n0.15\n13.4\n13.6\n13.8\n14.0\n14.2\n14.4\nAUC\nAUC\n(a) MIT-States\n22.0\n22.2\n22.4\n22.6\n22.8\nHM\nHM\n0.03\n0.06\n0.09\n0.12\n0.15\nLabel smoothing factor\n7.4\n7.6\n7.8\n8.0\n8.2\n8.4\nAUC\nAUC\n(b) C-GQA\nFigure 5. Impact of the label smoothing factor on (a) MIT-States and (b) C-GQA.\nD. Implementation details\nWe use NVIDIA PTX 3090 GPU to train all models un-\nder the Pytorch framework [31]. Since Lcomp leverages\nimage features during training, we use a Batch Normal-\nization, ReLU and 0.3 dropout for Image embedder. We\ntrain TRIDENT by Adam optimizer with weight decay 5e-\n5, learning rates 1.5e-6 for word embedding as well as 2e-4\nfor other modules on three datasets. We decay the learning\nrate by 10 at epoch 30 and 40. The temperature variable\nof cosine similarity δ is set to 0.05. For weighting coeffi-\ncients γortho, γcomp, γattr, andγobj, we set them to 0.1, 1,\n0.5, and 0.5, respectively.\nE. Impact of Hyperparameters\nTo provide more insight into the effect of visual features and\nlabel smoothing, we study on the performance of TRIDENT\nwith respect to different numbers of visual features and dif-\nferent label smoothing factors, respectively. Experiments\nexploring the impact of hyperparameters are conducted on\ndatasets MIT-States and C-GQA.\nImpact of the number of visual features. We illustrate\nthe performance of TRIDENT influenced by different num-\nbers of attribute features in Figure . In Figure 4a, the perfor-\nmance of our model on MIT-States generally improves with\nthe increasing number of visual features, but subsequently\ndeclines. This trend is reasonable, as a greater number of\nVisual features contains more useful information, thereby\nenhancing the performance. However, the number of useful\nfeatures is limited; thus, an excessive number of visual fea-\ntures may introduce redundancy and noise, ultimately ham-\npering the performance of the model.\nHowever, in Figure 4b, as the number of visual features\nincreases, the performance of the model on C-GQA tends to\n13\n\ndecline overall. This may be attributed to the model’s strong\nexpressive capability in handling composition reasoning. In\nthe low-noise C-GQA dataset, optimal performance can be\nachieved using only two features. Increasing the number of\nfeatures, however, results in heightened model complexity\nwithout tangible benefits, potentially impairing generaliza-\ntion to unseen compositions. In contrast, the MIT-States\ndataset exhibits significant noise; thus, while the increase\nof visual features may introduce more noise, it also neces-\nsitates a greater amount of useful information, which can\neffectively mitigate the impact of the noise.\nImpact of the number of label smoothing factor. The\nlabel smoothing factor α modulates the extent to which the\nmodel’s confidence in seen compositions is attenuated. Fig-\nure 5a shows that as α increases, the model’s performance\non MIT-States initially improves before subsequently de-\nclining. This is because if alpha is too small, label smooth-\ning fails to enhance generalization, while if alpha is too\nlarge, it adversely affects the model’s ability to learn the\nrepresentation of the original labels, resulting in more losses\nthan gains. However, as shown in Figure 5b, the model\nachieves the best performance with C-GQA a smaller α.\nThis may be attributed to the fact that, compared to every-\nday objects, LLMs are less familiar with in-the-wild ob-\njects, leading to relatively lower quality in the generated\nauxiliary attributes; thus, a smaller smoothing factor can\nmitigate the impact.\n14",
    "pdf_filename": "Leveraging_MLLM_Embeddings_and_Attribute_Smoothing_for_Compositional_Zero-Shot_Learning.pdf"
}