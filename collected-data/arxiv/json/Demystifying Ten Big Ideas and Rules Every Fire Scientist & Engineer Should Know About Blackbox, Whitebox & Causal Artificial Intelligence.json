{
    "title": "Demystifying Ten Big Ideas and Rules Every Fire Scientist & Engineer Should Know About Blackbox, Whitebox & Causal Artificial Intelligence",
    "context": "",
    "body": "Pre-print, 11/23/2021 \n1 \n \nDemystifying Ten Big Ideas and Rules Every Fire Scientist & Engineer Should Know \nAbout Blackbox, Whitebox & Causal Artificial Intelligence \nM.Z. Naser, PhD, PE \nSchool of Civil and Environmental Engineering, and Earth Sciences, Clemson University, Clemson, SC 29634, USA \nAI Research Institute for Science and Engineering (AIRISE), Clemson University, Clemson, SC 29634, USA \nE-mail: mznaser@clemson.edu, Website: www.mznaser.com \nSynopsis \nArtificial intelligence (AI) is paving the way towards the fourth industrial revolution with the fire \ndomain (Fire 4.0). As a matter of fact, the next few years will be elemental to how this technology \nwill shape our academia, practice, and entrepreneurship. Despite the growing interest between fire \nresearch groups, AI remains absent of our curriculum, and we continue to lack a methodical \nframework to adopt, apply and create AI solutions suitable for our problems. The above is also \ntrue for parallel engineering domains (i.e., civil/mechanical engineering), and in order to negate \nthe notion of “history repeats itself” (e.g., look at the continued debate with regard to modernizing \nstandardized fire testing, etc.), it is the motivation behind this letter to the Editor1 to demystify \nsome of the big ideas behind AI to jump-start prolific and strategic discussions on the front of AI \n& Fire. In addition, this letter intends to explain some of the most fundamental concepts and clear \ncommon misconceptions specific to the adoption of AI in fire engineering. This short letter is a \ncompanion to the Smart Systems in Fire Engineering special issue sponsored by Fire Technology. \nAn in-depth review of AI algorithms [1] and success stories to the proper implementations of such \nalgorithms can be found in the aforenoted special issue and collection of papers.  \nThis letter comprises two sections. The first section outlines big ideas pertaining to AI, and \nanswers some of the burning questions with regard to the merit of adopting AI in our domain. The \nsecond section presents a set of rules or technical recommendations an AI user may deem helpful \nto practice whenever AI is used as an investigation methodology. The presented set of rules are \ncomplementary to the big ideas.  \nBig ideas.  \nThe listed big ideas herein are tailored to present key concepts behind AI as a domain and method \nof investigation.  \nBig Idea 1: What is AI? \nA good start to this letter is by defining AI (as well as its derivatives, machine learning (ML), and \ndeep learning (DL)). From this view, The Oxford Dictionary defines AI as “the theory and \ndevelopment of computer systems able to perform tasks normally requiring human intelligence, \nsuch as visual perception, speech recognition, decision-making, and translation between \nlanguages” [2]. In fire engineering terms, AI is the development of fundamental principles that \nenable computing machines from carrying out tasks that require engineering intellect (i.e., identify \nfailure modes in fire-exposed beams as shear-dominant or flexure-dominant). The same dictionary \nalso defines ML as “a type of artificial intelligence in which computers use huge amounts of data \nto learn how to do tasks rather than being programmed to do them” [2]. In a way, ML encompasses \nthe development of codes/algorithms that enable machines to learn directly from data (e.g., \nalgorithm X, once applied to a set of data collected from combustion experiments, can help build \na surrogate model capable of identifying flammability limits of materials). Similarly, deep learning \n \n1 This letter is inspired in part by TZ Harmathy’s work “Ten rules of fire endurance rating” [31] (also published at \nFire Technology). \n\nPre-print, 11/23/2021 \n2 \n \n(DL) is a technique that can be applied to enable \nML. DL differs than other algorithms since it learns \nto process data through a series of processing layers \ndesigned to mimic how the brain processes data [3]. \nFigure 1 showcases the realm of AI, ML, and DL. \nThis figure is bound to change in the coming years \nas ML and DL flourish into their own disciplines.  \nBig idea 2: How does AI/ML/DL differ from data \nscience and statistics? \nGenerally \nspeaking, \ndata \nscience \ncomprises \nscientific methods and knowledge domain to extract \nknowledge from data. This data can be clean or \nnoisy, structured or unstructured, real or synthetic. \nStatistics is a sub-area of data science. Statistical \nmethods infer from a sample of a population with a “relatively” small number of parameters. In a \nstatistical analysis, a model often accompanies a pre-determined statistical distribution to fit the \ninput parameters to the result of a phenomenon [4]. Once the number of parameters grows, or the \nrelationship between them turns complicated, or the data does not satisfy assumptions/conditions \ntied with statistical methods, such methods become less effective [5]. Unlike statistical methods, \nAI derivatives make minimal assumptions about data type and directly learn from data in search \nof patterns (thereby becoming useful on unstandardized data or that containing highly nonlinear \ninteractions). A unique distinction between statistical methods and AI derivatives is that these \nderivatives need to satisfy a penalization function to attain convergence, as opposed to a set of \nrules [6].  \nBig idea 3: Why do we need AI/ML/DL in our domain? And why do we need them now? \nThe truth of the matter is that most data in our field, and just like other fields, is highly nonlinear, \nhighly dimensional, contains noise and outliners. Thus, much of the real-world data may not easily \nbe processed through traditional methods, and hence engineers tend to smoothen such data via \nassumptions, etc. Furthermore, classical methods (i.e., human-powered or numerical modeling) \nprimarily require specialized software and computing stations. On the other hand, AI derivatives \ncan be integrated with open-source and free-to-use software. Such derivatives provide orders of \nmagnitude faster solutions and thus can provide us with real-time means to process data and make \ndecisions. One can think of AI derivatives as new methods of investigation that can help \nsupplement those of traditional nature – as supposed to replace them. The addition of such tools \nwill expand our arsenal.  \nBig Idea 4: How does AI differ from testing or numerical simulations? \nWe design tests to examine phenomena [7]. Naturally, tests (or experiments) are limited in size \nand scope to comply with available testing facilities. The primary goal of our experiments is to tie \na cause(s) to an effect(s); or, holistically speaking input(s) to output(s). To maintain control, one \nparameter is often varied at a time to observe its sole influence [8]. In lieu of tests, we also develop \nnumerical models to provide us with affordable means to simulate phenomena [9]. A common \npractice of developing a numerical model is to first validate its predictions against tests [10, 11]. \nEven though we lack a standardized procedure to develop or validate numerical models, modeling \nremains a cornerstone in our domain [12]. \nArtificial \nintelligence \n(AI) \nArtificial \nintelligence \n(AI) \nMachine \nlearning (ML)\nMachine \nlearning (ML)\nDeep \nlearning (DL)\nDeep \nlearning (DL)\nFig. 1 A look into AI, ML and DL \n\nPre-print, 11/23/2021 \n3 \n \nUltimately, an analysis is carried out to reach an understanding of the cause-and-effect governing \nthe phenomenon on hand [13]. This understanding is often expressed via a formula (or design aid). \nA formula comes in handy as it: 1) can be easily applied, 2) expresses the relations between the \ninputs, as well as these parameters and the outcome/response observed, and most importantly, 3) \narticulates the space of influencing factors upon the phenomenon on hand [14, 15].  \nUnlike the above methods, the integration of AI can be broadly summed by a three-step linear \nprocedure: 1) collect data on a phenomenon, 2) apply algorithm X, and 3) declare that algorithm \nX can predict the phenomenon on hand via fitness tests. In many instances, this algorithm is a \nBlackbox – we do not know why it predicts the way it does, nor how it ties the parameters to the \nphenomenon on hand! However, we do know that the data primarily drive such an algorithm. A \nquestion often arises, how can we ensure that this data-driven approach matches that from physics?  \nHerein where a philosophical look into AI is warranted. Herein also where recent advancements \nin explainability and interpretability continue to rise – and should be of interest to us [16–18]. At \nthis point, an AI user can take due diligence to verify their AI’s model predictions (i.e., via a series \nof performance metrics, expert judgments, apply explainability and causal inference measures etc. \n– which is what we already do when we derive equations from tests or validate numerical models), \nor may opt not to use AI to investigate a particular phenomenon (i.e., say one that is complex or \nwith minimal insights/tests, etc.).  \nA decision is to be made, can this problem be solved in a data-driven approach? Do we need to \nsolve it via such an approach? And if so, can we accept the limitations of such an approach? In a \nway, we can think of AI as a tool to help us and, just like other methods of investigation, AI also \nhas its limitations; for now.  \nBig idea 5: Which algorithm(s) to use?  \nAnswering this question is not simple; however, it could perhaps be articulated by the following. \nAI derivatives can be generally classified into three categories; supervised, unsupervised and semi-\nsupervised learning [19]. The first type of learning is potentially applicable to the majority of \nproblems in fire engineering. In supervised learning, both the inputs and output(s) are known, but \nthe relation between inputs and output(s) is not. Thus, the objective of supervised learning is to \nrealize an accurate mapping function capable of predicting the output(s) from the inputs. \nSupervised learning can be further broken down into regression (when the output variable is \nnumeric, i.e., fire resistance time) and detection/classification (when the output variable is a \ncategory, i.e., fuel-controlled/ventilation-controlled fires) problems. \nOn the other hand, unsupervised learning can come in handy when only the inputs are known \nwithout any corresponding output(s). In this type of learning, an algorithm is tasked with \ndiscovering the underlying nature of the phenomenon. Unsupervised learning can be grouped into \nclustering (grouping by behavior, i.e., concrete of dense microstructure is vulnerable to spalling) \nand association (discover rules that describe large portions of the observations, i.e., columns made \nof dense concrete have high capacity at ambient conditions and could spall under fire). On the \nother hand, semi-supervised learning is suited where a large amount of inputs and only some of \nthe output data is available (i.e., can be applied in automatic tagging of images where the user tags \nsome images, say in unlabeled images where flying embers are to be detected).  \n\nPre-print, 11/23/2021 \n4 \n \nGoing back to our question, once you identify a problem, then you can very likely to land a learning \nprocess to examine such a problem. From there, arriving at a series of suitable algorithms turns \ninto an easy exercise (see [1, 20]).     \nBig idea 6: Do we have enough data to apply AI/ML/DL? \nWhile it is true that data in any domain can be limited in size, quality, or representation [21], \nhowever, the above overlooks the fact that while we do have data from years of fire research and \npractice, the data we have access to and can use to train AI/ML/DL models is limited. This creates \nan opportunity to share our data; noting how sharing data and AI codes/models can be carried out \nvia online repositories such as Mendeley. We need to work together and harmony to build \ncentralized or decentralized databases that are publicly shareable and free for use. A collective \neffort of stakeholders in our sector can prove effective in facilitating the integration of AI \nderivatives. Ensuring accessibility and inclusion2 will allow us to further AI-based solutions to our \nproblems.  \nBig Idea 7: How many data points do we normally need to run an AI analysis? \nA historical rule of thumb implies that a minimum dataset can be ten observations per predictor \n[22]. This has then been appended to 23 observations per predictor [23]. As of this point in time, \ninvestigations continue to be carried out to better answer this question. Both data quality, quantity, \nand range are key considerations for a proper dataset. More data may not always correspond to \nbetter models; more “good” data will likely get us there efficiently. \nBig Idea 8: What features to include in a model? \nFeatures refer to the inputs used in an AI model. Such features could be the width of an element, \nmaterial type, chemical composition, etc. At a minimum, a fire scientist or engineer should re-visit \nthe open literature to identify the prime features (parameters) governing a phenomenon. For \nexample, one may examine established publications/equations to identify such features. If such \nfeatures are unknown, then experts can be consulted, or the user may opt to adopt a data-driven \napproach and examine the importance of each available feature via weightage of algorithmic \nanalysis. A user may also create fictitious features via feature handling and feature engineering \ntools. Careful should be taken when treading the latter.  \nBig idea 9: Do fire scientists/engineers need to learn to code?  \nThanks to our colleagues in computer sciences, a multitude of algorithms are not only available \nfor immediate integration but these algorithms have been extensively examined and validated. \nSuch algorithms are also available for free use as they were developed via an open-source \n“community effort” wherein interested public members worked in harmony to improve the state \nof such algorithms. Perhaps we can follow a similar trend and develop fire-based algorithms that \nbest suit our domain in the near future. Hence, efforts to modernize our traditional fire engineering \ncurriculum will prove meritorious.  \nBig idea 10: A properly validated AI model is not always a casual AI model.  \nSay a model achieves impressive scores during training/validation and testing. Say the same model \nalso performs well on established databases [24]. Now also say that when diagnostic, the model \n \n2 This is an important point – especially since many schools with fire engineering courses may not have experimental \nfacilities. Sharing our data and codes will allow less representative schools to be active members of our domain – \nsimply since AI investigation can be carried out by freely available codes (yet, our data may not be freely available) \n– see Big Idea 9.  \n\nPre-print, 11/23/2021 \n5 \n \nshows similar behavior to that expected from physical observations. Yet, this model may still not \nbe one that can articulate the cause-effect relations between the inputs and the output(s). A causal \nmodel is one that is created following causality principles. A user is to decide what they seek – a \ndata-driven model or a causal model.  \nFigure 2 demonstrates a typical application of an AI-based model where a database is developed \nfrom the space of known features. In many instances, a database does not contain all known \nfeatures nor include those from the space of possible features due to any of the aforenoted \nlimitations. Thus, an AI-based model developed on such features is likely to be a data-driven \nmodel. Simply, a model capable of predicting the \nphenomenon on hand by comparing how the collected \nfeatures relate to the response (outcome of the \nphenomenon). Depending on the used features, such \nmodels can have a variety of degrees of usefulness. \nHowever, such models may also miss the true essence of \nthe problem. This is often mistaken for the cause-&-\neffect. Supplementing the above model with sensitivity, \nexplainability, interpretability methods is a good practice \n– since these methods provide us with a look into how a \nmodel behaves (i.e., effectively converting a blackbox to \na whitebox/glass model). However, this should not be \nmistaken by how the inputs cause the phenomenon. \nOn the other hand, a causal model can identify the true cause-effect relationships and hence is \ncapable of truly capturing the essence of a phenomenon. Such a model goes beyond the space of \nknown features into the space of possible features to articulate how the features interact with each \nother to yield the phenomenon on hand. The same model can be used to understand a phenomenon \nand explore how to tackle it. The same model also allows us to discover new knowledge3. \nBased on the above Big Ideas, a few Rules can be inferred. These rules (or simply suggestions) are \narticulated to allow maximizing the outcome of an AI investigation. These rules revolve around \nthe creation of AI-based models. The specifics to developing AI-based models (in terms of data \ncollection, handling, selection, etc.) are also important, and more on these can be found in \ntraditional textbooks [32, 33] as well as publications [27]. For brevity, these aspects were not \ntouched upon herein, given their generalist nature of being applicable to most, if not all, AI models. \nThese rules were collected based on the recommendation of published works [25–27], observations \nof current works on the front of fire, and personal experiences.  \nRule 1: Start with simple AI models. \nThere is genius in simplicity. Simple AI models are versatile, easy to use, tune and explain, and of \nlow computation needs. As such, it is advisable to start with a simple model vs. complex \n(advanced) models or ensembles. There is a time and place for complex models to be nurtured or \nfor AI algorithms to be blended into ensembles.  \n \n3 For fairness, new knowledge could also be discovered via data-driven models (but not to the same extent as that in \ncausalmodels).  \nPhenomenon \nSpace of known features  \nSpace of possible features  \nFig. 2 Data-driven vs. causal models \n\nPre-print, 11/23/2021 \n6 \n \nRule 2: Explore a series of algorithms, architectures, training/validation methods, and \nperformance metrics.   \nAs noted earlier, there exist many types of learnings and associated algorithms. Thus, it is of merit \nto explore how various algorithms will fair against a phenomenon (or more explicitly, will fair \nagainst the data on hand) before a model is finalized. In a way, no one algorithm is best suited for \nall problems. The same can be extended to algorithmic architecture and performance metrics. \nCorrelation does not imply causation. Avoid over tuning models, and watch out for the fine line \nof tuning for model development vs. model deployment.  \nRule 3: Favor experimental data over synthetic or augmented data alone.   \nReal data (i.e., obtained from experiments) may contain randomness effects, which can be \nattributed to outliers/noise – while in fact, remain part of the physical realm4 (e.g., sub-phenomena) \nthat we/our equipment cannot fully measure or explain. Yet, such measurements can be valuable \n(unless proven otherwise). Unlike real data, data obtained by synthetic means (drawn from space \nof observations) or augmented (i.e., via numerical models) may not capture the aforenoted effects \nsimply due to the idealization associated with such data. For example, running a numerical model \nwith a fixed material model to describe property degradation under fire conditions will inherently \ncause model predictions (both numerical and AI) to be skewed towards that particular material \nmodel [28]. In a way, relying on such data may effectively limit the generalizability of the AI \nmodel. The above may, or may not, be of concern (please re-visit Big Idea 10). \nPerhaps modelers can use a variety of material models (to describe property degradation or mix \nreal with synthetic/augmented data, which may minimize the noted skewness [29]. However, one \nmust remember that replicating randomness effects may not be as easily obtained – especially \nwhen high nonlinearity or instability exists. Thus, it is of equal merit to validate and test your \nmodel against observations taken from various groups (i.e., testing facilities, equipment, etc.), and \nacross different time spans.  \nRule 4: Explore the inclusion of physics principles into AI models.   \nInfusing prior knowledge of physical principles governing our systems is advantageous as such \nprinciples regularize the space of solutions and improve the robustness of the transformation \nfunctions (often hidden within algorithmic topologies). In addition, the infusing of such principles \nis a step forward over purely relying on data alone to guide the prediction of an AI model \n(especially when data is limited, expensive to compile, or incomplete). \nRule 5: Steer away from Blackbox models (unless the goal is to create a Blackbox model).   \nIn traditional methods of investigations, we create solutions that help us “understand” the \nphenomenon on hand. The same is also true whenever AI is used. While Blackbox models have \ntheir place, we do need transparent/white/glass AI models that allow us to see and justify model \npredictions – and most importantly discover new knowledge. A prime goal is to realize causal \nmodels that can articulate the “cause” and “effect” governing a phenomenon from physics, \nempirical, and data perspectives [13, 30]. These models will allow a smoother transition toward \ntrusting and embracing AI by our community.  \nIn a way, an equation, expression, or a numerical model can also be thought of as a blackbox. For \nexample, inputting erroneous values into an equation, expression, or a numerical model is expected \n \n4 One can argue that real data from tests may not be representative with real work observations (given the limitation \nof testing set-ups). This is a valid argument. The second paragraph of Rule 3 may counter such argument.  \n\nPre-print, 11/23/2021 \n7 \n \nto yield wrong predictions. Yet, because we can trace how did we arrive at an answer (say, by \nsolving the equation/expression/ matrices/partial differential equations), then we are much more \ncomfortable with accepting such model predictions. The same may not as easily as one can imagine \nwith AI models. \nRule 6: Be cognizant in navigating the trade-offs.   \nSimilar to other investigation methods, a user may \ncontemplate a few trade-offs5 while developing an AI model. \nSome of these trade-offs include those at the fronts of: bias-\nvariance, accuracy-generalizability, accuracy-complexity, \naccuracy-computational resources, complexity-explainability, \npredictivity-model size, to name a few (see Fig. 3). While for \nsome, if not most, the adversity of such trade-offs can be \nminute to our problems, in others, say turbulence analysis, \nthese trade-offs may hinder the development of a model. At \nthe end of the day, achieving a balance between the trade-offs \nis recommended.  \nRule 7: Supplement your model with confidence measures.   \nOnce an AI model completes its training/validation/testing \nprocess, such a model is often deployed to the real world. \nCertainly, real-world data can be different from that used to \ncreate this model (in terms of range, quality, etc.). A prime \nexample would be data from wildfires which can be sensitive \nto geo-spatial or seasonal (or both) effects. Hence, it is of importance that your AI model is able \nto flag predictions that the model struggles to predict or those likely to be faulty.  \nRule 8: Share your data and code (whenever possible) and full details of your model (at all times). \nThis rule builds on our discussion on Big Ideas no. 5 and 6. The same will also facilitate \nreproducibility and help improve our education on the front of AI & Fire. Aspire to create open-\nsource models.  \nRule 9: Transform your AI model into a graphical interface.   \nTo maximize the outreach of your AI model, such a model can be converted from lines of codes \ninto an App or software with an easy-to-use graphical interface. This may attract users to adopt \nyour model/software – especially that many of our colleagues may not be well versed with AI. At \nthe same time, such software can be copy-righted and marketed for subscriptions. This may indeed \nbring a new revenue stream to our industry. Finally, such software can also be used in an \neducational setting to introduce students to AI. \nRule 10: Keep an eye on your AI model beyond deployment.   \nSeek feedback and monitor the performance of the deployed models. Aim to improve and update \nyour model on a regular basis. Use feedback and newly acquired data to enhance your model. \nThe above Big Ideas, and Rules are expected to evolve over the next few years rapidly. I hope my \ncolleagues invest in extending and improving these ideas and rules.  \n \n5 One can think of a numerical model where dense mesh is used vs. low quality mesh. A trade-off on the front of \naccuracy-commutation resources is expected to exist.  \nComplexity \nError \nVariance \nBias \nOptimal point \nExplainability \nAccuracy  \nFig. 3 Examples of AI trade-offs \nDeep learning \nTree models \nLinear models \n\nPre-print, 11/23/2021 \n8 \n \nAcknowledgment  \nI would like to thank the Editor, Prof. Guillermo Rein, and Anonymous reviewer for their support \nof this work and constructive comments that enhanced the quality of this letter. \nData Availability \nSome or all data, models, or code that support the findings of this study are available from the \ncorresponding author upon reasonable request.  \nConflict of Interest  \nThe author declares no conflict of interest.  \nReferences \n1.  \nNaser MZ (2021) Mechanistically Informed Machine Learning and Artificial Intelligence \nin Fire Engineering and Sciences. Fire Technol 1–44. https://doi.org/10.1007/s10694-020-\n01069-8 \n2.  \nOxford English Dictionary (2017) Oxford English Dictionary Online. In: Oxford English \nDict. \n3.  \nLecun Y, Bengio Y, Hinton G (2015) Deep learning. Nature \n4.  \nChristodoulou E, Ma J, Collins GS, et al (2019) A systematic review shows no \nperformance benefit of machine learning over logistic regression for clinical prediction \nmodels. J. Clin. Epidemiol. \n5.  \nBzdok D, Altman N, Krzywinski M (2018) Statistics versus machine learning. Nat \nMethods. https://doi.org/10.1038/nmeth.4642 \n6.  \nIvezic Ž, Connolly AJ, VanderPlas JT, Gray A (2014) Statistics, Data Mining, and \nMachine Learning in Astronomy \n7.  \nChristensen BT, Schunn CD (2007) The relationship of analogical distance to analogical \nfunction and preinventive structure: The case of engineering design. Mem Cogn. \nhttps://doi.org/10.3758/BF03195939 \n8.  \nBiot MA (1943) Analytical And Experimental Methods in Engineering Seismology. Trans \nAm Soc Civ Eng. https://doi.org/10.1061/taceat.0005571 \n9.  \nZienkiewicz OC, Taylor RL (2000) The Finite Element Method Volume 1 : The Basis. \nMethods \n10.  \nTekkaya AE (2005) A guide for validation of FE-simulations in bulk metal forming. Arab \nJ Sci Eng \n11.  \nFerreira J, Gernay T, Franssen J, Vassant O (2018) Discussion on a systematic approach \nto validation of software for structures in fire - Romeiro Ferreira Joao Daniel. In: SiF \n2018: 10th international conference on structures in fire. Belfast \n12.  \nHawileh RAA, Naser MZZ (2012) Thermal-stress analysis of RC beams reinforced with \nGFRP bars. Compos Part B Eng 43:2135–2142. \nhttps://doi.org/10.1016/j.compositesb.2012.03.004 \n\nPre-print, 11/23/2021 \n9 \n \n13.  \nPearl J MD (2018) The Book of Why_ The New Science of Cause and Effect-Basic Books \n14.  \nEllingwood, B., Galambos, T. V., McGregor, J. G. & Cornell CA (1980) Development of \na Probability Based Load Criterion for American National Standard A58. U.S. Dep. \nCommer. Natl. Bur. Stand. \n15.  \nASCE (2016) Minimum Design Loads for Buildings and Other Structures (ASCE/SEI 7-\n16) \n16.  \nRudin C (2019) Stop explaining black box machine learning models for high stakes \ndecisions and use interpretable models instead. Nat. Mach. Intell. \n17.  \nMiller T (2019) Explanation in artificial intelligence: Insights from the social sciences. \nArtif. Intell. \n18.  \nNaser MZ (2021) An engineer’s guide to eXplainable Artificial Intelligence and \nInterpretable Machine Learning: Navigating causality, forced goodness, and the false \nperception of inference. Autom Constr 129:103821. \nhttps://doi.org/10.1016/J.AUTCON.2021.103821 \n19.  \nBishop C (2006) Pattern recognition and machine learning. Springer \n20.  \nLattimer BY, Hodges JL, Lattimer AM (2020) Using machine learning in physics-based \nsimulation of fire. Fire Saf J 102991. https://doi.org/10.1016/j.firesaf.2020.102991 \n21.  \nZhu X, Vondrick C, Fowlkes CC, Ramanan D (2016) Do We Need More Training Data? \nInt J Comput Vis. https://doi.org/10.1007/s11263-015-0812-2 \n22.  \nSmeden M van, Moons KG, Groot JA de, et al (2018) Sample size for binary logistic \nprediction models: Beyond events per variable criteria: \nhttps://doi.org/101177/0962280218784726 28:2455–2474. \nhttps://doi.org/10.1177/0962280218784726 \n23.  \nRiley RD, Snell KIE, Ensor J, et al (2019) Minimum sample size for developing a \nmultivariable prediction model: PART II - binary and time-to-event outcomes. Stat Med. \nhttps://doi.org/10.1002/sim.7992 \n24.  \nNaser MZ, Kodur V, Thai H-T, et al (2021) StructuresNet and FireNet: Benchmarking \ndatabases and machine learning algorithms in structural and fire engineering domains. J \nBuild Eng 102977. https://doi.org/10.1016/J.JOBE.2021.102977 \n25.  \nWujek B, Hall P, Güneș F (2016) Best Practices for Machine Learning Applications. SAS \nInst Inc \n26.  \nArtrith N, Butler KT, Coudert FX, et al (2021) Best practices in machine learning for \nchemistry. Nat. Chem. \n27.  \nWang AYT, Murdock RJ, Kauwe SK, et al (2020) Machine Learning for Materials \nScientists: An Introductory Guide toward Best Practices. Chem Mater. \nhttps://doi.org/10.1021/acs.chemmater.0c01907 \n28.  \nNaser MZ (2018) Deriving temperature-dependent material models for structural steel \nthrough artificial intelligence. Constr Build Mater 191:56–68. \n\nPre-print, 11/23/2021 \n10 \n \nhttps://doi.org/10.1016/J.CONBUILDMAT.2018.09.186 \n29.  \nNaser MZ, Kodur VK (2021) Explainable Machine Learning using Real, Synthetic and \nAugmented Fire Tests to Predict Fire Resistance and Spalling of RC Columns \n30.  \nNaser MZ (2021) Mapping functions: A physics-guided, data-driven and algorithm-\nagnostic machine learning approach to discover causal and descriptive expressions of \nengineering phenomena. Measurement 185:110098. \nhttps://doi.org/10.1016/J.MEASUREMENT.2021.110098 \n31.  \nHarmathy TZ (1965) Ten rules of fire endurance rating. Fire Technol. \nhttps://doi.org/10.1007/BF02588479 \n32.  \nBarber D (2012) Bayesian reasoning and machine learning \n33.  \nMurphy KP (2012) Machine learning: a probabilistic perspective (adaptive computation \nand machine learning series)",
    "pdf_filename": "Demystifying Ten Big Ideas and Rules Every Fire Scientist & Engineer Should Know About Blackbox, Whitebox & Causal Artificial Intelligence.pdf"
}