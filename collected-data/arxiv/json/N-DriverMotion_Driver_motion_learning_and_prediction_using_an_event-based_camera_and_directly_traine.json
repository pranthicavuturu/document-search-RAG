{
    "title": "N-DRIVERMOTION: DRIVER MOTION LEARNING AND",
    "abstract": "Drivermotionrecognitionisakeyfactorinensuringthesafetyofdrivingsystems.Thispaperpresents a novel system for learning and predicting driver motions, along with an event-based (720x720) dataset,N-DriverMotion,newlycollectedtotrainaneuromorphicvisionsystem. Thesystemincludes anevent-basedcamerathatgeneratesadrivermotiondatasetrepresentingspikeinputsandefficient spikingneuralnetworks(SNNs)thatareeffectiveintrainingandpredictingthedriver’sgestures. The eventdatasetconsistsof13drivermotioncategoriesclassifiedbydirection(front,side),illumination (bright,moderate,dark),andparticipant. Anoveloptimizedfour-layerconvolutionalspikingneural network (CSNN) was trained directly without any time-consuming preprocessing. This enables efficient adaptation to energy- and resource-constrained on-device SNNs for real-time inference onhigh-resolutionevent-basedstreams. Comparedtorecentgesturerecognitionsystemsadopting neuralnetworksforvisionprocessing,theproposedneuromorphicvisionsystemachievescompetitive accuracyof94.04%ina13-classclassificationtask,and97.24%inanunexpectedabnormaldriver motionclassificationtaskwiththeCSNNarchitecture. Additionally,whendeployedtoIntelLoihi 2neuromorphicchips,theenergy-delayproduct(EDP)ofthemodelachieved20,721timesmore efficientthanthatofanon-edgeGPU,and541timesmoreefficientthanedge-purposeGPU.Our proposedCSNNandthedatasetcanbeusedtodevelopsaferandmoreefficientdriver-monitoring systemsforautonomousvehiclesoredgedevicesrequiringanefficientneuralnetworkarchitecture. 1 Introduction Withtheadvancementofartificialintelligence(AI),itisbeingappliedacrossvariousindustrialfields,andamongthese, vehicleAIsystemsareemergingasoneofthemostprominentapplications. OnboardAIsinvehiclesareusedtoassist autonomousdrivingandthesafetyofdriversandpedestriansbyintegratingwiththecontrolsystem[1]. Inparticular, theEUandtheUnitedStateshaveintroducedmandatoryrequirementsforvariousdriverassistancesystemstoensure saferoadtrafficbyregulationsongeneralautomotivesafety[2]. Theprimaryobjectiveofthisregulationistoenhance theprotectionofelderlydrivers,vehicleoccupants,pedestrians,andcyclists. Giventhatresearchshowshumanerroris thecauseof95%ofaccidents,implementingthisregulationisprojectedtosavemorethan25,000livesandpreventat 4202 voN 81 ]VC.sc[ 2v97331.8042:viXra",
    "body": "N-DRIVERMOTION: DRIVER MOTION LEARNING AND\nPREDICTION USING AN EVENT-BASED CAMERA AND DIRECTLY\nTRAINED SPIKING NEURAL NETWORKS\nHyoJongChung ByungkonKang\nCollegeofEngineeringandAppliedSciences DepartmentofComputerScience\nStonyBrookUniversity TheStateUniversityofNewYorkKorea\n100NicollsRoad,NewYork,USA 119-2SongdoMunhwa-ro,Yeonsu-gu,Incheon,Korea\nhyojong.chung@stonybrook.edu byungkon.kang@sunykorea.ac.kr\nYoonseokYang\nDepartmentofComputerScience\nTheStateUniversityofNewYorkKorea\n119-2SongdoMunhwa-ro,Yeonsu-gu,Incheon,Korea\nyoonseok.yang@sunykorea.ac.kr\nNovember20,2024\nABSTRACT\nDrivermotionrecognitionisakeyfactorinensuringthesafetyofdrivingsystems.Thispaperpresents\na novel system for learning and predicting driver motions, along with an event-based (720x720)\ndataset,N-DriverMotion,newlycollectedtotrainaneuromorphicvisionsystem. Thesystemincludes\nanevent-basedcamerathatgeneratesadrivermotiondatasetrepresentingspikeinputsandefficient\nspikingneuralnetworks(SNNs)thatareeffectiveintrainingandpredictingthedriver’sgestures. The\neventdatasetconsistsof13drivermotioncategoriesclassifiedbydirection(front,side),illumination\n(bright,moderate,dark),andparticipant. Anoveloptimizedfour-layerconvolutionalspikingneural\nnetwork (CSNN) was trained directly without any time-consuming preprocessing. This enables\nefficient adaptation to energy- and resource-constrained on-device SNNs for real-time inference\nonhigh-resolutionevent-basedstreams. Comparedtorecentgesturerecognitionsystemsadopting\nneuralnetworksforvisionprocessing,theproposedneuromorphicvisionsystemachievescompetitive\naccuracyof94.04%ina13-classclassificationtask,and97.24%inanunexpectedabnormaldriver\nmotionclassificationtaskwiththeCSNNarchitecture. Additionally,whendeployedtoIntelLoihi\n2neuromorphicchips,theenergy-delayproduct(EDP)ofthemodelachieved20,721timesmore\nefficientthanthatofanon-edgeGPU,and541timesmoreefficientthanedge-purposeGPU.Our\nproposedCSNNandthedatasetcanbeusedtodevelopsaferandmoreefficientdriver-monitoring\nsystemsforautonomousvehiclesoredgedevicesrequiringanefficientneuralnetworkarchitecture.\n1 Introduction\nWiththeadvancementofartificialintelligence(AI),itisbeingappliedacrossvariousindustrialfields,andamongthese,\nvehicleAIsystemsareemergingasoneofthemostprominentapplications. OnboardAIsinvehiclesareusedtoassist\nautonomousdrivingandthesafetyofdriversandpedestriansbyintegratingwiththecontrolsystem[1]. Inparticular,\ntheEUandtheUnitedStateshaveintroducedmandatoryrequirementsforvariousdriverassistancesystemstoensure\nsaferoadtrafficbyregulationsongeneralautomotivesafety[2]. Theprimaryobjectiveofthisregulationistoenhance\ntheprotectionofelderlydrivers,vehicleoccupants,pedestrians,andcyclists. Giventhatresearchshowshumanerroris\nthecauseof95%ofaccidents,implementingthisregulationisprojectedtosavemorethan25,000livesandpreventat\n4202\nvoN\n81\n]VC.sc[\n2v97331.8042:viXra\nAPREPRINT-NOVEMBER20,2024\nleast140,000seriousinjuriesby2038. Thishasledtoanurgentneedtoresearchanddevelopdriverassistancesystems\n(ADAS)usingAI.\nConventionalAIvisionsystemsforautonomousvehiclesutilizeAIplatformsfortrainingwithlargedatasets,after\nwhichthelearnedinformationistransferredtoindividualvehicles. Thesesystemsaretypicallyimplementedusing\nGPUorAIaccelerator-basedhardwarefordatatrainingandinference,whichresultinhighpowerconsumption,slow\nresponsetimes,andchallengesinreal-timeprediction[3]. Theexponentialincreaseinneuralnetworkcomputations\nrequired for massive training datasets in autonomous driving has exacerbated these limitations. To address these\nchallenges,anewgenerationoflow-power,high-performance,andhighlyefficientAIvisionsystemsisexpectedto\nbecomecentraltoAIimplementationinautonomousdriving.\nNeuromorphicAIvisionsystems,inparticular,representawaytosupportAItechnologyforautonomousvehiclesthat\nisgainingincreasingsignificance[4]. Eventcameras,alsocalledneuromorphiccameras,andneuromorphicprocessors,\nwhichmimicthefunctionalityofthehumaneyeandbrain,offerultra-lowpowerconsumption,highefficiency,and\nrapidresponsiveness. Consequently,thesetechnologiesareactivelyappliedinvariousapplicationsinautonomous\nvehicles,wheretheyserveascorecomponentsforthesensoryandprocessingneeds[5,6,7].\nInresponsetothesedemands,weproposeN-DriverMotion,anevent-baseddatasetandvisionsystemforneuromorphic\nlearningandpredictingdrivermotionsonanefficientconvolutionalspikingneuralnetwork(CSNN).Thisdrivermotion\nrecognitionresearch,comprisinganeuromorphicframeworkforefficientCSNNconfigurationintermsofenergyand\nlatency,implementsdriversafetyassistancebyincorporatingahigh-resolutionevent-basedcamera. Thecontributions\noftheproposeddrivermotionrecognitionsystemincludethefollowing:\n• We create an event-based dataset, N-DriverMotion (720 * 720 resolution), for large-scale driver motion\nrecognition using an event-based camera: To the best of our knowledge, this is the first study for driver\nmotionrecognitionusingahigh-resolutioneventcameraandspikingneuralnetworks. Theevent-baseddataset\npresents13drivermotioncategoriesclassifiedbydirection(front,side),illumination(bright,moderate,dark),\nandparticipant.\n• Wepresentanovelsimplifiedfour-layerconvolutionalspikingneuralnetwork(CSNN),directlytrainedwith\ntheeventdatasetwithoutanytime-consumingpreprocessing. Thisenablesefficientadaptationtoon-device\nspikingneuralnetworks(SNNs)forreal-timeinferenceoverevent-basedcamerastreams. Furthermore,the\nproposedneuromorphicvisionsystemachievescompetitiveaccuracyof94.04%in13classclassificationtask,\nand97.24%inunexpectedabnormaldrivermotionclassificationtaskwiththeCSNNarchitecture,developing\nsafer and more efficient driver monitoring systems for autonomous vehicles or edge devices requiring a\nlow-powerandefficientneuralnetworkarchitecture.\n• WedesignanefficientCSNNfordrivermotionrecognitionforoneofthewidelyusedpracticalneuromorphic\nframeworks,theIntelLavaneuromorphicframework. WealsoproposeaversionofourCSNNfortheLoihi2\nprocessor[8,9]. Weobservedthattheenergy-delayproduct(EDP)oftheproposedmodelonLoihi2exhibited\n20,721timesmoreefficientthanthatofanon-edgeGPUand541timesmoreefficientthanthatofanedge\npurposeGPU,whichisconsideredtobeoneofthemostimportantaspectsofedge-deviceAI.\nWedesignedanoptimizedconvolutionalspikingneuralnetwork(CSNN)forefficientenergyuseanddeploymentonon-\ndeviceapplicationsandsystemslikeLoihi2. TotraintheproposedCSNN,weusedasurrogategradientdescent-based\nbackpropagationmethod,SLAYER[10],fordirecttrainingwithourN-DriverMotiondataset. Additionally,insteadof\nresizingthehigh-resolutioneventframesdirectly,weaddedapoolinglayertoenableadaptivedeploymentonGPUs\nandtheLoihi2system,dependingonmemoryrequirements.\nTheremainderofthispaperisorganizedasfollows:Section2brieflyreviewsrelatedwork,whileSection3describesthe\nproposeddrivermotionrecognitionsystem. Section4presentstheimplementationresultsandperformanceevaluations\nandSection4.7concludesthispaper.\n2 RELATEDWORK\nEvent-basedcamerashaveadvantagesoverframe-basedcamerasinpropertiessuchashighdynamicrange,hightemporal\nresolution,lowlatency,andlowpowerconsumptionduetotheirsparseeventstreamswhenintensitychanges[11].\nTherefore,theyhavebeenincreasinglyadoptedinmachinelearningvisionsystemsforgesturerecognitionandlearning.\nHand-gesturerecognition[12],gestureandfacialexpressionrecognition[13],DECOLLE[14]fordeepcontinuous\nlocallearning,EDDDforevent-baseddrowsinessdrivingdetection[15],TORE[16]fortime-orderedrecentevents,\nevent-basedasynchronoussparseconvolutionalnetworks[17],andEST[18]forend-to-endlearningofrepresentations\nforasynchronousevent-baseddatahavebeendevelopedondeeplearning-basedneuralnetworks(DNNs)andevent\n2\nAPREPRINT-NOVEMBER20,2024\nSynapse Dendrite Axon\nIN OUT\naxon\n)6\n9 0 ,4 ( g n ip p a M esp a n y S\n)6\n9 0 ,4 ( y ro m eM esp a n y S\n)291\n,8 ( rotalu m u ccA etird n eD p u o r G n o r u e\nNe ta\ntS & n o ita r u g ifn o C)4 2 0 ,1 (\n)4\n2 0 ,1 ( e ta tS A M O S\n)4\n2 0 ,1 ( g n ip p a M n o x A\n)6\n9 0 ,4 ( y ro m e M n o x A axon\nLearning\nn\no iss\ner\np x E\ng n in\nr\n)4\n2\n0 ,1\n( e c\na r T\nts\n:\n:\nU Np ord mat ae\nl\n( FL le oa wrning)\na o\ne P\nL\nFigure1: SimplifiedstructureofLoihiwhichillustrateshowneurocoreisstructuredandoperates.\nrepresentations,exploitingthesparsityandtemporaldispersionofevent-basedgestures. Riccardoetal.[6]traineda\ndeepneuralnetworkonapreprocessedDVShandgesturedatasetforrecognizinghandgesturesonaLoihineuromorphic\nprocessor. ThetrainedDNNwasconvertedintothespikedomainfordeploymentonIntelLoihi[19]forreal-time\ngesturerecognition.\nUnlikeDNN-basedapproaches,whichrequireconvertingevent-baseddatasetsintostaticpatternsforprocessing,recent\nresearchhasshiftedtowarddirecttrainingandlearningonSNNsusingevent-baseddatasets. Thisisbecauseeventsare\ninherentlysuitedforSNNsoperatingincontinuoustime. Moreover,SNNsdirectlyexploitthetemporalinformationof\neventsforenergy-efficientcomputation. SLAYERproposesanerrorbackpropagationmechanismforofflinetrainingof\nSNNs. Itreceiveseventsandleveragesthebackpropagationmethodtotrainsynapticweightsandaxonaldelaysdirectly.\nAmiretal.[20]proposedanend-to-endevent-basedgesturerecognitionsystemusinganevent-basedcameraanda\nTrueNorthprocessorconfiguredwithaconvolutionalneuralnetwork(CNN).Itrecognizedhandgesturesinrealtime\nfromgestureeventstreamscapturedbyaDynamicVisionSensor(DVS).\nEvent-basedvisionsystemshavebeenwidelyusedinvariousapplications,includingvisionsystemsforautonomous\ndriving[21,22]andobjectdetection[23,24]. Theseapplicationsexploitevent-basedcamerastocapturethespatialand\ntemporaleventdataofdrivingandobjects.\nLoihi2isthesecondgenerationofIntelneuromorphicchipdesignedforenergy-efficientAIapplications[9]. Loihichip\nappliesCurrentBasedLeaky-IntegrateandFireneuron(CUBALIF),asitsmainneuronmodel. Theoverallbehaviorof\nCUBALIFmodelisclosetothatofLIF,butismorebiologicallyplausibleasCUBALIFmodelincludesthetemporal\ndynamicsofthepostsynapticinputcurrent[25]. AsingleLoihichipconsistsof128neurocores,whicharegroupsof\nneuronsthatfunctionaccordingtothecompartments. Eachneurocoreisabletocommunicatewithotherneurocores\nthrough the asynchronous Network-on-Chip (NoC) where the messages are packetized for sending and receiving.\nAlthoughasinglechipprovidesonly128neurocores,Loihiiscapableoftransportinginformationwithoutincreasing\nlatencyformessageexchangebetweenmultiplechips. ThesimplifiedmodelofLoihiisshowninFigure1,which\nillustratestheasynchronousneuronmodelmechanism,alongwiththeimplementationofonlinelearningoracontinual\nlearningmechanism. TheLoihichiphasbeenusedwiththeLavasoftwareframeworktosupportthedevelopmentof\nagileneuromorphicsoftwareandapplications[27,26,28,29,30,31].\n3\nAPREPRINT-NOVEMBER20,2024\nWhileevent-basedsystemshaveshownpromiseinpreviousresearchforautonomousdrivingandgesturerecognition,\ntheystillfacechallengesinachievingreal-timeperformancewithlowpowerconsumptioninhighlydynamicenvi-\nronments. Additionally,existingsystemsmaystruggletohandlethediverseandunpredictablenatureofrecognition\nscenarios,particularlywhendealingwithrapidmotionsorchanginglightingconditions,whichcangeneratemassive\neventstreams. OurproposedsystemwithN-DriverMotionaddressesthesechallengesbyleveragingtheenergy-efficient\nCSNNarchitectureonLoihi2. Thisenablesfast,real-timeprocessingoflarge-scaleeventdatawhilemaintaininglow\npowerconsumption. Furthermore,thesystemensuresthateventdatafromeventcamerasisprocessedpromptlywithout\nbottlenecks,whichisimportantforsafety-criticalapplicationslikeautonomousdriving.\n3 PROPOSEDEVENT-BASEDDRIVERMOTIONRECOGNITIONSYSTEM\nInthissection,weintroduceefficientCSNNarchitectureandadrivermotionlearningandpredictionsystememploying\nadirecttrainingmechanismandevent-baseddatastreams. Asevent-baseddatadiffersfromstaticimagesinthatit\nincludestimetorepresentevents,weadoptadirectspikingtrainingmethodand3Dspikeconvolutionoperationto\nbuildanefficientCSNNmodel.\nFor the direct training of spiking neural networks, we exploited a gradient-based training method developed in\nSLAYER[10]. ItisgenerallyknownthatthecharacteristicofdiscretespikeeventshindersthedifferentiationinSNN.\nToresolvesuchproblems,variousattemptshavebeenmadetoproposetheapproximationforthederivativeofthe\nspikefunction[32,33,34]. However,noneofthefollowinghaveconsideredthetemporaldispersionbetweenspikes\ntoresolvetheproblem. Adifferentiableapproximation(i.e.,surrogategradient)ofthespikefunctionisintroduced\nwiththeprobabilityofachangeinthespikingstate. Itisexplainedthatthederivativeofthespikefunctionrepresents\ntheProbabilityDensityFunction(PDF)forthechangeofstateofaspikingneuron. Withanexpectationvalueofthe\nderivativeofthespikefunction,estimatedbackpropagationerror,andgradienttermsforweightanddelayinlayerl,it\nispossibletogetthederivativesofthetotallossE as\n(cid:40)\n∂L(t) ifl=n\ne(l) = ∂a(nl) l (1)\n(cid:0) W(l)(cid:1)T δ(l+1)(t) otherwise\nδ(l)(t)=ρ(l)(t)·(ε ⊙e(l))(t) (2)\nd\n(cid:90) T (cid:16) (cid:17)T\n∇ E = δ(l+1)(t)· a(l)(t) dt (3)\nW(l)\n0\n(cid:90) T\n∇ E =− a˙(l)(t)·e(l)(t)dt (4)\nd(l)\n0\nwhereρ(l)(t)denotestheprobabilitydensityfunctioninlayerlatcertaintimet,∆ξastherandomperturbation,W(l)\nistheweightvectorforlayerl,a(l)beingthespikeresponsesignal,L(t)beingthelossatacertaintimet,d(l)being\ntheaxonaldelay,ε beingthespikeresponsekernel,and⊙beingtheelement-wisecorrelationoperationintime. This\nd\nprovideseffectivedistributionoftheerrorsbackthroughlayersofaneuralnetworkasinDNNs. Ittakesaccountofthe\nerrorsintheprevioustimeline,acrucialfactortobeconsideredasspikingneuron’sstatesreliedonthepreviousstates.\nInourmodel,theevent-basedvideostreamsaredefinedasa3Dtensorwiththeshapeof(u,v,t)whereuandvdenote\nthe coordinate of the width and height of layer l, and t stands as the timestamp [35]. By configuring the optimal\ntemporalfrequency,userscancontroltfromtheoriginalsegment.\nAconvolutionalneuralnetwork(CNN)isafeed-forwardnetworkcomposedofmultiplelayerswherefiltersconvolve\naroundtheinputorsinglelayerforneuronstocollectmeaningfulfeaturesorpatterns[36]. However,astheevent-based\nvideohasnotonlyspatialinformationbutalsotime,thesampledinputsequenceS(n)wouldrequirea3Dconvolutional\nkernelforbuildingaspikingneuronalfeaturemapasshowninFigure2. Eachspikeinsidethekernelrepresentsa\nclusterofspiketrainss (t)wherethespike’scoordinateis(u,v),andisinrangeofthetemporalresolutionwindowt.\nu,v\nSpikesfromthekernelcontinuallygetcumulatedtotheneuronsinthefeaturemap,andwhenthespikesintheregion\nofthekernelexceedacertainthreshold,itwouldgeneratethemembranepotentialforasingleneuroninthefeature\nmap,creatingspatio-temporaldynamicpatterns.\nThe3Dspikingconvolutionoperationisperformedbyconvolvingthespiketrainsinthekernelwithaspikeresponse\nkernelandapplyingthethresholdfunction. Eachspiketrainisconvertedintoaspikeresponsesignal,thensubsequently\ntransformedintothemembranepotentialbyintegratingthesignalwiththerefractoryresponseoftheneuron:\n4\nAPREPRINT-NOVEMBER20,2024\nFigure2: The3Dconvolutionalspikingoperation.\na (t)=S (t)∗ε (t) (5)\nu,v u,v d\nK K\n(cid:88) (cid:88)\nu (t)= W a (t)+(S (t)∗ν(t)) (6)\nj,k m,n m+(j−1),n+(k−1) j,k\nm=1n=1\nS (t)=1andu (t)=0 when u (t)≥V (7)\nj,k j,k j,k thr\nwhere∗denotestheconvolutionoperator,W denotesthesynapticweightsat(m,n)ofthekernel,u represent\nm,n j,k\nthemembranepotentialatcoordinate(j,k)presentedbythefeaturemap,K representsthewidthandheightofthe\nconvolutionkernel,andν(t)istherefractorykernel.\nOurproposedmodelisdevelopedbasedonexistingmodels[20,10]forDVSGesturerecognition,whichimplements\nSNNtraining anda 3Dspike convolution operation. TheSLAYER andTrueNorthmodels consistof 8layersand\n16layers,respectively. Figure3presentsourproposednetworkmodelandsystem,wherethenetworkissimplified\nandcomprisesonly4layers. Whentestedwithamorecomplexmodel,weobservedthatitnotonlyoccupiedmore\nmemory,butalsohadadetrimentalimpactonoverallperformance,asmorelayershinderedtheneuronsfromfiring\nspikes,leadingtolowereventratebecausespikescouldnotbepassedtoconsecutiveneuronlayers.\nThe 720x720 resolution event streams for driver motion recognition were recorded by an event-based camera for\n3 seconds and delivered to the first 3D pooling layer. Unlike conventional convolutional neural networks, where\nconvolutionlayersareprocessedfirst,ourmodelimplementsapoolinglayerbeforepassingthedatatotheconvolution\nlayer. Themainreasonforthisistodown-samplefeaturemapstoresolvelargememoryusage,aswellastoextract\nfeatures. Comparedtoclassicalpoolinglayers,SNNpoolinglayersareconsideredtrainablelayer,astheynotonly\ndown-sampletheeventstreamsbutalsotakeintoaccounttemporalaspects,enablingthelearningofspatio-temporal\nfeaturesfromtheinput. Thepoolinglayerusesakernelofsize8x8andpassesdatatotheconvolutionlayer,which\nusesa5x5kerneltoproduce16channelsoffeatures. Thefeaturesarethenfedto2finalfully-connectedlayers,which\nprovide13outputsrepresentingthespikeratesforthegiveneventdata.\nWenoticethattheexistingspikingneuralnetworksdesignedforotherapplicationsarenotoptimizedforautonomous\ndriving,whichrequireslowpowerconsumption,real-timeinference,andefficientdeploymentonactualneuromorphic\nchips. Toeffectivelyimplementdrivermotionrecognitionforautonomousdriving,itisimportanttohaveaneural\nnetworkthatisoptimizedforneuromorphichardware(simplebutwithoutperformancedegradation)aswellasan\nevent-baseddatasetfortrainingsuchnetworks. Weaddressedtheseelementsbydesigning,implementing,andtestinga\nnetworkthattakesthesefeaturesintoconsideration.\n5\nAPREPRINT-NOVEMBER20,2024\nEvents\n3 seconds\nEvent camera Event stream Driver’s gesture stream\n(1280x720,3 seconds)\n0(Head tilts to side)\n1 (Head tilts forward)\n…\n11 (Looks back)\n12 (Bends down)\n3D Max Polling 3D Convolutional Flatten Fully-connected Output\n(90x90x2) (90x90x16) (512) (13)\nFigure3: ProposedEvent-basedDriverGestureRecognitionSystem.\n4 EXPERIMENTSANDRESULTS\n4.1 EVENT-BASEDCAMERAANDDATACONVERSION\nProphesee’sMetavisionEVK41cameraisoneofthelatestevent-basedvisionsensorsthatsupportsupto1280x720\npixelresolution. Generally,whenachangeinpixelvaluesexceedingacertainthresholddefinedbytheuseroccurs,the\nevent-basedcameraasynchronouslydetectssuchchangesinbrightnessandgenerateseventsspecifictothatpixel[11].\nEacheventcontainspixelinformationdescribingthepositioninthexandycoordinates,indicatingamotion’schange,\nandatimestampfortheoccurrenceoftheevent. Thedeviceoffersahighdynamicrange(86dB,however,itcanreach\nover120dBbasedonlowlightcutoffmeasurementbeing: 0.08lux)andatypicalandmaximumeventrateof1.06\ngiga-eventspersecond(Geps).\nIntheexperiment,inordertoallowdirectusageofthespikedataobtainedfromthehigh-resolutionevent-basedcamera\nwithoutrequiringadditionalresources,weminimizedmodificationtotheextractorsothatitcanretrievecoordinate,\npolarity,origin(anewlyimplementedfeatureintheEVK4),andtimestampinformation. Moreover,weobservedthat\nthemostmeaningfulmotioninformationwascontainedinthecentral720x720pixelssothatthesizeoftheinputevents\nwascroppedto720x720pixels.\n4.2 DATASET\nOver the past years, a profusion of gesture datasets captured with simple frame-based sensors has been presented.\nHowever,tostimulatetheimprovementofevent-basedcomputervision,Huetal.[37]stronglysupporttheimportance\noftheDVSdataset. Serranoetal.[38]introducedthefirstlabeledevent-basedneuromorphicvisionsensordataset\nwhichoriginatedfromtheclassicalMNISTdigitrecognitiondatasetbymovingtheimagesalongcertaindirections\nwithinthescreen. ThisdatasetwasfurtherdevelopedbyOrchardetal.[39],whoremovedseveralframeartifactsusing\napan-tiltunit. Itcanbeobservedthatgeneratingartificialmovementforstaticimagesenabledtheproductionofevent\n1EVK4cameraisanHDEvent-BasedVisionevaluationkitmountedwithIMX636sensor,developedbySonyandProphesee.\nPleaserefertohttps://www.prophesee.ai/event-camera-evk4/\n6\n… …\n…\nAPREPRINT-NOVEMBER20,2024\noutputs,whichbecameimportantforobjectrecognitioninneuromorphicsystemssuchasspikingneuralnetworks[7].\nHowever,ithasbeenobservedthatstaticimagerecognitionwithevent-basedvisionsensorsisineffective,astheirmain\npurposeisfocusedondynamicscenes. Recognizingthesedrawbacks,somedatasetsconsistingofdynamicsceneshave\nbeenpresented. Berneretal.[40]introducednoveldatasetsthatconvertedtheexistingvisualvideobenchmarksfor\nobjecttrackingandaction/objectrecognitionintospikingneuromorphicdatasetsusingtheDAVIScamera’soutput.\n(0a) (0b) (1) (2) (3) (4a) (4b)\n(5a) (5b) (6a) (6b) (7a) (7b) (8)\n(9) (10) (11a) (11b) (12a) (12b)\nFigure4: Sampleshotstakenforeachdrivergesture.\nTable1: Labelsofthecorrespondingdrivergestures\nLabel Gesture\nFront\n0 Headtiltstoside(a: left,b: right)\n1 Headtiltsforward\n2 Headtiltsbackward\n3 Normaldriving\n4 Phonecall(a: right,b: left)\n5 Looksback(a: left,b: right)\n6 Bendsdown(a: left,b: right)\nSide\n7 Headtilts(a: left,b: right)\n8 Headtiltsforward\n9 Headtiltsbackward\n10 Phonecall\n11 Looksback(a: left,b: right)\n12 Bendsdown(a: left,b: right)\nEventhoughthesedatasetsshowdynamicmovementsinaspikingneuromorphicway,accordingtoTanetal.[41],a\nDVScameraproducesmicrosecondtemporalresolutionoutput. Incontrast,datasetsproducedbyconventionalvision\ntoolssuchasvideorecordersorcolorcamerashavetensofmillisecondsoftemporalresolutions,whichcausesalossof\nhightemporalfrequencyduringconversion. Moreover,theyaddthatthereisahighchanceofsubsidiaryunwanted\nartifactsbeingpresentduringconversion. Toaddresstheseproblems,Amiretal.[20]presenttheDvsGesturedataset,\nwhichdirectlycapturedsceneswiththeDVS128camera. Additionally,theDvsGesturedatasetswerecapturedunder\ndifferentlightingconditions,asDVSsensorsarelessaffectedbybrightnessandintroducesomemeaningfulnoise.\nHowever, the DvsGesture dataset is mainly utilized for simple classification tasks, which may be considered less\npractical.\nTheN-DriverMotiondatasetcomprises1,239instancesofasetof13drivermotions(1normaldrivingmotionand12\ndrivingmotionsthatareconsideredpossiblydangerous)asshowninFigure4andTable1. Thesemotionsincludehead\nfalling(front,sides,andback),holdingacellphone(right/lefthand),lookingback(totheright/leftside),thebodygoing\ndown(totheright/leftside),andallthesamemotionsaboveinside-capturedmanner. Thedatasetwascollectedfrom23\nsubjectsunderthreedifferentlightingconditions. Allsubjectswerewithinaspecificagerange;theyhadnodisabilities,\n7\nAPREPRINT-NOVEMBER20,2024\nSeizuresduringdriving Suddenstanding Abnormalcameraangles\nCollisionswherethe Hand-swipegestures\ndriverhitsthewindshield\nFigure5: Sampleshotstakenofabnormaldrivergestures. Notethathand-swipegesturesarenotsimplehand-waving\nactionsbutareactionstoprotectoneselffromvariousattacks\nTable2: Abnormaldrivergestures\nSeizuresduringdriving\nSuddenstanding\nCollisionswherethedriverhitsthewindshield\nHand-swipegestures\nAbnormalcameraangles\noranyotherproblemsthatcouldaffectdriving. Eachsubjectperformedall13motionsinasinglerecording,where\neachmotionwasrecordedforaround3secondsunderthesamebrightnesscondition. Thethreebrightnessconditions\nwereclassifiedas\"bright,\"\"moderate,\"and\"dark\"totesthowmuchbrightnesswouldaffecttherecordingsandto\nexaminetherobustnessofeventdatainharshilluminationcondition. Wenotethatinformedconsentwasobtainedfrom\nallparticipantsduringdatacollection. Toevaluatetheclassifier’sperformance,werandomlyselected992motionsfor\nthetrainingset,andtheremaining247motionsweredesignatedasthetestset.\nDuetolimitationsinrecruitingparticipants,wewereunabletoincludeexperimentsinvolvingdriverswithmedical\nconditions or disabilities. This will be expanded in future work. Instead, to address unpredictable scenarios, we\nintroducedfivenewtypesofeventdatathatwerenotusedduringtraining. Theseadditionaltestcasesincludeabnormal\nreactionssuchasseizuresduringdriving,suddenstandingorrisingmovements,collisionswherethedriverhitsthe\nwindshield,hand-swipegestures,andabnormalcameraanglescausedbyaccidentsorissueswiththecamera’sfixed\nposition. Thesenewlycreatedevent-basedvideosequenceswerespecificallydesignedtotesthowthesystemresponds\ntootherabnormalcasesnotseenduringtraining.\n4.3 EXPERIMENTALSETUPANDTOOLFLOW\nTheexperimentismainlyconsistedof3sub-partstomeasuredifferentperformanceofourproposedmodel:theaccuracy\nofthe13-classdrivermotionclassification, theaccuracyofclassifyingunexpectedabnormaldrivermotionversus\nnormaldrivermotion,andtheenergyefficiencywithrespecttothroughputinvariousAIaccelerators.\n4.3.1 Multipledrivermotionclassification\nTheaimofthisexperimentliesinobservinghowaccuratelyourproposedsystemcanclassifyvariousdrivermotions\n(13classesintotal). WiththemodeldevelopedbasedontheLava-DLAPI,ourproposednetworkwillbeloadedonto\ntheRTX3080GPUandtrainedfor200epochs. Duringeachepochintraining,themodelswouldbeevaluatedwith\nthetestsetandproduceaccuracybasedonthespikeratesoftheoutputspikes. Ifthemodelproducesthehighesttest\naccuracy,themodel’sweightsarestoredasaPyTorchstatedictionaryfile. Furtherexplanationabouttheconfiguration\nofthesystemisshowninsubsection4.4.\n8\nAPREPRINT-NOVEMBER20,2024\n4.3.2 Inferenceonunseenabnormaldrivingmotions\nWhileitisimportanttoseehowthemodelperformsclassificationsbasedontheprovided13motionsets,itisalso\nimportant to check the model’s capability in classifying unexpected motion sets. Here, we make inferences on a\nseparatedatasetthatiscomposedofabnormaldrivermotionsnotincludedinthe13classes,aswellasnormalmotions,\nwherethemodelhasbeentrainedtoclassifythemaslabel3. Inthisexperiment,wehavesettheobjectiveasbinary\nclassification: abnormaldrivermotionandnormaldrivingmotion. Givensuchabnormaldrivermotions,ifthemodel\nproducespredictionsotherthanlabel3(normaldrivingmotion),wecanobservethatthemodeliscapableofclassifying\nthemasabnormalmotion,andviceversa. WehaveusedthefullytrainedCSNNmodelwiththesameconfigurationfor\neventdata(e.g.,samplingtime)thatwasusedformeasuringaccuracyonthe13-classclassificationtask. Thetestdata\nforthistaskiscomposedof82abnormaldrivermotions(10participantsperforming5differentunexpectedabnormal\ndrivingmotions,asshowninfigure5andtable2)alongwith63normaldrivermotionsfromtheoriginaltestset. The\nevaluation process was conducted in the same way as the multiple driver motion classification, except in a binary\nclassificationfashion.\n4.3.3 EnergyefficiencyinvariousAIaccelerators\nEnergyefficiencyisoneofthemostpromisingaspectsofSNN,andthisexperimentisdesignedtoshowtheoverall\nefficiencyofthemodelwithrespectto3AIaccelerators: NvidiaRTX3080,NvidiaJetsonXavierNX,andIntelLoihi\n2. ForbothNvidiaGPUs,weloadedthemodelsdirectlyontothedevicesandmeasuredvariousinferencecosts(latency,\ntotalenergyconsumption,etc.). DuetofastinferencespeedofGPUs,wemeasuredbyaveragingeachofthetotal\ncostsbythenumberofsamplesprocessedtomeasurethevalues. ForLoihi2,wedesignedaspecificpipelinethat\nenablesustoloadthemodelintothesystemandfeedtheeventdatadirectlytothemodel,alongwiththeutilizationof\nmeasurementtoolsprovidedbytheLavaAPI.DetailedinformationaboutexperimentalflowsandsettingsforJetson\nXavierNXandLoihi2isdescribedinsubsection4.5.\n4.4 NETWORKCONFIGURATIONANDTRAINING\nToenableadequatetraining,thedatasetwasconvertedfrombinaryformatintotensorsconsistingofx-coordinates,\ny-coordinates,polarities,andtimestamps. Wealsosetthesamplingtimeto2.0seconds,asthecoremovementswere\nmostlywithinthisrange. Moreimportantly,wewantedtocheckwhetherourproposedmodeliscapableofclassifying\ndrivermotionsinashortamountoftime,asinreal-timeapplications,suchsystemsshoulddetectabnormalactivities\nrapidly. Inthisform,thedatacannowbeusedasinputtotheCSNNduringruntime.\nAlloftheapplicationswereimplementedandmappedtoGPU/CPUtobeexecutedasthesimulationforLoihi2,using\ntheLavaversion0.4.4softwareframeworkandtheLava-DLlibraryfortraining. OurCSNNmodelsweredeveloped\nwithneuronmodelsandtrainedwiththeevent-baseddatasetusingtheSLAYERAPI,whichwereallprovidedbythe\nLava-DL.\nWetrainedmultipleCNNconfigurationstocompareaccuracyresults. Eachnetworkwastrainedfor200epochswitha\nsinglebatchsizeduetothedataset’shighresolution. WeselectedCUBALIFasthebaseneuronmodelforthespiking\nCNNmodels. Foroptimization,thelearningratewassetto3×10-3andADAMwasusedastheoptimizer. Fortheloss\nfunction,wechosespikerateloss. Thenetworkconfigurationforthe4-layeredCSNNandtheneuronparametersare\nshowninTable3\nNeuronParameters Value\nlayer mapsize features kernel padding\nVoltagethreshold 1.25\nN/A 720x720 2 N/A N/A\nCurrentdecay 0.25\n1 Pool 90x90 2 8 N/A\nVoltagedecay 0.03\n2 Conv 90x90 16 5 2\nTaugradient 0.03\n3 Flatten 90x90 16 N/A N/A\nScalegradient 3\n4 Dense 512 N/A N/A N/A\nTruerate 0.2\n5 Dense 13 N/A N/A N/A\nFalserate 0.03\nTable3: Networkconfigurationof4-layeredCSNN(left)withneuronparameters(right)\n4.5 EDGEDEVICECONFIGURATIONSANDINFERENCE\nInthissection,weprovideadetaileddescriptionofvariousAIacceleratorsthatarespecializedforuseinedgedevices.\nWeutilizedtwodifferentedgeAIaccelerators: JetsonXaiorNX,andIntelLoihi2tocompareoverallenergyefficiency\nwithrespecttoimageprocessingdelays.\n9\nAPREPRINT-NOVEMBER20,2024\nTrained CSNN\nDataLoader\nModel Weights\nLoihi2 (OheoGulch) Profilers\nEmbedded Neurocore Embedded - Power Profiler\n- Memory Profiler\nCore N-DriverMotionCSNN Core - Execution Time Profiler\n- Activity Profiler\nRingBuffer OuputProcess\nInput Adapter Output Adapter\n(spatio-temporal buffer) (WTA classification block)\nClassification Result\nFigure6: PipelineforInferenceofCSNNontheLoihi2processor. Theblocksthatarepartofthemainprocessesare\ncoloredasgray,whilethegreenblocksarenotpartofthemainprocess.\nTable4: Networkconfigurationofa4-layeredCSNNforLoihi2\nlayer mapsize channels kernel padding\nInput 40x40 2 N/A N/A\nConv 40x40 4 5 2\nFlatten 40x40 4 N/A N/A\nDense 512 N/A N/A N/A\nDense 13 N/A N/A N/A\n4.5.1 JetsonXaviorNX\nTheJetsonXaviorNXisafullyfeatureddevelopmentboardequippedwitha6-coreARMCPUanda48tensorcore\nprocessingunitsupportingaclockfrequencyupto1.1GHz,alongwith16GBofmemory. Tominimizepowerusage,\nweoperatedonthestandardmodule,wheretheGPU’sclockfrequencyissetto0.67GHz. Allinferenceconfigurations\nwere the same as with the RTX 3080, except for the library version. Due to the restriction of the Python version,\ninferenceontheJetsonXaviorNXwasexecutedwithalowerversionofthelava-dl(0.4.0)library.\n4.5.2 IntelLoihi2\nANeuromorphicchipisoneoftheedgeAIacceleratorsthatparticularlytargetstheimplementationofSNNs. Among\nvariousneuromorphicchips,weusedLoihi2toobservetheoverallpowerconsumptionofthebenchmarkthreefully\nconnectedlayerSNNmodel,andtheproposedCSNNmodel,asLoihi2adoptsCUBALIFasthebaseneuronmodel\nbestfittingalloftheproposedmodels[26]. Forthisexperiment,weusedtheOheoGulch,oneoftheLoihi2-based\nneuromorphicsystemsthatcontainsasingle-socketedLoihi2chip.DuetothelimitedavailabilityofLoihi2chips,along\nwiththenatureoflargeinputandmodelsize,someadjustmentstotheinputandmodelsweremade. Wedownsampled\ntheinputfrom720x720to40x40withmaxpoolingandextractedthemotionstreamtimefromtotalof3.0secondsto\n1.7seconds. Forthethreefullyconnectedlayermodel,therewerenochanges. FortheproposedCSNNmodel,we\nexcludedtheCUBApoolinglayersincetheinputhadalreadybeendownsampledandreducedthenumberoffeatures\nextractedfrom16to4. ThedescriptionofthenetworkmodelonLoihi2isshowninTable4. Wewereabletoloadthe\nthreefullyconnectedlayermodelontoasingleLoihi2chip,whereastheproposedCSNNmodelrequiredatotalof4\nLoihi2chipstobeloaded.\nToenableinferenceonLoihi2,itisnecessarytocreateapipelinethatcanprocessspatio-temporaldataforclassification\ntasksonLoihi2. WecreatedapipelinetoconductinferenceonLoihi2,whichisshowninFigure6. Theeventdata\n10\nAPREPRINT-NOVEMBER20,2024\nwasstoredinabufferblocknamedRingBuffer,whereitpassedtheframedataoneatatimetoLoihi2. Astheframe\ndatawaspassedtoLoihi2thelastlayerinthemodelproducedspikesthatgeneratedtheclassificationresultforthe\nsingleframe. ThespikeswerecumulativelystoredintheOutputProcessblockuntileveryframeintheeventdatawas\nprocessed. ThefinalclassificationwasconductedinaWinner-Takes-All(WTA)fashion,wheretheclasswiththemost\nspikeswasthepredictedvalue.\nThetotaldescriptionofhowthemodelswereconvertedandconvergedattheOheoGulchneuromorphichardwareis\nshowninTable5andTable6,whereeachcolumnindicatesthepercentageofthetotalusagefortheinputaxons,neuron\ngroup,neurons,synapses,axonalmapping,andaxonmemory,respectively.\nTable5: Resourceutilizationfora3-fullyconnectedlayermodelonLoihi2\nAxonIn NeuronGr Neurons Synapses AxonMap AxonMem Total Cores\n3.20% 12.50% 0.32% 12.80% 0.08% 0.00% 12.94% 1\n3.20% 12.50% 1.56% 73.60% 0.40% 0.00% 62.09% 8\n25.31% 12.50% 0.12% 50.62% 0.03% 0.00% 60.80% 103\nTotal 112\nTable6: ResourceutilizationfortheproposedCSNNmodelonLoihi2. Theamountofcoreutilizationfortheproposed\nmodelismorethantwiceofthe3-fullyconnectedlayermodelduetotheconvolutionallayeractingasthebottleneckof\nthemodel\nAxonIn NeuronGr Neurons Synapses AxonMap AxonMem Total Cores\n1.60% 12.50% 0.32% 6.40% 0.08% 0.00% 6.54% 1\n40.00% 12.50% 0.02% 40.00% 0.01% 0.00% 64.02% 256\n0.01% 12.50% 62.52% 0.11% 16.00% 0.00% 25.70% 3\nTotal 260\n4.6 Results\n4.6.1 Accuracyon13-classprediction\nTheresultsfor13motionclassificationsfromthetrainingmodelsareshowninFigure7andTable7. Inthebenchmark\ndatasetforevaluatingourmodels,weusedtheDVSGesturerecognitiondataset,comprising11handgesturecategories\nfrom29subjectsunder3illuminationconditions[20]. WetestedthreedifferentSNNconfigurations: 1)alllayers\ncomposedofdenseconnection,2)aconvolutionalspikinglayerwith2fullyconnectedlayers,withthepoolingkernel\nsizesetto8,and3)thesamemodelas2,exceptwiththepoolingkernelsizesetto18. Forallmodels,weselectively\nusedonlythefirst2.0secondsoutof3.0secondsofmotionstreamsforeachclasstoclassifytheactions. Additionally,\nthetemporalresolutionwassetto5millisecondstoincreasetrainingefficiency. Foreachexperiment,werecorded\nthe best loss and accuracy of the SNNs for both training and inference steps, as well as the confusion matrix for\nthe corresponding inference results. The confusion matrix in Figure 7 shows that the normal situation (label 3) is\ndistinguishedfromthehazardoussituations. Figure8illustratestheclassificationresultswithoutputspikescapturedon\ntheoutputlayer,classifying13drivermotions.\nThesystemfromtheproposedCSNNmodelwith4layersshowedthehighestaccuracy,reaching94.04%inthetest.\nTheinclusionofthepoolingandconvolutionlayersimprovedaccuracyperformancebyalmost8%overthe3-layered\nmodel,whichwascomposedonlyoffullyconnectedlayers. Eventhoughourmodelissimplifiedwithrestrictions\nappliedtoresolvetheout-of-memoryproblemcausedbythehigh-resolutioneventdatafortrainingandtesting,itwas\nDataset Method Architecture Accuracy\nTrueNorth SNN(16layers) 91.77%(94.59%)\nDVSGesture SLAYER SNN(8layers) 93.64±0.49%\nSLAYER ProposedCSNN(4layers) 92.80%\nSLAYER SNN(3fully-connectedlayers) 85.11%\nN-DriverMotion SLAYER ProposedCSNN(4layers,poolingkernelsize: 8) 94.04%\nSLAYER ProposedCSNN(4layers,poolingkernelsize: 18) 91.07%\nTable 7: Classification results comparing DVS gesture dataset and N-DriverMotion dataset. For DVS Gesture, it\nincludesresultsfrombothSLAYERandIBMTrueNorth.\n11\nAPREPRINT-NOVEMBER20,2024\n(a) Accuracy (3FC) (b) Loss (3FC) (c) Confusion matrix (3FC)\n(d) Accuracy (CSNN, PK=18) (e) Loss (CSNN, PK=18) (f) Confusion matrix (CSNN, PK=18)\n(g) Accuracy (CSNN, PK=8) (h) Loss (CSNN, PK=8) (i) Confusion matrix (CSNN, PK=8)\nFigure7: Results(accuracy,loss,andconfusionmatrix)oftheproposedCSNNwithpoolingkernelsizes8and18\n(PK=8andPK=18)vs. threefully-connectedlayerSNN(3FC).\n(a) Head tilts to side (label 0) (b) Normal driving (label 3)\n(c) Looks back (label 5) (d) Phone call (label 10)\nFigure8: Classificationresultwithoutputspikes\n12\nycaruccA\nycaruccA\nycaruccA\nssol\nssol\nssol\nAPREPRINT-NOVEMBER20,2024\nFigure9: Classificationresultforunexpectedabnormaldrivermotion. Themodeldetectsalltheabnormalactions\ncorrectly,demonstratinghighsensitivityindetectingtheseactions,andalsoclassifiesnormaldrivermotionswithhigh\naccuracy.\nabletodemonstratecomparableresults. Wenotethatourproposeddatasethasmorecategoriesforclassificationand\ndoesnotincluderepetitiveactions,unliketheDVSGesturerecognitiondataset.\n4.6.2 Accuracyonunexpectedabnormaldrivermotionclassificationtask\nTheresultsforbinaryclassificationofunexpectedabnormalmotionsandnormalmotionsaredisplayedinFigure9.\nExceptfor4normalmotionswhichwereclassifiedasabnormaldrivingmotions,themodelwashighlycapableof\ndistinguishingbetweentwocategories,reachinganaccuracyof97.24%. Onesignificantpointtonoteisthatthere\nwerenoabnormalmotionsmisclassifiedasnormalmotions,whichdemonstratestherobustnessofourproposedCSNN\nmodelindetectingsuchunexpectedmotions.\n4.6.3 LatencyandEnergy\nTomeasurethepowerconsumptionfortheNvidiaRTX3080,weusedtheNVIDIAManagementLibrary(NVML),\nwhichallowsustoextractthethroughputandenergyconsumptionwithrespecttosingleeventinference. Duetothe\nfastinferencetimeoftheGPU,wecalculatedthelatencybyaveragingthevalues,dividingtheoverallinferencetimeby\nthetotalnumberofinferencesamples. Energyconsumptionwasalsomeasuredbyaveragingthetotalenergyoverthe\ntotaltimeofinference.\nNvidiaJetsonXavierNXdoesnotsupportNVIDIAManagementLibrary,unliketheRTX3080GPU.Toresolvethis\nissue,weutilizedjetson_statslibrary,whichenablesustokeeptrackoftheenergyconsumptionsoftheGPUbylogging\nthestatisticsasaCSVfile. SimilartotheRTX3080,thecalculationforlatencywasdonebyaveragingthevalues. The\nexperimentwasexecutedwithnoadditionalGPUprocessesotherthanthemeasurementprocessbeingloaded.\nInthecaseofLoihi2,theLAVAframeworksupportsprofilertoolswhichenablesustoobtainvariousmeasurement\nvaluesrelatedtopower,executiontime,countsofneurocoreactivities,andneurocoreSRAMutilizationrates. Toget\npreciselatencyandenergydissipationresults,weranthemodel1milliontimes,whichprovidedenoughtimeforthe\nprofilertomeasuretheoverallcosts. AllexperimentsforlatencyandenergycomparisonsbetweentheGPUsandLoihi\n2wereconductedwithequalmodels,bothadjustedforLoihi2. Theresultsforthroughputandvariousenergy-related\nmeasurementsforsinglesampleinferenceareshowninTable8.\nTheoverallenergyconsumptionforLoihi2for1millioninferenceswas1.82Jwhichwashighlycomparabletothatof\nbothGPUs,wheretheRTX3080requiredover500J,andtheJetsonXaviorNXrequired1.13Jforasingleinference\n13\nAPREPRINT-NOVEMBER20,2024\nTable8: ComparisonsbetweenGPUsandLoihi2oninferencecostforasinglesample. Itisclearthatdespitethe\nthroughputoftheRTX3080beinglower,theenergydelayproduct(EDP)ofLoihi2ismuchlowerthanthatofboth\nGPUs,demonstratingthehighenergyefficiencyofLoihi2forSNNmodels\nInferenceCostPerSingleSample\nHardware Latency Throughput Energy EDP\n(ms) (sample/s) Total(mJ) Dynamic(mJ) µJs\nNvidiaRTX3080 11.544 86.627 5.163x105 N/A 5.94x106\nNDriverMotion3FCN NvidiaJetsonXavierNX 83.3 12.011 6.29x102 N/A 5.24x104\nLoihi2OheoGulch 1.33x103 442.364 2.42x10−3 1.89x10−4 3.219\nNvidiaRTX3080 15.103 66.214 5.218x105 N/A 7.88x106\nNDriverMotionCSNN NvidiaJetsonXavierNX 1.83x102 5.4617 1.13x103 N/A 2.06x105\nLoihi2OheoGulch 1.441x104 40.835 2.64x10−2 7.46x10−4 380.29\nontheCSNNmodel. Thethroughput,however,showedthatwhileLoihi2processed40.835samplesperseconds,the\nRTX3080processed66.214samplesperseconds. WebelievethatthiswasduetotheinferencemethodofLoihi2,\nwhichdidnotusematrixmultiplication,butonlyadditionfortheinputspikedata. TheJetsonXaviorNXperformed\nverypoorlyonthroughput,withonly5.46samplesbeingprocessedpersecond.\nTocomparethebalanceoflowenergyandfastruntimesofthedevices,wecalculatedtheenergy-delayproduct(EDP),\nwheretheenergyconsumptionandperformanceofthedeviceswereweightedequally[42]. ThecalculationforEDP\nwasconductedbymultiplyingthetotalenergybythelatencyforasinglesampleinference. IntheEDPcomparison\nbetweenLoihi2andthetwoGPUs,weobservedthatLoihi2hadabetterEDPthantheGPUsforbothmodels,withthe\n3FCNmodelbeing1.8milliontimesandtheCSNNmodelbeing20,721timesmoreefficientcomparedtotheRTX\n3080,andthe3FCNmodelbeing16,278timesandtheCSNNmodelbeing541timesmoreefficientthantheJetson\nXaviorNX.\n4.7 FUTUREWORK\nWhile our proposed driver motion recognition system achieves significant improvements in energy efficiency and\naccuracy,thereareseveraldirectionsforfutureworktofurtherenhanceitsperformanceandapplicability. First,weplan\ntoextendtheN-DriverMotiondatasetbyincreasingthediversityofdrivingscenarios,includingmorecomplexdriving\nconditions,variedmotionpatterns,andalargerpoolofparticipants. Thiswillenablethemodeltogeneralizemore\neffectivelytoreal-worlddrivingenvironments. Additionally,weplantoexploreapproachesforenhancingsafetyfor\npedestriansandvehiclesinautonomousdrivingconditions. Anotherimportantareaofexplorationistheintegrationof\nonlinelearningmechanismswithintheneuromorphicsystem,allowingthemodeltoadaptdynamicallytochanging\ndriving conditions and new motion patterns in real time. To achieve this, we will explore unsupervised learning\nmethodsbasedonthespike-timing-dependentplasticity(STDP)learningrule. Moreover,wewillevaluatethesystem’s\nscalabilityandperformanceacrossdifferenthardwareplatforms,includingmoreneuromorphicprocessorsandedge\ndevices. Lastly,weaimtooptimizetheneuromorphicpipelinetoaddresschallengessuchaslatencyreductionand\nincreasedrobustnessinhighlydynamicandunpredictabledrivingscenarios,furtherenhancingthesystem’spracticality\ninreal-timeautonomousdrivingapplications.\n5 CONCLUSION\nInthispaper,weproposeN-DriverMotion,anewlycollectedevent-baseddatasetdesignedtolearnandpredictdriver\nmotions using a neuromorphic vision system. Our system consists of an event-based camera that captures driver\nmovements as spike inputs and an optimized convolutional spiking neural network (CSNN) capable of efficient\ninference on 720 x 720 event streams without the need for computationally expensive preprocessing. The dataset\nincludes13drivermotioncategories,classifiedbydirection,lightingconditions,andparticipants,includingchallenging\nenvironmentslikelow-lightconditionsandtunnels.\nOurexperimentsshowthatthesystemachievedahighaccuracyof94.04%inrecognizingdrivermotions,evenunder\ndifficultconditionssuchasvaryinglightanddirectionalinputs. Thisdemonstratesthatourproposedfour-layerCSNN,\ndesignedfortrainingandinferenceonenergy-andresource-constrainedplatforms,canmeetperformancerequirements.\nInaddition,ourexperimentsdisplaythecapabilityofdistinguishingbetweenunexpectedabnormaldrivermotionsand\nnormaldrivermotionswithanaccuracyof97.24%. Oursystemwasespeciallyabletoclassifyalltheabnormaldriver\nmotions,indicatinghighreliabilityinsensingvariousabnormaldrivermotionswhendeployedinrealapplications.\nWe also demonstrated that our system was comparable to other applications that implemented direct use of event\nvisionsystemswithSNNs,suchastheDVSGesturerecognitiontask,whereourproposedsystemachievedsimilar\n14\nAPREPRINT-NOVEMBER20,2024\naccuracywithalesscomplexmodelstructure. WhendeployedonIntel’sLoihi2neuromorphicprocessor,thesystem\ndemonstrated significant energy efficiency, with an energy-delay product (EDP) that was over 20,721 times more\nefficientthantheRTX3080and541timesmoreefficientthantheJetsonXaviorNX.Thislevelofefficiencyiscritical\nforreal-time,on-deviceAIapplicationswherepowerconsumptionisakeyconstraint.\nBy eliminating the need for time-consuming preprocessing and significantly reducing power consumption while\nmaintaininghighaccuracy,oursystemcanadvancethesafetyandefficiencyofautonomousdrivingsystemsusing\nneuromorphicvisiontechnology. Furthermore,theN-DriverMotiondatasetprovidesavaluableresourceforfuture\nresearchindriverbehaviorprediction,particularlyinAI-drivensystemsrequiringahigh-resolutioneventdataset.\nACKNOWLEDGMENT\nThisworkwassupportedbytheNationalResearchFoundationofKorea(NRF)grantfundedbytheKoreagovernment\n(MSIT)(NRF-)andtheMinistryofEducation(NRF-).\nReferences\n[1] Y.MA,Z.Wang,H.Yang,andL.Yang,“ArtificialIntelligenceApplicationsintheDevelopmentofAutonomous\nVehicles: ASurvey,”IEEE/CAAJournalofAutomaticaSinica,vol.7,no.JAS-2019-0401,2020.p.315.\n[2] EU, “New rules to improve road safety and enable fully driverless vehicles in the EU,” 2022. Accessed on:\nSeptember30,2024.[Online].Available: https://ec.europa.eu/commission/presscorner/detail/en/ip_22_4312\n[3] P.Blouw,X.Choo,E.Hunsberger,E.Chris,“BenchmarkingKeywordSpottingEfficiencyonNeuromorphic\nHardware,“inNeuro-InspiredComputationalElements(NICE),2019,pp.1–8\n[4] C.Tan,S.Lallee,G.Orchard,“Benchmarkingneuromorphicvision: lessonslearntfromcomputervision,“in\nFrontiersinneuroscience,2015\n[5] S.Kim,S.Park,B.Na,S.Yoon,“Spiking-yolo: Spikingneuralnetworkforenergy-efficientobjectdetection,“in\nProceedingsoftheAAAIConferenceonArtificialIntelligence,2020,pp.11270–11277\n[6] R.Massa,A.Marchisio,M.Martina,M.Shafique,“AnEfficientSpikingNeuralNetworkforRecognizingGestures\nwithaDVSCameraontheLoihiNeuromorphicProcessor,“inInternationalJointConferenceonNeuralNetworks\n(IJCNN),2020,pp.1–9\n[7] J.Pérez-Carrasco,B.Zhao,C.Serrano,B.Acha,T.Serrano-Gotarredona,S.Chen,B.Linares-Barranco,“Mapping\nfromframe-driventoframe-freeevent-drivenvisionsystemsbylow-rateratecodingandcoincidenceprocessing\napplicationtofeedforwardConvNets,“inIEEEtransactionsonpatternanalysisandmachineintelligence,vol.35,\nno.11,2013,pp.2706–2719\n[8] “Lava,anopen-sourcesoftwareframeworkfordevelopingneuromorphicsystems,”2022.Accessed: September\n30,2024.[Online].Available: https://github.com/lava-nc/lava\n[9] M.Davis,“TakingneuromorphiccomputingtothenextlevelwithLoihi2,”inintelTechnologyBrief,2021\n[10] S.Shrestha,G.Orchard,“SLAYER:SpikeLayerErrorReassignmentinTime,“inAdvancesinNeuralInformation\nProcessingSystems,vol.31,2018\n[11] G.Gallego,T.Delbrück,G.Orchard,C.Bartolozzi,B.Taba,A.Censi,S.Leutenegger,A.Davison,J.Conradt,\nK.Daniilidis,D.Scaramuzza,“Event-BasedVision: ASurvey,“inIEEETransactionsonPatternAnalysisand\nMachineIntelligence,vol.44,no.1,2022,pp.154–180\n[12] E.Ceolini,C.Frenkel,S.Shrestha,G.Taverni,L.Khacef,M.Payvand,E.Donati,“Hand-GestureRecognition\nBasedonEMGandEvent-BasedCameraSensorFusion:ABenchmarkinNeuromorphicComputing,“inFrontiers\ninNeuroscience,vol.14,2020\n[13] R.Verschae,I.Bugueno-Cordova,“Event-BasedGestureandFacialExpressionRecognition: AComparative\nAnalysis,“inIEEEAccess,vol.11,2023,pp.121269–121283\n[14] J.Kaiser,H.Mostafa,E.Neftci,“SynapticPlasticityDynamicsforDeepContinuousLocalLearning(DECOLLE),“\ninFrontiersinNeuroscience,vol.14,2020\n[15] G.Chen,L.Hong,J.Dong,P.Liu,J.Conradt,A.Knoll,“EDDD:Event-BasedDrowsinessDrivingDetection\nThroughFacialMotionAnalysisWithNeuromorphicVisionSensor,“inIEEESensorsJournal,2020,vol.20,no.\n11,pp.6170–6181\n15\nAPREPRINT-NOVEMBER20,2024\n[16] R.Baldwin,R.Ruixu,M.Almatrafi,V.Asari,K.Hirakawa,“Time-OrderedRecentEvent(TORE)Volumesfor\nEventCameras,“inIEEETransactionsonPatternAnalysisandMachineIntelligence,vol.45,no.2,2023,pp.\n2519–2532\n[17] N.Messikommer,D.Gehrig,A.Loquercio,D.Scaramuzza,“Event-BasedAsynchronousSparseConvolutional\nNetworks,”inComputerVision–ECCV2020,2020,pp.415–431.\n[18] D. Gehrig, A. Loquercio, K. Derpanis, and D. Scaramuzza, “End-to-End Learning of Representations for\nAsynchronousEvent-BasedData,”in2019IEEE/CVFInternationalConferenceonComputerVision(ICCV),\n2019,pp.5632–5642.\n[19] M.Davies,N.Srinivasa,T.Lin,G.Chinya,Y.Cao,S.Choday,G.Dimou,P.Joshi,N.Imam,S.Jain,Y.Liao,C.\nLin,A.Lines,R.Liu,D.Mathaikutty,S.McCoy,A.Paul,J.Tse,G.Venkataramanan,Y.Weng,A.Wild,Y.Yang,\nH.Wang,“Loihi: ANeuromorphicManycoreProcessorwithOn-ChipLearning,“inIEEEMicro,vol.38,no.1,\n2018,pp.82–99\n[20] A.Amir,B.Taba,D.Berg,T.Melano,J.McKinstry,C.DiNolfo,T.Nayak,A.Andreopoulos,G.Garreau,M.\nMendoza,“Alowpower,fullyevent-basedgesturerecognitionsystem,“inProceedingsoftheIEEEconferenceon\ncomputervisionandpatternrecognition,2017,pp.7243–7252\n[21] A.Maqueda,A.Loquercio,G.Gallego,N.Garcia,D.Scaramuzza,“Event-BasedVisionMeetsDeepLearning\nonSteeringPredictionforSelf-DrivingCars,“in2018IEEE/CVFConferenceonComputerVisionandPattern\nRecognition(CVPR),2018,pp.5419–5427\n[22] J. Li, S. Dong, Z. Yu, Y. Tian, T. Huang, “Event-Based Vision Enhanced: A Joint Detection Framework in\nAutonomous Driving,“ in 2019 IEEE International Conference on Multimedia and Expo (ICME), 2019, pp.\n1396–1401\n[23] S.Kim,S.Park,B.Na,S.Yoon,“Spiking-YOLO:SpikingNeuralNetworkforEnergy-EfficientObjectDetection,“\ninProceedingsoftheAAAIConferenceonArtificialIntelligence,vol.34,2020,pp.11270–11277\n[24] Q.Su,Y.Chou,Y.Hu,J.Li,S.Mei,Z.Zhang,G.Li,“DeepDirectly-TrainedSpikingNeuralNetworksforObject\nDetection,“inIEEE/CVFInternationalConferenceonComputerVision(ICCV),2023,pp.6532–6542\n[25] B.MohamedSadek,C.Dalila,C.Elisabetta,K.Lyes,“Impactofspikingneuronsleakagesandnetworkrecurrences\nonevent-basedspatio-temporalpatternrecognition“inFrontiersinNeuroscienceVolume17,2023\n[26] A.Viale,A.Marchisio,M.Martina,G.Masera,M.Shafique,“CarSNN:AnEfficientSpikingNeuralNetworkfor\nEvent-BasedAutonomousCarsontheLoihiNeuromorphicResearchProcessor,“inInternationalJointConference\nonNeuralNetworks(IJCNN),2021,pp.1–10\n[27] M. Davies, A. Wild, G. Orchard, Y. Sandamirskaya, G. Guerra, P. Joshi, P. Plank, S. Risbud, “Advancing\nNeuromorphicComputingWithLoihi: ASurveyofResultsandOutlook,“inProceedingsoftheIEEE,vol.109,\nno.5,2021,pp.911–934\n[28] S.Liu,Y.Liang,Y.Yi,“DNN-SNNCo-LearningforSustainableSymbolDetectionin5GSystemsonLoihiChip,“\ninIEEETransactionsonSustainableComputing,vol.9,no.2,2024,pp.170–181\n[29] G.Orchard,E.Frady,D.Rubin,S.Sanborn,S.Shrestha,F.Sommer,M.Davies,“EfficientNeuromorphicSignal\nProcessingwithLoihi2,“in2021IEEEWorkshoponSignalProcessingSystems(SiPS),2021,pp.254–259\n[30] S.Shrestha,J.Timcheck,P.Frady,L.Campos-Macias,M.Davies,“EfficientVideoandAudioProcessingwith\nLoihi2,“inICASSP2024-2024IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing\n(ICASSP),2024,pp.13481–13485\n[31] G. Parpart, S. Risbud, G. Kenyon, Y. Watkins, “Implementing and Benchmarking the Locally Competitive\nAlgorithm on the Loihi 2 Neuromorphic Processor“ in Proceedings of the 2023 International Conference on\nNeuromorphicSystems(ICONS),2023\n[32] P.Panda,K.Roy,Kaushik,“UnsupervisedregenerativelearningofhierarchicalfeaturesinSpikingDeepNetworks\nforobjectrecognition,“inInternationalJointConferenceonNeuralNetworks(IJCNN),2016,pp.299–306\n[33] J.Lee,T.Delbruck,M.Pfeiffer,“Trainingdeepspikingneuralnetworksusingbackpropagation,“inFrontiersin\nneuroscience,vol.10,2016\n[34] F. Zenke, S. Ganguli, “Superspike: Supervised learning in multilayer spiking neural networks,“ in Neural\ncomputation,vol.30,no.6,2018,pp.1514–1541\n[35] Y.Xing,G.DiCaterina,andJ.Soraghan,“ANewSpikingConvolutionalRecurrentNeuralNetwork(SCRNN)\nWithApplicationstoEvent-BasedHandGestureRecognition,”FrontiersinNeuroscience,vol.14,2020.\n[36] D.Forsyth,J.Mundy,V.diGesú,R.Cipolla,Y.LeCun,P.Haffner,L.Bottou,Y.Bengio,“Objectrecognition\nwithgradient-basedlearning,“inShape,contourandgroupingincomputervision,1999,pp.319–345\n16\nAPREPRINT-NOVEMBER20,2024\n[37] Y.Hu,H.Liu,M.Pfeiffer,T.Delbruck,“DVSbenchmarkdatasetsforobjecttracking,actionrecognition,and\nobjectrecognition,“inFrontiersinneuroscience,vol.10,2016\n[38] T.Serrano-Gotarredona,B.Linares-Barranco,“Poker-DVSandMNIST-DVS.Theirhistory,howtheyweremade,\nandotherdetails,“inFrontiersinneuroscience,vol.9,2015\n[39] G. Orchard, A. Jayawant, G. Cohen, N. Thakor, “Converting static image datasets to spiking neuromorphic\ndatasetsusingsaccades,“,inFrontiersinneuroscience,vol.9,2015\n[40] R.Berner,C.Brandli,M.Yang,S.Liu,T.Delbruck,“A240×18010mw12uslatencysparse-outputvisionsensor\nformobileapplications,“inSymposiumonVLSICircuits,2013,pp.C186–C187\n[41] C.Tan,S.Lallee,G.Orchard,“Benchmarkingneuromorphicvision: lessonslearntfromcomputervision,“in\nFrontiersinneuroscience,vol.9,2015\n[42] I.Ratkovic´,N.Bežanic´,O.Ünsal,A.Cristal,V.Milutinovic´,“ChapterOne-AnOverviewofArchitecture-Level\nPower-andEnergy-EfficientDesignTechniques“inAdvancesinComputersVolume98,2015,pp.1–57,\n17",
    "pdf_filename": "N-DriverMotion_Driver_motion_learning_and_prediction_using_an_event-based_camera_and_directly_traine.pdf"
}