{
    "title": "N-DriverMotion Driver motion learning and prediction using an event-based camera and directly traine",
    "context": "Driver motion recognition is a key factor in ensuring the safety of driving systems. This paper presents a novel system for learning and predicting driver motions, along with an event-based (720x720) dataset, N-DriverMotion, newly collected to train a neuromorphic vision system. The system includes an event-based camera that generates a driver motion dataset representing spike inputs and efficient spiking neural networks (SNNs) that are effective in training and predicting the driver’s gestures. The event dataset consists of 13 driver motion categories classified by direction (front, side), illumination (bright, moderate, dark), and participant. A novel optimized four-layer convolutional spiking neural network (CSNN) was trained directly without any time-consuming preprocessing. This enables efficient adaptation to energy- and resource-constrained on-device SNNs for real-time inference on high-resolution event-based streams. Compared to recent gesture recognition systems adopting neural networks for vision processing, the proposed neuromorphic vision system achieves competitive accuracy of 94.04% in a 13-class classification task, and 97.24% in an unexpected abnormal driver motion classification task with the CSNN architecture. Additionally, when deployed to Intel Loihi 2 neuromorphic chips, the energy-delay product (EDP) of the model achieved 20,721 times more efficient than that of a non-edge GPU, and 541 times more efficient than edge-purpose GPU. Our proposed CSNN and the dataset can be used to develop safer and more efficient driver-monitoring systems for autonomous vehicles or edge devices requiring an efficient neural network architecture. 1 With the advancement of artificial intelligence (AI), it is being applied across various industrial fields, and among these, vehicle AI systems are emerging as one of the most prominent applications. Onboard AIs in vehicles are used to assist autonomous driving and the safety of drivers and pedestrians by integrating with the control system [1]. In particular, the EU and the United States have introduced mandatory requirements for various driver assistance systems to ensure safe road traffic by regulations on general automotive safety [2]. The primary objective of this regulation is to enhance the protection of elderly drivers, vehicle occupants, pedestrians, and cyclists. Given that research shows human error is the cause of 95% of accidents, implementing this regulation is projected to save more than 25,000 lives and prevent at arXiv:2408.13379v2  [cs.CV]  18 Nov 2024",
    "body": "N-DRIVERMOTION: DRIVER MOTION LEARNING AND\nPREDICTION USING AN EVENT-BASED CAMERA AND DIRECTLY\nTRAINED SPIKING NEURAL NETWORKS\nHyo Jong Chung\nCollege of Engineering and Applied Sciences\nStony Brook University\n100 Nicolls Road, New York, USA\nhyojong.chung@stonybrook.edu\nByungkon Kang\nDepartment of Computer Science\nThe State University of New York Korea\n119-2 Songdo Munhwa-ro, Yeonsu-gu, Incheon, Korea\nbyungkon.kang@sunykorea.ac.kr\nYoonseok Yang\nDepartment of Computer Science\nThe State University of New York Korea\n119-2 Songdo Munhwa-ro, Yeonsu-gu, Incheon, Korea\nyoonseok.yang@sunykorea.ac.kr\nNovember 20, 2024\nABSTRACT\nDriver motion recognition is a key factor in ensuring the safety of driving systems. This paper presents\na novel system for learning and predicting driver motions, along with an event-based (720x720)\ndataset, N-DriverMotion, newly collected to train a neuromorphic vision system. The system includes\nan event-based camera that generates a driver motion dataset representing spike inputs and efficient\nspiking neural networks (SNNs) that are effective in training and predicting the driver’s gestures. The\nevent dataset consists of 13 driver motion categories classified by direction (front, side), illumination\n(bright, moderate, dark), and participant. A novel optimized four-layer convolutional spiking neural\nnetwork (CSNN) was trained directly without any time-consuming preprocessing. This enables\nefficient adaptation to energy- and resource-constrained on-device SNNs for real-time inference\non high-resolution event-based streams. Compared to recent gesture recognition systems adopting\nneural networks for vision processing, the proposed neuromorphic vision system achieves competitive\naccuracy of 94.04% in a 13-class classification task, and 97.24% in an unexpected abnormal driver\nmotion classification task with the CSNN architecture. Additionally, when deployed to Intel Loihi\n2 neuromorphic chips, the energy-delay product (EDP) of the model achieved 20,721 times more\nefficient than that of a non-edge GPU, and 541 times more efficient than edge-purpose GPU. Our\nproposed CSNN and the dataset can be used to develop safer and more efficient driver-monitoring\nsystems for autonomous vehicles or edge devices requiring an efficient neural network architecture.\n1\nIntroduction\nWith the advancement of artificial intelligence (AI), it is being applied across various industrial fields, and among these,\nvehicle AI systems are emerging as one of the most prominent applications. Onboard AIs in vehicles are used to assist\nautonomous driving and the safety of drivers and pedestrians by integrating with the control system [1]. In particular,\nthe EU and the United States have introduced mandatory requirements for various driver assistance systems to ensure\nsafe road traffic by regulations on general automotive safety [2]. The primary objective of this regulation is to enhance\nthe protection of elderly drivers, vehicle occupants, pedestrians, and cyclists. Given that research shows human error is\nthe cause of 95% of accidents, implementing this regulation is projected to save more than 25,000 lives and prevent at\narXiv:2408.13379v2  [cs.CV]  18 Nov 2024\n\nA PREPRINT - NOVEMBER 20, 2024\nleast 140,000 serious injuries by 2038. This has led to an urgent need to research and develop driver assistance systems\n(ADAS) using AI.\nConventional AI vision systems for autonomous vehicles utilize AI platforms for training with large datasets, after\nwhich the learned information is transferred to individual vehicles. These systems are typically implemented using\nGPU or AI accelerator-based hardware for data training and inference, which result in high power consumption, slow\nresponse times, and challenges in real-time prediction [3]. The exponential increase in neural network computations\nrequired for massive training datasets in autonomous driving has exacerbated these limitations. To address these\nchallenges, a new generation of low-power, high-performance, and highly efficient AI vision systems is expected to\nbecome central to AI implementation in autonomous driving.\nNeuromorphic AI vision systems, in particular, represent a way to support AI technology for autonomous vehicles that\nis gaining increasing significance [4]. Event cameras, also called neuromorphic cameras, and neuromorphic processors,\nwhich mimic the functionality of the human eye and brain, offer ultra-low power consumption, high efficiency, and\nrapid responsiveness. Consequently, these technologies are actively applied in various applications in autonomous\nvehicles, where they serve as core components for the sensory and processing needs [5, 6, 7].\nIn response to these demands, we propose N-DriverMotion, an event-based dataset and vision system for neuromorphic\nlearning and predicting driver motions on an efficient convolutional spiking neural network (CSNN). This driver motion\nrecognition research, comprising a neuromorphic framework for efficient CSNN configuration in terms of energy and\nlatency, implements driver safety assistance by incorporating a high-resolution event-based camera. The contributions\nof the proposed driver motion recognition system include the following:\n• We create an event-based dataset, N-DriverMotion (720 * 720 resolution), for large-scale driver motion\nrecognition using an event-based camera: To the best of our knowledge, this is the first study for driver\nmotion recognition using a high-resolution event camera and spiking neural networks. The event-based dataset\npresents 13 driver motion categories classified by direction (front, side), illumination (bright, moderate, dark),\nand participant.\n• We present a novel simplified four-layer convolutional spiking neural network (CSNN), directly trained with\nthe event dataset without any time-consuming preprocessing. This enables efficient adaptation to on-device\nspiking neural networks (SNNs) for real-time inference over event-based camera streams. Furthermore, the\nproposed neuromorphic vision system achieves competitive accuracy of 94.04% in 13 class classification task,\nand 97.24% in unexpected abnormal driver motion classification task with the CSNN architecture, developing\nsafer and more efficient driver monitoring systems for autonomous vehicles or edge devices requiring a\nlow-power and efficient neural network architecture.\n• We design an efficient CSNN for driver motion recognition for one of the widely used practical neuromorphic\nframeworks, the Intel Lava neuromorphic framework. We also propose a version of our CSNN for the Loihi 2\nprocessor [8, 9]. We observed that the energy-delay product (EDP) of the proposed model on Loihi 2 exhibited\n20,721 times more efficient than that of a non-edge GPU and 541 times more efficient than that of an edge\npurpose GPU, which is considered to be one of the most important aspects of edge-device AI.\nWe designed an optimized convolutional spiking neural network (CSNN) for efficient energy use and deployment on on-\ndevice applications and systems like Loihi 2. To train the proposed CSNN, we used a surrogate gradient descent-based\nbackpropagation method, SLAYER [10], for direct training with our N-DriverMotion dataset. Additionally, instead of\nresizing the high-resolution event frames directly, we added a pooling layer to enable adaptive deployment on GPUs\nand the Loihi 2 system, depending on memory requirements.\nThe remainder of this paper is organized as follows: Section 2 briefly reviews related work, while Section 3 describes the\nproposed driver motion recognition system. Section 4 presents the implementation results and performance evaluations\nand Section 4.7 concludes this paper.\n2\nRELATED WORK\nEvent-based cameras have advantages over frame-based cameras in properties such as high dynamic range, high temporal\nresolution, low latency, and low power consumption due to their sparse event streams when intensity changes [11].\nTherefore, they have been increasingly adopted in machine learning vision systems for gesture recognition and learning.\nHand-gesture recognition [12], gesture and facial expression recognition [13], DECOLLE [14] for deep continuous\nlocal learning, EDDD for event-based drowsiness driving detection [15], TORE [16] for time-ordered recent events,\nevent-based asynchronous sparse convolutional networks [17], and EST [18] for end-to-end learning of representations\nfor asynchronous event-based data have been developed on deep learning-based neural networks (DNNs) and event\n2\n\nA PREPRINT - NOVEMBER 20, 2024\nIN\nSynapse\nSynapse Mapping (4,096)\nSynapse Memory (4,096)\nDendrite\nSOMA State (1,024)\nAxon\nAxon Mapping (1,024)\nAxon Memory (4,096)\nOUT\nLearning\nLearning Expression\nPost Trace (1,024)\n: Update (Learning)\n: Normal Flow\naxon\naxon\nNeuron Group\nConfiguration & State\n(1,024)\nDendrite Accumulator (8,192)\nFigure 1: Simplified structure of Loihi which illustrates how neurocore is structured and operates.\nrepresentations, exploiting the sparsity and temporal dispersion of event-based gestures. Riccardo et al. [6] trained a\ndeep neural network on a preprocessed DVS hand gesture dataset for recognizing hand gestures on a Loihi neuromorphic\nprocessor. The trained DNN was converted into the spike domain for deployment on Intel Loihi [19] for real-time\ngesture recognition.\nUnlike DNN-based approaches, which require converting event-based datasets into static patterns for processing, recent\nresearch has shifted toward direct training and learning on SNNs using event-based datasets. This is because events are\ninherently suited for SNNs operating in continuous time. Moreover, SNNs directly exploit the temporal information of\nevents for energy-efficient computation. SLAYER proposes an error backpropagation mechanism for offline training of\nSNNs. It receives events and leverages the backpropagation method to train synaptic weights and axonal delays directly.\nAmir et al. [20] proposed an end-to-end event-based gesture recognition system using an event-based camera and a\nTrueNorth processor configured with a convolutional neural network (CNN). It recognized hand gestures in real time\nfrom gesture event streams captured by a Dynamic Vision Sensor (DVS).\nEvent-based vision systems have been widely used in various applications, including vision systems for autonomous\ndriving [21, 22] and object detection [23, 24]. These applications exploit event-based cameras to capture the spatial and\ntemporal event data of driving and objects.\nLoihi 2 is the second generation of Intel neuromorphic chip designed for energy-efficient AI applications [9]. Loihi chip\napplies Current Based Leaky-Integrate and Fire neuron (CUBA LIF), as its main neuron model. The overall behavior of\nCUBA LIF model is close to that of LIF, but is more biologically plausible as CUBA LIF model includes the temporal\ndynamics of the postsynaptic input current [25]. A single Loihi chip consists of 128 neurocores, which are groups of\nneurons that function according to the compartments. Each neurocore is able to communicate with other neurocores\nthrough the asynchronous Network-on-Chip (NoC) where the messages are packetized for sending and receiving.\nAlthough a single chip provides only 128 neurocores, Loihi is capable of transporting information without increasing\nlatency for message exchange between multiple chips. The simplified model of Loihi is shown in Figure 1, which\nillustrates the asynchronous neuron model mechanism, along with the implementation of online learning or a continual\nlearning mechanism. The Loihi chip has been used with the Lava software framework to support the development of\nagile neuromorphic software and applications [27, 26, 28, 29, 30, 31].\n3\n\nA PREPRINT - NOVEMBER 20, 2024\nWhile event-based systems have shown promise in previous research for autonomous driving and gesture recognition,\nthey still face challenges in achieving real-time performance with low power consumption in highly dynamic envi-\nronments. Additionally, existing systems may struggle to handle the diverse and unpredictable nature of recognition\nscenarios, particularly when dealing with rapid motions or changing lighting conditions, which can generate massive\nevent streams. Our proposed system with N-DriverMotion addresses these challenges by leveraging the energy-efficient\nCSNN architecture on Loihi 2. This enables fast, real-time processing of large-scale event data while maintaining low\npower consumption. Furthermore, the system ensures that event data from event cameras is processed promptly without\nbottlenecks, which is important for safety-critical applications like autonomous driving.\n3\nPROPOSED EVENT-BASED DRIVER MOTION RECOGNITION SYSTEM\nIn this section, we introduce efficient CSNN architecture and a driver motion learning and prediction system employing\na direct training mechanism and event-based data streams. As event-based data differs from static images in that it\nincludes time to represent events, we adopt a direct spiking training method and 3D spike convolution operation to\nbuild an efficient CSNN model.\nFor the direct training of spiking neural networks, we exploited a gradient-based training method developed in\nSLAYER [10]. It is generally known that the characteristic of discrete spike events hinders the differentiation in SNN.\nTo resolve such problems, various attempts have been made to propose the approximation for the derivative of the\nspike function [32, 33, 34]. However, none of the following have considered the temporal dispersion between spikes\nto resolve the problem. A differentiable approximation (i.e., surrogate gradient) of the spike function is introduced\nwith the probability of a change in the spiking state. It is explained that the derivative of the spike function represents\nthe Probability Density Function (PDF) for the change of state of a spiking neuron. With an expectation value of the\nderivative of the spike function, estimated backpropagation error, and gradient terms for weight and delay in layer l, it\nis possible to get the derivatives of the total loss E as\ne(l) =\n( ∂L(t)\n∂a(nl)\nif l = nl\n\u0000W (l)\u0001T δ(l+1)(t)\notherwise\n(1)\nδ(l)(t) = ρ(l)(t) · (εd ⊙e(l))(t)\n(2)\n∇W (l)E =\nZ T\n0\nδ(l+1)(t) ·\n\u0010\na(l)(t)\n\u0011T\ndt\n(3)\n∇d(l)E = −\nZ T\n0\n˙a(l)(t) · e(l)(t) dt\n(4)\nwhere ρ(l)(t) denotes the probability density function in layer l at certain time t, ∆ξ as the random perturbation, W (l)\nis the weight vector for layer l, a(l) being the spike response signal, L(t) being the loss at a certain time t, d(l) being\nthe axonal delay, εd being the spike response kernel, and ⊙being the element-wise correlation operation in time. This\nprovides effective distribution of the errors back through layers of a neural network as in DNNs. It takes account of the\nerrors in the previous timeline, a crucial factor to be considered as spiking neuron’s states relied on the previous states.\nIn our model, the event-based video streams are defined as a 3D tensor with the shape of (u, v, t) where u and v denote\nthe coordinate of the width and height of layer l, and t stands as the timestamp [35]. By configuring the optimal\ntemporal frequency, users can control t from the original segment.\nA convolutional neural network (CNN) is a feed-forward network composed of multiple layers where filters convolve\naround the input or single layer for neurons to collect meaningful features or patterns [36]. However, as the event-based\nvideo has not only spatial information but also time, the sampled input sequence S(n) would require a 3D convolutional\nkernel for building a spiking neuronal feature map as shown in Figure 2. Each spike inside the kernel represents a\ncluster of spike trains su,v(t) where the spike’s coordinate is (u, v), and is in range of the temporal resolution window t.\nSpikes from the kernel continually get cumulated to the neurons in the feature map, and when the spikes in the region\nof the kernel exceed a certain threshold, it would generate the membrane potential for a single neuron in the feature\nmap, creating spatio-temporal dynamic patterns.\nThe 3D spiking convolution operation is performed by convolving the spike trains in the kernel with a spike response\nkernel and applying the threshold function. Each spike train is converted into a spike response signal, then subsequently\ntransformed into the membrane potential by integrating the signal with the refractory response of the neuron:\n4\n\nA PREPRINT - NOVEMBER 20, 2024\nFigure 2: The 3D convolutional spiking operation.\nau,v(t) = Su,v(t) ∗εd(t)\n(5)\nuj,k(t) =\nK\nX\nm=1\nK\nX\nn=1\nWm,nam+(j−1),n+(k−1)(t) + (Sj,k(t) ∗ν(t))\n(6)\nSj,k(t) = 1 and uj,k(t) = 0\nwhen\nuj,k(t) ≥Vthr\n(7)\nwhere ∗denotes the convolution operator, Wm,n denotes the synaptic weights at (m, n) of the kernel, uj,k represent\nthe membrane potential at coordinate (j, k) presented by the feature map, K represents the width and height of the\nconvolution kernel, and ν(t) is the refractory kernel.\nOur proposed model is developed based on existing models [20, 10] for DVS Gesture recognition, which implements\nSNN training and a 3D spike convolution operation. The SLAYER and TrueNorth models consist of 8 layers and\n16 layers, respectively. Figure 3 presents our proposed network model and system, where the network is simplified\nand comprises only 4 layers. When tested with a more complex model, we observed that it not only occupied more\nmemory, but also had a detrimental impact on overall performance, as more layers hindered the neurons from firing\nspikes, leading to lower event rate because spikes could not be passed to consecutive neuron layers.\nThe 720x720 resolution event streams for driver motion recognition were recorded by an event-based camera for\n3 seconds and delivered to the first 3D pooling layer. Unlike conventional convolutional neural networks, where\nconvolution layers are processed first, our model implements a pooling layer before passing the data to the convolution\nlayer. The main reason for this is to down-sample feature maps to resolve large memory usage, as well as to extract\nfeatures. Compared to classical pooling layers, SNN pooling layers are considered trainable layer, as they not only\ndown-sample the event streams but also take into account temporal aspects, enabling the learning of spatio-temporal\nfeatures from the input. The pooling layer uses a kernel of size 8x8 and passes data to the convolution layer, which\nuses a 5x5 kernel to produce 16 channels of features. The features are then fed to 2 final fully-connected layers, which\nprovide 13 outputs representing the spike rates for the given event data.\nWe notice that the existing spiking neural networks designed for other applications are not optimized for autonomous\ndriving, which requires low power consumption, real-time inference, and efficient deployment on actual neuromorphic\nchips. To effectively implement driver motion recognition for autonomous driving, it is important to have a neural\nnetwork that is optimized for neuromorphic hardware (simple but without performance degradation) as well as an\nevent-based dataset for training such networks. We addressed these elements by designing, implementing, and testing a\nnetwork that takes these features into consideration.\n5\n\nA PREPRINT - NOVEMBER 20, 2024\nEvent camera\nDriver’s gesture stream\n(1280x720, 3 seconds)\n3D Max Polling\n(90x90x2)\n3D Convolutional\n(90x90x16)\n…\nFully-connected\n(512)\n…\nOutput\n(13)\nEvent stream\n3 seconds\n0 (Head tilts to side)\n1 (Head tilts forward)\n11 (Looks back)\n12 (Bends down)\nEvents\nFlatten\n…\n…\nFigure 3: Proposed Event-based Driver Gesture Recognition System.\n4\nEXPERIMENTS AND RESULTS\n4.1\nEVENT-BASED CAMERA AND DATA CONVERSION\nProphesee’s Metavision EVK41 camera is one of the latest event-based vision sensors that supports up to 1280x720\npixel resolution. Generally, when a change in pixel values exceeding a certain threshold defined by the user occurs, the\nevent-based camera asynchronously detects such changes in brightness and generates events specific to that pixel [11].\nEach event contains pixel information describing the position in the x and y coordinates, indicating a motion’s change,\nand a timestamp for the occurrence of the event. The device offers a high dynamic range (86 dB, however, it can reach\nover 120 dB based on low light cutoff measurement being: 0.08 lux) and a typical and maximum event rate of 1.06\ngiga-events per second (Geps).\nIn the experiment, in order to allow direct usage of the spike data obtained from the high-resolution event-based camera\nwithout requiring additional resources, we minimized modification to the extractor so that it can retrieve coordinate,\npolarity, origin (a newly implemented feature in the EVK4), and timestamp information. Moreover, we observed that\nthe most meaningful motion information was contained in the central 720x720 pixels so that the size of the input events\nwas cropped to 720x720 pixels.\n4.2\nDATASET\nOver the past years, a profusion of gesture datasets captured with simple frame-based sensors has been presented.\nHowever, to stimulate the improvement of event-based computer vision, Hu et al. [37] strongly support the importance\nof the DVS dataset. Serrano et al. [38] introduced the first labeled event-based neuromorphic vision sensor dataset\nwhich originated from the classical MNIST digit recognition dataset by moving the images along certain directions\nwithin the screen. This dataset was further developed by Orchard et al. [39], who removed several frame artifacts using\na pan-tilt unit. It can be observed that generating artificial movement for static images enabled the production of event\n1EVK4 camera is an HD Event-Based Vision evaluation kit mounted with IMX636 sensor, developed by Sony and Prophesee.\nPlease refer to https://www.prophesee.ai/event-camera-evk4/\n6\n\nA PREPRINT - NOVEMBER 20, 2024\noutputs, which became important for object recognition in neuromorphic systems such as spiking neural networks [7].\nHowever, it has been observed that static image recognition with event-based vision sensors is ineffective, as their main\npurpose is focused on dynamic scenes. Recognizing these drawbacks, some datasets consisting of dynamic scenes have\nbeen presented. Berner et al. [40] introduced novel datasets that converted the existing visual video benchmarks for\nobject tracking and action/object recognition into spiking neuromorphic datasets using the DAVIS camera’s output.\n(0a)\n(0b)\n(1)\n(2)\n(3)\n(4a)\n(4b)\n(8)\n(7b)\n(9)\n(5b)\n(5a)\n(6a)\n(6b)\n(7a)\n(12b)\n(12a)\n(11b)\n(11a)\n(10)\nFigure 4: Sample shots taken for each driver gesture.\nTable 1: Labels of the corresponding driver gestures\nLabel\nGesture\nFront\n0\nHead tilts to side (a: left, b: right)\n1\nHead tilts forward\n2\nHead tilts backward\n3\nNormal driving\n4\nPhone call (a: right, b: left)\n5\nLooks back (a: left, b: right)\n6\nBends down (a: left, b: right)\nSide\n7\nHead tilts (a: left, b: right)\n8\nHead tilts forward\n9\nHead tilts backward\n10\nPhone call\n11\nLooks back (a: left, b: right)\n12\nBends down (a: left, b: right)\nEven though these datasets show dynamic movements in a spiking neuromorphic way, according to Tan et al. [41], a\nDVS camera produces microsecond temporal resolution output. In contrast, datasets produced by conventional vision\ntools such as video recorders or color cameras have tens of milliseconds of temporal resolutions, which causes a loss of\nhigh temporal frequency during conversion. Moreover, they add that there is a high chance of subsidiary unwanted\nartifacts being present during conversion. To address these problems, Amir et al. [20] present the DvsGesture dataset,\nwhich directly captured scenes with the DVS128 camera. Additionally, the DvsGesture datasets were captured under\ndifferent lighting conditions, as DVS sensors are less affected by brightness and introduce some meaningful noise.\nHowever, the DvsGesture dataset is mainly utilized for simple classification tasks, which may be considered less\npractical.\nThe N-DriverMotion dataset comprises 1,239 instances of a set of 13 driver motions (1 normal driving motion and 12\ndriving motions that are considered possibly dangerous) as shown in Figure 4 and Table 1. These motions include head\nfalling (front, sides, and back), holding a cellphone (right/left hand), looking back (to the right/left side), the body going\ndown (to the right/left side), and all the same motions above in side-captured manner. The dataset was collected from 23\nsubjects under three different lighting conditions. All subjects were within a specific age range; they had no disabilities,\n7\n\nA PREPRINT - NOVEMBER 20, 2024\nSeizures during driving\nCollisions where the\ndriver hits the windshield\nSudden standing\nAbnormal camera angles\nHand-swipe gestures\nFigure 5: Sample shots taken of abnormal driver gestures. Note that hand-swipe gestures are not simple hand-waving\nactions but are actions to protect oneself from various attacks\nTable 2: Abnormal driver gestures\nSeizures during driving\nSudden standing\nCollisions where the driver hits the windshield\nHand-swipe gestures\nAbnormal camera angles\nor any other problems that could affect driving. Each subject performed all 13 motions in a single recording, where\neach motion was recorded for around 3 seconds under the same brightness condition. The three brightness conditions\nwere classified as \"bright,\" \"moderate,\" and \"dark\" to test how much brightness would affect the recordings and to\nexamine the robustness of event data in harsh illumination condition. We note that informed consent was obtained from\nall participants during data collection. To evaluate the classifier’s performance, we randomly selected 992 motions for\nthe training set, and the remaining 247 motions were designated as the test set.\nDue to limitations in recruiting participants, we were unable to include experiments involving drivers with medical\nconditions or disabilities. This will be expanded in future work. Instead, to address unpredictable scenarios, we\nintroduced five new types of event data that were not used during training. These additional test cases include abnormal\nreactions such as seizures during driving, sudden standing or rising movements, collisions where the driver hits the\nwindshield, hand-swipe gestures, and abnormal camera angles caused by accidents or issues with the camera’s fixed\nposition. These newly created event-based video sequences were specifically designed to test how the system responds\nto other abnormal cases not seen during training.\n4.3\nEXPERIMENTAL SETUP AND TOOL FLOW\nThe experiment is mainly consisted of 3 sub-parts to measure different performance of our proposed model: the accuracy\nof the 13-class driver motion classification, the accuracy of classifying unexpected abnormal driver motion versus\nnormal driver motion, and the energy efficiency with respect to throughput in various AI accelerators.\n4.3.1\nMultiple driver motion classification\nThe aim of this experiment lies in observing how accurately our proposed system can classify various driver motions\n(13 classes in total). With the model developed based on the Lava-DL API, our proposed network will be loaded onto\nthe RTX 3080 GPU and trained for 200 epochs. During each epoch in training, the models would be evaluated with\nthe test set and produce accuracy based on the spike rates of the output spikes. If the model produces the highest test\naccuracy, the model’s weights are stored as a PyTorch state dictionary file. Further explanation about the configuration\nof the system is shown in subsection 4.4.\n8\n\nA PREPRINT - NOVEMBER 20, 2024\n4.3.2\nInference on unseen abnormal driving motions\nWhile it is important to see how the model performs classifications based on the provided 13 motion sets, it is also\nimportant to check the model’s capability in classifying unexpected motion sets. Here, we make inferences on a\nseparate dataset that is composed of abnormal driver motions not included in the 13 classes, as well as normal motions,\nwhere the model has been trained to classify them as label 3. In this experiment, we have set the objective as binary\nclassification: abnormal driver motion and normal driving motion. Given such abnormal driver motions, if the model\nproduces predictions other than label 3 (normal driving motion), we can observe that the model is capable of classifying\nthem as abnormal motion, and vice versa. We have used the fully trained CSNN model with the same configuration for\nevent data (e.g., sampling time) that was used for measuring accuracy on the 13-class classification task. The test data\nfor this task is composed of 82 abnormal driver motions (10 participants performing 5 different unexpected abnormal\ndriving motions, as shown in figure 5 and table 2) along with 63 normal driver motions from the original test set. The\nevaluation process was conducted in the same way as the multiple driver motion classification, except in a binary\nclassification fashion.\n4.3.3\nEnergy efficiency in various AI accelerators\nEnergy efficiency is one of the most promising aspects of SNN, and this experiment is designed to show the overall\nefficiency of the model with respect to 3 AI accelerators: Nvidia RTX 3080, Nvidia Jetson Xavier NX, and Intel Loihi\n2. For both Nvidia GPUs, we loaded the models directly onto the devices and measured various inference costs (latency,\ntotal energy consumption, etc.). Due to fast inference speed of GPUs, we measured by averaging each of the total\ncosts by the number of samples processed to measure the values. For Loihi 2, we designed a specific pipeline that\nenables us to load the model into the system and feed the event data directly to the model, along with the utilization of\nmeasurement tools provided by the Lava API. Detailed information about experimental flows and settings for Jetson\nXavier NX and Loihi 2 is described in subsection 4.5.\n4.4\nNETWORK CONFIGURATION AND TRAINING\nTo enable adequate training, the dataset was converted from binary format into tensors consisting of x-coordinates,\ny-coordinates, polarities, and timestamps. We also set the sampling time to 2.0 seconds, as the core movements were\nmostly within this range. More importantly, we wanted to check whether our proposed model is capable of classifying\ndriver motions in a short amount of time, as in real-time applications, such systems should detect abnormal activities\nrapidly. In this form, the data can now be used as input to the CSNN during runtime.\nAll of the applications were implemented and mapped to GPU/CPU to be executed as the simulation for Loihi 2, using\nthe Lava version 0.4.4 software framework and the Lava-DL library for training. Our CSNN models were developed\nwith neuron models and trained with the event-based dataset using the SLAYER API, which were all provided by the\nLava-DL.\nWe trained multiple CNN configurations to compare accuracy results. Each network was trained for 200 epochs with a\nsingle batch size due to the dataset’s high resolution. We selected CUBA LIF as the base neuron model for the spiking\nCNN models. For optimization, the learning rate was set to 3×10-3 and ADAM was used as the optimizer. For the loss\nfunction, we chose spike rate loss. The network configuration for the 4-layered CSNN and the neuron parameters are\nshown in Table 3\nlayer\nmap size\nfeatures\nkernel\npadding\nN/A\n720 x 720\n2\nN/A\nN/A\n1\nPool\n90 x 90\n2\n8\nN/A\n2\nConv\n90 x 90\n16\n5\n2\n3\nFlatten\n90 x 90\n16\nN/A\nN/A\n4\nDense\n512\nN/A\nN/A\nN/A\n5\nDense\n13\nN/A\nN/A\nN/A\nNeuron Parameters\nValue\nVoltage threshold\n1.25\nCurrent decay\n0.25\nVoltage decay\n0.03\nTau gradient\n0.03\nScale gradient\n3\nTrue rate\n0.2\nFalse rate\n0.03\nTable 3: Network configuration of 4-layered CSNN(left) with neuron parameters(right)\n4.5\nEDGE DEVICE CONFIGURATIONS AND INFERENCE\nIn this section, we provide a detailed description of various AI accelerators that are specialized for use in edge devices.\nWe utilized two different edge AI accelerators: Jetson Xaior NX, and Intel Loihi 2 to compare overall energy efficiency\nwith respect to image processing delays.\n9\n\nA PREPRINT - NOVEMBER 20, 2024\nRingBuffer\n(spatio-temporal buffer)\nOuputProcess\n(WTA classification block)\nInput Adapter\nOutput Adapter\nN-DriverMotion CSNN\nNeurocore\nProfilers\n-\nPower Profiler\n-\nMemory Profiler\n-\nExecution Time Profiler\n-\nActivity Profiler\nClassification Result\nDataLoader\nTrained CSNN\nModel Weights\nEmbedded\nCore\nEmbedded\nCore\nLoihi 2 (Oheo Gulch)\nFigure 6: Pipeline for Inference of CSNN on the Loihi 2 processor. The blocks that are part of the main processes are\ncolored as gray, while the green blocks are not part of the main process.\nTable 4: Network configuration of a 4-layered CSNN for Loihi 2\nlayer\nmap size\nchannels\nkernel\npadding\nInput\n40 x 40\n2\nN/A\nN/A\nConv\n40 x 40\n4\n5\n2\nFlatten\n40 x 40\n4\nN/A\nN/A\nDense\n512\nN/A\nN/A\nN/A\nDense\n13\nN/A\nN/A\nN/A\n4.5.1\nJetson Xavior NX\nThe Jetson Xavior NX is a fully featured development board equipped with a 6-core ARM CPU and a 48 tensor core\nprocessing unit supporting a clock frequency up to 1.1 GHz, along with 16 GB of memory. To minimize power usage,\nwe operated on the standard module, where the GPU’s clock frequency is set to 0.67 GHz. All inference configurations\nwere the same as with the RTX 3080, except for the library version. Due to the restriction of the Python version,\ninference on the Jetson Xavior NX was executed with a lower version of the lava-dl (0.4.0) library.\n4.5.2\nIntel Loihi 2\nA Neuromorphic chip is one of the edge AI accelerators that particularly targets the implementation of SNNs. Among\nvarious neuromorphic chips, we used Loihi 2 to observe the overall power consumption of the benchmark three fully\nconnected layer SNN model, and the proposed CSNN model, as Loihi 2 adopts CUBA LIF as the base neuron model\nbest fitting all of the proposed models [26]. For this experiment, we used the Oheo Gulch, one of the Loihi 2-based\nneuromorphic systems that contains a single-socketed Loihi 2 chip. Due to the limited availability of Loihi 2 chips, along\nwith the nature of large input and model size, some adjustments to the input and models were made. We downsampled\nthe input from 720x720 to 40x40 with max pooling and extracted the motion stream time from total of 3.0 seconds to\n1.7 seconds. For the three fully connected layer model, there were no changes. For the proposed CSNN model, we\nexcluded the CUBA pooling layer since the input had already been downsampled and reduced the number of features\nextracted from 16 to 4. The description of the network model on Loihi 2 is shown in Table 4. We were able to load the\nthree fully connected layer model onto a single Loihi 2 chip, whereas the proposed CSNN model required a total of 4\nLoihi 2 chips to be loaded.\nTo enable inference on Loihi 2, it is necessary to create a pipeline that can process spatio-temporal data for classification\ntasks on Loihi 2. We created a pipeline to conduct inference on Loihi 2, which is shown in Figure 6. The event data\n10\n\nA PREPRINT - NOVEMBER 20, 2024\nwas stored in a buffer block named RingBuffer, where it passed the frame data one at a time to Loihi 2. As the frame\ndata was passed to Loihi 2 the last layer in the model produced spikes that generated the classification result for the\nsingle frame. The spikes were cumulatively stored in the OutputProcess block until every frame in the event data was\nprocessed. The final classification was conducted in a Winner-Takes-All (WTA) fashion, where the class with the most\nspikes was the predicted value.\nThe total description of how the models were converted and converged at the Oheo Gulch neuromorphic hardware is\nshown in Table 5 and Table 6, where each column indicates the percentage of the total usage for the input axons, neuron\ngroup, neurons, synapses, axonal mapping, and axon memory, respectively.\nTable 5: Resource utilization for a 3-fully connected layer model on Loihi 2\nAxonIn\nNeuronGr\nNeurons\nSynapses\nAxonMap\nAxonMem\nTotal\nCores\n3.20%\n12.50%\n0.32%\n12.80%\n0.08%\n0.00%\n12.94%\n1\n3.20%\n12.50%\n1.56%\n73.60%\n0.40%\n0.00%\n62.09%\n8\n25.31%\n12.50%\n0.12%\n50.62%\n0.03%\n0.00%\n60.80%\n103\nTotal\n112\nTable 6: Resource utilization for the proposed CSNN model on Loihi 2. The amount of core utilization for the proposed\nmodel is more than twice of the 3-fully connected layer model due to the convolutional layer acting as the bottleneck of\nthe model\nAxonIn\nNeuronGr\nNeurons\nSynapses\nAxonMap\nAxonMem\nTotal\nCores\n1.60%\n12.50%\n0.32%\n6.40%\n0.08%\n0.00%\n6.54%\n1\n40.00%\n12.50%\n0.02%\n40.00%\n0.01%\n0.00%\n64.02%\n256\n0.01%\n12.50%\n62.52%\n0.11%\n16.00%\n0.00%\n25.70%\n3\nTotal\n260\n4.6\nResults\n4.6.1\nAccuracy on 13-class prediction\nThe results for 13 motion classifications from the training models are shown in Figure 7 and Table 7. In the benchmark\ndataset for evaluating our models, we used the DVS Gesture recognition dataset, comprising 11 hand gesture categories\nfrom 29 subjects under 3 illumination conditions [20]. We tested three different SNN configurations: 1) all layers\ncomposed of dense connection, 2) a convolutional spiking layer with 2 fully connected layers, with the pooling kernel\nsize set to 8, and 3) the same model as 2, except with the pooling kernel size set to 18. For all models, we selectively\nused only the first 2.0 seconds out of 3.0 seconds of motion streams for each class to classify the actions. Additionally,\nthe temporal resolution was set to 5 milliseconds to increase training efficiency. For each experiment, we recorded\nthe best loss and accuracy of the SNNs for both training and inference steps, as well as the confusion matrix for\nthe corresponding inference results. The confusion matrix in Figure 7 shows that the normal situation (label 3) is\ndistinguished from the hazardous situations. Figure 8 illustrates the classification results with output spikes captured on\nthe output layer, classifying 13 driver motions.\nThe system from the proposed CSNN model with 4 layers showed the highest accuracy, reaching 94.04% in the test.\nThe inclusion of the pooling and convolution layers improved accuracy performance by almost 8% over the 3-layered\nmodel, which was composed only of fully connected layers. Even though our model is simplified with restrictions\napplied to resolve the out-of-memory problem caused by the high-resolution event data for training and testing, it was\nDataset\nMethod\nArchitecture\nAccuracy\nTrueNorth\nSNN (16 layers)\n91.77% (94.59%)\nDVS Gesture\nSLAYER\nSNN (8 layers)\n93.64 ± 0.49%\nSLAYER\nProposed CSNN (4 layers)\n92.80%\nSLAYER\nSNN (3 fully-connected layers)\n85.11%\nN-DriverMotion\nSLAYER\nProposed CSNN (4 layers, pooling kernel size: 8)\n94.04%\nSLAYER\nProposed CSNN (4 layers, pooling kernel size: 18)\n91.07%\nTable 7: Classification results comparing DVS gesture dataset and N-DriverMotion dataset. For DVS Gesture, it\nincludes results from both SLAYER and IBM TrueNorth.\n11\n\nA PREPRINT - NOVEMBER 20, 2024\nloss\n(a) Accuracy (3FC)\n(b) Loss (3FC)\n(c) Confusion matrix (3FC)\n(d) Accuracy (CSNN, PK=18)\n(e) Loss (CSNN, PK=18)\n(f) Confusion matrix (CSNN, PK=18)\nloss\nAccuracy\nAccuracy\n(g) Accuracy (CSNN, PK=8)\n(h) Loss (CSNN, PK=8)\n(i) Confusion matrix (CSNN, PK=8)\nloss\nAccuracy\nFigure 7: Results (accuracy, loss, and confusion matrix) of the proposed CSNN with pooling kernel sizes 8 and 18\n(PK=8 and PK=18) vs. three fully-connected layer SNN (3FC).\n(a) Head tilts to side (label 0)\n(b) Normal driving (label 3)\n(c) Looks back (label 5)\n(d) Phone call (label 10)\nFigure 8: Classification result with output spikes\n12\n\nA PREPRINT - NOVEMBER 20, 2024\nFigure 9: Classification result for unexpected abnormal driver motion. The model detects all the abnormal actions\ncorrectly, demonstrating high sensitivity in detecting these actions, and also classifies normal driver motions with high\naccuracy.\nable to demonstrate comparable results. We note that our proposed dataset has more categories for classification and\ndoes not include repetitive actions, unlike the DVS Gesture recognition dataset.\n4.6.2\nAccuracy on unexpected abnormal driver motion classification task\nThe results for binary classification of unexpected abnormal motions and normal motions are displayed in Figure 9.\nExcept for 4 normal motions which were classified as abnormal driving motions, the model was highly capable of\ndistinguishing between two categories, reaching an accuracy of 97.24%. One significant point to note is that there\nwere no abnormal motions misclassified as normal motions, which demonstrates the robustness of our proposed CSNN\nmodel in detecting such unexpected motions.\n4.6.3\nLatency and Energy\nTo measure the power consumption for the Nvidia RTX 3080, we used the NVIDIA Management Library (NVML),\nwhich allows us to extract the throughput and energy consumption with respect to single event inference. Due to the\nfast inference time of the GPU, we calculated the latency by averaging the values, dividing the overall inference time by\nthe total number of inference samples. Energy consumption was also measured by averaging the total energy over the\ntotal time of inference.\nNvidia Jetson Xavier NX does not support NVIDIA Management Library, unlike the RTX 3080 GPU. To resolve this\nissue, we utilized jetson_stats library, which enables us to keep track of the energy consumptions of the GPU by logging\nthe statistics as a CSV file. Similar to the RTX 3080, the calculation for latency was done by averaging the values. The\nexperiment was executed with no additional GPU processes other than the measurement process being loaded.\nIn the case of Loihi 2, the LAVA framework supports profiler tools which enables us to obtain various measurement\nvalues related to power, execution time, counts of neurocore activities, and neurocore SRAM utilization rates. To get\nprecise latency and energy dissipation results, we ran the model 1 million times, which provided enough time for the\nprofiler to measure the overall costs. All experiments for latency and energy comparisons between the GPUs and Loihi\n2 were conducted with equal models, both adjusted for Loihi 2. The results for throughput and various energy-related\nmeasurements for single sample inference are shown in Table 8.\nThe overall energy consumption for Loihi 2 for 1 million inferences was 1.82 J which was highly comparable to that of\nboth GPUs, where the RTX 3080 required over 500 J, and the Jetson Xavior NX required 1.13 J for a single inference\n13\n\nA PREPRINT - NOVEMBER 20, 2024\nTable 8: Comparisons between GPUs and Loihi 2 on inference cost for a single sample. It is clear that despite the\nthroughput of the RTX 3080 being lower, the energy delay product (EDP) of Loihi 2 is much lower than that of both\nGPUs, demonstrating the high energy efficiency of Loihi 2 for SNN models\nHardware\nInference Cost Per Single Sample\nLatency\nThroughput\nEnergy\nEDP\n(ms)\n(sample/s)\nTotal (mJ)\nDynamic(mJ)\nµJs\nNDriverMotion 3FCN\nNvidia RTX 3080\n11.544\n86.627\n5.163 x 105\nN/A\n5.94 x 106\nNvidia Jetson Xavier NX\n83.3\n12.011\n6.29 x 102\nN/A\n5.24 x 104\nLoihi 2 Oheo Gulch\n1.33 x 103\n442.364\n2.42 x 10−3\n1.89 x 10−4\n3.219\nNDriverMotion CSNN\nNvidia RTX 3080\n15.103\n66.214\n5.218 x 105\nN/A\n7.88 x 106\nNvidia Jetson Xavier NX\n1.83 x 102\n5.4617\n1.13 x 103\nN/A\n2.06 x 105\nLoihi 2 Oheo Gulch\n1.441 x 104\n40.835\n2.64 x 10−2\n7.46 x 10−4\n380.29\non the CSNN model. The throughput, however, showed that while Loihi 2 processed 40.835 samples per seconds, the\nRTX 3080 processed 66.214 samples per seconds. We believe that this was due to the inference method of Loihi 2,\nwhich did not use matrix multiplication, but only addition for the input spike data. The Jetson Xavior NX performed\nvery poorly on throughput, with only 5.46 samples being processed per second.\nTo compare the balance of low energy and fast runtimes of the devices, we calculated the energy-delay product (EDP),\nwhere the energy consumption and performance of the devices were weighted equally [42]. The calculation for EDP\nwas conducted by multiplying the total energy by the latency for a single sample inference. In the EDP comparison\nbetween Loihi 2 and the two GPUs, we observed that Loihi 2 had a better EDP than the GPUs for both models, with the\n3FCN model being 1.8 million times and the CSNN model being 20,721 times more efficient compared to the RTX\n3080, and the 3FCN model being 16,278 times and the CSNN model being 541 times more efficient than the Jetson\nXavior NX.\n4.7\nFUTURE WORK\nWhile our proposed driver motion recognition system achieves significant improvements in energy efficiency and\naccuracy, there are several directions for future work to further enhance its performance and applicability. First, we plan\nto extend the N-DriverMotion dataset by increasing the diversity of driving scenarios, including more complex driving\nconditions, varied motion patterns, and a larger pool of participants. This will enable the model to generalize more\neffectively to real-world driving environments. Additionally, we plan to explore approaches for enhancing safety for\npedestrians and vehicles in autonomous driving conditions. Another important area of exploration is the integration of\nonline learning mechanisms within the neuromorphic system, allowing the model to adapt dynamically to changing\ndriving conditions and new motion patterns in real time. To achieve this, we will explore unsupervised learning\nmethods based on the spike-timing-dependent plasticity (STDP) learning rule. Moreover, we will evaluate the system’s\nscalability and performance across different hardware platforms, including more neuromorphic processors and edge\ndevices. Lastly, we aim to optimize the neuromorphic pipeline to address challenges such as latency reduction and\nincreased robustness in highly dynamic and unpredictable driving scenarios, further enhancing the system’s practicality\nin real-time autonomous driving applications.\n5\nCONCLUSION\nIn this paper, we propose N-DriverMotion, a newly collected event-based dataset designed to learn and predict driver\nmotions using a neuromorphic vision system. Our system consists of an event-based camera that captures driver\nmovements as spike inputs and an optimized convolutional spiking neural network (CSNN) capable of efficient\ninference on 720 x 720 event streams without the need for computationally expensive preprocessing. The dataset\nincludes 13 driver motion categories, classified by direction, lighting conditions, and participants, including challenging\nenvironments like low-light conditions and tunnels.\nOur experiments show that the system achieved a high accuracy of 94.04% in recognizing driver motions, even under\ndifficult conditions such as varying light and directional inputs. This demonstrates that our proposed four-layer CSNN,\ndesigned for training and inference on energy- and resource-constrained platforms, can meet performance requirements.\nIn addition, our experiments display the capability of distinguishing between unexpected abnormal driver motions and\nnormal driver motions with an accuracy of 97.24%. Our system was especially able to classify all the abnormal driver\nmotions, indicating high reliability in sensing various abnormal driver motions when deployed in real applications.\nWe also demonstrated that our system was comparable to other applications that implemented direct use of event\nvision systems with SNNs, such as the DVS Gesture recognition task, where our proposed system achieved similar\n14\n\nA PREPRINT - NOVEMBER 20, 2024\naccuracy with a less complex model structure. When deployed on Intel’s Loihi 2 neuromorphic processor, the system\ndemonstrated significant energy efficiency, with an energy-delay product (EDP) that was over 20,721 times more\nefficient than the RTX 3080 and 541 times more efficient than the Jetson Xavior NX. This level of efficiency is critical\nfor real-time, on-device AI applications where power consumption is a key constraint.\nBy eliminating the need for time-consuming preprocessing and significantly reducing power consumption while\nmaintaining high accuracy, our system can advance the safety and efficiency of autonomous driving systems using\nneuromorphic vision technology. Furthermore, the N-DriverMotion dataset provides a valuable resource for future\nresearch in driver behavior prediction, particularly in AI-driven systems requiring a high-resolution event dataset.\nACKNOWLEDGMENT\nThis work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government\n(MSIT) (NRF-) and the Ministry of Education (NRF-).\nReferences\n[1] Y. MA, Z. Wang, H. Yang, and L. Yang, “Artificial Intelligence Applications in the Development of Autonomous\nVehicles: A Survey,” IEEE/CAA Journal of Automatica Sinica, vol. 7, no. JAS-2019-0401, 2020. p. 315.\n[2] EU, “New rules to improve road safety and enable fully driverless vehicles in the EU,” 2022. Accessed on:\nSeptember 30, 2024. [Online]. Available: https://ec.europa.eu/commission/presscorner/detail/en/ip_22_4312\n[3] P. Blouw, X. Choo, E. Hunsberger, E. Chris, “Benchmarking Keyword Spotting Efficiency on Neuromorphic\nHardware,“ in Neuro-Inspired Computational Elements (NICE) ,2019, pp. 1–8\n[4] C. Tan, S. Lallee, G. Orchard, “Benchmarking neuromorphic vision: lessons learnt from computer vision,“ in\nFrontiers in neuroscience ,2015\n[5] S. Kim, S. Park, B. Na, S. Yoon, “Spiking-yolo: Spiking neural network for energy-efficient object detection,“ in\nProceedings of the AAAI Conference on Artificial Intelligence ,2020, pp. 11270–11277\n[6] R. Massa, A. Marchisio, M. Martina, M. Shafique, “An Efficient Spiking Neural Network for Recognizing Gestures\nwith a DVS Camera on the Loihi Neuromorphic Processor,“ in International Joint Conference on Neural Networks\n(IJCNN), 2020, pp. 1–9\n[7] J. Pérez-Carrasco, B. Zhao, C. Serrano, B. Acha, T. Serrano-Gotarredona, S. Chen, B. Linares-Barranco, “Mapping\nfrom frame-driven to frame-free event-driven vision systems by low-rate rate coding and coincidence processing\napplication to feedforward ConvNets,“ in IEEE transactions on pattern analysis and machine intelligence, vol. 35,\nno. 11, 2013, pp. 2706–2719\n[8] “Lava, an open-source software framework for developing neuromorphic systems,” 2022. Accessed: September\n30, 2024. [Online]. Available: https://github.com/lava-nc/lava\n[9] M. Davis, “Taking neuromorphic computing to the next level with Loihi 2,” in intel Technology Brief, 2021\n[10] S. Shrestha, G. Orchard, “SLAYER: Spike Layer Error Reassignment in Time,“ in Advances in Neural Information\nProcessing Systems, vol. 31, 2018\n[11] G. Gallego, T. Delbrück, G. Orchard, C. Bartolozzi, B. Taba, A. Censi, S. Leutenegger, A. Davison, J. Conradt,\nK. Daniilidis, D. Scaramuzza, “Event-Based Vision: A Survey,“ in IEEE Transactions on Pattern Analysis and\nMachine Intelligence, vol. 44, no. 1, 2022, pp. 154–180\n[12] E. Ceolini, C. Frenkel, S. Shrestha, G. Taverni, L. Khacef, M. Payvand, E. Donati, “Hand-Gesture Recognition\nBased on EMG and Event-Based Camera Sensor Fusion: A Benchmark in Neuromorphic Computing,“ in Frontiers\nin Neuroscience, vol. 14, 2020\n[13] R. Verschae, I. Bugueno-Cordova, “Event-Based Gesture and Facial Expression Recognition: A Comparative\nAnalysis,“ in IEEE Access, vol. 11, 2023, pp. 121269–121283\n[14] J. Kaiser, H. Mostafa, E. Neftci, “Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE),“\nin Frontiers in Neuroscience, vol. 14, 2020\n[15] G. Chen, L. Hong, J. Dong, P. Liu, J. Conradt, A. Knoll, “EDDD: Event-Based Drowsiness Driving Detection\nThrough Facial Motion Analysis With Neuromorphic Vision Sensor,“ in IEEE Sensors Journal, 2020, vol. 20, no.\n11, pp. 6170–6181\n15\n\nA PREPRINT - NOVEMBER 20, 2024\n[16] R. Baldwin, R. Ruixu, M. Almatrafi, V. Asari, K. Hirakawa, “Time-Ordered Recent Event (TORE) Volumes for\nEvent Cameras,“ in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 2, 2023, pp.\n2519–2532\n[17] N. Messikommer, D. Gehrig, A. Loquercio, D. Scaramuzza, “Event-Based Asynchronous Sparse Convolutional\nNetworks,” in Computer Vision – ECCV 2020, 2020, pp. 415–431.\n[18] D. Gehrig, A. Loquercio, K. Derpanis, and D. Scaramuzza, “End-to-End Learning of Representations for\nAsynchronous Event-Based Data,” in 2019 IEEE/CVF International Conference on Computer Vision (ICCV),\n2019, pp. 5632–5642.\n[19] M. Davies, N. Srinivasa, T. Lin, G. Chinya, Y. Cao, S. Choday, G. Dimou, P. Joshi, N. Imam, S. Jain, Y. Liao, C.\nLin, A. Lines, R. Liu, D. Mathaikutty, S. McCoy, A. Paul, J. Tse, G. Venkataramanan, Y. Weng, A. Wild, Y. Yang,\nH. Wang, “Loihi: A Neuromorphic Manycore Processor with On-Chip Learning,“ in IEEE Micro, vol. 38, no. 1,\n2018, pp. 82–99\n[20] A. Amir, B. Taba, D. Berg, T. Melano, J. McKinstry, C. Di Nolfo, T. Nayak, A. Andreopoulos, G. Garreau, M.\nMendoza, “A low power, fully event-based gesture recognition system,“ in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2017, pp. 7243–7252\n[21] A. Maqueda, A. Loquercio, G. Gallego, N. Garcia, D. Scaramuzza, “Event-Based Vision Meets Deep Learning\non Steering Prediction for Self-Driving Cars,“ in 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018, pp. 5419–5427\n[22] J. Li, S. Dong, Z. Yu, Y. Tian, T. Huang, “Event-Based Vision Enhanced: A Joint Detection Framework in\nAutonomous Driving,“ in 2019 IEEE International Conference on Multimedia and Expo (ICME), 2019, pp.\n1396–1401\n[23] S. Kim, S. Park, B. Na, S. Yoon, “Spiking-YOLO: Spiking Neural Network for Energy-Efficient Object Detection,“\nin Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, 2020, pp. 11270–11277\n[24] Q. Su, Y. Chou, Y. Hu, J. Li, S. Mei, Z. Zhang, G. Li, “Deep Directly-Trained Spiking Neural Networks for Object\nDetection,“ in IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 6532–6542\n[25] B. Mohamed Sadek, C. Dalila, C. Elisabetta, K. Lyes, “Impact of spiking neurons leakages and network recurrences\non event-based spatio-temporal pattern recognition“ in Frontiers in Neuroscience Volume 17, 2023\n[26] A. Viale, A. Marchisio, M. Martina, G. Masera, M. Shafique, “CarSNN: An Efficient Spiking Neural Network for\nEvent-Based Autonomous Cars on the Loihi Neuromorphic Research Processor,“ in International Joint Conference\non Neural Networks (IJCNN) ,2021, pp. 1–10\n[27] M. Davies, A. Wild, G. Orchard, Y. Sandamirskaya, G. Guerra, P. Joshi, P. Plank, S. Risbud, “Advancing\nNeuromorphic Computing With Loihi: A Survey of Results and Outlook,“ in Proceedings of the IEEE, vol. 109,\nno. 5, 2021, pp. 911–934\n[28] S. Liu, Y. Liang, Y. Yi, “DNN-SNN Co-Learning for Sustainable Symbol Detection in 5G Systems on Loihi Chip,“\nin IEEE Transactions on Sustainable Computing, vol. 9, no. 2, 2024, pp. 170–181\n[29] G. Orchard, E. Frady, D. Rubin, S. Sanborn, S. Shrestha, F. Sommer, M. Davies, “Efficient Neuromorphic Signal\nProcessing with Loihi 2,“ in 2021 IEEE Workshop on Signal Processing Systems (SiPS), 2021, pp. 254–259\n[30] S. Shrestha, J. Timcheck, P. Frady, L. Campos-Macias, M. Davies, “Efficient Video and Audio Processing with\nLoihi 2,“ in ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2024, pp. 13481–13485\n[31] G. Parpart, S. Risbud, G. Kenyon, Y. Watkins, “Implementing and Benchmarking the Locally Competitive\nAlgorithm on the Loihi 2 Neuromorphic Processor“ in Proceedings of the 2023 International Conference on\nNeuromorphic Systems (ICONS), 2023\n[32] P. Panda, K. Roy, Kaushik, “Unsupervised regenerative learning of hierarchical features in Spiking Deep Networks\nfor object recognition,“ in International Joint Conference on Neural Networks (IJCNN), 2016, pp. 299–306\n[33] J. Lee, T. Delbruck, M. Pfeiffer, “Training deep spiking neural networks using backpropagation,“ in Frontiers in\nneuroscience, vol. 10, 2016\n[34] F. Zenke, S. Ganguli, “Superspike: Supervised learning in multilayer spiking neural networks,“ in Neural\ncomputation, vol. 30, no. 6, 2018, pp. 1514–1541\n[35] Y. Xing, G. Di Caterina, and J. Soraghan, “A New Spiking Convolutional Recurrent Neural Network (SCRNN)\nWith Applications to Event-Based Hand Gesture Recognition,” Frontiers in Neuroscience, vol. 14, 2020.\n[36] D. Forsyth, J. Mundy, V. di Gesú, R. Cipolla, Y. LeCun, P. Haffner, L. Bottou, Y. Bengio, “Object recognition\nwith gradient-based learning,“ in Shape, contour and grouping in computer vision, 1999, pp. 319–345\n16\n\nA PREPRINT - NOVEMBER 20, 2024\n[37] Y. Hu, H. Liu, M. Pfeiffer, T. Delbruck, “DVS benchmark datasets for object tracking, action recognition, and\nobject recognition,“ in Frontiers in neuroscience, vol. 10, 2016\n[38] T. Serrano-Gotarredona, B. Linares-Barranco, “Poker-DVS and MNIST-DVS. Their history, how they were made,\nand other details,“ in Frontiers in neuroscience, vol. 9, 2015\n[39] G. Orchard, A. Jayawant, G. Cohen, N. Thakor, “Converting static image datasets to spiking neuromorphic\ndatasets using saccades,“, in Frontiers in neuroscience, vol. 9, 2015\n[40] R. Berner, C. Brandli, M. Yang, S. Liu, T. Delbruck, “A 240× 180 10mw 12us latency sparse-output vision sensor\nfor mobile applications,“ in Symposium on VLSI Circuits, 2013, pp. C186–C187\n[41] C. Tan, S. Lallee, G. Orchard, “Benchmarking neuromorphic vision: lessons learnt from computer vision,“ in\nFrontiers in neuroscience, vol. 9, 2015\n[42] I. Ratkovi´c, N. Bežani´c, O. Ünsal, A. Cristal, V. Milutinovi´c, “Chapter One - An Overview of Architecture-Level\nPower- and Energy-Efficient Design Techniques“ in Advances in Computers Volume 98, 2015, pp. 1–57,\n17",
    "pdf_filename": "N-DriverMotion_Driver_motion_learning_and_prediction_using_an_event-based_camera_and_directly_traine.pdf"
}