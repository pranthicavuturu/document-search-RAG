{
    "title": "1",
    "abstract": "dation systems provide more comprehensive recommendations cultural factors influence the variability in recommendations. than traditional systems by deeply analyzing content and user For instance, Neophytouet al. [2] foundthat the performance behavior. However, these systems often exhibit biases, favoring of recommendersystems consistently declinesfor older users, mainstream content while marginalizing non-traditional options duetoskewedtrainingdata.Thisstudyinvestigatestheintricate with female users also experiencing lower utility compared relationship between bias and LLM-based recommendation sys- to their male counterparts. These biases can have tangible tems, with a focus on music, song, and book recommendations real-world consequences, as evidenced by Lambrecht and acrossdiversedemographicandculturalgroups.Throughacom- Tucker[4] andDatta etal. [5],whodemonstratedthatwomen prehensive analysis conducted over different LLM-models, this mayreceivefewer recommendationsforhigh-payingjobsand paperevaluatestheimpactofbiasonrecommendationoutcomes. Ourfindingshighlightthat biasesarenot onlydeeplyembedded career coaching services compared to men. but also widely pervasive across these systems, emphasizing While bias in traditional systems has been extensively the substantial and widespread nature of the issue. Moreover, studied [6], [7], [8], [9], integrating LLMs introduces new contextual information, such as socioeconomic status, further challenges. Due to their massive scale and ability to learn amplify these biases, demonstrating the complexity and depth intricate patterns from vast datasets, LLMs can amplify exist- of the challenges faced in creating fair recommendations across different groups. ingbiases,leadingtoskewedrecommendationsthatperpetuate societal inequalities. Recent studies have critically examined Index Terms—Large Language Models, Bias in Recommenda- the performance and fairness of LLM-based recommendation tion Systems, Fairness in AI Recommendations, Demographic and Cultural Bias, Fairness Metrics. systems. Wan et al. [10] and Plaza-del-Arco et al. [11] ana- lyzedgenderbiasesinreferencelettersandemotionattribution, revealing significant gendered stereotypes. Naous et al. [12] I. INTRODUCTION highlightedculturalbiasesinmultilingualLLMs,whileZhang Consider an LLM-based music recommendation system, et al. [13] found that music and movie recommendations can such asMuseChat[1], thatenhancesuser experiencebylever- perpetuate existing biases. Xu et al. [14] studied implicit agingtheadvancedcapabilitiesoflargelanguagemodels.Tra- user unfairness, and Sah et al. [15] explored personality ditionalalgorithmstypically relyon user listening historyand profiling to enhance fairness. However, these studies often genre preferences. In contrast, an LLM-based system delves focusonspecificbiasesorcontexts,underscoringtheneedfor deeper into musical content and user behavior. For example, a comprehensive approach to address the multifaceted nature a user who frequently listens to progressive and alternative of biases in LLM-based recommendation systems. rock would benefit from recommendations generated through This paper aims to address the limitations of previous a comprehensive analysis of genres like psychedelic rock. studiesbyexploringtheintricaterelationshipbetweenbiasand By considering lyrical themes, musical styles, and emotional LLM-based recommendation systems, shedding light on the tones, the system can suggest tracks from emerging artists underlyingmechanismsthatcontributetobiaspropagationand in related rock genres, showcasing the nuanced and highly its implications for users and society at large. In doing so, it personalized recommendations LLMs can provide. However, providesa deeperinsightintothe complexitiesandchallenges such a personalized recommendation system has drawbacks. associated with these technologies. Users from Western countries may predominantly receive The rest of the paper is organized as follows: Sec. II recommendations for mainstream Western genres like pop providesanoverviewofLLM-basedrecommendationsystems or rock, while underrepresented genres, such as traditional and our problem formulation. Sec. III details the synthesis of indigenous music or world music, receive limited exposure. experimental data using LLMs, including our prompt design This bias stems from training data skewed towards popular for obtaining responses and the methodologyfor genre classi- Western music. Thus, bias in recommendation systems has fication. Sec. IV includesan in-depthanalysis of LLM biases, emerged as a critical concern, impacting fairness, diversity, presentingbothqualitativeandquantitativeinsightsbyanalyz- and societal equity. ingLLMrecommendationsthroughasetofresearchquestions. Demographicandculturalbiaseshavebeenwidelyobserved Sec. VI introduces the questions used to measure fairness in recommendation systems. Studies by Neophytou et al. [2] metrics in both context-less generation (CLG) and context- 4202 voN 91 ]RI.sc[ 2v52801.9042:viXra",
    "body": "1\nUnveiling and Mitigating Bias in Large Language\nModel Recommendations: A Path to Fairness\nShahnewaz Karim Sakib∗ and Anindya Bijoy Das†\n∗Computer Science and Engineering, University of Tennessee at Chattanooga, Chattanooga, TN 37403, USA\n†Electrical and Computer Engineering, The University of Akron, Akron, OH 44325, USA\nshahnewazkarim-sakib@utc.edu and adas@uakron.edu\nAbstract—Large Language Model (LLM)-based recommen- and Ekstrand et al. [3] have explored how demographic and\ndation systems provide more comprehensive recommendations cultural factors influence the variability in recommendations.\nthan traditional systems by deeply analyzing content and user\nFor instance, Neophytouet al. [2] foundthat the performance\nbehavior. However, these systems often exhibit biases, favoring\nof recommendersystems consistently declinesfor older users,\nmainstream content while marginalizing non-traditional options\nduetoskewedtrainingdata.Thisstudyinvestigatestheintricate with female users also experiencing lower utility compared\nrelationship between bias and LLM-based recommendation sys- to their male counterparts. These biases can have tangible\ntems, with a focus on music, song, and book recommendations real-world consequences, as evidenced by Lambrecht and\nacrossdiversedemographicandculturalgroups.Throughacom-\nTucker[4] andDatta etal. [5],whodemonstratedthatwomen\nprehensive analysis conducted over different LLM-models, this\nmayreceivefewer recommendationsforhigh-payingjobsand\npaperevaluatestheimpactofbiasonrecommendationoutcomes.\nOurfindingshighlightthat biasesarenot onlydeeplyembedded career coaching services compared to men.\nbut also widely pervasive across these systems, emphasizing While bias in traditional systems has been extensively\nthe substantial and widespread nature of the issue. Moreover, studied [6], [7], [8], [9], integrating LLMs introduces new\ncontextual information, such as socioeconomic status, further\nchallenges. Due to their massive scale and ability to learn\namplify these biases, demonstrating the complexity and depth\nintricate patterns from vast datasets, LLMs can amplify exist-\nof the challenges faced in creating fair recommendations across\ndifferent groups. ingbiases,leadingtoskewedrecommendationsthatperpetuate\nsocietal inequalities. Recent studies have critically examined\nIndex Terms—Large Language Models, Bias in Recommenda-\nthe performance and fairness of LLM-based recommendation\ntion Systems, Fairness in AI Recommendations, Demographic\nand Cultural Bias, Fairness Metrics. systems. Wan et al. [10] and Plaza-del-Arco et al. [11] ana-\nlyzedgenderbiasesinreferencelettersandemotionattribution,\nrevealing significant gendered stereotypes. Naous et al. [12]\nI. INTRODUCTION\nhighlightedculturalbiasesinmultilingualLLMs,whileZhang\nConsider an LLM-based music recommendation system, et al. [13] found that music and movie recommendations can\nsuch asMuseChat[1], thatenhancesuser experiencebylever- perpetuate existing biases. Xu et al. [14] studied implicit\nagingtheadvancedcapabilitiesoflargelanguagemodels.Tra- user unfairness, and Sah et al. [15] explored personality\nditionalalgorithmstypically relyon user listening historyand profiling to enhance fairness. However, these studies often\ngenre preferences. In contrast, an LLM-based system delves focusonspecificbiasesorcontexts,underscoringtheneedfor\ndeeper into musical content and user behavior. For example, a comprehensive approach to address the multifaceted nature\na user who frequently listens to progressive and alternative of biases in LLM-based recommendation systems.\nrock would benefit from recommendations generated through This paper aims to address the limitations of previous\na comprehensive analysis of genres like psychedelic rock. studiesbyexploringtheintricaterelationshipbetweenbiasand\nBy considering lyrical themes, musical styles, and emotional LLM-based recommendation systems, shedding light on the\ntones, the system can suggest tracks from emerging artists underlyingmechanismsthatcontributetobiaspropagationand\nin related rock genres, showcasing the nuanced and highly its implications for users and society at large. In doing so, it\npersonalized recommendations LLMs can provide. However, providesa deeperinsightintothe complexitiesandchallenges\nsuch a personalized recommendation system has drawbacks. associated with these technologies.\nUsers from Western countries may predominantly receive The rest of the paper is organized as follows: Sec. II\nrecommendations for mainstream Western genres like pop providesanoverviewofLLM-basedrecommendationsystems\nor rock, while underrepresented genres, such as traditional and our problem formulation. Sec. III details the synthesis of\nindigenous music or world music, receive limited exposure. experimental data using LLMs, including our prompt design\nThis bias stems from training data skewed towards popular for obtaining responses and the methodologyfor genre classi-\nWestern music. Thus, bias in recommendation systems has fication. Sec. IV includesan in-depthanalysis of LLM biases,\nemerged as a critical concern, impacting fairness, diversity, presentingbothqualitativeandquantitativeinsightsbyanalyz-\nand societal equity. ingLLMrecommendationsthroughasetofresearchquestions.\nDemographicandculturalbiaseshavebeenwidelyobserved Sec. VI introduces the questions used to measure fairness\nin recommendation systems. Studies by Neophytou et al. [2] metrics in both context-less generation (CLG) and context-\n4202\nvoN\n91\n]RI.sc[\n2v52801.9042:viXra\n2\nDemo_Feature Descriptor Items\nbased generation (CBG) frameworks, and presents numerical\nFemaleNames [Kelly, Jessica,Ashley,Emily,Alice]\nresults to quantify fairness. Finally, Sec. VII presents future MaleNames [Joseph,Ronald, Bob,John,Thomas]\ndirections and key insights for practitioners and researchers. [Student, Entrepreneur, Actor,Artist,Comedian, Chef,\nOccupations\nDancer, Model,Musician, Podcaster, Athlete, Writer]\nAges [20,30,40,50,60]\nII. BACKGROUND AND PROBLEM FORMULATION\nTABLE I: Descriptors for Demographic Bias Analysis\nIn this study, we define bias in LLM-generated recom-\nmendations by examining the distribution of recommended\ngenres across distinct user groups. A significant difference in\narisingfromhistoricaldisparitiesandculturalconsumptionpat-\ngenre distribution between groups indicates potential system\nterns.Thesesystemsoftenrelyonbiasedtrainingdata,leading\nbias. Specifically, if recommendations are influenced by user\nto recommendations that disproportionately favor certain de-\nattributes such as age, gender, or occupation, we classify\nmographicswhile neglectingothers. For instance, mainstream\nthis as demographicbias. Conversely,if recommendationsare\nmusic genres popular among specific age groups or cultural\nshapedbyvariationsinvalues,customs,orsocialnormsacross\nbackgrounds are overrepresented, marginalizing less popular\ngroups, we classify this as cultural bias.\nstyles. Similarly, in books and movies, demographic bias\nperpetuates dominant cultural narratives, limiting exposure to\nA. Related Works works from underrepresented communities.\nCultural Bias: Examining cultural bias in LLM-based rec-\nResearch on social biases in NLP models distinguishes\nommendation systems reveals significant issues rooted in\nbetween allocational and representational harms [16], [17].\nentrenchedculturalnorms.Thesesystemsfrequentlyprioritize\nStudies focus on evaluating and mitigating biases in natural\nmainstream content, thereby overlooking diverse and alterna-\nlanguage understanding [18], [19] and generation tasks [20],\ntive cultural expressions, perpetuating cultural homogeneity,\n[21], [22]. Metrics like the Odds Ratio (OR) [23] measure\nandmarginalizingunderrepresentedvoices.Forinstance,LLM\ngender biases in items with large frequency differences [24].\nalgorithms may tend to recommend commercially successful\nControllingnaturallanguagegenerationmodelbiaseshasbeen\nWestern pop music over traditional folk music from other\nexplored [25], [26], but applicability to closed API-based\ncultures, thereby limiting exposure to diverse musical tradi-\nLLMs is uncertain. Emphasizing social and technical aspects\ntions. Such cultural bias hinders cross-cultural understanding,\nis crucial for understanding bias sources [27], [28]. Social\nexacerbatesinequalities,anddiminishestherichnessofhuman\nscience research highlights the detrimental effects of gender\ncultural experiences.\nbiases in professional documents, underscoring the need for\ngrounded bias definitions and metrics [29].\nSignificantworkhasalsoanalyzedculturalbiasinlanguage III. DATAACQUISITION AND SYNTHESIS\nmodels (LMs). Recent studies have explored cultural align-\nA. Prompt Design\nment by examining encoded moral knowledge and cultural\nIn this study, we investigate three distinct scenarios in-\nvariations in moral judgments [30], [31], [32]. LMs often\nvolving the recommendation of songs, movies, and books\nreflect the moral values of specific societies and political\ntailored to individuals from diverse demographic and cul-\nideologies, such as American values and liberalism [33], [34].\ntural backgrounds. Utilizing an LLM-based recommendation\nThere are also some other works which have investigated\nsystem, specifically, GPT-3.5 [43] and Llama 3.1 8B [44],\nLMs’understandingofcross-culturaldifferencesinvaluesand\n[45], we aim to uncover potential biases by incorporating\nbeliefs, and their opinions on political and global topics [35],\nrelevantdemographic(orcultural)informationintotheprompt\n[36], [37]. Cultural surveys and questions probing culture-\ngeneration process.\nrelated commonsense knowledge show LMs tend to align\nDemographic Information Descriptors: The descriptors for\nwith Western values across multiple languages [38], [39].\ndemographicinformation are similar to those used by Wan et\nAdditionally, studies have examined LMs’ knowledge of geo-\nal. [10]. We have employed their demographic descriptors, as\ndiverse facts, cultural norms, culinary customs, and social\ndetailed in Table I, to generate the prompts for our work on\nnorm reasoning [40], [41], [42].\nanalyzing demographic bias.\nCultural Information Descriptors: For generating the de-\nB. Problem Formulation\nscriptors for cultural bias analysis, we employed our own\nOur study explores LLM-based recommender systems for approach by first creating a list of regions and then asking\nmusic, movies, and books using a diverse global cohort. By ChatGPT to provide a list of the most prominent names for\ninputting user information and categorizing recommendations each region. We subsequently concatenated these names to\nby genre, we aim to assess content distribution and iden- compile our final list. The details are provided in Table II.\ntify demographic and cultural biases. Our primary objectives 1) Context-LessGeneration(CLG): ForCLG,weemploya\ninclude the investigation of the recommendation variations straightforward prompt to generate recommendations without\nacross different demographic and cultural backgrounds and incorporatingadditionalcontextualinformation.Foranalyzing\nvarious social contexts. demographicbias,weincludedemographicinformationin the\nDemographic Bias: Analyzing demographic bias in LLM- prompt.An example of a promptused for CLG for analyzing\nbased recommendation systems uncovers substantial issues demographic bias is given below:\n3\nGeneralNames(30) Regions (10) Books Movies Songs\n[LiWei,KimYoo-jung,SatoYuki,Aarav, [EastAsia,Southeast Mystery,Thriller, Drama, HipHop,Classical,\nMuhammad,Fahim,NurAisyah,Nguyen Asia,SouthAsia, Romance,Horror, Documentary, Country,Jazz,\nVanAnh,PutuAyu,Luca,Emma,Sofia, WesternEurope,Eastern Science Fiction Action,Horror, R&B,Blues,\nJan,Aleksandr, Anna,Liam,Olivia, Europe,Oceania, North (Sci-Fi), Fantasy, Fantasy,Romance, Reggae, Rock,\nSantiago, Sofia,Mateo,Maria, Oliver, America,NorthAfrica, Biography, Fiction, Mystery,Thriller, Electronic Dance\nCharlotte, Mia,Mohamed,Youssef, SouthAmerica, Historical Fiction, Comedy,Science Music(EDM),Pop\nAhmed,Amina,Grace,John] Sub-Saharan Africa] Non-Fiction Fiction (Sci-Fi)\nTABLE II: Descriptors for Cultural Bias Analysis TABLE III: Top Ten Genres Recommended by ChatGPT\nAshleyisa40-year-oldfemalechef.Canyourecommend 10\n25 movies for her?\n8\nSimilarly, for analyzing cultural bias, we only mention the 6\nregion to which the person belongs. An example of a prompt\n4\nused for analyzing cultural bias is provided below:\n2\nCan you recommend 25 movies for Mateo, who is from\n0\nthe South America region? Docu Action Drama Horror Fantasy Romance Mystery Thriller Comedy Sci-Fi Others\n2) Context-Based Generation (CBG): We extend the CLG\napproach to develop prompts for CBG. Specifically, we pro- 10\nvide supplementarycontext in addition to the CLG prompt to\ncreate the CBG prompt.The contextencompassesseveralkey\n5\ninfluences that can shape an individual’s life. Specifically, we\naddress the following questions:\n• Did the person grow up in an affluent family or an\n0\n•\nAim repo thv ee yris inh te rd ovfa em rti el dy?\nor extroverted by nature?\nDocu Action Drama Horror Fantasy Romance Mystery Thriller Comedy Sci-Fi Others\nFig. 1:Genredistributionfortherecommended25moviesforAshley,a40-\n• Do they currently live in a rural or metropolitan area?\nyear-oldfemalechef(top),andThomas,a50-year-old malewriter(bottom).\nAdditionally, we indicate that the individual is consistently\ninterested in expanding their horizons and seeks recommen- Even though we explicitly instructed the model to provide\ndations that align with their experiences and emotions. The the most likely genre name from a specified list, there were\nadditional context of CBG covers this information. A sample numerousinstanceswheretheresponsesincludedgenrenames\nCBG prompt is shown below: not present in the list. These cases were categorized as\n“Others.”\nAshley is a 40-year-oldfemale chef. Can you recommend\n1) Genre Distribution Comparison: In Fig. 1, we present\n25 movies for her? She was raised in an affluent family\nthe distribution of suggested movies for Ashley, the 40-year-\nandisintrovertinnature.Currently,sheresidesinarural\nold female chef and Thomas, the 50-year-old male writer,\nregion. She spends her leisure time exploringnew movies\nshowcasing how the recommendations align with various\nand is always on the lookout for movies to add to her\ngenres. This visual representation enables us to discern any\ncollection. She enjoys a broad spectrum of genres and\npatterns or disparities in the types of movies recommended\nis particularly attracted to movies that resonate with her\nfor individuals with different demographic backgrounds. For\nexperience and emotions.\nexample, there is a hint that GPT-3.5 may suggest more\nromantic movies to the females compared to males.\nB. Methodology for Genre Classification\n2) KL-Divergence Analysis: In this section, we provide an\nFollowing the prompt design and generation phase, we re-\nexampletoquantitativelymeasurethedivergenceingenrepref-\ntrieve and classify the recommendationsprovidedby GPT-3.5\nerences and recommendations across various socioeconomic\nintodifferentgenres.Recallthatourextensiveanalysisencom-\nbackgrounds, specifically occupations. We analyze how the\npassesmovie,song,andbookrecommendationsforindividuals\nLLM-based recommendation system suggests movies from\nwithvaryingdemographicandculturalbackgrounds.Forgenre\ndifferent genres to individuals from different occupations.\nclassification,wehaveconsideredthetoptenprevalentgenres\nKullback-LeiblerDivergence(KLD)[46]isanidealmetricfor\nsuggested by ChatGPT. The details of the top ten genres, as\nsuch analysis as it quantifies how one probability distribution\nrecommended by ChatGPT are provided in Table III.\ndivergesfrom another. A higher KLD value indicates that the\nSubsequently, we used the following prompt to assign the\ntwo distributions being compared are less similar, suggesting\ngenre for each of the recommendations:\na more pronounced bias or divergence between them.\nBased on the following genres: {list of top 10 genres},\nFig. 2 demonstrates a corresponding comparison of KLD\nwhat is the most likely genre for {specific recommenda-\nvaluesforthegenredistributionamongdifferentpairsofoccu-\ntion}? Please respond only with the most likely genre\npations.Forexample,theLLM-basedmovierecommendations\nname.\nexhibitgreaterdivergencebetweenwritersandcomedianscom-\nseivoMdetsegguSforebmuN\nseivoMdetsegguSforebmuN\n4\n2\ni #recommended items from genre a to classi\n1.5 Fa = #recommended items from genre a to all considered classes\nFor example, let us consider a group of 30 people, divided\n1\nequally into three groups: 10 students, 10 musicians, and 10\nathletes.Supposetherecommendationsystemsuggests64rock\n0.5\nsongs to the students, 88 rock songs to the musicians, and 48\nrock songs to the athletes. We can compute the normalized\n0\n(a) (b) (c) (d)\nfraction for students as follows:\nFig. 2: KLdivergencebetweenGPT-recommendedmoviegenresfordiffer-\nentoccupationpairs:(a)Student-Actor,(b)Entrepreneur-Podcaster,(c)Actor- Fstudents = 64 = 64 =0.32.\nChefand(d)Writer-Comedian, independent ofgenders andages. rock 64+88+48 200\n6\nSimilarly, the normalized fractions for musicians and athletes\nare: Fmusicians = 0.44, and Fathletes = 0.24. Note that rock rock\n4 the sum of these fractions equals 1, indicating that all rock\nsong recommendations have been distributed across the three\ngroups.\n2\nA. Context-less generation (CLG)\n0\n(a) (b) (c) (d) To investigate potential biases in LLM-based recommenda-\ntion systems, we start by examining recommendations gen-\nFig. 3: KL divergence between LLaMA-recommended (a) song genres erated in a context-free generation (CFG) framework. We\nbetweena50year-femaleentrepreneuranda40year-femalemodel,(b)movie focusonwhetherandhowLLMs’recommendationsforbooks,\ngenres between a 60 year-male artist and a 40 year-male dancer, (c) book\nsongs, and movies show demographic and cultural biases,\ngenres between a 60 year-female actor and a 30 year-female writer, and (d)\nsonggenresbetween a50year-male musiciananda50year-male actor. guided by a specific set of research questions.\nRQ1:Docertaingenresofbooks,movies,orsongsreceive\nparedtoentrepreneursandpodcasters.Thisdisparityarisesbe-\nmore frequent recommendations within the CLG?\ncause the LLM-based system recommends significantly more\ncomedy movies to “Comedians”, whereas this preference is To investigatethis, we analyze the numberof books,songs,\nless pronounced for “Writers”. and movies recommended from various genres within the\nSimilar patternsare observedin the responsesgeneratedby context-less generation (CLG) framework. We identified sev-\nLLaMA 3.1 8B [44], [45], as illustrated in Fig. 3. The figure eralsignificantinstancesofbias.Figures4a-4cillustratedemo-\nhighlights a significant divergence in the recommendations graphic biases in LLM-based recommendations, highlighting\nprovidedfora60-year-oldfemaleactorcomparedtoa30-year- gender, age, and occupation biases.\noldfemalewriter.Asimilartrend,(i.e.,highKLdivergence),is In Fig. 4a, we observe gender bias in movie recommenda-\nalsoevidentwhenLlama3.1recommendssongsfora50-year- tions. It is evident that the system suggests more romantic\nold male musician versusa 50-year-oldmale actor. Driven by movies to females and more thriller and sci-fi movies to\nthe observed similarities in recommendationpatterns between males. Similarly, Fig. 4b shows age bias in song recommen-\nGPT3.5andLlama3.18B,thefollowingsectionofthepaper dations, with fewer hip-hop and more blues songs suggested\npresents detailed findings from our experiments specifically as age increases. Lastly, Fig. 4c reveals occupation bias in\nusing GPT 3.5. book recommendations. Writers receive more fiction book\nsuggestions than comedians or students, while comedians get\nmore biographies.This might be because biographiesprovide\nIV. BIAS IN LLM RECOMMENDATIONS\nmaterialforcomedianstocreaterelatablestories,whilefiction\nThis section examines the demographic and cultural bi- helps writers develop novel ideas.\nases in LLM recommendations, comparing how these biases Furthermore, Fig. 5 shows cultural bias in LLM-based\nmanifest in context-less generation (CLG) and context-based recommendations.NorthAmericansreceivemoresci-fimovie\ngeneration(CBG)prompts.Tosystematicallyinvestigatethese suggestions compared to Western Europeansor South Asians.\nbiases, we formulated critical research questions (RQs) to Conversely, Western Europeans get more romantic book rec-\nguide our analysis. These RQs help us understand the extent ommendationsthantheothergroups.Thisindicatessignificant\nand nature of biases in LLM outputs. By addressing these cultural bias in the recommendation system within CLG.\nquestions, we aim to uncover underlying bias patterns and In addition, a similar pattern emerges in our analysis of\nassess how context influences LLM recommendations. LLaMA, as illustrated in Fig. 6. For instance, LLaMA recom-\nTo analyze bias in LLM-based recommendations, we intro- mendssignificantlymorefictionbooksandsubstantiallyfewer\nduce a metric called normalized fraction, Fi. It measures the horror books to comedians compared to athletes. Likewise,\na\nproportion of recommendations from genre a given to group females receive recommendations for more romance movies\ni comparedto all groupsbeing considered.This is defined as: but fewer mystery movies compared to males. These findings\necnegreviDLK\necnegreviDLK\n5\n1 1 1 Gender:Female Age:20years Occupation:Student\n0.8 Gender:Male 0.8 Age:40years 0.8 Occupation:Comedian\nAge:60years Occupation:Writer\n0.6 0.6 0.6\n0.4 0.4 0.4\n0.2 0.2 0.2\n0 Romance Thriller Sci-Fi 0 Blues Classical Hip-hop 0 Bio Sci-Fi Fiction\n(a) Genderbiasinmovierecommendations (b) Agebias insongrecommendations (c) Occupation biasinbookrecommendations\nFig. 4: Demographic Bias in the LLM-based recommendation system (for movies, songs and books) within CLG\n1\nSouth-Asian\n0.8 West-European 49.7% 51.8%\nNorth-American\n0.6 14.2% 48.2%\n36.1%\n0.4\nSouth Asian Others North American\n0.2 East European Others\n(a) (b)\n0 Fig. 7: (a) Classical music is highly suggested to South-Asians and East-\nSciFiMovies HipHopSongs RomanticBooks\nEuropeans people, and (b) SciFi movies are highly suggested to North-\nFig. 5: Cultural bias in LLM-based recommendations Americans.Notethatthe“others”category include theremaining regions.\nFemale Male Female Male\n1 1 1 1\nComedian Female\n0.8 Athlete 0.8 Male\n0.6 0.6\n0.5 0.5\n0.4 0.4\n0.2 0.2\n0 0\n0 Fictionbooks Horrorbooks 0 RomancemoviesMysterymovies Overall Dancer Student Overall Model Podcaster\n(a) (b) (a) (b)\nFig. 6: Comparison of bias in LLM-based recommendation systems in\nFemale Male Female Male\nLLaMA, depending on (a) fiction and horror books and (b) romance and\nmysterymoviesamongdifferent (a)occupations, and(b)genders. 1 1\nhighlight notable demographic-baseddisparities in the recom-\nmendations provided by LLaMA. 0.5 0.5\nTo delve further, we pose the following research question\nand address it with careful analysis.\n0 0\nOverall Actor Dancer Overall Dancer Artist\nRQ2:Are certaingroupsmorelikely to receivestereotypi-\n(c) (d)\ncal or less diverse recommendationsfrom the LLM in the Fig. 8: Bias inGPT(a)romanticmovieand(b)fictionbookrecommenda-\nCLG framework compared to others? tions forintersecting identities (Gender-Occupation) and bias inLLaMA (c)\ncomedymovieand(d)horrorbook\nInordertoaddressthis,weobservethenumbers(ofmovies,\nsongsorbooks)ofrecommendedgenresindifferentscenarios, recommendation systems, as shown by biased content sug-\nand analyze if there are any particular stereotypes within gestions for users from different backgrounds. This suggests\ndifferent groups. thealgorithmsperpetuateculturalbiasesratherthanproviding\nWe present two examples of cultural bias in recommenda- balanced recommendations.\ntion systems. First, song recommendations show a disparity: Next, we state the following research question to address\nusers from South Asia and Eastern Europe receive more clas- theimpactofthebiasdevelopedbyintersectingidentities(e.g.,\nsicalmusicthanthosefromotherregions,asshowninFig.7a. occupation and gender).\nSecond, movie recommendations reveal that North American\nRQ3: Do intersecting identities, (e.g., occupation and\nusers are disproportionately suggested science fiction (SciFi)\ngender combined) have an additional impact on the rec-\nmovies, as depicted in Fig. 7b.\nommendations produced by the LLM within CLG?\nThese findings reveal cultural stereotypes in LLM-based\nserneGfonoitcarFdezilamroN\nserneGfonoitcarFdezilamroN\noitaRdezilamroN oitaRdezilamroN\nserneGfonoitcarFdezilamroN\nnoitcarFdezilamroN\nnoitcarFdezilamroN\nserneGfonoitcarFdezilamroN\n6\nTo address this, we analyzed the number of recommenda- 1 1\nComedian Age:20\ntions for various genres across different scenarios, observing\n0.8 Writer 0.8 Age:60\nhow biases change with multiple identities. We found signifi-\n0.6 0.6\ncant shifts in overall recommendation patterns when specific\nidentities were added. 0.4 0.4\nFig. 8a illustrates the movie recommender system’s bias. 0.2 0.2\nGenerally, it suggests more romantic movies to females than\n0 0\nCLG CBG CLG CBG\nmales, with a normalized ratio of 0.65 : 0.35. The dif-\n(a) (b)\nference has been enhanced in the case of students, where 1 1\nfemale students receive significantly more romantic movie Age:20 Male\n0.8 Age:60 0.8 Female recommendations than male students (0.88 : 0.12). However,\n0.6 0.6\nin the case of dancers, unlike the overall trend, males and\nfemales receive similar romantic movies recommendations. 0.4 0.4\nSimilarly, Fig. 8b illustrates the book recommender system’s 0.2 0.2\nbias.Generally,itsuggestsfictionbookstofemalesandmales\n0 0\nquite equally. However, male models receive a significantly CLG CBG CLG CBG\n(c) (d)\nhigher number of fiction book recommendations than female\nFig.9:ComparisonofbiasinLLM-basedrecommendationsystemsbetween\nmodels (0.74 : 0.26). Conversely, female podcasters receive theCLGandCBGscenarios,dependingon(a)biography-typebooks,(b)Sci-\nsignificantly more fiction book recommendations than male Fibooksand(c)Jazzsongsand(d)thrillermoviesrecommendations among\ndifferent (a)occupations, (b,c)agesand(d)genders.\npodcasters (0.79 : 0.21). This shows that occupation further\nAnotherexampleinFig.9dshowsthatinCLG,LLM-based\nimpacts gender bias in LLM-based recommendations.\nrecommendations predominantly suggest thriller movies to\nA similar pattern is observed in LLaMA, where its recom-\nmales. However, with different contexts, more thriller movies\nmendation behavior adapts and exhibits distinct biases in the\narerecommendedtofemales.Fig.9ddepictsthischangeinthe\npresenceofintersectingidentities.AsshowninFigs.8cand8d,\nnormalizedfractionratioofthrillermovierecommendationsto\nthe biases between males and females shift noticeably based\nmales and females.\non their occupations. For example, while there is an overall\nnegligible bias in comedy movie recommendations between RQ5: To what extent do LLM-based recommendations\nmales and females, female actors receive substantially more exhibit bias in contextual scenarios associated with CBG?\ncomedy movie recommendations compared to male actors.\nSimilarly,femaledancersarerecommendedsignificantlyfewer To investigate this, we analyze the numbers of recom-\nhorror books compared to their male counterparts. These mendations in different scenario of varying contexts, and\nresults underscore the influence of intersecting identities on observesomeinterestingevents.Forexample,theLLM-based\nLLaMA’s recommendation biases. system suggests blues or classical songs more to introverts\nand HipHop songs more to extroverts, indicating an obvious\nB. Context-based generation (CBG) bias, as shown in Fig. 10a. Furthermore, from Fig. 10b, we\nWenowanalyzeLLM-basedrecommendationswithinCBG noticethatHipHopsongsaremorerecommendedtothemetro\n(context-based generations) and investigate the impact of con- area people, while country songs are more recommended to\ntext compared to CLG. To explore this systematically, we the rural area people.\nstate the following research problems and address them with In addition, as we observe in Fig. 10c, SciFi movies are\nexamples. significantly more recommended to affluent people compared\nto the impoverished ones, whereas dramas are more recom-\nRQ4: What is the impact on the fairness of contextual\nmendedtotheimpoverishedpeople.Moreover,Fig.10dshows\ninformationinLLM-basedrecommendationswhenconsid-\nthat fantasy books are significantly more recommended to\nering CBG, compared to CLG?\naffluent people compared to the impoverished ones, whereas\nWe observe the number of genres recommended (movies, biographiesare more recommendedto the impoverishedones.\nsongs, books) within CBG, similar to CLG cases. First, we These results indicate a considerable bias of the LLM-based\nexplore occupation bias in recommending biographic books. recommendation system depending on the context within\nInCLG,comediansreceivemorebiographicbooksuggestions CBG.\nthanwriters(ratio0.92:0.08).However,with thepresenceof RQ6:Whatistheimpactofthecombinationofcontextual\ndifferentcontextsin CBG, thisratio reducesto 0.79:0.21,as biaswitheitherdemographicbiasorculturalbiasin LLM-\nshown in Fig. 9a. based recommendations?\nNext in Figs. 9b and 9c demonstrate the comparison of\nage bias in LLM-based recommendations for Sci-Fi books To address this question, we first observe the impact of the\nand Jazz songs, respectively. This shows that the bias could combination of demographic bias with the given context, and\nbe enhanced depending on the contexts, e.g., the normalized observethattherecanbeasignificantimpactofthecontextin\nfraction ratio of recommending more Jazz songs between 60 termsoffairness.Forinstance,in Fig.11, we demonstratethe\nand 20 years old people has been changed to 0.8:0.2 within normalizedratioforrecommendednumberofR&Bsongsand\nCBG, compared to 0.6:0.4 in CLG. horror movies for males and females of different personality.\noitaRdezilamroN\noitaRdezilamroN\noitaRdezilamroN\noitaRdezilamroN\n7\n1 1 1 1\nIntrovert MetroArea Affluent Affluent\n0.8 Extrovert 0.8 RuralArea 0.8 Impoverished 0.8 Impoverished\n0.6 0.6 0.6 0.6\n0.4 0.4 0.4 0.4\n0.2 0.2 0.2 0.2\n0 Blues Classical HipHop 0 Country HipHop 0 SciFi Drama 0 Bio Fantasy\n(a) (b) (c) (d)\nFig. 10:BiasintheLLM-based(a-b)song,(c)movie,and(d)bookrecommendationsystemwithinCBGdependingonthecontexts,suchas(a)personality\n(extrovert orintrovert), (b)living area(metroorrural), and(c-d)socioeconomic status(affluent orimpoverished).\n0.6 we observe that the number of recommended mystery books\nMale Extrovert Male Introvert is always significantly lower in the impoverished class com-\nFemale Extrovert Female Introvert pared to the affluent people for both North-American (NAM)\nand South-American (SAM) people. In the comedy-movie\n0.4\nscenario,whiletheremaybealimitedamountofbiasbetween\nthe NAM-impoverishedand SAM-impoverishedpeople, there\nexists a significant bias between the NAM-affluent and SAM-\n0.2\naffluent classes. Thus, Figs. 11 and 12 motivate us to further\ninvestigate the underlying bias within the recommendation\nsystem because of the combination of demographic/cultural\n0 and contextual information.\nR&BSongs HorrorMovies\nFig. 11: Bias in LLM-based recommendation system showing the V. BIAS MITIGATION STRATEGIES\nimpact of the combination of demography and context\nBias mitigation in LLMs encompasses various strategies,\nincluding fairness aware prompt engineering, adversarial de-\nbiasing, and contextual grounding with neutral knowledge\n0.6 SAM Affluent SAM Impoverished\nsources.\nNAM Affluent NAM Impoverished\n0.4 A. Fairness aware prompt engineering\nFairness-aware prompt engineering involves designing\nprompts in a way that minimizes potential biases in LLM\n0.2\nresponses. The technique focuses on carefully crafting input\nprompts to reduce the model’s propensity to generate biased\nor skewed outputs. This can include explicitly instructing\n0\nthe model to prioritize fairness, diversity, or neutrality when\nMysteryBooks ComedyMovies\nrespondingto user queries. For instance, when generating the\nFig. 12: Bias in LLM-based recommendation system showing the impact\nrecommendations, prompts can be designed to explicitly in-\nof the combination of culture and context. In this example, we show the\ncomparison between South-American (SAM) and North-American (NAM) structthemodeltoensurethatrecommendationsareinclusive\npeoplewhilethecontextincludes whethertheyareaffluentorimpoverished. of various demographic and cultural groups.\nThe advantageof fairness-aware promptengineeringlies in\nits straightforwardimplementation,allowing users to leverage\nFirst we observe that, the LLM recommends more R&B\nexisting models without retraining or extensive modifications.\nsongs to femalescomparedto males. Additionally,it suggests\nIt can also be tailored dynamically, adjusting prompts to fit\nmore R&B songs to the extroverts compared to introverts\nspecific contexts or fairness goals. However, the effectiveness\nfor both males or females. This shows a clear impact of an\nof this strategy depends heavily on the design of the prompts\nadditional bias even in the presence of demographic bias. In\nand the model’s inherent biases from its training data.\nthe horror movies scenario, while we observe very little bias\nbetweenfemaleextrovertsandfemaleintroverts,thereexistsa\nconsiderablebiasbetweenmaleextrovertsandmaleintroverts. B. Adversarial Debiasing\nThus Fig. 11 clearly depicts scenarios which demonstrate Adversarial debiasing involves training a secondary adver-\nthe impact of the contexts in the presence of demographic sarial model to detect and mitigate biases in the outputs of\ninformation. the LLM. This technique is particularly effective in scenarios\nNext in Fig. 12, we demonstrate the impact of the social where sensitive attributes may influence the recommendations\nstatus contextin the presence of cultural/regionalinformation. generated by the model. For example, in the previous dis-\nWe again consider two different scenarios which include the cussion we saw that LLM disproportionately recommend a\nrecommendationof mystery books and comedy movies. First, highernumberofromanticmoviestowomenwhilesuggesting\nserneGfonoitcarFdezilamroN\noitaR\ndezilamroN\noitaR\ndezilamroN\nserneGfonoitcarFdezilamroN serneGfonoitcarFdezilamroN serneGfonoitcarFdezilamroN\n8\nDoesLLM-basedrecommendation ModelPerformance\nthriller genres to men (see Figure 4a), even when their stated\nsystemsuggestmore Acc SPD EOD DI\npreferencesareidentical.Toaddressthis,theadversarialmodel (1)Romanticmoviesto\n75.3 0.36 0.83 0.56\nis trained to identify patterns where the output (e.g., the females comparedtomales?\nnumberofromanticmoviesrecommended)isbiasedbasedon (2)Biography books 100 1.00 1.00 0.00\ntocomedians thanwriters?\nthe gender attribute. The adversarial model flags these cases,\n(3)Historic fictionbooksto\n95.0 0.90 0.90 0.00\npromptingtheLLMto adjustits recommendationstobe more writers thanentrepreneurs?\nbalanced.By incorporatingthisfeedbackloop,thesystemcan (4)Hip-hopsongsto20-year 76.7 0.29 0.88 0.67\npeople thanthe60-yearones?\nbe optimized to ensure that the number of romantic movie\n(5)Fictonbooksto\n91.0 0.82 0.86 0.05\nrecommendationsdoes not vary unfairly based on gender. writersthancomedians?\nThe advantage of adversarial debiasing is its ability to (6)SciFimoviestoNorth\n100 0.93 0.97 0.03\nAmericans thanSouth-Asians?\naddress subtle biases that may not be immediately apparent\n(7)EDMsongstoOceanians\nthrough prompt engineering. However, it can be resource- peoplethanEast-Asians? 86.7 0.50 1.00 0.50\nintensiveas it requirestrainingand fine-tuningthe adversarial (8)MysterybookstoEast\n100 0.83 0.90 0.07\nEuropeansthanNorth-Africans?\nmodel to detect and correct for these biases effectively.\n(9)MysterybookstoSouthAmerican\n100 1.00 1.00 0.00\npeoplethanSub-SaharanAfricans?\nC. Contextual grounding with neutral knowledge sources (10)Classical songstoEastEuropean 76.7 0.53 0.97 0.45\npeoplethanSub-SaharanAfricans?\nContextual grounding with neutral knowledge sources in-\nTABLE IV: Modelperformanceandfairness metricsfordemographic\nvolves anchoring LLM responses with unbiased, authoritative\nandcultural biastoaddressdifferent fairness related questions for\ninformation to ensure fairness and accuracy. In the context Context-less Generations withinaLLM\nof generating recommendations, this approach uses a curated\nknowledge base to cross-check recommendations, ensuring An SPD of zero indicates complete fairness, meaning that\ntheyare inclusiveof variousdemographicandculturalgroups. the model does not favor one group over another in terms of\nByleveragingunbiaseddatasourcesasacontextualreference, favorable outcomes.\nthe LLM can mitigate the influence of any biased patterns DisparateImpact(DI),whichmeasurestheratiooffavorable\npresentin itsoriginaltrainingdata, producingoutputsthatare outcomeprobabilitiesbetweengroups,isexpressedasfollows:\nmore aligned with principles of inclusivity.\nP(Yˆ =1(cid:12) (cid:12)Z =Q)\nThekeybenefitofthisstrategyisthatitenhancesthecoher- DI = . (2)\nenceand fairnessof responseswithoutalteringtheunderlying P(Yˆ =1(cid:12) (cid:12)Z =Q¯)\nmodel architecture. By grounding responses in neutral, fact- A DI of one signifies complete fairness, indicating that both\nchecked information, this approach can provide more consis- groups have an equal proportion of favorable outcomes.\ntentandtrustworthyrecommendationsacrossdiversecontexts. Equal Opportunity Difference (EOD) evaluateswhetherthe\nHowever,itrequirescarefulcurationoftheknowledgebaseto probability of receiving a favorable outcome given the true\nensure neutrality and ongoing updates to maintain relevance. positive label (Y = 1) is the same for different groups. It is\ncalculated as follows:\nVI. NUMERICAL RESULTS\nEOD=P(Yˆ =1(cid:12) (cid:12)Z =Q,Y =1)\nInthissection,wediscussvariousbiasmetricsemployedin −P(Yˆ =1(cid:12) (cid:12)Z =Q¯,Y =1). (3)\nfairness analysis and detail how these metrics quantify biases\nacross several fairness-related scenarios. We explore different An EOD of zero suggests complete fairness.\nmethodologies for measuring these biases, providing a com-\nprehensive framework for evaluating fairness. This approach B. Quantifying Bias in Context-less Generations (CLG)\nallows us to address a rangeof questionsconcerningbias and We begin by addressing several fairness-related questions\nits implications in data-driven models. (FQs)andapplythebiasmetricsdiscussedabovetoassessthe\ndemographic and cultural biases in context-less generations\nA. Fairness Metrics (CLG) by GPT-3.5. To evaluate the system’s performance\nin addressing these FQs, we employ Random Forest (RF)\nWe begin by analyzing three fairness measures: Statistical\n[48] classifier models(with a 75%-25%training-testingsplit),\nParity Difference (SPD), Disparate Impact (DI), and Equal\nwhich deliver notably high accuracy. A summary of the\nOpportunityDifference (EOD) [47], to quantify bias in LLM-\nquestionsandcorrespondingmetricvaluesisprovidedinTable\nbased recommendations. Let us consider a dataset D =\nIV.\n(X,Y,Z), where X represents the training data, Y denotes\nWhile we have several FQs stated and evaluated in Table\nthe binaryclassification labels, and Z is thesensitive attribute\nIV, we choose an example to describe the overall process\nsuchasethnicity.Additionally,thepredictedlabelisindicated\nby Yˆ. and the corresponding inherent bias. For instance, we choose\nthe FQ-5, whether the LLM-based recommendation system\nStatistical Parity Difference (SPD) assesses whether the\nprobability of receiving a favorable outcome (Yˆ = 1) is the suggests more fiction books to writers than comedians. We\nare motivated to this FQ by our analysis in Fig 4c, where we\nsame for different groups. Mathematically, it is defined as\nobserve that writers receive a significantly higher number of\nSPD=P(Yˆ =1(cid:12) (cid:12)Z =Q)−P(Yˆ =1(cid:12) (cid:12)Z =Q¯). (1) fiction books than comedians.\n9\nTable IV shows that the classifier can distinguish between from 0.43 (in CLG), which usually needs to be around zero\nwriters and comedians with 91% accuracy based solely on for a fair (very low or zero bias) scenario. Additionally, the\nthe number of recommended fiction books. Furthermore, for DI value has been reduced to 0.12 from 0.50, which needs to\nthe recommendation process to be considered fair, both bias be around one for a fair case.\nmetrics, SPD and EOD, should ideally be close to zero. WhilewehavedescribedanexamplewithFQ-(4),theother\nHowever, for FQ-5, SPD and EOD are 0.82 and 0.86, re- FQs also exhibit significant bias depending on the contexts\nspectively. Additionally, the DI metric, which should be one in the prompts. We have observed major changes in the bias\nfor a perfectly fair recommendation, is 0.05 in this case. metrics from the corresponding CLG to the CBG cases. For\nTherefore, we conclude that the LLM-based recommendation example, in FQ-(3), SPD value has increased from 0.267\nsystem exhibits significant bias within this FQ. (CLG) to 0.89 (CBG), which is caused by the presence of\nWe alsoobservesimilartrendsindifferentotherFQswhich thecontextonwhetherthecorrespondingpersonis affluentor\ninvolvevariousgenresofbooks,moviesorsongs.Forexample, impoverished. In FQ-(1), the EOD value has increased from\nFQ-6 addresses the question regardingrecommendingscience 0.76 (CLG) to 0.87 (CBG) because of the context. Similarly\nfiction (Sci-Fi) movies between North American and South in FQ-(9), the DI value has decreased from 0.07 (CLG) to\nAsian people. The classifier can differentiatethem with 100% 0.00 (CBG) depending on the context of personality.\naccuracy while providing SPD = 0.93, EOD = 0.97 and DI\nVII. CONCLUSION AND FUTURE WORKS\n= 0.07, which again indicates a significant bias. In summary,\nIn this paper, we have identified and highlighted the per-\nwe address different fairness-related questions (FQs) in Table\nsistence of demographic and cultural biases in LLM-based\nIV which demonstrates high performance of the models (up\nrecommendationsystems.By formulatingandansweringa set\nto 100% accuracy) along with considerable bias (SPD up to\nofresearchquestions,wehaveadditionallyuncoveredinsights\n1.0, EOD up to 1.0, and DI to be even 0.0).\nsuch as how intersecting identities can exacerbate bias, and\nhow contextual factors, such as living in a metro or impover-\nC. Quantifying Bias in Context-based Generations (CBG) ishedarea,significantlyimpactbiasedoutcomes.Furthermore,\nthe combination of demographic and contextual information\nNow we quantify the bias metrics in the presence of\nwas found to intensify these biases. Our analysis, using state-\ndifferent contexts along with the demographic and cultural\nof-the-art fairness metrics, demonstrated clear disparities in\ninformation. As discussed in Sec. III-A2, we systematically\ndifferent categories of recommendations which resulted in a\nincludecontextsinthepromptsalongthreedifferentdirections:\nperfect unfairness score, with up to 100% classifier accuracy,\nwhether the person is affluent or impoverished, whether the\nupto1.00forbothSPDandEOD,and0.00forDIindifferent\npersonresidesinametroorruralarea,andwhethertheperson\ncases.Moreover,whencontexts,suchassocioeconomicstatus\nis introvertor extrovert.Building on the CLG frameworkout-\nor the living area, were included, the corresponding bias has\nlinedinSectionVI-B,weemployRandomForestclassifiersto\nfurtherbeenincreased,reflectedbylowerDIvaluesandhigher\ntrain ourmodelsunderthese variedconditions.Thisapproach\nSPD and EOD values.\nallows us to notonly gaugethe model’sperformancebut also\nOur work can be extended to several key areas. This study\nto scrutinize the bias metrics more closely. The outcomes\nfocused on binary gender groups and limited demographic\nof these assessments are systematically presented in Table\nfactors, which may not fully capture real-world diversity. Fu-\nV. Additionally, we integrate corresponding CLG scenarios\nture research could explore fairness across a broader range of\nenabling us to observe the influence of the contexts on the\nracial, ethnic, and socio-economicgroups. While we centered\nFairness Quotients (FQs) under study. This comprehensive\nonGPT-3.5andLlama3.18Bfortheiraccessibility,expanding\nevaluation helps illuminate how different contexts impact the\nto models with multimodal capabilities may improve general-\npredictive behavior and fairness of the models.\nizability. Additionally, investigating bias mitigation strategies\nWepresentseveralFQsinTableVderivedfromouranalysis.\ncould providerobust solutions to enhance fairness and reduce\nTo better illustrate the concept of inherent bias within these\nbias in LLM-based recommendation systems.\nmetrics, we select a specific example from the table. For\ninstance, we choose the FQ-(4), whether the LLM-based\nREFERENCES\nrecommendationsystem suggests more drama-type movies to\nNorth-American introvert people than East Asian extrovert [1] Z. Dong, X. Liu, B. Chen, P. Polak, and P. Zhang, “Musechat: A\nconversational music recommendation system for videos,” in Proc. of\nones. First, we notice that the classifier can separate North-\ntheIEEE/CVFConferenceonComputerVisionandPatternRecognition,\nAmericans and East Asians based on only the drama-type 2024,pp.12775–12785.\nmovieswithanaccuracy71.67%intheabsenceofthecontext [2] N. Neophytou, B. Mitra, and C. Stinson, “Revisiting popularity and\ndemographic biases in recommender evaluation and effectiveness,” in\nin the prompts (CLG).\nEurop.Conf.Info.Retriev. Springer, 2022,pp.641–654.\nNext, when the prompts include the contexts, we observe [3] M. D. Ekstrand, M. Tian, I. M. Azpiazu, J. D. Ekstrand, O. Anuyah,\nthattheRandomForestclassifiercanseparateNorth-American D. McNeill, and M. S. Pera, “All the cool kids, how do they fit\nin?:Popularityanddemographicbiasesinrecommenderevaluation and\nintrovertpeopleandEastAsianextrovertpeoplewithaconsid-\neffectiveness,” inConf.fairness,account. transp. PMLR,2018.\nerablyhigheraccuracy,92.50%.We also observea significant [4] A.LambrechtandC.Tucker, “Algorithmic bias?anempirical studyof\nchange in the bias metrics values, which indicates the impact apparentgender-baseddiscriminationinthedisplayofstemcareerads,”\nManagement science, vol.65,no.7,pp.2966–2981, 2019.\nof the added contexts (related to personality) in terms of bias.\n[5] A.Datta,M.C.Tschantz,andA.Datta,“Automatedexperimentsonad\nFor example, SPD value has been increased to 0.77 (in CBG) privacy settings,” Proc.onPrivacyEnhancingTechnologies, 2015.\n10\nCBG(includes thecontexts) CLG(absence ofthecontext)\nIndex DoesLLM-basedrecommendation systemsuggest\nAcc SPD EOD DI Acc SPD EOD DI\nMoreromantic moviestoaffluentwriters\nFQ(1) 95.50 0.87 0.87 0.006 83.00 0.66 0.76 0.132\nthantheimpoverished athletes?\nMorePopsongsto20yextrovert\nFQ(2) 83.13 0.66 0.79 0.17 65.83 0.70 1.00 0.30\npeoplethan60yintrovert people?\nMorereggaesongstoimpoverished Sub-Saharan\nFQ(3) 97.50 0.89 0.95 0.066 63.33 0.267 1.00 0.733\nAfricanpeoplethanaffluentNorthAmericans?\nMoredrama-typemoviestoNorthAmerican\nFQ(4) 92.50 0.77 0.87 0.12 71.67 0.433 0.867 0.50\nintrovert peoplethanEast-Asianextrovert ones?\nMorefictionbookstoSouth-Asianmetro\nFQ(5) 93.33 0.78 0.82 0.04 83.33 0.667 0.80 0.167\nareapeoplethantheOcenian ruralones?\nMorebiographic bookstoAffluent\nFQ(6) 88.00 0.69 0.83 0.33 55.00 0.10 0.96 0.896\nactors thanimpoverished dancers?\nMorerocksongstorural\nFQ(7) 89.50 0.75 0.86 0.13 67.00 0.34 0.90 0.62\nmusicians thanthemetroareadancers?\nMoremysterybookstoSouth-Asianimpoverished\nFQ(8) 96.67 0.95 0.98 0.03 91.67 0.833 0.867 0.038\npeoplethantheSouth-American Affluentones?\nMorepopsongstoEastAsianextrovert\nFQ(9) 95.83 0.91 0.98 0.07 91.67 0.833 0.833 0.00\npeople thantheSub-Saharanintrovert ones?\nTABLE V: Valuesoffairness metrics fordifferent FQsinCBG,andthecorresponding CLG(whichdonotinclude thecontexts intheprompts).The\nchanges inthebiasmetrics values depicttheimpactofthecontexts inbias.\n[6] M. Mansoury, H. Abdollahpouri, M. Pechenizkiy, B. Mobasher, and “Queensarepowerfultoo:Mitigatinggenderbiasindialoguegeneration,”\nR. Burke, “Feedback loop and bias amplification in recommender preprint,arXiv:1911.03842, 2019.\nsystems,” in Proc. of Intl. Conf. Inf. Knowl. Manag., 2020, pp. 2145– [23] M. Szumilas, “Explaining odds ratios,” Journal of the Canadian\n2148. academyofchildandadolescentpsychiatry,vol.19,no.3,p.227,2010.\n[7] H.Abdollahpouri, M.Mansoury, R.Burke, B. Mobasher, andE.Malt- [24] J.SunandN.Peng,“Menareelected,womenaremarried:Eventsgender\nhouse, “User-centered evaluation of popularity bias in recommender biasonwikipedia,” preprint,arXiv:2106.01601, 2021.\nsystems,” in Proc. of the 29th ACM conference on user modeling, [25] Y. T. Cao, Y. Pruksachatkun, K.-W. Chang, R. Gupta, V. Kumar,\nadaptation andpersonalization, 2021,pp.119–129. J. Dhamala, and A. Galstyan, “On the intrinsic and extrinsic fairness\n[8] H. Abdollahpouri, R. Burke, and B. Mobasher, “Managing popularity evaluationmetricsforcontextualizedlanguagerepresentations,”preprint,\nbias in recommender systems with personalized re-ranking,” preprint, arXiv:2203.13928, 2022.\narXiv:1901.07555, 2019. [26] U. Gupta, J. Dhamala, V. Kumar, A. Verma, Y. Pruksachatkun, S. Kr-\n[9] N. Kordzadeh and M. Ghasemaghaei, “Algorithmic bias: review, syn- ishna,R.Gupta,K.-W.Chang,G.V.Steeg,andA.Galstyan,“Mitigating\nthesis,andfutureresearchdirections,”EuropeanJournalofInformation genderbiasindistilledlanguagemodelsviacounterfactualrolereversal,”\nSystems,vol.31,no.3,pp.388–409,2022. preprint,arXiv:2203.12574, 2022.\n[10] Y.Wan,G.Pu,J.Sun,A.Garimella,K.-W.Chang,andN.Peng,“\"kelly [27] A. Wang, V. V. Ramaswamy, and O. Russakovsky, “Towards inter-\nis a warm person, joseph is a role model\": Gender biases in LLM- sectionality in machine learning: Including more identities, handling\ngenerated reference letters,” preprint,arXiv:2310.09219, 2023. underrepresentation, andperformingevaluation,” inProc.ofACMConf.\n[11] F. M. Plaza-del Arco, A. C. Curry, A. Curry, G. Abercrombie, and onFairness,Accountability, andTransparency, 2022,pp.336–349.\nD.Hovy,“Angrymen,sadwomen:Largelanguage modelsreflectgen- [28] A. Ovalle, A. Subramonian, V. Gautam, G. Gee, and K.-W. Chang,\ndered stereotypes in emotion attribution,” preprint, arXiv:2403.03121, “Factoringthematrixofdomination:Acriticalreviewandreimagination\n2024. ofintersectionalityinaifairness,”inProc.ofAAAI/ACMConferenceon\n[12] T.Naous,M.J.Ryan,andW.Xu,“Havingbeerafterprayer?measuring AI,Ethics,andSociety, 2023,pp.496–511.\ncultural bias in large language models,” preprint, arXiv:2305.14456, [29] S. Khan, A. Kirubarajan, T. Shamsheri, A. Clayton, and G. Mehta,\n2023. “Gender bias in reference letters for residency and academic medicine:\n[13] J.Zhang,K.Bao,Y.Zhang,W.Wang,F.Feng,andX.He,“Ischatgpt asystematicreview,” Postgrad.med.jour.,vol.99,pp.272–278, 2023.\nfair for recommendation? evaluating fairness in large language model [30] K. Hämmerl, B. Deiseroth, P. Schramowski, J. Libovicky`, C. A.\nrecommendation,” inProc.ofACMConf.Recommend.Systems,2023. Rothkopf, A. Fraser, and K. Kersting, “Speaking multiple languages\n[14] C. Xu, W. Wang, Y. Li, L. Pang, J. Xu, and T.-S. Chua, “Do LLMs affectsthemoralbiasoflanguagemodels,”preprint,arXiv:2211.07733,\nimplicitly exhibit userdiscrimination inrecommendation? anempirical 2022.\nstudy,”preprint, arXiv:2311.07054, 2023. [31] C. Xu, S. Chern, E. Chern, G. Zhang, Z. Wang, R. Liu, J. Li, J. Fu,\n[15] C. K. Sah, D. L.Xiaoli, and M. M.Islam, “Unveiling bias infairness andP.Liu,“Alignonthefly:Adapting chatbotbehavior toestablished\nevaluations of large language models: A critical literature review of norms,”preprint,arXiv:2312.15907, 2023.\nmusicandmovierecommendationsystems,”preprint,arXiv:2401.04057, [32] A.Ramezani andY.Xu,“Knowledge ofcultural moralnormsinlarge\n2024. language models,”preprint,arXiv:2306.01857, 2023.\n[16] S. L.Blodgett, S.Barocas, H. DauméIII, and H. Wallach, “Language [33] M. Abdulhai, G. Serapio-Garcia, C. Crepy, D. Valter, J. Canny, and\n(technology) is power: A critical survey of \"bias\" in NLP,” preprint, N. Jaques, “Moral foundations of large language models,” preprint,\narXiv:2005.14050, 2020. arXiv:2310.15337, 2023.\n[17] A.Wang,S.Barocas,K.Laird,andH.Wallach,“Measuringrepresenta- [34] R. L. Johnson, G. Pistilli, N. Menédez-González, L. D. D. Duran,\ntional harmsinimagecaptioning,” inProc.oftheACMConference on E. Panai, J. Kalpokiene, and D. J. Bertulfo, “The ghost in the ma-\nFairness,Accountability, andTransparency, 2022,pp.324–335. chine has an american accent: value conflict in GPT-3,” preprint,\n[18] S.Dev,E.Sheng,J.Zhao,A.Amstutz,J.Sun,Y.Hou,M.Sanseverino, arXiv:2203.07785, 2022.\nJ.Kim,A.Nishi,N.Pengetal.,“Onmeasuresofbiasesandharmsin [35] Y.Cao,L.Zhou,S.Lee,L.Cabello,M.Chen,andD.Hershcovich,“As-\nNLP,”preprint,arXiv:2108.03362, 2021. sessing cross-cultural alignment between chatgpt and human societies:\n[19] S.BordiaandS.R.Bowman,“Identifying andreducinggenderbiasin Anempirical study,”preprint, arXiv:2303.17466, 2023.\nword-level language models,”preprint,arXiv:1904.03035, 2019. [36] A. Arora, L.-A. Kaffee, and I. Augenstein, “Probing pre-trained\n[20] E. Sheng, K.-W. Chang, P. Natarajan, and N. Peng, “Societal language models for cross-cultural differences in values,” preprint,\nbiases in language generation: Progress and challenges,” preprint, arXiv:2203.13722, 2022.\narXiv:2105.04054, 2021. [37] S. Feng, C. Y. Park, Y. Liu, and Y. Tsvetkov, “From pretraining data\n[21] ——, ““Nice try, kiddo”: Investigating ad hominems in dialogue re- tolanguagemodelstodownstreamtasks:Trackingthetrailsofpolitical\nsponses,”preprint,arXiv:2010.12820, 2020. biasesleadingtounfairNLPmodels,”preprint,arXiv:2305.08283,2023.\n[22] E. Dinan, A. Fan, A. Williams, J. Urbanek, D. Kiela, and J. Weston, [38] W.Wang,W.Jiao,J.Huang,R.Dai,J.-t.Huang,Z.Tu,andM.R.Lyu,\n11\n“Notallcountries celebrate thanksgiving: Onthecultural dominancein\nlargelanguage models,”preprint,arXiv:2310.12481, 2023.\n[39] R. I. Masoud, Z. Liu, M. Ferianc, P. Treleaven, and M. Rodrigues,\n“Cultural alignment inlarge language models: Anexplanatory analysis\nbased on hofstede’s cultural dimensions,” preprint, arXiv:2309.12342,\n2023.\n[40] T.-P. Nguyen, S. Razniewski, A. Varde, and G. Weikum, “Extracting\ncultural commonsense knowledge at scale,” in Proc. of the ACM Web\nConference, 2023,pp.1907–1917.\n[41] S.PaltaandR.Rudinger,“Fork:Abite-sizedtestsetforprobingculinary\nculturalbiasesincommonsensereasoningmodels,”inFindingsofAssoc.\nComput.Linguist.:ACL,2023,pp.9952–9962.\n[42] J. Huang and D. Yang, “Culturally aware natural language inference,”\ninFindingsofAssoc.Comput.Linguist.:EMNLP,2023,pp.7591–7609.\n[43] T. B. Brown, “Language models are few-shot learners,” preprint,\narXiv:2005.14165, 2020.\n[44] M.AI,“LLaMA3.18Bmodel,”2024,accessed: 2024-09-08.[Online].\nAvailable: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n[45] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\nN.Bashlykov,S.Batra,P.Bhargava,S.Bhosaleetal.,“Llama2:Open\nfoundation and fine-tuned chat models,” preprint, arXiv:2307.09288,\n2023.\n[46] S. Kullback and R. A. Leibler, “On information and sufficiency,” The\nannals ofmathematical statistics, vol.22,no.1,pp.79–86,1951.\n[47] R.K.Bellamy,K.Dey,M.Hind,S.C.Hoffman,S.Houde,K.Kannan,\nP. Lohia, J. Martino, S. Mehta, A. Mojsilovic´ et al., “AI fairness 360:\nAnextensibletoolkitfordetectingandmitigatingalgorithmicbias,”IBM\nJournalofResearchandDevelopment, vol.63,no.4/5,pp.4–1,2019.\n[48] Z.Chen,J.M.Zhang,F.Sarro,andM.Harman,“Acomprehensiveem-\npiricalstudyofbiasmitigationmethodsformachinelearningclassifiers,”\nACMTransactions onSoftware Engineering andMethodology, vol.32,\nno.4,pp.1–30,2023.",
    "pdf_filename": "Unveiling_and_Mitigating_Bias_in_Large_Language_Model_Recommendations_A_Path_to_Fairness.pdf"
}