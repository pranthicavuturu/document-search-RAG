{
    "title": "Unveiling and Mitigating Bias in Large Language Model Recommendations A Path to Fairness",
    "context": "dation systems provide more comprehensive recommendations than traditional systems by deeply analyzing content and user behavior. However, these systems often exhibit biases, favoring mainstream content while marginalizing non-traditional options due to skewed training data. This study investigates the intricate relationship between bias and LLM-based recommendation sys- tems, with a focus on music, song, and book recommendations across diverse demographic and cultural groups. Through a com- prehensive analysis conducted over different LLM-models, this paper evaluates the impact of bias on recommendation outcomes. Our ﬁndings highlight that biases are not only deeply embedded but also widely pervasive across these systems, emphasizing the substantial and widespread nature of the issue. Moreover, contextual information, such as socioeconomic status, further amplify these biases, demonstrating the complexity and depth of the challenges faced in creating fair recommendations across different groups. Index Terms—Large Language Models, Bias in Recommenda- tion Systems, Fairness in AI Recommendations, Demographic and Cultural Bias, Fairness Metrics. Consider an LLM-based music recommendation system, such as MuseChat [1], that enhances user experience by lever- aging the advanced capabilities of large language models. Tra- ditional algorithms typically rely on user listening history and genre preferences. In contrast, an LLM-based system delves deeper into musical content and user behavior. For example, a user who frequently listens to progressive and alternative rock would beneﬁt from recommendations generated through a comprehensive analysis of genres like psychedelic rock. By considering lyrical themes, musical styles, and emotional tones, the system can suggest tracks from emerging artists in related rock genres, showcasing the nuanced and highly personalized recommendations LLMs can provide. However, such a personalized recommendation system has drawbacks. Users from Western countries may predominantly receive recommendations for mainstream Western genres like pop or rock, while underrepresented genres, such as traditional indigenous music or world music, receive limited exposure. This bias stems from training data skewed towards popular Western music. Thus, bias in recommendation systems has emerged as a critical concern, impacting fairness, diversity, and societal equity. Demographic and cultural biases have been widely observed in recommendation systems. Studies by Neophytou et al. [2] and Ekstrand et al. [3] have explored how demographic and cultural factors inﬂuence the variability in recommendations. For instance, Neophytou et al. [2] found that the performance of recommender systems consistently declines for older users, with female users also experiencing lower utility compared to their male counterparts. These biases can have tangible real-world consequences, as evidenced by Lambrecht and Tucker [4] and Datta et al. [5], who demonstrated that women may receive fewer recommendations for high-paying jobs and career coaching services compared to men. While bias in traditional systems has been extensively studied [6], [7], [8], [9], integrating LLMs introduces new challenges. Due to their massive scale and ability to learn intricate patterns from vast datasets, LLMs can amplify exist- ing biases, leading to skewed recommendations that perpetuate societal inequalities. Recent studies have critically examined the performance and fairness of LLM-based recommendation systems. Wan et al. [10] and Plaza-del-Arco et al. [11] ana- lyzed gender biases in reference letters and emotion attribution, revealing signiﬁcant gendered stereotypes. Naous et al. [12] highlighted cultural biases in multilingual LLMs, while Zhang et al. [13] found that music and movie recommendations can perpetuate existing biases. Xu et al. [14] studied implicit user unfairness, and Sah et al. [15] explored personality proﬁling to enhance fairness. However, these studies often focus on speciﬁc biases or contexts, underscoring the need for a comprehensive approach to address the multifaceted nature of biases in LLM-based recommendation systems. This paper aims to address the limitations of previous studies by exploring the intricate relationship between bias and LLM-based recommendation systems, shedding light on the underlying mechanisms that contribute to bias propagation and its implications for users and society at large. In doing so, it provides a deeper insight into the complexities and challenges associated with these technologies. The rest of the paper is organized as follows: Sec. II provides an overview of LLM-based recommendation systems and our problem formulation. Sec. III details the synthesis of experimental data using LLMs, including our prompt design for obtaining responses and the methodology for genre classi- ﬁcation. Sec. IV includes an in-depth analysis of LLM biases, presenting both qualitative and quantitative insights by analyz- ing LLM recommendations through a set of research questions. Sec. VI introduces the questions used to measure fairness metrics in both context-less generation (CLG) and context-",
    "body": "arXiv:2409.10825v2  [cs.IR]  19 Nov 2024\n1\nUnveiling and Mitigating Bias in Large Language\nModel Recommendations: A Path to Fairness\nShahnewaz Karim Sakib∗and Anindya Bijoy Das†\n∗Computer Science and Engineering, University of Tennessee at Chattanooga, Chattanooga, TN 37403, USA\n†Electrical and Computer Engineering, The University of Akron, Akron, OH 44325, USA\nshahnewazkarim-sakib@utc.edu and adas@uakron.edu\nAbstract—Large Language Model (LLM)-based recommen-\ndation systems provide more comprehensive recommendations\nthan traditional systems by deeply analyzing content and user\nbehavior. However, these systems often exhibit biases, favoring\nmainstream content while marginalizing non-traditional options\ndue to skewed training data. This study investigates the intricate\nrelationship between bias and LLM-based recommendation sys-\ntems, with a focus on music, song, and book recommendations\nacross diverse demographic and cultural groups. Through a com-\nprehensive analysis conducted over different LLM-models, this\npaper evaluates the impact of bias on recommendation outcomes.\nOur ﬁndings highlight that biases are not only deeply embedded\nbut also widely pervasive across these systems, emphasizing\nthe substantial and widespread nature of the issue. Moreover,\ncontextual information, such as socioeconomic status, further\namplify these biases, demonstrating the complexity and depth\nof the challenges faced in creating fair recommendations across\ndifferent groups.\nIndex Terms—Large Language Models, Bias in Recommenda-\ntion Systems, Fairness in AI Recommendations, Demographic\nand Cultural Bias, Fairness Metrics.\nI. INTRODUCTION\nConsider an LLM-based music recommendation system,\nsuch as MuseChat [1], that enhances user experience by lever-\naging the advanced capabilities of large language models. Tra-\nditional algorithms typically rely on user listening history and\ngenre preferences. In contrast, an LLM-based system delves\ndeeper into musical content and user behavior. For example,\na user who frequently listens to progressive and alternative\nrock would beneﬁt from recommendations generated through\na comprehensive analysis of genres like psychedelic rock.\nBy considering lyrical themes, musical styles, and emotional\ntones, the system can suggest tracks from emerging artists\nin related rock genres, showcasing the nuanced and highly\npersonalized recommendations LLMs can provide. However,\nsuch a personalized recommendation system has drawbacks.\nUsers from Western countries may predominantly receive\nrecommendations for mainstream Western genres like pop\nor rock, while underrepresented genres, such as traditional\nindigenous music or world music, receive limited exposure.\nThis bias stems from training data skewed towards popular\nWestern music. Thus, bias in recommendation systems has\nemerged as a critical concern, impacting fairness, diversity,\nand societal equity.\nDemographic and cultural biases have been widely observed\nin recommendation systems. Studies by Neophytou et al. [2]\nand Ekstrand et al. [3] have explored how demographic and\ncultural factors inﬂuence the variability in recommendations.\nFor instance, Neophytou et al. [2] found that the performance\nof recommender systems consistently declines for older users,\nwith female users also experiencing lower utility compared\nto their male counterparts. These biases can have tangible\nreal-world consequences, as evidenced by Lambrecht and\nTucker [4] and Datta et al. [5], who demonstrated that women\nmay receive fewer recommendations for high-paying jobs and\ncareer coaching services compared to men.\nWhile bias in traditional systems has been extensively\nstudied [6], [7], [8], [9], integrating LLMs introduces new\nchallenges. Due to their massive scale and ability to learn\nintricate patterns from vast datasets, LLMs can amplify exist-\ning biases, leading to skewed recommendations that perpetuate\nsocietal inequalities. Recent studies have critically examined\nthe performance and fairness of LLM-based recommendation\nsystems. Wan et al. [10] and Plaza-del-Arco et al. [11] ana-\nlyzed gender biases in reference letters and emotion attribution,\nrevealing signiﬁcant gendered stereotypes. Naous et al. [12]\nhighlighted cultural biases in multilingual LLMs, while Zhang\net al. [13] found that music and movie recommendations can\nperpetuate existing biases. Xu et al. [14] studied implicit\nuser unfairness, and Sah et al. [15] explored personality\nproﬁling to enhance fairness. However, these studies often\nfocus on speciﬁc biases or contexts, underscoring the need for\na comprehensive approach to address the multifaceted nature\nof biases in LLM-based recommendation systems.\nThis paper aims to address the limitations of previous\nstudies by exploring the intricate relationship between bias and\nLLM-based recommendation systems, shedding light on the\nunderlying mechanisms that contribute to bias propagation and\nits implications for users and society at large. In doing so, it\nprovides a deeper insight into the complexities and challenges\nassociated with these technologies.\nThe rest of the paper is organized as follows: Sec. II\nprovides an overview of LLM-based recommendation systems\nand our problem formulation. Sec. III details the synthesis of\nexperimental data using LLMs, including our prompt design\nfor obtaining responses and the methodology for genre classi-\nﬁcation. Sec. IV includes an in-depth analysis of LLM biases,\npresenting both qualitative and quantitative insights by analyz-\ning LLM recommendations through a set of research questions.\nSec. VI introduces the questions used to measure fairness\nmetrics in both context-less generation (CLG) and context-\n\n2\nbased generation (CBG) frameworks, and presents numerical\nresults to quantify fairness. Finally, Sec. VII presents future\ndirections and key insights for practitioners and researchers.\nII. BACKGROUND AND PROBLEM FORMULATION\nIn this study, we deﬁne bias in LLM-generated recom-\nmendations by examining the distribution of recommended\ngenres across distinct user groups. A signiﬁcant difference in\ngenre distribution between groups indicates potential system\nbias. Speciﬁcally, if recommendations are inﬂuenced by user\nattributes such as age, gender, or occupation, we classify\nthis as demographic bias. Conversely, if recommendations are\nshaped by variations in values, customs, or social norms across\ngroups, we classify this as cultural bias.\nA. Related Works\nResearch on social biases in NLP models distinguishes\nbetween allocational and representational harms [16], [17].\nStudies focus on evaluating and mitigating biases in natural\nlanguage understanding [18], [19] and generation tasks [20],\n[21], [22]. Metrics like the Odds Ratio (OR) [23] measure\ngender biases in items with large frequency differences [24].\nControlling natural language generation model biases has been\nexplored [25], [26], but applicability to closed API-based\nLLMs is uncertain. Emphasizing social and technical aspects\nis crucial for understanding bias sources [27], [28]. Social\nscience research highlights the detrimental effects of gender\nbiases in professional documents, underscoring the need for\ngrounded bias deﬁnitions and metrics [29].\nSigniﬁcant work has also analyzed cultural bias in language\nmodels (LMs). Recent studies have explored cultural align-\nment by examining encoded moral knowledge and cultural\nvariations in moral judgments [30], [31], [32]. LMs often\nreﬂect the moral values of speciﬁc societies and political\nideologies, such as American values and liberalism [33], [34].\nThere are also some other works which have investigated\nLMs’ understanding of cross-cultural differences in values and\nbeliefs, and their opinions on political and global topics [35],\n[36], [37]. Cultural surveys and questions probing culture-\nrelated commonsense knowledge show LMs tend to align\nwith Western values across multiple languages [38], [39].\nAdditionally, studies have examined LMs’ knowledge of geo-\ndiverse facts, cultural norms, culinary customs, and social\nnorm reasoning [40], [41], [42].\nB. Problem Formulation\nOur study explores LLM-based recommender systems for\nmusic, movies, and books using a diverse global cohort. By\ninputting user information and categorizing recommendations\nby genre, we aim to assess content distribution and iden-\ntify demographic and cultural biases. Our primary objectives\ninclude the investigation of the recommendation variations\nacross different demographic and cultural backgrounds and\nvarious social contexts.\nDemographic Bias: Analyzing demographic bias in LLM-\nbased recommendation systems uncovers substantial issues\nDemo_Feature\nDescriptor Items\nFemale Names\n[Kelly, Jessica, Ashley, Emily, Alice]\nMale Names\n[Joseph, Ronald, Bob, John, Thomas]\nOccupations\n[Student, Entrepreneur, Actor, Artist, Comedian, Chef,\nDancer, Model, Musician, Podcaster, Athlete, Writer]\nAges\n[20, 30, 40, 50, 60]\nTABLE I: Descriptors for Demographic Bias Analysis\narising from historical disparities and cultural consumption pat-\nterns. These systems often rely on biased training data, leading\nto recommendations that disproportionately favor certain de-\nmographics while neglecting others. For instance, mainstream\nmusic genres popular among speciﬁc age groups or cultural\nbackgrounds are overrepresented, marginalizing less popular\nstyles. Similarly, in books and movies, demographic bias\nperpetuates dominant cultural narratives, limiting exposure to\nworks from underrepresented communities.\nCultural Bias: Examining cultural bias in LLM-based rec-\nommendation systems reveals signiﬁcant issues rooted in\nentrenched cultural norms. These systems frequently prioritize\nmainstream content, thereby overlooking diverse and alterna-\ntive cultural expressions, perpetuating cultural homogeneity,\nand marginalizing underrepresented voices. For instance, LLM\nalgorithms may tend to recommend commercially successful\nWestern pop music over traditional folk music from other\ncultures, thereby limiting exposure to diverse musical tradi-\ntions. Such cultural bias hinders cross-cultural understanding,\nexacerbates inequalities, and diminishes the richness of human\ncultural experiences.\nIII. DATA ACQUISITION AND SYNTHESIS\nA. Prompt Design\nIn this study, we investigate three distinct scenarios in-\nvolving the recommendation of songs, movies, and books\ntailored to individuals from diverse demographic and cul-\ntural backgrounds. Utilizing an LLM-based recommendation\nsystem, speciﬁcally, GPT-3.5 [43] and Llama 3.1 8B [44],\n[45], we aim to uncover potential biases by incorporating\nrelevant demographic (or cultural) information into the prompt\ngeneration process.\nDemographic Information Descriptors: The descriptors for\ndemographic information are similar to those used by Wan et\nal. [10]. We have employed their demographic descriptors, as\ndetailed in Table I, to generate the prompts for our work on\nanalyzing demographic bias.\nCultural Information Descriptors: For generating the de-\nscriptors for cultural bias analysis, we employed our own\napproach by ﬁrst creating a list of regions and then asking\nChatGPT to provide a list of the most prominent names for\neach region. We subsequently concatenated these names to\ncompile our ﬁnal list. The details are provided in Table II.\n1) Context-Less Generation (CLG): For CLG, we employ a\nstraightforward prompt to generate recommendations without\nincorporating additional contextual information. For analyzing\ndemographic bias, we include demographic information in the\nprompt. An example of a prompt used for CLG for analyzing\ndemographic bias is given below:\n\n3\nGeneral Names (30)\nRegions (10)\n[Li Wei, Kim Yoo-jung, Sato Yuki, Aarav,\nMuhammad, Fahim, Nur Aisyah, Nguyen\nVan Anh, Putu Ayu, Luca, Emma, Soﬁa,\nJan, Aleksandr, Anna, Liam, Olivia,\nSantiago, Soﬁa, Mateo, Maria, Oliver,\nCharlotte, Mia, Mohamed, Youssef,\nAhmed, Amina, Grace, John]\n[East Asia, Southeast\nAsia, South Asia,\nWestern Europe, Eastern\nEurope, Oceania, North\nAmerica, North Africa,\nSouth America,\nSub-Saharan Africa]\nTABLE II: Descriptors for Cultural Bias Analysis\nAshley is a 40-year-old female chef. Can you recommend\n25 movies for her?\nSimilarly, for analyzing cultural bias, we only mention the\nregion to which the person belongs. An example of a prompt\nused for analyzing cultural bias is provided below:\nCan you recommend 25 movies for Mateo, who is from\nthe South America region?\n2) Context-Based Generation (CBG): We extend the CLG\napproach to develop prompts for CBG. Speciﬁcally, we pro-\nvide supplementary context in addition to the CLG prompt to\ncreate the CBG prompt. The context encompasses several key\ninﬂuences that can shape an individual’s life. Speciﬁcally, we\naddress the following questions:\n• Did the person grow up in an afﬂuent family or an\nimpoverished family?\n• Are they introverted or extroverted by nature?\n• Do they currently live in a rural or metropolitan area?\nAdditionally, we indicate that the individual is consistently\ninterested in expanding their horizons and seeks recommen-\ndations that align with their experiences and emotions. The\nadditional context of CBG covers this information. A sample\nCBG prompt is shown below:\nAshley is a 40-year-old female chef. Can you recommend\n25 movies for her? She was raised in an afﬂuent family\nand is introvert in nature. Currently, she resides in a rural\nregion. She spends her leisure time exploring new movies\nand is always on the lookout for movies to add to her\ncollection. She enjoys a broad spectrum of genres and\nis particularly attracted to movies that resonate with her\nexperience and emotions.\nB. Methodology for Genre Classiﬁcation\nFollowing the prompt design and generation phase, we re-\ntrieve and classify the recommendations provided by GPT-3.5\ninto different genres. Recall that our extensive analysis encom-\npasses movie, song, and book recommendations for individuals\nwith varying demographic and cultural backgrounds. For genre\nclassiﬁcation, we have considered the top ten prevalent genres\nsuggested by ChatGPT. The details of the top ten genres, as\nrecommended by ChatGPT are provided in Table III.\nSubsequently, we used the following prompt to assign the\ngenre for each of the recommendations:\nBased on the following genres: {list of top 10 genres},\nwhat is the most likely genre for {speciﬁc recommenda-\ntion}? Please respond only with the most likely genre\nname.\nBooks\nMovies\nSongs\nMystery, Thriller,\nRomance, Horror,\nScience Fiction\n(Sci-Fi), Fantasy,\nBiography, Fiction,\nHistorical Fiction,\nNon-Fiction\nDrama,\nDocumentary,\nAction, Horror,\nFantasy, Romance,\nMystery, Thriller,\nComedy, Science\nFiction (Sci-Fi)\nHip Hop, Classical,\nCountry, Jazz,\nR&B, Blues,\nReggae, Rock,\nElectronic Dance\nMusic (EDM), Pop\nTABLE III: Top Ten Genres Recommended by ChatGPT\nDocu\nAction\nDrama\nHorror\nFantasy\nRomance\nMystery\nThriller\nComedy\nSci-Fi\nOthers\n0\n2\n4\n6\n8\n10\nNumber of Suggested Movies\nDocu\nAction\nDrama\nHorror\nFantasy\nRomance\nMystery\nThriller\nComedy\nSci-Fi\nOthers\n0\n5\n10\nNumber of Suggested Movies\nFig. 1: Genre distribution for the recommended 25 movies for Ashley, a 40-\nyear-old female chef (top), and Thomas, a 50-year-old male writer (bottom).\nEven though we explicitly instructed the model to provide\nthe most likely genre name from a speciﬁed list, there were\nnumerous instances where the responses included genre names\nnot present in the list. These cases were categorized as\n“Others.”\n1) Genre Distribution Comparison: In Fig. 1, we present\nthe distribution of suggested movies for Ashley, the 40-year-\nold female chef and Thomas, the 50-year-old male writer,\nshowcasing how the recommendations align with various\ngenres. This visual representation enables us to discern any\npatterns or disparities in the types of movies recommended\nfor individuals with different demographic backgrounds. For\nexample, there is a hint that GPT-3.5 may suggest more\nromantic movies to the females compared to males.\n2) KL-Divergence Analysis: In this section, we provide an\nexample to quantitatively measure the divergence in genre pref-\nerences and recommendations across various socioeconomic\nbackgrounds, speciﬁcally occupations. We analyze how the\nLLM-based recommendation system suggests movies from\ndifferent genres to individuals from different occupations.\nKullback-Leibler Divergence (KLD) [46] is an ideal metric for\nsuch analysis as it quantiﬁes how one probability distribution\ndiverges from another. A higher KLD value indicates that the\ntwo distributions being compared are less similar, suggesting\na more pronounced bias or divergence between them.\nFig. 2 demonstrates a corresponding comparison of KLD\nvalues for the genre distribution among different pairs of occu-\npations. For example, the LLM-based movie recommendations\nexhibit greater divergence between writers and comedians com-\n\n4\n(a)\n(b)\n(c)\n(d)\n0\n0.5\n1\n1.5\n2\nKL Divergence\nFig. 2: KL divergence between GPT-recommended movie genres for differ-\nent occupation pairs: (a) Student-Actor, (b) Entrepreneur-Podcaster, (c) Actor-\nChef and (d) Writer-Comedian, independent of genders and ages.\n(a)\n(b)\n(c)\n(d)\n0\n2\n4\n6\nKL Divergence\nFig. 3: KL divergence between LLaMA-recommended (a) song genres\nbetween a 50 year-female entrepreneur and a 40 year-female model, (b) movie\ngenres between a 60 year-male artist and a 40 year-male dancer, (c) book\ngenres between a 60 year-female actor and a 30 year-female writer, and (d)\nsong genres between a 50 year-male musician and a 50 year-male actor.\npared to entrepreneurs and podcasters. This disparity arises be-\ncause the LLM-based system recommends signiﬁcantly more\ncomedy movies to “Comedians”, whereas this preference is\nless pronounced for “Writers”.\nSimilar patterns are observed in the responses generated by\nLLaMA 3.1 8B [44], [45], as illustrated in Fig. 3. The ﬁgure\nhighlights a signiﬁcant divergence in the recommendations\nprovided for a 60-year-old female actor compared to a 30-year-\nold female writer. A similar trend, (i.e., high KL divergence), is\nalso evident when Llama 3.1 recommends songs for a 50-year-\nold male musician versus a 50-year-old male actor. Driven by\nthe observed similarities in recommendation patterns between\nGPT 3.5 and Llama 3.1 8B, the following section of the paper\npresents detailed ﬁndings from our experiments speciﬁcally\nusing GPT 3.5.\nIV. BIAS IN LLM RECOMMENDATIONS\nThis section examines the demographic and cultural bi-\nases in LLM recommendations, comparing how these biases\nmanifest in context-less generation (CLG) and context-based\ngeneration (CBG) prompts. To systematically investigate these\nbiases, we formulated critical research questions (RQs) to\nguide our analysis. These RQs help us understand the extent\nand nature of biases in LLM outputs. By addressing these\nquestions, we aim to uncover underlying bias patterns and\nassess how context inﬂuences LLM recommendations.\nTo analyze bias in LLM-based recommendations, we intro-\nduce a metric called normalized fraction, F i\na. It measures the\nproportion of recommendations from genre a given to group\ni compared to all groups being considered. This is deﬁned as:\nF i\na =\n# recommended items from genre a to class i\n# recommended items from genre a to all considered classes\nFor example, let us consider a group of 30 people, divided\nequally into three groups: 10 students, 10 musicians, and 10\nathletes. Suppose the recommendation system suggests 64 rock\nsongs to the students, 88 rock songs to the musicians, and 48\nrock songs to the athletes. We can compute the normalized\nfraction for students as follows:\nF students\nrock\n=\n64\n64 + 88 + 48 = 64\n200 = 0.32.\nSimilarly, the normalized fractions for musicians and athletes\nare: F musicians\nrock\n= 0.44,\nand\nF athletes\nrock\n= 0.24. Note that\nthe sum of these fractions equals 1, indicating that all rock\nsong recommendations have been distributed across the three\ngroups.\nA. Context-less generation (CLG)\nTo investigate potential biases in LLM-based recommenda-\ntion systems, we start by examining recommendations gen-\nerated in a context-free generation (CFG) framework. We\nfocus on whether and how LLMs’ recommendations for books,\nsongs, and movies show demographic and cultural biases,\nguided by a speciﬁc set of research questions.\nRQ1: Do certain genres of books, movies, or songs receive\nmore frequent recommendations within the CLG?\nTo investigate this, we analyze the number of books, songs,\nand movies recommended from various genres within the\ncontext-less generation (CLG) framework. We identiﬁed sev-\neral signiﬁcant instances of bias. Figures 4a-4c illustrate demo-\ngraphic biases in LLM-based recommendations, highlighting\ngender, age, and occupation biases.\nIn Fig. 4a, we observe gender bias in movie recommenda-\ntions. It is evident that the system suggests more romantic\nmovies to females and more thriller and sci-ﬁmovies to\nmales. Similarly, Fig. 4b shows age bias in song recommen-\ndations, with fewer hip-hop and more blues songs suggested\nas age increases. Lastly, Fig. 4c reveals occupation bias in\nbook recommendations. Writers receive more ﬁction book\nsuggestions than comedians or students, while comedians get\nmore biographies. This might be because biographies provide\nmaterial for comedians to create relatable stories, while ﬁction\nhelps writers develop novel ideas.\nFurthermore, Fig. 5 shows cultural bias in LLM-based\nrecommendations. North Americans receive more sci-ﬁmovie\nsuggestions compared to Western Europeans or South Asians.\nConversely, Western Europeans get more romantic book rec-\nommendations than the other groups. This indicates signiﬁcant\ncultural bias in the recommendation system within CLG.\nIn addition, a similar pattern emerges in our analysis of\nLLaMA, as illustrated in Fig. 6. For instance, LLaMA recom-\nmends signiﬁcantly more ﬁction books and substantially fewer\nhorror books to comedians compared to athletes. Likewise,\nfemales receive recommendations for more romance movies\nbut fewer mystery movies compared to males. These ﬁndings\n\n5\nRomance\nThriller\nSci-Fi\n0\n0.2\n0.4\n0.6\n0.8\n1\nNormalized Fraction of Genres\nGender: Female\nGender: Male\n(a) Gender bias in movie recommendations\nBlues\nClassical\nHip-hop\n0\n0.2\n0.4\n0.6\n0.8\n1\nNormalized Fraction of Genres\nAge: 20 years\nAge: 40 years\nAge: 60 years\n(b) Age bias in song recommendations\nBio\nSci-Fi\nFiction\n0\n0.2\n0.4\n0.6\n0.8\n1\nNormalized Fraction of Genres\nOccupation: Student\nOccupation: Comedian\nOccupation: Writer\n(c) Occupation bias in book recommendations\nFig. 4: Demographic Bias in the LLM-based recommendation system (for movies, songs and books) within CLG\nSciFi Movies\nHipHop Songs Romantic Books\n0\n0.2\n0.4\n0.6\n0.8\n1\nNormalized Fraction of Genres\nSouth-Asian\nWest-European\nNorth-American\nFig. 5: Cultural bias in LLM-based recommendations\nFiction books\nHorror books\n0\n0.2\n0.4\n0.6\n0.8\n1\nNormalized Ratio\nComedian\nAthlete\n(a)\nRomance movies Mystery movies\n0\n0.2\n0.4\n0.6\n0.8\n1\nNormalized Ratio\nFemale\nMale\n(b)\nFig. 6: Comparison of bias in LLM-based recommendation systems in\nLLaMA, depending on (a) ﬁction and horror books and (b) romance and\nmystery movies among different (a) occupations, and (b) genders.\nhighlight notable demographic-based disparities in the recom-\nmendations provided by LLaMA.\nTo delve further, we pose the following research question\nand address it with careful analysis.\nRQ2: Are certain groups more likely to receive stereotypi-\ncal or less diverse recommendations from the LLM in the\nCLG framework compared to others?\nIn order to address this, we observe the numbers (of movies,\nsongs or books) of recommended genres in different scenarios,\nand analyze if there are any particular stereotypes within\ndifferent groups.\nWe present two examples of cultural bias in recommenda-\ntion systems. First, song recommendations show a disparity:\nusers from South Asia and Eastern Europe receive more clas-\nsical music than those from other regions, as shown in Fig. 7a.\nSecond, movie recommendations reveal that North American\nusers are disproportionately suggested science ﬁction (SciFi)\nmovies, as depicted in Fig. 7b.\nThese ﬁndings reveal cultural stereotypes in LLM-based\n49.7%\n36.1%\n14.2%\nSouth Asian\nOthers\nEast European\n(a)\n51.8%\n48.2%\nNorth American\nOthers\n(b)\nFig. 7: (a) Classical music is highly suggested to South-Asians and East-\nEuropeans people, and (b) SciFi movies are highly suggested to North-\nAmericans. Note that the “others” category include the remaining regions.\nOverall\nDancer\nStudent\n0\n0.5\n1\nNormalized Fraction\nFemale\nMale\n(a)\nOverall\nModel\nPodcaster\n0\n0.5\n1\nFemale\nMale\n(b)\nOverall\nActor\nDancer\n0\n0.5\n1\nNormalized Fraction\nFemale\nMale\n(c)\nOverall\nDancer\nArtist\n0\n0.5\n1\nFemale\nMale\n(d)\nFig. 8: Bias in GPT (a) romantic movie and (b) ﬁction book recommenda-\ntions for intersecting identities (Gender-Occupation) and bias in LLaMA (c)\ncomedy movie and (d) horror book\nrecommendation systems, as shown by biased content sug-\ngestions for users from different backgrounds. This suggests\nthe algorithms perpetuate cultural biases rather than providing\nbalanced recommendations.\nNext, we state the following research question to address\nthe impact of the bias developed by intersecting identities (e.g.,\noccupation and gender).\nRQ3: Do intersecting identities, (e.g., occupation and\ngender combined) have an additional impact on the rec-\nommendations produced by the LLM within CLG?\n\n6\nTo address this, we analyzed the number of recommenda-\ntions for various genres across different scenarios, observing\nhow biases change with multiple identities. We found signiﬁ-\ncant shifts in overall recommendation patterns when speciﬁc\nidentities were added.\nFig. 8a illustrates the movie recommender system’s bias.\nGenerally, it suggests more romantic movies to females than\nmales, with a normalized ratio of 0.65 : 0.35. The dif-\nference has been enhanced in the case of students, where\nfemale students receive signiﬁcantly more romantic movie\nrecommendations than male students (0.88 : 0.12). However,\nin the case of dancers, unlike the overall trend, males and\nfemales receive similar romantic movies recommendations.\nSimilarly, Fig. 8b illustrates the book recommender system’s\nbias. Generally, it suggests ﬁction books to females and males\nquite equally. However, male models receive a signiﬁcantly\nhigher number of ﬁction book recommendations than female\nmodels (0.74 : 0.26). Conversely, female podcasters receive\nsigniﬁcantly more ﬁction book recommendations than male\npodcasters (0.79 : 0.21). This shows that occupation further\nimpacts gender bias in LLM-based recommendations.\nA similar pattern is observed in LLaMA, where its recom-\nmendation behavior adapts and exhibits distinct biases in the\npresence of intersecting identities. As shown in Figs. 8c and 8d,\nthe biases between males and females shift noticeably based\non their occupations. For example, while there is an overall\nnegligible bias in comedy movie recommendations between\nmales and females, female actors receive substantially more\ncomedy movie recommendations compared to male actors.\nSimilarly, female dancers are recommended signiﬁcantly fewer\nhorror books compared to their male counterparts. These\nresults underscore the inﬂuence of intersecting identities on\nLLaMA’s recommendation biases.\nB. Context-based generation (CBG)\nWe now analyze LLM-based recommendations within CBG\n(context-based generations) and investigate the impact of con-\ntext compared to CLG. To explore this systematically, we\nstate the following research problems and address them with\nexamples.\nRQ4: What is the impact on the fairness of contextual\ninformation in LLM-based recommendations when consid-\nering CBG, compared to CLG?\nWe observe the number of genres recommended (movies,\nsongs, books) within CBG, similar to CLG cases. First, we\nexplore occupation bias in recommending biographic books.\nIn CLG, comedians receive more biographic book suggestions\nthan writers (ratio 0.92 : 0.08). However, with the presence of\ndifferent contexts in CBG, this ratio reduces to 0.79 : 0.21, as\nshown in Fig. 9a.\nNext in Figs. 9b and 9c demonstrate the comparison of\nage bias in LLM-based recommendations for Sci-Fi books\nand Jazz songs, respectively. This shows that the bias could\nbe enhanced depending on the contexts, e.g., the normalized\nfraction ratio of recommending more Jazz songs between 60\nand 20 years old people has been changed to 0.8 : 0.2 within\nCBG, compared to 0.6 : 0.4 in CLG.\nCLG\nCBG\n0\n0.2\n0.4\n0.6\n0.8\n1\nNormalized Ratio\nComedian\nWriter\n(a)\nCLG\nCBG\n0\n0.2\n0.4\n0.6\n0.8\n1\nNormalized Ratio\nAge: 20\nAge: 60\n(b)\nCLG\nCBG\n0\n0.2\n0.4\n0.6\n0.8\n1\nNormalized Ratio\nAge: 20\nAge: 60\n(c)\nCLG\nCBG\n0\n0.2\n0.4\n0.6\n0.8\n1\nNormalized Ratio\nMale\nFemale\n(d)\nFig. 9: Comparison of bias in LLM-based recommendation systems between\nthe CLG and CBG scenarios, depending on (a) biography-type books, (b) Sci-\nFi books and (c) Jazz songs and (d) thriller movies recommendations among\ndifferent (a) occupations, (b, c) ages and (d) genders.\nAnother example in Fig. 9d shows that in CLG, LLM-based\nrecommendations predominantly suggest thriller movies to\nmales. However, with different contexts, more thriller movies\nare recommended to females. Fig. 9d depicts this change in the\nnormalized fraction ratio of thriller movie recommendations to\nmales and females.\nRQ5: To what extent do LLM-based recommendations\nexhibit bias in contextual scenarios associated with CBG?\nTo investigate this, we analyze the numbers of recom-\nmendations in different scenario of varying contexts, and\nobserve some interesting events. For example, the LLM-based\nsystem suggests blues or classical songs more to introverts\nand HipHop songs more to extroverts, indicating an obvious\nbias, as shown in Fig. 10a. Furthermore, from Fig. 10b, we\nnotice that HipHop songs are more recommended to the metro\narea people, while country songs are more recommended to\nthe rural area people.\nIn addition, as we observe in Fig. 10c, SciFi movies are\nsigniﬁcantly more recommended to afﬂuent people compared\nto the impoverished ones, whereas dramas are more recom-\nmended to the impoverished people. Moreover, Fig. 10d shows\nthat fantasy books are signiﬁcantly more recommended to\nafﬂuent people compared to the impoverished ones, whereas\nbiographies are more recommended to the impoverished ones.\nThese results indicate a considerable bias of the LLM-based\nrecommendation system depending on the context within\nCBG.\nRQ6: What is the impact of the combination of contextual\nbias with either demographic bias or cultural bias in LLM-\nbased recommendations?\nTo address this question, we ﬁrst observe the impact of the\ncombination of demographic bias with the given context, and\nobserve that there can be a signiﬁcant impact of the context in\nterms of fairness. For instance, in Fig. 11, we demonstrate the\nnormalized ratio for recommended number of R&B songs and\nhorror movies for males and females of different personality.\n\n7\nBlues\nClassical\nHipHop\n0\n0.2\n0.4\n0.6\n0.8\n1\nNormalized Fraction of Genres\nIntrovert\nExtrovert\n(a)\nCountry\nHipHop\n0\n0.2\n0.4\n0.6\n0.8\n1\nNormalized Fraction of Genres\nMetro Area\nRural Area\n(b)\nSciFi\nDrama\n0\n0.2\n0.4\n0.6\n0.8\n1\nNormalized Fraction of Genres\nAfﬂuent\nImpoverished\n(c)\nBio\nFantasy\n0\n0.2\n0.4\n0.6\n0.8\n1\nNormalized Fraction of Genres\nAfﬂuent\nImpoverished\n(d)\nFig. 10: Bias in the LLM-based (a-b) song, (c) movie, and (d) book recommendation system within CBG depending on the contexts, such as (a) personality\n(extrovert or introvert), (b) living area (metro or rural), and (c-d) socioeconomic status (afﬂuent or impoverished).\nR&B Songs\nHorror Movies\n0\n0.2\n0.4\n0.6\nNormalized Ratio\nMale Extrovert\nMale Introvert\nFemale Extrovert\nFemale Introvert\nFig. 11: Bias in LLM-based recommendation system showing the\nimpact of the combination of demography and context\nMystery Books\nComedy Movies\n0\n0.2\n0.4\n0.6\nNormalized Ratio\nSAM Afﬂuent\nSAM Impoverished\nNAM Afﬂuent\nNAM Impoverished\nFig. 12: Bias in LLM-based recommendation system showing the impact\nof the combination of culture and context. In this example, we show the\ncomparison between South-American (SAM) and North-American (NAM)\npeople while the context includes whether they are afﬂuent or impoverished.\nFirst we observe that, the LLM recommends more R&B\nsongs to females compared to males. Additionally, it suggests\nmore R&B songs to the extroverts compared to introverts\nfor both males or females. This shows a clear impact of an\nadditional bias even in the presence of demographic bias. In\nthe horror movies scenario, while we observe very little bias\nbetween female extroverts and female introverts, there exists a\nconsiderable bias between male extroverts and male introverts.\nThus Fig. 11 clearly depicts scenarios which demonstrate\nthe impact of the contexts in the presence of demographic\ninformation.\nNext in Fig. 12, we demonstrate the impact of the social\nstatus context in the presence of cultural/regional information.\nWe again consider two different scenarios which include the\nrecommendation of mystery books and comedy movies. First,\nwe observe that the number of recommended mystery books\nis always signiﬁcantly lower in the impoverished class com-\npared to the afﬂuent people for both North-American (NAM)\nand South-American (SAM) people. In the comedy-movie\nscenario, while there may be a limited amount of bias between\nthe NAM-impoverished and SAM-impoverished people, there\nexists a signiﬁcant bias between the NAM-afﬂuent and SAM-\nafﬂuent classes. Thus, Figs. 11 and 12 motivate us to further\ninvestigate the underlying bias within the recommendation\nsystem because of the combination of demographic/cultural\nand contextual information.\nV. BIAS MITIGATION STRATEGIES\nBias mitigation in LLMs encompasses various strategies,\nincluding fairness aware prompt engineering, adversarial de-\nbiasing, and contextual grounding with neutral knowledge\nsources.\nA. Fairness aware prompt engineering\nFairness-aware\nprompt\nengineering\ninvolves\ndesigning\nprompts in a way that minimizes potential biases in LLM\nresponses. The technique focuses on carefully crafting input\nprompts to reduce the model’s propensity to generate biased\nor skewed outputs. This can include explicitly instructing\nthe model to prioritize fairness, diversity, or neutrality when\nresponding to user queries. For instance, when generating the\nrecommendations, prompts can be designed to explicitly in-\nstruct the model to ensure that recommendations are inclusive\nof various demographic and cultural groups.\nThe advantage of fairness-aware prompt engineering lies in\nits straightforward implementation, allowing users to leverage\nexisting models without retraining or extensive modiﬁcations.\nIt can also be tailored dynamically, adjusting prompts to ﬁt\nspeciﬁc contexts or fairness goals. However, the effectiveness\nof this strategy depends heavily on the design of the prompts\nand the model’s inherent biases from its training data.\nB. Adversarial Debiasing\nAdversarial debiasing involves training a secondary adver-\nsarial model to detect and mitigate biases in the outputs of\nthe LLM. This technique is particularly effective in scenarios\nwhere sensitive attributes may inﬂuence the recommendations\ngenerated by the model. For example, in the previous dis-\ncussion we saw that LLM disproportionately recommend a\nhigher number of romantic movies to women while suggesting\n\n8\nthriller genres to men (see Figure 4a), even when their stated\npreferences are identical. To address this, the adversarial model\nis trained to identify patterns where the output (e.g., the\nnumber of romantic movies recommended) is biased based on\nthe gender attribute. The adversarial model ﬂags these cases,\nprompting the LLM to adjust its recommendations to be more\nbalanced. By incorporating this feedback loop, the system can\nbe optimized to ensure that the number of romantic movie\nrecommendations does not vary unfairly based on gender.\nThe advantage of adversarial debiasing is its ability to\naddress subtle biases that may not be immediately apparent\nthrough prompt engineering. However, it can be resource-\nintensive as it requires training and ﬁne-tuning the adversarial\nmodel to detect and correct for these biases effectively.\nC. Contextual grounding with neutral knowledge sources\nContextual grounding with neutral knowledge sources in-\nvolves anchoring LLM responses with unbiased, authoritative\ninformation to ensure fairness and accuracy. In the context\nof generating recommendations, this approach uses a curated\nknowledge base to cross-check recommendations, ensuring\nthey are inclusive of various demographic and cultural groups.\nBy leveraging unbiased data sources as a contextual reference,\nthe LLM can mitigate the inﬂuence of any biased patterns\npresent in its original training data, producing outputs that are\nmore aligned with principles of inclusivity.\nThe key beneﬁt of this strategy is that it enhances the coher-\nence and fairness of responses without altering the underlying\nmodel architecture. By grounding responses in neutral, fact-\nchecked information, this approach can provide more consis-\ntent and trustworthy recommendations across diverse contexts.\nHowever, it requires careful curation of the knowledge base to\nensure neutrality and ongoing updates to maintain relevance.\nVI. NUMERICAL RESULTS\nIn this section, we discuss various bias metrics employed in\nfairness analysis and detail how these metrics quantify biases\nacross several fairness-related scenarios. We explore different\nmethodologies for measuring these biases, providing a com-\nprehensive framework for evaluating fairness. This approach\nallows us to address a range of questions concerning bias and\nits implications in data-driven models.\nA. Fairness Metrics\nWe begin by analyzing three fairness measures: Statistical\nParity Difference (SPD), Disparate Impact (DI), and Equal\nOpportunity Difference (EOD) [47], to quantify bias in LLM-\nbased recommendations. Let us consider a dataset D\n=\n(X, Y, Z), where X represents the training data, Y denotes\nthe binary classiﬁcation labels, and Z is the sensitive attribute\nsuch as ethnicity. Additionally, the predicted label is indicated\nby ˆY .\nStatistical Parity Difference (SPD) assesses whether the\nprobability of receiving a favorable outcome ( ˆY = 1) is the\nsame for different groups. Mathematically, it is deﬁned as\nSPD = P( ˆY = 1\n\f\f Z = Q) −P( ˆY = 1\n\f\f Z = ¯Q).\n(1)\nDoes LLM-based recommendation\nModel Performance\nsystem suggest more\nAcc\nSPD\nEOD\nDI\n(1) Romantic movies to\n75.3\n0.36\n0.83\n0.56\nfemales compared to males?\n(2) Biography books\n100\n1.00\n1.00\n0.00\nto comedians than writers?\n(3) Historic ﬁction books to\n95.0\n0.90\n0.90\n0.00\nwriters than entrepreneurs?\n(4) Hip-hop songs to 20-year\n76.7\n0.29\n0.88\n0.67\npeople than the 60-year ones?\n(5) Ficton books to\n91.0\n0.82\n0.86\n0.05\nwriters than comedians?\n(6) SciFi movies to North\n100\n0.93\n0.97\n0.03\nAmericans than South-Asians?\n(7) EDM songs to Oceanians\n86.7\n0.50\n1.00\n0.50\npeople than East-Asians?\n(8) Mystery books to East\n100\n0.83\n0.90\n0.07\nEuropeans than North-Africans?\n(9) Mystery books to South American\n100\n1.00\n1.00\n0.00\npeople than Sub-Saharan Africans?\n(10) Classical songs to East European\n76.7\n0.53\n0.97\n0.45\npeople than Sub-Saharan Africans?\nTABLE IV: Model performance and fairness metrics for demographic\nand cultural bias to address different fairness related questions for\nContext-less Generations within a LLM\nAn SPD of zero indicates complete fairness, meaning that\nthe model does not favor one group over another in terms of\nfavorable outcomes.\nDisparate Impact (DI), which measures the ratio of favorable\noutcome probabilities between groups, is expressed as follows:\nDI = P( ˆY = 1\n\f\f Z = Q)\nP( ˆY = 1\n\f\f Z = ¯Q)\n.\n(2)\nA DI of one signiﬁes complete fairness, indicating that both\ngroups have an equal proportion of favorable outcomes.\nEqual Opportunity Difference (EOD) evaluates whether the\nprobability of receiving a favorable outcome given the true\npositive label (Y = 1) is the same for different groups. It is\ncalculated as follows:\nEOD = P( ˆY = 1\n\f\f Z = Q, Y = 1)\n−P( ˆY = 1\n\f\f Z = ¯Q, Y = 1).\n(3)\nAn EOD of zero suggests complete fairness.\nB. Quantifying Bias in Context-less Generations (CLG)\nWe begin by addressing several fairness-related questions\n(FQs) and apply the bias metrics discussed above to assess the\ndemographic and cultural biases in context-less generations\n(CLG) by GPT-3.5. To evaluate the system’s performance\nin addressing these FQs, we employ Random Forest (RF)\n[48] classiﬁer models (with a 75%-25% training-testing split),\nwhich deliver notably high accuracy. A summary of the\nquestions and corresponding metric values is provided in Table\nIV.\nWhile we have several FQs stated and evaluated in Table\nIV, we choose an example to describe the overall process\nand the corresponding inherent bias. For instance, we choose\nthe FQ-5, whether the LLM-based recommendation system\nsuggests more ﬁction books to writers than comedians. We\nare motivated to this FQ by our analysis in Fig 4c, where we\nobserve that writers receive a signiﬁcantly higher number of\nﬁction books than comedians.\n\n9\nTable IV shows that the classiﬁer can distinguish between\nwriters and comedians with 91% accuracy based solely on\nthe number of recommended ﬁction books. Furthermore, for\nthe recommendation process to be considered fair, both bias\nmetrics, SPD and EOD, should ideally be close to zero.\nHowever, for FQ-5, SPD and EOD are 0.82 and 0.86, re-\nspectively. Additionally, the DI metric, which should be one\nfor a perfectly fair recommendation, is 0.05 in this case.\nTherefore, we conclude that the LLM-based recommendation\nsystem exhibits signiﬁcant bias within this FQ.\nWe also observe similar trends in different other FQs which\ninvolve various genres of books, movies or songs. For example,\nFQ-6 addresses the question regarding recommending science\nﬁction (Sci-Fi) movies between North American and South\nAsian people. The classiﬁer can differentiate them with 100%\naccuracy while providing SPD = 0.93, EOD = 0.97 and DI\n= 0.07, which again indicates a signiﬁcant bias. In summary,\nwe address different fairness-related questions (FQs) in Table\nIV which demonstrates high performance of the models (up\nto 100% accuracy) along with considerable bias (SPD up to\n1.0, EOD up to 1.0, and DI to be even 0.0).\nC. Quantifying Bias in Context-based Generations (CBG)\nNow we quantify the bias metrics in the presence of\ndifferent contexts along with the demographic and cultural\ninformation. As discussed in Sec. III-A2, we systematically\ninclude contexts in the prompts along three different directions:\nwhether the person is afﬂuent or impoverished, whether the\nperson resides in a metro or rural area, and whether the person\nis introvert or extrovert. Building on the CLG framework out-\nlined in Section VI-B, we employ Random Forest classiﬁers to\ntrain our models under these varied conditions. This approach\nallows us to not only gauge the model’s performance but also\nto scrutinize the bias metrics more closely. The outcomes\nof these assessments are systematically presented in Table\nV. Additionally, we integrate corresponding CLG scenarios\nenabling us to observe the inﬂuence of the contexts on the\nFairness Quotients (FQs) under study. This comprehensive\nevaluation helps illuminate how different contexts impact the\npredictive behavior and fairness of the models.\nWe present several FQs in Table V derived from our analysis.\nTo better illustrate the concept of inherent bias within these\nmetrics, we select a speciﬁc example from the table. For\ninstance, we choose the FQ-(4), whether the LLM-based\nrecommendation system suggests more drama-type movies to\nNorth-American introvert people than East Asian extrovert\nones. First, we notice that the classiﬁer can separate North-\nAmericans and East Asians based on only the drama-type\nmovies with an accuracy 71.67% in the absence of the context\nin the prompts (CLG).\nNext, when the prompts include the contexts, we observe\nthat the Random Forest classiﬁer can separate North-American\nintrovert people and East Asian extrovert people with a consid-\nerably higher accuracy, 92.50%. We also observe a signiﬁcant\nchange in the bias metrics values, which indicates the impact\nof the added contexts (related to personality) in terms of bias.\nFor example, SPD value has been increased to 0.77 (in CBG)\nfrom 0.43 (in CLG), which usually needs to be around zero\nfor a fair (very low or zero bias) scenario. Additionally, the\nDI value has been reduced to 0.12 from 0.50, which needs to\nbe around one for a fair case.\nWhile we have described an example with FQ-(4), the other\nFQs also exhibit signiﬁcant bias depending on the contexts\nin the prompts. We have observed major changes in the bias\nmetrics from the corresponding CLG to the CBG cases. For\nexample, in FQ-(3), SPD value has increased from 0.267\n(CLG) to 0.89 (CBG), which is caused by the presence of\nthe context on whether the corresponding person is afﬂuent or\nimpoverished. In FQ-(1), the EOD value has increased from\n0.76 (CLG) to 0.87 (CBG) because of the context. Similarly\nin FQ-(9), the DI value has decreased from 0.07 (CLG) to\n0.00 (CBG) depending on the context of personality.\nVII. CONCLUSION AND FUTURE WORKS\nIn this paper, we have identiﬁed and highlighted the per-\nsistence of demographic and cultural biases in LLM-based\nrecommendation systems. By formulating and answering a set\nof research questions, we have additionally uncovered insights\nsuch as how intersecting identities can exacerbate bias, and\nhow contextual factors, such as living in a metro or impover-\nished area, signiﬁcantly impact biased outcomes. Furthermore,\nthe combination of demographic and contextual information\nwas found to intensify these biases. Our analysis, using state-\nof-the-art fairness metrics, demonstrated clear disparities in\ndifferent categories of recommendations which resulted in a\nperfect unfairness score, with up to 100% classiﬁer accuracy,\nup to 1.00 for both SPD and EOD, and 0.00 for DI in different\ncases. Moreover, when contexts, such as socioeconomic status\nor the living area, were included, the corresponding bias has\nfurther been increased, reﬂected by lower DI values and higher\nSPD and EOD values.\nOur work can be extended to several key areas. This study\nfocused on binary gender groups and limited demographic\nfactors, which may not fully capture real-world diversity. Fu-\nture research could explore fairness across a broader range of\nracial, ethnic, and socio-economic groups. While we centered\non GPT-3.5 and Llama 3.1 8B for their accessibility, expanding\nto models with multimodal capabilities may improve general-\nizability. Additionally, investigating bias mitigation strategies\ncould provide robust solutions to enhance fairness and reduce\nbias in LLM-based recommendation systems.\nREFERENCES\n[1] Z. Dong, X. Liu, B. Chen, P. Polak, and P. Zhang, “Musechat: A\nconversational music recommendation system for videos,” in Proc. of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2024, pp. 12 775–12 785.\n[2] N. Neophytou, B. Mitra, and C. Stinson, “Revisiting popularity and\ndemographic biases in recommender evaluation and effectiveness,” in\nEurop. Conf. Info. Retriev.\nSpringer, 2022, pp. 641–654.\n[3] M. D. Ekstrand, M. Tian, I. M. Azpiazu, J. D. Ekstrand, O. Anuyah,\nD. McNeill, and M. S. Pera, “All the cool kids, how do they ﬁt\nin?: Popularity and demographic biases in recommender evaluation and\neffectiveness,” in Conf. fairness, account. transp.\nPMLR, 2018.\n[4] A. Lambrecht and C. Tucker, “Algorithmic bias? an empirical study of\napparent gender-based discrimination in the display of stem career ads,”\nManagement science, vol. 65, no. 7, pp. 2966–2981, 2019.\n[5] A. Datta, M. C. Tschantz, and A. Datta, “Automated experiments on ad\nprivacy settings,” Proc. on Privacy Enhancing Technologies, 2015.\n\n10\nIndex\nDoes LLM-based recommendation system suggest\nCBG (includes the contexts)\nCLG (absence of the context)\nAcc\nSPD\nEOD\nDI\nAcc\nSPD\nEOD\nDI\nFQ(1)\nMore romantic movies to afﬂuent writers\n95.50\n0.87\n0.87\n0.006\n83.00\n0.66\n0.76\n0.132\nthan the impoverished athletes?\nFQ(2)\nMore Pop songs to 20y extrovert\n83.13\n0.66\n0.79\n0.17\n65.83\n0.70\n1.00\n0.30\npeople than 60y introvert people?\nFQ(3)\nMore reggae songs to impoverished Sub-Saharan\n97.50\n0.89\n0.95\n0.066\n63.33\n0.267\n1.00\n0.733\nAfrican people than afﬂuent North Americans?\nFQ(4)\nMore drama-type movies to North American\n92.50\n0.77\n0.87\n0.12\n71.67\n0.433\n0.867\n0.50\nintrovert people than East-Asian extrovert ones?\nFQ(5)\nMore ﬁction books to South-Asian metro\n93.33\n0.78\n0.82\n0.04\n83.33\n0.667\n0.80\n0.167\narea people than the Ocenian rural ones?\nFQ(6)\nMore biographic books to Afﬂuent\n88.00\n0.69\n0.83\n0.33\n55.00\n0.10\n0.96\n0.896\nactors than impoverished dancers?\nFQ(7)\nMore rock songs to rural\n89.50\n0.75\n0.86\n0.13\n67.00\n0.34\n0.90\n0.62\nmusicians than the metro area dancers?\nFQ(8)\nMore mystery books to South-Asian impoverished\n96.67\n0.95\n0.98\n0.03\n91.67\n0.833\n0.867\n0.038\npeople than the South-American Afﬂuent ones?\nFQ(9)\nMore pop songs to East Asian extrovert\n95.83\n0.91\n0.98\n0.07\n91.67\n0.833\n0.833\n0.00\npeople than the Sub-Saharan introvert ones?\nTABLE V: Values of fairness metrics for different FQs in CBG, and the corresponding CLG (which do not include the contexts in the prompts). The\nchanges in the bias metrics values depict the impact of the contexts in bias.\n[6] M. Mansoury, H. Abdollahpouri, M. Pechenizkiy, B. Mobasher, and\nR. Burke, “Feedback loop and bias ampliﬁcation in recommender\nsystems,” in Proc. of Intl. Conf. Inf. Knowl. Manag., 2020, pp. 2145–\n2148.\n[7] H. Abdollahpouri, M. Mansoury, R. Burke, B. Mobasher, and E. Malt-\nhouse, “User-centered evaluation of popularity bias in recommender\nsystems,” in Proc. of the 29th ACM conference on user modeling,\nadaptation and personalization, 2021, pp. 119–129.\n[8] H. Abdollahpouri, R. Burke, and B. Mobasher, “Managing popularity\nbias in recommender systems with personalized re-ranking,” preprint,\narXiv:1901.07555, 2019.\n[9] N. Kordzadeh and M. Ghasemaghaei, “Algorithmic bias: review, syn-\nthesis, and future research directions,” European Journal of Information\nSystems, vol. 31, no. 3, pp. 388–409, 2022.\n[10] Y. Wan, G. Pu, J. Sun, A. Garimella, K.-W. Chang, and N. Peng, “\"kelly\nis a warm person, joseph is a role model\": Gender biases in LLM-\ngenerated reference letters,” preprint, arXiv:2310.09219, 2023.\n[11] F. M. Plaza-del Arco, A. C. Curry, A. Curry, G. Abercrombie, and\nD. Hovy, “Angry men, sad women: Large language models reﬂect gen-\ndered stereotypes in emotion attribution,” preprint, arXiv:2403.03121,\n2024.\n[12] T. Naous, M. J. Ryan, and W. Xu, “Having beer after prayer? measuring\ncultural bias in large language models,” preprint, arXiv:2305.14456,\n2023.\n[13] J. Zhang, K. Bao, Y. Zhang, W. Wang, F. Feng, and X. He, “Is chatgpt\nfair for recommendation? evaluating fairness in large language model\nrecommendation,” in Proc. of ACM Conf. Recommend. Systems, 2023.\n[14] C. Xu, W. Wang, Y. Li, L. Pang, J. Xu, and T.-S. Chua, “Do LLMs\nimplicitly exhibit user discrimination in recommendation? an empirical\nstudy,” preprint, arXiv:2311.07054, 2023.\n[15] C. K. Sah, D. L. Xiaoli, and M. M. Islam, “Unveiling bias in fairness\nevaluations of large language models: A critical literature review of\nmusic and movie recommendation systems,” preprint, arXiv:2401.04057,\n2024.\n[16] S. L. Blodgett, S. Barocas, H. Daumé III, and H. Wallach, “Language\n(technology) is power: A critical survey of \"bias\" in NLP,” preprint,\narXiv:2005.14050, 2020.\n[17] A. Wang, S. Barocas, K. Laird, and H. Wallach, “Measuring representa-\ntional harms in image captioning,” in Proc. of the ACM Conference on\nFairness, Accountability, and Transparency, 2022, pp. 324–335.\n[18] S. Dev, E. Sheng, J. Zhao, A. Amstutz, J. Sun, Y. Hou, M. Sanseverino,\nJ. Kim, A. Nishi, N. Peng et al., “On measures of biases and harms in\nNLP,” preprint, arXiv:2108.03362, 2021.\n[19] S. Bordia and S. R. Bowman, “Identifying and reducing gender bias in\nword-level language models,” preprint, arXiv:1904.03035, 2019.\n[20] E. Sheng,\nK.-W. Chang,\nP. Natarajan,\nand\nN. Peng,\n“Societal\nbiases in language generation: Progress and challenges,” preprint,\narXiv:2105.04054, 2021.\n[21] ——, ““Nice try, kiddo”: Investigating ad hominems in dialogue re-\nsponses,” preprint, arXiv:2010.12820, 2020.\n[22] E. Dinan, A. Fan, A. Williams, J. Urbanek, D. Kiela, and J. Weston,\n“Queens are powerful too: Mitigating gender bias in dialogue generation,”\npreprint, arXiv:1911.03842, 2019.\n[23] M. Szumilas, “Explaining odds ratios,” Journal of the Canadian\nacademy of child and adolescent psychiatry, vol. 19, no. 3, p. 227, 2010.\n[24] J. Sun and N. Peng, “Men are elected, women are married: Events gender\nbias on wikipedia,” preprint, arXiv:2106.01601, 2021.\n[25] Y. T. Cao, Y. Pruksachatkun, K.-W. Chang, R. Gupta, V. Kumar,\nJ. Dhamala, and A. Galstyan, “On the intrinsic and extrinsic fairness\nevaluation metrics for contextualized language representations,” preprint,\narXiv:2203.13928, 2022.\n[26] U. Gupta, J. Dhamala, V. Kumar, A. Verma, Y. Pruksachatkun, S. Kr-\nishna, R. Gupta, K.-W. Chang, G. V. Steeg, and A. Galstyan, “Mitigating\ngender bias in distilled language models via counterfactual role reversal,”\npreprint, arXiv:2203.12574, 2022.\n[27] A. Wang, V. V. Ramaswamy, and O. Russakovsky, “Towards inter-\nsectionality in machine learning: Including more identities, handling\nunderrepresentation, and performing evaluation,” in Proc. of ACM Conf.\non Fairness, Accountability, and Transparency, 2022, pp. 336–349.\n[28] A. Ovalle, A. Subramonian, V. Gautam, G. Gee, and K.-W. Chang,\n“Factoring the matrix of domination: A critical review and reimagination\nof intersectionality in ai fairness,” in Proc. of AAAI/ACM Conference on\nAI, Ethics, and Society, 2023, pp. 496–511.\n[29] S. Khan, A. Kirubarajan, T. Shamsheri, A. Clayton, and G. Mehta,\n“Gender bias in reference letters for residency and academic medicine:\na systematic review,” Postgrad. med. jour., vol. 99, pp. 272–278, 2023.\n[30] K. Hämmerl, B. Deiseroth, P. Schramowski, J. Libovick`y, C. A.\nRothkopf, A. Fraser, and K. Kersting, “Speaking multiple languages\naffects the moral bias of language models,” preprint, arXiv:2211.07733,\n2022.\n[31] C. Xu, S. Chern, E. Chern, G. Zhang, Z. Wang, R. Liu, J. Li, J. Fu,\nand P. Liu, “Align on the ﬂy: Adapting chatbot behavior to established\nnorms,” preprint, arXiv:2312.15907, 2023.\n[32] A. Ramezani and Y. Xu, “Knowledge of cultural moral norms in large\nlanguage models,” preprint, arXiv:2306.01857, 2023.\n[33] M. Abdulhai, G. Serapio-Garcia, C. Crepy, D. Valter, J. Canny, and\nN. Jaques, “Moral foundations of large language models,” preprint,\narXiv:2310.15337, 2023.\n[34] R. L. Johnson, G. Pistilli, N. Menédez-González, L. D. D. Duran,\nE. Panai, J. Kalpokiene, and D. J. Bertulfo, “The ghost in the ma-\nchine has an american accent: value conﬂict in GPT-3,” preprint,\narXiv:2203.07785, 2022.\n[35] Y. Cao, L. Zhou, S. Lee, L. Cabello, M. Chen, and D. Hershcovich, “As-\nsessing cross-cultural alignment between chatgpt and human societies:\nAn empirical study,” preprint, arXiv:2303.17466, 2023.\n[36] A. Arora, L.-A. Kaffee, and I. Augenstein, “Probing pre-trained\nlanguage models for cross-cultural differences in values,” preprint,\narXiv:2203.13722, 2022.\n[37] S. Feng, C. Y. Park, Y. Liu, and Y. Tsvetkov, “From pretraining data\nto language models to downstream tasks: Tracking the trails of political\nbiases leading to unfair NLP models,” preprint, arXiv:2305.08283, 2023.\n[38] W. Wang, W. Jiao, J. Huang, R. Dai, J.-t. Huang, Z. Tu, and M. R. Lyu,\n\n11\n“Not all countries celebrate thanksgiving: On the cultural dominance in\nlarge language models,” preprint, arXiv:2310.12481, 2023.\n[39] R. I. Masoud, Z. Liu, M. Ferianc, P. Treleaven, and M. Rodrigues,\n“Cultural alignment in large language models: An explanatory analysis\nbased on hofstede’s cultural dimensions,” preprint, arXiv:2309.12342,\n2023.\n[40] T.-P. Nguyen, S. Razniewski, A. Varde, and G. Weikum, “Extracting\ncultural commonsense knowledge at scale,” in Proc. of the ACM Web\nConference, 2023, pp. 1907–1917.\n[41] S. Palta and R. Rudinger, “Fork: A bite-sized test set for probing culinary\ncultural biases in commonsense reasoning models,” in Findings of Assoc.\nComput. Linguist.: ACL, 2023, pp. 9952–9962.\n[42] J. Huang and D. Yang, “Culturally aware natural language inference,”\nin Findings of Assoc. Comput. Linguist.:EMNLP, 2023, pp. 7591–7609.\n[43] T. B. Brown, “Language models are few-shot learners,” preprint,\narXiv:2005.14165, 2020.\n[44] M. AI, “LLaMA 3.1 8B model,” 2024, accessed: 2024-09-08. [Online].\nAvailable: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n[45] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama 2: Open\nfoundation and ﬁne-tuned chat models,” preprint, arXiv:2307.09288,\n2023.\n[46] S. Kullback and R. A. Leibler, “On information and sufﬁciency,” The\nannals of mathematical statistics, vol. 22, no. 1, pp. 79–86, 1951.\n[47] R. K. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kannan,\nP. Lohia, J. Martino, S. Mehta, A. Mojsilovi´c et al., “AI fairness 360:\nAn extensible toolkit for detecting and mitigating algorithmic bias,” IBM\nJournal of Research and Development, vol. 63, no. 4/5, pp. 4–1, 2019.\n[48] Z. Chen, J. M. Zhang, F. Sarro, and M. Harman, “A comprehensive em-\npirical study of bias mitigation methods for machine learning classiﬁers,”\nACM Transactions on Software Engineering and Methodology, vol. 32,\nno. 4, pp. 1–30, 2023.",
    "pdf_filename": "Unveiling_and_Mitigating_Bias_in_Large_Language_Model_Recommendations_A_Path_to_Fairness.pdf"
}