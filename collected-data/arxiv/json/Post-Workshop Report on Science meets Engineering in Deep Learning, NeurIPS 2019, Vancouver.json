{
    "title": "Post-Workshop Report on Science meets Engineering in Deep Learning, NeurIPS 2019, Vancouver",
    "context": "Science meets Engineering in Deep Learning took place in Vancouver as part of the Workshop section of NeurIPS 2019 [1]. As organizers of the workshop, we created the following report in an attempt to isolate emerging topics and recurring themes that have been presented throughout the event. Deep learning can still be a complex mix of art and engineering despite its tremendous success in recent years. The workshop aimed at gathering people across the board to address seemingly contrasting challenges in the problems they are working on. As part of the call for the workshop, particular attention has been given to the interdependence of architecture, data, and optimization that gives rise to an enormous landscape of design and performance intricacies that are not well-understood. This year, our goal was to emphasize the following directions in our community: (i) identify obstacles in the way to better models and algorithms; (ii) identify the general trends from which we would like to build scientiﬁc and potentially theoretical understand- ing; and (iii) the rigorous design of scientiﬁc experiments and experimental protocols whose purpose is to resolve and pinpoint the origin of mysteries while ensuring repro- ducibility and robustness of conclusions. In the event, these topics emerged and were broadly discussed, matching our expectations and paving the way for new studies in these directions. While we acknowledge that the text is naturally biased as it comes through our lens, here we present an attempt to do a fair job of highlighting the outcome of the workshop. 1Facebook AI 2DeepMind 3Google Brain 4Institut de Physique Thèorique - CEA Saclay 1 arXiv:2007.13483v2  [cs.LG]  29 Jul 2020",
    "body": "Post-Workshop Report on\nScience meets Engineering in Deep Learning\nNeurIPS 2019, Vancouver\nLevent Sagun1\nCaglar Gulcehre2\nAdriana Romero1\nNegar Rostamzadeh3\nStefano Sarao Mannelli4\nJuly 30, 2020\nAbstract\nScience meets Engineering in Deep Learning took place in Vancouver as part of the\nWorkshop section of NeurIPS 2019 [1]. As organizers of the workshop, we created the\nfollowing report in an attempt to isolate emerging topics and recurring themes that have\nbeen presented throughout the event.\nDeep learning can still be a complex mix of art and engineering despite its tremendous\nsuccess in recent years. The workshop aimed at gathering people across the board to\naddress seemingly contrasting challenges in the problems they are working on. As part of\nthe call for the workshop, particular attention has been given to the interdependence of\narchitecture, data, and optimization that gives rise to an enormous landscape of design\nand performance intricacies that are not well-understood.\nThis year, our goal was to emphasize the following directions in our community: (i)\nidentify obstacles in the way to better models and algorithms; (ii) identify the general\ntrends from which we would like to build scientiﬁc and potentially theoretical understand-\ning; and (iii) the rigorous design of scientiﬁc experiments and experimental protocols\nwhose purpose is to resolve and pinpoint the origin of mysteries while ensuring repro-\nducibility and robustness of conclusions. In the event, these topics emerged and were\nbroadly discussed, matching our expectations and paving the way for new studies in these\ndirections.\nWhile we acknowledge that the text is naturally biased as it comes through our lens,\nhere we present an attempt to do a fair job of highlighting the outcome of the workshop.\n1Facebook AI\n2DeepMind\n3Google Brain\n4Institut de Physique Thèorique - CEA Saclay\n1\narXiv:2007.13483v2  [cs.LG]  29 Jul 2020\n\n1\nIntroduction\nWhat do we mean by Science and Engineering\nEvery year, machine learning community shares thousands of articles through various confer-\nences and journals. We expect that the work we produce and share in our circles is scientiﬁcally\nsound and robust. At the same time, we acknowledge that the models that are deployed based\non these works have a tremendous impact on technological and social areas. Addressing this\nconnection has never been more critical.\nWhen we ﬁrst announced the workshop, we made an effort to emphasize the need for\ndeeper engagement across communities, in particular, the need for a platform to communicate\nacross groups with drastically different focuses in mind. However, we deliberately omitted a\ndeﬁnition of science and engineering much like the same way we didn’t go into the applied vs\ntheoretical distinction. Here we would like to clarify our approach:\nEach context draws its own boundary that separates seemingly opposite approaches. It may\ndo so by specifying the goals, or by the way it utilizes existing methods. Our title is an attempt\nto reﬂect the joys of increasing the bandwidth in communication with the other side, no matter\nwhich side one begins with. The sessions are not separated according to what approach they\ntake; rather, each session has a concrete problem(s) at hand which presents and discusses how\ndifferent approaches may help tackle the problem.\n2\nStructure and Content\nThe workshop consists of three themed sessions composed of invited talks and short panel\ndiscussions, followed by a panel and contributed talks covering a wide variety of methods and\napplications.\n2.1\nTheory\nTheoretical approaches in understanding the dynamics of neural networks have started in\nthe 90s with early results on the perceptron. In the 2010s, we have seen progress in deep\nlinear models and past few years marked a signiﬁcant development in deep non-linear models.\nQuestions on statistical properties and training dynamics of inﬁnite width limit (Yasaman),\ninﬁnite-depth limit (Surya), and the role of the structure of data (Florent) have shown promising\nprogress. However, in many ways, the developments left many more questions to be answered.\nIn the theory session, we invited a group of researchers to address current challenges in this\narea.\nThe Theory Session was moderated by Lenka Zdeborová, Institut de Physique Théorique\nof Saclay, and discussions have been curated together with Adji Bousso Dieng and Joan\nBruna.\n2\n\nSurya Ganguli1 from Stanford University presented “An analytic theory of generalization\ndynamics and transfer learning in deep linear networks”[20, 21, 13] were he shows how\nremarkable phenomena observed by practitioners in deep architectures emerged already in the\nstudy of deep linear models.\nYasaman Bahri2, researcher at Google Brain, in “Tractable limits for deep networks: an\noverview of the large width regime” [14] analyzed the large width limit of a neural network. In\nthe talk, she showed the advantages of coarse-graining over the parameters space and focusing\non understanding the phenomenology in functional space.\nFinally, Florent Krzakala3, from École Normale Supérieure, explained in “Learning with\n\"realistic\" synthetic data” [8] the limitation of the most common techniques for generating\nsynthetic datasets and proposed new models to go beyond such limits.\nDiscussion\nOverﬁtting, optimal stopping, double-descent curves, expressivity and learning dynamics in\ndeep and wide networks are some of the many aspects that theoretician are tackling in recent\nyears. They are different pieces of the puzzle that we must solve in order to develop a solid\nunderstanding of how deep models work. All those phenomena were studied in detail in models\ncomplex enough to capture those aspects and simple enough to be studied and provide insights.\nThe key takeaway is that most interesting phenomena already appear in very simple, even\nlinear models. And a deeper understanding of those cases shed light into what happens in\ncomplex cases.\nWhile simple models can be enlightening in the dynamics of learning algorithms, being\naware of the limitations of simple models is equally important. Where do they fail? How can\nwe adjust it so that it doesn’t? As an example in modelling, we know that linear models in large\ndimension are affected only by the ﬁrst two moments of their data distribution, therefore they\nfail to capture the impact of higher orders that may be inﬂuential in complex models. On the\ndata front, most theoretical work limits modelling of data with independent priors that doesn’t\nlead to any particular structure, hence we need better modelling of the data that is treatable.\nPutting forward and understanding the limitations of the model and the assumptions adopted\nare fundamental steps to build a solid theory of ML and to improve the communication with\nother communities.\nSince the ﬁeld of machine learning is continually expanding at an increasing pace, the role\nof theory is often questioned in the community. The speakers have addressed this question\nand discussed four ways theory contributes to the ﬁeld: it gives prescriptive principle to guide\npractitioners, it provides comfort with rigorous guarantees, it brings suggestions, and it brings\nconceptual understanding [4]. However, the time to develop new theoretical tools causes\n1Surya Ganguli’s talk An analytic theory of generalization dynamics and transfer learning in deep linear\nnetworks\n2Yasaman Bahri’s talk Tractable limits for deep networks: an overview of the large width regime\n3Florent Krzakala’s talk Learning with \"realistic\" synthetic data\n3\n\noften a delay between discoveries of practitioners and their theoretical understanding, but the\nsystematic application of the scientiﬁc method to carry out theoretical analysis shall help the\ncommunity in bringing clarity of thought in the dust of the results.\n2.2\nVision\nThe last decade has witnessed substantial progress in many computer vision tasks. This\nprogress has been in part enabled by the introduction of large scale benchmarks, such as Ima-\ngeNet, which have led researchers to explore and develop successful approaches in relatively\nconstrained scenarios. However, the visual world is rich and complex and offers interesting\nchallenges ahead. Moving towards real-world vision has been a recurrent topic at SEDL 2019.\nWhat does real-world vision entail? What does the path towards real-world vision look like?\nHow can the spectrum of hypotheses involving theory, application and engineering help us\nnavigate the upcoming challenges?\nThe Vision session was moderated by Natalia Neverova, FAIR, and discussions have been\ncurated with Ilija Radosavovic and Riza Alp Guler.\nThe talk of Carl Doersch from DeepMind on \"Self-supervised visual representation\nlearning\" exposed the importance of building representations that do not rely on huge amounts\nof annotated data and that are useful for downstream tasks, highlighting the potential of\nself-supervised learning to address problems beyond ImageNet [6, 9, 22].\nThe talk of Raquel Urtasun from Uber ATG on \"Science and Engineering for Self-driving\"\naddressed the importance of explainability when building safety-critical systems, and argued\nfor a holistic approach to speciﬁc applications as well as exploiting application-dependent\nprior knowledge, that can beneﬁt from a theoretical explanation. [15, 23, 11, 3].\nFinally, the presentation of Sanja Fidler from Nvidia AI talk emphasized the role of\nthe data when aiming to achieve high-end performance in many speciﬁc applications, and\ndiscusses how to reduce the data annotation burden, by introducing semi-automatic approaches\nto obtain annotations efﬁciently [2, 16].\nDiscussion\nResearchers seek to study real-world vision utilizing challenging computer vision applications\nsuch as self-driving cars or embodied systems. Tackling such applications may require de-\nsigning systems, which build representations that capture relevant semantic properties and\nare useful for downstream tasks, without relying on vast amounts of human-annotated data.\nBroadly speaking, these systems should be able to generalize to the unknown and adapt to any\nnew, dynamic environment – that is they should be able to act under all possible conditions and\nanywhere. Moreover, they should be able to reason about the 3D world and understand intent\nand causality. Besides the previously mentioned objectives, striving for system robustness and\nexplainability is crucial for any safety-critical application.\n4\n\nDuring the workshop, those points were discussed as potential steps were proposed to\naddress some of the above mentioned challenges. On the one hand, the human-annotation\nburden could be mitigated by making simulators more realistic, building semi-automatic\nannotation tools, or exploiting self-supervision and auxiliary data modalities as supervision\nsignals. On the other hand, representations could be improved by building systems that leverage\ngeneral knowledge about the world (e.g. laws of physics) or application-speciﬁc priors, which\nmay help uncover relevant but otherwise invisible variables.\nPresenters proposed potential future directions that would help enhance the synergy\nbetween theory, applications and engineering. First, what is the meaning of big data in the\ncontext of real world vision? Researchers hypothesize that understanding (i) why systems\nwork on current datasets (ImageNet), (ii) how the amount of data and annotations affects\nthe performance of the system, and (iii) how to perform data collection more efﬁciently (e.g.\nby means of very realistic simulated environments or semi-automatic data annotation tools),\nmight shed some light. Second, how could we solve the problems with less data? Researchers\nhypothesize that (i) designing suitable loss functions and regularizers, (ii) exploiting good data\npre-processing techniques, and (iii) including prior knowledge, could lead to improving the\ndata efﬁciency of the systems. Last, how could we better assess the robustness of the designed\nsystems? In this case, researchers hypothesize that deﬁning application-speciﬁc safety bounds\nand improving the understanding of adversarial examples could be two potential research\navenues.\n2.3\nFurther Applications\nDeep learning has made an early impact on application areas such as computer vision [12]\nand speech recognition [10], however, applications on other domains such as NLP, robotics,\nand self-driving cars followed later on. Such widening of application areas created unique\ntasks to tackle. The data distributions, the structure of the problem, the objective functions\nand modalities of the problem presents exciting scientiﬁc and engineering challenges. Some\nof these challenges have a workaround developed by practitioners but scientiﬁcally not very\nwell-studied such as the dataset generation and the state of art models like BERT [5]. Some\ninteresting questions like \"How should be a proper benchmark for an NLP task generated?\",\n\"When the theory can be helpful for the applications?\", and \"How do we detect overﬁtting in\ngenerative models?\" were raised during the talks.\nYann Dauphin from Google Brain moderated the further applications session and the\ndiscussions have been curated with the support of Orhan Firat and Dilan Gorur.\nThe talk Douwe Kiela4 from FAIR has focused on the adversarial NLI and how to create\nnew natural language benchmarks that require more robust models [19]. During the talk Douwe\ndiscussed the shortcomings of typical machine learning benchmarks, and how they can be\nexploited easily. Instead, dynamic benchmarks that can be extended by adopting an adversarial\nstrategy.\n4Douwe Kiela’s talk Benchmarking Progress in AI: A New Benchmark for Natural Language Understanding\n5\n\nAudrey Durand5, from Université Laval and MILA, discussed the trade-off of theory and\napplication in bandits [7, 17]. UCB was favored for their theoretical guarantees, however, some\npractitioners used Thompson sampling, a method with sparse theoretical results until 2011.\nFurther examples have been discussed on how such developments made a big impact on the\napplications of the method and the other way around as well.\nKamalika Chaudhuri6 from University of California San Diego presented a recent work\non a non-Parametric test to detect data-copying and overﬁtting in generative models [18].\nKamalika ﬁrst discussed how overﬁtting happens in generative models and proposed a method\nto detect overﬁtting in generative models by using 3-sample test instead of using methods\nsimilar to Mann-Whitney test.\nDiscussion\nAlthough the limits of applied deep learning techniques are unclear, those methods are (unde-\nniably) widely used and full surprises. Such potentially powerful tools consequently create\na competitive environment across domains. And each area has its own set of theoretical and\nengineering challenges. Common (and recurring) themes of challenges across disciplines\nemerge: evaluation of proposed models relying on test data, the concept of under/overﬁtting\nfor a given model, and the reliability-popularity trade-off of models with weak theoretical\nguarantees.\nTypically, problems that move beyond their infancy have their ﬁxed benchmark datasets\nfor which new models are evaluated. Even though benchmark datasets can be renewed, the\nnumber of proposed models far exceeds the number of existing benchmarks. This abundance\nof models in comparison to existing datasets leads to a competitive publication environment\nwhere breaking records on a given task is the sole goal. On the other hand, for every given\nmodel, one can adversarially modify the data in such a way that the update can break the\nmodel. Such fragility may imply either that the models are not good enough, or alternatively,\nthe framework in which we test our models is not good enough. If it is the latter, we can\nchange the way we curate data more frequently and adopt a notion of dynamic datasets along\nwith reproducible testing pipelines. A further emphasis of focus on datasets themselves seems\nnecessary. This change of focus will also imply changes in the way we test models and evaluate\nunder/over-ﬁtting.\nVarious areas of applications showcase the trade-off of theory and application in topics\nwhere simple, yet useful, abstractions have wide-ranging engineering applications. On the one\nhand, part of the community may wait for convergence guarantees before widely deploying\ncertain models even though they are demonstrated to work in practice. On the other hand,\nthe existence of working models also exempliﬁes how prior empirical ﬁndings help shape\ntheoretical efforts, thereby once again emphasizing the value of communication across groups.\n5Andray Durand’s talk Trading off theory and practice: A bandit perspective\n6Kamalika Chaudhuri’s talk A Three Sample Test to Detect Data Copying in Generative Models\n6\n\n2.4\nPanel - Interaction between science and engineering\nThe massive expansion of computer science as a ﬁeld gave rise to a considerable number\nof sub-communities that developed their own vocabulary and their methods. Nowadays, the\nseparation of those sub-communities, and the consequent communication issues, are at the\nbasis of several problems that affect the whole community. This can be seen in: old results\noften being independently rediscovered; and in the struggle of peer-reviewing on topics at\nthe cross-section of two or more sub-communities. All these problems are aggravated by the\nunsustainable demand for publications in the ﬁeld, that segregate the communities even further\nand lower the overall quality of the publications.\nDuring the panel, the lack of communication across these sub areas has been discussed,\nand it was pointed out that there is a need for a joint effort in reducing such distances across\nsub-communities. We should make an effort to increase awareness for the works that appear\nin our sub-disciplines while maintaining the identity and purpose of each group. An effort is\nneeded in reducing the number and improving the quality of publications, as well. Everyday, it\nbecomes harder to keep track of the publications, and the fast reviewing process is pauperizing\nthe quality of the publications. This leads to a situation in which it gets harder to trust published\nresults which also results in reproducibility issues. An idea that will help in this direction is\nto adopt open-reviewing strategies. Eventually adding a layer on-top of arXiv by giving the\npossibility to comment and reply in preprints might alleviate some of the troubles.\nIn the discussion Finale Doshi-Velez (Harvard), Surya Ganguli (Stanford), Been Kim\n(Google Brain), Aparna Lakshmiratan (Facebook), Jason Yosinski (Uber) intervene, and\nthe panel was moderated by Zack Lipton (Carnegie Mellon University) with advisory support\nby Michela Paganini.\n2.5\nContributed sessions\nWe received 45 diverse and high-quality submissions. With the help of 34 reviewers, we\nevaluated the quality of the papers and managed to check their ﬁtness to the theme of the\nworkshop discussed in our call for papers. In the end, we selected 27 submissions for a\nposter presentation, among which we further selected 5 where the authors delivered an oral\npresentation of their work.\nCall for papers invited speciﬁc types of works that either lie at the intersection of areas,\nfurthermore we also added three more additional optional criteria: (1) negative results, (2) sim-\npliﬁed versions of complex models from practitioners to be presented to theoretical scientists,\n(3) possible broader implications theoretical work addressed in a way that is accessible to the\npractically minded. We added these options because it can be particularly challenging to get\nthis kind of work accepted in today’s publication regime. But it is precisely this kind of work\nthat, in our opinion, fosters the communication across said groups.\nHere, we provide brief descriptions of 5 contributed presentations:\n7\n\nSho Yaida from Facebook AI Research presented Non-Gaussian Processes and Neural\nNetworks at Finite Widths analyzing in perturbation theory the effect on ﬁnite widths on an\ninﬁnitely-wide net. The study allows to go through the network ﬂow layer by layer integrating\nout random variables, analogous to the renormalization-group ﬂow.\nJonathan Frankle from MIT presented Training Batchnorm and Only Batchnorm a work\nin collaboration with David J Schwab (ITS, CUNY Graduate Center) and Ari S Morcos\n(Facebook AI Research (FAIR)). In the work, the authors show a network could achieve\nhigh accuraccy in real datasets having ﬁxed random layers and only tuning the parameters of\nbatchnorm.\nGuy Gur-Ari (Google) presented Asymptotics of Wide Networks from Feynman Diagrams,\na joint work with Ethan Dyer (Google). In the work, the authors used Faynman diagrams\nto analyse the training dynamics of wide networks going beyond the large width limit by\nobtaining closed-form expressions for the higher-order terms.\nYiDing Jiang from Google presented Fantastic Generalization Measures and Where to\nFind Them, in collaboration with Behnam Neyshabur (Google), Dilip Krishnan (Google),\nHossein Mobahi (Google Research) and Samy Bengio (Google Research, Brain Team). What\nis the right measure complexity measure for deep neaural network? In their work, numerous\nwell-designed experiments are compared through various complexity measures.\nMartin Ma and Muqiao Yang from Carnegie Mellon University presented their work\nComplex Transformer: A Framework for Modeling Complex-Valued Sequence in collaboration\nwith Dongyu Li (Carnegie Mellon University), Yao-Hung Tsai (Carnegie Mellon University)\nand Ruslan Salakhutdinov (Carnegie Mellon University). The work exploits phenomenon\nthat are better captured through the use Fourier Tranform. In their study, the authors de-\nvelop a transformer that is able to handle complex representations, achieving state-of-the-art\nperformance.\n3\nRecurring topics of debate\nDiscussion on two kinds of theoretical contributions\nThroughout the sessions and especially in the panel discussion, participants emphasized differ-\nences in theoretical approaches to studying inner workings of deep models. Two overarching\nlines of research emerged: (1) theoretical guarantees focusing on rigorous assumptions leading\nto rigorous theorems, (2) complex systems approach of probing the models to test their limits\nand ﬁnd out about their properties. A theme that emerged throughout the workshop is the limits\nof applicability of theory when one insists on the rigor to an extent that it allows provability but\nshaves off of their capacity to explain observed phenomena. Even though participants pointed\nout the value of mutual existence of both approaches, the cultural clash that systematically\ndevalues the latter approach has been criticized as it hinders progress in understanding the\nphenomenon.\n8\n\nBroader impact of theoretical studies\nImpact of theory has been discussed in two core areas: (1) limitations of theoretical results, (2)\neffect of theory in practical applications. In some sense, both questions are related to a notion\nof the boundary of applicability. In the former case, once the necessary restrictive assumptions\nhave been lifted, things may fall apart. In the latter, the phenomenon may hold for a broader\nset of not-yet-proven cases. A common theme across the sessions has been about the idea of\nresearchers being more aware and thoughtful of the boundaries of their work.\nThe importance of toy models\nScience/theory builds upon simple models. They are on the one hand useful for analytical\nstudies, but on the other hand, they help to identify relevant factors contributing to phenomena\nof interest. A model can also come from limiting situations of real architectures, for instance\ninﬁnitely wide or inﬁnitely deep networks may be far from reality but can already capture part\nof the phenomenology. In this limit, we can understand more and eventually move back to\nthe practical cases by expanding around the limits. An example of this is the current strong\ninterest around the neural tangent kernel limit. It may be true that this limit does not capture\nthe reality, but the large number of results collected in that set-up may pave the fundamental to\nthe next discovery.\nUnderstanding structure in data\nData, architecture and algorithm are the three pillars for funding the theory of machine learning.\nTheir study and, in particular, the study of their interactions is a necessity to step forward. So\nfar, a huge effort has been put to study architectures and algorithms, now we need to study\ndata better. All these directions are crucial, but the role of data is the least explored. Many\napplications rely on a large amount of data that in many cases, we can hardly rely on automatic\nlabelling. A better understanding will have a dramatic impact on the whole community. Better\ninsights, for instance, allow building more realistic simulators or designing better priors to\nreduce the number of data in training.\n4\nConclusions\nIn the last decade, the mathematical theory behind ML did not match the pace of the progress\nin its applications. We argue that the use of scientiﬁcally sound but non-rigorous methods will\nbring an important contribution to reduce the distance between those communities, in particular\nthe role of physics. In its history, physics developed a forma mentis that helped in dealing with\ncomplex problems. The mindset and several (non-rigorous) tools of physics can be applied in\nML as they have been applied to other (natural) phenomena. In particular, physics/physicists\nhelp the community by placing themselves in between mathematicians/theoretical computer\nscientists and engineers, in the abstract line that connects pure science to empirical sciences\n9\n\nor engineering. As a community, we should sustain the connection between theory and ap-\nplications. Fundamental knowledge gives prescriptive principle to guide practitioners, brings\nsuggestions and conceptual insides. All applied branches of ML would beneﬁt from those\npoints, in particular, critical applications where good performances of ML are not enough for\nits adoption but where explainability is needed, for instance, for accountability.\nThe complexity of the visual world, textual world, spaces of exploration and as such\nare widely complex, which hinders theoretical progress in the respective ﬁelds. Yet, many\ncommunities have successfully explored approaches that work well on various previously\nchallenging tasks. The goal we seek to reach is to put such advances to a solid grounding,\nthereby help develop further progress.\nSEDL 2019 workshop gathered different perspectives, from researchers and practitioners,\nto address seemingly contrasting challenges. The role of science and engineering was largely\ndiscussed in all the sessions, concluding that science and engineering are intertwined, and their\nsymbiotic interaction is beneﬁcial to push the boundaries of research in both theory-driven\nand application-driven sub-ﬁelds. However, to truly beneﬁt from the diversity of thought that\narises across such sub-ﬁelds, it is important to establish effective communication strategies\nthat mitigate the existing language barriers. Thinking of a continuous spectrum of interests\nacross sub-ﬁelds could help us improve communication and hopefully lead to more impactful\ncontributions.\nAcknowledgements\nThe authors of this text assume all the responsibility for the opinions selected and presented\nin the present report. However, we would like to take the chance to thank everyone involved\nin the making of the workshop. We thank Yasaman Bahri whose contributions shaped the\nbeginning of the workshop. Our advisors, Joan Bruna, Adji Bousso Dieng, Ilija Radosavovich,\nRiza Alp Guler, Orhan Firat, Dilan Gorur, and Michela Paganini, helped curate the talks\nand discussion points for each session. Our moderators, Lenka Zdeborova, Natalia Neverova,\nYann Dauphin, and Zack Lipton, helped bridge the talks and discussions. Our speakers, Surya\nGanguli, Yasaman Bahri, Florent Krzakala, Carl Doersch, Raquel Urtasun, Sanja Fidler, Douwe\nKiela, Audrey Durand, Kamalika Chaudhuri, delivered excellent seminars. And ﬁnally, our\npanelists, Finale Doshi-Velez, Surya Ganguli, Been Kim, Aparna Lakshmiratan, Jason Yosinski\ncontributed intriguing discussions. Our speakers for the contributed talks, Sho Yaida, Jonathan\nFrankle, Guy Gur-Ari, YiDing Jiang, Martin Ma, Muqiao Yang. We are glad to hear that all\nthe discussions hit right at the heart of the theme of the workshop.\nReferences\n[1] SEDL Workshop at NeurIPS 2019 - Website. https://sites.google.com/view/\nsedl-workshop/past-editions/2019-main.\n10\n\n[2] David Acuna, Huan Ling, Amlan Kar, and Sanja Fidler. Efﬁcient interactive annotation\nof segmentation datasets with polygon-rnn++. 2018.\n[3] Min Bai, Gellert Mattyus, Namdar Homayounfar, Shenlong Wang, Kowshika Laksh-\nmikanth, Shrinidhi, and Raquel Urtasun. Deep multi-sensor lane detection. In IROS,\n2018.\n[4] Leo Breiman. Reﬂections after refereeing papers for nips. The Mathematics of General-\nization, pages 11–15, 1995.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n[6] Carl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsupervised visual representation\nlearning by context prediction. In International Conference on Computer Vision (ICCV),\n2015.\n[7] Audrey Durand, Charis Achilleos, Demetris Iacovides, Katerina Strati, Georgios D.\nMitsis, and Joelle Pineau. Contextual bandits for adapting treatment in a mouse model\nof de novo carcinogenesis. In Finale Doshi-Velez, Jim Fackler, Ken Jung, David Kale,\nRajesh Ranganath, Byron Wallace, and Jenna Wiens, editors, Proceedings of the 3rd\nMachine Learning for Healthcare Conference, volume 85 of Proceedings of Machine\nLearning Research, pages 67–82, Palo Alto, California, 17–18 Aug 2018. Proceedings of\nMachine Learning Research (PMLR).\n[8] Sebastian Goldt, Marc Mézard, Florent Krzakala, and Lenka Zdeborová. Modelling the in-\nﬂuence of data structure on learning in neural networks. arXiv preprint arXiv:1909.11500,\n2019.\n[9] Olivier J. Hénaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali\nEslami, and Aäron van den Oord. Data-efﬁcient image recognition with contrastive\npredictive coding. Computing Research Repository (CoRR), abs/1905.09272, 2019.\n[10] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep\nJaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep\nneural networks for acoustic modeling in speech recognition: The shared views of four\nresearch groups. IEEE Signal processing magazine, 29(6):82–97, 2012.\n[11] Namdar Homayounfar, Wei-Chiu Ma, Justin Liang, Xinyu Wu, Jack Fan, and Raquel\nUrtasun. Dagmapper: Learning to map by discovering lane topology. In The IEEE\nInternational Conference on Computer Vision (ICCV), October 2019.\n[12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with\ndeep convolutional neural networks. In Advances in neural information processing\nsystems (NeurIPS), pages 1097–1105, 2012.\n11\n\n[13] Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics\nand transfer learning in deep linear networks. arXiv preprint arXiv:1809.10374, 2018.\n[14] Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha\nSohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as\nlinear models under gradient descent. In Advances in neural information processing\nsystems (NeurIPS), pages 8570–8581, 2019.\n[15] Ming Liang, Bin Yang, Yun Chen, Rui Hu, and Raquel Urtasun. Multi-task multi-sensor\nfusion for 3d object detection. In The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2019.\n[16] Huan Ling, Jun Gao, Amlan Kar, Wenzheng Chen, and Sanja Fidler. Fast interactive\nobject annotation with curve-gcn. In The IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 5257–5266. Computer Vision Foundation / IEEE,\n2019.\n[17] Andrei Lupu, Audrey Durand, and Doina Precup. Leveraging observations in bandits: Be-\ntween risks and beneﬁts. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvolume 33, pages 6112–6119, 2019.\n[18] Casey Meehan, Kamalika Chaudhuri, and Sanjoy Dasgupta. A non-parametric test to\ndetect data-copying in generative models. arXiv preprint arXiv:2004.05675, 2020.\n[19] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\nAdversarial nli: A new benchmark for natural language understanding. arXiv preprint\narXiv:1910.14599, 2019.\n[20] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlin-\near dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120,\n2013.\n[21] Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of\nsemantic development in deep neural networks. Proceedings of the National Academy of\nSciences (PNAS), 116(23):11537–11546, 2019.\n[22] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with con-\ntrastive predictive coding. Computing Research Repository (CoRR), abs/1807.03748,\n2018.\n[23] Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and\nRaquel Urtasun. End-to-end interpretable neural motion planner. In The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June 2019.\n12",
    "pdf_filename": "Post-Workshop Report on Science meets Engineering in Deep Learning, NeurIPS 2019, Vancouver.pdf"
}