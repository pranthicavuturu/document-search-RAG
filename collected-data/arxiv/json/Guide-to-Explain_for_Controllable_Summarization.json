{
    "title": "Guide-to-Explain for Controllable Summarization",
    "context": "Recently, large language models (LLMs) have demonstrated remarkable performance in ab- stractive summarization tasks. However, con- trollable summarization with LLMs remains underexplored, limiting their ability to generate summaries that align with specific user pref- erences. In this paper, we first investigate the capability of LLMs to control diverse attributes, revealing that they encounter greater challenges with numerical attributes, such as length and ex- tractiveness, compared to linguistic attributes. To address this challenge, we propose a guide- to-explain framework (GTE) for controllable summarization. Our GTE framework enables the model to identify misaligned attributes in the initial draft and guides it in explaining er- rors in the previous output. Based on this re- flection, the model generates a well-adjusted summary. As a result, by allowing the model to reflect on its misalignment, we generate sum- maries that satisfy the desired attributes in sur- prisingly fewer iterations than other iterative methods solely using LLMs. 1 Large language models (LLMs) have demonstrated tion, outperforming traditional encoder-decoder models by generating more contextually appro- priate and natural summaries (Goyal et al., 2023; Zhang et al., 2024; Pu et al., 2023; Ryu et al., 2024b). In addition, recent studies aimed to gen- erate higher-quality summaries by leveraging the self-correction capabilities of LLMs (Zhang et al., 2023a; Sun et al., 2024). However, given individ- uals’ diverse preferences for summary styles, it is essential to generate summaries that adjust personal needs (Zhang et al., 2023b). For instance, some users may prefer concise summaries or retain exact phrases from the original text. Therefore, controllable summarization has re- cently garnered attention (Zhong et al., 2021; Xu et al., 2023; Zhang et al., 2023b). Previous re- search employed encoder-decoder models to con- trol attributes (Mao et al., 2022; Zhang et al., 2022; Vig et al., 2022; Pagnoni et al., 2023; Wang et al., 2023; Urlana et al., 2024). Although LLMs ex- cel in generating high-quality summaries, they still face challenges in controlling attributes (Yuan et al., 2024; Tang et al., 2023), and their controllability has been underexplored (Liu et al., 2024). Thus, we analyze LLMs’ ability to control var- ious attributes in summarization and refine the measurements to more accurately assess these at- tributes. We reveal that while LLMs excel at controlling linguistic attributes such as topic and speaker, they severely struggle with numerical at- tributes such as extractiveness and length. To ad- dress this challenge, we propose a guide-to-explain (GTE), which enables precise attribute control solely through LLMs without relying on external modules or training. We first design an attribute- identification step to calculate misaligned attributes in LLM-generated summaries, subsequently guid- ing the model to explain the sources of its errors. By self-reflecting its own errors, the model can ad- equately adjust attributes in subsequent iterations. We introduce the self-refine strategy, primarily used in reasoning tasks with LLMs (Weng et al., 2023; Madaan et al., 2023; Dhuliawala et al., 2024; Gou et al., 2024), to controllable summarization. Additionally, we evaluate GTE on mixed attribute control datasets, MACSumDoc and MACSumDial (Zhang et al., 2023b). GTE success- fully controls each attribute with minimal iterations solely using LLMs, outperforming other iteration methods. We also demonstrate the high quality of the controlled summaries via multiple evaluation metrics. In addition, we analyzed whether LLMs can control multiple attributes simultaneously. We found out that LLMs struggle with jointly control- ling correlated numerical attributes. Our contribu- tions are as follows: arXiv:2411.12460v1  [cs.CL]  19 Nov 2024",
    "body": "Guide-to-Explain for Controllable Summarization\nSangwon Ryu1, Heejin Do1, Daehee Kim1,\nYunsu Kim3, Gary Geunbae Lee12, Jungseul Ok12\n1Graduate School of Artificial Intelligence, POSTECH, South Korea\n2Department of Computer Science and Engineering, POSTECH, South Korea\n3aiXplain Inc., Los Gatos, CA, USA\n{ryusangwon, heejindo, andrea0119, gblee, jungseul}@postech.ac.kr,\nyunsu.kim@aixplain.com\nAbstract\nRecently, large language models (LLMs) have\ndemonstrated remarkable performance in ab-\nstractive summarization tasks. However, con-\ntrollable summarization with LLMs remains\nunderexplored, limiting their ability to generate\nsummaries that align with specific user pref-\nerences. In this paper, we first investigate the\ncapability of LLMs to control diverse attributes,\nrevealing that they encounter greater challenges\nwith numerical attributes, such as length and ex-\ntractiveness, compared to linguistic attributes.\nTo address this challenge, we propose a guide-\nto-explain framework (GTE) for controllable\nsummarization. Our GTE framework enables\nthe model to identify misaligned attributes in\nthe initial draft and guides it in explaining er-\nrors in the previous output. Based on this re-\nflection, the model generates a well-adjusted\nsummary. As a result, by allowing the model to\nreflect on its misalignment, we generate sum-\nmaries that satisfy the desired attributes in sur-\nprisingly fewer iterations than other iterative\nmethods solely using LLMs.\n1\nIntroduction\nLarge language models (LLMs) have demonstrated\nsuperior performance in abstractive summariza-\ntion, outperforming traditional encoder-decoder\nmodels by generating more contextually appro-\npriate and natural summaries (Goyal et al., 2023;\nZhang et al., 2024; Pu et al., 2023; Ryu et al.,\n2024b). In addition, recent studies aimed to gen-\nerate higher-quality summaries by leveraging the\nself-correction capabilities of LLMs (Zhang et al.,\n2023a; Sun et al., 2024). However, given individ-\nuals’ diverse preferences for summary styles, it is\nessential to generate summaries that adjust personal\nneeds (Zhang et al., 2023b). For instance, some\nusers may prefer concise summaries or retain exact\nphrases from the original text.\nTherefore, controllable summarization has re-\ncently garnered attention (Zhong et al., 2021; Xu\net al., 2023; Zhang et al., 2023b). Previous re-\nsearch employed encoder-decoder models to con-\ntrol attributes (Mao et al., 2022; Zhang et al., 2022;\nVig et al., 2022; Pagnoni et al., 2023; Wang et al.,\n2023; Urlana et al., 2024). Although LLMs ex-\ncel in generating high-quality summaries, they still\nface challenges in controlling attributes (Yuan et al.,\n2024; Tang et al., 2023), and their controllability\nhas been underexplored (Liu et al., 2024).\nThus, we analyze LLMs’ ability to control var-\nious attributes in summarization and refine the\nmeasurements to more accurately assess these at-\ntributes.\nWe reveal that while LLMs excel at\ncontrolling linguistic attributes such as topic and\nspeaker, they severely struggle with numerical at-\ntributes such as extractiveness and length. To ad-\ndress this challenge, we propose a guide-to-explain\n(GTE), which enables precise attribute control\nsolely through LLMs without relying on external\nmodules or training. We first design an attribute-\nidentification step to calculate misaligned attributes\nin LLM-generated summaries, subsequently guid-\ning the model to explain the sources of its errors.\nBy self-reflecting its own errors, the model can ad-\nequately adjust attributes in subsequent iterations.\nWe introduce the self-refine strategy, primarily used\nin reasoning tasks with LLMs (Weng et al., 2023;\nMadaan et al., 2023; Dhuliawala et al., 2024; Gou\net al., 2024), to controllable summarization.\nAdditionally, we evaluate GTE on mixed\nattribute control datasets,\nMACSumDoc\nand\nMACSumDial (Zhang et al., 2023b). GTE success-\nfully controls each attribute with minimal iterations\nsolely using LLMs, outperforming other iteration\nmethods. We also demonstrate the high quality of\nthe controlled summaries via multiple evaluation\nmetrics. In addition, we analyzed whether LLMs\ncan control multiple attributes simultaneously. We\nfound out that LLMs struggle with jointly control-\nling correlated numerical attributes. Our contribu-\ntions are as follows:\narXiv:2411.12460v1  [cs.CL]  19 Nov 2024\n\n• We revisit the measurement of various at-\ntributes and analyze LLMs’ controllability.\n• We propose a guide-to-explain (GTE), which\nguides the model to explain its misalignments.\n• GTE effectively adjusts misaligned attributes\nonly in a few iterations.\n2\nEvaluating controllability of LLMs\n2.1\nControllable attributes\nWe investigate the controllability of LLMs for\nfour attributes: extractiveness, length, topic, and\nspeaker. Extractiveness evaluates how much of the\nsummary’s content directly overlaps with the origi-\nnal text. A highly extractive summary is required\nwhen users need to retain the original context, such\nas in academic papers; however, paraphrasing is\napplied to tailor the summary in general cases. The\nlength counts the ratio between the main text and\nthe summary. The preferred summary length varies\ndepending on the information density of the text\nand user preferences. For topics or speakers, users\nmay prefer summaries focused on a specific topic\nor speakers from a long document or dialogue.\nAttribute measurement\nPrevious methods have\nnot effectively accounted for attribute-focused as-\npects. In the case of extractiveness, it is straightfor-\nward to determine how much of the summary’s con-\ntent directly overlaps with the original text. How-\never, for length, prompts suggested by earlier works\nspecify a fixed number of sentences, e.g., \"3 sen-\ntences,\" but this approach fails to account for vari-\nations in sentence length and does not accurately\nreflect the summary’s actual length (Goyal et al.,\n2023; Liu et al., 2024; Yuan et al., 2024). Thus,\nwe calculate the summary length as a ratio relative\nto the main text. For topics, Zhang et al. (2023b)\ncalculated the frequency of topic-related words in\nthe summary. However, even if topic words do\nnot explicitly appear, the summary can still reflect\nthe core context of the topic, especially for LLM-\ngenerated summaries, which tend to paraphrase\nthe content. Therefore, we compute the embed-\nding similarity B between the topic word and each\nword in the summary s: 1\nn\nP\ni∈s B(topic, wordi),\nwhere n is the number of words in the sum-\nmary. If multiple topics k are present, we use\nthe average embedding similarity across all topics:\n1\nk\nP\nj∈k\n1\nn\nP\ni∈s B(topick, wordi). For speakers,\nZhang et al. (2023b) calculate the frequency of the\nNew Delhi, India / CNN)Police have arrested four employees \nof a popular Indian ethnic-wear chain after a minister \nspotted a security camera overlooking the changing room of \none of its stores.Federal education minister Smriti Irani was \nvisiting a FabIndia outlet ..\nFour FabIndia employees were arrested after a minister \ndiscovered a security camera overlooking the changing room \nat a Goa outlet. The camera was found to be capable of \ntaking photos of customers...\nThe length (word count) of the summary is 59 and the \nlength of the article is 223, so it is approximately 26.5%. \nExplain why your calculation is wrong, and revise the \nsummary in the 32.5% length of the article:\nThere was a mistake in my previous calculation ..\nFabIndia is in trouble after a minister found a security \ncamera overlooking the changing room at one of its Goa \noutlets. The camera was capable of taking photos of \ncustomers. The incident was exposed when Federal ..\nSummarize the above article in 32.5% length of the article:\nAIS\nGS\nGTE\n=\n+\nFigure 1: Overview of guide-to-explain system (GTE).\nspeaker’s spoken words appearing in the summary.\nIn contrast, we extract speaker utterances and con-\nstruct speaker set S and utilize BERTScore (Zhang\net al., 2020) to compute the embedding similarity\nbetween the summary p and S: BERTScore(p, S).\nLabel reinterpretation\nWe use the publicly\navailable MACSum dataset (Zhang et al., 2023b)\nfor controllable summarization. However, exist-\ning labels for numerical attributes are ambiguous\nas criteria degrees are not specified (e.g., high for\nextractiveness and short for length). To provide\ndetailed criteria, we reinterpret the labels based\non the attribute distributions in each training set.\nFor extractiveness, we set labels as {normal: 85%,\nhigh: 90%, fully: 100%}. Unlike previous meth-\nods, we define the summary length as a ratio of the\noriginal text rather than a fixed value. Since the\nexisting labels do not distinctly differentiate these\nratios, we set the lengths to {short: 7.5%, normal:\n15%, long: 32.5%}, providing clearer distinctions\nwith evaluating a broader range of controllability.\nLabeling details can be found in Appendix A.\n2.2\nControllablility assessment\nWe evaluate the ability of LLMs to adjust their out-\nputs based on specified attributes (Table 1, 2). Our\nassessment includes two evaluations: (1) the failure\nrate upon reaching the predefined maximum itera-\ntions without achieving the desired modifications\nand (2) the average iterations required to adjust\nan attribute, calculated only for successful cases.\nWe denote the naive iteration approach, which sim-\nply adjusts attributes repeatedly, as Iter. Most\nLLMs effectively control linguistic attributes, such\nas topic and speaker. However, LLMs struggle\nwith numerical attributes, including extractiveness\nand length. Both Llama-70B and GPT-4o-Iter\n\nModel\nExtractiveness (↓/ ↓)\nLength (↓/ ↓)\nTopic(↓)\nSpeaker(↓)\nnormal\nhigh\nfull\navg\nshort\nnormal\nlong\navg\nPhi-3-medium-Iter\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n38.08% / 0.22\n-\nPhi-3-medium-GTE\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n37.97% / 0.04\n-\nLlama3-8B-Iter\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n100.00% / ⟳\n57.14% / 0.12\n-\nLlama3-8B-GTE\n12.63% / 3.52\n11.63% / 2.53\n0.00% / 1.46\n11.70% / 3.26\n26.40% / 3.08\n10.92% / 2.26\n13.18% / 3.85\n14.99% / 2.80\n25.56% / 0.91\n-\nLlama3-70B-Iter\n54.82% / 8.44\n37.21% / 7.47\n2.70% / 3.78\n49.91% / 8.05\n18.40% / 6.58\n54.61% / 10.42\n67.44% / 12.00\n49.36% / 8.24\n0.00% / 0.24\n-\nLlama3-70B-AIS\n26.55% / 6.57\n18.60% / 7.81\n0.00% / 1.86\n24.14% / 6.52\n4.80% / 5.42\n2.73% / 3.81\n10.85% / 4.84\n5.12% / 4.39\n0.00% / 0.10\n-\nLlama3-70B-GTE\n0.21% / 3.28\n0.00% / 2.83\n0.00% / 1.50\n0.18% / 3.22\n0.00% / 1.10\n0.00% / 1.61\n2.32% / 3.14\n0.55% / 1.90\n0.00% / 0.01\n-\nGPT-3.5-Iter\n45.18% / 9.80\n60.47% / ⟳\n94.59% / ⟳\n49.73% / 9.80\n53.60% / ⟳\n80.89% / ⟳\n88.37% / ⟳\n76.42% / ⟳\n0.00% / 0.00\n-\nGPT-3.5-GTE\n17.56% / 3.86\n51.16% / 5.00\n67.57% / 4.00\n23.58% / 3.90\n5.60% / 4.63\n44.03% / 6.62\n78.29% / 7.00\n43.33% / 5.95\n0.00% / 0.00\n-\nGPT-4o-Iter\n34.69% / 6.77\n55.81% / ⟳\n78.38% / 3.00\n39.31% / 6.63\n72.00% / ⟳\n64.85% / ⟳\n79.07% / ⟳\n69.84% / ⟳\n0.38% / 0.02\n-\nGPT-4o-AIS\n35.12% / 5.50\n48.84% / 15.50\n62.16% / 6.00\n38.03% / 6.13\n60.00% / 8.79\n61.09% / 9.40\n78.29% / 2.00\n64.90% / 8.60\n0.00% / 0.04\n-\nGPT-4o-GTE\n0.00% / 2.76\n0.00% / 4.70\n0.00% / 2.03\n0.00% / 2.87\n0.00% / 1.20\n0.00% / 1.21\n0.00% / 1.96\n0.00% / 1.42\n0.00% / 0.02\n-\nTable 1: The results of controllability measured on the MACSumDoc dataset. The bold denotes the best performance.\nWe set the maximum number of Iterations to 20. If all the cases remained unadjusted in the initial draft and still\nnot be resolved after reaching the maximum iterations, they were marked with ⟳.\nModel\nExtractiveness (↓/ ↓)\nLength (↓/ ↓)\nTopic (↓)\nSpeaker (↓)\nnormal\nhigh\nfully\navg\nshort\nnormal\nlong\navg\nLlama3-70B-Iter\n31.78% / 8.13\n43.59% / 8.40\n8.16% / 5.39\n29.63% / 7.59\n12.00% / ⟳\n93.75% / 6.00\n98.00% / ⟳\n81.79% / 6.00\n0.00% / 0.01\n0.00% / 0.00\nLlama3-70B-AIS\n14.41% / 5.91\n23.08% / 5.31\n0.00% / 3.72\n13.27% / 5.50\n0.00% / 1.25\n62.05% / 5.70\n92.00% / 9.33\n57.10% / 5.62\n0.00% / 0.02\n0.00% / 0.00\nLlama3-70B-GTE\n0.00% / 2.31\n0.00% / 2.56\n4.08% / 3.64\n0.61% / 2.49\n0.00% / 1.00\n36.61% / 4.73\n80.00% / 5.70\n37.65% / 4.53\n0.00% / 0.01\n0.00% / 0.00\nGPT-4o-Iter\n79.24% / 4.36\n82.05% 3.67\n59.18% / 1.00\n76.54% / 4.00\n6.00% / ⟳\n98.21% / ⟳\n100.00% / ⟳\n84.26% / ⟳\n0.31% / 0.01\n0.00% / 0.00\nGPT-4o-AIS\n84.75% / 4.00\n87.18% 1.50\n53.06% 5.10\n80.25% / 4.32\n2.00% / 4.50\n96.43% / ⟳\n100.00% / ⟳\n82.41% / 4.50\n0.00% / 0.01\n0.00% / 0.00\nGPT-4o-GTE\n17.80% / 7.94\n25.64% / 7.92\n8.16% / 4.58\n17.28% / 7.53\n0.00% / 1.40\n9.82% / 2.75\n44.00% / 4.21\n13.58% / 2.90\n0.00% / 0.02\n0.00% / 0.00\nTable 2: The results of controllability measured on the MACSumDial dataset.\nshow a very high failure rate with numerous iter-\nations. In particular, LLMs struggle to adjust for\nnormal extractiveness and long length attributes.\n3\nGTE\nTherefore, we introduce a guide-to-explain (GTE)\nframework (Figure 1) to adjust the challenging nu-\nmerical attributes. We provide an attribute iden-\ntification step to adjust incorrectly generated re-\nsponses and guide the LLMs to reflect by explain-\ning the reasons behind these errors. Our approach\nallows the model to make appropriate corrections\nin subsequent iterations.\n3.1\nAttribute identification step\nWe first prompt the LLM to generate an initial draft\ns′ that reflects the specified attributes i. If the LLM\nfails to control the instructed attributes accurately,\nwe provide attribute identification step (AIS) to\nguide the model on how to adjust the attributes.\nLLM may have difficulty measuring attributes such\nas extractiveness or the length ratio relative to the\noriginal text. Thus, we provide a step-by-step ap-\nproach to instruct the model on revising its sum-\nmary based on the generated output.\n3.2\nGuidance step\nAfter providing instructions AIS on revising the\nsummary, we supply guidance step (GS) to the\nmodel to explain why it initially failed to adjust\nthe attributes correctly. This process is similar to\nhow humans solve complex problems by reviewing\ntheir mistakes to produce more accurate responses\nin the future. Receiving [a; i, s′; AIS; GS] as in-\nputs, where a denotes the article, the model first\nreflects on the reasons for the initial error before\ngenerating a revised summary. If the revised sum-\nmary still fails to satisfy the attributes, GTE repeats\nuntil the model produces an attribute-compliant\nsummary. As LLMs are known to struggle with\nnumber-related tasks (Thawani et al., 2021; Imani\net al., 2023), our guidance to explain why their cal-\nculation is incorrect, followed by generating sum-\nmaries assists in effectively controlling the numeric\nattributes. The used prompts are in Appendix B.\n4\nExperimental setup\nWe evaluate the controllability of various LLMs,\nincluding Phi-3 (Abdin et al., 2024), Llama3\n(Dubey et al., 2024), and GPT series (Brown, 2020;\nAchiam et al., 2023). To analyze model perfor-\nmance by size, we use both the 8B and quantized\n70B versions 1 of Llama3, and GPT-3.5 and GPT-4.\nWe used two datasets, the MACSumDoc and\nthe MACSumDial datasets (Zhang et al., 2023b),\nwhich comprise committee meetings and news con-\ntents each. Both datasets are for mixed-attribute\nsummarization that control multiple attributes si-\nmultaneously, but only MACSumDial has speaker\nattribute. Since we evaluate LLM performance on\n1casperhansen/llama-3-70b-instruct-awq\n\nModel\nCoherence\nConsistency\nFluency\nRelevance\nQuestEval\nIter (Ext)\n0.820\n0.800\n0.859\n0.696\n0.523\nAIS (Ext)\n0.884\n0.843\n0.905\n0.785\n0.554\nIter (Len)\n0.836\n0.803\n0.836\n0.759\n0.484\nAIS (Len)\n0.934\n0.834\n0.942\n0.887\n0.548\nGTE (Ext)\n0.941\n0.873\n0.937\n0.880\n0.590\nGTE (Len)\n0.937\n0.840\n0.944\n0.901\n0.553\nTable 3: Quality of the controlled summaries.\nindividual attributes, we use attributes separately.\n5\nResults and Discussions\nMain results\nWe define the strategy solely pro-\nviding attribute identification steps as AIS and de-\nnote our full guiding framework as GTE. As a\nresult, our GTE demonstrates remarkably lower\nfailure rates and fewer iterations when adjusting\nsummaries across all attributes, including challeng-\ning numerical attributes in MACSumDoc (Table 1).\nNotably, while applying GTE to smaller models\nsuch as Phi-3 and Llama3-8B resulted in signifi-\ncant performance improvements, we observed that\nfailures were almost nonexistent when applied to\nlarger models such as Llama-70B or GPT-4o.\nLLMs encounter more difficulties with the\nMACSumDial dataset (Table 2).\nThe dataset,\nwhich is derived from QMSum (Zhong et al.,\n2021), consists of lengthy and diverse content\nparliamentary and committee meetings, making\nit more challenging compared to the CNN-news-\nbased MACSumDoc. Notably, both the GPT-4o-\nIter and GPT-4o-AIS failed to adjust for long\nlength, whereas our GPT-4o-GTE demonstrated\na commendable success rate. Regarding extractive-\nness, the Iter and AIS of GPT-4o exhibit relatively\nlow iteration counts since the models mostly ex-\nceed the maximum iteration (⟳). While they fail\nnearly 80%, our GTE demonstrates a significantly\nlower failure rate at 17.28% with low iterations.\nQuality of controlled summary\nInstead of\nROUGE score(Lin, 2004), which does not ad-\nequately evaluate the quality of the summaries\n(Zhong et al., 2022; Scialom et al., 2021; Ryu et al.,\n2024a), we use UniEval (Zhong et al., 2022) and\nQuestEval (Scialom et al., 2021) to assess both the\ninherent quality of the summaries and their fac-\ntual consistency with the source text. UniEval, a\nmulti-dimensional evaluator with high human cor-\nrelation, assesses dimensions such as coherence,\nconsistency, fluency, and relevance, and QuestEval\nevaluates factuality via question answering. Table 3\nshows that our method’s summaries outperform all\nSingle attribute\nMixed attribute\nFigure 2: Performance in mixed-attribute.\nUniEval dimensions and QuestEval scores, demon-\nstrating effective attribute control while maintain-\ning overall summary quality.\nMixed attributes controllability\nWe observed\nthat when generating summaries controlled for all\nattributes simultaneously, the model effectively\nhandled linguistic attributes but faced challenges\nwith numerical attributes. Notably, satisfying all\nattributes within the maximum number of iterations\nproved challenging for all methods, including GTE.\nSequential-planning\nDiscovering the challenges\nin precisely controlling all attributes in parallel,\nwe introduce a sequential-planning strategy, which\ngradually adjusts attributes from the ill-controlled\nwith the initial draft using GTE. However, modi-\nfying one attribute often disrupted previously ad-\njusted attributes due to correlations. For example,\ncontrolling length first would still lead to changes\nin length when adjusting extractiveness. Conse-\nquently, sequential adjustments result in a modest\nperformance gap compared to the initial draft (Fig-\nure 2). We assess the attributes using the root mean\nsquared error (RMSE) between the instructed val-\nues and those in the generated summaries.\n6\nConclusion\nIn this work, we revisit the measurement of control-\nlable summarization with various attributes. We\nevaluate the controllability of multiple attributes\nin summary generation with LLMs, finding that\nLLMs struggle to adjust numeric attributes com-\npared to linguistic ones. To address this limitation,\nwe propose a guide-to-explain (GTE) approach,\nwhere the model is guided to explain its misalign-\nments and then grounded this explanation to pro-\nduce better-controlled summaries in subsequent it-\nerations. GTE enables LLMs to control challenging\nnumerical attributes with lower failure and fewer\niterations. Further, we validate the quality of the\ncontrolled summaries via a multi-dimensional eval-\nuation, demonstrating the high-quality generation.\n\nLimitation\nWe evaluated the controllability of various at-\ntributes in LLMs and introduced a novel guide-\nto-explain (GTE) framework to address challenges\nin numerical attributes. While GTE enhanced con-\ntrol over numerical attributes, it still struggled with\nhighly correlated mixed numerical attributes. Ad-\nditionally, sequential planning, which adjusts at-\ntributes in order of least alignment, also faced\ndifficulties achieving precise control. Even after\nproperly adjusting one attribute, modifying the cor-\nrelated numerical attribute caused the previously\nadjusted attribute to change. We believe further\nresearch could explore more effective methods for\naddressing these challenges.\nEthics\nWe used publicly available MACSum datasets for\nour research, conducting experiments with Phi-3,\nLlama3 2, GPT-3.5, and GPT-4o from April to Oc-\ntober 2024.\nAcknowledgments\nThis work was supported by the National Research\nFoundation of Korea (NRF) grant funded by the Ko-\nrea government (MSIT) (No. RS-2023-00217286)\nand Institute of Information & communications\nTechnology Planning & Evaluation (IITP) grant\nfunded by the Korea government (MSIT) (No.RS-\n2019-II191906, Artificial Intelligence Graduate\nSchool Program (POSTECH)).\nReferences\nMarah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,\nJyoti Aneja, Ahmed Awadallah, Hany Awadalla,\nNguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-\nrat Behl, et al. 2024. Phi-3 technical report: A highly\ncapable language model locally on your phone. arXiv\npreprint arXiv:2404.14219.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nAlan Akbik, Bergmann, et al. 2019. FLAIR: An easy-to-\nuse framework for state-of-the-art NLP. In Annual\nConference of the North American Chapter of the\nAssociation for Computational Linguistics (Demon-\nstrations).\n2Meta Llama3 Community License, Copyright © Meta\nPlatforms, Inc. All Rights Reserved. More details can be\nfound at: Llama3 License\nTom B Brown. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. 2024. Chain-of-verification reduces\nhallucination in large language models. In Findings\nof the Association for Computational Linguistics ACL\n2024, pages 3563–3578, Bangkok, Thailand and vir-\ntual meeting. Association for Computational Linguis-\ntics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nZhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen,\nYujiu Yang, Nan Duan, and Weizhu Chen. 2024.\nCRITIC: Large language models can self-correct\nwith tool-interactive critiquing. In The Twelfth Inter-\nnational Conference on Learning Representations.\nTanya Goyal et al. 2023.\nNews summarization\nand evaluation in the era of gpt-3.\nPreprint,\narXiv:2209.12356.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–\n42, Toronto, Canada. Association for Computational\nLinguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYixin Liu, Alexander Fabbri, Jiawen Chen, Yilun Zhao,\nSimeng Han, Shafiq Joty, Pengfei Liu, Dragomir\nRadev, Chien-Sheng Wu, and Arman Cohan. 2024.\nBenchmarking generation and evaluation capabili-\nties of large language models for instruction control-\nlable summarization. In Findings of the Association\nfor Computational Linguistics: NAACL 2024, pages\n4481–4501, Mexico City, Mexico. Association for\nComputational Linguistics.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nShashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdan-\nbakhsh, and Peter Clark. 2023. Self-refine: Itera-\ntive refinement with self-feedback. In Advances in\nNeural Information Processing Systems, volume 36,\npages 46534–46594. Curran Associates, Inc.\nZiming Mao, Chen Henry Wu, Ansong Ni, Yusen Zhang,\nRui Zhang, Tao Yu, Budhaditya Deb, Chenguang\nZhu, Ahmed Awadallah, and Dragomir Radev. 2022.\nDYLE: Dynamic latent extraction for abstractive\nlong-input summarization. In Proceedings of the\n\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1687–1698, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nArtidoro Pagnoni, Alex Fabbri, Wojciech Kryscinski,\nand Chien-Sheng Wu. 2023. Socratic pretraining:\nQuestion-driven pretraining for controllable summa-\nrization. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 12737–12755, Toronto,\nCanada. Association for Computational Linguistics.\nXiao Pu, Mingqi Gao, and Xiaojun Wan. 2023.\nSummarization is (almost) dead.\narXiv preprint\narXiv:2309.09558.\nSangwon Ryu, Heejin Do, Yunsu Kim, Gary Lee, and\nJungseul Ok. 2024a. Multi-dimensional optimization\nfor text summarization via reinforcement learning.\nIn Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 5858–5871, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nSangwon Ryu, Heejin Do, Yunsu Kim, Gary Geunbae\nLee, and Jungseul Ok. 2024b. Key-element-informed\nsllm tuning for document summarization. In Inter-\nspeech 2024, pages 1940–1944.\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\nBenjamin Piwowarski, Jacopo Staiano, Alex Wang,\nand Patrick Gallinari. 2021. QuestEval: Summariza-\ntion asks for fact-based evaluation. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 6594–6604, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nShichao Sun, Ruifeng Yuan, Ziqiang Cao, Wenjie Li,\nand Pengfei Liu. 2024. Prompt chaining or stepwise\nprompt? refinement in text summarization. In Find-\nings of the Association for Computational Linguistics\nACL 2024, pages 7551–7558, Bangkok, Thailand\nand virtual meeting. Association for Computational\nLinguistics.\nYuting Tang, Ratish Puduppully, Zhengyuan Liu, and\nNancy Chen. 2023. In-context learning of large lan-\nguage models for controlled dialogue summarization:\nA holistic benchmark and empirical analysis. In Pro-\nceedings of the 4th New Frontiers in Summarization\nWorkshop, pages 56–67, Singapore. Association for\nComputational Linguistics.\nAvijit Thawani, Jay Pujara, Filip Ilievski, and Pedro\nSzekely. 2021. Representing numbers in NLP: a\nsurvey and a vision. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 644–656, Online. As-\nsociation for Computational Linguistics.\nAshok Urlana, Pruthwik Mishra, Tathagato Roy, and\nRahul Mishra. 2024.\nControllable text summa-\nrization: Unraveling challenges, approaches, and\nprospects - a survey. In Findings of the Associa-\ntion for Computational Linguistics ACL 2024, pages\n1603–1623, Bangkok, Thailand and virtual meeting.\nAssociation for Computational Linguistics.\nJesse Vig, Alexander Fabbri, Wojciech Kryscinski,\nChien-Sheng Wu, and Wenhao Liu. 2022. Exploring\nneural models for query-focused summarization. In\nFindings of the Association for Computational Lin-\nguistics: NAACL 2022, pages 1455–1468, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nBin Wang, Zhengyuan Liu, and Nancy Chen. 2023. In-\nstructive dialogue summarization with query aggre-\ngations. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 7630–7653, Singapore. Association for Com-\nputational Linguistics.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He,\nShengping Liu, Bin Sun, Kang Liu, and Jun Zhao.\n2023. Large language models are better reasoners\nwith self-verification. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2023,\npages 2550–2575, Singapore. Association for Com-\nputational Linguistics.\nRuochen Xu, Song Wang, Yang Liu, Shuohang Wang,\nYichong Xu, Dan Iter, Pengcheng He, Chenguang\nZhu, and Michael Zeng. 2023. LMGQS: A large-\nscale dataset for query-focused summarization. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2023, pages 14764–14776, Singa-\npore. Association for Computational Linguistics.\nWeizhe Yuan, Ilia Kulikov, Ping Yu, Kyunghyun Cho,\nSainbayar Sukhbaatar, Jason Weston, and Jing Xu.\n2024. Following length constraints in instructions.\nPreprint, arXiv:2406.17744.\nHaopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023a.\nSummIt: Iterative text summarization via ChatGPT.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2023, pages 10644–10657, Sin-\ngapore. Association for Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In Proceedings of\nthe International Conference on Learning Represen-\ntations.\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,\nKathleen McKeown, and Tatsunori B. Hashimoto.\n2024. Benchmarking large language models for news\nsummarization. Transactions of the Association for\nComputational Linguistics, 12:39–57.\nYusen Zhang, Yang Liu, Ziyi Yang, Yuwei Fang, Yulong\nChen, Dragomir Radev, Chenguang Zhu, Michael\nZeng, and Rui Zhang. 2023b. MACSum: Control-\nlable summarization with mixed attributes. Transac-\ntions of the Association for Computational Linguis-\ntics, 11:787–803.\n\nYusen Zhang, Ansong Ni, Ziming Mao, Chen Henry Wu,\nChenguang Zhu, Budhaditya Deb, Ahmed Awadallah,\nDragomir Radev, and Rui Zhang. 2022. Summn: A\nmulti-stage summarization framework for long input\ndialogues and documents. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1592–\n1604, Dublin, Ireland. Association for Computational\nLinguistics.\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\nJiawei Han. 2022.\nTowards a unified multi-\ndimensional evaluator for text generation. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2023–\n2038, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia\nMutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli\nCelikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir\nRadev. 2021. QMSum: A new benchmark for query-\nbased multi-domain meeting summarization. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5905–5921, Online. Association for Computational\nLinguistics.\nA\nAttribute details\nA.1\nAttribute analysis\nWe\nanalysis\nthe\ndata\ndistributions\nin\nthe\nMACSumDoc and MACSumDial (Zhang et al.,\n2023b) datasets. The attributes used in the orig-\ninal datasets are described as follows:\n• Extractiveness: Controls how much of the\nsummary is directly extracted from the source\ntext.\nIt is evaluated using the average of\nROUGE-2 and ROUGE-3 precision scores.\n• Length: The number of words in the summary.\nIt is evaluated based on token length.\n• Topic: Control the summary to align with the\ngiven topic, and multiple topics can be present.\nThe evaluation is based on the proportion of\ntopic words appearing in the summary.\n• Speaker: Control the summary to focus on the\nspeech of a specific speaker. It is evaluated\nby the ratio of the speaker’s words included\nin the summary.\n• Specificity: Controls the level of detail or\ndescriptive content in the summary.\nThe\nevaluation formula is: Specificity = (0.1 ×\nverbs+0.2×tokens+0.3×nouns+0.4×\nnumerical tokens)/number of sentences.\nThe\ndetailed\ndata\ndistribution\nfor\nboth\nMACSumDoc and MACSumDial are presented in\nTable 4. We observe that the labels in previous\nwork do not account for attributes. In the case of\ntopic, the specific topic keywords may not always\nappear explicitly, even if the summary is focused\non the given topic. Similarly, for speaker, LLMs\nmay paraphrase the speaker’s words using different\nterms or expressions. Therefore, we reinterpreted\nthese labels in Section 2.1 to account for these\nfactors.\nFor the extractiveness and length attributes, the\noriginal labels were categorized as ‘normal’, ‘high’,\n‘fully’ for extractiveness and ‘short’, ‘normal’,\n‘long’ for length. However, since the labels were\nnot numerically defined, we deemed the criteria\nunclear. Therefore, we redefined these labels for\nour use.\nWe calculate extractiveness based on the pro-\nportion of words in the source text that appeared\nexactly in the summary. For length, while the orig-\ninal dataset was labelled based on word count and\ncompression rate, we only considered the ratio of\nthe summary to the original text. Table 4 shows\nthat the ratios were not strictly distinct, with some\noverlap between labels. Thus, we relabeled bound-\naries in Section 2.1. Since these label values are not\nexact for the reference summary, we adjusted at-\ntributes according to the specified new labels. Con-\nsequently, we did not use the reference summaries\nfrom the original dataset.\nFor specificity, we used the Flair 3 (Akbik et al.,\n2019) model for named entity recognition (NER)\nmodel to measure the number of entities in each\nsummary. Since the number of entities varies with\nsummary length, we calculated specificity as the\nratio of total entities to total words in the summary.\nHowever, the annotation for specificity in the origi-\nnal dataset was inversely related to our calculations,\nso we did not use this attribute in our study.\nA.2\nIteration threshlold\nWe set attribute-specific thresholds and iteratively\nadjusted them until the criteria were satisfied. For\nextractiveness and length, iterations were consid-\nered successful if the values fell within a relabeled\nvalue ±5 range. For the topic and speaker, we\nset the minimum embedding similarity score as\n3https://github.com/flairNLP/flair\n\nAttribute\nLabel\nMACSumDoc\nMACSumDial\nDist.\nRelabel\n# of summaries\nDist.\nRelabel\n# of summaries\nLength\nshort\n0.7 - 15.0%\n7.5%\n1059\n0.2 - 20.8%\n7.5%\n300\nnormal\n0.5 - 48.6%\n20.0%\n2194\n0.3 - 41.9%\n20.0%\n1693\nlong\n1.5 - 39.8%\n32.5%\n1025\n0.7 - 32.4%\n32.5%\n345\nExtractiveness\nnormal\n35.7 - 100.0%\n85.0%\n3731\n53.2 - 100.0%\n85.0%\n1661\nhigh\n55.0 - 100.0%\n90.0%\n287\n63.0 - 100.0%\n90.0%\n340\nfully\n84.6 - 100.0%\n100.0%\n260\n75.9 - 100.0%\n100.0%\n337\nTopic\n-\n74.8 - 88.8\n74\n2013\n73.6 - 87.0\n74\n2317\nSpeaker\n-\n-\n-\n-\n75.6 - 92.0\n75\n1796\nSpecificity\nnormal\n0.105%\n-\n3713\n0.115%\n-\n1702\nhigh\n0.062%\n-\n565\n0.110%\n-\n636\nTable 4: Data distribution of MACSumDoc and MACSumDial\nthe criterion for achieving a successful topic- or\nspeaker-focused summary.\nB\nGuide-to-explain (GTE) prompts\n\n(CNN)Easter is unique on the Christian calendar, a major point in the cycle of the religious year, and one that has always been able to resist the \ncommercialization and culture warring that surrounds Christmas. That's in part because Easter is genuinely about how religious impulses, and patterns, can \noperate in ways that affect our lives. Nevertheless, I'm often surprised by how little people, even those supposedly within the Christian tradition, actually know\nabout what is called Holy Week and its culmination on Easter Sunday. At a time when our culture is roiled by questions of identity and ethics (and tolerance) that \nhave profound religious implications, it's worth pausing to explore this crucial holiday -- and the awareness of the human condition, in all its sadness and glory, \nthat it engenders. After all, Holy Week calls mostly to those who incline their minds and hearts in its direction with seriousness of intent. Still, the fuss must \npuzzle those looking on, wondering what it all means. Why do Christians make so much of this springtime week, and make so much of Easter weekend? There is \na phrase that many never come across, even among Christians:  Easter Triduum .This refers to the three days of Easter that begin with Good Friday, proceed \nthrough Holy Saturday, and conclude with Easter Sunday. It's definitely a progression, although the word itself -- triduum -- can refer to any three days of prayer...\nArticle\nsummarize the above article in 32.5% length of the article:\nEaster is a significant Christian holiday that often gets overlooked, especially when compared to Christmas. Nearly everyone within the Christian tradition knows \nthe basics of Easter, but not many understand the full significance and rituals of Holy Week and the Easter Triduum, which are three days of prayer that begin on \nGood Friday and end on Easter Sunday. Holy Communion originated from the Last Supper, a Passover meal that Jesus shared with his disciples before being \ncrucified. On Maundy Thursday, Jesus washed the disciples' feet to demonstrate humility...\nThe calculation is wrong because the article length is 931 words, and the summary length is 157 words. To calculate the percentage, I should have divided 157 by \n931, which gives me 16.9%. This means the summary is only 16.9% of the original article, not 32.5%.\nTo revise the summary to reach 32.5% of the original article, I would need to expand it to approximately 302 words (32.5% of 931). Here is a revised summary:\nEaster is a significant Christian holiday that often gets overlooked, especially when compared to Christmas. Despite its importance, many people within the \nChristian tradition know little about the full significance and rituals of Holy Week and the Easter Triduum. This three-day period of prayer begins on Good Friday, \ncommemorating Jesus' crucifixion, and concludes on Easter Sunday, celebrating his resurrection...\nThe length (word count) of the summary is 157 and the length of the article is 931, so it is approximately 16.9%. Explain why your calculation is wrong, and revise \nthe summary in the 32.5% length of the article:\nSummary (16.9%)\nRevised Summary (29.4%)\nGTE\nInitial prompt\nFigure 3: Length guide-to-explain (GTE).\n(CNN)Easter is unique on the Christian calendar, a major point in the cycle of the religious year, and one that has always been able to resist the \ncommercialization and culture warring that surrounds Christmas. That's in part because Easter is genuinely about how religious impulses, and patterns, can \noperate in ways that affect our lives. Nevertheless, I'm often surprised by how little people, even those supposedly within the Christian tradition, actually know\nabout what is called Holy Week and its culmination on Easter Sunday. At a time when our culture is roiled by questions of identity and ethics (and tolerance) that \nhave profound religious implications, it's worth pausing to explore this crucial holiday -- and the awareness of the human condition, in all its sadness and glory, \nthat it engenders. After all, Holy Week calls mostly to those who incline their minds and hearts in its direction with seriousness of intent. Still, the fuss must \npuzzle those looking on, wondering what it all means. Why do Christians make so much of this springtime week, and make so much of Easter weekend? There is \na phrase that many never come across, even among Christians:  Easter Triduum .This refers to the three days of Easter that begin with Good Friday, proceed \nthrough Holy Saturday, and conclude with Easter Sunday. It's definitely a progression, although the word itself -- triduum -- can refer to any three days of prayer...\nArticle\nsummarize the above article in 32.5% length of the article:\nEaster is a significant Christian holiday that often gets overlooked, especially when compared to Christmas. Nearly everyone within the Christian tradition knows \nthe basics of Easter, but not many understand the full significance and rituals of Holy Week and the Easter Triduum, which are three days of prayer that begin on \nGood Friday and end on Easter Sunday. Holy Communion originated from the Last Supper, a Passover meal that Jesus shared with his disciples before being \ncrucified. On Maundy Thursday, Jesus washed the disciples' feet to demonstrate humility...\nEaster, a significant Christian holiday, remains untouched by commercialization and culture wars. Many people, including Christians, are unfamiliar with Holy \nWeek and its culmination on Easter Sunday. The Easter Triduum, a three-day period, begins with Maundy Thursday, where Jesus shared the Last Supper with his \ndisciples, demonstrating his humility by washing their feet. Good Friday, the day of the crucifixion, is a somber reminder of death, sacrifice, and fear. The \nGospels provide varying accounts of Jesus' final words, including his quote from the 22nd Psalm. Holy Saturday, often overlooked, serves as a passage between \nthe darkness of Good Friday and the hope of Easter Sunday. It's a day of anxiety and uncertainty, when the pain of the crucifixion still lingers, but hope begins to \nemerge. Easter Sunday, with its promise of transformation and resurrection, is a celebration of the human experience. The resurrection is not just a physical \nrevival but a total transformation, symbolizing the possibility of change and renewal in our own lives...\nThe length (word count) of the summary is 157 and the length of the article is 931, so it is approximately 16.9%. Summarize the above article in the 32.5% length \nof the article:\nSummary (16.9%)\nRevised Summary (23.4%)\nAIS\nInitial prompt\nFigure 4: Length problem-solving step (AIS).\n\n(CNN)Easter is unique on the Christian calendar, a major point in the cycle of the religious year, and one that has always been able to resist the \ncommercialization and culture warring that surrounds Christmas. That's in part because Easter is genuinely about how religious impulses, and patterns, can \noperate in ways that affect our lives. Nevertheless, I'm often surprised by how little people, even those supposedly within the Christian tradition, actually know\nabout what is called Holy Week and its culmination on Easter Sunday. At a time when our culture is roiled by questions of identity and ethics (and tolerance) that \nhave profound religious implications, it's worth pausing to explore this crucial holiday -- and the awareness of the human condition, in all its sadness and glory, \nthat it engenders. After all, Holy Week calls mostly to those who incline their minds and hearts in its direction with seriousness of intent. Still, the fuss must \npuzzle those looking on, wondering what it all means. Why do Christians make so much of this springtime week, and make so much of Easter weekend? There is \na phrase that many never come across, even among Christians:  Easter Triduum .This refers to the three days of Easter that begin with Good Friday, proceed \nthrough Holy Saturday, and conclude with Easter Sunday. It's definitely a progression, although the word itself -- triduum -- can refer to any three days of prayer...\nArticle\nsummarize the above article in 32.5% length of the article:\nEaster is a significant Christian holiday that often gets overlooked, especially when compared to Christmas. Nearly everyone within the Christian tradition knows \nthe basics of Easter, but not many understand the full significance and rituals of Holy Week and the Easter Triduum, which are three days of prayer that begin on \nGood Friday and end on Easter Sunday. Holy Communion originated from the Last Supper, a Passover meal that Jesus shared with his disciples before being \ncrucified. On Maundy Thursday, Jesus washed the disciples' feet to demonstrate humility...\nEaster is a unique Christian event that resisted commercialization. Many Christians lack understanding of Holy Week and Easter. The Easter Triduum - Good \nFriday, Holy Saturday, and Easter Sunday - marks death, transition, and resurrection. This pattern reflects our daily struggles, suffering, and rebirth, making \nEaster a celebration of moving from darkness to light.\nsummarize the above article in 32.5% length of the article:\nSummary (16.9%)\nRevised Summary (9.1%)\nIter\nInitial prompt\nFigure 5: Length iteration (Iter).\n(CNN)We did it again, in another American city. We set Baltimore on fire this time. We brutalized black bodies. We turned a funeral into a riot. We let things get \nout of hand. We looted. We threw stones at policemen. We threw stones at citizens. We created camera-ready chaos, and we replayed the images. We created a \nculture of such deep distrust and disrespect that violence seemed the inevitable response. We let the violence flow. We let the violence stand for everything \nthat's wrong with the things we already didn't like. By now you may be asking, \"Who's we? \"You may be saying with some irritation, \"Don't lump me in with them. I \ndidn't have anything to do with it.” To which the only real answer can be: Stop kidding yourself. The word \"we\" is one of the great American words. We the People. \nYes we can. We are family. I use \"we\" a lot when I talk about our country's achievements. I like to say we won the Second World War, we put a man on the moon, \nwe invented the Internet, we gave the world jazz. Well, if I -- a son of immigrants whose family had nothing to do with any of those accomplishments -- if I get to \nclaim those aspects of American history, then surely I have to claim the unsavory aspects too. “We\" cuts both ways. We enslaved Africans. We cut \nReconstruction short and made a mockery of equal citizenship. We supported Jim Crow, then redlined, subordinated, and ghettoized African-Americans...\nArticle\nsummarize the above article  using 85% of words directly from the original document:\nWe did it again, in another American city. We set Baltimore on fire this time. We brutalized black bodies. We turned a funeral into a riot. We let things get out of \nhand. We looted. We threw stones at policemen. We threw stones at citizens. We created camera-ready chaos, and we replayed the images. By now you may be \nasking, \"Who's we?\" You may be saying with some irritation, \"Don't lump me in with them. I didn't have anything to do with it.\" To which the only real answer can \nbe: Stop kidding yourself. The word \"we\" is one of the great American words. We the People...\nI apologize for the mistake. Upon re-calculating, I realized that the original summary contained 79.4% of words directly from the original document. To revise the \nsummary to meet the 85% threshold, I’ll incorporate more words directly from the original document. Here's the revised summary:\n\"We did it again, in another American city\", perpetuating a cycle of violence and chaos. \"We set Baltimore on fire this time\", and in doing so, \"we brutalized black \nbodies\". The author emphasizes the significance of \"we\", underscoring collective responsibility for both accomplishments and injustices. \"We the People\" \nimplies a shared history, encompassing achievements like \"we won the Second World War\", \"we put a man on the moon\", \"we invented the Internet\", and \"we \ngave the world jazz\", as well as darker aspects like \"we enslaved Africans\", \"we cut Reconstruction short and made a mockery of equal citizenship\", \"we \nsupported Jim Crow, then redlined, subordinated, and ghettoized African-Americans\", and \"we failed\"...\nThis summary contains 100.0% of words directly from the original document. Explain why your calculation is wrong, and revise the summary so that 85% of the \nwords are used directly as in the original document:\nSummary (100.0%)\nRevised Summary (86.8%)\nGTE\nInitial prompt\nFigure 6: Extractiveness guide-to-explain (GTE).\n\n(CNN)We did it again, in another American city. We set Baltimore on fire this time. We brutalized black bodies. We turned a funeral into a riot. We let things get \nout of hand. We looted. We threw stones at policemen. We threw stones at citizens. We created camera-ready chaos, and we replayed the images. We created a \nculture of such deep distrust and disrespect that violence seemed the inevitable response. We let the violence flow. We let the violence stand for everything \nthat's wrong with the things we already didn't like. By now you may be asking, \"Who's we? \"You may be saying with some irritation, \"Don't lump me in with them. I \ndidn't have anything to do with it.” To which the only real answer can be: Stop kidding yourself. The word \"we\" is one of the great American words. We the People. \nYes we can. We are family. I use \"we\" a lot when I talk about our country's achievements. I like to say we won the Second World War, we put a man on the moon, \nwe invented the Internet, we gave the world jazz. Well, if I -- a son of immigrants whose family had nothing to do with any of those accomplishments -- if I get to \nclaim those aspects of American history, then surely I have to claim the unsavory aspects too. “We\" cuts both ways. We enslaved Africans. We cut \nReconstruction short and made a mockery of equal citizenship. We supported Jim Crow, then redlined, subordinated, and ghettoized African-Americans...\nArticle\nsummarize the above article  using 85% of words directly from the original document:\nWe did it again, in another American city. We set Baltimore on fire this time. We brutalized black bodies. We turned a funeral into a riot. We let things get out of \nhand. We looted. We threw stones at policemen. We threw stones at citizens. We created camera-ready chaos, and we replayed the images. By now you may be \nasking, \"Who's we?\" You may be saying with some irritation, \"Don't lump me in with them. I didn't have anything to do with it.\" To which the only real answer can \nbe: Stop kidding yourself. The word \"we\" is one of the great American words. We the People...\nHere's a rewritten summary that meets the 85% requirement:\nWe allowed Baltimore to burn, brutalizing black bodies and turning a funeral into a riot. We created chaos, replaying the images and fostering a culture of \ndistrust and disrespect. When asked \"Who's we?\", some may object, saying \"Don't lump me in with them.\" But the truth is, we as a nation are responsible. The \nword \"we\" is synonymous with American identity. If we claim America's achievements, we must also claim its dark past, including slavery and racial inequity. We \nare the authors of Baltimore's story, and it's our responsibility to address the problem. We can't just point to individual faults or structural flaws; we must \nacknowledge both. It's time to put aside our differences, acknowledge the truth, and work together to create opportunity and change.\nThis summary contains 100.0% of words directly from the original document. Summarize the above article using 85% of the words directly from the original \ndocument:\nSummary (100.0%)\nRevised Summary (78.0%)\nAIS\nInitial prompt\nFigure 7: Extractiveness problem-solving step (AIS).\n(CNN)We did it again, in another American city. We set Baltimore on fire this time. We brutalized black bodies. We turned a funeral into a riot. We let things get \nout of hand. We looted. We threw stones at policemen. We threw stones at citizens. We created camera-ready chaos, and we replayed the images. We created a \nculture of such deep distrust and disrespect that violence seemed the inevitable response. We let the violence flow. We let the violence stand for everything \nthat's wrong with the things we already didn't like. By now you may be asking, \"Who's we? \"You may be saying with some irritation, \"Don't lump me in with them. I \ndidn't have anything to do with it.” To which the only real answer can be: Stop kidding yourself. The word \"we\" is one of the great American words. We the People. \nYes we can. We are family. I use \"we\" a lot when I talk about our country's achievements. I like to say we won the Second World War, we put a man on the moon, \nwe invented the Internet, we gave the world jazz. Well, if I -- a son of immigrants whose family had nothing to do with any of those accomplishments -- if I get to \nclaim those aspects of American history, then surely I have to claim the unsavory aspects too. “We\" cuts both ways. We enslaved Africans. We cut \nReconstruction short and made a mockery of equal citizenship. We supported Jim Crow, then redlined, subordinated, and ghettoized African-Americans...\nArticle\nsummarize the above article  using 85% of words directly from the original document:\nWe did it again, in another American city. We set Baltimore on fire this time. We brutalized black bodies. We turned a funeral into a riot. We let things get out of \nhand. We looted. We threw stones at policemen. We threw stones at citizens. We created camera-ready chaos, and we replayed the images. By now you may be \nasking, \"Who's we?\" You may be saying with some irritation, \"Don't lump me in with them. I didn't have anything to do with it.\" To which the only real answer can \nbe: Stop kidding yourself. The word \"we\" is one of the great American words. We the People...\nWe set Baltimore on fire this time. We brutalized black bodies. We turned a funeral into a riot. We let things get out of hand. We looted. We threw stones at \npolicemen. We threw stones at citizens. The word \"we\" is one of the great American words. We the People. I use \"we\" a lot when I talk about our country's \nachievements. Well, if I get to claim those aspects of American history, then surely I have to claim the unsavory aspects too. \"We\" cuts both ways. We enslaved \nAfricans. We cut Reconstruction short and made a mockery of equal citizenship. We supported Jim Crow, then redlined, subordinated, and ghettoized African-\nAmericans. We cut blacks out of the New Deal. We created a polity in which racial inequity and economic inequality magnify each other unrelentingly. We tried \nto put a lid on it with heavy policing and a War on Drugs. We failed...\nSummarize the above article using 85% of the words directly from the original document:\nSummary (100.0%)\nRevised Summary (100.0%)\nIter\nInitial prompt\nFigure 8: Extractiveness iteration (Iter).",
    "pdf_filename": "Guide-to-Explain_for_Controllable_Summarization.pdf"
}