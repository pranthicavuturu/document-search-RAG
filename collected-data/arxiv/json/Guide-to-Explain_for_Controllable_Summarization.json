{
    "title": "Guide-to-Explain for Controllable Summarization",
    "abstract": "searchemployedencoder-decodermodelstocon- Recently,largelanguagemodels(LLMs)have trolattributes(Maoetal.,2022;Zhangetal.,2022; demonstrated remarkable performance in ab- Vigetal.,2022;Pagnonietal.,2023;Wangetal., stractivesummarizationtasks. However,con- trollable summarization with LLMs remains 2023; Urlana et al., 2024). Although LLMs ex- underexplored,limitingtheirabilitytogenerate celingeneratinghigh-qualitysummaries,theystill summaries that align with specific user pref- facechallengesincontrollingattributes(Yuanetal., erences. Inthispaper,wefirstinvestigatethe 2024; Tang et al., 2023), and their controllability capabilityofLLMstocontroldiverseattributes, hasbeenunderexplored(Liuetal.,2024). revealingthattheyencountergreaterchallenges Thus,weanalyzeLLMs’abilitytocontrolvar- withnumericalattributes,suchaslengthandex- ious attributes in summarization and refine the tractiveness,comparedtolinguisticattributes. measurementstomoreaccuratelyassesstheseat- Toaddressthischallenge,weproposeaguide- to-explain framework (GTE) for controllable tributes. We reveal that while LLMs excel at summarization. OurGTEframeworkenables controlling linguistic attributes such as topic and themodeltoidentifymisalignedattributesin speaker,theyseverelystrugglewithnumericalat- theinitialdraftandguidesitinexplaininger- tributes such as extractivenessand length. To ad- rorsinthepreviousoutput. Basedonthisre- dressthischallenge,weproposeaguide-to-explain flection, the model generates a well-adjusted (GTE), which enables precise attribute control summary. Asaresult,byallowingthemodelto solelythroughLLMswithoutrelyingonexternal reflectonitsmisalignment,wegeneratesum- mariesthatsatisfythedesiredattributesinsur- modules or training. We first design an attribute- prisingly fewer iterations than other iterative identificationsteptocalculatemisalignedattributes methodssolelyusingLLMs. inLLM-generatedsummaries,subsequentlyguid- ing the model to explain the sources of its errors. 1 Introduction Byself-reflectingitsownerrors,themodelcanad- Largelanguagemodels(LLMs)havedemonstrated equatelyadjustattributesinsubsequentiterations. tion, outperforming traditional encoder-decoder inreasoningtaskswithLLMs(Wengetal.,2023; models by generating more contextually appro- Madaanetal.,2023;Dhuliawalaetal.,2024;Gou priateandnaturalsummaries(Goyaletal.,2023; etal.,2024),tocontrollablesummarization. Zhang et al., 2024; Pu et al., 2023; Ryu et al., Additionally, we evaluate GTE on mixed 2024b). In addition, recent studies aimed to gen- attribute control datasets, MACSum and Doc eratehigher-qualitysummariesbyleveragingthe MACSum (Zhangetal.,2023b). GTEsuccess- Dial self-correctioncapabilitiesofLLMs(Zhangetal., fullycontrolseachattributewithminimaliterations 2023a;Sunetal.,2024). However,givenindivid- solelyusingLLMs,outperformingotheriteration uals’diversepreferencesforsummarystyles,itis methods. Wealsodemonstratethehighqualityof essentialtogeneratesummariesthatadjustpersonal thecontrolledsummariesviamultipleevaluation needs (Zhang et al., 2023b). For instance, some metrics. Inaddition,weanalyzedwhetherLLMs usersmaypreferconcisesummariesorretainexact cancontrolmultipleattributessimultaneously. We phrasesfromtheoriginaltext. foundoutthatLLMsstrugglewithjointlycontrol- Therefore, controllable summarization has re- lingcorrelatednumericalattributes. Ourcontribu- centlygarneredattention(Zhongetal.,2021;Xu tionsareasfollows: 4202 voN 91 ]LC.sc[ 1v06421.1142:viXra",
    "body": "Guide-to-Explain for Controllable Summarization\nSangwonRyu1,HeejinDo1,DaeheeKim1,\nYunsuKim3,GaryGeunbaeLee12,JungseulOk12\n1GraduateSchoolofArtificialIntelligence,POSTECH,SouthKorea\n2DepartmentofComputerScienceandEngineering,POSTECH,SouthKorea\n3aiXplainInc.,LosGatos,CA,USA\n{ryusangwon, heejindo, andrea0119, gblee, jungseul}@postech.ac.kr,\nyunsu.kim@aixplain.com\nAbstract et al., 2023; Zhang et al., 2023b). Previous re-\nsearchemployedencoder-decodermodelstocon-\nRecently,largelanguagemodels(LLMs)have\ntrolattributes(Maoetal.,2022;Zhangetal.,2022;\ndemonstrated remarkable performance in ab-\nVigetal.,2022;Pagnonietal.,2023;Wangetal.,\nstractivesummarizationtasks. However,con-\ntrollable summarization with LLMs remains 2023; Urlana et al., 2024). Although LLMs ex-\nunderexplored,limitingtheirabilitytogenerate celingeneratinghigh-qualitysummaries,theystill\nsummaries that align with specific user pref- facechallengesincontrollingattributes(Yuanetal.,\nerences. Inthispaper,wefirstinvestigatethe 2024; Tang et al., 2023), and their controllability\ncapabilityofLLMstocontroldiverseattributes,\nhasbeenunderexplored(Liuetal.,2024).\nrevealingthattheyencountergreaterchallenges\nThus,weanalyzeLLMs’abilitytocontrolvar-\nwithnumericalattributes,suchaslengthandex-\nious attributes in summarization and refine the\ntractiveness,comparedtolinguisticattributes.\nmeasurementstomoreaccuratelyassesstheseat- Toaddressthischallenge,weproposeaguide-\nto-explain framework (GTE) for controllable tributes. We reveal that while LLMs excel at\nsummarization. OurGTEframeworkenables controlling linguistic attributes such as topic and\nthemodeltoidentifymisalignedattributesin speaker,theyseverelystrugglewithnumericalat-\ntheinitialdraftandguidesitinexplaininger-\ntributes such as extractivenessand length. To ad-\nrorsinthepreviousoutput. Basedonthisre-\ndressthischallenge,weproposeaguide-to-explain\nflection, the model generates a well-adjusted\n(GTE), which enables precise attribute control\nsummary. Asaresult,byallowingthemodelto\nsolelythroughLLMswithoutrelyingonexternal\nreflectonitsmisalignment,wegeneratesum-\nmariesthatsatisfythedesiredattributesinsur- modules or training. We first design an attribute-\nprisingly fewer iterations than other iterative identificationsteptocalculatemisalignedattributes\nmethodssolelyusingLLMs. inLLM-generatedsummaries,subsequentlyguid-\ning the model to explain the sources of its errors.\n1 Introduction\nByself-reflectingitsownerrors,themodelcanad-\nLargelanguagemodels(LLMs)havedemonstrated equatelyadjustattributesinsubsequentiterations.\nsuperior performance in abstractive summariza- Weintroducetheself-refinestrategy,primarilyused\ntion, outperforming traditional encoder-decoder inreasoningtaskswithLLMs(Wengetal.,2023;\nmodels by generating more contextually appro- Madaanetal.,2023;Dhuliawalaetal.,2024;Gou\npriateandnaturalsummaries(Goyaletal.,2023; etal.,2024),tocontrollablesummarization.\nZhang et al., 2024; Pu et al., 2023; Ryu et al., Additionally, we evaluate GTE on mixed\n2024b). In addition, recent studies aimed to gen- attribute control datasets, MACSum and\nDoc\neratehigher-qualitysummariesbyleveragingthe MACSum (Zhangetal.,2023b). GTEsuccess-\nDial\nself-correctioncapabilitiesofLLMs(Zhangetal., fullycontrolseachattributewithminimaliterations\n2023a;Sunetal.,2024). However,givenindivid- solelyusingLLMs,outperformingotheriteration\nuals’diversepreferencesforsummarystyles,itis methods. Wealsodemonstratethehighqualityof\nessentialtogeneratesummariesthatadjustpersonal thecontrolledsummariesviamultipleevaluation\nneeds (Zhang et al., 2023b). For instance, some metrics. Inaddition,weanalyzedwhetherLLMs\nusersmaypreferconcisesummariesorretainexact cancontrolmultipleattributessimultaneously. We\nphrasesfromtheoriginaltext. foundoutthatLLMsstrugglewithjointlycontrol-\nTherefore, controllable summarization has re- lingcorrelatednumericalattributes. Ourcontribu-\ncentlygarneredattention(Zhongetal.,2021;Xu tionsareasfollows:\n4202\nvoN\n91\n]LC.sc[\n1v06421.1142:viXra\n• We revisit the measurement of various at- New Delhi, India / CNN)Police have arrested four employees\nof a popular Indian ethnic-wear chain after a minister\ntributesandanalyzeLLMs’controllability. spotted a security camera overlooking the changing room of\none of its stores.Federal education minister Smriti Irani was\nvisiting a FabIndia outlet ..\n• Weproposeaguide-to-explain(GTE),which Summarize the above article in 32.5% length of the article:\nFour FabIndia employees were arrested after a minister\nguidesthemodeltoexplainitsmisalignments.\ndiscovered a security camera overlooking the changing room\nat a Goa outlet. The camera was found to be capable of\ntaking photos of customers...\n• GTEeffectivelyadjustsmisalignedattributes\nThe length (word count) of the summary is 59 and the\nonlyinafewiterations.\nl Ee xn pg lt ah\nin\no wf t hh ye\ny\na or uti rc l ce\na\nli cs u2 l2 a3 ti,\no\ns no ii st wis\nr\na op np gr ,o ax nim\nd\na rete vl iy\nse\n2 t6 h.5 e% . A G+I SS\nsummary in the 32.5% length of the article: =\nGTE\nThere was a mistake in my previous calculation ..\n2 EvaluatingcontrollabilityofLLMs\nFabIndia is in trouble after a minister found a security\ncamera overlooking the changing room at one of its Goa\n2.1 Controllableattributes outlets. The camera was capable of taking photos of\ncustomers. The incident was exposed when Federal ..\nWe investigate the controllability of LLMs for\nFigure1: Overviewofguide-to-explainsystem(GTE).\nfour attributes: extractiveness, length, topic, and\nspeaker. Extractivenessevaluateshowmuchofthe\nsummary’scontentdirectlyoverlapswiththeorigi- speaker’sspokenwordsappearinginthesummary.\nnaltext. Ahighlyextractivesummaryisrequired Incontrast,weextractspeakerutterancesandcon-\nwhenusersneedtoretaintheoriginalcontext,such structspeakersetS andutilizeBERTScore(Zhang\nas in academic papers; however, paraphrasing is etal.,2020)tocomputetheembeddingsimilarity\nappliedtotailorthesummaryingeneralcases. The betweenthesummarypandS: BERTScore(p,S).\nlengthcountstheratiobetweenthemaintextand\nLabel reinterpretation We use the publicly\nthesummary. Thepreferredsummarylengthvaries\navailableMACSumdataset(Zhangetal.,2023b)\ndepending on the information density of the text\nfor controllable summarization. However, exist-\nanduserpreferences. Fortopicsorspeakers,users\ninglabelsfornumericalattributesareambiguous\nmayprefersummariesfocusedonaspecifictopic\nascriteriadegreesarenotspecified(e.g.,highfor\norspeakersfromalongdocumentordialogue.\nextractiveness and short for length). To provide\nAttributemeasurement Previousmethodshave detailed criteria, we reinterpret the labels based\nnoteffectivelyaccountedforattribute-focusedas- on the attribute distributions in each training set.\npects. Inthecaseofextractiveness,itisstraightfor- Forextractiveness,wesetlabelsas{normal: 85%,\nwardtodeterminehowmuchofthesummary’scon- high: 90%, fully: 100%}. Unlike previous meth-\ntentdirectlyoverlapswiththeoriginaltext. How- ods,wedefinethesummarylengthasaratioofthe\never,forlength,promptssuggestedbyearlierworks original text rather than a fixed value. Since the\nspecify a fixed number of sentences, e.g., \"3 sen- existinglabelsdonotdistinctlydifferentiatethese\ntences,\"butthisapproachfailstoaccountforvari- ratios,wesetthelengthsto{short: 7.5%,normal:\nationsinsentencelengthanddoesnotaccurately 15%,long: 32.5%},providingclearerdistinctions\nreflect the summary’s actual length (Goyal et al., withevaluatingabroaderrangeofcontrollability.\n2023; Liu et al., 2024; Yuan et al., 2024). Thus, LabelingdetailscanbefoundinAppendixA.\nwecalculatethesummarylengthasaratiorelative\n2.2 Controllablilityassessment\ntothemaintext. Fortopics, Zhangetal.(2023b)\ncalculatedthefrequencyoftopic-relatedwordsin WeevaluatetheabilityofLLMstoadjusttheirout-\nthe summary. However, even if topic words do putsbasedonspecifiedattributes(Table1,2). Our\nnotexplicitlyappear,thesummarycanstillreflect assessmentincludestwoevaluations: (1)thefailure\nthecorecontextofthetopic,especiallyforLLM- rateuponreachingthepredefinedmaximumitera-\ngenerated summaries, which tend to paraphrase tionswithoutachievingthedesiredmodifications\nthe content. Therefore, we compute the embed- and (2) the average iterations required to adjust\ndingsimilarityB betweenthetopicwordandeach an attribute, calculated only for successful cases.\nwordinthesummarys: 1 (cid:80) B(topic,word ), Wedenotethenaiveiterationapproach,whichsim-\nn i∈s i\nwhere n is the number of words in the sum- ply adjusts attributes repeatedly, as Iter. Most\nmary. If multiple topics k are present, we use LLMseffectivelycontrollinguisticattributes,such\ntheaverageembeddingsimilarityacrossalltopics: as topic and speaker. However, LLMs struggle\n1 (cid:80) 1 (cid:80) B(topic ,word ). For speakers, withnumericalattributes,includingextractiveness\nk j∈k n i∈s k i\nZhangetal.(2023b)calculatethefrequencyofthe and length. Both Llama-70B and GPT-4o-Iter\nExtractiveness(↓/↓) Length(↓/↓)\nModel Topic(↓) Speaker(↓)\nnormal high full avg short normal long avg\nPhi-3-medium-Iter 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 38.08%/0.22 -\nPhi-3-medium-GTE 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 37.97%/0.04 -\nLlama3-8B-Iter 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 100.00%/⟳ 57.14%/0.12 -\nLlama3-8B-GTE 12.63%/3.52 11.63%/2.53 0.00%/1.46 11.70%/3.26 26.40%/3.08 10.92%/2.26 13.18%/3.85 14.99%/2.80 25.56%/0.91 -\nLlama3-70B-Iter 54.82%/8.44 37.21%/7.47 2.70%/3.78 49.91%/8.05 18.40%/6.58 54.61%/10.42 67.44%/12.00 49.36%/8.24 0.00%/0.24 -\nLlama3-70B-AIS 26.55%/6.57 18.60%/7.81 0.00%/1.86 24.14%/6.52 4.80%/5.42 2.73%/3.81 10.85%/4.84 5.12%/4.39 0.00%/0.10 -\nLlama3-70B-GTE 0.21%/3.28 0.00%/2.83 0.00%/1.50 0.18%/3.22 0.00%/1.10 0.00%/1.61 2.32%/3.14 0.55%/1.90 0.00%/0.01 -\nGPT-3.5-Iter 45.18%/9.80 60.47%/⟳ 94.59%/⟳ 49.73%/9.80 53.60%/⟳ 80.89%/⟳ 88.37%/⟳ 76.42%/⟳ 0.00%/0.00 -\nGPT-3.5-GTE 17.56%/3.86 51.16%/5.00 67.57%/4.00 23.58%/3.90 5.60%/4.63 44.03%/6.62 78.29%/7.00 43.33%/5.95 0.00%/0.00 -\nGPT-4o-Iter 34.69%/6.77 55.81%/⟳ 78.38%/3.00 39.31%/6.63 72.00%/⟳ 64.85%/⟳ 79.07%/⟳ 69.84%/⟳ 0.38%/0.02 -\nGPT-4o-AIS 35.12%/5.50 48.84%/15.50 62.16%/6.00 38.03%/6.13 60.00%/8.79 61.09%/9.40 78.29%/2.00 64.90%/8.60 0.00%/0.04 -\nGPT-4o-GTE 0.00%/2.76 0.00%/4.70 0.00%/2.03 0.00%/2.87 0.00%/1.20 0.00%/1.21 0.00%/1.96 0.00%/1.42 0.00%/0.02 -\nTable1:TheresultsofcontrollabilitymeasuredontheMACSum dataset. Thebolddenotesthebestperformance.\nDoc\nWesetthemaximumnumberofIterationsto20. Ifallthecasesremainedunadjustedintheinitialdraftandstill\nnotberesolvedafterreachingthemaximumiterations,theyweremarkedwith⟳.\nExtractiveness(↓/↓) Length(↓/↓)\nModel Topic(↓) Speaker(↓)\nnormal high fully avg short normal long avg\nLlama3-70B-Iter 31.78%/8.13 43.59%/8.40 8.16%/5.39 29.63%/7.59 12.00%/⟳ 93.75%/6.00 98.00%/⟳ 81.79%/6.00 0.00%/0.01 0.00%/0.00\nLlama3-70B-AIS 14.41%/5.91 23.08%/5.31 0.00%/3.72 13.27%/5.50 0.00%/1.25 62.05%/5.70 92.00%/9.33 57.10%/5.62 0.00%/0.02 0.00%/0.00\nLlama3-70B-GTE 0.00%/2.31 0.00%/2.56 4.08%/3.64 0.61%/2.49 0.00%/1.00 36.61%/4.73 80.00%/5.70 37.65%/4.53 0.00%/0.01 0.00%/0.00\nGPT-4o-Iter 79.24%/4.36 82.05%3.67 59.18%/1.00 76.54%/4.00 6.00%/⟳ 98.21%/⟳ 100.00%/⟳ 84.26%/⟳ 0.31%/0.01 0.00%/0.00\nGPT-4o-AIS 84.75%/4.00 87.18%1.50 53.06%5.10 80.25%/4.32 2.00%/4.50 96.43%/⟳ 100.00%/⟳ 82.41%/4.50 0.00%/0.01 0.00%/0.00\nGPT-4o-GTE 17.80%/7.94 25.64%/7.92 8.16%/4.58 17.28%/7.53 0.00%/1.40 9.82%/2.75 44.00%/4.21 13.58%/2.90 0.00%/0.02 0.00%/0.00\nTable2: TheresultsofcontrollabilitymeasuredontheMACSum dataset.\nDial\nshow a very high failure rate with numerous iter- howhumanssolvecomplexproblemsbyreviewing\nations. In particular, LLMs struggle to adjust for theirmistakestoproducemoreaccurateresponses\nnormalextractivenessandlonglengthattributes. in the future. Receiving [a; i, s′; AIS; GS] as in-\nputs, where a denotes the article, the model first\n3 GTE\nreflects on the reasons for the initial error before\ngeneratingarevisedsummary. Iftherevisedsum-\nTherefore,weintroduceaguide-to-explain(GTE)\nmarystillfailstosatisfytheattributes,GTErepeats\nframework(Figure1)toadjustthechallengingnu-\nuntil the model produces an attribute-compliant\nmerical attributes. We provide an attribute iden-\nsummary. As LLMs are known to struggle with\ntification step to adjust incorrectly generated re-\nnumber-relatedtasks(Thawanietal.,2021;Imani\nsponsesandguidetheLLMstoreflectbyexplain-\netal.,2023),ourguidancetoexplainwhytheircal-\ningthereasonsbehindtheseerrors. Ourapproach\nculationisincorrect,followedbygeneratingsum-\nallowsthemodeltomakeappropriatecorrections\nmariesassistsineffectivelycontrollingthenumeric\ninsubsequentiterations.\nattributes. TheusedpromptsareinAppendixB.\n3.1 Attributeidentificationstep\n4 Experimentalsetup\nWefirstprompttheLLMtogenerateaninitialdraft\ns′ thatreflectsthespecifiedattributesi. IftheLLM\nWe evaluate the controllability of various LLMs,\nfailstocontroltheinstructedattributesaccurately,\nincluding Phi-3 (Abdin et al., 2024), Llama3\nwe provide attribute identification step (AIS) to\n(Dubeyetal.,2024),andGPTseries(Brown,2020;\nguide the model on how to adjust the attributes.\nAchiam et al., 2023). To analyze model perfor-\nLLMmayhavedifficultymeasuringattributessuch\nmancebysize,weuseboththe8Bandquantized\nasextractivenessorthelengthratiorelativetothe 70Bversions1 ofLlama3,andGPT-3.5andGPT-4.\noriginaltext. Thus,weprovideastep-by-stepap-\nWe used two datasets, the MACSum and\nDoc\nproach to instruct the model on revising its sum-\nthe MACSum datasets (Zhang et al., 2023b),\nDial\nmarybasedonthegeneratedoutput.\nwhichcomprisecommitteemeetingsandnewscon-\ntents each. Both datasets are for mixed-attribute\n3.2 Guidancestep\nsummarization that control multiple attributes si-\nAfter providing instructions AIS on revising the\nmultaneously,butonlyMACSum hasspeaker\nDial\nsummary, we supply guidance step (GS) to the\nattribute. SinceweevaluateLLMperformanceon\nmodel to explain why it initially failed to adjust\ntheattributescorrectly. Thisprocessissimilarto 1casperhansen/llama-3-70b-instruct-awq\nModel Coherence Consistency Fluency Relevance QuestEval Single attribute Mixed attribute\nIter(Ext) 0.820 0.800 0.859 0.696 0.523\nAIS(Ext) 0.884 0.843 0.905 0.785 0.554\nIter(Len) 0.836 0.803 0.836 0.759 0.484\nAIS(Len) 0.934 0.834 0.942 0.887 0.548\nGTE(Ext) 0.941 0.873 0.937 0.880 0.590\nGTE(Len) 0.937 0.840 0.944 0.901 0.553\nTable3: Qualityofthecontrolledsummaries.\nFigure2: Performanceinmixed-attribute.\nindividualattributes,weuseattributesseparately.\nUniEvaldimensionsandQuestEvalscores,demon-\n5 ResultsandDiscussions\nstratingeffectiveattributecontrolwhilemaintain-\nMainresults Wedefinethestrategysolelypro- ingoverallsummaryquality.\nvidingattributeidentificationstepsasAISandde-\nMixed attributes controllability We observed\nnote our full guiding framework as GTE. As a\nthatwhengeneratingsummariescontrolledforall\nresult, our GTE demonstrates remarkably lower\nattributes simultaneously, the model effectively\nfailure rates and fewer iterations when adjusting\nhandled linguistic attributes but faced challenges\nsummariesacrossallattributes,includingchalleng-\nwith numerical attributes. Notably, satisfying all\ningnumericalattributesinMACSum (Table1).\nDoc attributeswithinthemaximumnumberofiterations\nNotably, while applying GTE to smaller models\nprovedchallengingforallmethods,includingGTE.\nsuch as Phi-3 and Llama3-8B resulted in signifi-\ncantperformanceimprovements,weobservedthat Sequential-planning Discoveringthechallenges\nfailureswerealmostnonexistentwhenappliedto in precisely controlling all attributes in parallel,\nlargermodelssuchasLlama-70BorGPT-4o. weintroduceasequential-planningstrategy,which\nLLMs encounter more difficulties with the graduallyadjustsattributesfromtheill-controlled\nMACSum Dial dataset (Table 2). The dataset, with the initial draft using GTE. However, modi-\nwhich is derived from QMSum (Zhong et al., fying one attribute often disrupted previously ad-\n2021), consists of lengthy and diverse content justedattributesduetocorrelations. Forexample,\nparliamentary and committee meetings, making controllinglengthfirstwouldstillleadtochanges\nit more challenging compared to the CNN-news- in length when adjusting extractiveness. Conse-\nbased MACSum Doc. Notably, both the GPT-4o- quently,sequentialadjustmentsresultinamodest\nIter and GPT-4o-AIS failed to adjust for long performancegapcomparedtotheinitialdraft(Fig-\nlength, whereas our GPT-4o-GTE demonstrated ure2). Weassesstheattributesusingtherootmean\nacommendablesuccessrate. Regardingextractive- squarederror(RMSE)betweentheinstructedval-\nness,theIterandAISofGPT-4oexhibitrelatively uesandthoseinthegeneratedsummaries.\nlow iteration counts since the models mostly ex-\nceed the maximum iteration (⟳). While they fail 6 Conclusion\nnearly80%,ourGTEdemonstratesasignificantly\nInthiswork,werevisitthemeasurementofcontrol-\nlowerfailurerateat17.28%withlowiterations.\nlable summarization with various attributes. We\nQuality of controlled summary Instead of evaluate the controllability of multiple attributes\nROUGE score(Lin, 2004), which does not ad- in summary generation with LLMs, finding that\nequately evaluate the quality of the summaries LLMs struggle to adjust numeric attributes com-\n(Zhongetal.,2022;Scialometal.,2021;Ryuetal., paredtolinguisticones. Toaddressthislimitation,\n2024a),weuseUniEval(Zhongetal.,2022)and we propose a guide-to-explain (GTE) approach,\nQuestEval(Scialometal.,2021)toassessboththe wherethemodelisguidedtoexplainitsmisalign-\ninherent quality of the summaries and their fac- ments and then grounded this explanation to pro-\ntual consistency with the source text. UniEval, a ducebetter-controlledsummariesinsubsequentit-\nmulti-dimensionalevaluatorwithhighhumancor- erations. GTEenablesLLMstocontrolchallenging\nrelation, assesses dimensions such as coherence, numericalattributeswithlowerfailureandfewer\nconsistency,fluency,andrelevance,andQuestEval iterations. Further, we validate the quality of the\nevaluatesfactualityviaquestionanswering. Table3 controlledsummariesviaamulti-dimensionaleval-\nshowsthatourmethod’ssummariesoutperformall uation,demonstratingthehigh-qualitygeneration.\nLimitation TomBBrown.2020. Languagemodelsarefew-shot\nlearners. arXivpreprintarXiv:2005.14165.\nWe evaluated the controllability of various at-\ntributes in LLMs and introduced a novel guide- Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and\nto-explain(GTE)frameworktoaddresschallenges\nJasonWeston.2024. Chain-of-verificationreduces\ninnumericalattributes. WhileGTEenhancedcon- hallucinationinlargelanguagemodels. InFindings\ntrolovernumericalattributes,itstillstruggledwith oftheAssociationforComputationalLinguisticsACL\nhighlycorrelatedmixednumericalattributes. Ad- 2024,pages3563–3578,Bangkok,Thailandandvir-\ntualmeeting.AssociationforComputationalLinguis-\nditionally, sequential planning, which adjusts at-\ntics.\ntributes in order of least alignment, also faced\ndifficulties achieving precise control. Even after AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,\nAbhishekKadian,AhmadAl-Dahle,AieshaLetman,\nproperlyadjustingoneattribute,modifyingthecor-\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nrelated numerical attribute caused the previously\nFan,etal.2024. Thellama3herdofmodels. arXiv\nadjusted attribute to change. We believe further preprintarXiv:2407.21783.\nresearchcouldexploremoreeffectivemethodsfor\nZhibinGou,ZhihongShao,YeyunGong,yelongshen,\naddressingthesechallenges.\nYujiu Yang, Nan Duan, and Weizhu Chen. 2024.\nCRITIC: Large language models can self-correct\nEthics\nwithtool-interactivecritiquing. InTheTwelfthInter-\nnationalConferenceonLearningRepresentations.\nWeusedpubliclyavailableMACSumdatasetsfor\nourresearch,conductingexperimentswithPhi-3, Tanya Goyal et al. 2023. News summarization\nLlama32,GPT-3.5,andGPT-4ofromApriltoOc- and evaluation in the era of gpt-3. Preprint,\narXiv:2209.12356.\ntober2024.\nShimaImani,LiangDu,andHarshShrivastava.2023.\nAcknowledgments\nMathPrompter: Mathematicalreasoningusinglarge\nlanguage models. In Proceedings of the 61st An-\nThisworkwassupportedbytheNationalResearch\nnualMeetingoftheAssociationforComputational\nFoundationofKorea(NRF)grantfundedbytheKo- Linguistics (Volume 5: Industry Track), pages 37–\nreagovernment(MSIT)(No. RS-2023-00217286) 42,Toronto,Canada.AssociationforComputational\nLinguistics.\nand Institute of Information & communications\nTechnology Planning & Evaluation (IITP) grant Chin-Yew Lin. 2004. ROUGE: A package for auto-\nfundedbytheKoreagovernment(MSIT)(No.RS- maticevaluationofsummaries. InTextSummariza-\ntionBranchesOut,pages74–81,Barcelona,Spain.\n2019-II191906, Artificial Intelligence Graduate\nAssociationforComputationalLinguistics.\nSchoolProgram(POSTECH)).\nYixinLiu,AlexanderFabbri,JiawenChen,YilunZhao,\nSimeng Han, Shafiq Joty, Pengfei Liu, Dragomir\nReferences Radev,Chien-ShengWu,andArmanCohan.2024.\nBenchmarking generation and evaluation capabili-\nMarahAbdin,SamAdeJacobs,AmmarAhmadAwan,\ntiesoflargelanguagemodelsforinstructioncontrol-\nJyoti Aneja, Ahmed Awadallah, Hany Awadalla,\nlablesummarization. InFindingsoftheAssociation\nNguyenBach,AmitBahree,ArashBakhtiari,Harki-\nforComputationalLinguistics: NAACL2024,pages\nratBehl,etal.2024. Phi-3technicalreport: Ahighly\n4481–4501, Mexico City, Mexico. Association for\ncapablelanguagemodellocallyonyourphone. arXiv\nComputationalLinguistics.\npreprintarXiv:2404.14219.\nAmanMadaan, NiketTandon,PrakharGupta,Skyler\nJoshAchiam,StevenAdler,SandhiniAgarwal,Lama\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nDiogoAlmeida,JankoAltenschmidt,SamAltman,\nShashank Gupta, Bodhisattwa Prasad Majumder,\nShyamalAnadkat,etal.2023. Gpt-4technicalreport.\nKatherine Hermann, Sean Welleck, Amir Yazdan-\narXivpreprintarXiv:2303.08774.\nbakhsh, and Peter Clark. 2023. Self-refine: Itera-\ntiverefinementwithself-feedback. InAdvancesin\nAlanAkbik,Bergmann,etal.2019. FLAIR:Aneasy-to-\nNeuralInformationProcessingSystems,volume36,\nuseframeworkforstate-of-the-artNLP. InAnnual\npages46534–46594.CurranAssociates,Inc.\nConference of the North American Chapter of the\nAssociationforComputationalLinguistics(Demon-\nZimingMao,ChenHenryWu,AnsongNi,YusenZhang,\nstrations).\nRui Zhang, Tao Yu, Budhaditya Deb, Chenguang\n2MetaLlama3 CommunityLicense, Copyright © Meta Zhu,AhmedAwadallah,andDragomirRadev.2022.\nPlatforms, Inc. All Rights Reserved. More details can be DYLE: Dynamic latent extraction for abstractive\nfoundat:Llama3License long-input summarization. In Proceedings of the\n60thAnnualMeetingoftheAssociationforCompu- prospects - a survey. In Findings of the Associa-\ntationalLinguistics(Volume1: LongPapers),pages tionforComputationalLinguisticsACL2024,pages\n1687–1698,Dublin,Ireland.AssociationforCompu- 1603–1623,Bangkok,Thailandandvirtualmeeting.\ntationalLinguistics. AssociationforComputationalLinguistics.\nArtidoro Pagnoni, Alex Fabbri, Wojciech Kryscinski, Jesse Vig, Alexander Fabbri, Wojciech Kryscinski,\nand Chien-Sheng Wu. 2023. Socratic pretraining: Chien-ShengWu,andWenhaoLiu.2022. Exploring\nQuestion-drivenpretrainingforcontrollablesumma- neuralmodelsforquery-focusedsummarization. In\nrization. InProceedingsofthe61stAnnualMeeting FindingsoftheAssociationforComputationalLin-\noftheAssociationforComputationalLinguistics(Vol- guistics: NAACL 2022, pages 1455–1468, Seattle,\nume1: LongPapers),pages12737–12755,Toronto, United States. Association for Computational Lin-\nCanada.AssociationforComputationalLinguistics. guistics.\nXiao Pu, Mingqi Gao, and Xiaojun Wan. 2023. BinWang,ZhengyuanLiu,andNancyChen.2023. In-\nSummarization is (almost) dead. arXiv preprint structivedialoguesummarizationwithqueryaggre-\narXiv:2309.09558. gations. InProceedingsofthe2023Conferenceon\nEmpiricalMethodsinNaturalLanguageProcessing,\nSangwonRyu,HeejinDo,YunsuKim,GaryLee,and\npages7630–7653,Singapore.AssociationforCom-\nJungseulOk.2024a. Multi-dimensionaloptimization\nputationalLinguistics.\nfor text summarization via reinforcement learning.\nIn Proceedings of the 62nd Annual Meeting of the\nYixuanWeng,MinjunZhu,FeiXia,BinLi,ShizhuHe,\nAssociationforComputationalLinguistics(Volume1:\nShengping Liu, Bin Sun, Kang Liu, and Jun Zhao.\nLongPapers),pages5858–5871,Bangkok,Thailand.\n2023. Large language models are better reasoners\nAssociationforComputationalLinguistics.\nwith self-verification. In Findings of the Associa-\ntionforComputationalLinguistics: EMNLP2023,\nSangwonRyu,HeejinDo,YunsuKim,GaryGeunbae\npages2550–2575,Singapore.AssociationforCom-\nLee,andJungseulOk.2024b. Key-element-informed\nputationalLinguistics.\nsllmtuningfordocumentsummarization. InInter-\nspeech2024,pages1940–1944.\nRuochenXu,SongWang,YangLiu,ShuohangWang,\nYichong Xu, Dan Iter, Pengcheng He, Chenguang\nThomasScialom,Paul-AlexisDray,SylvainLamprier,\nZhu, and Michael Zeng. 2023. LMGQS: A large-\nBenjaminPiwowarski,JacopoStaiano,AlexWang,\nscale dataset for query-focused summarization. In\nandPatrickGallinari.2021. QuestEval: Summariza-\nFindingsoftheAssociationforComputationalLin-\ntionasksforfact-basedevaluation. InProceedingsof\nguistics: EMNLP2023,pages14764–14776,Singa-\nthe2021ConferenceonEmpiricalMethodsinNatu-\npore.AssociationforComputationalLinguistics.\nralLanguageProcessing,pages6594–6604,Online\nandPuntaCana,DominicanRepublic.Association\nWeizheYuan,IliaKulikov,PingYu,KyunghyunCho,\nforComputationalLinguistics.\nSainbayar Sukhbaatar, Jason Weston, and Jing Xu.\nShichao Sun, Ruifeng Yuan, Ziqiang Cao, Wenjie Li, 2024. Following length constraints in instructions.\nandPengfeiLiu.2024. Promptchainingorstepwise Preprint,arXiv:2406.17744.\nprompt? refinementintextsummarization. InFind-\nHaopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023a.\ningsoftheAssociationforComputationalLinguistics\nSummIt: IterativetextsummarizationviaChatGPT.\nACL 2024, pages 7551–7558, Bangkok, Thailand\nIn Findings of the Association for Computational\nandvirtualmeeting.AssociationforComputational\nLinguistics: EMNLP2023,pages10644–10657,Sin-\nLinguistics.\ngapore.AssociationforComputationalLinguistics.\nYutingTang,RatishPuduppully,ZhengyuanLiu,and\nNancyChen.2023. In-contextlearningoflargelan- Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nguagemodelsforcontrolleddialoguesummarization: Weinberger,andYoavArtzi.2020. Bertscore: Eval-\nAholisticbenchmarkandempiricalanalysis. InPro- uatingtextgenerationwithbert. InProceedingsof\nceedingsofthe4thNewFrontiersinSummarization theInternationalConferenceonLearningRepresen-\nWorkshop,pages56–67,Singapore.Associationfor tations.\nComputationalLinguistics.\nTianyiZhang,FaisalLadhak,EsinDurmus,PercyLiang,\nAvijit Thawani, Jay Pujara, Filip Ilievski, and Pedro Kathleen McKeown, and Tatsunori B. Hashimoto.\nSzekely. 2021. Representing numbers in NLP: a 2024. Benchmarkinglargelanguagemodelsfornews\nsurvey and a vision. In Proceedings of the 2021 summarization. TransactionsoftheAssociationfor\nConference of the North American Chapter of the ComputationalLinguistics,12:39–57.\nAssociationforComputationalLinguistics: Human\nLanguageTechnologies,pages644–656,Online.As- YusenZhang,YangLiu,ZiyiYang,YuweiFang,Yulong\nsociationforComputationalLinguistics. Chen, Dragomir Radev, Chenguang Zhu, Michael\nZeng, andRuiZhang.2023b. MACSum: Control-\nAshok Urlana, Pruthwik Mishra, Tathagato Roy, and lablesummarizationwithmixedattributes. Transac-\nRahul Mishra. 2024. Controllable text summa- tionsoftheAssociationforComputationalLinguis-\nrization: Unraveling challenges, approaches, and tics,11:787–803.\nYusenZhang,AnsongNi,ZimingMao,ChenHenryWu, verbs+0.2×tokens+0.3×nouns+0.4×\nChenguangZhu,BudhadityaDeb,AhmedAwadallah, numericaltokens)/number of sentences.\nDragomirRadev,andRuiZhang.2022. Summn: A\nmulti-stagesummarizationframeworkforlonginput The detailed data distribution for both\ndialoguesanddocuments. InProceedingsofthe60th\nMACSum andMACSum arepresentedin\nAnnualMeetingoftheAssociationforComputational Doc Dial\nLinguistics (Volume 1: Long Papers), pages 1592– Table 4. We observe that the labels in previous\n1604,Dublin,Ireland.AssociationforComputational workdonotaccountforattributes. Inthecaseof\nLinguistics. topic,thespecifictopickeywordsmaynotalways\nappearexplicitly,evenifthesummaryisfocused\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and on the given topic. Similarly, for speaker, LLMs\nJiawei Han. 2022. Towards a unified multi- mayparaphrasethespeaker’swordsusingdifferent\ndimensional evaluator for text generation. In Pro-\ntermsorexpressions. Therefore,wereinterpreted\nceedingsofthe2022ConferenceonEmpiricalMeth-\nthese labels in Section 2.1 to account for these\nods in Natural Language Processing, pages 2023–\n2038,AbuDhabi,UnitedArabEmirates.Association factors.\nforComputationalLinguistics. Fortheextractivenessandlengthattributes,the\noriginallabelswerecategorizedas‘normal’,‘high’,\nMingZhong,DaYin,TaoYu,AhmadZaidi,Mutethia\n‘fully’ for extractiveness and ‘short’, ‘normal’,\nMutuma,RahulJha,AhmedHassanAwadallah,Asli\nCelikyilmaz,YangLiu,XipengQiu,andDragomir ‘long’ for length. However, since the labels were\nRadev.2021. QMSum: Anewbenchmarkforquery- not numerically defined, we deemed the criteria\nbasedmulti-domainmeetingsummarization. InPro-\nunclear. Therefore, we redefined these labels for\nceedingsofthe2021ConferenceoftheNorthAmer-\nouruse.\nicanChapteroftheAssociationforComputational\nLinguistics: HumanLanguageTechnologies,pages We calculate extractiveness based on the pro-\n5905–5921,Online.AssociationforComputational portion of words in the source text that appeared\nLinguistics.\nexactlyinthesummary. Forlength,whiletheorig-\ninaldatasetwaslabelledbasedonwordcountand\nA Attributedetails compressionrate,weonlyconsideredtheratioof\nthe summary to the original text. Table 4 shows\nA.1 Attributeanalysis\nthattheratioswerenotstrictlydistinct,withsome\nWe analysis the data distributions in the overlapbetweenlabels. Thus,werelabeledbound-\nMACSum and MACSum (Zhang et al., ariesinSection2.1. Sincetheselabelvaluesarenot\nDoc Dial\n2023b) datasets. The attributes used in the orig- exact for the reference summary, we adjusted at-\ninaldatasetsaredescribedasfollows: tributesaccordingtothespecifiednewlabels. Con-\nsequently,wedidnotusethereferencesummaries\n• Extractiveness: Controls how much of the\nfromtheoriginaldataset.\nsummaryisdirectlyextractedfromthesource Forspecificity,weusedtheFlair3 (Akbiketal.,\ntext. It is evaluated using the average of\n2019)modelfornamedentityrecognition(NER)\nROUGE-2andROUGE-3precisionscores.\nmodel to measure the number of entities in each\nsummary. Sincethenumberofentitiesvarieswith\n• Length: Thenumberofwordsinthesummary.\nsummary length, we calculated specificity as the\nItisevaluatedbasedontokenlength.\nratiooftotalentitiestototalwordsinthesummary.\n• Topic: Controlthesummarytoalignwiththe However,theannotationforspecificityintheorigi-\ngiventopic,andmultipletopicscanbepresent. naldatasetwasinverselyrelatedtoourcalculations,\nTheevaluationisbasedontheproportionof sowedidnotusethisattributeinourstudy.\ntopicwordsappearinginthesummary.\nA.2 Iterationthreshlold\n• Speaker: Controlthesummarytofocusonthe\nWesetattribute-specificthresholdsanditeratively\nspeech of a specific speaker. It is evaluated\nadjustedthemuntilthecriteriaweresatisfied. For\nby the ratio of the speaker’s words included\nextractivenessandlength, iterationswereconsid-\ninthesummary.\neredsuccessfulifthevaluesfellwithinarelabeled\nvalue ±5 range. For the topic and speaker, we\n• Specificity: Controls the level of detail or\nset the minimum embedding similarity score as\ndescriptive content in the summary. The\nevaluationformulais: Specificity = (0.1× 3https://github.com/flairNLP/flair\nAttribute Label\nMACSumDoc MACSumDial\nDist. Relabel #ofsummaries Dist. Relabel #ofsummaries\nshort 0.7-15.0% 7.5% 1059 0.2-20.8% 7.5% 300\nLength normal 0.5-48.6% 20.0% 2194 0.3-41.9% 20.0% 1693\nlong 1.5-39.8% 32.5% 1025 0.7-32.4% 32.5% 345\nnormal 35.7-100.0% 85.0% 3731 53.2-100.0% 85.0% 1661\nExtractiveness high 55.0-100.0% 90.0% 287 63.0-100.0% 90.0% 340\nfully 84.6-100.0% 100.0% 260 75.9-100.0% 100.0% 337\nTopic - 74.8-88.8 74 2013 73.6-87.0 74 2317\nSpeaker - - - - 75.6-92.0 75 1796\nnormal 0.105% - 3713 0.115% - 1702\nSpecificity\nhigh 0.062% - 565 0.110% - 636\nTable4: DatadistributionofMACSum andMACSum\nDoc Dial\nthe criterion for achieving a successful topic- or\nspeaker-focusedsummary.\nB Guide-to-explain(GTE)prompts\nArticle\n(CNN)Easter is unique on the Christian calendar, a major point in the cycle of the religious year, and one that has always been able to resist the\ncommercialization and culture warring that surrounds Christmas. That's in part because Easter is genuinely about how religious impulses, and patterns, can\noperate in ways that affect our lives. Nevertheless, I'm often surprised by how little people, even those supposedly within the Christian tradition, actually know\nabout what is called Holy Week and its culmination on Easter Sunday. At a time when our culture is roiled by questions of identity and ethics (and tolerance) that\nhave profound religious implications, it's worth pausing to explore this crucial holiday --and the awareness of the human condition, in all its sadness and glory,\nthat it engenders. After all, Holy Week calls mostly to those who incline their minds and hearts in its direction with seriousness of intent. Still, the fuss must\npuzzle those looking on, wondering what it all means. Why do Christians make so much of this springtime week, and make so much of Easter weekend? There is\na phrase that many never come across, even among Christians: Easter Triduum .This refers to the three days of Easter that beginwith Good Friday, proceed\nthrough Holy Saturday, and conclude with Easter Sunday. It's definitely aprogression, although the word itself --triduum --can refer to any three days of prayer...\nInitial prompt\nsummarize the above article in 32.5% length of the article:\nSummary (16.9%)\nEaster is a significant Christian holiday that often gets overlooked, especially when compared to Christmas. Nearly everyone within the Christian tradition knows\nthe basics of Easter, but not many understand the full significance and rituals of Holy Week and the Easter Triduum, which are three days of prayer that begin on\nGood Friday and end on Easter Sunday. Holy Communion originated from the Last Supper, a Passover meal that Jesus shared with hisdisciples before being\ncrucified. On Maundy Thursday, Jesus washed the disciples' feet to demonstrate humility...\nGTE\nThe length (word count) of the summary is 157 and the length of the article is 931, so it is approximately 16.9%.Explain why your calculation is wrong,and revise\nthe summary in the 32.5% length of the article:\nRevised Summary (29.4%)\nThe calculation is wrong because the article length is 931 words, and the summary length is 157 words. To calculate the percentage, I should have divided 157 by\n931, which gives me 16.9%. This means the summary is only 16.9% of the original article, not 32.5%.\nTo revise the summary to reach 32.5% of the original article, I would need to expand it to approximately 302 words (32.5% of 931). Here is a revised summary:\nEaster is a significant Christian holiday that often gets overlooked, especially when compared to Christmas. Despite its importance, many people within the\nChristian tradition know little about the full significance and rituals of Holy Week and the Easter Triduum. This three-day period of prayer begins on Good Friday,\ncommemorating Jesus' crucifixion, and concludes on Easter Sunday, celebrating his resurrection...\nFigure3: Lengthguide-to-explain(GTE).\nArticle\n(CNN)Easter is unique on the Christian calendar, a major point in the cycle of the religious year, and one that has always been able to resist the\ncommercialization and culture warring that surrounds Christmas. That's in part because Easter is genuinely about how religious impulses, and patterns, can\noperate in ways that affect our lives. Nevertheless, I'm often surprised by how little people, even those supposedly within the Christian tradition, actually know\nabout what is called Holy Week and its culmination on Easter Sunday. At a time when our culture is roiled by questions of identity and ethics (and tolerance) that\nhave profound religious implications, it's worth pausing to explore this crucial holiday --and the awareness of the human condition, in all its sadness and glory,\nthat it engenders. After all, Holy Week calls mostly to those who incline their minds and hearts in its direction with seriousness of intent. Still, the fuss must\npuzzle those looking on, wondering what it all means. Why do Christians make so much of this springtime week, and make so much of Easter weekend? There is\na phrase that many never come across, even among Christians: Easter Triduum .This refers to the three days of Easter that beginwith Good Friday, proceed\nthrough Holy Saturday, and conclude with Easter Sunday. It's definitely aprogression, although the word itself --triduum --can refer to any three days of prayer...\nInitial prompt\nsummarize the above article in 32.5% length of the article:\nSummary (16.9%)\nEaster is a significant Christian holiday that often gets overlooked, especially when compared to Christmas. Nearly everyone within the Christian tradition knows\nthe basics of Easter, but not many understand the full significance and rituals of Holy Week and the Easter Triduum, which are three days of prayer that begin on\nGood Friday and end on Easter Sunday. Holy Communion originated from the Last Supper, a Passover meal that Jesus shared with hisdisciples before being\ncrucified. On Maundy Thursday, Jesus washed the disciples' feet to demonstrate humility...\nAIS\nThe length (word count) of the summary is 157 and the length of the article is 931, so it is approximately 16.9%.Summarize the above article in the 32.5% length\nof the article:\nRevised Summary (23.4%)\nEaster, a significant Christian holiday, remains untouched by commercialization and culture wars. Many people, including Christians, are unfamiliar with Holy\nWeek and its culmination on Easter Sunday. The Easter Triduum, a three-day period, begins with Maundy Thursday, where Jesus shared the Last Supper with his\ndisciples, demonstrating his humility by washing their feet. Good Friday, the day of the crucifixion, is a somber reminder ofdeath, sacrifice, and fear. The\nGospels provide varying accounts of Jesus' final words, including his quote from the 22nd Psalm. Holy Saturday, often overlooked, serves as a passage between\nthe darkness of Good Friday and the hope of Easter Sunday. It's a day of anxiety and uncertainty, when the pain of the crucifixion still lingers, but hope begins to\nemerge. Easter Sunday, with its promise of transformation and resurrection, is a celebration of the human experience. The resurrection is not just a physical\nrevival but a total transformation, symbolizing the possibility of change and renewal in our own lives...\nFigure4: Lengthproblem-solvingstep(AIS).\nArticle\n(CNN)Easter is unique on the Christian calendar, a major point in the cycle of the religious year, and one that has always been able to resist the\ncommercialization and culture warring that surrounds Christmas. That's in part because Easter is genuinely about how religious impulses, and patterns, can\noperate in ways that affect our lives. Nevertheless, I'm often surprised by how little people, even those supposedly within the Christian tradition, actually know\nabout what is called Holy Week and its culmination on Easter Sunday. At a time when our culture is roiled by questions of identity and ethics (and tolerance) that\nhave profound religious implications, it's worth pausing to explore this crucial holiday --and the awareness of the human condition, in all its sadness and glory,\nthat it engenders. After all, Holy Week calls mostly to those who incline their minds and hearts in its direction with seriousness of intent. Still, the fuss must\npuzzle those looking on, wondering what it all means. Why do Christians make so much of this springtime week, and make so much of Easter weekend? There is\na phrase that many never come across, even among Christians: Easter Triduum .This refers to the three days of Easter that beginwith Good Friday, proceed\nthrough Holy Saturday, and conclude with Easter Sunday. It's definitely aprogression, although the word itself --triduum --can refer to any three days of prayer...\nInitial prompt\nsummarize the above article in 32.5% length of the article:\nSummary (16.9%)\nEaster is a significant Christian holiday that often gets overlooked, especially when compared to Christmas. Nearly everyone within the Christian tradition knows\nthe basics of Easter, but not many understand the full significance and rituals of Holy Week and the Easter Triduum, which are three days of prayer that begin on\nGood Friday and end on Easter Sunday. Holy Communion originated from the Last Supper, a Passover meal that Jesus shared with hisdisciples before being\ncrucified. On Maundy Thursday, Jesus washed the disciples' feet to demonstrate humility...\nIter\nsummarize the above article in 32.5% length of the article:\nRevised Summary (9.1%)\nEaster is a unique Christian event that resisted commercialization. Many Christians lack understanding of Holy Week and Easter. The Easter Triduum -Good\nFriday, Holy Saturday, and Easter Sunday -marks death, transition, and resurrection. This pattern reflects our daily struggles,suffering, and rebirth, making\nEaster a celebration of moving from darkness to light.\nFigure5: Lengthiteration(Iter).\nArticle\n(CNN)We did it again, in another American city. We set Baltimore on fire this time. We brutalized black bodies. We turned a funeral into a riot. We let things get\nout of hand. We looted. We threw stones at policemen. We threw stones at citizens. We created camera-ready chaos, and we replayed the images. We created a\nculture of such deep distrust and disrespect that violence seemed the inevitable response. We let the violence flow. We let the violence stand for everything\nthat's wrong with the things we already didn't like. By now you may be asking, \"Who's we? \"You may be saying with some irritation, \"Don't lump me in with them. I\ndidn't have anything to do with it.” To which the only real answer can be: Stop kidding yourself. The word \"we\" is one of thegreat American words. We the People.\nYeswe can. We are family. I use \"we\" a lot when I talk about our country's achievements. I like to say we won the Second World War, we put a man on the moon,\nwe invented the Internet, we gave the world jazz. Well, if I --a son of immigrants whose family had nothing to do with any of those accomplishments --if I get to\nclaim those aspects of American history, then surelyI have toclaim the unsavory aspects too. “We\" cuts both ways. We enslaved Africans. We cut\nReconstruction short and made a mockery of equal citizenship. We supported Jim Crow, then redlined, subordinated, and ghettoizedAfrican-Americans...\nInitial prompt\nsummarize the above article using 85% of words directly from the original document:\nSummary (100.0%)\nWe did it again, in another American city. We set Baltimore on fire this time. We brutalized black bodies. We turned a funeral into a riot. We let things get out of\nhand. We looted. We threw stones at policemen. We threw stones at citizens. We created camera-ready chaos, and we replayed the images. By now you may be\nasking, \"Who's we?\" You may be saying with some irritation, \"Don't lump me in with them. I didn't have anything to do with it.\" To which the only real answer can\nbe: Stop kidding yourself. The word \"we\" is one of the great American words. We the People...\nGTE\nThis summary contains 100.0% of words directly from the original document.Explain why your calculation is wrong,and revise the summary so that 85% of the\nwords are used directly as in the original document:\nRevised Summary (86.8%)\nI apologize for the mistake. Upon re-calculating, I realized that the original summary contained 79.4% of words directly from the original document. To revise the\nsummary to meet the 85% threshold, I’ll incorporate more words directly from the original document. Here's the revised summary:\n\"We did it again, in another American city\", perpetuating a cycle of violence and chaos. \"We set Baltimore on fire this time\", and in doing so, \"we brutalized black\nbodies\". The author emphasizes the significance of \"we\", underscoring collective responsibility for both accomplishments and injustices. \"We the People\"\nimplies a shared history, encompassing achievements like \"we won the Second World War\", \"we put a man on the moon\", \"we invented the Internet\", and \"we\ngave the world jazz\", as well as darker aspects like \"we enslaved Africans\", \"we cut Reconstruction short and made a mockery of equal citizenship\", \"we\nsupported Jim Crow, then redlined, subordinated, and ghettoized African-Americans\", and \"we failed\"...\nFigure6: Extractivenessguide-to-explain(GTE).\nArticle\n(CNN)We did it again, in another American city. We set Baltimore on fire this time. We brutalized black bodies. We turned a funeral into a riot. We let things get\nout of hand. We looted. We threw stones at policemen. We threw stones at citizens. We created camera-ready chaos, and we replayed the images. We created a\nculture of such deep distrust and disrespect that violence seemed the inevitable response. We let the violence flow. We let the violence stand for everything\nthat's wrong with the things we already didn't like. By now you may be asking, \"Who's we? \"You may be saying with some irritation, \"Don't lump me in with them. I\ndidn't have anything to do with it.” To which the only real answer can be: Stop kidding yourself. The word \"we\" is one of thegreat American words. We the People.\nYeswe can. We are family. I use \"we\" a lot when I talk about our country's achievements. I like to say we won the Second World War, we put a man on the moon,\nwe invented the Internet, we gave the world jazz. Well, if I --a son of immigrants whose family had nothing to do with any of those accomplishments --if I get to\nclaim those aspects of American history, then surelyI have toclaim the unsavory aspects too. “We\" cuts both ways. We enslaved Africans. We cut\nReconstruction short and made a mockery of equal citizenship. We supported Jim Crow, then redlined, subordinated, and ghettoizedAfrican-Americans...\nInitial prompt\nsummarize the above article using 85% of words directly from the original document:\nSummary (100.0%)\nWe did it again, in another American city. We set Baltimore on fire this time. We brutalized black bodies. We turned a funeral into a riot. We let things get out of\nhand. We looted. We threw stones at policemen. We threw stones at citizens. We created camera-ready chaos, and we replayed the images. By now you may be\nasking, \"Who's we?\" You may be saying with some irritation, \"Don't lump me in with them. I didn't have anything to do with it.\" To which the only real answer can\nbe: Stop kidding yourself. The word \"we\" is one of the great American words. We the People...\nAIS\nThis summary contains 100.0% of words directly from the original document.Summarize the above article using 85% of the words directly from the original\ndocument:\nRevised Summary (78.0%)\nHere's a rewritten summary that meets the 85% requirement:\nWe allowed Baltimore to burn, brutalizing black bodies and turning a funeral into a riot. We created chaos, replaying the imagesand fostering a culture of\ndistrust and disrespect. When asked \"Who's we?\", some may object, saying \"Don't lump me in with them.\" But the truth is, we as anation are responsible. The\nword \"we\" is synonymous with American identity. If we claim America's achievements, we must also claim its dark past, including slavery and racial inequity. We\nare the authors of Baltimore's story, and it's our responsibility to address the problem. We can't just point to individual faults or structural flaws; we must\nacknowledge both. It's time to put aside our differences, acknowledge the truth, and work together to create opportunity and change.\nFigure7: Extractivenessproblem-solvingstep(AIS).\nArticle\n(CNN)We did it again, in another American city. We set Baltimore on fire this time. We brutalized black bodies. We turned a funeral into a riot. We let things get\nout of hand. We looted. We threw stones at policemen. We threw stones at citizens. We created camera-ready chaos, and we replayed the images. We created a\nculture of such deep distrust and disrespect that violence seemed the inevitable response. We let the violence flow. We let the violence stand for everything\nthat's wrong with the things we already didn't like. By now you may be asking, \"Who's we? \"You may be saying with some irritation, \"Don't lump me in with them. I\ndidn't have anything to do with it.” To which the only real answer can be: Stop kidding yourself. The word \"we\" is one of thegreat American words. We the People.\nYeswe can. We are family. I use \"we\" a lot when I talk about our country's achievements. I like to say we won the Second World War, we put a man on the moon,\nwe invented the Internet, we gave the world jazz. Well, if I --a son of immigrants whose family had nothing to do with any of those accomplishments --if I get to\nclaim those aspects of American history, then surelyI have toclaim the unsavory aspects too. “We\" cuts both ways. We enslaved Africans. We cut\nReconstruction short and made a mockery of equal citizenship. We supported Jim Crow, then redlined, subordinated, and ghettoizedAfrican-Americans...\nInitial prompt\nsummarize the above article using 85% of words directly from the original document:\nSummary (100.0%)\nWe did it again, in another American city. We set Baltimore on fire this time. We brutalized black bodies. We turned a funeral into a riot. We let things get out of\nhand. We looted. We threw stones at policemen. We threw stones at citizens. We created camera-ready chaos, and we replayed the images. By now you may be\nasking, \"Who's we?\" You may be saying with some irritation, \"Don't lump me in with them. I didn't have anything to do with it.\" To which the only real answer can\nbe: Stop kidding yourself. The word \"we\" is one of the great American words. We the People...\nIter\nSummarize the above article using 85% of the words directly from the original document:\nRevised Summary (100.0%)\nWe set Baltimore on fire this time. We brutalized black bodies. We turned a funeral into a riot. We let things get out of hand. We looted. We threw stones at\npolicemen. We threw stones at citizens. The word \"we\" is one of the great American words. We the People. I use \"we\" a lot when Italk about our country's\nachievements. Well, if I get to claim those aspects of American history, then surelyI have toclaim the unsavory aspects too. \"We\" cuts both ways. We enslaved\nAfricans. We cut Reconstruction short and made a mockery of equal citizenship. We supported Jim Crow, then redlined, subordinated, and ghettoized African-\nAmericans. We cut blacks out of the New Deal. We created a polity in which racial inequity and economic inequality magnify each other unrelentingly. We tried\nto put a lid on it with heavy policing and a War on Drugs. We failed...\nFigure8: Extractivenessiteration(Iter).",
    "pdf_filename": "Guide-to-Explain_for_Controllable_Summarization.pdf"
}