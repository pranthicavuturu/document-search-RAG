{
    "title": "Brain-inspired Computing Based on Deep Learning for Human-computer Interaction A Review",
    "abstract": "",
    "body": "Brain-inspired Computing Based on Deep Learning for\nHuman-computer Interaction: A Review\nBihui Yua,b, Sibo Zhanga,b,∗, Lili Zhouc, Jingxuan Weia,b, Linzhuang Suna,b and Liping Bua,b\naShenyang Institute of Computing Technology, Chinese Academy of Sciences, Shenyang, 110168, China\nbUniversity of Chinese Academy of Sciences, Beijing, 100049, China\ncHeilongjiang Academy of Sciences Intelligent Manufacturing Institute, Harbin, 150090, China\nA R T I C L E I N F O\nKeywords:\nBrain-inspired computing\nHuman-computer interaction\nMachine learning\nDeep learning\nBiomedical research\nA B S T R A C T\nThe continuous development of artificial intelligence has a profound impact on biomedicine and other\nfields, providing new research ideas and technical methods. Brain-inspired computing is an important\nintersection between multimodal technology and biomedical field. Focusing on the application scenarios\nof decoding text and speech from brain signals in human-computer interaction, this paper presents a\ncomprehensive review of the brain-inspired computing models based on deep learning (DL), tracking its\nevolution, application value, challenges and potential research trends. We first reviews its basic concepts\nand development history, and divides its evolution into two stages: recent machine learning and current\ndeep learning, emphasizing the importance of each stage in the research of brain-inspired computing\nfor human-computer interaction. In addition, the latest progress of deep learning in different tasks of\nbrain-inspired computing for human-computer interaction is reviewed from five perspectives, including\ndatasets and different brain signals, and the application of key technologies in the model is elaborated in\ndetail. Despite significant advances in brain-inspired computational models, challenges remain to fully\nexploit their capabilities, and we provide insights into possible directions for future academic research.\nFor more detailed information, please visit our GitHub page:https://github.com/ultracoolHub/brain-\ninspired-computing.\n1. INTRODUCTION\nBrain-inspired intelligence is a kind of machine intelli-\ngence which is inspired by neural mechanism and cognitive\nbehavior mechanism by means of computational modeling\nand realized by software and hardware cooperation. Brain-\ninspired intelligence system is brain-inspired in information\nprocessing mechanism and human-like in cognitive behavior\nand intelligence level. Human brain activity is a complex and\ncontinuous dynamic process, and its complexity is far beyond\nthe upper limit that can be simulated by current computing\nresources, so people have not given up the exploration of\nthe brain. Brain-inspired computing is founded upon the\nstructural framework and operational principles of the human\nbrain, and integrates the current computational development\npath of computer science and neuroscience[1, 2, 3, 4].\nResearchers are constantly trying to understand the neural\nmechanisms and cognitive behavior through the study of the\nbrain. Traditional DL necessitates an extensive collection\nof annotated datasets for effective training, and manual\nannotated data is expensive and affected by human subjective\nconsciousness, so the annotation results are not completely\naccurate. In contrast, the brain weighs about 2.5 pounds\nand consumes only about 40% to 60% of the body’s blood\nsugar[5]. If people can use their own physiological data to\ndecode text or speech and complete codec tasks similar to\nmachine translation, it can not only save manpower, but\nAll authors contributed equally to this work.\n∗Corresponding author.\nyubihui@sict.ac.cn (B. Yu); zhangsibo22@mails.ucas.edu.cn (S.\nZhang); Zhoulilivip@126.com (L. Zhou); weijingxuan20@mails.ucas.edu.cn\n(J. Wei); sunlinzhuang21@mails.ucas.ac.cn (L. Sun); buliping@sict.ac.cn\n(L. Bu)\nalso have important significance and value for the cognitive\nmechanism and cognitive ability of the human brain.\nThe continuous exploration of the brain forms the basis of\nbrain-inspired computing, which not only draws inspiration\nfrom the complex structure and operation principle of the\nbrain, but also focuses on the innovative use of physiological\ndata of brain signals for practical applications. Brain-inspired\ncomputing models can be applied in human-computer in-\nteraction(HCI). And a typical application is brain-computer\ninterface(BCI) which aims to establish a channel between\nthe brain and the external environment, which does not\ndepend on the peripheral nervous system, and realizes the\ninformation exchange and control between the brain and\nexternal devices with processing or computing capabilities.\nThrough the acquisition, analysis and processing of brain\nsignals, human can directly interact with computers without\nrelying on external devices. For example, the main goal of\ncognitive BCI is to understand and analyze the processing\nmechanism of speech information in human brain. It can help\npatients overcome the difficulties in language expression and\ntext input by recognizing the ideas expressed by brain signals\nand converting them into speech or text output[47].\nAs an emerging research field, the exact definition of\nbrain-inspired computing is still unclear. This field covers\ncomputational theory, architecture design, hardware specifica-\ntions, etc., while learning from brain information processing\nmechanisms and biological physiological structures to build a\nvariety of models and algorithms[48]. Some comprehensive\nstudies systematically review the research progress in the\nfield of brain-inspired computing from multiple perspectives.\nSome studies focus on brain-inspired computing in a narrow\nsense, such as spike-based neural mimic computing in vision\nB.H.Yu et al.\nPage 1 of 26\narXiv:2312.07213v4  [cs.AI]  19 Nov 2024\n\nPaper\nStructure\nEvolution of\nBrain-inspired Computing\nfor HCI\nRecent Models\nBased on\nMachine Learning\nClassification\nNB[6], SVM[7], KNN[8],\nLR[9], RF[10], LDA[11],\nFeature Extraction\nPCA[12], AR[13], FFT[14], WT[15]\nCurrent Models\nBased on\nDeep Learning\nEarly DL Model\nCNN[16],RNN[17, 18]\nPretrained Model\nBERT[19],BART[20, 21, 22]\nApplication of\nDeep Learning-based\nBrain-inspired Computing\nModels for HCI Tasks\nData aquisition\nand preprocessing\nData Acquisition\n1. Experimental Design.\n2. EEG Acquisition.\n3. FMRI Acquisition.\n4. Eye-tracking Acquisition.\nData Preprocessing\n1. Filter processing.\n2. Artifact removal.\n3. Reference reset.\n4. Spatial correction.\n5. Time-frequency analysis.\nPublic Dataset\nZuCo[23], ZuCo2.0[24],\nScience[25], NC[26]\nModels based\non EEG\nRecent Progress\nAutomatic Speech Recognition[27],\nEEGNet[28],\nEEG-transformer[29],\nEEG-to-text[20],\nCSCL[21], DeWave[22]\nKey Technologies\n1. EEG-to-text Decoding.\n2. Curriculum Learning.\n3. Discrete Codex.\nModels based\non fMRI\nRecent Progress\nContext Into Language[30],\nBrain2word[31],\nmulti-timescale models[32],\nSemantic-based Classification[33],\nSelf-supervised Learning[34],\nCross-madal Cloze Task[19],\nUniCoRN[35],\nSemantic Reconstruction[36]\nKey Technologies\n1. Brain Decoding Model.\n2. Semantic Features Fusion.\nModels based\non MEG\nRecent Progress\nSeq2seq Learning[37],\nDecoding Speech[38],\nDecoding Imagined and Spoken Phrases[39],\nMEG Sensor Selection[40],\nDecoding Speech[41]\nKey Technologies\n1. CNNs And Transfer Learning.\n2. Signal Alignment.\nModels based\non ECoG\nRecent Progress\nMachine Translation[42],\nBrain2char[43],\nNeural Speech Decoding[44],\nDirect Speech Reconstruction[45],\nSynthesizing Speech[46]\nKey Technologies\n1. Decoding Approach.\n2. Neural Vocoder.\nChallenges and\nFuture Directions of\nDeep Learning-based\nBrain-inspired Computing\nModels for HCI Tasks\nChallenges\nModel Training\n1. Challenges for Brain Dataset.\n2. Computational Resource Demand.\n3. Acuracy and Generalization of the Model.\nModel Application\n4. Challenges for Real-time Processing.\n5. Challenges for BCI Technology.\nEthical Challenges\n6. Challenges for Ethical Concerns.\nFuture Directions\n1. Landing Application of Brain-inspired Computing.\n2. BCIs across Language Boundaries.\n3. Spiking Nerual Network(SNN).\nFigure 1: An introduction to the deep learning-based brain-inspired computing models for HCI tasks, including its history, currently\navailable methods and techniques, as well as the challenges encountered and potential solutions, and the direction of future\ndevelopment.\nB.H.Yu et al.\nPage 2 of 26\n\napplications[49]. At the same time, several studies have com-\nprehensively reviewed hybrid neural networks, highlighting\ntheir comprehensive balance with respect to artificial neural\nnetworks and spiking neural networks[50]. There are also\nstudies that explore cognitive engineering approaches, aiming\nto assist the research community in designing more consistent\nmethods and techniques to advance cognitive machines[51].\nIn addition, some reviews focus on the hardware design\nof brain-inspired computing architectures[52]. On the other\nhand, some reviews start from the challenges faced by the\nfield of brain-inspired computing and propose a general\nframework for brain-inspired computing systems in practical\napplications[53]. There are also some research from the\nperspective of the application of BCI systems, but only\nsummarize the signal processing and calculation methods\nrelated to EEG signal[54, 55].\nThese reviews provide valuable insights into the progress\nof brain-inspired computational models. Inspired by these\nreviews, this paper deeply investigates the development of\nbrain-inspired computing models from the perspective of\nmachine learning(ML) and deep learning(DL), focusing on\nthe brain-inspired computing models that decode text or\nspeech through brain signals in human-computer interaction.\nThis paper summarizes the application scenarios and lim-\nitations of different machine learning algorithms in brain-\ninspired computing models, and comprehensively reviews\nthe deep learning related models of brain-inspired computing\ninvolving EEG, fMRI, MEG, ECoG in recent years. At the\nsame time, the application of key technologies in these models\nis described in detail. This paper aims to summarize the\nenlightenment of artificial neural networks in brain-inspired\ncomputing models and algorithms in human-computer inter-\naction scenarios from the perspective of generalized brain-\ninspired computing, expecting to fill the gap in the review\nof brain-inspired computing models for human-computer\ninteraction based on deep learning. The contributions of this\nreview can be summarized in three main points:\n• To enhance understanding of deep learning-based\nbrain-inspired computing models for human-computer\ninteraction, we first introduce the fundamental con-\ncepts. Then, we review recent advancements in brain-\ninspired computing within machine learning and deep\nlearning, highlighting the critical role of machine\nlearning in feature extraction and classification, and\nleading to the primary focus on deep learning-based\nbrain-inspired computing models.\n• After reviewing the progress of the research, we\ndivide the tasks related to brain-inspired computing for\nhuman-computer interaction based on deep learning\nmodels into five parts, four of which introduce the\nprogress based on different brain signals, emphasize\nthe importance of experimental stimulus design and\ndata acquisition and preprocessing, and elaborate the\napplication of key technologies in the model.\n• In addition, we highlight the challenges faced by brain-\ninspired computational models based on deep learning\nfrom three aspects: model training, model application,\nand ethical issues, and propose potential solutions.\nWe look forward to future applications such as cross-\nlingual BCI and spiking neural network with greater\nbiological plausibility.\nThis paper will organize the rest following the structure\nshown in Figure 1: we will first delve into the basic concepts\nand development history related to brain-inspired computing,\nbrain-computer interfaces, brain signals and so on. In section\n2 we will promote a deeper understanding of this research\narea. Then, in section 3, this paper reviews the research results,\nprogress and some key technologies of researchers using\ndeep learning models and algorithms to study brain signal\ndata through a series of specific case studies to illustrate the\npractical effects. In section 4, we will deeply explore the\nchallenges and limitations of DL models in brain-inspired\ncomputing for human-computer interaction tasks, such as\ndata accuracy requirements and computational resource\nrequirements, as well as the limitations and security risks\nof BCI systems. Future research directions such as SNN and\ncross-lingual BCI systems are discussed in section 5. This\npaper concludes the entire discussion in section 6.\n2. BASIC CONCEPTS AND EVOLUTION\nThis section mainly expounds the related concepts of\nbrain-inspired computing, brain-computer interface and pre-\ntraining model, and focuses on the common algorithms of\nmachine learning in brain-inspired computing for human-\ncomputer interaction tasks, as well as the important applica-\ntions of deep learning models.\n2.1. Basic Concepts of Brain-inspired Computing\nfor HCI\nBrain-inspired computing commonly encompasses com-\nputational theories, architectural designs, hardware spec-\nifications, and various models and algorithms that draw\ninspiration from the information processing mechanisms\nand biophysiological structures of the brain[48]. This review\nstudies brain-inspired computing from a broad perspective.\nLearning from the structure and working principle of the\nbrain, but not limited to the simulation of the brain, but\nalso including the integration of traditional artificial neural\nnetwork with more brain-inspired characteristics of heteroge-\nneous neural networks, is an integration of current computer\nscience and neuroscience Computing development path. This\npaper focuses on the brain-inspired computing models and\nalgorithms for decoding text and speech from brain signals\nin HCI tasks through ML and DL.\nIn order to better understand the task of brain signal\ndecoding, we will briefly discuss the various brain signals that\nare commonly used, including electroencephalogram(EEG),\nfunctional magnetic resonance imaging(fMRI), Magnetoen-\ncephalography(MEG) and electrocorticography(ECoG).\nEEG, a non-invasive method for measuring brain activity,\nencapsulates a rich repository of information regarding the\nintricate workings of the brain, rendering it an appealing\nB.H.Yu et al.\nPage 3 of 26\n\ndata source for applications in DL. The advancement of DL\ntechniques in the analysis of EEG features holds tremen-\ndous potential for the development of novel diagnostic and\ntherapeutic tools within the realm of neurological disorders.\nTypically, EEG signals are acquired through the placement\nof electrodes on the scalp and are represented as a two-\ndimensional matrix or graph. This representation unfolds\nin time as one dimension and spatially in terms of electrode\nlocations as another. Researchers in the field commonly treat\nmultiple channels as the spatial dimension of EEG data,\naligning with the temporal dimension[56, 57].\nFMRI is a another powerful non-invasive tool for studying\nbrain function[58]. Used for psychologists, psychiatrists, and\nneurologists, fMRI provides high-quality visualizations of\nbrain activity in response to sensory stimulation or cognitive\nfunctions. Thus, it facilitates the study of the intricate\nworkings of a healthy brain[20, 58].The objective of fMRI\nanalysis is to robustly, sensitively, and validly detect the\nregions of the brain that exhibit heightened intensity during\nthe moments when stimulation is applied[21].\nMEG is a non-invasive neuroimaging technique used to\nrecord neural activity in the brain. It reflects brain activity\nby measuring the weak magnetic fields generated by neurons\nin the brain. These magnetic fields are able to penetrate\nthe skull and tissue without being affected by them, thus\nproviding EEG signals with high spatio-temporal resolution.\nIn MEG, an array of induction coils is placed around the\npatient’s head to detect and record changes in the magnetic\nfield caused by neuronal activity. Because MEG measures the\nmagnetic fields generated within the brain by nerve currents,\nrather than the electrical potentials (such as EEG) on the\nscalp surface, it provides a more precise spatial resolution.\nMEG is often used to study functional localization of the\nbrain, neuroplasticity, and brain activity associated with\nneurological diseases. Its non-invasive nature makes it an\nimportant tool for studying brain function and abnormal\nactivity. MEG is increasingly used in neuroscience, clinical\nmedicine and cognitive research, providing a powerful means\nto deeply understand the complex functions of the brain.\nThe three non-invasive methods described earlier, as a\ncontrast, ECoG is a technique that records electrical activity\nin the brain, but electrodes are implanted directly on the\nsurface of the patient’s brain rather than placed on the scalp.\nBy implanting an array of electrodes on the surface of\nthe brain[42], ECoG is able to provide EEG signals with\nhigher spatio-temporal resolution. This method is commonly\nused to study epilepsy, neuroplasticity, and brain function\nlocalization. Compared to traditional electroencephalograms,\nECoG provides a finer spatial resolution, allowing researchers\nto understand the electrical activity of specific brain regions\nin greater detail.\nThe above summarizes the commonly used brain signals.\nThe improvement of deep learning algorithms, especially the\ndevelopment of pre-trained models, has promoted the perfor-\nmance improvement of brain signal decoding tasks in the field\nof brain-inspired computing. Most methods of processing\nbrain physiological data with deep learning techniques are\ncombined with the currently hot pre-trained language model\n(PLM) which has undergone a series of evolution. As an early\ninitiative, ELMo[59] aimed to capture context-aware word\nrepresentations through the pre-training of a bidirectional\nlong short-term memory network (biLSTM). Unlike learning\nfixed word representations, ELMo’s approach involves fine-\ntuning the biLSTM network for specific downstream tasks.\nSubsequently, drawing on the highly parallelized transformer\narchitecture[60] and the self-attention mechanism, BERT[61]\nwas introduced. BERT entails pre-training two-way language\nmodels and specifically designed pre-training tasks using\na large-scale unlabeled corpus. These pre-trained, context-\naware word representations serve as highly effective, generic\nsemantic features, significantly elevating the performance\nstandard for a wide range of natural language processing tasks.\nThis study has spurred a substantial amount of subsequent\nresearch, establishing a prevalent \"pre-train and fine-tune\"\nlearning paradigm. Following this paradigm, a great deal\nof research has been done on PLMs, introducing different\narchitectures such as generative models GPT-2[62] and\nBART[63], in which PLM often needs to be fine-tuned to\nsuit different downstream tasks.\n2.2. The Evolution of Brain-inspired Computing\nfor HCI\nWith the rapid development of artificial intelligence\nmodels, brain-inspired computing models for HCI have also\nattracted wide attention. Brain-inspired computing tasks\nthat can be completed are increasingly complex from early\nmachine learning models to large-scale deep learning models\nwith hundreds of billions of parameters.\n2.2.1. Recent Models Based on Machine Learning\nIn recent years, the use of machine learning techniques\nto analyze brain signals has attracted a lot of attention. For\nexample, there is growing research evidence that machine\nlearning can extract meaningful information from high-\ndimensional and noisy EEG signals. Given the interest and\nwidespread use of the technology, this section focuses on\nrecent examples of researchers using machine learning to\nanalyze brain signals to build brain-inspired computational\nmodels, including machine learning methods and related\napplications. As shown in Table 1, two main applications\nof machine learning methods in the field of brain-inspired\ncomputing at different stages, namely classification and\nfeature extraction, are shown, and common machine learning\nalgorithms are listed.\nAmong the applications of brain-inspired computing for\nhuman-computer interaction, brain-computer interface is the\nmost widespread one. Brain-computer interface technology\nestablishes a connection between the human brain and\nthe computer and its external equipment for information\nexchange. As shown in Figure 2, a complete brain-computer\ninterface system generally includes three modules of brain sig-\nnal acquisition, data processing and BCI application, in which\ndifferent signals such as EEG, fMRI, MEG and ECoG can\nbe selected according to the actual situation. Data processing\nB.H.Yu et al.\nPage 4 of 26\n\nTable 1\nMachine learning algorithms used in brain-inspired computing tasks.\nTask\nStage Category\nRepresentative Work\nModel Based on Machine Learning\nClassification\nNB, [6]\nSVM, [7]\nKNN, [8]\nLR, [9]\nRF, [10]\nLDA, [11]\nFeature Extraction\nPCA, [12]\nAR, [13]\nFFT, [14]\nWT, [15]\nFigure 2: Diagram of the BCI system module.\nwill combine feature extraction algorithms and classification\nalgorithms to further enhance the collected brain signals,\nand then realize information exchange with external devices\nthrough brain computer interface applications[64, 65]. The\ntypical applications include SSVEP[66, 67], P300[68, 69],\nMI[70, 71] and some other human-computer interaction\napplication scenarios.\nAmong the above mentioned data processing algorithms,\nmachine learning algorithms are used to solve the problem\nin the earliest brain-inspired computing models.\nOn the one hand, machine learning algorithms are com-\nmonly employed in various traditional classification ap-\nproaches, encompassing both supervised and unsupervised\nlearning. Among these, supervised algorithms stand out as the\nmost renowned method in EEG data analysis[72]. It mainly\nincludes naive bayes (NB), support vector machine (SVM),\nk-nearest neighbor (KNN), logistic regression (LR), random\nforest (RF) and linear discriminant analysis(LDA). Each\nsupervised model employs a learning algorithm to produce a\nmore accurate model[73, 74]. These methods are often used\nto complete classification tasks in brain-inspired computing.\nFor example, NB is a probabilistic classifier that can be\napplied to EEG signal analysis. It only needs a small amount\nof training data set to classify data according to certain\nfeatures by Bayes’ theorem[75, 76], so it plays a certain\nadvantage in BCI system[77, 78]. The tasks of applying NB\ninclude emotion recognition[79], epilepsy detection[80] and\nmotor imagery[81, 82, 83, 84, 85]. However, NB has certain\nlimitations, under its basic assumption, all properties are\nindependent of each other, and the eigenvector has the same\neffect on the result[76].\nNext, SVM is widely used in EEG classification and BCI\nsystems[86, 87, 88, 89, 90]. SVM can effectively segregate\ntwo datasets, whether in a linear or non-linear fashion. For\nlinear separation, SVM leverages discriminant hyperplanes to\ndelineate classes, while for non-linear separation, it employs\nkernel functions to discern decision boundaries. Compared\nwith KNN and other supervised algorithms, SVM has lower\nB.H.Yu et al.\nPage 5 of 26\n\ncomputational complexity[91, 92]. SVM have found exten-\nsive application in EEG signal classification owing to their\nsimplicity and versatility in addressing classification chal-\nlenges, including the diagnosis of brain diseases[93, 94, 95].\nIn addition, SVM can also be used for other tasks, such as\nrobot control[96] and emotion classification[97]. However,\nSVM performance is affected by kernel function and penalty\ncoefficient parameters. Therefore, it is important to optimize\nthe parameters introduced into the SVM classifier[98].\nFurthermore, the basic idea of KNN algorithm is that\nin feature space, if most of the neighbors of an instance\nbelong to a certain class, then the instance also belongs\nto that class[8]. KNN algorithm has strong interpretability,\neasy implementation, and no parameter adjustment, only\nneed to select a suitable K value, so it is also used in\nthe classification task of BCI system[99, 100, 101, 102].\nHowever, KNN algorithm also has some limitations. The\nalgorithm is required to compute the distance between each\nsample and all training samples, so it requires a large amount\nof computation on large-scale data sets, and KNN algorithm\nis sensitive to noisy data, which may be greatly affected, re-\nsulting in inaccurate prediction results. Furthermore, despite\nthe decrease in computational complexity with an increase\nin the value of k for KNN, its classification performance also\ndiminishes[91, 103].\nNext, LR algorithm is a common classification algorithm[9],\nits core idea is to use logical functions to build a linear model\nto model the relationship between input features and output\nlabels. Logical functions such as sigmoid can map any real\nnumber to the interval [0,1] and can therefore be used to\nrepresent probabilities. In order to determine the parameters\nof the model, it is necessary to define a loss function to\nmeasure the degree of fit of the model. LR algorithm is\nsuitable for binary classification problems and has strong\ninterpretability. Thus, LR algorithms are commonly used in\nclassification tasks such as EEG classification[104, 105, 106]\nand BCI systems[107, 108, 109]. However, it also has some\nlimitations, is susceptible to outliers and redundant features,\nand is less effective when dealing with multiple classification\nproblems.\nMoreover, RF is a model composed of multiple decision\ntrees, each trained with a distinct subset of data and features.\nIt randomly selects data and features, and then integrates the\nprediction results of each tree to make the final prediction,\neffectively reducing overfitting and improving the prediction\naccuracy[110]. In solving classification problems, RF’s par-\nallel structure has better performance than other supervised\nalgorithms in processing large EEG datasets[82, 111, 112,\n113, 114]. In addition, it is used for image classification[115],\nbrain tumor detection[116] and some other BCI tasks[117].\nHowever, over-fitting and instability of trees can affect the\nperformance of RF models, especially for trees of different\nsizes[76].\nLastly, LDA is a supervised learning method designed\nto identify features that effectively differentiate between\ndifferent classes. Its fundamental concept involves max-\nimizing inter-class distance while minimizing intra-class\ndistance. By projecting data into a low-dimensional space,\nit finds the hyperplane that can best distinguish between\ndifferent categories, thus achieving dimensionality reduction\nwhile maximizing classification accuracy[118, 119, 120].\nLDA has proven successful in addressing classification\nchallenges within BCI systems[121, 122, 123, 124], such\nas motor imagery[85, 125], P300 spellers[126], brain states\ndecoding[127], and multi-class BCI[128], attributed to its\nstraightforward usability and minimal computational de-\nmands. Nevertheless, the primary limitation of this model\nlies in its linear nature, rendering it unsuitable for application\nto non-linear EEG data[103, 129].\nOn the other hand, in addition to the above classi-\nfication task algorithms, machine learning also plays an\nimportant role in the stage of feature extraction, several\nfeature extraction algorithms based on machine learning are\nreviewed next, including principal component analysis(PCA),\nautoregressive(AR), fast fourier transform(FFT) and wavelet\ntransform(WT).\nFirstly, the fundamental concept of the PCA algorithm is\nto transform high-dimensional data into a lower-dimensional\nspace, preserving the main features of the data, while re-\nmoving redundancy and correlation[12]. The PCA algorithm\nachieves dimensionality reduction through linear transfor-\nmation, making the data easier to visualize and analyze,\nwhile improving the performance and efficiency of machine\nlearning models. Therefore, the algorithm is suitable for a\nvariety of scenarios and can simplify complex problems. In\nbrain-inspired models, PCA algorithm is used for feature\nextraction[130, 131, 132, 133]. However, PCA also has some\nlimitations, for example, it is sensitive to noise and outliers,\nwhich may lead to the deviation of dimensionality reduction\nresults from the real situation. In addition, PCA ignores\nthe correlation between samples and does not consider the\nimportance of features, which may affect its comprehensive\ndescription of the data. In addition, its interpretation is poor.\nSecondly, AR is a prediction algorithm based on time\nseries, the basic idea is to use the observed value of the past\ntime step to predict the current observation value, that is, by\nestablishing a linear model, the data of the current moment\nand the data of the past moment are linearly combined, so as\nto obtain the data of the future moment. At the heart of the\nAR model is the autoregressive coefficient, which represents\nthe relationship between the data at the current moment and\nthe data at the past moment. The autoregressive coefficient\ncan be estimated by least square method to obtain an optimal\nautoregressive model[13]. AR models capture trends and\nhistorical dependencies in time series data. The application\nof AR algorithm includes feature extraction tasks[134], motor\nimagery tasks[135] and other BCI tasks[136, 137, 138].\nHowever, AR model also has some limitations, it can only\ncapture the autoregressive relationship, can not capture the\nmoving average relationship. The AR model ignores the error\nterm of the past time step and may not capture the moving\naverage in the data. For some time series data, the AR model\nmay require a higher order to fit the data well, resulting in an\nincrease in model complexity.\nB.H.Yu et al.\nPage 6 of 26\n\nMoreover, FFT algorithm reduces the computational\ncomplexity of discrete fourier transform(DFT)[139] algo-\nrithm and makes the processing of large-scale data efficient\nand feasible. Especially for large data sets. This makes\nit an important tool in signal processing, communication\nsystems and other fields[14]. Applications in BCI systems\nsuch as game controlling[140], feature extraction[141, 142]\nare commonly used. However, the FFT algorithm demands\na substantial amount of memory space for both data storage\nand result computation. The accuracy of FFT algorithm may\ndecrease for data with non-uniform distribution or noisy data.\nLastly, WT builds upon and enhances the concept of short-\ntime Fourier transform localization, addressing drawbacks\nlike the fixed window size that doesn’t adjust with frequency\nchanges. It systematically conducts multi-scale refinement\nof the signal through successive telescopic translation opera-\ntions. Consequently, the time subdivision at high frequency\nand frequency subdivision at low frequency dynamically\nconform to the demands of time-frequency signal analysis,\nallowing for focused attention on any signal details. WT can\nanalyze information of different frequency components on\nthe same time scale which can adapt to focus on arbitrary\ndetails of the signal which is particularly useful for analyzing\nnon-stationary signals and abrupt signals[15]. Thus, WT is\ncommonly used in feature extraction[143, 144, 145, 146] and\nother BCI tasks such as SSVEP[147, 148, 149, 150, 151].\nHowever, the computational complexity of the wavelet\ntransform is higher than that of the Fourier transform, and the\ncomputation time may be longer for large data sets. Because\nthe boundary of the signal cannot fully satisfy the periodicity\ncondition, there will be an end effect, which may affect the\naccuracy of the transformation result, and it is necessary to\nselect a suitable wavelet base.\nIn general, the brain-inspired computing models and\nalgorithms established by machine learning generally have\ncertain limitations. Firstly, the data requirements are high,\nand the data quality and data quantity have a great impact\non the performance of the model. In addition, due to the\ncharacteristics of some methods themselves, such as LDA as\na representative of linear classification algorithms can not\nsolve the nonlinear data classification problem. LR can well\nsolve the binary classification problem, but can not solve the\nneeds of brain-inspired computing model multi-classification\nproblem. KNN, FFT and other algorithms are sensitive to data\nnoise, the brain signal acquisition itself is easy to be disturbed\nby the surrounding environment and produce noise. When\nthese machine learning methods are only used to solve brain-\ninspired computing problems, the effect is often not ideal\nand the application scenarios are limited. The emergence\nand rapid development of deep learning algorithms has also\npromoted the progress of brain-inspired computing models.\nThe advantages of machine learning algorithms in data\npreprocessing and the advantages of deep learning algorithms\nin modeling and decoding can be comprehensively used to\nbetter deal with brain-inspired computing problems in various\nHCI application scenarios.\n2.2.2. Current Models Based on Deep Learning\nIn recent years, deep learning technology has attracted a\nlot of attention in several research fields, and can be used to\nprovide a novel solution for learning stable representations in\nthe physiological signals of the brain. The machine learning\ntechniques reviewed earlier can extract valid task-relevant\ninformation from signals. Signal classification plays an\nimportant role in research tasks and has been applied to\nmany signal control tasks. Although great progress has been\nmade in this regard, there remains considerable potential\nfor enhancing classification accuracy, and there is also a lot\nof room for exploration in the application of other tasks.\nHence, as a burgeoning frontier in the realm of machine\nlearning, deep learning has garnered the interest of numerous\nresearchers in the field of brain science.\nBased on the Transformer[60] architecture, BERT[61]\nlearns contextual information forward and backward, en-\nabling the model to better understand the meaning of words\nin sentences. Pre-trained on large-scale text, BERT learns\na common language representation that can then be fine-\ntuned for various NLP tasks, such as text classification and\nnamed entity recognition. BERT model has been applied to\nbrain-inspired computing tasks. For example, the cross-modal\ncloze task treats the decoding task as a fusion of a direct\nclassification task and a cloze task[19]. In natural language\nprocessing, cloze tasks have been effectively addressed by\nBERT, a pre-trained masking language model capable of\nrandomly masking certain words in the input and subse-\nquently predicting the masked words based on context[19].\nBy classifying brain images with context into words from\na large number of words, a given context should provide\nadditional information to predict words by narrowing down\npossible candidates.\nBART[63] performs particularly well in the current deep\nlearning pre-training models that combine brain-inspired\ncomputing fields. It is unique in its bidirectional and au-\ntoregressive learning style. Through a sequence-to-sequence\ntraining approach, BART introduces a de-noising mechanism\nthat allows the model to capture global text information\nby masking parts of the words of the input sequence and\nattempting to restore them. This design allows BART to excel\nin generative tasks such as text summarization and machine\ntranslation. Wang et al.[20] took the lead in decoding open\ndomain words for the first time based on ZuCo data set, and\nthe vocabulary size expanded from about 250 to about 50000.\nThe team of HIT[21] took into account the gap between topic\nand semantic on the basis of [20], and then retrained the EEG\ndata to narrow the gap between topics. The semantic features\nof the data are more complete, and the SOTA performance at\nthat time is achieved. The main innovation of DeWave[22] is\nthat it does not use eye movement dataset which believes\nthat people do not necessarily read words in order, and\neven the segmentation using eye movement data is not the\ncorresponding segmentation of data. Therefore, it is the first\ntime to try to realize the direct translation of raw brainwave\nto text, although the translation effect is not as good as the\nB.H.Yu et al.\nPage 7 of 26\n\nTable 2\nProgress in deep learning models based brain-inspired computing tasks.\nTask\nMethod Category\nRepresentative Work\nModel Based on Deep Learning\nEEG\nAutomatic Speech Recognition, [27]\nEEGNet, [28]\nEEG-transformer, [29]\nEEG-to-text, [20]\nCSCL, [21]\nDeWave, [22]\nfMRI\nContext Into Language,[30]\nBrain2word, [31]\nmulti-timescale models, [32]\nSemantic-based Classification, [33]\nSelf-supervised Learning, [34]\nCross-modal cloze task, [19]\nUniCoRN, [35]\nSemantic Reconstruction, [36]\nMEG\nSeq2seq Learning,[37]\nDecoding Speech(Dash D,et al.),[38]\nDecoding Imagined and Spoken Phrases, [39]\nMEG Sensor Selection, [40]\nDecoding Speech(Défossez A, et al.), [41]\nECoG\nMachine Translation, [42]\nBrain2char, [43]\nNeural Speech Decoding, [44]\nDirect Speech Reconstruction, [45]\nSynthesizing Speech, [46]\ncombination of EEG data and eye-tracking data. But at least\nit proves that it works.\nIn this section, the machine learning algorithms employed\nin human-computer interaction tasks related to brain-inspired\ncomputing are segmented into two groups: feature extraction\nalgorithms and classification algorithms. Within each cate-\ngory, we explore the relevant scenarios and limitations to\nprovide a comprehensive understanding of their applicability.\n3. DEEP LEARNING MODELS FOR\nBRAIN-INSPIRED COMPUTING\nThis section will discuss in detail the recent and current\ncontributions and research achievements of brain-inspired\ncomputing models for HCI tasks based on deep learning\nwhich are classified and listed in Table 2 according to the\ndifferent brain signals processed. And we will elaborate the\napplication of key technologies in the model.\n3.1. Data Acquisition And Preprocessing\nThe acquisition of brain signal datasets often requires\nhigh cost and strict experimental conditions. Some high-\nquality public datasets provide great convenience for re-\nsearchers in the field of brain-inspired computing. This\nsection will focus on the construction of high-quality datasets\nfrom three aspects: data acquisition stage, data preprocessing\nstage and the public datasets.\n3.1.1. Data Acquisition\nIn the latest research in the field of brain signals, several\nkey techniques have been widely used. The quality of the\ndata is guaranteed and improved, which is convenient for\nthe relevant personnel of computer science to carry out\nsubsequent algorithm research, and the following is a brief\nintroduction to these key technologies:\nStimuli & experimental design: In general, event-\nrelated potential(ERP) experiments use stimuli that are best\nperceived as simple and clear. If the properties of the stimulus\n(especially the content, intensity, duration, or location of the\nstimulus) are not relevant to the purpose of the experimental\nstudy, they need to be appropriately controlled. The so-called\n\"control\" is to be as consistent as possible between each spur\nwithin the condition and between different conditions. If a\nparameter of the stimulus is not possible to be consistent\nin practice, the researcher should also try to reduce the\nvariability of the parameter within the condition, and the\ndifference between the conditions should be statistically\ntested to ensure that the difference is not significant.\nB.H.Yu et al.\nPage 8 of 26\n\nIn the field of psychology, the event-related potential tech-\nnology is often used to study, which is also a great inspiration\nfor the design of brain signal acquisition experiments[152].\nExperimental design, especially the design of experimental\nstimuli, greatly affects the quality of collected data, and\nfurther affects the algorithms of data preprocessing and\nthe modeling process of brain signal decoding. In order to\nimprove the strength of brain signals, researchers often design\nexperiments by combining internal stimuli and external\nstimuli. External stimuli such as visual, auditory, and tactile\nstimuli can further strengthen internal stimuli, especially for\ncognitive tasks of brain signal decoding and some emotional\nperception tasks. For example, when designing visual stimuli,\nattention should be paid to the center of visual materials\n(such as pictures) to reduce or even avoid the appearance\nof eye movement artifacts. If the visual materials are text\nstimuli, certain control problems can be considered to ensure\nthe accuracy of brain signals and enhance the strength of\nbrain signals. When designing auditory stimuli, the properties\nof the audio material itself such as pitch, loudness and\ndiscrimination acuity should be taken into account. When\ndesigning tactile stimuli, one can focus on the effects of the\nduration and frequency of the stimuli. Most of the public data\nsets have been verified by a large number of experiments,\nand strict experimental schemes are used to focus on the data\nset collection task itself, which can ensure the quality of the\ncollected data, but it may not be suitable for all experiments.\nWhen the researchers need to design their own experiments,\nwe hope that the above tips in terms of experimental design\ncan bring some inspiration and help.\nEye-tracking acquisition: Eye-tracking serves as an\nindirect indicator of cognitive engagement, as gaze patterns\nexhibit a strong association with the cognitive workload at\ndifferent stages of human text processing[153]. For instance,\nfixation duration tends to be prolonged for lengthy, uncom-\nmon, and unfamiliar words[154]. Each dataset encompasses\ndistinct eye-tracking features, with the most prevalent fea-\ntures, namely first fixation duration, first pass duration, mean\nfixation duration, total fixation duration[155], and number of\nfixations, consistently available across all seven datasets[56].\nEEG acquisition: Electroencephalography (EEG) serves\nas a means to record the intricate electrical activity of the\nbrain, capturing voltage fluctuations across the scalp with\nexceptional temporal precision. Hauk[156] has presented\ncompelling evidence supporting the modulation of early\nelectrophysiological brain responses influenced by word\nfrequency, highlighting the rapid nature of lexical access\ntriggered by written word stimuli within a timeframe of\nless than 200 ms post-stimulus presentation. The EEG\ndatasets employed in this study[155] were derived from\nsessions involving either the reading of sentences or the\nlistening to natural speech. The extraction of word-level\nbrain activity was achieved through diverse methodologies,\nincluding mapping to eyetracking cues[23], alignment with\nauditory triggers[157], capturing solely the terminal word\nin each sentence[158], or employing the method of serial\nword presentation[159]. Preprocessing of EEG data adhered\nto standardized procedures across all four data sources,\nencompassing essential steps such as band-pass filtering and\nartifact removal[56].\nfMRI acquisition: Functional magnetic resonance imag-\ning (fMRI) is a technique utilized to measure and map\nbrain activity by detecting changes correlated with blood\nflow[155]. With a temporal resolution of two seconds, fMRI\ncaptures alterations in blood flow, allowing one scan to\nencompass multiple words during continuous stimuli like\nnatural reading or story listening. The datasets usually include\nboth isolated stimuli[25] and continuous stimuli[160]. While\nit is comparatively simpler to extract word-level signals from\nisolated stimuli, continuous stimuli offer the advantage of\nextracting signals in context across a broader vocabulary.\nFMRI data consists of representations of neural activity\nwithin millimeter-sized cubes known as voxels. As part of\nthe standard fMRI preprocessing procedures prior to the\nanalysis, various preprocessing methods, including motion\ncorrection, slice timing correction, and co-registration[155],\nwere implemented[56].\nIn summary, the design of experimental stimuli has a\nprofound impact on the quality of the data set, and certain\nnorms should be followed, such as controlling irrelevant\nvariables and combining internal and external stimuli to\nenhance the stimulus signal. Data collection is the key step.\nAfter the original data collection is completed, it also goes\nthrough a series of preprocessing such as denoising and data\nsegmentation. Subsequent work will use the processed data\nto complete the model construction, encoding and decoding.\n3.1.2. Data Preprocessing\nData preprocessing is crucial in brain signal decoding,\nbecause it helps to remove noise and artifacts, better highlight\nsignal components related to specific brain activities, improve\nsignal quality and reliability, make subsequent feature extrac-\ntion and model training more accurate and effective, strong\ndecoding performance and robustness. Next, some common\npretreatment steps of brain signals such as filtering and\nartifact removal are summarized, and the effects of different\npretreatment steps on different brain signals are explained.\nFilter processing: Filter processing is the most basic\nstep in brain signal preprocessing, which is used to remove\nunwanted frequency components. EEG and ECoG utilize\nfiltering to remove unwanted low and high frequency signals,\nand high-pass filtering, typically at 0.1-1 Hz, is used to\neliminate low-frequency drift, such as slow voltage fluctu-\nations in time that can result from poor electrode contact\nor certain condition changes in the scalp[161]. Low-pass\nfiltering, typically between 30-100 Hz, is used to remove high-\nfrequency interference such as muscle activity while main-\ntaining frequency signals associated with brain activity. In\nparticular, band-stop filters (50Hz) are often used to eliminate\nfrequency interference from power cords, which often comes\nfrom laboratory equipment or external electronics[162]. The\nfiltering process in MEG is similar, using bandpass filtering\nto remove low and high frequency signals of no interest\nsuch as muscle electrical activity or external high-frequency\nB.H.Yu et al.\nPage 9 of 26\n\ninterference[163], while accurately filtering out power line\nfrequency noise to ensure the accuracy of the magnetic field\nsignal. fMRI uses high-pass filtering to remove very low-\nfrequency drifts, chronic drifts that are often affected by\nphysiological noise such as heart rate and breathing, as well\nas changes in device temperature[164]. The purpose of high-\npass filtering is to exclude non-signal-related low-frequency\nchanges to improve the stability of time series data.\nArtifact removal: Due to the interference of various\nphysiological and non-physiological factors, the brain signals\noften contain artifacts. During EEG and MEG, the electrode\nsignal is susceptible to interference from eye movement, heart-\nbeat, and muscle activity. Independent component analysis\n(ICA) is used to decompose and identify the independent\nsignal sources, and then eliminate the components associated\nwith artifacts to purify the signal. EEG artifact correction may\nalso include manual examination and labeling of abnormal\nsignal segments[165]. ECoG requires artifact removal, espe-\ncially in relation to implant surgery. Artifacts of electrode\ndisplacement or non-neural activity that may occur during\nsurgery, such as patient body movement, can be identified\nand corrected by simultaneous video recording to ensure\nthe recorded signal purity[166]. Artifact correction in fMRI\nfocuses on physiological fluctuations, such as those caused\nby heartbeat and breathing[165]. This is usually done by\nrecording synchronized physiological signals and applying\nregression analysis methods to isolate the effects of these\nsignals from the fMRI data with the goal of retrieving pure\nBOLD responses.\nReference reset: Reference reset of EEG and ECoG\nis necessary because these techniques rely on measuring\nthe potential difference relative to a reference point. Using\nan average reference, that is, subtracting the average of all\nelectrodes from the individual electrode signals, can reduce\nthe non-uniformity of local areas, thus making the spatial\ndistribution of signals more uniform and reliable. Other com-\nmon reference methods include binaural reference, the use\nof symmetrical body surface electrodes, which help reduce\nthe artifact effect of the overall reference position[167].\nSpatial correction: Avoiding head movement artifacts is\ncrucial in fMRI, with the help of voxel alignment techniques\nsuch as rigid body transformation to correct head movements\nduring acquisition[168]. Usually, the 3D rigid body alignment\ntechnique is used to adjust the geometric position of each\nimage relative to the reference image, so as to avoid the error\nintroduced by motion. In addition, slight head movements\nduring MEG recording can affect results, so head positioning\nsensors are used to monitor and correct head movements in\nreal time. ECoG Because of the physiological fixation of the\nelectrode on the surface, the spatial correction is more related\nto post-operative fretting, and the electrode position is often\nrecalibrated by reference markers[169].\nTime-frequency analysis: EEG and ECoG make exten-\nsive use of time-frequency analysis to understand the dy-\nnamic spectral characteristics of signals. Short-time Fourier\ntransform (STFT) and wavelet transform are important tools\nto obtain frequency activity in different time periods[170].\nThrough these techniques, dynamic EEG rhythms such as 𝛼\nwave and 𝛽wave can be analyzed in specific time periods.\nMEG data can also be interpreted by time spectrum analysis,\nand the inverse problem solving method can be used for\nfurther neural source localization analysis[171]. In fMRI, this\ntype of analysis is used less, because fMRI focuses on spatial\nanalysis and long-term changes in time series signals[172].\nWith these processing steps, researchers can more ef-\nfectively remove interference, improve data quality, and\nultimately obtain more accurate analysis of brain function\nand behavior. These steps work together to ensure that the\nmost useful and accurate information is extracted in complex\nbrain function studies.\n3.1.3. Public Dataset\nBefore establishing the model, it is very important to\nselect high-quality dataset. Although high-quality dataset can\nbe obtained after a series of data preprocessing, due to the\nparticularity of brain signals, data acquisition equipment and\ncost requirements are high, so the collection conditions are\nnot necessarily available. Most experiments are completed\nbased on existing public data sets. The most commonly used\neye tracking datasets, EEG datasets, and fMRI datasets are\nsummarized as shown in Table 3.\nEye-tracking technology enables researchers to capture\nparticipants’ eye movements during silent reading with\nminimal guidance or interference from the researchers. Addi-\ntionally, in contrast to lexical decision tasks, eye-tracking\ntechnology can capture linguistic representations as they\nnaturally occur in everyday life, devoid of interference from\nadditional decision components or response mechanisms typi-\ncally present in lexical decision-making tasks. Utilizing state-\nof-the-art eye-tracking devices, the position of the eye can\nbe precisely determined every millisecond with high spatial\naccuracy, generating a rich and detailed dataset. The recorded\neye movements during reading are frequently employed to\nexplore visual word recognition in context[153, 180]. For\nexample,GECO[173] presents the first natural reading eye\ntracking corpus specifically for bilingual sentence reading,\nselecting participants based on their language history and\ncollecting detailed measures of proficiency. These data are\nwell suited for research at one or more language processing\nlevels[173]. In brain-inspired computing tasks, eye-tracking\ndata can help determine word boundaries[20].\nZuCo[23] is a dataset that integrates EEG and eye-\ntracking recordings of individuals engaged in reading natural\nsentences. Comprising high-density EEG and eye-tracking\ndata, ZuCo includes records from 12 healthy native English-\nspeaking adults who spent four to six hours each reading\nnatural English text. This dataset encompasses two standard\nreading tasks and one task-specific reading task, providing\nEEG and eye-tracking data for a total of 21,629 words\nacross 1,107 sentences and 154,173 fixation points. The EEG\nand eye-tracking signals from this dataset serve as valuable\nresources for training enhanced machine learning models\nacross various tasks, with a particular focus on information\nextraction tasks such as entity and relationship extraction, as\nB.H.Yu et al.\nPage 10 of 26\n\nTable 3\nCognitive data sources utilized in this study include the Coverage metric, representing the percentage of vocabulary in the data\nsource that aligns with the list of most frequently occurring English words in the British National Corpus. The statistical results in\nthe table are from [56].\nData type\nData source\nstimulus\nsubject\ntokens\ntypes\ncoverage\nEYE-TRACKING\nGECO([173])\ntext\n14\n68606\n5383\n95%\nDUNDEE([174])\ntext\n10\n58598\n9131\n94%\nCFILT-SARCASM([175])\ntext\n5\n23466\n4237\n85%\nZuCo([23])\ntext\n12\n13717\n4384\n90%\nCFILT-SCANPATH([176])\ntext\n5\n3677\n1314\n89%\nPROVO([177])\ntext\n84\n2743\n1192\n95%\nUCL([159])\ntext\n43\n1886\n711\n98%\nEEG\nZuCo([23])\ntext\n12\n13717\n4384\n90%\nNATURAL SPEECH([157])\nspeech\n19\n12000\n1625\n98%\nUCL([178])\ntext\n24\n1931\n711\n98%\nN400([157])\ntext\n9\n150\n140\n100%\nfMRI\nHARRY PORTER([160])\ntext\n8\n5169\n1295\n92%\nALICE([179])\nspeech\n27\n2066\n588\n99%\nPEREIRA([26])\ntext/image\n15\n180\n180\n99%\nNOUNS([25])\nimage\n9\n60\n60\n100%\nwell as sentiment analysis. Furthermore, the dataset proves\ninstrumental in advancing research on human reading and\nlanguage comprehension processes, delving into the intricate\ninterplay between brain activity and eye movement at a\ngranular level[155].\nZuCo2.0[24] introduces a novel and freely accessible\ncorpus, capturing both eye-tracking and brain electrical\nactivity during natural reading and annotation. Distinguishing\nitself as the inaugural dataset facilitating a direct comparison\nof these two reading paradigms, ZuCo2.0 meticulously\ndetails the material and experimental design, undergoing\nthorough validation to ensure the data’s quality. Tailored for\ncognitively inspired Natural Language Processing (NLP),\nthis corpus boasts broad potential for application and reuse.\nThe furnished word level and sentence level eye-tracking and\nEEG features offer utility in enhancing and assessing NLP and\nmachine learning methods. Moreover, given the inclusion\nof semantic relational labels and participant comments at\nthe sentence level, the dataset proves valuable for relational\nextraction and classification endeavors. As an upgraded\nversion of the ZuCo dataset. ZuCo2.0 takes into account\nthe effect of time periods on the test results, intentionally\nrepeating some stimulus text for both normal reading tasks\nand special reading tasks.\nAs shown in Table 4, the three tasks and stimulus control\nconditions set in the open dataset ZuCo are listed. Subjects\nrecord EEG data by reading sentences and answering control\nquestions, which provides high quality EEG datasets related\nto natural language processing tasks such as sentiment\nanalysis and information extraction for computer science\nresearchers.\nFunctional magnetic resonance imaging (fMRI) stands\nout among non-invasive neuroimaging techniques, offering\nthe highest spatial resolution. This series of investigations\ncommenced with Mitchell et al. in 2008, marking the in-\naugural demonstration of the feasibility of decoding words\nfrom fMRI data. The pioneering approach involved lever-\naging semantic representations of words and learning cross-\nmodal mappings between fMRI images and word vectors[25].\nMitchell et al. evaluated their learned neurodecoder through\na pairwise classification task, a binary classification exercise\ndistinguishing which of two stimuli corresponds to an fMRI\nimage. Subsequently, this paired classification methodology\nhas become widely adopted by researchers in non-invasive\nneural decoding for word decoding tasks[19].\nIn summary, both EEG and fMRI are used to study brain\nactivity, and are the two most commonly used brain signals\nto decode text or speech. Eye movement data sets can play\nan auxiliary role in decoding. These public datasets promote\nthe research of decoding using deep learning techniques.\n3.2. EEG-to-text(speech)\nSensory, language, emotion and other processes are very\nrapid, and the high temporal resolution of EEG makes it\nvery suitable for capturing these fast, dynamic and sequential\ncognitive events. Therefore, many researchers are engaged\nin the study of decoding text or speech by EEG. The\nEEG-to-text (speech) task provides a convenient and widely\napplicable means of human-computer interaction by non-\ninvasively measuring EEG electrical signals and converting\nthem into text or speech. The current EEG-to-text tasks aim\nto assume that the human brain acts as a special text encoder,\nimplementing an open vocabulary brain-to-text system using\nB.H.Yu et al.\nPage 11 of 26\n\nTable 4\nIllustration of three task data examples for the ZuCo dataset adapted from[23].\nTask\nEmotion Classification\nEntity Extraction\nRelation Extraction\nMaterial\nPositive, negative, or neutral\nsentences from movie reviews\nWikipedia sentences\ncontaining specific relations\nWikipedia sentences\ncontaining specific relations\nData Sample\nThe film often achieves a\nmesmerizing poetry (positive)\nTalia Shire (born April 25,\n1946) is an American actress\nof Italian descent.\nRelations: nationality, job title\nLincoln was the first\nRepublican president.\nRelation: political affiliation\nTask Description\nBased on the previous sentence,\nhow would you rate\nthis movie from 1-5?\nTalia Shire was a . . .\n1)singer 2)actress 3)doctor\nDoes the sentence contain the\npolitical affiliation relation?\n1)Yes 2)No\nFigure 3: The development of brain-inspired computing models based on deep learning pre-trained language models with EEG\nsignals in recent years.\na pre-trained language model. Figure 3 demonstrates the\ndevelopment of brain-inspired computing models based on\ndeep learning pre-trained language models with EEG signals\nin recent years. Figure 4 shows the EEG-to-text task flow\ndiagram.The subjects read sentences on the screen, the\nresearchers recorded the EEG signal during the reading\nprocess, and then decoded the text through the EEG signal.\n3.2.1. Recent Progress\nDecoding brain states into understandable representations\nhas long been the focus of research[181]. EEG signals are\nparticularly favored by researchers because they are non-\ninvasive and easy to record[182, 183]. Traditional EEG\ndecoding techniques mainly focus on classification tasks,\nsuch as emotion recognition[184, 185, 186, 187, 188], motor\nimagination[189, 190, 191], robot control[192, 193, 194,\n195, 196], and games[197, 198, 199, 200, 201]. However,\nthese task-bound tags are not sufficient for extensive brain-\ncomputer communication. As a result, interest in brain-to-text\n(speech) translation has surged in recent years.\nFor example, the researchers[20] extend the problem to\nopen word EEG on natural reading tasks to text sequence-\nto-sequence decoding and zero-sample sentence emotion\nclassification. Assuming that the human brain acts as a unique\ntext encoder, the authors introduce a novel framework that\nleverages pre-trained language models for the first time. The\nmodel achieved a BLEU-1 score of 40.1% in EEG-to-text\ndecoding and an F1 score of 55.6% in zero-sample EEG-based\nternary emotion classification, significantly better than the\nsupervised baseline. Furthermore, the model demonstrates\nthe capability to process data from diverse topics and sources,\nindicating substantial potential for high-performance open-\nvocabulary brain-to-text systems as more data becomes\naccessible.\nIn addition, the notable difference between subject-\nrelevant EEG representations and semantically relevant text\nrepresentations presents a significant challenge to this task.\nTo address this challenge, Feng et al.[21] propose a curricular\nSemantic Perception Contrastive Learning strategy (CSCL).\nThis approach effectively recalibrates subject-dependent EEG\nrepresentations into semantically dependent EEG representa-\ntions, reducing variance. CSCL specifically groups together\nsemantically similar EEG representations while separating\ndistinct EEG representations. Furthermore, to introduce more\nmeaningful contrast pairs, the researchers meticulously uti-\nlized curriculum learning, not only for generating significant\ncontrast pairs but also for ensuring progressive learning.\nSubsequent research not only demonstrated its superiority in\nsingle-agent and low-resource settings but also showcased\nstrong generalization in zero-sample settings.\nFinally, with the rapid development of large language\nmodels such as ChatGPT[202, 203], in-depth research into\nways to bridge the gap between brain signals and natural\nlanguage representations becomes critical. However, the early\nwork[204, 205, 42, 206] and several models mentioned above,\nrelied on external event markers like handwriting or eye-\ntracking fixations which may not be readily available. And\nthe order of eye-tracking data may not correspond to the order\nof spoken words. To address these problems, the authors\nintroduce a novel framework, DeWave[22], which integrates\nB.H.Yu et al.\nPage 12 of 26\n\nFigure 4: Illustration of the EEG-to-text generation task. The left portion depicts the EEG recording process, where a subject\nreads a sentence on the screen while EEG signals are recorded. Simultaneously, an eye-tracking device facilitates the precise\ndefinition of word boundaries through fixations. The objective of the task is to generate the sentence that elicits the recorded EEG\nsignals(adapted[21]).\ndiscrete coded sequences into the task of translating open\nvocabulary EEG to text. DeWave uses quantized variational\nencoders to derive discrete codex codes and align them with\npre-trained language models. This discrete code representa-\ntion mitigates sequence mismatches between eye fixation\nand spoken language and minimizes interference caused\nby individual differences in brainwaves by introducing text-\nEEG contrast alignment training which is the first to decode\nEEG signals first without the need for word-level sequential\nlabeling scoring 20.5 BLEU-1 and 29.5 Rouge-1 on the ZuCo\ndataset.\nTo sum up, from decoding words to decoding text to\ndecoding directly without the aid of eye-tracking data, from\nclosed-domain words to open-domain words, researchers\nhave used the latest deep learning techniques and methods to\ncomplete increasingly complex EEG-to-text decoding tasks.\n3.2.2. Key Technologies\nEEG-to-text models based on deep learning have ex-\nperienced different stages from closed vocabulary to open\nvocabulary, from dividing word boundaries by external means\nto directly translating brain waves, and each model has its\nown contributions and key techniques.\nEEG-to-text Decoding: The author tries to maximize\nthe probability of decoding the sentence as the following\nequation:\n𝑝(𝑆|𝜀) =\n𝑇\n∏\n𝑡=1\n𝑝(𝑠𝑡∈𝜈|𝜀, 𝑠< 𝑡)\n(1)\nWhere 𝑇represents the length of the target text sequence. The\nprimary challenge in the environment is that the vocabulary\nsize, denoted as |𝜀| (50,000), is considerably larger than in\nprevious sequence-to-sequence studies (250)[42].\nCurriculum Learning: The researchers proposed an\napproach based on curriculum learning. The overall training\nprocess adopts a two-step approach. Firstly, CSCL is em-\nployed to train the pre-encoder. Formally, a contrastive triple\n(𝐸𝑖, 𝐸+\n𝑖, 𝐸−\n𝑖) is obtained for a given anchor 𝐸𝑖. After the\ntransformation by the pre-encoder, the corresponding vectors\nbecome (ℎ𝑖, ℎ+\n𝑖, ℎ−\n𝑖), where ℎ𝑖is the averaged vector of the\npre-encoder outputs. Following the contrastive framework\nin[207, 57, 73], with 𝑁as the mini-batch size, the objective\nis to minimize the cross-entropy loss 𝑙𝑖defined by:\n𝑙𝑖= −𝑙𝑜𝑔\n𝑒𝑠𝑖𝑚(ℎ𝑖,ℎ+\n𝑖)∕𝑟\n∑𝑁\n𝑗=1(𝑒𝑠𝑖𝑚(ℎ𝑖,ℎ+\n𝑖)∕𝜏+ 𝑒𝑠𝑖𝑚(ℎ𝑖,ℎ−\n𝑖)∕𝜏)\n(2)\nwhere 𝜏is a temperature hyperparameter. 𝑠𝑖𝑚(ℎ𝑖, ℎ𝑗) is the\ncosine similarity.\nSecondly, leveraging the contrastive-trained pre-encoder,\njointly fine-tune all parameters of the BRAINTRANSLATOR\nto minimize the cross-entropy loss in a parallel training\ncorpus (𝐸, 𝑆)[57]:\n𝐿= −\n∑\n(𝐸,𝑆)∈(𝐸,𝑆)\nlog 𝑝(𝑆|𝐸; 𝜃)\n(3)\nDiscrete Codex: The key steps of the DeWave model\nare shown in Figure 5. First, the original brainwave is\nvectorized, the vectorized features are encoded into hidden\nwave variables and converted into discrete latent variables by\ncodex index. Finally, the pre-trained BART model converts\nthis discrete codex into text.\nGiven the EEG waves, it is firsted vectorized into em-\nbedding, where 𝑋is the embedding sequence as shown in\nEquation 4.\n𝑧𝑞(𝑋) = 𝑧𝑞(𝑥)𝑖\n(4)\nA codex book 𝑐𝑖∈𝑅𝑘∗𝑚is initialized with number 𝑘of latent\nembedding with size 𝑚as shown in Equation 5.\n𝑧𝑞(𝑥) = 𝑐𝑘\n(5)\nThe vectorized feature 𝑋is encoded into 𝑧𝑐(𝑋) through a\ntransformer encoder as shown in Equation 6. The discrete\nrepresentation is acquired by calculating the nearest embed-\nding in the codex of input embedding 𝑥∈𝑋as shown in\nEquation 4.\n𝑘= 𝑎𝑟𝑔𝑚𝑖𝑛𝑗||𝑧𝑐(𝑥) −𝑐𝑗||2\n(6)\nDewave directly decodes the translation output given the\nrepresentation 𝑧𝑞(𝑋). Given a pre-trained language model,\nthe decoder predicts text output.\nIn summary, with the help of deep learning technology,\nthe study of EEG-to-text decoding task relies less and less on\ndatasets, and the decoding accuracy is gradually improved\nB.H.Yu et al.\nPage 13 of 26\n\nFigure 5: The DeWave model vectorizes the original EEG waves into embeddings, the vectorized features are encoded into latent\nvariables by codex indexing into discrete latent variables, and finally, the pre-trained BART model converts this discrete codex\nrepresentation into text(adapted[22]).\n3.3. fMRI-to-text(speech)\nAs far as verbal stimuli are concerned, recent research\nhas shown that fMRI scans can be decoded into embeddings\nof the words the subject is reading[31]. The fMRI-to-text\n(speech) task uses functional magnetic resonance imaging for\ntext conversion, and is able to capture deep brain activity due\nto its high spatial resolution, providing broad possibilities\nfor human-computer interaction in pursuit of detailed and\ncomplex decoding. The research of decoding of text or speech\nbased on fMRI is also gradually increasing and fMRI signals\nhave been used more recently to decode text.\n3.3.1. Recent Progress\nFMRI provides the best spatial resolution of all non-\ninvasive neuroimaging techniques[19]. Most of the fMRI-\nto-text based tasks are pairwise classification tasks[208, 206,\n209], or direct classification tasks[31, 19].\nFor example,the authors[31] propose a model for decod-\ning fMRI scans into words, and verify the decoding perfor-\nmance and generalization performance of the model through\ntwo tasks of pair-classification and direct classification. The\ndirect classification task is to classify the brain scan directly\ninto one of the 𝑣words in the vocabulary under consideration.\nBecause scanning is mapped to word categories rather than to\nrepresentations of words, the task is not subject to limitations\nassociated with specific vector representations. In addition,\nwhen there is no data from the target subject at the time\nof training, the reserve-one method is used to train the\nmodel with data from 𝑛-1 subjects and test on the remaining\nsubjects, repeating the process for each subject. Due to the\nimpossibility of subject-specific preprocessing, the model\nrequires strong generalization, and the lack of consistency in\nFMRI scans across subjects and even across recorded sessions\nis a big challenge.\nDue to the inherent noise in brain recordings, prior\nresearch has simplified brain-to-word decoding into a binary\nclassification task, aiming to distinguish words corresponding\nto brain signals from incorrect ones. However, this paired\nclassification approach faces limitations for practical neurode-\ncoder development for two reasons. Firstly, it necessitates\nenumerating all pair combinations in the test set, making\nit inefficient for predicting words in a large vocabulary[19].\nSecondly, even with a perfectly paired decoder, the perfor-\nmance of direct classification is not guaranteed. In order to\naddress these challenges and advance realistic neurodecoders,\nthe authors introduce a novel cross-modal cloze (CMC) task.\nThis task involves predicting target words encoded in neural\nimages using context as a cue. The results demonstrate the\nfeasibility of decoding a single word from neural activity in\na large vocabulary[19, 20].\nAnother representative advancement is UniCoRN. The\ndisadvantage of fMRI is its low temporal resolution, and\nprevious fMRI signal decoding methods often relied on\nfeature extraction for predefined regions of interest (ROI),\nfailing to make full use of time series information. UniCoRN\nconstructs an efficient encoder through snapshot and sequence\nreconstruction, allowing the model to deeply analyze the time\ndependence and maximize the extraction of information from\nbrain signals. Specifically, UniCoRN contains two key stages:\nbrain signal reconstruction and brain signal decoding. The\nbrain signal reconstruction stage is divided into snapshot\nreconstruction and sequence reconstruction. The internal\nfeatures of each snapshot and the temporal relationship of the\nsnapshot sequence are integrated by training the encoder. In\nthe brain signal decoding stage, the representation of the brain\nsignal is converted into natural language, each fMRI frame is\ntreated as a word-level representation of the \"language spoken\nby the human brain\", and real human language is generated\nthrough a text decoder[35].\nFinally, some researchers have proposed using fMRI\nsignals to reconstruct auditory stimuli that subjects hear or\nimagine[36]. To overcome the low temporal resolution of\nfMRI, the decoder employs a non-end-to-end strategy of\nguessing candidate word sequences, assessing the likelihood\nthat each candidate triggers the currently measured brain\nresponse, and selecting the best candidate for decoding. In\nthe experiment, a coding model was trained for each subject\nto predict the semantic representation of text stimuli and the\ncorresponding brain response. The beam search algorithm\nis used to generate candidate sequences word-by-word and\npreserve the most likely continuation when detecting new\nwords in brain activity. Finally, the most likely continuation\nis preserved by scoring the coding model.\nOverall, these studies summarize cutting-edge models\nand methods for decoding fMRI signals in the brain-inspired\nB.H.Yu et al.\nPage 14 of 26\n\nfield, deepening the understanding of the relationship be-\ntween brain activity and cognitive processes such as language\nand perception.\n3.3.2. Key Technologies\nThe brain-inspired computing model based on fMRI-to-\ntext has developed from the original binary classification\nmodel to the cross-modal task, which makes the direct\nclassification task possible.\nBrain Decoding Model: In the model developed in\nthis paper[31], the decoder of the model takes the one-\ndimensional vector of the fMRI scan as input, filling it as\nneeded. By simply changing the output layer and the loss\nfunction, the model can be used for paired classification tasks.\nThe loss is calculated as follows:\n𝜁reg =\n𝑣\n∑\n𝑖=1\ncos(𝑦pr,𝑖, 𝑦true,𝑖) −\n𝑣\n∑\n𝑗≠𝑖\ncos(𝑦pr,𝑖, 𝑦true,𝑗)\n(7)\nWhere 𝑦pr,𝑖is the predicted word embeddings of the word 𝑖,\n𝑦true,𝑗is the true word embeddings of the word 𝑗, and 𝑐𝑜𝑠()\nis the cosine distance.\nSemantic Features Fusion: Semantic features are ex-\ntracted from fMRI images and can be seamlessly integrated\ndirectly into hidden states within the BERT embedding layer.\nTo address the centrality problem[210], an intermediate word\nembedding is introduced, and a retrieval-based method is\ndevised. The fundamental concept involves utilizing the\nintermediate word embedding for cross-modal mapping,\nfollowed by transforming the prediction into the BERT\nembedding space through retrieval[19].\nThe feature vector 𝑓𝑖for fMRI test sample is computed\nas follows:\n𝑓𝑖= 1\n𝑘\n𝑘\n∑\n𝑡=1\n𝑤′𝑗𝑡\n(8)\nWhere 𝑖= {1, … , 𝑁} and 𝑤′ denotes BERT embedding.\nTo be specific, let ℎ𝑖𝑚𝑎𝑠𝑘denote the hidden states of\nthe [MASK] token, try to directly update ℎ𝑖𝑚𝑎𝑠𝑘using the\nfollowing equation:\nℎ𝑖\n𝑚𝑎𝑠𝑘= (1 −𝛼)ℎ𝑖\n𝑚𝑎𝑠𝑘+ 𝛼𝑓𝑖\n(9)\nwhere 𝛼∈[0, 1] is a tuning parameter that controls how\nmuch information to fuse in.\nAs illustrated in Figure 6, the objective is to incorporate\nfeature vectors extracted from fMRI images into BERT\nfor enhanced word prediction. The rationale is that if the\nfeature vector contains valuable information about the target\nword, integrating it into the model should enhance prediction\nperformance, going beyond using context alone as input[19].\nIn summary, these key technologies have innovated the\nresearch method of fMRI-to-text task, improved the decoding\naccuracy, and provided new ideas for subsequent research.\n3.4. MEG-to-text(speech)\nMEG records magnetic field changes resulting from\ncurrent flow in the brain[211] and is capable of obtaining\nFigure 6: Illustration of the semantic feature fusion method.\nThe pre-trained language model BERT is used to predict target\nwords(adapted[19]).\nreal-time resolution recordings of brain activity at a higher\nspatial resolution than EEG or fMRI[212, 213]. According to\nour research, MEG signals have been used more recently to\ndecode speech. The unique data properties of MEG make it\nsuitable for brain disorders that are time- and space-sensitive.\nThe MEG-to-text (speech) task decodes signals by monitoring\nmagnetic activity in the brain, supporting human-computer\ninteraction applications that require fast responses with its\nsuperior temporal resolution. Additionally, MEG is quiet\nand, therefore, user-friendly. Prior MEG studies on speech\nperception hold promising prospects for decoding speech\nproduction in brain activity signals[214, 215, 216, 217, 213].\n3.4.1. Recent Progress\nThe convergence of advanced methodologies and tech-\nnologies has paved the way for breakthroughs in understand-\ning the spectral and temporal characteristics of MEG signals.\nFor example, the researchers delved into the spectral and\ntemporal characteristics of MEG signals and trained these fea-\ntures using CNN to classify neural signals corresponding to\nspecific phrases. Experiments show that CNN is remarkably\neffective in decoding speech in perception, imagination and\ngeneration tasks. In order to solve the problem of long CNN\ntraining time, the researchers used PCA algorithm to reduce\nthe spatial dimension of MEG data, and carried out the model\ninitialization through transfer learning. The experimental\nresults show that the accuracy of speech decoding in speech\ngeneration stage is significantly higher than that in perception\nand imagination stage[38].\nIn addition, some researchers propose a single architec-\nture data-driven approach for decoding natural language from\nMEG signals. Convolutional neural networks are used as\nencoders of brain signals and trained by contrast targets to\nalign deep audio representations generated by the pre-trained\nspeech self-supervised model wav2vec-2.0[218]. In addition,\nthe contrast loss of the CLIP[219] model is used to align\ndeep representations of the two modes of text and image, and\nthe model can efficiently encode multiple levels of linguistic\nfeatures and has a linear relationship with brain activation.\nThe method and ideas provide a basis for subsequent work\nB.H.Yu et al.\nPage 15 of 26\n\nthat can be effectively migrated to other tasks, including\ndecoding continuous text[41].\nIn summary, as a non-invasive method, MEG has gar-\nnered attention from research groups exploring its application\nin decoding speech or text. These advances represent the latest\nin the current MEG-to-text(speech) task.\n3.4.2. Key Technologies\nIn the cutting-edge research of MEG signal decoding\ntechnology, the key technologies such as convolutional neural\nnetworks, transfer learning and signal alignment have not only\ndeeply influenced the theoretical cognition of neuroscience,\nbut also promoted the innovation in the field of speech\ndecoding.\nCNNs and Transfer Learning: This paper[38] proposes\nan approach that the MEG signal was denoised by wavelet\nusing machine learning technology, and then the PCA space\ndimension was reduced and the PCA coefficient spectrum\nwas generated. A 3-layer 2D-CNN was then trained using\ndeep learning techniques for each of the 200 gradient sensors.\nConv3 features were then extracted from the 200 CNNs\ntrained to further classify the phrases. Finally, transfer\nlearning is used to reuse the pre-trained model for new data,\nand transfer the weight of the pre-trained network to the\nnew network to obtain faster convergence speed and better\nperformance. However, the researchers did not use these\npredefined networks in MEG signals, so the approach was to\nfirst perform neural speech decoding by training CNNS on\na single subject’s data, and then transfer the learning weight\nof that subject’s trained CNNS to the next subject’s speech\ndecoding. The weights of the classification layer, softmax\nlayer, and FC layer were not trained using data from another\nsubject[38].\nSignal Alignment: As shown in Figure 7, the researchers\ndecoded speech from brain activity recorded using mag-\nnetoencephalograms or electroencephalograms in healthy\nparticipants as they listened to sentences. To do this, their\nmodel extracts deep context representations of 3s speech\nsignals from the pre-trained speech module wav2vec2.0 and\nlearns the brain’s representations’ activity on the correspond-\ning 3s window to maximize alignment with these speech\nrepresentations with contrast loss. Models with missing\nsentences were input at the time of assessment, and the\nprobability of each 3-second speech fragment given each\nbrain representation was calculated. Thus, the final decoding\ncan be a \"zero sample\" because the audio snippet predicted by\nthe model does not need to be present in the training set[46].\nThese technical advances highlight the continuous evo-\nlution of MEG signal decoding, and through the integration\nof deep learning models, speech processing models and\nneural signals, the technical gist of MEG decoding is deeply\nunderstood.\n3.5. ECoG-to-text(speech)\nECoG is superior to surface EEG in terms of spatiotem-\nporal resolution and signal-to-noise ratio[46]. The ECoG-\nto-text(speech) task obtains high-resolution data by directly\nrecording the electrical activity of the cortex, and while\nFigure 7: The model of the zero shot MEG-to-speech decoding\nmethod(adapted[41]).\nproviding accurate and fast decoding, it is suitable for\nhuman-computer interaction scenarios that require highly\naccurate and rapid control. According to our research, ECoG\nsignals have been used more recently to decode speech. It is\nparticularly suitable for analyzing brain activity associated\nwith speech in high gamma-band[220, 221, 46].\n3.5.1. Recent Progress\nThis section will focus on two recent studies that cover\nthe exploration of decoding speech through electrical cor-\nticography (ECoG) signals and parsing continuous text using\nmachine translation models.\nFor instance, employing a deep neural network-based en-\ncoder and a pre-trained neural vocoder, the researchers effec-\ntively reconstructed spoken sentences from ECoG signals[46].\nWhen 13 participants spoke short sentences, the ECoG\nsignals were recorded, and the ECoG recordings could be\nmapped to spectrographs of spoken sentences using two-\nway Long short-term memory (BLSTM) or Transformer,\ncomparing the performance of the two encoders. In addition,\nthe combination of encoders and neural vocoders was evalu-\nated, and the experiment found that the use of Transformer\nencoders for spectral graph prediction is better than that\nof BLSTM encoders, and their combination with neural\nvocoders can effectively synthesize speech[46].\nIn the past research on decoding brain activity, decoding\nof continuous text is relatively insufficient and ineffective. In\norder to deal with this problem, the researchers consider the\nbrain signal as the source language and the corresponding\ncontinuous text as the target language, and use the machine\ntranslation model method to design a simple encoder-decoder\nstructure neural network to decode the continuous text in\nthe ECoG signal[42]. By introducing auxiliary loss in the\ntraining stage, the model is forced to accurately predict the\naudio representation of the speech at the corresponding time\nbased on the hidden layer representation of the encoder, so\nas to improve the information capture ability of the encoder.\nThe study achieved significant accuracy improvements, and\nthe average word error rate for some participants was reduced\nto 7%, providing a valuable reference for future research.\nIn summary, ECoG as an invasive data acquisition method\nhas the highest spatial resolution but also has significant\nB.H.Yu et al.\nPage 16 of 26\n\nFigure 8: Illustration of the machine translation network architecture. The encoder and decoder are shown unrolled in\ntime(adapted[42]).\nlimitations, and these new developments further enhance the\nunderstanding of ECoG-to-speech decoding tasks.\n3.5.2. Key Technologies\nECoG technology has high spatial and temporal resolu-\ntion when recording electrical activity in the brain. Because\nECoG is closer to the surface of the brain, it can provide more\naccurate and fine-grained information about neural activity.\nSome of the latest techniques for ECoG signal decoding also\nincorporate DL models.\nDecoding Approach: The researchers conducted a com-\nparison between the Transformer encoder and the BLSTM\nencoder. The Transformer encoder comprises 𝑋identical\nlayers, each consisting of two sub-layers: a multi-head self-\nattention mechanism and a fully connected feedforward\nnetwork. When these two sub-layers are replaced by BLSTM,\nthe model is referred to as a BLSTM encoder[46]. Residual\nconnection and layer normalization strategies are applied in\neach of the two sub-layers. The output from 𝑋stacks of the\nsame layer is fed into a second-layer feedforward network\nto represent a sequence of log-mel spectrographs. During\nthe training of the encoder model, the output is utilized to\napproximate a sequence of log-mel spectrographs with a\nlength 𝑁equal to the output size of the time convolution\nlayer[46].\nThe model first uses time-span convolution for feature\nextraction, down-sampling the timing features to 16HZ\nand input the features into the LSTM network to generate\ncontinuous text. In order to guide the encoder to capture\nmeaningful information, in addition to the end-to-end ECoG\nsignal decoding text training, an additional auxiliary loss is\nintroduced during the training phase. This loss requires the\nmodel to accurately predict the audio representation of the\nspeech at the corresponding moment, based on the hidden\nlayer representation of the encoder at each time step[42]. The\nnetwork architecture is shown in Figure 8, with encoders and\ndecoders displayed in time expansion, i.e. sequence elements.\nEncoder and decoder all layers in the same row have the\nsame input and output weights, and arrows in both directions\nrepresent bidirectional RNN.\nNeural Vocoder: Parallel WaveGAN[222] is used to syn-\nthesize speech waveforms. Parallel WaveGAN is a waveform\ngenerator based on a non-autoregressive WaveNet model\nthat is trained to synthesize speech based on a given Mayer\nspectrogram. The authors use an existing vocoder model pre-\ntrained on a corpus dataset containing Japanese speech[46].\nThese two core technologies provide powerful ways to\ndelve deeper into the relationship between brain signals and\nspeech.\nIn this section, the tasks associated with brain-inspired\ncomputing for human-computer interaction using deep learn-\ning models are reviewed in detail from five perspectives. In\naddition to perspectives on four common brain signals, we\nhighlight the importance of datasets.\n4. CHALLENGES AND SOLUTIONS\nDecoding language from non-invasive brain activity is\nof increasing interest to researchers in neuroscience and\nnatural language processing. Human beings have the ability\nto adapt to the complex environment, the ability to learn\nnew things independently and the ability to cooperate with\na variety of different cognitive abilities, but since the birth\nof artificial intelligence, no general intelligence system has\nreached the human level. The application of deep learning\nmodels in brain-inspired computing for HCI tasks faces\nseveral challenges and limitations. These issues must be\nseriously addressed and resolved to promote the development\nand application in this field. In this section, divided into six\nparts from three levels: model training, model application,\nand ethical issues, we will delve into the challenges faced\nby brain-inspired computing models and propose potential\nsolutions. Specifically, challenges related to model training\nare elaborated in sections 4.1, 4.2, and 4.3, challenges related\nto model application are discussed in sections 4.4 and 4.5,\nand finally, ethical issues are analyzed in section 4.6. The\nB.H.Yu et al.\nPage 17 of 26\n\nfollowing are the factors to analyze the challenges and future\nresearch direction of these tasks:\n4.1. Challenges for Brain Dataset\nMost high-performance methods require data from inva-\nsive devices, such as ECoG, which require extremely high\nconditions to be collected and studied[223, 224, 225]. Non-\ninvasive device data acquisition, such as fMRI, also has a\nlarge equipment threshold, and only professional institutions\nhave the acquisition conditions. In addition, due to the\nlimitation of word-level features, many current models based\non EEG data sets need to use eye movement datasets and\nother external means to demarcate word boundaries, which\nindirectly increases the difficulty and cost of data collection.\nAs a result, there are few public high-quality data sets. For\nexample, most of the EEG-based brain signal decoding tasks\nare based on the public ZuCo[23] and ZuCo2.0[24]. The\nrange of vocabularies is also limited, and many models are\nbased on closed vocabularies, which do not fully conform to\nthe concept of \"silent speech\" in direct thought translation by\nhuman brain. Deep learning models typically require large\ndatasets, and their lack of generalization and robustness in\nsmall sample environments has not been fully addressed.\nAlthough the data obtained by invasive devices is more\naccurate, researchers should be encouraged to use data\nsets collected by non-invasive devices to improve model\nperformance by improving technical means such as word\nembedding methods. For example, achieving direct EEG-to-\ntext translation is a great idea, and DeWave[22]’s introduction\nof discrete coding similar to word embeddings could be\na good step toward closing the gap between brainwaves\nand high-level language models. Expand the scope of the\nvocabulary and conduct research on the open vocabulary to\nimprove the usefulness of future brain-computer interface.\n4.2. Challenges for Computational Resource\nDemand\nWhen deep learning models are applied to the training\nand decoding of brain signal data, the models often have huge\ndemands on huge data sets and high computational resources.\nHigh-performance deep learning models often require exten-\nsive training data to accurately capture complex patterns and\nfeatures, but in brain signaling research, large-scale data sets\nare difficult to obtain because the data acquisition process\nis complex and limited by equipment conditions and ethical\nconsiderations. In addition, these models are less robust in\nsmall sample environments, which means that even though\ntraining costs are high, the models struggle to perform well\nwhen the data is insufficient. In practical applications, due to\nthe limitation of hardware resources, such as the processing\npower of smart phones or the computing power of portable\ndevices, this high demand for computing resources often\nbecomes an important bottleneck in the wide application of\ndeep learning models.\nTo address these challenges, researchers can adopt a\nvariety of strategies to optimize the training and reasoning\nprocesses of deep learning models. First, by introducing\ntransfer learning techniques, using models that have been\ntrained on similar tasks can significantly reduce the need\nfor large-scale data sets and improve model performance\nin small sample cases. Second, through data enhancement\ntechniques, small data sets can be manually expanded to\nimprove the generalization ability of the model on more\ndiverse data. In addition, exploring unsupervised learning\nmethods can reduce the dependence on data annotation and\ndiscover useful information in data structures. In terms of\ncomputing resources, the development of more efficient\ntraining and inference algorithms, as well as the deployment\nof models on specialized hardware that supports fast parallel\ncomputing, can reduce resource requirements without signifi-\ncantly sacrificing performance. Combined, these methods can\nhelp researchers effectively apply deep learning models in\nresource-constrained environments, thereby expanding their\nfeasibility and utility in real-world applications.\n4.3. Accuracy And Generalization of The Model\nThe most advanced brain-to-text systems have achieved\na degree of precision in using neural networks to decode\nlanguage directly from brain signals. However, many current\nmodels are limited to small closed vocabularies, which are\nfar from sufficient for natural communication. Although\nDeWave[22] enhances EEG to text translation using dis-\ncrete codex and raw wave coding, its accuracy is still poor\ncompared to traditional language-to-language translation.\nIn addition, due to the limitations of data collection in\nEEG-to-text tasks, the maximum number of subjects in\nthe open dataset is 18 at one time[24]. Moreover, due to\nthe particularity of EEG data, there are great differences\namong different subjects, so the improvement of model\ngeneralization performance is a long-term research direction.\nFor example. for the fMRI-to-text tasks, how to align\nfMRI signals with individual words when the stimulus is\npresented as a continuous time series of words is the basis for\ntranslating brain activity into text[19]. For the EEG-to-text\ntasks, we can start from the perspective of word embedding\nalgorithm, compare the characteristics of different word\nembedding algorithms[56], and improve the accuracy and\ngeneralization performance of the model by taking advantage\nof generative model and machine translation model. We can\nalso start from the perspective of data sets, because different\nlanguages also have a great impact on the generalization\nperformance of the model of converting EEG data into text.\nThe future needs to collect larger EEG text data sets and\nextend the current framework to multilingual environments.\n4.4. Challenges for Real-time Processing\nIn human-computer interactive applications, brain-inspired\ncomputing models need to deal with the challenges of\nimmediacy and diversity brought by user brain signal input\nwhen combined with deep learning. First, in order to provide\na smooth user experience, the model must respond with\nextremely low latency times, which puts higher real-time\nrequirements on complex deep learning models. Secondly,\nhuman-computer interaction scenarios often require the\nmodel to process multi-modal and multi-channel data, such\nB.H.Yu et al.\nPage 18 of 26\n\nas the combination of EEG data and ophthalmic data, which\nincreases the computational complexity and the burden\nof data processing. In addition, these interactions often\ntake place in resource-constrained environments, such as\nsmartphones or wearables, which have limited computing\npower and memory, making it more difficult to deploy and\noperate models efficiently.\nPotential solutions to improve the real-time performance\nof brain-inspired computational models include a variety\nof strategies. First, model parameters can be reduced by\nusing model compression and optimization techniques such\nas knowledge distillation and pruning, effectively reducing\nthe computational burden and speeding up response times.\nSecondly, deploying edge computing can carry out part of\nthe data processing task on the terminal device, reducing the\ndelay of data transmission and achieving more immediate\nfeedback. Using dedicated hardware such as Gpus in mobile\ndevices or dedicated neural processing units can improve\nthe speed and efficiency of model execution. In addition,\nlightweight and efficient network architecture is designed,\nand asynchronous processing and pipeline optimization\nallow multiple data streams to be processed simultaneously.\nThis not only ensures efficient operation in multimodal\ninteractions, but also ensures persistent real-time responses,\nthereby enhancing the overall human-computer interaction\nexperience.\n4.5. Challenges for BCI Technology\nThe current development of human-computer interaction\nbased on brain-inspired computing is still far from achieving\nthe performance expected by human beings, and faces many\nchallenges: for example, a significant feature of brain signals\nis high-dimensional and multi-modal, how to obtain more\nbrain data is a difficulty, and the correspondence between\nbrain signals and motion, emotion, language, etc., needs to\nbe explored more.\nIn addition, the common non-invasive EEG signals have\nseveral problems: low signal-to-noise ratio, low reliability,\nand unstable signals. These challenges lead to the need for\nfurther exploration and research on efficient algorithms to\nimprove accuracy, solve the curse of dimensionality, and\nreduce the time cost.\nMoreover, BCI systems require users to be highly concen-\ntrated during training, which will lead to high workload of\nbrain-computer interaction such as visual fatigue. Meanwhile,\nthe volume and shape of brain-computer interface devices\nneed to be further miniaturized to make users more comfort-\nable and portable.\nFurthermore, the broadening of brain channels, the imple-\nmentation and performance improvement of brain-computer\ninterface technology give humans the ability to directly\ninterpret and control thoughts, which inevitably brings new\nchallenges to human privacy rights, and there is also the risk\nof hacking, which touches the ethical bottom line.\n4.6. Challenges for Ethical Concerns\nThe ethical problems of brain-inspired computing models\nmainly focus on several aspects. First, the privacy protection\nof physiological signal data is a key issue, because these\nmodels need to process a large amount of sensitive brain\nsignal data, which can lead to personal privacy disclosure\nif improperly used or leaked. Second, algorithmic bias\nis also an important ethical consideration. When facing\nhuman-computer interaction scenarios, the model needs to\ntreat different user groups fairly, but if the training data is\nincomplete or biased, it may lead to discriminatory output\nand loss of user trust.\nIn addressing these ethical issues, a multifaceted strategy\ncan be adopted. First of all, in terms of data protection, encryp-\ntion technology and data anonymization are adopted to ensure\nthe security of user data during collection, transmission and\nstorage. In addition, strict data use protocols are implemented\nto limit access to and use of sensitive data. Secondly, in order\nto reduce algorithm bias, the model should be trained using\nmulti-subject data sets, and data enhancement techniques can\nalso be used appropriately to improve the robustness of the\nmodel. Finally, ethical guidelines are proposed and followed\nto ensure ethical compliance in technology development\nand application. Through these measures, we can ensure the\nethical and social responsibility of brain-inspired comput-\ning model while meeting the performance requirements of\nhuman-computer interaction.\nTo summarise, this section presents some current chal-\nlenges and corresponding solutions for brain-inspired com-\nputational models applied to human-computer interaction.\n5. FUTURE DIRECTIONS\nThe previous chapter summarizes some challenges faced\nby HCI for brain-inspired computing models and gives\nsome solutions. This chapter mainly summarizes possible\nfuture research directions. Inspired by the brain and the\ncontinuous progress and development of ML and DL models,\nthe performance of HCI for brain-inspired computing tasks\nand the complexity of tasks that can be solved have been\nimproved. In the future, more application scenarios can be\nbroadened, BCI systems across language boundaries can\nbe investigated, and paradigms for shifting neural network\narchitectures can be transformed.\n5.1. Landing Application of Brain-inspired\nComputing\nAt present, brain-inspired computing models based on\nmachine learning and deep learning have explored many tasks,\nsuch as binary classification tasks, information extraction\ntasks, and zero-sample emotion recognition tasks. One of\nthe future research directions is to explore more application\nscenarios of brain-inspired computing. For example, in the\nfields of medicine and psychology, people with speech\ndisorders can be helped to truly achieve silent electrical brain\ncommunication.\nThe research of human brain intelligence mechanism\ncan also promote the innovation and expansion of artificial\nintelligence. Nowadays, neuromorphic chip, brain-computer\ninterfaces, intelligent decision-making and other related\nB.H.Yu et al.\nPage 19 of 26\n\nresearch results are based on the extended development of\nbrain science and computing science research, these studies\nstill have a great space for development, and in the future\nmay even subvert the current technical cognition.We should\nactively promote the product to create greater value for\neconomic development and human social progress.\n5.2. BCIs across Language Boundaries\nBCI systems decode brain signals into text or speech to\nassist humans to interact with the outside world. Different\nlanguages have great differences in semantic understanding,\nsemantic alignment and representation. Nowadays, the ap-\nplication of both open datasets and existing deep learning\nmodels in brain-inspired computing is mostly based on\nEnglish. In fact, the application of BCI system should not be\nlimited to English-speaking countries. For example, China\nis a large country with a population of 1.4 billion, and\nthere is also a huge market demand for high-performance\nBCI systems. We hope that our suggestions can bring some\ninspiration to researchers to design a dedicated or even\nuniversal BCI system for humans of various languages in the\nworld which will bring good news to more people suffering\nfrom diseases.\nIn addition, we can consider the brain signal as an\nintermediate bridge for language translation. For example, for\nthe brain signal of the same thing, no matter what language the\nsignal comes from, we can extract the semantic information\ncarried by the brain signal itself, and then decode any required\nlanguage. This research can enable a greater degree of cross-\nlingual BCI.\n5.3. Spiking Neural Network(SNN)\nIn the past 10 years, artificial intelligence technology\nwith deep neural networks as the main thrust has made\ngreat progress. Based on this, that is, the broad definition of\nbrain-inspired computing, this paper summarizes the research\nprogress of brain-inspired computing models based on deep\nlearning and machine learning. In fact, as the most subtle\nintelligent organ in the human body, the human brain, only\noccupies 2% to 3% of the body weight, but almost controls\nall human behavior, so the performance of brain-inspired\ncomputing based on deep learning is far less than that of the\nhuman brain with about 100 billion neurons but very low\nconsumption.\nThe human brain possesses the remarkable ability to\nself-organize and coordinate various cognitive functions,\nallowing for flexible adaptation to changing environments. A\nsignificant challenge in the realms of artificial intelligence and\ncomputational neuroscience is the integration of multi-scale\nbiological principles to construct brain-inspired intelligent\nmodels[226]. SNN is a new generation of artificial neural\nnetwork inspired by biology, which is oriented by brain\nscience and develops along the direction of brain simulation.\nIt expresses information flow with 0/1 pulse sequence,\nthe encoding contains time information, and the internal\nneurons have dynamic characteristics, such as event-driven\nand sparse release. It converts input information into pulse\nsequence signals through pulse coding. And maintain the\ntime relationship between pulses during the information\ntransmission process, so neurons have microscopic memory\nproperties[227, 228].\nAs the third generation of neural networks[229], Spiking\nNeural Networks (SNNs) exhibit greater biological plausibil-\nity across multiple scales. This includes membrane potential,\nneuronal firing, synaptic transmission, synaptic plasticity,\nand the coordination of multiple brain areas. Crucially, SNNs\noffer enhanced biological interpret ability, increased energy\nefficiency, and a natural suitability for modeling diverse\ncognitive functions of the brain, making them well-suited for\nthe creation of brain-inspired AI[226].\nIn conclusion, the research on brain-inspired computing\nmodels applied to HCI is still in the preliminary stage, and\nthere are many directions worth exploring in the future. Cross-\nlingual Bcis are more practical, while SNNs that simulate\nmore biometric features are also a future trend.\n6. CONCLUSION\nWith the development of computer science, neuroscience,\nbrain cognitive science and psychology, the acquisition and\nanalysis of brain signals under various cognitive tasks has\nbecome an important intersection. Focusing on the human-\ncomputer interaction application scenarios of brain signal\ndecoding text and speech, this study deeply discusses the\nbrain-inspired computing models based on deep learning,\nsummarizes the research status, progress and hotspots of\nbrain-inspired computing for human-computer interaction,\nputs forward some existing limitations and gives some\nsolutions. At present, this field is still in the initial stage\nof development. With the continuous evolution of artificial\nintelligence technology such as deep learning and the in-depth\nintersection of multi-disciplinary fields such as brain science,\nwe believe that new breakthroughs can be continuously made.\nTherefore, we advocate continuing to pay attention to the\nco-evolution between multi-disciplines, and sincerely hope\nthat this review can bring some inspiration to researchers\nin the field of human-computer interaction of brain-inspired\ncomputing and promote future innovation and progress.\nReferences\n[1] Furber, S.B.: Brain-inspired computing. IET Computers & Digital\nTechniques 10(6), 299–305 (2016)\n[2] Zhang, Y., Qu, P., Ji, Y., Zhang, W., Gao, G., Wang, G., Song, S., Li,\nG., Chen, W., Zheng, W., et al.: A system hierarchy for brain-inspired\ncomputing. Nature 586(7829), 378–384 (2020)\n[3] Parhi, K.K., Unnikrishnan, N.K.: Brain-inspired computing: Models\nand architectures. IEEE Open Journal of Circuits and Systems 1,\n185–204 (2020)\n[4] Mehonic, A., Kenyon, A.J.: Brain-inspired computing needs a master\nplan. Nature 604(7905), 255–260 (2022)\n[5] Squire, L.R., Zola-Morgan, S.: Memory: brain systems and behavior.\nTrends in neurosciences 11(4), 170–175 (1988)\n[6] Webb, G.I., Keogh, E., Miikkulainen, R.: Naïve bayes. Encyclopedia\nof machine learning 15(1), 713–714 (2010)\n[7] Noble, W.S.: What is a support vector machine? Nature biotechnology\n24(12), 1565–1567 (2006)\n[8] Peterson, L.E.: K-nearest neighbor. Scholarpedia 4(2), 1883 (2009)\nB.H.Yu et al.\nPage 20 of 26\n\n[9] Cox, D.R.: The regression analysis of binary sequences. Journal of\nthe Royal Statistical Society Series B: Statistical Methodology 20(2),\n215–232 (1958)\n[10] Breiman, L.: Random forests. Machine learning 45, 5–32 (2001)\n[11] Balakrishnama, S., Ganapathiraju, A.: Linear discriminant analysis-\na brief tutorial. Institute for Signal and information Processing\n18(1998), 1–8 (1998)\n[12] Wold, S., Esbensen, K., Geladi, P.: Principal component analysis.\nChemometrics and intelligent laboratory systems 2(1-3), 37–52\n(1987)\n[13] Shibata, R.: Selection of the order of an autoregressive model by\nakaike’s information criterion. Biometrika 63(1), 117–126 (1976)\n[14] Nussbaumer, H.J., Nussbaumer, H.J.: The Fast Fourier Transform.\nSpringer, ??? (1982)\n[15] Farge, M.: Wavelet transforms and their applications to turbulence.\nAnnual review of fluid mechanics 24(1), 395–458 (1992)\n[16] Schirrmeister, R.T., Springenberg, J.T., Fiederer, L.D.J., Glasstetter,\nM., Eggensperger, K., Tangermann, M., Hutter, F., Burgard, W.,\nBall, T.: Deep learning with convolutional neural networks for eeg\ndecoding and visualization. Human brain mapping 38(11), 5391–\n5420 (2017)\n[17] Alhagry, S., Fahmy, A.A., El-Khoribi, R.A.: Emotion recognition\nbased on eeg using lstm recurrent neural network. International\nJournal of Advanced Computer Science and Applications 8(10)\n(2017)\n[18] Sakhavi, S., Guan, C., Yan, S.: Learning temporal information for\nbrain-computer interface using convolutional neural networks. IEEE\ntransactions on neural networks and learning systems 29(11), 5619–\n5629 (2018)\n[19] Zou, S., Wang, S., Zhang, J., Zong, C.: Cross-modal cloze task: A\nnew task to brain-to-word decoding. In: Findings of the Association\nfor Computational Linguistics: ACL 2022, pp. 648–657 (2022)\n[20] Wang, Z., Ji, H.: Open vocabulary electroencephalography-to-text\ndecoding and zero-shot sentiment classification. In: Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol. 36, pp. 5350–\n5358 (2022)\n[21] Feng, X., Feng, X., Qin, B., Liu, T.: Aligning semantic\nin brain and language: A curriculum contrastive method for\nelectroencephalography-to-text generation. IEEE Transactions on\nNeural Systems and Rehabilitation Engineering (2023)\n[22] Duan, Y., Zhou, J., Wang, Z., Wang, Y.-K., Lin, C.-T.: Dewave:\nDiscrete eeg waves encoding for brain dynamics to text translation.\narXiv preprint arXiv:2309.14030 (2023)\n[23] Hollenstein, N., Rotsztejn, J., Troendle, M., Pedroni, A., Zhang, C.,\nLanger, N.: Zuco, a simultaneous eeg and eye-tracking resource for\nnatural sentence reading. Scientific data 5(1), 1–13 (2018)\n[24] Hollenstein, N., Troendle, M., Zhang, C., Langer, N.: Zuco 2.0:\nA dataset of physiological recordings during natural reading and\nannotation. arXiv preprint arXiv:1912.00903 (2019)\n[25] Mitchell, T.M., Shinkareva, S.V., Carlson, A., Chang, K.-M., Malave,\nV.L., Mason, R.A., Just, M.A.: Predicting human brain activity\nassociated with the meanings of nouns. science 320(5880), 1191–\n1195 (2008)\n[26] Pereira, F., Lou, B., Pritchett, B., Ritter, S., Gershman, S.J., Kan-\nwisher, N., Botvinick, M., Fedorenko, E.: Toward a universal decoder\nof linguistic meaning from brain activation. Nature communications\n9(1), 963 (2018)\n[27] Krishna, G., Han, Y., Tran, C., Carnahan, M., Tewfik, A.H.: State-of-\nthe-art speech recognition using eeg and towards decoding of speech\nspectrum from eeg. arXiv preprint arXiv:1908.05743 (2019)\n[28] Cooney, C., Korik, A., Folli, R., Coyle, D.: Evaluation of hyperparam-\neter optimization in machine and deep learning methods for decoding\nimagined speech eeg. Sensors 20(16), 4629 (2020)\n[29] Lee, Y.-E., Lee, S.-H.: Eeg-transformer: Self-attention from trans-\nformer architecture for decoding eeg of imagined speech. In: 2022\n10th International Winter Conference on Brain-Computer Interface\n(BCI), pp. 1–4 (2022). IEEE\n[30] Jain, S., Huth, A.: Incorporating context into language encoding\nmodels for fmri. Advances in neural information processing systems\n31 (2018)\n[31] Affolter, N., Egressy, B., Pascual, D., Wattenhofer, R.: Brain2word:\nImproving brain decoding methods and evaluation. In: Medical Imag-\ning Meets Neurips Workshop-34th Conference on Neural Information\nProcessing Systems (2020)\n[32] Jain, S., Vo, V., Mahto, S., LeBel, A., Turek, J.S., Huth, A.:\nInterpretable multi-timescale models for predicting fmri responses\nto continuous natural speech. Advances in Neural Information\nProcessing Systems 33, 13738–13749 (2020)\n[33] Lin, Y., Hsieh, P.-J.: Neural decoding of speech with semantic-based\nclassification. Cortex 154, 231–240 (2022)\n[34] Millet, J., Caucheteux, C., Boubenec, Y., Gramfort, A., Dunbar, E.,\nPallier, C., King, J.-R., et al.: Toward a realistic model of speech\nprocessing in the brain with self-supervised learning. Advances in\nNeural Information Processing Systems 35, 33428–33443 (2022)\n[35] Xi, N., Zhao, S., Wang, H., Liu, C., Qin, B., Liu, T.: Unicorn: Unified\ncognitive signal reconstruction bridging cognitive signals and human\nlanguage. arXiv preprint arXiv:2307.05355 (2023)\n[36] Tang, J., LeBel, A., Jain, S., Huth, A.G.: Semantic reconstruction\nof continuous language from non-invasive brain recordings. Nature\nNeuroscience, 1–9 (2023)\n[37] Dash, D., Ferrari, P., Malik, S., Wang, J.: Automatic speech activity\nrecognition from meg signals using seq2seq learning. In: 2019 9th\nInternational IEEE/EMBS Conference on Neural Engineering (NER),\npp. 340–343 (2019). IEEE\n[38] Dash, D., Ferrari, P., Heitzman, D., Wang, J.: Decoding speech from\nsingle trial meg signals using convolutional neural networks and\ntransfer learning. In: 2019 41st Annual International Conference of\nthe IEEE Engineering in Medicine and Biology Society (EMBC), pp.\n5531–5535 (2019). IEEE\n[39] Dash, D., Ferrari, P., Wang, J.: Decoding imagined and spoken\nphrases from non-invasive neural (meg) signals. Frontiers in neu-\nroscience 14, 290 (2020)\n[40] Dash, D., Wisler, A., Ferrari, P., Davenport, E.M., Maldjian, J., Wang,\nJ.: Meg sensor selection for neural speech decoding. IEEE Access 8,\n182320–182337 (2020)\n[41] Défossez, A., Caucheteux, C., Rapin, J., Kabeli, O., King, J.-R.:\nDecoding speech perception from non-invasive brain recordings.\nNature Machine Intelligence, 1–11 (2023)\n[42] Makin, J.G., Moses, D.A., Chang, E.F.: Machine translation of\ncortical activity to text with an encoder–decoder framework. Nature\nneuroscience 23(4), 575–582 (2020)\n[43] Sun, P., Anumanchipalli, G.K., Chang, E.F.: Brain2char: a deep\narchitecture for decoding text from brain recordings. Journal of neural\nengineering 17(6), 066015 (2020)\n[44] Chen, X., Wang, R., Khalilian-Gourtani, A., Yu, L., Dugan, P.,\nFriedman, D., Doyle, W., Devinsky, O., Wang, Y., Flinker, A.: A\nneural speech decoding framework leveraging deep learning and\nspeech synthesis. bioRxiv, 2023–09 (2023)\n[45] Berezutskaya, J., Freudenburg, Z.V., Vansteensel, M.J., Aarnoutse,\nE.J., Ramsey, N.F., Gerven, M.A.: Direct speech reconstruction from\nsensorimotor brain activity with optimized deep learning models.\nJournal of Neural Engineering 20(5), 056010 (2023)\n[46] Shigemi, K., Komeiji, S., Mitsuhashi, T., Iimura, Y., Suzuki, H.,\nSugano, H., Shinoda, K., Yatabe, K., Tanaka, T.: Synthesizing\nspeech from ecog with a combination of transformer-based encoder\nand neural vocoder. In: ICASSP 2023-2023 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP),\npp. 1–5 (2023). IEEE\n[47] Daly, J.J., Wolpaw, J.R.: Brain–computer interfaces in neurological\nrehabilitation. The Lancet Neurology 7(11), 1032–1043 (2008)\n[48] Qu, P., Yang, L., Zheng, W., Zhang, Y.: A review of basic software for\nbrain-inspired computing. CCF Transactions on High Performance\nComputing 4(1), 34–42 (2022)\n[49] Hendy, H., Merkel, C.: Review of spike-based neuromorphic com-\nputing for brain-inspired vision: biology, algorithms, and hardware.\nJournal of Electronic Imaging 31(1), 010901–010901 (2022)\nB.H.Yu et al.\nPage 21 of 26\n\n[50] Liu, F., Zheng, H., Ma, S., Zhang, W., Liu, X., Chua, Y., Shi, L.,\nZhao, R.: Advancing brain-inspired computing with hybrid neural\nnetworks. National Science Review, 066 (2024)\n[51] Son, J., Mishra, A.K.: A survey of brain inspired technologies for\nengineering. In: 2016 Pattern Recognition Association of South\nAfrica and Robotics and Mechatronics International Conference\n(PRASA-RobMech), pp. 1–6 (2016). IEEE\n[52] Park, T.J., Deng, S., Manna, S., Islam, A.N., Yu, H., Yuan, Y., Fong,\nD.D., Chubykin, A.A., Sengupta, A., Sankaranarayanan, S.K., et al.:\nComplex oxides for brain-inspired computing: A review. Advanced\nMaterials 35(37), 2203352 (2023)\n[53] Li, G., Deng, L., Tang, H., Pan, G., Tian, Y., Roy, K., Maass, W.:\nBrain inspired computing: A systematic survey and future trends.\nAuthorea Preprints (2023)\n[54] Gu, X., Cao, Z., Jolfaei, A., Xu, P., Wu, D., Jung, T.-P., Lin, C.-\nT.: Eeg-based brain-computer interfaces (bcis): A survey of recent\nstudies on signal sensing technologies and computational intelligence\napproaches and their applications. IEEE/ACM transactions on com-\nputational biology and bioinformatics 18(5), 1645–1666 (2021)\n[55] Kalagi, S., Machado, J., Carvalho, V., Soares, F., Matos, D.: Brain\ncomputer interface systems using non-invasive electroencephalogram\nsignal: A literature review. In: 2017 International Conference on\nEngineering, Technology and Innovation (ICE/ITMC), pp. 1578–\n1583 (2017). IEEE\n[56] Hollenstein, N., Torre, A., Langer, N., Zhang, C.: Cognival: A\nframework for cognitive word embedding evaluation. arXiv preprint\narXiv:1909.09001 (2019)\n[57] Wang, B., Fu, X., Lan, Y., Zhang, L., Xiang, Y.: Large transformers\nare better eeg learners. arXiv preprint arXiv:2308.11654 (2023)\n[58] Smith, S.M.: Overview of fmri analysis. The British Journal of\nRadiology 77(suppl_2), 167–175 (2004)\n[59] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee,\nK., Zettlemoyer, L.: Deep contextualized word representations. In:\nProceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pp. 2227–2237. Association\nfor Computational Linguistics, New Orleans, Louisiana (2018). https:\n//doi.org/10.18653/v1/N18-1202 . https://aclanthology.org/N18-\n1202\n[60] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances\nin neural information processing systems 30 (2017)\n[61] Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: Bert: Pre-training\nof deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018)\n[62] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et\nal.: Language models are unsupervised multitask learners. OpenAI\nblog 1(8), 9 (2019)\n[63] Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A.,\nLevy, O., Stoyanov, V., Zettlemoyer, L.: Bart: Denoising sequence-\nto-sequence pre-training for natural language generation, translation,\nand comprehension. arXiv preprint arXiv:1910.13461 (2019)\n[64] Gürkök, H., Nijholt, A.: Brain–computer interfaces for multimodal\ninteraction: a survey and principles. International Journal of Human-\nComputer Interaction 28(5), 292–307 (2012)\n[65] Mridha, M.F., Das, S.C., Kabir, M.M., Lima, A.A., Islam, M.R.,\nWatanobe, Y.: Brain-computer interface: Advancement and chal-\nlenges. Sensors 21(17), 5746 (2021)\n[66] Zhang, Y., Zhou, G., Jin, J., Wang, X., Cichocki, A.: Ssvep recognition\nusing common feature analysis in brain–computer interface. Journal\nof neuroscience methods 244, 8–15 (2015)\n[67] Luo, A., Sullivan, T.J.: A user-friendly ssvep-based brain–computer\ninterface using a time-domain classifier. Journal of neural engineering\n7(2), 026010 (2010)\n[68] Fazel-Rezai, R., Allison, B.Z., Guger, C., Sellers, E.W., Kleih, S.C.,\nKübler, A.: P300 brain computer interface: current challenges and\nemerging trends. Frontiers in neuroengineering, 14 (2012)\n[69] Fouad, I.A., Labib, F.E.-Z.M., Mabrouk, M.S., Sharawy, A.A., Sayed,\nA.Y.: Improving the performance of p300 bci system using different\nmethods. Network Modeling Analysis in Health Informatics and\nBioinformatics 9, 1–13 (2020)\n[70] Hamedi, M., Salleh, S.-H., Noor, A.M.: Electroencephalographic\nmotor imagery brain connectivity analysis for bci: a review. Neural\ncomputation 28(6), 999–1041 (2016)\n[71] Jin, J., Miao, Y., Daly, I., Zuo, C., Hu, D., Cichocki, A.: Correlation-\nbased channel selection and regularized feature optimization for mi-\nbased bci. Neural Networks 118, 262–270 (2019)\n[72] Fiscon, G., Weitschek, E., Cialini, A., Felici, G., Bertolazzi, P.,\nDe Salvo, S., Bramanti, A., Bramanti, P., De Cola, M.C.: Combining\neeg signal processing with supervised methods for alzheimer’s\npatients classification. BMC medical informatics and decision making\n18(1), 1–10 (2018)\n[73] Saeidi, M., Karwowski, W., Farahani, F.V., Fiok, K., Taiar, R.,\nHancock, P., Al-Juaid, A.: Neural decoding of eeg signals with\nmachine learning: A systematic review. Brain Sciences 11(11), 1525\n(2021)\n[74] Tan, P.-N., Steinbach, M., Kumar, V.: Classification: basic concepts,\ndecision trees, and model evaluation. Introduction to data mining 1,\n145–205 (2006)\n[75] Rakshit, A., Khasnobish, A., Tibarewala, D.: A naïve bayesian\napproach to lower limb classification from eeg signals. In: 2016\n2nd International Conference on Control, Instrumentation, Energy &\nCommunication (CIEC), pp. 140–144 (2016). IEEE\n[76] Hosseini, M.-P., Hosseini, A., Ahi, K.: A review on machine\nlearning for eeg signal processing in bioengineering. IEEE reviews\nin biomedical engineering 14, 204–218 (2020)\n[77] Machado, J., Balbinot, A., Schuck, A.: A study of the naive bayes\nclassifier for analyzing imaginary movement eeg signals using the\nperiodogram as spectral estimator. In: 2013 ISSNIP Biosignals and\nBiorobotics Conference: Biosignals and Robotics for Better and Safer\nLiving (BRC), pp. 1–4 (2013). IEEE\n[78] He, L., Hu, D., Wan, M., Wen, Y., Von Deneen, K.M., Zhou,\nM.: Common bayesian network for classification of eeg-based\nmulticlass motor imagery bci. IEEE Transactions on Systems, man,\nand cybernetics: systems 46(6), 843–854 (2015)\n[79] Oktavia, N.Y., Wibawa, A.D., Pane, E.S., Purnomo, M.H.: Human\nemotion classification based on eeg signals using naïve bayes method.\nIn: 2019 International Seminar on Application for Technology of\nInformation and Communication (iSemantic), pp. 319–324 (2019).\nIEEE\n[80] Lestari, F.P., Haekal, M., Edison, R.E., Fauzy, F.R., Khotimah, S.N.,\nHaryanto, F.: Epileptic seizure detection in eegs by using random\ntree forest, naïve bayes and knn classification. In: Journal of Physics:\nConference Series, vol. 1505, p. 012055 (2020). IOP Publishing\n[81] Reshmi, G., Amal, A.: Design of a bci system for piloting a wheelchair\nusing five class mi based eeg. In: 2013 Third International Conference\non Advances in Computing and Communications, pp. 25–28 (2013).\nIEEE\n[82] Wang, H., Zhang, Y., et al.: Detection of motor imagery eeg signals\nemploying naïve bayes based learning process. Measurement 86,\n148–158 (2016)\n[83] Sagee, G., Hema, S.: Eeg feature extraction and classification in\nmulticlass multiuser motor imagery brain computer interface u sing\nbayesian network and ann. In: 2017 International Conference on\nIntelligent Computing, Instrumentation and Control Technologies\n(ICICICT), pp. 938–943 (2017). IEEE\n[84] Isa, N.M., Amir, A., Ilyas, M., Razalli, M.: Motor imagery classifica-\ntion in brain computer interface (bci) based on eeg signal by using\nmachine learning technique. Bulletin of Electrical Engineering and\nInformatics 8(1), 269–275 (2019)\n[85] Aggarwal, S., Chugh, N.: Signal processing techniques for motor\nimagery brain computer interface: A review. Array 1, 100003 (2019)\n[86] Lal, T.N., Schroder, M., Hinterberger, T., Weston, J., Bogdan, M.,\nBirbaumer, N., Scholkopf, B.: Support vector channel selection in\nbci. IEEE transactions on biomedical engineering 51(6), 1003–1010\n(2004)\nB.H.Yu et al.\nPage 22 of 26\n\n[87] Arbabi, E., Shamsollahi, M., Sameni, R.: Comparison between\neffective features used for the bayesian and the svm classifiers in bci.\nIn: 2005 IEEE Engineering in Medicine and Biology 27th Annual\nConference, pp. 5365–5368 (2006). IEEE\n[88] Li, Y., Guan, C., Li, H., Chin, Z.: A self-training semi-supervised svm\nalgorithm and its application in an eeg-based brain computer interface\nspeller system. Pattern Recognition Letters 29(9), 1285–1294 (2008)\n[89] Hortal, E., Úbeda, A., Iáñez, E., Planelles, D., Azorin, J.M.: Online\nclassification of two mental tasks using a svm-based bci system.\nIn: 2013 6th International IEEE/EMBS Conference on Neural\nEngineering (NER), pp. 1307–1310 (2013). IEEE\n[90] Hou, H.-R., Meng, Q.-H., Zeng, M., Sun, B.: Improving classification\nof slow cortical potential signals for bci systems with polynomial\nfitting and voting support vector machine. IEEE Signal Processing\nLetters 25(2), 283–287 (2017)\n[91] Palaniappan, R., Sundaraj, K., Sundaraj, S.: A comparative study\nof the svm and k-nn machine learning algorithms for the diagnosis\nof respiratory pathologies using pulmonary acoustic signals. BMC\nbioinformatics 15, 1–8 (2014)\n[92] Osowski, S., Siwek, K., Markiewicz, T.: Mlp and svm networks-\na comparative study. In: Proceedings of the 6th Nordic Signal\nProcessing Symposium, 2004. NORSIG 2004., pp. 37–40 (2004).\nIEEE\n[93] Moctezuma, L.A., Molinas, M.: Classification of low-density eeg\nfor epileptic seizures by energy and fractal features based on emd.\nJournal of biomedical research 34(3), 180 (2020)\n[94] Trambaiolli, L.R., Lorena, A.C., Fraga, F.J., Kanda, P.A., Anghinah,\nR., Nitrini, R.: Improving alzheimer’s disease diagnosis with machine\nlearning techniques. Clinical EEG and neuroscience 42(3), 160–165\n(2011)\n[95] Murugavel, A.M., Ramakrishnan, S.: Hierarchical multi-class svm\nwith elm kernel for epileptic eeg signal classification. Medical &\nbiological engineering & computing 54, 149–161 (2016)\n[96] Hortal, E., Planelles, D., Costa, A., Iánez, E., Úbeda, A., Azorín, J.M.,\nFernández, E.: Svm-based brain–machine interface for controlling a\nrobot arm through four mental tasks. Neurocomputing 151, 116–121\n(2015)\n[97] Mehmood, R.M., Lee, H.J.: Emotion classification of eeg brain signal\nusing svm and knn. In: 2015 IEEE International Conference on\nMultimedia & Expo Workshops (ICMEW), pp. 1–5 (2015). IEEE\n[98] Du, S.-C., Huang, D.-L., Wang, H.: An adaptive support vector\nmachine-based workpiece surface classification system using high-\ndefinition metrology. IEEE Transactions on Instrumentation and\nMeasurement 64(10), 2590–2604 (2015)\n[99] Yazdani, A., Ebrahimi, T., Hoffmann, U.: Classification of eeg\nsignals using dempster shafer theory and a k-nearest neighbor\nclassifier. In: 2009 4th International IEEE/EMBS Conference on\nNeural Engineering, pp. 327–330 (2009). IEEE\n[100] Rajini, N.H., Bhavani, R.: Classification of mri brain images using k-\nnearest neighbor and artificial neural network. In: 2011 International\nConference on Recent Trends in Information Technology (ICRTIT),\npp. 563–568 (2011). IEEE\n[101] Awan, U.I., Rajput, U., Syed, G., Iqbal, R., Sabat, I., Mansoor, M.:\nEffective classification of eeg signals using k-nearest neighbor algo-\nrithm. In: 2016 International Conference on Frontiers of Information\nTechnology (FIT), pp. 120–124 (2016). IEEE\n[102] Yudhana, A., Muslim, A., Wati, D.E., Puspitasari, I., Azhari, A.,\nMardhia, M.M.: Human emotion recognition based on eeg signal\nusing fast fourier transform and k-nearest neighbor. Adv. Sci. Technol.\nEng. Syst. J 5(6), 1082–1088 (2020)\n[103] Beyer, K., Goldstein, J., Ramakrishnan, R., Shaft, U.: When is\n“nearest neighbor” meaningful? In: Database Theory—ICDT’99:\n7th International Conference Jerusalem, Israel, January 10–12, 1999\nProceedings 7, pp. 217–235 (1999). Springer\n[104] Ryali, S., Supekar, K., Abrams, D.A., Menon, V.: Sparse logistic\nregression for whole-brain classification of fmri data. NeuroImage\n51(2), 752–764 (2010)\n[105] Zeng, H., Song, A.: Optimizing single-trial eeg classification by\nstationary matrix logistic regression in brain–computer interface.\nIEEE transactions on neural networks and learning systems 27(11),\n2301–2313 (2015)\n[106] Tomioka, R., Aihara, K., Müller, K.-R.: Logistic regression for single\ntrial eeg classification. Advances in neural information processing\nsystems 19 (2006)\n[107] Fiebig, K.-H., Jayaram, V., Peters, J., Grosse-Wentrup, M.: Multi-\ntask logistic regression in brain-computer interfaces. In: 2016 IEEE\nInternational Conference on Systems, Man, and Cybernetics (SMC),\npp. 002307–002312 (2016). IEEE\n[108] Siuly, S., Li, Y., Zhang, Y., Siuly, S., Li, Y., Zhang, Y.: Cross-\ncorrelation aided logistic regression model for the identification of\nmotor imagery eeg signals in bci applications. EEG Signal Analysis\nand Classification: Techniques and Applications, 153–172 (2016)\n[109] Miladinović, A., Ajčević, M., Battaglini, P.P., Silveri, G., Ciacchi,\nG., Morra, G., Jarmolowska, J., Accardo, A.: Slow cortical potential\nbci classification using sparse variational bayesian logistic regression\nwith automatic relevance determination. In: XV Mediterranean\nConference on Medical and Biological Engineering and Computing–\nMEDICON 2019: Proceedings of MEDICON 2019, September 26-28,\n2019, Coimbra, Portugal, pp. 1853–1860 (2020). Springer\n[110] Ramzan, M., Dawn, S.: Learning-based classification of valence\nemotion from electroencephalography. International Journal of Neu-\nroscience 129(11), 1085–1093 (2019)\n[111] Edla, D.R., Mangalorekar, K., Dhavalikar, G., Dodia, S.: Classifica-\ntion of eeg data for human mental state analysis using random forest\nclassifier. Procedia computer science 132, 1523–1532 (2018)\n[112] Antoniou, E., Bozios, P., Christou, V., Tzimourta, K.D., Kalafatakis,\nK., G. Tsipouras, M., Giannakeas, N., Tzallas, A.T.: Eeg-based eye\nmovement recognition using brain–computer interface and random\nforests. Sensors 21(7), 2339 (2021)\n[113] Kumar, J.L.M., Rashid, M., Musa, R.M., Razman, M.A.M., Sulaiman,\nN., Jailani, R., Majeed, A.P.A.: The classification of eeg-based\nwinking signals: a transfer learning and random forest pipeline. PeerJ\n9, 11182 (2021)\n[114] Peña, D.M.C., Miranda, M.V., Pineda, L.V., García, C.A.R., Guzmán,\nA.S.: Eeg signal-based eye blink classifier using random forest for\nbci systems. In: 2022 IEEE International Conference on Engineering\nVeracruz (ICEV), pp. 1–5 (2022). IEEE\n[115] Nayak, D.R., Dash, R., Majhi, B.: Brain mr image classification\nusing two-dimensional discrete wavelet transform and adaboost with\nrandom forests. Neurocomputing 177, 188–197 (2016)\n[116] Anitha, R., Siva Sundhara Raja, D.: Development of computer-aided\napproach for brain tumor detection using random forest classifier.\nInternational Journal of Imaging Systems and Technology 28(1),\n48–53 (2018)\n[117] Okumuş, H., Aydemır, Ö.: Random forest classification for brain\ncomputer interface applications. In: 2017 25th Signal Processing and\nCommunications Applications Conference (SIU), pp. 1–4 (2017).\nIEEE\n[118] Bandos, T.V., Bruzzone, L., Camps-Valls, G.: Classification of\nhyperspectral images with regularized linear discriminant analysis.\nIEEE Transactions on Geoscience and Remote Sensing 47(3), 862–\n873 (2009)\n[119] Tharwat, A., Gaber, T., Ibrahim, A., Hassanien, A.E.: Linear discrimi-\nnant analysis: A detailed tutorial. AI communications 30(2), 169–190\n(2017)\n[120] Lotte, F., Congedo, M., Lécuyer, A., Lamarche, F., Arnaldi, B.: A\nreview of classification algorithms for eeg-based brain–computer\ninterfaces. Journal of neural engineering 4(2), 1 (2007)\n[121] Gareis, I.E., Acevedo, R.C., Atum, Y.V., Gentiletti, G.G., Banuelos,\nV.M., Rufiner, H.L.: Determination of an optimal training strategy\nfor a bci classification task with lda. In: 2011 5th International\nIEEE/EMBS Conference on Neural Engineering, pp. 286–289 (2011).\nIEEE\n[122] Xu, P., Yang, P., Lei, X., Yao, D.: An enhanced probabilistic lda for\nmulti-class brain computer interface. PloS one 6(1), 14634 (2011)\n[123] Ishfaque, A., Awan, A.J., Rashid, N., Iqbal, J.: Evaluation of ann,\nB.H.Yu et al.\nPage 23 of 26\n\nlda and decision trees for eeg based brain computer interface. In:\n2013 IEEE 9th International Conference on Emerging Technologies\n(ICET), pp. 1–6 (2013). IEEE\n[124] Rashid, M., Sulaiman, N., Mustafa, M., Khatun, S., Bari, B.S.:\nThe classification of eeg signal using different machine learning\ntechniques for bci application. In: Robot Intelligence Technology\nand Applications: 6th International Conference, RiTA 2018, Kuala\nLumpur, Malaysia, December 16–18, 2018, Revised Selected Papers\n6, pp. 207–221 (2019). Springer\n[125] Molla, M.K.I., Saha, S.K., Yasmin, S., Islam, M.R., Shin, J.: Trial\nregeneration with subband signals for motor imagery classification\nin bci paradigm. IEEE Access 9, 7632–7642 (2021)\n[126] Bostanov, V.: Bci competition 2003-data sets ib and iib: feature\nextraction from event-related brain potentials with the continuous\nwavelet transform and the t-value scalogram. IEEE Transactions on\nBiomedical engineering 51(6), 1057–1061 (2004)\n[127] Xia, M., Song, S., Yao, L., Long, Z.: An empirical comparison\nof different lda methods in fmri-based brain states decoding. Bio-\nMedical Materials and Engineering 26(s1), 1185–1192 (2015)\n[128] Scherer, R., Muller, G., Neuper, C., Graimann, B., Pfurtscheller,\nG.: An asynchronously controlled eeg-based virtual keyboard: im-\nprovement of the spelling rate. IEEE Transactions on Biomedical\nEngineering 51(6), 979–984 (2004)\n[129] Garcia, G.N., Ebrahimi, T., Vesin, J.-M.: Support vector eeg classi-\nfication in the fourier and time-frequency correlation domains. In:\nFirst International IEEE EMBS Conference on Neural Engineering,\n2003. Conference Proceedings., pp. 591–594 (2003). IEEE\n[130] Kottaimalai, R., Rajasekaran, M.P., Selvam, V., Kannapiran, B.: Eeg\nsignal classification using principal component analysis with neural\nnetwork in brain computer interface applications. In: 2013 IEEE\nInternational Conference on Emerging Trends in Computing, Com-\nmunication and Nanotechnology (ICECCN), pp. 227–231 (2013).\nIEEE\n[131] Yu, X., Chum, P., Sim, K.-B.: Analysis the effect of pca for feature\nreduction in non-stationary eeg based motor imagery of bci system.\nOptik 125(3), 1498–1502 (2014)\n[132] Vijay, K., Selvakumar, K.: Brain fmri clustering using interaction\nk-means algorithm with pca. In: 2015 International Conference on\nCommunications and Signal Processing (ICCSP), pp. 0909–0913\n(2015). IEEE\n[133] Ilyas, M.Z., Saad, P., Ahmad, M.I.: A survey of analysis and\nclassification of eeg signals for brain-computer interfaces. In: 2015\n2nd International Conference on Biomedical Engineering (ICoBE),\npp. 1–6 (2015). IEEE\n[134] Wang, J., Xu, G., Wang, L., Zhang, H.: Feature extraction of\nbrain-computer interface based on improved multivariate adaptive\nautoregressive models. In: 2010 3rd International Conference on\nBiomedical Engineering and Informatics, vol. 2, pp. 895–898 (2010).\nIEEE\n[135] Hettiarachchi, I.T., Nguyen, T.T., Nahavandi, S.: Multivariate adap-\ntive autoregressive modeling and kalman filtering for motor imagery\nbci. In: 2015 IEEE International Conference on Systems, Man, and\nCybernetics, pp. 3164–3168 (2015). IEEE\n[136] Bufalari, S., Mattia, D., Babiloni, F., Mattiocco, M., Marciani, M.G.,\nCincotti, F.: Autoregressive spectral analysis in brain computer\ninterface context. In: 2006 International Conference of the IEEE\nEngineering in Medicine and Biology Society, pp. 3736–3739 (2006).\nIEEE\n[137] Garg, R., Cecchi, G.A., Rao, A.R.: Full-brain auto-regressive model-\ning (farm) using fmri. Neuroimage 58(2), 416–441 (2011)\n[138] Ting, C.-M., Seghouane, A.-K., Salleh, S.-H., Noor, A.M.: Estimating\neffective connectivity from fmri data using factor-based subspace\nautoregressive models. IEEE Signal Processing Letters 22(6), 757–\n761 (2014)\n[139] Winograd, S.: On computing the discrete fourier transform. Mathe-\nmatics of computation 32(141), 175–199\n[140] Djamal, E.C., Abdullah, M.Y., Renaldi, F.: Brain computer interface\ngame controlling using fast fourier transform and learning vector\nquantization. Journal of Telecommunication, Electronic and Com-\nputer Engineering (JTEC) 9(2-5), 71–74 (2017)\n[141] Amin, H.U., Hafeez, Y., Reza, M.F., Adil, S.H., Hasan, R.A.,\nAli, S.S.A.: Eeg feature extraction with fast fourier transform for\ninvestigating different brain regions in cognitive and reasoning\nactivity. In: 2022 IEEE 5th International Symposium in Robotics\nand Manufacturing Automation (ROMA), pp. 1–4 (2022). IEEE\n[142] Djamal, E.C., Indrawan, R., Pratama, J., Renaldi, F.: Eeg based\nneuropsychology of advertising video using fast fourier transform and\nsupport vector machine. Journal of Telecommunication, Electronic\nand Computer Engineering (JTEC) 9(3-7), 105–109 (2017)\n[143] Azim, M.R., Amin, M.S., Haque, S.A., Ambia, M.N., Shoeb, M.A.:\nof human sleep eeg signals using wavelet transform and fourier trans-\nform. In: 2010 2nd International Conference on Signal Processing\nSystems, vol. 3, pp. 3–701 (2010). IEEE\n[144] Akin, M.: Comparison of wavelet transform and fft methods in the\nanalysis of eeg signals. Journal of medical systems 26, 241–247\n(2002)\n[145] Kousarrizi, M.R.N., Ghanbari, A.A., Teshnehlab, M., Shorehdeli,\nM.A., Gharaviri, A.: Feature extraction and classification of eeg\nsignals using wavelet transform, svm and artificial neural networks\nfor brain computer interfaces. In: 2009 International Joint Conference\non Bioinformatics, Systems Biology and Intelligent Computing, pp.\n352–355 (2009). IEEE\n[146] Al Ghayab, H.R., Li, Y., Siuly, S., Abdulla, S.: A feature extraction\ntechnique based on tunable q-factor wavelet transform for brain signal\nclassification. Journal of neuroscience methods 312, 43–52 (2019)\n[147] Peng, Y., Wong, C.M., Wang, Z., Rosa, A.C., Wang, H.T., Wan, F.:\nFatigue detection in ssvep-bcis based on wavelet entropy of eeg. IEEE\nAccess 9, 114905–114913 (2021)\n[148] Huang, M., Wu, P., Liu, Y., Bi, L., Chen, H.: Application and contrast\nin brain-computer interface between hilbert-huang transform and\nwavelet transform. In: 2008 The 9th International Conference for\nYoung Computer Scientists, pp. 1706–1710 (2008). IEEE\n[149] Aydemir, O., Kayikcioglu, T., et al.: Wavelet transform based classi-\nfication of invasive brain computer interface data. Radioengineering\n20(1), 31–38 (2011)\n[150] Mohamed, E.A., Yusoff, M.Z.B., Selman, N.K., Malik, A.S.: Enhanc-\ning eeg signals in brain computer interface using wavelet transform.\nInternational Journal of Information and Electronics Engineering\n4(3), 234 (2014)\n[151] Khalaf, A., Sybeldon, M., Sejdic, E., Akcakaya, M.: A brain-computer\ninterface based on functional transcranial doppler ultrasound using\nwavelet transform and support vector machines. Journal of neuro-\nscience methods 293, 174–182 (2018)\n[152] Sur, S., Sinha, V.K.: Event-related potential: An overview. Industrial\npsychiatry journal 18(1), 70 (2009)\n[153] Rayner, K.: Eye movements in reading and information processing:\n20 years of research. Psychological bulletin 124(3), 372 (1998)\n[154] Just, M.A., Carpenter, P.A.: A theory of reading: from eye fixations\nto comprehension. Psychological review 87(4), 329 (1980)\n[155] Hollenstein, N.: Leveraging cognitive processing signals for natural\nlanguage understanding. PhD thesis, ETH Zurich (2021)\n[156] Hauk, O., Pulvermüller, F.: Effects of word length and frequency on\nthe human event-related potential. Clinical Neurophysiology 115(5),\n1090–1103 (2004)\n[157] Broderick, M.P., Anderson, A.J., Di Liberto, G.M., Crosse, M.J.,\nLalor, E.C.: Electrophysiological correlates of semantic dissimilarity\nreflect the comprehension of natural, narrative speech. Current\nBiology 28(5), 803–809 (2018)\n[158] Ettinger, A., Feldman, N., Resnik, P., Phillips, C.: Modeling n400\namplitude using vector space models of word representation. In:\nCogSci (2016)\n[159] Frank, S.L., Fernandez Monsalve, I., Thompson, R.L., Vigliocco, G.:\nReading time data for evaluating broad-coverage models of english\nsentence processing. Behavior research methods 45, 1182–1190\n(2013)\nB.H.Yu et al.\nPage 24 of 26\n\n[160] Wehbe, L., Vaswani, A., Knight, K., Mitchell, T.: Aligning context-\nbased statistical models of language with brain activity during reading.\nIn: Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pp. 233–243 (2014)\n[161] Khosla, A., Khandnor, P., Chand, T.: A comparative analysis of signal\nprocessing and classification methods for different applications based\non eeg signals. Biocybernetics and Biomedical Engineering 40(2),\n649–690 (2020)\n[162] Sharma, P.C., Raja, R., Vishwakarma, S.K., Sharma, S., Mishra, P.K.,\nKushwah, V.S.: Analysis of brain signal processing and real-time\neeg signal enhancement. Multimedia Tools & Applications 81(28)\n(2022)\n[163] Ferrante, O., Liu, L., Minarik, T., Gorska, U., Ghafari, T., Luo, H.,\nJensen, O.: Flux: A pipeline for meg analysis. NeuroImage 253,\n119047 (2022)\n[164] Kaplan, S., Meyer, D., Miranda-Dominguez, O., Perrone, A., Earl,\nE., Alexopoulos, D., Barch, D.M., Day, T.K., Dust, J., Eggebrecht,\nA.T., et al.: Filtering respiratory motion artifact from resting state\nfmri data in infant and toddler populations. NeuroImage 247, 118838\n(2022)\n[165] Bullock, M., Jackson, G.D., Abbott, D.F.: Artifact reduction in simul-\ntaneous eeg-fmri: a systematic review of methods and contemporary\nusage. Frontiers in neurology 12, 622719 (2021)\n[166] Islam, M.K., Rastegarnia, A., Sanei, S.: Signal artifacts and tech-\nniques for artifacts and noise removal. Signal Processing Techniques\nfor Computational Health Informatics, 23–79 (2021)\n[167] Taghaddossi, M., Moradi, P., Moradi, M.: Electroencephalogram\nsignal processing with python. In: Signal Processing with Python: A\nPractical Approach, pp. 4–1. IOP Publishing Bristol, UK, ??? (2024)\n[168] Krentz, M., Tutunji, R., Kogias, N., Mahadevan, H.M., Reppmann,\nZ.C., Krause, F., Hermans, E.J.: A comparison of fmri data-derived\nand physiological data-derived methods for physiological noise\ncorrection. bioRxiv, 2023–02 (2023)\n[169] Duraivel, S., Rahimpour, S., Chiang, C.-H., Trumpis, M., Wang,\nC., Barth, K., Harward, S.C., Lad, S.P., Friedman, A.H., Southwell,\nD.G., et al.: High-resolution neural recordings improve the accuracy\nof speech decoding. Nature communications 14(1), 6938 (2023)\n[170] Jiao, Y., Zheng, Q., Qiao, D., Lang, X., Xie, L., Pan, Y.: Eeg rhythm\nseparation and time–frequency analysis of fast multivariate empirical\nmode decomposition for motor imagery bci. Biological Cybernetics\n118(1), 21–37 (2024)\n[171] Minarik, T., Berger, B., Jensen, O.: Optimal parameters for rapid\n(invisible) frequency tagging using meg. NeuroImage 281, 120389\n(2023)\n[172] Belhaouari, S.B., Talbi, A., Hassan, S., Al-Thani, D., Qaraqe, M.: Pft:\nA novel time-frequency decomposition of bold fmri signals for autism\nspectrum disorder detection. Sustainability 15(5), 4094 (2023)\n[173] Cop, U., Dirix, N., Drieghe, D., Duyck, W.: Presenting geco: An\neyetracking corpus of monolingual and bilingual sentence reading.\nBehavior research methods 49, 602–615 (2017)\n[174] Kennedy, A., Hill, R., Pynte, J.: The dundee corpus. In: Proceedings\nof the 12th European Conference on Eye Movement (2003)\n[175] Mishra, A., Kanojia, D., Bhattacharyya, P.: Predicting readers’ sar-\ncasm understandability by modeling gaze behavior. In: Proceedings\nof the AAAI Conference on Artificial Intelligence, vol. 30 (2016)\n[176] Mishra, A., Bhattacharyya, P., Mishra, A., Bhattacharyya, P.: Scan-\npath complexity: modeling reading/annotation effort using gaze\ninformation. Cognitively Inspired Natural Language Processing: An\nInvestigation Based on Eye-tracking, 77–98 (2018)\n[177] Luke, S.G., Christianson, K.: The provo corpus: A large eye-tracking\ncorpus with predictability norms. Behavior research methods 50,\n826–833 (2018)\n[178] Frank, S.L., Otten, L.J., Galli, G., Vigliocco, G.: The erp response\nto the amount of information conveyed by words in sentences. Brain\nand language 140, 1–11 (2015)\n[179] Brennan, J.R., Stabler, E.P., Van Wagenen, S.E., Luh, W.-M., Hale,\nJ.T.: Abstract linguistic structure correlates with temporal activity\nduring naturalistic comprehension. Brain and language 157, 81–94\n(2016)\n[180] Rayner, K.: The 35th sir frederick bartlett lecture: Eye movements and\nattention in reading, scene perception, and visual search. Quarterly\njournal of experimental psychology 62(8), 1457–1506 (2009)\n[181] Kobler, R., Hirayama, J.-i., Zhao, Q., Kawanabe, M.: Spd domain-\nspecific batch normalization to crack interpretable unsupervised\ndomain adaptation in eeg. Advances in Neural Information Processing\nSystems 35, 6219–6235 (2022)\n[182] Pan, Y.-T., Chou, J.-L., Wei, C.-S.: Matt: a manifold attention\nnetwork for eeg decoding. Advances in Neural Information Processing\nSystems 35, 31116–31129 (2022)\n[183] Wagh, N., Wei, J., Rawal, S., Berry, B.M., Varatharajah, Y.: Eval-\nuating latent space robustness and uncertainty of eeg-ml models\nunder realistic distribution shifts. Advances in Neural Information\nProcessing Systems 35, 21142–21156 (2022)\n[184] Bos, D.O., et al.: Eeg-based emotion recognition. The influence of\nvisual and auditory stimuli 56(3), 1–17 (2006)\n[185] Jenke, R., Peer, A., Buss, M.: Feature extraction and selection\nfor emotion recognition from eeg. IEEE Transactions on Affective\ncomputing 5(3), 327–339 (2014)\n[186] Song, T., Zheng, W., Song, P., Cui, Z.: Eeg emotion recognition using\ndynamical graph convolutional neural networks. IEEE Transactions\non Affective Computing 11(3), 532–541 (2018)\n[187] Suhaimi, N.S., Mountstephens, J., Teo, J., et al.: Eeg-based emotion\nrecognition: A state-of-the-art review of current trends and opportu-\nnities. Computational intelligence and neuroscience 2020 (2020)\n[188] Wang, J., Wang, M.: Review of the emotional feature extraction and\nclassification using eeg signals. Cognitive robotics 1, 29–40 (2021)\n[189] Saa, J.F.D., Çetin, M.: Discriminative methods for classification of\nasynchronous imaginary motor tasks from eeg data. IEEE Trans-\nactions on Neural Systems and Rehabilitation Engineering 21(5),\n716–724 (2013)\n[190] Al-Saegh, A., Dawwd, S.A., Abdul-Jabbar, J.M.: Deep learning for\nmotor imagery eeg-based classification: A review. Biomedical Signal\nProcessing and Control 63, 102172 (2021)\n[191] Altaheri, H., Muhammad, G., Alsulaiman, M., Amin, S.U., Altuwai-\njri, G.A., Abdul, W., Bencherif, M.A., Faisal, M.: Deep learning\ntechniques for classification of electroencephalogram (eeg) motor\nimagery (mi) signals: A review. Neural Computing and Applications\n35(20), 14681–14722 (2023)\n[192] Millan, J.R., Renkens, F., Mourino, J., Gerstner, W.: Noninvasive\nbrain-actuated control of a mobile robot by human eeg. IEEE\nTransactions on biomedical Engineering 51(6), 1026–1033 (2004)\n[193] Tonin, L., Carlson, T., Leeb, R., Millán, J.d.R.: Brain-controlled\ntelepresence robot by motor-disabled people. In: 2011 Annual\nInternational Conference of the IEEE Engineering in Medicine and\nBiology Society, pp. 4227–4230 (2011). IEEE\n[194] Gandhi, V., Prasad, G., Coyle, D., Behera, L., McGinnity, T.M.: Eeg-\nbased mobile robot control through an adaptive brain–robot interface.\nIEEE Transactions on Systems, Man, and Cybernetics: Systems 44(9),\n1278–1285 (2014)\n[195] Bi, L., Fan, X.-A., Liu, Y.: Eeg-based brain-controlled mobile robots:\na survey. IEEE transactions on human-machine systems 43(2), 161–\n176 (2013)\n[196] Salazar-Gomez, A.F., DelPreto, J., Gil, S., Guenther, F.H., Rus, D.:\nCorrecting robot mistakes in real time using eeg signals. In: 2017\nIEEE International Conference on Robotics and Automation (ICRA),\npp. 6570–6577 (2017). IEEE\n[197] Nijholt, A.: Bci for games: A ‘state of the art’survey. In: International\nConference on Entertainment Computing, pp. 225–228 (2008).\nSpringer\n[198] De Vico Fallani, F., Nicosia, V., Sinatra, R., Astolfi, L., Cincotti, F.,\nMattia, D., Wilke, C., Doud, A., Latora, V., He, B., et al.: Defecting\nor not defecting: how to “read” human behavior during cooperative\ngames by eeg measurements. PloS one 5(12), 14187 (2010)\n[199] Liao, L.-D., Chen, C.-Y., Wang, I.-J., Chen, S.-F., Li, S.-Y., Chen,\nB.-W., Chang, J.-Y., Lin, C.-T.: Gaming control using a wearable and\nwireless eeg-based brain-computer interface device with novel dry\nB.H.Yu et al.\nPage 25 of 26\n\nfoam-based sensors. Journal of neuroengineering and rehabilitation\n9, 1–12 (2012)\n[200] Kerous, B., Skola, F., Liarokapis, F.: Eeg-based bci and video games:\na progress report. Virtual Reality 22, 119–135 (2018)\n[201] Vasiljevic, G.A.M., De Miranda, L.C.: Brain–computer interface\ngames based on consumer-grade eeg devices: A systematic literature\nreview. International Journal of Human–Computer Interaction 36(2),\n105–142 (2020)\n[202] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E.,\nKamar, E., Lee, P., Lee, Y.T., Li, Y., Lundberg, S., et al.: Sparks of\nartificial general intelligence: Early experiments with gpt-4. arXiv\npreprint arXiv:2303.12712 (2023)\n[203] Floridi, L., Chiriatti, M.: Gpt-3: Its nature, scope, limits, and\nconsequences. Minds and Machines 30, 681–694 (2020)\n[204] Anumanchipalli, G.K., Chartier, J., Chang, E.F.: Speech synthesis\nfrom neural decoding of spoken sentences. Nature 568(7753), 493–\n498 (2019)\n[205] Herff, C., Heger, D., De Pesters, A., Telaar, D., Brunner, P., Schalk,\nG., Schultz, T.: Brain-to-text: decoding spoken phrases from phone\nrepresentations in the brain. Frontiers in neuroscience 9, 217 (2015)\n[206] Sun, J., Wang, S., Zhang, J., Zong, C.: Towards sentence-level brain\ndecoding with distributed representations. In: Proceedings of the\nAAAI Conference on Artificial Intelligence, vol. 33, pp. 7047–7054\n(2019)\n[207] Gao, T., Yao, X., Chen, D.: Simcse: Simple contrastive learning of\nsentence embeddings. arXiv preprint arXiv:2104.08821 (2021)\n[208] Wang, S., Zhang, J., Wang, H., Lin, N., Zong, C.: Fine-grained neural\ndecoding with distributed word representations. Information Sciences\n507, 256–272 (2020)\n[209] Sun, J., Wang, S., Zhang, J., Zong, C.: Neural encoding and decoding\nwith distributed sentence representations. IEEE Transactions on\nNeural Networks and Learning Systems 32(2), 589–603 (2020)\n[210] Radovanovic, M., Nanopoulos, A., Ivanovic, M.: Hubs in space:\nPopular nearest neighbors in high-dimensional data. Journal of\nMachine Learning Research 11(sept), 2487–2531 (2010)\n[211] Cohen, D.: Magnetoencephalography: detection of the brain’s electri-\ncal activity with a superconducting magnetometer. Science 175(4022),\n664–666 (1972)\n[212] Proudfoot, M., Woolrich, M.W., Nobre, A.C., Turner, M.R.: Magne-\ntoencephalography. Practical neurology 14(5), 336–343 (2014)\n[213] Mihelj, E.: Machine learning applications to brain computer inter-\nfaces. PhD thesis, ETH Zurich (2021)\n[214] Suppes, P., Han, B.: Brain-wave representation of words by superpo-\nsition of a few sine waves. Proceedings of the National Academy of\nSciences 97(15), 8738–8743 (2000)\n[215] Chan, A.M., Halgren, E., Marinkovic, K., Cash, S.S.: Decoding word\nand category-specific spatiotemporal representations from meg and\neeg. Neuroimage 54(4), 3028–3039 (2011)\n[216] Guimaraes, M.P., Wong, D.K., Uy, E.T., Grosenick, L., Suppes, P.:\nSingle-trial classification of meg recordings. IEEE Transactions on\nBiomedical Engineering 54(3), 436–443 (2007)\n[217] Wang, J., Kim, M., Hernandez-Mulero, A.W., Heitzman, D., Ferrari,\nP.: Towards decoding speech production from single-trial magnetoen-\ncephalography (meg) signals. In: 2017 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pp. 3036–\n3040 (2017). IEEE\n[218] Baevski, A., Zhou, Y., Mohamed, A., Auli, M.: wav2vec 2.0: A\nframework for self-supervised learning of speech representations.\nAdvances in neural information processing systems 33, 12449–12460\n(2020)\n[219] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal,\nS., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning\ntransferable visual models from natural language supervision. In:\nInternational Conference on Machine Learning, pp. 8748–8763\n(2021). PMLR\n[220] Crone, N.E., Miglioretti, D.L., Gordon, B., Lesser, R.P.: Functional\nmapping of human sensorimotor cortex with electrocorticographic\nspectral analysis. ii. event-related synchronization in the gamma band.\nBrain: a journal of neurology 121(12), 2301–2315 (1998)\n[221] Pei, X., Leuthardt, E.C., Gaona, C.M., Brunner, P., Wolpaw, J.R.,\nSchalk, G.: Spatiotemporal dynamics of electrocorticographic high\ngamma activity during overt and covert word repetition. Neuroimage\n54(4), 2960–2972 (2011)\n[222] Yamamoto, R., Song, E., Kim, J.-M.: Parallel wavegan: A fast\nwaveform generation model based on generative adversarial networks\nwith multi-resolution spectrogram. In: ICASSP 2020-2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 6199–6203 (2020). IEEE\n[223] Śliwowski, M., Martin, M., Souloumiac, A., Blanchart, P., Aksenova,\nT.: Deep learning for ecog brain-computer interface: end-to-end vs.\nhand-crafted features. In: International Conference of the Italian\nAssociation for Artificial Intelligence, pp. 358–373 (2022). Springer\n[224] Śliwowski, M., Martin, M., Souloumiac, A., Blanchart, P., Aksenova,\nT.: Impact of dataset size and long-term ecog-based bci usage on deep\nlearning decoders performance. Frontiers in Human Neuroscience\n17, 1111645 (2023)\n[225] Sergeev, K., Runnova, A., Zhuravlev, M., Sitnikova, E., Rutskova, E.,\nSmirnov, K., Slepnev, A., Semenova, N.: Simple method for detecting\nsleep episodes in rats ecog using machine learning. Chaos, Solitons\n& Fractals 173, 113608 (2023)\n[226] Zeng, Y., Zhao, D., Zhao, F., Shen, G., Dong, Y., Lu, E., Zhang, Q.,\nSun, Y., Liang, Q., Zhao, Y., et al.: Braincog: A spiking neural network\nbased, brain-inspired cognitive intelligence engine for brain-inspired\nai and brain simulation. Patterns 4(8) (2023)\n[227] Nunes, J.D., Carvalho, M., Carneiro, D., Cardoso, J.S.: Spiking neural\nnetworks: A survey. IEEE Access 10, 60738–60764 (2022)\n[228] Lagani, G., Falchi, F., Gennaro, C., Amato, G.: Spiking neural\nnetworks and bio-inspired supervised deep learning: A survey. arXiv\npreprint arXiv:2307.16235 (2023)\n[229] Maass, W.: Networks of spiking neurons: the third generation of\nneural network models. Neural networks 10(9), 1659–1671 (1997)\nB.H.Yu et al.\nPage 26 of 26",
    "pdf_filename": "Brain-inspired_Computing_Based_on_Deep_Learning_for_Human-computer_Interaction_A_Review.pdf"
}