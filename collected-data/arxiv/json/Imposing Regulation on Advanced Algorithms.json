{
    "title": "Imposing Regulation on Advanced Algorithms",
    "context": "",
    "body": "Imposing Regulation on Advanced Algorithms \nFotios Fitsilis \nHead of Department for Scientific Documentation and Supervision  \nScientific Service \nHellenic Parliament \n \n \n \n \nEmail: fitsilisf@parliament.gr \nORCID: https://orcid.org/0000-0003-1531-4128 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThe final authenticated version is available online at \nhttps://doi.org/10.1007/978-3-030-27979-0\n\nii \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThis book would not have been possible without the support of my wife, espe-\ncially in the later phase of her pregnancy!  It is dedicated to her. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\niii \nForeword  \n \nIt is my very real pleasure to have the opportunity to write the foreword to this \nimportant new book by Dr. Fotios Fitsilis. The focus of his work is on how to reg-\nulate advanced algorithms, where the emergence of new technologies continues to \npresent challenges for, among other things, science, law, and politics. For public \nlawyers, advanced algorithms raise particular questions about core aspects of con-\nstitutional and administrative law, including as to the nature and identity of algo-\nrithmic decision-makers, the manner in which decisions might be reviewed (and \nupon which grounds), and broader matters about accountability and control, not to \nmention conceptions of privacy. Such questions provide much of the backdrop to \nDr. Fitsilis’ book, which offers novel and insightful answers about some of the \nways forward. I imagine that his commentary will become a key reference point \nfor people working in this field.   \nDr. Fitsilis’ work focuses upon the role that two sets of actors might play in re-\nlation to advanced algorithms: regulatory bodies which operate within systems of \nmultilevel governance; and Parliaments. He begins his analysis by examining how \nalgorithms have evolved and how they now intersect with legal values such as \nnon-discrimination, accountability and transparency. This provides the basis for a \nstudy of a number of key cases/legal moments involving algorithms, where he \nnotes the approaches within a range of different jurisdictions and proposes a future \nrole for a European algorithmic regulator. The remainder of his work considers the \nparticular function that Parliaments might perform moving forward, where he ar-\ngues that legislatures and executives need to work in tandem with technical and \nprivate-sector actors to set the parameters within which advanced algorithms \nmight be developed. He here highlights the role that Parliamentary Research Ser-\nvices might play given their expertise and experience. \nThere are many reasons why Dr. Fitsilis’ book might be regarded as important, \nthough I would highlight two in particular. The first is that it represents a truly in-\nterdisciplinary approach to the topic at hand, as Dr. Fitsilis has a background in \nengineering and economics, as well as law. This is where much of the strength of \nhis book is to be found, as the analysis within it borrows from his experience of \nworking within and across those different academic disciplines. I would also note \nthat he presently works within the Hellenic Parliament and that he thereby has a \nunique understanding of some of the issues that are analysed.  \nThe second reason concerns the institutional setting in which many of the \nbook’s ideas were developed, namely the Academy of European Public Law in \nGreece. That Academy was founded in 1995 at a time when public law remained \nin a more traditional form, but its mission has since expanded given the challenges \nthat public law now faces. Dr. Fitsilis’ book stands as one of the most prominent \nexamples of that expansion, and it is a testimony to his intellectual ability that he \nhas been able to bring together his backgrounds in science and law. He is to be \nwarmly congratulated on having done so in the form of this compelling and en-\ngaging book.   \n\niv \n \nGordon Anthony \nProfessor of Public Law \nQueen’s University, Belfast \nDirector of the Academy of European Public Law \n \n28 June 2019 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\nv \nPreface \n \nAny end time scenario is a good reason to tackle the causing issue with scientific \nmethod and precision. This may very well be the case with Artificial Intelligence \n(AI). When discussing AI, most people think of so-called ‘Artificial General Intel-\nligence’ (AGI), which refers, however, to the broader goal of reaching human-like \nintelligence, rather than to a single technology, or bunch of technologies. This \ngoal is gradually approached through the inception of intelligent algorithms, i.e. \nspecial computer programs, rather than complex hardware, which are influencing \nan ever-expanding number of human activities. Within the course of this book, the \nnotion of ‘advanced algorithms’ is developed to describe this kind of computer \nsoftware, while calling for a timely regulation through well-defined, structured \nprinciples and dedicated government agencies with an oversight mandate. \n   My interest in the topic was sparked by my general research on the future of par-\nliaments. Like every organization, parliaments will need to evolve to respond to \ndigital challenges, otherwise they risk losing significance in the powers balance.  \nIndeed, the concept of the ‘smart’ parliament involves the establishment of digital \nservices for both internal and external (i.e. citizens) stakeholders. Among others, \nfuture parliaments will find themselves in the position of debating and examining \nthe necessity to impose regulations to cover the use of advanced algorithms. Upon \nwhich criteria will the Members of Parliaments (MPs) decide? What kind of tech-\nnical support and consultation would be necessary for them to reach educated, in-\nformed and sustainable law-making decisions?  \nWhile attempting to respond to such questions in some of my previous articles, \nit quickly became clear that the field of regulation of advanced algorithms needs \nto be widened to include both the administrative and the judicial levels. My un-\nderstanding was further strengthened after attending a pair of workshops in 2017 \nand 2018, in Athens, Greece, under the name ‘Artificial Cosmoi and the Law’, or-\nganized, among others, by the Centre for Law, Economics and Society of Univer-\nsity College London. This book as such builds upon a series of lectures on admin-\nistrative law during the 2017 and 2018 sessions of the Academy of European \nPublic Law, the essence of which was gradually developed and brought to the pre-\nsent form.  \nThere were numerous challenges encountered whilst writing this book, mostly \ndealing with methodological issues as well as with the holistic approach that was \napparently needed to discuss an omnipresent topic. Hence, a comparative legal \nperspective is not methodologically intended but necessary to shed light on the re-\nsponse of different legal orders to certain effects of advanced algorithms. In addi-\ntion, there were inherent difficulties in choosing representative examples of algo-\nrithmic regulation from innumerous cases with significant national or international \nimpact. There are just a few examples for regulating algorithms per dedicated le-\ngal provisions (the MiFID (EU) and the GDPR (EU) constitute exceptions to this \nrule and are therefore examined in detail later). The decision was therefore taken \nto select cases that were ruled on grounds of major areas of law, such as competi-\n\nvi \ntion, labor, environment, data-protection, consumer protection law and others. As \na result, eight prominent case studies are presented in this book.  \nBack in my own field of study, the ‘evolution of parliamentary systems and \nprocedures’, this book argues that parliamentarians and the Executive, supported \nby technical and private-sector consultations, should work closely together to de-\nfine the legal grounds upon which advanced algorithms are to be developed and \noperated. In this respect, the role of Parliamentary Research Services (PaRS) and \nspecialized agencies is of particular significance as they may provide a much \nneeded in-depth analysis and strategic advice.  \nOverall, the book attempts a structured approach to systematize regulatory ac-\ntivities in the realm of advanced algorithms, a scientific field that is currently go-\ning through a development frenzy. Moreover, it constitutes a compact tool to pro-\nvide guidance to regulators, be they lawmakers, judges or administrators. In this \nsense, as current bibliography is rather scarce, this book offers to all stakeholders \na state-of-the-art introduction to the field of algorithmic regulation and supports \nthe creation of guidelines for organizations on how to structure their regulatory ac-\ntivity.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\nvii \nAcknowledgments    \n \nThis book has one author but several supporters. Without their invaluable assis-\ntance, it would not have been possible to research and produce it in the form you \nare looking at.  \nIn this regard, I would like to thank Prof. Georgios Yannopoulos, Law School \nof the National and Kapodistrian University of Athens, for his assistance in setting \nup a state-of-the-art structural framework for this book. Moreover, I would like to \nthank Prof. Spyridon Vlachopoulos, Law School of the National and Kapodistrian \nUniversity of Athens, and Prof. Jean-Bernard Auby, Emeritus Public Law Profes-\nsor of the Sciences Po, Paris, for providing legal-technical advice.  \nSpecial thanks go to Prof. Gordon Anthony, School of Law, Queen’s Universi-\nty, Belfast, for the final review of the manuscript, for writing the foreword and for \nproviding decisive comments. Many thanks also to Prof. Kostas Mavrias, Emeri-\ntus, Professor of Constitutional Law at the Law School of the National and \nKapodistrian University of Athens and President of the Scientific Council of the \nHellenic Parliament for his push towards scientific excellence and his steady sup-\nport throughout the years. I would also like to underline the unseen contribution of \nmany of my colleagues and fellow scientists, too numerous to mention, for their \nsupport and the fruitful discussions throughout the drafting phase.  \nMany thanks also to the friends and colleagues from the Hellenic Optical Char-\nacter Recognition (OCR) Team, for their true dedication to science and innovation \nthat has always been a source of inspiration to me. Dimitris Garantziotis deserves \na special mention for his valuable contribution in the last phase of drafting. Lots of \nthanks go to Antonia Becou who professionally redesigned all graphic material in \nthe book. Last, but not least, I would also like to thank Bruce Philip Todd, not on-\nly for the linguistic assessment and support, but also for the out-of-the-box general \ncomments.  \nAbove all, I would like to thank my family for all the support that gives me \nstrength to carry on. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\nviii \nContents \n \n \n \n \n \n \n1 Introduction, definitions and scope ................................................................... 1 \nReferences ......................................................................................................... 8 \n2 Evolution of Advanced Algorithms ................................................................. 11 \n2.1 Overview ................................................................................................... 11 \n2.1.1 Non-discrimination............................................................................. 13 \n2.1.2 Accountability .................................................................................... 14 \n2.1.3 Transparency ...................................................................................... 16 \n2.2 Necessity of Advanced Algorithms and their Regulation .......................... 18 \nReferences ....................................................................................................... 21 \n3 Administrative and Judicial Decisions on Advanced Algorithms ................ 25 \n3.1 Microsoft Media Player and Explorer ........................................................ 25 \n3.2 Volkswagen Emissions Case ..................................................................... 28 \n3.3 Ad-blocking ............................................................................................... 30 \n3.4 Block Controversial Content ..................................................................... 33 \n3.5 Sharing economy ....................................................................................... 37 \n3.5.1 The Airbnb case ................................................................................. 37 \n3.5.2 The Uber case ..................................................................................... 40 \n3.6 Algorithmic trading regulation .................................................................. 44 \n3.7 General Data Protection Regulation .......................................................... 47 \nReferences ....................................................................................................... 52 \n4 Development of Regulatory Bodies ................................................................. 57 \nReferences ....................................................................................................... 64 \n5 Conclusions and Outlook ................................................................................. 67 \nReferences ....................................................................................................... 75 \nAppendix .............................................................................................................. 77 \nTables .............................................................................................................. 77 \nIndex ................................................................................................................ 80 \n \n \n \n\nix \nAbout the Author     \nDr. Fotios Fitsilis has over 20 years of professional experience in science positions \nwithin in both the private and the public sector. Since 2008, Dr. Fotios Fitsilis is \nHead of Department for Scientific Documentation and Supervision and lead re-\nsearcher at the Scientific Service of the Hellenic Parliament. While operating on a \nglobal scale, he has been active in fields ranging from telecommunications and lo-\ngistics to management and good governance, which has included recent papers on \ne-governance and institutional development including improvements to parliamen-\ntary oversight committees. Dr. Fitsilis has been visiting Professor for parliamen-\ntary procedures and legislative drafting at the Universidad Complutense de Ma-\ndrid. In 2017, he founded the Hellenic OCR Team, a crowdsourcing initiative for \nthe study of parliamentary data. Dr. Fitsilis has an academic background in Law \n(LL.M. in International Law), Economics (Diploma in Financial Engineering) and \nEngineering (Diploma in Electrical Engineering), while also holding a doctoral \ndegree in Electrical Engineering.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\nx \n \nList of Abbreviations \n \nAEPL \nAcademy of European Public Law \nAECD \nAuxiliary Emission Control Device  \nAEDP \nAgencia Española de Protección de Datos  \nAI \nArtificial Intelligence \nAGI \nArtificial General Intelligence \nArt.  \nArticle \nBGH \nBundesgerichtshof \nCBPC \nCalifornia Business and Professions Code \nCEO \nChief Executive Officer \nCJEU \nCourt of Justice of the European Union \nCoC \nCertificate of Conformity \nCOMPAS \nCorrectional Offender Management Profiling for Alternative Sanctions \nDG \nDirection Générale (European Commission) \nDNN \nDeep Neural Networks \nDoJ \nDepartment of Justice \nDPA \nData Protection Agency \nDPO \nData Protection Officer \nEC \nEuropean Commission \nECHR \nEuropean Court of Human Rights \nECLI \nEuropean Case Law Identifier  \nECT \nEuropean Communities Treaty \nENISA \nEuropean Union Agency for Network and Information Security  \nEP \nEuropean Parliament \nEPA \nEnvironmental Protection Agency  \nEPRS \nEuropean Parliament Research Service \nESMA \nEuropean Securities and Markets Authority \nEU \nEuropean Union \nEULA \nEnd-User License Agreement  \nGDPR \nGeneral Data Protection Regulation  \nGUI \nGraphical User Interface  \nHFAT \nHigh-Frequency Algorithmic Trading  \nICJ \nInternational Court of Justice \nICO \nInformation Commissioner’s Office (UK) \nICT \nInformation and Communications Technology \nIE \nInternet Explorer \nIEEE \nInstitute of Electrical and Electronics Engineers \nIFLA \nInternational Federation of Library Associations and Institutions \n\nxi \nIPU \nInter-Parliamentary Union \nISB \nIndependent State Body \nISP \nInternet Service Provider \nIT \nInformation Technology \nJURI \nEuropean Parliament's Committee on Legal Affairs \nMAR \nMarket Abuse Regulation \nMEP \nMember of the European Parliament  \nMiFID \nMarkets in Financial Instruments Directive \nMiFIR \nMarkets in Financial Instruments Regulation  \nMP \nMember of Parliament \nNGO \nNon-Governmental Organization \nOBA \nOnline Behavioral Advertising \nOCR \nOptical Character Recognition \nOTC \nOver-The-Counter  \nPaRS \nParliamentary Research Services  \nPLS \nPost-Legislative Scrutiny  \nPοLA \nPrinciple of Least Astonishment \nSABAM \nSociété belge des auteurs, compositeurs et éditeurs SCRL  \nSME \nSmall and Medium-sized Enterprise  \nTFEU \nTreaty on the Functioning of the European Union \nTGI \nTribunal de Grande Instance \nUI \nUser Interface \nUK \nUnited Kingdom \nURL \nUniform Resource Locator \nUS \nUnited States \nUSC \nUnited States Code \nUWG \nGesetz gegen den unlauteren Wettbewerb  \nVW \nVolkswagen \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\nxii \nList of Figures \n \nFig. 1.1 \nFrom simple tasks to advanced algorithms  \nFig. 1.2 \nHow algorithmic regulation touches upon major areas of law  \nFig. 4.1 \nHierarchical mode of regulation for advanced algorithms   \nFig. 4.2 \nPrincipal design of an algorithmic monitor   \nFig. 5.1  \nAdvanced classification of regulatory measures: natural, type and timing modes  \n \n \n \nList of Tables \n \nTable 5.1 \nOverview of analyzed topics on advanced algorithms \nTable A \nOverview of court cases \nTable B \nOverview of presented decisions, directives, laws, regulations, resolutions \nand reports \n \n \n\n1 Introduction, definitions and scope     \nFotios Fitsilis \nAbstract    \nThis book focuses on cases where regulation has been imposed on advanced algo-\nrithms due to judicial or administrative decisions. From a series of different topics \nof algorithmic conduct, a number of case studies have been selected, in order to \ndetermine similarities or divergence in regulatory behavior. The development of \nregulatory bodies is therefore also discussed. Moreover, this book constitutes a \nfirst of its kind in terms of recording, classification and comparative legal assess-\nment of significant cases of algorithmic regulation, with a view to establishing \nbest practice and a responsible way forward. \n \nKeywords: legal assessment, advanced algorithms, regulatory modes, algorithmic \nregulation, regulatory bodies. \n \nFor decades technological innovation was linked to the invention of hardware \nproducts. Software, that is computer programs, has been developed to drive hard-\nware and little importance was given to it. The structured development of software \nas an engineering branch, as stated by the term ‘software engineering’, started \nmuch later.1 However, it is not without irony that the sci-fi genre, rather than tech-\nnologists, has first captured the strength and far-reaching consequences of algo-\nrithms, the cornerstone of software engineering, which, in popular culture, are \nusually paralleled with applications of Artificial General Intelligence (AGI).2 One \nmay just resemble the supercomputer Deep Thought in the novel The Hitchhiker's \nGuide to the Galaxy, from which originates the famous quote ‘The answer to the \nultimate question of life, the universe and everything is 42’.3 Or HAL 9000 in \n2001: A Space Odyssey, an intelligent computer on board the spaceship Discov-\nery, which malfunctions when presented with conflicting orders.4 It is because of \n                                                           \n1 On the history of software engineering see, e.g., Wirth (2008) and Booch \n(2018).   \n2 In AGI, machine intelligence matches human intellectual properties; see, e.g., \nGoertzel and Pennachin (2007); the respective point in time this might take place \nis known as ‘singularity’.  \n3 See Adams (1979). \n4 See Clarke (1968). \n\n2  \nthe rise of computer algorithms that large parts of our lives are now being man-\naged by powerful applications of digital computing. This book is about regulation \nof algorithms. Moreover, it speaks about regulation of ‘advanced algorithms’.  \nSuch algorithms are also influencing traditional institutions. Originating in An-\ncient Greece some 2.500 years ago, the concept of democracy is based on the core \nidea that governments are only legitimate when they are based on the consent of \nthe people (the governed). In addition, according to Rousseau, citizens are both \nsubject and sovereign, whereas law is the expression of the will of the people.5 In \ncontemporary democracies, people have to be guaranteed certain fundamental \nrights, such as the right to education, the right to equal protection before the law \nand the right to access to information.6 However, what happens when the evolu-\ntion of advanced algorithms, for instance in the form of digital communication \nplatforms, is threatening these basic rights and, through that, the building elements \nof democracy?7 Few would disagree that this situation calls for prompt regulatory \nmeasures, implementation of these measures and legislation, as well as oversight \nand enforcement capability. \nAs highlighted, the research field of the book is the regulation of advanced al-\ngorithms. It focuses on cases where regulation has been imposed on algorithms \npursuant to judicial or administrative decisions. Its main objective is to identify \npossible common patterns behind these decisions8 as guidance for future conduct \nwith advanced algorithms. In the legal domain, the book examines the general \nquestion about the future of law in the era of advanced algorithms. Legal chal-\nlenges presented by algorithms transcend different areas of law, which of course \ninclude administrative law.9 \nIn order to analyze the state-of-play in the field, a thorough literature review \nhas been conducted. In addition to already published research, several online re-\nsources by media, hackers, bloggers, investigative journalists and others have been \nused in this work. However, language barriers and issues of accessibility to case \nlaw limit this search to most ‘advanced’ legal orders. As a result, with a few ex-\nceptions regarding Japan and China, analysis is focused on relevant European and \n                                                           \n5 On Rousseau’s democratic principle and the surrounding discussion see, e.g., \nMacAdam (1983); the Universal Declaration of Human Rights states that ‘[t]he \nwill of the people shall be the basis of the authority of government; (…)’; in Unit-\ned Nations (1948), Art. 21 §3; Cohen and Rogers (1983), p. 149, introduce the \nPrinciple of Democratic Legitimacy as a principle for public justification. \n6 United Nation's 1948 Universal Declaration of Human Rights constitutes a \nmilestone document in the history of fundamental human rights; see also supra \nnote 5. \n7 Sunstein (2017) illumines the dangers to democracy that arise out of the un-\ncontrolled use of the internet. \n8 In the case of judicial decisions we generally speak of case law. \n9 A concise overview of this area of the law is provided by Leyland and Antho-\nny (2012). \n\n3 \nUS case law. However, a comparative legal perspective is not methodologically \nintended but necessary to shed light on the response of different legal orders of se-\nlected states to certain effects of advanced algorithms. This discussion is of partic-\nular importance when we highlight the steps towards the development of regulato-\nry bodies and their role, especially in the European context, where there are \ninteresting questions to be asked about the overlap between global, supranational \nand national institutions and processes.  \nConcretely, the book attempts to answer two basic research questions:  \n1. Do advanced algorithms need to be regulated? \n2. If so, who is going to be responsible for imposing such regulation?  \nWhile the answer to the first question is binary, i.e. ‘yes’ or ‘no’, there are a \nnumber of entities, that may impose algorithmic regulation. Several sources of \nregulation have been considered and are presented in this book. These may rely on \na centralized or decentralized approach and may involve a dedicated regulatory \nbody and/or even the parliament itself. Additional relevant issues, such as the de-\ngree of algorithmic regulation, have also been touched upon.  \nWe publicly speak of algorithms and their regulation, but what exactly is an \n‘algorithm’ and what is meant by the composite term ‘algorithmic regulation’? \nLiterature is not scarce when it comes to define an ‘algorithm’. In a 1983 book on \ncomputer programming an algorithm is defined as: \n‘(…) a finite sequence of instructions, each of which has a clear meaning and can be \nperformed with a finite amount of effort in a finite length of time.’10  \nThe etymology of the word is believed to be derived from al-Ḵwārizmī ‘the \nman of Ḵwārizm’, a corruption of the name given to the ninth century mathemati-\ncian Abū Ja‘far Muhammad ibn Mūsa, influenced by the Greek word αριθμός \n(arithmós i.e. number).11 A common misunderstanding by non-computer scientists \nis that algorithms are restricted to computer-related processes, whereas in reality \nthey are omnipresent and abundant in real life situations.12 The personal morning \nroutine or a cooking recipe are classic examples of real life algorithms. In general, \nan algorithm has the following attributes: \n• Its steps are ordered, \n• Its steps are well-defined, \n• There is a limited number of steps and \n• It produces a specific result.  \n                                                           \n10 Definition taken from Aho et al. (1983), p. 2. \n11 According to Oxford Dictionaries, s.v. “algorithm”, accessed May 30, 2019, \nhttps://en.oxforddictionaries.com/definition/algorithm.   \n12 See, e.g., Louridas (2017). \n\n4  \nIn computer science, an algorithm can be understood as detailed instructions to \na computer to perform a given task.13 A computer program, plainly defined as a \nset of instructions to perform a certain task, may rely on one or more core algo-\nrithms.14 Algorithms have been at the center of the development of Information \nand Communications Technologies (ICT). Moreover, their most advanced coun-\nterparts lie in the heart of potentially powerful as well as influential technologies, \nthat drive social networks, trade in stock markets and calculate our taxes. Hence, \nin order to differentiate these more elaborated and complex algorithms from their \nless-developed predecessors, we adhere the term ‘advanced algorithms’ to de-\nscribe them.15 Fig. 1.1 shows the natural evolution of algorithms in four distinct \nsteps, from simple tasks and processes that form some of the primary algorithms \nwhich are then embedded into more complex computer programs. The final step \nof evolution is currently populated by advanced algorithms. Following the techno-\nlogical singularity, this position will be taken over by AGI algorithms.  \nAs algorithms evolve and increase in complexity an issue that needs to be dealt \nwith is transparency in their operation. This is the so-called opacity problem and it \nis of particular importance when dealing with algorithmic decision-making sys-\ntems.16 In certain circumstances, their outcome may produce legal consequences \nthat have implications for the legal rights of data subjects or, as we have seen in \nthe case of digital communication platforms and social media, in the legitimacy of \ngovernance.17 \nWhen it comes to regulation –in this case, of digital platforms– few bring it \ncloser to the point than Cédric O, France’s Junior Minister for digital affairs:  \n‘It is not for the platforms to decide what justice they should apply. The state makes the \nrules’.18 \n                                                           \n13 Such instructions are encoded according to the typology of a computer lan-\nguage.  \n14 Additional commands may define, among others, the User Interface (UI), da-\nta handling, communication protocols and interaction with other system parame-\nters.  \n15 However the term is not new and has been used in science for decades. For \ninstance, Jayant (1986) and Wilamowski (2009) use the term to describe advanced \ncoding techniques for voice communication and advanced learning algorithms for \nneural networks, respectively. \n16 See, e.g., Pasquale (2015) and Burrell (2016). \n17 In Chapter 2, examples of algorithmic opacity are going to be discussed with-\nin the discussion that touches upon basic principles of administrative law, such as \nnon-discrimination and transparency.   \n18 See relevant CNN Business report by Gold and Siad (2019); France pursues \nan increasingly aggressive strategy in the regulation of social networks, see \nDesmaris et al. (2019), whereas at the moment the European Commission (2018) \n\n5 \nHowever, there is not a unified approach when defining the term ‘regulation’. \nOrbach concludes that ‘[r]egulation is state intervention in the private domain, \n(…)’.19 In the present context, the intervention of the state implies that its full ar-\nsenal of legislative and judicial tools may be employed to control, direct or man-\nage the development and effects of application of advanced algorithms. Tradition-\nally, command-and-control systems have been utilized to impose regulation. But \nin recent times, several new approaches have emerged.20 Since we will be investi-\ngating different legal orders across several continents, one needs to bear in mind \nthe paradox of calling for regulation in neo-liberal western democracies, where by \ndefinition deregulation should be the rule.21 Certainly, there is always an option \nfor States or supranational conglomerates not to proceed with any regulatory ac-\ntion.22 The absence of regulation can be an option of choice, a product of igno-\nrance or even mere inaction. However, we argue that, in cases of algorithmic con-\nduct, no-regulation will inevitably lead to self-regulation in order for a natural \nequilibrium to be restored.23  \nWe call these different approaches ‘modes of regulation’. Several regulatory \npossibilities may exist within a mode. A classification of modes cannot be abso-\nlute, because it is not always possible to avoid overlapping between different \nones.24 Hence, indicatively, the essence of regulation may be captured within the \nfollowing series of modes:25  \n1. Intervention mode:26 command-and-control, self-regulation or co-regulation, \n2. Hierarchical (or geographic) mode:27 global, supranational, national (or local), \n                                                                                                                                     \nis relying on voluntary action by the stakeholders, through an EU Code of Practice \non disinformation. \n19 In Orbach (2012), p. 10. \n20 See, e.g., Finck (2017) and Trubek and Trubek (2007).  \n21 See Jordana and Levi-Faur (2004), p. 10. \n22 Gibbons (1997, p. 483) names no regulation of cyberspace a ‘null choice’. \n23 See also Kleinsteuber (2004), p. 64. \n24 The timing mode, for instance, is event driven, i.e. regulation may be im-\nposed before (ex-ante) or after (ex-post) a certain event has occurred; similarly, \njudicial regulation within the type mode is (mostly) triggered following a series of \nevents that lead to legal action; in the following chapters, the type and the timing \nmodes will frequently be used interchangeably; this does not mean that they are \nalways identical, as legislative action may follow a judicial decision leading to a \nspiral of further actions from different stakeholders.  \n25 More details about these modes, as well as a more narrowly-defined regulato-\nry approach on advanced algorithms, will be presented in Chapter 4 and Chapter \n5. \n26 Principles-based self-regulatory/coregulatory measures have been proposed \nby the European Commission to regulate online platforms; see, e.g., Finck (2017);  \n27 This mode positions regulatory action in the chain of multi-level governance.  \n\n6  \n3. Natural mode:28 direct or indirect, \n4. Type mode:29 legislative or judicial, \n5. Timing mode:30 ex-ante or ex-post. \nDuring the research phase we have been presented with inherent difficulties in \nchoosing representative examples of algorithmic regulation. Dozens of different \ncases of algorithmic conduct across several continents have been screened to find \ncommon ground for a detailed legal discussion. Some of these were promising \ncandidates with potential necessity for algorithmic regulation. However, they were \nstill developing stories with unclear endings.31 Instead, the choice was made to se-\nlect landmark cases where algorithmic regulation had already been imposed. But \nhow to make choices from such a wide selection of cases? The decision was there-\nfore made to select cases of algorithmic interest that were ruled on grounds of ma-\njor areas of law. \nScreening has also been performed at the legislative level. Not unsurprisingly, \ngiven the complexity of algorithmic regulation,32 only a few examples of clear \nlegislative action have been detected, such as Directive (EU) 2014/65 and Regula-\ntion (EU) 2016/679.33 They constitute the European regulatory framework around \nalgorithmic trading and data protection, respectively. However, it should be clear \nthat these legal texts constitute exceptions to the rule and are therefore examined \nin detail.  \nAs a result, a number of case studies of algorithmic regulation that touch upon \neight major areas of law, such as competition, labor, environment, data-protection, \nconsumer protection law and others, have been selected to be discussed in this \nbook.34 Each one of the six case studies with judicial intervention has been ruled \nbased on one or more of these areas of law. Furthermore, the two mentioned legis-\nlative frameworks have been developed based on certain areas of law that lie at \n                                                           \n28 The nature of regulation is captured herein, i.e. whether regulation aims at \nchanging the code of a given algorithm (direct regulation) or its environment, such \nas the behavior of its controller (indirect regulation).   \n29 The type mode describes regulation coming out of two basic branches of \ngovernment, legislative and judicial; bylaws and other administrative decisions are \nfrequently not possible to be issued without prior legislative acts or judicial deci-\nsions.  \n30 See supra note 24 and accompanying text. \n31 See, e.g., the COMPAS and the Amazon’s Echo cases in Chapter 2.  \n32 The number of possible regulatory modes, which were presented earlier, \nspeaks for the complexity involved in imposing algorithmic regulation.  \n33 This also goes under the name General Data Protection Regulation (GDPR). \n34 Six of the case studies represent legal disputes that were ruled before the \ncourts (judicial regulation); in addition two legislative frameworks are discussed \n(GDPR and MiFID); these eight case studies are analyzed in Chapter 3. The ef-\nfects of algorithms in the realm of administrative law are discussed in Chapter 2. \n\n7 \ntheir core. In total, the eight areas of law that are displayed in Fig. 1.2 represent \nbasic legal sectors that have been applied in cases of algorithmic regulation. We \nemphasize the fact that the examined case studies are not exclusively covered by \nthe mentioned areas of law, nor that there is always a one-to-one match. For in-\nstance, the use of penal law for criminal investigations has been detected, but is \nnot fully relevant for most of the cases.  \nThe topics of choice are specific cases of algorithmic regulation or stand-alone \npieces of legislation that regulate a certain field, such as personal data protection.35 \nIn the Microsoft cases, the conditions that led to the separation of the operating \nsystem from the media player and the internet browser software are discussed. The \nVolkswagen emissions case dealt with an illegal algorithmic switch, which sensed \nthe operating conditions of the vehicle and adjusted gas emissions accordingly. \nAd-blocking was studied through a German case, Axel Springer AG v. Eyeo, \nwhich explored the inter-relations between online privacy, digital marketing and \nfair competition. The boundaries of the personal right to block online controversial \ncontent, widely known as the ‘right to be forgotten’, have been studied in the light \nof the case of Google Spain SL and Google Inc. v. AEPD and González. The area \nof ‘sharing economy’ has been approached through a series of high-profile cases \nagainst Airbnb and Uber. Finally, two topics have been dedicated to the discussion \nof algorithmic financial instruments, as regulated through Directive 2014/65/EU \non markets in financial instruments, and personal data protection, as in the General \nData Protection Regulation, 2016/679/EU. The main case attributes have been \nscreened and compared, ranging from geographical location and administrative \ndecisions to judicial reasoning and legal basis.  \nApart from the above, the evolution of advanced algorithms is presented and a \nnumber of considerations are discussed, such as algorithmic bias, which may re-\nproduce discriminatory behaviors as it is the case in real environments. The devel-\nopment of regulatory bodies is discussed in more detail as the complexity of ad-\nvanced algorithms and their rapid evolution makes a traditional generalist \napproach rather inefficient. Within the same context, the role of parliaments is also \ndiscussed. Moreover, the cost and other general considerations of algorithmic reg-\nulation are tackled. The book also aims to systematize further study in the field of \nresearch. For this purpose, added-value is provided through data visualization by a \nseries of tables and annexes. For example, Table A in the appendix presents the \nmost significant court cases discussed herein, including complaints, decisions and \nother court documents. Table B in the appendix gives a list of laws, decisions, di-\nrectives, regulations and resolutions from various national and international organ-\nizations, which have been analyzed in the course of this book. \nThe book, at its core, looks at universally applicable patterns in administrative \ndecisions and judicial rulings. Analysis has been conducted to determine similari-\nties or divergence in behavior among the different cases. Our assessment shows \n                                                           \n35 See Chapter 5 for an overview of analyzed topics and legal bases in the regu-\nlations of advanced algorithms. \n\n8  \nthat in several of the cases presented, sources of general law, such as competition \nor labor law, are invoked as a legal basis, possibly due to the lack of additional \nspecialized legislation on the subject area. In some occasions, it seems that a \ncommon law system is perhaps better placed to deal with this situation, as it can \nbe more flexible. In a further step, the book investigates the role of regulatory bod-\nies for advanced algorithms and considers the European Union Agency for Net-\nwork and Information Security (ENISA), based in Heraklion, Crete, Greece, that \nfocuses on network and information security, as an interesting candidate that could \nbe tasked as the dedicated regulatory agency for advanced algorithms. A new EU \nAgency is not required and would not improve oversight or efficiency in this sec-\ntor. The role of representative institutions in algorithmic regulation is also dis-\ncussed. Today’s parliaments may not yet be appropriately equipped, but their ca-\npacity can be strengthened to follow up on relevant regulatory provisions, e.g. in \nthe context of Post-Legislative Scrutiny. The book concludes that despite the \nabove concerns, governments should not be hesitant to invest in ameliorating the \nadministrative state. Still, the relevant technologies are not ripe enough and we \nshould use the time for the planning of regulatory principles and law-making. Sci-\nentific foresight and forward-thinking legal assessment should be widely em-\nployed in order to determine and regulate the effects of advanced algorithms in fu-\nture societies. All in all, this book constitutes a first of its kind in terms of \nrecording, classification and comparative legal assessment of significant cases of \nalgorithmic regulation, with a view to establishing best practice and a responsible \nway forward. \nApart from this introductory part, the book is structured in four further chap-\nters. Chapter 2 examines the evolution of algorithms and discusses broad concerns \nthat are raised over their ethical utilization. The necessity of advanced algorithms \nand of their regulation is highlighted. In this context, the application of some of \nthe core principles of administrative law, such as non-discrimination, accountabil-\nity and transparency, is discussed. Chapter 3 is dedicated to landmark cases of al-\ngorithmic regulation. In total, eight cases of algorithmic conduct from several reg-\nulatory modes are discussed. Chapter 4 covers the development of regulatory \nbodies and the several forms of oversight institutions with regards to algorithms \nthat have already been established. The role of representative institutions in algo-\nrithmic regulation is discussed and an existing EU agency, ENISA, is proposed as \na potential candidate to take on the role of a pan-European regulatory body. Chap-\nter 5 is devoted to the new perspectives around the regulation of advanced algo-\nrithms while presenting a summary of existing legal and administrative instru-\nments which prove to be rather insufficient when it comes to confronting the array \nof issues and problems related to advanced algorithms. \n\n9 \nReferences   \nAdams, Douglas. 1979. The Hitchhiker's Guide to the Galaxy. London: Pan Books. \nAho, Alfred V., John E. Hopcroft, and Jeffrey D. Ullman. 1983. Data structures and algorithms. \nBoston: Addison-Wesley Longman Publishing Co.  \nBooch, Grady. 2018. The History of Software Engineering. IEEE Software 35(5): 108-114. \nhttps://doi.org/10.1109/MS.2018.3571234.  \nBurrell, Jenna. 2016. How the machine ‘thinks’: Understanding opacity in machine learning al-\ngorithms. \n \nBig \nData \n& \nSociety \n(June \n2016): \n1-12. \nhttps://doi.org/10.1177/2053951715622512. \nClarke C., Arthur. 1968. 2001: A Space Odyssey. London: Hutchinson. \nCohen, Joshua and Joel Rogers. 1983. On Democracy: Toward a transformation of American \nSociety. Middlesex: Penguin.  \nDesmaris, Sacha, Pierre Dubreuil, and Benoît Loutrel. 2019. Regulation of social networks – Fa-\ncebook \nexperiment. \nInterim \nmission \nreport. \nMay \n2019. \nhttp://www.iicom.org/images/iic/themes/news/Reports/French-social-media-framework---\nMay-2019.pdf. Accessed 7 June 2019. \nEuropean \nCommission. \n2018. \nCode \nof \nPractice \non \nDisinformation. \nhttps://ec.europa.eu/newsroom/dae/document.cfm?doc_id=54454. Accessed 6 June 2019. \nFinck, Michèle. 2017. Digital Co-Regulation: Designing a Supranational Legal Framework for \nthe \nPlatform \nEconomy. \nLSE \nLaw, \nSociety \nand \nEconomy \nWorking \nPapers \n15/2017. http://dx.doi.org/10.2139/ssrn.2990043. Accessed 4 June 2019. \nGibbons, Llewellyn Joseph. 1997. No Regulation, Government Regulation, or Self-Regulation: \nSocial Enforcement or Social Contracting for Governance in Cyberspace. Cornell Journal of \nLaw and Public Policy 6(3): 475-551. \nGoertzel, Ben, and Cassio Pennachin, eds. 2007. Artificial General Intelligence. Berlin, Heidel-\nberg: Springer-Verlag. \nGold, Hadas, and Arnaud Siad. 2019. Why Mark Zuckerberg needed to impress Emmanuel Mac-\nron. CNN Business, May 10, 2019. https://edition.cnn.com/2019/05/10/tech/macron-\nzuckerberg-facebook-regulation/index.html. Accessed 5 June 2019. \nJayant, Nuggehally S. 1986. Coding speech at low bit rates: Advanced algorithms and hardware \nfor voice telecommunications are paring hit rates by at least a factor of four, without losing \nintelligibility. IEEE Spectrum 23(8): 58-63. https://doi.org/10.1109/mspec.1986.6371061.   \nJordana, Jacint, and David Levi-Faur, eds. 2004. The Politics of Regulation: Institutions and \nRegulatory Reforms for the Age of Governance. Cheltenham: Edward Elgar Publishing. \nKleinsteuber, Hans J. 2004. The Internet Between Regulation and Governance. In The Media \nFreedom Internet Cookbook, eds. Christian Möller and Arnaud Amouroux, 61-75. Vienna: \nOSCE. \nLeyland, Peter, and Gordon Anthony. 2012. Textbook on administrative law. 7th ed. Oxford: Ox-\nford University Press. \nLouridas, Panos. 2017. Real-world Algorithms: A Beginner's Guide. Cambridge: MIT Press. \nMacAdam, James. 1983. Rousseau's Democratic Principle. Journal of the History of Philosophy \n21(2):231-234. https://doi.org/10.1353/hph.1983.0031. \nOrbach, Barak. 2012. What Is Regulation? Yale Journal on Regulation Online 30(1): 1-10.   \nPasquale, Frank. 2015. Black Box Society. Harvard: Harvard University Press. \nSunstein, Cass R. 2017. #republic: Divided Democracy in the Age of Social Media. Princeton: \nPrinceton University Press. \nTrubek, David M., and Louise G. Trubek. 2007. New Governance & Legal Regulation: Com-\nplementarity, Rivalry, and Transformation. Columbia Journal of European Law 13(3): 539-\n564.   \nUnited Nations. 1948. Universal Declaration of Human Rights. https://www.un.org/en/universal-\ndeclaration-human-rights. Accessed 11 June 2019. \n\n10  \nWilamowski, Bogdan M. 2009. Neural network architectures and learning algorithms. IEEE In-\ndustrial Electronics Magazine 3(4): 53-63. https://doi.org/10.1109/mie.2009.934790.  \nWirth, Niklaus. 2008. A brief history of software engineering. IEEE Annals of the History of \nComputing 30(3): 32-39. https://doi.org/10.1109/MAHC.2008.33. \n \nFig. 1.1 From simple tasks to advanced algorithms  \n \nFig. 1.2 How algorithmic regulation touches upon major areas of law  \n\n2 Evolution of Advanced Algorithms     \nFotios Fitsilis     \nAbstract    \nTechnological evolution is based on advanced algorithms, but the pace of devel-\nopment raises concerns over their ethical, proper and legitimate utilization. This \nchapter discusses fundamental principles of administrative law, such as non-\ndiscrimination, accountability and transparency. As it seems, participation of ad-\nvanced algorithms in the lives of millions is to a great extent irreversible and regu-\nlation will be needed in order to confront issues related to their development and \nvalid operation.  \n \nKeywords: administrative law, algorithmic discrimination, accountability, trans-\nparency, counterfactual explanations.  \n2.1 Overview     \nNew technologies that have the potential to re-shape human societies are emerging \nday by day in an unprecedented way and often at extraordinary and perhaps ‘un-\nhealthy’ speed, as in the case of unregulated social media technology. Further-\nmore, it is difficult to find any aspect of our everyday lives that is not affected by \nthese technologies, from commerce and farming, to medical care and education1. \nThis technological revolution of our times is often characterized as the third tech-\nnological revolution.2 Gartner presents the top trends for emerging technologies in \nits annual ‘Hype Cycle’.3 From the broad range of fields presented therein, we ex-\ntract the following as the most relevant to potentially affect law4 and, in particular, \nadministrative law: Artificial Intelligence (AI), Deep Learning, Machine Learn-\n                                                           \n1 See, e.g., European Parliament (2017), p. 3. \n2 In Schwab (2016), Klaus Schwab, founder and executive chairman of the \nWorld Economic Forum, speaks of a ‘fourth industrial revolution’ that unlocks \nnew opportunities in human-machine interaction.  \n3 See Panetta (2017). \n4 Kemp (2016) presents legal aspects of three AI case studies, i.e. legal ser-\nvices, autonomous vehicles and smart contracts. \n\n12  \ning, Cognitive Computing, Blockchain, Virtual Reality and Augmented Reality. \nThese technologies have one thing in common; they are all heavily reliant on algo-\nrithms and, hence, we shall use in this book the collective term ‘advanced algo-\nrithms’ to describe them.5 In the past, some of these technologies have been con-\nsidered as being nothing more than mere science fiction. In the (not too distant) \nfuture, they may very well be the driving forces of human civilization.  \nAdvanced algorithms do not represent a technology of the future. Algorithms \nare already in force and here to stay. Their application in several economic and \ntechnical domains has already dramatically changed the way we do things. The \nongoing automation of several industrial (and everyday) processes, previously per-\nformed by human beings, is saving a lot of time and resources while producing \never more accurate data and results. Notably, today, even warfare is changing \ncompared to the past due to the use of drones and robotics, which allow for the \nengaging of targets through computer screens and dark rooms, thousand miles \naway from where the actual combat may be taking place.6 This is why these tech-\nnologies may also referred to as ‘critical infrastructure’.7 \nThe pace of development of these technologies raises legitimate ethical con-\ncerns over their proper utilization. Despite the overall undisputed benefits of ad-\nvanced algorithms, their introduction in a range of sectors may be linked to certain \ndangers, or threats, that need to be addressed for these technologies to fully exploit \ntheir full and positive potential. The replacement of human labor by robots and al-\ngorithms in several levels of industrial production and administrative processes is \ncausing a lot of anxiety around the rise of advanced algorithms. Taking this into \nconsideration, societies and governments first need to clearly identify these con-\ncerns in order to proceed with effective oversight and control mechanisms, legisla-\ntion and other countermeasures. The fact that man-made processes are now more \naccurate and fast, directly affects administrative science, as well as law. However, \nthe application of advanced algorithms in the public domain can be problematic \nand may have several implications. Administrative law directly affects our every-\nday lives, from food security to public safety and security.8 Thus, one of the pur-\nposes of this book is to examine from a holistic perspective the influence of these \nnew technologies on administrative law. New technologies change today’s socie-\nties in an unparalleled way and they will keep on transforming them. In bureau-\ncratic procedures, we have managed to limit human errors extensively through the \nuse of modern Information and Communication Technologies (ICT), including the \ninternet and organizations’ intranets. In public administration various organiza-\ntional, managerial and archiving processes are usually redundant and time-\n                                                           \n5 See Chapter 1 for a description of the shift from ‘algorithms’ to ‘advanced al-\ngorithms’. \n6 This is acknowledged in the relevant resolution by the Council of Europe \n(2015), as well as by Pasquale (2016). \n7 See Stone et al. (2016), p. 44. \n8 See Coglianese and Lehr (2017), p. 26. \n\n13 \nconsuming. Several of these are usually repetitive and non-critical tasks. The in-\ntroduction of advanced algorithms can be decisive in the automation of these pro-\ncedures, which may improve the quality of management and decision-making.9 \nHence, algorithms can be used to utilize employee output more efficiently, so that \nthey spend quality working time on essential and qualitative tasks.  \nThe previous examples highlight the advantages in the use of advanced algo-\nrithms, but this does not mean that the introduction of such technologies comes \nwithout risks. In this regard, a first question that needs to be asked is to what ex-\ntent we can legitimately and safely utilize algorithms in the administrative state, \nwhile maintaining an acceptable level of oversight and control? Certain fundamen-\ntal principles of administrative law will be threatened if algorithms are left to op-\nerate without an appropriate legal framework.10 In general, the principles of ad-\nministrative law are concerned with human decisions involved in the exercise of \nstate power and discretion. Furthermore, such principles offer a promising founda-\ntion for a regulatory framework for the growing number of algorithm-based deci-\nsions within the public sector. AI, as a sub-domain of advanced algorithms but \nmaybe also the final frontier,11 has attracted a lot of interest because of the afore-\nmentioned benefits. At the same time, it is being ferociously criticized for the pos-\nsible negative outcomes of its application. This fear rises from the particular char-\nacteristics of algorithms. In the following section, some of the fundamental \nprinciples of administrative law, i.e. non-discrimination, accountability and trans-\nparency, will be discussed in the light of these emerging technologies.12  \n \n2.1.1 Non-discrimination     \nWhen it comes to law, many concerns are based on the fact that laws have been \ndeveloped to apply to human beings, not to complex self-taught algorithms.13 First \nof all, there are concerns regarding the discriminating ‘behavior’ of such systems, \nsince the non-discrimination principle is an essential element of administrative \nlaw. Algorithms depend on variables and boundary conditions. Choosing the spe-\ncific variables for an algorithm is not an objective task. It is the developer, the \n                                                           \n9 See Ng (2017).  \n10 See European Parliament (2017), p. 6. \n11 In this case, we speak of Artificial General Intelligence; see, e.g., Goertzel \nand Pennachin (2007). \n12 When dealing with ‘Explainable AI’, a set of additional principles, each of \nwhich constitutes a separate research topic, may also apply, such as bias, fairness, \ntransparency, safety, causality and engineering. More on the topic in Wierzynski \n(2018).  \n13 See Coglianese and Lehr (2017), p. 6. \n\n14  \nmanager (private sector) or the state actor (public sector/government) who ulti-\nmately chooses the specific variables for a given scenario. This may lead to prob-\nlems, or conflicts of interest, since these choices define the way the algorithm \nworks, thus any bias in the variables may be directly translated within the technol-\nogy into discriminatory results.14  \nMoreover, there is a well-founded concern that inherently discriminatory auto-\nmated processes will prevail in the decision-making because of the evolutionary \nnature of some algorithms, which is where establishing an acceptable level of con-\ntrol comes in. However, such concerns are not only known within the context of \nadvanced algorithms. Bias and partisanship are also part of human nature and be-\nhavior.15 Humans often succumb to discriminatory practices. Hence, when it \ncomes to the incorporation of advanced algorithms into administrative processes, \ndiscrimination must be avoided, independently of who makes the decision, be it a \nhuman or an algorithm.16  \nAt this point, it needs to be noted that even if algorithms engender biased out-\ncomes, this does not necessarily mean that such outcomes are, or should be re-\ngarded, as legally problematic. It is possible for administrative decision-makers to \ndifferentiate between applications made to them, so long as any differential treat-\nment is justified and proportionate.17 \n2.1.2 Accountability     \nQuestions of accountability become even more urgent in relation to the use of al-\ngorithms.18 In the traditional context of the administrative state, it is the people \nwho make specific decisions which naturally have certain (legal) consequences. \nThus, they can be held accountable for their actions or omissions. It has been fair-\nly easy to locate the exact person(s) who have been responsible for a specific ad-\nministrative action and to hold them accountable for it.19 Unfortunately, that may \nnot be the case with some algorithms due to the ambiguous nature of the decision-\nmaking processes. Algorithms are creations of the human intellect, which obey \n                                                           \n14 See, e.g., Ng (2017); Williams et al. (2018) assesses ‘unacceptable judge-\nment(s)’ of algorithmic decision-making processes.  \n15 See, e.g., Ng (2017). \n16 See Coglianese and Lehr (2017), p. 59; Algorithmic bias is further analyzed \nin infra Section 2.2. \n17 This point is also true, to a certain extent, when discussing issues of ‘ac-\ncountability’ and ‘transparency’, though there might be an argument that those are \ncore values that underlie the principles of administrative law. \n18 See, e.g., Ng (2017). \n19 On the legal nature of (automated) administrative actions see, e.g., Lazaratos \n(1989).  \n\n15 \ntheir core code. Similarly, in the past, it has been possible to track down the per-\nson responsible for a programming fault, e.g. for conducting wrongful/illegal bank \ntransactions or influencing an industrial facility. However, advanced algorithms \nincorporate deep-learning, self-learning, fuzzy logic and a number of advanced \ntechnologies that largely decouple the human developer from the creation, that of \nthe algorithm. When utilizing such technology, mistakes or mishaps will happen, \njust like in the case of their human operatives. In such cases, who (or what) is to \nbe held accountable and which are the (legal) consequences? Ultimately, in the \nadministrative state, does it make any (legal) sense to hold an algorithm accounta-\nble for a wrong or bad decision? In the criminal law domain, particularly in US, \nwhere great importance to the concept of mens rea, the intending mind, is at-\ntached, challenges are raised when advanced algorithms come into play.20 Clearly, \nit is at best questionable to place accountability on machines and complex codes \nthan on the humans who realized the manufacture and programming of the ma-\nchines or developed the code.  \nA recent direction to answering this legal dilemma has been provided in 2016 \nin response to the so-called ‘monkey selfie case’, where copyrights have not been \nprovided to a monkey that shot a ‘selfie’.21 Following this decision, the US Copy-\nright Office updated its rules which now include a passage stating that ‘works \nproduced by a machine or mere mechanical process that operates randomly or au-\ntomatically without any creative input or intervention from a human author’ do not \nqualify for copyright protection under US law. However, there is legal dispute \nwhether the above decision tackles issues of ‘ownership’. Similarly, under US Pa-\ntent Law, there is not an explicit prohibition of protection for inventions of AI.22 \nWhere there is no legal ground, and in the instance of AI, where this is the case, \none may resort to alternative readings to look for guidance. For once, there are the \nAsimov’s laws, also known as the ‘Three Laws of Robotics’ (a fourth one was \nadded later), which aim to protect the integrity of human lives when interacting \nwith robots. Even though these laws have been created for science fiction novels, \nthey are considered crucial in modern discourse concerning AI and robots. In this \nregard, they have also been considered in the European Parliament’s report with \nrecommendations to the Commission on Civil Law Rules on Robotics.23 In the \nlight of the above, legislators will need to take interdisciplinary action in order to \nexamine ways to at least partially regulate this highly dynamic scientific field. \n                                                           \n20 See Stone et al. (2016), p. 47.  \n21 Naruto v. David John Slater et al., 3:2015-cv-04324 (9th Cir.). \n22 Title 35 of the U.S. Patent Code. \n23 See European Parliament (2017), p. 6.  \n\n16  \n2.1.3 Transparency \nAs discussed above, it is difficult to assess whether an algorithm is truly accounta-\nble, due to its so-called ‘black box’ nature, which prohibits the deeper understand-\ning of how and why it reached a conclusion in a particular task.24 This means that \nthe reasoning behind its decisions may not be evident. If it is not clear how an al-\ngorithm has come to a decision, it may be particularly difficult to detect any bias \nin the decision-making process and to test if a decision is fair or not. This potential \nlack of transparency needs to be addressed carefully in an administrative state, \nsince there may be serious concerns over the safeguard of democratic governance \nwith the introduction of AI.25 In addition, lack of transparency, which is an essen-\ntial value of the democratic administrative state, due to the use of advanced algo-\nrithms, may lead to the questioning of legitimacy of administrative decisions. \nThe European Union is attempting to tackle the ‘black box’ problem and the re-\nsulting transparency issue by providing to EU citizens the so-called ‘right to ex-\nplanation’.26 The ‘right to explanation’ can be identical with the personal right to \nbe provided with ‘meaningful information about the logic involved’ in automated \ndecisions.27 However, scholars argue on the legal existence and feasibility of such \na theoretical right.28  In principle, the ‘right to explanation’ intersects with the duty \nto give reasons for decisions, be it administrative29 or judicial.30 Moreover, it relies \non the well-founded right to information.31  \nAnother area of concern is related to the impact of advanced algorithms on em-\nployment. Replacing humans with machines has been a major trend since the first \nindustrial revolution, a trend that led to the formation of well-organized unions, as \na response to the threat to job security. Algorithms and machines are therefore \ntrusted, to an extent, to be faster, stronger and more efficient and effective than \nhuman beings. The term ‘digital unemployment’ has recently been coined to re-\nflect the loss of jobs because of the rise of digital technology. Currently, only \nstandardized procedures have been taken over by algorithms, while more complex \ntasks remain, at least for now, in human hands. In the past, machines have been \nseen as a way to ‘liberate’ the human race from hard labor, but today they may \npose a threat to a multitude of professions, both blue collar and white collar ones, \n                                                           \n24 See Coglianese and Lehr (2017), p. 18. \n25 See Coglianese and Lehr (2017), p. 49. \n26 This is attempted via the General Data Protection Regulation (EU) 2016/679, \nthough there is no single provision therein labeled as such. \n27 According to Selbst and Powles (2017), p. 242. \n28 See, e.g., Wachter et al. (2017).  \n29 In the case of administrative decisions in international law, a legal duty for \nadministrators appears to be contentious; see Hepburn (2012), p. 641. \n30 On the justification of judicial decisions see Shuman (1971). \n31 See, e.g., Peled and Rabin (2011).  \n\n17 \neven to that of the judge. While it may be disputed whether an AI would be ever \nable to completely replace a human judge, this may very well be the case in stand-\nardized cases in lower instances of administrative courts. Recent research using \nNatural Language Processing and Machine Learning technology was able to estab-\nlish predictive models in judicial decisions.32 Nevertheless, new studies are pro-\njecting AI under a whole new light, suggesting that its inherent dynamics regard-\ning employment might have changed, and are projecting two million new jobs in \n2025.33 \nAnother, positive this time, influence of new technologies is related to the in-\ntroduction of ICT in the judicial sector, as part of accepted and necessary reforms \nacross both the civil and criminal justice sectors. Currently, a large effort to net-\nwork judicial institutions is widely materializing, thus making judicial decisions \neasily accessible and the judicial process significantly more efficient.34 This de-\nvelopment towards an inter-linked judicial community is expected to continue \nwith the standardization of judicial documents, e.g. using the Akoma Ntoso XML-\nschema for legal documents.35 When it comes to legal informatics, it has been \nshown that discrepancies in the legal outcome may arise when digitizing formerly \nmanual procedures.36 Given that technology (the code) is a regulatory modality af-\nfecting human behavior, it should be ensured that the law is the driving force be-\nhind it. To achieve this goal, ‘the content of a law-related system should be de-\nfined, having as a guideline the hierarchy of legal sources prescribed by the legal \norder’ and then be tested on whether it serves its purpose satisfactory.37  \nThere are many additional challenges to be dealt with when handling new tech-\nnologies, including, but not limited to, the increase in existing inequalities, the \nbreach or invasion of private life, or the enhancement of discriminatory practic-\nes.38 These challenges exist not only for governments but also for the private sec-\n                                                           \n32  See relevant research by Aletras et al. (2016).  \n33 See Gartner (2017). \n34 The Council of the European Union in 2010 has taken the decision to imple-\nment the European Case Law Identifier (ECLI), which is a human readable and \ncomputer processable code that be assigned to every judicial decision from every \nnational or European court in order to facilitate cross-border accessibility of case \nlaw; see also the online resources of BO ECLI (2017), an EU project involving \nseveral partners from 10 member states.   \n35 In 2018, Akoma Ntoso has become an OASIS standard; the document archi-\ntecture, the relevant schemes and the general framework for legal documents are \navailable at Akoma Ntoso (2018); detailed information on the rationale of stand-\nard-based management of legal documents as well as a discussion of semantic re-\nsources is presented by Sartor et al. (2011).  \n36 See Yannopoulos (2012), p. 1024. \n37 See Yannopoulos (2012), p. 1028. \n38 According to Stone et al. (2016).  \n\n18  \ntor, which is pioneering the field. However, one should keep in mind that only \ngovernments enjoy over the regulatory initiative and the means to enforce it. \n2.2 Necessity of Advanced Algorithms and their Regulation     \nThe discussion about the necessity of advanced algorithms in contemporary and \nfuture societies, as well as the parameters of their regulation, is a complex one. A \nbasic set of research questions has been posed in the introductory chapter. In order \nto approach it in a structured order, it can be further broken down into a more spe-\ncialized set of sub-questions, such as the following:  \n• Is the use of advanced algorithms inevitable for open societies that need to reg-\nulate the lives of millions or even billions of people?  \n• Which are the underlying ethical dilemmas when developing or applying ad-\nvanced algorithms?  \nIt is outside the context of this book to tackle all these issues in detail. Instead, \na more pragmatic approach is followed in order to discuss the relevant framework \nby presenting a series of recent topics related to advanced algorithms. At the same \ntime, the book sheds light on the regulatory principles or guidelines that state ac-\ntors, be it the government or the judiciary, pursue in cases that involve utilization \nof advanced algorithms. Hence, we will try to support earlier claims that indicate \nshortcomings in existing legal instruments to confront issues related to advanced \nalgorithms.39 \nAlgorithms are omnipresent. In some areas, such as the case of welfare delivery \nand monitoring, their integration is progressing ‘at a breathtaking pace with little \nor no political discussion about their impacts’.40 Yet, the inherent properties of \nsoftware that make it attractive for certain applications can be potentially danger-\nous for others.41 Advanced algorithms in the form of always-on smart home devis-\nes and digital assistants42 have entered into the mainstream consumer market and \nenjoy a wide acceptance despite well-founded criticism. That such devices may \npose a threat to privacy has become visible in the US case of an Amazon’s Echo \ndevice, which has recorded and transmitted, without prior consent, a private com-\n                                                           \n39 See Barocas (2014). \n40 See Eubanks (2018), pp. 11-12. \n41 According to Grimmelmann (2005, p. 1758); in this regard, it is discussed \nwhether software can be a good regulatory tool for rule- and standard-based deci-\nsions. This distinction is considered a staple of jurisprudential theory, since soft-\nware applies rules rather than standards (Grimmelmann 2005, p. 1732). \n42 See, e.g., Google assistant (2018), a digital assistant that may handle a varie-\nty of everyday tasks up to a full first-person conversation with a human counter-\npart.  \n\n19 \nmunication to a random third party.43 Earlier, in 2017, it was demonstrated that \nsuch a device could be turned into a wiretap.44 While it is unclear whether there \nwill be an investigation from the competent authorities, or any legal action, it be-\ncomes clear that the use of such devises is linked to serious privacy and security \nthreats that have neither been tackled, nor fully understood as of yet. In a further \ncase, a matter of, literally, life and death, dramatically increased public interest \nand awareness in the UK to the potentially negative outcomes of applications of \nadvanced algorithms.  \nIn May 2018, it was reported that a bug in the information system of the Na-\ntional Health Service in Britain may have caused, from 2009 to date, the prema-\nture death of up to 270 women suffering from cancer.45 An algorithm that prevent-\ned sending alerts for planned mammograms to a specific age group of women is \nconsidered ‘responsible’ for this mishap. However, there is still much controversy \namong experts about how this ‘algorithm failure’ came about and whether or not \nany scandal had happened at all.46 A further concern in relation to advanced algo-\nrithms is their sometimes biased operation. Although one would expect such algo-\nrithms not to express favoritism, these may rely on existing data sets from real en-\nvironments or they may depict the mindset of their human programmers. Hence, if \nno proper or effective counteraction is taken, advanced algorithms may simply \npropagate or even maximize the original bias. A source of further consideration is \nthat the most vulnerable parts of society are the ones that are going to be more \nprone to being exposed to algorithmic decisions and their potentially discriminato-\nry outcomes.47 This is a notion that seems to be attracting an increasing level of \nsupport.48 \nAs computer programs and their underlying electronic systems are becoming \nmore complex and opaque,49 the task to debug advanced algorithms may prove \nchallenging. Such a discriminatory behavior may be either intentional or uninten-\ntional. An example of an intentionally induced discriminatory behavior is differen-\ntial pricing.50 The Correctional Offender Management Profiling for Alternative \n                                                           \n43 According to a KIRO7 report by Horcher (2018). \n44 See Barnes (2017). \n45 Jeremy Hunt, Secretary of State for Health and Social Care, brought the rele-\nvant issue to the House of Commons on 2 May 2018. The statement and the con-\nsequent debate may be found in the minutes of proceedings of the House; see \nHouse of Commons (2018).  \n46 Charette (2018) presents the public debate in an IEEE Spectrum report.  \n47 See, e.g., O'Neil (2016). \n48 See also Eubanks (2018). \n49 There exists rich literature on the problems that arise due to the opacity of \nadvanced algorithms. For more information, the reader may refer to Pascuale \n(2015), Burrell (2016) and de Laat (2017). \n50 The term is used when online users get different prices for the same product, \ndepending on the result of a system evaluation of their available personal data, al-\n\n20  \nSanctions (COMPAS) algorithm offers one of the most prominent examples of un-\nintentional discriminatory behavior. This is a computer program developed by \nNorthpointe, Inc. in US, in order to provide judges with risk assessments for the \nfuture criminal behavior of convicted individuals at every stage of the US criminal \njustice system. The US Department of Justice supports the use of such tools and a \nrelevant sentencing reform bill is currently pending in Congress.51 Previous evalu-\nations of the COMPAS risk models were encouraging52 and assessed that the algo-\nrithm ‘(…) performs well in predicting risk for offenders released from jail pretri-\nal’.53 However, in 2016, a larger independent examination by ProPublica54 showed \nthat the algorithm was in fact biased against black prisoners. The company con-\ntested the results of this study but declined to provide access to the algorithm, \nwhich calculates a risk assessment score based on 137 questions either answered \nby offenders or retrieved from their criminal records.55  \nIn a controversial case of prison labor, inmates in Finland have been used by a \nstartup company to classify data used in the training of AI algorithms56. The latter \nargues that this is a win-win situation for both the company and the prisoners, as \nthe approach helps the prisoners develop modern work skills. However, this thesis \ndoes not remain uncontested both ethically, as well as in its essence.57  \nCases such as these have long been sources of concern, particularly among hu-\nman rights activists and data ethics scientists. With advances in machine learning \ntechnology, these voices are becoming louder, even predicting that ‘AI could spell \nthe end for the human race’.58 \nAll these separate cases, whether small-scale incidents, as in Amazon’s Echo \ncase, or serious systemic errors, as in the COMPAS case, demonstrate that these \nconcerns have a solid foundation and need to be tackled sooner, rather than later. \nRecently, ICT giants have taken significant steps towards a more people-centric \n                                                                                                                                     \nso called ‘profiling’. Consequently, a used assessed as ‘wealthy’ may get a higher \nprice compared to the average user. According to Regulation (EU) 2016/679 –this \nis the General Data Protection Regulation– profiling is only allowed following ex-\nplicit user consent.   \n51 This is the Sentencing Reform and Corrections Act of 2015 at the 114th \nCongress (2015-2016).  \n52 See concluding remarks in Brennan et al. (2009), p. 34. \n53 See Blomberg et al. (2010), p. 91. \n54 ProPublica is an independent, nonprofit network of investigative journalists.  \n55 See the relevant report by Angwin et al. (2016).  \n56 The case was presented by Chen (2019) in theverge.com.  \n57 By Sarah T. Roberts, University of California, Los Angeles; in Chen (2019), \nsupra note 56.  \n58 According to Hawking (2014). \n\n21 \napproach to AI.59 Researchers are currently developing tools to help understand \nthe internal operation of (deep) machine learning systems, a research field that is \ncalled ‘Explainable AI’.60 IBM has announced an open-source toolkit to check for \nand mitigate unwanted bias in datasets, machine learning models and state-of-the-\nart algorithms.61 Another relatively new concept is the previously mentioned ‘right \nto explanation’. \nShould such a right be recognized to legal subjects, it could cause developers to \nstructure their algorithms and systems differently in order to offer a higher degree \nof transparency. This could seriously affect their functionality and operation. \nMoreover, it is not always possible for the general public nor the developer to un-\nderstand how an automated system has reached a particular decision. To make \nthings worse, automated decision-making systems, e.g. in the form of Deep Neural \nNetworks (DNNs), are changing rapidly because they constantly learn. As a result, \nan explanation that applies when at a certain stage of a DNN may not be the case \nin another. This is why a minimal solution has been developed in the form of what \nis called ‘counterfactual explanations’. Counterfactual explanations ‘provide in-\nsight into which external facts could be different in order to arrive at a desired \noutcome’.62 Other scholars call for paying more attention to socio-technical sys-\ntems, in order to come up with suggestions for the improvement of algorithmic \nregulation.63 From the above, it can be deducted that participation of advanced al-\ngorithms in the lives of millions is irreversible to a great extent. Regulation is \nneeded and several reasons that speak for such regulation have been discussed, \nsuch as biased operation and algorithmic malpractice.  \nChapter 3 presents in more detail a number of topics of algorithmic regulation. \nHowever, the extent of regulation will inevitably vary from case to case. Societies \n(rather than functionaries) will have to determine the degree of regulation and this \nis why the role of representative institutions is eventually going to be important.64 \nReferences \nAkoma Ntoso. 2018. Version 1.0 Part 1: XML Vocabulary. OASIS Standard. Edited by Monica \nPalmirani, Roger Sperberg, Grant Vergottini, and Fabio Vitali. http://docs.oasis-\n                                                           \n59 See for instance Google’s People + AI Research program, a ‘Human-\ncentered research and design to make AI partnerships productive, enjoyable, and \nfair’, according to Google PAIR (2018).  \n60 See, e.g., Wierzynski (2018).  \n61 See Varshney (2018). \n62 See Wachter et al. (2018), p. 880. \n63 See Medina (2015). \n64 The role of parliaments in the regulatory framework of advanced algorithms \nis discussed in Chapter 4: Development of Regulatory Bodies \n\n22  \nopen.org/legaldocml/akn-core/v1.0/os/part1-vocabulary/akn-core-v1.0-os-part1-\nvocabulary.html. Accessed 5 June 2019.  \nAletras, Nikolaos, Dimitrios Tsarapatsanis, Daniel Preoţiuc-Pietro, and Vasileios Lampos. 2016. \nPredicting judicial decisions of the European Court of Human Rights: a Natural Language \nProcessing perspective. PeerJ Computer Science 2:e93. https://doi.org/10.7717/peerj-cs.93. \nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine bias. ProPublica, \nMay \n23. \nhttps://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-\nsentencing. Accessed on 7 July 2018.  \nBarnes, \nMark. \n2017. \nAlexa, \nare \nyou \nlistening? \nMRW \nInfosecurity. \nAugust \n1. \nhttps://labs.mwrinfosecurity.com/blog/alexa-are-you-listening. Accessed 22 March 2019.  \nBarocas, Solon. 2014. Data mining and the discourse on discrimination. Conference on \nKnowledge \nDiscovery \nand \nData \nMining. \nAugust \n24-27. \nNew \nYork \nCity. \nhttps://dataethics.github.io/proceedings/DataMiningandtheDiscourseOnDiscrimination.pdf. \nAccessed 22 March 2019. \nBlomberg, Thomas, William Bales, Karen Mann, Ryan Meldrum, and Joe Nedelec. 2010. Vali-\ndation \nof \nthe \nCOMPAS \nrisk \nassessment \nclassification \ninstrument. \nhttp://criminology.fsu.edu/wp-content/uploads/Validation-of-the-COMPAS-Risk-\nAssessment-Classification-Instrument.pdf. Accessed 28 June 2018. \nBO ECLI. 2017. Building on European Case Law Identifier. http://bo-ecli.eu/. Accessed 22 \nMarch 2019.  \nBrennan, Tim, William Dieterich, and Beate Ehret. 2009. Evaluating the predictive validity of \nthe COMPAS risk and needs assessment system. Criminal Justice and Behavior 36 (1): 21-\n40. https://doi.org/10.1177/0093854808326545.  \nBurrell, Jenna. 2016. How the machine ‘thinks’: Understanding opacity in machine learning al-\ngorithms. \n \nBig \nData \n& \nSociety \n(June \n2016): \n1-12. \nhttps://doi.org/10.1177/2053951715622512. \nCharette, Robert. N.. 2018. 450,000 Women Missed Breast Cancer Screenings Due to “Algo-\nrithm \nFailure”. \nIEEE \nSpectrum, \nMay \n11. \nhttps://spectrum.ieee.org/riskfactor/computing/it/450000-woman-missed-breast-cancer-\nscreening-exams-in-uk-due-to-algorithm-failure.  \nChen, Angela. 2019. Inmates in Finland are training AI as part of prison labor. TheVerge.com, \nMarch \n28. \nhttps://www.theverge.com/2019/3/28/18285572/prison-labor-finland-artificial-\nintelligence-data-tagging-vainu. Accessed 5 June 2019. \nCoglianese, Cary, and David Lehr. 2017. Regulating by Robot: Administrative Decision Making \nin the Machine-Learning Era. Research Paper No. 17-8. Institute for Law and Economics. \nUniversity of Pennsylvania. https://www.law.upenn.edu/live/files/6329-coglianese-and-lehr-\nregulating-by-robot-penn-ile. Accessed 22 March 2019. \nCouncil of Europe. 2015. Drones and targeted killings: the need to uphold human rights and in-\nternational law. Resolution 2051. http://assembly.coe.int/nw/xml/XRef/Xref-DocDetails-\nEN.asp?FileID=21746&lang=EN. Accessed 22 March 2019. \nde Laat, Paul B. 2017. Algorithmic Decision-Making Based on Machine Learning from Big Da-\nta: Can Transparency Restore Accountability? Philosophy & Technology 31: 525–541. \nhttps://doi.org/10.1007/s13347-017-0293-z.  \nEubanks, Virginia. 2018. Automating inequality: How high-tech tools profile, police, and punish \nthe poor, New York: St. Martin’s Press. \nEuropean Parliament. 2017. Motion for a European Parliament resolution with recommendations \nto \nthe \nCommission \non \nCivil \nLaw \nRules \non \nRobotics \n(2015/2103(INL)). \nhttp://www.europarl.europa.eu/doceo/document/A-8-2017-0005_EN.pdf. Accessed 22 March \n2019. \nGartner. 2017. Gartner Says By 2020, Artificial Intelligence Will Create More Jobs Than It \nEliminates. Press release. https://www.gartner.com/en/newsroom/press-releases/2017-12-13-\ngartner-says-by-2020-artificial-intelligence-will-create-more-jobs-than-it-eliminates. \nAc-\ncessed 22 March 2019. \n\n23 \nGoertzel, Ben, and Cassio Pennachin, eds. 2007. Artificial General Intelligence. Berlin, Heidel-\nberg: Springer-Verlag. \nGoogle assistant. 2018. https://assistant.google.com/. Accessed 26 June 2018.  \nGoogle PAIR. 2018. https://ai.google/research/teams/brain/pair. Accessed 26 June 2018. \nGrimmelmann, James. 2005. Regulation by software. Yale Law Journal 114 (7): 1719-58.  \nHawking, Steven. 2014. BBC, December 2. https://www.youtube.com/watch?v=fFLVyWBDTfo. \nHepburn, Jarrod. 2012. The Duty to give reasons for administrative decisions in international \nlaw. \n \nThe \nInternational \nand \nComparative \nLaw \nQuarterly \n61(3): \n641-663. \nhttps://doi.org/10.1017/S0020589312000309. \nHorcher, Gary. 2018. Woman says her Amazon device recorded private conversation, sent it out \nto random contact. KIRO7, May 25. https://www.kiro7.com/news/local/woman-says-her-\namazon-device-recorded-private-conversation-sent-it-out-to-random-contact/755507974.  \nHouse \nof \nCommons. \n2018. \nMinutes \nof \nProceedings, \nMay \n2. \nhttps://hansard.parliament.uk/commons/2018-05-02/debates/BE9DB48A-C9FF-401B-AC54-\nFF53BC5BD83E/BreastCancerScreening.  \nKemp, \nRichard. \n2016. \nLegal \naspects \nof \nArtificial \nIntelligence. \nKemp \nIT \nLaw. \nhttp://www.kempitlaw.com/wp-content/uploads/2016/11/Legal-Aspects-of-AI-Kemp-IT-\nLaw-v1.0-Nov-2016-2.pdf. Accessed 22 March 2019. \nLazaratos, Panagiotis. 1989. Rechtliche Auswirkungen der Verwaltungsautomation auf das Ver-\nwaltungsverfahren. \nDoctoral \ndiss., \nUniversity \nof \nTübingen, \nGermany. \nhttps://www.didaktorika.gr/eadd/handle/10442/4608. Accessed 22 March 2018. \nMedina, Eden. 2015. Rethinking algorithmic regulation. Kybernetes 44 (6/7): 1005-19. \nhttps://doi.org/10.1108/K-02-2015-0052. \nNg, Vivian. 2017. Algorithmic Decision-Making and Human Rights, Human Rights. Big Data \nand Technology Project. https://www.hrbdt.ac.uk/algorithmic-decision-making-and-human-\nrights/. Accessed 22 March 2018. \nO'Neil, Cathy. 2016. Weapons of math destruction: How big data increases inequality and \nthreatens democracy, New York: Crown Publishers. \nPanetta, Kasey. 2017. Top Trends in the Gartner Hype Cycle for Emerging Technologies, 2017. \nGartner. \nhttp://www.gartner.com/smarterwithgartner/top-trends-in-the-gartner-hype-cycle-\nfor-emerging-technologies-2017/. Accessed 22 March 2019. \nPasquale, Frank. 2015. Black Box Society. Harvard: Harvard University Press. \nPasquale, Frank. 2016. The Emerging Law of Algorithms, Robots, and Predictive Analytics. \nhttps://balkin.blogspot.gr/2016/02/the-emerging-law-of-algorithms-robots.html. Accessed on \n8 June 2018. \nPeled, Roy, and Yoram Rabin. 2011. The Constitutional Right to Information. Columbia Human \nRights Law Review 42(2): 357-401. \nPeople + AI Research (2018). Google AI. https://ai.google/research/teams/brain/pair. Accessed \non 28 June 2018. \nSartor, Giovanni, Monica Palmirani, Enrico Francesconi, and Maria Angela Biasiotti (eds.). \n2011. Legislative XML for the semantic web: principles, models, standards for document \nmanagement (vol. 4). Dordrecht: Springer. \nSchwab, Klaus. 2016. The fourth industrial revolution. Geneva: World Economic Forum. \nSelbst, Andrew D., and Julia Powles. 2017. Meaningful information and the right to explanation. \nInternational Data Privacy Law 7.4: 233-242. https://doi.org/10.1093/idpl/ipx022. \nShuman, Samuel I. 1971. Justification of Judicial Decisions. California Law Review 59: 715-\n732. https://doi.org/10.15779/Z38QN1W. \nStone, Peter et al. 2016. Artificial Intelligence and life in 2030. One Hundred Year Study on Ar-\ntificial \nIntelligence. \nStanford \nUniversity. \nReport \nof \nthe \n2015 \nStudy \nPanel. \nhttps://ai100.stanford.edu/sites/g/files/sbiybj9861/f/ai_100_report_0831fnl.pdf. Accessed 20 \nMarch 2019. \nVarshney, \nKush. \n2018. \nIntroducing \nAI \nFairness \n360. \nIBM \nresearch \nblog.  \nhttps://www.ibm.com/blogs/research/2018/09/ai-fairness-360/. Accessed 4 June 2019. \n\n24  \nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2018. Counterfactual Explanations With-\nout Opening the Black Box: Automated Decisions and the GDPR. Harvard Journal of Law & \nTechnology 31 no.2 (Spring):841-887. \nWachter, Sandra, Brent Mittelstadt, and Luciano Floridi. 2017. Why a Right to Explanation of \nAutomated Decision-Making Does Not Exist in the General Data Protection Regulation. In-\nternational Data Privacy Law 7(2): 76-99. \nWierzynski, Casimir. 2018. The Challenges and Opportunities of Explainable AIIntel AI. \nhttps://ai.intel.com/the-challenges-and-opportunities-of-explainable-ai/. Accessed 22 March \n2019.  \nWilliams, Betsy Anne, Catherine F. Brooks, and Yotam Shmargad. 2018. How Algorithms Dis-\ncriminate Based on Data They Lack: Challenges, Solutions, and Policy Implications. Journal \nof Information Policy 8: 78-115. https://doi.org/10.5325/jinfopoli.8.2018.0078. \nYannopoulos, Georgios. 2012. Technology beats the law? In Values and Freedoms in Modern \nInformation Law and Ethics, eds. Maria Bottis, Eugenia Alexandropoulou, and Ioannis \nIglezakis, 1024-1031. Athens: Nomiki Bibliothiki. \n \n\n3 Administrative and Judicial Decisions on \nAdvanced Algorithms     \nFotios Fitsilis     \nAbstract    \nThis chapter constitutes the main chapter of the book, where landmark cases of al-\ngorithmic regulation are presented. In total, eight cases of algorithmic conduct are \ndiscussed. These involve both direct and indirect examples of regulation. Among \nothers, examples from the ICT sector (data-protection, internet browsers and ad-\nblockers) and the gig economy (i.e. Airbnb and Uber) are presented and discussed. \nRecent regulatory action, such as in the cases of the diesel emissions scandal and \nhigh-frequency trading are also analyzed. \nKeywords: Media player, Dieselgate, Airbnb, Uber, MiFID, GDPR. \n3.1 Microsoft Media Player and Explorer     \nIn May 1998, the US Department of Justice (DoJ) and the Attorneys General of 20 \nStates, as well as the District of Columbia, sued Microsoft.1 Among others, the al-\nlegations were related to anti-competition practices through Microsoft’s bundling \ntogether of its internet browser, Internet Explorer (IE) with its Windows operating \nsystems, an action that was considered illegal under Sections 1 and 2 of the Sher-\nman Antitrust Act.2 The District Court of Columbia found Microsoft guilty of an-\nticompetitive practices and ordered a series of conduct restrictions, along with an \n                                                           \n1 US DOJ Complaint 98-12320, 1998. \n2 The Sherman Antitrust Act (1980) is included in Title 15 USC. Section 1 of \nthe Sherman Act mentions that ‘[e]very contract, combination in the form of trust \nor otherwise, or conspiracy, in restraint of trade or commerce among the several \nStates, or with foreign nations, is declared to be illegal’. Section 2 mentions that \n‘(e)very person who shall monopolize, or attempt to monopolize, or combine or \nconspire with any other person or persons, to monopolize any part of the trade or \ncommerce among the several States, or with foreign nations, shall be deemed \nguilty of a felony’. \n\n26  \nextremely invasive structural remedy of splitting Microsoft in two ‘Baby Bills’.3 \nThe plaintiffs attempted to send Microsoft’s appeal to the Supreme Court, but the \ncourt declined to render an opinion and the case was handled at a federal level.4 A \nsettlement was reached between Microsoft, the DoJ and nine states, which was \napproved by the United States Court of Appeals for the District of Columbia Cir-\ncuit. The District of Columbia and nine other states rejected this settlement seek-\ning further remedial measures, most of which were eventually rejected by the \ncourt.5  \nThe settlement mentions that: \n‘[o]riginal Equipment Manufacturers (OEMs) must be free to install and display icons, \nshortcuts and menu entries for competing software in any size or shape, and, along with \nend users, can designate the competing software as a default’.6  \nYet, the consent decree did not require Microsoft to alter its code by removing \nIE. The company was only required to allow OEMs the ability to replace IE with a \ncompeting browser. The exclusion of IE from the Add/Remove Programs utility is \nalso in violation of the Sherman Act, Section 2. Therefore the settlement provides \nthat: \n ‘(…) on the desktop or Start menu, OEMs and end users must be free to enable or \nremove access to both Microsoft's and competing software by altering icons, shortcuts, or \nmenu entries’.7  \nFurthermore, Microsoft was prohibited from automatically altering OEMs or \nend user configurations through its operating system software, without confirma-\ntion from the user at least two weeks from the initial use of the computer. In addi-\ntion, the OEMs ability to exercise those rights should not be restricted through li-\ncensing terms. Finally, one interesting aspect of the terms of decree is that in this \ncase, due to the nature of rapid change in the high-technology industry, the com-\npliance period was shortened by half (5 years instead of 10).8 \nA complaint by Sun Microsystems about the behavior of the Microsoft Corpo-\nration, with regard to its non-disclosure of protocols for Windows NT in 1998, ini-\ntiated the European Commission’s investigation into the company.9 Microsoft’s \nbehavior resulted in the inability of Sun Microsystems’s server products to \nachieve full interoperability with Windows desktop products. Hence, consumers \nneeded to purchase both desktop and server products from Microsoft. The investi-\n                                                           \n3 See Economides (2001), p. 28. \n4 United States v. Microsoft Corp., 98-1232 (CKK), 231 F. Supp. 2d 144 \n(D.D.C.2002). \n5 New York v. Microsoft Corp., 98-1233 (CKK), 209 F. Supp. 2d 132 (D.D.C. \n2002); Harty and Wrobel (2003), pp. 200-201. \n6 See Cohen (2004), p. 340. \n7 See Cohen (2004), p. 341. \n8 See Cohen (2004), p. 344. \n9 See Gitter (2004), p. 187. \n\n27 \ngation was expanded in 2001 during the tying of Windows Media Player to the \nWindow’s PC operating system.10 Due to the company’s domination of the market \nfor operating systems, this action was bound to lead in a distortion of the competi-\ntion, since package-deal prices would urge consumers to favor Microsoft’s prod-\nucts over those of competing companies. \nIn its decision of March 2004,11 the European Commission concluded that Mi-\ncrosoft had to disclose interface information for its server products in order for full \ninteroperability of competitors to be achieved and enforced the creation and sale \nof the Windows operating system’ version without Windows Media Player, named \n‘Windows XP N’.12 Microsoft appealed the decision in the Court of First In-\nstance.13 The court’s final judgement (issued on 17 September 2007) basically ap-\nproved the European Commission’s decision and deemed compulsory licensing as \nan appropriate remedy in cases of violation of Art. 82 European Communities \nTreaty (ECT), if certain conditions apply.14 This also applies in the case of the \ntechnology industry, where reliance on intellectual property is fundamental to \npropel research and development. \nArt. 81 and 82 ECT, which have been replaced by Art. 101 and 102, respective-\nly, of the Treaty on the Functioning of the European Union (TFEU),15 appear to be \nsimilar to Sections 1 and 2 of the Sherman Act, nevertheless different approaches \nhave been followed to determine anti-competitive behavior. American courts use \nrules of reason, while under European Law behavior satisfying the conditions of \nArt. 81 §1 is deemed anticompetitive, unless it qualifies for an exception, as de-\nscribed in Art. 81 §3.16 \nDuring the early ’90s judicial procedures against Microsoft in Europe and US \nmanaged to achieve a level of coordination.17 This has been a step forward in in-\nternational high technology antitrust cooperation. Nonetheless, Europe’s more \n                                                           \n10 See Cohen (2004, p. 352. \n11 See EC Decision of 24 March 2004, Case COMP/C-3/37.392. \n12 See Cohen (2004), p. 352. \n13 \nMicrosoft \nv. \nCommission \nof \nthe \nEuropean \nCommunities, \nECLI:EU:T:2007:289. \n14 See Gitter (2004), p. 192. \n15 Art. 101§1 of the TFEU reads as follows: ‘The following shall be prohibited \nas incompatible with the internal market: all agreements between undertakings, \ndecisions by associations of undertakings and concerted practices which may af-\nfect trade between Member States and which have as their object or effect the pre-\nvention, restriction or distortion of competition within the internal market, (…)’; \nArt. 102 TFEU: Any abuse by one or more undertakings of a dominant position \nwithin the internal market or in a substantial part of it shall be prohibited as in-\ncompatible with the internal market insofar as it may affect trade between Member \nStates. (…). \n16 See Cohen (2004), p. 353. \n17 See Klein and Bansal (1996), pp. 179-180. \n\n28  \nbroad social welfare approach to competition law is bound to result in further ac-\ntion than that taken by US courts. Yet, full international cooperation in the future \nis essential due to the trans-jurisdictional effect of the high technology market. In \na concluding note, it is suggested that developments in software technology will \nspare governments the need to ‘interfere early in the standardization process, in \nthe effort to ensure that the best new product wins’, hence maintaining a healthy \ncompetition framework.18 \n3.2 Volkswagen Emissions Case     \nThe Volkswagen (VW) emissions scandal, sometimes also referred to as \n‘Dieselgate’, is an example of an ‘inverse’ algorithmic regulation, i.e. a type of \nregulation where the VW Group used algorithms negatively and covertly, in order \nto affect the outcome of pollution-control calibrations and measurements. Because \nof its significance, level of fine and the range of implications for the car industry, \nthe VW emissions scandal has quickly risen to a case study for academic and \nbusiness purposes. It all started in 2015, when US Environmental Protection \nAgency (EPA) announced that it had found evidence that certain car models with \ndiesel engines under testing conditions used an algorithmic switch, hereafter \n‘switch’, to reduce gas emissions.19 These switches constitute Auxiliary Emission \nControl Devices (AECDs) that were not previously declared, in order for the vehi-\ncles to obtain a so-called Certificate of Conformity (COC). However, when the \ncars operated under normal conditions, the switches were turned to a ‘separate \n“road calibration” which reduced the effectiveness of the emission control system \n(…)’.20   \nAccording to US EPA, switches that are ‘neither described nor justified in the \napplicable COC applications (…) are illegal defeat devices’ and referred the mat-\nter to the US DoJ.21 Both US and EU use a nearly identical language for the defi-\nnition of a defeat device.22 A second notice of violation from the EPA to the VW \n                                                           \n18 See Cohen (2004), p. 364. \n19 See relevant first ‘Notice of Violation’ letter to the VW Group by US EPA \n(2015a). \n20 See EPA (2015a), supra note 19, p. 4. \n21 See EPA (2015a), supra note 19, p. 4. \n22 A defeat device under U.S. regulation (40 CFR §86.1803-01) is ‘an auxiliary \nemission control device (AECD) that reduces the effectiveness of the emission \ncontrol system under conditions which may reasonably be expected to be encoun-\ntered in normal vehicle operation and use (…)’. Similarly, under Regulation (EC) \n(No 715/2007 Art. 3, par. 10) ‘defeat device’ means any element of design which \nsenses temperature, vehicle speed, engine speed (RPM), transmission gear, mani-\nfold vacuum or any other parameter for the purpose of activating, modulating, de-\n\n29 \nGroup followed a few months later with allegations for a defeat device in addi-\ntional car models.23 In January 2016, the US DoJ filed a civil complaint against \nthe VW Group for alleged Clean Air Act violations.24 Moreover, the case has trig-\ngered massive civil class action and investor lawsuits in the US and elsewhere.25 \nThe US DoJ also launched a criminal investigation against the VW Group on the \nmatter. A year later, in January 2017, a preliminary framework settlement was \nreached in the US. VW pled guilty and agreed to pay $4.3 billion in criminal and \ncivil penalties, for US residents only. In addition, six VW executives and employ-\nees have been charged with: \n‘(…) participating in a conspiracy to defraud the United States and VW’s U.S. customers \nand to violate the Clean Air Act by lying and misleading the EPA and U.S. customers’.26  \nIn a separate settlement approved by a district Judge in San Francisco in Octo-\nber 2016, VW agreed to pay $14.7 billion to US consumers and government agen-\ncies.27 \nIn relation to the VW emissions case, it is of importance to present and analyze \nthe relevant reactions from the European institutions level. In late 2015, following \nrapid developments in the case in the US, the EP set up a ‘Committee of Inquiry \ninto Emission Measurements in the Automotive Sector’.28 On 2 March 2017 the \nCommittee presented its report with recommendations, which, among others, \ncalled for the forming of a new European agency to oversee road transport with \nmarket surveillance powers,29 similar to the US EPA. However, EU member states \nwere split over the idea of having yet another costly and probably over-\nbureaucratic EU agency and declined the proposal. Instead, the Members of the \nEP (MEPs) voted to strengthen the EC and to provide it with more powers to \noversee the automotive industry.  \n                                                                                                                                     \nlaying or deactivating the operation of any part of the emission control system, \nthat reduces the effectiveness of the emission control system under conditions \nwhich may reasonably be expected to be encountered in normal vehicle operation \nand use’. \n23 See second ‘Notice of Violation’ letter to the VW Group by US EPA \n(2015b).   \n24 See relevant press release by US DoJ (2016); in particular, sections 204 and \n205 of the Clean Air Act are mentioned therein. \n25 For a more extensive, but not exhaustive, list of lawsuits triggered by the VW \ndiesel affair see Juul (2016). \n26 See relevant press release by US DoJ (2017) as well as the court documents \ntherein.  \n27 See VW “Clean Diesel” MDL (multidistrict litigation) on the US District \nCourt (2018) portal of the Northern District of California.  \n28 See Decision 2015/3037(RSO) of the European Parliament (2015).  \n29 See the Committee report 2016/2215(INI) of the European Parliament \n(2017), incl. findings and conclusions. \n\n30  \nIn 2016, after talks with the EC, Volkswagen committed to repair all its cars af-\nfected by the Dieselgate scandal by Autumn 2017.30 An additional 2-year warranty \noffer was given to VW owners, with the company refusing any financial compen-\nsation, arguing that a software update in its defeat devices would be enough to re-\nsolve the issues, without any loss of the car’s performance.31 This difference be-\ntween VW’s behavior towards its customers, i.e. cash payout in the US v. \nwarranty extension in the EU, stems from the different legal framework in Europe, \nwhere the responsibility to approve new car models and monitor car manufactur-\ners lay, at the time, with member states.32   \nA paradigm shift is seen in a new EU regulation, which imposes stricter re-\nquirements for emission tests and heavy fines for violations by the industry.33 The \nnew regulation for testing new vehicle models to be introduced on the EU market \nwill apply as of 1 September 2020. Consequently, both national authorities and the \nEuropean Commission will have the power to conduct compliance verification \nchecks. The regulation, therefore, updates an existing directive that established a \nframework ‘for the approval of motor vehicles and their trailers, and of systems, \ncomponents and separate technical units intended for such vehicles’.34 \n3.3 Ad-blocking       \nPeople spend a considerable amount of their professional or free time online. \nKnowledge of browsing activity of data subjects is hence important to both mar-\nketing specialists and product/service providers. Companies use a broad set of \ntools to collect and process information about online behavior, such as computer \ncookies or just ‘cookies’.35 The advertising activities that are based on the analysis \nof personal online patterns are also known under the term ‘Online Behavioral Ad-\nvertising’ (OBA). Several users who sensed, in the collection of their online ac-\ntivity data, a breach of their privacy, began using special software tools that limit \nOBA. The use of such tools is not a walk in the park for users, as it has been al-\nready demonstrated that such software may have serious inherent usability flaws.36 \n                                                           \n30 See relevant press release by the European Commission (2017). \n31 See relevant Reuters report by De Carbonnel (2017). \n32 See De Carbonnel (2017), supra note 31. \n33 Regulation (EU) 2018/858 of 30 May 2018 on the approval and market sur-\nveillance of motor vehicles and their trailers, and of systems, components and sep-\narate technical units intended for such vehicles. \n34 This is the Framework Directive 2007/46/EC. \n35 Cookies are files stored on a user’s computer. When a website is visited, the-\nse computer programs store specific activity and client-related data, which allows \nthe server to deliver user-tailored content. \n36 See, e.g., Leon et al. (2012). \n\n31 \nIn addition, a lack of sufficient knowledge about tracking technology is further de-\nteriorating the efficiency of existing privacy tools.37  \nOBA is a major source of financing for private online media, such as the Spie-\ngel Online news portal,38 contrary to public broadcasters that are financed either \ndirectly through the state budget or through legally guaranteed license fees. In-\ncome is mainly generated through advertising on stationary and mobile internet \npages, within dedicated iOS/android apps, embedded videos and social net-\nworks.39 Additional income, although on a much smaller scale, may be generated \nthrough so-called ‘native advertising’.40 This is a new means of web-advertising \nthat is thought to offer added-value to the users of online content. Native ads are \ndesigned to look and feel similar to the surrounding media format and suggest re-\nlated and clearly indicated content to website visitors. ‘Affiliate links’ are also in-\ndicated as such in host websites.41 These are Uniform Resource Locators (URLs) \nthat contain the affiliate's ID or username. For instance, in the case of a book re-\nview, a space that contains details to a particular book, i.e. Publisher, page number \nand price, may be displayed as well as links to the online retailers from which the \nbook can be purchased. Should someone click on those ads and make a purchase, \nthe provider of the original website gets a commission. \nThe blocking of OBA through ad blockers and anti-tracking software has \nsparked several judicial disputes, since it can be used to blend out several types of \nthe aforementioned online ads. Consequently, the overall advertising revenues \nsink.42 One of the most significant cases has been recently decided in Germany.43 \nThe German company Eye/o GmbH, hereafter Eyeo, offers online users a free ver-\nsion of the ad blocker AdBlock Plus. This software filters ads that are listed in a \nso-called ‘Black List’. Eyeo offers companies a possibility to circumvent this \nblockade by entering their ads into a ‘White List’. In order for the companies to do \nso, their ads need to fulfill a certain set of criteria and they need to share online \nrevenues with Eyeo. The company says it excludes Small and medium-sized En-\n                                                           \n37 Leon et al. (2012), p. 598. \n38 Spiegel Online GmbH & Co. KG, a private German media company, oper-\nates the portal; its parent company is SPIEGEL-Verlag Rudolf Augstein GmbH & \nCo. KG; the media company is used here solely as an OBA study case as the ma-\njority of online media uses similar advertising schemes to generate online revenue. \n39 For additional information and structured analyses on the financing of online \nmedia see, e.g., Ulin (2013) and Cea-Esteruelas (2013). \n40 More information on the effects of positioning and language in native adver-\ntising, sometimes also mentioned as ‘sponsored content’, may be found in \nWojdynski and Evans (2015). \n41 See, e.g., Edelman and Brandi (2015). \n42 Dörting et al. (2017) offer an indication of the share of the online revenues \ncompared to the total revenues for the case of Spiegel Online, i.e. in 2016 the \nshare has exceeded 80%. \n43 Axel Springer AG v. Eyeo, I ZR 154/16, BGH. \n\n32  \nterprises (SMEs) from the revenue participation condition.44 It is fair to assume \nthat online media saw in this computer program a disruption to their business \nmodel and therefore proceeded with legal action against the software development \ncompany.   \nAfter going through lower court instances, the case has been decided at the \nGerman Bundesgerichtshof (BGH, Federal Court) in Karlsruhe, which declared \nthe internet operation of AdBlock Plus lawful. According to the original claim by \nthe media company, Axel Springer AG, the software company violates German \ncompetition law (Gesetz gegen den unlauteren Wettbewerb, UWG)45 and accused \nEyeo of unfair competition and aggressive business practices, arguing that it \nthreatens to destroy the online business model of advertising-financed media.46 \nHowever, the court decided that the mentioned computer program which automat-\nically suppresses internet ads on web pages does not violate competition law. In \nparticular, it decided that there is no legal basis to ban software that –like the pop-\nular AdBlock Plus– allowed users to read desired content online, while opting out \nof digital advertising. The BGH ruling mentions that any financial damage caused \nwas not inflicted by the software provider but by users who downloaded its soft-\nware. The court also pointed out that media companies have always the possibility \nto exclude users that utilize ad blockers from their native content. The ruling con-\ncludes that: \n‘[t]here is also no general market obstruction because there is insufficient evidence that \nthe business model of providing free content on the internet is being disrupted’.47 \nAnother related landmark decision regarding filtering of content, at the EU lev-\nel this time, was reached back in 2011.48 Here, the Court of Justice of the Europe-\nan Union (CJEU) found that an injunction against a Belgian Internet Service Pro-\nvider (ISP), Scarlet Extended SA, to install a general filter for all electronic \ncommunications passing through its services in order to prevent illegal downloads, \nwas against EU law. The case was ruled on a completely deferent legal basis com-\npared with Axel Springer AG v. Eyeo, which was ruled on the grounds of German \ncompetition law.49 According to the Court, the injunction resulted in a:  \n                                                           \n44 See relevant press announcement by BGH (2018) on the Axel Springer AG v. \nEyeo case, supra note 43. \n45 In particular, the relevant provisions for the BGH ruling are §4 Nr. 4 UWG \nand §4a UWG. \n46 See BGH (2018), supra note 43.  \n47 Translation from German from BGH (2018), supra note 43. \n48 Scarlet Extended SA v. SABAM, ECLI:EU:C:2011:771. \n49 The legal basis for the Scarlet Extended SA v. SABAM ruling included: Di-\nrective 2001/29/EC on copyright, Directive 2004/48/EC on intellectual property \nrights, Directive 2000/31/EC on electronic commerce, Directive 95/46/EC on the \nprocessing of personal data and Directive 2002/58/EC on privacy and electronic \ncommunications. \n\n33 \n‘(…) serious infringement of the freedom of the ISP concerned to conduct its business, \nsince it would require it to install a complicated, costly, permanent computer system \nentirely at its own expense (…)’.50 \n The Court also considered the cost dimension of the original injunction under \nthe light of the provisions of Directive 2004/48/EC, which requires that ‘(…) \nmeasures to ensure the respect of intellectual property rights should not be unnec-\nessarily complicated or costly’.51 \n3.4 Block Controversial Content    \nGoogle and other search engines have acquired immense power as public opinion \nshapers, a fact which eventually goes hand in hand with the responsibility for pre-\nsented and promoted content. Modern search engines embed sophisticated algo-\nrithms to improve and speed up user searches, such as the algorithms for the auto-\ncomplete and suggest mechanisms. Based on their implementation, such \nalgorithms foresee the next word(s) a user is likely to type according to local, re-\ngional or global search criteria. In this respect, it has been argued that issues of in-\ntellectual property or personal data cannot be left to be regulated by a random in-\nternet majority at a certain moment.52 \nOn 13 May 2014 the CJEU ruled on a dispute between Google53 and the Span-\nish data protection agency ‘Agencia Española de Protección de Datos’ (AEPD), as \nwell as Mr. Mario Costeja González, a Spanish national resident in Spain, on the \nright of the latter to ask Google to remove results for queries that include the per-\nson's name.54 In this landmark case the story unfolds as follows. In 2010, Mr. \nGonzález and the AEPD filed a complaint against a daily newspaper in Catalonia \n(Spain), which is owned by La Vanguardia Ediciones SL, hereafter ‘La \nVanguardia’, Google Spain and Google Inc.     \nThe reason for the complaint was based on the fact that the Google search en-\ngine, when prompted with Mr. González’s name, returned links on two specific \npages of La Vanguardia’s newspaper. These pages dated back to 1998 and men-\ntioned Mr. González’s name in relation to a real-estate auction along with his con-\nnection to attachment proceedings for the recovery of social security debts prior to \nthat time. Mr. González requested from La Vanguardia either to remove or alter \nthose pages in order for his personal data to no longer be visible or, alternatively, \n                                                           \n50 Scarlet Extended SA v. SABAM, supra note 49. \n51 Scarlet Extended SA v. SABAM, supra note 49. \n52 See Yannopoulos (2013).  \n53 Google is registered trademark of Google LLC; its parent company is Alpha-\nbet Inc. \n54 Google Spain SL and Google Inc. v. Agencia Española de Protección de \nDatos (AEPD) and Mario Costeja González, ECLI:EU:C:2014:317. \n\n34  \nto use appropriate tools offered by search engines to protect his personal data. Ad-\nditionally, he turned to the Google search engine requesting by Google Spain or \nGoogle Inc. to either remove or conceal personal data relating to him, so as to stop \nappearing in related search results such as the links to La Vanguardia newspaper. \nIn this respect, he argued that the mentioned attachment proceedings had been ful-\nly resolved years ago, making any reference to them completely irrelevant. \nThe case has been first ruled before the Audiencia Nacional, the Spanish Na-\ntional High Court. The Court stated that the legal action raised the question of \nwhich obligations search engine operators have, in order to protect personal data \nof persons who do not wish certain personal information on third parties’ web-\nsites: \n• to be linked back to them, \n• to be located,  \n• to be indexed and \n• to be made available to internet users indefinitely.  \nThe court recognized that answering the above question is linked to the rele-\nvant interpretation of Directive 95/46/EC.55 The latter needs to address the under-\nlying technologies which have been developed after its publication in 1995. In the \nlight of the above, the court decided to refer to the CJEU for a preliminary ruling. \nThe CJEU used Directive 95/46/EC as well as the Charter of Fundamental Rights \nof the European Union56 as the legal basis for its ruling, the most significant points \nof which are presented and analyzed in the following paragraphs.  \nThis is a general ruling that applies beyond Google to all search engines that \noperate a branch or subsidiary in an EU member state. One of the most important \noutcomes is the assessment that search engine activity57 constitutes the ‘processing \nof personal data’ when information that is being handled contains personal data \n(Art. 2(b) Directive 95/46/EC ). As a direct consequence, the operator of the \nsearch engine must be regarded as a ‘controller’ that processes personal data in the \nsense of Art. 2(d) Directive 95/46/EC. Hence, if the relevant provisions are satis-\nfied, the CJEU ruled that the operator of a search engine is obliged to remove re-\nsults that contain links to web pages that contain personal information following a \n                                                           \n55 Since 24 May 2018, this directive is no longer in force. It is repealed by Reg-\nulation (EU) 2016/679 (GDPR); see infra Section 3.7; Art. 17 GDPR outlines the \n‘different circumstances under which individuals can exercise the right to be for-\ngotten’. \n56 The Charter of Fundamental Rights of the European Union (2000/C 364/01) \nwas published in 18 December 2000 in the Official Journal of the European \nCommunities.  \n57 Google Spain SL and Google Inc. v. AEPD and González, supra note 54; the \nruling mentions that search engine activity includes the finding, automatically in-\ndexing, temporally storing and the making available to internet users of infor-\nmation on the internet by third parties.  \n\n35 \nsearch made ‘on the basis of a person’s name’. Interestingly, the operator must \nremove the results of the search engine algorithm:  \n‘(…) also in a case where that name or information is not erased beforehand or \nsimultaneously from those web pages, and even, as the case may be, when its publication \nin itself on those pages is lawful’.58 \nWhat arises as an issue of widespread importance from the aforementioned rul-\ning is the fact that the exercise by a data subject of ‘the right to be forgotten’59, \nmeaning that its relevant personal information will no longer appear in a publicly \navailable search engine list, overrides, as a general rule both the ‘economic inter-\nest of the operator of the search engine’ and the ‘interest of the general public’ in \nhaving access to that information’.60 However, a balance is to be held when exer-\ncising these rights.61 For instance, compared to the average citizen, the ‘right to be \nforgotten’ of a data subject with a public function or a significant role in public \nlife is to be assessed in a different manner, stand-alone or vis-à-vis the other men-\ntioned rights. Nevertheless, who is to decide in which cases the ‘right to be forgot-\nten’ is to be granted? Certainly, in the first instance, the search engines are the \nones which absorb the entire workload of assessing numerous complaints from the \ndata subjects. To decide which request qualifies, search engines must examine the \nrelated results for being either ‘inadequate, irrelevant, no longer relevant, or ex-\ncessive’.62 In the case of a disagreement between the data subject and the search \nengine, the local Data Protection Agency (DPA) is to be consulted before further \n                                                           \n58 Google Spain SL and Google Inc. v. AEPD and González, supra note 54. \n59 Sometimes also referred to as the ‘right to erasure’. \n60 Here is reference to the ‘right to operate freely’, meaning that the operator of \nthe search engine should be able to operate the company in a financially viable \nmanner, and to the ‘right to information’ of the general public in having access to \na list of results that a search engine algorithm returns.  \n61 The CJEU has been criticized for its failure to achieve a ‘fair balance’ be-\ntween conflicting rights; see, e.g., Frantziou (2014), p. 768; The European Court \nof Human Rights has also dealt with conflicts between the right to privacy and the \nright of freedom of expression; overall, a high level of coherence in the relevant \ncase law between the two major European courts is detected; see Margaritis \n(2018); \nThe \ncase, \nM.L. \nand \nW.W. \nv. \nGermany, \nECLI:CE:ECHR:2018:0628JUD006079810, further elucidates the state of play on \nprivacy issues in Europe; the judgment provides strong protection of media ar-\nchives, while being consistent with the CJEU reasoning for search engine opera-\ntors in the Google Spain SL and Google Inc. v. AEPD and González case.  \n62 For instance, Google (2019) shows how a major search engine operator im-\nplements ‘the right to be forgotten’ and the way it copes with individual removal \nrequests. \n\n36  \nlegal action is considered.63 Finally, the courts, national or supra-national, consti-\ntute the last resort where any disputes shall settled.   \nThe new Regulation (EU) 2016/679 (GDPR) incorporates the essence of the \naforementioned CJEU ruling in Art. 17, titled ‘Right to Erasure (‘right to be for-\ngotten’)’. The root of this right can be traced back to French law, which recogniz-\nes ‘le droit à l’oubli’, which allows rehabilitated criminals to object to the publica-\ntion of the facts of their conviction and incarceration.64 Under GDPR, in \nexercising a data subject’s ‘right to be forgotten’, personal data needs to be erased \nfrom the controller's environment, taking into account ‘available technology and \nthe cost of implementation’. Cost considerations within regulatory provisions are \nnow new. They have been also part of previous regulatory provisions and have \nbeen invoked in court rulings.65 \n Also, the controller would have to make sure to erase any publicly made links \nto information containing personal data. Additionally, data subjects can request \nerasure of their personal information when a) this is no longer required for the \npurpose it was collected, b) there is no legal basis for processing it and c) consent \nhas been withdrawn. However, as seen in the above CJEU ruling, the GDPR also \nmentions some significant exceptions to the ‘right to be forgotten’. For instance, in \ncases where processing is necessary: \n• to exercise the right of freedom of expression and information,  \n• to comply with an EU or a member state legal obligation,  \n• to perform a public interest task or exercise of official authority,  \n• for public health reasons or  \n• for archival, research or statistical purposes.  \nImplementation of the ‘right to be forgotten’ has serious implications for data \ncontrollers and processors, not only to search engines, but to the entity of organi-\nzations66 which at any point of their operation involve collection and processing of \npersonal data, by own means or through third parties. Recent guidelines on profil-\ning and individual decision-making point out that the right to erasure applies to \nboth the input and output data, i.e. the personal data to create a profile and the re-\n                                                           \n63 See Google (2019), supra note 62.  \n64 See Rosen (2011), p.88. \n65 See, e.g., Directive 2004/48/EC, which requires that measures to protect in-\ntellectual property rights are - among others - not to be costly, and the Scarlet Ex-\ntended SA vs. SABAM case, which is discussed in supra Section 3.3; further cost \nconsiderations when regulating the internet are presented by Goldsmith and Sykes \n(2001) under the light of the US Dormant Commerce Clause. \n66 The term ‘organization’ covers here any form a legal entity may take, e.g. \npersonal company, SME, private/public company, agency, NGO, multi-national \ncorporation etc.  \n\n37 \nsult of the profiling process, respectively.67 Consequently, organizations need to \nhave in place appropriate legal and technical procedures to be able at any time to \nerase, upon request, any of the personal data held. As previously seen, compliance \nto this right requires a case by case assessment from the side of an organization, in \norder to ensure that the handling of personal information is considered appropri-\nately.68 As this right in not absolute, data controllers also need to include in their \ndecisions, several, sometimes conflicting, factors such as technology, costs and \nproportionality. \n3.5 Sharing economy    \nThe use of software platforms in the sharing economy is directly linked with effi-\nciencies deriving from reducing transaction costs, the encouraging of accountabil-\nity and competition, improving allocation of resources, and information and pric-\ning advantages.69 In the following paragraphs, a number of significant cases \naround two of the main actors of the sharing economy, Airbnb and Uber, are dis-\ncussed. \n3.5.1 The Airbnb case \nAirbnb is a popular online rental platform, which has evolved into a global phe-\nnomenon during its ten years of operation (2008 to date). Today, the website lists \nnearly five million residences in over 191 countries and 81,000 cities worldwide, \nwhile more than 500 million visitors have used its services.70 As a result of its rap-\nid expansion and its inherent dynamic, the service has stirred up local markets, \nparticularly the real estate and hotel industries and a multitude of cities and coun-\ntries have been urged to impose restrictions on the platform’s operation, e.g. in \nterms of the maximum number of properties per owner or the maximum number \nof days per year that a property can be made available through the platform. Typi-\ncal examples are most European metropoles, such as Paris, Amsterdam, Berlin, \nBarcelona, as well as regions that attract a high number of foreign visitors during \nthe summer period. However, the need to regulate the market for short-term leases \n                                                           \n67 See WP29 (2018); WP29 is an advisory body (working party) on data protec-\ntion and privacy set up under Art. 29 of Directive 95/46/EC. \n68 For this an organization needs to be able to assess the type of information, its \nsensitivity for the data subject and the interest of the general public. \n69 See, e.g., Edelman and Geradin (2016); in this relation, see also Interian \n(2016), pp. 130 and 137. \n70 See Airbnb (2019a). \n\n38  \nis more pronounced in urban areas that are already facing housing problems or \nshortages of supply.71 \nThe activity of sharing platforms has caused several judicial disputes across the \nglobe. A typical example involves the City of Paris taking multiple legal action, \nprimarily against Airbnb, but also against other providers of similar services, such \nas Wimdu. With reference to housing shortages, the city administration in Paris \nhas long tried to enforce strict regulatory measures upon online platforms. In the \nabove court case and via express procedure, the municipality of Paris asked the \ncourt to fine the platforms for not removing ads lacking a proper registration num-\nber,72 which is mandatory to every rented property in Paris.73 Additionally, the city \nhas imposed a maximum of 120 days per year for rented spaces as accommodation \nfor tourists.74 Moreover, the Assemblée Nationale back in 2016 decided that medi-\nation portals must report on a yearly basis the number of transactions and the rev-\nenues of their users directly to the national treasury department. The measure is to \nenter into force in 2020 for income generated in 2019.75  \nIn a prominent case, the City of Paris asked the court to fine the platforms for \nnot removing ads lacking a proper registration number, which is mandatory to \nevery rented property.76 The result of the hearing before the Tribunal de Grande \nInstance (TGI) de Paris (High Court of Paris) was expected with high anticipation \nto take place on 12 June 2018. However, Airbnb posed a question on a ‘priority is-\nsue of constitutionality’77 claiming violation of the constitutional principle of \nequality before public office, which was considered positively by the President of \nthe court, who referred the subject to the Court of Cassation. However, the court \nrejected the claim, stating that it does not have to refer the matter to the Constitu-\n                                                           \n71 In Greece, Nomos (Law) 4472/2017 (Art. 84) fully liberalized the activity of \nsharing platforms, such as Airbnb. However, restrictions may be imposed locally \nby ministerial decisions, e.g. in the case of housing problems. \n72 See relevant Reuters report by Rivet (2018). \n73 See relevant French ‘Law for a Digital Republic’ (Loi n° 2016-1321 du 7 \nOctobre 2016 pour une République numérique) and in particular Art. L.324-1-1 of \nthe Tourism Code introduced therein. \n74 See relevant decision (2017 DLH 128) by the Mayor of Paris. \n75 See amended 2016 French budget law (Loi n° 2016-1918 du 29 Décembre \n2016 de finances rectificative pour 2016) together with the French ‘Anti-Fraud \nAct’ (Loi n° 2018-898 du 23 Octobre 2018 relative à la lutte contre la fraude); in \nthis regard it is interesting to note that Art. 24 of the amended French budget law \nfor 2016 targets opérateurs des plateformes en ligne (online-platform operators) in \ngeneral, according to Art. L. 111-7 du code de la consummation (consumer code), \nthus allowing for a broad interpretation across business sectors and regardless of \nthe seat of a legal entity.  \n76 See supra note 73.  \n77 English for Question Prioritaire de Constitutionnalité (QPC). \n\n39 \ntional Council.78 In another similar case, in front of the TGI de Paris, the City of \nParis accused Airbnb of displaying properties without registration number. In its \ndecision, the court dismissed the claim considering that the absence of a registra-\ntion number for limited period rentals does not constitute unlawful behavior.79  \nApart from its dispute with the digital platforms, the City of Paris has also en-\ngaged directly multi-owners who do not declare their properties. Facing millions \nin fines, owners appealed in cassation before the Court of Cassation, arguing that \nthe relevant sanctions by French law do not comply with European law.80 In the \nmeantime, several legal proceedings initiated by the City of Paris before Paris \ncourts are suspended awaiting the CJEU ruling,81 which is expected to be pro-\nnounced in late 2019 or at the beginning of 2020.   \nThis kind of lengthy multi-instance and multi-dimensional legal battles around \nthe operations of Airbnb and its peers are also taking place elsewhere in Europe. \nGradually, it becomes clear that metropolitan cities, rather than the state (or the \nEuropean Union), are the most active peers in imposing regulation on the activi-\nties of the digital platforms. The reasons for that are at hand, such as a deteriorat-\ning housing situation that can be monitored more efficiently at the local level, as \nlocal self-government is more likely to feel directly the effects of the gig econo-\nmy, be it positive or negative.  \nJust like in the occasion of the City of Paris, several other major European cit-\nies are considering imposing a similar regulatory framework against sharing plat-\nforms such as Airbnb. As a basic measure, authorities limit the days per year a lo-\ncation/accomodation can serve as transient accommodation. In Amsterdam, the \ncity council wants to tighten the rules for renting apartments to tourists via online \nplatforms. Starting in 2019, landlords will be allowed to rent their apartments only \nfor 30 days per year, half of the previous maximum of 60 days per year. Similar to \nParis, there is also an obligation for landlords to notify their municipality in ad-\nvance should they offer their home for renting through an online housing plat-\nform.82  \nIn comparison to their US counterparts, European cities have presented faster \nreflexes to remedying home sharing externalities.83 A similar measure has also \nbeen adopted in Japan. The ‘Minpaku Shinpou’, the new law for the private lodg-\n                                                           \n78 Airbnb France v. City of Paris, ECLI:FR:CCASS:2019:C300154. \n79 City of Paris v. Airbnb France and Airbnb Ireland, TGI Paris, n° 18-54.632.  \n80 City of Paris v. Ms. Claire G.F., ECLI:FR:CCASS:2018:C301005. \n81 The preliminary question to the CJEU ruling is related to the conformity of \nArticle L.631-7 of the French Construction Code with the provisions of Directive \n2006/123/EC on services in the internal market. \n82 See Airbnb (2019b) for an overview of the home sharing issues in the City of \nAmsterdam; Dredge et al. (2016) present a comprehensive analysis of regulatory \napproaches for four major European Cities, i.e. Barcelona, Berlin, Amsterdam and \nParis. \n83 See Interian (2016), p. 157. \n\n40  \ning business, took effect as of 15 June 2018 and is considered to be the first rele-\nvant national legal framework in Asia.84 Minpaku limits home-sharing to 180 days \na year and requires homeowners to notify the city government and to be subject to \nadditional control measures. As a consequence of minpaku, Airbnb informed \nhomeowners that their properties would not be listed on the platform, unless they \nare in compliance with the new law. The fact that platform software needs to be \nadapted to check whether a landlord or a property is appropriately registered con-\nstitutes a clear and high-profile case of algorithmic regulation. \nIn view of the worldwide regulatory activity around sharing platforms, an in-\nteresting response from the companies may be observed that could be part of a fu-\nture generalized trend. At least in a couple of occasions, the private companies \nthat operate the digital platforms have opted to positively respond to authorities’ \ndemands.85 On the motives of such rather novel corporate behavior may be debat-\ned. Presumably, being pro-active will not help them evade further legal or admin-\nistrative action but consumers, administrators or the judiciary could indeed regard \nit as a move of goodwill. This again could translate in buying time and saving \nlarge amounts in fines.  \nThe fact that Airbnb operates a digital services platform does not mean that any \nregulations targeting it constitute per se cases of ‘direct regulation’. Take for in-\nstance the decrease in the number of days a property can be made available to \nhome sharing. In this case, regulators mainly attempt to tackle an underlying issue, \ne.g. provide solutions the housing issue in metropolitan areas or protect a tradi-\ntional economy sector such as the hospitality industry (or both). This type of regu-\nlation is named ‘indirect regulation’. We speak of the alternative option, i.e. ‘di-\nrect regulation’, when modifying the actual digital platform. Such modifications \nmay induce changes in the periphery software, the user interface or the primary \nalgorithm.86 Hence, the case where platform software needs to be adapted to check \nwhether a landlord or a property is appropriately registered constitutes a case of \ndirect algorithmic regulation. \n                                                           \n84 See, e.g., Matsui (2019), pp. 130-132.  \n85 Airbnb has committed to adequately respond to a respective call from the Eu-\nropean Commission and EU consumer authorities; see, e.g., EC (2018); another \npro-active move by the technology community is to be found in joining EU’s \nCode of Practice on Online Disinformation; see, e.g., Marsden and Meyer (2019), \np. 4. \n86 It is not always possible to determine whether a change in functionality is at-\ntributed to a minor software update or a major version revision that includes \nchanges in the core algorithm. This would require a study the source code, which \nin the mentioned cases constitutes proprietary intellectual property and may be \nprotected by patents, source code copyright or else; see, e.g., Liberman (1995). \n\n41 \n3.5.2 The Uber case  \nAnother significant case study of the sharing economy is to be found in the name \nof Uber Technologies Inc., hereafter Uber. Uber is an online platform that offers, \namong others, peer-to-peer personal transport services and has quickly evolved in-\nto a technology firm that deals with self-driving technology and urban air \ntransport. Similar to the Airbnb case, there is a wide range of lawsuits worldwide \nconcerning Uber,87 some of them aiming to even clarify the nature of the service. \nIn France, in particular, the law requires that each driver needs to return to the \nbase in between rides, thus preventing the circulation of vehicles in areas with an-\nticipated customer demand. This raises the cost of each ride and promotes traffic \ncongestion, as more distance needs to be covered.88  \nIn a landmark case, a Barcelona Court ‘Juzgado de lo Mercantil No 3’ request-\ned by the CJEU a preliminary ruling regarding the nature of the Uber ridesharing \nservice.89 In particular, the Spanish Court sought guidance in the case Asociación \nProfesional élite Taxi v. Uber Systems Spain SL on whether the Uber service can \nbe regarded as: \n• a transport service,  \n• an electronic intermediary service or  \n• an information society service. \nThe CJEU judged that Uber ‘intermediation services’ that are delivered ‘by \nmeans of a smartphone application’ are inherently linked to a transport service and \nmust be classified as ‘a service in the field of transport’.90 Hence, Uber’s services \n                                                           \n87 See, e.g., Edelman and Geradin (2016). \n88 See Loi  (Law) n°  2014-1104  du  1er  Octobre  2014  relative  aux  taxis  et  \naux  voitures  de  transport  avec  chauffeur [Relating  to  taxis  and  chauffeur-\ndriven  transport],  JORF  n°0228  15938 (2 October  2014). \n89 Asociación Profesional Elite Taxi v. Uber Systems Spain, SL, \nECLI:EU:C:2017:981. \n90 Within the meaning of Art. 58(1) TFEU. The operative part of the judgment \nreads as follows: ‘Article 56 TFEU, read together with Article 58(1) TFEU, as \nwell as Article 2(2)(d) of Directive 2006/123/EC of the European Parliament and \nof the Council of 12 December 2006 on services in the internal market, and Arti-\ncle 1(2) of Directive 98/34/EC of the European Parliament and of the Council of \n22 June 1998 laying down a procedure for the provision of information in the field \nof technical standards and regulations and of rules on Information Society ser-\nvices, as amended by Directive 98/48/EC of the European Parliament and of \nCouncil of 20 July 1998, to which Article 2(a) of Directive 2000/31/EC of the Eu-\nropean Parliament and of the Council of 8 June 2000 on certain legal aspects of in-\nformation society services, in particular electronic commerce, in the Internal Mar-\nket (‘Directive on electronic commerce’) refers, must be interpreted as meaning \n\n42  \nfall outside the scope of the Directive 2006/123/EC, which implements Art. 56 \nTFEU on the free movement of services. Instead, Art. 58(1) TFEU needs to be ap-\nplied, which relates specifically to the freedom to provide services in the field of \ntransport. The Court recognized that there are no European common rules based \non non-public urban transport services, meaning that it is up to the member states \nto regulate the conditions under which intermediation services are to be offered, \nthus creating additional national regulatory overhead with which companies such \nas Uber need to comply.91  \nOne of the many lawsuits against Uber in the US, Philadelphia Taxi Associa-\ntion Inc. v. Uber Technologies, was ruled in the US Court of Appeals. In particu-\nlar, 80 Philadelphia taxi companies that constitute the Philadelphia Taxi Associa-\ntion asked the Court of Appeals to reverse the order of a lower court contending \nthat: \n‘(…) Uber violated the antitrust laws because its entry into the Philadelphia taxicab \nmarket was illegal, predatory, and led to a sharp drop in the value of taxicab medallions as \nwell as a loss of profits’. 92 \n The appellants argued that traditional taxis are losing money because of unfair \ncompetition. However, the Court stated that the purpose of antitrust laws is to \n‘protect competition, not competitors’. Ultimately, the court decision –like other \nsimilar decisions in the US– concluded that taxi companies:  \n‘(…) have no right to exclude competitors from the taxicab market, even if those new \nentrants failed to obtain medallions or certificates of public convenience.’93 \nIn a further US lawsuit filed in 2017 in front of the Central District Court in \nCalifornia, Uber is charged to have implemented an ‘active, extensive, methodical \nscheme (…) to defraud drivers’.94 Uber is pricing its services using an algorithmic \n                                                                                                                                     \nthat an intermediation service such as that at issue in the main proceedings, the \npurpose of which is to connect, by means of a smartphone application and for re-\nmuneration, non-professional drivers using their own vehicle with persons who \nwish to make urban journeys, must be regarded as being inherently linked to a \ntransport service and, accordingly, must be classified as ‘a service in the field of \ntransport’ within the meaning of Article 58(1) TFEU. Consequently, such a ser-\nvice must be excluded from the scope of Article 56 TFEU, Directive 2006/123 and \nDirective 2000/31’. \n91 In Greece, the mentioned decision has been transposed in the national legal \norder via Art. 13 of Law 4530/2018 ‘Regulation of transport issues and other pro-\nvisions’ (translated from Greek). \n92 Philadelphia Taxi Association, Inc. v. Uber Technologies, 17-1871, US Court \nof Appeals. \n93 Philadelphia Taxi Association, Inc. v. Uber Technologies, supra note 92. \n94 See class action complaint, Sorvano Van v. Rasier, LLC., Rasier-CA, LLC., \nand Uber Technologies, INC., hereafter ‘Sorvano v. Uber’, case 2:17-cv-02550-\nDMG-JEM, US District Court for the Central District of the State of California. \n\n43 \npricing system, which calculates the total fare before, instead of after, a ride. Ac-\ncording to the claim, Uber uses ‘(…) a base fare plus a per-mile and per-minute \ncharge for the estimated time and distance of the travel, respectively’.95 An Uber \ndriver, Mr. Sophono Van, claimed that the company manipulated the software in \nthe Uber app to calculate longer and slower routes than the typical one needed to \nreach a passenger’s destination. Hence, according to the plaintiff of this class \nsuit,96 the fare for the passenger would be higher, while the driver’s commission is \ncalculated based on the typical (cheaper) route, with Uber keeping the difference. \nShould these allegations prove correct, then this will constitute another case of al-\ngorithmic misconduct, similar to US v. VW, where a defeat device was used to \ntrick emission tests in diesel engines. In the following paragraphs, a series of gen-\neral issues regarding the operation of software platforms in the sharing economy \nare discussed. \nConcerns regarding algorithmic bias have been presented in Sections 2.1.1 and \n2.2. In the above case, decentralized decisions by each host, on whether or not to \naccept customers, may allow for instances of discrimination. The findings of a rel-\nevant US study demonstrate that participation, pricing and ratings on Airbnb are in \naccordance with existing racial inequalities.97 This happens in spite of the overall \nameliorative effect that the use of the platform can have on the economy of the af-\nfected groups.98 \nThe issue of platform operators’ liability for the activities their platforms facili-\ntate or coordinate, is still under debate. This is not a specific focus however, of \nthis book. Nevertheless, liability represents one of the critical issues that employ-\nment will be confronted with in the sharing economy era.99 In the US, because of \nthe determination to create a friendlier environment for innovation, decreased lia-\nbility for Internet Service Providers (ISPs) was ensured through legislation such as \n§230 of the Communications Decency Act (CDA). In 2018, a first relevant federal \nruling on the sharing economy stated that Uber limousine drivers are independent \ncontractors and not the company’s employees within the meaning of the Fair La-\nbor Standards Act.100 The EU is looking forward to reform the current rules gov-\nerning ISPs copyright liability beyond the Directive on electronic commerce \n(2000/31/EC) and the copyright Directive (2001/29/EC). After long discussions, \nthe new Directive 2019/790 is about to enter into force.101 The underlying discus-\n                                                           \n95 See points 63 and 64 in Sorvano v. Uber, supra note 94. \n96 This is a type of lawsuit where one of the parties is a group of people who are \nrepresented collectively by a member of that group. \n97 See Cansoy and Schor (2017), pp. 12-15. \n98 See Cansoy and Schor (2017), p. 17. \n99 See Cunningham-Parmeter (2016). \n100 Razak v. Uber Technologies Inc., 2:16-cv-00573, US District Court for the \nEastern District of Pennsylvania. \n101 See Directive (EU) 2019/790 on copyright and related rights in the Digital \nSingle Market. \n\n44  \nsion on the necessity for pan-European liability rules, as well as relevant CJEU \ncase law, is presented in a paper ordered by the European Parliament’s Committee \non the Internal Market and Consumer Protection.102 \nIn addition, in the sharing economy, the issue of ‘offer of universal service’ is \nat stake. For instance, Taxi fleets and conventional hotels are required to provide a \npercentage of vehicles and rooms, respectively, that can accommodate wheel-\nchairs. To a great extent, this is not possible for the owner of a single house or ve-\nhicle. The offer of incentive payments –funded through special taxes– to providers \nof universal service, is a proposal worth investigating.103 Overall, it is the inherent \nexternalities occurring by the platforms’ operation that require regulatory inter-\nvention. Safety is another field of intervention. The lack of contractual relation-\nship removes the motive for Uber drivers to obtain a commercial endorsement for \ntheir driver’s license and Airbnb residences might lack proper fire escaping plan-\nning. Such issues constitute a field that should be regulated on a bare minimum \nrequirements basis.104  \nConcluding, in the sharing economy, algorithms and innovation find plenty of \nroom for expansion. Policymakers should embrace efficiencies generated through \nthe operation of software platforms by removing unnecessary or protectionist \nrules, while, at the same time, imposing a regulatory framework to ensure the le-\ngality and safety of these platforms, adapt market operation and uphold the gen-\neral public interest.105 \n3.6 Algorithmic trading regulation     \nThe rise of algorithmic trading offers an interesting and developing field of algo-\nrithmic regulation. The European Union has long been at the forefront of financial \nmarket regulation. Applicable since 2007, the Markets in Financial Instruments \nDirective (2004/39/EC), or ‘MiFID’ has been the cornerstone of the EU's regula-\ntion of financial markets, but recent market anomalies106 have made an update of \nthe relevant framework necessary. The new legislation, MiFID II (2014/65/EU), \nentered into force in 2018 and is accompanied by a set of new regulations, as well \nas by detailed technical rules (standards) for implementation.107 Significant im-\n                                                           \n102 See Nordemann (2018). \n103 See Edelman and Geradin (2016), p. 321. \n104 See Edelman and Geradin (2016), pp. 309-316. \n105 See Edelman and Geradin (2016), p. 296. \n106 For instance, the ‘Flash Crash” in May 2010 and the ‘Tweet Flash Crash’ in \nApril 2013 are understood here as market anomalies. \n107 The word is here about the Markets in Financial Instruments Regulation \n(MiFIR), Market Abuse Regulation (MAR) and ESMA’s Regulatory Technical \nStandards. \n\n45 \nprovements include new reporting requirements and tests that will increase the \namount of information available reducing the use of dark pools108 and Over-The-\nCounter (OTC) trading.109 In addition, there are stricter rules governing high-\nfrequency-trading. In relation to algorithms, the new regulatory scheme introduces \nstrict oversight and monitoring of algorithmic trading, by imposing new and de-\ntailed requirements on both algorithmic traders and trading venues (Art. 17 MiFID \nII). \nAlgorithmic trading utilizes advanced algorithms to automatically determine \nseveral order parameters110 and calculate optimal trading strategies with limited or \nno human intervention.111 Although the industry draws positive feedback from the \nintroduction of high frequency and algorithmic trading, regulators are concerned \nthat such technologies may cause market distortion. Specific concerns involve: \n‘(…) high order cancellation rate, increased risk of overloading systems, increased \nvolatility, the ability of algorithmic traders to withdraw liquidity at any time and \ninsufficient supervision by competent authorities’.112  \nConsequently, a company113 that is dealing with algorithmic trading will be re-\nquired to:  \n‘(…) have in place effective systems and risk controls (…) to ensure that its trading \nsystems are resilient and have sufficient capacity, are subject to appropriate trading \nthresholds and limits and prevent the sending of erroneous orders or the systems (…)’.114 \nAt the same time, trading venues need to be able to identify which orders have \nbeen placed by algorithmic trading. Both companies and trading venues need to \ntake ‘robust measures’ in order to ensure that ‘algorithmic trading or high-\nfrequency algorithmic trading techniques do not create a disorderly market and \ncannot be used for abusive purposes’.115 For instance, trading venues need to pro-\nvide their members with algorithm testing facilities. This is an important organiza-\ntional requirement, which allows companies to test their algorithms within a con-\ntrolled non-live environment, similar to the one that is used for the online real-\ntime trading (ex-ante identification of negative algorithmic impact). The relevant \nMiFID II provisions also lead to the implementation of systems and procedures \n                                                           \n108 Dark pool (also black pool) is a private forum for trading securities, deriva-\ntives, and other financial instruments. \n109 The phrase refers to stocks that trade via a dealer network rather than on a \ncentralized exchange. \n110 Order parameters may include the time of submission, price, order manage-\nment, timing of withdrawal etc. \n111 A definition of ‘algorithmic trading’ is also presented in MiFID II, recital \n39. \n112 See Norton Rose Fulbright (2014). \n113 MiFID uses the term ‘investment firm’ instead. \n114 See Art. 17(1) MiFID II. \n115 See Recital 64, MiFID II. \n\n46  \nthat are able to identify any negative impact ex-post, e.g. through implementation \nof a ‘kill button’ in order to cancel any outstanding orders. \nApart from kill buttons or switches, in order to avoid market distortion, regula-\ntors insisted in companies having specific minimum pre-trade risk limits on order \nsubmission,116 such as: \n1. price collars,  \n2. maximum order value,  \n3. maximum order volume,  \n4. maximum long-short positions, \n5. Maximum long/short overall strategy positions,  \n6. repeated automated execution throttles,  \n7. outbound message rates, \n8. maximum message limits and, where appropriate, \n9. market maker protections.  \nFor the same reason, the European Securities and Markets authority (ESMA) \nalso proposes that companies should be able to automatically block or cancel non-\neligible trade orders above the company’s risk management threshold.117 \nHigh-Frequency Algorithmic Trading (HFAT)118 is a subset of algorithmic \ntrading with high message intraday rates regarding orders, quotes or cancela-\ntions.119 Companies that utilize HFAT are required to store in an approved form \n‘accurate and time sequenced records of all (…) placed orders’ for at least five \nyears and ‘(…) shall make them available to the competent Member State authori-\nty upon request’.120 The records should be detailed enough to be able to identify:  \n‘(…) the person in charge of each algorithm, a description of the nature of each decision \nor execution algorithm and the key compliance and risk controls’.121  \nThe most significant changes to the relevant US legislation resulted from the \n2010 Dodd–Frank Wall Street Reform and Consumer Protection Act.122 Due to \ninherent complexity and the rather new legislation on the topic, judicial decisions \nwith regard to algorithmic trading or HFAT are not frequent.123  \n                                                           \n116 See ESMA (2014a), pp. 223-224. \n117 See ESMA (2014a), p. 224. \n118 For a definition of HFAT see Recital 40, MiFID II. \n119 Such a volume of intra-day messages would be in the range of 75,000 mes-\nsages per trading day on average over the year, according to the ESMA (2014b), \np. 231. \n120 See Art. 17(2) MiFID II. \n121 See Norton Rose Fulbright (2014). \n122 The Dodd–Frank Wall Street Reform and Consumer Protection Act became \nPublic Law No: 111-203 in 21 July 2010. \n123 See US DoJ docket for commodities fraud. \n\n47 \nIn 2015, a US District Court, in the Northern District of Illinois,124 convicted \nMr. Michael Coscia, a high-frequency trader who was accused of commodities \nfraud and ‘spoofing’.125 Mr. Coscia was accused of placing in the year 2011 large \norders into futures markets that he never intended to execute, thus creating an illu-\nsion of high demand, with the intent to lure other traders to markets. Spoofing was \ndeclared illegal in the commodities futures market in the US through the 2010 \nCommodity Exchange Act. Mr. Coscia appealed the decision in front of the Su-\npreme Court,126 which eventually rejected the petitioner’s arguments that the 2010 \nlaw’s definition of spoofing is ‘unconstitutionally vague’ and confirmed a 3-year \nprison sentence. Another pending spoofing case involves Mr. Navinder Sarao, \nwho faced criminal and civil spoofing charges in the US in relation to the afore-\nmentioned 2010 ‘flash crash’,127 to which Mr. Sarao pled guilty. The outcome of \nthis case is highly anticipated in both legal and algorithmic trader circles. Howev-\ner, the verdict in the US v. Coscia case was a clear indication that such practices \nare not being tolerated. Combined with the relevant European legal framework of \nMiFID II/MiFIR these regulations may be useful instruments in the hands of \nFraud Squad officers and prosecutors, in order to investigate further cases of mar-\nket manipulation. On the other hand, they could form a powerful legal/judicial \nbarrier for traders engaging in algorithmic trading or HFAT, to consider adjusting \ntheir practices. \nAn alternative way to approach algorithmic trading is the concept of providing \nreal rights to conducts that belong in the sphere of a virtual world.128 A virtual \nworld can be an online interactive one, where players create avatars performing \nvarious tasks and activities. The interaction between players, their virtual property, \ncreates a virtual economy, which imitates that of the real world. The relations be-\ntween providers and players are usually regulated via End-User License Agree-\nment (EULA) and the standard terms and conditions of the game hosting site.129 \nBut what is valid for a gaming community can be also valid for a community of \ntraders who are engaging in real markets via trading venues. Hence, as long as a \nsubscription fee is required, the status of consumer can be evoked by traders as \ndictated by EU’s Directive  93/13  (Unfair  Terms  in  Consumer  Contracts)  and  \nDirective  2005/29  (Unfair  Commercial  Practices  Directive), in order for them \nto benefit, for example, from bringing the case in their area of jurisdiction.130 In \ninstances of illegal handling of virtual rights, contractual relationship can be ex-\namined to establish contractual liability between involved parties. The introduc-\n                                                           \n124 US v. Coscia, 14-cr-00551. \n125 See 7 USC§ 6c (a) (5) (C) for a definition of spoofing: ‘bidding or offering \nwith the intent to cancel the bid or offer before execution’. \n126 Coscia v. US, 17-1099. \n127 US v. Sarao, 1:15-cr-00075. \n128 See, e.g., Yannopoulos (2012b). \n129 See Yannopoulos (2012b), p. 2. \n130 See Yannopoulos (2012b), p. 3. \n\n48  \ntion of digital signatures, security protocols and procedures either as regulations or \ntechnical standards have been proposed in order to safeguard those rights and the \nbenefits and properties of their owners.131 \n3.7 General Data Protection Regulation \nRapid technological developments in telecommunications, globalization of ser-\nvices, the rise of internet and the widespread use of mobile electronic devices have \ntriggered, among others, the need to modify Directive 2002/58/EC concerning the \nprocessing of personal data and the protection of privacy in the electronic commu-\nnications sector (Directive on privacy and electronic communications). After years \nof considerations and consultation with relevant stakeholders, the EU adopted on \n27 April 2016 its General Data Protection Regulation (GDPR),132 which became \ndirectly applicable on 25 May 2018 in all EU MS, without the necessity for addi-\ntional national legislation. Moreover, the EC has established an expert group on \nRegulation (EU) 2016/679 to ‘clarify how Member States' existing and future leg-\nislation will ensure effective and uniform application of the Regulation’.133 \nAmong other changes, GDPR alters the rules for ‘profiling’, i.e. the use of com-\nputerized data analysis to generate user profiles. As will be discussed below, pro-\nfiling is only allowed on an opt-in basis and special consent is required to auto-\nmatically generate the profile of a data subject. \nThe regulation contains well-known principles of pre-existing data protection \nlaw, such as ‘consent’, the ‘accountability principle’ and ‘privacy by design’.134 \nUnder the GDPR, both business customers and data controllers have certain legal \nobligations135. Despite the fact that the basic data protection principles remain un-\ntouched, the structures and procedures for the protection of the data subjects \nchange. For instance, the consent of the data subject in Art. 4(11) is far more de-\ntailed: \n‘‘consent’ of the data subject means any freely given, specific, informed and \nunambiguous indication of the data subject's wishes by which he or she, by a statement or \nby a clear affirmative action, signifies agreement to the processing of personal data \nrelating to him or her;’ \n                                                           \n131 See Yannopoulos (2012b), p. 7. \n132 Regulation (EU) 2016/679. \n133 The European Commission has established a relevant register that includes \ninformation on expert groups; for more details on the work of the expert group for \nGDPR see EC register (2016).  \n134 See also Cavoukian (2010). \n135 It is not the purpose of this book to provide a full analysis of the GDPR. \nNevertheless, some of its key concepts are to be highlighted.  \n\n49 \nCompared to previous legislation new key elements insist that the indication of \nconsent must be ‘unambiguous and involve clear affirmative action’ (opt-in).136 \nArt. 7 provides several additional conditions for consent, specifically on keeping \nrecords of consent, clarity of consent and the right to withdraw consent. In addi-\ntion, Art. 8 of the GDPR contains new provisions regarding children’s consent for \nonline services. As noted by UK’s Information Commissioner’s Office (ICO), the \nopt-in approach pursued through the GDPR is going to positively affect individu-\nals’ rights in a sense that existing rights are going to be further strengthened when \ndata processing happens on a consent-basis.137 The ICO also mentions two related \nexamples of such rights, the ‘right to be forgotten’138 and the ‘right to data porta-\nbility’.139 At this point, it is interesting to investigate what happens to existing data \nthat have been collected on an opt-out basis. Marketing140 under the GDPR is reg-\nulated like any other data processing activity and Art. 6 implies that the controller \nor a third party needs to demonstrate that it has a lawful basis to conduct such ac-\ntivities. Moreover, the data subject needs to be aware of who is collecting the data, \nfor which purpose and by whom they are to be further processed. Otherwise, data \ncollection and any processing actions are not lawful. In relation to direct electronic \nmarketing,141 a widely used business practice, it is noted that: \n ‘[i]f marketing is already lawfully conducted on an opt-out basis, the GDPR is unlikely to \nchange this (…).  If conducted on an opt-in basis, then further review and risk assessment \nmay be needed.’142 \nThis assessment comes to the result that businesses which previously went be-\nyond legal necessity in order to protect their client’s rights using an opt-in ap-\nproach, will find themselves in the situation to refresh those consents for GDPR \ncompliance, hence risking to compromise valuable marketing data in case several \ndata subjects will not re-consent.143 Art. 25 of the GDPR mentions two powerful \ndimensions in data protection using the terms ‘by design’ and ‘by default’.144 A \n                                                           \n136 See UK ICO (2017), p. 5. \n137 See UK ICO (2017). p. 7. \n138 On the ‘right to be forgotten’ see also supra Section 3.4. \n139 The right to data portability is mentioned in Art. 20 GDPR. \n140 Any form of marketing is concerned, e.g. postal, phone, e-mail, SMS etc. \n141 In the EU electronic marketing activities are also regulated by Directive \n2002/58/EC on the processing of personal data and the protection of privacy in the \nelectronic communications sector.  \n142 See Lee (2017). \n143 See Lee (2017). \n144 Art. 25(1) GDPR mentions data protection ‘by design’. Data protection ‘by \ndefault’ is mentioned in Article 25(2) GDPR. Article 25(3) GDPR enters on a vol-\nuntary basis a certification mechanism in place. \n\n50  \nclose look at Art. 25 is necessary to further study the effects and implications of \nthe provisions therein. 145 \nData protection ‘by design’ in not an original term coined by the GDPR draft-\ners. It means that ‘(…) privacy is embedded into the design and architecture of IT \nsystems and business practices’.146 Moreover, already in 2014, the European Un-\nion Agency for Network and Information Security (ENISA) produced a relevant \nreport to bridge the gap between the pre-GDPR legal framework and available \ntechnologies.147 Its goal is to motivate all stakeholders towards more privacy-\nfriendly systems and services design and development. When it comes to imple-\nmentation, several potentially limiting conditions such as ‘state of the art’ and \n‘cost of implementation’ need to be taken into consideration. These exact two \nconditions are also to be found in Art. 17(1) of the previous Data Protection Di-\nrective 95/46/EC.  \nData protection ‘by default’ means that privacy settings are originally imple-\nmented and built into the system. Hence, there is no necessity for a data subject to \nproceed to any actions to protect its privacy. The existence of predefined parame-\nters, usually presented as initial pre-settings in the Graphical User Interface (GUI), \ndoes not mean that changes are not possible. However, most users do not change \npre-settings, acting in good faith or these are skipped due to time considerations. \nData protection ‘by Default’ implies the use of the ‘Principle of Least Surprise’ \n                                                           \n145 Art. 25 GDPR Data protection by design and by default: \n1. Taking into account the state of the art, the cost of implementation and the \nnature, scope, context and purposes of processing as well as the risks of varying \nlikelihood and severity for rights and freedoms of natural persons posed by the \nprocessing, the controller shall, both at the time of the determination of the means \nfor processing and at the time of the processing itself, implement appropriate \ntechnical and organisational measures, such as pseudonymisation, which are de-\nsigned to implement data-protection principles, such as data minimization, in an \neffective manner and to integrate the necessary safeguards into the processing in \norder to meet the requirements of this Regulation and protect the rights of data \nsubjects. \n2. The controller shall implement appropriate technical and organizational \nmeasures for ensuring that, by default, only personal data which are necessary for \neach specific purpose of the processing are processed. That obligation applies to \nthe amount of personal data collected, the extent of their processing, the period of \ntheir storage and their accessibility. In particular, such measures shall ensure that \nby default personal data are not made accessible without the individual’s interven-\ntion to an indefinite number of natural persons. \n3. An approved certification mechanism pursuant to Art. 42 may be used as an \nelement to demonstrate compliance with the requirements set out in paragraphs 1 \nand 2 of this article. \n146 See Cavoukian (2010), p. 3. \n147 See Danezis et al. (2015). \n\n51 \n(or Astonishment) in software development, which states that the result of an op-\neration should be ‘obvious, consistent, and predictable’.148 Data protection ‘by De-\nfault’ contains less potentially limiting conditions compared to data protection ‘by \ndesign’ making it a potentially more powerful principle. Instead, it is related to the \n‘purpose limitation’ principle from Art. 5(1) GDPR.149 According to the GDPR, \ncompanies or organizations that deal with EU personal data need to decide on the \nappointment of a Data Protection Officer (DPO).150 This is rather linked to the \nvolume of sensitive personal information such an entity may process. For instance, \na company that routinely handles large volumes of personal data should be more \ninclined towards appointing a DPO. In general, a core requirement that arises from \nArt. 25 is that companies or organizations must implement mechanisms for data \nprotection, such as security or privacy checks, in all layers of operation, such as in \ntheir management, workings procedures and the provision of products or services.  \nHowever, Art. 25 provides DPOs with relatively little technical guidance on \nhow to accomplish such a widespread and complex task. There are two important \nterms that are mentioned here, ‘pseudonymisation’ and ‘data minimisation’. In ad-\ndition, the GDPR mentions the term ‘encryption’ elsewhere.151 The technical term \n‘pseudonymisation’ has here its first appearance in EU law. This new term refers \nto the processing of personal data in a way that the result cannot be attributed to a \nspecific data subject without the use of further related information.152 Those are to \nbe stored separately, using appropriate technical and security measures. Another \nsignificant requirement of Art. 25 is the so-called ‘data minimisation’, which re-\nsults in the processing of only a necessary amount of data for a specific purpose, \nalthough, in practical terms, this could be a difficult requirement to achieve.153 En-\n                                                           \n148 See Gunderloy (2005), p. 128. \n149 See relevant presentation by Hansen (2017), data protection commissioner in \nSchleswig-Holstein, Germany. \n150 See Art. 39 GDPR for the responsibilities and tasks of a DPO. A DPO \nshould not be a data controller as well and vice versa. \n151 See, for instance, Art. 6, 32 and 34 GDPR. \n152 The anonymization of data prior to the release of legal documents has been \nused previously as a balanced solution between the rights of access to information \nand protection of personal data. Approaches greatly differ from country to coun-\ntry; a relevant analysis for the domain of legal databases is offered by \nYannopoulos (2012a); a recent example of pseudonymization is offered by Deci-\nsions 1/2017 and 4/2017 of the Hellenic Data Authority in the case of introduction \nof the new electronic ticket for public transportation in Athens. ‘Hashing’ has \nbeen applied for the issuance of each new personalized card; hence, no document-\ned journey can be traced back to the cardholder but to a hash value. \n153 From a technical point of view, overcollection or over processing of data \ncannot be easily proven as these are relative terms (e.g. overcollection in relation \nto what?). There is no such thing as an optimal algorithm since algorithms are \nconstantly developed and optimized.   \n\n52  \ncryption is to be considered as a way to achieve data security, and hence privacy, \nby complicating and, ultimately, blocking data access to unauthorized third parties \nand data processors. Also, data subjects are to be allowed to access and alter their \nprivacy settings and information with the same ease as giving their consent to use \nand process their data, in analogy to Art. 7(3).154 A further critical examination of \nArt. 25 GDPR reveals several shortcomings that include:155 \n• ‘fuzzy legalese’, \n• insufficient clarity of parameters and methodologies to achieve its goals,  \n• lack of salient and strong incentives and  \n• missing communication link to the designers and developers of information \nsystems. \nThese shortcomings demonstrate how difficult it is to impose regulation on a \nhighly dynamic technology sector.   \nSummarizing, the provisions of Art. 25 have serious implications to a series of \nparameters of data processing, while access to the original and processed data by \nthird parties is generally not permitted:156 \n• amount of data collected, \n• extent to which the data is processed, \n• storage duration, \n• data accessibility.  \nDPOs need to bear in mind the above, in order to comply with the special \nGDPR requirements from Art. 25. Other key changes that the GDPR introduces \ninclude  stricter rules to report security breaches (Art. 33(1))157, as well as the ac-\ncountability principle, according to which there is a controller responsibility to \n‘demonstrate compliance’ with personal data processing rules (Art. 5(2)). From \nthe above, it becomes obvious that the GDPR constitutes a decisive evolution to \nthe pre-existing EU legal framework that provides public authorities with regula-\ntory guidelines, as well as a certain level of guidance to private sector and individ-\nual stakeholders. Nevertheless, the implementation of such a comprehensive pri-\nvacy framework needs to be monitored closely, both at the national, as well as the \nEU level, as it is likely to cause negative implications in indirectly related fields, \n                                                           \n154 See Art. 7(3): ‘The data subject shall have the right to withdraw his or her \nconsent at any time. (…) It shall be as easy to withdraw as to give consent’. \n155 See Bygrave (2017), p. 119. \n156 As previously commented, access to the data may be granted with the con-\nsent of the data subject(s). \n157 A legal obligation is introduced to notify the supervisory authority of a per-\nsonal data breach within 72 hours of knowing about it. \n\n53 \nsuch as in advertising.158 Certainly, the first judicial judgements on implementa-\ntion of the GDPR are awaited with great interest, as they are expected to provide \nclarity to some of the ambiguous concepts represented therein. Moreover, and in-\nline with other scholars, we expect a gradual establishment of relevant national \nand international regulatory bodies or agencies.159 \nReferences     \nAirbnb. 2019a. Fast facts. https://press.atairbnb.com/fast-facts/. Accessed 4 April 2019. \nAirbnb. 2019b. Next steps in Amsterdam. https://www.airbnbcitizen.com/next-steps-in-\namsterdam/. Accessed 22 April 2019. \nBGH. 2018. Bundesgerichtshof: Angebot des Werbeblockers AdBlock Plus nicht unlauter. Press \nannouncement \n(19 \nApril \n2018). \nhttp://juris.bundesgerichtshof.de/cgi-\nbin/rechtsprechung/document.py?Gericht=bgh&Art=pm&Datum=2018&Sort=3&nr=82856\n&pos=0&anz=78. Accessed 30 May 2018. \nBygrave, Lee A. 2017. Data Protection by Design and by Default: Deciphering the EU's Legisla-\ntive Requirements. Oslo Law Review 4(2): 105-120.  https://doi.org/10.18261/issn.2387-\n3299-2017-02-03. \nCansoy, Mehmet, and Juliet Schor. 2017. Who Gets to Share in the “Sharing Economy”? Racial \nDiscrimination in Participation, Pricing and Ratings on Airbnb. Working paper (un-\npublished). Boston University. \nCavoukian, Ann. 2010. Privacy by Design - The 7 Foundational Principles - Implementation and \nMapping of Fair Information Practices, Information and Privacy Commissioner of Ontario, 1-\n10. Canada. http://www.ontla.on.ca/library/repository/mon/24005/301946.pdf. Accessed 29 \nMay 2018. \nCea-Esteruelas, Nereida. 2013. Cybermedia economics: revenue model and sources of financing. \nEl profesional de la información 22(4): 353-361. https://doi.org/10.3145/epi.2013.jul.12  \nCohen, Amanda. 2004. Surveying the Microsoft Antitrust Universe. Berkeley Technology Law \nJournal 19(1):333-364. https://doi.org/10.15779/Z38MH4K. \nCunningham-Parmeter, Keith. 2016. From Amazon to Uber: Defining employment in the mod-\nern economy. Boston University Law Review 96: 1673-1728. \nDanezis, George, Stefan Schiffner, Marit Hansen, Rodica Tirtea, Josep Domingo-Ferrer, Daniel \nLe Métayer, and Jaap-Henk Hoepman. 2015. Privacy and Data Protection by Design – from \npolicy to engineering. Heraklion: ENISA. https://doi.org/10.2824/38623. \nDe Carbonnel, Alissa. 2017. Volkswagen to offer EU diesel car owners extended warranty but no \nmoney back: EC. Reuters, June 14. https://www.reuters.com/article/us-volkswagen-\nemissions-idUSKBN19527H. Accessed 19 June 2018. \nDörting, Thorsten, Matthias Streitz, and Jörn Sucher. 2017. So finanziert sich SPIEGEL \nONLINE. Spiegel Online, August 16. http://www.spiegel.de/extra/werbung-plus-daily-so-\nfinanziert-sich-spiegel-online-a-1162309.html. Accessed 30 May 2018. \nDredge, Diane, Szilvia Gyimóthy, Andreas Birkbak, Torben Elgaard Jensen, and Anders Koed \nMadsen. 2016. The impact of regulatory approaches targeting collaborative economy in the \ntourism accommodation sector: Barcelona, Berlin, Amsterdam and Paris. Impulse Paper No 9 \n                                                           \n158 See, e.g., Goldfarb and Tucker (2011), who investigated the impact of priva-\ncy regulation on online advertising in the EU; empirical evidence is provided that \nprivacy regulation can reduce the effectiveness of advertising. \n159 The development of regulatory bodies is presented in Chapter 4. \n\n54  \nprepared for the European Commission DG GROWTH. Copenhagen: Aalborg University. \nhttps://ec.europa.eu/docsroom/documents/19121/attachments/1/translations/en/renditions/nati\nve. Accessed 22 April 2019. \nEC register. 2016. Commission expert group on the Regulation (EU) 2016/679 and Directive \n(EU) \n2016/680(E03461).http://ec.europa.eu/transparency/regexpert/index.cfm?do=groupDetail.gro\nupDetail&groupID=3461. Accessed 29 May 2019.  \nEC. 2018. Press release (20 September 2018). EU consumer rules: Airbnb commits to complying \nwith \nEuropean \nCommission \nand \nEU \nconsumer \nauthorities' \ndemands. \nhttp://europa.eu/rapid/press-release_IP-18-5809_en.htm. Accessed 22 April 2019. \nEconomides, Nicholas. 2001. The Microsoft antitrust case. Journal of Industry, Competition and \nTrade 1 (1): 71-79. https://doi.org/10.1023/A:1011576827599. \nEconomides, Nicholas. The Microsoft Antitrust Case. NYU Law and Economics Working Paper \nSeries 01-003. http://dx.doi.org/10.2139/ssrn.253083. \nEdelman, Benjamin G and Damien Geradin. 2016. Efficiencies and regulatory shortcuts: How \nshould we regulate companies like Airbnb and Uber. Stanford Technology Law Review 19 \n(2): 293-328. \nEdelman, Benjamin, and Wesley Brandi. 2015. Risk, Information, and Incentives in Online Af-\nfiliate \nMarketing. \nJournal \nof \nMarketing \nResearch \n52(1): \n1–12. \nhttps://doi.org/10.1509/jmr.13.0472. \nESMA. \n2014a. \nDiscussion \npaper \nMiFID \nII/MiFIR. \n2014/548. \nhttps://www.esma.europa.eu/sites/default/files/library/2015/11/2014-\n548_discussion_paper_mifid-mifir.pdf. Accessed 28 May 2019. \nESMA. \n2014b. \nConsultation \npaper \nMiFID \nII/MiFIR. \n2014/549. \nhttps://www.esma.europa.eu/sites/default/files/library/2015/11/2014-549_-\n_consultation_paper_mifid_ii_-_mifir.pdf. Accessed 28 May 2019. \nEuropean Commission (2017). Consumer Authorities and the European Commission urge \nVolkswagen to finalise repairs of all cars affected by emissions scandal. Press release (7 Sep-\ntember 2017). http://europa.eu/rapid/press-release_IP-17-3102_en.htm. Accessed 19 June \n2018. \nEuropean Parliament. 2015. Setting up a Committee of Inquiry into emission measurements in \nthe automotive sector, its powers, numerical strength and term of office (2015/3037(RSO)) \n(17 \nDecember \n2015). \nhttp://www.europarl.europa.eu/sides/getDoc.do?type=TA&reference=P8-TA-2015-\n0462&language=en. Accessed 18 June 2018. \nEuropean Parliament. 2017. Report on the inquiry into emission measurements in the automotive \nsector \n(2016/2215(INI)). \nhttp://www.europarl.europa.eu/doceo/document/A-8-2017-\n0049_EN.html. Accessed 18 June 2018. \nFrantziou, Eleni. 2014. Further Developments in the Right to be Forgotten: The European Court \nof Justice’s Judgment in Case C-131/12, Google Spain, SL, Google Inc v Agencia Espanola \nde \nProteccion \nde \nDatos. \nHuman \nRights \nLaw \nReview \n14 \n(4): \n761-777. \nhttps://doi.org/10.1093/hrlr/ngu033. \nGitter, Donna M. 2004. Strong Medicine for Competition Ills: the Judgement of the European \nCourt of Justice in the IMS Health action and its implications for Microsoft Corporation. \nDuke Journal of Comparative & International Law 15(1): 153-192.  \nGoldfarb, Avi. and Catherine E. Tucker. 2011. Privacy regulation and online advertising, Man-\nagement science 57.1. https://doi.org/10.1287/mnsc.1100.1246. Accessed 29 May 2019. \nGoldsmith, Jack L., and Alan O. Sykes. 2001. The Internet and the dormant commerce clause. \nThe Yale Law Journal 110(5): 785-828. \nGoogle. 2019. Privacy & Terms. https://policies.google.com/faq?hl=en. Accessed 4 April 2019. \nGunderloy, Mike. 2005. Developer to designer: GUI design for the busy developer. Alameda: \nSYBEX. \n\n55 \nHansen, Marit. 2017. Data Protection by Default – Requirements, Solutions, Questions. Presen-\ntation \nat \nthe \nIPEN \nWorkshop. \nJune \n9. \nVienna. \nhttps://edps.europa.eu/sites/edp/files/publication/17-06-09_marit_hansen-\ndataprotectionbydefault_ipen-workshop_vienna_hansen_en.pdf. Accessed 29 May 2019. \nHarty, Ronan P., and Gregory G. Wrobel, eds. 2003. 2002 Annual Review of Antitrust Law De-\nvelopments. Chicago: American Bar Association.  \nInterian, Johanna. 2016. Up in the air: Harmonizing the sharing economy through Airbnb regula-\ntions, Boston College International and Comparative Law Review 39 (1): 129-161. \nJuul, Maria. 2016. Lawsuits triggered by the Volkswagen emissions case. European Parliamen-\ntary \nResearch \nService. \nPE \n583.793. \nBrussels: \nEuropean \nParliament. \nhttp://www.europarl.europa.eu/RegData/etudes/BRIE/2016/583793/EPRS_BRI(2016)583793\n_EN.pdf. Accessed 16 June 2018.  \nKlein, Joel, and Preeta Bansal. 1996. International Antitrust Enforcement in the Computer Indus-\ntry. Villanova Law Review 41(1): 173-192. \nLee, \nPhil. \n2017. \nRe-consenting \nto \nmarketing \nunder \nGDPR?. \nhttps://privacylawblog.fieldfisher.com/2017/re-consenting-to-marketing-under-gdpr. \nAc-\ncessed 29 May 2019. \nLeon, Pedro, Blase Ur, Richard Shay, Yang Wang, Rebecca Balebako, and Lorrie Cranor. 2012. \nWhy Johnny can't opt out: a usability evaluation of tools to limit online behavioral advertis-\ning. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems \n(CHI '12), 589-598. New York: ACM. https://doi.org/10.1145/2207676.2207759 \nLiberman, Michael. 1995. Overreaching Provisions in Software License Agreements. Richmond \nJournal of Law & Technology 1(1): 4. \nMargaritis, Konstantinos. 2018. The Role of Judicial Dialogue between the CJEU and the \nECtHR in the Formulation of the Right of Privacy. In The Right to be Forgotten in Europe \nand Beyond/Le droit à l’oubli en Europe et au-delà, eds. Olivia Tambou, and Sam Bourton, \n98-99. Luxembourg: Blogdroiteuropéen. \nMarsden, Chris, and Trisha Meyer. 2019. Regulating disinformation with artificial Intelligence. \nBrussels: European Parliament. https://doi.org/10.2861/003689. \nMatsui, Shigenori. 2019. Is Law Killing the Development of New Technologies: Uber and \nAirbnb in Japan. Boston University Journal of Science and Technology Law 25: 100-144. \nNordemann, Jan Bernd. 2018. Liability of Online Service Providers for Copyrighted Content – \nRegulatory \nAction \nNeeded? \nEuropean \nParliament, \nIP/A/IMCO/2017-08. \nhttp://www.europarl.europa.eu/RegData/etudes/IDAN/2017/614207/IPOL_IDA(2017)61420\n7_EN.pdf.  Accessed 28 May 2019. \nNorton Rose Fulbright. 2014. High frequency and algorithmic trading obligations. MiFID II / \nMiFIR series. http://www.nortonrosefulbright.com/knowledge/publications/115236/mifid-ii-\nmifir-series. Accessed 28 May 2019 \nRivet, Myriam. 2018. Paris asks court to fine Airbnb over unregistered listings. Reuters, April \n12. \nhttps://www.reuters.com/article/france-airbnb/paris-asks-court-to-fine-airbnb-over-\nunregistered-listings-idUSL8N1RP1LQ. Accessed 19 June 2018. \nRosen, Jeffrey. 2011. The right to be forgotten, Stanford Law Review Online 64: 88-92. \nUK ICO. 2017. Consultation: GDPR consent guidance. https://ico.org.uk/media/about-the-\nico/consultations/2013551/draft-gdpr-consent-guidance-for-consultation-201703.pdf. \nAc-\ncessed 29 May 2019. \nUlin, Jeff. 2013. The Business of Media Distribution: Monetizing Film, TV and Video Content in \nan Online World. New York: Routledge. \nUS \nDistrict \nCourt \n(2018). \nVolkswagen \n\"Clean \nDiesel\" \nMDL. \nhttps://www.cand.uscourts.gov/crb/vwmdl. Accessed 18 June 2018. \nUS DoJ docket for commodities fraud. https://www.justice.gov/criminal-fraud/commodities-\nfraud. Accessed 29 May 2019. \n\n56  \nUS DoJ. 2016. Press release, January 4. https://www.justice.gov/opa/pr/united-states-files-\ncomplaint-against-volkswagen-audi-and-porsche-alleged-clean-air-act. Accessed 18 June \n2018. \nUS DoJ. 2017. Press release, January 11. https://www.justice.gov/opa/pr/volkswagen-ag-agrees-\nplead-guilty-and-pay-43-billion-criminal-and-civil-penalties-six. Accessed 18 June 2018. \nUS \nEPA. \n2015a. \nNotice \nof \nViolation \n(18 \nSeptember \n2015). \nhttps://www.epa.gov/sites/production/files/2015-10/documents/vw-nov-caa-09-18-15.pdf. \nAccessed 16 June 2018.  \nUS \nEPA \n2015b. \nNotice \nof \nViolation \n(2 \nNovember \n2015). \nhttps://www.epa.gov/sites/production/files/2015-11/documents/vw-nov-2015-11-02.pdf. Ac-\ncessed 18 June 2018.   \nWachter, Sandra. 2019. Data protection in the age of big data. Nature Electronics 2:6-7. \nhttps://doi.org/10.1038/s41928-018-0193-y. Accessed 29 May 2019. \nWojdynski, Bartosz, and Nathaniel J. Evans. 2015. Going Native: Effects of Disclosure Position \nand Language on the Recognition and Evaluation of Online Native Advertising. Journal of \nAdvertising 45(2): 157-168.    http://dx.doi.org/10.1080/00913367.2015.1115380 \nWP29. 2018. Guidelines on Automated individual decision-making and Profiling for the purpos-\nes of Regulation 2016/679 (wp251rev.01). http://ec.europa.eu/newsroom/article29/item-\ndetail.cfm?item_id=612053. Accessed 7 June 2018. \nYannopoulos, Giorgos. 2012a. M., A. kai L. katá S. I anonymopoíisi ton váseon nomikón \ndedoménon. Díkaio Méson enimérosis kai Epikoinonías 1: 21-27. \nYannopoulos. Georgios. 2012b. Real Rights in Virtual Worlds and Virtual Rights in a Real \nWorld. 5th International Conference on Information Law and Ethics. June 29-30. Corfu. \nhttps://doi.org/10.6084/m9.figshare.8262299.v1. \nYannopoulos, Giorgos. 2013. I efthýni ton michanismón anazítisis gia tis ypiresíes ypódeixis \n(suggest) kai aftómatis symplírosis (autocomplete). Scholiasmós tis apófasis Monomeloús \nProtodikeíou Athinón 11339/2012. Díkaio Méson enimérosis kai Epikoinonías 2: 168-171. \n \n\n4 Development of Regulatory Bodies     \nFotios Fitsilis     \nAbstract \nThe development of national, supranational or global regulatory bodies for ad-\nvanced algorithms is essential. In this chapter, a comparison of several approaches \nis attempted. Among others, the first steps towards the formation of regulatory \nbodies is explained. Moreover, we present a set of modes for classification or reg-\nulatory activities. Finally, the role of parliamentary institutions is highlighted and \nthe idea of an ‘algorithmic monitor’ based on crowdsourcing is proposed. \n \nKeywords: regulatory body, parliamentary research service, post legislative scru-\ntiny, ENISA, algorithmic monitor. \n \nShould advanced algorithms continue to advance, a structured approach to regu-\nlate them should be adopted. In this chapter, the development of regulatory bodies \nshall be discussed and the pros and cons of different regulatory concepts will be \npresented, including issues of and opportunities for self-regulation, as well as \ntechnical considerations. \nSeveral forms of oversight institutions with regards to algorithms have already \nbeen established.1 In a resolution on civil law rules on robotics the EP expressed \nits position for the establishment of a ‘(…) European Agency for robotics and arti-\nficial intelligence in order to provide the technical, ethical and regulatory expertise \nneeded to support the relevant public actors (…)’.2 At the same time, the Commit-\ntee tried to approximate the potential operational framework for this prospective \nregulatory agency by mentioning a list of possible duties:3  \n• Cross-sectorial and multidisciplinary monitoring of robotics-based applications, \n• Identification of best practice,  \n• Recommendation of regulatory measures, \n                                                           \n1 A number of references to different proposals for algorithmic regulations, \nsuch as an AI watchdog, a Machine Learning Commission, a US FDA (Food and \nDrug Administration) for Algorithms etc. is included in Andrews (2017), pp. 10-\n11. \n2 See European Parliament (2017), p. 10, §16. \n3 See European Parliament (2017), p. 10, §17. \n\n58  \n• Definition of new principles and  \n• Addressing potential consumer protection issues and systematic challenges. \nThe interesting point here is that the proposed agency is not a regulatory body \nin itself. Instead, it would provide public authorities (‘public actors’) at all levels, \ni.e. the European Union and EU member states, with an ethical code of conduct \nand the necessary expertise in order for the latter to proceed to regulatory \nmeasures, such as recommendations for the implementation of ‘kill switches’ in \nsoftware design.4 The resolution and the related report have stirred up considera-\nble interest triggering a public consultation with the aim to encourage citizens and \nstakeholders to share their thoughts and considerations in the fields of robotics and \nAI.5 However, DG Connect of the European Commission could be mandated to \nfulfil this task, instead of EU establishing another costly and quite probably, inef-\nficient agency. Alternatively, an existing agency such as the European Union \nAgency for Network and Information Security (ENISA) could be tasked with the \nsame mandate, as shall be analyzed below. If not, then there would surely be over-\nlaps of competencies between all three. \nA preliminary evaluation of the results of the consultation shows that most re-\nspondents have positive attitudes towards robotics and AI, while a large majority \nexpresses the need for public regulation in the area. Interestingly enough, the re-\nspondents also specify that regulation should be conducted at EU and/or interna-\ntional level. Some responders go a step further indicating the nature of this body, \ni.e. the Center for Data Innovation suggested the establishment of a new direc-\ntorate in the EC for the support of technological advancements in the fields of ro-\nbotics and AI without focusing on ethical or regulatory issues.6 In its response to \nEP’s resolution, the EC admitted its intention to investigate over time several as-\npects and the regulatory dimension of the issue.7  \nIn the US, a broad discussion over regulatory matters is also taking place. The \nregulatory approach is described as ‘(…) sector-specific, with oversight by a va-\nriety of agencies’.8 Scherer proposed the creation of a federal regulatory regime \nfor AI under a new legislation he calls Artificial Intelligence Development Act \n(AIDA) by establishing a tort system approach.9 The proposed federal agency \nwould consist of two components for policymaking and certification, respectively. \nThe establishment of an AI certification process would require companies and de-\n                                                           \n4 See Hasselbalch (2017). \n5 The public consultation has been launched by the European Parliament's \nCommittee on Legal Affairs (JURI) in cooperation with the EPRS via the EP \nCommittee web space; The relevant document stack and first results are presented \nin JURI (2017), while the summary report is highly anticipated. \n6  See Castro et al. (2017), p. 18.  \n7 See Ponce Del Castillo (2017), p. 3.  \n8 See Stone et al. (2016), p. 44. \n9 See Scherer (2015), p. 394.  \n\n59 \nvelopers to seek certification before bringing an AI product or service onto the \nmarket.10 The establishment of a ‘Federal Robotics Commission’ –as a part of an \nEinsteinian thought experiment– has also been discussed elsewhere.11 However, \nnot everybody is in favor of regulatory bodies and there are also voices worrying \nthey could result in hamstringing innovation12, i.e. with too much red tape. The \nbasic logic behind this position is that the underlying risk analysis is often inade-\nquate, exaggerating risks and downgrading real benefits. \nThere have been suggestions to create a ‘trusted third party’ to scrutinize deci-\nsions of automated decision-making systems, thus providing an oversight mecha-\nnism for the application of advanced algorithms.13 Such a suggestion would make \nsense if implemented on the national level; the sheer amount of complaints and the \nrange of applications would constitute a similar super-national authority dysfunc-\ntional. Alternatively, a European regulator to audit algorithms - prior to their de-\nployment - is proposed therein.14 \nChina is actively challenging US leadership in AI through an aggressive five-\nyear plan (2016-2020). At the heart of China’s science and technology policy \nthere is the National Science, Technology and Education Leading Small Group, \nwhich is headed by The Premier of the State Council.15 He has identified five dis-\ntinct agencies responsible for the development and implementation of central gov-\nernment policies in AI, while there are also other centralized agencies responsible \nfor sectoral and industry-specific regulation.16  \nIn order to structure and discuss issues of regulation, a classification in differ-\nent modes may be attempted.17 For instance, depending on the timing of regulation \n(timing mode), one may distinguish between ex-ante, e.g. via administrative deci-\nsions or legislation, and ex-post regulation. A classic ex-post regulatory approach \nwould be regulation by judicial decisions. Judicial rulings regulate by definition \nafter a case has developed. In certain disputes, such as in the Microsoft cases, sev-\neral years of investigations by authorities may precede. Nevertheless, in this early \nstage of the development of advanced algorithms, we consider ex-ante regulation \nfar more difficult, since regulatory capacity needs first to be built-up in order to \n                                                           \n10 See Scherer (2015), p. 395. \n11 See Calo (2014). \n12 See relevant blog post by O’Sullivan (2017). \n13 See Wachter et al. (2017), p. 98. \n14 See Wachter et al. (2017). \n15 See He (2017), p. 4. \n16 See He (2017).  \n17 Five regulatory classes, i.e. ‘modes’ have been proposed: intervention mode, \nhierarchical mode, natural mode, type mode and timing mode; see also Chapter 1 \nfor definitions and more details; the type and nature modes will be highlighted in \nChapter 5, during analysis of the case studies.  \n\n60  \nkeep up with growing innovation in an increasingly complex interdisciplinary \nfield.18  \nA further distinction would concern the hierarchy of regulation. Fig. 4.1 pre-\nsents a proposal for the hierarchical classification of regulation for advanced algo-\nrithms. This may be also linked to the positioning of the regulatory body in the \nchain of multi-level governance. In this regard, a national body, such as an Inde-\npendent State Body (ISB), governmental agency or unit, a supranational body, \nsuch as an EU agency, or a global intergovernmental body, could be considered as \nappropriate candidates. Depending on its positioning, a regulatory body would \nhave a different set of legal instruments at its disposal. These may include tech-\nnology standards, pertinent legal frameworks and court rulings of all relevant in-\nstances. Standards, such as the Akoma Ntoso V1.0 OASIS standard for legal doc-\numents or the European Case Law Identifier (ECLI) for citing judgments from \nEuropean and national courts,19 may not be used per se in the regulatory process. \nUsually, they first need to be adopted or endorsed by administrative decisions, or \ntransposed by regulatory acts in the national or supranational legal orders. We \nthen speak of de jure standards.  This can be attributed to the fact that several \nstandards are independently developed by standardization bodies, such as the non-\ngovernmental International Organization for Standardization (ISO), or private \ncompanies.20 In a profound analogy to the sources of international law, additional \ngrounds for reaching a regulatory decision may include scholarly opinions, cus-\ntomary practices and general principles of law.21  \nAnother, more liberal, possibility is for a complimentary system of self-\nregulation amongst private sector actors, reporting and answerable to the estab-\nlished or mandated government body.22 In the social media regime, where Face-\nbook indisputably enjoys a position of power, an interesting new approach arises. \nAccording to the company, there are plans to create an ‘independent body’ of ex-\nperts, who will take responsibility for content-related decisions.23 Facebook names \nthis body ‘Oversight Board’ and underlines its independence, a claim that will \nsurely be challenged given that its members are both going to be remunerated and \nsupported, in terms of full-time staff, by Facebook itself. Therefore, the idea to \nprivatize regulatory action in critical domains, such as private communications \n                                                           \n18 See also the relevant Quora contribution by Zhao (2017). \n19 See Akoma Ntoso (2018) and European Council (2011), respectively.  \n20 When it comes to standardization, Marx (2017, p. 26) argues that the distinc-\ntion between public and private is blurring, as both sectors increasingly co-\nregulate. \n21 According to the Statute of the International Court of Justice (ICJ), Art. 38 \n(1); on the other hand, it is highly questionable whether a regulatory body will ev-\ner be able to decide a case ex aequo et bono. \n22 This would belong to the ‘intervention mode’; more on the self-regulatory \nregime in Finck (2017), as well as in Trubek and Trubek (2007).  \n23 See Clegg (2019) and the draft charter therein. \n\n61 \nand public policy discourse, is not likely to find many supporters among regula-\ntors, the European ones in particular.  \nFrom the relevant discussion it becomes clear that there is no consensus of how \nregulation should look like or, at least in some cases, what is to be regulated. \nHowever, should societies come to an agreement on the necessity to regulate ad-\nvanced algorithms, and despite the level and the extent of regulation, one needs to \nthink about the timing of regulation. Our understanding is that regulation is inevi-\ntable and needs to be applied, sooner rather than later, in the development cycle of \ntechnologies related to advanced algorithms, based on the previously mentioned \nconsideration that regulators need to be both educated and trained in the early \nstages of this rapidly developing field.  \nCurrently, possibly with the exception of China, most states and supranational \nstate unions, such as the European Union, mainly apply ex-post regulation of ad-\nvanced algorithms via judicial decisions of all instances. Our analysis finds that a \ncentralized regulatory model is favored among most scholars and stakeholders. In \nthe EU, this could take the form of an EU agency, as mentioned above, but a hy-\nbrid approach of a central agency with national offices could also be considered \ngiven the scattered nature of legal order in each EU member state.24 ENISA, given \nits existing status as an EU agency and its mandate could be an interesting incuba-\ntor for developing regulatory expertise on advanced algorithms, particularly when \nconsidering that it has conducted related work.25 Thus, a future regulatory body \ncould be an ENISA spin-off that incorporates both existing technical know-how \nand advisory expertise.  \nOn the national level, this body would be complemented by institutions in the \nform of existing or, where not present, new ISBs. These institutions would be the \neyes and ears of the competent EU agency at the member state level. Not only \nwould they be responsible for transposing EU guidelines and regulations into the \nnational legal framework, but they could also be used as a local observatory for is-\nsues of algorithmic relevance. This so-called ‘algorithmic monitor’ could use the \ncapacity of the crowd, i.e. crowdsourcing,26 in order to spot early and analyze cas-\nes where algorithmic regulation may apply. Fig. 4.2 depicts a principal design of \nan algorithmic monitor. It relies on the ‘wisdom of the crowd’ to collect infor-\nmation on specific algorithms or areas that may require regulation. These raw pro-\n                                                           \n24 See, for instance, the organization and operation of both the European Patent \nOffice (EPO) and the national patent offices. The central instance, the EPO, has \ntaken the form of an intergovernmental organization. Another option could be the \nan analogon to ENISA, an agency for network and information security, that \nworks together with EU member states and the private sector to deliver strategic \nadvice and solutions. \n25 See relevant ENISA report on privacy and data protection by Danezis et al. \n(2015). \n26 Orozco (2016) analyzes, among others, the effects of crowdsourcing on legal, \nregulatory and policy issues. \n\n62  \nposals need to undergo a processing step, which may include filtering, prioritiza-\ntion and deeper analysis. The output of the aforementioned process would then be \nmade available for study by a supervisory body or regulatory agency, which by \ndefinition have limited capacity to equally handle any incoming proposal or com-\nplaint.27 \nParliaments are democracy’s supreme institutions. As algorithms play an in-\ncreasingly important role in people’s lives, their role in algorithmic regulation \nneeds to be discussed. As a matter of fact, we consider the parliament’s involve-\nment so significant that it deserves a dedicated study. One may define at least two \nmajor situations where a national parliament28 may express opinion, guide or di-\nrectly regulate in cases that involve advanced algorithms. Again, one must differ-\nentiate between the ex-ante and the ex-post approach. Here, the point of reference \nwould be formed by the time a specific algorithm or a special category of algo-\nrithms29 hits the market or becomes widely operational outside the developer’s \nconfined testbed.  \nMost times, the ex-ante regulation would equal the standard legislative ap-\nproach. This is considered as the most demanding stage, as legal drafts are usually \nprepared by the government and then submitted to the parliament for subsequent \namendments and discussion. National parliaments retain the right to submit pro-\nposals of law. However, in the case of complex topics such as advanced algo-\nrithms, it may be doubted whether or not parliaments own the internal capacity to \nstudy in-depth and subsequently formulate adequate legal provisions. Beyond le-\ngal elaboration, after a law has passed, one is entering the regime of parliamentary \ncontrol. Modern parliaments apply new methods for screening implementation of \na law known as Post-Legislative Scrutiny (PLS).30 Nevertheless, to date, most par-\nliaments lack sufficient capacity to systematically follow up on the implementa-\ntion of passed legislation. Instead, the traditional system of –written or oral– ques-\ntions is used by parliamentary groups and MPs to exercise parliamentary control. \nPLS can be a domain where parliaments may increase their leverage against the \nexecutive, particularly when related to the evaluation of regulations related to ad-\nvanced algorithms. \nIn the parliamentary context, the ex-post regulatory approach to advanced algo-\nrithms would be similar to the one that a parliament uses on several occasions. \nThis would involve the forming of parliamentary committee(s), hereafter commit-\ntee(s), in order to discuss a topic on algorithmic regulation. The type of the com-\n                                                           \n27 It may also be noted that the algorithmic monitor may be an algorithm on its \nown, which automatically screens, prioritizes and forwards the most ‘significant’ \nregulatory proposals to its human operators.  \n28 That supranational parliaments may play a role has been already demonstrat-\ned in the case of the European Parliament (2017). \n29 For instance, algorithms that are utilized in HFAT constitute a special catego-\nry that is regulated by the EU’s MiFID II legislative framework. \n30 See De Vrieze and Hasson (2017). \n\n63 \nmittee also defines its importance within the parliamentary universe.31 Committees \nusually have the right to invite external experts, such as academics or consultants, \nto present their opinion and throw light on the issues discussed, while parliaments \nmay also form research or advisory committees with non-MPs as members. \nFurthermore, parliaments have the right to discuss in the competent committees \nor even in the plenary a report submitted by an agency, just like the proposed EU \nagency on algorithmic regulation, or a national ISB. Undoubtedly, this would con-\nstitute the least invasive option, since no new parliamentary bodies would have to \nbe formed and one could rely on existing procedures without the need to change \nthe standing orders. Typically, such discussions result in a resolution that is ad-\ndressed to the competent Ministry or the Government as a whole, which is then \ncalled to transform it to relevant administrative actions, e.g. draft laws or adminis-\ntrative decrees. However, practice has shown that the result of parliamentary ac-\ntion is usually non-tangible and of limited regulatory impact. \nParliamentary Research Services (PaRS) can have a significant role in \nstrengthening the operations and impact of representative institutions. This is why \nmost Parliaments have established PaRS and continue to invest considerable re-\nsources in their further development.32 In order to fulfill their role at the highest \npossible level, PARS are necessary to employ highly skilled researchers that have \nadvanced expertise in a wide range of fields.33 The work of researchers can be \nlinked to the application of scientific methodology, the following of a code of \nconduct and, most importantly, the publication of elaborated material.34 In recent \nyears, an increasing demand for more complex and synthetic information from \nPaRS can be attested. Advanced algorithms constitute a wide and active field of \nstudy and PaRS have the potential to:  \n‘(…) provide internal and external clients with independent, well-researched, timely, \nstructured and concentrated knowledge products, thus counterbalancing partisan \ninformation flows or even governmental superiority in analysis and dissemination of \ninformation’.35  \nIn order to be able to do so, PaRS clearly need to significantly advance their \nrelevant capacity, mainly in scientific fields, such as big data, data ethics and legal \ninformatics, for example. \n                                                           \n31 Parliamentary committees may be formed on a regular or ad-hoc basis. These \nare also several levels of committees, such as Standing Committees, Permanent \nCommittees, Special Permanent Committees etc., according to their significance \nin the parliamentary context. \n32 The Inter-Parliamentary Union (IPU) and the International Federation of Li-\nbrary Associations and Institutions (IFLA) have published in 2015 guidelines for \nPaRS in order to help developing legislatures to establish research services as well \nas to strengthen existing ones; see IPU and IFLA (2015). \n33 On the researcher role in parliaments see Fitsilis (2018), pp. E-48–E-50. \n34 See Fitsilis (2018).  \n35 See Fitsilis and Koutsogiannis (2017), p. 11. \n\n64  \nReferences     \nAkoma Ntoso. 2018. Version 1.0 Part 1: XML Vocabulary. OASIS Standard. Edited by Mon-ica \nPalmirani, Roger Sperberg, Grant Vergottini, and Fabio Vitali. http://docs.oasis-\nopen.org/legaldocml/akn-core/v1.0/os/part1-vocabulary/akn-core-v1.0-os-part1-\nvocabulary.html. Accessed 27 June 2019.  \nAndrews, Leighton. 2017. Algorithms, governance and regulation: beyond ‘the necessary \nhashtags’. In Algorithmic regulation, eds. Leighton Andrews et al., 7-12. Lodon: Centre for \nAnalysis \nof \nRisk \nand \nRegulation. \nhttp://www.lse.ac.uk/accounting/assets/CARR/documents/D-P/Disspaper85.pdf \nCalo, Ryan. 2014. The Case for a Federal Robotics Commission. September 2014. Brookings In-\nstitution \nCenter \nfor \nTechnology \nInnovation. \nhttps://www.brookings.edu/wp-\ncontent/uploads/2014/09/RoboticsCommissionR2_Calo.pdf. Accessed 20 March 2019. \nCastro, Daniel, Nicholas Wallace, and Joshua New. 2017. Response to the European Parlia-\nment’s Public Consultation on Civil Law Rules on Robotics. Center for Data Innovation, \nhttp://www2.datainnovation.org/2017-eu-ai-public-consultation.pdf. Accessed 20 March \n2019.  \nClegg, Nick. 2019. Charting a Course for an Oversight Board for Content Decisions. Januany 28. \nhttps://newsroom.fb.com/news/2019/01/oversight-board/. Accessed 28 June 2019. \nDanezis, George, Stefan Schiffner, Marit Hansen,  Rodica Tirtea, Josep Domingo-Ferrer,  Daniel \nLe Métayer, and  Jaap-Henk Hoepman. 2015. Privacy and Data Protection by Design – from \npolicy to engineering. Heraklion: ENISA. https://doi.org/10.2824/38623. \nDe Vrieze, Franklin and Victoria Hasson. 2017. Comparative study of practices of Post-\nLegislative Scrutiny in selected parliaments and the rationale for its place in democracy assis-\ntance. \nWestminster \nFoundation \nfor \nDemocracy. \nhttps://www.wfd.org/wp-\ncontent/uploads/2018/07/Comparative-Study-PLS-WEB.pdf. Accessed 21 March 2019. \nEuropean Council. 2011. Council conclusions inviting the introduction of the European Case \nLaw Identifier (ECLI) and a minimum set of uniform metadata for case law (2011/C 127/01). \nhttps://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:52011XG0429(01). Accessed \n27 June 2019. \nEuropean Parliament. 2017. Motion for a European Parliament resolution with recommendations \nto \nthe \nCommission \non \nCivil \nLaw \nRules \non \nRobotics \n(2015/2103(INL)). \nhttp://www.europarl.europa.eu/doceo/document/A-8-2017-0005_EN.pdf. Accessed 20 March \n2019. \nFinck, Michèle. 2017. Digital Co-Regulation: Designing a Supranational Legal Framework for \nthe \nPlatform \nEconomy. \nLSE \nLaw, \nSociety \nand \nEconomy \nWorking \nPapers \n15/2017. http://dx.doi.org/10.2139/ssrn.2990043. Accessed 4 June 2019. \nFitsilis, Fotios. 2018. Inter-parliamentary cooperation and its administrators. Perspectives on \nFederalism 10(3): E-28-E-55. https://doi.org/10.2478/pof-2018-0030. \nFitsilis, Fotios and Alexandros Koutsogiannis. 2017. Strengthening the Capacity of Parliaments \nthrough Development of Parliamentary Research Services. Thirteenth Wroxton Workshop of \nParliamentary \nScholars \nand \nParliamentarians \n29-30 \nJuly \n2017. \nhttp://wroxtonworkshop.org/wp-content/uploads/2017/07/2017-Session-5A-Fitsilis-and-\nKoutsogiannis.pdf. Accessed 21 March 2019. \nHasselbalch, Gry, 2017. New EU rules for the ethical and legal status of robots and AI. \nhttps://dataethics.eu/en/new-eu-rules-ethical-legal-status-robots-ai/. \nAccessed \n20 \nMarch \n2019.  \nHe, Yujia. 2017. How China is preparing for an AI-powered Future. Wilson Briefs. June 2017. \nhttps://www.wilsoncenter.org/sites/default/files/how_china_is_preparing_for_ai_powered_fut\nure.pdf. Accessed 21 March 2019. \nIPU \nand \nIFLA. \n2015. \nGuidelines \nfor \nparliamentary \nresearch \nservices. \nhttps://www.ifla.org/publications/node/9759. Accessed 21 March 2013. \n\n65 \nJURI. 2017. Report with recommendations to the Commission on Civil Law Rules on Robotics. \nhttp://www.europarl.europa.eu/committees/en/juri/subject-\nfiles.html?id=20170202CDT01121. Accessed 20 March 2019.  \nMarx, Axel. 2017. The Public-Private Distinction in Global Governance: How Relevant is it in \nthe Case of Voluntary Sustainability Standards? The Chinese Journal of Global Governance \n3(1): 1-26. https://doi.org/10.1163/23525207-12340022.  \nOrozco, David. 2016. The Use of Legal Crowdsourcing ('Lawsourcing') as a Means to Achieve \nLegal, Regulatory and Policy Objectives. American Business Law Journal 53(1): 145-192. \nhttps://doi.org/10.1111/ablj.12074. \nO’Sullivan, Andrea. 2017. Don’t Let Regulators Ruin AI. MIT Technology Review, Novem-\nber/December 2017. https://www.technologyreview.com/s/609132/dont-let-regulators-ruin-\nai/.  \nPonce Del Castillo, Aída. 2017. A law on robotics and artificial intelligence in the EU? Foresight \nBrief \n#02-September \n2017, \nEuropean \nTrade \nUnion \nInstitute, \nBrussels. \nhttps://www.etui.org/content/download/32583/302557/file/Foresight_Brief_02_EN.pdf. Ac-\ncessed 20 March 2019. \nScherer, Matthew U. 2015. Regulating artificial intelligence systems: Risks, challenges, compe-\ntencies, and strategies, Harvard Journal of Law & Technology 29, no. 2 (Spring):353-400. \nStone, Peter et al. 2016. Artificial Intelligence and life in 2030. One Hundred Year Study on Ar-\ntificial \nIntelligence. \nStanford \nUniversity. \nReport \nof \nthe \n2015 \nStudy \nPanel. \nhttps://ai100.stanford.edu/sites/g/files/sbiybj9861/f/ai_100_report_0831fnl.pdf. Accessed 20 \nMarch 2019.  \nTrubek, David M., and Louise G. Trubek. 2007. New Governance & Legal Regulation: Com-\nplementarity, Rivalry, and Transformation. Columbia Journal of European Law 13(3): 539-\n564. \nWachter, Sandra, Brent Mittelstadt, and Luciano Floridi. 2017. Why a Right to Explanation of \nAutomated Decision-Making Does Not Exist in the General Data Protection Regulation. In-\nternational Data Privacy Law 7(2): 76-99. https://doi.org/10.1093/idpl/ipx005. \nZhao, Ben Y. 2017. Response to the question “Should artificial intelligence be regulated?” \nQuora, 13 August 2017. https://www.quora.com/Should-artificial-intelligence-be-regulated. \n \n\n66  \nFig. 4.1 Hierarchical mode of regulation for advanced algorithms  \n \nFig. 4.2 Principal design of an algorithmic monitor  \n\n5 Conclusions and Outlook     \nFotios Fitsilis \nAbstract    \nThis book offers a new perspective in the discussion around the subject of regula-\ntion of advanced algorithms. It presents a number of significant cases where \nchanges in the code and/or the conduct of its developers or operators have been \nimposed following administrative or judicial decisions. But the existing legal \nweaponry is often insufficient to directly confront the array of problems related to \nadvanced algorithms. Therefore, the administrative state must begin to employ in-\nnovative steps and perhaps aggressive approaches, in order to meet and respond to \nthe challenges highlighted in this book. \n \nKeywords: disinformation, algorithmic malpractice, code of conduct, algorithmic \nmonitor, legislative regulation. \n \nThe use of algorithms is not something new, but today’s advanced algorithms are \nindeed far different and more evolved compared to those of the past. In modern \ndigital societies, people conduct confidential and private communications via \ncomplex algorithms owned by multinational corporations. That this may result in \nmassive abuse of private data has been demonstrated in the recent Cambridge \nAnalytica scandal, which involved the collection of personally identifiable infor-\nmation of millions of Facebook users, in order to influence US voters during a \nrun-up to a general election. Those revelations led to widespread discussions on \nthe regulation of algorithms and secured Facebook’s CEO a testimony to Senate \ncommittees as well as to the EUP about the company’s data collection practices.1 \nAs a result, some scholars support the case that important decisions should always \nremain in the hands of humans, thus eliminating fears of such decisions being \nmade in a non-transparent way and without any accountability or recourse.2  \nDemocracy itself may very well have died back in its birthplace in Athens, \nGreece, some 2500 years ago. Contemporary ‘democratic’ systems are a mere \n                                                           \n1 It is these revelations that may have led Facebook’s CEO to call for more \nonline regulation and a ‘more active role for governments and regulators’; see \nZuckerberg (2019).  \n2 See, e.g., Coglianese and Lehr (2017), p. 26, in combination with Van Loo \n(2017), p. 1274. \n\n68  \nevolution of the original notion. One simply needs to ask the question whether our \nwestern democratic systems are in the position to sufficiently govern societies of \nbillions. Well, maybe they are not. In this sense, any recommendation towards an \nalternative approach in governance is more than welcome.3 Under these circum-\nstances, the role of advanced algorithms in contemporary and future democracies \ndeserves more analysis, a giant task that is out of the scope of this book.4  \nYet, in this regard, it is worth mentioning that the issue of disinformation raises \nbroader concerns of societal harm. In the worst-case scenario, the impact of disin-\nformation campaigns can affect entire societies, be that through interference in \nelection, or misinformation about foods, or medicines. The stakes involved in ac-\ncurately identifying disinformation are high because it often affects free exchange \nof ideas and information, the core of political discourse, a point that should be of \nparticular concern to parliaments. Notably, in an effort to understand the spread \nand impact of disinformation as well as ensuring the transparency of the 2019 Eu-\nropean Elections, digital platforms including Facebook, Google and Twitter \nsigned up to a voluntary EU Code of Practice on disinformation.5 Moreover, \nGoogle laid out a process to curb fake news as malicious actors have attempted to \nharm or deceive on-line search users through a wide range of actions.6 Not to be \nunderestimated in terms of its regulatory impact is also the relevant announcement \nby the G20 Trade Ministers and Digital Economy Ministers that ‘AI actors should \nrespect the rule of law, human rights and democratic value, (…)’.7  \nSince the invasion of new technologies into our lives seems inevitable and at \nthe same time very profitable for a series of private actors, it is evident, more than \never, that the administrative state needs to adjust to this new digital environment. \nThis book has therefore offered some new perspectives in the discussion around \nthe regulation of advanced algorithms by presenting a number of significant cases \nwhere changes in software have been imposed following administrative or judicial \ndecisions. These cases have been classified and analyzed in order to provide in-\nsights to the inner mechanisms of algorithmic regulation.  \n \n \n                                                           \n3 Hence, the significance of the contribution by Runciman (2018). \n4 Nevertheless, this can be linked to our proposal to work on underlying legal \nvalues and principles rather than to produce scores of regulation. \n5 See European Commission (2019); the Code is an initiative of the European \nCommission (2018). \n6 These actions include for instance tricking online systems in order to promote \ntheir own content (via a set of practices referred to as ‘spam’), propagating mal-\nware, and engaging in illegal acts online, Google (2019). \n7 See G20 (2019), p. 11; this is the first time the countries that represent \nworld’s top economies agreed upon a set of principles for ‘responsible steward-\nship of trustworthy AI’, which may serve as guidelines for national and suprana-\ntional regulatory policies. \n\n69 \nTopic \nLegislative     \nregulation  \nJudicial          \nregulation  \nRegulatory \nacts \nLegal  \naction \nLegal basis \nRegulation \non algorithm \nRegulation \non controller \nMS Media \nPlayer and \nExplorer \ncase \n \n● \n \nUS v. Microsoft \n231 F.  Supp.  2d  \n144 \n15 USC §§1-2 \n15 USC§16(b)-(h) \n \n● \nMicrosoft v. EC \nT-201/04 \nArt. 81&82 ECT \n91/250/EEC \n● \n● \nEmission \ncase \n● \n● \n (EU) P8_TA-\nPROV(2018)0\n179 \nUS v. VW \n16-CR-20394 \n18USC§371 \n18USC§1512(c) \n18USC§542 \n \n● \nAd-block \ncase \n \n● \n \nAxel Springer AG \nv. Eyeo \n§4 Nr. 4 UWG & \n§4a UWG \n \n● \nBlock \nonline    \ncontent  \n● \n● \n (EU)2016/679 \nGoogle Spain SL \n& Google Inc. v. \nAEPD&González \n95/46/EC \n2000/C 364/01 \n● \n● \nSharing \neconomy \n \na. AirBnB \ncase \n \n● \n● \nMinpaku   \nShinpou (JP) \nNomos \n4472/2017 \n(GR)           \nLoi n° 2016-\n1321 (FR) \nAirbnb France v. \nCity of Paris \n18-40.043 \nArt. L.324-2-1 & \nL.324-1-1  \nTourism Code; \nArt. L. 631-7 et \nseq. Construction \nand Housing Code \n(FR) \n● \n● \nSharing \neconomy \n \nb. Uber  \ncase \n \n \n● \nLoi  n°  2014-\n1104  (FR) \nNomos \n4530/2018 \n(GR) \n \nElite Taxi v. Uber \nSystems Spain \n2006/123/EC \n \n● \nPhiladelphia Taxi \nAssoc. Inc. v. \nUber \n15 USC § 2 \n15 USC § 15 \nSorvano v. Uber \n15 USC § 1125(a) \nCBPC §17200 et \nseq. \nRazak v. Uber \n29 USC § 201 et \nseq. \nAlgorith-\nmic      \ntrading  \n● \n● \n2014/65/EU \nUS Public \nLaw No: 111-\n203 \nCoscia v. US \n17-1099 \n7 USC §6c(a)(5) \n7 USC \n§6c(a)(5)(C) \n7 USC §9 \n18 USC§1348 \n● \n● \nGDPR  \n● \n \n(EU)2016/679 \n \n \n● \n● \n\n70  \nTable 5.1 Overview of analyzed topics on advanced algorithms \nTable 5.1 presents a wide overview of the topics of regulation this book refers \nto. The ‘type mode’ of regulation, which consists of the legislative and judicial \nregulatory approach, is displayed, along with the relevant regulatory acts, such as \ndirectives, regulations, laws and decrees, or legal action that may apply to each \ncase. In case a higher instance has confirmed a decision of a lower court, the indi-\ncated case(s) may deviate from the one(s) discussed under the relevant topic.8 Fi-\nnally, the regulatory outcome is indicated, i.e. whether regulation has been im-\nposed on the environment or the controller (indirect regulation), or on the \nalgorithm itself (direct regulation). We speak here of the nature of regulation \n(‘natural mode’).  \nNot all cases that involve algorithms are cases of direct regulation, in terms of \nimposing (or aiming at) changes directly to the algorithm,9 such as in the Mi-\ncrosoft Windows Media Player case, which led to the creation of ‘Windows XP \nN’, a Windows operating system without the media player10. Some cases may re-\nlate to pure labor issues,11 or competition12 or restrictive measures to the developer \n/ controller of an algorithm.13 There, regulation may be regarded as ‘indirect’, i.e. \nwhen conditions are imposed on the surrounding environment of the algorithm or \nits controller. It is difficult to tell, whether classification according to the natural \nmode provides any hints regarding the gravity of intervention. This may be the \ncase, to the extent regulation affects the viability of the underlying business model \nthe algorithm serves or its principal functionality. Given the lack of appropriate \ngeneral regulatory principles on algorithmic regulation, or a sector specific legal \nframework, ‘traditional’ legal instruments, such as competition law, have been \nfrequently used for algorithmic regulation. Classification in regulatory types may \nalso include the timing of the intervention.14 \nHence, overall we classify regulation into five modes and their respective sub-\nclasses, according to the type (legislative or judicial), nature (direct or indirect), \ntiming (ex-ante or ex-post) and form of the intervention (command-and-control, \nself-regulation or co-regulation). These are embedded into a vertical dimension, \nthe ‘hierarchical mode’ (global/supranational/national). With the form of interven-\n                                                           \n8 See for instance Section 3.6: Algorithmic trading regulation. \n9 The notion of change in the architecture of the code is described by Lessig \n(1999), pp. 505-506. \n10 See EC Decision of 24 March 2004, case COMP/C-3/37.392. \n11 Razak v. Uber Technologies Inc., 2:16-cv-00573. \n12 Philadelphia Taxi Assoc. Inc. v. Uber, 17-1871. \n13 The MiFID II / MiFIR framework and the GDPR offer several such provi-\nsions. \n14 Usually, the ‘type mode’ is equivalent to the ‘timing mode’; however, this \nmay not always be the case, as shown in Chapter 1. \n\n71 \ntion already presented elsewhere15 and the ‘hierarchical mode’ being intrinsic to \nany form of regulation, we hereby underline the importance of the remaining three \nmodes, i.e. ‘type’, ‘nature’ and ‘timing’, for providing added-value to the sur-\nrounding discussion. Fig. 5.1 visualizes these regulatory modes and depicts their \nattributes and interrelations.  \nThe EU and US cases, which involve anti-competitive bundling of Microsoft’s \ninternet browser and media player with the underlying operating systems, have \nbeen paramount in shaping the computer and software industry. Regulation here \nwas achieved by judicial decisions rather than using a specific legal framework. \nHowever, in both legal orders, anti-competitive provisions of Art. 101 and 102 \nTFEU16 and Sections 1 and 2 of the Sherman Antitrust Act have been used as a \ngeneral legal basis. Interestingly, the settlement achieved by Microsoft in the US \ndid not require the company to alter the code by removing IE from its operating \nsystem. An EC decision against Microsoft, affirmed in its essence by the Court of \nFirst Instance, forced Microsoft to create a different version of its operating sys-\ntem without the Windows Media Player. \nIn the VW diesel emissions case, US investigators found out that the company \nhad used a defeat device in the form of an illegal algorithmic switch, which sensed \nthe operating conditions of the vehicle and adjusted gas emissions accordingly. \nWithin the context of the present book, the basic US case17 has been discussed as \nwell as the relevant settlements as VW pled guilty to a series of criminal and civil \ncharges. At the same time, the legal framework in the EU at the time did not allow \nfor an analogous prosecution of VW on a European level. What is of particular in-\nterest is the fact that VW, both in the US and the EU, has not been explicitly urged \nto immediately cease using the mentioned defeat device. Only after talks with the \nEC, VW committed to resolve the issue for the European consumers via a soft-\nware update.  \nThe inter-relations between online privacy, digital marketing and fair competi-\ntion have been discussed before German courts in an interesting case of legitimacy \nin using ad-blocking software. Following legal action from Axel Springer AG, a \nprivate media company, BGH, the German Federal Court, declared the operation \nof AdBlock Plus lawful. According to the Court, any (financial) damage was not \ncaused by the software provider but by the users who downloaded the ad blocker. \nIn addition, it was noted that media companies have always the possibility to \nblock users that use ad-blocking software. \nThe boundaries of the personal right to block online controversial content, \nwhich is widely known as the 'right to be forgotten', have been set by the CJEU in \nits landmark judgement of the case Google Spain SL and Google Inc. v. AEPD \n                                                           \n15 In a Communication by the European Commission (2016, p. 5), self-\nregulation and co-regulation mechanisms are suggested as command-and-control \nalternatives for regulating platform economy. \n16 Formerly known as Art. 81 and 82 ECT, respectively.  \n17 US v. Volkswagen, 16-CR-20394. \n\n72  \nand González. The Court’s ruling, which applies to search engines with a branch \nor subsidiary in the EU, provides data subjects with the right to have their personal \ninformation removed from relevant searches on the basis of their name, even if \nthis information appears on the indexed pages in a lawful way. Search engine op-\nerators have to comply with such requests while keeping a number of factors in \nbalance such as their own economic viability and the right of access to infor-\nmation of the general public. The ‘right to be forgotten’ has been incorporated in \nArt. 17 Regulation (EU) 2016/679 that has significant implications both for sys-\ntem operators/controllers and the utilized software/algorithms. \nThe sharing economy, as expressed through the Airbnb and Uber cases, took \ntraditional industries by surprise. In numerous countries worldwide, the latter are \nseeking state or judicial protection in order to avoid collapse. Both Airbnb and \nUber rely on algorithms that match supply with demand with the companies cash-\ning a small provision when the arranged service has been provided. In the home-\nsharing cases, such as the case of Airbnb,18 local regulators, e.g. city administra-\ntion, or the central government, are imposing strict limitations to the landlords. On \nthe other hand, in several cases, platforms are asked not to list any offers that have \nnot been declared with the competent authorities and do not bear an official regis-\ntration number. In the Uber case, most resistance comes from the traditionally \nstrong taxi-service syndicates. A couple of US lawsuits19 have been handpicked in \norder to present the state of play in the car-sharing business. Similarly, to a differ-\nent case concerning ad-blocking in Germany, Axel Springer AG v. Eyeo, competi-\ntion laws have been invoked in the first lawsuit to protect the existing status quo in \nthe industry. With argumentation analogous, to a certain extent, to the BGH rul-\ning, the US Court of Appeals rejected the plaintiff’s arguments by stating that the \ntaxi association had no right to exclude competitors from the market. This implies \nthat taxi companies would need to become more competitive in order to continue \nto be viable in this business.20 The second pending case involves allegations of al-\ngorithmic manipulation by Uber that contain parallels with the VW emissions \ncase. \nAlgorithmic trading and HFAT have been at the center of regulatory policies \nand judicial decisions as they bear potential for serious market distortion. In recent \nyears, the European Union has pioneered financial markets regulation through \n                                                           \n18 Regulation applies to other online platforms as well, such as Homestay, \nCouchsurfing, Home Exchange, Wimdu, Bedycasa and Culture Go Go. \n19 The two lawsuits are Philadelphia Taxi Assoc. Inc. v. Uber, 17-1871 and \nSorvano v. Uber, 2:17-cv-02550-DMG-JEM. \n20 The analogy with the BGH ruling lies in the fact that the German Federal \nCourt stated that there is no general market obstruction and that the plaintiff seek-\ning judicial protection must become active in order to ‘protect’ its business. What \nis remarkable here is the fact that competition law has been used for the judicial \nreasoning in different legal orders both in the EU (in Union and member state lev-\nel) and the US. \n\n73 \nMiFID and the MiFID II / MiFIR framework. In the US, the regulatory framework \ninvolves the Dodd–Frank Wall Street Reform and Consumer Protection Act. \nBased on the latter, unlawful HFAT practices have been prosecuted and the out-\ncome of a significant case in the US has been discussed. In this complex field, \nregulations are imposed to both algorithms, which are to operate within certain \nlimits, as well as to the controllers and trading venues, such as strict reporting re-\nquirements and rules on the admission of financial instruments to trading. \nFollowing rapid developments in technology and learning from cases of the \npast, some of which have been discussed herein, the EU went a decisive step fur-\nther towards an ex-ante regulation of personal data protection. This action by the \nEU institutions took the form of a Regulation, the GDPR. The GDPR contains \nsome relevant legal principles from pre-existing data protection law, enriches \nthem and incorporates new elements for enhanced personal data protection. In the \npresent context, the concepts of data protection by design and by default have been \nanalyzed and some special parameters of data processing, such as \npseudonymization and data minimization have been discussed.  \nThe present book examined therefore a series of major cases where advanced \nalgorithms came to play. One of the issues that has been investigated dealt with \nthe research question as to whether existing legal instruments are sufficient to con-\nfront an array of problems related to advanced algorithms. Legislative (or ex-ante) \nregulation in a field that leaps forward proves indeed difficult. Analysis shows that \ndespite ongoing algorithmisation of administrative decisions and private opera-\ntions, only the EU and possibly China (although the level of analysis herein is not \nsufficient) are proceeding with a general legislative framework to regulate algo-\nrithmic conduct. The EU is expressing its will to be a front-runner in establishing \na general legislative framework for algorithmic regulation. In this regard, the \nGDPR and MiFID II, in data privacy and algorithmic trading regulation respec-\ntively, offer clear indications of a more aggressive centralized approach compared \nto the US. On the other hand, the US is more decisive when it comes to legal ac-\ntion, with its prosecutors even charging perpetrators with criminal charges, such as \nin the VW case, which itself amounted to commercial fraud and intended \nmisselling of a product. \nNevertheless, even a firm regulatory framework cannot foresee any biases in \nalgorithms or reveal algorithmic malpractices. This can only be achieved through \nin-depth investigations and thorough analysis by appropriate and highly special-\nized bodies. In this regard, the establishment of a relevant agency for algorithmic \nregulation and control is proposed. According to the approach presented herein, \nhowever, it is not necessary to establish a new institution or agency from scratch. \nIn order to save valuable time and resources,21 the competencies of an existing \nagency could be expanded in order to cover the aforementioned issues. Hence, \n                                                           \n21 See relevant discussion on the EU level on the establishment of an agency \nwith market surveillance powers to oversee road transport. Here the main argu-\nments against it focused on the high cost for its implementation. \n\n74  \ngiven the nature of advanced algorithms, such as complexity and early stage of \ndevelopment, this contribution to the field considers ENISA, a European agency \nthat focuses on network and information security, or an appropriate spin-off, to be \nan interesting candidate for this role.  \nNational or supranational parliaments could also play an important role in algo-\nrithmic regulation, particularly in the course of their oversight function. Their role \ncould be further strengthened by establishing special mechanisms for following up \nimplementation of laws and by-laws, a process that is called post-legislative scru-\ntiny. For this, it is necessary to increase their administrative and scientific capaci-\nty, through for example further development of their parliamentary research ser-\nvices. In view of the rise of advanced algorithms and the overall significance of \nrepresentative institutions in democratic societies, a dedicated study should be \nconducted in order to analyze the parameters and conditions under which parlia-\nments could contribute in the field of algorithmic regulation.   \nA solid legal framework is a non-plus-ultra to achieve quick and well-founded \nlegal decisions in the many legal disputes that are going to take place in the time \nyet to come. As several cases where algorithms come to play have been legally as-\nsessed, scrutiny illustrates that the existing legal weaponry is still not sufficient to \ndirectly and efficiently regulate advanced algorithms. Instead, antitrust and com-\npetition laws have been utilized in many of the discussed cases. In a rapidly de-\nveloping field with ever shortening life cycles of advanced algorithms and of the \nrelated software products or platforms, yearlong investigations in such disputes \nwill practically result in a denial of justice. Specialized, rather than general,22 leg-\nislation, such as EU’s GDPR/MiFID II, cooperation between state institutions \naround the globe, such as in the Microsoft cases, and the evolution of dedicated \nagencies, will be necessary to spot problematic algorithmic cases and efficiently \ntackle related issues even before they arise. An alternative approach would be an \nattempt to develop a rigid regime of legal values, along with a set of related rights \nthat apply while designing, implementing and operating advanced algorithms.23 \nThe definition of those values and rights should be affected on the supranational \nlevel, such as in the form of a convention or resolution, rather than at the national \nlevel and it is a prerequisite for them to be applied by judges in an efficient man-\nner.  \nOverall, in the wide regime of advanced algorithms, technology moves faster \nthan governments can address its effects and a clear governmental regulatory pat-\ntern does not seem to exist. But even in cases of regulation through administrative \ndecisions, the competitive nature of the research field and the projections of its fu-\nture market value may result in a situation where those decisions are always con-\ntested in front of the competent courts and even up to the highest instance. As a \nresult, judicial (or ex-post) regulation is the rule. This is not expected to change in \nfuture algorithmic cases, even with further development of dedicated specialized \n                                                           \n22 E.g. competition or labor law \n23 The newly forged ‘right to be forgotten’ may constitute such a right. \n\n75 \nlegislation. Hence, the –constitutional– right to a speedy trial could be guaranteed \nby the definition of general legal values applicable to algorithmic cases, as men-\ntioned previously.    \nAlgorithmic malpractices, such as concerns regarding discrimination and \nbreach of privacy, just to name a few, are subject of a heated debate. While regula-\ntors may ensure that data subjects are protected against private sector offenders, a \nlegitimate question to be asked is whether these are also sufficiently protected \nfrom algorithmic malpractices by state organs. Differently formulated, and in the \nabsence of tailored legal values, how can it be ensured that the basic principles of \npublic law, such as non-discrimination, accountability, transparency, are not being \nviolated by the state itself? The outcome of our on-going investigation to this \nquestion is of particular significance, as it touches upon fundamental individual \nand collective rights that are essential for the functionality of modern democratic \nsocieties. The development of a code of conduct for public sector agencies, the es-\ntablishment of an ‘ethics advocate’ or a mere ‘trusted third party’, who represents \nan independent and trustworthy expert24 within critical public service units and \ncontinuous professional training on privacy, legitimacy and democratic values, all \nconstitute efficient tools to counterbalance informational superiority of the State \nagainst private data subjects and to ensure the legality of its actions.  \nThe main parameters that have been analyzed for each of the described cases \nwere location (EU, US or elsewhere), administrative or judicial reasoning and le-\ngal basis. As a next step, the geographical criterion could be expanded to cover \nmore cases from other continents. A general finding of the book is that the Euro-\npean Union is quick in taking legislative action, whereas the US is quick in taking \nlegal action. Further cases of algorithmic regulation are necessary to be studied to \nsupport this claim, however. Additional research is also necessary to determine the \noperational framework of a mechanism to screen cases where algorithms come to \nplay and potentially need regulation. This so-called ‘algorithmic monitor’, which \ncould also involve crowdsourcing, is considered to offer essential input to the \nwork of a regulatory body. The form and operation of such an apparatus requires \nsignificant study itself.  \nConcluding, despite the aforementioned concerns that come with the use of al-\ngorithms, governments should not be hesitant to invest in their immense possibili-\nties to ameliorate the administrative state. Still, such technologies are not ripe \nenough and we should use the time for the planning of regulatory principles and \nlaw-making. Scientific foresight and forward-thinking legal assessment should be \nwidely employed in order to determine and regulate the effects of advanced algo-\nrithms in and for future societies.    \n                                                           \n24 Alternatively, a body of experts could be regarded, depending on the com-\nplexity and significance of the administrative decisions to be made. \n\n76  \nReferences \nCoglianese, Cary, and David Lehr. 2017. Regulating by Robot: Administrative Decision Making \nin the Machine-Learning Era. Research Paper No. 17-8. Institute for Law and Economics. \nUniversity of Pennsylvania. https://www.law.upenn.edu/live/files/6329-coglianese-and-lehr-\nregulating-by-robot-penn-ile. Accessed 18 March 2019. \nEuropean Commission. 2016. Communication on Online Platforms and the Digital Single Mar-\nket. \nOpportunities and Challenges for Europe. COM (2016) 288 final. https://eur-lex.europa.eu/legal-\ncontent/EN/TXT/PDF/?uri=CELEX:52016DC0288&from=EN. Accessed 27 June 2019.    \nEuropean \nCommission. \n2018. \nCode \nof \nPractice \non \nDisinformation. \nhttps://ec.europa.eu/newsroom/dae/document.cfm?doc_id=54454. Accessed 6 June 2019. \nEuropean Commission. 2019. Code of Practice against disinformation: Commission calls on sig-\nnatories \nto \nintensify \ntheir \nefforts. \nPress \nrelease, \nJanuary \n29, \n2019. \nhttp://europa.eu/rapid/press-release_IP-19-746_en.htm. Accessed 31 May 2019.  \nGoogle. 2019. How Google Fights Disinformation: 9-16. https://storage.googleapis.com/gweb-\nuniblog-publish-prod/documents/How_Google_Fights_Disinformation.pdf. Accessed 31 May \n2019.    \nG20. 2019. G20 Ministerial Statement on Trade and Digital Economy. June, 8-9. Tsukuba, Ja-\npan. \nhttps://g20trade-\ndigital.go.jp/dl/Ministerial_Statement_on_Trade_and_Digital_Economy.pdf. Accessed 25 \nJune 2019. \nLessig, Lawrence. 1999. The law of the horse: What cyberlaw might teach. Harvard Law Re-\nview 113.2: 501-546. \nRunciman, David. 2018. How democracies end. London: Profile Books. \nVan Loo, Rory. 2017. Rise of the digital regulator. Duke Law Journal 66:1267-1329.  \nZuckerberg, Mark. 2019. The Internet needs new rules. Let’s start in these four areas. The Wash-\nington Post, March 30, 2019. https://www.washingtonpost.com/opinions/mark-zuckerberg-\nthe-internet-needs-new-rules-lets-start-in-these-four-areas/2019/03/29/9e6f0504-521a-11e9-\na3f7-78b7525a8d5f_story.html?noredirect=on&utm_term=.751b6e9e19e7. Accessed 31 May \n2019.  \n \nFig. 5.1 Advanced classification of regulatory measures: natural, type and timing modes \n\nAppendix   \nTables \nCase \nIdentifier \nDocument type \nYear \nAirbnb France v. City of Paris \n18-40.043 \nJudgement of the Court of \nCassation (Third Civil    \nChamber) \n2019 \nAsociación Profesional Elite Taxi v. \nUber Systems Spain, SL \nC- 434/15 \nJudgment of the Court   \n(Grand Chamber) \n2017 \nAxel Springer AG v. Eyeo \nI ZR 154/16 BGH \nPress announcement, BGH \n2018 \nCity of Paris v. Airbnb France and \nAirbnb Ireland \n18-54.632 \nJudgement, TGI \n2019 \nCity of Paris v. Ms. Claire G.F. \n17-26.158 \nJudgement of the Court of \nCassation (Third Civil    \nChamber) \n2018 \nCoscia v. United States \n17-1099 \nDocket for 17-1099 \nSupreme Court, US \n2018 \nGoogle Spain SL and Google Inc. v. \nAEPD and Mario Costeja González \nC-131/12 \nJudgment of the Court   \n(Grand Chamber) \n2014 \nMicrosoft v. European Commission \nT-201/04 \nJudgment, Court of First In-\nstance (Grand Chamber) \n2007 \nM.L. and W.W. v. Germany \nECHR 554 \nJudgment (fifth section) \n2018 \nNew York v. Microsoft Corp. \nF. Supp. 2d 132 \n(D.D.C. 2002) \nMemorandum Opinion \n2002 \nPhiladelphia Taxi Association, Inc. \nv. Uber Technologies \n17-1871 \nOpinion, US Court of Appeals \nfor the Third Circuit \n2017 \nRazak v. Uber Technologies Inc. \n2:16-cv-00573 \nMemorandum Re: Defendants’ \nMotion for Summary         \nJudgment \n2018 \nScarlet Extended SA v. SABAM \nC-70/10 \nCourt Judgment                     \n(Third Chamber) \n2011 \nSorvano v. Uber \n2:17-cv-02550-\nDMG-JEM \nClass action complaint \n2017 \nUS v. Coscia \n14-cr-00551 \nMemorandum Opinion  \n2014 \nUS v. Microsoft Corp.  \n231 F. Supp. 2d 144 \n(D.D.C.2002)     \nMemorandum Opinion  \n2002 \nUS v. Sarao \n1:15-cr-00075 \nPlea Agreement \n2016 \nUS v. Volkswagen \n16-CR-20394 \nPlea Agreement \n2017 \nTable A Overview of court cases  \n \n\n78  \nInstitution/Organization  \nIdentifier \nTitle \nYear \nArticle 29 Working Party (WP29) \n17/EN - \nWP251rev.01 \nGuidelines on Automated in-\ndividual decision-making and \nProfiling for the purposes of \nRegulation 2016/679 \n2018 \n \nAssemblée Nationale, FR \nLoi  (Law)               \nn° 2014-1104 \nLaw relating  to  taxis  and  \nchauffeur-driven  transport \n2014 \nAssemblée Nationale, FR \nLoi (Law)                \nn° 2016-1321 \nLaw for a Digital Republic \n2016 \nAssemblée Nationale, FR \nLoi (Law)                 \nn° 2016-1918 \nBudget Law \n2016 \nAssemblée Nationale, FR \nLoi (Law)                 \nn° 2018-898 \nAnti-Fraud Act \n2018 \nCongress, US \n42 USC 7401–7626 \nClean Air Act \n1963 \nCongress, US \nPublic Law No: 111-\n203 \nDodd–Frank Wall Street Re-\nform and Consumer Protection \nAct \n2010 \nCongress, US \nS.2123 - 114th Con-\ngress (2015-2016) \nSentencing Reform and Cor-\nrections Act (pending) \n2015 \nCouncil of Europe \nResolution 2051 \n(2015) \nDrones and targeted killings: \nthe need to uphold human \nrights and international law \n2015 \nEnvironmental Protection Agency, \nUS \nSEP 18 2015, NOV – \n2 2015 \nNotice of Violation (1st & 2nd) 2015 \nEuropean Parliament \n2015/2103(INL) \nReport with recommendations \nto the Commission on Civil \nLaw Rules on Robotics \n2017 \nEuropean Parliament \n2015/3037(RSO) \nCommittee of Inquiry into \nEmission Measurements in the \nAutomotive Sector \n2015 \nEuropean Parliament \n2016/2215(INI) \nCommittee report with find-\nings and conclusions \n2016 \nEuropean Parliament \nEPRS, 13 July 2017 Public consultation on robotics \nand AI - First results of public \nconsultation \n2017 \nEuropean Union \n2000/C364/01 \nCharter of Fundamental Rights \nof the European union \n2000 \nEuropean Union \n2000/31/EC \nDirective on electronic com-\nmerce \n2000 \nEuropean Union \n2001/29/EC \nCopyright Directive \n2001 \nEuropean Union \n2002/58/EC \nDirective on privacy and elec-\ntronic communications \n2002 \nEuropean Union \n2004/48/EC \nDirective on the enforcement \nof intellectual property rights \n2004 \nEuropean Union \n2006/123/EC \nServices in the internal market 2006 \nEuropean Union \n2007/46/EC \nApproval of motor vehicles \n2007 \n\n79 \nand their trailers, and of sys-\ntems, components and separate \ntechnical units intended for \nsuch vehicles \nEuropean Union \n2014/65/EU \nMarkets in Financial Instru-\nments Directive II \n2014 \nEuropean Union \n2016/679/EU \nGeneral Data Protection Regu-\nlation (GDPR) \n2016 \nEuropean Union \n2015/2120/EU \nRegulation on roaming on \npublic mobile communications \nnetworks within the Union \n2015 \nEuropean Union  \n2018/858/EU \nVehicle type approval frame-\nwork regulation  \n2018 \nEuropean Union \nCOM/2016/0593 fi-\nnal - 2016/0280 \n(COD) \nProposal for a Directive on \ncopyright in the Digital Single \nMarket \n2016 \nHellenic Data Protection Authority, \nGR \nDecision 1/2017 \nNotification for processing of \npersonal data for electronic \nticket application \n2017 \nHellenic Data Protection Authority, \nGR \nDecision 4/2017 \nProcessing of personal data in \nthe framework of the unified \nautomatic fare collection sys-\ntem \n2017 \nHellenic Parliament, GR \nNomos (Law)                \n4472/2017 \nArt. 83 and 84 regulate income \ntaxation and other issues relat-\ning to house sharing, respec-\ntively \n2017 \nHellenic Parliament, GR \nNomos (Law)                \n4530/2018 \nRegulation of transport issues \nand other provisions \n2018 \nHouse of Commons, UK \nMinutes of Proceed-\nings, 2 May 2018 \nBreast Cancer Screening \n2018 \nInformation Commissioners’ Office, \nUK \n- \nReport on the Consultation, \nGDPR consent guidance \n2017 \nInternational Court of Justice \n- \nStatute \n1945 \nInter-Parliamentary Union & Inter-\nnational Federation of Library Asso-\nciations and Institutions \nISBN 78-92-9142-\n630-0 \nGuidelines for PaRS \n2015 \nJuutaku shukuhaku jigyohō \n‘Minpaku Shinpou’, JP \nAct No. 65 \nPrivate House Lodging Busi-\nness Act \n2017 \nLa Mairie de Paris (Paris City Hall), \nFR \n2017 DLH 128 \nObligation to register a proper-\nty according to Art. L314-1-1 \nof the Tourism Code \n2017 \nTable B. Overview of presented decisions, directives, laws, regulations, resolutions and reports \n \n\n80  \nIndex   \n \n \n \n \n \nA \naccess to information, 2, 51, 72 \naccountability, 8, 14, 37, 48, 52 \nad blocker, 31, 71 \nadministrative decisions, 2, 7, 16, 59, 73–\n75 \nadministrative law, 2, 6, 11–14 \nadvanced algorithms, 2, 4, 12, 13–14, 18, \n58–63, 68, 73 \naffiliate links, 31 \nagency, 8, 28, 33, 36, 50, 57, 62, 73 \nakoma ntoso, 17 \nalgorithmic monitor, 61, 75 \nalgorithmic regulation, 3, 6, 8, 21, 28, 40, \n61–63, 68, 73, 75 \nalgorithmic trading, 6, 44–46, 72 \nantitrust, 25, 27, 42, 71, 74 \nartificial intelligence, 11, 57–58 \nB \nbias, 7, 14, 16, 19, 43 \nbig data, 63 \nblack box, 16 \nblockchain, 12 \nbrowser, 25, 71 \nby default (data protection), 50, 73 \nby design (data protection), 50, 73 \nC \ncode, 15 \ncode (program), 6, 15, 17, 26, 40, 70, 71 \ncode of conduct, 58, 63, 75 \ncommon rules, 42 \nconsent, 2, 18, 26, 36, 48, 52 \nconsumer protection, 6, 44, 46, 58, 73 \ncontent, 7, 17, 30, 33, 68, 71 \ncontroller, 6, 34, 37, 48, 52, 70, 73 \ncookies (computer), 30 \ncounterfactual explanations, 21 \ncriminal law, 15 \ncritical infrastructure, 12 \ncrowdsourcing, 61, 75 \nD \ndark pools, 45 \ndata minimisation, 51, 73 \ndata protection, 6, 48–50, 61, 73 \ndata subject, 4, 30, 35, 48, 50, 72 \ndecision-making, 4, 13–14, 21, 36, 59 \ndeep learning, 11, 15 \ndefeat device, 28, 43, 71 \ndemocracy, 2, 67 \ndeveloper, 13–15, 21, 70 \ndigital platforms, 4, See also online \nplatforms \ndisinformation, 40, 68 \ndrones, 12 \nE \nEcho (Amazon), 6, 18, 20 \nethics, 20, 63, 75 \nEU law, 32, 51 \nEuropean law, 27, See EU law \nF \nFacebook, 67 \nfreedom of expression, 35–36 \nfundamental rights, 2, 34 \nfuzzy logic, 15 \nG \ngovernment, 2, 8, 18, 28, 40, 59, 62 \nI \nintellectual property, 27, 33, 40 \nInternet Explorer (Microsoft). See also \nbrowser \nJ \njudicial decisions, 17, 61, 71, See judicial \nrulings \njudicial rulings, 7, 59 \njudiciary, 18, 40 \njurisdiction, 28, 47 \nK \nkill switch, 46, 58 \nL \nlegal framework, 13, 30, 40, 50, 52, 61, 71, \n74 \nlegal informatics, 17, 63 \nliability, 43, 47 \nliberal, 5, 60 \nlicensing, 26, 31, 47 \n\n81 \nM \nmachine learning, 12, 17, 20, 57 \nmanipulation, 47, 72 \nmarket distortion, 45, 72 \nmedia player (Microsoft), 7, 27, 70, 71 \nMinpaku Shinpou (Japan), 39 \nmodes of regulation, 5, See regulatory \nmodes \nmulti-level governance, 5, 60 \nN \nnative advertising, 31 \nnon-discrimination, 4, 8, 13, 75 \nO \nonline platforms, 5, 38, 72 \nopacity, 4, 19 \nopen source, 21 \nopt-in, 48, 49 \nopt-out, 49 \noversight, 8, 13, 45, 57, 59, 74 \nownership, 15 \nP \nparliamentary research services, 63, 74 \nparliaments, 7, 62, 74 \npenal law, 7, See criminal law \npersonal data, 7, 19, 32, 34, 36, 48, 73 \npost-legislative scrutiny, 8, 62, 74 \npricing, 37, 43 \nprivacy, 7, 18, 30, 35, 48, 50, 52, 61, 71 \nprivate sector, 14, 18, 52, 60, 75 \nprofiling, 19, 36, 48 \nprosecution, 71 \nprotectionism, 44 \npseudonymisation, 50, 51, 73 \npublic interest, 19, 44 \npublic sector, 14, 75 \nR \nregulatory bodies, 3, 7, 53, 57, 59 \nregulatory modes, 8, 71 \nrepresentative institutions, 21, See \nparliaments \nright to be forgotten, 7, 35, 49, 72 \nright to explanation, 16 \nrobotics, 15 \nS \nsearch engine, 33, 72, See browser \nsettlement, 26, 29, 71 \nsharing economy, 7, 37, 41, 72 \nsingularity, 1, 4 \nsmart home, 18 \nsocial networks, 4, 31 \nsoftware, 1, 7, 18, 26, 30, 31, 37, 40, 43, \n58, 68, 71, 74 \nstakeholders, 5, 48, 50, 52, 58, 61 \nT \ntransparency, 4, 8, 13–14, 67, 75 \ntransport services, 41–44 \ntrusted third party, 59, 75 \nU \nusability, 30 \nV \nvirtual world, 47 \nW \nwarfare, 12",
    "pdf_filename": "Imposing Regulation on Advanced Algorithms.pdf"
}