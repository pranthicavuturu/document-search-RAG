{
    "title": "Key-Element-Informed sLLM Tuning for Document Summarization",
    "abstract": "sionsentenceandtheninstructsafine-tunedsLLMtoinclude Remarkable advances in large language models (LLMs) have thekeyelementswhengeneratingasummary,wherethefine- enabledhigh-qualitytextsummarization. However, thiscapa- tuningisconductedtooptimizethesLLMforthekey-element- bilityiscurrentlyaccessibleonlythroughLLMsofsubstantial informedsummarization.WeevaluateKEITSumonadialogue sizeorproprietaryLLMswithusagefees.Inresponse,smaller- summarizationdataset,DialogSum[9],andanewssummariza- scaleLLMs(sLLMs)ofeasyaccessibilityandlowcostshave tiondataset, CNN/DM[10], usingamulti-dimensionalmetric beenextensivelystudied,yettheyoftensufferfrommissingkey toassesssummarizationquality,UniEval[11]. Itisdemon- informationandentities,i.e.,lowrelevance,inparticular,when stratedthatKEITSumimprovesthesummaryqualitycompared input documents are long. We hence propose a key-element- tothebaselineLLaMA2-7B,particularlyintermsofrelevance informedinstructiontuningforsummarization,so-calledKEIT- andwhensummarizinglongdialogsordocuments.Inaddition, Sum,whichidentifieskeyelementsindocumentsandinstructs wealsoobservedthatKEITSumiseffectiveinreducinghallu- sLLMtogeneratesummariescapturingthesekeyelements.Ex- cinations. perimental results on dialogue and news datasets demonstrate that sLLM with KEITSum indeed provides high-quality sum-",
    "body": "Key-Element-Informed sLLM Tuning for Document Summarization\nSangwonRyu∗1,HeejinDo∗1,YunsuKim3,GaryGeunbaeLee1,2,JungseulOk†1,2\n1GraduateSchoolofArtificialIntelligence,POSTECH,SouthKorea\n2DepartmentofComputerScienceandEngineering,POSTECH,SouthKorea\n3aiXplainInc.,LosGatos,CA,USA\n{ryusangwon, heejindo, gblee, jungseul}@postech.ac.kr, yunsu.kim@aixplain.com\nAbstract fieskeyelementsconsistingofthenamedentitiesandconclu-\nsionsentenceandtheninstructsafine-tunedsLLMtoinclude\nRemarkable advances in large language models (LLMs) have\nthekeyelementswhengeneratingasummary,wherethefine-\nenabledhigh-qualitytextsummarization. However, thiscapa-\ntuningisconductedtooptimizethesLLMforthekey-element-\nbilityiscurrentlyaccessibleonlythroughLLMsofsubstantial informedsummarization.WeevaluateKEITSumonadialogue\nsizeorproprietaryLLMswithusagefees.Inresponse,smaller-\nsummarizationdataset,DialogSum[9],andanewssummariza-\nscaleLLMs(sLLMs)ofeasyaccessibilityandlowcostshave\ntiondataset, CNN/DM[10], usingamulti-dimensionalmetric\nbeenextensivelystudied,yettheyoftensufferfrommissingkey toassesssummarizationquality,UniEval[11]. Itisdemon-\ninformationandentities,i.e.,lowrelevance,inparticular,when stratedthatKEITSumimprovesthesummaryqualitycompared\ninput documents are long. We hence propose a key-element- tothebaselineLLaMA2-7B,particularlyintermsofrelevance\ninformedinstructiontuningforsummarization,so-calledKEIT-\nandwhensummarizinglongdialogsordocuments.Inaddition,\nSum,whichidentifieskeyelementsindocumentsandinstructs\nwealsoobservedthatKEITSumiseffectiveinreducinghallu-\nsLLMtogeneratesummariescapturingthesekeyelements.Ex- cinations.\nperimental results on dialogue and news datasets demonstrate\nthat sLLM with KEITSum indeed provides high-quality sum-\n2. Relatedwork\nmarizationwithhigherrelevanceandlesshallucinations,com-\npetitivetoproprietaryLLM.\nInformationomissionindialoguesummarization. Informa-\nIndexTerms: naturallanguagegeneration,abstractivespoken\ntion or entity omission remains a persistent challenge in dia-\ndocumentsummarization,namedentityrecognition\nloguesummarization.Traditionalencoder-decodermodelshave\ntried to overcome this problem using various methods: [12]\n1. Introduction introduces a method to detect information missing in conver-\nsations. [13]introducescontrastiveandself-supervisedlosses\nWith the advent of Large Language Models (LLMs), recent\nto address entity omission and other inconsistency problems.\nstudies have utilized LLMs across a broad spectrum of ap-\n[14]guidedtheinclusionofimportantspansidentifiedthrough\nplications. Consequently, for summarization tasks, there is a\nQuestion-Answering (QA) signals into the summaries. How-\nparadigmshiftfromtraditionalencoder-decoder-basedmodels\never, research on addressing missing information in dialogue\n[1, 2, 3, 4, 5] to LLMs. It has been revealed that LLMs pro-\ndatasetsviasLLMshasnotyetbeenextensivelyexplored.\nducemorecontextualandnaturalsummariesthantheencoder-\nEntity extraction for summarization. Methods for extract-\ndecodermodels[6,7,8]whereLLMsdonotmerelyputwords\ningentitiesfromthedocumenttoensuretheirinclusioninthe\nfrom the document; instead, they substitute appropriate syn-\nsummaries have been introduced to mitigate entity omission\nonymsforasummary,resultinginmorenaturalexpressionsand\nin other summarization domains. [15] used the named entity\nflows [6]. Noticeably, LLMs often generate even better sum-\nrecognition(NER)bymaskingextractedentitiesinsteadofran-\nmariesthanhuman-writtenreferences[7,8].\ndomtokenswhenpre-trainingBART.However,itstillhasfun-\nHowever,suchahigh-qualitysummarizationhasbeenonly\ndamentallimitationsinherenttoencoder-decodermodels. [16]\naccessible by proprietary LLMs with usage fees or LLMs of\nemployed a two-stage CoT method, where elements were ex-\nlargesizes.Toimproveaccessibility,publiclyavailablesmaller-\ntractedviaGPT-3[17]intheinitialstage,andthenGPT-3was\nscale LLMs (sLLMs) can be considered. Noting that sLLMs\nutilizedagaintointegratethoseextractedelementstogenerate\ncan generate more fluent sentences than traditional encoder-\nasummary. However,itcouldachieveelementextractiononly\ndecodermodels,usingsLLMsforsummarizationisapromising\nwithmodelsexceeding175Bparameters,requiringtremendous\napproach.However,accordingtoourevaluation(Figure2),they\ncosts. Distinguishedfromtheirworks,weaimtoleveragethe\nstillsufferfromtheproblemofomittingkeyentitiesorinforma-\npreviouslyunexploredsLLM,LLaMA2-7B,totakeadvantage\ntionbutincludingsuperfluoussentencesinsummaries,i.e.,low\nofitscomprehendingabilitieswhilealleviatingthecostburden.\nrelevance.\nUnliketheAPI-reliedentityextractionof[16],oursimpleuse\nHence, we aim to unleash the summarization capabilities\nofNERfurtherdiminishestheburden.\nofsLLMsbyaddressingtheproblemoflowrelevance. Tothis\nend,weproposekey-element-informedsLLMtuningfordocu- Evaluation metrics. Recently, critical limitations of the\nmentsummarization(KEITSum),ofwhichanoverviewisillus- ROUGEscorehavebeenpointedout[16,18,19,7]: ithighly\ntratedinFigure1. Givenaninputdocument,KEITSumidenti- relies on the number of overlapping words and, thus, deval-\nues appropriate synonyms generated in LLM. Furthermore,\n*equalcontribution ROUGEisunabletoevaluateentityomissionorhallucination\n†correspondenceto:jungseul@postech.ac.kr [20, 21, 22, 23, 11, 24, 25, 26]. Therefore, various multi-\n4202\nvoN\n91\n]LC.sc[\n3v52640.6042:viXra\nDialogue Article\nSource Article #Person1#: Hello, can I help you? By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: .\n# #P Pe er rs so on n2 1# #: : I A'd lr il gik he t .t o H ore wn lt o a n gT o wy io llt ya o C ua nro el ea d. it? 0 st8 a: t0 ic7 cE aS rT a, v 2 a nM fa rorc mh c2 a0 r1 b3 o n. T mhr oe ne o m xide em pb oe ir ss o o nf in t gh e w s oa um lde h f aa vm ei l by e w eh no died in a\n# #P Pe er rs so on n2 1# #: : F Ho ar v e3 yd oa uys e. ver rented a car before? u con uc po ln es c Joio hu ns a'w ndit h Ai un d m rein y u Ct oe os' k, i wnv ee rest dig isa ct oo vrs e rs ea did a t lo od na gy s. i dT eh e th b eo ird die as u o gf h tm ea r,r ried\nExN traE cR tion + C Eo xn trc alu cts ioio nn # #P Pe er rs so on n2 1# #: : N Suo r, e I , h wa ev e hn a' vt. e C Ta on y I o c tah o Co as re o lt ah 'se ic no blo lar c o kf , t rh ee d c aa nr d? silver ... M Caa mu bre oe rn n, e a , t w t eh se t m Coo rb ni wle a h llo . m The e t ih ne qy u s eh sta sr e hd a vo en n T or wem oa pr ele n eH do m inte o P ta hr ek din e aths\nlast Saturday, with investigators ...\nInstruction\ninstruction:\nSummarize the following text concisely while ensuring the accuracy of important content. Specifically emphasize and accurately represent entities enclosed\nKey-element-informed between '<' and '>'. Additionally, conclude the summary with a '<conclusion>' and '<\\conclusion>'tag to encapsulate the main takeaway or conclusion of the\nInstruction news article. Provide a comprehensive yet brief overview:\ndialogue: article:\n#Person1#: Hello, can I help you? By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: .\n<conclusion>#Person2#: I'd like to rent a <Toyota>Carola.<\\conclusion> 08:07 EST, 2 March 2013 . <conclusion>Three members of the same family\nFine-tune #Person1#: Alright. How long will you need it? who died in a static caravan from carbon monoxide poisoning would have\n#Person2#: For <3 days>. been unconscious 'within minutes', investigators said <today>.<\\conclusion>\n#Person1#: Have you ever rented a car before? The bodies of married couple <John>and <Audrey <Cook>>were discovered\nLLaMA2-7B #Person2#: No, I haven't. Can I choose the color of the car? alongside their daughter, <Maureen>, at the mobile home they shared on\n#Person1#: Sure, we have <Toyota>Carola's in black, red and silver ... Tremarle Home Park in Camborne, west Cornwall. The inquests have now\nopened into the deaths <last Saturday>, with investigators ...\nsummary:\nsummary: John and .Audrey Cook were discovered alongside their daughter,\nSummary #Person2#'d like to rent a silver Toyota Carola and #Person1# helps Maureen .They were found at Tremarle Home Park in Cornwall .Investigators\n#Person2# go through procedures. say the three died of carbon monoxide .poisoning .\nFigure1:DescriptionofKEITSumFramework.Weextractnamedentitiesandconclusionsentencefromthesourcedocumentandinsert\nemphasistokens.Followingthis,wecreateafulldescriptionbyaddingdetailedinstructions.\ndimensional evaluation metrics have emerged [18, 19, 11, 24, Table1: ThenumbersinDialogueandSummaryrepresentthe\n25],amongwhichUniEvalisknowntohavethehighestcor- countofsamplescontainingeachentityoutof500inthevalida-\nrelation with human evaluation currently. UniEval assesses tionset.TheratioisthenumberintheSummarydividedbythat\nscoresforcoherence, consistency, fluency, andrelevance. We intheDialogue.Bluebackgroundhighlightsselectedentities.\nmainlyaimtoimproverelevance,whichevaluateswhetheronly\nthekeyinformationfromthedocumenthasbeenincludedinthe NamedEntity Ratio Dialogue Summary\nPERSON 0.839 186 156\nsummary. GPE 0.481 81 39\nLANGUAGE 0.474 19 9\nORG 0.411 56 23\n3. Key-element-informedtuning FAC 0.350 20 7\nNORP 0.333 42 14\nDATE 0.311 183 57\nToefficientlycapturethecriticalelementsforthedialoguedoc- MONEY 0.182 55 10\nORDINAL 0.180 50 9\nument,weproposeakey-element-informedtuningforsLLMs. CARDINAL 0.172 145 25\nSpecifically, wefirstextracttwodistinctkeyelements, identi- TIME 0.143 112 16\nLOC 0.071 14 1\nfiedasnamedentitiesandtheconclusionsentence,usingsepa-\nratemodels. Then,weperforminstructiontuningonthesLLM\nto guide the model in focusing on those extracted elements\nwhilegeneratingthesummary. stractivemethodstoimprovesummaryquality[28,29].Instead\nof explicitly passing the selected sentence, we merely mark\n3.1. Key-elementextraction the sentence in the document when designing the instruction.\nSpecifically, we highlight the key sentence by adding a dis-\nEntityextraction. WeusetheNERmechanismforentityex- tincttokenbyencapsulatingitbetween<conclusion>and\ntraction. Toselectthenamedentitiesforextraction,wecalcu- </conclusion>tokens. Amoreconcentratedsummarycan\nlatetheratioofentitiesappearinginboththedialoguesandthe be generated by implicitly guiding the model to conclude the\nsummaries. Table1presentsthe proportionofnamedentities summaryusingthemarkedmainpoints.Figure1illustratesthe\nappearinginthedialoguethatalsoappearinthereferencesum- overallstructure.\nmary.Ifanamedentityappearsinthereferencesummarywitha\nhighfrequency,itindicatesthatsuchnamedentityshouldbein-\n3.2. Instructiontuning\ncludedinthesummary;therefore,weselectnamedentitiesthat\nappearedinmorethan30%ofthesummaries.Additionally,we Asapromptforfine-tuningthesLLM,weprovidetheinstruc-\nconduct experiments with a news dataset and, following [16], tion with a key-element-informed document and a reference\nwe use entities suitable for the news domain, such as person, summary (Figure 1). For the instruction, we describe a task\ndate,organization,andevent.\ndefinitionandexplainhowthekeyelementsareemphasizedin\nAfter extracting the entities suitable for each domain, we thefollowing sourcedocument. Addressingmissinginforma-\nemphasize each entity in the document by surrounding them tionorentitiescanpotentiallyleadtohallucinations[12];thus,\nwiththeemphasistokens, <and>. Unlikeapriorwork[16], wesoughttomitigatethistrade-offbyexplicitlydemandingac-\nourapproachsolelyemphasizestheentitieswithtokenswithout curategenerationindetailedinstructions.Inparticular,wecon-\nexplicitlylistingtheirmeaning. catenate the instruction (i), converted document (d′), and ref-\nConclusion extraction. Furthermore, to extract the key sen- erencesummary(s)toconstructtheprompt([i;d′;s]). Then,\ntencefromthedocument,weemployapre-trainedBERT-based wefine-tunethesLLMusing thedesignedprompt. Thiskey-\nextractivesummarizer[27]andselectthetop-1sentence. This element-informed tuning enables the model to focus more on\nis motivated by combining extractive summarization with ab- importantpointswithinthedocumentinthegenerationprocess.\nTable 2: Comparison between encoder-decoder-based models, LLaMA-2-7B, and GPT-3 in DialogSum and CNN/DM dataset.\nKEITSum referstotheresultswhenallentitiesareextracted,whileKEITSum indicatestheresultswhenonlytheentitywiththe\nall top−1\nhighestproportionisextracted.Finally,KEITSumrepresentstheoutcomeswhenentitieswitharatioofover30%areextracted.\nUniEval\nDataset Model ROUGE-1\nCoherence Consistency Fluency Relevance Overall\nBART[2] 0.928 0.914 0.913 0.846 0.900 0.418\nT-5[4] 0.948 0.939 0.920 0.870 0.919 0.414\nLLaMA2-7B(Fine-tuned) 0.959 0.939 0.935 0.912 0.936 0.440\nDialogSum KEITSumall(ours) 0.963 0.942 0.939 0.914 0.939 0.429\nKEITSumtop−1(ours) 0.962 0.941 0.941 0.915 0.940 0.429\nKEITSum(ours) 0.965 0.942 0.942 0.918 0.942 0.430\nGPT-3[17] 0.969 0.907 0.944 0.932 0.938 0.270\nPEGASUS[3] 0.944 0.935 0.829 0.697 0.851 0.412\nBART[2] 0.956 0.943 0.837 0.684 0.855 0.415\nT5[4] 0.968 0.959 0.838 0.767 0.883 0.426\nBRIO[5] 0.947 0.927 0.833 0.790 0.874 0.455\nCNN/DM\nLLaMA2-7B(Fine-tuned) 0.943 0.933 0.870 0.770 0.879 0.364\nKEITSum(ours) 0.954 0.923 0.930 0.805 0.903 0.343\nGPT-3[17] 0.964 0.909 0.949 0.905 0.932 0.399\nGPT-3+CoT[16] 0.948 0.870 0.948 0.910 0.919 0.464\nTable3:TheperformancevariationofKEITSumontheDialogSumaccordingtodialoguelength.Thetestsetwasdividedbasedonthe\naveragelengthofdialogues.\nSummaryLength UniEval\nModel #ofDialogues\nDocument Summary Coherence Consistency Fluency Relevance Overall\nLLaMA2-7Bshort\n882 87.6\n17.2 0.961 0.948 0.933 0.914 0.939\nKEITSumshort 20.0 0.964 0.948 0.939 0.916 0.942\nLLaMA2-7Blong\n618 201.3\n32.5 0.955 0.927 0.938 0.909 0.932\nKEITSumlong 35.6 0.965 0.934 0.945 0.920 0.941\n4. Experimentalsetup GPT-3[17],wegeneratedsummariesusingGPT-3.5-turbo\nforDialogSum,whileweusedsummariescreatedby[16]using\nDatasets. WeusetheDialogSumdataset[9],alarge-scaledi-\nthetext-davinci-002forCNN/DM.\nalogue summarization dataset. It comprises a 12.5K training\nEvaluation metrics. ROUGE scores, which fail to evaluate\nsetanda1.5Ktestset,eachaccompaniedbyahuman-written\nsummaries properly [20, 21, 22, 23, 11, 24, 25], suffer from\nsummarythatcapturesthemostsalientinformationandentities.\nanothersignificantdrawback:heavyrelianceonreferencesum-\nIt encompasses a broad spectrum of daily-life topics through\nmaries.Recentresearchhighlightedthatthequalityofreference\nface-to-face spoken dialogues with a diverse distribution of\nsummariesinabstractivesummarizationisoftensubpar[7,34].\nlengths. To demonstrate domain extensibility, we employ the\nThus, to measure the omission and hallucination of the\nCNN/Daily Mail (CNN/DM) dataset [10], a news article col-\nsummaries precisely, we employ UniEval [11] and hu-\nlection paired with multi-sentence human-written summaries.\nman evaluation for multi-dimensional evaluation, and Chat-\nIn contrast to the encoder-decoder model trained on the full\nGPT evaluation [35] to examine the presence of inconsisten-\ndataset,thesLLMsweretrainedononly10,000subsetsforeffi-\nciesinthesummaries. UniEvalisarecentlyproposedmulti-\nciencyinbothdatasets. Followingpreviousresearchthathigh-\ndimensional evaluation tool for natural language generation\nlightsthepoorqualityofreferencesummariesintheCNN/DM\n(NLG) tasks, which demonstrates the highest correlation with\n[7],weusetherecentlyreleasedelement-awaretestset[16]de-\nhumanevaluationamongopen-sourcemulti-dimensionalevalu-\nsignedtoaddressthedeficienciesoftheoriginaldataset.\nationmetrics.Whilenotoverlyrelyingonreferencesummaries,\nModels. To extract entity, we use the Flair1 [30], a well-\nitprovidesfourexplainableevaluationdimensions: coherence,\ndesigned NER framework. It was specifically pre-trained on\nconsistency,fluency,andrelevance.Togaugetheextentofhal-\nthe OntoNote5 [31] for NER tasks in various domains, such\nlucinationsinmodel-generatedsummaries,weusetherecently\nas conversational speech and broadcast. For key sentence ex-\nintroducedChatGPTEvaluation[35,36]. Finally,weconduct\ntraction, we use the BERT summarizer2 [27]. For the sLLM,\nthehumanevaluation.\nwe fine-tune the smallest LLaMA2 [32] of 7 billion parame-\nters,oneofthefamousopen-sourcesLLMs. Wefine-tuneboth\n5. Resultsanddiscussions\nLLaMA2 and KEITSum via LoRA [33], which facilitates ef-\nficienttrainingbymodifyingalimitedparametersubsetwhile\n5.1. Mainresults\noriginal ones are frozen; thus, it eliminates the need for full-\nmodelretraining. Wefine-tuneLLaMA2usingabasicprompt Multi-dimensionalevaluation. AsshowninTable2,ourap-\nformat commonly used for summarization tasks. We set rank proach demonstrated improvements across all UniEval di-\nr=8,dropout=0.05,alpha=32,andepoch=3asLoRAhyper- mensions in the DialogSum. Emphasizing only the high-\nparameter. Asourcomparativemodels,weuserobustencoder- relevancenamedentities,ratherthanhighlightingallnameden-\ndecodermodels,suchasBART[2],T5[4],PEGASUS[3]and tities as done in KEITSum all or the most frequent named en-\nBRIO[5]. Theywerefine-tunedontheentiretrainingset. For tityasdoneinKEITSum top−1,slightlybenefitedperformance\nenhancement. Notably, by ensuring the inclusion of essential\n1https://github.com/flairNLP/flair elementsinthesummaries,KEITSumboostedrelevancescore\n2https://pypi.org/project/bert-extractive-summarizer/ in both the DialogSum and CNN/DM. As a result, our model\n0.7\nBART\n0.6\nT5\n0.5\n0.4 LLaMA-7B\n0.3 Reference\n0.2 KEITSum\n0.1\n0 0.1 0.2 0.3 0.4 0.5 0.6\n0\nKEITSum LLaMA2-7B BRIO GPT-3 GPT-3+CoT\nFigure3:HallucinationratioperdialoginDialogSum.\nPERSON DATE EVENT ORG\nFigure 2: The proportion of entities included in the element-\naware dataset that are also included in the summaries gener- seeninTables2,thereisagreaterperformanceimprovementin\natedbyeachmodel. theCNN/DM,whichhasalongeraveragetextlengththanDi-\nalogSum. Foramoredetailedanalysis,wedividetheDialog-\nSumdatasetintolongandshortcategoriesbasedontheaverage\nTable4:HumanevaluationinDialogSum.\nlengthofthetext.AsshowninTable3,whiletherewasaslight\nperformanceimprovementwhenthesummarylengthwasshort,\nModel Comprehension Faithfulness Relevance Fluency Overall\nBART 4.192 3.440 3.652 4.368 3.910 ourmodelshowednotableperformanceimprovementwhenthe\nT5 4.130 3.322 3.527 4.288 3.838 summarylengthwaslong.\nReference 4.363 3.812 3.950 4.460 4.137\nLLaMA2-7B 4.413 3.927 4.040 4.458 4.183\nKEITSum 4.527 4.115 4.145 4.562 4.347 5.3. Humanevaluation\nGPT-3 4.880 4.740 4.772 4.876 4.802\nWeconductedahumanevaluationtoascertaintheperformance\nimprovement of our model compared to other summarization\nachieved higher overall scores not only compared to existing models(Table4). WehiredthreeEnglishteacherstoassess20\nencoder-decoder-basedsummarizationmodelsbutalsocompa- dialoguesviaUpwork3.Theevaluationcriteriaencompasscom-\nrabletothemuchlargermodel,GPT-3. prehension, faithfulness, relevance, fluency, and overall score\nCompared to the encoder-decoder models fine-tuned with basedonindividualpreference,ratedonascaleof0to5(high-\nthe full dataset in the CNN/DM dataset, our model performs est). KEITSumsurpassedLLaMA2-7Binfaithfulnessandrel-\nbetterinmostdimensionsdespiteusingonly3.6%ofthetrain evance,reflectingbetteralignmentwiththeoriginaldocument\nset. In detail, it shows lower and consistency scores while and inclusion of only crucial information. This improvement\nexhibiting markedly higher fluency scores. This could be at- stemsfromourfocusonkeyentitiesandsentences,ensuringno\ntributed to the difference in the generation procedure, i.e., importantdetailsaremissedinthesummaries.\nencoder-decoder models often generate content directly from\nthe source text, resulting in high consistency, whereas our 5.4. Measuringhallucinations\ndecoder-only approach leads to diverse yet more appropriate\nAsIncorporatingmissingentitiescanpotentiallyleadtohallu-\nsynonyms in the summaries. Even GPT-3 and GPT-3+CoT\ncinations[12],wequantifiedhowinconsistentinformationwas\nshow comparable or lower scores on these aspects than T5,\npresentinthegeneratedsummaries. Inspiredbythefollowing\ntherebysupportingourassumption.\nresearchfindingsthatChatGPTcanevaluateinamannersimi-\nAdditionally, we measured the ROUGE-1 scores for each lartohumans[37,35,38],weemployedChatGPTtogaugethe\nmodel. Table2showedthattheROUGE-1scoreofKEITSum extentofhallucinationinmodel-generatedsummariesof20di-\nslightlydecreasedcomparedtotheLLaMA2-7BmodelonDi- aloguesamples;here,hallucinationreferstoanyincorrectcon-\nalogSum. Moreover, the GPT-3 model, known for generating tent,includingmisattribution,misinterpretation,andredundant\nthehighestqualitysummaries,showedlowerROUGE-1scores content.Figure3illustratesthatourmodelproducessummaries\nthanothersummarizationmodels.Thisunderscoresonceagain withanaverageof60%fewerhallucinationsperdialoguethan\nthat ROUGE scores are insufficient to measure the quality of thosegeneratedbyLLaMA2-7B,evensurpassingthereference\nsummariesgeneratedbyLLMsandfailtocapturedimensions summariesinhallucinationreduction.\nsuchasrelevance.\nEntityratio. Toverifytheactualinclusionoftheemphasized 6. Conclusion\nentities in the generated summaries, we investigate the entity\nratiousingtheCNN/DMelement-awaretestset[16]. Weex- With the advent of GPT-3, LLM-utilized summarization has\ntractedtheentitiesinthereferencesummariesandthencalcu- achieved superior performance. However, large-scale propri-\nlated the ratio of these entities present in the summaries pro- etary LLMs are only accessible via APIs and are expensive,\nducedbyeachmodel. Figure2showsthatKEITSummeasured while the smaller public model, sLLMs, still struggles with\nsimilarly to the tendencies of GPT-3 or GPT-3+CoT, exhibit- entity omission in summarization and delivers inferior perfor-\ninganotableimprovementoverLLaMA2-7Bacrossallentities. mance. Weproposeakey-element-informedinstructiontuning\nRemarkably,theratioofEVENTentitiesshowsaconsiderable methodtoovercomethisissueinsLLMs. Byaddingemphasis\nincrease,wheretheLLaMA2-7Bnotablyfailedtocapturewell. tokens to essential elements and detailed instruction for sum-\nmarization,UniEvalscoresnoticeablyimprovedinrelevance,\n5.2. Lengthdependency exhibitingacomparableoverallscoreofGPT-3. Furthermore,\nboth 60% reduced hallucinations on ChatGPT evaluation and\nWhenthedocumentislonger,morefrequentmissinginforma- 4.8% improved faithfulness in human evaluation, proving the\ntion issues occur. Therefore, our method, emphasizing enti- efficacyofourmethod.\ntiesandkeysentencestoensureaccurateentitiesareincluded\nin the summary, is more effective in longer text. Indeed, as 3https://www.upwork.com/\n7. Acknowledgements\n[18] T.Scialometal.,“QuestEval:Summarizationasksforfact-based\nevaluation,”inProceedingsofthe2021ConferenceonEmpirical\nThis work was supported by the National Research Founda- MethodsinNaturalLanguageProcessing,2021.\ntion of Korea (NRF) grant funded by the Korea government [19] O. Honovich et al., “q2: Evaluating factual consistency in\n(MSIT) (No. RS-2023-00217286) and Institute of Informa- knowledge-groundeddialoguesviaquestiongenerationandques-\ntion & communications Technology Planning & Evaluation tionanswering,”inProceedingsofthe2021ConferenceonEm-\n(IITP)grantfundedbytheKoreagovernment(MSIT)(No.RS- piricalMethodsinNaturalLanguageProcessing,2021.\n2019-II191906, Artificial Intelligence Graduate School Pro- [20] D.Wanetal.,“Faithfulness-awaredecodingstrategiesforabstrac-\ngram(POSTECH)). tivesummarization,”inProceedingsofthe17thConferenceofthe\nEuropeanChapteroftheAssociationforComputationalLinguis-\ntics,May2023.\n8. References\n[21] P.Roitetal.,“Factuallyconsistentsummarizationviareinforce-\n[1] Vaswanietal., “Attentionisallyouneed,” Advancesinneural mentlearningwithtextualentailmentfeedback,”inProceedings\ninformationprocessingsystems,2017. ofthe61stAnnualMeetingoftheAssociationforComputational\nLinguistics(Volume1:LongPapers),2023.\n[2] M. Lewis et al., “BART: Denoising sequence-to-sequence pre-\ntrainingfornaturallanguagegeneration,translation,andcompre- [22] T. Goyal et al., “Evaluating factuality in generation with\nhension,”inProceedingsofthe58thAnnualMeetingoftheAsso- dependency-levelentailment,”inFindingsoftheAssociationfor\nciationforComputationalLinguistics,2020. ComputationalLinguistics:EMNLP2020. AssociationforCom-\nputationalLinguistics,2020.\n[3] Zhangetal.,“Pegasus:pre-trainingwithextractedgap-sentences\n[23] W.Kryscinskietal., “Evaluatingthefactualconsistencyofab-\nforabstractivesummarization,”inProceedingsofthe37thInter-\nstractivetextsummarization,” inProceedingsofthe2020Con-\nnationalConferenceonMachineLearning,2020.\nferenceonEmpiricalMethodsinNaturalLanguageProcessing\n[4] C. Raffel et al., “Exploring the limits of transfer learning with (EMNLP),2020.\naunifiedtext-to-texttransformer,”JournalofMachineLearning\n[24] T. Zhang* et al., “Bertscore: Evaluating text generation with\nResearch,2020.\nbert,”inInternationalConferenceonLearningRepresentations,\n[5] Y.Liuetal., “BRIO:Bringingordertoabstractivesummariza- 2020.\ntion,”inProceedingsofthe60thAnnualMeetingoftheAssocia- [25] Y.Liuetal.,“G-eval:NLGevaluationusinggpt-4withbetterhu-\ntionforComputationalLinguistics,2022. manalignment,”inProceedingsofthe2023ConferenceonEm-\n[6] T.Goyaletal.,“Newssummarizationandevaluationintheeraof piricalMethodsinNaturalLanguageProcessing,2023.\ngpt-3,”2023. [26] S.Ryuetal., “Multi-dimensionaloptimizationfortextsumma-\n[7] T.Zhangetal.,“Benchmarkinglargelanguagemodelsfornews rizationviareinforcementlearning,”inProceedingsofthe62nd\nsummarization,”2023.\nAnnualMeetingoftheAssociationforComputationalLinguistics,\n2024.\n[8] X.Pu,M.Gao,andX.Wan,“Summarizationis(almost)dead,”\n[27] D.Miller,“Leveragingbertforextractivetextsummarizationon\narXivpreprintarXiv:2309.09558,2023.\nlectures,”2019.\n[9] Y.Chenetal.,“DialogSum:Areal-lifescenariodialoguesumma-\n[28] Y.Maoetal.,“Constrainedabstractivesummarization: Preserv-\nrizationdataset,”inFindingsoftheAssociationforComputational\ningfactualconsistencywithconstrainedgeneration,”2021.\nLinguistics:ACL-IJCNLP2021,2021.\n[29] Y. Liu et al., “Text summarization with pretrained encoders,”\n[10] R. Nallapati et al., “Abstractive text summarization using in Proceedings of the 2019 Conference on Empirical Methods\nsequence-to-sequenceRNNsandbeyond,”inProceedingsofthe inNaturalLanguageProcessingandthe9thInternationalJoint\n20th SIGNLL Conference on Computational Natural Language ConferenceonNaturalLanguageProcessing,2019.\nLearning,2016.\n[30] A.Akbik,Bergmannetal.,“FLAIR:Aneasy-to-useframework\n[11] M.Zhongetal.,“Towardsaunifiedmulti-dimensionalevaluator for state-of-the-art NLP,” in Annual Conference of the North\nfortextgeneration,” inProceedingsofthe2022Conferenceon AmericanChapteroftheAssociationforComputationalLinguis-\nEmpiricalMethodsinNaturalLanguageProcessing,2022. tics(Demonstrations),2019.\n[12] Y.Zouetal.,“Towardsunderstandingomissionindialoguesum- [31] S. Pradhan et al., “Towards robust linguistic analysis using\nmarization,”inProceedingsofthe61stAnnualMeetingoftheAs- ontonotes,” in Proceedings of the Seventeenth Conference on\nsociationforComputationalLinguistics(Volume1:LongPapers), ComputationalNaturalLanguageLearning,2013.\n2023. [32] Touvronetal., “Llama2: Openfoundationandfine-tunedchat\n[13] X.Tangetal.,“CONFIT:Towardfaithfuldialoguesummarization models,”arXivpreprintarXiv:2307.09288,2023.\nwithlinguistically-informedcontrastivefine-tuning,”inProceed- [33] E.Huetal.,“Lora:Low-rankadaptationoflargelanguagemod-\ningsofthe2022ConferenceoftheNorthAmericanChapterof els,”arXivpreprintarXiv:2106.09685,2021.\ntheAssociationforComputationalLinguistics:HumanLanguage [34] G. Adams et al., “Learning to revise references for faithful\nTechnologies,Seattle,UnitedStates,2022. summarization,”inFindingsoftheAssociationforComputational\n[14] D. Deutsch and D. Roth, “Incorporating question answering- Linguistics: EMNLP 2022, 2022. [Online]. Available: https:\nbasedsignalsintoabstractivesummarizationviasalientspanse- //aclanthology.org/2022.findings-emnlp.296\nlection,”inProceedingsofthe17thConferenceoftheEuropean [35] C.-H.Chiangandoth, “Canlargelanguagemodelsbeanalter-\nChapteroftheAssociationforComputationalLinguistics,2023. nativetohumanevaluations?” inProceedingsofthe61stAnnual\nMeetingoftheAssociationforComputationalLinguistics(Volume\n[15] S.Berezinetal.,“Namedentityinclusioninabstractivetextsum-\nmarization,”inProceedingsoftheThirdWorkshoponScholarly\n1:LongPapers),2023.\nDocumentProcessing,2022. [36] C.Shenetal., “Arelargelanguagemodelsgoodevaluatorsfor\nabstractive summarization?” arXiv preprint arXiv:2305.13091,\n[16] Y. Wang et al., “Element-aware summarization with large lan-\n2023.\nguage models: Expert-aligned evaluation and chain-of-thought\nmethod,” inProceedingsofthe61stAnnualMeetingoftheAs- [37] M.Gao,J.Ruan,R.Sun,X.Yin,S.Yang,andX.Wan,“Human-\nsociationforComputationalLinguistics,2023. like summarization evaluation with chatgpt,” arXiv preprint\narXiv:2304.02554,2023.\n[17] T.Brownetal.,“Languagemodelsarefew-shotlearners,”inAd-\n[38] L.Duetal.,“Quantifyingandattributingthehallucinationoflarge\nvancesinNeuralInformationProcessingSystems. CurranAsso-\nlanguagemodelsviaassociationanalysis,”2023.\nciates,Inc.,2020.",
    "pdf_filename": "Key-Element-Informed_sLLM_Tuning_for_Document_Summarization.pdf"
}