{
    "title": "Towards Explainability and Fairness in Swiss Judgement Prediction - Benchmarking on a Multilingual Dataset",
    "context": "The assessment of explainability in Legal Judgement Prediction (LJP) systems is of paramount importance in building trustworthy and transparent systems, particularly considering the reliance of these systems on factors that may lack legal relevance or involve sensitive attributes. This study delves into the realm of explainability and fairness in LJP models, utilizing Swiss Judgement Prediction (SJP), the only available multilingual LJP dataset. We curate a comprehensive collection of rationales that ‘support’ and ‘oppose’ judgement from legal experts for 108 cases in German, French, and Italian. By employing an occlusion-based explainability approach, we evaluate the explainability performance of state-of-the-art monolingual and multilingual BERT-based LJP models, as well as models developed with techniques such as data augmentation and cross-lingual transfer, which demonstrated prediction performance improvement. Notably, our findings reveal that improved prediction performance does not necessarily correspond to enhanced explainability performance, underscoring the significance of evaluating models from an explainability perspective. Additionally, we introduce a novel evaluation framework, Lower Court Insertion (LCI), which allows us to quantify the influence of lower court information on model predictions, exposing current models’ biases. Keywords: Fairness, Explainability, Multilingual, Legal Judgement Prediction 1. The task of Legal Judgement Prediction involves analyzing the textual description of case facts to de- termine various aspects of a case’s outcome, such as the winning party, violated provisions, and mo- tion results. It has garnered substantial attention in the mainstream NLP community (Aletras et al., 2016; Chalkidis et al., 2019; Malik et al., 2021; Niklaus et al., 2021; Semo et al., 2022; Santosh et al., 2023a) and is being considered as a bench- marking task for evaluating the capabilities of legal NLP (Chalkidis et al., 2022b; Niklaus et al., 2023a) and long range models (Condevaux and Harispe, 2022; Niklaus and Giofré, 2022; Chalkidis et al., 2022a; Hua et al., 2022; Niklaus et al., 2023b). The process of resolving legal cases encom- passes evidential reasoning through exchange of arguments between the litigating parities before a decision-making body (Santosh et al., 2022). Earlier methods to deal with outcome predic- tion task such as IBP (Brüninghaus and Ash- ley, 2003), SMILE+IBP (Brüninghaus and Ash- ley, 2005), VJF (Grabmair, 2017) typically involved identification/extraction of the factors from the tex- tual description of the facts, then employing a con- ceptual schema to relate the factors to legal issues and predicts the outcome by comparing them with the past cases, thus providing the explanations for those predictions in terms that are legally intuitive. However, in the context of modern deep learning- based solutions, the outcome is determined solely from the text of the case facts, effectively bypass- ing the interpretable legal reasoning process. This poses a significant risk, particularly in high-stakes domains like law, when utilizing such systems that rely on factors that may be predictive but lack le- gal relevance or involve sensitive attributes (e.g., the race of an accused person). Such reliance can lead to unjust and biased outcomes, undermin- ing the principles of fairness and equal treatment within the legal system. Hence, such systems need to be analyzed from an explainability standpoint, thus making them transparent and thereby enhanc- ing the trust of legal practitioners and stakeholders to comprehend the factors and legal principles that contribute to a particular prediction. In the line of explainable LJP, Chalkidis et al. 2021 investigated the rationales behind models’ decisions in Legal Judgment Prediction (LJP) for European Court of Human Rights (ECtHR) cases. Subsequent studies by Santosh et al. 2022 ex- tended the above dataset and Malik et al. 2021 created new dataset for Indian Jurisdiction. In con- trast to these works in English, our study focuses on assessing the explainability of LJP models trained on the Swiss-Judgment-Prediction (SJP) dataset, which is the only available multilingual LJP dataset. It contains cases from the Federal Supreme Court of Switzerland (FSCS), written in three official Swiss languages (German, French, Italian)1. To this end, we curate a multilingual set of rationales that ‘support and ‘oppose’ Judgment 1The dataset consists of non-parallel cases, with each case being unique and decisions being written in a single language. arXiv:2402.17013v1  [cs.CL]  26 Feb 2024",
    "body": "Towards Explainability and Fairness in Swiss Judgement\nPrediction: Benchmarking on a Multilingual Dataset\nSantosh T.Y.S.S1, Nina Baumgartner2, Matthias Stürmer2,3,\nMatthias Grabmair1, Joel Niklaus2,3,4\n1Technical University of Munich, 2University of Bern,\n3Bern University of Applied Sciences, 4Stanford University\nAbstract\nThe assessment of explainability in Legal Judgement Prediction (LJP) systems is of paramount importance in\nbuilding trustworthy and transparent systems, particularly considering the reliance of these systems on factors that\nmay lack legal relevance or involve sensitive attributes. This study delves into the realm of explainability and fairness\nin LJP models, utilizing Swiss Judgement Prediction (SJP), the only available multilingual LJP dataset. We curate a\ncomprehensive collection of rationales that ‘support’ and ‘oppose’ judgement from legal experts for 108 cases in\nGerman, French, and Italian. By employing an occlusion-based explainability approach, we evaluate the explainability\nperformance of state-of-the-art monolingual and multilingual BERT-based LJP models, as well as models developed\nwith techniques such as data augmentation and cross-lingual transfer, which demonstrated prediction performance\nimprovement. Notably, our findings reveal that improved prediction performance does not necessarily correspond\nto enhanced explainability performance, underscoring the significance of evaluating models from an explainability\nperspective. Additionally, we introduce a novel evaluation framework, Lower Court Insertion (LCI), which allows us to\nquantify the influence of lower court information on model predictions, exposing current models’ biases.\nKeywords: Fairness, Explainability, Multilingual, Legal Judgement Prediction\n1.\nIntroduction\nThe task of Legal Judgement Prediction involves\nanalyzing the textual description of case facts to de-\ntermine various aspects of a case’s outcome, such\nas the winning party, violated provisions, and mo-\ntion results. It has garnered substantial attention\nin the mainstream NLP community (Aletras et al.,\n2016; Chalkidis et al., 2019; Malik et al., 2021;\nNiklaus et al., 2021; Semo et al., 2022; Santosh\net al., 2023a) and is being considered as a bench-\nmarking task for evaluating the capabilities of legal\nNLP (Chalkidis et al., 2022b; Niklaus et al., 2023a)\nand long range models (Condevaux and Harispe,\n2022; Niklaus and Giofré, 2022; Chalkidis et al.,\n2022a; Hua et al., 2022; Niklaus et al., 2023b).\nThe process of resolving legal cases encom-\npasses evidential reasoning through exchange of\narguments between the litigating parities before\na decision-making body (Santosh et al., 2022).\nEarlier methods to deal with outcome predic-\ntion task such as IBP (Brüninghaus and Ash-\nley, 2003), SMILE+IBP (Brüninghaus and Ash-\nley, 2005), VJF (Grabmair, 2017) typically involved\nidentification/extraction of the factors from the tex-\ntual description of the facts, then employing a con-\nceptual schema to relate the factors to legal issues\nand predicts the outcome by comparing them with\nthe past cases, thus providing the explanations for\nthose predictions in terms that are legally intuitive.\nHowever, in the context of modern deep learning-\nbased solutions, the outcome is determined solely\nfrom the text of the case facts, effectively bypass-\ning the interpretable legal reasoning process. This\nposes a significant risk, particularly in high-stakes\ndomains like law, when utilizing such systems that\nrely on factors that may be predictive but lack le-\ngal relevance or involve sensitive attributes (e.g.,\nthe race of an accused person). Such reliance\ncan lead to unjust and biased outcomes, undermin-\ning the principles of fairness and equal treatment\nwithin the legal system. Hence, such systems need\nto be analyzed from an explainability standpoint,\nthus making them transparent and thereby enhanc-\ning the trust of legal practitioners and stakeholders\nto comprehend the factors and legal principles that\ncontribute to a particular prediction.\nIn the line of explainable LJP, Chalkidis et al.\n2021 investigated the rationales behind models’\ndecisions in Legal Judgment Prediction (LJP) for\nEuropean Court of Human Rights (ECtHR) cases.\nSubsequent studies by Santosh et al. 2022 ex-\ntended the above dataset and Malik et al. 2021\ncreated new dataset for Indian Jurisdiction. In con-\ntrast to these works in English, our study focuses\non assessing the explainability of LJP models\ntrained on the Swiss-Judgment-Prediction (SJP)\ndataset, which is the only available multilingual\nLJP dataset. It contains cases from the Federal\nSupreme Court of Switzerland (FSCS), written in\nthree official Swiss languages (German, French,\nItalian)1. To this end, we curate a multilingual set\nof rationales that ‘support and ‘oppose’ Judgment\n1The dataset consists of non-parallel cases, with each\ncase being unique and decisions being written in a single\nlanguage.\narXiv:2402.17013v1  [cs.CL]  26 Feb 2024\n\non 108 cases in German, French and Italian collec-\ntively. We employ a perturbation-based explainabil-\nity approach, namely Occlusion (Zeiler and Fergus,\n2014), wherein we remove the factors from the fact\nstatements and measure the change in the predic-\ntion confidence in comparison to a non-occluded\nbaseline. This occlusion based method facilitates\nto identify the contribution of each factor in arriving\nat the final prediction, which also links to the char-\nacteristics of earlier factor based formal methods\nof LJP which are known for their interpretability.\nTo enable a fair comparison across methods, we\nrelease four distinct occlusion test sets2. Each\ntest set involves occluding a different number of\nsentences (1, 2, 3, and 4) per experiment. This\ncomprehensive range of occlusion scenarios al-\nlows us to assess the impact of varying levels of\nfactor removal on the prediction outcomes.\nUsing the occluded datasets, we assess the ex-\nplainability performance of state-of-the-art mod-\nels developed for SJP task using both monolin-\ngual (Niklaus et al., 2021) and multilingual BERT\n(Niklaus et al., 2022) architectures, as well as mod-\nels developed with techniques such as data aug-\nmentation and cross-lingual transfer (Niklaus et al.,\n2022). Our findings highlight the fact that the pre-\ndiction performance improvement does not trans-\nlate to explainability improvement.\nFurthermore we leverage the peculiar charac-\nteristics of the Federal Supreme Court of Switzer-\nland (FSCS), which handles only the most con-\ntentious cases that lower courts have struggled to\nresolve adequately. In their decisions, the FSCS\noften focuses on specific portions of previous de-\ncisions, scrutinizing potential flaws in the lower\ncourt’s reasoning. This setup offers an interesting\ntestbed to systematically assess the bias of the\nlower court in the final predictions generated by\nour models. This approach is reminiscent of re-\ncent works (Chalkidis et al., 2022c; Wang et al.,\n2021) that have examined the fairness of LJP mod-\nels by examining group fairness or disparate im-\npact i.e., performance disparities across various\nattributes, such as gender, age, and region. Our\napproach, termed Lower Court Insertion (LCI)3,\nadopts a counterfactual fairness perspective, un-\nlike prior studies examining performance dispar-\nities in LJP models. This involves extracting in-\nstances of the lower court in each case document\nand replacing them with other lower courts to mea-\nsure the resulting changes in prediction confidence\n2Our\nOcclusion\ndataset\nis\navailable\nat\nhttps://huggingface.co/datasets/rcds/\nocclusion_swiss_judgment_prediction\n3Our\nLCI\ndataset\nis\navailable\nat\nhttps:\n//huggingface.co/datasets/rcds/lower_\ncourt_insertion_swiss_judgment_\nprediction\nscores. Remarkably, despite the lower court’s av-\nerage length being only 7 words in documents with\nan average length of 350 words, it has shown the\npotential to flip the prediction label in some cases.\nIn sum, our main contributions are as follows:\n• We release a new dataset of 108 cases from\na trilingual Switzerland Judgment Prediction\ncorpus with rationales annotated by experts to\nassess the explainability of SJP models.\n• We evaluate the state-of-the-art models devel-\noped for the SJP task, including monolingual\nand multilingual models and models trained\nwith several techniques, from an explainability\nstandpoint using the occlusion technique.\n• We perform systematic evaluation of lower\ncourt bias embodied in these models using\nthe LCI technique, allowing us to quantify the\ninfluence of the lower court on the final predic-\ntions generated by the models.\n2.\nRelated Work\nLegal Judgement Prediction:\nLJP has been\nstudied under various jurisdictions such as the Eu-\nropean Court of Human Rights (ECtHR) (Chalkidis\net al., 2019; Aletras et al., 2016; Liu and Chen,\n2017; Medvedeva et al., 2018, 2021; Santosh et al.,\n2022, 2023b,a; Chalkidis et al., 2022b, 2021; Kaur\nand Bozic, 2019) Chinese Criminal Courts (Luo\net al., 2017; Yue et al., 2021; Zhong et al., 2020),\nUS Supreme Court (Katz et al., 2017; Kaufman\net al., 2019), Indian Supreme Court (Malik et al.,\n2021; Shaikh et al., 2020) the French Court of\nCassation ( ¸Sulea et al., 2017b,a), Brazilian courts\n(Lage-Freitas et al., 2022; Bertalan and Ruiz,\n2020), the Turkish Constitutional Court (Sert\net al., 2021; Mumcuo˘glu et al., 2021) UK courts\n(Strickson and De La Iglesia, 2020), German\ncourts (Waltl et al., 2017), the Philippine Supreme\nCourt (Virtucio et al., 2018), the Thailand Supreme\nCourt (Kowsrihawat et al., 2018) and the Federal\nSupreme Court of Switzerland (Niklaus et al.,\n2021, 2022; Rasiah et al., 2023) – the only publicly\navailable multi-lingual LJP corpus – which is the\nmain focus of this work.\nSwiss Judgement Prediction (SJP): Niklaus\net al. 2021 evaluate different methods for the LJP\ntask on the Swiss-Judgment-Prediction (SJP)\ndataset. They achieve the best performance using\na hierarchical variant of BERT that overcomes the\ntoken input limitation. Niklaus et al. 2022 further\nenhance the performance through cross-lingual\ntransfer learning, adapter-based fine-tuning and\ndata augmentation using machine translation. In\ncontrast to previous works, this study examines\n\nthe explainability of these models and investigates\nif improved prediction performance translates into\nimproved explainability performance.\nExplainability:\nExplanations\nin\nExplainable\nArtificial Intelligence (XAI) methods are classified\nbased on two factors: whether the explanation is\nfor an individual prediction or the overall prediction\nprocess (local or global), and whether the explana-\ntion is derived directly from the prediction process\nor\nrequires\npost-processing\n(self-explaining\nor post-hoc) (Danilevsky et al., 2020).\nThese\nmethods can be model-agnostic (LIME (Ribeiro\net al., 2016), SHAP (Lundberg and Lee, 2017),\nOcclusion (Li et al., 2016; Zeiler and Fergus, 2014),\nAnchors (Ribeiro et al., 2018)), applicable to any\nmodel, or model-specific (Integrated Gradients\n(Sundararajan et al., 2017), Gradient Saliency, and\nAttention-Based Methods), designed for specific\nmodels or architectures.\nIn this study, we use\nocclusion, a model-agnostic, local, and post-hoc\nexplainability technique.\nFairness: Fairness in machine learning has been\ndefined in different ways to address various types\nof discrimination. These definitions include group\nfairness, individual fairness, and causality-based\nfairness.\nGroup fairness ensures equitable\npredictions\nacross\ndemographic\nsubgroups,\navoiding differential treatment based on attributes\nsuch as race, gender, or age (Zafar et al., 2017;\nHardt et al., 2016). Individual fairness focuses\non treating similar individuals similarly, avoiding\narbitrary distinctions based on their characteristics\n(Sharifi-Malvajerdi et al., 2019; Yurochkin et al.).\nCausality-based fairness considers underlying\ncausal mechanisms and aims to identify and\nmitigate biases caused by confounding variables\nor indirect discrimination (Wu et al., 2019; Zhang\nand Bareinboim, 2018). In this study, we examine\nbias related to the lower court using counterfactual\nand causal fairness estimation methods.\nExplainability and Fairness in LJP: Early works\nin the field of legal judgment prediction, such as\nHYPO (Rissland and Ashley, 1987), CATO (Aleven\nand Ashley, 1997), IBP (Brüninghaus and Ashley,\n2003) and IBP+SMILE (Brüninghaus and Ashley,\n2005), relied on symbolic AI techniques to incorpo-\nrate domain knowledge and provide interpretable\nexplanations for the outcomes. However, deep\nlearning models in LJP have prioritized prediction\nperformance over explainability. Nevertheless, re-\ncent research emphasizes the significance of ex-\nplainability in the legal domain for trust and the\nright to explanation principle. Efforts have been\nmade to investigate explainability in LJP. For in-\nstance, Chalkidis et al. 2021 introduced the task\nof rationale extraction from facts statements and\nreleased a dataset from ECtHR. They used neu-\nral models with regularization constraints to select\nrationales using a learned binary mask. Addition-\nally, Santosh et al. 2022 identified distractor words\nhighly correlated with outcomes but not legally rel-\nevant, and proposed an adversarial deconfounding\nprocedure to align model explanations with those\nchosen by legal experts. Xu et al. 2023 analyzed\nthe token-level alignment with LJP models on the\nECtHR corpus. Similarly, Malik et al. 2021 devel-\noped a dataset of Indian jurisdiction for explainabil-\nity assessment using the occlusion method. In this\nwork, we curate a dataset from the trilingual Swiss\njurisdiction, and employ the occlusion method to\nevaluate models for Swiss Judgment Prediction.\nFair machine learning in the legal domain is a\nrelatively new field. Studies such as Angwin et al.\n2016 identified racial bias in the COMPAS system,\na parole risk assessment tool in the US, where\nblack individuals were more likely to be mislabeled\nas high risk. Another study by Wang et al. 2021\nfound significant fairness gaps across gender in\nLSTM-based models for legal judgment consis-\ntency using a dataset of Chinese criminal cases.\nRecently, Chalkidis et al. 2022c developed the Fair-\nLex benchmark to facilitate research on bias mitiga-\ntion algorithms in the legal domain. It includes four\ndatasets from different jurisdictions and languages,\ncovering various sensitive attributes. While previ-\nous works focused on group fairness and quanti-\nfying predictions across demographic subgroups,\nthis study examines a specific variable of lower\ncourt from a counterfactual perspective.\n3.\nOcclusion & LCI Dataset for SJP\nThe SJP dataset (Niklaus et al., 2021) comprises\n85,000 cases from the Federal Supreme Court of\nSwitzerland (FSCS) spanning the years 2000 to\n2020, chronologically split into training (2000-14),\nvalidation (2015-16) and test (2017-20) splits and\nare written in three languages: German, French,\nand Italian. However, it is important to note that\nthe dataset is not evenly distributed among these\nlanguages with Italian having a much smaller num-\nber of documents (4K) compared to German (50K)\nand French (31k). Additionally, this representation\ndisparity is also evident across various legal areas\nand regions. For more detailed dataset statistics,\nplease refer to the work of Niklaus et al. 2021.\n3.1.\nRationale and Lower Court\nAnnotation\nWe sample a total of 108 cases from both the vali-\ndation and test sets (2015-20). These cases were\nequally distributed across the three languages.\n\nWithin each year of the validation and test sets,\nwe sampled six cases per language, resulting in\ntwo cases per legal area. Specifically, each legal\narea in every year contained one case with the\njudgment \"approved\" and one with the judgment\n\"dismissed.\" It is worth noting that our annotation\ndataset is balanced in terms of final outcomes and\nlanguages, in contrast to the SJP dataset, which\ncontains a majority of dismissed cases (> 3/4). The\nannotations were conducted by a team of three\nlegal experts, consisting of two law students pursu-\ning their master’s degrees and one lawyer, over a\nperiod of five months. Two legal experts are native\nGerman speakers with intermediate knowledge in\nFrench and basic Italian skills. The third expert is\na native speaker in German and Italian and fluent\nin French. The annotation was facilitated using the\nProdigy tool 4.\nThe annotation task was to highlight sentences\nor sub-sentences in the facts section of the judg-\nment that \"support\" or \"oppose\" the final outcome\nof the case. We have chosen sub-sentences as\nthe atomic unit for annotation after consulting with\nlegal experts who expressed that a sentence can\ncontain two sub-sentences opposing each other\nand hence should be annotated with different la-\nbels. The annotators had been given access to\nthe entire case to make their annotation instead\nof just the facts section, which is the actual input\nfor the models dealing with judgment prediction\ntask. These decisions have been taken to address\ntwo points: (i) Experts opined that sentences/sub-\nsentences may have opposing labels depending on\nhow the court interpreted those facts in its reason-\ning; hence providing them the entire case would\ngreatly assist them in arriving at explanations lead-\ning to higher inter-annotator agreement (ii) Having\nprior knowledge about a specific case allows an\nexpert to be familiar with its specific legal and fac-\ntual details, as well as the court’s opinions on the\nmatter. As a result, varying levels of prior familiarity\nwith a case can lead to different interpretations and\nperspectives in understanding it. Hence providing\nthe entire case levels the playing field and elimi-\nnates the possibility that some cases are known\nto only some experts before, possibly leading to\ndifferent annotations.\nThe experts are instructed to read through the\nfacts, the considerations, the ruling, and any other\nneeded legal document (such as relevant legisla-\ntion, analyses or case law) to understand the court\ncase and then annotate the rationale. Unlike the\nprevious works involving explainability annotations\nin LJP (Chalkidis et al., 2021; Santosh et al., 2022;\nMalik et al., 2021) which only collect rationales that\nhelp to arrive at the final outcome of the case, we\nintroduce and collect rationales at fine-grained level\n4https://prodi.gy\ntermed as \"Supports Judgement\" and \"Opposes\nJudgment\" which holds significance especially in\nthe task of judgment prediction due to the inherent\nnature of legal text of often operating within the\nrealm of gray areas rather than clear-cut black-and-\nwhite distinctions. Legal cases involve complex\nissues, conflicting facts leading to alternative legal\nreasoning, dissenting opinions, alternative inter-\npretations of the law and can serve as potential\ngrounds for challenging the ruling and can serve\nas a reference point for legal arguments or con-\nsiderations. Thus, including the fine granularity of\nlabels provides an opportunity to assess more nu-\nanced understanding of the case by the models,\nacknowledging that legal decisions are not always\nunanimous and different perspectives may exist\nwithin the legal community.\nAdditionally, we request annotators to label neu-\ntral sentences. This is not a label per se, but covers\nsentences not assigned other labels, as this assists\nin implementing the occlusion method to partition\nthe facts section into more coherent sentences\nwith minimal effort, as segmenting legal text is a\ncomplex task in itself (Read et al., 2012; Savelka\net al., 2017; Brugger et al., 2023).\nIn addition to sentences and sub-sentences indi-\ncating towards outcome explanations, we also ask\nannotators to label the lower court mentions in the\nfact section as indicated in the rubrum (header in-\ncluding identifiers, and listing judges, lawyers and\ninvolved parties) of the ruling.\nThe annotation task was conducted in two cy-\ncles to ensure high quality. The initial cycle in-\nvolved pilot annotations, highlighting uncertainties\nregarding guidelines. As a result, we refined the\nguidelines by providing more precise instructions to\naddress these concerns5. Subsequently, a discus-\nsion among the legal experts was held to resolve\nany conflicts and consolidate the annotations in\nthe most effective manner, thereby ensuring the\nhigh quality of the annotations.\n3.2.\nInter Annotator Agreement\nWe obtained annotations from three annotators\nonly for the German subset. Detailed distribution\nof labeled tokens per annotator can be found in\nFig. 1. Among the three, Annotator 1 annotated\nthe least amount of tokens. Annotator 3 annotated\nthe most comparable to the Annotator 2, espe-\ncially when using the Supports Judgment label.\nTo measure inter-annotator agreement for expla-\nnations, we use the machine translation metrics\nas suggested by Malik et al. 2021 like ROUGE-\n1, ROUGE-2, ROUGE-L (Lin, 2004), BLEU (Pa-\npineni et al., 2002) (unigram and bigram averag-\n5Our detailed annotation guidelines and discussions\nare available here.\n\nFigure 1: Mean number of tokens annotated per\nlabel per annotator in German subset\ning), METEOR (Agarwal and Lavie, 2007), Jaccard\nSimilarity, Overlap Maximum, Overlap Minimum\nand BERTScore (Zhang et al., 2019). We report\nthe inter-annotator agreement scores in the Ger-\nman subset for the first round of annotations of\nthe German dataset in Table 1. These scores are\naggregated over all the labels (supports, opposes\njudgment and lower court). Table 1 demonstrates\nhigh agreement across all scores, with values rang-\ning from 0.7 to 0.9. The high BERTScore indicates\nstrong similarities in non-lexical matches, while\nthe indication of OVERLAP Minimum suggests\nthat the annotations frequently overlapped as sub-\nsequences. Notably, Experts 2 and 3 exhibit the\nhighest agreement, which can be attributed to their\nlarger number of annotated tokens compared to\nExpert 1 as observed in Fig. 1. We also notice\nthat the agreement within the categories \"Lower\nCourt\" and \"Supports Judgment\" is notably high\nin comparison to \"Opposes Judgment\". The ex-\nperts confirmed that the higher variance in the \"Op-\nposes Judgment\" label stemmed from the difficulty\nin identifying these sentences and resolving these\nconflicts constituted a significant effort in landing\nwith final annotations. Distribution of final number\nof tokens obtained per label across language is\nvisualised in Fig. 2.\n3.3.\nOcclusion and LCI dataset\nTo evaluate the explainability of models and enable\na fair comparison among them, we derive four dis-\ntinct occlusion based datasets from the test split6 of\nabove annotated rationales data, consisting of 27,\n24 and 23 cases in German, French and Italian re-\nspectively. For each occlusion test set, we occlude\na different number of sentences (1, 2, 3, and 4)\n6We exclude the instances from the validation split,\nwhich is used for hyperparameter tuning during model\ntraining, to derive the occluded test set for explainability.\nFigure 2: Distribution of the number of tokens per\nlabel in the final dataset across each language.\nIAA metric\nA1-A2\nA1-A3\nA2-A3\nRouge-1\n0.78\n0.69\n0.87\nRouge-2\n0.74\n0.64\n0.85\nRouge-L\n0.77\n0.68\n0.87\nBLEU\n0.75\n0.69\n0.85\nMETEOR\n0.77\n0.71\n0.88\nJaccard Sim.\n0.73\n0.64\n0.82\nOverlap Max.\n0.68\n0.61\n0.74\nOverlap Min.\n0.83\n0.73\n0.81\nBERTScore\n0.91\n0.86\n0.93\nTable 1: IAA score between the annotators in the\nfirst cycle for German subset\nbelonging to same label (Supports/Opposes Judg-\nment/Neutral) per experiment in a case, adding no\nmarker or trace of the occlusion in the fact section\nto leave it as similar and natural as possible. For\nevery occlusion test instance, we also pair it with\na baseline with no text occluded. Thus, we arrive\nin a total of 28k occluded instances with varying\nlevels of occlusion, across three languages. Using\nthese occluded instances, we analyze the differ-\nence in prediction confidence in comparison to the\nnon-occluded baseline.\nFor LCI, we derive the counterfactual based test\nset wherein we use the lower court instances an-\nnotated by the annotator and replace the lower\ncourt instances with other lower court names in\nevery case resulting in a total of 1127 instances.\nThere are a total of 13, 9, and 16 unique lower\ncourt instances in German, French, and Italian re-\nspectively. Similar to above, each instance is also\npaired with a baseline representing the case text\nwith the actual lower court name without any re-\nplacement, which we use to analyze the change in\nprediction confidence.\nTables 2 and 3 provide statistics on the total num-\nber of instances in both the Occlusion and LCI test\nsets along with detailed breakdown of the number\n\nDE\nFR\nIT\n#Documents\n27\n24\n23\nSet 1\nOpposes\n55\n34\n31\nNeutral\n247\n164\n195\nSupports\n98\n85\n50\nSet 2\nOpposes\n66\n22\n23\nNeutral\n1097\n586\n827\nSupports\n203\n246\n69\nSet 3\nOpposes\n53\n7\n8\nNeutral\n3158\n1260\n2429\nSupports\n356\n659\n56\nSet 4\nOpposes\n27\n1\n1\nNeutral\n6622\n1801\n5704\nSupports\n586\n1477\n28\nTable 2: Split of number of instances per label in\neach occluded test across three languages.\nOcclusion\nLCI\nOpp.\nNeu.\nSup.\nTotal\nTotal\nDE\n201\n11325\n1243\n12769\n351\nFR\n64\n3875\n2467\n6406\n391\nIT\n63\n9218\n203\n9484\n312\nTable 3: Total number of instances per label across\nthree languages for occlusion and LCI test. Opp.,\nNeu., Sup. represent ‘Opposes Judgement’, ‘Neu-\ntral’ and ‘Supports Judgement’ respectively.\nof instances in each occlusion set by language.\nAcross languages, the German subset comprises\nthe largest portion of the test set compared to the\nrest. This is due to annotation of fewer sentences\nin Italian and French documents as can be noticed\nin Fig. 2. Among the labels, the ‘Opposes Judg-\nment’ label has fewer instances, due to the lower\nnumber of annotated tokens associated with this\nlabel.\n4.\nExperimental Setup\n4.1.\nModels\nWe assess the following six classes of models,\ndeveloped on the backbone of hierarchical BERT,\ndeveloped for the SJP task in previous literature\n(Niklaus et al., 2021, 2022). We follow the same\ndataset splits provided by Niklaus et al. 2021\nfor training and validation.\nHierarchical BERT\nis employed because the SJP dataset includes\ndocuments with more than 512 tokens.\nIn this\napproach, the text is split into 4 consecutive\nblocks of 512 tokens (90% of cases are less than\n2048 tokens) and fed into a shared standard\nBERT encoder independently.\nThen the CLS\ntoken of each block is passed through a 2-layer\ntransformer encoder to aggregate the information\nacross blocks, followed by max-pooling and a final\nclassification layer.\nMonoLingual: This variant uses monolingually\npre-trained BERT models i.e German-BERT (Chan\net al., 2019), CamemBERT (Martin et al., 2020)\nand UmBERTo (Parisi et al., 2020) for German,\nFrench and Italian. Each model is fine-tuned and\nevaluated using that language subset dataset.\nMultiLingual: This variant uses the multilingually\npre-trained XLM-R model (Conneau et al., 2019)\ninstead of language-specific pre-trained BERT.\nHowever, the fine-tuning process is still performed\nseparately for each language, similar to the\nmonolingual approach.\nMono/Multi Lingual with Data Augmentation:\nWe translate the cases in SJP dataset into other\nlanguages from the original language using the\nEasyNMT2 framework, following the approach\nproposed by Niklaus et al. 2022.\nThen these\ntranslated instances are then augmented with\nthe original data for a specific language during\nthe fine-tuning process with Mono/Multilingual\nBERT. This is similar to above experiment in setup,\nwith the main distinction being the additional\naugmented data.\nJoint Training without/with Data Augmentation:\nWe use a multilingual pre-trained model and fine-\ntune it across all the three language corpora jointly,\nwhich tries to capitalize on the inherited benefit\nof using larger multilingual corpora during fine-\ntuning. As discussed above, we translate each\ndocument from its original language to the rest\nof other two and train a data augmented version\nof models jointly with all the obtained translated\ndata. Unlike the previous approaches where sep-\narate models were fine-tuned for each language,\nthis method jointly fine-tunes on all languages, re-\nsulting in a single final model instead of multiple\nmodels for each language.\n4.2.\nImplementation Details\nWe use the code repositories from prior work\n(Niklaus et al., 2021, 2022) to assess the state-\nof-the-art models on SJP7. We employ a learning\nrate of 1e-5 with early stopping based on macro-\nF1 on the development set. All models are trained\nwith a batch size of 64 for 10 epochs using AdamW\noptimizer with mixed precision and gradient ac-\n7https://github.com/JoelNiklaus/\nSwissJudgementPrediction\n\nModel\nGerman\nFrench\nItalian\nMonoLingual\n69.08\n71.78\n67.82\nMultiLingual\n67.92\n69.24\n65.28\nMonoLingual + DA\n70.47\n71.24\n69.21\nMultiLingual + DA\n68.94\n71.06\n69.86\nJoint Training\n68.74\n70.82\n70.62\nJoint Training + DA\n70.58\n71.62\n71.18\nTable 4: Prediction Performance on Test set of\nNiklaus et al. 2021\ncumulation using huggingface library (Wolf et al.,\n2020). We use oversampling to handle class imbal-\nance. We use 4 segments with 512 tokens each\nin our hierarchical models resulting in a maximum\nsequence length of 2048.\n4.3.\nMetrics\nWe report macro-F1 following Niklaus et al. 2021,\n2022 for assessing prediction performance. For\nassessing explainability through occlusion exper-\niments, we calculate the explainability score Sexp\nfor every test instance as the difference between\nthe temperature-scaled8 confidence of the base-\nline and the occluded instance. (i.e., baseline -\noccluded). A negative (positive) Sexp score indi-\ncates that occluded text is opposing (supporting)\nits prediction. Then, we assign the label ‘Opposes\nJudgement’/‘Neutral’/’Supports Judgement’ based\non the sign of explainability score. Finally, we re-\nport F1-score for each of the labels across all the\noccluded instances.\nIn bias estimation using the LCI method, we cal-\nculate an explainability score for each instance.\nAs the explainability scores are sign dependent,\nwe separately compute the Mean of Explainability\nScores (MES), for positive and negative values,\nexpressed as a percentage. A positive explainabil-\nity score indicates that the insertion of the lower\ncourt decreases the probability, suggesting that\nthe inserted court has a pro-dismissal influence.\nConversely, a negative score indicates an increase\nin the probability, indicating a pro-approval trend of\nthe inserted lower court. For an ideally unbiased\nmodel, the presence of the lower court should not\naffect the probability of the prediction. Therefore, a\nvalue of the mean explainability score closer to 0 is\ndesirable. Additionally, we report the percentage of\ncases where the insertion of the lower court leads\nto a flip in the label of the prediction, changing it\nfrom 0 to 1 or vice versa.\n8We adopt temperature scaling (Guo et al., 2017) to\ncalibrate the confidence estimates of the model.\n4.4.\nAnalysis using Occlusion Test\nWe present the results of prediction performance\nand the explainability analysis using occlusion in\nTables 4 and 5, respectively. Analyzing Table 5, we\nobserve that the model achieves higher accuracy\nin classifying instances with Supports Judgment\ncompared to those with Neutral or Opposes Judg-\nment. This could be attributed to the fact that the\nOpposes Judgment category is underrepresented\nin the occlusion dataset (due to fewer annotated\ntokens with this label) and the challenging task to\nclassify Neutral instances. Among the three lan-\nguages, French exhibits the highest score for the\nSupports Judgment category, but it also shows\nlower scores for the other classes.\nDespite the MultiLingual model displaying a de-\ncrease in predictive performance, it shows some\nimprovement in occlusion performance, particularly\nfor the Supports Judgments class, across all lan-\nguages. A similar trend is observed in the Joint\ntraining model, which consistently demonstrates a\nsignificant increase in explainability scores across\nlanguages for most classes.\nWhile the inclusion of the DA component in both\nMonoLingual and MultiLingual models resulted in\nimproved explainability scores for most labels com-\npared to their counterparts, its addition to the Joint\ntraining model leads to mixed results. Surprisingly,\nthe addition of the DA component to the Joint train-\ning model consistently increases prediction perfor-\nmance but does not maintain consistency in ex-\nplainability performance. This finding emphasizes\nthe importance of evaluating explainability to de-\nvelop transparent systems that can make accurate\npredictions for the right reasons.\nOverall, the lower scores across the board indi-\ncate the flawed inference about factors predictive\nfor the outcome. Despite the impressive perfor-\nmances of state-of-the-art models on standard LJP\nprediction performance, there is still much progress\nto be made to make those models align as closely\nas possible with the rationales deemed relevant by\nlegal experts. To create practical value for the legal\nfield, the field of LJP should aim for a productive fu-\nsion of expert knowledge and data-driven insights,\nrather than data-driven correlation based learning.\n4.5.\nAnalysis using LCI Test\nFrom Table 6, we can observe that the modification\nof the lower court has a considerable influence\non the overall prediction confidence, as indicated\nby the changes in confidence scores up to 5% in\nboth directions across all languages, despite the\nlower court name on average spanning around\nseven tokens in documents of an average length\nof 350 tokens. However, these small changes in\nconfidence scores did result in label flips.\n\nModel\nGerman\nFrench\nItalian\nOpposes\nNeutral\nSupports\nOpposes\nNeutral\nSupports\nOpposes\nNeutral\nSupports\nMonoLingual\n3.02\n16.78\n15.10\n1.95\n3.68\n40.24\n0.49\n3.68\n11.24\nMultiLingual\n2.04\n11.90\n17.46\n1.77\n3.62\n42.77\n0.85\n5.72\n13.48\nMonoLingual + DA\n3.21\n16.26\n18.08\n1.78\n5.98\n43.12\n0.98\n4.39\n14.99\nMultiLingual + DA\n3.64\n19.06\n20.83\n1.43\n4.63\n45.77\n0.83\n4.84\n15.36\nJoint Training\n2.62\n15.72\n26.97\n1.67\n4.19\n48.51\n0.54\n5.37\n18.82\nJoint Training + DA\n3.75\n14.54\n21.95\n1.93\n5.73\n45.73\n0.63\n4.82\n19.68\nTable 5: Analysis of Explainability using Occlusion - F1-scores across all instances from all four test sets\nfor each label in every language. Higher the scores, better the explainability.\nModel\nGerman\nFrench\nItalian\n+\nMES\n-\nMES\nFlip\n1 →0\nFlip\n0 →1\n+\nMES\n-\nMES\nFlip\n1 →0\nFlip\n0 →1\n+\nMES\n-\nMES\nFlip\n1 →0\nFlip\n0 →1\nMonoLingual\n3.485.12\n−2.32.82\n2.28\n0.43\n2.562.65\n−2.23.79\n3.84\n1.02\n1.643.46\n−2.227.76\n0.12\n1.20\nMultiLingual\n3.395.43\n−2.773.64\n1.71\n0.32\n4.014.62\n−3.063.22\n0.51\n0.12\n1.722.18\n−1.782.52\n1.05\n2.88\nMonoLingual + DA\n3.095.15\n−2.775.42\n2.56\n0.22\n4.126.73\n−1.832.34\n1.24\n0.82\n1.252.23\n−1.291.97\n0.32\n2.19\nMultiLingual + DA\n5.328.27\n−3.355.24\n4.56\n2.56\n4.086.05\n−6.489.64\n1.53\n3.07\n2.883.51\n−6.128.08\n2.56\n3.85\nJoint Training\n3.326.18\n−1.862.89\n3.13\n1.99\n4.084.37\n−2.713.69\n0.51\n2.56\n6.136.79\n−2.662.12\n4.92\n4.24\nJoint Training + DA\n3.234.46\n−1.842.45\n2.85\n1.99\n3.043.96\n−4.044.31\n3.07\n2.32\n6.147.94\n−3.213.11\n4.09\n4.83\nTable 6: Analysis of Lower Court Bias using LCI - Results of Positive and Negative MES Scores, and\nLabel Flips across the three languages. Labels 0 and 1 indicates dismissal and approval respectively.\nLower scores indicate a less biased model. Subscript indicates the standard deviation values.\nOverall, no consensus exists on which model\nsetting has yielded lower MES scores in both direc-\ntions consistently across all the languages. Multi-\nLingual model’s prediction performance decreased\ncompared to the MonoLingual model across all\nthree languages and its bias scores increased sig-\nnificantly across all languages, barring -MES for\nItalian and +MES for German. While the inclu-\nsion of the DA (Data Augmentation) component\nresulted in improved prediction performance com-\npared to the non-DA variants in both MonoLingual\nand MultiLingual settings, the Multilingual + DA\nmodel exhibited a notable increase in bias. This\nsuggests that the model’s reliance on lower court\nnames is more pronounced in the presence of the\nDA component compared to its non-DA variant.\nJoint training models, which aim to generalize\nacross languages, demonstrate improved predic-\ntion performance, particularly for Italian, which is\nunderrepresented in the training set.\nHowever,\nthis improvement comes at the cost of higher\nMES scores, indicating potential overfitting to court-\nspecific correlations rather than capturing the ac-\ntual reasoning behind the predictions. Interestingly,\ndespite training a single model using data from\nall three languages, the Italian data shows a sig-\nnificant divergence in MES scores compared to\nGerman and French. This highlights that repre-\nsentational bias across languages seems to be a\ncrucial part. While adding DA to Joint training mod-\nels significantly improves prediction performance,\nbias scores lack a clear pattern.\nAcross all the models in the German setting,\nwe can witness an overall pro-dismissal trend\n(greater + MES scores compared to - MES) echo-\ning with more number of label flips from approval\nto dismissal. While French notices an overall pro-\ndismissal trend, Italian shows pro-approval trend,\nbarring the cross-lingual models. These observed\nbiases regarding lower court underscore the need\nfor continuous bias evaluation and mitigation in LJP\nmodels.\n5.\nConclusion\nIn this work, we present the rationale dataset cu-\nrated at fine-grained level of both ‘supporting’ and\n’opposing’ factors for Swiss Judgment Prediction\n(SJP), the only available multilingual LJP dataset.\nWe employ a perturbation-based occlusion ap-\nproach to assess various state-of-the-art models\ndeveloped for SJP and also release four distinct\nocclusion test sets, occluding a different number of\nsentences in each of the sets. Our lower explain-\nability scores suggest that the current models do\nnot align well with the legal experts which can lead\nto sub-optimal litigation strategies due to flawed in-\nference about factors responsible for the outcome.\nFurthermore, we assess the bias of the lower court\ninformation in the final predictions generated by the\nmodels using LCI test and notice that models learn\nspurious correlations about court-outcome in the\n\ndata. In the future, we would explore deconfound-\ning strategy (Santosh et al., 2022) to improve the\nalignment between what models and experts deem\nrelevant. One can explore different group robust al-\ngorithms such as adversarial removal, IRM, Group\nDRO and V-REx, as an effective bias-mitigation\nstrategy (Chalkidis et al., 2022c) and investigate\nits impact on explainability. We hope our data re-\nsource will be useful to the research community\nworking on Legal Judgement Prediction.\n6.\nLimitations\nIn this study, our approach to obtaining rationales\ninvolved a consolidation process wherein we aimed\nto achieve a final set of high-quality annotations\nthrough discussions with legal experts. However, it\nis important to acknowledge that the assumption\nof a single ground truth may overlook the presence\nof genuine human variation, which can arise due\nto factors such as disagreement, subjectivity in\nannotation, or the existence of multiple plausible\nanswers (Xu et al., 2023). Particularly in the field\nof law, where complexity and interpretation are in-\nherent, it is well-recognized that lawyers may have\ndiffering legal assessments of case facts and how\nthey contribute to the eventual outcome. Instead\nof attempting to resolve variations in expert labels,\nit is essential to acknowledge and embrace the\ninherent variation in human annotations. Moving\nforward, it is crucial to develop methods that can\ncomprehensively capture and account for variation\nfrom data to evaluation, enabling a more compre-\nhensive treatment of this variability in future re-\nsearch.\nIn the evaluation of our occlusion-based explain-\nability setup, we utilized the F1-score, which fo-\ncuses solely on the final label obtained from the\nchange in confidence score between the baseline\nand occluded instances. However, it is important to\nemphasize the need for a metric that takes into ac-\ncount the magnitude of the difference in confidence\nscores during aggregation, in order to present a\nmore comprehensive and holistic assessment.\n7.\nEthics Statement\nThe dataset used in this work comes from prior\nwork by Niklaus et al. 2021 and these are publicly\navailable on the https://entscheidsuche.ch\nplatform and the names of the parties have been\nredacted by the court to ensure anonymity.\nThis work does not endorse or advocate for\npractical use of such systems. Instead our aim\nin this work is to rather empirically demonstrate\nthat these systems are far from practical use due\nto their flawed inference about factors leading to\noutcome prediction. The scope of this work is to\nstudy LJP from an explainability standpoint and\nto showcase the discrepancy between the predic-\ntion performance and explainability performance\nand emphasize the need to build technology that\ncan help practitioners with reliable insights. Our\ndataset and findings associated with this work will\ncontribute to advancing the field of explainable le-\ngal judgement prediction and provide valuable in-\nsights for developing more reliable and unbiased\nmodels in the future.\nFurthermore, we would like to draw attention\nto the work by Tsarapatsanis and Aletras 2021\nwhich discusses various normative factors related\nto ethics in the context of legal natural language\nprocessing. These discussions are crucial for fos-\ntering ethical thinking within the legal NLP commu-\nnity and ensuring the responsible development of\nsystems that can assist lawyers, judges, and the\ngeneral public.\nAcknowledgements\nThanks to Ilias Chalkidis for providing feedback on\ninitial ideas and experiments and to Lynn Grau,\nThomas Lüthi & Angela Stefanelli for providing ex-\npert legal annotations.\n8.\nBibliographical References\nAbhaya Agarwal and Alon Lavie. 2007. Meteor:\nAn automatic metric for mt evaluation with high\nlevels of correlation with human judgments. Pro-\nceedings of WMT-08.\nNikolaos Aletras, Dimitrios Tsarapatsanis, Daniel\nPreo¸tiuc-Pietro, and Vasileios Lampos. 2016.\nPredicting judicial decisions of the european\ncourt of human rights: A natural language pro-\ncessing perspective. PeerJ Computer Science,\n2:e93.\nVincent Aleven and Kevin D Ashley. 1997. Teach-\ning case-based argumentation through a model\nand examples: Empirical evaluation of an intel-\nligent learning environment. In Artificial intelli-\ngence in education, volume 39, pages 87–94.\nCiteseer.\nJulia Angwin, Jeff Larson, Surya Mattu, and Lauren\nKirchner. 2016. Machine bias: There’s software\nused across the country to predict future crimi-\nnals. And it’s biased against blacks. ProPublica,\n23:77–91.\nVithor Gomes Ferreira Bertalan and Evandro Ed-\nuardo Seron Ruiz. 2020. Predicting judicial out-\ncomes in the brazilian legal system using textual\n\nfeatures. In DHandNLP@ PROPOR, pages 22–\n32.\nTobias Brugger,\nMatthias Stürmer,\nand Joel\nNiklaus. 2023. MultiLegalSBD: A Multilingual\nLegal Sentence Boundary Detection Dataset.\nArXiv:2305.01211 [cs].\nStefanie Brüninghaus and Kevin D Ashley. 2003.\nCombining case-based and model-based rea-\nsoning for predicting the outcome of legal cases.\nIn Case-Based Reasoning Research and Devel-\nopment: 5th International Conference on Case-\nBased Reasoning, ICCBR 2003 Trondheim, Nor-\nway, June 23–26, 2003 Proceedings 5, pages\n65–79. Springer.\nStefanie Brüninghaus and Kevin D Ashley. 2005.\nGenerating legal arguments and predictions\nfrom case texts. In Proceedings of ICAIL 2005,\npages 65–74.\nIlias Chalkidis, Ion Androutsopoulos, and Nikolaos\nAletras. 2019. Neural legal judgment prediction\nin english. In Proceedings of ACL 2019, pages\n4317–4323.\nIlias Chalkidis, Xiang Dai, Manos Fergadiotis,\nProdromos Malakasiotis, and Desmond Elliott.\n2022a. An exploration of hierarchical attention\ntransformers for efficient long document classifi-\ncation. arXiv preprint arXiv:2210.05529.\nIlias Chalkidis, Manos Fergadiotis, Dimitrios Tsara-\npatsanis, Nikolaos Aletras, Ion Androutsopoulos,\nand Prodromos Malakasiotis. 2021. Paragraph-\nlevel rationale extraction through regularization:\nA case study on european court of human rights\ncases. In Proceedings of the NAACL-HLT 2021,\npages 226–241.\nIlias Chalkidis, Abhik Jana, Dirk Hartung, Michael\nBommarito, Ion Androutsopoulos, Daniel Katz,\nand Nikolaos Aletras. 2022b. Lexglue: A bench-\nmark dataset for legal language understanding\nin english. In Proceedings of ACL 2022, pages\n4310–4330.\nIlias Chalkidis, Tommaso Pasini, Sheng Zhang,\nLetizia Tomada, Sebastian Schwemer, and An-\nders Søgaard. 2022c. Fairlex: A multilingual\nbenchmark for evaluating fairness in legal text\nprocessing. In Proceedings of the 60th Annual\nMeeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages\n4389–4406.\nBranden Chan, Timo Möller, Malte Pietsch, Tanay\nSoni, and Chin Man Yeung. 2019. Deepset-open\nsourcing german bert.\nCharles Condevaux and Sébastien Harispe. 2022.\nLsg attention: Extrapolation of pretrained trans-\nformers to long sequences.\narXiv preprint\narXiv:2210.15497.\nAlexis Conneau, Kartikay Khandelwal, Naman\nGoyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzmán, Edouard Grave, Myle Ott,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nUnsupervised cross-lingual representation learn-\ning at scale. arXiv preprint arXiv:1911.02116.\nMarina Danilevsky, Kun Qian, Ranit Aharonov, Yan-\nnis Katsis, Ban Kawas, and Prithviraj Sen. 2020.\nA survey of the state of explainable ai for natural\nlanguage processing. In Proceedings of the 1st\nConference of the Asia-Pacific Chapter of the As-\nsociation for Computational Linguistics and the\n10th International Joint Conference on Natural\nLanguage Processing, pages 447–459.\nMatthias Grabmair. 2017. Predicting trade secret\ncase outcomes using argument schemes and\nlearned quantitative value effect tradeoffs. In\nProceedings of the 16th edition of the Interna-\ntional Conference on Articial Intelligence and\nLaw, pages 89–98.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q\nWeinberger. 2017.\nOn calibration of modern\nneural networks. In International conference on\nmachine learning, pages 1321–1330. PMLR.\nMoritz Hardt, Eric Price, and Nati Srebro. 2016.\nEquality of opportunity in supervised learning.\nAdvances in neural information processing sys-\ntems, 29.\nWenyue Hua, Yuchen Zhang, Zhe Chen, Josie\nLi, and Melanie Weber. 2022.\nLegalrelec-\ntra: Mixed-domain language modeling for long-\nrange legal text comprehension. arXiv preprint\narXiv:2212.08204.\nDaniel Martin Katz, Michael J Bommarito, and Josh\nBlackman. 2017. A general approach for predict-\ning the behavior of the supreme court of the\nunited states. PloS one, 12(4):e0174698.\nAaron Russell Kaufman, Peter Kraft, and Maya\nSen. 2019. Improving supreme court forecasting\nusing boosted decision trees. Political Analysis,\n27(3):381–387.\nArshdeep Kaur and Bojan Bozic. 2019. Convolu-\ntional neural network-based automatic prediction\nof judgments of the european court of human\nrights. In AICS, pages 458–469.\nKankawin Kowsrihawat, Peerapon Vateekul, and\nPrachya Boonkwan. 2018. Predicting judicial\ndecisions of criminal cases from thai supreme\n\ncourt using bi-directional gru with attention mech-\nanism. In 2018 5th Asian Conference on De-\nfense Technology (ACDT), pages 50–55. IEEE.\nAndré Lage-Freitas, Héctor Allende-Cid, Orivaldo\nSantana, and Lívia Oliveira-Lage. 2022. Predict-\ning brazilian court decisions. PeerJ Computer\nScience, 8:e904.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un-\nderstanding neural networks through represen-\ntation erasure. arXiv preprint arXiv:1612.08220.\nChin-Yew Lin. 2004. Rouge: A package for auto-\nmatic evaluation of summaries. In Text summa-\nrization branches out, pages 74–81.\nZhenyu Liu and Huanhuan Chen. 2017. A predic-\ntive performance comparison of machine learn-\ning models for judicial cases.\nIn 2017 IEEE\nSymposium series on computational intelligence\n(SSCI), pages 1–6. IEEE.\nScott M Lundberg and Su-In Lee. 2017. A uni-\nfied approach to interpreting model predictions.\nAdvances in neural information processing sys-\ntems, 30.\nBingfeng Luo, Yansong Feng, Jianbo Xu, Xiang\nZhang, and Dongyan Zhao. 2017.\nLearning\nto predict charges for criminal cases with legal\nbasis. In Proceedings of EMNLP 2017, pages\n2727–2736.\nVijit Malik, Rishabh Sanjay, Shubham Kumar\nNigam, Kripabandhu Ghosh, Shouvik Kumar\nGuha, Arnab Bhattacharya, and Ashutosh Modi.\n2021.\nIldc for cjpe: Indian legal documents\ncorpus for court judgment prediction and expla-\nnation. In Proceedings of ACL-IJCNLP 2021,\npages 4046–4062.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary,\nÉric Villemonte de La Clergerie, Djamé Seddah,\nand Benoît Sagot. 2020. Camembert: a tasty\nfrench language model. In ACL 2020-58th An-\nnual Meeting of the Association for Computa-\ntional Linguistics.\nMasha Medvedeva, Ahmet Üstün, Xiao Xu, Michel\nVols, and Martijn Wieling. 2021.\nAutomatic\njudgement forecasting for pending applications\nof the european court of human rights.\nIn\nASAIL/LegalAIIA@ ICAIL.\nMasha Medvedeva, Michel Vols, and Martijn Wiel-\ning. 2018. Judicial decisions of the european\ncourt of human rights: Looking into the crystal\nball. In Proceedings of the conference on empir-\nical legal studies, page 24.\nEmre Mumcuo˘glu, Ceyhun E Öztürk, Haldun M\nOzaktas, and Aykut Koç. 2021. Natural language\nprocessing in law: Prediction of outcomes in the\nhigher courts of turkey. Information Processing\n& Management, 58(5):102684.\nJoel Niklaus, Ilias Chalkidis, and Matthias Stürmer.\n2021. Swiss-judgment-prediction: A multilingual\nlegal judgment prediction benchmark. In Pro-\nceedings of the Natural Legal Language Pro-\ncessing Workshop 2021, pages 19–35.\nJoel Niklaus and Daniele Giofré. 2022. Budget-\nlongformer: Can we cheaply pretrain a sota legal\nlanguage model from scratch?\narXiv preprint\narXiv:2211.17135.\nJoel Niklaus, Veton Matoshi, Pooja Rani, Andrea\nGalassi, Matthias Stürmer, and Ilias Chalkidis.\n2023a. Lextreme: A multi-lingual and multi-task\nbenchmark for the legal domain. arXiv preprint\narXiv:2301.13126.\nJoel Niklaus, Veton Matoshi, Matthias Stürmer, Il-\nias Chalkidis, and Daniel E. Ho. 2023b. Multile-\ngalpile: A 689gb multilingual legal corpus.\nJoel Niklaus, Matthias Stürmer, and Ilias Chalkidis.\n2022. An empirical study on cross-x transfer\nfor legal judgment prediction. In Proceedings of\nthe 2nd Conference of the Asia-Pacific Chapter\nof the Association for Computational Linguistics\nand the 12th International Joint Conference on\nNatural Language Processing, pages 32–46.\nKishore Papineni, Salim Roukos, Todd Ward, and\nWei-Jing Zhu. 2002.\nBleu: a method for au-\ntomatic evaluation of machine translation.\nIn\nProceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, pages\n311–318.\nLoreto Parisi, Simone Francia, and Paolo Mag-\nnani. 2020. Umberto: an italian language model\ntrained with whole word masking. Original-date,\n55:31Z.\nVishvaksenan Rasiah, Ronja Stern, Veton Matoshi,\nMatthias Stürmer, Ilias Chalkidis, Daniel E. Ho,\nand Joel Niklaus. 2023.\nSCALE: Scaling up\nthe Complexity for Advanced Language Model\nEvaluation. ArXiv:2306.09237 [cs].\nJonathon Read, Rebecca Dridan, Stephan Oepen,\nand Lars Jørgen Solberg. 2012.\nSentence\nboundary detection: A long solved problem? In\nProceedings of COLING 2012: Posters, pages\n985–994.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \" why should i trust you?\" ex-\nplaining the predictions of any classifier. In Pro-\nceedings of the 22nd ACM SIGKDD international\n\nconference on knowledge discovery and data\nmining, pages 1135–1144.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2018. Anchors: High-precision model-\nagnostic explanations. In Proceedings of the\nAAAI conference on artificial intelligence, vol-\nume 32.\nEdwina L Rissland and Kevin D Ashley. 1987. A\ncase-based system for trade secrets law.\nIn\nProceedings of the 1st international conference\non Artificial intelligence and law, pages 60–66.\nT. Y. S. S Santosh, Marcel Perez San Blas, Phillip\nKemper, and Matthias Grabmair. 2023a. Lever-\naging task dependency and contrastive learn-\ning for case outcome classification on european\ncourt of human rights cases.\narXiv preprint\narXiv:2302.00768.\nT. Y. S. S Santosh, Oana Ichim, and Matthias\nGrabmair. 2023b. Zero shot transfer of article-\naware legal outcome classification for european\ncourt of human rights cases.\narXiv preprint\narXiv:2302.00609.\nT.y.s.s Santosh, Shanshan Xu, Oana Ichim, and\nMatthias Grabmair. 2022. Deconfounding legal\njudgment prediction for European court of hu-\nman rights cases towards better alignment with\nexperts. In Proceedings of EMNLP 2022.\nJaromir Savelka, Vern R Walker, Matthias Grab-\nmair, and Kevin D Ashley. 2017.\nSentence\nboundary detection in adjudicatory decisions in\nthe united states. Traitement automatique des\nlangues, 58:21.\nGil Semo, Dor Bernsohn, Ben Hagag, Gila Hayat,\nand Joel Niklaus. 2022. Classactionprediction:\nA challenging benchmark for legal judgment pre-\ndiction of class action cases in the us. arXiv\npreprint arXiv:2211.00582.\nMehmet Fatih Sert, Engin Yıldırım, and ˙Irfan\nHa¸slak. 2021.\nUsing artificial intelligence to\npredict decisions of the turkish constitutional\ncourt. Social Science Computer Review, page\n08944393211010398.\nRafe Athar Shaikh, Tirath Prasad Sahu, and Veena\nAnand. 2020. Predicting outcomes of legal cases\nbased on legal factors using classifiers. Procedia\nComputer Science, 167:2393–2402.\nSaeed Sharifi-Malvajerdi, Michael Kearns, and\nAaron Roth. 2019. Average individual fairness:\nAlgorithms, generalization and experiments. Ad-\nvances in neural information processing sys-\ntems, 32.\nBenjamin Strickson and Beatriz De La Iglesia.\n2020. Legal judgement prediction for uk courts.\nIn Proceedings of the 2020 the 3rd international\nconference on information science and system,\npages 204–209.\nOctavia-Maria ¸Sulea, Marcos Zampieri, Shervin\nMalmasi, Mihaela Vela, Liviu P Dinu, and Josef\nvan Genabith. 2017a. Exploring the use of text\nclassification in the legal domain.\nOctavia-Maria ¸Sulea, Marcos Zampieri, Mihaela\nVela, and Josef van Genabith. 2017b. Predicting\nthe law area and decisions of french supreme\ncourt cases. In Proceedings of the International\nConference Recent Advances in Natural Lan-\nguage Processing, RANLP 2017, pages 716–\n722.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan.\n2017. Axiomatic attribution for deep networks.\nIn ICML, pages 3319–3328. PMLR.\nDimitrios Tsarapatsanis and Nikolaos Aletras.\n2021. On the ethical limits of natural language\nprocessing on legal text. In Findings of the As-\nsociation for Computational Linguistics: ACL-\nIJCNLP 2021, pages 3590–3599.\nMichael Benedict L Virtucio, Jeffrey A Aborot, John\nKevin C Abonita, Roxanne S Avinante, Rother\nJay B Copino, Michelle P Neverida, Vanesa O\nOsiana, Elmer C Peramo, Joanna G Syjuco, and\nGlenn Brian A Tan. 2018. Predicting decisions\nof the philippine supreme court using natural\nlanguage processing and machine learning. In\n2018 IEEE 42nd annual computer software and\napplications conference (COMPSAC), volume 2,\npages 130–135. IEEE.\nBernhard\nWaltl,\nGeorg\nBonczek,\nElena\nScepankova,\nJörg Landthaler,\nand Florian\nMatthes. 2017.\nPredicting the outcome of\nappeal decisions in germany’s tax law. In Inter-\nnational conference on electronic participation,\npages 89–99. Springer.\nYuzhong Wang, Chaojun Xiao, Shirong Ma, Haoxi\nZhong, Cunchao Tu, Tianyang Zhang, Zhiyuan\nLiu, and Maosong Sun. 2021. Equality before\nthe law: Legal judgment consistency analysis for\nfairness. arXiv preprint arXiv:2103.13868.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Rémi Louf, Morgan\nFuntowicz, et al. 2020.\nTransformers: State-\nof-the-art natural language processing. In Pro-\nceedings of the 2020 conference on empirical\nmethods in natural language processing: system\ndemonstrations, pages 38–45.\n\nYongkai Wu, Lu Zhang, and Xintao Wu. 2019.\nCounterfactual fairness: Unidentification, bound\nand algorithm. In Proceedings of the Twenty-\nEighth International Joint Conference on Artifi-\ncial Intelligence.\nShanshan Xu, Santosh T.y.s.s, Oana Ichim, Is-\nabella Risini, Barbara Plank, and Matthias Grab-\nmair. 2023. From dissonance to insights: Dis-\nsecting disagreements in rationale construction\nfor case outcome classification. In Proceedings\nof the 2023 Conference on Empirical Methods\nin Natural Language Processing, pages 9558–\n9576, Singapore. Association for Computational\nLinguistics.\nLinan Yue, Qi Liu, Binbin Jin, Han Wu, Kai Zhang,\nYanqing An, Mingyue Cheng, Biao Yin, and Day-\nong Wu. 2021.\nNeurjudge: a circumstance-\naware neural framework for legal judgment pre-\ndiction. In Proceedings of the 44th International\nACM SIGIR Conference on Research and Devel-\nopment in Information Retrieval, pages 973–982.\nMikhail Yurochkin, Amanda Bower, and Yuekai Sun.\nTraining individually fair ml models with sensitive\nsubspace robustness. In International Confer-\nence on Learning Representations.\nMuhammad Bilal Zafar, Isabel Valera, Manuel\nGomez Rodriguez, and Krishna P Gummadi.\n2017. Fairness beyond disparate treatment &\ndisparate impact: Learning classification without\ndisparate mistreatment. In Proceedings of the\n26th international conference on world wide web,\npages 1171–1180.\nMatthew D Zeiler and Rob Fergus. 2014. Visual-\nizing and understanding convolutional networks.\nIn Computer Vision–ECCV 2014: 13th European\nConference, Zurich, Switzerland, September 6-\n12, 2014, Proceedings, Part I 13, pages 818–\n833. Springer.\nJunzhe Zhang and Elias Bareinboim. 2018. Fair-\nness in decision-making—the causal explana-\ntion formula. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, volume 32.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore:\nEvaluating text generation with bert. In Interna-\ntional Conference on Learning Representations.\nHaoxi Zhong,\nYuzhong Wang,\nCunchao Tu,\nTianyang Zhang, Zhiyuan Liu, and Maosong Sun.\n2020. Iteratively questioning and answering for\ninterpretable legal judgment prediction. In Pro-\nceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, pages 1250–1257.\nA.\nAppendix\nA.1.\nExplainability performance for\ndifferent levels of occlusion\nWe report the label wise F1-score for each oc-\nclusion test for every language in Tables 7, 8, 9.\nOverall, \"Opposes judgement\" and \"Neutral\" are\nchallenging ones compared to \"Supports judge-\nment\". In the case of French, there was an im-\nprovement in scores as the number of occluded\nsentences increased. This improvement indicates\nthat the model was able to correctly associate the\nlabel \"supports judgements\" with the occluded sen-\ntences, thereby enhancing the model’s explainabil-\nity performance. However, in the case of French\nand Italian, a different trend was observed. The\nmodel did not exhibit the same improvement as\nthe number of occluded sentences increased. It\nis speculated that the model might have encoun-\ntered conflicting labels for each occluded sentence,\nleading to incorrect predictions when multiple oc-\nclusions were present.\n\nSet 1\nSet 2\nSet 3\nSet 4\nModel\nOpposes\nNeutral\nSupports\nOpposes\nNeutral\nSupports\nOpposes\nNeutral\nSupports\nOpposes\nNeutral\nSupports\nMonoLingual\n23.08\n20.98\n33.86\n8.80\n17.85\n27.30\n3.28\n20.19\n16.84\n1.15\n21.34\n11.89\nMultiLingual\n23.79\n18.05\n34.46\n8.12\n13.87\n26.81\n1.52\n12.79\n24.25\n1.08\n14..10\n11.17\nMonoLingual + DA\n24.05\n21.77\n25.12\n10.69\n19.29\n17.34\n3.93\n18.24\n10.01\n0.76\n10.80\n22.03\nMultiLingual + DA\n28.80\n26.40\n36.44\n12.45\n21.64\n27.98\n4.49\n19.83\n17.15\n1.04\n18.01\n11.08\nJoint Training\n24.52\n20.14\n31.87\n9.86\n10.62\n28.25\n2.69\n9.20\n27.52\n0.98\n5.75\n26.14\nJoint Training + DA\n23.85\n23.33\n38.3\n13.40\n16.02\n25.11\n4.71\n10.74\n12.67\n1.24\n7.77\n8.74\nTable 7: Explainability performance for German dataset over different occlusion test sets\nSet 1\nSet 2\nSet 3\nSet 4\nModel\nOpposes\nNeutral\nSupports\nOpposes\nNeutral\nSupports\nOpposes\nNeutral\nSupports\nOpposes\nNeutral\nSupports\nMonoLingual\n21.48\n10.17\n36.67\n4.34\n4.87\n38.17\n1.34\n4.17\n53.64\n0.18\n3.58\n67.99\nMultiLingual\n26.80\n2.40\n40.98\n6.19\n0.68\n38.14\n1.05\n0.79\n40.19\n0.00\n0.22\n51.58\nMonoLingual + DA\n20.05\n10.53\n28.57\n4.72\n8.67\n43.29\n1.28\n6.79\n58.81\n0.14\n4.35\n69.36\nMultiLingual + DA\n21.11\n4.73\n30.41\n6.17\n2.02\n38.24\n1.03\n1.26\n41.39\n0.00\n1.87\n40.86\nJoint Training\n21.43\n3.49\n37.37\n7.09\n0.68\n43.02\n1.11\n1.12\n49.77\n0.12\n1.12\n58.29\nJoint Training + DA\n28.11\n14.05\n38.78\n8.51\n9.69\n44.26\n1.21\n10.09\n43.65\n0.00\n8.80\n42.04\nTable 8: Explainability performance for French dataset over different occlusion test sets\nSet 1\nSet 2\nSet 3\nSet 4\nModel\nOpposes\nNeutral\nSupports\nOpposes\nNeutral\nSupports\nOpposes\nNeutral\nSupports\nOpposes\nNeutral\nSupports\nMonoLingual\n9.01\n24.70\n23.53\n2.01\n26.56\n6.72\n0.52\n31.08\n0.22\n0.08\n32.43\n0.00\nMultiLingual\n12.84\n19.18\n31.25\n2.54\n7.66\n14.86\n0.41\n7.22\n4.84\n0.0\n3.92\n1.08\nMonoLingual + DA\n22.11\n3.03\n27.10\n5.61\n0.24\n13.37\n0.84\n0.33\n6.12\n0.05\n0.35\n1.92\nMultiLingual + DA\n21.28\n11.43\n35.82\n8.66\n2.62\n20.77\n0.76\n1.06\n8.51\n0.06\n0.18\n2.46\nJoint Training\n17.24\n11.21\n25.61\n3.27\n5.61\n17.55\n0.37\n1.79\n8.07\n0.0\n0.63\n3.01\nJoint Training + DA\n20.99\n17.94\n33.53\n3.99\n6.50\n23.02\n0.46\n2.28\n11.3\n0.04\n0.77\n3.91\nTable 9: Explainability performance for Italian dataset over different occlusion test sets",
    "pdf_filename": "Towards Explainability and Fairness in Swiss Judgement Prediction - Benchmarking on a Multilingual Dataset.pdf"
}