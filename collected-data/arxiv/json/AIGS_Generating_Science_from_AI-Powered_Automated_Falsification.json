{
    "title": "AIGS Generating Science from AI-Powered Automated Falsification",
    "context": "Rapid development of artiﬁcial intelligence has drastically accelerated the devel- opment of scientiﬁc discovery. Trained with large-scale observation data, deep neural networks extract the underlying patterns in an end-to-end manner and as- sist human researchers with highly-precised predictions in unseen scenarios. The recent rise of Large Language Models (LLMs) and the empowered autonomous agents enable scientists to gain help through interaction in different stages of their research, including but not limited to literature review, research ideation, idea implementation, and academic writing. However, AI researchers instantiated by foundation model empowered agents with full-process autonomy are still in their infancy. In this paper, we study AI-Generated Science (AIGS), where agents in- dependently and autonomously complete the entire research process and discover scientiﬁc laws. By revisiting the deﬁnition of scientiﬁc research (Popper, 1935), we argue that falsiﬁcation is the essence of both human research process and the design of an AIGS system. Through the lens of falsiﬁcation, prior systems at- tempting towards AI-Generated Science either lack the part in their design, or rely heavily on existing veriﬁcation engines that narrow the use in specialized do- mains. In this work, we propose BABY-AIGS as a baby-step demonstration of a full-process AIGS system, which is a multi-agent system with agents in roles rep- resenting key research process. By introducing FALSIFICATIONAGENT, which identify and then verify possible scientiﬁc discoveries, we empower the system with explicit falsiﬁcation. Experiments on three tasks preliminarily show that BABY-AIGS could produce meaningful scientiﬁc discoveries, though not on par with experienced human researchers. Finally, we discuss on the limitations of current BABY-AIGS, actionable insights, and related ethical issues in detail.1 Heliocentric Theory: Hypothesis Falsification Scientific Discovery Geocentric Theory: Earth is stationary; Celestial bodies revolve around the Earth; Circular motion. Astronomy Observation: Mathematical Reasoning: Four Largest Satellites of Jupiter Transit of Venus Phases of Venus Astronomical Observation Data Mathematical Proof Kepler’s Laws of Planetary Motion Final Discovery: Earth is rotating; Celestial bodies revolve around the Sun; Elliptical orbit motion. ... Earth is rotating; Celestial bodies revolve around the Sun; Circular motion. ... ... Geocentric Theory Heliocentric Theory Celestial Motion Diagram: Figure 1: Examples of scientiﬁc research processes conducted by human researchers. Explicit falsi- ﬁcation serves as a vital stage to falsify or verify the proposed hypotheses from either empirical or theoretical experiments, leading to the ultimate scientiﬁc discovery. ∗indicates equal contribution. 1Ofﬁcial Website: https://agent-force.github.io/AIGS/. Code is released at https://github.com/AgentForceTeamOfficial/Baby-AIGS. 1",
    "body": "arXiv:2411.11910v1  [cs.LG]  17 Nov 2024\nAIGS: GENERATING SCIENCE FROM AI-POWERED\nAUTOMATED FALSIFICATION\nZijun Liu1∗, Kaiming Liu1∗, Yiqi Zhu1∗, Xuanyu Lei1,2∗, Zonghan Yang1∗,\nZhenhe Zhang1, Peng Li2, Yang Liu1,2\n1Department of Computer Science & Technology, Tsinghua University\n2Institute for AI Industry Research (AIR), Tsinghua University\nABSTRACT\nRapid development of artiﬁcial intelligence has drastically accelerated the devel-\nopment of scientiﬁc discovery. Trained with large-scale observation data, deep\nneural networks extract the underlying patterns in an end-to-end manner and as-\nsist human researchers with highly-precised predictions in unseen scenarios. The\nrecent rise of Large Language Models (LLMs) and the empowered autonomous\nagents enable scientists to gain help through interaction in different stages of their\nresearch, including but not limited to literature review, research ideation, idea\nimplementation, and academic writing. However, AI researchers instantiated by\nfoundation model empowered agents with full-process autonomy are still in their\ninfancy. In this paper, we study AI-Generated Science (AIGS), where agents in-\ndependently and autonomously complete the entire research process and discover\nscientiﬁc laws. By revisiting the deﬁnition of scientiﬁc research (Popper, 1935),\nwe argue that falsiﬁcation is the essence of both human research process and the\ndesign of an AIGS system. Through the lens of falsiﬁcation, prior systems at-\ntempting towards AI-Generated Science either lack the part in their design, or\nrely heavily on existing veriﬁcation engines that narrow the use in specialized do-\nmains. In this work, we propose BABY-AIGS as a baby-step demonstration of a\nfull-process AIGS system, which is a multi-agent system with agents in roles rep-\nresenting key research process. By introducing FALSIFICATIONAGENT, which\nidentify and then verify possible scientiﬁc discoveries, we empower the system\nwith explicit falsiﬁcation. Experiments on three tasks preliminarily show that\nBABY-AIGS could produce meaningful scientiﬁc discoveries, though not on par\nwith experienced human researchers. Finally, we discuss on the limitations of\ncurrent BABY-AIGS, actionable insights, and related ethical issues in detail.1\nHeliocentric Theory:\nHypothesis\nFalsification\nScientific Discovery\nGeocentric Theory:\nEarth is stationary;\n    Celestial bodies\n    revolve around the\n    Earth;\nCircular motion.\nAstronomy Observation:\nMathematical Reasoning: \nFour Largest\nSatellites of Jupiter\nTransit of Venus\nPhases of Venus\nAstronomical\nObservation\nData\nMathematical\nProof\nKepler’s Laws\nof Planetary\nMotion\nFinal Discovery:\nEarth is rotating;\n    Celestial bodies revolve around\n    the Sun;\nElliptical orbit motion.\n...\nEarth is rotating;\n    Celestial bodies\n    revolve around the\n    Sun;\nCircular motion.\n...\n...\nGeocentric Theory\nHeliocentric Theory\nCelestial Motion\nDiagram:\nFigure 1: Examples of scientiﬁc research processes conducted by human researchers. Explicit falsi-\nﬁcation serves as a vital stage to falsify or verify the proposed hypotheses from either empirical or\ntheoretical experiments, leading to the ultimate scientiﬁc discovery.\n∗indicates equal contribution.\n1Ofﬁcial\nWebsite:\nhttps://agent-force.github.io/AIGS/.\nCode\nis\nreleased\nat\nhttps://github.com/AgentForceTeamOfficial/Baby-AIGS.\n1\n\nCONTENTS\n1\nIntroduction\n3\n2\nThe Development of AI-Accelerated Scientiﬁc Discovery\n4\n2.1\nAI as a Performance Optimizer: Discoveries in Speciﬁc Tasks\n. . . . . . . . . . .\n4\n2.2\nAI as a Research Assistant: Co-pilot in Human-AI Collaboration . . . . . . . . . .\n5\n2.3\nAI as an Automated Scientist: Towards End-to-end Scientiﬁc Discovery . . . . . .\n6\n2.4\nAI forms a Research Community: Enable Academic Swarm Intelligence . . . . . .\n6\n3\nBABY-AIGS: A Baby Step Towards Full-Process AIGS\n6\n3.1\nDesign Principles of a Full-Process AIGS System . . . . . . . . . . . . . . . . . .\n6\n3.2\nBABY-AIGS System Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.3\nDetailed Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.3.1\nDomain-Speciﬁc Language (DSL) . . . . . . . . . . . . . . . . . . . . . .\n9\n3.3.2\nPROPOSALAGENT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.3.3\nREVIEWAGENT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.3.4\nMulti-Sampling Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.3.5\nFALSIFICATIONAGENT\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.4\nAutomated Full-Process Research Experiment . . . . . . . . . . . . . . . . . . . .\n15\n3.4.1\nSelected Research Topics . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.4.2\nEvaluation Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.5\nQuantitative and Qualitative Analysis\n. . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.6\nDiscussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4\nLimitations and Actionable Insights\n19\n5\nEthics and Impact Statement\n20\n5.1\nPotential Negative Impacts of AIGS Systems\n. . . . . . . . . . . . . . . . . . . .\n20\n5.2\nStrategies for Responsible and Ethical Development of Automated Research Systems\n21\n6\nConclusion\n22\nA Implementation Details of the BABY-AIGS system\n29\nA.1\nResearch-Agnostic Implementation . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nA.2\nResearch-Speciﬁc Implementation . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nB\nExperiment Details\n31\nB.1\nGuidelines for Human Evaluators . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nB.2\nAPI Costs of the Full-Process Research Experiment . . . . . . . . . . . . . . . . .\n31\nB.3\nDSL Demonstrations for Different Research Topics . . . . . . . . . . . . . . . . .\n31\nC Prompting Structure\n33\n2\n\n1\nINTRODUCTION\nDeep learning has revolutionized scientiﬁc research (LeCun et al., 2015; Vaswani et al., 2017;\nJumper et al., 2021; Achiam et al., 2023). Leveraging the enormous amount of experimental data,\ndeep learning methods extract the underlying patterns in an end-to-end manner and effectively gen-\neralize to unobserved scenarios. The breakthroughs from deep learning in scientiﬁc domains, such\nas protein structure prediction (Jumper et al., 2021), gravitational wave detection (George & Huerta,\n2018), and plasma control (Degrave et al., 2022), have received award-winning recognition. As a\nresult, AI for Science has emerged as a highly-regarded research ﬁeld (Wang et al., 2023a).\nIn the paradigm of AI for Science, AI primarily serves as a tool to assist researchers in making\ndiscoveries. With the rapid development of foundation models and autonomous agents (Park et al.,\n2023), AI techniques nowadays boast the capabilities of general-purposed textual understanding\nand autonomous interaction with the external world. These capabilities lead to the successful ap-\nplications of AI-as-research-assistants, ranging from single-cell analysis (Hou & Ji, 2024) to drug\ndiscovery (Wang et al., 2023b). The capability of providing research assistance leads to a more\nambitious challenge: Can foundation model-powered agents be autonomous researchers, indepen-\ndently completing the entire process of scientiﬁc discovery, thereby transforming AI for Science\ninto AI-Generated Science (AIGS)?\nWhen constructing an AIGS system with full-process autonomy, the desiderata of the system\ndesign should refer to the deﬁnition of the scientiﬁc research process itself. As stated by Popper\n(1935), scientiﬁc research follows a systematic process of proposing novel hypotheses, conducting\nexperiments through trial and error, and falsifying these hypotheses to conclude.\nWhile it is\nwidely-believed that creativity is indispensable in the process of research - which is also accounted\nin previous work (Si et al., 2024) - the central component of scientiﬁc research is falsiﬁcation:\ndesigning and executing experiments to validate or refute hypotheses, and falsiﬁed hypotheses\npose positive contributions to scientiﬁc progress as well2.\nMoreover, experienced researchers\naccumulate practical skills or reusable workﬂows (Gil et al., 2007) from hands-on experimentation,\nwhich eases the design and execution of experiments and hypothesis falsiﬁcation. The abstraction\nof workﬂows in experiments enables effective reuse, which reﬂects a high level of executability\nin scientiﬁc research.\nTo recapitulate, a creative idea is the beginning of a piece of scientiﬁc\nresearch, which is followed by experiments and analyses to be conducted; executability forms the\nbasis for falsiﬁcation, and a sequence of logically consistent falsiﬁcation processes turns a novel\nidea into scientiﬁc discoveries with genuine creativity. As a result, falsiﬁcation is the foundation\nof AI-Generated Science, pillared by experimenting scaffolds accounting for executability and\ntargeting at the ultimate goal of research creativity.\nSeveral preliminary works have been proposed to explore the potential of AIGS, which can be\nroughly divided into three lines. In the ﬁrst line, researchers evaluate and improve the capability\nof LLMs to generate research ideas with high creativity (Si et al., 2024; Hu et al., 2024b). The\nsecond line emphasizes the executability of research experiments, e.g., benchmarks like MLA-\ngentBench (Liu et al., 2023) and MLE-Bench (Chan et al., 2024) aim to evaluate the agentic\nability of LLMs to achieve high performance on the provided benchmarks via code generation.\nThese two lines of research investigate distinct sub-stages in the research process, failing to\naddress the full-process autonomy. The third line of research attempts to construct end-to-end\nAIGS systems that cover both creativity and executability. MLR-copilot (Li et al., 2024b) takes\nexisting research papers as input, and produces execution results by both generating ideas and\nimplementing experiments. AI Scientist (Lu et al., 2024) further claims to be able to organize\nthe generated ideas and experimental results into research papers as the output.\nThis line of\nresearch arouses signiﬁcant excitement in the community, but is feedbacked with controversy:\nCriticisms include the incremental nature of the generated knowledge “tweaks”, as well as the\npoor quality of the generated code and the paper presentation3. Indeed, as further benchmarked by\nDiscoveryWorld (Jansen et al., 2024) and ScienceAgentBench (Chen et al., 2024d), an automatic\nAIGS system that produces novel research in an end-to-end manner is still in the early stages,\nwith signiﬁcant gaps remains underexplored, especially in the area of autonomous falsiﬁcation.\nFurthermore, while specialized systems like AlphaGeometry (Trinh et al., 2024) have achieved\nstriking domain-speciﬁc performances, they rely heavily on the existing veriﬁcation engines, which\nalleviate the need of autonomous falsiﬁcation by AI itself.\n2https://ml-retrospectives.github.io/.\n3https://x.com/jimmykoppel/status/1828077203956850756.\n3\n\nIn this work, we initiate BABY-AIGS, our baby-step attempt toward a full-process AIGS system.\nBABY-AIGS comprises several LLM-powered agents, including PROPOSALAGENT, EXPAGENT,\nREVIEWAGENT, FALSIFICATIONAGENT, etc., each responsible for distinct stages within the re-\nsearch workﬂow, mimicking the full-process human research that falsiﬁes hypotheses based on em-\npirical or theoretical results for scientiﬁc discoveries. BABY-AIGS operates in two phases: the ﬁrst\nphase iteratively reﬁnes proposed ideas and methods through enriched feedback, incorporating ex-\nperimental outcomes, detailed reviews, and relevant literature. The second phase emphasizes explicit\nfalsiﬁcation, a key feature absent in prior systems (Lu et al., 2024), executed by FALSIFICATIONA-\nGENT. Based on experimental results related to the proposed methodology, the agent identiﬁes crit-\nical factors likely contributing to notable experimental phenomena, formulates hypotheses, and ulti-\nmately produces scientiﬁc insights veriﬁed through ablation experiments. Additionally, we introduce\na Domain-Speciﬁc Language (DSL) (Mernik et al., 2005) for PROPOSALAGENT to articulate ideas\nand methodologies in an executable format, enhancing research executability—particularly during\nexperiments. We observe that multi-sampling proposals combined with re-ranking based on valida-\ntion benchmarks can enhance the creativity of methodologies developed during BABY-AIGS ’s ﬁrst\nphase. We apply BABY-AIGS across three tasks: data engineering, self-instruct alignment, and lan-\nguage modeling. Preliminary experimental results indicate that BABY-AIGS can autonomously pro-\nduce meaningful scientiﬁc discoveries from automated falsiﬁcation, supported by qualitative analy-\nsis. We also observe consistent performance improvements during iterative reﬁnement of methods\nproposed by BABY-AIGS. Nevertheless, current performance remains below the results achieved\nby experienced researchers in top academic venues, suggesting avenues for further enhancement.\n2\nTHE DEVELOPMENT OF AI-ACCELERATED SCIENTIFIC DISCOVERY\nIn this section, we review and envision the development of AI-accelerated scientiﬁc discovery as\nfour paradigms (Figure 2): (I) AI as a Performance Optimizer, where deep neural networks are\ntrained with large-scale observation data in a speciﬁc scientiﬁc problem to extract the patterns in an\nend-to-end manner. In this paradigm, the AI techniques are used to optimize the speciﬁc prediction /\nregression performancein the pre-deﬁned scientiﬁc problem with the consideration of out-of-domain\ngeneralization. (II) AI as a Research Assistant, where LLM-driven research copilots are used to\nassist the human research process. The synergy between Paradigm (I) and (II) forms the AI-powered\nacceleration of scientiﬁc discovery nowadays. (III) AI as an Automated Scientist. In this regime,\nfoundation model empowered agents with scientist-like behavior should complete the entire research\nprocess, ranging from the initial idea proposal to the ultimate delivery of the scientiﬁc ﬁndings.\n(IV) AI Forms a Research Community. Upon the prosperity of fully-autonomous AI researchers\ndepicted in the previous stage, we envision the collaborations among the agentic researchers foster\nan AI-formed research community.\nAI Algorithm\nSpecific Task\nPre-Defined Env\nPerformance\nOptimization\nAI as a Performance Optimizer\nAI as a Research Assistant\nAI as an Automated Scientist\nAgent\nAgent\nAgent\nAgent\nAgent\nAgent\nAI Forms a Research Community\nScientific\nDiscovery\nAI OpenReview\nAI Conference\nAI Social Media\nAI arXiv\n...\nHuman\nLiterature Review\nResearch Ideation\nIdea Implementation\nAcademic Writing\nEvaluation\nAI Assistant\nAI Github\nFigure 2: Overview of the four paradigms of AI-accelerate scientiﬁc discovery systems.\n2.1\nAI AS A PERFORMANCE OPTIMIZER: DISCOVERIES IN SPECIFIC TASKS\nWith the rise of deep learning, AI has signiﬁcantly impacted scientiﬁc discoveries across vari-\nous ﬁelds, particularly in optimizing speciﬁc tasks by exploring well-deﬁned search spaces or ex-\ntracting patterns from piles of data. Utilizing specialized deep learning models, scientiﬁc break-\n4\n\nthroughs continue to emerge across diverse ﬁelds, including accurate protein structure predic-\ntion (Jumper et al., 2021; Abramson et al., 2024), drug discovery and materials design (Gilmer et al.,\n2017; Juan et al., 2021), and the simulation of physical systems (Sanchez-Gonzalez et al., 2020).\nMoreover, a longstanding open problem in mathematics has been resolved through training a spe-\ncialized Transformer-based expert (Alfarano et al., 2024). It is widely recognized that deep learning\nmodels are highly effective in learning representations and patterns from data, enabling scientiﬁc\ndiscovery when appropriately guided.\nLarge Language Models (LLMs), equipped with extensive world knowledge and advanced rea-\nsoning, are emerging as increasingly creative and autonomous agents.\nThey have demon-\nstrated remarkable proﬁciency in autonomously developing evolutionary strategies for instruc-\ntion datasets (Zeng et al., 2024), identifying and rectifying their own weaknesses (Cheng et al.,\n2024; McAleese et al., 2024),\nand optimizing organizational structures for improved efﬁ-\nciency (Zhang et al., 2024a; Hu et al., 2024a), highlighting their potential for performance opti-\nmization through structured search. Beyond language tasks, their creativity contributes to impres-\nsive discoveries in scientiﬁc ﬁelds. Via scientiﬁcally oriented, logically organized searches, LLMs\ncan be guided to discover mathematical solutions (Romera-Paredes et al., 2024) and physical equa-\ntions (Ma et al., 2024; Shojaee et al., 2024). Augmented with specialized tools and veriﬁcation\nengine, LLMs are capable of solving advanced geometry problems (Trinh et al., 2024), design-\ning chemical reactions (Chen et al., 2024a) and discovering novel materials (M. Bran et al., 2024;\nGhafarollahi & Buehler, 2024).\n2.2\nAI AS A RESEARCH ASSISTANT: CO-PILOT IN HUMAN-AI COLLABORATION\nEquipped with expanding scientiﬁc knowledge and generative capabilities, LLMs gradually exhibit\ngreat potential to assist researchers at various stages of the research process.\nLiterature review is a fundamental but tedious step for scientiﬁc research, highlighting the need for\nautonomous agents for this task. Advanced LLMs are employed to identify relevant literature for\na given research topic and generate structured summaries (Haman & ˇSkoln´ık, 2024; Huang & Tan,\n2023). For instance, Sharma et al. (2021) introduces a retrieval-augmented framework to produce\nreliable summaries based on latest studies. Furthermore, Hsu et al. (2024) utilizes LLMs to organize\nscientiﬁc studies within hierarchical structures and Li et al. (2024d) develops an agentic pipeline that\nproduces comparative literature summaries guided by human workﬂows. In summary, LLM-based\nagents have demonstrated the capability to produce readable and detailed literature reviews.\nFor research ideation, LLMs are employed to generate reasonable hypotheses (Wang et al., 2024a;\nQi et al., 2023; Zhou et al., 2024) based on internal knowledge and supplementary inputs. To com-\npare the quality of LLM-generated ideas with human experts, a large-scale human study (Si et al.,\n2024) ﬁnds that LLMs can generate research ideas of higher novelty but slightly weaker feasibility.\nFurthermore, Kumar et al. (2024) and Girotra et al. (2023) evaluate the idea generation capabilities\nof different LLMs and recognize their potential to serve as the sources of inspiration. To enhance\nLLM-driven ideation, Baek et al. (2024), Nigam et al. (2024a) and Nigam et al. (2024b) develop\nmulti-agent ideation frameworks based on scientiﬁc literature, generating novel research proposals\nto accelerate the life-cycle of research process. Despite these advancements, generating ideas that\nbalance both novelty and feasibility remains a signiﬁcant challenge for LLM-based agents (Si et al.,\n2024). To evolve initial proposals into validated knowledge therefore demands substantial effort.\nThe attempts in AI-assisted idea implementation and auto-experimentation are usually conducted\nas repo-level coding tasks, given the growing coding capabilities of LLMs. Focused on research-\nrelated repo-level coding, Jimenez et al. (2024), Liu et al. (2023) and Chan et al. (2024) present\nchallenging coding benchmarks targeting machine learning and software engineering tasks. Mean-\nwhile, Yang et al. (2024a), Wang et al. (2024b) and Tao et al. (2024) leverage agentic collaboration\nto automated coding from language instructions, offering promising avenues to reduce researchers’\ncoding workloads and enhance efﬁciency. However, the vision for agents to autonomously im-\nplement novel ideas and conduct experiments end-to-end imposes signiﬁcantly higher demands on\ncoding agents. Current challenges include a relatively low success rate Lu et al. (2024) and frequent\nmisalignment between proposed ideas and their coding implementations, highlighting the need for\nimprovements in both execution reliability and alignment with research objectives.\nIn the realm of academic writing, LLMs can be utilized for drafting structured outlines, reﬁn-\ning human-written texts and presenting research ﬁndings.\nRecent studies (Liang et al., 2024b;\nGeng & Trotta, 2024) have demonstrated a steady increase for LLM usage in scientiﬁc writing.\nThis trend presents both opportunities and challenges for academia. When properly used, LLMs\n5\n\ncould improve research efﬁciency and presentation; But when misused, risks emerge as well in\nterms of research integrity. Therefore, effective oversight through detection strategies (Liang et al.,\n2024a; Yang et al., 2024b; Ghosal et al., 2023) and watermarking techniques (Kirchenbauer et al.,\n2023; Zhao et al., 2023; Zhang et al., 2024b) is both beneﬁcial and necessary.\nAdditionally, following LLM-as-judge methods (Zheng et al., 2023), LLM-based agents are em-\nployed for comprehensive evaluation on research outputs (Lu et al., 2024; Li et al., 2024b). Com-\nparing model-generated reviews with expert evaluations, researchers have evaluated the capabilities\nof LLMs to provide insightful and high-quality reviews by constructing meticulously annotated\ndatasets (Du et al., 2024) or training preference models (Tyser et al., 2024). With multi-agent col-\nlaboration to promote in-depth analysis and constructive feedback, D’Arcy et al. (2024), Jin et al.\n(2024) and Yu et al. (2024) develop LLM-powered agent pipelines to perform paper reviews, helping\nresearchers improve the quality of their papers. Furthermore, Sun et al. (2024) introduces a review-\ning tool designed to support reviewers with knowledge-intensive annotations. In a notable develop-\nment, ICLR conference adopt reviewer agents to provide constructive feedback on human-submitted\nreviews, showcasing a promising application of AI-assisted reviewing 4. Recently, researchers also\nconstructed benchmarks for AI as a research assistant at more than one stages above (Lou et al.,\n2024). Overall, it is promising for LLMs to assist researchers with reliable research feedback.\n2.3\nAI AS AN AUTOMATED SCIENTIST: TOWARDS END-TO-END SCIENTIFIC DISCOVERY\nStructured in well-organized agentic pipelines, LLMs are increasingly capable of tackling complex\ntasks collaboratively, with end-to-end scientiﬁc research being one of the most ambitious and chal-\nlenging applications. For instance, Lu et al. (2024) develops an iterative multi-agent framework\nthat supports the entire research process, from proposing novel ideas to presenting polished ﬁnd-\nings. Similarly, Li et al. (2024b) introduces an automated research system for machine learning, and\nManning et al. (2024) employs LLMs to simulate scientists for social science research. Beyond re-\nsearch systems, Jansen et al. (2024) proposes a simulation environment designed to challenge agents\nin automated scientiﬁc discovery. Despite these advancements, current end-to-end research systems\nstill fall short of generating falsiﬁable scientiﬁc ﬁndings, constrained by the capabilities of both\ndesigned framework and foundation models. While previous research (Lu et al., 2024) has yielded\nwell-formulated outcomes, the vision of automated science discovery still requires further efforts.\n2.4\nAI FORMS A RESEARCH COMMUNITY: ENABLE ACADEMIC SWARM INTELLIGENCE\nThroughout human history, scientiﬁc progress has been greatly driven by collaboration, connection,\nand discussion among scientists, highlighting the power of a vibrant research community. We pro-\npose that a research community of AI scientists could signiﬁcantly accelerate the pace of automated\nscientiﬁc discovery. For agentic community construction, LLM-driven agents can be organized to\ngenerate believable, human-like behaviors (Park et al., 2022; Gao et al., 2024; Park et al., 2023) and\nto perform speciﬁc roles as assigned (Li et al., 2024a; Hua et al., 2023; Xu et al., 2023). Although\nagent-based simulations of research communities are in an early developmental stage, they represent\na promising avenue for the future of fully automated, AI-driven research.\n3\nBABY-AIGS: A BABY STEP TOWARDS FULL-PROCESS AIGS\nIn this section, we elaborate how a baby-step system towards the full-process AIGS is designed, in\nterms of design principles, overall system design, and detailed implementations.\n3.1\nDESIGN PRINCIPLES OF A FULL-PROCESS AIGS SYSTEM\nThe typical research process for human scientists (Popper, 1935) generally consists of two main\nstages: the pre-falsiﬁcation stage, which encompasses exploration of research ideas, reﬁnement of\nmethodologies, and theoretical or empirical analysis, and the falsiﬁcation stage, which involves\nhypothesizing scientiﬁc laws and validating these hypotheses based on theoretical or empirical ﬁnd-\nings. In research ﬁelds like machine learning, empirical results for falsiﬁcation process, i.e. ab-\nlation studies, are collected after researchers design and build a system, and conduct experiments.\nIn contrast, other ﬁelds operate differently. For example, in physics or biology, empirical results\nare gathered from instruments or equipment after the experimental design and execution, while in\nmathematics or the humanities, theoretical insights are often derived through logical reasoning or\n4https://blog.iclr.cc/2024/10/09/iclr2025-assisting-reviewers.\n6\n\nProposalAgent\nIdea1:\nProposal\nMethodology\nDSL\n...\nIdea2\nIdeaN\n...\nExpAgent\nExpResult\nMetricValue\nExpLogging\n...\nResult1:\nResult2\nResultN\n...\nReviewAgent\nExpReview\nProposalReview\nSuggestion\n...\nReview1:\nReview2\n...\nReviewN\nTurn1:\nIdea\nResult\nReview\nTurnM\nTurn2\nTurn3\n...\nHistory:\nFalsificationAgent\nScientific Discovery\nSignificance\nScreening\nDiscovery\nCandidate1:\nKey Factors\nHypothesis\nDiscCandK\n...\nAblationExp1\nAblationExp2\nAblationExpT\n...\nExpAgent\nVerification\nLiteratureAgent\nHuman Interface\nOptional\nModule\nDomain-specific ExpAgent\nEnvironmentAgent\nSecurityAgent\nFigure 3: Overview of our BABY-AIGS system design. The left part denotes Pre-Falsiﬁcation\nphase, where PROPOSALAGENT iteratively reﬁne the proposed idea and methodology based on em-\npirical and verbose feedback from EXPAGENT, REVIEWAGENT, etc. The iterative process summons\nmulti-turn logs as the history context, based on which FALSIFICATIONAGENT could produce scien-\ntiﬁc discovery in the Falsiﬁcation phase, as shown in the right part. Other modules are optional for\nthe automated full-process research.\nliterature review rather than empirical experimentation. These root falsiﬁcation processes of differ-\nent subjects in distinct knowledge source. In this work, we primarily focus on empirical subjects\nthat requires actual implementation of the methodology of a research idea to obtain empirical results\nfor falsiﬁcation process, e.g., machine learning, and leave other venues for future work.\nHuman scientiﬁc research workﬂow above reﬂects the design principles of a full-process AIGS sys-\ntem, which are falsiﬁcation, creativity, and executability. Each of the principle could be bridged\nwith a speciﬁc stage in the research workﬂow: (1) Ablation studies are fundamentally established\nupon falsiﬁcation, verifying any key factors that contribute to signiﬁcant experimental results. (2)\nTo achieve smooth and consistent experimentation, we emphasize the importance of executability of\nthe proposed methodology, which serves as the basis for collecting empirical results for both method\nreﬁnement and ablation studies. (3) Creativity of the proposed idea is the overall objective of the\nresearch process, which could be achieved through idea reﬁnement and be identiﬁed by falsiﬁcation\nprocess. We especially argue that the process of falsiﬁcation is equally, if not more, critical in\nAI-powered automated scientiﬁc discovery systems, given that human trust in AI-generated ﬁnd-\nings relies heavily on a convincing falsiﬁcation process that ensures scientiﬁc rigor and transparency.\nIn sum, falsiﬁcation is the foundation of a full-process AIGS system, pillared by experimenting\nscaffolds accounting for executability and targeting at the ultimate goal of high research creativity.\n3.2\nBABY-AIGS SYSTEM DESIGN\nHeading towards a full-process system for automated scientiﬁc discovery, we present the design of\nBABY-AIGS system in this section. We imitate the practice of human researchers and shape it into\nan LLM-powered multi-agent system. And we also take into account the capacity and behaviors of\ncurrent foundation models to ensure the executability in implementation.\nThe overall input for the system would be the topic of the research ﬁeld, an accessible and con-\nﬁgurable experiment environment, and other optional resources like a literature base; and the ﬁnal\noutcome would be a verbal scientiﬁc discovery and the falsiﬁcation process that support or falsify it.\nFollowing the principles in Section 3.1, the BABY-AIGS system operates in two phases (Figure 3):\n1. Pre-Falsiﬁcation: This phase contains several stages, such as idea formation, methodology\ndesign, experiment execution, result analysis, etc., and operates iteratively for M turns,\naiming to explore and reﬁne the proposed idea and method through feedback including\nexperimental outcomes, reviews, etc. Speciﬁcally, the experimental results of turn 0 is from\na trivial methodology at the default setting, e.g., no operation, identical mapping, etc. The\nmulti-turn log of agent communications is recorded for Falsiﬁcation. For better efﬁciency,\n7\n\nthis phase could be conducted in parallel in N threads by sampling multiple times, and the\nbest ones for the next phase could be identiﬁed with experimental results.\n2. Falsiﬁcation: This phase aims to explicitly execute falsiﬁcation by automating ablation\nstudies. The agent hypothesizes on what key factors are and how they might related to\nsigniﬁcant experimental phenomenon, and the ones pass T designed ablation experiments\nare veriﬁed as ﬁnal scientiﬁc discoveries. This could be also be K-parallel.\nIn the following sections, we elaborate important components of our BABY-AIGS system. Ahead\nof speciﬁc modules, we introduce the Domain-Speciﬁc Language (DSL) (Mernik et al., 2005). In\nan BABY-AIGS system, the DSL acts a critical role to ensure the automated pipeline is errorless.\nSpeciﬁcally, the DSL is a human-designed descriptive language which can help interpret the pro-\nposed idea and methodology into executable experimental instructions through a pre-deﬁned action\nspace. For instance, in a deep learning task, the DSL can directly be the codes that arrange training\nschedule of a model; While in a chemistry experiment, the DSL can be the interface with a certain\ninstrument or material. Consequently, the DSL bridges the gap between formulation of proposed\nidea and experimentation, aligning the BABY-AIGS system to the executability principle.\nHere, we brieﬂy depict the modules that construct the pipeline of BABY-AIGS:\n• PROPOSALAGENT is the module to propose ideas and methods within our system. It takes\nthe detailed description of the task, the record of past experiments, and the review gener-\nated by REVIEWAGENT as input, and outputs a proposal containing the idea, verbal and\nDSL-format methodology, and other necessary components to carry out the experiment for\nEXPAGENT. It could iteratively interact with EXPAGENT to reﬁne its proposal in order that\nthe experiment can be successfully completed based on its proposal.\n• EXPAGENT is responsible for experiment execution in the BABY-AIGS system. It receives\nthe proposal from PROPOSALAGENT and interprets DSL the components relevant to the\nexperiment into executable code. After execution, it transmits the experimental result as\nwell as the whole process of the experiment to REVIEWAGENT for review and analysis.\n• REVIEWAGENT reviews the proposed idea and method based on the empirical results. It\ntakes the whole record of both the experiments and the proposals as inputs, and generates\nthe multi-granular review content. The review is then returned to PROPOSALAGENT for\nthe next iteration of reﬁnement. Through this iterative process between agents above in the\nPre-Falsiﬁcation phase, creativity of the proposed idea evolves in tandem.\n• FALSIFICATIONAGENT is responsible for doing the ablation studies and deriving scientiﬁc\ndiscoveries as the ﬁnal outcome. FALSIFICATIONAGENT takes the multi-turn log of all\nother agents as input. It has access to the record of the whole process of Pre-Falsiﬁcation\nphase, and hypothesize possible key factors inﬂuencing signiﬁcant experimental phe-\nnomenon based on empirical results. Then, it designs and conducts ablation experiments\nfor T times to verify the hypothesis, leading to ﬁnal scientiﬁc discoveries.\n• Other optional modules include LITERATUREAGENT, SECURITYAGENT, ENVIRON-\nMENTAGENT, DOMAIN-SPECIFIC EXPAGENT, and HUMAN INTERFACE.\nLITERA-\nTUREAGENT is responsible for gathering and providing relevant literature to support all\nother agents.\nSECURITYAGENT ensures safe experiment execution by identifying and\npreventing actions that may pose potential hazards or infringe upon intellectual property\nrights.\nENVIRONMENTAGENT creates simulated environments to facilitate the testing\nand reﬁnement of ideas, enabling more controlled and accurate scientiﬁc discoveries.\nDOMAIN-SPECIFIC EXPAGENT is a customizable agent tailored for speciﬁc ﬁelds.\nHUMAN INTERFACE allows different agents in the system to ask human researchers for\nhelp when necessary.\nWe also acknowledge that the implementation of BABY-AIGS at the current stage has various lim-\nitations towards a general functionable full-process AIGS system. In Section 4, we outline these\nlimitations and discuss actionable insights for future improvements.\n3.3\nDETAILED IMPLEMENTATION\nIn the following sections, we elaborate on the the detailed implementation of our AIGS system\nthrough DSL, multi-sampling strategy, and three main agents: PROPOSALAGENT, REVIEWAGENT,\nand FALSIFICATIONAGENT. The rest of optional modules have been omitted for the sake of clarity.\nIn order to aid in the elaboration of the following sections, we present the research topic of data\n8\n\nFormalization Degree\nExecutability\nDSL\nNL\nCL\nChallenging to Achieve\nError-free Implementations\nCoding Language\nFlexible and\nFully Executable\nDomain Specific\nLanguage\nNon-executable\nNatural Language\nDomain Specific Language\nLanguage agents can gather high-quality trajectories by adjust-\ning actions in the original ReAct sequence and ...\nLLM Generated\nPre-Defined\nLLM Generated\nLLM Generated\nFramework Settings:\n   Method: Bootstrap / ...\n   Benchmark: AlfWorld / ...\nAvailable Functions:\n   random_truncate(param)\n   re_simulate(param)\n   call_llm(param)\n   ...\n{\n    “Method”: “Bootstrap”,\n    “Benchmark”: “AlfWorld”,\n    “Pseudocode”: “```python\nnew_traj = random_truncate()\nnew_traj = resimulate()\nprompt = ‘’\nnew_traj = call_llm(prompt)```”\n}\nRuntime Logging\n# main.py\nfrom trajectory import \\\n    random_truncate, resimulate\nfrom llm_handler import \\\n    call_llm\ndef main():\n    traj = random_truncate()\n    resimulated_traj =  \\\n    resimulate(traj)\n    ...\ndef generate_prompt(traj):\n    ...\nif __name__ == \"__main__\":\n    main()\n# trajectory.py\nimport random\n...\ndef secondary_simulation(traj):\n    return [step / (random. \\\n    randint(0, 5)) for step in traj]\n# llm_handler.py\n    ...\nTraceback:\n  ...\n  File \"trajectory.py\", line 38 ...\n    return [step / ...\nZeroDivisionError:\n  division by zero\nNatural Language\nCoding Language\nFigure 4: The relationship between formalization degree and system executability when express-\ning ideas through Natural Language (NL), Coding Language (CL), and Domain-Speciﬁc Lan-\nguage (DSL), illustrated with examples. NL expresses ideas in the simplest and most ﬂexible form\nbut is non-executable; CL offers greater precision but is challenging to achieve error-free implemen-\ntation; DSL achieves a better tradeoff between ﬂexibility and executability.\nengineering (Liu et al., 2024; Chen et al., 2024b; Li et al., 2024c; Zhao et al., 2024), which requires\nBABY-AIGS to identify key distinguishing features of datasets, and ﬁlter and extract high-quality\ndata subsets. Implementation details are elaborated in Appendix A and Appendix C.\n3.3.1\nDOMAIN-SPECIFIC LANGUAGE (DSL)\nSelf-Instruct Alignment\nPre-Defined\nFramework Settings:\n   Paradigm: Instruction Data Synthesis # Synthesize training \ndata based on the Self-Instruct framework and rewrite the seed\ndata within the framework.\nParameters:\n   Prompt: The prompt used to rewrite the data;\n   Seed: whether to use the seed data while training.\nData Engineering\nPre-Defined\nFramework Settings:\n   Paradigm: Data Sample Rating & Filtering # Score each data\npoint and filter the data based on the scores.\nParameters:\n   Principles: Prompt content of principles for scoring model;\n   Number: Total count of principles;\n   Threshold: The least surpassed number of principles to pass;\n   Ratio: The most proportion of data remained after filtering.\nLanguage Modeling\nPre-Defined\nFramework Settings:\n   Paradigm: Generative Pre-training # Modify the pre-training\nmethod of the language model.\nParameters:\n   LLM_name: Base model choice;\n   n_layer: Number of baby GPT model;\n   weight_sharing_layers: Config to share params across layer;\n   ...(other accessories)\nFigure 5: The DSL design in BABY-AIGS for\nexperimented research topics in Section 3.4.\nThe full demonstration is in Appendix B.\nA domain-speciﬁc language (Mernik et al., 2005)\nis created speciﬁcally for a particular application\ndomain, providing greater expressiveness and ease\nof use within that domain compared to general-\npurpose languages, traditionally for programming\nlanguages. However, we observed that the situa-\ntion is the same for agents in the AIGS systems.\nWhen conducting scientiﬁc research, agents have\naccess to a wide and diverse action space, making\nit challenging to perform error-free long-sequence\nactions for every stage of the research process, par-\nticularly when translating the methodology into ex-\necutable actions for experimentation. For instance,\nin machine learning research, an agent may edit\nmultiple code ﬁles and manipulate large amount\nof data, as part of the methodology execution.\nHowever, limited by the current capacity of foun-\ndation models, it remains a severe challenge for\nagents to carry out the proposed experiment with\nboth full-process autonomy and satisﬁable suc-\ncess rates (Jimenez et al., 2024; Chan et al., 2024;\nLu et al., 2024) without dedicated interface de-\nsign (Yang et al., 2024a; Wang et al., 2024b) or\ntool use (Paranjape et al., 2023; Qin et al., 2024).\nIn BABY-AIGS, we extend the original deﬁnition\nof DSL in programming to semi-structure objects\nwith pre-deﬁned grammars, making it a bridge that\nﬁlls the gap between the proposed methodology and experimentation. The DSL restricts the action\nspace of the agents while maintaining the freedom for agents to conduct proposed methods at the\nsame time, through dedicated design with human effort. To utilize the capabilities of current LLMs\n9\n\nin natural language and function-level coding, we design the semi-structured grammar to be ﬂexible\nbetween verbal instructions and structured statements. As shown in Figure 4, the DSL has both\na higher degree of formalization and executability than natural language; compared to the coding\nlanguage adopted in previous work (Lu et al., 2024), though DSL has a lower degree of formal-\nization, with human effort, it exhibits higher executability and thus ensures successful execution of\nexperiments, according to empirical analysis (Section 3.5). However, when the grammar is poorly\ndesigned, the DSL is likely to restrain the creativity of the system, because some ideas might not be\nable to be implemented, which is a limitation of BABY-AIGS for future work.\nWe present the pre-deﬁned grammar of DSL used in a few selected research topics in Figure 5.\nUnder a speciﬁc paradigm related to the research topic, the grammar contains a series of parameters\nin either structured statement, e.g., code, integers, etc., or natural language, collectively depicting\nthe methodology under the paradigm. PROPOSALAGENT would select a research paradigm when\nthere are multiple, and ﬁll out each parameter as required in the grammar. EXPAGENT is equipped\nwith a pre-deﬁned interpreter to translate the DSL into executable code lines, or inputs to speciﬁc\nLLMs or other models. For instance, one parameter of the DSL for data engineering is a few lines\nof data rating principles represented in natural language, and the model architecture parameters for\nlanguage modeling still remains in codes, indicating the ﬂexibility of DSL design. Please refer to\nSection 3.4 for detailed formulation of the research topics and topic-speciﬁc DSL designs.\n3.3.2\nPROPOSALAGENT\nAn example of the proposal from PROPOSALAGENT\nIdea & Methodology\nIdea: ...Key issues identiﬁed include overly brief or excessively lengthy answers, lack of\nunique words, irrelevant content, poor adherence to instructions, lack of coherence, low key-\nword overlap, and poor sentiment balance...\nMethodology: Key metrics to observe include the coherence of responses, adherence to\ninstructions, relevance to the prompt, depth of information provided, clarity of instructions\nand responses, engagement in the conversation...\nExperiment Settings\nBaseline: Iteration 0 (the trivial baseline)\nThought: ... we will ﬁlter the original dataset using the reﬁned DSL with weighted criteria.\n... and this will help in identifying the initial impact of the new criteria on the raw data and\nensure that the dataset is not overly biased by similarity...\nHypothesis & Related Feature\nHypothesis: After using the processed data, the model’s performance on the MT-bench task\nwill improve signiﬁcantly. The model should produce longer, more detailed, and coherent\nresponses, ... The responses should be rich in unique words, and demonstrate appropriate\nsentiment balance compared to the baseline.\nRelated Feature: ... length of responses, keyword overlap, unique word count, and sentiment\nbalance.\nRebuttal\nThe review should provide an overall view of the experiment result, focusing on whether the\nselected examples effectively demonstrate improvements in the key metrics. The review should\ncompare the performance of the model before and after the data curation to highlight the impact\nof the methodology. Speciﬁc examples should be used to illustrate both improvements and\nremaining issues to provide ...\nAs the ﬁrst step towards the scientiﬁc research, idea formation and methodology design usually\nlay the foundation for valuable insights or impactful discoveries from falsiﬁcation process based\non empirical results, i.e., creativity in the AIGS system. We refer to the corresponding module in\n10\n\nMetric\nLevel\nDescription\nExecution\nLength\nCorpus\nThe length and word count of responses\nPre-deﬁned statistic function\nKeyword Overlap\nThe keyword overlap between instructions and responses\nSentiment\nThe contained sentiment in model-generated responses\nNLTK (Bird & Loper, 2004)\nWorst Data Points\nSample\nThe worst rating samples compared with baselines\nRanking & reciting function\nBest Data Points\nThe best rating samples compared with baselines\n......\nCorpus /\nSample\nOther useful metrics generated by REVIEWAGENT or\npre-deﬁned by researchers\nFree-form code segment\nTable 1: Examples of multi-level metrics for REVIEWAGENT to empirically review the experimental\nresults and the proposal from PROPOSALAGENT in the data engineering research.\nBABY-AIGS as PROPOSALAGENT, drawing inspiration from human practice of proposing an idea\nand formulating the methodology before starting the experiments.\nPROPOSALAGENT is important part of the pre-falsiﬁcation phase. It takes the detailed description\nof research topic, the history log, including records of previous proposals and experiments, and the\nreview from REVIEWAGENT as the overall input, except for the ﬁrst iteration, in which only the\ndescription of the research topic is the input to PROPOSALAGENT. As shown in the case above on\nthe data engineering research topic, the output of PROPOSALAGENT includes\n• the proposed idea and methodology, that the former is a high-level thought and the latter\nis a semantically equal but concise description of instructions to be carried out in the ex-\nperiment in natural language and DSL format, aiming either to improve the experimental\nresults or to advance towards scientiﬁc discoveries,\n• the conﬁgurable experiment settings, such as specifying which turn’s proposal is considered\nthe baseline for the current iteration, along with other options speciﬁc to the research topic,\n• hypothesis on how would the experimental results change compared to and the most related\nfeature that may empirically reﬂect the hypothesis, which could guide REVIEWAGENT to\nidentify relevant components from all experimental results,\n• and rebuttal to the review from previous turns, except for the ﬁrst iteration.\nThus, the formulation of PROPOSALAGENT could be expressed as:\nProposal(i) =\nn\nIdea & Method.(i), Exp. Settings(i), Hypo. & Related Feat.(i), Rebuttal(i)o\n,\n= PROPOSALAGENT\n\u0010\nResearch Topic | History(i)\u0011\n, 1 ≤i ≤M,\n(1)\nwhere\nHistory(i) =\n\n\n\n∅,\nif i = 1\nn\nProposal(j), Exp. Res.(j), Review(j)oi−1\nj=1 ,\nif 1 < i ≤M ,\n(2)\ni indicates the number of iteration, N denotes the maximum iteration, PROPOSALAGENT(· | ·) indi-\ncates the agentic workﬂow, and experiment result and review are from EXPAGENT and REVIEWA-\nGENT elaborated in Section 3.3.3. The DSL format of the proposed methodology is illustrated\nin Appendix B. Building upon the aforementioned components, PROPOSALAGENT puts forward a\ncomprehensive yet highly executable proposal, which is then submitted to EXPAGENT for execution.\nUpon receiving the review form REVIEWAGENT, PROPOSALAGENT can initiate the next iteration,\neither exploring a brand new direction or optimizing current experimental results.\n3.3.3\nREVIEWAGENT\nDrawing inspiration from human practice, we recognize that signiﬁcant insights and breakthroughs\noften emerge from in-depth analysis of experiments and reﬂection on methodology based on empir-\nical results. To facilitate this process, we design REVIEWAGENT to analyze the experimental results\nand provide feedback to PROPOSALAGENT, iteratively improving the overall proposal.\nIn order to conduct a comprehensive and constructive review, REVIEWAGENT performs analysis\nat different levels of granularity. For ﬁne-grained analysis, REVIEWAGENT examines comprehen-\nsive experimental logs, analyzing intermediate results from multi-level metrics which could be pre-\ndeﬁned by human researchers, e.g. performance indicators of the benchmark, or self-generated in\ncode segment (examples for data engineering shown in Table 1). The review of the experimental\n11\n\nresults identiﬁes hidden patterns in the empirical details, resulting in fruitful low-level feedback\nmainly on experiment design and adjustment on the expectation of PROPOSALAGENT for the ex-\nperimental results. For coarse-grained analysis, it evaluates the general validity and reasonableness\nof the methodology and hypothesis, providing review of the whole proposal. This review content\nserves as high-level advice on the idea and methodology, with the aim of provoking PROPOSALA-\nGENT toward higher creativity. An example of a review of data engineering research is as follows:\nAn example of the review from REVIEWAGENT\nReview of the Experimental Results\nSummary and Actionable Insights: Based on the comprehensive analysis of various features\ninﬂuencing the scores of responses in the Alpaca-GPT4 Database, here are the key ﬁndings\nand recommendations for optimizing the dataset...\nKey Insights:\n1. Length and Word Count: High-quality responses tend to be longer, with word counts above\n1000 for answers and around 15-20 words for queries.\n2. Conciseness: While length...\nReview of the Proposal\nEvaluation of Current Research Components:\nYour proposal effectively identiﬁes key issues within the Alpaca-GPT4 dataset, such as... Ad-\nditionally, the need for speciﬁc, measurable criteria for evaluating data points to improve...\nSuggestions: 1. Data Distribution Analysis: Perform a quantitative analysis to understand the\nprevalence and distribution of these issues within your dataset...\nFormally, the outcome of REVIEWAGENT could be expressed as:\nReview(i) =\nn\nReview of the Exp. Res.(i), Review of the Proposal(i)o\n,\n= REVIEWAGENT\n\u0010\nResearch Topic | Proposal(i), Exp. Res.(i), History(i)\u0011\n, 1 ≤i ≤M,\n(3)\nwhere REVIEWAGENT(· | ·, ·, ·) indicates the agentic workﬂow, and experiment result contains\nthe benchmark results and other metric values extracted from experiments. In addition, human\nscientists derive valuable insights not only from a literature review and reasoning, but also through\nempirical analysis and detailed inspection of the experimental phenomenon, especially for subjects\nrelying largely on empirical studies. Compared to previous work (Lu et al., 2024; Su et al., 2024)\nthat improve ideation creativity primarily based on literature, our system advances this approach by\nintroducing multi-granular review of experimental results and processes. We argue the groundtruth\nof scientiﬁc laws root and get reﬂected in experimental outcomes, which could serve as process\nsupervision in our iterative reﬁnement of the proposal in the pre-falsiﬁcation phase, and might\ncontribute to the overall creativity of BABY-AIGS. Please refer to Section 3.5 for empirical analysis.\n3.3.4\nMULTI-SAMPLING STRATEGY\nIn this section, we formalize the multi-sampling strategy employed in the pre-falsiﬁcation phase\nof BABY-AIGS system. This strategy is designed for better efﬁciency and quality of iterative ex-\nploration by parallel executing PROPOSALAGENT, EXPAGENT, REVIEWAGENT, etc. for multiple\nthreads, combined with reranking to retain the most promising threads for further exploration.\nAs shown in Figure 3, the multi-sampling strategy operates orthogonal to the iterative reﬁnement of\nthe proposal, where the pre-falsiﬁcation process of each iteration i involves parallel sampling across\nN threads, and each sampled thread represents a full pre-falsiﬁcation process, including ideation,\nexperimentation, reviewing, etc. Formally, let S(i) = {s(i)\n1 , s(i)\n2 , . . . , s(i)\nN }, i = 1, ..., M represent\nthe set of threads sampled in iteration i. Each sample s(i)\nj , j = 1, ..., N undergoes experiments and\nreranking based on pre-deﬁned criteria, and only a subset with top-ranked samples S(i)\ntop ⊂S(i) of\nsize Ns is retained for the next iteration. The process can be summarized as follows:\n1. Sampling Step: In each iteration i, the system generates N samples {s(i)\n1 , s(i)\n2 , . . . , s(i)\nN }\nin parallel. If the former samples S(i−1)\ntop\nare available, i.e., it is not the ﬁrst iteration,\n12\n\neach st+1\nj\n, j = 1, ..., N is generated by taking into account the historical log from the\n\u0010\nj⌊N\nNs ⌋+ 1\n\u0011\n-th sample of the previous S(i−1)\ntop\nthreads.\n2. Reranking: All samples are reranked on the basis of the benchmarking result during ex-\nperimentation. For simplicity, we adopt the average performance score of all benchmarks.\n3. Selection for Next Iteration: After step 2, the samples are reranked and the top Ns samples\nare selected to form the set S(i)\ntop for the next iteration.\nWithin BABY-AIGS, the multi-sampling strategy with reranking is applied primarily in the Pre-\nFalsiﬁcation phase, facilitating an extensive yet efﬁcient exploration of ideas, methods, and experi-\nmental conﬁgurations. By iteratively narrowing down to the top candidates, this strategy effectively\nfocuses resources on promising pathways. In Section 3.6, we empirically demonstrate the multi-\nsampling strategy, coupled with reranking, is essential for guiding the iterative process in BABY-\nAIGS towards scientiﬁcally signiﬁcant discoveries in an effective and potentially scalable manner.\n3.3.5\nFALSIFICATIONAGENT\nIn the research process, there is usually a gap between the experimental results indicating improve-\nment in performance and the ﬁnal conclusions of the scientiﬁc ﬁndings, and human researchers usu-\nally perform ablation studies to verify the authenticity of scientiﬁc discoveries. We term progress\nlike this falsiﬁcation, which is a critical step towards full-process automated scientiﬁc discoveries.\nTurn1\nTurn2\nTurn3\nTurn4\nTurn5\nTurn6\nTurn7\nTurn8\nBaseline\nBenchmark Results\nIteration Round\nSignificant\nResults\nDecrease\nSignificant\nResults\nIncrease\nTurn9\nTurn10\nFigure 6: Illustration of “Signiﬁcance Screening”\non history records.\nThe starting point of each\nturn represents the modiﬁcations and experiments\nbased on proposals from that round. The “Signif-\nicance Screening” process identiﬁes results with\nsigniﬁcant performance increase or decrease.\nRecognizing\nthe\nimportance\nof\nfalsiﬁca-\ntion,\nwe introduce FALSIFICATIONAGENT,\na novel component not present in previous\nwork (Lu et al., 2024; Su et al., 2024).\nFAL-\nSIFICATIONAGENT has access to all history\nrecords, including proposals from PROPOS-\nALAGENT, experiment results from EXPA-\nGENT, and reviews from REVIEWAGENT. We\nhypothesize that scientiﬁc discoveries are more\nlikely to emerge from signiﬁcant experimental\nphenomena, i.e.\nchanges in results, thus,\nFALSIFICATIONAGENT in BABY-AIGS ﬁrst\nperforms a “Signiﬁcance Screening” to identify\nadjacent turns of pre-falsiﬁcation phase with\ngreatest performance discrepancies, as shown\nin Figure 6.\nFollowing this, FALSIFICA-\nTIONAGENT\ngenerates\nscientiﬁc\ndiscovery\ncandidates from these selected turns.\nThen\nFALSIFICATIONAGENT generates the plans and the ablated methods for ablation experiments.\nWe require that at most T plans are made for each discovery candidate, indicating that at most T\nablation experiments will be conducted, and each ablation experiment focuses on the veriﬁcation\nof a single factor that may inﬂuence the experimental result. Speciﬁcally, FALSIFICATIONAGENT\nmust select an iteration as the baseline for the ablation study, and FALSIFICATIONAGENT follows\nthe “Experiment Settings” of the baseline, and modify the methodology according to the ablated\nfactor.\nAttempting to reach a robust and reliable conclusion of the ablation study, both baseline and ablation\nexperiments are repeated multiple times. FALSIFICATIONAGENT is given the complete record of\nthese experiments to decide the validity of the associated scientiﬁc principle. If a particular discovery\nwithstands this process and consistently produces results similar to those in the main experiment, it\nis regarded as a veriﬁed and valuable scientiﬁc discovery. And it is falsiﬁed otherwise.\nFormally, the outcome of FALSIFICATIONAGENT, which is also the output of BABY-AIGS, is:\nScientiﬁc Discovery = FALSIFICATIONAGENT (Research Topic | History) ,\n(4)\nwhere\nHistory =\nn\nProposal(i), Exp. Res.(i), Review(i)oM\ni=1 ,\n(5)\nand FALSIFICATIONAGENT(· | ·) indicates the agentic workﬂow. We also provide an example on\nthe data engineering research to better describe the different parts of the output of FALSIFICATION-\nAGENT in BABY-AIGS as follows, in which speciﬁc parts of the methodology are ablated and\nreasonable conclusions are made based on the results of the ablation experiment:\n13\n\nAn example of the falsiﬁcation process from FALSIFICATIONAGENT\nDiscovery Candidate\nKey Factor: Importance of Context and Speciﬁcity.\nAblation Experiment Plan\nConduct an ablation study by systematically removing or altering one element related to con-\ntext retention or speciﬁcity at a time. For example, test the impact of removing speciﬁc instruc-\ntions or reducing context retention by limiting the number of conversational turns accessible\nto the model. This will help identify which speciﬁc factors within context and speciﬁcity\ncontribute most signiﬁcantly to model performance on MT-bench.\nMethodology\nMethodology for Ablation Experiments:{...“Principles”: “...7. Responses should be concise\nand fall within the optimal length range (800-1500 characters).\\n8. Responses should engage\nthe user naturally and be informative.\\n9. Weighting of each principle should be considered\nbased on its importance to the downstream task.\\n10. Incorporate dynamic thresholding to\nadjust based on the number of data points passing the initial ﬁlter.”...}\nMethodology for Baseline Experiments:{...“Principles”: “...7. Responses should be concise\nand fall within the optimal length range (800-1500 characters).\\n8. Responses should demon-\nstrate context retention and follow multi-turn dialogue accurately.\\n9. Responses should en-\ngage the user naturally and be informative.\\n10. Weighting of each principle should be consid-\nered based on its importance to the downstream task.\\n11. Incorporate dynamic thresholding\nto adjust based on the number of data points passing the initial ﬁlter.\\n12. Break down com-\nplex criteria into more speciﬁc sub-criteria to capture nuances better. For example, ’context\nretention’ can include sub-criteria like ’long-term memory’ and ’contextual continuity.’\\n13.\nEvaluate the impact of each criterion through ablation studies.”...}\nExperiment Result\nMetric\nAblation\nBaseline\nTrial 1\nTrial 2\nTrial 1\nTrial 2\nVicuna-Bench (Validation) ↑\n7.1625\n6.7500\n6.475\n6.5375\nMT-Bench (Test) ↑\n4.10625\n4.1125\n4.05625\n3.96875\nVeriﬁcation & Scientiﬁc Discovery\nVeriﬁcation: The discovery regarding the importance of context and speciﬁcity in\nmulti-turn conversations appears to have a negative inﬂuence on the quality of the\ndata for this speciﬁc task. The ablation study results suggest that limiting the num-\nber of conversational turns and focusing on fewer criteria can lead to better\nscores, which implies that the discovery might not positively affect the data qual-\nity as initially thought.\nScientiﬁc Discovery: The true scientiﬁc discovery here is that while maintaining\ncontext and speciﬁcity is generally important, overemphasizing these elements may\nnot always lead to improved performance in all tasks. Simplifying the criteria and\nfocusing on essential conversational elements without excessive context retention can\nlead to better quality results in the context of the MT-bench task.\nTo our knowledge, FALSIFICATIONAGENT is the ﬁrst agent within AI-accelerated scientiﬁc discov-\nery systems capable of autonomously completing the falsiﬁcation process, by independently propos-\ning scientiﬁc discovery candidates, designing and executing ablation experiments, and performing\nveriﬁcation. For a detailed qualitative analysis, see Section 3.5.\n14\n\n3.4\nAUTOMATED FULL-PROCESS RESEARCH EXPERIMENT\n3.4.1\nSELECTED RESEARCH TOPICS\nWe conduct experiments on three primary research topics in machine learning to evaluate BABY-\nAIGS in autonomous full-process research. Formally, let Dk = {(xi, yi)}N\ni=1 denote the k-th bench-\nmark of a given ML problem, where xi represents input features and yi represents the corresponding\nlabels. The goal is building a system f : X →Y that maximizes metric functions Lk(f(x), y) over\nall benchmark Dk. We split benchmarks into validation and test ones, and only the former is avail-\nable in the pre-falsiﬁcation phase, avoiding wrong scientiﬁc discoveries from over-ﬁt results.\nData Engineering\nData engineering is a critical research topic that focuses on the identiﬁcation,\nextraction, and processing of relevant data features that signiﬁcantly inﬂuence model performance.\nWe formulate the research goal as follows: Given a data set H that contains instruction-response\npairs, the goal is to identify the key distinguishing characteristics of H, which in turn enables\nthe system to ﬁlter and extract high-quality data subsets H′ ⊂H for the development of LLMs.\nThis process is crucial to improving the quality and relevance of data for a wide range of areas,\nensuring downstream tasks, such as in-context learning (Brown et al., 2020) and Supervised Fine-\nTuning (SFT) for LLM alignment (Ouyang et al., 2022), are more effective. Speciﬁcally, we lever-\nage Alpaca-GPT4 dataset (Peng et al., 2023) as the dataset H. We follow previous work (Liu et al.,\n2024; Chen et al., 2024b; Li et al., 2024c; Zhao et al., 2024) in this ﬁeld and let the AIGS systems\nwrite principles for LLMs to rate data samples and extract the top rated ones as the reﬁned dataset.\nThus, for BABY-AIGS, we input the description of the topic and design the main DSL as a list of\nrequired principles for the evaluation of the data sample and a threshold indicating the least number\nof principles that a data sample in the reﬁned dataset has to pass.\nSelf-Instruct Alignment\nThe self-instruct alignment (Wang et al., 2023c) is a well adopted data\nsynthesis paradigm for LLM alignment. The objective of this research topic is to synthesize a set\nof SFT data with high quality and diversity for LLM alignment (Ouyang et al., 2022) by rewriting\na seed set of data, thereby enhancing the performance of the ﬁne-tuned model on this dataset. In\nthe research process, an AIGS system is required to construct an optimal set of instructions from\na seed instruction dataset, which are used to generate an instruction-response dataset from LLMs.\nThis dataset is then leveraged to reﬁne the alignment of an LLM via SFT. In the experiment, we\nrewrite the original seed instruction set, and use the same LLM in instruction synthesis and response\ngeneration for SFT data. Speciﬁcally, for BABY-AIGS, the DSL is designed as an option whether\nto use the seed instruction set, and a list of requirements for the given LLM to generate instructions.\nLanguage Modeling\nLanguage modeling is a core research topic in natural language processing\nthat aims to improve the ability of a model to understand and generate human language. Currently,\nthe mainstream approach is generative pre-training (Radford et al., 2018), and the objective is to\nmaximize the perplexity of the next token prediction, i.e. minimize the model perplexity. The AIGS\nsystem seeks to explore different architectural and training schedule modiﬁcations to enhance quality\nof language model pre-trained on large corpora. We designed DSL of the BABY-AIGS system as a\nset of constrained conﬁgurations of model architecture and training hyper-parameters.\nEach of these research topics requires unique methodological innovations of an AIGS system to\nfoster high creativity, executability, and falsiﬁcation capabilities. We demonstrate the pre-deﬁned\ngrammars of BABY-AIGS in Figure 5. Please refer to Appendix B for detailed settings.\n3.4.2\nEVALUATION SETTINGS\nWe evaluate BABY-AIGS based on three key principles central to AIGS systems as proposed in\nSection 3.1: falsiﬁcation, creativity, and executability. We introduce the AI Scientist (Lu et al.,\n2024) as the baseline of the automated research system, and also select published literature from top\nconference as the baseline of research from experienced human researchers.\nFalsiﬁcation\nWe assess BABY-AIGS ’s ability to perform falsiﬁcation through human evaluation,\nfocusing on the falsiﬁcation process carried out by FALSIFICATIONAGENT. This process involves\nhypothesizing potential inﬂuencing factors, identifying the key variables that may impact experi-\nmental results, designing and conducting ablation experiments, and ultimately validating the real\nfactors contributing to the experimental signiﬁcance. The human evaluation is carried out by vol-\nunteer researchers with experience in publishing at top-tier conferences. Evaluators assess the fal-\n15\n\nMetric\nAVG\nSTD\nP-Value\nMIN\nMAX\nImportance Score (0 ∼2)\nBABY-AIGS (Ours)\n1.80\n0.41\n0.02\n0.00\n2.00\nTop Conference\n2.00\n0.00\n—\n2.00\n2.00\nConsistency Score (0 ∼2)\nBABY-AIGS (Ours)\n1.00\n0.86\n0.00\n0.00\n2.00\nTop Conference\n2.00\n0.00\n—\n2.00\n2.00\nCorrectness Score (0 ∼2)\nBABY-AIGS (Ours)\n0.95\n0.94\n0.00\n0.00\n2.00\nTop Conference\n2.00\n0.00\n—\n2.00\n2.00\nOverall Score (0 ∼2)\nBABY-AIGS (Ours)\n1.25\n0.47\n0.00\n0.67\n2.00\nTop Conference\n2.00\n0.00\n—\n2.00\n2.00\nTable 2: Statistic results of human evaluation on the falsiﬁcation process in our data engineering\nresearch experiments.\nsiﬁcation process based on three key dimensions, each scored on a scale from 0 to 2, with a higher\nscore indicating better performance:\n• Importance Score: This score reﬂects the importance of the scientiﬁc discovery candidate.\nIt evaluates the extent to which the identiﬁed factors can inﬂuence the experimental results,\nconsidering their relevance and potential impact with the primary experiments.\n• Consistency Score: This score assesses whether the proposed ablation experiment plan is\naligned with the identiﬁed scientiﬁc discovery candidate. It considers whether the experi-\nments are designed to ablate the factor of interest and appropriately test the hypothesis.\n• Correctness Score: This score evaluates the accuracy of the ﬁnal scientiﬁc discovery de-\nrived from the ablation studies. It considers whether the conclusions drawn from the abla-\ntion experiments and baseline results are correct, based on the observed empirical results.\nAdditionally, several studies from the top conferences (Liu et al., 2024; Chen et al., 2024b; Li et al.,\n2024c; Zhao et al., 2024) are included in the evaluation set to serve as a baseline. We conduct the\nevaluation on the data engineering research experiment, with statistic results shown in Table 2, where\nthe p-values obtained from a left-tailed hypothesis test against the top conference baseline.\nCreativity\nWe measure the creativity of BABY-AIGS by evaluating the performance improvement\nof the proposed idea and methodology against the baseline result, i.e., the result from the trivial\nmethodology on the test benchmarks. Here are the benchmark settings for each research experiment:\n• Data Engineering: For the reﬁned dataset, we conduct 15-shot In-Context Learning\n(ICL) (Jiang et al., 2024) and SFT for LLM alignment to evaluate the overall quality.\nWe evaluate the ICL-aligned LLM on the Vicuna-Bench, as a efﬁcient validation bench-\nmark, and ICL- and the SFT-aligned LLM on the MT-Bench (Zheng et al., 2023), which\nare used as test benchmarks.\nThe baseline of turn 0 uses the original Alpaca-GPT4\ndataset (Peng et al., 2023). We replicate AI Scientist with the same experiment template.\nMoreover, we replicate Deita (Liu et al., 2024) as the human research of the topic from the\ntop conference.\n• Self-Instruct Alignment: We also assess the aligned LLM on the Vicuna-Bench, as the\nvalidation benchmark, and the MT-Bench, as the test benchmark. The baseline of turn 0 is\nthe result of the original self-instruct method (Wang et al., 2023c).\n• Language Modeling: We pre-train a mini-sized language model with the modiﬁed archi-\ntecture based on the conﬁgured training schedule, on three different training sets (Karpathy,\n2015; Hutter, 2006; Mahoney, 2011). The validation and test benchmarks are the perplex-\nity of LM on the split validation and test sets. With reference to Lu et al. (2024), we adopt\nthe default settings of the nanoGPT project5 as the baseline.\nResults on all test benchmarks are in Table 3, Table 4, and Table 5, for each topic, respectively.\n5https://github.com/karpathy/nanoGPT.\n16\n\nMethod\nMT-Bench ↑\n15-shot ICL\nSFT\nBaseline (Turn 0)\n4.18\n4.53\nAI Scientist\n4.36\n4.67\nBABY-AIGS (Ours)\n4.51\n4.77\nTop Conference\n4.45\n5.01\nMethodology Summarization (Data Engineering)\n1. Rate the response based on its contextual coher-\nence, ensuring it logically follows the conversation.\n2. Evaluate the relevance by checking if the answer\nstays on-topic with minimal digression.\n3. Check for logical reasoning in explanations, ensur-\ning the response is not just factual but also thoughtful.\n4. Consider if the complexity and detail match the\nquestion’s requirements, avoiding oversimpliﬁcation.\n5. Finally, evaluate the tone for politeness, clarity, and\nnatural conversational ﬂow.\nTable 3: Benchmarking results on the test benchmarks of the data engineering research experiment\n(left) and a summarization of the corresponding proposed methodology from BABY-AIGS (right).\nMethod\nMT-Bench ↑\nBaseline (Turn 0)\n2.45\nBABY-AIGS (Ours)\n3.26\nMethodology Summarization (Self-Instruct Alignment)\nMake the instruction to cover different scenarios if it lacks speci-\nﬁcity, clearer if ambiguous, aligned with natural conversations,\nand to contain a diverse range of task types if it lacks variety.\nTable 4: Benchmarking results on the test benchmark of the self-instruct alignment research exper-\niment (left) and a summarization of the corresponding proposed methodology from BABY-AIGS\n(right).\nExecutability\nWe evaluate the BABY-AIGS system’s stability to execute research ideas error-\nlessly from ideation to implementation, measured by the success rate of obtaining meaningful exper-\nimental outcomes and scientiﬁc insights, termed as Experiment Success Rate (Exp. SR) and Overall\nSuccess Rate (Overall SR), respectively. We report the overall results on all research experiments on\nthe three topics. AI Scientist as the baseline method, are also evaluated executability on the selected\ntasks in their original implementation (Lu et al., 2024). Results are shown in Table 6.\n3.5\nQUANTITATIVE AND QUALITATIVE ANALYSIS\nBABY-AIGS could produce valid scientiﬁc discoveries with falsiﬁcation process.\nTo validate\nthe falsiﬁcation process in BABY-AIGS, we assess its ability to perform ablation studies and identify\ncausative factors for experimental results. The qualitative analysis in Table 2 shows that FALSIFI-\nCATIONAGENT could produce valid scientiﬁc discoveries in current design, as the maximum value\nof each metric is tied to the top-conference baseline, contributing positively to the automation of sci-\nentiﬁc insights. However, there are two critical ﬁndings that indicate further improvement is needed.\n(1) The average value of the importance score is higher than the consistency and correctness score,\nindicating that FALSIFICATIONAGENT could identify important factors potentially related to a sci-\nentiﬁc discovery but failed to design a concrete experiment plan and verify the hypothesis. The\nfailure could be attribute to the capacity of foundation model or the lack of high-quality demon-\nstration of experiment design in prompts. (2) The p-values indicate that the falsiﬁcation process of\nBABY-AIGS is signiﬁcantly less satisfactory than the existing literature from top conferences from\nhuman perspectives, which emphasizes the importance of designing user-friendly interfaces besides\nreﬁning the design of ablation experiments. Also, we acknowledge that the scale of the study is\nsmall compared to Si et al. (2024), which requires future effort.\nBABY-AIGS demonstrates creativity during research idea exploration and reﬁnement.\nTa-\nble 3, Table 4, and Table 5 show the results of the test benchmarks for data engineering, self-instruct\nalignment, and language modeling research experiments, respectively, where BABY-AIGS outper-\nforms the baseline method, demonstrating the system’s creativity in ideation and corresponding\nmethod design. For data engineering, BABY-AIGS outperforms AI Scientist with a signiﬁcant\nmargin, demonstrating the effectiveness of the enriched feedback, including multi-granular metrics,\nverbose review on both experiment process and methodology design, etc., in exploring research idea.\nHowever, the result of SFT alignment is inferior than Deita (Liu et al., 2024), indicating that the lack\nof validation benchmarking of speciﬁc downstream tasks might result in an suboptimal outcome.\n17\n\nMethod\nPerplexity ↓\nshakespeare char\nenwik8\ntext8\nBaseline (Turn 0)\n1.473\n1.003\n0.974\nBABY-AIGS (Ours)\n1.499\n0.984\n0.966\nMethodology\nSummarization\n(Language Modeling)\nReduce the dropout rate with more at-\ntention heads to increase model ex-\npressiveness. And implement a cycli-\ncal learning rate and adjust the weight\ndecay to regularize the model.\nTable 5: Benchmarking results on the test benchmarks of the language modeling research experiment\n(left) and a summarization of the corresponding proposed methodology from BABY-AIGS (right).\nMethod\nExperiment Success Rate (Exp. SR)\nOverall Success Rate (Overall SR)\nAI Scientist\n44.8%\n29.2%\nBaby-AIGS (Ours)\nAlmost 100%\nAlmost 100%\nTable 6: Success rates on three selected tasks of AI Scientist and Baby-AIGS. Exp. SR denotes the\ntimes a system successfully conducted experiments out of all trials, and Overall SR denotes the times\na system produces the ﬁnal scientiﬁc discoveries. Higher numbers indicate better executability.\nBABY-AIGS has remarkable executability in experimentation and full research process.\nAs\nshown in Table 6, our quantitative analysis highlights signiﬁcant improvements in executability, with\nBABY-AIGS achieving nearly 100% success rates in translating the generated ideas into experimen-\ntal results and the ﬁnal scientiﬁc discovery. This high executability, attributed to our DSL design\nfor errorless experimentation, prevents restarting from in-process failures and enables an efﬁcient\nautomated research process. Detailed API costs are elaborated in Appendix B.2.\n3.6\nDISCUSSIONS\nQ1: How do current LLMs perform in the falsiﬁcation process?\nFalsiﬁcation (Popper, 1935) is\nessential in AIGS systems as it provides a rigorous mechanism for veriﬁcation of potential scientiﬁc\ndiscoveries, a core component in the scientiﬁc method. In BABY-AIGS, FALSIFICATIONAGENT\nplays the corresponding role. Thus, it demands related abilities in the foundation model, such as\nreasonable hypothesis generation, ablation experiment design, summarization and self-correction\nbased on input empirical results, etc. As shown in the case in Section 3.3.5 and Table 2, current\nLLMs are far from desired in the agentic workﬂow of FALSIFICATIONAGENT. Additionally, the\nconstraints may come from the ability of the LLM to understand the environment outside FAL-\nSIFICATIONAGENT. For instance, from our observation, FALSIFICATIONAGENT seldom proposes\nexperiment plans beyond the provided experiment templates. In this case, although DSL makes sure\nthe executability of the experimentation by omitting extra operations, the experiment process would\ndiffer from the original plan, thus creating inconsistency.\nMethod\nBaseline\nTurn 1\nTurn 2\nTurn 3\nTurn 4\nTurn 5\nMulti-Sampling@1\n4.18\n3.68\n4.01\n4.05\n3.88\n3.90\nMulti-Sampling@32\n4.18\n4.02\n4.05\n4.50\n4.51\n4.42\nTable 7: Results on MT-Bench (15-shot ICL) of the ablation study on the multi-sampling strategy of\nour BABY-AIGS system in the data engineering research experiment. N in “Multi-Sampling@N”\nindicates the number of parallel threads of multi-sampling.\nQ2: How does the BABY-AIGS system boost creativity?\nBABY-AIGS enhances creativity by\nintegrating a multi-sampling approach combined with re-ranking, allowing it to generate diverse\nresearch proposals and rank them based on validation benchmarks. We provide detailed results of\nan ablation study of this process in Table 7. We observed that the performance on the test bench-\nmark is steadily increasing with multi-sampling with large numbers of threads. This strategy is\nrelated to search-based inference-cost scaling methods (Snell et al., 2024; Brown et al., 2024). The\ninsight is to pick random high-performing samples for better overall performance. However, since\nthe objective of AIGS is to discover science on a research topic, the reranking method here could\n18\n\nbe large-scale validation benchmarks indicating generalization performance, rather than reward-\nmodel-based (Stiennon et al., 2020) or self-veriﬁcation methods for a speciﬁc query. As depicted in\nSection 3.3.3, we argue that the groundtruth of scientiﬁc laws is rooted and reﬂected in benchmark-\ning results from actual experiments, which could serve as process supervision, which could be more\naccurate than reward models. It explains how collapse in self-reﬁnement-style methods (Xu et al.,\n2024) is avoided in this setting, which is also empirically validated through the ablation results.\nQ3: Why could DSL help with executability?\nThe use of a Domain-Speciﬁc Language (DSL)\nin BABY-AIGS facilitates executability by providing a structured and executable representation of\nideas and methodologies proposed by PROPOSALAGENT. DSL enhances the system’s ability to\ntranslate complex scientiﬁc workﬂows into actionable experiment plans. As shown in Table 6, DSL\nsigniﬁcantly improved success rates in generating scientiﬁc discoveries, regardless of correctness,\nunderscoring its role in achieving high executability. We acknowledge that the design of DSL\nrequires human effort and might not be able to cover all possible method implementations. However,\nwe believe it is a promising interface between agents and experimentation in full-process research.\n4\nLIMITATIONS AND ACTIONABLE INSIGHTS\nEnvisioning the future of AI-Generated Science systems powered by foundation models in real-\nworld, in this section, we enumerate a few limitations for current BABY-AIGS system and provide\ninsights on the next steps of research for AIGS.\nBalance idea diversity and system executability.\nAs discussed in Section 3.3.1, the design of the\nDSL enhances the system executability but may constrain the idea diversity. Achieving a balance\nbetween idea diversity and system executability requires further empirical analysis. One potential\navenue is enabling agents to develop their own DSLs, which could enhance the executability of\ngenerated ideas without diminishing their diverse potential.\nEstablish systematic mechanisms for evaluation and feedback.\nThe quality of AIGS system\ndepends heavily on rigorous evaluation of prior proposals, methods, and results. Current approaches\noften adopt a peer review format, leveraging LLMs to generate feedback on results and guide fu-\nture optimization (Lu et al., 2024; Yu et al., 2024; Jin et al., 2024). However, it remains unclear\nwhether this method is the most effective for large-scale research settings. Future work should ex-\nplore systematic mechanisms to analyze outcomes across iterations, maximizing experience transfer\nand continuous improvement.\nStrengthen the falsiﬁcation procedure.\nOur research underscores the importance of falsiﬁcation\nto enhance the scientiﬁc rigor of the research ﬁndings. While we have prototyped the falsiﬁcation\nprocess in our BABY-AIGS system, more efforts are required to strengthen the modules related to\nknowledge falsiﬁcation, including the exploitation of the patterns and relationships derived from\nhistorical experiments for the guidance of reﬁned research proposals. Besides, it is also vital for\nAIGS systems to investigate whether the delivered new scientiﬁc knowledge could generalize across\ndiverse research domains in an autonomous manner.\nExpand channels for scientiﬁc knowledge dissemination.\nFacilitating the exchange of AI-\nGenerated Science is critical, both between humans and AI and among AI systems. While Lu et al.\n(2024) focus on disseminating knowledge through research papers, alternative formats like posters,\npodcasts, and videos are gaining traction with the rise of multi-modal agents. Future research should\nalso explore more efﬁcient communication channels between AI systems, beyond structured text or\nnatural language (Pham et al., 2024; Chen et al., 2024c).\nExploring communication dynamics among autonomous AI researchers.\nAs discussed in Sec-\ntion 2, the advancement of AI-accelerated scientiﬁc discovery spans four paradigms, culminating in\nthe emergence of an autonomous AI research community (Paradigm IV). Within this community,\nindividual agentic researchers engage in interactions that parallel collaborative dynamics found in\nhuman scientiﬁc networks. Analyzing these communication dynamics is essential to understand\nhow fully-autonomous AI agents might effectively collaborate, exchange knowledge, and drive col-\nlective progress. In particular, a deeper exploration of these interactions in a multi-agent system will\nhelp establish communication frameworks that support optimal collaboration, fostering a robust and\nproductive AI-accelerated research community.\n19\n\nPromote interdisciplinary knowledge integration and experimentation.\nIn this work, we pri-\nmarily focused on the application of AIGS systems within the domain of machine learning, where\nexperiments could be executed in computers. However, future developments should extend these\nsystems to address challenges in other scientiﬁc ﬁelds, such as biology, which has been prelimi-\nnarily explored in a concurrent work (Swanson et al., 2024), chemistry, and physics, where cross-\ndisciplinary knowledge integration is often crucial. One major challenge lies in how AI agents can\nsynthesize and align domain-speciﬁc knowledge from multiple ﬁelds, which often have distinct ter-\nminologies, methodologies, and epistemological assumptions. Another critical challenge is the ex-\nperiment environment, which could be hardly automated and might be highly resource-consuming.\nWe hope the integrity and development of optional modules like DOMAIN-SPECIFIC EXPAGENT\nand ENVIRONMENTAGENT mentioned in Section 3.2 could alleviate the challenges, and further\neffort is needed and will be made in future work.\n5\nETHICS AND IMPACT STATEMENT\nIn our BABY-AIGS system, the agent did not perform harmful operations on computer systems\nor environment because of the design of DSL, task constraints and no access to external tools.\nHowever, while the system developed in this study is limited in scope, AIGS systems as a whole\nmay have signiﬁcant impacts in the future, with potential risks that should not be overlooked. This\nsection explores the potential negative impacts of such systems, drawing on prior research, and\noffers suggestions for promoting their positive development.\n5.1\nPOTENTIAL NEGATIVE IMPACTS OF AIGS SYSTEMS\nImpact on Human Researchers and Academic Community\nIn the absence of robust publica-\ntion standards and academic review processes, AIGS systems could ﬂood the academic community\nwith low-quality literature, which will further increase researchers’ workload and disrupt the efﬁ-\ncient dissemination of knowledge (Lu et al., 2024; Si et al., 2024; Hu et al., 2024b). And although\nSi et al. (2024) and Kumar et al. (2024) suggest that LLMs can generate ideas more creative than\nhumans, the extent of such creativity remains uncertain. LLM-powered AIGS systems tend to rely\nheavily on existing data and patterns, which could foster path dependency and limit opportunities\nfor groundbreaking discoveries. Additionally, these systems might inadvertently use proprietary or\ncopyrighted material, raising concerns about intellectual property infringement (Kumar et al., 2024).\nFurthermore, AIGS systems also present several unpredictable challenges for human researchers:\n• Dependence Effect and Cognitive Inertia: Over-reliance on AI-generated insights may\ndiminish researchers’ independent thinking, leading to cognitive stagnation and a decline\nin critical thinking skills (Si et al., 2024; Hu et al., 2024b).\n• Ambiguity in Responsibility Attribution: The involvement of AI complicates the as-\nsignment of credit and responsibility, potentially disrupting existing incentive struc-\nture (Si et al., 2024; Hu et al., 2024b).\n• Weakened Collaboration and Increased Isolation: As AIGS systems become capable\nof independently generating publishable work, researchers may increasingly rely on these\nsystems, reducing the need for direct collaboration and communication with colleagues.\nThis shift could lead to a decline in interpersonal interaction, weakening traditional re-\nsearch networks built on teamwork and shared discourse (Si et al., 2024; Hu et al., 2024b).\nOver time, the diminishing frequency of collaborative exchanges may foster a sense of\nprofessional isolation among human researchers, heightening the risk of loneliness, disen-\ngagement, and reduced psychological well-being.\n• Exacerbated Technological Barriers: Without equitable access to advanced AIGS sys-\ntems, a technological divide could emerge, disadvantaging researchers unfamiliar with or\nlacking access to these systems, thereby exacerbating inequalities within the community.\nImpact on Environment\nAIGS systems can conduct large-scale experiments in parallel, but their\ndependence on iterative processes carries the risk of inefﬁcient feedback loops, potentially leading\nto issues such as inﬁnite loops. This inefﬁciency, caused by limited reasoning capabilities, the\nmisuse of erroneous information, or ambiguity in task deﬁnition, could drive up energy consumption.\nMoreover, poorly regulated experiments, especially without adequate simulation environments, can\nlead to unintended environmental harm. For example, untested chemical processes in materials\nscience may yield hazardous by-products, while unchecked experiments in nuclear research could\nincrease the risk of radiation leaks (Tang et al., 2024).\n20\n\nImpact on Social Security\nAIGS systems, particularly when compromised by jailbreak attacks,\ncould generate responses that conﬂict with human values, such as providing instructions for creating\nexplosives. This raises concerns about their misuse for harmful purposes, such as designing more ad-\nvanced adversarial attack strategies (Tang et al., 2024; Si et al., 2024; Lu et al., 2024; Kumar et al.,\n2024; Hu et al., 2024b). Even with benign intentions, unsupervised scientiﬁc research may intro-\nduce unforeseen societal risks. For instance, monopolizing breakthroughs in autonomous AI could\nlead to severe unemployment, market monopolies, and social unrest (Tang et al., 2024).\n5.2\nSTRATEGIES FOR RESPONSIBLE AND ETHICAL DEVELOPMENT OF AUTOMATED\nRESEARCH SYSTEMS\nStrengthening the Security of Foundation Models\nThe most fundamental step in mitigating\nsecurity risks associated with AIGS systems is enhancing the security of their foundation models.\nIncorporating instructions for handling unsafe research into the alignment training corpus, alongside\nconducting rigorous safety audits prior to model deployment, are both crucial strategies to ensure\nthe systems be robust and secure (Tang et al., 2024).\nAligning Scientiﬁc Agents with Human Intentions, Environment and Self-constraints\nScien-\ntiﬁc agents in AIGS systems should align with human intentions, the environments in which they\noperate, and self-constraints (Yang et al., 2024c).\n• Human Intentions: Agents must accurately interpret user intent, going beyond literal lan-\nguage to capture the deeper purpose of scientiﬁc inquiries.\n• Environment: Agents need to adapt to the speciﬁc environments in which they function by\napplying domain-speciﬁc knowledge accurately and utilizing specialized tools effectively.\n• Self-Constraints: Agents must evaluate task feasibility, manage resources wisely, and min-\nimize waste to ensure sustainable operation. This includes setting boundaries to prevent\nredundant work or harmful behavior, which is essential for maintaining system efﬁciency.\nProviding Comprehensive Training for Human Users\nComprehensive and rigorous training is\nessential for users to fully leverage AIGS systems and prevent unintended consequences (Aidan,\n2024). Proper training minimizes the risk of misuse that could lead to environmental harm, resource\nwaste, or unethical research outcomes. Training programs should focus not only on technical skills\nbut also on ethical considerations, ensuring users understand the limitations and responsibilities\nassociated with these systems (Tang et al., 2024).\nBuilding a Collaborative Framework Between Automated Research Systems and Human Re-\nsearchers\nTo prevent AIGS systems from exerting excessive inﬂuence on the academic commu-\nnity, collaboration between AIGS systems and human researchers will play a crucial role (Si et al.,\n2024; Hu et al., 2024b). It is essential to explore the new roles and responsibilities that human sci-\nentists may need to assume in this evolving research landscape shaped by the presence of AIGS\nsystems. A well-structured partnership can leverage the complementary strengths of both, enabling\noutcomes that neither could achieve independently. Moreover, such collaboration fosters interaction\namong human researchers, encouraging deeper communication and mitigating the sense of isolation\nthat may arise from increased reliance on automated tools.\nEstablishing Comprehensive Legal and Accountability Frameworks\nA robust legal and ac-\ncountability framework is crucial to govern the use of AIGS systems. This framework should:\n• Deﬁne Clear Scientiﬁc Research Boundaries: Specify the permissible scope and limita-\ntions of these systems, where regulate agents with the DSL might be helpful.\n• Clarify Responsibility and Credit Allocation: Establish guidelines for assigning credit\nand responsibility for research outcomes generated with the assistance of AIGS sys-\ntems (Si et al., 2024; Hu et al., 2024b).\n• Implement Penalties for Misuse: Outline liability measures and penalties to address\nharmful behavior or unethical practices involving these systems.\nUsing AIGS Systems to Address Its Own Challenges\nAIGS systems can also play a proactive\nrole in addressing the challenges and even ethical issues introduced by themselves. For exam-\nple, AIGS systems could be used to monitor and evaluate outputs from other automated systems,\n21\n\nidentifying potential ethical issues, biases, or environmental risks before they escalate. Moreover,\nAIGS systems can facilitate the development of guidelines, by automating the analysis of research\ntrends and regulatory needs, thus helping shape future policies for responsible AI use. When em-\nployed strategically, AIGS systems become not only tools for discovery but also mechanisms for\nself-regulation, creating a virtuous cycle of innovation and governance.\n6\nCONCLUSION\nWe introduce the concept of AIGS in this paper and implement BABY-AIGS, a baby-step toward\nfull-process automated scientiﬁc discovery systems, with a focus on incorporating falsiﬁcation into\nthe research process. By integrating a FALSIFICATIONAGENT, the multi-agent system can identify\nand verify potential discoveries. Techniques as DSL and multi-sampling strategy are introduced\nfor two other principles of AIGS systems design, executability and creativity. Preliminary exper-\niments show promise, though the system’s performance remains below that of experienced human\nresearchers. This work lays the groundwork for future developments in AIGS systems, with further\nimprovements over BABY-AIGS and ethical considerations necessary for advancing the ﬁeld.\nREFERENCES\nJosh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf\nRonneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, et al. Accurate structure\nprediction of biomolecular interactions with AlphaFold 3. Nature, pp. 1–3, 2024.\nJosh\nAchiam,\nSteven\nAdler,\nSandhini Agarwal,\nLama\nAhmad,\nIlge\nAkkaya,\nFloren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anad-\nkat, et al.\nGPT-4 technical report.\nArXiv preprint, abs/2303.08774, 2023.\nURL\nhttps://arxiv.org/abs/2303.08774.\nToner-Rodgers Aidan. Artiﬁcial Intelligence, Scientiﬁc Discovery, and Product Innovation. preprint,\n2024. URL https://aidantr.github.io/files/AI_innovation.pdf.\nAlberto Alfarano, Franc¸ois Charton, and Amaury Hayat.\nGlobal Lyapunov functions:\na\nlong-standing open problem in mathematics, with symbolic transformers.\nArXiv preprint,\nabs/2410.08304, 2024. URL https://arxiv.org/abs/2410.08304.\nJinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. Researchagent: Iterative\nresearch idea generation over scientiﬁc literature with large language models. arXiv preprint\narXiv:2404.07738, 2024.\nSteven Bird and Edward Loper. NLTK: The natural language toolkit. In Proceedings of the ACL\nInteractive Poster and Demonstration Sessions, pp. 214–217, Barcelona, Spain, July 2004. Asso-\nciation for Computational Linguistics. URL https://aclanthology.org/P04-3031.\nBradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher R´e, and\nAzalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling.\narXiv preprint arXiv:2407.21787, 2024. URL https://arxiv.org/abs/2407.21787.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv preprint,\nabs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165.\nJun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio\nStarace, Kevin Liu, Leon Maksin, Tejal Patwardhan, et al. MLE-bench: Evaluating machine\nlearning agents on machine learning engineering. ArXiv preprint, abs/2410.07095, 2024. URL\nhttps://arxiv.org/abs/2410.07095.\nKexin Chen, Junyou Li, Kunyi Wang, Yuyang Du, Jiahui Yu, Jiamin Lu, Lanqing Li, Jiezhong Qiu,\nJianzhang Pan, Yi Huang, Qun Fang, Pheng Ann Heng, and Guangyong Chen. Chemist-X: Large\nlanguage model-empowered agent for reaction condition recommendation in chemical synthesis,\n2024a. URL https://arxiv.org/abs/2311.10776.\n22\n\nLichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay\nSrinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training a Better Alpaca\nModel with Fewer Data. In The Twelfth International Conference on Learning Representations,\n2024b. URL https://openreview.net/forum?id=FdVXgSJhvz.\nWeize Chen, Chenfei Yuan, Jiarui Yuan, Yusheng Su, Chen Qian, Cheng Yang, Ruobing Xie,\nZhiyuan Liu, and Maosong Sun. Beyond natural language: LLMs leveraging alternative for-\nmats for enhanced reasoning and communication. In Yaser Al-Onaizan, Mohit Bansal, and Yun-\nNung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp.\n10626–10641, Miami, Florida, USA, November 2024c. Association for Computational Linguis-\ntics. URL https://aclanthology.org/2024.findings-emnlp.623.\nZiru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao,\nChen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-\nAmpratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, and Huan Sun. ScienceAgentBench:\nToward rigorous assessment of language agents for data-driven scientiﬁc discovery. arXiv preprint\narXiv:2410.05080, 2024d.\nJiale Cheng, Yida Lu, Xiaotao Gu, Pei Ke, Xiao Liu, Yuxiao Dong, Hongning Wang, Jie Tang,\nand Minlie Huang.\nAutoDetect: Towards a uniﬁed framework for automated weakness de-\ntection in large language models.\nIn Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen\n(eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 6786–\n6803, Miami, Florida, USA, November 2024. Association for Computational Linguistics. URL\nhttps://aclanthology.org/2024.findings-emnlp.397.\nMike D’Arcy, Tom Hope, Larry Birnbaum, and Doug Downey.\nMARG: Multi-Agent re-\nview generation for scientiﬁc papers.\nArXiv preprint, abs/2401.04259, 2024.\nURL\nhttps://arxiv.org/abs/2401.04259.\nJonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco\nCarpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Mag-\nnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602(7897):414–\n419, 2022.\nJiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Zou,\nPranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Haoran Zhang, Vipul Gupta, Yinghui\nLi, Tao Li, Fei Wang, Qin Liu, Tianlin Liu, Pengzhi Gao, Congying Xia, Chen Xing, Cheng Ji-\nayang, Zhaowei Wang, Ying Su, Raj Shah, Ruohao Guo, Jing Gu, Haoran Li, Kangda Wei, Zihao\nWang, Lu Cheng, Surangika Ranathunga, Meng Fang, Jie Fu, Fei Liu, Ruihong Huang, Eduardo\nBlanco, Yixin Cao, Rui Zhang, Philip Yu, and Wenpeng Yin. LLMs assist NLP researchers: Cri-\ntique paper (meta-)reviewing. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.),\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp.\n5081–5099, Miami, Florida, USA, November 2024. Association for Computational Linguistics.\nURL https://aclanthology.org/2024.emnlp-main.292.\nChen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, and Yong\nLi. Large language models empowered agent-based modeling and simulation: A survey and\nperspectives. Humanities and Social Sciences Communications, 11(1):1–24, 2024.\nMingmeng Geng and Roberto Trotta. Is ChatGPT transforming academics’ writing style? ArXiv\npreprint, abs/2404.08627, 2024. URL https://arxiv.org/abs/2404.08627.\nDaniel George and EA Huerta. Deep neural networks to enable real-time multimessenger astro-\nphysics. Physical Review D, 97(4):044039, 2018.\nAlireza Ghafarollahi and Markus J Buehler. ProtAgents: protein discovery via large language model\nmulti-agent collaborations combining physics and machine learning. Digital Discovery, 2024.\nSoumya Suvra\nGhosal,\nSouradip\nChakraborty,\nJonas\nGeiping,\nFurong\nHuang,\nDinesh\nManocha,\nand Amrit Singh Bedi.\nTowards possibilities & impossibilities of AI-\ngenerated text detection:\nA survey.\nArXiv preprint, abs/2310.15264, 2023.\nURL\nhttps://arxiv.org/abs/2310.15264.\nYolanda Gil, Ewa Deelman, Mark Ellisman, Thomas Fahringer, Geoffrey Fox, Dennis Gannon,\nCarole Goble, Miron Livny, Luc Moreau, and Jim Myers. Examining the challenges of scientiﬁc\nworkﬂows. Computer, 40(12):24–32, 2007.\n23\n\nJustin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural\nmessage passing for quantum chemistry. In Doina Precup and Yee Whye Teh (eds.), Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia,\n6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 1263–1272.\nPMLR, 2017. URL http://proceedings.mlr.press/v70/gilmer17a.html.\nKaran Girotra, Lennart Meincke, Christian Terwiesch, and Karl T Ulrich. Ideas are dimes a dozen:\nLarge language models for idea generation in innovation. Available at SSRN 4526071, 2023.\nMichael Haman and Milan ˇSkoln´ık. Using ChatGPT to conduct a literature review. Accountability\nin research, 31(8):1244–1246, 2024.\nWenpin Hou and Zhicheng Ji. Assessing GPT-4 for cell type annotation in single-cell RNA-seq\nanalysis. Nature Methods, pp. 1–4, 2024.\nChao-Chun Hsu, Erin Bransom, Jenna Sparks, Bailey Kuehl, Chenhao Tan, David Wadden, Lucy\nWang, and Aakanksha Naik. CHIME: LLM-assisted hierarchical organization of scientiﬁc studies\nfor literature review support. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings\nof the Association for Computational Linguistics: ACL 2024, pp. 118–132, Bangkok, Thailand,\nAugust 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.ﬁndings-acl.8.\nURL https://aclanthology.org/2024.findings-acl.8.\nEdward\nJ\nHu,\nYelong\nShen,\nPhillip\nWallis,\nZeyuan\nAllen-Zhu,\nYuanzhi\nLi,\nShean\nWang,\nLu\nWang,\nand\nWeizhu\nChen.\nLoRA: Low-rank adaptation of large lan-\nguage models.\nIn International Conference on Learning Representations, 2022.\nURL\nhttps://openreview.net/forum?id=nZeVKeeFYf9.\nShengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. ArXiv preprint,\nabs/2408.08435, 2024a. URL https://arxiv.org/abs/2408.08435.\nXiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin,\nLili Pan, and Zhenzhong Lan. Nova: An iterative planning and search approach to enhance\nnovelty and diversity of LLM generated ideas. ArXiv preprint, abs/2410.14255, 2024b. URL\nhttps://arxiv.org/abs/2410.14255.\nWenyue Hua,\nLizhou Fan,\nLingyao Li,\nKai Mei,\nJianchao Ji,\nYingqiang Ge,\nLibby\nHemphill, and Yongfeng Zhang.\nWar and Peace (WarAgent):\nLarge language model-\nbased multi-agent simulation of world wars.\nArXiv preprint, abs/2311.17227, 2023.\nURL\nhttps://arxiv.org/abs/2311.17227.\nJingshan Huang and Ming Tan. The role of ChatGPT in scientiﬁc communication: writing better\nscientiﬁc review articles. American journal of cancer research, 13(4):1148, 2023.\nMarcus Hutter. The hutter prize, 2006. URL http://prize.hutter1.net.\nPaul Jaccard. ´Etude comparative de la distribution ﬂorale dans une portion des alpes et des jura.\nBull Soc Vaudoise Sci Nat, 37:547–579, 1901.\nPeter Jansen, Marc-Alexandre Cˆot´e, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bod-\nhisattwa Prasad Majumder, Oyvind Tafjord, and Peter Clark. DISCOVERYWORLD: A virtual\nenvironment for developing and evaluating automated scientiﬁc discovery agents. arXiv preprint\narXiv:2406.06769, 2024.\nYixing Jiang, Jeremy Andrew Irvin, Ji Hun Wang, Muhammad Ahmed Chaudhry, Jonathan H\nChen,\nand Andrew Y. Ng.\nMany-shot in-context learning in multimodal founda-\ntion\nmodels.\nIn\nICML\n2024\nWorkshop\non\nIn-Context\nLearning,\n2024.\nURL\nhttps://openreview.net/forum?id=j2rKwWXdcz.\nCarlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Oﬁr Press, and\nKarthik R Narasimhan.\nSWE-bench:\nCan language models resolve real-world github is-\nsues?\nIn The Twelfth International Conference on Learning Representations, 2024.\nURL\nhttps://openreview.net/forum?id=VTF8yNQM66.\nYiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, and Jindong Wang. Agen-\ntReview: Exploring peer review dynamics with LLM agents. ArXiv preprint, abs/2406.12708,\n2024. URL https://arxiv.org/abs/2406.12708.\n24\n\nYongfei Juan, Yongbing Dai, Yang Yang, and Jiao Zhang. Accelerating materials discovery using\nmachine learning. Journal of Materials Science & Technology, 79:178–190, 2021.\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,\nKathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al. Highly accurate\nprotein structure prediction with AlphaFold. Nature, 596(7873):583–589, 2021.\nAndrej Karpathy.\nThe unreasonable effectiveness of recurrent neural networks, 2015.\nURL\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/.\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein.\nA watermark for large language models.\nIn Andreas Krause, Emma Brunskill, Kyunghyun\nCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Confer-\nence on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume\n202 of Proceedings of Machine Learning Research, pp. 17061–17084. PMLR, 2023.\nURL\nhttps://proceedings.mlr.press/v202/kirchenbauer23a.html.\nSandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, and Asif Ekbal. Can large language mod-\nels unlock novel scientiﬁc research ideas?\nArXiv preprint, abs/2409.06185, 2024.\nURL\nhttps://arxiv.org/abs/2409.06185.\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,\n2015.\nJunkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and Yang\nLiu. Agent Hospital: A simulacrum of hospital with evolvable medical agents. ArXiv preprint,\nabs/2405.02957, 2024a. URL https://arxiv.org/abs/2405.02957.\nRuochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. MLR-Copilot: Autonomous machine\nlearning research based on large language models agent. arXiv preprint arXiv:2408.14033,2024b.\nYunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Ling-\nHao Chen, Junhao Liu, Tongliang Liu, Fei Huang, and Yongbin Li.\nOne-shot learning as\ninstruction data prospector for large language models.\nIn Lun-Wei Ku, Andre Martins, and\nVivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pp. 4586–4601, Bangkok, Thailand, August\n2024c. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.252. URL\nhttps://aclanthology.org/2024.acl-long.252.\nYutong Li, Lu Chen, Aiwei Liu, Kai Yu, and Lijie Wen. ChatCite: LLM agent with human workﬂow\nguidance for comparative literature summary. ArXiv preprint, abs/2403.02574, 2024d. URL\nhttps://arxiv.org/abs/2403.02574.\nWeixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao\nChen, Haotian Ye, Sheng Liu, Zhi Huang, et al. Monitoring AI-modiﬁed content at scale: A case\nstudy on the impact of ChatGPT on AI conference peer reviews. ArXiv preprint, abs/2403.07183,\n2024a. URL https://arxiv.org/abs/2403.07183.\nWeixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao,\nHancheng Cao,\nSheng Liu,\nSiyu He,\nZhi Huang,\net al.\nMapping the increasing\nuse of LLMs in scientiﬁc papers.\nArXiv preprint,\nabs/2404.01268, 2024b.\nURL\nhttps://arxiv.org/abs/2404.01268.\nWei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.\nWhat Makes Good Data\nfor Alignment?\nA Comprehensive Study of Automatic Data Selection in Instruction Tun-\ning.\nIn The Twelfth International Conference on Learning Representations, 2024.\nURL\nhttps://openreview.net/forum?id=BTKAeLqLMw.\nYuliang Liu, Xiangru Tang, Zefan Cai, Junjie Lu, Yichi Zhang, Yanjun Shao, Zexuan Deng, Helan\nHu, Zengxian Yang, Kaikai An, et al. ML-Bench: Evaluating large language models and agents\nfor machine learning tasks on repository-level code. ArXiv preprint, abs/2311.09835, 2023. URL\nhttps://arxiv.org/abs/2311.09835.\nRenze Lou, Hanzi Xu, Sijia Wang, Jiangshu Du, Ryo Kamoi, Xiaoxin Lu, Jian Xie, Yux-\nuan Sun, Yusen Zhang, Jihyun Janice Ahn, Hongchao Fang, Zhuoyang Zou, Wenchao Ma,\nXi Li, Kai Zhang, Congying Xia, Lifu Huang, and Wenpeng Yin.\nAAAR-1.0:\nAs-\nsessing AI’s potential to assist research.\nArXiv preprint, abs/2410.22394, 2024.\nURL\nhttps://arxiv.org/abs/2410.22394.\n25\n\nChris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI Scien-\ntist: Towards fully automated open-ended scientiﬁc discovery. ArXiv preprint, abs/2408.06292,\n2024. URL https://arxiv.org/abs/2408.06292.\nAndres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe\nSchwaller. Augmenting large language models with chemistry tools. Nature Machine Intelli-\ngence, pp. 1–11, 2024.\nPingchuan Ma, Tsun-Hsuan Wang, Minghao Guo, Zhiqing Sun, Joshua B Tenenbaum, Daniela Rus,\nChuang Gan, and Wojciech Matusik. LLM and simulation as bilevel optimizers: A new paradigm\nto advance physical scientiﬁc discovery.\nIn International Conference on Machine Learning.\nPMLR, 2024.\nMatt Mahoney. About the test data, 2011. URL http://mattmahoney.net/dc/textdata.html.\nBenjamin S Manning, Kehang Zhu, and John J Horton. Automated social science: Language models\nas scientist and subjects. Technical report, National Bureau of Economic Research, 2024.\nNat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Tre-\nbacz, and Jan Leike. LLM critics help catch LLM bugs. ArXiv preprint, abs/2407.00215, 2024.\nURL https://arxiv.org/abs/2407.00215.\nMarjan Mernik, Jan Heering, and Anthony M. Sloane. When and how to develop domain-speciﬁc\nlanguages. ACM Comput. Surv., 37(4):316–344, 2005. ISSN 0360-0300. doi: 10.1145/1118890.\n1118892. URL https://doi.org/10.1145/1118890.1118892.\nHarshit Nigam, Manasi Patwardhan, Lovekesh Vig, and Gautam Shroff.\nAcceleron:\nA\ntool to accelerate research ideation.\nArXiv preprint, abs/2403.04382, 2024a.\nURL\nhttps://arxiv.org/abs/2403.04382.\nHarshit Nigam, Manasi Patwardhan, Lovekesh Vig, and Gautam Shroff.\nAn interactive Co-\nPilot for accelerated research ideation.\nIn Su Lin Blodgett, Amanda Cercas Curry, Sunipa\nDey, Michael Madaio, Ani Nenkova, Diyi Yang, and Ziang Xiao (eds.), Proceedings of the\nThird Workshop on Bridging Human–Computer Interaction and Natural Language Processing,\npp. 60–73, Mexico City, Mexico, 2024b. Association for Computational Linguistics.\nURL\nhttps://aclanthology.org/2024.hcinlp-1.6.\nLong\nOuyang,\nJeffrey\nWu,\nXu\nJiang,\nDiogo\nAlmeida,\nCarroll\nWainwright,\nPamela\nMishkin,\nChong Zhang,\nSandhini Agarwal,\nKatarina Slama,\nAlex Ray,\nJohn Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter\nWelinder, Paul F Christiano, Jan Leike, and Ryan Lowe.\nTraining language models\nto follow instructions with human feedback.\nIn S. Koyejo, S. Mohamed, A. Agar-\nwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Pro-\ncessing Systems, volume 35, pp. 27730–27744. Curran Associates, Inc., 2022.\nURL\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f5880\nBhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and\nMarco Tulio Ribeiro. ART: Automatic multi-step reasoning and tool-use for large language mod-\nels. arXiv preprint, abs/2303.09014, 2023. URL https://arxiv.org/abs/2303.09014.\nJoon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael S\nBernstein. Social simulacra: Creating populated prototypes for social computing systems. In\nProceedings of the 35th Annual ACM Symposium on User Interface Software and Technology, pp.\n1–18, 2022.\nJoon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings\nof the 36th annual acm symposium on user interface software and technology, pp. 1–22, 2023.\nBaolin Peng,\nChunyuan Li,\nPengcheng He,\nMichel Galley,\nand Jianfeng Gao.\nIn-\nstruction\ntuning\nwith\nGPT-4.\nArXiv\npreprint,\nabs/2304.03277,\n2023.\nURL\nhttps://arxiv.org/abs/2304.03277.\nChau Pham, Boyi Liu, Yingxiang Yang, Zhengyu Chen, Tianyi Liu, Jianbo Yuan, Bryan A. Plum-\nmer, Zhaoran Wang, and Hongxia Yang. Let models speak ciphers: Multiagent debate through\nembeddings. In International Conference on Learning Representations (ICLR), 2024.\n26\n\nKarl R. Popper. The Logic of Scientiﬁc Discovery. Routledge, London, England, 1935.\nBiqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Zhang-Ren Chen, and Bowen Zhou.\nLarge language models are zero shot hypothesis proposers. ArXiv preprint, abs/2311.05965,2023.\nURL https://arxiv.org/abs/2311.05965.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, et al. ToolLLM: Facilitating large language models to master 16000+ real-world\nAPIs. In International Conference on Learning Representations (ICLR), 2024.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding by generative pre-training. 2018.\nBernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog,\nM Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang,\nOmar Fawzi, et al. Mathematical discoveries from program search with large language models.\nNature, 625(7995):468–475, 2024.\nAlvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W.\nBattaglia. Learning to simulate complex physics with graph networks. In Proceedings of the 37th\nInternational Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,\nvolume 119 of Proceedings of Machine Learning Research, pp. 8459–8468. PMLR, 2020. URL\nhttp://proceedings.mlr.press/v119/sanchez-gonzalez20a.html.\nAbheesht Sharma, Gunjan Chhablani, Harshit Pandey, and Rajaswa Patil. DRIFT: A toolkit for\ndiachronic analysis of scientiﬁc literature.\nIn Heike Adel and Shuming Shi (eds.), Proceed-\nings of the 2021 Conference on Empirical Methods in Natural Language Processing: System\nDemonstrations, EMNLP 2021, Online and Punta Cana, Dominican Republic, 7-11 November,\n2021, pp. 361–371. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.\nEMNLP-DEMO.40. URL https://doi.org/10.18653/v1/2021.emnlp-demo.40.\nParshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K Reddy.\nLLM-SR: Scientiﬁc equation discovery via programming with large language models. arXiv\npreprint arXiv:2404.18400, 2024.\nChenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can LLMs generate novel research ideas? ArXiv\npreprint, abs/2409.04109, 2024. URL https://arxiv.org/abs/2409.04109.\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute opti-\nmally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314,\nabs/2408.03314, 2024. URL https://arxiv.org/abs/2408.03314.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul Christiano. Learning to summarize from human feedback. In Proceed-\nings of the 34th International Conference on Neural Information Processing Systems, NIPS ’20,\nRed Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\nHaoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang,\nand Nanqing Dong.\nTwo heads are better than one:\nA multi-agent system has the po-\ntential to improve scientiﬁc idea generation.\nArXiv preprint, abs/2410.09403, 2024.\nURL\nhttps://arxiv.org/abs/2410.09403.\nLu Sun, Aaron Chan, Yun Seo Chang, and Steven P Dow. ReviewFlow: Intelligent scaffolding\nto support academic peer reviewing. In Proceedings of the 29th International Conference on\nIntelligent User Interfaces, pp. 120–137, 2024.\nKyle\nSwanson,\nWesley\nWu,\nNash\nL.\nBulaong,\nJohn\nE.\nPak,\nand\nJames\nZou.\nThe\nvirtual\nlab:\nAi\nagents\ndesign\nnew\nsars-cov-2\nnanobodies\nwith\nexperi-\nmental\nvalidation.\nbioRxiv,\n2024.\ndoi:\n10.1101/2024.11.11.623004.\nURL\nhttps://www.biorxiv.org/content/early/2024/11/12/2024.11.11.623004.\nXiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng\nQu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, et al.\nPrioritizing safeguarding over au-\ntonomy: Risks of LLM agents for science.\nArXiv preprint, abs/2402.04247, 2024.\nURL\nhttps://arxiv.org/abs/2402.04247.\n27\n\nWei Tao, Yucheng Zhou, Wenqiang Zhang, and Yu Cheng.\nMAGIS: LLM-based multi-\nagent framework for GitHub issue resolution.\nArXiv preprint, abs/2403.17927, 2024.\nURL\nhttps://arxiv.org/abs/2403.17927.\nTrieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry\nwithout human demonstrations. Nature, 625(7995):476–482, 2024.\nKeith Tyser, Ben Segev, Gaston Longhitano, Xin-Yu Zhang, Zachary Meeks, Jason Lee, Uday Garg,\nNicholas Belsten, Avi Shporer, Madeleine Udell, et al. AI-driven review systems: Evaluating\nLLMs in scalable and bias-aware academic reviews. arXiv preprint arXiv:2408.10365, 2024.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez,\nLukasz Kaiser,\nand Illia Polosukhin.\nAttention is all you need.\nIn Is-\nabelle Guyon,\nUlrike von Luxburg, Samy Bengio,\nHanna M. Wallach,\nRob Fergus,\nS. V. N. Vishwanathan,\nand Roman Garnett (eds.),\nAdvances in Neural Information\nProcessing Systems 30:\nAnnual Conference on Neural Information Processing\nSys-\ntems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017.\nURL\nhttps://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abs\nHanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak,\nShengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientiﬁc discovery in the age of artiﬁcial\nintelligence. Nature, 620(7972):47–60, 2023a.\nRui Wang, Hongsong Feng, and Guo-Wei Wei.\nChatGPT in drug discovery: A case study on\nanticocaine addiction drug development with Chatbots. Journal of chemical information and\nmodeling, 63(22):7189–7209, 2023b.\nRuocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D. Goodman.\nHypothesis search: Inductive reasoning with language models. In The Twelfth International Con-\nference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenRe-\nview.net, 2024a. URL https://openreview.net/forum?id=G7UtIGQmjm.\nXingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan,\nYueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng,\nBill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert\nBrennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An open platform for AI software\ndevelopers as generalist agents, 2024b. URL https://arxiv.org/abs/2407.16741.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-Instruct: Aligning language models with self-generated instructions.\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13484–\n13508, Toronto, Canada, July 2023c. Association for Computational Linguistics. doi: 10.18653/\nv1/2023.acl-long.754. URL https://aclanthology.org/2023.acl-long.754.\nWenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Wang. Pride\nand prejudice: LLM ampliﬁes self-bias in self-reﬁnement. In Lun-Wei Ku, Andre Martins, and\nVivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pp. 15474–15492, Bangkok, Thailand, August\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.826. URL\nhttps://aclanthology.org/2024.acl-long.826.\nYuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu.\nExploring large language models for communication games: An empirical study on werewolf.\narXiv preprint arXiv:2309.04658, 2023. URL https://arxiv.org/abs/2309.04658.\nJohn Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan,\nand Oﬁr Press. SWE-agent: Agent-Computer interfaces enable automated software engineering,\n2024a.\nXianjun Yang, Liangming Pan, Xuandong Zhao, Haifeng Chen, Linda Petzold, William Yang\nWang,\nand Wei Cheng.\nA survey on detection of LLMs-generated content.\nIn\nYaser Al-Onaizan,\nMohit Bansal,\nand Yun-Nung Chen (eds.),\nFindings of the As-\nsociation\nfor\nComputational\nLinguistics:\nEMNLP\n2024,\npp.\n9786–9805,\nMiami,\nFlorida,\nUSA, November 2024b. Association\nfor Computational Linguistics.\nURL\nhttps://aclanthology.org/2024.findings-emnlp.572.\n28\n\nZonghan Yang, An Liu, Zijun Liu, Kaiming Liu, Fangzhou Xiong, Yile Wang, Zeyuan Yang,\nQingyuan Hu, Xinrui Chen, Zhenhe Zhang, Fuwen Luo, Zhicheng Guo, Peng Li, and Yang Liu.\nPosition: Towards uniﬁed alignment between agents, humans, and environment. In Forty-ﬁrst\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024.\nOpenReview.net, 2024c. URL https://openreview.net/forum?id=DzLna0cFL1.\nJianxiang Yu, Zichen Ding, Jiaqi Tan, Kangyang Luo, Zhenmin Weng, Chenghua Gong,\nLong Zeng, RenJing Cui, Chengcheng Han, Qiushi Sun, Zhiyong Wu, Yunshi Lan, and\nXiang Li.\nAutomated peer reviewing in paper SEA: Standardization, evaluation, and\nanalysis.\nIn Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings\nof the Association for Computational Linguistics:\nEMNLP 2024, pp. 10164–10184, Mi-\nami, Florida, USA, November 2024. Association for Computational Linguistics.\nURL\nhttps://aclanthology.org/2024.findings-emnlp.595.\nWeihao Zeng, Can Xu, Yingxiu Zhao, Jian-Guang Lou, and Weizhu Chen. Automatic instruction\nevolving for large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen\n(eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Pro-\ncessing, pp. 6998–7018, Miami, Florida, USA, November 2024. Association for Computational\nLinguistics. URL https://aclanthology.org/2024.emnlp-main.397.\nJiayi Zhang,\nJinyu Xiang,\nZhaoyang Yu,\nFengwei Teng,\nXionghui Chen,\nJiaqi Chen,\nMingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang,\net al.\nAFlow:\nAutomat-\ning agentic workﬂow generation.\nArXiv preprint,\nabs/2410.10762,\n2024a.\nURL\nhttps://arxiv.org/abs/2410.10762.\nRuisi Zhang, Shehzeen Samarah Hussain, Paarth Neekhara, and Farinaz Koushanfar. {REMARK-\nLLM}: A robust and efﬁcient watermarking framework for generative large language models. In\n33rd USENIX Security Symposium (USENIX Security 24), pp. 1813–1830, 2024b.\nHao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Long is more for\nalignment: A simple but tough-to-beat baseline for instruction ﬁne-tuning. In Ruslan Salakhut-\ndinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix\nBerkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, vol-\nume 235 of Proceedings of Machine Learning Research, pp. 60674–60703. PMLR, 21–27 Jul\n2024. URL https://proceedings.mlr.press/v235/zhao24b.html.\nXuandong Zhao, Yu-Xiang Wang, and Lei Li.\nProtecting language generation models via\ninvisible watermarking.\nIn Andreas Krause, Emma Brunskill, Kyunghyun Cho, Bar-\nbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on\nMachine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202\nof Proceedings of Machine Learning Research, pp. 42187–42199. PMLR, 2023.\nURL\nhttps://proceedings.mlr.press/v202/zhao23i.html.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In Alice Oh, Tristan Naumann,\nAmir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural\nInformation Processing Systems 36: Annual Conference on Neural Information Processing\nSystems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL\nhttp://papers.nips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-\nYangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, and Chenhao Tan.\nHypoth-\nesis generation with large language models.\nArXiv preprint, abs/2404.04326, 2024.\nURL\nhttps://arxiv.org/abs/2404.04326.\nA\nIMPLEMENTATION DETAILS OF THE BABY-AIGS SYSTEM\nIn this section, we elaborate the implementation details of the BABY-AIGS system. All artifacts are\nused as intended with their license strictly followed in our work.\n29\n\nA.1\nRESEARCH-AGNOSTIC IMPLEMENTATION\nSystem Pipeline\nWe posit that all agents mentioned in Section 3.2 contribute to a full-process\nAIGS system, but based on preliminary experiments, we simplify the design of EXPAGENT and\nLITERATUREAGENT to a large extent in our implementation. For EXPAGENT, given the design of\nDSL with human effort, proposed methodology generated by PROPOSALAGENT can be executed\nreliably in experiments, which is also shown in Section 6. This reduces the need of iteratively reﬁn-\ning proposals between PROPOSALAGENT and EXPAGENT. For LITERATUREAGENT, preliminary\nresults show literature integration did not signiﬁcantly impact the outcomes in both phases of BABY-\nAIGS. We conclude the reason as that agents failed to understand the in-depth literature information\nand the retrieval of literature did not match the need of each agent perfectly. Therefore, in our im-\nplementation, we minimize the design of these two agents: EXPAGENT functions through ﬁxed\ncode, and LITERATUREAGENT was not put into pratical use. Other optional agents are designed to\nfunction in broader research ﬁelds, and we chose to omit them in experiments based on the selected\nresearch topics for experiments (Section 3.4).\nHyper-Parameters\nExperiments in ICL (In Context Learning) of the data engineering research\nand in language modeling research are conducted on 8 NVIDIA GeForce RTX 3090 24 GB GPUs.\nExperiments in SFT (Supervised Fine-tuning) of the data engineering research and in Self-Instruct\nalignment research are conducted on 8 A100 80GB GPUs. All researches utilize the gpt-4o-2024-\n05-13 model as the underlying model for our agents. When agents invoke GPT-4o, we use the\nopenai module6 with a temperature setting of 0.7, while all other parameters are setting as default\nvalues. During the synthesis of proposals, PROPOSALAGENT generates three sets of proposals\nwith a temperature of 0.7. After generation, the Jaccard similarity (Jaccard, 1901) of bigram sets is\ncalculated between the methodology of each proposal and the methodology produced in the previous\niteration. The proposal with the lowest similarity in methodology is selected as the ﬁnal output to\nincrease its diversity. For REVIEWAGENT and FALSIFICATIONAGENT, they invoke the GPT-4o\nonly once each time when generating responses.\nA.2\nRESEARCH-SPECIFIC IMPLEMENTATION\nData Engineering\nIn this research experiment, our system is tasked with exploring different ap-\nproaches to improve the quality of Alpaca-GPT4 dataset (Peng et al., 2023). The DSL conﬁguration\nand instance are shown in Figure 5 and Figure 7. The Llama-3-8B-Instruct7 model is employed to\nrate all data samples with the principles in DSL. We deploy Llama-3-8B-Instruct using vLLM8,\nconﬁguring the temperature to 0.05, while keeping all other parameters at the default settings. We\nuse Llama-3-8B9 for ICL- and SFT-alignment, and the model and the ﬁne-tuned checkpoints are\ndeployed using vLLM with a maximum token limit of 1024, while other parameters follow the\ndefault conﬁgurations provided by FastChat10. In falsiﬁcation process, the BABY-AIGS system\nidentiﬁes the factors that contribute to quality improvements and conclude whether there are ways\nto stably improve the quality of the extracted dataset, thus delivering valuable scientiﬁc discoveries.\nFor signiﬁcance screening in FALSIFICATIONAGENT, iterations are identiﬁed as having signiﬁcant\nimprovements if the difference of adjacent benchmarking results exceeds 1.5 for the ICL-aligned\nLlama-3-8B on the Vicuna-Bench (the validation benchmark) or 0.5 on the MT-Bench (the test\nbenchmark). From these iterations, candidates for scientiﬁc discovery are extracted. For hyper-\nparameters, we set the total iteration number M = 5 and set the multi-sample threads number\nN = 32.\nSelf-Instruct Alignment\nIn this research experiment, our system is tasked with exploring dif-\nferent approaches to improve the quality of synthesized SFT data from a seed dataset in Self-\nInstruct11 (Wang et al., 2023c). We use GPT-4o to rewrite the seed data for better quality with\nthe temperature parameter set to 0.05. The DSL conﬁguration and instance are shown in Figure 5\nand Figure 8. We use the Llama-3-8B12 model to generate instructions and responses, with it also\nserving as the base model for SFT alignment. We use LoRA (Hu et al., 2022) method from LLaMA-\n6https://github.com/openai/openai-python\n7https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n8https://github.com/vllm-project/vllm.\n9https://huggingface.co/meta-llama/Meta-Llama-3-8B.\n10https://github.com/lm-sys/FastChat.\n11https://github.com/yizhongw/self-instruct.\n12https://huggingface.co/meta-llama/Meta-Llama-3-8B.\n30\n\nFactory13 to ﬁne-tune the model with default training hyper-parameters14. The other experiment set-\nting is the same as data engineering research. For hyper-parameters, we set the total iteration number\nM = 15 and set the multi-sample threads number N = 1 due to limited computing resources for\nparallel model training.\nLanguage Modeling\nIn this research, the system is tasked to pre-train a mini-sized language\nmodel on several small corpora, aiming to improve performance by minimizing loss on the se-\nlected datasets. The experiment mainly follows the same setup as the language modeling task in AI\nScientist (Lu et al., 2024), based on the nanoGPT project 15. The DSL conﬁguration and instance\nare shown in Figure 5 and Figure 9, where we guide the models in adjusting parameters related to\nmodel architecture and training process. For the experiments, we use the sampling scripts provided\nin the template code without modiﬁcations. For hyper-parameters, we set the total iteration number\nM = 10 and set the multi-sample threads number N = 1 due to limited computing resources for\nparallel model training.\nB\nEXPERIMENT DETAILS\nB.1\nGUIDELINES FOR HUMAN EVALUATORS\nTo thoroughly assess the quality of our falsiﬁcation process, we conducted a human evaluation of\n20 agent-generated falsiﬁcation logs. The guidelines are summarized as follows:\n• Importance Score: Assess the signiﬁcance of the proposed scientiﬁc discovery candidate,\nconsidering its potential impact on experimental results and its relevance and consistency\nwith the main experiments.\n• Consistency Score: Evaluate whether the proposed ablation experiments align with the\nscientiﬁc discovery candidate and whether the experiment appropriately isolates the factor\nin question.\n• Correctness Score: Determine whether the ﬁnal scientiﬁc discovery drawn from the falsi-\nﬁcation process is correct based on the ablation and baseline results.\nFor each dimension, the evaluator assigns an integer score ranging from 0 to 2, where a higher score\nindicates better performance. The overall statistic results are shown in Table 2.\nB.2\nAPI COSTS OF THE FULL-PROCESS RESEARCH EXPERIMENT\nIn our experiments, we measured the average token counts and costs of different phases of BABY-\nAIGS (Section 3.2) for invoking the GPT-4o API and the results are presented in Table 8. Note\nthat as the experimental records in past iterations are used as input in most requests, with the rise of\niteration, the length of record will consequently increase, leading to the use of more tokens.\nInput Tokens\nGenerated Tokens\nCost ($)\nPre-Falsiﬁcation (per iter.)\n6,616.2\n761.5\n0.045\nFalsiﬁcation (per disc. cand.)\n43,375.5\n1,120.3\n0.234\nTable 8: Average token consumption and API costs for GPT-4o API in the full-process research\nexperiment. The costs at pre-falsiﬁcation phase is calculated for each iteration, and the costs at\nfalsiﬁcation phase is calculated for each discovery candidate.\nB.3\nDSL DEMONSTRATIONS FOR DIFFERENT RESEARCH TOPICS\nWe present an example of the methodology in DSL format generated during the experiment for\neach research topic, as shown in Figure 7, Figure 8 and Figure 9, corresponding to data engineering,\nself-instruct alignment, and language modeling, respectively.\n13https://github.com/hiyouga/LLaMA-Factory.\n14https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/train_lora/llama3_lora_sft.yaml\n15https://github.com/karpathy/nanoGPT.\n31\n\nData Engineering\nLLM Generated\n{\n    “Paradigm”: “Data Sample Rating & Filtering”,\n    “Principles”: “Relevance: The data point should be relevant to the task of multi-turn conversation and instruction-following.\\n\nCompleteness: The conversation should be complete and coherent, without abrupt endings or missing context.\\nInstruction adherence:\nThe responses should strictly follow the given instructions and provide accurate information.\\nConversational flow: The dialogue should\nflow naturally, with each turn logically following the previous one.\\nComplexity handling: The data point should demonstrate the ability to\nhandle complex and challenging questions effectively.\\nEngagement: The conversation should be engaging and maintain the user's\ninterest throughout.\\nEach data point should involve multi-turn dialogue.\\nResponses should be contextually relevant to the preceding\nturns.\\nData points should cover a diverse range of topics to mimic common use cases.\\nResponses should be coherent and\ngrammatically correct.\\nEach response should show logical progression and consistency across turns.\\nRelevance to the prompt: The\nresponse should directly address the question or task presented in the prompt.\\nCoherence: The response should be logically structured\nand easy to follow.\\nGrammar and syntax: The response should be free of grammatical and syntactic errors.\\nCreativity and depth: The \nresponse should demonstrate creative thinking and provide in-depth information when required.\\nConsistency: The response should\nmaintain consistency in its argument or narrative throughout.\\nLength: Ensure responses are comprehensive, aiming for lengths similar\nto high-scoring entries (1000 to 3000 characters).\\nWord Count: Encourage comprehensive and thorough responses, ensuring the\ncontent is relevant and informative.\\nUnique Words: Ensure responses contain a broad range of unique words while maintaining\nrelevance and coherence.\\nStopwords Count: Ensure responses are detailed and contextually rich.\\nKeyword Overlap: Ensure\nresponses are relevant and contextually appropriate.\\nDiversity: Aim for answer diversity in the range of 0.396 to 0.690.\\nAverage Word \nLength: Encourage balanced word lengths between queries and answers.\\nSentiment: Train models to deliver engaging, relevant, and \npositive responses.\\nCoherence Score: Refine the scoring method to better capture logical progression and consistency.\\nInstruction \nAdherence: Ensure responses have high instruction adherence.\\nComplexity Score: Prioritize generating detailed and complex answers.\n\\nEngagement Score: Ensure responses are engaging and interactive.”,\n    “Number”: 27,\n    “Threshold”: 15,\n    “Ratio”: 0.7\n}\nFigure 7: The DSL instance for data engineering research.\nSelf-Instruct Alignment\nLLM-Generated\n{\n    “Paradigm”: “Instruction Data Synthesis”,\n    “Prompt”: “1. Ensure queries are between 50-150 characters and answers are between 300-1500 characters. Aim for clear and\nconcise queries (10-26 words) and detailed yet concise answers (55-254 words).\\n2. Balance specificity to provide clear and relevant\ninformation without being overly detailed (Query specificity: 1, Answer specificity: 2-4). Ensure specific terms are contextually relevant.\\n\n3. Maintain moderate complexity in language to ensure clarity and conciseness (Query clarity score: 2-5, Answer clarity score: 3-7).\nAvoid jargon unless necessary.\\n4. Increase relevance by incorporating task-specific keywords and ensuring both queries and answers\nare contextually relevant and detailed. Ensure answers directly address the queries.\\n5. Diversify the seed data to cover a broad range\nof tasks, topics, and scenarios, including more complex instructions. Include tasks of varying complexity and from different domains\n(e.g., healthcare, finance, education).\\n6. Use an LLM to perform the initial evaluation and rewrite. Have human reviewers refine the\nrewritten instructions.\\n7. Implement a structured feedback mechanism to continuously refine the principles and methodology.\\n\n8. Analyze high-scoring tasks and responses on VicunaBench and MT-bench to tailor the principles.”,\n    “Seed”: true\n}\nFigure 8: The DSL instance for self-instruct alignment research.\n32\n\nLanguage Modeling\nLLM Generated\n{\n    “Paradigm”: “Generative Pre-training”,\n    “LLM_name”: “gpt-4o”,\n    “n_layer”: 6,\n    “n_embd”: 384,\n    “dropout”: 0.2,\n    “bias”: false,\n    “learning_rate”: 0.001,\n    “max_iters”: 5000,\n    “weight_decay”: 0.1,\n    “beta1”: 0.9,\n    “beta2”: 0.99,\n    “grad_clip”: 1.0,\n    “decay_lr”: true,\n    “warmup_iters”: 100\n    “lr_decay_iters”: 15,\n    “min_lr”: 0.0001\n}\nFigure 9: The DSL instance for language modeling research.\nC\nPROMPTING STRUCTURE\nIn this section, we will brieﬂy introduce the prompting structures of the PROPOSALAGENT, RE-\nVIEWAGENT, and FALSIFICATIONAGENT as shown in Figure 10, Figure 11, and Figure 12, respec-\ntively. For detailed prompts, please refer to our code repository16 .\nProposalAgent\nOutput\n                                     System Prompt\nYou are an experienced scientist.\nThe task you are faced with a certain scientific research\ntask and you need to make some scientific discoveries.\nYou are provided with the task, the goal to fulfill which is related\nto the task, and the existed experiment results and review.\n...\n                                        User Prompt\nYou are now an data curation scientist. \nYou are faced with a potentially large-scaled, mixed-quality data.\nYou would be ...\nLLM\nLLM\nLLM\nLLM\nLLM\n...\nOutput\nOutput\nOutput\nOutput\n...\nSelect the \ngreatest\nsemantic\ndifference\nfrom\nprevious\nmethodology\nFinal\nOutput\n                                            History\n## Turn 1 ...\nFigure 10: The prompting structure for the PROPOSALAGENT includes a general system prompt, a\nresearch-topic-speciﬁc user prompt and history logs. The LLM generates multiple outputs, covering\nelements such as idea, methodology, DSL, etc. From these outputs, the one whose methodology has\nthe greatest semantic difference from the previous round’s methodology is selected as the idea for\nthe current round, aiming to boost creativity in ideation.\n16https://github.com/AgentForceTeamOfficial/Baby-AIGS.\n33\n\nReviewAgent\n...\n**query** is the query of ...\n**answer** is the answer of ...\n**dataset** is a list of ...\nFor example, the metric can be **length**\n'''python\n    return {\"query_length\": len(query),\n    \"answer_length\": len(answer)}'''...\nSystem Prompt\nIn this case, the proposal is ...\nThe methodology is ...\nThe hypothesis is ...\nThe metric is ...\nThe metrics and code in the last iteration...\nUser Prompt\nLLM\nNew Metrics\nMetric Value1\nYou are an experienced scientist tasked with guiding a young scholar through their research project.\nThroughout the process, the young scholar will present their research topic, including their proposal,\nmethodology, domain-specific language (DSL), hypothesis, metrics and some experiment results.\nHere is the breakdown of these elements: ...\nYou need to evaluate his proposal, methodology, and DSL based on his experimental results, and \nprovide insightful suggestions for the next steps in his research.\nSystem Prompt\n## My Research Task Description\nI want to curate the Alpaca-GPT4 Database to make it a high-quality one for the MT-bench. ...\n## Current Research ...\n## Experiment Result ...\nUser Prompt\nYou are an experienced\nscientist tasked with guiding\na young scholar through their\nresearch project. ...\nSystem Prompt\n...\n## Data on Vicuna-Bench\nData: ...; Metric Value: ...\n...\n## Final Score\n...\nUser Prompt\n...\nLLM\nEval with\nMetric1\nEval with\nMetric2\nEval with\nMetricN\n...\nLLM\nExpReview\nMerge\nLLM\nProposal\nReview\nMetric Value2\nMetric Value3\nMetric ValueN\n## Past Research\n### Turn1\nProposal: xxx; Methodology: xxx; ...\n### Turn2 ...\nHistory\nFigure 11: The REVIEWAGENT will ﬁrst generate new metrics and then analyze each metric indi-\nvidually using the LLM. Following this, the REVIEWAGENT will call the LLM to merge the anal-\nysis results for each metric, resulting in the ExpReview. Next, the REVIEWAGENT will assess the\nexperimental results by integrating insights from previous ideas and experiments, yielding the Pro-\nposalReview.\n34\n\nFalsificationAgent\n...\nThe student has already done a lot of experiments\nand got some improvement against the baseline.\nHowever, he has no idea what lead to the\nimprovement of the result and what is the real\nscientific rule behind the improvement.\nYou should...\nSystem Prompt\n...You are faced with a potentially large-scaled,\nmixed-quality data. You would be provided later a\ndescription of a downstream task, and you should\ncurate the provided data to ...\nUser Prompt\nLLM\nDiscCandK\nDiscCand1\nDiscCand2\nDiscCand3\n...\nSystem Prompt\n...\nNow you need to carry out an ablation\nstudy. You have access to all the\nexperiment records, and a proposal\nabout what ablation study you need ...\nFirst, you should select a BASELINE...\n...You are faced with a potentially\nlarge-scaled, mixed-quality data. You\nwould be provided later a description\nof a downstream task, and you should\ncurate the provided data to ...\nUser Prompt\nFor\neach\none\nLLM\nAblationExp1\n...\nAblationExp2\nAblationExp3\nAblationExpT\nSystem Prompt\nUser Prompt\nLLM\nDSL1\nDSL2\n...\nDSLT\nResult1\n...\nResult2\nResult3\nResultT\nResult4\nSystem Prompt\n...\nThe student has already done the main experiment\nand got the discovery from the experiment. Also,\nhe has conducted the ablation experiment.\nNow, you need to decide whether the discovery is\nstill reasonable based on the result of the ablation\nstudy. ...\n...You are faced with a potentially large-scaled,\nmixed-quality data. You would be provided later a\ndescription of a downstream task, and you should\ncurate the provided data to ...\nUser Prompt\nLLM\nScientific\nDiscovery\nTurnM\nTurn1\nTurn2\n...\nTurnQn\nTurnQ1\nTurnQ2\n...\nSignificance\nScreening\nFigure 12: The FALSIFICATIONAGENT ﬁrst screens all history turns to identify turns with notable\nchanges in results. It then generates discovery candidates from the results obtained through signif-\nicance screening. For each discovery candidate, it then creates several ablation experiment setups\nand generates the corresponding DSL to obtain experimental results. Once the experimental results\nare obtained, the FALSIFICATIONAGENT calls on the LLM to produce the ﬁnal scientiﬁc discovery.\n35",
    "pdf_filename": "AIGS_Generating_Science_from_AI-Powered_Automated_Falsification.pdf"
}