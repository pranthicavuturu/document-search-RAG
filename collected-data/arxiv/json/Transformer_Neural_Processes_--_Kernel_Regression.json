{
    "title": "Transformer Neural Process - Kernel Regression",
    "abstract": "The principle challenge of modern spatiotemporal Stochastic processes model various natu- Bayesian modeling is scale. As the number of ob- ral phenomena from disease transmission to served locations increases from tens to thousands or stock prices, but simulating and quantify- hundreds of thousands, traditional techniques used to ing their uncertainty can be computationally model spatiotemporal phenomena break down. Per- challenging. For example, modeling a Gaus- haps the most common method typically employed sian Process with standard statistical meth- to model spatiotemporal processes is the Gaussian ods incurs an O(n3) penalty, and even us- Process (GP). Gaussian Processes are a particularly ing state-of-the-art Neural Processes (NPs) well-behaved class of stochastic processes. Specifi- incursanO(n2)penaltyduetotheattention cally, for a finite index set {t ∈ T}, the collection mechanism. We introduce the Transformer X = (X ,...,X ) follows a multivariate Gaussian 1 T Neural Process - Kernel Regression (TNP- distribution. This makes various analytic calculations KR), a new architecture that incorporates tractable, facilitating regression, marginalization, and a novel transformer block we call a Kernel sampling with GPs. Regression Block (KRBlock), which reduces While GPs provide a significant degree of flexibility thecomputationalcomplexityofattentionin in modeling, the analytic solutions they yield do not transformer-based Neural Processes (TNPs) scale well in the number of observed locations. Using from O((n + n )2) to O(n2 + n n ) by C T C C T aGPtomodelspatialrandomeffectswithinaMarkov eliminating masked computations, where n C ChainMonteCarlo(MCMC)samplerincursanO(n3) isthenumberofcontext, andn isthenum- T costpersample,wherenisthenumberofobservedlo- ber of test points, respectively, and a fast at- cations. This is because the covariance matrix must tention variant that further reduces all at- be inverted, or factorized in the case of Cholesky de- tention calculations to O(n ) in space and C composition, at each iteration in order to generate a time complexity. In benchmarks spanning sample. Unfortunately, this means that for only n = suchtasksasmeta-regression, Bayesianopti- 1,000 locations, nearly a billion operations must be mization, and image completion, we demon- performed to generate a single sample. strate that the full variant matches the per- formance of state-of-the-art methods while In order to accelerate Bayesian inference with spa- trainingfasterandscalingtwoordersofmag- tiotemporal stochastic processes, there have been at nitude higher in number of test points, and least three prominent strains of research. The first is the fast variant nearly matches that perfor- VariationalInference(VI),whichaimstorecastthein- mance while scaling to millions of both test ferenceproblemasanoptimizationproblemandmax- and context points on consumer hardware. imizetheEvidenceLowerBound(ELBO).Thesecond aimstoacceleratesamplingbyusingagenerativeneu- ral network-based approximation. This family tends to leverage Variational Autoencoders (VAEs). The third is a recent family of deep learning models called Neural Processes (NPs). These models use a meta- Preliminary work. learningobjective,meaningthatoncetrained,thefor- Correspondence to: daniel.jenson@worc.ox.ac.uk. ward pass of the model takes as input “context” or ∗These authors jointly supervised this work. observed points and returns a function. This function 4202 voN 91 ]GL.sc[ 1v20521.1142:viXra",
    "body": "Transformer Neural Process - Kernel Regression\nDaniel Jenson1 Jhonathan Navott1 Mengyan Zhang1 Makkunda Sharma1\nElizaveta Semenova2,∗ Seth Flaxman1,∗\nUniversity of Oxford1 Imperial College London2\nAbstract 1 INTRODUCTION\nThe principle challenge of modern spatiotemporal\nStochastic processes model various natu- Bayesian modeling is scale. As the number of ob-\nral phenomena from disease transmission to served locations increases from tens to thousands or\nstock prices, but simulating and quantify- hundreds of thousands, traditional techniques used to\ning their uncertainty can be computationally model spatiotemporal phenomena break down. Per-\nchallenging. For example, modeling a Gaus- haps the most common method typically employed\nsian Process with standard statistical meth- to model spatiotemporal processes is the Gaussian\nods incurs an O(n3) penalty, and even us- Process (GP). Gaussian Processes are a particularly\ning state-of-the-art Neural Processes (NPs) well-behaved class of stochastic processes. Specifi-\nincursanO(n2)penaltyduetotheattention cally, for a finite index set {t ∈ T}, the collection\nmechanism. We introduce the Transformer X = (X ,...,X ) follows a multivariate Gaussian\n1 T\nNeural Process - Kernel Regression (TNP- distribution. This makes various analytic calculations\nKR), a new architecture that incorporates tractable, facilitating regression, marginalization, and\na novel transformer block we call a Kernel sampling with GPs.\nRegression Block (KRBlock), which reduces\nWhile GPs provide a significant degree of flexibility\nthecomputationalcomplexityofattentionin\nin modeling, the analytic solutions they yield do not\ntransformer-based Neural Processes (TNPs)\nscale well in the number of observed locations. Using\nfrom O((n + n )2) to O(n2 + n n ) by\nC T C C T aGPtomodelspatialrandomeffectswithinaMarkov\neliminating masked computations, where n\nC ChainMonteCarlo(MCMC)samplerincursanO(n3)\nisthenumberofcontext, andn isthenum-\nT costpersample,wherenisthenumberofobservedlo-\nber of test points, respectively, and a fast at-\ncations. This is because the covariance matrix must\ntention variant that further reduces all at-\nbe inverted, or factorized in the case of Cholesky de-\ntention calculations to O(n ) in space and\nC composition, at each iteration in order to generate a\ntime complexity. In benchmarks spanning\nsample. Unfortunately, this means that for only n =\nsuchtasksasmeta-regression, Bayesianopti-\n1,000 locations, nearly a billion operations must be\nmization, and image completion, we demon-\nperformed to generate a single sample.\nstrate that the full variant matches the per-\nformance of state-of-the-art methods while In order to accelerate Bayesian inference with spa-\ntrainingfasterandscalingtwoordersofmag- tiotemporal stochastic processes, there have been at\nnitude higher in number of test points, and least three prominent strains of research. The first is\nthe fast variant nearly matches that perfor- VariationalInference(VI),whichaimstorecastthein-\nmance while scaling to millions of both test ferenceproblemasanoptimizationproblemandmax-\nand context points on consumer hardware. imizetheEvidenceLowerBound(ELBO).Thesecond\naimstoacceleratesamplingbyusingagenerativeneu-\nral network-based approximation. This family tends\nto leverage Variational Autoencoders (VAEs). The\nthird is a recent family of deep learning models called\nNeural Processes (NPs). These models use a meta-\nPreliminary work. learningobjective,meaningthatoncetrained,thefor-\nCorrespondence to: daniel.jenson@worc.ox.ac.uk. ward pass of the model takes as input “context” or\n∗These authors jointly supervised this work.\nobserved points and returns a function. This function\n4202\nvoN\n91\n]GL.sc[\n1v20521.1142:viXra\nTransformer Neural Process - Kernel Regression\ncan then be evaluated at any collection of test points 2.1 Variational Inference (VI)\nand returns both their mean predictions and associ-\nated uncertainties. VI (Blei, Kucukelbir, and McAuliffe, 2017; Murphy,\n2023)approximatestheposteriordistributionbyfram-\nThe NP family of models has grown rapidly over the\ning inference as an optimization problem, aiming to\nlast few years, but recently Transformer Neural Pro-\nmaximizetheEvidenceLowerBound(ELBO)bymin-\ncesses(TNPs),includingTNP-D,TNP-ND,andTNP-\nimizingtheKullback-Leibler(KL)divergencebetween\nA have come to dominate the landscape (Nguyen and\na variational distribution q (z) and the true posterior\nGrover, 2023). These models, however, suffer from ψ\np (z|x). AlthoughVIiswidelyused, itseffectiveness\nan O(n2) complexity due to the attention mechanism θ\ndependsonselectinganappropriatevariationalfamily,\nused in transformer encoder blocks. We extend this\nand there are no guarantees on how close the ELBO\nfamily with the Transformer Neural Process - Ker-\nis to the true log-likelihood, making uncertainty esti-\nnel Regression (TNP-KR) model. TNP-KR uses a\nmationchallengingwhenthevariationalfamilypoorly\nnovel transformer block we call a Kernel Regression\napproximates the true distribution (Yao et al., 2018;\nBlock (KRBlock), which reduces the cost of atten-\nHuggins et al., 2020).\ntion in transformer-based NPs from O((n + n )2)\nC T\nto O(n2 +n n ) where n is the number of context\nC c T C\npoints and n is the number of test points. We also\nT 2.2 Stochastic Process Emulation\nintroduceanevenfastervariant,whichusesPerformer\nattention inside the KRBlock (Choromanski et al.,\nAnotherlineofresearchaimstoacceleratesamplingby\n2022). Performer attention uses a kernel approxima-\napproximating samples from computationally inten-\ntiontosoftmaxattentionandfurtherreducesthecom-\nsivestochasticprocesses. Thisistheaimofmodelslike\nplexity to O(n ), enabling the model to scale to mil-\nC PriorVAE, PriorCVAE, and πVAE (Semenova, Xu, et\nlions of points on consumer hardware. We show that\nal.,2022;Semenova,Verma,etal.,2023;Mishraetal.,\nthe full variant matches the performance of state-of-\n2022). Currently these models are all based on Vari-\nthe-art methods while training faster and scaling two\national Autoencoders (VAEs) (Kingma and Welling,\norders of magnitude higher in number of test points,\n2022). VAEs consist of an encoder and decoder com-\nand the fast variant nearly matches that performance\nbined with a latent sampling process. They encode\nwhilescalingtomillionsofbothtestandcontextpoints\nraw data into a vector of latent parameters, which are\non consumer hardware.\nthen used to sample a latent vector. This latent vec-\ntor is then passed through the decoder, whose objec-\ntive is to recreate the original data. The advantage\nof models like these is that if the latent distribution\nis simple, i.e. a multivariate normal with diagonal co-\n2 BACKGROUND\nvariance, it can be very easy to sample. This means\nthat a fully trained network can generate new sam-\nples from the original data distribution by sampling\nA number of general techniques have been developed latents and passing them through the decoder. Fur-\nto reduce the computational burden of modeling large thermore, this can often be done in time linear in the\nspatiotemporal datasets. These include, but are not number of layers in the network, which can be two\nlimited to variational inference (VI) (Blei, Kucukel- orders of magnitude faster than sampling from a real\nbir, and McAuliffe, 2017), stochastic process emula- GP (Semenova, Verma, et al., 2023). Because neural\ntion (Mishra et al., 2022; Semenova, Xu, et al., 2022; networks are inherently differentiable, they can also\nSemenova, Verma, et al., 2023), and neural processes be transparently integrated into inference frameworks\n(NPs)(Garnelo,Schwarz,etal.,2018;Garnelo,Rosen- like NumPyro, where gradient-informed samplers like\nbaum, et al., 2018; Kim et al., 2019; Lee et al., 2020; theNo-U-TurnSampler(NUTS)caneasilypassgradi-\nGordonetal.,2020;NguyenandGrover,2023). There ents through the model. The principle challenge with\nis also a long literature on approximate methods to this class of models is that the number of input and\nscale up Gaussian processes in particular which we output locations is fixed and ordered, which means a\ndo not cover in detail, see e.g. Hensman, Fusi, and new model must be retrained each time the number\nLawrence (2013), Rue, Martino, and Chopin (2009), of observed locations changes or the location associ-\nSolin and S¨arkk¨a (2020), Wilson, Dann, and Nick- ated with each input changes. These models are also\nisch(2015),andLindgren,Lindstr¨om,andRue(2010). sensitive to the dimensionality of the latent vector,\nWhile using distinct algorithmic approaches, all these which induces an information bottleneck on autoen-\nmethods provide approximations to the posterior dis- coded data and can cause oversmoothing in generated\ntributions. samples.\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\n2.3 Neural Processes (NPs) The advantage of NPs is that they can be pretrained\nacrossavarietyofpriorsandnumberofcontextpoints.\nNeuralProcesses(NPs)areafamilyofmodelsthatuse And at test time, they can perform GP regression\ndeepneuralnetworkstorepresentstochasticprocesses. for thousands of paths simultaneously in seconds on\nNPs are considered “meta-learners” because, instead aGPU.Thiscanbeseveralordersofmagnitudefaster\nof modelling a single function h : s → f, they take as than calculating uncertainty for each path within an\ninputcontextpoints(s C,f C)andreturnadistribution inference framework. The disadvantage of these net-\nover functions g : (s C,f C) → P h|sC,fC(h : s T → f T), works is that previously they significantly underfit\nallowing evaluation at test points s T without retrain- relative to true GPs, particularly at context or ob-\ning. Tobevalidstochasticprocesses,NPsmustensure served data points. However, NPs have evolved from\ninvariance to context/test point ordering and consis- simple MLP-based networks to sophisticated convo-\ntency under marginalization. lutional and transformer-based architectures. These\nnewer variants are often just as accurate as baseline\nTherearetwomainclassesofNeuralProcesses, latent\nGPs and at least an order of magnitude faster.\nneuralprocessesandconditionalneuralprocesses. La-\ntent neural processes dedicate part of their architec- Wedetailtheessentialaspectsofmoderncanonicalde-\nture to generating latent parameters, which are sam- signs in this section. For brevity, we omit the original\npled and passed through the decoder to generate co- NP (Garnelo, Schwarz, et al., 2018) and CNP (Gar-\nherent samples, similar to VAEs. The fundamental nelo, Rosenbaum, et al., 2018) models, which used an\nassumption of latent NPs is that the test points are autoencoderframeworkwithMLPs. Alsorelevant,but\nindependent conditional on the latent vector. For in- not included here are Bootstrapping Neural Processes\nstance, if (s C,f C) represents a tuple of locations and (Lee et al., 2020), which represent an extension to all\nfunction values at context (observed) points, (s T,f T) NP models.\nrepresents locations and function values at test (un-\nobserved) points, and z represents a sampled latent\nvector, the likelihood of the function values at test lo- 2.3.1 Attentive Neural Process (ANP)\ncations can be formulated as follows:\nThe Attentive Neural Process (ANP) was introduced\nto address several limitations of the NP and CNP ar-\np(f |s ,s ,f )\nT T C C chitectures (Kim et al., 2019). In particular, NPs and\n(cid:90)\nCNPs underfit on observed context points and gener-\n= p(z|s ,f )p(f |s ,z)dz\nC C T T ally tend to oversmooth the posterior predictive. One\nof the principle reasons for underfitting is that the de-\n(cid:90) |T|\n= p(z|s ,f )(cid:89) p(f(i) |s(i),z)dz coder is unable to differentially attend to each of the\nC C T T\ncontext points when decoding at test points. For in-\ni=1\nstance, if one context point is located at s =−2 and\nc\nConditional neural processes often have very similar the test point is at s = 2, i.e. opposite ends of the\nt\narchitectures,excepttheyavoidsamplingalatentvec- training region, s equally influences the context vec-\nc\ntor and condition on a fixed representation, r, of the tor used to decode at s despite having little to no\nt\ncontext points. This implies the following factoriza- influence on the behavior of function at s = 2. Ac-\nt\ntion, assuming the encoder has already processed the cordingly, the authors propose an attentive version of\ncontext points, enc(s ,f )=r: theNPthatallowsbothlocalandglobalcontexttobe\nC C\nincorporated when decoding at test locations.\nThe ANP shares the NP architecture, but replaces\n|T|\np(f |s ,s ,f )=(cid:89) p(f(i) |s(i),r) MLPs with multihead dot product self-attention in\nT T C C T T\nboth the latent and deterministic encoding paths.\ni=1\nThen,inthedeterministicpathitaddscross-attention\nIn practice, conditional neural processes tend to per- between test points and context points so that the as-\nform better. It is unclear whether this is because the sociated context vector, r , summarizes information\nt\nlatent vectors, z, are an insufficient representation of relevant to the test location s , rather than being a\nt\nthe latent parameters, or if the models are expend- global representation. This localized context vector,\ning some of their finite capacity on producing good r , is passed through the decoder with the global la-\nt\nestimates of z at the cost of final predictive accuracy. tent representation z, and the location, s , to produce\nt\nEither way, the most performant NPs as measured by (µ ,σ2). With the ANP and its conditional variant\nt t\nlog-likelihood scores are conditional, specifically con- CANP, the NP family started to become a viable re-\nditional transformer neural processes (TNPs). placement to true GPs for inference.\nTransformer Neural Process - Kernel Regression\n2.3.2 Convolutional Conditional Neural Because standard transformers are designed to work\nProcess (ConvCNP) with sequences of data and use fixed positional em-\nbeddings, the TNP authors had to modify the archi-\nConvolutional Conditional Neural Processes (Con- tecture for NPs. Accordingly, TNPs dispense with\nvCNP)weredesignedtoincorporatetranslationequiv- thepositionalembeddings,mergethecontextandtest\narianceintoNPs(Gordonetal.,2020). Whenamodel sequences as input, and introduce a special atten-\nexhibits translation equivariance, it is able to identify tion mask. The context sequence consists of location\na feature or function behavior regardless of how far and function value pairs, [(s ,f ),...,(s ,f )], and\nits input has shifted in the domain. This improves the test sequence consists of1 loc1 ation andnC zern oC -padded\nthemodel’scapacitytogeneralizebeyondthetraining function value pairs, [(s ,0),...,(s ,0)]. Within\nregion. each layer, a mask is ap1 plied that n pT revents context\npoints from attending to test points and prevents test\nThe authors of ConvCNP define two architectures,\npointsfromattendingtoothertestpoints. Thismeans\none for on-the-grid data and one for off-the-grid data.\nthatcontextpointsonlyattendtoothercontextpoints\nHere we detail the off-the-grid version since it is more\nandtestpointsalsoonlyattendtocontextpoints. Af-\ngeneric and can be used in both cases. First, the do-\nter the encoding stage, the embeddings for test points\nmain is partitioned into a fixed, uniform grid. Then,\nare passed through a prediction head that estimates\na positive-definite Reproducing Kernel Hilbert Space\nthe mean and covariance structures.\n(RKHS) kernel is evaluated at each of the grid points\nusing the context set. This value is then normalized TherearethreeoriginalTNPvariants: TNP-D,TNP-\nusing a density channel so the point values are invari- ND, and TNP-A. TNP-D (Diagonal) assumes test\nant to the cardinality of the context set. This grid points can be factorized independently conditional on\nis then run through a CNN-based architecture, e.g. the context points, in line with most CNP variants.\nResNet (He et al., 2016), to create an updated hidden TNP-ND (Non-Diagonal) parameterizes a covariance\nstate representation at grid locations. Finally, the de- matrix by estimating the elements in the lower tri-\ncoder uses another RHKS kernel, typically the same angular matrix of a Cholesky decomposition for test\none used in the encoder, to decode test points using points. Lastly, TNP-A (Autoregressive) assumes that\nthe underlying hidden state grid values. thetestpointscanbefactorizedautoregressively. This\nmeansthateachtimeatestpointispredicted,itstrue\nConvCNPs perform very well on the standard NP\nvalue is then added to the context set and used when\nbenchmarks,oftenexceedingtheperformanceofother\npredicting the next test point.\nmodelsatafractionofthenumberoflearnableparam-\neters. However, beyond simple examples, ConvCNP In practice, we found TNP-D to consistently perform\nrequiresmanymoreparameters,oftenonthesameor- well. On the other hand, we found TNP-ND could\nder as other models, to perform competitively. Fur- become arbitrarily miscalibrated. When there are\nthermore, ConvCNP is very sensitive to the parame- very few context points, TNP-ND maximizes the log-\nterizationoftheintermediategridandtheeffectivere- likelihoodbycollapsingthestandarddeviationfortest\nceptive field of the CNN layer. For instance, at lower pointsthatlienearcontextpoints,whichprovidesvery\nlengthscales, the model performs better when there is preciseestimates. However,thisalsocausesthemodel\na higher grid resolution, but this increase in grid reso- tocollapseuncertaintyestimatesfortestpointsthatlie\nlution changes the receptive field of the CNN layer, so farawayfromcontextpoints,leadingtobothhighlog-\nthe CNN’s kernels must be optimized in conjunction. likelihoodsandhighmiscalibrationrates. Ontheother\nLastly, due to the fixed intermediate grid, ConvCNP hand,TNP-Aassumesthatthetruetestpointsareob-\nsuffersfromthecurseofdimensionalityandisdifficult servedautoregressively,whichisequivalenttorunning\nto evaluate in non-contiguous regions of the domain. any NP model forward one test point at a time. This\nmethod yields log-likelihoods that correspond to se-\n2.3.3 Transformer Neural Processes (TNP) quential observations, rather than joint distributions\noverallunobservedpoints. Asweareinterestedinthe\nTransformer Neural Processes (TNPs) (Nguyen and general case, we do not include experiments on TNP-\nGrover, 2023) can be considered an extension of the A.\nConditional Attentive Neural Process (CANP) that\nuses multiple transformer encoder blocks instead of\nstacked self-attention and cross-attention. A trans-\nformer encoder block consists of a self-attention layer\nfollowed by a feedfoward network with residual con-\nnections interspersed. The pseudocode can be viewed\nin Appendix 5.\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nqs' ks' representing a test point, k is a key embedding rep-\nj\nresenting a context point, and v is the value embed-\nj\nding associated with the same context point, cross-\nattentionintheKRBlockcanbeformulatedexplicitly\nFFN FFN\nas Nadaraya-Watson kernel regression:\nLayerNorm LayerNorm\nf(q i)=(cid:88) (cid:80)α( αq (i q,k ,j k) )v j (1)\nj j i j\nMultihead Multihead\nStacking KRBlocks then allows the model to perform\nAttention Attention\niterative kernel regression on increasingly complex\ninternal representations of test and context points.\nqueries keys values queries keys values Cross-attention from test to context points costs\nO(n n ) and the self-attention among contxt points\nC T\nLayerNorm LayerNorm costs O(n2), making the total complexity O(n2 +\nC C\nn n ).\nC T\nqsks When there are a large number of context points,\nthe O(n2) term can still prove computationally pro-\nC\nhibitive. For example, in satellite imagery, a compar-\nFigure 1: KRBlock architecture. Queries and keys atively small image of 300x500 results in 150,000 lo-\nhave their own residual pathways, but keys are used cations or pixels (Heaton et al., 2018). A common\nto update queries through cross-attention. application is to inpaint pixels missing due to cloud\ncover. With20%ofpixelsmissing,therewouldstillbe\n3 TRANSFORMER NEURAL 120000 context points. This means that even with a\nPROCESS - KERNEL KRBlock, the space and time complexity would be on\nREGRESSION (TNP-KR) the order of 1200002, requiring nearly 60GB of mem-\nory and 14.4 billion multiplications per self-attention\napplication. Thus,inordertofurtherscaleKRBlocks,\n3.1 KRBlock\nweincorporatePerformerattention,alsoknownasfast\nattention (Choromanski et al., 2022).\nTNP-KR was inspired by TNP-D, which consistently\nperforms well on NP benchmarks and does not make\nthesameassumptionsasTNP-Aorsufferfrommiscal- 3.2 Fast Attention\nibrationlikeTNP-ND.Oneoftheprincipallimitations\nFastattentionisbasedonacompletealgorithmcalled\nofallTNPvariants,however,istheO((n +n )2)at-\nC T FastAttentionwithOrthogonalRandomfeatures(FA-\ntention used by the encoder layers. Recall that n\nC VOR+). FAVOR+allowsattentiontobecalculatedin\nis the number of context points and n is the num-\nT linear space and time complexity without making any\nber of test points. This also is not strictly necessary\nassumptionsaboutthesparsityorrankoftheattention\nbecause the special mask applied after attention dis-\nmatrix. It is nearly unbiased and offers uniform con-\ncards many of the calculations, namely, those from\nverenge and low variance (Choromanski et al., 2022).\ncontextpointstotestpointsandthosefromtestpoints\nFAVOR+constructsanattentionmatrixAL×L where\nto other test points. Accordingly, we introduce the\nA(i,j) = K(q ,k ) without ever fully materializing\nKRBlock, a transformer block that avoids computing i j\nit. For a randomized mapping ϕ : Rd → Rr, entries\nO(n2 + n nT) attention values altogether. Unlike +\nT C can be estimated with K(q,k) = E[ϕ(q)⊺ϕ(k)]. In\nthe original transformer encoder block, TNP-KR also\nfact, most kernels can be modeled using the following\nusespre-normalizedresidualconnections(Xiongetal.,\nmapping:\n2020), which has been shown to stabilize training and\nimprove performance. h(x)(cid:20)\nϕ(x)= √ f (ω⊺ x),...,f (ω⊺ x),...,\nWe call this a KRBlock because the cross-attention m 1 1 1 m\n(cid:21)\nfrom test to context points can be viewed as a form ⊺ ⊺\nf (ω x),...,f (ω x)\nof Nadaraya-Watson kernel regression where the loca- l 1 l m\ntions are the feature embeddings and the kernel is a\ndot product softmax kernel. Specifically, if α is the where f 1,...,f l : R → R, h : Rd → R, and\niid\ndot product softmax kernel, q is a query embedding ω ,...,ω ∼ D. When D = N(0,I ), this leads to\ni 1 m d\nTransformer Neural Process - Kernel Regression\n√\nthe Gaussian kernel, K . Ignoring the d normal- lengthscales from 0.1 to 0.6 uniformly to model over\ngauss\nization, the softmax kernel is defined as SM(q,k) := domain on the domain [−2,2], we sample lengthscales\nexp(q⊺k). Furthermore,thesoftmaxkernelcanbeex- according to ℓ ∼ Beta(α = 3,β = 7), which has both\npressed as a function of K : a mean and median of approximately 0.3. This is a\ngauss\nmore challenging benchmark that allows for greater\n(cid:18) ∥q∥2(cid:19) (cid:18) ∥k∥2(cid:19)\ndifferentiation among models since more than 50% of\nSM(q,k)=exp K (q,k)exp\n2 gauss 2 lengthscales fallbelow 0.3 and lessthan 10% lie above\n0.5. (In practice, we found most models could easily\nThus,thesoftmaxkernelcanbedefinedusingϕwhere learn lengthscales above 0.5). For the periodic ker-\n(cid:16) (cid:17)\nh(x) = exp\n∥x∥2\n, f = sin, and f = cos. Unfortu- nel, we also sample the period uniformly from 0.5 to\n2 1 2\nnately,thisrepresentationbehavespoorlyaroundzero 2, which represents between 2 and 8 cycles on the do-\nwhen the trigonometric functions return negative val- main [−2,2].\nues, so Choromanski et al. introduce positive random\nEach model is seeded 5 times and trained on 100,000\nfeatures (PRFs) for softmax. This results in the form:\nbatches of size 32. For each seed, the models are eval-\n(cid:20) (cid:21) uated on a final test set consisting of 5,000 batches\nSM(q,k)=E\nω∼N(0,Id)\neω⊺q−∥q 2∥2 eω⊺k−∥k 2∥2 of size 32. Each sample in each batch consists of 50\nrandomly sampled locations and 100 linearly spaced\n(cid:16) (cid:17) points throughout the domain. Between 3 and 50\nwhich implies h(x) = exp\n−∥x∥2\nand f = exp.\n2 1 of the randomly sampled points are used as context\nThisapproximationisfurtherimprovedbyforcingthe points, and the test points consist of all 150 points.\nω random vectors to be normalized and orthogonal Because many of these models use different heuris-\n(OR+). ticstopreventthecollapseofstandarddeviationator\nnear context points in the test set, we add an obser-\nMapping the query and key matrices through these\nvationnoiseof0.1. Thisaddednoisepreventscollapse\nrandom projections, ϕ(Q) = Q′ ∈ RL×r and ϕ(K) =\nand allows us to avoid using model-dependent heuris-\nK′ ∈RL×r, attention can be reexpressed as follows:\ntics,whichcouldartificiallyhampertheirperformance\nAt(cid:92) tention(Q,K,V)=Dˆ−1(Q′((K′)⊺\nV)),\nandpreventhonestcomparison. Lastly, allmodelsare\ntrained with a single cosine annealing learning rate\n(cid:124) (cid:123)(cid:122) (cid:125)\nO(Lrd) schedule, gradient norm clipping with a max norm of\nDˆ =diag(Q′((K′)⊺ 1 )). 3.0,andtheYOGIoptimizer(Zaheeretal.,2018). Ta-\nL\nble3showsthemeanandstandarderrorsacrossthese\nThus,bycarefullycontrollingtheorderofmatrixmul- test sets for each model and each kernel.\ntiplications, space complexity can be reduced from\nO(L2+Ld) to O(Lr+Ld+rd) and time complexity\ncan be reduced from O(L2d) to O(Lrd). We demon- Table 1: Microseconds (µs) per sample by varying\nnumberoftestpointsandkeepingcontextpointsfixed\nstrate the impact of the KRBlock and fast attention\nat 100. “OOM” indicates Out of Memory error.\non runtime and memory in Tables 1 and 2.\n# Test TNP-D TNP-KR-Full TNP-KR-Fast\n4 EXPERIMENTS 100 118 171 194\n101 119 287 327\nIn order to evaluate the performance of TNP-KR, 102 129 267 282\n103 286 230 226\nwe test its performance across standard benchmarks,\n104 45,000 474 436\nwhich include GPs, image completion, and simple\n105 OOM 7,040 4,510\nBayesian Optimization (BO). All experiments were 106 OOM 86,800 64,300\nperformed on a single 24GB Nvidia RTX 4090 GPU.\n4.1 1D Gaussian Processes These models can then be used in a simple one-\ndimensional Bayesian Optimization (BO) setting\nFor one-dimensional GPs, we evaluate the principal whereeachmodelisgiventenobservedstartingpoints.\nNP models on the the RBF, periodic, and Mat´ern 3/2 BO then uses the model to calculate the expected im-\nkernels. For each of these kernels, σ2 can be factored provement across the domain and selects the point\nout and used to standardize the data. Accordingly, in where that metric is greatest. There is a fixed bud-\nour tests, we assume the data has been standardized get of 50 iterations and the objective is to identify\nso the models can focus on learning and differentiat- the function’s minimum. Table 4 shows the mean and\ning lengthscales. While standard benchmarks sample standard error of regret across 5 seeds for each model\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nFigure 2: Posterior predictive samples for TNP-KR-Fast. Black line is the true function, black dots are noisy\nobservations, blue line is model mean prediction, and shaded blue area is 95% confidence region.\nTable 2: Microseconds (µs) per sample by varying Table 3: 1D GP mean and standard error of negative\nnumberofcontextpointsandkeepingtestpointsfixed log-likelihoods over 5 runs, where the score for each\nat 100. “OOM” indicates Out of Memory error. runconsistsoftheaveragenegativelog-likelihoodover\n5,000 batches of 32. Best model in bold.\n# Context TNP-D TNP-KR-Full TNP-KR-Fast\n100 99 98 138\nModel Matern3/2 Periodic RBF\n101 99 220 277\nNP 0.638±0.003 1.342±0.001 0.338±0.007\n102 114 286 299 CNP 0.547±0.002 1.165±0.002 0.238±0.006\n103 247 302 234 BNP 0.523±0.003 1.165±0.003 0.165±0.005\n104 44,700 36,700 632 ANP 0.043±0.008 0.829±0.007 −0.341±0.003\n105 OOM OOM 6,900 CANP 0.047±0.014 0.818±0.005 −0.310±0.024\n106 OOM OOM 88,400 BANP 0.023±0.007 0.761±0.011 −0.347±0.008\nConvCNP −0.018±0.003 0.543±0.003 −0.456±0.003\nTNP-D −0.023±0.003 0.516±0.003 −0.455±0.004\nTNP-KR-Fast −0.006±0.002 0.562±0.006 −0.425±0.005\nand each kernel. There are 100 tasks per seed. Here, TNP-KR-Full −0.024±0.002 0.516±0.002 −0.453±0.004\nConvCNP performs best across the board, which is\nlikely due to the fact that the convolutional kernels in\nthismodelintroduceastronglocalinductivebias,soit\nis able to identify functional patterns that fall within 128 of the randomly selected points, which represents\nits kernel width with very few context points. between 5% and 50% of the number on the uniform\ngrid. Each model is trained on 100,000 batches of\n4.2 2D Gaussian Processes size 16 and tested on 5,000 batches of size 16. The\nother training settings remain the same as the one-\nFor the two-dimensional Gaussian Processes, we test dimensional case. Here TNP-D and TNP-KR with\nthe the RBF kernel on the domain [−2,2]2. The test full attention perform best. Although ConvCNP was\npoints consist of 128 randomly selected points on the omittedduetothetimerequiredtooptimizeitskernel\ndomain as well as 256 on a uniform grid over the do- and grid for the benchmark, we believe it would also\nmain. The context points consist of between 12 and perform competitively here.\nTransformer Neural Process - Kernel Regression\nTable 4: 1D Bayesian Optimization mean and stan- Table 7: CelebA mean and standard error of negative\ndard error of 500 samples on domain [-2, 2]. Best log-likelihoods over 5 runs, where the score for each\nmodel in bold. runconsistsoftheaveragenegativelog-likelihoodover\n5,000 batches of 32. Best model in bold.\nModel Matern 3/2 Periodic RBF Model NLL\nNP 0.145±0.013 0.131±0.010 0.079±0.010\nNP −0.107±0.002\nCNP 0.106±0.011 0.331±0.020 0.061±0.009\nCNP −0.127±0.001\nBNP 0.126±0.012 0.293±0.019 0.069±0.009\nANP −0.983±0.023\nANP 0.072±0.010 0.123±0.012 0.041±0.009\nCANP 0.065±0.010 0.154±0.014 0.042±0.008 CANP −0.554±0.096\nBANP 0.059±0.009 0.106±0.011 0.037±0.008 TNPD −1.524±0.005\nConvCNP 0.011±0.003 0.043±0.006 0.003±0.001 TNPKR Fast −1.006±0.025\nTNP-D 0.059±0.009 0.044±0.007 0.023±0.007 TNPKR Full −1.561±0.008\nTNP-KR-Fast 0.062±0.009 0.052±0.007 0.027±0.006\nTNP-KR-Full 0.054±0.009 0.039±0.005 0.025±0.006\nTable8: Cifar-10meanandstandarderrorofnegative\nlog-likelihoods over 5 runs, where the score for each\nTable 5: 2D GP mean and standard error of negative\nrunconsistsoftheaveragenegativelog-likelihoodover\nlog-likelihoods over 5 runs, where the score for each\n5,000 batches of 32. Best model in bold.\nrunconsistsoftheaveragenegativelog-likelihoodover\nModel NLL\n5,000 batches of 16. Best model in bold.\nNP 0.054±0.004\nModel RBF CNP 0.009±0.001\nNP 1.209±0.002 ANP −0.696±0.092\nCNP 1.174±0.002 CANP −0.663±0.096\nANP 0.753±0.012 TNP-D −1.416±0.004\nCANP 0.611±0.011 TNP-KR-Fast −0.823±0.012\nTNP-D 0.484±0.003 TNP-KR-Full −1.428±0.007\nTNP-KR-Fast 0.554±0.002\nTNP-KR-Full 0.482±0.005\n5 CONCLUSION\nIn this work, we introduce TNP-KR, an extension to\n4.3 Image Completion\nTNP-D that incorporates KRBlocks and fast atten-\ntion. We believe this represents an important step in\nThe last standard benchmarks consist of image com-\nscaling transformer-based models for large spatiotem-\npletion. In Tables 6, 7, and 8 we compare the same\nporal applications, which include population genetics,\nmodelsontheMNIST,CelebA,andCIFAR-10bench-\nepidemiology, and meteorlology, among others. In the\nmarks. Each model is trained on 100,000 batches of\nfuture, we plan to explore alternative kernels in both\nsize 32 and tested on 5,000 batches of size 32. In each\nfullandfastattention,aswellasattentionbiasthatex-\nof the samples, the model is presented with 200 test\nplicitly takes into account pairwise distances. We also\npointswhereanywherebetween3and100ofthemare\nplan to explore mechanisms for sampling transformer-\nprovided as context points.\nbased models that respect covariance structures in\nsuch a fashion that we could embed TNP-KR and its\nvariantsininferenceframeworkssuchasNumPyroand\nTable 6: MNIST mean and standard error of negative\nStan.\nlog-likelihoods over 5 runs, where the score for each\nrunconsistsoftheaveragenegativelog-likelihoodover\nAcknowledgements\n5,000 batches of 32. Best model in bold.\nModel NLL D.J. acknowledges his Google DeepMind scholarship.\nNP −0.716±0.007 E.S. acknowledges support in part by the AI2050 pro-\nCNP −0.776±0.006 gram at Schmidt Sciences (Grant [G-22-64476]). S.F.\nANP −0.865±0.021 acknowledges the EPSRC (EP/V002910/2).\nCANP −0.855±0.010\nTNP-D −1.189±0.007\nTNP-KR-Fast −1.049±0.006\nTNP-KR-Full −1.172±0.006\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nReferences Lee, Juho et al. (Oct. 27, 2020). Bootstrapping Neu-\nral Processes. doi: 10 . 48550 / arXiv . 2008 .\nBlei,DavidM.,AlpKucukelbir,andJonD.McAuliffe 02956. arXiv: 2008.02956[cs,stat]. url: http:\n(Apr.3,2017).“VariationalInference:AReviewfor / / arxiv . org / abs / 2008 . 02956 (visited on\nStatisticians”. In: Journal of the American Statis- 05/19/2024).\ntical Association 112.518, pp. 859–877. issn: 0162- Lindgren, Finn, Johan Lindstr¨om, and H˚avard Rue\n1459, 1537-274X. doi: 10.1080/01621459.2017. (2010).“Anexplicitlinkbetweengaussianfieldsand\n1285773. arXiv: 1601 . 00670[cs , stat]. url: gaussianmarkovrandomfields;thespdeapproach”.\nhttp://arxiv.org/abs/1601.00670 (visited on In.\n07/30/2024). Mishra, Swapnil et al. (Sept. 13, 2022). πVAE: a\nChoromanski, Krzysztof et al. (Nov. 19, 2022). stochastic process prior for Bayesian deep learn-\n“Rethinking Attention with Performers”. In: ing with MCMC. doi: 10.48550/arXiv.2002.\narXiv:2009.14794. doi: 10.48550/arXiv.2009. 06873. arXiv: 2002.06873[cs,stat]. url: http:\n14794. arXiv: 2009 . 14794[cs , stat]. url: / / arxiv . org / abs / 2002 . 06873 (visited on\nhttp://arxiv.org/abs/2009.14794 (visited on 07/30/2024).\n05/01/2024). Murphy,KevinP.(2023).ProbabilisticMachineLearn-\nGarnelo, Marta, Dan Rosenbaum, et al. (July 4, ing: Advanced Topics. MIT Press. url: http://\n2018). “Conditional Neural Processes”. In: probml.github.io/book2.\narXiv:1807.01613. doi: 10.48550/arXiv.1807. Nguyen, Tung and Aditya Grover (Feb. 7, 2023).\n01613. arXiv: 1807 . 01613[cs , stat]. url: Transformer Neural Processes: Uncertainty-Aware\nhttp://arxiv.org/abs/1807.01613 (visited on Meta Learning Via Sequence Modeling. doi: 10.\n07/03/2024). 48550 / arXiv . 2207 . 04179. arXiv: 2207 .\nGarnelo,Marta,JonathanSchwarz,etal.(2018).Neu- 04179[cs]. url: http://arxiv.org/abs/2207.\nral Processes. arXiv: 1807.01622 [cs.LG]. url: 04179 (visited on 04/13/2024).\nhttps://arxiv.org/abs/1807.01622. Rue, H˚avard, Sara Martino, and Nicolas Chopin\nGordon, Jonathan et al. (June 25, 2020). Convolu- (2009). “Approximate Bayesian inference for latent\ntional Conditional Neural Processes. arXiv: 1910. GaussianmodelsbyusingintegratednestedLaplace\n13556[cs,stat]. url: http://arxiv.org/abs/ approximations”. In: Journal of the Royal Statisti-\n1910.13556 (visited on 04/16/2024). cal Society Series B: Statistical Methodology 71.2,\nHe,Kaimingetal.(June2016).“DeepResidualLearn- pp. 319–392.\ning for Image Recognition”. In: pp. 770–778. doi: Semenova, Elizaveta, Prakhar Verma, et al. (2023).\n10.1109/CVPR.2016.90. “PriorCVAE: scalable MCMC parameter inference\nHeaton, Matthew J. et al. (Apr. 25, 2018). A Case withBayesiandeepgenerativemodelling”.In:arXiv\nStudy Competition Among Methods for Analyzing preprint arXiv:2304.04307.\nLarge Spatial Data. doi: 10.48550/arXiv.1710. Semenova, Elizaveta, Yidan Xu, et al. (June 2022).\n05013. arXiv: 1710 . 05013[stat]. url: http : “PriorVAE: Encoding spatial priors with VAEs for\n/ / arxiv . org / abs / 1710 . 05013 (visited on small-area estimation”. In: Journal of The Royal\n12/01/2023). Society Interface 19.191, p. 20220094. issn: 1742-\nHensman, James, Nicolo Fusi, and Neil D Lawrence 5662.doi:10.1098/rsif.2022.0094.arXiv:2110.\n(2013).“Gaussianprocessesforbigdata”.In:arXiv 10422[cs,stat]. url: http://arxiv.org/abs/\npreprint arXiv:1309.6835. 2110.10422 (visited on 07/30/2024).\nHuggins,Jonathanetal.(2020).“Validatedvariational Solin, Arno and Simo S¨arkk¨a (2020). “Hilbert space\ninference via practical posterior error bounds”. In: methods for reduced-rank Gaussian process regres-\nInternational Conference on Artificial Intelligence sion”. In: Statistics and Computing 30.2, pp. 419–\nand Statistics. PMLR, pp. 1792–1802. 446.\nKim, Hyunjik et al. (July 9, 2019). “Attentive Neural Wilson, Andrew Gordon, Christoph Dann, and\nProcesses”. In: arXiv:1901.05761. doi: 10.48550/ Hannes Nickisch (2015). “Thoughts on massively\narXiv.1901.05761. arXiv: 1901.05761[cs,stat]. scalable Gaussian processes”. In: arXiv preprint\nurl: http://arxiv.org/abs/1901.05761 (visited arXiv:1511.01870.\non 04/02/2024). Xiong, Ruibin et al. (2020). On Layer Normalization\nKingma, Diederik P and Max Welling (2022). Auto- intheTransformerArchitecture.arXiv:2002.04745\nEncoding Variational Bayes. arXiv: 1312 . 6114 [cs.LG]. url: https://arxiv.org/abs/2002.\n[stat.ML]. url: https://arxiv.org/abs/1312. 04745.\n6114. Yao, Yuling et al. (2018). “Yes, but did it work?:\nEvaluating variational inference”. In: International\nTransformer Neural Process - Kernel Regression\nConferenceonMachineLearning.PMLR,pp.5581–\n5590.\nZaheer, Manzil et al. (2018). “Adaptive Methods\nfor Nonconvex Optimization”. In: Advances in\nNeural Information Processing Systems. Ed. by\nS. Bengio et al. Vol. 31. Curran Associates,\nInc. url: https : / / proceedings . neurips .\ncc / paper _ files / paper / 2018 / file /\n90365351ccc7437a1309dc64e4db32a3-Paper.pdf.\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nAPPENDIX\nALGORITHMS\nAlgorithm 1 Transformer Encoder Block forward pass.\n1: Input: x , mask, p\n0 dropout\n2: x ←MultiheadAttention(x,x,x,mask)\n1\n3: x ←x +Dropout(x ,p )\n2 0 1 dropout\n4: x ←LayerNorm(x )\n3 2\n5: x ←FeedForward(x )\n4 3\n6: x ←x +Dropout(x ,p )\n5 3 4 dropout\n7: return x\n5\nAlgorithm 2 KRBlock forward pass.\nInput: qs , ks , mask, p\n0 0 dropout\nqs ←LayerNorm (qs )\n1 1 0\nks ←LayerNorm (ks )\n1 1 0\nqs ←MultiheadAttention(qs ,ks ,ks ,mask)\n2 1 1 1\nks ←MultiheadAttention(ks ,ks ,ks ,mask)\n2 1 1 1\nqs ←qs +Dropout(qs ,p )\n3 0 2 dropout\nks ←ks +Dropout(ks ,p )\n3 0 2 dropout\nqs ←LayerNorm (qs )\n4 2 3\nks ←LayerNorm (ks )\n4 2 3\nqs ←FeedForward(qs )\n5 4\nks ←FeedForward(ks )\n5 4\nqs ←qs +Dropout(qs ,p )\n6 3 5 dropout\nks ←ks +Dropout(ks ,p )\n6 3 5 dropout\nreturn qs ,ks\n6 6\nTransformer Neural Process - Kernel Regression\nADDITIONAL FIGURES\nTNP-KR Fast\nTNP-KR Full\n0.08\nTNP-D\n0.06\n0.04\n0.02\n0.00\n1 10 100 1000 10000 100000 1000000\nNum. Test\nFigure 3: Scaling test points in TNP models, keeping number of context points fixed at 100.\nTNP-KR Fast\nTNP-KR Full\n0.08 TNP-D\n0.06\n0.04\n0.02\n0.00\n1 10 100 1000 10000 100000 1000000\nNum. Context\nFigure 4: Scaling context points in TNP, keeping number of test points fixed at 100.\nelpmaS\nrep\nsdnoceS\nelpmaS\nrep\nsdnoceS\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nFigure 5: 1D GP Matern 3/2 samples. The black line is the true function, the black dots are noisy observations,\nthe blue line is the model’s mean prediction, and the shaded blue is the uncertainty. Models perform similarly,\nbut ConvCNP and the TNP variants have the tightest uncertainty bounds.\nTransformer Neural Process - Kernel Regression\nFigure6: 1DGPperiodicsamples. Theblacklineisthetruefunction, theblackdotsarenoisyobservations, the\nblue line is the model’s mean prediction, and the shaded blue is the uncertainty. Some models struggle to model\nlow period periodic functions, particularly NPs. ConvCNP and TNP variants have the tightest uncertainty\nbounds.\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nFigure 7: 1D GP RBF samples. The black line is the true function, the black dots are noisy observations, the\nblue line is the model’s mean prediction, and the shaded blue is the uncertainty. Most models do well on this\ntask, but ConvCNP and the TNP variants have marginally tighter uncertainty bounds.\nTransformer Neural Process - Kernel Regression\nFigure8: 2DGPRBFsample. IntheTaskpanel,maskedbluepixelsareunobervedlocations. IntheUncertainty\npanel,warmercolorssignifygreateruncertainty. NPandCNPstruggletomodelintwodimensions. TNPvariants\nperform better than ANP variants, largely due to the increased accuracy of transformer blocks.\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nFigure 9: MNIST sample. In the Task panel, masked blue pixels are unoberved locations. In the Uncertainty\npanel, warmer colors signify greater uncertainty. Predictions are relatively consistent among models, but the\nTNP variants have better uncertainty bounds, particularly around observed context points.\nTransformer Neural Process - Kernel Regression\nFigure 10: CelebA sample. In the Task panel, masked blue pixels are unoberved locations. In the Uncertainty\npanel,warmercolorssignifygreateruncertainty. NPandCNPtendtooversmooth,whileTNPandANPvariants\nare able to identify more details, particularly at observed context points. There are purple dots for TNP and\nANP variants, indicating that the model is highly confident in the uncertainty bounds around observed points,\nbut NP and CNP tend to oversmooth at context points.\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nFigure 11: Cifar 10 sample. In the Task panel, masked blue pixels are unoberved locations. In the Uncertainty\npanel,warmercolorssignifygreateruncertainty. NPandCNPmodelsoversmooth,whileTNPandANPvariants\nappear grainy due to attention focusing on pixels in different regions of the image.",
    "pdf_filename": "Transformer_Neural_Processes_--_Kernel_Regression.pdf"
}