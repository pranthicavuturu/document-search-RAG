{
    "title": "Transformer Neural Processes -- Kernel Regression",
    "context": "Stochastic processes model various natu- ral phenomena from disease transmission to stock prices, but simulating and quantify- ing their uncertainty can be computationally challenging. For example, modeling a Gaus- sian Process with standard statistical meth- ods incurs an O(n3) penalty, and even us- ing state-of-the-art Neural Processes (NPs) incurs an O(n2) penalty due to the attention mechanism. We introduce the Transformer Neural Process - Kernel Regression (TNP- KR), a new architecture that incorporates a novel transformer block we call a Kernel Regression Block (KRBlock), which reduces the computational complexity of attention in transformer-based Neural Processes (TNPs) from O((nC + nT )2) to O(n2 C + nCnT ) by eliminating masked computations, where nC is the number of context, and nT is the num- ber of test points, respectively, and a fast at- tention variant that further reduces all at- tention calculations to O(nC) in space and time complexity. In benchmarks spanning such tasks as meta-regression, Bayesian opti- mization, and image completion, we demon- strate that the full variant matches the per- formance of state-of-the-art methods while training faster and scaling two orders of mag- nitude higher in number of test points, and the fast variant nearly matches that perfor- mance while scaling to millions of both test and context points on consumer hardware. Preliminary work. Correspondence to: daniel.jenson@worc.ox.ac.uk. ∗These authors jointly supervised this work. 1 The principle challenge of modern spatiotemporal Bayesian modeling is scale. As the number of ob- served locations increases from tens to thousands or hundreds of thousands, traditional techniques used to model spatiotemporal phenomena break down. Per- haps the most common method typically employed to model spatiotemporal processes is the Gaussian Process (GP). Gaussian Processes are a particularly well-behaved class of stochastic processes. Specifi- cally, for a finite index set {t ∈T}, the collection X = (X1, . . . , XT ) follows a multivariate Gaussian distribution. This makes various analytic calculations tractable, facilitating regression, marginalization, and sampling with GPs. While GPs provide a significant degree of flexibility in modeling, the analytic solutions they yield do not scale well in the number of observed locations. Using a GP to model spatial random effects within a Markov Chain Monte Carlo (MCMC) sampler incurs an O(n3) cost per sample, where n is the number of observed lo- cations. This is because the covariance matrix must be inverted, or factorized in the case of Cholesky de- composition, at each iteration in order to generate a sample. Unfortunately, this means that for only n = 1,000 locations, nearly a billion operations must be performed to generate a single sample. In order to accelerate Bayesian inference with spa- tiotemporal stochastic processes, there have been at least three prominent strains of research. The first is Variational Inference (VI), which aims to recast the in- ference problem as an optimization problem and max- imize the Evidence Lower Bound (ELBO). The second aims to accelerate sampling by using a generative neu- ral network-based approximation. This family tends to leverage Variational Autoencoders (VAEs). The third is a recent family of deep learning models called Neural Processes (NPs). These models use a meta- learning objective, meaning that once trained, the for- ward pass of the model takes as input “context” or observed points and returns a function. This function arXiv:2411.12502v1  [cs.LG]  19 Nov 2024",
    "body": "Transformer Neural Process - Kernel Regression\nDaniel Jenson1\nJhonathan Navott1\nMengyan Zhang1\nMakkunda Sharma1\nElizaveta Semenova2,∗\nSeth Flaxman1,∗\nUniversity of Oxford1\nImperial College London2\nAbstract\nStochastic processes model various natu-\nral phenomena from disease transmission to\nstock prices, but simulating and quantify-\ning their uncertainty can be computationally\nchallenging. For example, modeling a Gaus-\nsian Process with standard statistical meth-\nods incurs an O(n3) penalty, and even us-\ning state-of-the-art Neural Processes (NPs)\nincurs an O(n2) penalty due to the attention\nmechanism.\nWe introduce the Transformer\nNeural Process - Kernel Regression (TNP-\nKR), a new architecture that incorporates\na novel transformer block we call a Kernel\nRegression Block (KRBlock), which reduces\nthe computational complexity of attention in\ntransformer-based Neural Processes (TNPs)\nfrom O((nC + nT )2) to O(n2\nC + nCnT ) by\neliminating masked computations, where nC\nis the number of context, and nT is the num-\nber of test points, respectively, and a fast at-\ntention variant that further reduces all at-\ntention calculations to O(nC) in space and\ntime complexity.\nIn benchmarks spanning\nsuch tasks as meta-regression, Bayesian opti-\nmization, and image completion, we demon-\nstrate that the full variant matches the per-\nformance of state-of-the-art methods while\ntraining faster and scaling two orders of mag-\nnitude higher in number of test points, and\nthe fast variant nearly matches that perfor-\nmance while scaling to millions of both test\nand context points on consumer hardware.\nPreliminary work.\nCorrespondence to: daniel.jenson@worc.ox.ac.uk.\n∗These authors jointly supervised this work.\n1\nINTRODUCTION\nThe principle challenge of modern spatiotemporal\nBayesian modeling is scale.\nAs the number of ob-\nserved locations increases from tens to thousands or\nhundreds of thousands, traditional techniques used to\nmodel spatiotemporal phenomena break down. Per-\nhaps the most common method typically employed\nto model spatiotemporal processes is the Gaussian\nProcess (GP). Gaussian Processes are a particularly\nwell-behaved class of stochastic processes.\nSpecifi-\ncally, for a finite index set {t ∈T}, the collection\nX = (X1, . . . , XT ) follows a multivariate Gaussian\ndistribution. This makes various analytic calculations\ntractable, facilitating regression, marginalization, and\nsampling with GPs.\nWhile GPs provide a significant degree of flexibility\nin modeling, the analytic solutions they yield do not\nscale well in the number of observed locations. Using\na GP to model spatial random effects within a Markov\nChain Monte Carlo (MCMC) sampler incurs an O(n3)\ncost per sample, where n is the number of observed lo-\ncations. This is because the covariance matrix must\nbe inverted, or factorized in the case of Cholesky de-\ncomposition, at each iteration in order to generate a\nsample. Unfortunately, this means that for only n =\n1,000 locations, nearly a billion operations must be\nperformed to generate a single sample.\nIn order to accelerate Bayesian inference with spa-\ntiotemporal stochastic processes, there have been at\nleast three prominent strains of research. The first is\nVariational Inference (VI), which aims to recast the in-\nference problem as an optimization problem and max-\nimize the Evidence Lower Bound (ELBO). The second\naims to accelerate sampling by using a generative neu-\nral network-based approximation. This family tends\nto leverage Variational Autoencoders (VAEs).\nThe\nthird is a recent family of deep learning models called\nNeural Processes (NPs).\nThese models use a meta-\nlearning objective, meaning that once trained, the for-\nward pass of the model takes as input “context” or\nobserved points and returns a function. This function\narXiv:2411.12502v1  [cs.LG]  19 Nov 2024\n\nTransformer Neural Process - Kernel Regression\ncan then be evaluated at any collection of test points\nand returns both their mean predictions and associ-\nated uncertainties.\nThe NP family of models has grown rapidly over the\nlast few years, but recently Transformer Neural Pro-\ncesses (TNPs), including TNP-D, TNP-ND, and TNP-\nA have come to dominate the landscape (Nguyen and\nGrover, 2023).\nThese models, however, suffer from\nan O(n2) complexity due to the attention mechanism\nused in transformer encoder blocks. We extend this\nfamily with the Transformer Neural Process - Ker-\nnel Regression (TNP-KR) model.\nTNP-KR uses a\nnovel transformer block we call a Kernel Regression\nBlock (KRBlock), which reduces the cost of atten-\ntion in transformer-based NPs from O((nC + nT )2)\nto O(n2\nC + ncnT ) where nC is the number of context\npoints and nT is the number of test points. We also\nintroduce an even faster variant, which uses Performer\nattention inside the KRBlock (Choromanski et al.,\n2022). Performer attention uses a kernel approxima-\ntion to softmax attention and further reduces the com-\nplexity to O(nC), enabling the model to scale to mil-\nlions of points on consumer hardware. We show that\nthe full variant matches the performance of state-of-\nthe-art methods while training faster and scaling two\norders of magnitude higher in number of test points,\nand the fast variant nearly matches that performance\nwhile scaling to millions of both test and context points\non consumer hardware.\n2\nBACKGROUND\nA number of general techniques have been developed\nto reduce the computational burden of modeling large\nspatiotemporal datasets. These include, but are not\nlimited to variational inference (VI) (Blei, Kucukel-\nbir, and McAuliffe, 2017), stochastic process emula-\ntion (Mishra et al., 2022; Semenova, Xu, et al., 2022;\nSemenova, Verma, et al., 2023), and neural processes\n(NPs) (Garnelo, Schwarz, et al., 2018; Garnelo, Rosen-\nbaum, et al., 2018; Kim et al., 2019; Lee et al., 2020;\nGordon et al., 2020; Nguyen and Grover, 2023). There\nis also a long literature on approximate methods to\nscale up Gaussian processes in particular which we\ndo not cover in detail, see e.g. Hensman, Fusi, and\nLawrence (2013), Rue, Martino, and Chopin (2009),\nSolin and S¨arkk¨a (2020), Wilson, Dann, and Nick-\nisch (2015), and Lindgren, Lindstr¨om, and Rue (2010).\nWhile using distinct algorithmic approaches, all these\nmethods provide approximations to the posterior dis-\ntributions.\n2.1\nVariational Inference (VI)\nVI (Blei, Kucukelbir, and McAuliffe, 2017; Murphy,\n2023) approximates the posterior distribution by fram-\ning inference as an optimization problem, aiming to\nmaximize the Evidence Lower Bound (ELBO) by min-\nimizing the Kullback-Leibler (KL) divergence between\na variational distribution qψ(z) and the true posterior\npθ(z | x). Although VI is widely used, its effectiveness\ndepends on selecting an appropriate variational family,\nand there are no guarantees on how close the ELBO\nis to the true log-likelihood, making uncertainty esti-\nmation challenging when the variational family poorly\napproximates the true distribution (Yao et al., 2018;\nHuggins et al., 2020).\n2.2\nStochastic Process Emulation\nAnother line of research aims to accelerate sampling by\napproximating samples from computationally inten-\nsive stochastic processes. This is the aim of models like\nPriorVAE, PriorCVAE, and πVAE (Semenova, Xu, et\nal., 2022; Semenova, Verma, et al., 2023; Mishra et al.,\n2022). Currently these models are all based on Vari-\national Autoencoders (VAEs) (Kingma and Welling,\n2022). VAEs consist of an encoder and decoder com-\nbined with a latent sampling process.\nThey encode\nraw data into a vector of latent parameters, which are\nthen used to sample a latent vector. This latent vec-\ntor is then passed through the decoder, whose objec-\ntive is to recreate the original data. The advantage\nof models like these is that if the latent distribution\nis simple, i.e. a multivariate normal with diagonal co-\nvariance, it can be very easy to sample. This means\nthat a fully trained network can generate new sam-\nples from the original data distribution by sampling\nlatents and passing them through the decoder. Fur-\nthermore, this can often be done in time linear in the\nnumber of layers in the network, which can be two\norders of magnitude faster than sampling from a real\nGP (Semenova, Verma, et al., 2023). Because neural\nnetworks are inherently differentiable, they can also\nbe transparently integrated into inference frameworks\nlike NumPyro, where gradient-informed samplers like\nthe No-U-Turn Sampler (NUTS) can easily pass gradi-\nents through the model. The principle challenge with\nthis class of models is that the number of input and\noutput locations is fixed and ordered, which means a\nnew model must be retrained each time the number\nof observed locations changes or the location associ-\nated with each input changes. These models are also\nsensitive to the dimensionality of the latent vector,\nwhich induces an information bottleneck on autoen-\ncoded data and can cause oversmoothing in generated\nsamples.\n\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\n2.3\nNeural Processes (NPs)\nNeural Processes (NPs) are a family of models that use\ndeep neural networks to represent stochastic processes.\nNPs are considered “meta-learners” because, instead\nof modelling a single function h : s →f, they take as\ninput context points (sC, fC) and return a distribution\nover functions g : (sC, fC) →Ph|sC,fC(h : sT →fT ),\nallowing evaluation at test points sT without retrain-\ning. To be valid stochastic processes, NPs must ensure\ninvariance to context/test point ordering and consis-\ntency under marginalization.\nThere are two main classes of Neural Processes, latent\nneural processes and conditional neural processes. La-\ntent neural processes dedicate part of their architec-\nture to generating latent parameters, which are sam-\npled and passed through the decoder to generate co-\nherent samples, similar to VAEs.\nThe fundamental\nassumption of latent NPs is that the test points are\nindependent conditional on the latent vector. For in-\nstance, if (sC, fC) represents a tuple of locations and\nfunction values at context (observed) points, (sT , fT )\nrepresents locations and function values at test (un-\nobserved) points, and z represents a sampled latent\nvector, the likelihood of the function values at test lo-\ncations can be formulated as follows:\np(fT | sT , sC, fC)\n=\nZ\np(z | sC, fC)p(fT | sT , z)dz\n=\nZ\np(z | sC, fC)\n|T |\nY\ni=1\np(f (i)\nT\n| s(i)\nT , z)dz\nConditional neural processes often have very similar\narchitectures, except they avoid sampling a latent vec-\ntor and condition on a fixed representation, r, of the\ncontext points. This implies the following factoriza-\ntion, assuming the encoder has already processed the\ncontext points, enc(sC, fC) = r:\np(fT | sT , sC, fC) =\n|T |\nY\ni=1\np(f (i)\nT\n| s(i)\nT , r)\nIn practice, conditional neural processes tend to per-\nform better. It is unclear whether this is because the\nlatent vectors, z, are an insufficient representation of\nthe latent parameters, or if the models are expend-\ning some of their finite capacity on producing good\nestimates of z at the cost of final predictive accuracy.\nEither way, the most performant NPs as measured by\nlog-likelihood scores are conditional, specifically con-\nditional transformer neural processes (TNPs).\nThe advantage of NPs is that they can be pretrained\nacross a variety of priors and number of context points.\nAnd at test time, they can perform GP regression\nfor thousands of paths simultaneously in seconds on\na GPU. This can be several orders of magnitude faster\nthan calculating uncertainty for each path within an\ninference framework. The disadvantage of these net-\nworks is that previously they significantly underfit\nrelative to true GPs, particularly at context or ob-\nserved data points. However, NPs have evolved from\nsimple MLP-based networks to sophisticated convo-\nlutional and transformer-based architectures.\nThese\nnewer variants are often just as accurate as baseline\nGPs and at least an order of magnitude faster.\nWe detail the essential aspects of modern canonical de-\nsigns in this section. For brevity, we omit the original\nNP (Garnelo, Schwarz, et al., 2018) and CNP (Gar-\nnelo, Rosenbaum, et al., 2018) models, which used an\nautoencoder framework with MLPs. Also relevant, but\nnot included here are Bootstrapping Neural Processes\n(Lee et al., 2020), which represent an extension to all\nNP models.\n2.3.1\nAttentive Neural Process (ANP)\nThe Attentive Neural Process (ANP) was introduced\nto address several limitations of the NP and CNP ar-\nchitectures (Kim et al., 2019). In particular, NPs and\nCNPs underfit on observed context points and gener-\nally tend to oversmooth the posterior predictive. One\nof the principle reasons for underfitting is that the de-\ncoder is unable to differentially attend to each of the\ncontext points when decoding at test points. For in-\nstance, if one context point is located at sc = −2 and\nthe test point is at st = 2, i.e. opposite ends of the\ntraining region, sc equally influences the context vec-\ntor used to decode at st despite having little to no\ninfluence on the behavior of function at st = 2. Ac-\ncordingly, the authors propose an attentive version of\nthe NP that allows both local and global context to be\nincorporated when decoding at test locations.\nThe ANP shares the NP architecture, but replaces\nMLPs with multihead dot product self-attention in\nboth the latent and deterministic encoding paths.\nThen, in the deterministic path it adds cross-attention\nbetween test points and context points so that the as-\nsociated context vector, rt, summarizes information\nrelevant to the test location st, rather than being a\nglobal representation. This localized context vector,\nrt, is passed through the decoder with the global la-\ntent representation z, and the location, st, to produce\n(µt, σ2\nt ).\nWith the ANP and its conditional variant\nCANP, the NP family started to become a viable re-\nplacement to true GPs for inference.\n\nTransformer Neural Process - Kernel Regression\n2.3.2\nConvolutional Conditional Neural\nProcess (ConvCNP)\nConvolutional Conditional Neural Processes (Con-\nvCNP) were designed to incorporate translation equiv-\nariance into NPs (Gordon et al., 2020). When a model\nexhibits translation equivariance, it is able to identify\na feature or function behavior regardless of how far\nits input has shifted in the domain.\nThis improves\nthe model’s capacity to generalize beyond the training\nregion.\nThe authors of ConvCNP define two architectures,\none for on-the-grid data and one for off-the-grid data.\nHere we detail the off-the-grid version since it is more\ngeneric and can be used in both cases. First, the do-\nmain is partitioned into a fixed, uniform grid. Then,\na positive-definite Reproducing Kernel Hilbert Space\n(RKHS) kernel is evaluated at each of the grid points\nusing the context set. This value is then normalized\nusing a density channel so the point values are invari-\nant to the cardinality of the context set.\nThis grid\nis then run through a CNN-based architecture, e.g.\nResNet (He et al., 2016), to create an updated hidden\nstate representation at grid locations. Finally, the de-\ncoder uses another RHKS kernel, typically the same\none used in the encoder, to decode test points using\nthe underlying hidden state grid values.\nConvCNPs perform very well on the standard NP\nbenchmarks, often exceeding the performance of other\nmodels at a fraction of the number of learnable param-\neters. However, beyond simple examples, ConvCNP\nrequires many more parameters, often on the same or-\nder as other models, to perform competitively. Fur-\nthermore, ConvCNP is very sensitive to the parame-\nterization of the intermediate grid and the effective re-\nceptive field of the CNN layer. For instance, at lower\nlengthscales, the model performs better when there is\na higher grid resolution, but this increase in grid reso-\nlution changes the receptive field of the CNN layer, so\nthe CNN’s kernels must be optimized in conjunction.\nLastly, due to the fixed intermediate grid, ConvCNP\nsuffers from the curse of dimensionality and is difficult\nto evaluate in non-contiguous regions of the domain.\n2.3.3\nTransformer Neural Processes (TNP)\nTransformer Neural Processes (TNPs) (Nguyen and\nGrover, 2023) can be considered an extension of the\nConditional Attentive Neural Process (CANP) that\nuses multiple transformer encoder blocks instead of\nstacked self-attention and cross-attention.\nA trans-\nformer encoder block consists of a self-attention layer\nfollowed by a feedfoward network with residual con-\nnections interspersed. The pseudocode can be viewed\nin Appendix 5.\nBecause standard transformers are designed to work\nwith sequences of data and use fixed positional em-\nbeddings, the TNP authors had to modify the archi-\ntecture for NPs.\nAccordingly, TNPs dispense with\nthe positional embeddings, merge the context and test\nsequences as input, and introduce a special atten-\ntion mask. The context sequence consists of location\nand function value pairs, [(s1, f1), . . . , (snC, fnC)], and\nthe test sequence consists of location and zero-padded\nfunction value pairs, [(s1, 0), . . . , (snT , 0)].\nWithin\neach layer, a mask is applied that prevents context\npoints from attending to test points and prevents test\npoints from attending to other test points. This means\nthat context points only attend to other context points\nand test points also only attend to context points. Af-\nter the encoding stage, the embeddings for test points\nare passed through a prediction head that estimates\nthe mean and covariance structures.\nThere are three original TNP variants: TNP-D, TNP-\nND, and TNP-A. TNP-D (Diagonal) assumes test\npoints can be factorized independently conditional on\nthe context points, in line with most CNP variants.\nTNP-ND (Non-Diagonal) parameterizes a covariance\nmatrix by estimating the elements in the lower tri-\nangular matrix of a Cholesky decomposition for test\npoints. Lastly, TNP-A (Autoregressive) assumes that\nthe test points can be factorized autoregressively. This\nmeans that each time a test point is predicted, its true\nvalue is then added to the context set and used when\npredicting the next test point.\nIn practice, we found TNP-D to consistently perform\nwell.\nOn the other hand, we found TNP-ND could\nbecome arbitrarily miscalibrated.\nWhen there are\nvery few context points, TNP-ND maximizes the log-\nlikelihood by collapsing the standard deviation for test\npoints that lie near context points, which provides very\nprecise estimates. However, this also causes the model\nto collapse uncertainty estimates for test points that lie\nfar away from context points, leading to both high log-\nlikelihoods and high miscalibration rates. On the other\nhand, TNP-A assumes that the true test points are ob-\nserved autoregressively, which is equivalent to running\nany NP model forward one test point at a time. This\nmethod yields log-likelihoods that correspond to se-\nquential observations, rather than joint distributions\nover all unobserved points. As we are interested in the\ngeneral case, we do not include experiments on TNP-\nA.\n\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nLayerNorm\nLayerNorm\nMultihead \nAttention\nMultihead \nAttention\nqueries\nqueries\nvalues\nkeys\nqs\nkeys\nvalues\nLayerNorm\nLayerNorm\nFFN\nFFN\nqs' ks'\nks\nFigure 1: KRBlock architecture.\nQueries and keys\nhave their own residual pathways, but keys are used\nto update queries through cross-attention.\n3\nTRANSFORMER NEURAL\nPROCESS - KERNEL\nREGRESSION (TNP-KR)\n3.1\nKRBlock\nTNP-KR was inspired by TNP-D, which consistently\nperforms well on NP benchmarks and does not make\nthe same assumptions as TNP-A or suffer from miscal-\nibration like TNP-ND. One of the principal limitations\nof all TNP variants, however, is the O((nC +nT )2) at-\ntention used by the encoder layers.\nRecall that nC\nis the number of context points and nT is the num-\nber of test points. This also is not strictly necessary\nbecause the special mask applied after attention dis-\ncards many of the calculations, namely, those from\ncontext points to test points and those from test points\nto other test points.\nAccordingly, we introduce the\nKRBlock, a transformer block that avoids computing\nO(n2\nT + nCnT) attention values altogether.\nUnlike\nthe original transformer encoder block, TNP-KR also\nuses pre-normalized residual connections (Xiong et al.,\n2020), which has been shown to stabilize training and\nimprove performance.\nWe call this a KRBlock because the cross-attention\nfrom test to context points can be viewed as a form\nof Nadaraya-Watson kernel regression where the loca-\ntions are the feature embeddings and the kernel is a\ndot product softmax kernel. Specifically, if α is the\ndot product softmax kernel, qi is a query embedding\nrepresenting a test point, kj is a key embedding rep-\nresenting a context point, and vj is the value embed-\nding associated with the same context point, cross-\nattention in the KRBlock can be formulated explicitly\nas Nadaraya-Watson kernel regression:\nf(qi) =\nX\nj\nα(qi, kj)\nP\nj α(qi, kj)vj\n(1)\nStacking KRBlocks then allows the model to perform\niterative kernel regression on increasingly complex\ninternal representations of test and context points.\nCross-attention from test to context points costs\nO(nCnT ) and the self-attention among contxt points\ncosts O(n2\nC), making the total complexity O(n2\nC +\nnCnT ).\nWhen there are a large number of context points,\nthe O(n2\nC) term can still prove computationally pro-\nhibitive. For example, in satellite imagery, a compar-\natively small image of 300x500 results in 150,000 lo-\ncations or pixels (Heaton et al., 2018).\nA common\napplication is to inpaint pixels missing due to cloud\ncover. With 20% of pixels missing, there would still be\n120000 context points. This means that even with a\nKRBlock, the space and time complexity would be on\nthe order of 1200002, requiring nearly 60GB of mem-\nory and 14.4 billion multiplications per self-attention\napplication. Thus, in order to further scale KRBlocks,\nwe incorporate Performer attention, also known as fast\nattention (Choromanski et al., 2022).\n3.2\nFast Attention\nFast attention is based on a complete algorithm called\nFast Attention with Orthogonal Random features (FA-\nVOR+). FAVOR+ allows attention to be calculated in\nlinear space and time complexity without making any\nassumptions about the sparsity or rank of the attention\nmatrix. It is nearly unbiased and offers uniform con-\nverenge and low variance (Choromanski et al., 2022).\nFAVOR+ constructs an attention matrix AL×L where\nA(i, j) = K(qi, kj) without ever fully materializing\nit. For a randomized mapping ϕ : Rd →Rr\n+, entries\ncan be estimated with K(q, k) = E [ϕ(q)⊺ϕ(k)].\nIn\nfact, most kernels can be modeled using the following\nmapping:\nϕ(x) = h(x)\n√m\n\u0014\nf1(ω⊺\n1x), . . . , f1(ω⊺\nmx), . . . ,\nfl(ω⊺\n1x), . . . , fl(ω⊺\nmx)\n\u0015\nwhere f1, . . . , fl\n: R →R, h : Rd\n→R, and\nω1, . . . , ωm\niid\n∼D. When D = N(0, Id), this leads to\n\nTransformer Neural Process - Kernel Regression\nthe Gaussian kernel, Kgauss. Ignoring the\n√\nd normal-\nization, the softmax kernel is defined as SM(q, k) :=\nexp(q⊺k). Furthermore, the softmax kernel can be ex-\npressed as a function of Kgauss:\nSM(q, k) = exp\n\u0012∥q∥2\n2\n\u0013\nKgauss(q, k) exp\n\u0012∥k∥2\n2\n\u0013\nThus, the softmax kernel can be defined using ϕ where\nh(x) = exp\n\u0010\n∥x∥2\n2\n\u0011\n, f1 = sin, and f2 = cos. Unfortu-\nnately, this representation behaves poorly around zero\nwhen the trigonometric functions return negative val-\nues, so Choromanski et al. introduce positive random\nfeatures (PRFs) for softmax. This results in the form:\nSM(q, k) = Eω∼N (0,Id)\n\u0014\neω⊺q−∥q∥2\n2 eω⊺k−∥k∥2\n2\n\u0015\nwhich implies h(x) = exp\n\u0010\n−∥x∥2\n2\n\u0011\nand f1 = exp.\nThis approximation is further improved by forcing the\nω random vectors to be normalized and orthogonal\n(OR+).\nMapping the query and key matrices through these\nrandom projections, ϕ(Q) = Q′ ∈RL×r and ϕ(K) =\nK′ ∈RL×r, attention can be reexpressed as follows:\n\\\nAttention(Q, K, V) = ˆD−1(Q′((K′)⊺V)\n|\n{z\n}\nO(Lrd)\n),\nˆD = diag(Q′((K′)⊺1L)).\nThus, by carefully controlling the order of matrix mul-\ntiplications, space complexity can be reduced from\nO(L2 + Ld) to O(Lr + Ld + rd) and time complexity\ncan be reduced from O(L2d) to O(Lrd). We demon-\nstrate the impact of the KRBlock and fast attention\non runtime and memory in Tables 1 and 2.\n4\nEXPERIMENTS\nIn order to evaluate the performance of TNP-KR,\nwe test its performance across standard benchmarks,\nwhich include GPs, image completion, and simple\nBayesian Optimization (BO). All experiments were\nperformed on a single 24GB Nvidia RTX 4090 GPU.\n4.1\n1D Gaussian Processes\nFor one-dimensional GPs, we evaluate the principal\nNP models on the the RBF, periodic, and Mat´ern 3/2\nkernels. For each of these kernels, σ2 can be factored\nout and used to standardize the data. Accordingly, in\nour tests, we assume the data has been standardized\nso the models can focus on learning and differentiat-\ning lengthscales. While standard benchmarks sample\nlengthscales from 0.1 to 0.6 uniformly to model over\ndomain on the domain [−2, 2], we sample lengthscales\naccording to ℓ∼Beta(α = 3, β = 7), which has both\na mean and median of approximately 0.3. This is a\nmore challenging benchmark that allows for greater\ndifferentiation among models since more than 50% of\nlengthscales fall below 0.3 and less than 10% lie above\n0.5. (In practice, we found most models could easily\nlearn lengthscales above 0.5).\nFor the periodic ker-\nnel, we also sample the period uniformly from 0.5 to\n2, which represents between 2 and 8 cycles on the do-\nmain [−2, 2].\nEach model is seeded 5 times and trained on 100,000\nbatches of size 32. For each seed, the models are eval-\nuated on a final test set consisting of 5,000 batches\nof size 32. Each sample in each batch consists of 50\nrandomly sampled locations and 100 linearly spaced\npoints throughout the domain.\nBetween 3 and 50\nof the randomly sampled points are used as context\npoints, and the test points consist of all 150 points.\nBecause many of these models use different heuris-\ntics to prevent the collapse of standard deviation at or\nnear context points in the test set, we add an obser-\nvation noise of 0.1. This added noise prevents collapse\nand allows us to avoid using model-dependent heuris-\ntics, which could artificially hamper their performance\nand prevent honest comparison. Lastly, all models are\ntrained with a single cosine annealing learning rate\nschedule, gradient norm clipping with a max norm of\n3.0, and the YOGI optimizer (Zaheer et al., 2018). Ta-\nble 3 shows the mean and standard errors across these\ntest sets for each model and each kernel.\nTable 1:\nMicroseconds (µs) per sample by varying\nnumber of test points and keeping context points fixed\nat 100. “OOM” indicates Out of Memory error.\n# Test\nTNP-D\nTNP-KR-Full\nTNP-KR-Fast\n100\n118\n171\n194\n101\n119\n287\n327\n102\n129\n267\n282\n103\n286\n230\n226\n104\n45, 000\n474\n436\n105\nOOM\n7, 040\n4, 510\n106\nOOM\n86, 800\n64, 300\nThese models can then be used in a simple one-\ndimensional\nBayesian\nOptimization\n(BO)\nsetting\nwhere each model is given ten observed starting points.\nBO then uses the model to calculate the expected im-\nprovement across the domain and selects the point\nwhere that metric is greatest. There is a fixed bud-\nget of 50 iterations and the objective is to identify\nthe function’s minimum. Table 4 shows the mean and\nstandard error of regret across 5 seeds for each model\n\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nFigure 2: Posterior predictive samples for TNP-KR-Fast. Black line is the true function, black dots are noisy\nobservations, blue line is model mean prediction, and shaded blue area is 95% confidence region.\nTable 2:\nMicroseconds (µs) per sample by varying\nnumber of context points and keeping test points fixed\nat 100. “OOM” indicates Out of Memory error.\n# Context\nTNP-D\nTNP-KR-Full\nTNP-KR-Fast\n100\n99\n98\n138\n101\n99\n220\n277\n102\n114\n286\n299\n103\n247\n302\n234\n104\n44,700\n36,700\n632\n105\nOOM\nOOM\n6,900\n106\nOOM\nOOM\n88,400\nand each kernel. There are 100 tasks per seed. Here,\nConvCNP performs best across the board, which is\nlikely due to the fact that the convolutional kernels in\nthis model introduce a strong local inductive bias, so it\nis able to identify functional patterns that fall within\nits kernel width with very few context points.\n4.2\n2D Gaussian Processes\nFor the two-dimensional Gaussian Processes, we test\nthe the RBF kernel on the domain [−2, 2]2. The test\npoints consist of 128 randomly selected points on the\ndomain as well as 256 on a uniform grid over the do-\nmain. The context points consist of between 12 and\nTable 3: 1D GP mean and standard error of negative\nlog-likelihoods over 5 runs, where the score for each\nrun consists of the average negative log-likelihood over\n5,000 batches of 32. Best model in bold.\nModel\nMatern 3/2\nPeriodic\nRBF\nNP\n0.638 ± 0.003\n1.342 ± 0.001\n0.338 ± 0.007\nCNP\n0.547 ± 0.002\n1.165 ± 0.002\n0.238 ± 0.006\nBNP\n0.523 ± 0.003\n1.165 ± 0.003\n0.165 ± 0.005\nANP\n0.043 ± 0.008\n0.829 ± 0.007\n−0.341 ± 0.003\nCANP\n0.047 ± 0.014\n0.818 ± 0.005\n−0.310 ± 0.024\nBANP\n0.023 ± 0.007\n0.761 ± 0.011\n−0.347 ± 0.008\nConvCNP\n−0.018 ± 0.003\n0.543 ± 0.003\n−0.456 ± 0.003\nTNP-D\n−0.023 ± 0.003\n0.516 ± 0.003\n−0.455 ± 0.004\nTNP-KR-Fast\n−0.006 ± 0.002\n0.562 ± 0.006\n−0.425 ± 0.005\nTNP-KR-Full\n−0.024 ± 0.002\n0.516 ± 0.002\n−0.453 ± 0.004\n128 of the randomly selected points, which represents\nbetween 5% and 50% of the number on the uniform\ngrid.\nEach model is trained on 100,000 batches of\nsize 16 and tested on 5,000 batches of size 16. The\nother training settings remain the same as the one-\ndimensional case.\nHere TNP-D and TNP-KR with\nfull attention perform best. Although ConvCNP was\nomitted due to the time required to optimize its kernel\nand grid for the benchmark, we believe it would also\nperform competitively here.\n\nTransformer Neural Process - Kernel Regression\nTable 4: 1D Bayesian Optimization mean and stan-\ndard error of 500 samples on domain [-2, 2].\nBest\nmodel in bold.\nModel\nMatern 3/2\nPeriodic\nRBF\nNP\n0.145 ± 0.013\n0.131 ± 0.010\n0.079 ± 0.010\nCNP\n0.106 ± 0.011\n0.331 ± 0.020\n0.061 ± 0.009\nBNP\n0.126 ± 0.012\n0.293 ± 0.019\n0.069 ± 0.009\nANP\n0.072 ± 0.010\n0.123 ± 0.012\n0.041 ± 0.009\nCANP\n0.065 ± 0.010\n0.154 ± 0.014\n0.042 ± 0.008\nBANP\n0.059 ± 0.009\n0.106 ± 0.011\n0.037 ± 0.008\nConvCNP\n0.011 ± 0.003\n0.043 ± 0.006\n0.003 ± 0.001\nTNP-D\n0.059 ± 0.009\n0.044 ± 0.007\n0.023 ± 0.007\nTNP-KR-Fast\n0.062 ± 0.009\n0.052 ± 0.007\n0.027 ± 0.006\nTNP-KR-Full\n0.054 ± 0.009\n0.039 ± 0.005\n0.025 ± 0.006\nTable 5: 2D GP mean and standard error of negative\nlog-likelihoods over 5 runs, where the score for each\nrun consists of the average negative log-likelihood over\n5,000 batches of 16. Best model in bold.\nModel\nRBF\nNP\n1.209 ± 0.002\nCNP\n1.174 ± 0.002\nANP\n0.753 ± 0.012\nCANP\n0.611 ± 0.011\nTNP-D\n0.484 ± 0.003\nTNP-KR-Fast\n0.554 ± 0.002\nTNP-KR-Full\n0.482 ± 0.005\n4.3\nImage Completion\nThe last standard benchmarks consist of image com-\npletion. In Tables 6, 7, and 8 we compare the same\nmodels on the MNIST, CelebA, and CIFAR-10 bench-\nmarks. Each model is trained on 100,000 batches of\nsize 32 and tested on 5,000 batches of size 32. In each\nof the samples, the model is presented with 200 test\npoints where anywhere between 3 and 100 of them are\nprovided as context points.\nTable 6: MNIST mean and standard error of negative\nlog-likelihoods over 5 runs, where the score for each\nrun consists of the average negative log-likelihood over\n5,000 batches of 32. Best model in bold.\nModel\nNLL\nNP\n−0.716 ± 0.007\nCNP\n−0.776 ± 0.006\nANP\n−0.865 ± 0.021\nCANP\n−0.855 ± 0.010\nTNP-D\n−1.189 ± 0.007\nTNP-KR-Fast\n−1.049 ± 0.006\nTNP-KR-Full\n−1.172 ± 0.006\nTable 7: CelebA mean and standard error of negative\nlog-likelihoods over 5 runs, where the score for each\nrun consists of the average negative log-likelihood over\n5,000 batches of 32. Best model in bold.\nModel\nNLL\nNP\n−0.107 ± 0.002\nCNP\n−0.127 ± 0.001\nANP\n−0.983 ± 0.023\nCANP\n−0.554 ± 0.096\nTNPD\n−1.524 ± 0.005\nTNPKR Fast\n−1.006 ± 0.025\nTNPKR Full\n−1.561 ± 0.008\nTable 8: Cifar-10 mean and standard error of negative\nlog-likelihoods over 5 runs, where the score for each\nrun consists of the average negative log-likelihood over\n5,000 batches of 32. Best model in bold.\nModel\nNLL\nNP\n0.054 ± 0.004\nCNP\n0.009 ± 0.001\nANP\n−0.696 ± 0.092\nCANP\n−0.663 ± 0.096\nTNP-D\n−1.416 ± 0.004\nTNP-KR-Fast\n−0.823 ± 0.012\nTNP-KR-Full\n−1.428 ± 0.007\n5\nCONCLUSION\nIn this work, we introduce TNP-KR, an extension to\nTNP-D that incorporates KRBlocks and fast atten-\ntion. We believe this represents an important step in\nscaling transformer-based models for large spatiotem-\nporal applications, which include population genetics,\nepidemiology, and meteorlology, among others. In the\nfuture, we plan to explore alternative kernels in both\nfull and fast attention, as well as attention bias that ex-\nplicitly takes into account pairwise distances. We also\nplan to explore mechanisms for sampling transformer-\nbased models that respect covariance structures in\nsuch a fashion that we could embed TNP-KR and its\nvariants in inference frameworks such as NumPyro and\nStan.\nAcknowledgements\nD.J. acknowledges his Google DeepMind scholarship.\nE.S. acknowledges support in part by the AI2050 pro-\ngram at Schmidt Sciences (Grant [G-22-64476]). S.F.\nacknowledges the EPSRC (EP/V002910/2).\n\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nReferences\nBlei, David M., Alp Kucukelbir, and Jon D. McAuliffe\n(Apr. 3, 2017). “Variational Inference: A Review for\nStatisticians”. In: Journal of the American Statis-\ntical Association 112.518, pp. 859–877. issn: 0162-\n1459, 1537-274X. doi: 10.1080/01621459.2017.\n1285773. arXiv: 1601 . 00670[cs , stat]. url:\nhttp://arxiv.org/abs/1601.00670 (visited on\n07/30/2024).\nChoromanski,\nKrzysztof\net\nal.\n(Nov.\n19,\n2022).\n“Rethinking\nAttention\nwith\nPerformers”.\nIn:\narXiv:2009.14794. doi: 10 . 48550 / arXiv . 2009 .\n14794.\narXiv:\n2009 . 14794[cs , stat].\nurl:\nhttp://arxiv.org/abs/2009.14794 (visited on\n05/01/2024).\nGarnelo, Marta, Dan Rosenbaum, et al. (July 4,\n2018).\n“Conditional\nNeural\nProcesses”.\nIn:\narXiv:1807.01613. doi: 10 . 48550 / arXiv . 1807 .\n01613.\narXiv:\n1807 . 01613[cs , stat].\nurl:\nhttp://arxiv.org/abs/1807.01613 (visited on\n07/03/2024).\nGarnelo, Marta, Jonathan Schwarz, et al. (2018). Neu-\nral Processes. arXiv: 1807 . 01622 [cs.LG]. url:\nhttps://arxiv.org/abs/1807.01622.\nGordon, Jonathan et al. (June 25, 2020). Convolu-\ntional Conditional Neural Processes. arXiv: 1910.\n13556[cs,stat]. url: http://arxiv.org/abs/\n1910.13556 (visited on 04/16/2024).\nHe, Kaiming et al. (June 2016). “Deep Residual Learn-\ning for Image Recognition”. In: pp. 770–778. doi:\n10.1109/CVPR.2016.90.\nHeaton, Matthew J. et al. (Apr. 25, 2018). A Case\nStudy Competition Among Methods for Analyzing\nLarge Spatial Data. doi: 10.48550/arXiv.1710.\n05013. arXiv: 1710 . 05013[stat]. url: http :\n/ / arxiv . org / abs / 1710 . 05013 (visited on\n12/01/2023).\nHensman, James, Nicolo Fusi, and Neil D Lawrence\n(2013). “Gaussian processes for big data”. In: arXiv\npreprint arXiv:1309.6835.\nHuggins, Jonathan et al. (2020). “Validated variational\ninference via practical posterior error bounds”. In:\nInternational Conference on Artificial Intelligence\nand Statistics. PMLR, pp. 1792–1802.\nKim, Hyunjik et al. (July 9, 2019). “Attentive Neural\nProcesses”. In: arXiv:1901.05761. doi: 10.48550/\narXiv.1901.05761. arXiv: 1901.05761[cs,stat].\nurl: http://arxiv.org/abs/1901.05761 (visited\non 04/02/2024).\nKingma, Diederik P and Max Welling (2022). Auto-\nEncoding Variational Bayes. arXiv: 1312 . 6114\n[stat.ML]. url: https://arxiv.org/abs/1312.\n6114.\nLee, Juho et al. (Oct. 27, 2020). Bootstrapping Neu-\nral Processes. doi: 10 . 48550 / arXiv . 2008 .\n02956. arXiv: 2008.02956[cs,stat]. url: http:\n/ / arxiv . org / abs / 2008 . 02956 (visited on\n05/19/2024).\nLindgren, Finn, Johan Lindstr¨om, and H˚avard Rue\n(2010). “An explicit link between gaussian fields and\ngaussian markov random fields; the spde approach”.\nIn.\nMishra, Swapnil et al. (Sept. 13, 2022). πVAE: a\nstochastic process prior for Bayesian deep learn-\ning with MCMC. doi: 10 . 48550 / arXiv . 2002 .\n06873. arXiv: 2002.06873[cs,stat]. url: http:\n/ / arxiv . org / abs / 2002 . 06873 (visited on\n07/30/2024).\nMurphy, Kevin P. (2023). Probabilistic Machine Learn-\ning: Advanced Topics. MIT Press. url: http : / /\nprobml.github.io/book2.\nNguyen, Tung and Aditya Grover (Feb. 7, 2023).\nTransformer Neural Processes: Uncertainty-Aware\nMeta Learning Via Sequence Modeling. doi: 10 .\n48550 / arXiv . 2207 . 04179.\narXiv:\n2207 .\n04179[cs]. url: http://arxiv.org/abs/2207.\n04179 (visited on 04/13/2024).\nRue, H˚avard, Sara Martino, and Nicolas Chopin\n(2009). “Approximate Bayesian inference for latent\nGaussian models by using integrated nested Laplace\napproximations”. In: Journal of the Royal Statisti-\ncal Society Series B: Statistical Methodology 71.2,\npp. 319–392.\nSemenova, Elizaveta, Prakhar Verma, et al. (2023).\n“PriorCVAE: scalable MCMC parameter inference\nwith Bayesian deep generative modelling”. In: arXiv\npreprint arXiv:2304.04307.\nSemenova, Elizaveta, Yidan Xu, et al. (June 2022).\n“PriorVAE: Encoding spatial priors with VAEs for\nsmall-area estimation”. In: Journal of The Royal\nSociety Interface 19.191, p. 20220094. issn: 1742-\n5662. doi: 10.1098/rsif.2022.0094. arXiv: 2110.\n10422[cs,stat]. url: http://arxiv.org/abs/\n2110.10422 (visited on 07/30/2024).\nSolin, Arno and Simo S¨arkk¨a (2020). “Hilbert space\nmethods for reduced-rank Gaussian process regres-\nsion”. In: Statistics and Computing 30.2, pp. 419–\n446.\nWilson,\nAndrew\nGordon,\nChristoph\nDann,\nand\nHannes Nickisch (2015). “Thoughts on massively\nscalable Gaussian processes”. In: arXiv preprint\narXiv:1511.01870.\nXiong, Ruibin et al. (2020). On Layer Normalization\nin the Transformer Architecture. arXiv: 2002.04745\n[cs.LG]. url: https://arxiv.org/abs/2002.\n04745.\nYao, Yuling et al. (2018). “Yes, but did it work?:\nEvaluating variational inference”. In: International\n\nTransformer Neural Process - Kernel Regression\nConference on Machine Learning. PMLR, pp. 5581–\n5590.\nZaheer, Manzil et al. (2018). “Adaptive Methods\nfor Nonconvex Optimization”. In: Advances in\nNeural Information Processing Systems. Ed. by\nS.\nBengio\net\nal.\nVol.\n31.\nCurran\nAssociates,\nInc. url: https : / / proceedings . neurips .\ncc / paper _ files / paper / 2018 / file /\n90365351ccc7437a1309dc64e4db32a3-Paper.pdf.\n\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nAPPENDIX\nALGORITHMS\nAlgorithm 1 Transformer Encoder Block forward pass.\n1: Input: x0, mask, pdropout\n2: x1 ←MultiheadAttention(x, x, x, mask)\n3: x2 ←x0 + Dropout(x1, pdropout)\n4: x3 ←LayerNorm(x2)\n5: x4 ←FeedForward(x3)\n6: x5 ←x3 + Dropout(x4, pdropout)\n7: return x5\nAlgorithm 2 KRBlock forward pass.\nInput: qs0, ks0, mask, pdropout\nqs1 ←LayerNorm1(qs0)\nks1 ←LayerNorm1(ks0)\nqs2 ←MultiheadAttention(qs1, ks1, ks1, mask)\nks2 ←MultiheadAttention(ks1, ks1, ks1, mask)\nqs3 ←qs0 + Dropout(qs2, pdropout)\nks3 ←ks0 + Dropout(ks2, pdropout)\nqs4 ←LayerNorm2(qs3)\nks4 ←LayerNorm2(ks3)\nqs5 ←FeedForward(qs4)\nks5 ←FeedForward(ks4)\nqs6 ←qs3 + Dropout(qs5, pdropout)\nks6 ←ks3 + Dropout(ks5, pdropout)\nreturn qs6, ks6\n\nTransformer Neural Process - Kernel Regression\nADDITIONAL FIGURES\n1\n10\n100\n1000\n10000\n100000\n1000000\nNum. Test\n0.00\n0.02\n0.04\n0.06\n0.08\nSeconds per Sample\nTNP-KR Fast\nTNP-KR Full\nTNP-D\nFigure 3: Scaling test points in TNP models, keeping number of context points fixed at 100.\n1\n10\n100\n1000\n10000\n100000\n1000000\nNum. Context\n0.00\n0.02\n0.04\n0.06\n0.08\nSeconds per Sample\nTNP-KR Fast\nTNP-KR Full\nTNP-D\nFigure 4: Scaling context points in TNP, keeping number of test points fixed at 100.\n\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nFigure 5: 1D GP Matern 3/2 samples. The black line is the true function, the black dots are noisy observations,\nthe blue line is the model’s mean prediction, and the shaded blue is the uncertainty. Models perform similarly,\nbut ConvCNP and the TNP variants have the tightest uncertainty bounds.\n\nTransformer Neural Process - Kernel Regression\nFigure 6: 1D GP periodic samples. The black line is the true function, the black dots are noisy observations, the\nblue line is the model’s mean prediction, and the shaded blue is the uncertainty. Some models struggle to model\nlow period periodic functions, particularly NPs.\nConvCNP and TNP variants have the tightest uncertainty\nbounds.\n\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nFigure 7: 1D GP RBF samples. The black line is the true function, the black dots are noisy observations, the\nblue line is the model’s mean prediction, and the shaded blue is the uncertainty. Most models do well on this\ntask, but ConvCNP and the TNP variants have marginally tighter uncertainty bounds.\n\nTransformer Neural Process - Kernel Regression\nFigure 8: 2D GP RBF sample. In the Task panel, masked blue pixels are unoberved locations. In the Uncertainty\npanel, warmer colors signify greater uncertainty. NP and CNP struggle to model in two dimensions. TNP variants\nperform better than ANP variants, largely due to the increased accuracy of transformer blocks.\n\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nFigure 9: MNIST sample. In the Task panel, masked blue pixels are unoberved locations. In the Uncertainty\npanel, warmer colors signify greater uncertainty. Predictions are relatively consistent among models, but the\nTNP variants have better uncertainty bounds, particularly around observed context points.\n\nTransformer Neural Process - Kernel Regression\nFigure 10: CelebA sample. In the Task panel, masked blue pixels are unoberved locations. In the Uncertainty\npanel, warmer colors signify greater uncertainty. NP and CNP tend to oversmooth, while TNP and ANP variants\nare able to identify more details, particularly at observed context points. There are purple dots for TNP and\nANP variants, indicating that the model is highly confident in the uncertainty bounds around observed points,\nbut NP and CNP tend to oversmooth at context points.\n\nDaniel Jenson1, Jhonathan Navott1, Mengyan Zhang1, Makkunda Sharma1\nFigure 11: Cifar 10 sample. In the Task panel, masked blue pixels are unoberved locations. In the Uncertainty\npanel, warmer colors signify greater uncertainty. NP and CNP models oversmooth, while TNP and ANP variants\nappear grainy due to attention focusing on pixels in different regions of the image.",
    "pdf_filename": "Transformer_Neural_Processes_--_Kernel_Regression.pdf"
}