{
    "title": "PaperWeaver - Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers",
    "context": "With the rapid growth of scholarly archives, researchers subscribe to “paper alert” systems that periodically provide them with recom- mendations of recently published papers that are similar to previ- ously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present tions, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users’ research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better under- stand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers. CCS CONCEPTS • Human-centered computing →Interactive systems and tools; Empirical studies in HCI; Natural language interfaces. ∗Work completed during a researcher internship at Semantic Scholar Research, Allen Institute for AI. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). CHI ’24, May 11–16, 2024, Honolulu, HI, USA © 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0330-0/24/05. https://doi.org/10.1145/3613904.3642196 KEYWORDS Scientific Paper, Contextualized Descriptions, Large Language Mod- els ACM Reference Format: Yoonjoo Lee, Hyeonsu B. Kang, Matt Latzke, Juho Kim, Jonathan Bragg, Joseph Chee Chang, and Pao Siangliulue. 2024. PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User- collected Papers. In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI ’24), May 11–16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA, 19 pages. https://doi.org/10.1145/3613904.3642196 1 Managing an ever-increasing accumulation of knowledge has long been a challenge for scholars [8]. With the recent proliferation of published materials, researchers face an even bigger challenge of keeping up with the literature [25, 42, 55]. Fundamentally, scholars need to both discover new and relevant papers and contextual- ize them to their own research interests. One popular practice re- cently is to leverage recommender systems that can help researchers retrieve potentially relevant, such as arXivist1, arXiv Sanity2 or Google Scholar3. Typically, these systems allow users to create “pa- per alerts” by providing a short description of a specific research topic and a set of seed papers as examples of papers of interest. For example, Semantic Scholar4 allows users to save sets of collected papers under named topical folders. Users would then receive pe- riodic paper alerts that contain a list of recently published papers similar to the collected papers. These alerts help researchers to quickly narrow down from all recent publications to a small set of potentially relevant papers, allowing them to more easily stay up to date on research topics that are of interest to them. However, when receiving s set of potentially relevant papers in an alert, researchers still need to more deeply inspect each paper to 1https://arxivist.com/ 2https://arxiv-sanity-lite.com/ 3https://scholar.google.com/ 4https://www.semanticscholar.org/ arXiv:2403.02939v2  [cs.DL]  9 May 2024",
    "body": "PaperWeaver: Enriching Topical Paper Alerts by Contextualizing\nRecommended Papers with User-collected Papers\nYoonjoo Lee∗\nSchool of Computing, KAIST\nDaejeon, Republic of Korea\nyoonjoo.lee@kaist.ac.kr\nHyeonsu B. Kang\nHuman-Computer Interaction\nInstitute, Carnegie Mellon University\nPittsburgh, PA, USA\nhyeonsuk@andrew.cmu.edu\nMatt Latzke\nAllen Institute for AI\nSeattle, WA, USA\nmattl@allenai.org\nJuho Kim\nSchool of Computing, KAIST\nDaejeon, Republic of Korea\njuhokim@kaist.ac.kr\nJonathan Bragg\nAllen Institute for AI\nSeattle, WA, USA\njbragg@allenai.org\nJoseph Chee Chang\nAllen Institute for AI\nSeattle, WA, USA\njosephc@allenai.org\nPao Siangliulue\nAllen Institute for AI\nSeattle, WA, USA\npaos@allenai.org\nABSTRACT\nWith the rapid growth of scholarly archives, researchers subscribe\nto “paper alert” systems that periodically provide them with recom-\nmendations of recently published papers that are similar to previ-\nously collected papers. However, researchers sometimes struggle to\nmake sense of nuanced connections between recommended papers\nand their own research context, as existing systems only present\npaper titles and abstracts. To help researchers spot these connec-\ntions, we present PaperWeaver, an enriched paper alerts system\nthat provides contextualized text descriptions of recommended\npapers based on user-collected papers. PaperWeaver employs a\ncomputational method based on Large Language Models (LLMs) to\ninfer users’ research interests from their collected papers, extract\ncontext-specific aspects of papers, and compare recommended and\ncollected papers on these aspects. Our user study (N=15) showed\nthat participants using PaperWeaver were able to better under-\nstand the relevance of recommended papers and triage them more\nconfidently when compared to a baseline that presented the related\nwork sections from recommended papers.\nCCS CONCEPTS\n• Human-centered computing →Interactive systems and\ntools; Empirical studies in HCI; Natural language interfaces.\n∗Work completed during a researcher internship at Semantic Scholar Research, Allen\nInstitute for AI.\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\n© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0330-0/24/05.\nhttps://doi.org/10.1145/3613904.3642196\nKEYWORDS\nScientific Paper, Contextualized Descriptions, Large Language Mod-\nels\nACM Reference Format:\nYoonjoo Lee, Hyeonsu B. Kang, Matt Latzke, Juho Kim, Jonathan Bragg,\nJoseph Chee Chang, and Pao Siangliulue. 2024. PaperWeaver: Enriching\nTopical Paper Alerts by Contextualizing Recommended Papers with User-\ncollected Papers. In Proceedings of the CHI Conference on Human Factors in\nComputing Systems (CHI ’24), May 11–16, 2024, Honolulu, HI, USA. ACM,\nNew York, NY, USA, 19 pages. https://doi.org/10.1145/3613904.3642196\n1\nINTRODUCTION\nManaging an ever-increasing accumulation of knowledge has long\nbeen a challenge for scholars [8]. With the recent proliferation of\npublished materials, researchers face an even bigger challenge of\nkeeping up with the literature [25, 42, 55]. Fundamentally, scholars\nneed to both discover new and relevant papers and contextual-\nize them to their own research interests. One popular practice re-\ncently is to leverage recommender systems that can help researchers\nretrieve potentially relevant, such as arXivist1, arXiv Sanity2 or\nGoogle Scholar3. Typically, these systems allow users to create “pa-\nper alerts” by providing a short description of a specific research\ntopic and a set of seed papers as examples of papers of interest. For\nexample, Semantic Scholar4 allows users to save sets of collected\npapers under named topical folders. Users would then receive pe-\nriodic paper alerts that contain a list of recently published papers\nsimilar to the collected papers. These alerts help researchers to\nquickly narrow down from all recent publications to a small set of\npotentially relevant papers, allowing them to more easily stay up\nto date on research topics that are of interest to them.\nHowever, when receiving s set of potentially relevant papers in\nan alert, researchers still need to more deeply inspect each paper to\n1https://arxivist.com/\n2https://arxiv-sanity-lite.com/\n3https://scholar.google.com/\n4https://www.semanticscholar.org/\narXiv:2403.02939v2  [cs.DL]  9 May 2024\n\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nLee et al.\nUncontextualized description\nAbstract summary This work proposes SPECTER, a new \nmethod to generate document-level embedding of scientific \npapers based on pretraining a Transformer language model on \na powerful signal of document- level relatedness: the citation \ngraph, and shows that Specter outperforms a variety of \ncompetitive baselines on the benchmark.\nResearch Support Systems\nPaper 1\nPaper 2\nPaper 3\nSciBERT: A \nPretrained \nLanguage Model \nfor Scientific Text\nPaper 5\nPaper 6\nPaper 7\nPaper 8\nPaper 9\nContextualized description: \nPaper-paper description\nBoth SPECTER and SciBERT utilize Transformer-based \nlanguage models for their respective tasks. [Summary of \nSPECTER][Summary of SciBERT] The key difference lies in their \napplications; SciBERT is used as a building block in SPECTER \nfor understanding scientific text at a document-level, enhancing \nits performance by leveraging citation information, whereas \nSciBERT emphasizes unsupervised pretraining for performance \nenhancement on multiple scientific NLP tasks.\nDocument level representation power is limited \nin transformers like BERT due to lack of \nleveraging document-relatedness\nContextualized description:\nAspect-based paper summaries\nProblem\nMethod\nFindings\nSPECTER, the proposed method, uses citation-\ninformed Transformers to generate scientific \npaper embeddings\nSPECTER outperforms other models in tasks like \ncitation prediction, document classification, and \nrecommendation.\nCites a\nPaper\nSPECTER: Document-\nlevel Representation \nLearning using Citation-\ninformed Transformers\nRecommended paper\nFigure 1: In contrast to existing topical paper alert systems that show descriptions with no personalized context for the\nrecommended papers, PaperWeaver provides contextualized descriptions that surface the relevance of recommended papers\nand anchor them to familiar user-collected folders to help users better make sense of recommended papers.\nunderstand its relevance. This typically involves making meaning-\nful connections between newly encountered information and their\nexisting knowledge of the literature—a process that can incur high\ncognitive costs for researchers. To illustrate this process with an\nexample, a researcher working on Research Support Systems may\nencounter a relevant new paper titled “SPECTER: Document-level\nRepresentation Learning using Citation-informed Transformers”\n[15]. However, it can be challenging for the researcher to realize its\nrelevance since the paper title only contains information about how\nit uses Transformers to generate document embeddings. Only when\nthe researcher carefully examines the full abstract can they learn\nthat the paper further described how the pre-trained model can be\n“easily applied to develop downstream applications,” and that the\npaper implemented a “research paper recommender system” for its\nevaluation. Further, if the researcher spent additional efforts to ex-\namine the content of the recommended paper, they might discover\nthat the related work section described how the proposed method\nwas built on SciBERT[6]—a familiar paper they had recently saved\nand read—but extends its capability from embedding sentences to\nlong documents. This manual process is effortful but important be-\ncause failure to identify meaningful connections between new and\nexisting knowledge will lead to overlooking new papers relevant\nto a researcher’s interests.\nFurther, existing paper recommender systems typically only\nshow a list of titles and abstract summaries for the recommended\npapers with little information on how they were relevant to the\ntopic of the folder or the set of seed papers that the recommenda-\ntions were based on. Findings from our formative study (Section 3)\nsuggest that the title of a paper often lacks enough details that help\nresearchers understand the paper’s relevance yet the abstract is\noften too long to skim through for paper alerts. Prior work has\nexplored an approach that generates one-sentence TL;DR (too long;\ndidn’t read) summaries [10] that are easier to consume but they\nlack contextualization to the folder topic and the collected papers\nin them. Specifically, since the summaries are not tailored to a\nuser’s folder context, parts of the abstracts that showed how the\nrecommended papers were relevant to the folder can sometimes be\nomitted. While there has been research exploring ways to better\n\nPaperWeaver: Enriching Paper Alerts by Contextualizing Descriptions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\ncontextualize paper recommendations by surfacing personalized\nsocial signals (e.g., based on a user’s prior interaction or publication\nhistory [28]), they do not describe how the content of the papers is\nrelevant to the users. As a result, users are left to decide whether to\ncarefully examine the recommended papers to find potential con-\nnections, with no guarantee that the effort will pay off. As shown\nin the above example, this sensemaking process can be effortful\nfor users, and failure to effectively triage paper recommendations\ncould result in overlooking important paper recommendations and\nreducing the effectiveness of such systems.\nIn this work, we investigate what types of information in paper\nalerts help scholars deeply understand the relevance of recom-\nmended papers to their topical folders. In a formative study with\nseven researchers, we investigated challenges in identifying rele-\nvance from existing paper alerts and desired alternative descriptions\nof the recommended papers. Our formative findings suggested that\nscholars desire paper alerts that were contextualized to their folders\ncompared to only showing titles, abstracts, and uncontextualized\nsummaries. Participants found that descriptions about the recom-\nmended papers that revealed connections of a recommended paper\nto the user’s folder context helped them understand the recom-\nmended paper more effectively since it spotlights where to focus on\namong many aspects of the paper. We also found that presenting\nthe comparison and contrasting descriptions of multiple papers\nallowed participants to understand how the recommended papers\nbuild on prior work they had already collected in their folders.\nAnchoring unfamiliar papers with collected familiar papers also\nreduced the cognitive load of processing new information.\nBased on our formative findings, we propose PaperWeaver, a\nnew paper alert system that can enrich existing paper recommender\nsystems by generating descriptions of how each recommended pa-\nper relates to a user’s interests and their collection of papers. Our\nsystem is built on an existing document recommender system. Pa-\nperWeaver leverages recent advancements in Large Language\nModels (LLMs) for text generation to support users in several ways.\nFirst, PaperWeaver generates a compact topic description of a\nset of user-collected papers. This compact topic description pro-\nvides users with a quick summary of the collected papers. It is\nuser-editable and useful for generating future descriptions for each\nrecommended paper contextualized to a user’s interest. Second, to\nhelp users understand how a recommended paper is relevant to their\nresearch context, PaperWeaver generates two types of complemen-\ntary contextual descriptions: contextualized aspect-based summaries\nand paper-paper descriptions (Fig. 1). Contextualized aspect-based\nsummaries leverage the generated topic descriptions to extract state-\nments on the problems, methods, and findings of papers that are\nhighly relevant to the topic of interest. Paper-paper descriptions\nsummarize how a recommended paper relates to collected papers,\nwhich are more familiar to the users. If the recommended paper\ncites collected papers, PaperWeaver summarizes these citation\ndescriptions and, if not, it synthesizes relationships by comparing\nand contrasting aspects from the papers. Motivated by how multi-\nple alternative descriptions can improve understanding of complex\nscientific topics [1, 2], we design an interactive paper alert interface\nwhere users can explore multiple descriptions for recommended\npapers (Fig. 2).\nTo evaluate PaperWeaver, we conducted a within-subjects study\n(𝑁= 15) with researchers who were interested in receiving paper\nrecommendation alerts. To ensure participants were motivated in\nthe study, the paper alerts used in the study were generated based\non their actual set of collected papers. We compared PaperWeaver\nwith a strong baseline similar to existing paper alert systems but\nadditionally enriched with uncontextualized summaries and ex-\ntracted related work sections from the recommended papers. Our\nuser study results showed that participants were able to better un-\nderstand the nuanced relevance of the recommended papers and\nwere able to triage them more confidently with PaperWeaver.\nFurther, participants were able to capture richer relationships be-\ntween recommended and collected papers in their notes when using\nPaperWeaver compared to the baseline.\nThe contributions of this work are as follows:\n• Qualitative findings from a formative study employing de-\nsign probes with researchers that identified user challenges\naround making sense of recommended papers and the need\nfor contextualized summaries.\n• PaperWeaver, a tool that provides additional contextualized\ndescriptions for a set of recommended papers, tailored to\nthe user-collected papers. PaperWeaver uses an LLM-based\npipeline that synthesizes content from both recommended\nand user-collected papers.\n• Findings from a user study (𝑁= 15) that demonstrated how\nusing PaperWeaver facilitates sense-making of paper rec-\nommendations and aids in uncovering useful relationships\nbetween recommended and collected papers.\n2\nRELATED WORK\nDue to the increasing difficulties for scholars to keep up with the\nrapidly growing scholarly publications, significant research has\nbeen devoted to supporting scholars in better understanding the\nliterature. Most prior work in scholar support tools has focused\non two stages of this process: broadening scholars’ reach in dis-\ncovering relevant papers (§2.1) and helping scholars to deeply\nunderstand the literature as they read (§2.2). PaperWeaver focuses\non a less explored area that bridges between the two stages: pro-\nviding lightweight and contextualized sensemaking support when\nscholars are presented with a set of paper recommendation alerts.\nIn this section, we briefly describe the two major threads of re-\nsearch to situate this current paper within the literature. Finally,\nwe present prior work on contextualization in the settings where\nusers acquire new knowledge and how they influence and differ\nfrom our approach (§2.3).\n2.1\nFacilitating Broad Scholarly Exploration\nand Paper Discovery\nSignificant research has focused on helping scholars explore and dis-\ncover relevant papers in the literature. For example, SPECTER [15,\n51] leveraged citations between documents to train dense vector\nrepresentations to encode the content-similarity of research papers.\nThe vectors can then be used to power different downstream appli-\ncations such as search or recommender systems. Prior research has\nalso leveraged alternative similarity signals such as co-citations [46],\ndomains [31], and authorships [33, 47] to broaden the range of\n\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nLee et al.\nrecommendations. Recent work has begun to explore facilitating\nresearch paper retrieval with a deeper understanding of its semantic\ncontent. For example, [11, 27, 32] focused on extracting a problem-\nmethod schema from paper, which is rooted in cognitive science\ntheories of analogical processing and creativity [18, 19]. Diversify-\ning retrieval based on one aspect (e.g., mechanisms used to tackle\na problem) while constraining the similarity on another (e.g., the\nproblems tackled in a paper), can effectively broaden recommenda-\ntions across different domains and increase creativity in scientific\nideation [32].\nAt the same time, research has suggested that users can still\nbe overwhelmed by the number of paper recommendations they\nreceive [28] and pointed to the importance of helping user prioritize\nand understand paper recommendations. A research thread more\nclosely related to our work has focused on enriching recommenda-\ntions with lightweight relevance signals which can help scholars\nefficiently understand connections to other work and effectively\ntriage them. For example, prior work has examined presenting spa-\ntial overviews of paper collections based on semantic similarity\n(cf. Apolo [13], LitSense [52], FeedLens [34]) and surfacing person-\nalized social signals based on coauthorship and citations [28] or\npublication venues and institutions [34].\nHowever, most previous systems either rely on external struc-\ntured signals that are easy to understand but are divorced from the\ncontent of the papers (e.g., “you have cited the authors before” [28])\nor latent semantic signals that are based on the content of the papers\nbut difficult to understand (e.g., clusters of papers based on embed-\nding distances5). In contrast, PaperWeaver draws from literature\non schematic processing [20] and leverages recent advancements\nin LLMs to extract problem-method-finding aspects across papers\nand generate easy-to-understand compare and contrasting state-\nments that anchor the recommended papers to the user’s familiar\npapers and context. Furthermore, while in evaluation we used a\nspecific existing implementation of paper recommender system,\nour proposed post-hoc enrichment technique can potentially be\napplied to other paper discovery techniques outlined above.\n2.2\nReusing Related Work Sections for Deeper\nScholarly Sensemaking\nThere is a recent thread of research focusing on how to reuse content\nfrom the related work sections of published papers to help schol-\nars deeply understand the literature. For example, Relatedly [43]\nallowed its users to search across related work sections extracted\nfrom many published papers and provided support for reading\nscattered paragraphs. It highlighted how rich synthesis contained\nin related work sections can help scholars better understand the\nlandscape of a research field while gaining a deep understanding\nof how different prior work compare and contrast with one an-\nother. More closely related to this current work, CiteSee [12] and\nCiteRead [48] are two paper reading tools that extracted citing sen-\ntences, or citances as coined by [41], from other papers to provide\nin-situ sensemaking support when reading a new paper. Partic-\nularly, CiteRead presented incoming citances as margin notes in\nrelevant regions of the paper a user was reading to help them bet-\nter contextualize the current paper with relevant follow-on work.\n5https://www.connectedpapers.com/\nAnother line of research focused on helping users clip citances\nand organize them into notable threads of research while reading\npapers to build up a better understanding of a field. For example,\nThreddy [30] supported dynamic clipping and organization while\nreading to help preserve users’ context of reading when switching\nbetween different papers, and Synergi [29] extended this idea to\nautomatically structure a thread-based hierarchy of relevant prior\nwork, contextualized to user-selected citances in a paper.\nIn contrast to prior work that focused on reusing related work\nsections to support users when they are deeply engaged in reading\nand reviewing the literature, this paper explores how to support\nusers by enriching paper alerts to better understand and triage\nrecommended papers. The opportunity we exploit here is that rec-\nommended papers in a paper alert are likely to cite and describe how\nthey compare and contrast with the user-collected papers in their\nrelated work sections. Specifically, related work sections typically\nstart with contextualized summaries of relevant prior work (which\ncould contain one or more seed papers) and end with a contrasting\nstatement to differentiate their work. As an example to illustrate\nthis opportunity, in this current section, we first summarized how\nCiteRead (i.e., a user-collected paper) also reused contents of exist-\ning related work sections and ended with a contrasting statement\nthat it focused on supporting deep reading as opposed to this cur-\nrent paper (i.e., a recommended paper) which focuses on enriching\npaper recommendation alerts. PaperWeaver leverages LLMs to ex-\ntract this relationship from related work sections of recommended\npapers to generate a short contrasting statement so that users can\neasily anchor unfamiliar papers to familiar contexts, allowing them\nto see meaningful connections between papers in the literature.\n2.3\nProviding contextualized explanation\nKnowledge acquisition requires assimilating new information into\nexisting knowledge [4]. Existing work across different domains has\nexplored mechanisms and effects of explanations that are contextu-\nalized to a user’s existing knowledge. Prior research has considered\nvarious definitions of context. For example, Tutorons [23] and Schol-\narPhi [24] provide explanations of code and scientific notation, re-\nspectively, that are relevant to the user’s reading context. To explain\nscientific concepts to children, DAPIE [35] adapts how it explains\nconcepts (e.g., through simplification or examples) according to a\nchild’s responses in previous dialogue turns. To provide adaptive\nexplanations even when contextual information about the user is\nunavailable, AXIS [54] leverages other users’ ratings on provided\nexplanations to identify effective explanations that can be provided\nto future users. Recent work in NLP [50] has investigated how to\nguide LLMs to personalize their generated text based on “user pro-\nfiles” (e.g., previous data or content that a user has written) which\ncan help to explain, paraphrase, or summarize text into language\nthat is familiar to the user. Building on this thread of research, our\nwork investigates an approach to synthesizing papers collected\nby a scholar into folder descriptions. These descriptions are then\nleveraged as representations of both the user’s interest and prior un-\nderstanding when generating descriptions to contextually explain\nnew paper recommendations. Regarding scientific literature un-\nderstanding, ACCoRD[39] defines unfamiliar scientific concepts in\nterms of different reference concepts by taking advantage of diverse\n\nPaperWeaver: Enriching Paper Alerts by Contextualizing Descriptions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nways a concept is mentioned across the scientific literature. Notably,\nthis work found that users prefer multiple descriptions to a single\nbest description. At a high level, ACCoRD and PaperWeaver both\naim to make unfamiliar ideas more accessible by creating bridges\nupon the user’s prior knowledge. However, the approach explored\nin ACCoRD operates at the unit of concepts, while PaperWeaver\naims to help scholars understand the relevance between different\npapers that may employ many interconnected concepts and other\naspects (e.g., problem, method, findings) of academic papers.\n3\nFORMATIVE STUDY\nWe conducted a formative study with two goals. Firstly, to learn\nabout researchers’ current challenges when reading existing paper\nalerts that consist of a list of titles and abstract. Secondly, to gain\na deeper understanding of the types of information that would\nbe more useful in a paper alert setting to inform the design of\nPaperWeaver.\n3.1\nProcedure and Apparatus\nWe conducted a targeted recruitment of seven participants who\nare graduate students expressed interest or have worked on one of\ntwo predefined topics: “Reading support tools for scientific articles”\nor “Scholarly document processing.” Four participants identified\ntheir discipline as human-computer interaction (HCI) and three\nas natural language processing (NLP).6 To seed the two folders\nwith papers that the participants are likely to be familiar with,\nwe chose relevant papers published by their research groups. To\ngenerate paper recommendations for each of the two topics, we\nused a publicly available paper recommender API with the seed\npapers as inputs.7\nWe designed five different types of contextualized text descrip-\ntions as design probes to gather feedback. The design probes cov-\nered different ways of describing the recommended papers in the\ncontext of a paper alert. The descriptions were generated either\nmanually or using a previous scientific abstract summarization\nmethod called TL;DR [10] and an LLM with prompts that focused\non including different types of information, such as relevance to\nthe folder via the folder name or relevance to the seed papers (List\nof design probes in Supplementary Materials).\nTo simulate realistic paper alert usage, we asked participants\nto consider the pre-defined folders as their own. In the first 15\nminutes of the interview, participants reviewed 10 seed papers on\nthe topic they had chosen to ensure that they are familiar with the\npapers we had collected for them. We then probed their experience\nwith current paper alert systems by showing them recommended\npapers in the standard presentation as a list of titles and abstracts,\nand asked them about their goals and pain points. Afterward, we\nconducted a think-aloud interview by walking them through a set of\ndesign mock-ups with the design probes for 40 minutes. We probed\nthe costs and benefits based on their reactions to different types of\ndescriptions. Participants were also asked to participate in co-design\nby revising the descriptions as they saw fit. The interviews were\nscreen recorded, and the first author conducted a thematic analysis\n6Two participants joined in person and the rest joined remotely through Google Meet.\nThis study was approved by our internal review board. Each participant was paid 30\nUSD for one hour of their time.\n7https://api.semanticscholar.org/api-docs/recommendations\nto capture qualitative insights [9, 16]. The rest of this section lists\nthe common themes from the interviews and describes the design\nprobes when relevant.\n3.2\nFormative Study Findings\n3.2.1\nChallenges in Making Sense of Existing Paper Alerts. When\nwe showed a design probe similar to existing paper alert systems\nwith a list of titles and abstracts for the recommended papers, all\nparticipants mentioned that the biggest challenge is to figure out\nhow all the recommended papers are relevant to the topic of their\nfolders. It is hard to quickly decide whether they should save and\nread the recommended papers from just their titles and abstracts. In\norder to effectively identify relevance, participants pointed to how\nthey need to carefully read the full abstracts or even the papers\nthemselves. However, doing so would take more time and effort\nthan they typically spend on consuming paper recommendation\nalerts. The cost of carefully reading a list of full abstracts is so\nhigh that many participants resort to using a less effective strat-\negy where they spot mentions of certain keywords in the titles or\nwhether it was published by a familiar author (P1, 3, 4, and 6). At\nthe same time, they acknowledge that this strategy leads to over-\nlooking relevant papers. Participants pointed to two reasons: (1)\nThey might not know all relevant authors in the space which sug-\ngests previous social-signal-based approaches could be insufficient\n[28]. (2) The keyword spotting only allows them to find high-level\ntopical relevance (e.g., “scientific papers”) but does not allow them\nto find deeper connections hidden in the abstracts (e.g., “semantic\nembeddings that can support citation predictions” [15]).\n3.2.2\nSurfacing Relevant Aspects when Summarizing Recommended\nPapers. We explicitly asked participants to compare different types\nof description in three mock-ups, each with different descriptions\nto the same set of paper recommendations: full abstracts, “TL;DR”\nabstract summaries generated using [10], and abstract summaries\nthat focused on including information relevant to the folder name\ngenerated using an LLM. When comparing with the full abstracts,\nparticipants reacted positively with the two shorter abstract sum-\nmaries over reading the full abstracts that were longer and less\nconcise. Additionally, they preferred the LLM-generated contex-\ntualized summaries over the uncontextualized TL;DR summaries.\nSpecifically, participants felt that they could more effectively un-\nderstand why the recommended papers were relevant while the\nTL;DR summaries often omitted parts of the abstract that were rel-\nevant to the folder. For example, P2 and P5 liked the contextualized\nsentence: “You may be interested in this paper because it addresses\nthe issue of trustworthiness and factuality in question answering sys-\ntems, which can be relevant to processing scientific documents[i.e.,\nthe folder name],” which highlights parts of the abstract most rele-\nvant to the folder. Additionally, we observed that participants less\nfamiliar with the topic reacted particularly strongly with this type\nof explanation while more experienced participants felt the descrip-\ntions were accurate but sometimes trivial because they had more\nbackground knowledge in the domain.\n\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nLee et al.\n3.2.3\nComparing and Contrasting Recommended and User-collected\nPapers. Participants emphasized that the main goal when making\nsense of paper recommendations is to situate recently published pa-\npers within their own research context. With this goal, participants\nmentioned that the design probe with descriptions that focused on\nshowing connections between recommended and collected papers\nhelped them recognize recommended papers that directly build on\na paper they had collected. Additionally, anchoring recommended\npapers to a familiar collected paper helped them more effectively\nunderstand how they were situated within the literature. For exam-\nple, P3 found a description that connected two papers by a similar\nresearch problem to be useful, and said “By aligning the [similar]\nmotivations of Foundwright [i.e., [44]; a recommended paper] with\nthe motivation of Citesee[i.e., [12]; a collected paper], I don’t need\nto find and read two [separate] sentences that explain the problem\nin the [two] abstracts.”\n3.2.4\nBringing New Perspectives to Previously Collected Papers. One\ninteresting observation was that participants found value not only\nin seeing information about the recommended papers, but their\nconnections to papers in the folders also provided new insights\nabout them (P1, P3, P6). For example, when seeing a design mock\nthat showed the citation context (i.e., citing sentences) in a recently\npublished recommended paper that cited one of the papers in the\nfolder, participants felt that they had “rediscovered” the collected\npapers and how the descriptions “makes me feel like the collected\npapers are still relevant.” and that they “[re]introduced papers that\nI already saved but had not read deeply” providing them with “new\nopportunities to better understand them and learn new perspectives\nabout them” (P1).\n3.3\nDesign Goals\nBased on insights from the formative interviews, we formalize the\nfollowing Design Goals for our system:\n[DG1] Describe details about the recommended papers in a way that\nhelps users understand how they are relevant to the topic\nof their research context (e.g., folders) to avoid overlooking\nrelevant recommended papers.\n[DG2] Help users make connections between the recommended\nand collected papers by comparing and contrasting them.\n[DG3] Reveal new aspects of previously collected papers to keep\ntheir understanding of the papers up-to-date and remind\nusers of unread collected papers that have become more\nrelevant with new surfaced connections.\n4\nPAPERWEAVER\nBased on our design goals, we present PaperWeaver (Fig. 2), a pa-\nper alert system that contextualizes recommended papers’ descrip-\ntions based on a user’s topical folder information. PaperWeaver\nenables users to explore these descriptions while reading paper\nalerts. With lessons learned from the formative study, our com-\nputational method leverages a combination of an LLM-generated\ntext and extracted related work sections from the recommended\npapers. PaperWeaver extracts folder-specific aspects from a rec-\nommended paper to surface how it is relevant to the user’s context\n(DG1). The system describes how the recommended paper is simi-\nlar to and different from a topical folder paper by identifying the\nrelationships between papers (DG2). To remind users about papers\nthey have previously collected in the folder, PaperWeaver includes\ninformation about both the recommended and the collected pa-\npers in the descriptions (DG3). Given a paper recommendation\nand a set of collected papers in a named topical folder, the system\nprovides “paper-paper descriptions” that compare and contrast the\nrecommended paper anchored to a relevant collected paper. To gen-\nerate this type of descriptions, the system uses a pair of an abstract\nand a citance as input when available. For recommended papers\nthat do not cite any of the collected papers, PaperWeaver uses an\nLLM to identify latent relationships from their abstracts. Finally,\nPaperWeaver also generates “contextualized aspect-based paper\ndescriptions” that summarizes a recommended paper’s abstract in\na way that reflects the paper’s relevance to the topic of the given\nfolder.\n4.1\nExample User Scenario\nImagine a computer science researcher who started working on\nthe topic of Reading-supporting interfaces a few weeks ago. She\nhas collected a list of papers related to this topic in a folder. Her\nfamiliarity with each paper in the folder varies. She has read some\nof the papers during her prior project. For some other papers, she\nonly saved them because she saw relevant keywords in the title\nof the paper but has not had the time to read them yet. Because\nthis is her active project, she is looking out for new papers relevant\nto the topic. She signed up for a paper alert service that regularly\nprovides her with a list of papers based on papers that she has\ncollected. However, she felt that the information provided was\nlacking. She often had to wade into a paper’s abstract and its full\ntext to see whether the paper was relevant to her research interests.\nThis activity requires more time and attention than she usually has\nwhen reading paper alerts.\nLooking for a more effective way to process paper alerts, she\ngives PaperWeaver a try. She starts using the system by creating\na folder titled Reading-supporting interfaces and adding the list of\npapers she has collected to the folder. PaperWeaver automatically\nexpanded on the folder title to provide an overview summary of the\nsource papers she collected. Although she is mostly satisfied with\nthe automated summary, she removes the keyword medical litera-\nture in the description and adds another keyword Large Language\nModels to better reflect her research interests. She also notices a\nkeyword multitouch technology, which reveals an aspect she has\nnot thought of before. PaperWeaver saves the updated description\nand uses this description throughout the rest of the process.\nAfter setting up the folder, the researcher receives a paper alert\nemail when there is a new set of recommended papers. By clicking\non the link in the email, she is directed to an interactive paper alert\ninterface (Fig. 2). She bookmarks this link for future reference. She\nquickly goes through the recommended papers and immediately\nsaves papers with obvious relevance (e.g., “The Semantic Reader\nProject: Augmenting Scholarly Documents through AI-Powered\nInteractive Reading Interface”[36]) based on the provided title and\nmetadata (authors and TL;DR). For a paper with non-obvious con-\nnections, she explores the contextualized descriptions of the pa-\nper. She first opens the Problem, method, and findings tab to see\na summary of the paper by the folder context broken down into\n\nPaperWeaver: Enriching Paper Alerts by Contextualizing Descriptions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nA\nB\nC\nD\nE\nF\nB\nC\nFigure 2: PaperWeaver’s paper alert interface. The page shows the folder title and description (A) and a list of recommended\npapers. Each recommended paper is shown on a paper card with the title, authors, venue, and publication year. The bottom of\nthe card features descriptions in three tabs: Relate to Paper (B) allows users to select a paper from existing papers in the folder\nwith the description related to the recommended paper. These descriptions are paper-paper Descriptions Based on Citances and\nPaper-paper Descriptions via Generated Pseudo-citances that focus on specific relationships between two papers. The description\nassigns the label \"Paper A\" to the recommended paper and \"Paper B\" to the existing paper in the library and highlights the\nreferences in different colors; Problem, method, and findings (C) describes the recommended papers in three aspects— problem,\nmethod, and findings—related to the folder context with contextualized Aspect-based Paper Summaries; Abstract (D) shows the\nunmodified abstract of the paper. The user can save a paper to the library (E) and take notes about the recommended paper (F).\nproblem, method, and findings. For example, for the recommended\npaper “Synergi: A Mixed-Initiative System for Scholarly Synthesis\nand Sensemaking” [29], she can see that PaperWeaver-provided\nmethod of the paper is relevant to her interest in LLM (\"Method:\nDeveloped Synergi, a mixed-initiative workflow tying user input,\ncitation graphs, and LLMs\") and the paper’s problem is \"Problem:\nsynthesizing research threads from multiple papers is challenging\".\nSeeing such connections with Reading-supporting interfaces, she\nsaves the paper to her library folder. Feeling curious about the\nSynergi paper, she decides to explore the paper-paper relationship\ndescriptions under the Relate to Paper tab. She chooses to compare\nthe recommended paper with a paper “CiteRead: Integrating Local-\nized Citation Contexts into Scientific Paper Reading” [48] that she\nis already familiar with. She learns that the system in the Synergi\npaper requires more active engagement from users compared to\nthe system in the CiteRead paper. Now, she notes interactivity as\none dimension of the reading interfaces design space. She also sees\nthat there is a description for the Synergi paper and a paper she has\ncollected in passing. By reading the description, she now recalls\nthat “Scim: Intelligent Skimming Support for Scientific Papers” [17]\nsupports paper reading by automatically highlighting important\ncontent in a paper and, unlike Synergi, there is no personalization\ncomponent in the system. Compared to other traditional paper\nalert systems that only show a list of titles and, at most, abstracts,\nPaperWeaver helps the researcher figure out how each of the rec-\nommended papers connects to her topic of interest, as well as their\nnuanced relationships to papers she had already collected.\n\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nLee et al.\n4.2\nMethods for Generating Contextualized\nDescriptions\nTo generate contextualized descriptions like those presented in\nthe scenario, we developed an LLM-based pipeline (Fig. 3) that\nprocesses the user’s folder information and the recommended pa-\nper’s content to generate descriptions for each recommended pa-\nper. LLMs can make meaningful improvements in comprehension\nand summarization, particularly for long, complex documents that\ndemand a high degree of accuracy [3]. This capability enables Pa-\nperWeaver to identify relevant aspects with the given context\nin papers and synthesize descriptions from multiple aspects. Pa-\nperWeaver generates three types of descriptions: (1) contextu-\nalized aspect-based paper summaries, (2) paper-paper descriptions\nbased on citances, and (3) paper-paper descriptions via generated\npseudo-citances. We describe how PaperWeaver generates a com-\npact summary of collected papers in a topical folder (§4.2.1) and\nuses the folder summary to generate contextualized aspect-based\nsummaries (§4.2.2), contextualized paper-paper relationships when\nrecommended papers cite one or more collected papers (§4.2.3), and\nwhen they do not (§4.2.4). Full prompts are in Appendix A.\n4.2.1\nSuggested Topic Description. To ensure the three types of\ndescriptions (detailed in the following subsections) are relevant\nto the topic of the folder, PaperWeaver allows users to provide a\ncompact description for their folders in addition to the folder name.\nThis topic description is used in all subsequent LLM prompts to\nconvey the user’s interests. To lower the effort of writing folder\ndescriptions, PaperWeaver uses an LLM to generate a default\ndescription based on collected papers already saved in the folder.\nWe adapted our prompt design from the prior LAMP approach[50],\nwhich creates a user profile prompt with the information from\nthe user’s own paper. Our prompt takes a task instruction and\na list of titles of papers included in the folder as inputs (T1 in\nAppendix A.1). Then, our prompt instructs an LLM to generate\na description including the folder title, shared goals of collected\npapers, and a list of topical keywords (Fig. 2A; folder title and\ndescription). This default folder description was then shown to\nthe user. The user could further edit them to reflect their interests\nbeyond the papers that they had collected.\n4.2.2\nContextualized Aspect-based Paper Summaries. Among the\nvarious aspects of the recommended paper, we extract the aspects\n(i.e., rhetorical structure elements indicating problem, method, find-\nings [40]) that the user who has curated this library folder might\nfind relevant. The problems, methods, and findings are typically the\nmain pillars of most papers [11]. At the same time, these aspects\ncan describe research in a specific and comprehensive way. To\nextract a set of problems, methods, and findings in the context of\nthe library folder, our method takes a title and an abstract of the\nrecommended paper and the folder description that represents the\nuser’s research interest as inputs (T2 in Appendix A.2). We guide an\nLLM to identify as many relevant problems from the recommended\npaper as possible. Then, we describe the specific methods applied\nby the paper for each problem and elaborate on the specific find-\nings identified by applying each method. Our method uses these\naspects not only as the summary of a single recommended paper,\nwe further build on the extracted aspects to align two papers to\ncreate pseudo-citances as described in §4.2.4.\n4.2.3\nPaper-paper Descriptions Based on Citances. For this descrip-\ntion type, we exploit citations between recommended papers and\ncollected papers. Citation sentences (i.e., citances) are widely used\nas proxies of the relationship between the citing and the cited pa-\npers [38]. Among the intents of citances (i.e., background, method,\nor results), citances with the background intent give more context\nabout a problem, concept, approach, topic, or importance of the\nproblem in the field [14]. Citances with the background intent of-\nten contain information about how a citing paper presents a new\napproach and how it compares to a cited paper. Since our formative\nstudy revealed that participants regarded “build on” relationship\nto be important, we prioritize background citances as classified by\n[14] when selecting a citance to generate a description.\nA citance by itself is hard to understand for users without any\ncontext. Once a citance was selected in a recommended paper, we\nextracted the paragraph which the citance was in (i.e., citing para-\ngraph) to obtain additional context around it. Rather than showing\nthe citing paragraph that only has partial content of the recom-\nmended paper, we employ an LLM to generate a compact but de-\ntailed description that describes both the recommended paper and\nits relationship to the cited collected paper as mentioned in the\nciting paragraph. Additionally, to support our DG3 of helping users\nlearn new aspects of collected papers, we added a short summary\nof the cited collected paper. To obtain this description, we designed\ninputs of the prompt to include the titles and abstracts of both the\nrecommended and the cited paper, as well as the citing paragraph\n(T3 in Appendix A.3).\n4.2.4\nPaper-paper Descriptions via Generated Pseudo-citances. The\nmethod described in the above section (§4.2.3) does not apply to all\nrecommended papers. Authors typically do not comprehensively\ncite all relevant papers. Some relevant papers are omitted from the\ncitances even when they are relevant to some of the collected papers.\nWe build on prior work that used the problem-method-findings\nschema to show similarities and differences between papers in a\nsearch engine setting. Our method generates structured summaries\nusing a similar approach to describe paper recommendations an-\nchored to previously collected papers. In our design, this type of\ndescription features two core relations primitives between papers: 1)\ncomparisons, which provide concise descriptions of the most salient\nsimilarities along either the problem or the method aspect and 2)\ncontrasts, which surface differences along a different aspect (e.g.,\ntwo papers that tackled the same research problem but used differ-\nent methods). This structure has been shown to facilitate scholarly\nsensemaking and inspirations (§2.2.)\nRather than concatenating each paper’s aspects that are similar\nor different with extractive techniques, our method aims to provide\nwell-aligned comparisons and contrasts between two papers. This\nmethod is motivated by the formative study where participants men-\ntioned that simple concatenation was not helpful in making sense\nof the relationships between two papers. To create this description,\nour method follows the following process: (1) find relevant papers,\n(2) identify shared aspects for each pair of the recommended paper\nand a collected paper to find similarities and differences between\nthem, (3) verify whether the shared aspect is aligned with both\npapers and (4) generate a structured summary.\n\nPaperWeaver: Enriching Paper Alerts by Contextualizing Descriptions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nCitance-based Paper-Paper Description\n\nSynergi and Relatedly both tackle the challenge of \nsynthesizing and navigating complex scientific literature, \nemploying integrated systems with user input, citation \ngraphs, and machine learning. In Synergi, [...] Relatedly, on \nthe other hand, [...] While both papers aim to augment \nscholarly discovery, Synergi emphasizes on user \npersonalization and iterative synthesis, and Relatedly \nfocuses on managing overlapping references and \nspotlighting unexplored information.\nPsuedo-citance Paper-Paper Description\n\nBoth Synergi and Scim address the shared issue of \nmanaging and synthesizing information from the \nexpanding volume of scientific literature, with a focus on AI \nintegration and enhanced user interaction. Synergi presents \n[...] Scim, introduces [...] While both papers aim to aid in \nliterature management, Synergi focuses on personalizing \nand iterating synthesis output, whereas Scim emphasizes \nskimming assistance through adjustable highlight density.\nContextualized Aspect-based Paper Summaries\n\nProblem: Efficiency in reviewing and synthesizing scholarly \nliterature is challenging due to growing publications.\nMethod: Synergi, a computational pipeline combining user-\ninput, citation graphs and LLMs for structuring research \nthreads.\nFinding: Synergi efficiently helps scholars make sense of \nrelevant threads, broadens perspectives, and increases \ncuriosity.\nFind Relevant \nFolder Paper To \nCompare\nPaper A\nPaper B\nPaper C\nExtract Folder \nDescription-Based \nAspects\nBackground Problem\nUsage Of Method\nFinding\nGenerate Description \nUsing Citing Paragraph\nRecommended\nPaper A\nIf there is\na citation\nFolder-based \nTopic Description\nT1\nT3\nT2\nT4\nT5\nT6\nIdentify Shared \nAspect Between \nPapers\nIf there is\nno citation\nVerifying \nAlignment\nGenerating \nDescription Using \nPsuedo-Citance\nIf there is a\nrelevant paper\nIf there is no\nrelevant paper\nFigure 3: Overview of PaperWeaver’s pipeline to generate contextualized descriptions. If there is a citation from the recom-\nmended paper to a collected paper, PaperWeaver generates paper-paper descriptions based on citances using citing paragraph.\nIf there are no citances, PaperWeaver synthesizes pseudo-citing sentences that shows the relationship between recommended\npaper and relevant collected paper to make paper-paper descriptions via generated pseudo-citances. If there are no relevant\ncollected papers, PaperWeaver generates contextualized aspect-based paper summaries with the folder’s overall context.\nPrompt T1-T6 are in the Appendix A.\nHere, we explain our approach in more detail, using the case\nwhere the recommended paper and the collected paper share sim-\nilar “problems”. Our approach first selects the top-5 most similar\ncollected papers to a recommended paper based on the abstract simi-\nlarity using Flag embeddings [56], a state-of-the-art text embedding\nmodel. Then, with the aim of finding similarities and differences\nbetween papers, we use the method described in §4.2.2 to extract\nmultiple problem-method aspect pairs from each paper’s abstract.\nBy providing the titles and the aspects of all five relevant collected\npapers and the recommended paper as inputs, we instruct an LLM\nto (1) identify the top-5 papers that have problems that are the most\nsimilar to the problems in the recommended paper, (2) list all of\nthe identified pairs (one from the given paper and the other from\na collected paper), and (3) describe one shared problem that could\nencompass the two identified problems. To confirm whether the\nshared problem encompasses problems of both the recommended\npaper and the chosen collected paper, our approach prompts an\nLLM to verify whether the shared problem is addressed in each\npaper. This was done by providing each paper’s title, abstract, and\nthe generated shared problem. Finally, by inputting respective con-\ntrasting methods employed in the recommended and the folder\npaper, along with the generated shared problem and the abstracts\nof both papers, the LLM generates the structured summary. This\nsummary includes comparing and contrasting sentences with short\nsummaries of two papers. While this process explains how to gen-\nerate a description for two papers with similar problems, a similar\nprocess applies to a description of two papers with similar methods.\n4.3\nPaper Alert Interface\nFor each folder, PaperWeaver takes the folder title, the folder\ndescription, and a list of papers in the folder as inputs. We use\nthe publicly available Semantic Scholar Paper Recommendation\nAPI8 to retrieve a list of recommended papers for a given set of\nfolded papers. For each folder, a dedicated web page allows users\nto view recommendations displayed on detailed cards, featuring\ninformation about the paper (title, authors, venue, and year) and a\nmachine-generated TL;DR summary [10]. The title of a paper links\nto the corresponding paper details page on Semantic Scholar where\nusers can access the pdf file of the paper (if available) together\nwith other information such as the paper’s citations and references.\nUsers can explore different descriptions displayed in three tabs:\n• Related to Paper (Fig. 2B) shows paper-paper descriptions\nthat are focused on showing relationships between the rec-\nommended paper and a specific paper previously saved in\nthe folder. The descriptions shown in this tab are paper-paper\ndescriptions based on citances and paper-paper descriptions\nvia generated pseudo-citances. The two papers mentioned in\nthe text are highlighted in different colors to indicate which\npaper the description refers to. Users can use the dropdown\ncontrol to select descriptions for the relationships between\nthe recommended paper and a specific paper.\n• Problem, method, and findings (Fig. 2C) shows the contex-\ntualized aspect-based paper summary for the recommended\npaper broken down into three aspects.\n8https://api.semanticscholar.org/api-docs/recommendations\n\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nLee et al.\n• Abstract (Fig. 2D) shows the original full abstract of the\npaper.\nThrough the paper alert interface, users can save a recommended\npaper (Fig. 2E) to their folder and write a note (Fig. 2F) about the\nrecommended paper for future references.\n4.4\nImplementation Details\nPaperWeaver interface is a standard web application. The back-\nend was implemented in Python using Flask for an HTTP server\nand PostgresSQL for a database. The front-end was written in Type-\nScript using React framework. PaperWeaver retrieves the paper’s\nmetadata (title, authors, abstract, etc.), TL;DR and citances from the\npublic Semantic Scholar API 9. We obtain the extracted plain text\nfor papers using S2ORC, an open-source PDF-to-text extraction\npipeline and a corpus of processed 81.1M academic papers across\nmultiple disciplines [37]. We use GPT4-0613 through OpenAI API10\nfor all text generation with an LLM.\n5\nUSER STUDY\nWe conducted a within-subjects laboratory study to investigate\nwhether PaperWeaver can help readers efficiently understand the\nrelevance of new papers, triage them, and deeply understand their\nrelevance to users’ own context.\n5.1\nResearch Questions\nTo this end, we formulated our research questions as follows:\n[RQ1] How do relevance descriptions augmented by user-collected\npapers’ information help a user understand how recom-\nmended papers are relevant to the user’s research context?\n[RQ2] How do relevance descriptions augmented by user-collected\npapers’ information help a user triage the paper?\n[RQ3] How do relevance descriptions augmented by relationships\nhelp a user understand the relationship between user-collected\nand recommended papers?\n[RQ4] How do relevance descriptions augmented with a set of user-\ncollected papers help a user understand new aspects of the\ncollected papers?\n5.2\nStudy Design\n5.2.1\nParticipants. We recruited 15 researchers who are Ph.D./MS\nstudents in the CS domain for the study through snowball sampling\nand the authors’ social media (Twitter). Two of the participants\nwere Master’s students, seven were junior (first- and second-year)\ndoctoral students, six were senior (third-, fourth-, and fifth-year)\ndoctoral students. Our study focused on Ph.D./MS students as they\nmay receive the greatest benefit from explaining why those papers\nare connected to their personal research context. Ph.D./MS students\nneed to keep up with the large amount of literature the most. All\nparticipants reported that they read paper alerts, from monthly to\ndaily.\n9https://api.semanticscholar.org/api-docs/graph#tag/Paper-Data/operation/get_\ngraph_get_paper\n10https://api.openai.com/v1/chat/completions\nFigure 4: In the baseline condition, participants also see\nthe recommended papers in a similar paper card to Paper-\nWeaver paper alert interface. Instead of the three tabs with\ndifferent types of explanation, there are two tabs: Abstract\nthat shows the abstract of the recommended paper and Re-\nlated work that shows the text of the related work section\nof the recommended paper. The rest of the functionalities\nremain the same across conditions.\n5.2.2\nConditions. For the study, participants read paper alerts and\nlisted what they learned from them using either PaperWeaver or a\nbaseline system. Like in PaperWeaver, each recommended paper\nin the system for the baseline condition (Fig 4) contains its TL;DR,\nabstract, and the link to the paper’s details (the paper details page on\nSemantic Scholar). Additionally, we provide excerpts of the related\nwork section of the recommended papers in the baseline condition.\nThe related work section usually shows relationships between the\npaper and prior work. Researchers are likely to check this section to\nlook for relationships between the recommended paper and other\npapers. While both PaperWeaver and the baseline system give\nthe link to the full paper to inspect the paper further, having the\nrelated work section on the baseline condition helps balance the\namount of readily available information between the two conditions.\nIn the PaperWeaver condition, contextualized aspect-based paper\nsummaries and paper-paper descriptions are additionally provided.\nThe ordering of the conditions was counterbalanced to mitigate\npotential ordering effects.\n5.2.3\nPaper Alert Setup. We asked participants to create their own\nlibrary folders (paper collections) on Semantic Scholar and save pa-\npers into them before the study. To balance the quality and familiar-\nity of folder content between the conditions, we asked participants\nto follow the instructions:\n1. (Scenario and Number of Topic Folders) Pick three research\ntopics for curation, and fix the use scenario (e.g., surveying\nthe literature to write a related work section in your paper).\n2. (Topic Familiarity) Topics should be similarly familiar to you.\n3. (Number and Familiarity of Papers) Collect five papers you\nfully read, three papers that you have read somewhat, and\ntwo papers that you have seen but not read.\nWe chose two folders from the three participants brought with\nthem based on the following criteria, to reduce the significance of\npotential quality and familiarity differences between the papers\n\nPaperWeaver: Enriching Paper Alerts by Contextualizing Descriptions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nactually used in the experiment: We generated recommendations\nfor each folder and ranked them based on the content similarity\nusing the average of cosine similarity between library papers’ and\nrecommended papers’ SPECTER [15] embeddings; pick the top two.\nRegarding folder descriptions, participants saw an example and\nedited them before the study.\n5.2.4\nProcedure. The study was conducted remotely using video\nconferencing software. After a brief introduction to the overall\nstudy, participants were given a short tutorial of the system (ei-\nther treatment or control, counterbalanced). They interacted with\nthe interface for 2 minutes to familiarize themselves with the in-\nteraction features using fake data. After the tutorial, participants\nwere given 12 minutes to read the descriptions on 8 recommended\npapers. After they read the descriptions, they were provided with\nsurvey questions. Once the survey was completed, participants\nwere given 10 minutes to write down a list of what they learned\nfrom reviewing the descriptions and wanted to retain for future\nuse. Participants were asked to use their own language to describe\nthe things and were informed the quality of writing was irrelevant.\nThe steps above were repeated for the second condition. Finally,\nthe experimenter conducted a semi-structured interview about par-\nticipants’ overall experience. The study lasted for ∼90 minutes, and\nparticipants were compensated $80 USD for their time. The study\nwas approved by our internal review board.\n5.3\nMeasures\n5.3.1\nOverview. In relation to the research questions described in\n§5.1, we measured participants’ subjective responses to correspond-\ning survey questions. To triangulate the survey responses, two of\nthe authors manually coded what participants wrote down as their\nlearning from reading the descriptions in terms of whether it con-\ntained elaboration on the relationship between a recommended\npaper and the saved papers participants began with or their cu-\nriosity for further investigation. We also employed a survey asking\nparticipants’ general experience with each system. Finally, we con-\nducted a sanity check on the degree of factual hallucination among\nthe descriptions generated in PaperWeaver, to quantify its current\nlimitations and point to future work directions.\n5.3.2\nTargeted Survey Responses. We collected participants’ re-\nsponses (7-point Likert scale) to the following five questions from\nthe survey: “The system helped me understand how the recommended\npapers are relevant to my research interest.” (RQ1), “The system helped\nme decide which papers are worth saving.” (RQ2), “I was confident in\ndeciding which papers were relevant and worth saving.” (RQ2), “The\nsystem helped me understand how the recommended papers relate to\nthe papers I have already saved.” (RQ3), and “The system helped me\nlearn something new about the papers I have already saved.” (RQ4).\n5.3.3\nGeneral Experience Survey Responses. To measure perceived\nworkload, the survey also included five questions (excluding the\nphysical demand question) from the NASA-TLX questionnaire [22],\nwhere a more compact 7-point scale, mapped to the original 21-\npoint scale, was instrumented [49].\n5.3.4\nAnnotating Participants’ Learning. To deeply analyze partic-\nipants’ learning from reviewing the descriptions, we constructed\nan annotation regime in five dimensions to capture the quantity\nof facts and relationships described in participants’ notes, and the\nlevel of elaboration around relationship- or curiosity-related de-\ntails. Specifically, the five dimensions were: 1) the number of facts\nabout papers; 2) the number of groups of papers synthesized by\nparticipants; 3) the number of relationships described between a\npaper and a group of papers or between groups of papers; 4) the\nlevel of factual details on the relationships; 5) mentions of curios-\nity and/or authentic motivation for further investigation. The first\nthree dimensions were coded using an interval scale, ranging from\n0; the last two dimensions were coded using a binary scale, where\n0 represented ‘lacking elaboration’. We sampled a set of randomly\nsampled 15 notes participants wrote from the full set of 169 notes\nfor two annotators (two of the authors who were not experimenters\nin the study). The annotators coded and discussed these samples\nover two rounds to reach a consensus. The following is the rubrics\ndeveloped for annotation:\nStep 1 Identify a paper or a group of papers described in the com-\nment.\nStep 2 Identify whether there is a relationship described between a\npaper and a group of papers, among a group of papers, or\nbetween groups of papers.\nStep 3 Once at least one relationship is found, find whether there are\nfactual details or concepts elaborated about the relationship\nin the comment.\nStep 4 Similarly with Step 3, identify if participants expressed sur-\nprise, curiosity, or motivation for future investigation in the\ntext (e.g., “I’m not sure how that difference manifests yet.”,\n“I’m surprised by the results, I’ll look into the papers later.”).\nStep 5 Count the number of distinct facts, using the clause or sen-\ntence boundaries.\nThe annotators annotated 20 shared samples (randomly drawn)\nblind-to-condition to check for inter-rater reliability. This resulted\nin Krippendorff’s 𝛼’s ranging between 0.46 (the binary measure of\nfactual detailedness in descried relationships) and 0.78 (the number\nof groups of papers synthesized by participants).\n5.3.5\nAccuracy of LLM-generated Descriptions. Additionally, as\nLLM can generate hallucinations (i.e., factually incorrect informa-\ntion), we collected annotations of factual correctness for 60 de-\nscriptions for each of contextualized aspect-based description and\npaper-paper description used in the user study. Since evaluating the\nfactual correctness of our description requires expertise in these\ndomains, we recruited two experts per domain for the HCI and\nNLP domains for annotation. Each expert annotated the data for\nrecommended papers in their domain of expertise. For each descrip-\ntion, experts rated its factual correctness on a three-point scale\nbased on counting the number of factually incorrect mentions in\ndescriptions (3: None, 2: 1-2, 1: 3 or more) and wrote how they are\nincorrect. Two experts in each domain independently annotated 40\ndescriptions per expert (20 per each type), where 20 descriptions\nget two annotations (60 samples are evaluated in total per domain).\nKrippendorff’s 𝛼is 0.55 for HCI paper descriptions and 0.41 for\nNLP ones.\n\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nLee et al.\n5.4\nAnalysis\nWe analyzed the survey data (§5.3.2 and §5.3.3) by first conducting\nthe Shapiro-Wilk test to determine if the data required a parametric\n(P) or non-parametric (NP) testing procedure and proceeded with a\npaired t-test (when parametric) and a Wilcoxon signed-rank test\n(when non-parametric) accordingly. In order to account for the\nbetween-subject variability in participants’ notes data (§5.3.4), we\nemployed a Mixed Linear Model Regression. The dependent vari-\nable in the model was each of the five dimensions of annotations,\nrandom effects were the participant IDs, and the fixed effects in\nthe model were the experimental conditions. We used Restricted\nMaximum Likelihood (REML) for parameter estimation to ensure\nrobust estimates of the variance and covariance parameters in cases\nwhen the homoscedasticity and normality of data assumptions are\nviolated. We report on the results of the regression analyses as re-\ngression coefficients (𝛽’s) and 𝑝-values, with significance indicated\nat the 𝛼= .05 level. We also report the fixed effects sizes (semi-\npartial R2, which represent the % outcome variance, controlling for\nthe predictors and random effects in the model) when applicable.\n6\nFINDINGS\nOur results showed that PaperWeaver helped participants un-\nderstand the relevance of recommended papers and make more\neffective relevance judgments. Additionally, PaperWeaver pro-\nmoted the discovery of more connections between papers that were\nrelevant to the topic of the folder, and led to writing more detailed\nnotes that contained rich connections between papers.\n6.1\nGeneral Behavioral Differences\nDuring the study, we asked participants to think-aloud and observed\nhow they interacted with the two systems. In both conditions, par-\nticipants typically did a quick pass over the recommended paper\nlist to filter out few recommended papers that seemed obviously\nirrelevant. When interacting with the baseline system, participants\nrelied on the titles and the TL;DR summaries for this quick triaging\nprocess, while when interacting with PaperWeaver they addition-\nally considered the contextualized aspect-based description. P2 men-\ntioned that even though the two summaries were similar in length,\nTL;DR summaries typically focused on the research problems that\nwere not always relevant to their folder’s topic. In contrast, the\ncontextualized aspect-based description provided by PaperWeaver\noften surfaced parts of the abstracts that were relevant. At the same\ntime, participants in both conditions said that after this first pass\nthey still needed to examine most of the papers more carefully to\nunderstand how they are relevant and to confidently judge which\nones to save.\nWhen interacting with the baseline system, participants contin-\nued to read the full abstract or open the papers to see its figures\nbut said this process was effortful. For example, P9 said that “I tried\nto understand [the recommended] papers but reading all of the ab-\nstracts is overwhelming.” Further, even when they tried to carefully\nexamine the recommended papers in the baseline condition, they\noften failed to identify the connections. For example, P1 mentioned\nthat “I cannot understand why this paper is recommended to me. It\nseems they are talking about just their own topic” and P15 said that\n“Abstract alone cannot answer ‘how does this paper relevant to my\nresearch context?’.\nIn contrast, when interacting with PaperWeaver, participants\nrelied on paper-paper description after this first pass. In this case,\nmany participants mentioned that they can better understand “how\nthis paper is different from the paper that they’ve already known” and\n“what new contributions are there [in the recommended papers]” that\nwere relevant to the collected paper. Interestingly, after reading the\npaper-paper description, participants often still continued to examine\nthe abstracts. Participants mentioned the goal was to both verify the\nLLM-generated paper-paper description with the original sources\nand to “gain deeper context” around the paper-paper description\nonce they became interested in a recommended paper (more details\naround this behavior in §6.4).\n6.2\nExploring Papers Broadly by Understanding\nRelevance\nBased on the post-survey, participants felt that they could under-\nstand how the recommended papers were relevant to their own\nresearch interest significantly better in PaperWeaver (𝑀= 6.07,\n𝑆𝐷= 0.68) than the baseline (𝑀= 3.80, 𝑆𝐷= 1.64, 𝑝= 0.0013, NP).\nWe also found evidence that PaperWeaver supported decision-\nmaking. Specifically, participants felt PaperWeaver helped them\ndecide which papers were worth saving (PaperWeaver: 𝑀= 5.27,\n𝑆𝐷= 1.06, baseline: 𝑀= 4.00, 𝑆𝐷= 1.55, 𝑝= 0.0024, P) and\nwere significantly more confident in their decisions (PaperWeaver:\n𝑀= 6.07, 𝑆𝐷= 0.85, baseline: 𝑀= 5.33, 𝑆𝐷= 1.07, 𝑝= 0.0124,\nNP). Qualitative insights revealed that these perceptions were due\nto how PaperWeaver contextualized explanations for each user\nbased on their topical folder. According to our participants, both\ncontextualized aspect-based description and paper-paper description\neffectively highlighted which parts of the abstracts or papers they\nshould focus on (P9, P13). Specifically, while contextualized aspect-\nbased description surfaced “explicitly relevant aspects dispersed in\nmultiple sections [in the paper, relevant] to my folder context”. Par-\nticipants also appreciated how paper-paper description connected\nrecommended and collected papers with detailed and insightful\nrelationships with “narrower research interest perspective” when\ncompared to uncontextualized TL;DR summaries. For example, P15\nmentioned that “I don’t have time to read all the abstracts but TL;DRs\nare too high-level. So with the [baseline], I do not have evidence to\nchoose what to save. With the descriptions in PaperWeaver, I was\nmore confident in my decision because I can get more understandable\nevidence from the explanations. For example, how the recommended\npapers [were relevant but] tackled different problems than the paper I\nknew already)”.\n6.3\nDeeper Insights on Both Recommended and\nPreviously Collected Papers\nTo gain a deeper understanding of the knowledge participants had\ngained from interacting with PaperWeaver and the baseline, we\nfurther analyzed the notes participants took during the study in the\ntwo conditions (while being blind to which conditions the notes\ncame from). We found that when using PaperWeaver, participants\non average captured significantly more notes that described con-\nnections between papers (PaperWeaver: 𝑀= 2.21, 𝑆𝐷= 1.81,\n\nPaperWeaver: Enriching Paper Alerts by Contextualizing Descriptions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nFigure 5: Results of the post-survey showed that participants found various benefits when using PaperWeaver compared to a\nstrong baseline. **, *, and ns indicate significance of 𝑝< 0.01, 𝑝< 0.05, and 𝑝> 0.05, respectively.\nFigure 6: Based on a NASA-TLX survey, participants perceived similar workload when using PaperWeaver and the baseline\nconditions. **, *, and ns indicate significance of 𝑝< 0.01, 𝑝< 0.05, and 𝑝> 0.05, respectively.\nbaseline: 𝑀= 1.07, 𝑆𝐷= 0.96, 𝑝= 0.0459, NP). At the same time,\nwhen describing the relationships, participants included similar lev-\nels of details after accounting for between- and within-participant\nvariability (𝛽= .113, 𝑝= .13). This suggests that the bottleneck\nof learning connections between papers is a recall problem, and\nthat it is easier to capture these connections with PaperWeaver\ncompared to looking at multiple paper abstracts in the baseline\ncondition. These results based on participant behavior also corrob-\norate their perceived understanding of the information that was\npresented in the two conditions. Specifically, in the post-survey,\nparticipants reported that they could understand how the recom-\nmended papers connect to their collected papers significantly better\nwith PaperWeaver (PaperWeaver: 𝑀= 5.80, 𝑆𝐷= 1.04. baseline:\n𝑀= 3.00, 𝑆𝐷= 1.67, 𝑝= 0.0021, NP).\nOne possible trade-off for the positive learning effect could re-\nquire significantly higher cognitive demands on the users. However,\nbased on our NASA-TLX survey, participants did not report a higher\nworkload when comparing using PaperWeaver and the baseline.\nOn the other hand, qualitative data do show anecdotal evidence of\nhigher mental and temporal demand for some participants when us-\ning PaperWeaver. For example, P6 and P9 pointed to descriptions\nfrom PaperWeaver that peaked their interest in the recommended\npapers and urged them to explore them in more detail. P4 further\ncommented that “I felt mentally more demanded using PaperWeaver\n\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nLee et al.\nbecause I actively compare the new papers with other [collected] pa-\npers. This might be a positive side effect of showing relationships.”\nThese comments suggest that even for participants who felt using\nPaperWeaver was more cognitively demanding, they perceived it\npositively because it prompted them to become more actively en-\ngaged with the paper alert, motivating them to more deeply process\nthe information in front of them.\nFinally, in addition to understanding the recommended papers,\nour analysis of the post-survey data also revealed that participants\nfelt PaperWeaver helped them “learn something new about the\npapers that they have already saved in the folder” significantly more\nthan the baseline (PaperWeaver: 𝑀= 5.33, 𝑆𝐷= 1.49. baseline:\n𝑀= 3.67, 𝑆𝐷= 2.18, 𝑝= 0.011, NP). In the interviews, participants\npointed to different benefits when seeing descriptions that cov-\nered relevant collected papers in the folders, including refreshing\ntheir memories about papers they had previously read and gaining\nnew knowledge and perspectives about them based on how recom-\nmended papers described them in their related work sections. In\naddition, we also found some participants who had saved papers in\ntheir folder that they had not yet read, in this case, they describe\nhow the descriptions from PaperWeaver helped them rediscover\npreviously collected papers with a renewed interest.\n6.4\nAccuracy of LLM-Generated Descriptions\nOne known issue with current LLMs is that they can be prone to\nhallucinations (cf. [5, 7, 53]), and we did find that some descriptions\ncontained one or two mistakes based on manual evaluation. In\ngeneral, contextualized aspect-based description are more extractive\nand contain fewer mistakes than paper-paper description which\nrelies on training knowledge to align the extracted aspects. Specif-\nically, for contextualized aspect-based description, 8% of samples\nhas one or two mistakes (HCI: 6%, NLP: 10%, based on 30 samples\neach), and paper-paper description, 20% (HCI: 16 %, NLP: 23%, based\non 30 samples each). One fundamental question here is that - is\nthis an acceptable level of accuracy in the context of paper alerts?\nResults from our user study around participants’ strategies when\ninteracting with PaperWeaver offered some insights.\nFirstly, like most retrieval systems, results from document rec-\nommender systems can also contain errors (i.e., documents falsely\nclassified as relevant) [15]. Because of this, we found that when in-\nteracting with paper alerts, participants were already in the mindset\nof verifying the machine-generated document recommendations,\nin both conditions. We also found participants go through the list of\nrecommended papers in multiple passes, typically using the titles\nfirst to rule out clear irrelevant recommended papers, then reading\nthe generated descriptions in the second pass, and finally verifying\nthem with the abstract or the content of the papers. For example,\nrepresentative quotes from P6 and P8 mentioned “validate the con-\nnections between them [papers] in the abstract” and P2 provided\nmore details around their strategy: “[I read abstracts] not only to\nsee deeper levels of information but also to verify the content of the\ndescriptions.”\nSecondly, many participants also explicitly mentioned that they\nsee the PaperWeaver descriptions as “supplementary material”\nthat help them triage the recommended papers, suggesting that par-\nticipants could appropriately adjust their levels of reliance on the\nLLM-generated descriptions. P2 said “I used the descriptions from [Pa-\nperWeaver] as supplementary material going from titles and TL;DR\nto abstract,” representative quote from P8 additionally pointed to\nbenefits around awareness and discovery “PaperWeaver’s descrip-\ntions triggered me to become interested in two highly relevant [recom-\nmended] papers and have curiosity about them so that I can [decide\nto] read the abstracts and [then the] whole papers to get more infor-\nmation. These descriptions are supplementary bridges from the titles\nand TL;DRs to the abstracts rather than replacing them.”\nFinally, results based on post-survey and NASA-TLX question-\nnaire suggested that PaperWeaver did not increase perceived\nworkload even though participants were actively verifying LLM-\ngenerated content and that they were able to judge the relevance\nof the recommended papers more confidently and captured richer\nrelationships between papers in their notes. We acknowledge that\nour participants were computer science researchers who currently\nmight be more familiar with recent advancements in LLMs than\nresearchers in other domains, although the increasing popularity of\nLLM-powered end-user tools also provides them increasing oppor-\ntunities to interact with LLMs in other scenarios. Nevertheless, our\nresults suggest that interfaces that make use of LLM-generated text\nshould always provide both adequate indication when generated\ntext is presented to its users and allow users to freely turn on or\nturn off generated content. Moreover, it is essential to design ef-\nfective mechanisms for users to verify content efficiently. Inspired\nby AngleKindling’s system design [45] that shows the connection\nbetween LLM-generated angles and source material to support\njournalists, one approach involves making specific sections of the\npaper, particularly relevant to the LLM-generated descriptions, ac-\ncessible—such as through highlighting or indicating whether there\nis clear citations with a hyperlink to source papers. We also can\nindicate whether there is clear source material that the output is\ngrounded on by marking citations at the end of each sentence.\n7\nLIMITATIONS AND FUTURE WORK\n7.1\nLimitations\nOur work has some limitations. We conducted our study with com-\nputer science graduate students who might not represent the broad\nspectrum of academic domains. While our approach is motivated\nby a general use case and designed to accommodate a broad range\nof academic domains, further evaluation with researchers from\nother academic domains, especially less technology-oriented ones,\ncan help us understand how the generated descriptions can be ap-\nplied broadly. Another limitation is our use of the Problem-Method-\nFindings schema. While this schema fits a large portion of papers,\nit might not cover some other papers such as survey papers or\nsystematic review papers [26]. Further, researchers might want\nto apply their own schema (e.g., medical researchers would be in-\nterested in specific aspects of clinical trials). We discuss how to\nextend the schematic digests beyond Problem-Method-Findings in\n§7.4. Finally, the measurements in our user study are more focused\non subjective measures because it is inherently challenging to get\nrobust and valid measures that score participants’ understanding\nof the relevance between papers and measure their triaging perfor-\nmance due to the evolving notion of relevance personalized to each\nindividual. Standardization of this notion would require careful\n\nPaperWeaver: Enriching Paper Alerts by Contextualizing Descriptions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nexaminations of prolonged interaction scenarios beyond the scope\nof the laboratory experiment conducted here. While we obtained\nobjective metrics in the experiment that can be evaluated indepen-\ndently of each researcher’s context, as a future work, conducting a\nlongitudinal deployment study could involve participants assessing\ntheir own written outputs over time, allowing for evaluations that\nconsider evolving individual contexts.\n7.2\nPairwise vs. Multiple-Paper Descriptions\nIn this work, we focused on describing pairs of recommended and\ncollected papers to help users contextualize unfamiliar papers with\nfamiliar papers. In our current design, each description only covers\ntwo papers: a recommended paper and a collected paper. This design\ndecision was based on our formative study where we observed that\nparticipants with different familiarity with the topics all found\nbenefits in seeing descriptions that covered two papers, but less\nexperienced participants felt that descriptions covering three or\nmore papers were too complex for paper alerts. Future work could\nexplore ways to allow users to further customize their paper alerts\nby adjusting the complexity of the descriptions. One opportunity for\nfuture work in this direction is to look to research that focuses on\nautomatic related work section generation [43] or multi-document\nsummarization [21] that aimed to generate descriptions for many\ndocuments. One interesting and relevant use case we observed in\nthe user study was one participant who had saved a survey paper in\ntheir folder. In this case, the description PaperWeaver generated\nallowed participants to compare and contrast recommended papers\nwith different research threads that were described in the survey\npaper’s abstract and was perceived positively by the participants.\n7.3\nLonger-Term Usage and Evolving Folder\nDescriptions\nIn the user study, we observed that participants collected richer\nnotes that captured information that connects multiple papers as\nopposed to about a single paper. Since PaperWeaver leverages\na user’s folder name and description in its prompts to generate a\ndescription that can better reflect a user’s knowledge about the\nfolder topic, an interesting future direction is to allow users to\nupdate their folder name and descriptions based on what they had\nlearned in a paper alert. This information can be used to update\nthe folder description to represent the user’s current knowledge\nand this can, in turn, improve subsequent paper alert generations.\nIn this sense, the user’s folder can serve as an evolving external\nrepresentation of the user’s understanding on a specific research\ntopic. However, the longer-term effects of accumulating additional\nnotes need further investigation.\n7.4\nExtending Schematic Digests beyond\nProblem-Method-Findings\nParticipants in our studies have also commented on how they would\nlike to further customize the information presented in paper alerts,\npointing to avenues for future work. First, some participants com-\nmented that they wanted to be able to surface information along\ncertain other aspects of the schema, such as differences or sim-\nilarities in evaluation regimes and their outcomes (e.g., positive,\nnegative); what types of study designs were run and how they were\nconducted (e.g., controlled lab studies, field deployment studies,\nRCTs); approaches to developing AI models in a given problem\ndomain; and the design of interaction features in proposed sys-\ntems. Such aspects of schemas were customized to different par-\nticipants, suggesting that while the default information provided\nin PaperWeaver’s problem-method-findings schema served as a\nuseful entry into recommended papers, a deeper inspection follow-\ning users’ triage would benefit from further digesting papers along\nuser-defined secondary schemas.\nHowever, the first-and-secondary division of schemas that adapts\nto users’ interaction with paper digests over time also suggests that\nthere is an important gradient of specificity that may have to be\ndesigned to be adaptively adjustable based on user interaction, for\nexample by utilizing a form of passive sensing over users’ intent\nbased on their interaction. Clear examples of this are when partic-\nipants stated that they wanted to “see more” in the aspect-based\npaper summaries, demonstrating an intent for lower-level details;\nanother intent that participants may express would be wanting\nto “adjust” an explanation provided for a recommended paper, for\nexample, as pointed out by P4, when the problem-method-findings\nschema in our approach did not provide useful information for\nsurvey papers because the problem and method descriptions were\npresented at too high of a level, abstracting away useful details\nof any individual papers or groups of papers synthesized by the\nauthors of the survey paper.\n7.5\nContextualized Paper Descriptions beyond\nPaper Alerts\nFinally, while we focused on the scenario of helping users make\nsense of paper recommendation alerts, the proposed pipeline can\npotentially be generalized to other scenarios where users need to\nmake sense of unfamiliar papers. For example, using one’s publica-\ntions as “collected papers” and generating descriptions for papers\nfrom another author to explore common research interests and\nfacilitate collaborations. Another opportunity is to enrich a user’s\nexperience when reading a related work sections in a paper (cf.\nCiteSee [12], Threddy [30], CiteRead [48]) by generating alterna-\ntive descriptions about the cited papers based on papers already\nfamiliar to the current user. Beyond comparing with existing papers,\nusers can also compare their own draft of a paper with new papers\nthat might be relevant to them to get insights or new perspectives\nfor framing the contrast and comparisons between papers when\norganizing related work sections.\n8\nCONCLUSION\nThis work presents PaperWeaver, an enriched paper alert system\nthat provides contextualized text descriptions of recommended\npapers based on user-collected papers. PaperWeaver leverages an\nLLM-based computational method to infer users’ research interests\nfrom their collected papers and extracts contextualized aspects in\nrecommended papers that are relevant to the inferred user’s interest.\nOur method establishes relationships between recommended and\ncollected papers by comparing and contrasting these contextualized\naspects. Through a within-subjects user study (𝑁= 15), we found\nthat PaperWeaver helped researchers more confidently make sense\nof paper recommendations and discover more useful relationships\n\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nLee et al.\nbetween recommended and collected papers in the folder when\ncomparing with a baseline interface that enriched existing paper\nalert systems with abstract summaries and extracted related work\nsections.\nACKNOWLEDGMENTS\nThe authors would like to thank Daniel S. Weld, Doug Downey, and\nthe researchers in the Semantic Scholar team for their insightful\nfeedback and all the support for this work. We also thank Benjamin\nNewman and Tae Soo Kim for their thoughtful discussions. We\nalso thank the anonymous reviewers for their constructive feed-\nback. Finally, we would like to thank the various researchers who\nparticipated in our pilot test and user study.\nREFERENCES\n[1] Shaaron Ainsworth. 2006. DeFT: A conceptual framework for considering learn-\ning with multiple representations. Learning and instruction 16, 3 (2006), 183–198.\n[2] Shaaron Ainsworth. 2008. The educational value of multiple-representations\nwhen learning complex scientific concepts. In Visualization: Theory and practice\nin science education. Springer, 191–208.\n[3] Anthropic. 2023. Introducing Claude 2.1.\nhttps://www.anthropic.com/index/\nclaude-2-1 Accessed: 2023-11-21.\n[4] David Paul Ausubel. 2012. The acquisition and retention of knowledge: A cognitive\nview. Springer Science & Business Media.\n[5] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan\nWilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask,\nmultilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and\ninteractivity. arXiv preprint arXiv:2302.04023 (2023).\n[6] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language\nModel for Scientific Text. In Conference on Empirical Methods in Natural Language\nProcessing. https://api.semanticscholar.org/CorpusID:202558505\n[7] Michael J. Black. 2022. Michael J. Black on Twitter. https://twitter.com/Michael_\nJ_Black/status/1593133722316189696 Accessed: 2023-03-28.\n[8] Ann M Blair. 2010. Too much to know: Managing scholarly information before the\nmodern age. Yale University Press.\n[9] Richard E. Boyatzis. 1998. Transforming Qualitative Information: Thematic\nAnalysis and Code Development.\n[10] Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel S. Weld. 2020. TLDR: Extreme\nSummarization of Scientific Documents. ArXiv abs/2004.15011 (2020). https:\n//api.semanticscholar.org/CorpusID:216867622\n[11] Joel Chan, Joseph Chee Chang, Tom Hope, Dafna Shahaf, and Aniket Kittur.\n2018. SOLVENT: A Mixed Initiative System for Finding Analogies between\nResearch Papers. Proc. ACM Hum.-Comput. Interact. 2, CSCW, Article 31 (Nov.\n2018), 21 pages. https://doi.org/10.1145/3274300\n[12] Joseph Chee Chang, Amy X Zhang, Jonathan Bragg, Andrew Head, Kyle Lo, Doug\nDowney, and Daniel S Weld. 2023. CiteSee: Augmenting Citations in Scientific\nPapers with Persistent and Personalized Historical Context. In Proceedings of the\n2023 CHI Conference on Human Factors in Computing Systems. 1–15.\n[13] Duen Horng Chau, Aniket Kittur, Jason I. Hong, and Christos Faloutsos. 2011.\nApolo: Interactive Large Graph Sensemaking by Combining Machine Learning\nand Visualization. In Proceedings of the 17th ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining (San Diego, California, USA) (KDD ’11).\nAssociation for Computing Machinery, New York, NY, USA, 739–742.\nhttps:\n//doi.org/10.1145/2020408.2020524\n[14] Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019.\nStructural Scaffolds for Citation Intent Classification in Scientific Publica-\ntions. ArXiv abs/1904.01608 (2019). https://api.semanticscholar.org/CorpusID:\n102483154\n[15] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. 2020.\nSPECTER: Document-level Representation Learning using Citation-informed\nTransformers. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics. Association for Computational Linguistics, Online,\n2270–2282. https://doi.org/10.18653/v1/2020.acl-main.207\n[16] Lynne M. Connelly. 2013. Grounded theory. Medsurg nursing : official journal of\nthe Academy of Medical-Surgical Nurses 22 2 (2013), 124, 127.\n[17] Raymond Fok, Hita Kambhamettu, Luca Soldaini, Jonathan Bragg, Kyle Lo, An-\ndrew Head, Marti A. Hearst, and Daniel S. Weld. 2022. Scim: Intelligent Skimming\nSupport for Scientific Papers. Proceedings of the 28th International Conference\non Intelligent User Interfaces (2022). https://api.semanticscholar.org/CorpusID:\n254591867\n[18] Dedre Gentner and Russell Landers. 1985. ANALOGICAL REMINDING: A GOOD\nMATCH IS HARD TO FIND.. In Unknown Host Publication Title. IEEE, 607–613.\n[19] Mary L Gick and Keith J Holyoak. 1980. Analogical problem solving. Cognitive\npsychology 12, 3 (1980), 306–355.\n[20] Mary L. Gick and Keith J. Holyoak. 1983. Schema induction and analogical\ntransfer. Cognitive Psychology 15, 1 (1983), 1 – 38. https://doi.org/10.1016/0010-\n0285(83)90002-6\n[21] John Giorgi, Luca Soldaini, Bo Wang, Gary Bader, Kyle Lo, Lucy Lu Wang, and\nArman Cohan. 2022. Towards multi-document summarization in the open-\ndomain. https://api.semanticscholar.org/CorpusID:258865156\n[22] Sandra G Hart and Lowell E Staveland. 1988. Development of NASA-TLX (Task\nLoad Index): Results of empirical and theoretical research. In Advances in psy-\nchology. Vol. 52. Elsevier, 139–183.\n[23] Andrew Head, Codanda Appachu, Marti A Hearst, and Björn Hartmann. 2015.\nTutorons: Generating context-relevant, on-demand explanations and demonstra-\ntions of online code. In 2015 IEEE Symposium on Visual Languages and Human-\nCentric Computing (VL/HCC). IEEE, 3–12.\n[24] Andrew Head, Kyle Lo, Dongyeop Kang, Raymond Fok, Sam Skjonsberg, Daniel S\nWeld, and Marti A Hearst. 2021. Augmenting scientific papers with just-in-time,\nposition-sensitive definitions of terms and symbols. In Proceedings of the 2021\nCHI Conference on Human Factors in Computing Systems. 1–18.\n[25] Paul Hemp. 2009. Death by information overload. Harvard business review 87 9\n(2009), 82–9, 121. https://api.semanticscholar.org/CorpusID:584292\n[26] Julian PT Higgins, Sally Green, et al. 2008. Cochrane handbook for systematic\nreviews of interventions. (2008).\n[27] Tom Hope, Joel Chan, Aniket Kittur, and Dafna Shahaf. 2017. Accelerating\nInnovation Through Analogy Mining. In Proceedings of the 23rd ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining (Halifax, NS,\nCanada) (KDD ’17). ACM, New York, NY, USA, 235–243. https://doi.org/10.1145/\n3097983.3098038\n[28] Hyeonsu Kang, Rafal Kocielnik, Andrew Head, Jiangjiang Yang, Matt Latzke,\nAniket Kittur, Daniel Weld S., Doug Downey, and Jonathan Bragg. 2022. From\nWho You Know to What You Read: Augmenting Scientific Recommendations\nwith Implicit Social Networks. (2022), 1–16.\n[29] Hyeonsu Kang, Sherry Tongshuang Wu, Joseph Chee Chang, and Aniket Kit-\ntur. 2023.\nSynergi: A Mixed-Initiative System for Scholarly Synthesis and\nSensemaking. In Proceedings of the 36th Annual ACM Symposium on User In-\nterface Software and Technology (San Francisco, CA, USA) (UIST ’23). Asso-\nciation for Computing Machinery, New York, NY, USA, 19 pages.\nhttps:\n//doi.org/10.1145/3586183.3606759\n[30] Hyeonsu B Kang, Joseph Chee Chang, Yongsung Kim, and Aniket Kittur. 2022.\nThreddy: An Interactive System for Personalized Thread-based Exploration and\nOrganization of Scientific Literature. Proceedings of the 35th Annual ACM Sympo-\nsium on User Interface Software and Technology (2022). https://api.semanticscholar.\norg/CorpusID:251402552\n[31] Hyeonsu B Kang, Sheshera Mysore, Kevin J Huang, Haw-Shiuan Chang, Thorben\nPrein, Andrew McCallum, Aniket Kittur, and Elsa Olivetti. 2022. Augmenting\nScientific Creativity with Retrieval across Knowledge Domains. In Second Work-\nshop on Bridging Human-Computer Interaction and Natural Language Processing\nat NAACL 2022. arXiv. https://doi.org/10.48550/ARXIV.2206.01328\n[32] Hyeonsu B. Kang, Xin Qian, Tom Hope, Dafna Shahaf, Joel Chan, and Aniket\nKittur. 2022. Augmenting Scientific Creativity with an Analogical Search Engine.\nACM Trans. Comput.-Hum. Interact. (mar 2022). https://doi.org/10.1145/3530013\nJust Accepted.\n[33] Hyeonsu B Kang, Nouran Soliman, Matt Latzke, Joseph Chee Chang, and Jonathan\nBragg. 2023. ComLittee: Literature Discovery with Personal Elected Author\nCommittees. In Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems (Hamburg, Germany) (CHI ’23). Association for Computing\nMachinery, New York, NY, USA, Article 738, 20 pages. https://doi.org/10.1145/\n3544548.3581371\n[34] Harmanpreet Kaur, Doug Downey, Amanpreet Singh, Evie Yu-Yen Cheng,\nDaniel S. Weld, and Jonathan Bragg. 2022. FeedLens: Polymorphic Lenses for\nPersonalizing Exploratory Search over Knowledge Graphs (UIST ’22).\n[35] Yoonjoo Lee, Tae Soo Kim, Sungdong Kim, Yohan Yun, and Juho Kim. 2023.\nDAPIE: Interactive Step-by-Step Explanatory Dialogues to Answer Children’s\nWhy and How Questions. In Proceedings of the 2023 CHI Conference on Human\nFactors in Computing Systems. 1–22.\n[36] Kyle Lo, Joseph Chee Chang, Andrew Head, Jonathan Bragg, Amy X. Zhang, Cas-\nsidy Trier, Chloe Anastasiades, Tal August, Russell Authur, Danielle Bragg, Erin\nBransom, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Yen-Sung\nChen, Evie (Yu-Yen) Cheng, Yvonne Chou, Doug Downey, Rob Evans, Raymond\nFok, F.Q. Hu, Regan Huff, Dongyeop Kang, Tae Soo Kim, Rodney Michael Kinney,\nAniket Kittur, Hyeonsu B Kang, Egor Klevak, Bailey Kuehl, Michael Langan,\nMatt Latzke, Jaron Lochner, Kelsey MacMillan, Eric Stuart Marsh, Tyler Murray,\nAakanksha Naik, Ngoc-Uyen Nguyen, Srishti Palani, Soya Park, Caroline Paulic,\nNapol Rachatasumrit, Smita R Rao, Paul L Sayre, Zejiang Shen, Pao Siangliulue,\nLuca Soldaini, Huy Tran, Madeleine van Zuylen, Lucy Lu Wang, Christopher Wil-\nhelm, Caroline M Wu, Jiangjiang Yang, Angele Zamarron, Marti A. Hearst, and\nDaniel S. Weld. 2023. The Semantic Reader Project: Augmenting Scholarly Docu-\nments through AI-Powered Interactive Reading Interfaces. ArXiv abs/2303.14334\n\nPaperWeaver: Enriching Paper Alerts by Contextualizing Descriptions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\n(2023). https://api.semanticscholar.org/CorpusID:257766269\n[37] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Michael Kinney, and Daniel S.\nWeld. 2020. S2ORC: The Semantic Scholar Open Research Corpus. In Annual Meet-\ning of the Association for Computational Linguistics. https://api.semanticscholar.\norg/CorpusID:215416146\n[38] Kelvin Luu, Xinyi Wu, Rik Koncel-Kedziorski, Kyle Lo, Isabel Cachola, and\nNoah A. Smith. 2020. Explaining Relationships Between Scientific Documents.\nIn Annual Meeting of the Association for Computational Linguistics.\nhttps:\n//api.semanticscholar.org/CorpusID:236459799\n[39] Sonia K. Murthy, Kyle Lo, Daniel King, Chandra Bhagavatula, Bailey Kuehl,\nSophie Johnson, Jon Borchardt, Daniel S. Weld, Tom Hope, and Doug Downey.\n2022. ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions\nof Scientific Concepts. ArXiv abs/2205.06982 (2022). https://api.semanticscholar.\norg/CorpusID:248811750\n[40] Sheshera Mysore, Timothy J. O’Gorman, Andrew McCallum, and Hamed Za-\nmani. 2021. CSFCube - A Test Collection of Computer Science Research Ar-\nticles for Faceted Query by Example.\nArXiv abs/2103.12906 (2021).\nhttps:\n//api.semanticscholar.org/CorpusID:232335540\n[41] Preslav I Nakov, Ariel S Schwartz, Marti Hearst, et al. 2004. Citances: Citation\nsentences for semantic analysis of bioscience text. In Proceedings of the SIGIR,\nVol. 4. Citeseer, 81–88.\n[42] Elisabeth Pain. 2016. How to keep up with the scientific literature. Science (2016).\nhttps://api.semanticscholar.org/CorpusID:158399837\n[43] Srishti Palani, Aakanksha Naik, Doug Downey, Amy X Zhang, Jonathan Bragg,\nand Joseph Chee Chang. 2023. Relatedly: Scaffolding Literature Reviews with\nExisting Related Work Sections. arXiv preprint arXiv:2302.06754 (2023).\n[44] Haekyu Park, Gonzalo A. Ramos, Jina Suh, Christopher Meek, Rachel Ng, and\nMary Czerwinski. 2023. FoundWright: A System to Help People Re-find Pages\nfrom Their Web-history. ArXiv abs/2305.07930 (2023). https://api.semanticscholar.\norg/CorpusID:258685533\n[45] Savvas Petridis, Nicholas Diakopoulos, Kevin Crowston, Mark Hansen, Keren\nHenderson, Stan Jastrzebski, Jeffrey V Nickerson, and Lydia B Chilton. 2023.\nAngleKindling: Supporting Journalistic Angle Ideation with Large Language\nModels. In Proceedings of the 2023 CHI Conference on Human Factors in Computing\nSystems (<conf-loc>, <city>Hamburg</city>, <country>Germany</country>,\n</conf-loc>) (CHI ’23). Association for Computing Machinery, New York, NY,\nUSA, Article 225, 16 pages. https://doi.org/10.1145/3544548.3580907\n[46] Antoine Ponsard, Francisco Escalona, and Tamara Munzner. 2016. PaperQuest:\nA visualization tool to support literature review. In Proceedings of the 2016 CHI\nConference Extended Abstracts on Human Factors in Computing Systems. 2264–\n2271.\n[47] Jason Portenoy, Marissa Radensky, Jevin D West, Eric Horvitz, Daniel S Weld,\nand Tom Hope. 2022. Bursting Scientific Filter Bubbles: Boosting Innovation via\nNovel Author Discovery. In Proceedings of the 2022 CHI Conference on Human\nFactors in Computing Systems (New Orleans, LA, USA) (CHI ’22). Association\nfor Computing Machinery, New York, NY, USA, Article 309, 13 pages.\nhttps:\n//doi.org/10.1145/3491102.3501905\n[48] Napol Rachatasumrit, Jonathan Bragg, Amy X Zhang, and Daniel S Weld. 2022.\nCiteRead: Integrating Localized Citation Contexts into Scientific Paper Reading.\nIn 27th International Conference on Intelligent User Interfaces. 707–719.\n[49] Sherry Ruan, Jacob O. Wobbrock, Kenny Liou, Andrew Ng, and James A. Landay.\n2018. Comparing Speech and Keyboard Text Entry for Short Messages in Two\nLanguages on Touchscreen Phones. Proc. ACM Interact. Mob. Wearable Ubiquitous\nTechnol. 1, 4, Article 159 (jan 2018), 23 pages.\n[50] Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. 2023.\nLaMP: When Large Language Models Meet Personalization. arXiv preprint\narXiv:2304.11406 (2023).\n[51] Amanpreet Singh, Mike D’Arcy, Arman Cohan, Doug Downey, and Sergey Feld-\nman. 2022. SciRepEval: A Multi-Format Benchmark for Scientific Document\nRepresentations. ArXiv abs/2211.13308 (2022).\n[52] Nicole Sultanum, Christine Murad, and Daniel Wigdor. 2020. Understanding and\nsupporting academic literature review workflows with litsense. In Proceedings of\nthe International Conference on Advanced Visual Interfaces. 1–5.\n[53] H Holden Thorp. 2023. ChatGPT is fun, but not an author. , 313–313 pages.\n[54] Joseph Jay Williams, Juho Kim, Anna Rafferty, Samuel Maldonado, Krzysztof Z\nGajos, Walter S Lasecki, and Neil Heffernan. 2016. Axis: Generating explanations\nat scale with learnersourcing and machine learning. In Proceedings of the Third\n(2016) ACM Conference on Learning@ Scale. 379–388.\n[55] Chris Woolston. 2019. PhDs: the tortuous truth. Nature 575 (2019), 403 – 406.\nhttps://api.semanticscholar.org/CorpusID:207986664\n[56] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023.\nC-Pack: Packaged Resources To Advance General Chinese Embedding.\narXiv:2309.07597 [cs.CL]\nA\nPROMPTS\nAll prompts used in PaperWeaver are listed below. The blue text\nrepresents the input content.\nA.1\n[T1] Generating Folder Description\nSystem Prompt You are an intelligent and precise assistant\nthat can understand the contents of research papers. You\nare knowledgeable in different fields and domains of\nscience, in particular computer science.\nUser Prompt This is my scholarly library, titled folder\ntitle. The following papers are included. Write down\ntwo-line descriptions about this library that deal\nwith high-level characteristics of these works commonly\nshared. Present the result as \"Title: <given title>;\nDescription: <two-line descriptions starting with \"It\nencompasses\">.\n[Library papers]\nA set of titles of library papers\nA.2\n[T2] Generating Contextualized\nAspect-based Paper Summaries\nSystem Prompt You are an intelligent and precise assistant\nthat can understand the contents of research papers.\nYou are knowledgeable in different fields and domains\nof science, in particular computer science. You are\nable to interpret research papers based on the user’s\nperspective.\nUser Prompt We would like you to extract the dimensions\nof the paper based on my research interest. You will\nbe given my research interest and a paper and will be\nasked to extract the problem, method, and findings that\nI might have interest in from the paper. You will be\nprovided with the title and abstract of the paper and\nmy research interest that describes the topics that I’m\ncurrently interested in.\n[The Start of My Research Interest]\nfolder description\n[The End of My Research Interest]\n[The Start of Given Paper]\nTitle: title\nAbstract:abstract\n[The End of Given Paper]\n[System]\nPlease identify as many relevant aspects from the paper\nwith respect to any research problems in the topic of\nfolder title. Once you identified the research problems,\ndescribe what specific methods the following paper is\napplying for each of the problems. Each method from the\npaper should resolve the matched problem and they should\nbe specific, which means not widely used. Once you\nidentified the methods, describe what specific findings\nthe following paper identified by applying each of the\n\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nLee et al.\nmethods.\nFinally, return a result as Python dictionary object of\nthe following format: \"[{\"Problem\": <problem composed\nof 20-word long phrase>, \"Method\": <method composed of\n20-word long phrase>, \"Findings\": <findings composed\nof 20-word long phrase>}, ..]\". If there is no specific\nmethod to resolve the problem, then write down \"N/A\".\nA.3\n[T3] Generating Paper-paper Descriptions\nBased on Citances\nSystem Prompt You are an intelligent and precise assistant\nthat can understand the contents of research papers. You\nare knowledgeable in different fields and domains of\nscience, in particular computer science. You are able\nto interpret research papers to identify similarities\nand differences between research papers.\nUser Prompt We would like you to compare two research\npapers for a researcher. You will be provided with the\ntitle and abstract of each paper. To help you when you\ncompare the papers, we provided a subsection of Paper\nA where Paper B is cited. In the subsection of Paper\nA, cited Paper B already identified methods that are\nsimilar between the papers and what problems are solved\nin each paper using these shared methods.\n[The Start of Paper A]\nTitle: title\nAbstract:abstract\nSubsection of Paper A: subsection of Paper A where Paper\nB is cited\n[The End of Paper A]\n[The Start of Paper B]\nTitle: title\nAbstract:abstract\n[The End of Paper B]\n[System]\nPlease explain the content of Paper A for a researcher.\nExplain the paper by comparing it to Paper B, and\ninterpreting the relationships between these papers.\nYour explanation should only be four sentences long and\nit should follow the following structure: a sentence\nthat states what aspects are similar between Paper A and\nPaper B, one sentence summary of Paper A, one sentence\nsummary of Paper B, and one sentence comparing and\ncontrasting between Paper A and B.\nA.4\n[T4] Paper-paper Descriptions via\nGenerated Pseudo-citances - Finding similar\nproblem aspects across the recommended\nand collected papers using LLM\nSystem Prompt You are an intelligent and precise assistant\nthat can understand the contents of research papers. You\nare knowledgeable in different fields and domains of\nscience, in particular computer science. You are able\nto interpret research papers to identify similarities\nand differences between research papers.\nUser Prompt\nWe would like you to examine a set of\npapers. You will be given a paper and will be asked to\ncompare this paper to a list of papers labeled A, B, C,\nand D. You will be provided with the title of each paper\nand a set of dimensions that describe the content of\nthe paper. These dimensions describe different problems\nthat were addressed by the paper, the method applied\nin the paper to address each problem, and findings\nrelated to that problem and method. These dimensions\nare provided in a Python JSON format.\n[The Start of Given Paper]\nTitle: title\nDimensions:dimensions\n[The End of Given Paper]\n[The Start of Paper A]\nTitle: title\nDimensions:dimensions\n[The End of Paper A]\n[The Start of Paper B]\nTitle: title\nDimensions:dimensions\n[The End of Paper B]\n[The Start of Paper C]\nTitle: title\nDimensions:dimensions\n[The End of Paper C]\n[The Start of Paper D]\nTitle: title\nDimensions:dimensions\n[The End of Paper D]\n[System]\nPlease compare the problems of the given paper with the\nproblems of the other listed papers. Please identify\npapers in the list that have problems that are the\nmost similar with problems in the given paper. Focus\non identifying problems that are similar even though\nthey may be resolved with different types of methods.\nList all of the identified pairs of similar problems,\nwhere one problem is from the given paper and the other\nis a similar problem from another paper in the list.\nFor each pair, please describe one shared problem that\ncould contain the two problems. You should avoid just\nsimply concatenating two problems when describing a\nshared problem. Also, you should avoid containing a\nphrase that is only included in one of the papers even\nthough it is a very small part.\n\nPaperWeaver: Enriching Paper Alerts by Contextualizing Descriptions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\nFinally, return the list of pairs of similar problems\nand the shared problem for each pair as a list in a\nPython JSON object of the following format: \"[{\"chosen_paper\":\n<title of the paper that has a problem that is similar\nto one in the given paper>, \"similar_problem\": <problem\nthat is similar to a problem in the given paper>,\n\"given_problem”: <problem from given paper that is\nsimilar to the identified problem>, \"shared_problem\":\n<one challenge that can encompass the two similar problems>},\n..]\". You should ensure that you return a valid JSON\nobject by escaping any quote marks in your output.\n(Example:\n{\"valid_object\":\n\"This\nis\na\n\"valid\"\nJSON\nobject that escapes any ¨characters.\"}) If there were no\npapers that share common problems with the given paper,\nthen only write down \"N/A\".\nA.5\n[T5] Paper-paper Descriptions via\nGenerated Pseudo-citances - Verifying\nwhether shared problem is aligned with\neach paper\nSystem Prompt You are an intelligent and precise assistant\nthat can understand the contents of research papers. You\nare knowledgeable in different fields and domains of\nscience, in particular computer science. You are able\nto interpret research papers to identify similarities\nand differences between research papers.\nUser Prompt\nYou will be provided with the title and\nabstract of Paper A and the given problem.\n[Title of Paper A]\ntitle\n[The End of the title]\n[Abstract of Paper A]\nabstract\n[The End of the title]\n[The Start of Given Problem]\nshared problems from paper A and B\n[The End of Given Problem]\n[System]\nPlease verify whether Paper A tackled the given problem\nbased on the abstract of the paper. Provide the result\nas True if Paper A tackled the given problem with their\nown method, else provide False. If the part of the given\nproblem is not aligned with Paper A’s challenges, it\nshould be verified as False.\nA.6\n[T6] Paper-paper Descriptions via\nGenerated Pseudo-citances - Generating\nstructured summary\nSystem Prompt You are an intelligent and precise assistant\nthat can understand the contents of research papers. You\nare knowledgeable in different fields and domains of\nscience, in particular computer science. You are able\nto interpret research papers to identify similarities\nand differences between research papers.\nUser Prompt We would like you to compare two research\npapers for a researcher. You will be provided with\nthe title of each paper and a set of dimensions that\ndescribe the content of the paper. These dimensions\ndescribe\ndifferent\nproblems\nthat\nwere\naddressed\nby\nthe paper, the method taken by the paper to address\neach problem, and findings related to that problem\nand method. These dimensions are provided in a Python\ndictionary format. To help you when you compare the\npapers, we have already identified problems that are\nsimilar between the papers and what methods are adopted\nin each paper to solve the shared problem.\n[The Start of Paper A]\nTitle: title\nDimensions:dimensions\n[The End of Paper A]\n[The Start of Paper B]\nTitle: title\nDimensions:dimensions\n[The End of Paper B]\n[The Start of Shared Problems]]\nshared problem addressed in Paper A and B\n[The End of Shared Problem]\n[The Start of Methods]\nPaper A: method that is used in Paper A to resolve the\naligned problem\nPaper B:method that is used in Paper B to resolve the\naligned problem\n[The End of Methods]\n[The Start of Research interest]\nfolder description\n[The End of Research Interest]\n[System]\nPlease explain the content of Paper A for a researcher.\nExplain the paper by comparing it to Paper B, and\ninterpreting the similarities and differences between\nthese papers. You should consider the researchers’ research\ninterest, which is described above when explaining\nPaper A. Ensure that your explanation includes information\nthat may be fascinating or engaging for the researcher\nbased on their interests. Your explanation should only\nbe four sentences long and it should follow the following\nstructure: a sentence that states what aspects are\nsimilar between Paper A and Paper B, one sentence\nsummary of Paper A, one sentence summary of Paper B, and\none sentence comparing and contrasting between Paper A\nand B.",
    "pdf_filename": "PaperWeaver - Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers.pdf"
}