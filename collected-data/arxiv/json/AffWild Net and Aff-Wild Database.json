{
    "title": "AffWild Net and Aff-Wild Database",
    "context": "",
    "body": "IMPERIAL COLLEGE LONDON\nDEPARTMENT OF COMPUTING\nAff-wild Database and Aff-wild Net\nAuthor:\nAlvertos Benroumpi\nSupervisor:\nDimitrios Kollias\nSubmitted in partial fulﬁllment of the requirements for the MSc degree in\nComputing Science / Machine Learning of Imperial College London\nSeptember 2018\narXiv:1910.05376v2  [cs.LG]  13 Dec 2019\n\nAbstract\nEmotions recognition is the task of recognizing people’s emotions.\nUsually it is\nachieved by analyzing expression of peoples faces. There are two ways for repre-\nsenting emotions: The categorical approach and the dimensional approach by using\nvalence and arousal values. Valence shows how negative or positive an emotion\nis and arousal shows how much it is activated. Recent deep learning models, that\nhave to do with emotions recognition, are using the second approach, valence and\narousal. Moreover, a more interesting concept, which is useful in real life is the ”in\nthe wild” emotions recognition. ”In the wild” means that the images analyzed for\nthe recognition task, come from from real life sources(online videos, online photos,\netc.) and not from staged experiments. So, they introduce unpredictable situations\nin the images, that have to be modeled.\nThe purpose of this project is to study the previous work that was done for the ”in\nthe wild” emotions recognition concept, design a new dataset which has as a stan-\ndard the ”Aff-wild” database, implement new deep learning models and evaluate the\nresults. First, already existing databases and deep learning models are presented.\nThen, inspired by them a new database is created which includes 507.208 frames\nin total from 106 videos, which were gathered from online sources. Then, the data\nare tested in a CNN model based on CNN-M architecture, in order to be sure about\ntheir usability. Next, the main model of this project is implemented. That is a Re-\ngression GAN which can execute unsupervised and supervised learning at the same\ntime. More speciﬁcally, it keeps the main functionality of GANs, which is to produce\nfake images that look as good as the real ones, while it can also predict valence and\narousal values for both real and fake images. Finally, the database created earlier is\napplied to this model and the results are presented and evaluated.\n\nii\n\nContents\n1\nIntroduction\n1\n1.1\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nAims . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.3\nOutcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2\nBackground and Related Work\n3\n2.1\nEmotions Representation . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.1.1\nCategorical Approach\n. . . . . . . . . . . . . . . . . . . . . .\n3\n2.1.2\nDimensional Approach . . . . . . . . . . . . . . . . . . . . . .\n3\n2.2\nExisting Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2.1\nSEMAINE . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2.2\nRECOLA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2.3\nSEWA Database . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2.4\nAFEW-VA Database . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2.5\nThe Aff-Wild Database . . . . . . . . . . . . . . . . . . . . . .\n6\n2.3\nNeural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.3.1\nCNN (Convolutional Neural Network)\n. . . . . . . . . . . . .\n7\n2.3.2\nRNN (Recurrent Neural Network) . . . . . . . . . . . . . . . .\n9\n2.3.3\nGANs (Generative adversarial networks) . . . . . . . . . . . .\n11\n2.3.4\nDCGAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.3.5\nSSGAN\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.3.6\nCapsule networks . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n2.3.7\nCapsuleGAN\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n2.4\nArchitecture of AffWildNet . . . . . . . . . . . . . . . . . . . . . . . .\n17\n2.4.1\nVGG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n2.4.2\nResNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n2.4.3\nCNN-RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.4.4\nCNN-M\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.4.5\nResults Of The Above Architectures . . . . . . . . . . . . . . .\n20\n3\nDataset\n22\n3.1\nKind of data needed\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3.2\nData gathering\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3.3\nData pre-processing . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3.4\nData annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n3.5\nExtra data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\niii\n\nCONTENTS\nTable of Contents\n3.6\nFace detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.7\nAnnotations to labels conversion\n. . . . . . . . . . . . . . . . . . . .\n30\n3.8\nFinal dataset\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4\nNeural Network Models\n33\n4.1\nLoss function for regression . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.2\nData Split . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.2.1\nTesting Set\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.2.2\nTraining With Small Dataset . . . . . . . . . . . . . . . . . . .\n34\n4.2.3\nTraining With Big Dataset . . . . . . . . . . . . . . . . . . . .\n34\n4.3\nTesting The Database In A CNN . . . . . . . . . . . . . . . . . . . . .\n35\n4.4\nRegression GAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.4.1\nFormat of Images . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.4.2\nGenerator . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n4.4.3\nDiscriminator . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n4.4.4\nLoss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n4.4.5\nTraining Tricks . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n5\nTechnical Procedure\n41\n5.1\nPre-processing Scripts\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n5.2\nTensorFlow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n5.3\nInput Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n5.4\nCode Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n5.5\nTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n5.6\nTesting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n5.7\nEvaluating Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n6\nEvaluation of Results\n45\n6.1\nCNN-M\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n6.2\nRegression GAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n6.2.1\nLoss Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n6.2.2\nImage Generation Evaluation . . . . . . . . . . . . . . . . . .\n50\n6.2.3\nSemi-Supervised Learning Evaluation . . . . . . . . . . . . . .\n51\n7\nConclusion and Future Work\n54\n7.1\nDataset Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n7.2\nNeural Networks Summary\n. . . . . . . . . . . . . . . . . . . . . . .\n54\n7.3\nFuture Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\nAppendices\n61\nA\nLSEPI Checklist\n62\niv\n\n\nChapter 1\nIntroduction\n1.1\nMotivation\nThe past twenty years, the research in facial behavior analysis was limited to\ndata that were speciﬁcally created for this scope, with experiments that contained\ndata recorded in highly controlled situations. Moreover, most of these experiments\nwhich are trying to represent humans emotions, are based on categorical emotion\nrepresentation, which includes the seven basic categories, i.e. Anger, Disgust, Fear,\nHappiness, Sadness, Surprise and Neutral. However, the dimensional emotion rep-\nresentation is more appropriate to represent the whole range of humans emotions.\nThe most usual dimensional emotion representation, which is used on this project,\nis the 2-D Valence and Arousal space. In order to make good use of this emotional\nrange, various facial expressions have to be analyzed from videos ”in the wild”. More\nspeciﬁcally, these will be videos, not made speciﬁcally for emotion analysis, which\ninclude people’s faces reacting to something.\n1.2\nAims\nFor the ﬁrst part of the project, the aim is to create a new ”in the wild” dataset,\nusing web shared videos as source. The whole procedure of creating the dataset\nhas to be followed, by using as guidance the [1] paper.\nThat includes the pre-\nprocessing of the data, the face detection [2] and the annotation. Hopefully, all the\nrange of emotions will be covered from this dataset in terms of valence and arousal.\nBefore moving to the second part of the project, previous work on applying this\ntype of data to deep learning models will be studied. Moreover, state of the art\nnetworks such as GANs and Capsule networks will be also studied. Finally, it will\nbe decided which model is going to be implemented with the new dataset and the\nresults will be evaluated by comparing them with the results of previous works.\n1\n\n1.3. OUTCOMES\nChapter 1. Introduction\n1.3\nOutcomes\nThe ﬁnal dataset consists of 507.208 frames gathered from 106 videos and its\nsize is approximately 48GB. These frames includes approximately 150 faces of dif-\nferent people. The distribution of valence and arousal values was good and it did\nnot leave uncovered values. However, the annotation was carried by one person and\nthat makes the annotations biased. Hence, the results of the models may be a little\nbetter than the normal.\nFor the deep learning part of the project, it was decided to implement a standard\nCNN network just to test the data at ﬁrst. However, The basic implementation of\nthe project is a semi-supervised GAN that generate images and performs emotions\nprediction at the same time. Although that the results were acceptable, there are\nmany improvements that can be made to the model, in order to perform better both\nin terms of quality of generated images and in the accuracy of the regression.\n2\n\nChapter 2\nBackground and Related Work\n2.1\nEmotions Representation\n2.1.1\nCategorical Approach\nThe most common approach to classify emotions is the one proposed in [3].\nAccording to P. Ekman, based on facial expressions, emotions can be categorized in\nas the following:\n• Anger\n• Disgust\n• Fear\n• Happiness\n• Sadness\n• Surprise\nThis way of representing emotions has been used in many relevant projects until\nnow [4]. However, people’s emotions is something complex and a classiﬁcation with\nonly six classes cannot be entirely realistic about the real emotion that a person\nexpresses. A broader range of emotions would be preferable, in order for people’s\nemotions to be classiﬁed better.\n2.1.2\nDimensional Approach\nThe ﬁrst concept that was studied and is necessary for this project is Valence and\nArousal. The most usual dimensional emotion representation is the 2-D Valence and\nArousal space, which in contrary with categorical representation, it can represent a\nmuch bigger range of emotions. Valence and arousal are describing the reaction of\na person in an event by taking values in a 2-D space. Valence ranges from highly\n3\n\n2.1. EMOTIONS REPRESENTATION\nChapter 2. Background and Related Work\npositive to highly negative, whereas arousal ranges from calming or soothing to ex-\nciting or agitating. Hence, these two values are rating someone’s reaction according\nto how positive or negative and how intense this reaction is. The graph below gives\na better understanding of these two values [5]:\nFigure 2.1: Example 2D dimensional space of valence-arousal.\nsource: https://www.pinterest.com/pin/354588170647498721/\nIf one thinks valence and arousal as a measure of someone’s emotional condi-\ntion, then it would be easy to think that these two values can be connected with\none’s face expressions. More speciﬁcally, the idea is that people tend to have similar\nface expressions when they want to express particular emotions. Hence, a pattern\nbetween this face-expression - emotion is created [6], which it may can be decoded\nwith a machine learning model [7, 8], by having images as input and valence and\narousal as output.\n4\n\nChapter 2. Background and Related Work\n2.2. EXISTING DATABASES\n2.2\nExisting Databases\n2.2.1\nSEMAINE\nIn paper [9], SEMAINE has created a large audiovisual database as part of\nan iterative approach to building agents that can engage a person in a sustained,\nemotionally colored conversation, using the Sensitive Artiﬁcial Listener (SAL) [10]\nparadigm. The database consists of 150 participants who were undergraduate and\npostgraduate students, for a total of 959 conversations with individual SAL charac-\nters, lasting approximately 5 minutes each. Videos are recorded at 49.979 frames\nper second and at a spatial resolution of 780 x 580 pixels.\nFor the annotation part, SEMAINE members decided to include 5 dimensions\nwhich are well established in the psychological literature. These dimensions are\nValence, Activation, Power, Expectation and Intensity. Valence and Activation(as\nArousal) are described above, but the other dimensions are not introduced yet. The\npower dimension combines the power and control concepts of people and it depends\non what they are facing. Anticipation/Expectation also subsumes various concepts\nthat can be separated - expecting, anticipating, being taken unawares. Intensity, is\nabout the distance of a person from a completely calm state.\nAfter the annotator had rated the data in terms of the ﬁve basic dimensions,\nhe/she had to choose between 4 categories of labels to rate. These categories are\nBasic Emotions, Epistemic states, Interaction Process Analysis and Validity. Basic\nemotions are the following 7 emotions: Fear, Anger, Happiness, Sadness, Disgust,\nContempt, Amusement. Epistemic states are states that can occur in people’s interac-\ntion (e.g. Certain / not certain, Agreeing / not agreeing, Interested / not interested).\nInteraction Process Analysis includes behavior that people show in a dialogue (e.g.\nSolidarity, Antagonism, Tension). Finally, Validity is a set of labels that shows if there\nis a contradiction between the feelings expressed from the participant and his/her\nreal feelings.\n2.2.2\nRECOLA\nIn paper [11], RECOLA is introduced, which stands for REmote COLlaborative\nand Affective interactions. For the construction of this database, participants were\nrecorded in dyads during a video conference while completing a task requiring col-\nlaboration. Participants (mean age: 22 years, standard deviation: 3 years) were in\ntotal 46 (27 females, 19 males) and they were recruited as 23 dyadic teams work.\nAll subjects are French speaking while having different mother tongues: 33 are orig-\ninally French speaking, 8 Italian, 4 German and 1 Portuguese. According to the\nself-reports ﬁlled by the users, only 20% of participants knew well their teammate.\nThis database is annotated in terms of valence and arousal. During the proce-\ndure, various techniques of mood induction and emotion manipulation were used, in\norder to have a broader range of annotated data. Annotators used the tool developed\nfor RECOLA project, called ANNEMO. In order to be familiar with this tool before\nannotating the database, they ﬁrst rated 2 sequences from SEMAINE database.\n5\n\n2.2. EXISTING DATABASES\nChapter 2. Background and Related Work\nIn order to keep and annotate only the interesting part of the recordings, only\nthe ﬁrst ﬁve minutes were kept for each one. Hence, from more than 9.5 hours of\nrecordings, a reduced set of 3.8 hours was obtained.\n2.2.3\nSEWA Database\nThe SEWA database [12] is a set of visual and audio content, that was collected\nfor the SEWA project. People participated in this project are from six different coun-\ntries with a broad range of ages. At the registration of the project, a form with\ndemographic measures was fulﬁlled by the participants. For the data collection, vol-\nunteers have to complete two experiments and they are separated based on their\ncultural background, age and gender.\nFor the ﬁrst experiment, participants had to watch four adverts which were\nsupposed to trigger emotions including amusement, empathy, liking and boredom.\nAfter watching the adverts, they had to complete a questionnaire about their real\nemotional state while watching the adverts.\nFor the second experiment, volunteers had to do an online web-chat with an-\nother volunteer about the last advert they watched. More speciﬁcally, it was sup-\nposed to be a general discussion about their opinion for the advert and the product\nthat was advertised. As before, after the discussion they had to complete a question-\nnaire about their emotional state during the conversation.\nThe ﬁnal database after the above experiments, contained 199 sessions of record-\nings, with a total of 2.075 minutes of recorded data, from 398 individual people.\n2.2.4\nAFEW-VA Database\nAFEW-VA database [13] contains data that are coming from ﬁlms. More specif-\nically 600 or more videos were extracted, with their length varying from short(10\nframes) to long(more than 120 frames) videos. Moreover, the videos include scenes\nwith challenging image conditions, which considering the background of the image\nor the people that are included.\nThe way that the annotation procedure was carried out in AFEW-VA database,\nmade the annotations to be very detailed and very accurate. That’s because the\nannotators were 2 experts, who had to annotate the data per frame with values in\nthe range of [-10, 10]. When the annotators did not agree about the values, they\nhad a discussion about the content they were watching and ﬁnally they were coming\nup with a unique solution.\n2.2.5\nThe Aff-Wild Database\nAs mentioned above, except from the databases described before, there is one\nmore existing database consisting of videos with more than 30 hours length. These\nvideos were collected from YouTube and they were chose with various criteria. The\ncommon key-word that was used was the word ”reaction”. These videos consist\nof people reacting to different things, such as unexpected movie plots, activities or\n6\n\nChapter 2. Background and Related Work\n2.3. NEURAL NETWORKS\njokes [1, 14, 15]. This database has been already annotated in terms of valence and\narousal, which are the two dimensions described before. The aim is to ﬁnd more\nsimilar videos and enrich the already existing database by covering the whole range\nof valence and arousal values, in order to improve the performance of the predictive\nmodel.\nFigure 2.2: Some representative frames from aff-wild database.\nsource: [1, 14, 15]\nAnnotation of Aff-Wild Database\nFor the annotation part, a software has already been implemented. This soft-\nware gives the ability to continuously annotate a video, as time passes, for valence\nand arousal individually, which is less demanding for the annotator, so the annota-\ntion quality may be higher. The annotator follows the annotation instructions and\nuses a joystick in order to perform the labeling. The instructions include examples of\nwell identiﬁed frames with their valence and arousal values. Moreover, in the exist-\ning database, some data pre-processing and post-processing procedures have been\nimplemented, which are analytically described in [1, 14, 15].\n2.3\nNeural Networks\n2.3.1\nCNN (Convolutional Neural Network)\nConvolution Neural Networks, know as CNN, are widely used for image pro-\ncessing tasks after 2012 [16, 17, 18, 19, 20, 21]. Big companies such as Facebook,\nAmazon and Google [22, 23] are using CNNs for various services that require deep\nlearning solutions. CNNs are going to be used in this project because the videos in\nour database consist of frames, namely images.\nAn image consists of pixels, so an image-input is basically an array of pixels. If\nwe had an image 320x320, then we would have an array 320x320x3(in RGB format)\nas input. An overview of the structure of CNNs would be that we give the input,\nCNN pass it through a series of convolutional, nonlinear, pooling(downsampling),\nand fully connected layers and ﬁnally we get an output [24]. A high level descrip-\ntion for the convolutional layers would be that they work as feature identiﬁers. For\nevery layer there is a ﬁlter of a particular size that scans the whole image and re-\nturns an activation map. By creating a series of convolutional, ReLU or pool layers\nwe basically detect high level features of the image such as curves, edges, etc. The\noutput(activation map) of these layers is the input for a fully connected layer, which\nreturns the ﬁnal output of our model, as usual.\n7\n\n2.3. NEURAL NETWORKS\nChapter 2. Background and Related Work\nFigure 2.3: How a CNN works.\nsource: https://www.mathworks.com/discovery/convolutional-neural-network.html\nMore speciﬁcally, when a new image is presented to the network, it is unknown\nwhere the features are. This is where convolution layers are introduced. For each\nfeature, a ﬁlter is created which scans the whole image and it creates a map which\nindicates in which area of the image the feature has a strong appearance. This is\nachieved by applying convolutions between the ﬁlter and the image areas. Convolu-\ntion layers result to a set of ﬁltered images, that will be fed to the next layers.\nAnother common tool that is used in CNNs is pooling layers. For every image in\nthe set of ﬁltered images that resulted above, the pooling layer is reducing its size. It\nis actually a small window that scans the image with a speciﬁc step and for the set of\npixels that are inside the window, it returns the maximum pixel value (maxpooling).\nIt ﬁnally returns a set of images, with the same number of images as before, but their\nsize is smaller.\nThe activation function that is commonly used between a set of convolution-\npooling layers is called ReLU [25]. ReLU is a very simple mathematical operation\nthat substitutes every pixel of the image which has a negative value with zero. This\nhelps the CNN stay mathematically healthy by keeping learned values from getting\nstuck near 0 or blowing up toward inﬁnity.\nEither for a regression or classiﬁcation task, the last step of the CNNS are the\nfully connected layers. These layers treats the features input as a list and they are\nmatching each feature with its most probable class. Their output size is the number\nof features or classes that they have to predict.\n8\n\nChapter 2. Background and Related Work\n2.3. NEURAL NETWORKS\nFigure 2.4: Result after convolution\nlayer.\nFigure 2.5: Result after maxpooling\nlayer.\nFigure 2.6: Result after ReLU activa-\ntion.\nFigure 2.7:\nResult after fully con-\nnected layer (for images of X and O\nclassiﬁcation).\nsource: https://brohrer.github.io/how convolutional neural networks work.html\n2.3.2\nRNN (Recurrent Neural Network)\nAs described in [26], Recurrent Neural Networks (RNN) are networks with\nloops in them, allowing information to persist. RNNs share parameters across dif-\nferent positions/ index of time/ time steps of the sequence, which makes it possible\nto generalize well to examples of different sequence length. RNN is usually a better\nalternative to position-independent classiﬁers and sequential models that treat each\nposition differently.\nIn the last few years, they are broadly used in projects, such as speech recog-\nnition and language modeling, which include sequential data and time-series. For\nthis project, the ability of RNNs to keep information may be useful by using previous\nvideo frames to help the understanding of a present frame.\n9\n\n2.3. NEURAL NETWORKS\nChapter 2. Background and Related Work\nFigure 2.8: An unrolled recurrent neural network.\nsource: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\nLSTM (Long Short Term Memory)\nFor many tasks like these there is also a special kind of RNN , the LSTM. LSTMs\nare explicitly designed to avoid the long-term dependency problem. As it is analyt-\nically described in [26], they work as ﬁlters of the previous state of the NN, which\nhave the ability to remove or add information to the cell state, carefully regulated\nby structures called gates. LSTMs are very useful and they have contributed in the\nsuccess of RNNs, because they can keep the information that passes from state to\nstate clean, even for a long sequence of data.\nFigure 2.9: The structure of a LSTM cell.\nsource: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\nGRU (Gated Recurrent Unit)\nGRU can be considered as a variation on the LSTM because they have similar\ndesign and, in some cases, produce equally excellent results. In order to solve the\nvanishing gradient problem of a standard RNN, GRU uses two gates, the update and\nthe reset gate. By using these gates, GRUs are able to store and ﬁlter information.\n10\n\nChapter 2. Background and Related Work\n2.3. NEURAL NETWORKS\nHence, the vanishing gradient problem is not present anymore, because the model\nis not washing out the new input every single time but keeps the relevant informa-\ntion and passes it down to the next time steps of the network. If GRUs get trained\ncarefully, they can have an outstanding performance in difﬁcult scenarios. The math-\nematics behind GRUs are explained in detail in [27].\nFigure 2.10: The structure of a GRU cell.\nsource: https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\n2.3.3\nGANs (Generative adversarial networks)\nIn 2014, a new framework for estimating generative models via an adversarial\nprocess was proposed [28]. This framework includes a generative model G and a\ndiscriminative model D. The generative model takes as input a random noise and\nproduces a sample (e.g. an image). The discriminative model takes as input sam-\nples(training data), which are same type as the output of G, and gives as an output\nthe probability that the sample from G came from the training data rather than G.\n11\n\n2.3. NEURAL NETWORKS\nChapter 2. Background and Related Work\nFigure 2.11: The overview of a GAN.\nsource: image source: https://deeplearning4j.org/generative-adversarial-network\nThe framework applies better when the two models are both multilayer per-\nceptrons. G is differentiable function represented by a multilayer perceptron with\nparameters θg and a mapping to data space is represented by G(z; θg), where pz(z)\nis a prior on input noise variables. D(x; θd) is a second multilayer perceptron that\noutputs a scalar, where x is the input data. The output of D is the probability that x\ncame from the data rather than pg. The goal is to train D to maximize the probability\nof assigning the correct label to both the samples of G(fake) and data from x(real).\nAccordingly, G have to be trained to minimize log(1−D(G(z))). Hence, the following\nminimax game is created by G and D with value function V(G,D):\nminG maxD V (D, G) = Ex∼pdata(x)[log(D(x))] + Ez∼pz(Z)[log(1 −D(g(Z)))].\nOne application of GANs that could be proved useful for this project is data\naugmentation [29, 30, 31]. If a GAN is fed with frames for a particular emotional\nstate (or in our case a particular valence-arousal range), then it could produce more\nframes similar with these we used as input, which express the same emotional range.\nWith this technique, the current database could be enriched with data and a better\nperformance could be achieved for the main emotions classiﬁcation model.\n2.3.4\nDCGAN\nAccording to the [32] paper, historical attempts to scale up GANs using com-\nmonly accepted and efﬁcient CNN architectures have been unsuccessful.\nBut by\napplying some modiﬁcations on a typical CNN architecture, a more stable training\nprocess across a range of databases was achieved. Those modiﬁcations are targeting\nthree parts of a CNN architecture.\nFirst of all, the paper proposes to substitute all pooling function (such as max-\npooling) with strided convolutions, in order to allow network to learn from its own\n12\n\nChapter 2. Background and Related Work\n2.3. NEURAL NETWORKS\nspatial downsampling. This modiﬁcation can be used in both the generator and the\ndiscriminator.\nThe second change concerns the connection between the highest convolution\nlayer and the input or output of the generator or the discriminator respectively.\nEliminating fully connected layers and using global average pooling increased the\nstability of the model but hurt convergence speed. So, a middle ground of these two\nis to directly connect input/output with convolution layers by using just a reshape.\nLast modiﬁcation that was implemented, is the use of batch normalization [33],\nwhich stabilizes the input by assigning zero mean of variance of one. Batch nor-\nmalization solves problems that can occur from bad initialization and helps gradient\nﬂow in deeper models. Moreover, it provided great contribution when used in gener-\nator, preventing it from collapsing all samples to a single point, which is a common\nproblem that occurs in GANs. However, the application of batch normalization to\nthe last layer of generator and the ﬁrst layer of discriminator was avoided, because\nit resulted to model instability.\nAnother point that is made in this paper, is that it was observed that using ReLU\n[25] activation function for the generator, except for the output layer that tanh was\nused, and leaky ReLU [34] for the discriminator was very beneﬁcial for the training\nprocess.\n2.3.5\nSSGAN\nChapter ﬁve of [35] introduces Semi-Supervised learning in GANs. More specif-\nically, it presents a classic classiﬁer which classiﬁes a data point x into K possible\nclasses, which is trained like a standard supervised classiﬁer. So, the paper proposes\nto convert the learning to semi-supervised by simply adding the generated samples\nto the dataset and labeling them with a new class y=K+1, so the classiﬁer output\ndimension while be K+1. After this change, the classiﬁer could learn from unlabeled\ndata, as long as these data correspond to one of the K classes of real data.\nSupposing the half of the data are real and half generated the Loss function for\nthe classiﬁer/discriminator will be:\nL = Lsupervised + Lunsupervised,\nwhere\nLsupervised = −Ex,y∼pdata(x,y) log pmodel(y|x, y < K + 1)\nand\nLunsupervised = −{Ex∼pdata(x) log[1−pmodel(y = K+1|x)]+Ex∼G log[pmodel(y = K+1|x)]}\n13\n\n2.3. NEURAL NETWORKS\nChapter 2. Background and Related Work\nFigure 2.12: SSGAN architecture.\nsource: https://sthalles.github.io/assets/semi-supervised/GAN classiﬁer.png\nSSGAN is the closest implementation to the network that is going to be created\nlater for this project [36]. The difference is that the classiﬁcation problem has to be\nconverted in regression problem. That affects the output of the discriminator and\nalso both the unsupervised and supervised loss for the discriminator. The results of\nthe SSGAN for the cifar-10 [37] will be presented below, because they will be useful\nto compare with the Regression GAN model of this project.\nFigure 2.13: Generated samples (1000th epochs).\nsource: https://github.com/gitlimlab/SSGAN-Tensorﬂow\n14\n\nChapter 2. Background and Related Work\n2.3. NEURAL NETWORKS\nFigure 2.14: The supervised loss.\nsource: https://github.com/gitlimlab/SSGAN-Tensorﬂow\nFigure 2.15: Discriminator total loss.\nsource: https://github.com/gitlimlab/SSGAN-Tensorﬂow\nFigure 2.16: The loss of Generator.\nsource: https://github.com/gitlimlab/SSGAN-Tensorﬂow\n2.3.6\nCapsule networks\nAt September of 2017, a new paper [38] was published by Hinton and his team,\nwhich introduced a new type of neural network, based on capsules. This new type\nof neural network could be considered as an improvement of traditional CNNs. The\nlogic behind CNNs is that they can detect and recognize features of an image, but\nthey cannot keep the relative positions between these features, so this make CNNs\nvulnerable to classify different images as the same. That is what Hinton is trying to\nsolve with Capsule Networks.\nThe use of max pooling after a convolutional layer, leaves behind useful infor-\nmation and ignores relative spatial relationships between features. If the position\n15\n\n2.3. NEURAL NETWORKS\nChapter 2. Background and Related Work\nof an object is shifted by a little in an input image, because of the max pooling,\nthe output of neurons will not change and the network will still detect the object.\nCapsules solve this problem by saving information about the state of a feature in a\nvector form.\nLet’s take an example how a face would be detected with capsules, if we as-\nsume that there are 2 layers of capsules. In the ﬁrst layer there are three lower level\ncapsules which detect nose, eyes and mouth. Every capsule encodes some internal\nstates and the probability of existence for each of the above features accordingly.\nEach of these capsules are then multiplied by some weights, which encode spatial\nrelationship with the higher level capsule, the face. In other words, these output\nvectors point where the face should be, according to the lower level features(mouth,\nnose, eyes). If these predictions are close, then it should exist a face. Finally, the\nﬁnal output from the higher level capsule gives the pose and the probability that the\nface exists.\nFigure 2.17: Summary of the internal workings of the capsule.\nsource:\nhttps://medium.com/ai%C2%B3-theory-practice-business/understanding-\nhintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66\n2.3.7\nCapsuleGAN\nMotivated by the better performance of CapsuleNets in compare with CNNs, a\nnew framework which combines GANs and CapsuleNets is proposed in [39]. The\nmain idea is to substitute the traditional GAN discriminator, which consists of con-\nvolutional layers, with capsule layers, with goal to perform the 2-class classiﬁcation\ntask (real or fake). The architecture of the capsuleGAN discriminator is similar as\nthe one described in 2.8.\nThe capsuleGAN was implemented and compared with traditional GANs in\nMNIST and CIFAR-10 databases. There were some images generated by both net-\nworks that did not represent any digit(in case of MNIST). However, images gen-\nerated from capsuleGAN seem to have less diversity in terms of classes and to be\ncleaner.\n16\n\nChapter 2. Background and Related Work\n2.4. ARCHITECTURE OF AFFWILDNET\nFigure 2.18: Results of semi supervised classiﬁcation - MNIST.\nsource :[39]\nFigure 2.19: Results of semi supervised classiﬁcation - CIFAR-10.\nsource :[39]\n2.4\nArchitecture of AffWildNet\nIn paper [40], there is already some job done in terms of the architecture\nof a deep learning network that utilizes the existing Aff-Wild database.\nThis in-\ncludes evaluating architectures based on ResNet L50, VGG Face network and VGG-\n16 network, an approach where a CNN was used trained only with video frames,\nand a CNN plus RNN approach which can exploit temporal properties of human\nbehavior[41, 42].\n2.4.1\nVGG\nIn paper[43], the Visual Geometry Group (VGG) team tries to achieve high accu-\nracy scores by increasing the depth of convolutional networks and more speciﬁcally\nup to 19 layers of weight. The architecture of the proposed networks is similar to the\ntraditional convolution networks, but smaller ﬁlters are used in convolutional layers.\nTheir size is usually 3x3, but there is also a case where a 1x1 ﬁlter was used and it\nworked as a linear transformation of the input channels. In general the network is\nmade from a stack of convolution layers, then fully connected layers and a softmax\nlayer.\nThe above architecture was trained and tested in various datasets and had bet-\nter results than other not so deep networks. This increased accuracy of this type of\nnetworks conﬁrms the importance of depth in image classiﬁcation tasks.\n17\n\n2.4. ARCHITECTURE OF AFFWILDNET\nChapter 2. Background and Related Work\nFigure 2.20: Architecture for CNN network based on VGG-Face/VGG-16.\nsource :[14]\n2.4.2\nResNet\nDeep convolutional neural networks have shown great results for image clas-\nsiﬁcation in the past years. They have the ability to recognize low/mid/high level\nfeatures, according to the number of stacked layers existing in the network.\nAs\n[44] mentions, the depth of a network is very crucial, but in very deep networks\nthe problem of vanishing gradient occurs. According to the paper, the proposed so-\nlution for this problem is normalized initialization and intermediate normalization\nlayers, which enable networks with many layers to start converging for stochastic\ngradient descent (SGD) with backpropagation. Another problem arises when deeper\nnetworks start converging, which is that accuracy gets saturated and then degrades\nrapidly. If more layers are added to the network, the training error gets higher in-\nstead of lower, so there exists a degradation of the training error.\nIn paper [44], a solution is addressed to the degradation problem, by introduc-\ning a deep residual learning framework. Instead of just stacking layers and hope that\nthey directly ﬁt a desired underlying mapping, these layers are explicitly pushed to\nﬁt a residual mapping. The desired underlying mapping can be denoted as H(x), so\nthe stacked nonlinear layers ﬁt another mapping of F(x) := H(x) – x and the original\nmapping is recast into F(x)+x. This formulation can be realized by feedforward neu-\nral networks with skipping one or more layers, also known as shortcut connections.\nThese networks can be still trained by SGD with backpropagation, by using common\nsoftware libraries.\nDeep residual nets successfully achieved very challenging results for a number\nof over 1000 layers. The advantage of these networks is that they are easy to opti-\nmize, in contrary with the ”plain” networks whose training error gets higher when\n18\n\nChapter 2. Background and Related Work\n2.4. ARCHITECTURE OF AFFWILDNET\ndepth increases. Moreover, the architecture of residual networks can more easily\nhave accuracy gains from greatly increased depth than any other network.\nFigure 2.21: Architecture for CNN network based on ResNet L50.\nsource :[14]\n2.4.3\nCNN-RNN\nThe CNN-RNN architecture was also use in [1, 14, 15] to evaluate the aff-wild\ndataset. The input of the RNN network is the output of the ﬁrst or the second fully\nconnected layer of the respective CNN. Moreover, the RNN consists of one or two\nhidden layers with 100-150 hidden units. According to [14], the above network\nstructure provided the best results and parameters optimization was performed.\nFigure 2.22: Architecture of CNN-RNN network.\nsource :[14]\n2.4.4\nCNN-M\nAccording to [1, 14, 15], as there was no previous experiments where it was\nattempted to predict valence and arousal values in the wild, the following method\n19\n\n2.4. ARCHITECTURE OF AFFWILDNET\nChapter 2. Background and Related Work\nwas followed, based on the CNN-M [45] network. More speciﬁcally, the pre-trained\nCNN-M network with the FaceValue [46] dataset was used as starting structure and\ntransfer learning was performed, with the exact structure shown in Figure 2.23.\nThe ﬁne-tuning could be performed with two possible ways. First, by freezing the\nCNN part of the network and performing ﬁne-tuning of the fully connected layers.\nSecond, by performing ﬁne-tuning to the whole network.\nFigure 2.23: The architecture of the CNN-M model.\nsource :[1, 14, 15]\n2.4.5\nResults Of The Above Architectures\nFigure 2.24: Results of VGGFace, VGG-16 and ResNet50 architectures, that were de-\nscribed above.\nsource :[14]\n20\n\nChapter 2. Background and Related Work\n2.4. ARCHITECTURE OF AFFWILDNET\nFigure 2.25: Result of the CNN-RNN architecture that described above, with GRU and\nLSTM cells.\nsource :[14]\nFigure 2.26: Best results of the above CNN and CNN-RNN architectures.\nsource :[14]\nFigure 2.27: The results of the modiﬁed CNN-M model.\nsource :[1, 14, 15]\nAll the ﬁgures of this section(2.24, 2.25, 2.26, 2.27) are very important for this\nproject, because they are going to be example results that will be compared with the\nresults of the models of this project. The CCC value will be used for this project, so\nthis value is the one that is more important in these ﬁgures. It can be observed that\nfor the RNN and CNN architectures the best mean CCC is approximately 0.5 and 0.4\nrespectively. For the modiﬁed CNN-M model the mean CCC is approximately 0.13.\n21\n\nChapter 3\nDataset\n3.1\nKind of data needed\nFor the present project, data from video are needed and more speciﬁcally videos\nthat are not made for any speciﬁc task of emotions recognition, in order to satisfy\nthe ”in the wild” concept. Hence, like in [1, 14, 15], the videos are gathered from\nYouTube and they are usually videos which can be found with the keyword ”Reac-\ntion”. Moreover, they contain people who usually react in unexpected plots, exciting\nsituations, etc.\nThese videos will be the input to various Deep Learning models, in the format of\nimages (extracted frames for the videos), and the output will be valence and arousal\nvalues. Hence, it is important that our videos have the best possible quality, because\nit had be shown that quality affects the performance of Neural Networks [47]. So,\nwe expect the size of the data to be large for some videos/frames, but as long as the\nquality is high, data size is not a concern.\nFigure 3.1: Some frames from the aff-wild database.\nsource: [1, 14, 15]\n3.2\nData gathering\nWith the [48] library we were able to download videos in best quality available,\nby running a speciﬁc command in command line for every video. Because of the big\nnumber of videos, a python script was created, which reads the videos link from a\ntext ﬁle, ran the needed command automatically for every video and ﬁnally saved\nthe video ﬁle in a speciﬁc directory. The total size of all gathered videos was 5 GB.\n22\n\nChapter 3. Dataset\n3.3. DATA PRE-PROCESSING\n3.3\nData pre-processing\nBefore the annotation procedure, the downloaded videos have to be prepro-\ncessed for two reasons. First, they need to all have the same frame rate. Second,\nthey need to include only scenes where people react and not irrelevant scenes(e.g.\nan advertisement).\nIn order to specify the frame rate which all the videos need to have, the follow-\ning procedure was implemented. The frame rate of every video was found and an\nhistogram with the frame rate in FPS was created, in order to observe the distribu-\ntion of the frame rate in our videos. A python script was used for this scope with\nthe ffprobe[49] command, which gathers information of multimedia streams(e.g. a\nvideo), including the FPS.\nFigure 3.2: The FPS value for every video.\n30 FPS\nNo 30 FPS\nNumber of videos\n41\n30\nDuration\n3:48:00\n2:13:00\nTable 3.1: Number of videos and duration for videos that are 30 fps and for videos that\nare not 30 fps.\nMost videos had near or exactly 30 FPS, so the value of 30 was decided to be the\nFPS value for all the videos of the database. At this point, 71 videos were gathered\n23\n\n3.4. DATA ANNOTATION\nChapter 3. Dataset\nand because some of them contain 2 people or more, they will be separated in a next\nstage. So, the number of videos is expected to increase.\nFor the second part of data pre-processing, the video trimming part, OpenShot\nwas used [50]. The main purpose was to delete scenes that are not useful for the\nproject.\nThese scenes could be an ad, an intro or an outro.\nIn this case, every\nvideo was processed individually. [50] has the functionality to export the videos\nin a selected frame rate for every selected quality. So, instead of creating another\nprocedure for the frame rate conversion, the new fps value was selected through the\n[50].\nSo, the dataset that occurred from the above steps includes videos with the\nsame frame rate and with frames that include only human faces. Some videos had\nto be separated, because they included individual scenes with different people. For\nexample, video 45 that shows the reaction of 7 people inside a car in different scenes\nfor every person, it was separated in 7 videos. So, 78 videos are ready to be fed to\nthe annotation procedure.\n3.4\nData annotation\nFor data annotation the same tool as [14] was used. Every video was annotated\nin terms of valence and arousal individually. With the tool and the joystick provided\nand with some examples of annotated frames, a value for each valence and arousal\nwas given for a constant time-step. The output of the annotation procedure is two\ntext ﬁles for every video with these 2 series of values.\nFigure 3.3: Guidance image for annotation.\nimage source: https://www.researchgate.net/ﬁgure/Two-dimensional-valence-arousal-\nspace ﬁg1 304124018\n24\n\nChapter 3. Dataset\n3.4. DATA ANNOTATION\nThe value of valence or arousal in a video can rapidly change. For example,\nif a person in calm state sees something sad, he/she will develop negative valence\nwith low arousal. Since the annotation happens in real time, the annotator must be\nable to fast adjust the values of valence or arousal according to what he/she sees.\nElse, the annotation values would have a delay and they would be not assigned to\nthe right frame. The joystick contributes a lot to solving this issue, because it is easy\nto move and it has scales of motion(small move for small motion - big move for\nbig motion). Therefore, the only delay between the frame and the corresponding\nannotation value is the reaction time of the annotator.\nFigure 3.4: Valence-Arousal annota-\ntions for video6.\nFigure 3.5: Valence-Arousal annota-\ntions for video37.\nFigure 3.6: Valence-Arousal annota-\ntions for video45 2.\nFigure 3.7: Valence-Arousal annota-\ntions for video61.\nThe choice of the videos was not random. A Neural Network [51] needs a good\ndistribution of labels in order to be trained properly. So, videos with a broad range\n25\n\n3.5. EXTRA DATA\nChapter 3. Dataset\nof values for valence and arousal had to be chosen. This means that all the emotions\nare wanted to be exposed to this experiment. Since the source for the data is the\nweb, the most common values for valence and arousal were those near zero, which\nmostly correspond to a person who is calmly talking. Nevertheless, as it can be seen\nin the following ﬁgure, the distribution was not as good as expected. That is because\nmost of the valence and arousal values of the frames are gathered near zero.\nFigure 3.8: Valence and arousal distribution for the whole dataset (479.022 frames\nbefore face detection).\nIt should be mentioned that videos that contain 2 or more people in the same\nscene, are labeled for each person. For example video55 contains one man and\none woman and their corresponding annotation ﬁles are 2 and they are named\nvideo55 left.txt and video55.txt, where the ﬁrst ﬁle is the person on left as we look\nat the screen. In this case the video counts as two videos.\n3.5\nExtra data\nSince the current distribution of valence and arousal values is not the desired\none, more data will be added to the dataset. For the new videos the same procedure\nas before will be followed. The goal for the new dataset is to have a broad range of\nannotation values, so when it get merged with the initial database, to have a good\noverall distribution.\n26\n\nChapter 3. Dataset\n3.5. EXTRA DATA\nThe ﬁrst thing to do after downloading the new 21 videos is to check the FPS\ndistribution, in order to conﬁrm that 30 fps is still the appropriate value to keep.\nFigure 3.9: The FPS value for every video (for the new videos).\n30 FPS\nNo 30 FPS\nNumber of videos\n9\n12\nDuration\n0:52:00\n1:49:00\nTable 3.2: Number of videos and duration for videos that are 30 fps and for videos that\nare not 30 fps(for the new videos).\nStill more than the half data are 30 fps and there are many videos near 30 fps,\nso that is the value that will be kept. After processing the videos with the same tool\nas before, 2 new videos came up, so in total 23 new videos were added to the initial\ndatabase.\nLast thing that it needs to be checked is if the annotation distribution is the\ndesired one. According to the ﬁgure below, clearly the values are better distributed\nin low and high valence and arousal values.\n27\n\n3.6. FACE DETECTION\nChapter 3. Dataset\nFigure 3.10: Valence and arousal distribution for the new dataset (85.556 frames before\nface detection).\n3.6\nFace detection\nThe last step before the dataset is ready to be fed to any neural network is to\ndetect and crop the faces that exist on every frame for every video. So, the task is\nto detect face in an image/frame. There are several ways to achieve face detection,\nbut for this case the menpo library [52] was used. More speciﬁcally, the menpode-\ntect pack was used with the fﬂd2 detector. The fﬂd2 detector achieves frontal face\n28\n\nChapter 3. Dataset\n3.6. FACE DETECTION\ndetection using the DPM Baseline model provided by Mathias et. al. Faces are being\ndetected even when hands are covering a part of them or if the person wear glasses.\nThat is very desirable in this project, in order to keep the ”in the wild” concept.\nFigure 3.11: Original frame.\nFigure 3.12: Frame after face detection.\nAnother thing that was important about this part of the data processing, was\nto keep the loss of frame due the face detection procedure low. The frames of the\noriginal videos before the frames detection were 564.578. After the face detection\nthe resulting dataset consisted of 507.208 frames, meaning that there was a 10%\nloss, which is good enough to proceed to the next steps.\nFigure 3.13: Frames before and after the face detection for every video. (Videos which\ninclude 2 people have been counted in the same bar twice.)\n29\n\n3.7. ANNOTATIONS TO LABELS CONVERSION\nChapter 3. Dataset\nIn order to have a consistent way of saving the frames which the detector gave as\noutput, the following way of saving was followed. Every video has its own directory\nnamed with the video name(e.g. video1, video2, etc) and it contains the detected\nframes with the number of frame as name in 6-digit format (e.g. 0000001.png,\n0000021.png, etc).\n3.7\nAnnotations to labels conversion\nThe output of the annotation tool was in the following format:\nValence\nArousal\nSeconds\nValues\nSeconds\nValues\n0.010\n0\n0.016\n0\n0.541\n-408\n0.037\n0\n0.556\n-432\n0.116\n0\n0.576\n-448\n0.176\n-37\n0.587\n-448\n0.218\n-116\nTable 3.3: Output of annotation tool for valence and arousal for video89\nThere are several problems that have to be solved, in order to convert the format\nin Table 3.1 into format of labels that can be fed to Neural networks.\nFirst, the time step that the software captures values for valence and arousal\nis different and it is captured in seconds. This is a problem, because our dataset\ncontains frames of videos (30 frames for every second), so a way must be found to\nmatch these two values. Linear interpolation [53] for valence and arousal sequences\nindividually was used for this task. More speciﬁcally, since there is a standard frame\nrate for every video it was easy to create the appropriate sequences to feed into\npython interp(x, xp, fp) function, according to the documentation [54]:\nx: The x-coordinates at which to evaluate the interpolated values. A label for every\nframe is wanted. If total frames of video are set as ’frames’:\n1, 2, 3, 4, ..., frames −1, frames\nxp: The x coordinates of data points. Since the previous sequence is deﬁned in\nframes, this should also be deﬁned in frames. So it will be deﬁned as the values in\nseconds from annotation tool multiplied by 30 (fps):\n(in case of valence of video 89)\n0.010 ∗30, 0.541 ∗30, 0.556 ∗30, 0.576 ∗30, ..., 74.754 ∗30\nfp: The y-coordinates of the data points. Here, just the values of the sequence for\nvalence or arousal will be entered:\n30\n\nChapter 3. Dataset\n3.8. FINAL DATASET\n(in case of valence of video 89)\n0, −408, −432, −448, ..., −828\nNow that there is a valance-arousal value for every frame, these values should\nbe scaled. The current range of the values is [-1000,1000] and the desired range is\n[-1,1], so all values will be divided by 1000.\nThe last step for creating the labels, is to create a text ﬁle that contains the\nvalence-arousal values, which correspond to every frame that has been detected\nwith a face. As mentioned before, every video is saved in a directory with the video’s\nname. So, for every directory the frame number are found and the corresponding\nvalence-arousal values are printed in the text ﬁle. Hence, every video has its own\nlabels ﬁle named with the video’s name (e.g. video89.txt).\n3.8\nFinal dataset\nThe ﬁnal dataset consists of 507.208 frames gathered from 106 videos and its\nsize is approximately 48GB. These frames includes approximately 150 faces of dif-\nferent people.\nThe resulting frames can be considered as very clean data for any ”in the wild”\nface related machine learning task. The annotation was created by one person, so\nit is not as objective as in the ”Aff-wild” database. Nevertheless, the dataset has the\npotential to work right with the experiments that are going to take place.\nVideos\nFrames\nno of males\nno of females\nStage 1\n71\n479.022\n53\n30\nStage 2\n78\n479.022\nStage 3\n21\n85.556\n15\n8\nStage 4\n23\n85.556\nStage 5\n101\n564.578\n68\n38\nStage 6\n106\n507.208\nTable 3.4: Number of videos,frames and number of males and females for every stage.\nStage 1: Initial database before processing\nStage 2: Initial database after processing (same fps, just separated videos)\nStage 3: Extra database before processing\nStage 4: Extra database after processing (same fps, just separated videos)\nStage 5: Total database before face detection\nStage 6: Final database after face detection\n31\n\n3.8. FINAL DATASET\nChapter 3. Dataset\nFigure 3.14: Some challenging sequences of frames.\n32\n\nChapter 4\nNeural Network Models\n4.1\nLoss function for regression\nThe dataset that was created in the previous steps is ready to be fed in Neural\nNetworks. Since there is no evidence that the particular dataset will provide accurate\npredictions for valence and arousal, it has to be ﬁrst trained in a small network that\nhas been used before.\nNo matter if a CNN or a GAN is being trained, the loss of the network will in-\nclude a supervised loss, which will present how accurate the predictions are in terms\nof valence and arousal. The loss function that will be used for this task is the Con-\ncordance Coefﬁcient Correlation (CCC). CCC is deﬁned as:\nccc =\n2sxy\ns2\nx + s2\ny + (¯x −¯y)2\nwhere s2\nx and s2\ny are the variances of the predicted and ground truth values respec-\ntively, ¯x and ¯y are the corresponding mean values and sxy is the respective covariance\nvalue. In case that predictions and ground truth values are identical CCC gets the\nvalue of 1 and in case that these values are completely uncorrelated the values gets\nthe value of 0. Optimizing the loss needs to be a minimization problem, so the loss\nthat will be used in the Neural Networks is 1 - CCC.\n4.2\nData Split\n4.2.1\nTesting Set\nIn order to create a testing set, some videos were subtracted from the initial\ndataset and they were deﬁned as testing set. Those videos were video7, video19,\nvideo25, video45 1, video45 2, video45 3, video45 4, video45 5, video45 6, video45 7,\nvideo48, video62, video72, video73 and they include in total 24.226 frames, with\n8 males and 6 females and their valence-arousal distribution is shown in the next\nﬁgure.\n33\n\n4.2. DATA SPLIT\nChapter 4. Neural Network Models\nFigure 4.1: The valence-arousal distribution of the testing set.\n4.2.2\nTraining With Small Dataset\nThe extra dataset that was created is deﬁned as small dataset. It includes 80.280\nframes, with 15 males and 8 females and its distribution is shown in ﬁgure 3.10.\n4.2.3\nTraining With Big Dataset\nThe big dataset does not include the frames from the testing dataset, while it\nincludes the frames from the initial dataset plus the frames from the small dataset.\nSo, there are in total 482.982 frames in the dataset, with 60 males and 32 females\nand the valence-arousal distribution is shown in the next ﬁgure.\nFigure 4.2: The valence-arousal distribution of the big dataset.\n34\n\nChapter 4. Neural Network Models\n4.3. TESTING THE DATABASE IN A CNN\n4.3\nTesting The Database In A CNN\nIn order to ensure that the dataset creates in the previous steps is well made, it\nwill be tested in an already tried CNN architecture. A simple architecture as CNN M\nwas chosen for this task and only 110.000 (80.000 for training and 30.000 for test-\ning) frames were used, in order to achieve overﬁtting and make sure that the label-\ning makes sense.\nThe network architecture that was used for this scope, was\nthe modiﬁed CNN-M [45] network from paper [1, 14, 15].\nFigure 4.3: The architecture of the model.\nThe small dataset is not derived randomly from the big one. Even with less\nimages, a good valence-arousal distribution is needed for the training dataset, in\norder to evaluate the model. This distribution is shown in ﬁgure 3.10.\n4.4\nRegression GAN\n4.4.1\nFormat of Images\nThe ﬁrst thing that had to be done for the gathered data was to set a constant\nimage size, which the images had to be reshaped before they were fed to the model.\nSince the database was big and that could make the training procedure really slow\nin case of a big image size, the ideal size was decided by ﬁnding the minimum size\nof all the images in the dataset. The minimum size was 69x69, so a size of 64x64,\nwhich is already used in many famous GANs implementations, was chosen and all\nthe images were reshaped to this size.\n35\n\n4.4. REGRESSION GAN\nChapter 4. Neural Network Models\nFigure 4.4: Minimum, mean and maximum width and height for every video (The mean\nfor all the frames of a video was used)\nNo data augmentation happened nor noise was added to the data. The only\nchange was, that all the images which had an initial range of values [0, 255], were\nscaled to [-1, 1]. That was proposed in DCGAN paper [32]. In the next section,\nwhere the structure of the generator is presented, it will be stated that the activation\nfunction after the last convolution of the generator is the Tanh function, which out-\nputs values in range [-1, 1]. Since both the real images and the generated samples\nhave to be fed to the discriminator, they have to be in the same range of values,\nwhich is [-1, 1].\n4.4.2\nGenerator\nFor the generator, the structure of DCGAN generator was followed. As input, a\nnoise generated from uniform distribution with range from -1 to 1 and dimension\n100 was used. In order to connect the input with the convolution layers(project and\nreshape), a fully connected layer was used. Since the desired starting size for the\ninput of convolution layers was [batch size, 4, 4, 512], the output dimensions of the\nfully connected layer had to be 4 ∗4 ∗512.\nAfter projecting and reshaping the input, four strided convolution layers fol-\nlowed with kernel size [5, 5] and stride [2, 2].\nBatch normalization and ReLU\nactivation function was used in all the convolution layers, except for the last one.\nwhere batch normalization was not used and the activation function was Tanh.\n36\n\nChapter 4. Neural Network Models\n4.4. REGRESSION GAN\nFigure 4.5: Generator of DCGAN: The input is a random noise with dimension 100,\nwhich is projected and reshaped in [batch size, 4, 4, 1024]. Then 4 convolution layers\nfollow with kernel=[5, 5], stride=[2, 2] and output dimensions 512, 256, 128 and 3\nrespectively. The output is the fake image.\n4.4.3\nDiscriminator\nThe discriminator works as a CNN, modiﬁed in order to avoid the GANs training\ndifﬁculties. Having as inputs images of size 64x64, it outputs 3 predictions, which\nare valence, arousal and the real/fake prediction. In this case, four convolution\nlayers are used with batch normalization following them. The activation function\nused is the leaky ReLU and dropout layers were used with dropout probability 0.5.\nAfter the last layer of convolution-dropout-batch norm-activation, Global Average\nPooling was performed, which formed the ﬁnal output of the discriminator.\nNo\nactivation function was necessary for the output, because this is a regression task,\nnot a classiﬁcation task.\n37\n\n4.4. REGRESSION GAN\nChapter 4. Neural Network Models\nLayer\nﬁlter\nstride\npadding\nno of units\nconv1\n[5, 5, 3, 64]\n[1, 2, 2, 1]\n’SAME’\nlrelu\ndropout\nconv2\n[5, 5, 64, 128]\n[1, 2, 2, 1]\n’SAME’\nbatch norm\nlrelu\ndropout\nconv3\n[5, 5, 128, 256]\n[1, 2, 2, 1]\n’SAME’\nbatch norm\nlrelu\ndropout\nconv4\n[5, 5, 256, 512]\n[1, 2, 2, 1]\n’SAME’\nbatch norm\nlrelu\ndropout\nGAP\ndense\n3\nTable 4.1: Architecture of discriminator.\n4.4.4\nLoss\nThe loss for the Regression GAN has 2 parts, the discriminator and the generator\nloss. Those two losses are optimized individually and it is possible that they are\nupdated different number of times during one iteration.\nFor the discriminator part, the regression loss is ﬁrst deﬁned. As described in\nsection 4.1,\nLsupervised = 1 −ccc\nis the function that will be used, where CCC is the concordance coefﬁcient correla-\ntion between the predicted and the real labels. However, the discriminator needs to\nbe also evaluated on how good classiﬁed the real and the fake images. Hence the\nunsupervised loss will be(assuming that K+1 is the real/fake class):\nLunsupervised = −{Ex∼pdata(x) log[1−pmodel(y = K+1|x)]+Ex∼G log[pmodel(y = K+1|x)]}\nThe total discriminator loss is deﬁned as:\nLdiscriminator = Lsupervised + Lunsupervised\nThe generator loss also consists of two parts. First, it needs to be evaluated\non how many of the samples that generated did the discriminator classiﬁed as real\nimages.So, the loss for this part will be(assuming that K+1 is the real/fake class):\nLG1 = −Ex∼G log[1 −pmodel(y = K + 1|x)]\n38\n\nChapter 4. Neural Network Models\n4.4. REGRESSION GAN\nThe second part of the generator loss is called feature matching. As described\nin the Improved Techniques for GANs paper [35], feature matching is deﬁned as\npenalizing the mean absolute error between the average value of some set of features\non the training data and the average values of that set of features on the generated\nsamples.\nInstead of mean absolute error, the Huber loss was used. The Huber loss [55]\nfor the difference of real and predicted data a = y −f(x) is deﬁned as:\nLδ(y, f(x)) =\n\n\n\n\n\n1\n2(y −f(x))2\nfor |y −f(x)| ≤δ\nδ|y −f(x)| −1\n2δ2\notherwise\n\n\n\n\n\n,\nwhere\nδ\nis\nusually\n1,\ny\nis\nthe\nlabels\nand\nf(x)\nthe\npredicted\nvalues.\nTwo of the most known loss functions are the Mean Absolute Error (MAE) and\nMean Squared Error (MSE). The squared loss has the disadvantage that it has the\ntendency to be dominated by outliers values when the distribution is heavy tailed.\nAs a solution to this problem, Huber loss combines much of the sensitivity of the\nMSE and the robustness of the MAE.\nFigure 4.6: MAE vs MSE vs Huber (delta = 1.0)\nsource:\nhttps://www.researchgate.net/ﬁgure/Plots-of-the-L1-L2-and-smooth-L1-\nloss-functions ﬁg4 321180616\nHence, as deﬁned above, the second part of the generator loss will be:\nLG2 = Ex∼pdata(x)Lδ(x, G(z)),\nwhere x is the the real images and G(z) is the samples generated from noise z.\n39\n\n4.4. REGRESSION GAN\nChapter 4. Neural Network Models\nFinally the total loss of the generator is:\nLG = LG1 + LG2\n4.4.5\nTraining Tricks\nSince GANs are difﬁcult to train, some tricks had to be applied in the structure\nof the framework in order to avoid the fast convergence and stabilize training.\n• Higher learning rate is applied to the training of the generator.\n• One-sided label smoothing is applied to the positive labels.\n• Gradient clipping trick is applied\n• Reconstruction loss with an annealed weight is applied as an auxiliary loss to\nhelp the generator get rid of the initial local minimum.\n40\n\nChapter 5\nTechnical Procedure\n5.1\nPre-processing Scripts\nMany procedures were needed in order to gather and get the data ready for the\nNN models. For every such need, python scripts were used. Most of the scripts were\njust executing data manipulation procedures with common libraries like Numpy. In\ncase where a modiﬁcation that concerns media streams (e.g. video) needed, it was\nimplemented by using ffmpeg [49] commands. But because these commands are\nexecuting in linux terminals for every video individually, an automated process had\nto be created. That was achieved by using the Subprocess module (Built-in module\nin python) for calling ffmpeg commands inside a loop, for all the videos.\n5.2\nTensorFlow\nTensorﬂow is an open source tool developed from Google and it is commonly\nused for machine learning and Deep learning applications. In the Tensorﬂow envi-\nronment, the computations are performed with the use of data ﬂow graphs. These\ngraphs are are consisted of nodes and graph edges. The nodes represent mathe-\nmatical operations and the edges are tensors which are passed between the nodes.\nTensorﬂow offers its API in various programming languages, but the one used in this\nproject is Python. Finally, CPU and GPU versions of the API are offered and since\nthere was access to a GPU cluster by the university, the GPU version was used.\n5.3\nInput Data\nAs mentioned in the Chapter 3, the ﬁnal dataset is organized in 6 directories,\n’frames’(big dataset), ’extra frames’(small dataset), ’test frames’(testing dataset) and\n’labels’, ’extra labels’, ’test labels’. The frames directory contains one directory for\nevery video, where the frames of this video are stored. The labels directory contains\na text ﬁle for every video, where every line of a text is the valence and arousal values\nfor the corresponding frame. Having the data in this form, the following process was\nfollowed in order to ﬁt the data to the NN models.\n41\n\n5.4. CODE STRUCTURE\nChapter 5. Technical Procedure\nFirst, as stated in the dataset chapter, the size of all the videos that were used\nwas 5 GB. So, since the videos had to be converted to raw images during the faces\ndetection, it is obvious that the total size of all the images would be much bigger.\nMore speciﬁcally, the size of the resulting dataset was 48 GB. Therefore, another\nway has to be used instead of loading the whole dataset into the machine, because\nit would obviously not ﬁt in the machine’s memory.\nIn order to load the dataset into smaller pieces of data, the following was done.\nFirst, all the paths of all the images were loaded into a list. Then, some functions\nfor loading and modifying the images were set.\nFinally, the data was loaded in\nmini batches from the paths list, following those load and modiﬁcation functions for\nevery such mini batch. This procedure was carried out by process data(data path,\nlabels path, istraining) function, which is described in detail in the next section.\n5.4\nCode Structure\nThe code that implements the basic functionality of our model is placed in just\none ﬁle. This ﬁle is organized in methods. In this section, those methods are going\nto be described in detail:\nhuber loss(labels, predictions, delta=1.0): Huber loss was described earlier as a\nloss function. This method takes as arguments the arrays with the predictions and\nthe labels and also the delta value, which is by default 1. It executes all the appro-\npriate calculations and it returns the Huber loss.\nlrelu(x, n, leak=0.2): Leaky ReLU was described earlier as an activation func-\ntion. This method takes as arguments the input x(ouput of batch normalization),\nthe name of the variable n, and the slope, which is by default 0.2. It performs the\nactivation and it give it as output.\ntf ccc(r1, r2): Concordance coefﬁcient correlation was also described earlier as a\nloss function. This method takes as input the predictions and the labels and returns\nthe CCC as a ﬂoat value.\nprocess data(data path, labels path, istraining): This method does all the pro-\ncessing that concerns the images and the labels. First it creates 3 lists. The ﬁrst list is\nfor the images paths, the second for the corresponding valence values and the third\nfor the arousal values. Then, both lists are converted to tensors and are passed to\ninput slice producer(), which create a queue for the data. The next step is to load\nimages, reshape and rescale them as described in the previous chapter. Finally, the\noutput batch is produced by train.batch() method and is returned.\ngenerator(input, random dim, is train, reuse=False): This method contains the\narchitecture for the generator network. The input is passed through all the layers\ndescribed in the previous chapter and after the last activation function(Tanh), the\n42\n\nChapter 5. Technical Procedure\n5.5. TRAINING\nmethod returns it as an output.\ndiscriminator(input, is train, reuse=False): This method contains the architec-\nture for the discriminator network. The input is passed through all the layers de-\nscribed in the previous chapter and after the Global Average Pooling, the method\nreturns the logits as an output.\ntrain(): This is the main method that brings everything together and its function-\nality will be described brieﬂy. First, the tensorﬂow’s placeholders are created. Place-\nholders are just empty tensors with pre-deﬁned dimensions that are fed later with\ndata. Placeholders needed for this network are for the images(real image), the la-\nbels(labels), the random noise(random input) and a boolean which states if the feed\nis for training or testing(is train). Then, fake images is returned from the generator\nand both real and fake images are fed individually to the discriminator. Next step\nis the calculation of all the losses and updating variables. Since training and test\ndata are needed, paths for both of them are deﬁned and process data function is\ncalled twice, once for training and once for testing data. The session initialization\nfollows. It is conﬁgured with GPU settings and it searches for any last checkpoint of\nthe model. If it ﬁnds any, it loads the saved variables and it continues training, else\nit starts the training from scratch. The last part of this method is the loops of epochs\nand iterations, in which the data are fed to the model and the results are returned.\nProgress for training is saved for every iteration in a text ﬁle. Testing data are being\nfed to the model every 60 iterations and also new generated images are being saved\nevery 60 iterations.\n5.5\nTraining\nThe optimization algorithm for the training of the model was the Adam opti-\nmizer with starting learning rate 0.0001 for the discriminator and 0.0002 for the\ngenerator. The discriminator maintains half of the learning rate of the generator,\nduring the whole training process. Moreover, for the ﬁrst GAN experiment generator\nand discriminator have the same update rate, which is one iteration each. For the\nsecond experiment, the update rate was 2 for the generator.\n5.6\nTesting\nTesting for this model include two parts, the generator and the supervised test-\ning. Both parts are carried out every 60 iterations. For the supervised testing, a batch\nof size 1000 from the testing dataset is fed to the discriminator with boolean\nis train set to False. Then, the loss(1-CCC) is calculated and saved to a text ﬁle.\nFor the generator testing, a random noise is fed to the generator and its output is\nreturned. In order to evaluate the semi-supervised performance of the network, the\nartiﬁcial images are also fed to the discriminator in order to keep its output, which is\n43\n\n5.7. EVALUATING RESULTS\nChapter 5. Technical Procedure\nthe valence and arousal values. Since the output of the generator is the artiﬁcial im-\nages, they are saved in one image in a format of 8x8 (images number is same as the\ntraining batch size e.g. 64). Finally the text ﬁle with the output of the discriminator\nis saved with the same name of the image ﬁle.\n5.7\nEvaluating Results\nThe results that concern the losses are evaluated by python scripts which process\nthe text ﬁles created above. By doing that, there is freedom in design graphs and\nstatistical values by using various python libraries.\n44\n\nChapter 6\nEvaluation of Results\n6.1\nCNN-M\nAs stated in Chapter 4, since the dataset is created from scratch, a ﬁrst test has\nto be done, in order to check whether a model can actually learn from the data.\nSo, the small dataset was used, in order to train a modiﬁed CNN-M network from\n[1, 14, 15], which has already proved that it can return good results for the valence-\narousal regression task.\nFigure 6.1: Loss of CNN-M with small dataset for training data and testing dataset.\n45\n\n6.1. CNN-M\nChapter 6. Evaluation of Results\nTraining error(1-CCC)\nTesting error(1-CCC)\nMinimum\n0.04\n0.58\nMean\n0.19\n0.76\nstd deviation\n0.28\n0.1\nTable 6.1: Minimum, mean and standard deviation for training and testing error of\nCNN-M network.\nBy evaluating Figure 6.1, it can be concluded that the model actually learns\nfrom the data, because the training error converges near zero. However, there are\nsome occasions that the graph does some spikes and it is not stable. This could\nhappen either because of bad annotation, or from noise in the images. Nevertheless,\nthe training error enables the use of other models with this dataset.\nBefore looking to the above results, the model was expected to overﬁt and return\na very bad testing error, due tot the small amount of data. Instead of this, there is a\npretty good test error with minimum value 0.58. That means that the CCC between\nthe real valence-arousal values and the predicted values is almost 0.4. According\nto the [1, 14, 15], the CCC that was achieved from the same CNN-M network was\n0.10-0.15, with a much bigger dataset than this. Two possible reasons for this to\nhappen could be the following. First, that the valence-arousal distribution of the test\ndataset does not cover all the valence-arousal range of values and it returns good\nresults for the same values. Second, because the annotation was carried out from\none annotator it may has bias in some emotions and in combination with the small\ndataset, it returns good results.\nHowever, the testing error it is not very important in this case, because this\nmodel was used as a step that will conﬁrm that the dataset is not useless.\n46\n\nChapter 6. Evaluation of Results\n6.2. REGRESSION GAN\n6.2\nRegression GAN\n6.2.1\nLoss Evaluation\nFigure 6.2: Loss of Regression GAN with small dataset for training and testing dataset.\nGenerator error\nDiscriminator error\nMinimum\n1.55\n0.35\nMean\n3.7\n0.68\nstd deviation\n0.77\n0.26\nTable 6.2: Minimum, mean and standard deviation for Generator and Discriminator\nerror (trained with small dataset).\nTraining error\nTesting error\nMinimum\n0.04\n0.66\nMean\n0.12\n0.77\nstd deviation\n0.14\n0.2\n47\n\n6.2. REGRESSION GAN\nChapter 6. Evaluation of Results\nTable 6.3: Minimum, mean and standard deviation for training and testing error(1-\nCCC) of Regression GAN network(trained with small dataset).\nDiscriminator accuracy\nReal Images\n0.99\nFake Images\n0.07\nTable 6.4: The discriminator accuracy after 18.000 iterations, for all the real images of\nthe dataset and 12.000 fake images.\nThe above results are coming from the Regression GAN trained with the small\ndataset. Since it provides results that show that both unsupervised and supervised\ntasks are executed, the same model will be trained again with the big dataset. The\nonly change in the training will be that the update rate of the generator will be set\nto 2.\nFigure 6.3: Loss of Regression GAN with big dataset for training and testing dataset .\nGenerator error\nDiscriminator error\nMinimum\n1.3\n0.41\nMean\n2.66\n1.06\nstd deviation\n0.45\n0.2\n48\n\nChapter 6. Evaluation of Results\n6.2. REGRESSION GAN\nTable 6.5: Minimum, mean and standard deviation for Generator and Discriminator\nerror (trained with big dataset).\nTraining error\nTesting error\nMinimum\n0.05\n0.4\nMean\n0.24\n0.52\nstd deviation\n0.11\n0.2\nTable 6.6: Minimum, mean and standard deviation for training and testing error(1-\nCCC) of Regression GAN network(trained with big dataset).\nBy observing the generator-discriminator loss(ﬁgure 6.3), it can be inferred that\nduring the 18.000 iterations of training, the game between generator and discrim-\ninator is still played. The spikes in the loss graph is every time the generator fools\nthe discriminator, then the discriminator improves itself and the generator tries to\nfool it again. However, the absolute value of the generator could be lower, in order\nto produce better fake images.\nAbout the supervised loss (for the big dataset training), the training error(1-\nCCC) has a mean value of 0.24, which indicates that the model is getting trained both\non predicting valence-arousal values and on predicting real-fake images. Moreover,\nthe testing error(1-CCC) has a mean value of 0.52. As mentioned in the previous\ncase, this value is more than acceptable but it concerns only the current dataset,\nsince the emotion annotation is a subjective procedure.\nIn summary, it seems that the discriminator is able to ﬁnd a way to split its\nweights in order to get trained for both the unsupervised and supervised tasks.\nCompared with the best results of the models implemented in [14], this network\nperformed very well, with always having in mind that the data are different in terms\nof annotation and because of that it may be easier to achieve good performance. It\nworths to remind that the CCC for the best model in [14] was 0.4, while for this\nmodel is 0.48, but that is just mentioned just for reference since different models\ntrained with different data cannot be directly compared.\nRegression GAN\nMax CCC\nMean CCC\nSmall dataset\n0.34\n0.23\nBig dataset\n0.6\n0.48\nTable 6.7:\nMinimum and mean CCC of Regression GAN model for small and big\ndatabase respectively.\n49\n\n6.2. REGRESSION GAN\nChapter 6. Evaluation of Results\n6.2.2\nImage Generation Evaluation\nFigure 6.4: Fake images after 1\niteration.\nFigure 6.5:\nFake images after\n1.250 iterations.\nFigure 6.6:\nFake images after\n7.500 iterations.\nFigure 6.7:\nFake images after\n11.250 iterations.\nFigure 6.8:\nFake images after\n13.750 iterations.\nFigure 6.9:\nFake images after\n18.000 iterations.\n50\n\nChapter 6. Evaluation of Results\n6.2. REGRESSION GAN\nIn the above ﬁgures, the evolution of the generated images is shown. In ﬁgure\n6.3 there is just the input noise and as the training continues, images are taking\nshapes that look like human faces. Moreover, there are features that get combined\nand make the faces more interesting(e.g. glasses, beard, etc). The quality of the\nimages is not perfect and with more training or with more tuning of the model could\nbe improved.\n6.2.3\nSemi-Supervised Learning Evaluation\nFigure 6.10: Fake images\nFigure 6.11:\nValence-Arousal\npredictions for images.\n51\n\n6.2. REGRESSION GAN\nChapter 6. Evaluation of Results\nFigure 6.12: Fake images\nFigure 6.13:\nValence-Arousal\npredictions for images.\nFigure 6.14: Fake images\nFigure 6.15:\nValence-Arousal\npredictions for images.\n52\n\nChapter 6. Evaluation of Results\n6.2. REGRESSION GAN\nFigure 6.16: Fake images\nFigure 6.17:\nValence-Arousal\npredictions for images.\nThe ﬁgures above (6.9 - 6.16) are very interesting on how the semi-supervised\nlearning performs. The evaluation is done just by observing the images and the\npredicted values side by side. The ﬁrst thing that can be observed from these ﬁgures\nis that the predictor avoids big absolute values of arousal and that there are no many\nnegative valence values. However, one can say that this reﬂects how the annotations\nof the training data is distributed, as shown in Chapter 3.\nSome images where a face cannot be displayed clearly is not easy to evaluate the\npredictions results. For the images that it is not hard to tell that there is a face, there\nis a good number of occasions where the predictions seem to agree with the reality.\nIt is certain that there is a big space for improvement in order to have acceptable\nemotion predictions in generated samples.\n53\n\nChapter 7\nConclusion and Future Work\n7.1\nDataset Summary\nThe dataset created in this project could be proved useful in many machine\nlearning projects [56, 57, 36, 58, 40], because it follows the ”in the wild” concept.\nThis concept is very useful in the real world, because any applications that analyze\nhuman behavior, such as emotions recognition, should be able to be used in real\nsituations. If a model is trained with data recorded inside a laboratory, then when is\napplied in the real world it will probably be inaccurate, due to the factor of people’s\nunpredictability, which cannot not been taken in account.\nThe dataset quality is high in terms of the images it includes. The face detec-\ntor was very accurate and there is no much noise [59] that can hurt any machine\nlearning model. In most cases, faces are clear and the image is cut without enabling\nsomething other than the face being in the image. The detector had no issue to also\ndetect faces with glasses or with the hand in front of the mouth and other situa-\ntions like these. This also helped to support the ”in the wild” concept, as mentioned\nbefore. However the dataset can be enriched with more unique people, in order\nto enrich the diversity of the data and with higher absolute values of valence and\narousal, in order to maintain a good emotion distribution.\nAs mentioned before, the annotation task was carried out from one person. That\nsaid, the labels in many cases may be biased and not close the the proper value that\na psychologist would have suggested. For the original ”aff-wild database” [1, 14, 15]\nthe annotation was much more objective, since it was carried out by 6-8 annotators\nand more specialized techniques were used in order to ensure annotation quality.\n7.2\nNeural Networks Summary\nAfter creating a database and before trying complex deep learning models, it\nwas decided to use an already implemented CNN network, in order to test the\ndataset. For this reason, the modiﬁed CNN-M from [1, 14, 15] was chosen. There\nwas no need to use a deeper network like ResNet L50 or VGG, because the purpose\nwas only to overﬁt the data and then continue with the main model of the project.\nIndeed, the outcome of the model was a very low training error and surprisingly\n54\n\nChapter 7. Conclusion and Future Work\n7.3. FUTURE WORK\na very good testing error, in compare with the CNN results that were presented in\nChapter 2. However, as mentioned earlier, there were reasons like annotation bias\nthat could have returned better results than the reality.\nWhen the background and related work was studied, it seemed like a good idea\nto combine two successful concepts like emotions predictions and images genera-\ntion. First, from previous works, it was proved that CNNs had a decent performance\nwith the ”in the wild” data. Second, from the background study, GANs had a good\nperformance on generating images, especially with faces. So, the idea was to imple-\nment a combination of emotions predicting and images generation.\nFinally a regression GAN was implemented. The results presented in the previ-\nous chapter indicate that it can work and produce decent results. But since GANs are\ndifﬁcult to train, a reliable and stable model needs more time to be implemented.\nHowever, this type of network can be really useful for data augmentation tasks, if\nsomeone takes advantage of the semi-supervised attribute of the model.\n7.3\nFuture Work\nEmotions recognition ”in the wild” with valance and arousal values is a very\npromising concept, that has already produced very good results in many papers. In\nthis project, it was attempted to combine GANs with the prediction procedure by\nimplementing a Regression GAN in a newly created dataset. Of course, there is a lot\nof work that could be done in order to achieve good and reliable results that could\ncompete other state of the art implementations.\nThe dataset created for this project having as guidance the ”aff-wild” dataset\ncould be enriched with more data. First, in order to help GANs produce better fake\nimages, the dataset should be enriched in terms of diversity, with more different and\nunique people. Second, the data should have more distributed valence and arousal\nvalues especially in very low or high values. Then, the data would really help the\nunsupervised and the supervised part of the Regression GAN to perform better.\nThere is a lot of space for tuning in the project’s network. However, something\nthat may produce better results, would be the use of Capsule GANs. As described\nin Chapter 2, Capsule GANs have the ability not to only learn the features of the\nimage, but they also keep their spatial information. This is achieved by substituting\nthe discriminator of a GAN, which is actually a CNN version, with a capsule network.\nIt was shown in [39] that they can provide better results than the traditional GANs.\n55\n\nBibliography\n[1] S. Zafeiriou, D. Kollias, M. A. Nicolaou, A. Papaioannou, G. Zhao, and I. Kotsia,\n“Aff-wild: Valence and arousal’in-the-wild’challenge,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition Workshops, pp. 34–41,\n2017. pages 1, 7, 19, 20, 21, 22, 35, 45, 46, 54\n[2] Y. Avrithis, N. Tsapatsoulis, and S. Kollias, “Broadcast news parsing using visual\ncues: A robust face detection approach,” in 2000 IEEE International Conference\non Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast\nChanging World of Multimedia (Cat. No. 00TH8532), vol. 3, pp. 1469–1472,\nIEEE, 2000. pages 1\n[3] P. Ekman, Emotion in the Human Face.\nCambridge University Press, 1982.\npages 3\n[4] D. Kollias and S. Zafeiriou, “Training deep neural networks with different\ndatasets in-the-wild: The emotion recognition paradigm,” in 2018 Interna-\ntional Joint Conference on Neural Networks (IJCNN), pp. 1–8, IEEE, 2018. pages\n3\n[5] E. Kensinger, “Remembering emotional experiences: The contribution of va-\nlence and arousal,” vol. 15, pp. 241–51, 02 2004. pages 4\n[6] D. Kollias, G. Marandianos, A. Raouzaiou, and A.-G. Stafylopatis, “Interweav-\ning deep learning and semantic techniques for emotion analysis in human-\nmachine interaction,” in 2015 10th International Workshop on Semantic and So-\ncial Media Adaptation and Personalization (SMAP), pp. 1–6, IEEE, 2015. pages\n4\n[7] D. Kollias, A. Tagaris, and A. Stafylopatis, “On line emotion detection using\nretrainable deep neural networks,” in 2016 IEEE Symposium Series on Compu-\ntational Intelligence (SSCI), pp. 1–8, IEEE, 2016. pages 4\n[8] D. Kollias, M. Yu, A. Tagaris, G. Leontidis, A. Stafylopatis, and S. Kollias, “Adap-\ntation and contextualization of deep neural network models,” in 2017 IEEE\nSymposium Series on Computational Intelligence (SSCI), pp. 1–8, IEEE, 2017.\npages 4\n[9] G. McKeown, M. Valstar, R. Cowie, M. Pantic, and M. Schroder, “The semaine\ndatabase: Annotated multimodal records of emotionally colored conversations\n56\n\nBIBLIOGRAPHY\nBIBLIOGRAPHY\nbetween a person and a limited agent,” IEEE Transactions on Affective Comput-\ning, vol. 3, pp. 5–17, Jan 2012. pages 5\n[10] E. Douglas-Cowie, R. Cowie, C. Cox, N. Amir, and D. Heylen, “The sensitive\nartiﬁcial listener: an induction technique for generating emotionally coloured\nconversation,” 01 2008. pages 5\n[11] F. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne, “Introducing the recola\nmultimodal corpus of remote collaborative and affective interactions,” 2013.\npages 5\n[12] “Sewa database.” pages 6\n[13] J. Kossaiﬁ, G. Tzimiropoulos, S. Todorovic, and M. Pantic, “Afew-va database\nfor valence and arousal estimation in-the-wild,” Image Vision Comput., vol. 65,\npp. 23–36, Sept. 2017. pages 6\n[14] D. Kollias, M. A. Nicolaou, I. Kotsia, G. Zhao, and S. Zafeiriou, “Recognition\nof affect in the wild using deep neural networks,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition Workshops, pp. 26–33,\n2017. pages 7, 18, 19, 20, 21, 22, 24, 35, 45, 46, 49, 54\n[15] D. Kollias, P. Tzirakis, M. A. Nicolaou, A. Papaioannou, G. Zhao, B. Schuller,\nI. Kotsia, and S. Zafeiriou, “Deep affect prediction in-the-wild:\nAff-wild\ndatabase and challenge, deep architectures, and beyond,” International Jour-\nnal of Computer Vision, vol. 127, no. 6-7, pp. 907–929, 2019. pages 7, 19, 20,\n21, 22, 35, 45, 46, 54\n[16] D. Kollias, A. Tagaris, A. Stafylopatis, S. Kollias, and G. Tagaris, “Deep neu-\nral architectures for prediction in healthcare,” Complex & Intelligent Systems,\nvol. 4, no. 2, pp. 119–131, 2018. pages 7\n[17] N. Simou and S. Kollias, “Fire: A fuzzy reasoning engine for impecise knowl-\nedge,” in K-Space PhD Students Workshop, Berlin, Germany, vol. 14, Citeseer,\n2007. pages 7\n[18] A. Tagaris, D. Kollias, and A. Stafylopatis, “Assessment of parkinsons disease\nbased on deep neural networks,” in International Conference on Engineering\nApplications of Neural Networks, pp. 391–403, Springer, 2017. pages 7\n[19] N. Simou, T. Athanasiadis, G. Stoilos, and S. Kollias, “Image indexing and\nretrieval using expressive fuzzy description logics,” Signal, Image and Video\nProcessing, vol. 2, no. 4, pp. 321–335, 2008. pages 7\n[20] I. Kollia, N. Simou, A. Stafylopatis, and S. Kollias, “Semantic image analysis\nusing a symbolic neural architecture,” Image Analysis & Stereology, vol. 29,\nno. 3, pp. 159–172, 2010. pages 7\n57\n\nBIBLIOGRAPHY\nBIBLIOGRAPHY\n[21] A. Tagaris, D. Kollias, A. Stafylopatis, G. Tagaris, and S. Kollias, “Machine learn-\ning for neurodegenerative disorder diagnosissurvey of practices and launch\nof benchmark dataset,” International Journal on Artiﬁcial Intelligence Tools,\nvol. 27, no. 03, p. 1850011, 2018. pages 7\n[22] I. K. B. G. I. Horrocks, “Answering queries over owl ontologies with sparql,”\n2011. pages 7\n[23] B. Glimm, Y. Kazakov, I. Kollia, and G. B. Stamou, “Using the tbox to optimise\nsparql queries.,” Description Logics, vol. 1014, pp. 181–196, 2013. pages 7\n[24] A. Deshpande, “A beginner’s guide to understanding convolutional neural net-\nworks,” 2016. pages 7\n[25] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted boltzmann\nmachines vinod nair,” 06 2010. pages 8, 13\n[26] C. Olah, “Understanding lstm networks,” 2015. pages 9, 10\n[27] S. Kostadinov, “Understanding gru networks,” 2017. pages 11\n[28] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,\nA. Courville, and Y. Bengio, “Generative adversarial nets,” in Advances in Neural\nInformation Processing Systems 27 (Z. Ghahramani, M. Welling, C. Cortes, N. D.\nLawrence, and K. Q. Weinberger, eds.), pp. 2672–2680, Curran Associates, Inc.,\n2014. pages 11\n[29] D. Kollias, S. Cheng, M. Pantic, and S. Zafeiriou, “Photorealistic facial synthesis\nin the dimensional affect space,” in Proceedings of the European Conference on\nComputer Vision (ECCV), pp. 0–0, 2018. pages 12\n[30] G. Caridakis, A. Raouzaiou, K. Karpouzis, and S. Kollias, “Synthesizing gesture\nexpressivity based on real sequences,” in Workshop on Multimodal Corpora.\nFrom Multimodal Behaviour Theories to Usable Models. 5th International Con-\nference on Language Resources and Evaluation (LREC2006), pp. 19–23, 2006.\npages 12\n[31] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Generating faces\nfor affect analysis,” arXiv preprint arXiv:1811.05027, 2018. pages 12\n[32] A.\nRadford,\nL.\nMetz,\nand\nS.\nChintala,\n“Unsupervised\nrepresentation\nlearning with deep convolutional generative adversarial networks,” CoRR,\nvol. abs/1511.06434, 2015. pages 12, 36\n[33] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network\ntraining by reducing internal covariate shift,” CoRR, vol. abs/1502.03167,\n2015. pages 13\n[34] A. L. Maas, A. Y. Hannun, and A. Y. Ng, “Rectiﬁer nonlinearities improve neural\nnetwork acoustic models,” in in ICML Workshop on Deep Learning for Audio,\nSpeech and Language Processing, 2013. pages 13\n58\n\nBIBLIOGRAPHY\nBIBLIOGRAPHY\n[35] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen,\n“Improved techniques for training gans,” CoRR, vol. abs/1606.03498, 2016.\npages 13, 39\n[36] D. Kollias and S. Zafeiriou, “A multi-task learning & generation frame-\nwork: Valence-arousal, action units & primary expressions,” arXiv preprint\narXiv:1811.07771, 2018. pages 14, 54\n[37] A. Krizhevsky, V. Nair, and G. Hinton, “Cifar-10 (canadian institute for ad-\nvanced research),” pages 14\n[38] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between capsules,” in\nAdvances in Neural Information Processing Systems 30 (I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, eds.),\npp. 3856–3866, Curran Associates, Inc., 2017. pages 15\n[39] A. Jaiswal, W. AbdAlmageed, Y. Wu, and P. Natarajan, “Capsulegan: Gener-\native adversarial capsule network,” CoRR, vol. abs/1802.06167, 2018. pages\n16, 17, 55\n[40] D. Kollias and S. Zafeiriou, “Expression, affect, action unit recognition: Aff-\nwild2, multi-task learning and arcface,” arXiv preprint arXiv:1910.04855,\n2019. pages 17, 54\n[41] D. Kollias and S. Zafeiriou, “A multi-component cnn-rnn approach for di-\nmensional emotion recognition in-the-wild,” arXiv preprint arXiv:1805.01452,\n2018. pages 17\n[42] D. Kollias and S. Zafeiriou, “Exploiting multi-cnn features in cnn-rnn based di-\nmensional emotion recognition on the omg in-the-wild dataset,” arXiv preprint\narXiv:1910.01417, 2019. pages 17\n[43] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-\nscale image recognition,” CoRR, vol. abs/1409.1556, 2014. pages 17\n[44] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recog-\nnition,” CoRR, vol. abs/1512.03385, 2015. pages 18\n[45] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of the devil in\nthe details: Delving deep into convolutional nets,” CoRR, vol. abs/1405.3531,\n2014. pages 20, 35\n[46] S. Albanie and A. Vedaldi, “Learning grimaces by watching tv,” in Proceedings\nof the British Machine Vision Conference (BMVC), 2016. pages 20\n[47] S. Dodge and L. Karam, “Understanding how image quality affects deep neural\nnetworks,” in 2016 Eighth International Conference on Quality of Multimedia\nExperience (QoMEX), pp. 1–6, June 2016. pages 22\n[48] “Download videos from youtube.” pages 22\n59\n\nBIBLIOGRAPHY\nBIBLIOGRAPHY\n[49] “Documentation of ffmpeg (ffprobe command).” pages 23, 41\n[50] J. Thomas, “Openshot video editor,” 2008. pages 24\n[51] I. Kollia, N. Simou, G. Stamou, and A. Stafylopatis, “Interweaving knowledge\nrepresentation and adaptive neural networks,” in Workshop on Inductive Rea-\nsoning and Machine Learning on the Semantic Web, 2009. pages 25\n[52] “The menpo project.” pages 28\n[53] P. L. Peres, I. S. Bonatti, and W. C. Borelli, “The linear interpolation method: a\nsampling theorem approach,” Sba: Controle & Automac¸˜ao Sociedade Brasileira\nde Automatica, vol. 14, no. 4, pp. 439–444, 2003. pages 30\n[54] E. Jones, T. Oliphant, P. Peterson, et al., “SciPy: Open source scientiﬁc tools for\nPython used for interp function,” 2001–. [Online; accessed ¡today¿]. pages 30\n[55] P. J. Huber, Robust Estimation of a Location Parameter, pp. 492–518. New York,\nNY: Springer New York, 1992. pages 39\n[56] D. Kollias and S. Zafeiriou, “Aff-wild2: Extending the aff-wild database for\naffect recognition,” arXiv preprint arXiv:1811.07770, 2018. pages 54\n[57] G. Goudelis, K. Karpouzis, and S. Kollias, “Exploring trace transform for robust\nhuman action recognition,” Pattern Recognition, vol. 46, no. 12, pp. 3238–\n3248, 2013. pages 54\n[58] A. D. Doulamis, Y. S. Avrithis, N. D. Doulamis, and S. D. Kollias, “Interactive\ncontent-based retrieval in video databases using fuzzy classiﬁcation and rel-\nevance feedback,” in Proceedings IEEE International Conference on Multimedia\nComputing and Systems, vol. 2, pp. 954–958, IEEE, 1999. pages 54\n[59] K. A. Raftopoulos, S. D. Kollias, D. D. Sourlas, and M. Ferecatu, “On the ben-\neﬁcial effect of noise in vertex localization,” International Journal of Computer\nVision, vol. 126, no. 1, pp. 111–139, 2018. pages 54\n60\n\nAppendices\n61\n\nAppendix A\nLSEPI Checklist\n62\n\nChapter A. LSEPI Checklist\nWhen the word ”data” is used in this project, it is referred to images which con-\ntain human faces. The expressions of the faces in the images, which come from on-\nline sources, are analyzed by deep learning models, in order to predict the emotions.\nSo, the project does contain humans, 68 males and 38 females, which in majority\nthey are adults. These images were generated from videos that were collected on-\nline. As mentioned before, the emotions predictions are based on the observation of\npeoples expressions in those images. Finally, some already existing databases were\npresented in Chapter 2 but none of them was merged with the one that was created\nduring the project.\n63",
    "pdf_filename": "AffWild Net and Aff-Wild Database.pdf"
}