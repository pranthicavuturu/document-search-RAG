{
    "title": "Explainable Human-AI Interaction - A Planning Perspective",
    "context": "",
    "body": "Explainable Human-AI Interaction:\nA Planning Perspective\nSarath Sreedharan, Anagha Kulkarni, Subbarao Kambhampati\nArizona State University\narXiv:2405.15804v1  [cs.AI]  19 May 2024\n\n\nAbstract\nFrom its inception, AI has had a rather ambivalent relationship with humans—swinging\nbetween their augmentation and replacement.\nNow, as AI technologies enter our ev-\neryday lives at an ever increasing pace, there is a greater need for AI systems to work\nsynergistically with humans. One critical requirement for such synergistic human-AI in-\nteraction is that the AI systems be explainable to the humans in the loop. To do this\neffectively, AI agents need to go beyond planning with their own models of the world,\nand take into account the mental model of the human in the loop. Drawing from sev-\neral years of research in our lab, we will discuss how the AI agent can use these mental\nmodels to either conform to human expectations, or change those expectations through\nexplanatory communication.\nWhile the main focus of the book is on cooperative sce-\nnarios, we will point out how the same mental models can be used for obfuscation and\ndeception. Although the book is primarily driven by our own research in these areas, in\nevery chapter, we will provide ample connections to relevant research from other groups.\nKeywords:\nHuman-aware AI Systems, Human-AI Interaction, Explainability, Inter-\npretability, Human-aware planning, Obfuscation\ni\n\nii\n\nContents\nPreface\n1\nAcknowledgements\n5\n1\nIntroduction\n7\n1.1\nHumans & AI Agents: An Ambivalent Relationship . . . . . . . . . . . . . .\n7\n1.2\nExplanations in Humans . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n1.2.1\nWhen and Why do Humans expect explanations from each other?\n.\n9\n1.2.2\nHow do Humans Exchange Explanations? . . . . . . . . . . . . . . .\n10\n1.2.3\n(Why) Should AI Systems be Explainable?\n. . . . . . . . . . . . . .\n11\n1.3\nDimensions of Explainable AI systems . . . . . . . . . . . . . . . . . . . . .\n11\n1.3.1\nUse cases for explanations in Human-AI Interaction\n. . . . . . . . .\n12\n1.3.2\nRequirements on Explanations\n. . . . . . . . . . . . . . . . . . . . .\n12\n1.3.3\nExplanations as Studied in the AI Literature . . . . . . . . . . . . .\n13\n1.3.4\nExplainable AI: The Landscape & The Tribes . . . . . . . . . . . . .\n14\n1.4\nOur Perspective on Human-Aware and Explainable AI Agents . . . . . . . .\n15\n1.4.1\nHow do we make AI agents Human-Aware? . . . . . . . . . . . . . .\n15\n1.4.2\nMental Models in Explainable AI Systems . . . . . . . . . . . . . . .\n15\n1.5\nOverview of this Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n2\nMeasures of Interpretability\n21\n2.1\nPlanning Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n2.2\nModes of Interpretable Behavior\n. . . . . . . . . . . . . . . . . . . . . . . .\n24\n2.2.1\nExplicability\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n2.2.2\nLegibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n2.2.3\nPredictability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n2.3\nCommunication to Improve Interpretability . . . . . . . . . . . . . . . . . .\n27\n2.3.1\nCommunicating Model Information . . . . . . . . . . . . . . . . . . .\n27\n2.4\nOther Considerations in Interpretable Planning . . . . . . . . . . . . . . . .\n28\n2.5\nGeneralizing Interpretability Measures . . . . . . . . . . . . . . . . . . . . .\n30\n2.6\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n3\nExplicable Behavior Generation\n33\n3.1\nExplicable Planning Problem . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n3.2\nModel-Based Explicable Planning . . . . . . . . . . . . . . . . . . . . . . . .\n36\n3.2.1\nPlan Generation through Reconciliation Search . . . . . . . . . . . .\n36\n3.2.2\nPossible Distance Functions . . . . . . . . . . . . . . . . . . . . . . .\n38\n3.3\nModel-Free Explicable Planning . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n3.3.1\nProblem Formulation\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n3.3.2\nLearning Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n3.3.3\nPlan Generation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n3.4\nEnvironment Design for Explicability . . . . . . . . . . . . . . . . . . . . . .\n42\n3.4.1\nProblem Setting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n3.4.2\nFramework for Design for Explicability . . . . . . . . . . . . . . . . .\n44\n3.4.3\nSearch for Optimal Design . . . . . . . . . . . . . . . . . . . . . . . .\n48\n3.4.4\nDemonstration of Environment Design for Explicability\n. . . . . . .\n49\n3.5\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\niii\n\niv\n4\nLegible Behavior\n51\n4.1\nControlled Observability Planning Problem . . . . . . . . . . . . . . . . . .\n51\n4.1.1\nHuman’s Belief Space\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n4.1.2\nComputing Solutions to COPP variants . . . . . . . . . . . . . . . .\n53\n4.1.3\nVariants of COPP\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\n4.2\nGoal Legibility\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\n4.2.1\nComputing Goal Legible Plans . . . . . . . . . . . . . . . . . . . . .\n57\n4.3\nPlan Legibility\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n4.3.1\nComputing Plan Legible Plans\n. . . . . . . . . . . . . . . . . . . . .\n59\n4.4\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n5\nExplanation as Model Reconciliation\n61\n5.1\nModel-Reconciliation as Explanation . . . . . . . . . . . . . . . . . . . . . .\n61\n5.1.1\nThe Fetch Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n5.2\nExplanation Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\n5.2.1\nExplanation types\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n5.2.2\nDesiderata for Explanations as Discussed in Social Sciences . . . . .\n68\n5.2.3\nModel Space Search for Minimal Explanations\n. . . . . . . . . . . .\n69\n5.3\nApproximate Explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n5.3.1\nExplicit Contrastive Explanations\n. . . . . . . . . . . . . . . . . . .\n73\n5.3.2\nApproximate MCE . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n5.4\nUser Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n5.5\nOther Explanatory Methods . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n5.6\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n6\nAcquiring Mental Models for Explanations\n79\n6.1\nThe Urban Search and Reconnaissance Domain . . . . . . . . . . . . . . . .\n79\n6.2\nModel Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n6.3\nModel-Free Explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\n6.4\nAssuming Prototypical Models\n. . . . . . . . . . . . . . . . . . . . . . . . .\n88\n6.5\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\n7\nBalancing Communication and Behavior\n93\n7.1\nModified USAR Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n7.2\nBalancing Explanation and Explicable Behavior Generation . . . . . . . . .\n94\n7.2.1\nGenerating Balanced Plans . . . . . . . . . . . . . . . . . . . . . . .\n96\n7.2.2\nStage of Interaction and Epistemic Side Effects . . . . . . . . . . . .\n99\n7.2.3\nOptimizing for Explicability of the Plan . . . . . . . . . . . . . . . . 100\n7.3\nBalancing Communication and Behavior For other Measures . . . . . . . . . 101\n7.4\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n8\nExplaining in the Presence of Vocabulary Mismatch\n105\n8.1\nRepresentation of Robot Model . . . . . . . . . . . . . . . . . . . . . . . . . 105\n8.2\nSetting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n8.2.1\nLocal Approximation of Planning Model . . . . . . . . . . . . . . . . 107\n8.2.2\nMontezuma’s Revenge . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n8.3\nAcquiring Interpretable Models . . . . . . . . . . . . . . . . . . . . . . . . . 109\n8.4\nQuery Specific Model Acquisition . . . . . . . . . . . . . . . . . . . . . . . . 110\n8.4.1\nExplanation Generation: . . . . . . . . . . . . . . . . . . . . . . . . . 111\n8.4.2\nIdentifying Explanations through Sample-Based Trials . . . . . . . . 112\n\nv\n8.5\nExplanation Confidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n8.6\nHandling Uncertainty in Concept Mapping\n. . . . . . . . . . . . . . . . . . 114\n8.7\nAcquiring New Vocabulary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n8.8\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n9\nObfuscatory Behavior and Deceptive Communication\n117\n9.1\nObfuscation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n9.1.1\nGoal Obfuscation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n9.1.2\nSecure Goal Obfuscation . . . . . . . . . . . . . . . . . . . . . . . . . 120\n9.1.3\nPlan Obfuscation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n9.1.4\nDeception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n9.2\nMulti-Observer Simultaneous obfuscation and legibility . . . . . . . . . . . . 123\n9.2.1\nMixed-Observer Controlled Observability Planning Problem . . . . . 123\n9.2.2\nPlan Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n9.3\nLies\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n9.3.1\nWhen Should an Agent Lie? . . . . . . . . . . . . . . . . . . . . . . . 129\n9.3.2\nHow can an Agent Lie?\n. . . . . . . . . . . . . . . . . . . . . . . . . 129\n9.3.3\nImplications of Lies\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n9.4\nBibliographical Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n10 Applications\n133\n10.1 Collaborative Decision-Making\n. . . . . . . . . . . . . . . . . . . . . . . . . 133\n10.2 Humans as Actors\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n10.2.1 RADAR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n10.2.2 MA-RADAR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n10.2.3 RADAR-X\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n10.3 Model Transcription Assistants . . . . . . . . . . . . . . . . . . . . . . . . . 138\n10.3.1 D3WA+ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n10.4 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n11 Conclusion\n143\nBibliography\n146\nAuthors’ Biographies\n155\n\nvi\n\nPreface\nArtificial Intelligence (AI) systems that interact with us the way we interact with each\nother have long typified Hollywood’s image, whether you think of HAL in “2001: A Space\nOdyssey,” Samantha in “Her,” or Ava in “Ex Machina.” It thus might surprise people that\nmaking systems that interact, assist or collaborate with humans has never been high on\nthe technical agenda.\nFrom its beginning, AI has had a rather ambivalent relationship with humans. The\nbiggest AI successes have come either at a distance from humans (think of the “Spirit” and\n“Opportunity” rovers navigating the Martian landscape) or in cold adversarial faceoffs (the\nDeep Blue defeating world chess champion Gary Kasparov, or AlphaGo besting Lee Sedol).\nIn contrast to the magnetic pull of these “replace/defeat humans” ventures, the goal of\ndesigning AI systems that are human-aware, capable of interacting and collaborating with\nhumans and engendering trust in them, has received much less attention.\nMore recently, as AI technologies started capturing our imaginations, there has been\na conspicuous change — with “human” becoming the desirable adjective for AI systems.\nThere are so many variations — human-centered, human-compatible, human-aware AI,\netc.\n— that there is almost a need for a dictionary of terms.\nSome of this interest\narose naturally from a desire to understand and regulate the impacts of AI technologies\non people. Of particular interest for us are the challenges and impacts of AI systems\nthat continually interact with humans — as decision support systems, personal assistants,\nintelligent tutoring systems, robot helpers, social robots, AI conversational companions,\netc.\nTo be aware of humans, and to interact with them fluently, an AI agent needs to exhibit\nsocial intelligence. Designing agents with social intelligence received little attention when\nAI development was focused on autonomy rather than coexistence. Its importance for\nhumans cannot be overstated, however.\nAfter all, evolutionary theory shows that we\ndeveloped our impressive brains not so much to run away from lions on the savanna but\nto get along with each other.\nA cornerstone of social intelligence is the so-called “theory of mind” — the ability to\nmodel mental states of humans we interact with. Developmental psychologists have shown\n(with compelling experiments like the Sally-Anne test) that children, with the possible\nexception of those on the autism spectrum, develop this ability quite early.\nSuccessful AI agents need to acquire, maintain and use such mental models to modulate\ntheir own actions. At a minimum, AI agents need approximations of humans’ task and\ngoal models, as well as the human’s model of the AI agent’s task and goal models. The\nformer will guide the agent to anticipate and manage the needs, desires and attention of\nhumans in the loop (think of the prescient abilities of the character Radar on the TV series\n“M*A*S*H*”), and the latter allow it to act in ways that are interpretable to humans\n— by conforming to their mental models of it — and be ready to provide customized\nexplanations when needed.\n1\n\n2\nWith the increasing use of AI-based decision support systems in many high-stakes ar-\neas, including health and criminal justice, the need for AI systems exhibiting interpretable\nor explainable behavior to humans has become quite critical. The European Union’s Gen-\neral Data Protection Regulation posits a right to contestable explanations for all machine\ndecisions that affect humans (e.g., automated approval or denial of loan applications).\nWhile the simplest form of such explanations could well be a trace of the reasoning steps\nthat lead to the decision, things get complex quickly once we recognize that an explanation\nis not a soliloquy and that the comprehensibility of an explanation depends crucially on\nthe mental states of the receiver. After all, your physician gives one kind of explanation\nfor her diagnosis to you and another, perhaps more technical one, to her colleagues.\nProvision of explanations thus requires a shared vocabulary between AI systems and\nhumans, and the ability to customize the explanation to the mental models of humans.\nThis task becomes particularly challenging since many modern data-based decision-making\nsystems develop their own internal representations that may not be directly translatable to\nhuman vocabulary. Some emerging methods for facilitating comprehensible explanations\ninclude explicitly having the machine learn to translate explanations based on its internal\nrepresentations to an agreed-upon vocabulary.\nAI systems interacting with humans will need to understand and leverage insights\nfrom human factors and psychology. Not doing so could lead to egregious miscalculations.\nInitial versions of Tesla’s auto-pilot self-driving assistant, for example, seemed to have\nbeen designed with the unrealistic expectation that human drivers can come back to full\nalertness and manually override when the self-driving system runs into unforeseen modes,\nleading to catastrophic failures. Similarly, the systems will need to provide an appropriate\nemotional response when interacting with humans (even though there is no evidence, as\nyet, that emotions improve an AI agent’s solitary performance). Multiple studies show\nthat people do better at a task when computer interfaces show appropriate affect. Some\nhave even hypothesized that part of the reason for the failure of Clippy, the old Microsoft\nOffice assistant, was because it had a permanent smug smile when it appeared to help\nflustered users.\nAI systems with social intelligence capabilities also produce their own set of ethical\nquandaries. After all, trust can be weaponized in far more insidious ways than a rampag-\ning robot. The potential for manipulation is further amplified by our own very human\ntendency to anthropomorphize anything that shows even remotely human-like behavior.\nJoe Weizenbaum had to shut down Eliza, history’s first chatbot, when he found his staff\npouring their hearts out to it; and scholars like Sherry Turkle continue to worry about the\nartificial intimacy such artifacts might engender. Ability to manipulate mental models\ncan also allow AI agents to engage in lying or deception with humans, leading to a form\nof “head fakes” that will make today’s deep fakes tame by comparison. While a certain\nlevel of “white lies” are seen as the glue for human social fabric, it is not clear whether\nwe want AI agents to engage in them.\nAs AI systems increasingly become human-aware, even quotidian tools surrounding\nus will start gaining mental-modeling capabilities. This adaptivity can be both a boon\nand a bane. While we talked about the harms of our tendency to anthropomorphize AI\nartifacts that are not human-aware, equally insidious are the harms that can arise when\nwe fail to recognize that what we see as a simple tool is actually mental-modeling us.\n\n3\nIndeed, micro-targeting by social media can be understood as a weaponized version of\nsuch manipulation; people would be much more guarded with social media platforms if\nthey realized that those platforms are actively profiling them.\nGiven the potential for misuse, we should aim to design AI systems that must under-\nstand human values, mental models and emotions, and yet not exploit them with intent to\ncause harm. In other words, they must be designed with an overarching goal of beneficence\nto us.\nAll this requires a meaningful collaboration between AI and humanities — including\nsociology, anthropology and behavioral psychology. Such interdisciplinary collaborations\nwere the norm rather than the exception at the beginning of the AI field and are coming\nback into vogue.\nFormidable as this endeavor might be, it is worth pursuing. We should be proactively\nbuilding a future where AI agents work along with us, rather than passively fretting about\na dystopian one where they are indifferent or adversarial. By designing AI agents to be\nhuman-aware from the ground up, we can increase the chances of a future where such\nagents both collaborate and get along with us.\nThis book then is a step towards designing such a future.\nWe focus in particular\non recent research efforts on making AI systems explainable. Of particular interest are\nsettings where the AI agents make a sequence of decisions in support of their objectives,\nand the humans in the loop get to observe the resulting behavior. We consider techniques\nfor making this behavior explicable to the humans out of the box, or after an explanation\nfrom the AI agent. These explanations are modeled as reconciliations of the mental models\nthe humans have of the AI agents’ goals and capabilities. The central theme of many of\nthese techniques is reasoning with the mental models of the humans in the loop. While\nmuch of our focus is on cooperative scenarios, we also discuss how the same techniques\ncan be adapted to support lies and deception in adversarial scenarios. In addition to the\nformal frameworks and algorithms, this book also discusses several applications of these\ntechniques in decision-support and human-robot interaction scernarios. While we focus\non the developments from our group, we provide ample context of related developments\nacross several research groups and areas of AI currently focusing on the explainability of\nAI systems.\n\n4\n\nAcknowledgements\nThis book is largely the result of a strand of research conducted at the Yochan Lab at\nArizona State University over the last five years. The authors would like to express their\nsincere thanks to multiple past and present members of the Yochan group as well as\nexternal collaborators for their help and role in the development of these ideas.\nFirst and foremost, we would like to thank Tathagata Chakraborti, who was the driving\nforce behind multiple topics and frameworks described in this paper. Tathagata could not\nactively take part in the writing of this book, but his imprints are there throughout the\nmanuscript.\nOther Yochan group members who played a significant role in the development of\nthe ideas described in this book include Sailik Sengupta (currently at Amazon Science),\nSachin Grover and Yantian Zha. Siddhant Bhambri and Karthik Valmeekam read a draft\nof the book and gave comments. We would also like to thank Yu (Tony) Zhang and Satya\nGautam Vadlamudi, who were post-doctoral scholars at Yochan during the initial stages\nof this work.\nDavid Smith (formerly of NASA AMES) and Hankz Hankui Zhuo (of Sun-Yat Sen\nUniversity) have been frequent visitors to our group; David in particular is a co-author on\nmultiple papers covered in this book. Matthias Scheutz of Tufts has been a long standing\ncollaborator; an ONR MURI project with him a decade back was the original motivation\nfor our interest in human-AI teaming. Our colleague Nancy Cooke, an expert in human-\nfactors and human-human teaming, has been a co-investigator on several of the projects\nwhose results are included in this book.\nOther external collaborators of the work described here include Siddharth Srivastava\nof ASU, Christian Muise (formerly of IBM AI Research, currently at Queens University,\nCanada) and Sarah Keren of Technion.\nAdditionally, we have benefited from our conversations on Human-AI interaction (as\nwell as encouragement) from multiple colleagues within and outside ASU, including: Dan\nWeld (U. Washington), Barbara Grosz (Harvard), Manuela Veloso (J.P. Morgan Research),\nPat Langley (ISLE), Ece Kamar (MSR), Been Kim (Google), David Aha (NRL), Eric\nHorvitz (Microsoft), and Julie Shah (MIT).\nMuch of the research reported here has been supported over the years by generous sup-\nport from multiple federal funding agencies. We would like to particularly thank the Office\nof Naval Research–and the program managers Behzad Kamgar-Parsi, Tom McKenna, Jeff\nMorrison, Marc Steinberg and John Tangney for their sustained support. Thanks are also\ndue to Benjamin Knott formerly of AFOSR, Laura Steckman of AFOSR, and Purush Iyer\nof Army Research Labs for their support and encouragement.\nParts of the material in this book is drawn from our refereed research papers published\nin several venues; we cite all the sources in the bibliography. Parts of the preface are drawn\nfrom a column on human-aware AI systems that first appeared in The Hill.\n5\n\n6\n\nChapter 1\nIntroduction\nArtificial Intelligence, the discipline many of us call our intellectual home, is suddenly\nhaving a rather huge cultural moment. It is hard to turn anywhere without running into\nmentions of AI technology and hype about its expected positive and negative societal\nimpacts. AI has been compared to fire and electricity, and commercial interest in the AI\ntechnologies has sky rocketed. Universities – even high schools – are rushing to start new\ndegree programs or colleges dedicated to AI. Civil society organizations are scrambling to\nunderstand the impact of AI technology on humanity, and governments are competing to\nencourage or regulate AI research and deployment.\nThere is considerable hand-wringing by pundits of all stripes on whether in the future,\nAI agents will get along with us or turn on us. Much is being written about the need to\nmake AI technologies safe and delay the “doomsday”. We believe that as AI researchers,\nwe are not (and cannot be) passive observers. It is our responsibility to design agents\nthat can and will get along with us. Making such human-aware AI agents, however poses\nseveral foundational research challenges that go beyond simply adding user interfaces\npost facto. In particular, human-aware AI systems need to be designed such that their\nbehavior is explainable to the humans interacting with them. This book describes some\nof the state-of-the-art approaches in making AI systems explainable.\n1.1\nHumans & AI Agents: An Ambivalent Relationship\nIn this book we focus on human-aware AI systems—goal directed autonomous systems that\nare capable of effectively interacting, collaborating and teaming with humans. Although\ndeveloping such systems seems like a rather self-evidently fruitful enterprise, and popular\nimaginations of AI, dating back to HAL, almost always assume we already do have human-\naware AI systems technology, little of the actual energies of the AI research community\nhave gone in this direction.\nFrom its inception, AI has had a rather ambivalent relationship to humans—swinging\nbetween their augmentation and replacement. Most high profile achievements of AI have\neither been far away from the humans—think Spirit and Opportunity exploring Mars; or\nin a decidedly adversarial stance with humans, be it Deep Blue, AlphaGo or Libratus.\nResearch into effective ways of making AI systems interact, team and collaborate with\nhumans has received significantly less attention. It is perhaps no wonder that many lay\npeople have fears about AI technology!\nThis state of affairs is a bit puzzling given the rich history of early connections between\nAI and psychology. Part of the initial reluctance to work on these issues had to do with the\nworry that focusing on AI systems working with human might somehow dilute the grand\ngoals of the AI enterprise, and might even lead to temptations of “cheating,” with most\nof the intelligent work being done by the humans in the loop. After all, prestidigitation\n7\n\n8\nIntroduction\nhas been a concern since the original mechanical turk. Indeed, much of the early work on\nhuman-in-the-loop AI systems mostly focused on using humans as a crutch for making up\nthe limitations of the AI systems [Allen, 1994]. In other words, early AI had humans be\n“AI-aware” (rather than AI be “human-aware”).\nNow, as AI systems are maturing with increasing capabilities, the concerns about\nthem depending on humans as crutches are less severe. We would also argue that focus\non humans in the loop doesn’t dilute the goals of AI enterprise, but in fact broadens them\nin multiple ways. After all, evolutionary theories tell us that humans may have developed\nthe brains they have, not so much to run away from the lions of the savanna or tigers of\nBengal but rather to effectively cooperate and compete with each other. Psychological\ntests such as the Sally Anne Test [Wimmer and Perner, 1983] demonstrate the importance\nof such social cognitive abilities in the development of collaboration abilities in children.\nSome branches of AI, aimed at specific human-centric applications, such as intelligent\ntutoring systems [VanLehn, 2006], and social robotics [Breazeal, 2004, 2003, Scassellati,\n2002], did focus on the challenges of human-aware AI systems for a long time.\nIt is\ncrucial to note however that human-aware AI systems are needed in a much larger class\nof quotidian applications beyond those.\nThese include human-aware AI assistants for\nmany applications where humans continue to be at the steering wheel, but will need\nnaturalistic assistance from AI systems—akin to what they can expect from a smart human\nsecretary. Increasingly, as AI systems become common-place, human-AI interaction will\nbe the dominant form of human-computer interaction [Amershi et al., 2019].\nFor all these reasons and more, human-aware AI has started coming to the forefront of\nAI research of late. Recent road maps for AI research, including the 2016 JASON report1\nand the 2016 White House OSTP report2 emphasize the need for research in human-aware\nAI systems. The 2019 White House list of strategic R&D priorities for AI lists “developing\neffective methods for human-AI collaboration” at the top of the list of priorities3. Human-\nAware AI was the special theme for the 2016 International Joint Conference on AI (with\nthe tagline “why intentionally design a dystopian future and spend time being paranoid\nabout it?”); it has been a special track at AAAI since 2018.\n1.2\nExplanations in Humans\nSince our books is about explainable AI systems, it is useful to start with a broad overview\nof explainability and explanations in humans.\n1https://fas.org/irp/agency/dod/jason/ai-dod.pdf\n2https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/\nmicrosites/ostp/NSTC/national_ai_rd_strategic_plan.pdf\n3https://www.whitehouse.gov/wp-content/uploads/2019/06/National-AI-Research-and-Development-\nStrategic-Plan-2019-Update-June-2019.pdf\n\n1.2 Explanations in Humans\n9\nFig. 1.1: Architecture of an intelligent agent that takes human mental models into account. All portions\nin yellow are additions to the standard agent architecture, that are a result of the agent being human-aware.\nMR\nh is the mental model the human has of the AI agent’s goals and capabilities and MH\nr is the (mental)\nmodel the AI agent has of the human’s goal and capabilities (see the section on Mental Models in Human-\nAware AI)\n1.2.1\nWhen and Why do Humans expect explanations from each other?\nTo understand the different use cases for explanations offered by AI systems, it is useful\nto survey the different scenarios where humans ask for explanations from each other:\nWhen they are confused and or surprised by the behavior of the other person\nPeople expect explanations when the behavior from the other person is not what they\nexpected–and is thus inexplicable. It is worth noting that this confusion at the other\nperson’s behavior is not predicated on that person’s behavior being incorrect or in-\noptimal. We may well be confused/surprised when a toddler, for example, makes\nan optimal chess move. In other words, the need for explanations arises when the\nother person’s behavior is not consistent with the model we have of the other person.\nExplanations are thus meant to reconcile these misaligned expectations.\nWhen they want to teach the other person We offer explanations either to make\nthe other person understand the real rationale behind a decision or to convince\nthe other person that our decision in this case is not a fluke. Explanatory dialog\nallows either party to correct the mental model of the other party. The explanations\nbecome useful in localizing the fault, if any, in the other person’s understanding our\ndecision.\nNote that the need for explanation is thus dependent on one person’s model of the\nother person’s capabilities/reasoning. Mental models thus play a crucial part in offering\ncustomized explanations. Indeed, a doctor explains her diagnostic decision to her patient\n\n10\nIntroduction\nin one way and to her peers in a different (possibly more “jargon-filled” way), because\nshe intuitively understands the levels of abstraction of the mental models they have of\ndiseases and diagnoses.\nIt is also worth considering how the need for explanations reduces over time. It natu-\nrally reduces as the mental models we have of the other agent gets better aligned with that\nother agent’s capabilities, reducing any residual surprise at their behavior. This explains\nthe ideal of “wordless collaboration” between long-time collaborators.\nThe need for explanations is also modulated by the trust we have in the other person.\nTrust can be viewed as our willingness to put ourselves in a position of vulnerability.\nWhen we trust the other person, even when their behavior/decision is inexplicable to us,\nwe might defer our demands for any explanations from that person. This explains why\nwe ask fewer explanations from people we trust.\n1.2.2\nHow do Humans Exchange Explanations?\nWe now turn to a qualitative understanding of the ways in which humans exchange ex-\nplanations, with a view to gleaning lessons for explanations in the human-AI interaction.\nWhile explanations might occur in multiple modalities, we differentiate two broad types:\nPointing explanations and Symbolic Explanations. A given explanatory interaction might\nbe interspersed with both types of explanations.\nPointing (Tacit) Explanations These are the type of explanations where the main\nexplanation consists of pointing to some specific features of the object that both the\nexplainer and explainee can see. This type of explanations may well be the only\nfeasible one to exchange when the agents share little beyond what they perceive in\ntheir immediate environment. Pointing explanations can get quite unwieldy (both in\nterms of the communication bandwidth and the cognitive load for processing them)–\nespecially when explaining sequential decisions (such as explaining why you took an\nearlier flight than the one the other person expected)–as they will involve pointing\nto the relevant regions of the shared “video history” between the agents (or, more\ngenerally, space time signal tubes).\nSymbolic (Explicit) Explanations These involve exchanging explanations in a sym-\nbolic vocabulary. Clearly, these require that the explainer and explainee share a\nsymbolic vocabulary to begin with.\nTypically, pointing explanations are used for tacit knowledge tasks, and symbolic ex-\nplanations are used for explicit knowledge tasks. Interestingly, over time, people tend\nto develop symbolic vocabulary even for exchanging explanations over tacit knowledge\ntasks. Consider, for example, terms such as pick and roll in basket ball, that are used as\na shorthand for a complex space time tube–even though the overall task is largely a tacit\nknowledge one.\nThe preference for symbolic explanations is not merely because of their compactness,\n\n1.3 Dimensions of Explainable AI systems\n11\nbut also because they significantly reduce the cognitive load on the receiver. This prefer-\nence seems to exist despite the fact that the receiver may likely have to recreate in their\nown mind the “video” (space time signal tube) versions of symbolic explanations within\ntheir own minds.\n1.2.3\n(Why) Should AI Systems be Explainable?\nBefore we delve into explainability in AI systems, we have to address the view point that\nexplainability from AI systems is largely unnecessary. Some have said, for example, that\nAI systems–such as those underlying Facebook–routinely make millions of decisions/rec-\nommendations (of the “you might be interested in seeing these pages” variety) and no users\nask for explanations. Some even take the view that since AI systems might eventually be\n“more intelligent” than any mere mortal human, requiring them to provide explanations\nfor their (obviously correct) decisions unnecessarily hobbles them. Despite this, there are\nmultiple reasons why we want AI systems to be explainable\n• Since humans do expect explanations from each other, a naturalistic human-AI\ninteraction requires that AI systems be explainable\n• Contestability of decisions is an important part of ensuring that the decisions are\nseen to be fair and transparent, thus engendering trust in humans. If AI systems\nmake high stakes decisions, they too need to be contestable.\n• Since there is always an insurmountable gap between the true preferences of humans\nand the AI system’s estimate of those preferences, an explanatory dialog allows for\nhumans to “teach” the AI systems their true preferences in a demand-driven fashion.\n• Given that AI systems–especially those that are trained purely from raw data–may\nhave unanticipated failure modes, explanations for their decisions often help the\nhumans get a better sense of these failure modes. As a case in point, recently, it has\nbeen reported that when some Facebook users saw a video of an African male in a\nquotidian situation, the system solicitously asked the users Do you like to see more\nprimate videos? As egregious as this “heads-up” of the failure mode of the underlying\nsystem sounds, it can be more insidious when the system just silently acts on the\ndecisions arrived at through those failure modes, such as, for example, inexplicably\nfilling the user’s feeds with a slight uptick of primate videos. Explanations are thus a\nway for us to catch failure modes of these alien intelligences that we are increasingly\nsurrounded by.\n1.3\nDimensions of Explainable AI systems\nNow that we have reviewed how explanations and explainability play a part in human-\nhuman interactions, we turn to explainability in human-AI interactions. We will specif-\nically look at the use cases for explanations in human-AI interaction, some desirable\nrequirements on explanations, and an overview of the ongoing research on explainable AI\nsystems.\n\n12\nIntroduction\n1.3.1\nUse cases for explanations in Human-AI Interaction\nWhen humans are in the loop with AI systems, they might be playing a variety of different\nroles. Being interpretable to one human in one specific role may or may not translate to\ninterpretability for other humans in other roles. Perhaps the most popular role considered\nin the explainable AI literature to-date is humans as debuggers trying to flag and correct\nan AI system’s behavior. In fact, much of the explainable machine learning research has\nbeen focused on this type of debugging role for humans. Given that the humans in the\nloop here have invested themselves into debugging the system, they are willing to go into\nthe land of the AI agents, rather than expect them to come into theirs. This lowers the\npremium on comprehensibility of explanations.\nNext we can have humans as observers of the robot’s behavior – either in a peer to\npeer, or student/teacher setting. The observer might be a lay person who doesn’t have\naccess to the robot’s model of the task; or an expert one who does.\nFinally, the human might be a collaborator–who actively takes part in completing a\njoint task with the robot.\nNo matter the role of the human, one important issue is whether the interaction\nbetween the human and the robot is a one-off one (i.e., they only interact once in the\ncontext of that class of tasks) or a longitudinal one (where the human interacts with\nthe same robot over extended periods). In this latter case, the robot can engender trust\nin the human through its behavior, which, in turn reduces the need for interpretability.\nIn particular, the premium on interpretability of the behavior itself is reduced when the\nhumans develop trust over the capabilities and general beneficence of the robot.\n1.3.2\nRequirements on Explanations\nThere are a variety of requirements that can be placed on explanations that an AI agent\ngives the human in the loop:\nComprehensibility: The explanation should be comprehensible to the human in the\nloop. This not only means that it should be in terms that the human can under-\nstand, but should not pose undue cognitive load (i.e., expect unreasonable inferential\ncapabilities).\nCustomization: The explanations should be in a form and at a level that is accessible\nto the receiving party (explainee).\nCommunicability: The explanation should be easy to exchange. For example, symbolic\nexplanations are much easier than pointing explanations (especially in sequential\ndecision problems when they have to point to space time signal tubes).\nSoundness: This is the guarantee from the AI agent that this explanation is really the\nreason behind its decision. Such a guarantee also implicitly implies that the agent\nwill stand behind the explanation–and that the decision will change if the condi-\ntions underlying the explanation are falsified. For example, if the explanation for a\n\n1.3 Dimensions of Explainable AI systems\n13\nloan denial is that the applicant has no collateral, and the applicant then produces\ncollateral, it is fairly expected that the loan denial decision will be reversed.\nSatisfctoriness: This aims to measure how satisfied the end user is with the explanation\nprovided. While incomprehensible, uncustomized and poorly communicated expla-\nnations will likely be unsatisfatory to the user, it is also possible that the users are\nsatisfied with misleading explanations that align well with what they want to hear.\nIndeed, making explanations satisfactory to the users is a slippery slope. Imagine\nthe end-user explanations of the kind that the EU GDPR regulations require being\nprovided by a system with a GPT-3 back-end generating plausible explanations that\nare likely to satisfy the users. This is why it is important for systems not to take an\n“end to end” machine learning approach and learn what kind of explanations make\nthe end users happy.\n1.3.3\nExplanations as Studied in the AI Literature\nExplanations have been studied in the context of AI systems long before the recent interest\nin the explainable AI. We will start by differentiating two classes of explanations: internal\nexplanations that the system develops to help its own reasoning, and external explanations\nthat the agent offers to other agents.\nInternal explanations have been used in AI systems to guide their search (e.g. explanation-\nbased or dependency directed backtracking), or to focus their learning. The whole area\nof explanation-based learning–which attempts to focus the system’s learning element on\nthe parts of the scenarios that are relevant to the decision at hand (thus providing feature\nrelevance assessment). While much of the work in explanation-based search and learning\nhave been done in the context of explicit symbolic models, self explanations can also be\nuseful for systems learning their own representations Zha et al. [2021].\nExternal explanations may be given by AI systems to other automated agents/AI\nsystems (as is the case in autonomous and multi-agent systems research), or to other\nhuman agents.\nThere are however significant differences that arise based on whether\nthe other agent is human or automated. In particular, issues such as cognitive load and\ninferential capacity in parsing the explanation play an important role when the other agent\nis a human, but not so much if it is automated. In particular, the work on certificates\nand proofs of optimality of the decision, which give gigabits of provenance information to\nsupport the decision, may work fine for automated agents but not human agents.\nHistorically, (external) explanations (to human agents) have been considered in the\ncontext of AI systems for as long as they have been deployed AI systems. There is a rich\ntradition of explanations in the context of expert systems. In all these cases, the form of\nthe explanation does depend on (a) the role the human plays (debugger vs. lay observer)\nand (b) whether the task is an explicit knowledge one or a tacit knowledge one.\n\n14\nIntroduction\n1.3.4\nExplainable AI: The Landscape & The Tribes\nThe work in explainable AI systems can be classified into multiple dimensions. The first\nis whether what needs to be explained is a single-decision (e.g. classification) task or a\nbehavior resulting from a sequence of decisions. Second dimension is whether the task\nis guided by explicit (verbalizable) knowledge or is a tacit task. The third is whether\nthe interaction between the human and the AI agent is one-shot or iterative/longitudi-\nnal. Armed with these dimensions, we can discern several research “tribes” focusing on\nexplainable AI systems:\nExplainable Classification\nMost work on the so-called “explainable ML” focused on\nexplainable classification. The classification tasks may be “tacit” in that the machine\nlearns its own potentially inscrutable features/representations.\nA prominent subclass\nhere is image recognition/classification tasks based on deep learning approaches. In these\ncases, the default communication between the AI agents and humans will be over the\nshared substrate of the (spatial) image itself. Explanations here thus amount to “saliency\nannotations” over the image–showing which pixels/regions in the image have played a\nsignificant part in the final classification decision. Of course, there are other classification\ntasks–such as loan approval or fraudulent transaction detection–where human-specified\nfeatures, rather than pixels/signals are used as the input to the classifier. Here explana-\ntions can be in terms of the relative importance of various features (e.g. shapley values).\nExplainable Behavior\nHere we are interested in sequential decision settings, such as\nhuman-robot or human-AI interactions, where humans and AI agents work in tandem\nto achieve certain goals. Depending on the setting, the human might be a passive ob-\nserver/monitor, or an active collaborator. The objective here is for the AI agent (e.g. the\nrobot) to exhibit behavior that is interpretable to the human in the loop. A large part\nof this work focuses on tacit interactions between the humans and robots (e.g. robots\nmoving in ways that avoid colliding with the human, robots signaling which point they\nplan to go to etc.). The interactions are tacit in that the human has no shared vocab-\nulary with the robot other than the observed behavior of the robot. Here explainability\nor interpretability typically depend on the robot’s ability to exhibit a behavior that helps\nthe human understand its “intentions”. Concepts that have been explored in this context\ninclude explicability–the behavior being in conformance with human’s expectation of the\nrobot, predictability–the behavior being predictable over small time periods (e.g. next few\nactions), and legibility–the behavior signaling the goals of the robot.\nOf course, not all human-robot interactions have to be tacit; in many cases the humans\nand robots might share explicit knowledge and vocabulary about their collaborative task.\nIn such cases, the robot can also ensure interpretability through exchange of explanations\nin the shared symbolic vocabulary.\nMuch of this monograph focuses on explainable behavior, in the context of explicit\nknowledge tasks. These are the scenarios where an AI agent’s ability to plan its behavior\nup front interacts synergistically with its desire to be interpretable to the human in the\nloop. We will focus first on scenarios where the human and the agent share a common\nvocabulary, but may have differing task models, and discuss how to ensure explicability\n\n1.4 Our Perspective on Human-Aware and Explainable AI Agents\n15\nor provide explanations in that scenario. Towards the end, we will also consider the more\ngeneral scenario where even the vocabulary is not common–with the AI agent using its\nown internal representations to guide its planning and decision-making, and discuss how\nexplanations can still be provided in human’s vocabularly.\n1.4\nOur Perspective on Human-Aware and Explainable AI\nAgents\nIn this section, we give a brief summary of the the broad perspective taken in this book\nin designing human-aware and explainable AI systems. This will be followed in the next\nsection by the overview of the book itself.\n1.4.1\nHow do we make AI agents Human-Aware?\nWhen two humans collaborate to solve a task, both of them will develop approximate\nmodels of the goals and capabilities of each other (the so called “theory of mind”), and\nuse them to support fluid team performance. AI agents interacting with humans – be they\nembodied or virtual – will also need to take this implicit mental modeling into account.\nThis certainly poses several research challenges. Indeed, it can be argued that acquiring\nand reasoning with such models changes almost every aspect of the architecture of an\nintelligent agent. As an illustration, consider the architecture of an intelligent agent that\ntakes human mental models into account shown in Figure 1.1. Clearly most parts of the\nagent architecture – including state estimation, estimation of the evolution of the world,\nprojection of its own actions, as well as the task of using all this knowledge to decide\nwhat course of action the agent should take – are all critically impacted by the need to\ntake human mental models into account. This in turn gives rise to many fundamental\nresearch challenges. In this book, we will use the research in our lab to illustrate some\nof these challenges as well as our attempts to address them. Our work has focused on\nthe challenges of human-aware AI in the context of human-robot interaction scenarios\n[Chakraborti et al., 2018], as well as human decision support scenarios [Sengupta et al.,\n2017b]. Figure 1.2 shows some of the test beds and micro-worlds we have used in our\nongoing work.\n1.4.2\nMental Models in Explainable AI Systems\nIn our research, we address the following central question in designing human-aware AI\nsystems: What does it take for an AI agent to show explainable behavior in the presence of\nhumans? Broadly put, our answer is this: To synthesize explainable behavior, AI agents\nneed to go beyond planning with their own models of the world, and take into account the\nmental model of the human in the loop. The mental model here is not just the goals and\ncapabilities of the human in the loop, but includes the human’s model of the AI agent’s\ngoals/capabilities.\n\n16\nIntroduction\nFig. 1.2: Test beds developed to study the dynamics of trust and teamwork between autonomous agents\nand their human teammates.\nFig. 1.3: Use of different mental models in synthesizing explainable behavior. (Left) The AI system can\nuse its estimation of human’s mental model, MH\nr , to take into account the goals and capabilities of the\nhuman thus providing appropriate help to them. (Right) The AI system can use its estimation of human’s\nmental model of its capabilities MR\nh to exhibit explicable behavior and to provide explanations when needed.\n\n1.5 Overview of this Book\n17\nLet MR and MH correspond to the actual goal/capability models of the AI agent and\nhuman. To support collaboration, the AI agent needs an approximation of MH, we will\ncall it f\nMH\nr , to take into account the goals and capabilities of the human. The AI agent\nalso needs to recognize that the human will have a model of its goals/capabilities MR\nh ,\nand needs an approximation of this, denoted f\nMR\nh . It is important to note that while MR\nand MH are intended to be “executable models,” in that courses of action consistent with\nthem are in fact executable by the corresponding agent–robot or human, MR\nh and MH\nr\nare models of “expectation” by the other agent, and thus may not actually be executable\nby the first agent. All phases of the “sense–plan–act” cycle of an intelligent agent will have\nto change appropriately to track the impact on these models (as shown in Figure 1.1).\nOf particular interest to us in this book is the fact that synthesizing explainable be-\nhavior becomes a challenge of supporting planning in the context of these multiple models.\nIn particular, we shall see that the AI agent uses MH\nr to anticipate the human behavior\nand provide appropriate assistance (or at least get out of the way), while the agent uses\nf\nMR\nh , its estimate of MR\nh , to understand human’s expectation on its behavior, and use\nthat understanding to either conform to the human expectation or actively communicate\nwith the human to induce them to change MR\nh . We discuss the conformance aspect in\nterms of generating explicable behavior. For the model communication part, either the\ncommunication can be done implicitly–which is discussed in terms of generating legible\nbehavior, or can be done explicitly–with communication actions. This latter part is what\nwe view as the “explanation” process, and address multiple challenges involved in gener-\nating such explanations. Finally, while much of the book is focused on cooperative and\nnon-adversarial scenarios, the mental model framework in Figure 1.3 can also be used by\nthe agent to obfuscate its behavior or provide deceptive communication. We also discuss\nhow such selective obfuscation and deception is facilitated.\n1.5\nOverview of this Book\nThrough the rest of the book, we will look at some of the central challenges related to\nhuman-aware planning that arise due to and can be addressed through working with the\nhuman’s mental model. In particular, we will ground our discussions of the topics within\nthe context of using such models to generate either interpretable or deceptive behavior.\nThe book is structured as follows:\nChapter 2\nIn this chapter, we will focus on formally defining the goal-directed deter-\nministic planning formalisms and the associated notations that we will be using to study\nand ground the technical discussions throughout this book. We will also define the three\nmain interpretability metrics; namely, Explicability, Legibility, and Predictability. We will\nbe revisiting these three measures in the following chapters. We will see how the different\nmethods discussed throughout the book relate to these measures, and in various cases\ncould be understood as being designed to optimize, at the very least a variant of these\nmeasures.\n\n18\nIntroduction\nChapter 3\nWe next focus on one of these measures, namely, explicability, and see how\nwe could allow the robot to choose plans that maximize explicability scores. In particular,\nwe will look at two main paradigms to generate such explicable plans. First, we consider\na model-based method called reconciliation search that will use the given human model\nalong with a distance function (which could potentially be learned) to generate robot\nplans with high explicability scores. Then we will look at a model-free paradigm, where\nwe use feedback from users to learn a proxy for the human model in the form of a labeling\nmodel and use that simpler model to drive the explicable planning. We will also look at\nhow one could use environment design to allow the agents to generate plans with higher\nexplicability scores. Within the design framework, we will also look at the evolution of\nplan explicability within the context of longitudinal interactions.\nChapter 4\nWe next turn our focus onto legibility.\nWe look at how the robot can\nreduce the human observer’s uncertainty over its goals and plans, particularly when the\nobserver has imperfect observations of its activities. The robot can reduce the observer’s\nuncertainty about its goals or plans by implicitly communicating information through its\nbehavior i.e. by acting legibly. We formulate this problem as a controlled observability\nplanning problem, where in the robot can choose specific actions that allow it to modulate\nthe human’s belief over the set of candidate robot goals or plans. Further we will also\ndiscuss the relationship between plan legibility as discussed in this framework with the\nnotion of plan predictability defined in the literature.\nChapter 5\nIn this chapter, we return to the problem of maximizing explicability and\ninvestigate an alternate strategy, namely explanations. Under this strategy, rather than\nletting the robot choose possibly suboptimal plans that may better align with human\nexpectations, the robot can follow its optimal plans and provide explanations as to why\nthe plans are in fact optimal in its own model.\nThe explanation in this context tries\nto resolve any incorrect beliefs the human may hold about the robot and thus helps\nto reconcile the differences between the human’s mental model and the robot’s model.\nIn the chapter, we will look at some specific types of model reconciliation explanations\nand a model space search algorithm to generate such explanation. We will also consider\nsome approximations for such explanations and discuss some user studies that have been\nperformed to validate such explanations.\nChapter 6\nIn this chapter, we continue our discussion on model reconciliation and focus\non one specific assumption made in the earlier chapter, namely that the human’s mental\nmodel may be known. We will study how one could still generate model reconciliation\nexplanations when this assumption may not be met. In particular, we will consider three\ncases of incrementally decreasing access to human mental model. We will start by consid-\nering cases where an incomplete version of the model may be available, then we will look\nat scenarios where we can potentially learn a model proxy from human data and finally\nwe will see the kind of explanatory queries we can field by just assuming a prototypical\nmodel for the human.\n\n1.5 Overview of this Book\n19\nChapter 7\nIn this chapter, we return back to the problem of explicability and look\nat how one could combine the two strategies discussed in earlier chapters for addressing\nexplicability; namely, explicable planning and explanation. In fact we will see how one\ncould let the robot trade-off the cost of explaining the plan against the overhead of choosing\na potentially suboptimal but explicable plan and will refer to this process as balanced\nplanning. In the chapter, we will look at some particular classes of balanced planning and\nintroduce a concept of self-explaining plans. We will also introduce a compilation based\nmethod to generate such plans and show how the concept of balancing communication\nand behavior selection can be extended to other interpretability measures.\nChapter 8\nIn this chapter, we look at yet another assumption made in the generation\nof explanations, namely, that there exists a shared vocabulary between the human and\nthe robot through which it can communicate. Such assumptions may be hard to meet in\ncases where the agent may be using learned and/or inscrutable models. Instead we will\nlook at how we could learn post-hoc representations of the agent’s model using concepts\nspecified by the user and use this representation for model reconciliation. We will see how\nsuch concepts could be operationalized by learning classifiers for each concepts. In this\nchapter, we will also look at how we could calculate confidence over such explanations and\nhow these methods could be used when we are limited to noisy classifiers for each concept.\nFurther, we will discuss how we could acquire new concepts, when the originally specified\nset of concepts may not be enough to generate an adequate representation of the model.\nChapter 9\nIn this chapter, we turn our attention to adversarial settings, where the robot\nmay have to act in obfuscatory manner to minimize the leakage of sensitive information. In\nparticular, we look at settings where the adversary has partial observability of the robot’s\nactivities. In such settings, we show how the robot can leverage the adversary’s noisy\nsensor model to hide information about its goals and plans. We also discuss an approach\nwhich maintains goal obfuscation even when the adversary is capable of diagnosing the\ngoal obfuscation algorithm with different inputs to glean additional information. Further,\nwe discuss a general setting where both adversarial as well as a cooperative observers with\npartial observations of robot’s activities may exist. We show that in such a case, the robot\nhas to balance the amount of information hidden from the adversarial observer with the\namount of information shared with the cooperative observer. We will also look at the use\nof deceptive communication in the form of lies. We will show how we can leverage the\ntools of model reconciliation to generate lies and we will also discuss how such lies could\nin fact be used to benefit the human-robot team.\nChapter 10\nIn this chapte, we provide a discussion of some of the applications based\non the approaches discussed in the book. We will look at two classes of applications. In\nthe first, we look at decision support systems and in particular the methods related to\nexplanations could be used to help decision-makers better understand the decisions being\nproposed by the user. In the second case, we will look at a specific application designed\nto help a user with transcribing a declarative model for the task. This application helps\nin identifying mistakes in current version of the model by reconciling against an empty\nuser model.\n\n20\nIntroduction\nChapter 11\nFinally we will close the book with a quick discussion on some of the most\nchallenging open problems in Human-Aware AI.\nAlthough this monograph is informed primarily by our group’s work, in each chapter,\nwe provide bibliographic remarks at the end connecting the material discussed to other\nrelated works.\n\nChapter 2\nMeasures of Interpretability\nThis chapter will act as the introduction to the technical discussions in the book. We\nwill start by establishing some of the basic notations that we will use, including the\ndefinitions of deterministic goal-directed planning problems, incomplete planning models,\nsensor models, etc. With the basic notations in place, we will then focus on establishing\nthe three main interpretability measures in human-aware planning; namely, Explicability,\nLegibility, and Predictability. We will revisit two of these measures (i.e., explicability and\nlegibility) and discuss methods to boost these measures throughout the later chapters.\n2.1\nPlanning Models\nWe will be using goal-oriented STRIPS planning models to represent the planning prob-\nlems used in our discussions.\nHowever, the ideas discussed in this book apply to any\nof the popular planning representations. Under this representation scheme, a planning\nmodel (sometimes also referred to as a planning problem) can be represented by the tuple\nM = ⟨F, A, I, G, C⟩, where the elements correspond to\n• F - A set of propositional fluents that define the space of possible task states. Each\nstate corresponds to a specific instantiation of the propositions. We will denote the\nset of states by S. When required we will uniquely identify each state by the subset\nof fluents which are true in the given state. This representation scheme implicitly\nencodes the fact that any proposition from F not present in the set representation\nof the state is false in the underlying state.\n• A - The set of actions that are available to the agent. Under this representation\nscheme, each action ai ∈A is described by a tuple of the form\nai = ⟨pre(ai), adds(ai), dels(ai)⟩,\nwhere\n– pre(ai) - The preconditions for executing the action. For most of the discus-\nsion, we will follow the STRIPS execution semantics, wherein an action is only\nallowed to execute in a state where the preconditions are ‘met’. In general, pre-\nconditions can be any logical formula over the propositional fluents provided\nin F. Moreover, we would say an action precondition is met in a given state\nif the logical formula holds in that state (remember a state here correspond\nto a specific instantiation of fluents) We will mostly consider cases where the\nprecondition is captured as a conjunctive formula over a subset of state fluents,\nwhich we can equivalently represent as a set over these fluents. This represen-\ntation allows us to test whether a precondition holds by checking if the set of\nfluents that are part of the precondition is a subset of the fluents that are true\nin the given state. Thus the action ai is executable in a state sk, if pre(ai) ⊆sk.\n21\n\n22\nMeasures of Interpretability\n– adds(ai)/dels(ai) - The add and delete effects of the action ai together captures\nthe effects of executing the action in a state. The add effects represent the set of\nstate fluents that will be made true by the action and the delete effects capture\nthe state fluents that will be turned false. Thus executing an action ai in state\nsj results in a state sk = (sj \\ dels(ai)) ∪adds(ai)\n• I - The initial state from which the agent starts.\n• G - The goal that the agent is trying to achieve. Usually the goal is considered to be\na partially specified state. That is, the agent is particularly interested in achieving\nspecific values for certain state fluents and that the values of other state fluents\ndo not matter. Thus any state where the specified goal fluent values are met are\nconsidered to be valid goal states.\n• C - The cost function (C : A →R>0) that specifies the cost of executing a particular\naction.\nGiven such a planning model, the solution takes the form of a plan, which is a sequence\nof actions. A plan π = ⟨a1, ..., ak⟩is said to be a valid plan for a planning model M, if\nexecuting the sequence of the plan results in a state that satisfies the goal. Given the cost\nfunction, we can also define the cost of a plan, C(π) = P\nai∈π C(ai). A plan, π, is said\nto be optimal if there exists no valid plan that costs less than π. We will use the term\nbehavior to refer to the observed state action sequence generated by the execution of a\ngiven plan. For models where all actions have unit cost, we will generally skip C from\nthe model definition. We will use the superscript ‘∗’ to refer to optimal plans. Further,\nwe will use the modifier ‘ ¯ ’ to refer to prefixes of a plan and ‘+’ operator to refer to\nconcatenation of action sequences. In cases where we are comparing the cost of the same\nplan over different models, we will overload the cost functions C to take the model as an\nargument, i.e., we will use C(π, M1) to denote the cost of plan π in the model M1, while\nC(π, M2) denotes its cost in M1. Additionally, we will use the notation C∗\nM to denote\nthe cost of an optimal plan in the model M.\nSince most of the discussion in this book will rely on reasoning about the properties\nof such plan in different models, we will use a transition function δ to capture the effect of\nexecuting the plan under a given model, such that the execution of an action ai in state\nsj for a model M, where δ(sj, a, M), gives the state that results from the execution of\naction a in accordance with the model M.\nIncomplete Models\nIn this book, we will also be dealing with scenarios where the\nmodel may not be completely known.\nIn particular, we will consider cases where the\nspecification may be incomplete insofar as we do not know every part of the model with\nabsolute certainty, instead we will allow information on parts of the model that may be\npossible. To represent such models, we can follow the conventions of an annotated model,\nwherein the model definition is quite similar to STRIPS one, except that each action ai is\nnow defined as ai = ⟨pre(ai), poss_precai, adds(ai), g\npre(ai), g\ndels(ai), g\ndels(ai)⟩, where ‘g\npre’,\n‘]\nadds’ and ‘ g\ndels’ represent the possible preconditions, adds and deletes of an action. If a\nfluent f is part of such a possible list, say a possible precondition, then we are, in effect,\nsaying that we need to consider two possible versions of action ai; one where it has a\nprecondition f and one where it does not. If the model in total contains k possible model\n\n2.1 Planning Models\n23\ncomponents (including preconditions and effects), then it is in fact representing 2k possible\nmodels (this set of possible models are sometimes referred to as the completion set of an\nannotated model).\nSensor Models\nMost of the settings discussed in the book contain multiple agents and\nrequire one of the agents to observe and make sense of the actions performed by the other.\nWe will refer to the former agent as the observer and the other as the actor (though these\nroles need not be fixed in a given problem). In many cases, the observer may not have\nperfect visibility of the actor’s activities. In particular, we will consider cases where the\nobserver’s sensor model may not be able to distinguish between multiple different activities\nperformed by the actor. To capture cases like this, we will use the tuple, Obs = ⟨Ω, O⟩to\nspecify the sensor model of the observer, where the elements correspond to:\n• Ω- A set of observation tokens that are distinguishable by the observer. We will\nuse ω to denote an observation token, and ⟨ω⟩to denote a sequence of tokens.\n• O - An observation function (O : A×S →Ω) maps an action performed and the next\nstate reached by the actor to an observation token in Ω. This function captures any\nlimitations present in the observer’s sensor model. If the agent cannot distinguish\nbetween multiple different activities, we say that the agent has partial observability.\nGiven a sequence of tokens, ⟨ω⟩, a plan π is consistent with this sequence if and only\nif the observation at any step could have been generated by the corresponding plan\nstep (denoted as ⟨ω⟩|= π).\nModels in Play in Human-Aware Planning Problems\nThroughout most of this\nbook, we will use the tuple MR = ⟨F R, AR, IR, GR, CR⟩to capture the robot’s plan-\nning model that it uses to plan its actions, MH = ⟨F H, AH, IH, GH, CH⟩the model\nhuman may use to capture their own capabilities and plan their behavior and MR\nh =\n⟨F R\nh , AR\nh , IR\nh , GR\nh , CR\nh ⟩the mental model the human maintains of the robot. In cases where\nwe want to differentiate between the different definitions of the same action under different\nmodels, we will use the specific model name in the superscript to differentiate them. For\nexample, we will refer to the definition of action ai in the robot’s original model as aMR\ni\n,\nwhile the human’s expectation of this model will be captured as a\nMR\nh\ni\n. In cases where\nthe human is maintaining an explicit set of possible robot models then we will use MR\nh to\nrepresent this set. In this book, we will be using the term robot to refer to the autonomous\nagent that will be acting in the world and interacting with the human. Though, by no\nmeans are the methods discussed in the book are limited to physically embodied agents.\nIn fact, in Chapter 10, we will see many examples of software agents capable of using the\nsame methods. Much of the discussions in this book will be focused on cases where the\nhuman teammate is merely an observer, and thus in terms of human models we will be\nfocusing on MR\nh . For the sensor models, since the human will be the observer, we will\ndenote the human sensor model by the tuple, ObsH\nr = ⟨ΩH\nr , OH\nr ⟩.\n\n24\nMeasures of Interpretability\n(a) Plan legibility / transparency.\n(b) Plan explicability.\n(c) Plan predictability.\nFig. 2.1: A simple illustration of the differences between plan explicability, legibility and predictability.\nIn this Gridworld, the robot can travel across cells, but cannot go backwards. Figure 2.1a illustrates a\nlegible plan (green) in the presence of 3 possible goals of the robot, marked with ?s. The red plan is\nnot legible since all three goals are likely in its initial stages. Figure 2.1b illustrates an explicable plan\n(green) which goes straight to the goal G as we would expect. The red plan may be more favorable to\nthe robot due to its internal constraints (the arm sticking out might hit the wall), but is inexplicable (i.e.\nsub-optimal) in the observer’s model. Finally, Figure 2.1c illustrates a predictable plan (green) since there\nis only one possible plan after it performs the first action. The red plans fail to disambiguate among two\npossible completions of the plan. Note that all the plans shown in Figure 2.1c are explicable (optimal in\nthe observer’s model) but only one of them is predictable – i.e. explicable plans may not be predictable.\nSimilarly, in Figure 2.1b, the red plan is predictable after the first action (even though not optimal, since\nthere is only one likely completion) but not explicable – i.e. predictable plans may not be explicable.\nWithout a prefix in Figure 2.1b, the green plan is the only predictable plan.\n2.2\nModes of Interpretable Behavior\nThe term interpretable behavior has been used to cover a wide variety of behaviors.\nHowever, one unifying thread that runs throughout the different views is the fact that they\nare all strategies designed to handle the asymmetry between what the human knows about\nthe robot (in terms of its model or the plan it is following) and the robot’s model or plans.\nA robot’s actions may be uninterpretable when it does not conform to the expectations or\npredictions engendered by the human’s model of the agent. In this chapter, we will look at\nthree notions of interpretability; namely explicability, legibility, and predictability.\nEach of these interpretability types captures a different aspect of the asymmetry. For\neach type, we will define a scoring function, that maps a given behavior and the human’s\nbeliefs about the robot to a score. Thus a robot that desires to exhibit a certain type\nof interpretable behavior can do so by selecting behaviors that maximize the given score.\nFigure 2.1, present some sample behavior illustrating the various interpretability measures.\n2.2.1\nExplicability\nThe notion of explicability is related to the human’s understanding of the robot’s model\nand how well it explains or aligns with the behavior generated by the robot.\n\n2.2 Modes of Interpretable Behavior\n25\nExplicability measures how close a plan is to the expectations of the observer.\nTo formally define explicability, we need to define the notion of distance between the\nrobot’s plan and that expected by the human observer.\nSuch distance measure could\nbe based on a number of different aspects, including the cost of the given plan or even\non syntactic features like actions used. For now, we will assume, we have access to a\nfunction D that takes two plans and a model and returns a positive number that reflects\nthe distance between the plans. From the semantics of explicability, the farther the plan\nis from one of the ‘expected’ plans, the more unexpected it is to the observer. Thus, the\nexplicability score for a robot’s plan can be written as,\nE(π, MR\nh ) ∝max\nπ′∈ΠMR\nh −1 ∗D(π, π′, MR\nh )\nwhere, ΠMR\nh are the set of plans expected by the human. That is, the explicability score\nof a robot’s plan is negatively related to the minimum possible plan distance from the\nhuman’s set of expected plans. Throughout the book we will mostly leave the exact func-\ntion mapping distances to explicability score undefined. Instead we will assume that this\nmapping could be any monotonic increasing function over the term −1 ∗D(π, π′, MR\nh ).\nThus the objective of explicable planning becomes,\nFind: π\nmax\nπ∈ΠMR E(π, MR\nh )\nwhere ΠMR is the set of plans valid for the model MR. We will be returning to this\nscore multiple times as it represents one of the key desirable properties of a human-aware\nrobot – namely the ability to generate plans that the human can “interpret”, insofar that\nit aligns with her expectations about the robot.\n2.2.2\nLegibility\nThe robot may be capable of performing multiple different tasks in a given environment.\nIn such environments, the human observer may sometimes have ambiguity over the current\ntask being pursued by the robot. In such cases, the robot can make its behavior inter-\npretable to the human by implicitly communicating its task through its behavior. This\nbrings us to another type of interpretable behavior, namely, legible behavior. In general,\nlegible behavior which allows the robot to convey some information implicitly. In this\nsetting, the inherent assumption is that the human observer maintains a set of possible\nhypotheses about the robot model but is unaware of the true model. In a nutshell,\nLegibility reduces observer’s ambiguity over possible models the robot is using.\nLet MR\nh be the set of possible models that the human thinks the robot may have. Here\nthe robot’s objective is to execute a plan that reduces the human observer’s ambiguity\nover the possible tasks under consideration. Therefore, the legibility score of a plan π\ncan be defined as follows:\n\n26\nMeasures of Interpretability\nL(π, MR\nh , MR) ∝\n1\nfMR\namb ({Mj | Mj ∈MR\nh ∧δ(Ij, π, Mj) |= Gj})\nWhere fMR\namb is a measure of ambiguity over the set of models consistent with the\nbehavior observed. There are multiple ways such an ambiguity function could be instan-\ntiated including, cardinality of the set, the similarity of the remaining models, and in the\nprobabilistic case, the probability that the human would attach to the models that meet\nsome specific criteria (for example, it shares some parameter with the true robot model).\nThus the objective of legible planning thus becomes\nFind: π\nmax\nπ∈ΠMR L(π, MR\nh )\nIn Chapter 4, we will see specific cases of legibility namely, goal legibility and plan\nlegibility, defined in settings where the human has partial observability of the robot’s\nactivities. Due to the human’s partial observability of the robot’s activities, the human\nmay suffer from uncertainty over the robot’s true goal, given a set of candidate goals\n(goal legibility) or robot’s true plan given a set of possible plans towards a goal (plan\nlegibility). Moreover, for scenarios with partial observability, we will be defining legibility\nover observation sequence generated by the plan as opposed to over the original plans.\n2.2.3\nPredictability\nIn the case of explicability, as long as the robot’s behavior conforms to one of the human’s\nexpected plans, the behavior is explicable to the human. However, predictability goes\nbeyond explicability, in that, the behavior has to reduce the number of possible plan\ncompletions in the human’s mental model given the robot’s task. In a nutshell,\nPlan predictability reduces ambiguity over possible plan completions, given a\nplan prefix.\nThat is, a predictable behavior is one that allows the human observer to guess or\nanticipate the robot’s actions towards its goal. We can define the predictability score,\ni.e., predictability score of a plan prefix, ¯π, given the human’s mental model, MR\nh , as\nfollows:\nP(¯π, MR\nh ) ∝\n1\n| {˜π | ˜π ∈complMR\nh (¯π)} |\n\n2.3 Communication to Improve Interpretability\n27\nHere, complM(·) denotes the set of possible completions with respect to model M, i.e.,\n∀π ∈{complM(·)}, δ(I, π, M) |= G. The predictability score is higher with less number of\npossible completions for a given prefix in the human’s mental model. Further, the objective\nin the problem of predictable planning is to select an action that reduces the number of\npossible completions given the prefix and the robot’s goal. Note that, this measure is\nbeing defined in an online setting, that is the robot has executed a partial prefix and is in\nthe process of completing the execution in a predictable fashion. Therefore, the problem\nof predictable planning with respect to a given prefix ¯π is defined as follows:\nFind: a\nmax\na∈AR P(¯π + a, MR\nh )\nWhile predictability is a popular measure for interpretability, in this particular book\nwe will not be spending much time investigating this particular measure.\n2.3\nCommunication to Improve Interpretability\nOne of the core reasons for the uninterpretability of any agent behavior is the asymmetry\nbetween the agent’s knowledge and what the human observer knows about the agent.\nExplicability and legibility stem from the human’s misunderstanding or lack of knowledge\nabout the agent’s model and predictability deals with the human’s ignorance about the\nspecific plan being followed by the agent. This means that in addition to adjusting the\nagent behavior, another strategy the agent could adopt is to inform the human about its\nmodel or the current plan.\n2.3.1\nCommunicating Model Information\nAmong the communication strategies, we will focus primarily on communicating model\ninformation. Such communication requires the ability to decompose the complete model\ninto meaningful components and be able to reason about the effect of receiving information\nabout that component in the human model. We will do this by assuming that the planning\nmodels can be represented a set of model features or parameters that can be meaningfully\ncommunicated to the human. STRIPS like models gives us a natural parameterization\nscheme of the form Γ : M 7→s represents any planning problem M = ⟨F, A, I, G, C⟩as a\nstate s ⊆F as follows –\n\n28\nMeasures of Interpretability\nτ(f) =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninit-has-f\nif f ∈I,\ngoal-has-f\nif f ∈G,\na-has-precondition-f\nif f ∈pre(a), a ∈A\na-has-add-effect-f\nif f ∈adds(a), a ∈A\na-has-del-effect-f\nif f ∈dels(a), a ∈A\na-has-cost-f\nif f = C(a), a ∈A\nΓ(M) =\n\bτ(f) | ∀f ∈I ∪G∪\n[\na∈A\n{f′ | ∀f′ ∈{C(a)} ∪pre(a) ∪adds(a) ∪dels(a)}\n\t\nNow the agent can communicate to the user in terms of these model features, wherein\nthey can assert that certain features are either part of the model or not. If µ is a set\nsuch messages, then let MR\nh + µ be the updated human model that incorporates these\nmessages. If I is an interpretability score (say explicability or legibility) and π the current\nplan, then the goal of the communication becomes to identify a set of messages µ such\nthat\nI(π, MR\nh + µ) > I(π, MR\nh )\nThe discussion in Chapter 5 focuses on the cases where the robot’s plan is fixed and it\ntries to identify the set of messages that best communicate this plan. There may be cases\nwhere the most desirable approach for the agent is to not only communicate but also adapt\nits behavior to reduce uninterpretability and communication overhead. We will refer to\nsuch approaches as balanced planning and will dive deeper into such methods in Chapter\n6.\nApart from the knowledge asymmetry, another reason for uninterpretability could be\nthe difference in the computational capabilities of the human and the agent. We could\nalso use communication in these cases to help improve the human’s ability to comprehend\nthe plan. While we won’t look at methods for generating such explanatory messages in\ndetail, we will briefly discuss them in Chapter 11.\n2.4\nOther Considerations in Interpretable Planning\nIn this section, we will look at some of the other factors that have been studied in the\ncontext of interpretable behavior generation and human-aware planning in general.\nModes of Operation\nOne additional factor we can take into account while describing\ninterpretable behavior is what part of the plan is being analyzed by the human observer.\nAre they watching the plan being executed on the fly or are they trying to analyze the\nentire plan? We will refer to the former as online setting and the latter as offline or post-\nhoc setting. In the previous sections, while we defined predictability in an online setting\nboth legibility and explicability were defined for an offline setting. This is not to imply\n\n2.4 Other Considerations in Interpretable Planning\n29\nthat legibility and explicability are not useful in online settings (though the usefulness of\noffline predictability is debatable in fully observable settings, we will touch on the topic\nin Chapter 4). The online formulations are useful when the robot’s reward/cost functions\nare tied to the interpretability score at every execution time step. For online variations of\nthe scores, the score for each prefix may be defined in regard to the expected completions\nof the prefix. A related setting that does not fit into either paradigms, is one where the\nhuman is interested in interpretable behavior after the execution of first k steps by the\nagent, we will refer to such settings as k-step interpretability settings. These settings are\nnot the same as the online planning setting, as they allow for cases where the human\nwould not pose any penalty for uninterpretable behavior for the first k −1 steps.\nMotion vs Task Planning\nThe notions of interpretable behaviors have been studied in\nboth motion planning and task planning. From the algorithmic perspective, this is simply\ndifferentiated in usual terms – e.g. continuous versus discrete state variables. However, the\nnotion of interpretability in task planning engenders additional challenges. This is because\na reasonable human’s mental model of the robot for motion planning can be assumed to\nbe one that prefers straight-line plans and thus need not be modeled explicitly (or needs\nto be acquired or learned). For task planning in general, this is less straightforward. In\nChapters 3 and 6, we will see how we could learn such human models.\nComputational Capabilities of the Human\nOne aspect of decision-making we haven’t\nmodeled explicitly here is the computational/inferential capabilities of the robot and the\nhuman. The robot’s computation capabilities determine the kind of behaviors they can\nexhibit and the human’s computational capabilities determine the kind of behavior they\nexpect from the robot. Unfortunately, most current works in interpretable behavior gen-\neration generally tend to assume they are dealing with perfectly rational humans and\nagents (with few exceptions such as the user of noisy-rational models). Developing and\nusing more sophisticated models of bounded rationality that better simulate the human\nand the robot’s reasoning capabilities remain an exciting future direction for interpretable\nbehavior generation.\nBehavior versus Plans\nOur discussion has mostly been confined to analysis of be-\nhaviors – i.e. one particular observed instantiation of a plan or policy. In particular, a\nplan – which can be seen as a set of constraints on behavior – engenders a candidate set\nof behaviors some of which may have certain interpretable properties while others may\nnot. However, this also means that an algorithm that generates plan with a specific inter-\npretable property, can also do so for a particular behavior it models, since in the worst case\na behavior is also a plan that has a singular candidate completion. A general treatment\nof a plan can be very useful in the offline setting – e.g. in decision-support where human\ndecision-makers are deliberating over possible plans with the support from an automated\nplanner. Unfortunately, interpretability of such plans has received very little attention\nbeyond explanation generation.\n\n30\nMeasures of Interpretability\n2.5\nGeneralizing Interpretability Measures\nIn this chapter and in the rest of the book, we focus one three interpretability measures\nthat were identified by previous works as capturing some desirable behaviors in human-\nrobot interaction settings.\nNote that these are specific instances of larger behavioral\npatterns the robot could exhibit in such scenarios. Another possible way to categorize\nagent behaviors would be to organize them based on how they influence or use the human\nmental models. In the case of settings with humans as observers, this gives rise to two\nbroad categories, namely;\nModel-Communication Behaviors: Model-communication involves molding the\nhuman’s mental models through implicit (i.e tacit) or explicit communication, to allow\nthe agent to achieve their objectives. Examples of model-communication behaviors in-\nclude legible behavior generation where the agent is trying to implicitly communicate\nsome model information and the model-reconciliation explanation where the agent is di-\nrectly communicating some model information so that the robot behavior appears more\nexplicable.\nModel-Following Behaviors: This strategy involves taking the current models of\nthe human and generating behavior that conforms to current human expectations or ex-\nploits it in the robot’s favor. The examples of this strategy we have seen so far include\nexplicability and predictability.\nOne could see the agent engaging cyclically in model-communication and model-\nfollowing behaviors, wherein the agent may choose to mold the user’s expectations to\na point where its behavior may be better received by the observer. We could go one step\nfurther and get away from behavior categorization altogether, and simply define a single\nhuman-aware planning problem that generates these individual interpretable behaviors in\ndifferent scenarios. In fact, Sreedharan et al. [2021b] defines a generalized human-aware\nplanning problem as follows\nDefinition 1. A Generalized human-aware planning problem or G-HAP is a\ntuple ΠH = ⟨MR, MR\nh , P 0\nh, Pℓ, CH⟩, where MR as always is the robot model, MR\nh is the set\nof models human maintains of the robot, P 0\nh is the human’s initial prior over the models in\nthe hypothesis set MR\nh , Pℓare the set of inference models the human associates with each\npossible model and CH is a generalized cost function that depends on the exact objective\nof the agent.\nHere MR\nh also includes special model denoted as M0.\nM0 corresponds to a high\nentropy model, that is meant to capture the fact that the human’s current set of explicit\nmodels cannot account for the observed robot behavior. The formulation assumes the\nhuman is a Bayesian reasoner who reasoning about the robot behavior given their beliefs\nabout the robot model (and previous observations about the robot behavior).\nThis problem formulation generates explicable behavior as discussed in this book if\n(a) MR\nh = {MR\nh , M0}, (b) the objective of the planning problem is to select behavior that\nmaximizes the posterior probability associated with model MR\nh and (c) if, in human’s\nexpectation, the likelihood of the robot selecting a plan for MR\nh (i.e. MR\nh ) is inversely\n\n2.6 Bibliographic Remarks\n31\nproportional to a distance from a set of expected plans. [Sreedharan et al., 2021b] specif-\nically shows this for cases where the distance measure corresponds to cost difference and\nthe expected plans correspond to optimal plans.\nSimilarly, the formulation generates legible behaviors when the prior on M0 is zero\nand if the objective is to select behavior that maximizes the cumulative posterior over all\nthe models that share the model parameter the robot is trying to communicate (say a\ngoal). Finally, predictable behaviors are generated when M0 has zero prior, the MR\nh =\n{MR\nh , M0} and the objective of the planning problem is to choose behavioral prefixes,\nwhose completions have a high posterior likelihood.\n2.6\nBibliographic Remarks\nWe would refer the reader to look at Geffner and Bonet [2013] for a detailed discussion\nof the planning formalism we will be focusing on. In most cases, we can leverage any\nappropriate planning algorithm (satisficing or optimal depending on the setting) that\napplies to this formalism. In cases, where we require the use of a specifically designed\nsearch or planning algorithm, we will generally include a pseudo code in the chapter.\nIn terms of the interpretability measures, the original work to introduce explicability\nwas Zhang et al. [2017], though it used a learned labeling model as a proxy to measure\nthe explicability score.\nThe measure was further developed in Kulkarni et al. [2019a]\nto take into account the user’s model and a learned distance function. Parallelly, works\nChakraborti et al. [2017a] and Chakraborti et al. [2019c] have looked at applying commu-\nnication based strategies to address inexplicability but primarily in the context of when\nthe distance function is based on cost and expected plan consists of plans that are optimal\nin the robot model.\nWith respect to legibility and predictability, initial works were done in the context of\nmotion-planning problems by Dragan et al. [2013] and Dragan [2017]. Legibility was later\nextended to task planning settings by MacNally et al. [2018] and further into partially\nobservable domains by Kulkarni et al. [2019b].\nA k-step version of predictability was\nintroduced in Fisac et al. [2018]. The noisy-rational model as a stand-in for the human’s\ninferential capability was used by Dragan et al. [2013], Dragan [2017], and Fisac et al.\n[2018]. Even outside the interpretability literature, there is evidence to suggest that such\nmodels can be an effective way to capture the inferential process of humans Baker and\nTenenbaum [2014]. Chakraborti et al. [2019a] provides a good overview of the entire space\nof works done in this area and provides a categorization of existing individual works done\nin this space into these three core scores.\nThe communication paradigm discussed in the chapter is based on work done in\nChakraborti et al. [2017a]. Such model parameterization was later extended to MDPs\nin Sreedharan et al. [2019a]. Even outside of explanations, the use of representing models\nhas been investigated in the context of other applications that require a search over a\nspace of plausible models, like in the case of Bryce et al. [2016].\n\n32\nMeasures of Interpretability\n\nChapter 3\nExplicable Behavior Generation\nIn chapter 2, among other things we defined the notion of explicability of a plan and\nlaid out an informal description of explicable planning.\nIn this chapter, we will take\na closer look at explicability and discuss some practical methods to facilitate explicable\nplanning. This would include discussion on both planning algorithms specifically designed\nfor generating explicable behavior and how one could design/update the task to make the\ngeneration of explicable plans easier.\nIn the earlier chapter, we discussed how the generation of explicable plans requires the\nrobot to simultaneously reason with both models MR and MR\nh . This is because the robot\nneeds to select feasible and possibly low-cost plans from MR, that align with or are close\nto plans expected by the human (as computed from MR\nh ). This implies the robot needs to\nhave access to MR\nh (at least an approximation of it in some form). An immediate question\nthe reader could ask is if MR\nh is available to the robot, why is MR required in the plan\ngeneration process at all? This is necessary since the human’s mental model might entail\nplans that are not even feasible for the robot or are prohibitively expensive, and can thus\nat best serve as a guide, and not as an oracle, to the explicable plan generation process.\nIn fact, the critical consideration in the computation of explicable behavior would be\nthe form in which the human’s mental model, MR\nh , is accessible to the robot. We present\ntwo settings in this chapter, one where the robot directly has access to MR\nh , and another\nwhere the robot learns an approximation of it, f\nMR\nh .\nIn the first setting, we hypothesize that a prespecified plan distance measure can\nquantify the distance between the robot plan πMR and the expected plans πMR\nh from\nMR\nh .\nThere are many scenarios in which the system could have direct access to the\nhuman’s mental model. In domains such as household or factory floor scenarios, there is\ngenerally a clear expectation of how a task should be performed. In such cases, MR\nh can\nbe constructed following the norms or protocols that are relevant to that domain. Most\ndeployed products make use of inbuilt models of user expectations in some form or the\nother. In domains like urban search and rescue, the human and the robot may start with\nthe same model and they may diverge over time as the robot acquires more information\nowing to the use of more advanced sensors. With MR\nh in place, we can then use it along\nwith MR to generate plans that are closer to the plans expected by the human. Section\n3.2 presents a specific heuristic search method for generating such plans and the basic\napproach is illustrated in Figure 3.1.\nIn the second setting, we learn an approximation of the human’s mental model rep-\nresented as f\nMR\nh . This model is learned based on an underlying assumption that humans\ntend to associate tasks/sub-goals with actions in a given plan. Thus the approximate\nmodel, g\nMR\nh , takes the form of a labeling function that maps plan steps to specific labels.\nThis learned model is then used as a heuristic to generate explicable plans. This approach\nis illustrated in Figure 3.2 and is discussed in Section 3.3.\n33\n\n34\nExplicable Behavior Generation\nFig. 3.1: Schematic diagram of the model based explicable planning.\nFinally, we will conclude this chapter by looking at methods that can help improve\nthe feasibility of these behaviors.\nThe plan expected by the human observer may be\nprohibitively expensive and/or infeasible for the robot. In such cases, it may be wise to\nredesign the environment itself in which the human and the robot are operating, especially\nwhen tasks need to be performed repeatedly in the environment. In Section 3.4, we discuss\nhow the environment design techniques can be leveraged to improve the explicability of\nthe robot’s behaviors, and also explore the longitudinal impact on explicable behaviors in\na repeated interaction setting.\n3.1\nExplicable Planning Problem\nAs a reminder from Chapter 2, the problem of explicable planning arises when the robot\nmodel differs from the human’s expectation of it. Let πMR be the robot’s plan solution to\nthe planning problem, MR = ⟨F, AR, IR, GR, CR⟩; whereas, ΠMR\nh be the set of plans that\nthe human expects given her mental model of robot’s model, MR\nh = ⟨F, AR\nh , IR\nh , GR\nh , CR\nh ⟩.\nThe differences in the human’s mental model can lead to different plan solutions.\nThus explicable planning consists of the robot coming up with plans that are close to\nwhat the human expects (measured using a distance function D).\nDefinition 2. The explicable planning problem is defined as a tuple PExp = ⟨MR, MR\nh , D⟩,\nwhere, D is the distance function that the human uses to compute the distance between\n\n3.1 Explicable Planning Problem\n35\nFig. 3.2:\nSchematic diagram of model-free explicable planning: A learned labeling model is used as\na proxy for the actual human model. This model is used to compute the heuristic for explicable plan\ngeneration.\n\n36\nExplicable Behavior Generation\nher expected plan and the robot’s plan.\nIn general, explicable planning is a multi-objective planning problem where the robot\nis trying to choose a behavior that tries to both maximize its explicability score (i.e.\nminimize the distance to the expected plan) and minimize its cost. To keep the planning\nformulation simple, we will assume that we can reduce the planning objective to a linear\ncombination of the cost of the plan in the robot model and the distance of the plan\nfrom the expected human plan.\nThus the solution to an explicable planning problem\nis an explicable plan that achieves the goal and minimizes the plan distances while also\nminimizing the cost of the plan.\nDefinition 3. A maximally explicable plan is a plan, π∗\nMR, starting at IR that\nachieves the goal GR, such that, arg min\nπMR\nC(πMR) +\nmin\nπ∈ΠMR\nh\nfIE(D(πMR, π, MR)).\nWhere fIE is any increasing monotonic function over the distance D (the actual choice\nof which may depend on the specific optimization algorithm). We will refer to the term\nminπ∈ΠMR\nh\nfIE(D(πMR, π, MR)) as the inexplicability score or simply IE. Readers would\nnote that IE is the opposite of the explicability score defined in the previous chapter. In\nthis chapter, we will mostly focus on planning algorithms that are trying to minimize the\ncost of plans generated, and as such focusing on inexplicability score, makes it easier to\nintegrate the concept of explicabillity into the algorithms as we can treat IE as yet another\ncost term. Though other planning formulations say utility maximization formulation, may\nbenefit from the use of explicability score.\n3.2\nModel-Based Explicable Planning\nWe will start by looking at explicable behavior generation in cases where the human’s\nmental model, MR\nh , and the distance function, D are known beforehand. Here we assume\nthat the set of plans expected by the human correspond to the optimal plans in MR\nh , i.e.,\nΠMR\nh = {π|CR\nh (π) = C∗\nMR\nh }. For each robot plan, the minimum distance (per the given\ndistance function, D) is computed with respect to this set of expected plans.\n3.2.1\nPlan Generation through Reconciliation Search\nOur primary objective here is to use the distance to the expected plans in MR\nh as a\nheuristic to come up with plans that are both valid for the model MR and explicable.\nOne obvious idea we could try is to first use MR to come up with the expected plan set\nΠR\nh and then during planning using the model MR (via a heuristic progression search),\nexpand only the plan prefixes which have the minimal distance to one of the expected\nplans in the set ΠR\nh (we will assume that the inexplicability score here is equal to the\ndistance and use the terms inexplicability score and distance interchangeably).\nUnfortunately, this won’t work in practice due to the non-monotonicity of the inexpli-\ncability score. As a partial plan grows, each new action may contribute either positively\n\n3.2 Model-Based Explicable Planning\n37\nAlgorithm 1 Reconciliation Search\nInput: PExp = ⟨MR, MR\nh , D⟩and max_cost\nOutput: ΠExp\n1: ΠExp ←∅\n{Explicable plan set}\n2: open ←∅\n{Open list }\n3: closed ←∅\n{Closed list }\n4: open.insert(IR, 0, inf)\n5: while open ̸= ∅do\n6:\nn ←open.remove()\n{Node with highest h(·) }\n7:\nif n |= GR then\n8:\nΠExp.insert(π s.t. δMR(IR, π) |= n)\n9:\nclosed.insert(n)\n10:\nfor each v ∈successors(n) do\n11:\nif v /∈closed then\n12:\nif g(n) + cost(n, v) ≤max_cost then\n13:\nopen.insert(v, g(n) + cost(n, v), h(v))\n14:\nelse\n15:\nif h(n) < h(v) then\n16:\nclosed.remove(v)\n17:\nopen.insert(v, g(n) + cost(n, v), h(v))\n18:\n19: return ΠExp\nor negatively to the distance, thus making the final inexplicability score non-monotonic.\nConsider that the goal of an autonomous car is to park itself in a parking spot on its left\nside. The car takes the left turn, parks, and turns on its left indicator. Here the turning\non of the left tail light after having parked is an inexplicable action. The first two actions\nare explicable to the human drivers and contribute positively to the overall explicability\nof the plan but the last action has a negative impact.\nDue to the non-monotonic nature of distance, we cannot stop the search process after\nfinding the first solution. Consider the following case, if d1 is the distance of the current\nprefix, then a node may exist in the open list (set of unexpanded nodes) whose distance\nis greater than d1, which when expanded may result in a solution plan with distance less\nthan d1. A greedy method that expands a node with the lowest distance score of the\ncorresponding prefix at each step is not guaranteed to find an optimal explicable plan\n(one of the plans with the lowest inexplicability score) as its first solution. One possible\nsolution that has been studied in the literature has been to use a cost-bounded anytime\ngreedy search algorithm called reconciliation search that generates all the valid candidate\nplans up to a given cost bound, and then progressively searches for plans with lower\ninexplicability scores. The value of the heuristic h(v) in a particular state v encountered\nduring the search is based entirely on the distance of the agent plan prefix πMR up to\nthat state, h(v) = D(πMR, π′\nMR\nh , MR\nh ) s.t. δMR(IR, πMR) |= v and δMR\nh (IR, π′\nMR\nh ) |= v.\nThe approach is described in detail in Algorithm 1. At each iteration of the algorithm,\nthe plan prefix of the agent model is compared with the explicable trace π′\nMR\nh (these are\nthe plans generated using MR\nh up to the current state in the search process) for the given\nproblem. There are few choices one could consider in creating such prefixes including,\n\n38\nExplicable Behavior Generation\nconsidering prefixes from among the expected plans, optimal or even valid plans to the\nstate in the human model. The search algorithm then makes a locally optimal choice of\nstates and continues till a solution is found. The search is not stopped after generating\nthe first solution, but instead, it restarts from the initial state. The search continues to\nfind all the valid candidate solutions within the given cost bound or until the state space\nis completely explored.\n3.2.2\nPossible Distance Functions\nIn the above discussion, we mostly assumed we are given some specific distance function\nD. A simple candidate for distance function could be the cost difference, but that pre-\nsupposes that we know exactly the cost function of the human and that the explicability\nconsideration of the human observer is strictly limited to cost difference. In general, it may\nbe possible that the human is using more complex distance functions and we may want\nto learn the distance function directly from the human. In this case, we could consider\nlearning a distance function that is a combination of various individual distance functions\nthat can be directly computed from the human’s model. Some of the candidate distance\nfunctions we could consider include measures like action-distance, causal-link distance,\nand state distance.\nAction distance\nWe denote the set of unique actions in a plan π as A(π) = {a | a ∈π}.\nGiven the action sets A(πMR) and A(π∗\nMR\nh ) of two plans πMR and π∗\nMR\nh respectively, the\naction distance is, DA(πMR, π∗\nMR\nh ) = 1 −\n|A(πMR)∩A(π∗\nMR\nh\n)|\n|A(πMR)∪A(π∗\nMR\nh\n)|. Here, two plans are similar\n(and hence their distance measure is smaller) if they contain same actions. Note that it\ndoes not consider the ordering of actions.\nCausal link distance\nA causal link represents a tuple of the form ⟨ai, pi, ai+1⟩, where pi\nis a predicate variable that is produced as an effect of action ai and used as a precondition\nfor the next action ai+1. The causal link distance measure is represented using the causal\nlink sets Cl(πMR) and Cl(π∗\nMR\nh ), DC(πMR, π∗\nMR\nh ) = 1 −\n|Cl(πMR)∩Cl(π∗\nMR\nh\n)|\n|Cl(πMR)∪Cl(π∗\nMR\nh\n)|.\nState sequence distance\nThis distance measure finds the difference between sequences\nof the states. Given two state sequences (sR\n0 , . . . , sR\nn ) and (sH\n0 , . . . , sH\nn′) for πMR and π∗\nMR\nh\nrespectively, where n ≥n′ are the lengths of the plans, the state sequence distance is,\nDS(πMR, π∗\nMR\nh ) = 1\nn\n\u0002 Pn′\nk=1 d(sR\nk , sH\nk )+n−n′\u0003 , where d(sR\nk , sH\nk ) = 1−|sR\nk ∩sH\nk |\n|sR\nk ∪sH\nk | represents\nthe distance between two states (where sR\nk is overloaded to denote the set of predicate\nvariables in state sR\nk ). The first term measures the normalized difference between states\nup to the end of the shortest plan, while the second term, in the absence of a state to\ncompare to, assigns maximum difference possible.\nAs mentioned earlier, the actual distance used by the human observer could be some\n\n3.3 Model-Free Explicable Planning\n39\ncombination of such constituent distance function, so we will need to consider composite\ndistance functions.\nDefinition 4. A composite distance, Dcomp is a distance between pair of two plans\n⟨πMR, πMR\nh ⟩, such that, Dcomp(πMR, πMR\nh , MR\nh ) = ||DA(πMR, πMR\nh ) + DC(πMR, πMR\nh ) +\nDS(πMR, πMR\nh )||.\nBut for each robot plan we want to find the minimum distance with respect to the set\nof human’s expected plans. We say a distance minimizing plan in the set of the expected\nplans is defined as follows:\nDefinition 5. A distance minimizing plan, π∗\nMR\nh , is a plan in ΠMR\nh , such that for a\nrobot plan, πMR, the composite distance is minimized, i.e., ∀π ∈ΠMR\nh Dcomp(πMR,π,MR\nh ) ≥\nDcomp(πMR,π∗\nMR\nh\n,MR\nh ).\nOur overall objective is to learn an inexplicability score from the individual distances.\nOne way to do it would be to first collect scores human participants and then try to learn\na mapping to human specified scores from the individual plan distances between a robot\nplan and corresponding distance minimizing plan in the set of expected plans. To that\nend, we define a explicability feature vector as follows:\nDefinition 6. An explicability feature vector, F, is a three-dimensional vector, which\nis given with respect to a distance minimizing plan pair, ⟨πMR, π∗\nMR\nh ⟩, such that, F =\n⟨D(πMR, π∗\nMR\nh ), DC(πMR, π∗\nMR\nh ), DS(πMR, π∗\nMR\nh )⟩T .\nThis allows us to learn an explicability distance function, ˆDExp(πMR / π∗\nMR\nh ), which\nis essentially a regression function, f, that fits the three plan distances to the total plan\nscores, with b as the parameter vector, and F as the explicability feature vector, such that,\nˆDExp(πMR / π∗\nMR\nh ) ≈f(F, b). Therefore, a regression model can be trained to learn the\nexplicability assessment (total plan scores) of the users by mapping this assessment to\nthe explicability feature vector which consists of plan distances for corresponding plans.\nSince we are unaware of the distance measure, we can never guess the right plan against\nwhich the human may be comparing the current behavior. The distance minimizing plan\nis a stand-in for this unknown plan. As such any distance function learned from features\nderived from this plan will be an approximation of the actual distance. Though in many\ncases such approximation acts as a reasonable guide for behavior generation.\n3.3\nModel-Free Explicable Planning\nIf the robot does not have access to the human’s mental model, then an approximation\nof it can be learned. This approach shows that it is not necessary to build a full-fledged\nplanning model of the human’s mental model, rather it is enough to learn a simple labeling\nfunction. Here the underlying hypothesis is that humans tend to associate abstract tasks or\nsub-goals to actions in a plan. If the human-in-the-loop can associate any domain-specific\n\n40\nExplicable Behavior Generation\nlabel to an action in the plan then the action is assumed to be explicable, otherwise, the\naction is considered inexplicable. Such a labeling scheme can be learned from training\nexamples annotated by humans.\nThis learned model can then be used as a heuristic\nfunction in the planning process. This approach is illustrated in Figure 3.2.\n3.3.1\nProblem Formulation\nIn this case, since the MR\nh is not known beforehand, the distance function D(·, ·, ·) in Defi-\nnition 2 is approximated using a learning method. Here the terms D(πMR, πMR\nh , MR\nh ) and\nby extension the inexplicability scores are measured in terms of the labels the human would\nassociate with the plan. Thus the method expects a set of task labels T = {T1, T2, . . . , TM}\nbe provided for each domain. Depending on how the human view the role of the action\nin the plan, each action may be associated with multiple labels. The set of action labels\nfor explicability is the power set of task labels: L = 2T . When an action label includes\nmultiple task labels, the action is interpreted as contributing to multiple tasks; when an\naction label is an empty set, the action is interpreted as inexplicable.\nFor a given plan πMR, we can define the inexplicability score of the plan as follows\nIE(πMR, MR\nh ) = f ◦L∗(πMR), where f is a domain-independent function that takes plan\nlabels as input, and L∗is the labeling scheme of the human for agent plans based on MR\nh .\nA possible candidate for f could be to compute the ratio between the number of actions\nwith empty action labels and the number of all actions.\n3.3.2\nLearning Approach\nNow the question is, for a new plan how do we predict the labels human would associate\nwith it? As mentioned earlier, one could use a training set consisting of plans annotated\nwith labels and train a sequence to sequence model. Here, possible models, one could\nuse include sequential neural networks (including Recurrent Neural Networks [Goodfellow\net al., 2016] variants and transformers) and probabilistic models like Hidden Markov Mod-\nels [Russell and Norvig, 2002] or Conditional Random Fields [Lafferty et al., 2001]. Here\neach step in the sequence corresponds to a step in the plan execution and the possible\nfeatures that could be associated with each step include features like the action descrip-\ntion, the state fluents obtained after executing each action in the sequence ⟨a0, ..., an⟩from\nthe initial state, etc. One could also potentially include other features like motion level\ninformation, like gripper position, orientation, etc. We will use the notation L′ to denote\nthe learned labeling model and to differentiate it from the true labeling model L∗.\n3.3.3\nPlan Generation\nOnce we have learned a labeling model, we can then use it to drive the behavior generation.\n\n3.3 Model-Free Explicable Planning\n41\n3.3.3.1\nPlan Selection\nThe most straightforward method is to perform plan selection on a set of candidate plans\nwhich can simply be a set of plans that are within a certain cost bound of the optimal\nplan. Candidate plans can also be generated to be diverse with respect to various plan\ndistances. For each plan, the features of the actions are extracted as we discussed earlier.\nThen the trained model L′ can be used to produce the labels for the actions in the plan\nand the inexplicability score IE can then be computed as given by the mappings above.\nThe inexplicability score can then be used to choose a plan that is least inexplicable.\nAlgorithm 2 Synthesis of Explicable Plan\nInput: MR, L′\nOutput: π∗\nMR\n1: open ←∅\n2: open.push(IR)\n3: while open ̸= ∅do\n4:\ns ←open.remove()\n5:\nif GR is reached then\n6:\nreturn s.plan {the plan that leads to s from IR)}\n7:\nCompute all possible next states N from s\n8:\nfor n ∈N do\n9:\nCompute the relaxed plan πRELAX for n\n10:\nConcatenate s.plan with πRELAX as πprime {s.plan contains plan features and\nπRELAX only contains action descriptions}\n11:\nCompute and add other relevant features\n12:\nCompute LπR = L′(πprime)\n13:\nCompute h = f(IE, hcost) {f is a combination function; hcost is the relaxed\nplanning heuristic}\n14:\nif h(n∗) < h∗{n∗∈N with minimum h} then\n15:\nopen.clear()\n16:\nopen.push(n∗); h∗= h(n∗) {h∗is initially MAX}\n17:\nelse\n18:\nPush all n ∈N into open\n3.3.3.2\nPlan Synthesis\nA more efficient way is to incorporate these measures as heuristics into the planning pro-\ncess. Algorithm 2 presents a simple modified version of Enforced Hill Climbing [Hoffmann,\n2005], that can make use of such learned heuristics. To compute the heuristic value given\na planning state, one can use the relaxed planning graph to construct the remaining plan-\nning steps. However, since relaxed planning does not ensure a valid plan, one can only\nuse action descriptions as plan features for actions that are beyond the current planning\nstate when estimating the inexplicability measures. These estimates are then combined\nwith the relaxed planning heuristic (which only considers plan cost) to guide the search.\n\n42\nExplicable Behavior Generation\n3.4\nEnvironment Design for Explicability\nThe environment in which the robot is operating may not always be conducive to explica-\nble behavior. As a result, making its behavior explicable may be prohibitively expensive\nfor the robot. Additionally, certain behaviors that are explicable with respect to the hu-\nman’s mental model may not be feasible for the robot. Fortunately, in highly structured\nsettings, where the robot is expected to solve repetitive tasks (like in warehouses, fac-\ntories, restaurants, etc.), it might be feasible to redesign the environment in a way that\nimproves explicability of the robot’s behavior, given a set of tasks. This brings us to the\nnotion of environment design which involves redesigning the environment to maximize (or\nminimize) some objective for the robot. Thus, environment design can be used to boost\nthe explicability of the robot’s behavior, especially in settings that require solving repeti-\ntive tasks and a one-time environment design cost to boost explicable behavior might be\npreferable over the repetitive cost overhead of explicable behavior borne by the robot.\nHowever, environment design alone may not be a panacea for explicability. For one, the\ndesign could be quite expensive, not only in terms of making the required environment\nchanges but also in terms of limiting the capabilities of the robot. Moreover, in many\ncases, there may not be a single set of design modifications that will work for a given set\nof tasks. For instance, designing a robot with wheels for efficient navigation on the floor\nwill not optimize the robot’s motion up a stairwell. This means, to achieve truly effective\nsynergy with autonomous robots in a shared space, we need a synthesis of environment\ndesign and human-aware behavior generation. In this section, we will talk about how we\ncould trade off the one-time (but potentially expensive) design changes, against repetitive\ncosts borne by the robot to exhibit explicable behavior.\n3.4.0.1\nMotivating Example\nConsider a restaurant with a robot server (Figure 9.3a). Let G1 and G2 represent the\nrobot’s possible goals of serving the two booths: it travels between the kitchen and the\ntwo booths. The observers consist of customers at the restaurant. Given the position of the\nkitchen, the observers may have expectations on the route taken by the robot. However,\nunbeknownst to the observers, the robot can not traverse between the two tables and can\nonly take the route around the tables. Therefore, the path marked in red is the cheapest\npath for the robot but the observers expect the robot to take the path marked in green\nin Figure 9.3a.\nIn this environment, there is no way for the robot to behave as per the human’s\nexpectations. Applying environment design provides us with alternatives. For example,\nthe designer could choose to build two barriers as shown in Figure 3.3b.\nWith these\nbarriers in place, the humans would expect the robot to follow the path highlighted in\ngreen. However, whether it is preferable to perform environment modifications or to bear\nthe impact of inexplicable behavior depends on the cost of changing the environment\nversus the cost of inexplicability caused by the behavior.\n\n3.4 Environment Design for Explicability\n43\n(a) Explicable behavior is costlier without design.\n(b) Optimal behavior is explicable with design.\nFig. 3.3:\nUse of environment design to improve the explicability of a robot’s behavior in a shared\nenvironment.\n\n44\nExplicable Behavior Generation\n3.4.1\nProblem Setting\nWe again consider an explicability problem of the PExp = ⟨MR, MR\nh , DMR\nh ⟩.\nIn this\nsubsection we will again consider cases where the human mental model is given upfront.\nLet the expected set of plans in the human model. ΠMR\nh , be the set of plans optimal\nin MR\nh . The robot plans to minimize the inexplicability score in the human’s mental\nmodel. We will use the notation Π∗\nIE(·,MR\nh ,δMR\nh\n) (in the absence of the parameter πR) to\nrefer to the set of plans in the robot’s model with the lowest inexplicability score, and\nIEmin(PExp) to represent the lowest inexplicability score associated with the set. Further,\nlet fExp be the decision function used by the explicable robot: fExp(PExp) represents the\ncheapest plan that minimizes the inexplicability score, i.e. fExp(PExp) ∈Π∗\nIE(·,MR\nh ,δMR\nh\n)\nand ¬∃π′ : π′ ∈Π∗\nIE(·,MR\nh ,δMR\nh\n) such that cR(π′) < cR(fExp(PExp)).\n3.4.1.1\nEnvironment Design\nBefore we delve more into the problem of environment design to boost explicability, lets\ntake a general look at environment design problems. An environment design problem takes\nas input the initial environment configuration along with a set of available modifications\nand computes a subset of modifications that can be applied to the initial environment to\nderive a new environment in which a desired objective is optimized.\nWe consider MR0 = ⟨F 0, AR0, IR0, GR0, CR0⟩as the initial environment and ρR as\nthe set of valid configurations of that environment: MR0 ∈ρR. Let O be some metric\nthat needs to be optimized with environment design, i.e a planning model with lower value\nfor O is preferred. A design problem is a tuple ⟨MR0, ∆, ΛR, C, O⟩where, ∆is the set of\nall modifications, ΛR : ρR × 2∆→ρR is the model transition function that specifies the\nresulting model after applying a subset of modifications to the existing model, C : ∆→R\nis the cost function that maps each design choice to its cost.\nThe modifications are\nindependent of each other and their costs are additive. We will overload the notation and\nuse C as the cost function for a subset of modifications as well, i.e. C(ξ) = P\nξi∈ξ C(ξ).\nThe set of possible modifications could include modifications to the state space, action\npreconditions, action effects, action costs, initial state and goal. In general, the space of\ndesign modifications, which are an input to our system, may also involve modifications to\nthe robot itself (since the robot is part of the environment that is being modified). An\noptimal solution to a design problem identifies the subset of design modifications, ξ, that\nminimizes the following objective consisting of the cost of modifications and the metric\nO: min O(ΛR(MR0, ξ)), C(ξ).\n3.4.2\nFramework for Design for Explicability\nIn this framework, we not only discuss the problem of environment design with respect to\nexplicability but also in the context of (1) a set of tasks that the robot has to perform in\n\n3.4 Environment Design for Explicability\n45\nthe environment, and (2) over the lifetime of the tasks i.e. the time horizon over which the\nrobot is expected to repeat the execution of the given set of tasks. These considerations\nadd an additional dimension to the environment design problem since the design will have\nlasting effects on the robot’s behavior. In the following, we will first introduce the design\nproblem for a single explicable planning problem, then extend it to a set of explicable\nplanning problems and lastly extend it over a time horizon.\n3.4.2.1\nDesign for a Single Explicable Problem\nIn the design problem for explicability, the inexplicability score becomes the metric that\nwe want to optimize for. That is we want to find an environment design such that the\ninexplicability score is reduced in the new environment. This problem can be defined as\nfollows:\nDefinition 7. The design problem for explicability is a tuple,\nDPExp = ⟨P0\nExp, ∆, ΛExp, C, IEmin⟩,\nwhere:\n• P0\nExp ∈ρExp is the initial configuration of the explicable planning problem, where\nρExp represents the set of valid configurations for PExp.\n• ∆is the set of available design modifications. The space of all possible modifications\nis the power-set 2∆.\n• ΛExp : ρExp × 2∆→ρExp is the transition function over the explicable planning\nproblem, which gives an updated problem after applying the modifications.\n• C is the additive cost associated with each design in ∆.\n• IEmin : ρExp →R is the minimum possible inexplicability score in a configuration,\ni.e. the inexplicability score associated with the most explicable plan.\nWith respect to our motivating example in Figure 3.3a, DPExp is the problem of\ndesigning the environment to improve the robot’s explicability given its task of serving\nevery new customer at a booth (say G1) only once.\nThe optimal solution to DPExp\ninvolves finding a configuration which minimizes the minimum inexplicability score. We\nalso need to take into account an additional optimization metric which is the effect of\ndesign modifications on the robot’s plan cost. That is, we need to examine to what extent\nthe decrease in inexplicability is coming at the robot’s expense. For instance, if you confine\nthe robot to a cage so that it cannot move, its behavior becomes completely and trivially\nexplicable, but the cost of achieving its goals goes to infinity.\nDefinition 8. An optimal solution to DPExp, is a subset of modifications ξ∗that\nminimizes the following:\nmin IEmin( P∗\nExp), C(ξ∗), cR(fExp(P∗\nExp))\n(3.1)\nwhere P∗\nExp = ΛExp(P0\nExp, ξ∗) is the final modified explicable planning problem, IEmin(·)\nrepresents the minimum possible inexplicability score for a given configuration, C(ξ∗) de-\nnotes the cost of the design modifications and cR(fExp(P∗\nExp)) is the cost of the cheapest\nmost explicable plan in a configuration.\n\n46\nExplicable Behavior Generation\n3.4.2.2\nDesign for Multiple Explicable Problems\nWe will now show how DPExp evolves when there are multiple explicable planning prob-\nlems in the environment that the robot needs to solve. When there are multiple tasks\nthere may not exist a single set of design modifications that may benefit all the tasks.\nIn such cases, a solution might involve performing design modifications that benefit\nsome subset of the tasks while allowing the robot to act explicably with respect to\nthe remaining set of tasks. Let there be k explicable planning problems, given by the\nset PExp = {⟨MR(0), MR\nh (0), δMR\nh (0)⟩, . . . , ⟨MR(k), MR\nh (k), δMR\nh (k)⟩}, with a categorical\nprobability distribution P over the problems. We use PExp(i) ∈PExp to denote the ith\nexplicable planning problem. These k explicable problems may differ in terms of their\ninitial state and goal conditions. Now the design problem can be defined as:\nDPExp,P = ⟨P0\nExp, P, ∆, ΛExp, C, IEmin,P⟩,\n(3.2)\nwhere P0\nExp, is the set of planning tasks in the initial environment configuration, IEmin,P\nis a function that computes the minimum possible inexplicability score in a given environ-\nment configuration by taking the expectation over the minimum inexplicability score for\neach explicable planning problem, i.e., IEmin,P(PExp) = E[IEmin(PExp)], where PExp ∼P.\nWith respect to our running example, DPExp,P is the problem of designing the environ-\nment given the robot’s task of serving every new customer only once at either of the booths\n(G1, G2) with probability given by P.\nThe solution to DPExp,P has to take into account the distribution over the set of\nexplicable planning problems. Therefore the optimal solution is given by:\nmin\nIEmin,D( P∗\nExp), C(ξ∗), E[cR(fExp(P∗\nExp))]\n(3.3)\nwhere P∗\nExp ∼P. A valid configuration minimizes the minimum possible inexplicability\nscore, which involves 1) expectation over minimum inexplicability scores for each explicable\nplanning problem; 2) the cost of the design modifications (these modifications are applied\nto each explicable planning problem); and 3) the expectation over the cheapest most\nexplicable plan for each explicable planning problem.\n3.4.2.3\nLongitudinal Impact on Explicable Behavior\nThe process of applying design modifications to an environment makes more sense if the\ntasks are going to be performed repeatedly in the presence of a human (i.e. the robot\ndoes not have to bear the cost of being explicable repeatedly). This has quite a different\ntemporal characteristic in comparison to that of execution of one-time explicable behavior.\nFor instance, design changes are associated with a one-time cost (i.e. the cost of applying\nthose changes in the environment). On the other hand, if we are relying on the robot to\nexecute explicable plans at the cost of foregoing optimal plans, then it needs to bear this\ncost multiple times in the presence of a human over the time horizon.\nWe will use a discrete time formulation where the design problem is associated with a\ntime horizon T . At each time step, one of the k explicable planning problems is chosen.\n\n3.4 Environment Design for Explicability\n47\nFig. 3.4: Illustration of longitudinal impact on explicability. Prob determines the probability associated\nwith executing each task in PExp. For each task, the reward is determined by the inexplicability score of\nthat task. The probability of achieving this reward is determined by γ × probability of executing that\ntask. Additionally, with a probability (1 −γ) the human ignores the inexplicability of a task and the\nassociated reward is given by an inexplicability score of 0.\nNow the design problem can be defined as:\nDPExp,P,T = ⟨P0\nExp, P, ∆, ΛExp, C, IEmin,P, T ⟩\n(3.4)\nIn our running example, DPExp,P,T is the problem of designing the environment given\nthe robot’s task of serving the same customer at either of the booths with a distribution\nP over a horizon T .\nNote than earlier discussions on explicable behavior generation focused on cases where\nthere was only a single interaction between the human and the robot. However, in this\nsection we consider a time horizon, T > 1, over which the robot’s interaction with the\nhuman may be repeated multiple times for the same task.\nThis means the human’s\nexpectations about the task can evolve over time. This may not be a problem if the robot’s\nbehavior aligns perfectly with the human’s expectations. Although, if the robot’s plan for\na given task is associated with a non-zero inexplicability score, then the human is likely to\nbe more surprised the very first time she notices the inexplicable behavior than she would\nbe if she noticed the inexplicable behavior subsequent times. As the task is performed\nover and over, the amount of surprise associated with the inexplicable behavior starts\ndecreasing. In fact, there is a probability that the human may ignore the inexplicability of\nthe robot’s behavior after sufficient repetitions of the task. We incorporate this intuition\nby using discounting.\nFigure 3.4 illustrates the Markov reward process to represent the dynamics of this\nsystem. Let (1 −γ) denote the probability that the human will ignore the inexplicability\nof the robot’s plan, i.e, the reward will have inexplicability score 0. γ times the probability\nof executing a task represents the probability that the reward will have the minimum\ninexplicability score associated with that task. Assuming γ < 1, the minimum possible\n\n48\nExplicable Behavior Generation\ninexplicability score for a set of explicable planning problems is:\nfT (IEmin,P(PExp)) = IEmin,P(PExp)\n+ γ ∗IEmin,P(PExp) + . . . +\nγT−1 ∗IEmin,P(PExp)\nfT (IEmin,P(PExp)) = 1 −γT\n1 −γ ∗IEmin,P(PExp)\n(3.5)\nThus the optimal solution to DPExp,P,T is given by:\nmin\nfT (IEmin,P(P∗\nExp)), C(ξ∗),\nE[cR(fExp(P∗\nExp))] ∗T\n(3.6)\nwhere, P∗\nExp ∼P. The optimal solution is a valid configuration that minimizes 1) the\nminimum possible inexplicability over the set of explicable planning problems given the\nhuman’s tolerance to inexplicable behavior; 2) one-time cost of the design modifications;\nand 3) the expectation over the cheapest most explicable plan for each explicable planning\nproblem given a time horizon. Note that, since the design cost is not discounted and we\nalways make the design changes before the task is solved, there is never a reason to delay\nthe design execution to future steps in the horizon. Instead it can be executed before the\nfirst time step.\n3.4.3\nSearch for Optimal Design\nWe can find the optimal solution for DPExp,P,T , by performing a breadth-first search over\nthe space of environment configurations that are achievable from the initial configuration\nthrough the application of the given set of modifications. The performance of the search\ndepends on the number of designs available. By choosing appropriate design strategies,\nsignificant scale up can be attained. Each search node is a valid environment configuration\nand the possible actions are the applicable designs. For simplicity, we convert the multi-\nobjective optimization in Equation 3.1 into a single objective as a linear combination of\neach term associated with a coefficients α, β, and κ, respectively. The value of each node\nis decided by the aforementioned objective function. For each node, it is straightforward\nto calculate the design modification cost. However, in order to calculate the minimum\ninexplicability score and the robot’s plan cost, we have to generate a plan that minimizes\nthe inexplicability score for each explicable planning problem in that environment con-\nfiguration. In this setting one could use the method discussed in Section 3.2. Since the\ndistances here are specifically limited to cost differences, we can also use a compilation\nbased method, that compiles the problem of generating the explicable plan to a classical\nplanning problem. We will discuss this compilation in detail in Chapter 6. Essentially, the\nsearch has two loops: the outer loop which explores all valid environment configurations,\nand the inner loop which performs search in a valid environment configuration to find a\nplan that minimizes the inexplicability score. At the end of the search, the node with best\nvalue is chosen, and the corresponding set of design modifications, ξ∗, is output.\nOne way to optimize the search over the space of environment configurations is to only\nconsider the designs that are relevant to the actions in the optimal robot plans (Π∗\nMR)\n\n3.5 Bibliographic Remarks\n49\nand those in the human’s expected plans (Π∗\nMR\nh ) given the set of tasks.\nThis can be\nimplemented as a pruning strategy that prunes out designs that are not relevant to the\nactions.\n3.4.4\nDemonstration of Environment Design for Explicability\nWe use our running example from Figure 9.3a to demonstrate how the design problem\nevolves. We constructed a domain where the robot had 3 actions: pick-up and put-down to\nserve the items on a tray and move to navigate between the kitchen and the booths. Some\ngrid cells are blocked due to the tables and the robot cannot pass through these: cell(0,\n1) and cell(1, 1). Therefore, the following passages are blocked: cell(0, 0)-cell(0, 1), cell(0,\n1)-cell(0, 2), cell(0, 1)-cell(1, 1), cell(1, 0)-cell(1, 1), cell(1, 1)-cell(1, 2), cell(1, 1)-cell(2,\n1). We considered 6 designs, each consisting of putting a barrier at one of the 6 passages\nto indicate the inaccessibility to the human (i.e. the design space has 26 possibilities).\nFor the following parameters: α = 1, β = 30, κ = 0.25 and γ = 0.9, let us consider the\nproblem of identifying design for three settings: (a) single explicable problem for T = 1,\n(b) multiple explicable problems for T = 1, and (c) multiple explicable problems for\nT = 10. As mentioned earlier, we will use the set of plans optimal in the human model as\nthe expected plan set and the difference between cost of the plans as the distance function.\nLet us concretize the three settings as follows, (a) involves serving a new customer at a\nbooth (say G1) only once, (b) involves serving a new customer only once at either of the\nbooths with equal probability and (c) involves serving each customer at most 10 times\nat either of the booths with equal probability. We found that for settings (a) and (b)\nno design was chosen. This is because these settings are over a single time step and the\ncost of installing design modifications in the environment is higher than the amount of\ninexplicability caused by the robot (β > α).\nOn the other hand, for setting (c), the\nalgorithm generated the design in Figure 3.3b, which makes the robot’s roundabout path\ncompletely explicable to the customers.\n3.5\nBibliographic Remarks\nThe first paper to introduce explicability was Zhang et al. [2017] that introduced the\nmodel-free explicability. The paper used CRF Lafferty et al. [2001] to learn the labeling\nmodel and a modified version of the FF planner [Hoffmann, 2001] to generate the plan. The\npaper also performed some limited user studies to test whether such models can capture\npeople’s measure of plan explicability. The model-based explanation was introduced by\nKulkarni et al. [2019a] and used a modified version of Helmert [2006] for the reconciliation\nsearch and used a learned distance function which used features described in Definition 6.\nThe specific distance functions considered was originally discussed and used in the context\nof diverse planning (cf. [Srivastava et al., 2007, Nguyen et al., 2012]) where these distance\nfunctions are used to generate plans of differing characteristics. The paper also used data\ncollected from participants to learn the distance function. The design for explicability\nwas proposed by Kulkarni et al. [2020a] and followed the other design works that have\nbeen considered within the context of planning (cf. [Keren et al., 2014, 2016, 2018]). In\n\n50\nExplicable Behavior Generation\ngeneral, most of these works can be seen as special cases of the more general environment\ndesign work studied in papers like Zhang et al. [2009]. The specific paper Kulkarni et al.\n[2020a] used a compilation based method to generate the explicable plan as the paper\nlooked at a cost difference based distance. The compilation follows the methods discussed\nin Sreedharan et al. [2020a], but removes the use of explanatory actions. We will discuss\nthe base compilation in more detail in Chapter 7.\n\nChapter 4\nLegible Behavior\nIn this chapter, the discussion will focus on another type of interpretable behavior, namely\nlegibility. The notion of legibility allows the robot to implicitly communicate information\nabout its goals, plans (or model, in general) to a human observer. For instance, consider\na human robot cohabitation scenario consisting of a multi-tasking robot with varied ca-\npabilities that is capable of performing a multitude of tasks in an environment. In such\na scenario, it is crucial for the robot to aid the human’s goal or plan recognition process,\nas the human observer may not always know the robot’s intentions or objectives before-\nhand. Hence, in such cases, it may be useful for the robot to communicate information\nthat the human is unaware of. As the better off the human is at identifying the robot’s\ngoals or plans accurately, the better off is the overall team performance. However, explicit\ncommunication of objectives might not always be suitable. For instance, the what, when\nand how of explicit communication may require additional thought. Further, several other\naspects like cost of communication (in terms of resources or time), delay in communication\n(communications signals may take time to reach the human), feasibility of communication\n(broken or unavailable sensors), etc., may also need to be considered. On the other hand,\nthe robot can simply synthesize a behavior that implicitly communicates the necessary\ninformation to the human observer.\nWe will discuss the notion of legibility from the perspective of an offline setting where\nthe observer has partial observability of the robot’s actions. That is, the robot has to\nsynthesize legible behavior in a setting where the human observer has access to the ob-\nservations emitted from the entire execution trace of the robot and these observations do\nnot always reveal the robot’s exact action or state to the human. As the human is trying\nto infer the robot’s goals or plans, she operates in a belief space due to the partial observ-\nability of the robot’s activities. The robot has to modulate the human’s observability and\nchoose actions such that the ambiguity over specific goals or plans is reduced. We refer to\nthis as the controlled observability planning problem (COPP). In the upcoming sections,\nwe will see how the COPP formulation can be used to synthesize goal legible behavior as\nwell as plan legible behavior in an offline setting with partial observability of the robot’s\nactivities.\n4.1\nControlled Observability Planning Problem\nIn this framework, we consider two agents: a robot and the human observer. The robot\nhas full observability of its activities. However, the observer only has partial observability\nof the robot’s activities. The observer is aware of the robot’s planning model and receives\nobservations emitted as a side effect of the robot’s execution. This framework supports\nan offline setting, and therefore the observer only receives the observations after the robot\nhas finished executing the entire plan.\nIn this setting, the robot has a set of candidate goals, inclusive of its true goal. The\n51\n\n52\nLegible Behavior\ncandidate goals are the set of possible goals that the robot may achieve in the given\nenvironment. The observer is aware of the robot’s candidate goal set but is unaware of\nthe robot’s true goal. We now introduce a general COPP framework that will be used to\ndefine the goal legibility and plan legibility problems in the upcoming sections.\nDefinition 9. A controlled observability planning problem is a tuple, PCO =\n⟨MR, G, Ω, O⟩, where,\n• MR is the planning model of the robot.\n• G = {G1 ∪G2 . . . ∪Gn−1 ∪GR} is a set of candidate goal conditions, each defined by\nsubsets of fluent instantiations, where GR is the true goal of the robot.\n• Ω= {oi|i = 1, . . . , m} is a set of m observations that can be emitted as a result of\nthe action taken and the state transition.\n• O : (A × S) →Ωis a many-to-one observation function associated with the human\nwhich maps the action taken and the next state reached to an observation in Ω.\nThat is to say, the observations are deterministic, each ⟨a, s′⟩pair is associated with\na single observation but multiple pairs can be mapped to the same observation.\nNote that here the human’s expectation of the robot (i.e. MR\nh ) is defined in a dis-\ntributed fashion. In particular, this setting assumes that the expectation matches the real\nmodel in all aspects but the goals. That is the human is unaware of the true goal and\nonly has access to the set of potential goals. Moreover the human is a noisy observer\n(as defined by the observation model O and the set of observations Ω). To simplify the\nsetting we will focus on the cases where the human is aware of the observation model,\ni.e., from the observation they could potentially guess the possible state action pairs that\ncould have generated it. Therefore, the human observer operates in the belief space. The\nrobot takes the belief space of the observer into account during its planning process, so as\nto modulate what the human believes the current possible states could be.\n4.1.1\nHuman’s Belief Space\nThe human may use its observations of the robot’s activity to maintain a belief state. A\nbelief state is simply a set of possible states consistent with the observations. We use ˆs as\na notational aid to denote a state that is a member of the belief state.\nDefinition 10. The initial belief, b0, induced by observation, o0 is defined as, b0 =\n{ˆs0 | O(∅, s0) = o0 ∧O(∅, ˆs0) = o0}.\nWhenever a new action is taken by the robot, and the state transition occurs, the\nhuman’s belief updates as follows:\nDefinition 11. A belief update, bi+1 for belief bi is defined as, bi+1 = update(bi, oi+1) =\n{ˆsi+1 | ∃ˆa, δ(ˆsi, ˆa) |= ˆsi+1 ∧ˆsi ∈bi ∧O(ˆa, ˆsi+1) = oi+1}.\n\n4.1 Controlled Observability Planning Problem\n53\nA sequence of belief updates gives us the observer’s belief sequence that is consistent\nwith the observation sequence emitted by the robot.\nDefinition 12. A belief sequence induced by a plan p starting at state s0, BS(p, s0),\nis defined as a sequence of beliefs ⟨bo, b1, . . . , bn⟩such that there exist o0, o1, o2, . . . , on ∈Ω\nwhere,\n• oi = O(ai, si)\n• bi+1 = update(bi, oi+1)\nThe set of plans that are consistent with the belief sequence of a given plan are called\nas belief plan set.\nDefinition 13. A belief plan set, BPS(p, s0) = {p1, . . . , pn}, induced by a plan p starting\nat s0, is a set of plans that are formed by causally consistent chaining of state sequences\nin BS(p, s0), i.e., BPS(p, s0) = {⟨ˆs0, ˆa1, ˆs1, . . . , ˆsn⟩| ∀ˆaj, ˆsj−1 |= pre(ˆaj) ∧ˆsj−1 ∈\nbj−1 ∧ˆsj |= ˆsj−1 ∪adds(ˆaj) \\ dels(ˆaj) ∧ˆsj ∈bj}.\nThe robot’s objective is to generate a belief sequence in human’s belief space, such\nthat, the last belief in the sequence satisfies certain desired conditions.\n4.1.2\nComputing Solutions to COPP variants\nNow we present a common algorithm template that can be used to compute plans for the\nCOPP problem variants.\n4.1.2.1\nAlgorithm for Plan Computation\nIn this algorithm, each search node is represented by a belief estimate, which is a ∆-sized\nsubset of the belief state. A search node, b∆, consists of state s and a set of possible states,\n˜b (cardinality of ˜b is given by ∆−1, where ∆is a counter). The successor node uses the\ns in b∆to generate the successor state, and b∆to create the next belief state. There\nare two loops in the algorithm: the outer and the inner loop. The outer loop maintains\nthe cardinality of b∆by incrementing the value of ∆in each iteration, such that value\nof ∆ranges from 1, 2, . . . , |S|. In the inner loop, a heuristic-guided forward search (for\ninstance, GBFS[Russell and Norvig, 2002]) can be used to search over space of belief states\nof cardinality less than or equal to ∆. These loops ensure the complete exploration of the\nbelief space. The algorithm terminates either when a plan is found or after running the\nouter loop for |S| iterations. The outer loop ensures that all the paths in the search space\nare explored.\nProposition 1. The algorithm necessarily terminates in finite number of |S| iterations,\nsuch that, the following conditions hold:\n(Completeness) The algorithm explores the complete solution space of PCO, that is, if\nthere exists a πPCO that correctly solves PCO, it will be found.\n\n54\nLegible Behavior\nAlgorithm 3 COOP Solution Plan Algorithm\nInput: PCO = ⟨D, G, Ω, O⟩\nOutput: Solution πPCO, observation sequence, OPCO\n1: Initialize open, closed, unopened lists and the counter ∆←1\n2: b∆←{I} ; b0 ←{O(∅, I)} {Initialize initial search node, initial belief}\n3: open.push(⟨b∆, b0⟩, priority = 0)\n4: while ∆⩽|S| do\n5:\nwhile open ̸= ∅do\n6:\nb∆, b, h(b∆) ←open.pop()\n7:\nif |b∆| > ∆then\n8:\nunopened.push(⟨b∆, b⟩, h(b∆)); continue\n9:\nclosed ←closed ∪b∆\n10:\nif ⟨b∆, b⟩|= GOAL-TEST(G) then\n11:\nreturn πPCO, OPCO\n12:\nfor s′ ∈successors(s) do\n13:\no ←O(a, s′)\n14:\nb′ ←Belief-Generation(b, o)\n15:\nb′\n∆= ⟨s′,˜b′⟩{˜b′ of size ∆-1}\n16:\nh(b′\n∆) ←HEURISTIC-FUNCTION(b′\n∆, b′)\n17:\nadd b′\n∆to open if not in closed\n18:\n∆←∆+ 1\n19:\ncopy items of unopened to open, empty unopened\n20:\n21: procedure Belief-Generation(b, o)\n22: b′ ←{}\n23: for ˆs ∈b do\n24:\nfor ˆa ∈A do\n25:\nif O(ˆa, δ(ˆs, ˆa)) = o then\n26:\nb′ ←b′ ∪δ(ˆs, ˆa)\n27: return b′\n(Soundness) The plan, πPCO, found by the algorithm correctly solves PCO as ensured by\nthe corresponding goal-test.\nWe have not yet defined the goal-test and the notion of validity of a solution for a\nCOPP problem. We will be encoding the notion of legibility we wish to pursue as part\nof the goal-test. We will do so by ensuring that the legibility score for the corresponding\nplan is above certain threshold. We will look at some specific legibility scores we can use\nas we ground this specific algorithmic framework for various use cases.\n4.1.2.2\nOptimization\nIn order to speed up the search process, we perform an optimization on the aforementioned\nalgorithm.\nFor each search node, b∆, apart from the approximate belief estimate, we\nmaintain the full belief update b consistent with a path to s. The approximate belief\nupdate b∆can be generated by choosing ∆-sized combinations of states from the complete\n\n4.2 Goal Legibility\n55\nbelief. For example, when ∆= 1, b∆only consists of the state s but still maintains full\nbelief update b, when ∆= 2, b∆consists of a new combination of approximate belief of\nsize 2 derived from the maintained full belief. When ∆= 1, because of the check for\nduplicate states in the closed list, only one path to the search node is explored. Therefore,\nthe use of ∆allows the search process to explore multiple paths leading to a particular\nsearch node. The complete b helps in finding the problem variant solutions faster at lower\n∆values. We present the details of the optimization in Algorithm 3. In the following\nsections, we show how we customize the goal-test (line 10) and the heuristic function (line\n16) to suit the needs of each of the COPP problem variants.\n4.1.3\nVariants of COPP\nWithin this framework, we will discuss the problem of goal legibility and plan legibility.\nWith goal legibility, the objective of the robot is to convey at most j goals to the observer.\nWith plan legibility, we look at a specific instance of COPP problem where |G| = 1. Since\nwe know that COPP problems require that GR be part of |G| this means that in these\ncases the human knows the true goal of the robot. But even then the human observer may\nnot know the exact plan being executed by the robot given their limited sensors. Thus\nthe objective of the robot within plan legibility becomes to constrain the uncertainty of\nthe observer to at most m similar plans to the goal. We will see both of these problems\nin detail in the following sections. The COPP framework can be also used in adversarial\nenvironments. These variants will be covered in Chapter 10.\n4.2\nGoal Legibility\nIn this setting, the robot’s objective is to convey some information about its candidate\ngoal set to the human observer. This may involve communicating its true goal to human,\nby ensuring at most one goal is consistent with the observation sequence produced. Or in\ngeneral, the robot may want to communicate a set of at most j candidate goals inclusive\nof its true goal, to the human observer. Essentially, the point with goal legibility is to\nreduce the observer’s ambiguity over the robot’s possible goal set by narrowing it down\nto at most j goals. Which mean in this scenario, for a given plan π the corresponding\nlegibility score is given as L(π, PCO) ∝\n1\n| {G | G∈G∧G∈BS(π,I) |.\n4.2.0.1\nExample\nLet’s understand this problem with an example. Consider a situation where the robot is\na port management agent and a human is the supervisor that has sensors or subordinates\nat the port who provide partial information about the nature of activity being carried out\nat the port (refer Figure 4.1). For instance, when a specific crate is loaded onto the ship,\nthe observer finds out that something was loaded, but not the identity of the loaded crate.\nThe observer knows the initial inventory at the port, but when new cargo is acquired\nby the port, the observer’s sensors reveal only that more cargo was received; they do not\nspecify the numbers or identities of the received crates. A legible plan for loading sensitive\n\n56\nLegible Behavior\n(a)\n(b)\nFig. 4.1: The differences in belief sequences induced by different plans for an observer with noisy\nsensors.\n\n4.2 Goal Legibility\n57\ncargo (the red crate) and acquiring more cargo may first load the crate and then acquire\nmore crates. This plan reveals the identity of the crate that was loaded based on the\nobservers’ information about the remaining cargo in the port: the final belief state has a\nunique crate loaded on the ship even though it retains uncertainty about the new cargo\nin the port. However, if the plan were to first acquire more cargo, the observer’s sensors\nare insufficient to determine which crate was loaded: the plan maintains ambiguity in the\nobserver’s belief. This is reflected in the observer’s belief state sequence, where the last\nbelief state includes states with all different types of crates in the ship. Although both\nplans have the same cost and effects for the dock, one conveys the activity being carried\nout while the other adds more uncertainty. The COPP framework allows the robot to\nselect plans that may be legible in this manner.\n4.2.0.2\nGoal Legibility Problem\nAs mentioned earlier, in goal legibility problem, the robot’s aim is to take actions exclusive\nto the goal so as to help the observer in goal deduction.\nDefinition 14. A goal legibility planning problem is a PCO, where, G = {GR ∪G1 ∪\n. . . ∪Gn−1} is the set of n goals where GR is the true goal of the robot, and G1, . . . , Gn−1\nare confounding goals.\nRather than optimizing directly for the legibility score, we will instead limit ourselves\nto solutions whose legibility score is above a certain threshold.\nIn particular, we can\ngenerate a plan that conveys at most j candidate goals inclusive of its true goal. Thus\nrobot has to ensure that the observation sequence of a legible plan is consistent with at\nmost j goals so as to limit the number of goals in the observer’s final belief state.\nDefinition 15. A plan, πj, is a j-legible plan, if δ(I, πj) |= GR and the last belief,\nbn ∈BS(πj, I), satisfies the following, |G ∈G : ∃s ∈bn, s |= G| ⩽j, where 1 ⩽j ⩽n.\n4.2.1\nComputing Goal Legible Plans\nIn the case of goal legibility, we run the algorithm with the robot’s true goal.\n4.2.1.1\nGoal test\nIn order to ensure that the computed plan is consistent with at most j true goals, we\nchange our goal condition to additionally check whether at most j −1 confounding goals\nhave been achieved in the last belief in BS, or it can be interpreted as at least n −j goals\nare absent in the belief.\n\n58\nLegible Behavior\nFig. 4.2: Illustration of the impact of plan legibility on the observer’s plan inference process.\n4.2.1.2\nHeuristic function\nIn this case, our objective is to avoid at least n−j goals, that is be consistent with at most\nj goals. We achieve this by minimizing the heuristic cost to the true goal from the robot’s\nactual state and to the j −1 confounding goals from the robot’s belief state. However, we\nmaximize the heuristic cost to other n −j goals in order to achieve at most j goals in the\nlast belief state. This is written as,\nh(s) = hGA(s) + hGj−1(b) −hGn−j(b)\n(4.1)\n4.3\nPlan Legibility\nIn this problem setting, the observer is aware of the robot’s goal, since the candidate goal\nset consists of a single goal. Although, the observer is unaware of the robot’s choice of\nplan to achieve that goal. In general, there can be multiple different plans that allow the\nrobot to achieve a goal. This is further complicated by the fact that the observer has\npartial observability of the robot’s activities. Therefore, in this problem setting the goal\nof the robot is to reduce the ambiguity over the possible plans to its goal. The robot can\nachieve this, by computing a plan whose observation sequence conforms to a set of plans\nthat are similar in terms of their actions, thereby making it easy for the observer to guess\nthe actions executed by the agent.\nFor instance, in Figure 4.2, the goal of the agent is to pick up a medkit and treat the\nvictim on the disaster site. The observer has a noisy sensor model and it is not accurate\nenough to give the exact cell location of the robot. Therefore, the observer relies on how\nmany steps the robot has moved to guess the robot’s path of execution. In the initial\nstate shown in the leftmost subfigure in Figure 4.2, it can be seen that the robot has\ntwo choices of medkits to pick from. If it chooses the medkit as shown in the rightmost\nsub-figure, there are only two paths of length 2 towards the medkit and one path of length\n2 towards the victim. In comparison, the middle sub-figure, shows the various different\npaths of length 4 that can lead to the other medkit, making it harder for the human to\nguess the robot’s actions. Therefore, to reduce the ambiguity over its path, the robot\n\n4.3 Plan Legibility\n59\nchooses the medkit shown in the rightmost figure. This involves an observation sequence\nwith two similar paths leading to the goal, making it easy for the human to guess some\nof the actions of the robot. Thus, the plan legibility problem can be solved by finding an\nobservation sequence that is consistent with plans that are similar to each other.\nDefinition 16. A plan legibility planning problem is a tuple, PPL = ⟨D, GPL, Ω, O⟩,\nwhere, |GPL| = 1.\n4.3.1\nComputing Plan Legible Plans\nThe solution to a plan legibility problem is an m-similar plan. An m-similar plan is a plan\nwhose observation sequence is consistent with at least m similar plans to the goal, such\nthat, these plans are at most d distance away from each other. In order to compute an m-\nsimilar plan, we need to keep track of the plans that are consistent with the observation\nsequence and reach the goal. To compute the diversity between all the pairs of plans\nconsistent with the observation sequence, a plan distance measure like action distance,\ncausal link distance, state sequence distance (discussed in Chapter 3) can be used. This\napproach can use any valid plan distance. We now define an m-similar plan.\nDefinition 17. Two plans, p1, p2, are a d-distant pair with respect to distance function\nD if, D(p1, p2) = d, where D is a diversity measure.\nWe will be using this distance function as the basis of legibility score, but again we\nwill focus on generating plans whose legibility score is above some prespecified threshold,\nby requiring that the possible plans are no farther than d-distance away.\nDefinition 18. A BPS induced by plan p starting at s0 is maximally d-distant,\ndmax(BPS(p, s0)) if d =\nmax\np1,p2∈BPS(p,s0) D(p1, p2)\n.\nDefinition 19. A plan, πm, is an m-similar plan, if for a given value of d and distance\nfunction D, dmax(BPS(πm, I)) ≤d, |BPS(πm, I)| ≥m, where m ≥2 and every plan in\nBPS(πm, I) achieves the goal in GPL.\nIn order to generate a solution to the plan legibility problem, we use the algorithm\npresented in Algorithm 3. Again, the goal test and heuristic function are customized to\nensure that there are at least m similar plans to the true goal that are consistent with the\nobservation sequence and the maximum distance between these plans is at most d.\n4.3.1.1\nGoal test\nTo ensure the plans in BPS, induced by an m-similar plan, can achieve the goal in GPL,\nwe check whether at least m plans are reaching the goal or not and whether the maximum\ndistance between plans in BPS is at most d. Also in order to ensure termination of the\nalgorithm, there is a cost-bound given as input to the algorithm.\n\n60\nLegible Behavior\n4.3.1.2\nHeuristic function\nApart from minimizing the heuristic cost to the goal, the customized heuristic given below\nalso minimizes the d of dmax(BPS(p, s0)) induced by plan p starting at s0. This decreases\nthe maximum distance between the plan pairs in the BPS. This distance can be computed\nusing a plan distance measure.\nh(s) = hGA(s) + dmax(BPS(p, s0))\n(4.2)\n4.3.1.3\nPlan Legibility as Offline Predictability\nPlan legibility can be likened to the notion of offline predictability insofar that plan legi-\nble behaviors allows the robot to reduce the observer’s ambiguity over the possible plans\nexecuted given a goal, which is also a property exhibited by predictable behaviors. How-\never, predictable behaviors are also inherently easy to anticipate, either globally or locally.\nGlobally predictable behavior is a behavior that an observer would anticipate the robot\nto perform for a certain goal, versus locally predictable behavior is a behavior where given\na plan prefix, the rest of the suffix towards a certain goal can be easily anticipated by the\nobserver. This notion of predictability has been mostly explored in the motion planning\ncommunity. However, plan legibility does not always lead to predictable behaviors. This\nis because in plan legibility, the emphasis is on making the robot’s actions easy to guess\ngiven a corresponding observation sequence. However, the observation sequence in itself\nmight not be globally or even locally predictable to the observer.\n4.4\nBibliographic Remarks\nIn the motion planning and robotics community, legibility has been a well-studied topic\n[Dragan et al., 2013, Dragan and Srinivasa, 2013, Dragan et al., 2015, Knepper et al.,\n2017]. The original work on legibility used Bayesian formulation to generate legible robot\nmotions in an online setting. A legible motion is one that ensures that the actual robot goal\nhas the highest likelihood amongst the candidate goals. The legible planning discussed\nhere is formulated within the controlled observability planning framework (introduced in\nKulkarni et al. [2019b]), which generalizes the notion of legibility in terms of human’s\nobservability as well as in terms of the amount of information divulged (with at most j\ngoals, or plans that are at most d distance away). This framework uses the notion of\nsensor models to capture the human’s imperfect observations and to model the human’s\nbelief update [Geffner and Bonet, 2013, Bonet and Geffner, 2014, Keren et al., 2016]. The\ngoal and plan legible behaviors have been categorized into legible planning and predictable\nplanning in a recent survey on interpretable behaviors [Chakraborti et al., 2019a].\n\nChapter 5\nExplanation as Model Reconciliation\nIn this chapter, we revisit the explicability score and investigate an alternate strategy to\nimprove the explicability of the robot behavior, namely explanations. Rather than force\nthe robot to choose behaviors that are inherently explicable in the human model, here we\nwill let the robot choose a behavior optimal in its model and use communication to address\nthe central reason why the human is confused about the behavior in the first place, i.e.,\nthe model difference. That is, the robot will help the human understand why the behavior\nwas performed, by choosing to reveal parts of its model that were previously unknown to\nthe human. This would allow us to overcome one of the main shortcomings of the plan\ngeneration methods discussed in Chapter 2, namely that there might not exist a plan in\nthe robot model that has a high explicability score. In this scenario, the explicability score\nof the plan is only limited by the agent’s ability to effectively explain it. In this chapter, in\naddition to introducing the basic framework of explanation as model reconciliation under\na certain set of assumptions, we will also look at several types of model reconciliation\nexplanations, study some of their properties and consider some simple approximations. In\nthe coming chapters, we will further extend the idea of explanations and look at ways of\nrelaxing some of the assumptions made in this chapter.\n5.1\nModel-Reconciliation as Explanation\nAs mentioned in chapter 2, the explicability score of a plan π generated using the robot’s\nmodel MR is given by the expression\nE(π, MR\nh ) ∝max\nπ′∈ΠMR\nh −1 ∗D(π, π′, MR\nh )\nWhere D is the distance function, MR\nh the human’s estimate of what the robot’s model\nmay be, and π′ the plan closest to π in MR\nh (per the distance function D). This means,\neven if the robot chooses a plan in its own model (MR), a human observer may find it\ninexplicable if their estimate of the robot model differs from MR. Thus a way to address\nthe inexplicability of the plan would be to explain the difference between the robot’s own\nmodel and human’s estimation. This explanatory process is referred to as model recon-\nciliation.We will specifically focus on cases, where D is given by the cost difference and\nthe expected set corresponds to plans optimal in the human’s model. While this method\nassumes the human is a perfectly rational decision-maker (which isn’t necessarily true),\nstudies have shown this method to still result in useful explanations in many scenarios.\nOf course we could replace the distance function used and expected plan set with more\nrealistic choices and the basic algorithms presented here should stay mostly same.\nThe explanation process captured by Model Reconciliation begins with the following ques-\ntion:\nQ1: Why plan π?\n61\n\n62\nExplanation as Model Reconciliation\nAn explanation here needs to ensure that both the explainer and the explainee agree\nthat π is the best decision that could have been made in the given problem. In the setting\nwe are considering here that would mean providing model artifacts to the explainee so that\nπ is now also optimal in the updated mental model and thus have high explicability score\nunder our current set of assumptions (we will refer to this as the completeness property\nlater).\nDefinition 20. The model reconciliation problem (MRP) is represented as a tuple\n⟨π∗, ⟨MR, MR\nh ⟩⟩where C(π∗, MR) = C∗\nMR, i.e. the plan π∗is the optimal plan in the\nrobot model MR but may not be so in the human mental model MR\nh .\nNow we can define an explanation as\nDefinition 21. An explanation is a solution to the model reconciliation problem in the\nform of (1) a model update E such that the (2) robot optimal plan is (3) also optimal in\nthe updated mental model.\n•\nc\nMR\nh ←−MR\nh + E; and\n• C(π, MR) = C∗\nMR;\n• C(π, c\nMR\nh ) = C∗\nc\nMR\nh\n.\nNow the problem of finding the explanation becomes a search over the set of model\ninformation that can be provided to the user to get an updated user-model of desired\nproperty. This in term can be visualized as the search over the space of possible human\nmodels that can result from providing information consistent with the robot model. We\ncan facilitate such a search over the model space, by leveraging the model parameterization\nscheme specified in Chapter 2. Specifically such a scheme results in a state space of the\nform\nF = {init-has-f | ∀f ∈F R\nh ∪F R} ∪{goal-has-f | ∀f ∈F R\nh ∪F R}\n[\na∈AR\nh ∪AR\n{a-has-precondition-f, a-has-add-effect-f,\na-has-del-effect-f | ∀f ∈F R\nh ∪F R}\n∪{a-has-cost-C(a) | a ∈AR\nh } ∪{a-has-cost-C(a) | a ∈AR}.\nAgain the mapping function Γ : M 7→s can convert the given planning problem\nM = ⟨F, A, I, G, C⟩as a state s ⊆F, as follows –\n\n5.1 Model-Reconciliation as Explanation\n63\nτ(f) =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninit-has-f\nif f ∈I,\ngoal-has-f\nif f ∈G,\na-has-precondition-f\nif f ∈pre(a), a ∈A\na-has-add-effect-f\nif f ∈adds(a), a ∈A\na-has-del-effect-f\nif f ∈dels−(a), a ∈A\na-has-cost-f\nif f = C(a), a ∈A\nΓ(M) =\n\bτ(f) | ∀f ∈I ∪G∪\n[\na∈A\n{f′ | ∀f′ ∈{C(a)} ∪pre(a) ∪adds(a) ∪dels(a)}\n\t\nDefinition 22. The model-space search problem is specified as ⟨F, Λ, Γ(M1), Γ(M2)⟩\nwith a new action set Λ containing unit model change actions λ ∈Λ, λ : F →F such that\n|(s1\\s2)∪(s2\\s1)| = 1. The new transition or edit function is given by δM1,M2(s1, λ) = s2\nsuch that,condition 1: s2\\s1 ⊆Γ(M2) and condition 2: s1\\s2 ̸⊆Γ(M2) are satisfied.\nThis means that model change actions can only make a single change to a domain at a\ntime, and all these changes are consistent with the model of the planner. The solution to\na model-space search problem is given by a set of edit functions {λi} that transforms the\nmodel M1 to M2, i.e. δM1,M2(Γ(M1), {λi}) = Γ(M2). An explanation can thus be cast\nas a solution to the model-space search problem ⟨F, Λ, Γ(MR\nh ), Γ( c\nM)⟩with the transition\nfunction δMR\nh ,MR such that Condition (3) mentioned in Definition 21 is preserved.\n5.1.1\nThe Fetch Domain\nLet us look at an example domain to see how model reconciliation explanation could be\nused. Consider the Fetch robot 1 whose design requires it to tuck its arms and lower its\ntorso or crouch before moving. This is not obvious to a human navigating it and it may\nlead to an unbalanced base and toppling of the robot if the human deems such actions as\nunnecessary. The move action for the robot is described in PDDL in the following model\nsnippet –\n(:action move\n:parameters\n(?from ?to - location)\n:precondition\n(and (robot-at ?from) (hand-tucked) (crouched))\n:effect\n(and (robot-at ?to) (not (robot-at ?from))))\n(:action tuck\n:parameters\n()\n:precondition\n()\n:effect\n(and (hand-tucked) (crouched)))\n1https://fetchrobotics.com/fetch-mobile-manipulator/\n\n64\nExplanation as Model Reconciliation\nFig. 5.1: The Fetch in the crouched position with arm tucked (left), torso raised and arm outstretched\n(middle) and the rather tragic consequences of a mistaken action model (right showing a fractured head\nfrom an accident).\n(:action crouch\n:parameters\n()\n:precondition\n()\n:effect\n(and (crouched)))\nNotice that the tuck action also involves a lowering of torso so that the arm can rest\non the base once it is tucked in.2 Now, consider a planning problem where the the robot\nneeds to transport a block from one location to another, with the following initial and\ngoal states –\n(:init (block-at b1 loc1) (robot-at loc1) (hand-empty))\n(:goal (and (block-at b1 loc2)))\nAn optimal plan for the robot involves a tuck action followed by a move:\npick-up b1 -> tuck -> move loc1 loc2 -> put-down b1\nThe human, on the other hand, expects a much simpler model, as shown below. The\nmove action does not have the preconditions for tucking the arm and lowering the torso,\nwhile tuck does not automatically lower the torso either.\n(:action move\n:parameters\n(?from ?to - location)\n:precondition\n(and (robot-at ?from)\n:effect\n(and (robot-at ?to) (not (robot-at ?from))))\n2Fetch User Manual: https://docs.fetchrobotics.com/\n\n5.2 Explanation Generation\n65\n(:action tuck\n:parameters\n()\n:precondition\n()\n:effect\n(and (hand-tucked))\n(:action crouch\n:parameters\n()\n:precondition\n()\n:effect\n(and (crouched)))\nThe original plan is no longer optimal to the human who can envisage better alterna-\ntives (a shorter plan without the extra tuck action) in their mental model. An explanation\nhere is a model update that can mitigate this disagreement –\nExplanation >> MOVE_LOC1_LOC2-has-precondition-HAND-TUCKED\nThis correction brings the mental model closer to the robot’s ground truth and is\nnecessary and sufficient to make the robot’s plan optimal in the resultant domain so that\nthe human cannot envisage any better alternatives.\nThis is the essence of the model\nreconciliation process.\n5.2\nExplanation Generation\nBefore we go on to specific methods we could use to identify the required explanation\nfor a model reconciliation problem, let us consider the various desirable properties that\ncharacterize explanations in such settings.\nP1. Completeness - Explanations of a plan should be able to be compared and con-\ntrasted against other alternatives, so that no better solution exists. We can enforce\nthis property by requiring that in the updated human mental model the plan being\nexplained is now optimal.\n– An explanation is complete iff C(π, c\nMR\nh ) = C∗\nc\nMR\nh\n.\nP2. Conciseness - Explanation should be concise so that it is easily understandable to\nthe explainee. Larger an explanation is, the harder it is for the human to process\nthat information. Thus we can use the length of the explanation as a useful proxy\nor first approximation for the complexity of an explanation.\nP3. Monotonicity - This property ensures that remaining model differences cannot\nchange the completeness of an explanation, i.e. all aspects of the model that were\nrelevant to the plan have been reconciled. Thus, monotonicity of an explanation\nsubsumes completeness and requires more detail.\n\n66\nExplanation as Model Reconciliation\n– An explanation is monotonic iff\nC(π∗, ˆ\nM) = C∗\nˆ\nM ∀ˆ\nM : ((Γ( ˆ\nM) \\ Γ(MR\nh )) ∪(Γ(MR\nh ) \\ Γ( ˆ\nM))) ⊂(Γ( ˆ\nM) \\\nΓ(MR\nh )) ∪(Γ(MR\nh ) \\ Γ( ˆ\nM)).\nThus an updated model would satisfy monontonicity property if no additional in-\nformation about the robot model would render the plan being explained suboptimal\nor invalid.\nP4. Computability - While conciseness deals with how easy it is for the explainee\nto understand an explanation, computability measures the ease of computing the\nexplanation from the point of view of the planner.\nWe will now introduce different kinds of multi-model explanations that can partici-\npate in the model reconciliation process, provide examples, propose algorithms to compute\nthem, and compare and contrast their respective properties. We note that the require-\nments outlined above are in fact often at odds with each other - an explanation that is\nvery easy to compute may be very hard to comprehend.\n5.2.1\nExplanation types\nA simple way to explain would be to provide the model differences pertaining to only the\nactions that are present in the plan being explained –\nDefinition 23. A plan patch explanation (PPE) is given by –\nEPPE = (FMR(π) \\ FMR\nh (π)) ∪(FMR\nh (π) \\ FMR(π))\nwhere FM(π) gives the model parameters of M corresponding to all the actions in the\nplan π (i.e., FM(π) = S\n{C(a)}∪pre(a)∪adds(a)∪dels(a):a∈π τ(f)).\nClearly, such an explanation is easy to compute and concise by focusing only on plan\nbeing explained.\nHowever, it may also contain information that need not have been\nrevealed, while at the same time ignoring model differences elsewhere in MR\nh that could\nhave contributed to the plan being suboptimal in it. Thus, it is not complete. On the\nother hand, an easy way to compute a complete explanation would be to provide the entire\nmodel difference to the human –\nDefinition 24. A model patch explanation (MPE) is given by –\nEMPE = (Γ(MR) \\ Γ(MR\nh )) ∪(Γ(MR\nh ) \\ Γ(MR))\nThis is also easy to compute but can be quite large and is hence far from being concise.\nInstead, we can try to minimize the size (and hence increase the comprehensibility) of\nexplanations by searching in the space of models and thereby not exposing information\nthat is not relevant to the plan being explained while still trying to satisfy as many\nrequirements as we can.\n\n5.2 Explanation Generation\n67\nDefinition 25. A minimally complete explanation (MCE) is the shortest possible\nexplanation that is complete –\nEMCE = arg min\nE\n|(Γ( c\nM) \\ Γ(MR\nh )) ∪(Γ(MR\nh ) \\ Γ( c\nM))| with C(π, c\nMR\nh ) = C∗\nc\nMR\nh\nThe explanation provided before in the Fetch domain is indeed the smallest set of\ndomain changes that may be made to make the given plan optimal in the updated action\nmodel, and is thus an example of a minimally complete explanation.\nThe optimality criterion happens to be relevant to both the cases where the human\nexpectation is better, or worse, than the plan computed by the planner.\nThis might\nbe counter to intuition, since in the latter case one might expect that just establishing\nfeasibility of a better plan would be enough. Unfortunately, this is not the case, as can\nbe easily seen by creating counter-examples where other faulty parts of the human model\nmight disprove the optimality of the plan. Which brings us to the proposition,\nProposition 2. If C(π∗, MR\nh ) < minπ C(π, MR\nh ), then ensuring feasibility of the plan in\nthe modified planning problem, i.e. δc\nM(bI, π∗) |= bG, is a necessary but not a sufficient\ncondition for c\nM = ⟨bF, bA, bI, bG⟩to yield a valid explanation.\nNote that a minimally complete explanation can be rendered invalid given further\nupdates to the model. This can be easily demonstrated in our running example in the\nFetch domain.\nImagine that if, at some point, the human were to find out that the\naction (move) also has a precondition (crouched), then the previous robot plan will no\nlonger make sense to the human since now according to the human’s faulty model (being\nunaware that the tucking action also lowers the robot’s torso) the robot would need to do\nboth tuck and crouch actions before moving. Consider the following explanation in the\nFetch domain instead –\nExplanation >> TUCK-has-add-effect-CROUCHED\nExplanation >> MOVE_LOC2_LOC1-has-precondition-CROUCHED\nThis explanation does not reveal all model differences but at the same time ensures\nthat the plan remains optimal for this problem, irrespective of any other changes to the\nmodel, by accounting for all the relevant parts of the model that engendered the plan. It\nis also the smallest possible among all such explanations.\nDefinition 26. A minimally monotonic explanation (MME) is the shortest expla-\nnation that preserves both completeness and monotonicity –\nEMME = arg min\nE\n|(Γ( c\nM) \\ Γ(MR\nh )) ∪(Γ(MR\nh ) \\ Γ( c\nM))| with P1 & P3\n\n68\nExplanation as Model Reconciliation\nFig. 5.2: Illustration of the different kinds of explanations in the Fetch domain. Here the PPE and MPE\nare equivalent (which is the worst case for the former) and both longer than the MCE or the MME. Also,\nthe MCE is shorter than, and not a subset of the MME.\nAn MCE or MME solution may not be unique to an MRP problem. This can happen\nwhen there are multiple model differences supporting the same causal links in the plan -\na minimal explanation can get by (i.e. guarantee optimality in the modified model) by\nonly exposing one of them to the human.\nAlso it is easy to see that an MCE may not necessarily be part of an actual MME.\nThis is illustrated in Figure 5.2.\nProposition 3. An MCE may not be a subset of an MME, but it is always smaller or\nequal in size, i.e. |MCE| ≤|MME|.\n5.2.2\nDesiderata for Explanations as Discussed in Social Sciences\nBefore, we delve into the actual algorithms for generating such explanations, lets take a\nquick look at how model reconciliation explanation connects to the wider literature on\nexplanations. One of the key takeaways from many of the works dealing with this topic\nfrom social sciences has been the identification of three crucial properties for effective\nexplanations, namely, explanations need to be contrastive, social and selective.\nContrastive explanations are explanations that take the form of answers to questions\nof the type “Why P and not Q?” Where P is the fact being explained and Q the foil or the\nalternate option raised by the explainee (i.e. the one who is asking for the explanation).\nThus explanations for such question need to contrast the choice P over the raised foil.\nIn the case of planning problems, a very common form of contrastive questions involve\ncases where P , or the fact, is the plan itself being proposed by the system and the foil\nis some alternate plan expected by the user. In model reconciliation, the explanations\nbeing provided as part of model reconciliation can be viewed as answering a contrastive\nquery where the foil is implicitly specified, i.e., the foil could be any plan in the human\nmodel. Given the fact that these explanations establish optimality, the current plan must\nbe better than or as good as any alternate plan the user could have come up with.\n\n5.2 Explanation Generation\n69\nSocial explanations are those that explicitly take into account the beliefs of the user.\nModel-reconciliation framework discussed here is clearly social in the sense that expla-\nnations are tailored to what the human believes to be the robot model. Selectivity of\nexplanations deals with the fact that explanations need to only include relevant details\n(and in some sense minimal). As we will see below, the actual approaches to explana-\ntion generation focus on generating the minimal information needed to meet the required\nproperties.\n5.2.3\nModel Space Search for Minimal Explanations\nIn the following, we will see how the state space provided by Γ can be used in model-space\nsearch for computing MCEs and MMEs (computation of PPE and MPE follows directly\nfrom MR, MR\nh and π∗).\n5.2.3.1\nModel Space Search for MCEs\nTo compute MCEs, we employ A∗search in the space of models, as shown in Algorithm 4.\nThe algorithm is referred to as MEGA – Multi-model Explanation Generation Algorithm.\nGiven an MRP, we start off with the initial state Γ(MR\nh ) derived from the human’s\nexpectation of a given planning problem MR, and modify it incrementally until we arrive\nat a planning problem c\nM with C(π∗, c\nM) = C∗\nc\nM, i.e. the given plan is explained. Note\nthat the model changes are represented as a set, i.e.\nthere is no sequentiality in the\nsearch problem. Also, we assign equal importance to all model corrections. We can easily\ncapture differential importance of model updates by attaching costs to the edit actions\nλ - the algorithm remains unchanged. One could also employ a selection strategy for\nsuccessor nodes to speed up search (by overloading the way the priority queue is popped)\nby first processing model changes that are relevant to actions in π∗\nR and πH before the\nrest.\nProposition 4. The successor selection strategy outlined in Algorithm 5 will never remove\na valid solution from the search space.\nProof Sketch. Let E be the MCE for an MRP problem and let E′ be any intermediate\nexplanation found by our search such that E′ ⊂E, then the set E \\E′ must contain at least\none λ related to actions in the set {a | a ∈π∗\nR ∨a ∈π′} (where π′ is the optimal plan for\nthe model ˆ\nM where δMR\nh ,MR(Γ(MR\nh ), E′) = Γ( ˆ\nM). To see why this is true, consider an E′\nwhere |E′| = |E|−1. If the action in E \\E′ does not belong to either π∗\nR or π′ then it cannot\nimprove the cost of π∗\nR in comparison to π′ and hence E cannot be the MCE. Similarly we\ncan show that this relation will hold for any size of E′. We can leverage this knowledge\nabout E \\ E′ to create an admissible heuristic that considers only relevant changes.\n\n70\nExplanation as Model Reconciliation\nFig. 5.3: Illustration contrasting MCE search with MME search.\n5.2.3.2\nModel Space Search for MMEs\nAs per definition, beyond the model obtained from the minimally monotonic explanation,\nthere do not exist any models which are not explanations of the same MRP, while at the\nsame time making as few changes to the original problem as possible. It follows that this\nis the largest set of changes that can be done on MR and still find a model c\nM where\nC(π∗, c\nM) = C∗\nc\nM - this property can be used in the search for MMEs.\nProposition 5. EMME = arg maxE |(Γ( c\nM) \\ Γ(MR)) ∪(Γ(MR) \\ Γ( c\nM))| such that\n∀ˆ\nM ((Γ( ˆ\nM) \\ Γ(MR)) ∪(Γ(MR) \\ Γ( ˆ\nM))) ⊆((Γ( c\nM) \\ Γ(MR)) ∪(Γ(MR) \\ Γ( c\nM)))\nit is guaranteed to have C(π∗, ˆ\nM) = C∗\nˆ\nM.\nThis is similar to the model-space search for MCEs, but this time starting from the\nrobot’s model MR instead. The goal here is to find the largest set of model changes for\nwhich the explicability criterion becomes invalid for the first time (due to either subopti-\nmality or inexecutability). This requires a search over the entire model space (Algorithm\n6). We can leverage Proposition 3 to reduce our search space. Starting from MR, given\na set of model changes E where δMR,MH(Γ(MR), E) = Γ( c\nM) and C(π∗, c\nM) > C∗\nc\nM, no\nsuperset of E can lead to an MME solution. In Algorithm 6, we keep track of such un-\nhelpful model changes in the list h_list. The variable EMME keeps track of the current\nbest list of model changes. Whenever the search finds a new set of model changes where\nπ∗is optimal and is larger than EMME, EMME is updated with E. The resulting MME is\nall the possible model changes that did not appear in EMME.\nFigure 5.3 contrasts MCE search with MME search. MCE search starts from MR\nh , up-\ndates c\nM towards MR and returns the first node (indicated in orange) where C(π∗, c\nM) =\nC∗\nc\nM. MME search starts from MR and moves towards MR\nh . It finds the longest path\n(indicated in blue) where C(π∗, c\nM) = C∗\nc\nM for all c\nM in the path. The MME (green) is\nthe rest of the path towards MR\nh .\n5.3\nApproximate Explanations\nIn this section, we will discuss how some of the methods discussed in this section can be\nsimplified to either 1) generate simpler explanation (in terms of the explanation size or\n\n5.3 Approximate Explanations\n71\nAlgorithm 4 Search for Minimally Complete Explanations\n1: MCE-Search\n2: Input: MRP ⟨π∗, ⟨MR, MR\nh ⟩⟩\n3: Output: Explanation EMCE\n4: Procedure:\n5: fringe ←Priority_Queue()\n6: c_list ←{} {Closed list}\n7: π∗\nR ←π∗{Optimal plan being explained}\n8: πH ←π such that C(π, MR\nh ) = C∗\nMR\nh {Plan expected by human}\n9: fringe.push(⟨MR\nh , {}⟩, priority = 0)\n10: while True do\n11:\n⟨c\nM, E⟩, c ←fringe.pop( c\nM)\n12:\nif C(π∗\nR, c\nM) = C∗\nc\nM then\n13:\nreturn E {Returns E if π∗\nR optimal in c\nM}\n14:\nelse\n15:\nc_list ←c_list ∪c\nM\n16:\nfor f ∈Γ( c\nM) \\ Γ(MR) {Models that satisfy Condition 1} do\n17:\nλ ←⟨1, { c\nM}, {}, {f}⟩{Removes f from c\nM}\n18:\nif δMR\nh ,MR(Γ( c\nM), λ) ̸∈c_list then\n19:\nfringe.push(⟨δMR\nh ,MR(Γ( c\nM), λ), E ∪λ⟩, c + 1)\n20:\nfor f ∈Γ(MR) \\ Γ( c\nM) {Models that satisfy Condition 2} do\n21:\nλ ←⟨1, { c\nM}, {f}, {}⟩{Adds f to c\nM}\n22:\nif δMR\nh ,MR(Γ( c\nM), λ) ̸∈c_list then\n23:\nfringe.push(⟨δMR\nh ,MR(Γ( c\nM), λ), E ∪λ⟩, c + 1)\nAlgorithm 5 Selection strategy for identifying model updates relevant to the current\nplan\nProcedure Priority_Queue.pop\nˆ\nM\ncandidates ←{⟨⟨c\nM, E⟩, c∗⟩| c∗= arg minc⟨⟨c\nM, E⟩, c⟩}\npruned_list ←{}\nπH ←π such that C(π, ˆ\nM) = C∗\nˆ\nM\nfor ⟨⟨c\nM, E⟩, c⟩∈candidates do\nif ∃a ∈π∗\nR∪πH such that τ −1((Γ( c\nM) \\ Γ( ˆ\nM))∪(Γ( ˆ\nM)\\Γ( c\nM))) ∈{C(a)}∪pre(a)∪\nadds(a) ∪dels(a) then\npruned_list ←pruned_list ∪⟨⟨c\nM, E⟩, c⟩\nif pruned_list = ϕ then\n⟨c\nM, E⟩, c ∼Unif(candidate_list)\nelse\n⟨c\nM, E⟩, c ∼Unif(pruned_list)\n\n72\nExplanation as Model Reconciliation\nAlgorithm 6 Search for Minimally Monotonic Explanations\n1: MME-Search\n2: Input: MRP ⟨π∗, ⟨MR, MR\nh ⟩⟩\n3: Output: Explanation EMME\n4: Procedure:\n5: EMME ←{}\n6: fringe ←Priority_Queue()\n7: c_list ←{} {Closed list}\n8: h_list ←{} {List of incorrect model changes}\n9: fringe.push(⟨MR, {}⟩, priority = 0)\n10: while fringe is not empty do\n11:\n⟨c\nM, E⟩, c ←fringe.pop( c\nM)\n12:\nif C(π∗, c\nM) > C∗\nc\nM then\n13:\nh_list ←h_list ∪((Γ( c\nM) \\ Γ(MR)) ∪(Γ(MR) \\ Γ( c\nM))) {Updating h_list }\n14:\nelse\n15:\nc_list ←c_list ∪c\nM\n16:\nfor f ∈Γ( c\nM) \\ Γ(MR\nh ) {Models that satisfy Condition 1} do\n17:\nλ ←⟨1, { c\nM}, {}, {f}⟩{Removes f from c\nM}\n18:\nif δMR,MR\nh (Γ( c\nM), λ) ̸∈c_list\nand ∄S s.t. ((Γ( c\nM) \\ Γ(MR)) ∪(Γ(MR) \\ Γ( c\nM))) ⊇S ∈h_list then\n19:\nfringe.push(⟨δMR,MR\nh (Γ( c\nM), λ), E ∪λ⟩, c + 1)\n20:\nEMME ←max|·|{EMME, E}\n21:\nfor f ∈Γ(MR\nh ) \\ Γ( c\nM) {Models that satisfy Condition 2} do\n22:\nλ ←⟨1, { c\nM}, {f}, {}⟩{Adds f from c\nM}\n23:\nif δMR,MR\nh (Γ( c\nM), λ) ̸∈c_list\nand ∄S s.t. ((Γ( c\nM) \\ Γ(MR)) ∪(Γ(MR) \\ Γ( c\nM))) ⊇S ∈h_list then\n24:\nfringe.push(⟨δMR,MR\nh (Γ( c\nM), λ), E ∪λ⟩, c + 1)\n25:\nEMME ←max|·|{EMME, E}\n26: EMME ←((Γ( c\nM) \\ Γ(MR)) ∪(Γ(MR) \\ Γ( c\nM))) \\ EMME\n27: return EMME\n\n5.3 Approximate Explanations\n73\nmore directed to a specific user query) and 2) reduce the computational overhead of the\nsearch.\n5.3.1\nExplicit Contrastive Explanations\nAs mentioned earlier, the explanations generated by model reconciliation can be viewed\nas an answer to an implicit contrastive query.\nAlthough this may lead to user being\nprovided more information than required. This may be particularly unnecessary if the\nuser’s original expected set of plans is much smaller than the set of all optimal plans. In\nthese cases, we could let the user directly specify their expected set of plans in the form of\nexplicit foils. Thus explanation can focus on establishing how the current plan compares\nagainst their original expected set of plans. So this allows the system to not only provide\nless information, the system can now potentially also provide additional information that\ncan allow the user to understand why the current plan is better than the plans they were\nexpecting.However in this case, the explanation could be a multi-step process where they\nraise additional queries (in the form of more foils) after each explanation.\nNow if ˆΠ is the set of alternate foils raised by the human, the objective of a minimal\nexplanation generation method would be\nEcontr = arg min\nE\n|(Γ( c\nM)\\Γ(MR\nh ))∪(Γ(MR\nh )\\Γ( c\nM))| with C(π∗, c\nMR\nh ) ≤C(π′, c\nMR\nh ) ∀π′ ∈ˆΠ\nNow we can modify the MCE search to use this new objective to identify the required\nexplanation. Now the foil set ˆΠ may be either explicitly specified or may be implicitly\nspecified in terms of constraints satisfied by plans in the set.\n5.3.2\nApproximate MCE\nBoth MCEs and MMEs may be hard to compute - in the worst case it involves a search\nover the entire space of model differences. Thus the biggest bottleneck here is the check\nfor optimality of a plan given a new model. A check for necessary or sufficient conditions\nfor optimality, without actually computing optimal plans can be used as a way to further\nprune the search tree.\nIn the following section, we investigate an approximation to an MCE by employing a\nfew simple proxies to the optimality test. By doing this we lose the completeness guarantee\nbut improve computability. Specifically, we replace the equality test in line 12 of Algorithm\n4 by the following rules –\nC(1) δc\nM(bI, π∗\nR) |= bG; and\nC(2) C(π∗\nR, c\nM) < C(π∗\nR, MR\nh ) or δc\nM(bI, π∗\nH) ̸|= bG; and\nC(3) Each action contributes at least one causal link to π∗\nR.\n\n74\nExplanation as Model Reconciliation\nFig. 5.4: Interface for a user study where participants assumed the role of the external commander and\nevaluated plans provided by the internal robot. They could request for plans and explanations to those\nplans (if not satisfied) and rate them as optimal or suboptimal or (if unsatisfied) can chose to pass.\nC(1) ensures that the plan π∗\nR originally computed is actually valid in the new model.\nC(2) requires that this plan has either become better in the new model or at least that\nthe human’s expected plan π∗\nH has been disproved. Finally, C(3), ensures that for each\naction ai ∈π∗\nR there exists an effect p that satisfies the precondition of at least one\naction ak (where ai ≺ak) and there exists no action aj (where ai ≺aj ≺ak) such that\np ∈dels(aj). Such explanations are only able to preserve local properties of a plan and\nhence incomplete.\nProposition 6. C(3) is a necessary condition for optimality of π∗in c\nM.\nProof Sketch. Assume that for an optimal plan π∗\nR, there exists an action ai where crite-\nrion (3) is not met. Now we can rewrite π∗\nR as π′\nR = ⟨a0, a1, . . . , ai−1, ai, ai+1, . . . , an, an+1⟩,\nwhere pre(a0) = ϕ and adds(a0) = {I} and pre(an+1) = {G} and adds(an+1) = dels(an+1) =\nϕ.\nIt is easy to see that δc\nM(ϕ, π′\nR) |= G.\nNow let us consider a cheaper plan\nˆ\nπ′\nR =\n⟨a0, a1, . . . , ai−1, ai+1, . . . , an, an+1⟩. Since ai does not contribute any causal links to the\noriginal plan π∗\nR, we will also have δc\nM(ϕ, ˆπ′\nR) |= G. This contradicts our original assump-\ntion of π∗\nR being optimal, hence proved.\n5.4\nUser Studies\nApart from the underlying formal theory and intuition for the methods discussed here,\nuser studies have also been performed to explicitly test the effectiveness of these type of ex-\nplanations. In one study, the participants were shown a simulated robot navigating a floor\nin a building, and the floor map was made available to the user. They were then asked to\n\n5.5 Other Explanatory Methods\n75\nFig. 5.5: Interface for Study-1: Participants assumed the role of the internal agent and explained their\nplans to a teammate with a possibly different map of the world.\nevaluate whether the robot plans were valid and optimal. The participants had the option\nto query for explanations that revealed details about the floor map that were previously\nunknown to the user. Figure 5.4 presents a screenshot of this interface along with a sample\nplan. The user study also had a reward structure that disincentivized participants from\nproviding incorrect answers (in regards to whether the plans were valid and/or optimal).\nThe study showed the majority of participants were correctly able to identify the validity\nand optimality of the plan based on the explanations. A subjective survey at the end of\nthe study showed that the majority of participants found the explanations were easy to\nunderstand and useful.\nAdditional studies were also performed that placed participants in the role of the\nexplainer. In this study, they were asked to explain why a given plan was optimal. They\nwere shown the original map and the map the explainee would see.\nAfter analyzing\nthe explanations given by the participants, it was found that most of them perfectly\nmatched up with many of the explanation types studied in this chapter. Particularly,\nwhen the participants were asked to minimize the information to be provided the majority\nof explanation provided were MCEs. This points to the fact that in many cases, people\ndo naturally calculate explanations of the type discussed in this chapter.\n5.5\nOther Explanatory Methods\nIn terms of explanations, this chapter focused on one specific type of explanation, namely\nexplanation as model reconciliation. But this is not the only type of explanation that has\nbeen studied in the literature. In particular, we will look at two other types of explanations\nthat are popular within the explainable planning community (generally referred to as\nXAIP), and in this section, we will look at how these explanation types could be viewed\nin the context of human-aware planning.\nExplanations to Address Inferential Asymmetry.\nThe first type of explanation is\none designed to address explanatory queries rising from differences in inferential capabil-\nities of the human and the robot. Going back to the setting where the human is a pure\n\n76\nExplanation as Model Reconciliation\nobserver, even if the human had access to the exact model, there is no guarantee that\nthey would be able to correctly evaluate the validity or optimality of the plan being fol-\nlowed by the robot given potential inferential limitations of the human. So in addition to\nreconciling any model differences, the system may need to help address any confusion on\nthe human’s end stemming from such asymmetry in inferential capabilities. Some general\nstrategies for addressing such considerations include,\n1. Allowing users to ask questions about plans: In particularly allowing for contrastive\nqueries, in which the human raises specific alternatives they were expecting Sreed-\nharan et al. [2018b]. As mentioned earlier, such questions help expose the human’s\nexpectations about the robot and thus allowing the robot to provide a more directed\nexplanation. Having access to the human’s expected behavior, also means the robot\ncould provide additional information that helps contrast the given decision against\nthe alternatives the human had in mind (say by tracing the execution of the current\nplan and the alternative).\n2. Simplifying the problem: The next strategy one could employ may be to simplify\nthe problem being explained. One of the obvious strategy we can employ here is\nto use state abstractions, we will look at this strategy in more detail in Chapter 6\nthough in a slightly different context. But this is one of many problem simplification\nstrategies we could employ. Other strategies include the use of local approximation\n(also discussed in Chapter 8 (Section 8.2.1)), decomposing the problem into smaller\nsubproblems (for example, focusing on the achievement of a subgoal instead of the\ntrue goal as done in the case of Sreedharan et al. [2019b]) and even simplifying the\nclass of the reasoning/planning problem. For example, in many cases it may suffice\nto focus on a determinization or a qualitative non-deterministic representation of a\nproblem to address many of the explanatory challenges related to stochastic planning\nproblems.\n3. Providing explanatory witness: In many cases while the explanation may be in\nregards to a space of possible solutions, it may be helpful to provide the human\nwith specific example to how the current solution may satisfy a property that is\nin question. For example, if the human were to raise a partial specification of an\nalternative, say she asks why the robot didn’t choose a particular action at a state,\nthen the robot could point to a specific example plan that includes an action and\npoint out that this alternate plan is costlier than the current plan (this is the strategy\nfollowed by Krarup et al. [2019]). This doesn’t necessarily explain why all plans that\ninclude that action may be worse off than the current plan, but providing an example\ncould help the user better understand the plan in question.\nNote that all the above strategies are generally based on computational intuition on how\nto reduce computational overhead of a problem (many of these methods have their roots\non heuristics for planning). On the other hand, a systematic framework to address such\nasymmetry would require us to correctly model and capture the human inferential capa-\nbilities, so that the agent can predict the inferential overhead placed on the human by a\nspecific explanation approach. Unfortunately, we are still far from achieving such accurate\nmodeling of the computational capabilities of a bounded rational agent let alone a human.\n\n5.6 Bibliographic Remarks\n77\nPlan and Policy Summary\nAnother category of work that we haven’t had a chance\nto discuss in the book are the ones related to plan and policy summaries. Particularly\nin cases, where the plan may need to be approved by the human before it is executed.\nIn such cases, the robot would need to communicate its plans to the human before it\ncan be executed. Several works have looked at the problem of effectively summarizing a\ngenerated course of action. Many of these works focus on stochastic planning settings,\nwhere the solution concept takes the form of policies which could in general be harder to\nunderstand than traditional sequential plans. Some general strategies used in such policy\nsummarization method include, the use of abstractions (for example Topin and Veloso\n[2019] use state abstraction, while Sreedharan et al. [2020d] use temporal abstraction)\nand selecting some representative state-action pairs, so the human can reconstruct the\nrest of the policy (for example Amir and Amir [2018], Lage et al. [2019]). Generally most\nworks in this latter direction assume that the human correctly understands the task at\nhand and thus the reconstruction can be done correctly by the human without the aid\nof any model reconciliation. One may also need to address the problem of vocabulary\nmismatch in the context of policy communication, wherein the policy may need to be\ntranslated first into terms the user can understand (as done by the work Hayes and Shah\n[2017]).\nOther Popular Forms of Explanation Categorization.\nAs final note on other forms\nof explanation studied in XAIP, a popular way to categorize explanatory information with\nin the literature is to organize them based on the type of explanatory questions they may\nbe addressing. Fox et al. [2017] provides a very comprehensive list of explanatory questions\nthat could be relevant to planning. This work not only includes question related to specific\nplans that are generated by the agent, but also questions related to the planning process\nas a whole. For example, the paper considers questions like why the agent decided to\nreplan at a particular point during the plan execution.\n5.6\nBibliographic Remarks\nThe idea of explanation as model reconciliation was first introduced in the paper Chakraborti\net al. [2017a]. The paper specifically looked at the application of the methods for STRIPS\nstyle models. The user studies for evaluating the explanations were presented in the paper\nChakraborti et al. [2019b]. The paper looked at three types of model-reconciliation ex-\nplanations, namely MCE, MPE and PPE. In the case of PPE, a slightly modified version\nof PPE was considered that also takes into account one of the possible optimal plans in\nthe human model (thereby ensuring the explanation is still contrastive). Sreedharan et al.\n[2021a] also presents a more unified view of the various earlier works done in the area of\nmodel-reconciliation explanation. While most works in model reconciliation explanations\nhave focused on deterministic STRIPS-style planning models, researchers have also started\nlooking at other planning formalisms. For example, Sreedharan et al. [2019a] looked at\nmodel reconciliation in the context of MDPs and Vasileiou et al. [2021] looked at model\nreconciliation in the context of logical programs.\nThere are even parallel works on explanation from fields like machine learning, that\ncould be understood as a special case of model reconciliation. A particularly significant\n\n78\nExplanation as Model Reconciliation\nexample being LIME [Ribeiro et al., 2016]. In this work, the authors propose a model-\nagnostic method that take a model, and the specific instance to be explained and generates\nan interpretable linear model that locally approximates the dynamics of the current in-\nstance. In this case the final explanation takes the form of details about the linear model\nand the human’s model is taken to be empty. One important factor to note here is the\nmodel being transferred to the user is not the original model but rather a local approxima-\ntion of it. Moreover, the model is post-hoc expressed in terms of human understandable\nfeatures. These strategies are also applicable in the context of model reconciliation ex-\nplanation in the sequential decision making settings, and we will discuss methods that\nemploy such methods in chapter 8. Some examples of explanation works that use abstrac-\ntion of the model and/or plan include Zahavy et al. [2016], Topin and Veloso [2019], and\nSreedharan et al. [2020d]. Another method popular in explaining the utility of a specific\naction in plan involve providing causal chain contributed by that action to the overall plan\n[Veloso, 1992, Seegebarth et al., 2012, Bercher et al., 2014]. Another technique popular in\nexplaining multi-objective planning problem is to provide conflicts between the different\nobjectives Eifler et al. [2020]. For more works done in this direction, readers can refer to\nthe survey in Chakraborti et al. [2020]. Miller [2019] presents a concise introduction into\nvarious works from disciplines like social sciences, psychology and philosophy that have\nlooked at explanations.\n\nChapter 6\nAcquiring Mental Models for\nExplanations\nThe previous chapter sketches out some of the central ideas behind generating explanation\nas model reconciliation, but it does so while making some strong assumptions. Particularly,\nthe setting assumes that the human’s model of the robot is known exactly upfront. In this\nchapter, we will look at how we can relax this assumption and see how we can perform\nmodel reconciliation in scenarios where the robot has progressively less information about\nthe human mental model. We will start by investigating how the robot can perform model\nreconciliation with incomplete model information. Next we will look at cases where the\nrobot doesn’t have a human mental model but can collect feedback from users.1\nWe\nwill see how we can use such feedback to learn simple labeling models that suffice to\ngenerate explanations. We will also look at generating model reconciliation explanations\nby assuming the human has a simpler mental model, specifically one that is an abstraction\nof the original model and also see how this method can help reduce inferential burden on\nthe human. Throughout this chapter, we will focus on generating MCE though most of\nthe methods discussed here could also be extended to MME and contrastive versions of\nthe explanations.\n6.1\nThe Urban Search and Reconnaissance Domain\nWe will concretize the discussion in this section in a typical Urban Search and Reconnais-\nsance (USAR) domain where a remote robot is put into disaster response operation often\ncontrolled partly or fully by an external human commander, as shown in Figure 6.1. The\nrobot’s job is to scout areas that may be otherwise harmful to humans and report on its\nsurroundings as instructed by the external supervisor. The scenario can also have other\ninternal agents (humans or robots) with whom the robot needs to coordinate.\nHere, even though all agents start off with the same model – i.e. the blueprint of\nthe building – their models diverge as the internal agent interacts with the scene. Due\nto the disaster, new paths may have opened up due to collapsed walls or old paths may\nno longer be available due to rubble. This means that plans that are valid and optimal\nin the robot’s (current) model may not make sense to the external commander. In the\nscenario in Figure 6.2, the robot is tasked to go from its current location marked blue to\nconduct reconnaissance in the location marked orange. The green path is most optimal\nin its current model but this is blocked in the external’s mental model while the expected\nplan in the mental model is no longer possible due to rubble. Without removing rubble in\nthe blocked paths, the robot can instead communicate that the path at the bottom is no\n1Note that the model we need for explicability and explanation, MR\nh can’t just be learned in general\nfrom past behavior traces of the humans themselves, since those give us information only about MH and\nnot MR\nh . While there may be self-centered humans who consider the AI robot’s model to be same as\ntheirs, more generally, these models will be different.\n79\n\n80\nAcquiring Mental Models for Explanations\nFig. 6.1: A typical USAR domain with an internal robot and an external commander.\nFig. 6.2: Model differences in the USAR domain.\nlonger blocked. This explanation preserves the validity and optimality of its plan in the\nupdated model (even though further differences exist).\n6.2\nModel Uncertainty\nWe will start by considering cases where the human’s mental model may not be known\nexactly upfront. There may be parts of the human model the robot may know exactly,\nwhile it may be uncertain about others. We will leverage the annotated model represen-\ntation to capture such incomplete models. Under this representation, in addition to the\nstandard preconditions and effects associated with actions, the model includes possible\npreconditions and effects which may or may not be realized in practice.\nDefinition 27. An annotated model is the tuple M = ⟨F, A, I, G, C⟩where F is a finite\nset of fluents that define a state s ⊆F, and A is a finite set of annotated actions, I =\n⟨I0, I+⟩, G = ⟨G0, G+⟩; the annotated initial and goal states ( such that I0, G0, I+, G+ ⊆\nF) and C the cost function. Action a ∈A is a tuple ⟨pre(a), g\npre(a), adds(a), dels(a), g\nadds(a), g\ndels(a)⟩\n\n6.2 Model Uncertainty\n81\nwhere in addition to pre(a), adds(a), dels(a) ⊆F, i.e., the known preconditions and ad-\nd/delete effects each action also contains possible preconditions g\npre(a) ⊆F contain-\ning propositions that it might need as preconditions, and possible add (delete) effects\ng\nadds(a), g\ndels(a), ⊆F) containing propositions that it might add (delete, respectively) after\nexecution. I0, G0 (and I+, G+) are the known (and possible) parts of the initial and goal\nstates.\nEach possible condition f ∈g\npre(a) ∪]\nadds(a) ∪g\ndels(a) has an associated probability\np(f) denoting how likely it is to be a known condition in the ground truth model – i.e. p(f)\nmeasures the confidence with which that condition has been learned. The sets of known\nand possible conditions of a model M are denoted by Sk(M) and Sp(M) respectively.\nAn instantiation of an annotated model M is a classical planning model where a subset\nof the possible conditions have been realized, and is thus given by the tuple inst(M) =\n⟨F, A, I, G⟩, initial and goal states I = I0∪χ; χ ⊆I+ and G = G0∪χ; χ ⊆G+ respectively,\nand action A ∋a = ⟨pre(a) ←pre(a) ∪χ; χ ⊆g\npre(a), adds(a) ←adds(a) ∪χ; χ ⊆\n]\nadds(a), dels(a) ←dels(a) ∪χ; χ ⊆g\ndels(a)⟩. Clearly, given an annotated model with k\npossible conditions, there may be 2k such instantiations, which forms its completion set.\nDefinition 28. Likelihood Pℓof instantiation inst(M) of an annotated model M is:\nPℓ(inst(M)) =\nY\nf∈Sp(M)∧Sk(inst(M))\np(f)\n×\nY\nf∈Sp(M)\\Sk(inst(M))\n(1 −p(f))\nAs discussed before, such models turn out to be especially useful for the representation\nof human (mental) models learned from observations, where uncertainty after the learning\nprocess can be represented in terms of model annotations. Let MR\nh be the culmination of\na model learning process and {MR\nhi}i be the completion set of MR\nh . One of these models\nis the actual ground truth (i.e.\nthe human’s real mental model).\nWe refer to this as\ng(MR\nh ). We will explore now how this representation will allow us to compute conformant\nexplanations that can explain with respect to all possible mental models and conditional\nexplanations that engage the explainee in dialogue to minimize the size of the completion\nset to compute shorter explanations.\n6.2.0.1\nConformant Explanations\nIn this situation, the robot can try to compute MCEs for each possible configuration.\nHowever, this can result in situations where the explanations computed for individual\nmodels independently are not consistent across all possible target domains.\nThus, in\nthe case of model uncertainty, such an approach cannot guarantee that the resulting\nexplanation will be acceptable.\nInstead, the objective is to find an explanation such that ∀i π∗\nc\nMR\nhi\n≡π∗\nMR (as shown\nin Figure 6.3). This is a single set of model updates that makes the given plan optimal\n(and hence explained) in all the updated models. At first glance, it appears that such an\napproach, even though desirable, might turn out to be prohibitively expensive especially\nsince solving for a single MCE involves search in the model space where each search node\n\n82\nAcquiring Mental Models for Explanations\nFig. 6.3: Model reconciliation in the presence of model uncertainty or multiple explainees.\nis an optimal planning problem. However, it turns out that the same search strategy can\nbe employed here as well by representing the human mental model as an annotated model.\nThe optimality condition for MCE now becomes –\nC(π, g(MR\nh )) = C∗\ng(MR\nh )\nWe define robustness of an explanation for an incomplete mental model as the proba-\nbility mass of models where it is a valid explanation.\nDefinition 29. Robustness of an explanation E is given by –\nR(E) =\nX\ninst(c\nMR\nh ) s.t. C(π,inst(c\nMR\nh ))=C∗\ninst( b\nMR\nh )\nPℓ(inst( c\nMR\nh ))\nDefinition 30. A conformant explanation is such that R(E) = 1.\nWhich is equivalent to saying that conformant explanation ensures that the given plan\nis explained in all the models in the completion set of the human model. Let’s look at an\nexample. Consider again the USAR domain (Figure 6.4), the robot is now at P1 (blue)\nand needs to collect data from P5. While the commander understands the goal, she is\nunder the false impression that the paths from P1 to P9 and P4 to P5 are unusable (red\nquestion marks). She is also unaware of the robot’s inability to use its hands. On the\n\n6.2 Model Uncertainty\n83\nFig. 6.4: Back to our USAR scenario: the robot plan is marked in blue and uncertain parts of the human\nmodel is marked with red question marks.\nother hand, while the robot does not have a complete picture of her mental model, it\nunderstands that any differences between the models are related to (1) the path from P1\nto P9; (2) the path from P4 to P5; (3) its ability to use its hands; and (4) whether it\nneeds its arm to clear rubble. Thus, from the robot’s perspective, the mental model can\nbe one of sixteen possible models (one of which is the actual one). Here, a conformant\nexplanation for the optimal robot plan (blue) is as follows –\nExplanation >> remove-known-INIT-has-add-effect-hand_capable\nExplanation >> add-annot-clear_passage-has-precondition-hand_capable\nExplanation >> remove-annot-INIT-has-add-effect-clear_path P1 P9\n6.2.0.2\nModel-Space Search for Conformant Explanations\nAs we discussed before, we cannot launch an MCE-search for each possible mental model\nseparately, both for issues of complexity and consistency of the solutions. However, in\nthe following discussion, we will see how we can reuse the model space search from the\nprevious section with a compilation trick.\nWe begin by defining two models – the most relaxed model possible Mmax and the\nleast relaxed one Mmin. The former is the model where all the possible add effects and\nnone of the possible preconditions and deletes hold, the state has all the possible conditions\nset to true, and the goal is the smallest one possible; while in the latter all the possible\npreconditions and deletes and none of the possible adds are realized and with the minimal\nstart state and the maximal goal. This means that, if a plan is executable in Mmin it\nwill be executable in all the possible models. Also, if this plan is optimal in Mmax, then\nit must be optimal throughout the set. Of course, such a plan may not exist, and we are\nnot trying to find one either. Instead, we are trying to find a set of model updates which\nwhen applied to the annotated model, produce a new set of models where a given plan is\noptimal. In providing these model updates, we are in effect reducing the set of possible\nmodels to a smaller set. The new set need not be a subset of the original set of models\nbut will be equal or smaller in size to the original set. For any given annotated model,\nsuch an explanation always exists (entire model difference in the worst case), and the goal\nhere becomes to find the smallest one. MR\nh thus affords the following two models –\n\n84\nAcquiring Mental Models for Explanations\nMmax = ⟨F, A, I, G⟩\n- initial state I ←I0 ∪I+; given I\n- goal state G ←G0; given G\n- ∀a ∈A\n- pre(a) ←pre(a); a ∈A\n- adds(a) ←adds(a) ∪]\nadds(a); a ∈A\n- dels(a) ←dels(a); a ∈A\nMmin = ⟨F, A, I, G⟩\n- initial state I ←I0; given I\n- goal state G ←G0 ∪G+; given G\n- ∀a ∈A\n- pre(a) ←pre(a) ∪f\npre(a); a ∈A\n- adds(a) ←adds(a); a ∈A\n- dels ←dels ∪g\ndels; a ∈A\nAs explained before, Mmax is a model where all the add effects hold and it is easiest\nto achieve the goal, and similarly Mmin is the model where it is the hardest to achieve\nthe goal. These definitions might end up creating inconsistencies (e.g. in an annotated\nBlocksWorld domain, the unstack action may have add effects to make the block both\nholding and ontable at the same time), but the model reconciliation process will take\ncare of these.\nProposition 7. For a given MRP Ψ = ⟨π, ⟨MR, MR\nh ⟩⟩, if the plan π is optimal in Mmax\nand executable in Mmin, then the plan is optimal for all i.\nThis now becomes the new criterion to satisfy in the course of search for an MCE\nfor a set of models.\nWe again reuse the state representation in Chapter 5 (generated\nby Γ as described in Section 5.1). We start the conformant search by first creating the\ncorresponding Mmax and Mmin model for the given annotated model MR\nh . While the goal\ntest for the original MCE only included an optimality test, here we need to both check\nthe optimality of the plan in Mmax and verify the correctness of the plan in Mmin. As\nstated in Proposition 7, the plan is only optimal in the entire set of possible models if it\nsatisfies both tests. Since the correctness of a given plan can be verified in polynomial\ntime with respect to the plan size, this is a relatively easy test to perform.\nThe other important point of difference between the algorithm mentioned above and\nthe original MCE is how the applicable model updates are calculated. Here we consider\nthe superset of model differences between the robot model and Mmin and the differences\nbetween the robot model and Mmax. This could potentially mean that the search might\nend up applying a model update that is already satisfied in one of the models but not in\n\n6.2 Model Uncertainty\n85\nthe other. Since all the model update actions are formulated as set operations, the original\nMRP formulation can handle this without any further changes. The models obtained by\napplying the model update to Mmin and Mmax are then pushed to the open queue.\nProposition 8. Mmax and Mmin only need to be computed once – i.e. with a model\nupdate E to M: Mmax ←Mmax + E and Mmin ←Mmin + E.\nThese models form the new Mmin and Mmax models for the set of models obtained\nby applying the current set of model updates to the original annotated model.\nThis\nproposition ensures that we no longer have to keep track of the current list of models or\nrecalculate Mmin and Mmax for the new set.\n6.2.0.3\nConditional Explanations\nConformant explanations can contain superfluous information – i.e. asking the human\nto remove non-existent conditions or add existing ones.\nIn the previous example, the\nsecond explanation (regarding the need of the hand to clear rubble) was already known\nto the human and was thus superfluous information. Such redundant information can be\nannoying and may end up reducing the human’s trust in the robot. This can be avoided\nby –\n- Increasing the cost of model updates involving uncertain conditions relative to those\ninvolving known preconditions or effects. This ensures that the search prefers ex-\nplanations that contain known conditions. By definition, such explanations will not\nhave superfluous information.\n- However, sometimes such explanations may not exist. Instead, we can convert con-\nformant explanations into conditional ones. This can be achieved by turning each\nmodel update for an annotated condition into a question and only provide an ex-\nplanation if the human’s response warrants it – e.g. instead of asking the human\nto update the precondition of clear_passage, the robot can first ask if the human\nthinks that action has a precondition hand_usable. Thus, one way of removing\nsuperfluous explanations is to reduce the size of the completion set by gathering\ninformation from the human.\nBy using information gathering actions, we can do even better than just remove re-\ndundant information from an already generated conformant explanation. For example,\nconsider the following exchange in the USAR scenario –\nR : Are you aware that the path from P1 to P4 has collapsed?\nH : Yes.\n> R realizes the plan is optimal in all possible models.\n> It does not need to explain further.\nIf the robot knew that the human thought that the path from P1 to P4 was collapsed,\nit would know that the robot’s plan is already optimal in the human mental model and\n\n86\nAcquiring Mental Models for Explanations\nhence be required to provide no further explanation. This form of explanations can thus\nclearly be used to cut down on the size of conformant explanations by reducing the size\nof the completion set.\nDefinition 31. A conditional explanation is represented by a policy that maps the\nannotated model (represented by a Mmin and Mmax model pair) to either a question\nregarding the existence of a condition in the human ground model or a model update\nrequest. The resultant annotated model is produced, by either applying the model update\ndirectly into the current model or by updating the model to conform to human’s answer\nregarding the existence of the condition.\nIn asking questions such as these, the robot is trying to exploit the human’s (lack of)\nknowledge of the problem in order to provide more concise explanations. This can be\nconstrued as a case of lying by omission and can raise interesting ethical considerations\n(we will look at the role of lies in team in more detail in Chapter 9, Section 9.3). Humans,\nduring an explanation process, tend to undergo this same “selection” process as well in\ndetermining which of the many reasons that could explain an event is worth highlighting.\nModified AO∗-search to find Conditional Explanations\nWe can generate condi-\ntional explanations by either performing post-processing on conformant explanations or by\nperforming an AND-OR graph search with AO∗Nilsson [2014]. Here each model update\nrelated to a known condition forms an OR successor node while each possible condition\ncan be applied on the current state to produce a pair of AND successors, where the first\nnode reflects a node where the annotated condition holds while the second one represents\nthe state where it does not. So the number of possible conditions reduces by one in each\none of these AND successor nodes. This AND successor relates to the answers the hu-\nman could potentially provide when asked about the existence of that particular possible\ncondition. Note that this AND-OR graph will not contain any cycles as we only provide\nmodel updates that are consistent with the robot model and hence we can directly use\nthe AO∗search here.\nUnfortunately, if we used the standard AO∗search, it will not produce a conditional\nexplanation that contains this “less robust” explanation as one of the potential branches\nin the conditional explanation. This is because, in the above example, if the human had\nsaid that the path was free, the robot would need to revert to the original conformant\nexplanation. Thus the cost of the subtree containing this solution will be higher than the\none that only includes the original conformant explanation.\nTo overcome this shortcoming, we can use a discounted version of the AO∗search\nwhere the cost contributed by a pair of AND successors is calculated as –\nmin(node1.h_val, node2.h_val) + γ ∗max(node1.h_val, node2.h_val)\nwhere node1 and node2 are the successor nodes and node1.h_val, node2.h_val are their\nrespective h-values. Here γ represents the discount fact and controls how much the search\nvalues short paths in its solution subtree. When γ = 1, the search becomes standard AO∗\nsearch and when γ = 0, the search myopically optimizes for short branches (at the cost\n\n6.3 Model-Free Explanations\n87\nof the depth of the solution subtree). The rest of the algorithm stays the same as the\nstandard AO∗search.\n6.2.0.4\nAnytime Explanations\nBoth the algorithms discussed above can be computationally expensive, in spite of the\ncompilation trick to reduce the set of possible models to two representative models. How-\never, as we did previously with MCEs, we can also aim for an approximate solution by\nrelaxing the minimality requirement of explanation to achieve much shorter explanation\ngeneration time when required. For this we use an anytime depth first explanation genera-\ntion algorithm. Here, for each state, the successor states include all the nodes that can be\ngenerated by applying the model edit actions on all the known predicates and two possible\nsuccessors for each possible condition – one where the condition holds and one where it\ndoes not. Once the search reaches a goal state (a new model where the target plan is\noptimal throughout its completion set), it queries the human to see if the assumptions it\nhas made regarding possible conditions hold in the human mental model (the list of model\nupdates made related to possible conditions). If all the assumptions hold in the human\nmodel, then we return the current solution as the final explanation (or use the answers\nto look for smaller explanations), else continue the search after pruning the search space\nusing the answers provided by the human. Such approaches may also be able to facilitate\niterative presentation of model reconciliation explanations to the human.\n6.3\nModel-Free Explanations\nThe previous section focused on cases where there still exists an estimate (incomplete\nthough) of the human’s knowledge about the robot. The conciseness of the explanation\ngenerated using this method would still rely on how accurate and complete that estimate\nis.\nIn many cases the robot may start with no such estimate.\nWhile technically one\ncould start with an incomplete model that will cover the space of all possible models\nrepresentable using the current set of action and fluent set, it is unlikely that such a\ngeneral human mental model would lead to concise explanations. One possible alternative\nis to consider a plausible assumption about the human model (such as assuming its an\nabstraction of the robot model or even that the human is completely ignorant of the robot\nmodel). Such assumptions are more effective when the user is ready to interact with the\nsystem while receiving the explanation. A slightly different possibility might be to learn\nthe human’s model. This approach may be particularly well suited when it is possible\nto assume that it is possible for the system to collect data from a set of people about\ntheir ability to make sense of system decisions and the final end user would have similar\nmental model to the set of users. Note that this assumption is similar to the one made\nin model-free explicable plan generation in Chapter 3 (Section 3.3) and we will make use\nof a similar approach to learn simple labeling models that are able to predict whether a\nhuman would find a given explanation satisfactory.\nUnlike the setting in learning for explicability, we will assume the agent now also\nhas access to a set of explanatory messages µ. Note that such messages only contain\n\n88\nAcquiring Mental Models for Explanations\ninformation about the original robot model and are independent of what the human may\nor may not know. So assuming a STRIPS model of representation for explanation, this\ncould contain messages about what propositions are or aren’t part of the robot model. As\nsuch µ could be derived from Γ (i.e the mapping from the model to a set of propositions)\ndiscussed in earlier chapter.\nOnce we have access to such a set of messages our goal is now quite similar to the\none described in chapter 3, as we would want to learn a labelling model over the steps in\nthe plan. To keep the discussion simple, we will assume the labeling model here merely\ncaptures the fact whether the user find the plan step explicable or not (and will not worry\nabout the exact task label they may associate with the step). So the labeling function we\nuse in this context will look something like\nLπR : F × 2µ →{0, 1}\nWhere F is the space of features for the plan steps. Note that unlike previous learned\nlabeling model, here the function actually captures the fact that the human’s expectation\ncould shift once she is given addition information about the robot model. Now we can\nagain take individual labels of the steps and calculate an explicability score for the entire\nplan using a function fexp, where fexp returns 1 if all the steps are labeled as explicable,\nwhich means, according to the labeling model, the plan is perfectly explicable. Thus the\nlearned model labeling model allows us to evaluate the explicability of the plan at each\nmodified model. This means that we can now run the model space search even without\nhaving access to the original human mental model MR\nh . Here the possible model space\nis represented by the space of possible message sets the robot can provide. Each possible\nmodified model in the original scenario should map to at least one message set in this new\nsetting. Instead of running an optimality check, we can check the labeling model to see if\nfor a given set of messages every step in the plan would be labeled as being explicable. In\nthis new setting, one could calculate an approximation of MCE for a plan πR as follows\narg min\nm⊆µ\nC(m) such that fexp(πR, m) = 1\nWhere C gives the cost associated with the set of messages m.\nThe formulation\nlooks for the cheapest set of messages that would render the entire plan explicable. One\ncould similarly use such labeling models to also try extracting explanations that meet the\nrequirements of other forms of explanations. Of course, the effectiveness of this method\nrelies on the ability to learn good labeling models. Instead of making the explicability\nscore a hard constraint, it can also be made part of the objective function.\n6.4\nAssuming Prototypical Models\nAll the previous discussion focused on cases where we handled lack of information about\nhuman model, by either trying to use uncertain model or learning simple model alterna-\ntives. But in many cases such uncertain models may not be available or the system may\n\n6.4 Assuming Prototypical Models\n89\nnot have previous data or be able to interact with the human for a long enough time to\nlearn useful models. An alternative possibility might be to assume a simple prototypical\nmodel for the human and then use it to generate the explanation. In this section, we\nwill consider such a possibility, specifically we will look at cases where we can perform\nmodel reconciliation by assuming human model is some abstraction of the robot model\n(which includes the possibility that its just an empty model). We will look at the kind\nof explanatory queries such assumptions can support and how we could extend them to\ncover all the cases.\nIn particular, we will consider cases where the human model may be considered as a\nstate abstraction of the original models. We will describe abstraction operations in terms\nof transition systems of models. Formally a transition system T corresponding to a model\nM can be represented by a tuple of the form T = ⟨S, L, T, so, Sg⟩, where S is the set of\npossible states in M, L is the set of transition labels (corresponding to the action that\ninduce that transition), T is the set of possible labeled transitions, s0 is the initial state\nand Sg is the set of states that satisfies the goal specified by M.\nDefinition 32. A propositional abstraction function fΛ for a set of propositions Λ\nand state space S, defines a surjective mapping of the form fΛ : S →X, where X is a\nprojection of S, such that for every state s ∈S, there exists a state fΛ(s) ∈X where\nfΛ(s) = s \\ Λ.\nDefinition 33. For a planning model M = ⟨F, A, I, G⟩with a corresponding transition\nsystem T , a model M′ = ⟨F ′, A′, I′, G′⟩with a transition system T ′ is considered an\nabstraction of M\nM\nM for a set of propositions Λ, if for every transition s1\na−→s2 in T\ncorresponding to an action a, there exists an equivalent transition fΛ(s1) a′\n−→fΛ(s2) in T ′,\nwhere a′ is part of the new action set A′.\nWe will slightly abuse notation and extend the abstraction functions to models and\nactions, i.e in the above case, we will have M′ ∈fΛ(M) (where fΛ(M) is the set of all\nmodels that satisfy the above definition for the set of fluents Λ) and similarly we will have\na′ ∈fΛ(a). As per Definition 33, the abstract model is complete in the sense that all plans\nthat were valid in the original model will have an equivalent plan in this new model. We\nwill use the operator < to capture the fact that a model M is less abstract than the model\nM′, i.e if M < M′ then there exist a set of propositions Λ such that M′ ∈fΛ(M).\nIn particular, we will focus on abstractions formed by projecting out a subset of propo-\nsitional fluents. Where for a given model M = ⟨F, A, I, G⟩and a set of propositions Λ,\nwe define the abstract model to be M′ = ⟨F ′, A′, I′, G′⟩, where F ′ = F −Λ, I′ = fΛ(I),\nG′ = fΛ(G) and for every a ∈A (where a = ⟨pre(a), adds(a), dels(a)⟩) there exists a′ ∈A′,\nsuch that a′ = ⟨pre(a) \\ Λ, pre(a) \\ Λ, adds(a) \\ Λ, dels(a) \\ Λ⟩.\nThus if the system has information on some subset of fluents that the user may be\nunaware of, then we can approximate the user model by using the abstraction of the\nmodel by setting Λ to be these fluents. As we will see even if the approximation is more\npessimistic than the true model, we will still be able to generate the required explanation.\nNotice also that here if we set Λ to P, we essentially get an empty model that can be\nthought of as representing the model of a user who isn’t aware of any of the details about\nthe task, which effectively means the user thinks all possible plans are feasible.\n\n90\nAcquiring Mental Models for Explanations\nNow going back to the question of explanation, let us consider cases where the user\nis raising explicit contrastive queries of the form “Why P and not Q?\", where ‘P’ is the\ncurrent plan being proposed by the robot and ‘Q’ is the alternate plan (or set of plans)\nbeing proposed by the human.\nIf the assumption holds that the human’s model is a\ncomplete abstraction then we don’t need to provide any information to justify P since\nit should be valid in the human model. So our focus would be to address the foil plans\nraised by the user. Now consider the cases where the foils raised by the human are in fact\ninfeasible in the robot model. Then as part of the explanation should reveal parts of the\nrobot model that will help show the infeasibility of the human plan. Thus explanation\nhere can be characterized by the set of fluents that was previously missing from the human\nmodel and whose introduction into the human model will help her understand the planning\nproblem better, and thus model reconciliation would leave the human model less abstract\nthan it was before.\nNow what happens if the assumption about the human’s model being an abstraction\nof the robot model is not met? Let us assume the system is aware of the set of fluents\nthat the human may not be completely aware of, but now instead of them simply missing\nthe human may hold incorrect beliefs about them. Interestingly, we can still use the same\napproach discussed above to refute the foils. That is you can still identify the set of fluents\nto refute the foils as if the user’s model was a complete abstraction. But now we need\nadditional mechanisms to provide explanations about the validity of the plan. One simple\napproach would be to allow users to ask specific questions about the steps in the plan\nthey assume are invalid and provide explanation for those queries.\nIn addition to allowing for a reasonable assumption about the user’s model, the use of\nabstractions can also provide us with the tools to address the human’s limited inferential\ncapabilities.\nNote that here the explanations can be provided at the minimal level of\nabstraction needed to explain the foils, i.e., only introduce the minimal set of fluents\nneeded to address the user queries. Thus the inferential overhead on the human’s end\nto make sense of the explanation is minimized. Furthermore, if the human’s confusion\nabout the plan is coming from limited inferential capabilities as opposed to a difference\nin knowledge regarding the task, we can still provide explanations by following the steps\ndescribed in this section. In this case, the user is exposed to an abstract version of the\nmodel and asked to re-evaluate the plan and foils with respect to this simplified model.\n6.5\nBibliographic Remarks\nThe first work to consider model reconciliation in the presence of uncertain models was\nSreedharan et al. [2018a], and the form of the uncertain models used by the work was\nadapted from earlier work in robust planning Nguyen et al. [2017].\nThese annotated\nmodels can also be used to provide explanations for multiple users, by combining the\nmental model of each users into a single uncertain model. The AO∗search described in\nthe chapter is adapted from Nilsson [2014] introduced to solve planning problems where\nthe solution takes the conditional plans that can be represented as acyclic graphs. The\nmethod for model-free model reconciliation was introduced in Sreedharan et al. [2019a]\nfor MDP settings, for the discussion in this chapter we adapted it to classical planning\nproblems. The abstraction based explanation framework called HELM was first proposed\n\n6.5 Bibliographic Remarks\n91\nin Sreedharan et al. [2018b], and then later adapted to partial foil specifications and\nexplaining unsolvability in Sreedharan et al. [2019b].\nSreedharan et al. [2021c] builds\non these earlier works and provides a clearer discussion on connections between model\nreconciliation and HELM style explanation. This paper also presents a user study testing\nthe effectiveness of abstraction to reduce cognitive load on the explainee.\nInstead of\nassuming a set of fluents to build abstractions on, these works generally assume access to\na lattice of abstract models, which might also contain multiple maximally abstract models.\nThis means in these setting they also have to perform an additional localization operation\nto identify possible models that could correspond to the human. These lattices could also\nbe used to encode information about the complexity of concepts there by also allowing\nthem to provide simplest explanations for a given query.\n\n92\nAcquiring Mental Models for Explanations\n\nChapter 7\nBalancing Communication and Behavior\nIn the previous sections, we considered how in human robot teaming scenarios, the robot\nbehavior influences and is influenced by the human’s mental model of the robot.\nWe\nhave been quantifying some of the interaction between the behavior and human’s model\nin terms of three interpretability scores, each of which corresponds to some desirable\nproperty one would expect the robot behavior to satisfy under cooperative scenarios.\nWith these measures defined one of the strategies the robot can adopt is to specifically\ngenerate behavior that optimizes these measures. Though for one of those measures, we\nalso investigated an alternate strategy, namely to use communication to alter the human’s\nmental model to improve explicability of the current plan. Interestingly these strategies\nare not mutually exclusive, they can in fact be combined to capitalize on the individual\nstrengths of each to create behaviors that are unique. In this chapter, we will start by\nfocusing on how one can combine explicable and explanation generation and will look at a\ncompilation based method to generate such plans. In general, communication is a strategy\nwe can use for the other two measures as well. As such, we will end this chapter with a\nbrief discussion on how we could use the idea of combining communication and selective\nbehavior generation to improve legibility and predictability scores.\n7.1\nModified USAR Domain\nA typical Urban Search and Rescue (USAR) scenario consists of an autonomous robot\ndeployed to a disaster scene with an external commander who is monitoring its activities.\nBoth agents start with the same model of the world (i.e the map of the building before the\ndisaster) but the models diverge over time since the robot, being internal to the scene, has\naccess to updated information about the building. This model divergence could lead to\nthe commander incorrectly evaluating valid plans from the robot as sub-optimal or even\nunsafe. One way to satisfy the commander would be to communicate or explain changes\nto the model that led the robot to come up with those plans in the first place.\nFigure 7.1 illustrates a scenario where the robot needs to travel from P1 to its goal at\nP17. The optimal plan expected by the commander is highlighted in grey in their map\nand involves the robot moving through waypoint P7 and follow that corridor to go to P15\nand then finally to P16. The robot knows that it should in fact be moving to P2 – its\noptimal plan is highlighted in blue. This disagreement rises from the fact that the human\nincorrectly believes that the path from P16 to P17 is clear while that from P2 to P3 is\nblocked.\nIf the robot were to follow the explanation scheme established in Chapter 5, it would\nstick to its own plan and provide the following explanation:\n> remove-( c l e a r\np16 p17 )-from-I\n( i . e .\nPath from P16 to P17\ni s\nblocked )\n> add-( c l e a r\np2 p3 )-to-I\n93\n\n94\nBalancing Communication and Behavior\nFig. 7.1: Illustration of the robot model and the corresponding mental model of the human. The robot\nstarts at P1 and needs to go to P17. The human incorrectly believes that the path from P16 to P17 is clear\nand the one from P2 to P3 is blocked due to fire. Both agents know that there is movable rubble between\nP5 and P6 which can be cleared with a costly clear_passage action. Finally, in the mental model, the\ndoor at P8 is locked while it is unlocked in the model for the robot which cannot open unlocked doors.\n( i . e .\nPath from P2 to P3 i s\nc l e a r )\nIf the robot were to stick to a purely explicable plan then it can choose to use the\npassage through P5 and P6 after performing a costly clear_passage action (this plan is\nnot optimal in either of the models).\n7.2\nBalancing Explanation and Explicable Behavior Gener-\nation\nThroughout this chapter, our focus would be mainly on how one could balance selective\nbehavior generation with communication in the context of explicability. In the case of\nexplicable plan generation, we focus on generating behavior that aligns with observer\nexpectations. This process is inherently limited by what the robot can perform and one\ncan not always guarantee that the robot can generate plans with high explicability score,\nlet alone perfectly explicable ones. Moreover under this scheme, the robot is forced to\nperform suboptimal plans that may be quite far from the best course of action prescribed\nby the robot’s model for the given task.\nOn the other hand, under explanation, the agent tries to identify the appropriate\nmodel information that would help convince the observer of the optimality of given robot\nplan, and thus help improve the explicability of the plan in the updated human model.\nUnder this scheme, the robot is free to choose a plan that is optimal under its model but\nnow needs to meet the overhead of communicating the explanation, which depending on\nthe scenario could be quite substantial.\nThe goal of this section is to discuss how one could effectively combine the strengths\nof these two disparate methods. We will achieve this by folding in the how difficult it is to\nexplain a plan into the plan generation process. Thereby allowing the robot to explicitly\n\n7.2 Balancing Explanation and Explicable Behavior Generation\n95\nreason about the gains made by the plan in terms of explicability with the cost of the\nplan and the cost of communicating the explanations. We will call such plans as Balanced\nPlans.\nIn the case of explicability, balanced planning consists of optimizing over the following\nobjectives\n1. Plan Cost C(π): The first objective is the cost of the plan that the robot is going\nto follow.\n2. Communication Cost C(E): The next objective is the cost of communicating the\nexplanation. Ideally this cost should reflect both the cost accrued at the robot’s end\n(corresponding to the cost of actions that need to be performed to communicate the\ninformation) and a cost relating to the difficulty faced by the user to understand the\nexplanation.\n3. Penalty of Inexplicability (CIE(π, MR\nh +E)): This corresponds to the cost related\nto the inexplicability experienced by the human for the current plan in their updated\nmental model. We will assume this to be proportional to the inexplicability score of\nthe plan after receiving the explanations E.\nWhile in the most general case generating a balanced plan would be a multi-objective\noptimization, for simplicity, we will assume that we have weight parameters that allow us\nto combine the individual costs into a single cost. Thus the cost of a balanced plan (which\nincludes both an explanation and a plan), would be given as\nC((E, π)) = α ∗C(π) + β ∗C(E) + γ ∗CIE(π, MR\nh + E)\nThus balanced planning would ideally consist of selecting the plan that minimizes\nthis cost. In the USAR scenario considered in this chapter, assuming all the component\nweights are one and all action except clear_passage is twice the cost, and approximating\nthe inexplicability score IE to be just exponential of the cost difference. If we set the\ncost of communicating a single piece of model information to be high (say 50), then the\noptimal balanced plan would be the most explicable plan, namely the one involving the\nagent moving through the rubble filled path. Depending on the weights and the exact\ncost involved the same formulation could give rise to wildly different behaviors. We will\nrefer to the plan that is optimal with respect to the above objective function as optimal\nbalanced plans. Though in some scenarios, we may want to focus on more constrained\nversions of the problem. In particular, two versions that would be interesting to consider\nwould be\n1. Perfectly Explicable Plans: The first special case is one where we restrict our-\nselves to cases where the plan is perfectly explicable, i.e., optimal, in the resultant\nmodel (i.e. after receiving the explanation). Therefore in our objective we can ignore\nthe inexplicability penalty term and the optimization objective becomes.\nmin α ∗C(π) + β ∗C(E)\nsubject to CIE(π, MR\nh + E) = 0\n\n96\nBalancing Communication and Behavior\nNote that while the plan may be optimal in the human’s updated model, the plan\nneed not be optimal for the robot.\nIn the example considered, an example for\nperfectly explicable plan would be again the plan through the rubble, but now\nincludes an explanation that the path from P16 to P17 is blocked due to fire.\n2. Perfectly Explicable Optimal Plans: In this case, we constrain ourselves to\nidentifying not only plans and explanations that will ensure perfect explicability, but\nwe also try to ensure that the plans are optimal in the robot model. Such methods\ncan be particularly effective when the cost of communicating the explanation is much\nsmaller than the cost of the plan itself.\nThus the objective in this case becomes\njust\nmin C(E)\nsubject to CIE(π, MR\nh + E) = 0\nand C(π, MR) ∈C∗\nMR\nInterestingly, this involves identifying the optimal plan in the robot’s model with\nthe cheapest MCE (in terms of the explanation cost). In the USAR scenario, an\nexample for perfectly explicable optimal plan would be the plan through P4, but\nnow you need to explain that the paths from P16 to P17 and from P2 to P3 are\nactually clear.\nIn the following sections, we will see how one could generate these balanced plans.\n7.2.1\nGenerating Balanced Plans\nNow to actually generate such plans, one could use modifed versions of the model space\nsearch discussed in chapter 6. Specifically if we are limiting ourselves to plans that are\noptimal in the human mental model, then for each possible model in the search space\nwe could consider the space of possible optimal plans and then select the one that best\nmeets our requirement (which would be executability in the robot model). Since this is an\noptimization problem one would have to keep repeating this step till the stopping criteria\nis met.\nIn this case, we need to keep repeating this till we find a model where there\nis at least one plan that is both optimal for the updated model and the original robot\nmodel. Alternatively, one could also consider converting balancing into a single a planning\nproblem. One way to go about this would be by assuming that the robot has access to\na set of communicative actions called explanatory actions through which it can deliver\nthe required explanations to the human. These actions can, in fact, be seen as actions\nwith epistemic effects in as much as they are aimed towards modifying the human mental\nmodel (knowledge state). This means that a solution to a balanced planning problem can\nbe seen as self-explaining plans, in the sense that some of the actions in the plan are aimed\nat helping people better understand the rest of it.\nInclusion of explanatory actions puts balanced planning squarely in the purview of\nepistemic planning, but the additional constraints enforced by the setting allow us to\nleverage relatively efficient methods to solve the problem at hand.\nThese constraints\ninclude facts such as: all epistemic actions are public, modal depth is restricted to one,\n\n7.2 Balancing Explanation and Explicable Behavior Generation\n97\nmodal operators only applied to literals, for any literal the observer believes it to be true\nor false and the robot is fully aware of all of the observer beliefs.\nModel updates in the form of epistemic effects of communication actions also open\nup the possibility of other actions having epistemic side effects as well. The definition of\nbalanced planning makes no claims as to how the model update information is delivered.\nIt is quite possible that actions that the agent is performing to achieve the goal (henceforth\nreferred to as task-level actions to differentiate it from primarily communication actions)\nitself could have epistemic side-effects.\nThis is something people leverage to simplify\ncommunication in day to day lives – e.g. one might avoid providing prior description of\nsome skill they are about to use when they can simply demonstrate it. The compilation\nof balanced planning into a single planning problem allows us to capture such epistemic\nside effects. By the same token the compilation can also capture task level constraints\nthat may be imposed on the communication actions.\n7.2.1.1\nCompilation to Classical Planning\nTo support such self-explaining plans, we can adopt a formulation that is similar to ones\npopular in epistemic planning literature to compile it into classical planning formalisms.\nIn our setting, each explanatory action can be viewed as an action with epistemic effects.\nOne interesting distinction to make here is that the mental model now not only includes\nthe human’s belief about the task state but also their belief about the robot’s model. This\nmeans that the planning model will need to separately keep track of (1) the current robot\nstate, (2) the human’s belief regarding the current state, (3) how actions would affect each\nof these (as humans may have differing expectations about the effects of each action) and\n(4) how those expectations change with explanations.\nGiven the human and robot model, MR and MR\nh , we will generate a new planning\nmodel MΨ = ⟨F Ψ, AΨ, IΨ, GΨ, CΨ⟩as follows F Ψ = F ∪FB ∪Fµ ∪{G, I}, where FB is a\nset of new fluents that will be used to capture the human’s belief about the task state and\nFµ is a set of meta fluents that we will use to capture the effects of explanatory actions\nand G and I are special goal and initial state propositions. We will use the notation B(p)\nto capture the human’s belief about the fluent p. We are able to use a single fluent to\ncapture the human belief for each (as opposed to introducing two new fluents B(p) and\nB(¬p)) as we are specifically dealing with a scenario where the human’s belief about the\nrobot model is fully known and human either believes each of the fluent to be true or false.\nFµ will contain an element for every part of the human model that can be changed\nby the robot through explanations. A meta fluent corresponding to a literal ϕ from the\nprecondition of an action a takes the form of µ+(ϕpre(a)), where the superscript + refers to\nthe fact that the clause ϕ is part the precondition of the action a in the robot model (for\ncases where the fluent represents an incorrect human belief we will be using the superscript\n“−”).\nFor every action aR = ⟨pre(aR), adds(aR), dels(aR)⟩∈AR and its human counterpart\naR\nh = ⟨pre(aR\nh ), adds(aR\nh ), dels(aR\nh )⟩∈AR\nh , we define a new action\naΨ = ⟨pre(aΨ), adds(aΨ), dels(aΨ)⟩∈AΨ ∈MΨ\n\n98\nBalancing Communication and Behavior\nwhose precondition is given as:\npre(aΨ) = pre(aR\nh ) ∪{µ+(ϕpre(aR)) →B(ϕ)|ϕ ∈pre(aR) \\ pre(aR\nh )}\n∪{µ−(ϕpre(aR)) →B(ϕ)|ϕ ∈pre(aR\nh ) \\ pre(aR)} ∪{B(ϕ)|ϕ ∈pre(aR\nh ) ∩pre(aR)}\nThe important point to note here is that at any given state, an action in the augmented\nmodel is only applicable if the action is executable in robot model and the human believes\nthe action to be executable. Unlike the executability of the action in the robot model\n(captured through unconditional preconditions) the human’s beliefs about the action exe-\ncutability can be manipulated by turning the meta fluents on and off. The effects of these\nactions can also be defined similarly by conditioning them on the relevant meta fluent.\nIn addition to these task level actions (represented by the set Aτ), we can also define\nexplanatory actions (Aµ) that either add µ+(∗) fluents or delete µ−(∗). Special actions a0\nand a∞that are responsible for setting all the initial state conditions true and checking\nthe goal conditions are also added into the domain model. a0 has a single precondition\nthat checks for I and has the following effects:\nadds(a0) = {⊤→p | p ∈IR} ∪{⊤→B(p) | p ∈IR\nh } ∪{⊤→p | p ∈Fµ−}\ndels(a0) = {I}\nwhere Fµ−is the subset of Fµ that consists of all the fluents of the form µ−(∗). Similarly,\nthe precondition of action a∞is set using the original goal and adds the special goal\nproposition G.\npre(a∞) = GR ∪{µ+(pG) →B(p) | p ∈GR \\ GR\nh }∪\n{µ−(pG) →B(p) | p ∈GR\nh \\ GR} ∪{B(p) | GR\nh ∩GR}\nFinally the new initial state and the goal specification becomes IE = {I} and GE = {G}\nrespectively. To see how such a compilation would look in practice, consider an action\n(move_from p1 p2) that allows the robot to move from p1 to p2 only if the path is clear.\nThe action is defined as follows in the robot model:\n( : action move_from_p1_p2\n: precondition\n( and ( at_p1 )\n( clear_p1_p2 ))\n: e f f e c t\n( and ( not ( at_p1 ))\n( at_p2 )\n))\nLet us assume the human is aware of this action but does not care about the status of\nthe path (as they assume the robot can move through any debris filled path). In this case,\nthe corresponding action in the augmented model and the relevant explanatory action will\nbe:\n( : action move_from_p1_p2\n: precondition\n( and ( at_p1 )\n(B (( at_p1 ) ) )\n( clear_p1_p2 )\n( implies\n(µ+\npre (move_from_p1_p2 ,\n( clear_p1_p2 ) ) )\n(B (( clear_p1_p2 ) ) ) ) )\n: e f f e c t\n( and ( not ( at_p1 ))\n( at_p2 )\n\n7.2 Balancing Explanation and Explicable Behavior Generation\n99\n( not B( at_p1 )) B( at_p2 ) ) ) )\n( : action\nexplain_µ+\npre_move_from_clear\n: precondition\n( and )\n: e f f e c t\n( and µ+\npre (move_from_p1_p2 ,\n( clear_p1_p2 ) ) ) )\nFinally CΨ captures the cost of all explanatory and task level actions. For now, we will\nassume that the cost of task-level actions are set to the original action cost in either robot\nor human model and the explanatory action costs are set according to CE. Later, we will\ndiscuss how we can adjust the explanatory action costs to generate desired behavior.\nWe will refer to an augmented model that contains an explanatory action for each\npossible model updates and has no actions with effects on both the human’s mental model\nand the task level states as the canonical augmented model. Given an augmented model,\nlet πE be a plan that is valid for this model (πE(IΨ) ⊆GΨ). From πE, we extract two\ntypes of information – the model updates induced by the actions in the plan (represented\nas E(πE)) and the sequence of actions that have some effect on the task state (represented\nas T(πE)). We refer to the output of T as the task level fragment of the original plan πE.\nE(πE) may also contain effects from action in T(πE).\n7.2.2\nStage of Interaction and Epistemic Side Effects\nOne of the important parameters of the problem setting that we have yet to discuss is\nwhether the explanation is meant for a plan that is proposed by the system (i.e the system\npresents a sequence of actions to the human) or are we explaining some plan that is being\nexecuted either in the real world or some simulation that the human observer has access\nto. Even though the above formulation can be directly used for both scenarios, we can use\nthe fact that the human is observing the execution of the plans to simplify the explanatory\nbehavior by leveraging the fact that many of these actions may have epistemic side effects.\nThis allows us to not explain any of the effects of the actions that the human can observe\n(for those effects we can directly update the believed value of the corresponding state\nfluent and even the meta-fluent provided the human doesn’t hypothesize a conditional\neffect).1 This is beyond the capability of any of the existing algorithms in this space of\nthe explicability-explanation dichotomy.\nThis consideration also allows for the incorporation of more complicated epistemic\nside-effects wherein the human may infer facts about the task that may not be directly\ntied to the effects of actions. Such effects may be specified by domain experts or generated\nusing heuristics. Once identified, adding them to the model is relatively straightforward as\nwe can directly add the corresponding meta fluent into the effects of the relevant action.\nAn example for a simple heuristic would be to assume that the firing of a conditional\neffect results in the human believing the condition to be true, provided the observer can’t\ndirectly observe the fluents it was conditioned on. For example, if we assume that the\nrobot had an action (open_door_d1_p3) that had a conditional effect:\n1This means that when the plan is being executed, the problem definition should include the observation\nmodel of the human (which we assume to be deterministic). To keep the formulation simple, we ignore\nthis for now. Including this additional consideration is straightforward for deterministic sensor models.\n\n100\nBalancing Communication and Behavior\n(when ( and ( unlocked_d1 ))\n(open_d1 ))\nwhich says the door will open if it was unlocked. Then in the compiled model, we can add\na new effect to this conditional effect:\n(when ( and ( unlocked_d1 ))\n( and B(open_d1) B( unlocked_d1 ) ) )\nwhich basically says, that if the conditional effect executes the human will believe that\nthe door was unlocked. Even in this simple case, it may be useful to restrict the rule to\ncases where the effect is conditioned on previously unused fluents so the robot does not\nexpect the observer to be capable of regressing over the entire plan.\n7.2.3\nOptimizing for Explicability of the Plan\nThe balancing formulations discussed in this chapter require more than just generating\nplans that are valid in both robot and updated human model. They require us to either\noptimize for or ensure complete explicability of plans. For the Perfectly Explicable Plans\nand Perfectly Explicable Optimal Plans formulation, where plans need to be optimal in the\nhuman model we will need to enforce this as an additional goal test for the planner while\nfor the most general formulation the inexplicability score could be added as additional\npenalty to the last action of the plan.\nThough to allow for Perfectly Explicable Optimal Plans, we need to restrict the plans\nto only ones that are optimal in the robot model. We can generate such robot optimal\nplans by setting lower explanatory action costs.\nBefore we formally state the bounds\nfor explanatory costs, let us define the concept of optimality delta (denoted as ∆πM) for\na planning model, which captures the cost difference between the optimal plan and the\nsecond most optimal plan. More formally ∆πM can be specified as:\n∆πM = min{v | v ∈R ∧\n̸ ∃π1, π2((0 < (C(π1) −C(π2)) < v)\n∧π1(IM) |=M GM ∧π2(IM) ∈Π∗\nM}\nProposition 9. In a canonical augmented model MΨ for the human and robot model\ntuple Ψ = ⟨MR, MR\nh ⟩, if the sum of costs of all explanatory actions is ≤∆πMR and if π\nis the cheapest valid plan for MΨ such that T(π) ∈Π∗\nMΨ+E(π), then:\n(1) T(π) is optimal for MR\n(2) E(π) is the MCE for T(π)\n(3) There exists no plan ˆπ ∈Π∗\nR such that MCE for T(ˆπ) is cheaper than E(π), i.e. the\nsearch will find an the plan with the smallest MCE.\nFirst off, we can see that there exists no valid plan π′ for the augmented model (MΨ)\nwith a cost lower than that of π and where the task level fragment (T(π′)) is optimal for\n\n7.3 Balancing Communication and Behavior For other Measures\n101\nthe human model. Let’s assume T(π) ̸∈Π∗\nR (i.e current plan’s task-level fragment is not\noptimal in robot model) and let ˆπ ∈Π∗\nR. Now let’s consider a plan ˆπE for augmented\nmodel that corresponds to the plan ˆπ, i.e, E(ˆπE) is the MCE for the plan ˆπ and T(ˆπE) = ˆπ.\nThen the given augmented plan ˆπE is a valid solution for our augmented planning problem\nMΨ (since the ˆπE consists of the MCE for ˆπ, the plan must be valid and optimal in the\nhuman model), moreover the cost of ˆπE must be lower than π. This contradicts our earlier\nassumption hence we can show that T(π) is in fact optimal for the robot model.\nUsing a similar approach we can also show that no cheaper explanation exists for πE\nand there exists no other plan with a cheaper explanation.\nNote that while it is hard to find the exact value of the optimality ∆πM, it is guaranteed\nto be ≥1 for domains with only unit cost actions or ≥(C2 −C1), where C1 is the cost\nof the cheapest action and C2 is the cost of the second cheapest action, i.e. ∀a(CM(a) <\nC2 →CM(a) = C1). Thus allowing us to easily scale the cost of the explanatory actions\nto meet this criteria.\n7.3\nBalancing Communication and Behavior For other Mea-\nsures\nWhile the majority of the chapter is focused on balancing communication and behavior\nselection to improve explicability, it should be easy to see that such schemes should be\nextendable to other behavioral metrics.\nIn the case of legibility, since the original framework itself is focused on communica-\ntion, it is easy to replace specialized behavior with communication. In particular, for a\nformulation that dealt exclusively with goal uncertainty, you are effectively trading off\na single message (namely the goal of the robot) with the additional effort that needs to\nbe taken by the robot to communicate its eventual goal implicitly. Though one could\neasily consider the more general version of legibility where the uncertainty is over entire\nmodels or different model components. In such cases, one could foresee a combination of\ncommunication and selective behavior to effectively communicate the actual parameters.\nIn this case, the communication takes a form quite similar to the explanation since we\nwill be communicating model information. The planning objective for a setting that tries\nto maximize legibility within k steps, would be\nC(⟨E, π⟩) = α ∗C(π) + β ∗C(E) + γ ∗P(θ|MR\nh + E, ˆπk)\nWhere θ is the target model parameter being communicated (which can be the full\nmodel), MR\nh the hypothesis set maintained by the human observer, ˆπk is the k step prefix\nof the plan π. Thus the formulation tries to optimize for the probability that the human\nwould associate with the target model parameter. Similar to the explicability case, one\ncould also formulate the more constrained cases.\nNow moving onto predictability, one main difference would be the communication\n\n102\nBalancing Communication and Behavior\nstrategy to be used. Predictability, as originally conceived, was meant for cases where\nthe human and robot shares the same mental model, but the human observer is unsure\nabout the future behavior the robot will exhibit.\nSo even if the human has incorrect\nbeliefs or uncertainty about the robot model, the communication strategy here needs to\ninclude more than just model information. A useful strategy here could be for the robot to\ncommunicate potential constraints on the behavior that the robot could actually follow.\nA possibility here could be to communicate a partially ordered set of action or state\nfacts, which basically conveys a commitment on the part of the robot that no matter the\nexact behavior the robot ends up choosing it would be one that involves either performing\nthe actions or achieving facts in the order specified, thereby constraining the possible\ncompletions it can follow and thus allowing the rest of the behavior. Now using E again as\nthe stand-in for the information being conveyed, MR\nh +E as the updated human model and\napplying the constraints specified in the communication, then the objective for planning\nfor balanced plans that maximize predictability in k steps is given as\nC(⟨E, π⟩) = α ∗C(π) + β ∗C(E) + γ ∗P(π|MR\nh + E, ˆπk)\nWhere P(π|MR\nh + E, ˆπk) is the probability the human observer associates with the actual\nplan after observing the prefix P(π|MR\nh + E, ˆπk). Similar to the earlier formulations, we\ncan also look for constrained versions of the objective, that requires perfect predictability\nand optimality in robot model.\nMoreover, in both these cases, we can adapt the compilation method discussed earlier\nto fit these new objectives. In particular, we just need to change the goal-test in the case\nof constrained version and or the additional penalty calculated from IE to the respective\ninterpretability measures.\n7.4\nBibliographic Remarks\nThe first work to consider balanced planning was Chakraborti et al. [2019c], that consid-\ners the generation of the Perfectly Explicable Plans. In particular, they consider a model\nspace search based solution to identify the plan. Unfortunately, in the case of the model\nspace search, the only way to guarantee the explanation generated as part of Perfectly\nExplicable Plans is minimal would be to iterate over all the optimal plans in the given\nmodel. Thus the method they ended up operationalizing was an approximate version,\nwhere they guaranteed that while the plan generated is perfectly explicable, the expla-\nnation may be larger than required. The paper also presents some initial user studies\nto validate the usefulness of such balanced plans. The classical planning compilation for\nbalanced planning was Sreedharan et al. [2020a]. This was also the first work to connect\nepistemic planning to model reconciliation and the central compilation method was de-\nrived from an earlier work to compiling restricted forms of epistemic planning to classical\nplanning Muise et al. [2015]. The paper also discusses how all three types of balanced\nplans (i.e OPtimal Balanced Plans, Perfectly Explicable Plans and, Perfectly Explicable\nOptimal Plans) can be generated. In terms of balancing for other interpretability mea-\nsures, while we are unaware of works that consider them in the general form, works like\nChakraborti et al. [2018] have looked at applying these ideas in more specific context.\nIn particular, Chakraborti et al. [2018] looks at using mixed reality based visualization\n\n7.4 Bibliographic Remarks\n103\nto improve, explicability, predictability and legibility. The kind of scenarios they consid-\nered included a block stacking scenario where they considered use of mixed reality cues\nto highlight potential blocks that might be used in the plan and thereby improving the\nidentification of the plan and eventual goal of the agent.\n\n104\nBalancing Communication and Behavior\n\nChapter 8\nExplaining in the Presence of Vocabulary\nMismatch\nAll previous discussions on model-reconciliation explanations implicitly assume that the\nrobot can communicate information about the model to the user.\nThis suggests that\nthe human and the robot share a common vocabulary that can be used to describe the\nmodel. However, this cannot be guaranteed unless the robots are using models that are\nspecified by an expert. Since many of the modern AI systems rely on learned models,\nthey may use representational schemes that would be inscrutable to most users. So in this\nchapter, we will focus on the question of generating model reconciliation explanations in\nthe absence of shared vocabulary. Specifically, we will see how one could identify relevant\nmodel information in terms of concepts specified by a user. We will describe how the\nagent could use classifiers learned over such concepts to identify fragments of symbolic\nmodels specified in terms of such concepts that are sufficient to provide explanations. We\nwill also discuss how one could measure the probability of the generated model fragments\nbeing true (especially when the learned classifiers may be noisy) and also how to identify\ncases where the user specified concepts may be insufficient.\n8.1\nRepresentation of Robot Model\nNote that the assumption that the agent can communicate model information in itself\nentails two separate parts, (a) the model follows a representation scheme that is intuitive\nor understandable to the end users and (b) the model is actually represented using fac-\ntors that makes sense to its end user. Model-reconciliation, does not technically require\nexplanation to be carried out in the same terms as the representation scheme used by the\nagent. Instead we could always map the given robot model information into representation\nschemes that are easier for people to understand. In the previous chapters, we assumed\nthat the robot model is already represented using STRIPS like description languages and\nin this chapter we will look at mapping the original model of the robot (regardless of its\ncurrent representation scheme) into a STRIPS model. Such representation schemes are not\nonly expressive (any deterministic goal directed planning problem with finite states and\nactions can be captured as STRIPS planning problem), but they also have their roots in\nfolk psychology [Miller, 2019]. Thus model information represented in such representation\nschemes are relatively easier for people to make sense of.\nA more pressing question is what should be the state factors or action labels that we\nshould use to represent the model. The fact that the model is represented using STRIPS in\nand of itself wouldn’t facilitate effective explanation, if the information about an action’s\nprecondition is represented using concepts alien to the end user. We will use the term\nvocabulary used by the model to refer to the state fluents and the action names used by it.\nThus access to a shared vocabulary is a prerequisite to facilitating model reconciliation\nexplanations. In this chapter, we will look at cases where this is not a given. We will look\nat one possible way of collecting vocabulary concepts from the end users and discuss how\n105\n\n106\nExplaining in the Presence of Vocabulary Mismatch\nwe could use the collected vocabulary to generate the model or at least identify parts of\nthe model relevant to the explanation. We will look at (a) how to handle cases where the\nvocabulary items we gathered are insufficient (b) how to ensure that models learned this\nway actually reflect true model and (c) how to handle cases where the mappings we learn\nfrom human concepts to task states may be noisy. We will also see how this process of\nmapping the model into a secondary representation in user defined terms could also give\nan opportunity to approximate and simplify the model and by extension the explanation\nitself.\nIn this chapter we will focus on acquiring human understandable state variables/factors\nand assume the action labels are given beforehand. We do this because if the human and\nthe robot have a one to one correspondence between what constitutes an atomic action,\nit is usually easier to establish their labels/names. Acquiring action labels could become\nchallenging if the agent and the human are viewing the action space at varying levels of\nabstraction, but we will ignore this possibility for now.\n8.2\nSetting\nFor the purposes of discussion, we will look at a slightly different scenario where the robot\nhas access to a deterministic simulator of the task of the form Msim = ⟨S, A, T, C⟩(where\nthis simulator effectively acts as the robot model), where S represents the set of possible\nworld states, A the set of actions and T the transition function that specifies the problem\ndynamics. The state space S here need not be defined over any state factors and may\njust be atomic states. The transition function is defined as T : S × A →S ∪{⊥}, where\n⊥corresponds to an invalid absorber-state generated by the execution of an infeasible\naction. Invalid state could be used to capture failure states that could occur when the\nagent violates hard constraints like safety constraints. Finally, C : A →R captures the\ncost function of executing an action in any state where the action is feasible. We will\noverload the transition function T to also work on action sequence, i.e., T(s, ⟨a1, ..., ak⟩) =\nT(...T(T(s, a1), a2), ..., ak). We will assume the problem is still goal-directed in the sense\nthat the robot needs to come up with the plan π, that will drive the state of the world to\na goal state. In general we will use the tuple Πsim = ⟨I, G, Msim⟩to represent the decision\nmaking problem, where I is the initial state and G the set of goal states. Similar to the\nearlier settings, a plan is optimal if it achieves the goal and there exists no cheaper plan\nthat can achieve the goal (where C(I, π) is the total cost of executing the plan π).\nNow the challenge before us is to map this blackbox simulator model into a STRIPS\nmodel defined using vocabulary that makes sense to the human. In particular we want to\nfirst collect the set of propositional state fluents that the human uses to define the model\nMR\nh . Before we discuss how to collect such state fluents information, let us take a quick\nlook at the state fluents themselves. For one, how does one connect a model defined over\natomic space to a set of propositional fluents? The basic convention we will follow here is\nthat each proposition state fluent correspond to a fact about the task, that is either true\nor false in all states of the given simulator Msim. For a given set of state fluents, each\nstate in Msim is mapped to the set of propositional fluents that are true in that state and\neach propositional fluent can be for our purposes by the subset of states S where that\nfluent is true.\n\n8.2 Setting\n107\nNow lets assume the human observer is ready to list all these fluents (captured by the\nset C) they believe are relevant to the problem at hand. We can hardly expect the human\nto be able to list or point out all possible states corresponding to each factor. Instead we\nwill employ a different method to learn the mapping between atomic states and fluents.\nIn this chapter, we will assume that each factor can be approximated by a classifier. Now\nwe can learn such a classifier for a fluent (or concept) in the set C by asking the user for\na set of positive and negative examples for the concept. This means that the explanatory\nsystem should have some method of exposing the simulator states to the user. A common\nway to satisfy this requirement would be by having access to visual representations for\nthe states. The simulator state itself doesn’t need to be an image as long as we have a\nway to visualize it. The concept list can also be mined from qualitative descriptions of the\ndomain and we can crowdsource the collection of example states for each concept. Once\nwe have learned such a set of classifiers, we can then use these factors to learn a model of\nthe task and start using it for the explanations.\n8.2.1\nLocal Approximation of Planning Model\nIn most of the previous discussions in the book the implicit assumption was that the\nexplanation was given with respect to a representation of the entire robot model. This\ncould mean the model being used for explanation is quite complex and large even after\nthe application of simple abstraction methods. A strategy quite popular in explaining\nclassification decisions is to explain a given decision with respect to an approximation of\nthe original model that only captures the behavior of the model for a subset of the input\nspace. We can rely on a similar strategy for explanation in sequential decision-making\nscenarios. Namely, we can focus on a symbolic representation of the task that aims to\ncapture the dynamics of the underlying domain model only for a subset of states.\nMore formally, consider a STRIPS model defined over a set of concepts C, MC =\n⟨C, AC, C(I), C(G), CC\nS ⟩, where C(G) = T\nsg∈G C(sg). We will call this model to be a local\napproximation of the simulation Msim = ⟨S, T, A, C⟩for regions of interest ˆS ⊆S, if ∀s ∈ˆS\nand ∀a ∈A, we have an equivalent action aC ∈AC\nS, such that aC(C(s)) = C(T(s, a))\n(assuming C(⊥) = ⊥) and CC\nS (a) = C(a).\nNow one of the important aspects of this formulation is the subset of states over which\nwe will define the approximation. A popular strategy used in machine learning approaches\nis to select a set of input points close to the data point being explained, where closeness\nof two data points is defined via some pre-specified distance function. This strategy can\nalso be adapted to sequential decision-making settings. In this case, the data point would\ncorrespond to the plan (and in some cases also the foils) and the state space of interest can\nbe limited to states close to the ones that appear on the plan or foil execution trace. The\ndistance function can be defined over some representation of the underlying state or use\nreachability measures to define closeness (i.e. whether the states can be reached within\nsome predefined number of steps).\n\n108\nExplaining in the Presence of Vocabulary Mismatch\nFig. 8.1: An overview of the overall system described in the chapter. The screenshot is from a game\ncalled Montezuma’s revenge. In this particular instance, the agent is tasked with making its way to the\nkey on the left side of the screen while avoiding the skull on the ground.\n8.2.2\nMontezuma’s Revenge\nFor the purposes of the discussion we will consider the game Montezuma’s revenge as the\nexample scenario. The game was first launched in 1984 for Atari gaming systems, and fea-\ntured a character that needs to navigate various levels filled with traps and enemies while\ncollecting treasures. The game has recently received a lot of attention as a benchmark for\nRL algorithms and is a domain that requires long term planning to solve. The simulator\nhere consists of the game engine, which stores the current state and predicts what happens\nwhen a specific action is executed. Two popular state representation schemes for the game\nthat is used by RL algorithms either involves the use of images of the game state or the\nRAM state of the game controller corresponding to the game state. Neither of which is\nparticularly easy for a naive user to understand. The human observer could be a user\nof this game playing system, and as such we will sometimes refer to the human as the\nuser in the text. In this chapter we will mostly focus on the first level of the game that\nis pictured in 8.1. In particular, we will focus on the task of getting the agent from the\npoint visualized in the image to the key it needs to pick up. A successful plan for this task\nwould involve the agent making its way to the lowest platform and them moving towards\nthe skull, jumping over it once it close enough and then climbing up the ladder before\ngoing for the key. Now if we were to come up with a set of concepts to describe the agent,\nmany of them would involve the actual position of the agent (like if the agent is on the\nleftmost ladder or if the agent is on the topmost platform etc.), whether the agent has\npossession of the key, where the agent is with respect to the skull (especially if the agent\nis right next to it) etc. One can train a classifier that takes either the image or the RAM\nstate of a specific game state and determines whether the concept is absent or present\nin that state. Now the goal of many of the methods discussed in this chapter would be\nto build models that can approximate the dynamics of the game in this level using such\n\n8.3 Acquiring Interpretable Models\n109\nconcepts.\n8.3\nAcquiring Interpretable Models\nNow to provide explanations in this setting, the first approach would be to try constructing\nthe model MC, particularly the action definition as the initial states and goal specification\nare easier to construct. For now we will assume that we have a perfect classifier for each\nof the concept listed in the list C. This means that for any state sampled from the set we\nknow the exact list of concepts present in that state. Now we will use the simulator to\nobserve how each action transforms the state and convert it into symbolic representation\nthrough the classifiers.\nMore specifically, we will sample a set of states from the simulator state space (where\nthe samples are limited to ones that meet the desired distance requirement discussed in\nprevious section). For each state sampled, we test whether the actions are executable (i.e\nit doesn’t lead to an invalid state), and for each action we can update the definition of the\naction a as follows (assuming the sample state is si and the state resulting from executing\na is denoted as a(si))\npre(ac) = pre(ac) ∩C(si)\nadds(ac) = C(a(si)) \\ C(si)\ndels(ac) = C(si) \\ C(a(si))\nWhere the original estimate of the action preconditions (pre(ac)) is set to be equal to C.\nNote that learning using this method makes the specific assumption that each action in\nthe exact local symbolic approximation only contains preconditions that are conjunctions\nof positive concepts and are free of conditional effects. If the model in fact does not meet\nthese assumptions, we may need to rely on more complex model learning techniques. 1\nOnce such a model is learned we could then leverage many of the explanatory methods\ndiscussed in the book to generate explanations (especially the ones making simplifying\nassumptions about the human model), though one obvious issue that could occur is that\nthe original concept list provided to the explanatory system may be incomplete. That is\nthe fluent list of the exact local approximation (we will represent this as M∗) may be a\nsuperset of C. Let the learned model be\nˆ\nMC, and for now let us assume that this model\nis the fix-point of the learning process described above. Then it is easy to see that if the\nassumptions about M∗hold (i.e., it is conditional effect free and has only conjunction of\npositive literals as precondition) then\nˆ\nMC, must be a complete abstraction of M∗, in so\nfar as any plan viable in M∗must also be valid in ˆ\nMC. This is because the above learning\nmethod will ensure that the learned preconditions and effects are a subset of the original\naction precondition and effects. As we have already seen in Chapter 6 (Section 33), this\nmeans that even though this is not the exact model, this model suffices to address many\nspecific user queries. Whenever the user raises a foil that cannot be refuted through ˆ\nMC,\nthen the system can use that as a signal that the current vocabulary is incomplete.\n1Though if the only assumption not met is the form of the precondition then it can be mapped to a\ncase of missing concepts that could be handled by assuming the exact approximation model meets the\nsame representational assumptions but is defined over a larger set of concepts.\n\n110\nExplaining in the Presence of Vocabulary Mismatch\n8.4\nQuery Specific Model Acquisition\nThe previous section talked about how we can acquire a representation of the entire model\nthat can then be used in the standard model reconciliation process. However, this could\nbe a pretty expensive process, especially if the action space is quite large. In cases where\nthe agents are raising specific explanatory queries we may not require the entire model,\nbut rather only learn the parts of the model necessary to answer the specific user query.\nTo illustrate this process, let us take a look at the simplest contrastive explanation setting.\nHere the human explanatory query consists of asking why the system chose to follow the\ncurrent plan instead of a set of alternative plans. Specifically in this case, for the decision-\nmaking problem specified by the tuple Πsim = ⟨I, G, Msim⟩the system identifies a plan π.\nWhen presented with the plan, the user of the system responds by raising an alternative\nplan πf (the foil) that they believe should be followed instead. Now the system would\nneed to explain why the plan π is preferred over the foil πf in question. The only two\npossibilities here are that either the foil is inexecutable and hence cannot be followed or\nit is costlier than the plan in question.\nDefinition 34. The plan π is said to be preferred over a foil πf for a problem Πsim =\n⟨I, G, Msim⟩, if either of the following conditions are met, i.e.,\n1. πf is inexecutable, which means, either (a) T(I, πf) ̸∈G, i.e the action sequence\ndoesn’t lead to a possible goal state, or (b) the execution of the plan leads to an\ninvalid state, i.e., T(I, πf) = ⊥.\n2. Or πf is costlier than π, i.e., C(I, π) < C(I, πf)\nSo going back to our montezuma example. Let’s assume the plan involves the agent\nstarting from the highest platform, and the goal is to get to the key. The specified plan\nπ may require the agent to make its way to the lowest level, jump over the skull, and\nthen go to the key with a total cost of 20. Let us consider a case where the user raises\ntwo possible foils that are quite similar to π, but, (a) in the first foil, instead of jumping\nthe agent just moves left (as in it tries to move through the skull) and (b) in the second,\ninstead of jumping over the skull, the agent performs the attack action (not part of the\noriginal game, but added here for illustrative purposes) and then moves on to the key.\nNow using the simulator, the system could tell that in the first case, moving left would\nlead to an invalid state and in the second case, the foil is more expensive. It may however\nstruggle to explain to the user what particular aspects of the state or state sequence lead\nto the invalidity or suboptimality as it cannot directly expose relevant model information.\nInstead, as in earlier parts it would need to map it to a specific symbolic model and\nexpose information about that model. However in this case, rather than first learning a\nfull symbolic model and then identifying relevant model components to provide, we can\ndirectly try to learn the model parts. To capture scenarios like the one mentioned above,\nwe will allow for a class of richer symbolic, namely one that allows for cost functions that\nare conditioned on the state in addition to the action. This means the cost function will\ntake the form CS : 2F ×AS →R to capture the cost of valid action executions in a specific\nstate. Internally, such state models may be represented using conditional cost models. In\nthis discussion, we won’t try to reconstruct the exact cost function but will rather try to\nestimate an abstract version of the cost function.\n\n8.4 Query Specific Model Acquisition\n111\nAs a quick aside, most of the discussion in this section focuses on generating explana-\ntion to refute the alternate plan and not really on explaining why the current plan works\nor has the cost assigned to it. Since the human can observe the robot and we had previ-\nously assumed that there exists mechanism to visualize the simulator states, in theory the\nrobot could just demonstrate the outcome of executing the plan. One could also adapt\nthe methods we discuss for refuting the foil to answer more specific question the human\nuser might have about the robot, for example by learning an abstract cost function for\nthe actions in the plan or by identifying whether a concept the human had in mind is in\nfact a precondition or not.\n8.4.1\nExplanation Generation:\nFor establishing the invalidity of πf, we just need to focus on explaining the failure of\nthe first failing action ai, i.e., the last action in the shortest prefix that would lead to an\ninvalid state (which in our running example is the move-left action in the state presented\nin Figure 8.1 for the first foil). We can do so by informing the user that the failing action\nhas an unmet precondition, as per the symbolic model, in the state it was executed in.\nFormally\nDefinition 35. For a failing action ai for the foil πf = ⟨a1, .., ai, .., an⟩, ci ∈C is con-\nsidered an explanation for failure if ci ∈pre(ai) \\ C(si), where si is the state where ai is\nmeant to be executed (i.e si = T(I, ⟨a1, .., ai−1⟩)).\nIn our example for the invalid foil, a possible explanation would be to inform the user\nthat move-left can only be executed in states for which the concept skull-not-on-left\nis true; and the concept is false in the given state. This formulation is enough to capture\nboth conditions for foil inexecutability by appending an additional goal action at the end\nof each sequence. The goal action causes the state to transition to an end state and it fails\nfor all states except the ones in G. Our approach to identifying the minimal information\nneeded to explain specific query follows from studies in social sciences that have shown\nthat selectivity or minimality is an essential property of effective explanations.\nFor explaining the suboptimality of the foil, we have to inform the user about CC\nS . To\nensure minimality of explanations, rather than generating the entire cost function or even\ntrying to figure out individual conditional components of the function, we will instead try\nto learn an abstraction of the cost function Cabs\ns\n, defined as follows\nDefinition 36. For the symbolic model MC\nS = ⟨C, AC\nS, C(I), C(G), CC\nS ⟩, an abstract cost\nfunction Cabs\nS\n: 2C × AC\nS →R is specified as follows Cabs\nS ({c1, .., ck}, a) = min{CC\nS(s, a)|s ∈\nSMC\nS ∧{c1, .., ck} ⊆s}].\nIntuitively, Cabs\nS ({c1, .., ck}, a) = k can be understood as stating that executing the action\na, in the presence of concepts {c1, .., ck} costs at least k. We can use Cabs\nS\nin an explanation\nof the form\nDefinition 37. For a valid foil πf = ⟨a1, .., ak⟩, a plan π and a problem Πsim = ⟨I, G, Msim⟩,\nthe sequence of concept sets of the form Cπf = ⟨ˆC1, ..., ˆCk⟩along with Cabs\ns\nis considered a\nvalid explanation for relative suboptimality of the foil (denoted as Cabs\nS (Cπf , πf) > C(I, π)),\n\n112\nExplaining in the Presence of Vocabulary Mismatch\nif ∀ˆCi ∈Cπf , ˆCi is a subset of concepts presents in the corresponding state (where state\nis I for i = 1 and T(I, ⟨a1, ..., ai−1⟩) for i > 1). and Σi={1..k}Cabs\nS (ˆCi, ai) > C(I, π)\nIn the earlier example, the explanation would include the fact that executing the action\nattack in the presence of the concept skull-on-left, will cost at least 500 (as opposed\nto original plan cost of 20).\n8.4.2\nIdentifying Explanations through Sample-Based Trials\nFor identifying the model parts for explanatory query, we can rely on the agent’s ability\nto interact with the simulator to build estimates. Given the fact that we can separate the\ntwo cases at the simulator level, we will keep the discussion of identifying each explanation\ntype separate and only focus on identifying the model parts once we know the failure type.\n8.4.2.1\nIdentifying failing precondition:\nTo identify the missing preconditions, we can rely on the simple intuition that while\nsuccessful execution of an action a in the state sj with a concept Ci doesn’t necessarily\nestablish that Ci is a precondition, we can guarantee that any concept false in that state\ncannot be a precondition of that action. This is a common line of reasoning exploited by\nmany of the model learning methods. So we start with the set of concepts that are absent\nin the the state (sfail) where the failing action (afail) was executed, i.e., poss_pre_set\n= C \\ C(sfail).\nWe then randomly sample for states where afail is executable.\nEach\nnew sampled state si where the action is executable can then be used to update the\npossible precondition set as poss_pre_set = poss_pre_set ∩C(si). That is, if a state\nis identified where the action is executable but a concept is absent then it can’t be part\nof the precondition. We will keep repeating this sampling step until the sampling budget\nis exhausted or if one of the following exit conditions is met. (a) In cases where we are\nguaranteed that the concept list is exhaustive, we can quit as soon as the set of possibilities\nreduce to one (since there has to be a missing precondition at the failure state). (b) The\nsearch results in an empty list. The list of concepts left at the end of exhausting the\nsampling budget represents the most likely candidates for preconditions. An empty list\nhere signifies the fact that whatever concept is required to differentiate the failure state\nfrom the executable one is not present in the initial concept list C. This can be taken as\nevidence to query the user for more task-related concepts.\nIdentifying cost function:\nNow we will employ a similar sampling based method to\nidentify the right cost function abstraction. Unlike the precondition failure case, there is\nno single action we can choose but rather we need to choose a level of abstraction for each\naction in the foil (though it may be possible in many cases to explain the suboptimality\nof foil by only referrring to a subset of actions in the foil). Our approach here would be\nto find the most abstract representation of the cost function at each step such that of\nthe total cost of the foil becomes greater than that of the specified plan. Thus for a foil\nπf = ⟨a1, ..., ak⟩our objective become\nminˆC1,...,ˆCkΣi=1..k∥ˆCi∥subject to Cabs\ns\n(Cπf , πf) > C(I, π)\n\n8.5 Explanation Confidence\n113\nFor any given ˆCi, Cabs\ns\n(ˆCi, ai) can be approximated by sampling states randomly and\nfinding the minimum cost of executing the action ai in states containing the concepts ˆCi.\nWe can again rely on a sampling budget to decide how many samples to check and enforce\nrequired locality within sampler.\n8.5\nExplanation Confidence\nThe methods discussed above (both for learning full model and model component) are\nguaranteed to identify the exact model in the limit, the accuracy of the methods is still\nlimited by practical sampling budgets we can employ. So this means it is important that we\nare able to establish some level of confidence in the solutions identified. In case of learning\nfull model, there are some loose PAC learning guarantees we could employ, but for learning\nmodel components we will strive for a more accurate measure. To assess confidence, we will\nfollow the probabilistic relationship between the random variables as captured by Figure\n8.2 (A) for precondition identification and Figure 8.2 (B) for cost calculation. Where the\nvarious random variables captures the following facts: Os\na - indicates that action a can\nbe executed in state s, ci ∈pa - concept ci is a precondition of a, Os\nci - the concept ci is\npresent in state s, Cabs\ns\n({ci}, a) ≥k - the abstract cost function is guaranteed to be higher\nthan or equal to k and finally OC(s,a)≥k - stands for the fact that the action execution in\nthe state resulted in cost higher than or equal to k. We will allow for inference over these\nmodels, by relying on the following simplifying assumptions - (1) the distribution of all\nnon-precondition concepts in states where the action is executable is the same as their\noverall distribution across the problem states (which can be empirically estimated), (3)\ncost distribution of an action over states corresponding to a concept that does not affect\nthe cost function is identical to the overall distribution of cost for the action (which can\nagain be empirically estimated).\nThe first assumption implies that you are as likely to see a non-precondition concept in\na sampled state where the action is executable as the concept was likely to appear at any\nsampled state with the same set of concepts (this distribution is denoted as P(Os\nci|OC\\cs\ni ),\nwhere OC\\cs\ni represents the other observed concepts). While the second one implies that\nfor a concept that has no bearing on the cost function for an action, the likelihood that\nexecuting the action in a state where the concept is present will result in a cost greater\nthan k will be the same as that of the action execution resulting in cost greater than k\nfor a randomly sampled state (pC(.,a)≥k).\nFor a single sample, the posterior probability of explanations for each case can be ex-\npressed as follows: for precondition estimation, updated posterior probability for a positive\nobservation can be computed as P(ci ∈pa|Os\nci ∧OC\\cs\ni ∧Os\na) = (1−P(ci ̸∈pa|Os\nci ∧OC\\cs\ni ∧Os\na)),\nwhere\nP(ci ̸∈pa|Os\nci ∧Os\na) =\nP(Os\nci|OC\\cs\ni ) × P(ci ̸∈pa)\nP(Osci|Osa)\n\n114\nExplaining in the Presence of Vocabulary Mismatch\nFig. 8.2: A simplified probabilistic graphical models for explanation inference, Subfigure (A) and\n(B) assumes classifiers to be completely correct, while (C) and (D) presents cases with noisy\nclassifier.\nand for the case of cost function approximation\nP(Cabs\ns\n({ci}, a) ≥k|Os\nci ∧OC\\cs\ni ∧OC(s,a)≥k) =\nP(Cabs\ns\n({ci}, a) ≥k)\nP(Cabs\ns\n({ci}, a) ≥k)) + pC(.,a)≥k ∗P(¬Cabs\ns\n({ci}, a) ≥k))\nThe distribution used in the cost explanation, can either be limited to distribution over\nstates where action ai is executable or allow for the cost of executing an action in a state\nwhere it is not executable to be infinite. The full derivation of these formulas can be found\nin the paper Sreedharan et al. [2020c].\n8.6\nHandling Uncertainty in Concept Mapping\nGiven how unlikely it is to have access to a perfect classifier for any concept, a more\npractical assumption to adopt could be that we have access to a noisy classifier. However,\nwe assume that we also have access to a probabilistic model for its prediction.\nThat\nis, we have access to a function PC : C →[0, 1] that gives the probability that the\nconcept predicted by the classifier is actually associated with the state. Such probability\nfunctions could be learned from the test set used for learning the classifier. Allowing for the\npossibility of noisy observation generally has a more significant impact on the precondition\ncalculation than the cost function approximation. Since we are relying on just generating\na lower bound for the cost function, we can be on the safer side by under-approximating\nthe cost observations received (though this could lead to larger than required explanation).\nIn the case of precondition estimation, we can no longer use a single failure (execution of\nan action in a state where the concept is absent) as evidence for discarding the concept.\nThough we can still use it as an evidence to update the probability of the given concept\nbeing a precondition. We can remove a particular possible precondition from consideration\nonce the probability of it not being a precondition crosses a specified threshold.\nTo see how we can incorporate these probabilistic observations into our confidence\ncalculation, consider the updated relationships presented in Figure 8.2 (C) and (D) for\n\n8.7 Acquiring New Vocabulary\n115\nprecondition and cost function approximation. Note that in previous sections, we made\nno distinction between the concept being part of the state and actually observing the\nconcept. Now we will differentiate between the classifier saying that a concept is present\n(Os\nci) from the fact that the concept is part of the state (ci ∈C(S)). Now we can use this\nupdated model for calculating the confidence. We can update the posterior of a concept\nnot being a precondition given a negative observation (Os\n¬ci) using the formula\nP(ci ̸∈pa|Os\n¬ci ∧Os\na ∧OC\\cs\ni ) =\nP(Os\n¬ci|ci ̸∈pa ∧Os\na ∧OC\\cs\ni ) ∗P(ci ̸∈pa)\nP(O¬ci|Osa)\nSimilarly we can modify the update for a positive observation to include the observation\nmodel and also do the same for the cost explanation. For calculation of cost confidence,\nwe will now need to calculate P(OC(s,a)≥k|ci ̸∈C(s), Cabs\ns\n({ci}, a) ≥k). This can either be\nempirically calculated from samples with true label or we can assume that this value is\ngoing to be approximately equal to the overall distribution of the cost for the action.\n8.7\nAcquiring New Vocabulary\nA core capability of the method discussed in the earlier sections is the ability to detect\nscenarios where the user specified concept list may not suffice to represent the required\nmodel component. This is important as it is unlikely that the user would always be able to\nprovide the required concepts, either because they are unaware of it or they may just have\noverlooked some of the concepts when coming up with the list provided to the system. The\nformer case could happen when the user is not an expert or if the simulator is modeling\nnovel phenomena and is itself learned from experience. Systems capable of handling such\nscenarios would require the capability of teaching the user new concepts and even coming\nwith intuitive labels for such concepts. We are unaware any methods that can handle this\nin a general enough manner.\nHandling the latter case would generally be easier, since now the system would only\nneed to query the human for a concept they may know but did not specify. One possible\nway such concepts can be queried is by presenting state pairs and asking the user to\nprovide concepts absent in one but present in another. To see how we could perform such\nqueries , consider the case of identifying failing preconditions. Let us denote the state\nwhere the foil fails as sf. Now once the system has established that the current set of\nconcepts cannot reliably explain the failing state, we can sample one of the states where\naction succeeds (s′ and we will refer to such states as positive states) and present the\ntwo states to the user and ask them to specify a concept that is absent in state sf but\nis present in s′. Now the system can keep showing more and more positive states and\nconfirm that identified concept is still present in the state. The process can continue till\neither the system has enough samples to reliably establish the confidence of the fact that\nthe concept is a precondition or the user notes a state where the concept is absent. If the\nlatter happens you can ask the user to pick another concept that is absent in the original\nfailing state but present in all the positive states. We can try to reduce cognitive load on\nthe user’s end on choosing the right concept by sampling positive states that are similar\nto sf (which would generally mean less number of conceptual difference). Moreover, in\ncases where the user is being presented a visual representation of the state, then the\nsystem could also provide saliency maps showing areas in the state that are important\nto the decision maker (which should generally cover patches on the image corresponding\n\n116\nExplaining in the Presence of Vocabulary Mismatch\nto precondition concept). Though this would mean restricting positive states to the ones\nwhere the decision maker would have used the failing action.\n8.8\nBibliographic Remarks\nMany of the specific methods discussed in the chapter were first published in Sreedharan\net al. [2020c]. As mentioned in the chapter many of the methods also have parallels in\nexplainable Machine Learning. For example, Ribeiro et al. [2016] is a popular explanation\nmethod for classifiers that relies on local approximation of models and Kim et al. [2018]\ngenerates concept based explanations, where individual concepts are captured through\nclassifiers. Concept as classifier has also been utilized by Hayes and Shah [2017] to provide\npolicy summaries and Waa et al. [2018] to provide contrastive explanations for MDPs.\nThough in the case of Waa et al. [2018], they require designer to also assign positive\nand negative outcomes to each action that is used to explain why one policy is preferred\nover another and their method also do not really allow for explanation of infeasible foils.\nOutside of explanations, the idea of using classifiers to capture state fluents have also\nbeen utilized by Konidaris et al. [2018] to learn post-hoc symbolic models from low-level\ncontinuous models.\nOne possibility alluded to in the chapter though not discussed in\ndetails is that of using the methods to establish the cost of the plan itself. As one can guess\nhere the process would be nearly identical to that for establishing the cost of the foil, but\ninstead of trying to find max cost bounds, we will be looking for min ones. Also the utility\nof establishing a representation of the agent in symbolic terms the user can understand\ngoes beyond just providing helpful explanation. The paper by Kambhampati et al. [2021]\nmakes a case to develop a symbolic middle layer expressed in human-understandable terms\nas a basis for all human-AI interaction.\n\nChapter 9\nObfuscatory Behavior and Deceptive\nCommunication\nIn this chapter, we will focus the discussion on some of the behavioral and communication\nstrategies that a robot can employ in adversarial environments. So far in this book, we have\nlooked at how the robot can be interpretable to the human in the loop while it is interacting\nwith her either through its behavior or through explicit communication. However, in the\nreal world not all of the robot’s interactions may be of purely cooperative nature. The\nrobot may come across entities of adversarial nature while it is completing its tasks in the\nenvironment. In such cases, the robot may have some secondary objectives like privacy\npreservation, minimizing information leakage, etc. in addition to its primary objective of\nachieving the task. Further, in adversarial settings, it is not only essential to minimize\ninformation leakage but also to ensure that this process of minimizing information leakage\nis secure. Since, an adversarial observer may use diagnosis to infer the internal information\nand then use that information to interfere with the robot’s objectives.\nTo prevent leakage of sensitive information, the robot should be capable of generating\nbehaviors that can obfuscate the sensitive information about its goals or plans. However,\nthe execution of such obfuscating behaviors may be expensive and in such cases it may\nbe essential to balance the amount of obfuscation needed with the resources available.\nFurther, there may be complex real-world settings that involve both cooperative as well\nas adversarial observers. In such mixed settings, the robot has to balance the amount\nof obfuscation desired for adversarial entities with the amount of legibility required for\nthe cooperative entities. Additionally, in this chapter we will look at how communication\nand the model-reconciliation techniques could be used for deception, in particularly to\ngenerate lies. As we will see, white lies could even be used to improve the overall team\nperformance, although the ethics of such an undertaking has to be carefully considered.\nNow let’s look at different types of adversarial behaviors and communication strategies\nthat are available to the robot.\n9.1\nObfuscation\nA robot can hide its true objective from an adversarial observer by making several objec-\ntives seem plausible at the same time. Thus the true objective stays obfuscated from the\nobserver. The robot can choose to obfuscate its activities as well apart from its objectives.\nIn this section, we will see different types of obfuscation approaches that a robot can\nemploy to hide sensitive information in adversarial situations.\n117\n\n118\nObfuscatory Behavior and Deceptive Communication\nFig. 9.1: Illustration of impact of goal obfuscation and secure goal obfuscation on human’s mental model.\n9.1.1\nGoal Obfuscation\nThe problem of goal obfuscation arises when the robot wants to obfuscate its true goal\nfrom the adversary. This is possible in settings where the adversarial entity is unaware of\nthe robot’s true goal but is aware of the possible goals that the robot may try to achieve\nin the given domain. In this book, we will mostly focus on scenarios where the adversary\nis a human, but the methods we talk about could also be applied in scenarios where we\nhave another autonomous agent as our adversary. This problem setting is also part of the\ncontrolled observability planning problem, introduced in Chapter 4. In fact, it is similar\nto the setting seen in the offline goal legibility problem. However, here because of the\nexistence of an adversarial observer, instead of being legible with respect to its true goal,\nthe robot obfuscates its true goal. In order to obfuscate effectively, the robot needs to have\naccess to the adversaries models, incluging both their belief about the robot model (MR\nh )\nand the observer’s sensor model (O). As in the case of Chapter 4, the adversaries may\nnot be able to differentiate between certain states/actions leading to partial observability.\nThe adversary may use these observations resulting from the robot’s behavior to infer\nsome sensitive information about the robot’s goals. Depending on the granularity of the\nobservations available to the adversary, the problem of goal obfuscation becomes easy\nor complex. That is with fine-grained observations, goal obfuscation might be harder to\nachieve or may even be infeasible in some cases, whereas with coarse-grained observations,\nit might be easier for the robot to obfuscate its true goal. Further, the adversary may\nbe able to use the information gleaned from observations to interfere with or hamper\nthe robot’s activities. Therefore, in this setting, the robot is tasked with the additional\nobjective of preventing the adversary from learning sensitive information about its goal.\nSimilar to Chapter 4, we will focus on scenarios where the adversaries expectations about\nthe robot model aligns with the robot’s true model, i.e., MR\nh is the same as MR.\nDefinition 38. A goal obfuscation planning problem, is a controlled observability\nplanning problem, where, G = {GR ∪G1 ∪. . . ∪Gn−1}, is the set of n goals where GR is\nthe true goal of the robot, and G1, . . . , Gn−1 are the decoy goals.\nA solution to a goal obfuscation planning problem is a k-ambiguous plan. Recall that,\nsince the observer receives observations of the robot’s behavior, each plan is associated\nwith a corresponding observations sequence. Therefore, the objective of a k-ambiguous\nplan is to make the observation sequence consistent with at least k goals, out of which\n\n9.1 Obfuscation\n119\nk −1 are decoy goals, such that, k ≤n. This is done by taking actions towards robot’s\ntrue goal, such that, the corresponding observation sequence exploits the observer’s belief\nspace in order to be consistent with multiple goals. That is, a k-ambiguous plan produces\na final belief state consistent with its observation sequence where at least k goals are\npossible. The k −1 decoy goals can be chosen specifically to maximize the obfuscation.\nFigure 9.1 illustrates goal obfuscation for k = 2 in the middle image. Here the true goal\nof the robot to pickup the medkit marked in the leftmost image. The adversarial observer\nhas partial observability of the direction the robot moves in (say, the observer receives\ncoarse observations from the GPS sensor). In order to obfuscate its true goal from the\nadversarial observer, the robot performs a 4-step k-ambiguous plan (for k=2), as shown\nin the middle image.\nTo define a k-ambiguous plan, we use the notation for belief state, introduced in\nChapter 4.\nDefinition 39. A plan, πk, is a k-ambiguous plan, if δ(I, πk) |= GR and the final\nbelief state, bn ∈BS(πk, I), satisfies the following, |G ∈G : ∃s ∈bn, s |= G| ⩾k, where\n1 ⩽k ⩽n.\nDefinition 40. An observation sequence Ok = ⟨o1, . . . , on⟩is k-ambiguous observation\nsequence if it is an observation sequence emitted by a k-ambiguous plan.\nThe underlying algorithm for the synthesis of the solution is the same as the one\ndescribed for the offline legibility planning problem. However, the goal test and heuristic\nfunction are customized to ensure that the goal obfuscation constraints are met.\n9.1.1.1\nGoal test\nThe goal test ensures that the solution plan does not end unless all k goals occur in the\nobserver’s belief state. Specifically, it validates whether the true goal has been achieved\nin the robot’s true state and also whether the k −1 goals have been achieved in the belief\nstate. If there doesn’t exist a plan that can achieve the true goal with the given k goals,\nwe restart the search with next combination of k goals (inclusive of true goal) among the\ntotal n candidate goals. In the worst case, the value of k can be reduced until a solution\nis found.\n9.1.1.2\nHeuristic function\nHere the heuristic strategy is to generate a plan where the observer’s final belief state\nsatisfies k goals, making it a k-ambiguous plan. We use two heuristic functions to estimate\na node’s cost: h(s) = hGA(s) + hGk−1(b), where the first part computes the heuristic cost\nto the true goal from the robot’s actual state, and the second part computes the heuristic\ncost to k −1 goals from the belief induced by the emitted observation. This is computed\nby finding the max of heuristic cost to each of the k −1 goals. The heuristic value of a\nstate is computed as the total of these two values. Thus the heuristic helps to guide the\nsearch towards at least k goals in the observer’s belief.\n\n120\nObfuscatory Behavior and Deceptive Communication\n9.1.2\nSecure Goal Obfuscation\nThe above approach to synthesizing goal obfuscating behavior is not secure. In the sense\nthat it reveals information when the following assumptions are not satisfied: (1) adversary\nhas access to the algorithm and has the means to simulate the algorithm for a given input.\nThis means the adversary could identify the potential states generated by the algorithm.\n(2) the adversary can run the algorithm with different inputs. For example, the adversary\ncan set different goals as true goal to check the variability of the output. Figure 9.1,\nrightmost subfigure illustrates how an adversary can rerun the algorithm with different\ninput and glean information about the true goal. By setting the true goal to some other\ncandidate goal, the adversary can run the algorithm and find out the robot’s true goal\nwhich gave the original observation sequence.\nOne way to synthesize a secure goal obfuscating behavior would be to introduce some\nrandomization in the solution synthesis process, that would prevent leakage of additional\ninformation. To that end, one could randomly choose a set of k goals (inclusive of true goal)\nfrom n candidate goals. Then randomly pick a goal from this set as a “true goal” and run\nthe algorithm. Let’s call this goal as the decoy true goal. We have to maintain the plans to\neach of the k goals that are consistent with the observation sequence (which includes the\nrobot goal GR). Then by performing a post processing step, we can retrieve the plan that\nreaches the robot’s true goal and is consistent with the observation sequence produced\nby the plan to the decoy true goal. This process ensures that even if the adversary runs\nthe algorithm again with either the same or different input (different goal as true goal, or\ndifferent combination of k goals), the identity of the true goal remains k-ambiguous even\nwith the observer being aware of the randomization. Note that unlike the other sections\nhere we are not just considering an observer who is reasoning on their own based on the\nobservations, but rather we are considering a sophisticated adversary who is able to use\nmore sophisticated computing resources to simulate the algorithm.\nIn this setting, a perfectly rational adversary would be one that has unbounded com-\nputational resources, and can use them to make perfect inferences. In other words, that is\nthe worst case scenario for this algorithm. The following proposition shows that our algo-\nrithm provides k-ambiguous guarantee even in the worst case, such that a perfectly rational\nadversary remains uncertain about the true goal, provided the goals are independent.\nProposition 10. Let H be a perfectly rational adversary. Given a goal obfuscation plan-\nning problem for the robot R. Plans computed using Algorithm 3 with k-ambiguous goal\ntest and heuristic function are such that, H will be able to infer the true goal with proba-\nbility ⩽1/k provided the goals are independent.\nLet’s say goal Gi from the set Gk is chosen randomly to be the decoy true goal.\nThe observation sequence Oi is obtained by running the algorithm with GA = Gi. The\nadversary can run the algorithm with each goal from Gk to search for Gi. But Gi can only\nbe the robot’s true goal with probability 1/k. Therefore the adversary can detect the true\ngoal with probability ⩽1/k.\nNote that the goals can be specified using sub-formulae. This makes the input specifi-\ncation much smaller depending on how many states are in the domain that are consistent\n\n9.1 Obfuscation\n121\nFig. 9.2: Illustration of the impact of plan obfuscation on human’s mental model.\nwith the sub-formulae. In the least constrained domain, that may be exponential in the\nnumber of propositions not used.\n9.1.3\nPlan Obfuscation\nIn this problem setting, the adversarial observer is aware of the robot’s goal. The robot,\nhowever, obfuscates its activities towards its goal thereby hiding sensitive information\nabout its activities. We refer to this behavior as plan obfuscatory behavior. The robot\ncan achieve a plan obfuscatory behavior by computing a plan whose observation sequence\nconforms to a set of plans that are diverse in terms of their actions, thereby making it hard\nfor the adversarial observer to guess the actions executed by the robot. For instance, in\nFigure 9.2, the goal of the robot is to pick up a medkit and treat the victim on the disaster\nsite. It has two choices of medkits to pick from. If it chooses the medkit as shown in the\nrightmost sub-figure, it can maximize the diversity with respect to its actions. Here, the\nmultiple different paths to the medkit produce the same observation sequence (due to the\nunderlying coarse GPS sensor model that the observer is using), thus hiding the robot’s\ntrue actions from the adversarial observer.\nDefinition 41. A plan obfuscation planning problem is a controlled observability\nplanning problem, where, |G| = 1, and therefore the true goal is known to the observer.\nThe solution to a plan obfuscation problem is an ℓ-diverse plan. An ℓ-diverse plan is\na plan whose observation sequence is consistent with at least ℓdiverse plans to the goal,\nsuch that, the diverse plans are at least d distance away from each other. In order to\ncompute an ℓ-diverse plan, we need to keep track of the plans that are consistent with the\nobservation sequence and reach the goal. To compute the diversity between all the pairs\nof plans consistent with the observation sequence, a plan distance measure like action\ndistance, causal link distance, state sequence distance (introduced in Chapter 3) can be\nused. In general, the approach can use any valid plan distance. We now define an ℓ-diverse\nplan.\nWe refer to the set of plans that are consistent with the belief sequence of a given plan,\nas a belief plan set (BPS), introduced in Chapter 4.\n\n122\nObfuscatory Behavior and Deceptive Communication\nDefinition 42. A BPS induced by plan p starting at s0 is minimally d-distant, dmin(BPS(p, s0)),\nif d =\nmin\np1,p2∈BPS(p,s0) D(p1, p2).\nDefinition 43. A plan, πl, is an ℓ-diverse plan, if for a given value of d and distance\nfunction δ, dmin(BPS(πl, I)) ≥d, |BPS(πl, I)| ≥ℓ, where ℓ≥2 and every plan in\nBPS(πl, I) achieves the goal in G.\nWe can again leverage Algorithm 3 to generate the plans for obfuscation problem. The\ngoal test and heuristic function are customized to ensure that there are at least ℓdiverse\nplans to the true goal that are consistent with the observation sequence and the minimum\ndistance between these plans is at least d.\n9.1.3.1\nGoal test\nTo ensure the plans in BPS, induced by an ℓ-diverse plan, can achieve the goal in G, we\ncan check whether at least ℓplans are reaching the goal or not and whether the minimum\ndistance between plans in BPS is at least d. Also in order to ensure termination of the\nalgorithm, there is a cost-bound given as input to the algorithm.\n9.1.3.2\nHeuristic function\nApart from minimizing the heuristic cost to the goal, the customized heuristic given below\nalso maximizes the d of dmin(BPS(p, s0)) induced by plan p starting at s0. This increases\nthe minimum distance between the plan pairs. This distance is computed using a plan\ndistance measure.\nh(s) = hGA(s) −dmin(BPS(p, s0))\n(9.1)\n9.1.4\nDeception\nIn addition to obfuscation, the robot can also employ deceptive behaviors to hide sensitive\ninformation.\nWhile the concept of obfuscation deals with the notion of confusing the\nadversary with several decoy candidates, the concept of deception involves making one\nof the decoy candidates more likely than the robot’s true objective/activities.\nThus,\nin order to deceive an adversarial observer, it is crucial to have access to their goal or\nplan recognition models. By incorporating the human’s reasoning ability (in regards to\nidentifying the goal or the plan), the robot can synthesize behavior that deceives the\nadversarial observer into believing that the decoy candidate is the true candidate. In the\nprior literature, synthesis of deceptive behaviors with respect to robot’s goals has been\nstudied in path planning scenarios where the observer has full observability of robot’s\nactivities. In order to successfully deceive an adversarial observer, the robot’s plan has to\nend when a decoy goal is achieved. However, in reality, the robot has a primary objective\nof achieving its true goal. Therefore, in cases where the observer has full observability\nof the robot’s activities, deceptive behavior may be hard to synthesize. To that end, the\n\n9.2 Multi-Observer Simultaneous obfuscation and legibility\n123\nnotion of the radius of maximum probability with respect to a goal has been studied in\nthe literature on deceptive planning. This is a radius around a goal location, within which\nthat goal itself becomes the most likely goal. So deception can be maintained until the\nrobot reaches the area within the radius of maximum probability for its true goal.\n9.2\nMulti-Observer Simultaneous obfuscation and legibility\nObfuscatory strategies are also crucial in scenarios where there are multiple different types\nof observers, such that some are of adversarial nature while some others are of cooperative\nnature. In such cases, the robot has to ensure that its behavior is simultaneously legible\nto cooperative observers and obfuscatory to adversarial ones.\nFor instance, in soccer,\na player may perform feinting trick to confuse an opponent while signaling a teammate.\nSynthesizing a single behavior that is legible and obfuscatory to different observers presents\nsignificant technical challenges. In particular, the robot may have to be deliberately less\nlegible to its friends so that it can be effectively more obfuscatory to its adversaries. This\nproblem gives rise to a novel optimization space that involves trading-off the amount of\nobfuscation desired for adversaries with the amount of legibility desired for friends.\n9.2.1\nMixed-Observer Controlled Observability Planning Problem\nWe now discuss a problem framework called mixed-observer controlled observability plan-\nning problem, MO-COPP, that allows a robot to simultaneously control information\nyielded to both cooperative and adversarial observers while achieving its goal.\nThis\nframework models and exploits situations where different observers have differing sensing\ncapabilities, which result in different “perceptions” of the same information. Typically, dif-\nferent observers in an environment may have different “sensors” (perception capabilities)\ndue to differences in prior communication, background knowledge, and innate differences\nin sensing capabilities.\n9.2.1.1\nExample\nConsider the example in Figure 9.3, where an automated truck has to deliver two packages.\nAs shown in Figure 9.3a, there are two packages at factory A and two at factory B. Let\nthere be two observers who know the initial inventory at each factory, but do not know\nwhich two packages will be delivered by the truck. Each observer has different sensors (or\nsources of information) about the truck’s activities. Observer-C (cooperative observer) has\naccess to a database of barcodes which allows it to determine the identity of the factory\nthat the package was loaded from (but not the identity of the individual packages), whereas\nobserver-X (adversarial observer) does not have any additional information and cannot\ndifferentiate between any of the packages. The truck wants to convey its true goal to\nobserver-C but obfuscate it to observer-X. When the truck loads one package each from A\nand B (Figure 9.3b), observer-C cannot distinguish the packages. However, when it loads\nboth the packages from A (Figure 9.3c), observer-C can infer the identity of the loaded\npackages, while observer-X still does not know the identity of the packages. Therefore,\n\n124\nObfuscatory Behavior and Deceptive Communication\n(a) The robot’s goal is to deliver two packages to the delivery area:\n(b) Plan-1 - Robot delivers 1 package from factory A and 1 from B:\n(c) Plan-2 - Robot delivers both the packages from factory A, helping observer-C and thwarting observer-X:\nFig. 9.3: Different belief updates induced by the same plan for two observers; observer-C is cooperative,\nobserver-X is adversarial.\nplan-2 makes the true goal legible to observer-C while obfuscating the true goal with all\nthe possible goals for observer-X.\nMO-COPP setting involves an actor (R) (i.e., the robot) and two observers, where one\nis adversarial observer (X) while the other is cooperative (C). This framework assumes\nthat the robot has full observability of its own activities and knows the sensor models used\nby the observers, and that the observers have different sensor models. When the robot\ntakes an action and reaches a new state, each observer receives an observation. After\nobtaining the observations, each observer updates its belief, similar to the belief update\nin COPP formulation. The robot leverages the known limits in the observers’ sensors to\ncontrol the observability of multiple observers in the environment simultaneously. Given\na set of candidate goals, the objective of the robot is to convey information about its goal\nto the cooperative observer and to hide it from the adversarial observer. This involves\nincreasing the number of candidate goals possible in the adversary’s belief, while decreasing\nthe number of candidate goals possible in cooperative observer’s belief.\nDefinition 44. A mixed-observer controlled observability planning problem is a\ntuple,\n\n9.2 Multi-Observer Simultaneous obfuscation and legibility\n125\nMO −COPP = ⟨Λ, MR, G, {Ωi}i∈Λ, {Oi}i∈Λ, {Bi\n0}i∈{X,C}⟩, where,\n• Λ = {R, C, X} is the set of agents.\n• MR = ⟨F R, AR, IR, GR⟩is the robot model Also, for a ∈A, pre(a), adds(a), dels(a)\nare each a subset of fluents representing precondition, add effects and delete effects\nof a.\n• G = {G1, G2, . . . , Gn−1, GR} is the set of candidate goals, where GR is the true goal\nof R, which is not known to both C and X.\n• Ωi is the set of observation symbols for agent i, which are emitted when R takes an\naction and reaches a new state. Further, ΩR = {oR\na,s|a ∈A, s ∈S}.\n• Oi : A×S →Ωi is agent i’s deterministic sensor model. S is the set of states, where\neach state is instantiation of all fluents. Further, OR maps each action-state pair to\na unique observation, ∀a, a′ ∈A, s, s′ ∈S, a ̸= a′ ∧s ̸= s′ : OR(a, s) ̸= OR(a′, s′),\nwhile OX and OC are noisy sensor models that map multiple action-state pairs to\nthe same observation symbol.\n• Bi\n0 is the initial belief of an observer, i ∈{X, C}. The initial belief is a set of states\ninclusive of I.\nEvery time the robot acts, each i ∈Λ receives an observation consistent with its sensor\nmodel. The sensor model of an observer i ∈{X, C} supports many-to-one mapping of\n⟨a, s⟩pairs to observation symbols, i.e., ∃a, a′ ∈A, s, s′ ∈S, a ̸= a′ ∧s ̸= s′ : Oi(a, s) =\nOi(a′, s′). For an agent i, the inverse of sensor model gives the set of ⟨a, s⟩pairs consistent\nwith an observation symbol oi ∈Ωi, i.e., O−1\ni\n(oi) = {⟨a, s⟩|∀a ∈A, s ∈S, Oi(a, s) = oi}.\nAgain, we assume the observer is aware of the true robot model.\nEach observer i ∈{X, C} maintains its own belief, which is a set of states. δ(·) is a\ntransition function, such that, δ(s, a) = ⊥if s ̸|= pre(a); else δ(s, a) = s\\dels(a)∪adds(a).\nNow we can define the belief update: (1) at time step t = 0, the initial belief of observer i is\ngiven by Bi\n0, (2) at time step t ∈{1, . . . , T }, Bi\nt = {ˆs | ∃ˆa, ¯s δ(¯s, ˆa) = ˆs; ¯s ∈Bi\nt−1; Oi(ˆa, ˆs) =\noi\nt}, where T is the time horizon. That is, the belief is updated using the previous belief\nand the observation received.\nA plan π (or sequence of actions) is associated with a\nsequence of observations, ObsSeqi(π) = {oi\n1, . . . , oi\nT } for each observer i. The belief of R\nis a single state due to full observability.\n9.2.1.2\nMO-COPP Solutions\nA plan π solves a MO−COPP = ⟨Λ, MR, G, {Ωi}i∈Λ, {Oi}i∈Λ, {Bi\n0}i∈{X,C}⟩, iff: δ(I, π) |=\nGA. In other words, any solution to MR is a solution to MO-COPP. We measure the\nquality of a MO-COPP solution in terms of the extent to which the plan is legible to C and\nobfuscating to X. Given a solution plan, π, let Gπ\nX = {G ∈G : ∃s ∈BX\nT , s |= G}, denote\nthe goals present in X’s final belief and Gπ\nC = {G ∈G : ∃s ∈BC\nT , s |= G}, denote the\ngoals present in C’s final belief. Also, we can define the inverse of its observation sequence\nfor observer i, as follows: ∀i ∈{X, C}, ObsSeq−1\ni\n(π) = {ˆπ|∃si\n0 ∈Bi\n0, G ∈Gπ\ni , δ(si\n0, ˆπ) |=\nG, ObsSeqi(π) = ObsSeqi(ˆπ)}.\n\n126\nObfuscatory Behavior and Deceptive Communication\nHere |Gπ\nX| represents the amount of goal obfuscation for X, and by increasing the\nnumber of goals in Gπ\nX, we can increase the goal obfuscation. Similarly, |Gπ\nC| represents\nthe amount of goal legibility for C, and by decreasing the number of goals in Gπ\nC, we\ncan increase the goal legibility. We now introduce a goal difference metric to estimate the\nquality of a solution plan to MO-COPP.\nDefinition 45. Given a solution plan π that solves MO-COPP, where |G| = n, the goal\ndifference, of π is given by: GD(π) = |Gπ\nX|−|Gπ\nC|\nn−1\nwhere the denominator represents the\ndifference between ideal values of Gπ\nX and Gπ\nC.\nAn optimal solution to MO-COPP maximizes the trade-off between amount of goal\nobfuscation and goal legibility. That is, it maximizes the difference between the number of\ngoals in Gπ\nX and Gπ\nC. Equivalently, closer the GD(π) value to 1, better is the plan quality.\nA solution plan with GD(π) = 1 is an optimal plan. E.g., in Figure 9.3c, plan-2 is an\noptimal plan with GD(plan-2) = 6−1\n6−1 = 1. The denominator is essential for comparing\nthe quality of plans across different problems, with varying number of candidate goals.\nProposition 11. Given a solution plan, π, to MO-COPP, if |Gπ\nC| = 1, then GR ∈Gπ\nC.\nThe above proposition states that when maximum goal legibility is achieved, only one\ngoal GA is present in C’s final belief. This is because I ∈BC\n0 , and while performing\nbelief update at t, ⟨at, st⟩∈O−1\nC (oC\nt ). That is the true state is part of each belief update.\nTherefore, GR ∈Gπ\nC.\n9.2.2\nPlan Computation\nWe now present two solution approaches to solve MO-COPP. In the first approach, MO-\nCOPP can be formulated as a constraint optimization problem and provide an IP encoding\nto solve it in T steps. The IP encoding provides an optimal solution for the given horizon\nby maximizing the trade-off between the amount of obfuscation and legibility. The second\napproach builds up on the search algorithm presented in Chapter 4 to solve COPP prob-\nlems. The search algorithm generates solutions that satisfy a prespecified lower bound on\nthe amount of goal obfuscation and goal legibility.\n9.2.2.1\nMO-COPP as Integer Program\nThe IP encoding includes variables for an action being executed at a time, each state being\ntrue at a time, for a specific observation to emitted at a give time and for specific state\nand goal being in the belief state. The objective function is essentially the numerator of\nGD(·) metric, i.e.,\nmax\nX\nG∈G\ngX\nG,T −\nX\nG∈G\ngC\nG,T\n(9.2)\n\n9.2 Multi-Observer Simultaneous obfuscation and legibility\n127\nWhere gi\nG,T is an indicator variable capturing the fact that goal G is present in the\nbelief state for the agent i at the time step T . The denominator of the GD metric is\nskipped, as it is a constant and does not contribute to the optimization. Maximizing for\nthis objective provides a single solution that achieves the maximum difference between\nthe number of goals possible for the two observers. One could also formalize the problem\nas a multi-objective optimization problem, and look at various pareto optimal solutions\nthat trade off the various goals that each observer believes will be achieved.\nThe constraints provided to the problem, captures the exact evolution of the state\nand belief (for each agent) given the robot action, the observation generated by them and\nthe fact that the robot needs to achieve its goal within the specified horizon limit. The\nexact IP encoding can be found in the paper Kulkarni et al. [2020b]. Also note that, the\ncurrent objective function trades off goal obfuscation with goal legibility for the observers.\nHowever, the robot can ensure a predefined level of goal obfuscation (say obfuscate with\nat least k candidate goals) by adding an additional constraint that enforces a bound for\ngoal obfuscation and goal legibility.\n9.2.2.2\nSearch Algorithm\nIt is also possible to leverage search techniques that address goal obfuscation and goal\nlegibility in isolation to solve MO-COPP. The search algorithm in Chapter 4 is adapted\nto address goal obfuscation and goal legibility simultaneously to two different observers.\nThe bounds on the amount of goal obfuscation and goal legibility desired can be specified,\nsimilar to the ones seen in the IP: obfuscate with at least k goals, make it legible with at\nmost j goals. These bounds, Φ = ⟨ΦX, ΦC⟩, are given as input to the search algorithm.\nAlgorithm 7 Heuristic-Guided Search\n1: Initialize open, closed and temp lists; ∆= 1\n2: ⟨bX\n∆, bC\n∆⟩←approx(I, BX\n0 , BC\n0 )\n3: open.push(I, ⟨bX\n∆, bC\n∆⟩, ⟨BX\n0 , BC\n0 ⟩, priority = 0)\n4: while ∆⩽|S| do\n5:\nwhile open ̸= ∅do\n6:\ns, ⟨bX\n∆, bC\n∆⟩, ⟨BX, BC⟩, hnode ←open.pop()\n7:\nif |bX\n∆| > ∆or |bC\n∆| > ∆then\n8:\ntemp.push(s, ⟨bX\n∆, bC\n∆⟩, ⟨BX, BC⟩, hnode)\n9:\ncontinue\n10:\nadd ⟨bX\n∆, bC\n∆⟩to closed\n11:\nif s |= GA and BX |= ΦX and BC |= ΦC then\n12:\nreturn π, ObsSeqX(π), ObsSeqC(π)\n13:\nfor s′ ∈successors(s) do\n14:\noX ←OX(a, s′); oC ←OC(a, s′)\n15:\nd\nBX = Update(BX, oX); d\nBC = Update(BC, oC)\n16:\n⟨c\nbX\n∆, c\nbC\n∆⟩←approx(s′, d\nBX, d\nBC)\n17:\nhnode ←hGA(s′) + hGk−1(d\nBX) −hGG−j(d\nBC)\n18:\nadd new node to open if ⟨c\nbX\n∆, c\nbC\n∆⟩not in closed\n19:\nincrement ∆; copy items from temp to open; empty temp\n\n128\nObfuscatory Behavior and Deceptive Communication\nEach search node maintains the associated beliefs for both observers. The approx\nfunction generates an approximate belief, bi\n∆, of size ∆(i.e. cardinality of bi\n∆is ∆). bi\n∆\nis always inclusive of the true state of the robot, this is because the robot can only take\nactions that are consistent with its true state. If all such ∆−sized beliefs (or smaller) are\nexplored then bi\n∆of ∆+ 1 size is computed, and this node gets put in the temporary list\nand is explored in the next outer iteration when ∆has been incremented. For each ∆,\nall ∆-sized unique combinations of belief (that include the actual state of the robot) are\nexplored. This allows systematic and complete exploration of multiple paths to a given\nsearch node. The inner iteration performs heuristic guided forward search (we use greedy\nbest first search) to find a plan while tracking at most ∆states in each bi\n∆. In the inner\nloop, the node expansion is guided by (1) customized heuristic function, which computes\nvalue of the node based on true goal and belief constraints given by Φ for the observers,\nand (2) goal test, which checks for satisfaction of true goal and satisfaction of the belief\nconstraints given by Φ. The algorithm stops either when a solution is found or when all\nthe ∆iterations have been explored.\nProperty In both the solution approaches, we can assert a lower bound on the extent of\ngoal obfuscation and goal legibility for a MO-COPP solution plan. In IP, we can specify\nthe aforementioned goal constraints to assert this minimum value, while in the search,\nthe goal tests allow us to assert it. By setting k, j to desired values, we can eliminate\nsolutions with low GD score. This affords the following guarantee:\nProposition 12. Let X and C be perfectly rational adversarial and cooperative ob-\nservers respectively.\nGiven a MO-COPP = ⟨Λ, MR, G, {Ωi}i∈Λ, {Oi}i∈Λ, {Bi\n0}i∈{X,C}⟩\nwith equally probable and independent goals, |G| = n, and goal constraints of at least k\ngoal obfuscation for X and at most j goal legibility for C, then a solution plan, π, gives\nthe following guarantees:\n1. X can infer GA with probability ⩽1/k,\n2. C can infer GA with probability ⩾1/j, and\n3. Goal difference metric, GD(π) ⩾k−j\nn−1\nGiven |Gπ\nX| ⩾k, X can infer GA with probability ⩽1/k. Similarly, given |Gπ\nC| ⩽j, C\ncan infer GA with probability ⩾1/j. Also, similarly, GD(π) ⩾k−j\nn−1. The above proposi-\ntion states that, based on the observation equivalence there is no additional information\nrevealed about the robot’s true goal. Therefore, we can derive goal detection upper bound\nfor X and lower bound for C. Also this allows us to derive a lower bound on the plan\nquality.\n9.3\nLies\nMost accounts of explanations studied in this book consider cases where an automated\nagent shares information about its decision-making model, to accurately justify decisions\nit has made. It is of course possible that if a system would desire so, it could hijack such\na model reconciliation communication process to shape the observer’s expectation to be\n\n9.3 Lies\n129\ndivergent from the true model that is being used by the robot. In this section, we will refer\nto such manipulation of the model reconciliation process as lies. As with the implicit com-\nmunication/behavioral manifestations of deception, lies, and explanations are inexorably\nconnected. One could argue that any poct hoc reconstructive explanations discussed in\nChapter 8 constitute a form of lies (insofar as the model being communicated was never\nexplicitly used in its decision-making process), or by relying on minimal explanations, the\nagent is engaging in lies by omission (insofar that they may be relying on the human’s\nfalse beliefs to simplify explanations). To simplify the discussion, when we refer to lies we\nare referring to scenarios wherein the agent is communicating information it knows to be\nfalse.\n9.3.1\nWhen Should an Agent Lie?\nTo start with, the first question we might want to ask is, why would we ever want an\nagent that lies?\nOutside purely adversarial scenarios, are there cases where we might\nwant to employ such systems? Rather than look at scenarios where the agent is taking\nan explicitly adversarial stance, we will consider cases where the lies are used in service of\nimproving the team utility in some capacity. There is enough evidence from social sciences\nthat lies do play a (potentially positive) role in many real world human-human interaction\nscenarios. For example consider the interaction between a doctor and a patient. There\nare few scenarios where the role of lies have been thoroughly debated than in doctor-\npatient interactions.\nWhether it relates to the prescription of placebo or withholding\ncertain information from the patient, there are many scenarios within a doctor-patient\nrelationship wherein the people have argued for potential usefulness of deception or lies.\nIn the case of human-machine teaming scenario, some potential use cases for such lies\ninclude\n1. Belief Shaping: Here the system could choose to shape the beliefs of their potential\nteammate, through explicit communication, to persuade your teammate to engage\nbehavior the autonomous agent believes would result in higher utility.\n2. White Lies: This case could involve cases where the exact explanation may be too\nexpensive to communicate, and the robot could choose to communicate a simpler\n‘explanation’, which while technically contains untrue information is used to justify\nthe optimal behavior. Such white lies could be particularly helpful when it may be\nexpensive computationally or time consuming for the human teammate to process\nthe original explanations.\n9.3.2\nHow can an Agent Lie?\nNow the question would be how one could generate lies. In general one could leverage\nthe same model space search, and instead of restricting the model updates to only those\nconstrained by the true robot model, in this case the search is free to make any update\non the model. If the agent is not allowed to introduce new fluents or new actions, this is\nstill a finite search space for classical planning domains. Though this would constitute a\nmuch larger search space than the one considered in the explanation, which brings up an\n\n130\nObfuscatory Behavior and Deceptive Communication\ninteresting point that at least in computational terms telling the truth might be an easier\nstrategy to following.\nOne way to constrain the search space would be to limit the kind of changes that are\nallowed under the lies. One natural choice may be to limit lies to those that remove factors\nfrom the propositional representation of the model (i.e Γ(MR\nh )) (Section 5.1). Such lies\nhave been sometimes referred to as lies by omission in the literature. Another possibility\nmay be to leverage theories of model evolution or drift when such information may be\navailable to consider believable changes to models. Or if the lies need to include new\npossibilities (including new actions or fluents), consider leveraging large language models\n(like Brown et al. [2020]) or knowledge bases like WordNet [Fellbaum, 2010] to introduce\nthem in a meaningful way.\nThat is consider existing actions and fluents and use the\nknowledge base to look for related concepts and try to introduce them into the model.\n9.3.3\nImplications of Lies\nUser studies (as reported in Chakraborti and Kambhampati [2019b]) have shown that at\nthe very least lay users seem to be open to the idea of an agent engaging in such deceptive\nbehavior, when it is guaranteed to lead to higher team utility. In this book, we will not\ninvestigate the moral and ethical quandaries raised by designing an agent capable of lying,\nbut rather look at a much simpler question – if we are allowing for deceptive behavior in\nthe hope of higher utility, how does one even guarantee that such behavior would in fact\nlead to higher team utility? In some cases, the agent could very well be aware of the fact\nthat it has access to more information than the other humans in the environment (owing\nto more sophisticated sensors or its location in the environment) and it may be confident\nin its ability to process and reason with the information it has access to. Though in many\nscenarios there is a small probability that the information it has access to is incorrect\n(owing to a faulty sensor or just change in information) or it may have overlooked some\nfactor in its computation. As such the lie could lead to unanticipated negative outcomes.\nIn such cases, it is quite possible that the human teammate could be a lot more critical\nof its automated teammate that chose to lie as compared to a case where the robot made\nan unintentional mistake.\nTo us, this speaks for the need for significant further work\nto be done to not only understand under what conditions an automated system could\nconfidently make such calls, but also better tools to model trust of teammates.\n9.4\nBibliographical Remarks\nThe different obfuscatory behaviors discussed in this chapter have been formulated within\nthe controlled observability planning framework introduced by Kulkarni et al. [2019b].\nThe work on deceptive planning was introduced by Masters and Sardina [2017]. The gen-\neralized extension of controlled observability framework (referred to as mo-copp) used\nfor balancing goal obfuscation for adversaries with the goal legibility for cooperative ob-\nservers was introduced by Kulkarni et al. [2020b]. In terms of lies, the discussion provided\nis based on the papers, Chakraborti and Kambhampati [2019b] and Chakraborti and\nKambhampati [2019a], where the works respectively considered when and why the agents\n\n9.4 Bibliographical Remarks\n131\nshould consider generating lies and the computational mechanisms that could be employed\ntowards generating such explanations. Outside of these specific papers, there is a lot of\nwork investigating the utility of lies in various teaming scenarios. There exists a particular\nextensive literature on the role of lies in decision-making in medical literature Palmieri\nand Stern [2009]. In particular, many works have argued for the importance of a doctor\nwithholding information from the patient (cf. Korsch and Harding [1998] and Holmes\n[1895]).\n\n132\nObfuscatory Behavior and Deceptive Communication\n\nChapter 10\nApplications\nIn this section, we will look at four different applications that leverage the ideas discussed\nin this book. In particular, all the systems discussed in this chapter will explicitly model\nthe human’s mental model of the task and among other things use it to generate expla-\nnations. In particular, we will look at two broad application domains. One where the\nsystems are designed for collaborative decision-making, i.e systems designed to help user\ncome up with decisions for a specific task and another system designed for helping users\nspecify a declarative model of task (specifically in the context of dialogue planning for an\nenterprise chat agent).\n10.1\nCollaborative Decision-Making\nOur first set of applications will be centered around systems that are designed to help\nend-users make decisions.\n10.2\nHumans as Actors\nAll the theoretical and formal discussions in the book until now have focused on scenarios,\nwhere the robot is the one acting in the world and the human is just an observer. The\nspecific decision-support scenarios, we will look at in this section, requires us to consider\na different scenario, one where the human is the primary actor and the robot is either\nan observer or an assistant to the robot. The setting is illustrated in figure 10.1. In this\nscenario, we have a human with a model MH who is acting in the world, and a robot\nobserving the actor. The robot has access to an approximation of the human model MH\nr\nand may also have access to a model M∗which they believe is the true model of the task\nthat the human is pursuing. In addition to the decision-support settings, these settings\nare also present in settings where the AI system may be trying to teach the human (as in\nITS systems VanLehn [2006]) and even cases where a robot may be trying to assist the\nhuman achieve their goal Chakraborti et al. [2015].\nModel-reconciliation explanation in this setting would consist of the robot communi-\ncating information present in M∗, but may be absent in MH (or the robot believes it to\nbe absent based on its estimate MH\nr ). One could also formalize a notion of explicable\nplan in this setting, particularly for cases where the robot may be suggesting plans to the\nhuman. In this case, the explicable plan consists of solving for the following objective\nFind: π\nmax\nπ∈ΠMH\nr\nE(π, MH\nr )\n133\n\n134\nApplications\nFig. 10.1: Illustration of the scenarios where human is the primary actor and robot the observer.\nSuch that π is executable in both MH\nr and M∗\nThat is the goal here becomes to suggest a plan that is explicable with respect to the\nmodel MH\nr in that it is close to the plan expected under the model MH\nr , but is executable\nin both MH\nr and M∗. Additionally, we may also want to choose plans whose cost in the\nmodel M∗is low (could be captured by adding a term −1 × C∗(π) to the objective).\n10.2.1\nRADAR\nThe first example, we will consider is a Proactive Decision Support system called RADAR\n[Sengupta et al., 2017a]. Proactive Decision Support (PDS) aims at improving the decision\nmaking experience of human decision makers by enhancing both the quality of the decisions\nand the ease of making them. RADAR leverages techniques from automated planning\ncommunity that aid the human decision maker in constructing plans. Specifically, the\nsystem focuses on expert humans in the loop who share a detailed, if not complete, model\nof the domain with the assistant, but may still be unable to compute plans due to cognitive\n\n10.2 Humans as Actors\n135\nFig. 10.2:\nRADAR system being applied to a firefighting domain, where the goal is to extinguish a\nnumber of fires. (1) RADAR knows that in the environment, the commander needs to inform the fire\nstation’s fire chief before deploying big engines and rescuers. In green, Adminfire’s fire chief is alerted\nto deploy big engines from Admin Fire Station. In red, Mesa fire stations’ fire chief is alerted to deploy\nrescuers from Mesa fire station. (2) The human’s model believes that there is no need to inform fire chiefs\nand questions RADAR to explain his plan.\nRADAR finds these differences in the domain model and\nreports it to the human. The human acknowledges that before deploying rescuers one might need to alert\nthe fire chief and rejects the update the fire chief needs to be alerted before deploying big engines. (3) In\nthe alternative plan suggested by RADAR, it takes into account the humans knowledge and plans with\nthe updated model. (4) Clicking on ‘Explain This Plan’ generates no explanations as there are none (with\nrespect to the current plan) after the models were updated.\noverload.\nThe system provides a number of useful features like plan suggestion, completion,\nvalidation and summarization to the users of the system.\nThe entire system is built\nto allow users to perform naturalistic decision-making, whereby the system ensures the\nhuman is in control of the decision-making process. This means the proactive decision\nsupport system focuses on aiding and alerting the human in the loop with his/her decisions\nrather than generating a static plan that may not work in the dynamic worlds that the\nplan has to execute in. Figure 10.2, presents a screenshot of the RADAR system.\nThe component that is of particular interest to discussions in this book is the ability\nto support explanations. The system adapts model reconciliation explanations to address\npossible differences in the planner’s model of the domain and the human expectation of\nit. Such differences could occur if the system may be automatically collecting information\nfrom external sources and thus the current estimate of the model diverges from the original\nspecification provided/approved by the user. Here the system performs a model-space\nsearch to come up with Minimally Complete Explanation (Chapter 5 Section 5.2.1) to\nexplain the plan being suggested. An important distinction in the use of explanations\nfrom previous chapter here is the fact that the human has the power to veto the model\nupdate if she believes that the planner’s model is the one which is faulty, by choosing to\napprove or not approve individual parts of the explanation.\n\n136\nApplications\nFor example, consider the scenario highlighted in Figure 10.2, which presents a fire-\nfighting scenario where a commander is trying to come up with a plan to extinguish a\nfire in the city of Tempe. In this scenario, the RADAR system presents a plan (which\nitself is a completion of some action suggestions made by the commander), which contains\nunexpected steps to notify the Fire chief at multiple points. When the commander asks\nfor an explanation, the system responds by pointing out that actions for both deploying\nbig fire engines and the deploying rescuers has a precondition that the fire chief should be\nalerted in its model (which according to the system’s model of the commander is missing\nfrom the commander’s model). The commander responds by agreeing with the system\non the fact that the fire chief needs to be informed before deploying rescuers, but also\ninforms the system that fire chief doesn’t need to be informed before deploying big fire\nengines. The system uses this new information to update its own models about the task\nand generates a new plan.\n10.2.2\nMA-RADAR\nNext we will consider yet another extension of RADAR, but one that is focused on sup-\nporting multiple user types. In particular, MA-RADAR [Sengupta et al., 2018] considers a\nscenario where there are multiple users of possibly different backgrounds working together\nto construct a single solution. Such decision-making scenarios are further complicated\nwhen the different users may have differing levels of access and may have privacy concerns\nlimiting the sharing some of the information with other decision-makers in the loop. As\nsuch, one of the focuses of MA-RADAR was to use augmented reality (AR) to allow the\ndifferent users to view differing views of the task while working together on the same in-\nterface. The fact that they are using AR techniques means that in addition to the public\ninformation (i.e. the information that all the users can access), each user can view their\nspecific private information which may not be accessible to others. For example, revisiting\nthe firefighting domain, let us consider the case where there are two commanders working\ntogether to come up with a plan for controlling the fires. Here in addition to each com-\nmanders personal understanding of the task which may be inconsistent with each other\nand even what the system knows (for example status of some resources etc.), but may have\nknowledge about certain fluents and actions that are private to each commander and may\nnot necessarily want the other commander to know. In terms of the explanations, each\nuser could have different background knowledge and the system should strive to establish\na common ground whenever possible. As such, the system uses the multi-model expla-\nnation method discussed in Chapter 6 (Section 6.2). Specifically, MA-RADAR combines\nthe individual models into a single annotated model and explanations are generated with\nrespect to this annotated model. The generated explanation is then filtered with respect\nto each user, so they only view information relevant to them (i.e the information is not\nredundant as per their model and is not private to any of the other users). Each user can\nthen use their AR interface to view their specific explanations. Figure 10.3, presents the\naugmented reality interface that is shown to the user of the system.\n\n10.2 Humans as Actors\n137\nFig. 10.3: A user wearing an augmented reality (AR) device to use the MA-RADAR interface. The\nfigures show, how in addition to the common information shown on the screen the AR device allow users\nto view information private to each user.\n10.2.3\nRADAR-X\nNext we will consider a variant of the basic RADAR system was extended to support\nexplanatory dialogue and iterative planning. The system, named RADAR-X [Valmeekam\net al., 2020], assumes that the user may have latent preferences that may not be specified\nto the system upfront or need not be completely realizable. The explanatory dialogue,\nparticularly contrastive questions (i.e the user asks Why this plan P as opposed to plan\nQ?) thus becomes a way for the user to expose their underlying preferences. This system\nassumes that as a response to a specific plan, the users responds with a partial specification\nof the plans they prefer. In particular the system expects users to provide partial plans\nthat can be defined by a tuple of the form ˆπ = ⟨ˆA, ≺⟩, where ˆA specifies a multi-set of\nactions the user expects to see in the plan and ≺specifies the set of ordering constraints\ndefined over ˆA that the user expects to be satisfied. A sequential plan is said to satisfy a\ngiven partial plan ˆπ if the plan contains each action specified in ˆA and they satisfy all the\nordering constraints specified in ≺.\n\n138\nApplications\nIf the partially specified plan is feasible, the system suggests one of the possible in-\nstantiations of the partial plan (i.e. a plan that satisfies the given specification) to the\ndecision-maker. The user could possibly further refine their choice by adding more in-\nformation into the partial specification, until she is happy with the choice made by the\nsystem. If the partial specification is not feasible, then the system responds by first pro-\nviding an explanation as to why the foil is not feasible. This is similar to the explanation\ngeneration method specified in Section 5.3.1, but with the end condition being the case of\nidentifying the updated model where the foils are infeasible.\nOnce the explanation is provided, the system tries to generate plans that are closer\nto the specified foil. The system currently tries to find a subset of the original partial\nplan that the user specified that can be realized by the system, where for a given partial\nplan ˆπ = ⟨ˆA, ≺⟩a partial plan ˆπ′ = ⟨ˆA, ≺⟩is considered a subset if ˆA′ ⊆ˆA, and for\nevery a1, a2 ∈ˆA′, a1 ≺′ a2 if and only in a1 ≺a2. The system currently considers three\nstrategies for coming up with such subsets.\n1. Present the closest plan: Among the possible maximal subsets the system chooses\none of them and presents a plan that corresponds to this subset.\n2. Present plausible subsets: The user is presented with all possible maximal subsets\nand they can select the one that most closely represents their preferences.\n3. Present conflict sets: The user with minimal conflict sets. That is minimal subset\nof actions and their corresponding ordering constraints that cannot be achieved\ntogether. So the user is asked to make a choice to remove one of the conflicting\naction from the set.\nThe system also looks at possible approximations that could be used to speed up the\ncalculations.\n10.3\nModel Transcription Assistants\nIn this section, we will consider the problem of systems designed to allow domain experts\nto transcribe the models in some declarative form.\nHere the difference in the mental\nmodels actually comes from possible mistakes that the user may make while writing the\ndeclarative model. Clearly in this case, the system doesn’t have access to the human’s\nmental model and the direction of reconciliation is to get closer to the user’s mental model.\nSo here the process involves generating explanations for specific user queries by assuming\nthat the human’s model is an abstract version of the current model.\nThus exposing\nrelevant fragments of models that correspond to the relevant behavior and thus letting\nuser directly fix any inconsistencies in the exposed fragment.\n10.3.1\nD3WA+\nThe tool D3WA+ Sreedharan et al. [2020b] implemented this idea in the context of tran-\nscribing declarative models for dialogue planning for an automated chat agent. It was an\n\n10.3 Model Transcription Assistants\n139\nextension of a previous system called D3WA Muise et al. [2019] developed by IBM. D3WA\nallowed dialogue editors to encode their knowledge about the dialogue tree in the form\nof a non-deterministic planning domain. D3WA+ took this base system and extended by\nproviding debug tools to the domain designers that allowed them to query the system to\nbetter understand why the system was generating the current dialogue tree. Figure 10.4\npresents a screenshot of the D3WA+ interface.\nIn particular, the system focused on providing the domain writer with the ability to\nraise two types of queries:\n1. Why are there no solutions?\n2. Why does the generated dialogue tree not conform to their expectation?\nThe first question is expected to be raised when there exists no possible trace from initial\nstate to goal under the current model specification, and the latter when the domain writer\nwas expecting the dialogue tree to take a particular form that is not satisfied by the\none generated by the system. Thus the system would need to explain why the current\nproblem is unsolvable. In the second case, the domain writer has to specify their expected\ndialogue flow on the storyboard panel.\nA dialogue flow in this case would consist of\npossible questions the chat agent could raise and possible outcomes. Note the specified\nflow doesn’t need to contain a complete dialogue sequence (which starts at the beginning,\nends with the end-user, i.e. the one who is expected to use the chat bot, getting the desired\noutcome and contains every possible intermediate steps) but could very well be a partial\nspecification of the dialogue flow that highlights some part of the dialogue. Assuming that\nthe expected flow can not be supported by the current model specification, the system\nwould need to explain why this particular flow isn’t possible. If the user had specified a\ncomplete dialogue sequence, then the system can merely test the sequence in the model and\nprovide the failure point. Though this will no longer be possible if the user only specified\na partial foil. In such cases, we would need to respond to why any possible sequence that\nsatisfies the partial specification provided by the domain writer will be impossible. This\ncan now be mapped into explaining the unsolvability of a modified planning problem, one\nthat constrains the original problem to only allow solutions that align with the specified\nflow.\nThus the answer to both these questions maps into explanations of problem unsolv-\nability. The system here leverages approaches discussed in Sreedharan et al. [2019b], to\nfind the minimal subset of fluents for which the problem is unsolvable. A minimal abstrac-\ntion over this set of fluents (with others projected out) is then presented to the domain\nwriter as an unsolvable core of the problem that they can then try to fix. In addition\nto the model abstraction, two additional debugging information is provided to the user.\nAn unreachable landmark and the failure information for an example trace. The unsolv-\nable landmark is extracted by considering the delete relaxation of the unsolvable abstract\nmodel. The example trace is generated from the most concrete solvable model abstraction\nfor the original model, so that more detailed plans are provided to the user. Such concrete\nmodels are generated by searching for the minimum number of fluents to be projected out\nto make the problem solvable.\n\n140\nApplications\nFig. 10.4:\nAn overview of the interface provided by D3WA+ to its end users.\nWhich includes an\noption to specify foils or expected behavior, a panel that shows an abstract version of the domain and\nalso additional debugging information provided to the user that includes information like: example plan\nfailure, unachievable landmarks etc.\n10.4\nBibliographic Remarks\nMany of the applications we consider in this chapter have been demonstrated at various\nconferences including ICAPS and AAAI. RADAR system was first described in Sengupta\net al. [2017a].\nA version of the system that focused on plan of study generation was\ndescribed in Grover et al. [2020]. The paper also presented a user study which validated\nvarious components that were part of the system. The multi agent version MA-RADAR\nwas presented in Sengupta et al. [2018] and was demonstrated in ICAPS 2018 demo track.\nThe contrastive version, i.e., RADAR-X was presented in Valmeekam et al. [2020] and was\nalso presented as a demo at AAAI-21. The D3WA+ system was introduced in Sreedharan\net al. [2020b] , the system was presented as a demo in ICAPS-20 and was awarded the best\ndemo award. Apart from the ones listed in this chapter the ideas presented in the book\nhave also been used in other applications. One prominent one is the FRESCO system\n[Chakraborti et al., 2017b] that was part of an IBM project for a smart room. The system\nmade use of a variant of the model reconciliation for selective plan visualization. They\ntry to model plan visualization as a process of model reconciliation against an empty\nuser model.\nThe process involved identifying the minimum number of components of\nthe actions in the current plan (i.e., the preconditions and effects) that needed to be\ncommunicated to the user for the plan to make sense (i.e the plan is valid and optimal).\nAs mentioned earlier, many of the decision-support applications look at settings where\nthe human is the actor and the system has access to a model MH\nr of the human. All the\nspecific applications discussed assume that MH\nr is accurate in that it is close to MH, but\nthis may not be true in general. While this is quite similar to the asymmetry between\nMR and MR\nh . There is a major difference in that unlike the earlier case, the agent with\naccess to the true model, in this case, the human, may not be invested in performing the\nreconciliation even if she is aware of the asymmetry. In this scenario, the robot may have\nto initiate the process of reconciliation, one possible way may be to make use of questions\n\n10.4 Bibliographic Remarks\n141\nto identify information about the human model Grover et al. [2020].\n\n142\nApplications\n\nChapter 11\nConclusion\nThis book presents a concise introduction to recent research on human-aware decision-\nmaking, particularly ones focused on the generation of behavior that a human would\nfind explainable or deceptive. Human-aware AI or HAAI techniques are characterized\nby the acknowledgment that for automated agents to successfully interact with humans,\nthey need to explicitly take into account the human’s expectations about the agent. In\nparticular, we look at how for the robot to successfully work with humans, it needs to not\nonly take into account its model MR, which encodes the robot’s beliefs about the task\nand their capabilities but also take into account the human’s expectation of the robot\nmodel MR\nh , which captures what the human believes the task to be and what the robot is\ncapable of. It is the model MR\nh that determines what the human would expect the robot\nto do and as such if the robot expects to adhere to or influence the human expectations, it\nneeds to take into account this model. Additionally, this book also introduces three classes\nof interpretability measures, which capture certain desirable properties of robot behavior.\nSpecifically, we introduce the measures Explicability, Legibility, and Predictability.\nIn the book, we mostly focused on developing and discussing methods to address the\nfirst two of these interpretability measures. We looked at specific algorithms that allow\nus to generate behaviors that boost explicability and legibility. We also looked at the\nproblem of generating explanations for a given plan, and how it could be viewed as the\nuse of communication to boost the explicability score of a selected plan by updating human\nexpectations. Additionally, we looked at variations of this basic explanation framework\nunder differing settings, including cases where the human models of the robot may be\nunknown or where the robot’s decision-making model may be expressed in terms that\nthe human doesn’t understand. We also saw a plan generation method that is able to\nincorporate reasoning about the overhead of explanation into the plan selection process,\nthereby allowing for a method that is able to combine the benefits of purely explicable plan\ngeneration methods and those that identify explanations after the plan has been selected.\nAdditionally, we also saw that modeling human expectations not only allows us to\ncreate interpretable behaviors but also provides us with the tools needed to generate\ndeceptive and adversarial behaviors. In particular, we saw how one could leverage the\nmodeling of the other agent to create behaviors that obfuscate certain agent model infor-\nmation from an adversarial observer or even deceive them about the model component.\nWe also saw how model reconciliation methods could be molded to create lies, which even\nin non-adversarial scenarios could help the agent to achieve higher team utility at the cost\nof providing some white lies to the human.\nWhile the problem of generating explanations for AI decisions or developing deceptive\nAI agents has been studied for a while, the framing of these problems within the context\nof human-aware AI settings is a relatively recent effort. As such, there exists a number of\nexciting future directions to be explored and technical challenges to overcome, before we\ncan have truly human-aware systems. Some of these challenges are relatively straightfor-\nward (at least in their conception), as in the case of scaling up the methods and applying\nthem within more complex scenarios that are more faithful to real-world scenarios. Then\n143\n\n144\nConclusion\nthere are challenges that may require us to rethink our current strategies and whose for-\nmalization itself presents a significant challenge. Here we would like to take a quick look\nat a few of these challenges.\nCreating a Symbolic Middle Layer\nThe necessity of symbols in intelligent decision-\nmaking has been a topic that has been widely debated within the field of AI for a very\nlong time.\nRegardless of whether symbols are necessary for intelligence, it remains a\nfact that people tend to communicate in terms of symbols and concepts. As such, if we\nwant to create systems that can interact with people effectively they should be capable\nof communicating using symbols and concepts people understand. In chapter 8, we have\nalready seen an example, where an agent translates information about its model into terms\nthat are easier for a human to understand, but if we want to create successful systems\nthat are truly able to collaborate with humans then we need to go beyond just creating\nexplanation generation systems. They need to be able to take input from the human\nin symbolic terms even when the model may not be represented in those terms. Such\nadvice could include information like instructions the agent should follow, possible domain,\nand preference information. Interestingly the advice provided by the human would be\ncolored by what they believe the agent model to be, and as such correct interpretation\nof the specified information may require analyzing the human input in the light of their\nexpectation about the agent model.\nSome preliminary works in this direction include\nGuan et al. [2020] and for a discussion on the overall direction, the readers can refer to\nKambhampati et al. [2021].\nTrust and Longitudinal Interaction\nMost of the interactions discussed in this book\nare single-step interactions, in so far that they focus on the agent proposing a plan for\na single task and potentially handling any interaction requirements related to that plan.\nBut we are not focused on creating single-use robots. Rather we want to create automated\nagents that we can cohabit with and work with on a day-to-day basis. This means the\nrobot’s choice of actions can no longer be made in isolation, rather it should consider the\nimplications of choosing a certain course of action on future interaction with humans. We\nsaw some flavors of such consideration in methods like Minimally Monotonic Explanations\n(MME) (Chapter 5, Section 5.2.1) and the discounting of explicability over a time horizon\n(Chapter 3, Section 3.4.2), though these are still limited cases. As we move forward, a\npressing requirement is for the robots to be capable of modeling the level of trust the\nhuman holds for the robot and how the robot’s actions may influence the human trust.\nSuch trust-level modeling is of particular importance in longitudinal settings, as it would\nbe the human trust on the robot that would determine whether or not the human would\nchoose to work with it on future tasks. Thus the impact of the robot action on human\ntrust should be a metric it should consider while coming up with its actions. At the same\ntime, reasoning about trust levels also brings up the question of trust manipulation and\nhow to design agents that are guaranteed to not induce undeserved trust in its capabilities.\nSome preliminary works in this direction are presented in Zahedi et al. [2021].\nLearning Human Models\nThe defining feature of many of the techniques discussed\nin this book is the inclusion of the human’s expectations, sensory capabilities, and in\ngeneral their models into the reasoning process. In many scenarios, such models may\n\n145\nnot be directly available to the robot and it may be required to learn such models either\nby observing the human or by receiving feedback from the human. We have already seen\nexamples of learning specific model proxies that are sufficient to generate specific classes of\nbehaviors. But these are specialized models meant for specific applications. More general\nproblems may require the use of additional information and even the complete model.\nWhile the model MH could be learned by observing the human behavior, the model MR\nh\nis usually more expensive to learn as it requires the human to either provide feedback on\nthe robot behavior or provide the full plan they are expecting from the robot. As such\nlearning a fully personalized model for a person may require too much information to be\nprovided by a single person. A more scalable strategy may be to learn approximate models\nfor different user types from previously collected data. As and when the robot comes into\ncontact with a new human, they can use the set of learned models to identify the closest\npossible model. This model can act as a starting point for the interaction between the\nsystem and the user and can be refined over time as the robot interacts with the human.\nSome initial work in this direction can be found in Soni et al. [2021].\nSuper Human AI and Safety\nCurrently, most successful AI systems are generally\nless competent than humans overall but may have an edge over people on specific narrow\ntasks. There is no particular reason to believe that this condition should always persist.\nIn fact, one could easily imagine a future where AI systems outpace humans in almost\nall tasks. It may be worth considering how the nature of human-robot interaction may\nchange in such a world. For one thing, the nature and goal of explanation may no longer\nbe about helping humans understand the exact reason for selecting a decision but rather\nabout giving a general sense of why the decisions make sense. For example, it may be\nenough to establish why the decision is better than any alternative the human could come\nup with. Going back to the problem of vocabulary mismatch introduced in Chapter 8, it\nmay very well be the case that there may be concepts that the system makes use of that\nhave no equivalent term in human vocabulary and as such explanation might require the\nsystem teaching new concepts to the human. The introduction of the ability to reason\nand model the human mental model also raises additional safety and ethical questions\nin the context of such superhuman human-aware AI. For one, the questions of white lies\nand deception for improving team utility (Chapter 9, Section 9.3) takes on a whole new\ndimension and could potentially head into the realm of wireheading. It is very much an\nopen question as to how one can build robust methods and safeguards to control for and\navoid such potential safety concerns that may arise from the deployment of such systems.\nAs we see more AI systems embrace human-aware AI principles, it becomes even more\nimportant to study and try ameliorate unique safety concerns that arise in systems capable\nof modeling and influencing human’s mental models and beliefs.\n\n146\nConclusion\n\nBibliography\nJames F Allen. Mixed initiative planning: Position paper. In ARPA/Rome Labs Planning\nInitiative Workshop, 1994.\nSaleema Amershi, Daniel S. Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi,\nPenny Collisson, Jina Suh, Shamsi T. Iqbal, Paul N. Bennett, Kori Inkpen, Jaime\nTeevan, Ruth Kikin-Gil, and Eric Horvitz.\nGuidelines for human-ai interaction.\nIn\nProceedings of the 2019 CHI Conference on Human Factors in Computing Systems,\nCHI 2019, Glasgow, Scotland, UK, May 04-09, 2019, page 3, 2019.\nDan Amir and Ofra Amir. Highlights: Summarizing agent behavior to people. In Pro-\nceedings of the 17th International Conference on Autonomous Agents and MultiAgent\nSystems, pages 1168–1176, 2018.\nChris L Baker and Joshua B Tenenbaum. Modeling human plan recognition using bayesian\ntheory of mind. Plan, activity, and intent recognition: Theory and practice, 7:177–204,\n2014.\nPascal Bercher, Susanne Biundo, Thomas Geier, Thilo Hoernle, Florian Nothdurft, Felix\nRichter, and Bernd Schattenberg. Plan, Repair, Execute, Explain – How Planning Helps\nto Assemble Your Home Theater. In ICAPS, 2014.\nBlai Bonet and Hector Geffner. Belief tracking for planning with sensing: Width, com-\nplexity and approximations.\nJournal of Artificial Intelligence Research, 50:923–970,\n2014.\nCynthia Breazeal. Toward sociable robots. Robotics and autonomous systems, 42(3-4):\n167–175, 2003.\nCynthia L Breazeal. Designing sociable robots. MIT press, 2004.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nDan Bryce, J Benton, and Michael W Boldt. Maintaining evolving domain models. In\nProceedings of the twenty-fifth international joint conference on artificial intelligence,\npages 3053–3059, 2016.\nTathagata Chakraborti and Subbarao Kambhampati. (How) Can AI Bots Lie? In XAIP\nWorkshop, 2019a.\nTathagata Chakraborti and Subbarao Kambhampati. (when) can ai bots lie?\nIn Pro-\nceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 53–59,\n2019b.\nTathagata Chakraborti, Gordon Briggs, Kartik Talamadupula, Yu Zhang, Matthias\nScheutz, David Smith, and Subbarao Kambhampati. Planning for serendipity. In 2015\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages\n5300–5306. IEEE, 2015.\n147\n\n148\nConclusion\nTathagata Chakraborti, Sarath Sreedharan, Yu Zhang, and Subbarao Kambhampati. Plan\nExplanations as Model Reconciliation: Moving Beyond Explanation as Soliloquy. In\nIJCAI, 2017a.\nTathagata Chakraborti, Kartik Talamadupula, Mishal Dholakia, Biplav Srivastava, Jef-\nfrey O Kephart, and Rachel KE Bellamy. Mr. jones–towards a proactive smart room\norchestrator. In AAAI Fall Symposia, 2017b.\nTathagata Chakraborti, Sarath Sreedharan, Anagha Kulkarni, and Subbarao Kambham-\npati. Projection-Aware Task Planning and Execution for Human-in-the-Loop Operation\nof Robots. In IROS, 2018.\nTathagata Chakraborti, Anagha Kulkarni, Sarath Sreedharan, David Smith, and Sub-\nbarao Kambhampati. Explicability? Legibility? Predictability? Transparency? Pri-\nvacy? Security?: The Emerging Landscape of Interpretable Agent Behavior. In ICAPS,\n2019a.\nTathagata Chakraborti, Sarath Sreedharan, Sachin Grover, and Subbarao Kambhampati.\nPlan explanations as model reconciliation–an empirical study. In 2019 14th ACM/IEEE\nInternational Conference on Human-Robot Interaction (HRI), pages 258–266. IEEE,\n2019b.\nTathagata Chakraborti, Sarath Sreedharan, and Subbarao Kambhampati. Balancing Ex-\nplicability and Explanation in Human-Aware Planning. In IJCAI, 2019c.\nTathagata Chakraborti, Sarath Sreedharan, and Subbarao Kambhampati. The emerging\nlandscape of explainable ai planning and decision making. In IJCAI, 2020.\nAnca Dragan and Siddhartha Srinivasa. Generating Legible Motion. In RSS, 2013.\nAnca D Dragan. Robot Planning with Mathematical Models of Human State and Action.\narXiv:1705.04226, 2017.\nAnca D Dragan, Kenton CT Lee, and Siddhartha S Srinivasa. Legibility and Predictability\nof Robot Motion. In HRI, 2013.\nAnca D Dragan, Shira Bauman, Jodi Forlizzi, and Siddhartha S Srinivasa.\nEffects of\nRobot Motion on Human-Robot Collaboration. In HRI, 2015.\nRebecca Eifler, Michael Cashmore, Jörg Hoffmann, Daniele Magazzeni, and Marcel Stein-\nmetz. A New Approach to Plan-Space Explanation: Analyzing Plan-Property Depen-\ndencies in Oversubscription Planning. In AAAI, 2020.\nChristiane Fellbaum. Wordnet. In Theory and applications of ontology: computer appli-\ncations, pages 231–243. Springer, 2010.\nJaime F Fisac, Chang Liu, Jessica B Hamrick, S Shankar Sastry, J Karl Hedrick, Thomas L\nGriffiths, and Anca D Dragan. Generating Plans that Predict Themselves. In WAFR,\n2018.\nMaria Fox, Derek Long, and Daniele Magazzeni. Explainable Planning. In IJCAI XAI\nWorkshop, 2017.\nHector Geffner and Blai Bonet. A Concise Introduction to Models and Methods for Au-\ntomated Planning. Synthesis Lectures on Artificial Intelligence and Machine Learning,\n2013.\n\n149\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\nSachin Grover, David Smith, and Subbarao Kambhampati.\nModel elicitation through\ndirect questioning. arXiv preprint arXiv:2011.12262, 2020.\nLin Guan, Mudit Verma, Sihang Guo, Ruohan Zhang, and Subbarao Kambhampati.\nExplanation augmented feedback in human-in-the-loop reinforcement learning. arXiv\npreprint arXiv:2006.14804, 2020.\nBradley Hayes and Julie A Shah. Improving robot controller transparency through au-\ntonomous policy explanation. In 2017 12th ACM/IEEE International Conference on\nHuman-Robot Interaction (HRI), pages 303–312. IEEE, 2017.\nMalte Helmert. The Fast Downward Planning System. JAIR, 2006.\nJörg Hoffmann. Ff: The fast-forward planning system. AI magazine, 22(3):57–57, 2001.\nJörg Hoffmann. Where’ignoring delete lists’ works: Local search topology in planning\nbenchmarks. Journal of Artificial Intelligence Research, 24:685–758, 2005.\nOliver Wendell Holmes. Medical Essays, 1842-1882, volume 9. Houghton, Mifflin, 1895.\nSubbarao Kambhampati, Sarath Sreedharan, Mudit Verma, Yantian Zha, and Lin Guan.\nSymbols as a lingua franca for bridging human-ai chasm for explainable and advisable\nai systems. In AAAI Senior Member Track, 2021.\nSarah Keren, Avigdor Gal, and Erez Karpas. Goal Recognition Design. In ICAPS, 2014.\nSarah Keren, Avigdor Gal, and Erez Karpas. Privacy Preserving Plans in Partially Ob-\nservable Environments. In IJCAI, 2016.\nSarah Keren, Avigdor Gal, and Erez Karpas. Strong Stubborn Sets for Efficient Goal\nRecognition Design. In ICAPS, 2018.\nBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Vie-\ngas, et al. Interpretability beyond feature attribution: Quantitative testing with concept\nactivation vectors (tcav). In International conference on machine learning, pages 2668–\n2677. PMLR, 2018.\nRoss A Knepper, Christoforos I Mavrogiannis, Julia Proft, and Claire Liang. Implicit\ncommunication in a joint action. In Proceedings of the 2017 ACM/IEEE International\nConference on Human-Robot Interaction, pages 283–292. ACM, 2017.\nGeorge Konidaris, Leslie Pack Kaelbling, and Tomas Lozano-Perez. From skills to symbols:\nLearning symbolic representations for abstract high-level planning. Journal of Artificial\nIntelligence Research, 61:215–289, 2018.\nBarbara M Korsch and Caroline Harding. The intelligent patient’s guide to the doctor-\npatient relationship: learning how to talk so your doctor will listen. Oxford University\nPress, 1998.\nBenjamin Krarup, Michael Cashmore, Daniele Magazzeni, and Tim Miller. Model-Based\nContrastive Explanations for Explainable Planning. In XAIP Workshop, 2019.\n\n150\nConclusion\nAnagha Kulkarni, Tathagata Chakraborti, Yantian Zha, Satya Gautam Vadlamudi,\nYu Zhang, and Subbarao Kambhampati.\nExplicable Robot Planning as Minimizing\nDistance from Expected Behavior. In AAMAS Extended Abstract, 2019a.\nAnagha Kulkarni, Siddharth Srivastava, and Subbarao Kambhampati. A Unified Frame-\nwork for Planning in Adversarial and Cooperative Environments. In AAAI, 2019b.\nAnagha Kulkarni, Sarath Sreedharan, Sarah Keren, Tathagata Chakraborti, David Smith,\nand Subbarao Kambhampati. Designing environments conducive to interpretable robot\nbehavior. In IROS, 2020a.\nAnagha Kulkarni, Siddharth Srivastava, and Subbarao Kambhampati. Signaling friends\nand head-faking enemies simultaneously: Balancing goal obfuscation and goal legibility.\nIn Proceedings of the 19th International Conference on Autonomous Agents and MultiA-\ngent Systems, AAMAS ’20, page 1889–1891. International Foundation for Autonomous\nAgents and Multiagent Systems, 2020b.\nJohn Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random fields:\nProbabilistic models for segmenting and labeling sequence data. In ICML, 2001.\nIsaac Lage, Daphna Lifschitz, Finale Doshi-Velez, and Ofra Amir. Exploring Computa-\ntional User Models for Agent Policy Summarization. In IJCAI, 2019.\nAleck M MacNally, Nir Lipovetzky, Miquel Ramirez, and Adrian R Pearce. Action Selec-\ntion for Transparent Planning. In AAMAS, 2018.\nPeta Masters and Sebastian Sardina. Deceptive Path Planning. In IJCAI, 2017.\nTim Miller. Explanation in Artificial Intelligence: Insights from the Social Sciences. Ar-\ntificial Intelligence, 2019.\nChristian Muise, Vaishak Belle, Paolo Felli, Sheila McIlraith, Tim Miller, Adrian Pearce,\nand Liz Sonenberg. Planning over multi-agent epistemic states: A classical planning\napproach. In Proceedings of the AAAI Conference on Artificial Intelligence, 2015.\nChristian Muise, Tathagata Chakraborti, Shubham Agarwal, Ondrej Bajgar, Arunima\nChaudhary, Luis A Lastras-Montano, Josef Ondrej, Miroslav Vodolan, and Charlie\nWiecha. Planning for goal-oriented dialogue systems. arXiv preprint arXiv:1910.08137,\n2019.\nTuan Nguyen, Sarath Sreedharan, and Subbarao Kambhampati. Robust planning with\nincomplete domain models. Artificial Intelligence, 245:134–161, 2017.\nTuan Anh Nguyen, Minh Do, Alfonso Emilio Gerevini, Ivan Serina, Biplav Srivastava, and\nSubbarao Kambhampati. Generating diverse plans to handle unknown and partially\nknown user preferences. Artificial Intelligence, 190(0):1 – 31, 2012.\nNils J Nilsson. Principles of artificial intelligence. Morgan Kaufmann, 2014.\nJohn J Palmieri and Theodore A Stern. Lies in the doctor-patient relationship. Primary\ncare companion to the Journal of clinical psychiatry, 11(4):163, 2009.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\"\nexplaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD\ninternational conference on knowledge discovery and data mining, pages 1135–1144,\n2016.\n\n151\nStuart Russell and Peter Norvig. Artificial intelligence: a modern approach. Prentice Hall,\n2002.\nBrian Scassellati. Theory of mind for a humanoid robot. Auton. Robots, 12(1):13–24,\n2002.\nBastian Seegebarth, Felix Müller, Bernd Schattenberg, and Susanne Biundo.\nMaking\nHybrid Plans More Clear to Human Users – A Formal Approach for Generating Sound\nExplanations. In ICAPS, 2012.\nSailik Sengupta, Tathagata Chakraborti, Sarath Sreedharan, and Subbarao Kambham-\npati. RADAR – A Proactive Decision Support System for Human-in-the-Loop Planning.\nIn AAAI Fall Symposium, 2017a.\nSailik Sengupta, Tathagata Chakraborti, Sarath Sreedharan, Satya Gautam Vadlamudi,\nand Subbarao Kambhampati. Radar-a proactive decision support system for human-\nin-the-loop planning. In AAAI Fall Symposia, pages 269–276, 2017b.\nSailik Sengupta, Tathagata Chakraborti, and Subbarao Kambhampati.\nMa-radar–a\nmixed-reality interface for collaborative decision making. ICAPS UISP, 2018.\nUtkarsh Soni, Sarath Sreedharan, and Subbarao Kambhampati. Not all users are the\nsame: Providing personalized explanations for sequential decision making problems.\narXiv preprint arXiv:2106.12207, 2021.\nSarath Sreedharan, Subbarao Kambhampati, et al. Handling model uncertainty and mul-\ntiplicity in explanations via model reconciliation. In Proceedings of the International\nConference on Automated Planning and Scheduling, 2018a.\nSarath Sreedharan, Siddharth Srivastava, and Subbarao Kambhampati. Hierarchical Ex-\npertise Level Modeling for User Specific Contrastive Explanations. In IJCAI, 2018b.\nSarath Sreedharan, Alberto Olmo, Aditya Prasad Mishra, and Subbarao Kambhampati.\nModel-free model reconciliation. In AAAI, 2019a.\nSarath Sreedharan, Siddharth Srivastava, David Smith, and Subbarao Kambhampati.\nWhy can’t you do that hal? explaining unsolvability of planning tasks. In International\nJoint Conference on Artificial Intelligence, 2019b.\nSarath Sreedharan, Tathagata Chakraborti, Christian Muise, and Subbarao Kambham-\npati. Planning with Explanatory Actions: A Joint Approach to Plan Explicability and\nExplanations in Human-Aware Planning. In AAAI, 2020a.\nSarath Sreedharan, Tathagata Chakraborti, Christian Muise, Yasaman Khazaeni, and\nSubbarao Kambhampati.\n–d3wa+–a case study of xaip in a model acquisition task\nfor dialogue planning. In Proceedings of the International Conference on Automated\nPlanning and Scheduling, volume 30, pages 488–497, 2020b.\nSarath Sreedharan, Utkash Soni, Mudit Verma, Siddharth Srivastava, and Subbarao\nKambhampati.\nBridging the gap:\nProviding post-hoc symbolic explanations for\nsequential decision-making problems with black box simulators.\narXiv preprint\narXiv:2002.01080, 2020c.\n\n152\nConclusion\nSarath Sreedharan, Siddharth Srivastava, and Subbarao Kambhampati.\nTldr: Policy\nsummarization for factored ssp problems using temporal abstractions. In Proceedings of\nthe International Conference on Automated Planning and Scheduling, volume 30, pages\n272–280, 2020d.\nSarath Sreedharan, Tathagata Chakraborti, and Subbarao Kambhampati. Foundations\nof explanations as model reconciliation. Artificial Intelligence, 301:103558, 2021a.\nSarath Sreedharan, Anagha Kulkarni, David E Smith, and Subbarao Kambhampati. A\nunifying bayesian formulation of measures of interpretability in human-ai interaction.\nIn International Joint Conference on Artificial Intelligence, pages 4602–4610, 2021b.\nSarath Sreedharan, Siddharth Srivastava, and Subbarao Kambhampati. Using state ab-\nstractions to compute personalized contrastive explanations for ai agent behavior. Ar-\ntificial Intelligence, 301:103570, 2021c.\nBiplav Srivastava, Tuan Anh Nguyen, Alfonso Gerevini, Subbarao Kambhampati,\nMinh Binh Do, and Ivan Serina. Domain independent approaches for finding diverse\nplans. In IJCAI, pages 2016–2022, 2007.\nNicholay Topin and Manuela Veloso. Generation of Policy-Level Explanations for Rein-\nforcement Learning. In AAAI, 2019.\nKarthik Valmeekam, Sarath Sreedharan, Sailik Sengupta, and Subbarao Kambhampati.\nRadar-x: An interactive interface pairing contrastive explanations with revised plan\nsuggestions. In XAIP ICAPS, 2020.\nKurt VanLehn. The behavior of tutoring systems. I. J. Artificial Intelligence in Education,\n16(3):227–265, 2006.\nStylianos Loukas Vasileiou, Alessandro Previti, and William Yeoh. On exploiting hitting\nsets for model reconciliation. In AAAI, 2021.\nManuela M Veloso. Learning by Analogical Reasoning in General Problem Solving. Doc-\ntoral Thesis, 1992.\nJ Waa, J van Diggelen, K Bosch, and M Neerincx. Contrastive Explanations for Reinforce-\nment Learning in Terms of Expected Consequences. In IJCAI Workshop on explainable\nAI (XAI), 2018.\nH Wimmer and J Perner. Beliefs about beliefs: Representation and constraining function\nof wrong beliefs in young children’s understanding of deception. Cognition, 1983.\nTom Zahavy, Nir Ben-Zrihem, and Shie Mannor. Graying the Black Box: Understanding\nDQNs. In ICML, 2016.\nZahra Zahedi, Mudit Verma, Sarath Sreedharan, and Subbarao Kambhampati. Trust-\naware planning:\nModeling trust evolution in longitudinal human-robot interaction.\narXiv preprint arXiv:2105.01220, 2021.\nYantian Zha, Lin Guan, and Subbarao Kambhampati. Learning from ambiguous demon-\nstrations with self-explanation guided reinforcement learning, 2021.\nHaoqi Zhang, Yiling Chen, and David C Parkes. A General Approach to Environment\nDesign with One Agent. In IJCAI, 2009.\n\n153\nYu Zhang, Sarath Sreedharan, Anagha Kulkarni, Tathagata Chakraborti, Hankz Hankui\nZhuo, and Subbarao Kambhampati. Plan Explicability and Predictability for Robot\nTask Planning. In ICRA, 2017.\n\n154\nConclusion\n\nAuthors’ Biographies\nSarath Sreedharan\nSarath Sreedharan is a Ph.D. student at Arizona State Uni-\nversity working with Prof.\nSubbarao Kambhampati.\nHis pri-\nmary research interests lie in the area of human-aware and ex-\nplainable AI, with a focus on sequential-decision making prob-\nlems.\nSarath’s research has been featured in various premier\nresearch conferences, including IJCAI, AAAI, AAMAS, ICAPS,\nICRA, IROS, etc, and journals like AIJ. He was also the recipi-\nent of Outstanding Program Committee Member Award at AAAI-\n2020.\nAnagha Kulkarni\nAnagha\nKulkarni\nis\nan\nAI\nResearch\nScientist\nat\nInvi-\ntae.\nBefore\nthat,\nshe\nreceived\nher\nPh.D.\nin\nComputer\nScience\nfrom\nArizona\nState\nUniversity.\nHer\nPh.D.\nthesis\nwas\nin\nthe\narea\nof\nhuman-aware\nAI\nand\nautomated\nplan-\nning.\nAnagha’s\nresearch\nhas\nfeatured\nin\nvarious\npremier\nconferences\nlike\nAAAI,\nIJCAI,\nICAPS,\nAAMAS,\nICRA\nand\nIROS.\nSubbarao Kambhampati\nSubbarao Kambhampati is a professor in the School of Computing\n& AI at Arizona State University. Kambhampati studies fundamental\nproblems in planning and decision making, motivated in particular by\nthe challenges of human-aware AI systems. He is a fellow of Associa-\ntion for the Advancement of Artificial Intelligence, American Associa-\ntion for the Advancement of Science, and Association for Computing\nmachinery, and was an NSF Young Investigator. He was the presi-\ndent of the Association for the Advancement of Artificial Intelligence,\ntrustee of International Joint Conference on Artificial Intelligence, and\na founding board member of Partnership on AI. Kambhampati’s research as well as his\nviews on the progress and societal impacts of AI have been featured in multiple national\nand international media outlets.\n155",
    "pdf_filename": "Explainable Human-AI Interaction - A Planning Perspective.pdf"
}