{
    "title": "Green My LLM Studying the key factors affecting the energy consumption of code assistants",
    "abstract": "In recent years, Large Language Models (LLMs) have significantly improved in generating high-quality code, enabling their integration into developers’ Integrated Development Environments (IDEs) as code assistants. These as- sistants, such as GitHub Copilot, deliver real-time code suggestions and can greatly enhance developers’ productivity. However, the environmental impact of these tools, in particular their energy consumption, remains a key concern. This paper investigates the energy consumption of LLM-based code assistants by simulating developer interactions with GitHub Copilot and analyzing various configuration factors. We collected a dataset of develop- ment traces from 20 developers and conducted extensive software project development simulations to measure energy usage under different scenarios. Our findings reveal that the energy consumption and performance of code assistants are influenced by various factors, such as the number of concurrent developers, model size, quantization methods, and the use of streaming. No- tably, a substantial portion of generation requests made by GitHub Copi- lot is either canceled or rejected by developers, indicating a potential area for reducing wasted computations. Based on these findings, we share action- able insights into optimizing configurations for different use cases, demon- strating that careful adjustments can lead to significant energy savings.",
    "body": "Green My LLM: Studying the key factors affecting\nthe energy consumption of code assistants\nTristan Coigniona, Cl´ement Quintona, Romain Rouvoya\naUniv. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France\nAbstract\nIn recent years, Large Language Models (LLMs) have significantly improved\nin generating high-quality code, enabling their integration into developers’\nIntegrated Development Environments (IDEs) as code assistants. These as-\nsistants, such as GitHub Copilot, deliver real-time code suggestions and\ncan greatly enhance developers’ productivity. However, the environmental\nimpact of these tools, in particular their energy consumption, remains a key\nconcern. This paper investigates the energy consumption of LLM-based code\nassistants by simulating developer interactions with GitHub Copilot and\nanalyzing various configuration factors. We collected a dataset of develop-\nment traces from 20 developers and conducted extensive software project\ndevelopment simulations to measure energy usage under different scenarios.\nOur findings reveal that the energy consumption and performance of code\nassistants are influenced by various factors, such as the number of concurrent\ndevelopers, model size, quantization methods, and the use of streaming. No-\ntably, a substantial portion of generation requests made by GitHub Copi-\nlot is either canceled or rejected by developers, indicating a potential area\nfor reducing wasted computations. Based on these findings, we share action-\nable insights into optimizing configurations for different use cases, demon-\nstrating that careful adjustments can lead to significant energy savings.\nKeywords:\nlarge language models, code assistants, energy consumption\nEmail address: tristan.coignion@inria.fr (Tristan Coignion)\nPreprint submitted to Elsevier\nNovember 20, 2024\narXiv:2411.11892v1  [cs.SE]  7 Nov 2024\n\n1. Introduction\nIn recent years, Large Language Models (LLMs) for code have significantly\nbecome better at generating code, facilitating their seamless integration into\ndevelopers’ Integrated Development Environments (IDEs) as code assistants.\nCode assistants offer auto-completion suggestions that developers can either\naccept or reject. The generation process is typically initiated automatically\nafter a brief pause in typing, but can also be manually triggered via a com-\nmand or keyboard shortcut.\nNumerous code assistants, like GitHub Copilot, Tabnine, and Code-\nWhisperer, including open-source options—like Tabby and Cody—offer\nIDE extensions and manage inference servers for code suggestions.\nHowever, the energy consumption of software has gained prominence as\na significant environmental and societal concern. In the context of Artif-\nical Intelligence (AI), Green AI has been defined by Schwartz et al. [1] as\nresearch that yields novel results while considering the computational cost,\nmaking practitioners ideally reduce resources spent. Specifically to Green AI\napplied to LLMs, studies have observed the environmental impact of training\nand using such LLMs. For instance, Samsi et al. benchmarked the energy\nconsumption of LLM inference and were able to estimate the energy of a\nsingle response from an LLM [2]. Other works focused more on the impact\nof training the model, such as the carbon footprint of the Bloom model\nestimated by Luccioni et al. [3]. However, the evaluation of LLMs’ energy\nconsumption remains challenging when assessing LLMs dedicated to specific\npurposes. In the context of LLMs for code, existing studies have only focused\non the impact of the generated code [4, 5].\nIn this paper, we aim to determine how much energy an average developer\nconsumes when using a code assistant similar to GitHub Copilot and how\nto reduce it. To the best of our knowledge, our work is one of the first to study\nthe energy consumption of LLMs in code assistants. In particular, we wish to\ndeliver actionable insights into the energy consumption of the code assistant\nfrom the perspective of both the service provider (e.g., GitHub Copilot)\nand the end-user interacting with the code assistant. All the more in the\ncontext of code assistants, like GitHub Copilot, the end-user knows little\nabout the internal workings and impacts of the service: as the LLM inference\nis executed remotely in the cloud, it is difficult for the developer to perceive\nthe computing impact of using a code assistant. Thus, we aim to answer the\nfollowing research questions:\n2\n\nRQ1: What is the impact of certain factors on the energy consumption and\nperformance of code assistants? Specifically, we study the impacts of\nthe number of concurrent developers using the assistant, the streaming\nand manual triggering of the requests, the model and its quantization,\nthe maximum number of concurrent requests, and the number of GPUs.\nRQ2: How many generation requests made by GitHub Copilot are use-\nful? Knowing how many generations are useful to the developer could\nallow future works to improve the efficiency of code assistants.\nRQ3: How much energy does a developer consume when using a code assis-\ntant similar to GitHub Copilot under different scenarios and ob-\njectives?\nWe aim to get a broad look at the potential impacts of a\ncode assistant by leveraging the knowledge about the various factors\nwe gathered when answering RQ1.\nWe chose to study GitHub Copilot specifically for three reasons: (i) it\nis the most used code assistant according to the 2023 StackOverflow developer\nsurvey,1 (ii) the inference server provided by GitHub Copilot exhibits\nlow latency and correct quality [6], which we may not be able to reproduce\nwith our own inference server, and (iii) GitHub Copilot provides many\nmechanisms making it one of the state-of-the-art code assistant, such as\npreventing some generations requests that may not be useful, caching the\nresults of the previous generations requests, canceling the previous request\nwhen a new one is sent, and building of prompts that take into accounts\nmultiple files. We are aware of the existence of these mechanisms thanks to\nthe Copilot Explorer project [7], while evidence of similar mechanisms\nin other code assistants is unclear, and verification is time-consuming.\nIn this paper, we share the following contributions:\n- We provide the first dataset of development traces from developers\nusing GitHub Copilot, which allows the simulation of developers\nusing a code assistant on a real inference server.\n- We study the impact of some configuration options of the inference\nserver and the code assistant on its energy consumption and perfor-\nmance.\n1https://survey.stackoverflow.co/2023/#section-most-popular-technologies-ai-developer-tools\n3\n\n- We analyze the ratio of useful generation requests from GitHub Copi-\nlot.\n- We estimate how much energy a developer would consume when devel-\noping under different configurations of the inference server.\nMain findings & implications. Our results reveal that a considerable\nportion of energy is wasted due to suggestions being cancelled or not wanted\nby the users. Thus, manually triggering the generation of the code assistant\ncan significantly reduce its energy consumption, by minimizing unnecessary\nrequests. Additionally, the number of concurrent developers on a single in-\nference server greatly affects the energy consumption of a single developer,\nas having more concurrent developers better uses the resources of the server.\nProviders should aim to maximize the number of developers per server or\nshare inference servers in order to maximize efficiency. Lastly, the configura-\ntion of the inference server of the code assistant plays a significant role in its\nenergy consumption. For instance, smaller and models tends to use less elec-\ntricity, reducing the number of GPUs decreases the energy consumption at\nthe cost of a higher latency, and some quantization techniques can positively\nboth the energy consumption and latency of the code assistant.\nOutline.\nFrom section 2 to section 4, we describe how we collected\nour dataset, performed our experiments, and explain the methodology we\nfollowed to analyze the results obtained.\nWe report in section 5 on the\nresults of our evaluation and provide a critical discussion in section 6. In\nsection 7, we discuss the limitations of our study. Finally, section 8 presents\nrelated works, and section 9 concludes the paper.\n2. Code Assistant Dataset\nTo investigate our research questions, it was imperative to measure the\nenergy consumption of a code assistant in a realistic usage setting. To that\nend, we designed an experiment with participants using GitHub Copilot\nto gather a dataset of development traces, enabling us to simulate develop-\ners using a code assistant on an inference server under our control. This\napproach was necessitated by our inability to access GitHub Copilot’s\ninference server and our objective to measure its power consumption. More-\nover, conducting this experiment in two distinct phases—(i) having partic-\nipants use a code assistant to develop a small application followed by (ii)\nexploring the energy consumption through traces replay—allowed us to gain\n4\n\nmore freedom when it came to controlling the configuration of the code as-\nsistant and facilitated the reproducibility of our experiment. Hereafter, we\nrefer to the traces dataset we collected as AssistantTraces.\n2.1. Involved participants\nWe recruited 20 volunteers among Computer Science (CS) students and\nCS professionals, using mailing lists and word of mouth. All of the partici-\npants were proficient in Java programming. 13 participants were between 18\nand 25 years old, 6 were between 26 and 35 years old, and 1 was between 36\nand 45 years old. 13 of them were students in CS (12 in a master’s degree and\n1 in a bachelor’s degree), and 7 of them were CS professionals. 1 participant\ndid not know GitHub Copilot before the experiment, 7 participants knew\nGitHub Copilot but never used it, 4 already used it a little, and 8 used it\nregularly. All of the participants were familiar with VSCode. The partici-\npants were compensated for their time with 50€ each. This experiment was\napproved by our institution’s Ethical Board.\nBefore the experiment, the 20 participants filled out a survey with their\nage, development experience, and familiarity with GitHub Copilot. Fol-\nlowing the experiment, 19 out of the 20 participants agreed to complete a\nsubsequent survey concerning their experience and feelings during the exper-\niment. This last survey was used to report ideas for future research.\n2.2. Assigned task\nThe task given to the participants consisted of developing a CLI Con-\nnect 4 game2 in Java in one hour using VScode and GitHub Copilot, it\nis derived from a project given in an university OOP introductory course. A\nskeleton of the project and associated unit tests were provided. The partici-\npants then had to design and write the game loop and logic and handle the\nboard display to the players using the CLI. The instructions for the partici-\npants, the skeleton project, and the projects created by the participants are\navailable in our replication package.\nThe participants used the GitHub Copilot VScode extension only\nthrough inline and panel completions; that is, they could not use other fea-\ntures of GitHub Copilot, such as the chat. They were also given access\nto the Internet while being forbidden from using other AI assistant (e.g.,\nChatGPT).\n2https://en.wikipedia.org/wiki/Connect_Four\n5\n\nWe collected GitHub Copilot’s telemetry by modifying the VScode\nextension and redirecting the telemetry to the computer of the participant.\nGitHub Copilot’s telemetry includes a plethora of different messages that\nenable us to retrace the history of a generation, from the moment GitHub\nCopilot decides to send a generation request to the moment of its accep-\ntance from the user.\nThe participants all used the same laptop: A Dell Latitude 7410, with\n32 GiB of memory, an Intel Core i7-10610U and a CML GT2 Mesa Intel graph-\nics card. It was running on Debian (bookworm). The monitor was a Dell U2720Q.\n2.3. Dataset description\nThe AssistantTraces dataset consists of all the telemetry sent by\nGitHub Copilot during the experiment in JSON format. There are 119, 774\ntelemetry messages in total, including 9, 633 generation requests. The dataset\nis available at https://doi.org/10.5281/zenodo.11503612.\n3. Experimental setup\nTo estimate the energy consumption of GitHub Copilot, we leveraged\nthe collected traces to simulate the developers’ behavior and the front end of\nthe code assistant on an inference server. Specifically, the inference servers\nwere run on a cluster, whose nodes consist of AMD EPYC 7513 (Zen 3), with\n512 GiB of memory and 4 Nvidia A100-SXM4-40GB (40 GiB). The server’s\ndistribution and OS were Debian 6 on Linux 5.10.0-28-amd64.\nAll the artifacts of this study, including our results, code, and datasets,\nare available in the following public repository: https://doi.org/10.5281/\nzenodo.13167546.\n3.1. Simulation client\nThanks to the telemetry data from the AssistantTraces dataset, we\ncould reproduce the API requests that GitHub Copilot sent to its in-\nference server. We developed a simulator (the client) to mimic developers’\ninteractions with GitHub Copilot by replaying generation requests to the\ninference server.\nThe main benefit of our approach is that we can make\nmultiple simulations with different scenarios by varying e.g., the number of\ndevelopers or the behavior of the code assistant.\nWhen performing simulations with concurrent developers, we limited the\nsimulation to the first hour of development to account for some developers\n6\n\nwho needed more than one hour to complete the task and to keep the simu-\nlation times short and manageable. For other developers who completed the\ntask in less than an hour, we delayed their start times so that the middle\nof each simulation aligned, thus creating a more realistic distribution of the\nserver’s workload. For example, a 1-hour telemetry session starts at minute\n0 and reaches its midpoint after 30 minutes, while a 50-minute session begins\nat minute 5 and also reaches its midpoint at minute 30.\nWhen simulating less than 20 developers, we repeated the simulation mul-\ntiple times with different developers until all were included (e.g., we repeated\na simulation of 2 developers 10 times with varying pairs of developers every\ntime, so that all 20 developers would be simulated). For simulations with\nmore than 20 developers, we randomly picked replicates among the 20 devel-\nopers. To avoid issues with simultaneous identical requests, we offset each\nduplicate by a random number of 0 to 30 seconds.\n3.2. Inference server\nTo handle generation requests and generate code suggestions, we set up\nan inference server.\nCode suggestions are generated using an LLMs.\nIn\nparticular, we used the Text Generation Inference (TGI) server,3 a\nserver for text generation inference that is easily configurable to operate\nwith different LLMs and that supports sharding between multiple GPUs and\nmultiple parallel requests (using continuous batching). The server was set up\nwith its default parameters, except for the number of shards, quantization\nmethod, and the number of concurrent requests which we describe in the\nnext section.\n3.3. Studied factors\nUsing the aforementioned setup, we performed several simulations with\nvarying elements in the setup’s configuration. We chose to study specific\nfactors because we hypothesized they would affect the energy consumption\nof the code assistant. The factors we studied are as follows:\nNumber of concurrent developers: We varied the number of devel-\nopers concurrently querying the code assistant ranging from 1 to 500 with\ndiscrete values: 1, 2, 5, 10, 20, 30, 50, 75, 100, 150, 200, 300, 400, 500.\nStreaming the requests: We emulated different request-sending behav-\niors from GitHub Copilot by activating and deactivating streaming when\n3https://github.com/huggingface/text-generation-inference\n7\n\nsending requests. By default, GitHub Copilot uses streaming, which al-\nlows it to cancel a previous request that is still generating when a new one\nis sent. Deactivating streaming signifies that every request that is sent by\nGitHub Copilot has to complete the triggered generation, even if it is no\nlonger useful for the user.\nManually triggering the code assistant: Typically, GitHub Copi-\nlot’s generation mechanism is triggered automatically. We also considered\nemulating a manual trigger for the generation behavior by only sending gen-\neration requests that were completed and displayed to the user based on\nthe assumption that the developer manually triggering GitHub Copilot\nwould wait for a response. While this method is not perfect, it serves as an\napproximation of the user behavior.\nCode assistant model: We tested three different LLMs from the Star-\nCoder family, namely: StarCoder (15.5B parameters) [8], StarCoder2-7b and\nStarCoder2-15b [9]. We chose these models as they are popular open-source\nmodels for code generation. The range of models allows us to see the impact\nof the size of the model, as well as its architecture. Indeed, StarCoder is\na decoder-only transformer with Multi-Query-Attention and positional em-\nbeddings, whereas StarCoder2 is a decoder-only transformer with Grouped-\nQuery-Attention and Rotary-Positional-Encodings.\nQuantization method: Quantization is a method used to reduce the\ncomputational and memory costs of running inferences. We studied mul-\ntiple quantization methods for running the models on the inference server:\nEETQ [10], BitsAndBytes-NF4, BitsAndBytes-FP4 [11] and no quantization\nat all.\nMaximum number of concurrent requests: We varied the number\nof concurrent requests on the TGI server which effectively modified the num-\nber of requests that could be queued simultaneously. When the server was\noverloaded, it returned an error to the caller.\nNumber of GPUs: All of our servers had 4 GPUs available which\nenabled us to run the simulations using a subset of the available GPUs, such\nas two or just one GPU. When using less than 4 GPUs, we considered the\nunused GPUs nonexistent and did not account for their consumption.\n3.4. Configuration space exploration\nWe define a configuration as a set of factors with a specific value. One\nexample of configuration is [20 concurrent developers; streamed requests;\n8\n\n1\n2\nRatio of accepted to \nrejected suggestions\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProportion of users\n0.10\n0.15\n0.20\nRatio of accepted to \nrequested suggestions\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProportion of users\n5\n10\n15\nRequests per minute\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProportion of users\n50\n100\n150\nTotal time taken (minutes)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProportion of users\nFigure 1: Various statistics on the participants’ usage of the code assistant and their time\ntaken to finish the experiment.\nThe first figure represents the ratio of the number of\nsuggestions accepted by the user to the number of suggestions by the user. The second\nfigure represents the ratio of the number of accepted suggestions to the total number of\nsuggestions requested by the code assistant. The third one focuses on the frequency of\nrequests made by the participant, and the last figure represents the time taken by the\nparticipant to finish the experiment.\nrequests are automatically triggered; the model used is StarCoder2-7b; no\nquantization method; 4 GPUs].\nThe combination of these multiple factors and their varying modalities\nresults in 4, 896 unique possible configurations. Considering each configura-\ntions takes between 1 and 20 hours to simulate and measure, we decided not\nto explore the whole configuration space. To keep the exploration manage-\nable yet informative, some factors were fixed while exploring the impact of\nothers (e.g., fixing the model and number of GPUs while varying the num-\nber of developers or the quantization method). In total, we performed 829\nsimulations with 314 unique configurations. As explained in subsection 3.1,\nconfigurations with less than 20 concurrent developers needed to be simulated\nmultiple times (2 for 10 developers, 4 for 5 developers, 10 for 2 developers,\n20 for 1 developer), which partly explains why there are more simulations\nthan configurations. Moreover, we simulated some other configurations up\nto five times to measure the stability of our setup.\n3.5. Energy consumption measurements\nWe measured the energy consumption of the inference server’s CPU and\nGPU using perf and nvidia-smi utilities, respectively. We also collected\nthe time taken for generations to complete (latency), the number of rejected\n9\n\nrequests (due to server saturation), and the number of completed generation\nrequests. When measuring a configuration multiple times, we found that the\nstandard deviation of the power consumption was only 1.4% of the mean\npower consumption. We considered this level of standard deviation accept-\nable and concluded that our measuring setup was stable. To estimate the\ncarbon emissions related to the energy consumption measured during our ex-\nperiments, we considered France’s 2023 carbon intensity, equivalent to 56 g\nof CO2 per kWh [12].\n4. Data analysis\nIn this section, we describe our methodology for analyzing the results we\nobtained from section 3.\n4.1. Simulations analysis\nTo get an overview of the data, we computed the mean power consump-\ntion (in Watts) and total energy used (in Wh) for every simulation, as well\nas the mean latency, the number of rejected requests, and the number of\ncompleted generations. We also derived the power consumption per devel-\noper, which allocates an equal share of the server’s energy consumption to\nevery developer using the code assistant. When analyzing a simulation with\nmultiple concurrent developers, only the period in the simulation where all\ndevelopers were active in parallel was considered.\nServer saturation.\nWhen the server receives more requests than it\ncan handle, the number of rejected requests or the latency may increase,\ndepending on the client and server configuration. We consider that a server\nis saturated when the number of rejected requests exceeds 10% or when the\nmean latency of the requests exceeds 20 seconds. Note that this saturation\nthreshold was set arbitrarily; searching for an ideal saturation threshold is\nout of the scope of this paper.\nMeasuring the impact of a factor. To assess the impact of a factor\non energy consumption or latency, we compared pairs of configurations that\ndiffered only by the factor being studied.\nThis method ensures that any\nobserved differences are solely due to the factor in question.\n4.2. Participant analysis\nFor each participant, we performed analyses using the gathered teleme-\ntry and survey data. Specifically, we calculated the number of generations\n10\n\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\nEnergy consumption ratio (the lower the better)\n128 requests \n 1000 requests\nStarCoder2 15B \n StarCoder2 7B\nStarCoder \n StarCoder2 7B\nStarCoder \n StarCoder2 15B\n1 GPU \n 2 GPUs\n2 GPUs \n 4 GPUS\n1 GPU \n 4 GPUS\nBitsAndBytesNF4 \n EETQ\nBitsAndBytesFP4 \n EETQ\nNo quantization \n EETQ\nNo quantization \n BitsAndBytesFP4\nNo quantization \n BitsAndBytesNF4\nBitsAndBytesFP4 \n BitsAndBytesNF4\nStreaming disabled \n Streaming enabled\nAutomatic trigger \n Manual trigger\nFactors and options\nMax concurrent requests\nModel\nNumber of GPUs\nQuantization\nStreaming\nTrigger mechanism\nFigure 2: Energy impact ratio from switching from one option to another. A ratio of 1\nmeans no change, a ratio of 2 means the energy consumption doubled, and so on. Points\ncorrespond to the ratio in energy when comparing neighboring configurations.\n0\n1\n2\n3\n4\n5\n6\n7\nLatency ratio (the lower the better)\n128 requests \n 1000 requests\nStarCoder2 15B \n StarCoder2 7B\nStarCoder \n StarCoder2 7B\nStarCoder \n StarCoder2 15B\n1 GPU \n 4 GPUS\n1 GPU \n 2 GPUs\n2 GPUs \n 4 GPUS\nBitsAndBytesFP4 \n EETQ\nBitsAndBytesNF4 \n EETQ\nBitsAndBytesFP4 \n BitsAndBytesNF4\nNo quantization \n EETQ\nNo quantization \n BitsAndBytesFP4\nNo quantization \n BitsAndBytesNF4\nStreaming disabled \n Streaming enabled\nAutomatic trigger \n Manual trigger\nFactors and options\nMax concurrent requests\nModel\nNumber of GPUs\nQuantization\nStreaming\nTrigger mechanism\nFigure 3: Latency impact ratio from switching from one option to another. A ratio of 1\nmeans no change, a ratio of 2 means the latency doubled, and so on. Points correspond\nto the ratio in latency when comparing neighboring configurations.\n11\n\nrequested, displayed, and accepted, and those remaining in the code after a\ncertain period. To determine which generation requests remain in the par-\nticipant’s code, we leveraged GitHub Copilot’s telemetry, which indicates\nwhether a generation is still present in the code. This is assessed using edit\ndistance at the character and word levels, with the generation considered\nremoved when the word edit distance falls below 50% [7].\nIn Figure 1, we share various statistics on the participants code assistant\nusage and time taken to realize the experiment. We observe variations in the\nway the participants use GitHub Copilot, notably in the ratio of accepted\nsuggestions over the number of rejected suggestions or total sent generations\nrequests. We also notice that some participants trigger generations more fre-\nquently than others. Lastly, there is also a great variation when it comes to\nthe time the participant took to finish the experiment, indeed, 5 participants\nrequested to have more than one hour to finish, and 6 completed the task\nwithin 40 minutes. Out of the 20 participants, 9 did not complete the task\nthey were given and decided to end the experiment at the one hour mark.\nWhen calibrating the duration and complexity of the task given to the partic-\nipants, we aimed at having the most development traces as possible. Thus,\nwe chose to reduce the chances of the participants finishing early, thereby\nincreasing the chances of the participant finishing late or giving up after one\nhour. We find that participants not completing the whole task is not an issue\nfor our experiment, as the usage of the code assistant roughly stays the same\nduring the whole task.\n5. Results\nIn this section, we summarize the key observations from our experiment\nand answer our research questions. We make our complete results available\nin the companion notebook in the replication package.\n5.1. Relationship between participant’s characteristics and their usage of GitHub\nCopilot\nBefore answering any of the research questions, we wanted to have a look\nat the relationship between some of the participant’s characteristics, such\nas their experience, familiarity with GitHub Copilot, if they coded in java\nduring the last year, or whether or not they finished the task, and their usage\nof GitHub Copilot, that is, the rate at which the participants made requests,\nand the ratio of code suggestions they accepted when presented to them. We\n12\n\nMetric\nIndependent variable\nGroups\nTest used\nStat\nP-value\nSignificant\nRatio of accepted suggestion\nover rejected suggestions\nComputer experience\nprofessional, student\nt-test\n-2.496\n0.022\n✓\nGitHub Copilot familiarity\nregularly, sometimes, never used\nANOVA\n1.743\n0.205\n✗\nCoded Java in the last year\ndid code in java, didn’t code in java\nt-test\n-0.570\n0.576\n✗\nFinished\nfinished, didn’t finish\nt-test\n0.127\n0.900\n✗\nRequests per minute\nComputer experience\nprofessional, student\nt-test\n0.808\n0.429\n✗\nGitHub Copilot familiarity\nregularly, sometimes, never used\nANOVA\n1.034\n0.377\n✗\nCoded Java in the last year\ndid code in java, didn’t code in java\nt-test\n0.243\n0.811\n✗\nFinished\nfinished, didn’t finish\nt-test\n0.799\n0.434\n✗\nTable 1: Inferential statistics of the effect of three independent variables on two metrics.\nThe significance was asserted using a Benjamini-Hochberg correction with a False Discov-\nery Rate of 0.20\nperformed this investigation in order to find if any noticeable effect could\nbias our subsequent evaluations, so the metrics were chosen because they\nbecome important later on. In total, we performed 8 post-hoc tests, which\nare described in Table 1. In order to reduce the Type I error rate (false\npositives), we also performed a Benjamini-Hochberg procedure to determine\nthe significance of the tests using a false-discovery rate (FDR) of 0.20.\nFrom the data in Table 1, we cannot conclude that there is any effect\nbetween most of the characteristics studied and the studied metrics, except\nbetween the computer experience and the ratio of accepted suggestions over\nrejected suggestions.\nWhen analyzing the data, we see that professional\ndevelopers have a ratio of 0.67 whereas students have a ratio of 1.26. This\nindicates that the professional developers in our sample are more conservative\ntowards the suggestions made by GitHub Copilot compared to the students.\nThe students tend to accept the suggestions made by Copilot twice as much\nas the professional developers.\n5.2. RQ1: What is the impact of studied factors on the energy consumption\nand performance of code assistants ?\nIn this section, we report on our findings on the different studied factors\nand their impacts. In Figure 2 and Figure 3, we depict the impact of switch-\ning from one option to another on energy consumption and request latency,\nrespectively.\nThe impacts were calculated using the method described in\nsubsection 4.1. Each hue represents one factor, while each row represents the\nswitch from one option to the next. The X-axis reflects the ratio in measured\nlatency or energy consumption between the first and second options.\nNumber of concurrent developers.\nWhen increasing the number\nof concurrent developers, we observe an increase in the average power con-\n13\n\n25\n50\n75\n100\n125\n150\n175\n200\n1\nNumber of developers using the server concurrently\n0\n50\n100\n150\n200\n250\n300\n350\nPower consumption per developer (W)\n0\n200\n400\n600\n800\n1000\nMean power consumption (W)\n0\n50\n100\n150\n200\nMean latency (s)\nPower consumption per developer (W)\nMean power consumption (W)\nMean latency (s)\nFigure 4: Evolution of the average power consumption and the latency depending on the\nnumber of developers, for the following configuration: StarCoder2-7B; no quantization; no\nstreaming; manual trigger; maximum 1000 concurrent requests; 4 GPUs. The latency and\npower consumption are superposed in order to easily visualize how they interact when the\nnumber of developer increases.\nsumption of the machine and the latency of the server up to a certain point.\nThanks to the continuous batching techniques used by TGI, adding more\ndevelopers only marginally increases the latency and consumption of the\nserver, thus reducing the energy consumption per developer. This is illus-\ntrated in Figure 4, where we can observe that the energy consumption per\ndeveloper decreases whenever a developer is added. On the other hand, with\ntoo many developers, the latency becomes excessively high, making the code\nassistant unusable. With the configuration shown in Figure 4, the average\nlatency reaches 16 seconds with 75 concurrent developers, and increases ex-\nponentially from that point with each developer added (50 seconds at 100\ndevelopers, 210 seconds at 150 developers, etc.). It finally reaches a plateau\npast 150 concurrent developers as the server reaches the maximum number\nof concurrent requests limit and starts rejecting new requests.\n14\n\nStreaming the requests. Enabling streaming to send requests—i.e.,\ncanceling previous generation requests when a new one arrives—reduces\nserver latency by 62%, on average, and reduces power consumption by 7%.\nThe lesser reduction in power consumption compared to the reduction in la-\ntency is due to the inference server still spending most of its time generating\nresponses. As a result of the lower latency, streaming allows more concurrent\ndevelopers to use the assistant.\nManually triggering the code assistant.\nManually triggering the\ncode assistant by only requesting the generations that were proposed to the\ndeveloper, reduces energy consumption by 15% and reduces latency by 35%.\nThe highest reduction in energy consumption (25%) is observed with the\nconfiguration [StarCoder2; no quantization; no streaming automatic trigger;\nmaximum 1000 concurrent requests; 2 GPUs; 5 developers], which also re-\nduces the latency by 25%. On the other hand, the lowest reduction of the\nenergy consumption (5%) is observed with the configuration [StarCoder2-7b;\nno quantization; no streaming; automatic trigger; maximum 1000 concurrent\nrequests; 4 GPUs; 75 developers] which, however, reduces the latency by 93%.\nIn this specific configuration, enabling an automatic trigger made the server\nsaturate (226 seconds of latency and 59% of rejected requests), while using\na manual trigger allowed it to handle the load from the 75 developers (14.6\nseconds of latency and 0% of rejected requests).\nQuantization. Quantization generally increases latency. Notably, em-\nploying the method EETQ over no quantization at all increases the latency\nby 3.1%, and using BitsAndBytes-NF4 or BitsAndBytes-FP4 increases the la-\ntency by 138.6% and 156%, respectively. In some cases, the use of EETQ\nalso reduces energy consumption: on average, it reduces energy consumption\nby 1.1%, but it can reduce it by up to 12.6% in the case of the configu-\nration [StarCoder; streaming; automatic trigger; maximum 1000 concurrent\nrequests; 4 GPUs; 5 developers]. BitsAndBytes, however, slightly increases\nthe power consumption by 5% on average.\nModel. Using LLMs with fewer parameters reduces energy consumption\nand latency. For instance, switching from StarCoder2-15B to StarCoder2-7B\nreduces energy consumption by 15.6% and latency by 10.0%. However, the\nmodel architecture also plays an important role in latency. For example,\nwhile StarCoder (15.5B) and StarCoder2-15B exhibit a similar number of\nparameters and power consumption, using the latter over the former increases\nthe latency by 149% on average. We believe this might be due to differences\nin the architecture of the two models. For instance, StarCoder uses Multi-\n15\n\nQuery-Attention, whereas StarCoder2 adopts Grouped-Query-Attention.\nMaximum number of concurrent requests. Increasing the maxi-\nmum number of concurrent requests allowed by the TGI server does not af-\nfect the energy consumption of the server, but greatly increases latency when\nthere are too many concurrent developers—i.e., up to 597% increase with\nthe configuration [StarCoder2-7B; EETQ; no streaming; automatic trigger;\n4 GPUs; 50 developers]. Providers should prefer having a lower maximum\nto reject the requests early rather than making the users wait for several\nminutes.\nNumber of GPUs. As intuitively expected, reducing the number of\nGPUs allocated to the inference server reduces the energy consumption of\nthe server and increases latency. For example, using 4 GPUs instead of 2\nincreases consumption by 51.8% and can decrease the latency in some cases,\nhence enabling some more concurrent developers to use the code assistant\nas more memory is available. Downsizing the number of GPUs can be a\nviable option for saving energy if the number of concurrent developers is low\nenough.\nUsage of the code assistant. Participants’ usage of GitHub Copi-\nlot varied, with the average amount of requests per minute ranging from 1.9\nto 14.7, overall averaging 9 requests per minute. We observe that the more\na developer uses GitHub Copilot (i.e.,, the more generation requests are\nmade), the more power consumption increases (correlation of 0.93). We infer\nthat in its current state, the usage of a code assistant is directly correlated\nwith the amount of code a participant writes—i.e., the more time a par-\nticipant spends writing code in the IDE, the more a code assistant triggers\ngenerations.\nRQ1: Various factors significantly impact the energy consumption and\nperformance of code assistants. Increasing the number of concurrent de-\nvelopers improves energy efficiency, but risks server saturation. Stream-\ning requests and manually triggering generations both significantly re-\nduce energy consumption and latency. Quantization methods and the\nchoice of LLM also affect performance, with smaller models showing bet-\nter efficiency. Adjusting the number of GPUs is crucial for optimizing\nenergy use. Our findings, therefore, confirm the importance of carefully\nselecting and optimizing these factors.\n16\n\n5.3. RQ2: How many generation requests made by GitHub Copilot are\nactually useful?\nFigure 5 illustrates the final state of generation requests sent by GitHub\nCopilot during our experiment. Out of the 9, 634 generation requests made\nby GitHub Copilot over the 20 participants, only 2, 944 finished generating\nand were displayed to the user (30%). Of these, 1, 066 were accepted (11%\nof all requests), and 816 (8.5%) were kept in the code at the end of the\nexperiment according to GitHub Copilot.\nOur analysis reveals that the majority of generation requests made by\nGitHub Copilot were canceled, often because they were started while the\nuser was still typing, leading to new requests that replaced the previous\nones. Empty completions typically occur at the end of a sentence, or before\na closing parentheses or brackets. The completions that were displayed but\nnot accepted are most likely due to either the users not wanting or needing\nthe suggestion in the first place, or to the suggestions not matching what the\nuser expected.\nPreviously, we discussed the energy implications of these inefficiencies\nwhen presenting the energy saving of manually triggering GitHub Copilot.\nBy eliminating unnecessary requests, i.e., canceled and empty requests, we\ncould save 15% of energy on average. This relatively modest energy saving,\ndespite removing almost 70% of the generations is due to the lower-than-\naverage computing time used by canceled and empty generations, compared\nto completed generations. Assessing the energy impact of suggestions dis-\nplayed, but not accepted, by users is left for future research.\nRQ2: An analysis of GitHub Copilot’s generation requests reveals a\nsignificant share of them are not useful to the end user, indicating that\nmany generation requests are either unnecessary or not helpful to users.\nThis inefficiency highlights the potential for substantial energy savings by\noptimizing when and how generation requests are made, possibly through\nmore intelligent triggering mechanisms or better user interaction designs.\n5.4. RQ3: Under different scenarios and objectives, how much does a devel-\noper using a code assistant such as GitHub Copilot cost in energy?\nFor the sake of simplicity, this section only focuses on a subset of configu-\nrations and their impacts, given the large number of different configurations\n17\n\nCancelled\nbefore completion (55.4%)\nEmpty result (13.8%)\nDisplayed,\nbut not accepted (19.5%)\nAccepted but\nnot in code anymore (2.6%)\nAccepted and \nstill in code (8.5%)\nFigure 5: Percentage of requests sent by GitHub Copilot depending on their completion\nstate. Red-tinted categories represent requests that did not benefit the user. Blue-tinted\ncategories represent requests that benefited the user.\navailable (314). However, the entire set of configurations can be found in our\ncompanion notebook. When presenting these results, we assume that the\nserver load is constant—i.e., all developers are working simultaneously—and\nthat the server is turned on only when the developers are working.\nTo select the configurations, we defined three different development sce-\nnarios, each with two objectives. The goal of these scenarios is to highlight\nhow different use cases require different configurations and have varying im-\npacts. The scenarios are as follows:\n• Small team: 5 developers concurrently requesting a single dedicated\nserver machine.\n• Medium team: 20 developers concurrently requesting a single dedi-\ncated server machine.\n• Distributed service (e.g., GitHub Copilot): A large number of\ndevelopers (sharing) requesting many server machines. The aim is to\nmaximize the number of developers per machine while not saturating\nthe servers. We report on the impact of a single machine in this sce-\nnario.\nThe two objectives a development team may aim for we considered in our\nstudy are as follows:\n18\n\nSmall team\nMedium team\nDistributed service\nScenario\n0\n20\n40\n60\n80\n100\n120\nHourly consumption of a single dev (Wh)\n126.62\n44.94\n20.76\n49.87\n18.16\n11.45\nobjective\nfrugal\nperformance\nFigure 6: Hourly energy consumption (Wh) of a single developer based on the objective\nand the number of developers using the assistant.\n• Performance: By design, the generation requests are triggered auto-\nmatically, and the previous ones are canceled (using streaming). The\nlatency between requests is thus minimized.\nGenerations are trig-\ngered automatically so that the developer always receives suggestions\nas quickly as possible. This is also the default triggering and streaming\nmode of GitHub Copilot.\n• Frugality: The energy consumption per developer is minimized. The\ngeneration requests are triggered manually, and the previous ones are\nnot canceled (not using streaming), as we expect the developers to wait\nfor their manually triggered generation requests to finish.\nTable 2 shows the best configurations for each scenario and objective.\nFor each of the 6 configurations, its latency and various energy consumption\nmetrics are presented. To better put in perspective the energy usages of the\nconfigurations, we also show them in Figure 6. As we can see from both\nthe table and the figure, the energy consumption of a single developer varies\n19\n\nsignificantly across the different configurations. The setup machine’s high\nidle power consumption (˜270 W for four GPUs and one CPU) results in a\nsubstantial per-developer energy impact when fewer developers utilize many\nGPUs. Conversely, increasing the number of concurrent developers reduces\nindividual energy consumption by leveraging the continuous batching from\nthe TGI server, aligning with the findings discussed in Section 5.2.\nIf GitHub Copilot were using a setup similar to ours with maximized\nserver load, the average power consumption per developer could range from\n10 W to 20 W depending on their objective. However, with dedicated servers\nfor small teams, under-utilization could result in a high power consumption\nper developer (˜120 W). This can be mitigated by using fewer GPUs and\nsmaller models, or considering server sharing.\nTo put it in perspective, we measured the energy of the laptop and the\nmonitor of the last two participants using a power-meter. The average power\nconsumption of the laptop and monitor when used by our participants was\n19.7 W and 18 W, respectively. Thus, in GitHub Copilot’s best-case sce-\nnario using our setup, the power consumption per developer would be about\n58% the consumption of a laptop’s consumption, and at worst it would be\nroughly equal to that of a laptop.\nWe observe that all performance configurations uses StarCoder due to\nits low latency. However, among the three studied models, StarCoder is the\nworst at generating code: StarCoder has a 33.6 pass@1 on HumanEval while\nStarCoder2-7B and 15B have 35.4 and 46.4 pass@1 [8, 9], respectively.\nRQ3: The energy cost for a developer using a code assistant like GitHub\nCopilot varies significantly based on its configuration.\nSmall teams\nwith dedicated servers can see high per-developer energy consumption\n(˜120 W), while large-scale services can achieve much lower consumption\n(10–20 W) by maximizing server load. Overall, our results emphasize the\nimportance of choosing appropriate configurations based on team size\nand objectives to optimize both energy consumption and performance\nwhen using code assistants.\n6. Discussion and implications\nIn this section, we discuss the results reported in the previous section and\ntheir implications.\n20\n\nScenario\nSmall team\n(dedicated server)\nMedium team\n(dedicated server)\nDistributed service\nObjective\nfrugal\nperformance\nfrugal\nperformance\nfrugal\nperformance\nNumber of concurrent developers\n5\n5\n20\n20\n75\n50\nModel\nStarCoder2-7B\nStarCoder\nStarCoder2-7B\nStarCoder\nStarCoder2-7B\nStarCoder\nQuantization method\n-\n-\n-\n-\nEETQ\n-\nNumber of GPUs\n1\n4\n1\n4\n4\n4\nStreaming\n✗\n✓\n✗\n✓\n✗\n✓\nManual trigger emulation\n✓\n✗\n✓\n✗\n✓\n✗\nAverage latency (s)\n6.4\n1.2\n7.7\n1.7\n16.5\n2.3\nAverage server power (W)\n249.3\n633.1\n363.2\n898.8\n858.9\n1038.2\nEnergy per 1000 generation requests (Wh)\n12.0\n30.7\n0.9\n2.2\n0.1\n0.4\nEnergy per hour per developer (Wh)\n49.9\n126.6\n18.2\n44.9\n11.4\n20.8\nCO2 emissions per hour per developer (g)\n2.8\n7.1\n0.9\n1.0\n0.6\n1.2\nTable 2: Optimal configurations for each scenario and objective, and their energy and\nperformance impacts.\nOn energy usage. As reported in our results, the configuration choice\nfor a code assistant significantly impacts per-developer power consumption.\nHowever, a considerable portion of the energy spent on generations is wasted\ndue to cancellations and disinterest from users. Substantial energy savings\ncould be achieved by requiring users to manually trigger GitHub Copi-\nlot and encouraging them to rethink their interactions with the tool, or\nby enhancing the request-triggering mechanism. For instance, Mozannar et\nal. [13] proposed a novel method for triggering generation requests by lever-\naging human feedback collected from GitHub Copilot’s telemetry. Fur-\nthermore, Barke et al. [14] showed that developers exhibited two kinds of\nbehaviors when interacting with GitHub Copilot—acceleration mode and\nexploration mode—which could also be leveraged to adapt the triggering\nmechanism of GitHub Copilot.\nOne of our objectives with this paper is to raise awareness among prac-\ntitioners, tool providers and researchers about the environmental impact of\nusing code assistants. Developers using code assistants can significantly re-\nduce their energy usage by manually triggering the generations and using the\nassistant in a more conscious manner to avoid unnecessary generations. On\nthe other hand, the providers of the assistants should maximize the number\nof users per inference server and carefully select the quantization method so\nas to decrease the resource usage. The choice of the model is also paramount,\nas it dictates the amount of memory left for batching requests, and the com-\n21\n\nputational cost of a single request.\nWe encourage researchers to pursue our work by finding more ways to\nenable users and providers to reduce the energy consumption of their code\nassistants. One avenue of work is to evaluate the environmental footprint\nof the assistant using life-cycle analysis, in order to incorporate the hard-\nware and training costs. Lastly, while code assistants and more specifically\nGitHub Copilot can improve developer productivity [15], the rebound ef-\nfects [16] stemming from faster development needs to be assessed. We hope\nthat our dataset, AssistantTraces, will serve as a valuable resource for\nresearchers and tool providers. We encourage researchers to build upon As-\nsistantTraces, enhance its collection methodology, and refine it through\ntheir own user studies.\nOn the number of developers.\nOur results indicate that using a\ninference server for a small number of developers can be inefficient in terms\nof hardware and energy. Optimizing the number of developers per machine\nimproves utilization, but a balance is needed between developer load and\ncode assistant performance.\nHaving few developers complicates hardware\nand model selection, as smaller GPUs may struggle with large models, while\nsmaller models may generate lower-quality code. In this case, practitioners\nshould consider sharing hardware and its costs with others, so that they may\nall benefit from higher-end hardware and a lower individual impact.\nWe\nalso encourage researchers to develop and improve sharing models for LLM\ninference, such as Petals [17].\nOn the difference in latency between StarCoder models.\nWe\nobserved a sharp increase in latency between the StarCoder and StarCoder2-\n15B models. While we believe this is due to the architecture changes intro-\nduced by the StarCoder2 model [9], more research is needed to get a better\ninsight into the reasons for such an increase. As latency plays a key role\nin the maximum number of concurrent developers an inference server can\nhandle, finding ways to reduce said latency could help further improve the\nefficiency of code assistants.\nOn quantization. In our results, we found that quantization effectively\nincreased the latency and had close to no effect on the energy consumption.\nWe find this result surprising considering quantization reduces the LLM’s\nsize. It is possible that these specific quantization method increase the com-\nputational cost of inference. We advise researchers to investigate the reasons\nfor this effect, and explore the energy savings of other quantization methods.\nOn the generalization of our results. Our study uses one specific\n22\n\nsetup and varies its configuration. While the energy consumption we ob-\nserved in our different scenarios give a good idea of the scale of the energy\nrequired to run a code assistant, the observed energy consumption is specific\nto the context of our experiment, and should not be generalized to other code\nassistants, especially GitHub Copilot. However, even though other code\nassistants might use a different hardware, model or serving framework, they\ncan still benefit from our results. Another setup, completely different from\nours, with users making a different amount of requests per minute (which is\nhighly correlated with the energy consumption of the assistant) would still\nsave energy by maximizing the number of concurrent developers per machine,\nby using smaller models, by having the users manually trigger the generations\nand by cancelling them early if they’re not needed anymore, etc.\nOn the environmental footprint of the code assistants We only\ncalculated the CO2 emissions that were due to the energy consumption of\nthe inference server. Future studies could also include the other sources of\nimpact such as the hardware the server is hosted on, and the datacenter\nits in. This could also be an opportunity to present other metrics such as\nresource depletion or water use, which can be as important as CO2 emissions\nwhen talking about ICT. [18]\n7. Limitations\nIn this section, we address several limitations of our study:\nParticipants and task selection. Selecting participants by word of\nmouth, and mailing lists from university students and professionals allowed\nus to easily recruit enough qualified participants for our study. Moreover,\nthe task assigned to the participants was designed to be short and simple for\nan experimental setting, yet long and complex enough to simulate a realistic\ndevelopment session. One alternative was to perform our dataset collection\non developers working for their companies or school projects. While this\nalternative could have provided us with more realistic development traces,\nit would have been logistically harder to perform. Another alternative was\nto assign multiple short tasks, following Vaithlingam et al. [19], which could\nsimplify generalization, while compromising the realism of the full develop-\nment process, including including design, debugging, and testing.\nHaving a non-representative sample of the population of developers and a\nnot complex-enough task may affect some of our results such as the number\nof accepted/rejected generation requests, as well as the number of requests\n23\n\nper minute made by the participants. As the rate of generation requests is\nhighly correlated (0.93) with the energy consumption of code assistant, the\nraw energy consumption numbers would also be affected. If our sample was\nindeed biased, we believe it would not affect the results from RQ1 and RQ3,\nas our experiment focuses mainly on the impact of changing configuration\noptions of the code assistant. Another setup, completely different from ours,\nwith users making a different amount of requests per minute (which is highly\ncorrelated with the energy consumption of the assistant) will still save energy\nby maximizing the number of concurrent developers per machine, by using\nsmaller models, by having the users manually trigger the generations and\ncancel them early if they’re not needed anymore, etc.\nConfiguration space and exploration. To keep the number of sim-\nulations low, we had to limit both the configuration space’s size and its\nexploration. Indeed, we restricted our evaluation to three models from the\nStarCoder family and ran our simulations without varying the hardware. Ad-\nditionally, we only considered a subset of TGI parameters. As a result, some\nconfigurations yielding different or optimal results might not be explored.\nWe mitigated this effect by curating the configurations to explore so that\nthey cover a large spectrum.\nEmulation of a manual trigger of a code assistant. Our method of\nemulating the manual triggering of GitHub Copilot by only sending gen-\neration requests displayed to the user is only an approximation that allowed\nus to save time and resources by simplifying our dataset collection process. In\nreality, user behavior may differ, and this emulation might not fully capture\nthe nuances of manual interactions. A dedicated study involving real users\nmanually triggering generations is needed to obtain more accurate insights.\nSimulation of the developers. Simulating the developers instead of\nperforming live measures allowed us to have more freedom and explore more\nconfigurations. In return, our simulations do not allow for any kind of change\nin behavior from the simulated developer—i.e., the same request will be sent\nat the same times no matter the latency or quality of the responses. An\nalternative was to measure the energy as the participants were developing\nand modify GitHub Copilot to use our inference server. While this would\nyield more accurate reactions from the developers, it would greatly limit the\namount of configurations we could explore. A compromise was to develop\nautonomous agents simulating the developers, which we deemed to difficult\nto realize.\nQuality of the generated code. When performing the simulations, we\n24\n\ndecided not to assess or take into consideration the quality or correctness of\nthe code generated by the LLMs. It was possible to estimate the quality of the\ncode by comparing it to the code written to the developer using a BiLingual\nEvaluation Understudy (BLEU) score [20]. However, as shown by Chen et\nal. [21], any score relating to the proximity between two texts is unreliable\nat best when evaluating LLMs for code. Another alternative was to evaluate\nthe generations by hand, which would have been too much time intensive.\nA third alternative was to consider the reported pass@k from the model’s\ncreators, which could have been easy to implement, but would have made\nour search of optimal configuration harder in subsection 5.4. The rationale\nfor our choice can be explained by the lack of methods to evaluate the quality\nof the generated code in the context of our experiment and the simplicity of\nnot considering the code quality when searching for the best models under\nspecific scenarios and objectives. Nevertheless, the trade-off between energy\nconsumption, performance, and code quality is a consideration that should\nbe addressed in future research.\n8. Related Works\nPrevious research have investigated various properties of LLMs special-\nized in code generation: previous research has investigated various aspects of\nLLMs for code-related tasks, including the security of their suggestions [22,\n23, 24], the prevalence of bugs in the generated code [25], how developers\ninteract with them\n[14, 19, 24] or just the quality and correctness of the\ncode they generate [6, 26, 27, 28]. There have also been efforts to measure\nthe efficiency of LLMs through the creation of benchmarks for comparing\nthem, such as HumanEval [21], MBPP [29], CoderEval [30], APPS [31],\nCodeXGLUE [32] or ReCode [33]. Xu et al. [34] also conducted a com-\nparative evaluation of multiple LLMs for code.\nSpecifically, researchers have also investigated the energy consumption\nand carbon footprint of various aspects of Deep Learning, such as computer\nvision [3, 35, 36] and natural language processing [3, 4].\nNotably, in the\ncontext of LLMs, Vartziotis et al. studied the Green Capacity of LLMs for\ncode [4]. They quantified the sustainability awareness of ChatGPT, GitHub\nCopilot and CodeWhisperer, based on the energy consumption and perfor-\nmance of the solutions they generated for Leetcode problems. Coignion et al.\nstudied the performance of the code generated by multiple LLMs on Leet-\ncode, and found that the LLMs studied had similar code performance [5].\n25\n\nLuccioni et al. measured the inference cost in energy and carbon of multiple\nLLMs, and found that multi-purpose LLMs are orders of magnitude more\nexpensive than specialized LLMs [3, 35].\nSamsi et al. benchmarked the inference performance and energy costs of\ndifferent sizes of LLaMa models on two GPU types [2]. The study provided\nthe words per second performance of the LLaMa models, as well as the\nenergy per second, per decoded token and per response. Multiple factors\nwere studied, such as the dataset used to perform the generations, the batch\nsize, the type of GPU and the length of the generation. Chakravarty et al.\nanalyzed the effect of quantization on the energy efficiency, accuracy, memory\nusage, and speed of speech recognition models [37].\nOur study stands out by specifically examining the energy consumption of\nLLM-based code assistants during real-world developer interactions. Unlike\nprevious research that focused on LLM training impacts or isolated inference\nimpacts, we analyze the energy usage of code assistants in a practical setting.\nOur dataset and methodology allow us to explore various factors influencing\nenergy consumption, such as the number of concurrent developers, offering\ninsights for service providers and end-users to optimize resource usage. By\naddressing these gaps and highlighting practical implications, our research\ncontributes to Green AI, making it one of the first investigations into the\nenergy consumption of code assistants.\n9. Conclusion\nIn this study, we explored the energy consumption of code assistants\npowered by LLMs, focusing on GitHub Copilot. Our findings indicate that\nvarious configuration factors, such as the number of concurrent developers,\nmodel size, and use of streaming, impact both the energy consumption and\nperformance of these tools. Notably, a substantial amount of energy is spent\non generation requests that are ultimately unused, highlighting an area for\npotential improvement.\nOur results indicate that significant energy savings can be achieved by\nservice providers and end-users if they take the following steps: disable the\nautomatic triggering of code assistant suggestions on the client side, utilize\nbatching techniques to maximize inference server usage and support more\nconcurrent developers, and reduce model resource consumption through ef-\nfective quantization and the use of more efficient models.\n26\n\nOverall, while LLM-based code assistants offer productivity benefits, it is\ncrucial to consider their environmental impact. By adopting more efficient\nconfigurations and usage practices, we can make these tools more sustainable\nwithout compromising their utility.\nAcknowledgment\nThis work received support from the French government through the\nAgence Nationale de la Recherche (ANR) under the France 2030 program,\nincluding partial funding from the CARECloud (ANR-23-PECL-0003), DIS-\nTILLER (ANR-21-CE25-0022), and KOALA (ANR-19-CE25-0003-01) projects.\nExperiments presented in this paper were carried out using the Grid’5000\ntestbed, supported by a scientific interest group hosted by Inria and including\nCNRS, RENATER and several Universities as well as other organizations.4\n4See https://www.grid5000.fr\n27\n\nReferences\n[1] R. Schwartz, J. Dodge, N. A. Smith, O. Etzioni, Green AI, Communi-\ncations of the ACM 63 (12) (2020) 54–63. doi:10.1145/3381831.\n[2] S. Samsi, D. Zhao, J. McDonald, B. Li, A. Michaleas, M. Jones, W. Berg-\neron, J. Kepner, D. Tiwari, V. Gadepally, From Words to Watts: Bench-\nmarking the Energy Costs of Large Language Model Inference, in: 2023\nIEEE High Performance Extreme Computing Conference (HPEC), 2023,\npp. 1–9. doi:10.1109/HPEC58863.2023.10363447.\n[3] S. Luccioni, Y. Jernite, E. Strubell, Power Hungry Processing: Watts\nDriving the Cost of AI Deployment?, in: The 2024 ACM Conference\non Fairness, Accountability, and Transparency, ACM, Rio de Janeiro\nBrazil, 2024, pp. 85–99. doi:10.1145/3630106.3658542.\n[4] T. Vartziotis, I. Dellatolas, G. Dasoulas, M. Schmidt, F. Schneider,\nT. Hoffmann, S. Kotsopoulos, M. Keckeisen, Learn to Code Sustain-\nably: An Empirical Study on LLM-based Green Code Generation (Mar.\n2024). arXiv:2403.03344.\n[5] T. Coignion, C. Quinton, R. Rouvoy, A Performance Study of LLM-\nGenerated Code on Leetcode, in: EASE’24 - 28th International Confer-\nence on Evaluation and Assessment in Software Engineering, 2024.\n[6] J.-B. D¨oderlein, M. Acher, D. E. Khelladi, B. Combemale, Piloting Copi-\nlot and Codex: Hot Temperature, Cold Prompts, or Black Magic? (Jun.\n2023). doi:10.2139/ssrn.4496380.\n[7] P. Thakkar, Copilot-explorer (2024).\nURL\nhttps://thakkarparth007.github.io/copilot-explorer/\nposts/copilot-internals.html\n[8] R. Li, et al., StarCoder: May the source be with you! (2023). arXiv:\n2305.06161.\n[9] A. Lozhkov, et al., StarCoder 2 and The Stack v2: The Next Generation\n(Feb. 2024). arXiv:2402.19173, doi:10.48550/arXiv.2402.19173.\n[10] NetEase Fuxi Lab, Easy & Efficient Quantization for Transformers\n(EETQ) (May 2024).\nURL https://github.com/NetEase-FuXi/EETQ\n28\n\n[11] T. Dettmers, TimDettmers/bitsandbytes, HuggingFace (May 2024).\nURL https://huggingface.co/docs/bitsandbytes/main/en/index\n[12] Ember, Carbon intensity of electricity generation – ember and energy\ninstitute (2024).\n[13] H. Mozannar, G. Bansal, A. Fourney, E. Horvitz, When to Show a\nSuggestion? Integrating Human Feedback in AI-Assisted Programming,\nAAAI Conference on Artificial Intelligence 38 (9) (2024) 10137–10144.\ndoi:10.1609/aaai.v38i9.28878.\n[14] S. Barke, M. B. James, N. Polikarpova, Grounded copilot: How pro-\ngrammers interact with code-generating models, Proc. ACM Program.\nLang. 7 (OOPSLA1) (apr 2023). doi:10.1145/3586030.\n[15] Measuring GitHub Copilot’s Impact on Productivity – Communications\nof the ACM (Feb. 2024).\nURL\nhttps://cacm.acm.org/research/\nmeasuring-github-copilots-impact-on-productivity/\n[16] J. Dimitropoulos, Energy productivity improvements and the rebound\neffect: An overview of the state of knowledge, Energy Policy 35 (12)\n(2007) 6354–6363. doi:10.1016/j.enpol.2007.07.028.\n[17] A. Borzunov, D. Baranchuk, T. Dettmers, M. Riabinin, Y. Belkada,\nA. Chumachenko, P. Samygin, C. Raffel, Petals: Collaborative Infer-\nence and Fine-tuning of Large Models, in: 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 3: System Demon-\nstrations), Association for Computational Linguistics, Toronto, Canada,\n2023, pp. 558–568. doi:10.18653/v1/2023.acl-demo.54.\n[18] T. Simon, P. Rust, R. Rouvoy, J. Penhoat, Uncovering the Environ-\nmental Impact of Software Life Cycle, in:\n2023 International Con-\nference on ICT for Sustainability (ICT4S), 2023, pp. 176–187.\ndoi:\n10.1109/ICT4S58814.2023.00026.\n[19] P. Vaithilingam, et al., Expectation vs. Experience: Evaluating the Us-\nability of Code Generation Tools Powered by Large Language Models,\nin: CHI Conference on Human Factors in Computing Systems Extended\nAbstracts, 2022, pp. 1–7. doi:10.1145/3491101.3519665.\n29\n\n[20] K. Papineni, S. Roukos, T. Ward, W.-J. Zhu, BLEU: A method for\nautomatic evaluation of machine translation, in: 40th Annual Meeting\non Association for Computational Linguistics, ACL ’02, Association for\nComputational Linguistics, 2002, pp. 311–318. doi:10.3115/1073083.\n1073135.\n[21] M. Chen, et al., Evaluating Large Language Models Trained on Code\n(Jul. 2021). arXiv:2107.03374.\n[22] H. Pearce, et al., Asleep at the Keyboard? Assessing the Security of\nGitHub Copilot’s Code Contributions, in: 2022 IEEE Symposium on\nSecurity and Privacy (SP), 2022, pp. 754–768. doi:10.1109/SP46214.\n2022.9833571.\n[23] G. Sandoval, et al., Lost at C: A User Study on the Security Implications\nof Large Language Model Code Assistants, in: 32nd USENIX Security\nSymposium (USENIX Security 23), 2023, pp. 2205–2222.\n[24] N. Perry, et al., Do Users Write More Insecure Code with AI Assis-\ntants?, in: 2023 ACM SIGSAC Conference on Computer and Communi-\ncations Security, CCS ’23, 2023, pp. 2785–2799. doi:10.1145/3576915.\n3623157.\n[25] K. Jesse, et al., Large Language Models and Simple, Stupid Bugs, in:\n2023 IEEE/ACM 20th International Conference on Mining Software\nRepositories (MSR), 2023, pp. 563–575. doi:10.1109/MSR59073.2023.\n00082.\n[26] B. Yetistiren, et al., Assessing the quality of GitHub copilot’s code gen-\neration, in: 18th Int. Conf. on Predictive Models and Data Analyt-\nics in Software Engineering, 2022, pp. 62–71. doi:10.1145/3558489.\n3559072.\n[27] N. Nguyen, S. Nadi, An empirical evaluation of GitHub copilot’s code\nsuggestions, in:\n19th International Conference on Mining Software\nRepositories, 2022, pp. 1–5. doi:10.1145/3524842.3528470.\n[28] J. Liu, C. S. Xia, Y. Wang, L. Zhang, Is your code generated by chat-\ngpt really correct?\nrigorous evaluation of large language models for\ncode generation, Advances in Neural Information Processing Systems\n36 (2024).\n30\n\n[29] J. Austin, et al., Program Synthesis with Large Language Models (Aug.\n2021). arXiv:2108.07732.\n[30] H. Yu, et al., CoderEval: A Benchmark of Pragmatic Code Generation\nwith Generative Pre-trained Models, in: IEEE/ACM 46th International\nConference on Software Engineering, ACM, 2024, pp. 1–12. doi:10.\n1145/3597503.3623316.\n[31] D. Hendrycks, et al., Measuring Coding Challenge Competence With\nAPPS, Neural Information Processing Systems Track on Datasets and\nBenchmarks 1 (Dec. 2021).\n[32] S. Lu, et al., CodeXGLUE: A Machine Learning Benchmark Dataset\nfor Code Understanding and Generation, Neural Information Processing\nSystems Track on Datasets and Benchmarks 1 (Dec. 2021).\n[33] S. Wang, et al., ReCode: Robustness Evaluation of Code Generation\nModels, in: 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), Association for Computational\nLinguistics, Toronto, Canada, 2023, pp. 13818–13843. doi:10.18653/\nv1/2023.acl-long.773.\n[34] F. F. Xu, et al., A systematic evaluation of large language models\nof code, in: 6th International Symposium on Machine Programming,\nMAPS 2022, 2022, pp. 1–10. doi:10.1145/3520312.3534862.\n[35] A. S. Luccioni, A. Hernandez-Garcia, Counting Carbon: A Survey of\nFactors Influencing the Emissions of Machine Learning (Feb. 2023).\narXiv:2302.08476.\n[36] R. Selvan, B. Pepin, C. Igel, G. Samuel, E. B. Dam, Equity through\nAccess: A Case for Small-scale Deep Learning (Mar. 2024).\narXiv:\n2403.12562, doi:10.48550/arXiv.2403.12562.\n[37] A. Chakravarty, Deep Learning Models in Speech Recognition: Measur-\ning GPU Energy Consumption, Impact of Noise and Model Quantization\nfor Edge Deployment (May 2024). arXiv:2405.01004.\n31",
    "pdf_filename": "Green_My_LLM_Studying_the_key_factors_affecting_the_energy_consumption_of_code_assistants.pdf"
}