{
    "title": "1",
    "abstract": "nance, Quality-Diversity algorithms have been used to gener- multiple solutions can simply be used in order to promote ate collections of both diverse and high-performing solutions. innovation in the downstream task. In this context, QD has Multi-Objective Quality-Diversity algorithms have emerged as been used for creating diverse video game levels [10], [11] a promising approach for applying these methods to complex, multi-objective problems. However, existing methods are limited and generating building designs [12]. by their search capabilities. For example, Multi-Objective Map- Despite the growing traction of QD, most research in this Elites depends on random genetic variations which struggle field has focused on single-objective applications. However, in high-dimensional search spaces. Despite efforts to enhance multi-objective (MO) problems pervade many real-world do- search efficiency with gradient-based mutation operators, ex- mains,includingengineering[13],[14],finance[15],anddrug isting approaches consider updating solutions to improve on each objective separately rather than achieving desired trade- design[16] andmanystate-of-the-art MO algorithmsoriginate offs. In this work, we address this limitation by introducing from Evolutionary Algorithm community [17]–[20]. Multi-Objective Map-Elites with Preference-Conditioned Policy- Recently, Multi-Objective MAP-Elites algorithm (MOME) Gradient and Crowding Mechanisms: a new Multi-Objective [21] marked the first attempt at bridging ideas from QD and Quality-Diversity algorithm that uses preference-conditioned MO optimisation.In MOQD,theoverarchinggoalistoidentify policy-gradient mutations to efficiently discover promising re- gionsoftheobjectivespaceandcrowdingmechanismstopromote a broad collection of solutions that exhibit diverse features a uniform distribution of solutions on the Pareto front. We and achieve distinct performances across multiple objectives. evaluate our approach on six robotics locomotion tasks and More specifically, given a feature space that is tessellated into showthatourmethodoutperformsormatchesallstate-of-the-art cells, the aim is to find a collection of solutions within each Multi-Objective Quality-Diversity methods in all six, including cell which offer different trade-offs on each of the objectives twonewlyproposedtri-objectivetasks.Importantly,ourmethod alsoachievesasmoothersetoftrade-offs,asmeasuredbynewly- (see Figure 1). As an example, consider the task of designing proposed sparsity-based metrics. This performance comes at a building sites. Within this context, it may be interesting to lowercomputationalstoragecostcomparedtopreviousmethods. find different designs that vary in the number of buildings on IndexTerms—Quality-Diversity,Multi-Objectiveoptimisation, the site. Then for each possible number of buildings, further MAP-Elites, Neuroevolution, Reinforcement Learning options can be generated which present different trade-offs of ventilation and noise levels [12]. This approach equips end- users with a spectrum of viable options, thereby broadening I. INTRODUCTION their perspective on the array of feasible design possibilities. Over recent years, Deep Reinforcement Learning (RL) has The MOME algorithm demonstrated promising results in enabled breakthroughs in mastering games [1], [2] as well findinglargecollectionsofdiversesolutionsthatbalancemul- as continuous control domains for locomotion [3], [4] and tiple objectives. However, MOME predominantly depends on manipulation [5]. These milestones have demonstrated the random genetic variations that can cause slow convergence in extraordinary potential of RL algorithms to solve specific large search spaces [22]–[24]. This renders it less suitable for problems. However, most approaches return only one highly- evolving neural networks with a large number of parameters. specialised solution to a single problem. In contrast, there is a SincetheinceptionoftheMOMEframework,severalrelated growing shift in focus towards not just uncovering one single works exploring the domain of MOQD have emerged [12], solutionthatachieveshighrewards,butinsteadmanysolutions [25], [26]. Among them, MOME-PGX [25] builds upon the thatexhibitdifferentwaysofdoingso[6].Withinthiscontext, MOME framework and was shown to achieve state-of-the-art Quality-Diversity (QD) algorithms [7] have emerged as one performance on high-dimensional continuous control robotics promising approach for tackling this challenge. tasksthatcanbeframedasMarkovDecisionProcesses.Ituses In QD, the primary goal is to produce a variety of high- crowding addition and selection mechanisms to encourage an quality solutions, rather than to focus exclusively on finding even distribution of solutions on the Pareto front and employs the single best one. One motivation for QD algorithms is that, policy-gradient mutations for each objective function in order finding many solutions can provide availability of alternative, to drive the exploration process toward promising regions back-up solutions in the event that the highest-performing of the solution space. However, the MOME-PGX approach solution is no longer suitable. For example, in robotics, gen- is not without its own set of challenges. Firstly, it employs erating large collections of solutions has been shown to be separate actor-critic networks for each objective function, helpful for addressing large simulation to reality gaps [8] and which can be resource-intensive and may not scale with an 4202 voN 91 ]IA.sc[ 1v33421.1142:viXra",
    "body": "1\nPreference-Conditioned Gradient Variations for\nMulti-Objective Quality-Diversity\nHannah Janmohamed1,2, Maxence Faldor1, Thomas Pierrot2 and Antoine Cully1\n1 Adaptive and Intelligent Robotics Lab, Imperial College London. 2 InstaDeep\nAbstract—In a variety of domains, from robotics to fi- adapting to unforeseen damages [8], [9]. Alternatively, having\nnance, Quality-Diversity algorithms have been used to gener- multiple solutions can simply be used in order to promote\nate collections of both diverse and high-performing solutions.\ninnovation in the downstream task. In this context, QD has\nMulti-Objective Quality-Diversity algorithms have emerged as\nbeen used for creating diverse video game levels [10], [11]\na promising approach for applying these methods to complex,\nmulti-objective problems. However, existing methods are limited and generating building designs [12].\nby their search capabilities. For example, Multi-Objective Map- Despite the growing traction of QD, most research in this\nElites depends on random genetic variations which struggle field has focused on single-objective applications. However,\nin high-dimensional search spaces. Despite efforts to enhance\nmulti-objective (MO) problems pervade many real-world do-\nsearch efficiency with gradient-based mutation operators, ex-\nmains,includingengineering[13],[14],finance[15],anddrug\nisting approaches consider updating solutions to improve on\neach objective separately rather than achieving desired trade- design[16] andmanystate-of-the-art MO algorithmsoriginate\noffs. In this work, we address this limitation by introducing from Evolutionary Algorithm community [17]–[20].\nMulti-Objective Map-Elites with Preference-Conditioned Policy- Recently, Multi-Objective MAP-Elites algorithm (MOME)\nGradient and Crowding Mechanisms: a new Multi-Objective\n[21] marked the first attempt at bridging ideas from QD and\nQuality-Diversity algorithm that uses preference-conditioned\nMO optimisation.In MOQD,theoverarchinggoalistoidentify\npolicy-gradient mutations to efficiently discover promising re-\ngionsoftheobjectivespaceandcrowdingmechanismstopromote a broad collection of solutions that exhibit diverse features\na uniform distribution of solutions on the Pareto front. We and achieve distinct performances across multiple objectives.\nevaluate our approach on six robotics locomotion tasks and More specifically, given a feature space that is tessellated into\nshowthatourmethodoutperformsormatchesallstate-of-the-art\ncells, the aim is to find a collection of solutions within each\nMulti-Objective Quality-Diversity methods in all six, including\ncell which offer different trade-offs on each of the objectives\ntwonewlyproposedtri-objectivetasks.Importantly,ourmethod\nalsoachievesasmoothersetoftrade-offs,asmeasuredbynewly- (see Figure 1). As an example, consider the task of designing\nproposed sparsity-based metrics. This performance comes at a building sites. Within this context, it may be interesting to\nlowercomputationalstoragecostcomparedtopreviousmethods. find different designs that vary in the number of buildings on\nIndexTerms—Quality-Diversity,Multi-Objectiveoptimisation, the site. Then for each possible number of buildings, further\nMAP-Elites, Neuroevolution, Reinforcement Learning options can be generated which present different trade-offs of\nventilation and noise levels [12]. This approach equips end-\nusers with a spectrum of viable options, thereby broadening\nI. INTRODUCTION\ntheir perspective on the array of feasible design possibilities.\nOver recent years, Deep Reinforcement Learning (RL) has The MOME algorithm demonstrated promising results in\nenabled breakthroughs in mastering games [1], [2] as well findinglargecollectionsofdiversesolutionsthatbalancemul-\nas continuous control domains for locomotion [3], [4] and tiple objectives. However, MOME predominantly depends on\nmanipulation [5]. These milestones have demonstrated the random genetic variations that can cause slow convergence in\nextraordinary potential of RL algorithms to solve specific large search spaces [22]–[24]. This renders it less suitable for\nproblems. However, most approaches return only one highly- evolving neural networks with a large number of parameters.\nspecialised solution to a single problem. In contrast, there is a SincetheinceptionoftheMOMEframework,severalrelated\ngrowing shift in focus towards not just uncovering one single works exploring the domain of MOQD have emerged [12],\nsolutionthatachieveshighrewards,butinsteadmanysolutions [25], [26]. Among them, MOME-PGX [25] builds upon the\nthatexhibitdifferentwaysofdoingso[6].Withinthiscontext, MOME framework and was shown to achieve state-of-the-art\nQuality-Diversity (QD) algorithms [7] have emerged as one performance on high-dimensional continuous control robotics\npromising approach for tackling this challenge. tasksthatcanbeframedasMarkovDecisionProcesses.Ituses\nIn QD, the primary goal is to produce a variety of high- crowding addition and selection mechanisms to encourage an\nquality solutions, rather than to focus exclusively on finding even distribution of solutions on the Pareto front and employs\nthe single best one. One motivation for QD algorithms is that, policy-gradient mutations for each objective function in order\nfinding many solutions can provide availability of alternative, to drive the exploration process toward promising regions\nback-up solutions in the event that the highest-performing of the solution space. However, the MOME-PGX approach\nsolution is no longer suitable. For example, in robotics, gen- is not without its own set of challenges. Firstly, it employs\nerating large collections of solutions has been shown to be separate actor-critic networks for each objective function,\nhelpful for addressing large simulation to reality gaps [8] and which can be resource-intensive and may not scale with an\n4202\nvoN\n91\n]IA.sc[\n1v33421.1142:viXra\n2\nFig. 1. Left. Multi-Objective MAP-Elites repertoire. The feature space C ⊂Rd is tessellated into cells Ci. A Pareto Front is stored in each cell. The aim\nof MOQD algorithmsistofilleachcellwithsolutionsthatarePareto-optimal.Right.Overviewofpreference-conditionedpolicygradientinthe MOME-P2C\nalgorithm. By conditioning policy-gradients on updates solutions can be improved toward achieving different trade-offs of objectives (illustrated by blue\narrows).Bycontrast,inMOME-PGX,solutionsareonlyupdatedtoimproveperformanceoneachobjectiveseparately(illustratedbylightbluearrows).\nincreasing number of objectives. Furthermore, although using II. BACKGROUND\npolicy gradient-based updates helps with exploration in high-\ndimensional search spaces, the approach in MOME-PGX only A. Quality-Diversity\nconsiders improving solutions on each objective separately.\nHowever, in the context of multi-objective problems, the goal Quality-Diversity algorithms aim to discover collections\nisoftennotjusttomaximiseeachobjectiveindependentlybut of solutions that are both high-performing and diverse [27].\nrather to find solutions which offer different trade-offs among Similar to standard optimisation algorithms, a solution θ ∈Θ\nthem. In this way, if end users have different preferences is assessed via a fitness function f : Θ → R that reflects\nregarding the relative importance of each objective, they have its performance on the task. For example, consider the task\na range of solutions to choose from. of generating an image of a celebrity from a text prompt.\nIn this case, the fitness of a solution could be the CLIP\nscore [28] which measures the fidelity of an image to its\nIn this paper, we address the limitations of MOME-PGX\ncaption that was used to generate it. However, an additional\nby introducing a new MOQD algorithm: Multi-Objective\ncentral component to QD algorithms, is the concept of the\nMap-Elites with Preference-Conditioned Policy-Gradient and feature function Φ : Θ → Rd that characterizes solutions in\nCrowding Mechanisms (MOME-P2C). Rather than using a\na meaningful way for the type of diversity desired [27]. The\nseparate actor-critic framework for each objective, MOME-\nfeature of a solution Φ(θ ) is a vector that captures some of\ni\nP2C uses a single preference-conditioned actor and a sin-\nits notable characteristics, which is then consequently used to\ngle preference-conditioned critic. Similar to MOME-PGX, the\nquantify its novelty relative to other solutions. In the image\nactor-critic framework in MOME-P2C can be used to pro-\ngeneration example, the feature could be the hair length or\nvide policy-gradient mutations which offer efficient search\nage of the subject in the photo [29]. In this example, the QD\nspace exploration for high-dimensional neural-network poli-\nalgorithm would then aim to generate images in which the\ncies. However, as illustrated in Figure 1, by conditioning\nsubjecthasadiverserangeofhairlengthsandages,andwhich\nthe actor and critic networks on a preference, policy-gradient\nclosely obey the given text prompt used to generate it.\nupdates can be used to improve solutions toward achieving\nOne branch of algorithms in the QD family stems from the\na given weighting over the objectives, rather than improve\nMAP-ELITESalgorithm[30],whichhasgainedprominencefor\nsolutionsoneachobjectivedisjointly.Moreover,usingasingle\nits simplicity and effectiveness. MAP-ELITES operates by dis-\npreference-conditioned actor-critic framework rather than one\ncretisingthefeaturespaceintoagrid-likestructure,whereeach\nperobjectivealsoreducesthememorycostsandtrainingcosts\ncellC ofthegridbecomesa“niche”thatcanbeoccupiedby\nassociated with maintaining the separate actor-critic networks i\nasolution.Tessellatingthefeaturespaceinthismannercreates\nof MOME-PGX.\na systematic method for exploring of different niches within\nthis space [31]. Each iteration of MAP-ELITES first involves\nWe show that MOME-P2C outperforms or matches the selecting solutions from these niches, creating copies of them\nperformance of MOME-PGX across six robotic control MOQD and mutating these copies to create new candidate solutions.\ntasks, including newly introduced tri-objective ones (see Sec- Then, the fitness and features of the candidate solutions are\ntion V-A). MOME-P2C also outperforms MOME-PGX on two evaluated, and they are added to the appropriate niches based\nnewly introduced sparsity-based MOQD metrics (see Sec- ontheirfitness.Ifthecellcorrespondingtothenewsolution’s\ntion V-C) demonstrating that it is able to attain a smoother feature vector is unoccupied, the new solution is added to the\nsetoftrade-offsthan MOME-PGX.Thecodefor MOME-P2C is cell. If the cell is occupied, but the evaluated solution has a\nfullycontainerisedandavailableatCodehiddenforanonymity, higherfitnessthanthecurrentoccupant,itisaddedtothegrid.\nwill be released upon acceptance.. Otherwise,thesolutionisdiscarded.Thisprocesscontinuesfor\n3\na fixed number of iterations, progressively populating the grid While the hypervolume metric quantifies the coverage of\nstructure with an array of diverse, high-quality solutions. the objective space by solutions on the Pareto front, sparsity\nMAP-ELITES algorithms aim to maximise the total number providescomplementaryinformationregardingthedistribution\nofoccupiedcellsattheendoftheprocessandtheperformance and evenness of these solutions. It is calculated by evaluating\nof the solutions within each of them. Given a search space Θ theaveragenearestneighbourdistanceamongsolutionsonthe\nand a feature space C that has been tessellated into k cells C , Pareto front, given by [32]:\ni\nthe MAP-ELITES objective, or QD-score [7] can be formally\nm |P|−1\nexpressed as: S(P)= 1 (cid:88) (cid:88) (P˜(i)−P˜(i+1))2 (3)\n|P|−1 j j\nk j=1 i=1\n(cid:88)\nmax f(θ ), where ∀i,Φ(θ )∈C (1)\nθ∈Θ\ni=1\ni i i where P˜ j(i) denotes the i-th solution of the list of solutions\non the front P, sorted according to the j-th objective and |P|\ndenotes the number of solutions on the front. To ensure that\nB. Multi-Objective Optimisation\nthesparsityisnotskewedduetodifferentscalesofeachofthe\nMulti-Objective(MO)optimizationprovidesanapproachfor objectives,theobjectivefunctionsmustbenormalisedpriorto\naddressing problems that involve the simultaneous considera- calculating it.\ntionofmultiple,oftenconflictingobjectivesF=[f 1,...,f m]. A low-sparsity metric indicates that solutions are well-\nIn MO problems, objectives often compete with each other, dispersed through the objective space, highlighting the al-\nmeaning that improving one objective typically comes at the gorithm’s ability to provide diverse trade-off solutions. In\nexpense of another. For example, in engineering, improving contrast, a high-sparsity metric suggests that solutions are\nperformance might increase cost, and vice versa. To navigate clustered in specific regions, potentially indicating that the\nthis landscape, the concept of Pareto-dominance is commonly algorithm struggles to explore and represent the full range of\nemployed to establish a preference ordering among solutions. possible trade-offs.\nA solution θ is said to dominate another solution θ if it is\n1 2\nequal or superior in at least one objective and not worse in\nC. Multi-Objective Quality-Diversity Algorithms\nany other [32]. That is, θ ≻θ , if ∀i:f (θ )≥f (θ )∧∃j :\n1 2 i 1 i 2\nf j(θ 1)>f j(θ 2). Multi-Objective Quality-Diversity (MOQD) combines the\nSolutions that are not dominated by any other solutions are goals of QD and MO optimisation. Specifically, the goal of\ntermed non-dominated. Given a set of candidate solutions S, MOQD is to return the Pareto front of solutions in each cell\nthe non-dominated solutions of this set θ\ni\n∈ S collectively of the feature space with maximum hypervolume, P(C i) [21].\nform a Pareto front, which represents the boundary of achiev- This MOQD goal can be mathematically formulated as:\nabletrade-offsamongobjectives.ThegoalofMOoptimisation\nk\nis to find an approximation to the optimal Pareto front, which max(cid:88) Ξ(P ), where ∀i,P =P(θ|Φ(θ)∈C ) (4)\ni i i\nis the Pareto front over the entire search space Θ. θ∈Θ\ni=1\nMOME [21] was the first MOQD algorithm that aimed to\nachieve this MOQD goal. To achieve this, MOME maintains\na Pareto front in each cell of a MAP-Elites grid. At each\niteration, a cell is uniformly selected and then a solution from\nthe corresponding Pareto front is uniformly selected. Then,\nthe algorithm follows a standard MAP-ELITES procedure: the\nsolution undergoes genetic variation and is evaluated. The\nevaluated solution is added back to the grid if it lies on the\nPareto front of the cell corresponding to its feature vector.\nFig. 2. Sets of solutions that form approximations to two Pareto fronts.\nThe hypervolume of the outer solutions is larger as they achieve higher\nperformance on the objectives. Likewise, the sparsity metric of the outer\nD. Problem Formulation\nsolutionswillalsobehigherastheyaremoreevenlyspaced.\nIn this work, we consider an agent sequentially interacting\nTherearetwometrics,thehypervolumeandsparsitymetric\nwithanenvironmentforanepisodeoflengthT,modelledasa\n(see Figure 2), that play pivotal roles in comprehensively\nMulti-Objective Markov Decision Process (MOMDP), defined\nassessing the quality and diversity of solutions within the\nby ⟨S,A,P,R,Ω⟩. At each discrete time step t, the agent\nPareto front [32], [33]. The hypervolume Ξ of a Pareto front\nobserves the current state s ∈S and takes an action a ∈A\nt t\nP, measures the volume of the objective space enclosed by a\nby following a policy π parameterized by θ. Consequently,\nθ\nsetofsolutionsrelativetoafixedreferencepointr.Thismetric\ntheagenttransitionstoanewstatesampledfromthedynamics\nprovides a quantitative measure of the quality and spread of\nprobability distribution s ∼ p(s |s ,a ). The agent also\nt+1 t+1 t t\nsolutionsintheobjectivespaceandiscalculatedas[32],[33]:\nreceives a reward vector r = [r (s ,a ),...,r (s ,a )],\nt 1 t t m t t\nwhere each reward function r : S × A → R defines\nΞ(P)=λ(θ ∈Θ|∃s∈P,s≻x≻r) (2) i\nan objective. The multi-objective fitness of a policy π is\nwhere λ denotes the Lebesgue measure. defined as a vector F(π) = [f (π),...,f (π)]. Here, each f\n1 m i\n4\nrepresents the expected discounted sum of rewards, calculated specificselectionstrategies,crossoverandmutationoperators,\nas f = E [(cid:80) γtr (s ,a )] for a given reward function r . population management techniques, and how they maintain\ni π t i t t i\nThe discount rate γ ∈[0,1] controls the relative weighting of diversity in the population [36].\nimmediate and long-term rewards. Non-dominated Sorting Genetic Algorithm II NSGA-II [19]\nand Strength Pareto Evolutionary Algorithm 2 (SPEA2) [18]\nboth use biased selection mechanisms to guide the optimi-\nE. Reinforcement Learning\nsation process. Both methods select solutions that are higher\nIn the single-objective case (m=1), the MOMDP collapses performing and occupy less dense regions of the objective\ninto a simple Markov Decision Process (MDP) with scalar\nspace with higher probability. This guides the population\nrewards, where the goal is to find a policy π that maximises\ntowardshigher-performingParetofronts,whilesimultaneously\nthe expected discounted sum of rewards or return, F(π) =\nensuring solutions are well-distributed across the front.\nE π[(cid:80) tγtr(s t,a t)]. Numerous Reinforcement Learning (RL)\nOur method, MOME-P2C has synergies with many methods\nmethods have been developed to address the challenge of\nfrom MOEA literature including biased selection and addition\nfinding policies that optimize this cumulative reward. One\nmechanisms (see Section IV-A) and we refer the interested\nparticularly relevant approach is the Twin Delayed Deep\nreading to a comprehensive survey of MOEA algorithms for\nDeterministic Policy Gradient algorithm (TD3) [34].\nmore details [36]. However, our method differs from tradi-\nThe TD3 algorithm belongs to the broader family of actor-\ntionalMOEAapproachesintwosignificantaspects.First,item-\ncritic RL techniques [35], which involve two key components: ploys a MAP-ELITES grid to explicitly maintain solutions that\nan actor network and a critic network. The actor network is a\nare diverse in feature space while optimising over objectives.\npolicy parameterised by ϕ, denoted π that is used to interact\nϕ Second,itincorporatestechniquesfromreinforcementlearning\nwiththeenvironment.Thetransitions(s ,a ,r ,s )coming\nt t t t+1 to form gradient-based mutations which help to overcome the\nfrom the interactions with the environment are stored in a\nlimited search power of traditional GA variations for high-\nreplay buffer B and used to train the actor and the critic. The\ndimensional search spaces [22].\ncritic network is an action-value function parameterised by ψ,\ndenoted Q that evaluates the quality of the actor’s actions\nψ B. Multi-Objective Reinforcement Learning\nand helps the agent learn to improve its decisions over time.\nIn multi-objective reinforcement learning (MORL) the ex-\nThecriticestimatestheexpectedreturnobtainedwhenstarting\npected sum of rewards is a vector J(π) = E [(cid:80) r ].\nfromstates,takingactionaandfollowingpolicyπ thereafter, π t t\nQ (s,a)=E [(cid:80) γtr(s ,a )|s =s,a =a]. Consequently,thereisnotastraightforwardnotionofareward\nψ TheTD3algπ oritht m,uset sat pair0 ofcriti0 cnetworksQ ψ1,Q ψ2, m dia scx oim vei rs ii nn gg aag se in nt g. leSi pn og ll ie c- ypo tl hic ay\nt\naM chO iR evL esap apr do ea sc ih ree ds f to rac du es -oo fn\nf\nrather than a single critic network in order to reduce overesti-\nof objectives. Often, this is achieved by employing a scalar-\nmationbiasandmitigatebootstrappingerrors.Thesenetworks\nization function which transforms the performance on various\nare trained using samples (s ,a ,r ,s ) from the replay\nt t t t+1\nobjectivesintoasinglescalarutilityvalue.Forexample,many\nbuffer and then regression to the same target:\napproachesaimtofindapolicyπ thatmaximisestheexpected\ny =r(s t,a t)+γ im =1in ,2Q ψi(s t+1,π ϕ′(s t+1)+ϵ) (5) weighted sum of rewards,\nwhere Q ψ′,Q ψ′ and π ϕ′ are target networks that are used in J(π,ω)=E\nπ(cid:104)(cid:88)\nω⊺ r\nt(cid:105)\n=ω⊺E\nπ(cid:104)(cid:88)\nr\nt(cid:105)\n=ω⊺ J(π) (7)\n1 2\norder to increase the stability of the training and ϵ is sampled t t\n(cid:80)\nGaussian noise to improve exploration and smoothing of the Here, ω is referred to as a preference, with iω i = 1.\nactor policy. The actor network is updated to choose actions The preference quantifies the relative importance of each\nwhich lead to higher estimated value according to the first of the objective functions for the end-user and, when the\ncritic network Q ψ1. This is achieved via a policy gradient preference is fixed, we can collapse the MOMDP into a single-\n(PG) update: objective setting that can be optimised with well-established\n∇ J(π\n)=E(cid:2)\n∇ π (s)∇ Q (s,a)|\n(cid:3)\n(6)\nRL approaches.\nϕ ϕ ϕ ϕ a ψ1 a=πϕ(s) In single-policy approaches, the challenge arises in deter-\nThese actor PG updates are executed less frequently than the mining the preference vector beforehand, as it may prove to\ncritic network training in order to enhance training stability. be a complex task or may vary among different users [32].\nInstead, it may be useful to find solutions which are optimal\nfor different preference values so that the user can examine\nIII. RELATEDWORKS\ntherangeofpossiblesolutionsthatisonofferandthenassign\nA. Multi-Objective Evolutionary Algorithms\ntheir preferences retrospectively [32]. With this perspective\nMulti-Objective Evolutionary Algorithms (MOEA) evolve in mind, multi-policy MORL methods aim to find a set of\na population of potential solutions iteratively over several policiesthatexcelacrossarangeofdifferentpreferences[37],\ngenerationstoidentifyanoptimalsetofsolutionsthatbalance [38]. Often, each policy in the set is trained using preference-\nconflicting objectives. At each iteration, solutions are selected conditioned policy-gradient derived from a multi-objective,\nfrom the population and undergo genetic variation (through preference-conditioned action-value function [37]–[39].\ncrossover and mutation operators) and are then added back to Some methods straddle the line between single-policy and\nthe population. Different MOEAs can vary in terms of their multi-policyMORLbyseekingasinglepreference-conditioned\n5\npolicy that can maximise the weighted sum of expected method is to provide more nuanced gradient information.\nreturns(Equation(7))foranygivenpreference[39]–[42].This Conditioning the policy-gradient on the feature value of a\napproach offers advantages such as reduced storage costs and solution provides a way to update the solution toward higher\nrapid adaptability [41]. However, while having preference- performance, given that it has a certain behaviour. However,\nconditioned policy approaches might be cheaper and more this method only considered mono-objective problems. Other\nflexible, these methods have been observed to achieve worse than MOME-PGX (see Section III-D) we are unaware of\nperformance on the objective functions for any given prefer- gradient-based QD methods applied multi-objective problems.\nence than having specialised policies [38].\nOur work combines elements of both preference-\nD. Multi-Objective Quality-Diversity Algorithms\nconditioned and multi-policy approaches. Our actor-critic\nRecently, policy gradient variations, inspired by single-\nnetworks are preference-conditioned. However, within each\nobjective methods, have played a pivotal role in shaping the\ncell of the MAP-ELITES grid, we adopt a multi-policy\ndevelopment of techniques in MOQD. Notably, while MOME\napproach. While storing many policies in each cell is more\n(see Section III-D) is a simple and effective MOQD approach,\ncostly in terms of memory, relying solely on a single\nit relies on GA policy-gradient mutations as an exploration\npreference-conditioned policy in each grid cell would not\nmechanism which makes it inefficient in high-dimensional\nofferastraightforwardmeanstoassesswhetheranewsolution\nsearch spaces. To overcome this challenge, Multi-Objective\nis superior or not. One possible strategy would be to evaluate\nMAP-Elites with Policy-Gradient Assistance and Crowding-\neach policy on a predefined set of preferences, and replace\nbased Exploration (MOME-PGX) [25] was recently introduced\nthe policy in the grid if it achieves higher values on those\nasanefforttoimprovetheperformanceanddata-efficiencyof\npreferences. However, this would require multiple costly\nMOME in tasks that can be framed as a MOMDP. MOME-PGX\nevaluations so this approach is not practical. To the best of\nmaintains an actor and critic network for each objective func-\nour knowledge, there is no prior research in multi-objective\ntion separately and uses policy gradient mutation operators in\nreinforcementlearning(MORL)thatactivelyseekstodiversify\norder to drive better exploration in the solution search space.\nbehaviours in this manner.\nMOME-PGX also uses crowding-based selection and addition\nmechanismstobiasexplorationinsparseregionsofthePareto\nC. Gradients in Quality-Diversity front and to maintain a uniform distribution of solutions on\nQD algorithms belong to the wider class of Genetic Algo- the front. MOME-PGX was shown to outperform MOME and\nother baselines across a suite of multi-objective robotics tasks\nrithms (GA), which broadly adhere to a common structure of\ninvolving high-dimensional neural network policies. Despite\nselection, variation and addition to a population of solutions.\nWhilethesemethodshavebeenobservedtobehighly-effective\nthis success, MOME-PGX requires maintaining distinct actor-\ncritic pairs for each objective, which is costly in memory.\nblack box methods, one key limitation is their lack of scala-\nMoreover, since each actor-critic network pair learns about\nbility to high-dimensional search spaces. In tasks in which\nsolutions are the parameters of a neural network, the search each of the objective separately, the PG variations may only\nprovide disjoint gradient information about each of the objec-\nspace can be thousands of dimensions and thus traditional\ntives, and fail to capture nuanced trade-offs.\nGA variation operators do not provide sufficient exploration\npower. To address this, many works in single-objective QD\nTothebestofourknowledgeMOMEandMOME-PGXarethe\nleverage the search power of gradient-based methods in high-\nonlyexistingMOQDalgorithmstodate.However,wealsonote\nof two particularly relevant approaches which have synergies\ndimensional search spaces [22]–[24], [43]–[45]. The pioneer\nofthesemethods,Policy-gradientassisted MAP-ELITES (PGA- withthe MOQD setting.Multi-CriteriaExploration(MCX)[12]\nwhich uses a tournament ranking strategy to condense a solu-\nME) [22], combines the TD3 algorithm with the MAP-ELITES\ntion’s score across multiple objectives into a single value, and\nalgorithms to apply QD to high-dimensional robotics control\nthen uses a standard MAP-Elites strategy. Similarly, Many-\ntasks. In particular, during the evaluation of solutions in PGA-\nME, environment transitions are stored and used to train actor\nobjective Optimisation via Voting for Elites (MOVE) [26] uses\naMAP-Elitesgridtofindsolutionswhicharehigh-performing\nand critic networks, using the training procedures explained\non many-objective problems. In this method, each cell of the\ninSectionII-E.Then, PGA-ME followsanormal MAP-ELITES\ngrid represents a different subset of objectives and a solution\nloopexceptineachiteration,halfofthesolutionsaremutated\nreplacestheexistingsolutioninthecellifitisbetteronatleast\nvia GA variations and the other half are mutated via policy-\ngradient (PG) updates. half of the objectives for the cell. While both MCX and MOVE\nconsider the simultaneous maximisation of many objectives,\nSince PGA-ME, several other QD algorithms with gradient-\ntheybothaimtofindonesolutionpercellinthe MAP-ELITES\nbased variation operators have been proposed. Some of these\ngrid rather than Pareto fronts for different features. Therefore,\nare tailored to consider different task settings which have\nweconsidertheirgoalstobefundamentallydifferentfromthe\ndifferentiable objective and feature functions [43] or discrete\nMOQD goal defined in Equation (4).\naction spaces [46].Other methods use policygradient updates\nto improve both the fitness and diversity of solutions [24],\nIV. MOME-P2C\n[43], [44]. A particular method of note is DCG-ME [45], [47]\nwhich uses policy-gradient variations conditioned on features In this section, we introduce Multi-Objective Map-Elites\nof solutions. Similar to MOME-P2C, the motivation for this with Preference-Conditioned Policy-Gradient and Crowding\n6\nAlgorithm 1 MOME-P2C pseudo-code\nInput:\n• MOME archive A and total number of iterations N\n• PG batch size b p, GA batch size b g (with b = b p +b g)\nand actor injection batch size b\na\n// Initialisation\nInitialise archive A with random solutions θ\nk\nInitialise replay buffer B with transitions from θ\nk\nInitialise actor and critic networks π , Q , Q\nϕ ψ1 ψ2\n// Main loop\nfor i=1→N do\n// Sample solutions\nFig.3. Overviewof MOME-P2Calgorithm.ParetoFrontsarestoredineach\ncellofaMAP-ELITESgrid.Ateachiteration,abatchofsolutionsareselected, θ 1,...,θ\nb\n←crowding selection(A)\nundergovariationandareaddedbacktothegridbasedontheirperformance\nand crowding-distances. As solutions are evaluated, environment transitions\n// Generate offspring\nare gathered in a replay buffer and used to train preference-conditioned\nnetworks. These networks are used with a preference sampler to perform ω ,...,ω ←preference sampler(θ ,...,θ )\npreference-conditionedPGupdates. θ˜1 ,...,θ˜bp\n←pg variation(θ ,...,θ\n1\n,ω\n,..b .p\n,ω )\n1 bp 1 bp 1 bp\nθ ˜ ,...,θ˜ ←ga variation(θ ,...,θ )\nbp+1 b bp+1 b\nMechanisms(MOME-P2C),anew MOQD algorithmthatlearns ω bp+1,...,ω bp+ba ←actor sampler\nasingle,preference-conditionedactor-criticframeworktopro- θ b˜ +1,...,θ b+˜ ba ←actor inject(π ϕ,ω bp+1,...,ω bp+ba)\nvide policy-gradient variations in tasks that can be framed as\nMDP. The algorithm inherits the core framework of existing // Evaluate offspring\nMOQD methods, which involves maintaining a Pareto front (f 1,...,f m,d,transitions)←evaluate(π θ˜ 1,...,π θb+˜ ba)\nwithin each feature cell of a MAP-ELITES grid and follows B ←insert(transitions)\na MAP-ELITES loop of selection, variation, and addition for π ϕ,Q ψ1,Q\nψ2\n←train networks(B,π ϕ,Q ψ1,Q ψ2)\na given budget. Building on the approach of MOME-PGX,\nour method not only employs traditional genetic variation // Add to archive\noperators but also integrates policy gradient mutations that A←crowding addition(θ˜ ,...,θ ˜ )\nb+1 b+ba\nimprove sample-efficiency, particularly in high-dimensional\nsearch spaces. Similar to MOME-PGX, MOME-P2C adopts // Update iterations\ncrowding-based selection, which strategically directs explo- i←i+1\nrationtowardslessexploredareasofthesearchspaceandalso\nreturn A\nutilizes crowding-based addition mechanisms to promote a\ncontinuousdistributionofsolutionsalongtheParetofront.Dis-\ntinct from MOME-PGX, which operates with a separate actor-\ndistance of a solution is defined as the average Manhattan\ncriticframeworkforeachobjectivefunction,MOME-P2Cinno-\ndistance between itself and its k-nearest neighbours, in objec-\nvates by employing a singular, preference-conditioned actor-\ntivespace.In MOME-PGX,itwasshownthatbiasingsolutions\ncritic. This design streamlines preference-conditioned policy\nin this manner provides an effective method for guiding the\ngradient variation updates togenotypes, significantly reducing\noptimisation process toward under-explored regions of the\nthe memory requirements of the algorithm and making it\nsolution space.\nmorescalabletoproblemswithahighernumberofobjectives.\nSimilarly, we also use a crowding-informed addition mech-\nFurthermore,MOME-P2Cleveragesthepreference-conditioned\nanisms to replace solutions on the Pareto front. It is important\nactor by injecting it into the main population. A visual\nto note that all MOQD methods we consider use a fixed\nrepresentation of the algorithm is depicted in Figure 3, and\nmaximumsizefortheParetofrontofeachcell.Thisisdonein\nthe accompanying pseudo-code is provided in Algorithm 1.\nordertoexploittheparallelismcapabilitiesofrecenthardware\nDetailed descriptions of each component of MOME-P2C are\nadvances [48], [49] and consequently affords many thousands\navailable in the following sections.\nofevaluationsinashortperiodoftime.However,ifasolution\nisaddedtoaParetofrontthatisatalreadymaximumcapacity,\nA. Crowding-based Selection and Addition\nanother solutionmust alsonecessarily be removed.In MOME-\nIn MOME-P2C, following MOME-PGX [25], we choose to P2C, following from MOME-PGX, we remove the solution\nuse biased selection and addition mechanisms. In particular, with the minimum crowding distance in order to sparsity of\nwhen selecting parent solutions from the grid, we first select solutions on the front.\na cell with uniform probability and then select an individual Further details regarding the crowding-based mechanisms\nfromthecell’sParetofrontwithprobabilityproportionaltoits can be found in the MOME-PGX paper [25]. To justify these\ncrowding distance. As defined in NSGA-II [19], the crowding selection and addition mechanisms are still valid for MOME-\n7\nP2C,weincludeanablationofthesecrowdingmechanismsin reward values using a running mean and variance throughout\nour ablation study (see Section VI-B). the algorithm. Solutions are stored and added to the archive\nbased on unnormalised fitnesses.\nB. Preference-Conditioned Actor-Critic\nC. Preference-Conditioned Policy Gradient Variation\nIn MOME-PGX, a separate actor-critic framework was used\ntofindapolicyπ thatmarginallymaximisedtheexpectedsum Giventhepreference-conditionedactor-criticframeworkde-\nofrewardsJi(π)=E [(cid:80) ri]foreachobjectivei=1,...,m. scribed in Section IV-B, we can form preference-conditioned\nπ t t\nHowever, in MOME-P2C, we do not require a separate actor- PG variations on solutions in the archive. In MOME-P2C at\ncritic framework for each objective function. Instead, we use each iteration, we select b p solutions from the archive and\na single actor-critic framework that aims to find a single actor performn ofpreference-conditioned policygradient stepsvia:\npolicy to maximise J(π,ω) = E π[(cid:80) tω⊺r t] for any given\n∇\nJ(θ,ω)=E(cid:2)\n∇ π (s)∇ Q (s,a|ω)|\n(cid:3)\n(11)\npreference ω. θ θ θ a ψ1 a=πθ(s)\nAccordingly, we modify the actor network π ϕ(s) to be a ThePGupdategivenbyEquation(11)dependsonapreference\nconditioned on a preference π (s|ω). By doing so, the actor vector ω. However, it is not straightforward to determine the\nϕ\nnetworknowaimstopredictthebestactiontotakefromstate beststrategyforchoosingthevalueofthisvector.Inthiswork,\ns\nt\ngiven that its preference over objectives is ω. In practice, we use the term “PG preference sampler” to refer to present\nthismeansthattheactortakesitscurrentstates t concatenated the strategy we use for determining the preference that the PG\nwith a preference-vector ω as input, and outputs an action a t. variationisconditionedon(illustratedinFigure3).In MOME-\nTraining a preference-conditioned actor requires a corre- P2C, we choose the PG preference sampler to simply be a\nsponding preference-conditioned critic that evaluates the per- random uniform sampler as we found this to be a simple,\nformance of the actor based on the actor’s preference. In this yet effective strategy. We examine other choices for the PG\nsetting, we take corresponding preference-conditioned action- preference sampler in our ablation study (Section VI-B).\nvalue function Qπ(s,a|ω) to be:\n(cid:34) T (cid:35) D. Actor Injection\nQπ(s,a|ω)=E π(·|ω) (cid:88) γtω⊺ r(s t,a t)|s 0 =s,a 0 =a InPGA-ME,MOME-PGXandothergradient-basedQDmeth-\nt=0 ods, the actor policy has the same shape as the policies stored\n=ω⊺E\n(cid:34) (cid:88)T\nγtr(s ,a )|s =s,a\n=a(cid:35) in the MAP-ELITES grid and so can be regularly injected into\nπ(·|ω) t t 0 0 the main offspring batch as a genotype, with no additional\nt=0 cost to the main algorithm. However, in MOME-P2C, the\n=ω⊺ Qπ(s,a)\npolicies in the MAP-ELITES grid only take the current state\n(8) s as input, whereas the preference-conditioned actor takes\nt\nHere, Qπ(s,a|ω) denotes the preference-conditioned vec- the state concatenated with a preference [s t,ω] as input.\ntorised action-value function. Equation (8) demonstrates that Therefore, the actor has a different architecture to the policies\nthat the we can estimate the preference-conditioned action- so cannot be added to the repertoire. In this work, we take a\nvalue function by training a critic Q (s,a|ω) → Rm to similar approach to the one taken by DCRL-ME to inject the\nψ\npredictthevectorisedaction-valuefunctionandthenweighting conditioned actor within the population [47].\nits output by the preference. To train this critic network, we Given the weights W ∈ Rn×(|S|+m) and bias B ∈ Rn of\nmodify the target TD3 algorithm given in Equation (5) to be: thefirstlayeroftheactornetwork,wenoteifwefixthevalue\nof ω we can express the value of the n neurons in the first\ny =ω⊺ r(s ,a )+γ minω⊺ Q (s ,π (s |ω)+ϵ|ω) layer as:\nt t\ni=1,2\nψi t+1 ϕ′ t+1\n(cid:20) (cid:21) (cid:20) (cid:21)\n(9) W s t +B =(cid:2) W W (cid:3) s t +B\nω s ω ω (12)\nwhich we estimate from minibatches of environment transi-\ntions (s t,a t,r t,s t+1) stored in the replay buffer B. =W ss t+(W ωω+B)\nIn order to train the preference-conditioned actor, we use In other words, if the input preference ω to the actor is fixed,\na preference-conditioned version of the policy gradient from we can reshape the preference-conditioned actor network to\nEquation (6): be the same shape as the policies in the MAP-ELITES grid by\n∇\nJ(ϕ,ω)=ω⊺E(cid:2)\n∇ π (s|ω)∇ Q (s,a|ω)|\n(cid:3) absorbing the weights corresponding to the preference input\nϕ\n=E(cid:2)\n∇\nπϕ (sϕ\n|ω)∇\nω⊺a Qψ (s,a|ω)|a=πϕ(s|ω)\n(cid:3) into the bias term of the first layer. This method provides\nϕ ϕ a ψ a=πϕ(s|ω) a cheap approach to use the preference-conditioned actor\n(10)\nnetwork to generate offspring which have the same shape as\nThe updates of the actor and critic networks, given by other policies in the grid. To take advantage of this in MOME-\nEquation (9) and Equation (10), depend on the value of the P2C,ateachiterationwesamplen apreferencesfroman“actor\npreference ω. In MOME-P2C, for each sampled transition, we preferencesampler”(seeFigure3)andusethemtoreshapethe\nuniformly sample a preference and use this to form a single actor network into n new policies. In this work, we choose\na\npolicy gradient update. Since the preference vector assumes the actor preference sampler to generate n −m uniformly\na\nthateachoftheobjectivesarescaledequally,wenormalisethe sampled preference vectors for exploration, and m one-hot\n8\nTABLEI\nSUMMARYOFEVALUATIONTASKS.\nNAME ANT-2 ANT-3 HALFCHEETAH-2 HOPPER-2 HOPPER-3 WALKER-2\nFEATURE FeetContactProportion\nREWARDS\n• Forward • xvelocity • Forward • Forward • Forward • Forward\nvelocity • yvelocity velocity velocity velocity velocity\n• Energy • Energy • Energy • Energy • Jumpingheight • Energy\nconsumption consumption consumption consumption • Energy consumption\nconsumption\nvectors (with a one at the index for each of the objectives) to B. Baselines\nensurethatfitnessineachoftheobjectivesisalwayspushed.\nWe compare MOME-P2C to five baselines: MOME-PGX,\nMOME,PGA-ME,NSGA-IIandSPEA2.MOME-PGXandMOME\nV. EXPERIMENTALSETUP are both MOQD algorithms so are straightforward to evaluate.\nIn PGA-ME, we convert the multiple objectives into a single\nIn this section, we describe the evaluation tasks, baselines\none by adding them. To ensure all algorithms have equal\nand metrics we use to evaluate our approach. Importantly,\npopulation sizes, if we use a grid of k cells with maximum\nwe introduce two new tri-objective MOQD tasks, ANT-3 and\nPareto length of |P| for MOQD methods, we use k×|P| cells\nHOPPER-3, which allow us to evaluate the capabilities of\nfor PGA-ME and a population size of k×|P| for NSGA-II and\ndifferent MOQD approaches to scale to a larger number of\nobjectives..WealsointroducetwonewMOQDmetrics,MOQD-\nSPEA2. To report metrics for PGA-ME, NSGA-II and SPEA2,\nSPARSITY-SCORE and GLOBAL-SPARSITY, which we argue weuseapassivearchivewiththesamestructureasthe MOQD\nmethods. At each iteration, we fill the passive archive with\nare important ways to assess whetherMOQD algorithms are\nsolutionsfoundbythealgorithmandthencalculatemetricson\nable to achieve smooth sets of trade-offs. These new tasks\nthesearchives.Importantly,thepassivearchivesdonotinteract\nand metrics form two key contributions of this work.\nwithin the primary algorithmic loop, ensuring that there is no\neffect on the behaviour of the baseline algorithms.\nA. Evaluation Tasks\nWeevaluateourapproachonsixcontinuouscontrolrobotics C. Metrics\ntasks, which are summarised in Table I. In these tasks, solu-\nWe evaluate our method based on six metrics:\ntions correspond to the parameters of closed-loop neural net-\n1) The MOQD-SCORE [21], [25] is the sum of the hyper-\nwork controllers which determine the torque commands given\nvolumes of the Pareto fronts stored in the archive A:\nto each of the robot’s joints. We use four robot morphologies\nk\nfrom the Brax suite [50]. In all of the tasks, the feature is (cid:88)\nΞ(P ), where ∀i,P =P(x∈A|Φ(x)∈C )\nthe proportion of time that each of the robot’s legs spends in i i i\ni=1\ncontact with the ground. Using this characterisation, solutions\nthat have diverse features will exhibit different gaits [8], [21]. This metric aims to assess if an algorithm can find high-\nperforming Pareto fronts, for a range of features.\nIn four of the tasks (ANT-2, HALFCHEETAH-2, HOPPER-\n2, WALKER-2) the aim is to maximise the forward velocity\n2) We introduce the MOQD-SPARSITY-SCORE, which we\ndefine as the average sparsity of each Pareto front Ξ(P )\nof the robot while minimising its energy consumption [21], i\nof the archive:\n[25]. However, we also introduce two tri-objective MOQD\nenvironments: ANT-3 and HOPPER-3. In the ANT-3 task, the 1 (cid:88)k\nobjectives are the robot’s x-velocity, y-velocity and energy k S(P i), where ∀i,P i =P(x∈A|Φ(x)∈C i)\nconsumption. Hence the goal is to discover controllers that i=1\nlead to different gaits, and for each of these gaits to find We introduce this metric in MOQD settings as an attempt\ncontrollers that travel in different directions while minimising to measure whether, for each feature, the algorithm suc-\ntheenergycost.IntheHOPPER-3task,therewardscorrespond ceedsinfindingasmoothtrade-offofobjectivefunctions.\nto the robot’s forward velocity, torso height and energy con- 3) TheGLOBAL-HYPERVOLUMEisthehypervolumeofthe\nsumption. The corresponding MOQD goal is to therefore find Paretofrontformedoverallofthesolutionsinthearchive\nsolutions which have different gaits, and for each of these (which we term the global Pareto front). The metric\ngaits to find controllers that make the hopper jump to dif- assesses the elitist performance of an algorithm. That is,\nferent heights or travel forward, while minimising the energy the performance of solutions on the objective functions\ncost. We designed these tasks as they present interesting and thatarepossiblewhendisregardingthesolution’sfeature.\nrealistic objectives, and also provide opportunity to compare 4) Bythe samereasoningas the MOQD-SPARSITY-SCORE,\nMOQD algorithms on tasks with m>2. we also introduce the GLOBAL-SPARSITY, which is\n9\nFig.4. MOQD-SCORE,GLOBAL-HYPERVOLUMEandMAXIMUMSUMOFSCORES(SectionV-C)forMOME-P2Ccomparedtoallbaselinesacrossalltasks.\nEachexperimentisreplicated20timeswithrandomseeds.Thesolidlineisthemedianandtheshadedarearepresentsthefirstandthirdquartiles.\nthe sparsity of the Pareto front formed over all of the training parameters were kept the same across all algorithms\nsolutions in the archive. and are provided in the supplementary material.\n5) WecalculatedtheMAXIMUMSUMOFSCORESofobjec-\ntive functions to compare our approach with traditional VI. RESULTS\nQD algorithms which directly aim to maximise this.\nIn this section, we present the results for all baselines.\n6) The COVERAGE is the proportion of cells in of an\nEach experiment is replicated 20 times with random seeds.\narchive that are occupied. It reflects how many different\nWe report p-values based on the Wilcoxon–Mann–Whitney\nfeaturesthealgorithmisabletouncover(regardlessofthe\nU test with Holm-Bonferroni correction to ensure statistical\nperformanceofthesolutions).Sinceallofthealgorithms\nvalidation of the results [52], [53].\nachieved a similar performance on this metric, we report\nthe results in the supplementary materials.\nThe MOQD metrics (1 and 2) form evaluation methods\nthat most closely align with assessing whether an algorithm\nachieves the MOQD goal given by Equation (4). The global\nmetrics(3and4)assessthealgorithmsmulti-objectiveperfor-\nmance, and allow for direct comparison with MO baselines.\nSince the sparsity metrics can be impacted by imbalanced\nscales, we run all of the baselines and find the minimum and\nmaximum of the objectives seen across all of the baselines.\nWe then normalise all of the fitnesses based on these values, Fig.5. Boxplotstodisplaysparsitymetricscalcultedonthefinalarchiveof\nMOME-P2C and MOME-PGX over 20 replications. The labels A2, A3, HC2,\nandreportthesparsitymetricsbasedofthefinalarchivesfrom\nH2, H3 and W2 correspond to the Ant-2, Ant-3, HalfCheetah-2, Hopper-2,\nthe normalised fitness values. Hopper-3andWalker-2environmentsrespectively.\nD. Hyperparameters A. Main Results\nAll experiments were run for the same total budget of The experimental results presented in Figure 4 demonstrate\n4000 iterations with a batch size of 256 evaluations per that MOME-P2C outperforms or matches all baselines on all\ngeneration,correspondingtoatotalof1,024,000evaluations. tasks and all metrics. MOME-P2C achieves a significantly\nWe used CVT tessellation [31] to create archives with 128 higher MOQD-SCORE than all baselines on Ant-2, Ant-3,\ncells, each with a maximum of Pareto Front length of 50. HalfCheetah-2 and Hopper-3 (p<0.01). MOME-P2C matches\nFor all experiments, we use a Iso+LineDD operator [51] as the MOQD-SCORE of MOME-PGX, the previous state-of-the-\nthe GA variation operator, with σ 1 = 0.005 and σ 2 = 0.05. art, on the remaining environments Hopper-2 and Walker-2,\nThe reference points for each environment and the actor-critic but at a lower storage and computational cost. Crucially, in\n10\nFig.6. MOQD-SCORE (SectionV-C)for MOME-P2C comparedtoallablationsacrossalltasks.Eachexperimentisreplicated 20 timeswithrandomseeds.\nThesolidlineisthemedianandtheshadedarearepresentsthefirstandthirdquartiles.\nscenarios where MOME-P2C does not markedly outperform underperforms compared to the standard MOME-P2C across\nMOME-PGX, it still attains lower sparsity scores (Figure 5), all tasks (p<10−4). This uniform selection and replacement\nindicating that it achieves smoother array of trade-offs. strategyevidentlylacktherefinedsearchcapabilitiesprovided\nMOME-P2C also outperforms NSGA-II and SPEA2 on the by crowding, underscoring the importance of these mecha-\nGLOBAL-HYPERVOLUME metric, algorithms that specifi- nismsinguidingthealgorithmtowardsmorediverseandhigh-\ncally aim to maximise this metric. Furthermore, MOME-P2C quality solutions.\nachieves a better MAXIMUM SUM OF SCORES than PGA-ME Finally, the modifications in preference sampling strate-\nacross all tasks (p < 10−5) except Hopper-2 and Walker-2 gies, as explored in the KEEP-PREF and ONE-HOT ablations,\nwhere it still shows improved but not statistically significant markedly influence MOME-P2C ’s performance. The KEEP-\nperformance. Importantly, we see that MOME-P2C achieves PREF ablation, which retains the parent’s preference in the\nbetterperformanceontri-dimensionaltasks,affirmingnotonly policy-gradientvariationoperatorshowsnoimprovementover\nits computational efficiency but also its scalability in handling MOME-P2C across all tasks. The ONE-HOT ablation, employ-\nmore complex tasks with an increased number of objectives. ing one-hot preferences in equal batch sizes achieves equal\nor worse MOQD-SCORE compared to MOME-P2C across all\nB. Ablations\ntasksexceptHopper-2.Theseresultshighlightthecriticalrole\n1) Ablation studies: In our ablation studies, we evaluate\nof preference management within MOME-P2C and open up a\nMOME-P2C against MOME-PGX together with four distinct\npromising avenue for future research. Specifically, developing\nmodifications to understand the contribution of each compo-\nstrategies to predict preferences that might lead to significant\nnent in MOME-P2C. These ablations include:\nhypervolume improvements, could further enhance MOME-\n• NO-ACTOR: MOME-P2C without the actor injection P2C’s performance, especially in complex multi-objective op-\nmechanism. Instead of generating 64 policy-gradient off- timization tasks [38].\nspringand64actor-injectionoffspringateachgeneration,\nNO-ACTOR produces 128 policy-gradient offspring.\nVII. CONCLUSION\n• NO-CROWDING: MOME-P2C without crowding mecha-\nInthispaper,wehaveintroducedanovelalgorithm,MOME-\nnisms, that employs uniform selection and replacement.\nP2C,whichrepresentsasignificantadvancementinthefieldof\n• KEEP-PREF:MOME-P2Cwithapolicy-gradientvariation\nMulti-ObjectiveQuality-Diversity(MOQD)optimization.Our\noperator that keeps the preference of the parent instead\nexperiments and ablation studies have demonstrated MOME-\nof sampling a new preference.\nP2C’s ability to balance multiple objectives effectively, out-\n• ONE-HOT: MOME-P2C with a policy-gradient variation\nperforming existing state-of-the-art methods in challenging\noperator that uses equal batch sizes of one-hot prefer-\ncontinuous control environments.\nences. The ONE-HOT ablation is the same as MOME-PGX\nOne of the key strengths of MOME-P2C is its use of\nexceptwithapreference-conditionedactor-criticnetwork,\npreference-conditioned policy gradient mutations, which not\nrather than separate networks.\nonly enhance the exploration process but also ensures an even\n2) Results: The results from our ablation studies provide a\ndistributionofsolutionsacrosstheParetofront.Thisapproach\ndeeper understanding of the individual components contribut-\naddresses the limitations of MOME-PGX that struggled with\ning to MOME-P2C’s effectiveness. Notably, the NO-ACTOR\nscalabilityandefficiency.Furthermore,MOME-P2C’sabilityto\nablation, which removes the actor injection mechanism from\nperform well in tri-dimensional tasks highlights its scalability\nMOME-P2C, shows an interesting pattern. In the Ant-2 and\nand adaptability to more complex and realistic scenarios.\nHalfCheetah-2 (p < 10−4), MOME-P2C significantly outper-\nOur ablation studies highlight that the strategy for sampling\nforms the NO-ACTOR ablation, suggesting that some tasks\npreferences can have a large impact on the performance of\nwhichrequireamorenuancedexplorationstrategymaybenefit\nMOME-P2C.Theexplorationofusingmodelstopredictwhich\ngreatly from the the actor injection mechanism. Moreover, in\npreference will lead to the largest hypervolume gain [38]\nall of the other tasks except for Hopper-2, NO-ACTOR either\npresents an exciting direction for further research.\nmatches or falls behind the full MOME-P2C model.\nFurthermore, the NO-CROWDING ablation, where MOME-\nP2C operates without its crowding mechanisms, significantly\n11\nREFERENCES [24] T.Pierrot,V.Mace´,F.Chalumeau,A.Flajolet,G.Cideron,K.Beguir,\nA. Cully, O. Sigaud, and N. Perrin-Gilbert, “Diversity policy gradient\nfor sample efficient quality-diversity optimization,” in ICLR Workshop\n[1] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nonAgentLearninginOpen-Endedness,2022.\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\n[25] H.Janmohamed,T.Pierrot,andA.Cully,“Improvingthedataefficiency\nM.Lanctotetal.,“Masteringthegameofgowithdeepneuralnetworks\nof multi-objective quality-diversity through gradient assistance and\nandtreesearch,”nature,vol.529,no.7587,pp.484–489,2016.\ncrowdingexploration,”inProceedingsoftheGeneticandEvolutionary\n[2] V.Mnih,K.Kavukcuoglu,D.Silver,A.Graves,I.Antonoglou,D.Wier-\nComputationConference,2023,pp.165–173.\nstra, and M. Riedmiller, “Playing atari with deep reinforcement learn-\n[26] J. Dean and N. Cheney, “Many-objective optimization via voting for\ning,”2013.\nelites,” in Proceedings of the Companion Conference on Genetic and\n[3] L. Smith, I. Kostrikov, and S. Levine, “A walk in the park: Learning\nEvolutionaryComputation,2023,pp.131–134.\nto walk in 20 minutes with model-free reinforcement learning,” arXiv\n[27] J.K.Pugh,L.B.Soros,andK.O.Stanley,“Qualitydiversity:Anew\npreprintarXiv:2208.07860,2022.\nfrontier for evolutionary computation,” Frontiers in Robotics and AI,\n[4] X.Cheng,K.Shi,A.Agarwal,andD.Pathak,“Extremeparkourwith\nvol.3,p.40,2016.\nleggedrobots,”2023.\n[28] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\n[5] OpenAI,I.Akkaya,M.Andrychowicz,M.Chociej,M.Litwin,B.Mc- G.Sastry,A.Askell,P.Mishkin,J.Clark,G.Krueger,andI.Sutskever,\nGrew,A.Petron,A.Paino,M.Plappert,G.Powell,R.Ribas,J.Schnei- “Learning transferable visual models from natural language supervi-\nder,N.Tezak,J.Tworek,P.Welinder,L.Weng,Q.Yuan,W.Zaremba, sion,”2021.\nandL.Zhang,“Solvingrubik’scubewitharobothand,”2019. [29] M.C.FontaineandS.Nikolaidis,“Covariancematrixadaptationmap-\n[6] T.Zahavy,V.Veeriah,S.Hou,K.Waugh,M.Lai,E.Leurent,N.Toma- annealing,”2023.\nsev, L. Schut, D. Hassabis, and S. Singh, “Diversifying ai: Towards [30] J.-B. Mouret and J. Clune, “Illuminating search spaces by mapping\ncreativechesswithalphazero,”2023. elites,”arXivpreprintarXiv:1504.04909,2015.\n[7] A.CullyandY.Demiris,“Qualityanddiversityoptimization:Aunifying [31] V. Vassiliades, K. Chatzilygeroudis, and J.-B. Mouret, “Using cen-\nmodularframework,”IEEETransactionsonEvolutionaryComputation, troidal voronoi tessellations to scale up the multidimensional archive\nvol.22,no.2,pp.245–259,2018. of phenotypic elites algorithm,” IEEE Transactions on Evolutionary\n[8] A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret, “Robots that can Computation,vol.22,no.4,pp.623–630,2018.\nadaptlikeanimals,”Nature,vol.521,no.7553,pp.503–507,2015. [32] C.F.Hayes,R.Ra˘dulescu,E.Bargiacchi,J.Ka¨llstro¨m,M.Macfarlane,\n[9] M. Allard, S. C. Smith, K. Chatzilygeroudis, B. Lim, and A. Cully, M. Reymond, T. Verstraeten, L. M. Zintgraf, R. Dazeley, F. Heintz\n“Onlinedamagerecoveryforphysicalrobotswithhierarchicalquality- etal.,“Apracticalguidetomulti-objectivereinforcementlearningand\ndiversity,” ACM Transactions on Evolutionary Learning, vol. 3, no. 2, planning,”AutonomousAgentsandMulti-AgentSystems,vol.36,no.1,\npp.1–23,2023. p.26,2022.\n[10] A. Khalifa, S. Lee, A. Nealen, and J. Togelius, “Talakat: Bullet hell [33] Y.Cao,B.J.Smucker,andT.J.Robinson,“Onusingthehypervolume\ngeneration through constrained map-elites,” in Proceedings of The indicatortocompareparetofronts:Applicationstomulti-criteriaoptimal\nGenetic and Evolutionary Computation Conference, 2018, pp. 1047– experimentaldesign,”JournalofStatisticalPlanningandInference,vol.\n1054. 160,pp.60–74,2015.\n[11] V.Bhatt,B.Tjanaka,M.Fontaine,andS.Nikolaidis,“Deepsurrogate [34] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approxi-\nassisted generation of environments,” Advances in Neural Information mation error in actor-critic methods,” in International conference on\nProcessingSystems,vol.35,pp.37762–37777,2022. machinelearning. PMLR,2018,pp.1587–1596.\n[12] A. Gaier, J. Stoddart, L. Villaggi, and P. J. Bentley, “T-domino: [35] I.Grondman,L.Busoniu,G.A.Lopes,andR.Babuska,“Asurveyof\nExploring multiple criteria with quality-diversity and the tournament actor-critic reinforcement learning: Standard and natural policy gradi-\ndominanceobjective,”inInternationalConferenceonParallelProblem ents,” IEEE Transactions on Systems, Man, and Cybernetics, Part C\nSolvingfromNature. Springer,2022,pp.263–277. (ApplicationsandReviews),vol.42,no.6,pp.1291–1307,2012.\n[13] P.V.R.Ferreira,R.Paffenroth,A.M.Wyglinski,T.M.Hackett,S.G. [36] A. Zhou, B.-Y. Qu, H. Li, S.-Z. Zhao, P. N. Suganthan, and\nBilen, R. C. Reinhart, and D. J. Mortensen, “Reinforcement learning Q. Zhang, “Multiobjective evolutionary algorithms: A survey of the\nforsatellitecommunications:Fromleotodeepspaceoperations,”IEEE state of the art,” Swarm and Evolutionary Computation, vol. 1, no. 1,\nCommunicationsMagazine,vol.57,no.5,pp.70–75,2019. pp. 32–49, 2011. [Online]. Available: https://www.sciencedirect.com/\nscience/article/pii/S2210650211000058\n[14] T. Verstraeten, P.-J. Daems, E. Bargiacchi, D. M. Roijers, P. J. Libin,\n[37] H. Mossalam, Y. M. Assael, D. M. Roijers, and S. White-\nand J. Helsen, “Scalable optimization for wind farm control using\nson, “Multi-objective deep reinforcement learning,” arXiv preprint\ncoordinationgraphs,”arXivpreprintarXiv:2101.07844,2021.\narXiv:1610.02707,2016.\n[15] E. Krasheninnikova, J. Garc´ıa, R. Maestre, and F. Ferna´ndez, “Rein-\n[38] J.Xu,Y.Tian,P.Ma,D.Rus,S.Sueda,andW.Matusik,“Prediction-\nforcement learning for pricing strategy optimization in the insurance\nguidedmulti-objectivereinforcementlearningforcontinuousrobotcon-\nindustry,”Engineeringapplicationsofartificialintelligence,vol.80,pp.\ntrol,”inInternationalconferenceonmachinelearning. PMLR,2020,\n8–19,2019.\npp.10607–10616.\n[16] J.HorwoodandE.Noutahi,“Moleculardesigninsyntheticallyaccessi-\n[39] R. Yang, X. Sun, and K. Narasimhan, “A generalized algorithm for\nblechemicalspaceviadeepreinforcementlearning,”ACSomega,vol.5,\nmulti-objectivereinforcementlearningandpolicyadaptation,”2019.\nno.51,pp.32984–32994,2020.\n[40] S. Parisi, M. Pirotta, and M. Restelli, “Multi-objective reinforcement\n[17] E.ZitzlerandL.Thiele,“Anevolutionaryalgorithmformultiobjective\nlearning through continuous pareto manifold approximation,” Journal\noptimization:Thestrengthparetoapproach,”TIK-report,vol.43,1998.\nofArtificialIntelligenceResearch,vol.57,pp.187–227,2016.\n[18] E.Zitzler,M.Laumanns,andL.Thiele,“Spea2:Improvingthestrength\n[41] A. Abels, D. Roijers, T. Lenaerts, A. Nowe´, and D. Steckelmacher,\nparetoevolutionaryalgorithm,”TIK-report,vol.103,2001.\n“Dynamic weights in multi-objective deep reinforcement learning,” in\n[19] K. Deb, S. Agrawal, A. Pratap, and T. Meyarivan, “A fast elitist non- Internationalconferenceonmachinelearning. PMLR,2019,pp.11–20.\ndominated sorting genetic algorithm for multi-objective optimization: [42] B. Zhu, M. Dang, and A. Grover, “Scaling pareto-efficient decision\nNsga-ii,” in International conference on parallel problem solving from makingviaofflinemulti-objectiverl,”arXivpreprintarXiv:2305.00567,\nnature. Springer,2000,pp.849–858. 2023.\n[20] Q.ZhangandH.Li,“Moea/d:Amultiobjectiveevolutionaryalgorithm [43] M. Fontaine and S. Nikolaidis, “Differentiable quality diversity,” Ad-\nbasedondecomposition,”IEEETransactionsonevolutionarycomputa- vancesinNeuralInformationProcessingSystems,vol.34,pp.10040–\ntion,vol.11,no.6,pp.712–731,2007. 10052,2021.\n[21] T.Pierrot,G.Richard,K.Beguir,andA.Cully,“Multi-objectivequality [44] B. Tjanaka, M. C. Fontaine, J. Togelius, and S. Nikolaidis,\ndiversityoptimization,”inProceedingsoftheGeneticandEvolutionary “Approximating gradients for differentiable quality diversity in\nComputationConference,2022,pp.139–147. reinforcementlearning,”inProceedingsoftheGeneticandEvolutionary\n[22] O. Nilsson and A. Cully, “Policy gradient assisted map-elites,” in Computation Conference, ser. GECCO ’22. New York, NY, USA:\nProceedingsoftheGeneticandEvolutionaryComputationConference, Association for Computing Machinery, 2022, p. 1102–1111. [Online].\n2021,pp.866–875. Available:https://doi.org/10.1145/3512290.3528705\n[23] M. Flageat, F. Chalumeau, and A. Cully, “Empirical analysis of pga- [45] M. Faldor, F. Chalumeau, M. Flageat, and A. Cully, “Map-elites with\nmap-elitesforneuroevolutioninuncertaindomains,”ACMTransactions descriptor-conditioned gradients and archive distillation into a single\nonEvolutionaryLearning,2022. policy,”2023.\n12\n[46] R. Boige, G. Richard, J. Dona, T. Pierrot, and A. Cully, “Gradient-\ninformed quality diversity for the illumination of discrete spaces,”\nin Proceedings of the Genetic and Evolutionary Computation\nConference. ACM, jul 2023. [Online]. Available: https://doi.org/10.\n1145%2F3583131.3590407\n[47] M. Faldor, F. Chalumeau, M. Flageat, and A. Cully, “Synergizing\nQuality-Diversity with Descriptor-Conditioned Reinforcement Learn-\ning,” ACM Trans. Evol. Learn. Optim., 2024. [Online]. Available:\nhttps://dl.acm.org/doi/10.1145/3696426\n[48] B. Lim, M. Allard, L. Grillotti, and A. Cully, “Accelerated quality-\ndiversity for robotics through massive parallelism,” in ICLR Workshop\nonAgentLearninginOpen-Endedness,2022.\n[49] F. Chalumeau, B. Lim, R. Boige, M. Allard, L. Grillotti, M. Flageat,\nV. Mace´, A. Flajolet, T. Pierrot, and A. Cully, “Qdax: A library\nfor quality-diversity and population-based algorithms with hardware\nacceleration,”2023.\n[50] C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and\nO.Bachem,“Brax–adifferentiablephysicsengineforlargescalerigid\nbodysimulation,”arXivpreprintarXiv:2106.13281,2021.\n[51] V.VassiliadesandJ.-B.Mouret,“Discoveringtheelitehypervolumeby\nleveraginginterspeciescorrelation,”in ProceedingsoftheGeneticand\nEvolutionaryComputationConference,2018,pp.149–156.\n[52] F. Wilcoxon, Individual comparisons by ranking methods. Springer,\n1992.\n[53] S. Holm, “A simple sequentially rejective multiple test procedure,”\nScandinavianjournalofstatistics,pp.65–70,1979.\n13\nFig.7. Mediancoverageperformanceof20seeds,theshadedregionsshowtheinter-quartilerange.\nAPPENDIXA\nCOVERAGERESULTS\nFigure 7 presents the coverage results for all of the base-\nline algorithms. As expected, all Quality-Diversity algorithms\nachieve a higher coverage score than the MOEA baselines, as\nthey explicitly seek diverse solutions.\nAPPENDIXB\nHYPERVOLUMEREFERENCEPOINTS\nTable II presents the reference points used to calculate the\nhypervolume metrics in each of the tasks. The same reference\npoints were used for all of the experiments.\nTABLEII\nREFERENCEPOINTS\nAnt-2 [-350,-4500]\nAnt-3 [-1200,-1200,-4500]\nHalfCheetah-2 [-2000,-800]\nHopper-2 [-50,-2]\nHopper-3 [-750,-3,0]\nWalker-2 [-210,-15]\nAPPENDIXC\nPOLICYGRADIENTHYPERPARAMETERS\nTableIIIpresentsallofthepolicygradienthyperparameters\nthat are used for our algorithms. All hyperparameters were\nkept the same for each task and for all algorithms which used\nPG variations.\nTABLEIII\nPOLICYGRADIENTNETWORKHYPERPARAMETERS\nReplaybuffersize 1,000,000\nCritictrainingbatchsize 256\nCriticlayerhiddensizes [256,256]\nCriticlearningrate 3×10−4\nActorlearningrate 3×10−4\nPolicylearningrate 1×10−3\nNumberofcritictrainingsteps 300\nNumberofpolicygradienttrainingsteps 100\nPolicynoise 0.2\nNoiseclip 0.2\nDiscountfactor 0.99\nSoftτ-updateproportion 0.005\nPolicydelay 2",
    "pdf_filename": "Preference-Conditioned_Gradient_Variations_for_Multi-Objective_Quality-Diversity.pdf"
}