{
    "title": "Preference-Conditioned Gradient Variations for Multi-Objective Quality-Diversity",
    "context": "nance, Quality-Diversity algorithms have been used to gener- ate collections of both diverse and high-performing solutions. Multi-Objective Quality-Diversity algorithms have emerged as a promising approach for applying these methods to complex, multi-objective problems. However, existing methods are limited by their search capabilities. For example, Multi-Objective Map- Elites depends on random genetic variations which struggle in high-dimensional search spaces. Despite efforts to enhance search efficiency with gradient-based mutation operators, ex- isting approaches consider updating solutions to improve on each objective separately rather than achieving desired trade- offs. In this work, we address this limitation by introducing Multi-Objective Map-Elites with Preference-Conditioned Policy- Gradient and Crowding Mechanisms: a new Multi-Objective Quality-Diversity algorithm that uses preference-conditioned policy-gradient mutations to efficiently discover promising re- gions of the objective space and crowding mechanisms to promote a uniform distribution of solutions on the Pareto front. We evaluate our approach on six robotics locomotion tasks and show that our method outperforms or matches all state-of-the-art Multi-Objective Quality-Diversity methods in all six, including two newly proposed tri-objective tasks. Importantly, our method also achieves a smoother set of trade-offs, as measured by newly- proposed sparsity-based metrics. This performance comes at a lower computational storage cost compared to previous methods. Index Terms—Quality-Diversity, Multi-Objective optimisation, MAP-Elites, Neuroevolution, Reinforcement Learning Over recent years, Deep Reinforcement Learning (RL) has enabled breakthroughs in mastering games [1], [2] as well as continuous control domains for locomotion [3], [4] and manipulation [5]. These milestones have demonstrated the extraordinary potential of RL algorithms to solve specific problems. However, most approaches return only one highly- specialised solution to a single problem. In contrast, there is a growing shift in focus towards not just uncovering one single solution that achieves high rewards, but instead many solutions that exhibit different ways of doing so [6]. Within this context, Quality-Diversity (QD) algorithms [7] have emerged as one promising approach for tackling this challenge. In QD, the primary goal is to produce a variety of high- quality solutions, rather than to focus exclusively on finding the single best one. One motivation for QD algorithms is that, finding many solutions can provide availability of alternative, back-up solutions in the event that the highest-performing solution is no longer suitable. For example, in robotics, gen- erating large collections of solutions has been shown to be helpful for addressing large simulation to reality gaps [8] and adapting to unforeseen damages [8], [9]. Alternatively, having multiple solutions can simply be used in order to promote innovation in the downstream task. In this context, QD has been used for creating diverse video game levels [10], [11] and generating building designs [12]. Despite the growing traction of QD, most research in this field has focused on single-objective applications. However, multi-objective (MO) problems pervade many real-world do- mains, including engineering [13], [14], finance [15], and drug design [16] and many state-of-the-art MO algorithms originate from Evolutionary Algorithm community [17]–[20]. Recently, Multi-Objective MAP-Elites algorithm (MOME) [21] marked the first attempt at bridging ideas from QD and MO optimisation. In MOQD, the overarching goal is to identify a broad collection of solutions that exhibit diverse features and achieve distinct performances across multiple objectives. More specifically, given a feature space that is tessellated into cells, the aim is to find a collection of solutions within each cell which offer different trade-offs on each of the objectives (see Figure 1). As an example, consider the task of designing building sites. Within this context, it may be interesting to find different designs that vary in the number of buildings on the site. Then for each possible number of buildings, further options can be generated which present different trade-offs of ventilation and noise levels [12]. This approach equips end- users with a spectrum of viable options, thereby broadening their perspective on the array of feasible design possibilities. The MOME algorithm demonstrated promising results in finding large collections of diverse solutions that balance mul- tiple objectives. However, MOME predominantly depends on random genetic variations that can cause slow convergence in large search spaces [22]–[24]. This renders it less suitable for evolving neural networks with a large number of parameters. Since the inception of the MOME framework, several related works exploring the domain of MOQD have emerged [12], [25], [26]. Among them, MOME-PGX [25] builds upon the MOME framework and was shown to achieve state-of-the-art performance on high-dimensional continuous control robotics tasks that can be framed as Markov Decision Processes. It uses crowding addition and selection mechanisms to encourage an even distribution of solutions on the Pareto front and employs policy-gradient mutations for each objective function in order to drive the exploration process toward promising regions of the solution space. However, the MOME-PGX approach is not without its own set of challenges. Firstly, it employs separate actor-critic networks for each objective function, which can be resource-intensive and may not scale with an arXiv:2411.12433v1  [cs.AI]  19 Nov 2024",
    "body": "1\nPreference-Conditioned Gradient Variations for\nMulti-Objective Quality-Diversity\nHannah Janmohamed1,2, Maxence Faldor1, Thomas Pierrot2 and Antoine Cully1\n1 Adaptive and Intelligent Robotics Lab, Imperial College London. 2 InstaDeep\nAbstract—In a variety of domains, from robotics to fi-\nnance, Quality-Diversity algorithms have been used to gener-\nate collections of both diverse and high-performing solutions.\nMulti-Objective Quality-Diversity algorithms have emerged as\na promising approach for applying these methods to complex,\nmulti-objective problems. However, existing methods are limited\nby their search capabilities. For example, Multi-Objective Map-\nElites depends on random genetic variations which struggle\nin high-dimensional search spaces. Despite efforts to enhance\nsearch efficiency with gradient-based mutation operators, ex-\nisting approaches consider updating solutions to improve on\neach objective separately rather than achieving desired trade-\noffs. In this work, we address this limitation by introducing\nMulti-Objective Map-Elites with Preference-Conditioned Policy-\nGradient and Crowding Mechanisms: a new Multi-Objective\nQuality-Diversity algorithm that uses preference-conditioned\npolicy-gradient mutations to efficiently discover promising re-\ngions of the objective space and crowding mechanisms to promote\na uniform distribution of solutions on the Pareto front. We\nevaluate our approach on six robotics locomotion tasks and\nshow that our method outperforms or matches all state-of-the-art\nMulti-Objective Quality-Diversity methods in all six, including\ntwo newly proposed tri-objective tasks. Importantly, our method\nalso achieves a smoother set of trade-offs, as measured by newly-\nproposed sparsity-based metrics. This performance comes at a\nlower computational storage cost compared to previous methods.\nIndex Terms—Quality-Diversity, Multi-Objective optimisation,\nMAP-Elites, Neuroevolution, Reinforcement Learning\nI. INTRODUCTION\nOver recent years, Deep Reinforcement Learning (RL) has\nenabled breakthroughs in mastering games [1], [2] as well\nas continuous control domains for locomotion [3], [4] and\nmanipulation [5]. These milestones have demonstrated the\nextraordinary potential of RL algorithms to solve specific\nproblems. However, most approaches return only one highly-\nspecialised solution to a single problem. In contrast, there is a\ngrowing shift in focus towards not just uncovering one single\nsolution that achieves high rewards, but instead many solutions\nthat exhibit different ways of doing so [6]. Within this context,\nQuality-Diversity (QD) algorithms [7] have emerged as one\npromising approach for tackling this challenge.\nIn QD, the primary goal is to produce a variety of high-\nquality solutions, rather than to focus exclusively on finding\nthe single best one. One motivation for QD algorithms is that,\nfinding many solutions can provide availability of alternative,\nback-up solutions in the event that the highest-performing\nsolution is no longer suitable. For example, in robotics, gen-\nerating large collections of solutions has been shown to be\nhelpful for addressing large simulation to reality gaps [8] and\nadapting to unforeseen damages [8], [9]. Alternatively, having\nmultiple solutions can simply be used in order to promote\ninnovation in the downstream task. In this context, QD has\nbeen used for creating diverse video game levels [10], [11]\nand generating building designs [12].\nDespite the growing traction of QD, most research in this\nfield has focused on single-objective applications. However,\nmulti-objective (MO) problems pervade many real-world do-\nmains, including engineering [13], [14], finance [15], and drug\ndesign [16] and many state-of-the-art MO algorithms originate\nfrom Evolutionary Algorithm community [17]–[20].\nRecently, Multi-Objective MAP-Elites algorithm (MOME)\n[21] marked the first attempt at bridging ideas from QD and\nMO optimisation. In MOQD, the overarching goal is to identify\na broad collection of solutions that exhibit diverse features\nand achieve distinct performances across multiple objectives.\nMore specifically, given a feature space that is tessellated into\ncells, the aim is to find a collection of solutions within each\ncell which offer different trade-offs on each of the objectives\n(see Figure 1). As an example, consider the task of designing\nbuilding sites. Within this context, it may be interesting to\nfind different designs that vary in the number of buildings on\nthe site. Then for each possible number of buildings, further\noptions can be generated which present different trade-offs of\nventilation and noise levels [12]. This approach equips end-\nusers with a spectrum of viable options, thereby broadening\ntheir perspective on the array of feasible design possibilities.\nThe MOME algorithm demonstrated promising results in\nfinding large collections of diverse solutions that balance mul-\ntiple objectives. However, MOME predominantly depends on\nrandom genetic variations that can cause slow convergence in\nlarge search spaces [22]–[24]. This renders it less suitable for\nevolving neural networks with a large number of parameters.\nSince the inception of the MOME framework, several related\nworks exploring the domain of MOQD have emerged [12],\n[25], [26]. Among them, MOME-PGX [25] builds upon the\nMOME framework and was shown to achieve state-of-the-art\nperformance on high-dimensional continuous control robotics\ntasks that can be framed as Markov Decision Processes. It uses\ncrowding addition and selection mechanisms to encourage an\neven distribution of solutions on the Pareto front and employs\npolicy-gradient mutations for each objective function in order\nto drive the exploration process toward promising regions\nof the solution space. However, the MOME-PGX approach\nis not without its own set of challenges. Firstly, it employs\nseparate actor-critic networks for each objective function,\nwhich can be resource-intensive and may not scale with an\narXiv:2411.12433v1  [cs.AI]  19 Nov 2024\n\n2\nFig. 1. Left. Multi-Objective MAP-Elites repertoire. The feature space C ⊂Rd is tessellated into cells Ci. A Pareto Front is stored in each cell. The aim\nof MOQD algorithms is to fill each cell with solutions that are Pareto-optimal. Right. Overview of preference-conditioned policy gradient in the MOME-P2C\nalgorithm. By conditioning policy-gradients on updates solutions can be improved toward achieving different trade-offs of objectives (illustrated by blue\narrows). By contrast, in MOME-PGX, solutions are only updated to improve performance on each objective separately (illustrated by light blue arrows).\nincreasing number of objectives. Furthermore, although using\npolicy gradient-based updates helps with exploration in high-\ndimensional search spaces, the approach in MOME-PGX only\nconsiders improving solutions on each objective separately.\nHowever, in the context of multi-objective problems, the goal\nis often not just to maximise each objective independently but\nrather to find solutions which offer different trade-offs among\nthem. In this way, if end users have different preferences\nregarding the relative importance of each objective, they have\na range of solutions to choose from.\nIn this paper, we address the limitations of MOME-PGX\nby introducing a new\nMOQD algorithm: Multi-Objective\nMap-Elites with Preference-Conditioned Policy-Gradient and\nCrowding Mechanisms (MOME-P2C). Rather than using a\nseparate actor-critic framework for each objective, MOME-\nP2C uses a single preference-conditioned actor and a sin-\ngle preference-conditioned critic. Similar to MOME-PGX, the\nactor-critic framework in MOME-P2C can be used to pro-\nvide policy-gradient mutations which offer efficient search\nspace exploration for high-dimensional neural-network poli-\ncies. However, as illustrated in Figure 1, by conditioning\nthe actor and critic networks on a preference, policy-gradient\nupdates can be used to improve solutions toward achieving\na given weighting over the objectives, rather than improve\nsolutions on each objective disjointly. Moreover, using a single\npreference-conditioned actor-critic framework rather than one\nper objective also reduces the memory costs and training costs\nassociated with maintaining the separate actor-critic networks\nof MOME-PGX.\nWe show that\nMOME-P2C outperforms or matches the\nperformance of MOME-PGX across six robotic control MOQD\ntasks, including newly introduced tri-objective ones (see Sec-\ntion V-A). MOME-P2C also outperforms MOME-PGX on two\nnewly introduced sparsity-based\nMOQD metrics (see Sec-\ntion V-C) demonstrating that it is able to attain a smoother\nset of trade-offs than MOME-PGX. The code for MOME-P2C is\nfully containerised and available at Code hidden for anonymity,\nwill be released upon acceptance..\nII. BACKGROUND\nA. Quality-Diversity\nQuality-Diversity algorithms aim to discover collections\nof solutions that are both high-performing and diverse [27].\nSimilar to standard optimisation algorithms, a solution θ ∈Θ\nis assessed via a fitness function f : Θ →R that reflects\nits performance on the task. For example, consider the task\nof generating an image of a celebrity from a text prompt.\nIn this case, the fitness of a solution could be the CLIP\nscore [28] which measures the fidelity of an image to its\ncaption that was used to generate it. However, an additional\ncentral component to QD algorithms, is the concept of the\nfeature function Φ : Θ →Rd that characterizes solutions in\na meaningful way for the type of diversity desired [27]. The\nfeature of a solution Φ(θi) is a vector that captures some of\nits notable characteristics, which is then consequently used to\nquantify its novelty relative to other solutions. In the image\ngeneration example, the feature could be the hair length or\nage of the subject in the photo [29]. In this example, the QD\nalgorithm would then aim to generate images in which the\nsubject has a diverse range of hair lengths and ages, and which\nclosely obey the given text prompt used to generate it.\nOne branch of algorithms in the QD family stems from the\nMAP-ELITES algorithm [30], which has gained prominence for\nits simplicity and effectiveness. MAP-ELITES operates by dis-\ncretising the feature space into a grid-like structure, where each\ncell Ci of the grid becomes a “niche” that can be occupied by\na solution. Tessellating the feature space in this manner creates\na systematic method for exploring of different niches within\nthis space [31]. Each iteration of MAP-ELITES first involves\nselecting solutions from these niches, creating copies of them\nand mutating these copies to create new candidate solutions.\nThen, the fitness and features of the candidate solutions are\nevaluated, and they are added to the appropriate niches based\non their fitness. If the cell corresponding to the new solution’s\nfeature vector is unoccupied, the new solution is added to the\ncell. If the cell is occupied, but the evaluated solution has a\nhigher fitness than the current occupant, it is added to the grid.\nOtherwise, the solution is discarded. This process continues for\n\n3\na fixed number of iterations, progressively populating the grid\nstructure with an array of diverse, high-quality solutions.\nMAP-ELITES algorithms aim to maximise the total number\nof occupied cells at the end of the process and the performance\nof the solutions within each of them. Given a search space Θ\nand a feature space C that has been tessellated into k cells Ci,\nthe MAP-ELITES objective, or QD-score [7] can be formally\nexpressed as:\nmax\nθ∈Θ\nk\nX\ni=1\nf(θi), where ∀i, Φ(θi) ∈Ci\n(1)\nB. Multi-Objective Optimisation\nMulti-Objective (MO) optimization provides an approach for\naddressing problems that involve the simultaneous considera-\ntion of multiple, often conflicting objectives F = [f1, . . . , fm].\nIn MO problems, objectives often compete with each other,\nmeaning that improving one objective typically comes at the\nexpense of another. For example, in engineering, improving\nperformance might increase cost, and vice versa. To navigate\nthis landscape, the concept of Pareto-dominance is commonly\nemployed to establish a preference ordering among solutions.\nA solution θ1 is said to dominate another solution θ2 if it is\nequal or superior in at least one objective and not worse in\nany other [32]. That is, θ1 ≻θ2, if ∀i : fi(θ1) ≥fi(θ2) ∧∃j :\nfj(θ1) > fj(θ2).\nSolutions that are not dominated by any other solutions are\ntermed non-dominated. Given a set of candidate solutions S,\nthe non-dominated solutions of this set θi ∈S collectively\nform a Pareto front, which represents the boundary of achiev-\nable trade-offs among objectives. The goal of MO optimisation\nis to find an approximation to the optimal Pareto front, which\nis the Pareto front over the entire search space Θ.\nFig. 2.\nSets of solutions that form approximations to two Pareto fronts.\nThe hypervolume of the outer solutions is larger as they achieve higher\nperformance on the objectives. Likewise, the sparsity metric of the outer\nsolutions will also be higher as they are more evenly spaced.\nThere are two metrics, the hypervolume and sparsity metric\n(see Figure 2), that play pivotal roles in comprehensively\nassessing the quality and diversity of solutions within the\nPareto front [32], [33]. The hypervolume Ξ of a Pareto front\nP, measures the volume of the objective space enclosed by a\nset of solutions relative to a fixed reference point r. This metric\nprovides a quantitative measure of the quality and spread of\nsolutions in the objective space and is calculated as [32], [33]:\nΞ(P) = λ(θ ∈Θ | ∃s ∈P, s ≻x ≻r)\n(2)\nwhere λ denotes the Lebesgue measure.\nWhile the hypervolume metric quantifies the coverage of\nthe objective space by solutions on the Pareto front, sparsity\nprovides complementary information regarding the distribution\nand evenness of these solutions. It is calculated by evaluating\nthe average nearest neighbour distance among solutions on the\nPareto front, given by [32]:\nS(P) =\n1\n|P| −1\nm\nX\nj=1\n|P|−1\nX\ni=1\n( ˜Pj(i) −˜Pj(i + 1))2\n(3)\nwhere ˜Pj(i) denotes the i-th solution of the list of solutions\non the front P, sorted according to the j-th objective and |P|\ndenotes the number of solutions on the front. To ensure that\nthe sparsity is not skewed due to different scales of each of the\nobjectives, the objective functions must be normalised prior to\ncalculating it.\nA low-sparsity metric indicates that solutions are well-\ndispersed through the objective space, highlighting the al-\ngorithm’s ability to provide diverse trade-off solutions. In\ncontrast, a high-sparsity metric suggests that solutions are\nclustered in specific regions, potentially indicating that the\nalgorithm struggles to explore and represent the full range of\npossible trade-offs.\nC. Multi-Objective Quality-Diversity Algorithms\nMulti-Objective Quality-Diversity (MOQD) combines the\ngoals of QD and MO optimisation. Specifically, the goal of\nMOQD is to return the Pareto front of solutions in each cell\nof the feature space with maximum hypervolume, P(Ci) [21].\nThis MOQD goal can be mathematically formulated as:\nmax\nθ∈Θ\nk\nX\ni=1\nΞ(Pi), where ∀i, Pi = P(θ|Φ(θ) ∈Ci)\n(4)\nMOME [21] was the first MOQD algorithm that aimed to\nachieve this MOQD goal. To achieve this, MOME maintains\na Pareto front in each cell of a MAP-Elites grid. At each\niteration, a cell is uniformly selected and then a solution from\nthe corresponding Pareto front is uniformly selected. Then,\nthe algorithm follows a standard MAP-ELITES procedure: the\nsolution undergoes genetic variation and is evaluated. The\nevaluated solution is added back to the grid if it lies on the\nPareto front of the cell corresponding to its feature vector.\nD. Problem Formulation\nIn this work, we consider an agent sequentially interacting\nwith an environment for an episode of length T, modelled as a\nMulti-Objective Markov Decision Process (MOMDP), defined\nby ⟨S, A, P, R, Ω⟩. At each discrete time step t, the agent\nobserves the current state st ∈S and takes an action at ∈A\nby following a policy πθ parameterized by θ. Consequently,\nthe agent transitions to a new state sampled from the dynamics\nprobability distribution st+1 ∼p(st+1|st, at). The agent also\nreceives a reward vector rt = [r1(st, at), . . . , rm(st, at)],\nwhere each reward function ri\n: S × A →R defines\nan objective. The multi-objective fitness of a policy π is\ndefined as a vector F(π) = [f1(π), ..., fm(π)]. Here, each fi\n\n4\nrepresents the expected discounted sum of rewards, calculated\nas fi = Eπ [P\nt γtri(st, at)] for a given reward function ri.\nThe discount rate γ ∈[0, 1] controls the relative weighting of\nimmediate and long-term rewards.\nE. Reinforcement Learning\nIn the single-objective case (m = 1), the MOMDP collapses\ninto a simple Markov Decision Process (MDP) with scalar\nrewards, where the goal is to find a policy π that maximises\nthe expected discounted sum of rewards or return, F(π) =\nEπ [P\nt γtr(st, at)]. Numerous Reinforcement Learning (RL)\nmethods have been developed to address the challenge of\nfinding policies that optimize this cumulative reward. One\nparticularly relevant approach is the Twin Delayed Deep\nDeterministic Policy Gradient algorithm (TD3) [34].\nThe TD3 algorithm belongs to the broader family of actor-\ncritic RL techniques [35], which involve two key components:\nan actor network and a critic network. The actor network is a\npolicy parameterised by ϕ, denoted πϕ that is used to interact\nwith the environment. The transitions (st, at, rt, st+1) coming\nfrom the interactions with the environment are stored in a\nreplay buffer B and used to train the actor and the critic. The\ncritic network is an action-value function parameterised by ψ,\ndenoted Qψ that evaluates the quality of the actor’s actions\nand helps the agent learn to improve its decisions over time.\nThe critic estimates the expected return obtained when starting\nfrom state s, taking action a and following policy π thereafter,\nQψ(s, a) = Eπ[P\nt γtr(st, at)|s0 = s, a0 = a].\nThe TD3 algorithm, uses a pair of critic networks Qψ1, Qψ2,\nrather than a single critic network in order to reduce overesti-\nmation bias and mitigate bootstrapping errors. These networks\nare trained using samples (st, at, rt, st+1) from the replay\nbuffer and then regression to the same target:\ny = r(st, at) + γ min\ni=1,2 Qψi(st+1, πϕ′(st+1) + ϵ)\n(5)\nwhere Qψ′\n1, Qψ′\n2 and πϕ′ are target networks that are used in\norder to increase the stability of the training and ϵ is sampled\nGaussian noise to improve exploration and smoothing of the\nactor policy. The actor network is updated to choose actions\nwhich lead to higher estimated value according to the first\ncritic network Qψ1. This is achieved via a policy gradient\n(PG) update:\n∇ϕJ(πϕ) = E\n\u0002\n∇ϕ πϕ(s) ∇aQψ1(s, a) | a=πϕ(s)\n\u0003\n(6)\nThese actor PG updates are executed less frequently than the\ncritic network training in order to enhance training stability.\nIII. RELATED WORKS\nA. Multi-Objective Evolutionary Algorithms\nMulti-Objective Evolutionary Algorithms (MOEA) evolve\na population of potential solutions iteratively over several\ngenerations to identify an optimal set of solutions that balance\nconflicting objectives. At each iteration, solutions are selected\nfrom the population and undergo genetic variation (through\ncrossover and mutation operators) and are then added back to\nthe population. Different MOEAs can vary in terms of their\nspecific selection strategies, crossover and mutation operators,\npopulation management techniques, and how they maintain\ndiversity in the population [36].\nNon-dominated Sorting Genetic Algorithm II NSGA-II [19]\nand Strength Pareto Evolutionary Algorithm 2 (SPEA2) [18]\nboth use biased selection mechanisms to guide the optimi-\nsation process. Both methods select solutions that are higher\nperforming and occupy less dense regions of the objective\nspace with higher probability. This guides the population\ntowards higher-performing Pareto fronts, while simultaneously\nensuring solutions are well-distributed across the front.\nOur method, MOME-P2C has synergies with many methods\nfrom MOEA literature including biased selection and addition\nmechanisms (see Section IV-A) and we refer the interested\nreading to a comprehensive survey of MOEA algorithms for\nmore details [36]. However, our method differs from tradi-\ntional MOEA approaches in two significant aspects. First, it em-\nploys a MAP-ELITES grid to explicitly maintain solutions that\nare diverse in feature space while optimising over objectives.\nSecond, it incorporates techniques from reinforcement learning\nto form gradient-based mutations which help to overcome the\nlimited search power of traditional GA variations for high-\ndimensional search spaces [22].\nB. Multi-Objective Reinforcement Learning\nIn multi-objective reinforcement learning (MORL) the ex-\npected sum of rewards is a vector J(π) = Eπ[P\nt rt].\nConsequently, there is not a straightforward notion of a reward\nmaximising agent. Single-policy MORL approaches focus on\ndiscovering a single policy that achieves a desired trade-off\nof objectives. Often, this is achieved by employing a scalar-\nization function which transforms the performance on various\nobjectives into a single scalar utility value. For example, many\napproaches aim to find a policy π that maximises the expected\nweighted sum of rewards,\nJ(π, ω) = Eπ\nh X\nt\nω⊺rt\ni\n= ω⊺Eπ\nh X\nt\nrt\ni\n= ω⊺J(π) (7)\nHere, ω is referred to as a preference, with P\ni ωi = 1.\nThe preference quantifies the relative importance of each\nof the objective functions for the end-user and, when the\npreference is fixed, we can collapse the MOMDP into a single-\nobjective setting that can be optimised with well-established\nRL approaches.\nIn single-policy approaches, the challenge arises in deter-\nmining the preference vector beforehand, as it may prove to\nbe a complex task or may vary among different users [32].\nInstead, it may be useful to find solutions which are optimal\nfor different preference values so that the user can examine\nthe range of possible solutions that is on offer and then assign\ntheir preferences retrospectively [32]. With this perspective\nin mind, multi-policy MORL methods aim to find a set of\npolicies that excel across a range of different preferences [37],\n[38]. Often, each policy in the set is trained using preference-\nconditioned policy-gradient derived from a multi-objective,\npreference-conditioned action-value function [37]–[39].\nSome methods straddle the line between single-policy and\nmulti-policy MORL by seeking a single preference-conditioned\n\n5\npolicy that can maximise the weighted sum of expected\nreturns (Equation (7)) for any given preference [39]–[42]. This\napproach offers advantages such as reduced storage costs and\nrapid adaptability [41]. However, while having preference-\nconditioned policy approaches might be cheaper and more\nflexible, these methods have been observed to achieve worse\nperformance on the objective functions for any given prefer-\nence than having specialised policies [38].\nOur\nwork\ncombines\nelements\nof\nboth\npreference-\nconditioned and multi-policy approaches. Our actor-critic\nnetworks are preference-conditioned. However, within each\ncell of the\nMAP-ELITES grid, we adopt a multi-policy\napproach. While storing many policies in each cell is more\ncostly in terms of memory, relying solely on a single\npreference-conditioned policy in each grid cell would not\noffer a straightforward means to assess whether a new solution\nis superior or not. One possible strategy would be to evaluate\neach policy on a predefined set of preferences, and replace\nthe policy in the grid if it achieves higher values on those\npreferences. However, this would require multiple costly\nevaluations so this approach is not practical. To the best of\nour knowledge, there is no prior research in multi-objective\nreinforcement learning (MORL) that actively seeks to diversify\nbehaviours in this manner.\nC. Gradients in Quality-Diversity\nQD algorithms belong to the wider class of Genetic Algo-\nrithms (GA), which broadly adhere to a common structure of\nselection, variation and addition to a population of solutions.\nWhile these methods have been observed to be highly-effective\nblack box methods, one key limitation is their lack of scala-\nbility to high-dimensional search spaces. In tasks in which\nsolutions are the parameters of a neural network, the search\nspace can be thousands of dimensions and thus traditional\nGA variation operators do not provide sufficient exploration\npower. To address this, many works in single-objective QD\nleverage the search power of gradient-based methods in high-\ndimensional search spaces [22]–[24], [43]–[45]. The pioneer\nof these methods, Policy-gradient assisted MAP-ELITES (PGA-\nME) [22], combines the TD3 algorithm with the MAP-ELITES\nalgorithms to apply QD to high-dimensional robotics control\ntasks. In particular, during the evaluation of solutions in PGA-\nME, environment transitions are stored and used to train actor\nand critic networks, using the training procedures explained\nin Section II-E. Then, PGA-ME follows a normal MAP-ELITES\nloop except in each iteration, half of the solutions are mutated\nvia GA variations and the other half are mutated via policy-\ngradient (PG) updates.\nSince PGA-ME, several other QD algorithms with gradient-\nbased variation operators have been proposed. Some of these\nare tailored to consider different task settings which have\ndifferentiable objective and feature functions [43] or discrete\naction spaces [46]. Other methods use policy gradient updates\nto improve both the fitness and diversity of solutions [24],\n[43], [44]. A particular method of note is DCG-ME [45], [47]\nwhich uses policy-gradient variations conditioned on features\nof solutions. Similar to MOME-P2C, the motivation for this\nmethod is to provide more nuanced gradient information.\nConditioning the policy-gradient on the feature value of a\nsolution provides a way to update the solution toward higher\nperformance, given that it has a certain behaviour. However,\nthis method only considered mono-objective problems. Other\nthan\nMOME-PGX (see Section III-D) we are unaware of\ngradient-based QD methods applied multi-objective problems.\nD. Multi-Objective Quality-Diversity Algorithms\nRecently, policy gradient variations, inspired by single-\nobjective methods, have played a pivotal role in shaping the\ndevelopment of techniques in MOQD. Notably, while MOME\n(see Section III-D) is a simple and effective MOQD approach,\nit relies on GA policy-gradient mutations as an exploration\nmechanism which makes it inefficient in high-dimensional\nsearch spaces. To overcome this challenge, Multi-Objective\nMAP-Elites with Policy-Gradient Assistance and Crowding-\nbased Exploration (MOME-PGX) [25] was recently introduced\nas an effort to improve the performance and data-efficiency of\nMOME in tasks that can be framed as a MOMDP. MOME-PGX\nmaintains an actor and critic network for each objective func-\ntion separately and uses policy gradient mutation operators in\norder to drive better exploration in the solution search space.\nMOME-PGX also uses crowding-based selection and addition\nmechanisms to bias exploration in sparse regions of the Pareto\nfront and to maintain a uniform distribution of solutions on\nthe front. MOME-PGX was shown to outperform MOME and\nother baselines across a suite of multi-objective robotics tasks\ninvolving high-dimensional neural network policies. Despite\nthis success, MOME-PGX requires maintaining distinct actor-\ncritic pairs for each objective, which is costly in memory.\nMoreover, since each actor-critic network pair learns about\neach of the objective separately, the PG variations may only\nprovide disjoint gradient information about each of the objec-\ntives, and fail to capture nuanced trade-offs.\nTo the best of our knowledge MOME and MOME-PGX are the\nonly existing MOQD algorithms to date. However, we also note\nof two particularly relevant approaches which have synergies\nwith the MOQD setting. Multi-Criteria Exploration (MCX) [12]\nwhich uses a tournament ranking strategy to condense a solu-\ntion’s score across multiple objectives into a single value, and\nthen uses a standard MAP-Elites strategy. Similarly, Many-\nobjective Optimisation via Voting for Elites (MOVE) [26] uses\na MAP-Elites grid to find solutions which are high-performing\non many-objective problems. In this method, each cell of the\ngrid represents a different subset of objectives and a solution\nreplaces the existing solution in the cell if it is better on at least\nhalf of the objectives for the cell. While both MCX and MOVE\nconsider the simultaneous maximisation of many objectives,\nthey both aim to find one solution per cell in the MAP-ELITES\ngrid rather than Pareto fronts for different features. Therefore,\nwe consider their goals to be fundamentally different from the\nMOQD goal defined in Equation (4).\nIV. MOME-P2C\nIn this section, we introduce Multi-Objective Map-Elites\nwith Preference-Conditioned Policy-Gradient and Crowding\n\n6\nFig. 3. Overview of MOME-P2C algorithm. Pareto Fronts are stored in each\ncell of a MAP-ELITES grid. At each iteration, a batch of solutions are selected,\nundergo variation and are added back to the grid based on their performance\nand crowding-distances. As solutions are evaluated, environment transitions\nare gathered in a replay buffer and used to train preference-conditioned\nnetworks. These networks are used with a preference sampler to perform\npreference-conditioned PG updates.\nMechanisms (MOME-P2C), a new MOQD algorithm that learns\na single, preference-conditioned actor-critic framework to pro-\nvide policy-gradient variations in tasks that can be framed as\nMDP. The algorithm inherits the core framework of existing\nMOQD methods, which involves maintaining a Pareto front\nwithin each feature cell of a MAP-ELITES grid and follows\na MAP-ELITES loop of selection, variation, and addition for\na given budget. Building on the approach of MOME-PGX,\nour method not only employs traditional genetic variation\noperators but also integrates policy gradient mutations that\nimprove sample-efficiency, particularly in high-dimensional\nsearch spaces. Similar to MOME-PGX, MOME-P2C adopts\ncrowding-based selection, which strategically directs explo-\nration towards less explored areas of the search space and also\nutilizes crowding-based addition mechanisms to promote a\ncontinuous distribution of solutions along the Pareto front. Dis-\ntinct from MOME-PGX, which operates with a separate actor-\ncritic framework for each objective function, MOME-P2C inno-\nvates by employing a singular, preference-conditioned actor-\ncritic. This design streamlines preference-conditioned policy\ngradient variation updates to genotypes, significantly reducing\nthe memory requirements of the algorithm and making it\nmore scalable to problems with a higher number of objectives.\nFurthermore, MOME-P2C leverages the preference-conditioned\nactor by injecting it into the main population. A visual\nrepresentation of the algorithm is depicted in Figure 3, and\nthe accompanying pseudo-code is provided in Algorithm 1.\nDetailed descriptions of each component of MOME-P2C are\navailable in the following sections.\nA. Crowding-based Selection and Addition\nIn MOME-P2C, following MOME-PGX [25], we choose to\nuse biased selection and addition mechanisms. In particular,\nwhen selecting parent solutions from the grid, we first select\na cell with uniform probability and then select an individual\nfrom the cell’s Pareto front with probability proportional to its\ncrowding distance. As defined in NSGA-II [19], the crowding\nAlgorithm 1 MOME-P2C pseudo-code\nInput:\n• MOME archive A and total number of iterations N\n• PG batch size bp, GA batch size bg (with b = bp + bg)\nand actor injection batch size ba\n// Initialisation\nInitialise archive A with random solutions θk\nInitialise replay buffer B with transitions from θk\nInitialise actor and critic networks πϕ, Qψ1, Qψ2\n// Main loop\nfor i = 1 →N do\n// Sample solutions\nθ1, ..., θb ←crowding selection(A)\n// Generate offspring\nω1, ..., ωbp ←preference sampler(θ1, ..., θbp)\n˜θ1, ..., ˜\nθbp ←pg variation(θ1, ..., θbp, ω1, ..., ωbp)\n˜\nθbp+1, ..., ˜θb ←ga variation(θbp+1, ..., θb)\nωbp+1, ..., ωbp+ba ←actor sampler\n˜\nθb+1, ...,\n˜\nθb+ba ←actor inject(πϕ, ωbp+1, ..., ωbp+ba)\n// Evaluate offspring\n(f1, ..., fm, d, transitions) ←evaluate(π ˜\nθ1, ..., π\n˜\nθb+ba )\nB ←insert(transitions)\nπϕ, Qψ1, Qψ2 ←train networks(B, πϕ, Qψ1, Qψ2)\n// Add to archive\nA ←crowding addition( ˜\nθb+1, ...,\n˜\nθb+ba)\n// Update iterations\ni ←i + 1\nreturn A\ndistance of a solution is defined as the average Manhattan\ndistance between itself and its k-nearest neighbours, in objec-\ntive space. In MOME-PGX, it was shown that biasing solutions\nin this manner provides an effective method for guiding the\noptimisation process toward under-explored regions of the\nsolution space.\nSimilarly, we also use a crowding-informed addition mech-\nanisms to replace solutions on the Pareto front. It is important\nto note that all MOQD methods we consider use a fixed\nmaximum size for the Pareto front of each cell. This is done in\norder to exploit the parallelism capabilities of recent hardware\nadvances [48], [49] and consequently affords many thousands\nof evaluations in a short period of time. However, if a solution\nis added to a Pareto front that is at already maximum capacity,\nanother solution must also necessarily be removed. In MOME-\nP2C, following from MOME-PGX, we remove the solution\nwith the minimum crowding distance in order to sparsity of\nsolutions on the front.\nFurther details regarding the crowding-based mechanisms\ncan be found in the MOME-PGX paper [25]. To justify these\nselection and addition mechanisms are still valid for MOME-\n\n7\nP2C, we include an ablation of these crowding mechanisms in\nour ablation study (see Section VI-B).\nB. Preference-Conditioned Actor-Critic\nIn MOME-PGX, a separate actor-critic framework was used\nto find a policy π that marginally maximised the expected sum\nof rewards Ji(π) = Eπ[P\nt ri\nt] for each objective i = 1, ..., m.\nHowever, in MOME-P2C, we do not require a separate actor-\ncritic framework for each objective function. Instead, we use\na single actor-critic framework that aims to find a single actor\npolicy to maximise J(π, ω) = Eπ[P\nt ω⊺rt] for any given\npreference ω.\nAccordingly, we modify the actor network πϕ(s) to be a\nconditioned on a preference πϕ(s|ω). By doing so, the actor\nnetwork now aims to predict the best action to take from state\nst given that its preference over objectives is ω. In practice,\nthis means that the actor takes its current state st concatenated\nwith a preference-vector ω as input, and outputs an action at.\nTraining a preference-conditioned actor requires a corre-\nsponding preference-conditioned critic that evaluates the per-\nformance of the actor based on the actor’s preference. In this\nsetting, we take corresponding preference-conditioned action-\nvalue function Qπ(s, a|ω) to be:\nQπ(s, a|ω) = Eπ(·|ω)\n\" T\nX\nt=0\nγtω⊺r(st, at) | s0 = s, a0 = a\n#\n= ω⊺Eπ(·|ω)\n\" T\nX\nt=0\nγtr(st, at) | s0 = s, a0 = a\n#\n= ω⊺Qπ(s, a)\n(8)\nHere, Qπ(s, a|ω) denotes the preference-conditioned vec-\ntorised action-value function. Equation (8) demonstrates that\nthat the we can estimate the preference-conditioned action-\nvalue function by training a critic Qψ(s, a|ω) →Rm to\npredict the vectorised action-value function and then weighting\nits output by the preference. To train this critic network, we\nmodify the target TD3 algorithm given in Equation (5) to be:\ny = ω⊺r(st, at)+γ min\ni=1,2 ω⊺Qψi(st+1, πϕ′(st+1|ω)+ϵ|ω)\n(9)\nwhich we estimate from minibatches of environment transi-\ntions (st, at, rt, st+1) stored in the replay buffer B.\nIn order to train the preference-conditioned actor, we use\na preference-conditioned version of the policy gradient from\nEquation (6):\n∇ϕJ(ϕ, ω) = ω⊺E\n\u0002\n∇ϕ πϕ(s|ω) ∇aQψ(s, a|ω) | a=πϕ(s|ω)\n\u0003\n= E\n\u0002\n∇ϕ πϕ(s|ω) ∇aω⊺Qψ(s, a|ω) | a=πϕ(s|ω)\n\u0003\n(10)\nThe updates of the actor and critic networks, given by\nEquation (9) and Equation (10), depend on the value of the\npreference ω. In MOME-P2C, for each sampled transition, we\nuniformly sample a preference and use this to form a single\npolicy gradient update. Since the preference vector assumes\nthat each of the objectives are scaled equally, we normalise the\nreward values using a running mean and variance throughout\nthe algorithm. Solutions are stored and added to the archive\nbased on unnormalised fitnesses.\nC. Preference-Conditioned Policy Gradient Variation\nGiven the preference-conditioned actor-critic framework de-\nscribed in Section IV-B, we can form preference-conditioned\nPG variations on solutions in the archive. In MOME-P2C at\neach iteration, we select bp solutions from the archive and\nperform n of preference-conditioned policy gradient steps via:\n∇θJ(θ, ω) = E\n\u0002\n∇θ πθ(s) ∇aQψ1(s, a|ω) | a=πθ(s)\n\u0003\n(11)\nThe PG update given by Equation (11) depends on a preference\nvector ω. However, it is not straightforward to determine the\nbest strategy for choosing the value of this vector. In this work,\nwe use the term “PG preference sampler” to refer to present\nthe strategy we use for determining the preference that the PG\nvariation is conditioned on (illustrated in Figure 3). In MOME-\nP2C, we choose the PG preference sampler to simply be a\nrandom uniform sampler as we found this to be a simple,\nyet effective strategy. We examine other choices for the PG\npreference sampler in our ablation study (Section VI-B).\nD. Actor Injection\nIn PGA-ME, MOME-PGX and other gradient-based QD meth-\nods, the actor policy has the same shape as the policies stored\nin the MAP-ELITES grid and so can be regularly injected into\nthe main offspring batch as a genotype, with no additional\ncost to the main algorithm. However, in MOME-P2C, the\npolicies in the MAP-ELITES grid only take the current state\nst as input, whereas the preference-conditioned actor takes\nthe state concatenated with a preference [st, ω] as input.\nTherefore, the actor has a different architecture to the policies\nso cannot be added to the repertoire. In this work, we take a\nsimilar approach to the one taken by DCRL-ME to inject the\nconditioned actor within the population [47].\nGiven the weights W ∈Rn×(|S|+m) and bias B ∈Rn of\nthe first layer of the actor network, we note if we fix the value\nof ω we can express the value of the n neurons in the first\nlayer as:\nW\n\u0014st\nω\n\u0015\n+ B =\n\u0002\nWs\nWω\n\u0003 \u0014st\nω\n\u0015\n+ B\n= Wsst + (Wωω + B)\n(12)\nIn other words, if the input preference ω to the actor is fixed,\nwe can reshape the preference-conditioned actor network to\nbe the same shape as the policies in the MAP-ELITES grid by\nabsorbing the weights corresponding to the preference input\ninto the bias term of the first layer. This method provides\na cheap approach to use the preference-conditioned actor\nnetwork to generate offspring which have the same shape as\nother policies in the grid. To take advantage of this in MOME-\nP2C, at each iteration we sample na preferences from an “actor\npreference sampler” (see Figure 3) and use them to reshape the\nactor network into na new policies. In this work, we choose\nthe actor preference sampler to generate na −m uniformly\nsampled preference vectors for exploration, and m one-hot\n\n8\nTABLE I\nSUMMARY OF EVALUATION TASKS.\nNAME\nANT-2\nANT-3\nHALFCHEETAH-2\nHOPPER-2\nHOPPER-3\nWALKER-2\nFEATURE\nFeet Contact Proportion\nREWARDS\n• Forward\nvelocity\n• Energy\nconsumption\n• x velocity\n• y velocity\n• Energy\nconsumption\n• Forward\nvelocity\n• Energy\nconsumption\n• Forward\nvelocity\n• Energy\nconsumption\n• Forward\nvelocity\n• Jumping height\n• Energy\nconsumption\n• Forward\nvelocity\n• Energy\nconsumption\nvectors (with a one at the index for each of the objectives) to\nensure that fitness in each of the objectives is always pushed.\nV. EXPERIMENTAL SETUP\nIn this section, we describe the evaluation tasks, baselines\nand metrics we use to evaluate our approach. Importantly,\nwe introduce two new tri-objective MOQD tasks, ANT-3 and\nHOPPER-3, which allow us to evaluate the capabilities of\ndifferent MOQD approaches to scale to a larger number of\nobjectives.. We also introduce two new MOQD metrics, MOQD-\nSPARSITY-SCORE and GLOBAL-SPARSITY, which we argue\nare important ways to assess whetherMOQD algorithms are\nable to achieve smooth sets of trade-offs. These new tasks\nand metrics form two key contributions of this work.\nA. Evaluation Tasks\nWe evaluate our approach on six continuous control robotics\ntasks, which are summarised in Table I. In these tasks, solu-\ntions correspond to the parameters of closed-loop neural net-\nwork controllers which determine the torque commands given\nto each of the robot’s joints. We use four robot morphologies\nfrom the Brax suite [50]. In all of the tasks, the feature is\nthe proportion of time that each of the robot’s legs spends in\ncontact with the ground. Using this characterisation, solutions\nthat have diverse features will exhibit different gaits [8], [21].\nIn four of the tasks (ANT-2, HALFCHEETAH-2, HOPPER-\n2, WALKER-2) the aim is to maximise the forward velocity\nof the robot while minimising its energy consumption [21],\n[25]. However, we also introduce two tri-objective MOQD\nenvironments: ANT-3 and HOPPER-3. In the ANT-3 task, the\nobjectives are the robot’s x-velocity, y-velocity and energy\nconsumption. Hence the goal is to discover controllers that\nlead to different gaits, and for each of these gaits to find\ncontrollers that travel in different directions while minimising\nthe energy cost. In the HOPPER-3 task, the rewards correspond\nto the robot’s forward velocity, torso height and energy con-\nsumption. The corresponding MOQD goal is to therefore find\nsolutions which have different gaits, and for each of these\ngaits to find controllers that make the hopper jump to dif-\nferent heights or travel forward, while minimising the energy\ncost. We designed these tasks as they present interesting and\nrealistic objectives, and also provide opportunity to compare\nMOQD algorithms on tasks with m > 2.\nB. Baselines\nWe compare MOME-P2C to five baselines: MOME-PGX,\nMOME, PGA-ME, NSGA-II and SPEA2. MOME-PGX and MOME\nare both MOQD algorithms so are straightforward to evaluate.\nIn PGA-ME, we convert the multiple objectives into a single\none by adding them. To ensure all algorithms have equal\npopulation sizes, if we use a grid of k cells with maximum\nPareto length of |P| for MOQD methods, we use k × |P| cells\nfor PGA-ME and a population size of k ×|P| for NSGA-II and\nSPEA2. To report metrics for PGA-ME, NSGA-II and SPEA2,\nwe use a passive archive with the same structure as the MOQD\nmethods. At each iteration, we fill the passive archive with\nsolutions found by the algorithm and then calculate metrics on\nthese archives. Importantly, the passive archives do not interact\nwithin the primary algorithmic loop, ensuring that there is no\neffect on the behaviour of the baseline algorithms.\nC. Metrics\nWe evaluate our method based on six metrics:\n1) The MOQD-SCORE [21], [25] is the sum of the hyper-\nvolumes of the Pareto fronts stored in the archive A:\nk\nX\ni=1\nΞ(Pi), where ∀i, Pi = P(x ∈A|Φ(x) ∈Ci)\nThis metric aims to assess if an algorithm can find high-\nperforming Pareto fronts, for a range of features.\n2) We introduce the MOQD-SPARSITY-SCORE, which we\ndefine as the average sparsity of each Pareto front Ξ(Pi)\nof the archive:\n1\nk\nk\nX\ni=1\nS(Pi), where ∀i, Pi = P(x ∈A|Φ(x) ∈Ci)\nWe introduce this metric in MOQD settings as an attempt\nto measure whether, for each feature, the algorithm suc-\nceeds in finding a smooth trade-off of objective functions.\n3) The GLOBAL-HYPERVOLUME is the hypervolume of the\nPareto front formed over all of the solutions in the archive\n(which we term the global Pareto front). The metric\nassesses the elitist performance of an algorithm. That is,\nthe performance of solutions on the objective functions\nthat are possible when disregarding the solution’s feature.\n4) By the same reasoning as the MOQD-SPARSITY-SCORE,\nwe also introduce the\nGLOBAL-SPARSITY, which is\n\n9\nFig. 4. MOQD-SCORE, GLOBAL-HYPERVOLUME and MAXIMUM SUM OF SCORES (Section V-C) for MOME-P2C compared to all baselines across all tasks.\nEach experiment is replicated 20 times with random seeds. The solid line is the median and the shaded area represents the first and third quartiles.\nthe sparsity of the Pareto front formed over all of the\nsolutions in the archive.\n5) We calculated the MAXIMUM SUM OF SCORES of objec-\ntive functions to compare our approach with traditional\nQD algorithms which directly aim to maximise this.\n6) The COVERAGE is the proportion of cells in of an\narchive that are occupied. It reflects how many different\nfeatures the algorithm is able to uncover (regardless of the\nperformance of the solutions). Since all of the algorithms\nachieved a similar performance on this metric, we report\nthe results in the supplementary materials.\nThe MOQD metrics (1 and 2) form evaluation methods\nthat most closely align with assessing whether an algorithm\nachieves the MOQD goal given by Equation (4). The global\nmetrics (3 and 4) assess the algorithms multi-objective perfor-\nmance, and allow for direct comparison with MO baselines.\nSince the sparsity metrics can be impacted by imbalanced\nscales, we run all of the baselines and find the minimum and\nmaximum of the objectives seen across all of the baselines.\nWe then normalise all of the fitnesses based on these values,\nand report the sparsity metrics based of the final archives from\nthe normalised fitness values.\nD. Hyperparameters\nAll experiments were run for the same total budget of\n4000 iterations with a batch size of 256 evaluations per\ngeneration, corresponding to a total of 1, 024, 000 evaluations.\nWe used CVT tessellation [31] to create archives with 128\ncells, each with a maximum of Pareto Front length of 50.\nFor all experiments, we use a Iso+LineDD operator [51] as\nthe GA variation operator, with σ1 = 0.005 and σ2 = 0.05.\nThe reference points for each environment and the actor-critic\ntraining parameters were kept the same across all algorithms\nand are provided in the supplementary material.\nVI. RESULTS\nIn this section, we present the results for all baselines.\nEach experiment is replicated 20 times with random seeds.\nWe report p-values based on the Wilcoxon–Mann–Whitney\nU test with Holm-Bonferroni correction to ensure statistical\nvalidation of the results [52], [53].\nFig. 5. Boxplots to display sparsity metrics calculted on the final archive of\nMOME-P2C and MOME-PGX over 20 replications. The labels A2, A3, HC2,\nH2, H3 and W2 correspond to the Ant-2, Ant-3, HalfCheetah-2, Hopper-2,\nHopper-3 and Walker-2 environments respectively.\nA. Main Results\nThe experimental results presented in Figure 4 demonstrate\nthat MOME-P2C outperforms or matches all baselines on all\ntasks and all metrics. MOME-P2C achieves a significantly\nhigher MOQD-SCORE than all baselines on Ant-2, Ant-3,\nHalfCheetah-2 and Hopper-3 (p < 0.01). MOME-P2C matches\nthe MOQD-SCORE of MOME-PGX, the previous state-of-the-\nart, on the remaining environments Hopper-2 and Walker-2,\nbut at a lower storage and computational cost. Crucially, in\n\n10\nFig. 6.\nMOQD-SCORE (Section V-C) for MOME-P2C compared to all ablations across all tasks. Each experiment is replicated 20 times with random seeds.\nThe solid line is the median and the shaded area represents the first and third quartiles.\nscenarios where MOME-P2C does not markedly outperform\nMOME-PGX, it still attains lower sparsity scores (Figure 5),\nindicating that it achieves smoother array of trade-offs.\nMOME-P2C also outperforms NSGA-II and SPEA2 on the\nGLOBAL-HYPERVOLUME metric, algorithms that specifi-\ncally aim to maximise this metric. Furthermore, MOME-P2C\nachieves a better MAXIMUM SUM OF SCORES than PGA-ME\nacross all tasks (p < 10−5) except Hopper-2 and Walker-2\nwhere it still shows improved but not statistically significant\nperformance. Importantly, we see that MOME-P2C achieves\nbetter performance on tri-dimensional tasks, affirming not only\nits computational efficiency but also its scalability in handling\nmore complex tasks with an increased number of objectives.\nB. Ablations\n1) Ablation studies: In our ablation studies, we evaluate\nMOME-P2C against MOME-PGX together with four distinct\nmodifications to understand the contribution of each compo-\nnent in MOME-P2C. These ablations include:\n• NO-ACTOR:\nMOME-P2C without the actor injection\nmechanism. Instead of generating 64 policy-gradient off-\nspring and 64 actor-injection offspring at each generation,\nNO-ACTOR produces 128 policy-gradient offspring.\n• NO-CROWDING: MOME-P2C without crowding mecha-\nnisms, that employs uniform selection and replacement.\n• KEEP-PREF: MOME-P2C with a policy-gradient variation\noperator that keeps the preference of the parent instead\nof sampling a new preference.\n• ONE-HOT: MOME-P2C with a policy-gradient variation\noperator that uses equal batch sizes of one-hot prefer-\nences. The ONE-HOT ablation is the same as MOME-PGX\nexcept with a preference-conditioned actor-critic network,\nrather than separate networks.\n2) Results: The results from our ablation studies provide a\ndeeper understanding of the individual components contribut-\ning to MOME-P2C’s effectiveness. Notably, the NO-ACTOR\nablation, which removes the actor injection mechanism from\nMOME-P2C, shows an interesting pattern. In the Ant-2 and\nHalfCheetah-2 (p < 10−4), MOME-P2C significantly outper-\nforms the NO-ACTOR ablation, suggesting that some tasks\nwhich require a more nuanced exploration strategy may benefit\ngreatly from the the actor injection mechanism. Moreover, in\nall of the other tasks except for Hopper-2, NO-ACTOR either\nmatches or falls behind the full MOME-P2C model.\nFurthermore, the NO-CROWDING ablation, where MOME-\nP2C operates without its crowding mechanisms, significantly\nunderperforms compared to the standard MOME-P2C across\nall tasks (p < 10−4). This uniform selection and replacement\nstrategy evidently lack the refined search capabilities provided\nby crowding, underscoring the importance of these mecha-\nnisms in guiding the algorithm towards more diverse and high-\nquality solutions.\nFinally, the modifications in preference sampling strate-\ngies, as explored in the KEEP-PREF and ONE-HOT ablations,\nmarkedly influence MOME-P2C ’s performance. The KEEP-\nPREF ablation, which retains the parent’s preference in the\npolicy-gradient variation operator shows no improvement over\nMOME-P2C across all tasks. The ONE-HOT ablation, employ-\ning one-hot preferences in equal batch sizes achieves equal\nor worse MOQD-SCORE compared to MOME-P2C across all\ntasks except Hopper-2. These results highlight the critical role\nof preference management within MOME-P2C and open up a\npromising avenue for future research. Specifically, developing\nstrategies to predict preferences that might lead to significant\nhypervolume improvements, could further enhance MOME-\nP2C’s performance, especially in complex multi-objective op-\ntimization tasks [38].\nVII. CONCLUSION\nIn this paper, we have introduced a novel algorithm, MOME-\nP2C, which represents a significant advancement in the field of\nMulti-Objective Quality-Diversity (MOQD) optimization. Our\nexperiments and ablation studies have demonstrated MOME-\nP2C’s ability to balance multiple objectives effectively, out-\nperforming existing state-of-the-art methods in challenging\ncontinuous control environments.\nOne of the key strengths of MOME-P2C is its use of\npreference-conditioned policy gradient mutations, which not\nonly enhance the exploration process but also ensures an even\ndistribution of solutions across the Pareto front. This approach\naddresses the limitations of MOME-PGX that struggled with\nscalability and efficiency. Furthermore, MOME-P2C’s ability to\nperform well in tri-dimensional tasks highlights its scalability\nand adaptability to more complex and realistic scenarios.\nOur ablation studies highlight that the strategy for sampling\npreferences can have a large impact on the performance of\nMOME-P2C. The exploration of using models to predict which\npreference will lead to the largest hypervolume gain [38]\npresents an exciting direction for further research.\n\n11\nREFERENCES\n[1] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\nM. Lanctot et al., “Mastering the game of go with deep neural networks\nand tree search,” nature, vol. 529, no. 7587, pp. 484–489, 2016.\n[2] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\nstra, and M. Riedmiller, “Playing atari with deep reinforcement learn-\ning,” 2013.\n[3] L. Smith, I. Kostrikov, and S. Levine, “A walk in the park: Learning\nto walk in 20 minutes with model-free reinforcement learning,” arXiv\npreprint arXiv:2208.07860, 2022.\n[4] X. Cheng, K. Shi, A. Agarwal, and D. Pathak, “Extreme parkour with\nlegged robots,” 2023.\n[5] OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. Mc-\nGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schnei-\nder, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba,\nand L. Zhang, “Solving rubik’s cube with a robot hand,” 2019.\n[6] T. Zahavy, V. Veeriah, S. Hou, K. Waugh, M. Lai, E. Leurent, N. Toma-\nsev, L. Schut, D. Hassabis, and S. Singh, “Diversifying ai: Towards\ncreative chess with alphazero,” 2023.\n[7] A. Cully and Y. Demiris, “Quality and diversity optimization: A unifying\nmodular framework,” IEEE Transactions on Evolutionary Computation,\nvol. 22, no. 2, pp. 245–259, 2018.\n[8] A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret, “Robots that can\nadapt like animals,” Nature, vol. 521, no. 7553, pp. 503–507, 2015.\n[9] M. Allard, S. C. Smith, K. Chatzilygeroudis, B. Lim, and A. Cully,\n“Online damage recovery for physical robots with hierarchical quality-\ndiversity,” ACM Transactions on Evolutionary Learning, vol. 3, no. 2,\npp. 1–23, 2023.\n[10] A. Khalifa, S. Lee, A. Nealen, and J. Togelius, “Talakat: Bullet hell\ngeneration through constrained map-elites,” in Proceedings of The\nGenetic and Evolutionary Computation Conference, 2018, pp. 1047–\n1054.\n[11] V. Bhatt, B. Tjanaka, M. Fontaine, and S. Nikolaidis, “Deep surrogate\nassisted generation of environments,” Advances in Neural Information\nProcessing Systems, vol. 35, pp. 37 762–37 777, 2022.\n[12] A. Gaier, J. Stoddart, L. Villaggi, and P. J. Bentley, “T-domino:\nExploring multiple criteria with quality-diversity and the tournament\ndominance objective,” in International Conference on Parallel Problem\nSolving from Nature.\nSpringer, 2022, pp. 263–277.\n[13] P. V. R. Ferreira, R. Paffenroth, A. M. Wyglinski, T. M. Hackett, S. G.\nBilen, R. C. Reinhart, and D. J. Mortensen, “Reinforcement learning\nfor satellite communications: From leo to deep space operations,” IEEE\nCommunications Magazine, vol. 57, no. 5, pp. 70–75, 2019.\n[14] T. Verstraeten, P.-J. Daems, E. Bargiacchi, D. M. Roijers, P. J. Libin,\nand J. Helsen, “Scalable optimization for wind farm control using\ncoordination graphs,” arXiv preprint arXiv:2101.07844, 2021.\n[15] E. Krasheninnikova, J. Garc´ıa, R. Maestre, and F. Fern´andez, “Rein-\nforcement learning for pricing strategy optimization in the insurance\nindustry,” Engineering applications of artificial intelligence, vol. 80, pp.\n8–19, 2019.\n[16] J. Horwood and E. Noutahi, “Molecular design in synthetically accessi-\nble chemical space via deep reinforcement learning,” ACS omega, vol. 5,\nno. 51, pp. 32 984–32 994, 2020.\n[17] E. Zitzler and L. Thiele, “An evolutionary algorithm for multiobjective\noptimization: The strength pareto approach,” TIK-report, vol. 43, 1998.\n[18] E. Zitzler, M. Laumanns, and L. Thiele, “Spea2: Improving the strength\npareto evolutionary algorithm,” TIK-report, vol. 103, 2001.\n[19] K. Deb, S. Agrawal, A. Pratap, and T. Meyarivan, “A fast elitist non-\ndominated sorting genetic algorithm for multi-objective optimization:\nNsga-ii,” in International conference on parallel problem solving from\nnature.\nSpringer, 2000, pp. 849–858.\n[20] Q. Zhang and H. Li, “Moea/d: A multiobjective evolutionary algorithm\nbased on decomposition,” IEEE Transactions on evolutionary computa-\ntion, vol. 11, no. 6, pp. 712–731, 2007.\n[21] T. Pierrot, G. Richard, K. Beguir, and A. Cully, “Multi-objective quality\ndiversity optimization,” in Proceedings of the Genetic and Evolutionary\nComputation Conference, 2022, pp. 139–147.\n[22] O. Nilsson and A. Cully, “Policy gradient assisted map-elites,” in\nProceedings of the Genetic and Evolutionary Computation Conference,\n2021, pp. 866–875.\n[23] M. Flageat, F. Chalumeau, and A. Cully, “Empirical analysis of pga-\nmap-elites for neuroevolution in uncertain domains,” ACM Transactions\non Evolutionary Learning, 2022.\n[24] T. Pierrot, V. Mac´e, F. Chalumeau, A. Flajolet, G. Cideron, K. Beguir,\nA. Cully, O. Sigaud, and N. Perrin-Gilbert, “Diversity policy gradient\nfor sample efficient quality-diversity optimization,” in ICLR Workshop\non Agent Learning in Open-Endedness, 2022.\n[25] H. Janmohamed, T. Pierrot, and A. Cully, “Improving the data efficiency\nof multi-objective quality-diversity through gradient assistance and\ncrowding exploration,” in Proceedings of the Genetic and Evolutionary\nComputation Conference, 2023, pp. 165–173.\n[26] J. Dean and N. Cheney, “Many-objective optimization via voting for\nelites,” in Proceedings of the Companion Conference on Genetic and\nEvolutionary Computation, 2023, pp. 131–134.\n[27] J. K. Pugh, L. B. Soros, and K. O. Stanley, “Quality diversity: A new\nfrontier for evolutionary computation,” Frontiers in Robotics and AI,\nvol. 3, p. 40, 2016.\n[28] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever,\n“Learning transferable visual models from natural language supervi-\nsion,” 2021.\n[29] M. C. Fontaine and S. Nikolaidis, “Covariance matrix adaptation map-\nannealing,” 2023.\n[30] J.-B. Mouret and J. Clune, “Illuminating search spaces by mapping\nelites,” arXiv preprint arXiv:1504.04909, 2015.\n[31] V. Vassiliades, K. Chatzilygeroudis, and J.-B. Mouret, “Using cen-\ntroidal voronoi tessellations to scale up the multidimensional archive\nof phenotypic elites algorithm,” IEEE Transactions on Evolutionary\nComputation, vol. 22, no. 4, pp. 623–630, 2018.\n[32] C. F. Hayes, R. R˘adulescu, E. Bargiacchi, J. K¨allstr¨om, M. Macfarlane,\nM. Reymond, T. Verstraeten, L. M. Zintgraf, R. Dazeley, F. Heintz\net al., “A practical guide to multi-objective reinforcement learning and\nplanning,” Autonomous Agents and Multi-Agent Systems, vol. 36, no. 1,\np. 26, 2022.\n[33] Y. Cao, B. J. Smucker, and T. J. Robinson, “On using the hypervolume\nindicator to compare pareto fronts: Applications to multi-criteria optimal\nexperimental design,” Journal of Statistical Planning and Inference, vol.\n160, pp. 60–74, 2015.\n[34] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approxi-\nmation error in actor-critic methods,” in International conference on\nmachine learning.\nPMLR, 2018, pp. 1587–1596.\n[35] I. Grondman, L. Busoniu, G. A. Lopes, and R. Babuska, “A survey of\nactor-critic reinforcement learning: Standard and natural policy gradi-\nents,” IEEE Transactions on Systems, Man, and Cybernetics, Part C\n(Applications and Reviews), vol. 42, no. 6, pp. 1291–1307, 2012.\n[36] A. Zhou, B.-Y. Qu, H. Li, S.-Z. Zhao, P. N. Suganthan, and\nQ. Zhang, “Multiobjective evolutionary algorithms: A survey of the\nstate of the art,” Swarm and Evolutionary Computation, vol. 1, no. 1,\npp. 32–49, 2011. [Online]. Available: https://www.sciencedirect.com/\nscience/article/pii/S2210650211000058\n[37] H.\nMossalam,\nY.\nM.\nAssael,\nD.\nM.\nRoijers,\nand\nS.\nWhite-\nson, “Multi-objective deep reinforcement learning,” arXiv preprint\narXiv:1610.02707, 2016.\n[38] J. Xu, Y. Tian, P. Ma, D. Rus, S. Sueda, and W. Matusik, “Prediction-\nguided multi-objective reinforcement learning for continuous robot con-\ntrol,” in International conference on machine learning.\nPMLR, 2020,\npp. 10 607–10 616.\n[39] R. Yang, X. Sun, and K. Narasimhan, “A generalized algorithm for\nmulti-objective reinforcement learning and policy adaptation,” 2019.\n[40] S. Parisi, M. Pirotta, and M. Restelli, “Multi-objective reinforcement\nlearning through continuous pareto manifold approximation,” Journal\nof Artificial Intelligence Research, vol. 57, pp. 187–227, 2016.\n[41] A. Abels, D. Roijers, T. Lenaerts, A. Now´e, and D. Steckelmacher,\n“Dynamic weights in multi-objective deep reinforcement learning,” in\nInternational conference on machine learning. PMLR, 2019, pp. 11–20.\n[42] B. Zhu, M. Dang, and A. Grover, “Scaling pareto-efficient decision\nmaking via offline multi-objective rl,” arXiv preprint arXiv:2305.00567,\n2023.\n[43] M. Fontaine and S. Nikolaidis, “Differentiable quality diversity,” Ad-\nvances in Neural Information Processing Systems, vol. 34, pp. 10 040–\n10 052, 2021.\n[44] B.\nTjanaka,\nM.\nC.\nFontaine,\nJ.\nTogelius,\nand\nS.\nNikolaidis,\n“Approximating\ngradients\nfor\ndifferentiable\nquality\ndiversity\nin\nreinforcement learning,” in Proceedings of the Genetic and Evolutionary\nComputation Conference, ser. GECCO ’22.\nNew York, NY, USA:\nAssociation for Computing Machinery, 2022, p. 1102–1111. [Online].\nAvailable: https://doi.org/10.1145/3512290.3528705\n[45] M. Faldor, F. Chalumeau, M. Flageat, and A. Cully, “Map-elites with\ndescriptor-conditioned gradients and archive distillation into a single\npolicy,” 2023.\n\n12\n[46] R. Boige, G. Richard, J. Dona, T. Pierrot, and A. Cully, “Gradient-\ninformed quality diversity for the illumination of discrete spaces,”\nin\nProceedings\nof\nthe\nGenetic\nand\nEvolutionary\nComputation\nConference.\nACM, jul 2023. [Online]. Available: https://doi.org/10.\n1145%2F3583131.3590407\n[47] M. Faldor, F. Chalumeau, M. Flageat, and A. Cully, “Synergizing\nQuality-Diversity with Descriptor-Conditioned Reinforcement Learn-\ning,” ACM Trans. Evol. Learn. Optim., 2024. [Online]. Available:\nhttps://dl.acm.org/doi/10.1145/3696426\n[48] B. Lim, M. Allard, L. Grillotti, and A. Cully, “Accelerated quality-\ndiversity for robotics through massive parallelism,” in ICLR Workshop\non Agent Learning in Open-Endedness, 2022.\n[49] F. Chalumeau, B. Lim, R. Boige, M. Allard, L. Grillotti, M. Flageat,\nV. Mac´e, A. Flajolet, T. Pierrot, and A. Cully, “Qdax: A library\nfor quality-diversity and population-based algorithms with hardware\nacceleration,” 2023.\n[50] C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and\nO. Bachem, “Brax–a differentiable physics engine for large scale rigid\nbody simulation,” arXiv preprint arXiv:2106.13281, 2021.\n[51] V. Vassiliades and J.-B. Mouret, “Discovering the elite hypervolume by\nleveraging interspecies correlation,” in Proceedings of the Genetic and\nEvolutionary Computation Conference, 2018, pp. 149–156.\n[52] F. Wilcoxon, Individual comparisons by ranking methods.\nSpringer,\n1992.\n[53] S. Holm, “A simple sequentially rejective multiple test procedure,”\nScandinavian journal of statistics, pp. 65–70, 1979.\n\n13\nFig. 7. Median coverage performance of 20 seeds, the shaded regions show the inter-quartile range.\nAPPENDIX A\nCOVERAGE RESULTS\nFigure 7 presents the coverage results for all of the base-\nline algorithms. As expected, all Quality-Diversity algorithms\nachieve a higher coverage score than the MOEA baselines, as\nthey explicitly seek diverse solutions.\nAPPENDIX B\nHYPERVOLUME REFERENCE POINTS\nTable II presents the reference points used to calculate the\nhypervolume metrics in each of the tasks. The same reference\npoints were used for all of the experiments.\nTABLE II\nREFERENCE POINTS\nAnt-2\n[-350, -4500]\nAnt-3\n[-1200, -1200, -4500]\nHalfCheetah-2\n[-2000, -800]\nHopper-2\n[-50, -2]\nHopper-3\n[-750, -3, 0]\nWalker-2\n[-210, -15]\nAPPENDIX C\nPOLICY GRADIENT HYPERPARAMETERS\nTable III presents all of the policy gradient hyperparameters\nthat are used for our algorithms. All hyperparameters were\nkept the same for each task and for all algorithms which used\nPG variations.\nTABLE III\nPOLICY GRADIENT NETWORK HYPERPARAMETERS\nReplay buffer size\n1,000,000\nCritic training batch size\n256\nCritic layer hidden sizes\n[256, 256]\nCritic learning rate\n3 × 10−4\nActor learning rate\n3 × 10−4\nPolicy learning rate\n1 × 10−3\nNumber of critic training steps\n300\nNumber of policy gradient training steps\n100\nPolicy noise\n0.2\nNoise clip\n0.2\nDiscount factor\n0.99\nSoft τ-update proportion\n0.005\nPolicy delay\n2",
    "pdf_filename": "Preference-Conditioned_Gradient_Variations_for_Multi-Objective_Quality-Diversity.pdf"
}