{
    "title": "Symbolic Computation in Software Science - My Personal View",
    "context": "between the three aspects. Working mathematicians will have to master the three aspects equally well and integrate them into their daily work. More speciﬁcally, working in math- knowledge and computational methods) and, at the same time, on the meta-level of develop- ing automated reasoning methods for supporting research on the object level. This massage of the mathematical brain by jumping back and forth between the object and the meta-level will guide mathematics onto a new level of sophistication. Symbolic computation is just a",
    "body": "T. Kutsia (Ed.): Symbolic Computation\nin Software Science (SCSS’21)\nEPTCS 342, 2021, pp. 1–13, doi:10.4204/EPTCS.342.1\n© B. Buchberger\nThis work is licensed under the\nCreative Commons Attribution License.\nSymbolic Computation in Software Science:\nMy Personal View\nBruno Buchberger\nResearch Institute for Symbolic Computation (RISC)\nJohannes Kepler University\nLinz / Schloss Hagenberg, Austria\nBruno.Buchberger@risc.jku.at\nIn this note, I develop my personal view on the scope and relevance of symbolic computation in\nsoftware science. For this, I discuss the interaction and differences between symbolic computation,\nsoftware science, automatic programming, mathematical knowledge management, artiﬁcial intelli-\ngence, algorithmic intelligence, numerical computation, and machine learning. In the discussion of\nthese notions, I allow myself to refer also to papers (1982, 1985, 2001, 2003, 2013) of mine in which\nI expressed my views on these areas at early stages of some of these ﬁelds.\nThe Intention of This Note\nIt is a great joy to see that the SCSS (Symbolic Computation in Software Science) conference series,\nthis year, experiences its 9th edition. A big Thank You to the organizers, referees, and contributors who\nkept the series going over the years! The series emerged from a couple of meetings of research groups\nin Austria, Japan, and Tunisia, including my Theorema Group at RISC, see the home pages of the SCSS\nseries since 2006. In 2012, we decided to deﬁne “Symbolic Computation in Software Science” as the\nscope for our meetings and to establish them as an open conference series with this title.\nAs always, when one puts two terms like “symbolic computation” and “software science” together,\none is tempted to read the preposition in between - in our case “in” - as just a set-theoretic union.\nPragmatically, this is reasonable if one does not want to embark on scrutinizing discussions. However,\nsince I was one of the initiators of the SCSS series, let me take the opportunity to explain the intention\nbehind SC in SS in this note. Also, this note, for me, is a kind of revision and summary of thoughts I\nhad over the years on the subject of SCSS and related subjects. Hence, allow me to refer to a couple\nof my papers with basic considerations on the subject. I do not discuss, however, any of my technical\ncontributions to the subject of SCSS (which would be, mainly, Gr¨obner bases and the Theorema system).\nIn some way, this note continues, updates, and specializes the note on mathematics in the 21st century I\ngave at the beginning of SCSS 2013, see [3], from which I quote:\nIn my view, mathematics of the 21st century will evolve as a uniﬁed body of mathemat-\nical logic, abstract structural mathematics, and computer mathematics with no boundaries\nbetween the three aspects. Working mathematicians will have to master the three aspects\nequally well and integrate them into their daily work. More speciﬁcally, working in math-\nematics will proceed on the object level of developing new mathematical content (abstract\nknowledge and computational methods) and, at the same time, on the meta-level of develop-\ning automated reasoning methods for supporting research on the object level. This massage\nof the mathematical brain by jumping back and forth between the object and the meta-level\nwill guide mathematics onto a new level of sophistication. Symbolic computation is just a\n\n2\nSymbolic Computation in Software Science: My Personal View\nway of expressing this general view of mathematics of the 21 st century and it also should\nbe clear that software science is just another way of expressing the algorithmic aspect of this\nview.\nContinuing the discussion on the intended meaning of “Symbolic Computation in Software Science”\nin this note will hopefully help to advocate the central importance of this topic for the future of mathe-\nmatics, logic, and computer science. This should also motivate more and more people to submit papers\nto the conferences in the SCSS series.\nWhat is Symbolic Computation?\nIn 1984, Academic Press London issued a call for designing a new journal for a new ﬁeld that had\nemerged approximately since 1960. Various names were in use for this ﬁeld: computer algebra, symbolic\nand algebraic manipulation, analytic computation, formula manipulation, computation in ﬁnite terms,\nsymbolic computation, and others. As a response to this call, I submitted a proposal to Academic Press\nfor a “Journal of Symbolic Computation”. My proposal was selected and my clariﬁcation of the scope\nof “symbolic computation” formed also the Editorial of the journal, see [2].\nI deﬁned “symbolic computation” as the area that deals with algorithms on symbolic objects, and I\nproposed “symbolic objects” to be deﬁned as ﬁnitary representations of inﬁnite mathematical entities.\nHere, “ﬁnitary” means “storable in a computer memory”. For example, ﬁnitely many generators with\nﬁnitely many relations between words formed from the generators form a ﬁnitary object that may repre-\nsent an inﬁnite group (or, at least, a “large” group, i.e. a group whose number of elements is much much\nlarger than the size of the ﬁnitary representation). Algorithms can only work on ﬁnitary objects and the\nﬂavor of “symbolic” is exactly the point that we want to solve problems on inﬁnite (or “large”) mathe-\nmatical entities by ﬁnding algorithms that work on ﬁnitary (small), “symbolic”, representations of these\nentities. Also, numerical computation works on ﬁnitary representations (for example, lists of rational\nnumbers that represent a function consisting of inﬁnitely many pairs of real numbers). In this sense, nu-\nmerical computation is a subﬁeld of symbolic computation. However, usually, for algorithms to be called\n“symbolic” we request that the representation of the abstract mathematical domains by ﬁnitary domains\nmust be an isomorphism w.r.t. to the operations on the objects we study. In numerical computation, for\nthe sake of efﬁciency, this request has to be given up. Instead, we are satisﬁed with “approximations”.\nPragmatically, in the editorial of the Journal of Symbolic Computation, I named three main areas\nfor symbolic computation: computer algebra, automated reasoning, and “automatic programming”.\nI also emphasized that all aspects of these areas should be in the scope of the Journal of Symbolic\nComputation: mathematical theory on which symbolic algorithms can be based, the algorithms with\ntheir correctness proofs and complexity analysis, the details of the implementation of the algorithms,\nlanguages and software systems for symbolic computation, and applications. Indeed, the three main\nbranches of symbolic computation consider three important classes of “symbolic objects”:\n– computer algebra: symbolic objects that represent algebraic entities like terms that represent func-\ntions, differential operators, etc. or ﬁnite relations that represent residue class structures;\n– automated reasoning: symbolic objects containing (quantiﬁed) variables that are considered as\nstatements on (inﬁnite) domains;\n– automatic programming: symbolic objects containing variables that are considered as programs\nthat deﬁne processes on potentially inﬁnitely many inputs.\n\nB. Buchberger\n3\n(Of course, these three sub-areas of symbolic computation are intimately connected and, in some\nprecise way, even embedded in each other. The distinction between the three areas is more or less only a\nmatter of “ﬂavor”.)\nIn other words, symbolic objects are ﬁnitary objects that have “semantics” attached to them where,\ntypically, the semantics is “large”, even inﬁnite, not tangible by computers whereas the symbolic ob-\njects are “small”, ﬁnitary, tangible by algorithms. Any ﬁeld of mathematics can be studied under the\n“symbolic” view and, actually, in any ﬁeld of mathematics, if we want to solve problems by algorithms,\nwe have to ﬁnd ﬁnitary representations for the objects in the ﬁeld. Finding suitable ﬁnitary represen-\ntations, by itself, may be a difﬁcult - sometimes provably impossible - mathematical problem: Before\nembarking on deeper questions, deciding whether or not two symbolic objects represent the same ab-\nstract mathematical object and ﬁnding “canonical” representatives for symbolic objects may already be\nvery difﬁcult (sometimes provably impossible). By ﬁnding representations of mathematical objects in\nany ﬁeld of mathematics, the ﬁeld becomes “algebraic”, and problems in the algebraic disguise of the\nﬁeld, essentially, become combinatorial problems. Thus, very sketchy, one may say: symbolic computa-\ntion, ultimately, is the “combinatorization” of all of mathematics via ﬁnitary representations of inﬁnite\nmathematical entities.\nIt is a common misunderstanding that symbolic computation is the trivial side of mathematics, i.e.\nsome people believe that, whereas “pure” mathematics lives in difﬁcult spaces needing deep and difﬁcult\nthinking, algorithmic mathematics (which must be “symbolic” in the above sense) “just” puts every-\nthing to the computer and presses the start button. The truth is, that the “just” needs more and deeper\nmathematics than a mathematics that allows non-algorithmic constructions for problem-solving like the\nunlimited set quantiﬁer, inﬁnite summation, inﬁnite unions, transition to residua class domains etc. (A\ntrivial example: In “pure” mathematics, a Gr¨obner basis for given ideal generators can be “easily” found\nby just taking the ideal generated by the generators. However, the deﬁnition of the ideal generated by\ngenerators involves an inﬁnite set construction!) Hence, with some provocation, in my view, mathematics\nonly starts at the moment when it tries to solve problems by “symbolic computation”.\nRecently, in 2020, we issued a call for running for the editor-in-chief position of the Journal of\nSymbolic Computation (JSC). At that occasion, we asked the candidates to submit also their views on\nthe scope of “symbolic computation” and of the JSC. Interestingly, the view of symbolic computation in\nthe editorial of the JSC (and summarized above) was backed by all candidates and, basically, no dramatic\nchanges or extensions were proposed except that “artiﬁcial intelligence” was mentioned a couple of\ntimes.\nMentioning artiﬁcial intelligence, for me, raises some nostalgia because, when I founded the Re-\nsearch Institute for Symbolic Computation (also in 1985), for some time I was torn between using “sym-\nbolic computation” or “artiﬁcial intelligence” as the main notion in the name of the new institute. At that\ntime, bringing symbolic computation under the umbrella of artiﬁcial intelligence was quite tempting and\nalso quite common: For example, ﬁnding symbolic integrals was considered an “artiﬁcial intelligence”\ntask like playing chess, with lots of heuristics. Correspondingly, the most comprehensive symbolic com-\nputation software system at that time, MACSYMA, had “MAC” (= Machine Aided Cognition) in its\nname! And, of course, implementing heuristics is still a very important approach for improving the\npractical efﬁciency of methods for symbolic computation problems. However, in 1985, I deliberately de-\ncided against having “artiﬁcial intelligence” in the name of my institute since I wanted to emphasize the\nlogical, mathematical, formal approach to problem-solving over the psychological, experimental aspect,\nwhich some people (then and now) believe that goes “beyond mathematics”. I will go deeper into the\nanalysis of the relationship between symbolic computation and artiﬁcial intelligence later in this note.\nAnyway, although symbolic computation (in the sense of the editorial of the JSC) seems to be a quite\n\n4\nSymbolic Computation in Software Science: My Personal View\nestablished and stable notion, as a matter of fact, in the JSC over the years one can observe that\n– the majority of papers in the JSC is on computer algebra,\n– more and more, but still much fewer, papers are in automated reasoning,\n– only a few papers came in on automatic programming.\nSymbolic Computation in Software Science\nSoftware science is the science of the process of developing software. This process starts from problems\nin some “reality” (part of the real world) and creates software that solves the problems in an appropriate\nﬁnitary model of this reality. Since the beginning of the software age, the software development process\nhas matured from being a kind of “magic” and being an “art” to a decent engineering discipline called\n“software engineering”. In parallel, since the very beginning, people have also tried to establish a “sci-\nence” of software and the software development process to make the process more reliable, provably\ncorrect, faster, more ﬂexible, more economic, and ultimately automatic or semi-automatic. Research in\nthis direction is mostly summarized under the heading “theoretical computer science”. Interestingly, the\nterm “software science”, which seems quite natural to me, in comparison to “theoretical computer sci-\nence”, is only used quite rarely. (This can easily be veriﬁed by googling the two notions and comparing\nthe number of relevant results.)\nAnyway, I think that “software science” is a quite useful notion that focuses on the actual devel-\nopment process of software and on its automation and, hence, has a high impact on one of the central\ntechnologies - if not the central technology - of our age.\nSince the objects of software science are formal models (domains with ﬁnitary objects and algorith-\nmic operators on the objects), automation or semi-automation of the software design process is essentially\na “symbolic computation” process according to the deﬁnition of symbolic computation we considered\nabove. In other words, it should be clear that symbolic computation is the area that naturally should\ninclude also the (semi-)automation of the software development process. Unfortunately, this logical\nanalysis did not really create a big stream of papers on automating software development to the JSC (and\nneither to conferences in the area of symbolic computation like ISSAC, ACA, SYNASC, etc.). There-\nfore, in 2012, I argued that the topic “Symbolic Computation in Software Science” could and should get\nspecial attention by turning our group meetings into a conference series with this name.\nStill, the idea that symbolic computation should have a major application in software science - in par-\nticular in the automation of the software development process - did not create a big echo in the symbolic\ncomputation community. Neither do many people who work in software engineering realize that the\nautomation of the software development process is essentially a symbolic computation task. One reason\nfor this is, surely, that there are strong conference series and journals in the area of automated reasoning\nand related subjects. (A side-remark: As some readers may know, when I built up RISC starting from\n1985, I also devoted much of my time to building up the “Softwarepark Hagenberg”. With this, I wanted\nto demonstrate that the mathematically deep ﬁeld of symbolic computation has also the power to create\nsomething with a strong practical impact: I started the Softwarepark with 25 people. When I stepped\nback as the director of the Softwarepark Hagenberg in 2013, 2500 people working and/or studying in the\nSoftwarepark. I hoped that the government and the administration of my university would have noticed\nand recognized the unique power that RISC / symbolic computation had created and had turned into in-\nnovation in the software foundations, into software development, and into software business. Therefore,\n\nB. Buchberger\n5\nin 2013, I asked the government and university administration to establish an extra professorship “Soft-\nware Science” in the frame of RISC with the task of continuing my work for directing and expanding the\nSoftwarepark based on solid research on symbolic computation in software science. Indeed, in response\nto my request and argumentation, a professor position for “Software Science” was created but then, much\nto my displeasure, giving in to the pressure of the informatics department, the position was ﬁnally used\nfor something “more useful” for the education of the informatics undergraduates.)\nNow, what I called “automatic programming” in the preface of the JSC, could also be called “sym-\nbolic computation in software science”. In more detail, I want to make this clear in this note. If seen in\nthe right way, I think that symbolic computation in software science is / could be / should be the most /\none of the most fascinating topics of the next stage of mathematics / logic / computer science. (I like to\ncall mathematics, logic, and computer science together “thinking technologies” or just “full-stack math-\nematics”. Unfortunately, “mathematics” sounds old-fashioned to some people, sounds “non-creative”\nto others, boring to others, intimidating to again others. However, one may bend and turn this as one\nlikes, ﬁnally, at the top of the creative hierarchy of problem-solving and gaining knowledge by thinking,\nthere is mathematics at higher and higher levels - whether certain people in politics, science, economy,\nphilosophy, culture, media or the people at the beer table like it or not.)\nA Stream of Problems on the Way\nOn the way from a problem description / a collection of problem descriptions to an algorithm / program\n/ software system that solves the problem there are many creative steps each of which can be handled\nad hoc for the particular problem at hand by a mathematician, computer scientist, developer. Each of\nthese steps, however, can also be considered as a problem on the meta-level with some symbolic objects\n(like software requirements, programs, algorithm schemata, veriﬁcation conditions, etc.) as input and\nsymbolic objects as output for which we would like to have a general algorithmic solution.\nIn this section, we assume that all the symbolic objects on the way from a problem speciﬁca-\ntion/requirements to an algorithm / piece of software are expressions that describe or at least try to\ndescribe something “in general terms”. In particular, we assume that the problem speciﬁcation (even a\nvague attempt of a speciﬁcation that may need much clariﬁcation and reformulation) tries to describe the\nproblem in general and not only by examples.\nThe important case that a problem speciﬁcation, for certain reasons, can only be given by examples\nand cannot be explained in general terms, is analyzed in detail in the next section in the paragraph on\nmachine learning.\nIn the majority of cases, problem speciﬁcations are explicit in the sense that they are speciﬁed by\nan expression P[x,y] with input variable(s) x and output variables(s) y, and a solution algorithm A has to\nsatisfy P[x,A[x]], for all x. (However, there are important classes of algorithmic problems that cannot be\ndescribed in explicit form. For example, a canonical simpliﬁer A for an equivalence relation P cannot be\ndescribed in this form. More generally, for example, the speciﬁcation of operations on data structures by\naxioms or the construction of algorithmic isomorphic representations of mathematical domains is not an\nexplicit speciﬁcation. We cannot go into more details about this here.)\nDepending on the situation, the initial (often vague) problem descriptions may be given in natural\nlanguage, maybe mixed with drawings and diagrams, or in some formal language.\nAlso, it is important to distinguish between two extremes:\n• Finding algorithms for fundamental, non-trivial, stand-alone algorithmic problems: In this case,\nthe problem speciﬁcation and the solution algorithm are completely formal, symbolic objects and\n\n6\nSymbolic Computation in Software Science: My Personal View\neverything that happens between problem and solution should be amenable to algorithmic treat-\nment on the meta-level, i.e. to symbolic computation. For such problems, typically, time and\nmemory complexity is an issue. Examples: the problem of ﬁnding shortest paths in graphs; the\nproblem of ﬁnding symbolic integrals; the problem of ﬁnding Gr¨obner bases; etc.\n• Developing software for an entire application: In this case, the individual parts of the system\n(called “units”) should implement a (big) number of functionalities, most of which are not really\ndifﬁcult. Only some of the functionalities may involve the algorithmic solution of fundamental\nproblems. The algorithms for these functionalities, typically, are known and taken from reliable\nsources. The complexity of such systems originates from the huge number of units and the various\n(desired and undesired) interactions of the units. Also tuning of the known algorithms to the\napplication at hand is an issue.\nThis distinction is important for the following reason: The application of formal methods for estab-\nlishing the correctness of software only makes sense if we consider non-trivial algorithmic problems. In\ncontrast, for most of the millions of units to be developed in large software systems a formal speciﬁcation\nof the problem to be solved by the unit would be essentially identical to the code to be developed. In\nother words, a proposal for the code of a unit, in the case of “easy” problems, is a way for describing the\nproblem to be solved. This is the reason why rapid prototyping and agile software development, in such\nsituations, is so useful. It is also the reason why formal algorithm veriﬁcation methods are rarely used in\nthe practice of developing large software systems.\nExample. In a calendar software system, probably, we want one unit that should check whether a\nproposed new calendar entry collides with one of the existing entries. Let us assume that a calendar\nentry is characterized by its start time and end time. The input to the unit will then consist of four time\nmoments s1,e1,s2,e2 for the start time and the end time of the ﬁrst and the second entry, respectively,\nwith input condition s1 < e1 and s2 < e2. “After some thinking”, the problem will then be described by\nmost developers by a sentence like this: “The two calendar entries characterized by s1,e1,s2,e2 collide\niff s2 ≤s1 ≤e2 or s1 ≤s2 ≤e1.” Now it is clear that this “speciﬁcation” of the problem is, basically,\nalready the solution algorithm. Only some transformation into the syntax of the programming language\nused is necessary. No powerful algorithm veriﬁcation method or algorithm synthesis method is necessary\nin such a case.\nAs simple as the example is, it is not too simple to guarantee the avoidance of severe ﬂaws in the\ndevelopment. I tested the example out by presenting it to various (reasonably experienced) developers.\nAmazingly, a few came up with the following speciﬁcation / code: “The two calendar entries charac-\nterized by s1,e1,s2,e2 collide iff either s2 ≤s1 ≤e2 or s2 ≤e1 ≤e2.” This speciﬁcation is “incorrect”\nbecause it does not include the case s1 < s2 < e2 < e1, which of course “everybody” would also consider\nas a collision, even a “particularly heavy one”. (I put “incorrect” in quotation marks because, at the very\nﬁrst stage of uttering a request, the “customer is always right”. Maybe, he really wants what he tells!\nEither one just implements what he tells or one may consider the subsequent discussion as a way to ﬁnd\nout what he “really wants” or to change his mind about what he wants.) This shows that already in the\n“thinking” between a vague indication of a problem and its speciﬁcation (by a general statement, not\nonly by examples), severe mistakes may be made (or, considered differently, the request of the customer\nmay undergo serious changes). In our example, we also could start a little “earlier” and just say: “The\ntwo calendar entries characterized by s1,e1,s2,e2 collide iff the time interval [s1,e1] intersects with the\ntime interval [s2,e2].” Now we could question the notion “intersects” and might agree on the following:\n“The two calendar entries characterized by s1,e1,s2,e2 collide iff there is a time moment x such that\n\nB. Buchberger\n7\ns1 ≤x ≤e1 and s2 ≤x ≤e2.” In this form, we can send the condition into a quantiﬁer elimination algo-\nrithm and we will get an answer which will be equivalent to “s2 ≤s1 ≤e2 or s1 ≤s2 ≤e1.” (Please try\nit out, it is worthwhile!) Hence, this simple example shows that, actually, already in the very early stage\nof discussing and clarifying even seemingly simple requirements a lot of systematic/formal thinking is\ninvolved, which in principle should be amenable to automating and, hence, symbolic computation!\nThus, we start at the very early stage of having a vague desire of achieving something by software\nand go through all the stages of developing a piece of software that fulﬁlls the desire and, further, through\nall the stages of maintaining, updating, improving, and integrating pieces of software to fulﬁll more and\nmore sophisticated desires. Through all these stages, we ask ourselves how much of this process can\nbe (semi-)automated. This gives a rich list of R&D topics, which make up the important topics in the\nscope of “symbolic computation in software science” as described in the calls for the SCSS series, see\nthe latest version in the call for SCSS 2021. This call contains, roughly, 20 important and quite diverse\nbut strongly interconnected topics on the way from requirements to software.\nI do not list these topics and comment on all these topics here. Rather, let me give some personal\nremarks that emphasize, and maybe expand, some of the subjects, themes, and objectives of the topics in\nthe SCSS calls.\n• My feeling is that relatively little research is available on (semi-)automating the development of\nlarge software systems consisting of tons of simple “units” of the type we have seen above in\nthe example. Research has focused more on symbolic methods for algorithm veriﬁcation and\nsynthesis for non-trivial algorithms. In some way, this is unfortunate because the construction of\ntons of software is necessary today, semi-automation of this process is needed and could be a big\nbusiness. Our research results are too much oriented on automating the invention of “important”,\n“difﬁcult” algorithms. However, the (semi-) automation of the development of huge amounts of\nsimple programs and their interaction, in some way, is quite challenging, much needed, and asks\nfor formal methods to guarantee the quality of the process.\n• As a variant of developing big software systems consisting of many simple units we also should\nconsider the task of re-engineering big software systems that were written years ago in program-\nming languages that are antiquated now. Often, the documentation of such systems is lost or\nfragmentary, and ﬁnding out what the units should do, i.e. getting a problem speciﬁcation from\ncode, is a major task.\n• In most cases, software development starts from vague requirements in natural language (maybe\nwith diagrams or drawings). The task is to come up, maybe in an interactive dialogue, with a bunch\nof formal requirements. Here, we should allow natural language or, maybe, a simpliﬁed version\nof natural language as a symbolic language: The sentences that formulate requirements are “ﬁni-\ntary” with inﬁnite semantics since, normally, requirements have hidden universal quantiﬁers in it.\n(See the simple example above: The requirement is formulated for arbitrary calendar entries. In\nour ﬁrst step towards formalization, the hidden universal quantiﬁer goes over s1,e1,s2,e2.) We\nalso should allow diagrams or drawings as symbolic objects: They are surely “ﬁnitary” and, usu-\nally, have inﬁnite semantics, since a drawing normally tries to convey the important features of\ninﬁnitely many possible individual situations. Specifying requirements by natural language text\nor drawings is very different from the speciﬁcation of requirements by ﬁnitely many input/output\npairs, see the analysis of “machine learning” in the next section. Allowing natural language or\ndrawings in requirements is of course a big challenge but I think we should take this deliberately\nunder the umbrella of SCSS because it will need much more than just ordinary NLP (natural lan-\nguage processing) and graphics. Rather, a systematic connection to logic is necessary. (In fact,\n\n8\nSymbolic Computation in Software Science: My Personal View\nin dynamic geometry systems a lot of work in this direction is already done when graphical input\nexplaining geometrical situations “in general positions” is allowed.)\n• As we have seen in the simple example above, we also would like to go a step further and go\nfrom requirements in natural language and/or drawings right away to algorithms/programs that\nsatisfy the requirements. As we have argued in the example, in the majority of “units” in appli-\ncation software systems this will not be signiﬁcantly more difﬁcult than coming up with formal\nrequirements.\n• The individual algorithms/programs in software systems do not live in empty air but inside a whole\nhierarchy of data structures and domains which, depending on the context, are called (algorithmic)\n“models” of reality. Such models consist of problem speciﬁcations, deﬁnitions of notions, knowl-\nedge, algorithms, and - in the ideal case - arguments/proofs why the operations/algorithms in the\nsystem meet their speciﬁcations. Hence, seen in this way, software systems can also be considered\nmathematical knowledge systems. Hence, (semi-)automation of building and maintaining such\nsystems can also be seen under the umbrella of Mathematical Knowledge Management (MKM).\nWe introduced this term a couple of years ago in a slightly different context, see the preface of the\nproceedings [4], which were expanded as the special issue [5]. We propose that SCSS and MKM\nshould be considered together and, maybe, SCSS and MKM should be collocated in the future.\n• In practice, the correctness of software is established by testing rather than by formal veriﬁcation.\nTesting is a highly developed “technology” in software engineering: There is an arsenal of “au-\ntomated software testing” systems available. They are well integrated into the various software\ndevelopment environments and they are quite helpful for managing large test suites for the consec-\nutive versions of software systems. However, I think that much more could be done by applying\nformal methods for coming up with complete systems of test data from a given problem speciﬁ-\ncation and program. Here, completeness means that we would get one test input/output for each\nequivalence class of inputs that generate the same program path during execution. Of course, in\ngeneral, the set of these equivalence classes is not ﬁnite. However, in the practical case of large\nsoftware systems consisting of a huge number of relatively simple units, the set of equivalence\nmay well be ﬁnite, see the example above. As can be seen in the example, generating a complete\nsystem of equivalence classes for inputs might be essentially the same task as coming up with the\ncode for the program. In fact, this automated generation of equivalence classes should start from\nthe problem speciﬁcation and not from a program code - as most of the commercial “white box”\ntest generation programs do.\nHow Does Artiﬁcial Intelligence Fit into the Picture?\nUndoubtedly, in the past two decades, artiﬁcial intelligence has gained enormous attention. This is due\nto the fact that, by the drastically increased computational power of current computer systems and the\navailability of huge databases of “labeled” data, a couple of difﬁcult and urgent problems have received\nimpressive solutions by artiﬁcial intelligence methods, as for example machine translation of natural\nlanguages.\nAmazingly, there is still a lot of mystery, nebulosity, and misunderstanding around what artiﬁcial\nintelligence (AI) actually is and why it is / may be / is believed to be essentially different from all com-\nputational approaches so far. This nebulosity is all over the place: in politics, in the media, even in\nscience, and, of course, with the man on the street. At times, I have the impression that even quite some\n\nB. Buchberger\n9\nresearchers in the AI area do not have a very clear picture of the distinctive characteristics of AI when\ncompared with other computational approaches. Also, labeling a project with AI, may have a beneﬁcial\neffect when it comes to funding, societal respect, political inﬂuence etc. Thus, it is tempting to keep the\nnotion ambiguous. What amazes me, even more, is that the nebulosity about the essence of AI did not\ndisappear since the ﬁeld started in the middle of the ﬁfties. I remember talks of AI evangelists around\n1980, i.e. in the “ﬁrst wave of AI research”, who believed and spread that “AI can solve hard problems\nthat cannot be solved by mathematics”. And still, when I participate in political discussions about the im-\nportance of mathematical education (in the sense of training mathematical thinking), I hear the argument\nthat, actually, the ability to do mathematics will be less and less important because “tedious” mathemati-\ncal thinking, in the presence of “artiﬁcial intelligences” (plural!), will not be necessary anymore and that\nwe should teach the youngsters more “creative” things than mathematics.\nNow all such statements may be true or false according to which notion of artiﬁcial intelligence one\nhas in mind. For clarifying this notion, I want to distinguish three possible characterizations of AI:\nHard Problems: Artiﬁcial Intelligence may be described as the ﬁeld that tries to solve problems that, at a\ncertain historic moment, are considered to be “hard” in the sense that they apparently need a decent\namount of (human) “intelligence” to solve them. For example, playing chess or ﬁnding symbolic\nintegrals, at some historic moment, were considered as needing human intelligence. Algorithms\n(invented by humans!) that ﬁnally were able to solve these problems were then (and still are)\nconsidered to be the result of “AI research”.\nNow, in my opinion, this deﬁnition of the notion of AI is quite shallow. It is the natural ﬂow of\nscience and technology that we can solve harder and harder problems automatically, i.e. by algorithms.\nHowever, from some point on, people think that now “algorithms are taking over”, “artiﬁcial intelligence\nis replacing humans” etc. forgetting that this happened and happens already since centuries and that this\nis the very goal of science and technology. And, of course, whatever the methods behind automation were\nand are, we humans should stay in control and decide how far we let problems be solved and decisions\nbe taken by algorithms. Anyway, the notion of a “hard” problem is relative and “hard” problems for\nwhich an algorithmic solution was ﬁnally found very soon are considered to be “easy” by the consumer.\nFor example, car drivers nowadays take the functionality of a navigation system for granted. Some thirty\nyears ago, the current functionality of navigation systems would have been considered unbelievably\nintelligent. In fact, the stack of scientiﬁc ﬁndings and algorithmic techniques involved in a navigation\nsystem for guiding a driver from A to B is quite deep.\nIn my opinion, one should not use the notion of “artiﬁcial intelligence” for “ﬁnding algorithms for\nhard problems” but rather continue to call this just “mathematical, algorithmic solution of hard prob-\nlems”. Attaching the label “AI” to algorithms depending on whether they solve hard or easy problems is\nmore a question of marketing rather than a logically sound distinction.\nSimulate the Brain: A completely different view (and branch) of artiﬁcial intelligence is artiﬁcial intelli-\ngence as the science of understanding and simulating biological structures that show “intelligence”,\nnotably the human brain. This type of AI research, historically, was one of the origins of the ﬁeld\nof AI that started, maybe, 1943 with the investigations of W. McCulloch and W. Pitts who intro-\nduced a simple mathematical model of the functionality of a neuron. Of course, understanding\nand simulating the most complex biological systems, which are commonly considered to display\n“intelligence”, is a highly fascinating and relevant undertaking. Well, why not call this type of\nresearch “artiﬁcial intelligence” in the same way as a technical realization of the phenomenon of\nﬂying could be called “artiﬁcial ﬂying”.\n\n10\nSymbolic Computation in Software Science: My Personal View\n“Artiﬁcial intelligence” in the sense of brain simulation has little overlap with symbolic computation\nin software science except that, of course, there may be applications of symbolic computation in devel-\noping models of the brain. Also, studying biological structures (like the brain, like swarms of animals,\nor like the evolution of life on earth) motivated some of the algorithmic methods that today are called\n“AI methods”, see next paragraphs.\n“Intelligent” Methods: The third approach of characterizing artiﬁcial intelligence is by specifying cer-\ntain algorithmic methods as “intelligent”. These algorithms would constitute the area of “artiﬁcial\nintelligence”. I hope I do not overlook something important but my impression is that, essentially,\n“machine learning” is the only such method or, better, class of methods that has not already been\naround before the term “artiﬁcial intelligence” was coined. The many other algorithmic methods\nthat are often labeled as “AI methods”, like automated reasoning, semantic networks, graph search,\nexpert systems, regression, etc., in my view, are algorithmic methods that are not speciﬁc to AI.\nThey are, so to say, usual algorithmic methods and were applied also to problems that, for some\nreason, got the label “AI”, see the remarks about hard problems above.\nIn my view, machine learning methods cannot actually be speciﬁed by the way how they work but,\nrather, by the way the problems these methods should solve are speciﬁed. As we have seen in the\nprevious section, the fundamental part of algorithm and software development is the transition from\na given problem speciﬁcation P to an algorithm (program, system) A that solves the problem for any\nadmissible input. As long as the steps for going from a problem speciﬁcation to a solution algorithm are\ndone by a human this is just the “usual” business of mathematics/informatics. If ﬁnding these steps is\n(partially) supported by algorithms (invented by humans) this is what we can call “symbolic computation\nin software science”. How and when does “machine learning” come in and why, if at all, is this different\nfrom “usual” mathematics and “usual” (maybe quite sophisticated) symbolic computation in software\nscience?\nThe point is that, in many situations, when we want to specify a problem, we do not have a speciﬁca-\ntion “in general terms” available. For example, let’s consider the seemingly simple problem of deciding\nwhether a given English sentence contains information of the type “somebody cooperates with some-\nbody else”. An algorithm for this problem should produce the answer “NO” in case no such information\nis in the input sentence and should produce “YES” and the two “somebodies” if such information is in\nthe sentence. Now, of course, before trying to invent such an algorithm, we will ask: What exactly do\nyou mean by “cooperate”? Among the English speaking community, under the natural assumption of\na long experience of using English in thousands of different situations, it would be natural the start to\nexplain “cooperate” in terms of a couple of other notions like “working together”, “having a common\ngoal”, . . . Oh, “having a common goal” may not always be sufﬁcient for speaking about “cooperation”.\nOne may have a common goal but ﬁght against each other. Thus, “supporting each other” etc. should\nbe added. Some more subtle details should be explained, some other things excluded etc. A long list of\nsentences explaining the meaning of “cooperate” would be necessary. Then one could, in the attempt of\nﬁnding an algorithm for this little problem, try to put these numerous explanations into algorithmic rules\n(assuming that we already have access to a powerful grammar parsing algorithm for all of English). As\na result, we would hope that this rule system would be able to do the job. For example, if we now would\ninput “Peter and Ann found a way to help each other for passing the exam”, the algorithm should answer\n“YES”, “Peter”, “Ann”. If we would input “Peter and Ann passed the exam on the same day”, it should\nanswer “NO”. Should it really answer “NO”? Shouldn’t it rather answer “DON’T KNOW” or “COULD\nBE” or “COULD BE BUT NOT EXPLICITLY MENTIONED”.\nI now want to explain what, in my view, is the essence of the machine learning approach. For this,\n\nB. Buchberger\n11\nwe need not at all bother about what “learning” is. I just consider those methods that, over the years,\nhave been named “machine learning” methods. The common feature of these methods is not how they\nproceed but the type of speciﬁcation of the problems to which they are applied: Namely, they all are\napplied to problems of the kind above where a spelled-out complete speciﬁcation is not possible or, at\nleast, not feasible. Now, the fundamental idea of machine learning for solving such problems is:\n• Instead of spending time trying to specify the problem by a huge number of general deﬁnitions,\ncases, rules, etc., one spends the time giving a huge number of examples of input instances together\nwith the answers. (In this paper, we consider only “supervised learning”.) In this context, the\nanswers are called “labels”.\n• One sets up an algorithm from a certain class of relatively simply structured algorithms (like the\nclass of neural networks, the class of hyperplanes in a high-dimensional space, the class of nested\nif-then-else expressions, etc.) with some constants c1,...,cn (for example the weights at the inputs\nof neurons in neural networks) in the algorithm left unspeciﬁed. For each choice of numerical\nvalues for the c1,...,cn, the algorithm would produce an answer for each admissible input for the\nproblem.\n• One uses techniques of mathematical optimization (or other, experimental techniques, for example\ntechniques that mimic biological evolution) to change the initial values for c1,...,cn iteratively\nuntil the answer of the algorithm to more and more inputs from the set of labeled data would give\nthe answer speciﬁed by the label. In the jargon of machine learning, this iteration is called “training\na model”.\n• One stops the iteration on the c1,...,cn when sufﬁciently many answers are identical to the labels.\nPractically, at the beginning of the whole operation, one partitions the set of labeled input into a\n“training set” that is used for the iterations and a “test set” on which the algorithm with the current\nvalues for the c1,...,cn - which in the jargon of machine learning is called the “trained model” - is\ntested.\nThe impressive success of this approach in the past two decades hinges on three ingredients:\n• a huge amount of mathematical research on good and, partly provably convergent, techniques\nfor improving the algorithm parameters c1,...,cn; such research was partly already available in\nthe ﬁrst phase of AI between 1960 and 1980, but it did not convince because of the next two\ningredients were not available,\n• huge corpora of labeled data; for example, in the spectacular application of machine translation, a\nhuge amount of “labeled data” is now available in the form of ﬁles that contain an original book\nand its translation - by humans - to some other language,\n• high-performance computing; the number of iterations of the machine learning steps for determin-\ning suitable c1,...,cn and the computational effort in each step is huge and is only manageable by\ncomputers in recent years.\nIn principle, the approach is not radically new. Examples of historical “learning from examples”\nproblems are: Given points in the plane, ﬁnd the coefﬁcients c1,...,cn of a polynomial that goes through\nthe given points (the interpolation problem). Given a function with some properties on differentiability,\nan interval, and a distance, ﬁnd the coefﬁcients c1,...,cn of a polynomial that is closer to the function\nthan the given distance everywhere on the interval (approximation problem). Given points in the plane,\nﬁnd the coefﬁcients of a straight line such that the distance to all points is minimal (regression problem).\n\n12\nSymbolic Computation in Software Science: My Personal View\nGiven a function with certain differentiability properties, ﬁnd the coefﬁcients c1,...,cn of a ﬁnite Fourier\napproximates of the function. Etc.\nArtiﬁcial Intelligence in the form of machine learning falls neatly into the “automatic programming”\nview: It is the method of choice in cases where the problem is not speciﬁed by general (formal or natural\nlanguage) statements but, rather, is speciﬁed only by a (huge) number of examples of admissible input\nand desired output. In the case of general speciﬁcations of problems, the transition from the problem to\na solving algorithm, in principle, is a reasoning process that is executed by humans or, in the symbolic\ncomputation approach advocated in this paper, is a reasoning process (partly) executed by symbolic\ncomputation methods. In the case of problems that are speciﬁed only by examples, this process can still\nbe automated by the machine learning approach.\nFrom the simple summary of the machine learning approach I gave above, one important deﬁciency\nof the machine learning approach should be clear: The algorithm which we get for a given problem just\ndoes the job of delivering (in sufﬁciently many cases) desired answers. However, in general, no reason\ncan be given why, for example, the particular neural network that translates one natural language to the\nother mimics certain fundamental insights about the environment both languages share as their semantics.\nThis is, in fact, similar to the situation in the historical predecessors of “learning from examples”: The\nFourier analysis just does the job of ﬁnding an optimal Fourier sum. In the example, where the function\nto be represented is the frequency spectrum of a musical tone, the representation by a ﬁnite Fourier\nsum has a reasonable “explanatory” power: The tone is composed of tones and overtones that occur in\nthe physical “music” world (for example, when picking the strings of a guitar). However, if a Fourier\nrepresentation of some arbitrary other function is executed, there will be some outcome but there may not\nbe any reasonable interpretation of what this representation means in the reality from which the function\nis taken.\nThe problem of weak explanatory power in the models (algorithms) created in machine learning is\nwell known. Lots of research was recently started to extract “meaning” from such models. This research\narea is called “explainable AI”.\nIn the frame of the analysis of this paper, I summarize: The machine learning approach can be well\nsubsumed under the general target of (semi-)automating the process of software development (“auto-\nmatic programming”). It can be viewed as a numerical, rather than a symbolic, approach to automatic\nprogramming. Thus, it is probably a very good idea to integrate machine learning into the scope of the\nSCSS series because, of course, the interaction of symbolic and numerical computation as the two pos-\nsible approaches to compute on ﬁnitary representations of abstract mathematical domains is of utmost\nimportance. The integration of machine learning into the scope of SCSS can generate a stream of new\nideas in both directions: Applying symbolic methods to mathematical sub-problems of machine learning\n(e.g. the determination of weights in neural networks) and applying machine learning to symbolic al-\ngorithms (e.g. “learning” a priori complexity estimates for computation-intensive methods like Gr¨obner\nbases etc.).\nHowever, there is no reason to establish a ﬂavor of “intelligence beyond mathematics” when speaking\nabout machine learning: I hope I have been able to show the machine learning is just another mathemat-\nical method. As in the past, of course, we can hope and expect also for the future that more and more\npowerful algorithmic problem-solving methods will be invented.\nPersonally, when speaking to people who do not (want to) understand the timeless, universal, al-\nways new, creative power of mathematics, I like to use the term algorithmic intelligence for what we are\ndoing: Algorithmic intelligence is the human intelligence that produces algorithms for more and more\nchallenging problems in all areas of human activity. By an algorithm, an inﬁnite class of individual\nproblem instances can then be treated by a completely unintelligent machine. People who do not really\n\nB. Buchberger\n13\nunderstand what is going on may believe that these machines display “intelligence”. The algorithmic\nintelligence - by reﬂection, i.e. jumps to higher and higher meta-levels - also provides more and more so-\nphisticated algorithms for producing algorithms. The incompleteness theorem of Kurt G¨odel (1931), in\na somewhat liberal interpretation, shows that this tour through higher and higher levels of algorithmiza-\ntion has no upper bound. In comparison to “artiﬁcial intelligence”, the term “algorithmic intelligence”\nis used quite rarely, which can be veriﬁed by Googling. However, my impression is that “algorithmic\nintelligence” appears in more serious discussions about the essence of AI. Therefore, I like to expand the\nabbreviation “AI” as “algorithmic intelligence”.\nImplicitly, I expressed this view already in the early days of AI, see [1]. At the “Spring School of AI”\nin Teisendorf, 1982, I contributed a long article summarizing the most important “symbolic” methods\nfor automating the algorithm/software development process that were known at that time. And we had\nlong, intensive, and quite controversial discussions at this conference on the question of whether AI is\nsomething that goes beyond mathematics. As you may guess, my answer then was “no” with essentially\nthe arguments which I expanded above. In my hectic years of research on methods for “algorithmic\nintelligence” and research management, I never found the time and occasion to spell out these arguments\nin a paper. Thus, I am grateful that I am given the opportunity here.\nReferences\n[1] Bruno Buchberger (1982): Computer-unterst¨utzter Algorithmenentwurf (Computer-Aided Algorithm De-\nsign).\nIn Wolfgang Bibel & J¨org H. Siekmann, editors: Proceedings of the Fr¨uhjahrsschule K¨unstliche\nIntelligenz (Spring School in Artiﬁcial Intelligence), Teisendorf, Germany, 15.-24. M¨arz 1982, Informatik-\nFachberichte 59, Springer, pp. 141–201, doi:10.1007/978-3-642-68828-7_4.\n[2] Bruno Buchberger (1985): Symbolic Computation (An Editorial). Journal of Symbolic Computation 1(1), pp.\n1–6, doi:10.1016/S0747-7171(85)80025-0.\n[3] Bruno Buchberger (2013): Mathematics of the 21st Century: A Personal View. In Laura Kov´acs & Temur\nKutsia, editors: Proceedings of the Fifth International Symposium on Symbolic Computation in Software\nScience (SCSS 2013), RISC Report Series 13-06, Johannes Kepler University, Linz/Hagenberg, Austria, p. 1.\nAvailable at https://www.risc.jku.at/publications/download/risc_4737/TR_13_06_SCSS2013.\npdf. (See also the link to the slides of this talk on the website of SCSS 2013 at https://www.risc.jku.\nat/conferences/scss2013/program.html.).\n[4] Bruno Buchberger & Olga Caprotti, editors (2001):\nElectronic Proceedings of the First Interna-\ntional Workshop on Mathematical Knowledge Management (MKM 2001). RISC, Johannes Kepler Uni-\nversity, Linz/Hagenberg, Austria.\nAvailable at https://www.risc.jku.at/conferences/MKM2001/\nProceedings/.\n[5] Bruno Buchberger, Gaston H. Gonnet & Michiel Hazewinkel (2003): Mathematical Knowledge Management.\nSpecial issue of the Annals of Mathematics and Artiﬁcial Intelligence 38(1-3), pp. 1–2, doi:10.1023/A:\n1022900528196.",
    "pdf_filename": "Symbolic Computation in Software Science - My Personal View.pdf"
}