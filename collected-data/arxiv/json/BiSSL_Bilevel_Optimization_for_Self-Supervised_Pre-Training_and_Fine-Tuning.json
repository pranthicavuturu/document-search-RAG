{
    "title": "BiSSL Bilevel Optimization for Self-Supervised Pre-Training and Fine-Tuning",
    "abstract": "In this work, we present BiSSL, a first-of-its-kind training framework that in- troduces bilevel optimization to enhance the alignment between the pretext pre- training and downstream fine-tuning stages in self-supervised learning. BiSSL formulates the pretext and downstream task objectives as the lower- and upper- level objectives in a bilevel optimization problem and serves as an intermediate training stage within the self-supervised learning pipeline. By more explicitly modeling the interdependence of these training stages, BiSSL facilitates enhanced information sharing between them, ultimately leading to a backbone parameter initialization that is better suited for the downstream task. We propose a training algorithm that alternates between optimizing the two objectives defined in BiSSL. Using a ResNet-18 backbone pre-trained with SimCLR on the STL10 dataset, we demonstrate that our proposed framework consistently achieves improved or com- petitive classification accuracies across various downstream image classification datasets compared to the conventional self-supervised learning pipeline. Quali- tative analyses of the backbone features further suggest that BiSSL enhances the alignment of downstream features in the backbone prior to fine-tuning. 1 INTRODUCTION In the absence of sufficient labeled data, self-supervised learning (SSL) has emerged as a promis- ing approach for training deep learning models. Rather than relying solely on labeled data, the SSL framework aims to learn representations from unlabeled data which proves beneficial for subsequent use on various downstream tasks. These representations are learned by solving a pretext task, which utilizes supervisory signals extracted from the unlabeled data itself. Extensive efforts has gone into designing effective pretext tasks, achieving state-of-the-art or competitive performance in various fields such as computer vision (Chen et al., 2020b; Bardes et al., 2022; He et al., 2020; Grill et al., 2020; Caron et al., 2020; 2021; He et al., 2022; Oquab et al., 2024), audio signal processing (Schnei- der et al., 2019; Baevski et al., 2020; Hsu et al., 2021; Niizumi et al., 2021; Chung & Glass, 2018; Chung et al., 2019; Yadav et al., 2024) and natural language processing (Devlin et al., 2019; Lewis et al., 2019; Brown et al., 2020; He et al., 2021; Touvron et al., 2023). Making a self-supervised pre-trained backbone suitable for a downstream task typically involves attaching additional layers that are compatible with that task, followed by fine-tuning the entire or parts of the composite model in a supervised manner (Zhai et al., 2019; Dubois et al., 2022). When a backbone is pre-trained on a distribution that differs from the distribution of the downstream data, the representations learned during pre-training may not be initially well-aligned with the downstream task. During fine-tuning, this distribution misalignment could cause relevant semantic information, learned during the pre-training phase, to vanish from the representation space (Zaiem et al., 2024; Chen et al., 2020a; Boschini et al., 2022). A potential strategy for alleviating the negative effects of these distribution discrepancies would be to enhance the alignment between the pretext pre-training and downstream fine-tuning stages. However, since the conventional SSL pipeline treats these stages as two disjoint processes, this poses a significant challenge in devising a strategy that enhances such alignment while not compromising on the benefits that SSL offers. 1 arXiv:2410.02387v2  [cs.LG]  19 Nov 2024",
    "body": "Preprint. Under review.\nBISSL:\nBILEVEL\nOPTIMIZATION\nFOR\nSELF-\nSUPERVISED PRE-TRAINING AND FINE-TUNING\nGustav Wagner Zakarias1,3\nLars Kai Hansen2,3\nZheng-Hua Tan1,3\n1Aalborg University\n2Technical University of Denmark\n3Pioneer Centre for Artificial Intelligence, Denmark\n[gwz,zt]@es.aau.dk, lkai@dtu.dk\nABSTRACT\nIn this work, we present BiSSL, a first-of-its-kind training framework that in-\ntroduces bilevel optimization to enhance the alignment between the pretext pre-\ntraining and downstream fine-tuning stages in self-supervised learning. BiSSL\nformulates the pretext and downstream task objectives as the lower- and upper-\nlevel objectives in a bilevel optimization problem and serves as an intermediate\ntraining stage within the self-supervised learning pipeline. By more explicitly\nmodeling the interdependence of these training stages, BiSSL facilitates enhanced\ninformation sharing between them, ultimately leading to a backbone parameter\ninitialization that is better suited for the downstream task. We propose a training\nalgorithm that alternates between optimizing the two objectives defined in BiSSL.\nUsing a ResNet-18 backbone pre-trained with SimCLR on the STL10 dataset, we\ndemonstrate that our proposed framework consistently achieves improved or com-\npetitive classification accuracies across various downstream image classification\ndatasets compared to the conventional self-supervised learning pipeline. Quali-\ntative analyses of the backbone features further suggest that BiSSL enhances the\nalignment of downstream features in the backbone prior to fine-tuning.\n1\nINTRODUCTION\nIn the absence of sufficient labeled data, self-supervised learning (SSL) has emerged as a promis-\ning approach for training deep learning models. Rather than relying solely on labeled data, the SSL\nframework aims to learn representations from unlabeled data which proves beneficial for subsequent\nuse on various downstream tasks. These representations are learned by solving a pretext task, which\nutilizes supervisory signals extracted from the unlabeled data itself. Extensive efforts has gone into\ndesigning effective pretext tasks, achieving state-of-the-art or competitive performance in various\nfields such as computer vision (Chen et al., 2020b; Bardes et al., 2022; He et al., 2020; Grill et al.,\n2020; Caron et al., 2020; 2021; He et al., 2022; Oquab et al., 2024), audio signal processing (Schnei-\nder et al., 2019; Baevski et al., 2020; Hsu et al., 2021; Niizumi et al., 2021; Chung & Glass, 2018;\nChung et al., 2019; Yadav et al., 2024) and natural language processing (Devlin et al., 2019; Lewis\net al., 2019; Brown et al., 2020; He et al., 2021; Touvron et al., 2023).\nMaking a self-supervised pre-trained backbone suitable for a downstream task typically involves\nattaching additional layers that are compatible with that task, followed by fine-tuning the entire or\nparts of the composite model in a supervised manner (Zhai et al., 2019; Dubois et al., 2022). When a\nbackbone is pre-trained on a distribution that differs from the distribution of the downstream data, the\nrepresentations learned during pre-training may not be initially well-aligned with the downstream\ntask. During fine-tuning, this distribution misalignment could cause relevant semantic information,\nlearned during the pre-training phase, to vanish from the representation space (Zaiem et al., 2024;\nChen et al., 2020a; Boschini et al., 2022). A potential strategy for alleviating the negative effects of\nthese distribution discrepancies would be to enhance the alignment between the pretext pre-training\nand downstream fine-tuning stages. However, since the conventional SSL pipeline treats these stages\nas two disjoint processes, this poses a significant challenge in devising a strategy that enhances such\nalignment while not compromising on the benefits that SSL offers.\n1\narXiv:2410.02387v2  [cs.LG]  19 Nov 2024\n\nPreprint. Under review.\nConventional SSL Pipeline\nPretext Task\nDownstream Fine-Tuning \nSSL Pipeline With BiSSL\nPretext Task Warm-up \nDownstream Fine-Tuning \nDownstream Head Warm-up\nBiSSL\n \nUpper-Level (Downstream) \nLower-Level (Pretext) \n Init \n \n \nFigure 1: The conventional self-supervised learning pipeline alongside the proposed pipeline in-\nvolving BiSSL. The symbols θ and ϕ represent obtained backbone and task-specific attached head\nparameters, respectively. When they are transmitted to the respective training stages, they are used\nas initializations.\nMeanwhile, bilevel optimization (BLO) has risen as a powerful tool for solving certain optimization\nproblems within deep learning. It entails a main optimization problem constrained by the solution\nto a secondary optimization problem that depends on the parameters of the main objective. This hi-\nerarchical setup causes the solutions of both optimization problems to depend on each other, either\ndirectly or implicitly, which has proven advantageous in deep learning tasks that optimize multiple\ninter-dependent objectives simultaneously (Zhang et al., 2023a). Notable mentions of tasks within\ndeep learning where BLO has proven useful are parameter pruning (Zhang et al., 2022b), invariant\nrisk minimization (Arjovsky et al., 2019; Zhang et al., 2023b), meta-learning (Rajeswaran et al.,\n2019; Finn et al., 2017), adversarial robustness (Zhang et al., 2021), hyper-parameter optimiza-\ntion (Franceschi et al., 2018) and coreset selection (Borsos et al., 2020).\nIn this study, we propose BiSSL, a novel training framework that leverages BLO to enhance the\nalignment between the pretext pre-training and downstream fine-tuning stages in SSL. Acting as\nan intermediate training stage within the SSL pipeline, BiSSL frames the pretext and downstream\ntask objectives as the lower- and upper-level objectives in a BLO problem - a challenging approach\nthat has not been explored until now. The objectives in BiSSL are connected by substituting the\nlower-level backbone solution for the upper-level backbone parameters, while simultaneously en-\nforcing the lower-level backbone solution to resemble the upper-level backbone parameters. This\napproach more explicitly captures the interdependence between pretext pre-training and downstream\nfine-tuning, potentially leading to a lower-level backbone better aligned with the downstream task.\nFigure 1 compares the conventional SSL pipeline with our suggested pipeline involving BiSSL. Ad-\nditionally, we propose a training algorithm for BiSSL and demonstrate that it consistently improves\nor maintains comparable downstream performance across a range of image classification datasets.\nFor our experiments, we use SimCLR (Chen et al., 2020b) to pre-train a ResNet-18 backbone (He\net al., 2016) on the unlabeled partition of the STL10 dataset (Coates et al., 2011), a setup offering\nsuitable model capacity and dataset complexity while being less resource-intensive than larger-scale\nalternatives. The code implementation and pre-trained model weights are publicly available.1\n2\nRELATED WORK\nBilevel Optimization in Self-Supervised Learning\nBilevel optimization (BLO) refers to a con-\nstrained optimization problem, where the constraint itself is a solution to another optimization prob-\n1https://github.com/ICLR25-10484/ICLR25_10484_BiSSL\n2\n\nPreprint. Under review.\nlem, which depends on the parameters of the “main” optimization problem. The general BLO\nproblem is formulated as\nmin\nξ\nf(ξ, ψ∗(ξ))\ns.t.\nψ∗(ξ) ∈argmin\nψ\ng(ξ, ψ),\n(1)\nwhere f and g are referred to as the upper-level and lower-level objectives, respectively. While\nthe lower objective g has knowledge of the parameters ξ from the upper-level objective, the upper-\nlevel objective f possesses full information of the lower objective g itself through its dependence\non the lower-level solution ψ∗(ξ). Some works have incorporated bilevel optimization within self-\nsupervised learning. Gupta et al. (2022) suggest formulating the contrastive self-supervised pretext\ntask as a bilevel optimization problem, dedicating the upper-level and lower-level objectives for up-\ndating the backbone and projection head parameters respectively. Other frameworks such as the\nLocal and Global (LoGo) (Zhang et al., 2022a) and Only Self-Supervised Learning (OSSL) Boonlia\net al. (2022) utilize auxiliary models, wherein the lower-level objective optimizes the parameters of\nthe auxiliary model, while the upper-level objective is dedicated to training the feature extraction\nmodel. MetaMask (Li et al., 2022b) introduces a meta-learning based approach, where the upper-\nlevel learns masks that filter out irrelevant information from inputs that are provided to a lower-level\nself-supervised contrastive pretext task. Chen et al. (2023) introduces a pseudo-BLO setup where\nthe upper-level optimization still benefits from knowledge of the lower-level objective, but the pa-\nrameters of the lower-level objective are fixed during training. In Somayajula et al. (2023), a two-\nstaged BLO problem is proposed to fine-tune self-supervised pre-trained large language models in\nlow-resource scenarios. Their approach focuses on solving downstream tasks while simultaneously\nlearning a task-dependent similarity structure. BLO-SAM (Zhang et al., 2024) is tailored towards\nfine-tuning the segment anything model (SAM) (Kirillov et al., 2023) by interchangeably alternating\nbetween learning (upper-level) prompt embeddings and fine-tuning the (lower-level) segmentation\nmodel. The aforementioned frameworks integrate bilevel optimization into either the pre-training or\nfine-tuning stage exclusively and are tailored towards specific pretext or downstream tasks. In con-\ntrast, our proposed BiSSL employs a BLO problem that comprehensively incorporates both training\nstages of pretext pre-training and downstream fine-tuning, without being confined to any specific\ntype of pretext or downstream task.\nPriming Pre-Trained Backbones Prior To Fine-Tuning\nPrevious works have demonstrated that\ndownstream performance can be enhanced by introducing techniques that modify the backbone be-\ntween the pre-training and fine-tuning stages. Contrastive Initialization (COIN) (Pan et al., 2022)\nintroduces a supervised contrastive loss, to be utilised on backbones pre-trained with contrastive\nSSL techniques. Noisy-Tune (Wu et al., 2022) perturbs the pre-trained backbone with tailored noise\nbefore fine-tuning. Speaker-invariant clustering (Spin) (Chang et al., 2023) utilizes speaker dis-\nentanglement and vector quantization for improving speech representations for speech signal spe-\ncific downstream tasks. RIFLE (Li et al., 2020) conducts multiple fine-tuning sessions sequentially,\nwhere the attached downstream specific layers are re-initialized in between every session. Unlike\nBiSSL, these techniques either do not incorporate knowledge of both the pretext task and down-\nstream task objectives and their relationship or do so only implicitly.\n3\nPROPOSED METHOD\n3.1\nNOTATION\nWe denote the unlabeled pretext dataset DP = {zk}CP\nk=1 and labeled downstream dataset DD =\n{xl, yl}CD\nk=1, respectively, where zk, xl ∈RN. Let fθ : RN →RM denotes a feature extracting\nbackbone with trainable parameters θ and pϕ : RM →RP a task specific projection head with\ntrainable parameters ϕ. Given pretext and downstream models gϕP ◦fθP and hϕD ◦fθD with\nθP , θD ∈RL, we denote the pretext and downstream training objectives LP (θP , ϕP ; DP ) and\nLD(θD, ϕD; DD), respectively. To simplify notation, we omit the dataset specification from the\ntraining objectives, e.g. LD(θD, ϕD) := LD(θD, ϕD; DD).\n3.2\nOPTIMIZATION PROBLEM FORMULATION\nThe conventional setup of self-supervised pre-training directly followed by supervised fine-tuning\nrelies on using a single backbone model with parameters θ.\nIn that instance, we minimize\n3\n\nPreprint. Under review.\nLP (θ, ϕP ) to produce a backbone parameter configuration θ∗which is then used as an initial-\nization when subsequently minimizing the downstream training objective LD(θ, ϕD). We deviate\nfrom this by instead considering θP and θD as two separate parameter vectors that are strongly cor-\nrelated. In continuation, we suggest combining the two traditionally separate optimization problems\nof pretext and downstream training into a joint optimization problem through bilevel optimization\ncalled BiSSL. We formulate BiSSL as\nmin\nθD,ϕD\nLD (θ∗\nP (θD) , ϕD) + γLD (θD, ϕD)\n(2)\ns.t.\nθ∗\nP (θD) ∈argmin\nθP\nmin\nϕP LP (θP , ϕP ) + λr(θD, θP )\n(3)\nwith γ ∈R+ and r being some convex regularisation objective weighted by λ ∈R+ enforcing\nsimilarity between θD and θP . The upper-level training objective in equation 2 is tasked with\nminimizing the downstream task objective LD, while the lower-level objective in equation 3 aims\nto minimize the pretext task objective LP while also ensuring its backbone remains similar to the\nupper-level backbone. As seen in the left term of equation 2, the backbone parameters θ∗\nP (θD)\nare transferred into the downstream training objective, mirroring how the backbone is transferred in\nthe conventional SSL pipeline. Although the second term of equation 2 is not strictly necessary, it\nhas empirically shown to improve stability and aid convergence of the upper-level solution during\ntraining. Unlike the traditional SSL setup, the backbone solution of the pretext objective θ∗\nP (θD)\nis now a function of the parameters of the downstream backbone θD, as the lower-level problem is\ndependent on the upper-level backbone parameters.\nAs the upper-level objective in equation 2 depends on the solution θ∗\nP (θD) of the lower-level ob-\njective in equation 3, this enables the incorporation of information from the pretext objective when\nsolving the upper-level optimization problem. By including a regularization objective r that enforces\nsimilarity between the lower-level and upper-level backbone parameters, this setup is hypothesized\nto guide the lower-level to achieve a configuration of model backbone parameters that is more ben-\neficial for subsequent conventional fine-tuning on the downstream task. To more precisely under-\nstand how the pretext objective influences the downstream training procedure in this setup, we delve\ndeeper into the expression of the gradient of the upper-level training objective in equation 2 in the\nfollowing subsection.\n3.3\nUPPER-LEVEL DERIVATIVE\nGiven the upper-level objective F(θD, ϕD) := LD(θ∗\nP (θD), ϕD)+γLD(θD, ϕD) from equation 2,\nits derivative with respect to θD is given by\ndF\ndθD\n= dθ∗\nP (θD)\ndθD\nT\n|\n{z\n}\nIG\n∇θLD(θ, ϕD)|θ=θ∗\nP (θD) + γ∇θLD(θ, ϕD)|θ=θD.\n(4)\nDue to the dependence of the lower-level solution on the upper-level parameters, the first term of\nequation 4 includes the implicit gradient (IG) of the implicit function θ∗\nP (θD). To simplify notation,\nwe let ∇ξh(ξ)|ξ=ψ := ∇ξh(ψ) when it is clear from context which variables are differentiated\nwith respect to. Following an approach similar to Rajeswaran et al. (2019), with details on the\nderivations and underlying assumptions outlined in Section A.1 of Appendix A, the IG in equation 4\ncan be explicitly expressed as\ndθ∗\nP (θD)\ndθD\nT\n= −∇2\nθDθP r(θD, θ∗\nP (θD))\n\u0014\n∇2\nθ\n\u0012 1\nλLP (θ∗\nP (θD), ϕP ) + r(θD, θ∗\nP (θD))\n\u0013\u0015−1\n.\n(5)\nA common convex regularization objective, which will also be the choice in the subsequent exper-\niments of this work, is r(ξ, ψ) = 1\n2∥ξ −ψ∥2\n2. Using this regularization objective simplifies equa-\ntion 5 down to\ndθ∗\nP (θD)\ndθD\nT\n=\n\u0014 1\nλ∇2\nθLP (θ∗\nP (θD), ϕP ) + IL\n\u0015−1\n,\n(6)\nwhere IL is the L × L-dimensional identity matrix. Hence the upper-level derivative in equation 4\ncan be expressed as\ndF\ndθD\n=\n\u0014 1\nλ∇2\nθLP (θ∗\nP (θD), ϕP ) + IL\n\u0015−1\n∇θLD(θ∗\nP (θD), ϕD) + γ∇θLD(θD, ϕD).\n(7)\n4\n\nPreprint. Under review.\nThe inverse Hessian-vector product in the left term of equation 7 is computationally infeasible to\ncalculate directly, hence it is approximated using the conjugate gradient (CG) method (Nazareth,\n2009; Shewchuk, 1994). While CG is established as a successful approach for approximating the\ninverse Hessian-vector products in previous works (Pedregosa, 2016; Zhang et al., 2021; Rajeswaran\net al., 2019), it still introduces significant computational overhead due to its need for iterative evalu-\nations of multiple Hessian vector products. Future work may explore alternative methods that offer\nmore efficient approximations without compromising downstream task performance. We employ a\nlayer-wise implementation of the CG method based on that of Rajeswaran et al. (2019) and refer to\ntheir work for more details on applying CG in a deep learning setup with BLO. For a comprehensive\noverview of other common methods used to approximate the upper-level derivative in BLO, we refer\nto Zhang et al. (2023a).\nWith an explicit expression of the IG in equation 6, we can interpret the impact of the scaling factor\nλ from equation 3 and equation 7: When λ is very large, the dependence of lower-level objective on\nthe upper-level parameters θD is also very large. This effectively drives the lower-level backbone\nparameters toward the trivial solution θ∗\nP (θD) = θD. Meanwhile, the IG in equation 6 approxi-\nmately equals IL, thereby diminishing the influence of the lower-level objective on the upper-level\ngradient in equation 7. This roughly makes the task of the upper-level equivalent to conventional\nfine-tuning. Conversely, if λ is very small, the lower-level objective in equation 3 effectively defaults\nto conventional pretext task training. Additionally, the implicit gradient in equation 6 would consist\nof numerically tiny entries, making the optimization of the first term in the upper-level objective in\nequation 2 equivalent to probing of the downstream head on the frozen pretext backbone θ∗\nP (θD).\n3.4\nTRAINING ALGORITHM AND PIPELINE\nAlgorithm 1 BiSSL Training Algorithm\n1: Input: Backbone and projection head parameter initializations θ, ϕP , ϕD. Training objectives\nLP , LD. Weights λ, γ ∈R+. Optimizers optP , optD. Number of training stage alternations\nT ∈N with upper and lower-level iterations NU, NL ∈N. Upper-level backbone adaption\nfrequency Na ∈N and strength α ∈[0, 1].\n2: Initialize θP ←θ and θD ←θ.\n3: for t = 1, . . . , T do\n4:\nfor n = 1, . . . , NL do\n▷Lower-level\n5:\nCompute gϕP = ∇ϕLP (θP , ϕ)|ϕ=ϕP .\n6:\nCompute gθP = ∇θLP (θ, ϕP )|θ=θP + λ∇θr(θD, θ)|θ=θP .\n7:\nUpdate ϕP ←optP (ϕP , gϕP ) and θP ←optP (θP , gθP ).\n8:\nif t mod Na ≡0 then\n9:\nθD ←(1 −α)θD + αθP .\n10:\nfor n = 1, . . . , NU do\n▷Upper-level\n11:\nCompute gϕD = ∇ϕLD(θP , ϕ)|ϕ=ϕD + γ∇ϕLD(θD, ϕ)|ϕ=ϕD.\n12:\nCompute v = ∇θLD(θ, ϕD)|θ=θP .\n13:\nApproximate vIG ≈\n\u0002\nIM + 1\nλ∇2\nθLP (θ, ϕP )|θ=θP\n\u0003−1v.\n▷Use CG\n14:\nCompute gθD = vIG + γ∇θLD(θ, ϕD)|θ=θD.\n15:\nUpdate ϕD ←optD(ϕD, gϕD) and θD ←optD(θD, gθD).\n16: Return: Backbone Parameters θP .\nAlgorithm 1 outlines the proposed training algorithm, which iteratively alternates between solv-\ning the lower-level (equation 3) and upper-level (equation 2) optimization problems in BiSSL. The\nlower-level training optimizes the pretext task objective, while additionally including the gradient of\nthe regularization term r for the backbone parameter updates, complying with equation 3. For the\nupper-level training, the gradient with respect to the backbone parameters as represented by the left\nterm on the right-hand side in equation 7, is approximated using the CG method. Additionally, the\n5\n\nPreprint. Under review.\npretext backbone parameters θP are weighted by α and added to the downstream backbone param-\neters θD every Na alternations to further enforce similarity between them, which empirically has\nshown to aid convergence during training.\nFrom Section A.1 in Appendix A, we get that θ∗\nP (θD) must fulfill the stationary condition\n∇θ\n\u0000LP (θ, ϕP ) + λr(θD, θ)\n\u0001\n|θ=θ∗\nP (θD) = 0 to justify the explicit expression of the implicit gra-\ndient in equation 6. This means that executing Algorithm 1 using random initializations of θ and\nϕP will likely not suffice. The same applies to ϕD, as a random initialization of ϕD typically leads\nto rapid initial changes of the backbone parameters θD during fine-tuning. This would then likely\nviolate the assumed stationary condition due to the dependence between θD and θP through the reg-\nularization objective r. Figure 1 illustrates the suggested pipeline alongside the conventional SSL\npipeline. First conventional pretext pre-training is performed on the unlabeled dataset DP to obtain\ninitializations of θ and ϕP . Next, the downstream head is fitted on top of the frozen backbone θ us-\ning the downstream dataset DD, which provides an initialization of the downstream head parameters\nϕD. Then, BiSSL training is conducted as outlined in Algorithm 1, yielding an updated configura-\ntion of backbone parameters θ∗\nP (θD). These updated backbone parameters are subsequently used\nas an initialization for the final supervised fine-tuning on the downstream task.\n4\nEXPERIMENTS AND RESULTS\n4.1\nDATASETS\nThe STL10 dataset (Coates et al., 2011) is used throughout the experiments. It comprises two par-\ntitions: 100.000 unlabeled images and 13.000 labeled images with 10 classes in total whereas 5000\nand 8000 are assigned for training and testing, respectively. All images are natural images of reso-\nlution 96 × 96, with the unlabeled partition drawn from a similar but broader distribution than the\nlabeled partition. This dataset strikes a balance between complexity and computational feasibility,\noffering higher resolution and more diverse content than smaller datasets like CIFAR10 (Krizhevsky,\n2012) while being less resource-intensive than larger-scale datasets such as ImageNet (Deng et al.,\n2009). For ease of reference, STL10U and STL10L will denote the unlabeled and labeled parti-\ntions, respectively. In all experiments, STL10U will be employed for self-supervised pre-training.\nFor downstream fine-tuning and evaluation, we leverage a varied set of natural image classification\ndatasets that encompass a wide array of tasks, including general image classification, fine-grained\nrecognition across species and objects, scene understanding, and texture categorization. The datasets\ninclude STL10L, Oxford 102 Flowers (Nilsback & Zisserman, 2008), StanfordCars (Yang et al.,\n2015), FGVC Aircraft (Maji et al., 2013), Describable Textures Dataset (DTD) (Cimpoi et al., 2014),\nOxford-IIIT Pets (Parkhi et al., 2012), FashionMNIST (Xiao et al., 2017), CIFAR10 (Krizhevsky,\n2012), CIFAR100 (Krizhevsky, 2012), Caltech-101 (Li et al., 2022a), Food 101 (Bossard et al.,\n2014), SUN397 scene dataset (Xiao et al., 2010), Caltech-UCSD Birds-200-2011 (CUB200) (Wah\net al., 2011) and PASCAL VOC 2007 (Everingham et al.). All downstream datasets are split into\ntraining, validation, and test partitions, with details on how these assignments are made provided in\nSection B.1 of Appendix B.\n4.2\nIMPLEMENTATION DETAILS\n4.2.1\nBASELINE SETUP\nPretext Task Training\nThe SimCLR (Chen et al., 2020b) pretext task with temperature τ = 0.5\nis used for pre-training a ResNet-18 (He et al., 2016) backbone model. We selected this widely\nadopted architecture due to its proven ability to extract high-quality visual representations while\nmaintaining relatively low computational requirements, striking an effective balance between per-\nformance and resource efficiency. On top of the backbone, a projection head is used, consisting of\ntwo fully connected layers with batch normalization (Ioffe & Szegedy, 2015) and ReLU (Agarap,\n2018) followed by a single linear layer. Each layer consists of 256 neurons.\nThe image augmentation scheme follows the approach used in Bardes et al. (2022), with minor\nmodifications: The image size is set to 96 × 96 instead of 224 × 224, and the minimal ratio of the\nrandom crop is adjusted accordingly to 0.5 instead of 0.08.\n6\n\nPreprint. Under review.\nThe implementation of the LARS optimizer (You et al., 2017) from Bardes et al. (2022) is employed,\nwith a “trust” coefficient of 0.001, a weight decay of 10−6 and a momentum of 0.9. The learning rate\nincreases linearly during the first 10 epochs, reaching a peak base learning rate of 4.8, followed by\na cosine decay towards 0 with no restarts (Loshchilov & Hutter, 2017) for the remaining epochs. A\nbatch size of 1024 is used and, unless otherwise specified, pre-training is conducted for 600 epochs.\nFine-Tuning on the Downstream Task\nFor downstream fine-tuning, a single linear layer is at-\ntached to the output of the pre-trained backbone. The training procedure utilizes the cross-entropy\nloss, the SGD optimizer with a momentum of 0.9, and a cosine decaying learning rate scheduler\nwithout restarts (Loshchilov & Hutter, 2017). Fine-tuning is conducted for 400 epochs with a batch\nsize of 256. An augmentation scheme similar to the fine-tuning augmentation scheme in Bardes\net al. (2022) is employed, where images are center cropped and resized to 96 × 96 pixels with a\nminimal crop ratio of 0.5, followed by random horizontal flips.\nA random grid search of 200 hyper-parameter configurations for the learning rates and weight decays\nis conducted, where one model is fine-tuned for each configuration. Base learning rates and weight\ndecays are log-uniformly sampled over the ranges of 0.0001 to 1.0 and 0.00001 to 0.01, respectively.\nValidation data accuracy is evaluated after each epoch. The hyper-parameter configuration yielding\nthe best balance between high validation accuracy and low validation loss is considered the optimal\nhyper-parameter configuration.2 The corresponding optimal hyper-parameters for each downstream\ndataset are documented in Table 2 of Appendix B.\nFor subsequent evaluation on the test data, we train 10 models with different random seeds, each\nusing the considered optimal hyper-parameter configurations. During the training of each respective\nmodel, the model parameters are stored after each epoch if the top-1 validation accuracy (or 11-\npoint mAP for the VOC07 dataset) has increased compared to the previous highest top-1 validation\naccuracy achieved during training. Top-1 and top-5 test data accuracies (or 11-point mAP for the\nVOC07 dataset) are evaluated for each of the 10 models, from which the calculated means and\nstandard deviations of these accuracies are documented.\n4.2.2\nBISSL SETUP\nIn this section, we detail each stage of the proposed training pipeline for BiSSL, as outlined in the\nright part of Figure 1.\nPretext Warm-up\nThe backbone θ and projection head ϕP are initialized by self-supervised pre-\ntraining using a setup almost identical to the baseline pretext task training setup in Section 4.2.1.\nThe only difference is that this training stage is conducted for 500 epochs instead of 600 epochs,\nand that the peak base learning rate is set to 1.0 instead of 4.8. This adjustment is made because the\nBiSSL training stage will conduct what is roughly equivalent to 100 pretext epochs, as detailed more\nspecifically in the composite configuration paragraph below. This ensures that the total number of\npretext pre-training steps is comparable to those conducted in the baseline setup.\nDownstream Head Warm-up\nThe training setup for the downstream head warm-up closely mir-\nrors the fine-tuning setup of Section 4.2.1. The main difference is that only the linear downstream\nhead is fitted on top of the now frozen backbone obtained from the pretext warm-up. Learning rates\nand weight decays are initially selected based on those listed in Table 2, with adjustments made\nas needed when preliminary testing indicated a potential for improved convergence. These values\nare provided in Table 3 in Appendix B. The authors recognize that more optimal hyper-parameter\nconfigurations may exist and leave further exploration of this for future refinement. The downstream\nhead warm-up is conducted for 20 epochs with a constant learning rate.\nLower-level of BiSSL\nThe training configuration for the lower-level primarily follows the setup\ndescribed for pretext pre-training in Section 4.2.1, with the modifications outlined here. As specified\nin equation 3, the lower-level loss function is the sum of the pretext task objective LP (in our case,\nthe NT-Xent loss from SimCLR (Chen et al., 2020b)) and the regularization term r(θD, θP ) =\n2In certain scenarios during the experiments, the configuration that achieved the highest validation accuracy\nalso yielded a notably higher relative validation loss. To ensure better generalizability, an alternative configu-\nration with a more favorable trade-off was selected in these cases.\n7\n\nPreprint. Under review.\nTable 1: Test classification accuracies. Accuracies significantly different from their counterparts are\nmarked in bold font.\nDataset\nTop-1 Accuracy (*: 11-point mAP)\nTop-5 Accuracy\nBiSSL\nOnly FT\nAvg Diff\nBiSSL\nOnly FT\nAvg Diff\nSTL10L\n90.2 ± 0.1\n90.3 ± 0.1\n−0.1\n99.7 ± 0.0\n99.6 ± 0.0\n+0.1\nFlowers\n74.8 ± 0.2\n73.4 ± 0.4\n+1.4\n89.8 ± 0.3\n90.0 ± 0.4\n−0.2\nCars\n73.0 ± 0.4\n72.7 ± 0.5\n+0.3\n91.5 ± 0.3\n91.4 ± 0.4\n+0.1\nAircrafts\n46.9 ± 0.5\n46.1 ± 0.9\n+0.8\n78.9 ± 0.4\n79.3 ± 0.6\n−0.4\nDTD\n51.8 ± 0.5\n49.3 ± 0.5\n+2.5\n79.9 ± 0.3\n79.1 ± 0.4\n+0.8\nPets\n67.8 ± 0.2\n65.0 ± 0.5\n+2.8\n92.3 ± 0.3\n90.7 ± 0.3\n+1.6\nFMNIST\n94.3 ± 0.2\n94.1 ± 0.1\n+0.2\n100.0 ± 0.0\n100.0 ± 0.0\n0.0\nCIFAR10\n93.9 ± 0.1\n93.8 ± 0.1\n+0.1\n99.9 ± 0.0\n99.8 ± 0.0\n+0.1\nCIFAR100\n73.0 ± 0.1\n73.2 ± 0.2\n−0.2\n93.7 ± 0.1\n92.8 ± 0.1\n+0.9\nCaltech-101\n80.6 ± 0.7\n78.1 ± 0.5\n+2.5\n95.5 ± 0.2\n94.7 ± 0.2\n+0.8\nFood\n72.0 ± 0.2\n71.7 ± 0.2\n+0.3\n90.4 ± 0.1\n90.4 ± 0.1\n0.0\nSUN397\n41.1 ± 0.2\n40.0 ± 0.3\n+1.1\n71.0 ± 0.2\n69.9 ± 0.4\n+1.1\nCUB200\n47.1 ± 0.4\n45.7 ± 0.4\n+1.4\n72.1 ± 0.3\n70.7 ± 0.6\n+1.4\nVOC07\n∗60.4 ± 0.1\n∗58.6 ± 0.3\n+1.8\n−\n−\n−\n1\n2||θD −θP ||2\n2. Based on early experiments, the regularization weight λ = 0.001 was selected,\nas it appeared to strike a well-balanced compromise between the convergence rates of both the\nlower- and upper-level objectives. The lower-level is trained for the equivalent of approximately\n100 conventional pre-training epochs, with further details provided in the composite configuration\nparagraph. Each time the BiSSL training alternates back to the lower-level, the first 5 batches used\nfor lower-level training are stored. These stored batches are utilized to approximate the Hessian of\nthe lower-level objective when approximating the upper-level gradient. Further details are specified\nin Section B.3 of Appendix B and the paragraph below.\nUpper-level of BiSSL\nThe upper-level training stage also shares many similarities with the down-\nstream training setup described in Section 4.2.1, and again, only the differences are addressed here.\nThe weight decays and base learning rates are set to match those obtained from the downstream\nhead warm-up detailed in Table 3 of Appendix B. The weighting of the conventional downstream\nloss objective is set to γ = 0.01. To approximate the upper-level gradient in equation 7, the conju-\ngate gradient method (Nazareth, 2009; Shewchuk, 1994) is employed. Further details regarding the\nsetup for the upper-level gradient approximation are covered in Section B.3 of Appendix B.\nComposite Configuration Details of BiSSL\nAs outlined in Algorithm 1, both lower- and upper-\nlevel backbone parameters θP and θD are initialized with the backbone parameters obtained during\nthe pretext warm-up, and the training procedure alternates between solving the lower- and upper-\nlevel optimization problems. In this experimental setup, the lower-level performs NL = 20 gradient\nsteps before alternating to the upper-level, which then conducts NU = 8 gradient steps. A total of\nT = 500 training stage alternations are executed. As the STL10U dataset with the current batch size\nof 1024 amounts to a total of 98 training batches without replacement, these T = 500 training stage\nalternations roughly equal 100 conventional pretext epochs. Section B.4 in Appendix B outlines\nfurther details on how data batches are handled during training. The upper-level backbone adaptation\nfrequency and strength are set to Na = 100 and α = 0.1, respectively. Additionally, gradient\nnormalization is employed on gradients exceeding ℓ2-norms of 10.\nFine-Tuning on the Downstream Task\nSubsequent downstream fine-tuning is conducted in a\nmanner identical to that described in the ‘Fine-Tuning on the Downstream Task” paragraph of sec-\ntion 4.2.1. Table 4 in Appendix B lists the considered optimal hyper-parameter configurations for\neach dataset.\n8\n\nPreprint. Under review.\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTotal Number of Pretext Epochs\n70\n71\n72\n73\n74\n75\n76\n77\n%\nTop 1 Accuracy\nBiSSL\nOnly FT\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTotal Number of Pretext Epochs\n86\n87\n88\n89\n90\n91\n%\nTop 5 Accuracy\nBiSSL\nOnly FT\nTest Classification Accuracies\nFigure 2: Test classification accuracies on the flowers dataset over varying numbers of pretext\nepochs.\n4.3\nDOWNSTREAM TASK PERFORMANCE\nThe impact of using BiSSL compared to the conventional self-supervised training pipeline is bench-\nmarked by evaluating classification accuracies on the various specified downstream datasets. Ta-\nble 1 presents the means and standard deviations of top-1 and top-5 classification accuracies (or\nthe 11-point mAP on the VOC2007 dataset) on these downstream test datasets, comparing results\nobtained from the conventional SSL pipeline with those achieved using the BiSSL pipeline. The re-\nsults demonstrate that training with BiSSL significantly improves either top-1 or top-5 classification\naccuracy, or 11-point mAP in the case of VOC07, on 10 out of 14 datasets, with no single result\nshowing a significant decline in performance compared to the baseline.\n4.3.1\nPERFORMANCE OVER VARYING PRE-TRAINING EPOCHS\nTable 1 demonstrates that BiSSL can significantly enhance classification accuracies across various\ndatasets. To determine whether this improvement persists across different levels of pretext pre-\ntraining, additional experiments are conducted using varying numbers of total pre-training epochs.\nThe accuracies are compared with the total number of pretext pre-training epochs kept roughly the\nsame, so the pretext warm-up in the BiSSL pipeline is conducted for 100 epochs less than the base-\nline pretext pre-training, as justified in the “Composite Configuration Details of BiSSL” paragraph\nof Section 4.2.2. The flowers dataset, which showed substantial benefits from BiSSL on the top-1\nclassification accuracies, is selected for these experiments. The results presented in Figure 2 re-\nveal that BiSSL generally sustains the relative performance gap over the baseline, regardless of the\npre-training duration. This suggests that the advantages conferred by BiSSL are not contingent on\nthe amount of pre-training. Rather, this indicates that BiSSL may provide a more efficient learning\ntrajectory, stemming from the enhanced information sharing it facilitates between the pretext and\ndownstream tasks.\n4.4\nVISUAL INSPECTION OF LATENT FEATURES\nTo gain deeper insight into how BiSSL affects the representations learned compared to conventional\npretext pre-training, we perform a qualitative visual inspection of latent spaces. This involves com-\nparing features processed by backbones trained solely by pretext pre-training to those derived from\nlower-level backbones obtained after conducting BiSSL, each trained as described in the “Pretext\nTask Training” and “Lower-level of BiSSL” paragraphs in Section 4.2.1 and 4.2.2, respectively. By\ncomparing these features, we aim to assess whether BiSSL nudges the latent features toward be-\ning more semantically meaningful for the downstream task. The t-Distributed Stochastic Neighbor\nEmbedding (t-SNE) (Cieslak et al., 2020) technique is employed for dimensionality reduction. Fur-\nther details regarding the experimental setup are outlined in Section C.1 of Appendix C. Figure 3\n9\n\nPreprint. Under review.\nBiSSL Backbone Features\nPretext Backbone Features\nFigure 3: Visualization of features from backbones trained using pretext pre-training exclusively\nand backbones derived from lower-level backbones obtained after applying BiSSL, respectively.\nFeatures are extracted from the test partition of the flowers dataset. Each color represents a different\nclass. Details are outlined in Section C.1 of Appendix C\nillustrates the results on the flowers dataset, indicating that BiSSL yields backbones with improved\ndownstream feature alignment. Further plots on a selection of downstream datasets in Section C.1\nreinforce this finding, also demonstrating that this trend persists even for datasets where BiSSL did\nnot impose any classification accuracy improvements.\n5\nCONCLUSION\nThis study integrates pretext pre-training and downstream fine-tuning into a unified bilevel optimiza-\ntion problem, from which the BiSSL training framework is proposed. BiSSL explicitly models the\ninheritance of backbone parameters from the pretext task, enhancing the transfer of relevant infor-\nmation between the pretext and downstream tasks. We propose a practical training algorithm and\npipeline that incorporates BiSSL as an intermediate stage between pretext pre-training and down-\nstream fine-tuning. Experiments across various image classification datasets demonstrate that BiSSL\nconsistently achieves improved or comparable downstream classification performance relative to the\nconventional self-supervised learning pipeline. Additionally, our findings indicate that in instances\nwhere BiSSL improves performance, this improvement remains consistent regardless of the pre-\ntext pre-training duration. Further analysis suggests that BiSSL enhances the downstream semantic\nrichness of learned representations, as evidenced by qualitative inspections of latent spaces. BiSSL\nmarks a potential advancement towards enhancing the alignment between the pretext pre-training\nand downstream fine-tuning stages, revealing a new direction for self-supervised learning algorithm\ndesigns that leverage bilevel optimization.\n5.1\nFUTURE WORK\nFormulating the self-supervised pipeline as a bilevel optimization problem offers various strategies\nwith trade-offs in computational complexity and theoretical justification. While this study presents\na promising approach for improving downstream performance, further investigation of alternative\nformulations is needed to identify setups are are potentially more optimal. Although BiSSL is theo-\nretically applicable to any downstream task and model size, our experiments focused on small-scale\nimage classification due to resource constraints. Therefore, it remains uncertain whether BiSSL\ncan scale to larger setups and tasks. Additionally, a potential future advancement would integrating\nmore novel methods for solving BLO problems, which promise benefits in terms of reduced com-\nputational costs and improved solution convergence (Zhang et al., 2023a; Yang et al., 2021; Choe\net al., 2023; Huang, 2024). Lastly, the current BiSSL framework relies on full access to pre-training\ndata and pretext tasks. Future research could investigate the use of only a subset of pre-training data\nand alternative pretext tasks to maintain BiSSL’s benefits under these conditions.\n10\n\nPreprint. Under review.\nACKNOWLEDGMENTS\nThis project is supported by the Pioneer Centre for Artificial Intelligence, Denmark.3 The authors\nwould like to thank Sijia Liu and Yihua Zhang (Michigan State University) providing valuable feed-\nback in a discussion, which helped to refine and solidify our perspective on the topic of integrating\nbilevel optimization in deep learning.\nREFERENCES\nAbien Fred Agarap.\nDeep learning using rectified linear units (relu).\narXiv preprint\narXiv:1803.08375, 2018.\nMart´ın Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.\nArXiv, abs/1907.02893, 2019.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A frame-\nwork for self-supervised learning of speech representations.\nAdvances in neural information\nprocessing systems, 33:12449–12460, 2020.\nAdrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regular-\nization for self-supervised learning. In International Conference on Learning Representations,\n2022.\nHarshita Boonlia, Tanmoy Dam, Md Meftahul Ferdaus, Sreenatha G Anavatti, and Ankan Mullick.\nImproving self-supervised learning for out-of-distribution task via auxiliary classifier. In 2022\nIEEE International Conference on Image Processing (ICIP), pp. 3036–3040. IEEE, 2022.\nZal´an Borsos, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for continual\nlearning and streaming. Advances in neural information processing systems, 33:14879–14890,\n2020.\nMatteo Boschini, Lorenzo Bonicelli, Angelo Porrello, Giovanni Bellitto, Matteo Pennisi, Simone\nPalazzo, Concetto Spampinato, and Simone Calderara. Transfer without forgetting. In European\nConference on Computer Vision, pp. 692–709. Springer, 2022.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative com-\nponents with random forests. In European Conference on Computer Vision, 2014.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei.\nLanguage models are few-shot learners.\nIn H. Larochelle, M. Ranzato,\nR. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,\nvolume 33, pp. 1877–1901. Curran Associates, Inc., 2020.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments. Advances in neural\ninformation processing systems, 33:9912–9924, 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv´e Jegou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin.\nEmerging properties in self-supervised vision transformers.\nIn 2021\nIEEE/CVF International Conference on Computer Vision (ICCV), pp. 9630–9640, 2021. doi:\n10.1109/ICCV48922.2021.00951.\nHeng-Jui Chang, Alexander H. Liu, and James Glass. Self-supervised Fine-tuning for Improved\nContent Representations by Speaker-invariant Clustering. In INTERSPEECH 2023, pp. 2983–\n2987. ISCA, August 2023. doi: 10.21437/Interspeech.2023-847.\n3https:\\www.aicentre.dk\n11\n\nPreprint. Under review.\nCan (Sam) Chen, Jingbo Zhou, Fan Wang, Xue Liu, and Dejing Dou. Structure-aware protein self-\nsupervised learning. Bioinformatics, 39(4):btad189, 04 2023. ISSN 1367-4811. doi: 10.1093/\nbioinformatics/btad189.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. Recall and\nlearn: Fine-tuning deep pretrained language models with less forgetting.\nIn Bonnie Webber,\nTrevor Cohn, Yulan He, and Yang Liu (eds.), EMNLP (1), pp. 7870–7881. Association for Com-\nputational Linguistics, 2020a. ISBN 978-1-952148-60-6.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning,\npp. 1597–1607. PMLR, 2020b.\nSang Keun Choe, Willie Neiswanger, Pengtao Xie, and Eric Xing. Betty: An automatic differenti-\nation library for multilevel optimization. In The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.net/forum?id=LV_MeMS38Q9.\nYu-An Chung and James Glass. Speech2Vec: A Sequence-to-Sequence Framework for Learning\nWord Embeddings from Speech. In Proc. Interspeech 2018, pp. 811–815, 2018. doi: 10.21437/\nInterspeech.2018-2341.\nYu-An Chung, Wei-Ning Hsu, Hao Tang, and James Glass. An Unsupervised Autoregressive Model\nfor Speech Representation Learning. In Proc. Interspeech 2019, pp. 146–150, 2019. doi: 10.\n21437/Interspeech.2019-1473.\nMatthew C. Cieslak, Ann M. Castelfranco, Vittoria Roncalli, Petra H. Lenz, and Daniel K. Hartline.\nt-distributed stochastic neighbor embedding (t-sne): A tool for eco-physiological transcriptomic\nanalysis. Marine Genomics, 51:100723, 2020. ISSN 1874-7787. doi: https://doi.org/10.1016/j.\nmargen.2019.100723.\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In\nProceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014.\nAdam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised\nfeature learning. In Geoffrey Gordon, David Dunson, and Miroslav Dud´ık (eds.), Proceedings\nof the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of\nProceedings of Machine Learning Research, pp. 215–223, Fort Lauderdale, FL, USA, 11–13 Apr\n2011. PMLR.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\npp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In North American Chapter of the Associ-\nation for Computational Linguistics, 2019.\nJeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Dar-\nrell. Decaf: A deep convolutional activation feature for generic visual recognition. In Eric P. Xing\nand Tony Jebara (eds.), Proceedings of the 31st International Conference on Machine Learning,\nvolume 32 of Proceedings of Machine Learning Research, pp. 647–655, Bejing, China, 22–24\nJun 2014. PMLR.\nA.L. Dontchev and R.T. Rockafellar.\nImplicit Functions and Solution Mappings: A View from\nVariational Analysis. Springer Series in Operations Research and Financial Engineering. Springer\nNew York, 2014. ISBN 9781493910373.\nYann Dubois, Tatsunori Hashimoto, Stefano Ermon, and Percy Liang. Improving self-supervised\nlearning by characterizing idealized representations. ArXiv, abs/2209.06235, 2022.\nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.\nThe\nPASCAL Visual Object Classes Challenge 2007 (VOC2007) Results.\nhttp://www.pascal-\nnetwork.org/challenges/VOC/voc2007/workshop/index.html.\n12\n\nPreprint. Under review.\nChen Fan, Parikshit Ram, and Sijia Liu. Sign-MAML: Efficient model-agnostic meta-learning by\nsignSGD. In Fifth Workshop on Meta-Learning at the Conference on Neural Information Pro-\ncessing Systems, 2021.\nChelsea Finn, P. Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. In International Conference on Machine Learning, 2017.\nLuca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel\nprogramming for hyperparameter optimization and meta-learning. In International conference on\nmachine learning, pp. 1568–1577. PMLR, 2018.\nJean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\net al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural\ninformation processing systems, 33:21271–21284, 2020.\nKartik Gupta, Thalaiyasingam Ajanthan, Anton van den Hengel, and Stephen Gould. Understanding\nand improving the role of projection head in self-supervised learning. ArXiv, abs/2212.11491,\n2022.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\nMomentum contrast for\nunsupervised visual representation learning.\nIn Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 9729–9738, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick. Masked au-\ntoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 16000–16009, 2022.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\nwith disentangled attention. In International Conference on Learning Representations, 2021.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked\nprediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n29:3451–3460, 2021.\nFeihu Huang.\nOptimal Hessian/Jacobian-free nonconvex-PL bilevel optimization.\nIn Ruslan\nSalakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and\nFelix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learn-\ning, volume 235 of Proceedings of Machine Learning Research, pp. 19598–19621. PMLR, 21–27\nJul 2024.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning, pp. 448–456.\npmlr, 2015.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll´ar, and Ross B. Girshick.\nSegment anything. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp.\n3992–4003, 2023.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 05\n2012.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdel rahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\nBart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation, and comprehension. In Annual Meeting\nof the Association for Computational Linguistics, 2019.\n13\n\nPreprint. Under review.\nFei-Fei Li, Marco Andreeto, Marc’Aurelio Ranzato, and Pietro Perona. Caltech 101, Apr 2022a.\nJiangmeng Li, Wenwen Qiang, Yanan Zhang, Wenyi Mo, Changwen Zheng, Bing Su, and Hui\nXiong. Metamask: Revisiting dimensional confounder for self-supervised learning. Advances in\nNeural Information Processing Systems, 35:38501–38515, 2022b.\nXingjian Li, Haoyi Xiong, Haozhe An, Cheng-Zhong Xu, and Dejing Dou. Rifle: Backpropagation\nin depth for deep transfer learning through re-initializing the fully-connected layer. In Interna-\ntional Conference on Machine Learning, pp. 6010–6019. PMLR, 2020.\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In Inter-\nnational Conference on Learning Representations, 2017.\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of\naircraft. Technical report, 2013.\nJ. L. Nazareth. Conjugate gradient method. WIREs Computational Statistics, 1(3):348–353, 2009.\ndoi: https://doi.org/10.1002/wics.13.\nDaisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, and Kunio Kashino. Byol for\naudio: Self-supervised learning for general-purpose audio representation. In 2021 International\nJoint Conference on Neural Networks (IJCNN), pp. 1–8. IEEE, 2021.\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number\nof classes. In Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.\nMaxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khali-\ndov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran,\nNicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra,\nMichael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick\nLabatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features with-\nout supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL\nhttps://openreview.net/forum?id=a68SUt6zFt.\nHaolin Pan, Yong Guo, Qinyi Deng, Hao-Fan Yang, Yiqun Chen, and Jian Chen. Improving fine-\ntuning of self-supervised models with contrastive initialization. Neural networks : the official\njournal of the International Neural Network Society, 159:198–207, 2022.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE\nConference on Computer Vision and Pattern Recognition, 2012.\nFabian Pedregosa. Hyperparameter optimization with approximate gradient. In Maria Florina Bal-\ncan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Ma-\nchine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 737–746, New\nYork, New York, USA, 20–22 Jun 2016. PMLR.\nAravind Rajeswaran, Chelsea Finn, Sham M. Kakade, and Sergey Levine. Meta-learning with im-\nplicit gradients. In Neural Information Processing Systems, 2019.\nSteffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised\nPre-Training for Speech Recognition. In Proc. Interspeech 2019, pp. 3465–3469, 2019. doi:\n10.21437/Interspeech.2019-1873.\nJonathan R Shewchuk. An introduction to the conjugate gradient method without the agonizing\npain. Technical report, Carnegie Mellon University, 1994.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\nSai Ashish Somayajula, Lifeng Jin, Linfeng Song, Haitao Mi, and Dong Yu. Bi-level finetuning\nwith task-dependent similarity structure for low-resource training. In Anna Rogers, Jordan Boyd-\nGraber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics:\nACL 2023, pp. 8569–8588, Toronto, Canada, July 2023. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2023.findings-acl.544.\n14\n\nPreprint. Under review.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nC. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011\ndataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.\nChuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, and Xing Xie. Noisytune: A little noise can\nhelp you finetune pretrained language models better. In Annual Meeting of the Association for\nComputational Linguistics, 2022.\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-\ning machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\nJianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\nLarge-scale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition, pp. 3485–3492, 2010. doi: 10.1109/CVPR.2010.\n5539970.\nSarthak Yadav, Sergios Theodoridis, Lars Kai Hansen, and Zheng-Hua Tan. Masked autoencoders\nwith multi-window local-global attention are better audio learners. In The Twelfth International\nConference on Learning Representations, 2024.\nJunjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimization. In\nA. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural In-\nformation Processing Systems, 2021.\nLinjie Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang. A large-scale car dataset for fine-\ngrained categorization and verification. In 2015 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 3973–3981, 2015. doi: 10.1109/CVPR.2015.7299023.\nYang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv:\nComputer Vision and Pattern Recognition, 2017.\nSalah Zaiem, Titouan Parcollet, and Slim Essid. Less forgetting for better generalization: Exploring\ncontinual-learning fine-tuning methods for speech self-supervised representations. arXiv preprint\narXiv:2407.00756, 2024.\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l: Self-supervised semi-\nsupervised learning. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp.\n1476–1485, 2019.\nLi Zhang, Youwei Liang, Ruiyi Zhang, Amirhosein Javadi, and Pengtao Xie. BLO-SAM: Bi-level\noptimization based finetuning of the segment anything model for overfitting-preventing semantic\nsegmentation.\nIn Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria\nOliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International\nConference on Machine Learning, volume 235 of Proceedings of Machine Learning Research,\npp. 59289–59309. PMLR, 21–27 Jul 2024.\nTong Zhang, Congpei Qiu, Wei Ke, Sabine S¨usstrunk, and Mathieu Salzmann.\nLeverage your\nlocal and global representations: A new self-supervised learning strategy. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16580–16589, 2022a.\nYihua Zhang, Guanhua Zhang, Prashant Khanduri, Min-Fong Hong, Shiyu Chang, and Sijia Liu.\nRevisiting and advancing fast adversarial training through the lens of bi-level optimization. In\nInternational Conference on Machine Learning, 2021.\nYihua Zhang, Yuguang Yao, Parikshit Ram, Pu Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang,\nand Sijia Liu. Advancing model pruning via bi-level optimization. In S. Koyejo, S. Mohamed,\nA. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing\nSystems, volume 35, pp. 18309–18326. Curran Associates, Inc., 2022b.\n15\n\nPreprint. Under review.\nYihua Zhang, Prashant Khanduri, Ioannis C. Tsaknakis, Yuguang Yao, Min-Fong Hong, and Sijia\nLiu. An introduction to bi-level optimization: Foundations and applications in signal processing\nand machine learning. ArXiv, abs/2308.00788, 2023a.\nYihua Zhang, Pranay Sharma, Parikshit Ram, Mingyi Hong, Kush R. Varshney, and Sijia Liu. What\nis missing in IRM training and evaluation? challenges and solutions. In The Eleventh Interna-\ntional Conference on Learning Representations, 2023b.\nNicolas Zucchet and Jo˜ao Sacramento. Beyond backpropagation: bilevel optimization through im-\nplicit differentiation and equilibrium propagation. Neural Computation, 34(12):2309–2346, 2022.\nA\nTHEORETICAL INSIGHTS AND FRAMEWORK COMPARISONS IN BISSL\nA.1\nDERIVATION OF THE IMPLICIT GRADIENT\nAssume the setup of the BiSSL optimization problem described in equation 2 and equation 3. In\nthe following derivations, we will assume that ϕP is fixed, allowing us to simplify the expressions\ninvolved. To streamline the notation further, we continue to use the convention ∇ξh(ξ)|ξ=ψ :=\n∇ξh(ψ), when it is clear from context which variables are differentiated with respect to. Under\nthese circumstances, we then define the lower-level objective from equation 3 as\nG(θD, θP ) := LP (θP , ϕP ) + λr(θD, θP ).\n(8)\nRecalling that r is a convex regularization objective, adequate scaling of λ effectively “convexi-\nfies” the lower-level objective G, a strategy also employed on the lower-level objective in previous\nworks (Rajeswaran et al., 2019; Zhang et al., 2022b; 2023a). This is advantageous because assuming\nconvexity of G ensures that for any θD ∈RL, there exists a corresponding ˆθP ∈RL that satisfies\nthe stationary condition ∇θP G(θD, ˆθP ) = 0. In other words, we are assured that a minimizer of\nG(θD, ·) exists for all θD ∈RL. Now, further assume that ∇θP G(θD, θP ) is continuously differen-\ntiable and that the Hessian matrix ∇2\nθP G(θD, ˆθP ) is invertible for all θD ∈RL. Under these condi-\ntions, the implicit function theorem (Dontchev & Rockafellar, 2014; Zucchet & Sacramento, 2022)\nguarantees the existence of an implicit unique and differentiable function θ∗\nP : N(θD) →RL, with\nN(θD) being a neighborhood of θD, that satisfies θ∗\nP (θD) = ˆθP and ∇θP G(˜θD, θ∗\nP (˜θD)) = 0 for\nall ˜θD ∈N(θD).\nAs the lower-level solution θ∗\nP (θD) is indeed a differentiable function under these conditions, this\njustifies that the expression\nd\ndθD\n∇θP [G(θD, θ∗\nP (θD))] = 0\nis valid for all θD ∈RL. By applying the chain rule, the expression becomes\n∇2\nθDθP G(θD, θ∗\nP (θD)) + dθ∗\nP (θD)\ndθD\nT\n∇2\nθP G(θD, θ∗\nP (θD)) = 0.\nRecalling that ∇2\nθP G(θD, θ∗\nP (θD)) is assumed to be invertible, the IG dθ∗\nP (θD)\ndθD\nT can be isolated\ndθ∗\nP (θD)\ndθD\nT\n= −∇2\nθDθP G(θD, θ∗\nP (θD))\n\u0002\n∇2\nθP G(θD, θ∗\nP (θD))\n\u0003−1,\nand by substituting the expression for G from equation 8, the expression becomes\ndθ∗\nP (θD)\ndθD\nT\n= −∇2\nθDθP r(θD, θ∗\nP (θD))\n\u0014\n∇2\nθP\n\u0012 1\nλLP (θ∗\nP (θD), ϕP ) + r(θD, θ∗\nP (θD))\n\u0013\u0015−1\n.\n(9)\nTo summarize, given the following assumptions:\n• The lower-level pretext projection head parameters ϕP are fixed.\n• G is convex such that ∇θP G(θD, θ∗\nP (θD)) = 0 is fulfilled for every θD ∈RL.\n16\n\nPreprint. Under review.\n• The Hessian matrix ∇2\nθP G(θD, θ∗\nP (θD)) exists and is invertible for all θD ∈RL.\nThen, the IG dθ∗\nP (θD)\ndθD\nT can be explicitly expressed by equation 9. The authors acknowledge that\nan explicit expression for the IG without fixing ϕP is achievable, though this is left for future\nexploration.\nA.2\nDISTINCTION FROM BILEVEL OPTIMIZATION IN META-LEARNING\nWhile bilevel optimization (BLO) has been applied in meta-learning frameworks such as\nMAML (Finn et al., 2017), Sign-MAML (Fan et al., 2021) and iMAML (Rajeswaran et al., 2019),\nBiSSL represents a distinct application and implementation of BLO, tailored for the challenges of\nself-supervised learning (SSL). In the aforementioned works, BLO is primarily utilized to address\nfew-shot learning scenarios, focusing on efficiently adapting models to new tasks with minimal data.\nConversely, BiSSL applies BLO to concurrently manage the more complex task of self-supervised\npretext pre-training with downstream fine-tuning. Another key distinction is that in meta-learning,\nthe upper- and lower-level objectives are closely related, with the upper-level objective formulated\nas a summation of the lower-level tasks. In contrast, BiSSL involves fundamentally distinct ob-\njectives at each level, utilizing separate datasets and tasks for pre-training and fine-tuning. This\ndesign allows BiSSL to better align the pre-trained model with the requirements of a specific down-\nstream task. Conversely, the BLO in meta-learning aims to broadly generalize across a wide range of\ntasks, prioritizing adaptability rather than task-specific optimization. Additionally, unlike BiSSL, the\nmeta-learning frameworks discussed reinitialize the lower-level backbone parameters with a copy of\nthe upper-level parameters at every iteration. In BiSSL, the closest comparable mechanism is the\noccasional update of the upper-level backbone using an EMA update with the lower-level parame-\nters (see Algorithm 1), though this occurs far less frequently. Lastly, while the meta-learning with\nBLO approaches often target smaller models and datasets, BiSSL is aimed for larger-scale setups\ncharacteristic of modern SSL workflows.\nB\nEXPERIMENTAL DETAILS\nB.1\nDATASET PARTITIONS\nThe Caltech-101 (Li et al., 2022a) dataset does not come with a pre-defined train/test split, so the\nsame convention as previous works is followed (Chen et al., 2020b; Donahue et al., 2014; Simonyan\n& Zisserman, 2014), where 30 random images per class are selected for the training partition, and\nthe remaining images are assigned for the test partition. For the DTD (Cimpoi et al., 2014) and\nSUN397 (Xiao et al., 2010) datasets, which offer multiple proposed train/test partitions, the first\nsplits are used, consistent with the approach in Chen et al. (2020b).\nFor downstream hyperparameter optimization, portions of the training partitions from each respec-\ntive labeled dataset are designated as validation datasets. The FGVC Aircraft (Maji et al., 2013),\nOxford 102 Flowers (Nilsback & Zisserman, 2008), DTD, and Pascal VOC 2007 (Everingham et al.)\ndatasets already have designated validation partitions. For all the remaining labeled datasets, the val-\nidation data partitions are randomly sampled while ensuring that class proportions are maintained.\nFor the multi-attribute VOC07 dataset, sampling is performed with class balance concerning the first\nattribute present in each image. Roughly 20% of the training data is allocated for validation.\nB.2\nDOWNSTREAM TASK FINE-TUNING OF THE BASELINE SETUP\nIn Table 2, the learning rates and weight decays used for each respective downstream dataset of the\nexperiments described in Section 4.2.1 are outlined.\nB.3\nDOWNSTREAM HEAD WARMUP AND UPPER-LEVEL OF BISSL\nTable 3 outlines the learning rates and weight decays used for the downstream head warm-up and\nupper-level of BiSSL of each respective downstream dataset, as described in the BiSSL experimental\nsetup of Section 4.2.2.\n17\n\nPreprint. Under review.\nTable 2: Hyper-parameter configurations used for downstream fine-tuning after conventional pretext\npre-training yielding the highest top-1 classification accuracies (11-point mAP for the VOC2007\ndataset).\nDataset\nLearning Rate\nWeight Decay\nSTL10L\n0.0136\n0.001\nFlowers\n0.113\n0.00226\nCars\n0.035\n0.00658\nAircrafts\n0.0167\n0.00996\nDTD\n0.0262\n0.00332\nPets\n0.0235\n0.00472\nFashionMNIST\n0.0009\n0.00829\nCIFAR10\n0.0067\n0.00128\nCIFAR100\n0.005\n0.00127\nCaltech-101\n0.0096\n0.00902\nFood\n0.015\n0.00699\nSUN397\n0.0097\n0.00121\nCUB200\n0.0722\n0.00568\nVOC2007\n0.0108\n0.00894\nTable 3: Hyper-parameters used for the Downstream Head Warm-up and Upper-level of BiSSL.\nDataset\nLearning Rate\nWeight Decay\nSTL10L\n0.015\n0.01\nFlowers\n0.05\n0.01\nCars\n0.035\n0.007\nAircrafts\n0.015\n0.01\nDTD\n0.015\n0.0075\nPets\n0.03\n0.005\nFashionMNIST\n0.05\n0.004\nCIFAR10\n0.03\n0.006\nCIFAR100\n0.03\n0.001\nCaltech-101\n0.03\n0.007\nFood\n0.015\n0.01\nSUN397\n0.03\n0.002\nCUB200\n0.05\n0.005\nVOC2007\n0.03\n0.006\nThe first term of the upper-level gradient equation 7 is approximated using the Conjugate Gradi-\nent (CG) method (Nazareth, 2009; Shewchuk, 1994). Our implementation follows a similar struc-\nture to that used in Rajeswaran et al. (2019), employing Nc = 5 iterations and a dampening term\nλdamp = 10. Given matrix A and vector v, the CG method iteratively approximates A−1v, which re-\nquires evaluation of multiple matrix-vector products Ad1, . . ., AdNc. In practice, storing the matrix\nA (in our case, the Hessian ∇2\nθP LP (θ∗\nP (θD), ϕP )) in its full form is typically infeasible. Instead,\na function that efficiently computes the required matrix-vector products instead of explicitly storing\nthe matrix is typically utilized. For our setup, this function is detailed in Algorithm 2, showing\nhow the K stored lower-level batches (we use K = 5 as previously outlined in the “Lower-level of\n18\n\nPreprint. Under review.\nBiSSL” paragraph in Section 4.2.2) are used to calculate Hessian-vector products. This approach en-\nsures that the output of the CG algorithm is an approximation of the inverse Hessian-vector product\nin the first term of Equation equation 7 as intended.\nAlgorithm 2 Hessian Vector Product Calculation fH (To use in the CG Algorithm)\n1: Input: Input vector v. Model parameters θP , ϕP . Training objective LP . Lower-level data\nbatches [z1, . . . , zK]. Regularization weight λ and dampening λdamp.\n2: Initialize y ←0\n▷Initialize Hessian vector product y\n3: for k = 1, . . . , K do\n4:\nπ(θP ) ←\n\u0000∇θLP (θ, ϕP ; zk) |θ=θP\n\u0001T v\n5:\ng ←∇θπ(θ)\n\f\f\nθ=θP\n▷Memory efficient calculation of ∇2\nθLP (θ, ϕP ; zk)|θ=θP x.\n6:\ny ←y + 1\nK g\n7: y ←v +\n1\nλ+λdamp y\n8: Return: fH(v) := y\nB.4\nCOMPOSITE CONFIGURATION OF BISSL\nTo avoid data being reshuffled between every training stage alternation, the respective batched lower-\nand upper-level training datasets are stored in separate stacks from which data is drawn. The stacks\nare only “reset” when the number of remaining batches is smaller than the number of gradient\nsteps required before alternating to the other level. For example, the lower-level stack is reshuffled\nevery fourth training stage alternation. If the downstream dataset does not provide enough data for\nmaking NU = 8 batches with non-overlapping data points, the data is simply reshuffled every time\nthe remaining number of data points is smaller than the upper-level batch size (256 images in these\nexperiments).\nB.5\nDOWNSTREAM FINE-TUNING AFTER BISSL\nThe learning rates and weight decays used for downstream fine-tuning after BiSSL for each respec-\ntive downstream dataset are outlined in Table 4. Section 4.2.2 outlines the experimental setup.\nC\nADDITIONAL RESULTS\nC.1\nVISUAL INSPECTION OF LATENT FEATURES\nTest data features of the downstream test data processed by backbones trained through conventional\npretext pre-training are compared against those trained with BiSSL. This allows for an inspection of\nthe learned representations prior to the final fine-tuning stage.\nDuring the evaluation, it is important to note that the batch normalization layers (Ioffe & Szegedy,\n2015) of the pre-trained backbones utilize the running mean and variance inferred during train-\ning. Since these pre-trained backbones have not been exposed to the downstream datasets during\ntraining, their batch normalization statistics may not be optimal for these new datasets. To address\nthis, the training dataset is divided into batches of 256 samples, and roughly 100 batches are then\nforward-passed through the backbones. This procedure ensures that the batch normalization statis-\ntics are better suited to the downstream datasets, thereby providing a fairer comparison of the learned\nrepresentations.\nFor the dimensionality reduction and visualization of these latent features, the t-Distributed Stochas-\ntic Neighbor Embedding (t-SNE) (Cieslak et al., 2020) technique is employed. This method allows\nus to visually assess the clustering and separation of features in the latent space, providing qualitative\ninsights into the semantic structure of the representations learned through BiSSL.\n19\n\nPreprint. Under review.\nTable 4: Hyper-parameter configurations used for downstream fine-tuning after BiSSL leading to\nthe highest top-1 classification accuracies (11-point mAP for the VOC2007 dataset).\nDataset\nLearning Rate\nWeight Decay\nSTL10L\n0.0005\n0.00043\nFlowers\n0.0009\n0.00005\nCars\n0.0293\n0.00851\nAircrafts\n0.011\n0.00612\nDTD\n0.0008\n0.00764\nPets\n0.0002\n0.00543\nFashionMNIST\n0.0022\n0.00876\nCIFAR10\n0.0003\n0.00991\nCIFAR100\n0.0012\n0.00422\nCaltech-101\n0.0008\n0.00011\nFood\n0.006\n0.0095\nSUN397\n0.0011\n0.00028\nCUB200\n0.035\n0.00868\nVOC2007\n0.0002\n0.00015\nBiSSL Backbone Features\nPretext Backbone Features\nairplane\nautomobile\nbird\ncat\ndeer\ndog\nfrog\nhorse\nship\ntruck\nFigure 4: CIFAR10\nFigures 3 to 11 illustrate the outcomes of these visual inspections on a selection of the downstream\ndatasets described in Section 4.1, highlighting the differences in feature representations between\nconventional pretext pre-training and BiSSL.\n20\n\nPreprint. Under review.\nBiSSL Backbone Features\nPretext Backbone Features\nFigure 5: CUB200\nBiSSL Backbone Features\nPretext Backbone Features\nFigure 6: Caltech-101\nBiSSL Backbone Features\nPretext Backbone Features\nairplane\nbird\ncar\ncat\ndeer\ndog\nhorse\nmonkey\nship\ntruck\nFigure 7: STL10L\n21\n\nPreprint. Under review.\nBiSSL Backbone Features\nPretext Backbone Features\nFigure 8: Pets\nBiSSL Backbone Features\nPretext Backbone Features\nFigure 9: DTD\nBiSSL Backbone Features\nPretext Backbone Features\nFigure 10: Aircrafts\n22\n\nPreprint. Under review.\nBiSSL Backbone Features\nPretext Backbone Features\nFigure 11: Cars\n23",
    "pdf_filename": "BiSSL_Bilevel_Optimization_for_Self-Supervised_Pre-Training_and_Fine-Tuning.pdf"
}