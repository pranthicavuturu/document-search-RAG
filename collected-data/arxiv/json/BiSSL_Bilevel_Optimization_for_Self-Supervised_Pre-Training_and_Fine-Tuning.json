{
    "title": "Preprint. Underreview.",
    "abstract": "In this work, we present BiSSL, a first-of-its-kind training framework that in- troduces bilevel optimization to enhance the alignment between the pretext pre- training and downstream fine-tuning stages in self-supervised learning. BiSSL formulates the pretext and downstream task objectives as the lower- and upper- level objectives in a bilevel optimization problem and serves as an intermediate training stage within the self-supervised learning pipeline. By more explicitly modelingtheinterdependenceofthesetrainingstages,BiSSLfacilitatesenhanced information sharing between them, ultimately leading to a backbone parameter initializationthatisbettersuitedforthedownstreamtask. Weproposeatraining algorithmthatalternatesbetweenoptimizingthetwoobjectivesdefinedinBiSSL. UsingaResNet-18backbonepre-trainedwithSimCLRontheSTL10dataset,we demonstratethatourproposedframeworkconsistentlyachievesimprovedorcom- petitive classification accuracies across various downstream image classification datasets compared to the conventional self-supervised learning pipeline. Quali- tativeanalysesofthebackbonefeaturesfurthersuggestthatBiSSLenhancesthe alignmentofdownstreamfeaturesinthebackbonepriortofine-tuning. 1 INTRODUCTION In the absence of sufficient labeled data, self-supervised learning (SSL) has emerged as a promis- ingapproachfortrainingdeeplearningmodels. Ratherthanrelyingsolelyonlabeleddata,theSSL frameworkaimstolearnrepresentationsfromunlabeleddatawhichprovesbeneficialforsubsequent useonvariousdownstreamtasks. Theserepresentationsarelearnedbysolvingapretexttask,which utilizessupervisorysignalsextractedfromtheunlabeleddataitself. Extensiveeffortshasgoneinto designing effective pretext tasks, achieving state-of-the-art or competitive performance in various fieldssuchascomputervision(Chenetal.,2020b;Bardesetal.,2022;Heetal.,2020;Grilletal., 2020;Caronetal.,2020;2021;Heetal.,2022;Oquabetal.,2024),audiosignalprocessing(Schnei- deretal.,2019;Baevskietal.,2020;Hsuetal.,2021;Niizumietal.,2021;Chung&Glass,2018; Chungetal.,2019;Yadavetal.,2024)andnaturallanguageprocessing(Devlinetal.,2019;Lewis etal.,2019;Brownetal.,2020;Heetal.,2021;Touvronetal.,2023). Making a self-supervised pre-trained backbone suitable for a downstream task typically involves attachingadditionallayersthatarecompatiblewiththattask, followedbyfine-tuningtheentireor partsofthecompositemodelinasupervisedmanner(Zhaietal.,2019;Duboisetal.,2022).Whena backboneispre-trainedonadistributionthatdiffersfromthedistributionofthedownstreamdata,the representations learned during pre-training may not be initially well-aligned with the downstream task. Duringfine-tuning,thisdistributionmisalignmentcouldcauserelevantsemanticinformation, learnedduringthepre-trainingphase, tovanishfromtherepresentationspace(Zaiemetal.,2024; Chenetal.,2020a;Boschinietal.,2022). Apotentialstrategyforalleviatingthenegativeeffectsof thesedistributiondiscrepancieswouldbetoenhancethealignmentbetweenthepretextpre-training anddownstreamfine-tuningstages.However,sincetheconventionalSSLpipelinetreatsthesestages astwodisjointprocesses,thisposesasignificantchallengeindevisingastrategythatenhancessuch alignmentwhilenotcompromisingonthebenefitsthatSSLoffers. 1 4202 voN 91 ]GL.sc[ 2v78320.0142:viXra",
    "body": "Preprint. Underreview.\nBISSL: BILEVEL OPTIMIZATION FOR SELF-\nSUPERVISED PRE-TRAINING AND FINE-TUNING\nGustavWagnerZakarias1,3 LarsKaiHansen2,3 Zheng-HuaTan1,3\n1AalborgUniversity 2TechnicalUniversityofDenmark\n3PioneerCentreforArtificialIntelligence,Denmark\n[gwz,zt]@es.aau.dk, lkai@dtu.dk\nABSTRACT\nIn this work, we present BiSSL, a first-of-its-kind training framework that in-\ntroduces bilevel optimization to enhance the alignment between the pretext pre-\ntraining and downstream fine-tuning stages in self-supervised learning. BiSSL\nformulates the pretext and downstream task objectives as the lower- and upper-\nlevel objectives in a bilevel optimization problem and serves as an intermediate\ntraining stage within the self-supervised learning pipeline. By more explicitly\nmodelingtheinterdependenceofthesetrainingstages,BiSSLfacilitatesenhanced\ninformation sharing between them, ultimately leading to a backbone parameter\ninitializationthatisbettersuitedforthedownstreamtask. Weproposeatraining\nalgorithmthatalternatesbetweenoptimizingthetwoobjectivesdefinedinBiSSL.\nUsingaResNet-18backbonepre-trainedwithSimCLRontheSTL10dataset,we\ndemonstratethatourproposedframeworkconsistentlyachievesimprovedorcom-\npetitive classification accuracies across various downstream image classification\ndatasets compared to the conventional self-supervised learning pipeline. Quali-\ntativeanalysesofthebackbonefeaturesfurthersuggestthatBiSSLenhancesthe\nalignmentofdownstreamfeaturesinthebackbonepriortofine-tuning.\n1 INTRODUCTION\nIn the absence of sufficient labeled data, self-supervised learning (SSL) has emerged as a promis-\ningapproachfortrainingdeeplearningmodels. Ratherthanrelyingsolelyonlabeleddata,theSSL\nframeworkaimstolearnrepresentationsfromunlabeleddatawhichprovesbeneficialforsubsequent\nuseonvariousdownstreamtasks. Theserepresentationsarelearnedbysolvingapretexttask,which\nutilizessupervisorysignalsextractedfromtheunlabeleddataitself. Extensiveeffortshasgoneinto\ndesigning effective pretext tasks, achieving state-of-the-art or competitive performance in various\nfieldssuchascomputervision(Chenetal.,2020b;Bardesetal.,2022;Heetal.,2020;Grilletal.,\n2020;Caronetal.,2020;2021;Heetal.,2022;Oquabetal.,2024),audiosignalprocessing(Schnei-\nderetal.,2019;Baevskietal.,2020;Hsuetal.,2021;Niizumietal.,2021;Chung&Glass,2018;\nChungetal.,2019;Yadavetal.,2024)andnaturallanguageprocessing(Devlinetal.,2019;Lewis\netal.,2019;Brownetal.,2020;Heetal.,2021;Touvronetal.,2023).\nMaking a self-supervised pre-trained backbone suitable for a downstream task typically involves\nattachingadditionallayersthatarecompatiblewiththattask, followedbyfine-tuningtheentireor\npartsofthecompositemodelinasupervisedmanner(Zhaietal.,2019;Duboisetal.,2022).Whena\nbackboneispre-trainedonadistributionthatdiffersfromthedistributionofthedownstreamdata,the\nrepresentations learned during pre-training may not be initially well-aligned with the downstream\ntask. Duringfine-tuning,thisdistributionmisalignmentcouldcauserelevantsemanticinformation,\nlearnedduringthepre-trainingphase, tovanishfromtherepresentationspace(Zaiemetal.,2024;\nChenetal.,2020a;Boschinietal.,2022). Apotentialstrategyforalleviatingthenegativeeffectsof\nthesedistributiondiscrepancieswouldbetoenhancethealignmentbetweenthepretextpre-training\nanddownstreamfine-tuningstages.However,sincetheconventionalSSLpipelinetreatsthesestages\nastwodisjointprocesses,thisposesasignificantchallengeindevisingastrategythatenhancessuch\nalignmentwhilenotcompromisingonthebenefitsthatSSLoffers.\n1\n4202\nvoN\n91\n]GL.sc[\n2v78320.0142:viXra\nPreprint. Underreview.\nSSL Pipeline With BiSSL\nPretext Task Warm-up\nConventional SSL Pipeline Init\nPretext Task\nUpper-Level (Downstream)\nDownstream Head Warm-up\nBiSSL\nDownstream Fine-Tuning\nLower-Level (Pretext)\nDownstream Fine-Tuning\nFigure 1: The conventional self-supervised learning pipeline alongside the proposed pipeline in-\nvolvingBiSSL.Thesymbolsθ andϕrepresentobtainedbackboneandtask-specificattachedhead\nparameters,respectively. Whentheyaretransmittedtotherespectivetrainingstages,theyareused\nasinitializations.\nMeanwhile,bileveloptimization(BLO)hasrisenasapowerfultoolforsolvingcertainoptimization\nproblemswithindeeplearning. Itentailsamainoptimizationproblemconstrainedbythesolution\ntoasecondaryoptimizationproblemthatdependsontheparametersofthemainobjective. Thishi-\nerarchicalsetupcausesthesolutionsofbothoptimizationproblemstodependoneachother,either\ndirectlyorimplicitly,whichhasprovenadvantageousindeeplearningtasksthatoptimizemultiple\ninter-dependentobjectivessimultaneously(Zhangetal.,2023a). Notablementionsoftaskswithin\ndeeplearningwhereBLOhasprovenusefulareparameterpruning(Zhangetal.,2022b),invariant\nrisk minimization (Arjovsky et al., 2019; Zhang et al., 2023b), meta-learning (Rajeswaran et al.,\n2019; Finn et al., 2017), adversarial robustness (Zhang et al., 2021), hyper-parameter optimiza-\ntion(Franceschietal.,2018)andcoresetselection(Borsosetal.,2020).\nIn this study, we propose BiSSL, a novel training framework that leverages BLO to enhance the\nalignment between the pretext pre-training and downstream fine-tuning stages in SSL. Acting as\nan intermediate training stage within the SSL pipeline, BiSSL frames the pretext and downstream\ntaskobjectivesasthelower-andupper-levelobjectivesinaBLOproblem-achallengingapproach\nthat has not been explored until now. The objectives in BiSSL are connected by substituting the\nlower-level backbone solution for the upper-level backbone parameters, while simultaneously en-\nforcing the lower-level backbone solution to resemble the upper-level backbone parameters. This\napproachmoreexplicitlycapturestheinterdependencebetweenpretextpre-traininganddownstream\nfine-tuning,potentiallyleadingtoalower-levelbackbonebetteralignedwiththedownstreamtask.\nFigure1comparestheconventionalSSLpipelinewithoursuggestedpipelineinvolvingBiSSL.Ad-\nditionally,weproposeatrainingalgorithmforBiSSLanddemonstratethatitconsistentlyimproves\nor maintains comparable downstream performance across a range of image classification datasets.\nForourexperiments,weuseSimCLR(Chenetal.,2020b)topre-trainaResNet-18backbone(He\netal.,2016)ontheunlabeledpartitionoftheSTL10dataset(Coatesetal.,2011), asetupoffering\nsuitablemodelcapacityanddatasetcomplexitywhilebeinglessresource-intensivethanlarger-scale\nalternatives. Thecodeimplementationandpre-trainedmodelweightsarepubliclyavailable.1\n2 RELATED WORK\nBilevelOptimizationinSelf-SupervisedLearning Bileveloptimization(BLO)referstoacon-\nstrainedoptimizationproblem,wheretheconstraintitselfisasolutiontoanotheroptimizationprob-\n1https://github.com/ICLR25-10484/ICLR25_10484_BiSSL\n2\nPreprint. Underreview.\nlem, which depends on the parameters of the “main” optimization problem. The general BLO\nproblemisformulatedas\nminf(ξ,ψ∗(ξ)) s.t. ψ∗(ξ)∈argming(ξ,ψ), (1)\nξ ψ\nwhere f and g are referred to as the upper-level and lower-level objectives, respectively. While\nthelowerobjectiveg hasknowledgeoftheparametersξ fromtheupper-levelobjective,theupper-\nlevel objective f possesses full information of the lower objective g itself through its dependence\nonthelower-levelsolutionψ∗(ξ). Someworkshaveincorporatedbileveloptimizationwithinself-\nsupervisedlearning. Guptaetal.(2022)suggestformulatingthecontrastiveself-supervisedpretext\ntaskasabileveloptimizationproblem,dedicatingtheupper-levelandlower-levelobjectivesforup-\ndating the backbone and projection head parameters respectively. Other frameworks such as the\nLocalandGlobal(LoGo)(Zhangetal.,2022a)andOnlySelf-SupervisedLearning(OSSL)Boonlia\netal.(2022)utilizeauxiliarymodels,whereinthelower-levelobjectiveoptimizestheparametersof\nthe auxiliary model, while the upper-level objective is dedicated to training the feature extraction\nmodel. MetaMask (Li et al., 2022b) introduces a meta-learning based approach, where the upper-\nlevellearnsmasksthatfilteroutirrelevantinformationfrominputsthatareprovidedtoalower-level\nself-supervised contrastive pretext task. Chen et al. (2023) introduces a pseudo-BLO setup where\ntheupper-leveloptimizationstillbenefitsfromknowledgeofthelower-levelobjective, butthepa-\nrametersofthelower-levelobjectivearefixedduringtraining. InSomayajulaetal.(2023), atwo-\nstagedBLOproblemisproposedtofine-tuneself-supervisedpre-trainedlargelanguagemodelsin\nlow-resourcescenarios. Theirapproachfocusesonsolvingdownstreamtaskswhilesimultaneously\nlearning a task-dependent similarity structure. BLO-SAM (Zhang et al., 2024) is tailored towards\nfine-tuningthesegmentanythingmodel(SAM)(Kirillovetal.,2023)byinterchangeablyalternating\nbetween learning (upper-level) prompt embeddings and fine-tuning the (lower-level) segmentation\nmodel.Theaforementionedframeworksintegratebileveloptimizationintoeitherthepre-trainingor\nfine-tuningstageexclusivelyandaretailoredtowardsspecificpretextordownstreamtasks. Incon-\ntrast,ourproposedBiSSLemploysaBLOproblemthatcomprehensivelyincorporatesbothtraining\nstages of pretext pre-training and downstream fine-tuning, without being confined to any specific\ntypeofpretextordownstreamtask.\nPrimingPre-TrainedBackbonesPriorToFine-Tuning Previousworkshavedemonstratedthat\ndownstreamperformancecanbeenhancedbyintroducingtechniquesthatmodifythebackbonebe-\ntween the pre-training and fine-tuning stages. Contrastive Initialization (COIN) (Pan et al., 2022)\nintroduces a supervised contrastive loss, to be utilised on backbones pre-trained with contrastive\nSSLtechniques. Noisy-Tune(Wuetal.,2022)perturbsthepre-trainedbackbonewithtailorednoise\nbefore fine-tuning. Speaker-invariant clustering (Spin) (Chang et al., 2023) utilizes speaker dis-\nentanglement and vector quantization for improving speech representations for speech signal spe-\ncificdownstreamtasks. RIFLE(Lietal.,2020)conductsmultiplefine-tuningsessionssequentially,\nwhere the attached downstream specific layers are re-initialized in between every session. Unlike\nBiSSL, these techniques either do not incorporate knowledge of both the pretext task and down-\nstreamtaskobjectivesandtheirrelationshipordosoonlyimplicitly.\n3 PROPOSED METHOD\n3.1 NOTATION\nWe denote the unlabeled pretext dataset DP = {z }CP and labeled downstream dataset DD =\nk k=1\n{x ,y }CD , respectively, where z ,x ∈ RN. Let f : RN → RM denotes a feature extracting\nl l k=1 k l θ\nbackbone with trainable parameters θ and p : RM → RP a task specific projection head with\nϕ\ntrainable parameters ϕ. Given pretext and downstream models g ◦ f and h ◦ f with\nϕP θP ϕD θD\nθ ,θ ∈ RL, we denote the pretext and downstream training objectives LP(θ ,ϕ ;DP) and\nP D P P\nLD(θ ,ϕ ;DD), respectively. To simplify notation, we omit the dataset specification from the\nD D\ntrainingobjectives,e.g. LD(θ ,ϕ ):=LD(θ ,ϕ ;DD).\nD D D D\n3.2 OPTIMIZATIONPROBLEMFORMULATION\nThe conventional setup of self-supervised pre-training directly followed by supervised fine-tuning\nrelies on using a single backbone model with parameters θ. In that instance, we minimize\n3\nPreprint. Underreview.\nLP(θ,ϕ ) to produce a backbone parameter configuration θ∗ which is then used as an initial-\nP\nizationwhensubsequentlyminimizingthedownstreamtrainingobjectiveLD(θ,ϕ ). Wedeviate\nD\nfromthisbyinsteadconsideringθ andθ astwoseparateparametervectorsthatarestronglycor-\nP D\nrelated. Incontinuation,wesuggestcombiningthetwotraditionallyseparateoptimizationproblems\nof pretext and downstream training into a joint optimization problem through bilevel optimization\ncalledBiSSL.WeformulateBiSSLas\nmin LD(θ∗ (θ ),ϕ )+γLD(θ ,ϕ ) (2)\nP D D D D\nθD,ϕD\ns.t. θ∗ (θ )∈argminminLP (θ ,ϕ )+λr(θ ,θ ) (3)\nP D P P D P\nθP ϕP\nwith γ ∈ R and r being some convex regularisation objective weighted by λ ∈ R enforcing\n+ +\nsimilarity between θ and θ . The upper-level training objective in equation 2 is tasked with\nD P\nminimizing the downstream task objective LD, while the lower-level objective in equation 3 aims\nto minimize the pretext task objective LP while also ensuring its backbone remains similar to the\nupper-level backbone. As seen in the left term of equation 2, the backbone parameters θ∗(θ )\nP D\naretransferredintothedownstreamtrainingobjective,mirroringhowthebackboneistransferredin\ntheconventionalSSLpipeline. Althoughthesecondtermofequation2isnotstrictlynecessary,it\nhas empirically shown to improve stability and aid convergence of the upper-level solution during\ntraining. Unlike the traditional SSL setup, the backbone solution of the pretext objective θ∗(θ )\nP D\nisnowafunctionoftheparametersofthedownstreambackboneθ ,asthelower-levelproblemis\nD\ndependentontheupper-levelbackboneparameters.\nAs the upper-level objective in equation 2 depends on the solution θ∗(θ ) of the lower-level ob-\nP D\njectiveinequation3,thisenablestheincorporationofinformationfromthepretextobjectivewhen\nsolvingtheupper-leveloptimizationproblem.Byincludingaregularizationobjectiverthatenforces\nsimilaritybetweenthelower-levelandupper-levelbackboneparameters,thissetupishypothesized\ntoguidethelower-leveltoachieveaconfigurationofmodelbackboneparametersthatismoreben-\neficial for subsequent conventional fine-tuning on the downstream task. To more precisely under-\nstandhowthepretextobjectiveinfluencesthedownstreamtrainingprocedureinthissetup,wedelve\ndeeperintotheexpressionofthegradientoftheupper-leveltrainingobjectiveinequation2inthe\nfollowingsubsection.\n3.3 UPPER-LEVELDERIVATIVE\nGiventheupper-levelobjectiveF(θ ,ϕ ):=LD(θ∗(θ ),ϕ )+γLD(θ ,ϕ )fromequation2,\nD D P D D D D\nitsderivativewithrespecttoθ isgivenby\nD\ndF dθ∗(θ )T\n= P D ∇ LD(θ,ϕ )| +γ∇ LD(θ,ϕ )| . (4)\ndθ\nD\ndθ\nD\nθ D θ=θ P∗(θD) θ D θ=θD\n(cid:124) (cid:123)(cid:122) (cid:125)\nIG\nDue to the dependence of the lower-level solution on the upper-level parameters, the first term of\nequation4includestheimplicitgradient(IG)oftheimplicitfunctionθ∗(θ ).Tosimplifynotation,\nP D\nwe let ∇ h(ξ)| := ∇ h(ψ) when it is clear from context which variables are differentiated\nξ ξ=ψ ξ\nwith respect to. Following an approach similar to Rajeswaran et al. (2019), with details on the\nderivationsandunderlyingassumptionsoutlinedinSectionA.1ofAppendixA,theIGinequation4\ncanbeexplicitlyexpressedas\ndθ∗(θ )T (cid:20) (cid:18) 1 (cid:19)(cid:21)−1\nP D =−∇2 r(θ ,θ∗(θ )) ∇2 LP(θ∗(θ ),ϕ )+r(θ ,θ∗(θ )) . (5)\ndθ θDθP D P D θ λ P D P D P D\nD\nAcommonconvexregularizationobjective,whichwillalsobethechoiceinthesubsequentexper-\nimentsofthiswork,isr(ξ,ψ) = 1∥ξ−ψ∥2. Usingthisregularizationobjectivesimplifiesequa-\n2 2\ntion5downto\ndθ∗(θ )T (cid:20) 1 (cid:21)−1\nP D = ∇2LP(θ∗(θ ),ϕ )+I , (6)\ndθ λ θ P D P L\nD\nwhereI istheL×L-dimensionalidentitymatrix. Hencetheupper-levelderivativeinequation4\nL\ncanbeexpressedas\ndF\n(cid:20)\n1\n(cid:21)−1\n= ∇2LP(θ∗(θ ),ϕ )+I ∇ LD(θ∗(θ ),ϕ )+γ∇ LD(θ ,ϕ ). (7)\ndθ λ θ P D P L θ P D D θ D D\nD\n4\nPreprint. Underreview.\nThe inverse Hessian-vector product in the left term of equation 7 is computationally infeasible to\ncalculate directly, hence it is approximated using the conjugate gradient (CG) method (Nazareth,\n2009; Shewchuk, 1994). While CG is established as a successful approach for approximating the\ninverseHessian-vectorproductsinpreviousworks(Pedregosa,2016;Zhangetal.,2021;Rajeswaran\netal.,2019),itstillintroducessignificantcomputationaloverheadduetoitsneedforiterativeevalu-\nationsofmultipleHessianvectorproducts. Futureworkmayexplorealternativemethodsthatoffer\nmoreefficientapproximationswithoutcompromisingdownstreamtaskperformance. Weemploya\nlayer-wiseimplementationoftheCGmethodbasedonthatofRajeswaranetal.(2019)andreferto\ntheirworkformoredetailsonapplyingCGinadeeplearningsetupwithBLO.Foracomprehensive\noverviewofothercommonmethodsusedtoapproximatetheupper-levelderivativeinBLO,werefer\ntoZhangetal.(2023a).\nWithanexplicitexpressionoftheIGinequation6,wecaninterprettheimpactofthescalingfactor\nλfromequation3andequation7: Whenλisverylarge,thedependenceoflower-levelobjectiveon\nthe upper-level parameters θ is also very large. This effectively drives the lower-level backbone\nD\nparameters toward the trivial solution θ∗(θ ) = θ . Meanwhile, the IG in equation 6 approxi-\nP D D\nmatelyequalsI ,therebydiminishingtheinfluenceofthelower-levelobjectiveontheupper-level\nL\ngradient in equation 7. This roughly makes the task of the upper-level equivalent to conventional\nfine-tuning.Conversely,ifλisverysmall,thelower-levelobjectiveinequation3effectivelydefaults\ntoconventionalpretexttasktraining. Additionally,theimplicitgradientinequation6wouldconsist\nofnumericallytinyentries,makingtheoptimizationofthefirsttermintheupper-levelobjectivein\nequation2equivalenttoprobingofthedownstreamheadonthefrozenpretextbackboneθ∗(θ ).\nP D\n3.4 TRAININGALGORITHMANDPIPELINE\nAlgorithm1BiSSLTrainingAlgorithm\n1: Input: Backboneandprojectionheadparameterinitializationsθ,ϕ P,ϕ D. Trainingobjectives\nLP, LD. Weights λ,γ ∈ R . Optimizers opt , opt . Number of training stage alternations\n+ P D\nT ∈ N with upper and lower-level iterations N ,N ∈ N. Upper-level backbone adaption\nU L\nfrequencyN ∈Nandstrengthα∈[0,1].\na\n2: Initializeθ P ←θandθ D ←θ.\n3: fort=1,...,T do\n4: forn=1,...,N Ldo ▷Lower-level\n5: Computeg ϕP =∇ ϕLP(θ P,ϕ)| ϕ=ϕP .\n6: Computeg θP =∇ θLP(θ,ϕ P)| θ=θP +λ∇ θr(θ D,θ)| θ=θP.\n7: Updateϕ P ←opt P(ϕ P,g ϕP)andθ P ←opt P(θ P,g θP).\n8: iftmodN a ≡0then\n9: θ D ←(1−α)θ D+αθ P.\n10: forn=1,...,N U do ▷Upper-level\n11: Computeg ϕD =∇ ϕLD(θ P,ϕ)| ϕ=ϕD +γ∇ ϕLD(θ D,ϕ)| ϕ=ϕD.\n12: Computev=∇ θLD(θ,ϕ D)| θ=θP.\n13: Approximatev IG ≈(cid:2) I M + λ1∇2 θLP(θ,ϕ P)| θ=θP(cid:3)−1 v. ▷UseCG\n14: Computeg θD =v IG+γ∇ θLD(θ,ϕ D)| θ=θD.\n15: Updateϕ D ←opt D(ϕ D,g ϕD)andθ D ←opt D(θ D,g θD).\n16: Return: BackboneParametersθ P.\nAlgorithm 1 outlines the proposed training algorithm, which iteratively alternates between solv-\ningthelower-level(equation3)andupper-level(equation2)optimizationproblemsinBiSSL.The\nlower-leveltrainingoptimizesthepretexttaskobjective,whileadditionallyincludingthegradientof\ntheregularizationtermr forthebackboneparameterupdates, complyingwithequation3. Forthe\nupper-leveltraining,thegradientwithrespecttothebackboneparametersasrepresentedbytheleft\ntermontheright-handsideinequation7,isapproximatedusingtheCGmethod. Additionally,the\n5\nPreprint. Underreview.\npretextbackboneparametersθ areweightedbyαandaddedtothedownstreambackboneparam-\nP\neters θ every N alternations to further enforce similarity between them, which empirically has\nD a\nshowntoaidconvergenceduringtraining.\nFrom Section A.1 in Appendix A, we get that θ∗(θ ) must fulfill the stationary condition\nP D\n∇ (cid:0) LP(θ,ϕ )+λr(θ ,θ)(cid:1) | = 0 to justify the explicit expression of the implicit gra-\nθ P D θ=θ P∗(θD)\ndient in equation 6. This means that executing Algorithm 1 using random initializations of θ and\nϕ willlikelynotsuffice. Thesameappliestoϕ ,asarandominitializationofϕ typicallyleads\nP D D\ntorapidinitialchangesofthebackboneparametersθ duringfine-tuning. Thiswouldthenlikely\nD\nviolatetheassumedstationaryconditionduetothedependencebetweenθ andθ throughthereg-\nD P\nularization objective r. Figure 1 illustrates the suggested pipeline alongside the conventional SSL\npipeline. Firstconventionalpretextpre-trainingisperformedontheunlabeleddatasetDP toobtain\ninitializationsofθandϕ . Next,thedownstreamheadisfittedontopofthefrozenbackboneθus-\nP\ningthedownstreamdatasetDD,whichprovidesaninitializationofthedownstreamheadparameters\nϕ . Then,BiSSLtrainingisconductedasoutlinedinAlgorithm1,yieldinganupdatedconfigura-\nD\ntion of backbone parameters θ∗(θ ). These updated backbone parameters are subsequently used\nP D\nasaninitializationforthefinalsupervisedfine-tuningonthedownstreamtask.\n4 EXPERIMENTS AND RESULTS\n4.1 DATASETS\nTheSTL10dataset(Coatesetal.,2011)isusedthroughouttheexperiments. Itcomprisestwopar-\ntitions: 100.000unlabeledimagesand13.000labeledimageswith10classesintotalwhereas5000\nand8000areassignedfortrainingandtesting,respectively. Allimagesarenaturalimagesofreso-\nlution96×96,withtheunlabeledpartitiondrawnfromasimilarbutbroaderdistributionthanthe\nlabeledpartition. Thisdatasetstrikesabalancebetweencomplexityandcomputationalfeasibility,\nofferinghigherresolutionandmorediversecontentthansmallerdatasetslikeCIFAR10(Krizhevsky,\n2012)whilebeinglessresource-intensivethanlarger-scaledatasetssuchasImageNet(Dengetal.,\n2009). For ease of reference, STL10U and STL10L will denote the unlabeled and labeled parti-\ntions, respectively. Inallexperiments, STL10Uwillbeemployedforself-supervisedpre-training.\nFordownstreamfine-tuningandevaluation,weleverageavariedsetofnaturalimageclassification\ndatasets that encompass a wide array of tasks, including general image classification, fine-grained\nrecognitionacrossspeciesandobjects,sceneunderstanding,andtexturecategorization.Thedatasets\ninclude STL10L, Oxford 102 Flowers (Nilsback & Zisserman, 2008), StanfordCars (Yang et al.,\n2015),FGVCAircraft(Majietal.,2013),DescribableTexturesDataset(DTD)(Cimpoietal.,2014),\nOxford-IIIT Pets (Parkhi et al., 2012), FashionMNIST (Xiao et al., 2017), CIFAR10 (Krizhevsky,\n2012), CIFAR100 (Krizhevsky, 2012), Caltech-101 (Li et al., 2022a), Food 101 (Bossard et al.,\n2014),SUN397scenedataset(Xiaoetal.,2010),Caltech-UCSDBirds-200-2011(CUB200)(Wah\net al., 2011) and PASCAL VOC 2007 (Everingham et al.). All downstream datasets are split into\ntraining,validation,andtestpartitions,withdetailsonhowtheseassignmentsaremadeprovidedin\nSectionB.1ofAppendixB.\n4.2 IMPLEMENTATIONDETAILS\n4.2.1 BASELINESETUP\nPretextTaskTraining TheSimCLR(Chenetal.,2020b)pretexttaskwithtemperatureτ = 0.5\nis used for pre-training a ResNet-18 (He et al., 2016) backbone model. We selected this widely\nadopted architecture due to its proven ability to extract high-quality visual representations while\nmaintaining relatively low computational requirements, striking an effective balance between per-\nformanceandresourceefficiency. Ontopofthebackbone,aprojectionheadisused,consistingof\ntwo fully connected layers with batch normalization (Ioffe & Szegedy, 2015) and ReLU (Agarap,\n2018)followedbyasinglelinearlayer. Eachlayerconsistsof256neurons.\nThe image augmentation scheme follows the approach used in Bardes et al. (2022), with minor\nmodifications: Theimagesizeissetto96×96insteadof224×224,andtheminimalratioofthe\nrandomcropisadjustedaccordinglyto0.5insteadof0.08.\n6\nPreprint. Underreview.\nTheimplementationoftheLARSoptimizer(Youetal.,2017)fromBardesetal.(2022)isemployed,\nwitha“trust”coefficientof0.001,aweightdecayof10−6andamomentumof0.9.Thelearningrate\nincreaseslinearlyduringthefirst10epochs,reachingapeakbaselearningrateof4.8,followedby\nacosinedecaytowards0withnorestarts(Loshchilov&Hutter,2017)fortheremainingepochs. A\nbatchsizeof1024isusedand,unlessotherwisespecified,pre-trainingisconductedfor600epochs.\nFine-Tuning on the Downstream Task For downstream fine-tuning, a single linear layer is at-\ntachedtotheoutputofthepre-trainedbackbone. Thetrainingprocedureutilizesthecross-entropy\nloss, the SGD optimizer with a momentum of 0.9, and a cosine decaying learning rate scheduler\nwithoutrestarts(Loshchilov&Hutter,2017). Fine-tuningisconductedfor400epochswithabatch\nsize of 256. An augmentation scheme similar to the fine-tuning augmentation scheme in Bardes\net al. (2022) is employed, where images are center cropped and resized to 96×96 pixels with a\nminimalcropratioof0.5,followedbyrandomhorizontalflips.\nArandomgridsearchof200hyper-parameterconfigurationsforthelearningratesandweightdecays\nisconducted,whereonemodelisfine-tunedforeachconfiguration. Baselearningratesandweight\ndecaysarelog-uniformlysampledovertherangesof0.0001to1.0and0.00001to0.01,respectively.\nValidationdataaccuracyisevaluatedaftereachepoch. Thehyper-parameterconfigurationyielding\nthebestbalancebetweenhighvalidationaccuracyandlowvalidationlossisconsideredtheoptimal\nhyper-parameterconfiguration.2 Thecorrespondingoptimalhyper-parametersforeachdownstream\ndatasetaredocumentedinTable2ofAppendixB.\nFor subsequent evaluation on the test data, we train 10 models with different random seeds, each\nusingtheconsideredoptimalhyper-parameterconfigurations. Duringthetrainingofeachrespective\nmodel, the model parameters are stored after each epoch if the top-1 validation accuracy (or 11-\npointmAPfortheVOC07dataset)hasincreasedcomparedtotheprevioushighesttop-1validation\naccuracy achieved during training. Top-1 and top-5 test data accuracies (or 11-point mAP for the\nVOC07 dataset) are evaluated for each of the 10 models, from which the calculated means and\nstandarddeviationsoftheseaccuraciesaredocumented.\n4.2.2 BISSLSETUP\nInthissection,wedetaileachstageoftheproposedtrainingpipelineforBiSSL,asoutlinedinthe\nrightpartofFigure1.\nPretextWarm-up Thebackboneθandprojectionheadϕ areinitializedbyself-supervisedpre-\nP\ntraining using a setup almost identical to the baseline pretext task training setup in Section 4.2.1.\nThe only difference is that this training stage is conducted for 500 epochs instead of 600 epochs,\nandthatthepeakbaselearningrateissetto1.0insteadof4.8. Thisadjustmentismadebecausethe\nBiSSLtrainingstagewillconductwhatisroughlyequivalentto100pretextepochs,asdetailedmore\nspecificallyinthecompositeconfigurationparagraphbelow. Thisensuresthatthetotalnumberof\npretextpre-trainingstepsiscomparabletothoseconductedinthebaselinesetup.\nDownstreamHeadWarm-up Thetrainingsetupforthedownstreamheadwarm-upcloselymir-\nrorsthefine-tuningsetupofSection4.2.1. Themaindifferenceisthatonlythelineardownstream\nheadisfittedontopofthenowfrozenbackboneobtainedfromthepretextwarm-up. Learningrates\nand weight decays are initially selected based on those listed in Table 2, with adjustments made\nas needed when preliminary testing indicated a potential for improved convergence. These values\nare provided in Table 3 in Appendix B. The authors recognize that more optimal hyper-parameter\nconfigurationsmayexistandleavefurtherexplorationofthisforfuturerefinement.Thedownstream\nheadwarm-upisconductedfor20epochswithaconstantlearningrate.\nLower-level of BiSSL The training configuration for the lower-level primarily follows the setup\ndescribedforpretextpre-traininginSection4.2.1,withthemodificationsoutlinedhere.Asspecified\ninequation3,thelower-levellossfunctionisthesumofthepretexttaskobjectiveLP (inourcase,\nthe NT-Xent loss from SimCLR (Chen et al., 2020b)) and the regularization term r(θ ,θ ) =\nD P\n2Incertainscenariosduringtheexperiments,theconfigurationthatachievedthehighestvalidationaccuracy\nalsoyieldedanotablyhigherrelativevalidationloss. Toensurebettergeneralizability,analternativeconfigu-\nrationwithamorefavorabletrade-offwasselectedinthesecases.\n7\nPreprint. Underreview.\nTable1: Testclassificationaccuracies. Accuraciessignificantlydifferentfromtheircounterpartsare\nmarkedinboldfont.\nTop-1Accuracy(*: 11-pointmAP) Top-5Accuracy\nDataset\nBiSSL OnlyFT AvgDiff BiSSL OnlyFT AvgDiff\nSTL10L 90.2±0.1 90.3±0.1 −0.1 99.7±0.0 99.6±0.0 +0.1\nFlowers 74.8±0.2 73.4±0.4 +1.4 89.8±0.3 90.0±0.4 −0.2\nCars 73.0±0.4 72.7±0.5 +0.3 91.5±0.3 91.4±0.4 +0.1\nAircrafts 46.9±0.5 46.1±0.9 +0.8 78.9±0.4 79.3±0.6 −0.4\nDTD 51.8±0.5 49.3±0.5 +2.5 79.9±0.3 79.1±0.4 +0.8\nPets 67.8±0.2 65.0±0.5 +2.8 92.3±0.3 90.7±0.3 +1.6\nFMNIST 94.3±0.2 94.1±0.1 +0.2 100.0±0.0 100.0±0.0 0.0\nCIFAR10 93.9±0.1 93.8±0.1 +0.1 99.9±0.0 99.8±0.0 +0.1\nCIFAR100 73.0±0.1 73.2±0.2 −0.2 93.7±0.1 92.8±0.1 +0.9\nCaltech-101 80.6±0.7 78.1±0.5 +2.5 95.5±0.2 94.7±0.2 +0.8\nFood 72.0±0.2 71.7±0.2 +0.3 90.4±0.1 90.4±0.1 0.0\nSUN397 41.1±0.2 40.0±0.3 +1.1 71.0±0.2 69.9±0.4 +1.1\nCUB200 47.1±0.4 45.7±0.4 +1.4 72.1±0.3 70.7±0.6 +1.4\nVOC07 ∗60.4±0.1 ∗58.6±0.3 +1.8 − − −\n1||θ −θ ||2. Based on early experiments, the regularization weight λ = 0.001 was selected,\n2 D P 2\nas it appeared to strike a well-balanced compromise between the convergence rates of both the\nlower- and upper-level objectives. The lower-level is trained for the equivalent of approximately\n100conventionalpre-trainingepochs, withfurtherdetailsprovidedinthecompositeconfiguration\nparagraph. EachtimetheBiSSLtrainingalternatesbacktothelower-level,thefirst5batchesused\nforlower-leveltrainingarestored. ThesestoredbatchesareutilizedtoapproximatetheHessianof\nthelower-levelobjectivewhenapproximatingtheupper-levelgradient. Furtherdetailsarespecified\ninSectionB.3ofAppendixBandtheparagraphbelow.\nUpper-levelofBiSSL Theupper-leveltrainingstagealsosharesmanysimilaritieswiththedown-\nstreamtrainingsetupdescribedinSection4.2.1,andagain,onlythedifferencesareaddressedhere.\nThe weight decays and base learning rates are set to match those obtained from the downstream\nhead warm-up detailed in Table 3 of Appendix B. The weighting of the conventional downstream\nlossobjectiveissettoγ =0.01. Toapproximatetheupper-levelgradientin equation7,theconju-\ngategradientmethod(Nazareth,2009;Shewchuk,1994)isemployed. Furtherdetailsregardingthe\nsetupfortheupper-levelgradientapproximationarecoveredinSectionB.3ofAppendixB.\nCompositeConfigurationDetailsofBiSSL AsoutlinedinAlgorithm1,bothlower-andupper-\nlevelbackboneparametersθ andθ areinitializedwiththebackboneparametersobtainedduring\nP D\nthe pretext warm-up, and the training procedure alternates between solving the lower- and upper-\nleveloptimizationproblems. Inthisexperimentalsetup,thelower-levelperformsN =20gradient\nL\nstepsbeforealternatingtotheupper-level,whichthenconductsN = 8gradientsteps. Atotalof\nU\nT =500trainingstagealternationsareexecuted.AstheSTL10Udatasetwiththecurrentbatchsize\nof1024amountstoatotalof98trainingbatcheswithoutreplacement,theseT =500trainingstage\nalternations roughly equal 100 conventional pretext epochs. Section B.4 in Appendix B outlines\nfurtherdetailsonhowdatabatchesarehandledduringtraining.Theupper-levelbackboneadaptation\nfrequency and strength are set to N = 100 and α = 0.1, respectively. Additionally, gradient\na\nnormalizationisemployedongradientsexceedingℓ -normsof10.\n2\nFine-Tuning on the Downstream Task Subsequent downstream fine-tuning is conducted in a\nmanneridenticaltothatdescribedinthe‘Fine-TuningontheDownstreamTask”paragraphofsec-\ntion 4.2.1. Table 4 in Appendix B lists the considered optimal hyper-parameter configurations for\neachdataset.\n8\nPreprint. Underreview.\nTest Classification Accuracies\nTop 1 Accuracy Top 5 Accuracy\n77 BiSSL 91 BiSSL\nOnly FT Only FT\n76\n90\n75\n74 89\n73\n88\n72\n71 87\n70\n86\n200 300 400 500 600 700 800 900 1000 200 300 400 500 600 700 800 900 1000\nTotal Number of Pretext Epochs Total Number of Pretext Epochs\nFigure 2: Test classification accuracies on the flowers dataset over varying numbers of pretext\nepochs.\n4.3 DOWNSTREAMTASKPERFORMANCE\nTheimpactofusingBiSSLcomparedtotheconventionalself-supervisedtrainingpipelineisbench-\nmarked by evaluating classification accuracies on the various specified downstream datasets. Ta-\nble 1 presents the means and standard deviations of top-1 and top-5 classification accuracies (or\nthe 11-point mAP on the VOC2007 dataset) on these downstream test datasets, comparing results\nobtainedfromtheconventionalSSLpipelinewiththoseachievedusingtheBiSSLpipeline. There-\nsultsdemonstratethattrainingwithBiSSLsignificantlyimproveseithertop-1ortop-5classification\naccuracy, or 11-point mAP in the case of VOC07, on 10 out of 14 datasets, with no single result\nshowingasignificantdeclineinperformancecomparedtothebaseline.\n4.3.1 PERFORMANCEOVERVARYINGPRE-TRAININGEPOCHS\nTable1demonstratesthatBiSSLcansignificantlyenhanceclassificationaccuraciesacrossvarious\ndatasets. To determine whether this improvement persists across different levels of pretext pre-\ntraining,additionalexperimentsareconductedusingvaryingnumbersoftotalpre-trainingepochs.\nTheaccuraciesarecomparedwiththetotalnumberofpretextpre-trainingepochskeptroughlythe\nsame,sothepretextwarm-upintheBiSSLpipelineisconductedfor100epochslessthanthebase-\nlinepretextpre-training,asjustifiedinthe“CompositeConfigurationDetailsofBiSSL”paragraph\nofSection4.2.2. Theflowersdataset, whichshowedsubstantialbenefitsfromBiSSLonthetop-1\nclassification accuracies, is selected for these experiments. The results presented in Figure 2 re-\nvealthatBiSSLgenerallysustainstherelativeperformancegapoverthebaseline,regardlessofthe\npre-trainingduration. ThissuggeststhattheadvantagesconferredbyBiSSLarenotcontingenton\ntheamountofpre-training. Rather,thisindicatesthatBiSSLmayprovideamoreefficientlearning\ntrajectory, stemming from the enhanced information sharing it facilitates between the pretext and\ndownstreamtasks.\n4.4 VISUALINSPECTIONOFLATENTFEATURES\nTogaindeeperinsightintohowBiSSLaffectstherepresentationslearnedcomparedtoconventional\npretextpre-training,weperformaqualitativevisualinspectionoflatentspaces. Thisinvolvescom-\nparingfeaturesprocessedbybackbonestrainedsolelybypretextpre-trainingtothosederivedfrom\nlower-level backbones obtained after conducting BiSSL, each trained as described in the “Pretext\nTaskTraining”and“Lower-levelofBiSSL”paragraphsinSection4.2.1and4.2.2,respectively. By\ncomparing these features, we aim to assess whether BiSSL nudges the latent features toward be-\ningmoresemanticallymeaningfulforthedownstreamtask. Thet-DistributedStochasticNeighbor\nEmbedding(t-SNE)(Cieslaketal.,2020)techniqueisemployedfordimensionalityreduction. Fur-\nther details regarding the experimental setup are outlined in Section C.1 of Appendix C. Figure 3\n9\n% %\nPreprint. Underreview.\nBiSSL Backbone Features Pretext Backbone Features\nFigure 3: Visualization of features from backbones trained using pretext pre-training exclusively\nand backbones derived from lower-level backbones obtained after applying BiSSL, respectively.\nFeaturesareextractedfromthetestpartitionoftheflowersdataset. Eachcolorrepresentsadifferent\nclass. DetailsareoutlinedinSectionC.1ofAppendixC\nillustratestheresultsontheflowersdataset,indicatingthatBiSSLyieldsbackboneswithimproved\ndownstreamfeaturealignment. FurtherplotsonaselectionofdownstreamdatasetsinSectionC.1\nreinforcethisfinding,alsodemonstratingthatthistrendpersistsevenfordatasetswhereBiSSLdid\nnotimposeanyclassificationaccuracyimprovements.\n5 CONCLUSION\nThisstudyintegratespretextpre-traininganddownstreamfine-tuningintoaunifiedbileveloptimiza-\ntionproblem,fromwhichtheBiSSLtrainingframeworkisproposed. BiSSLexplicitlymodelsthe\ninheritanceofbackboneparametersfromthepretexttask, enhancingthetransferofrelevantinfor-\nmation between the pretext and downstream tasks. We propose a practical training algorithm and\npipeline that incorporates BiSSL as an intermediate stage between pretext pre-training and down-\nstreamfine-tuning.ExperimentsacrossvariousimageclassificationdatasetsdemonstratethatBiSSL\nconsistentlyachievesimprovedorcomparabledownstreamclassificationperformancerelativetothe\nconventionalself-supervisedlearningpipeline. Additionally,ourfindingsindicatethatininstances\nwhere BiSSL improves performance, this improvement remains consistent regardless of the pre-\ntextpre-trainingduration. FurtheranalysissuggeststhatBiSSLenhancesthedownstreamsemantic\nrichnessoflearnedrepresentations,asevidencedbyqualitativeinspectionsoflatentspaces. BiSSL\nmarks a potential advancement towards enhancing the alignment between the pretext pre-training\nanddownstreamfine-tuningstages,revealinganewdirectionforself-supervisedlearningalgorithm\ndesignsthatleveragebileveloptimization.\n5.1 FUTUREWORK\nFormulatingtheself-supervisedpipelineasabileveloptimizationproblemoffersvariousstrategies\nwithtrade-offsincomputationalcomplexityandtheoreticaljustification. Whilethisstudypresents\na promising approach for improving downstream performance, further investigation of alternative\nformulationsisneededtoidentifysetupsarearepotentiallymoreoptimal. AlthoughBiSSListheo-\nreticallyapplicabletoanydownstreamtaskandmodelsize,ourexperimentsfocusedonsmall-scale\nimage classification due to resource constraints. Therefore, it remains uncertain whether BiSSL\ncanscaletolargersetupsandtasks. Additionally,apotentialfutureadvancementwouldintegrating\nmorenovelmethodsforsolvingBLOproblems, whichpromisebenefitsintermsofreducedcom-\nputational costs and improved solution convergence (Zhang et al., 2023a; Yang et al., 2021; Choe\netal.,2023;Huang,2024). Lastly,thecurrentBiSSLframeworkreliesonfullaccesstopre-training\ndataandpretexttasks. Futureresearchcouldinvestigatetheuseofonlyasubsetofpre-trainingdata\nandalternativepretexttaskstomaintainBiSSL’sbenefitsundertheseconditions.\n10\nPreprint. Underreview.\nACKNOWLEDGMENTS\nThisprojectissupportedbythePioneerCentreforArtificialIntelligence, Denmark.3 Theauthors\nwouldliketothankSijiaLiuandYihuaZhang(MichiganStateUniversity)providingvaluablefeed-\nbackinadiscussion,whichhelpedtorefineandsolidifyourperspectiveonthetopicofintegrating\nbileveloptimizationindeeplearning.\nREFERENCES\nAbien Fred Agarap. Deep learning using rectified linear units (relu). arXiv preprint\narXiv:1803.08375,2018.\nMart´ınArjovsky,Le´onBottou,IshaanGulrajani,andDavidLopez-Paz.Invariantriskminimization.\nArXiv,abs/1907.02893,2019.\nAlexeiBaevski, YuhaoZhou, AbdelrahmanMohamed, andMichaelAuli. wav2vec2.0: Aframe-\nwork for self-supervised learning of speech representations. Advances in neural information\nprocessingsystems,33:12449–12460,2020.\nAdrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regular-\nization for self-supervised learning. In International Conference on Learning Representations,\n2022.\nHarshitaBoonlia,TanmoyDam,MdMeftahulFerdaus,SreenathaGAnavatti,andAnkanMullick.\nImproving self-supervised learning for out-of-distribution task via auxiliary classifier. In 2022\nIEEEInternationalConferenceonImageProcessing(ICIP),pp.3036–3040.IEEE,2022.\nZala´nBorsos,MojmirMutny,andAndreasKrause. Coresetsviabileveloptimizationforcontinual\nlearning and streaming. Advances in neural information processing systems, 33:14879–14890,\n2020.\nMatteo Boschini, Lorenzo Bonicelli, Angelo Porrello, Giovanni Bellitto, Matteo Pennisi, Simone\nPalazzo,ConcettoSpampinato,andSimoneCalderara. Transferwithoutforgetting. InEuropean\nConferenceonComputerVision,pp.692–709.Springer,2022.\nLukasBossard,MatthieuGuillaumin,andLucVanGool. Food-101–miningdiscriminativecom-\nponentswithrandomforests. InEuropeanConferenceonComputerVision,2014.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,\nJeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,\nBenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,\nand Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,\nR.Hadsell,M.F.Balcan,andH.Lin(eds.),AdvancesinNeuralInformationProcessingSystems,\nvolume33,pp.1877–1901.CurranAssociates,Inc.,2020.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervisedlearningofvisualfeaturesbycontrastingclusterassignments. Advancesinneural\ninformationprocessingsystems,33:9912–9924,2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herve´ Jegou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin. Emerging properties in self-supervised vision transformers. In 2021\nIEEE/CVF International Conference on Computer Vision (ICCV), pp. 9630–9640, 2021. doi:\n10.1109/ICCV48922.2021.00951.\nHeng-Jui Chang, Alexander H. Liu, and James Glass. Self-supervised Fine-tuning for Improved\nContent Representations by Speaker-invariant Clustering. In INTERSPEECH 2023, pp. 2983–\n2987.ISCA,August2023. doi: 10.21437/Interspeech.2023-847.\n3https:\\www.aicentre.dk\n11\nPreprint. Underreview.\nCan(Sam)Chen,JingboZhou,FanWang,XueLiu,andDejingDou. Structure-awareproteinself-\nsupervised learning. Bioinformatics, 39(4):btad189, 04 2023. ISSN 1367-4811. doi: 10.1093/\nbioinformatics/btad189.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. Recall and\nlearn: Fine-tuning deep pretrained language models with less forgetting. In Bonnie Webber,\nTrevorCohn,YulanHe,andYangLiu(eds.),EMNLP(1),pp.7870–7881.AssociationforCom-\nputationalLinguistics,2020a. ISBN978-1-952148-60-6.\nTingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. Asimpleframeworkfor\ncontrastivelearningofvisualrepresentations. InInternationalconferenceonmachinelearning,\npp.1597–1607.PMLR,2020b.\nSangKeunChoe,WillieNeiswanger,PengtaoXie,andEricXing. Betty: Anautomaticdifferenti-\nationlibraryformultileveloptimization. InTheEleventhInternationalConferenceonLearning\nRepresentations,2023. URLhttps://openreview.net/forum?id=LV_MeMS38Q9.\nYu-An Chung and James Glass. Speech2Vec: A Sequence-to-Sequence Framework for Learning\nWordEmbeddingsfromSpeech. InProc.Interspeech2018,pp.811–815,2018. doi: 10.21437/\nInterspeech.2018-2341.\nYu-AnChung,Wei-NingHsu,HaoTang,andJamesGlass. AnUnsupervisedAutoregressiveModel\nfor Speech Representation Learning. In Proc. Interspeech 2019, pp. 146–150, 2019. doi: 10.\n21437/Interspeech.2019-1473.\nMatthewC.Cieslak,AnnM.Castelfranco,VittoriaRoncalli,PetraH.Lenz,andDanielK.Hartline.\nt-distributedstochasticneighborembedding(t-sne): Atoolforeco-physiologicaltranscriptomic\nanalysis. MarineGenomics,51:100723,2020. ISSN1874-7787. doi: https://doi.org/10.1016/j.\nmargen.2019.100723.\nM.Cimpoi,S.Maji,I.Kokkinos,S.Mohamed,,andA.Vedaldi. Describingtexturesinthewild. In\nProceedingsoftheIEEEConf.onComputerVisionandPatternRecognition(CVPR),2014.\nAdamCoates,AndrewNg,andHonglakLee. Ananalysisofsingle-layernetworksinunsupervised\nfeature learning. In Geoffrey Gordon, David Dunson, and Miroslav Dud´ık (eds.), Proceedings\noftheFourteenthInternationalConferenceonArtificialIntelligenceandStatistics,volume15of\nProceedingsofMachineLearningResearch,pp.215–223,FortLauderdale,FL,USA,11–13Apr\n2011.PMLR.\nJiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehier-\narchicalimagedatabase. In2009IEEEConferenceonComputerVisionandPatternRecognition,\npp.248–255,2009. doi: 10.1109/CVPR.2009.5206848.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectionaltransformersforlanguageunderstanding. InNorthAmericanChapteroftheAssoci-\nationforComputationalLinguistics,2019.\nJeffDonahue,YangqingJia,OriolVinyals,JudyHoffman,NingZhang,EricTzeng,andTrevorDar-\nrell.Decaf:Adeepconvolutionalactivationfeatureforgenericvisualrecognition.InEricP.Xing\nandTonyJebara(eds.),Proceedingsofthe31stInternationalConferenceonMachineLearning,\nvolume 32 of Proceedings of Machine Learning Research, pp. 647–655, Bejing, China, 22–24\nJun2014.PMLR.\nA.L. Dontchev and R.T. Rockafellar. Implicit Functions and Solution Mappings: A View from\nVariationalAnalysis.SpringerSeriesinOperationsResearchandFinancialEngineering.Springer\nNewYork,2014. ISBN9781493910373.\nYann Dubois, Tatsunori Hashimoto, Stefano Ermon, and Percy Liang. Improving self-supervised\nlearningbycharacterizingidealizedrepresentations. ArXiv,abs/2209.06235,2022.\nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The\nPASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascal-\nnetwork.org/challenges/VOC/voc2007/workshop/index.html.\n12\nPreprint. Underreview.\nChen Fan, Parikshit Ram, and Sijia Liu. Sign-MAML: Efficient model-agnostic meta-learning by\nsignSGD. In Fifth Workshop on Meta-Learning at the Conference on Neural Information Pro-\ncessingSystems,2021.\nChelsea Finn, P. Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeepnetworks. InInternationalConferenceonMachineLearning,2017.\nLucaFranceschi,PaoloFrasconi,SaverioSalzo,RiccardoGrazzi,andMassimilianoPontil. Bilevel\nprogrammingforhyperparameteroptimizationandmeta-learning. InInternationalconferenceon\nmachinelearning,pp.1568–1577.PMLR,2018.\nJean-Bastien Grill, Florian Strub, Florent Altche´, Corentin Tallec, Pierre Richemond, Elena\nBuchatskaya, CarlDoersch,BernardoAvilaPires,ZhaohanGuo,MohammadGheshlaghiAzar,\netal. Bootstrapyourownlatent-anewapproachtoself-supervisedlearning. Advancesinneural\ninformationprocessingsystems,33:21271–21284,2020.\nKartikGupta,ThalaiyasingamAjanthan,AntonvandenHengel,andStephenGould.Understanding\nand improving the role of projection head in self-supervised learning. ArXiv, abs/2212.11491,\n2022.\nKaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778,2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on\ncomputervisionandpatternrecognition,pp.9729–9738,2020.\nKaimingHe, XinleiChen, SainingXie, YanghaoLi, PiotrDolla´r, andRossGirshick. Maskedau-\ntoencodersarescalablevisionlearners. InProceedingsoftheIEEE/CVFconferenceoncomputer\nvisionandpatternrecognition,pp.16000–16009,2022.\nPengchengHe,XiaodongLiu,JianfengGao,andWeizhuChen. Deberta: Decoding-enhancedbert\nwithdisentangledattention. InInternationalConferenceonLearningRepresentations,2021.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\nandAbdelrahmanMohamed. Hubert: Self-supervisedspeechrepresentationlearningbymasked\npredictionofhiddenunits.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,\n29:3451–3460,2021.\nFeihu Huang. Optimal Hessian/Jacobian-free nonconvex-PL bilevel optimization. In Ruslan\nSalakhutdinov,ZicoKolter,KatherineHeller,AdrianWeller,NuriaOliver,JonathanScarlett,and\nFelix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learn-\ning,volume235ofProceedingsofMachineLearningResearch,pp.19598–19621.PMLR,21–27\nJul2024.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducinginternalcovariateshift. InInternationalconferenceonmachinelearning,pp.448–456.\npmlr,2015.\nAlexanderKirillov, EricMintun, NikhilaRavi, HanziMao, ChloeRolland, LauraGustafson, Tete\nXiao,SpencerWhitehead,AlexanderC.Berg,Wan-YenLo,PiotrDolla´r,andRossB.Girshick.\nSegment anything. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp.\n3992–4003,2023.\nAlexKrizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. UniversityofToronto,05\n2012.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdel rahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation, and comprehension. In Annual Meeting\noftheAssociationforComputationalLinguistics,2019.\n13\nPreprint. Underreview.\nFei-FeiLi,MarcoAndreeto,Marc’AurelioRanzato,andPietroPerona. Caltech101,Apr2022a.\nJiangmeng Li, Wenwen Qiang, Yanan Zhang, Wenyi Mo, Changwen Zheng, Bing Su, and Hui\nXiong. Metamask: Revisitingdimensionalconfounderforself-supervisedlearning. Advancesin\nNeuralInformationProcessingSystems,35:38501–38515,2022b.\nXingjianLi,HaoyiXiong,HaozheAn,Cheng-ZhongXu,andDejingDou. Rifle: Backpropagation\nin depth for deep transfer learning through re-initializing the fully-connected layer. In Interna-\ntionalConferenceonMachineLearning,pp.6010–6019.PMLR,2020.\nIlyaLoshchilovandFrankHutter. SGDR:Stochasticgradientdescentwithwarmrestarts. InInter-\nnationalConferenceonLearningRepresentations,2017.\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of\naircraft. Technicalreport,2013.\nJ.L.Nazareth. Conjugategradientmethod. WIREsComputationalStatistics,1(3):348–353,2009.\ndoi: https://doi.org/10.1002/wics.13.\nDaisukeNiizumi,DaikiTakeuchi,YasunoriOhishi,NoboruHarada,andKunioKashino. Byolfor\naudio: Self-supervisedlearningforgeneral-purposeaudiorepresentation. In2021International\nJointConferenceonNeuralNetworks(IJCNN),pp.1–8.IEEE,2021.\nMaria-ElenaNilsbackandAndrewZisserman. Automatedflowerclassificationoveralargenumber\nofclasses.InIndianConferenceonComputerVision,GraphicsandImageProcessing,Dec2008.\nMaxime Oquab, Timothe´e Darcet, The´o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khali-\ndov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran,\nNicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra,\nMichael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick\nLabatut, ArmandJoulin, andPiotrBojanowski. DINOv2: Learningrobustvisualfeatureswith-\nout supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL\nhttps://openreview.net/forum?id=a68SUt6zFt.\nHaolinPan, YongGuo, QinyiDeng, Hao-FanYang, YiqunChen, andJianChen. Improvingfine-\ntuning of self-supervised models with contrastive initialization. Neural networks : the official\njournaloftheInternationalNeuralNetworkSociety,159:198–207,2022.\nOmkarM.Parkhi,AndreaVedaldi,AndrewZisserman,andC.V.Jawahar. Catsanddogs. InIEEE\nConferenceonComputerVisionandPatternRecognition,2012.\nFabianPedregosa. Hyperparameteroptimizationwithapproximategradient. InMariaFlorinaBal-\ncanandKilianQ.Weinberger(eds.),ProceedingsofThe33rdInternationalConferenceonMa-\nchine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 737–746, New\nYork,NewYork,USA,20–22Jun2016.PMLR.\nAravindRajeswaran,ChelseaFinn,ShamM.Kakade,andSergeyLevine. Meta-learningwithim-\nplicitgradients. InNeuralInformationProcessingSystems,2019.\nSteffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised\nPre-Training for Speech Recognition. In Proc. Interspeech 2019, pp. 3465–3469, 2019. doi:\n10.21437/Interspeech.2019-1873.\nJonathan R Shewchuk. An introduction to the conjugate gradient method without the agonizing\npain. Technicalreport,CarnegieMellonUniversity,1994.\nKarenSimonyanandAndrewZisserman. Verydeepconvolutionalnetworksforlarge-scaleimage\nrecognition. arXivpreprintarXiv:1409.1556,2014.\nSai Ashish Somayajula, Lifeng Jin, Linfeng Song, Haitao Mi, and Dong Yu. Bi-level finetuning\nwithtask-dependentsimilaritystructureforlow-resourcetraining. InAnnaRogers,JordanBoyd-\nGraber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics:\nACL2023,pp.8569–8588,Toronto,Canada,July2023.AssociationforComputationalLinguis-\ntics. doi: 10.18653/v1/2023.findings-acl.544.\n14\nPreprint. Underreview.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe´e\nLacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.\nC. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011\ndataset. TechnicalReportCNS-TR-2011-001,CaliforniaInstituteofTechnology,2011.\nChuhanWu,FangzhaoWu,TaoQi,YongfengHuang,andXingXie. Noisytune: Alittlenoisecan\nhelp you finetune pretrained language models better. In Annual Meeting of the Association for\nComputationalLinguistics,2022.\nHanXiao,KashifRasul,andRolandVollgraf. Fashion-mnist:anovelimagedatasetforbenchmark-\ningmachinelearningalgorithms. arXivpreprintarXiv:1708.07747,2017.\nJianxiongXiao,JamesHays,KristaA.Ehinger,AudeOliva,andAntonioTorralba. Sundatabase:\nLarge-scale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference\nonComputerVisionandPatternRecognition, pp.3485–3492, 2010. doi: 10.1109/CVPR.2010.\n5539970.\nSarthakYadav,SergiosTheodoridis,LarsKaiHansen,andZheng-HuaTan. Maskedautoencoders\nwithmulti-windowlocal-globalattentionarebetteraudiolearners. InTheTwelfthInternational\nConferenceonLearningRepresentations,2024.\nJunjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimization. In\nA. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural In-\nformationProcessingSystems,2021.\nLinjie Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang. A large-scale car dataset for fine-\ngrainedcategorizationandverification.In2015IEEEConferenceonComputerVisionandPattern\nRecognition(CVPR),pp.3973–3981,2015. doi: 10.1109/CVPR.2015.7299023.\nYangYou,IgorGitman,andBorisGinsburg.Largebatchtrainingofconvolutionalnetworks.arXiv:\nComputerVisionandPatternRecognition,2017.\nSalahZaiem,TitouanParcollet,andSlimEssid. Lessforgettingforbettergeneralization: Exploring\ncontinual-learningfine-tuningmethodsforspeechself-supervisedrepresentations. arXivpreprint\narXiv:2407.00756,2024.\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l: Self-supervised semi-\nsupervisedlearning. 2019IEEE/CVFInternationalConferenceonComputerVision(ICCV),pp.\n1476–1485,2019.\nLiZhang,YouweiLiang,RuiyiZhang,AmirhoseinJavadi,andPengtaoXie. BLO-SAM:Bi-level\noptimizationbasedfinetuningofthesegmentanythingmodelforoverfitting-preventingsemantic\nsegmentation. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria\nOliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International\nConference on Machine Learning, volume 235 of Proceedings of Machine Learning Research,\npp.59289–59309.PMLR,21–27Jul2024.\nTong Zhang, Congpei Qiu, Wei Ke, Sabine Su¨sstrunk, and Mathieu Salzmann. Leverage your\nlocalandglobalrepresentations: Anewself-supervisedlearningstrategy. InProceedingsofthe\nIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.16580–16589,2022a.\nYihua Zhang, Guanhua Zhang, Prashant Khanduri, Min-Fong Hong, Shiyu Chang, and Sijia Liu.\nRevisiting and advancing fast adversarial training through the lens of bi-level optimization. In\nInternationalConferenceonMachineLearning,2021.\nYihuaZhang,YuguangYao,ParikshitRam,PuZhao,TianlongChen,MingyiHong,YanzhiWang,\nand Sijia Liu. Advancing model pruning via bi-level optimization. In S. Koyejo, S. Mohamed,\nA.Agarwal,D.Belgrave,K.Cho,andA.Oh(eds.),AdvancesinNeuralInformationProcessing\nSystems,volume35,pp.18309–18326.CurranAssociates,Inc.,2022b.\n15\nPreprint. Underreview.\nYihua Zhang, Prashant Khanduri, Ioannis C. Tsaknakis, Yuguang Yao, Min-Fong Hong, and Sijia\nLiu. Anintroductiontobi-leveloptimization: Foundationsandapplicationsinsignalprocessing\nandmachinelearning. ArXiv,abs/2308.00788,2023a.\nYihuaZhang,PranaySharma,ParikshitRam,MingyiHong,KushR.Varshney,andSijiaLiu. What\nis missing in IRM training and evaluation? challenges and solutions. In The Eleventh Interna-\ntionalConferenceonLearningRepresentations,2023b.\nNicolasZucchetandJoa˜oSacramento. Beyondbackpropagation: bileveloptimizationthroughim-\nplicitdifferentiationandequilibriumpropagation.NeuralComputation,34(12):2309–2346,2022.\nA THEORETICAL INSIGHTS AND FRAMEWORK COMPARISONS IN BISSL\nA.1 DERIVATIONOFTHEIMPLICITGRADIENT\nAssume the setup of the BiSSL optimization problem described in equation 2 and equation 3. In\nthefollowingderivations,wewillassumethatϕ isfixed,allowingustosimplifytheexpressions\nP\ninvolved. To streamline the notation further, we continue to use the convention ∇ h(ξ)| :=\nξ ξ=ψ\n∇ h(ψ), when it is clear from context which variables are differentiated with respect to. Under\nξ\nthesecircumstances,wethendefinethelower-levelobjectivefromequation3as\nG(θ ,θ ):=LP(θ ,ϕ )+λr(θ ,θ ). (8)\nD P P P D P\nRecalling that r is a convex regularization objective, adequate scaling of λ effectively “convexi-\nfies”thelower-levelobjectiveG,astrategyalsoemployedonthelower-levelobjectiveinprevious\nworks(Rajeswaranetal.,2019;Zhangetal.,2022b;2023a).Thisisadvantageousbecauseassuming\nconvexityofGensuresthatforanyθ ∈ RL,thereexistsacorrespondingθˆ ∈ RL thatsatisfies\nD P\nthe stationary condition ∇ G(θ ,θˆ ) = 0. In other words, we are assured that a minimizer of\nG(θ ,·)existsforallθ\n∈θPRL.ND ow,P\nfurtherassumethat∇ G(θ ,θ )iscontinuouslydifferen-\nD D θP D P\ntiableandthattheHessianmatrix∇2 G(θ ,θˆ )isinvertibleforallθ ∈RL. Underthesecondi-\nθP D P D\ntions,theimplicitfunctiontheorem(Dontchev&Rockafellar,2014;Zucchet&Sacramento,2022)\nguaranteestheexistenceofanimplicituniqueanddifferentiablefunctionθ∗ :N(θ )→RL,with\nP D\nN(θ )beinganeighborhoodofθ ,thatsatisfiesθ∗(θ )=θˆ and∇ G(θ˜ ,θ∗(θ˜ ))=0for\nD D P D P θP D P D\nallθ˜ ∈N(θ ).\nD D\nAsthelower-levelsolutionθ∗(θ )isindeedadifferentiablefunctionundertheseconditions,this\nP D\njustifiesthattheexpression\nd\n∇ [G(θ ,θ∗(θ ))]=0\ndθ θP D P D\nD\nisvalidforallθ ∈RL. Byapplyingthechainrule,theexpressionbecomes\nD\ndθ∗(θ )T\n∇2 G(θ ,θ∗(θ ))+ P D ∇2 G(θ ,θ∗(θ ))=0.\nθDθP D P D dθ θP D P D\nD\nRecallingthat∇2 θPG(θ D,θ P∗(θ D))isassumedtobeinvertible,theIG dθ P∗ dθ( DθD)T canbeisolated\ndθ P∗(θ D)T =−∇2 G(θ ,θ∗(θ ))(cid:2) ∇2 G(θ ,θ∗(θ ))(cid:3)−1 ,\ndθ θDθP D P D θP D P D\nD\nandbysubstitutingtheexpressionforGfromequation8,theexpressionbecomes\ndθ∗(θ )T (cid:20) (cid:18) 1 (cid:19)(cid:21)−1\nP D =−∇2 r(θ ,θ∗(θ )) ∇2 LP(θ∗(θ ),ϕ )+r(θ ,θ∗(θ )) . (9)\ndθ θDθP D P D θP λ P D P D P D\nD\nTosummarize,giventhefollowingassumptions:\n• Thelower-levelpretextprojectionheadparametersϕ arefixed.\nP\n• Gisconvexsuchthat∇ G(θ ,θ∗(θ ))=0isfulfilledforeveryθ ∈RL.\nθP D P D D\n16\nPreprint. Underreview.\n• TheHessianmatrix∇2 G(θ ,θ∗(θ ))existsandisinvertibleforallθ ∈RL.\nθP D P D D\nThen, the IG dθ P∗(θD)T can be explicitly expressed by equation 9. The authors acknowledge that\ndθD\nan explicit expression for the IG without fixing ϕ is achievable, though this is left for future\nP\nexploration.\nA.2 DISTINCTIONFROMBILEVELOPTIMIZATIONINMETA-LEARNING\nWhile bilevel optimization (BLO) has been applied in meta-learning frameworks such as\nMAML(Finnetal.,2017),Sign-MAML(Fanetal.,2021)andiMAML(Rajeswaranetal.,2019),\nBiSSLrepresentsadistinctapplicationandimplementationofBLO,tailoredforthechallengesof\nself-supervisedlearning(SSL).Intheaforementionedworks, BLOisprimarilyutilizedtoaddress\nfew-shotlearningscenarios,focusingonefficientlyadaptingmodelstonewtaskswithminimaldata.\nConversely,BiSSLappliesBLOtoconcurrentlymanagethemorecomplextaskofself-supervised\npretextpre-trainingwithdownstreamfine-tuning. Anotherkeydistinctionisthatinmeta-learning,\ntheupper-andlower-levelobjectivesarecloselyrelated,withtheupper-levelobjectiveformulated\nas a summation of the lower-level tasks. In contrast, BiSSL involves fundamentally distinct ob-\njectives at each level, utilizing separate datasets and tasks for pre-training and fine-tuning. This\ndesignallowsBiSSLtobetteralignthepre-trainedmodelwiththerequirementsofaspecificdown-\nstreamtask.Conversely,theBLOinmeta-learningaimstobroadlygeneralizeacrossawiderangeof\ntasks,prioritizingadaptabilityratherthantask-specificoptimization.Additionally,unlikeBiSSL,the\nmeta-learningframeworksdiscussedreinitializethelower-levelbackboneparameterswithacopyof\nthe upper-level parameters at every iteration. In BiSSL, the closest comparable mechanism is the\noccasionalupdateoftheupper-levelbackboneusinganEMAupdatewiththelower-levelparame-\nters(seeAlgorithm1), thoughthisoccursfarlessfrequently. Lastly, whilethemeta-learningwith\nBLO approaches often target smaller models and datasets, BiSSL is aimed for larger-scale setups\ncharacteristicofmodernSSLworkflows.\nB EXPERIMENTAL DETAILS\nB.1 DATASETPARTITIONS\nThe Caltech-101 (Li et al., 2022a) dataset does not come with a pre-defined train/test split, so the\nsameconventionaspreviousworksisfollowed(Chenetal.,2020b;Donahueetal.,2014;Simonyan\n&Zisserman,2014), where30randomimagesperclassareselectedforthetrainingpartition, and\nthe remaining images are assigned for the test partition. For the DTD (Cimpoi et al., 2014) and\nSUN397 (Xiao et al., 2010) datasets, which offer multiple proposed train/test partitions, the first\nsplitsareused,consistentwiththeapproachinChenetal.(2020b).\nFordownstreamhyperparameteroptimization,portionsofthetrainingpartitionsfromeachrespec-\ntive labeled dataset are designated as validation datasets. The FGVC Aircraft (Maji et al., 2013),\nOxford102Flowers(Nilsback&Zisserman,2008),DTD,andPascalVOC2007(Everinghametal.)\ndatasetsalreadyhavedesignatedvalidationpartitions.Foralltheremaininglabeleddatasets,theval-\nidationdatapartitionsarerandomlysampledwhileensuringthatclassproportionsaremaintained.\nForthemulti-attributeVOC07dataset,samplingisperformedwithclassbalanceconcerningthefirst\nattributepresentineachimage. Roughly20%ofthetrainingdataisallocatedforvalidation.\nB.2 DOWNSTREAMTASKFINE-TUNINGOFTHEBASELINESETUP\nInTable2,thelearningratesandweightdecaysusedforeachrespectivedownstreamdatasetofthe\nexperimentsdescribedinSection4.2.1areoutlined.\nB.3 DOWNSTREAMHEADWARMUPANDUPPER-LEVELOFBISSL\nTable 3 outlines the learning rates and weight decays used for the downstream head warm-up and\nupper-levelofBiSSLofeachrespectivedownstreamdataset,asdescribedintheBiSSLexperimental\nsetupofSection4.2.2.\n17\nPreprint. Underreview.\nTable2:Hyper-parameterconfigurationsusedfordownstreamfine-tuningafterconventionalpretext\npre-training yielding the highest top-1 classification accuracies (11-point mAP for the VOC2007\ndataset).\nDataset LearningRate WeightDecay\nSTL10L 0.0136 0.001\nFlowers 0.113 0.00226\nCars 0.035 0.00658\nAircrafts 0.0167 0.00996\nDTD 0.0262 0.00332\nPets 0.0235 0.00472\nFashionMNIST 0.0009 0.00829\nCIFAR10 0.0067 0.00128\nCIFAR100 0.005 0.00127\nCaltech-101 0.0096 0.00902\nFood 0.015 0.00699\nSUN397 0.0097 0.00121\nCUB200 0.0722 0.00568\nVOC2007 0.0108 0.00894\nTable3: Hyper-parametersusedfortheDownstreamHeadWarm-upandUpper-levelofBiSSL.\nDataset LearningRate WeightDecay\nSTL10L 0.015 0.01\nFlowers 0.05 0.01\nCars 0.035 0.007\nAircrafts 0.015 0.01\nDTD 0.015 0.0075\nPets 0.03 0.005\nFashionMNIST 0.05 0.004\nCIFAR10 0.03 0.006\nCIFAR100 0.03 0.001\nCaltech-101 0.03 0.007\nFood 0.015 0.01\nSUN397 0.03 0.002\nCUB200 0.05 0.005\nVOC2007 0.03 0.006\nThe first term of the upper-level gradient equation 7 is approximated using the Conjugate Gradi-\nent(CG)method(Nazareth,2009;Shewchuk,1994). Ourimplementationfollowsasimilarstruc-\nture to that used in Rajeswaran et al. (2019), employing N = 5 iterations and a dampening term\nc\nλ =10.GivenmatrixAandvectorv,theCGmethoditerativelyapproximatesA−1v,whichre-\ndamp\nquiresevaluationofmultiplematrix-vectorproductsAd ,...,Ad . Inpractice,storingthematrix\n1 Nc\nA(inourcase,theHessian∇2 LP(θ∗(θ ),ϕ ))initsfullformistypicallyinfeasible. Instead,\nθP P D P\nafunctionthatefficientlycomputestherequiredmatrix-vectorproductsinsteadofexplicitlystoring\nthe matrix is typically utilized. For our setup, this function is detailed in Algorithm 2, showing\nhowtheK storedlower-levelbatches(weuseK = 5aspreviouslyoutlinedinthe“Lower-levelof\n18\nPreprint. Underreview.\nBiSSL”paragraphinSection4.2.2)areusedtocalculateHessian-vectorproducts.Thisapproachen-\nsuresthattheoutputoftheCGalgorithmisanapproximationoftheinverseHessian-vectorproduct\ninthefirsttermofEquationequation7asintended.\nAlgorithm2HessianVectorProductCalculationf (TouseintheCGAlgorithm)\nH\n1: Input: Input vector v. Model parameters θ P, ϕ P. Training objective LP. Lower-level data\nbatches[z ,...,z ]. Regularizationweightλanddampeningλ .\n1 K damp\n2: Initializey←0 ▷InitializeHessianvectorproducty\n3: fork =1,...,K do\n4: π(θ P)←(cid:0) ∇ θ (cid:12)LP (θ,ϕ P;z k)| θ=θP(cid:1)T v\n5: g←∇ θπ(θ)(cid:12)\nθ=θP\n▷Memoryefficientcalculationof∇2 θLP(θ,ϕ P;z k)| θ=θPx.\n6: y←y+ 1g\nK\n7: y←v+ 1 y\nλ+λdamp\n8: Return: f H(v):=y\nB.4 COMPOSITECONFIGURATIONOFBISSL\nToavoiddatabeingreshuffledbetweeneverytrainingstagealternation,therespectivebatchedlower-\nandupper-leveltrainingdatasetsarestoredinseparatestacksfromwhichdataisdrawn. Thestacks\nare only “reset” when the number of remaining batches is smaller than the number of gradient\nstepsrequiredbeforealternatingtotheotherlevel. Forexample,thelower-levelstackisreshuffled\neveryfourthtrainingstagealternation. Ifthedownstreamdatasetdoesnotprovideenoughdatafor\nmakingN = 8batcheswithnon-overlappingdatapoints,thedataissimplyreshuffledeverytime\nU\ntheremainingnumberofdatapointsissmallerthantheupper-levelbatchsize(256imagesinthese\nexperiments).\nB.5 DOWNSTREAMFINE-TUNINGAFTERBISSL\nThelearningratesandweightdecaysusedfordownstreamfine-tuningafterBiSSLforeachrespec-\ntivedownstreamdatasetareoutlinedinTable4. Section4.2.2outlinestheexperimentalsetup.\nC ADDITIONAL RESULTS\nC.1 VISUALINSPECTIONOFLATENTFEATURES\nTestdatafeaturesofthedownstreamtestdataprocessedbybackbonestrainedthroughconventional\npretextpre-trainingarecomparedagainstthosetrainedwithBiSSL.Thisallowsforaninspectionof\nthelearnedrepresentationspriortothefinalfine-tuningstage.\nDuringtheevaluation,itisimportanttonotethatthebatchnormalizationlayers(Ioffe&Szegedy,\n2015) of the pre-trained backbones utilize the running mean and variance inferred during train-\ning. Since these pre-trained backbones have not been exposed to the downstream datasets during\ntraining,theirbatchnormalizationstatisticsmaynotbeoptimalforthesenewdatasets. Toaddress\nthis, the training dataset is divided into batches of 256 samples, and roughly 100 batches are then\nforward-passedthroughthebackbones. Thisprocedureensuresthatthebatchnormalizationstatis-\nticsarebettersuitedtothedownstreamdatasets,therebyprovidingafairercomparisonofthelearned\nrepresentations.\nForthedimensionalityreductionandvisualizationoftheselatentfeatures,thet-DistributedStochas-\nticNeighborEmbedding(t-SNE)(Cieslaketal.,2020)techniqueisemployed. Thismethodallows\nustovisuallyassesstheclusteringandseparationoffeaturesinthelatentspace,providingqualitative\ninsightsintothesemanticstructureoftherepresentationslearnedthroughBiSSL.\n19\nPreprint. Underreview.\nTable 4: Hyper-parameter configurations used for downstream fine-tuning after BiSSL leading to\nthehighesttop-1classificationaccuracies(11-pointmAPfortheVOC2007dataset).\nDataset LearningRate WeightDecay\nSTL10L 0.0005 0.00043\nFlowers 0.0009 0.00005\nCars 0.0293 0.00851\nAircrafts 0.011 0.00612\nDTD 0.0008 0.00764\nPets 0.0002 0.00543\nFashionMNIST 0.0022 0.00876\nCIFAR10 0.0003 0.00991\nCIFAR100 0.0012 0.00422\nCaltech-101 0.0008 0.00011\nFood 0.006 0.0095\nSUN397 0.0011 0.00028\nCUB200 0.035 0.00868\nVOC2007 0.0002 0.00015\nBiSSL Backbone Features Pretext Backbone Features\nairplane automobile bird cat deer dog frog horse ship truck\nFigure4: CIFAR10\nFigures3to11illustratetheoutcomesofthesevisualinspectionsonaselectionofthedownstream\ndatasets described in Section 4.1, highlighting the differences in feature representations between\nconventionalpretextpre-trainingandBiSSL.\n20\nPreprint. Underreview.\nBiSSL Backbone Features Pretext Backbone Features\nFigure5: CUB200\nBiSSL Backbone Features Pretext Backbone Features\nFigure6: Caltech-101\nBiSSL Backbone Features Pretext Backbone Features\nairplane bird car cat deer dog horse monkey ship truck\nFigure7: STL10L\n21\nPreprint. Underreview.\nBiSSL Backbone Features Pretext Backbone Features\nFigure8: Pets\nBiSSL Backbone Features Pretext Backbone Features\nFigure9: DTD\nBiSSL Backbone Features Pretext Backbone Features\nFigure10: Aircrafts\n22\nPreprint. Underreview.\nBiSSL Backbone Features Pretext Backbone Features\nFigure11: Cars\n23",
    "pdf_filename": "BiSSL_Bilevel_Optimization_for_Self-Supervised_Pre-Training_and_Fine-Tuning.pdf"
}