{
    "title": "T-GAE Transferable Graph Autoencoder for Network Alignment",
    "abstract": "Network alignment is the task of establishing one-to-one correspondences be- tween the nodes of different graphs. Although finding a plethora of applications in high-impact domains, this task is known to be NP-hard in its general form. Existing optimization algorithms do not scale up as the size of the graphs in- creases. While being able to reduce the matching complexity, current GNN approaches fit a deep neural network on each graph and requires re-train on unseen samples, which is time and memory inefficient. To tackle both challenges we propose T-GAE, a transferable graph autoencoder framework that leverages transferability and stability of GNNs to achieve efficient network alignment on out-of-distribution graphs without retraining. We prove that GNN-generated embeddings can achieve more accurate alignment compared to classical spec- tral methods. Our experiments on real-world benchmarks demonstrate that T-GAE outperforms the state-of-the-art optimization method and the best GNN approach by up to 38.7% and 50.8%, respectively, while being able to reduce 90% of the training time when matching out-of-distribution large scale networks. We conduct ablation studies to highlight the effectiveness of the proposed en- coder architecture and training objective in enhancing the expressiveness of GNNs to match perturbed graphs. T-GAE is also proved to be flexible to uti- lize matching algorithms of different complexities. Our code is available at https://github.com/Jason-Tree/T-GAE. 1 Introduction Network alignment, also known as graph matching, is a classical problem in graph theory, that aims to find node correspondence across different graphs and is vital in a number of high-impact domains [Emmert-Streib et al., 2016]. In social networks, for instance, network alignment has been used for user deanonymization [Nilizadeh et al., 2014] and analysis [Ogaard et al., 2013], while in bioinformatics it is a key tool to identify functionalities in protein complexes [Singh et al., 2008], or to identify gene–drug modules [Chen et al., 2018a]. Graph matching also finds application in computer vision [Conte et al., 2003], sociology [Racz and Sridhar, 2021], to name a few. However, this problem is usually cast as a quadratic assignment problem (QAP), which is in general NP-hard. Various approaches have been developed to tackle network alignment and can be divided into two main categories; i) optimization algorithms that attempt to approximate the QAP problem by relaxing the combinatorial constraints, ii) embedding methods that approach the problem by implicitly or explicitly generating powerful node embeddings that facilitate the alignment task. Optimization approaches, as [Anstreicher and Brixius, 2001, Vogelstein et al., 2015] employ quadratic programming relaxations, while [Klau, 2009] and [Peng et al., 2010] utilize semidefinite or Lagrangian-based relaxations respectively, [Du et al., 2019] and [Du et al., 2022] proposed to solve network alignment together with link prediction. Successive convex approximations were also proposed by [Konar and Sidiropoulos, 2020] to handle the QAP. Challenges associated with these methods include high computational cost, infeasible solutions, or nearly optimal initialization requirements. Embedding methods, on the other hand, overcome these challenges, but they usually produce inferior solutions, due to an inherent J. He et al., T-GAE: Transferable Graph Autoencoder for Network Alignment. Proceedings of the Third Learning on Graphs Conference (LoG 2024), PMLR 269, Virtual Event, November 26–29, 2024. arXiv:2310.03272v4  [cs.LG]  18 Nov 2024",
    "body": "T-GAE: Transferable Graph Autoencoder for Network\nAlignment\nJiashu He\nUniversity of Pennsylvania\njiashuhe@seas.upenn.edu\nCharilaos Kanatsoulis\nStanford University\ncharilaos@cs.stanford.edu\nAlejandro Ribeiro\nUniversity of Pennsylvania\naribeiro@seas.upenn.edu\nAbstract\nNetwork alignment is the task of establishing one-to-one correspondences be-\ntween the nodes of different graphs. Although finding a plethora of applications\nin high-impact domains, this task is known to be NP-hard in its general form.\nExisting optimization algorithms do not scale up as the size of the graphs in-\ncreases. While being able to reduce the matching complexity, current GNN\napproaches fit a deep neural network on each graph and requires re-train on\nunseen samples, which is time and memory inefficient. To tackle both challenges\nwe propose T-GAE, a transferable graph autoencoder framework that leverages\ntransferability and stability of GNNs to achieve efficient network alignment on\nout-of-distribution graphs without retraining. We prove that GNN-generated\nembeddings can achieve more accurate alignment compared to classical spec-\ntral methods. Our experiments on real-world benchmarks demonstrate that\nT-GAE outperforms the state-of-the-art optimization method and the best GNN\napproach by up to 38.7% and 50.8%, respectively, while being able to reduce\n90% of the training time when matching out-of-distribution large scale networks.\nWe conduct ablation studies to highlight the effectiveness of the proposed en-\ncoder architecture and training objective in enhancing the expressiveness of\nGNNs to match perturbed graphs. T-GAE is also proved to be flexible to uti-\nlize matching algorithms of different complexities. Our code is available at\nhttps://github.com/Jason-Tree/T-GAE.\n1\nIntroduction\nNetwork alignment, also known as graph matching, is a classical problem in graph theory, that\naims to find node correspondence across different graphs and is vital in a number of high-impact\ndomains [Emmert-Streib et al., 2016]. In social networks, for instance, network alignment has been\nused for user deanonymization [Nilizadeh et al., 2014] and analysis [Ogaard et al., 2013], while in\nbioinformatics it is a key tool to identify functionalities in protein complexes [Singh et al., 2008],\nor to identify gene–drug modules [Chen et al., 2018a]. Graph matching also finds application in\ncomputer vision [Conte et al., 2003], sociology [Racz and Sridhar, 2021], to name a few. However,\nthis problem is usually cast as a quadratic assignment problem (QAP), which is in general NP-hard.\nVarious approaches have been developed to tackle network alignment and can be divided into two main\ncategories; i) optimization algorithms that attempt to approximate the QAP problem by relaxing the\ncombinatorial constraints, ii) embedding methods that approach the problem by implicitly or explicitly\ngenerating powerful node embeddings that facilitate the alignment task. Optimization approaches, as\n[Anstreicher and Brixius, 2001, Vogelstein et al., 2015] employ quadratic programming relaxations,\nwhile [Klau, 2009] and [Peng et al., 2010] utilize semidefinite or Lagrangian-based relaxations\nrespectively, [Du et al., 2019] and [Du et al., 2022] proposed to solve network alignment together with\nlink prediction. Successive convex approximations were also proposed by [Konar and Sidiropoulos,\n2020] to handle the QAP. Challenges associated with these methods include high computational\ncost, infeasible solutions, or nearly optimal initialization requirements. Embedding methods, on the\nother hand, overcome these challenges, but they usually produce inferior solutions, due to an inherent\nJ. He et al., T-GAE: Transferable Graph Autoencoder for Network Alignment. Proceedings of the Third Learning\non Graphs Conference (LoG 2024), PMLR 269, Virtual Event, November 26–29, 2024.\narXiv:2310.03272v4  [cs.LG]  18 Nov 2024\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nFigure 1: To enhance the expressiveness of GNNs to align the unseen graphs, our proposed encoder\nprocesses the input features by a local MLP, we then incorporate attention mechanism on (1) input to\neach message-passing layer by attending to the output of the last layer and processed input feature.\n(2) output of the encoder by attending to the output of each message passing layer.\ntrade-off between embedding permutation-equivariance and the ability to capture the structural\ninformation of the graph. Typical embedding techniques include spectral and factorization methods\n[Umeyama, 1988, Feizi et al., 2019, Zhang and Tong, 2016, Kanatsoulis and Sidiropoulos, 2022],\nstructural feature engineering methods [Berlingerio et al., 2013, Heimann et al., 2018], and random\nwalk approaches [Perozzi et al., 2014, Grover and Leskovec, 2016a]. Recently [Chen et al., 2020,\nKarakasis et al., 2021] have proposed joint node embedding and network alignment, to overcome\nthese challenges, but these methods do not scale up as the size of the graph increases.\nGraph Neural Networks (GNNs) are powerful architectures that learn graph representations (em-\nbeddings) in a self-supervised way [Kipf and Welling, 2016, You et al., 2020]. They have shown\nstate-of-the-art performance in several tasks, including biomedical [Gainza et al., 2020, Strokach\net al., 2020, Jiang et al., 2021, Hu et al., 2023, Li et al., 2024], quantum chemistry [Gilmer et al.,\n2017], social networks and recommender systems [Ying et al., 2018, Wu et al., 2020, Liu et al.,\n2023a, Yang et al., 2024]. A line of studies has been conducted to formulate key-point matching on\nimages as graph matching problems [Fey et al., 2020, Yu et al., 2020]. These frameworks rely on a\npowerful domain-specific encoder (CNNs, for example) to provide high-quality features. Given these\nhigh-quality features, GNNs are able to match graphs without training [Liu et al., 2022]. However,\nexpressive features are expensive and sometimes infeasible to build [Zhang et al., 2020, Zhou et al.,\n2021]. [Liang et al., 2021] uses another trainable matrix to parameterize the internal connectivity\nbetween nodes of different graphs for faithful node alignment. Recently, [Gao et al., 2021] proposed\nuse GNNs to learn node embedding, and match nodes with small Wasserstein distances. However,\nthis method needs to fit a GNN on every input graph, which results in very high training cost, since\ntraining deep GNNs with large sizes graphs is computationally prohibitive, as GNNs have limited\nscalability with respect to graph and model sizes [Chen et al., 2018b,c, Chiang et al., 2019, Zeng\net al., 2020].\nTo address these challenges, we propose T-GAE, a novel self-supervised GNN framework to perform\nnetwork alignment. Specifically, we propose to utilize the transferability and robuseness of GNN to\nproduce permutation equivariant and highly expressive embeddings. T-GAE trains the encoder on\nmultiple families of small graphs and produce expressive/permutation equivariant representations for\nlarger unseen networks. We further prove that GNN representations combine the eigenvectors of the\ngraph in a nonlinear fashion and are at least as good in network alignment as certain spectral methods.\nT-GAE is a one-shot solution that tackles the challenges of real-time network alignment from (1)\nOptimization based algorithms: high computational cost, assume ground truth node correspondence\nas initialization. (2) Deep-learning based frameworks: re-train for every pair of graphs, rely on\nhigh quality of node features. Extensive experiments with real-world benchmarks demonstrate the\neffectiveness and efficiency of the proposed approach in both graph and sub-graph matching, thereby\nsheds light on the potential of GNN to tackle the highly-complex network optimization problems.\n2\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nFigure 2: Proposed training objective: For each of the training graph, we generate a number of\naugmented samples by randomly adding or removing edges. The node embedding of these augmented\nsamples are decoded non-parametrically, and compared with the corresponding original graph to train\nthe T-GAE encoder.\n2\nPreliminaries\nGraphs are represented by G := (V, E), where V = {1, . . . , N} is the set of vertices (nodes) and\nE = {(v, u)} correspond to edges between pairs of vertices. A graph is represented in a matrix form\nby a graph operator S ∈RN×N, where S(i, j) quantifies the relation between node i and node j\nand N = |V| is the total number of vertices. In this work, we use the normalized graph adjacency\nand study the most general form of network alignment where there is no given graph attributes and\nground truth node correspondence(anchor links).\n2.1\nNetwork Alignment\nDefinition 2.1 (Network Alignment). Given a pair of graphs G := (V, E), ˆG := (ˆV, ˆE), with graph\nadjacencies S, ˆS, network alignment aims to find a bijection g : V →ˆV which minimizes the\nnumber of edge disagreements between the two graphs. Formally, the problem can be written as:\nmin\nP ∈P\n\r\r\r S −P ˆSP T \r\r\r\n2\nF ,\n(1)\nwhere P is the set of permutation matrices.\nAs mentioned in the introduction, network alignment, is equivalent to the QAP, which has been\nproven to be NP-hard [Koopmans and Beckmann, 1957].\n2.2\nSpectral Decomposition of the Graph\nA popular approach to tackle network alignment is by learning powerful node embeddings associated\nwith connectivity information in the graph. Network alignment can be achieved by matching the\nnode embeddings of different graphs rather than graph adjacencies, as follows:\nmin\nP ∈P\n\r\r\r E −P ˆE\n\r\r\r\n2\nF ,\n(2)\nwhere E ∈RN×F is embedding matrix and E[i, :] is the vector representation of node i. The\noptimization problem in (2) is a linear assignment problem and can be optimally solved in O\n\u0000N 3\u0001\nby the Hungarian method [Kuhn, 1955a]. Simpler sub-optimal alternatives also exist that operate\nwith O\n\u0000N 2\u0001\nor O (N log(N)) flops.\nA question that naturally arises is how to generate powerful node embeddings that capture the network\nconnectivity and also be effective in aligning different graphs. A natural and effective approach\nis to leverage the spectral decomposition of the graph, S = V ΛV T , where V is the orthonormal\nmatrix of the eigenvectors, and Λ is the diagonal matrix of corresponding eigenvalues. Note that\nwe assume undirected graphs and thus S is symmetric. Spectral decomposition has been proven to\n3\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nbe an efficient approach to generating meaningful node embedding for graph matching [Umeyama,\n1988, Feizi et al., 2019]. In particular, E = V or E = V Λ are node embeddings that capture the\nnetwork connectivity since they can perfectly reconstruct the graph. However, V is not unique. Thus\ncomputing the spectral decomposition of the same graph with node relabelling, ˜S = P SP T is not\nguaranteed to produce a permuted version of V , i.e., P V . Even in the case where S does not have\nrepeated eigenvalues V is only unique up to column sign, which prevents effective matching.\nTo overcome the aforementioned uniqueness limitation, one can focus on the top m eigenvectors that\ncorrespond to non-repeated eigenvalues in both S and ˆS and compute their absolute values. Then\nnetwork alignment can be cast as:\nmin\nP ∈P\n\r\r\r |Vm| −P\n\f\f\f ˆVm\n\f\f\f\n\r\r\r\n2\nF ,\n(3)\nwhere Vm ∈RN×m corresponds to the subspace of non-repeated eigenvalues. The formulation in\n(3) is a similar to the problem solved in [Umeyama, 1988].\n3\nGraph Neural Networks (GNNs) Upper-Bounds Spectral Methods for\nNetwork Alignment\nA GNN is a cascade of layers and performs local, message-passing operations that are usually defined\nby the following recursive equation:\nx(l+1)\nv\n= g\n\u0010\nx(l)\nv , f\n\u0010n\nx(l)\nu : u ∈N (v)\no\u0011\u0011\n,\n(4)\nwhere N (v) is the neighborhood of vertex v, i.e., u ∈N (v) if and only if (u, v) ∈E. The function\nf operates on multisets ({·} represents a multiset) and f, g are ideally injective. Common choices\nfor f are the summation or mean function, and for g the linear function, or the multi-layer perceptron\n(MLP).\nOverall, the output of the L−th layer of a GNN is a function ϕ (X; S, H) : RN×D →RN×DL,\nwhere S is the graph operator, and H is the tensor of the trainable parameters in all L layers and\nproduces DL−dimensional embeddings for the nodes of the graph defined by S.\nGNNs admit some very valuable properties. First, they are permutation equivariant:\nTheorem 3.1 ([Xu et al., 2019a, Maron et al., 2018]). Let ϕ (X; S, H) : RN×D →RN×DL be a\nGNN with parameters H. For ˜\nX = P X and ˜S = P SP T that correspond to node relabelling\naccording to the permutation matrix P , the output of the GNN takes the form:\n˜\nX(L) = ϕ\n\u0010\n˜\nX; ˜S, H\n\u0011\n= P ϕ (X; S, H)\n(5)\nThe above property is not satisfied by other spectral methods. GNNs are also stable [Gama et al.,\n2020, Parada-Mayorga et al., 2023, Ruiz et al., 2021], transferable [Ruiz et al., 2020], and have high\nexpressive power [Xu et al., 2019a, Abboud et al., 2021, Kanatsoulis and Ribeiro, 2022].\n3.1\nGNNs and Network Alignment\nTo characterize the ability of a GNN to perform network alignment we first pointed out the GNNs\nperform nonlinear spectral operations. Details can be found in Appendix Section D.1. We can further\nprove that:\nTheorem 3.2. Let G, ˆG be graphs with adjacencies S, ˆS that have non-repeated eigenvalues. Also\nlet P ⋄, ˇP be solutions to the optimization problems in (1) and (3) respectively. Then there exists a\nGNN ϕ (X; S, H) : RN×D →RN×DL such that:\n\r\r\r S −P ⋄ˆSP ⋄T \r\r\r\n2\nF ≤\n\r\r\r S −P ∗ˆSP ∗T \r\r\r\n2\nF ≤\n\r\r\r S −ˇ\nP ˆS ˇ\nP T \r\r\r\n2\nF\nwith\nP ∗= arg min\nP ∈P\n\r\r\r ϕ (X; S, H) −P ϕ\n\u0010\nˆ\nX; ˆS, H\n\u0011 \r\r\r\n2\nF .\n4\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nThe proof can be found in Appendix D. The assumption that the graph adjacencies have different\neigenvalues is not restrictive. Real nonisomorphic graphs have different eigenvalues with very high\nprobability [Haemers and Spence, 2004]. Theorem 3.2 compares the network alignment power of\na GNN with that of a spectral algorithm Umeyama [1988], that uses the absolute values of graph\nadjacency eigenvectors to match two different graphs. According to Theorem 3.2 there always exists\na GNN that can perform at least as well as the spectral approach. The proof studies a GNN with white\nrandom input and measures the variance of the filter output. Then it shows that message-passing\nlayers are able to compute the absolute values of the graph adjacency eigenvectors when the adjacency\nhas non-repeated eigenvalues. As a result there always exists a single layer GNN that outputs the\nsame node features as the ones used in Umeyama [1988], which concludes our proof. The questions\nis: How do we train such a GNN that is (1) Expressive to the structural information thus its output\ncan be used to match corresponding nodes. (2) Robust to different perturbations and even larger scale\nunseen graphs so that it can be deployed efficiently without re-training. (3) Agnostic to the ground\ntruth node correspondence so that it can be trained unsupervisedly, which makes it generalizable to\nmost real-world settings. To answer these questions, we introduce our proposed training framework\nin the following section.\n4\nProposed Method\nWe now leverage the favorable properties of GNNs (permutation equivariance, expressivity, and\ntransferability) to tackle real-world network alignment. Our approach learns low-dimensional node\nembeddings (Eq. 4) that enable graph matching via solving the linear assignment in (2) rather than\na quadratic assignment problem in (1). We design a robust GNN framework such that the node\nembeddings are expressive to accurately match similar nodes and also stable to graph perturbations.\n4.1\nLearning Network Geometry with Transferable Graph Auto-encoders\nThe goal of the proposed framework is to learn a function that maps graphs to node representations\nand effectively match nodes from different graphs. This function is modeled by a GNN encoder\nϕ (X; S, H), described by Fig. 1. The learned encoder should work for a family of training graphs\n{G0, . . . , Gi, . . . , GI} with a set of adjacency matrices S = {S0, . . . , Si, . . . , SI}, rather than a single\ngraph. So the idea is not to train a GNN on a single graph [Kipf and Welling, 2016], but train a\ntransferable graph auto-encoder by solving the following optimization problem.\nmin\nH\nE\nh\nl\n\u0010\nρ\n\u0010\nϕ (X; Si, H) ϕ (X; Si, H)T \u0011\n, Si\n\u0011i\n,\n(6)\nwhere l (·) is the binary cross entropy (BCE) and ρ (·) is the logistic function. Si ∈S is a realization\nfrom a family of graphs and the expectation (empirical expectation is practice) is computed over this\ngraph family. The generalized framework in (6) learns a mapping from graphs to node representations,\nand can be applied to out-of-distribution graphs that have not been observed during training. This\ntwist in the architecture enables node embedding and graph matching for the unseen and larger\nscale networks without re-training, where fitting a GNN is computationally prohibitive in real-world\napplications.\n4.2\nRobust and Generalizable Node representations with self-supervised learning (data\naugmentation)\nSo far we proposed a GNN framework to produce expressive node representations to perform\nnetwork alignment. In this subsection, we further upgrade our framework by ensuring the robustness\nand generalization ability of the proposed mapping. In particular, for each graph, Si ∈S, we\naugment the training set with perturbed versions that are described by the following set of graph\nadjacencies Mi =\nn\nS(0)\ni\n, . . . , S(j)\ni\n, . . . , S(J)\ni\no\n, that are perturbed versions of Si. To do so we add\nor remove an edge with a certain probability yielding ˜Si ∈M, such that ˜Si = Si + Mi, where\nMi ∈{−1, 0, 1}N×N. Note that M changes for each ˜Si, and M[m, n] can be equal to 1 and −1\nonly if S[m, n] is equal to 0 and 1 respectively. To train the proposed transferable graph-autoencoder\nwe consider the following optimization problem:\nmin\nH\nES\n\u0014\nEMi\n\u0014\nl\n\u0012\nρ\n\u0012\nϕ\n\u0010\nX; ˜Si, H\n\u0011\nϕ\n\u0010\nX; ˜Si, H\n\u0011T \u0013\n, Si\n\u0013\u0015\u0015\n,\n(7)\n5\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nFigure 3: The pre-trained encoder operates on out-of-distribution samples. The generated node\nembeddings are then matched greedily.\nwhere ES is the expectation with respect to the family of graphs S and EMi is the expectation with\nrespect to the perturbed graphs Mi. In practice, ES, EM correspond to empirical expectations. Note\nthat training according to (7) also benefits the robustness of the model, which is crucial in deep\nlearning tasks [Wang et al., 2022, He, 2021]. A schematic illustration of the training process can be\nfound in Fig. 2.\nRemark 4.1. (Large-scale network alignment by transferability of GNN)\nThe proposed framework learns a mapping ϕ : G →RN×F that produces expressive and robust\nnode representations for a family of graphs G ∈G. This mapping is designed in such a way that the\nproblem in (2) approximates the problem in (1) and allows solving network alignment in polynomial\ntime. One of the main benefits of the proposed framework is that it enables large-scale network\nalignment. Task specific augmentation during training is the key to prompt transferability of deep\nneural networks [Li et al., 2022]. And the transferability analysis of GNN encoders [Ruiz et al.,\n2020] suggests that we can train with small graphs and efficiently execute with much larger graphs\nwhen the substructures (motifs) that appear in the tested graphs, were also partially observed during\ntraining. Since the proposed transferable graph auto-encoder is trained with multiple graphs, a variety\nof motifs are observed during training, which cannot be observed with a classical graph autoencoder,\nand the proposed GNN encoder can be transferred to larger-scale out-of-distribution graphs.\n4.3\nAlignment and Complexity analysis\nAfter learning the powerful node embeddings, network alignment is performed by solving the linear\nassignment problem in (2). An illustration of the assignment is presented in Fig. 3. The node\nfeatures produced by T-GAE are used to calculate a pairwise distance matrix, followed by the\ngreedy Hungarian algo rithm to predict node correspondences. To analyze the complexity of our\napproach we study the 3 main parts of T-GAE: a) The design of the input structural features, b) The\nmessage-passing GNN that produces node embeddings, and c) the linear assignment algorithm.\nThe computation of our neighborhood-based structural features is expected to take O (|V|) in real\ngraphs, as proved in Henderson et al. [2011]. The computational and memory complexity of\nthe message-passing GNN is O\n\u0000|V| c2 + |E| c\n\u0001\n, and O (|V| c), where c is the width of the GNN.\nThe computational complexity to align the nodes of the graph is O\n\u0010\n|V|2\u0011\nsince we are using the\nsuboptimal greedy Hungarian. If we want to optimally solve the linear assignment problem we need\nto use the Hungarian algorithm that has O\n\u0010\n|V|3\u0011\ncomplexity. If we want to process large graphs\nwe can use efficient nearest neighbors algorithms with complexity O (|V| log (|V|)) to perform soft\nlinear assignment. However, this efficient algorithm only works to match graphs with its permuted\nsamples. We include detailed discussion in Appendix Section H. Overall the complexity of T-GAE is\nO\n\u0010\n|V|2\u0011\n, or O\n\u0000|V| c2 + |E| c + |V| log (|V|)\n\u0001\nfor un-perturbed samples.\n5\nExperiments\n5.1\nExperiments Setup\nThe experiments included in this section are designed to answer the following research questions:(1)\nCan T-GAE generate competing graph matching accuracy on real-world networks from various\n6\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\ndomains with different sizes, while being efficient by utilizing transferability of GNN for large-\nscale our-of-distribution graphs? We answer this question in Section 5 and Appendix Section G\nby comparing the performance on matching small to middle sized graphs with unseen perturbed\nsamples, and large scale out-of-distribution networks under different perturbation distributions. (2) Is\nT-GAE robust to different graph matching tasks and real-world noise distributions? In Section 5.3, we\nconduct sub-graph matching experiments to align two different real-world networks that are partially\naligned. (3) How does the proposed network architecture (Figure 1) and the training objective (Figure\n2) contribute to network alignment? To demonstrate the contribution of each proposed component,\nwe conduct ablation studies in Section 5.4, to compare T-GAE with untrained T-GAE, T-GAE trained\nwith GAE objective, and GAE. (4) How much efficiency does T-GAE offer compared to the existing\noptimization and GNN approaches, and what are the possible trade-offs between efficiency and\nmatching accuracy of T-GAE? We compare the efficiency of different matching algorithms and\nempirically prove that (a) The matching accuracy of T-GAE can be further improved by leveraging\nthe exact Hungarian algorithm. (b) The efficiency of T-GAE to match graphs with their permuted\nsamples can be enhanced if we adopt the more efficient matching algorithm introduced in Section 4.3.\nThis set of experiments are included in Appendix Section H.\nFor each of the above mentioned experiments, we compare T-GAE with three categories of graph\nmatching approaches: (a)GNN based methods: WAlign [Gao et al., 2021], GAE and VGAE [Kipf\nand Welling, 2016]; (b)Graph/Node embedding techniques: NetSimile [Berlingerio et al., 2013],\nSpectral [Umeyama, 1988], DeepWalk [Perozzi et al., 2014], [Grover and Leskovec, 2016b],\nGraphWave [Donnat et al., 2018] and LINE [Tang et al., 2015]. (c)Optimization based graph matching\nalgorithms: S-GWL [Xu et al., 2019b], ConeAlign [Chen et al., 2020] and FINAL [Zhang and Tong,\n2016]. Note that LINE, VGAE, DeepWalk, and Node2Vec are omitted from some experiments\nsince they show very poor performance. The reason behind that is that they are not permutation\nequivariant. GraphWave is also excluded from the sub-graph matching experiment, it could not\nidentify correlated nodes in two different graphs. In the case of graphs without attributes FINAL is\nequivalent to the popular Isorank [Singh et al., 2008] algorithm, and FINAL is omitted in sub-graph\nmatching experiments due to weak performance.\nWe include detailed descriptions of our included datasets, implementation details of T-GAE and all\nthe competing baselines in Appendix Section E.\n5.2\nGraph Matching Experiments\nIn this subsection we compare the performance of T-GAE with all competing baselines to match the\ngraphs with permuted and perturbed versions of them. In particular, let G be a graph with adjacency\nmatrix S. We then produce 10 permuted-perturbed versions according to ˆS = P (S + M) P T ,\nwhere M ∈{−1, 0, 1}N×N and P is a permutation matrix. For each perturbation level p ∈\n{0, 1%, 5%}, the total number of perturbations is defined as p|E|, where |E| is the number of edges\nof the original graph.\nSpecifically, we train T-GAE according to (7), where S consist of the small-size networks, i.e.,\nCelegans, Arena, Douban, and Cora. Then we resort to transfer learning and use the T-GAE encoder\nto produce node embedding for perturbed versions of (a) Celegans, Arena, Douban, and Cora, and\n(b) larger graphs, i.e., Dblp, and Coauthor CS. Note that none of these perturbed versions were\nconsidered during training. This is in contrast with all GNN baselines that are retrained on every\npair of networks in the testing dataset. We report the average and standard deviation of the matching\naccuracy for 10 randomly generated perturbation samples under uniform edge editing in Table 1,\nwhere each edge and non-edge shares the same probability of being removed or augmented. We\nreport the results for removing edges according to degrees and the relevant discussed in Appendix G.\nT-GAE framework results in a robust and transferable GNN to perform network alignment\nat a large scale. Our first observation is that for zero perturbation most algorithms are able to\nachieve a high level of matching accuracy. This is expected, since for zero perturbation the network\nalignment is equivalent to graph isomorphism. On the smaller networks(Celegans, Arenas, Douban,\nCora), T-GAE performs at least as well as the current state-of-the-art optimization approaches (S-GWL\nand ConeAlign). Specifically, it achieves up to 38.7% and 44.7% accuracy increase compared to\nS-GWL and ConeAlign, respectively. Regarding the ability of T-GAE to perform large-scale network\nalignment the results are definitive. T-GAE enables low-complexity training with small graphs,\nand execution at larger settings by leveraging transfer learning, and it consistently outperforms all\n7\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nTable 1: Graph matching accuracy on 10 randomly perturbed samples under different levels of edge\nediting on Uniform model. The proposed T-GAE is trained on the clean Celegans, Arena, Douban,\nand Cora networks, and tested on noisy versions of them and the larger Dblp, and Coauthor CS.\nAccuracy above 80% is highlighted in green, 60% to 80% accuracy is in yellow, and performance\nbelow 60% is in red.\nDataset \\ Algorithm\nFeature Engineering based\nOptimization based\nGNN based\nSpectral\nNetsimile\nGraphWave\nFINAL\nS-GWL\nConeAlign\nWAlign\nGAE\nT-GAE\n0% perturbation\nCelegans\n87.8 ± 1.5\n72.7 ± 0.9\n65.3 ± 1.7\n92.2 ± 1.2\n93.0 ± 1.5\n66.6 ± 1.2\n88.4 ± 1.6\n86.3 ± 1.3\n91.0 ± 1.1\nArenas\n97.7 ± 0.4\n94.7 ± 0.3\n81.7 ± 0.7\n97.5 ± 0.3\n97.5 ± 0.3\n87.8 ± 0.6\n97.4 ± 0.5\n97.6 ± 0.4\n97.8 ± 0.4\nCora\n85.0 ± 0.4\n73.7 ± 0.4\n8.3 ± 0.4\n87.5 ± 0.7\n87.3 ± 0.7\n38.5 ± 0.7\n87.2 ± 0.4\n87.1 ± 0.8\n87.5 ± 0.4\nDouban\n89.9 ± 0.4\n46.4 ± 0.4\n17.5 ± 0.2\n89.9 ± 0.3\n90.1 ± 0.3\n68.1 ± 0.4\n90.0 ± 0.4\n89.5 ± 0.4\n90.1 ± 0.3\nDblp\n84.5 ± 0.1\n63.7 ± 0.2\ndoesn’t scale\n85.6 ± 0.2\n> 48 hours\n44.3 ± 0.6\n85.6 ± 0.2\n85.2 ± 0.3\n85.6 ± 0.2\nCoauthor CS\n97.5 ± 0.1\n90.9 ± 0.1\ndoesn’t scale\n97.6 ± 0.1\n> 48 hours\n75.8 ± 0.5\n97.5 ± 0.2\n97.6 ± 0.3\n97.6 ± 0.1\n1% perturbation\nCelegans\n68.5 ± 16.1\n66.3 ± 3.8\n22.5 ± 22.4\n33.2 ± 7.8\n87.1 ± 6.1\n60.9 ± 2.5\n80.7 ± 3.0\n33.2 ± 8.4\n86.5 ± 1.1\nArenas\n85.0 ± 10.0\n87.8 ± 1.0\n40.5 ± 23.8\n32.5 ± 5.9\n94.2 ± 0.7\n84.6 ± 1.0\n90.0 ± 3.1\n30.1 ± 17.6\n96.0 ± 1.0\nCora\n59.1 ± 9.3\n66.4 ± 1.6\n3.7 ± 2.9\n30.0 ± 3.3\n46.4 ± 6.9\n33.5 ± 1.6\n80.1 ± 1.2\n57.9 ± 5.3\n85.1 ± 0.5\nDouban\n25.8 ± 27.2\n40.0 ± 1.2\n9.9 ± 5.9\n27.8 ± 5.7\n72.1 ± 0.7\n64.7 ± 0.4\n77.2 ± 4.8\n38.3 ± 16.4\n87.3 ± 0.4\nDblp\n55.6 ± 19.0\n55.1 ± 1.7\ndoesn’t scale\n15.2 ± 3.3\n> 48 hours\n37.8 ± 1.1\n73.1 ± 1.6\n19.4 ± 0.6\n83.3 ± 0.4\nCoauthor CS\n58.2 ± 22.1\n75.2 ± 2.2\ndoesn’t scale\n13.3 ± 5.0\n> 48 hours\n68.5 ± 2.8\n75.2 ± 5.4\n49.5 ± 7.8\n93.2 ± 0.8\n5% perturbation\nCelegans\n24.9 ± 15.9\n41.1 ± 13.0\n7.6 ± 9.2\n10.4 ± 2.7\n68.3 ± 12.7\n50.5 ± 3.4\n42.4 ± 21.1\n6.5 ± 2.4\n69.2 ± 2.1\nArenas\n52.1 ± 16.5\n52.3 ± 5.3\n6.9 ± 7.2\n7.2 ± 2.6\n88.3 ± 3.2\n75.0 ± 2.7\n30.4 ± 17.5\n1.4 ± 1.4\n81.2 ± 1.4\nCora\n29.5 ± 0.8\n41.2 ± 3.3\n0.8 ± 0.3\n6.7 ± 2.8\n39.9 ± 5.5\n23.0 ± 2.0\n33.4 ± 7.3\n9.6 ± 2.7\n67.7 ± 1.3\nDouban\n23.8 ± 20.6\n20.7 ± 4.6\n1.9 ± 2.8\n7.8 ± 3.0\n68.6 ± 0.8\n54.1 ± 1.2\n36.6 ± 13.4\n0.6 ± 0.3\n70.2 ± 2.5\nDblp\n28.0 ± 7.8\n19.5 ± 4.8\ndoesn’t scale\n2.7 ± 0.9\n> 48 hours\n24.4 ± 2.9\n15.9 ± 8.3\n1.4 ± 0.2\n60.8 ± 1.9\nCoauthor CS\n9.7 ± 5.0\n26.3 ± 6.0\ndoesn’t scale\n2.0 ± 0.4\n> 48 hours\n51.4 ± 5.1\n11.3 ± 7.5\n0.6 ± 0.1\n66.0 ± 1.4\nTable 2: Subgraph matching performance comparison. The proposed T-GAE is trained on the two\nreal-world graphs, and test to match the aligned portion of them.\nAlgorithm \\ Hit Rate\nACM-DBLP\nDouban Online-Offline\nHit@1\nHit@5\nHit@10\nHit@50\nHit@1\nHit@5\nHit@10\nHit@50\nNetsimile\n2.59%\n8.32%\n12.09%\n26.42%\n1.07%\n2.77%\n4.74%\n15.03%\nSpectral\n1.40%\n4.62%\n7.21%\n16.34%\n0.54%\n1.34%\n2.95%\n13.95%\nGAE\n8.1%\n22.5%\n30.1%\n45.1%\n3.3%\n9.2%\n14.1%\n32.1%\nWAlign\n62.02%\n81.96%\n87.31%\n93.89%\n36.40%\n53.94%\n67.08%\n85.33%\nT-GAE\n73.89%\n91.73%\n95.33%\n98.22%\n36.94%\n60.64%\n69.77%\n89.62%\ncompeting baselines on the two networks with more than 10k nodes. In particular, it is able to achieve\nvery high levels of matching accuracy for both Dblp and Coauthor CS, for p = 0%, 1%. It is also the\nonly method that consistently achieves at least 60% accuracy at 5% perturbation. To the best of our\nknowledge, our experiments on DBLP [Pan et al., 2016] and Coauthor CS [Shchur et al., 2018] are\nthe first attempts to perform exact alignment on networks at the order of 20k nodes and 80k edges.\nThe benefits of processing structural node features with T-GAE is clear. There is a clear benefit\nof processing the structural embeddings with TGAE since it offers up to 43.7% performance increase\ncompared to NetSimile. When some perturbation is added, the conclusions are straightforward. Es-\npecially when perturbations are added, our proposed T-GAE markedly outperforms all the competing\nGNN alternatives(WAlign and GAE) and shows the desired robustness to efficiently perform network\nalignment. We observe that neither GAE nor WAlign is robust to noise. This highlights the benefit\nof T-GAE in handling the distribution shift brought by the structural dissimilarity between different\ngraphs.\n5.3\nSub-graph Matching Experiments\nWe further test the performance of T-GAE in matching subgraphs of different networks that have\naligned nodes (nodes that represent the same entities in different networks). Specifically, in ACM-\nDBLP, the task is to match the papers that appear in both citation networks; in Douban Online-Offline,\nwe aim to identify the users that take part into both online and offline activities. Note that this is the\nmost realistic graph matching experiment we can perform since we match 2 different real graphs with\npartially aligned nodes.\nT-GAE uniformly achieves the best performance in the most realistic scenario of network\nalignment. Most optimization based approaches do not generalize to this real-world scenario\nbecause their optimization objective usually prevents from matching graphs with different numbers\nof nodes. There is a significant improvement in matching accuracy with GNN-based methods\n8\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nFigure 4: Graph matching performance comparison of T-GAE,T-GAE trained on a single graph(T-\nGAE single), the untrained T-GAE (T-GAE untrained), and GAE. The proposed training objective\nand encoder structure helps to prompt the expressiveness of GNN thus achieve higher accuracy as we\nintroduce more perturbations.\n(TGAE and WAlign) compared to traditional graph or node embedding techniques. In particular,\nT-GAE consistently achieves the best performance among all competing methods. This suggests\nthat the encoder model illustrated in Fig. 1 and the training framework (7) illustrated in Fig. 2,\nprovide an efficient approach to generate powerful node embedding, that is robust to real-world noise\ndistributions for the task of network alignment, compared to the existing GNN frameworks.\n5.4\nAblation study\nThe proposed architecture and training objective prompts the robustness of GNN when match-\ning graphs with their highly perturbed versions. From Figure 4, we observe that T-GAE outper-\nforms the untrained T-GAE by a great margin when matching highly permuted samples. This implies\nthat the proposed training objective effectively improves the robustness of GNN, which is the key\nproperty to deploy GNN to match perturbed graphs. Further, the performance gap between GAE and\nT-GAE single underscores the efficacy of incorporating attention mechanism on each layer for both\ninput and output node features, as illustrated in Figure 1.\n6\nLimitation\nAlthough our approach achieves state-of-the-art performance in aligning real-graphs, on both graph\nmatching and sub-graph matching tasks, approaching network alignment with a learning method,\nremains a heuristic and does not offer optimality guarantees. Furthermore, in order to process large\ngraphs we cast network alignment as a self-supervised task. As a result in small-scale settings\nwhere the task can be tackled with computationally intensive efficient methods, our algorithm is not\nexpected to perform the best. Finally, the complexity of T-GAE O(|V|2) is limiting, this bottleneck\ncomes from the greedy linear assignment algorithm to match the node embedding, and therefore\nthe alternative method with complexity O(|V|c2 + |E|c + |V|log(|V|)) should be deployed when we\nmatch very large scale graphs with their permuted versions.\n7\nConclusion\nWe proposed T-GAE, a graph autoencoder framework that utilizes transferability and robustness\nof GNN to perform network alignment.\nT-GAE is an unsupervised approach that tackles\nthe high computational cost of existing optimization based algorithms, and can be trained on\nmultiple small to middle sized graphs to produce robust and permutation equivariant embeddings\nfor larger scale unseen networks.\nWe proved that the produced embeddings of GNNs are\nrelated to the spectral decomposition of the graph and are at least as good in graph matching\nas certain spectral methods.\nOur experiments with real-world benchmarks on both graph\nmatching and sub-graph matching demonstrated the great potential of utilizing the good prop-\nerties of GNNs to solve network optimization problems in a more efficient and scalable way.\n9\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nReferences\nFrank Emmert-Streib, Matthias Dehmer, and Yongtang Shi. Fifty years of graph matching, network\nalignment and network comparison. Information sciences, 346:180–197, 2016. 1\nShirin Nilizadeh, Apu Kapadia, and Yong-Yeol Ahn. Community-enhanced de-anonymization of\nonline social networks. In Proceedings of the 2014 acm sigsac conference on computer and\ncommunications security, pages 537–548, 2014. 1\nKirk Ogaard, Heather Roy, Sue Kase, Rakesh Nagi, Kedar Sambhoos, and Moises Sudit. Discovering\npatterns in social networks with graph matching algorithms. In Ariel M. Greenberg, William G.\nKennedy, and Nathan D. Bos, editors, Social Computing, Behavioral-Cultural Modeling and\nPrediction, pages 341–349, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg. ISBN 978-3-\n642-37210-0. 1\nRohit Singh, Jinbo Xu, and Bonnie Berger. Global alignment of multiple protein interaction networks\nwith application to functional orthology detection. Proceedings of the National Academy of\nSciences, 105(35):12763–12768, 2008. 1, 7, 21\nJiazhou Chen, Hong Peng, Guoqiang Han, Hongmin Cai, and Jiulun Cai. HOGMMNC: a higher\norder graph matching with multiple network constraints model for gene–drug regulatory modules\nidentification.\nBioinformatics, 35(4):602–610, 07 2018a.\nISSN 1367-4803.\ndoi: 10.1093/\nbioinformatics/bty662. URL https://doi.org/10.1093/bioinformatics/bty662. 1\nD. Conte, P. Foggia, C. Sansone, and M. Vento. Graph matching applications in pattern recognition\nand image processing. In Proceedings 2003 International Conference on Image Processing (Cat.\nNo.03CH37429), volume 2, pages II–21, 2003. doi: 10.1109/ICIP.2003.1246606. 1\nMiklos Racz and Anirudh Sridhar. Correlated stochastic block models: Exact graph matching with\napplications to recovering communities. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang,\nand J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,\npages 22259–22273. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/\npaper/2021/file/baf4f1a5938b8d520b328c13b51ccf11-Paper.pdf. 1\nKurt M Anstreicher and Nathan W Brixius. Solving quadratic assignment problems using convex\nquadratic programming relaxations. Optimization Methods and Software, 16(1-4):49–68, 2001. 1\nJoshua T Vogelstein, John M Conroy, Vince Lyzinski, Louis J Podrazik, Steven G Kratzer, Eric T\nHarley, Donniell E Fishkind, R Jacob Vogelstein, and Carey E Priebe. Fast approximate quadratic\nprogramming for graph matching. PLOS one, 10(4):e0121002, 2015. 1\nGunnar W Klau. A new graph-based method for pairwise global network alignment. BMC bioinfor-\nmatics, 10(1):1–9, 2009. 1\nJiming Peng, Hans Mittelmann, and Xiaoxue Li. A new relaxation framework for quadratic assign-\nment problems based on matrix splitting. Mathematical Programming Computation, 2:59–77,\n2010. 1\nXingbo Du, Junchi Yan, and Hongyuan Zha. Joint link prediction and network alignment via cross-\ngraph embedding. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial\nIntelligence, IJCAI-19, pages 2251–2257. International Joint Conferences on Artificial Intelligence\nOrganization, 7 2019. doi: 10.24963/ijcai.2019/312. URL https://doi.org/10.24963/ijcai.\n2019/312. 1\nXingbo Du, Junchi Yan, Rui Zhang, and Hongyuan Zha. Cross-network skip-gram embedding\nfor joint network alignment and link prediction. IEEE Transactions on Knowledge and Data\nEngineering, 34(3):1080–1095, 2022. doi: 10.1109/TKDE.2020.2997861. 1\nAritra Konar and Nicholas D Sidiropoulos. Graph matching via the lens of supermodularity. IEEE\nTransactions on Knowledge and Data Engineering, 34(5):2200–2211, 2020. 1\nS. Umeyama. An eigendecomposition approach to weighted graph matching problems. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 10(5):695–703, 1988. doi: 10.1109/\n34.6778. 2, 4, 5, 7, 20\nSoheil Feizi, Gerald Quon, Mariana Recamonde-Mendoza, Muriel Medard, Manolis Kellis, and Ali\nJadbabaie. Spectral alignment of graphs. IEEE Transactions on Network Science and Engineering,\n7(3):1182–1197, 2019. 2, 4\n10\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nSi Zhang and Hanghang Tong. Final: Fast attributed network alignment. In Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD\n’16, page 1345–1354, New York, NY, USA, 2016. Association for Computing Machinery. ISBN\n9781450342322. doi: 10.1145/2939672.2939766. URL https://doi.org/10.1145/2939672.\n2939766. 2, 7, 19, 20, 21\nCharilaos I Kanatsoulis and Nicholas D Sidiropoulos. Gage: Geometry preserving attributed graph\nembeddings. In Proceedings of the Fifteenth ACM International Conference on Web Search and\nData Mining, pages 439–448, 2022. 2\nMichele Berlingerio, Danai Koutra, Tina Eliassi-Rad, and Christos Faloutsos. Network similarity\nvia multiple social theories. In Proceedings of the 2013 IEEE/ACM International Conference on\nAdvances in Social Networks Analysis and Mining, pages 1439–1440, 2013. 2, 7, 20, 21\nMark Heimann, Haoming Shen, Tara Safavi, and Danai Koutra. Regal: Representation learning-based\ngraph alignment. In Proceedings of the 27th ACM international conference on information and\nknowledge management, pages 117–126, 2018. 2\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations.\nIn Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and\ndata mining, KDD ’14. ACM, August 2014.\ndoi: 10.1145/2623330.2623732.\nURL http:\n//dx.doi.org/10.1145/2623330.2623732. 2, 7, 20\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings\nof the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,\npages 855–864, 2016a. 2\nXiyuan Chen, Mark Heimann, Fatemeh Vahedian, and Danai Koutra. Cone-align: Consistent\nnetwork alignment with proximity-preserving node embedding. In Proceedings of the 29th ACM\nInternational Conference on Information & Knowledge Management, pages 1985–1988, 2020. 2,\n7, 21\nParis A Karakasis, Aritra Konar, and Nicholas D Sidiropoulos. Joint graph embedding and alignment\nwith spectral pivot. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, pages 851–859, 2021. 2\nThomas N. Kipf and Max Welling. Variational graph auto-encoders, 2016. URL https://arxiv.\norg/abs/1611.07308. 2, 5, 7, 20\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph\ncontrastive learning with augmentations. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan,\nand H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages\n5812–5823. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_\nfiles/paper/2020/file/3fe230348e9a12c13120749e3f9fa4cd-Paper.pdf. 2\nP. Gainza, F. Sverrisson, F. Monti, E. Rodolà, D. Boscaini, M. M. Bronstein, and B. E. Correia.\nDeciphering interaction fingerprints from protein molecular surfaces using geometric deep learning.\nNature Methods, 17(2):184–192, February 2020. 2\nAlexey Strokach, David Becerra, Carles Corbi-Verge, Albert Perez-Riba, and Philip M. Kim. Fast\nand flexible protein design using deep graph neural networks. Cell Systems, 11(4):402–411.e4,\nOctober 2020. 2\nDejun Jiang, Zhenxing Wu, Chang Yu Hsieh, Guangyong Chen, Ben Liao, Zhe Wang, Chao Shen,\nDongsheng Cao, Jian Wu, and Tingjun Hou. Could graph neural networks learn better molecular\nrepresentation for drug discovery? a comparison study of descriptor-based and graph-based models.\nJournal of Cheminformatics, 13(1):12, dec 2021. 2\nXinyue Hu, Zenan Sun, Yi Nian, Yifang Dang, Fang Li, Jingna Feng, Evan Yu, and Cui Tao.\nExplainable graph neural network for alzheimer’s disease and related dementias risk prediction,\n2023. 2\nChenxin Li, Xinyu Liu, Cheng Wang, Yifan Liu, Weihao Yu, Jing Shao, and Yixuan Yuan. Gtp-4o:\nModality-prompted heterogeneous graph learning for omni-modal biomedical representation, 2024.\nURL https://arxiv.org/abs/2407.05540. 2\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural\nmessage passing for quantum chemistry. In International conference on machine learning, pages\n1263–1272. PMLR, 2017. 2\n11\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec.\nGraph convolutional neural networks for web-scale recommender systems. Proceedings of the\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 10:974–983,\nJune 2018. 2\nShiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. Graph neural networks in recommender\nsystems: A survey, 2020. URL https://arxiv.org/abs/2011.02260. 2\nX. Liu, R. Wang, D. Sun, J. Li, C. Youn, Y. Lyu, J. Zhan, D. Wu, X. Xu, M. Liu, X. Lei, Z. Xu,\nY. Zhang, Z. Li, Q. Yang, and T. Abdelzaher. Influence pathway discovery on social media. In 2023\nIEEE 9th International Conference on Collaboration and Internet Computing (CIC), pages 105–\n109, Los Alamitos, CA, USA, nov 2023a. IEEE Computer Society. doi: 10.1109/CIC58953.2023.\n00023. URL https://doi.ieeecomputersociety.org/10.1109/CIC58953.2023.00023.\n2\nQikai Yang, Panfeng Li, Zhicheng Ding, Wenjing Zhou, Yi Nian, and Xinhe Xu. A comparative\nstudy on enhancing prediction in social network advertisement through data augmentation. arXiv\npreprint arXiv:2404.13812, 2024. 2\nMatthias Fey, Jan E. Lenssen, Christopher Morris, Jonathan Masci, and Nils M. Kriege. Deep graph\nmatching consensus, 2020. URL https://arxiv.org/abs/2001.09621. 2\nTianshu Yu, Runzhong Wang, Junchi Yan, and Baoxin Li. Learning deep graph matching with\nchannel-independent embedding and hungarian attention. In International Conference on Learning\nRepresentations, 2020. URL https://openreview.net/forum?id=rJgBd2NYPH. 2\nZhiyuan Liu, Yixin Cao, Fuli Feng, Xiang Wang, Jie Tang, Kenji Kawaguchi, and Tat-Seng Chua.\nTraining free graph neural networks for graph matching, 2022. URL https://arxiv.org/abs/\n2201.05349. 2\nZiwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on graphs: A survey, 2020. URL https:\n//arxiv.org/abs/1812.04202. 2\nJie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,\nChangcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications,\n2021. URL https://arxiv.org/abs/1812.08434. 2\nZhehan Liang, Yu Rong, Chenxin Li, Yunlong Zhang, Yue Huang, Tingyang Xu, Xinghao Ding,\nand Junzhou Huang.\nUnsupervised large-scale social network alignment via cross network\nembedding.\nIn Proceedings of the 30th ACM International Conference on Information &\nKnowledge Management, CIKM ’21, page 1008–1017, New York, NY, USA, 2021. Associa-\ntion for Computing Machinery. ISBN 9781450384469. doi: 10.1145/3459637.3482310. URL\nhttps://doi.org/10.1145/3459637.3482310. 2\nJi Gao, Xiao Huang, and Jundong Li. Unsupervised graph alignment with wasserstein distance\ndiscriminator. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &\nData Mining, KDD ’21, page 426–435, New York, NY, USA, 2021. Association for Computing\nMachinery. ISBN 9781450383325. doi: 10.1145/3447548.3467332. URL https://doi.org/\n10.1145/3447548.3467332. 2, 7, 20\nJianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with\nvariance reduction, 2018b. URL https://arxiv.org/abs/1710.10568. 2\nJie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via\nimportance sampling, 2018c. URL https://arxiv.org/abs/1801.10247. 2\nWei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An\nefficient algorithm for training deep and large graph convolutional networks. In Proceedings of\nthe 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD\n’19, page 257–266, New York, NY, USA, 2019. Association for Computing Machinery. ISBN\n9781450362016. doi: 10.1145/3292500.3330925. URL https://doi.org/10.1145/3292500.\n3330925. 2\nHanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graph-\nsaint: Graph sampling based inductive learning method, 2020. URL https://arxiv.org/abs/\n1907.04931. 2\nTjalling C Koopmans and Martin Beckmann. Assignment problems and the location of economic\nactivities. Econometrica: journal of the Econometric Society, pages 53–76, 1957. 3\n12\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nHarold W Kuhn. The hungarian method for the assignment problem. Naval research logistics\nquarterly, 2(1-2):83–97, 1955a. 3\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\nnetworks?\nIn International Conference on Learning Representations, 2019a. URL https:\n//openreview.net/forum?id=ryGs6iA5Km. 4, 21\nHaggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph\nnetworks. In International Conference on Learning Representations, 2018. 4\nFernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability properties of graph neural networks.\nIEEE Transactions on Signal Processing, 68:5680–5695, 2020. 4\nAlejandro Parada-Mayorga, Zhiyang Wang, Fernando Gama, and Alejandro Ribeiro. Stability of\naggregation graph neural networks. IEEE Transactions on Signal and Information Processing over\nNetworks, 9:850–864, 2023. doi: 10.1109/TSIPN.2023.3341408. 4\nLuana Ruiz, Zhiyang Wang, and Alejandro Ribeiro. Graphon and graph neural network stability. In\nICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 5255–5259, 2021. doi: 10.1109/ICASSP39728.2021.9414838. 4\nLuana Ruiz, Fernando Gama, and Alejandro Ribeiro. Gated graph recurrent neural networks. IEEE\nTransactions on Signal Processing, 68:6303–6318, 2020. 4, 6\nRalph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The surprising power\nof graph neural networks with random node initialization. In IJCAI, 2021. 4\nCharilaos I Kanatsoulis and Alejandro Ribeiro. Graph neural networks are more powerful than we\nthink. arXiv preprint arXiv:2205.09801, 2022. 4\nWillem H Haemers and Edward Spence. Enumeration of cospectral graphs. European Journal of\nCombinatorics, 25(2):199–211, 2004. 5\nXuezhi Wang, Haohan Wang, and Diyi Yang. Measure and improve robustness in nlp models: A\nsurvey, 2022. 6\nJiashu He. Performance analysis of facial recognition: A critical review through glass factor, 2021. 6\nChenxin Li, Xin Lin, Yijin Mao, Wei Lin, Qi Qi, Xinghao Ding, Yue Huang, Dong Liang, and Yizhou\nYu. Domain generalization on medical imaging classification using episodic training with task\naugmentation. Computers in Biology and Medicine, 141:105144, 2022. ISSN 0010-4825. doi:\nhttps://doi.org/10.1016/j.compbiomed.2021.105144. URL https://www.sciencedirect.com/\nscience/article/pii/S0010482521009380. 6\nKeith Henderson, Brian Gallagher, Lei Li, Leman Akoglu, Tina Eliassi-Rad, Hanghang Tong, and\nChristos Faloutsos. It’s who you know: graph mining using recursive structural features. In\nProceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data\nmining, pages 663–671, 2011. 6\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks, 2016b. 7, 21\nClaire Donnat, Marinka Zitnik, David Hallac, and Jure Leskovec. Learning structural node em-\nbeddings via diffusion wavelets.\nIn Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, KDD ’18. ACM, July 2018.\ndoi:\n10.1145/3219819.3220025. URL http://dx.doi.org/10.1145/3219819.3220025. 7, 21\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale\ninformation network embedding. In Proceedings of the 24th International Conference on World\nWide Web, WWW ’15. International World Wide Web Conferences Steering Committee, May 2015.\ndoi: 10.1145/2736277.2741093. URL http://dx.doi.org/10.1145/2736277.2741093. 7,\n21\nHongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph\npartitioning and matching, 2019b. 7, 21\nShirui Pan, Jia Wu, Xingquan Zhu, Chengqi Zhang, and Yang Wang. Tri-party deep network\nrepresentation. In Subbarao Kambhampati, editor, Proceedings of the Twenty-Fifth International\nJoint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages\n1895–1901. IJCAI/AAAI Press, 2016. 8, 19\n13\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls\nof graph neural network evaluation. Relational Representation Learning Workshop, NeurIPS 2018,\n2018. 8, 19, 24\nJiashu He, Mingyu Derek Ma, Jinxuan Fan, Dan Roth, Wei Wang, and Alejandro Ribeiro. Give:\nStructured reasoning with knowledge graph inspired veracity extrapolation, 2024. URL https:\n//arxiv.org/abs/2410.08475. 15\nCongcong Ge, Xiaoze Liu, Lu Chen, Baihua Zheng, and Yunjun Gao. Make it easy: An effective\nend-to-end entity alignment framework. In Proceedings of the 44th International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR ’21, page 777–786,\nNew York, NY, USA, 2021a. Association for Computing Machinery. ISBN 9781450380379. doi:\n10.1145/3404835.3462870. URL https://doi.org/10.1145/3404835.3462870. 15\nCongcong Ge, Xiaoze Liu, Lu Chen, Yunjun Gao, and Baihua Zheng. Largeea: aligning entities for\nlarge-scale knowledge graphs. Proceedings of the VLDB Endowment, 15(2):237–245, October\n2021b. ISSN 2150-8097. doi: 10.14778/3489496.3489504. URL http://dx.doi.org/10.\n14778/3489496.3489504. 15\nYunjun Gao, Xiaoze Liu, Junyang Wu, Tianyi Li, Pengfei Wang, and Lu Chen. Clusterea: Scalable\nentity alignment with stochastic training and normalized mini-batch similarities. In Proceed-\nings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD\n’22, page 421–431, New York, NY, USA, 2022. Association for Computing Machinery. ISBN\n9781450393850. doi: 10.1145/3534678.3539331. URL https://doi.org/10.1145/3534678.\n3539331. 15\nCongcong Ge, Pengfei Wang, Lu Chen, Xiaoze Liu, Baihua Zheng, and Yunjun Gao. Collaborem: A\nself-supervised entity matching framework using multi-features collaboration. IEEE Transactions\non Knowledge and Data Engineering, 35(12):12139–12152, 2023. doi: 10.1109/TKDE.2021.\n3134806. 15\nXiaoze Liu, Junyang Wu, Tianyi Li, Lu Chen, and Yunjun Gao. Unsupervised entity alignment\nfor temporal knowledge graphs. In Proceedings of the ACM Web Conference 2023, WWW ’23,\npage 2528–2538, New York, NY, USA, 2023b. Association for Computing Machinery. ISBN\n9781450394161. doi: 10.1145/3543507.3583381. URL https://doi.org/10.1145/3543507.\n3583381. 15\nBolin Zhu, Xiaoze Liu, Xin Mao, Zhuo Chen, Lingbing Guo, Tao Gui, and Qi Zhang. Universal\nmulti-modal entity alignment via iteratively fusing modality similarity paths, 2023. URL https:\n//arxiv.org/abs/2310.05364. 15\nJian Ding, Zongming Ma, Yihong Wu, and Jiaming Xu. Efficient random graph matching via degree\nprofiles, 2020. 15\nYihong Wu, Jiaming Xu, and Sophie H. Yu. Settling the sharp reconstruction thresholds of random\ngraph matching, 2022. 15\nCheng Mao, Yihong Wu, Jiaming Xu, and Sophie H. Yu. Random graph matching at otter’s threshold\nvia counting chandeliers, 2023. 15\nStefania Sardellitti, Sergio Barbarossa, and Paolo Di Lorenzo. On the graph fourier transform for\ndirected graphs. IEEE Journal of Selected Topics in Signal Processing, 11(6):796–811, 2017. 16\nHarold W Kuhn. The hungarian method for the assignment problem. Naval research logistics\nquarterly, 2(1-2):83–97, 1955b. 18, 24\nJérôme Kunegis. Konect: the koblenz network collection. Proceedings of the 22nd International\nConference on World Wide Web, 2013. 19, 24\nJure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http:\n//snap.stanford.edu/data, June 2014. 19\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\nCollective classification in network data. AI magazine, 29(3):93–93, 2008. 19\nSi Zhang and Hanghang Tong. Attributed network alignment: Problem definitions and fast solutions.\nIEEE Transactions on Knowledge and Data Engineering, 31:1680–1692, 2019. URL https:\n//api.semanticscholar.org/CorpusID:70142000. 19\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.\nIn International Conference on Learning Representations, 2017. 20\n14\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nA\nAppendix\nB\nNotation\nOur notation is summarized in Table 1.\nTable 1: Key notations used in this paper.\nG\n≜\nGraph\nV\n≜\nSet of nodes\nE\n≜\nSet of edges\nN\n≜\nNumber of nodes\nD\n≜\nDegree matrix\nS\n≜\n{0, 1}N×N adjacency matrix\nX\n≜\nN × D feature matrix\nH\n≜\naggregation results of GNN convolution\nW\n≜\nweight matrix of the Graph Neural Network\nNv\n≜\nneighbors of node v\nI\n≜\nIdentity matrix\n0\n≜\nvector or matrix of zeros\nAT\n≜\ntranspose of matrix A\nArc\n≜\nentry at r-th row and j-th column of matrix A\n∥·∥F\n≜\nFrobenius norm\nC\nNetwork Alignment in broader domains\nIn this paper, we study the general form of network alignment to match nodes between two graphs.\nThis task is challenging because of the little given information. However, it’s not trivial to correctly\nand efficiently utilize the information contained in text-rich networks, such as KGs [He et al., 2024].\nIn the domain of knowledge graph (KG) mining, entity alignment (EA) is the task to identify the\nsame entities existing in knowledge graphs. [Ge et al., 2021a] proposes to replace the labor-intensive\npre-processing with entity names mining. A structural-based refinement procedure is then applied to\nrefine the entity name matching results. [Ge et al., 2021b, Gao et al., 2022] solves large scale EA by\naligning the KG in mini-batches. [Ge et al., 2023] further proposes a self-supervised EA framework\nby automatically generating positive and negative matched node pairs. [Liu et al., 2023b] generalizes\nthe unsupervised matching algorithm to temporal KGs. Specifically, it encodes the temporal and\nrelational information respectively before an innovative jointly decoding process. Recently, the ability\nto deploy EA algorithms in real-world scenarios is enhanced by [Zhu et al., 2023], which aligns\nmulti-modal knowledge graphs.\nNetwork alignment, as an important problem, has been studied not only by the community of data\nmining, it has also been mathematically and statistically investigated. A number of approaches have\nbeen proposed to solve this problem for Erd˝os Rényi random graphs G(n, d\nn). It has been proved that\na perfectly true vertex correspondence can be recovered in polynomial time with high probability\n[Ding et al., 2020]. Furthermore, a sharp threshold has been proved for both Erd˝os Rényi model and\nGaussian model [Wu et al., 2022]. Most recently, a novel approach to calculate similarity scores\nbased on counting weighted trees rooted at each vertex has been proposed Mao et al. [2023]. Such\napproach has been proved to be effective in solving the aforementioned network alignment problem\non random graphs with high probability. Readers are encouraged to refer to the authors of these\npublications [Ding et al., 2020, Wu et al., 2022, Mao et al., 2023] for further reading.\nD\nProof of Theorem 3.2\nD.1\nSpectral characterization of GNNs\nWhat remains to be answered is the ability of a GNN to approximate a function that performs graph\nalignment. To understand the function approximation properties of GNNs we study them in the\n15\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nspectral domain. To this end, we consider the recursive formula in (4) where f is the summation\nfunction and g is multivariate linear for K −2 layers, and the MLP in the (K −1)-th layer. The\noverall operation can be written in a matrix form as:\nX(l+1) = σ\n K−1\nX\nk=0\nSkX(l)H(l)\nk\n!\n,\n(8)\nwhere H(l)\nk\n∈RDl+1×Dl is a linear mapping. Computing the spectral decomposition of S yields:\nX(l+1) = σ\n K−1\nX\nk=0\nV ΛkV T X(l)H(l)\nk\n!\n= σ\n K−1\nX\nk=0\nN\nX\nn=1\nλk\nnvnvT\nn X(l)H(l)\nk\n!\n.\n(9)\nThen each each column of X(l+1) can be written as\nX(l+1)[:, i] = σ\n K−1\nX\nk=0\nN\nX\nn=1\nλk\nnvnvT\nn X(l)H(l)\nk [:, i]\n!\n= σ\n N\nX\nn=1\na(i)\nn vn\n!\n,\n(10)\nwhere λn, vn are the n−th eigenvalue and eigenvector and a(i)\nn = vT\nn X(l) PK−1\nk=0 λk\nnH(l)\nk [:, i] is a\nscalar related to the Graph Fourier Transform (GFT) of X(l) [Sardellitti et al., 2017]. It is clear from\nequation (10) that the output of each layer is a linear combination of the adjacency eigenvectors,\nfollowed by a pointwise non-linearity. Thus, a GNN can produce unique and more powerful graph\nembeddings than spectral methods by processing the eigenvectors and eigenvalues of the adjacency\nmatrix. To prove Theorem 3.2. We consider one layer GNN with a vector input x ∈RN. This GNN\ncan be represented by the following equation:\nY = σ\n K−1\nX\nk=0\nSkxhT\nk\n!\n,\n(11)\nwhere hk ∈Rm and xhT\nk is an outer-product operation. The equation in (11) describes a set of m\ngraph filters of the form:\nyi = σ\n K−1\nX\nk=0\nhi\nkSkx\n!\n, for i = 1, . . . , m\n(12)\nD.2\nWhite random input and variance computation\nLet x be a white random vector with E [x] = 0 and E\n\u0002\nxxT \u0003\n= I, where I is the diagonal matrix.\nAlso let σ (·) = (·)2 be the elementwise square function. Then (12) can be written as:\nyi =\n K−1\nX\nk=0\nhi\nkSkx\n!2\n= diag\n\n\nK−1\nX\nk=0\nhi\nkSkxxT\nK−1\nX\nj=0\nhi\njSjT\n\n\n(13)\nSince x is a random vector yi is also a random vector. The expected value of yi yields:\nE [yi] = E\n\ndiag\n\n\nK−1\nX\nk=0\nhi\nkSkxxT\nK−1\nX\nj=0\nhi\njSjT\n\n\n\n\n(14)\n= diag\n\n\nK−1\nX\nk=0\nhi\nkSkE\n\u0002\nxxT \u0003 K−1\nX\nj=0\nhi\njSjT\n\n\n= diag\n\n\nK−1\nX\nk=0\nhi\nkSk\nK−1\nX\nj=0\nhi\njSjT\n\n\n(15)\n16\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nD.3\nSingle band filtering\nIn the second part of the proof we study the graph filter using the spectral decomposition of the graph:\ny =\nK−1\nX\nk=0\nhkSkx\n(16)\n=\nK−1\nX\nk=0\nhkV ΛkV T x\n(17)\n=\nK−1\nX\nk=0\nhk\nN\nX\nn=1\nλk\nnvnvT\nn x\n(18)\n=\nN\nX\nn=1\nvT\nn x\nK−1\nX\nk=0\nhkλk\nnvn.\n(19)\nLet us focus on the following polynomial:\n˜h (λ) =\nK−1\nX\nk=0\nhkλk,\n(20)\nthat represents a graph filter in the frequency domain by. For q distinct eigenvalues we can write a\nsystem of linear equations using the polynomial in (20):\n\n\n˜h (λ1)\n˜h (λ2)\n...\n˜h (λq)\n\n=\n\n\n1 λ1 λ2\n1 . . . λK−1\n1\n1 λ2 λ2\n2 . . . λK−1\n2\n...\n1 λq λ2\nq . . . λK−1\nq\n\n\n\n\nh0\nh1\n...\nhK−1\n\n= W h\n(21)\nW is a Vandermonde matrix and when K = q the determinant of W takes the form:\ndet (W ) =\nY\n1≤i<j≤q\n(λi −λj)\n(22)\nSince the values λi are distinct, W has full column rank and there exists a graph filter with unique\nparameters h that passes only the λ eigenvalue, i.e.,\n˜h (λi) =\n\u001a\n1, if λi = λ\n0, if λi ̸= λ\n(23)\nUnder this parametrization, equation (16) takes the form y = vλvT\nλ x, where vλ is the eigenvector\ncorresponding to λ.\nD.4\nGNN and absolute eigenvectors\nUsing the previous analysis we can design parameters hk such that:\nK−1\nX\nk=0\nhkSk = vλvT\nλ\n(24)\nand then equation (14) takes the form:\nE [yi] = diag\n\n\nK−1\nX\nk=0\nhi\nkSk\nK−1\nX\nj=0\nhi\njSjT\n\n\n(25)\n= diag\n\u0000vλvT\nλ vλvT\nλ\n\u0001\n(26)\n= diag\n\u0000vλvT\nλ\n\u0001\n(27)\n= |vλ|2\n(28)\n17\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nWe can therefore design hk ∈Rm for k = 0, . . . , m −1 to compute the absolute value of m\neigenvectors of S that correspond to the top m distinct eigenvalues, i.e.,\nE [yi] = |ui|2,\ni = 1, . . . , m\n(29)\n(30)\nWe can do the same for graph ˆS and compute:\nE [ˆyi] = |ˆui|2,\ni = 1, . . . , m\n(31)\n(32)\nSince both S, ˆS have distinct eigenvalues, we can concatenate the output of each neuron and result\nin layer-1 outputs as:\nY (1) = |U|,\nˆY (1) = | ˆU|\n(33)\nAs a result, the previously described GNN can a least yield the same alignment accuracy as the\nabsolute values of the eigenvectors.\nD.5\nGeneralization to multiple graph pairs\nThe analysis in the previoius subsections is indeed presented for a pair of graphs but can be directly\nextended for any set of graphs. We can generalize the Theorem 3.2, to read as: Let {G1, . . . , GM} be\na set of graphs with adjacencies {S1, . . . , SM} that have non-repeated eigenvalues. Then for any\nS, ˆS ∈{S1, . . . , SM}, there exists a GNN ϕ (X; S, H) : RN×D →RN×DL such that:\n\r\r\r S −P ⋄ˆSP ⋄T \r\r\r\n2\nF ≤\n\r\r\r S −P ∗ˆSP ∗T \r\r\r\n2\nF ≤\n\r\r\r S −ˇ\nP ˆS ˇ\nP T \r\r\r\n2\nF\nwith\nP ∗= arg min\nP ∈P\n\r\r\r ϕ (X; S, H) −P ϕ\n\u0010\nˆ\nX; ˆS, H\n\u0011 \r\r\r\n2\nF ,\nwhere P ⋄, ˇP are solutions to the optimization problems in (1) and (3) respectively.\nE\nImplementation Details\nIn this section we discuss the implementation details of our framework.\nE.1\nAssignment Optimization\nThe proposed T-GAE learns learns a GNN encoder that can produce node representations for different\ngraphs. Let ϕ (X; S, H) represent the embeddings of the nodes corresponding to the graph with\nadjacency S and ϕ\n\u0010\nˆ\nX; ˆS, H\n\u0011\nrepresent the embeddings of the nodes corresponding to the graph\nwith adjacency ˆS. Then network alignment boils down to solving the following optimization problem:\nmin\nP ∈P\n\r\r\r ϕ (X; S, H) −P ϕ\n\u0010\nˆ\nX; ˆS, H\n\u0011 \r\r\r\n2\nF .\n(34)\nThe problem in (34) can be optimally solved in O\n\u0000N 3\u0001\nflops by the Hungarian algorithm [Kuhn,\n1955b]. To avoid this computational burden we employ the greedy Hungarian approach that has\ncomputational complexity O\n\u0000N 2\u0001\nand usually works well in practice.\nThe\ngreedy\nHungarian\napproach\nis\ndescribed\nin\nAlgorithm\n1.\nFor\neach\nrow\nof\nϕ (X; S, H) , ϕ\n\u0010\nˆ\nX; ˆS, H\n\u0011\n, which corresponds to the node embeddings of the different graphs, we\ncompute the pairwise Euclidean distance which is stores in the distance matrix D. Then, at each\niteration, we find the nodes with the smallest distance and remove the aligned pairs from D. This\nprocess is repeated until all the nodes are paired up for alignment.\n18\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nAlgorithm 1: Greedy Hungarian Algorithm\nInput: Feature matrices X, ˆ\nX\nOutput: Assignment Matrix\n1 P := 0N×N\n// Initialize permutation matrix\n2 D := PairwiseDistance\n\u0010\nX, ˆ\nX\n\u0011\n// pairwise Euclidean distance\n3 rows := 0,1,...,N-1\n// Corresponds to X\n4 cols := 0,1,...,N-1\n// Corresponds to ˆ\nX\n/* Iterate to assign node pairs with minimum Euclidean distance\n*/\n5 for n=1 to N do\n6\ni, j := argmin (D)\n7\nr := rows [i]\n8\nc := cols [j]\n9\nPrc := 1\n10\nRemove r from rows\n11\nRemove c from cols\n12\nRemove the i-th row from D\n13\nRemove the j-th column from D\n14 return P\nTable 2: Summary of Dataset statistics that are included in Section 5\nTask\nDataset\n|V|\n|E|\n# Aligned Edges\nNetwork Type\nGraph Matching\nCelegans [Kunegis, 2013]\n453\n2,025\n2,025\nInteractome\nArenas [Leskovec and Krevl, 2014]\n1,133\n5,451\n5,451\nEmail Communication\nCora [Sen et al., 2008]\n2,708\n5,278\n5,278\nCitation Network\nDouban [Zhang and Tong, 2016]\n3,906\n7,215\n7,215\nSocial Network\nDblp [Pan et al., 2016]\n17,716\n52,867\n52,867\nCitation Network\nCoauthor CS [Shchur et al., 2018]\n18,333\n81,894\n81,894\nCoauthor Network\nSubraph Matching\nACM-DBLP [Zhang and Tong, 2019]\n9,872\n9,916\n39,561\n44,808\n6,352\nCitation Network\nDouban Online-Offline [Zhang and Tong, 2016]\n3,906\n1,118\n1,632\n3,022\n1,118\nSocial Network\nE.2\nDatasets\nWe include statistics of the datasets used in our experiments in Table 2. The detailed descriptions of\neach dataset are presented below:\n• Celegans [Kunegis, 2013]: The vertices represent proteins and the edges their protein-protein\ninteractions.\n• Arenas Email [Leskovec and Krevl, 2014]: The email communication network at the University\nRovira i Virgili in Tarragona in the south of Catalonia in Spain. Nodes are users and each edge\nrepresents that at least one email was sent.\n• Douban [Zhang and Tong, 2016]: Contains user-user relationship on the Chinese movie review\nplatform. Each edge implies that two users are contacts or friends.\n• Cora [Sen et al., 2008]: The dataset consists of 2708 scientific publications, with edges repre-\nsenting citation relationships between them. Cora has been one of the major benchmark datasets\nin many graph mining tasks.\n• Dblp [Pan et al., 2016]: A citation network dataset that is extracted from DBLP, Association\nfor Computing Machinery (ACM), Microsoft Academic Graph (MAG), and other sources. It is\nconsidered a benchmark in multiple tasks.\n• Coauthor_CS [Shchur et al., 2018]: The coauthorship graph is generated from MAG. Nodes are\nthe authors and they are connected with an edge if they coauthored at least one paper.\n• ACM−DBLP [Zhang and Tong, 2019]: The citation networks that share some common nodes.\nThe task is to identify the publications that appear in both networks.\n19\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\n• Douban Online−Offline [Zhang and Tong, 2016]: The two social networks contained in this\ndataset represents the online and offline events of the Douban social network. The task is to\nidentify users that participate in both online and offline events.\nE.3\nBaselines\nE.3.1\nGraph Neural Network(GNN) based methods\nTo have a fair comparison with the node embedding models, GAE is implemented using the same\nset of parameters as T-GAE, which can be found at Section E.4. WAlign is implemented using\nparameters suggested by the author. We report the best results they achieved during training.\n• WAlign [Gao et al., 2021] fits a GNN to each of the input graphs, trains the model by reconstruct-\ning the given inputs and minimizing an approximation of Wasserstein distance between the node\nembeddings. We use the author’s implementation from https://github.com/gaoji7777/\nwalign.git.\n• GAE, VGAE[Kipf and Welling, 2016] are self-supervised graph learning frameworks that are\ntrained by reconstructing the graph. The encoder is a GCN[Kipf and Welling, 2017] and linear\ndecoder is applied to predict the original adjacency. In VGAE, Gausian Noise is introduced\nto the node embeddings before passing to the decoder. We use the implementation from\nhttps://github.com/DaehanKim/vgae_pytorch. We train GAE by reconstructing the\ngiven network using the netsimile node embedding.\nE.3.2\nGraph/Node embedding techniques\n• NetSimile [Berlingerio et al., 2013] uses the structural features described earlier to match the\nnodes of the graphs. Since the NetSimile features are used as input to the T-GAE, they provide\na measure to assess the benefit of using T-GAE for node embedding. It proposed 7 egonet-based\nfeatures, to measure network similarity. We process these features by Algorithm 1 to perform\nnetwork alignment. The 7-dimensional Netsimile features are:\n– di = degree of node i\n– ci = number of triangles connected to node i over the number of connected triples centered\non node i\n– ¯dNi = 1\ndi\nP\nj∈Ni dj, average number of two-hop neighbors\n– ¯cNi = 1\ndi\nP\nj∈Ni cj, average clustering coefficient\n– Number of edges in node i’s egonet\n– Number of outgoing edges from node i’s egonet\n– Number of neighbors in node i’s egonet\nThe implementation is based on netrd library where we use the feature extraction function.\nThe source code can be found at https://netrd.readthedocs.io/en/latest/_modules/\nnetrd/distance/netsimile.html\n• Spectral [Umeyama, 1988] It solves the following optimization problem:\nmin\nP ∈P\n\r\r\r |V | −P\n\f\f\f ˆV\n\f\f\f\n\r\r\r\n2\nF ,\n(35)\nwhere V , ˆV are the eigenvectors corresponding to the adjacencies of the graphs that we want to\nmatch. In our initial experiments, we observed that a subset of the eigenvectors yields improved\nresults compared to the whole set. We tried 1 −10 top eigenvectors and concluded that 4\neigenvectors are those that yield the best results on average. Thus we solve the above problem\nwith the top-4 eigenvectors.\n• DeepWalk [Perozzi et al., 2014]: A node embedding approach, simulates random walks on the\ngraph and apply skip-gram on the walks to generate node embedding. We use the implementation\nfrom Karateclub. The algorithm is implemented with the default parameters as suggested by this\nrepository, the number of random walks is 10 with each walk of length 80. The dimensionality\nof embedding is set to be 128. We run the algorithm with 1 epoch and set the learning rate to be\n0.05.\n20\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\n• Node2Vec [Grover and Leskovec, 2016b]: An improve version of DeepWalk, it has weights\non the randomly generated random walks, to make the neighborhood preserving objective\nmore flexible. We use the implementation from Karateclub. The default parameters are used.\nWe simulate 10 random walks on the graph with length 80. p and q are both equal to 1.\nDimensionality of embeddings is set to be 4 and we run 1 epoch with learning rate 0.05.\n• GraphWave [Donnat et al., 2018]: The structure information of the graphs is captured by\nsimulating heat diffusion process on them. We use the implementation from Karateclub with\nthe default parameters: number of evaluation points is 200, step size is 0.1, heat coefficient\nis 1.0 and Chebyshev polynomial order is set to be 100. Note that this implementation does\nnot work on graphs with more than 10,000 nodes, so we exclude this model on the DBLP and\nCoauthor_CS dataset.\n• LINE [Tang et al., 2015]: An optimization based graph embedding approach that aims to preserve\nlocal and global structures of the network by considering substructures and structural-quivariant\nnodes. We use the PyTorch implementation from https://github.com/zxhhh97/ABot. All\nparameters are set to default as the authors suggested.\nE.3.3\nOptimization based graph matching algorithms\n• FINAL [Zhang and Tong, 2016] is an optimization approach, following an alignment consistency\nprinciple, and tries to match nodes with similar topology. In the case of graphs without attributes\nFINAL is equivalent to the popular\n• Isorank [Singh et al., 2008] algorithm, whereas using NetSimile as an input to FINAL resulted\nin inferior performance and was therefore omitted. We use the code in https://github.com/\nsizhang92/FINAL-KDD16 with H being the degree similarity matrix, α = 0.8, maxiter =\n30, tol = 1e −4 as suggested in the repository.\n• ConeAlign [Chen et al., 2020] is a graph embedding based approach. The matching is optimized\nin each iteration by the Wasserstein Procrustes distances between the matched embeddings cal-\nculated on a mini batch in order to preserve scalability. We use the official implementation from\nhttps://github.com/GemsLab/CONE-Align and preserved all the suggested parameters.\n• S-GWL [Xu et al., 2019b] matches two given graphs by retrieving node correspondence from the\noptimal transport associated with the Gromov-Wasserstein discrepancy between the graphs. We\nuse the implementation by the authors in https://github.com/HongtengXu/s-gwl, since\nthe performance of S-GWL is very sensitive to the parameter gamma, as suggested by the\nauthors, we fine-tuned this parameter over the range of [0.001, 0.1] for each dataset on the\ncleaned graph, and use that optimal parameter for all other experiments on this dataset.\nE.4\nT-GAE model details\nAs illustrated in Figure 1, the structure of our proposed encoder consists of two MLPs and a series\nof GNN layers. The node features are processed by a 2-layer MLP and passed to all the GNN\nlayers. We add skip connections between this MLP layer and all the subsequent GNN layers. The\noutputs of all GNN layers are concatenated and passed to another 2-layer MLP, followed by a linear\ndecoder to generate the reconstructed graph. The model is optimized end to end by equation 7. For\ngraph matching experiments, since we consider the general case where graphs are given without\nnode attributes, we use the 7 structural features proposed in [Berlingerio et al., 2013]. The features\ninclude the degree of each node, the local and average clustering coefficient, and the number of\nedges, outgoing edges, and neighbors in each node’s egonet. This input feature is applied for all\nGNN-based methods. As a result, the performance of NetSimile, vanilla GAE and WAlign provide\nmeasures to assess the benefit of using T-GAE for node embedding. Note that one can choose different\nmessage passing functions as f and g in Equation (4), and any structure-preserving node features.\nOur reported results are based on GIN [Xu et al., 2019a] and Netsimile [Berlingerio et al., 2013].\nF\nMore Baseline Results\nWe present the graph matching accuracy for the baseline methods that are not permutation equavariant\nin Table 3, and sub-graph matching accuracy on Douban Online-Offline dataset for GraphWave in\nTable 4, as it is not scalable on ACM/DBLP.\n21\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nTable 3: Graph matching accuracy on 10 randomly perturbed samples under different levels of edge\nediting for VGAE, LINE and DeepWalk.\nDataset\nVGAE\nLINE\nDeepWalk\n0%\nCelegans\n0.3 ± 0.1\n1.0 ± 0.5\n1.8 ± 0.6\nArenas\n0.1 ± 0.1\n0.2 ± 0.1\n0.3 ± 0.2\nDouban\n0.0 ± 0.0\n0.0 ± 0.0\n0.1 ± 0.0\nCora\n0.1 ± 0.0\n0.0 ± 0.0\n0.1 ± 0.0\n1%\nCelegans\n0.3 ± 0.1\n1.0 ± 0.4\n1.2 ± 0.5\nArenas\n0.1 ± 0.1\n0.1 ± 0.1\n0.3 ± 0.1\nDouban\n0.0 ± 0.0\n0.0 ± 0.0\n0.1 ± 0.0\nCora\n0.1 ± 0.1\n0.1 ± 0.0\n0.2 ± 0.1\n5%\nCelegans\n0.6 ± 0.3\n0.9 ± 0.3\n1.0 ± 0.3\nArenas\n0.2 ± 0.1\n0.2 ± 0.2\n0.2 ± 0.1\nDouban\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nCora\n0.1 ± 0.0\n0.1 ± 0.0\n0.1 ± 0.0\nTable 4: Sub-graph matching performance for GraphWave on Douban Online-Offline\nHit rate\nGraphWave\nHit@1\n0.09\nHit@5\n0.36\nHit@10\n0.81\nHit@50\n4.74\nHit@100\n9.12\nG\nDegree Perturbation Model Results\nTable 5: Graph matching accuracy on 10 randomly perturbed samples under different levels of edge\nremoval on Degree model. The proposed T-GAE is trained on the clean Celegans, Arena, Douban,\nand Cora networks, and tested on noisy versions of them and the larger Dblp, and Coauthor CS.\nAccuracy above 80% is highlighted in green, 60% to 80% accuracy is in yellow, and performance\nbelow 60% is in red.\nDataset \\ Algorithm\nFeature Engineering based\nOptimization based\nGNN based\nSpectral\nNetsimile\nGraphWave\nFINAL\nS-GWL\nConeAlign\nWAlign\nGAE\nT-GAE\n0% perturbation\nCelegans\n87.8 ± 1.5\n72.7 ± 0.9\n65.3 ± 1.7\n92.2 ± 1.2\n93.0 ± 1.5\n66.6 ± 1.2\n88.4 ± 1.6\n90.9 ± 2.6\n91.0 ± 1.1\nArenas\n97.7 ± 0.4\n94.7 ± 0.3\n81.7 ± 0.7\n97.5 ± 0.3\n97.5 ± 0.3\n87.8 ± 0.6\n97.4 ± 0.5\n97.6 ± 0.4\n97.8 ± 0.4\nDouban\n89.9 ± 0.4\n46.4 ± 0.4\n17.5 ± 0.2\n89.9 ± 0.3\n90.1 ± 0.3\n68.1 ± 0.4\n90.0 ± 0.4\n89.5 ± 0.4\n90.1 ± 0.3\nCora\n85.0 ± 0.4\n73.7 ± 0.4\n8.3 ± 0.4\n87.5 ± 0.7\n87.3 ± 0.7\n38.5 ± 0.7\n87.2 ± 0.4\n87.1 ± 0.8\n87.5 ± 0.4\nDblp\n84.5 ± 0.1\n63.7 ± 0.2\ndoesn’t scale\n85.6 ± 0.2\n> 48 hours\n44.3 ± 0.6\n85.6 ± 0.2\n85.2 ± 0.3\n85.6 ± 0.2\nCoauthor CS\n97.5 ± 0.1\n90.9 ± 0.1\ndoesn’t scale\n97.6 ± 0.1\n> 48 hours\n75.8 ± 0.5\n97.5 ± 0.2\n97.6 ± 0.3\n97.6 ± 0.1\n1% perturbation\nCelegans\n21.8 ± 16.9\n63.1 ± 1.4\n4.9 ± 1.1\n62.5 ± 4.1\n73.1 ± 11.7\n61.4 ± 3.2\n70.7 ± 4.4\n7.8 ± 2.5\n63.4 ± 6.1\nArenas\n66.0 ± 21.6\n90.9 ± 0.7\n5.8 ± 2.6\n56.7 ± 2.7\n92.3 ± 1.2\n86.1 ± 0.6\n96.0 ± 0.5\n0.9 ± 0.5\n96.7 ± 0.6\nDouban\n9.8 ± 13.5\n37.6 ± 0.6\n0.6 ± 0.3\n35.0 ± 1.1\n69.9 ± 1.2\n59.6 ± 2.2\n81.3 ± 1.9\n0.4 ± 0.0\n83.7 ± 2.2\nCora\n25.3 ± 13.4\n64.4 ± 1.2\n0.1 ± 0.0\n32.0 ± 2.0\n31.1 ± 4.4\n30.6 ± 2.7\n74.6 ± 0.7\n28.2 ± 0.3\n81.0 ± 2.5\nDblp\n3.4 ± 0.8\n52.2 ± 0.5\ndoesn’t scale\n24.5 ± 0.6\n> 48 hours\n21.1 ± 1.9\n67.4 ± 1.1\n10.5 ± 0.6\n77.1 ± 0.6\nCoauthor CS\n8.6 ± 4.6\n76.8 ± 0.6\ndoesn’t scale\n19.1 ± 0.5\n> 48 hours\n64.0 ± 2.0\n88.9 ± 0.8\n3.8 ± 0.1\n94.0 ± 0.4\nDegree Model: In this model we only remove edges. Edges with higher degrees are more likely to\nbe removed to preserve the structure of the graph. Specifically, the probability of removing edge\n(i, j) is set to\nsijdidj\nP\nij sijdidj , where di is the degree of node vi and si,j is the (i, j) element of the graph\nadjacency.\nWe test the performance of T-GAE for large-scale network alignment on the degree perturbation\nmodel, as described in Section 5.2. We adopt the same setting as in Section 5.2 to train the T-GAE\naccording to (6) on small-size networks, i.e., Celegans, Arena, Douban, and Cora, and conduct\ntransfer learning experiments on the larger graphs, i.e., Dblp, and Coauthor CS. The trained T-GAE\nis used to generate node embedding for the graphs, and Algorithm 1 computes the assignment matrix.\nThe results presented in Table 5 are based on the average and standard deviation of the matching\naccuracy over 10 randomly generated perturbed samples.\n22\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nWe observe that the benefit of processing the NetSimile embeddings with GNNs is still significant in\nthis perturbation model as we observe up to 46% performance increase at the presence of perturbation.\nWhen testing on perturbed graphs at 1% level of edge removal, our proposed T-GAE consistently\noutperforms all the competing baselines, while being robust and efficient when performing network\nalignment under different perturbation models. Especially, on large-scale networks, T-GAE is able to\nachieve very high levels of matching accuracy for both Dblp and Coauthor CS, for p = 0%, 1%.\nThe benefit of our proposed T-GAE framework in improving the expressiveness of GNN still stands\nout if we compare the accuracy with WAlign and GAE. It also consistently achieves the best result\namong all baseline methods that are salable.\nH\nEfficiency analysis\nH.1\nRunning time comparison\nT-GAE is a scalable and efficient approach for network alignment. We analyze the efficiency\nof the proposed graph matching framework by comparing its running time with the competing\nalgorithms. T-GAE achieves at most ×2000 less running time, on graph matching tasks, compared\nto the optimization-based methods, as shown in Table 6. Compared to the existing GNN based\napproaches, T-GAE consistently achieves the shortest training time and inference time, this is because\nwe replace some message passing layers by the local MLP which serves as an attention function on\nthe output of all GNN layers, such models are empirically proved to be more efficient, at the same\ntime, prompting the expressiveness of GNN layers, to generate node embedding for graph matching.\nIt should be noted that T-GAE is also the only approach that is transferable, which means it does not\nneed to re-train on every pair of new graphs. T-GAE greatly improves the scalability of optimization\nbased methods, as well as the effectiveness of the existing GNN frameworks.\nFigure 5: Training time comparison (20 epoches) between T-GAE and WAlign for graph-matching.\nTGAE_s is the specific setting where we train the encoder GNN according to Equation (7), whereas\nTGAE_t means training according to Equation (8) on a family of graphs. (Celegans, Arenas, Cora,\nDouban)\n23\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nTable 6: Runtime(inference+matching) in seconds for the competing algorithms on graph matching\ntasks.\nAlgorithm\nCelegans\nArenas\nCora\nDouban\nDblp\nCoauthor CS\nSpectrul\n5.712\n2.819\n54.770\n60.298\n> 48 hours\n> 48 hours\nNetsimile\n1.212\n1.560\n1.616\n4.400\n195.542\n225.546\nGraphWave\n8.308\n32.994\n131.230\n281.629\n5470.724\n6291.368\nFINAL(Matlab)\n0.030\n0.081\n0.498\n1.007\n86.788\n118.065\nS-GWL\n27.844\n37.443\n311.201\n3394.522\n> 48 hours\n> 48 hours\nConeAlign\n1.333\n3.500\n13.799\n31.955\n887.090\n1099.145\nWAlign\n0.078\n0.205\n0.766\n1.800\n169.694\n189.032\nGAE\n0.074\n0.212\n0.757\n1.749\n164.410\n184.062\nT-GAE(ours)\n0.068\n0.201\n0.742\n1.734\n163.836\n183.289\nThe proposed training objective (7) scales well from networks with 400 nodes [Kunegis, 2013] to\ndenser networks with ×50 nodes [Shchur et al., 2018], with minor running time increase, compared\nto other GNN-based frameworks(WAlign), as shown in Figure 5.\nH.2\nMatching algorithms comparison\nIn this subsection, we evaluate the accuracy and matching time of different matching algorithms, to\ndemonstrate how matching algorithms of different time complexity influence the performance of\nthe proposed T-GAE framework. To guarantee fairness of comparison, we use an untrained T-GAE\nencoder to encode the graphs, and use (1) approximated NN algorithm introduced in Section 4.3 of\ntime complexity O(NlogN). (2) greedy Hungarian algorithm as applied in Section 5 and 5.3 of time\ncomplexity O(N 2). (3) exact Hungarian algorithm [Kuhn, 1955b] of time complexity O(N 3), where\nN is the number of nodes in the graph. We report average accuracy and running time on matching 10\nrandomly generated samples.\nTable 7: Graph matching performance and matching time on Celegans and Arenas using untrained\nT-GAE encoder. We report results of approximated NN matching algorithm, greedy Hungarian\nalgorithm, and exact Hungarian algorithm. We highlight the performance gain of exact Hungarian\nover the approximated NN and Greedy Hungarian.\n# Dataset/Perturbation\nCelegans\nArenas\n0\n0.01\n0.05\n0\n0.01\n0.05\nAcc\nTime\nAcc\nTime\nAcc\nTime\nAcc\nTime\nAcc\nTime\nAcc\nTime\nO(NlogN)\n1 Approximated NN\n89.8 ± 1.0 0.004\n0.8 ± 0.3\n0.004\n0.7 ± 0.2\n0.004\n98.0 ± 2.6 0.009\n0.1 ± 0.1\n0.009\n0.0 ± 0.0\n0.009\nO(N 2)\n2 Greedy Hungarian\n88.4 ± 0.9 0.068 80.3 ± 0.3\n0.068\n58.2 ± 3.5\n0.067\n97.6 ± 0.4 0.173 93.1 ± 0.5\n0.176\n60.2 ± 5.2\n0.174\nO(N 3)\n3 Exact Hungarian\n88.5 ± 0.8 0.225 84.0 ± 0.2 24.778 64.7 ± 1.8 73.674 97.6 ± 0.4 0.411 93.8 ± 0.4 562.221 70.0 ± 2.0 1624.713\nO(N 3)\n4 Acc Gain in % (Exact vs Approx/Greedy)\n-1.3 / +0.1\n+83.2 / +3.7\n+64.0 / +6.5\n-0.4 / 0\n+93.7 / +0.7\n+70.0 / +9.8\nThe performance of T-GAE to match small graphs can be further improved by adopting the\nexact Hungarian algorithm. Comparing Exact Hungarian and Greedy Hungarian, we observe that\nwhen there is no perturbation involved, greedy Hungarian and exact Hungarian achieves comparable\nperformance. However, the exact Hungarian algorithm outperforms the greedy version in occurrence\nof perturbations, and the performance gap increases as we introduce more topology noise. Specifically,\nit offers a 6.5% and 9.8% matching accuracy increase on Celegans and Arenas, respectively, on an\nuntrained T-GAE encoder. However, it can take more than ×9000 longer than the greedy algorithm,\non Arenas 5% perturbation, for example. This implies that whenever applicable, the exact Hungarian\nalgorithm should be applied to further improve the performance of T-GAE to match small graphs,\nespecially when the noise level is high, but there is a trade-off between matching accuracy and\nefficiency.\nThe efficiency of T-GAE to match permuted graphs can be enhanced by applying the approxi-\nmated NN algorithm. We observe that the approximated NN algorithm we introduced in Section\n4.3 effectively match the aligned nodes of permuted graphs, and on Arenas dataset of 1,133 nodes, it\n24\n\nT-GAE: Transferable Graph Autoencoder for Network Alignment\nsaves 95% matching time compared to the greedy Hungarian algorithm. However, this algorithm\nfails to match perturbed graphs. This is because the 1-dimensional feature is not expressive enough\nto to catch the topological noise. Our experiments prove that this efficient NN algorithm should be\napplied when we match very large scale networks with their permuted versions.\nOverall, we divide graph matching using T-GAE in three scenarios: (1) Matching small graphs, exact\nHungarian algorithm should be applied. (2) Matching large scale networks without perturbation, the\napproximation NN algorithm should be deployed to enhance efficiency. (3) The greedy Hungarian\nalgorithm provides a good trade-off between time and efficiency for the general form of graph\nmatching.\n25",
    "pdf_filename": "T-GAE_Transferable_Graph_Autoencoder_for_Network_Alignment.pdf"
}