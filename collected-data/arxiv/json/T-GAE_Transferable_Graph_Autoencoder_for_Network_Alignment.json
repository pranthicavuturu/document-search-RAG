{
    "title": "T-GAE: Transferable Graph Autoencoder for Network",
    "abstract": "Networkalignmentisthetaskofestablishingone-to-onecorrespondencesbe- tweenthenodesofdifferentgraphs. Althoughfindingaplethoraofapplications inhigh-impactdomains,thistaskisknowntobeNP-hardinitsgeneralform. Existing optimization algorithms do not scale up as the size of the graphs in- creases. While being able to reduce the matching complexity, current GNN approaches fit a deep neural network on each graph and requires re-train on unseensamples,whichistimeandmemoryinefficient. Totacklebothchallenges weproposeT-GAE,atransferablegraphautoencoderframeworkthatleverages transferabilityandstabilityofGNNstoachieveefficientnetworkalignmenton out-of-distribution graphs without retraining. We prove that GNN-generated embeddingscanachievemoreaccuratealignmentcomparedtoclassicalspec- tral methods. Our experiments on real-world benchmarks demonstrate that T-GAEoutperformsthestate-of-the-artoptimizationmethodandthebestGNN approachbyupto38.7%and50.8%,respectively,whilebeingabletoreduce 90%ofthetrainingtimewhenmatchingout-of-distributionlargescalenetworks. Weconductablationstudiestohighlighttheeffectivenessoftheproposeden- coder architecture and training objective in enhancing the expressiveness of GNNstomatchperturbedgraphs. T-GAEisalsoprovedtobeflexibletouti- lize matching algorithms of different complexities. Our code is available at https://github.com/Jason-Tree/T-GAE. 1 Introduction Network alignment, also known as graph matching, is a classical problem in graph theory, that aimstofindnodecorrespondenceacrossdifferentgraphsandisvitalinanumberofhigh-impact domains[Emmert-Streibetal.,2016]. Insocialnetworks,forinstance,networkalignmenthasbeen usedforuserdeanonymization[Nilizadehetal.,2014]andanalysis[Ogaardetal.,2013],whilein bioinformaticsitisakeytooltoidentifyfunctionalitiesinproteincomplexes[Singhetal.,2008], ortoidentifygene–drugmodules[Chenetal.,2018a]. Graphmatchingalsofindsapplicationin computervision[Conteetal.,2003],sociology[RaczandSridhar,2021],tonameafew. However, thisproblemisusuallycastasaquadraticassignmentproblem(QAP),whichisingeneralNP-hard. Variousapproacheshavebeendevelopedtotacklenetworkalignmentandcanbedividedintotwomain categories;i)optimizationalgorithmsthatattempttoapproximatetheQAPproblembyrelaxingthe combinatorialconstraints,ii)embeddingmethodsthatapproachtheproblembyimplicitlyorexplicitly generatingpowerfulnodeembeddingsthatfacilitatethealignmenttask. Optimizationapproaches,as [AnstreicherandBrixius,2001,Vogelsteinetal.,2015]employquadraticprogrammingrelaxations, while [Klau, 2009] and [Peng et al., 2010] utilize semidefinite or Lagrangian-based relaxations respectively,[Duetal.,2019]and[Duetal.,2022]proposedtosolvenetworkalignmenttogetherwith linkprediction. Successiveconvexapproximationswerealsoproposedby[KonarandSidiropoulos, 2020] to handle the QAP. Challenges associated with these methods include high computational cost,infeasiblesolutions,ornearlyoptimalinitializationrequirements. Embeddingmethods,onthe otherhand,overcomethesechallenges,buttheyusuallyproduceinferiorsolutions,duetoaninherent J.Heetal.,T-GAE:TransferableGraphAutoencoderforNetworkAlignment.ProceedingsoftheThirdLearning onGraphsConference(LoG2024),PMLR269,VirtualEvent,November26–29,2024. 4202 voN 81 ]GL.sc[ 4v27230.0132:viXra",
    "body": "T-GAE: Transferable Graph Autoencoder for Network\nAlignment\nJiashuHe CharilaosKanatsoulis AlejandroRibeiro\nUniversityofPennsylvania StanfordUniversity UniversityofPennsylvania\njiashuhe@seas.upenn.edu charilaos@cs.stanford.edu aribeiro@seas.upenn.edu\nAbstract\nNetworkalignmentisthetaskofestablishingone-to-onecorrespondencesbe-\ntweenthenodesofdifferentgraphs. Althoughfindingaplethoraofapplications\ninhigh-impactdomains,thistaskisknowntobeNP-hardinitsgeneralform.\nExisting optimization algorithms do not scale up as the size of the graphs in-\ncreases. While being able to reduce the matching complexity, current GNN\napproaches fit a deep neural network on each graph and requires re-train on\nunseensamples,whichistimeandmemoryinefficient. Totacklebothchallenges\nweproposeT-GAE,atransferablegraphautoencoderframeworkthatleverages\ntransferabilityandstabilityofGNNstoachieveefficientnetworkalignmenton\nout-of-distribution graphs without retraining. We prove that GNN-generated\nembeddingscanachievemoreaccuratealignmentcomparedtoclassicalspec-\ntral methods. Our experiments on real-world benchmarks demonstrate that\nT-GAEoutperformsthestate-of-the-artoptimizationmethodandthebestGNN\napproachbyupto38.7%and50.8%,respectively,whilebeingabletoreduce\n90%ofthetrainingtimewhenmatchingout-of-distributionlargescalenetworks.\nWeconductablationstudiestohighlighttheeffectivenessoftheproposeden-\ncoder architecture and training objective in enhancing the expressiveness of\nGNNstomatchperturbedgraphs. T-GAEisalsoprovedtobeflexibletouti-\nlize matching algorithms of different complexities. Our code is available at\nhttps://github.com/Jason-Tree/T-GAE.\n1 Introduction\nNetwork alignment, also known as graph matching, is a classical problem in graph theory, that\naimstofindnodecorrespondenceacrossdifferentgraphsandisvitalinanumberofhigh-impact\ndomains[Emmert-Streibetal.,2016]. Insocialnetworks,forinstance,networkalignmenthasbeen\nusedforuserdeanonymization[Nilizadehetal.,2014]andanalysis[Ogaardetal.,2013],whilein\nbioinformaticsitisakeytooltoidentifyfunctionalitiesinproteincomplexes[Singhetal.,2008],\nortoidentifygene–drugmodules[Chenetal.,2018a]. Graphmatchingalsofindsapplicationin\ncomputervision[Conteetal.,2003],sociology[RaczandSridhar,2021],tonameafew. However,\nthisproblemisusuallycastasaquadraticassignmentproblem(QAP),whichisingeneralNP-hard.\nVariousapproacheshavebeendevelopedtotacklenetworkalignmentandcanbedividedintotwomain\ncategories;i)optimizationalgorithmsthatattempttoapproximatetheQAPproblembyrelaxingthe\ncombinatorialconstraints,ii)embeddingmethodsthatapproachtheproblembyimplicitlyorexplicitly\ngeneratingpowerfulnodeembeddingsthatfacilitatethealignmenttask. Optimizationapproaches,as\n[AnstreicherandBrixius,2001,Vogelsteinetal.,2015]employquadraticprogrammingrelaxations,\nwhile [Klau, 2009] and [Peng et al., 2010] utilize semidefinite or Lagrangian-based relaxations\nrespectively,[Duetal.,2019]and[Duetal.,2022]proposedtosolvenetworkalignmenttogetherwith\nlinkprediction. Successiveconvexapproximationswerealsoproposedby[KonarandSidiropoulos,\n2020] to handle the QAP. Challenges associated with these methods include high computational\ncost,infeasiblesolutions,ornearlyoptimalinitializationrequirements. Embeddingmethods,onthe\notherhand,overcomethesechallenges,buttheyusuallyproduceinferiorsolutions,duetoaninherent\nJ.Heetal.,T-GAE:TransferableGraphAutoencoderforNetworkAlignment.ProceedingsoftheThirdLearning\nonGraphsConference(LoG2024),PMLR269,VirtualEvent,November26–29,2024.\n4202\nvoN\n81\n]GL.sc[\n4v27230.0132:viXra\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nFigure1: ToenhancetheexpressivenessofGNNstoaligntheunseengraphs,ourproposedencoder\nprocessestheinputfeaturesbyalocalMLP,wethenincorporateattentionmechanismon(1)inputto\neachmessage-passinglayerbyattendingtotheoutputofthelastlayerandprocessedinputfeature.\n(2)outputoftheencoderbyattendingtotheoutputofeachmessagepassinglayer.\ntrade-off between embedding permutation-equivariance and the ability to capture the structural\ninformationofthegraph. Typicalembeddingtechniquesincludespectralandfactorizationmethods\n[Umeyama,1988,Feizietal.,2019,ZhangandTong,2016,KanatsoulisandSidiropoulos,2022],\nstructuralfeatureengineeringmethods[Berlingerioetal.,2013,Heimannetal.,2018],andrandom\nwalkapproaches[Perozzietal.,2014,GroverandLeskovec,2016a]. Recently[Chenetal.,2020,\nKarakasisetal.,2021]haveproposedjointnodeembeddingandnetworkalignment,toovercome\nthesechallenges,butthesemethodsdonotscaleupasthesizeofthegraphincreases.\nGraphNeuralNetworks(GNNs)arepowerfularchitecturesthatlearngraphrepresentations(em-\nbeddings)inaself-supervisedway[KipfandWelling,2016,Youetal.,2020]. Theyhaveshown\nstate-of-the-artperformanceinseveraltasks,includingbiomedical[Gainzaetal.,2020,Strokach\netal.,2020,Jiangetal.,2021,Huetal.,2023,Lietal.,2024],quantumchemistry[Gilmeretal.,\n2017], social networks and recommender systems [Ying et al., 2018, Wu et al., 2020, Liu et al.,\n2023a,Yangetal.,2024]. Alineofstudieshasbeenconductedtoformulatekey-pointmatchingon\nimagesasgraphmatchingproblems[Feyetal.,2020,Yuetal.,2020]. Theseframeworksrelyona\npowerfuldomain-specificencoder(CNNs,forexample)toprovidehigh-qualityfeatures. Giventhese\nhigh-qualityfeatures,GNNsareabletomatchgraphswithouttraining[Liuetal.,2022]. However,\nexpressivefeaturesareexpensiveandsometimesinfeasibletobuild[Zhangetal.,2020,Zhouetal.,\n2021]. [Liangetal.,2021]usesanothertrainablematrixtoparameterizetheinternalconnectivity\nbetweennodesofdifferentgraphsforfaithfulnodealignment. Recently,[Gaoetal.,2021]proposed\nuseGNNstolearnnodeembedding,andmatchnodeswithsmallWassersteindistances. However,\nthismethodneedstofitaGNNoneveryinputgraph,whichresultsinveryhightrainingcost,since\ntrainingdeepGNNswithlargesizesgraphsiscomputationallyprohibitive,asGNNshavelimited\nscalabilitywithrespecttographandmodelsizes[Chenetal.,2018b,c,Chiangetal.,2019,Zeng\netal.,2020].\nToaddressthesechallenges,weproposeT-GAE,anovelself-supervisedGNNframeworktoperform\nnetworkalignment. Specifically,weproposetoutilizethetransferabilityandrobusenessofGNNto\nproducepermutationequivariantandhighlyexpressiveembeddings. T-GAEtrainstheencoderon\nmultiplefamiliesofsmallgraphsandproduceexpressive/permutationequivariantrepresentationsfor\nlargerunseennetworks. WefurtherprovethatGNNrepresentationscombinetheeigenvectorsofthe\ngraphinanonlinearfashionandareatleastasgoodinnetworkalignmentascertainspectralmethods.\nT-GAEisaone-shotsolutionthattacklesthechallengesofreal-timenetworkalignmentfrom(1)\nOptimizationbasedalgorithms: highcomputationalcost,assumegroundtruthnodecorrespondence\nas initialization. (2) Deep-learning based frameworks: re-train for every pair of graphs, rely on\nhighqualityofnodefeatures. Extensiveexperimentswithreal-worldbenchmarksdemonstratethe\neffectivenessandefficiencyoftheproposedapproachinbothgraphandsub-graphmatching,thereby\nshedslightonthepotentialofGNNtotacklethehighly-complexnetworkoptimizationproblems.\n2\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nFigure 2: Proposed training objective: For each of the training graph, we generate a number of\naugmentedsamplesbyrandomlyaddingorremovingedges.Thenodeembeddingoftheseaugmented\nsamplesaredecodednon-parametrically,andcomparedwiththecorrespondingoriginalgraphtotrain\ntheT-GAEencoder.\n2 Preliminaries\nGraphsarerepresentedbyG := (V,E),whereV = {1,...,N}isthesetofvertices(nodes)and\nE ={(v,u)}correspondtoedgesbetweenpairsofvertices. Agraphisrepresentedinamatrixform\nbyagraphoperatorS ∈ RN×N,whereS(i,j)quantifiestherelationbetweennodeiandnodej\nandN =|V|isthetotalnumberofvertices. Inthiswork,weusethenormalizedgraphadjacency\nandstudythemostgeneralformofnetworkalignmentwherethereisnogivengraphattributesand\ngroundtruthnodecorrespondence(anchorlinks).\n2.1 NetworkAlignment\nDefinition2.1(NetworkAlignment). GivenapairofgraphsG :=(V,E), Gˆ:=(Vˆ,Eˆ),withgraph\nadjacencies S, Sˆ, network alignment aims to find a bijection g : V → Vˆ which minimizes the\nnumberofedgedisagreementsbetweenthetwographs. Formally,theproblemcanbewrittenas:\n(cid:13) (cid:13)2\nmin (cid:13)S−PSˆPT (cid:13) , (1)\n(cid:13) (cid:13)\nP∈P F\nwhereP isthesetofpermutationmatrices.\nAs mentioned in the introduction, network alignment, is equivalent to the QAP, which has been\nproventobeNP-hard[KoopmansandBeckmann,1957].\n2.2 SpectralDecompositionoftheGraph\nApopularapproachtotacklenetworkalignmentisbylearningpowerfulnodeembeddingsassociated\nwithconnectivityinformationinthegraph. Networkalignmentcanbeachievedbymatchingthe\nnodeembeddingsofdifferentgraphsratherthangraphadjacencies,asfollows:\n(cid:13) (cid:13)2\nmin (cid:13)E−PEˆ (cid:13) , (2)\n(cid:13) (cid:13)\nP∈P F\nwhere E ∈ RN×F is embedding matrix and E[i,:] is the vector representation of node i. The\noptimizationproblemin(2)isalinearassignmentproblemandcanbeoptimallysolvedinO(cid:0) N3(cid:1)\nbytheHungarianmethod[Kuhn,1955a]. Simplersub-optimalalternativesalsoexistthatoperate\nwithO(cid:0) N2(cid:1)\norO(N log(N))flops.\nAquestionthatnaturallyarisesishowtogeneratepowerfulnodeembeddingsthatcapturethenetwork\nconnectivity and also be effective in aligning different graphs. A natural and effective approach\nistoleveragethespectraldecompositionofthegraph,S =VΛVT,whereV istheorthonormal\nmatrixoftheeigenvectors,andΛisthediagonalmatrixofcorrespondingeigenvalues. Notethat\nweassumeundirectedgraphsandthusS issymmetric. Spectraldecompositionhasbeenprovento\n3\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nbeanefficientapproachtogeneratingmeaningfulnodeembeddingforgraphmatching[Umeyama,\n1988,Feizietal.,2019]. Inparticular,E =V orE =VΛarenodeembeddingsthatcapturethe\nnetworkconnectivitysincetheycanperfectlyreconstructthegraph. However,V isnotunique. Thus\ncomputingthespectraldecompositionofthesamegraphwithnoderelabelling,S˜ =PSPT isnot\nguaranteedtoproduceapermutedversionofV,i.e.,PV. EveninthecasewhereS doesnothave\nrepeatedeigenvaluesV isonlyuniqueuptocolumnsign,whichpreventseffectivematching.\nToovercometheaforementioneduniquenesslimitation,onecanfocusonthetopmeigenvectorsthat\ncorrespondtonon-repeatedeigenvaluesinbothS andSˆandcomputetheirabsolutevalues. Then\nnetworkalignmentcanbecastas:\n(cid:13) (cid:12) (cid:12) (cid:13)2\nmin (cid:13) |V |−P (cid:12)Vˆ (cid:12) (cid:13) , (3)\n(cid:13) m (cid:12) m(cid:12) (cid:13)\nP∈P F\nwhereV ∈RN×mcorrespondstothesubspaceofnon-repeatedeigenvalues. Theformulationin\nm\n(3)isasimilartotheproblemsolvedin[Umeyama,1988].\n3 GraphNeuralNetworks(GNNs)Upper-BoundsSpectralMethodsfor\nNetworkAlignment\nAGNNisacascadeoflayersandperformslocal,message-passingoperationsthatareusuallydefined\nbythefollowingrecursiveequation:\n(cid:16) (cid:16)(cid:110) (cid:111)(cid:17)(cid:17)\nx(l+1) =g x(l),f x(l) :u∈N (v) , (4)\nv v u\nwhereN (v)istheneighborhoodofvertexv,i.e.,u∈N (v)ifandonlyif(u,v)∈E. Thefunction\nf operatesonmultisets({·}representsamultiset)andf, gareideallyinjective. Commonchoices\nforf arethesummationormeanfunction,andforgthelinearfunction,orthemulti-layerperceptron\n(MLP).\nOverall, the output of the L−th layer of a GNN is a function ϕ(X;S,H) : RN×D → RN×DL,\nwhereS isthegraphoperator,andHisthetensorofthetrainableparametersinallLlayersand\nproducesD −dimensionalembeddingsforthenodesofthegraphdefinedbyS.\nL\nGNNsadmitsomeveryvaluableproperties. First,theyarepermutationequivariant:\nTheorem3.1([Xuetal.,2019a,Maronetal.,2018]). Letϕ(X;S,H) : RN×D → RN×DL bea\nGNN with parameters H. For X˜ = PX and S˜ = PSPT that correspond to node relabelling\naccordingtothepermutationmatrixP,theoutputoftheGNNtakestheform:\n(cid:16) (cid:17)\nX˜(L) =ϕ X˜;S˜,H =Pϕ(X;S,H) (5)\nTheabovepropertyisnotsatisfiedbyotherspectralmethods. GNNsarealsostable[Gamaetal.,\n2020,Parada-Mayorgaetal.,2023,Ruizetal.,2021],transferable[Ruizetal.,2020],andhavehigh\nexpressivepower[Xuetal.,2019a,Abboudetal.,2021,KanatsoulisandRibeiro,2022].\n3.1 GNNsandNetworkAlignment\nTocharacterizetheabilityofaGNNtoperformnetworkalignmentwefirstpointedouttheGNNs\nperformnonlinearspectraloperations. DetailscanbefoundinAppendixSectionD.1. Wecanfurther\nprovethat:\nTheorem3.2. LetG, GˆbegraphswithadjacenciesS, Sˆthathavenon-repeatedeigenvalues. Also\nletP⋄, Pˇ besolutionstotheoptimizationproblemsin(1)and(3)respectively. Thenthereexistsa\nGNNϕ(X;S,H):RN×D →RN×DL suchthat:\n(cid:13) (cid:13)S−P⋄SˆP⋄T(cid:13) (cid:13)2 ≤(cid:13) (cid:13)S−P∗SˆP∗T(cid:13) (cid:13)2 ≤(cid:13) (cid:13)S−PˇSˆPˇT(cid:13) (cid:13)2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\nF F F\nwith\n(cid:13) (cid:16) (cid:17) (cid:13)2\nP∗ =argmin (cid:13)ϕ(X;S,H)−Pϕ Xˆ;Sˆ,H (cid:13) .\n(cid:13) (cid:13)\nP∈P F\n4\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nTheproofcanbefoundinAppendixD.Theassumptionthatthegraphadjacencieshavedifferent\neigenvaluesisnotrestrictive. Realnonisomorphicgraphshavedifferenteigenvalueswithveryhigh\nprobability[HaemersandSpence,2004]. Theorem3.2comparesthenetworkalignmentpowerof\naGNNwiththatofaspectralalgorithmUmeyama[1988],thatusestheabsolutevaluesofgraph\nadjacencyeigenvectorstomatchtwodifferentgraphs. AccordingtoTheorem3.2therealwaysexists\naGNNthatcanperformatleastaswellasthespectralapproach. TheproofstudiesaGNNwithwhite\nrandominputandmeasuresthevarianceofthefilteroutput. Thenitshowsthatmessage-passing\nlayersareabletocomputetheabsolutevaluesofthegraphadjacencyeigenvectorswhentheadjacency\nhasnon-repeatedeigenvalues. AsaresulttherealwaysexistsasinglelayerGNNthatoutputsthe\nsamenodefeaturesastheonesusedinUmeyama[1988],whichconcludesourproof. Thequestions\nis: HowdowetrainsuchaGNNthatis(1)Expressivetothestructuralinformationthusitsoutput\ncanbeusedtomatchcorrespondingnodes. (2)Robusttodifferentperturbationsandevenlargerscale\nunseengraphssothatitcanbedeployedefficientlywithoutre-training. (3)Agnostictotheground\ntruthnodecorrespondencesothatitcanbetrainedunsupervisedly,whichmakesitgeneralizableto\nmostreal-worldsettings. Toanswerthesequestions,weintroduceourproposedtrainingframework\ninthefollowingsection.\n4 ProposedMethod\nWe now leverage the favorable properties of GNNs (permutation equivariance, expressivity, and\ntransferability)totacklereal-worldnetworkalignment. Ourapproachlearnslow-dimensionalnode\nembeddings(Eq. 4)thatenablegraphmatchingviasolvingthelinearassignmentin(2)ratherthan\na quadratic assignment problem in (1). We design a robust GNN framework such that the node\nembeddingsareexpressivetoaccuratelymatchsimilarnodesandalsostabletographperturbations.\n4.1 LearningNetworkGeometrywithTransferableGraphAuto-encoders\nThegoaloftheproposedframeworkistolearnafunctionthatmapsgraphstonoderepresentations\nandeffectivelymatchnodesfromdifferentgraphs. ThisfunctionismodeledbyaGNNencoder\nϕ(X;S,H),describedbyFig. 1. Thelearnedencodershouldworkforafamilyoftraininggraphs\n{G ,...,G ,...,G }withasetofadjacencymatricesS={S ,...,S ,...,S },ratherthanasingle\n0 i I 0 i I\ngraph. So the idea is not to train a GNN on a single graph [Kipf and Welling, 2016], but train a\ntransferablegraphauto-encoderbysolvingthefollowingoptimizationproblem.\n(cid:104) (cid:16) (cid:16) (cid:17) (cid:17)(cid:105)\nmin E l ρ ϕ(X;S ,H)ϕ(X;S ,H)T ,S , (6)\ni i i\nH\nwherel(·)isthebinarycrossentropy(BCE)andρ(·)isthelogisticfunction. S ∈Sisarealization\ni\nfromafamilyofgraphsandtheexpectation(empiricalexpectationispractice)iscomputedoverthis\ngraphfamily.Thegeneralizedframeworkin(6)learnsamappingfromgraphstonoderepresentations,\nandcanbeappliedtoout-of-distributiongraphsthathavenotbeenobservedduringtraining. This\ntwist in the architecture enables node embedding and graph matching for the unseen and larger\nscalenetworkswithoutre-training,wherefittingaGNNiscomputationallyprohibitiveinreal-world\napplications.\n4.2 RobustandGeneralizableNoderepresentationswithself-supervisedlearning(data\naugmentation)\nSo far we proposed a GNN framework to produce expressive node representations to perform\nnetworkalignment. Inthissubsection,wefurtherupgradeourframeworkbyensuringtherobustness\nand generalization ability of the proposed mapping. In particular, for each graph, S ∈ S, we\ni\naugmentthetrainingsetwithperturbedversionsthataredescribedbythefollowingsetofgraph\n(cid:110) (cid:111)\nadjacenciesM = S(0),...,S(j),...,S(J) ,thatareperturbedversionsofS . Todosoweadd\ni i i i i\nor remove an edge with a certain probability yielding S˜ ∈ M, such that S˜ = S +M , where\ni i i i\nM ∈ {−1,0,1}N×N. NotethatM changesforeachS˜ ,andM[m,n]canbeequalto1and−1\ni i\nonlyifS[m,n]isequalto0and1respectively. Totraintheproposedtransferablegraph-autoencoder\nweconsiderthefollowingoptimizationproblem:\n(cid:20) (cid:20) (cid:18) (cid:18) (cid:16) (cid:17) (cid:16) (cid:17)T(cid:19) (cid:19)(cid:21)(cid:21)\nm Hin E S E M i l ρ ϕ X;S˜ i,H ϕ X;S˜ i,H ,S i , (7)\n5\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nFigure 3: The pre-trained encoder operates on out-of-distribution samples. The generated node\nembeddingsarethenmatchedgreedily.\nwhereE SistheexpectationwithrespecttothefamilyofgraphsSandE M istheexpectationwith\nrespecttotheperturbedgraphsM i. Inpractice,E S, E Mcorrespondtoempi iricalexpectations. Note\nthat training according to (7) also benefits the robustness of the model, which is crucial in deep\nlearningtasks[Wangetal.,2022,He,2021]. Aschematicillustrationofthetrainingprocesscanbe\nfoundinFig. 2.\nRemark4.1. (Large-scalenetworkalignmentbytransferabilityofGNN)\nTheproposedframeworklearnsamappingϕ : G → RN×F thatproducesexpressiveandrobust\nnoderepresentationsforafamilyofgraphsG ∈G. Thismappingisdesignedinsuchawaythatthe\nproblemin(2)approximatestheproblemin(1)andallowssolvingnetworkalignmentinpolynomial\ntime. One of the main benefits of the proposed framework is that it enables large-scale network\nalignment. Taskspecificaugmentationduringtrainingisthekeytoprompttransferabilityofdeep\nneural networks [Li et al., 2022]. And the transferability analysis of GNN encoders [Ruiz et al.,\n2020]suggeststhatwecantrainwithsmallgraphsandefficientlyexecutewithmuchlargergraphs\nwhenthesubstructures(motifs)thatappearinthetestedgraphs,werealsopartiallyobservedduring\ntraining. Sincetheproposedtransferablegraphauto-encoderistrainedwithmultiplegraphs,avariety\nofmotifsareobservedduringtraining,whichcannotbeobservedwithaclassicalgraphautoencoder,\nandtheproposedGNNencodercanbetransferredtolarger-scaleout-of-distributiongraphs.\n4.3 AlignmentandComplexityanalysis\nAfterlearningthepowerfulnodeembeddings,networkalignmentisperformedbysolvingthelinear\nassignment problem in (2). An illustration of the assignment is presented in Fig. 3. The node\nfeatures produced by T-GAE are used to calculate a pairwise distance matrix, followed by the\ngreedyHungarianalgorithmtopredictnodecorrespondences. Toanalyzethecomplexityofour\napproachwestudythe3mainpartsofT-GAE:a)Thedesignoftheinputstructuralfeatures,b)The\nmessage-passingGNNthatproducesnodeembeddings,andc)thelinearassignmentalgorithm.\nThecomputationofourneighborhood-basedstructuralfeaturesisexpectedtotakeO(|V|)inreal\ngraphs, as proved in Henderson et al. [2011]. The computational and memory complexity of\nthe message-passing GNN is\nO(cid:0) |V|c2+|E|c(cid:1)\n, and O(|V|c), where c is the width of the GNN.\n(cid:16) (cid:17)\nThecomputationalcomplexitytoalignthenodesofthegraphisO |V|2 sinceweareusingthe\nsuboptimalgreedyHungarian. Ifwewanttooptimallysolvethelinearassignmentproblemweneed\n(cid:16) (cid:17)\ntousetheHungarianalgorithmthathasO |V|3 complexity. Ifwewanttoprocesslargegraphs\nwecanuseefficientnearestneighborsalgorithmswithcomplexityO(|V|log(|V|))toperformsoft\nlinearassignment. However,thisefficientalgorithmonlyworkstomatchgraphswithitspermuted\nsamples. WeincludedetaileddiscussioninAppendixSectionH.OverallthecomplexityofT-GAEis\nO(cid:16) |V|2(cid:17) ,orO(cid:0) |V|c2+|E|c+|V|log(|V|)(cid:1)\nforun-perturbedsamples.\n5 Experiments\n5.1 ExperimentsSetup\nTheexperimentsincludedinthissectionaredesignedtoanswerthefollowingresearchquestions:(1)\nCan T-GAE generate competing graph matching accuracy on real-world networks from various\n6\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\ndomains with different sizes, while being efficient by utilizing transferability of GNN for large-\nscale our-of-distribution graphs? We answer this question in Section 5 and Appendix Section G\nby comparing the performance on matching small to middle sized graphs with unseen perturbed\nsamples,andlargescaleout-of-distributionnetworksunderdifferentperturbationdistributions. (2)Is\nT-GAErobusttodifferentgraphmatchingtasksandreal-worldnoisedistributions? InSection5.3,we\nconductsub-graphmatchingexperimentstoaligntwodifferentreal-worldnetworksthatarepartially\naligned. (3)Howdoestheproposednetworkarchitecture(Figure1)andthetrainingobjective(Figure\n2)contributetonetworkalignment? Todemonstratethecontributionofeachproposedcomponent,\nweconductablationstudiesinSection5.4,tocompareT-GAEwithuntrainedT-GAE,T-GAEtrained\nwithGAEobjective,andGAE.(4)HowmuchefficiencydoesT-GAEoffercomparedtotheexisting\noptimization and GNN approaches, and what are the possible trade-offs between efficiency and\nmatching accuracy of T-GAE? We compare the efficiency of different matching algorithms and\nempiricallyprovethat(a)ThematchingaccuracyofT-GAEcanbefurtherimprovedbyleveraging\ntheexactHungarianalgorithm. (b)TheefficiencyofT-GAEtomatchgraphswiththeirpermuted\nsamplescanbeenhancedifweadoptthemoreefficientmatchingalgorithmintroducedinSection4.3.\nThissetofexperimentsareincludedinAppendixSectionH.\nForeachoftheabovementionedexperiments,wecompareT-GAEwiththreecategoriesofgraph\nmatching approaches: (a)GNN based methods: WAlign [Gao et al., 2021], GAE and VGAE [Kipf\nandWelling,2016];(b)Graph/Nodeembeddingtechniques: NetSimile[Berlingerioetal.,2013],\nSpectral [Umeyama, 1988], DeepWalk [Perozzi et al., 2014], [Grover and Leskovec, 2016b],\nGraphWave[Donnatetal.,2018]andLINE[Tangetal.,2015].(c)Optimizationbasedgraphmatching\nalgorithms: S-GWL[Xuetal.,2019b],ConeAlign[Chenetal.,2020]andFINAL[ZhangandTong,\n2016]. Note that LINE, VGAE, DeepWalk, and Node2Vec are omitted from some experiments\nsince they show very poor performance. The reason behind that is that they are not permutation\nequivariant. GraphWave is also excluded from the sub-graph matching experiment, it could not\nidentifycorrelatednodesintwodifferentgraphs. InthecaseofgraphswithoutattributesFINALis\nequivalenttothepopularIsorank[Singhetal.,2008]algorithm,andFINALisomittedinsub-graph\nmatchingexperimentsduetoweakperformance.\nWeincludedetaileddescriptionsofourincludeddatasets,implementationdetailsofT-GAEandall\nthecompetingbaselinesinAppendixSectionE.\n5.2 GraphMatchingExperiments\nInthissubsectionwecomparetheperformanceofT-GAEwithallcompetingbaselinestomatchthe\ngraphswithpermutedandperturbedversionsofthem. Inparticular,letG beagraphwithadjacency\nmatrix S. We then produce 10 permuted-perturbed versions according to Sˆ = P (S+M)PT,\nwhere M ∈ {−1,0,1}N×N and P is a permutation matrix. For each perturbation level p ∈\n{0,1%,5%},thetotalnumberofperturbationsisdefinedasp|E|,where|E|isthenumberofedges\noftheoriginalgraph.\nSpecifically, we train T-GAE according to (7), where S consist of the small-size networks, i.e.,\nCelegans,Arena,Douban,andCora. ThenweresorttotransferlearningandusetheT-GAEencoder\ntoproducenodeembeddingforperturbedversionsof(a)Celegans,Arena,Douban,andCora,and\n(b) larger graphs, i.e., Dblp, and Coauthor CS. Note that none of these perturbed versions were\nconsideredduringtraining. ThisisincontrastwithallGNNbaselinesthatareretrainedonevery\npairofnetworksinthetestingdataset. Wereporttheaverageandstandarddeviationofthematching\naccuracyfor10randomlygeneratedperturbationsamplesunderuniformedgeeditinginTable1,\nwhereeachedgeandnon-edgesharesthesameprobabilityofbeingremovedoraugmented. We\nreporttheresultsforremovingedgesaccordingtodegreesandtherelevantdiscussedinAppendixG.\nT-GAEframeworkresultsinarobustandtransferableGNNtoperformnetworkalignment\nat a large scale. Our first observation is that for zero perturbation most algorithms are able to\nachieveahighlevelofmatchingaccuracy. Thisisexpected,sinceforzeroperturbationthenetwork\nalignmentisequivalenttographisomorphism. Onthesmallernetworks(Celegans,Arenas,Douban,\nCora),T-GAEperformsatleastaswellasthecurrentstate-of-the-artoptimizationapproaches(S-GWL\nandConeAlign). Specifically,itachievesupto38.7%and44.7%accuracyincreasecomparedto\nS-GWLandConeAlign,respectively. RegardingtheabilityofT-GAEtoperformlarge-scalenetwork\nalignment the results are definitive. T-GAE enables low-complexity training with small graphs,\nandexecutionatlargersettingsbyleveragingtransferlearning,anditconsistentlyoutperformsall\n7\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nTable1: Graphmatchingaccuracyon10randomlyperturbedsamplesunderdifferentlevelsofedge\neditingonUniformmodel. TheproposedT-GAEistrainedonthecleanCelegans,Arena,Douban,\nand Cora networks, and tested on noisy versions of them and the larger Dblp, and Coauthor CS.\nAccuracyabove80%ishighlightedingreen,60%to80%accuracyisinyellow,andperformance\nbelow60%isinred.\nFeatureEngineeringbased Optimizationbased GNNbased\nDataset\\Algorithm\nSpectral Netsimile GraphWave FINAL S-GWL ConeAlign WAlign GAE T-GAE\nCelegans 87.8±1.5 72.7±0.9 65.3±1.7 92.2±1.2 93.0±1.5 66.6±1.2 88.4±1.6 86.3±1.3 91.0±1.1\nArenas 97.7±0.4 94.7±0.3 81.7±0.7 97.5±0.3 97.5±0.3 87.8±0.6 97.4±0.5 97.6±0.4 97.8±0.4\nCora 85.0±0.4 73.7±0.4 8.3±0.4 87.5±0.7 87.3±0.7 38.5±0.7 87.2±0.4 87.1±0.8 87.5±0.4\nDouban 89.9±0.4 46.4±0.4 17.5±0.2 89.9±0.3 90.1±0.3 68.1±0.4 90.0±0.4 89.5±0.4 90.1±0.3\nDblp 84.5±0.1 63.7±0.2 doesn’tscale 85.6±0.2 >48hours 44.3±0.6 85.6±0.2 85.2±0.3 85.6±0.2\nCoauthorCS 97.5±0.1 90.9±0.1 doesn’tscale 97.6±0.1 >48hours 75.8±0.5 97.5±0.2 97.6±0.3 97.6±0.1\nCelegans 68.5±16.1 66.3±3.8 22.5±22.4 33.2±7.8 87.1±6.1 60.9±2.5 80.7±3.0 33.2±8.4 86.5±1.1\nArenas 85.0±10.0 87.8±1.0 40.5±23.8 32.5±5.9 94.2±0.7 84.6±1.0 90.0±3.1 30.1±17.6 96.0±1.0\nCora 59.1±9.3 66.4±1.6 3.7±2.9 30.0±3.3 46.4±6.9 33.5±1.6 80.1±1.2 57.9±5.3 85.1±0.5\nDouban 25.8±27.2 40.0±1.2 9.9±5.9 27.8±5.7 72.1±0.7 64.7±0.4 77.2±4.8 38.3±16.4 87.3±0.4\nDblp 55.6±19.0 55.1±1.7 doesn’tscale 15.2±3.3 >48hours 37.8±1.1 73.1±1.6 19.4±0.6 83.3±0.4\nCoauthorCS 58.2±22.1 75.2±2.2 doesn’tscale 13.3±5.0 >48hours 68.5±2.8 75.2±5.4 49.5±7.8 93.2±0.8\nCelegans 24.9±15.9 41.1±13.0 7.6±9.2 10.4±2.7 68.3±12.7 50.5±3.4 42.4±21.1 6.5±2.4 69.2±2.1\nArenas 52.1±16.5 52.3±5.3 6.9±7.2 7.2±2.6 88.3±3.2 75.0±2.7 30.4±17.5 1.4±1.4 81.2±1.4\nCora 29.5±0.8 41.2±3.3 0.8±0.3 6.7±2.8 39.9±5.5 23.0±2.0 33.4±7.3 9.6±2.7 67.7±1.3\nDouban 23.8±20.6 20.7±4.6 1.9±2.8 7.8±3.0 68.6±0.8 54.1±1.2 36.6±13.4 0.6±0.3 70.2±2.5\nDblp 28.0±7.8 19.5±4.8 doesn’tscale 2.7±0.9 >48hours 24.4±2.9 15.9±8.3 1.4±0.2 60.8±1.9\nCoauthorCS 9.7±5.0 26.3±6.0 doesn’tscale 2.0±0.4 >48hours 51.4±5.1 11.3±7.5 0.6±0.1 66.0±1.4\nTable2: Subgraphmatchingperformancecomparison. TheproposedT-GAEistrainedonthetwo\nreal-worldgraphs,andtesttomatchthealignedportionofthem.\nACM-DBLP DoubanOnline-Offline\nAlgorithm\\HitRate\nHit@1 Hit@5 Hit@10 Hit@50 Hit@1 Hit@5 Hit@10 Hit@50\nNetsimile 2.59% 8.32% 12.09% 26.42% 1.07% 2.77% 4.74% 15.03%\nSpectral 1.40% 4.62% 7.21% 16.34% 0.54% 1.34% 2.95% 13.95%\nGAE 8.1% 22.5% 30.1% 45.1% 3.3% 9.2% 14.1% 32.1%\nWAlign 62.02% 81.96% 87.31% 93.89% 36.40% 53.94% 67.08% 85.33%\nT-GAE 73.89% 91.73% 95.33% 98.22% 36.94% 60.64% 69.77% 89.62%\ncompetingbaselinesonthetwonetworkswithmorethan10knodes. Inparticular,itisabletoachieve\nveryhighlevelsofmatchingaccuracyforbothDblpandCoauthorCS,forp=0%, 1%. Itisalsothe\nonlymethodthatconsistentlyachievesatleast60%accuracyat5%perturbation. Tothebestofour\nknowledge,ourexperimentsonDBLP[Panetal.,2016]andCoauthorCS[Shchuretal.,2018]are\nthefirstattemptstoperformexactalignmentonnetworksattheorderof20knodesand80kedges.\nThebenefitsofprocessingstructuralnodefeatureswithT-GAEisclear. Thereisaclearbenefit\nofprocessingthestructuralembeddingswithTGAEsinceitoffersupto43.7%performanceincrease\ncomparedtoNetSimile. Whensomeperturbationisadded,theconclusionsarestraightforward. Es-\npeciallywhenperturbationsareadded,ourproposedT-GAEmarkedlyoutperformsallthecompeting\nGNNalternatives(WAlignandGAE)andshowsthedesiredrobustnesstoefficientlyperformnetwork\nalignment. WeobservethatneitherGAEnorWAlignisrobusttonoise. Thishighlightsthebenefit\nofT-GAEinhandlingthedistributionshiftbroughtbythestructuraldissimilaritybetweendifferent\ngraphs.\n5.3 Sub-graphMatchingExperiments\nWefurthertesttheperformanceofT-GAEinmatchingsubgraphsofdifferentnetworksthathave\nalignednodes(nodesthatrepresentthesameentitiesindifferentnetworks). Specifically,inACM-\nDBLP,thetaskistomatchthepapersthatappearinbothcitationnetworks;inDoubanOnline-Offline,\nweaimtoidentifytheusersthattakepartintobothonlineandofflineactivities. Notethatthisisthe\nmostrealisticgraphmatchingexperimentwecanperformsincewematch2differentrealgraphswith\npartiallyalignednodes.\nT-GAE uniformly achieves the best performance in the most realistic scenario of network\nalignment. Most optimization based approaches do not generalize to this real-world scenario\nbecausetheiroptimizationobjectiveusuallypreventsfrommatchinggraphswithdifferentnumbers\nof nodes. There is a significant improvement in matching accuracy with GNN-based methods\n8\nnoitabrutrep%0\nnoitabrutrep%1\nnoitabrutrep%5\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nFigure4: GraphmatchingperformancecomparisonofT-GAE,T-GAEtrainedonasinglegraph(T-\nGAEsingle),theuntrainedT-GAE(T-GAEuntrained),andGAE.Theproposedtrainingobjective\nandencoderstructurehelpstoprompttheexpressivenessofGNNthusachievehigheraccuracyaswe\nintroducemoreperturbations.\n(TGAEandWAlign)comparedtotraditionalgraphornodeembeddingtechniques. Inparticular,\nT-GAE consistently achieves the best performance among all competing methods. This suggests\nthat the encoder model illustrated in Fig. 1 and the training framework (7) illustrated in Fig. 2,\nprovideanefficientapproachtogeneratepowerfulnodeembedding,thatisrobusttoreal-worldnoise\ndistributionsforthetaskofnetworkalignment,comparedtotheexistingGNNframeworks.\n5.4 Ablationstudy\nTheproposedarchitectureandtrainingobjectivepromptstherobustnessofGNNwhenmatch-\ninggraphswiththeirhighlyperturbedversions. FromFigure4,weobservethatT-GAEoutper-\nformstheuntrainedT-GAEbyagreatmarginwhenmatchinghighlypermutedsamples. Thisimplies\nthattheproposedtrainingobjectiveeffectivelyimprovestherobustnessofGNN,whichisthekey\npropertytodeployGNNtomatchperturbedgraphs. Further,theperformancegapbetweenGAEand\nT-GAEsingleunderscorestheefficacyofincorporatingattentionmechanismoneachlayerforboth\ninputandoutputnodefeatures,asillustratedinFigure1.\n6 Limitation\nAlthoughourapproachachievesstate-of-the-artperformanceinaligningreal-graphs,onbothgraph\nmatchingandsub-graphmatchingtasks,approachingnetworkalignmentwithalearningmethod,\nremainsaheuristicanddoesnotofferoptimalityguarantees. Furthermore,inordertoprocesslarge\ngraphs we cast network alignment as a self-supervised task. As a result in small-scale settings\nwherethetaskcanbetackledwithcomputationallyintensiveefficientmethods,ouralgorithmisnot\nexpectedtoperformthebest. Finally,thecomplexityofT-GAEO(|V|2)islimiting,thisbottleneck\ncomesfromthegreedylinearassignmentalgorithmtomatchthenodeembedding, andtherefore\nthealternativemethodwithcomplexityO(|V|c2+|E|c+|V|log(|V|))shouldbedeployedwhenwe\nmatchverylargescalegraphswiththeirpermutedversions.\n7 Conclusion\nWe proposed T-GAE, a graph autoencoder framework that utilizes transferability and robustness\nof GNN to perform network alignment. T-GAE is an unsupervised approach that tackles\nthe high computational cost of existing optimization based algorithms, and can be trained on\nmultiplesmalltomiddlesizedgraphstoproducerobustandpermutationequivariantembeddings\nfor larger scale unseen networks. We proved that the produced embeddings of GNNs are\nrelated to the spectral decomposition of the graph and are at least as good in graph matching\nas certain spectral methods. Our experiments with real-world benchmarks on both graph\nmatching and sub-graph matching demonstrated the great potential of utilizing the good prop-\nerties of GNNs to solve network optimization problems in a more efficient and scalable way.\n9\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nReferences\nFrankEmmert-Streib,MatthiasDehmer,andYongtangShi. Fiftyyearsofgraphmatching,network\nalignmentandnetworkcomparison. Informationsciences,346:180–197,2016. 1\nShirinNilizadeh,ApuKapadia,andYong-YeolAhn. Community-enhancedde-anonymizationof\nonline social networks. In Proceedings of the 2014 acm sigsac conference on computer and\ncommunicationssecurity,pages537–548,2014. 1\nKirkOgaard,HeatherRoy,SueKase,RakeshNagi,KedarSambhoos,andMoisesSudit. Discovering\npatternsinsocialnetworkswithgraphmatchingalgorithms. InArielM.Greenberg,WilliamG.\nKennedy, and Nathan D. Bos, editors, Social Computing, Behavioral-Cultural Modeling and\nPrediction,pages341–349,Berlin,Heidelberg,2013.SpringerBerlinHeidelberg. ISBN978-3-\n642-37210-0. 1\nRohitSingh,JinboXu,andBonnieBerger. Globalalignmentofmultipleproteininteractionnetworks\nwith application to functional orthology detection. Proceedings of the National Academy of\nSciences,105(35):12763–12768,2008. 1,7,21\nJiazhouChen,HongPeng,GuoqiangHan,HongminCai,andJiulunCai. HOGMMNC:ahigher\nordergraphmatchingwithmultiplenetworkconstraintsmodelforgene–drugregulatorymodules\nidentification. Bioinformatics, 35(4):602–610, 07 2018a. ISSN 1367-4803. doi: 10.1093/\nbioinformatics/bty662. URLhttps://doi.org/10.1093/bioinformatics/bty662. 1\nD.Conte,P.Foggia,C.Sansone,andM.Vento. Graphmatchingapplicationsinpatternrecognition\nandimageprocessing. InProceedings2003InternationalConferenceonImageProcessing(Cat.\nNo.03CH37429),volume2,pagesII–21,2003. doi: 10.1109/ICIP.2003.1246606. 1\nMiklosRaczandAnirudhSridhar. Correlatedstochasticblockmodels: Exactgraphmatchingwith\napplicationstorecoveringcommunities. InM.Ranzato,A.Beygelzimer,Y.Dauphin,P.S.Liang,\nandJ.WortmanVaughan,editors,AdvancesinNeuralInformationProcessingSystems,volume34,\npages22259–22273.CurranAssociates,Inc.,2021. URLhttps://proceedings.neurips.cc/\npaper/2021/file/baf4f1a5938b8d520b328c13b51ccf11-Paper.pdf. 1\nKurtMAnstreicherandNathanWBrixius. Solvingquadraticassignmentproblemsusingconvex\nquadraticprogrammingrelaxations. OptimizationMethodsandSoftware,16(1-4):49–68,2001. 1\nJoshuaTVogelstein,JohnMConroy,VinceLyzinski,LouisJPodrazik,StevenGKratzer,EricT\nHarley,DonniellEFishkind,RJacobVogelstein,andCareyEPriebe. Fastapproximatequadratic\nprogrammingforgraphmatching. PLOSone,10(4):e0121002,2015. 1\nGunnarWKlau. Anewgraph-basedmethodforpairwiseglobalnetworkalignment. BMCbioinfor-\nmatics,10(1):1–9,2009. 1\nJimingPeng,HansMittelmann,andXiaoxueLi. Anewrelaxationframeworkforquadraticassign-\nmentproblemsbasedonmatrixsplitting. MathematicalProgrammingComputation, 2:59–77,\n2010. 1\nXingboDu,JunchiYan,andHongyuanZha. Jointlinkpredictionandnetworkalignmentviacross-\ngraphembedding. InProceedingsoftheTwenty-EighthInternationalJointConferenceonArtificial\nIntelligence,IJCAI-19,pages2251–2257.InternationalJointConferencesonArtificialIntelligence\nOrganization,72019.doi:10.24963/ijcai.2019/312.URLhttps://doi.org/10.24963/ijcai.\n2019/312. 1\nXingbo Du, Junchi Yan, Rui Zhang, and Hongyuan Zha. Cross-network skip-gram embedding\nfor joint network alignment and link prediction. IEEE Transactions on Knowledge and Data\nEngineering,34(3):1080–1095,2022. doi: 10.1109/TKDE.2020.2997861. 1\nAritraKonarandNicholasDSidiropoulos. Graphmatchingviathelensofsupermodularity. IEEE\nTransactionsonKnowledgeandDataEngineering,34(5):2200–2211,2020. 1\nS. Umeyama. An eigendecomposition approach to weighted graph matching problems. IEEE\nTransactionsonPatternAnalysisandMachineIntelligence,10(5):695–703,1988. doi: 10.1109/\n34.6778. 2,4,5,7,20\nSoheilFeizi,GeraldQuon,MarianaRecamonde-Mendoza,MurielMedard,ManolisKellis,andAli\nJadbabaie. Spectralalignmentofgraphs. IEEETransactionsonNetworkScienceandEngineering,\n7(3):1182–1197,2019. 2,4\n10\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nSi Zhang and Hanghang Tong. Final: Fast attributed network alignment. In Proceedings of the\n22ndACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining,KDD\n’16,page1345–1354,NewYork,NY,USA,2016.AssociationforComputingMachinery. ISBN\n9781450342322. doi: 10.1145/2939672.2939766. URLhttps://doi.org/10.1145/2939672.\n2939766. 2,7,19,20,21\nCharilaosIKanatsoulisandNicholasDSidiropoulos. Gage: Geometrypreservingattributedgraph\nembeddings. InProceedingsoftheFifteenthACMInternationalConferenceonWebSearchand\nDataMining,pages439–448,2022. 2\nMicheleBerlingerio,DanaiKoutra,TinaEliassi-Rad,andChristosFaloutsos. Networksimilarity\nviamultiplesocialtheories. InProceedingsofthe2013IEEE/ACMInternationalConferenceon\nAdvancesinSocialNetworksAnalysisandMining,pages1439–1440,2013. 2,7,20,21\nMarkHeimann,HaomingShen,TaraSafavi,andDanaiKoutra. Regal:Representationlearning-based\ngraphalignment. InProceedingsofthe27thACMinternationalconferenceoninformationand\nknowledgemanagement,pages117–126,2018. 2\nBryanPerozzi,RamiAl-Rfou,andStevenSkiena.Deepwalk:onlinelearningofsocialrepresentations.\nInProceedingsofthe20thACMSIGKDDinternationalconferenceonKnowledgediscoveryand\ndata mining, KDD ’14. ACM, August 2014. doi: 10.1145/2623330.2623732. URL http:\n//dx.doi.org/10.1145/2623330.2623732. 2,7,20\nAdityaGroverandJureLeskovec. node2vec: Scalablefeaturelearningfornetworks. InProceedings\nofthe22ndACMSIGKDDinternationalconferenceonKnowledgediscoveryanddatamining,\npages855–864,2016a. 2\nXiyuan Chen, Mark Heimann, Fatemeh Vahedian, and Danai Koutra. Cone-align: Consistent\nnetworkalignmentwithproximity-preservingnodeembedding. InProceedingsofthe29thACM\nInternationalConferenceonInformation&KnowledgeManagement,pages1985–1988,2020. 2,\n7,21\nParisAKarakasis,AritraKonar,andNicholasDSidiropoulos. Jointgraphembeddingandalignment\nwithspectralpivot.InProceedingsofthe27thACMSIGKDDConferenceonKnowledgeDiscovery\nandDataMining,pages851–859,2021. 2\nThomasN.KipfandMaxWelling. Variationalgraphauto-encoders,2016. URLhttps://arxiv.\norg/abs/1611.07308. 2,5,7,20\nYuningYou,TianlongChen,YongduoSui,TingChen,ZhangyangWang,andYangShen. Graph\ncontrastivelearningwithaugmentations. InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,\nand H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages\n5812–5823.CurranAssociates,Inc.,2020. URLhttps://proceedings.neurips.cc/paper_\nfiles/paper/2020/file/3fe230348e9a12c13120749e3f9fa4cd-Paper.pdf. 2\nP. Gainza, F. Sverrisson, F. Monti, E. Rodolà, D. Boscaini, M. M. Bronstein, and B. E. Correia.\nDecipheringinteractionfingerprintsfromproteinmolecularsurfacesusinggeometricdeeplearning.\nNatureMethods,17(2):184–192,February2020. 2\nAlexeyStrokach,DavidBecerra,CarlesCorbi-Verge,AlbertPerez-Riba,andPhilipM.Kim. Fast\nandflexibleproteindesignusingdeepgraphneuralnetworks. CellSystems,11(4):402–411.e4,\nOctober2020. 2\nDejunJiang,ZhenxingWu,ChangYuHsieh,GuangyongChen,BenLiao,ZheWang,ChaoShen,\nDongshengCao,JianWu,andTingjunHou. Couldgraphneuralnetworkslearnbettermolecular\nrepresentationfordrugdiscovery?acomparisonstudyofdescriptor-basedandgraph-basedmodels.\nJournalofCheminformatics,13(1):12,dec2021. 2\nXinyue Hu, Zenan Sun, Yi Nian, Yifang Dang, Fang Li, Jingna Feng, Evan Yu, and Cui Tao.\nExplainablegraphneuralnetworkforalzheimer’sdiseaseandrelateddementiasriskprediction,\n2023. 2\nChenxinLi,XinyuLiu,ChengWang,YifanLiu,WeihaoYu,JingShao,andYixuanYuan. Gtp-4o:\nModality-promptedheterogeneousgraphlearningforomni-modalbiomedicalrepresentation,2024.\nURLhttps://arxiv.org/abs/2407.05540. 2\nJustinGilmer,SamuelSSchoenholz,PatrickFRiley,OriolVinyals,andGeorgeEDahl. Neural\nmessagepassingforquantumchemistry. InInternationalconferenceonmachinelearning,pages\n1263–1272.PMLR,2017. 2\n11\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nRexYing,RuiningHe,KaifengChen,PongEksombatchai,WilliamL.Hamilton,andJureLeskovec.\nGraphconvolutionalneuralnetworksforweb-scalerecommendersystems. Proceedingsofthe\nACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining,10:974–983,\nJune2018. 2\nShiwenWu,FeiSun,WentaoZhang,XuXie,andBinCui. Graphneuralnetworksinrecommender\nsystems: Asurvey,2020. URLhttps://arxiv.org/abs/2011.02260. 2\nX.Liu, R.Wang, D.Sun, J.Li, C.Youn, Y.Lyu, J.Zhan, D.Wu, X.Xu, M.Liu, X.Lei, Z.Xu,\nY.Zhang,Z.Li,Q.Yang,andT.Abdelzaher. Influencepathwaydiscoveryonsocialmedia. In2023\nIEEE9thInternationalConferenceonCollaborationandInternetComputing(CIC),pages105–\n109,LosAlamitos,CA,USA,nov2023a.IEEEComputerSociety. doi: 10.1109/CIC58953.2023.\n00023. URLhttps://doi.ieeecomputersociety.org/10.1109/CIC58953.2023.00023.\n2\nQikaiYang,PanfengLi,ZhichengDing,WenjingZhou,YiNian,andXinheXu. Acomparative\nstudyonenhancingpredictioninsocialnetworkadvertisementthroughdataaugmentation. arXiv\npreprintarXiv:2404.13812,2024. 2\nMatthiasFey,JanE.Lenssen,ChristopherMorris,JonathanMasci,andNilsM.Kriege. Deepgraph\nmatchingconsensus,2020. URLhttps://arxiv.org/abs/2001.09621. 2\nTianshu Yu, Runzhong Wang, Junchi Yan, and Baoxin Li. Learning deep graph matching with\nchannel-independentembeddingandhungarianattention. InInternationalConferenceonLearning\nRepresentations,2020. URLhttps://openreview.net/forum?id=rJgBd2NYPH. 2\nZhiyuanLiu,YixinCao,FuliFeng,XiangWang,JieTang,KenjiKawaguchi,andTat-SengChua.\nTrainingfreegraphneuralnetworksforgraphmatching,2022. URLhttps://arxiv.org/abs/\n2201.05349. 2\nZiweiZhang,PengCui,andWenwuZhu. Deeplearningongraphs: Asurvey,2020. URLhttps:\n//arxiv.org/abs/1812.04202. 2\nJieZhou,GanquCui,ShengdingHu,ZhengyanZhang,ChengYang,ZhiyuanLiu,LifengWang,\nChangchengLi,andMaosongSun. Graphneuralnetworks: Areviewofmethodsandapplications,\n2021. URLhttps://arxiv.org/abs/1812.08434. 2\nZhehanLiang, YuRong, ChenxinLi, YunlongZhang, YueHuang, TingyangXu, XinghaoDing,\nand Junzhou Huang. Unsupervised large-scale social network alignment via cross network\nembedding. In Proceedings of the 30th ACM International Conference on Information &\nKnowledge Management, CIKM ’21, page 1008–1017, New York, NY, USA, 2021. Associa-\ntionforComputingMachinery. ISBN9781450384469. doi: 10.1145/3459637.3482310. URL\nhttps://doi.org/10.1145/3459637.3482310. 2\nJi Gao, Xiao Huang, and Jundong Li. Unsupervised graph alignment with wasserstein distance\ndiscriminator. InProceedingsofthe27thACMSIGKDDConferenceonKnowledgeDiscovery&\nDataMining,KDD’21,page426–435,NewYork,NY,USA,2021.AssociationforComputing\nMachinery. ISBN9781450383325. doi: 10.1145/3447548.3467332. URLhttps://doi.org/\n10.1145/3447548.3467332. 2,7,20\nJianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with\nvariancereduction,2018b. URLhttps://arxiv.org/abs/1710.10568. 2\nJieChen,TengfeiMa,andCaoXiao. Fastgcn: Fastlearningwithgraphconvolutionalnetworksvia\nimportancesampling,2018c. URLhttps://arxiv.org/abs/1801.10247. 2\nWei-LinChiang,XuanqingLiu,SiSi,YangLi,SamyBengio,andCho-JuiHsieh. Cluster-gcn: An\nefficientalgorithmfortrainingdeepandlargegraphconvolutionalnetworks. InProceedingsof\nthe25thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining,KDD\n’19,page257–266,NewYork,NY,USA,2019.AssociationforComputingMachinery. ISBN\n9781450362016. doi: 10.1145/3292500.3330925. URLhttps://doi.org/10.1145/3292500.\n3330925. 2\nHanqingZeng,HongkuanZhou,AjiteshSrivastava,RajgopalKannan,andViktorPrasanna. Graph-\nsaint: Graphsamplingbasedinductivelearningmethod,2020. URLhttps://arxiv.org/abs/\n1907.04931. 2\nTjallingCKoopmansandMartinBeckmann. Assignmentproblemsandthelocationofeconomic\nactivities. Econometrica: journaloftheEconometricSociety,pages53–76,1957. 3\n12\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nHarold W Kuhn. The hungarian method for the assignment problem. Naval research logistics\nquarterly,2(1-2):83–97,1955a. 3\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\nnetworks? In International Conference on Learning Representations, 2019a. URL https:\n//openreview.net/forum?id=ryGs6iA5Km. 4,21\nHaggaiMaron,HeliBen-Hamu,NadavShamir,andYaronLipman. Invariantandequivariantgraph\nnetworks. InInternationalConferenceonLearningRepresentations,2018. 4\nFernandoGama,JoanBruna,andAlejandroRibeiro. Stabilitypropertiesofgraphneuralnetworks.\nIEEETransactionsonSignalProcessing,68:5680–5695,2020. 4\nAlejandroParada-Mayorga,ZhiyangWang,FernandoGama,andAlejandroRibeiro. Stabilityof\naggregationgraphneuralnetworks. IEEETransactionsonSignalandInformationProcessingover\nNetworks,9:850–864,2023. doi: 10.1109/TSIPN.2023.3341408. 4\nLuanaRuiz,ZhiyangWang,andAlejandroRibeiro. Graphonandgraphneuralnetworkstability. In\nICASSP2021-2021IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing\n(ICASSP),pages5255–5259,2021. doi: 10.1109/ICASSP39728.2021.9414838. 4\nLuanaRuiz,FernandoGama,andAlejandroRibeiro. Gatedgraphrecurrentneuralnetworks. IEEE\nTransactionsonSignalProcessing,68:6303–6318,2020. 4,6\nRalphAbboud,IsmailIlkanCeylan,MartinGrohe,andThomasLukasiewicz. Thesurprisingpower\nofgraphneuralnetworkswithrandomnodeinitialization. InIJCAI,2021. 4\nCharilaosIKanatsoulisandAlejandroRibeiro. Graphneuralnetworksaremorepowerfulthanwe\nthink. arXivpreprintarXiv:2205.09801,2022. 4\nWillemHHaemersandEdwardSpence. Enumerationofcospectralgraphs. EuropeanJournalof\nCombinatorics,25(2):199–211,2004. 5\nXuezhiWang,HaohanWang,andDiyiYang. Measureandimproverobustnessinnlpmodels: A\nsurvey,2022. 6\nJiashuHe. Performanceanalysisoffacialrecognition: Acriticalreviewthroughglassfactor,2021. 6\nChenxinLi,XinLin,YijinMao,WeiLin,QiQi,XinghaoDing,YueHuang,DongLiang,andYizhou\nYu. Domaingeneralizationonmedicalimagingclassificationusingepisodictrainingwithtask\naugmentation. ComputersinBiologyandMedicine,141:105144,2022. ISSN0010-4825. doi:\nhttps://doi.org/10.1016/j.compbiomed.2021.105144. URLhttps://www.sciencedirect.com/\nscience/article/pii/S0010482521009380. 6\nKeithHenderson,BrianGallagher,LeiLi,LemanAkoglu,TinaEliassi-Rad,HanghangTong,and\nChristos Faloutsos. It’s who you know: graph mining using recursive structural features. In\nProceedingsofthe17thACMSIGKDDinternationalconferenceonKnowledgediscoveryanddata\nmining,pages663–671,2011. 6\nAdityaGroverandJureLeskovec. node2vec: Scalablefeaturelearningfornetworks,2016b. 7,21\nClaire Donnat, Marinka Zitnik, David Hallac, and Jure Leskovec. Learning structural node em-\nbeddings via diffusion wavelets. In Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, KDD ’18. ACM, July 2018. doi:\n10.1145/3219819.3220025. URLhttp://dx.doi.org/10.1145/3219819.3220025. 7,21\nJianTang,MengQu,MingzheWang,MingZhang,JunYan,andQiaozhuMei. Line: Large-scale\ninformationnetworkembedding. InProceedingsofthe24thInternationalConferenceonWorld\nWideWeb,WWW’15.InternationalWorldWideWebConferencesSteeringCommittee,May2015.\ndoi: 10.1145/2736277.2741093. URLhttp://dx.doi.org/10.1145/2736277.2741093. 7,\n21\nHongtengXu, DixinLuo, andLawrenceCarin. Scalablegromov-wassersteinlearningforgraph\npartitioningandmatching,2019b. 7,21\nShirui Pan, Jia Wu, Xingquan Zhu, Chengqi Zhang, and Yang Wang. Tri-party deep network\nrepresentation. InSubbaraoKambhampati,editor,ProceedingsoftheTwenty-FifthInternational\nJointConferenceonArtificialIntelligence,IJCAI2016,NewYork,NY,USA,9-15July2016,pages\n1895–1901.IJCAI/AAAIPress,2016. 8,19\n13\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nOleksandrShchur,MaximilianMumme,AleksandarBojchevski,andStephanGünnemann. Pitfalls\nofgraphneuralnetworkevaluation. RelationalRepresentationLearningWorkshop,NeurIPS2018,\n2018. 8,19,24\nJiashuHe, MingyuDerekMa, JinxuanFan, DanRoth, WeiWang, andAlejandroRibeiro. Give:\nStructuredreasoningwithknowledgegraphinspiredveracityextrapolation,2024. URLhttps:\n//arxiv.org/abs/2410.08475. 15\nCongcongGe,XiaozeLiu,LuChen,BaihuaZheng,andYunjunGao. Makeiteasy: Aneffective\nend-to-endentityalignmentframework. InProceedingsofthe44thInternationalACMSIGIR\nConferenceonResearchandDevelopmentinInformationRetrieval,SIGIR’21,page777–786,\nNewYork,NY,USA,2021a.AssociationforComputingMachinery. ISBN9781450380379. doi:\n10.1145/3404835.3462870. URLhttps://doi.org/10.1145/3404835.3462870. 15\nCongcongGe,XiaozeLiu,LuChen,YunjunGao,andBaihuaZheng. Largeea: aligningentitiesfor\nlarge-scaleknowledgegraphs. ProceedingsoftheVLDBEndowment,15(2):237–245,October\n2021b. ISSN 2150-8097. doi: 10.14778/3489496.3489504. URL http://dx.doi.org/10.\n14778/3489496.3489504. 15\nYunjunGao,XiaozeLiu,JunyangWu,TianyiLi,PengfeiWang,andLuChen. Clusterea: Scalable\nentity alignment with stochastic training and normalized mini-batch similarities. In Proceed-\nings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD\n’22,page421–431,NewYork,NY,USA,2022.AssociationforComputingMachinery. ISBN\n9781450393850. doi: 10.1145/3534678.3539331. URLhttps://doi.org/10.1145/3534678.\n3539331. 15\nCongcongGe,PengfeiWang,LuChen,XiaozeLiu,BaihuaZheng,andYunjunGao. Collaborem: A\nself-supervisedentitymatchingframeworkusingmulti-featurescollaboration. IEEETransactions\non Knowledge and Data Engineering, 35(12):12139–12152, 2023. doi: 10.1109/TKDE.2021.\n3134806. 15\nXiaoze Liu, Junyang Wu, Tianyi Li, Lu Chen, and Yunjun Gao. Unsupervised entity alignment\nfortemporalknowledgegraphs. InProceedingsoftheACMWebConference2023,WWW’23,\npage 2528–2538, New York, NY, USA, 2023b. Association for Computing Machinery. ISBN\n9781450394161. doi: 10.1145/3543507.3583381. URLhttps://doi.org/10.1145/3543507.\n3583381. 15\nBolinZhu,XiaozeLiu,XinMao,ZhuoChen,LingbingGuo,TaoGui,andQiZhang. Universal\nmulti-modalentityalignmentviaiterativelyfusingmodalitysimilaritypaths,2023. URLhttps:\n//arxiv.org/abs/2310.05364. 15\nJianDing,ZongmingMa,YihongWu,andJiamingXu. Efficientrandomgraphmatchingviadegree\nprofiles,2020. 15\nYihongWu,JiamingXu,andSophieH.Yu. Settlingthesharpreconstructionthresholdsofrandom\ngraphmatching,2022. 15\nChengMao,YihongWu,JiamingXu,andSophieH.Yu. Randomgraphmatchingatotter’sthreshold\nviacountingchandeliers,2023. 15\nStefaniaSardellitti,SergioBarbarossa,andPaoloDiLorenzo. Onthegraphfouriertransformfor\ndirectedgraphs. IEEEJournalofSelectedTopicsinSignalProcessing,11(6):796–811,2017. 16\nHarold W Kuhn. The hungarian method for the assignment problem. Naval research logistics\nquarterly,2(1-2):83–97,1955b. 18,24\nJérômeKunegis. Konect: thekoblenznetworkcollection. Proceedingsofthe22ndInternational\nConferenceonWorldWideWeb,2013. 19,24\nJureLeskovecandAndrejKrevl. SNAPDatasets: Stanfordlargenetworkdatasetcollection. http:\n//snap.stanford.edu/data,June2014. 19\nPrithvirajSen,GalileoNamata,MustafaBilgic,LiseGetoor,BrianGalligher,andTinaEliassi-Rad.\nCollectiveclassificationinnetworkdata. AImagazine,29(3):93–93,2008. 19\nSiZhangandHanghangTong. Attributednetworkalignment: Problemdefinitionsandfastsolutions.\nIEEE Transactions on Knowledge and Data Engineering, 31:1680–1692, 2019. URL https:\n//api.semanticscholar.org/CorpusID:70142000. 19\nThomasNKipfandMaxWelling. Semi-supervisedclassificationwithgraphconvolutionalnetworks.\nInInternationalConferenceonLearningRepresentations,2017. 20\n14\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nA Appendix\nB Notation\nOurnotationissummarizedinTable1.\nTable1: Keynotationsusedinthispaper.\nG ≜ Graph\nV ≜ Setofnodes\nE ≜ Setofedges\nN ≜ Numberofnodes\nD ≜ Degreematrix\nS ≜ {0,1}N×N adjacencymatrix\nX ≜ N ×Dfeaturematrix\nH ≜ aggregationresultsofGNNconvolution\nW ≜ weightmatrixoftheGraphNeuralNetwork\nN ≜ neighborsofnodev\nv\nI ≜ Identitymatrix\n0 ≜ vectorormatrixofzeros\nAT ≜ transposeofmatrixA\nA ≜ entryatr-throwandj-thcolumnofmatrixA\nrc\n∥·∥ ≜ Frobeniusnorm\nF\nC NetworkAlignmentinbroaderdomains\nInthispaper,westudythegeneralformofnetworkalignmenttomatchnodesbetweentwographs.\nThistaskischallengingbecauseofthelittlegiveninformation. However,it’snottrivialtocorrectly\nandefficientlyutilizetheinformationcontainedintext-richnetworks,suchasKGs[Heetal.,2024].\nInthedomainofknowledgegraph(KG)mining,entityalignment(EA)isthetasktoidentifythe\nsameentitiesexistinginknowledgegraphs. [Geetal.,2021a]proposestoreplacethelabor-intensive\npre-processingwithentitynamesmining. Astructural-basedrefinementprocedureisthenappliedto\nrefinetheentitynamematchingresults. [Geetal.,2021b,Gaoetal.,2022]solveslargescaleEAby\naligningtheKGinmini-batches. [Geetal.,2023]furtherproposesaself-supervisedEAframework\nbyautomaticallygeneratingpositiveandnegativematchednodepairs. [Liuetal.,2023b]generalizes\ntheunsupervisedmatchingalgorithmtotemporalKGs. Specifically,itencodesthetemporaland\nrelationalinformationrespectivelybeforeaninnovativejointlydecodingprocess. Recently,theability\nto deploy EA algorithms in real-world scenarios is enhanced by [Zhu et al., 2023], which aligns\nmulti-modalknowledgegraphs.\nNetworkalignment,asanimportantproblem,hasbeenstudiednotonlybythecommunityofdata\nmining,ithasalsobeenmathematicallyandstatisticallyinvestigated. Anumberofapproacheshave\nbeenproposedtosolvethisproblemforErdo˝sRényirandomgraphsG(n, d). Ithasbeenprovedthat\nn\naperfectlytruevertexcorrespondencecanberecoveredinpolynomialtimewithhighprobability\n[Dingetal.,2020]. Furthermore,asharpthresholdhasbeenprovedforbothErdo˝sRényimodeland\nGaussianmodel[Wuetal.,2022]. Mostrecently,anovelapproachtocalculatesimilarityscores\nbasedoncountingweightedtreesrootedateachvertexhasbeenproposedMaoetal.[2023]. Such\napproachhasbeenprovedtobeeffectiveinsolvingtheaforementionednetworkalignmentproblem\nonrandomgraphswithhighprobability. Readersareencouragedtorefertotheauthorsofthese\npublications[Dingetal.,2020,Wuetal.,2022,Maoetal.,2023]forfurtherreading.\nD ProofofTheorem3.2\nD.1 SpectralcharacterizationofGNNs\nWhatremainstobeansweredistheabilityofaGNNtoapproximateafunctionthatperformsgraph\nalignment. To understand the function approximation properties of GNNs we study them in the\n15\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nspectraldomain. Tothisend,weconsidertherecursiveformulain(4)wheref isthesummation\nfunctionandg ismultivariatelinearforK −2layers,andtheMLPinthe(K −1)-thlayer. The\noveralloperationcanbewritteninamatrixformas:\n(cid:32)K−1 (cid:33)\nX(l+1) =σ (cid:88) SkX(l)H(l) , (8)\nk\nk=0\nwhereH(l) ∈RDl+1×Dl isalinearmapping. ComputingthespectraldecompositionofS yields:\nk\n(cid:32)K−1 (cid:33) (cid:32)K−1 N (cid:33)\nX(l+1) =σ (cid:88) VΛkVTX(l)H(l) =σ (cid:88) (cid:88) λkv vTX(l)H(l) . (9)\nk n n n k\nk=0 k=0 n=1\nTheneacheachcolumnofX(l+1)canbewrittenas\n(cid:32)K−1 N (cid:33) (cid:32) N (cid:33)\nX(l+1)[:,i]=σ (cid:88) (cid:88) λkv vTX(l)H(l)[:,i] =σ (cid:88) a(i)v , (10)\nn n n k n n\nk=0 n=1 n=1\nwhereλ ,v arethen−theigenvalueandeigenvectoranda(i) =vTX(l)(cid:80)K−1λkH(l)[:,i]isa\nn n n n k=0 n k\nscalarrelatedtotheGraphFourierTransform(GFT)ofX(l)[Sardellittietal.,2017]. Itisclearfrom\nequation(10)thattheoutputofeachlayerisalinearcombinationoftheadjacencyeigenvectors,\nfollowedbyapointwisenon-linearity. Thus,aGNNcanproduceuniqueandmorepowerfulgraph\nembeddingsthanspectralmethodsbyprocessingtheeigenvectorsandeigenvaluesoftheadjacency\nmatrix. ToproveTheorem3.2. WeconsideronelayerGNNwithavectorinputx∈RN. ThisGNN\ncanberepresentedbythefollowingequation:\n(cid:32)K−1 (cid:33)\n(cid:88)\nY =σ SkxhT , (11)\nk\nk=0\nwhereh ∈RmandxhT isanouter-productoperation. Theequationin(11)describesasetofm\nk k\ngraphfiltersoftheform:\n(cid:32)K−1 (cid:33)\n(cid:88)\ny =σ hiSkx , fori=1,...,m (12)\ni k\nk=0\nD.2 Whiterandominputandvariancecomputation\nLetxbeawhiterandomvectorwithE[x]=0andE(cid:2) xxT(cid:3)\n=I,whereI isthediagonalmatrix.\nAlsoletσ(·)=(·)2betheelementwisesquarefunction. Then(12)canbewrittenas:\n(cid:32)K−1 (cid:33)2  K−1 K−1 \ny i = (cid:88) hi kSkx =diag(cid:88) hi kSkxxT (cid:88) hi jSjT  (13)\nk=0 k=0 j=0\nSincexisarandomvectory isalsoarandomvector. Theexpectedvalueofy yields:\ni i\n  \nK−1 K−1\nE[y i]=E diag(cid:88) hi kSkxxT (cid:88) hi jSjT  (14)\nk=0 j=0\n \nK−1 K−1\n=diag(cid:88) hi kSkE(cid:2) xxT(cid:3) (cid:88) hi jSjT\n\nk=0 j=0\n \nK−1 K−1\n=diag(cid:88) hi kSk (cid:88) hi jSjT  (15)\nk=0 j=0\n16\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nD.3 Singlebandfiltering\nInthesecondpartoftheproofwestudythegraphfilterusingthespectraldecompositionofthegraph:\nK−1\n(cid:88)\ny = h Skx (16)\nk\nk=0\nK−1\n(cid:88)\n= h VΛkVTx (17)\nk\nk=0\nK−1 N\n(cid:88) (cid:88)\n= h λkv vTx (18)\nk n n n\nk=0 n=1\nN K−1\n(cid:88) (cid:88)\n= vTx h λkv . (19)\nn k n n\nn=1 k=0\nLetusfocusonthefollowingpolynomial:\nK−1\nh˜(λ)= (cid:88) h λk, (20)\nk\nk=0\nthatrepresentsagraphfilterinthefrequencydomainby. Forqdistincteigenvalueswecanwritea\nsystemoflinearequationsusingthepolynomialin(20):\n h˜(λ )  1λ λ2...λK−1 h \n1 1 1 1 0\n   h˜(λ . . . 2)   =   1λ 2λ2 2. . . ...λK 2 −1       h . . .1   =Wh (21)\nh˜(λ q) 1λ q λ2 q...λ qK−1 h K−1\nW isaVandermondematrixandwhenK =qthedeterminantofW takestheform:\n(cid:89)\ndet(W)= (λ −λ ) (22)\ni j\n1≤i<j≤q\nSincethevaluesλ aredistinct,W hasfullcolumnrankandthereexistsagraphfilterwithunique\ni\nparametershthatpassesonlytheλeigenvalue,i.e.,\n(cid:26)\nh˜(λ )= 1, if λ i =λ (23)\ni 0, if λ ̸=λ\ni\nUnderthisparametrization,equation(16)takestheformy =v vTx,wherev istheeigenvector\nλ λ λ\ncorrespondingtoλ.\nD.4 GNNandabsoluteeigenvectors\nUsingthepreviousanalysiswecandesignparametersh suchthat:\nk\nK−1\n(cid:88)\nh Sk =v vT (24)\nk λ λ\nk=0\nandthenequation(14)takestheform:\n \nK−1 K−1\nE[y i]=diag(cid:88) hi kSk (cid:88) hi jSjT  (25)\nk=0 j=0\n=diag(cid:0) v vTv vT(cid:1) (26)\nλ λ λ λ\n=diag(cid:0)\nv\nvT(cid:1)\n(27)\nλ λ\n=|v |2 (28)\nλ\n17\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nWe can therefore design h ∈ Rm for k = 0,...,m−1 to compute the absolute value of m\nk\neigenvectorsofS thatcorrespondtothetopmdistincteigenvalues,i.e.,\nE[y ]=|u |2, i=1,...,m (29)\ni i\n(30)\nWecandothesameforgraphSˆandcompute:\nE[yˆ]=|uˆ |2, i=1,...,m (31)\ni i\n(32)\nSincebothS, Sˆhavedistincteigenvalues,wecanconcatenatetheoutputofeachneuronandresult\ninlayer-1outputsas:\nY(1) =|U|, Yˆ(1) =|Uˆ| (33)\nAs a result, the previously described GNN can a least yield the same alignment accuracy as the\nabsolutevaluesoftheeigenvectors.\nD.5 Generalizationtomultiplegraphpairs\nTheanalysisintheprevioiussubsectionsisindeedpresentedforapairofgraphsbutcanbedirectly\nextendedforanysetofgraphs. WecangeneralizetheTheorem3.2,toreadas: Let{G ,...,G }be\n1 M\nasetofgraphswithadjacencies{S ,...,S }thathavenon-repeatedeigenvalues. Thenforany\n1 M\nS, Sˆ ∈{S ,...,S },thereexistsaGNNϕ(X;S,H):RN×D →RN×DL suchthat:\n1 M\n(cid:13) (cid:13)S−P⋄SˆP⋄T(cid:13) (cid:13)2 ≤(cid:13) (cid:13)S−P∗SˆP∗T(cid:13) (cid:13)2 ≤(cid:13) (cid:13)S−PˇSˆPˇT(cid:13) (cid:13)2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\nF F F\nwith\n(cid:13) (cid:16) (cid:17) (cid:13)2\nP∗ =argmin (cid:13)ϕ(X;S,H)−Pϕ Xˆ;Sˆ,H (cid:13) ,\n(cid:13) (cid:13)\nP∈P F\nwhereP⋄, Pˇ aresolutionstotheoptimizationproblemsin(1)and(3)respectively.\nE ImplementationDetails\nInthissectionwediscusstheimplementationdetailsofourframework.\nE.1 AssignmentOptimization\nTheproposedT-GAElearnslearnsaGNNencoderthatcanproducenoderepresentationsfordifferent\ngraphs. Letϕ(X;S,H)representtheembeddingsofthenodescorrespondingtothegraphwith\n(cid:16) (cid:17)\nadjacencyS andϕ Xˆ;Sˆ,H representtheembeddingsofthenodescorrespondingtothegraph\nwithadjacencySˆ.Thennetworkalignmentboilsdowntosolvingthefollowingoptimizationproblem:\n(cid:13) (cid:16) (cid:17) (cid:13)2\nmin (cid:13)ϕ(X;S,H)−Pϕ Xˆ;Sˆ,H (cid:13) . (34)\n(cid:13) (cid:13)\nP∈P F\nTheproblemin(34)canbeoptimallysolvedinO(cid:0) N3(cid:1)\nflopsbytheHungarianalgorithm[Kuhn,\n1955b]. To avoid this computational burden we employ the greedy Hungarian approach that has\ncomputationalcomplexityO(cid:0) N2(cid:1)\nandusuallyworkswellinpractice.\nThe greedy Hungarian approach is described in Algorithm 1. For each row of\n(cid:16) (cid:17)\nϕ(X;S,H), ϕ Xˆ;Sˆ,H ,whichcorrespondstothenodeembeddingsofthedifferentgraphs,we\ncomputethepairwiseEuclideandistancewhichisstoresinthedistancematrixD. Then,ateach\niteration,wefindthenodeswiththesmallestdistanceandremovethealignedpairsfromD. This\nprocessisrepeateduntilallthenodesarepairedupforalignment.\n18\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nAlgorithm1:GreedyHungarianAlgorithm\nInput: FeaturematricesX,Xˆ\nOutput: AssignmentMatrix\n1 P :=0 N×N //Initializepermutationmatrix\n(cid:16) (cid:17)\n2 D :=PairwiseDistance X,Xˆ //pairwiseEuclideandistance\n3 rows:=0,1,...,N-1 //CorrespondstoX\n4 cols:=0,1,...,N-1 //CorrespondstoXˆ\n/*IteratetoassignnodepairswithminimumEuclideandistance */\n5 forn=1toN do\n6 i, j :=argmin(D)\n7 r :=rows[i]\n8 c :=cols[j]\n9 P rc :=1\n10 Removerfromrows\n11 Removecfromcols\n12 Removethei-throwfromD\n13 Removethej-thcolumnfromD\n14 returnP\nTable2: SummaryofDatasetstatisticsthatareincludedinSection5\nTask Dataset |V| |E| #AlignedEdges NetworkType\nCelegans[Kunegis,2013] 453 2,025 2,025 Interactome\nArenas[LeskovecandKrevl,2014] 1,133 5,451 5,451 EmailCommunication\nGraphMatching Cora[Senetal.,2008] 2,708 5,278 5,278 CitationNetwork\nDouban[ZhangandTong,2016] 3,906 7,215 7,215 SocialNetwork\nDblp[Panetal.,2016] 17,716 52,867 52,867 CitationNetwork\nCoauthorCS[Shchuretal.,2018] 18,333 81,894 81,894 CoauthorNetwork\n9,872 39,561\nACM-DBLP[ZhangandTong,2019] 6,352 CitationNetwork\n9,916 44,808\nSubraphMatching\n3,906 1,632\nDoubanOnline-Offline[ZhangandTong,2016] 1,118 SocialNetwork\n1,118 3,022\nE.2 Datasets\nWeincludestatisticsofthedatasetsusedinourexperimentsinTable2. Thedetaileddescriptionsof\neachdatasetarepresentedbelow:\n• Celegans[Kunegis,2013]: Theverticesrepresentproteinsandtheedgestheirprotein-protein\ninteractions.\n• ArenasEmail[LeskovecandKrevl,2014]: TheemailcommunicationnetworkattheUniversity\nRoviraiVirgiliinTarragonainthesouthofCataloniainSpain. Nodesareusersandeachedge\nrepresentsthatatleastoneemailwassent.\n• Douban[ZhangandTong,2016]: Containsuser-userrelationshipontheChinesemoviereview\nplatform. Eachedgeimpliesthattwousersarecontactsorfriends.\n• Cora[Senetal.,2008]: Thedatasetconsistsof2708scientificpublications,withedgesrepre-\nsentingcitationrelationshipsbetweenthem. Corahasbeenoneofthemajorbenchmarkdatasets\ninmanygraphminingtasks.\n• Dblp[Panetal.,2016]: AcitationnetworkdatasetthatisextractedfromDBLP,Association\nforComputingMachinery(ACM),MicrosoftAcademicGraph(MAG),andothersources. Itis\nconsideredabenchmarkinmultipletasks.\n• Coauthor_CS[Shchuretal.,2018]: ThecoauthorshipgraphisgeneratedfromMAG.Nodesare\ntheauthorsandtheyareconnectedwithanedgeiftheycoauthoredatleastonepaper.\n• ACM−DBLP[ZhangandTong,2019]: Thecitationnetworksthatsharesomecommonnodes.\nThetaskistoidentifythepublicationsthatappearinbothnetworks.\n19\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\n• DoubanOnline−Offline[ZhangandTong,2016]: Thetwosocialnetworkscontainedinthis\ndatasetrepresentstheonlineandofflineeventsoftheDoubansocialnetwork. Thetaskisto\nidentifyusersthatparticipateinbothonlineandofflineevents.\nE.3 Baselines\nE.3.1 GraphNeuralNetwork(GNN)basedmethods\nTohaveafaircomparisonwiththenodeembeddingmodels,GAEisimplementedusingthesame\nset of parameters as T-GAE, which can be found at Section E.4. WAlign is implemented using\nparameterssuggestedbytheauthor. Wereportthebestresultstheyachievedduringtraining.\n• WAlign[Gaoetal.,2021]fitsaGNNtoeachoftheinputgraphs,trainsthemodelbyreconstruct-\ningthegiveninputsandminimizinganapproximationofWassersteindistancebetweenthenode\nembeddings. Weusetheauthor’simplementationfromhttps://github.com/gaoji7777/\nwalign.git.\n• GAE, VGAE[Kipf and Welling, 2016] are self-supervised graph learning frameworks that are\ntrainedbyreconstructingthegraph. TheencoderisaGCN[KipfandWelling,2017]andlinear\ndecoderisappliedtopredicttheoriginaladjacency. InVGAE,GausianNoiseisintroduced\nto the node embeddings before passing to the decoder. We use the implementation from\nhttps://github.com/DaehanKim/vgae_pytorch. We train GAE by reconstructing the\ngivennetworkusingthenetsimilenodeembedding.\nE.3.2 Graph/Nodeembeddingtechniques\n• NetSimile[Berlingerioetal.,2013]usesthestructuralfeaturesdescribedearliertomatchthe\nnodesofthegraphs. SincetheNetSimilefeaturesareusedasinputtotheT-GAE,theyprovide\nameasuretoassessthebenefitofusingT-GAEfornodeembedding. Itproposed7egonet-based\nfeatures,tomeasurenetworksimilarity. WeprocessthesefeaturesbyAlgorithm1toperform\nnetworkalignment. The7-dimensionalNetsimilefeaturesare:\n– d =degreeofnodei\ni\n– c =numberoftrianglesconnectedtonodeioverthenumberofconnectedtriplescentered\ni\nonnodei\n– d¯ = 1 (cid:80) d ,averagenumberoftwo-hopneighbors\nNi di j∈Ni j\n– c¯ = 1 (cid:80) c ,averageclusteringcoefficient\nNi di j∈Ni j\n– Numberofedgesinnodei’segonet\n– Numberofoutgoingedgesfromnodei’segonet\n– Numberofneighborsinnodei’segonet\nThe implementation is based on netrd library where we use the feature extraction function.\nThesourcecodecanbefoundathttps://netrd.readthedocs.io/en/latest/_modules/\nnetrd/distance/netsimile.html\n• Spectral[Umeyama,1988]Itsolvesthefollowingoptimizationproblem:\n(cid:13) (cid:12) (cid:12) (cid:13)2\nmin (cid:13) |V|−P (cid:12)Vˆ(cid:12) (cid:13) , (35)\n(cid:13) (cid:12) (cid:12) (cid:13)\nP∈P F\nwhereV, Vˆ aretheeigenvectorscorrespondingtotheadjacenciesofthegraphsthatwewantto\nmatch. Inourinitialexperiments,weobservedthatasubsetoftheeigenvectorsyieldsimproved\nresults compared to the whole set. We tried 1−10 top eigenvectors and concluded that 4\neigenvectorsarethosethatyieldthebestresultsonaverage. Thuswesolvetheaboveproblem\nwiththetop-4eigenvectors.\n• DeepWalk[Perozzietal.,2014]: Anodeembeddingapproach,simulatesrandomwalksonthe\ngraphandapplyskip-gramonthewalkstogeneratenodeembedding.Weusetheimplementation\nfromKarateclub. Thealgorithmisimplementedwiththedefaultparametersassuggestedbythis\nrepository,thenumberofrandomwalksis10witheachwalkoflength80. Thedimensionality\nofembeddingissettobe128. Werunthealgorithmwith1epochandsetthelearningratetobe\n0.05.\n20\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\n• Node2Vec[GroverandLeskovec,2016b]: AnimproveversionofDeepWalk,ithasweights\non the randomly generated random walks, to make the neighborhood preserving objective\nmoreflexible. WeusetheimplementationfromKarateclub. Thedefaultparametersareused.\nWe simulate 10 random walks on the graph with length 80. p and q are both equal to 1.\nDimensionalityofembeddingsissettobe4andwerun1epochwithlearningrate0.05.\n• GraphWave [Donnat et al., 2018]: The structure information of the graphs is captured by\nsimulatingheatdiffusionprocessonthem. WeusetheimplementationfromKarateclubwith\nthe default parameters: number of evaluation points is 200, step size is 0.1, heat coefficient\nis1.0andChebyshevpolynomialorderissettobe100. Notethatthisimplementationdoes\nnotworkongraphswithmorethan10,000nodes,soweexcludethismodelontheDBLPand\nCoauthor_CSdataset.\n• LINE[Tangetal.,2015]:Anoptimizationbasedgraphembeddingapproachthataimstopreserve\nlocalandglobalstructuresofthenetworkbyconsideringsubstructuresandstructural-quivariant\nnodes. WeusethePyTorchimplementationfromhttps://github.com/zxhhh97/ABot. All\nparametersaresettodefaultastheauthorssuggested.\nE.3.3 Optimizationbasedgraphmatchingalgorithms\n• FINAL[ZhangandTong,2016]isanoptimizationapproach,followinganalignmentconsistency\nprinciple,andtriestomatchnodeswithsimilartopology. Inthecaseofgraphswithoutattributes\nFINALisequivalenttothepopular\n• Isorank[Singhetal.,2008]algorithm,whereasusingNetSimileasaninputtoFINALresulted\nininferiorperformanceandwasthereforeomitted. Weusethecodeinhttps://github.com/\nsizhang92/FINAL-KDD16withH beingthedegreesimilaritymatrix, α = 0.8, maxiter =\n30, tol=1e−4assuggestedintherepository.\n• ConeAlign[Chenetal.,2020]isagraphembeddingbasedapproach.Thematchingisoptimized\nineachiterationbytheWassersteinProcrustesdistancesbetweenthematchedembeddingscal-\nculatedonaminibatchinordertopreservescalability. Weusetheofficialimplementationfrom\nhttps://github.com/GemsLab/CONE-Alignandpreservedallthesuggestedparameters.\n• S-GWL[Xuetal.,2019b]matchestwogivengraphsbyretrievingnodecorrespondencefromthe\noptimaltransportassociatedwiththeGromov-Wassersteindiscrepancybetweenthegraphs. We\nusetheimplementationbytheauthorsinhttps://github.com/HongtengXu/s-gwl,since\nthe performance of S-GWL is very sensitive to the parameter gamma, as suggested by the\nauthors, we fine-tuned this parameter over the range of [0.001,0.1] for each dataset on the\ncleanedgraph,andusethatoptimalparameterforallotherexperimentsonthisdataset.\nE.4 T-GAEmodeldetails\nAsillustratedinFigure1,thestructureofourproposedencoderconsistsoftwoMLPsandaseries\nof GNN layers. The node features are processed by a 2-layer MLP and passed to all the GNN\nlayers. WeaddskipconnectionsbetweenthisMLPlayerandallthesubsequentGNNlayers. The\noutputsofallGNNlayersareconcatenatedandpassedtoanother2-layerMLP,followedbyalinear\ndecodertogeneratethereconstructedgraph. Themodelisoptimizedendtoendbyequation7. For\ngraphmatchingexperiments, sinceweconsiderthegeneralcasewheregraphsaregivenwithout\nnodeattributes,weusethe7structuralfeaturesproposedin[Berlingerioetal.,2013]. Thefeatures\ninclude the degree of each node, the local and average clustering coefficient, and the number of\nedges, outgoingedges, andneighborsineachnode’segonet. Thisinputfeatureisappliedforall\nGNN-basedmethods. Asaresult,theperformanceofNetSimile,vanillaGAEandWAlignprovide\nmeasurestoassessthebenefitofusingT-GAEfornodeembedding.Notethatonecanchoosedifferent\nmessagepassingfunctionsasf andginEquation(4),andanystructure-preservingnodefeatures.\nOurreportedresultsarebasedonGIN[Xuetal.,2019a]andNetsimile[Berlingerioetal.,2013].\nF MoreBaselineResults\nWepresentthegraphmatchingaccuracyforthebaselinemethodsthatarenotpermutationequavariant\ninTable3,andsub-graphmatchingaccuracyonDoubanOnline-OfflinedatasetforGraphWavein\nTable4,asitisnotscalableonACM/DBLP.\n21\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nTable3: Graphmatchingaccuracyon10randomlyperturbedsamplesunderdifferentlevelsofedge\neditingforVGAE,LINEandDeepWalk.\nDataset VGAE LINE DeepWalk\nCelegans 0.3±0.1 1.0±0.5 1.8±0.6\nArenas 0.1±0.1 0.2±0.1 0.3±0.2\nDouban 0.0±0.0 0.0±0.0 0.1±0.0\nCora 0.1±0.0 0.0±0.0 0.1±0.0\nCelegans 0.3±0.1 1.0±0.4 1.2±0.5\nArenas 0.1±0.1 0.1±0.1 0.3±0.1\nDouban 0.0±0.0 0.0±0.0 0.1±0.0\nCora 0.1±0.1 0.1±0.0 0.2±0.1\nCelegans 0.6±0.3 0.9±0.3 1.0±0.3\nArenas 0.2±0.1 0.2±0.2 0.2±0.1\nDouban 0.0±0.0 0.0±0.0 0.0±0.0\nCora 0.1±0.0 0.1±0.0 0.1±0.0\nTable4: Sub-graphmatchingperformanceforGraphWaveonDoubanOnline-Offline\nHitrate GraphWave\nHit@1 0.09\nHit@5 0.36\nHit@10 0.81\nHit@50 4.74\nHit@100 9.12\nG DegreePerturbationModelResults\nTable5: Graphmatchingaccuracyon10randomlyperturbedsamplesunderdifferentlevelsofedge\nremovalonDegreemodel. TheproposedT-GAEistrainedonthecleanCelegans,Arena,Douban,\nand Cora networks, and tested on noisy versions of them and the larger Dblp, and Coauthor CS.\nAccuracyabove80%ishighlightedingreen,60%to80%accuracyisinyellow,andperformance\nbelow60%isinred.\nFeatureEngineeringbased Optimizationbased GNNbased\nDataset\\Algorithm\nSpectral Netsimile GraphWave FINAL S-GWL ConeAlign WAlign GAE T-GAE\nCelegans 87.8±1.5 72.7±0.9 65.3±1.7 92.2±1.2 93.0±1.5 66.6±1.2 88.4±1.6 90.9±2.6 91.0±1.1\nArenas 97.7±0.4 94.7±0.3 81.7±0.7 97.5±0.3 97.5±0.3 87.8±0.6 97.4±0.5 97.6±0.4 97.8±0.4\nDouban 89.9±0.4 46.4±0.4 17.5±0.2 89.9±0.3 90.1±0.3 68.1±0.4 90.0±0.4 89.5±0.4 90.1±0.3\nCora 85.0±0.4 73.7±0.4 8.3±0.4 87.5±0.7 87.3±0.7 38.5±0.7 87.2±0.4 87.1±0.8 87.5±0.4\nDblp 84.5±0.1 63.7±0.2 doesn’tscale 85.6±0.2 >48hours 44.3±0.6 85.6±0.2 85.2±0.3 85.6±0.2\nCoauthorCS 97.5±0.1 90.9±0.1 doesn’tscale 97.6±0.1 >48hours 75.8±0.5 97.5±0.2 97.6±0.3 97.6±0.1\nCelegans 21.8±16.9 63.1±1.4 4.9±1.1 62.5±4.1 73.1±11.7 61.4±3.2 70.7±4.4 7.8±2.5 63.4±6.1\nArenas 66.0±21.6 90.9±0.7 5.8±2.6 56.7±2.7 92.3±1.2 86.1±0.6 96.0±0.5 0.9±0.5 96.7±0.6\nDouban 9.8±13.5 37.6±0.6 0.6±0.3 35.0±1.1 69.9±1.2 59.6±2.2 81.3±1.9 0.4±0.0 83.7±2.2\nCora 25.3±13.4 64.4±1.2 0.1±0.0 32.0±2.0 31.1±4.4 30.6±2.7 74.6±0.7 28.2±0.3 81.0±2.5\nDblp 3.4±0.8 52.2±0.5 doesn’tscale 24.5±0.6 >48hours 21.1±1.9 67.4±1.1 10.5±0.6 77.1±0.6\nCoauthorCS 8.6±4.6 76.8±0.6 doesn’tscale 19.1±0.5 >48hours 64.0±2.0 88.9±0.8 3.8±0.1 94.0±0.4\nDegreeModel: Inthismodelweonlyremoveedges. Edgeswithhigherdegreesaremorelikelyto\nberemovedtopreservethestructureofthegraph. Specifically,theprobabilityofremovingedge\n(i,j)issetto sijdidj ,whered isthedegreeofnodev ands isthe(i,j)elementofthegraph\n(cid:80) ijsijdidj i i i,j\nadjacency.\nWe test the performance of T-GAE for large-scale network alignment on the degree perturbation\nmodel,asdescribedinSection5.2. WeadoptthesamesettingasinSection5.2totraintheT-GAE\naccording to (6) on small-size networks, i.e., Celegans, Arena, Douban, and Cora, and conduct\ntransferlearningexperimentsonthelargergraphs,i.e.,Dblp,andCoauthorCS.ThetrainedT-GAE\nisusedtogeneratenodeembeddingforthegraphs,andAlgorithm1computestheassignmentmatrix.\nTheresultspresentedinTable5arebasedontheaverageandstandarddeviationofthematching\naccuracyover10randomlygeneratedperturbedsamples.\n22\nnoitabrutrep%0\nnoitabrutrep%1\n%0\n%1\n%5\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nWeobservethatthebenefitofprocessingtheNetSimileembeddingswithGNNsisstillsignificantin\nthisperturbationmodelasweobserveupto46%performanceincreaseatthepresenceofperturbation.\nWhentestingonperturbedgraphsat1%levelofedgeremoval, ourproposedT-GAEconsistently\noutperformsallthecompetingbaselines,whilebeingrobustandefficientwhenperformingnetwork\nalignmentunderdifferentperturbationmodels. Especially,onlarge-scalenetworks,T-GAEisableto\nachieveveryhighlevelsofmatchingaccuracyforbothDblpandCoauthorCS,forp=0%, 1%.\nThebenefitofourproposedT-GAEframeworkinimprovingtheexpressivenessofGNNstillstands\noutifwecomparetheaccuracywithWAlignandGAE.Italsoconsistentlyachievesthebestresult\namongallbaselinemethodsthataresalable.\nH Efficiencyanalysis\nH.1 Runningtimecomparison\nT-GAEisascalableandefficientapproachfornetworkalignment. Weanalyzetheefficiency\nof the proposed graph matching framework by comparing its running time with the competing\nalgorithms. T-GAEachievesatmost×2000lessrunningtime,ongraphmatchingtasks,compared\nto the optimization-based methods, as shown in Table 6. Compared to the existing GNN based\napproaches,T-GAEconsistentlyachievestheshortesttrainingtimeandinferencetime,thisisbecause\nwereplacesomemessagepassinglayersbythelocalMLPwhichservesasanattentionfunctionon\ntheoutputofallGNNlayers,suchmodelsareempiricallyprovedtobemoreefficient,atthesame\ntime,promptingtheexpressivenessofGNNlayers,togeneratenodeembeddingforgraphmatching.\nItshouldbenotedthatT-GAEisalsotheonlyapproachthatistransferable,whichmeansitdoesnot\nneedtore-trainoneverypairofnewgraphs. T-GAEgreatlyimprovesthescalabilityofoptimization\nbasedmethods,aswellastheeffectivenessoftheexistingGNNframeworks.\nFigure5: Trainingtimecomparison(20epoches)betweenT-GAEandWAlignforgraph-matching.\nTGAE_sisthespecificsettingwherewetraintheencoderGNNaccordingtoEquation(7),whereas\nTGAE_tmeanstrainingaccordingtoEquation(8)onafamilyofgraphs. (Celegans,Arenas,Cora,\nDouban)\n23\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nTable6: Runtime(inference+matching)insecondsforthecompetingalgorithmsongraphmatching\ntasks.\nAlgorithm Celegans Arenas Cora Douban Dblp CoauthorCS\nSpectrul 5.712 2.819 54.770 60.298 >48hours >48hours\nNetsimile 1.212 1.560 1.616 4.400 195.542 225.546\nGraphWave 8.308 32.994 131.230 281.629 5470.724 6291.368\nFINAL(Matlab) 0.030 0.081 0.498 1.007 86.788 118.065\nS-GWL 27.844 37.443 311.201 3394.522 >48hours >48hours\nConeAlign 1.333 3.500 13.799 31.955 887.090 1099.145\nWAlign 0.078 0.205 0.766 1.800 169.694 189.032\nGAE 0.074 0.212 0.757 1.749 164.410 184.062\nT-GAE(ours) 0.068 0.201 0.742 1.734 163.836 183.289\nTheproposedtrainingobjective(7)scaleswellfromnetworkswith400nodes[Kunegis,2013]to\ndensernetworkswith×50nodes[Shchuretal.,2018],withminorrunningtimeincrease,compared\ntootherGNN-basedframeworks(WAlign),asshowninFigure5.\nH.2 Matchingalgorithmscomparison\nInthissubsection,weevaluatetheaccuracyandmatchingtimeofdifferentmatchingalgorithms,to\ndemonstratehowmatchingalgorithmsofdifferenttimecomplexityinfluencetheperformanceof\ntheproposedT-GAEframework. Toguaranteefairnessofcomparison,weuseanuntrainedT-GAE\nencodertoencodethegraphs,anduse(1)approximatedNNalgorithmintroducedinSection4.3of\ntimecomplexityO(NlogN). (2)greedyHungarianalgorithmasappliedinSection5and5.3oftime\ncomplexityO(N2). (3)exactHungarianalgorithm[Kuhn,1955b]oftimecomplexityO(N3),where\nN isthenumberofnodesinthegraph. Wereportaverageaccuracyandrunningtimeonmatching10\nrandomlygeneratedsamples.\nTable7: GraphmatchingperformanceandmatchingtimeonCelegansandArenasusinguntrained\nT-GAE encoder. We report results of approximated NN matching algorithm, greedy Hungarian\nalgorithm,andexactHungarianalgorithm. WehighlighttheperformancegainofexactHungarian\novertheapproximatedNNandGreedyHungarian.\nCelegans Arenas\n# Dataset/Perturbation 0 0.01 0.05 0 0.01 0.05\nAcc Time Acc Time Acc Time Acc Time Acc Time Acc Time\nO(NlogN)\n1 ApproximatedNN 89.8±1.0 0.004 0.8±0.3 0.004 0.7±0.2 0.004 98.0±2.6 0.009 0.1±0.1 0.009 0.0±0.0 0.009\nO(N2)\n2 GreedyHungarian 88.4±0.9 0.068 80.3±0.3 0.068 58.2±3.5 0.067 97.6±0.4 0.173 93.1±0.5 0.176 60.2±5.2 0.174\nO(N3)\n3 ExactHungarian 88.5±0.8 0.225 84.0±0.2 24.778 64.7±1.8 73.674 97.6±0.4 0.411 93.8±0.4 562.221 70.0±2.0 1624.713\nO(N3)\n4 AccGainin%(ExactvsApprox/Greedy) -1.3/+0.1 +83.2/+3.7 +64.0/+6.5 -0.4/0 +93.7/+0.7 +70.0/+9.8\nTheperformanceofT-GAEtomatchsmallgraphscanbefurtherimprovedbyadoptingthe\nexactHungarianalgorithm. ComparingExactHungarianandGreedyHungarian,weobservethat\nwhenthereisnoperturbationinvolved,greedyHungarianandexactHungarianachievescomparable\nperformance. However,theexactHungarianalgorithmoutperformsthegreedyversioninoccurrence\nofperturbations,andtheperformancegapincreasesasweintroducemoretopologynoise.Specifically,\nitoffersa6.5%and9.8%matchingaccuracyincreaseonCelegansandArenas,respectively,onan\nuntrainedT-GAEencoder. However,itcantakemorethan×9000longerthanthegreedyalgorithm,\nonArenas5%perturbation,forexample. Thisimpliesthatwheneverapplicable,theexactHungarian\nalgorithmshouldbeappliedtofurtherimprovetheperformanceofT-GAEtomatchsmallgraphs,\nespecially when the noise level is high, but there is a trade-off between matching accuracy and\nefficiency.\nTheefficiencyofT-GAEtomatchpermutedgraphscanbeenhancedbyapplyingtheapproxi-\nmatedNNalgorithm. WeobservethattheapproximatedNNalgorithmweintroducedinSection\n4.3effectivelymatchthealignednodesofpermutedgraphs,andonArenasdatasetof1,133nodes,it\n24\nT-GAE:TransferableGraphAutoencoderforNetworkAlignment\nsaves95%matchingtimecomparedtothegreedyHungarianalgorithm. However,thisalgorithm\nfailstomatchperturbedgraphs. Thisisbecausethe1-dimensionalfeatureisnotexpressiveenough\ntotocatchthetopologicalnoise. OurexperimentsprovethatthisefficientNNalgorithmshouldbe\nappliedwhenwematchverylargescalenetworkswiththeirpermutedversions.\nOverall,wedividegraphmatchingusingT-GAEinthreescenarios: (1)Matchingsmallgraphs,exact\nHungarianalgorithmshouldbeapplied. (2)Matchinglargescalenetworkswithoutperturbation,the\napproximationNNalgorithmshouldbedeployedtoenhanceefficiency. (3)ThegreedyHungarian\nalgorithm provides a good trade-off between time and efficiency for the general form of graph\nmatching.\n25",
    "pdf_filename": "T-GAE_Transferable_Graph_Autoencoder_for_Network_Alignment.pdf"
}