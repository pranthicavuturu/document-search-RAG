{
    "title": "KTO Model Alignment as Prospect Theoretic Optimization",
    "abstract": "Kahneman & Tversky’s prospect theory tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, hu- mans are famously loss-averse. We show that ob- jectives for aligning LLMs with human feedback implicitly incorporate many of these biases—the success of these objectives (e.g., DPO) over cross- entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call human-aware losses (HALOs). However, the utility functions these methods attribute to hu- mans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.",
    "body": "KTO: Model Alignment as Prospect Theoretic Optimization\nKawin Ethayarajh 1 Winnie Xu 2 Niklas Muennighoff 2 Dan Jurafsky 1 Douwe Kiela 1 2\nAbstract\nKahneman & Tversky’s prospect theory tells us\nthat humans perceive random variables in a biased\nbut well-defined manner (1992); for example, hu-\nmans are famously loss-averse. We show that ob-\njectives for aligning LLMs with human feedback\nimplicitly incorporate many of these biases—the\nsuccess of these objectives (e.g., DPO) over cross-\nentropy minimization can partly be ascribed to\nthem belonging to a family of loss functions that\nwe call human-aware losses (HALOs). However,\nthe utility functions these methods attribute to hu-\nmans still differ from those in the prospect theory\nliterature. Using a Kahneman-Tversky model of\nhuman utility, we propose a HALO that directly\nmaximizes the utility of generations instead of\nmaximizing the log-likelihood of preferences, as\ncurrent methods do. We call this approach KTO,\nand it matches or exceeds the performance of\npreference-based methods at scales from 1B to\n30B, despite only learning from a binary signal\nof whether an output is desirable. More broadly,\nour work suggests that there is no one HALO that\nis universally superior; the best loss depends on\nthe inductive biases most appropriate for a given\nsetting, an oft-overlooked consideration.\n1. Introduction\nAligning generative models with human feedback has been\nsuccessfully used to make generations more helpful, factual,\nand ethical, among other desiderata (Ouyang et al., 2022;\nTian et al., 2023). For LLMs, alignment methods such as\nRLHF and DPO have consistently proven to be more benefi-\ncial than doing supervised finetuning (SFT) alone. However,\nhuman feedback is often discussed only in the context of\npreferences (e.g., output yw ≻yl for input x), even though\nit can take many forms (e.g., approval/disapproval of y given\n1Stanford University (first author was an intern at Contex-\ntual AI) 2Contextual AI. Correspondence to: Kawin Ethayarajh\n<kawin@stanford.edu>.\nProceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\nFigure 1. The utility that a human gets from the outcome of a\nrandom variable, as implied by different human-aware losses (HA-\nLOs). Notice that the implied value functions share properties\nsuch as loss aversion with the canonical human value function in\nprospect theory (Tversky & Kahneman, 1992).\nx). This is because preferences, despite being a kind of data\nthat is relatively scarce and expensive to collect in practice\n(Casper et al., 2023), are required by the alignment methods\nshown to work best—RLHF (Christiano et al., 2017) and\nDPO (Rafailov et al., 2023).\nTo understand why these methods work so well, and whether\nfeedback needs to be in preference form, we frame align-\nment through the lens of prospect theory (Kahneman &\nTversky, 1979; Tversky & Kahneman, 1992). Prospect the-\nory explains why humans make decisions about uncertain\nevents that do not maximize their expected value. It formal-\nizes how humans perceive random variables in a biased but\nwell-defined manner; for example, relative to some refer-\nence point, humans are more sensitive to losses than gains,\na property called loss aversion. We show that popular align-\nment methods such as DPO and PPO-Clip (Schulman et al.,\n2017) implicitly model some of these biases, helping ex-\nplain their success independently of the data used (§3.2).\nWe then propose a more general class of such loss functions\ncalled human-aware losses (HALOs).1\n1We use the term human-aware to draw an analogy with how\nhardware-aware methods benefit from being designed around hard-\nware limitations (Dao et al., 2022), not to claim that a simple loss\nfunction is fully aware of human behavior.\n1\narXiv:2402.01306v4  [cs.LG]  19 Nov 2024\n\nModel Alignment as Prospect Theoretic Optimization\nAlthough it is impossible to say that HALOs are categori-\ncally better than non-HALOs, we find that among existing\nmethods, those that meet the definition of a HALO work\nbetter than those that do not (§3.3). We find that DPO per-\nformance can even be matched at most scales by running an\noffline PPO variant on dummy +1/-1 rewards, suggesting\nthat preference data might not be needed if the inductive\nbias in the loss function is good enough. However, despite\nthe surprising success of this simple baseline, it significantly\nlags behind DPO at the 30B LLM scale and suffers from\nhyperparameter sensitivity, making it difficult to use.\nTaking a more principled approach, we derive a HALO us-\ning the model of human utility that Kahneman & Tversky\nproposed to describe how humans make decisions about un-\ncertain monetary outcomes (Tversky & Kahneman, 1992).\nThis approach, which we call Kahneman-Tversky Optimiza-\ntion (KTO), directly maximizes the utility of generations\ninstead of maximizing the log-likelihood of preferences, as\nmost current methods do (§4.1). KTO only requires a binary\nsignal of whether an output is desirable or undesirable for\nan input. This data is more abundant, cheaper, and faster to\ncollect in the real world, making it easier to scale alignment\nin production and rapidly iterate on models. We find that:\n• KTO matches or exceeds DPO performance at scales\nfrom 1B to 30B parameters (§4.3).2 That is, breaking\nup n preferences meant for DPO into 2n examples for\nKTO can yield better LLM generations, as determined\nby closed-ended tasks such as mathematical reasoning\nand open-ended judgments from humans and GPT-4.\n• KTO can handle extreme data imbalances, matching\nDPO performance while using up to 90% fewer desir-\nable examples (i.e., examples of good generations). Its\nsuccess cannot be ascribed to the alignment data being\nsourced from a preference dataset.\n• When the pretrained model is sufficiently good, one\ncan skip supervised finetuning (SFT) and go straight\nto KTO without a loss in generation quality, whereas\nSFT is always needed for best results with DPO.\nThe intent behind KTO was that even if the model learns\nfrom a weaker signal, we could compensate with the higher\nvolume of data that could be accessed in practice; the fact\nthat KTO can match and even outperform DPO on the same\ndata is thus surprising. We conclude by discussing some\ntheoretical explanations for this phenomenon (§4.4). De-\nspite the success of KTO in our experiments, our work\nultimately suggests that there is no one HALO that is uni-\nversally superior; the best HALO depends on the inductive\nbiases appropriate for a given setting, and this choice should\nbe made deliberately instead of defaulting to any one loss.\n2Our code is available on Github; models are on Huggingface.\n2. Background\nFor a detailed survey, please see Appendix A. In brief, LLMs\nare traditionally trained in three stages (Ouyang et al., 2022):\nPretraining\nGiven a large corpus, train the model to max-\nimize the log-likelihood of the next token conditioned on\nthe preceding text. Let π0 denote the pretrained model.\nSupervised Finetuning (SFT)\nFinetune the model to pre-\ndict the next token on data that is more relevant to the down-\nstream task. Often, such data will comprise instructions and\nan appropriate response (i.e., instruction finetuning). Let\nπref denote the finetuned model.\nRLHF\nGiven a dataset D of preferences (x, yw, yl)—\nwhere x is an input, yw, yl are the preferred and dispreferred\noutputs (i.e., yw ≻yl for x), and r∗is the “true” reward\nfunction underlying the preferences—it is first assumed that\nthe probability that yw is preferred to yl can be captured with\na specific function class, typically a Bradley-Terry model\n(Bradley & Terry, 1952). Where σ is the logistic function:\np∗(yw ≻yl|x) = σ(r∗(x, yw) −r∗(x, yl))\n(1)\nSince getting the true reward from a human would be in-\ntractably expensive, a reward model rϕ learns to serve as a\nproxy, done by minimizing the negative log-likelihood of\nthe human preference data:\nLR(rϕ) = Ex,yw,yl∼D[−log σ(rϕ(x, yw) −rϕ(x, yl))]\nBut solely maximizing the reward might come at the ex-\npense of desiderata such as generating grammatical text. To\navoid this, a KL divergence penalty is introduced to restrict\nhow far the language model can drift from πref. Where πθ is\nthe model we are optimizing, the optimal model π∗is that\nwhich maximizes\nEx∈D,y∈πθ[rϕ(x, y)] −βDKL(πθ(y|x)∥πref(y|x))\n(2)\nwhere β > 0 is a hyperparameter. Since this objective is\nnot differentiable, we need to use an RL algorithm like PPO\n(Schulman et al., 2017).\nHowever, RLHF is often slow (largely because of having\nto sample generations) and quite unstable in practice (espe-\ncially in a distributed setting). For this reason, recent work\nhas focused on designing closed-form losses that maximize\nthe margin between the preferred and dispreferred genera-\ntions. In particular, Direct Preference Optimization (DPO)\n(Rafailov et al., 2023) has emerged as a popular alterna-\ntive as it allows the same optimal policy as in RLHF to be\nrecovered under certain conditions:\nLDPO(πθ, πref) = Ex,yw,yl∼D\n\u0014\n−log σ\n\u0012\nβ log πθ(yw|x)\nπref(yw|x) −β log πθ(yl|x)\nπref(yl|x)\n\u0013\u0015\n(3)\n2\n\nModel Alignment as Prospect Theoretic Optimization\n3. A Prospect Theoretic View of Alignment\nTo understand why alignment methods work so well, we\nnow frame them through the lens of prospect theory (Tver-\nsky & Kahneman, 1992). Prospect theory explains why,\nwhen faced with an uncertain event, humans make decisions\nthat do not maximize their expected value. For example,\nbecause humans are loss-averse, given a gamble that returns\n$100 with 80% probability and $60 with 20% probability, a\nperson might accept $60 to avoid the gamble, despite their\ncertainty equivalent of $60 being less than the expected\nvalue of $80.\n3.1. Prospect Theory\nIn prospect theory, human utility depends on a value func-\ntion and a weighting function:3\nDefinition 3.1. A value function v : Z →R maps an out-\ncome z, relative to some reference point z0, to its perceived\n(or subjective) value. For example, these functions capture\nthe fact that humans tend to be more sensitive to relative\nlosses than relative gains of the same magnitude.\nDefinition 3.2. A weighting function ω is the derivative of\na capacity function that maps cumulative probabilities to\nperceived cumulative probabilities. These functions capture,\nfor example, the fact that humans tend to overestimate the\nchance of rare events. Let ωz denote the weight placed on\noutcome z.\nDefinition 3.3. The utility of a random variable Z is a\nfunction of its outcomes: u(Z) ≜P\nz∈Z ωzv(z −z0).\nHowever, because humans do not see the full probability\ndistribution of an LLM, weighting functions are not salient\nto this discussion; we will focus only on value functions. Us-\ning experiments that presented real humans with monetary\ngambles and asked for their certainty equivalent, Tversky &\nKahneman (1992) proposed the following functional form\nfor human value:\nv(z; λ, α, z0) =\n(\n(z −z0)α\nif z ≥z0\n−λ(z0 −z)α\nif z < z0\n(4)\nwhere the median value of hyperparameter α = 0.88 and\nλ = 2.25 across individuals. α controls the curvature of the\nfunction, which reflects risk aversion; λ controls its steep-\nness, which reflects loss aversion. While the shape of the\nmedian Kahneman-Tversky value function is illustrated in\nFigure 1, it should be noted that it varies across individuals\n(Tversky & Kahneman, 1992). There are also other func-\ntional forms for the value function that have been proposed\nin later work (Gurevich et al., 2009). The salient qualities of\na value function are: the existence of a reference point that\n3Cumulative prospect theory is the full name of the expanded\ntheory we dicuss here (Tversky & Kahneman, 1992).\nis used to get the relative gain or loss; concavity in relative\ngains (i.e., diminishing sensitivity away from z0); and loss\naversion (i.e., greater sensitivity to losses).\n3.2. HALOs\nDefinition 3.4 (HALOs). Let θ denote the trainable param-\neters of the model πθ : X →P(Y) being aligned, πref the\nreference model, l : Y →R+ a normalizing factor, and\nrθ(x, y) = l(y) log[πθ(y|x)/πref(y|x)] the implied reward.\nWhere Q(Y ′|x) is a reference point distribution over Y and\nv : R →R is non-decreasing everywhere and concave in\n(0, ∞), the human value of (x, y) is\nv(rθ(x, y) −EQ[rθ(x, y′)])\n(5)\nA function f is a human-aware loss for v if ∃ax,y ∈\n{−1, +1} such that:\nf(πθ, πref) =\nEx,y∼D[ax,yv(rθ(x, y) −EQ[rθ(x, y′)])] + CD\n(6)\nwhere D is the feedback data and CD ∈R is a data-specific\nconstant.\nIn a classic prospect theory experiment, rθ would be the dol-\nlar amount assigned to each outcome; here, rθ is measured\nin nats, as the decrease in conditional surprisal when going\nfrom πref to πθ, normalized according to l. This follows\nnaturally from the next-token prediction objective used to\npretrain and finetune LLMs. As πθ is aligned, we would\nexpect rθ to grow increasingly positive for desirable outputs\nand increasingly negative for undesirable outputs.\nAnother perspective on the reward comes from the RLHF\nobjective in (2). The policy that maximizes this objective\nhas a closed-form expression (Peng et al., 2019; Peters &\nSchaal, 2007); where Z(x) is the partition function:\nπ∗(y|x) =\n1\nZ(x)πref(y|x) exp\n\u0012 1\nβ r∗(x, y)\n\u0013\nLetting l(·) = β, we get\nrθ∗(x, y) = r∗(x, y) −β log Z(x)\n(7)\nUnder θ∗, the HALO-defined reward is just the optimal\nreward shifted by an input-specific term, meaning that rθ∗\nis in the same equivalence class as r∗and would also induce\nthe optimal policy π∗(Lemma 1, Rafailov et al. (2023)).\nThe reference point in a HALO is the expected reward from\nthe human’s perspective, where Q(Y ′|x) describes the ex-\namples that are used to construct a baseline. We require\nthat the value function be concave in gains but not neces-\nsarily convex in losses—unlike the canonical Kahneman-\nTversky value function—because a minority of individuals\nare risk-averse in both the gain and loss regime, resulting in\nconcavity everywhere (Kahneman & Tversky, 1979).\n3\n\nModel Alignment as Prospect Theoretic Optimization\nFigure 2. HALOs (DPO, offline PPO variant) outperform non-HALOs (SLiC, CSFT), as measured by the GPT-4-0613-judged winrate\nof the aligned model’s generations against a hard-to-beat baseline: the outputs that would have been used as the targets for SFT. The y-axis\nhere plots the winrate above chance (i.e., the winrate – 50%). The difference between methods is only significant (p < 0.05) at 13B+\nparameters, and only the HALO-aligned Llama-{13B, 30B} models are able to match the baseline and yield a winrate at or above chance.\nTheorem 3.5. DPO and PPO-Clip are human-aware losses.\nThe proof is deferred to Appendix B. In Figure 1, we can\nsee this more intuitively by plotting the value function (i.e.,\nthe implied human utility).\n3.3. Does being a HALO matter?\nIt is difficult to answer whether being a HALO is useful,\nsince both HALOs and non-HALOs are diverse function\nclasses, but we attempt to do so by comparing popular meth-\nods that qualify as a HALO with those that do not:\n1. CSFT: Conditional SFT is a simple alignment method\nwhere a control token is prepended to the output during\ntraining; then, at inference, the control token corre-\nsponding to desirable generations is appended to the\ninput to induce good generations (Korbak et al., 2023).\nThis is not a HALO, since that would demand that\n−log πref(y|x) always equal the reference point; how-\never, since the reference point is fixed for a given x, this\nis not possible when πref is a non-uniform distribution.\n2. SLiC: Sequence Likelihood Calibration (Zhao et al.,\n2023), which combines a max-margin loss for prefer-\nences with a language modeling loss, is not a HALO\n(for the same reasons given above for CSFT):\nLcal(πθ) = Ex,yw,yl∼D\n\u0014\nmax\n\u0012\n0, δ −log πθ(yw|x)\nπθ(yl|x)\n\u0013\u0015\nLreg(πθ, πref) = Ex∼D,y∼πref(x)[−log πθ(y|x)]\nLSLiC(πθ, πref) = Lcal(πθ) + λregLreg(πθ, πref)\n3. DPO: DPO is a HALO (Theorem 3.5).\n4. PPO (offline): The standard RLHF objective in (2) is\ntypically optimized with PPO-Clip, which works by\n“clipping” how far πθ can drift from the version πold at\nthe previous step:\nLPPO (offline) = −Ex,y,t∼D[min(qθA(x:y<t, yt),\nclip(qθ, 1 −ϵ, 1 + ϵ)A(x:y<t, yt))]\nwhere qθ =\nπθ(yt|x:y<t)\nπold(yt|x:y<t) and A(x:y<t, yt) is the per-\ntoken advantage (i.e., the surplus benefit from produc-\ning a given token in a given state).\nPPO is an online algorithm—generations are sampled\nfrom the current model, judged by a reward model, and\nthen used to update the current version. However, for\na fair comparison with offline methods, we consider a\nvariant of PPO that is only fed offline data. Because\nRLHF is also quite unstable in a distributed setting, we\nnever update πold and keep it as πref, instead clipping\nless conservatively than we traditionally would. Ba-\nheti et al. (2023) found that these changes, along with\ntreating the entire output sequence as a single action,\ngreatly improves stability. However, since RLHF tradi-\ntionally uses token-level advantages, we omit the third\nchange and only preserve the first two. The PPO-Clip\nloss is unchanged and remains a HALO (Theorem 3.5).\nCalling this method PPO is somewhat imprecise, be-\ncause it is offline and takes only one step, but to avoid\nintroducing too many new terms, we will call this PPO\n(offline). Instead of using learned rewards, we simplify\neven further and use dummy +1/-1 rewards4 for yw and\nyl instead. Further details on the implementation of\nthis method can be found in Appendix C.\nExperimental Setup\nWe compare these baselines on two\nmodel families, Pythia-{1.4B, 2.8B, 6.9B, 12B} (Bider-\n4Note that +1/-1 do not refer to the HALO-implied rewards, but\nrather the rewards used for the advantage A(x:y<t, yt) calculation.\n4\n\nModel Alignment as Prospect Theoretic Optimization\nman et al., 2023) and Llama-{7B, 13B, 30B} (Touvron\net al., 2023). This permits us to see how LLM alignment\nscales within a model family (Llama-2 lacks a 30B model,\nhence our use of Llama). Later experiments (§4.3) are done\non Mistral-7B derivatives (Jiang et al., 2023) and Llama-3\n(Dubey et al., 2024). The models are trained on a combina-\ntion of Anthropic-HH (Ganguli et al., 2022), OpenAssistant\n(K¨opf et al., 2023), and SHP (Ethayarajh et al., 2022).\nAll models are aligned under identical settings on the same\ndata, save for hyperparameters unique to them. Similar to\nRafailov et al. (2023), the target sequences for SFT are a\nsubset of {yw}. We use GPT-4-0613 to judge whether\nthe aligned model’s response is better than the SFT target\nfor a given test input with respect to helpfulness, harmless-\nness, and conciseness, a now standard practice (Zheng et al.,\n2023; Li et al., 2023).5 Note that while the SFT target is\nconsidered a desirable output for x, it is by no means the\nbest output, meaning that an aligned model can certainly\nachieve a winrate above 50%, although this is difficult given\nthat many of the target sequences are human-written.\nIn Figure 2, we see the results of this analysis:\n• HALOs either match or outperform non-HALOs\nat every scale, though the gap is only significant (p <\n0.05) at 13B+ model sizes after correcting for multiple\ncomparisons (Holm, 1979). In fact, only the HALO-\naligned Llama-{13B, 30B} models match or exceed a\nwin rate of 50% (i.e., are able to match or exceed the\ngeneration quality of the SFT targets in the test data).\n• Up to a scale of 7B parameters, alignment provides\nvirtually no gains over SFT alone. However, it is\nworth noting that if the base models were more perfor-\nmant, or if the SFT data distribution were less similar to\nthe preference data, then the gains from the alignment\nstage would ostensibly be greater.\n• Despite only using dummy +1/-1 rewards, our of-\nfline PPO variant performs as well as DPO for all\nmodels except Llama-30B. This challenges conven-\ntional wisdom, which places heavy emphasis on reward\nlearning (Casper et al., 2023), and suggests that even\nthe simplest rewards can prove useful when used in a\nloss function that has the right inductive bias. Despite\nits success, our offline PPO baseline still suffers from\nhyperparameter sensitivity and training instability, al-\nbeit not to the same extent as traditional RLHF.\n4. Kahneman-Tversky Optimization\nThe surprising success of offline PPO with dummy +1/-1\nrewards suggests that—with the right inductive biases—a\n5We validate that GPT-4 judgments concur with human judg-\nments in Appendix D.\nbinary signal of good/bad generations may be sufficient\nto reach DPO-level performance, even if the offline PPO\napproach itself was unable to do so past a certain scale\n(§3.3). Taking a more principled approach, we now derive a\nHALO using the Kahneman-Tversky model of human value,\nwhich allows us to directly optimize for utility instead of\nmaximizing the log-likelihood of preferences.\n4.1. Derivation\nThe canonical Kahneman-Tversky value function (4) suffers\nfrom numerical instability during optimization due to the\nexponent a, so we replace it with the logistic function σ,\nwhich is also concave in gains and convex in losses.\nTo control the degree of risk aversion, we introduce a hy-\nperparameter β ∈R+ as part of the value function. The\ngreater β is, the more quickly the value saturates, meaning\nthe human is simultaneously more risk-averse in gains and\nmore risk-seeking in losses. In practice, this has a similar\neffect as β in the DPO loss, which controls how far πθ drifts\nfrom πref, though we introduce it here explicitly to control\nrisk aversion; in DPO, it carries over from the KL constraint\nin the RLHF objective (2) and is part of the reward.\nWe replace the loss aversion coefficient λ in the origi-\nnal Kahneman-Tversky value function (4) with {λD, λU},\nwhere λD and λU are hyperparameters for desirable and\nundesirable outputs respectively; more complex schemes\ncould also be used for importance sampling.\nRather than having just one dispreferred generation serve as\nthe reference point z0, as in DPO, we assume that humans\njudge the quality of y|x in relation to all possible outputs.\nThis implies that Q(Y ′|x) is the policy and that the refer-\nence point is the KL divergence KL(πθ(y′|x)∥πref(y′|x)).\nHowever, as we discuss further below, we end up taking a\nbiased estimate of this term in practice.\nWhere λy denotes λD(λU) when y is desirable(undesirable)\nrespectively, the default KTO loss is:6\nLKTO(πθ, πref) = Ex,y∼D[λy −v(x, y)]\n(8)\nwhere\nrθ(x, y) = log πθ(y|x)\nπref(y|x)\nz0 = KL(πθ(y′|x)∥πref(y′|x))\nv(x, y) =\n(\nλDσ(β(rθ(x, y) −z0)) if y ∼ydesirable|x\nλUσ(β(z0 −rθ(x, y))) if y ∼yundesirable|x\n6An earlier draft separated out λD, λU into a function w. For\nan easier comparison with (4), they—along with β—have been\nmoved into the value function itself. Note that λy exists solely\nto make the loss non-negative and can be removed. The use of\nmultiple x′ to estimate z0 in practice was included in the loss\ndefinition originally, but has now been moved outside for clarity.\n5\n\nModel Alignment as Prospect Theoretic Optimization\nTable 1. Recommended hyperparameter settings for different losses and models when aligned on UltraFeedback, evaluated on the\nbenchmarks discussed in §4.3. The hyperparameter sweeps were done with AdamW, an effective batch size of 32, and λD = λU = 1.\nDepending on your task and ratio of desirable:undesirable examples, the optimal choice of λD, λU might be significantly different.\nModel\nMethod\nLR\nβ\nAlpacaEval (LC) ↑\nBBH ↑\nGSM8K (8-shot) ↑\nLlama-3 8B\nSFT+KTO\n5e-6\n0.05\n10.59\n65.15\n60.20\nLlama-3 8B\nKTO\n5e-6\n0.10\n11.25\n65.26\n57.92\nQwen2.5 3B Instruct\nSFT+KTO\n5e-6\n0.10\n13.01\n32.39\n61.11\nQwen2.5 3B Instruct\nKTO\n5e-6\n0.50\n16.63\n20.41\n60.35\nFor more stable training, we do not backpropagate through\nz0; it exists purely to control the loss saturation.\nIntuitively, KTO works as follows: if the model increases\nthe reward of a desirable example in a blunt manner, then the\nKL penalty also rises and no progress is made. This forces\nthe model to learn exactly what makes an output desirable,\nso that the reward can be increased while keeping the KL\nterm flat (or even decreasing it). The argument works in the\nother direction as well, though the non-negativity of the KL\nterm allows faster saturation in the loss regime.\nKL Estimate\nIn practice, estimating z0 as it is defined\nabove is impractical because sampling from πθ is slow. In-\nstead, we take a biased but convenient estimate by shifting\noutputs in the same microbatch to induce mismatched pairs\n{(x1, y2), (x2, y3), ..., (xm, y0)}, then estimating a shared\nreference point z0 for all examples in the same microbatch\nas follows. Where j = (i + 1) mod m,\nˆz0 = max\n\n0, 1\nm\nX\n1≤i<m\nlog πθ(yj|xi)\nπref(yj|xi)\n\n\nBecause of clamping, our estimator has a positive bias but\nlower variance than the standard unbiased estimator. Al-\nthough it costs an additional forward pass, we use a mis-\nmatched output yj instead of the corresponding yi because\nthe latter have often been deliberately chosen to be canoni-\ncally good or bad outputs, and thus have unrepresentative\nhigh-magnitude rewards. It is worth noting that although our\nestimator is biased, so would the human-perceived reference\npoint, since humans do not perceive the full distribution in-\nduced by πθ and would employ an “availability heuristic”\nthat would overweight outputs for which they have recently\ngiven feedback, regardless of whether those outputs are a\ngood continuation of x (Tversky & Kahneman, 1973).\nIf KTO is preceded by SFT done on the same data that\nis used as desirable feedback and the SFT model is used\nas πref, then the KL estimate will quickly approach zero.\nHaving already learned what is desirable during SFT, the\npolicy will tend to scatter the mass placed on undesirable\nexamples, leading to minimal divergence. Also, because\nthe policy may learn to place less mass on undesirable yi\nregardless of whether it is preceded by xi, ˆz0 might actually\nbe an under-estimate. In such cases, one can avoid the extra\ncomputation and set ˆz0 = 0. However, when KTO is not\npreceded by SFT, or when the SFT data is not a subset of\nthe KTO data, estimating ˆz0 is necessary.\nData\nIf the alignment data is naturally binary, every posi-\ntive example can be assumed to be drawn from ydesirable|x\nand every negative example from yundesirable|x. However, the\ncanonical feedback datasets in academic research (HH, SHP,\nOASST) are in preference format, since the methods that\nhave worked best up until now are preference-based. In our\nexperiments, we convert preference data yw ≻yl by assum-\ning that yw is drawn from the desirable distribution and yl\nfrom the undesirable one. This is a naive assumption, made\nfor the sake of simplicity, and a more complex deconstruc-\ntion of preferences into binary feedback would likely yield\nbetter results, which we leave for future work. To show that\nKTO can be used with non-preference data, we also sub-\nsample exactly one y per x for some experiments (denoted\none-y-per-x), removing any trace of paired preferences at\nthe cost of reducing the data volume.\nIf human feedback is in the form of scores or ratings, the\nsimplest means of incorporating it into KTO is to con-\nstruct a weighting function such that high-magnitude data is\nweighed more and that examples with scores above(below)\nsome threshold are desirable(undesirable). It is also possible\nto construct score-based HALOs from first principles, but\nwe leave the design of such losses to future work.\n4.2. Hyperparameters\nIn Table 1, we provide recommended hyperparameter set-\ntings for Llama-3 8B (Dubey et al., 2024) and Qwen2.5\n3B Instruct (Yang et al., 2024) based on benchmarks such\nas MMLU (0-shot) (Hendrycks et al., 2021), GSM8K (8-\nshot, chain-of-thought) (Cobbe et al., 2021), HumanEval\n(0-shot) (Chen et al., 2021), and BigBench-Hard (3-shot\nchain-of-thought) (Srivastava et al., 2022).\nLearning Rate\nWe find that that the performance of an\naligned model is more sensitive to the learning rate than any\nother hyperparameter. The optimal learning rate for KTO is\n6\n\nModel Alignment as Prospect Theoretic Optimization\nFigure 3. KTO is as good or better than DPO at all scales, as measured by the GPT-4-0613-judged winrate of the aligned model’s\ngenerations against the outputs that would have been used for SFT. In fact, for the Llama models, KTO alone matches the performance of\nSFT+DPO and is significantly better than DPO alone. Error bars denote a 90% binomial confidence interval.\nusually 2x to 10x the optimal learning rate for DPO; since\nthe reference-adjusted reward tends to be much smaller in\nmagnitude for KTO, one needs to use a more aggressive\nlearning rate to compensate. For example, the default learn-\ning rate for DPO is 5e-7 (Rafailov et al., 2023), but we find\nthat a default of 5e-6 works better for KTO. In our experi-\nments, we use the default DPO learning rate with RMSProp\nfor all methods to ensure an apples-to-apples comparison\nwith Rafailov et al. (2023), but when using KTO in practice,\nwe recommend starting at 5e-6 with AdamW and adjusting\nthe learning rate as needed.\nBatch Size\nKTO needs a microbatch size ≥2 to estimate\nthe reference point in a single step. The experiments in this\npaper all use an effective batch size of 32, and in general we\nrecommend using a batch size between 8 and 128.\nRisk Aversion\nThe degree of risk aversion/seeking is con-\ntrolled by β; the greater β is, the greater the risk aversion\nin gains and risk seeking in losses. In practice, lower val-\nues of β in the range [0.01, 0.10] work better for larger\nmodels that have already undergone SFT; higher values of\nβ in the range [0.10, 1.00] work better for smaller models\nundergoing KTO directly, without SFT prior.\nLoss Aversion\nThe default weighting function controls\nthe degree of loss aversion with λD, λU, which are both set\nto 1 by default. In general, where nD and nU refer to the\nnumber of desirable and undesirable examples respectively,\nwe find that it is generally best to set λD, λU such that\nλDnD\nλUnU\n∈\n\u0014\n1, 3\n2\n\u0015\n(9)\nFor example, if there were a 1:10 ratio of desirable to un-\ndesirable examples, we would set λU = 1, λD ∈[10, 15].\nThis interval was determined empirically, and implies that—\nafter adjusting for class imbalances—gain sensitivity yields\nbetter performance than loss sensitivity, ostensibly because\nproducing good outputs is more important than avoiding\nbad outputs for success on most benchmarks. This is not a\nhard rule, however. In tasks where minimizing the downside\nis more important, like toxicity prevention, setting λD, λU\nsuch that λDnD < λUnU may work better. Unless other-\nwise stated, we use λD = λU = 1 in our experiments.\nThe current configuration permits different sensitivities to\ndesirable and undesirable examples based on the premise\nthat upon convergence, all undesirable outputs will yield\nnegative rewards and all desirable outputs will yield positive\nones, in which case λU, λD would directly correspond to λ\nin the original Kahneman-Tversky value function. However,\nduring training itself, a desirable output may have a negative\nreward (and vice-versa), yet λD would be the same regard-\nless of whether rθ(x, y) −z0 were positive or negative. We\nmay want to change this so that not only is asymmetry pos-\nsible upon convergence, but even during training within the\nsame class of output: in this case, using a higher λD when\nrθ(x, y) −z0 is negative than when it is positive. We leave\nthe design of dynamic hyperparameter selection schemes as\ndirections for future work.\n4.3. Experiments\nKTO ≥DPO\nAs seen in Figure 3, when rerunning the\nwinrate evaluation in §3.3, SFT+KTO is competitive with\nSFT+DPO at scales from 1B to 30B, despite the model\nlearning from a weaker signal. KTO alone is better than\nDPO alone for the Llama-{7B, 13B, 30B} models, and this\ngap is significant (p < 0.01) at 7B and 30B even after cor-\nrecting for multiple comparisons (Holm, 1979). Among the\nPythia models, there is no significant difference between the\n7\n\nModel Alignment as Prospect Theoretic Optimization\nTable 2. (top) Results from aligning Zephyr-β-SFT (Tunstall et al.,\n2023) on UltraFeedback for exactly 1 epoch. Even when only\none of the two outputs in each preference is seen by KTO, it still\noutperforms DPO, despite this reducing the volume of data by half\n(one-y-per-x). (middle) Changing the structure of the KTO loss,\neven in subtle ways, makes the aligned model worse, supporting\nour design choices. (bottom) Fixing λU = 1, we try different\nlevels of loss and risk aversion by changing λD and β respectively\n(see Appendix C for more results).\nDataset (→)\nMMLU\nGSM8k\nHumanEval\nBBH\nMetric (→)\nEM\nEM\npass@1\nEM\nSFT\n57.2\n39.0\n30.1\n46.3\nDPO\n58.2\n40.0\n30.1\n44.1\nORPO (λ = 0.1)\n57.1\n36.5\n29.5\n47.5\nKTO (β = 0.1, λD = 1)\n58.6\n53.5\n30.9\n52.6\nKTO (one-y-per-x)\n58.0\n50.0\n30.7\n49.9\nKTO (no z0)\n58.5\n49.5\n30.7\n49.0\nKTO (concave, v = log σ)\n58.3\n42.5\n30.6\n43.2\nKTO (risk-neutral, v(·) = ·)\n57.3\n42.0\n28.8\n6.1\nKTO (no πref, λD = 1.75)\n57.5\n47.5\n29.5\n51.6\nKTO (β = 0.01)\n57.3\n54.0\n33.2\n49.8\nKTO (β = 0.50)\n58.3\n45.0\n32.7\n51.8\nKTO (λD = 0.50)\n57.4\n47.0\n32.2\n30.5\nKTO (λD = 0.75)\n58.3\n52.0\n30.3\n50.6\nKTO (λD = 1.33)\n58.5\n52.0\n32.8\n45.4\nKTO (λD = 2.00)\n58.1\n43.0\n32.1\n45.3\ntwo, suggesting that a minimum model capacity is needed\nfor these differences to emerge. KTO also fares better than\nDPO and other baselines on generative benchmarks (Table\n2). This is most pronounced for certain tasks: on GSM8K,\na mathematical reasoning dataset, just swapping DPO for\nKTO when aligning Zephyr-β-SFT (Tunstall et al., 2023)\non UltraFeedback (Cui et al., 2023) improves performance\nby 13.5 points.\nAt sufficient scale, KTO does not need SFT.\nA KTO-\naligned Llama-{13B, 30B} model is competitive with its\nSFT+KTO counterpart despite not undergoing SFT first,\nand is the only alignment method of the ones we tested\nto show this behavior. This is perhaps due to KTO alone\nkeeping the average response length roughly the same, while\nrunning DPO without SFT prior causes the response length\nto increase dramatically (Figure 4).\nKTO data need not come from preferences.\nMight KTO\nbe secretly benefiting from its 2n examples in the previous\nexperiment coming from n preference pairs instead of a nat-\nurally unpaired data distribution? To test this, we randomly\ndiscard increasingly large fractions of the desirable data be-\nfore KTO-aligning a Llama-7B model. For example, if we\ndiscard 90% of the desirable data while leaving the undesir-\nable data untouched, then the ratio of desirable:undesirable\nexamples goes from 1:1 to 1:10 and the vast majority of un-\ndesirable examples no longer have a preferred counterpart.\nWe handle such imbalances by changing λD, λU to satisfy\nFigure 4. Without doing SFT first, DPO-aligned models tend to\nramble and hallucinate entire conversations. KTO does not suffer\nfrom this issue.\nthe criteria in (9); when we drop 90% of the desirable data,\nwe set λu = 1, λD = 13.33 for example. For Llama-7B,\nwe find that up to 90% of the desirable data can in fact be\ndiscarded while still outperforming DPO (Figure 5).\nWe further verify this claim by aligning Mistral-7B on Ope-\nnAssistant using DPO (on n pairs), standard KTO (on all 2n\noutputs), and KTO where only one y per x is used. Since the\noutput of one y in OpenAssistant is not conditioned on the\nother y for the same x, the latter captures the setting where\nthe data is from an inherently unpaired distribution. Despite\nthe one-y-per-x setup decreasing the amount of training data\nby 72%, the KTO-aligned model still outperforms both its\nDPO counterpart and the official instruction-tuned Mistral-\n7B (Jiang et al., 2023), as seen in Table 3.\nChanging the design of KTO makes it significantly worse.\nFor one, removing the reference point z0—which is neces-\nsary for KTO to qualify as a HALO—causes a 3.6 and 4.0\npoint drop on BBH and GSM8K respectively (Table 2, mid-\ndle). Even changes that allow KTO to remain a HALO are\ntypically suboptimal. For example, removing the symmetry\nof the value function—going from 1−σ(·) to −log σ(·) (i.e.,\nmaking the value function concave everywhere, as in DPO)—\ncauses a 9.4 and 11.0 point drop on BBH and GSM8K re-\nspectively. Making the value function risk-neutral by setting\nit to the identity function leads to a total collapse in BBH\nperformance. Changing the curvature and slope via the risk\nand loss aversion hyperparameters can, depending on the\ntask, improve or degrade performance (Table 2, bottom).\nKTO works without a reference model or SFT, but not as\nwell as standard KTO.\nIf we can avoid storing the refer-\nence model in memory, then we can do much more memory-\nefficient alignment. The naive way to do this is to assume\nthat πref returns a uniform distribution over outputs for all\nx, which simplifies rθ −z0 to log πθ(y|x) −H(πθ(y′|x)),\nwhere H denotes the entropy. As seen in Table 2 (mid-\ndle), if we set λD = 1.75, this memory-efficient variant of\nKTO is better than DPO on some tasks and worse on others,\n8\n\nModel Alignment as Prospect Theoretic Optimization\nFigure 5. A KTO-aligned Llama-7B model can match or exceed\nthe performance of its DPO-aligned counterpart while aligned\non a smaller and highly imbalanced version of the same dataset,\neven with as few as 0.1 positive/desirable examples for every\nnegative/undesirable one.\nthough it still trails standard KTO. We find that it is also\nmore sensitive to the loss aversion hyperparameters; setting\nλD ∈{1.5, 2.0} reduces performance on GSM8K and BBH\nby several points. Still, it strictly outperforms ORPO (Hong\net al., 2024), a recently-proposed reference-free method,\nwhile also using less memory than all existing approaches,\nsince πref need never be loaded into memory and a batch of\nm KTO outputs is smaller than a batch of m preferences\n(i.e., 2m outputs) used for ORPO/DPO.\n4.4. Theoretical Analysis\nKTO was designed with the motivation that even if binary\nfeedback were weaker, one could compensate with sheer\nvolume, as such data is much more abundant, cheaper, and\nfaster to collect than preferences. So why does KTO perform\nas well or better than DPO on the same preference data (that\nhas been broken up)? Greater data efficiency helps, but it is\nnot the only answer, given that even after adjusting for this\nfactor in the one-y-per-x setup, KTO still outperforms.\nIn this section, we will discuss two theoretical explanations\nfor this phenomenon: (1) preference likelihood can be max-\nimized without necessarily maximizing underlying human\nutility; (2) KTO implicitly ignores noisy and intransitive\ndata, which is common in real-world feedback.\nProposition 4.1. As the reward implied by the current policy\ntends to ±∞, the KTO update of πθ tends to zero.\nThis means that if (x, y) is implied by the current policy\nπθ to be too difficult or too easy to learn from, then it is\neffectively ignored. In some cases, this may be a blessing in\ndisguise: since real-world feedback is very noisy (Hoeffler\n& Ariely, 1999), the reason a desirable example has a highly\nnegative implied reward may be because it is mislabelled.\nBy avoiding this hard-to-learn data, KTO avoids fitting to\nnoise. However, this also means that KTO could end up ig-\nTable 3. In aligning Mistral-7B on the OpenAssistant dataset, we\nfind that using KTO with only one output per input still outper-\nforms DPO, despite this restriction reducing the amount of training\ndata by 72%. A 90% binomial confidence interval is given.\nMethod\nWinrate vs. SFT Target\nMistral-7B (unaligned)\n0.525 ± 0.037\nMistral-7B + DPO\n0.600 ± 0.037\nMistral-7B + KTO (all y per x)\n0.652 ± 0.036\nMistral-7B + KTO (one y per x)\n0.631 ± 0.036\nMistral-7B-Instruct\n0.621 ± 0.031\nnoring data that is hard-to-learn but necessary to recover r∗,\npotentially resulting in underfitting to complex distributions.\nSuch underfitting may be mitigated by aligning the model\nwith lower β and for more epochs.\nTheorem 4.2. Assuming the value function is logistic, for a\nreward function r∗\na that maximizes (2), there exists a reward\nfunction in its equivalence class (i.e., r∗\nb(x, y) = r∗\na(x, y) +\nh(x) for some h(x)) that induces the same optimal policy\nπ∗and the same Bradley-Terry preference distribution but\na different human value distribution.\nA key insight from Rafailov et al. (2023) is that reward\nfunctions in the same equivalence class (i.e., differing only\nin an input-specific component) induce the same optimal\npolicy under (2) and the same Bradley-Terry preference\ndistribution. However, we show under mild assumptions\nthat the value distribution—i.e., human utility—is affected\nby such input-specific changes, so maximizing preference\nlikelihood does not mean one is maximizing human utility.\nThis helps explain why the margin between KTO and DPO\nis even bigger in human evaluations than it is in automated\nLLM-as-a-judge evaluations (Appendix D).\nTheorem 4.3. For input x with outputs {ya, yb}, let dataset\nD comprise contradictory preferences ya ≻yb and yb ≻ya\nin proportion p ∈(0.5, 1) and (1 −p) ∈(0, 0.5) respec-\ntively. If p1/βπref(ya|x) < (1 −p)1/βπref(yb|x), then the\noptimal DPO policy is more likely to produce the minority-\npreferred yb; the optimal KTO policy will strictly produce\nthe majority-preferred ya for a loss-neutral value function\n(λD = λU).\nInformally, say there are two contradictory preferences over\nthe output for x, with the majority p preferring ya and the\nminority 1 −p preferring yb. In the worst-case, when p\nis sufficiently low and the reference model is sufficiently\nunaligned, the optimal DPO policy is more likely to produce\nthe minority-preferred output yb even though the implied\nreward rθ(x, yb) > rθ(x, ya). In contrast, the optimal KTO\npolicy will deterministically produce the majority-preferred\nya if the value function is loss-neutral (λD = λU), which\nis the default setting. This suggests that KTO has better\nworst-case outcomes when handling feedback intransitivity.\n9\n\nModel Alignment as Prospect Theoretic Optimization\n4.5. KTO vs. DPO – when to use which?\nWhen human feedback is in a binary format, and especially\nwhen there is an imbalance between the number of desirable\nand undesirable examples, KTO is the natural choice. When\nyour data is in the form of preferences, the choice is less\nclear. Putting aside the greater data efficiency of KTO, our\ntheoretical analysis suggests that if your preference data has\nsufficiently little noise and sufficiently little intransitivity,\nthen DPO will work better, since there is some risk of KTO\nunderfitting; this risk can be mitigated by using a lower\nvalue of β and aligning with KTO for more epochs. But\nif there is enough noise and intransitivity, then the better\nworst-case guarantees of KTO will win out. Most publicly\navailable preference datasets (e.g., SHP, OpenAssistant)\ncontain noisy feedback from many different humans whose\npreferences likely contradict to some extent, which explains\nwhy KTO was able to match or exceed DPO performance\nin our experiments. Even synthetic feedback can be noisy\nand intransitive, which helps explain why KTO outperforms\nDPO when aligning with UltraFeedback.\n5. Future Work\nThe existence of HALOs raises many questions. For one,\nKTO is based on the Kahneman-Tversky value function\nfor monetary gambles, which is almost certainly different\nfrom how humans perceive the relative goodness of text.\nWhat value functions and reference point distributions best\ndescribe how humans perceive language, and how do they\nvary across domains and individuals? How can we identify\nthe best HALO for each individual and setting instead of\nusing one default loss?\nOn a more technical level, important directions include\ndeveloping HALOs that: (1) incorporate granular feedback,\nsuch as a score, especially when optimizing for multiple\ndesiderata; (2) work for other modalities (e.g. images) and\nmodel classes (e.g., diffusion models), especially models\nthat do not produce an explicit distribution over the output\nspace; (3) can resolve contradictions in feedback according\nto different definitions of fairness; (4) are designed to be\nused with online data, where the direction of feedback is\nimplied by rθ or some external reward data.\nEcologically valid evaluation (De Vries et al., 2020), where\nthe aligned models are deployed in real-world settings, are\nalso needed to judge the merits of different HALOs.\n6. Conclusion\nAlthough model alignment has historically been reward-\ncentric, we found that the inductive biases of alignment\nobjectives are critical to their success. Moreover, these in-\nductive biases have analogs in the prospect theory literature,\nsuggesting that they work in part because they reflect human\nbiases in decision-making. We abstracted these insights into\na family of alignment objectives called human-aware losses\n(HALOs). We then proposed a HALO called Kahneman-\nTversky Optimization (KTO) for directly maximizing the\nutility of generations instead of maximizing the likelihood of\npreferences, as existing methods do. Despite only learning\nfrom a binary signal of whether an output is (un)desirable,\nKTO was as good or better than preference-based methods\nin our experiments. More broadly, our work suggests that\nakin to how there is no one reward model that is univer-\nsally superior, there is no one loss function either—the best\nHALO depends on the inductive biases that are most ap-\npropriate for a given setting, and much work remains to be\ndone in identifying the best HALO for each context.\nAcknowledgements\nWe thank Percy Liang, Dilip Arumugam, Arya McCarthy,\nand Nathan Lambert for feedback. We thank Stas Bekman\nand Gautam Mittal for cluster assistance and Alex Manthey\nfor helping with human evaluation.\nImpact Statement\nThe methods discussed in this paper have the potential to\nmake LLMs more helpful and safer, which is often needed\nfor models deployed in production. It is possible that in\nmaking models more helpful, we increase the utility of one\nperson at the expense of broader society. In aligning models\nwith human feedback, one may also—without even fully\nrecognizing it—be aligning to an unrepresentative subset of\nthe population, which may hinder the ability of individuals\noutside that subset to benefit equally from using the model.\nThe data used for LLM alignment, including the datasets\nused in this paper (e.g., SHP, HH, OASST) contain prefer-\nences of groups that are not representative of the broader\npopulation. Biases in this data have the potential to be propa-\ngated downstream when used to align models with methods\nlike KTO, especially when no efforts are made to adjust\nfor the different population. KTO in particular implicitly\nresolves contradictions in feedback by taking the majority-\npreferred outcome for a loss-neutral value function, which\ndoes not comport with many theories of fairness (e.g., Rawl-\nsianism). Since user preferences are, in turn, affected by the\nmodels they interact with, this also risks the homogeniza-\ntion of preferences and utility functions when KTO-aligned\nmodels are deployed at scale. The design of HALOs that\nresolve contradictions in more diverse ways is an important\ndirection for future work.\nOn the other hand, because KTO works with binary feed-\nback, which is more abundant, cheaper, and faster to collect\nin the real world, it significantly lowers the barrier to data\n10\n\nModel Alignment as Prospect Theoretic Optimization\ncollection. This makes it easier to collect feedback from\ntraditionally under-represented groups and serve different\nmodels to different users, instead of just one monolithic\nmodel being served to everyone.\nReferences\nAzar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M.,\nValko, M., and Calandriello, D. A general theoretical\nparadigm to understand learning from human preferences.\nIn International Conference on Artificial Intelligence and\nStatistics, pp. 4447–4455. PMLR, 2024.\nBaheti, A., Lu, X., Brahman, F., Le Bras, R., Sap, M., and\nRiedl, M. Improving language models with advantage-\nbased offline policy gradients. In The Twelfth Interna-\ntional Conference on Learning Representations, 2023.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,\net al. Training a helpful and harmless assistant with rein-\nforcement learning from human feedback. arXiv preprint\narXiv:2204.05862, 2022.\nBiderman, S., Schoelkopf, H., Anthony, Q. G., Bradley,\nH., O’Brien, K., Hallahan, E., Khan, M. A., Purohit, S.,\nPrashanth, U. S., Raff, E., et al. Pythia: A suite for ana-\nlyzing large language models across training and scaling.\nIn International Conference on Machine Learning, pp.\n2397–2430. PMLR, 2023.\nBradley, R. A. and Terry, M. E. Rank analysis of incom-\nplete block designs: I. the method of paired comparisons.\nBiometrika, 39(3/4):324–345, 1952.\nBusa-Fekete, R., Sz¨or´enyi, B., Weng, P., Cheng, W., and\nH¨ullermeier, E. Preference-based reinforcement learning:\nevolutionary direct policy search using a preference-based\nracing algorithm. Machine learning, 97:327–351, 2014.\nCasper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J.,\nRando, J., Freedman, R., Korbak, T., Lindner, D., Freire,\nP., et al. Open problems and fundamental limitations of re-\ninforcement learning from human feedback. Transactions\non Machine Learning Research, 2023.\nChan, L., Critch, A., and Dragan, A. Human irrationality:\nboth bad and good for reward inference. arXiv preprint\narXiv:2111.06956, 2021.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021.\nChen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play\nfine-tuning converts weak language models to strong lan-\nguage models. arXiv preprint arXiv:2401.01335, 2024.\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg,\nS., and Amodei, D. Deep reinforcement learning from\nhuman preferences. Advances in neural information pro-\ncessing systems, 30, 2017.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., Hesse, C., and Schulman, J. Training verifiers to solve\nmath word problems. arXiv preprint arXiv:2110.14168,\n2021.\nCui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G.,\nLiu, Z., and Sun, M. Ultrafeedback: Boosting language\nmodels with high-quality feedback, 2023.\nDao, T., Fu, D., Ermon, S., Rudra, A., and R´e, C. Flashat-\ntention: Fast and memory-efficient exact attention with\nio-awareness. Advances in Neural Information Process-\ning Systems, 35:16344–16359, 2022.\nDe Vries, H., Bahdanau, D., and Manning, C. Towards\necologically valid research on language user interfaces.\narXiv preprint arXiv:2007.14435, 2020.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nEthayarajh, K., Choi, Y., and Swayamdipta, S.\nUnder-\nstanding dataset difficulty with V-usable information. In\nChaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu,\nG., and Sabato, S. (eds.), Proceedings of the 39th In-\nternational Conference on Machine Learning, volume\n162 of Proceedings of Machine Learning Research, pp.\n5988–6008. PMLR, 17–23 Jul 2022.\nGanguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y.,\nKadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse,\nK., et al. Red teaming language models to reduce harms:\nMethods, scaling behaviors, and lessons learned. arXiv\npreprint arXiv:2209.07858, 2022.\nGurevich, G., Kliger, D., and Levy, O. Decision-making\nunder uncertainty–a field study of cumulative prospect\ntheory. Journal of Banking & Finance, 33(7):1221–1229,\n2009.\nHe, X., Liao, L., Zhang, H., Nie, L., Hu, X., and Chua,\nT.-S. Neural collaborative filtering. In Proceedings of\nthe 26th international conference on world wide web, pp.\n173–182, 2017.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\nSong, D., and Steinhardt, J. Measuring massive multitask\nlanguage understanding. Proceedings of the International\nConference on Learning Representations (ICLR), 2021.\n11\n\nModel Alignment as Prospect Theoretic Optimization\nHoeffler, S. and Ariely, D. Constructing stable preferences:\nA look into dimensions of experience and their impact on\npreference stability. Journal of consumer psychology, 8\n(2):113–139, 1999.\nHolm, S. A simple sequentially rejective multiple test pro-\ncedure. Scandinavian journal of statistics, pp. 65–70,\n1979.\nHong, J., Lee, N., and Thorne, J. Reference-free monolithic\npreference optimization with odds ratio. arXiv preprint\narXiv:2403.07691, 2024.\nJain, A., Wojcik, B., Joachims, T., and Saxena, A. Learning\ntrajectory preferences for manipulators via iterative im-\nprovement. Advances in neural information processing\nsystems, 26, 2013.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\nKahneman, D. and Tversky, A. Prospect theory: An analysis\nof decision under risk. Econometrica, 47(2):263–292,\n1979.\nK¨opf, A., Kilcher, Y., von R¨utte, D., Anagnostidis, S.,\nTam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stan-\nley, O., Nagyfi, R., et al. Openassistant conversations–\ndemocratizing large language model alignment. arXiv\npreprint arXiv:2304.07327, 2023.\nKorbak, T., Shi, K., Chen, A., Bhalerao, R. V., Buckley,\nC., Phang, J., Bowman, S. R., and Perez, E. Pretrain-\ning language models with human preferences. In Inter-\nnational Conference on Machine Learning, pp. 17506–\n17533. PMLR, 2023.\nKoren, Y., Bell, R., and Volinsky, C. Matrix factorization\ntechniques for recommender systems. Computer, 42(8):\n30–37, 2009.\nKreutzer, J., Uyheng, J., and Riezler, S. Reliability and\nlearnability of human bandit feedback for sequence-to-\nsequence reinforcement learning. In Proceedings of the\n56th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp. 1777–\n1788, 2018.\nKwon, M., Biyik, E., Talati, A., Bhasin, K., Losey, D. P.,\nand Sadigh, D. When humans aren’t optimal: Robots that\ncollaborate with risk-aware humans. In Proceedings of\nthe 2020 ACM/IEEE international conference on human-\nrobot interaction, pp. 43–52, 2020.\nLi, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Alpacae-\nval: An automatic evaluator of instruction-following\nmodels.\nhttps://github.com/tatsu-lab/\nalpaca_eval, 2023.\nLu, X., Welleck, S., Hessel, J., Jiang, L., Qin, L., West,\nP., Ammanabrolu, P., and Choi, Y. Quark: Controllable\ntext generation with reinforced unlearning. Advances in\nneural information processing systems, 35:27591–27609,\n2022.\nMunos, R., Valko, M., Calandriello, D., Azar, M. G., Row-\nland, M., Guo, Z. D., Tang, Y., Geist, M., Mesnard, T.,\nMichi, A., et al. Nash learning from human feedback.\narXiv preprint arXiv:2312.00886, 2023.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information\nProcessing Systems, 35:27730–27744, 2022.\nPeng, X. B., Kumar, A., Zhang, G., and Levine, S.\nAdvantage-weighted regression: Simple and scalable\noff-policy reinforcement learning.\narXiv preprint\narXiv:1910.00177, 2019.\nPeters, J. and Schaal, S. Reinforcement learning by reward-\nweighted regression for operational space control. In\nProceedings of the 24th international conference on Ma-\nchine learning, pp. 745–750, 2007.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D.,\nErmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. In Thirty-\nseventh Conference on Neural Information Processing\nSystems, 2023.\nRosset, C., Cheng, C.-A., Mitra, A., Santacroce, M., Awadal-\nlah, A., and Xie, T. Direct nash optimization: Teaching\nlanguage models to self-improve with general preferences.\narXiv preprint arXiv:2404.03715, 2024.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nGarriga-Alonso, A., et al. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of language\nmodels. arXiv preprint arXiv:2206.04615, 2022.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,\nVoss, C., Radford, A., Amodei, D., and Christiano,\nP. F. Learning to summarize with human feedback. Ad-\nvances in Neural Information Processing Systems, 33:\n3008–3021, 2020.\n12\n\nModel Alignment as Prospect Theoretic Optimization\nSun, L., Zhan, W., Hu, Y., and Tomizuka, M. Interpretable\nmodelling of driving behaviors in interactive driving sce-\nnarios based on cumulative prospect theory. In 2019 IEEE\nIntelligent Transportation Systems Conference (ITSC), pp.\n4329–4335. IEEE, 2019.\nSwamy, G., Dann, C., Kidambi, R., Wu, Z. S., and Agarwal,\nA. A minimaximalist approach to reinforcement learning\nfrom human feedback. arXiv preprint arXiv:2401.04056,\n2024.\nTian, K., Mitchell, E., Yao, H., Manning, C. D., and Finn,\nC. Fine-tuning language models for factuality. arXiv\npreprint arXiv:2311.08401, 2023.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023.\nTunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul,\nK., Belkada, Y., Huang, S., von Werra, L., Fourrier, C.,\nHabib, N., Sarrazin, N., Sanseviero, O., Rush, A. M.,\nand Wolf, T. Zephyr: Direct distillation of lm alignment,\n2023.\nTversky, A. and Kahneman, D. Availability: A heuristic for\njudging frequency and probability. Cognitive psychology,\n5(2):207–232, 1973.\nTversky, A. and Kahneman, D. Advances in prospect theory:\nCumulative representation of uncertainty. Journal of Risk\nand uncertainty, 5:297–323, 1992.\nvon Werra, L., Belkada, Y., Tunstall, L., Beeching, E.,\nThrush, T., Lambert, N., and Huang, S.\nTrl: Trans-\nformer reinforcement learning.\nhttps://github.\ncom/huggingface/trl, 2020.\nWelleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K.,\nand Weston, J. Neural text generation with unlikelihood\ntraining. In International Conference on Learning Repre-\nsentations, 2019.\nXu, H., Sharaf, A., Chen, Y., Tan, W., Shen, L., Van Durme,\nB., Murray, K., and Kim, Y. J. Contrastive preference\noptimization: Pushing the boundaries of llm performance\nin machine translation. arXiv preprint arXiv:2401.08417,\n2024.\nYang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C.,\nLi, C., Li, C., Liu, D., Huang, F., et al. Qwen2 technical\nreport. arXiv preprint arXiv:2407.10671, 2024.\nYuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J.,\nand Weston, J. Self-rewarding language models. arXiv\npreprint arXiv:2401.10020, 2024.\nZhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and\nLiu, P. J. Slic-hf: Sequence likelihood calibration with\nhuman feedback. arXiv preprint arXiv:2305.10425, 2023.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. arXiv\npreprint arXiv:2306.05685, 2023.\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford,\nA., Amodei, D., Christiano, P., and Irving, G. Fine-tuning\nlanguage models from human preferences. arXiv preprint\narXiv:1909.08593, 2019.\n13\n\nModel Alignment as Prospect Theoretic Optimization\nA. Related Work\nLLM Alignment\nHuman feedback has been used to improve LLM capabilities in translation (Kreutzer et al., 2018), sum-\nmarization (Stiennon et al., 2020), sentiment-conditioned generation (Ziegler et al., 2019), and instruction-following (Ouyang\net al., 2022). The RLHF framework (Christiano et al., 2017; Bai et al., 2022) traditionally used to accomplish this is detailed\nin §2. Still, momentum has largely shifted in favor of closed-form losses that directly operate on offline preferences, such\nas DPO (Rafailov et al., 2023). This single stage of optimization distinguishes DPO from the conventional approach in\npreference-based RL, which learns a reward and then fits the policy to those rewards (Jain et al., 2013; Busa-Fekete et al.,\n2014). Other preference-based losses include CPO (Xu et al., 2024) and IPO (Azar et al., 2024).\nBinary Feedback\nDespite not being a human-aware loss, unlikelihood training was among the first methods to align\nlanguage models using a binary signal (Welleck et al., 2019). However, Korbak et al. (2023) found unlikelihood training to\nbe worse than the CSFT baseline we tested in this work, which is among various approaches that convert a binary/discrete\nsignal into a control token (Lu et al., 2022). Learning from sparse binary feedback is a staple of information retrieval and\nrecommender systems (He et al., 2017; Koren et al., 2009). Many retrieval-augmented generation systems use contrastive\nlearning to ensure that generations are grounded. This can be framed as learning from synthetic binary feedback, although\ndepending on the implementation, it may be the retriever and not the LLM that is updated (Lu et al., 2022).\nOnline Alignment\nA recent string of work has centered on the idea of “self-training” or “self-play”, during which the\npolicy is continually aligned on online data sampled from itself and then filtered (Chen et al., 2024; Yuan et al., 2024). Many\nof these approaches frame the learning of a preference model as a two-player min-max game between two policies (Munos\net al., 2023; Rosset et al., 2024; Swamy et al., 2024). In theory, KTO can also be adapted for online alignment, though we\nleave this as a direction for future work.\nProspect Theory\nProspect theory, despite being influential in behavioral economics, has had a muted impact in machine\nlearning, with work concentrated in human-robot interaction (Kwon et al., 2020; Sun et al., 2019; Chan et al., 2021).\nB. Proofs\nTheorem 3.5 (restated)\nDPO and PPO-Clip are human-aware loss functions.\nProof. For a loss f to be a HALO, we need to first construct the human value\nv(rθ(x, y) −EQ[rθ(x, y′)])\nwhere rθ(x, y) = l(x, y) log πθ(y|x)\nπref(y|x) is the implied reward (normalized by factor l(y)), Q(Y ′|x) is an input-conditioned\nreference point distribution, and v : R →R is a value function (in the prospect theoretic sense) that is non-decreasing\neverywhere and concave in (0, ∞).\nThe DPO loss is\nLDPO(πθ, πref) = Ex,yw,yl\n\u0014\n−log σ\n\u0012\nβ log πθ(yw|x)\nπref(yw|x) −β log πθ(yl|x)\nπref(yl|x)\n\u0013\u0015\nwhere β > 0 is a hyperparameter. DPO meets the criteria with the following construction: l(y) = β; rθ = β log πθ(y|x)\nπref(y|x);\nv(·) = log σ(·) is increasing and concave everywhere; Q places all mass on (x, yl), where yl is a dispreferred output for x\nsuch that y ≻yl; and ax,y = −1.\nThe PPO-Clip loss is\nLPPO (offline) = −Ex,y,t∼D[min(qθA(x:y<t, yt), clip(qθ, 1 −ϵ, 1 + ϵ)A(x:y<t, yt))]\nwhere qθ = πθ(yt|x:y<t)\nπref(yt|x:y<t) are the token-level probability ratios (where y<t denotes the output sequence up to the t-th token),\nA denotes the token-level advantages, and ϵ ∈(0, 1) is a hyperparameter.\nSince this is a token-level objective, let x:y<t denote the actual input and the token yi the actual output for the purpose\nof framing this as a HALO. The advantage function A(x:y<t, yt) can be expressed as Qπ(x:y<t, yt) −V π(x:y<t), the\n14\n\nModel Alignment as Prospect Theoretic Optimization\ndifference between the action-value and value functions. Because V π(x:y<t) = Ey∼πQπ(x:y<t, y), the reference point\ndistribution is simply the policy.\nThe HALO-defined reward rθ is then implied by the product qθQπ(x:y<t, y). Assume without loss of generality that Qπ is\nnon-negative, since a constant can be added to Qπ without changing the advantage. Then means ∃u ≥1, qθQπ(x:y<t, y) =\nlog u = log ˆπθ(x:y<t, y)/ˆπref(x:y<t, y), where ˆπθ, ˆπref are some implied policy and reference distributions. It is trivial to\nshow that the latter exist but are not unique.\nFor clarity, we can first write the value function piecewise. Where qθA = rθ −z0 in the HALO notation:\nv(qθA) =\n(\nA min(qθ, 1 + ϵ)\nif A(x:y<t, yt) ≥0\nA max(qθ, 1 −ϵ)\nif A(x:y<t, yt) < 0\nwhich we can combine as v(qθA) = min(qθA, A(1 + sign(qθA)ϵ)). ax,y = −1 completes the construction.\nProposition 4.1 (restated)\nAs the reward rθ(x, y) implied by the current policy tends to ±∞, the KTO update of πθ tends\nto zero.\nProof. Where d(y) is -1(+1) when y is desirable(undesirable), λy is λD(λU) when y is desirable(undesirable), and\nz = rθ(x, y) −z0, the derivative of the KTO loss is\n∇θLKTO(πθ, πref) = Ex,y∼D [d(y)λyσ(βz)(1 −σ(βz))β∇θ log πθ(y|x)]\n(10)\nNote that we do not backpropagate through the KL term in the KTO loss and β, λy > 0. This gradient is simple to interpret:\nif y is desirable, then d(y) is negative and we push up the probability of πθ(y|x) to minimize the loss; if y is undesirable,\nthen d(y) is positive and we push down the probability of πθ(y|x) to minimize the loss. As rθ tends to ±∞, the gradient\nwill tend to zero since either (1 −σ(βz)) or σ(βz) will tend to zero.\nTheorem 4.2 (restated)\nAssuming the value function is logistic, for a reward function r∗\na that maximizes (2), there exists\na reward function in its equivalence class (i.e., r∗\nb(x, y) = r∗\na(x, y) + h(x) for some h(x)) that induces the same optimal\npolicy π∗and the same Bradley-Terry preference distribution but a different human value distribution.\nProof. Following the definition in Rafailov et al. (2023), we say r∗\na and r∗\nb are in the same equivalence class if there exists\nsome function h(x) such that r∗\nb(x, y) = r∗\na(x, y) + h(x). From Lemma 1 in Rafailov et al. (2023), we know that two\nfunctions in the same equivalence class induce the same optimal policy:\nπ∗\nra(y|x) =\n1\nZ(x)πref(y|x) exp\n\u0012 1\nβ r∗\na(x, y)\n\u0013\n=\n1\nP\ny πref(y|x) exp\n\u0010\n1\nβ r∗a(x, y)\n\u0011\nexp\n\u0010\n1\nβ h(x)\n\u0011πref(y|x) exp\n\u0012 1\nβ r∗\na(x, y)\n\u0013\nexp\n\u0012 1\nβ h(x)\n\u0013\n=\n1\nP\ny πref(y|x) exp\n\u0010\n1\nβ (r∗a(x, y) + h(x))\n\u0011πref(y|x) exp\n\u0012 1\nβ (r∗\na(x, y) + h(x))\n\u0013\n= π∗\nrb(y|x)\nFor a Bradley-Terry model of preferences, it is trivial to show that p(yw ≻yl|x) is unaffected by h(x) since it is added to the\nreward of both yw and yl. We will now show that the two reward functions do not necessarily induce the same distribution\nof human values.\nFirst, we assume\nA Taylor series expansion of the human value of r∗\na(x, y) around 0 would be:\nσ(0) + σ′(0)(r∗\na(x, y) −z0) + σ′′(0)\n2\n(r∗\na(x, y) −z0)2 + ...\n15\n\nModel Alignment as Prospect Theoretic Optimization\nA Taylor series expansion of the value of r∗\na(x, y) + h(x) around h(x) would be:\nσ(h(x)) + σ′(h(x))(r∗\na(x, y) −z0) + σ′′(h(x))\n2\n(r∗\na(x, y) −z0)2 + ...\nSince σ is strictly monotonic, for these series to be equal, we must have h(x) = 0. If this is not the case, then the values of\nr∗\na(x, y) and r∗\nb(x, y) will be different. Thus two arbitrary reward functions in the same equivalence class do not induce the\nsame distribution of human values.\nTheorem 4.3 (restated)\nFor input x with outputs {ya, yb}, let dataset D comprise contradictory preferences ya ≻yb and\nyb ≻ya in proportion p ∈(0.5, 1) and (1 −p) ∈(0, 0.5) respectively. If p1/βπref(ya|x) < (1 −p)1/βπref(yb|x), then the\noptimal DPO policy is more likely to produce the minority-preferred yb; the optimal KTO policy will strictly produce the\nmajority-preferred ya for a loss-neutral value function (λD = λU).\nProof. Where u = β(rθ(x, ya) −rθ(x, yb)), we can write the total DPO loss for x as\nLDPO(x) = p(−log σ(u)) + (1 −p)(−log σ(−u))\nTaking the derivative with respect to u and setting to zero, we get\n0 = −pσ(u)σ(−u)\nσ(u)\n+ (1 −p)σ(−u)σ(u)\nσ(−u)\n= −p(1 −σ(u)) + (1 −p)σ(u) = −p + σ(u)\n=⇒u = σ−1(p)\nβr∗\nθ(x, ya) = σ−1(p) + βr∗\nθ(x, yb)\nβ log π∗\nθ(ya|x)\nπref(ya|x) = log\np\n1 −p + β log π∗\nθ(yb|x)\nπref(yb|x)\nπ∗\nθ(ya|x) =\n\u0012\np\n1 −p\n\u00131/β\n· πref(ya|x)\nπref(yb|x) · π∗\nθ(yb|x)\nThus when p1/βπref(ya|x) < (1−p)1/βπref(yb|x), we have π∗\nθ(ya|x) < π∗\nθ(yb|x), meaning the optimal DPO policy is more\nlikely to produce the minority-preferred yb.\nWhere ua = β(rθ(x, ya) −EQ[rθ(x, y′)]) and ub = β(rθ(x, yb) −EQ[rθ(x, y′)]), noting that 1 −σ(−u) = σ(u), we can\nwrite the total KTO loss for x as\nLKTO(x) = pλD(1 −σ(ua)) + (1 −p)λUσ(ua) + pλUσ(ub) + (1 −p)λD(1 −σ(ub))\n= pλD + ((1 −p)λU −pλD)σ(ua) + (1 −p)λD + (pλU −(1 −p)λD)σ(ub)\n= λD + ((1 −p)λU −pλD)σ(ua) + (pλU −(1 −p)λD)σ(ub)\n= λD + λD((1 −2p)σ(ua) + (2p −1)σ(ub))\n(under loss neutrality)\nGiven that p > 0.5 by assumption and λD > 0 by definition, the KTO loss is decreasing in ua and increasing in ub—and thus\ndecreasing in rθ(x, ya) and increasing in rθ(x, yb) respectively. The optimal KTO policy is thus π∗\nθ(y|x) = 1[y = ya].\nC. Implementations\nSLiC\nInstead of sampling from the reference model to calculate the Lreg as Zhao et al. (2023) do—as it is very slow—we\njust apply the cross-entropy loss to the SFT data, assuming that the reference model recovers the SFT distribution.\nDPO\nWe use the implementation of DPO in the code provided by Rafailov et al. (2023). We found that, as mentioned\nin the original paper, β = 0.1 works best for most settings. Other training configurations, such as the learning rate and\noptimizer, were borrowed from the original paper.\nCSFT\nThe control tokens used for generating the good and bad outputs are ⟨|good|⟩and ⟨|bad|⟩respectively, following\nthe precedent set in Korbak et al. (2023).\n16\n\nModel Alignment as Prospect Theoretic Optimization\nKTO\nWe use a β = 0.1 in our experiments unless otherwise specified (the same setting as for DPO), as it is close-to-\noptimal for most settings. By default, λD = λU = 1. In experiments on imbalanced data subsampled from [SHP, HH,\nOASST], we found that setting λU, λD such that the effective ratio of desirable:undesirable examples was 4:3 worked best,\nregardless of which group was in the minority (see (9)). However, in running data-imbalanced experiments on UltraFeedback,\nwe found that an effective ratio of 1:1 worked best. The other hyperparameters (e.g., learning rate) are the same as in DPO.\nPPO\nPPO-Clip is the traditional means of optimizing the RLHF objective (2). However, most implementations of\nPPO-Clip for LLM alignment suffer from instability, particularly during distributed training. We find that running the\nPPO-Clip objective on offline data with the following “tricks” leads to much more stable training:\n• We never update the reference distribution (i.e., the policy only takes one step in the trust region). Baheti et al. (2023)\nrecommend this as well. To accommodate for this conservative change, we clip the probability ratios more liberally,\nfinding that an asymmetric interval of [0.25, 4.0] works best instead of the small symmetrical interval (e.g., [0.8, 1.2])\nthat is traditionally recommended.\n• Including a KL penalty (between the policy and reference distributions) in addition to the clipping makes training more\nstable, as is also done in the implementation by von Werra et al. (2020). We find that it is important to estimate the KL\nterm not using the entire distribution but rather as the mean difference in the predicted log probabilities of the actual\noutput tokens (i.e., the labels). We suspect that this makes a difference because the rest of the distribution can be poorly\ncalibrated.\n• The value of a state is generally predicted by some value head attached to the policy model; the value loss is the MSE\nbetween the predicted value and the discounted sum of future rewards for each token. This is a linear layer in many\nRLHF implementations (von Werra et al., 2020). However, we find that backpropagating the value loss through this\nhead and the policy leads to worse performance. Instead, we make the value head a 3-layer MLP and detach it from the\ncomputational graph, so that the value losses are not backpropagated through the policy model but the value head still\nhas sufficient capacity to learn good estimates.\nD. Human Evaluation\nFor human evaluation, we randomly sampled 256 prompts from the OpenAssistant test set and generated outputs from\nMistral 7B models aligned with DPO and KTO. All inputs were multi-turn conversations between a user and an assistant,\nwhere the LLM played the role of the assistant (see Table 6 for an example) and the last turn in the input was that of the user.\nThese were sent to a third-party data annotation service where a pool of workers picked either the generated output or the\nSFT target (from the OpenAssistant dataset) as the more appropriate response by the assistant. Any questions that required\nspecific domain experience (e.g., coding) were skipped, leading to 214 comparisons for DPO and KTO each.\nThe winrates of the aligned model over the SFT targets are 72.9% ± 5.3 for KTO and 62.1% ± 5.7 for DPO (where the\nintervals are 90% binomial confidence intervals). In contrast, Table 3 contains the winrates when the same experiment is\nrun with GPT-4 as a judge instead: 65.2% ± 3.6 for KTO and 60.0% ± 3.7 for DPO. Thus although there is no significant\ndifference in the GPT-4-based evaluation, there is a significant difference with human evaluation at p < 0.05. We found that\n68.7% of the individual human judgments concurred with the GPT-4 judgments for KTO; this number fell to 65.9% for\nDPO.\nE. Additional Experiments\n17\n\nModel Alignment as Prospect Theoretic Optimization\nTable 4. KTO alignment with only desirable/positive examples (i.e., the same data that would be used for SFT), yields similar results to\nSFT when β is small; this is intuitive because it is less bound to the reference model. λD is fixed at 1.\nDataset (→)\nMMLU\nGSM8k\nHumanEval\nBBH\nTydiQA\nAlpacaEval 2\nAvg.\nSetup (→)\n0-shot\n8-shot, CoT\n0-shot, Synth.\n3-Shot, CoT\n1-Shot, GP\n0-shot\nMetric (→)\nEM\nEM\npass@1\nEM\nF1\n% Win\nSFT\n57.2\n1.0\n54.4\n33.2\n25.4\n7.17\n29.7\nKTO (β = 0.1)\n57.8\n12.5\n53.2\n29.5\n28.0\n5.46\n31.1\nKTO (β = 0.01)\n57.3\n1.5\n53.7\n33.4\n26.5\n5.31\n29.6\nFigure 6. AlpacaEval 2 winrates of Mistral7B aligned with various methods on the OpenAssistant dataset. GPT-4-turbo with chain-of-\nthought is used as the LM judge. KTO outperforms the SFT and DPO-aligned models at all temperatures.\nTable 5. Results from aligning Zephyr-β-SFT (Tunstall et al., 2023) on UltraFeedback. Note that the base model had not been finetuned\non UltraFeedback but a similar dataset called UltraChat; the first row here denotes the finetuning results. All the methods below see\nexactly 1 epoch of UltraFeedback, for an apples-to-apples comparison. AlpacaEval 2 was run using GPT4 Turbo CoT as the LM judge.\nAll evaluations were run with sampling temperature 0.7. Among the KTO variants: “one-y-per-x” denotes training standard KTO with\neither the positive or negative output (not both); “no z0” refers to a constant reference point of 0; “concave” refers to a variant where the\nhuman value function is concave in the loss regime (i.e., minimize −log(·) instead of 1 −(·)); “no πref” refers to a reference model-free\nvariant; “risk-neutral” refers to a variant where the human value function is just the identity function, implying risk neutrality.\nDataset (→)\nMMLU\nGSM8k\nHumanEval\nBBH\nTydiQA\nAlpacaEval 2\nAvg.\nSetup (→)\n0-shot\n8-shot, CoT\n0-shot, Synth.\n3-Shot, CoT\n1-Shot, GP\n0-shot\nMetric (→)\nEM\nEM\npass@1\nEM\nF1\n% Win\nSFT\n57.2\n39.0\n30.1\n46.3\n36.3\n6.4\n35.9\nDPO\n58.2\n40.0\n30.1\n44.1\n36.5\n7.8\n36.1\nORPO (λ = 0.1)\n57.1\n36.5\n29.5\n47.5\n38.5\n5.0\n35.7\nKTO\n58.6\n53.5\n30.9\n52.6\n31.2\n12.5\n39.9\nKTO (one-y-per-x)\n58.0\n50.0\n30.7\n49.9\n37.7\n11.6\n39.7\nKTO (no z0)\n58.5\n49.5\n30.7\n49.0\n29.0\n12.1\n38.1\nKTO (concave)\n58.3\n42.5\n30.6\n43.2\n36.6\n7.4\n36.4\nKTO (no πref, λD = 1.75)\n57.5\n47.5\n29.5\n51.6\n36.8\n9.7\n38.8\nKTO (risk-neutral)\n57.3\n42.0\n28.8\n6.1\n33.6\n10.3\n29.7\n18\n\nModel Alignment as Prospect Theoretic Optimization\nPrompt\n< |user| >\nI need some show recommendations that are similar to Breaking Bad.\n< |assistant| >\nAre you looking for something with drama, violence, drugs, politics and murder?\n< |user| >\nThis is exactly what I’m looking for in a show\n< |assistant| >\nSFT\nWell, you’ve just asked for one of the best shows ever made, and they all have those elements\nin some combination. There are a lot of shows with those elements.\nSFT+KTO\nSome great choices for you might be Narcos, Sons of Anarchy, and Better Call Saul. They\nare all very popular and have a lot of the same themes as Breaking Bad.\nSFT+DPO\nWell, you might like *******, which is also a dark, suspenseful drama about political intrigue\nand crime. Or you might also like *******, which is also a dark, suspenseful drama about\ncrime and corruption in law enforcement.\n—\nMy friend has some really bad opinions about TV shows, and I want to correct him, can you\nhelp?\n—\nSure! You can try to politely explain to your friend why his or her opinion is incorrect or why\nyou disagree. However, it’s important to keep in mind that different people enjoy different\nthings. If your friend has some strongly held opinions about TV shows, there’s no reason\nwhy you need to agree with him. You can both enjoy shows that you like and your friend can\nenjoy the shows that he likes. That’s just the way it is.\n—\nIs there any TV show similar to Breaking Bad that doesn’t have violence in it?\n—\nIt’s very hard to find a TV show that doesn’t have any violence in it at all. However, there are\nsome TV shows that are not focused on violence. For example, you might like ******* or\n*******, which are both TV shows about people who solve crimes without using violence\nor killing people.\n—\nDo you have any shows that are similar to Breaking Bad but without all the crime and\nviolence?\n—\nSure! You might like ******* or *******, which are both dramas with dark themes and\nstories, without the high level of violence and brutality in Breaking Bad. Or you might like\n*******, which is also a dark and suspenseful drama.\n—\nWhere can I find a list of TV shows similar to Breaking Bad, with all the plot elements and\ndetails?\n—\nI’m not sure there’s an exact list of TV shows similar to Breaking Bad, but you can search\nonline to find shows like this. For example, you might like *******, which is also a dark\nand suspenseful drama about crime and power ...\nSFT+PPO (offline)\nWell, you’ve just asked for one of the best shows ever made, and they only made 5 seasons\nof it. If you want a similar show with a similarly good story, you might want to check out\nGame of Thrones, which is a bit different but also tells an epic story about power, politics\nand violence. Let me know if you need any more recommendations, it’s no problem!\nTable 6. Sample generations from the different aligned versions of Llama-30B for a prompt about show recommendations (all models\nwere aligned with data following the user-assistant format). Note that the SFT answer is not helpful and the SFT+DPO answer hallucinates\nmultiple turns of the conversation (in fact, we had to truncate the answer shown here because the complete answer is too long). The\nSFT+PPO (offline) answer is helpful but only provides one recommendation, while SFT+KTO is succinct and provides multiple options.\n19",
    "pdf_filename": "KTO_Model_Alignment_as_Prospect_Theoretic_Optimization.pdf"
}