{
    "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
    "abstract": "Kahneman&Tversky’sprospecttheorytellsus thathumansperceiverandomvariablesinabiased butwell-definedmanner(1992);forexample,hu- mansarefamouslyloss-averse. Weshowthatob- jectivesforaligningLLMswithhumanfeedback implicitlyincorporatemanyofthesebiases—the successoftheseobjectives(e.g.,DPO)overcross- entropy minimization can partly be ascribed to thembelongingtoafamilyoflossfunctionsthat wecallhuman-awarelosses(HALOs). However, theutilityfunctionsthesemethodsattributetohu- mansstilldifferfromthoseintheprospecttheory Figure1.The utility that a human gets from the outcome of a literature. UsingaKahneman-Tverskymodelof randomvariable,asimpliedbydifferenthuman-awarelosses(HA- humanutility,weproposeaHALOthatdirectly LOs). Notice that the implied value functions share properties maximizes the utility of generations instead of suchaslossaversionwiththecanonicalhumanvaluefunctionin maximizingthelog-likelihoodofpreferences,as prospecttheory(Tversky&Kahneman,1992). currentmethodsdo. WecallthisapproachKTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B,despiteonlylearningfromabinarysignal x). Thisisbecausepreferences,despitebeingakindofdata ofwhetheranoutputisdesirable. Morebroadly, thatisrelativelyscarceandexpensivetocollectinpractice ourworksuggeststhatthereisnooneHALOthat (Casperetal.,2023),arerequiredbythealignmentmethods isuniversallysuperior;thebestlossdependson showntoworkbest—RLHF(Christianoetal.,2017)and theinductivebiasesmostappropriateforagiven DPO(Rafailovetal.,2023). setting,anoft-overlookedconsideration. Tounderstandwhythesemethodsworksowell,andwhether feedbackneedstobeinpreferenceform,weframealign- 1.Introduction ment through the lens of prospect theory (Kahneman & Tversky,1979;Tversky&Kahneman,1992). Prospectthe- Aligninggenerativemodelswithhumanfeedbackhasbeen oryexplainswhyhumansmakedecisionsaboutuncertain successfullyusedtomakegenerationsmorehelpful,factual, eventsthatdonotmaximizetheirexpectedvalue. Itformal- andethical,amongotherdesiderata(Ouyangetal.,2022; izeshowhumansperceiverandomvariablesinabiasedbut Tianetal.,2023). ForLLMs,alignmentmethodssuchas well-defined manner; for example, relative to some refer- RLHFandDPOhaveconsistentlyproventobemorebenefi- encepoint,humansaremoresensitivetolossesthangains, cialthandoingsupervisedfinetuning(SFT)alone. However, apropertycalledlossaversion. Weshowthatpopularalign- humanfeedbackisoftendiscussedonlyinthecontextof mentmethodssuchasDPOandPPO-Clip(Schulmanetal., preferences(e.g.,outputy ≻y forinputx),eventhough w l 2017) implicitly model some of these biases, helping ex- itcantakemanyforms(e.g.,approval/disapprovalofygiven plain their success independently of the data used (§3.2). 1Stanford University (first author was an intern at Contex- Wethenproposeamoregeneralclassofsuchlossfunctions tualAI)2ContextualAI.Correspondenceto: KawinEthayarajh calledhuman-awarelosses(HALOs).1 <kawin@stanford.edu>. 1Weusethetermhuman-awaretodrawananalogywithhow Proceedings of the 41st International Conference on Machine hardware-awaremethodsbenefitfrombeingdesignedaroundhard- Learning,Vienna,Austria.PMLR235,2024.Copyright2024by warelimitations(Daoetal.,2022),nottoclaimthatasimpleloss theauthor(s). functionisfullyawareofhumanbehavior. 1 4202 voN 91 ]GL.sc[ 4v60310.2042:viXra",
    "body": "KTO: Model Alignment as Prospect Theoretic Optimization\nKawinEthayarajh1 WinnieXu2 NiklasMuennighoff2 DanJurafsky1 DouweKiela12\nAbstract\nKahneman&Tversky’sprospecttheorytellsus\nthathumansperceiverandomvariablesinabiased\nbutwell-definedmanner(1992);forexample,hu-\nmansarefamouslyloss-averse. Weshowthatob-\njectivesforaligningLLMswithhumanfeedback\nimplicitlyincorporatemanyofthesebiases—the\nsuccessoftheseobjectives(e.g.,DPO)overcross-\nentropy minimization can partly be ascribed to\nthembelongingtoafamilyoflossfunctionsthat\nwecallhuman-awarelosses(HALOs). However,\ntheutilityfunctionsthesemethodsattributetohu-\nmansstilldifferfromthoseintheprospecttheory\nFigure1.The utility that a human gets from the outcome of a\nliterature. UsingaKahneman-Tverskymodelof\nrandomvariable,asimpliedbydifferenthuman-awarelosses(HA-\nhumanutility,weproposeaHALOthatdirectly\nLOs). Notice that the implied value functions share properties\nmaximizes the utility of generations instead of\nsuchaslossaversionwiththecanonicalhumanvaluefunctionin\nmaximizingthelog-likelihoodofpreferences,as\nprospecttheory(Tversky&Kahneman,1992).\ncurrentmethodsdo. WecallthisapproachKTO,\nand it matches or exceeds the performance of\npreference-based methods at scales from 1B to\n30B,despiteonlylearningfromabinarysignal\nx). Thisisbecausepreferences,despitebeingakindofdata\nofwhetheranoutputisdesirable. Morebroadly,\nthatisrelativelyscarceandexpensivetocollectinpractice\nourworksuggeststhatthereisnooneHALOthat\n(Casperetal.,2023),arerequiredbythealignmentmethods\nisuniversallysuperior;thebestlossdependson\nshowntoworkbest—RLHF(Christianoetal.,2017)and\ntheinductivebiasesmostappropriateforagiven\nDPO(Rafailovetal.,2023).\nsetting,anoft-overlookedconsideration.\nTounderstandwhythesemethodsworksowell,andwhether\nfeedbackneedstobeinpreferenceform,weframealign-\n1.Introduction\nment through the lens of prospect theory (Kahneman &\nTversky,1979;Tversky&Kahneman,1992). Prospectthe-\nAligninggenerativemodelswithhumanfeedbackhasbeen\noryexplainswhyhumansmakedecisionsaboutuncertain\nsuccessfullyusedtomakegenerationsmorehelpful,factual,\neventsthatdonotmaximizetheirexpectedvalue. Itformal-\nandethical,amongotherdesiderata(Ouyangetal.,2022;\nizeshowhumansperceiverandomvariablesinabiasedbut\nTianetal.,2023). ForLLMs,alignmentmethodssuchas\nwell-defined manner; for example, relative to some refer-\nRLHFandDPOhaveconsistentlyproventobemorebenefi-\nencepoint,humansaremoresensitivetolossesthangains,\ncialthandoingsupervisedfinetuning(SFT)alone. However,\napropertycalledlossaversion. Weshowthatpopularalign-\nhumanfeedbackisoftendiscussedonlyinthecontextof\nmentmethodssuchasDPOandPPO-Clip(Schulmanetal.,\npreferences(e.g.,outputy ≻y forinputx),eventhough\nw l 2017) implicitly model some of these biases, helping ex-\nitcantakemanyforms(e.g.,approval/disapprovalofygiven\nplain their success independently of the data used (§3.2).\n1Stanford University (first author was an intern at Contex- Wethenproposeamoregeneralclassofsuchlossfunctions\ntualAI)2ContextualAI.Correspondenceto: KawinEthayarajh calledhuman-awarelosses(HALOs).1\n<kawin@stanford.edu>.\n1Weusethetermhuman-awaretodrawananalogywithhow\nProceedings of the 41st International Conference on Machine hardware-awaremethodsbenefitfrombeingdesignedaroundhard-\nLearning,Vienna,Austria.PMLR235,2024.Copyright2024by warelimitations(Daoetal.,2022),nottoclaimthatasimpleloss\ntheauthor(s). functionisfullyawareofhumanbehavior.\n1\n4202\nvoN\n91\n]GL.sc[\n4v60310.2042:viXra\nModelAlignmentasProspectTheoreticOptimization\nAlthoughitisimpossibletosaythatHALOsarecategori- 2.Background\ncallybetterthannon-HALOs,wefindthatamongexisting\nForadetailedsurvey,pleaseseeAppendixA.Inbrief,LLMs\nmethods, those that meet the definition of a HALO work\naretraditionallytrainedinthreestages(Ouyangetal.,2022):\nbetterthanthosethatdonot(§3.3). WefindthatDPOper-\nformancecanevenbematchedatmostscalesbyrunningan\nPretraining Givenalargecorpus,trainthemodeltomax-\nofflinePPOvariantondummy+1/-1rewards,suggesting\nimizethelog-likelihoodofthenexttokenconditionedon\nthat preference data might not be needed if the inductive\ntheprecedingtext. Letπ denotethepretrainedmodel.\nbiasinthelossfunctionisgoodenough. However,despite 0\nthesurprisingsuccessofthissimplebaseline,itsignificantly\nSupervisedFinetuning(SFT) Finetunethemodeltopre-\nlagsbehindDPOatthe30BLLMscaleandsuffersfrom\ndictthenexttokenondatathatismorerelevanttothedown-\nhyperparametersensitivity,makingitdifficulttouse.\nstreamtask. Often,suchdatawillcompriseinstructionsand\nTakingamoreprincipledapproach,wederiveaHALOus- an appropriate response (i.e., instruction finetuning). Let\ningthemodelofhumanutilitythatKahneman&Tversky π denotethefinetunedmodel.\nref\nproposedtodescribehowhumansmakedecisionsaboutun-\ncertainmonetaryoutcomes(Tversky&Kahneman,1992). RLHF Given a dataset D of preferences (x,y ,y )—\nw l\nThisapproach,whichwecallKahneman-TverskyOptimiza- wherexisaninput,y ,y arethepreferredanddispreferred\nw l\ntion (KTO), directly maximizes the utility of generations outputs (i.e., y ≻ y for x), and r∗ is the “true” reward\nw l\ninsteadofmaximizingthelog-likelihoodofpreferences,as functionunderlyingthepreferences—itisfirstassumedthat\nmostcurrentmethodsdo(§4.1).KTOonlyrequiresabinary theprobabilitythaty ispreferredtoy canbecapturedwith\nw l\nsignalofwhetheranoutputisdesirableorundesirablefor aspecificfunctionclass,typicallyaBradley-Terrymodel\naninput. Thisdataismoreabundant,cheaper,andfasterto (Bradley&Terry,1952). Whereσisthelogisticfunction:\ncollectintherealworld,makingiteasiertoscalealignment\np∗(y ≻y |x)=σ(r∗(x,y )−r∗(x,y )) (1)\ninproductionandrapidlyiterateonmodels. Wefindthat: w l w l\nSince getting the true reward from a human would be in-\n• KTOmatchesorexceedsDPOperformanceatscales tractablyexpensive,arewardmodelr learnstoserveasa\nϕ\nfrom1Bto30Bparameters(§4.3).2 Thatis,breaking proxy,donebyminimizingthenegativelog-likelihoodof\nupnpreferencesmeantforDPOinto2nexamplesfor thehumanpreferencedata:\nKTOcanyieldbetterLLMgenerations,asdetermined\nL (r )=E [−logσ(r (x,y )−r (x,y ))]\nbyclosed-endedtaskssuchasmathematicalreasoning R ϕ x,yw,yl∼D ϕ w ϕ l\nandopen-endedjudgmentsfromhumansandGPT-4.\nBut solely maximizing the reward might come at the ex-\n• KTOcanhandleextremedataimbalances,matching penseofdesideratasuchasgeneratinggrammaticaltext. To\nDPOperformancewhileusingupto90%fewerdesir- avoidthis,aKLdivergencepenaltyisintroducedtorestrict\nableexamples(i.e.,examplesofgoodgenerations). Its howfarthelanguagemodelcandriftfromπ . Whereπ is\nref θ\nsuccesscannotbeascribedtothealignmentdatabeing themodelweareoptimizing,theoptimalmodelπ∗isthat\nsourcedfromapreferencedataset. whichmaximizes\n• When the pretrained model is sufficiently good, one E [r (x,y)] −βD (π (y|x)∥π (y|x)) (2)\nx∈D,y∈πθ ϕ KL θ ref\ncanskipsupervisedfinetuning(SFT)andgostraight\nwhereβ > 0isahyperparameter. Sincethisobjectiveis\ntoKTOwithoutalossingenerationquality,whereas\nnotdifferentiable,weneedtouseanRLalgorithmlikePPO\nSFTisalwaysneededforbestresultswithDPO.\n(Schulmanetal.,2017).\nTheintentbehindKTOwasthatevenifthemodellearns However,RLHFisoftenslow(largelybecauseofhaving\nfromaweakersignal,wecouldcompensatewiththehigher tosamplegenerations)andquiteunstableinpractice(espe-\nvolumeofdatathatcouldbeaccessedinpractice;thefact ciallyinadistributedsetting). Forthisreason,recentwork\nthatKTOcanmatchandevenoutperformDPOonthesame hasfocusedondesigningclosed-formlossesthatmaximize\ndata is thus surprising. We conclude by discussing some themarginbetweenthepreferredanddispreferredgenera-\ntheoretical explanations for this phenomenon (§4.4). De- tions. Inparticular,DirectPreferenceOptimization(DPO)\nspite the success of KTO in our experiments, our work (Rafailov et al., 2023) has emerged as a popular alterna-\nultimatelysuggeststhatthereisnooneHALOthatisuni- tiveasitallowsthesameoptimalpolicyasinRLHFtobe\nversallysuperior;thebestHALOdependsontheinductive recoveredundercertainconditions:\nbiasesappropriateforagivensetting,andthischoiceshould L (π ,π )=E\nbemadedeliberatelyinsteadofdefaultingtoanyoneloss.\nDPO θ ref x,yw,yl∼D\n(cid:20) (cid:18) π (y |x) π (y |x) (cid:19)(cid:21) (3)\n−logσ βlog θ w −βlog θ l\n2OurcodeisavailableonGithub;modelsareonHuggingface. π (y |x) π (y |x)\nref w ref l\n2\nModelAlignmentasProspectTheoreticOptimization\n3.AProspectTheoreticViewofAlignment isusedtogettherelativegainorloss;concavityinrelative\ngains(i.e.,diminishingsensitivityawayfromz );andloss\n0\nTo understand why alignment methods work so well, we\naversion(i.e.,greatersensitivitytolosses).\nnowframethemthroughthelensofprospecttheory(Tver-\nsky & Kahneman, 1992). Prospect theory explains why,\n3.2.HALOs\nwhenfacedwithanuncertainevent,humansmakedecisions\nthat do not maximize their expected value. For example, Definition3.4(HALOs). Letθdenotethetrainableparam-\nbecausehumansareloss-averse,givenagamblethatreturns etersofthemodelπ :X →P(Y)beingaligned,π the\nθ ref\n$100with80%probabilityand$60with20%probability,a reference model, l : Y → R+ a normalizing factor, and\npersonmightaccept$60toavoidthegamble,despitetheir r (x,y)=l(y)log[π (y|x)/π (y|x)]theimpliedreward.\nθ θ ref\ncertainty equivalent of $60 being less than the expected WhereQ(Y′|x)isareferencepointdistributionoverY and\nvalueof$80. v : R → Risnon-decreasingeverywhereandconcavein\n(0,∞),thehumanvalueof(x,y)is\n3.1.ProspectTheory\nv(r (x,y)−E [r (x,y′)]) (5)\nθ Q θ\nInprospecttheory,humanutilitydependsonavaluefunc-\nA function f is a human-aware loss for v if ∃ a ∈\ntionandaweightingfunction:3 x,y\n{−1,+1}suchthat:\nDefinition3.1. Avaluefunctionv :Z →Rmapsanout-\ncomez,relativetosomereferencepointz 0,toitsperceived f(π θ,π ref)=\n(6)\n(orsubjective)value. Forexample,thesefunctionscapture E [a v(r (x,y)−E [r (x,y′)])]+C\nx,y∼D x,y θ Q θ D\nthe fact that humans tend to be more sensitive to relative\nwhereDisthefeedbackdataandC ∈Risadata-specific\nlossesthanrelativegainsofthesamemagnitude. D\nconstant.\nDefinition3.2. Aweightingfunctionωisthederivativeof\na capacity function that maps cumulative probabilities to Inaclassicprospecttheoryexperiment,r wouldbethedol-\nθ\nperceivedcumulativeprobabilities. Thesefunctionscapture, laramountassignedtoeachoutcome;here,r ismeasured\nθ\nforexample,thefactthathumanstendtooverestimatethe innats,asthedecreaseinconditionalsurprisalwhengoing\nchanceofrareevents. Letω denotetheweightplacedon from π to π , normalized according to l. This follows\nz ref θ\noutcomez. naturallyfromthenext-tokenpredictionobjectiveusedto\npretrain and finetune LLMs. As π is aligned, we would\nDefinition 3.3. The utility of a random variable Z is a θ\nfunctionofitsoutcomes:\nu(Z)≜(cid:80)\nω v(z−z ).\nexpectr\nθ\ntogrowincreasinglypositivefordesirableoutputs\nz∈Z z 0\nandincreasinglynegativeforundesirableoutputs.\nHowever, because humans do not see the full probability\nAnotherperspectiveontherewardcomesfromtheRLHF\ndistributionofanLLM,weightingfunctionsarenotsalient\nobjectivein(2). Thepolicythatmaximizesthisobjective\ntothisdiscussion;wewillfocusonlyonvaluefunctions.Us-\nhasaclosed-formexpression(Pengetal.,2019;Peters&\ningexperimentsthatpresentedrealhumanswithmonetary\nSchaal,2007);whereZ(x)isthepartitionfunction:\ngamblesandaskedfortheircertaintyequivalent,Tversky&\n(cid:18) (cid:19)\nKahneman(1992)proposedthefollowingfunctionalform 1 1\nπ∗(y|x)= π (y|x)exp r∗(x,y)\nforhumanvalue: Z(x) ref β\n(cid:40)\n(z−z )α ifz ≥z Lettingl(·)=β,weget\nv(z;λ,α,z )= 0 0 (4)\n0 −λ(z 0−z)α ifz <z 0 r θ∗(x,y)=r∗(x,y)−βlogZ(x) (7)\nwherethemedianvalueofhyperparameterα = 0.88and Under θ∗, the HALO-defined reward is just the optimal\nλ=2.25acrossindividuals. αcontrolsthecurvatureofthe rewardshiftedbyaninput-specificterm,meaningthatr θ∗\nfunction,whichreflectsriskaversion;λcontrolsitssteep- isinthesameequivalenceclassasr∗andwouldalsoinduce\nness,whichreflectslossaversion. Whiletheshapeofthe theoptimalpolicyπ∗(Lemma1,Rafailovetal.(2023)).\nmedianKahneman-Tverskyvaluefunctionisillustratedin\nThereferencepointinaHALOistheexpectedrewardfrom\nFigure1,itshouldbenotedthatitvariesacrossindividuals\nthehuman’sperspective,whereQ(Y′|x)describestheex-\n(Tversky&Kahneman,1992). Therearealsootherfunc-\namples that are used to construct a baseline. We require\ntionalformsforthevaluefunctionthathavebeenproposed\nthatthevaluefunctionbeconcaveingainsbutnotneces-\ninlaterwork(Gurevichetal.,2009). Thesalientqualitiesof\nsarilyconvexin losses—unlikethe canonicalKahneman-\navaluefunctionare: theexistenceofareferencepointthat\nTverskyvaluefunction—becauseaminorityofindividuals\n3Cumulativeprospecttheoryisthefullnameoftheexpanded arerisk-averseinboththegainandlossregime,resultingin\ntheorywedicusshere(Tversky&Kahneman,1992). concavityeverywhere(Kahneman&Tversky,1979).\n3\nModelAlignmentasProspectTheoreticOptimization\nFigure2.HALOs(DPO,offlinePPOvariant)outperformnon-HALOs(SLiC,CSFT),asmeasuredbytheGPT-4-0613-judgedwinrate\nofthealignedmodel’sgenerationsagainstahard-to-beatbaseline:theoutputsthatwouldhavebeenusedasthetargetsforSFT.They-axis\nhereplotsthewinrateabovechance(i.e.,thewinrate–50%).Thedifferencebetweenmethodsisonlysignificant(p<0.05)at13B+\nparameters,andonlytheHALO-alignedLlama-{13B,30B}modelsareabletomatchthebaselineandyieldawinrateatorabovechance.\nTheorem3.5. DPOandPPO-Cliparehuman-awarelosses. “clipping”howfarπ candriftfromtheversionπ at\nθ old\nthepreviousstep:\nTheproofisdeferredtoAppendixB.InFigure1,wecan\nL =−E [min(q A(x:y ,y ),\nseethismoreintuitivelybyplottingthevaluefunction(i.e., PPO(offline) x,y,t∼D θ <t t\ntheimpliedhumanutility). clip(q θ,1−ϵ,1+ϵ)A(x:y <t,y t))]\nwhereq = πθ(yt|x:y<t) andA(x:y ,y )istheper-\n3.3.DoesbeingaHALOmatter? θ πold(yt|x:y<t) <t t\ntokenadvantage(i.e.,thesurplusbenefitfromproduc-\nIt is difficult to answer whether being a HALO is useful, ingagiventokeninagivenstate).\nsince both HALOs and non-HALOs are diverse function\nPPOisanonlinealgorithm—generationsaresampled\nclasses,butweattempttodosobycomparingpopularmeth-\nfromthecurrentmodel,judgedbyarewardmodel,and\nodsthatqualifyasaHALOwiththosethatdonot:\nthenusedtoupdatethecurrentversion. However,for\nafaircomparisonwithofflinemethods,weconsidera\n1. CSFT:ConditionalSFTisasimplealignmentmethod\nvariantofPPOthatisonlyfedofflinedata. Because\nwhereacontroltokenisprependedtotheoutputduring\nRLHFisalsoquiteunstableinadistributedsetting,we\ntraining; then, at inference, the control token corre-\nneverupdateπ andkeepitasπ ,insteadclipping\nold ref\nspondingtodesirablegenerationsisappendedtothe\nless conservatively than we traditionally would. Ba-\ninputtoinducegoodgenerations(Korbaketal.,2023).\nhetietal.(2023)foundthatthesechanges,alongwith\nThis is not a HALO, since that would demand that\ntreatingtheentireoutputsequenceasasingleaction,\n−logπ (y|x)alwaysequalthereferencepoint;how-\nref greatlyimprovesstability. However,sinceRLHFtradi-\never,sincethereferencepointisfixedforagivenx,this\ntionallyusestoken-leveladvantages,weomitthethird\nisnotpossiblewhenπ isanon-uniformdistribution.\nref changeandonlypreservethefirsttwo. ThePPO-Clip\n2. SLiC:SequenceLikelihoodCalibration(Zhaoetal., lossisunchangedandremainsaHALO(Theorem3.5).\n2023),whichcombinesamax-marginlossforprefer- CallingthismethodPPOissomewhatimprecise,be-\nenceswithalanguagemodelingloss,isnotaHALO causeitisofflineandtakesonlyonestep,buttoavoid\n(forthesamereasonsgivenaboveforCSFT): introducingtoomanynewterms,wewillcallthisPPO\n(cid:20) (cid:18) (cid:19)(cid:21) (offline). Insteadofusinglearnedrewards,wesimplify\nπ (y |x)\nL cal(π θ)=E x,yw,yl∼D max 0,δ−log πθ (yw |x) evenfurtherandusedummy+1/-1rewards4fory wand\nθ l y instead. Further details on the implementation of\nl\nL (π ,π )=E [−logπ (y|x)]\nreg θ ref x∼D,y∼πref(x) θ thismethodcanbefoundinAppendixC.\nL (π ,π )=L (π )+λ L (π ,π )\nSLiC θ ref cal θ reg reg θ ref\nExperimentalSetup Wecomparethesebaselinesontwo\n3. DPO:DPOisaHALO(Theorem3.5). model families, Pythia-{1.4B, 2.8B, 6.9B, 12B} (Bider-\n4. PPO(offline): ThestandardRLHFobjectivein(2)is 4Notethat+1/-1donotrefertotheHALO-impliedrewards,but\ntypically optimized with PPO-Clip, which works by rathertherewardsusedfortheadvantageA(x:y ,y )calculation.\n<t t\n4\nModelAlignmentasProspectTheoreticOptimization\nman et al., 2023) and Llama-{7B, 13B, 30B} (Touvron binary signal of good/bad generations may be sufficient\netal.,2023). ThispermitsustoseehowLLMalignment to reach DPO-level performance, even if the offline PPO\nscaleswithinamodelfamily(Llama-2lacksa30Bmodel, approach itself was unable to do so past a certain scale\nhenceouruseofLlama). Laterexperiments(§4.3)aredone (§3.3). Takingamoreprincipledapproach,wenowderivea\nonMistral-7Bderivatives(Jiangetal.,2023)andLlama-3 HALOusingtheKahneman-Tverskymodelofhumanvalue,\n(Dubeyetal.,2024). Themodelsaretrainedonacombina- whichallowsustodirectlyoptimizeforutilityinsteadof\ntionofAnthropic-HH(Gangulietal.,2022),OpenAssistant maximizingthelog-likelihoodofpreferences.\n(Ko¨pfetal.,2023),andSHP(Ethayarajhetal.,2022).\n4.1.Derivation\nAllmodelsarealignedunderidenticalsettingsonthesame\ndata,saveforhyperparametersuniquetothem. Similarto ThecanonicalKahneman-Tverskyvaluefunction(4)suffers\nRafailovetal.(2023), thetargetsequencesforSFTarea fromnumericalinstabilityduringoptimizationduetothe\nsubset of {y w}. We use GPT-4-0613 to judge whether exponent a, so we replace it with the logistic function σ,\nthealignedmodel’sresponseisbetterthantheSFTtarget whichisalsoconcaveingainsandconvexinlosses.\nforagiventestinputwithrespecttohelpfulness,harmless-\nTocontrolthedegreeofriskaversion,weintroduceahy-\nness,andconciseness,anowstandardpractice(Zhengetal.,\n2023;Lietal.,2023).5 NotethatwhiletheSFTtargetis perparameter β ∈ R+ as part of the value function. The\ngreaterβ is,themorequicklythevaluesaturates,meaning\nconsideredadesirableoutputforx,itisbynomeansthe\nthehumanissimultaneouslymorerisk-averseingainsand\nbest output, meaning that an aligned model can certainly\nmorerisk-seekinginlosses. Inpractice,thishasasimilar\nachieveawinrateabove50%,althoughthisisdifficultgiven\neffectasβintheDPOloss,whichcontrolshowfarπ drifts\nthatmanyofthetargetsequencesarehuman-written. θ\nfromπ ,thoughweintroduceithereexplicitlytocontrol\nref\nInFigure2,weseetheresultsofthisanalysis: riskaversion;inDPO,itcarriesoverfromtheKLconstraint\nintheRLHFobjective(2)andispartofthereward.\n• HALOseithermatchoroutperformnon-HALOs\nateveryscale,thoughthegapisonlysignificant(p< We replace the loss aversion coefficient λ in the origi-\n0.05)at13B+modelsizesaftercorrectingformultiple nalKahneman-Tverskyvaluefunction(4)with{λ D,λ U},\ncomparisons(Holm,1979). Infact,onlytheHALO- where λ D and λ U are hyperparameters for desirable and\nalignedLlama-{13B,30B}modelsmatchorexceeda undesirable outputs respectively; more complex schemes\nwinrateof50%(i.e.,areabletomatchorexceedthe couldalsobeusedforimportancesampling.\ngenerationqualityoftheSFTtargetsinthetestdata).\nRatherthanhavingjustonedispreferredgenerationserveas\n• Uptoascaleof7Bparameters,alignmentprovides thereferencepointz 0,asinDPO,weassumethathumans\nvirtually no gains over SFT alone. However, it is\njudgethequalityofy|xinrelationtoallpossibleoutputs.\nworthnotingthatifthebasemodelsweremoreperfor-\nThisimpliesthatQ(Y′|x)isthepolicyandthattherefer-\nmant,oriftheSFTdatadistributionwerelesssimilarto ence point is the KL divergence KL(π θ(y′|x)∥π ref(y′|x)).\nthepreferencedata,thenthegainsfromthealignment However,aswediscussfurtherbelow,weenduptakinga\nstagewouldostensiblybegreater. biasedestimateofthisterminpractice.\nWhereλ denotesλ (λ )whenyisdesirable(undesirable)\n• Despite only using dummy +1/-1 rewards, our of- y D U\nrespectively,thedefaultKTOlossis:6\nflinePPOvariantperformsaswellasDPOforall\nmodels except Llama-30B. This challenges conven- L (π ,π )=E [λ −v(x,y)] (8)\nKTO θ ref x,y∼D y\ntionalwisdom,whichplacesheavyemphasisonreward\nlearning(Casperetal.,2023),andsuggeststhateven where\nthesimplestrewardscanproveusefulwhenusedina\nπ (y|x)\nlossfunctionthathastherightinductivebias. Despite r θ(x,y)=log πθ\n(y|x)\nitssuccess,ourofflinePPObaselinestillsuffersfrom ref\nz =KL(π (y′|x)∥π (y′|x))\nhyperparametersensitivityandtraininginstability,al- 0 θ ref\n(cid:40)\nbeitnottothesameextentastraditionalRLHF. λ σ(β(r (x,y)−z ))ify ∼y |x\nv(x,y)= D θ 0 desirable\nλ σ(β(z −r (x,y)))ify ∼y |x\nU 0 θ undesirable\n4.Kahneman-TverskyOptimization\n6Anearlierdraftseparatedoutλ ,λ intoafunctionw.For\nD U\nThesurprisingsuccessofofflinePPOwithdummy+1/-1 aneasiercomparisonwith(4),they—alongwithβ—havebeen\nrewardssuggeststhat—withtherightinductivebiases—a movedintothevaluefunctionitself. Notethatλ existssolely\ny\ntomakethelossnon-negativeandcanberemoved. Theuseof\n5WevalidatethatGPT-4judgmentsconcurwithhumanjudg- multiple x′ to estimate z in practice was included in the loss\n0\nmentsinAppendixD. definitionoriginally,buthasnowbeenmovedoutsideforclarity.\n5\nModelAlignmentasProspectTheoreticOptimization\nTable1. RecommendedhyperparametersettingsfordifferentlossesandmodelswhenalignedonUltraFeedback, evaluatedonthe\nbenchmarksdiscussedin§4.3.ThehyperparametersweepsweredonewithAdamW,aneffectivebatchsizeof32,andλ =λ =1.\nD U\nDependingonyourtaskandratioofdesirable:undesirableexamples,theoptimalchoiceofλ ,λ mightbesignificantlydifferent.\nD U\nModel Method LR β AlpacaEval(LC)↑ BBH↑ GSM8K(8-shot)↑\nLlama-38B SFT+KTO 5e-6 0.05 10.59 65.15 60.20\nLlama-38B KTO 5e-6 0.10 11.25 65.26 57.92\nQwen2.53BInstruct SFT+KTO 5e-6 0.10 13.01 32.39 61.11\nQwen2.53BInstruct KTO 5e-6 0.50 16.63 20.41 60.35\nFormorestabletraining,wedonotbackpropagatethrough regardlessofwhetheritisprecededbyx ,zˆ mightactually\ni 0\nz ;itexistspurelytocontrolthelosssaturation. beanunder-estimate. Insuchcases,onecanavoidtheextra\n0\ncomputationandsetzˆ = 0. However,whenKTOisnot\nIntuitively,KTOworksasfollows: ifthemodelincreases 0\nprecededbySFT,orwhentheSFTdataisnotasubsetof\ntherewardofadesirableexampleinabluntmanner,thenthe\ntheKTOdata,estimatingzˆ isnecessary.\nKLpenaltyalsorisesandnoprogressismade. Thisforces 0\nthemodeltolearnexactlywhatmakesanoutputdesirable,\nsothattherewardcanbeincreasedwhilekeepingtheKL Data Ifthealignmentdataisnaturallybinary,everyposi-\ntermflat(orevendecreasingit). Theargumentworksinthe tiveexamplecanbeassumedtobedrawnfromy |x\ndesirable\notherdirectionaswell,thoughthenon-negativityoftheKL andeverynegativeexamplefromy |x. However,the\nundesirable\ntermallowsfastersaturationinthelossregime. canonicalfeedbackdatasetsinacademicresearch(HH,SHP,\nOASST)areinpreferenceformat,sincethemethodsthat\nKL Estimate In practice, estimating z as it is defined haveworkedbestupuntilnowarepreference-based. Inour\n0\naboveisimpracticalbecausesamplingfromπ θ isslow. In- experiments,weconvertpreferencedatay w ≻y l byassum-\nstead,wetakeabiasedbutconvenientestimatebyshifting ingthaty w isdrawnfromthedesirabledistributionandy l\noutputsinthesamemicrobatchtoinducemismatchedpairs fromtheundesirableone. Thisisanaiveassumption,made\n{(x ,y ),(x ,y ),...,(x ,y )},thenestimatingashared forthesakeofsimplicity,andamorecomplexdeconstruc-\n1 2 2 3 m 0\nreferencepointz forallexamplesinthesamemicrobatch tionofpreferencesintobinaryfeedbackwouldlikelyyield\n0\nasfollows. Wherej =(i+1)modm, betterresults,whichweleaveforfuturework. Toshowthat\nKTO can be used with non-preference data, we also sub-\n \nsampleexactlyoneyperxforsomeexperiments(denoted\nzˆ 0 =max0,\nm1 (cid:88)\nlog\nππ θ( (y yj| |x xi)\n) one-y-per-x),removinganytraceofpairedpreferencesat\n1≤i<m ref j i thecostofreducingthedatavolume.\nIfhumanfeedbackisintheformofscoresorratings,the\nBecauseofclamping,ourestimatorhasapositivebiasbut\nsimplest means of incorporating it into KTO is to con-\nlower variance than the standard unbiased estimator. Al-\nstructaweightingfunctionsuchthathigh-magnitudedatais\nthough it costs an additional forward pass, we use a mis-\nweighedmoreandthatexampleswithscoresabove(below)\nmatchedoutputy insteadofthecorrespondingy because\nj i somethresholdaredesirable(undesirable).Itisalsopossible\nthelatterhaveoftenbeendeliberatelychosentobecanoni-\ntoconstructscore-basedHALOsfromfirstprinciples,but\ncallygoodorbadoutputs,andthushaveunrepresentative\nweleavethedesignofsuchlossestofuturework.\nhigh-magnituderewards.Itisworthnotingthatalthoughour\nestimatorisbiased,sowouldthehuman-perceivedreference\n4.2.Hyperparameters\npoint,sincehumansdonotperceivethefulldistributionin-\nducedbyπ andwouldemployan“availabilityheuristic” InTable1,weproviderecommendedhyperparameterset-\nθ\nthatwouldoverweightoutputsforwhichtheyhaverecently tings for Llama-3 8B (Dubey et al., 2024) and Qwen2.5\ngivenfeedback,regardlessofwhetherthoseoutputsarea 3BInstruct(Yangetal.,2024)basedonbenchmarkssuch\ngoodcontinuationofx(Tversky&Kahneman,1973). asMMLU(0-shot)(Hendrycksetal.,2021), GSM8K(8-\nshot, chain-of-thought) (Cobbe et al., 2021), HumanEval\nIf KTO is preceded by SFT done on the same data that\n(0-shot) (Chen et al., 2021), and BigBench-Hard (3-shot\nis used as desirable feedback and the SFT model is used\nchain-of-thought)(Srivastavaetal.,2022).\nas π , then the KL estimate will quickly approach zero.\nref\nHavingalreadylearnedwhatisdesirableduringSFT,the\npolicywilltendtoscatterthemassplacedonundesirable LearningRate Wefindthatthattheperformanceofan\nexamples, leading to minimal divergence. Also, because alignedmodelismoresensitivetothelearningratethanany\nthepolicymaylearntoplacelessmassonundesirabley otherhyperparameter. TheoptimallearningrateforKTOis\ni\n6\nModelAlignmentasProspectTheoreticOptimization\nFigure3.KTOisasgoodorbetterthanDPOatallscales,asmeasuredbytheGPT-4-0613-judgedwinrateofthealignedmodel’s\ngenerationsagainsttheoutputsthatwouldhavebeenusedforSFT.Infact,fortheLlamamodels,KTOalonematchestheperformanceof\nSFT+DPOandissignificantlybetterthanDPOalone.Errorbarsdenotea90%binomialconfidenceinterval.\nusually2xto10xtheoptimallearningrateforDPO;since Thisintervalwasdeterminedempirically,andimpliesthat—\nthereference-adjustedrewardtendstobemuchsmallerin afteradjustingforclassimbalances—gainsensitivityyields\nmagnitude for KTO, one needs to use a more aggressive betterperformancethanlosssensitivity,ostensiblybecause\nlearningratetocompensate. Forexample,thedefaultlearn- producing good outputs is more important than avoiding\ningrateforDPOis5e-7(Rafailovetal.,2023),butwefind badoutputsforsuccessonmostbenchmarks. Thisisnota\nthatadefaultof5e-6worksbetterforKTO.Inourexperi- hardrule,however.Intaskswhereminimizingthedownside\nments,weusethedefaultDPOlearningratewithRMSProp ismoreimportant,liketoxicityprevention,settingλ ,λ\nD U\nforallmethodstoensureanapples-to-applescomparison suchthatλ n < λ n mayworkbetter. Unlessother-\nD D U U\nwithRafailovetal.(2023),butwhenusingKTOinpractice, wisestated,weuseλ =λ =1inourexperiments.\nD U\nwerecommendstartingat5e-6withAdamWandadjusting\nThecurrentconfigurationpermitsdifferentsensitivitiesto\nthelearningrateasneeded.\ndesirableandundesirableexamplesbasedonthepremise\nthat upon convergence, all undesirable outputs will yield\nBatchSize KTOneedsamicrobatchsize≥2toestimate\nnegativerewardsandalldesirableoutputswillyieldpositive\nthereferencepointinasinglestep. Theexperimentsinthis\nones,inwhichcaseλ ,λ woulddirectlycorrespondtoλ\nU D\npaperalluseaneffectivebatchsizeof32,andingeneralwe\nintheoriginalKahneman-Tverskyvaluefunction. However,\nrecommendusingabatchsizebetween8and128.\nduringtrainingitself,adesirableoutputmayhaveanegative\nreward(andvice-versa),yetλ wouldbethesameregard-\nD\nRiskAversion Thedegreeofriskaversion/seekingiscon- lessofwhetherr (x,y)−z werepositiveornegative. We\nθ 0\ntrolledbyβ;thegreaterβ is,thegreatertheriskaversion\nmaywanttochangethissothatnotonlyisasymmetrypos-\ningainsandriskseekinginlosses. Inpractice,lowerval-\nsibleuponconvergence,butevenduringtrainingwithinthe\nues of β in the range [0.01, 0.10] work better for larger sameclassofoutput: inthiscase,usingahigherλ when\nD\nmodelsthathavealreadyundergoneSFT;highervaluesof r (x,y)−z isnegativethanwhenitispositive. Weleave\nθ 0\nβ intherange[0.10,1.00]workbetterforsmallermodels\nthedesignofdynamichyperparameterselectionschemesas\nundergoingKTOdirectly,withoutSFTprior.\ndirectionsforfuturework.\nLoss Aversion The default weighting function controls 4.3.Experiments\nthedegreeoflossaversionwithλ ,λ ,whicharebothset\nD U\nto1bydefault. Ingeneral,wheren andn refertothe KTO≥DPO AsseeninFigure3, whenrerunningthe\nD U\nnumberofdesirableandundesirableexamplesrespectively, winrateevaluationin§3.3,SFT+KTOiscompetitivewith\nwefindthatitisgenerallybesttosetλ ,λ suchthat SFT+DPO at scales from 1B to 30B, despite the model\nD U\nlearning from a weaker signal. KTO alone is better than\n(cid:20) (cid:21)\nλ n 3 DPOalonefortheLlama-{7B,13B,30B}models,andthis\nD D ∈ 1, (9)\nλ n 2 gapissignificant(p<0.01)at7Band30Bevenaftercor-\nU U\nrectingformultiplecomparisons(Holm,1979). Amongthe\nForexample,iftherewerea1:10ratioofdesirabletoun- Pythiamodels,thereisnosignificantdifferencebetweenthe\ndesirableexamples,wewouldsetλ = 1,λ ∈ [10,15].\nU D\n7\nModelAlignmentasProspectTheoreticOptimization\nTable2.(top)ResultsfromaligningZephyr-β-SFT(Tunstalletal.,\n2023) on UltraFeedback for exactly 1 epoch. Even when only\noneofthetwooutputsineachpreferenceisseenbyKTO,itstill\noutperformsDPO,despitethisreducingthevolumeofdatabyhalf\n(one-y-per-x).(middle)ChangingthestructureoftheKTOloss,\neveninsubtleways,makesthealignedmodelworse,supporting\nour design choices. (bottom) Fixing λ = 1, we try different\nU\nlevelsoflossandriskaversionbychangingλ andβrespectively\nD\n(seeAppendixCformoreresults).\nDataset(→) MMLU GSM8k HumanEval BBH\nMetric(→) EM EM pass@1 EM\nFigure4.WithoutdoingSFTfirst,DPO-alignedmodelstendto\nSFT 57.2 39.0 30.1 46.3\nrambleandhallucinateentireconversations.KTOdoesnotsuffer\nDPO 58.2 40.0 30.1 44.1\nORPO(λ=0.1) 57.1 36.5 29.5 47.5 fromthisissue.\nKTO(β=0.1,λD=1) 58.6 53.5 30.9 52.6\nKTO(one-y-per-x) 58.0 50.0 30.7 49.9\nthecriteriain(9);whenwedrop90%ofthedesirabledata,\nKTO(noz0) 58.5 49.5 30.7 49.0\nKTO(concave,v=logσ) 58.3 42.5 30.6 43.2 wesetλ = 1,λ = 13.33forexample. ForLlama-7B,\nu D\nKTO(risk-neutral,v(·)=·) 57.3 42.0 28.8 6.1 wefindthatupto90%ofthedesirabledatacaninfactbe\nKTO(noπ ref,λD=1.75) 57.5 47.5 29.5 51.6\ndiscardedwhilestilloutperformingDPO(Figure5).\nKTO(β=0.01) 57.3 54.0 33.2 49.8\nKTO(β=0.50) 58.3 45.0 32.7 51.8 WefurtherverifythisclaimbyaligningMistral-7BonOpe-\nKTO(λD=0.50) 57.4 47.0 32.2 30.5\nnAssistantusingDPO(onnpairs),standardKTO(onall2n\nKTO(λD=0.75) 58.3 52.0 30.3 50.6\nKTO(λD=1.33) 58.5 52.0 32.8 45.4 outputs),andKTOwhereonlyoneyperxisused.Sincethe\nKTO(λD=2.00) 58.1 43.0 32.1 45.3 outputofoneyinOpenAssistantisnotconditionedonthe\notheryforthesamex,thelattercapturesthesettingwhere\nthedataisfromaninherentlyunpaireddistribution. Despite\ntwo,suggestingthataminimummodelcapacityisneeded theone-y-per-xsetupdecreasingtheamountoftrainingdata\nforthesedifferencestoemerge. KTOalsofaresbetterthan by72%,theKTO-alignedmodelstilloutperformsbothits\nDPOandotherbaselinesongenerativebenchmarks(Table DPOcounterpartandtheofficialinstruction-tunedMistral-\n2). Thisismostpronouncedforcertaintasks: onGSM8K, 7B(Jiangetal.,2023),asseeninTable3.\namathematicalreasoningdataset,justswappingDPOfor\nKTOwhenaligningZephyr-β-SFT(Tunstalletal.,2023) ChangingthedesignofKTOmakesitsignificantlyworse.\nonUltraFeedback(Cuietal.,2023)improvesperformance Forone,removingthereferencepointz —whichisneces-\n0\nby13.5points. saryforKTOtoqualifyasaHALO—causesa3.6and4.0\npointdroponBBHandGSM8Krespectively(Table2,mid-\nAt sufficient scale, KTO does not need SFT. A KTO- dle). EvenchangesthatallowKTOtoremainaHALOare\nalignedLlama-{13B,30B}modeliscompetitivewithits typicallysuboptimal. Forexample,removingthesymmetry\nSFT+KTO counterpart despite not undergoing SFT first, ofthevaluefunction—goingfrom1−σ(·)to−logσ(·)(i.e.,\nand is the only alignment method of the ones we tested makingthevaluefunctionconcaveeverywhere,asinDPO)—\ntoshowthisbehavior. ThisisperhapsduetoKTOalone causesa9.4and11.0pointdroponBBHandGSM8Kre-\nkeepingtheaverageresponselengthroughlythesame,while spectively. Makingthevaluefunctionrisk-neutralbysetting\nrunningDPOwithoutSFTpriorcausestheresponselength ittotheidentityfunctionleadstoatotalcollapseinBBH\ntoincreasedramatically(Figure4). performance. Changingthecurvatureandslopeviatherisk\nandlossaversionhyperparameterscan,dependingonthe\nKTOdataneednotcomefrompreferences. MightKTO task,improveordegradeperformance(Table2,bottom).\nbesecretlybenefitingfromits2nexamplesintheprevious\nexperimentcomingfromnpreferencepairsinsteadofanat- KTOworkswithoutareferencemodelorSFT,butnotas\nurallyunpaireddatadistribution? Totestthis,werandomly wellasstandardKTO. Ifwecanavoidstoringtherefer-\ndiscardincreasinglylargefractionsofthedesirabledatabe- encemodelinmemory,thenwecandomuchmorememory-\nforeKTO-aligningaLlama-7Bmodel. Forexample,ifwe efficientalignment. Thenaivewaytodothisistoassume\ndiscard90%ofthedesirabledatawhileleavingtheundesir- thatπ returnsauniformdistributionoveroutputsforall\nref\nabledatauntouched,thentheratioofdesirable:undesirable x,whichsimplifiesr −z tologπ (y|x)−H(π (y′|x)),\nθ 0 θ θ\nexamplesgoesfrom1:1to1:10andthevastmajorityofun- where H denotes the entropy. As seen in Table 2 (mid-\ndesirableexamplesnolongerhaveapreferredcounterpart. dle),ifwesetλ =1.75,thismemory-efficientvariantof\nD\nWehandlesuchimbalancesbychangingλ ,λ tosatisfy KTOisbetterthanDPOonsometasksandworseonothers,\nD U\n8\nModelAlignmentasProspectTheoreticOptimization\nTable3.InaligningMistral-7BontheOpenAssistantdataset,we\nfindthatusingKTOwithonlyoneoutputperinputstilloutper-\nformsDPO,despitethisrestrictionreducingtheamountoftraining\ndataby72%.A90%binomialconfidenceintervalisgiven.\nMethod Winratevs.SFTTarget\nMistral-7B(unaligned) 0.525±0.037\nMistral-7B+DPO 0.600±0.037\nMistral-7B+KTO(allyperx) 0.652±0.036\nMistral-7B+KTO(oneyperx) 0.631±0.036\nMistral-7B-Instruct 0.621±0.031\nFigure5.AKTO-alignedLlama-7Bmodelcanmatchorexceed\nthe performance of its DPO-aligned counterpart while aligned\nnoringdatathatishard-to-learnbutnecessarytorecoverr∗,\nonasmallerandhighlyimbalancedversionofthesamedataset, potentiallyresultinginunderfittingtocomplexdistributions.\neven with as few as 0.1 positive/desirable examples for every Suchunderfittingmaybemitigatedbyaligningthemodel\nnegative/undesirableone. withlowerβ andformoreepochs.\nTheorem4.2. Assumingthevaluefunctionislogistic,fora\nrewardfunctionr∗thatmaximizes(2),thereexistsareward\nthough it still trails standard KTO. We find that it is also a\nfunctioninitsequivalenceclass(i.e.,r∗(x,y)=r∗(x,y)+\nmoresensitivetothelossaversionhyperparameters;setting b a\nh(x)forsomeh(x))thatinducesthesameoptimalpolicy\nλ ∈{1.5,2.0}reducesperformanceonGSM8KandBBH\nD π∗andthesameBradley-Terrypreferencedistributionbut\nbyseveralpoints. Still,itstrictlyoutperformsORPO(Hong\nadifferenthumanvaluedistribution.\net al., 2024), a recently-proposed reference-free method,\nwhilealsousinglessmemorythanallexistingapproaches,\nA key insight from Rafailov et al. (2023) is that reward\nsinceπ needneverbeloadedintomemoryandabatchof\nref functionsinthesameequivalenceclass(i.e.,differingonly\nmKTOoutputsissmallerthanabatchofmpreferences\nin an input-specific component) induce the same optimal\n(i.e.,2moutputs)usedforORPO/DPO.\npolicy under (2) and the same Bradley-Terry preference\ndistribution. However, we show under mild assumptions\n4.4.TheoreticalAnalysis\nthatthevaluedistribution—i.e.,humanutility—isaffected\nbysuchinput-specificchanges,somaximizingpreference\nKTOwasdesignedwiththemotivationthatevenifbinary\nlikelihooddoesnotmeanoneismaximizinghumanutility.\nfeedback were weaker, one could compensate with sheer\nThishelpsexplainwhythemarginbetweenKTOandDPO\nvolume,assuchdataismuchmoreabundant,cheaper,and\nisevenbiggerinhumanevaluationsthanitisinautomated\nfastertocollectthanpreferences.SowhydoesKTOperform\nLLM-as-a-judgeevaluations(AppendixD).\naswellorbetterthanDPOonthesamepreferencedata(that\nhasbeenbrokenup)? Greaterdataefficiencyhelps,butitis Theorem4.3. Forinputxwithoutputs{y ,y },letdataset\na b\nnottheonlyanswer,giventhatevenafteradjustingforthis Dcomprisecontradictorypreferencesy ≻y andy ≻y\na b b a\nfactorintheone-y-per-xsetup,KTOstilloutperforms. in proportion p ∈ (0.5,1) and (1−p) ∈ (0,0.5) respec-\ntively. If p1/βπ (y |x) < (1−p)1/βπ (y |x), then the\nInthissection,wewilldiscusstwotheoreticalexplanations ref a ref b\noptimalDPOpolicyismorelikelytoproducetheminority-\nforthisphenomenon: (1)preferencelikelihoodcanbemax-\npreferredy ;theoptimalKTOpolicywillstrictlyproduce\nimizedwithoutnecessarilymaximizingunderlyinghuman b\nthemajority-preferredy foraloss-neutralvaluefunction\nutility; (2) KTO implicitly ignores noisy and intransitive a\n(λ =λ ).\ndata,whichiscommoninreal-worldfeedback. D U\nProposition4.1. Astherewardimpliedbythecurrentpolicy Informally,saytherearetwocontradictorypreferencesover\ntendsto±∞,theKTOupdateofπ θ tendstozero. theoutputforx,withthemajorityppreferringy a andthe\nminority 1−p preferring y . In the worst-case, when p\nb\nThis means that if (x,y) is implied by the current policy is sufficiently low and the reference model is sufficiently\nπ to be too difficult or too easy to learn from, then it is unaligned,theoptimalDPOpolicyismorelikelytoproduce\nθ\neffectivelyignored. Insomecases,thismaybeablessingin theminority-preferredoutputy eventhoughtheimplied\nb\ndisguise: sincereal-worldfeedbackisverynoisy(Hoeffler rewardr (x,y )>r (x,y ). Incontrast,theoptimalKTO\nθ b θ a\n&Ariely,1999),thereasonadesirableexamplehasahighly policywilldeterministicallyproducethemajority-preferred\nnegativeimpliedrewardmaybebecauseitismislabelled. y ifthevaluefunctionisloss-neutral(λ = λ ),which\na D U\nByavoidingthishard-to-learndata,KTOavoidsfittingto is the default setting. This suggests that KTO has better\nnoise. However,thisalsomeansthatKTOcouldendupig- worst-caseoutcomeswhenhandlingfeedbackintransitivity.\n9\nModelAlignmentasProspectTheoreticOptimization\n4.5.KTOvs.DPO–whentousewhich? suggestingthattheyworkinpartbecausetheyreflecthuman\nbiasesindecision-making. Weabstractedtheseinsightsinto\nWhenhumanfeedbackisinabinaryformat,andespecially\nafamilyofalignmentobjectivescalledhuman-awarelosses\nwhenthereisanimbalancebetweenthenumberofdesirable\n(HALOs). WethenproposedaHALOcalledKahneman-\nandundesirableexamples,KTOisthenaturalchoice. When\nTverskyOptimization(KTO)fordirectlymaximizingthe\nyourdataisintheformofpreferences, thechoiceisless\nutilityofgenerationsinsteadofmaximizingthelikelihoodof\nclear. PuttingasidethegreaterdataefficiencyofKTO,our\npreferences,asexistingmethodsdo. Despiteonlylearning\ntheoreticalanalysissuggeststhatifyourpreferencedatahas\nfromabinarysignalofwhetheranoutputis(un)desirable,\nsufficientlylittlenoiseandsufficientlylittleintransitivity,\nKTOwasasgoodorbetterthanpreference-basedmethods\nthenDPOwillworkbetter,sincethereissomeriskofKTO\ninourexperiments. Morebroadly,ourworksuggeststhat\nunderfitting; this risk can be mitigated by using a lower\nakin to how there is no one reward model that is univer-\nvalue of β and aligning with KTO for more epochs. But\nsallysuperior,thereisnoonelossfunctioneither—thebest\nif there is enough noise and intransitivity, then the better\nHALO depends on the inductive biases that are most ap-\nworst-caseguaranteesofKTOwillwinout. Mostpublicly\npropriateforagivensetting,andmuchworkremainstobe\navailable preference datasets (e.g., SHP, OpenAssistant)\ndoneinidentifyingthebestHALOforeachcontext.\ncontainnoisyfeedbackfrommanydifferenthumanswhose\npreferenceslikelycontradicttosomeextent,whichexplains\nAcknowledgements\nwhyKTOwasabletomatchorexceedDPOperformance\ninourexperiments. Evensyntheticfeedbackcanbenoisy\nWethankPercyLiang,DilipArumugam,AryaMcCarthy,\nandintransitive,whichhelpsexplainwhyKTOoutperforms\nandNathanLambertforfeedback. WethankStasBekman\nDPOwhenaligningwithUltraFeedback.\nandGautamMittalforclusterassistanceandAlexManthey\nforhelpingwithhumanevaluation.\n5.FutureWork\nImpactStatement\nTheexistenceofHALOsraisesmanyquestions. Forone,\nKTO is based on the Kahneman-Tversky value function\nThemethodsdiscussedinthispaperhavethepotentialto\nformonetarygambles,whichisalmostcertainlydifferent\nmakeLLMsmorehelpfulandsafer,whichisoftenneeded\nfrom how humans perceive the relative goodness of text.\nfor models deployed in production. It is possible that in\nWhatvaluefunctionsandreferencepointdistributionsbest\nmakingmodelsmorehelpful,weincreasetheutilityofone\ndescribehowhumansperceivelanguage,andhowdothey\npersonattheexpenseofbroadersociety. Inaligningmodels\nvaryacrossdomainsandindividuals? Howcanweidentify\nwith human feedback, one may also—without even fully\nthebestHALOforeachindividualandsettinginsteadof\nrecognizingit—bealigningtoanunrepresentativesubsetof\nusingonedefaultloss?\nthepopulation,whichmayhindertheabilityofindividuals\nOn a more technical level, important directions include outsidethatsubsettobenefitequallyfromusingthemodel.\ndevelopingHALOsthat: (1)incorporategranularfeedback,\nThedatausedforLLMalignment, includingthedatasets\nsuch as a score, especially when optimizing for multiple\nusedinthispaper(e.g.,SHP,HH,OASST)containprefer-\ndesiderata;(2)workforothermodalities(e.g. images)and\nencesofgroupsthatarenotrepresentativeofthebroader\nmodelclasses(e.g., diffusionmodels), especiallymodels\npopulation.Biasesinthisdatahavethepotentialtobepropa-\nthatdonotproduceanexplicitdistributionovertheoutput\ngateddownstreamwhenusedtoalignmodelswithmethods\nspace;(3)canresolvecontradictionsinfeedbackaccording\nlike KTO, especially when no efforts are made to adjust\nto different definitions of fairness; (4) are designed to be\nforthedifferentpopulation. KTOinparticularimplicitly\nusedwith onlinedata, where thedirectionof feedbackis\nresolvescontradictionsinfeedbackbytakingthemajority-\nimpliedbyr orsomeexternalrewarddata.\nθ preferredoutcomeforaloss-neutralvaluefunction,which\nEcologicallyvalidevaluation(DeVriesetal.,2020),where doesnotcomportwithmanytheoriesoffairness(e.g.,Rawl-\nthealignedmodelsaredeployedinreal-worldsettings,are sianism). Sinceuserpreferencesare,inturn,affectedbythe\nalsoneededtojudgethemeritsofdifferentHALOs. modelstheyinteractwith,thisalsorisksthehomogeniza-\ntionofpreferencesandutilityfunctionswhenKTO-aligned\nmodelsaredeployedatscale. ThedesignofHALOsthat\n6.Conclusion\nresolvecontradictionsinmorediversewaysisanimportant\nAlthough model alignment has historically been reward- directionforfuturework.\ncentric, we found that the inductive biases of alignment\nOntheotherhand,becauseKTOworkswithbinaryfeed-\nobjectivesarecriticaltotheirsuccess. Moreover,thesein-\nback,whichismoreabundant,cheaper,andfastertocollect\nductivebiaseshaveanalogsintheprospecttheoryliterature,\nintherealworld,itsignificantlylowersthebarriertodata\n10\nModelAlignmentasProspectTheoreticOptimization\ncollection. This makes it easier to collect feedback from Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg,\ntraditionallyunder-representedgroupsandservedifferent S.,andAmodei,D. Deepreinforcementlearningfrom\nmodels to different users, instead of just one monolithic humanpreferences. Advancesinneuralinformationpro-\nmodelbeingservedtoeveryone. cessingsystems,30,2017.\nCobbe,K.,Kosaraju,V.,Bavarian,M.,Chen,M.,Jun,H.,\nReferences\nKaiser,L.,Plappert,M.,Tworek,J.,Hilton,J.,Nakano,\nAzar,M.G.,Guo,Z.D.,Piot,B.,Munos,R.,Rowland,M., R.,Hesse,C.,andSchulman,J.Trainingverifierstosolve\nValko, M., and Calandriello, D. A general theoretical mathwordproblems. arXivpreprintarXiv:2110.14168,\nparadigmtounderstandlearningfromhumanpreferences. 2021.\nInInternationalConferenceonArtificialIntelligenceand\nCui,G.,Yuan,L.,Ding,N.,Yao,G.,Zhu,W.,Ni,Y.,Xie,G.,\nStatistics,pp.4447–4455.PMLR,2024.\nLiu,Z.,andSun,M. Ultrafeedback: Boostinglanguage\nBaheti,A.,Lu,X.,Brahman,F.,LeBras,R.,Sap,M.,and modelswithhigh-qualityfeedback,2023.\nRiedl,M. Improvinglanguagemodelswithadvantage-\nDao,T.,Fu,D.,Ermon,S.,Rudra,A.,andRe´,C. Flashat-\nbased offline policy gradients. In The Twelfth Interna-\ntention: Fastandmemory-efficientexactattentionwith\ntionalConferenceonLearningRepresentations,2023.\nio-awareness. AdvancesinNeuralInformationProcess-\nBai,Y.,Jones,A.,Ndousse,K.,Askell,A.,Chen,A.,Das- ingSystems,35:16344–16359,2022.\nSarma,N.,Drain,D.,Fort,S.,Ganguli,D.,Henighan,T.,\netal. Trainingahelpfulandharmlessassistantwithrein- De Vries, H., Bahdanau, D., and Manning, C. Towards\nforcementlearningfromhumanfeedback. arXivpreprint ecologicallyvalidresearchonlanguageuserinterfaces.\narXiv:2204.05862,2022. arXivpreprintarXiv:2007.14435,2020.\nBiderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nH.,O’Brien,K.,Hallahan,E.,Khan,M.A.,Purohit,S., A.,Letman,A.,Mathur,A.,Schelten,A.,Yang,A.,Fan,\nPrashanth,U.S.,Raff,E.,etal. Pythia: Asuiteforana- A., et al. The llama 3 herd of models. arXiv preprint\nlyzinglargelanguagemodelsacrosstrainingandscaling. arXiv:2407.21783,2024.\nIn International Conference on Machine Learning, pp.\nEthayarajh, K., Choi, Y., and Swayamdipta, S. Under-\n2397–2430.PMLR,2023.\nstandingdatasetdifficultywithV-usableinformation. In\nBradley, R.A.andTerry, M.E. Rankanalysisofincom- Chaudhuri,K.,Jegelka,S.,Song,L.,Szepesvari,C.,Niu,\npleteblockdesigns: I.themethodofpairedcomparisons. G., and Sabato, S. (eds.), Proceedings of the 39th In-\nBiometrika,39(3/4):324–345,1952. ternational Conference on Machine Learning, volume\n162ofProceedingsofMachineLearningResearch,pp.\nBusa-Fekete, R., Szo¨re´nyi, B., Weng, P., Cheng, W., and\n5988–6008.PMLR,17–23Jul2022.\nHu¨llermeier,E. Preference-basedreinforcementlearning:\nevolutionarydirectpolicysearchusingapreference-based Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y.,\nracingalgorithm. Machinelearning,97:327–351,2014. Kadavath,S.,Mann,B.,Perez,E.,Schiefer,N.,Ndousse,\nK.,etal. Redteaminglanguagemodelstoreduceharms:\nCasper,S.,Davies,X.,Shi,C.,Gilbert,T.K.,Scheurer,J.,\nMethods,scalingbehaviors,andlessonslearned. arXiv\nRando,J.,Freedman,R.,Korbak,T.,Lindner,D.,Freire,\npreprintarXiv:2209.07858,2022.\nP.,etal.Openproblemsandfundamentallimitationsofre-\ninforcementlearningfromhumanfeedback.Transactions\nGurevich, G., Kliger, D., andLevy, O. Decision-making\nonMachineLearningResearch,2023.\nunderuncertainty–afieldstudyofcumulativeprospect\ntheory. JournalofBanking&Finance,33(7):1221–1229,\nChan,L.,Critch,A.,andDragan,A. Humanirrationality:\nbothbadandgoodforrewardinference. arXivpreprint 2009.\narXiv:2111.06956,2021.\nHe, X., Liao, L., Zhang, H., Nie, L., Hu, X., and Chua,\nChen,M.,Tworek,J.,Jun,H.,Yuan,Q.,Pinto,H.P.d.O., T.-S. Neural collaborative filtering. In Proceedings of\nKaplan,J.,Edwards,H.,Burda,Y.,Joseph,N.,Brockman, the26thinternationalconferenceonworldwideweb,pp.\nG., etal. Evaluatinglargelanguagemodelstrainedon 173–182,2017.\ncode. arXivpreprintarXiv:2107.03374,2021.\nHendrycks,D.,Burns,C.,Basart,S.,Zou,A.,Mazeika,M.,\nChen,Z.,Deng,Y.,Yuan,H.,Ji,K.,andGu,Q. Self-play Song,D.,andSteinhardt,J. Measuringmassivemultitask\nfine-tuningconvertsweaklanguagemodelstostronglan- languageunderstanding.ProceedingsoftheInternational\nguagemodels. arXivpreprintarXiv:2401.01335,2024. ConferenceonLearningRepresentations(ICLR),2021.\n11\nModelAlignmentasProspectTheoreticOptimization\nHoeffler,S.andAriely,D. Constructingstablepreferences: Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,\nAlookintodimensionsofexperienceandtheirimpacton Guestrin,C.,Liang,P.,andHashimoto,T.B. Alpacae-\npreferencestability. Journalofconsumerpsychology,8 val: An automatic evaluator of instruction-following\n(2):113–139,1999. models. https://github.com/tatsu-lab/\nalpaca_eval,2023.\nHolm,S. Asimplesequentiallyrejectivemultipletestpro-\nLu, X., Welleck, S., Hessel, J., Jiang, L., Qin, L., West,\ncedure. Scandinavian journal of statistics, pp. 65–70,\nP.,Ammanabrolu,P.,andChoi,Y. Quark: Controllable\n1979.\ntextgenerationwithreinforcedunlearning. Advancesin\nneuralinformationprocessingsystems,35:27591–27609,\nHong,J.,Lee,N.,andThorne,J. Reference-freemonolithic\n2022.\npreferenceoptimizationwithoddsratio. arXivpreprint\narXiv:2403.07691,2024. Munos,R.,Valko,M.,Calandriello,D.,Azar,M.G.,Row-\nland,M.,Guo,Z.D.,Tang,Y.,Geist,M.,Mesnard,T.,\nJain,A.,Wojcik,B.,Joachims,T.,andSaxena,A. Learning\nMichi, A., et al. Nash learning from human feedback.\ntrajectorypreferencesformanipulatorsviaiterativeim- arXivpreprintarXiv:2312.00886,2023.\nprovement. Advancesinneuralinformationprocessing\nsystems,26,2013. Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,\nMishkin,P.,Zhang,C.,Agarwal,S.,Slama,K.,Ray,A.,\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., et al. Training language models to follow instructions\nChaplot,D.S.,Casas,D.d.l.,Bressand,F.,Lengyel,G., withhumanfeedback. AdvancesinNeuralInformation\nLample,G.,Saulnier,L.,etal. Mistral7b. arXivpreprint ProcessingSystems,35:27730–27744,2022.\narXiv:2310.06825,2023.\nPeng, X. B., Kumar, A., Zhang, G., and Levine, S.\nAdvantage-weighted regression: Simple and scalable\nKahneman,D.andTversky,A.Prospecttheory:Ananalysis\noff-policy reinforcement learning. arXiv preprint\nof decision under risk. Econometrica, 47(2):263–292,\narXiv:1910.00177,2019.\n1979.\nPeters,J.andSchaal,S. Reinforcementlearningbyreward-\nKo¨pf, A., Kilcher, Y., von Ru¨tte, D., Anagnostidis, S., weighted regression for operational space control. In\nTam,Z.-R.,Stevens,K.,Barhoum,A.,Duc,N.M.,Stan- Proceedingsofthe24thinternationalconferenceonMa-\nley, O., Nagyfi, R., etal. Openassistantconversations– chinelearning,pp.745–750,2007.\ndemocratizing large language model alignment. arXiv\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D.,\npreprintarXiv:2304.07327,2023.\nErmon,S.,andFinn,C. Directpreferenceoptimization:\nYourlanguagemodelissecretlyarewardmodel.InThirty-\nKorbak, T., Shi, K., Chen, A., Bhalerao, R. V., Buckley,\nseventh Conference on Neural Information Processing\nC., Phang, J., Bowman, S. R., and Perez, E. Pretrain-\nSystems,2023.\ninglanguagemodelswithhumanpreferences. InInter-\nnationalConferenceonMachineLearning,pp.17506– Rosset,C.,Cheng,C.-A.,Mitra,A.,Santacroce,M.,Awadal-\n17533.PMLR,2023. lah,A.,andXie,T. Directnashoptimization: Teaching\nlanguagemodelstoself-improvewithgeneralpreferences.\nKoren,Y.,Bell,R.,andVolinsky,C. Matrixfactorization arXivpreprintarXiv:2404.03715,2024.\ntechniquesforrecommendersystems. Computer,42(8):\n30–37,2009. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\nKreutzer, J., Uyheng, J., and Riezler, S. Reliability and arXivpreprintarXiv:1707.06347,2017.\nlearnabilityofhumanbanditfeedbackforsequence-to-\nSrivastava,A.,Rastogi,A.,Rao,A.,Shoeb,A.A.M.,Abid,\nsequencereinforcementlearning. InProceedingsofthe\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\n56th Annual Meeting of the Association for Computa-\nGarriga-Alonso, A., et al. Beyond the imitation game:\ntional Linguistics (Volume 1: Long Papers), pp. 1777–\nQuantifyingandextrapolatingthecapabilitiesoflanguage\n1788,2018.\nmodels. arXivpreprintarXiv:2206.04615,2022.\nKwon, M., Biyik, E., Talati, A., Bhasin, K., Losey, D.P., Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,\nandSadigh,D. Whenhumansaren’toptimal:Robotsthat Voss, C., Radford, A., Amodei, D., and Christiano,\ncollaboratewithrisk-awarehumans. InProceedingsof P.F. Learningtosummarizewithhumanfeedback. Ad-\nthe2020ACM/IEEEinternationalconferenceonhuman- vances in Neural Information Processing Systems, 33:\nrobotinteraction,pp.43–52,2020. 3008–3021,2020.\n12\nModelAlignmentasProspectTheoreticOptimization\nSun,L.,Zhan,W.,Hu,Y.,andTomizuka,M. Interpretable Zhao,Y.,Joshi,R.,Liu,T.,Khalman,M.,Saleh,M.,and\nmodellingofdrivingbehaviorsininteractivedrivingsce- Liu,P.J. Slic-hf: Sequencelikelihoodcalibrationwith\nnariosbasedoncumulativeprospecttheory.In2019IEEE humanfeedback. arXivpreprintarXiv:2305.10425,2023.\nIntelligentTransportationSystemsConference(ITSC),pp.\nZheng,L.,Chiang,W.-L.,Sheng,Y.,Zhuang,S.,Wu,Z.,\n4329–4335.IEEE,2019.\nZhuang,Y.,Lin,Z.,Li,Z.,Li,D.,Xing,E.,etal. Judging\nSwamy,G.,Dann,C.,Kidambi,R.,Wu,Z.S.,andAgarwal, llm-as-a-judgewithmt-benchandchatbotarena. arXiv\nA. Aminimaximalistapproachtoreinforcementlearning preprintarXiv:2306.05685,2023.\nfromhumanfeedback. arXivpreprintarXiv:2401.04056,\nZiegler,D.M.,Stiennon,N.,Wu,J.,Brown,T.B.,Radford,\n2024.\nA.,Amodei,D.,Christiano,P.,andIrving,G.Fine-tuning\nTian,K.,Mitchell,E.,Yao,H.,Manning,C.D.,andFinn, languagemodelsfromhumanpreferences. arXivpreprint\nC. Fine-tuning language models for factuality. arXiv arXiv:1909.08593,2019.\npreprintarXiv:2311.08401,2023.\nTouvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,\nM.-A.,Lacroix,T.,Rozie`re,B.,Goyal,N.,Hambro,E.,\nAzhar,F.,etal. Llama:Openandefficientfoundationlan-\nguagemodels. arXivpreprintarXiv:2302.13971,2023.\nTunstall,L.,Beeching,E.,Lambert,N.,Rajani,N.,Rasul,\nK.,Belkada,Y.,Huang,S.,vonWerra,L.,Fourrier,C.,\nHabib, N., Sarrazin, N., Sanseviero, O., Rush, A. M.,\nandWolf,T. Zephyr: Directdistillationoflmalignment,\n2023.\nTversky,A.andKahneman,D. Availability: Aheuristicfor\njudgingfrequencyandprobability. Cognitivepsychology,\n5(2):207–232,1973.\nTversky,A.andKahneman,D.Advancesinprospecttheory:\nCumulativerepresentationofuncertainty. JournalofRisk\nanduncertainty,5:297–323,1992.\nvon Werra, L., Belkada, Y., Tunstall, L., Beeching, E.,\nThrush, T., Lambert, N., and Huang, S. Trl: Trans-\nformer reinforcement learning. https://github.\ncom/huggingface/trl,2020.\nWelleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K.,\nandWeston,J. Neuraltextgenerationwithunlikelihood\ntraining. InInternationalConferenceonLearningRepre-\nsentations,2019.\nXu,H.,Sharaf,A.,Chen,Y.,Tan,W.,Shen,L.,VanDurme,\nB., Murray, K., and Kim, Y. J. Contrastive preference\noptimization: Pushingtheboundariesofllmperformance\ninmachinetranslation. arXivpreprintarXiv:2401.08417,\n2024.\nYang,A.,Yang,B.,Hui,B.,Zheng,B.,Yu,B.,Zhou,C.,\nLi,C.,Li,C.,Liu,D.,Huang,F.,etal. Qwen2technical\nreport. arXivpreprintarXiv:2407.10671,2024.\nYuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J.,\nandWeston,J. Self-rewardinglanguagemodels. arXiv\npreprintarXiv:2401.10020,2024.\n13\nModelAlignmentasProspectTheoreticOptimization\nA.RelatedWork\nLLMAlignment HumanfeedbackhasbeenusedtoimproveLLMcapabilitiesintranslation(Kreutzeretal.,2018),sum-\nmarization(Stiennonetal.,2020),sentiment-conditionedgeneration(Ziegleretal.,2019),andinstruction-following(Ouyang\netal.,2022). TheRLHFframework(Christianoetal.,2017;Baietal.,2022)traditionallyusedtoaccomplishthisisdetailed\nin§2. Still,momentumhaslargelyshiftedinfavorofclosed-formlossesthatdirectlyoperateonofflinepreferences,such\nasDPO(Rafailovetal.,2023). ThissinglestageofoptimizationdistinguishesDPOfromtheconventionalapproachin\npreference-basedRL,whichlearnsarewardandthenfitsthepolicytothoserewards (Jainetal.,2013;Busa-Feketeetal.,\n2014). Otherpreference-basedlossesincludeCPO(Xuetal.,2024)andIPO(Azaretal.,2024).\nBinaryFeedback Despitenotbeingahuman-awareloss,unlikelihoodtrainingwasamongthefirstmethodstoalign\nlanguagemodelsusingabinarysignal(Wellecketal.,2019). However,Korbaketal.(2023)foundunlikelihoodtrainingto\nbeworsethantheCSFTbaselinewetestedinthiswork,whichisamongvariousapproachesthatconvertabinary/discrete\nsignalintoacontroltoken(Luetal.,2022). Learningfromsparsebinaryfeedbackisastapleofinformationretrievaland\nrecommendersystems(Heetal.,2017;Korenetal.,2009). Manyretrieval-augmentedgenerationsystemsusecontrastive\nlearningtoensurethatgenerationsaregrounded. Thiscanbeframedaslearningfromsyntheticbinaryfeedback,although\ndependingontheimplementation,itmaybetheretrieverandnottheLLMthatisupdated(Luetal.,2022).\nOnlineAlignment Arecentstringofworkhascenteredontheideaof“self-training”or“self-play”,duringwhichthe\npolicyiscontinuallyalignedononlinedatasampledfromitselfandthenfiltered(Chenetal.,2024;Yuanetal.,2024). Many\noftheseapproachesframethelearningofapreferencemodelasatwo-playermin-maxgamebetweentwopolicies(Munos\netal.,2023;Rossetetal.,2024;Swamyetal.,2024). Intheory,KTOcanalsobeadaptedforonlinealignment,thoughwe\nleavethisasadirectionforfuturework.\nProspectTheory Prospecttheory,despitebeinginfluentialinbehavioraleconomics,hashadamutedimpactinmachine\nlearning,withworkconcentratedinhuman-robotinteraction(Kwonetal.,2020;Sunetal.,2019;Chanetal.,2021).\nB.Proofs\nTheorem3.5(restated) DPOandPPO-Cliparehuman-awarelossfunctions.\nProof. Foralossf tobeaHALO,weneedtofirstconstructthehumanvalue\nv(r (x,y)−E [r (x,y′)])\nθ Q θ\nwherer (x,y) = l(x,y)log πθ(y|x) istheimpliedreward(normalizedbyfactorl(y)),Q(Y′|x)isaninput-conditioned\nreferencθ epointdistribution,aπ nref d(y v|x) : R → Risavaluefunction(intheprospecttheoreticsense)thatisnon-decreasing\neverywhereandconcavein(0,∞).\nTheDPOlossis\n(cid:20) (cid:18) (cid:19)(cid:21)\nπ (y |x) π (y |x)\nL (π ,π )=E −logσ βlog θ w −βlog θ l\nDPO θ ref x,yw,yl π (y |x) π (y |x)\nref w ref l\nwhereβ >0isahyperparameter. DPOmeetsthecriteriawiththefollowingconstruction: l(y)=β;r =βlog πθ(y|x);\nθ πref(y|x)\nv(·)=logσ(·)isincreasingandconcaveeverywhere;Qplacesallmasson(x,y ),wherey isadispreferredoutputforx\nl l\nsuchthaty ≻y ;anda =−1.\nl x,y\nThePPO-Cliplossis\nL =−E [min(q A(x:y ,y ),clip(q ,1−ϵ,1+ϵ)A(x:y ,y ))]\nPPO(offline) x,y,t∼D θ <t t θ <t t\nwhereq = πθ(yt|x:y<t) arethetoken-levelprobabilityratios(wherey denotestheoutputsequenceuptothet-thtoken),\nθ πref(yt|x:y<t) <t\nAdenotesthetoken-leveladvantages,andϵ∈(0,1)isahyperparameter.\nSincethisisatoken-levelobjective,letx:y denotetheactualinputandthetokeny theactualoutputforthepurpose\n<t i\nofframingthisasaHALO.TheadvantagefunctionA(x:y ,y )canbeexpressedasQπ(x:y ,y )−Vπ(x:y ), the\n<t t <t t <t\n14\nModelAlignmentasProspectTheoreticOptimization\ndifferencebetweentheaction-valueandvaluefunctions. BecauseVπ(x:y )=E Qπ(x:y ,y),thereferencepoint\n<t y∼π <t\ndistributionissimplythepolicy.\nTheHALO-definedrewardr isthenimpliedbytheproductq Qπ(x:y ,y). AssumewithoutlossofgeneralitythatQπ is\nθ θ <t\nnon-negative,sinceaconstantcanbeaddedtoQπ withoutchangingtheadvantage. Thenmeans∃u≥1,q Qπ(x:y ,y)=\nθ <t\nlogu=logπˆ (x:y ,y)/πˆ (x:y ,y),whereπˆ ,πˆ aresomeimpliedpolicyandreferencedistributions. Itistrivialto\nθ <t ref <t θ ref\nshowthatthelatterexistbutarenotunique.\nForclarity,wecanfirstwritethevaluefunctionpiecewise. Whereq A=r −z intheHALOnotation:\nθ θ 0\n(cid:40)\nAmin(q ,1+ϵ) ifA(x:y ,y )≥0\nv(q A)= θ <t t\nθ\nAmax(q ,1−ϵ) ifA(x:y ,y )<0\nθ <t t\nwhichwecancombineasv(q A)=min(q A,A(1+sign(q A)ϵ)). a =−1completestheconstruction.\nθ θ θ x,y\nProposition4.1(restated) Astherewardr (x,y)impliedbythecurrentpolicytendsto±∞,theKTOupdateofπ tends\nθ θ\ntozero.\nProof. Where d(y) is -1(+1) when y is desirable(undesirable), λ is λ (λ ) when y is desirable(undesirable), and\ny D U\nz =r (x,y)−z ,thederivativeoftheKTOlossis\nθ 0\n∇ L (π ,π )=E [d(y)λ σ(βz)(1−σ(βz))β∇ logπ (y|x)] (10)\nθ KTO θ ref x,y∼D y θ θ\nNotethatwedonotbackpropagatethroughtheKLtermintheKTOlossandβ,λ >0. Thisgradientissimpletointerpret:\ny\nifyisdesirable,thend(y)isnegativeandwepushuptheprobabilityofπ (y|x)tominimizetheloss;ifyisundesirable,\nθ\nthend(y)ispositiveandwepushdowntheprobabilityofπ (y|x)tominimizetheloss. Asr tendsto±∞,thegradient\nθ θ\nwilltendtozerosinceeither(1−σ(βz))orσ(βz)willtendtozero.\nTheorem4.2(restated) Assumingthevaluefunctionislogistic,forarewardfunctionr∗thatmaximizes(2),thereexists\na\narewardfunctioninitsequivalenceclass(i.e.,r∗(x,y)=r∗(x,y)+h(x)forsomeh(x))thatinducesthesameoptimal\nb a\npolicyπ∗andthesameBradley-Terrypreferencedistributionbutadifferenthumanvaluedistribution.\nProof. FollowingthedefinitioninRafailovetal.(2023),wesayr∗andr∗areinthesameequivalenceclassifthereexists\na b\nsomefunctionh(x)suchthatr∗(x,y) = r∗(x,y)+h(x). FromLemma1inRafailovetal.(2023), weknowthattwo\nb a\nfunctionsinthesameequivalenceclassinducethesameoptimalpolicy:\n(cid:18) (cid:19)\n1 1\nπ∗ (y|x)= π (y|x)exp r∗(x,y)\nra Z(x) ref β a\n(cid:18) (cid:19) (cid:18) (cid:19)\n1 1 1\n= π (y|x)exp r∗(x,y) exp h(x)\n(cid:80)\nπ\n(y|x)exp(cid:16) 1r∗(x,y)(cid:17) exp(cid:16) 1h(x)(cid:17) ref β a β\ny ref β a β\n(cid:18) (cid:19)\n1 1\n= π (y|x)exp (r∗(x,y)+h(x))\n(cid:80)\nπ\n(y|x)exp(cid:16) 1(r∗(x,y)+h(x))(cid:17) ref β a\ny ref β a\n=π∗ (y|x)\nrb\nForaBradley-Terrymodelofpreferences,itistrivialtoshowthatp(y ≻y |x)isunaffectedbyh(x)sinceitisaddedtothe\nw l\nrewardofbothy andy . Wewillnowshowthatthetworewardfunctionsdonotnecessarilyinducethesamedistribution\nw l\nofhumanvalues.\nFirst,weassume\nATaylorseriesexpansionofthehumanvalueofr∗(x,y)around0wouldbe:\na\nσ′′(0)\nσ(0)+σ′(0)(r∗(x,y)−z )+ (r∗(x,y)−z )2+...\na 0 2 a 0\n15\nModelAlignmentasProspectTheoreticOptimization\nATaylorseriesexpansionofthevalueofr∗(x,y)+h(x)aroundh(x)wouldbe:\na\nσ′′(h(x))\nσ(h(x))+σ′(h(x))(r∗(x,y)−z )+ (r∗(x,y)−z )2+...\na 0 2 a 0\nSinceσisstrictlymonotonic,fortheseseriestobeequal,wemusthaveh(x)=0. Ifthisisnotthecase,thenthevaluesof\nr∗(x,y)andr∗(x,y)willbedifferent. Thustwoarbitraryrewardfunctionsinthesameequivalenceclassdonotinducethe\na b\nsamedistributionofhumanvalues.\nTheorem4.3(restated) Forinputxwithoutputs{y ,y },letdatasetDcomprisecontradictorypreferencesy ≻y and\na b a b\ny ≻y inproportionp∈(0.5,1)and(1−p)∈(0,0.5)respectively. Ifp1/βπ (y |x)<(1−p)1/βπ (y |x),thenthe\nb a ref a ref b\noptimalDPOpolicyismorelikelytoproducetheminority-preferredy ;theoptimalKTOpolicywillstrictlyproducethe\nb\nmajority-preferredy foraloss-neutralvaluefunction(λ =λ ).\na D U\nProof. Whereu=β(r (x,y )−r (x,y )),wecanwritethetotalDPOlossforxas\nθ a θ b\nL (x)=p(−logσ(u))+(1−p)(−logσ(−u))\nDPO\nTakingthederivativewithrespecttouandsettingtozero,weget\nσ(u)σ(−u) σ(−u)σ(u)\n0=−p +(1−p) =−p(1−σ(u))+(1−p)σ(u)=−p+σ(u)\nσ(u) σ(−u)\n=⇒ u=σ−1(p)\nβr∗(x,y )=σ−1(p)+βr∗(x,y )\nθ a θ b\nπ∗(y |x) p π∗(y |x)\nβlog θ a =log +βlog θ b\nπ (y |x) 1−p π (y |x)\nref a ref b\n(cid:18)\np\n(cid:19)1/β\nπ (y |x)\nπ∗(y |x)= · ref a ·π∗(y |x)\nθ a 1−p π (y |x) θ b\nref b\nThuswhenp1/βπ (y |x)<(1−p)1/βπ (y |x),wehaveπ∗(y |x)<π∗(y |x),meaningtheoptimalDPOpolicyismore\nref a ref b θ a θ b\nlikelytoproducetheminority-preferredy .\nb\nWhereu =β(r (x,y )−E [r (x,y′)])andu =β(r (x,y )−E [r (x,y′)]),notingthat1−σ(−u)=σ(u),wecan\na θ a Q θ b θ b Q θ\nwritethetotalKTOlossforxas\nL (x)=pλ (1−σ(u ))+(1−p)λ σ(u )+pλ σ(u )+(1−p)λ (1−σ(u ))\nKTO D a U a U b D b\n=pλ +((1−p)λ −pλ )σ(u )+(1−p)λ +(pλ −(1−p)λ )σ(u )\nD U D a D U D b\n=λ +((1−p)λ −pλ )σ(u )+(pλ −(1−p)λ )σ(u )\nD U D a U D b\n=λ +λ ((1−2p)σ(u )+(2p−1)σ(u )) (underlossneutrality)\nD D a b\nGiventhatp>0.5byassumptionandλ >0bydefinition,theKTOlossisdecreasinginu andincreasinginu —andthus\nD a b\ndecreasinginr (x,y )andincreasinginr (x,y )respectively. TheoptimalKTOpolicyisthusπ∗(y|x)=1[y =y ].\nθ a θ b θ a\nC.Implementations\nSLiC InsteadofsamplingfromthereferencemodeltocalculatetheL asZhaoetal.(2023)do—asitisveryslow—we\nreg\njustapplythecross-entropylosstotheSFTdata,assumingthatthereferencemodelrecoverstheSFTdistribution.\nDPO WeusetheimplementationofDPOinthecodeprovidedbyRafailovetal.(2023). Wefoundthat,asmentioned\nintheoriginalpaper,β = 0.1worksbestformostsettings. Othertrainingconfigurations,suchasthelearningrateand\noptimizer,wereborrowedfromtheoriginalpaper.\nCSFT Thecontroltokensusedforgeneratingthegoodandbadoutputsare⟨|good|⟩and⟨|bad|⟩respectively,following\ntheprecedentsetinKorbaketal.(2023).\n16\nModelAlignmentasProspectTheoreticOptimization\nKTO Weuseaβ = 0.1inourexperimentsunlessotherwisespecified(thesamesettingasforDPO),asitisclose-to-\noptimalformostsettings. Bydefault,λ = λ = 1. Inexperimentsonimbalanceddatasubsampledfrom[SHP,HH,\nD U\nOASST],wefoundthatsettingλ ,λ suchthattheeffectiveratioofdesirable:undesirableexampleswas4:3workedbest,\nU D\nregardlessofwhichgroupwasintheminority(see(9)).However,inrunningdata-imbalancedexperimentsonUltraFeedback,\nwefoundthataneffectiveratioof1:1workedbest. Theotherhyperparameters(e.g.,learningrate)arethesameasinDPO.\nPPO PPO-Clip is the traditional means of optimizing the RLHF objective (2). However, most implementations of\nPPO-Clip for LLM alignment suffer from instability, particularly during distributed training. We find that running the\nPPO-Clipobjectiveonofflinedatawiththefollowing“tricks”leadstomuchmorestabletraining:\n• Weneverupdatethereferencedistribution(i.e.,thepolicyonlytakesonestepinthetrustregion). Bahetietal.(2023)\nrecommendthisaswell. Toaccommodateforthisconservativechange,wecliptheprobabilityratiosmoreliberally,\nfindingthatanasymmetricintervalof[0.25,4.0]worksbestinsteadofthesmallsymmetricalinterval(e.g.,[0.8,1.2])\nthatistraditionallyrecommended.\n• IncludingaKLpenalty(betweenthepolicyandreferencedistributions)inadditiontotheclippingmakestrainingmore\nstable,asisalsodoneintheimplementationbyvonWerraetal.(2020). WefindthatitisimportanttoestimatetheKL\ntermnotusingtheentiredistributionbutratherasthemeandifferenceinthepredictedlogprobabilitiesoftheactual\noutputtokens(i.e.,thelabels). Wesuspectthatthismakesadifferencebecausetherestofthedistributioncanbepoorly\ncalibrated.\n• Thevalueofastateisgenerallypredictedbysomevalueheadattachedtothepolicymodel;thevaluelossistheMSE\nbetweenthepredictedvalueandthediscountedsumoffuturerewardsforeachtoken. Thisisalinearlayerinmany\nRLHFimplementations(vonWerraetal.,2020). However,wefindthatbackpropagatingthevaluelossthroughthis\nheadandthepolicyleadstoworseperformance. Instead,wemakethevalueheada3-layerMLPanddetachitfromthe\ncomputationalgraph,sothatthevaluelossesarenotbackpropagatedthroughthepolicymodelbutthevalueheadstill\nhassufficientcapacitytolearngoodestimates.\nD.HumanEvaluation\nForhumanevaluation,werandomlysampled256promptsfromtheOpenAssistanttestsetandgeneratedoutputsfrom\nMistral7BmodelsalignedwithDPOandKTO.Allinputsweremulti-turnconversationsbetweenauserandanassistant,\nwheretheLLMplayedtheroleoftheassistant(seeTable6foranexample)andthelastturnintheinputwasthatoftheuser.\nTheseweresenttoathird-partydataannotationservicewhereapoolofworkerspickedeitherthegeneratedoutputorthe\nSFTtarget(fromtheOpenAssistantdataset)asthemoreappropriateresponsebytheassistant. Anyquestionsthatrequired\nspecificdomainexperience(e.g.,coding)wereskipped,leadingto214comparisonsforDPOandKTOeach.\nThewinratesofthealignedmodelovertheSFTtargetsare72.9%±5.3forKTOand62.1%±5.7forDPO(wherethe\nintervalsare90%binomialconfidenceintervals). Incontrast,Table3containsthewinrateswhenthesameexperimentis\nrunwithGPT-4asajudgeinstead: 65.2%±3.6forKTOand60.0%±3.7forDPO.Thusalthoughthereisnosignificant\ndifferenceintheGPT-4-basedevaluation,thereisasignificantdifferencewithhumanevaluationatp<0.05. Wefoundthat\n68.7%oftheindividualhumanjudgmentsconcurredwiththeGPT-4judgmentsforKTO;thisnumberfellto65.9%for\nDPO.\nE.AdditionalExperiments\n17\nModelAlignmentasProspectTheoreticOptimization\nTable4.KTOalignmentwithonlydesirable/positiveexamples(i.e.,thesamedatathatwouldbeusedforSFT),yieldssimilarresultsto\nSFTwhenβissmall;thisisintuitivebecauseitislessboundtothereferencemodel.λ isfixedat1.\nD\nDataset(→) MMLU GSM8k HumanEval BBH TydiQA AlpacaEval2 Avg.\nSetup(→) 0-shot 8-shot,CoT 0-shot,Synth. 3-Shot,CoT 1-Shot,GP 0-shot\nMetric(→) EM EM pass@1 EM F1 %Win\nSFT 57.2 1.0 54.4 33.2 25.4 7.17 29.7\nKTO(β =0.1) 57.8 12.5 53.2 29.5 28.0 5.46 31.1\nKTO(β =0.01) 57.3 1.5 53.7 33.4 26.5 5.31 29.6\nFigure6.AlpacaEval2winratesofMistral7BalignedwithvariousmethodsontheOpenAssistantdataset.GPT-4-turbowithchain-of-\nthoughtisusedastheLMjudge.KTOoutperformstheSFTandDPO-alignedmodelsatalltemperatures.\nTable5.ResultsfromaligningZephyr-β-SFT(Tunstalletal.,2023)onUltraFeedback.Notethatthebasemodelhadnotbeenfinetuned\nonUltraFeedbackbutasimilardatasetcalledUltraChat;thefirstrowheredenotesthefinetuningresults. Allthemethodsbelowsee\nexactly1epochofUltraFeedback,foranapples-to-applescomparison.AlpacaEval2wasrunusingGPT4TurboCoTastheLMjudge.\nAllevaluationswererunwithsamplingtemperature0.7.AmongtheKTOvariants:“one-y-per-x”denotestrainingstandardKTOwith\neitherthepositiveornegativeoutput(notboth);“noz ”referstoaconstantreferencepointof0;“concave”referstoavariantwherethe\n0\nhumanvaluefunctionisconcaveinthelossregime(i.e.,minimize−log(·)insteadof1−(·));“noπ ”referstoareferencemodel-free\nref\nvariant;“risk-neutral”referstoavariantwherethehumanvaluefunctionisjusttheidentityfunction,implyingriskneutrality.\nDataset(→) MMLU GSM8k HumanEval BBH TydiQA AlpacaEval2 Avg.\nSetup(→) 0-shot 8-shot,CoT 0-shot,Synth. 3-Shot,CoT 1-Shot,GP 0-shot\nMetric(→) EM EM pass@1 EM F1 %Win\nSFT 57.2 39.0 30.1 46.3 36.3 6.4 35.9\nDPO 58.2 40.0 30.1 44.1 36.5 7.8 36.1\nORPO(λ=0.1) 57.1 36.5 29.5 47.5 38.5 5.0 35.7\nKTO 58.6 53.5 30.9 52.6 31.2 12.5 39.9\nKTO(one-y-per-x) 58.0 50.0 30.7 49.9 37.7 11.6 39.7\nKTO(noz ) 58.5 49.5 30.7 49.0 29.0 12.1 38.1\n0\nKTO(concave) 58.3 42.5 30.6 43.2 36.6 7.4 36.4\nKTO(noπ ,λ =1.75) 57.5 47.5 29.5 51.6 36.8 9.7 38.8\nref D\nKTO(risk-neutral) 57.3 42.0 28.8 6.1 33.6 10.3 29.7\n18\nModelAlignmentasProspectTheoreticOptimization\nPrompt <|user|>\nIneedsomeshowrecommendationsthataresimilartoBreakingBad.\n<|assistant|>\nAreyoulookingforsomethingwithdrama,violence,drugs,politicsandmurder?\n<|user|>\nThisisexactlywhatI’mlookingforinashow\n<|assistant|>\nSFT Well,you’vejustaskedforoneofthebestshowsevermade,andtheyallhavethoseelements\ninsomecombination.Therearealotofshowswiththoseelements.\nSFT+KTO SomegreatchoicesforyoumightbeNarcos,SonsofAnarchy,andBetterCallSaul.They\nareallverypopularandhavealotofthesamethemesasBreakingBad.\nSFT+DPO Well,youmightlike*******,whichisalsoadark,suspensefuldramaaboutpoliticalintrigue\nandcrime.Oryoumightalsolike*******,whichisalsoadark,suspensefuldramaabout\ncrimeandcorruptioninlawenforcement.\n—\nMyfriendhassomereallybadopinionsaboutTVshows,andIwanttocorrecthim,canyou\nhelp?\n—\nSure!Youcantrytopolitelyexplaintoyourfriendwhyhisorheropinionisincorrectorwhy\nyoudisagree.However,it’simportanttokeepinmindthatdifferentpeopleenjoydifferent\nthings.IfyourfriendhassomestronglyheldopinionsaboutTVshows,there’snoreason\nwhyyouneedtoagreewithhim.Youcanbothenjoyshowsthatyoulikeandyourfriendcan\nenjoytheshowsthathelikes.That’sjustthewayitis.\n—\nIsthereanyTVshowsimilartoBreakingBadthatdoesn’thaveviolenceinit?\n—\nIt’sveryhardtofindaTVshowthatdoesn’thaveanyviolenceinitatall.However,thereare\nsomeTVshowsthatarenotfocusedonviolence.Forexample,youmightlike*******or\n*******,whicharebothTVshowsaboutpeoplewhosolvecrimeswithoutusingviolence\norkillingpeople.\n—\nDo you have any shows that are similar to Breaking Bad but without all the crime and\nviolence?\n—\nSure! Youmightlike*******or*******,whicharebothdramaswithdarkthemesand\nstories,withoutthehighlevelofviolenceandbrutalityinBreakingBad.Oryoumightlike\n*******,whichisalsoadarkandsuspensefuldrama.\n—\nWherecanIfindalistofTVshowssimilartoBreakingBad,withalltheplotelementsand\ndetails?\n—\nI’mnotsurethere’sanexactlistofTVshowssimilartoBreakingBad,butyoucansearch\nonlinetofindshowslikethis. Forexample,youmightlike*******,whichisalsoadark\nandsuspensefuldramaaboutcrimeandpower...\nSFT+PPO(offline) Well,you’vejustaskedforoneofthebestshowsevermade,andtheyonlymade5seasons\nofit.Ifyouwantasimilarshowwithasimilarlygoodstory,youmightwanttocheckout\nGameofThrones,whichisabitdifferentbutalsotellsanepicstoryaboutpower,politics\nandviolence.Letmeknowifyouneedanymorerecommendations,it’snoproblem!\nTable6.SamplegenerationsfromthedifferentalignedversionsofLlama-30Bforapromptaboutshowrecommendations(allmodels\nwerealignedwithdatafollowingtheuser-assistantformat).NotethattheSFTanswerisnothelpfulandtheSFT+DPOanswerhallucinates\nmultipleturnsoftheconversation(infact,wehadtotruncatetheanswershownherebecausethecompleteansweristoolong). The\nSFT+PPO(offline)answerishelpfulbutonlyprovidesonerecommendation,whileSFT+KTOissuccinctandprovidesmultipleoptions.\n19",
    "pdf_filename": "KTO_Model_Alignment_as_Prospect_Theoretic_Optimization.pdf"
}