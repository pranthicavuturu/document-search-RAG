{
    "title": "AComputationalMethodforMeasuring“OpenCodes”inQualitativeAnalysis",
    "abstract": "",
    "body": "AComputationalMethodforMeasuring“OpenCodes”inQualitativeAnalysis\nJOHNCHEN,NorthwesternUniversity,UnitedStatesofAmerica\nALEXANDROSLOTSOS,NorthwesternUniversity,UnitedStatesofAmerica\nCAIYIWANG,NorthwesternUniversity,UnitedStatesofAmerica\nLEXIEZHAO,NorthwesternUniversity,UnitedStatesofAmerica\nJESSICAHULLMAN,NorthwesternUniversity,UnitedStatesofAmerica\nBRUCEL.SHERIN,NorthwesternUniversity,UnitedStatesofAmerica\nURIJ.WILENSKY,NorthwesternUniversity,UnitedStatesofAmerica\nMICHAELS.HORN,NorthwesternUniversity,UnitedStatesofAmerica\nQualitativeanalysisiscriticaltounderstandinghumandatasetsinmanysocialsciencedisciplines.Opencodingisaninductive\nqualitativeprocessthatidentifiesandinterprets\"opencodes\"fromdatasets.Yet,meetingmethodologicalexpectations(suchas\"as\nexhaustiveaspossible\")canbechallenging.Whilemanymachinelearning(ML)/generativeAI(GAI)studieshaveattemptedtosupport\nopencoding,fewhavesystematicallymeasuredorevaluatedGAIoutcomes,increasingpotentialbiasrisks.BuildingonGrounded\nTheoryandThematicAnalysistheories,wepresentacomputationalmethodtomeasureandidentifypotentialbiasesfrom\"open\ncodes\"systematically.Insteadofoperationalizinghumanexpertresultsasthe\"groundtruth,\"ourmethodisbuiltuponateam-based\napproachbetweenhumanandmachinecoders.WeexperimentwithtwoHCIdatasetstoestablishthismethod’sreliabilityby1)\ncomparingitwithhumananalysis,and2)analyzingitsoutputstability.Wepresentevidence-basedsuggestionsandexampleworkflows\nforML/GAItosupportopencoding.\nAdditionalKeyWordsandPhrases:OpenCoding,QualitativeAnalysis,LargeLanguageModels,Human-AICollaboration,Inductive\nCoding,ThematicAnalysis,GroundedTheory,Measurement\nACMReferenceFormat:\nJohnChen,AlexandrosLotsos,CaiyiWang,LexieZhao,JessicaHullman,BruceL.Sherin,UriJ.Wilensky,andMichaelS.Horn.2024.\nAComputationalMethodforMeasuring“OpenCodes”inQualitativeAnalysis.In.ACM,NewYork,NY,USA,24pages.\n1 INTRODUCTION\nQualitativeanalysisistheprocessofsystematicallyidentifying,generating,andorganizingconceptsfromdata.Ithas\nbeenwidelyadoptedinmanysocialsciencedisciplines(suchaseducation,sociology,psychology,andmedicine),as\nwellasinterdisciplinaryareassuchashuman-computerinteraction(HCI)1tounderstandpeople’sperceptions,feelings,\nandnuancedinteractionswithtechnology[2].\nHowever,qualitativemethodscanbechallenging,time-consuming,andmaylacktransparency[9].Theproblemis\nparticularlyacuteduringtheprocessofopencoding,whereresearchersinductivelyidentifyemergentcodesfrom\nrawdatawithoutapreconceivedcodingscheme.Asthefirststepofqualitativeanalysis,practitionersandtheoristsof\nthematicanalysis(TA)[10,63]andgroundedtheory(GT)[23]makefrequentuseofopencodingandagreeonitsgoal:\n1Forexample,theCHIconferenceitselfhasincreasinglyincludedpapersusingqualitativemethods:36.7%in2021,48.4%in2022,48.9%in2023,53.9%\nin2024.ThisresultcomesfromsearchesintheACMdigitallibrarywiththekeywords:\"thematicanalysis,\"\"qualitativeanalysis,\"\"qualitativecoding,\"\n\"qualitativeresearch,\"\"discourseanalysis,\"and\"groundedtheory.\"\n2024.ManuscriptsubmittedtoACM\n1\n4202\nvoN\n91\n]LC.sc[\n1v24121.1142:viXra\n,,\nChen,etal.\ntocaptureasmanyaspects,patterns,or\"codingmoments\"aspossible.However,thecriteriaofbeing\"open\"and\"as\nexhaustiveaspossible\"areofteninconsistentandsubjectiveinpractice[30,58].\nMachinelearning(ML)andgenerativeAI(GAI)techniqueshavethepotentialtosupportandimprovequalitative\nresearch,butthispotentialhasyettobefullyrealized.Paststudieshavemostlyfocusedondeductivecoding[40,55,71],\nwhereresearcherssystematicallyapplyexistingcodingschemestonewdatasets.However,attemptstoapplyML/GAI\ntoopencodinghavebeenmostlyexploratory.WhiletheHCIliteraturerepeatedlyfoundthathumanstendtoover-rely\nonAIsystemsandcanbeeasilymisledwhenAIsystemsgoastray[15],commercialproviders(suchasAtlas.ti)have\nalreadystartedtoprovideGAIsolutionsforopencoding.Sinceallfutureanalysesrelyonopencodingresults,thereis\napressingneedforsystematicmeasurementmethodsonopencodingresults(or\"opencodes\").\nWeproposeatheory-informedcomputationalmethodtosystematicallymeasureopencodes.Theoristsofthematic\nanalysisandgroundedtheoryadvocateateam-basedapproach[18,24]thatdependsonmultiplecoderstofind\"asmany\nrelevantconceptsorinterpretationsaspossible\"fromthedata[22,24,63].Buildingonthisconcept,wecomputationally\ntransformedeachcoder’sinductivecodesintoaCodeSpace,andcalculatedthesumofallcodespacesasanAggregated\nCodeSpace.Thisenablesmeasuringindividualcoders’resultsagainstateamofcoders,whosecombinedcodesmay\ngetclosertothetheoreticalexpectation.WeproposeandoperationalizefourconceptualmetricsforassessingCode\nSpaces:Coverage,Density,Novelty,andDivergence.\nWedemonstrateourmethod’spotentialasanovelcomputationallenstounderstand,compare,andevaluateopen\ncodes(fromeitherhumanormachinecoders).WorkingontwoHCIdatasets,wecomputationallymeasuredopencodes\ngeneratedbyfivepreviouslypublishedML/GAIapproachesandsixmainstreamGAImodels.Wevalidatethereliability\nofourmethodby1)comparingitsresultswithhumananalysis,and2)statisticallyanalyzingitsoutputstability.By\ncombiningmachinemeasurementandhumaninterpretation,wepresentanexampleofahuman-AIcollaborative\nworkflowtoevaluateopencodes,resultinginevidence-basedsuggestionsforqualitativeresearcherstoadoptGAIin\ninductiveanalysis.\n2 BACKGROUND\n2.1 OpenQualitativeCoding\nQualitativeanalysisenablesanin-depthexplorationofhumanexperiencesbyfocusingonthenuancedinterpretations,\nemotions, and subjective experiences that shape individual and social realities [52]. Thematic Analysis (TA) and\nGroundedTheory(GT),twocommonlyusedqualitativemethodsinHCIresearch[1,2,9,21,48,50,53,54,56],both\nadvocateforopencodingasanessentialfirststeptouncovertheunderlyingreasonsandprocessesthatdrivethe\nformationandtransformationofmeaning[52,62].However,conductingopencodingcanbechallenging,particularly\nwhenscalingup.Ambiguity[4,9,66],inconsistentterminology[12],andinsufficientreporting[14]canallobscurehow\nqualitativeanalysiswasperformed,makingithardertoassessitsquality[9,49].Fewstudieshaveattemptedtomeasure\norevaluateopencodingresultsbasedontheoreticalexpectations,furthercompoundingthechallenge.\n2.1.1 ExpectationsofOpenCoding. OpencodingisakeyapproachinTAandGTthatinductivelyderivescodesfrom\nrawdata.Itcangeneratenewcodingschemeswithoutorinadditiontoatheoreticalframework[24,62].Researchers\ncanthenapplythecodingschemesystematicallyondatasetsthroughthedeductivecodingprocess[7].Withthe\ncapabilityoffindingnovelinsightsbeyondexistingknowledge,GTtheoristsrequireopencoding[22,24,62],andTA\ntheoristsadvocateforit[63].Sincesubsequentanalysisdependsontheopencodingresults,withoutfirstestablishing\n2\n,,\nAComputationalMethodforMeasuring“OpenCodes”inQualitativeAnalysis\ntheschemes’relativecompleteness,researchersriskenteringabiasedorunbalancedconceptspacethatwouldharm\ntherigorofqualitativeresearch.Itis,therefore,imperativetoreviewtheexpectationsandchallengesofopencoding.\nManytheoristsofTAandGTagreethatopencodingshouldcaptureasmanyaspects,patterns,or\"codablemoments\"\naspossible[22,24,63].WhileopencodingisnotalwaysrequiredforTA,TAtheoristssuchasBraunandClarke[10,13]\narguethatthefocusofqualitativeassuranceshouldshiftfromaccuracy(i.e.,theconsistencyindeductivecoding)\ntoreflexivity(i.e.,howtheopencodingprocessreflectstheunderlyingdataset).Similarly,comingfromanimplicit\nconstructivistepistemologicalstance,Terryetal.contendthatopencodingoutcomesmaybestrongorweak,but\ncannotbeobjectivelyrightorwrong.ThisshifttowardreflexivityalignswithGT’sinductiveprocess,whichemphasizes\nopennesstonewinsights.GTstressesthatresearchersmustcontinuallylookforemergingideasuntiltheoretical\nsaturationisreached,atwhichpointnofurtherinsightsemerge[24,47].Sincethegoalistodiscoverpotentialideas,\nopencodesdonotneedtobeconsistentorsystematic,andasingleexamplemaysuffice.\n2.1.2 EvaluationofOpenCoding. Derivingpatternsdirectlyfromrawdatawithoutapredefinedframeworkisinherently\nchallenging[43].Thischallengeiscompoundedbytheunderdevelopmentofmeasurementforopencodingandtheorists’\nfocusonresearcherquality(qualifications,experience,andperspective)overresearchquality[23].Somemetrics,suchas\nvalidityorcredibility,areeasiertojudgebythedataorkeeptrackof[28].Othermetricsaremoreelusive:forexample,\nhowcanweoperationalizetheexpectationsofinductivecoding-being\"open\"and\"asexhaustiveaspossible\"?Even\nwhencodersidentifysomevalidandcrediblecodes,asCorbinandStraussacknowledge,theyareonlysomeofthe\n\"manyplausibleinterpretationspossiblefromdata.\"\nSomescholarshaveattemptedtoadoptdeductivecodingmetricssuchasinter-raterreliability(IRR)inopencoding[44].\nYet,IRRcanonlyassessconsistencybetweencoders.Asqualitativeresearchersmaynothaveorreachtheground\ntruth,evencompleteconsistencycannotmeasurethe\"openness\"or\"exhaustiveness\"ofthecodes.Mostqualitative\nresearcherswhoadoptnon-positivistepistemologicalstancesbelievethat[17]:post-positivismbelievesinobjective\ntruth,yethumansmayonlyapproximateit[29,37];interpretivismarguesthattruthissubjectiveandsituated[64],while\nconstructivismholdsthattruthisahumanconstructiontounderstandreality[37,46].Ontheotherhand,whileafew\nscholarshaveadvocatedforpositivistqualitativeresearchandbelieveingroundtruth[6,70],itisunclearhowsuch\ntruthcanbereliablyidentifiedandvalidated.\nGTtheoristshaveexploredanotherpotentialpathwayofevaluation.CorbinandStraussproposedtwocriteriarelated\ntoinductivecoding:1)depth,whichreferstotherichnessofdescriptivedetailthataddssubstancetofindings,and2)\nvariation,whichdemonstratesthecomplexityofhumanlifebyincorporatingdiversecasesoutsideofdominantpatterns.\nHowever,operationalizingthesecriteriacanbedifficult.Traditionally,GTtheoristssuggestworkingtowardstheoretical\nsaturation,wherenonewinterpretationsemergefromadditionaldata,andexistingonesareallwell-definedwith\nsufficientvariation[23].Yet,inreal-worldresearchcontexts,thelogicfordeterminingsaturationisofteninconsistent\nandsubjective[2,30,58],asitischallengingtojudgewhethercodinghastrulybecomeexhaustive[32],orhassimply\nreachedaconvenientstoppingpoint.\nWhiletheabsolutesaturationmaybeimpractical,TA[18]andGTtheorists[24]advocateforateam-basedapproach.\nAnalyzingdatafrommultipleperspectivesreducesthelikelihoodofmissingkeycodesorover-interpretingdata.By\nconstantlycomparingandcontrastingcodesfromdifferentindividuals[18],researcherscanopenuptheanalysisto\nthescrutinyofotherresearchers[24],resultingin\"newinsights[and]increasedsensitivity\"while\"guardingagainst\nbias.\"Similarly,theGT-inspiredgeneralinductiveapproach[65]suggestsmergingindependentcoders’opencoding\n3\n,,\nChen,etal.\nresultsandcheckingtheproportionofoverlappingcodes.Alowdegreeofoverlapmaynecessitatemorediscussionand\nfurtheranalysis.\nRecently,someTAscholarshaveattemptedtoquantify“saturation”bycountingthenumberofcodesthatemerged\nthroughoutthecodingprocessuntilthecurveflattens[38,42].However,theworkisbuilton(possiblyoverly)strong\nassumptionsabouttheinductivecodingprocess:1)duplicatedcodesdonotexist,and2)researcherswillconsistently\n“sample”fromall“discoverable”codes.Whilethislineofworkhaspracticalvaluesforpredictingthenumberof\ninterviewsorsurveysneeded[42],bothassumptionsareunlikelytoholdforhumancodersduringinductivecoding.As\nML/GAImodelsmaybesystematicallybiased(e.g.,[59,69]),theissueisonlyexacerbatedformachineapproaches.\nInformedbytheliterature,wecontributeatheory-drivencomputationalapproachtomeasureinductivecodingresults,\ntakinganimportantsteptowardsaddressingthebroaderconcernsaboutefficiencyandtransparencyinqualitative\nanalysis.ThenextsectionwillreviewexistingML/GAIworkonthistopic.\n2.2 Machine-AssistedQualitativeAnalysis\nManyfields(e.g.,computervision,dataanalytics,etc.)havewidelyusedML/GAImethodstolabeldata.Qualitative\ncoding,whetherinductiveordeductive,canalsomechanicallybeseenasapplyinglabels(i.e.,codes)toagivenpiece\nofdata.ThissectionreviewstwoML/GAIperspectivesformachine-assistedqualitativecoding-classificationand\ngeneration.Whileclassificationnaturallyfitsintodeductivecodingscenarioswithestablishedevaluationmetrics,its\npotentialforinductivecodingisintrinsicallylimited.Generation,ontheotherhand,hasstartedtopickuptractionfor\ninductivecoding.Yet,fewstudieshaveattemptedtoevaluatethem,resultinginsignificantrisksforresearchrigor.\n2.2.1 ML/GAI For Classification. From a computational perspective, most current work views machine-assisted\nqualitativecodingasaclassificationtask,wherealgorithmsaimtoproducecodesassimilarlytohumanresearchersas\npossible.Morerecently,researchershaveworkedonthreeclassificationapproaches:\n(1) SupervisedMLmethodsthataretrainedonhuman-codeddatasets.Liewetal.,forexample,approachmachine-\nassistedqualitativecodingasa“multi-labelclassificationtask”usingSVMstrainedonhand-codeddatasetstoaid\ninsocialscienceresearch.CoAICoder[31]similarlyleveragednaturallanguageunderstandingmodelstrained\nonhumancoders’worktoaidincollaborativequalitativecoding.\n(2) Rule-basedAIsystemsthatextracttext-matchingrulesfromhumancodingresultsandapplythemtomore\ndatasets.Forexample,[55]and[33]encodehumancoders’codingresultsintorulesforAItosuggestcodesin\nunseendata.Theusageofhuman-readableandeditablerulesincreasesthetransparencyinmachinecoding.\n(3) LLM-basedmethodsthatinstructLLMstolabeldatawithapredeterminedcodebook.Forexample,[71]used\nGPT-3tolabeldatawithafixedsetofcodes,thussupportingresearchersindeductivecoding.Despitethe\nincreasedexplainability,someevaluationstudieshavepointedtoincreasedbiasasapotentialsetback[3].\nClassificationapproachesaregenerallyeasiertoscaleandevaluate,makinglarge-scaledatasetsmoreaccessible.\nGiventheobjectiveforalgorithmstoproducehuman-likecodes,researchershaveadoptedtraditionalMLanddeductive\ncodingmetricstoevaluatealgorithmoutcomes.Forexample,Liewetal.usedhumancodingasthegroundtruthand\nappliedtwocommonMLmetrics:precision,whetherthepositivepredictionsofthemodelwereaccurate,andrecall,how\nwellthemodelcouldidentifypositiveinstancesinthetrainingset.InXiaoetal.,GPT-3’sperformancewasevaluated\nwiththeinter-raterreliability(IRR)withhumancoders’independentcodingresults.\nWhiletheclassificationapproachismoresuitablefordeductivecoding,ithassomeintrinsictheoreticallimitations\nforinductivecoding.Bypositioninghumancoders’workas\"groundtruth,\"classificationlimitsitsoutputstolabels\n4\n,,\nAComputationalMethodforMeasuring“OpenCodes”inQualitativeAnalysis\nprovidedbyhumancoders.Whilethislimitationalsoappliestodeductivecoding,itismorepronouncedininductive\ncoding,wherecodersaresupposedtointerpret\"novel\"insightsfromthedataset.Theyareexpectedtokeepanopen\nmindandcaptureasmanycodesaspossible.Astheprocesslacksapredefined\"groundtruth,\"truth-based(suchas\nprecisionandrecall)orconvergence-basedmetrics,becomemuchlessuseful(see2.1.1).\n2.2.2 ML/GAIForGeneration. Opencoding,ontheotherhand,isanaturalfitforgenerationtasks:thenumberand\nnatureofunderlyingcodes(orthemes)areunknownbeforetheinductiveprocess.Whilesomescholarsattemptto\nevaluatetheresultsbymatchinghumanexpertcodes[51,73],suchevaluationunderutilizesGAI’spotential,which\ncouldaidresearchersinidentifyingnovelopencodesfortheorybuilding.Twoapproacheshavebeenmorestudied:\n(1) Topicmodeling,anunsupervisedMLtechnique,identifiessemanticallysimilarwordgroups(topics)intext-\nbaseddatasets.Ithasbeenusedforinductivecodinginvariouscontexts,suchassurveydata[5],socialmedia\nposts,andonlinediscussions[57].Theresultingtopicshelpresearchersfocusonkeypartsofthedatasetand\nautomatecodingforfutureanalysis.Despiterecentefforts,thedifficultyininterpretingandevaluatingtheresults\nstilllimitsitspower[36,60].\n(2) GAImodelshavebeenmorerecentlyadoptedforopen,inductivecoding.Byiterativelyprovidingdatapieces\nwithrelevantinstructions(e.g.,researchquestions,codinginstructions,desiredoutputformat),LLMscanproduce\ncodesthathumansfindmoreinterpretableanduseful[26,61].However,theymaystillmissnuanceandstruggle\nwithlesslinguisticallystraightforwardthemes[26,39],whilegeneratingnon-groundedresultsoroperatingata\ncoarserlevelofanalysis[16,72].Carefulpromptdesignandpromptingstrategiesareessentialtomitigatethese\nlimitations,yetfewcomparisonstudiesexist[16,61].\nRegardlessofthemethodused,thelackofa\"groundtruth\"andagreed-uponstatisticalmetricsposesagreater\nchallengeforevaluatingopencodes.Whileresearcherscanassessalgorithms’outputbasedonusefulnessorexplain-\nability,thesubjectivemeasuresarelabor-intensive.Moreover,thosemeasuresmayoverlookthecomplexityofinductive\ncoding.Evenifalloutputcodesseemusefulorexplainable,thealgorithmmaystillmisscriticalcodeswithoutthe\nevaluators’knowledge.SimilarproblemsexistforDePaoliandMathis’scomputationalmeasurement,wherethe\nresearchersequentiallyfeedsdatapiecesintogpt-3.5-turbountilnomorecodesarefound:first,notallcodesmay\nuniformlyexistamongdatapieces.IftheLLMaccidentallymissesacodeinthefirstinterview,thereisnoguarantee\nthatitwillpickupagainlater;second,theLLMmaybesystematicallybiasedtomisscertaincodesthroughoutthe\nprocess.\nAssuch,thereisapressingneedtoaligntheoreticalexpectationsofqualitativeanalysiswithpracticalevaluation\nmechanismsformachine-assistedinductivecoding.Ourworkaddressesthisbyintroducinginnovative,theory-informed\ncomputationalmethodsforinductivecodingandanovelcomputationalapproachforevaluatingtheseresults.\n3 MEASURINGINDUCTIVECODES\nInformedbyliterature(see2.1.2),wedevelopedacomputationalmeasurementthat1)aggregatesopencodesofmultiple\nhuman/machinecodersand2)measureseachagainsttheaggregation.Usingthemethod,wemeasuredtheoutcomesof\ndifferentML/GAIapproachesontwodatasetsandcomparedthefirstdataset’sresultswithhumanevaluation.With\nhuman-AIcollaboration,weexploredthepotentialbiasofML/GAIcodingapproaches.Sincepartofourmethodinvolved\ngenerativeAI,wevalidateditsreliabilitywithanoutputanalysis.\n5\n,,\nChen,etal.\n3.1 TheConceptualMethod\nFollowingthesuggestionsofTAandGT,weadoptateam-basedapproachandmeasureindividualcoders’results\nagainsttheaggregationofmultiplecoders.Toachievethat,weproposedaconceptualstructure,CodeSpace(CSP),to\nrepresentinductivecodesproducedbyeachindividual.ThesumofindividualCSPsbecomesanAggregatedCode\nSpace(ACS).UsingACSasareferenceforevaluation,weproposedfourconceptualmetrics(Coverage,Density,Novelty,\nandDivergence)tomeasureindividualCSP’srelativeperformance.\nFig.1. A:AconceptualillustrationofanACSmergedfrom𝑐𝑠𝑝 1and𝑐𝑠𝑝 2.B:Measuring𝑐𝑠𝑝 1usingthemergedACSasareference.\n3.1.1 Code Spaces (CSP), Aggregated Code Spaces (ACS). Assuming we need to measure sets of inductive codes\n𝐶𝑜𝑑𝑖𝑛𝑔_𝑅𝑒𝑠𝑢𝑙𝑡𝑠.WedefineaCodeSpace(CSP)asamulti-dimensionalconceptualspacethatcoversmultiplecodes\nidentifiedorinterpretedfromtheunderlyingqualitativedataset,eachrepresentingasetofcodes.Eachcodeisrepresented\nbyanetworknodeandhasmultipledimensionsrelatedtoitsconceptualnature.\nCombiningmultipleCodeSpaces,wegetaAggregatedCodeSpace(ACS)thatcoversallcodesfromall𝐶𝑜𝑑𝑖𝑛𝑔_𝑅𝑒𝑠𝑢𝑙𝑡𝑠\n(Fig1A).Similarbutnotequivalentcodesarenowconnectedbylinksandconsideredas𝑁𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠.TheideaofACS\nacknowledgesthathumansmaynotfindallpossibleinterpretationsbutcouldstillbuildan\"aggregated\"setofcodes\ntocomecloser.ACSisalsoasnapshotintimethatdocumentsresearchers’currentefforts.Itcanbeusedtolookfor\nconvergenceanddivergenceamongtheteammembers,supportingdifferentstagesofqualitativeanalysis.Convergence\nmayhelpresearchersdeterminethecurrentconsensusandindicateeachcode’svisibilityandimportance.Divergence\nmayrevealunderlyingbiases,differentfocuses,ormissedopportunitiesforresearchers.Forexample,arecentstudy\n[45]critiquesoverrelianceonconsensusbuildingandadvocatesformoreattentionto“dissonances,disagreements,and\ndifferences.”\n6\n,,\nAComputationalMethodforMeasuring“OpenCodes”inQualitativeAnalysis\nTheconceptsofCSPandACSenableustomeasureinductivecodingresults𝐶𝑜𝑑𝑖𝑛𝑔_𝑅𝑒𝑠𝑢𝑙𝑡𝑠.Whilethispaper\nmayhavecoinedthetermACS,manyqualitativeresearchershavelongreliedonsimilarideasandpractices(e.g.,\nThomas).ExaminationofACScouldhappenduringopencoding,whenresearchersdiscusswhatwasdiscoveredand\nnotdiscoveredbyindividualteammemberstohelpthemreorientthenextcodingbatch;duringaxialcoding,when\nresearcherscollaboratetoconsolidatetheACSintocodebooks;andduringselectivecoding,whenresearcherstruncate\ntheACStocoalescearoundacorecategoryrelevanttotheresearchquestion.\n3.1.2 ConceptualizetheEvaluatingMetrics. WeproposefourconceptualmetricstomeasureindividualCSPsagainst\ntheiraggregationACS(Fig1B),whichwillbeoperationalizedinlatersections:\n• CoverageandDensity:HowmuchconceptualspacedoesaCSPcover,andhowmanycodesdoesituseto\ncoverthisspace?BothTAandGTstrivefor\"richness\"ofcodes.Inpractice,researchersneedtostriveforbreadth\nanddepth.Breadthmeanscoveringasdiversesetsofconceptsaspossible.Depthmeansthedescriptivedetails\nthatensurestheconcept’smeaningfulnessandrichness,thussupportingresearchers’furtheranalysis[11,23].\nCombiningthem,researchersmaybeabletocapturefindingswithdepthandvariation,twocriteriaforevaluating\nGTstudies[23],totheextentthat“nothinghasbeenleftout.”[35]Ontheotherhand,notallcodesareofequal\ninteresttoresearchers.Someconceptsaremorelikelytobegroundedinorrelevanttotheresearchquestion.\nThemetricsmustaccountforeachconcept’simportanceandweighitaccordingly.\n• Novelty:Howmuchofthe“novel”conceptualspacedoesaCSPinclude?Wedefine\"novel\"codesasonesthat\nfewerthan𝑁𝑜𝑣𝑒𝑙𝑡𝑦_𝑇ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑CSPshaveincluded.Mostofthetime,CSPswillnotbe(evencloseto)identical.\nSometimes,aresearcherbringsinnovelideasorlensesthatweremissedbyothers;inothercases,aresearcher\ncouldmakeasliporinappropriatelynameacode.Identifying\"novel\"codescouldhelpusidentifyeitherscenario\nandsupporthuman-humanorhuman-AIcollaboration.\n• Divergence:HowfarisaCSP’scodedistributionfromtheACS?SupposetwoCSPs,AandB,havethesame\ncoverageanddensity.However,AhasmorecodesincommonwithmostCSPs.B,ontheotherhand,hasmore\ncodesincommonwithnooneelse(\"novel\"codes)orfewinthegroup.Todetectthisatthemacrolevel,we\ncalculateeachCSP’sdivergenceastheseparationfromitsprobabilitydistributionofcodestothatoftheACS,\nusingthelatterasa“groundtruth.”Inotherwords,thecloseraCSPconceptdistributionistotheACS,theless\n\"divergent\"itis.\nSinceourevaluationfocusesmoreontherelativecomparisonbetweencodingresults,itdoesnotdirectlyaddress\npotentialissuesofgroundedness,i.e.,whethercodescouldbereasonablyre-identifiedbyanothercoderfromthe\nunderlyingdata.However,ourmetricsdoprovideanindirectpathwayforhuman-AIcollaboration:bylookingatthe\n\"novel\"partsofCSPand/orcodeswithveryfewexamples,wemaybeabletoidentifycodeswithpotentialgroundedness\nissuesmoreeasily.\n3.2 TheComputationalMethod\nTheoperationalizationofourmethodstartsfromconsolidating𝐶𝑜𝑑𝑖𝑛𝑔_𝑅𝑒𝑠𝑢𝑙𝑡𝑠intoanACSforevaluationreference.\nEach𝐶𝑜𝑑𝑖𝑛𝑔_𝑅𝑒𝑠𝑢𝑙𝑡 shouldcomefromanindividualmachineorhumancoder.For𝐶𝑜𝑑𝑖𝑛𝑔_𝑅𝑒𝑠𝑢𝑙𝑡 tobetreatedas\naCSP,each𝐶𝑜𝑑𝑒 shouldhavea𝐿𝑎𝑏𝑒𝑙,anoptionallistof𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛𝑠,andalistof𝐸𝑥𝑎𝑚𝑝𝑙𝑒𝑠 (i.e.thepiecesofdata\nwherethecodewasidentified).IntheoutputACS,each𝐶𝑜𝑑𝑒 willhaveaconsolidated𝐿𝑎𝑏𝑒𝑙,𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛,alistof\n𝐸𝑥𝑎𝑚𝑝𝑙𝑒𝑠,alistof𝑁𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠(othercodesthataresemanticallyclosetothisone),andalistof𝑂𝑤𝑛𝑒𝑟𝑠(𝐶𝑜𝑑𝑖𝑛𝑔_𝑅𝑒𝑠𝑢𝑙𝑡𝑠\nthathaveincludedoneormorevariationsoftheconsolidatedcode).𝑁𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠enablethenetworkstructureofACS\n7\n,,\nChen,etal.\nandthederivationofcode𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑠.EvaluatingindividualCSPsagainsttheACS,wecancalculate𝐶𝑜𝑣𝑒𝑟𝑎𝑔𝑒,𝐷𝑒𝑛𝑠𝑖𝑡𝑦,\n𝑁𝑜𝑣𝑒𝑙𝑡𝑦,and𝐷𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒 asoverallandcluster-levelmetrics.Foranonymizationreasons,wewillopen-sourceour\nimplementationafterthepaper’sacceptance.\n3.2.1 ConsolidatingtheACS. Overall,consolidatinganACSinvolvesmultipleiterationsof:\n(1) Findingequivalentcodesandmergethemintoone.\n(2) Generateanewlabelanddefinitionformergedcodes.\n(3) Repeattheprocesswiththenewlistofcodes,untilnothingmoreismerged.\nFindingequivalentcodesismorecomplicatedthanitseems.Humanormachinecodersoftenusedifferentphrasesto\ndescribethesameorsimilarideas.Forexample,supposecoderAidentified\"usersuggestion,\"whilecoderBfound\"user\nsuggestions.\"Theyareclearlyreferringtothesameidea.SupposecoderBfound\"userfeedback\"instead.AreAandB\nreferringtoequivalent,similar,ordifferentidea(s)?Todeterminethat:\n(1) Weencodeeachcodewithtextembedding,transformingits𝐿𝑎𝑏𝑒𝑙 and𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛(𝑠)intoahigh-dimensional\nvector.\n(2) Weusecosinedistance,acommonlyusedtextsimilaritymeasure,tocalculatethedistancebetweencodes.\n(3) Weuseahierarchicalclusteringalgorithmtochoosemergingcandidates,sinceitdoesnotspecifythe\nexpectednumberofresults.Foreachnodeinthealgorithm’sdendrogramtreestructure,weapplytwoinput\ndistancethresholds:𝑙𝑜𝑤𝑒𝑟 and𝑢𝑝𝑝𝑒𝑟.\n(a) Differentcodes:Differentcodeswithadistanceabovethe𝑢𝑝𝑝𝑒𝑟 thresholdwillneverbemerged.\n(b) Equivalentcodes:Verysimilarcodeswithadistancebelowthe𝑙𝑜𝑤𝑒𝑟 thresholdwillalwaysbemerged.\n(c) Forcodeswithadistancebelowthe𝑢𝑝𝑝𝑒𝑟 butabovethe𝑙𝑜𝑤𝑒𝑟,thealgorithmpenalizes21)theproportion\nofnon-overlappingexamples;and2)thesizeofuniqueexamplesaftermerging(comparedwiththeaverage\nexamplesizesofallcodes).\n(i) Alsoequivalentcodes:Ifthedistanceisbelowthe𝑢𝑝𝑝𝑒𝑟 afterthepenalty,thecodeswillbemerged.\n(ii) Similarcodes:Ifthisisthelastiteration,thosecodesbecome𝑁𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠toeachother.\n3.2.2 ImplementingtheConsolidation. Weimplementedtheconsolidatingprocessbymergingclosercodesfirstand\nfarthercodeslater.ThepurposeistohelpLLMscreatelessgenerallabelsanddefinitions.Wewilldiscussthechoiceof\ntextembeddingmodelsandparametersin3.3.3.\n(1) Wemergecodeswithexactlythesame𝐿𝑎𝑏𝑒𝑙;\n(2) Weiterativelymergecodeswithverysimilar𝐿𝑎𝑏𝑒𝑙 (𝑢𝑝𝑝𝑒𝑟 =0.35,𝑙𝑜𝑤𝑒𝑟 =0.35).Sincecodesinthisstepare\nusuallyverysimilar,tooptimizethetokenusage,wesimplyusetheshorterone.Iftheinputdoesnotincludea\n𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛,wegenerateoneforit.\n(3) Weiterativelymergecodeswithsimilar𝐿𝑎𝑏𝑒𝑙 and𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛(𝑢𝑝𝑝𝑒𝑟 =0.5,𝑙𝑜𝑤𝑒𝑟 =0.4).\n(4) Weiterativelymergecodeswithsimilar𝐿𝑎𝑏𝑒𝑙and𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛(𝑢𝑝𝑝𝑒𝑟=0.6,𝑙𝑜𝑤𝑒𝑟=0.4).Duringthelastiteration,\nweconsiderallcodeswithdistancesunder𝑢𝑝𝑝𝑒𝑟 tobe𝑁𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠.\n2Theideaofpenaltycomesfromourinitialexploration,wheremergingbydistancealonecouldleadtoundesirableoutcomes.Forexample,assumethe\ncode\"userfeedback\"iscloseenoughto\"solicitingfeedback\"and\"integratingfeedback,\"yetthelattertwoarefarenoughapart.IfanACSstartswiththe\nlattertwocodes,theywillnotbemerged.Yet,byadding\"userfeedback\"totheACS,astraightforwardalgorithmwillmergeallthreecodesintoone,\nlosingthenuancesbetween\"soliciting\"and\"integrating.\"Topreventthis,ouralgorithmpenalizes\"oversized\"mergingresults.Forexample,since\"user\nfeedback\"willlikelyaccumulatetoomanyexamplesaftermergingthe\"solicitingfeedback\"and\"integratingfeedback,\"thedistancethresholdtomerge\nintoitwillbestricter(lower).Toreducemergingsimilarlynamedcodeswithdifferentintentions,thealgorithmalsopenalizesthedifferencesbetween\ncodes’examples.\n8\n,,\nAComputationalMethodforMeasuring“OpenCodes”inQualitativeAnalysis\nEachiterationofthemergingprocessisimplementedasthefollows:\n(1) Wecalculatethepenaltycoefficient:\n𝑝𝑒𝑛𝑎𝑙𝑡𝑦=𝑢𝑝𝑝𝑒𝑟−𝑙𝑜𝑤𝑒𝑟 (1)\n(2) Foreachpairofcodes𝑥,𝑦,wecalculatetheirdistancewithpenalty:\n(a) Wecalculatethecosinedistance:\n𝑥·𝑦\n𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑥,𝑦)=1− (2)\n∥𝑥∥∥𝑦∥\n(b) Whenever𝑙𝑜𝑤𝑒𝑟 <𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑥,𝑦) <𝑢𝑝𝑝𝑒𝑟:\n(i) Wecalculatethepercentageofthedifferencebetweenexamplesofthetwocodes.Then,wesquaredthe\nresulttoreducethepenaltyonsmalldifferences.\n#(𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠[𝑥]∩𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠[𝑦])\n𝑑𝑖𝑓𝑓(𝑥,𝑦)=( )2 (3)\n#(𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠[𝑥]∪𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠[𝑦])\n(ii) Weapplythepenaltyofdifferentexamplesonthedistance.\n𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑥,𝑦)=𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑥,𝑦)+𝑑𝑖𝑓𝑓(𝑥,𝑦)∗𝑝𝑒𝑛𝑎𝑙𝑡𝑦 (4)\n(3) Weusethehierarchicalclusteringalgorithmtoproduceadendogramtreestructurefromthedistancematrix.\n(4) Weiteratethroughthetreestructurefromthetoptothebottom.Eachnodeofthetreerepresentsapotential\nmerge𝑥,withalistofcodestomerge;a𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑥);anditsuniqueexamplesaftermerged𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠(𝑥).\n(a) Wecalculatethepercentageofover-sizing(clampedbetween0%-200%):\n#(𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠[𝑥])\n𝑜𝑣𝑒𝑟𝑠𝑖𝑧𝑒(𝑥)=𝑐𝑙𝑎𝑚𝑝( ,100%,300%)−1 (5)\n𝑎𝑣𝑔(#(𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠))\n(b) Wethencalculateanewmergingthresholdwiththepenalty:\n𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑(𝑥)=𝑢𝑝𝑝𝑒𝑟−(𝑜𝑣𝑒𝑟𝑠𝑖𝑧𝑒(𝑥)∗0.5)2∗𝑝𝑒𝑛𝑎𝑙𝑡𝑦 (6)\n(c) Weonlymergethecodeswhen𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑥) <𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑(𝑥).\n3.2.3 CalculatingtheMetrics. Wenowcalculatethemetricsin3.1.2tomeasureindividualCSPsagainsttheconsolidated\nACS.ToavoidCSPswithmanyredundantcodesgaininganunfairadvantage,whenevertwoormorecodesaremerged\nintheACS,theirCSPcounterpartsarealsoconsideredmerged.SupposecoderAidentifiedtenvariationsofthesame\nconcept\"usersuggestion\",ourmethodwilltreatthemasonlyonecodeaslongastheyaredetectedandmerged.\nFirst,wecalculatethe𝑤𝑒𝑖𝑔ℎ𝑡 ofeachcodeusingthesumofindividualCSP’scoverage𝑣𝑎𝑙𝑢𝑒forthiscode.The\ncoverage𝑣𝑎𝑙𝑢𝑒is1forCSPthathasthecode,orapercentagebasedonhowmany𝑛𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠theCSPincludescompared\nwiththetotalnumberof𝑛𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠 ofthecode.Forexample,iftwoCSPscoveredthesamecodeandthethirdCSP\ncoveredtwooutoffourneighborsbutnotthecodeitself,the𝑣𝑎𝑙𝑢𝑒wouldbe1,1,0.5.The𝑤𝑒𝑖𝑔ℎ𝑡 wouldbe2.5.\n𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝,𝑐𝑜𝑑𝑒)=1,𝑖𝑓𝑐𝑠𝑝 ∈𝑜𝑤𝑛𝑒𝑟𝑠(𝑐𝑜𝑑𝑒)\n#(𝑐𝑜𝑑𝑒 ∈𝑜𝑤𝑛𝑒𝑑𝑛𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠) (7)\n=\n#(𝑐𝑜𝑑𝑒 ∈𝑛𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠)\n9\n,,\nChen,etal.\n∑︁\n𝑤𝑒𝑖𝑔ℎ𝑡(𝑐𝑜𝑑𝑒)=𝑣𝑎𝑙𝑢𝑒(𝑎𝑐𝑠,𝑐𝑜𝑑𝑒)= 𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝,𝑐𝑜𝑑𝑒) (8)\n𝑐𝑠𝑝∈𝑎𝑐𝑠\nThen,wecalculatethecomputationalmetricsweightedbythe𝑣𝑎𝑙𝑢𝑒ofeachcode.ForeachCSP:\n(1) CoveragemeasurestheproportionoftheCSP’stotalcode𝑣𝑎𝑙𝑢𝑒againsttheACS’stotalcode𝑤𝑒𝑖𝑔ℎ𝑡;\n∑︁\n𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒_𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝)= 𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝,𝑐𝑜𝑑𝑒) (9)\n𝑐𝑜𝑑𝑒∈𝑎𝑐𝑠\n𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒_𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝)\n𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒(𝑐𝑠𝑝)= (10)\n𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒_𝑣𝑎𝑙𝑢𝑒(𝑎𝑐𝑠)\n(2) DensitymeasurestheCSP’srelativedensity(i.e.howmanyconsolidatedcodesdoestheCSPincludeforits\ncoverage)againsttheACS;\n#(𝑐𝑜𝑑𝑒 ∈𝑐𝑠𝑝)\n𝑑𝑒𝑛𝑠𝑖𝑡𝑦_𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝)= (11)\n𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒_𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝)\n𝑑𝑒𝑛𝑠𝑖𝑡𝑦_𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝) #(𝑐𝑜𝑑𝑒 ∈𝑐𝑠𝑝)\n𝑑𝑒𝑛𝑠𝑖𝑡𝑦(𝑐𝑠𝑝)= = (12)\n𝑑𝑒𝑛𝑠𝑖𝑡𝑦_𝑣𝑎𝑙𝑢𝑒(𝑎𝑐𝑠) #(𝑐𝑜𝑑𝑒 ∈𝑎𝑐𝑠)∗𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒_𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝)\n(3) NoveltymeasurestheproportionoftheCSP’stotalnovelcode𝑣𝑎𝑙𝑢𝑒againsttheACS’stotalnovelcode𝑤𝑒𝑖𝑔ℎ𝑡.\nInthisstudy,weused𝑁𝑜𝑣𝑒𝑙𝑡𝑦_𝑇ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 =1,i.e.acodeisnovelifonlyonecodebookexplicitlycontainsit;\n𝑛𝑜𝑣𝑒𝑙𝑡𝑦(𝑐𝑠𝑝)=\n(cid:205) 𝑐𝑜𝑑𝑒∈𝑐𝑠𝑝(𝑛𝑜𝑣𝑒𝑙=1)𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝,𝑐𝑜𝑑𝑒)\n(13)\n(cid:205) 𝑐𝑜𝑑𝑒∈𝑎𝑐𝑠(𝑛𝑜𝑣𝑒𝑙=1)𝑣𝑎𝑙𝑢𝑒(𝑎𝑐𝑠,𝑐𝑜𝑑𝑒)\n(4) DivergenceiscalculatedastheseperationbetweentheCSPandtheACS’sprobabilitydistribution.Tomeasure\nthat,wenormalizedtheCSPandtheACS’svalues(orweights)intolists.SinceindividualCSPsmaycompletely\nmissacodeanditsneighbors,wechosetheJenson-ShannonDivergence(JSD)totoleratetheresultingzero\nelements.Wereporteditsmetricversion,Jenson-ShannonDistance,bytakingasquareroot(citationhere).\n𝑑𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒(𝑐𝑠𝑝)=√︁𝐽𝑆𝐷(𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛(𝑐𝑠𝑝)||𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛(𝑎𝑐𝑠)\n(14)\nACS’snetworkstructureenablesthedetectionofcodeclusters(orcommunities,differentfromtheclustering\nalgorithmweusedformergingcodes)andamorenuancedmeasurement.Sincesomecodesmaynothaveaneighbor,\nwealsoassignlinksfromeachcode’sthreeclosestcounterpartswithareducedweightforcommunitydetectionwith\ntheLouvainalgorithm[8].Forreproducibility,wereportcommunitiesidentifiedwithseed=0.Weapplythemetrics\nwithineachclustertoanswermorenuancedquestions,e.g.,whetherahumanormachinecodercouldhaveoversampled\norundersampledspecificportionsofcodes.\n3.3 StudyDesign\nWeconductedempiricalexperimentsontwoseparateHCIdatasetsandresearchquestionstodemonstratetwouse\ncasesofourmethod.SinceourmeasurementinvolvesGAIingeneratingcodelabelsanddefinitions,wevalidateits\nreliabilityby1)comparingitwithhumanevaluationresults;and2)analyzingitsoutputstability.\n3.3.1 TasksandDatasets. WeexperimentedwithopencodingresultsontwoHCIdatasets,eachwithitsresearch\nquestionandcontext.\n(1) PhysicsLab’sonlinecommunitydataset(127messagesfromthebeginningofthecommunity)betweendesigners\nandteacherusers.Theresearchquestionwas:\"HowdidPhysicsLab’sonlinecommunityemerge?\"Fourhuman\n10\n,,\nAComputationalMethodforMeasuring“OpenCodes”inQualitativeAnalysis\ncoders have previously open-coded the dataset. Three were PhD students, and one was an undergraduate\nassistant.\n(2) TwointerviewsfromaCHI2024study[20],eachlastaroundtwohours.Theresearchquestionwas:\"Inthe\ncontextofNetLogolearningandpractice:Whatperceptions-strengths,weaknesses,andadoptionplans-do\nintervieweesperceiveinLLM-driveninterfaces?Howdotheyuseittosupporttheirwork?Whataretheirneeds\nforLLM-basedinterfaces?\"Threehumancodershavepreviouslyopen-codedthedataset.TwowerePhDstudents,\nandonehadamaster’sdegree.\nWhilebothdatasetsconsistofmanymoreitems,humanexpertsspenttensofhoursfinishingopencodingofthe\nsubset.Ontheotherhand,sincethegoalistoidentifyconcepts\"asexhaustiveaspossible,\"thesmalldatasetenablesus\ntoevaluateML/GAI’spotentialinfew-shotanalyses.\n3.3.2 ChoiceofML/GAICodingApproaches. InCase1,wereplicatedfivepublishedML/GAIapproachesonopencoding\nwithbothdatasets[19].Weprovidedthesameresearchquestionanddatasetinformationforhumansandmachines.We\nassignedthesamerolesandtaskstomachinecoders,suchas“Youareanexpertinthematicanalysiswithgrounded\ntheory,workingonopencoding.”Theexactprompts,parametersandDataset1’ssampleoutputscanbefoundin[19],\norsupplementarymaterialsofthispaper.Here,weprovideanoverviewofthefiveapproaches:\n(1) BERTopic+LLMusestopicmodeling[68],anunsupervisedMLtechniquetoidentifygroupsofsemantically\nsimilarwords(i.e.,topics)andexplainsthetopicswithLLMs.SinceBERTopic’sinstructionwasintendedfor\ngeneral-purposelabel-making,wegaveadditionalinstructionsabouttheresearchquestionandcontexts.\n(2) ChunkLevelasksLLMstoidentifyopencodesfromchunks(e.g.,aconversation,aninterview,etc.)ofdata.\nManyvariantsofthisapproachhavebeenadoptedbyrecentpapers(e.g.,[41,67]).\n(3) ChunkLevelStructuredasksLLMstogeneratemorethanonelevelofconcepts:thefirstfor\"categories\"or\n\"themes,\"thesecondfor\"codes\"or\"subcodes.\"Somerecentpapershavestartedtoadoptthisapproach[19,61].\n(4) ItemLevelasksLLMstoconductline-by-linecoding,assuggestedbythegroundedtheoryliterature[34].Afew\npapershaveadoptedthisapproachtogenerateone[61]ormultiplecodes[19]perline.\n(5) ItemLevelwithVerbPhrasesbuildsonthepreviousapproachbutinstructsLLMstouseverbphrasesfor\nlabelsexplicitly.Thedesignwasinspiredbyagroundedtheoryliterature[25]andreportedbyarecentstudy[19].\nTable1presentsanexcerptfromDataset1,codedbyfourhumanandfivemachineapproaches.Allmachinecoders\nweredrivenbythesamemodel(GPT-4o-0513)withthesametemperature(0.5).\n3.3.3 ChoiceofModelsandHyperparameters. Ourcomputationalmethodutilizestextembeddingandgeneration\nmodelsduringevaluation.Fortextembeddingmodels,weconsultedtheMTEBleaderboardbetweenMayandJune\n2024andexperimentedwithafewalternatives.Forthisstudy,weusedgecko-768fromGoogleduetoitsrelativelyhigh\nperformance,lowdimensions(withbettercomputationalefficiency),andeasyaccessibility.\nWeusedcosinedistanceforthresholdsin3.2.2,where0meansthetwocodesareidentical;1fornocorrelation;and\n2forabsolutelydifferent.Duringourinitialexperiments,wesetthethresholdsof𝑙𝑜𝑤𝑒𝑟 =0.4and𝑢𝑝𝑝𝑒𝑟 =0.6basedon\ntwoindicators:1)wereferredtothedistributionofpairwisedistancesandlocatedlocalturningpoints;and2)whether\nthecodepairsunderthecriteriameetourdefinition,wherehumancanrecognizecodescloserthan𝑙𝑜𝑤𝑒𝑟 asthesame\nidea,whilecodesfurtherthan𝑢𝑝𝑝𝑒𝑟 arelikelytobedifferent.Differentresearchcontextsandtextembeddingmodels\nmayrequiredifferentparameters.\n11\n,,\nChen,etal.\nIftherecouldbeanexportfunction,ortheabilitytosaveorimportexperiments,itwould\nUser4232\nbeconvenient.Wecouldsetuptheparametersintheofficeanddirectlyimporttheminclass.\nBERTopic+LLM featurerequestsforphysicsexperiments\nChunkLevel communityfeedbackloop,contextofuse,participatorydesign,userfeedbackandsuggestions\nChunkStructured userfeedback,practicalapplication,usabilityimprovements,featurerequests\nItemLevel featurerequest,usabilityimprovement,classroomapplication,import/exportfunctionality\nItemVerbPhrases proposeadditionalfeatures,emphasizeconvenience,suggestpracticalusecase\n4HumanCoders communityfeedback,featurerequest,proposeanotherfeature,givesreasonforit\nDesigner Therewillbe.\nBERTopic+LLM futureplanninganddevelopment\nChunkLevel designerresponsiveness\nChunkStructured designerresponse,acknowledgmentandimplementation\nItemLevel designerconfirmation,featureimplementation,futureplanning\nItemVerbPhrases confirmfutureimplementation,validateuserrequest,planfeaturedevelopment\n4HumanCoders developerresponse,acknowledgement,acceptanceofrequest,promisingitwillrealize\nDesigner Doestheclasshaveinternet?\nBERTopic+LLM technicalandinfrastructuralchallengesineducationalsettings\nChunkLevel contextualconstraints\nChunkStructured featureupdatesandenhancements,user-designerinteraction\nItemLevel internetavailabilityinquiry,classroomsetup,technicalconsideration\nItemVerbPhrases inquireaboutclassroomconditions,gathercontext,considertechnicalrequirements\n4HumanCoders engagingwithcommunity,seekingcontext,askfollowupquestiononusagescenario\nGenerallynot.Eversinceanadultimagepoppedupduringamajorcity-levelopenclass,\nUser4232\ntheschoolhasdisabledthenetworkonclassroomcomputers[Emoji].\nBERTopic+LLM technicalandinfrastructuralchallengesineducationalsettings\nChunkLevel contextofuse,contextualconstraints\nChunkStructured featureupdatesandenhancements,user-designerinteraction\nItemLevel internetrestriction,classroomenvironment,securityconcern,pastincident\nItemVerbPhrases explainlackofinternet,providecontext,sharepastincident\ncontextualizingresponse,humor,personalanecdote,sharinginformationfordesign,\n4HumanCoders\nstorysharing,givesananswer,explainstheanswer\nTable1. AnexampleexchangebetweenateacheruserandadesignerfromourDataset1,codedbyfivemachinecodersandfour\nhumancoders.Welightlymergedverysimilarcodes(e.g.,askaquestionvs.question)fromthefourhumancoderstosavespace.\nTounderstandhowthechoiceofmodelandtemperaturemayinfluencetheevaluationprocess,weusedGPT-4o-0513\n(0513),GPT-4o-mini,andLlama3-70Bbecause:1)GPT-4o-0513wasoneofthemostpowerfulmodels(intermsof\nstate-of-the-artevaluationbenchmarks)atthetimeoftheexperiment;2)GPT-4o-miniwasoneofthemostpotent\nsmallermodels;3)Llama3-70Bwasoneofthemostpotentopen-sourcemodels.\n3.3.4 ExperimentDesign. Weconductedtwocasestudiestounderstandtworesearchquestions:\n• WhatcanwelearnaboutexistingML/GAIapproachesforopencoding?\n• Isourmethodstatisticallyreliableenoughtoevaluateopencodesfrommachineandhumancoders?\nCase1examinesthefivemachinecodingapproachesusing1)theoverallmetricsformachineandhumancoders;and\n2)thecluster-levelmetricsofrelativecoverage,ametricbetween-100%(completelyundersampled)to+inf(extremely\noversampled).Twohumanresearchersindependentlyinterpretedthethemeforeachnetworkcluster(see3.2.3)based\nonitsconstituentcodesandreconciledtheirdifferencesintoasinglelabel.\n12\n,,\nAComputationalMethodforMeasuring“OpenCodes”inQualitativeAnalysis\n𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒(𝑐𝑠𝑝,𝑐𝑙𝑢𝑠𝑡𝑒𝑟)\n𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒 𝑟𝑒𝑙𝑎𝑡𝑖𝑣𝑒(𝑐𝑠𝑝,𝑐𝑙𝑢𝑠𝑡𝑒𝑟)=\n𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒(𝑐𝑠𝑝)\n−100% (15)\nCase2evaluatedsixmainstreamLLMs’performanceinopencodingwiththesameprompt:GPT-3.5-turbo;GPT-4o-\n0513;Llama3-70B;Mixtral8x22b;Claude3-haiku;andClaude3.5-sonnet.3Tosavespace,weonlyreportthemetrics\nfromItem-levelVerbPhrases,thebest-performingGAIapproach.Themeasurementincludesallmachineandhuman\ncoders’results,enablingcomparisonsbetweenLLMs;andbetweentheaggregatedresultsofLLMsandhumans.\nThereliabilitystudyevaluatedourmeasurement’soutcomestabilitybasedonCase1.Werepeatedthemeasurement\nwiththreemodels,fivetemperatures(0,0.25,0.5,0.75,1).Thisresultsin15combinationsofLLMandtemperature.For\neachcombination,werepeated10independentruns,totaling150runs.WerecordedtheevaluatorLLM,temperature,\nrunnumber,coderidentity,andfourmetrics.Withthose,weevaluated:\n• Thecoefficientofvariation𝑐𝑜𝑣 foreachmetric,eachCSP,andeachcombinationofLLMandtemperature.On\neachdataset,weranfixed-effectsregressionstounderstandifthechoiceofLLM,temperature,orCSP’s𝑚𝑒𝑎𝑛\nvalueofmetricinfluencedthe𝑐𝑜𝑣.\n• WhetherusingdifferentcombinationofLLMandtemperatureimpactspairwisecomparisons.On\neachdataset,weranaseriesofpairwiseANOVAexperimentsandrecordedsignificantpairson1)theentire\noutputfrom150runs;2)eachofthe15combinations.Wecalculatedtheintersectionofallpairsandlookedfor\nanyindividualcombinationthatwouldproducedifferentconclusionsthantheentirety.Forexample,suppose\nwefoundtheItem-Levelapproach’scoveragesignificantlyhigherthantheChunk-Levelapproach.Ifweonly\nevaluatedwithoneLLMandonetemperature,willtheconclusionchange?\nSincewefoundthechoiceofLLMandtemperaturetohavenosignificantimpactonthepairwisecomparison\noutcome,weonlyconducted10evaluationrunswithLlama3-70Bat0.5temperatureforCase2.\n3.4 EmpiricalResults\n3.4.1 Case1:EvaluatingMachineCodingApproaches. Figure2,3demonstrates𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒and𝑑𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒,twoofthe\nmorestablemetrics(see3.4.3),forallmachineandhumancodersinbothdatasets.Wenoted:\n• Similartoaprevioushumanevaluation[19],wefoundthatitem-levelcodingapproacheshavehighercoverage\nandlowerdivergencethanchunk-levelandBERTopicapproaches.\n• Whiletheaggregationofhumanresearchershashighercoverageandlowerdivergencethananysinglemachine\ncodingapproachincoverageandmostindivergence,theaggregationofmachinecodershashighercoverageand\nlowerdivergencethaneachandallhumanresearchers.Notethatwedonotclaimthatmachinesoutperformed\nhumans,andanongoingstudyiscurrentlyexaminingtheimplications.\n• Notethatthenumbersofcodesdonotstrictlycorrelatewiththecoveragemetric.Forexample,inDataset1,the\nItem-LevelVerb-Phraseapproachfound282rawcodeswith79.1%coverage,whiletheaggregationofhuman\ncodersfound340rawcodeswithonly75.5%.\nWefurtherexploredthepotentialbiasesofhumanandmachinecoderswithcluster-levelmetrics.Tables2,3presents\nclustersforbothdatasets’ACSandtherelativecoverageofeachCSP.Clustersaresortedbythesumoftheircomponent\ncodes’weights.Thus,clusterswithmorecodesandhigherconsensuslevelsarelistedfirst.Numbershigherthan\n3ThelistdoesnotincludeGPT-4o-mini,releasedafterwecompletedthecodingtask.\n13\n,,\nChen,etal.\nFig.2. Thecoveragemetricforallcodingapproaches,Dataset1(withfourhumans)and2(withthreehumans).Forgrounded\ntheoryopencoding,higherisbetter.\nFig.3. Thedivergencemetricforallcodingapproaches,Dataset1(withfourhumans)and2(withthreehumans).\n0representoversamplingofcodes,whilenumberslowerthan0representundersampling.Thestandarddeviation\nindicatesthedegreeofimbalancebetweenclusters.\nWeimmediatelynotedthatalmostallmachineandhumancodershadcompletelymissedsomeclustersfromthe\ndata.Acrossthetwodatasets,theonlyexceptionisItem-levelLLMcodingwithverbphrases.Italsohasthelowest𝑠𝑡𝑑\nofrelativecoverageacrossallindividualcoders,showingitscomparativelyuniformsamplingacrossallcodeclusters\ncomparablewiththeaggregationofallhumancoders.Examiningthequalitativenatureofmissedorundersampled\n14\n,,\nAComputationalMethodforMeasuring“OpenCodes”inQualitativeAnalysis\nID # BERTopic Chunk Chunk-S Item Item-V AllAI AllHuman\n1 34 +2.30% +9.90% +13.80% +26.40% +12.00% +6.60% +14.40%\n2 45 +99.10% +16.00% -43.10% +12.10% -1.50% +3.80% -12.20%\n3 26 +73.10% +59.20% +58.20% +15.20% +15.20% +3.30% +25.80%\n4 28 +1.40% +62.80% +46.00% -5.00% +13.60% +2.20% +12.20%\n5 26 -75.00% +29.50% +44.90% -17.20% +3.10% -3.80% +8.30%\n6 35 -86.20% -65.80% -65.90% -19.90% -40.50% -13.50% -2.70%\n7 20 +12.40% +36.50% +17.50% +28.10% +13.40% +9.30% -6.50%\n8 23 -14.20% -80.50% -37.90% -1.80% +0.00% -4.50% -6.30%\n9 20 +1.80% -55.00% -19.30% +12.50% -14.80% -1.10% -19.40%\n10 16 +20.20% +31.90% +41.70% -1.00% -9.60% +5.40% -19.70%\n11 16 -100.00% -70.40% +30.30% -40.70% +10.10% -0.50% +5.10%\n12 16 +116.10% -20.40% -100.00% -42.40% -27.10% -18.90% -8.30%\n13 11 -100.00% -48.30% -7.10% -2.00% +11.30% +6.60% +5.90%\n14 14 -100.00% -100.00% -100.00% -54.60% -32.60% -3.80% -64.70%\n15 10 -100.00% -100.00% -80.70% -54.60% -5.20% +1.70% -55.90%\n16 10 -100.00% -100.00% -100.00% -50.50% -42.50% -29.40% -51.90%\n17 6 -100.00% -100.00% -100.00% -100.00% -57.90% -63.00% -11.70%\nStd 75.26% 60.30% 59.03% 34.98% 23.10% 17.81% 25.37%\nTable2. ACSofDataset1:ClustersofCodes,andEachCodingApproach’sRelativeCoverage(-100%=CompletelyMissed;+Inf=\nInfinitelyOversampled).\nclustersiscrucialtounderstandingpotentialbiasfurther.Itallowsresearcherstosituatetheanalysisinthecontextof\nthedataandresearchquestions.Below,webrieflyinterprettheclustersmissedbyeachapproach:\n• BERTopic+LLM.ThecoverageofBERTopicislow(21.1%)andhighlyunevenlydistributedamongclusters.\nTheseeminglyoversamplingoncluster12(designers’reactionstousersuggestions)wasonlybecauseofthelow\nbaselinenumber,asitonlyincludedtwo(plusoneneighbor)outof14codes.Moreover,BERTopicmissesclusters\n13-17,whereothercodersnotedmanydetailsaboutthedesignexchanges,suchasneeds,designconsiderations,\nandcommunicationstrategies.\n• Chunk-levelapproaches.Bothapproacheshaverelativelylowcoverage(39.2%,43.2%).Thestructuredapproach\nhasslightlybettercoverage,buttheoverallunevennesslevelwithinclustersstayssimilar,inadditiontocompletely\nmissingcluster12.Bothstarttocovercluster13(designers’apologies,explanations,andreassurancetousers),yet\nstillmissclusters14-17.Bothapproachesalsooversamplepositiveexpressions(e.g.,cluster3)whileundersampling\nclusterswithnegativeimplications(e.g.,clusters6and12).\n• Item-levelapproaches.Bothapproacheshavehighcoverage(74.7%,76.6%).Theverbphraseapproachhas\nslightlybettercoverageandunevennesslevelandavoidsthebaselineapproach’smissofcluster17(designers’\nresponsestrategies).Thisisnotasmallachievement:allothercodersexceptonehumanmissedthisclusterof6\ncodesaltogether.\nWeobservedsimilartrendsinDataset2,whereBERTopic,chunk-levelLLMcoding,andhumancodersmissedmany\nclustersaroundinterviewees’nuancedreflections(clusters12-19).Moreover,item-levelcodingapproachescontinued\ntodobetterthanotherapproaches.Notably,humancodersmisscluster19(reflectingandcriticizingworkingand\nhelp-seekingcultures)altogether.\n15\n,,\nChen,etal.\nID # BERTopic Chunk Chunk-S Item Item-V AllAI AllHuman\n1 21 -30.10% +8.10% -48.50% +18.60% +14.90% +7.60% +13.00%\n2 26 -53.00% +68.50% +39.50% +1.70% +13.90% -0.40% +23.70%\n3 24 +83.00% -22.80% +43.70% +8.50% -3.00% -2.40% +39.10%\n4 22 +151.50% -62.40% +104.80% +13.00% +4.20% +0.30% +16.30%\n5 20 +134.40% -36.00% +96.10% -7.00% +4.10% -2.80% +14.00%\n6 15 +86.20% +124.90% -38.80% +16.60% -10.20% +4.70% -11.50%\n7 15 -17.50% -100.00% -63.00% +16.50% +3.00% +4.70% +22.20%\n8 16 -55.80% +54.10% -100.00% +1.70% +5.90% -4.40% +12.60%\n9 17 -81.70% +129.60% +56.30% -14.40% -10.90% +0.20% -14.00%\n10 16 -100.00% +49.40% +1.70% -9.50% +4.10% +3.40% -52.00%\n11 12 -36.40% +55.30% +5.70% -11.10% -1.60% -5.30% -6.40%\n12 14 -100.00% -100.00% -48.50% -10.90% -16.10% -2.20% -36.20%\n13 14 -100.00% -100.00% -48.50% -10.90% +11.80% +7.60% -85.80%\n14 16 -100.00% +75.20% -100.00% -51.80% -54.70% -15.10% -42.50%\n15 10 -100.00% -100.00% -100.00% -4.20% -1.60% +7.60% -58.40%\n16 7 -41.10% -100.00% -100.00% -27.40% -18.00% +7.60% -48.00%\n17 7 -100.00% -100.00% -100.00% -81.30% -47.30% -38.50% -33.20%\n18 5 -100.00% -100.00% -46.00% -44.00% -29.70% -23.20% +11.40%\n19 5 -100.00% -100.00% -46.00% -6.60% -12.10% +7.60% -100.00%\nStd 87.17% 87.28% 68.77% 25.24% 18.97% 12.03% 39.81%\nTable3. ACSofDataset2:ClustersofCodes,andEachCodingApproach’sRelativeCoverage(-100%=CompletelyMissed;+Inf=\nInfinitelyOversampled).\n3.4.2 Case2:BenchmarkingMainstreamLLMs. InFigures4and5,weappliedourmethodtobenchmarksixmainstream\nLLMsinmid-2024.Wenoted:\n• Significantgapsindifferentmodels’coverageanddivergence.Inbothdatasets,wefoundGPT-4o,Claude3.5\nSonnet,andLlama3-70BsignificantlyoutperformedGPT-3.5-Turbo,Claude3Haiku,andMixtral8x22B.Without\ndesignatingagroundtruth,ourmethodsroughlydistinguishlarger,more\"powerful\"models(measuredby\ncommonMLbenchmarks,e.g.,MMLU)fromsmallerones.\n• Machinecoders’performanceswerenotalwaysconsistentamongthetwodatasets.Forexample,allmodels\nbutClaude3HaikuhaveseenaperformancedropinDataset2,withClaude3.5Sonnetdroppingalmost12%.\n• Machinecoders’aggregatedresultconsistentlyachievedbettercoverage,density,anddivergencethan\nhumancoders’aggregatedresults,withcoveragesaround95%andsignificantlylowerdivergencesinboth\ndatasets.Evenwhenmostindividualmodels’performancedroppedinDataset2,theperformanceofmachine\ncoders’aggregationalmoststayedthesame.\n• WealsonotedtheperformanceofaggregatedhumancodersdroppinginDataset2.Onepotentialreason:only3\nhumancodersconductedopencodingforDataset2,comparedwith4forDataset2.\n3.4.3 ReliabilityStudy. Figures6,7demonstratetheoutcomevarianceofeachmetricacrossthreeLLMsontwodatasets.\nWenoted:\n• Overall,𝑑𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒 consistentlyhastheloweststandardvariance(𝑠𝑡𝑑,2-3%)andcoefficientofvariance(𝑐𝑜𝑣,\n1.5-5.5%),while𝑛𝑜𝑣𝑒𝑙𝑡𝑦hasthehighest𝑠𝑡𝑑(3-6%)and𝑐𝑜𝑣(8-65%).\n16\n,,\nAComputationalMethodforMeasuring“OpenCodes”inQualitativeAnalysis\nFig.4. ThecoveragemetricforsixLLMs,Dataset1(withfourhumans)and2(withthreehumans).Forgroundedtheoryopen\ncoding,higherisbetter.\nFig.5. ThedivergencemetricforsixLLMs,Dataset1(withfourhumans)and2(withthreehumans).\n• Aftercontrollingthetemperatureandcoderidentities,themodelchoicesoccasionallyhaveasignificant\nbutsmallimpact.However,thereisnorelationshipbetweenthesizesofmodelsandthe𝑐𝑜𝑣impacts,andno\nmodelshaveconsistentlylower𝑐𝑜𝑣.\n• Themodeltemperaturedoesnothaveasignificantimpact.\n• For𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒,𝑑𝑒𝑛𝑠𝑖𝑡𝑦,and𝑛𝑜𝑣𝑒𝑙𝑡𝑦,their𝑐𝑜𝑣 arenegativelycorrelatedwithindividualcoders’𝑚𝑒𝑎𝑛valueofthe\nmetric.Forexample,acoderwithhigher𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒willlikelyhavealower𝑐𝑜𝑣thanacoderwithlower𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒.\n17\n,,\nChen,etal.\nFig.6. CoefficientofVarianceforEachMetricamongLLMs,Dataset1\nFig.7. CoefficientofVarianceforEachMetricamongLLMs,Dataset2\nSincerunning150evaluationrunsforasinglecomparisonstudymaybecost-ineffective,weevaluatedourmethod’s\nreliabilitywhenonlyonemodel,andtemperature,and10evaluationrunsareused.Here,weassumethepairwise\ncomparisonresultsof150runsasthegroundtruth.Acrossall15combinationsonfourmetricsandtwodatasets,weonly\nidentifiedtwocasesoffalsepositivesforapairwithaminusculemeandifferenceof𝑑𝑒𝑛𝑠𝑖𝑡𝑦(BERTopicvs.Chunk-level\nStructured,32.71%vs.32.41%)inDataset1.Besidesthat,weonlyidentifiedfalsenegativesforcomparisonpairswith\nsmallmeandifferences.Inotherwords,unlessthegoalistocomparemetricswithsmallnumericaldifferences,10runs\nwouldbegenerallysufficient.\n3.5 Discussions\n3.5.1 ReliabilityoftheMeasurement. WhileusingGAIinourmethodinevitablyintroducesstochasticityandpotential\nbiases,weestablishedthereliabilityofourcomputationalmeasurementwiththereliabilitystudy(3.4.3).\n18\n,,\nAComputationalMethodforMeasuring“OpenCodes”inQualitativeAnalysis\nOur method’s only source of stochasticity comes from GAI’s generation of code definitions and labels, which\ninfluencesthedownstreamdecisionofcodemerging.Naturally,codingresultswithfewercodeswillbeimpacted\nmore,andourempiricalresultssupportthisassumption.Inplainwords,ourmethodprovidesmorestableoutcomesat\nevaluating“better”codingresultsbutlesssofor“worse”ones.Thisisespeciallytruefor𝑛𝑜𝑣𝑒𝑙𝑡𝑦,whichmeasuresthe\nnumberofcodesthatnooneelsehasidentified.Therefore,weonlyrecommendusing𝑛𝑜𝑣𝑒𝑙𝑡𝑦asaqualitativeindicator\nforpotentialoutliers,whileusing𝑑𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒,𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒,and𝑑𝑒𝑛𝑠𝑖𝑡𝑦forquantitativecomparisons.\nWeconfirmedthatusingasingleLLMandtemperaturedoesnotgenerallyleadtofalsereportsonpairedcomparisons.\nInonlyonecase,10runswithasingleLLMandtemperaturecausedafalsepositiveagainsttheentiretyof150runs.\nMoreover,theresultshowsthatourmethoddoesnotrelyonthemostpowerfulmodels.Open-sourcemodelswiththe\npotentialforlocaldeploymentandbetterprivacyprotection(suchasLlama3-70B)orsmallermodelswithcheapercosts\nandlessecologicalfootprints(suchasGPT-4o-mini)willlikelyworkformostscenarios.\n3.5.2 SuggestionsforUsingML/GAIinInductiveCoding. Case1reachessimilarconclusionsasaprevioushuman\nevaluation[19]:theitem-levelapproachesperformedbestonbothdatasetsandresearchquestions.Thecluster-level\nmetricsenabledustocompareML/GAIcodingapproacheswithmorenuances.BERTopicandchunk-levelapproaches\nhadworseoverallmetrics,missedentireclustersofcodes,andwerelesscapableofidentifyingnuancesofhuman\ninteractions,particularlyoneswithseeminglylessconnectionwiththeresearchquestion.Theyalsooversampled\npositiveemotionsorfeedback,implyingapotentialbiasfortheircodingresults.\nIncontrast,thesuccessofitem-levelapproachessuggeststhepotentialofembeddinghumanprocessesforqualitative\nanalysisintoLLMprompts.WhilewestartedmentioninggroundedtheoryintheChunk-LevelStructuredapproach,its\nperformancewasnotmuchbetterthantheChunk-Level.ThesituationchangedwhenweinstructedLLMstostrictly\nfollowthegroundedtheoryprocess[62]:bycodingthedataitem-by-item,theitem-levelapproachesproducedmuch\nbetterresults.Followingexistingliterature[25],ouradoptionofverbphrasesaslabelsfurtherenabledamorenuanced\ninterpretationofthedata.\nCase2providesaquickassessmentofstate-of-the-artLLMsavailableinmid-2024.Ourfindingsuggeststhatexisting\nNLUbenchmarksforLLMsmaypositivelycorrelatewiththeirperformanceoninductivequalitativecoding.Onthe\notherhand,thesameLLMsmayperformdifferentlyondifferentresearchquestionsanddatasets,necessitatingbroader\nevaluativestudies.Whilewesuggestresearchersusethebestavailablemodel,ourfindingsalsoindicatethefeasibilityof\npowerfulopen-sourcemodelssuchasLlama-370B,particularlyforscenarioswheredataprivacyconcernsarestronger.\nMoreover,wesuggestusingmultiplemodelssimultaneouslyasanevenbetterapproach.Thecombinationofsixmodels\nconsistentlycoveredalmostallcodesidentifiedbyhumanresearchers,evenwhenindividualperformancesvaried\nacrossdatasets.\n3.5.3 Human-AICollaborationinInterpretation. Thiswork’scorecontributionisanovelcomputationalmeasurement\ntounderstand,compare,andevaluateopencodesfrommultiplemachineorhumancoders.Hence,itiscrucialtoshare\nhowweinterpretedthealgorithmicresultsanddiscussscenarioswhereourmeasurementcouldcontributetohuman-AI\ncollaborationinopencodingprocesses.\nThenatureofqualitativeresearchpreventsusfrommakingageneralclaimwithonlytwodatasetsandresearch\nquestions,yetthisismoreofafeaturethanabug.Boththeopencodesandthecorrespondingevaluationarenaturally\nboundedbythedatasets,researchquestions,andperspectivesresearchersadopted.Thus,resultsfromourmeasurement\nmustbeinterpretedincontext.Below,wepresentanexampleflowbasedonTable2onDataset1:\n19\n,,\nChen,etal.\n• AresearcherfirstlookedatthegeneralmetricsofeachCSPinarandomevaluationrunofCase1,whereBERTopic\nhadthehighest𝑑𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒acrossallruns.SincetheresearcherbelievestheACStobereasonablywell-covered\nduetothepresenceofmultiplehumancoders,itseemsthatBERTopicproducedthemostdeviantresults.\n• Here,twopossibilitiesexist:eitherBERTopicidentifiedsomethingnovelandinsightful,oritsimplymissedtoo\nmanycodes.Toevaluatethis,theresearchercheckedthe𝑛𝑜𝑣𝑒𝑙𝑡𝑦metricandfoundBERTopicthelowestagain.\nSoitisunlikelytobethefirstsituation;infact,itonlyhas3codesthatnooneelsefound.Thosecodesareall\nrelatedtoaspectsofthesoftwarefeature,whichisfarfromtheresearchquestionaboutcommunityformation.\n• NotethatBERTopiconlyidentified23codes.Forlow-levelcodingapproachesthatfoundhundredsofcodes,the\nresearchermayneedadditionalsupporttounderstandtheirpotentialbiases.Therefore,theresearcherclustered\ncodesintheACSandcalculatedcluster-levelmetrics.\n• TheresearcherfoundthattheItem-Levelapproachmissesandunder-samplesmoreclustersthantheItem-Level\nVerbPhraseapproach,indicatingahigherriskforpotentialbias.Fromthere,basedontheresearchgoalsand\nquestions,theresearcherqualitativelyinterpretedthetwoapproaches’outcomedifferencesanddecidedtoadopt\ntheItem-LevelVerbPhraseapproachfortherestofthecodingprocess.\nWhile computational measurement is crucial in the process, such interpretations are only possible through a\nqualitative understanding of the codes and clusters. This paper’s human evaluation focuses on the open coding\nprocesses,whereeverygroundedinterpretationiswelcomed.However,whenresearchersconductfurtheranalysissteps,\ndependingontheresearchcontext,acodecouldbecometoonuancedandwithtoofewcases;toobroadandcovers\neverything;irrelevanttothecoreaspectsoftheresearchquestion;orhasbeenreportedmanytimesbefore.Whileour\nconvergencemetricsautomaticallydevalueoutliercodesfromfewcoders,itisstillpossiblethatafew\"well-performed\"\napproachesormodelsactuallycontributelittletothefinalanalysis.Itstillreliesonhumans(potentially,incollaboration\nwithAI)tomaketheseinterpretationsanddecisions,openingupopportunitiesforfurtherresearchworkandinterface\ndesign.\nThepotentialofourevaluationmethodisnotlimitedtomachinecodersalone.InCase1,wepresentedhowindividual\nhumancodersmayhavemissedinsightsfromthedata.Duringourstudyprocess,wefoundthevisualizationofACS’s\nnetworkstructureparticularlyuseful,asitrevealedthedifferentfocusesofhumancodersandfacilitatedfruitful\ndiscussionaroundtheresearchquestion.Movingforward,wewillfocusondesignworkthatcanmaterializethe\nmethod’spotentialinsupportinghumancoders,withorwithoutmachinecodersinACS.\nACKNOWLEDGMENTS\nWewholeheartedlyacknowledgetheintellectualcontributionofthefollowingresearchers:Prof.GolnazArastoopour\nIrgens;Prof.DavidRibes;Prof.ElizabethGerber;Prof.FoadHamidi;Prof.XiLu;andmembersoftheCenterfor\nConnectedLearningandComputer-BasedModeling&TIDALlabatNorthwesternUniversity.Wealsowanttothanka\ngroupofscholarswhogaveinsightfulfeedback:ZilinMaandotheranonymousfriends.Inaddition,weacknowledge\nthecontributionofourresearchintern,LilyYang,whoparticipatedinthedataanalysisofourfirstdataset.\nREFERENCES\n[1] AnneAdams,AnnBlandford,andPeterLunt.2005.Socialempowermentandexclusion:Acasestudyondigitallibraries.ACMTransactionson\nComputer-HumanInteraction12,2(June2005),174–200. https://doi.org/10.1145/1067860.1067863\n[2] AnneAdams,PeterLunt,andPaulCairns.2008.AQualitativeApproachtoHCIResearch.InResearchMethodsforHuman-ComputerInteraction.\nCambridgeUniversityPress.\n20\n,,\nAComputationalMethodforMeasuring“OpenCodes”inQualitativeAnalysis\n[3] JulianAshwin,AdityaChhabra,andVijayendraRao.2023.Usinglargelanguagemodelsforqualitativeanalysiscanintroduceseriousbias.arXiv\npreprintarXiv:2309.17147(2023).\n[4] JenniferAttride-Stirling.2001. Thematicnetworks:ananalytictoolforqualitativeresearch. QualitativeResearch1,3(Dec.2001),385–405.\nhttps://doi.org/10.1177/146879410100100307\n[5] EricP.S.Baumer,DavidMimno,ShionGuha,EmilyQuan,andGeriK.Gay.2017.Comparinggroundedtheoryandtopicmodeling:Extremedivergence\norunlikelyconvergence?JournaloftheAssociationforInformationScienceandTechnology68,6(2017),1397–1410. https://doi.org/10.1002/asi.23786\n_eprint:https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.23786.\n[6] IzhakBerkovich.2018.Beyondqualitative/quantitativestructuralism:Thepositivistqualitativeresearchandtheparadigmaticdisclaimer.Quality&\nQuantity52,5(2018),2063–2077. https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11135-017-0607-3&\ncasa_token=JlBYcYvjqpwAAAAA:Qn6arDr_zncTUy62N2jsyRQgPblLU2nVHRlN6nUkQib4UkbPw4-HgrsOhMi0Bhsz-lFEhcU2DbSDb58oPublisher:\nSpringer.\n[7] AndreaBinghamandPatriciaWitkowsky.2021.DeductiveandInductiveApproachestoQualitativeDataAnalysis.InAnalyzingandInterpreting\nQualitativeResearch:AftertheInterview.SAGEPublications,Inc. https://books.google.com/books?hl=en&lr=&id=0xIoEAAAQBAJ&oi=fnd&pg=\nPA133&dq=deductive+coding+qualitative&ots=1wZ007ufMy&sig=tLe5PscNzbMG_yMvXduwFPOquvY#v=onepage&q=deductive%20coding%\n20qualitative&f=false\n[8] VincentDBlondel,Jean-LoupGuillaume,RenaudLambiotte,andEtienneLefebvre.2008.Fastunfoldingofcommunitiesinlargenetworks.Journal\nofstatisticalmechanics:theoryandexperiment2008,10(2008),P10008.\n[9] RobertBowman,CamilleNadal,KellieMorrissey,AnjaThieme,andGavinDoherty.2023. UsingThematicAnalysisinHealthcareHCIat\nCHI:AScopingReview.InProceedingsofthe2023CHIConferenceonHumanFactorsinComputingSystems.ACM,HamburgGermany,1–18.\nhttps://doi.org/10.1145/3544548.3581203\n[10] VirginiaBraunandVictoriaClarke.2006. Usingthematicanalysisinpsychology. QualitativeResearchinPsychology3,2(Jan.2006),77–101.\nhttps://doi.org/10.1191/1478088706qp063oa\n[11] VirginiaBraunandVictoriaClarke.2013.Successfulqualitativeresearch:Apracticalguideforbeginners.(2013).\n[12] VirginiaBraunandVictoriaClarke.2021.Onesizefitsall?Whatcountsasqualitypracticein(reflexive)thematicanalysis?QualitativeResearchin\nPsychology18,3(July2021),328–352. https://doi.org/10.1080/14780887.2020.1769238\n[13] VirginiaBraunandVictoriaClarke.2022. Conceptualanddesignthinkingforthematicanalysis. QualitativePsychology9,1(Feb.2022),3–26.\nhttps://doi.org/10.1037/qup0000196\n[14] JoyD.Bringer,LynneH.Johnston,andCeliaH.Brackenridge.2004. MaximizingTransparencyinaDoctoralThesis1:TheComplexitiesof\nWritingAbouttheUseofQSR*NVIVOWithinaGroundedTheoryStudy.QualitativeResearch4,2(Aug.2004),247–265. https://doi.org/10.1177/\n1468794104044434\n[15] ZanaBuçinca,MajaBarbaraMalaya,andKrzysztofZGajos.2021.Totrustortothink:cognitiveforcingfunctionscanreduceoverrelianceonAIin\nAI-assisteddecision-making.ProceedingsoftheACMonHuman-computerInteraction5,CSCW1(2021),1–21.\n[16] CourtniByun,PiperVasicek,andKevinSeppi.2024. ChainofThoughtPromptingforLargeLanguageModel-drivenQualitativeAnalysis.In\nProceedingsofthe2024CHIConferenceonHumanFactorsinComputingSystems(CHI’24).AssociationforComputingMachinery.\n[17] LaraCarminati.2018.GeneralizabilityinQualitativeResearch:ATaleofTwoTraditions.QualitativeHealthResearch28,13(Nov.2018),2094–2101.\nhttps://doi.org/10.1177/1049732318788379\n[18] M.ArielCascio,EunlyeLee,NicoleVaudrin,andDarcyA.Freedman.2019.ATeam-basedApproachtoOpenCoding:ConsiderationsforCreating\nIntercoderConsensus.FieldMethods31,2(May2019),116–130. https://doi.org/10.1177/1525822X19838237\n[19] JohnChen,AlexandrosLotsos,LexieZhao,GraceWang,UriWilensky,BruceSherin,andMichaelHorn.2024.PromptsMatter:ComparingML/GAI\nApproachesforGeneratingInductiveQualitativeCodingResults. arXiv:2411.06316[cs.CL] https://arxiv.org/abs/2411.06316\n[20] JohnChen,XiLu,YuzhouDu,MichaelRejtig,RuthBagley,MichaelS.Horn,andUriJ.Wilensky.2024.LearningProgrammingofAgent-based\nModelingwithLLMCompanions:ExperiencesofNovicesandExpertsUsingChatGPT&NetLogoChat.InProceedingsofthe2024CHIConferenceon\nHumanFactorsinComputingSystems.\n[21] NedCooper,TiffanieHorne,GillianHayes,CourtneyHeldreth,MichalLahav,JessSconHolbrook,andLaurenWilcox.2022.ASystematicReview\nandThematicAnalysisofCommunity-CollaborativeApproachestoComputingResearch.InCHIConferenceonHumanFactorsinComputingSystems.\n1–18. https://doi.org/10.1145/3491102.3517716arXiv:2207.04171[cs].\n[22] JulietCorbinandAnselmStrauss.2008. Chapter10/AnalyzingDataforConcepts. InBasicsofQualitativeResearch(3rded.):Techniquesand\nProceduresforDevelopingGroundedTheory.SAGEPublications,Inc.,2455TellerRoad,ThousandOaksCalifornia91320UnitedStates. https:\n//doi.org/10.4135/9781452230153\n[23] JulietCorbinandAnselmStrauss.2008.Chapter14/CriteriaforEvaluation.InBasicsofQualitativeResearch(3rded.):TechniquesandProceduresfor\nDevelopingGroundedTheory.SAGEPublications,Inc.,2455TellerRoad,ThousandOaksCalifornia91320UnitedStates. https://doi.org/10.4135/\n9781452230153\n[24] JulietM.CorbinandAnselmStrauss.1990.Groundedtheoryresearch:Procedures,canons,andevaluativecriteria.Qualitativesociology13,1(1990),\n3–21. Publisher:Springer.\n[25] NatalieR.Davis,ShirinVossoughi,andJohnF.Smith.2020.Learningfrombelow:Amicro-ethnographicaccountofchildren’sself-determination\nassociopoliticalandintellectualaction. Learning,CultureandSocialInteraction24(2020),100373. https://www.sciencedirect.com/science/\n21\n,,\nChen,etal.\narticle/pii/S2210656119302107?casa_token=R60cWsxYBRgAAAAA:JTuwj92trq-YVKvmrCwtjFxlRCgBsFxzdpnKu2WNMa9l2OqmQ8wO_h_\nqvLqL49FXXQAx1gXPqwPublisher:Elsevier.\n[26] StefanoDePaoli.2023.PerforminganInductiveThematicAnalysisofSemi-StructuredInterviewsWithaLargeLanguageModel:AnExploration\nandProvocationontheLimitsoftheApproach.SocialScienceComputerReview0,0(Dec.2023),1–23. https://doi.org/10.1177/08944393231220483\n[27] StefanoDePaoliandWalterSMathis.2024.Reflectionsoninductivethematicsaturationasapotentialmetricformeasuringthevalidityofan\ninductivethematicanalysiswithLLMs.Quality&Quantity(2024),1–27.\n[28] JenniferFeredayandEimearMuir-Cochrane.2006.DemonstratingRigorUsingThematicAnalysis:AHybridApproachofInductiveandDeductive\nCodingandThemeDevelopment.InternationalJournalofQualitativeMethods5,1(March2006),80–92. https://doi.org/10.1177/160940690600500107\n[29] NickJFox.2008.Post-positivism.TheSAGEencyclopediaofqualitativeresearchmethods2,1(2008),659–664.\n[30] DominicFurniss,AnnBlandford,andPaulCurzon.2011.ConfessionsfromagroundedtheoryPhD:experiencesandlessonslearnt.InProceedingsof\ntheSIGCHIConferenceonHumanFactorsinComputingSystems.ACM,VancouverBCCanada,113–122. https://doi.org/10.1145/1978942.1978960\n[31] JieGao,KennyTsuWeiChoo,JunmingCao,RoyKa-WeiLee,andSimonPerrault.2023.CoAIcoder:ExaminingtheEffectivenessofAI-assisted\nHuman-to-HumanCollaborationinQualitativeAnalysis.ACMTransactionsonComputer-HumanInteraction31,1(Nov.2023),6:1–6:38. https:\n//doi.org/10.1145/3617362\n[32] SusanGasson.[n.d.]. RigorInGroundedTheoryResearch:AnInterpretivePerspectiveonGeneratingTheoryFromQualitativeFieldStudies.\n([n.d.]).\n[33] SimretArayaGebreegziabher,ZhengZhang,XiaohangTang,YihaoMeng,ElenaL.Glassman,andTobyJia-JunLi.2023. PaTAT:Human-AI\nCollaborativeQualitativeCodingwithExplainableInteractiveRuleSynthesis.InProceedingsofthe2023CHIConferenceonHumanFactorsin\nComputingSystems(CHI’23).AssociationforComputingMachinery,NewYork,NY,USA,1–19. https://doi.org/10.1145/3544548.3581352\n[34] GrahamR.Gibbs.2007.Thematiccodingandcategorizing.Analyzingqualitativedata703,38-56(2007). https://study.sagepub.com/sites/default/\nfiles/analyzing-qualitative-da.pdf\n[35] BarneyGGlaser,JudithHolton,etal.2004.Remodelinggroundedtheory.InForumqualitativesozialforschung/forum:qualitativesocialresearch,\nVol.5.\n[36] MaartenGrootendorst.2022.BERTopic:Neuraltopicmodelingwithaclass-basedTF-IDFprocedure.arXivpreprintarXiv:2203.05794(2022).\n[37] EgonGGubaandYvonnASLincoln.1994.CompetingParadigmsinQualitativeResearch.InHandbookofqualitativeresearch.SagePublications,\nInc.,105–117.\n[38] GregGuest,EmilyNamey,andMarioChen.2020.Asimplemethodtoassessandreportthematicsaturationinqualitativeresearch.PloSone15,5\n(2020),e0232076. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0232076Publisher:PublicLibraryofScienceSanFrancisco,CA\nUSA.\n[39] LeahHamilton,DeshaElliott,AaronQuick,SimoneSmith,andVictoriaChoplin.2023. ExploringtheUseofAIinQualitativeAnalysis:A\nComparativeStudyofGuaranteedIncomeData. InternationalJournalofQualitativeMethods22(March2023),16094069231201504. https:\n//doi.org/10.1177/16094069231201504Publisher:SAGEPublicationsInc.\n[40] JasySuetYanLiew,NancyMcCracken,ShichunZhou,andKevinCrowston.2014.OptimizingFeaturesinActiveMachineLearningforComplex\nQualitativeContentAnalysis.InProceedingsoftheACL2014WorkshoponLanguageTechnologiesandComputationalSocialScience,Cristian\nDanescu-Niculescu-Mizil,JacobEisenstein,KathleenMcKeown,andNoahA.Smith(Eds.).AssociationforComputationalLinguistics,Baltimore,\nMD,USA,44–48. https://doi.org/10.3115/v1/W14-2513\n[41] SaríahLopez-FierroandHaNguyen.2024.MakingHuman-AIContributionsTransparentinQualitativeCoding.(2024). https://repository.isls.org/\n/handle/1/10537Publisher:InternationalSocietyoftheLearningSciences.\n[42] AndrewLowe,AnthonyC.Norris,A.JaneFarris,andDuncanR.Babbage.2018.QuantifyingThematicSaturationinQualitativeDataAnalysis.\nFieldMethods30,3(Aug.2018),191–207. https://doi.org/10.1177/1525822X17749386\n[43] PennyMackieson,AronShlonsky,andMarieConnolly.2019.Increasingrigorandreducingbiasinqualitativeresearch:Adocumentanalysisof\nparliamentarydebatesusingappliedthematicanalysis.QualitativeSocialWork18,6(Nov.2019),965–980. https://doi.org/10.1177/1473325018786996\n[44] NoraMcDonald,SaritaSchoenebeck,andAndreaForte.2019.ReliabilityandInter-raterReliabilityinQualitativeResearch:NormsandGuidelines\nforCSCWandHCIPractice.ProceedingsoftheACMonHuman-ComputerInteraction3,CSCW(Nov.2019),1–23. https://doi.org/10.1145/3359174\n[45] JanetMcGawandAlasdairVance.2023.Dissonance,Disagreement,Difference:ChallengingThematicConsensustoDecoloniseGroundedTheory.\nInternationalJournalofQualitativeMethods22(Jan.2023),16094069231220775. https://doi.org/10.1177/16094069231220775\n[46] JaneMills,AnnBonner,andKarenFrancis.2006.Thedevelopmentofconstructivistgroundedtheory.Internationaljournalofqualitativemethods5,\n1(2006),25–35.\n[47] AlirezaMoghaddam.2006.Codingissuesingroundedtheory.IssuesInEducationalResearch16(2006). https://www.iier.org.au/iier16/moghaddam.\nhtml\n[48] MichaelMuller,ShionGuha,EricP.S.Baumer,DavidMimno,andN.SadatShami.2016. MachineLearningandGroundedTheoryMethod:\nConvergence,Divergence,andCombination.InProceedingsofthe2016ACMInternationalConferenceonSupportingGroupWork(GROUP’16).\nAssociationforComputingMachinery,NewYork,NY,USA,3–8. https://doi.org/10.1145/2957276.2957280\n[49] LorelliS.Nowell,JillM.Norris,DeborahE.White,andNancyJ.Moules.2017.ThematicAnalysis:StrivingtoMeettheTrustworthinessCriteria.\nInternationalJournalofQualitativeMethods16,1(Dec.2017),160940691773384. https://doi.org/10.1177/1609406917733847\n22\n,,\nAComputationalMethodforMeasuring“OpenCodes”inQualitativeAnalysis\n[50] StevenPace.2004.Agroundedtheoryoftheflowexperiencesofwebusers.InternationalJournalofHuman-ComputerStudies60,3(March2004),\n317–363. https://doi.org/10.1016/j.ijhcs.2003.08.005\n[51] AngelinaParfenovaetal.2024.AutomatingQualitativeDataAnalysiswithLargeLanguageModels.InProceedingsofthe62ndAnnualMeetingofthe\nAssociationforComputationalLinguistics(Volume4:StudentResearchWorkshop).177–185.\n[52] MdShidurRahman.2016.TheAdvantagesandDisadvantagesofUsingQualitativeandQuantitativeApproachesandMethodsinLanguage“Testing\nandAssessment”Research:ALiteratureReview.JournalofEducationandLearning6,1(Nov.2016),102. https://doi.org/10.5539/jel.v6n1p102\n[53] MaryamN.RazaviandLeeIverson.2006.Agroundedtheoryofinformationsharingbehaviorinapersonallearningspace.InProceedingsofthe2006\n20thanniversaryconferenceonComputersupportedcooperativework.ACM,BanffAlbertaCanada,459–468. https://doi.org/10.1145/1180875.1180946\n[54] DavidRibes.2019. HowILearnedWhataDomainWas. ProceedingsoftheACMonHuman-ComputerInteraction3,CSCW(Nov.2019),1–12.\nhttps://doi.org/10.1145/3359140\n[55] TimRietzandAlexanderMaedche.2021. Cody:AnAI-BasedSystemtoSemi-AutomateCodingforQualitativeResearch.InProceedingsof\nthe2021CHIConferenceonHumanFactorsinComputingSystems(CHI’21).AssociationforComputingMachinery,NewYork,NY,USA,1–14.\nhttps://doi.org/10.1145/3411764.3445591\n[56] JohnHamonSalisburyandTomCole.[n.d.].GroundedTheoryinGamesResearch:MakingtheCaseandExploringtheOptions.([n.d.]).\n[57] SinaMahdipourSaravani,SadafGhaffari,YanyeLuther,JamesFolkestad,andMarciaMoraes.2023.AutomatedCodeExtractionfromDiscussion\nBoardTextDataset.InAdvancesinQuantitativeEthnography,CrinaDamşaandAmandaBarany(Eds.).Vol.1785.SpringerNatureSwitzerland,\nCham,227–238. https://doi.org/10.1007/978-3-031-31726-2_16SeriesTitle:CommunicationsinComputerandInformationScience.\n[58] BenjaminSaunders,JuliusSim,TomKingstone,ShulaBaker,JackieWaterfield,BernadetteBartlam,HeatherBurroughs,andClareJinks.2018.\nSaturationinqualitativeresearch:exploringitsconceptualizationandoperationalization. Quality&Quantity52,4(July2018),1893–1907.\nhttps://doi.org/10.1007/s11135-017-0574-8\n[59] NikhilSharma,QVeraLiao,andZiangXiao.2024.GenerativeEchoChamber?EffectofLLM-PoweredSearchSystemsonDiverseInformation\nSeeking.InProceedingsoftheCHIConferenceonHumanFactorsinComputingSystems.1–17.\n[60] CarsonSievertandKennethShirley.2014.LDAvis:Amethodforvisualizingandinterpretingtopics.InProceedingsoftheworkshoponinteractive\nlanguagelearning,visualization,andinterfaces.63–70.\n[61] RaviSinha,IdrisSolola,HaNguyen,HillarySwanson,andLuEttaMaeLawrence.2024.TheRoleofGenerativeAIinQualitativeResearch:GPT-4’s\nContributionstoaGroundedTheoryAnalysis.InProceedingsoftheSymposiumonLearning,DesignandTechnology.ACM,DelftNetherlands,17–25.\nhttps://doi.org/10.1145/3663433.3663456\n[62] AnselmStraussandJulietCorbin.1998.Basicsofqualitativeresearch:Techniquesandproceduresfordevelopinggroundedtheory,2nded.Basicsof\nqualitativeresearch:Techniquesandproceduresfordevelopinggroundedtheory,2nded.(1998),xiii,312–xiii,312. Place:ThousandOaks,CA,US\nPublisher:SagePublications,Inc.\n[63] GarethTerry,NikkiHayfield,VictoriaClarke,andVirginiaBraun.2017.Thematicanalysis.TheSAGEhandbookofqualitativeresearchinpsychology\n2,17-37(2017),25. https://books.google.com/books?hl=en&lr=&id=AAniDgAAQBAJ&oi=fnd&pg=PA17&dq=Thematic+analysis+terry+&ots=\ndpi2nmHiMV&sig=959tII4BUp9su6Hv2JJui1KjP5QPublisher:SAGEPublicationsLtd.\n[64] NguyenCaoThanhandTTThanh.2015.Theinterconnectionbetweeninterpretivistparadigmandqualitativemethodsineducation.American\njournalofeducationalscience1,2(2015),24–27.\n[65] DavidR.Thomas.2006.AGeneralInductiveApproachforAnalyzingQualitativeEvaluationData.AmericanJournalofEvaluation27,2(June2006),\n237–246. https://doi.org/10.1177/1098214005283748\n[66] AnthonyG.Tuckett.2005.Applyingthematicanalysistheorytopractice:Aresearcher’sexperience.ContemporaryNurse19,1-2(Aug.2005),75–87.\nhttps://doi.org/10.5172/conu.19.1-2.75\n[67] ThomasÜbellacker.2024.AcademiaOS:AutomatingGroundedTheoryDevelopmentinQualitativeResearchwithLargeLanguageModels.arXiv\npreprintarXiv:2403.08844(2024).\n[68] HannaM.Wallach.2006.Topicmodeling:beyondbag-of-words.InProceedingsofthe23rdinternationalconferenceonMachinelearning-ICML’06.\nACMPress,Pittsburgh,Pennsylvania,977–984. https://doi.org/10.1145/1143844.1143967\n[69] YixinWan,GeorgePu,JiaoSun,AparnaGarimella,Kai-WeiChang,andNanyunPeng.2023. \"kellyisawarmperson,josephisarolemodel\":\nGenderbiasesinllm-generatedreferenceletters.arXivpreprintarXiv:2310.09219(2023).\n[70] CarlaWilligandWendyStaintonRogers.2017.TheSAGEhandbookofqualitativeresearchinpsychology.Sage. https://books.google.com/books?hl=\nen&lr=&id=AAniDgAAQBAJ&oi=fnd&pg=PR7&dq=The+SAGE+Handbook+of+Qualitative+Research+in+Psychology&ots=dpi2nmHiK_&sig=\nuyFhaXXsypuO5HjEBxbBLAaNG50\n[71] ZiangXiao,XingdiYuan,Q.VeraLiao,RaniaAbdelghani,andPierre-YvesOudeyer.2023.SupportingQualitativeAnalysiswithLargeLanguage\nModels:CombiningCodebookwithGPT-3forDeductiveCoding.InCompanionProceedingsofthe28thInternationalConferenceonIntelligentUser\nInterfaces(IUI’23Companion).AssociationforComputingMachinery,NewYork,NY,USA,75–78. https://doi.org/10.1145/3581754.3584136\n[72] AndresFelipeZambrano,XinerLiu,AmandaBarany,RyanS.Baker,JuhanKim,andNidhiNasiar.2023.FromnCodertoChatGPT:FromAutomated\nCodingtoRefiningHumanCoding.InAdvancesinQuantitativeEthnography(CommunicationsinComputerandInformationScience),Golnaz\nArastoopourIrgensandSimonKnight(Eds.).SpringerNatureSwitzerland,Cham,470–485. https://doi.org/10.1007/978-3-031-47014-1_32\n[73] FengxiangZhao,FanYu,andYiShang.2024.ANewMethodSupportingQualitativeDataAnalysisThroughPromptGenerationforInductive\nCoding.In2024IEEEInternationalConferenceonInformationReuseandIntegrationforDataScience(IRI).IEEE,164–169.\n23\n,,\nChen,etal.\nReceived14September2023;revised12December2023;accepted19January2024\n24",
    "pdf_filename": "A_Computational_Method_for_Measuring_Open_Codes_in_Qualitative_Analysis.pdf"
}