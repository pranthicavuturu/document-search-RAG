{
    "title": "AComputationalMethodforMeasuringâ€œOpenCodesâ€inQualitativeAnalysis",
    "abstract": "",
    "body": "AComputationalMethodforMeasuringâ€œOpenCodesâ€inQualitativeAnalysis\nJOHNCHEN,NorthwesternUniversity,UnitedStatesofAmerica\nALEXANDROSLOTSOS,NorthwesternUniversity,UnitedStatesofAmerica\nCAIYIWANG,NorthwesternUniversity,UnitedStatesofAmerica\nLEXIEZHAO,NorthwesternUniversity,UnitedStatesofAmerica\nJESSICAHULLMAN,NorthwesternUniversity,UnitedStatesofAmerica\nBRUCEL.SHERIN,NorthwesternUniversity,UnitedStatesofAmerica\nURIJ.WILENSKY,NorthwesternUniversity,UnitedStatesofAmerica\nMICHAELS.HORN,NorthwesternUniversity,UnitedStatesofAmerica\nQualitativeanalysisiscriticaltounderstandinghumandatasetsinmanysocialsciencedisciplines.Opencodingisaninductive\nqualitativeprocessthatidentifiesandinterprets\"opencodes\"fromdatasets.Yet,meetingmethodologicalexpectations(suchas\"as\nexhaustiveaspossible\")canbechallenging.Whilemanymachinelearning(ML)/generativeAI(GAI)studieshaveattemptedtosupport\nopencoding,fewhavesystematicallymeasuredorevaluatedGAIoutcomes,increasingpotentialbiasrisks.BuildingonGrounded\nTheoryandThematicAnalysistheories,wepresentacomputationalmethodtomeasureandidentifypotentialbiasesfrom\"open\ncodes\"systematically.Insteadofoperationalizinghumanexpertresultsasthe\"groundtruth,\"ourmethodisbuiltuponateam-based\napproachbetweenhumanandmachinecoders.WeexperimentwithtwoHCIdatasetstoestablishthismethodâ€™sreliabilityby1)\ncomparingitwithhumananalysis,and2)analyzingitsoutputstability.Wepresentevidence-basedsuggestionsandexampleworkflows\nforML/GAItosupportopencoding.\nAdditionalKeyWordsandPhrases:OpenCoding,QualitativeAnalysis,LargeLanguageModels,Human-AICollaboration,Inductive\nCoding,ThematicAnalysis,GroundedTheory,Measurement\nACMReferenceFormat:\nJohnChen,AlexandrosLotsos,CaiyiWang,LexieZhao,JessicaHullman,BruceL.Sherin,UriJ.Wilensky,andMichaelS.Horn.2024.\nAComputationalMethodforMeasuringâ€œOpenCodesâ€inQualitativeAnalysis.In.ACM,NewYork,NY,USA,24pages.\n1 INTRODUCTION\nQualitativeanalysisistheprocessofsystematicallyidentifying,generating,andorganizingconceptsfromdata.Ithas\nbeenwidelyadoptedinmanysocialsciencedisciplines(suchaseducation,sociology,psychology,andmedicine),as\nwellasinterdisciplinaryareassuchashuman-computerinteraction(HCI)1tounderstandpeopleâ€™sperceptions,feelings,\nandnuancedinteractionswithtechnology[2].\nHowever,qualitativemethodscanbechallenging,time-consuming,andmaylacktransparency[9].Theproblemis\nparticularlyacuteduringtheprocessofopencoding,whereresearchersinductivelyidentifyemergentcodesfrom\nrawdatawithoutapreconceivedcodingscheme.Asthefirststepofqualitativeanalysis,practitionersandtheoristsof\nthematicanalysis(TA)[10,63]andgroundedtheory(GT)[23]makefrequentuseofopencodingandagreeonitsgoal:\n1Forexample,theCHIconferenceitselfhasincreasinglyincludedpapersusingqualitativemethods:36.7%in2021,48.4%in2022,48.9%in2023,53.9%\nin2024.ThisresultcomesfromsearchesintheACMdigitallibrarywiththekeywords:\"thematicanalysis,\"\"qualitativeanalysis,\"\"qualitativecoding,\"\n\"qualitativeresearch,\"\"discourseanalysis,\"and\"groundedtheory.\"\n2024.ManuscriptsubmittedtoACM\n1\n4202\nvoN\n91\n]LC.sc[\n1v24121.1142:viXra\n,,\nChen,etal.\ntocaptureasmanyaspects,patterns,or\"codingmoments\"aspossible.However,thecriteriaofbeing\"open\"and\"as\nexhaustiveaspossible\"areofteninconsistentandsubjectiveinpractice[30,58].\nMachinelearning(ML)andgenerativeAI(GAI)techniqueshavethepotentialtosupportandimprovequalitative\nresearch,butthispotentialhasyettobefullyrealized.Paststudieshavemostlyfocusedondeductivecoding[40,55,71],\nwhereresearcherssystematicallyapplyexistingcodingschemestonewdatasets.However,attemptstoapplyML/GAI\ntoopencodinghavebeenmostlyexploratory.WhiletheHCIliteraturerepeatedlyfoundthathumanstendtoover-rely\nonAIsystemsandcanbeeasilymisledwhenAIsystemsgoastray[15],commercialproviders(suchasAtlas.ti)have\nalreadystartedtoprovideGAIsolutionsforopencoding.Sinceallfutureanalysesrelyonopencodingresults,thereis\napressingneedforsystematicmeasurementmethodsonopencodingresults(or\"opencodes\").\nWeproposeatheory-informedcomputationalmethodtosystematicallymeasureopencodes.Theoristsofthematic\nanalysisandgroundedtheoryadvocateateam-basedapproach[18,24]thatdependsonmultiplecoderstofind\"asmany\nrelevantconceptsorinterpretationsaspossible\"fromthedata[22,24,63].Buildingonthisconcept,wecomputationally\ntransformedeachcoderâ€™sinductivecodesintoaCodeSpace,andcalculatedthesumofallcodespacesasanAggregated\nCodeSpace.Thisenablesmeasuringindividualcodersâ€™resultsagainstateamofcoders,whosecombinedcodesmay\ngetclosertothetheoreticalexpectation.WeproposeandoperationalizefourconceptualmetricsforassessingCode\nSpaces:Coverage,Density,Novelty,andDivergence.\nWedemonstrateourmethodâ€™spotentialasanovelcomputationallenstounderstand,compare,andevaluateopen\ncodes(fromeitherhumanormachinecoders).WorkingontwoHCIdatasets,wecomputationallymeasuredopencodes\ngeneratedbyfivepreviouslypublishedML/GAIapproachesandsixmainstreamGAImodels.Wevalidatethereliability\nofourmethodby1)comparingitsresultswithhumananalysis,and2)statisticallyanalyzingitsoutputstability.By\ncombiningmachinemeasurementandhumaninterpretation,wepresentanexampleofahuman-AIcollaborative\nworkflowtoevaluateopencodes,resultinginevidence-basedsuggestionsforqualitativeresearcherstoadoptGAIin\ninductiveanalysis.\n2 BACKGROUND\n2.1 OpenQualitativeCoding\nQualitativeanalysisenablesanin-depthexplorationofhumanexperiencesbyfocusingonthenuancedinterpretations,\nemotions, and subjective experiences that shape individual and social realities [52]. Thematic Analysis (TA) and\nGroundedTheory(GT),twocommonlyusedqualitativemethodsinHCIresearch[1,2,9,21,48,50,53,54,56],both\nadvocateforopencodingasanessentialfirststeptouncovertheunderlyingreasonsandprocessesthatdrivethe\nformationandtransformationofmeaning[52,62].However,conductingopencodingcanbechallenging,particularly\nwhenscalingup.Ambiguity[4,9,66],inconsistentterminology[12],andinsufficientreporting[14]canallobscurehow\nqualitativeanalysiswasperformed,makingithardertoassessitsquality[9,49].Fewstudieshaveattemptedtomeasure\norevaluateopencodingresultsbasedontheoreticalexpectations,furthercompoundingthechallenge.\n2.1.1 ExpectationsofOpenCoding. OpencodingisakeyapproachinTAandGTthatinductivelyderivescodesfrom\nrawdata.Itcangeneratenewcodingschemeswithoutorinadditiontoatheoreticalframework[24,62].Researchers\ncanthenapplythecodingschemesystematicallyondatasetsthroughthedeductivecodingprocess[7].Withthe\ncapabilityoffindingnovelinsightsbeyondexistingknowledge,GTtheoristsrequireopencoding[22,24,62],andTA\ntheoristsadvocateforit[63].Sincesubsequentanalysisdependsontheopencodingresults,withoutfirstestablishing\n2\n,,\nAComputationalMethodforMeasuringâ€œOpenCodesâ€inQualitativeAnalysis\ntheschemesâ€™relativecompleteness,researchersriskenteringabiasedorunbalancedconceptspacethatwouldharm\ntherigorofqualitativeresearch.Itis,therefore,imperativetoreviewtheexpectationsandchallengesofopencoding.\nManytheoristsofTAandGTagreethatopencodingshouldcaptureasmanyaspects,patterns,or\"codablemoments\"\naspossible[22,24,63].WhileopencodingisnotalwaysrequiredforTA,TAtheoristssuchasBraunandClarke[10,13]\narguethatthefocusofqualitativeassuranceshouldshiftfromaccuracy(i.e.,theconsistencyindeductivecoding)\ntoreflexivity(i.e.,howtheopencodingprocessreflectstheunderlyingdataset).Similarly,comingfromanimplicit\nconstructivistepistemologicalstance,Terryetal.contendthatopencodingoutcomesmaybestrongorweak,but\ncannotbeobjectivelyrightorwrong.ThisshifttowardreflexivityalignswithGTâ€™sinductiveprocess,whichemphasizes\nopennesstonewinsights.GTstressesthatresearchersmustcontinuallylookforemergingideasuntiltheoretical\nsaturationisreached,atwhichpointnofurtherinsightsemerge[24,47].Sincethegoalistodiscoverpotentialideas,\nopencodesdonotneedtobeconsistentorsystematic,andasingleexamplemaysuffice.\n2.1.2 EvaluationofOpenCoding. Derivingpatternsdirectlyfromrawdatawithoutapredefinedframeworkisinherently\nchallenging[43].Thischallengeiscompoundedbytheunderdevelopmentofmeasurementforopencodingandtheoristsâ€™\nfocusonresearcherquality(qualifications,experience,andperspective)overresearchquality[23].Somemetrics,suchas\nvalidityorcredibility,areeasiertojudgebythedataorkeeptrackof[28].Othermetricsaremoreelusive:forexample,\nhowcanweoperationalizetheexpectationsofinductivecoding-being\"open\"and\"asexhaustiveaspossible\"?Even\nwhencodersidentifysomevalidandcrediblecodes,asCorbinandStraussacknowledge,theyareonlysomeofthe\n\"manyplausibleinterpretationspossiblefromdata.\"\nSomescholarshaveattemptedtoadoptdeductivecodingmetricssuchasinter-raterreliability(IRR)inopencoding[44].\nYet,IRRcanonlyassessconsistencybetweencoders.Asqualitativeresearchersmaynothaveorreachtheground\ntruth,evencompleteconsistencycannotmeasurethe\"openness\"or\"exhaustiveness\"ofthecodes.Mostqualitative\nresearcherswhoadoptnon-positivistepistemologicalstancesbelievethat[17]:post-positivismbelievesinobjective\ntruth,yethumansmayonlyapproximateit[29,37];interpretivismarguesthattruthissubjectiveandsituated[64],while\nconstructivismholdsthattruthisahumanconstructiontounderstandreality[37,46].Ontheotherhand,whileafew\nscholarshaveadvocatedforpositivistqualitativeresearchandbelieveingroundtruth[6,70],itisunclearhowsuch\ntruthcanbereliablyidentifiedandvalidated.\nGTtheoristshaveexploredanotherpotentialpathwayofevaluation.CorbinandStraussproposedtwocriteriarelated\ntoinductivecoding:1)depth,whichreferstotherichnessofdescriptivedetailthataddssubstancetofindings,and2)\nvariation,whichdemonstratesthecomplexityofhumanlifebyincorporatingdiversecasesoutsideofdominantpatterns.\nHowever,operationalizingthesecriteriacanbedifficult.Traditionally,GTtheoristssuggestworkingtowardstheoretical\nsaturation,wherenonewinterpretationsemergefromadditionaldata,andexistingonesareallwell-definedwith\nsufficientvariation[23].Yet,inreal-worldresearchcontexts,thelogicfordeterminingsaturationisofteninconsistent\nandsubjective[2,30,58],asitischallengingtojudgewhethercodinghastrulybecomeexhaustive[32],orhassimply\nreachedaconvenientstoppingpoint.\nWhiletheabsolutesaturationmaybeimpractical,TA[18]andGTtheorists[24]advocateforateam-basedapproach.\nAnalyzingdatafrommultipleperspectivesreducesthelikelihoodofmissingkeycodesorover-interpretingdata.By\nconstantlycomparingandcontrastingcodesfromdifferentindividuals[18],researcherscanopenuptheanalysisto\nthescrutinyofotherresearchers[24],resultingin\"newinsights[and]increasedsensitivity\"while\"guardingagainst\nbias.\"Similarly,theGT-inspiredgeneralinductiveapproach[65]suggestsmergingindependentcodersâ€™opencoding\n3\n,,\nChen,etal.\nresultsandcheckingtheproportionofoverlappingcodes.Alowdegreeofoverlapmaynecessitatemorediscussionand\nfurtheranalysis.\nRecently,someTAscholarshaveattemptedtoquantifyâ€œsaturationâ€bycountingthenumberofcodesthatemerged\nthroughoutthecodingprocessuntilthecurveflattens[38,42].However,theworkisbuilton(possiblyoverly)strong\nassumptionsabouttheinductivecodingprocess:1)duplicatedcodesdonotexist,and2)researcherswillconsistently\nâ€œsampleâ€fromallâ€œdiscoverableâ€codes.Whilethislineofworkhaspracticalvaluesforpredictingthenumberof\ninterviewsorsurveysneeded[42],bothassumptionsareunlikelytoholdforhumancodersduringinductivecoding.As\nML/GAImodelsmaybesystematicallybiased(e.g.,[59,69]),theissueisonlyexacerbatedformachineapproaches.\nInformedbytheliterature,wecontributeatheory-drivencomputationalapproachtomeasureinductivecodingresults,\ntakinganimportantsteptowardsaddressingthebroaderconcernsaboutefficiencyandtransparencyinqualitative\nanalysis.ThenextsectionwillreviewexistingML/GAIworkonthistopic.\n2.2 Machine-AssistedQualitativeAnalysis\nManyfields(e.g.,computervision,dataanalytics,etc.)havewidelyusedML/GAImethodstolabeldata.Qualitative\ncoding,whetherinductiveordeductive,canalsomechanicallybeseenasapplyinglabels(i.e.,codes)toagivenpiece\nofdata.ThissectionreviewstwoML/GAIperspectivesformachine-assistedqualitativecoding-classificationand\ngeneration.Whileclassificationnaturallyfitsintodeductivecodingscenarioswithestablishedevaluationmetrics,its\npotentialforinductivecodingisintrinsicallylimited.Generation,ontheotherhand,hasstartedtopickuptractionfor\ninductivecoding.Yet,fewstudieshaveattemptedtoevaluatethem,resultinginsignificantrisksforresearchrigor.\n2.2.1 ML/GAI For Classification. From a computational perspective, most current work views machine-assisted\nqualitativecodingasaclassificationtask,wherealgorithmsaimtoproducecodesassimilarlytohumanresearchersas\npossible.Morerecently,researchershaveworkedonthreeclassificationapproaches:\n(1) SupervisedMLmethodsthataretrainedonhuman-codeddatasets.Liewetal.,forexample,approachmachine-\nassistedqualitativecodingasaâ€œmulti-labelclassificationtaskâ€usingSVMstrainedonhand-codeddatasetstoaid\ninsocialscienceresearch.CoAICoder[31]similarlyleveragednaturallanguageunderstandingmodelstrained\nonhumancodersâ€™worktoaidincollaborativequalitativecoding.\n(2) Rule-basedAIsystemsthatextracttext-matchingrulesfromhumancodingresultsandapplythemtomore\ndatasets.Forexample,[55]and[33]encodehumancodersâ€™codingresultsintorulesforAItosuggestcodesin\nunseendata.Theusageofhuman-readableandeditablerulesincreasesthetransparencyinmachinecoding.\n(3) LLM-basedmethodsthatinstructLLMstolabeldatawithapredeterminedcodebook.Forexample,[71]used\nGPT-3tolabeldatawithafixedsetofcodes,thussupportingresearchersindeductivecoding.Despitethe\nincreasedexplainability,someevaluationstudieshavepointedtoincreasedbiasasapotentialsetback[3].\nClassificationapproachesaregenerallyeasiertoscaleandevaluate,makinglarge-scaledatasetsmoreaccessible.\nGiventheobjectiveforalgorithmstoproducehuman-likecodes,researchershaveadoptedtraditionalMLanddeductive\ncodingmetricstoevaluatealgorithmoutcomes.Forexample,Liewetal.usedhumancodingasthegroundtruthand\nappliedtwocommonMLmetrics:precision,whetherthepositivepredictionsofthemodelwereaccurate,andrecall,how\nwellthemodelcouldidentifypositiveinstancesinthetrainingset.InXiaoetal.,GPT-3â€™sperformancewasevaluated\nwiththeinter-raterreliability(IRR)withhumancodersâ€™independentcodingresults.\nWhiletheclassificationapproachismoresuitablefordeductivecoding,ithassomeintrinsictheoreticallimitations\nforinductivecoding.Bypositioninghumancodersâ€™workas\"groundtruth,\"classificationlimitsitsoutputstolabels\n4\n,,\nAComputationalMethodforMeasuringâ€œOpenCodesâ€inQualitativeAnalysis\nprovidedbyhumancoders.Whilethislimitationalsoappliestodeductivecoding,itismorepronouncedininductive\ncoding,wherecodersaresupposedtointerpret\"novel\"insightsfromthedataset.Theyareexpectedtokeepanopen\nmindandcaptureasmanycodesaspossible.Astheprocesslacksapredefined\"groundtruth,\"truth-based(suchas\nprecisionandrecall)orconvergence-basedmetrics,becomemuchlessuseful(see2.1.1).\n2.2.2 ML/GAIForGeneration. Opencoding,ontheotherhand,isanaturalfitforgenerationtasks:thenumberand\nnatureofunderlyingcodes(orthemes)areunknownbeforetheinductiveprocess.Whilesomescholarsattemptto\nevaluatetheresultsbymatchinghumanexpertcodes[51,73],suchevaluationunderutilizesGAIâ€™spotential,which\ncouldaidresearchersinidentifyingnovelopencodesfortheorybuilding.Twoapproacheshavebeenmorestudied:\n(1) Topicmodeling,anunsupervisedMLtechnique,identifiessemanticallysimilarwordgroups(topics)intext-\nbaseddatasets.Ithasbeenusedforinductivecodinginvariouscontexts,suchassurveydata[5],socialmedia\nposts,andonlinediscussions[57].Theresultingtopicshelpresearchersfocusonkeypartsofthedatasetand\nautomatecodingforfutureanalysis.Despiterecentefforts,thedifficultyininterpretingandevaluatingtheresults\nstilllimitsitspower[36,60].\n(2) GAImodelshavebeenmorerecentlyadoptedforopen,inductivecoding.Byiterativelyprovidingdatapieces\nwithrelevantinstructions(e.g.,researchquestions,codinginstructions,desiredoutputformat),LLMscanproduce\ncodesthathumansfindmoreinterpretableanduseful[26,61].However,theymaystillmissnuanceandstruggle\nwithlesslinguisticallystraightforwardthemes[26,39],whilegeneratingnon-groundedresultsoroperatingata\ncoarserlevelofanalysis[16,72].Carefulpromptdesignandpromptingstrategiesareessentialtomitigatethese\nlimitations,yetfewcomparisonstudiesexist[16,61].\nRegardlessofthemethodused,thelackofa\"groundtruth\"andagreed-uponstatisticalmetricsposesagreater\nchallengeforevaluatingopencodes.Whileresearcherscanassessalgorithmsâ€™outputbasedonusefulnessorexplain-\nability,thesubjectivemeasuresarelabor-intensive.Moreover,thosemeasuresmayoverlookthecomplexityofinductive\ncoding.Evenifalloutputcodesseemusefulorexplainable,thealgorithmmaystillmisscriticalcodeswithoutthe\nevaluatorsâ€™knowledge.SimilarproblemsexistforDePaoliandMathisâ€™scomputationalmeasurement,wherethe\nresearchersequentiallyfeedsdatapiecesintogpt-3.5-turbountilnomorecodesarefound:first,notallcodesmay\nuniformlyexistamongdatapieces.IftheLLMaccidentallymissesacodeinthefirstinterview,thereisnoguarantee\nthatitwillpickupagainlater;second,theLLMmaybesystematicallybiasedtomisscertaincodesthroughoutthe\nprocess.\nAssuch,thereisapressingneedtoaligntheoreticalexpectationsofqualitativeanalysiswithpracticalevaluation\nmechanismsformachine-assistedinductivecoding.Ourworkaddressesthisbyintroducinginnovative,theory-informed\ncomputationalmethodsforinductivecodingandanovelcomputationalapproachforevaluatingtheseresults.\n3 MEASURINGINDUCTIVECODES\nInformedbyliterature(see2.1.2),wedevelopedacomputationalmeasurementthat1)aggregatesopencodesofmultiple\nhuman/machinecodersand2)measureseachagainsttheaggregation.Usingthemethod,wemeasuredtheoutcomesof\ndifferentML/GAIapproachesontwodatasetsandcomparedthefirstdatasetâ€™sresultswithhumanevaluation.With\nhuman-AIcollaboration,weexploredthepotentialbiasofML/GAIcodingapproaches.Sincepartofourmethodinvolved\ngenerativeAI,wevalidateditsreliabilitywithanoutputanalysis.\n5\n,,\nChen,etal.\n3.1 TheConceptualMethod\nFollowingthesuggestionsofTAandGT,weadoptateam-basedapproachandmeasureindividualcodersâ€™results\nagainsttheaggregationofmultiplecoders.Toachievethat,weproposedaconceptualstructure,CodeSpace(CSP),to\nrepresentinductivecodesproducedbyeachindividual.ThesumofindividualCSPsbecomesanAggregatedCode\nSpace(ACS).UsingACSasareferenceforevaluation,weproposedfourconceptualmetrics(Coverage,Density,Novelty,\nandDivergence)tomeasureindividualCSPâ€™srelativeperformance.\nFig.1. A:AconceptualillustrationofanACSmergedfromğ‘ğ‘ ğ‘ 1andğ‘ğ‘ ğ‘ 2.B:Measuringğ‘ğ‘ ğ‘ 1usingthemergedACSasareference.\n3.1.1 Code Spaces (CSP), Aggregated Code Spaces (ACS). Assuming we need to measure sets of inductive codes\nğ¶ğ‘œğ‘‘ğ‘–ğ‘›ğ‘”_ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğ‘ .WedefineaCodeSpace(CSP)asamulti-dimensionalconceptualspacethatcoversmultiplecodes\nidentifiedorinterpretedfromtheunderlyingqualitativedataset,eachrepresentingasetofcodes.Eachcodeisrepresented\nbyanetworknodeandhasmultipledimensionsrelatedtoitsconceptualnature.\nCombiningmultipleCodeSpaces,wegetaAggregatedCodeSpace(ACS)thatcoversallcodesfromallğ¶ğ‘œğ‘‘ğ‘–ğ‘›ğ‘”_ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğ‘ \n(Fig1A).Similarbutnotequivalentcodesarenowconnectedbylinksandconsideredasğ‘ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘Ÿğ‘ .TheideaofACS\nacknowledgesthathumansmaynotfindallpossibleinterpretationsbutcouldstillbuildan\"aggregated\"setofcodes\ntocomecloser.ACSisalsoasnapshotintimethatdocumentsresearchersâ€™currentefforts.Itcanbeusedtolookfor\nconvergenceanddivergenceamongtheteammembers,supportingdifferentstagesofqualitativeanalysis.Convergence\nmayhelpresearchersdeterminethecurrentconsensusandindicateeachcodeâ€™svisibilityandimportance.Divergence\nmayrevealunderlyingbiases,differentfocuses,ormissedopportunitiesforresearchers.Forexample,arecentstudy\n[45]critiquesoverrelianceonconsensusbuildingandadvocatesformoreattentiontoâ€œdissonances,disagreements,and\ndifferences.â€\n6\n,,\nAComputationalMethodforMeasuringâ€œOpenCodesâ€inQualitativeAnalysis\nTheconceptsofCSPandACSenableustomeasureinductivecodingresultsğ¶ğ‘œğ‘‘ğ‘–ğ‘›ğ‘”_ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğ‘ .Whilethispaper\nmayhavecoinedthetermACS,manyqualitativeresearchershavelongreliedonsimilarideasandpractices(e.g.,\nThomas).ExaminationofACScouldhappenduringopencoding,whenresearchersdiscusswhatwasdiscoveredand\nnotdiscoveredbyindividualteammemberstohelpthemreorientthenextcodingbatch;duringaxialcoding,when\nresearcherscollaboratetoconsolidatetheACSintocodebooks;andduringselectivecoding,whenresearcherstruncate\ntheACStocoalescearoundacorecategoryrelevanttotheresearchquestion.\n3.1.2 ConceptualizetheEvaluatingMetrics. WeproposefourconceptualmetricstomeasureindividualCSPsagainst\ntheiraggregationACS(Fig1B),whichwillbeoperationalizedinlatersections:\nâ€¢ CoverageandDensity:HowmuchconceptualspacedoesaCSPcover,andhowmanycodesdoesituseto\ncoverthisspace?BothTAandGTstrivefor\"richness\"ofcodes.Inpractice,researchersneedtostriveforbreadth\nanddepth.Breadthmeanscoveringasdiversesetsofconceptsaspossible.Depthmeansthedescriptivedetails\nthatensurestheconceptâ€™smeaningfulnessandrichness,thussupportingresearchersâ€™furtheranalysis[11,23].\nCombiningthem,researchersmaybeabletocapturefindingswithdepthandvariation,twocriteriaforevaluating\nGTstudies[23],totheextentthatâ€œnothinghasbeenleftout.â€[35]Ontheotherhand,notallcodesareofequal\ninteresttoresearchers.Someconceptsaremorelikelytobegroundedinorrelevanttotheresearchquestion.\nThemetricsmustaccountforeachconceptâ€™simportanceandweighitaccordingly.\nâ€¢ Novelty:Howmuchoftheâ€œnovelâ€conceptualspacedoesaCSPinclude?Wedefine\"novel\"codesasonesthat\nfewerthanğ‘ğ‘œğ‘£ğ‘’ğ‘™ğ‘¡ğ‘¦_ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘CSPshaveincluded.Mostofthetime,CSPswillnotbe(evencloseto)identical.\nSometimes,aresearcherbringsinnovelideasorlensesthatweremissedbyothers;inothercases,aresearcher\ncouldmakeasliporinappropriatelynameacode.Identifying\"novel\"codescouldhelpusidentifyeitherscenario\nandsupporthuman-humanorhuman-AIcollaboration.\nâ€¢ Divergence:HowfarisaCSPâ€™scodedistributionfromtheACS?SupposetwoCSPs,AandB,havethesame\ncoverageanddensity.However,AhasmorecodesincommonwithmostCSPs.B,ontheotherhand,hasmore\ncodesincommonwithnooneelse(\"novel\"codes)orfewinthegroup.Todetectthisatthemacrolevel,we\ncalculateeachCSPâ€™sdivergenceastheseparationfromitsprobabilitydistributionofcodestothatoftheACS,\nusingthelatterasaâ€œgroundtruth.â€Inotherwords,thecloseraCSPconceptdistributionistotheACS,theless\n\"divergent\"itis.\nSinceourevaluationfocusesmoreontherelativecomparisonbetweencodingresults,itdoesnotdirectlyaddress\npotentialissuesofgroundedness,i.e.,whethercodescouldbereasonablyre-identifiedbyanothercoderfromthe\nunderlyingdata.However,ourmetricsdoprovideanindirectpathwayforhuman-AIcollaboration:bylookingatthe\n\"novel\"partsofCSPand/orcodeswithveryfewexamples,wemaybeabletoidentifycodeswithpotentialgroundedness\nissuesmoreeasily.\n3.2 TheComputationalMethod\nTheoperationalizationofourmethodstartsfromconsolidatingğ¶ğ‘œğ‘‘ğ‘–ğ‘›ğ‘”_ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğ‘ intoanACSforevaluationreference.\nEachğ¶ğ‘œğ‘‘ğ‘–ğ‘›ğ‘”_ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ shouldcomefromanindividualmachineorhumancoder.Forğ¶ğ‘œğ‘‘ğ‘–ğ‘›ğ‘”_ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ tobetreatedas\naCSP,eachğ¶ğ‘œğ‘‘ğ‘’ shouldhaveağ¿ğ‘ğ‘ğ‘’ğ‘™,anoptionallistofğ·ğ‘’ğ‘“ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ ,andalistofğ¸ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘  (i.e.thepiecesofdata\nwherethecodewasidentified).IntheoutputACS,eachğ¶ğ‘œğ‘‘ğ‘’ willhaveaconsolidatedğ¿ğ‘ğ‘ğ‘’ğ‘™,ğ·ğ‘’ğ‘“ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›,alistof\nğ¸ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ ,alistofğ‘ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘Ÿğ‘ (othercodesthataresemanticallyclosetothisone),andalistofğ‘‚ğ‘¤ğ‘›ğ‘’ğ‘Ÿğ‘ (ğ¶ğ‘œğ‘‘ğ‘–ğ‘›ğ‘”_ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğ‘ \nthathaveincludedoneormorevariationsoftheconsolidatedcode).ğ‘ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘Ÿğ‘ enablethenetworkstructureofACS\n7\n,,\nChen,etal.\nandthederivationofcodeğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘ .EvaluatingindividualCSPsagainsttheACS,wecancalculateğ¶ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’,ğ·ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦,\nğ‘ğ‘œğ‘£ğ‘’ğ‘™ğ‘¡ğ‘¦,andğ·ğ‘–ğ‘£ğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘›ğ‘ğ‘’ asoverallandcluster-levelmetrics.Foranonymizationreasons,wewillopen-sourceour\nimplementationafterthepaperâ€™sacceptance.\n3.2.1 ConsolidatingtheACS. Overall,consolidatinganACSinvolvesmultipleiterationsof:\n(1) Findingequivalentcodesandmergethemintoone.\n(2) Generateanewlabelanddefinitionformergedcodes.\n(3) Repeattheprocesswiththenewlistofcodes,untilnothingmoreismerged.\nFindingequivalentcodesismorecomplicatedthanitseems.Humanormachinecodersoftenusedifferentphrasesto\ndescribethesameorsimilarideas.Forexample,supposecoderAidentified\"usersuggestion,\"whilecoderBfound\"user\nsuggestions.\"Theyareclearlyreferringtothesameidea.SupposecoderBfound\"userfeedback\"instead.AreAandB\nreferringtoequivalent,similar,ordifferentidea(s)?Todeterminethat:\n(1) Weencodeeachcodewithtextembedding,transformingitsğ¿ğ‘ğ‘ğ‘’ğ‘™ andğ·ğ‘’ğ‘“ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘ )intoahigh-dimensional\nvector.\n(2) Weusecosinedistance,acommonlyusedtextsimilaritymeasure,tocalculatethedistancebetweencodes.\n(3) Weuseahierarchicalclusteringalgorithmtochoosemergingcandidates,sinceitdoesnotspecifythe\nexpectednumberofresults.Foreachnodeinthealgorithmâ€™sdendrogramtreestructure,weapplytwoinput\ndistancethresholds:ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ andğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ.\n(a) Differentcodes:Differentcodeswithadistanceabovetheğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ thresholdwillneverbemerged.\n(b) Equivalentcodes:Verysimilarcodeswithadistancebelowtheğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ thresholdwillalwaysbemerged.\n(c) Forcodeswithadistancebelowtheğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ butabovetheğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ,thealgorithmpenalizes21)theproportion\nofnon-overlappingexamples;and2)thesizeofuniqueexamplesaftermerging(comparedwiththeaverage\nexamplesizesofallcodes).\n(i) Alsoequivalentcodes:Ifthedistanceisbelowtheğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ afterthepenalty,thecodeswillbemerged.\n(ii) Similarcodes:Ifthisisthelastiteration,thosecodesbecomeğ‘ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘Ÿğ‘ toeachother.\n3.2.2 ImplementingtheConsolidation. Weimplementedtheconsolidatingprocessbymergingclosercodesfirstand\nfarthercodeslater.ThepurposeistohelpLLMscreatelessgenerallabelsanddefinitions.Wewilldiscussthechoiceof\ntextembeddingmodelsandparametersin3.3.3.\n(1) Wemergecodeswithexactlythesameğ¿ğ‘ğ‘ğ‘’ğ‘™;\n(2) Weiterativelymergecodeswithverysimilarğ¿ğ‘ğ‘ğ‘’ğ‘™ (ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ =0.35,ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ =0.35).Sincecodesinthisstepare\nusuallyverysimilar,tooptimizethetokenusage,wesimplyusetheshorterone.Iftheinputdoesnotincludea\nğ·ğ‘’ğ‘“ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›,wegenerateoneforit.\n(3) Weiterativelymergecodeswithsimilarğ¿ğ‘ğ‘ğ‘’ğ‘™ andğ·ğ‘’ğ‘“ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ =0.5,ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ =0.4).\n(4) Weiterativelymergecodeswithsimilarğ¿ğ‘ğ‘ğ‘’ğ‘™andğ·ğ‘’ğ‘“ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ=0.6,ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ=0.4).Duringthelastiteration,\nweconsiderallcodeswithdistancesunderğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ tobeğ‘ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘Ÿğ‘ .\n2Theideaofpenaltycomesfromourinitialexploration,wheremergingbydistancealonecouldleadtoundesirableoutcomes.Forexample,assumethe\ncode\"userfeedback\"iscloseenoughto\"solicitingfeedback\"and\"integratingfeedback,\"yetthelattertwoarefarenoughapart.IfanACSstartswiththe\nlattertwocodes,theywillnotbemerged.Yet,byadding\"userfeedback\"totheACS,astraightforwardalgorithmwillmergeallthreecodesintoone,\nlosingthenuancesbetween\"soliciting\"and\"integrating.\"Topreventthis,ouralgorithmpenalizes\"oversized\"mergingresults.Forexample,since\"user\nfeedback\"willlikelyaccumulatetoomanyexamplesaftermergingthe\"solicitingfeedback\"and\"integratingfeedback,\"thedistancethresholdtomerge\nintoitwillbestricter(lower).Toreducemergingsimilarlynamedcodeswithdifferentintentions,thealgorithmalsopenalizesthedifferencesbetween\ncodesâ€™examples.\n8\n,,\nAComputationalMethodforMeasuringâ€œOpenCodesâ€inQualitativeAnalysis\nEachiterationofthemergingprocessisimplementedasthefollows:\n(1) Wecalculatethepenaltycoefficient:\nğ‘ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦=ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿâˆ’ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ (1)\n(2) Foreachpairofcodesğ‘¥,ğ‘¦,wecalculatetheirdistancewithpenalty:\n(a) Wecalculatethecosinedistance:\nğ‘¥Â·ğ‘¦\nğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘¥,ğ‘¦)=1âˆ’ (2)\nâˆ¥ğ‘¥âˆ¥âˆ¥ğ‘¦âˆ¥\n(b) Wheneverğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ <ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘¥,ğ‘¦) <ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ:\n(i) Wecalculatethepercentageofthedifferencebetweenexamplesofthetwocodes.Then,wesquaredthe\nresulttoreducethepenaltyonsmalldifferences.\n#(ğ‘’ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ [ğ‘¥]âˆ©ğ‘’ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ [ğ‘¦])\nğ‘‘ğ‘–ğ‘“ğ‘“(ğ‘¥,ğ‘¦)=( )2 (3)\n#(ğ‘’ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ [ğ‘¥]âˆªğ‘’ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ [ğ‘¦])\n(ii) Weapplythepenaltyofdifferentexamplesonthedistance.\nğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘¥,ğ‘¦)=ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘¥,ğ‘¦)+ğ‘‘ğ‘–ğ‘“ğ‘“(ğ‘¥,ğ‘¦)âˆ—ğ‘ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦ (4)\n(3) Weusethehierarchicalclusteringalgorithmtoproduceadendogramtreestructurefromthedistancematrix.\n(4) Weiteratethroughthetreestructurefromthetoptothebottom.Eachnodeofthetreerepresentsapotential\nmergeğ‘¥,withalistofcodestomerge;ağ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘¥);anditsuniqueexamplesaftermergedğ‘’ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ (ğ‘¥).\n(a) Wecalculatethepercentageofover-sizing(clampedbetween0%-200%):\n#(ğ‘’ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ [ğ‘¥])\nğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘–ğ‘§ğ‘’(ğ‘¥)=ğ‘ğ‘™ğ‘ğ‘šğ‘( ,100%,300%)âˆ’1 (5)\nğ‘ğ‘£ğ‘”(#(ğ‘’ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ ))\n(b) Wethencalculateanewmergingthresholdwiththepenalty:\nğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘(ğ‘¥)=ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿâˆ’(ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘–ğ‘§ğ‘’(ğ‘¥)âˆ—0.5)2âˆ—ğ‘ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦ (6)\n(c) Weonlymergethecodeswhenğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘¥) <ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘(ğ‘¥).\n3.2.3 CalculatingtheMetrics. Wenowcalculatethemetricsin3.1.2tomeasureindividualCSPsagainsttheconsolidated\nACS.ToavoidCSPswithmanyredundantcodesgaininganunfairadvantage,whenevertwoormorecodesaremerged\nintheACS,theirCSPcounterpartsarealsoconsideredmerged.SupposecoderAidentifiedtenvariationsofthesame\nconcept\"usersuggestion\",ourmethodwilltreatthemasonlyonecodeaslongastheyaredetectedandmerged.\nFirst,wecalculatetheğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ ofeachcodeusingthesumofindividualCSPâ€™scoverageğ‘£ğ‘ğ‘™ğ‘¢ğ‘’forthiscode.The\ncoverageğ‘£ğ‘ğ‘™ğ‘¢ğ‘’is1forCSPthathasthecode,orapercentagebasedonhowmanyğ‘›ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘Ÿğ‘ theCSPincludescompared\nwiththetotalnumberofğ‘›ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘Ÿğ‘  ofthecode.Forexample,iftwoCSPscoveredthesamecodeandthethirdCSP\ncoveredtwooutoffourneighborsbutnotthecodeitself,theğ‘£ğ‘ğ‘™ğ‘¢ğ‘’wouldbe1,1,0.5.Theğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ wouldbe2.5.\nğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘ğ‘ ğ‘,ğ‘ğ‘œğ‘‘ğ‘’)=1,ğ‘–ğ‘“ğ‘ğ‘ ğ‘ âˆˆğ‘œğ‘¤ğ‘›ğ‘’ğ‘Ÿğ‘ (ğ‘ğ‘œğ‘‘ğ‘’)\n#(ğ‘ğ‘œğ‘‘ğ‘’ âˆˆğ‘œğ‘¤ğ‘›ğ‘’ğ‘‘ğ‘›ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘Ÿğ‘ ) (7)\n=\n#(ğ‘ğ‘œğ‘‘ğ‘’ âˆˆğ‘›ğ‘’ğ‘–ğ‘”â„ğ‘ğ‘œğ‘Ÿğ‘ )\n9\n,,\nChen,etal.\nâˆ‘ï¸\nğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡(ğ‘ğ‘œğ‘‘ğ‘’)=ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘ğ‘ğ‘ ,ğ‘ğ‘œğ‘‘ğ‘’)= ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘ğ‘ ğ‘,ğ‘ğ‘œğ‘‘ğ‘’) (8)\nğ‘ğ‘ ğ‘âˆˆğ‘ğ‘ğ‘ \nThen,wecalculatethecomputationalmetricsweightedbytheğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ofeachcode.ForeachCSP:\n(1) CoveragemeasurestheproportionoftheCSPâ€™stotalcodeğ‘£ğ‘ğ‘™ğ‘¢ğ‘’againsttheACSâ€™stotalcodeğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡;\nâˆ‘ï¸\nğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’_ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘ğ‘ ğ‘)= ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘ğ‘ ğ‘,ğ‘ğ‘œğ‘‘ğ‘’) (9)\nğ‘ğ‘œğ‘‘ğ‘’âˆˆğ‘ğ‘ğ‘ \nğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’_ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘ğ‘ ğ‘)\nğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’(ğ‘ğ‘ ğ‘)= (10)\nğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’_ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘ğ‘ğ‘ )\n(2) DensitymeasurestheCSPâ€™srelativedensity(i.e.howmanyconsolidatedcodesdoestheCSPincludeforits\ncoverage)againsttheACS;\n#(ğ‘ğ‘œğ‘‘ğ‘’ âˆˆğ‘ğ‘ ğ‘)\nğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦_ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘ğ‘ ğ‘)= (11)\nğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’_ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘ğ‘ ğ‘)\nğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦_ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘ğ‘ ğ‘) #(ğ‘ğ‘œğ‘‘ğ‘’ âˆˆğ‘ğ‘ ğ‘)\nğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦(ğ‘ğ‘ ğ‘)= = (12)\nğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦_ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘ğ‘ğ‘ ) #(ğ‘ğ‘œğ‘‘ğ‘’ âˆˆğ‘ğ‘ğ‘ )âˆ—ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’_ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘ğ‘ ğ‘)\n(3) NoveltymeasurestheproportionoftheCSPâ€™stotalnovelcodeğ‘£ğ‘ğ‘™ğ‘¢ğ‘’againsttheACSâ€™stotalnovelcodeğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡.\nInthisstudy,weusedğ‘ğ‘œğ‘£ğ‘’ğ‘™ğ‘¡ğ‘¦_ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ =1,i.e.acodeisnovelifonlyonecodebookexplicitlycontainsit;\nğ‘›ğ‘œğ‘£ğ‘’ğ‘™ğ‘¡ğ‘¦(ğ‘ğ‘ ğ‘)=\n(cid:205) ğ‘ğ‘œğ‘‘ğ‘’âˆˆğ‘ğ‘ ğ‘(ğ‘›ğ‘œğ‘£ğ‘’ğ‘™=1)ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘ğ‘ ğ‘,ğ‘ğ‘œğ‘‘ğ‘’)\n(13)\n(cid:205) ğ‘ğ‘œğ‘‘ğ‘’âˆˆğ‘ğ‘ğ‘ (ğ‘›ğ‘œğ‘£ğ‘’ğ‘™=1)ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘ğ‘ğ‘ ,ğ‘ğ‘œğ‘‘ğ‘’)\n(4) DivergenceiscalculatedastheseperationbetweentheCSPandtheACSâ€™sprobabilitydistribution.Tomeasure\nthat,wenormalizedtheCSPandtheACSâ€™svalues(orweights)intolists.SinceindividualCSPsmaycompletely\nmissacodeanditsneighbors,wechosetheJenson-ShannonDivergence(JSD)totoleratetheresultingzero\nelements.Wereporteditsmetricversion,Jenson-ShannonDistance,bytakingasquareroot(citationhere).\nğ‘‘ğ‘–ğ‘£ğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘›ğ‘ğ‘’(ğ‘ğ‘ ğ‘)=âˆšï¸ğ½ğ‘†ğ·(ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘ğ‘ ğ‘)||ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘ğ‘ğ‘ )\n(14)\nACSâ€™snetworkstructureenablesthedetectionofcodeclusters(orcommunities,differentfromtheclustering\nalgorithmweusedformergingcodes)andamorenuancedmeasurement.Sincesomecodesmaynothaveaneighbor,\nwealsoassignlinksfromeachcodeâ€™sthreeclosestcounterpartswithareducedweightforcommunitydetectionwith\ntheLouvainalgorithm[8].Forreproducibility,wereportcommunitiesidentifiedwithseed=0.Weapplythemetrics\nwithineachclustertoanswermorenuancedquestions,e.g.,whetherahumanormachinecodercouldhaveoversampled\norundersampledspecificportionsofcodes.\n3.3 StudyDesign\nWeconductedempiricalexperimentsontwoseparateHCIdatasetsandresearchquestionstodemonstratetwouse\ncasesofourmethod.SinceourmeasurementinvolvesGAIingeneratingcodelabelsanddefinitions,wevalidateits\nreliabilityby1)comparingitwithhumanevaluationresults;and2)analyzingitsoutputstability.\n3.3.1 TasksandDatasets. WeexperimentedwithopencodingresultsontwoHCIdatasets,eachwithitsresearch\nquestionandcontext.\n(1) PhysicsLabâ€™sonlinecommunitydataset(127messagesfromthebeginningofthecommunity)betweendesigners\nandteacherusers.Theresearchquestionwas:\"HowdidPhysicsLabâ€™sonlinecommunityemerge?\"Fourhuman\n10\n,,\nAComputationalMethodforMeasuringâ€œOpenCodesâ€inQualitativeAnalysis\ncoders have previously open-coded the dataset. Three were PhD students, and one was an undergraduate\nassistant.\n(2) TwointerviewsfromaCHI2024study[20],eachlastaroundtwohours.Theresearchquestionwas:\"Inthe\ncontextofNetLogolearningandpractice:Whatperceptions-strengths,weaknesses,andadoptionplans-do\nintervieweesperceiveinLLM-driveninterfaces?Howdotheyuseittosupporttheirwork?Whataretheirneeds\nforLLM-basedinterfaces?\"Threehumancodershavepreviouslyopen-codedthedataset.TwowerePhDstudents,\nandonehadamasterâ€™sdegree.\nWhilebothdatasetsconsistofmanymoreitems,humanexpertsspenttensofhoursfinishingopencodingofthe\nsubset.Ontheotherhand,sincethegoalistoidentifyconcepts\"asexhaustiveaspossible,\"thesmalldatasetenablesus\ntoevaluateML/GAIâ€™spotentialinfew-shotanalyses.\n3.3.2 ChoiceofML/GAICodingApproaches. InCase1,wereplicatedfivepublishedML/GAIapproachesonopencoding\nwithbothdatasets[19].Weprovidedthesameresearchquestionanddatasetinformationforhumansandmachines.We\nassignedthesamerolesandtaskstomachinecoders,suchasâ€œYouareanexpertinthematicanalysiswithgrounded\ntheory,workingonopencoding.â€Theexactprompts,parametersandDataset1â€™ssampleoutputscanbefoundin[19],\norsupplementarymaterialsofthispaper.Here,weprovideanoverviewofthefiveapproaches:\n(1) BERTopic+LLMusestopicmodeling[68],anunsupervisedMLtechniquetoidentifygroupsofsemantically\nsimilarwords(i.e.,topics)andexplainsthetopicswithLLMs.SinceBERTopicâ€™sinstructionwasintendedfor\ngeneral-purposelabel-making,wegaveadditionalinstructionsabouttheresearchquestionandcontexts.\n(2) ChunkLevelasksLLMstoidentifyopencodesfromchunks(e.g.,aconversation,aninterview,etc.)ofdata.\nManyvariantsofthisapproachhavebeenadoptedbyrecentpapers(e.g.,[41,67]).\n(3) ChunkLevelStructuredasksLLMstogeneratemorethanonelevelofconcepts:thefirstfor\"categories\"or\n\"themes,\"thesecondfor\"codes\"or\"subcodes.\"Somerecentpapershavestartedtoadoptthisapproach[19,61].\n(4) ItemLevelasksLLMstoconductline-by-linecoding,assuggestedbythegroundedtheoryliterature[34].Afew\npapershaveadoptedthisapproachtogenerateone[61]ormultiplecodes[19]perline.\n(5) ItemLevelwithVerbPhrasesbuildsonthepreviousapproachbutinstructsLLMstouseverbphrasesfor\nlabelsexplicitly.Thedesignwasinspiredbyagroundedtheoryliterature[25]andreportedbyarecentstudy[19].\nTable1presentsanexcerptfromDataset1,codedbyfourhumanandfivemachineapproaches.Allmachinecoders\nweredrivenbythesamemodel(GPT-4o-0513)withthesametemperature(0.5).\n3.3.3 ChoiceofModelsandHyperparameters. Ourcomputationalmethodutilizestextembeddingandgeneration\nmodelsduringevaluation.Fortextembeddingmodels,weconsultedtheMTEBleaderboardbetweenMayandJune\n2024andexperimentedwithafewalternatives.Forthisstudy,weusedgecko-768fromGoogleduetoitsrelativelyhigh\nperformance,lowdimensions(withbettercomputationalefficiency),andeasyaccessibility.\nWeusedcosinedistanceforthresholdsin3.2.2,where0meansthetwocodesareidentical;1fornocorrelation;and\n2forabsolutelydifferent.Duringourinitialexperiments,wesetthethresholdsofğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ =0.4andğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ =0.6basedon\ntwoindicators:1)wereferredtothedistributionofpairwisedistancesandlocatedlocalturningpoints;and2)whether\nthecodepairsunderthecriteriameetourdefinition,wherehumancanrecognizecodescloserthanğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ asthesame\nidea,whilecodesfurtherthanğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ arelikelytobedifferent.Differentresearchcontextsandtextembeddingmodels\nmayrequiredifferentparameters.\n11\n,,\nChen,etal.\nIftherecouldbeanexportfunction,ortheabilitytosaveorimportexperiments,itwould\nUser4232\nbeconvenient.Wecouldsetuptheparametersintheofficeanddirectlyimporttheminclass.\nBERTopic+LLM featurerequestsforphysicsexperiments\nChunkLevel communityfeedbackloop,contextofuse,participatorydesign,userfeedbackandsuggestions\nChunkStructured userfeedback,practicalapplication,usabilityimprovements,featurerequests\nItemLevel featurerequest,usabilityimprovement,classroomapplication,import/exportfunctionality\nItemVerbPhrases proposeadditionalfeatures,emphasizeconvenience,suggestpracticalusecase\n4HumanCoders communityfeedback,featurerequest,proposeanotherfeature,givesreasonforit\nDesigner Therewillbe.\nBERTopic+LLM futureplanninganddevelopment\nChunkLevel designerresponsiveness\nChunkStructured designerresponse,acknowledgmentandimplementation\nItemLevel designerconfirmation,featureimplementation,futureplanning\nItemVerbPhrases confirmfutureimplementation,validateuserrequest,planfeaturedevelopment\n4HumanCoders developerresponse,acknowledgement,acceptanceofrequest,promisingitwillrealize\nDesigner Doestheclasshaveinternet?\nBERTopic+LLM technicalandinfrastructuralchallengesineducationalsettings\nChunkLevel contextualconstraints\nChunkStructured featureupdatesandenhancements,user-designerinteraction\nItemLevel internetavailabilityinquiry,classroomsetup,technicalconsideration\nItemVerbPhrases inquireaboutclassroomconditions,gathercontext,considertechnicalrequirements\n4HumanCoders engagingwithcommunity,seekingcontext,askfollowupquestiononusagescenario\nGenerallynot.Eversinceanadultimagepoppedupduringamajorcity-levelopenclass,\nUser4232\ntheschoolhasdisabledthenetworkonclassroomcomputers[Emoji].\nBERTopic+LLM technicalandinfrastructuralchallengesineducationalsettings\nChunkLevel contextofuse,contextualconstraints\nChunkStructured featureupdatesandenhancements,user-designerinteraction\nItemLevel internetrestriction,classroomenvironment,securityconcern,pastincident\nItemVerbPhrases explainlackofinternet,providecontext,sharepastincident\ncontextualizingresponse,humor,personalanecdote,sharinginformationfordesign,\n4HumanCoders\nstorysharing,givesananswer,explainstheanswer\nTable1. AnexampleexchangebetweenateacheruserandadesignerfromourDataset1,codedbyfivemachinecodersandfour\nhumancoders.Welightlymergedverysimilarcodes(e.g.,askaquestionvs.question)fromthefourhumancoderstosavespace.\nTounderstandhowthechoiceofmodelandtemperaturemayinfluencetheevaluationprocess,weusedGPT-4o-0513\n(0513),GPT-4o-mini,andLlama3-70Bbecause:1)GPT-4o-0513wasoneofthemostpowerfulmodels(intermsof\nstate-of-the-artevaluationbenchmarks)atthetimeoftheexperiment;2)GPT-4o-miniwasoneofthemostpotent\nsmallermodels;3)Llama3-70Bwasoneofthemostpotentopen-sourcemodels.\n3.3.4 ExperimentDesign. Weconductedtwocasestudiestounderstandtworesearchquestions:\nâ€¢ WhatcanwelearnaboutexistingML/GAIapproachesforopencoding?\nâ€¢ Isourmethodstatisticallyreliableenoughtoevaluateopencodesfrommachineandhumancoders?\nCase1examinesthefivemachinecodingapproachesusing1)theoverallmetricsformachineandhumancoders;and\n2)thecluster-levelmetricsofrelativecoverage,ametricbetween-100%(completelyundersampled)to+inf(extremely\noversampled).Twohumanresearchersindependentlyinterpretedthethemeforeachnetworkcluster(see3.2.3)based\nonitsconstituentcodesandreconciledtheirdifferencesintoasinglelabel.\n12\n,,\nAComputationalMethodforMeasuringâ€œOpenCodesâ€inQualitativeAnalysis\nğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’(ğ‘ğ‘ ğ‘,ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ)\nğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’ ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’(ğ‘ğ‘ ğ‘,ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ)=\nğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’(ğ‘ğ‘ ğ‘)\nâˆ’100% (15)\nCase2evaluatedsixmainstreamLLMsâ€™performanceinopencodingwiththesameprompt:GPT-3.5-turbo;GPT-4o-\n0513;Llama3-70B;Mixtral8x22b;Claude3-haiku;andClaude3.5-sonnet.3Tosavespace,weonlyreportthemetrics\nfromItem-levelVerbPhrases,thebest-performingGAIapproach.Themeasurementincludesallmachineandhuman\ncodersâ€™results,enablingcomparisonsbetweenLLMs;andbetweentheaggregatedresultsofLLMsandhumans.\nThereliabilitystudyevaluatedourmeasurementâ€™soutcomestabilitybasedonCase1.Werepeatedthemeasurement\nwiththreemodels,fivetemperatures(0,0.25,0.5,0.75,1).Thisresultsin15combinationsofLLMandtemperature.For\neachcombination,werepeated10independentruns,totaling150runs.WerecordedtheevaluatorLLM,temperature,\nrunnumber,coderidentity,andfourmetrics.Withthose,weevaluated:\nâ€¢ Thecoefficientofvariationğ‘ğ‘œğ‘£ foreachmetric,eachCSP,andeachcombinationofLLMandtemperature.On\neachdataset,weranfixed-effectsregressionstounderstandifthechoiceofLLM,temperature,orCSPâ€™sğ‘šğ‘’ğ‘ğ‘›\nvalueofmetricinfluencedtheğ‘ğ‘œğ‘£.\nâ€¢ WhetherusingdifferentcombinationofLLMandtemperatureimpactspairwisecomparisons.On\neachdataset,weranaseriesofpairwiseANOVAexperimentsandrecordedsignificantpairson1)theentire\noutputfrom150runs;2)eachofthe15combinations.Wecalculatedtheintersectionofallpairsandlookedfor\nanyindividualcombinationthatwouldproducedifferentconclusionsthantheentirety.Forexample,suppose\nwefoundtheItem-Levelapproachâ€™scoveragesignificantlyhigherthantheChunk-Levelapproach.Ifweonly\nevaluatedwithoneLLMandonetemperature,willtheconclusionchange?\nSincewefoundthechoiceofLLMandtemperaturetohavenosignificantimpactonthepairwisecomparison\noutcome,weonlyconducted10evaluationrunswithLlama3-70Bat0.5temperatureforCase2.\n3.4 EmpiricalResults\n3.4.1 Case1:EvaluatingMachineCodingApproaches. Figure2,3demonstratesğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’andğ‘‘ğ‘–ğ‘£ğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘›ğ‘ğ‘’,twoofthe\nmorestablemetrics(see3.4.3),forallmachineandhumancodersinbothdatasets.Wenoted:\nâ€¢ Similartoaprevioushumanevaluation[19],wefoundthatitem-levelcodingapproacheshavehighercoverage\nandlowerdivergencethanchunk-levelandBERTopicapproaches.\nâ€¢ Whiletheaggregationofhumanresearchershashighercoverageandlowerdivergencethananysinglemachine\ncodingapproachincoverageandmostindivergence,theaggregationofmachinecodershashighercoverageand\nlowerdivergencethaneachandallhumanresearchers.Notethatwedonotclaimthatmachinesoutperformed\nhumans,andanongoingstudyiscurrentlyexaminingtheimplications.\nâ€¢ Notethatthenumbersofcodesdonotstrictlycorrelatewiththecoveragemetric.Forexample,inDataset1,the\nItem-LevelVerb-Phraseapproachfound282rawcodeswith79.1%coverage,whiletheaggregationofhuman\ncodersfound340rawcodeswithonly75.5%.\nWefurtherexploredthepotentialbiasesofhumanandmachinecoderswithcluster-levelmetrics.Tables2,3presents\nclustersforbothdatasetsâ€™ACSandtherelativecoverageofeachCSP.Clustersaresortedbythesumoftheircomponent\ncodesâ€™weights.Thus,clusterswithmorecodesandhigherconsensuslevelsarelistedfirst.Numbershigherthan\n3ThelistdoesnotincludeGPT-4o-mini,releasedafterwecompletedthecodingtask.\n13\n,,\nChen,etal.\nFig.2. Thecoveragemetricforallcodingapproaches,Dataset1(withfourhumans)and2(withthreehumans).Forgrounded\ntheoryopencoding,higherisbetter.\nFig.3. Thedivergencemetricforallcodingapproaches,Dataset1(withfourhumans)and2(withthreehumans).\n0representoversamplingofcodes,whilenumberslowerthan0representundersampling.Thestandarddeviation\nindicatesthedegreeofimbalancebetweenclusters.\nWeimmediatelynotedthatalmostallmachineandhumancodershadcompletelymissedsomeclustersfromthe\ndata.Acrossthetwodatasets,theonlyexceptionisItem-levelLLMcodingwithverbphrases.Italsohasthelowestğ‘ ğ‘¡ğ‘‘\nofrelativecoverageacrossallindividualcoders,showingitscomparativelyuniformsamplingacrossallcodeclusters\ncomparablewiththeaggregationofallhumancoders.Examiningthequalitativenatureofmissedorundersampled\n14\n,,\nAComputationalMethodforMeasuringâ€œOpenCodesâ€inQualitativeAnalysis\nID # BERTopic Chunk Chunk-S Item Item-V AllAI AllHuman\n1 34 +2.30% +9.90% +13.80% +26.40% +12.00% +6.60% +14.40%\n2 45 +99.10% +16.00% -43.10% +12.10% -1.50% +3.80% -12.20%\n3 26 +73.10% +59.20% +58.20% +15.20% +15.20% +3.30% +25.80%\n4 28 +1.40% +62.80% +46.00% -5.00% +13.60% +2.20% +12.20%\n5 26 -75.00% +29.50% +44.90% -17.20% +3.10% -3.80% +8.30%\n6 35 -86.20% -65.80% -65.90% -19.90% -40.50% -13.50% -2.70%\n7 20 +12.40% +36.50% +17.50% +28.10% +13.40% +9.30% -6.50%\n8 23 -14.20% -80.50% -37.90% -1.80% +0.00% -4.50% -6.30%\n9 20 +1.80% -55.00% -19.30% +12.50% -14.80% -1.10% -19.40%\n10 16 +20.20% +31.90% +41.70% -1.00% -9.60% +5.40% -19.70%\n11 16 -100.00% -70.40% +30.30% -40.70% +10.10% -0.50% +5.10%\n12 16 +116.10% -20.40% -100.00% -42.40% -27.10% -18.90% -8.30%\n13 11 -100.00% -48.30% -7.10% -2.00% +11.30% +6.60% +5.90%\n14 14 -100.00% -100.00% -100.00% -54.60% -32.60% -3.80% -64.70%\n15 10 -100.00% -100.00% -80.70% -54.60% -5.20% +1.70% -55.90%\n16 10 -100.00% -100.00% -100.00% -50.50% -42.50% -29.40% -51.90%\n17 6 -100.00% -100.00% -100.00% -100.00% -57.90% -63.00% -11.70%\nStd 75.26% 60.30% 59.03% 34.98% 23.10% 17.81% 25.37%\nTable2. ACSofDataset1:ClustersofCodes,andEachCodingApproachâ€™sRelativeCoverage(-100%=CompletelyMissed;+Inf=\nInfinitelyOversampled).\nclustersiscrucialtounderstandingpotentialbiasfurther.Itallowsresearcherstosituatetheanalysisinthecontextof\nthedataandresearchquestions.Below,webrieflyinterprettheclustersmissedbyeachapproach:\nâ€¢ BERTopic+LLM.ThecoverageofBERTopicislow(21.1%)andhighlyunevenlydistributedamongclusters.\nTheseeminglyoversamplingoncluster12(designersâ€™reactionstousersuggestions)wasonlybecauseofthelow\nbaselinenumber,asitonlyincludedtwo(plusoneneighbor)outof14codes.Moreover,BERTopicmissesclusters\n13-17,whereothercodersnotedmanydetailsaboutthedesignexchanges,suchasneeds,designconsiderations,\nandcommunicationstrategies.\nâ€¢ Chunk-levelapproaches.Bothapproacheshaverelativelylowcoverage(39.2%,43.2%).Thestructuredapproach\nhasslightlybettercoverage,buttheoverallunevennesslevelwithinclustersstayssimilar,inadditiontocompletely\nmissingcluster12.Bothstarttocovercluster13(designersâ€™apologies,explanations,andreassurancetousers),yet\nstillmissclusters14-17.Bothapproachesalsooversamplepositiveexpressions(e.g.,cluster3)whileundersampling\nclusterswithnegativeimplications(e.g.,clusters6and12).\nâ€¢ Item-levelapproaches.Bothapproacheshavehighcoverage(74.7%,76.6%).Theverbphraseapproachhas\nslightlybettercoverageandunevennesslevelandavoidsthebaselineapproachâ€™smissofcluster17(designersâ€™\nresponsestrategies).Thisisnotasmallachievement:allothercodersexceptonehumanmissedthisclusterof6\ncodesaltogether.\nWeobservedsimilartrendsinDataset2,whereBERTopic,chunk-levelLLMcoding,andhumancodersmissedmany\nclustersaroundintervieweesâ€™nuancedreflections(clusters12-19).Moreover,item-levelcodingapproachescontinued\ntodobetterthanotherapproaches.Notably,humancodersmisscluster19(reflectingandcriticizingworkingand\nhelp-seekingcultures)altogether.\n15\n,,\nChen,etal.\nID # BERTopic Chunk Chunk-S Item Item-V AllAI AllHuman\n1 21 -30.10% +8.10% -48.50% +18.60% +14.90% +7.60% +13.00%\n2 26 -53.00% +68.50% +39.50% +1.70% +13.90% -0.40% +23.70%\n3 24 +83.00% -22.80% +43.70% +8.50% -3.00% -2.40% +39.10%\n4 22 +151.50% -62.40% +104.80% +13.00% +4.20% +0.30% +16.30%\n5 20 +134.40% -36.00% +96.10% -7.00% +4.10% -2.80% +14.00%\n6 15 +86.20% +124.90% -38.80% +16.60% -10.20% +4.70% -11.50%\n7 15 -17.50% -100.00% -63.00% +16.50% +3.00% +4.70% +22.20%\n8 16 -55.80% +54.10% -100.00% +1.70% +5.90% -4.40% +12.60%\n9 17 -81.70% +129.60% +56.30% -14.40% -10.90% +0.20% -14.00%\n10 16 -100.00% +49.40% +1.70% -9.50% +4.10% +3.40% -52.00%\n11 12 -36.40% +55.30% +5.70% -11.10% -1.60% -5.30% -6.40%\n12 14 -100.00% -100.00% -48.50% -10.90% -16.10% -2.20% -36.20%\n13 14 -100.00% -100.00% -48.50% -10.90% +11.80% +7.60% -85.80%\n14 16 -100.00% +75.20% -100.00% -51.80% -54.70% -15.10% -42.50%\n15 10 -100.00% -100.00% -100.00% -4.20% -1.60% +7.60% -58.40%\n16 7 -41.10% -100.00% -100.00% -27.40% -18.00% +7.60% -48.00%\n17 7 -100.00% -100.00% -100.00% -81.30% -47.30% -38.50% -33.20%\n18 5 -100.00% -100.00% -46.00% -44.00% -29.70% -23.20% +11.40%\n19 5 -100.00% -100.00% -46.00% -6.60% -12.10% +7.60% -100.00%\nStd 87.17% 87.28% 68.77% 25.24% 18.97% 12.03% 39.81%\nTable3. ACSofDataset2:ClustersofCodes,andEachCodingApproachâ€™sRelativeCoverage(-100%=CompletelyMissed;+Inf=\nInfinitelyOversampled).\n3.4.2 Case2:BenchmarkingMainstreamLLMs. InFigures4and5,weappliedourmethodtobenchmarksixmainstream\nLLMsinmid-2024.Wenoted:\nâ€¢ Significantgapsindifferentmodelsâ€™coverageanddivergence.Inbothdatasets,wefoundGPT-4o,Claude3.5\nSonnet,andLlama3-70BsignificantlyoutperformedGPT-3.5-Turbo,Claude3Haiku,andMixtral8x22B.Without\ndesignatingagroundtruth,ourmethodsroughlydistinguishlarger,more\"powerful\"models(measuredby\ncommonMLbenchmarks,e.g.,MMLU)fromsmallerones.\nâ€¢ Machinecodersâ€™performanceswerenotalwaysconsistentamongthetwodatasets.Forexample,allmodels\nbutClaude3HaikuhaveseenaperformancedropinDataset2,withClaude3.5Sonnetdroppingalmost12%.\nâ€¢ Machinecodersâ€™aggregatedresultconsistentlyachievedbettercoverage,density,anddivergencethan\nhumancodersâ€™aggregatedresults,withcoveragesaround95%andsignificantlylowerdivergencesinboth\ndatasets.Evenwhenmostindividualmodelsâ€™performancedroppedinDataset2,theperformanceofmachine\ncodersâ€™aggregationalmoststayedthesame.\nâ€¢ WealsonotedtheperformanceofaggregatedhumancodersdroppinginDataset2.Onepotentialreason:only3\nhumancodersconductedopencodingforDataset2,comparedwith4forDataset2.\n3.4.3 ReliabilityStudy. Figures6,7demonstratetheoutcomevarianceofeachmetricacrossthreeLLMsontwodatasets.\nWenoted:\nâ€¢ Overall,ğ‘‘ğ‘–ğ‘£ğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘›ğ‘ğ‘’ consistentlyhastheloweststandardvariance(ğ‘ ğ‘¡ğ‘‘,2-3%)andcoefficientofvariance(ğ‘ğ‘œğ‘£,\n1.5-5.5%),whileğ‘›ğ‘œğ‘£ğ‘’ğ‘™ğ‘¡ğ‘¦hasthehighestğ‘ ğ‘¡ğ‘‘(3-6%)andğ‘ğ‘œğ‘£(8-65%).\n16\n,,\nAComputationalMethodforMeasuringâ€œOpenCodesâ€inQualitativeAnalysis\nFig.4. ThecoveragemetricforsixLLMs,Dataset1(withfourhumans)and2(withthreehumans).Forgroundedtheoryopen\ncoding,higherisbetter.\nFig.5. ThedivergencemetricforsixLLMs,Dataset1(withfourhumans)and2(withthreehumans).\nâ€¢ Aftercontrollingthetemperatureandcoderidentities,themodelchoicesoccasionallyhaveasignificant\nbutsmallimpact.However,thereisnorelationshipbetweenthesizesofmodelsandtheğ‘ğ‘œğ‘£impacts,andno\nmodelshaveconsistentlylowerğ‘ğ‘œğ‘£.\nâ€¢ Themodeltemperaturedoesnothaveasignificantimpact.\nâ€¢ Forğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’,ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦,andğ‘›ğ‘œğ‘£ğ‘’ğ‘™ğ‘¡ğ‘¦,theirğ‘ğ‘œğ‘£ arenegativelycorrelatedwithindividualcodersâ€™ğ‘šğ‘’ğ‘ğ‘›valueofthe\nmetric.Forexample,acoderwithhigherğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’willlikelyhavealowerğ‘ğ‘œğ‘£thanacoderwithlowerğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’.\n17\n,,\nChen,etal.\nFig.6. CoefficientofVarianceforEachMetricamongLLMs,Dataset1\nFig.7. CoefficientofVarianceforEachMetricamongLLMs,Dataset2\nSincerunning150evaluationrunsforasinglecomparisonstudymaybecost-ineffective,weevaluatedourmethodâ€™s\nreliabilitywhenonlyonemodel,andtemperature,and10evaluationrunsareused.Here,weassumethepairwise\ncomparisonresultsof150runsasthegroundtruth.Acrossall15combinationsonfourmetricsandtwodatasets,weonly\nidentifiedtwocasesoffalsepositivesforapairwithaminusculemeandifferenceofğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦(BERTopicvs.Chunk-level\nStructured,32.71%vs.32.41%)inDataset1.Besidesthat,weonlyidentifiedfalsenegativesforcomparisonpairswith\nsmallmeandifferences.Inotherwords,unlessthegoalistocomparemetricswithsmallnumericaldifferences,10runs\nwouldbegenerallysufficient.\n3.5 Discussions\n3.5.1 ReliabilityoftheMeasurement. WhileusingGAIinourmethodinevitablyintroducesstochasticityandpotential\nbiases,weestablishedthereliabilityofourcomputationalmeasurementwiththereliabilitystudy(3.4.3).\n18\n,,\nAComputationalMethodforMeasuringâ€œOpenCodesâ€inQualitativeAnalysis\nOur methodâ€™s only source of stochasticity comes from GAIâ€™s generation of code definitions and labels, which\ninfluencesthedownstreamdecisionofcodemerging.Naturally,codingresultswithfewercodeswillbeimpacted\nmore,andourempiricalresultssupportthisassumption.Inplainwords,ourmethodprovidesmorestableoutcomesat\nevaluatingâ€œbetterâ€codingresultsbutlesssoforâ€œworseâ€ones.Thisisespeciallytrueforğ‘›ğ‘œğ‘£ğ‘’ğ‘™ğ‘¡ğ‘¦,whichmeasuresthe\nnumberofcodesthatnooneelsehasidentified.Therefore,weonlyrecommendusingğ‘›ğ‘œğ‘£ğ‘’ğ‘™ğ‘¡ğ‘¦asaqualitativeindicator\nforpotentialoutliers,whileusingğ‘‘ğ‘–ğ‘£ğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘›ğ‘ğ‘’,ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’,andğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦forquantitativecomparisons.\nWeconfirmedthatusingasingleLLMandtemperaturedoesnotgenerallyleadtofalsereportsonpairedcomparisons.\nInonlyonecase,10runswithasingleLLMandtemperaturecausedafalsepositiveagainsttheentiretyof150runs.\nMoreover,theresultshowsthatourmethoddoesnotrelyonthemostpowerfulmodels.Open-sourcemodelswiththe\npotentialforlocaldeploymentandbetterprivacyprotection(suchasLlama3-70B)orsmallermodelswithcheapercosts\nandlessecologicalfootprints(suchasGPT-4o-mini)willlikelyworkformostscenarios.\n3.5.2 SuggestionsforUsingML/GAIinInductiveCoding. Case1reachessimilarconclusionsasaprevioushuman\nevaluation[19]:theitem-levelapproachesperformedbestonbothdatasetsandresearchquestions.Thecluster-level\nmetricsenabledustocompareML/GAIcodingapproacheswithmorenuances.BERTopicandchunk-levelapproaches\nhadworseoverallmetrics,missedentireclustersofcodes,andwerelesscapableofidentifyingnuancesofhuman\ninteractions,particularlyoneswithseeminglylessconnectionwiththeresearchquestion.Theyalsooversampled\npositiveemotionsorfeedback,implyingapotentialbiasfortheircodingresults.\nIncontrast,thesuccessofitem-levelapproachessuggeststhepotentialofembeddinghumanprocessesforqualitative\nanalysisintoLLMprompts.WhilewestartedmentioninggroundedtheoryintheChunk-LevelStructuredapproach,its\nperformancewasnotmuchbetterthantheChunk-Level.ThesituationchangedwhenweinstructedLLMstostrictly\nfollowthegroundedtheoryprocess[62]:bycodingthedataitem-by-item,theitem-levelapproachesproducedmuch\nbetterresults.Followingexistingliterature[25],ouradoptionofverbphrasesaslabelsfurtherenabledamorenuanced\ninterpretationofthedata.\nCase2providesaquickassessmentofstate-of-the-artLLMsavailableinmid-2024.Ourfindingsuggeststhatexisting\nNLUbenchmarksforLLMsmaypositivelycorrelatewiththeirperformanceoninductivequalitativecoding.Onthe\notherhand,thesameLLMsmayperformdifferentlyondifferentresearchquestionsanddatasets,necessitatingbroader\nevaluativestudies.Whilewesuggestresearchersusethebestavailablemodel,ourfindingsalsoindicatethefeasibilityof\npowerfulopen-sourcemodelssuchasLlama-370B,particularlyforscenarioswheredataprivacyconcernsarestronger.\nMoreover,wesuggestusingmultiplemodelssimultaneouslyasanevenbetterapproach.Thecombinationofsixmodels\nconsistentlycoveredalmostallcodesidentifiedbyhumanresearchers,evenwhenindividualperformancesvaried\nacrossdatasets.\n3.5.3 Human-AICollaborationinInterpretation. Thisworkâ€™scorecontributionisanovelcomputationalmeasurement\ntounderstand,compare,andevaluateopencodesfrommultiplemachineorhumancoders.Hence,itiscrucialtoshare\nhowweinterpretedthealgorithmicresultsanddiscussscenarioswhereourmeasurementcouldcontributetohuman-AI\ncollaborationinopencodingprocesses.\nThenatureofqualitativeresearchpreventsusfrommakingageneralclaimwithonlytwodatasetsandresearch\nquestions,yetthisismoreofafeaturethanabug.Boththeopencodesandthecorrespondingevaluationarenaturally\nboundedbythedatasets,researchquestions,andperspectivesresearchersadopted.Thus,resultsfromourmeasurement\nmustbeinterpretedincontext.Below,wepresentanexampleflowbasedonTable2onDataset1:\n19\n,,\nChen,etal.\nâ€¢ AresearcherfirstlookedatthegeneralmetricsofeachCSPinarandomevaluationrunofCase1,whereBERTopic\nhadthehighestğ‘‘ğ‘–ğ‘£ğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘›ğ‘ğ‘’acrossallruns.SincetheresearcherbelievestheACStobereasonablywell-covered\nduetothepresenceofmultiplehumancoders,itseemsthatBERTopicproducedthemostdeviantresults.\nâ€¢ Here,twopossibilitiesexist:eitherBERTopicidentifiedsomethingnovelandinsightful,oritsimplymissedtoo\nmanycodes.Toevaluatethis,theresearchercheckedtheğ‘›ğ‘œğ‘£ğ‘’ğ‘™ğ‘¡ğ‘¦metricandfoundBERTopicthelowestagain.\nSoitisunlikelytobethefirstsituation;infact,itonlyhas3codesthatnooneelsefound.Thosecodesareall\nrelatedtoaspectsofthesoftwarefeature,whichisfarfromtheresearchquestionaboutcommunityformation.\nâ€¢ NotethatBERTopiconlyidentified23codes.Forlow-levelcodingapproachesthatfoundhundredsofcodes,the\nresearchermayneedadditionalsupporttounderstandtheirpotentialbiases.Therefore,theresearcherclustered\ncodesintheACSandcalculatedcluster-levelmetrics.\nâ€¢ TheresearcherfoundthattheItem-Levelapproachmissesandunder-samplesmoreclustersthantheItem-Level\nVerbPhraseapproach,indicatingahigherriskforpotentialbias.Fromthere,basedontheresearchgoalsand\nquestions,theresearcherqualitativelyinterpretedthetwoapproachesâ€™outcomedifferencesanddecidedtoadopt\ntheItem-LevelVerbPhraseapproachfortherestofthecodingprocess.\nWhile computational measurement is crucial in the process, such interpretations are only possible through a\nqualitative understanding of the codes and clusters. This paperâ€™s human evaluation focuses on the open coding\nprocesses,whereeverygroundedinterpretationiswelcomed.However,whenresearchersconductfurtheranalysissteps,\ndependingontheresearchcontext,acodecouldbecometoonuancedandwithtoofewcases;toobroadandcovers\neverything;irrelevanttothecoreaspectsoftheresearchquestion;orhasbeenreportedmanytimesbefore.Whileour\nconvergencemetricsautomaticallydevalueoutliercodesfromfewcoders,itisstillpossiblethatafew\"well-performed\"\napproachesormodelsactuallycontributelittletothefinalanalysis.Itstillreliesonhumans(potentially,incollaboration\nwithAI)tomaketheseinterpretationsanddecisions,openingupopportunitiesforfurtherresearchworkandinterface\ndesign.\nThepotentialofourevaluationmethodisnotlimitedtomachinecodersalone.InCase1,wepresentedhowindividual\nhumancodersmayhavemissedinsightsfromthedata.Duringourstudyprocess,wefoundthevisualizationofACSâ€™s\nnetworkstructureparticularlyuseful,asitrevealedthedifferentfocusesofhumancodersandfacilitatedfruitful\ndiscussionaroundtheresearchquestion.Movingforward,wewillfocusondesignworkthatcanmaterializethe\nmethodâ€™spotentialinsupportinghumancoders,withorwithoutmachinecodersinACS.\nACKNOWLEDGMENTS\nWewholeheartedlyacknowledgetheintellectualcontributionofthefollowingresearchers:Prof.GolnazArastoopour\nIrgens;Prof.DavidRibes;Prof.ElizabethGerber;Prof.FoadHamidi;Prof.XiLu;andmembersoftheCenterfor\nConnectedLearningandComputer-BasedModeling&TIDALlabatNorthwesternUniversity.Wealsowanttothanka\ngroupofscholarswhogaveinsightfulfeedback:ZilinMaandotheranonymousfriends.Inaddition,weacknowledge\nthecontributionofourresearchintern,LilyYang,whoparticipatedinthedataanalysisofourfirstdataset.\nREFERENCES\n[1] AnneAdams,AnnBlandford,andPeterLunt.2005.Socialempowermentandexclusion:Acasestudyondigitallibraries.ACMTransactionson\nComputer-HumanInteraction12,2(June2005),174â€“200. https://doi.org/10.1145/1067860.1067863\n[2] AnneAdams,PeterLunt,andPaulCairns.2008.AQualitativeApproachtoHCIResearch.InResearchMethodsforHuman-ComputerInteraction.\nCambridgeUniversityPress.\n20\n,,\nAComputationalMethodforMeasuringâ€œOpenCodesâ€inQualitativeAnalysis\n[3] JulianAshwin,AdityaChhabra,andVijayendraRao.2023.Usinglargelanguagemodelsforqualitativeanalysiscanintroduceseriousbias.arXiv\npreprintarXiv:2309.17147(2023).\n[4] JenniferAttride-Stirling.2001. Thematicnetworks:ananalytictoolforqualitativeresearch. QualitativeResearch1,3(Dec.2001),385â€“405.\nhttps://doi.org/10.1177/146879410100100307\n[5] EricP.S.Baumer,DavidMimno,ShionGuha,EmilyQuan,andGeriK.Gay.2017.Comparinggroundedtheoryandtopicmodeling:Extremedivergence\norunlikelyconvergence?JournaloftheAssociationforInformationScienceandTechnology68,6(2017),1397â€“1410. https://doi.org/10.1002/asi.23786\n_eprint:https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.23786.\n[6] IzhakBerkovich.2018.Beyondqualitative/quantitativestructuralism:Thepositivistqualitativeresearchandtheparadigmaticdisclaimer.Quality&\nQuantity52,5(2018),2063â€“2077. https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11135-017-0607-3&\ncasa_token=JlBYcYvjqpwAAAAA:Qn6arDr_zncTUy62N2jsyRQgPblLU2nVHRlN6nUkQib4UkbPw4-HgrsOhMi0Bhsz-lFEhcU2DbSDb58oPublisher:\nSpringer.\n[7] AndreaBinghamandPatriciaWitkowsky.2021.DeductiveandInductiveApproachestoQualitativeDataAnalysis.InAnalyzingandInterpreting\nQualitativeResearch:AftertheInterview.SAGEPublications,Inc. https://books.google.com/books?hl=en&lr=&id=0xIoEAAAQBAJ&oi=fnd&pg=\nPA133&dq=deductive+coding+qualitative&ots=1wZ007ufMy&sig=tLe5PscNzbMG_yMvXduwFPOquvY#v=onepage&q=deductive%20coding%\n20qualitative&f=false\n[8] VincentDBlondel,Jean-LoupGuillaume,RenaudLambiotte,andEtienneLefebvre.2008.Fastunfoldingofcommunitiesinlargenetworks.Journal\nofstatisticalmechanics:theoryandexperiment2008,10(2008),P10008.\n[9] RobertBowman,CamilleNadal,KellieMorrissey,AnjaThieme,andGavinDoherty.2023. UsingThematicAnalysisinHealthcareHCIat\nCHI:AScopingReview.InProceedingsofthe2023CHIConferenceonHumanFactorsinComputingSystems.ACM,HamburgGermany,1â€“18.\nhttps://doi.org/10.1145/3544548.3581203\n[10] VirginiaBraunandVictoriaClarke.2006. Usingthematicanalysisinpsychology. QualitativeResearchinPsychology3,2(Jan.2006),77â€“101.\nhttps://doi.org/10.1191/1478088706qp063oa\n[11] VirginiaBraunandVictoriaClarke.2013.Successfulqualitativeresearch:Apracticalguideforbeginners.(2013).\n[12] VirginiaBraunandVictoriaClarke.2021.Onesizefitsall?Whatcountsasqualitypracticein(reflexive)thematicanalysis?QualitativeResearchin\nPsychology18,3(July2021),328â€“352. https://doi.org/10.1080/14780887.2020.1769238\n[13] VirginiaBraunandVictoriaClarke.2022. Conceptualanddesignthinkingforthematicanalysis. QualitativePsychology9,1(Feb.2022),3â€“26.\nhttps://doi.org/10.1037/qup0000196\n[14] JoyD.Bringer,LynneH.Johnston,andCeliaH.Brackenridge.2004. MaximizingTransparencyinaDoctoralThesis1:TheComplexitiesof\nWritingAbouttheUseofQSR*NVIVOWithinaGroundedTheoryStudy.QualitativeResearch4,2(Aug.2004),247â€“265. https://doi.org/10.1177/\n1468794104044434\n[15] ZanaBuÃ§inca,MajaBarbaraMalaya,andKrzysztofZGajos.2021.Totrustortothink:cognitiveforcingfunctionscanreduceoverrelianceonAIin\nAI-assisteddecision-making.ProceedingsoftheACMonHuman-computerInteraction5,CSCW1(2021),1â€“21.\n[16] CourtniByun,PiperVasicek,andKevinSeppi.2024. ChainofThoughtPromptingforLargeLanguageModel-drivenQualitativeAnalysis.In\nProceedingsofthe2024CHIConferenceonHumanFactorsinComputingSystems(CHIâ€™24).AssociationforComputingMachinery.\n[17] LaraCarminati.2018.GeneralizabilityinQualitativeResearch:ATaleofTwoTraditions.QualitativeHealthResearch28,13(Nov.2018),2094â€“2101.\nhttps://doi.org/10.1177/1049732318788379\n[18] M.ArielCascio,EunlyeLee,NicoleVaudrin,andDarcyA.Freedman.2019.ATeam-basedApproachtoOpenCoding:ConsiderationsforCreating\nIntercoderConsensus.FieldMethods31,2(May2019),116â€“130. https://doi.org/10.1177/1525822X19838237\n[19] JohnChen,AlexandrosLotsos,LexieZhao,GraceWang,UriWilensky,BruceSherin,andMichaelHorn.2024.PromptsMatter:ComparingML/GAI\nApproachesforGeneratingInductiveQualitativeCodingResults. arXiv:2411.06316[cs.CL] https://arxiv.org/abs/2411.06316\n[20] JohnChen,XiLu,YuzhouDu,MichaelRejtig,RuthBagley,MichaelS.Horn,andUriJ.Wilensky.2024.LearningProgrammingofAgent-based\nModelingwithLLMCompanions:ExperiencesofNovicesandExpertsUsingChatGPT&NetLogoChat.InProceedingsofthe2024CHIConferenceon\nHumanFactorsinComputingSystems.\n[21] NedCooper,TiffanieHorne,GillianHayes,CourtneyHeldreth,MichalLahav,JessSconHolbrook,andLaurenWilcox.2022.ASystematicReview\nandThematicAnalysisofCommunity-CollaborativeApproachestoComputingResearch.InCHIConferenceonHumanFactorsinComputingSystems.\n1â€“18. https://doi.org/10.1145/3491102.3517716arXiv:2207.04171[cs].\n[22] JulietCorbinandAnselmStrauss.2008. Chapter10/AnalyzingDataforConcepts. InBasicsofQualitativeResearch(3rded.):Techniquesand\nProceduresforDevelopingGroundedTheory.SAGEPublications,Inc.,2455TellerRoad,ThousandOaksCalifornia91320UnitedStates. https:\n//doi.org/10.4135/9781452230153\n[23] JulietCorbinandAnselmStrauss.2008.Chapter14/CriteriaforEvaluation.InBasicsofQualitativeResearch(3rded.):TechniquesandProceduresfor\nDevelopingGroundedTheory.SAGEPublications,Inc.,2455TellerRoad,ThousandOaksCalifornia91320UnitedStates. https://doi.org/10.4135/\n9781452230153\n[24] JulietM.CorbinandAnselmStrauss.1990.Groundedtheoryresearch:Procedures,canons,andevaluativecriteria.Qualitativesociology13,1(1990),\n3â€“21. Publisher:Springer.\n[25] NatalieR.Davis,ShirinVossoughi,andJohnF.Smith.2020.Learningfrombelow:Amicro-ethnographicaccountofchildrenâ€™sself-determination\nassociopoliticalandintellectualaction. Learning,CultureandSocialInteraction24(2020),100373. https://www.sciencedirect.com/science/\n21\n,,\nChen,etal.\narticle/pii/S2210656119302107?casa_token=R60cWsxYBRgAAAAA:JTuwj92trq-YVKvmrCwtjFxlRCgBsFxzdpnKu2WNMa9l2OqmQ8wO_h_\nqvLqL49FXXQAx1gXPqwPublisher:Elsevier.\n[26] StefanoDePaoli.2023.PerforminganInductiveThematicAnalysisofSemi-StructuredInterviewsWithaLargeLanguageModel:AnExploration\nandProvocationontheLimitsoftheApproach.SocialScienceComputerReview0,0(Dec.2023),1â€“23. https://doi.org/10.1177/08944393231220483\n[27] StefanoDePaoliandWalterSMathis.2024.Reflectionsoninductivethematicsaturationasapotentialmetricformeasuringthevalidityofan\ninductivethematicanalysiswithLLMs.Quality&Quantity(2024),1â€“27.\n[28] JenniferFeredayandEimearMuir-Cochrane.2006.DemonstratingRigorUsingThematicAnalysis:AHybridApproachofInductiveandDeductive\nCodingandThemeDevelopment.InternationalJournalofQualitativeMethods5,1(March2006),80â€“92. https://doi.org/10.1177/160940690600500107\n[29] NickJFox.2008.Post-positivism.TheSAGEencyclopediaofqualitativeresearchmethods2,1(2008),659â€“664.\n[30] DominicFurniss,AnnBlandford,andPaulCurzon.2011.ConfessionsfromagroundedtheoryPhD:experiencesandlessonslearnt.InProceedingsof\ntheSIGCHIConferenceonHumanFactorsinComputingSystems.ACM,VancouverBCCanada,113â€“122. https://doi.org/10.1145/1978942.1978960\n[31] JieGao,KennyTsuWeiChoo,JunmingCao,RoyKa-WeiLee,andSimonPerrault.2023.CoAIcoder:ExaminingtheEffectivenessofAI-assisted\nHuman-to-HumanCollaborationinQualitativeAnalysis.ACMTransactionsonComputer-HumanInteraction31,1(Nov.2023),6:1â€“6:38. https:\n//doi.org/10.1145/3617362\n[32] SusanGasson.[n.d.]. RigorInGroundedTheoryResearch:AnInterpretivePerspectiveonGeneratingTheoryFromQualitativeFieldStudies.\n([n.d.]).\n[33] SimretArayaGebreegziabher,ZhengZhang,XiaohangTang,YihaoMeng,ElenaL.Glassman,andTobyJia-JunLi.2023. PaTAT:Human-AI\nCollaborativeQualitativeCodingwithExplainableInteractiveRuleSynthesis.InProceedingsofthe2023CHIConferenceonHumanFactorsin\nComputingSystems(CHIâ€™23).AssociationforComputingMachinery,NewYork,NY,USA,1â€“19. https://doi.org/10.1145/3544548.3581352\n[34] GrahamR.Gibbs.2007.Thematiccodingandcategorizing.Analyzingqualitativedata703,38-56(2007). https://study.sagepub.com/sites/default/\nfiles/analyzing-qualitative-da.pdf\n[35] BarneyGGlaser,JudithHolton,etal.2004.Remodelinggroundedtheory.InForumqualitativesozialforschung/forum:qualitativesocialresearch,\nVol.5.\n[36] MaartenGrootendorst.2022.BERTopic:Neuraltopicmodelingwithaclass-basedTF-IDFprocedure.arXivpreprintarXiv:2203.05794(2022).\n[37] EgonGGubaandYvonnASLincoln.1994.CompetingParadigmsinQualitativeResearch.InHandbookofqualitativeresearch.SagePublications,\nInc.,105â€“117.\n[38] GregGuest,EmilyNamey,andMarioChen.2020.Asimplemethodtoassessandreportthematicsaturationinqualitativeresearch.PloSone15,5\n(2020),e0232076. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0232076Publisher:PublicLibraryofScienceSanFrancisco,CA\nUSA.\n[39] LeahHamilton,DeshaElliott,AaronQuick,SimoneSmith,andVictoriaChoplin.2023. ExploringtheUseofAIinQualitativeAnalysis:A\nComparativeStudyofGuaranteedIncomeData. InternationalJournalofQualitativeMethods22(March2023),16094069231201504. https:\n//doi.org/10.1177/16094069231201504Publisher:SAGEPublicationsInc.\n[40] JasySuetYanLiew,NancyMcCracken,ShichunZhou,andKevinCrowston.2014.OptimizingFeaturesinActiveMachineLearningforComplex\nQualitativeContentAnalysis.InProceedingsoftheACL2014WorkshoponLanguageTechnologiesandComputationalSocialScience,Cristian\nDanescu-Niculescu-Mizil,JacobEisenstein,KathleenMcKeown,andNoahA.Smith(Eds.).AssociationforComputationalLinguistics,Baltimore,\nMD,USA,44â€“48. https://doi.org/10.3115/v1/W14-2513\n[41] SarÃ­ahLopez-FierroandHaNguyen.2024.MakingHuman-AIContributionsTransparentinQualitativeCoding.(2024). https://repository.isls.org/\n/handle/1/10537Publisher:InternationalSocietyoftheLearningSciences.\n[42] AndrewLowe,AnthonyC.Norris,A.JaneFarris,andDuncanR.Babbage.2018.QuantifyingThematicSaturationinQualitativeDataAnalysis.\nFieldMethods30,3(Aug.2018),191â€“207. https://doi.org/10.1177/1525822X17749386\n[43] PennyMackieson,AronShlonsky,andMarieConnolly.2019.Increasingrigorandreducingbiasinqualitativeresearch:Adocumentanalysisof\nparliamentarydebatesusingappliedthematicanalysis.QualitativeSocialWork18,6(Nov.2019),965â€“980. https://doi.org/10.1177/1473325018786996\n[44] NoraMcDonald,SaritaSchoenebeck,andAndreaForte.2019.ReliabilityandInter-raterReliabilityinQualitativeResearch:NormsandGuidelines\nforCSCWandHCIPractice.ProceedingsoftheACMonHuman-ComputerInteraction3,CSCW(Nov.2019),1â€“23. https://doi.org/10.1145/3359174\n[45] JanetMcGawandAlasdairVance.2023.Dissonance,Disagreement,Difference:ChallengingThematicConsensustoDecoloniseGroundedTheory.\nInternationalJournalofQualitativeMethods22(Jan.2023),16094069231220775. https://doi.org/10.1177/16094069231220775\n[46] JaneMills,AnnBonner,andKarenFrancis.2006.Thedevelopmentofconstructivistgroundedtheory.Internationaljournalofqualitativemethods5,\n1(2006),25â€“35.\n[47] AlirezaMoghaddam.2006.Codingissuesingroundedtheory.IssuesInEducationalResearch16(2006). https://www.iier.org.au/iier16/moghaddam.\nhtml\n[48] MichaelMuller,ShionGuha,EricP.S.Baumer,DavidMimno,andN.SadatShami.2016. MachineLearningandGroundedTheoryMethod:\nConvergence,Divergence,andCombination.InProceedingsofthe2016ACMInternationalConferenceonSupportingGroupWork(GROUPâ€™16).\nAssociationforComputingMachinery,NewYork,NY,USA,3â€“8. https://doi.org/10.1145/2957276.2957280\n[49] LorelliS.Nowell,JillM.Norris,DeborahE.White,andNancyJ.Moules.2017.ThematicAnalysis:StrivingtoMeettheTrustworthinessCriteria.\nInternationalJournalofQualitativeMethods16,1(Dec.2017),160940691773384. https://doi.org/10.1177/1609406917733847\n22\n,,\nAComputationalMethodforMeasuringâ€œOpenCodesâ€inQualitativeAnalysis\n[50] StevenPace.2004.Agroundedtheoryoftheflowexperiencesofwebusers.InternationalJournalofHuman-ComputerStudies60,3(March2004),\n317â€“363. https://doi.org/10.1016/j.ijhcs.2003.08.005\n[51] AngelinaParfenovaetal.2024.AutomatingQualitativeDataAnalysiswithLargeLanguageModels.InProceedingsofthe62ndAnnualMeetingofthe\nAssociationforComputationalLinguistics(Volume4:StudentResearchWorkshop).177â€“185.\n[52] MdShidurRahman.2016.TheAdvantagesandDisadvantagesofUsingQualitativeandQuantitativeApproachesandMethodsinLanguageâ€œTesting\nandAssessmentâ€Research:ALiteratureReview.JournalofEducationandLearning6,1(Nov.2016),102. https://doi.org/10.5539/jel.v6n1p102\n[53] MaryamN.RazaviandLeeIverson.2006.Agroundedtheoryofinformationsharingbehaviorinapersonallearningspace.InProceedingsofthe2006\n20thanniversaryconferenceonComputersupportedcooperativework.ACM,BanffAlbertaCanada,459â€“468. https://doi.org/10.1145/1180875.1180946\n[54] DavidRibes.2019. HowILearnedWhataDomainWas. ProceedingsoftheACMonHuman-ComputerInteraction3,CSCW(Nov.2019),1â€“12.\nhttps://doi.org/10.1145/3359140\n[55] TimRietzandAlexanderMaedche.2021. Cody:AnAI-BasedSystemtoSemi-AutomateCodingforQualitativeResearch.InProceedingsof\nthe2021CHIConferenceonHumanFactorsinComputingSystems(CHIâ€™21).AssociationforComputingMachinery,NewYork,NY,USA,1â€“14.\nhttps://doi.org/10.1145/3411764.3445591\n[56] JohnHamonSalisburyandTomCole.[n.d.].GroundedTheoryinGamesResearch:MakingtheCaseandExploringtheOptions.([n.d.]).\n[57] SinaMahdipourSaravani,SadafGhaffari,YanyeLuther,JamesFolkestad,andMarciaMoraes.2023.AutomatedCodeExtractionfromDiscussion\nBoardTextDataset.InAdvancesinQuantitativeEthnography,CrinaDamÅŸaandAmandaBarany(Eds.).Vol.1785.SpringerNatureSwitzerland,\nCham,227â€“238. https://doi.org/10.1007/978-3-031-31726-2_16SeriesTitle:CommunicationsinComputerandInformationScience.\n[58] BenjaminSaunders,JuliusSim,TomKingstone,ShulaBaker,JackieWaterfield,BernadetteBartlam,HeatherBurroughs,andClareJinks.2018.\nSaturationinqualitativeresearch:exploringitsconceptualizationandoperationalization. Quality&Quantity52,4(July2018),1893â€“1907.\nhttps://doi.org/10.1007/s11135-017-0574-8\n[59] NikhilSharma,QVeraLiao,andZiangXiao.2024.GenerativeEchoChamber?EffectofLLM-PoweredSearchSystemsonDiverseInformation\nSeeking.InProceedingsoftheCHIConferenceonHumanFactorsinComputingSystems.1â€“17.\n[60] CarsonSievertandKennethShirley.2014.LDAvis:Amethodforvisualizingandinterpretingtopics.InProceedingsoftheworkshoponinteractive\nlanguagelearning,visualization,andinterfaces.63â€“70.\n[61] RaviSinha,IdrisSolola,HaNguyen,HillarySwanson,andLuEttaMaeLawrence.2024.TheRoleofGenerativeAIinQualitativeResearch:GPT-4â€™s\nContributionstoaGroundedTheoryAnalysis.InProceedingsoftheSymposiumonLearning,DesignandTechnology.ACM,DelftNetherlands,17â€“25.\nhttps://doi.org/10.1145/3663433.3663456\n[62] AnselmStraussandJulietCorbin.1998.Basicsofqualitativeresearch:Techniquesandproceduresfordevelopinggroundedtheory,2nded.Basicsof\nqualitativeresearch:Techniquesandproceduresfordevelopinggroundedtheory,2nded.(1998),xiii,312â€“xiii,312. Place:ThousandOaks,CA,US\nPublisher:SagePublications,Inc.\n[63] GarethTerry,NikkiHayfield,VictoriaClarke,andVirginiaBraun.2017.Thematicanalysis.TheSAGEhandbookofqualitativeresearchinpsychology\n2,17-37(2017),25. https://books.google.com/books?hl=en&lr=&id=AAniDgAAQBAJ&oi=fnd&pg=PA17&dq=Thematic+analysis+terry+&ots=\ndpi2nmHiMV&sig=959tII4BUp9su6Hv2JJui1KjP5QPublisher:SAGEPublicationsLtd.\n[64] NguyenCaoThanhandTTThanh.2015.Theinterconnectionbetweeninterpretivistparadigmandqualitativemethodsineducation.American\njournalofeducationalscience1,2(2015),24â€“27.\n[65] DavidR.Thomas.2006.AGeneralInductiveApproachforAnalyzingQualitativeEvaluationData.AmericanJournalofEvaluation27,2(June2006),\n237â€“246. https://doi.org/10.1177/1098214005283748\n[66] AnthonyG.Tuckett.2005.Applyingthematicanalysistheorytopractice:Aresearcherâ€™sexperience.ContemporaryNurse19,1-2(Aug.2005),75â€“87.\nhttps://doi.org/10.5172/conu.19.1-2.75\n[67] ThomasÃœbellacker.2024.AcademiaOS:AutomatingGroundedTheoryDevelopmentinQualitativeResearchwithLargeLanguageModels.arXiv\npreprintarXiv:2403.08844(2024).\n[68] HannaM.Wallach.2006.Topicmodeling:beyondbag-of-words.InProceedingsofthe23rdinternationalconferenceonMachinelearning-ICMLâ€™06.\nACMPress,Pittsburgh,Pennsylvania,977â€“984. https://doi.org/10.1145/1143844.1143967\n[69] YixinWan,GeorgePu,JiaoSun,AparnaGarimella,Kai-WeiChang,andNanyunPeng.2023. \"kellyisawarmperson,josephisarolemodel\":\nGenderbiasesinllm-generatedreferenceletters.arXivpreprintarXiv:2310.09219(2023).\n[70] CarlaWilligandWendyStaintonRogers.2017.TheSAGEhandbookofqualitativeresearchinpsychology.Sage. https://books.google.com/books?hl=\nen&lr=&id=AAniDgAAQBAJ&oi=fnd&pg=PR7&dq=The+SAGE+Handbook+of+Qualitative+Research+in+Psychology&ots=dpi2nmHiK_&sig=\nuyFhaXXsypuO5HjEBxbBLAaNG50\n[71] ZiangXiao,XingdiYuan,Q.VeraLiao,RaniaAbdelghani,andPierre-YvesOudeyer.2023.SupportingQualitativeAnalysiswithLargeLanguage\nModels:CombiningCodebookwithGPT-3forDeductiveCoding.InCompanionProceedingsofthe28thInternationalConferenceonIntelligentUser\nInterfaces(IUIâ€™23Companion).AssociationforComputingMachinery,NewYork,NY,USA,75â€“78. https://doi.org/10.1145/3581754.3584136\n[72] AndresFelipeZambrano,XinerLiu,AmandaBarany,RyanS.Baker,JuhanKim,andNidhiNasiar.2023.FromnCodertoChatGPT:FromAutomated\nCodingtoRefiningHumanCoding.InAdvancesinQuantitativeEthnography(CommunicationsinComputerandInformationScience),Golnaz\nArastoopourIrgensandSimonKnight(Eds.).SpringerNatureSwitzerland,Cham,470â€“485. https://doi.org/10.1007/978-3-031-47014-1_32\n[73] FengxiangZhao,FanYu,andYiShang.2024.ANewMethodSupportingQualitativeDataAnalysisThroughPromptGenerationforInductive\nCoding.In2024IEEEInternationalConferenceonInformationReuseandIntegrationforDataScience(IRI).IEEE,164â€“169.\n23\n,,\nChen,etal.\nReceived14September2023;revised12December2023;accepted19January2024\n24",
    "pdf_filename": "A_Computational_Method_for_Measuring_Open_Codes_in_Qualitative_Analysis.pdf"
}