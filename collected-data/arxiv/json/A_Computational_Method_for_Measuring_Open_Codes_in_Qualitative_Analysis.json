{
    "title": "A Computational Method for Measuring Open Codes in Qualitative Analysis",
    "context": "Qualitative analysis is the process of systematically identifying, generating, and organizing concepts from data. It has been widely adopted in many social science disciplines (such as education, sociology, psychology, and medicine), as well as interdisciplinary areas such as human-computer interaction (HCI) 1 to understand people’s perceptions, feelings, and nuanced interactions with technology[2]. However, qualitative methods can be challenging, time-consuming, and may lack transparency[9]. The problem is particularly acute during the process of open coding, where researchers inductively identify emergent codes from raw data without a preconceived coding scheme. As the first step of qualitative analysis, practitioners and theorists of thematic analysis (TA)[10, 63] and grounded theory (GT)[23] make frequent use of open coding and agree on its goal: 1For example, the CHI conference itself has increasingly included papers using qualitative methods: 36.7% in 2021, 48.4% in 2022, 48.9% in 2023, 53.9% in 2024. This result comes from searches in the ACM digital library with the keywords: \"thematic analysis,\" \"qualitative analysis,\" \"qualitative coding,\" \"qualitative research,\" \"discourse analysis,\" and \"grounded theory.\"",
    "body": "A Computational Method for Measuring “Open Codes” in Qualitative Analysis\nJOHN CHEN, Northwestern University, United States of America\nALEXANDROS LOTSOS, Northwestern University, United States of America\nCAIYI WANG, Northwestern University, United States of America\nLEXIE ZHAO, Northwestern University, United States of America\nJESSICA HULLMAN, Northwestern University, United States of America\nBRUCE L. SHERIN, Northwestern University, United States of America\nURI J. WILENSKY, Northwestern University, United States of America\nMICHAEL S. HORN, Northwestern University, United States of America\nQualitative analysis is critical to understanding human datasets in many social science disciplines. Open coding is an inductive\nqualitative process that identifies and interprets \"open codes\" from datasets. Yet, meeting methodological expectations (such as \"as\nexhaustive as possible\") can be challenging. While many machine learning (ML)/generative AI (GAI) studies have attempted to support\nopen coding, few have systematically measured or evaluated GAI outcomes, increasing potential bias risks. Building on Grounded\nTheory and Thematic Analysis theories, we present a computational method to measure and identify potential biases from \"open\ncodes\" systematically. Instead of operationalizing human expert results as the \"ground truth,\" our method is built upon a team-based\napproach between human and machine coders. We experiment with two HCI datasets to establish this method’s reliability by 1)\ncomparing it with human analysis, and 2) analyzing its output stability. We present evidence-based suggestions and example workflows\nfor ML/GAI to support open coding.\nAdditional Key Words and Phrases: Open Coding, Qualitative Analysis, Large Language Models, Human-AI Collaboration, Inductive\nCoding, Thematic Analysis, Grounded Theory, Measurement\nACM Reference Format:\nJohn Chen, Alexandros Lotsos, Caiyi Wang, Lexie Zhao, Jessica Hullman, Bruce L. Sherin, Uri J. Wilensky, and Michael S. Horn. 2024.\nA Computational Method for Measuring “Open Codes” in Qualitative Analysis. In . ACM, New York, NY, USA, 24 pages.\n1\nINTRODUCTION\nQualitative analysis is the process of systematically identifying, generating, and organizing concepts from data. It has\nbeen widely adopted in many social science disciplines (such as education, sociology, psychology, and medicine), as\nwell as interdisciplinary areas such as human-computer interaction (HCI) 1 to understand people’s perceptions, feelings,\nand nuanced interactions with technology[2].\nHowever, qualitative methods can be challenging, time-consuming, and may lack transparency[9]. The problem is\nparticularly acute during the process of open coding, where researchers inductively identify emergent codes from\nraw data without a preconceived coding scheme. As the first step of qualitative analysis, practitioners and theorists of\nthematic analysis (TA)[10, 63] and grounded theory (GT)[23] make frequent use of open coding and agree on its goal:\n1For example, the CHI conference itself has increasingly included papers using qualitative methods: 36.7% in 2021, 48.4% in 2022, 48.9% in 2023, 53.9%\nin 2024. This result comes from searches in the ACM digital library with the keywords: \"thematic analysis,\" \"qualitative analysis,\" \"qualitative coding,\"\n\"qualitative research,\" \"discourse analysis,\" and \"grounded theory.\"\n2024. Manuscript submitted to ACM\n1\narXiv:2411.12142v1  [cs.CL]  19 Nov 2024\n\n, ,\nChen, et al.\nto capture as many aspects, patterns, or \"coding moments\" as possible. However, the criteria of being \"open\" and \"as\nexhaustive as possible\" are often inconsistent and subjective in practice[30, 58].\nMachine learning (ML) and generative AI (GAI) techniques have the potential to support and improve qualitative\nresearch, but this potential has yet to be fully realized. Past studies have mostly focused on deductive coding[40, 55, 71],\nwhere researchers systematically apply existing coding schemes to new data sets. However, attempts to apply ML/GAI\nto open coding have been mostly exploratory. While the HCI literature repeatedly found that humans tend to over-rely\non AI systems and can be easily misled when AI systems go astray[15], commercial providers (such as Atlas.ti) have\nalready started to provide GAI solutions for open coding. Since all future analyses rely on open coding results, there is\na pressing need for systematic measurement methods on open coding results (or \"open codes\").\nWe propose a theory-informed computational method to systematically measure open codes. Theorists of thematic\nanalysis and grounded theory advocate a team-based approach[18, 24] that depends on multiple coders to find \"as many\nrelevant concepts or interpretations as possible\" from the data[22, 24, 63]. Building on this concept, we computationally\ntransformed each coder’s inductive codes into a Code Space, and calculated the sum of all code spaces as an Aggregated\nCode Space. This enables measuring individual coders’ results against a team of coders, whose combined codes may\nget closer to the theoretical expectation. We propose and operationalize four conceptual metrics for assessing Code\nSpaces: Coverage, Density, Novelty, and Divergence.\nWe demonstrate our method’s potential as a novel computational lens to understand, compare, and evaluate open\ncodes (from either human or machine coders). Working on two HCI datasets, we computationally measured open codes\ngenerated by five previously published ML/GAI approaches and six mainstream GAI models. We validate the reliability\nof our method by 1) comparing its results with human analysis, and 2) statistically analyzing its output stability. By\ncombining machine measurement and human interpretation, we present an example of a human-AI collaborative\nworkflow to evaluate open codes, resulting in evidence-based suggestions for qualitative researchers to adopt GAI in\ninductive analysis.\n2\nBACKGROUND\n2.1\nOpen Qualitative Coding\nQualitative analysis enables an in-depth exploration of human experiences by focusing on the nuanced interpretations,\nemotions, and subjective experiences that shape individual and social realities [52]. Thematic Analysis (TA) and\nGrounded Theory (GT), two commonly used qualitative methods in HCI research[1, 2, 9, 21, 48, 50, 53, 54, 56], both\nadvocate for open coding as an essential first step to uncover the underlying reasons and processes that drive the\nformation and transformation of meaning[52, 62]. However, conducting open coding can be challenging, particularly\nwhen scaling up. Ambiguity[4, 9, 66], inconsistent terminology[12], and insufficient reporting[14] can all obscure how\nqualitative analysis was performed, making it harder to assess its quality[9, 49]. Few studies have attempted to measure\nor evaluate open coding results based on theoretical expectations, further compounding the challenge.\n2.1.1\nExpectations of Open Coding. Open coding is a key approach in TA and GT that inductively derives codes from\nraw data. It can generate new coding schemes without or in addition to a theoretical framework[24, 62]. Researchers\ncan then apply the coding scheme systematically on datasets through the deductive coding process[7]. With the\ncapability of finding novel insights beyond existing knowledge, GT theorists require open coding[22, 24, 62], and TA\ntheorists advocate for it[63]. Since subsequent analysis depends on the open coding results, without first establishing\n2\n\nA Computational Method for Measuring “Open Codes” in Qualitative Analysis\n, ,\nthe schemes’ relative completeness, researchers risk entering a biased or unbalanced concept space that would harm\nthe rigor of qualitative research. It is, therefore, imperative to review the expectations and challenges of open coding.\nMany theorists of TA and GT agree that open coding should capture as many aspects, patterns, or \"codable moments\"\nas possible[22, 24, 63]. While open coding is not always required for TA, TA theorists such as Braun and Clarke[10, 13]\nargue that the focus of qualitative assurance should shift from accuracy (i.e., the consistency in deductive coding)\nto reflexivity (i.e., how the open coding process reflects the underlying dataset). Similarly, coming from an implicit\nconstructivist epistemological stance, Terry et al. contend that open coding outcomes may be strong or weak, but\ncannot be objectively right or wrong. This shift toward reflexivity aligns with GT’s inductive process, which emphasizes\nopenness to new insights. GT stresses that researchers must continually look for emerging ideas until theoretical\nsaturation is reached, at which point no further insights emerge[24, 47]. Since the goal is to discover potential ideas,\nopen codes do not need to be consistent or systematic, and a single example may suffice.\n2.1.2\nEvaluation of Open Coding. Deriving patterns directly from raw data without a predefined framework is inherently\nchallenging[43]. This challenge is compounded by the underdevelopment of measurement for open coding and theorists’\nfocus on researcher quality (qualifications, experience, and perspective) over research quality[23]. Some metrics, such as\nvalidity or credibility, are easier to judge by the data or keep track of[28]. Other metrics are more elusive: for example,\nhow can we operationalize the expectations of inductive coding - being \"open\" and \"as exhaustive as possible\"? Even\nwhen coders identify some valid and credible codes, as Corbin and Strauss acknowledge, they are only some of the\n\"many plausible interpretations possible from data.\"\nSome scholars have attempted to adopt deductive coding metrics such as inter-rater reliability (IRR) in open coding[44].\nYet, IRR can only assess consistency between coders. As qualitative researchers may not have or reach the ground\ntruth, even complete consistency cannot measure the \"openness\" or \"exhaustiveness\" of the codes. Most qualitative\nresearchers who adopt non-positivist epistemological stances believe that[17]: post-positivism believes in objective\ntruth, yet humans may only approximate it[29, 37]; interpretivism argues that truth is subjective and situated[64], while\nconstructivism holds that truth is a human construction to understand reality[37, 46]. On the other hand, while a few\nscholars have advocated for positivist qualitative research and believe in ground truth [6, 70], it is unclear how such\ntruth can be reliably identified and validated.\nGT theorists have explored another potential pathway of evaluation. Corbin and Strauss proposed two criteria related\nto inductive coding: 1) depth, which refers to the richness of descriptive detail that adds substance to findings, and 2)\nvariation, which demonstrates the complexity of human life by incorporating diverse cases outside of dominant patterns.\nHowever, operationalizing these criteria can be difficult. Traditionally, GT theorists suggest working towards theoretical\nsaturation, where no new interpretations emerge from additional data, and existing ones are all well-defined with\nsufficient variation[23]. Yet, in real-world research contexts, the logic for determining saturation is often inconsistent\nand subjective[2, 30, 58], as it is challenging to judge whether coding has truly become exhaustive [32], or has simply\nreached a convenient stopping point.\nWhile the absolute saturation may be impractical, TA[18] and GT theorists[24] advocate for a team-based approach.\nAnalyzing data from multiple perspectives reduces the likelihood of missing key codes or over-interpreting data. By\nconstantly comparing and contrasting codes from different individuals[18], researchers can open up the analysis to\nthe scrutiny of other researchers[24], resulting in \"new insights [and] increased sensitivity\" while \"guarding against\nbias.\" Similarly, the GT-inspired general inductive approach[65] suggests merging independent coders’ open coding\n3\n\n, ,\nChen, et al.\nresults and checking the proportion of overlapping codes. A low degree of overlap may necessitate more discussion and\nfurther analysis.\nRecently, some TA scholars have attempted to quantify “saturation” by counting the number of codes that emerged\nthroughout the coding process until the curve flattens[38, 42]. However, the work is built on (possibly overly) strong\nassumptions about the inductive coding process: 1) duplicated codes do not exist, and 2) researchers will consistently\n“sample” from all “discoverable” codes. While this line of work has practical values for predicting the number of\ninterviews or surveys needed[42], both assumptions are unlikely to hold for human coders during inductive coding. As\nML/GAI models may be systematically biased (e.g., [59, 69]), the issue is only exacerbated for machine approaches.\nInformed by the literature, we contribute a theory-driven computational approach to measure inductive coding results,\ntaking an important step towards addressing the broader concerns about efficiency and transparency in qualitative\nanalysis. The next section will review existing ML/GAI work on this topic.\n2.2\nMachine-Assisted Qualitative Analysis\nMany fields (e.g., computer vision, data analytics, etc.) have widely used ML/GAI methods to label data. Qualitative\ncoding, whether inductive or deductive, can also mechanically be seen as applying labels (i.e., codes) to a given piece\nof data. This section reviews two ML/GAI perspectives for machine-assisted qualitative coding - classification and\ngeneration. While classification naturally fits into deductive coding scenarios with established evaluation metrics, its\npotential for inductive coding is intrinsically limited. Generation, on the other hand, has started to pick up traction for\ninductive coding. Yet, few studies have attempted to evaluate them, resulting in significant risks for research rigor.\n2.2.1\nML/GAI For Classification. From a computational perspective, most current work views machine-assisted\nqualitative coding as a classification task, where algorithms aim to produce codes as similarly to human researchers as\npossible. More recently, researchers have worked on three classification approaches:\n(1) Supervised ML methods that are trained on human-coded datasets. Liew et al., for example, approach machine-\nassisted qualitative coding as a “multi-label classification task” using SVMs trained on hand-coded datasets to aid\nin social science research. CoAICoder [31] similarly leveraged natural language understanding models trained\non human coders’ work to aid in collaborative qualitative coding.\n(2) Rule-based AI systems that extract text-matching rules from human coding results and apply them to more\ndatasets. For example, [55] and [33] encode human coders’ coding results into rules for AI to suggest codes in\nunseen data. The usage of human-readable and editable rules increases the transparency in machine coding.\n(3) LLM-based methods that instruct LLMs to label data with a predetermined codebook. For example, [71] used\nGPT-3 to label data with a fixed set of codes, thus supporting researchers in deductive coding. Despite the\nincreased explainability, some evaluation studies have pointed to increased bias as a potential setback[3].\nClassification approaches are generally easier to scale and evaluate, making large-scale datasets more accessible.\nGiven the objective for algorithms to produce human-like codes, researchers have adopted traditional ML and deductive\ncoding metrics to evaluate algorithm outcomes. For example, Liew et al. used human coding as the ground truth and\napplied two common ML metrics: precision, whether the positive predictions of the model were accurate, and recall, how\nwell the model could identify positive instances in the training set. In Xiao et al., GPT-3’s performance was evaluated\nwith the inter-rater reliability (IRR) with human coders’ independent coding results.\nWhile the classification approach is more suitable for deductive coding, it has some intrinsic theoretical limitations\nfor inductive coding. By positioning human coders’ work as \"ground truth,\" classification limits its outputs to labels\n4\n\nA Computational Method for Measuring “Open Codes” in Qualitative Analysis\n, ,\nprovided by human coders. While this limitation also applies to deductive coding, it is more pronounced in inductive\ncoding, where coders are supposed to interpret \"novel\" insights from the dataset. They are expected to keep an open\nmind and capture as many codes as possible. As the process lacks a predefined \"ground truth,\" truth-based (such as\nprecision and recall) or convergence-based metrics, become much less useful (see 2.1.1).\n2.2.2\nML/GAI For Generation. Open coding, on the other hand, is a natural fit for generation tasks: the number and\nnature of underlying codes (or themes) are unknown before the inductive process. While some scholars attempt to\nevaluate the results by matching human expert codes[51, 73], such evaluation underutilizes GAI’s potential, which\ncould aid researchers in identifying novel open codes for theory building. Two approaches have been more studied:\n(1) Topic modeling, an unsupervised ML technique, identifies semantically similar word groups (topics) in text-\nbased datasets. It has been used for inductive coding in various contexts, such as survey data[5], social media\nposts, and online discussions[57]. The resulting topics help researchers focus on key parts of the dataset and\nautomate coding for future analysis. Despite recent efforts, the difficulty in interpreting and evaluating the results\nstill limits its power[36, 60].\n(2) GAI models have been more recently adopted for open, inductive coding. By iteratively providing data pieces\nwith relevant instructions (e.g., research questions, coding instructions, desired output format), LLMs can produce\ncodes that humans find more interpretable and useful [26, 61]. However, they may still miss nuance and struggle\nwith less linguistically straightforward themes[26, 39], while generating non-grounded results or operating at a\ncoarser level of analysis[16, 72]. Careful prompt design and prompting strategies are essential to mitigate these\nlimitations, yet few comparison studies exist[16, 61].\nRegardless of the method used, the lack of a \"ground truth\" and agreed-upon statistical metrics poses a greater\nchallenge for evaluating open codes. While researchers can assess algorithms’ output based on usefulness or explain-\nability, the subjective measures are labor-intensive. Moreover, those measures may overlook the complexity of inductive\ncoding. Even if all output codes seem useful or explainable, the algorithm may still miss critical codes without the\nevaluators’ knowledge. Similar problems exist for De Paoli and Mathis’s computational measurement, where the\nresearcher sequentially feeds data pieces into gpt-3.5-turbo until no more codes are found: first, not all codes may\nuniformly exist among data pieces. If the LLM accidentally misses a code in the first interview, there is no guarantee\nthat it will pick up again later; second, the LLM may be systematically biased to miss certain codes throughout the\nprocess.\nAs such, there is a pressing need to align theoretical expectations of qualitative analysis with practical evaluation\nmechanisms for machine-assisted inductive coding. Our work addresses this by introducing innovative, theory-informed\ncomputational methods for inductive coding and a novel computational approach for evaluating these results.\n3\nMEASURING INDUCTIVE CODES\nInformed by literature (see 2.1.2), we developed a computational measurement that 1) aggregates open codes of multiple\nhuman/machine coders and 2) measures each against the aggregation. Using the method, we measured the outcomes of\ndifferent ML/GAI approaches on two datasets and compared the first dataset’s results with human evaluation. With\nhuman-AI collaboration, we explored the potential bias of ML/GAI coding approaches. Since part of our method involved\ngenerative AI, we validated its reliability with an output analysis.\n5\n\n, ,\nChen, et al.\n3.1\nThe Conceptual Method\nFollowing the suggestions of TA and GT, we adopt a team-based approach and measure individual coders’ results\nagainst the aggregation of multiple coders. To achieve that, we proposed a conceptual structure, Code Space (CSP), to\nrepresent inductive codes produced by each individual. The sum of individual CSPs becomes an Aggregated Code\nSpace (ACS). Using ACS as a reference for evaluation, we proposed four conceptual metrics (Coverage, Density, Novelty,\nand Divergence) to measure individual CSP’s relative performance.\nFig. 1. A: A conceptual illustration of an ACS merged from 𝑐𝑠𝑝1 and 𝑐𝑠𝑝2. B: Measuring 𝑐𝑠𝑝1 using the merged ACS as a reference.\n3.1.1\nCode Spaces (CSP), Aggregated Code Spaces (ACS). Assuming we need to measure sets of inductive codes\n𝐶𝑜𝑑𝑖𝑛𝑔_𝑅𝑒𝑠𝑢𝑙𝑡𝑠. We define a Code Space (CSP) as a multi-dimensional conceptual space that covers multiple codes\nidentified or interpreted from the underlying qualitative dataset, each representing a set of codes. Each code is represented\nby a network node and has multiple dimensions related to its conceptual nature.\nCombining multiple Code Spaces, we get a Aggregated Code Space (ACS) that covers all codes from all𝐶𝑜𝑑𝑖𝑛𝑔_𝑅𝑒𝑠𝑢𝑙𝑡𝑠\n(Fig 1A). Similar but not equivalent codes are now connected by links and considered as 𝑁𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠. The idea of ACS\nacknowledges that humans may not find all possible interpretations but could still build an \"aggregated\" set of codes\nto come closer. ACS is also a snapshot in time that documents researchers’ current efforts. It can be used to look for\nconvergence and divergence among the team members, supporting different stages of qualitative analysis. Convergence\nmay help researchers determine the current consensus and indicate each code’s visibility and importance. Divergence\nmay reveal underlying biases, different focuses, or missed opportunities for researchers. For example, a recent study\n[45] critiques overreliance on consensus building and advocates for more attention to “dissonances, disagreements, and\ndifferences.”\n6\n\nA Computational Method for Measuring “Open Codes” in Qualitative Analysis\n, ,\nThe concepts of CSP and ACS enable us to measure inductive coding results 𝐶𝑜𝑑𝑖𝑛𝑔_𝑅𝑒𝑠𝑢𝑙𝑡𝑠. While this paper\nmay have coined the term ACS, many qualitative researchers have long relied on similar ideas and practices (e.g.,\nThomas). Examination of ACS could happen during open coding, when researchers discuss what was discovered and\nnot discovered by individual team members to help them reorient the next coding batch; during axial coding, when\nresearchers collaborate to consolidate the ACS into codebooks; and during selective coding, when researchers truncate\nthe ACS to coalesce around a core category relevant to the research question.\n3.1.2\nConceptualize the Evaluating Metrics. We propose four conceptual metrics to measure individual CSPs against\ntheir aggregation ACS (Fig 1B), which will be operationalized in later sections:\n• Coverage and Density: How much conceptual space does a CSP cover, and how many codes does it use to\ncover this space? Both TA and GT strive for \"richness\" of codes. In practice, researchers need to strive for breadth\nand depth. Breadth means covering as diverse sets of concepts as possible. Depth means the descriptive details\nthat ensures the concept’s meaningfulness and richness, thus supporting researchers’ further analysis [11, 23].\nCombining them, researchers may be able to capture findings with depth and variation, two criteria for evaluating\nGT studies [23], to the extent that “nothing has been left out.”[35] On the other hand, not all codes are of equal\ninterest to researchers. Some concepts are more likely to be grounded in or relevant to the research question.\nThe metrics must account for each concept’s importance and weigh it accordingly.\n• Novelty: How much of the “novel” conceptual space does a CSP include? We define \"novel\" codes as ones that\nfewer than 𝑁𝑜𝑣𝑒𝑙𝑡𝑦_𝑇ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑CSPs have included. Most of the time, CSPs will not be (even close to) identical.\nSometimes, a researcher brings in novel ideas or lenses that were missed by others; in other cases, a researcher\ncould make a slip or inappropriately name a code. Identifying \"novel\" codes could help us identify either scenario\nand support human-human or human-AI collaboration.\n• Divergence: How far is a CSP’s code distribution from the ACS? Suppose two CSPs, A and B, have the same\ncoverage and density. However, A has more codes in common with most CSPs. B, on the other hand, has more\ncodes in common with no one else (\"novel\" codes) or few in the group. To detect this at the macro level, we\ncalculate each CSP’s divergence as the separation from its probability distribution of codes to that of the ACS,\nusing the latter as a “ground truth.” In other words, the closer a CSP concept distribution is to the ACS, the less\n\"divergent\" it is.\nSince our evaluation focuses more on the relative comparison between coding results, it does not directly address\npotential issues of groundedness, i.e., whether codes could be reasonably re-identified by another coder from the\nunderlying data. However, our metrics do provide an indirect pathway for human-AI collaboration: by looking at the\n\"novel\" parts of CSP and/or codes with very few examples, we may be able to identify codes with potential groundedness\nissues more easily.\n3.2\nThe Computational Method\nThe operationalization of our method starts from consolidating 𝐶𝑜𝑑𝑖𝑛𝑔_𝑅𝑒𝑠𝑢𝑙𝑡𝑠into an ACS for evaluation reference.\nEach 𝐶𝑜𝑑𝑖𝑛𝑔_𝑅𝑒𝑠𝑢𝑙𝑡should come from an individual machine or human coder. For 𝐶𝑜𝑑𝑖𝑛𝑔_𝑅𝑒𝑠𝑢𝑙𝑡to be treated as\na CSP, each 𝐶𝑜𝑑𝑒should have a 𝐿𝑎𝑏𝑒𝑙, an optional list of 𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛𝑠, and a list of 𝐸𝑥𝑎𝑚𝑝𝑙𝑒𝑠(i.e. the pieces of data\nwhere the code was identified). In the output ACS, each 𝐶𝑜𝑑𝑒will have a consolidated 𝐿𝑎𝑏𝑒𝑙, 𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛, a list of\n𝐸𝑥𝑎𝑚𝑝𝑙𝑒𝑠, a list of 𝑁𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠(other codes that are semantically close to this one), and a list of 𝑂𝑤𝑛𝑒𝑟𝑠(𝐶𝑜𝑑𝑖𝑛𝑔_𝑅𝑒𝑠𝑢𝑙𝑡𝑠\nthat have included one or more variations of the consolidated code). 𝑁𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠enable the network structure of ACS\n7\n\n, ,\nChen, et al.\nand the derivation of code 𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑠. Evaluating individual CSPs against the ACS, we can calculate 𝐶𝑜𝑣𝑒𝑟𝑎𝑔𝑒, 𝐷𝑒𝑛𝑠𝑖𝑡𝑦,\n𝑁𝑜𝑣𝑒𝑙𝑡𝑦, and 𝐷𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒as overall and cluster-level metrics. For anonymization reasons, we will open-source our\nimplementation after the paper’s acceptance.\n3.2.1\nConsolidating the ACS. Overall, consolidating an ACS involves multiple iterations of:\n(1) Finding equivalent codes and merge them into one.\n(2) Generate a new label and definition for merged codes.\n(3) Repeat the process with the new list of codes, until nothing more is merged.\nFinding equivalent codes is more complicated than it seems. Human or machine coders often use different phrases to\ndescribe the same or similar ideas. For example, suppose coder A identified \"user suggestion,\" while coder B found \"user\nsuggestions.\" They are clearly referring to the same idea. Suppose coder B found \"user feedback\" instead. Are A and B\nreferring to equivalent, similar, or different idea(s)? To determine that:\n(1) We encode each code with text embedding, transforming its 𝐿𝑎𝑏𝑒𝑙and 𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛(𝑠) into a high-dimensional\nvector.\n(2) We use cosine distance, a commonly used text similarity measure, to calculate the distance between codes.\n(3) We use a hierarchical clustering algorithm to choose merging candidates, since it does not specify the\nexpected number of results. For each node in the algorithm’s dendrogram tree structure, we apply two input\ndistance thresholds: 𝑙𝑜𝑤𝑒𝑟and 𝑢𝑝𝑝𝑒𝑟.\n(a) Different codes: Different codes with a distance above the 𝑢𝑝𝑝𝑒𝑟threshold will never be merged.\n(b) Equivalent codes: Very similar codes with a distance below the 𝑙𝑜𝑤𝑒𝑟threshold will always be merged.\n(c) For codes with a distance below the 𝑢𝑝𝑝𝑒𝑟but above the 𝑙𝑜𝑤𝑒𝑟, the algorithm penalizes2 1) the proportion\nof non-overlapping examples; and 2) the size of unique examples after merging (compared with the average\nexample sizes of all codes).\n(i) Also equivalent codes: If the distance is below the 𝑢𝑝𝑝𝑒𝑟after the penalty, the codes will be merged.\n(ii) Similar codes: If this is the last iteration, those codes become 𝑁𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠to each other.\n3.2.2\nImplementing the Consolidation. We implemented the consolidating process by merging closer codes first and\nfarther codes later. The purpose is to help LLMs create less general labels and definitions. We will discuss the choice of\ntext embedding models and parameters in 3.3.3.\n(1) We merge codes with exactly the same 𝐿𝑎𝑏𝑒𝑙;\n(2) We iteratively merge codes with very similar 𝐿𝑎𝑏𝑒𝑙(𝑢𝑝𝑝𝑒𝑟= 0.35, 𝑙𝑜𝑤𝑒𝑟= 0.35). Since codes in this step are\nusually very similar, to optimize the token usage, we simply use the shorter one. If the input does not include a\n𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛, we generate one for it.\n(3) We iteratively merge codes with similar 𝐿𝑎𝑏𝑒𝑙and 𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛(𝑢𝑝𝑝𝑒𝑟= 0.5, 𝑙𝑜𝑤𝑒𝑟= 0.4).\n(4) We iteratively merge codes with similar 𝐿𝑎𝑏𝑒𝑙and 𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛(𝑢𝑝𝑝𝑒𝑟= 0.6, 𝑙𝑜𝑤𝑒𝑟= 0.4). During the last iteration,\nwe consider all codes with distances under 𝑢𝑝𝑝𝑒𝑟to be 𝑁𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠.\n2The idea of penalty comes from our initial exploration, where merging by distance alone could lead to undesirable outcomes. For example, assume the\ncode \"user feedback\" is close enough to \"soliciting feedback\" and \"integrating feedback,\" yet the latter two are far enough apart. If an ACS starts with the\nlatter two codes, they will not be merged. Yet, by adding \"user feedback\" to the ACS, a straightforward algorithm will merge all three codes into one,\nlosing the nuances between \"soliciting\" and \"integrating.\" To prevent this, our algorithm penalizes \"oversized\" merging results. For example, since \"user\nfeedback\" will likely accumulate too many examples after merging the \"soliciting feedback\" and \"integrating feedback,\" the distance threshold to merge\ninto it will be stricter (lower). To reduce merging similarly named codes with different intentions, the algorithm also penalizes the differences between\ncodes’ examples.\n8\n\nA Computational Method for Measuring “Open Codes” in Qualitative Analysis\n, ,\nEach iteration of the merging process is implemented as the follows:\n(1) We calculate the penalty coefficient:\n𝑝𝑒𝑛𝑎𝑙𝑡𝑦= 𝑢𝑝𝑝𝑒𝑟−𝑙𝑜𝑤𝑒𝑟\n(1)\n(2) For each pair of codes 𝑥, 𝑦, we calculate their distance with penalty:\n(a) We calculate the cosine distance:\n𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑥,𝑦) = 1 −\n𝑥· 𝑦\n∥𝑥∥∥𝑦∥\n(2)\n(b) Whenever 𝑙𝑜𝑤𝑒𝑟< 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑥,𝑦) < 𝑢𝑝𝑝𝑒𝑟:\n(i) We calculate the percentage of the difference between examples of the two codes. Then, we squared the\nresult to reduce the penalty on small differences.\n𝑑𝑖𝑓𝑓(𝑥,𝑦) = ( #(𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠[𝑥] ∩𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠[𝑦])\n#(𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠[𝑥] ∪𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠[𝑦]) )2\n(3)\n(ii) We apply the penalty of different examples on the distance.\n𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑥,𝑦) = 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑥,𝑦) + 𝑑𝑖𝑓𝑓(𝑥,𝑦) ∗𝑝𝑒𝑛𝑎𝑙𝑡𝑦\n(4)\n(3) We use the hierarchical clustering algorithm to produce a dendogram tree structure from the distance matrix.\n(4) We iterate through the tree structure from the top to the bottom. Each node of the tree represents a potential\nmerge 𝑥, with a list of codes to merge; a 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑥); and its unique examples after merged 𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠(𝑥).\n(a) We calculate the percentage of over-sizing (clamped between 0%-200%):\n𝑜𝑣𝑒𝑟𝑠𝑖𝑧𝑒(𝑥) = 𝑐𝑙𝑎𝑚𝑝( #(𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠[𝑥])\n𝑎𝑣𝑔(#(𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠)) , 100%, 300%) −1\n(5)\n(b) We then calculate a new merging threshold with the penalty:\n𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑(𝑥) = 𝑢𝑝𝑝𝑒𝑟−(𝑜𝑣𝑒𝑟𝑠𝑖𝑧𝑒(𝑥) ∗0.5)2 ∗𝑝𝑒𝑛𝑎𝑙𝑡𝑦\n(6)\n(c) We only merge the codes when 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑥) < 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑(𝑥).\n3.2.3\nCalculating the Metrics. We now calculate the metrics in 3.1.2 to measure individual CSPs against the consolidated\nACS. To avoid CSPs with many redundant codes gaining an unfair advantage, whenever two or more codes are merged\nin the ACS, their CSP counterparts are also considered merged. Suppose coder A identified ten variations of the same\nconcept \"user suggestion\", our method will treat them as only one code as long as they are detected and merged.\nFirst, we calculate the 𝑤𝑒𝑖𝑔ℎ𝑡of each code using the sum of individual CSP’s coverage 𝑣𝑎𝑙𝑢𝑒for this code. The\ncoverage 𝑣𝑎𝑙𝑢𝑒is 1 for CSP that has the code, or a percentage based on how many 𝑛𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠the CSP includes compared\nwith the total number of 𝑛𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠of the code. For example, if two CSPs covered the same code and the third CSP\ncovered two out of four neighbors but not the code itself, the 𝑣𝑎𝑙𝑢𝑒would be 1, 1, 0.5. The 𝑤𝑒𝑖𝑔ℎ𝑡would be 2.5.\n𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝,𝑐𝑜𝑑𝑒) = 1,𝑖𝑓𝑐𝑠𝑝∈𝑜𝑤𝑛𝑒𝑟𝑠(𝑐𝑜𝑑𝑒)\n= #(𝑐𝑜𝑑𝑒∈𝑜𝑤𝑛𝑒𝑑𝑛𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠)\n#(𝑐𝑜𝑑𝑒∈𝑛𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠)\n(7)\n9\n\n, ,\nChen, et al.\n𝑤𝑒𝑖𝑔ℎ𝑡(𝑐𝑜𝑑𝑒) = 𝑣𝑎𝑙𝑢𝑒(𝑎𝑐𝑠,𝑐𝑜𝑑𝑒) =\n∑︁\n𝑐𝑠𝑝∈𝑎𝑐𝑠\n𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝,𝑐𝑜𝑑𝑒)\n(8)\nThen, we calculate the computational metrics weighted by the 𝑣𝑎𝑙𝑢𝑒of each code. For each CSP:\n(1) Coverage measures the proportion of the CSP’s total code 𝑣𝑎𝑙𝑢𝑒against the ACS’s total code 𝑤𝑒𝑖𝑔ℎ𝑡;\n𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒_𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝) =\n∑︁\n𝑐𝑜𝑑𝑒∈𝑎𝑐𝑠\n𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝,𝑐𝑜𝑑𝑒)\n(9)\n𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒(𝑐𝑠𝑝) = 𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒_𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝)\n𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒_𝑣𝑎𝑙𝑢𝑒(𝑎𝑐𝑠)\n(10)\n(2) Density measures the CSP’s relative density (i.e. how many consolidated codes does the CSP include for its\ncoverage) against the ACS;\n𝑑𝑒𝑛𝑠𝑖𝑡𝑦_𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝) =\n#(𝑐𝑜𝑑𝑒∈𝑐𝑠𝑝)\n𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒_𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝)\n(11)\n𝑑𝑒𝑛𝑠𝑖𝑡𝑦(𝑐𝑠𝑝) = 𝑑𝑒𝑛𝑠𝑖𝑡𝑦_𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝)\n𝑑𝑒𝑛𝑠𝑖𝑡𝑦_𝑣𝑎𝑙𝑢𝑒(𝑎𝑐𝑠) =\n#(𝑐𝑜𝑑𝑒∈𝑐𝑠𝑝)\n#(𝑐𝑜𝑑𝑒∈𝑎𝑐𝑠) ∗𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒_𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝)\n(12)\n(3) Novelty measures the proportion of the CSP’s total novel code 𝑣𝑎𝑙𝑢𝑒against the ACS’s total novel code 𝑤𝑒𝑖𝑔ℎ𝑡.\nIn this study, we used 𝑁𝑜𝑣𝑒𝑙𝑡𝑦_𝑇ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑= 1, i.e. a code is novel if only one codebook explicitly contains it;\n𝑛𝑜𝑣𝑒𝑙𝑡𝑦(𝑐𝑠𝑝) =\nÍ\n𝑐𝑜𝑑𝑒∈𝑐𝑠𝑝(𝑛𝑜𝑣𝑒𝑙=1) 𝑣𝑎𝑙𝑢𝑒(𝑐𝑠𝑝,𝑐𝑜𝑑𝑒)\nÍ\n𝑐𝑜𝑑𝑒∈𝑎𝑐𝑠(𝑛𝑜𝑣𝑒𝑙=1) 𝑣𝑎𝑙𝑢𝑒(𝑎𝑐𝑠,𝑐𝑜𝑑𝑒)\n(13)\n(4) Divergence is calculated as the seperation between the CSP and the ACS’s probability distribution. To measure\nthat, we normalized the CSP and the ACS’s values (or weights) into lists. Since individual CSPs may completely\nmiss a code and its neighbors, we chose the Jenson-Shannon Divergence (JSD) to tolerate the resulting zero\nelements. We reported its metric version, Jenson-Shannon Distance, by taking a square root (citation here).\n𝑑𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒(𝑐𝑠𝑝) =\n√︁\n𝐽𝑆𝐷(𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛(𝑐𝑠𝑝)||𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛(𝑎𝑐𝑠)\n(14)\nACS’s network structure enables the detection of code clusters (or communities, different from the clustering\nalgorithm we used for merging codes) and a more nuanced measurement. Since some codes may not have a neighbor,\nwe also assign links from each code’s three closest counterparts with a reduced weight for community detection with\nthe Louvain algorithm[8]. For reproducibility, we report communities identified with seed = 0. We apply the metrics\nwithin each cluster to answer more nuanced questions, e.g., whether a human or machine coder could have oversampled\nor undersampled specific portions of codes.\n3.3\nStudy Design\nWe conducted empirical experiments on two separate HCI datasets and research questions to demonstrate two use\ncases of our method. Since our measurement involves GAI in generating code labels and definitions, we validate its\nreliability by 1) comparing it with human evaluation results; and 2) analyzing its output stability.\n3.3.1\nTasks and Datasets. We experimented with open coding results on two HCI datasets, each with its research\nquestion and context.\n(1) Physics Lab’s online community dataset (127 messages from the beginning of the community) between designers\nand teacher users. The research question was: \"How did Physics Lab’s online community emerge?\" Four human\n10\n\nA Computational Method for Measuring “Open Codes” in Qualitative Analysis\n, ,\ncoders have previously open-coded the dataset. Three were PhD students, and one was an undergraduate\nassistant.\n(2) Two interviews from a CHI 2024 study[20], each last around two hours. The research question was: \"In the\ncontext of NetLogo learning and practice: What perceptions - strengths, weaknesses, and adoption plans - do\ninterviewees perceive in LLM-driven interfaces? How do they use it to support their work? What are their needs\nfor LLM-based interfaces?\" Three human coders have previously open-coded the dataset. Two were PhD students,\nand one had a master’s degree.\nWhile both datasets consist of many more items, human experts spent tens of hours finishing open coding of the\nsubset. On the other hand, since the goal is to identify concepts \"as exhaustive as possible,\" the small dataset enables us\nto evaluate ML/GAI’s potential in few-shot analyses.\n3.3.2\nChoice of ML/GAI Coding Approaches. In Case 1, we replicated five published ML/GAI approaches on open coding\nwith both datasets[19]. We provided the same research question and dataset information for humans and machines. We\nassigned the same roles and tasks to machine coders, such as “You are an expert in thematic analysis with grounded\ntheory, working on open coding.” The exact prompts, parameters and Dataset 1’s sample outputs can be found in[19],\nor supplementary materials of this paper. Here, we provide an overview of the five approaches:\n(1) BERTopic + LLM uses topic modeling[68], an unsupervised ML technique to identify groups of semantically\nsimilar words (i.e., topics) and explains the topics with LLMs. Since BERTopic’s instruction was intended for\ngeneral-purpose label-making, we gave additional instructions about the research question and contexts.\n(2) Chunk Level asks LLMs to identify open codes from chunks (e.g., a conversation, an interview, etc.) of data.\nMany variants of this approach have been adopted by recent papers (e.g., [41, 67]).\n(3) Chunk Level Structured asks LLMs to generate more than one level of concepts: the first for \"categories\" or\n\"themes,\" the second for \"codes\" or \"subcodes.\" Some recent papers have started to adopt this approach[19, 61].\n(4) Item Level asks LLMs to conduct line-by-line coding, as suggested by the grounded theory literature[34]. A few\npapers have adopted this approach to generate one [61] or multiple codes [19] per line.\n(5) Item Level with Verb Phrases builds on the previous approach but instructs LLMs to use verb phrases for\nlabels explicitly. The design was inspired by a grounded theory literature[25] and reported by a recent study[19].\nTable 1 presents an excerpt from Dataset 1, coded by four human and five machine approaches. All machine coders\nwere driven by the same model (GPT-4o-0513) with the same temperature (0.5).\n3.3.3\nChoice of Models and Hyperparameters. Our computational method utilizes text embedding and generation\nmodels during evaluation. For text embedding models, we consulted the MTEB leaderboard between May and June\n2024 and experimented with a few alternatives. For this study, we used gecko-768 from Google due to its relatively high\nperformance, low dimensions (with better computational efficiency), and easy accessibility.\nWe used cosine distance for thresholds in 3.2.2, where 0 means the two codes are identical; 1 for no correlation; and\n2 for absolutely different. During our initial experiments, we set the thresholds of 𝑙𝑜𝑤𝑒𝑟= 0.4 and 𝑢𝑝𝑝𝑒𝑟= 0.6 based on\ntwo indicators: 1) we referred to the distribution of pairwise distances and located local turning points; and 2) whether\nthe code pairs under the criteria meet our definition, where human can recognize codes closer than 𝑙𝑜𝑤𝑒𝑟as the same\nidea, while codes further than 𝑢𝑝𝑝𝑒𝑟are likely to be different. Different research contexts and text embedding models\nmay require different parameters.\n11\n\n, ,\nChen, et al.\nUser 4232\nIf there could be an export function, or the ability to save or import experiments, it would\nbe convenient. We could set up the parameters in the office and directly import them in class.\nBERTopic + LLM\nfeature requests for physics experiments\nChunk Level\ncommunity feedback loop, context of use, participatory design, user feedback and suggestions\nChunk Structured\nuser feedback, practical application, usability improvements, feature requests\nItem Level\nfeature request, usability improvement, classroom application, import/export functionality\nItem Verb Phrases\npropose additional features, emphasize convenience, suggest practical use case\n4 Human Coders\ncommunity feedback, feature request, propose another feature, gives reason for it\nDesigner\nThere will be.\nBERTopic + LLM\nfuture planning and development\nChunk Level\ndesigner responsiveness\nChunk Structured\ndesigner response, acknowledgment and implementation\nItem Level\ndesigner confirmation, feature implementation, future planning\nItem Verb Phrases\nconfirm future implementation, validate user request, plan feature development\n4 Human Coders\ndeveloper response, acknowledgement, acceptance of request, promising it will realize\nDesigner\nDoes the class have internet?\nBERTopic + LLM\ntechnical and infrastructural challenges in educational settings\nChunk Level\ncontextual constraints\nChunk Structured\nfeature updates and enhancements, user-designer interaction\nItem Level\ninternet availability inquiry, classroom setup, technical consideration\nItem Verb Phrases\ninquire about classroom conditions, gather context, consider technical requirements\n4 Human Coders\nengaging with community, seeking context, ask followup question on usage scenario\nUser 4232\nGenerally not. Ever since an adult image popped up during a major city-level open class,\nthe school has disabled the network on classroom computers [Emoji].\nBERTopic + LLM\ntechnical and infrastructural challenges in educational settings\nChunk Level\ncontext of use, contextual constraints\nChunk Structured\nfeature updates and enhancements, user-designer interaction\nItem Level\ninternet restriction, classroom environment, security concern, past incident\nItem Verb Phrases\nexplain lack of internet, provide context, share past incident\n4 Human Coders\ncontextualizing response, humor, personal anecdote, sharing information for design,\nstory sharing, gives an answer, explains the answer\nTable 1. An example exchange between a teacher user and a designer from our Dataset 1, coded by five machine coders and four\nhuman coders. We lightly merged very similar codes (e.g., ask a question vs. question) from the four human coders to save space.\nTo understand how the choice of model and temperature may influence the evaluation process, we used GPT-4o-0513\n(0513), GPT-4o-mini, and Llama3-70B because: 1) GPT-4o-0513 was one of the most powerful models (in terms of\nstate-of-the-art evaluation benchmarks) at the time of the experiment; 2) GPT-4o-mini was one of the most potent\nsmaller models; 3) Llama3-70B was one of the most potent open-source models.\n3.3.4\nExperiment Design. We conducted two case studies to understand two research questions:\n• What can we learn about existing ML/GAI approaches for open coding?\n• Is our method statistically reliable enough to evaluate open codes from machine and human coders?\nCase 1 examines the five machine coding approaches using 1) the overall metrics for machine and human coders; and\n2) the cluster-level metrics of relative coverage, a metric between -100% (completely undersampled) to +inf (extremely\noversampled). Two human researchers independently interpreted the theme for each network cluster (see 3.2.3) based\non its constituent codes and reconciled their differences into a single label.\n12\n\nA Computational Method for Measuring “Open Codes” in Qualitative Analysis\n, ,\n𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒𝑟𝑒𝑙𝑎𝑡𝑖𝑣𝑒(𝑐𝑠𝑝,𝑐𝑙𝑢𝑠𝑡𝑒𝑟) = 𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒(𝑐𝑠𝑝,𝑐𝑙𝑢𝑠𝑡𝑒𝑟)\n𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒(𝑐𝑠𝑝)\n−100%\n(15)\nCase 2 evaluated six mainstream LLMs’ performance in open coding with the same prompt: GPT-3.5-turbo; GPT-4o-\n0513; Llama3-70B; Mixtral 8x22b; Claude3-haiku; and Claude3.5-sonnet. 3 To save space, we only report the metrics\nfrom Item-level Verb Phrases, the best-performing GAI approach. The measurement includes all machine and human\ncoders’ results, enabling comparisons between LLMs; and between the aggregated results of LLMs and humans.\nThe reliability study evaluated our measurement’s outcome stability based on Case 1. We repeated the measurement\nwith three models, five temperatures (0, 0.25, 0.5, 0.75, 1). This results in 15 combinations of LLM and temperature. For\neach combination, we repeated 10 independent runs, totaling 150 runs. We recorded the evaluator LLM, temperature,\nrun number, coder identity, and four metrics. With those, we evaluated:\n• The coefficient of variation 𝑐𝑜𝑣for each metric, each CSP, and each combination of LLM and temperature. On\neach dataset, we ran fixed-effects regressions to understand if the choice of LLM, temperature, or CSP’s 𝑚𝑒𝑎𝑛\nvalue of metric influenced the 𝑐𝑜𝑣.\n• Whether using different combination of LLM and temperature impacts pairwise comparisons. On\neach dataset, we ran a series of pairwise ANOVA experiments and recorded significant pairs on 1) the entire\noutput from 150 runs; 2) each of the 15 combinations. We calculated the intersection of all pairs and looked for\nany individual combination that would produce different conclusions than the entirety. For example, suppose\nwe found the Item-Level approach’s coverage significantly higher than the Chunk-Level approach. If we only\nevaluated with one LLM and one temperature, will the conclusion change?\nSince we found the choice of LLM and temperature to have no significant impact on the pairwise comparison\noutcome, we only conducted 10 evaluation runs with Llama 3-70B at 0.5 temperature for Case 2.\n3.4\nEmpirical Results\n3.4.1\nCase 1: Evaluating Machine Coding Approaches. Figure 2, 3 demonstrates 𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒and 𝑑𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒, two of the\nmore stable metrics (see 3.4.3), for all machine and human coders in both datasets. We noted:\n• Similar to a previous human evaluation[19], we found that item-level coding approaches have higher coverage\nand lower divergence than chunk-level and BERTopic approaches.\n• While the aggregation of human researchers has higher coverage and lower divergence than any single machine\ncoding approach in coverage and most in divergence, the aggregation of machine coders has higher coverage and\nlower divergence than each and all human researchers. Note that we do not claim that machines outperformed\nhumans, and an ongoing study is currently examining the implications.\n• Note that the numbers of codes do not strictly correlate with the coverage metric. For example, in Dataset 1, the\nItem-Level Verb-Phrase approach found 282 raw codes with 79.1% coverage, while the aggregation of human\ncoders found 340 raw codes with only 75.5%.\nWe further explored the potential biases of human and machine coders with cluster-level metrics. Tables 2, 3 presents\nclusters for both datasets’ ACS and the relative coverage of each CSP. Clusters are sorted by the sum of their component\ncodes’ weights. Thus, clusters with more codes and higher consensus levels are listed first. Numbers higher than\n3The list does not include GPT-4o-mini, released after we completed the coding task.\n13\n\n, ,\nChen, et al.\nFig. 2. The coverage metric for all coding approaches, Dataset 1 (with four humans) and 2 (with three humans). For grounded\ntheory open coding, higher is better.\nFig. 3. The divergence metric for all coding approaches, Dataset 1 (with four humans) and 2 (with three humans).\n0 represent oversampling of codes, while numbers lower than 0 represent undersampling. The standard deviation\nindicates the degree of imbalance between clusters.\nWe immediately noted that almost all machine and human coders had completely missed some clusters from the\ndata. Across the two datasets, the only exception is Item-level LLM coding with verb phrases. It also has the lowest 𝑠𝑡𝑑\nof relative coverage across all individual coders, showing its comparatively uniform sampling across all code clusters\ncomparable with the aggregation of all human coders. Examining the qualitative nature of missed or undersampled\n14\n\nA Computational Method for Measuring “Open Codes” in Qualitative Analysis\n, ,\nID\n#\nBERTopic\nChunk\nChunk-S\nItem\nItem-V\nAll AI\nAll Human\n1\n34\n+2.30%\n+9.90%\n+13.80%\n+26.40%\n+12.00%\n+6.60%\n+14.40%\n2\n45\n+99.10%\n+16.00%\n-43.10%\n+12.10%\n-1.50%\n+3.80%\n-12.20%\n3\n26\n+73.10%\n+59.20%\n+58.20%\n+15.20%\n+15.20%\n+3.30%\n+25.80%\n4\n28\n+1.40%\n+62.80%\n+46.00%\n-5.00%\n+13.60%\n+2.20%\n+12.20%\n5\n26\n-75.00%\n+29.50%\n+44.90%\n-17.20%\n+3.10%\n-3.80%\n+8.30%\n6\n35\n-86.20%\n-65.80%\n-65.90%\n-19.90%\n-40.50%\n-13.50%\n-2.70%\n7\n20\n+12.40%\n+36.50%\n+17.50%\n+28.10%\n+13.40%\n+9.30%\n-6.50%\n8\n23\n-14.20%\n-80.50%\n-37.90%\n-1.80%\n+0.00%\n-4.50%\n-6.30%\n9\n20\n+1.80%\n-55.00%\n-19.30%\n+12.50%\n-14.80%\n-1.10%\n-19.40%\n10\n16\n+20.20%\n+31.90%\n+41.70%\n-1.00%\n-9.60%\n+5.40%\n-19.70%\n11\n16\n-100.00%\n-70.40%\n+30.30%\n-40.70%\n+10.10%\n-0.50%\n+5.10%\n12\n16\n+116.10%\n-20.40%\n-100.00%\n-42.40%\n-27.10%\n-18.90%\n-8.30%\n13\n11\n-100.00%\n-48.30%\n-7.10%\n-2.00%\n+11.30%\n+6.60%\n+5.90%\n14\n14\n-100.00%\n-100.00%\n-100.00%\n-54.60%\n-32.60%\n-3.80%\n-64.70%\n15\n10\n-100.00%\n-100.00%\n-80.70%\n-54.60%\n-5.20%\n+1.70%\n-55.90%\n16\n10\n-100.00%\n-100.00%\n-100.00%\n-50.50%\n-42.50%\n-29.40%\n-51.90%\n17\n6\n-100.00%\n-100.00%\n-100.00%\n-100.00%\n-57.90%\n-63.00%\n-11.70%\nStd\n75.26%\n60.30%\n59.03%\n34.98%\n23.10%\n17.81%\n25.37%\nTable 2. ACS of Dataset 1: Clusters of Codes, and Each Coding Approach’s Relative Coverage (-100% = Completely Missed; +Inf =\nInfinitely Oversampled).\nclusters is crucial to understanding potential bias further. It allows researchers to situate the analysis in the context of\nthe data and research questions. Below, we briefly interpret the clusters missed by each approach:\n• BERTopic + LLM. The coverage of BERTopic is low (21.1%) and highly unevenly distributed among clusters.\nThe seemingly oversampling on cluster 12 (designers’ reactions to user suggestions) was only because of the low\nbaseline number, as it only included two (plus one neighbor) out of 14 codes. Moreover, BERTopic misses clusters\n13-17, where other coders noted many details about the design exchanges, such as needs, design considerations,\nand communication strategies.\n• Chunk-level approaches. Both approaches have relatively low coverage (39.2%, 43.2%). The structured approach\nhas slightly better coverage, but the overall unevenness level within clusters stays similar, in addition to completely\nmissing cluster 12. Both start to cover cluster 13 (designers’ apologies, explanations, and reassurance to users), yet\nstill miss clusters 14-17. Both approaches also oversample positive expressions (e.g., cluster 3) while undersampling\nclusters with negative implications (e.g., clusters 6 and 12).\n• Item-level approaches. Both approaches have high coverage (74.7%, 76.6%). The verb phrase approach has\nslightly better coverage and unevenness level and avoids the baseline approach’s miss of cluster 17 (designers’\nresponse strategies). This is not a small achievement: all other coders except one human missed this cluster of 6\ncodes altogether.\nWe observed similar trends in Dataset 2, where BERTopic, chunk-level LLM coding, and human coders missed many\nclusters around interviewees’ nuanced reflections (clusters 12-19). Moreover, item-level coding approaches continued\nto do better than other approaches. Notably, human coders miss cluster 19 (reflecting and criticizing working and\nhelp-seeking cultures) altogether.\n15\n\n, ,\nChen, et al.\nID\n#\nBERTopic\nChunk\nChunk-S\nItem\nItem-V\nAll AI\nAll Human\n1\n21\n-30.10%\n+8.10%\n-48.50%\n+18.60%\n+14.90%\n+7.60%\n+13.00%\n2\n26\n-53.00%\n+68.50%\n+39.50%\n+1.70%\n+13.90%\n-0.40%\n+23.70%\n3\n24\n+83.00%\n-22.80%\n+43.70%\n+8.50%\n-3.00%\n-2.40%\n+39.10%\n4\n22\n+151.50%\n-62.40%\n+104.80%\n+13.00%\n+4.20%\n+0.30%\n+16.30%\n5\n20\n+134.40%\n-36.00%\n+96.10%\n-7.00%\n+4.10%\n-2.80%\n+14.00%\n6\n15\n+86.20%\n+124.90%\n-38.80%\n+16.60%\n-10.20%\n+4.70%\n-11.50%\n7\n15\n-17.50%\n-100.00%\n-63.00%\n+16.50%\n+3.00%\n+4.70%\n+22.20%\n8\n16\n-55.80%\n+54.10%\n-100.00%\n+1.70%\n+5.90%\n-4.40%\n+12.60%\n9\n17\n-81.70%\n+129.60%\n+56.30%\n-14.40%\n-10.90%\n+0.20%\n-14.00%\n10\n16\n-100.00%\n+49.40%\n+1.70%\n-9.50%\n+4.10%\n+3.40%\n-52.00%\n11\n12\n-36.40%\n+55.30%\n+5.70%\n-11.10%\n-1.60%\n-5.30%\n-6.40%\n12\n14\n-100.00%\n-100.00%\n-48.50%\n-10.90%\n-16.10%\n-2.20%\n-36.20%\n13\n14\n-100.00%\n-100.00%\n-48.50%\n-10.90%\n+11.80%\n+7.60%\n-85.80%\n14\n16\n-100.00%\n+75.20%\n-100.00%\n-51.80%\n-54.70%\n-15.10%\n-42.50%\n15\n10\n-100.00%\n-100.00%\n-100.00%\n-4.20%\n-1.60%\n+7.60%\n-58.40%\n16\n7\n-41.10%\n-100.00%\n-100.00%\n-27.40%\n-18.00%\n+7.60%\n-48.00%\n17\n7\n-100.00%\n-100.00%\n-100.00%\n-81.30%\n-47.30%\n-38.50%\n-33.20%\n18\n5\n-100.00%\n-100.00%\n-46.00%\n-44.00%\n-29.70%\n-23.20%\n+11.40%\n19\n5\n-100.00%\n-100.00%\n-46.00%\n-6.60%\n-12.10%\n+7.60%\n-100.00%\nStd\n87.17%\n87.28%\n68.77%\n25.24%\n18.97%\n12.03%\n39.81%\nTable 3. ACS of Dataset 2: Clusters of Codes, and Each Coding Approach’s Relative Coverage (-100% = Completely Missed; +Inf =\nInfinitely Oversampled).\n3.4.2\nCase 2: Benchmarking Mainstream LLMs. In Figures 4 and 5, we applied our method to benchmark six mainstream\nLLMs in mid-2024. We noted:\n• Significant gaps in different models’ coverage and divergence. In both datasets, we found GPT-4o, Claude 3.5\nSonnet, and Llama3-70B significantly outperformed GPT-3.5-Turbo, Claude 3 Haiku, and Mixtral 8x22B. Without\ndesignating a ground truth, our methods roughly distinguish larger, more \"powerful\" models (measured by\ncommon ML benchmarks, e.g., MMLU) from smaller ones.\n• Machine coders’ performances were not always consistent among the two datasets. For example, all models\nbut Claude 3 Haiku have seen a performance drop in Dataset 2, with Claude 3.5 Sonnet dropping almost 12%.\n• Machine coders’ aggregated result consistently achieved better coverage, density, and divergence than\nhuman coders’ aggregated results, with coverages around 95% and significantly lower divergences in both\ndatasets. Even when most individual models’ performance dropped in Dataset 2, the performance of machine\ncoders’ aggregation almost stayed the same.\n• We also noted the performance of aggregated human coders dropping in Dataset 2. One potential reason: only 3\nhuman coders conducted open coding for Dataset 2, compared with 4 for Dataset 2.\n3.4.3\nReliability Study. Figures 6, 7 demonstrate the outcome variance of each metric across three LLMs on two datasets.\nWe noted:\n• Overall, 𝑑𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒consistently has the lowest standard variance (𝑠𝑡𝑑, 2-3%) and coefficient of variance (𝑐𝑜𝑣,\n1.5-5.5%), while 𝑛𝑜𝑣𝑒𝑙𝑡𝑦has the highest 𝑠𝑡𝑑(3-6%) and 𝑐𝑜𝑣(8-65%).\n16\n\nA Computational Method for Measuring “Open Codes” in Qualitative Analysis\n, ,\nFig. 4. The coverage metric for six LLMs, Dataset 1 (with four humans) and 2 (with three humans). For grounded theory open\ncoding, higher is better.\nFig. 5. The divergence metric for six LLMs, Dataset 1 (with four humans) and 2 (with three humans).\n• After controlling the temperature and coder identities, the model choices occasionally have a significant\nbut small impact. However, there is no relationship between the sizes of models and the 𝑐𝑜𝑣impacts, and no\nmodels have consistently lower 𝑐𝑜𝑣.\n• The model temperature does not have a significant impact.\n• For 𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒, 𝑑𝑒𝑛𝑠𝑖𝑡𝑦, and 𝑛𝑜𝑣𝑒𝑙𝑡𝑦, their 𝑐𝑜𝑣are negatively correlated with individual coders’ 𝑚𝑒𝑎𝑛value of the\nmetric. For example, a coder with higher 𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒will likely have a lower 𝑐𝑜𝑣than a coder with lower 𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒.\n17\n\n, ,\nChen, et al.\nFig. 6. Coefficient of Variance for Each Metric among LLMs, Dataset 1\nFig. 7. Coefficient of Variance for Each Metric among LLMs, Dataset 2\nSince running 150 evaluation runs for a single comparison study may be cost-ineffective, we evaluated our method’s\nreliability when only one model, and temperature, and 10 evaluation runs are used. Here, we assume the pairwise\ncomparison results of 150 runs as the ground truth. Across all 15 combinations on four metrics and two datasets, we only\nidentified two cases of false positives for a pair with a minuscule mean difference of 𝑑𝑒𝑛𝑠𝑖𝑡𝑦(BERTopic vs. Chunk-level\nStructured, 32.71% vs. 32.41%) in Dataset 1. Besides that, we only identified false negatives for comparison pairs with\nsmall mean differences. In other words, unless the goal is to compare metrics with small numerical differences, 10 runs\nwould be generally sufficient.\n3.5\nDiscussions\n3.5.1\nReliability of the Measurement. While using GAI in our method inevitably introduces stochasticity and potential\nbiases, we established the reliability of our computational measurement with the reliability study (3.4.3).\n18\n\nA Computational Method for Measuring “Open Codes” in Qualitative Analysis\n, ,\nOur method’s only source of stochasticity comes from GAI’s generation of code definitions and labels, which\ninfluences the downstream decision of code merging. Naturally, coding results with fewer codes will be impacted\nmore, and our empirical results support this assumption. In plain words, our method provides more stable outcomes at\nevaluating “better” coding results but less so for “worse” ones. This is especially true for 𝑛𝑜𝑣𝑒𝑙𝑡𝑦, which measures the\nnumber of codes that no one else has identified. Therefore, we only recommend using 𝑛𝑜𝑣𝑒𝑙𝑡𝑦as a qualitative indicator\nfor potential outliers, while using 𝑑𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒, 𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒, and 𝑑𝑒𝑛𝑠𝑖𝑡𝑦for quantitative comparisons.\nWe confirmed that using a single LLM and temperature does not generally lead to false reports on paired comparisons.\nIn only one case, 10 runs with a single LLM and temperature caused a false positive against the entirety of 150 runs.\nMoreover, the result shows that our method does not rely on the most powerful models. Open-source models with the\npotential for local deployment and better privacy protection (such as Llama3-70B) or smaller models with cheaper costs\nand less ecological footprints (such as GPT-4o-mini) will likely work for most scenarios.\n3.5.2\nSuggestions for Using ML/GAI in Inductive Coding. Case 1 reaches similar conclusions as a previous human\nevaluation[19]: the item-level approaches performed best on both datasets and research questions. The cluster-level\nmetrics enabled us to compare ML/GAI coding approaches with more nuances. BERTopic and chunk-level approaches\nhad worse overall metrics, missed entire clusters of codes, and were less capable of identifying nuances of human\ninteractions, particularly ones with seemingly less connection with the research question. They also oversampled\npositive emotions or feedback, implying a potential bias for their coding results.\nIn contrast, the success of item-level approaches suggests the potential of embedding human processes for qualitative\nanalysis into LLM prompts. While we started mentioning grounded theory in the Chunk-Level Structured approach, its\nperformance was not much better than the Chunk-Level. The situation changed when we instructed LLMs to strictly\nfollow the grounded theory process[62]: by coding the data item-by-item, the item-level approaches produced much\nbetter results. Following existing literature[25], our adoption of verb phrases as labels further enabled a more nuanced\ninterpretation of the data.\nCase 2 provides a quick assessment of state-of-the-art LLMs available in mid-2024. Our finding suggests that existing\nNLU benchmarks for LLMs may positively correlate with their performance on inductive qualitative coding. On the\nother hand, the same LLMs may perform differently on different research questions and datasets, necessitating broader\nevaluative studies. While we suggest researchers use the best available model, our findings also indicate the feasibility of\npowerful open-source models such as Llama-3 70B, particularly for scenarios where data privacy concerns are stronger.\nMoreover, we suggest using multiple models simultaneously as an even better approach. The combination of six models\nconsistently covered almost all codes identified by human researchers, even when individual performances varied\nacross datasets.\n3.5.3\nHuman-AI Collaboration in Interpretation. This work’s core contribution is a novel computational measurement\nto understand, compare, and evaluate open codes from multiple machine or human coders. Hence, it is crucial to share\nhow we interpreted the algorithmic results and discuss scenarios where our measurement could contribute to human-AI\ncollaboration in open coding processes.\nThe nature of qualitative research prevents us from making a general claim with only two datasets and research\nquestions, yet this is more of a feature than a bug. Both the open codes and the corresponding evaluation are naturally\nbounded by the datasets, research questions, and perspectives researchers adopted. Thus, results from our measurement\nmust be interpreted in context. Below, we present an example flow based on Table 2 on Dataset 1:\n19\n\n, ,\nChen, et al.\n• A researcher first looked at the general metrics of each CSP in a random evaluation run of Case 1, where BERTopic\nhad the highest 𝑑𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒across all runs. Since the researcher believes the ACS to be reasonably well-covered\ndue to the presence of multiple human coders, it seems that BERTopic produced the most deviant results.\n• Here, two possibilities exist: either BERTopic identified something novel and insightful, or it simply missed too\nmany codes. To evaluate this, the researcher checked the 𝑛𝑜𝑣𝑒𝑙𝑡𝑦metric and found BERTopic the lowest again.\nSo it is unlikely to be the first situation; in fact, it only has 3 codes that no one else found. Those codes are all\nrelated to aspects of the software feature, which is far from the research question about community formation.\n• Note that BERTopic only identified 23 codes. For low-level coding approaches that found hundreds of codes, the\nresearcher may need additional support to understand their potential biases. Therefore, the researcher clustered\ncodes in the ACS and calculated cluster-level metrics.\n• The researcher found that the Item-Level approach misses and under-samples more clusters than the Item-Level\nVerb Phrase approach, indicating a higher risk for potential bias. From there, based on the research goals and\nquestions, the researcher qualitatively interpreted the two approaches’ outcome differences and decided to adopt\nthe Item-Level Verb Phrase approach for the rest of the coding process.\nWhile computational measurement is crucial in the process, such interpretations are only possible through a\nqualitative understanding of the codes and clusters. This paper’s human evaluation focuses on the open coding\nprocesses, where every grounded interpretation is welcomed. However, when researchers conduct further analysis steps,\ndepending on the research context, a code could become too nuanced and with too few cases; too broad and covers\neverything; irrelevant to the core aspects of the research question; or has been reported many times before. While our\nconvergence metrics automatically devalue outlier codes from few coders, it is still possible that a few \"well-performed\"\napproaches or models actually contribute little to the final analysis. It still relies on humans (potentially, in collaboration\nwith AI) to make these interpretations and decisions, opening up opportunities for further research work and interface\ndesign.\nThe potential of our evaluation method is not limited to machine coders alone. In Case 1, we presented how individual\nhuman coders may have missed insights from the data. During our study process, we found the visualization of ACS’s\nnetwork structure particularly useful, as it revealed the different focuses of human coders and facilitated fruitful\ndiscussion around the research question. Moving forward, we will focus on design work that can materialize the\nmethod’s potential in supporting human coders, with or without machine coders in ACS.\nACKNOWLEDGMENTS\nWe wholeheartedly acknowledge the intellectual contribution of the following researchers: Prof. Golnaz Arastoopour\nIrgens; Prof. David Ribes; Prof. Elizabeth Gerber; Prof. Foad Hamidi; Prof. Xi Lu; and members of the Center for\nConnected Learning and Computer-Based Modeling & TIDAL lab at Northwestern University. We also want to thank a\ngroup of scholars who gave insightful feedback: Zilin Ma and other anonymous friends. In addition, we acknowledge\nthe contribution of our research intern, Lily Yang, who participated in the data analysis of our first dataset.\nREFERENCES\n[1] Anne Adams, Ann Blandford, and Peter Lunt. 2005. Social empowerment and exclusion: A case study on digital libraries. ACM Transactions on\nComputer-Human Interaction 12, 2 (June 2005), 174–200. https://doi.org/10.1145/1067860.1067863\n[2] Anne Adams, Peter Lunt, and Paul Cairns. 2008. A Qualitative Approach to HCI Research. In Research Methods for Human-Computer Interaction.\nCambridge University Press.\n20\n\nA Computational Method for Measuring “Open Codes” in Qualitative Analysis\n, ,\n[3] Julian Ashwin, Aditya Chhabra, and Vijayendra Rao. 2023. Using large language models for qualitative analysis can introduce serious bias. arXiv\npreprint arXiv:2309.17147 (2023).\n[4] Jennifer Attride-Stirling. 2001. Thematic networks: an analytic tool for qualitative research. Qualitative Research 1, 3 (Dec. 2001), 385–405.\nhttps://doi.org/10.1177/146879410100100307\n[5] Eric P. S. Baumer, David Mimno, Shion Guha, Emily Quan, and Geri K. Gay. 2017. Comparing grounded theory and topic modeling: Extreme divergence\nor unlikely convergence? Journal of the Association for Information Science and Technology 68, 6 (2017), 1397–1410. https://doi.org/10.1002/asi.23786\n_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.23786.\n[6] Izhak Berkovich. 2018. Beyond qualitative/quantitative structuralism: The positivist qualitative research and the paradigmatic disclaimer. Quality &\nQuantity 52, 5 (2018), 2063–2077. https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11135-017-0607-3&\ncasa_token=JlBYcYvjqpwAAAAA:Qn6arDr_zncTUy62N2jsyRQgPblLU2nVHRlN6nUkQib4UkbPw4-HgrsOhMi0Bhsz-lFEhcU2DbSDb58o Publisher:\nSpringer.\n[7] Andrea Bingham and Patricia Witkowsky. 2021. Deductive and Inductive Approaches to Qualitative Data Analysis. In Analyzing and Interpreting\nQualitative Research: After the Interview. SAGE Publications, Inc. https://books.google.com/books?hl=en&lr=&id=0xIoEAAAQBAJ&oi=fnd&pg=\nPA133&dq=deductive+coding+qualitative&ots=1wZ007ufMy&sig=tLe5PscNzbMG_yMvXduwFPOquvY#v=onepage&q=deductive%20coding%\n20qualitative&f=false\n[8] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. Fast unfolding of communities in large networks. Journal\nof statistical mechanics: theory and experiment 2008, 10 (2008), P10008.\n[9] Robert Bowman, Camille Nadal, Kellie Morrissey, Anja Thieme, and Gavin Doherty. 2023. Using Thematic Analysis in Healthcare HCI at\nCHI: A Scoping Review. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. ACM, Hamburg Germany, 1–18.\nhttps://doi.org/10.1145/3544548.3581203\n[10] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative Research in Psychology 3, 2 (Jan. 2006), 77–101.\nhttps://doi.org/10.1191/1478088706qp063oa\n[11] Virginia Braun and Victoria Clarke. 2013. Successful qualitative research: A practical guide for beginners. (2013).\n[12] Virginia Braun and Victoria Clarke. 2021. One size fits all? What counts as quality practice in (reflexive) thematic analysis? Qualitative Research in\nPsychology 18, 3 (July 2021), 328–352. https://doi.org/10.1080/14780887.2020.1769238\n[13] Virginia Braun and Victoria Clarke. 2022. Conceptual and design thinking for thematic analysis. Qualitative Psychology 9, 1 (Feb. 2022), 3–26.\nhttps://doi.org/10.1037/qup0000196\n[14] Joy D. Bringer, Lynne H. Johnston, and Celia H. Brackenridge. 2004. Maximizing Transparency in a Doctoral Thesis1: The Complexities of\nWriting About the Use of QSR*NVIVO Within a Grounded Theory Study. Qualitative Research 4, 2 (Aug. 2004), 247–265. https://doi.org/10.1177/\n1468794104044434\n[15] Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z Gajos. 2021. To trust or to think: cognitive forcing functions can reduce overreliance on AI in\nAI-assisted decision-making. Proceedings of the ACM on Human-computer Interaction 5, CSCW1 (2021), 1–21.\n[16] Courtni Byun, Piper Vasicek, and Kevin Seppi. 2024. Chain of Thought Prompting for Large Language Model-driven Qualitative Analysis. In\nProceedings of the 2024 CHI Conference on Human Factors in Computing Systems (CHI ’24). Association for Computing Machinery.\n[17] Lara Carminati. 2018. Generalizability in Qualitative Research: A Tale of Two Traditions. Qualitative Health Research 28, 13 (Nov. 2018), 2094–2101.\nhttps://doi.org/10.1177/1049732318788379\n[18] M. Ariel Cascio, Eunlye Lee, Nicole Vaudrin, and Darcy A. Freedman. 2019. A Team-based Approach to Open Coding: Considerations for Creating\nIntercoder Consensus. Field Methods 31, 2 (May 2019), 116–130. https://doi.org/10.1177/1525822X19838237\n[19] John Chen, Alexandros Lotsos, Lexie Zhao, Grace Wang, Uri Wilensky, Bruce Sherin, and Michael Horn. 2024. Prompts Matter: Comparing ML/GAI\nApproaches for Generating Inductive Qualitative Coding Results. arXiv:2411.06316 [cs.CL] https://arxiv.org/abs/2411.06316\n[20] John Chen, Xi Lu, Yuzhou Du, Michael Rejtig, Ruth Bagley, Michael S. Horn, and Uri J. Wilensky. 2024. Learning Programming of Agent-based\nModeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat. In Proceedings of the 2024 CHI Conference on\nHuman Factors in Computing Systems.\n[21] Ned Cooper, Tiffanie Horne, Gillian Hayes, Courtney Heldreth, Michal Lahav, Jess Scon Holbrook, and Lauren Wilcox. 2022. A Systematic Review\nand Thematic Analysis of Community-Collaborative Approaches to Computing Research. In CHI Conference on Human Factors in Computing Systems.\n1–18. https://doi.org/10.1145/3491102.3517716 arXiv:2207.04171 [cs].\n[22] Juliet Corbin and Anselm Strauss. 2008. Chapter 10 / Analyzing Data for Concepts. In Basics of Qualitative Research (3rd ed.): Techniques and\nProcedures for Developing Grounded Theory. SAGE Publications, Inc., 2455 Teller Road, Thousand Oaks California 91320 United States.\nhttps:\n//doi.org/10.4135/9781452230153\n[23] Juliet Corbin and Anselm Strauss. 2008. Chapter 14 / Criteria for Evaluation. In Basics of Qualitative Research (3rd ed.): Techniques and Procedures for\nDeveloping Grounded Theory. SAGE Publications, Inc., 2455 Teller Road, Thousand Oaks California 91320 United States. https://doi.org/10.4135/\n9781452230153\n[24] Juliet M. Corbin and Anselm Strauss. 1990. Grounded theory research: Procedures, canons, and evaluative criteria. Qualitative sociology 13, 1 (1990),\n3–21. Publisher: Springer.\n[25] Natalie R. Davis, Shirin Vossoughi, and John F. Smith. 2020. Learning from below: A micro-ethnographic account of children’s self-determination\nas sociopolitical and intellectual action. Learning, Culture and Social Interaction 24 (2020), 100373.\nhttps://www.sciencedirect.com/science/\n21\n\n, ,\nChen, et al.\narticle/pii/S2210656119302107?casa_token=R60cWsxYBRgAAAAA:JTuwj92trq-YVKvmrCwtjFxlRCgBsFxzdpnKu2WNMa9l2OqmQ8wO_h_\nqvLqL49FXXQAx1gXPqw Publisher: Elsevier.\n[26] Stefano De Paoli. 2023. Performing an Inductive Thematic Analysis of Semi-Structured Interviews With a Large Language Model: An Exploration\nand Provocation on the Limits of the Approach. Social Science Computer Review 0, 0 (Dec. 2023), 1–23. https://doi.org/10.1177/08944393231220483\n[27] Stefano De Paoli and Walter S Mathis. 2024. Reflections on inductive thematic saturation as a potential metric for measuring the validity of an\ninductive thematic analysis with LLMs. Quality & Quantity (2024), 1–27.\n[28] Jennifer Fereday and Eimear Muir-Cochrane. 2006. Demonstrating Rigor Using Thematic Analysis: A Hybrid Approach of Inductive and Deductive\nCoding and Theme Development. International Journal of Qualitative Methods 5, 1 (March 2006), 80–92. https://doi.org/10.1177/160940690600500107\n[29] Nick J Fox. 2008. Post-positivism. The SAGE encyclopedia of qualitative research methods 2, 1 (2008), 659–664.\n[30] Dominic Furniss, Ann Blandford, and Paul Curzon. 2011. Confessions from a grounded theory PhD: experiences and lessons learnt. In Proceedings of\nthe SIGCHI Conference on Human Factors in Computing Systems. ACM, Vancouver BC Canada, 113–122. https://doi.org/10.1145/1978942.1978960\n[31] Jie Gao, Kenny Tsu Wei Choo, Junming Cao, Roy Ka-Wei Lee, and Simon Perrault. 2023. CoAIcoder: Examining the Effectiveness of AI-assisted\nHuman-to-Human Collaboration in Qualitative Analysis. ACM Transactions on Computer-Human Interaction 31, 1 (Nov. 2023), 6:1–6:38. https:\n//doi.org/10.1145/3617362\n[32] Susan Gasson. [n. d.]. Rigor In Grounded Theory Research: An Interpretive Perspective on Generating Theory From Qualitative Field Studies.\n([n. d.]).\n[33] Simret Araya Gebreegziabher, Zheng Zhang, Xiaohang Tang, Yihao Meng, Elena L. Glassman, and Toby Jia-Jun Li. 2023. PaTAT: Human-AI\nCollaborative Qualitative Coding with Explainable Interactive Rule Synthesis. In Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems (CHI ’23). Association for Computing Machinery, New York, NY, USA, 1–19. https://doi.org/10.1145/3544548.3581352\n[34] Graham R. Gibbs. 2007. Thematic coding and categorizing. Analyzing qualitative data 703, 38-56 (2007). https://study.sagepub.com/sites/default/\nfiles/analyzing-qualitative-da.pdf\n[35] Barney G Glaser, Judith Holton, et al. 2004. Remodeling grounded theory. In Forum qualitative sozialforschung/forum: qualitative social research,\nVol. 5.\n[36] Maarten Grootendorst. 2022. BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794 (2022).\n[37] Egon G Guba and Yvonn A S Lincoln. 1994. Competing Paradigms in Qualitative Research. In Handbook of qualitative research. Sage Publications,\nInc., 105–117.\n[38] Greg Guest, Emily Namey, and Mario Chen. 2020. A simple method to assess and report thematic saturation in qualitative research. PloS one 15, 5\n(2020), e0232076. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0232076 Publisher: Public Library of Science San Francisco, CA\nUSA.\n[39] Leah Hamilton, Desha Elliott, Aaron Quick, Simone Smith, and Victoria Choplin. 2023. Exploring the Use of AI in Qualitative Analysis: A\nComparative Study of Guaranteed Income Data. International Journal of Qualitative Methods 22 (March 2023), 16094069231201504.\nhttps:\n//doi.org/10.1177/16094069231201504 Publisher: SAGE Publications Inc.\n[40] Jasy Suet Yan Liew, Nancy McCracken, Shichun Zhou, and Kevin Crowston. 2014. Optimizing Features in Active Machine Learning for Complex\nQualitative Content Analysis. In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, Cristian\nDanescu-Niculescu-Mizil, Jacob Eisenstein, Kathleen McKeown, and Noah A. Smith (Eds.). Association for Computational Linguistics, Baltimore,\nMD, USA, 44–48. https://doi.org/10.3115/v1/W14-2513\n[41] Saríah Lopez-Fierro and Ha Nguyen. 2024. Making Human-AI Contributions Transparent in Qualitative Coding. (2024). https://repository.isls.org/\n/handle/1/10537 Publisher: International Society of the Learning Sciences.\n[42] Andrew Lowe, Anthony C. Norris, A. Jane Farris, and Duncan R. Babbage. 2018. Quantifying Thematic Saturation in Qualitative Data Analysis.\nField Methods 30, 3 (Aug. 2018), 191–207. https://doi.org/10.1177/1525822X17749386\n[43] Penny Mackieson, Aron Shlonsky, and Marie Connolly. 2019. Increasing rigor and reducing bias in qualitative research: A document analysis of\nparliamentary debates using applied thematic analysis. Qualitative Social Work 18, 6 (Nov. 2019), 965–980. https://doi.org/10.1177/1473325018786996\n[44] Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and Inter-rater Reliability in Qualitative Research: Norms and Guidelines\nfor CSCW and HCI Practice. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (Nov. 2019), 1–23. https://doi.org/10.1145/3359174\n[45] Janet McGaw and Alasdair Vance. 2023. Dissonance, Disagreement, Difference: Challenging Thematic Consensus to Decolonise Grounded Theory.\nInternational Journal of Qualitative Methods 22 (Jan. 2023), 16094069231220775. https://doi.org/10.1177/16094069231220775\n[46] Jane Mills, Ann Bonner, and Karen Francis. 2006. The development of constructivist grounded theory. International journal of qualitative methods 5,\n1 (2006), 25–35.\n[47] Alireza Moghaddam. 2006. Coding issues in grounded theory. Issues In Educational Research 16 (2006). https://www.iier.org.au/iier16/moghaddam.\nhtml\n[48] Michael Muller, Shion Guha, Eric P.S. Baumer, David Mimno, and N. Sadat Shami. 2016. Machine Learning and Grounded Theory Method:\nConvergence, Divergence, and Combination. In Proceedings of the 2016 ACM International Conference on Supporting Group Work (GROUP ’16).\nAssociation for Computing Machinery, New York, NY, USA, 3–8. https://doi.org/10.1145/2957276.2957280\n[49] Lorelli S. Nowell, Jill M. Norris, Deborah E. White, and Nancy J. Moules. 2017. Thematic Analysis: Striving to Meet the Trustworthiness Criteria.\nInternational Journal of Qualitative Methods 16, 1 (Dec. 2017), 160940691773384. https://doi.org/10.1177/1609406917733847\n22\n\nA Computational Method for Measuring “Open Codes” in Qualitative Analysis\n, ,\n[50] Steven Pace. 2004. A grounded theory of the flow experiences of web users. International Journal of Human-Computer Studies 60, 3 (March 2004),\n317–363. https://doi.org/10.1016/j.ijhcs.2003.08.005\n[51] Angelina Parfenova et al. 2024. Automating Qualitative Data Analysis with Large Language Models. In Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 4: Student Research Workshop). 177–185.\n[52] Md Shidur Rahman. 2016. The Advantages and Disadvantages of Using Qualitative and Quantitative Approaches and Methods in Language “Testing\nand Assessment” Research: A Literature Review. Journal of Education and Learning 6, 1 (Nov. 2016), 102. https://doi.org/10.5539/jel.v6n1p102\n[53] Maryam N. Razavi and Lee Iverson. 2006. A grounded theory of information sharing behavior in a personal learning space. In Proceedings of the 2006\n20th anniversary conference on Computer supported cooperative work. ACM, Banff Alberta Canada, 459–468. https://doi.org/10.1145/1180875.1180946\n[54] David Ribes. 2019. How I Learned What a Domain Was. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (Nov. 2019), 1–12.\nhttps://doi.org/10.1145/3359140\n[55] Tim Rietz and Alexander Maedche. 2021. Cody: An AI-Based System to Semi-Automate Coding for Qualitative Research. In Proceedings of\nthe 2021 CHI Conference on Human Factors in Computing Systems (CHI ’21). Association for Computing Machinery, New York, NY, USA, 1–14.\nhttps://doi.org/10.1145/3411764.3445591\n[56] John Hamon Salisbury and Tom Cole. [n. d.]. Grounded Theory in Games Research: Making the Case and Exploring the Options. ([n. d.]).\n[57] Sina Mahdipour Saravani, Sadaf Ghaffari, Yanye Luther, James Folkestad, and Marcia Moraes. 2023. Automated Code Extraction from Discussion\nBoard Text Dataset. In Advances in Quantitative Ethnography, Crina Damşa and Amanda Barany (Eds.). Vol. 1785. Springer Nature Switzerland,\nCham, 227–238. https://doi.org/10.1007/978-3-031-31726-2_16 Series Title: Communications in Computer and Information Science.\n[58] Benjamin Saunders, Julius Sim, Tom Kingstone, Shula Baker, Jackie Waterfield, Bernadette Bartlam, Heather Burroughs, and Clare Jinks. 2018.\nSaturation in qualitative research: exploring its conceptualization and operationalization. Quality & Quantity 52, 4 (July 2018), 1893–1907.\nhttps://doi.org/10.1007/s11135-017-0574-8\n[59] Nikhil Sharma, Q Vera Liao, and Ziang Xiao. 2024. Generative Echo Chamber? Effect of LLM-Powered Search Systems on Diverse Information\nSeeking. In Proceedings of the CHI Conference on Human Factors in Computing Systems. 1–17.\n[60] Carson Sievert and Kenneth Shirley. 2014. LDAvis: A method for visualizing and interpreting topics. In Proceedings of the workshop on interactive\nlanguage learning, visualization, and interfaces. 63–70.\n[61] Ravi Sinha, Idris Solola, Ha Nguyen, Hillary Swanson, and LuEttaMae Lawrence. 2024. The Role of Generative AI in Qualitative Research: GPT-4’s\nContributions to a Grounded Theory Analysis. In Proceedings of the Symposium on Learning, Design and Technology. ACM, Delft Netherlands, 17–25.\nhttps://doi.org/10.1145/3663433.3663456\n[62] Anselm Strauss and Juliet Corbin. 1998. Basics of qualitative research: Techniques and procedures for developing grounded theory, 2nd ed. Basics of\nqualitative research: Techniques and procedures for developing grounded theory, 2nd ed. (1998), xiii, 312–xiii, 312. Place: Thousand Oaks, CA, US\nPublisher: Sage Publications, Inc.\n[63] Gareth Terry, Nikki Hayfield, Victoria Clarke, and Virginia Braun. 2017. Thematic analysis. The SAGE handbook of qualitative research in psychology\n2, 17-37 (2017), 25. https://books.google.com/books?hl=en&lr=&id=AAniDgAAQBAJ&oi=fnd&pg=PA17&dq=Thematic+analysis+terry+&ots=\ndpi2nmHiMV&sig=959tII4BUp9su6Hv2JJui1KjP5Q Publisher: SAGE Publications Ltd.\n[64] Nguyen Cao Thanh and TT Thanh. 2015. The interconnection between interpretivist paradigm and qualitative methods in education. American\njournal of educational science 1, 2 (2015), 24–27.\n[65] David R. Thomas. 2006. A General Inductive Approach for Analyzing Qualitative Evaluation Data. American Journal of Evaluation 27, 2 (June 2006),\n237–246. https://doi.org/10.1177/1098214005283748\n[66] Anthony G. Tuckett. 2005. Applying thematic analysis theory to practice: A researcher’s experience. Contemporary Nurse 19, 1-2 (Aug. 2005), 75–87.\nhttps://doi.org/10.5172/conu.19.1-2.75\n[67] Thomas Übellacker. 2024. AcademiaOS: Automating Grounded Theory Development in Qualitative Research with Large Language Models. arXiv\npreprint arXiv:2403.08844 (2024).\n[68] Hanna M. Wallach. 2006. Topic modeling: beyond bag-of-words. In Proceedings of the 23rd international conference on Machine learning - ICML ’06.\nACM Press, Pittsburgh, Pennsylvania, 977–984. https://doi.org/10.1145/1143844.1143967\n[69] Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, and Nanyun Peng. 2023. \" kelly is a warm person, joseph is a role model\":\nGender biases in llm-generated reference letters. arXiv preprint arXiv:2310.09219 (2023).\n[70] Carla Willig and Wendy Stainton Rogers. 2017. The SAGE handbook of qualitative research in psychology. Sage. https://books.google.com/books?hl=\nen&lr=&id=AAniDgAAQBAJ&oi=fnd&pg=PR7&dq=The+SAGE+Handbook+of+Qualitative+Research+in+Psychology&ots=dpi2nmHiK_&sig=\nuyFhaXXsypuO5HjEBxbBLAaNG50\n[71] Ziang Xiao, Xingdi Yuan, Q. Vera Liao, Rania Abdelghani, and Pierre-Yves Oudeyer. 2023. Supporting Qualitative Analysis with Large Language\nModels: Combining Codebook with GPT-3 for Deductive Coding. In Companion Proceedings of the 28th International Conference on Intelligent User\nInterfaces (IUI ’23 Companion). Association for Computing Machinery, New York, NY, USA, 75–78. https://doi.org/10.1145/3581754.3584136\n[72] Andres Felipe Zambrano, Xiner Liu, Amanda Barany, Ryan S. Baker, Juhan Kim, and Nidhi Nasiar. 2023. From nCoder to ChatGPT: From Automated\nCoding to Refining Human Coding. In Advances in Quantitative Ethnography (Communications in Computer and Information Science), Golnaz\nArastoopour Irgens and Simon Knight (Eds.). Springer Nature Switzerland, Cham, 470–485. https://doi.org/10.1007/978-3-031-47014-1_32\n[73] Fengxiang Zhao, Fan Yu, and Yi Shang. 2024. A New Method Supporting Qualitative Data Analysis Through Prompt Generation for Inductive\nCoding. In 2024 IEEE International Conference on Information Reuse and Integration for Data Science (IRI). IEEE, 164–169.\n23\n\n, ,\nChen, et al.\nReceived 14 September 2023; revised 12 December 2023; accepted 19 January 2024\n24",
    "pdf_filename": "A_Computational_Method_for_Measuring_Open_Codes_in_Qualitative_Analysis.pdf"
}