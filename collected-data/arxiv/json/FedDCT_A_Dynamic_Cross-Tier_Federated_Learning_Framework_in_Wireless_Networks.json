{
    "title": "FedDCT: A Dynamic Cross-Tier Federated",
    "abstract": "learning paradigm, trains a global model across devices without expos- inglocaldata.However,resourceheterogeneityandinevitablestragglers in wireless networks severely impact the efficiency and accuracy of FL training. In this paper, we propose a novel Dynamic Cross-Tier Feder- atedLearningframework(FedDCT).Firstly,wedesignadynamictiering strategythatdynamicallypartitionsdevicesintodifferenttiersbasedon their response times and assigns specific timeout thresholds toeach tier to reduce single-round training time. Then, we propose a cross-tier de- viceselectionalgorithmthatselectsdevicesthatrespondquicklyandare conducive to model convergence to improve convergence efficiency and accuracy.Experimentalresultsdemonstratethattheproposedapproach underwirelessnetworksoutperformsthebaselineapproach,withanaver- agereductionof54.7%inconvergencetimeandanaverageimprovement of 1.83% in convergence accuracy.",
    "body": "FedDCT: A Dynamic Cross-Tier Federated\nLearning Framework in Wireless Networks\nYouquan Xian1,2, Xiaoyun Gan1,2, Chuanjian Yao1,2, Dongcheng Li1,2, Peng\nWang1,2, Peng Liu1,2(B), and Ying Zhao3(B)\n1 Key Lab of Education Blockchain and Intelligent Technology, Ministry of\nEducation, GuangxiNormal University,Guilin 54104, China\n2 School of Computer Science and Engineering, Guangxi Normal University,Guilin\n54104, China\n3 School of Business, Guilin University of Electronic Technology, Guilin 54104, China\nxianyouquan@stu.gxnu.edu.cn,liupeng@gxnu.edu.cn,zhaoying@guet.edu.cn\nAbstract. Federated Learning (FL), as a privacy-preserving machine\nlearning paradigm, trains a global model across devices without expos-\ninglocaldata.However,resourceheterogeneityandinevitablestragglers\nin wireless networks severely impact the efficiency and accuracy of FL\ntraining. In this paper, we propose a novel Dynamic Cross-Tier Feder-\natedLearningframework(FedDCT).Firstly,wedesignadynamictiering\nstrategythatdynamicallypartitionsdevicesintodifferenttiersbasedon\ntheir response times and assigns specific timeout thresholds toeach tier\nto reduce single-round training time. Then, we propose a cross-tier de-\nviceselectionalgorithmthatselectsdevicesthatrespondquicklyandare\nconducive to model convergence to improve convergence efficiency and\naccuracy.Experimentalresultsdemonstratethattheproposedapproach\nunderwirelessnetworksoutperformsthebaselineapproach,withanaver-\nagereductionof54.7%inconvergencetimeandanaverageimprovement\nof 1.83% in convergence accuracy.\nKeywords: Wireless networks · Federated learning · Resource hetero-\ngeneity.\n1 Introduction\nDrivenbytherapidgrowthofdistributeddatamining,FederatedLearning(FL)\nhasgarneredsignificantattentionfromboththeacademicandindustrialsectors\nduetoitsnatureofdistributedtrainingandprivacypreservation[4].FLenables\nthe training of a global model across devices without exposing local data. The\nFL processcanbe summarizedasfollows:the serverinitializes the globalmodel\nand selects devices to distribute the global model. The chosen devices train\nusingtheobtainedglobalmodelandlocaldata,andthetrainedmodelsarethen\nuploadedtotheserver.Finally,theserverappliesaggregationalgorithmssuchas\nweightedaveraging(e.g.,FedAvg[11])toaggregatetheuploadedmodelsintothe\nglobal model, and subsequently selects new participating devices to distribute\nthe aggregatednew model.\n4202\nvoN\n91\n]CD.sc[\n2v02440.7032:viXra\n2 Xian et al.\nIn wireless networks, devices often exhibit heterogeneity in computational\nand communication resources, and issues such as communication failures or de-\nvice malfunctions can result in a significant number of devices dropping out.\nDropout devices or those with lower computational capabilities may lag signif-\nicantly behind other devices, leading to inefficiencies in a single round of FL\ntraining[14,18].Tomitigatethe adverseeffects ofresourceheterogeneityonFL,\nFedMCCS [1] predicts whether devices can complete tasks based on their com-\nputational resources and communication capabilities, maximizing the selection\nof devices to enhance convergence speed. Leng et al. [9] and Zhang et al. [20],\nfromthe perspectiveofnetworkresources,allocatedsufficientnetworkresources\ntotrainingdevicestoreducetrainingtime.Similarly,Zhangetal.[19]employed\nreinforcement learning to select participating devices and allocate different lo-\ncal iteration numbers and network resources to participants. However, device\ndropout in practicalnetworksis unavoidable.While asynchronousFL no longer\nrequires waiting for other devices to upload model parameters in each training\nround,avoidingdropoutissues [16], asynchronousFL typically accompaniesthe\nmodelstalenesseffect,leadingtodifficultiesinmodelconvergence[10].Moreover,\nthe aforementioned approaches, aiming for efficiency, overly focus on resource-\nrichdevices,exacerbatingthe disparity in training participationamongdevices,\ncausing model drift, and reducing model convergence accuracy [6].\nThus, TiFL [2] proposed the concept of tiered FL, dividing devices into dif-\nferent tiers based on their training response times, and then randomly selecting\ndevicesfromeachtiertoparticipateintraining.Itnotonlyreducesthedisparity\nin single-rounddevice training times, improving single-round training efficiency\nbutalsoconductstrainingonatier-by-tierbasis,alleviatingtheimpactofmodel\ndrift [12]. However, tiered FL methods like TiFL still face challenges of under-\nutilized resources, and their simplistic tiering approach fails to accurately par-\ntition devices, especially in cases of resource heterogeneity and device dropout.\nTherefore,the centralissue ishow todynamically partitiondevicesin awireless\nnetworkenvironmentwhile improvingtraining efficiency without causing model\ndrift.\nWhile asynchronous FL [16] significantly boosts the efficiency of a single\nround of training by eliminating the need to wait for lagging devices, asyn-\nchronous FL training often requires more iterations and incurs higher commu-\nnicationoverhead[17,3].Additionally,itis difficultto combinewiththe existing\nsynchronous FL applications [2]. Therefore, TiFL [2] introduces the concept of\ntiered FL, categorizing devices into different tiers based on their training re-\nsponse times. Devices are then randomly selected from each tier to participate\nin training, reducing the disparity in individual device training times and en-\nhancingtheefficiencyofasingleroundoftraining.However,tieredFL solutions\nlike TiFL only address the reduction of resource heterogeneity among devices\nin a single training round and do not consider the possibility of devices drop-\nping out in wireless networks,potentially leading to a significantincreasein the\nwaiting time for a single round. Therefore, a central challenge remains: how to\nTitle Suppressed Dueto Excessive Length 3\nimprovetheconvergenceefficiencyandaccuracyofFLinthepresenceofresource\nheterogeneity and dropout issues in wireless networks.\nIn this paper, we propose a novel Dynamic Cross-Tier Federated Learning\nframework(FedDCT),aimingtomaximizeconvergenceefficiencywhileavoiding\nmodel drift. This framework comprises two core modules: the dynamic tiering\nmodule and the cross-tier client selection module, which can be seamlessly in-\ntegrated with existing FL applications in a non-intrusive manner. Firstly, the\ndynamic tiering module dynamically evaluates the response times ofclients and\ncategorizesthemintodifferentlogicaltiers,assigningspecifictimeoutthresholds\nto each tier. Then, the cross-tier client selection module selects devices for FL\ntraining that exhibit fast response times and facilitate model convergence. The\nmain contributions of this paper are as follows:\n– To address the challenges of resource heterogeneity and device dropout in\nwirelessnetworks,wedesignadynamictieringstrategy.Itinvolvesreal-time\nevaluation of device response times, tiering, and assigning specific timeout\nthresholds to each tier, enhancing the convergence efficiency of FL.\n– We propose a cross-tier client selection strategy. It first adaptively selects\ntiers that facilitate model convergence and exhibit fast response times, as\nwell as the participating devices within those tiers. Effectively optimizing\nthe utilization of idle resources,enhancing convergencespeed and accuracy.\n– Through simulation experiments, we verify that the proposed approach in\nwireless network scenarios, compared to the baseline solution, achieves an\naveragereductionof54.7%inconvergencetimeandanaverageimprovement\nof 1.83% in convergence accuracy.\n2 FedDCT: Dynamic Cross-Tier Federated Learning\n2.1 Overview of FedDCT\nFedDCTconsistsofthreemaincomponents:1)AggregationServer:Responsible\nfor globallysynchronizingmodel updates. 2)Dynamic TieringModule: Dynam-\nically assesses the response time of clients categorizes them into different tiers,\nand assigns specific timeout thresholds to each tier. 3) Cross-Tier Client Selec-\ntion Module: Selects tiers based on the current accuracy changes in the global\nmodel,thenselectsparticipatingdeviceswithineachtierbasedontheirtraining\ninformation.Theproposeddynamictieringmoduleandcross-tierclientselection\nmodule can operate as independent plugins running on the aggregation server.\nTakingtheparticipationofdevicesselectedas{tier ,tier }inthefirstroundand\n1 2\n{tier ,tier ,tier } in the secondroundas an example,the processis illustrated\n1 2 3\nin Fig. 1.\n1 During the initialization phase, the dynamic tiering module evaluates the\naverage response time t of all participating devices. Subsequently, clients\na\narestratifiedintoM tiersdenotedas{tier ,...,tier }basedontheresponse\n1 M\n4 Xian et al.\nFig.1. Overviewof FedDCT.\ntimes ofeachdevice.Here, tier representsthe fastesttier,andtier repre-\n1 M\nsentstheslowesttier.Aspartofthetieringprocess,distincttimeoutthresh-\nolds are assigned to devices within each tier.($2.2)\n2 The client selection module, based on the accuracy change in the globally\naggregated model from the previous round, chooses the tier j for participa-\ntion in the current training round. Subsequently, devices are selected from\nthe tier set {tier ,...,tier } with weighted consideration, forming the set of\n1 j\nparticipating devices C .($2.3)\nr\n3 The aggregationserverdistributesthe latestglobalmodelW tothe selected\nparticipating devices. Devices then train their models based on the global\nmodel and local data, subsequently returning their training results. For de-\nvicesthatexceedthe timeoutthresholdDj ,the servernolongerwaitsfor\nmax\ntheir uploads, marking them as dropout devices. The system undergoes a\nreevaluation and tiering process for these devices.\n4 The dynamic tiering module updates the average response time based on\nthe actualtime usageof alldevices in the currenttrainingroundand subse-\nquentlyperformsare-tieringprocess.UnlikeapproachessuchasTiFL[2]and\nFedAT[3],whichassessdevicesonlyintheinitializationphase,thisdynamic\nevaluation more accurately reflects the variability in resource heterogeneity\nwithin wireless networks.($2.2)\nThe iterative process of steps 2 - 4 continues until a specified number\nof training rounds is completed or the model converges to the desired accuracy\nrequirement.\nTitle Suppressed Dueto Excessive Length 5\n2.2 Dynamic Tiering\nFig.2. Response time of devicesin different tiers.\nThe dynamic tiering module primarily incorporates three main functionali-\nties:1)Evaluatetheaverageresponsetimeofparticipatingdevices.2)Categorize\ndevices into different logicaltiers basedon their averageresponse times. 3) Cal-\nculate the timeout threshold for devices within each tier based on their average\nresponsetimes.Specifically,themodulecategorizesdevicesintoM tiersbasedon\ntheir averageresponse times t during the ct[i] training round. Devices with an\na\naverage response time t exceeding a threshold are considered dropout devices,\na\nand they undergo re-evaluation and re-tiering after κ rounds.\nAlgorithm 1: Tiering\nInput: theaverage response time of clients t ,the numberof client in tier Ts.\na\nOutput: tiering of clients ts.\n1 for client c, time t in t do\na\n2 tmp[c]=(c,t) ;\n3 tmp=SortAscByTime(tmp);\n4 for index i, client c in tmp do\n5 ts[i/Ts][i%Ts]=c ;\n6 return ts;\nInAlgorithm1,weprovideadetaileddescriptionofhowthe dynamictiering\nmodule categorizes devices into M different logical tiers based on their average\nresponsetime t .The logicaltiers,arrangedfromlowtohigh,reflectanincreas-\na\ning order of average response times of the devices within each tier. The tiering\neffectisillustratedinFig.2,wheredevicesintierstier throughtier exhibitin-\n1 M\ncreasingresponsetimes,anddeviceswithineachtierhaveapproximatelysimilar\nresponse times.\nThe purpose of setting the timeout threshold is to prevent excessive waiting\ntime caused by resource heterogeneity and dropout devices. However, unlike\nconventionalFL approaches,FL with tieringshouldadoptmorerefinedtimeout\nthresholds. Therefore, we utilize the average response time of devices in tier j,\n6 Xian et al.\ndenotedas\nPi∈ts[t]ta[i]\n,multipliedbyatolerancelimitβ asthetimeoutthreshold\nlen(ts[t])\nDj for that tier. The tolerance limit β reflects the degree of tolerance for\nmax\ndelayedresponsesfromdevicesinwirelessnetworks.Alargerβ notonlysignifies\nmore tolerance for delays, as illustrated in Fig. 3, but also allows devices that\nexceed Dj to be deemed as dropoutdevices. These devices undergo κ rounds\nmax\nof re-evaluation until normal completion of κ rounds, after which they are re-\ntiered and reintroduced into subsequent training. At the same time, we also set\namaximumtimeoutthresholdofΩ tolimittheaveragetrainingtimeofthistier\nto be too long.\nt [i]\nDj =min( i∈ts[t] a ×β,Ω) (1)\nmax len(ts[t])\nP\nFig.3. Response time analysis of tier and tier in a task.\n1 2\n2.3 Cross-Tier Client Selection\nThe cross-tier client selection module selects participating devices for FL to\nachievefastresponseperformancewhileensuringmodelconvergence.Thismod-\nuleisdividedintotwomainsteps:1)Selectingparticipatingtiersand2)Selecting\ndevices within those tiers.\nInitially, based on the tiering characteristics described in $2.2, the expecta-\ntion is to select tiers from low to high. If devices in the fast-responding tier\nj\ncontribute to the convergence of the global model, devices from tier are not\nj+1\nselected. The change in accuracy of the global model υ is used as a criterion. If\nthecurrentlyevaluatedaccuracyυ afteraggregationishigherthantheaccuracy\nυ inthepreviousround,itindicatesthatthedevicesfromtier currentlyused\nlast j\ncan still contribute to the convergence of the global model. To reduce training\ntime, an attempt can be made to select devices from tier j−1 in the next round.\nIt’s important to note that, to minimize idle waiting time for devices, as illus-\ntrated in Fig. 3, the proposed approach allows for cross-tier selection. In other\nwords, when selecting tier , devices from {tier ,...,tier } are actually chosen.\nj 1 j\nTitle Suppressed Dueto Excessive Length 7\nAlgorithm 2: Client Selection\nInput: current tier j, last test accuracy υ , global model W,tiering of\nlast\nclients ts,the numberof client training ct, thenumberof client\nselection in a tier τ.\nOutput: theclients selection C .\nr\n1 υ=Evaluation(W,TestData);\n2 if υ≥υ then\nlast\n3 j =max(j−1,1) ;\n4 else\n5 j =min(j+1,T) ;\n6 for tier t=1 → j do\n7 for client c in ts[t] do\n8 probs[c]= 1/ct[c] ;\nPi∈ts[t]1/ct[i]\n9 clients = (select τ clients from tier t with probs);\n10 C ← clients ;\nr\n11 return C ;\nr\nmin(j+1,M), υ <υ\nlast\nj = (2)\n(max(j−1,1), υ ≥υ\nlast\nTo prevent significant differences in the participation frequency among de-\nvices within tiers, which may lead to model drift [6], we increase the selection\nprobabilityofdeviceswithfewerparticipationtimeswhenselectingnodeswithin\ntiers. Therefore, we allocate different selection probabilities probs based on the\nparticipationtimesctofdevicesintier .Finally,accordingtotheselectionprob-\nj\nabilities probs of devices within tiers, τ devices C are chosen to participate in\nr\ntraining for this round, as depicted in Algorithm 2.\n3 Experimental Evaluation\nWe referredto a portionofthe implementationmethods fromFedlab[5]andim-\nplemented FedDCT and other FL baseline methods using PyTorch. All experi-\nments were conducted on a high-performanceserver with 2 × Intel(R) Xeon(R)\nGold6230CPUs,128GBofmemory,and2×NVIDIATeslaV100FHHLGPUs.\nWesimulatedascenariowhereoneserverand50clientsparticipatedinFLtrain-\ning on this machine.\n3.1 Experimental Setup\nWeconductedexperimentsonthreecommonlyuseddatasets,MNIST[8],CIFAR-\n10[7], and Fashion-MNIST[15]. Two classic neural network models, CNN and\nResNet8, were employed for training. We used the CNN model for training on\n8 Xian et al.\nMNIST and Fashion-MNIST datasets and the ResNet8 model following the ap-\nproach in the literature [13] for training on CIFAR-10. The proposed approach\nwill be compared with three classic algorithms for synchronous (FedAvg[11]),\nasynchronous (FedAsync[16]), and tiered FL (TiFL[2]).\nWe used momentum as the optimization algorithm with a learning rate of\n0.001 and momentum of 0.9. For each dataset, we trained with the following\nconfigurations:localepoch=1,batchsize=10,τ =5,β =0.1,Ω =30s,κ=3.\nWe used the same parameters for other FL approaches. The default number of\nselected clients for training in each round was 5, but for FedDCT, the number\nof selected clients per round varied with the selected tier.\nTo simulate the response time differences caused by resource heterogeneity\nin wireless networks, we assigned random response delays with a variance of\n2 from a Gaussian distribution with expectations of {5,10,15,20,25} seconds\nfor devices. Additionally, to simulate dropout occurrences, we randomly added\ndelaysintherangeof(30−60)secondsduringtraining,controlledbythedropout\nrate µ to determine the probability of its occurrence. Finally, to analyze the\ntrainingeffectsunderdifferentdatadistributionscenarios,werandomlyassigned\namainclasstoeachclient,where#%ofthe datainthatdevicebelongedto the\nmain class, and the remaining data belonged to the other classes.\nTable 1. Comparison of the best average accuracy and time which reach the preset\naccuracyofeachbaselinealgorithm.#representsthepercentageofprimaryclasslabel\nin each client. Accuracy shows the best average accuracy achieved after convergence.\nTime representsthetimetakenbythemodeltoconvergetothespecified precision(s).\nFor CIFAR-10, Fashion-MNIST, and MNIST, the convergence accuracy is preset as\n0.7,0.88,and0.98,respectively(CIFAR-10#=0.7ispresetas0.6separately).impr.(a)\nand (b)representtheimprovedtraining accuracy of FedDCTandthereducedtimeof\nconvergence to the specified accuracy compared with the best baseline FL method,\nrespectively.\nDataset CIFAR-10 Fashion-MNISTMNIST\n(#Non-IID) IID #0.3 #0.5 #0.7 #0.7 #0.7\nAccuracy 0.7843 0.7407 0.7150 0.6592 0.8914 0.9892\nFedAvg\nTime(s) 1617.0 2403.5 3416.2 3033.8 2544.1 1481.9\nAccuracy 0.7826 0.7401 0.7071 0.6475 0.8862 0.9894\nTiFL\nTime(s) 1980.8 1945.5 3389.9 2363.2 2431.4 1261.6\nAccuracy 0.7718 0.7252 0.7001 0.6234 0.8786 0.9868\nFedAsync\nTime(s) 3709.6 4885.5 6268.6 7435.5 6417.0 2427.4\nAccuracy0.79200.75260.72870.6897 0.9080 0.9897\nFedDCT Time(s) 685.6 618.5 1479.41077.3 965.8 864.7\nimpr.(a) 0.98% 1.60% 1.91% 4.62% 1.86% 0.03%\nimpr.(b) 57.6% 68.2% 56.3% 54.4% 60.2% 31.4%\nTitle Suppressed Dueto Excessive Length 9\n3.2 Experimental Results\nTable 1 presents the best average accuracy and the time spent to reach the\npreset accuracy for all datasets. The results show that, across all six scenarios,\nthe proposedapproachachievedanaverageaccuracyimprovementof1.83%and\nreduced time overhead by 54.7% compared to the optimal baseline. Under the\nsame experimental configuration, FedDCT consistently achieved higher conver-\ngence accuracy and significantly reduced convergence time in all experiments.\nParticularly, the improvementcompared to TiFL indicates that 1) the dynamic\ntiering in the proposed approach is more accurate and adaptable to changes in\nthe dynamic environment, and 2) the selection of devices across tiers effectively\nexploits device performance, enhancing convergence speed. Meanwhile, we ob-\nservedthatTiFL doesnotperformwellinthe presenceofunexpecteddropouts,\nleading to suboptimal convergence accuracy and time.\n0.7 0.7 0.7\n0.6 0.6 0.6\n0.5 0.5 0.5\nFedAvg FedAvg FedAvg\nTiFL TiFL TiFL\n0.4 FedDCT 0.4 FedDCT 0.4 FedDCT\nFedAsync FedAsync FedAsync\n0.30 5000 10000 15000 0.30 5000 10000 15000 0.30 5000 10000 15000\nTime(s) Time(s) Time(s)\nTarget accuracy: 0.7 Target accuracy: 0.7 Target accuracy: 0.6 7435.58\n6000 4885.59 6000 6268.69 6000\n24 00 00 00 2403.53 1945.55 24 00 00 00 3416.30 3389.95 1479.46 24 00 00 00 3033.88 2363.24 1077.33\n0 FedAvg(a)TiF#L =0F6 e1 .d8 D3.5 C5 T FedAsync 0 FedAvg(b)TiF#L =0Fe.dD5CT FedAsync 0 FedAv(gc) #TiFL=0F.ed7DCT FedAsync\nFig.4. The effect of different # on training.\n0.7 0.7 0.7\n0.6 0.6 0.6\n0.5 0.5 0.5\nFedAvg FedAvg FedAvg\nTiFL TiFL TiFL\n0.4 FedDCT 0.4 FedDCT 0.4 FedDCT\nFedAsync FedAsync FedAsync\n0.30 5000 10000 15000 0.30 5000 10000 15000 0.30 5000 10000 15000\nTime(s) Time(s) Time(s)\nTarget accuracy: 0.65 3074.97 4000 Target accuracy: 0.65 4506.90 6000 Target accuracy: 0.65 6722.36\n2000 4000\n1126.19 694.34 402.95 2000 1976.31 2124.18 885.54 2000 2610.23 2515.99 661.36\n0 FedAvg(a)TiFLµ=0FedDCT FedAsync 0 FedAvg(b)TµiFL=0F.e1dDCT FedAsync 0 FedAvg(c)TµiFL=0F.e4dDCT FedAsync\nFig.5. The effect of different µ on training.\n)s(emiT\nycaruccA\ntseT\nycaruccA\ntseT\n)s(emiT\n)s(emiT\nycaruccA\ntseT\nycaruccA\ntseT\n)s(emiT\nycaruccA\ntseT\n)s(emiT\nycaruccA\ntseT\n)s(emiT\n10 Xian et al.\nFig. 4-5 illustrates the training performance of all schemes under different\ndata distributions # and various dropout rates µ. Fig. 4 indicates that the\nproposedscheme performs well under different data distributions. Although the\noverallconvergenceaccuracydecreaseswiththeincreasingheterogeneityofdata\ndistribution, our scheme can still achieve faster convergence and higher final\nconvergence accuracy compared to other baseline schemes. Fig. 5 demonstrates\nthatasthe dropoutrateµ increases,the overallconvergencetime alsogradually\nincreases. However, we observe that the impact of the dropout rate µ on the\nconvergence of FedDCT is not significant. This is attributed to the dynamic\ntieringmoduleinFedDCT,whichcansignificantlyalleviatetheimpactofdevice\ndropouts on FL.\nFig. 6 presents the training performance of all schemes under different net-\nworkenvironments.Specifically,inFig.6(a),wesetthedropoutrateµto0,and\nin (b), we intensify the response time differences among devices, with response\ntime expectations set to {1,3,10,30,100}seconds.The results indicate that the\nproposedschemeexhibitsgoodrobustness,achievingfavorableresultsinvarious\nnetwork environments.\n0.9 0.9\n0.8 0.8\nFedAvg FedAvg\n0.7 0.7\nTiFL TiFL\nFedDCT FedDCT\nFedAsync FedAsync\n0.6 0.6\n0 5000 10000 15000 0 10000 20000 30000\nTime(s) Time(s)\nTarget accuracy: 0.88 Target accuracy: 0.88\n6000 16000 15826.52\n4867.60\n12000\n4000 8000 9015.13 7973.90\n2000 1780.22 1441.10 683.30 4000 2209.85\n0 0\nFedAvg TiFL FedDCT FedAsync FedAvg TiFL FedDCT FedAsync\n(a) Stable Network (b) Complex Network\nFig.6. Training performance underdifferent network environments.\nFinally, to explore why FedDCT could converge faster, we recorded the se-\nlected tier during the training process, averaged it every 10 rounds, and fitted\nit with a linear regression model. As shown in Fig. 7, the overall trend of the\nselectedtierincreaseswithtrainingrounds.Itisconsistentwiththeexpectations\nof the proposed design. FedDCT first uses the clients in the tier with a short\ntrainingtimefortraininguntilitisdifficulttoimprovetheaccuracyoftheglobal\nmodel, and then uses the clients in the other tier with a longer training time.\nycaruccA\ntseT\n)s(emiT\nycaruccA\ntseT\n)s(emiT\nTitle Suppressed Dueto Excessive Length 11\n5\n4.5\n4.0\n4\n3.5\n3 3.0\n2.5\n2\n2.0\n1.5\n1\n0 100 200 300 400\nRound\nFig.7. The changes of theselected tier duringthetraining.\n4 Conclusion\nTo mitigate the adverse impact of wireless networks on the training of FL, this\npaper proposes a novel dynamic cross-tier federated learning Framework. Fed-\nDCTadoptsadynamictieringapproachtoreducewaitingtimesduringtraining\ncausedbyresourcedisparitiesandunexpecteddevice dropouts,therebyenhanc-\ning the efficiency of a single training round. Furthermore,we design a cross-tier\nclientselectionalgorithm,enablingFedDCT to effectivelyutilize devicetraining\ninformationfordeviceselection,therebyimprovingoverallconvergenceefficiency\nandaccuracy.Experimentalresultsdemonstrate thatour approachoutperforms\ntraditional solutions in wireless networks, achieving superior convergence accu-\nracy and speed.\nAcknowledgments. TheresearchwassupportedinpartbytheGuangxiScienceand\nTechnology Major Project (No. AA22068070), the National Natural Science Founda-\ntion of China (Nos. 62166004,U21A20474), the Basic Ability Enhancement Program\nfor Young and Middle-aged Teachers of Guangxi (No.2022KY0057, 2023KY0062), In-\nnovation Project of Guangxi Graduate Education (Nos. XYCBZ2024025).\nReferences\n1. AbdulRahman, S., Tout, H., Mourad, A., Talhi, C.: Fedmccs: Multicriteria client\nselectionmodelforoptimaliotfederatedlearning.IEEEInternetofThingsJournal\n8(6), 4723–4735 (2020)\n2. Chai, Z., Ali, A., Zawad, S., Truex, S., Anwar, A., Baracaldo, N., Zhou, Y., Lud-\nwig, H.,Yan,F., Cheng, Y.: Tifl: A tier-based federated learning system. In:Pro-\nceedings of the 29th International Symposium on High-Performance Parallel and\nDistributed Computing. pp.125–136. ACM (2020)\n3. Chai, Z., Chen, Y.,Anwar, A.,Zhao, L., Cheng, Y., Rangwala, H.:Fedat: ahigh-\nperformance and communication-efficient federated learning system with asyn-\nchronous tiers. In: Proceedings of the International Conference for High Perfor-\nmance Computing, Networking, Storage and Analysis. pp.1–16. ACM(2021)\nreiT\ndetceleS\n12 Xian et al.\n4. Duan,Q.,Huang,J., Hu,S.,Deng,R.,Lu,Z.,Yu,S.:Combiningfederated learn-\ningandedgecomputingtoward ubiquitousintelligence in6g network:Challenges,\nrecentadvances,andfuturedirections.IEEECommunicationsSurveys&Tutorials\n(2023)\n5. Dun Zeng, Siqi Liang, X.H., Xu, Z.: Fedlab: A flexible federated learning frame-\nwork. arXiv preprint arXiv:2107.11621 (2021)\n6. Huang,T.,Lin,W.,Wu,W.,He,L.,Li,K.,Zomaya,A.Y.:Anefficiency-boosting\nclientselectionschemeforfederatedlearningwithfairnessguarantee.IEEETrans-\nactions on Parallel and Distributed Systems32(7), 1552–1564 (2020)\n7. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny\nimages (2009)\n8. LeCun,Y.,Bottou,L.,Bengio,Y.,Haffner,P.:Gradient-basedlearningappliedto\ndocument recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)\n9. Leng,J., Lin,Z., Ding,M., Wang, P.,Smith,D., Vucetic,B.: Client schedulingin\nwirelessfederatedlearningbasedonchannelandlearningqualities.IEEEWireless\nCommunications Letters(2022)\n10. Liu, J., Jia, J., Che, T., Huo, C., Ren, J., Zhou, Y., Dai, H., Dou, D.: Fedasmu:\nEfficientasynchronousfederatedlearningwithdynamicstaleness-awaremodelup-\ndate. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 38,\npp.13900–13908 (2024)\n11. McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.:\nCommunication-efficient learning of deep networks from decentralized data. In:\nArtificial intelligence and statistics. pp.1273–1282. PMLR (2017)\n12. Pfeiffer, K., Rapp, M., Khalili, R., Henkel, J.: Federated learning for computa-\ntionally constrained heterogeneous devices: A survey. ACM Computing Surveys\n55(14s), 1–27 (2023)\n13. Shang,X.,Lu,Y.,Huang,G.,Wang,H.:Federatedlearningonheterogeneousand\nlong-taileddataviaclassifierre-trainingwithfederatedfeatures.In:Proceedingsof\ntheThirty-FirstInternationalJointConferenceonArtificialIntelligence,IJCAI-22.\npp.2218–2224 (2022)\n14. Wang,Z.,Zhang, Z.,Tian, Y.,Yang,Q.,Shan,H.,Wang,W., Quek,T.Q.: Asyn-\nchronous federated learning over wireless communication networks. IEEE Trans-\nactions on Wireless Communications (2022)\n15. Xiao, H., Rasul, K., Vollgraf, R.: Fashion-mnist: a novelimage dataset for bench-\nmarking machinelearning algorithms. arXiv preprint arXiv:1708.07747 (2017)\n16. Xie,C.,Koyejo,S.,Gupta,I.:Asynchronousfederatedoptimization.arXivpreprint\narXiv:1903.03934 (2019)\n17. Xu,C.,Qu,Y.,Xiang,Y.,Gao, L.:Asynchronousfederated learning on heteroge-\nneousdevices: A survey.arXiv preprintarXiv:2109.04269 (2021)\n18. Ye, M., Fang, X., Du, B., Yuen, P.C., Tao, D.: Heterogeneous federated learning:\nState-of-the-art and research challenges. ACM Computing Surveys 56(3), 1–44\n(2023)\n19. Zhang,J.,Chen,S.,Zhou,X.,Wang,X.,Lin,Y.B.:Jointschedulingofparticipants,\nlocal iterations, and radio resources for fair federated learning over mobile edge\nnetworks. IEEE Transactions on Mobile Computing (2022)\n20. Zhang, T., Lam, K.Y., Zhao, J., Li, F., Han, H., Jamil, N.: Enhancing federated\nlearning with spectrum allocation optimization and device selection. IEEE/ACM\nTransactions on Networking (2023)",
    "pdf_filename": "FedDCT_A_Dynamic_Cross-Tier_Federated_Learning_Framework_in_Wireless_Networks.pdf"
}