{
    "title": "FedDCT A Dynamic Cross-Tier Federated Learning Framework in Wireless Networks",
    "context": "learning paradigm, trains a global model across devices without expos- ing local data. However, resource heterogeneity and inevitable stragglers in wireless networks severely impact the eﬃciency and accuracy of FL training. In this paper, we propose a novel Dynamic Cross-Tier Feder- ated Learning framework (FedDCT). Firstly, we design a dynamic tiering strategy that dynamically partitions devices into diﬀerent tiers based on their response times and assigns speciﬁc timeout thresholds to each tier to reduce single-round training time. Then, we propose a cross-tier de- vice selection algorithm that selects devices that respond quickly and are conducive to model convergence to improve convergence eﬃciency and accuracy. Experimental results demonstrate that the proposed approach under wireless networks outperforms the baseline approach, with an aver- age reduction of 54.7% in convergence time and an average improvement of 1.83% in convergence accuracy. Keywords: Wireless networks · Federated learning · Resource hetero- geneity. 1 Driven by the rapid growth of distributed data mining, Federated Learning (FL) has garnered signiﬁcant attention from both the academic and industrial sectors due to its nature of distributed training and privacy preservation [4]. FL enables the training of a global model across devices without exposing local data. The FL process can be summarized as follows: the server initializes the global model and selects devices to distribute the global model. The chosen devices train using the obtained global model and local data, and the trained models are then uploaded to the server. Finally, the server applies aggregation algorithms such as weighted averaging (e.g., FedAvg [11]) to aggregate the uploaded models into the global model, and subsequently selects new participating devices to distribute the aggregated new model.",
    "body": "arXiv:2307.04420v2  [cs.DC]  19 Nov 2024\nFedDCT: A Dynamic Cross-Tier Federated\nLearning Framework in Wireless Networks\nYouquan Xian1,2, Xiaoyun Gan1,2, Chuanjian Yao1,2, Dongcheng Li1,2, Peng\nWang1,2, Peng Liu1,2(B), and Ying Zhao3(B)\n1 Key Lab of Education Blockchain and Intelligent Technology, Ministry of\nEducation, Guangxi Normal University, Guilin 54104, China\n2 School of Computer Science and Engineering, Guangxi Normal University, Guilin\n54104, China\n3 School of Business, Guilin University of Electronic Technology, Guilin 54104, China\nxianyouquan@stu.gxnu.edu.cn,liupeng@gxnu.edu.cn,zhaoying@guet.edu.cn\nAbstract. Federated Learning (FL), as a privacy-preserving machine\nlearning paradigm, trains a global model across devices without expos-\ning local data. However, resource heterogeneity and inevitable stragglers\nin wireless networks severely impact the eﬃciency and accuracy of FL\ntraining. In this paper, we propose a novel Dynamic Cross-Tier Feder-\nated Learning framework (FedDCT). Firstly, we design a dynamic tiering\nstrategy that dynamically partitions devices into diﬀerent tiers based on\ntheir response times and assigns speciﬁc timeout thresholds to each tier\nto reduce single-round training time. Then, we propose a cross-tier de-\nvice selection algorithm that selects devices that respond quickly and are\nconducive to model convergence to improve convergence eﬃciency and\naccuracy. Experimental results demonstrate that the proposed approach\nunder wireless networks outperforms the baseline approach, with an aver-\nage reduction of 54.7% in convergence time and an average improvement\nof 1.83% in convergence accuracy.\nKeywords: Wireless networks · Federated learning · Resource hetero-\ngeneity.\n1\nIntroduction\nDriven by the rapid growth of distributed data mining, Federated Learning (FL)\nhas garnered signiﬁcant attention from both the academic and industrial sectors\ndue to its nature of distributed training and privacy preservation [4]. FL enables\nthe training of a global model across devices without exposing local data. The\nFL process can be summarized as follows: the server initializes the global model\nand selects devices to distribute the global model. The chosen devices train\nusing the obtained global model and local data, and the trained models are then\nuploaded to the server. Finally, the server applies aggregation algorithms such as\nweighted averaging (e.g., FedAvg [11]) to aggregate the uploaded models into the\nglobal model, and subsequently selects new participating devices to distribute\nthe aggregated new model.\n\n2\nXian et al.\nIn wireless networks, devices often exhibit heterogeneity in computational\nand communication resources, and issues such as communication failures or de-\nvice malfunctions can result in a signiﬁcant number of devices dropping out.\nDropout devices or those with lower computational capabilities may lag signif-\nicantly behind other devices, leading to ineﬃciencies in a single round of FL\ntraining [14,18]. To mitigate the adverse eﬀects of resource heterogeneity on FL,\nFedMCCS [1] predicts whether devices can complete tasks based on their com-\nputational resources and communication capabilities, maximizing the selection\nof devices to enhance convergence speed. Leng et al. [9] and Zhang et al. [20],\nfrom the perspective of network resources, allocated suﬃcient network resources\nto training devices to reduce training time. Similarly, Zhang et al. [19] employed\nreinforcement learning to select participating devices and allocate diﬀerent lo-\ncal iteration numbers and network resources to participants. However, device\ndropout in practical networks is unavoidable. While asynchronous FL no longer\nrequires waiting for other devices to upload model parameters in each training\nround, avoiding dropout issues [16], asynchronous FL typically accompanies the\nmodel staleness eﬀect, leading to diﬃculties in model convergence [10]. Moreover,\nthe aforementioned approaches, aiming for eﬃciency, overly focus on resource-\nrich devices, exacerbating the disparity in training participation among devices,\ncausing model drift, and reducing model convergence accuracy [6].\nThus, TiFL [2] proposed the concept of tiered FL, dividing devices into dif-\nferent tiers based on their training response times, and then randomly selecting\ndevices from each tier to participate in training. It not only reduces the disparity\nin single-round device training times, improving single-round training eﬃciency\nbut also conducts training on a tier-by-tier basis, alleviating the impact of model\ndrift [12]. However, tiered FL methods like TiFL still face challenges of under-\nutilized resources, and their simplistic tiering approach fails to accurately par-\ntition devices, especially in cases of resource heterogeneity and device dropout.\nTherefore, the central issue is how to dynamically partition devices in a wireless\nnetwork environment while improving training eﬃciency without causing model\ndrift.\nWhile asynchronous FL [16] signiﬁcantly boosts the eﬃciency of a single\nround of training by eliminating the need to wait for lagging devices, asyn-\nchronous FL training often requires more iterations and incurs higher commu-\nnication overhead [17,3]. Additionally, it is diﬃcult to combine with the existing\nsynchronous FL applications [2]. Therefore, TiFL [2] introduces the concept of\ntiered FL, categorizing devices into diﬀerent tiers based on their training re-\nsponse times. Devices are then randomly selected from each tier to participate\nin training, reducing the disparity in individual device training times and en-\nhancing the eﬃciency of a single round of training. However, tiered FL solutions\nlike TiFL only address the reduction of resource heterogeneity among devices\nin a single training round and do not consider the possibility of devices drop-\nping out in wireless networks, potentially leading to a signiﬁcant increase in the\nwaiting time for a single round. Therefore, a central challenge remains: how to\n\nTitle Suppressed Due to Excessive Length\n3\nimprove the convergence eﬃciency and accuracy of FL in the presence of resource\nheterogeneity and dropout issues in wireless networks.\nIn this paper, we propose a novel Dynamic Cross-Tier Federated Learning\nframework (FedDCT), aiming to maximize convergence eﬃciency while avoiding\nmodel drift. This framework comprises two core modules: the dynamic tiering\nmodule and the cross-tier client selection module, which can be seamlessly in-\ntegrated with existing FL applications in a non-intrusive manner. Firstly, the\ndynamic tiering module dynamically evaluates the response times of clients and\ncategorizes them into diﬀerent logical tiers, assigning speciﬁc timeout thresholds\nto each tier. Then, the cross-tier client selection module selects devices for FL\ntraining that exhibit fast response times and facilitate model convergence. The\nmain contributions of this paper are as follows:\n– To address the challenges of resource heterogeneity and device dropout in\nwireless networks, we design a dynamic tiering strategy. It involves real-time\nevaluation of device response times, tiering, and assigning speciﬁc timeout\nthresholds to each tier, enhancing the convergence eﬃciency of FL.\n– We propose a cross-tier client selection strategy. It ﬁrst adaptively selects\ntiers that facilitate model convergence and exhibit fast response times, as\nwell as the participating devices within those tiers. Eﬀectively optimizing\nthe utilization of idle resources, enhancing convergence speed and accuracy.\n– Through simulation experiments, we verify that the proposed approach in\nwireless network scenarios, compared to the baseline solution, achieves an\naverage reduction of 54.7% in convergence time and an average improvement\nof 1.83% in convergence accuracy.\n2\nFedDCT: Dynamic Cross-Tier Federated Learning\n2.1\nOverview of FedDCT\nFedDCT consists of three main components: 1) Aggregation Server: Responsible\nfor globally synchronizing model updates. 2) Dynamic Tiering Module: Dynam-\nically assesses the response time of clients categorizes them into diﬀerent tiers,\nand assigns speciﬁc timeout thresholds to each tier. 3) Cross-Tier Client Selec-\ntion Module: Selects tiers based on the current accuracy changes in the global\nmodel, then selects participating devices within each tier based on their training\ninformation. The proposed dynamic tiering module and cross-tier client selection\nmodule can operate as independent plugins running on the aggregation server.\nTaking the participation of devices selected as {tier1, tier2} in the ﬁrst round and\n{tier1, tier2, tier3} in the second round as an example, the process is illustrated\nin Fig. 1.\n1\nDuring the initialization phase, the dynamic tiering module evaluates the\naverage response time ta of all participating devices. Subsequently, clients\nare stratiﬁed into M tiers denoted as {tier1, ..., tierM} based on the response\n\n4\nXian et al.\nFig. 1. Overview of FedDCT.\ntimes of each device. Here, tier1 represents the fastest tier, and tierM repre-\nsents the slowest tier. As part of the tiering process, distinct timeout thresh-\nolds are assigned to devices within each tier.($2.2)\n2\nThe client selection module, based on the accuracy change in the globally\naggregated model from the previous round, chooses the tier j for participa-\ntion in the current training round. Subsequently, devices are selected from\nthe tier set {tier1, ..., tierj} with weighted consideration, forming the set of\nparticipating devices Cr.($2.3)\n3\nThe aggregation server distributes the latest global model W to the selected\nparticipating devices. Devices then train their models based on the global\nmodel and local data, subsequently returning their training results. For de-\nvices that exceed the timeout threshold Dj\nmax, the server no longer waits for\ntheir uploads, marking them as dropout devices. The system undergoes a\nreevaluation and tiering process for these devices.\n4\nThe dynamic tiering module updates the average response time based on\nthe actual time usage of all devices in the current training round and subse-\nquently performs a re-tiering process. Unlike approaches such as TiFL [2] and\nFedAT [3], which assess devices only in the initialization phase, this dynamic\nevaluation more accurately reﬂects the variability in resource heterogeneity\nwithin wireless networks.($2.2)\nThe iterative process of steps\n2 - 4\ncontinues until a speciﬁed number\nof training rounds is completed or the model converges to the desired accuracy\nrequirement.\n\nTitle Suppressed Due to Excessive Length\n5\n2.2\nDynamic Tiering\nFig. 2. Response time of devices in diﬀerent tiers.\nThe dynamic tiering module primarily incorporates three main functionali-\nties: 1) Evaluate the average response time of participating devices. 2) Categorize\ndevices into diﬀerent logical tiers based on their average response times. 3) Cal-\nculate the timeout threshold for devices within each tier based on their average\nresponse times. Speciﬁcally, the module categorizes devices into M tiers based on\ntheir average response times ta during the ct[i] training round. Devices with an\naverage response time ta exceeding a threshold are considered dropout devices,\nand they undergo re-evaluation and re-tiering after κ rounds.\nAlgorithm 1: Tiering\nInput: the average response time of clients ta, the number of client in tier T s.\nOutput: tiering of clients ts.\n1 for client c, time t in ta do\n2\ntmp[c] = (c, t) ;\n3 tmp = SortAscByT ime(tmp) ;\n4 for index i, client c in tmp do\n5\nts[i/T s][i%T s] = c ;\n6 return ts;\nIn Algorithm 1, we provide a detailed description of how the dynamic tiering\nmodule categorizes devices into M diﬀerent logical tiers based on their average\nresponse time ta. The logical tiers, arranged from low to high, reﬂect an increas-\ning order of average response times of the devices within each tier. The tiering\neﬀect is illustrated in Fig. 2, where devices in tiers tier1 through tierM exhibit in-\ncreasing response times, and devices within each tier have approximately similar\nresponse times.\nThe purpose of setting the timeout threshold is to prevent excessive waiting\ntime caused by resource heterogeneity and dropout devices. However, unlike\nconventional FL approaches, FL with tiering should adopt more reﬁned timeout\nthresholds. Therefore, we utilize the average response time of devices in tier j,\n\n6\nXian et al.\ndenoted as\nP\ni∈ts[t] ta[i]\nlen(ts[t])\n, multiplied by a tolerance limit β as the timeout threshold\nDj\nmax for that tier. The tolerance limit β reﬂects the degree of tolerance for\ndelayed responses from devices in wireless networks. A larger β not only signiﬁes\nmore tolerance for delays, as illustrated in Fig. 3, but also allows devices that\nexceed Dj\nmax to be deemed as dropout devices. These devices undergo κ rounds\nof re-evaluation until normal completion of κ rounds, after which they are re-\ntiered and reintroduced into subsequent training. At the same time, we also set\na maximum timeout threshold of Ωto limit the average training time of this tier\nto be too long.\nDj\nmax = min(\nP\ni∈ts[t] ta[i]\nlen(ts[t])\n× β, Ω)\n(1)\nFig. 3. Response time analysis of tier1 and tier2 in a task.\n2.3\nCross-Tier Client Selection\nThe cross-tier client selection module selects participating devices for FL to\nachieve fast response performance while ensuring model convergence. This mod-\nule is divided into two main steps: 1) Selecting participating tiers and 2) Selecting\ndevices within those tiers.\nInitially, based on the tiering characteristics described in $2.2, the expecta-\ntion is to select tiers from low to high. If devices in the fast-responding tierj\ncontribute to the convergence of the global model, devices from tierj+1 are not\nselected. The change in accuracy of the global model υ is used as a criterion. If\nthe currently evaluated accuracy υ after aggregation is higher than the accuracy\nυlast in the previous round, it indicates that the devices from tierj currently used\ncan still contribute to the convergence of the global model. To reduce training\ntime, an attempt can be made to select devices from tierj−1 in the next round.\nIt’s important to note that, to minimize idle waiting time for devices, as illus-\ntrated in Fig. 3, the proposed approach allows for cross-tier selection. In other\nwords, when selecting tierj, devices from {tier1, ..., tierj} are actually chosen.\n\nTitle Suppressed Due to Excessive Length\n7\nAlgorithm 2: Client Selection\nInput: current tier j, last test accuracy υlast, global model W , tiering of\nclients ts, the number of client training ct, the number of client\nselection in a tier τ.\nOutput: the clients selection Cr.\n1 υ = Evaluation(W, T estData) ;\n2 if υ ≥υlast then\n3\nj = max(j −1, 1) ;\n4 else\n5\nj = min(j + 1, T ) ;\n6 for tier t = 1 →j do\n7\nfor client c in ts[t] do\n8\nprobs[c] =\n1/ct[c]\nP\ni∈ts[t] 1/ct[i] ;\n9\nclients = (select τ clients from tier t with probs) ;\n10\nCr ←clients ;\n11 return Cr;\nj =\n(\nmin(j + 1, M),\nυ < υlast\nmax(j −1, 1),\nυ ≥υlast\n(2)\nTo prevent signiﬁcant diﬀerences in the participation frequency among de-\nvices within tiers, which may lead to model drift [6], we increase the selection\nprobability of devices with fewer participation times when selecting nodes within\ntiers. Therefore, we allocate diﬀerent selection probabilities probs based on the\nparticipation times ct of devices in tierj. Finally, according to the selection prob-\nabilities probs of devices within tiers, τ devices Cr are chosen to participate in\ntraining for this round, as depicted in Algorithm 2.\n3\nExperimental Evaluation\nWe referred to a portion of the implementation methods from Fedlab[5] and im-\nplemented FedDCT and other FL baseline methods using PyTorch. All experi-\nments were conducted on a high-performance server with 2 × Intel(R) Xeon(R)\nGold 6230 CPUs, 128GB of memory, and 2 × NVIDIA Tesla V100 FHHL GPUs.\nWe simulated a scenario where one server and 50 clients participated in FL train-\ning on this machine.\n3.1\nExperimental Setup\nWe conducted experiments on three commonly used datasets, MNIST[8], CIFAR-\n10[7], and Fashion-MNIST[15]. Two classic neural network models, CNN and\nResNet8, were employed for training. We used the CNN model for training on\n\n8\nXian et al.\nMNIST and Fashion-MNIST datasets and the ResNet8 model following the ap-\nproach in the literature [13] for training on CIFAR-10. The proposed approach\nwill be compared with three classic algorithms for synchronous (FedAvg[11]),\nasynchronous (FedAsync[16]), and tiered FL (TiFL[2]).\nWe used momentum as the optimization algorithm with a learning rate of\n0.001 and momentum of 0.9. For each dataset, we trained with the following\nconﬁgurations: local epoch = 1, batch size = 10, τ = 5, β = 0.1, Ω= 30s, κ = 3.\nWe used the same parameters for other FL approaches. The default number of\nselected clients for training in each round was 5, but for FedDCT, the number\nof selected clients per round varied with the selected tier.\nTo simulate the response time diﬀerences caused by resource heterogeneity\nin wireless networks, we assigned random response delays with a variance of\n2 from a Gaussian distribution with expectations of {5, 10, 15, 20, 25} seconds\nfor devices. Additionally, to simulate dropout occurrences, we randomly added\ndelays in the range of (30−60) seconds during training, controlled by the dropout\nrate µ to determine the probability of its occurrence. Finally, to analyze the\ntraining eﬀects under diﬀerent data distribution scenarios, we randomly assigned\na main class to each client, where #% of the data in that device belonged to the\nmain class, and the remaining data belonged to the other classes.\nTable 1. Comparison of the best average accuracy and time which reach the preset\naccuracy of each baseline algorithm. # represents the percentage of primary class label\nin each client. Accuracy shows the best average accuracy achieved after convergence.\nTime represents the time taken by the model to converge to the speciﬁed precision(s).\nFor CIFAR-10, Fashion-MNIST, and MNIST, the convergence accuracy is preset as\n0.7, 0.88, and 0.98, respectively (CIFAR-10 #=0.7 is preset as 0.6 separately). impr.(a)\nand (b) represent the improved training accuracy of FedDCT and the reduced time of\nconvergence to the speciﬁed accuracy compared with the best baseline FL method,\nrespectively.\nDataset\nCIFAR-10\nFashion-MNIST MNIST\n(#Non-IID)\nIID\n#0.3\n#0.5\n#0.7\n#0.7\n#0.7\nFedAvg\nAccuracy 0.7843 0.7407 0.7150 0.6592\n0.8914\n0.9892\nTime(s)\n1617.0 2403.5 3416.2 3033.8\n2544.1\n1481.9\nTiFL\nAccuracy 0.7826 0.7401 0.7071 0.6475\n0.8862\n0.9894\nTime(s)\n1980.8 1945.5 3389.9 2363.2\n2431.4\n1261.6\nFedAsync Accuracy 0.7718 0.7252 0.7001 0.6234\n0.8786\n0.9868\nTime(s)\n3709.6 4885.5 6268.6 7435.5\n6417.0\n2427.4\nFedDCT\nAccuracy 0.7920 0.7526 0.7287 0.6897\n0.9080\n0.9897\nTime(s)\n685.6\n618.5 1479.4 1077.3\n965.8\n864.7\nimpr.(a)\n0.98%\n1.60%\n1.91%\n4.62%\n1.86%\n0.03%\nimpr.(b) 57.6%\n68.2%\n56.3%\n54.4%\n60.2%\n31.4%\n\nTitle Suppressed Due to Excessive Length\n9\n3.2\nExperimental Results\nTable 1 presents the best average accuracy and the time spent to reach the\npreset accuracy for all datasets. The results show that, across all six scenarios,\nthe proposed approach achieved an average accuracy improvement of 1.83% and\nreduced time overhead by 54.7% compared to the optimal baseline. Under the\nsame experimental conﬁguration, FedDCT consistently achieved higher conver-\ngence accuracy and signiﬁcantly reduced convergence time in all experiments.\nParticularly, the improvement compared to TiFL indicates that 1) the dynamic\ntiering in the proposed approach is more accurate and adaptable to changes in\nthe dynamic environment, and 2) the selection of devices across tiers eﬀectively\nexploits device performance, enhancing convergence speed. Meanwhile, we ob-\nserved that TiFL does not perform well in the presence of unexpected dropouts,\nleading to suboptimal convergence accuracy and time.\n0\n5000\n10000\n15000\nTime(s)\n0.3\n0.4\n0.5\n0.6\n0.7\nT\nest Accuracy\nFedAvg\nTiFL\nFedDCT\nFedAsync\nFedAvg\nTiFL\nFedDCT\nFedAsync\n0\n2000\n4000\n6000\nTime(s)\n2403.53\n1945.55\n618.55\n4885.59\nT\narget accuracy: 0.7\n(a) #=0.3\n0\n5000\n10000\n15000\nTime(s)\n0.3\n0.4\n0.5\n0.6\n0.7\nT\nest Accuracy\nFedAvg\nTiFL\nFedDCT\nFedAsync\nFedAvg\nTiFL\nFedDCT\nFedAsync\n0\n2000\n4000\n6000\nTime(s)\n3416.30\n3389.95\n1479.46\n6268.69\nT\narget accuracy: 0.7\n(b) #=0.5\n0\n5000\n10000\n15000\nTime(s)\n0.3\n0.4\n0.5\n0.6\n0.7\nT\nest Accuracy\nFedAvg\nTiFL\nFedDCT\nFedAsync\nFedAvg\nTiFL\nFedDCT\nFedAsync\n0\n2000\n4000\n6000\nTime(s)\n3033.88\n2363.24\n1077.33\n7435.58\nT\narget accuracy: 0.6\n(c) #=0.7\nFig. 4. The eﬀect of diﬀerent # on training.\n0\n5000\n10000\n15000\nTime(s)\n0.3\n0.4\n0.5\n0.6\n0.7\nT\nest Accuracy\nFedAvg\nTiFL\nFedDCT\nFedAsync\nFedAvg\nTiFL\nFedDCT\nFedAsync\n0\n2000\nTime(s)\n1126.19\n694.34\n402.95\n3074.97\nT\narget accuracy: 0.65\n(a) µ=0\n0\n5000\n10000\n15000\nTime(s)\n0.3\n0.4\n0.5\n0.6\n0.7\nT\nest Accuracy\nFedAvg\nTiFL\nFedDCT\nFedAsync\nFedAvg\nTiFL\nFedDCT\nFedAsync\n0\n2000\n4000\nTime(s)\n1976.31\n2124.18\n885.54\n4506.90\nT\narget accuracy: 0.65\n(b) µ=0.1\n0\n5000\n10000\n15000\nTime(s)\n0.3\n0.4\n0.5\n0.6\n0.7\nT\nest Accuracy\nFedAvg\nTiFL\nFedDCT\nFedAsync\nFedAvg\nTiFL\nFedDCT\nFedAsync\n0\n2000\n4000\n6000\nTime(s)\n2610.23\n2515.99\n661.36\n6722.36\nT\narget accuracy: 0.65\n(c) µ=0.4\nFig. 5. The eﬀect of diﬀerent µ on training.\n\n10\nXian et al.\nFig. 4-5 illustrates the training performance of all schemes under diﬀerent\ndata distributions # and various dropout rates µ. Fig. 4 indicates that the\nproposed scheme performs well under diﬀerent data distributions. Although the\noverall convergence accuracy decreases with the increasing heterogeneity of data\ndistribution, our scheme can still achieve faster convergence and higher ﬁnal\nconvergence accuracy compared to other baseline schemes. Fig. 5 demonstrates\nthat as the dropout rate µ increases, the overall convergence time also gradually\nincreases. However, we observe that the impact of the dropout rate µ on the\nconvergence of FedDCT is not signiﬁcant. This is attributed to the dynamic\ntiering module in FedDCT, which can signiﬁcantly alleviate the impact of device\ndropouts on FL.\nFig. 6 presents the training performance of all schemes under diﬀerent net-\nwork environments. Speciﬁcally, in Fig. 6(a), we set the dropout rate µ to 0, and\nin (b), we intensify the response time diﬀerences among devices, with response\ntime expectations set to {1, 3, 10, 30, 100} seconds. The results indicate that the\nproposed scheme exhibits good robustness, achieving favorable results in various\nnetwork environments.\n0\n5000\n10000\n15000\nTime(s)\n0.6\n0.7\n0.8\n0.9\nT\nest Accuracy\nFedAvg\nTiFL\nFedDCT\nFedAsync\nFedAvg\nTiFL\nFedDCT\nFedAsync\n0\n2000\n4000\n6000\nTime(s)\n1780.22\n1441.10\n683.30\n4867.60\nT\narget accuracy: 0.88\n(a) Stable Network\n0\n10000\n20000\n30000\nTime(s)\n0.6\n0.7\n0.8\n0.9\nT\nest Accuracy\nFedAvg\nTiFL\nFedDCT\nFedAsync\nFedAvg\nTiFL\nFedDCT\nFedAsync\n0\n4000\n8000\n12000\n16000\nTime(s)\n9015.13\n7973.90\n2209.85\n15826.52\nT\narget accuracy: 0.88\n(b) Complex Network\nFig. 6. Training performance under diﬀerent network environments.\nFinally, to explore why FedDCT could converge faster, we recorded the se-\nlected tier during the training process, averaged it every 10 rounds, and ﬁtted\nit with a linear regression model. As shown in Fig. 7, the overall trend of the\nselected tier increases with training rounds. It is consistent with the expectations\nof the proposed design. FedDCT ﬁrst uses the clients in the tier with a short\ntraining time for training until it is diﬃcult to improve the accuracy of the global\nmodel, and then uses the clients in the other tier with a longer training time.\n\nTitle Suppressed Due to Excessive Length\n11\n0\n100\n200\n300\n400\nRound\n1\n2\n3\n4\n5\nSelected Tier\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nFig. 7. The changes of the selected tier during the training.\n4\nConclusion\nTo mitigate the adverse impact of wireless networks on the training of FL, this\npaper proposes a novel dynamic cross-tier federated learning Framework. Fed-\nDCT adopts a dynamic tiering approach to reduce waiting times during training\ncaused by resource disparities and unexpected device dropouts, thereby enhanc-\ning the eﬃciency of a single training round. Furthermore, we design a cross-tier\nclient selection algorithm, enabling FedDCT to eﬀectively utilize device training\ninformation for device selection, thereby improving overall convergence eﬃciency\nand accuracy. Experimental results demonstrate that our approach outperforms\ntraditional solutions in wireless networks, achieving superior convergence accu-\nracy and speed.\nAcknowledgments. The research was supported in part by the Guangxi Science and\nTechnology Major Project (No. AA22068070), the National Natural Science Founda-\ntion of China (Nos. 62166004,U21A20474), the Basic Ability Enhancement Program\nfor Young and Middle-aged Teachers of Guangxi (No.2022KY0057, 2023KY0062), In-\nnovation Project of Guangxi Graduate Education (Nos. XYCBZ2024025).\nReferences\n1. AbdulRahman, S., Tout, H., Mourad, A., Talhi, C.: Fedmccs: Multicriteria client\nselection model for optimal iot federated learning. IEEE Internet of Things Journal\n8(6), 4723–4735 (2020)\n2. Chai, Z., Ali, A., Zawad, S., Truex, S., Anwar, A., Baracaldo, N., Zhou, Y., Lud-\nwig, H., Yan, F., Cheng, Y.: Tiﬂ: A tier-based federated learning system. In: Pro-\nceedings of the 29th International Symposium on High-Performance Parallel and\nDistributed Computing. pp. 125–136. ACM (2020)\n3. Chai, Z., Chen, Y., Anwar, A., Zhao, L., Cheng, Y., Rangwala, H.: Fedat: a high-\nperformance and communication-eﬃcient federated learning system with asyn-\nchronous tiers. In: Proceedings of the International Conference for High Perfor-\nmance Computing, Networking, Storage and Analysis. pp. 1–16. ACM (2021)\n\n12\nXian et al.\n4. Duan, Q., Huang, J., Hu, S., Deng, R., Lu, Z., Yu, S.: Combining federated learn-\ning and edge computing toward ubiquitous intelligence in 6g network: Challenges,\nrecent advances, and future directions. IEEE Communications Surveys & Tutorials\n(2023)\n5. Dun Zeng, Siqi Liang, X.H., Xu, Z.: Fedlab: A ﬂexible federated learning frame-\nwork. arXiv preprint arXiv:2107.11621 (2021)\n6. Huang, T., Lin, W., Wu, W., He, L., Li, K., Zomaya, A.Y.: An eﬃciency-boosting\nclient selection scheme for federated learning with fairness guarantee. IEEE Trans-\nactions on Parallel and Distributed Systems 32(7), 1552–1564 (2020)\n7. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny\nimages (2009)\n8. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)\n9. Leng, J., Lin, Z., Ding, M., Wang, P., Smith, D., Vucetic, B.: Client scheduling in\nwireless federated learning based on channel and learning qualities. IEEE Wireless\nCommunications Letters (2022)\n10. Liu, J., Jia, J., Che, T., Huo, C., Ren, J., Zhou, Y., Dai, H., Dou, D.: Fedasmu:\nEﬃcient asynchronous federated learning with dynamic staleness-aware model up-\ndate. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence. vol. 38,\npp. 13900–13908 (2024)\n11. McMahan,\nB.,\nMoore,\nE.,\nRamage,\nD.,\nHampson,\nS.,\ny\nArcas,\nB.A.:\nCommunication-eﬃcient learning of deep networks from decentralized data. In:\nArtiﬁcial intelligence and statistics. pp. 1273–1282. PMLR (2017)\n12. Pfeiﬀer, K., Rapp, M., Khalili, R., Henkel, J.: Federated learning for computa-\ntionally constrained heterogeneous devices: A survey. ACM Computing Surveys\n55(14s), 1–27 (2023)\n13. Shang, X., Lu, Y., Huang, G., Wang, H.: Federated learning on heterogeneous and\nlong-tailed data via classiﬁer re-training with federated features. In: Proceedings of\nthe Thirty-First International Joint Conference on Artiﬁcial Intelligence, IJCAI-22.\npp. 2218–2224 (2022)\n14. Wang, Z., Zhang, Z., Tian, Y., Yang, Q., Shan, H., Wang, W., Quek, T.Q.: Asyn-\nchronous federated learning over wireless communication networks. IEEE Trans-\nactions on Wireless Communications (2022)\n15. Xiao, H., Rasul, K., Vollgraf, R.: Fashion-mnist: a novel image dataset for bench-\nmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 (2017)\n16. Xie, C., Koyejo, S., Gupta, I.: Asynchronous federated optimization. arXiv preprint\narXiv:1903.03934 (2019)\n17. Xu, C., Qu, Y., Xiang, Y., Gao, L.: Asynchronous federated learning on heteroge-\nneous devices: A survey. arXiv preprint arXiv:2109.04269 (2021)\n18. Ye, M., Fang, X., Du, B., Yuen, P.C., Tao, D.: Heterogeneous federated learning:\nState-of-the-art and research challenges. ACM Computing Surveys 56(3), 1–44\n(2023)\n19. Zhang, J., Chen, S., Zhou, X., Wang, X., Lin, Y.B.: Joint scheduling of participants,\nlocal iterations, and radio resources for fair federated learning over mobile edge\nnetworks. IEEE Transactions on Mobile Computing (2022)\n20. Zhang, T., Lam, K.Y., Zhao, J., Li, F., Han, H., Jamil, N.: Enhancing federated\nlearning with spectrum allocation optimization and device selection. IEEE/ACM\nTransactions on Networking (2023)",
    "pdf_filename": "FedDCT_A_Dynamic_Cross-Tier_Federated_Learning_Framework_in_Wireless_Networks.pdf"
}