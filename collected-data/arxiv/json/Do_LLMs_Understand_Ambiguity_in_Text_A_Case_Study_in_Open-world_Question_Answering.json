{
    "title": "Do LLMs Understand Ambiguity in Text? A Case",
    "abstract": "challenges to Large Language Models (LLMs) used for open- domain question answering. LLMs often struggle with the inherent uncertainties of human communication, leading to misinterpretations, miscommunications, hallucinations, and bi- ased responses. This significantly weakens their ability to be used for tasks like fact-checking, question answering, feature extraction, and sentiment analysis. Using open-domain question answering as a test case, we compare off-the-shelf and few-shot LLM performance, focusing on measuring the impact of explicit disambiguationstrategies.Wedemonstratehowsimple,training- free,token-leveldisambiguationmethodsmaybeeffectivelyused Fig.1. Theproblemofambiguityinopendomainquestionanswering(QA) toimproveLLMperformanceforambiguousquestionanswering (left),andhowwetrytosolveitforlargelanguagemodelQA(right). tasks.Weempiricallyshowourfindingsanddiscussbestpractices and broader impacts regarding ambiguity in LLMs. Index Terms—ambiguity, sensitivity, LLM, large language trust and ability to be used for real-world tasks. Ambiguity model, question-answering in natural language poses significant challenges to Large Language Models: much recent work has demonstrated how I. INTRODUCTION LLMs struggle to understand ambiguous text in prompts and Recent years have seen unprecedented advancements in instructions. This is particularly challenging when lay users the development of large language models (LLMs). Today, prompt LLMs for solving tasks, obtaining answers to trivia LLMs are ubiquitous and easily accessible for use by the questions, etc. - in such cases, the LLM may fail to ‘under- general public - either via platforms that allow API calls, stand’ the context and either fail to respond properly, or even such as the OpenAI API1, or through openly available model hallucinate a factually wrong response with high confidence weights for open LLMs, such as via Huggingface2. Since late [3]. Given the importance of evaluating the sensitivity of 2022, large and powerful LLMs have taken over the world LLM to ambiguity in context, in this work we use open- of written communication with at least 56% of students using domain question answering as a test case to compare the AI in their college work according to a survey [1]. Most of off-the-shelf LLM performance on ambiguous questions. In these students, and people overall, harness the conversational ouranalyses,wefurtherexploresimple,training-freemethods capability of this AI for tasks such as problem solving and for disambiguating questions and compare such performance question-answering. Agentic AI workflows have also started with naive prompting. Through experiments on two state-of- to increase in popularity [2], where these LLMs are used for the-art LLMs with a publicly available ambiguous question- NLP tasks such as sentiment analysis and data annotation. answering dataset, we present interesting insights and discuss However,humanlanguageishighlycontext-dependentand implications and best practices. complex. Much of the meaning in language, both spoken and written,comesfromthecontextinwhichitisused,aswellas II. BACKGROUNDANDRELATEDWORK social and psychological cues. This makes it challenging for Large language models are complex neural network based LLMs to grasp human language, which otherwise would be models [4] that are capable of generating human-like text [5]. simple and straightforward for human listeners or readers to Most recent LLMs consist of transformer-based architectures, understand. LLMs often struggle with the inherent uncertain- with a huge number of parameters, on the order of billions. ties of human communication, leading to misinterpretations, Recent LLMs that are most widely used (such as OpenAI’s miscommunications,andbiasedresponseswhichweakentheir GPTfamilyofmodels[6]–[8],MetaAI’sLlama[9],[10],etc.) 1https://platform.openai.com are trained on massive amounts of textual data scraped from 2https://huggingface.co/models the internet, then further fine-tuned using instruction-style 4202 voN 91 ]LC.sc[ 1v59321.1142:viXra",
    "body": "Do LLMs Understand Ambiguity in Text? A Case\nStudy in Open-world Question Answering\nAryan Keluskar Amrita Bhattacharjee Huan Liu\nSchool of Computing & AI School of Computing & AI School of Computing & AI\nArizona State University Arizona State University Arizona State University\nTempe, AZ, USA Tempe, AZ, USA Tempe, AZ, USA\nakeluska@asu.edu abhatt43@asu.edu huanliu@asu.edu\nAbstract—Ambiguity in natural language poses significant\nchallenges to Large Language Models (LLMs) used for open-\ndomain question answering. LLMs often struggle with the\ninherent uncertainties of human communication, leading to\nmisinterpretations, miscommunications, hallucinations, and bi-\nased responses. This significantly weakens their ability to be\nused for tasks like fact-checking, question answering, feature\nextraction, and sentiment analysis. Using open-domain question\nanswering as a test case, we compare off-the-shelf and few-shot\nLLM performance, focusing on measuring the impact of explicit\ndisambiguationstrategies.Wedemonstratehowsimple,training-\nfree,token-leveldisambiguationmethodsmaybeeffectivelyused Fig.1. Theproblemofambiguityinopendomainquestionanswering(QA)\ntoimproveLLMperformanceforambiguousquestionanswering (left),andhowwetrytosolveitforlargelanguagemodelQA(right).\ntasks.Weempiricallyshowourfindingsanddiscussbestpractices\nand broader impacts regarding ambiguity in LLMs.\nIndex Terms—ambiguity, sensitivity, LLM, large language trust and ability to be used for real-world tasks. Ambiguity\nmodel, question-answering in natural language poses significant challenges to Large\nLanguage Models: much recent work has demonstrated how\nI. INTRODUCTION\nLLMs struggle to understand ambiguous text in prompts and\nRecent years have seen unprecedented advancements in instructions. This is particularly challenging when lay users\nthe development of large language models (LLMs). Today, prompt LLMs for solving tasks, obtaining answers to trivia\nLLMs are ubiquitous and easily accessible for use by the questions, etc. - in such cases, the LLM may fail to ‘under-\ngeneral public - either via platforms that allow API calls, stand’ the context and either fail to respond properly, or even\nsuch as the OpenAI API1, or through openly available model hallucinate a factually wrong response with high confidence\nweights for open LLMs, such as via Huggingface2. Since late [3]. Given the importance of evaluating the sensitivity of\n2022, large and powerful LLMs have taken over the world LLM to ambiguity in context, in this work we use open-\nof written communication with at least 56% of students using domain question answering as a test case to compare the\nAI in their college work according to a survey [1]. Most of off-the-shelf LLM performance on ambiguous questions. In\nthese students, and people overall, harness the conversational ouranalyses,wefurtherexploresimple,training-freemethods\ncapability of this AI for tasks such as problem solving and for disambiguating questions and compare such performance\nquestion-answering. Agentic AI workflows have also started with naive prompting. Through experiments on two state-of-\nto increase in popularity [2], where these LLMs are used for the-art LLMs with a publicly available ambiguous question-\nNLP tasks such as sentiment analysis and data annotation. answering dataset, we present interesting insights and discuss\nHowever,humanlanguageishighlycontext-dependentand implications and best practices.\ncomplex. Much of the meaning in language, both spoken and\nwritten,comesfromthecontextinwhichitisused,aswellas II. BACKGROUNDANDRELATEDWORK\nsocial and psychological cues. This makes it challenging for\nLarge language models are complex neural network based\nLLMs to grasp human language, which otherwise would be\nmodels [4] that are capable of generating human-like text [5].\nsimple and straightforward for human listeners or readers to\nMost recent LLMs consist of transformer-based architectures,\nunderstand. LLMs often struggle with the inherent uncertain-\nwith a huge number of parameters, on the order of billions.\nties of human communication, leading to misinterpretations,\nRecent LLMs that are most widely used (such as OpenAI’s\nmiscommunications,andbiasedresponseswhichweakentheir\nGPTfamilyofmodels[6]–[8],MetaAI’sLlama[9],[10],etc.)\n1https://platform.openai.com are trained on massive amounts of textual data scraped from\n2https://huggingface.co/models the internet, then further fine-tuned using instruction-style\n4202\nvoN\n91\n]LC.sc[\n1v59321.1142:viXra\ndata.Suchastepiscalledinstruction-tuning[11]viawhichthe answers from the selected LLMs: (1) a naive (or baseline)\nLLMlearnshowtofollowinstructionsinuserprompts.While direct question-answering prompt, (2) a rephrasing strategy\nrecent LLMs have achieved admirable levels of fluency and that attempts to add linguistic perturbation to the ambiguous\nperformance on a variety of natural language understanding question, and (3) a contextual enrichment approach that uses\n(NLU) as well as general tasks such as code generation, the model’s internal knowledge to disambiguate the given\nsolving math problems and even reasoning [5], [12], [13], question. The selection of these strategies are aimed to com-\nperformance often depends on the way in which these models prehensively measure the models’ accuracy to comprehend\nare prompted. The sensitivity of LLMs to variation in prompt ambiguity through different types of perturbations. We apply\nis an active area of research. thesethreepromptingstrategiestotheLLMsoverarandomly\nMany recent works have looked into the sensitivity of sampled subset of 1,000 ambiguous questions from a larger\nLLMs to minor variations in the prompt [14], such as with publicly available dataset of ambiguous question-answers.\nrespecttotheformat[15],etc.Promptsmayalsohavevarious We also incorporated variations in temperature parameters\ntypes of ambiguity; some recent work has tried looking at the to observe their impact on the consistency and accuracy of\nresponseofLLMstothetaskambiguityintheprompt[16].In the model output; we used two temperature settings: default\nparticular, the aspect of ambiguity in language has also been (1) and low (0.2) on a scale of 0 to 2. The outputs are\nexplored [16]. However, although the concept of ambiguity in collected, stored, and analyzed using two metrics to draw\nnaturallanguagehasbeenexploredsignificantlyfromtheper- comparisonsbetweenthemodels’out-of-the-boxperformance\nspective of computational linguistics [17], [18], the effect on and disambiguation methods. The metrics are chosen such\nambiguous language in prompts on the performance of LLMs that both the meaning and the wording of the final output are\nis still under-explored. Therefore in this work, we investigate assessed.Wedrawcomparativeinsightsintothemodels’base-\nLLMsensitivitytoambiguityinprompts,especiallyinthetask line performance and examine the effect of disambiguation\nof open-domain question answering. techniques into mitigating the inherent challenges posed by\nambiguoushumanlanguage.Wefurtherperformasmall-scale\nIII. PROBLEMDEFINITION\nfine-tuning to evaluate whether task-specific disambiguation\nAmbiguityisinherenttoopen-domainquestionanswering, fine-tuning helps to improve performance.\nmaking it very difficult to prompt these LLMs such that we\nA. Dataset(s)\nget a single, unambiguous answer. The most popular LLMs\nfail to address this in their system cards, even though this Since this analysis is in open-domain question answer-\ncan lead to misinformation and bias when social context is ing, we use the publicly available NQ-Open [20] Dataset\nignoredwhileansweringanambiguousquestion.Forexample, by Google, which contains real-world queries issued to the\nsomeoneasking‘WhatisthehomestadiumoftheCardinals?’ Google search engine before January 2018. It has more than\nhas different answers depending on whether the question is 300,000 question-and-answer pairs, with the answer manually\nabout‘ArizonaCardinalsinfootball’or‘St.LouisCardinalsin annotatedbyreferencinginformationfromWikipedia.Around\nbaseball’.AccordingtoOpenAI,generationofcontentthrough 50% of NQ-Open questions do not have an answer label\na human-like tone increases hallucinations, which makes as- becausetheyareperceivedasambiguous,sincetheannotators\nsessing an LLM on ambiguous questions in human language found diverse sources of ambiguity such as event and entity\nmore important [19]. Given the prevalence of such inherent references. This research focuses primarily on a subset of the\nambiguity in questions from a user prompting an LLM, we dataset: AmbigQA [21] which covers 14,042 questions with a\nmeasuretheperformanceoftheLLMasaquestion-answering diverse set of ambiguities.\nagent on (i) ambiguous questions such as the example above, AmbigQA is divided into 3 subsets: dev, train and test.\nand compare with (ii) disambiguated version of the question. The train set contains 10,036 question-answer pairs, while\nMore formally, say we are given a dataset Damb with triples dev contains 2,002. Each element in test and dev includes\n(q ,qd,aˆ ),i ∈ (1,n) where q is the ambiguous question, a question, one or more answers, the original NQ answer,\ni i i i\nqd is the disambiguated version of that question, and aˆ is contents of the visited Wikipedia pages, search queries used\ni i\nthe ground truth answer for this question. Given an LLM to obtain evidence articles & search results. However, each\nM of choice, we aim to compare M(q ) and M(qd) for element in test contains only a question and contents of\ni i\ni ∈ (1,n), i.e. compare performance across ambiguous and the evidence articles in html and plaintext. NQ-Open and\ndisambiguated questions. some subsets of AmbigQA also contain evidence documents\nattached to each question, which have been proven to be\nIV. METHODOLOGYANDEXPERIMENTALSETTINGS\ncrucialtoidentifyandcharacterizeambiguities.Eachquestion\nWe conduct a series of controlled experiments involving inAmbigQAalsohasmultiplehuman-provideddisambiguated\nthetwoLLMsonadatasetofambiguousreal-worldquestions. questions, where each one of them has a different context.\nOur approach emphasizes the evaluation of LLM sensitivity For this research project, we have used a sample of 1,000\nby measuring the effect of linguistic and contextual modifica- randomquestionsfromthetrainsubset.Theaveragelengthof\ntions on its output accuracy to answer ambiguous questions. a question in this sample is 8.93 words with 47.09 characters,\nWe employed three distinct prompting strategies to generate and that of an answer is 2.30 words with 12.94 characters.\nWithin this sample, 879 questions have one factual answer, Rewrite this question replacing all questions with a what,\n70 have two, 20 have three, and the rest have more than three but retain the meaning by specifying what entity or what\nanswers.InAmbigQA,everyplausibleanswerispairedwitha person or what timeframe the “what” is answering. Also,\ndisambiguatedrewriteoftheoriginalquestion,suchasspecifi- specifythecurrentyearis2018ifneededtoansweratime-\ncallyasking“WhereisthehomestadiumofArizonaCardinals based question.\nin the NFL?” This is particularly helpful in evaluating the Question: {x p}\nlimits of accuracy when used with question disambiguating\nAdding Context to the Ambiguous Question: Since\ntechniques.\nLLMshavevastamountsofworldknowledgeduetotheexten-\nsive pre-training and instruction tuning done on them, we use\nB. LLMs used\nthat world knowledge from LLMs to find and return relevant\nWe use two variants of state-of-the-art LLMs from Ope- informationabouttheambiguousquestion.Thisapproachwas\nnAI:GPT-4o3isamultimodalGPTmodeldevelopedbyOpe- chosen to reduce the effect of scope ambiguity [22], which\nnAI for real-time applications, offering enhanced capabilities canbecausedbydifferingsemanticstructures.Weareableto\nto answer factual questions. GPT-4o mini 4 is a smaller and differentiatebetweentwosentenceswiththesamestructurebut\na cost-effective version of the GPT-4o model. This model has differentmeaningsbecauseofthegeneralknowledgewehave\na context window of 128K tokens and has a knowledge cutoff about the entities it is referring to. After generating context\nof October 2023. Both of these models have been pre-trained aboutthequestion,weappendtheoriginalambiguousquestion\non diverse datasets, including web text, scientific articles, and at the end of this information blob and pass it back to the\nconversationaldata,toenhancetheircontextualunderstanding LLM to obtain the final answer. For our random sample of\nandgenerationcapabilities.Weusethegpt-4oandgpt-4o-mini 1,000 questions in these experiments, the average length of\nmodelendpointsrespectivelyonOpenAIAPI5 toaccessthese the question with added context was 126.90 words, which is\nmodelswithversionasofOctober25th.Additionalparameters approximately 14.2 times longer than the original question.\nincluded top p set to 1 and presence penalty set to 0, which\nAdd extra information to the following question. Also\narebothdefaultsintheAPI,andmax completion tokenswas\nspecify the current month and year is January 2018, so\nleft unspecified. Each experiment is executed using API calls\nanswer questions accordingly. Your aim is to disambiguate\nwithin a few days of each other to minimize the impact of\nwhat it is asking.\nmodel updates.\nQuestion: {x }\np\nC. Disambiguation Methods\nD. Evaluation Metrics\nInourexperiments,weusedtwoprompt-leveldisambigua-\nWe measure the effect of our disambiguation methods on\ntionmethods.Weexperimentedwithavarietyofprompts,and\ntheoverallaccuracyoftheLLMsbyusingsemanticsimilarity\nfinally selected two appropriate prompts for disambiguating a\nbetween the LLM responses and the ground truth responses.\ngivenquestionusingthetwoGPTmodels.Wealsoanalyzethe\nWedothisovermeasuringtokenoverlapdirectlysinceseveral\neffect of lowering the temperature parameter on the model’s\ninstances have ground truth answers that may be rephrased\naccuracy in answering ambiguous questions.\nin multiple ways, all of which are correct. This allows for\nNaive: For each question, we prompt the out-of-the-box\nmore meaningful evaluations by taking phrasing variations\nLLM to answer it as concisely as possible to get a baseline\ninto account. Specifically, we use OpenAI’s text-embedding-\nfor our experiment.\n3-large6 vector embedding model to generate the vectors\nAnswer the question as concisely as possible with ONLY and then computed the cosine similarity metric between two\none answer without any other text: given texts. The distances are normalized on a scale from 0\nQuestion: {x p} to 1, where a value of 1 indicates that the compared texts\nare identical, while lower values suggest varying degrees of\nRephrase using What: In preliminary experiments, we\ndivergence. In our experiments and results, we measure:\nfound that rephrasing a question to begin with “what” makes\n1. Distance Between Ambiguous and Disambiguated\nit more specific than the initial ambiguous question, reducing\nQuestions: This metric quantifies the semantic shift intro-\nthe variability of responses. After obtaining the rephrased\nduced by each disambiguating prompt in the question. Since\nquestion, we pass it back to the LLM to obtain the final\nwe have an upper bound for the performance of an LLM\nanswer. For our random sample of 1,000 questions in these\nwith disambiguation, this metric also allows us to observe\nexperiments,theaveragelengthoftherephrasedquestionwas\nhow effective were our prompts in changing the question in\n13.01words,whichisabout1.45timeslongerthantheoriginal\ncomparison to the given disambiguated question.\nquestion.\n2. Distance Between Baseline Answer and Disam-\nbiguated Answer: This measures the difference between the\n3https://platform.openai.com/docs/models/gpt-4o\n4https://openai.com/index/gpt-4o-mini-advancing-cost-efficient- responses generated by the LLM when given the original\nintelligence/\n5https://platform.openai.com/docs/guides/text-generation 6https://platform.openai.com/docs/guides/embeddings\nFig.2. KernelDensityEstimate(KDE)PlottocomparetheCosineSimilarity Fig.3. KernelDensityEstimate(KDE)PlottocomparetheCosineSimilarity\nbetweenGroundTruthAnswerandLLMResponseforthe2disambiguation betweenGroundTruthAnswerandLLMResponseforthe2disambiguation\nstrategiesfortherandomlysampledsubsetof1,000AmbiguousQuestions. strategies for the subset of AmbigQA where human-provided answer for\nhuman-provideddisambiguatedquestionmatchedthegroundtruth.\nTABLEI\nPERFORMANCEOFGPT-4OONAMBIGUOUSANDDISAMBIGUATED TABLEII\nQUESTIONS. PERFORMANCEOFGPT-4O-MINIONAMBIGUOUSANDDISAMBIGUATED\nQUESTIONS.\nDisamb. Disamb. Upper-bound(viaGT\nMetric Naive\nvia‘what’ viacontext disamb.questions) Disamb. Disamb. Upper-bound(viaGT\nMetric Naive\nvia‘what’ viacontext disamb.questions)\nQuestion\n- 0.905 0.743 0.317\ncoherence Question\n- 0.907 0.739 0.317\ncoherence\nNaiveAnswer\n- 0.826 0.799 0.895\nOverlap NaiveAnswer\n- 0.771 0.739 0.745\nOverlap\nGTAnswer\n0.759 0.778 0.789 0.858\nOverlap↑ GTAnswer\n0.692 0.707 0.71 0.783\nOverlap↑\nambiguous question and the disambiguated question. A larger\ndistance in this case implies that the model’s answer remains • RQ1: How well do off-the-shelf LLMs perform on am-\nrelatively consistent across different contexts and phrasings, biguous QA in a zero shot setting and does training free-\nindicating robustness while answering ambiguity. disambiguation help?\n3. Distance Between Baseline Answer and Ground • RQ2: Does few-shot fine-tuning improve performance?\nTruth: This metric acts as a reference point to evaluate the • RQ3:DoesreducingthetemperatureforLLMgeneration\ninitialeffectivenessoftheout-of-the-boxLLManswersbefore help in improving performance?\nany disambiguation. This provides insights into how closely RQ1: We show the results for RQ1 for GPT-4o and\nthe model approximates human-annotated answers without GPT-4o-mini in Tables I and II respectively. In both tables,\nadditionalprompts.Comparingthisdistancewithdistancesin- Question coherencereferstothesemanticsimilaritybetween\nvolvingdisambiguatedstrategiesishowwemeasureimprove- the ground truth disambiguated question and the ambiguous\nments in accuracy achieved through the proposed prompting question when disambiguated via the LLM following one\ntechniques. of the two methods; Naive Answer Overlap refers to the\n4.DistanceBetweenGroundTruthandDisambiguated semantic similarity between LLM responses obtained via the\nAnswer: This metric measures the effectiveness of each disambiguating prompts vs. the naive prompt; and finally GT\ndisambiguation method in bridging the gap between model- Answer Overlap refers to the semantic similarity between\ngeneratedanswersandhuman-annotatedgroundtruthanswers. the LLM response and the ground truth answer in the dataset.\nAn increase in this metric after disambiguation indicates that Ideally, we want higher values for this metric. Interestingly,\nthechosenpromptsandcontextualenrichmenttechniqueshave we see that for both GPT 4o and 4o-mini, using simple\nbeen able to pivot the LLM towards generating more accurate disambiguating prompts improves performance over the naive\nanswers to ambiguous questions. setting, implying that simple prompt-based, training-free ap-\nproaches may be useful in improving LLM performance for\nV. RESULTSANDDISCUSSION\nambiguous queries. Out of the two simple disambiguating\nIn order to understand how leading LLMs tackle ambigu- methods explored, we see that disambiguation via adding\nousquestionsinaquestionansweringtask,weinvestigateand context performs better for both LLMs.\naim to answer the following research question: RQ2: To evaluate whether small scale fine-tuning helps\nminor improvements in some cases, the difference is not\nthat significant. This implies simply using a lower value of\ntemperaturemaynotprovideanybenefitsinLLMperformance\nfor answering ambiguous questions.\nProblem with naive contextual enrichment: The Figures\n2 and 3 show why the average is not going up when an\nLLM is prompted to insert context into a question. Although\nadding context should skew the plot 2 to the right (ie: be\nmore similar to the ground truth), but instead its skewed to\nthe left since its being held back every time it adds the wrong\ncontextwhichisnotaproblemwhenwehavesimplyrephrased\nthe question. Then, we take a subset of AmbigQA where the\nhuman-provided answer of a human-provided disambiguated\nquestion exactly matches the ground truth answer. Here, we\nFig. 4. Comparison of GT Answer Overlap for GPT 4o and 4o-mini for\nseethatplot3ofcontextualenrichmentdoesskewtotheright.\nbothhighandlowtemperatures.High=1.0,low=0.2.Higheroverlapscores\narebetter. This shows that LLMs are able to better understand certain\nsocial cues to correctly disambiguate the provided question in\ncases where the human annotator was able to disambiguate\nin improving LLM performance on ambiguous questions, we\nthem as well.\nperform few-shot fine-tuning on GPT 4o-mini 7. To adapt\nour model for handling ambiguous questions, we fine-tuned\nthe model using OpenAI’s API. We randomly sampled 50\nquestion-answerpairsfromAmbigQA.Eachambiguousques-\nVI. CONCLUSIONANDFUTUREWORKS\ntion was stored as the prompt for the LLM, with ground truth\nbeing stored as the expected response from the LLM. The file Our results indicate that contextual enrichment has the\nwasformattedasshownbelowforthe50questions.Usingthis, ability to significantly enhance model disambiguation accu-\nwe initiated a fine-tuning job on OpenAI’s fine-tuning API 8 racy,butitisofteninaccuratebecauseittendstoaddirrelevant\nwhich returned a model checkpoint. We used this fine-tuned contexttoquestions,makingthemimpossibletofixbyprompt-\nmodel ID instead of gpt-4o-mini within our baseline prompt ing. However, when we took a subset of AmbigQA where the\nconfiguration. human-provided answer of a human-provided disambiguated\nquestionprovidedmatchesthegroundtruth,addingcontextto\n{“messages”:\nthose questions increases the accuracy of the model. There-\n{“role”: “user”, “content”: <ambiguous question>},\nfore, our analysis shows that even though LLMs struggle\n{“role”: “assistant”, “content”: <ground truth answer>”}}\nwith ambiguity in prompts, simple training-free prompt-based\nTo evaluate the performance of this fine-tuned model, we disambiguation methods may help significantly in improving\nsample1,000ambiguousquestionsfromthedatasetatrandom the performance of the LLM.\nandcomparetheperformancebetweennaivepromptingonthe\nIn future work, we plan to fine-tune the LLM for accurate\n4o-minimodelandnaivepromptingonthefine-tuned4o-mini\ncontext-enhancement. We will take the contextually enriched\nmodel. The GT Answer Overlap for the 4o-mini model is\ninformation blob and fine-tune the model to generate a dis-\n0.643 while that for the fine-tuned 4o-mini model is 0.626.\nambiguated question that is as close as possible to human-\nTherefore, we see that fine-tuning, at least at this small scale,\nprovided disambiguation to maximize accuracy for question-\ndoes not provide any improvement in LLM performance on\ndisambiguation based strategies. This will help in increasing\nambiguous questions. This reinforces our insight that simple\ntheaccuracyoverallsinceLLMsintheopen-worlddonothave\ntraining-freepromptingmethodsfordisambiguationworkwell\naccess to disambiguated versions of a question, so through\nin improving performance.\nthis fine-tuning we expect the LLM to learn social cues while\nRQ3:UsingalowertemperaturevalueforLLMgeneration\ndisambiguating a question, which will improve the accuracy\nresults in reducing the ‘stochasticity’ of the generated text,\nfor questions outside of AmbigQA dataset. We also plan to\nwhereby the variance is reduced and the generated text is\nassesstheseprompt-baseddisambiguationtechniquesinopen-\nmore predictable over multiple runs. As an ablation, we eval-\nsource models such as Llama-3.1-8B-Instruct and Mixtral-\nuate whether lower values of temperature help in improving\n8x7B [23], as well as to test the performance of our fine-\nperformance of the LLM on ambiguous questions. We show\ntuned model on general factual question-answering by using\nthe results for this in Figure 4: we see that although lower\nSimpleQA by OpenAI and other questions from the NQ-\ntemperature (0.2 instead of 1.0, in this case) seem to have\nOpen dataset. This is a promising way forward to reduce\nhallucinations caused by ambiguity and better guard for an\n7owingtothelowercostoffine-tuningtheminiversionofGPT-4o.\nLLM’s prompt sensitivity when its tasked with answering\n8https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-\nfine-tuned ambiguous questions.\nLIMITATIONS [10] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama\nThis study adopted a general form of “ambiguity” in ana-\n2: Open foundation and fine-tuned chat models,” arXiv preprint\nlyzingtheperformanceofLargeLanguageModels.Werecog- arXiv:2307.09288,2023.\nnizethatamorethoroughinvestigationofvarioustypesofam- [11] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu,\nT. Zhang, F. Wu et al., “Instruction tuning for large language models:\nbiguity—such as multipleanswers, time-dependent interpreta-\nAsurvey,”arXivpreprintarXiv:2308.10792,2023.\ntions, multiple answer types, and other specific forms—could [12] A.BhattacharjeeandH.Liu,“Fightingfirewithfire:canchatgptdetect\nprovide deeper insights into how a model falters in different ai-generated text?” ACM SIGKDD Explorations Newsletter, vol. 25,\nno.2,pp.14–21,2024.\nkinds of ambiguity.\n[13] H.Liu,Z.Zheng,Y.Qiao,H.Duan,Z.Fei,F.Zhou,W.Zhang,S.Zhang,\nAdditionally, we suspect that our fine-tuning approach D.Lin,andK.Chen,“Mathbench:Evaluatingthetheoryandapplication\nmay have underperformed due to catastrophic forgetting [24]. proficiencyofllmswithahierarchicalmathematicsbenchmark,”arXiv\npreprintarXiv:2405.12209,2024.\nAs suggested in Section VI, future work could adopt more\n[14] A.SalinasandF.Morstatter,“Thebutterflyeffectofalteringprompts:\ntargeted and intentful fine-tuning strategies, such as devel- How small changes and jailbreaks affect large language model perfor-\nopment of a dedicated question disambiguation model or mance,”arXivpreprintarXiv:2401.03729,2024.\n[15] M. Sclar, Y. Choi, Y. Tsvetkov, and A. Suhr, “Quantifying language\nthe application of linguistic refinements that current LLMs\nmodels’ sensitivity to spurious features in prompt design or: How i\ncannotperforminazero-shotsetting.Futurestudiescouldalso learned to start worrying about prompt formatting,” in The Twelfth\nmitigate the effects of catastrophic forgetting with methods InternationalConferenceonLearningRepresentations,2024.[Online].\nAvailable:https://openreview.net/forum?id=RIu5lyNXjT\nsuch as Singular Value Decomposition [25]. Implementing\n[16] A.Tamkin,K.Handa,A.Shrestha,andN.Goodman,“Taskambiguityin\nsuch strategies could enhance the stability and effectiveness humansandlanguagemodels,”arXivpreprintarXiv:2212.10711,2022.\nof fine-tuned models in answering ambiguous questions. [17] E. Davis. (na) Notes on ambiguity. [Online]. Available: https:\n//cs.nyu.edu/∼davise/ai/ambiguity.html\nREFERENCES [18] H. Levesque, E. Davis, and L. Morgenstern, “The winograd schema\nchallenge,” in Thirteenth international conference on the principles of\n[1] J. Nam, “56% of college students have used AI on assignments knowledgerepresentationandreasoning,2012.\nor exams: Bestcolleges,” Nov 2023. [Online]. Available: https://www. [19] OpenAI, “Gpt-4o system card,” 2024. [Online]. Available: https:\nbestcolleges.com/research/most-college-students-have-used-ai-survey/ //openai.com/index/gpt-4o-system-card/\n[2] S.Manjrekar.(2024)Agenticaimakesautonomousenterprisesareality. [20] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\n[Online].Available:https://www.forbes.com/councils/forbestechcouncil/ C. Alberti, D. Epstein, I. Polosukhin, M. Kelcey, J. Devlin, K. Lee,\n2024/10/30/agentic-ai-makes-autonomous-enterprises-a-reality/ K.N.Toutanova,L.Jones,M.-W.Chang,A.Dai,J.Uszkoreit,Q.Le,\n[3] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, andS.Petrov,“Naturalquestions:abenchmarkforquestionanswering\nE. Zhao, Y. Zhang, Y. Chen, L. Wang, A. T. Luu, W. Bi, research,”TransactionsoftheAssociationofComputationalLinguistics,\nF. Shi, and S. Shi, “Siren’s song in the ai ocean: A survey on 2019.\nhallucination in large language models,” 2023. [Online]. Available: [21] S. Min, J. Michael, H. Hajishirzi, and L. Zettlemoyer, “AmbigQA:\nhttps://arxiv.org/abs/2309.01219 Answeringambiguousopen-domainquestions,”inEMNLP,2020.\n[4] A.Vaswani,“Attentionisallyouneed,”AdvancesinNeuralInformation [22] G. Kamath, S. Schuster, S. Vajjala, and S. Reddy, “Scope ambiguities\nProcessingSystems,2017. inlargelanguagemodels,”TransactionsoftheAssociationforCompu-\n[5] S.Bubeck,V.Chandrasekaran,R.Eldan,J.Gehrke,E.Horvitz,E.Ka- tationalLinguistics,vol.12,pp.738–754,2024.\nmar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg et al., “Sparks of artificial [23] A.Jiang,A.Sablayrolles,A.Mensch,C.Bamford,D.Chaplot,D.delas\ngeneral intelligence: Early experiments with gpt-4,” arXiv preprint Casas,F.Bressand,G.Lengyel,G.Lample,L.Saulnieretal.,“Mistral\narXiv:2303.12712,2023. 7b(2023),”arXivpreprintarXiv:2310.06825,2023.\n[6] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,I.Sutskeveretal., [24] Y. Luo, Z. Yang, F. Meng, Y. Li, J. Zhou, and Y. Zhang,\n“Language models are unsupervised multitask learners,” OpenAI blog, “An empirical study of catastrophic forgetting in large language\nvol.1,no.8,p.9,2019. models during continual fine-tuning,” 2024. [Online]. Available:\n[7] T.B.Brown,“Languagemodelsarefew-shotlearners,”arXivpreprint https://arxiv.org/abs/2308.08747\narXiv:2005.14165,2020. [25] J. K. Franke, M. Hefenbrock, and F. Hutter, “Preserving principal\n[8] J.Achiam,S.Adler,S.Agarwal,L.Ahmad,I.Akkaya,F.L.Aleman, subspaces to reduce catastrophic forgetting in fine-tuning,” in ICLR\nD. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4 2024 Workshop on Mathematical and Empirical Understanding of\ntechnicalreport,”arXivpreprintarXiv:2303.08774,2023. Foundation Models, 2024. [Online]. Available: https://openreview.net/\n[9] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, forum?id=XoWtroECJU\nT. Lacroix, B. Rozie`re, N. Goyal, E. Hambro, F. Azhar et al.,\n“Llama:Openandefficientfoundationlanguagemodels,”arXivpreprint\narXiv:2302.13971,2023.",
    "pdf_filename": "Do_LLMs_Understand_Ambiguity_in_Text_A_Case_Study_in_Open-world_Question_Answering.pdf"
}