{
    "title": "Do LLMs Understand Ambiguity in Text A Case Study in Open-world Question Answering",
    "abstract": "challenges to Large Language Models (LLMs) used for open- domain question answering. LLMs often struggle with the inherent uncertainties of human communication, leading to misinterpretations, miscommunications, hallucinations, and bi- ased responses. This significantly weakens their ability to be used for tasks like fact-checking, question answering, feature extraction, and sentiment analysis. Using open-domain question answering as a test case, we compare off-the-shelf and few-shot LLM performance, focusing on measuring the impact of explicit disambiguation strategies. We demonstrate how simple, training- free, token-level disambiguation methods may be effectively used to improve LLM performance for ambiguous question answering tasks. We empirically show our findings and discuss best practices and broader impacts regarding ambiguity in LLMs. Index Terms—ambiguity, sensitivity, LLM, large language model, question-answering I. INTRODUCTION Recent years have seen unprecedented advancements in the development of large language models (LLMs). Today, LLMs are ubiquitous and easily accessible for use by the general public - either via platforms that allow API calls, such as the OpenAI API1, or through openly available model weights for open LLMs, such as via Huggingface2. Since late 2022, large and powerful LLMs have taken over the world of written communication with at least 56% of students using AI in their college work according to a survey [1]. Most of these students, and people overall, harness the conversational capability of this AI for tasks such as problem solving and question-answering. Agentic AI workflows have also started to increase in popularity [2], where these LLMs are used for NLP tasks such as sentiment analysis and data annotation. However, human language is highly context-dependent and complex. Much of the meaning in language, both spoken and written, comes from the context in which it is used, as well as social and psychological cues. This makes it challenging for LLMs to grasp human language, which otherwise would be simple and straightforward for human listeners or readers to understand. LLMs often struggle with the inherent uncertain- ties of human communication, leading to misinterpretations, miscommunications, and biased responses which weaken their 1https://platform.openai.com 2https://huggingface.co/models Fig. 1. The problem of ambiguity in open domain question answering (QA) (left), and how we try to solve it for large language model QA (right). trust and ability to be used for real-world tasks. Ambiguity in natural language poses significant challenges to Large Language Models: much recent work has demonstrated how LLMs struggle to understand ambiguous text in prompts and instructions. This is particularly challenging when lay users prompt LLMs for solving tasks, obtaining answers to trivia questions, etc. - in such cases, the LLM may fail to ‘under- stand’ the context and either fail to respond properly, or even hallucinate a factually wrong response with high confidence [3]. Given the importance of evaluating the sensitivity of LLM to ambiguity in context, in this work we use open- domain question answering as a test case to compare the off-the-shelf LLM performance on ambiguous questions. In our analyses, we further explore simple, training-free methods for disambiguating questions and compare such performance with naive prompting. Through experiments on two state-of- the-art LLMs with a publicly available ambiguous question- answering dataset, we present interesting insights and discuss implications and best practices. II. BACKGROUND AND RELATED WORK Large language models are complex neural network based models [4] that are capable of generating human-like text [5]. Most recent LLMs consist of transformer-based architectures, with a huge number of parameters, on the order of billions. Recent LLMs that are most widely used (such as OpenAI’s GPT family of models [6]–[8], Meta AI’s Llama [9], [10], etc.) are trained on massive amounts of textual data scraped from the internet, then further fine-tuned using instruction-style arXiv:2411.12395v1  [cs.CL]  19 Nov 2024",
    "body": "Do LLMs Understand Ambiguity in Text? A Case\nStudy in Open-world Question Answering\nAryan Keluskar\nSchool of Computing & AI\nArizona State University\nTempe, AZ, USA\nakeluska@asu.edu\nAmrita Bhattacharjee\nSchool of Computing & AI\nArizona State University\nTempe, AZ, USA\nabhatt43@asu.edu\nHuan Liu\nSchool of Computing & AI\nArizona State University\nTempe, AZ, USA\nhuanliu@asu.edu\nAbstract—Ambiguity in natural language poses significant\nchallenges to Large Language Models (LLMs) used for open-\ndomain question answering. LLMs often struggle with the\ninherent uncertainties of human communication, leading to\nmisinterpretations, miscommunications, hallucinations, and bi-\nased responses. This significantly weakens their ability to be\nused for tasks like fact-checking, question answering, feature\nextraction, and sentiment analysis. Using open-domain question\nanswering as a test case, we compare off-the-shelf and few-shot\nLLM performance, focusing on measuring the impact of explicit\ndisambiguation strategies. We demonstrate how simple, training-\nfree, token-level disambiguation methods may be effectively used\nto improve LLM performance for ambiguous question answering\ntasks. We empirically show our findings and discuss best practices\nand broader impacts regarding ambiguity in LLMs.\nIndex Terms—ambiguity, sensitivity, LLM, large language\nmodel, question-answering\nI. INTRODUCTION\nRecent years have seen unprecedented advancements in\nthe development of large language models (LLMs). Today,\nLLMs are ubiquitous and easily accessible for use by the\ngeneral public - either via platforms that allow API calls,\nsuch as the OpenAI API1, or through openly available model\nweights for open LLMs, such as via Huggingface2. Since late\n2022, large and powerful LLMs have taken over the world\nof written communication with at least 56% of students using\nAI in their college work according to a survey [1]. Most of\nthese students, and people overall, harness the conversational\ncapability of this AI for tasks such as problem solving and\nquestion-answering. Agentic AI workflows have also started\nto increase in popularity [2], where these LLMs are used for\nNLP tasks such as sentiment analysis and data annotation.\nHowever, human language is highly context-dependent and\ncomplex. Much of the meaning in language, both spoken and\nwritten, comes from the context in which it is used, as well as\nsocial and psychological cues. This makes it challenging for\nLLMs to grasp human language, which otherwise would be\nsimple and straightforward for human listeners or readers to\nunderstand. LLMs often struggle with the inherent uncertain-\nties of human communication, leading to misinterpretations,\nmiscommunications, and biased responses which weaken their\n1https://platform.openai.com\n2https://huggingface.co/models\nFig. 1. The problem of ambiguity in open domain question answering (QA)\n(left), and how we try to solve it for large language model QA (right).\ntrust and ability to be used for real-world tasks. Ambiguity\nin natural language poses significant challenges to Large\nLanguage Models: much recent work has demonstrated how\nLLMs struggle to understand ambiguous text in prompts and\ninstructions. This is particularly challenging when lay users\nprompt LLMs for solving tasks, obtaining answers to trivia\nquestions, etc. - in such cases, the LLM may fail to ‘under-\nstand’ the context and either fail to respond properly, or even\nhallucinate a factually wrong response with high confidence\n[3]. Given the importance of evaluating the sensitivity of\nLLM to ambiguity in context, in this work we use open-\ndomain question answering as a test case to compare the\noff-the-shelf LLM performance on ambiguous questions. In\nour analyses, we further explore simple, training-free methods\nfor disambiguating questions and compare such performance\nwith naive prompting. Through experiments on two state-of-\nthe-art LLMs with a publicly available ambiguous question-\nanswering dataset, we present interesting insights and discuss\nimplications and best practices.\nII. BACKGROUND AND RELATED WORK\nLarge language models are complex neural network based\nmodels [4] that are capable of generating human-like text [5].\nMost recent LLMs consist of transformer-based architectures,\nwith a huge number of parameters, on the order of billions.\nRecent LLMs that are most widely used (such as OpenAI’s\nGPT family of models [6]–[8], Meta AI’s Llama [9], [10], etc.)\nare trained on massive amounts of textual data scraped from\nthe internet, then further fine-tuned using instruction-style\narXiv:2411.12395v1  [cs.CL]  19 Nov 2024\n\ndata. Such a step is called instruction-tuning [11] via which the\nLLM learns how to follow instructions in user prompts. While\nrecent LLMs have achieved admirable levels of fluency and\nperformance on a variety of natural language understanding\n(NLU) as well as general tasks such as code generation,\nsolving math problems and even reasoning [5], [12], [13],\nperformance often depends on the way in which these models\nare prompted. The sensitivity of LLMs to variation in prompt\nis an active area of research.\nMany recent works have looked into the sensitivity of\nLLMs to minor variations in the prompt [14], such as with\nrespect to the format [15], etc. Prompts may also have various\ntypes of ambiguity; some recent work has tried looking at the\nresponse of LLMs to the task ambiguity in the prompt [16]. In\nparticular, the aspect of ambiguity in language has also been\nexplored [16]. However, although the concept of ambiguity in\nnatural language has been explored significantly from the per-\nspective of computational linguistics [17], [18], the effect on\nambiguous language in prompts on the performance of LLMs\nis still under-explored. Therefore in this work, we investigate\nLLM sensitivity to ambiguity in prompts, especially in the task\nof open-domain question answering.\nIII. PROBLEM DEFINITION\nAmbiguity is inherent to open-domain question answering,\nmaking it very difficult to prompt these LLMs such that we\nget a single, unambiguous answer. The most popular LLMs\nfail to address this in their system cards, even though this\ncan lead to misinformation and bias when social context is\nignored while answering an ambiguous question. For example,\nsomeone asking ‘What is the home stadium of the Cardinals?’\nhas different answers depending on whether the question is\nabout ‘Arizona Cardinals in football’ or ‘St. Louis Cardinals in\nbaseball’. According to OpenAI, generation of content through\na human-like tone increases hallucinations, which makes as-\nsessing an LLM on ambiguous questions in human language\nmore important [19]. Given the prevalence of such inherent\nambiguity in questions from a user prompting an LLM, we\nmeasure the performance of the LLM as a question-answering\nagent on (i) ambiguous questions such as the example above,\nand compare with (ii) disambiguated version of the question.\nMore formally, say we are given a dataset Damb with triples\n(qi, qd\ni , ˆai), i ∈(1, n) where qi is the ambiguous question,\nqd\ni is the disambiguated version of that question, and ˆai is\nthe ground truth answer for this question. Given an LLM\nM of choice, we aim to compare M(qi) and M(qd\ni ) for\ni ∈(1, n), i.e. compare performance across ambiguous and\ndisambiguated questions.\nIV. METHODOLOGY AND EXPERIMENTAL SETTINGS\nWe conduct a series of controlled experiments involving\nthe two LLMs on a dataset of ambiguous real-world questions.\nOur approach emphasizes the evaluation of LLM sensitivity\nby measuring the effect of linguistic and contextual modifica-\ntions on its output accuracy to answer ambiguous questions.\nWe employed three distinct prompting strategies to generate\nanswers from the selected LLMs: (1) a naive (or baseline)\ndirect question-answering prompt, (2) a rephrasing strategy\nthat attempts to add linguistic perturbation to the ambiguous\nquestion, and (3) a contextual enrichment approach that uses\nthe model’s internal knowledge to disambiguate the given\nquestion. The selection of these strategies are aimed to com-\nprehensively measure the models’ accuracy to comprehend\nambiguity through different types of perturbations. We apply\nthese three prompting strategies to the LLMs over a randomly\nsampled subset of 1,000 ambiguous questions from a larger\npublicly available dataset of ambiguous question-answers.\nWe also incorporated variations in temperature parameters\nto observe their impact on the consistency and accuracy of\nthe model output; we used two temperature settings: default\n(1) and low (0.2) on a scale of 0 to 2. The outputs are\ncollected, stored, and analyzed using two metrics to draw\ncomparisons between the models’ out-of-the-box performance\nand disambiguation methods. The metrics are chosen such\nthat both the meaning and the wording of the final output are\nassessed. We draw comparative insights into the models’ base-\nline performance and examine the effect of disambiguation\ntechniques into mitigating the inherent challenges posed by\nambiguous human language. We further perform a small-scale\nfine-tuning to evaluate whether task-specific disambiguation\nfine-tuning helps to improve performance.\nA. Dataset(s)\nSince this analysis is in open-domain question answer-\ning, we use the publicly available NQ-Open [20] Dataset\nby Google, which contains real-world queries issued to the\nGoogle search engine before January 2018. It has more than\n300,000 question-and-answer pairs, with the answer manually\nannotated by referencing information from Wikipedia. Around\n50% of NQ-Open questions do not have an answer label\nbecause they are perceived as ambiguous, since the annotators\nfound diverse sources of ambiguity such as event and entity\nreferences. This research focuses primarily on a subset of the\ndataset: AmbigQA [21] which covers 14,042 questions with a\ndiverse set of ambiguities.\nAmbigQA is divided into 3 subsets: dev, train and test.\nThe train set contains 10,036 question-answer pairs, while\ndev contains 2,002. Each element in test and dev includes\na question, one or more answers, the original NQ answer,\ncontents of the visited Wikipedia pages, search queries used\nto obtain evidence articles & search results. However, each\nelement in test contains only a question and contents of\nthe evidence articles in html and plaintext. NQ-Open and\nsome subsets of AmbigQA also contain evidence documents\nattached to each question, which have been proven to be\ncrucial to identify and characterize ambiguities. Each question\nin AmbigQA also has multiple human-provided disambiguated\nquestions, where each one of them has a different context.\nFor this research project, we have used a sample of 1,000\nrandom questions from the train subset. The average length of\na question in this sample is 8.93 words with 47.09 characters,\nand that of an answer is 2.30 words with 12.94 characters.\n\nWithin this sample, 879 questions have one factual answer,\n70 have two, 20 have three, and the rest have more than three\nanswers. In AmbigQA, every plausible answer is paired with a\ndisambiguated rewrite of the original question, such as specifi-\ncally asking “Where is the home stadium of Arizona Cardinals\nin the NFL?” This is particularly helpful in evaluating the\nlimits of accuracy when used with question disambiguating\ntechniques.\nB. LLMs used\nWe use two variants of state-of-the-art LLMs from Ope-\nnAI: GPT-4o 3 is a multimodal GPT model developed by Ope-\nnAI for real-time applications, offering enhanced capabilities\nto answer factual questions. GPT-4o mini 4 is a smaller and\na cost-effective version of the GPT-4o model. This model has\na context window of 128K tokens and has a knowledge cutoff\nof October 2023. Both of these models have been pre-trained\non diverse datasets, including web text, scientific articles, and\nconversational data, to enhance their contextual understanding\nand generation capabilities. We use the gpt-4o and gpt-4o-mini\nmodel endpoints respectively on OpenAI API 5 to access these\nmodels with version as of October 25th. Additional parameters\nincluded top p set to 1 and presence penalty set to 0, which\nare both defaults in the API, and max completion tokens was\nleft unspecified. Each experiment is executed using API calls\nwithin a few days of each other to minimize the impact of\nmodel updates.\nC. Disambiguation Methods\nIn our experiments, we used two prompt-level disambigua-\ntion methods. We experimented with a variety of prompts, and\nfinally selected two appropriate prompts for disambiguating a\ngiven question using the two GPT models. We also analyze the\neffect of lowering the temperature parameter on the model’s\naccuracy in answering ambiguous questions.\nNaive: For each question, we prompt the out-of-the-box\nLLM to answer it as concisely as possible to get a baseline\nfor our experiment.\nAnswer the question as concisely as possible with ONLY\none answer without any other text:\nQuestion: {xp}\nRephrase using What: In preliminary experiments, we\nfound that rephrasing a question to begin with “what” makes\nit more specific than the initial ambiguous question, reducing\nthe variability of responses. After obtaining the rephrased\nquestion, we pass it back to the LLM to obtain the final\nanswer. For our random sample of 1,000 questions in these\nexperiments, the average length of the rephrased question was\n13.01 words, which is about 1.45 times longer than the original\nquestion.\n3https://platform.openai.com/docs/models/gpt-4o\n4https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-\nintelligence/\n5https://platform.openai.com/docs/guides/text-generation\nRewrite this question replacing all questions with a what,\nbut retain the meaning by specifying what entity or what\nperson or what timeframe the “what” is answering. Also,\nspecify the current year is 2018 if needed to answer a time-\nbased question.\nQuestion: {xp}\nAdding Context to the Ambiguous Question: Since\nLLMs have vast amounts of world knowledge due to the exten-\nsive pre-training and instruction tuning done on them, we use\nthat world knowledge from LLMs to find and return relevant\ninformation about the ambiguous question. This approach was\nchosen to reduce the effect of scope ambiguity [22], which\ncan be caused by differing semantic structures. We are able to\ndifferentiate between two sentences with the same structure but\ndifferent meanings because of the general knowledge we have\nabout the entities it is referring to. After generating context\nabout the question, we append the original ambiguous question\nat the end of this information blob and pass it back to the\nLLM to obtain the final answer. For our random sample of\n1,000 questions in these experiments, the average length of\nthe question with added context was 126.90 words, which is\napproximately 14.2 times longer than the original question.\nAdd extra information to the following question. Also\nspecify the current month and year is January 2018, so\nanswer questions accordingly. Your aim is to disambiguate\nwhat it is asking.\nQuestion: {xp}\nD. Evaluation Metrics\nWe measure the effect of our disambiguation methods on\nthe overall accuracy of the LLMs by using semantic similarity\nbetween the LLM responses and the ground truth responses.\nWe do this over measuring token overlap directly since several\ninstances have ground truth answers that may be rephrased\nin multiple ways, all of which are correct. This allows for\nmore meaningful evaluations by taking phrasing variations\ninto account. Specifically, we use OpenAI’s text-embedding-\n3-large6 vector embedding model to generate the vectors\nand then computed the cosine similarity metric between two\ngiven texts. The distances are normalized on a scale from 0\nto 1, where a value of 1 indicates that the compared texts\nare identical, while lower values suggest varying degrees of\ndivergence. In our experiments and results, we measure:\n1. Distance Between Ambiguous and Disambiguated\nQuestions: This metric quantifies the semantic shift intro-\nduced by each disambiguating prompt in the question. Since\nwe have an upper bound for the performance of an LLM\nwith disambiguation, this metric also allows us to observe\nhow effective were our prompts in changing the question in\ncomparison to the given disambiguated question.\n2. Distance Between Baseline Answer and Disam-\nbiguated Answer: This measures the difference between the\nresponses generated by the LLM when given the original\n6https://platform.openai.com/docs/guides/embeddings\n\nFig. 2. Kernel Density Estimate (KDE) Plot to compare the Cosine Similarity\nbetween Ground Truth Answer and LLM Response for the 2 disambiguation\nstrategies for the randomly sampled subset of 1,000 Ambiguous Questions.\nTABLE I\nPERFORMANCE OF GPT-4O ON AMBIGUOUS AND DISAMBIGUATED\nQUESTIONS.\nMetric\nNaive\nDisamb.\nvia ‘what’\nDisamb.\nvia context\nUpper-bound (via GT\ndisamb. questions)\nQuestion\ncoherence\n-\n0.905\n0.743\n0.317\nNaive Answer\nOverlap\n-\n0.826\n0.799\n0.895\nGT Answer\nOverlap ↑\n0.759\n0.778\n0.789\n0.858\nambiguous question and the disambiguated question. A larger\ndistance in this case implies that the model’s answer remains\nrelatively consistent across different contexts and phrasings,\nindicating robustness while answering ambiguity.\n3. Distance Between Baseline Answer and Ground\nTruth: This metric acts as a reference point to evaluate the\ninitial effectiveness of the out-of-the-box LLM answers before\nany disambiguation. This provides insights into how closely\nthe model approximates human-annotated answers without\nadditional prompts. Comparing this distance with distances in-\nvolving disambiguated strategies is how we measure improve-\nments in accuracy achieved through the proposed prompting\ntechniques.\n4. Distance Between Ground Truth and Disambiguated\nAnswer: This metric measures the effectiveness of each\ndisambiguation method in bridging the gap between model-\ngenerated answers and human-annotated ground truth answers.\nAn increase in this metric after disambiguation indicates that\nthe chosen prompts and contextual enrichment techniques have\nbeen able to pivot the LLM towards generating more accurate\nanswers to ambiguous questions.\nV. RESULTS AND DISCUSSION\nIn order to understand how leading LLMs tackle ambigu-\nous questions in a question answering task, we investigate and\naim to answer the following research question:\nFig. 3. Kernel Density Estimate (KDE) Plot to compare the Cosine Similarity\nbetween Ground Truth Answer and LLM Response for the 2 disambiguation\nstrategies for the subset of AmbigQA where human-provided answer for\nhuman-provided disambiguated question matched the ground truth.\nTABLE II\nPERFORMANCE OF GPT-4O-MINI ON AMBIGUOUS AND DISAMBIGUATED\nQUESTIONS.\nMetric\nNaive\nDisamb.\nvia ‘what’\nDisamb.\nvia context\nUpper-bound (via GT\ndisamb. questions)\nQuestion\ncoherence\n-\n0.907\n0.739\n0.317\nNaive Answer\nOverlap\n-\n0.771\n0.739\n0.745\nGT Answer\nOverlap ↑\n0.692\n0.707\n0.71\n0.783\n• RQ1: How well do off-the-shelf LLMs perform on am-\nbiguous QA in a zero shot setting and does training free-\ndisambiguation help?\n• RQ2: Does few-shot fine-tuning improve performance?\n• RQ3: Does reducing the temperature for LLM generation\nhelp in improving performance?\nRQ1: We show the results for RQ1 for GPT-4o and\nGPT-4o-mini in Tables I and II respectively. In both tables,\nQuestion coherence refers to the semantic similarity between\nthe ground truth disambiguated question and the ambiguous\nquestion when disambiguated via the LLM following one\nof the two methods; Naive Answer Overlap refers to the\nsemantic similarity between LLM responses obtained via the\ndisambiguating prompts vs. the naive prompt; and finally GT\nAnswer Overlap refers to the semantic similarity between\nthe LLM response and the ground truth answer in the dataset.\nIdeally, we want higher values for this metric. Interestingly,\nwe see that for both GPT 4o and 4o-mini, using simple\ndisambiguating prompts improves performance over the naive\nsetting, implying that simple prompt-based, training-free ap-\nproaches may be useful in improving LLM performance for\nambiguous queries. Out of the two simple disambiguating\nmethods explored, we see that disambiguation via adding\ncontext performs better for both LLMs.\nRQ2: To evaluate whether small scale fine-tuning helps\n\nFig. 4.\nComparison of GT Answer Overlap for GPT 4o and 4o-mini for\nboth high and low temperatures. High = 1.0, low = 0.2. Higher overlap scores\nare better.\nin improving LLM performance on ambiguous questions, we\nperform few-shot fine-tuning on GPT 4o-mini 7. To adapt\nour model for handling ambiguous questions, we fine-tuned\nthe model using OpenAI’s API. We randomly sampled 50\nquestion-answer pairs from AmbigQA. Each ambiguous ques-\ntion was stored as the prompt for the LLM, with ground truth\nbeing stored as the expected response from the LLM. The file\nwas formatted as shown below for the 50 questions. Using this,\nwe initiated a fine-tuning job on OpenAI’s fine-tuning API 8\nwhich returned a model checkpoint. We used this fine-tuned\nmodel ID instead of gpt-4o-mini within our baseline prompt\nconfiguration.\n{“messages”:\n{“role”: “user”, “content”: <ambiguous question>},\n{“role”: “assistant”, “content”: <ground truth answer>”}}\nTo evaluate the performance of this fine-tuned model, we\nsample 1,000 ambiguous questions from the dataset at random\nand compare the performance between naive prompting on the\n4o-mini model and naive prompting on the fine-tuned 4o-mini\nmodel. The GT Answer Overlap for the 4o-mini model is\n0.643 while that for the fine-tuned 4o-mini model is 0.626.\nTherefore, we see that fine-tuning, at least at this small scale,\ndoes not provide any improvement in LLM performance on\nambiguous questions. This reinforces our insight that simple\ntraining-free prompting methods for disambiguation work well\nin improving performance.\nRQ3: Using a lower temperature value for LLM generation\nresults in reducing the ‘stochasticity’ of the generated text,\nwhereby the variance is reduced and the generated text is\nmore predictable over multiple runs. As an ablation, we eval-\nuate whether lower values of temperature help in improving\nperformance of the LLM on ambiguous questions. We show\nthe results for this in Figure 4: we see that although lower\ntemperature (0.2 instead of 1.0, in this case) seem to have\n7owing to the lower cost of fine-tuning the mini version of GPT-4o.\n8https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-\nfine-tuned\nminor improvements in some cases, the difference is not\nthat significant. This implies simply using a lower value of\ntemperature may not provide any benefits in LLM performance\nfor answering ambiguous questions.\nProblem with naive contextual enrichment: The Figures\n2 and 3 show why the average is not going up when an\nLLM is prompted to insert context into a question. Although\nadding context should skew the plot 2 to the right (ie: be\nmore similar to the ground truth), but instead its skewed to\nthe left since its being held back every time it adds the wrong\ncontext which is not a problem when we have simply rephrased\nthe question. Then, we take a subset of AmbigQA where the\nhuman-provided answer of a human-provided disambiguated\nquestion exactly matches the ground truth answer. Here, we\nsee that plot 3 of contextual enrichment does skew to the right.\nThis shows that LLMs are able to better understand certain\nsocial cues to correctly disambiguate the provided question in\ncases where the human annotator was able to disambiguate\nthem as well.\nVI. CONCLUSION AND FUTURE WORKS\nOur results indicate that contextual enrichment has the\nability to significantly enhance model disambiguation accu-\nracy, but it is often inaccurate because it tends to add irrelevant\ncontext to questions, making them impossible to fix by prompt-\ning. However, when we took a subset of AmbigQA where the\nhuman-provided answer of a human-provided disambiguated\nquestion provided matches the ground truth, adding context to\nthose questions increases the accuracy of the model. There-\nfore, our analysis shows that even though LLMs struggle\nwith ambiguity in prompts, simple training-free prompt-based\ndisambiguation methods may help significantly in improving\nthe performance of the LLM.\nIn future work, we plan to fine-tune the LLM for accurate\ncontext-enhancement. We will take the contextually enriched\ninformation blob and fine-tune the model to generate a dis-\nambiguated question that is as close as possible to human-\nprovided disambiguation to maximize accuracy for question-\ndisambiguation based strategies. This will help in increasing\nthe accuracy overall since LLMs in the open-world do not have\naccess to disambiguated versions of a question, so through\nthis fine-tuning we expect the LLM to learn social cues while\ndisambiguating a question, which will improve the accuracy\nfor questions outside of AmbigQA dataset. We also plan to\nassess these prompt-based disambiguation techniques in open-\nsource models such as Llama-3.1-8B-Instruct and Mixtral-\n8x7B [23], as well as to test the performance of our fine-\ntuned model on general factual question-answering by using\nSimpleQA by OpenAI and other questions from the NQ-\nOpen dataset. This is a promising way forward to reduce\nhallucinations caused by ambiguity and better guard for an\nLLM’s prompt sensitivity when its tasked with answering\nambiguous questions.\n\nLIMITATIONS\nThis study adopted a general form of “ambiguity” in ana-\nlyzing the performance of Large Language Models. We recog-\nnize that a more thorough investigation of various types of am-\nbiguity—such as multiple answers, time-dependent interpreta-\ntions, multiple answer types, and other specific forms—could\nprovide deeper insights into how a model falters in different\nkinds of ambiguity.\nAdditionally, we suspect that our fine-tuning approach\nmay have underperformed due to catastrophic forgetting [24].\nAs suggested in Section VI, future work could adopt more\ntargeted and intentful fine-tuning strategies, such as devel-\nopment of a dedicated question disambiguation model or\nthe application of linguistic refinements that current LLMs\ncannot perform in a zero-shot setting. Future studies could also\nmitigate the effects of catastrophic forgetting with methods\nsuch as Singular Value Decomposition [25]. Implementing\nsuch strategies could enhance the stability and effectiveness\nof fine-tuned models in answering ambiguous questions.\nREFERENCES\n[1] J. Nam, “56% of college students have used AI on assignments\nor exams: Bestcolleges,” Nov 2023. [Online]. Available: https://www.\nbestcolleges.com/research/most-college-students-have-used-ai-survey/\n[2] S. Manjrekar. (2024) Agentic ai makes autonomous enterprises a reality.\n[Online]. Available: https://www.forbes.com/councils/forbestechcouncil/\n2024/10/30/agentic-ai-makes-autonomous-enterprises-a-reality/\n[3] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang,\nE.\nZhao,\nY.\nZhang,\nY.\nChen,\nL.\nWang,\nA.\nT.\nLuu,\nW.\nBi,\nF. Shi, and S. Shi, “Siren’s song in the ai ocean: A survey on\nhallucination in large language models,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2309.01219\n[4] A. Vaswani, “Attention is all you need,” Advances in Neural Information\nProcessing Systems, 2017.\n[5] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Ka-\nmar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg et al., “Sparks of artificial\ngeneral intelligence: Early experiments with gpt-4,” arXiv preprint\narXiv:2303.12712, 2023.\n[6] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[7] T. B. Brown, “Language models are few-shot learners,” arXiv preprint\narXiv:2005.14165, 2020.\n[8] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,\nD. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4\ntechnical report,” arXiv preprint arXiv:2303.08774, 2023.\n[9] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al.,\n“Llama: Open and efficient foundation language models,” arXiv preprint\narXiv:2302.13971, 2023.\n[10] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama\n2: Open foundation and fine-tuned chat models,” arXiv preprint\narXiv:2307.09288, 2023.\n[11] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu,\nT. Zhang, F. Wu et al., “Instruction tuning for large language models:\nA survey,” arXiv preprint arXiv:2308.10792, 2023.\n[12] A. Bhattacharjee and H. Liu, “Fighting fire with fire: can chatgpt detect\nai-generated text?” ACM SIGKDD Explorations Newsletter, vol. 25,\nno. 2, pp. 14–21, 2024.\n[13] H. Liu, Z. Zheng, Y. Qiao, H. Duan, Z. Fei, F. Zhou, W. Zhang, S. Zhang,\nD. Lin, and K. Chen, “Mathbench: Evaluating the theory and application\nproficiency of llms with a hierarchical mathematics benchmark,” arXiv\npreprint arXiv:2405.12209, 2024.\n[14] A. Salinas and F. Morstatter, “The butterfly effect of altering prompts:\nHow small changes and jailbreaks affect large language model perfor-\nmance,” arXiv preprint arXiv:2401.03729, 2024.\n[15] M. Sclar, Y. Choi, Y. Tsvetkov, and A. Suhr, “Quantifying language\nmodels’ sensitivity to spurious features in prompt design or: How i\nlearned to start worrying about prompt formatting,” in The Twelfth\nInternational Conference on Learning Representations, 2024. [Online].\nAvailable: https://openreview.net/forum?id=RIu5lyNXjT\n[16] A. Tamkin, K. Handa, A. Shrestha, and N. Goodman, “Task ambiguity in\nhumans and language models,” arXiv preprint arXiv:2212.10711, 2022.\n[17] E.\nDavis.\n(na)\nNotes\non\nambiguity.\n[Online].\nAvailable:\nhttps:\n//cs.nyu.edu/∼davise/ai/ambiguity.html\n[18] H. Levesque, E. Davis, and L. Morgenstern, “The winograd schema\nchallenge,” in Thirteenth international conference on the principles of\nknowledge representation and reasoning, 2012.\n[19] OpenAI, “Gpt-4o system card,” 2024. [Online]. Available: https:\n//openai.com/index/gpt-4o-system-card/\n[20] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\nC. Alberti, D. Epstein, I. Polosukhin, M. Kelcey, J. Devlin, K. Lee,\nK. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszkoreit, Q. Le,\nand S. Petrov, “Natural questions: a benchmark for question answering\nresearch,” Transactions of the Association of Computational Linguistics,\n2019.\n[21] S. Min, J. Michael, H. Hajishirzi, and L. Zettlemoyer, “AmbigQA:\nAnswering ambiguous open-domain questions,” in EMNLP, 2020.\n[22] G. Kamath, S. Schuster, S. Vajjala, and S. Reddy, “Scope ambiguities\nin large language models,” Transactions of the Association for Compu-\ntational Linguistics, vol. 12, pp. 738–754, 2024.\n[23] A. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. Chaplot, D. de las\nCasas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al., “Mistral\n7b (2023),” arXiv preprint arXiv:2310.06825, 2023.\n[24] Y.\nLuo,\nZ.\nYang,\nF.\nMeng,\nY.\nLi,\nJ.\nZhou,\nand\nY.\nZhang,\n“An empirical study of catastrophic forgetting in large language\nmodels\nduring\ncontinual\nfine-tuning,”\n2024.\n[Online].\nAvailable:\nhttps://arxiv.org/abs/2308.08747\n[25] J. K. Franke, M. Hefenbrock, and F. Hutter, “Preserving principal\nsubspaces to reduce catastrophic forgetting in fine-tuning,” in ICLR\n2024 Workshop on Mathematical and Empirical Understanding of\nFoundation Models, 2024. [Online]. Available: https://openreview.net/\nforum?id=XoWtroECJU",
    "pdf_filename": "Do_LLMs_Understand_Ambiguity_in_Text_A_Case_Study_in_Open-world_Question_Answering.pdf"
}