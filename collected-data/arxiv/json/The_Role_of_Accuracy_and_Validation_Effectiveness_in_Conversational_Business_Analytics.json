{
    "title": "The Role of Accuracy and Validation Effectiveness in Conversational Business Analytics",
    "abstract": "This study examines conversational business analytics, an approach that utilizes AI to address the technical competency gaps that hindered end users from effectively using traditional self-service analytics. By facilitating natu- ral language interactions, conversational business analytics aims to enable end users to independently retrieve data and generate insights. The analysis fo- cuses on Text-to-SQL as a representative technology for translating natural language requests into SQL statements. Using models grounded in expected utility theory, the study identifies conditions under which conversational busi- ness analytics, through partial or full support, can outperform delegation to human experts. The results indicate that partial support, which focuses solely on information generation by AI, is viable when the accuracy of AI-generated SQL queries exceeds a defined threshold. In contrast, full support includes not only information generation but also validation through explanations pro- vided by the AI, and requires sufficiently high validation effectiveness to be reliable. However, user-based validation presents challenges, such as misjudg- ment and rejection of valid SQL queries, which may limit the effectiveness of conversational business analytics. These challenges underscore the need for robust validation mechanisms, including improved user support, automated processes, and methods for assessing quality independently of end users’ tech- nical competencies. 1 Introduction Business analytics aims to generate actionable insights that support data-driven decision-making across diverse organizational contexts. Self-service analytics, a sig- nificant development in this domain, empowers end users to independently fulfill their information needs without relying on experts such as data engineers or data scientists. By providing tools for retrieving, preparing, analyzing, and visualizing data, self-service analytics enhances flexibility and agility in addressing dynamic business demands. Despite these advantages, self-service analytics has notable limitations. While end users are often domain experts, they frequently lack the technical skills required for advanced analytics tasks, such as navigating complex data structures, writing 1 arXiv:2411.12128v1  [cs.AI]  18 Nov 2024",
    "body": "The Role of Accuracy and Validation Effectiveness\nin Conversational Business Analytics\nAdem Alparslan\nDepartment of Business Analytics, FOM University of Applied Sciences\nadem.alparslan@fom.de\nAbstract\nThis study examines conversational business analytics, an approach that\nutilizes AI to address the technical competency gaps that hindered end users\nfrom effectively using traditional self-service analytics. By facilitating natu-\nral language interactions, conversational business analytics aims to enable end\nusers to independently retrieve data and generate insights. The analysis fo-\ncuses on Text-to-SQL as a representative technology for translating natural\nlanguage requests into SQL statements. Using models grounded in expected\nutility theory, the study identifies conditions under which conversational busi-\nness analytics, through partial or full support, can outperform delegation to\nhuman experts. The results indicate that partial support, which focuses solely\non information generation by AI, is viable when the accuracy of AI-generated\nSQL queries exceeds a defined threshold. In contrast, full support includes\nnot only information generation but also validation through explanations pro-\nvided by the AI, and requires sufficiently high validation effectiveness to be\nreliable. However, user-based validation presents challenges, such as misjudg-\nment and rejection of valid SQL queries, which may limit the effectiveness of\nconversational business analytics. These challenges underscore the need for\nrobust validation mechanisms, including improved user support, automated\nprocesses, and methods for assessing quality independently of end users’ tech-\nnical competencies.\n1\nIntroduction\nBusiness analytics aims to generate actionable insights that support data-driven\ndecision-making across diverse organizational contexts. Self-service analytics, a sig-\nnificant development in this domain, empowers end users to independently fulfill\ntheir information needs without relying on experts such as data engineers or data\nscientists. By providing tools for retrieving, preparing, analyzing, and visualizing\ndata, self-service analytics enhances flexibility and agility in addressing dynamic\nbusiness demands.\nDespite these advantages, self-service analytics has notable limitations. While\nend users are often domain experts, they frequently lack the technical skills required\nfor advanced analytics tasks, such as navigating complex data structures, writing\n1\narXiv:2411.12128v1  [cs.AI]  18 Nov 2024\n\nprogram code, or utilizing machine learning techniques. This skills gap can result in\nerrors and a continued dependency on technical experts, undermining the autonomy\nand effectiveness of self-service analytics.\nRecent advancements in generative AI, particularly the development of large lan-\nguage models, provide a transformative solution to these challenges. These models\nenable natural language interaction with analytics systems, removing the necessity\nfor technical expertise. Building on this foundation, conversational business ana-\nlytics is emerging as an innovative paradigm that redefines how users interact with\nbusiness analytics systems.\nBy leveraging large language models, conversational\nbusiness analytics allows users to delegate tasks such as data retrieval, analysis,\nand visualization to AI capable of understanding and generating natural language\noutputs. This approach bridges the skills gap and extends access to sophisticated\nanalytics tools across a broader spectrum of organizational roles.\nThere is a growing number of initiatives to extend self-service analytics by a\nAI-powered natural language interface. In response, software vendors are expanding\ntheir product portfolios to leverage individual data analysis. These solutions are\ndesigned to empower end users to interact with data seamlessly, generate reports,\nand perform a wide range of analytical tasks independently.\nThis study develops models, grounded in expected utility theory, to identify the\nconditions under which conversational business analytics using Text-to-SQL outper-\nforms delegation to human experts. Central to this analysis is the interplay between\naccuracy (the ability of AI to generate correct information) and validation effective-\nness (the performance in correctly distinguishing between true and false informa-\ntion). The models examine two levels of AI support: partial support, where the\nAI generates information without additional validation, and full support, which in-\ncludes a validation process to enhance trustworthiness. The conditions under which\nconversational business analytics surpasses delegation to human experts are identi-\nfied, with a particular focus on scenarios where validation should be performed to\nenhance trustworthiness and those where it should be omitted to avoid a degradation\nin decision quality. These insights provide a structured framework for determining\nwhen and how AI-driven insight generation, combined with validation, can increase\nutility while maintaining reliability, offering practical guidance for implementing\nconversational business analytics in diverse business contexts.\nThe structure of this study is as follows: Chapter 2 provides an overview of\ntraditional self-service analytics, highlighting its limitations. Chapter 3 introduces\nconversational business analytics, exploring its transformative potential with a focus\non large language models. This chapter also illustrates the challenges of information\ngeneration and subsequent validation using Text-to-SQL as an example. Chapter 4\npresents the models that examine the dynamics of partial and full support strategies,\nfocusing on their implications for both information generation and validation. Fi-\nnally, Chapter 5 concludes with a summary of the key findings, an acknowledgment\nof the study’s limitations, and a discussion of directions for future research.\n2\n\n2\nTraditional Self-Service Analytics\nBusiness analytics focuses on generating actionable insights to support data-driven\ndecision-making within business contexts [10, 17, 28, 78]. To achieve these goals,\nanalytics ecosystems consist of components [30, 83, 63, 54] that automate four core\ninterconnected tasks for generating insights: retrieving data, preparing data, gener-\nating information, and visualizing information (see Figure 1). In addition to these\ncore tasks, there are complementary tasks such as ensuring data and information\nquality and establishing governance mechanisms [84]. Although these complemen-\ntary tasks are important for achieving the goals associated with business analytics,\nthey are not discussed further in this study.\nFigure 1: Core insight generation tasks of business analytics.\nRetrieving data involves gathering raw data from various source systems, such\nas structured data from relational databases, semi-structured data like JSON or\nXML, and unstructured data such as text or images. The next step, preparing data,\nincludes cleansing, transformation, and enrichment processes to ensure quality and\nusability. Prepared data is stored in data warehouses and data marts for structured\nanalysis, while semi-structured or unstructured data is stored in data lakes [44] or\nvariants, prepared when needed.\nGenerating information applies the three main methods of business analytics [16]:\ndescriptive, predictive, and prescriptive analytics. Descriptive analytics summarizes\nhistorical and real-time data, providing insights into past and current performance\nusing key performance indicators (KPIs), trend analysis, and target-versus-actual\ncomparisons [17]. Predictive analytics uses statistical models and machine learn-\ning techniques to forecast future developments, such as predicting customer churn\nor market trends. Prescriptive analytics recommends actions based on forecasts,\nutilizing planning, simulation, and optimization methods to identify effective de-\ncisions. These methods are complementary, with prescriptive analytics relying on\nboth descriptive and predictive results.\nFinally, visualizing information transforms analytical results into intuitive for-\nmats that support decision-making [69]. Effective visualizations, such as dashboards,\nsimplify complex insights, enabling to explore trends, identify opportunities, and act\nquickly.\nDay-to-day information needs are typically met through predefined reports pro-\nvided by analytics ecosystems. However, when new information needs arise, self-\nservice analytics [29, 1, 46] enables end users to fulfill information needs indepen-\ndently, providing a viable alternative to the traditional reliance on expert (e.g. data\nengineers and data scientists) intervention. In this context, an end user refers to\nany organizational member who uses information to make decisions – whether to\ninform their own decisions or to prepare information for others, such as supervisors.\n3\n\nThese individuals are typically domain experts with extensive knowledge in their\nrespective fields, distributed across various departments within the organization.\nThe implementation of self-service analytics provides significant benefits [29, 1,\n59]. It enables flexible and timely information generation, supporting early iden-\ntification of opportunities and risks. This allows for faster decision making, both\nto capitalize on positive outcomes and to mitigate negative impacts. Self-service\nanalytics also falls under the umbrella of ”end-user computing” [57], where tasks\ntraditionally managed by specialized departments are now performed by members\nof the organization. By eliminating the need to delegate information retrieval tasks\nto experts, self-service analytics removes dependencies and delays, thereby stream-\nlining processes. In addition, it reduces the agency costs associated with delegation\n[26] by minimizing the exposure to hidden actions, which in turn reduces the need\nfor incentives or oversight.\nSelf-service analytics for descriptive purposes has traditionally relied on the data\nmart approach, enabling users to access predefined information and perform OLAP\noperations [13, 9] like drill-down and roll-up. However, much of the data—customer,\nproduct, or process-related—exists outside data marts, in warehouses, lakes, or\nsource systems. While real-time OLAP and in-memory computing have improved\nresponsiveness, generating actionable insights often still requires expert assistance,\nleading to delays. To address this, greater flexibility is needed [1], allowing to inde-\npendently retrieve and prepare data, perform analytics, and visualize results. This\nautonomy enhances decision-making and supports agile responses to changing busi-\nness needs.\nEffective utilization of self-service analytics necessitates technical expertise [66],\nwhich many end users lack. While end users typically possess domain-specific knowl-\nedge, they often lack proficiency in critical areas such as data retrieval, modeling,\nmachine learning, programming, and navigating complex data structures [60]. For\ninstance, Microsoft’s financial data warehouse, comprising 632 tables, over 4,000\ncolumns, and 200 views, illustrates the significant challenges posed by navigating\nlarge and intricate databases [21]. This knowledge gap not only increases the likeli-\nhood of errors but also limits to independently generate actionable insights.\nSeveral studies underscore the importance of technical skills for the effective\napplication of self-service analytics [29, 37, 5].\nImhoff and White highlight the\nnecessity of user-friendly tools, noting that ”sophisticated analytics are often too in-\ntricate, complex, or difficult to construct for many information workers.” Similarly,\nLennerholt, Laere, and Söderström emphasize the dual importance of intuitive tool\ndesign and comprehensive training programs to address the technical knowledge gap\namong non-technical users [37]. In an empirical study, Alparslan and Hügens ex-\namine the challenges small and medium-sized enterprises face in leveraging analytic\necosystems [3]. Their findings reveal that for approximately 70% of respondents,\ninadequate technical skills represent a primary obstacle to transforming raw data\ninto actionable insights.\n4\n\nFigure 2: CBA facilitating natural language requests and generating insights [2]\n3\nConversational Business Analytics\n3.1\nOverview\nBusiness analytics is undergoing significant development with the emergence of gen-\nerative AI, giving rise to a new approach termed “Conversational Business Analytics”\n(CBA). This new paradigm leverages natural language processing to address per-\nsistent challenges in traditional self-service analytics, particularly the technical skill\ngap among end users. CBA enables natural language interactions for tasks such as\ndata processing, analysis, and insight generation, presenting the potential to enhance\nthe accessibility and efficiency of business analytics.\nCBA shifts the focus from traditional graphical user interfaces to natural language-\ndriven interactions, supported by advancements in large language models (LLMs)\n[77, 18, 7]. These models process natural language by tokenizing text and employing\nself-attention mechanisms to interpret contextual relationships. This architecture\nallows for context-aware and relevant outputs, with responses generated token by\ntoken based on probabilistic modeling. Trained on extensive text corpora, LLMs\noptimize billions of parameters to achieve high performance in natural language\nunderstanding and generation. As depicted in Figure 2, CBA facilitates the trans-\nformation of natural language inputs into actionable outputs, such as structured\nreports or visualizations.\nCBA extends the capabilities of business analytics by automating core tasks for\ninsight generation, including data retrieval, preparation, analytics, and visualization.\n5\n\nTask\nReferences\nData Retrieval\n[75], [71], [33], [32], [24]\nData Preparation\n[90], [67], [34], [11], [56]\nInformation Extraction and Generation\n[52], [70], [39]\nInformation Visualization\n[86], [68], [45], [49], [15]\nTable 1: Selected advancements in CBA regarding the core tasks of business an-\nalytics. These advancements are categorized based on their focus on specific core\ntasks of business analytics. It is important to note that many of these advancements\naffect multiple tasks simultaneously, highlighting the interconnected nature of tasks\nsuch as data retrieval, preparation, analysis, and insight generation.\nIt supports structured data (e.g., from data warehouses), semi-structured data (e.g.,\nfrom data lakes), and unstructured data (e.g., textual documents). For structured\nand semi-structured data, semantic parsing techniques are used to convert natural\nlanguage queries into executable code. For unstructured data, LLMs extract insights\nthrough advanced text processing methods. The Table 1 below summarizes core\ntasks and their corresponding references for recent advancements.\nCBA also introduces interactive exchanges between users and AI, enabling refine-\nment of requests, validation of outputs, and clarification of insights. Such interaction\nunderscores its user-centric and adaptive design, as exemplified by tools like Ope-\nnAI’s ChatGPT Advanced Data Analysis, which guide users through tasks such as\nanalytical modeling and workflow optimization [70].\nBy integrating natural language processing, interactive communication, and sup-\nport for diverse tasks and data types, CBA introduces a novel form of delegation in\nbusiness analytics (see Figure 3). Through CBA, complex tasks such as data pro-\ncessing and information generation can be assigned to AI, enabling the autonomous\ngeneration of insights on behalf of the end user. This delegation has the potential\nto reduce dependence on human experts while addressing the limitations associated\nwith traditional self-service analytics.\nGiven the dynamic nature of CBA, this study focuses on Text-to-SQL as a key\nsemantic parsing technology [33, 6, 4, 19, 87]. Text-to-SQL, a component of natu-\nral language interfaces to databases [41], translates natural language prompts into\nStructured Query Language (SQL) queries, enabling the retrieval and transforma-\ntion of data from relational databases. This technology supports both data retrieval\nand the preparation of comprehensive workflows for generating actionable insights.\nThe origins of Text-to-SQL can be traced back to Codd’s vision in the 1970s,\nwhich proposed natural language interfaces for ”casual users” to interact with re-\nlational databases [12].\nEarly implementations, such as rule-based systems [82],\nwere limited in flexibility, but subsequent advancements, including Long Short-Term\nMemory (LSTM) networks [91], improved the handling of sequential inputs. More\nrecently, transformer-based architectures have replaced LSTM networks, demon-\nstrating superior performance for complex natural language processing tasks. Mod-\nern Text-to-SQL systems leverage large language models (LLMs) for both training\nand inference, significantly improving accuracy and capability.\nThe primary focus of this study is on the accuracy of Text-to-SQL, defined as\n6\n\nFigure 3: Delegation of core tasks to AI.\nits effectiveness in generating correct SQL queries. Ensuring accuracy is crucial,\nas errors in query generation can lead to incorrect insights and suboptimal deci-\nsions. Additionally, the study examines validation effectiveness, which reflects the\nability to correctly distinguish between accurate and inaccurate outputs. Validation\neffectiveness plays a key role in verifying the reliability of generated information,\nparticularly in high-stakes scenarios where flawed data can have significant conse-\nquences. The dual challenges of ensuring effectiveness in information generation and\nvalidation are explored in detail in the following sections.\n3.2\nAccuracy\nDelegating tasks to human experts is grounded in their ability to accurately inter-\npret and respond to information needs, as well as to build a shared understanding of\nthe underlying goals and requirements [22, 31]. A similar dynamic exists when in-\nteracting with AI: the AI must understand the user’s request to generate the desired\noutcome. However, while natural language is flexible and often ambiguous, SQL is\nhighly structured and formal. For instance, the key figure ”material availability”\nmay be interpreted differently by the logistics and maintenance departments within\nthe same organization due to the coexistence of multiple terminological systems\n[50, 85, 27]. Beyond terminological differences, ambiguities may also arise from the\nlinguistic complexity of the request itself, such as context dependencies or vague\nformulations. To translate such requests into correct SQL queries, the AI must map\nthe terms used in the request to the corresponding tables and columns within the\ndata model (schema mapping). This requires a deep understanding of the semantics\nof the data model, including the specific meaning of fields and the relationships be-\ntween tables. Thus, the AI must not only understand the request but also identify\nthe relevant tables and columns that match the user’s inquiry.\nThe AI’s ability to interpret a natural language request and translate it into\ncorrect SQL code is referred to as its accuracy. Several distinct types of accuracy\ncan be identified to assess different aspects of AI performance [33, 89, 74, 32].\nSyntactic accuracy examines whether the SQL statements generated by the AI are\nexecutable. Execution accuracy evaluates whether the SQL query generated by the\nAI produces the expected result, even if the query’s structure differs from a reference\nSQL statement (often referred to as the ”gold standard”). In contrast, exact match\naccuracy is more stringent, as it requires not only the correct result but also that\n7\n\nFigure 4: Integration of LLM and Retrieval-Augmented Generation for Text-to-\nSQL; based on [24, 20, 6, 72].\nthe generated SQL query exactly matches the gold standard.\nTwo main strategies have been proposed to enhance the accuracy of Text-to-SQL:\nfine-tuning and prompt design. Each of these methods addresses different aspects:\nfine-tuning focuses on adjusting the model parameters of the LLMs, while prompt\ndesign involves crafting the input to the LLM.\n1. Fine-tuning: Training an LLM from scratch is resource-intensive, requiring\nvast amounts of data and computational power [64, 62, 72, 24]. As an alterna-\ntive, organizations can leverage transfer learning, wherein a pre-trained LLM,\nwhich has already learned general language representations, is refined with\ndomain-specific data. Fine-tuning involves adjusting the parameters of a pre-\ntrained LLM to better align with specific organizational needs and domains. In\nthe context of Text-to-SQL, fine-tuning involves training the model on labeled\ndatasets consisting of pairs of natural language requests and corresponding\nSQL queries. This process helps the model understand the organization’s spe-\ncific data models, schema, and terminology, enabling it to generate accurate\nSQL queries based on user input.\nOne major downside of fine-tuning is the potential for overfitting. This oc-\ncurs when the model becomes overly specialized in domain-specific data and\nis less capable of adapting to other queries, making it harder to generalize\nto different contexts.\nMoreover, ongoing maintenance poses challenges: as\ndata repositories such as data warehouses or marts evolve, schemas frequently\nchange. Therefore, the fine-tuned LLM may require periodic updates to re-\nmain effective, increasing operational complexity and costs. Fine-tuning is not\na one-time task; it requires continuous updates to avoid becoming outdated as\nthe organizational data landscape evolves.\n2. Prompt Design: Unlike fine-tuning, prompt design [24, 43, 62] does not alter\n8\n\nthe underlying parameters of the LLM. Instead, it leverages the model’s gener-\nalization capabilities by crafting prompts that guide the LLM toward accurate\nSQL query generation. In few-shot learning [7, 43], for example, a user’s re-\nquest is enriched with additional contextual information, such as data model\ndetails or examples of analogous SQL queries. The LLM processes the enriched\nprompt to infer the appropriate SQL query. Here, the LLM does not acquire\nnew knowledge; rather, it uses its existing understanding along with the pro-\nvided context to generate SQL queries that it has not previously encountered.\nRetrieval-Augmented Generation (RAG) further enhances prompt design by\nretrieving relevant contextual information, such as metadata stored in a vec-\ntor database. Upon receiving user input, RAG integrates this context into the\nprompt, enabling the LLM to generate SQL queries. These queries undergo\nautomated checks for syntax, schema alignment, and compliance with user re-\nquirements. If discrepancies are detected, the LLM’s self-correction mechanism\nrefines the query. Once validated, the query is executed, and the information\nis presented to the user. Figure 4 illustrates the integration of LLMs with\nRAG, highlighting seamless context retrieval and query refinement.\nPrompt design has the advantage of eliminating the need for continuous fine-\ntuning, saving both time and computational resources. However, as more con-\ntextual information is added, the number of tokens increases, pushing against\nthe limits of the LLM’s context window—the maximum number of tokens it\ncan process in a single instance. Although modern LLMs have extended con-\ntext windows, this limitation can still lead to information loss in very complex\nor lengthy prompts, potentially impacting the overall accuracy of the gener-\nated query.\nThe integration of LLMs has significantly advanced the performance of Text-to-\nSQL. On the SPIDER benchmark [89], which assesses the generalization capabilities\nof Text-to-SQL models across a wide variety of database schemas, the performance of\nleading models has markedly improved, with execution accuracy increasing from ap-\nproximately 54% to 91%. Exact match accuracy, which was around 5% initially, has\nrisen to 82%. By comparison, the BIRD benchmark [40], which presents even greater\ncomplexity, reveals that current LLM-based Text-to-SQL models achieve execution\naccuracy of around 73%, underscoring the challenges posed by this benchmark.\n3.3\nValidation Effectiveness\nWhen delegating information production to human experts, there is a challenge of\n”hidden action,” where the actions of experts are not fully observable or assessable,\npotentially leading to misalignment of interests [26]. Similarly, when delegated to\nAI, the issue shifts also to the transparency and reliability of outputs. While human\nexperts may act in self-interest, leading to agency costs through false information or\nrequiring additional incentives, AI systems present the challenge of output opacity.\nEnd users cannot evaluate the correctness of SQL queries and resulting KPIs until\nafter implementation, risking decisions based on false information with economic\nconsequences. Ensuring the reliability of AI-generated information is crucial, par-\nticularly in high-stakes decision-making scenarios where false information can result\n9\n\nin significant negative business outcomes. The requirement for reliability is further\nintensified by the inherent tendency of LLMs to produce hallucinations [42, 61],\nresulting in fabricated or erroneous SQL queries that may appear credible. Conse-\nquently, robust techniques are essential to ensure the validation of these generated\nSQL queries.\nThe techniques for validating automatically generated SQL queries fall within\nthe broader field of explainable AI [14, 65, 48, 36, 23]. Since the advent of expert\nsystems, researchers have emphasized the pivotal role of explanations in enhancing\nthe interpretability of AI-generated results [80, 53]. In recent years, the growing de-\nmand for transparency in machine learning has prompted significant advances in the\nfield of explainable AI, leading to an expansion in both the scope and sophistication\nof explanation techniques. The primary objective of explainable AI is to provide\nexplanations that elucidate the underlying processes behind AI outputs, thereby fa-\ncilitating tasks such as debugging, regulatory compliance, the establishment of user\ntrust, and the effective utilization of AI.\nIn the specific context of Text-to-SQL, validation necessitates the provision of\nexplanations that help to comprehend and verify AI-generated SQL queries. These\nexplanations must detail the structure of the queries, including the involved tables,\nfields, and operations (e.g., filtering, grouping, or joining). Such detailed informa-\ntion enables users to critically evaluate the logical coherence of the query and its\nalignment with their intended objectives, thereby supporting an independent val-\nidation process.\nEffective validation ensures that users can reliably differentiate\nbetween correct and incorrect SQL queries. However, ineffective validation—where\nerroneous queries are mistakenly validated or accurate ones are erroneously dis-\nmissed—compromises the reliability of the process and may result in suboptimal\ndecision-making.\nValidation in Text-to-SQL extend beyond mere explanation, often encompassing\nalso techniques for error correction. These techniques reflect the interactive nature\nof CBA and aim to support users in both identifying and rectifying errors in SQL\nqueries. In this study, the interpretability of AI outputs and their correction are\ntreated separately, with a focus on validation effectiveness as the primary metric.\nValidation serves as the foundation for reliable decision-making, as correct validation\nis essential for subsequent error correction.\nCurrent literature identifies three different techniques for validating and explain-\ning SQL queries (see [58] for an overview). Decomposition techniques (see Figure 5)\nbreak SQL queries into components such as tables, fields, and operations, presenting\nintermediate results to trace each element’s contribution to the final output, aiding\nerror detection [55]. Visualization techniques use graphical representations to map\nrelationships among query components, enhancing clarity and navigation in complex\nquery structures [38, 47, 51]. Dialogue techniques employ LLMs to provide natural\nlanguage explanations, enabling users to interactively explore and refine queries by\naddressing specific components or the underlying data model [73, 19, 35, 25, 81, 88].\nEmpirical studies on validation techniques for Text-to-SQL reveal varying lev-\nels of effectiveness in supporting SQL comprehension and error correction. Ning et\nal., through a user study, observed that decomposition, visualization, and dialogue\ntechniques achieved an effectiveness rate of approximately 56% [58]. In contrast,\n10\n\nFigure 5: Decomposition technique for explaining SQL queries [55].\nTian et al. demonstrated that their technique, which combines decomposition and\ndialogue techniques, significantly outperformed existing techniques, achieving a val-\nidation and error correction effectiveness of approximately 85% [73]. These findings\nunderscore substantial progress in user-based validation and error correction, show-\ncasing their transformative potential to enhance the reliability of AI-generated SQL\nqueries. However, the results also highlight persistent challenges, particularly in\nreal-world scenarios involving large-scale, highly complex SQL queries and exten-\nsive data models, where existing techniques may prove inadequate.\nA critical factor influencing the success of validation is the interplay between the\ncompetence of the end user and the quality of the explanations provided. While Tian\net al. demonstrated that their technique consistently achieved high effectiveness\nregardless of users’ technical expertise [73], the intricacies of SQL queries in complex\nbusiness environments such as data warehouses with hundreds of interconnected\ntables and fields necessitate a more nuanced evaluation.\nThe end user’s background and domain experience significantly affect their ability\nto assess the correctness of SQL queries, and this capability is further shaped by the\nclarity, detail, and comprehensiveness of the explanations generated. Additionally,\nthe complexity of the underlying data model [8] and the resulting SQL query plays\na pivotal role in the validation outcome.\nComplex data models with numerous\ninterdependencies and abstract schema structures can obscure the logic of even well-\nconstructed SQL queries, making validation tasks challenging for end users.\n4\nModels of CBA\n4.1\nBasic Assumptions\nThis study introduces models based on rational choice theory, with an emphasis\non expected utility theory [79, 76], to evaluate the effective use of CBA. The anal-\nysis focuses on the example of Text-to-SQL. Two primary influencing factors are\n11\n\nconsidered:\n• accuracy, defined as the AI’s ability to generate correct SQL queries.\n• validation effectiveness, defined as the ability to distinguish between correct\nand incorrect SQL queries.\nIt is assumed that the end user knows the levels of these factors determining\nthe performance of the delegation of insight-generating tasks to AI at hand and can\njudge the suitability accordingly. Delegation to a data engineer is considered as an\nalternative to using AI for insight generation.\nIn an ideal scenario, both accuracy and validation effectiveness would be perfect,\nensuring the generation of reliable and actionable information. However, these fac-\ntors are inherently probabilistic and subject to imperfections. The models examine\nhow the interaction between accuracy and validation effectiveness influences the ef-\nfectiveness of Text-to-SQL and its potential advantages over delegation to a human\nexpert. Specifically, the models evaluate how varying levels of these factors affect\nthe relative advantage of delegating tasks to AI, with or without validation sup-\nport. By exploring this interplay, the models aim to identify the conditions under\nwhich Text-to-SQL can serve as a viable alternative to human expertise, provid-\ning a structured framework for its adoption in CBA. This structured approach not\nonly quantifies the trade-offs between accuracy and validation effectiveness but also\nhighlights how CBA bridges the competency gaps that hinder traditional self-service\nanalytics. By doing so, it underscores AI’s role in empowering end users to make\ninformed decisions without relying on advanced technical skills, thereby expanding\nthe accessibility and effectiveness of analytics.\nThe models assume a risk-neutral perspective, where the end user has a linear\nutility function. This simplification allows for a focus on average outcomes (expected\nvalues) while ignoring the variability in potential gains and losses. By assuming\nlinearity, the models quantify all values in monetary units.\nThe analysis begins with the end user’s need for information that cannot be\nprovided by standard reporting. The current value of a KPI is critical for making\ninformed business decisions. While users may have access to data marts, the neces-\nsary data resides in data warehouses, requiring an SQL query for extraction. This\nquery retrieves and prepares the data, ultimately generating the required KPI. A\ncorrectly formulated SQL query produces a valid KPI, enabling optimal decisions\nand yielding a net profit of +1. Conversely, an erroneous query generates a mislead-\ning KPI, resulting in suboptimal decision and a financial loss of −1. The analysis\nassumes that the data warehouse contains high-quality, undistorted data, making\nthe correctness of the SQL query the sole determinant of information quality. De-\nspite possessing expertise in business processes, the end user lacks technical skills\nrequired to apply traditional self-service analytics effectively.\nCurrently, no data engineer is available to create the KPI, and such a resource\nwill only be accessible at a later time. The delayed delivery of the KPI results in\na reduced (net) profit v (where 0 < v < 1). This profit is lower than the achiev-\nable maximum of +1, reflecting the adverse effects of delayed action. In extreme\ncases, v may even approach zero, rendering the KPI almost valueless. Additional\n12\n\nfactors, such as the effort required to align requirements and potential motivational\nchallenges that could negatively impact collaboration between end user and data\nengineer, are not considered in this analysis. Consequently, v serves as a measure of\nthe urgency of KPI procurement, with lower values of v indicating greater urgency.\nThe process of KPI generation can be delegated to AI equipped with a natural\nlanguage interface powered by LLMs. AI offers two levels of assistance: partial sup-\nport (PS) and full support (FS). Under PS, the AI translates the natural language\nrequest into SQL code, which is executed on the data warehouse to calculate the\nKPI. The KPI is then used to inform decision-making, resulting in a business im-\npact. FS includes an additional validation step to ensure the correctness of the SQL\nquery. During this step, the end user is provided with explanations of the generated\nSQL query, enabling independent validation of its correctness. If the SQL query\nis deemed correct, the process concludes with confirmation, allowing to act on the\nKPI. If the query is identified as erroneous, it is rejected, and no action is taken,\navoiding both gains and losses. In this process, AI acts as a collaborative partner,\nassisting in overcoming technical challenges while ensuring that the business decision\nis supported by a correct and validated KPI.\nIn contrast to the dynamic, iterative nature of CBA, this study adopts a static\nperspective in which KPI generation and validation occur in a single execution cycle.\nThis idealized approach isolates the roles of information generation and validation,\nenabling a focused analysis of their individual impacts on the effectiveness of Text-\nto-SQL. The static perspective is also chosen, as iterative processes for information\ngeneration and validation within CBA are still under development.\nIt is further assumed that the end user incurs no direct costs (e.g., for data\nstorage, processing, or the use of the LLM) when utilizing Text-to-SQL. This aligns\nwith the prevailing practice in traditional self-service analytics, where no fees are\ncharged for individual instances of information generation.\nSince traditional self-service analytics is not feasible due to a lack of expertise, the\nend user evaluates whether the KPI should be generated via delegation to AI (either\nPS or FS), or whether it is more advantageous to wait for the data engineer to receive\nthe KPI at a later time. The end user selects the option with the highest benefit.\nWhen CBA (either PS or FS) offers a higher expected value than the profit through\nthe data engineer, it is preferred. Conversely, if the expected value from CBA is\nlower, human delegation is preferred. In such cases, CBA is deemed ineffective,\nas it fails to provide timely and decision-relevant information. By addressing the\nlack of technical skills required for traditional self-service analytics, CBA closes the\ncompetency gap that often hinders non-technical users from independently accessing\nand analyzing data.\nThe following table provides the mathematical symbols used in the following and\ntheir explanation.\n4.2\nPartial Support (PS)\nThe first stage involves the end user evaluating the feasibility of PS. PS addresses\nthe competency gaps that prevent end users from utilizing traditional self-service\nanalytics by automating the translation of natural language inputs into SQL queries.\n13\n\nSymbol\nExplanation\nα\naccuracy\nβ\nvalidation effectiveness\nv\nprofit obtainable through data engineer\nEPS(α)\nexpected value in PS\nEFS(α, β)\nexpected value in FS\nα∗\nPS\naccuracy threshold for PS to outperform data engineer\nα∗\nFS\naccuracy threshold for FS to outperform data engineer\nβ∗\nthreshold for validation effectiveness to outperform data engineer\nβ∗∗\nthreshold for validation effectiveness to outperform PS\nTable 2: Overview of the mathematical symbols used and their meaning.\nThis capability eliminates the need for technical proficiency in SQL syntax, allowing\nusers to focus on decision-making rather than query formulation.\nThe success of this approach depends primarily on the accuracy of the AI in\ntranslating natural language requests into correct SQL queries. As outlined ear-\nlier, current advancements such as fine-tuning combined with prompt design are\nemployed to enhance the AI’s information generation capabilities. Given the proba-\nbilistic nature of LLMs, the generation of SQL queries is treated as a discrete random\nvariable. The probability of generating a correct SQL query, denoted as α (where\n0 < α < 1), corresponds to execution accuracy. This probability is determined\nby the ratio of successfully generated SQL queries to the total number of natural\nlanguage requests of a similar type.\nThe expected value EPS(α) under PS is calculated as the weighted sum of the\npossible outcomes. A correct SQL query yields a profit of +1, while an incorrect\nSQL query results in a loss of −1. These outcomes are weighted by the probabilities\nof success (α) and failure (1 −α), respectively. The expected value is therefore\nexpressed as:\nEPS(α) = α · (+1) + (1 −α) · (−1) = 2α −1.\nDelegation to AI through PS is preferred over waiting for a data engineer when\nthe expected value EPS(α) exceeds the delayed profit (v). This condition can be\nformalized as the ”AI delegation condition of PS”:\nEPS(α) > v ⇔α∗\nPS > (1 + v)/2.\n(1)\nThe threshold (1) represents the accuracy that muss be exceeded for PS to be\nmore advantageous than relying on a data engineer. When this condition is met (α >\nα∗\nPS), PS is viable and enables effective KPI generation without relying on human\nexpertise. This is particularly beneficial since traditional self-service analytics is\nconstrained by the end user’s limited technical skills. Conversely, if this condition is\nnot satisfied (α ≤α∗\nPS), the expected value from PS falls below the profit achievable\nthrough delegation to a data engineer, rendering PS unsuitable for time-sensitive,\ndecision-critical business analytics.\n14\n\n4.3\nFull Support (FS)\nThe second stage involves assessing the viability of FS, which includes an additional\nvalidation process to enhance the reliability of AI-generated SQL queries.\nThis\nevaluation determines whether validation meaningfully improves the quality of the\ngenerated queries, ensuring their correctness and relevance for decision-making. By\nincorporating validation, FS demonstrates AI’s role as an active partner in ensuring\nreliability and correctness. The validation process not only identifies and mitigates\nerrors but also enhances the end user’s trust in the system by providing transparent\nexplanations.\nThis collaboration between AI and the end user enables decision-\nmaking that aligns more closely with organizational goals, even in scenarios with\nimperfect accuracy.\nVarious techniques such as decomposition, visualization, and dialog-based are\nused to enhance the interpretability and traceability of automatically generated\nSQL queries. The effectiveness of these validation techniques depends on several\nfactors, including the clarity, comprehensiveness, and contextual relevance of the\nexplanations provided, as well as the user’s technical expertise in evaluating them.\nHigh-quality explanations can significantly aid users in identifying errors and ver-\nifying the correctness of SQL queries. However, the end user’s ability to critically\nevaluate these explanations plays an equally crucial role in determining the success\nof the validation process.\nValidation is inherently uncertain due to variability in user comprehension and\nthe probabilistic nature of AI-generated explanations. These explanations are not\nguaranteed to be correct, as LLMs generate outputs based on probabilities rather\nthan deterministic logic.\nPoor validation can amplify risks by failing to detect\nerroneous SQL queries or by incorrectly rejecting valid queries. This dual risk un-\ndermines decision-making and may reduce trust in the system. While user-based\nvalidation is intended to mitigate inaccuracies, its effectiveness heavily depends on\nthe user’s ability to interpret complex SQL logic and explanations. This reliance\nintroduces variability, making validation inconsistent across users with differing ex-\npertise levels.\nTo quantify this uncertainty, validation effectiveness is modeled as a random\nvariable, denoted as β (where 0 < β < 1). This probability reflects the likelihood\nthat a user can correctly assess the SQL query based on the explanations provided.\nIt is defined as the ratio of successful validations to the total number of validation\nattempts. By adopting this probabilistic framework, the model accounts for the\ninherent uncertainties associated with validation processes.\nThe expected value EFS(α, β) for FS is calculated as:\nEFS(α, β) = αβ · (+1) + (1 −α)(1 −β) · (−1) = α + β −1.\nThe expected value EFS(α, β) is influenced by two key factors: accuracy and\nvalidation effectiveness. Higher values of α indicate improved accuracy in generating\ncorrect SQL queries, while higher values of β represent enhanced effectiveness in\nvalidating the correctness of those queries. The combined improvement in these\nfactors increases the expected value of FS, making it more likely to yield reliable and\nactionable KPI. This relationship highlights the complementary roles of accuracy\n15\n\nand validation effectiveness in ensuring the success of FS in CBA.\n4.4\nComparison of Full Support with Partial Support and\nDelegation to the Data Engineer\nThe end user will choose FS only if it provides a higher expected value than the\nprofit achievable through delegation\nEFS(α, β) > v\nand offers a better outcome than PS\nEFS(α, β) > EPS(α).\nFor FS to outperform delegation to a data engineer, the following condition, referred\nto as the ”AI delegation condition for FS” must be satisfied:\nEFS(α, β) > v ⇔β∗> (1 −α) + v.\n(2)\nThis condition (2) requires the validation to be sufficiently effective and robust (e.g.,\nagainst the shortcomings of the end user conducting the validation based on the\nprovided explanation) to compensate for inaccuracies in AI-generated information\n((1 −α)) and to deliver an overall expected value exceeding the guaranteed, but\ndelayed profit v. If the AI delegation condition for FS (2) is satisfied, it is prioritized\nover delegation to the data engineer. However, this preference also depends on the\naccuracy of AI-generated information being sufficiently high compared to the profit\nv. Specifically, the ”FS feasibility condition” must hold:\nα∗\nFS > v.\n(3)\nThis condition (3) ensures that the expected gain (α · (+1)) without validation\nexceeds the guaranteed profit v. If this condition is not satisfied, it is not justified\nto use FS, even with a perfect validation effectiveness, as the AI’s base accuracy\nwould be insufficient to outperform human delegation. The accuracy threshold (3)\nfor FS is lower than the threshold (1) for\nα∗\nFS < α∗\nPS.\nThis indicates that the inclusion of a validation step in FS broadens the potential\napplicability of CBA. However, whether FS is advantageous compared to delegation\nto a data engineer depends on the level of validation effectiveness as defined in the\nAI delegation condition for FS (2).\nThe end user, acting as a rational decision-maker, will only increase the AI\nsupport level from PS to FS if the probability of successful validation exceeds the\nprobability of successful SQL generation.\nThis requirement is formalized as the\n”validation dominance condition”:\nEFS(α, β) > EPS(β) ⇔β∗∗> α.\n(4)\n16\n\nIn condition (4), β∗∗represents the threshold where validation effectiveness ex-\nceeds accuracy, making FS more advantageous than PS. It is important to note that\nall conditions described so far strongly depend on the values of the potential gain\n(+1) and loss (−1) and the assumption that their magnitudes are equal. If the\nvalue of the loss outweighs that of the gain, the threshold for β in the validation\ndominance condition shifts, allowing a less stringent validation standard for FS to\noutperform PS.\nFigure 6: Relationship between accuracy (α) and validation effectiveness (β): con-\nditions for effective AI Delegation.\nFigure 6 illustrates the relationship between accuracy, validation effectiveness\nand the profit achievable through delegation to the data engineer (all values are\nmeasured in monetary units). The dotted area highlights the range of α and β values\nwhere FS outperforms PS and the data engineer, as all three conditions regarding\nAI delegation (2), the feasibility of FS (3), and the dominance of validation (4) are\nsatisfied. Outside this area, FS does not yield sufficient expected value to justify\n17\n\nits use. The figure also identifies the threshold α∗\nPS which is determined by the AI\ndelegation condition for PS (1).\nThe analysis reveals three distinct regions based on the interplay between accu-\nracy, validation effectiveness, and the profit achievable through delegation.\nIn region A, the FS feasibility condition (3) is not met, as the accuracy of in-\nformation generation is less than or equal to the profit achievable through the data\nengineer (α ≤v). In this situation, neither PS nor FS offers sufficient expected\nvalue. PS fails due to low accuracy, and FS is unable to compensate for this limi-\ntation, even when validation effectiveness would be perfect. Delegation to the data\nengineer remains the only viable option for retrieving the KPI. In this scenario, CBA\nis not utilized because it fails to match the effectiveness of the data engineer, ren-\ndering CBA unsuccessful. Furthermore, if the data engineer provides the KPI with\nsignificant delay, rendering it irrelevant for the end user due to its diminished value\nfor timely decision-making (v approaching zero), this not only signifies the failure\nof CBA but also highlights a broader limitation of business analytics in fulfilling its\nfundamental objective: delivering timely and decision-relevant information.\nMoreover, the requirements for accuracy in PS and FS, as well as for validation\neffectiveness in FS, increase as v rises. A reduction in urgency causes a rightward\nshift of v, α∗\nPS, and the delegation threshold condition for FS (2). In other words,\nthe less urgent the KPI (making it less problematic to wait for the data engineer)\nthe greater the performance demands on CBA to generate the correct SQL query\nand support the reliable assessment of its correctness.\nIn Region B, the FS feasibility condition (3) is met (α > v), but the AI delegation\ncondition for PS (1) is not satisfied (α ≤α∗\nPS). In this case, PS is less effective than\ndelegating to the data engineer. However, FS becomes a viable option if validation\neffectiveness meets the AI delegation condition for FS (2). Under this condition,\nvalidation introduces a ”boosting effect” by identifying and mitigating errors in SQL\nqueries, ensuring that flawed KPIs do not influence the decision-making process.\nThis boosting effect enables FS to enhance the overall reliability of information,\ncompensating for the limitations of lower accuracy. For example, even if accuracy\nfalls below the threshold required for PS (1) (e.g., α1), sufficiently high validation\neffectiveness in FS (e.g., β1) can still yield an expected value greater than v. While\nthe generation of the SQL query may fail, resulting in incorrect KPI, sufficiently\neffective validation can still identify these errors, ensuring that incorrect KPIs do\nnot adversely affect the decision process.\nIn region C, where the AI delegation condition for PS (1) is met (α > α∗\nPS), PS\nbecomes a viable alternative to delegation to the data engineer, as it provides suffi-\ncient utility on its own. If the AI delegation condition for FS (2) and the validation\ndominance condition (4) are not fulfilled (e.g., at α2 and β2), FS has a ”devastat-\ning impact” as the validation process undermines the reliability of the SQL queries\ngenerated. Not only are erroneous SQL queries less effectively identified, but in-\nsufficient validation also leads to the unwarranted rejection of correct SQL queries.\nConsequently, FS is not only less effective than PS but also inferior to delegation\nto the data engineer. Therefore, using AI-based assistance for KPI generation with\nAI-supported validation is not recommended. This underscores the inherent risks of\nrelying on user-based validation without robust mechanisms to support in correctly\n18\n\nevaluating AI-generated SQL queries. In cases where expertise is low or explana-\ntions are unclear, FS may inadvertently introduce errors or misjudgments. This\nhighlights the critical need for improved validation tools that reduce user depen-\ndency or provide guided assistance during the evaluation process.\nFS becomes favorable when, in region C, both conditions, (2) and (4), are fulfilled\n(e.g., at β2 and α3). In such cases, the validation process enhances the reliability\nof the generated KPI and maximizes the benefits of FS. As accuracy increases, the\nrequirements for validation effectiveness must also rise to ensure that FS remains\nbeneficial. A particular challenge arises when accuracy is very high; in such scenar-\nios, user-based validation must become even more effective to provide added value\nover PS. This places significant demands on validation mechanisms, especially in\nterms of users’ ability to evaluate and assess complex SQL queries based on the\ngiven explanations.\nThese findings highlight the critical interplay between accuracy and validation ef-\nfectiveness in determining the optimal use of PS or FS in CBA. High accuracy favors\nPS, while lower accuracy requires robust validation effectiveness for FS to deliver\nreliable results. Insufficient validation, however, compromises performance, making\ndelegation to a data engineer the preferable alternative. This analysis emphasizes\nthe importance of aligning validation performance with accuracy to enhance the ef-\nfectiveness of CBA. Although FS can improve decision-making, its success depends\non validation mechanisms that appropriately scale with accuracy. When this balance\ncannot be achieved, PS or data engineer may offer more dependable outcomes.\n5\nConclusion\nThis study analyzes CBA, an emerging approach with considerable potential to ad-\ndress the competency gaps inherent in traditional self-service analytics. Through\nthe use of LLMs, CBA facilitates natural language interactions, enabling end users\nto independently perform tasks such as data retrieval, preparation, and insight gen-\neration. In this context, CBA represents a new form of task delegation, shifting\nthe responsibility for generating and interpreting information from human experts\nto AI, thus making advanced analytics more accessible to users without technical\nexpertise. A focus of this study is Text-to-SQL, a well-established semantic parsing\ntechnology that has seen transformative advancements through the application of\nLLMs, enhancing its ability to translate natural language requests into structured\nSQL statements.\nThe models developed in this study examine the conditions under delegation\nto AI, through PS and FS, outperforms the delegation of tasks to human experts.\nPS is effective when the accuracy (α) exceeds a specified threshold, as outlined in\ncondition (1). FS, however, is only beneficial when three conditions are met: the AI\ndelegation condition ((2)), FS feasibility condition ((3)), and validation dominance\ncondition ((4)).\nThe study highlights the critical role of validation effectiveness (β) in FS. While\nFS can significantly improve decision-making when validation is robust, the use of\nuser-based validation introduces risks, particularly when users lack the necessary\nexpertise or when explanations are unclear. These risks can undermine decision\n19\n\nquality, making FS less effective than PS and, in worst case, even less effective than\ndelegation to a data engineer. This underscores the necessity of robust validation\nmechanisms to assist users in accurately evaluating AI-generated information. With-\nout such mechanisms, the advantages of CBA may be significantly compromised.\nThis study also opens several avenues for future research. The uncertainties as-\nsociated with user-based validation warrant further investigation. Future research\ncould explore techniques to support end users in the validation process, including\ntraining programs to build technical expertise and AI-driven tools that provide guid-\nance during validation. Moreover, there is an urgent need for empirical studies that\nmore clearly define the boundaries of user-based validation based on explanations,\nparticularly in real-world scenarios involving complex SQL queries and data models.\nWhile the primary goal of CBA is to overcome the technical barriers that hinder\nthe use of traditional self-service analytics, user-based validation poses a paradox.\nThe process relies on the same competencies that were insufficient for conducting\nindependent analyses in the first place. This raises a critical question: is it advisable\nto involve end users with limited technical expertise in the validation of results, given\nthat they may face similar challenges in assessing the quality of the analysis as they\ndid in performing it?\nThis question highlights the importance of more clearly delineating the bound-\naries of user-based validation. While CBA relies on end-user interaction, additional\nsupport mechanisms may be necessary to help users validate the accuracy of results\nwithout risking misinterpretation or circular reasoning. One promising avenue for\nfuture research is the development of hybrid approaches that combine expertise with\nautomated validation mechanisms.\nFuture studies could explore the implementation of automated techniques to in-\ndependently assess the reliability of generated KPIs. Potential approaches might\ninclude statistical methods such as confidence intervals to evaluate KPI correct-\nness, automated tools for schema alignment and comparison, and cross-validation\nwith pre-validated KPIs. Integrating such methods could reduce reliance on user\nexpertise, thereby enhancing the robustness and scalability of CBA systems.\nThe models presented in this study rely on several simplifying assumptions.\nFirst, the gain from correct decisions is fixed at +1 and the loss from incorrect\ndecisions at −1, which may not fully capture the variability observed in real-world\nscenarios. Additionally, the analysis adopts a static perspective, assuming a single\nexecution cycle for KPI generation and validation, thus overlooking the iterative\nnature of real-world analytics. The models also omit costs associated with LLM\nusage, data storage, and processing, which may lead to an overestimation of the\neconomic feasibility of CBA. Expanding these models to incorporate dynamic, cost-\nsensitive, and multi-stage decision processes could provide deeper insights.\nIn conclusion, this study illustrates the transformative potential of CBA in ad-\ndressing the limitations of traditional self-service analytics. However, realizing its\nfull potential depends on addressing the inherent challenges of validation, advancing\nuser support mechanisms, and exploring automated alternatives. Through contin-\nued innovation, CBA has the potential to evolve into a reliable and indispensable\ntool for data-driven decision-making across a wide range of business contexts.\n20\n\nReferences\n[1]\nPaul Alpar and Michael Schulz. “Self-Service Business Intelligence”. In: Busi-\nness & Information Systems Engineering 58 (Feb. 2016), pp. 151–155.\n[2]\nAdem Alparslan and Denis Chernenko. “AI-Powered Opportunities for Busi-\nness Analytics – Self-service Analytics goes Conversational”. In: BI-Spektrum\n(2023), pp. 26–30.\n[3]\nAdem Alparslan and Torben Hügens. “Business Analytics im Mittelstand”. In:\nWirtschaftsinformatik & Management 15”, number = (2023). doi: 10.1365/\ns35764-023-00462-6.\n[4]\nArian Askari, Christian Poelitz, and Xinye Tang. MAGIC: Generating Self-\nCorrection Guideline for In-Context Text-to-SQL. 2024. arXiv: 2406.12692\n[cs.CL]. url: https://arxiv.org/abs/2406.12692.\n[5]\nImad Bani-Hani, Olgerta Tona, and Sven Carlsson. “Modes of engagement in\nSSBA: a service dominant logic perspective”. In: Proceedings of Americas Con-\nference on Information Systems. Association for Information Systems. 2019.\n[6]\nTobi Beck. “How to Simplify SQL with Text-to-SQL Technology”. In: (2024).\nAccessed: 2024-11-15.\n[7]\nTom B. Brown et al. Language Models are Few-Shot Learners. 2020. arXiv:\n2005.14165 [cs.CL]. url: https://arxiv.org/abs/2005.14165.\n[8]\nHock Chuan Chan, Hock-Hai Teo, and XH Zeng. “An evaluation of novice\nend-user computing performance: Data modeling, query writing, and compre-\nhension”. In: Journal of the American Society for Information Science and\nTechnology 56.8 (2005), pp. 843–853.\n[9]\nSurajit Chaudhuri and Umeshwar Dayal. “An Overview of Data Warehousing\nand OLAP Technology”. In: SIGMOD Rec. 26.1 (1997), pp. 65–74.\n[10]\nHsinchun Chen, Roger H. L. Chiang, and Veda C. Storey. “Business Intelli-\ngence and Analytics: From Big Data to Big Impact”. In: MIS Quarterly 36.4\n(2012), pp. 1165–1188.\n[11]\nSibei Chen et al. “ChatPipe: Orchestrating Data Preparation Pipelines by\nOptimizing Human-ChatGPT Interactions”. In: Companion of the 2024 Inter-\nnational Conference on Management of Data. 2024, pp. 484–487.\n[12]\nE. F. Codd. “Seven Steps to Rendezvous with the Casual User”. In: IFIP\nWorking Conference Data Base Management. 1974. url: https : / / api .\nsemanticscholar.org/CorpusID:28690513.\n[13]\nEdgar F. Codd, Sally B. Codd, and Clynch T. Salley. “Providing OLAP to\nUser-Analysts: An IT-Mandate”. In: Codd and Associates (1993).\n[14]\nDARPA. Explainable Artificial Intelligence (XAI). Tech. rep. DARPA-BAA-\n16-53. 2016.\n[15]\nEmily Barrow DeJeu. “Using generative AI to facilitate data analysis and\nvisualization: A case study of Olympic athletes”. In: Journal of Business and\nTechnical Communication (2024), p. 10506519241239923.\n21\n\n[16]\nDursun Delen and Haluk Demirkan. “Data, information and analytics as ser-\nvices”. In: Decision Support Systems 55 (Apr. 2013), pp. 359–363.\n[17]\nDursun Delen and Sudha Ram. “Research challenges and opportunities in\nbusiness analytics”. In: Journal of Business Analytics 1.1 (2018), pp. 2–12.\ndoi: 10.1080/2573234X.2018.1507324. eprint: https://doi.org/10.1080/\n2573234X.2018.1507324. url: https://doi.org/10.1080/2573234X.\n2018.1507324.\n[18]\nJacob Devlin et al. “BERT: Pre-training of Deep Bidirectional Transformers\nfor Language Understanding”. In: North American Chapter of the Association\nfor Computational Linguistics. 2019. url: https://api.semanticscholar.\norg/CorpusID:52967399.\n[19]\nAhmed Elgohary, Saghar Hosseini, and Ahmed Hassan Awadallah. Speak to\nyour Parser: Interactive Text-to-SQL with Natural Language Feedback. 2020.\narXiv: 2005.02539 [cs.CL]. url: https://arxiv.org/abs/2005.02539.\n[20]\nNitin Eusebius, Arghya Banerjee, and Randy DeFauw. Generating Value from\nEnterprise Data: Best Practices for Text2SQL and Generative AI. https://\naws.amazon.com/de/blogs/machine-learning/generating-value-from-\nenterprise-data-best-practices-for-text2sql-and-generative-ai/.\nAccessed: 2024-11-15.\n[21]\nAvrilia Floratou et al. “NL2SQL is a solved problem... Not!” In: CIDR. 2024.\nurl: https://www.cidrdb.org/cidr2024/papers/p74-floratou.pdf.\n[22]\nMartin Glinz and Samuel A. Fricker. “On shared understanding in software\nengineering: an essay”. In: Computer Science - Research and Development 30.3\n(2015), pp. 363–376. doi: 10.1007/s00450-014-0256-x. url: https://doi.\norg/10.1007/s00450-014-0256-x.\n[23]\nDavid Gunning and David Aha. “DARPA’s explainable artificial intelligence\n(XAI) program”. In: AI magazine 40.2 (2019), pp. 44–58.\n[24]\nChunxi Guo et al. “Retrieval-Augmented GPT-3.5-Based Text-to-SQL Frame-\nwork with Sample-Aware Prompting and Dynamic Revision Chain”. In: Neural\nInformation Processing. Ed. by Biao Luo et al. Singapore: Springer Nature\nSingapore, 2024, pp. 341–356. isbn: 978-981-99-8076-5.\n[25]\nIzzeddin Gür et al. “Dialsql: Dialogue based structured query generation”. In:\nProceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). 2018, pp. 1339–1349.\n[26]\nVijay Gurbaxani and Chris F. Kemerer. “An Agency Theory View of Man-\nagement of End-User Computing”. In: ICIS 1990 Proceedings. 28th European\nConference on Information Systems. 1990.\n[27]\nRudy Hirschheim, Heinz K Klein, and Kalle Lyytinen. Information systems de-\nvelopment and data modeling: conceptual and philosophical foundations. Vol. 9.\nCambridge University Press, 1995.\n[28]\nClyde W. Holsapple, Anita Lee-Post, and Ramakrishnan Pakath. “A unified\nfoundation for business analytics”. In: Decision Support Systems 64 (2014),\npp. 130–141.\n22\n\n[29]\nClaudia Imhoff and Colin White. “Self-service Business Intelligence: Empow-\nering Users to Generate Insights”. In: TDWI Best practices report (2011).\n[30]\nW.H. Inmon and Daniel Linstedt. “Data Architecture: a Primer for the Data\nScientist”. In: ed. by W.H. Inmon and Daniel Linstedt. Boston: Morgan Kauf-\nmann, 2015.\n[31]\nCarmen Iriarte and Sussy Bayona. “IT projects success factors: a literature\nreview”. In: International Journal of Information Systems and Project Man-\nagement 8 (2020), p. 15.\n[32]\nGeunyeong Jeong et al. “Improving Text-to-SQL with a Hybrid Decoding\nMethod”. In: Entropy 25.3 (2023). issn: 1099-4300. doi: 10.3390/e25030513.\nurl: https://www.mdpi.com/1099-4300/25/3/513.\n[33]\nGeorge Katsogiannis-Meimarakis and Georgia Koutrika. “A survey on deep\nlearning approaches for text-to-SQL”. In: The VLDB Journal 32.4 (2023),\npp. 905–936. doi: 10.1007/s00778-022-00776-8. url: https://doi.org/\n10.1007/s00778-022-00776-8.\n[34]\nChristian Koch, Markus Stadi, and Lukas Berle. “From Data Engineering\nto Prompt Engineering: Solving data preparation tasks with ChatGPT”. In:\n(2023).\n[35]\nAndreas Kokkalis et al. “Logos: a system for translating queries into narra-\ntives”. In: Proceedings of the 2012 ACM SIGMOD International Conference on\nManagement of Data. SIGMOD ’12. Scottsdale, Arizona, USA: Association for\nComputing Machinery, 2012, 673–676. isbn: 9781450312479. doi: 10.1145/\n2213836.2213929. url: https://doi.org/10.1145/2213836.2213929.\n[36]\nMarkus Langer et al. “What do we want from Explainable Artificial Intelli-\ngence (XAI)? – A stakeholder perspective on XAI and a conceptual model\nguiding interdisciplinary XAI research”. In: Artificial Intelligence 296 (2021),\np. 103473. issn: 0004-3702. doi: https://doi.org/10.1016/j.artint.\n2021.103473. url: https://www.sciencedirect.com/science/article/\npii/S0004370221000242.\n[37]\nChristian Lennerholt, Joeri Van Laere, and Eva Söderström. “User-Related\nChallenges of Self-Service Business Intelligence”. In: Information Systems Man-\nagement 38.4 (2021), pp. 309–323. doi: 10.1080/10580530.2020.1814458.\neprint: https://doi.org/10.1080/10580530.2020.1814458. url: https:\n//doi.org/10.1080/10580530.2020.1814458.\n[38]\nAristotelis Leventidis et al. “QueryVis: Logic-based Diagrams help Users Un-\nderstand Complicated SQL Queries Faster”. In: Proceedings of the 2020 ACM\nSIGMOD International Conference on Management of Data. SIGMOD ’20.\nPortland, OR, USA: Association for Computing Machinery, 2020, 2303–2318.\nisbn: 9781450367356. doi: 10.1145/3318464.3389767. url: https://doi.\norg/10.1145/3318464.3389767.\n[39]\nHuaxia Li et al. “Extracting Financial Data from Unstructured Sources: Lever-\naging Large Language Models”. In: (2023).\n23\n\n[40]\nJinyang Li et al. “Can LLM Already Serve as A Database Interface? A BIg\nBench for Large-Scale Database Grounded Text-to-SQLs”. In: Thirty-seventh\nConference on Neural Information Processing Systems Datasets and Bench-\nmarks Track. 2023. url: https://openreview.net/forum?id=dI4wzAE6uV.\n[41]\nYunyao Li, Dragomir Radev, and Davood Rafiei. Natural language interfaces\nto databases. Springer, 2024.\n[42]\nZichao Lin et al. “Towards trustworthy LLMs: a review on debiasing and\ndehallucinating in large language models”. In: Artificial Intelligence Review\n57.9 (2024), p. 243. doi: 10 . 1007 / s10462 - 024 - 10896 - y. url: https :\n//doi.org/10.1007/s10462-024-10896-y.\n[43]\nPengfei Liu et al. “Pre-train, Prompt, and Predict: A Systematic Survey of\nPrompting Methods in Natural Language Processing”. In: ACM Comput. Surv.\n55.9 (Jan. 2023). issn: 0360-0300. doi: 10.1145/3560815. url: https://doi.\norg/10.1145/3560815.\n[44]\nMarilex Rea Llave. “Data lakes in business intelligence: reporting from the\ntrenches”. In: Procedia Computer Science 138 (2018), pp. 516–524.\n[45]\nPaula Maddigan and Teo Susnjak. “Chat2VIS: generating data visualizations\nvia natural language using ChatGPT, codex and GPT-3 large language mod-\nels”. In: Ieee Access 11 (2023), pp. 45181–45193.\n[46]\nSven Michalczyk et al. “A State-of-the-Art Overview and Future Research\nAvenues of Self-Service Business Intelligence and Analytics”. In: ECIS 2020\nProceedings – Twenty-Eighth European Conference on Information Systems,\nMarrakesh, Marokko, June 15 - 17, 2020. 28th European Conference on Infor-\nmation Systems. ECIS 2020. 2020.\n[47]\nDaphne Miedema and George Fletcher. “SQLVis: Visual Query Representa-\ntions for Supporting SQL Learners”. In: 2021 IEEE Symposium on Visual\nLanguages and Human-Centric Computing (VL/HCC). 2021, pp. 1–9. doi:\n10.1109/VL/HCC51201.2021.9576431.\n[48]\nDang Minh et al. “Explainable artificial intelligence: a comprehensive review”.\nIn: Artificial Intelligence Review 55.5 (2022), pp. 3503–3568. doi: 10.1007/\ns10462- 021- 10088- y. url: https://doi.org/10.1007/s10462- 021-\n10088-y.\n[49]\nWalbert Cunha Monteiro et al. “Workload Evaluation to Create Data Visual-\nization Using ChatGPT”. In: 2023 27th International Conference Information\nVisualisation (IV). 2023, pp. 136–141. doi: 10.1109/IV60283.2023.00032.\n[50]\nAngelo Rossi Mori. “Coding systems and controlled vocabularies for hospi-\ntal information systems”. In: International Journal of Bio-Medical Computing\n39.1 (1995), pp. 93–98.\n[51]\nTakehiko Murakawa and Masaru Nakagawa. “Comprehension Support of SQL\nStatement using Double-tree Structure.” In: vol. 1. Jan. 2011, pp. 318–323.\n24\n\n[52]\nNurlan Musazade, József Mezei, and Xiaolu Wang. “Exploring the Perfor-\nmance of Large Language Models for Data Analysis Tasks Through the CRISP-\nDM Framework”. In: Good Practices and New Perspectives in Information\nSystems and Technologies. Ed. by Álvaro Rocha et al. Cham: Springer Nature\nSwitzerland, 2024, pp. 56–65.\n[53]\nRobbie T. Nakatsu. “Explanatory Power of Intelligent Systems”. In: Intelligent\nDecision-making Support Systems: Foundations, Applications and Challenges.\nLondon: Springer London, 2006, pp. 123–143. doi: 10.1007/1-84628-231-\n4_7. url: https://doi.org/10.1007/1-84628-231-4_7.\n[54]\nAthira Nambiar and Divyansh Mundra. “An Overview of Data Warehouse\nand Data Lake in Modern Enterprise Data Management”. In: Big Data and\nCognitive Computing 6.4 (2022). issn: 2504-2289. doi: 10.3390/bdcc6040132.\nurl: https://www.mdpi.com/2504-2289/6/4/132.\n[55]\nArpit Narechania et al. “DIY: Assessing the Correctness of Natural Language\nto SQL Systems”. In: Proceedings of the 26th International Conference on\nIntelligent User Interfaces. IUI ’21. College Station, TX, USA: Association for\nComputing Machinery, 2021, 597–607. isbn: 9781450380171. doi: 10.1145/\n3397481.3450667. url: https://doi.org/10.1145/3397481.3450667.\n[56]\nMehran Nasseri et al. “Applications of large language models (llms) in business\nanalytics–exemplary use cases in data preparation tasks”. In: International\nConference on Human-Computer Interaction. Springer. 2023, pp. 182–198.\n[57]\nR. Ryan Nelson, ed. End-user computing: Concepts, issues, and applications.\nUSA: John Wiley & Sons, Inc., 1989. isbn: 0471613592.\n[58]\nZheng Ning et al. “An empirical study of model errors and user error discov-\nery and repair strategies in natural language database queries”. In: Proceed-\nings of the 28th International Conference on Intelligent User Interfaces. 2023,\npp. 633–649.\n[59]\nMarcin Pałys and Andrzej Pałys. “Benefits and Challenges of Self-Service Busi-\nness Intelligence Implementation”. In: Procedia Computer Science 225 (2023).\n27th International Conference on Knowledge Based and Intelligent Informa-\ntion and Engineering Sytems (KES 2023), pp. 795–803. issn: 1877-0509. doi:\nhttps://doi.org/10.1016/j.procs.2023.10.066. url: https://www.\nsciencedirect.com/science/article/pii/S1877050923012243.\n[60]\nJens Passlick et al. “Self-service business intelligence and analytics application\nscenarios: A taxonomy for differentiation”. In: Information Systems and e-\nBusiness Management 21.1 (2023), pp. 159–191. doi: 10.1007/s10257-022-\n00574-3. url: https://doi.org/10.1007/s10257-022-00574-3.\n[61]\nGabrijela Perković, Antun Drobnjak, and Ivica Botički. “Hallucinations in\nLLMs: Understanding and Addressing Challenges”. In: 2024 47th MIPRO ICT\nand Electronics Convention (MIPRO). 2024, pp. 2084–2088. doi: 10.1109/\nMIPRO60963.2024.10569238.\n25\n\n[62]\nMohammadreza Pourreza and Davood Rafiei. “DIN-SQL: decomposed in-context\nlearning of text-to-SQL with self-correction”. In: Proceedings of the 37th In-\nternational Conference on Neural Information Processing Systems. NIPS ’23.\nNew Orleans, LA, USA: Curran Associates Inc., 2024.\n[63]\nCSR Prabhu et al. Big data analytics. Springer, 2019.\n[64]\nColin Raffel et al. “Exploring the Limits of Transfer Learning with a Unified\nText-to-Text Transformer”. In: Journal of Machine Learning Research 21.140\n(2020), pp. 1–67. url: http://jmlr.org/papers/v21/20-074.html.\n[65]\nCynthia Rudin. “Stop explaining black box machine learning models for high\nstakes decisions and use interpretable models instead”. In: Nature Machine\nIntelligence 1.5 (2019), pp. 206–215. doi: 10.1038/s42256- 019- 0048- x.\nurl: https://doi.org/10.1038/s42256-019-0048-x.\n[66]\nBill Schmarzo. AI & Data Literacy: Empowering Citizens of Data Science.\nPackt Publishing Ltd, 2023.\n[67]\nAnkita Sharma et al. “Automatic data transformation using large language\nmodel-an experimental study on building energy data”. In: 2023 IEEE Inter-\nnational Conference on Big Data (BigData). IEEE. 2023, pp. 1824–1834.\n[68]\nAndreas Stöckl. “Information Visualization with ChatGPT”. In: Artificial In-\ntelligence and Visualization: Advancing Visual Knowledge Discovery. Ed. by\nBoris Kovalerchuk et al. Cham: Springer Nature Switzerland, 2024, pp. 469–\n485. isbn: 978-3-031-46549-9. doi: 10.1007/978-3-031-46549-9_17. url:\nhttps://doi.org/10.1007/978-3-031-46549-9_17.\n[69]\nD. Stodder. “Visual Analytics for Making Smarter Decisions Faster”. In: TDWI\nBest practices report (2015).\n[70]\nSoroosh Tayebi Arasteh et al. “Large language models streamline automated\nmachine learning for clinical studies”. In: Nature Communications 15.1 (2024),\np. 1603. doi: 10.1038/s41467-024-45879-8. url: https://doi.org/10.\n1038/s41467-024-45879-8.\n[71]\nAstha Thapa and Rajvardhan Patil. “ChatGPT based ChatBot Application”.\nIn: SoutheastCon 2024. 2024, pp. 157–164. doi: 10.1109/SoutheastCon52093.\n2024.10500264.\n[72]\nDayton G. Thorpe, Andrew J. Duberstein, and Ian A. Kinsey. Dubo-SQL:\nDiverse Retrieval-Augmented Generation and Fine Tuning for Text-to-SQL.\n2024. arXiv: 2404.12560 [cs.CL]. url: https://arxiv.org/abs/2404.\n12560.\n[73]\nYuan Tian et al. SQLucid: Grounding Natural Language Database Queries\nwith Interactive Explanations. 2024. arXiv: 2409.06178 [cs.HC]. url: https:\n//arxiv.org/abs/2409.06178.\n[74]\nMihaela Tomova et al. “Assessing the utility of text-to-SQL approaches for\nsatisfying software developer information needs”. In: Empirical Software En-\ngineering 29.1 (2023), p. 15. doi: 10.1007/s10664- 023- 10374- z. url:\nhttps://doi.org/10.1007/s10664-023-10374-z.\n26\n\n[75]\nQuoc-Bao-Huy Tran, Aagha Abdul Waheed, and Sun-Tae Chung. “Robust\nText-to-Cypher Using Combination of BERT, GraphSAGE, and Transformer\n(CoBGT) Model”. In: Applied Sciences 14.17 (2024). issn: 2076-3417. doi:\n10.3390/app14177881. url: https://www.mdpi.com/2076-3417/14/17/\n7881.\n[76]\nRichard Tunney. A Primer of Judgment and Decision Making. Springer, 2024.\n[77]\nAshish Vaswani et al. “Attention is all you need”. In: Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems. NIPS’17.\nLong Beach, California, USA: Curran Associates Inc., 2017, 6000–6010. isbn:\n9781510860964.\n[78]\nCarlo Vercellis. Business Intelligence: Data Mining and Optimization for De-\ncision Making. 1st. 2009.\n[79]\nJohn Von Neumann and Oskar Morgenstern. “Theory of games and economic\nbehavior: 60th anniversary commemorative edition”. In: Theory of games and\neconomic behavior. Princeton university press, 2007.\n[80]\nJerold W Wallis and Edward H Shortliffe. “Explanatory power for medical\nexpert systems: studies in the representation of causal relationships for clinical\nconsultations”. In: Methods of Information in Medicine 21.03 (1982), pp. 127–\n136.\n[81]\nXiaxia Wang et al. “An interactive NL2SQL approach with reuse strategy”. In:\nDatabase Systems for Advanced Applications: 26th International Conference,\nDASFAA 2021, Taipei, Taiwan, April 11–14, 2021, Proceedings, Part II 26.\nSpringer. 2021, pp. 280–288.\n[82]\nDavid H.D. Warren and Fernando C.N. Pereira. “An Efficient Easily Adapt-\nable System for Interpreting Natural Language Queries”. In: American Jour-\nnal of Computational Linguistics 8.3-4 (1982), pp. 110–122. url: https://\naclanthology.org/J82-3002.\n[83]\nHugh Watson. “Update Tutorial: Big Data Analytics: Concepts, Technology,\nand Applications”. In: Communications of the Association for Information\nSystems 44 (Jan. 2019), pp. 364–379. doi: 10.17705/1CAIS.04421.\n[84]\nHugh J Watson. “Business intelligence: Past, present and future”. In: (2009).\n[85]\nJack H Westbrook and Walter Grattidge. “Terminology Standards for Mate-\nrials Databases”. In: Reference 15 (1992), p. 15.\n[86]\nYang Wu et al. “Automated Data Visualization from Natural Language via\nLarge Language Models: An Exploratory Study”. In: Proceedings of the ACM\non Management of Data 2.3 (2024), pp. 1–28.\n[87]\nZiyu Yao et al. “Model-based interactive semantic parsing: A unified formula-\ntion and a text-to-SQL case study”. In: 2019 Conference on Empirical Methods\nin Natural Language Processing (EMNLP’19). 2019.\n[88]\nZiyu Yao et al. “Model-based interactive semantic parsing: A unified frame-\nwork and a text-to-SQL case study”. In: arXiv preprint arXiv:1910.05389\n(2019).\n27\n\n[89]\nTao Yu et al. Spider: A Large-Scale Human-Labeled Dataset for Complex and\nCross-Domain Semantic Parsing and Text-to-SQL Task. 2019. arXiv: 1809.\n08887 [cs.CL]. url: https://arxiv.org/abs/1809.08887.\n[90]\nHaochen Zhang et al. Large Language Models as Data Preprocessors. 2023.\narXiv: 2308.16361 [cs.AI]. url: https://arxiv.org/abs/2308.16361.\n[91]\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2SQL: Generating\nStructured Queries from Natural Language using Reinforcement Learning. 2017.\narXiv: 1709.00103 [cs.CL]. url: https://arxiv.org/abs/1709.00103.\n28",
    "pdf_filename": "The_Role_of_Accuracy_and_Validation_Effectiveness_in_Conversational_Business_Analytics.pdf"
}