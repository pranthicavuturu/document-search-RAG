{
    "title": "DiM: f-Divergence Minimization Guided Sharpness-Aware Optimization for",
    "abstract": "tures have made significant advancements in medical im- As a technique to alleviate the pressure of data an- agesegmentation,suchasU-Net[24],U-Net++[46],andH- notation, semi-supervised learning (SSL) has attracted DenseUNet[13]. However, the success of these technolo- widespreadattention. Inthespecificdomainofmedicalim- gieslargelyreliesonlarge-scale,pixel-levelannotateddata. age segmentation, semi-supervised methods (SSMIS) have In practice, annotating medical images is not only costly becomearesearchhotspotduetotheirabilitytoreducethe butalsochallengingduetoissuessuchaslowcontrastand needforlargeamountsofpreciselyannotateddata. SSMIS noise, making it difficult to clearly display images. More- focuses on enhancing the model’s generalization perfor- over, medical images require more specialized knowledge mancebyleveragingasmallnumberoflabeledsamplesand compared to natural images, which makes constructing a alargenumberofunlabeledsamples. Thelatestsharpness- large-scale, accurately annotated medical image database aware optimization (SAM) technique, which optimizes the nearly an impossible task. In contrast, semi-supervised model by reducing the sharpness of the loss function, has learning [23, 35] offers a new solution to the problem of shownsignificantsuccessinSSMIS.However,SAMandits insufficient data supervision in weakly supervised learn- variants may not fully account for the distribution differ- ing [47]. It primarily utilizes a small amount of labeled encesbetweendifferentdatasets. Toaddressthisissue, we dataandalargeamountofunlabeleddataforjointtraining. propose a sharpness-aware optimization method based on Clearly,semi-supervisedlearningissignificantlymoresuit- f-divergenceminimization(DiM)forsemi-supervisedmed- able for medical image segmentation and adapting to real- icalimagesegmentation.Thismethodenhancesthemodel’s worldclinicalscenariosthantraditionalsupervisedlearning stability by fine-tuning the sensitivity of model parameters methods. andimprovesthemodel’sadaptabilitytodifferentdatasets Due to the easy availability of unlabeled data, doctors through the introduction of f-divergence. By reducing f- maynothavethetimetoverifyitsdistributionwhenfaced divergence, the DiM method not only improves the perfor- withmassiveamountsofdata. This”domainshift”[7,25] mance balance between the source and target datasets but issue can lead to significant performance degradation in alsopreventsperformancedegradationduetooverfittingon models, and it isa critical concern when developing semi- thesourcedataset. supervisedmedicalimagesegmentation(SSMIS)[21]mod- els. In fact, we should allow unlabeled data to come from one or more different distributions. However, existing un- 1.Introduction superviseddomainadaptation(UDA)[11,43,44]methods donotdirectlyaddressthisissuebecausetheyrelyonlarge Medical Image Segmentation (MIS)[3, 14, 33] plays a amounts of labeled source domain data, which is exactly crucial role in assisting computers with disease diagno- whatSSMISaimstoresolve. sis and treatment research by helping identify key organs Recent studies, such as Sharpness-Aware Minimization or lesions in abnormal images. Recently, numerous su- (SAM)[8], enhance model generalization performance by *CorrespondingAuthor reducingthesharpnessofthelossfunction. Here,Lrepre- 4202 voN 91 ]VC.sc[ 1v05321.1142:viXra",
    "body": "DiM: f-Divergence Minimization Guided Sharpness-Aware Optimization for\nSemi-supervised Medical Image Segmentation\nBingliWang HouchengSu NanYin\nTsinghuaUniversity(SZ) UniversityofMacau ZayedUniversityofArtificialIntelligence\nUnitedArabEmirates\nyinnan8911@gmail.com\nMengzhuWang* LiShen*\nHebeiUniversityofTechnology SunYat-senUniversity\ndreamkily@gmail.com mathshenli@gmail.com\nAbstract pervisedlearning-basedencoder-decodernetworkarchitec-\ntures have made significant advancements in medical im-\nAs a technique to alleviate the pressure of data an- agesegmentation,suchasU-Net[24],U-Net++[46],andH-\nnotation, semi-supervised learning (SSL) has attracted DenseUNet[13]. However, the success of these technolo-\nwidespreadattention. Inthespecificdomainofmedicalim- gieslargelyreliesonlarge-scale,pixel-levelannotateddata.\nage segmentation, semi-supervised methods (SSMIS) have In practice, annotating medical images is not only costly\nbecomearesearchhotspotduetotheirabilitytoreducethe butalsochallengingduetoissuessuchaslowcontrastand\nneedforlargeamountsofpreciselyannotateddata. SSMIS noise, making it difficult to clearly display images. More-\nfocuses on enhancing the model’s generalization perfor- over, medical images require more specialized knowledge\nmancebyleveragingasmallnumberoflabeledsamplesand compared to natural images, which makes constructing a\nalargenumberofunlabeledsamples. Thelatestsharpness- large-scale, accurately annotated medical image database\naware optimization (SAM) technique, which optimizes the nearly an impossible task. In contrast, semi-supervised\nmodel by reducing the sharpness of the loss function, has learning [23, 35] offers a new solution to the problem of\nshownsignificantsuccessinSSMIS.However,SAMandits insufficient data supervision in weakly supervised learn-\nvariants may not fully account for the distribution differ- ing [47]. It primarily utilizes a small amount of labeled\nencesbetweendifferentdatasets. Toaddressthisissue, we dataandalargeamountofunlabeleddataforjointtraining.\npropose a sharpness-aware optimization method based on Clearly,semi-supervisedlearningissignificantlymoresuit-\nf-divergenceminimization(DiM)forsemi-supervisedmed- able for medical image segmentation and adapting to real-\nicalimagesegmentation.Thismethodenhancesthemodel’s worldclinicalscenariosthantraditionalsupervisedlearning\nstability by fine-tuning the sensitivity of model parameters methods.\nandimprovesthemodel’sadaptabilitytodifferentdatasets Due to the easy availability of unlabeled data, doctors\nthrough the introduction of f-divergence. By reducing f- maynothavethetimetoverifyitsdistributionwhenfaced\ndivergence, the DiM method not only improves the perfor- withmassiveamountsofdata. This”domainshift”[7,25]\nmance balance between the source and target datasets but issue can lead to significant performance degradation in\nalsopreventsperformancedegradationduetooverfittingon models, and it isa critical concern when developing semi-\nthesourcedataset. supervisedmedicalimagesegmentation(SSMIS)[21]mod-\nels. In fact, we should allow unlabeled data to come from\none or more different distributions. However, existing un-\n1.Introduction superviseddomainadaptation(UDA)[11,43,44]methods\ndonotdirectlyaddressthisissuebecausetheyrelyonlarge\nMedical Image Segmentation (MIS)[3, 14, 33] plays a\namounts of labeled source domain data, which is exactly\ncrucial role in assisting computers with disease diagno-\nwhatSSMISaimstoresolve.\nsis and treatment research by helping identify key organs\nRecent studies, such as Sharpness-Aware Minimization\nor lesions in abnormal images. Recently, numerous su-\n(SAM)[8], enhance model generalization performance by\n*CorrespondingAuthor reducingthesharpnessofthelossfunction. Here,Lrepre-\n4202\nvoN\n91\n]VC.sc[\n1v05321.1142:viXra\nsents the loss function to be minimized, and θ represents flatregionwithasmallloss.\nthe parameters of the neural network. SAM first com- • Tothebestofourknowledge,thisisthefirstworktoap-\nputes a weight perturbation ϵ that maximizes the empiri- plyf-divergenceconstraintstoSAMparadigm.\ncalriskL(θ),andthenminimizesthelossoftheperturbed • wedemonstratethesuperiorperformanceofDiMtostate-\nnetwork. In short, SAM aims to reduce the maximum of-the-artsonthreeSSMISbenchmarks.\nloss near the model parameters θ. Due to the complexity\nof this minimization-maximization optimization problem, 2.RelatedWork\nSAMapproximatesLwithasurrogatelossfunctionL (θ)\np\n2.1.Semi-supervisedMedicalImageSegmentation\nforminimization.However,itisimportanttonotethatmini-\nmizingL (θ)doesnotguaranteereachingtheflatminimum\np Duetothecomplexityofmedicalimages,extensivemanual\nregion for SSMIS [48]. KL divergence [32] has demon-\nannotation by experts is both challenging and costly [49].\nstrated strong performance in SSMIS. The application of\nTo address this, semi-supervised medical image segmen-\nKL divergence in SSMIS primarily improves model train-\ntation approaches have emerged as effective solutions that\ning efficiency and accuracy by measuring the differences\nleveragelimitedlabeleddata[3]. Luoetal. [19]proposed\nbetween different probability distributions. This is partic-\na dual-task consistency-based semi-supervised framework\nularly useful when dealing with limited labeled data and a\ntosimultaneouslypredictper-pixelsegmentationmapsand\nlargeamountofunlabeleddata,asithelpsguidethemodel\ngeometrically-aware level set representations, introducing\nto learn more useful information in a semi-supervised set-\na dual-task consistency regularization to enhance perfor-\nting. For example, MMLBF [6] propose a region-based\nmance. Wu et al. [39] presented MC-Net++, which em-\nmulti-phase level set method based on KL divergence. Lu\nploys a shared encoder and multiple distinct decoders and\netal.[18]estimateuncertaintybycalculatingtheKullback-\nintroduced a new mutual consistency constraint. This ap-\nLeibler divergence between the predictions of the student\nproachstatisticallyidentifiesuncertainregions,particularly\nandteachermodels,anddirectlyusethisuncertaintytocor-\nhard regions within unlabeled data. Luo et al. [20] in-\nrectthelearningofnoisypseudo-labels,ratherthansetting\ncorporated a cross-teaching approach between CNNs and\na fixed threshold to filter pseudo-labels. SwinMM[37] in-\nTransformers, resulting in a simple yet efficient semi-\ncludesamaskedmulti-viewencoderandanovelproxytask\nsupervised learning framework. Miao et al. [23] high-\nbased on mutual learning, which contributes to effective\nlightedtheimportanceofmodelindependencebetweennet-\nself-supervisedpretraining.\nworks or branches in semi-supervised medical segmenta-\nHowever, all of these methods are considered from the tion (SSMS). Ma et al. [22] identified the issue of perfor-\nperspective of KL divergence, which is highly sensitive mance degradation in semi-supervised medical image seg-\nto probability values close to zero in the target distribu- mentation due to shared domain distributions, proposing\ntion, often leading to an infinite divergence. In contrast, Mixed-domain Semi-supervised Medical Image Segmen-\nf-divergencecanreducethissensitivitybyselectinganap- tation (MIDSS). They emphasized that generating reliable\npropriate function, making it more stable and robust, es- pseudo-labelsforunlabeleddataiscrucialindomainshifts\npeciallywhendealingwithsparseorextremedistributions. inlabeleddata. Nevertheless,duetotheinherentcomplex-\nInSSMIS,SAMemphasizesachievingstabilitybycontrol- ity of medical images, these models often exhibit limited\nling the sensitivity of model parameters, while the intro- generalizationabilityandconvergenceinstability,whichre-\nduction of f-divergence helps further regulate the model’s mainsachallengingproblem.\nadaptability across different domains. By minimizing f-\n2.2.UnsupervisedDomainAdaptation\ndivergence, SAM can enhance the balanced performance\nof the model across both the source and target domains,\nUnsupervised Domain Adaptation (UDA) [9, 15, 34] aims\nwhile avoiding performance degradation due to overfitting\nto adapt models from a labeled source domain to an unla-\nthesourcedomain. Inthiswork,toovercomethelimitation\nbeled target domain by minimizing the domain shift. This\nofSAMandexplorethefullpotentialoff-divergence, we\nalignmentoffeaturedistributionsenablesknowledgetrans-\npresent a novel method f-divergence minimization guided\nfer from source to target, enhancing classification perfor-\nsharpness-aware optimization for semi-supervised medical\nmance [10, 38]. Many UDA approaches use a domain\nimage segmentation (DiM). By consider the f-divergence\nclassifier to distinguish source from target features, while\nand sharpness-aware minimization, which can still be ef-\nthe feature extractor learns to match feature distributions\nfectivelycomputedevenwhenthesupportsetsofthedistri-\n[16,30]. UDAiswidelyusedintaskslikeimageclassifica-\nbutions are different. Our main contributions can be sum-\ntion[15],semanticsegmentation[28],andobjectdetection\nmarizedasfollows:\n[27]. Semi-supervised domain adaptation further incorpo-\n• weanalyzethelimitationsofSAM-likemethodsandpro- ratesasmallamountoflabeledtargetdatatoimprovetrans-\npose f-divergence to ensure the model convergence to a fer[26].\nTable1.Variouscommonlyusedf-divergenceswiththeirderivativesandsecondderivatives.\nf-divergence f(x) f′(x) f′′(x)\nReverseKL xlogx logx+1 1\nx\nForwardKL −logx −1 1\nx x2\nJeffrey (x−1)logx logx+1− 1 1 + 1\nx x x2\nJensen-Shannon −x+1logx+1 + xlogx 1log 2x 1\n2 2 2 2 x+1 2x(x+1)\nPearson (1−x)2 1− 1 2\nx x2 x3\n2.3.Sharpness-AwareMinimization(SAM) alization in the target domain. Additionally, f-divergence\noperates effectively in high-dimensional spaces, making\nForet et al. [8] observed that solely minimizing training\nit well-suited for capturing subtle but critical variations\nloss can lead to suboptimal model quality. They proposed\nin medical image features. By aligning feature distribu-\nSharpness-Aware Minimization (SAM), which seeks pa-\ntions, f-divergence fosters a stable and consistent feature\nrameters in neighborhoods of uniformly low loss, result-\nrepresentation, enhancing segmentation accuracy in semi-\ning in a Min-Max optimization problem suitable for gra-\nsupervisedconditions.\ndient descent. Andriushchenko et al. [1] provided theo-\nForaconvexfunctionf(x) : R+ → Rwithf(1) = 0,\nretical insights on SAM’s implicit bias in diagonal linear\nthef-divergenceD (p ∥p )isdefinedas:\nnetworksandempiricallyexamineditsbehaviorinnonlin- f label unlabel\near networks. Zhou et al. [45] addressed SAM’s limita- (cid:20) (cid:18) (cid:19)(cid:21)\np (x)\nt ti ao iln ci ln ash sa en sd ,l bin yg ic nl ta ross duim cib na gla In mc be, alp aa nr cti ec du -l Sa Arly Mov (Ie mrfi bt Sti Ang Mt )o\n,\nD f(p label∥p unlabel)=E x∼punlabel f p unla lb abel el(x) (1)\n+f′(∞)p (p =0),\na class-aware smoothing approach effective in long-tailed label unlabel\nclassificationandsemi-supervisedanomalydetectiontasks.\nwhere f′(∞) = lim tf(cid:0)1(cid:1) . The second term repre-\nWangetal. [36]introducedamodelintegratingMedSAM t→0 t\nsents the contribution of points x in the support of p\nwith an uncertainty-aware loss function and SharpMin op- label\nwhere p (x) = 0, which accounts for cases where the\ntimizer, enhancing segmentation accuracy and robustness. unlabel\nlabeledandunlabeleddatadistributionsdonotoverlap.\nHowever, a tailored solution for semi-supervised medical\nimagesegmentationremainsabsent. Toalignp labelandp unlabel,wedefinethealignmentloss:\nL =D (p ∥p )\n3.Methods align f label unlabel\n(cid:20) (cid:18) p (x) (cid:19)(cid:21) (2)\n3.1.AligningFeaturesviaf-Divergence =E x∼punlabel f p label (x)\nunlabel\nSemi-supervised medical image segmentation is challeng-\nMinimizing L encourages the logits of unlabeled\ning due to the scarcity of labeled data and the high- align\ndata to approximate those of labeled data, enhancing fea-\ndimensionalcomplexityofmedicalimages. Inthissetting,\ntureconsistencyforsemi-supervisedlearning.\nmodels must leverage both labeled and unlabeled data to\nTo quantify this alignment, we utilize specific f-\nlearn precise segmentation boundaries. However, limited\ndivergence variants frequently used in machine learning,\nlabeleddatacanleadtofeaturedrift,wheretherepresenta-\nincluding Jeffrey divergence, Jensen-Shannon divergence,\ntionslearnedfromunlabeleddatadeviatefromthosebased\nand Pearson divergence. Each variant has a unique form\nonlabeleddata.Thismisalignmentbetweenlabeledandun-\nof f(x), f′(x), and f′′(x), as shown in Table 1, enabling\nlabeledfeaturedistributionsreducessegmentationaccuracy\nflexible divergence calculations between p and p .\nandlimitsgeneralizationonunseendata. label unlabel\nToaddressthis,weutilizef-divergencetoalignthehigh- The f-divergences are computed via Monte Carlo estima-\ntion based on samples from p , applying the respec-\ndimensional logits from labeled and unlabeled data, con- unlabel\ntivevaluesinTable1toevaluateL andfacilitateback-\nstraining the features of unlabeled data based on a limited align\nset of labeled data. Let p and p denote the distri- propagationduringtraining.\nlabel unlabel\nbutions of logits for the labeled and unlabeled data over a\n3.2.Sharpness-AwareEntropyMinimization\ndiscrete set X. Our goal is to guide p by minimiz-\nunlabel\ning the f-divergence between these distributions in high- Semi-supervised medical image segmentation leverages\ndimensionalspace. both labeled and unlabeled data for accurate boundary de-\nThisalignmentmitigatesfeaturedrift,improvinggener- tection. However, limited labeled data and distribution\nshiftsoftenleadtofeatureinconsistencyandunreliablepre- In this study, we introduce Sharpness-Aware Entropy\ndictions, especially on unseen test samples, necessitating Minimization(SAEM), an approach that combines entropy\nmethods that improve model robustness to distributional minimization with sharpness-aware training to achieve\nvariations.Sharpness-awareminimization(SAM)enhances adaptive entropy reduction, enhancing model stability un-\nmodelgeneralizationbyoptimizingwithinlow-lossneigh- derchallengingconditions. Here,S(x)andESA(x;θ)rep-\nborhoods,stabilizingperformanceunderdistributionshifts. resententropymeasuresasdefinedinEquations(3)and(4),\nHowever, directly filtering unreliable test samples using respectively. Thelearnableparametersdesignatedforadap-\ngradient norms is challenging due to variations in scale tationaredenotedasθ˜⊂θ.\nacrossmodelsandshifts. Inessence,SAEMoffersarobustframeworkbyintegrat-\nDirectlyusinggradientnormstofilteroutunreliabletest ingentropyfilteringwithsharpness-awaretraining,yielding\nsamples is challenging due to variability in scale across adaptiveentropyreductionwhileensuringmodelresilience,\nmodels and types of distribution shifts. Instead, we lever- particularlyunderdemandingconditions.\nage entropy as a proxy for gradient magnitude, selecting\n3.3.LossFunction\nsampleswithlowentropyvaluestofocusadaptationoncon-\nfidentpredictions. GivenanentropyfunctionE(x;θ)fora TheoveralllossL iscomposedofthefollowingcompo-\ntotal\nsamplexwithmodelparametersθ,wedefinetheselective nents:\nentropyminimizationas: 1. Supervised Loss (L ): Applied to labeled data\ns\nto guide predictions with ground truth, combining cross-\nm θinS(x)E(x;θ), S(x)≜I {E(x;θ)<E0}(x) (3) entropyanddicelossesforaccuratesegmentation.\n2. Intermediate Losses (L and L ): Defined for\nin out\nwhereS(x)isanindicatorfunctionthatactivateswhenthe intermediate samples us and us , each using weighted\nentropyE(x;θ)isbelowapre-definedthresholdE . This in out\n0 cross-entropy (L ) and dice loss (L ) to enforce con-\nce dice\napproach ensures that only samples with low entropy (i.e.,\nsistencybetweenpseudolabelsandmodelpredictions.\nhigh confidence) contribute to the training, effectively fil-\ntering out unreliable samples that might otherwise induce L =L (pˆ ,ps ,w )+L (pˆ ,ps ,w ) (8)\nin ce in in in dice in in in\nlargegradients.\nForfurtherstability,weaimtoguidethemodeltowards\nL =L (pˆ ,ps ,w )\nflatterregionsoftheentropylosslandscape,whichreduces out ce out out out (9)\n+L (pˆ ,ps ,w )\nsensitivitytonoisygradients. Wedefineasharpness-aware dice out out out\nentropyobjective, ESA(x;θ), thatmeasuresthemaximum wherew andw arepixel-wiseweightssetbyaconfi-\nin out\nentropywithinaperturbationneighborhoodaroundthecur- dencethresholdtofilterunreliablepseudolabels.\nrentparameters: Eachcomponentplaysacrucialroleinenforcingrobust\nsupervisiononbothlabeledandunlabeleddata,supporting\nminESA(x;θ), ESA(x;θ)≜ max E(x;θ+ϵ) (4) reliable predictions across domains. The overall loss L\nθ ∥ϵ∥2≤ρ total\nisdefinedasfollows:\nwhere ϵ is a perturbation vector constrained within a Eu-\nclidean ball of radius ρ. This inner maximization encour- L total =L s+λ(L in+L out+λL sym)+L align (10)\nages the model to be robust against perturbations, promot-\nwhereλisatime-dependentcoefficientthatscalesunsuper-\ningaflatminimumfortheentropyloss.FollowingtheSAM\nvisedcomponentsastrainingprogresses,definedby:\napproach,weapproximateϵ∗(θ)by:\nλ(t)=e−5(1−t/ttotal). (11)\n|∇ E(x;θ)|\nϵˆ(θ)=ρ sign(∇ E(x;θ)) θ (5) 4.Experiments\nθ ∥∇ E(x;θ)∥\nθ 2\n4.1.ExperimentDatasets\nSubstituting ϵˆ(θ) back into the objective, we obtain an\napproximationforthegradientthatencouragesflatminima: Fundusdatasetconsistsofretinalfundusimagesgathered\nfromfourmedicalcenters,mainlyintendedfortasksinvolv-\n(cid:12)\n∇ ESA(x;θ)≈∇ E(x;θ)(cid:12) (6) ingthesegmentationoftheopticcupanddisc. Eachimage\nθ θ (cid:12)\nθ+ϵˆ(θ) has been cropped to create a region of interest within an\nOur final objective for Reliable Sharpness-Aware En- 800×800boundingbox. Wethenresizeandrandomlycrop\ntropy Minimization combines selective entropy minimiza- theseimagestoasizeof256×256.\ntionandsharpness-awareoptimization: ProstatedatasetincludesprostateT2-weightedMRIdata,\ncomplete with segmentation masks, sourced from six dif-\nminS(x)ESA(x;θ) (7)\nferentlocationsacrossthreepublicdatasets. Werandomly\nθ˜\ndividethedatasetintotrainingandtestingsetsataratioof supervised and unsupervised domain adaptation methods,\n4:1, resizing and randomly cropping each 2D slice to 384 such as error accumulation and limited knowledge trans-\n×384. Labeledsamplesarechosenfromconsecutiveslices fer,ensuringbothhighaccuracyandgeneralizabilityacross\nwithin individual cases, ensuring there is at most one case multipledomains.\noverlapandnooverlapofsliceswithunlabeledsamples. Results on Prostate dataset. As shown in Table 3,\nDiM achieves outstanding performance across all metrics\n4.2.ComparisonMethodsandSettings\nonProstatedataset. ItisalsonoteworthythatDiMachieves\nOur method is implemented in PyTorch and utilizes an thehighestDCandJCaverageswhilemaintainingthelow-\nNVIDIA GeForce RTX 3090 GPU. We establish default est HD and ASD scores, suggesting higher segmentation\nexperimental parameters for training. Optimization is per- accuracy and boundary precision. The inclusion of SAM\nformedusingtheSAMoptimizer,withabaseoptimizerof likelycontributestoimprovedgeneralizationbymitigating\nStochastic Gradient Descent (SGD) set to a momentum of sharpminima, whilef-divergencelossenhancesalignment\n0.9,aweightdecayof0.0001,andaninitiallearningrateof of the predicted and true distributions, reducing segmen-\n0.03. Thebatchsizeissetto8,comprising4labeledand4 tation errors. These results underscore the robustness and\nunlabeledsamples. Weconductatotalof30,000iterations effectivenessofourmethod.\nfortheFundusdatasetand60,000iterationsfortheProstate\n4.4.AblationStudy\ndataset.\nDuringtesting, thefinalsegmentationresultsaregener- Firstly, We conduct ablation studies to show the impact of\nated by the student model. Our approach is benchmarked eachcomponentinDiM.Sencondly,ourobjectiveistoas-\nagainst several state-of-the-art (SOTA) methods, including sesswhethervaryingtypesoff-divergencesleadtonotable\nsupervisedtechniquessuchasUA-MT[42],FixMatch[31], performancedifferencesinthemodelandtoidentifytheop-\nCPS[5],CoraNet[29],SS-Net[40],BCP[2],CauSSL[23], timalformforsuperioroutcomes.\nand MiDSS [21], as well as domain-unsupervised adap- TheeffectivenessofSAM.AsdemonstratedinTable4,in-\ntation methods like FDA [41], SIFA [4], and UDA- corporatingtheSharpness-AwareMinimization(SAM)op-\nVAE++[17]. timizer (as seen in Method #2 and #4 compared to #1.)\nIneachexperiment,alimitedamountofdatafromades- enhances model performance on the Optic Cup/Disc seg-\nignateddomain(e.g.,Domain1inTab.2)islabeled,while mentationtask. SAMeffectivelyreducesthemodel’sloss,\ntheremainingdataaretreatedasunlabeled. Fortheupper- increasing robustness to minor data distribution shifts and\nbound comparison, we utilized the f-divergence, specifi- enabling more efficient capture of inter-sample similarity.\ncally employing the Jensen-Shannon divergence to calcu- Consequently,SAMimprovestheDCandJCscoreswhile\nlate the distance between logits, and used the most naive reducingtheHDandASD.TheseresultsindicatethatSAM\nSAM optimizer in our experiments. For the upper bound, not only strengthens the model’s generalization ability but\nwefollowedtheresultsoftheMiDSSpaper,whichapplied also enhances segmentation accuracy and boundary preci-\nUCPwithintheFixMatchframework,utilizingallavailable sion.\ntrainingdatafromaspecificdomainaslabeleddata,provid- The effectiveness of f-Divergence. The incorporation of\ningthemodelwithcomprehensivesourcedomaininforma- f-divergence (as seen in Method #3 and #4 compared to\ntion. #1.) contributestonotableimprovementsinmodelperfor-\nOurevaluationmetricsincludetheDicecoefficient(DC), mance,particularlythroughamarkedreductioninASDand\nJaccardcoefficient(JC),95%HausdorffDistance(HD),and HD metrics, as shown in Table 4. By quantifying the dis-\nAverage Surface Distance (ASD). Except for SIFA, which crepancy between probability distributions of labeled and\nincorporates ResNet blocks [12] for its generator and de- unlabeleddata,f-divergenceenhancesthemodel’scapacity\ncoder,allmethodsemploytheU-Netbackbone[24]. to represent features within the unlabeled dataset. Exper-\nimental results demonstrate that introducing f-divergence\n4.3.ComparisonwithState-of-the-ArtMethods\nallowsthemodel tomorepreciselycapturethe boundaries\nResults on Fundus dataset. With only 20 labeled sam- ofstructurallysimilarregions. Thisresultsinfurthergains\nples,DiMachievessuperiorperformanceacrossalldomains inDCandJCmetrics,alongwithsubstantialreductionsin\nin the optic cup/disc segmentation task, as illustrated in HDandASD,therebyindicatingimprovedaccuracyinseg-\nTable 2. DiM consistently outperforms competing meth- mentation,especiallyalongedgedetails.\nods in all metrics, which achieves the highest average DC f-Divergence strategies. Based on the experimental re-\nand JC scores while maintaining the lowest HD and ASD, sults shown in the Table 5, different f-divergence strate-\nhighlightingitsrobustnessandprecisioninsegmentingdual giesdemonstratevaryingdegreesofeffectivenessforoptic\nobjects with overlapping regions. These results suggest cupanddiscsegmentationacrossfourdomainsintheFun-\nthat DiM effectively mitigates issues faced by other semi- dusdataset. Generally,theJSdivergenceandJeffreydiver-\nTask OpticCup/DiscSegmentation\nDC↑ DC↑ JC↑ HD↓ ASD↓\nMethod #L\nDomain1 Domain2 Domain3 Domain4 Avg. Avg. Avg. Avg.\nU-Net 20 59.54/73.89 71.28/74.23 50.87/64.29 35.61/63.30 61.63 52.65 48.28 28.86\nUA-MT MICCAI’19 20 59.35/78.46 63.08/74.45 35.24/47.73 36.18/55.43 56.24 47.00 48.64 31.35\nFDA CVPR’20 20 76.99/89.94 77.69/89.63 78.27/90.96 64.52/74.29 80.29 71.05 16.23 8.44\nSIFA TMI’20 20 50.67/75.30 64.44/80.69 61.67/83.77 55.07/70.67 67.78 54.77 20.16 10.93\nFixMatch NeurIPS’20 20 81.18/91.29 72.04/87.60 80.41/92.95 74.58/87.07 83.39 73.48 11.77 5.60\nCPS CVPR’21 20 64.53/86.25 70.26/86.97 42.92/54.94 36.98/46.70 61.19 52.69 34.44 26.79\nCoraNet TMI’21 20 61.64/87.32 65.56/87.05 66.12/83.54 49.01/77.73 72.25 60.50 20.52 10.44\nUDA-VAE++ CVPR’22 20 55.01/80.76 68.87/85.94 63.23/84.92 68.42/80.89 73.51 61.40 17.60 9.86\nSS-Net MICCAI’22 20 59.42/78.15 67.32/85.05 45.69/69.91 38.76/61.13 63.18 53.49 44.90 25.73\nBCP CVPR’23 20 71.65/91.10 77.19/92.00 72.63/90.77 77.67/91.42 83.05 73.66 11.05 5.80\nCauSSL ICCV’23 20 63.38/80.60 67.52/80.72 49.53/63.88 39.43/49.43 61.81 51.80 41.25 23.94\nMiDSS CVPR’24 20 83.39/92.96 73.12/88.88 83.50/92.97 78.63/93.38 85.85 76.95 9.06 4.40\nDiM 20 84.68/92.91 78.16/90.49 84.82/93.36 81.63/92.18 87.28 78.53 7.83 3.82\nUpperbound * 85.53/93.41 80.55/90.90 85.44/93.04 85.61/93.21 88.46 80.35 7.41 3.70\nTable2.ComparisonofmethodsontheFundusdataset.#Lindicatesthenumberoflabeledsamples.Inthe”Upperbound”row,*denotes\nusingalltrainingsamplesinadomainaslabeleddata.Anupwardarrow(↑)signifiesthathighervaluesindicatebetterperformance,while\nadownwardarrow(↓)indicatestheopposite.Thebestresultsarebolded,withthesecond-bestunderlined.\nTask ProstateSegmentation\nDC↑ DC↑ JC↑ HD↓ ASD↓\nMethod #L\nRUNMC BMC HCRUDB UCL BIDMC HK Avg. Avg. Avg. Avg.\nU-Net 40 31.11 35.07 20.04 38.18 19.41 26.62 28.41 23.24 95.11 65.84\nUA-MT MICCAI’19 40 29.44 4.68 12.49 39.42 17.94 18.22 20.37 14.88 112.07 77.58\nFDA CVPR’20 40 47.44 35.37 24.54 61.01 28.19 40.51 39.51 32.17 76.67 47.87\nSIFA TMI’20 40 72.67 70.37 64.08 73.49 71.62 65.16 69.57 56.78 29.43 13.03\nFixMatch NeurIPS’20 40 83.58 69.17 73.63 79.21 56.07 84.78 74.41 65.96 24.18 14.09\nCPS CVPR’21 40 29.83 9.21 11.84 43.84 13.51 14.56 20.47 15.12 115.96 78.51\nCoraNet TMI’21 40 69.43 31.16 16.29 69.33 24.66 22.16 38.84 31.48 67.91 44.98\nUDA-VAE++ CVPR’22 40 68.73 69.36 65.49 67.19 63.29 65.15 66.54 52.80 34.20 15.48\nSS-Net MICCAI’22 40 29.10 13.49 14.20 51.96 23.83 13.23 24.30 18.74 109.54 71.13\nBCP CVPR’23 40 70.15 71.97 46.15 58.93 74.21 67.47 64.81 55.17 52.60 27.22\nCauSSL ICCV’23 40 24.10 27.46 16.94 27.23 15.28 14.56 20.93 15.48 114.62 73.30\nMiDSS CVPR’24 40 87.94 85.30 77.74 86.29 88.54 86.43 85.37 77.30 13.44 6.18\nDiM 40 88.43 85.67 87.56 87.27 88.23 87.55 87.45 79.52 10.57 4.35\nUpperbound * 88.52 88.61 85.71 88.61 88.98 89.49 88.32 80.71 10.05 4.12\nTable3.ComparisonofdifferentmethodsonProstatedataset.\nTask OpticCup/DiscSegmentation 4.5.VisualizationAnalysis\nMethod Baseline SAM f-Divergence DC↑ JC↑ HD↓ ASD↓\n#1 ✓ 88.27 80.02 7.19 3.48\n#2 ✓ ✓ 88.33 80.22 7.10 3.52 Domain 1\n#3 ✓ ✓ 88.32 80.14 7.22 3.47 Domain 2\n#4 ✓ ✓ ✓ 88.80 80.74 6.81 3.27 D Do om ma ai in n 3 4\nTable4.Ablationexperimentsacrossdomain1onFundus\ndataset.\nDomain 1\nDomain 2\nDomain 3\ngencestrategiesperformwell,oftenyieldinghigherDCand Domain 4\nJCwhilereducingHDandASD.Specifically,JSdivergence (a)MiDSS (b) DiM\ntendstoprovidemoreconsistentresultsinedgecases,asev-\nFigure1.AT-snevisualizationanalysiswasperformedonthe\nidencedbylowerHDandASDvalues,whilealsoachieving\nFundusdatasetexperiment.\ncompetitivesegmentationaccuracy. Pearsondivergence,on\nthe other hand, exhibits a balanced performance, particu- T-SNE visualization. We adopt the T-SNE visualization\nlarlyexcellingasthesecond-bestinsomemetrics. method , which graphically represents the learning rep-\nTask OpticCup/DiscSegmentation\nDC↑ JC↑ HD↓ ASD↓\nType Domain\nCup Disc Cup Disc Cup Disc Cup Disc\nbaseline 83.38 93.15 72.68 87.36 8.28 6.10 4.01 2.95\nJS 84.68 92.91 74.51 86.96 7.63 5.98 3.56 2.97\nDomain1\nJeffrey 82.82 92.57 72.07 86.45 8.20 6.21 4.13 3.20\nPearson 83.54 92.94 73.15 87.07 7.91 6.23 3.94 3.09\nbaseline 73.11 88.88 59.76 80.78 12.89 13.69 6.56 6.11\nJS 78.10 89.32 65.54 81.28 10.71 13.72 5.29 6.21\nDomain2\nJeffrey 78.16 90.49 65.44 82.99 9.62 8.24 4.83 4.37\nPearson 78.50 90.07 66.29 82.41 10.62 10.67 5.19 5.11\nbaseline 83.49 93.36 72.77 87.82 8.06 6.19 3.89 3.15\nJS 84.82 93.36 74.66 87.53 7.56 6.24 3.62 3.15\nDomain3\nJeffrey 83.58 93.12 72.71 87.40 8.78 6.23 4.00 3.17\nPearson 83.03 93.14 72.02 87.43 9.08 6.23 4.20 3.19\nbaseline 78.63 93.56 66.27 88.16 10.95 6.28 5.44 3.06\nJS 81.63 92.18 70.23 85.87 9.32 8.01 4.42 3.87\nDomain4\nJeffrey 79.24 93.31 66.86 87.74 10.48 6.15 5.00 3.16\nPearson 80.03 93.12 67.92 87.43 10.51 6.31 4.93 3.28\n\u00008\u0000\u0010\u00001\u0000H\u0000W \u00008\u0000$\u0000\u0010\u00000\u00007 \u00006\u00006\u0000\u0010\u00001\u0000H\u0000W \u0000%\u0000&\u00003 \u0000&\u0000D\u0000X\u00006\u00006\u0000/ \u00000\u0000L\u0000'\u00006\u00006\nTable5.Performancecomparisonofdifferentf-divergencestrategiesacrossfourdomainsonFundusdataset.Metricsmarkedwith↑\nindicatethathighervaluesimplybetterperformance,whilethosewith↓suggesttheopposite.Thebestperformanceresultsare\nhighlightedinbold,andthesecond-bestareunderlined.\n\u00008\u0000\u0010\u00001\u0000H\u0000W \u00008\u0000$\u0000\u0010\u00000\u00007 \u00006\u00006\u0000\u0010\u00001\u0000H\u0000W \u0000%\u0000&\u00003 \u0000&\u0000D\u0000X\u00006\u00006\u0000/ \u00000\u0000L\u0000'\u00006\u00006 \u0000'\u0000L\u00000\u0000\u000b\u00002\u0000X\u0000U\u0000V\u0000\f \u0000*\u00007\nFigure2.VisualcomparisonofsegmentationresultsonFundusdatasetacrossdifferentmodels.RedandgreenrepresenttheOpticalCup\nandDisc,respectively.Thefirstrowshowssegmentationresultsontestsamplesfromthelabeleddomain(Domain1),whilethesecond\nrowpresentsresultsontestsamplesfromadifferentdomain(Domain4).\nresentation obtained from our method, as shown in Fig- withmorepreciseedgedetailretention, andthesegmenta-\nure.1(a)and 1(b). DiMsignificantlyimprovesthefeature tionwithintheopticdiscandcupregionsismorecohesive,\nalignmentbetweendifferentdomainscomparedtoMiDSS, closelymatchingthegroundtruth(GT).\nresultinginatighterandmorecoherentdatadistribution. We also conducted visualization experiments on the\nSegmentation visualization. As shown in Figure 2 , prostate segmentation task in Figure 3. Results indicate\nDiM achieves higher accuracy and better preservation of that DiM continues to outperform other methods, such as\nedgedetailsinopticdiscandcupsegmentationtaskscom- BCP,CauSSL,andMiDSS,byachievingclearerboundary\npared to other methods. Models such as U-Net, UA-MT, delineation and better edge detail preservation. The seg-\nand SS-Net show noticeable boundary blurring, with seg- mentationcloselymatchestheGT,demonstratingimproved\nmentation results that lack precision, particularly at the accuracy and cohesion within the prostate region, even in\nstructuralboundariesoftheopticdiscandcup. Thesemod- challengingboundaryareas.\nelstendtoeithermisssegmentsorover-segmentcertainre- ModelPerformance. ThevalidationlossandDicecoeffi-\ngions. In contrast, our model produces clearer boundaries cientcurvesacrossthefourdomainsontheFundusdataset\n\u00008\u0000\u0010\u00001\u0000H\u0000W \u00008\u0000$\u0000\u0010\u00000\u00007 \u00006\u00006\u0000\u0010\u00001\u0000H\u0000W \u0000%\u0000&\u00003 \u0000&\u0000D\u0000X\u00006\u00006\u0000/ \u00000\u0000L\u0000'\u00006\u00006\n\u00008\u0000\u0010\u00001\u0000H\u0000W \u00008\u0000$\u0000\u0010\u00000\u00007 \u00006\u00006\u0000\u0010\u00001\u0000H\u0000W \u0000%\u0000&\u00003 \u0000&\u0000D\u0000X\u00006\u00006\u0000/ \u00000\u0000L\u0000'\u00006\u00006 \u0000\u000b\u0000'\u0000L\u00000\u0000\f\u00002\u0000X\u0000U\u0000V \u0000*\u00007\nFigure3.VisualresultsonProstatedataset.\n\u0000\u0014\u0000\u0011\u0000\u0013 \u0000\u0014\u0000\u0011\u0000\u0013 \u0000\u0014\u0000\u0011\u0000\u0013 \u0000\u0014\u0000\u0011\u0000\u0013\n\u0000\u0013\u0000\u0011\u0000\u001b \u0000/\u0000R\u0000V\u0000V \u0000\u0013\u0000\u0011\u0000\u001b \u0000/\u0000R\u0000V\u0000V \u0000\u0013\u0000\u0011\u0000\u001b \u0000/\u0000R\u0000V\u0000V \u0000\u0013\u0000\u0011\u0000\u001b \u0000/\u0000R\u0000V\u0000V\n\u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019\n\u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0013\u0000\u0011\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u0015 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000\u0013\u0000\u0011\u0000\u0015\n\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013 \u0000\u0014\u0000\u0013 \u0000\u0015\u0000\u0013 \u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013 \u0000\u0018\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013 \u0000\u0014\u0000\u0013 \u0000\u0015\u0000\u0013 \u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013 \u0000\u0018\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013 \u0000\u0014\u0000\u0013 \u0000\u0015\u0000\u0013 \u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013 \u0000\u0018\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013 \u0000\u0014\u0000\u0013 \u0000\u0015\u0000\u0013 \u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013 \u0000\u0018\u0000\u0013\n\u0000(\u0000S\u0000R\u0000F\u0000K \u0000(\u0000S\u0000R\u0000F\u0000K \u0000(\u0000S\u0000R\u0000F\u0000K \u0000(\u0000S\u0000R\u0000F\u0000K\n(a)Domain1Loss (b)Domain2Loss (c)Domain3Loss (d)Domain4Loss\n\u0000\u0014\u0000\u0011\u0000\u0013 \u0000\u0014\u0000\u0011\u0000\u0013 \u0000\u0014\u0000\u0011\u0000\u0013 \u0000\u0014\u0000\u0011\u0000\u0013\n\u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001b\n\u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019\n\u0000&\u0000X\u0000S\u0000\u0003\u0000'\u0000L\u0000F\u0000H \u0000&\u0000X\u0000S\u0000\u0003\u0000'\u0000L\u0000F\u0000H \u0000&\u0000X\u0000S\u0000\u0003\u0000'\u0000L\u0000F\u0000H \u0000&\u0000X\u0000S\u0000\u0003\u0000'\u0000L\u0000F\u0000H\n\u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0013\u0000\u0011\u0000\u0017\n\u0000'\u0000L\u0000V\u0000F\u0000\u0003\u0000'\u0000L\u0000F\u0000H \u0000'\u0000L\u0000V\u0000F\u0000\u0003\u0000'\u0000L\u0000F\u0000H \u0000'\u0000L\u0000V\u0000F\u0000\u0003\u0000'\u0000L\u0000F\u0000H \u0000'\u0000L\u0000V\u0000F\u0000\u0003\u0000'\u0000L\u0000F\u0000H\n\u0000\u0013\u0000\u0011\u0000\u0015 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000\u0013\u0000\u0011\u0000\u0015\n\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013 \u0000\u0014\u0000\u0013 \u0000\u0015\u0000\u0013 \u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013 \u0000\u0018\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013 \u0000\u0014\u0000\u0013 \u0000\u0015\u0000\u0013 \u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013 \u0000\u0018\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013 \u0000\u0014\u0000\u0013 \u0000\u0015\u0000\u0013 \u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013 \u0000\u0018\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013 \u0000\u0014\u0000\u0013 \u0000\u0015\u0000\u0013 \u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013 \u0000\u0018\u0000\u0013\n\u0000(\u0000S\u0000R\u0000F\u0000K \u0000(\u0000S\u0000R\u0000F\u0000K \u0000(\u0000S\u0000R\u0000F\u0000K \u0000(\u0000S\u0000R\u0000F\u0000K\n(e)Domain1Dice (f)Domain2Dice (g)Domain3Dice (h)Domain4Dice\nFigure4.ValidationLossandDiceacrossfourdomainsonFundusdataset.\ndemonstratestablemodelperformance,whicharedepicted \u0000\u0014\u0000\u0011\u0000\u0013\ninFigure5. Lossdecreasesrapidlyintheearlyepochsand \u0000\u0013\u0000\u0011\u0000\u001b \u0000/\u0000R\u0000V\u0000V\nconvergesacrossalldomains, indicatingeffectivetraining. \u0000\u0013\u0000\u0011\u0000\u0019\nDicescoresforboththeopticcupanddiscsteadilyincrease \u0000\u0013\u0000\u0011\u0000\u0017\nand plateau at high values, with the optic disc achieving \u0000\u0013\u0000\u0011\u0000\u0015\nnear-perfectaccuracy. \u0000\u0013\u0000\u0011\u0000\u0013\n\u0000\u0013 \u0000\u0015\u0000\u0013 \u0000\u0017\u0000\u0013 \u0000\u0019\u0000\u0013 \u0000\u001b\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0015\u0000\u0013 \u0000\u0014\u0000\u0017\u0000\u0013 \u0000\u0014\u0000\u0019\u0000\u0013 \u0000\u0014\u0000\u001b\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0015\u0000\u0013\n\u0000(\u0000S\u0000R\u0000F\u0000K\n5.Conclusion\n(a)Domain1Loss\n\u0000\u0014\u0000\u0011\u0000\u0013\nThis study addresses the challenge of data annotation in\n\u0000\u0013\u0000\u0011\u0000\u001b\nmedical image segmentation by introducing a sharpness-\n\u0000\u0013\u0000\u0011\u0000\u0019\naware optimization method based on f-divergence min-\n\u0000\u0013\u0000\u0011\u0000\u0017\nimization (DiM) for semi-supervised learning. While\n\u0000'\u0000L\u0000F\u0000H\n\u0000\u0013\u0000\u0011\u0000\u0015\nexisting semi-supervised methods (SSMIS), including\n\u0000\u0013\u0000\u0011\u0000\u0013\nsharpness-aware optimization (SAM), have shown suc- \u0000\u0013 \u0000\u0015\u0000\u0013 \u0000\u0017\u0000\u0013 \u0000\u0019\u0000\u0013 \u0000\u001b\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0015\u0000\u0013 \u0000\u0014\u0000\u0017\u0000\u0013 \u0000\u0014\u0000\u0019\u0000\u0013 \u0000\u0014\u0000\u001b\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0015\u0000\u0013\n\u0000(\u0000S\u0000R\u0000F\u0000K\ncess, they often overlook distribution differences between\ndatasets. The proposed DiM method enhances model sta- (b)Domain1Dice\nbility by adjusting the sensitivity of model parameters and Figure5.ValidationLossandDiceforDomain1ontheProstate\nimproves adaptability to varying datasets. By reducing f- dataset.\ndivergence, DiM achieves a better balance in performance\nbetween source and target datasets and mitigates overfit- breaking progress in Dice scores on the prostate dataset,\nting. Experimental results demonstrate that DiM signifi- withsimilarsuccessacrossthreepublicdatasets.\ncantly improves performance, as evidenced by its ground-\n\u0000\u0014\u0000\u0003\u0000Q\u0000L\u0000D\u0000P\u0000R\u0000'\n\u0000\u0014\u0000\u0003\u0000Q\u0000L\u0000D\u0000P\u0000R\u0000'\n\u0000V\u0000V\u0000R\u0000/\n\u0000H\u0000U\u0000R\u0000F\u00006\u0000\u0003\u0000H\u0000F\u0000L\u0000'\nReferences [16] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and\nMichael I Jordan. Conditional adversarial domain adapta-\n[1] MaksymAndriushchenkoandNicolasFlammarion.Towards\ntion. NeurIPS,31,2018. 2\nunderstanding sharpness-aware minimization. In ICML,\n[17] Changjie Lu, Shen Zheng, and Gaurav Gupta. Unsuper-\npages639–668.PMLR,2022. 3\nviseddomainadaptationforcardiacsegmentation: Towards\n[2] Yunhao Bai, Duowen Chen, Qingli Li, Wei Shen, and Yan structuremutualinformationmaximization.InCVPR,pages\nWang.Bidirectionalcopy-pasteforsemi-supervisedmedical 2588–2597,2022. 5\nimagesegmentation.InCVPR,pages11514–11524,2023.5\n[18] Liyun Lu, Mengxiao Yin, Liyao Fu, and Feng Yang.\n[3] Gerda Bortsova, Florian Dubost, Laurens Hogeweg, Ioan- Uncertainty-aware pseudo-label and consistency for semi-\nnisKatramados,andMarleenDeBruijne. Semi-supervised supervisedmedicalimagesegmentation. BiomedicalSignal\nmedical image segmentation via learning consistency un- ProcessingandControl,79:104203,2023. 2\ndertransformations. InMICCAI,pages810–818.Springer, [19] Xiangde Luo, Jieneng Chen, Tao Song, and Guotai Wang.\n2019. 1,2 Semi-supervisedmedicalimagesegmentationthroughdual-\n[4] ChengChen,QiDou,HaoChen,JingQin,andPhengAnn taskconsistency. InAAAI,pages8801–8809,2021. 2\nHeng.Unsupervisedbidirectionalcross-modalityadaptation [20] Xiangde Luo, Minhao Hu, Tao Song, Guotai Wang, and\nviadeeplysynergisticimageandfeaturealignmentformed- Shaoting Zhang. Semi-supervised medical image segmen-\nicalimagesegmentation. TMI,39(7):2494–2505,2020. 5 tationviacrossteachingbetweencnnandtransformer.InIn-\n[5] Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong ternationalconferenceonmedicalimagingwithdeeplearn-\nWang. Semi-supervised semantic segmentation with cross ing,pages820–833.PMLR,2022. 2\npseudosupervision. InCVPR,pages2613–2622,2021. 5 [21] QingheMa,JianZhang,LeiQi,QianYu,YinghuanShi,and\n[6] DansongCheng,FengTian,LinLiu,XiaofangLiu,andYe YangGao.Constructingandexploringintermediatedomains\nJin. Imagesegmentationbasedonmulti-regionmulti-scale inmixeddomainsemi-supervisedmedicalimagesegmenta-\nlocalbinaryfittingandkullback–leiblerdivergence. Signal, tion. InCVPR,pages11642–11651,2024. 1,5\nImageandVideoProcessing,12:895–903,2018. 2 [22] QingheMa,JianZhang,LeiQi,QianYu,YinghuanShi,and\n[7] SabyasachiDash,SushilKumarShakyawar,MohitSharma, YangGao.Constructingandexploringintermediatedomains\nandSandeepKaushik. Bigdatainhealthcare: management, inmixeddomainsemi-supervisedmedicalimagesegmenta-\nanalysisandfutureprospects. JournalofBigData,6(1):1– tion. InCVPR,pages11642–11651,2024. 2\n25,2019. 1 [23] JuzhengMiao,ChengChen,FuruiLiu,HaoWei,andPheng-\nAnn Heng. Caussl: Causality-inspired semi-supervised\n[8] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam\nlearning for medical image segmentation. In ICCV, pages\nNeyshabur. Sharpness-aware minimization for efficiently\n21426–21437,2023. 1,2,5\nimprovinggeneralization.arXivpreprintarXiv:2010.01412,\n2020. 1,3 [24] OlafRonneberger,PhilippFischer,andThomasBrox.U-net:\nConvolutionalnetworksforbiomedicalimagesegmentation.\n[9] YaroslavGaninandVictorLempitsky.Unsuperviseddomain\nInMICCAI,pages234–241.Springer,2015. 1,5\nadaptationbybackpropagation. InICML,pages1180–1189.\n[25] VivekARudrapatna,AtulJButte,etal. Opportunitiesand\nPMLR,2015. 2\nchallengesinusingreal-worlddataforhealthcare.TheJour-\n[10] JoumanaGhosnandYoshuaBengio. Biaslearning,knowl-\nnalofClinicalInvestigation,130(2):565–574,2020. 1\nedgesharing. TNN,14(4):748–765,2003. 2\n[26] KuniakiSaito,DonghyunKim,StanSclaroff,TrevorDarrell,\n[11] HaoGuanandMingxiaLiu. Domainadaptationformedical\nand Kate Saenko. Semi-supervised domain adaptation via\nimageanalysis:asurvey. IEEETransactionsonBiomedical\nminimaxentropy. InICCV,pages8050–8058,2019. 2\nEngineering,69(3):1173–1185,2021. 1\n[27] KuniakiSaito,YoshitakaUshiku,TatsuyaHarada,andKate\n[12] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Saenko.Strong-weakdistributionalignmentforadaptiveob-\nDeep residual learning for image recognition. In CVPR, jectdetection. InCVPR,pages6956–6965,2019. 2\npages770–778,2016. 5\n[28] Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain,\n[13] Xiaomeng Li, Hao Chen, Xiaojuan Qi, Qi Dou, Chi-Wing SerNamLim,andRamaChellappa.Learningfromsynthetic\nFu,andPheng-AnnHeng.H-denseunet:hybriddenselycon- data:Addressingdomainshiftforsemanticsegmentation.In\nnected unet for liver and tumor segmentation from ct vol- CVPR,pages3752–3761,2018. 2\numes. TMI,37(12):2663–2674,2018. 1 [29] Yinghuan Shi, Jian Zhang, Tong Ling, Jiwen Lu, Yefeng\n[14] QuandeLiu,ChengChen,JingQin,QiDou,andPheng-Ann Zheng,QianYu,LeiQi,andYangGao.Inconsistency-aware\nHeng. Feddg: Federateddomaingeneralizationonmedical uncertainty estimation for semi-supervised medical image\nimagesegmentationviaepisodiclearningincontinuousfre- segmentation. TMI,41(3):608–620,2021. 5\nquencyspace. InCVPR,pages1013–1023,2021. 1 [30] RuiShu,HungHBui,HirokazuNarui,andStefanoErmon.\n[15] Xiaofeng Liu, Chaehwa Yoo, Fangxu Xing, Hyejin Oh, Adirt-tapproachtounsuperviseddomainadaptation. arXiv\nGeorgesElFakhri,Je-WonKang,JonghyeWoo,etal. Deep preprintarXiv:1802.08735,2018. 2\nunsupervised domain adaptation: A review of recent ad- [31] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao\nvancesandperspectives.APSIPATransactionsonSignaland Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,\nInformationProcessing,11(1),2022. 2 AlexeyKurakin,andChun-LiangLi.Fixmatch:Simplifying\nsemi-supervised learning with consistency and confidence. [47] Zhi-Hua Zhou. A brief introduction to weakly supervised\nNeurIPS,33:596–608,2020. 5 learning. NationalScienceReview,5(1):44–53,2018. 1\n[32] TimVanErvenandPeterHarremos. Re´nyidivergenceand [48] Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui,\nkullback-leiblerdivergence. IEEETransactionsonInforma- Hartwig Adam, Nicha Dvornek, Sekhar Tatikonda, James\ntionTheory,60(7):3797–3820,2014. 2 Duncan,andTingLiu.Surrogategapminimizationimproves\n[33] KaipingWang,BoZhan,ChenZu,XiWu,JiliuZhou,Lup- sharpness-awaretraining. arXivpreprintarXiv:2203.08065,\ningZhou, andYanWang. Semi-supervisedmedicalimage 2022. 2\nsegmentationviaatripled-uncertaintyguidedmeanteacher [49] XiahaiZhuangetal. Challengesandmethodologiesoffully\nmodel with contrastive learning. Medical Image Analysis, automatic whole heart segmentation: a review. Journal of\n79:102447,2022. 1 HealthcareEngineering,4:371–407,2013. 2\n[34] MeiWangandWeihongDeng. Deepvisualdomainadapta-\ntion:Asurvey. Neurocomputing,312:135–153,2018. 2\n[35] Qin Wang, Wen Li, and Luc Van Gool. Semi-supervised\nlearning by augmented distribution alignment. In ICCV,\npages1466–1475,2019. 1\n[36] Xin Wang, Xiaoyu Liu, Peng Huang, Pu Huang, Shu\nHu, and Hongtu Zhu. U-medsam: Uncertainty-aware\nmedsam for medical image segmentation. arXiv preprint\narXiv:2408.08881,2024. 3\n[37] YiqingWang,ZihanLi,JieruMei,ZihaoWei,LiLiu,Chen\nWang, Shengtian Sang, Alan L Yuille, Cihang Xie, and\nYuyinZhou. Swinmm: maskedmulti-viewwithswintrans-\nformers for 3d medical image segmentation. In MICCAI,\npages486–496,2023. 2\n[38] KarlWeiss,TaghiMKhoshgoftaar,andDingDingWang. A\nsurvey of transfer learning. Journal of Big Data, 3:1–40,\n2016. 2\n[39] YichengWu, ZongyuanGe, DonghaoZhang, MinfengXu,\nLeiZhang, YongXia, andJianfeiCai. Mutualconsistency\nlearning for semi-supervised medical image segmentation.\nMedicalImageAnalysis,81:102530,2022. 2\n[40] YichengWu,ZhonghuaWu,QianyiWu,ZongyuanGe,and\nJianfeiCai. Exploringsmoothnessandclass-separationfor\nsemi-supervisedmedicalimagesegmentation. InMICCAI,\npages34–43.Springer,2022. 5\n[41] Yanchao Yang and Stefano Soatto. Fda: Fourier domain\nadaptation for semantic segmentation. In CVPR, pages\n4085–4095,2020. 5\n[42] LequanYu,ShujunWang,XiaomengLi,Chi-WingFu,and\nPheng-AnnHeng.Uncertainty-awareself-ensemblingmodel\nforsemi-supervised3dleftatriumsegmentation.InMICCAI,\npages605–613.Springer,2019. 5\n[43] Yifan Zhang, Ying Wei, Qingyao Wu, Peilin Zhao,\nShuaichengNiu,JunzhouHuang,andMingkuiTan. Collab-\norative unsupervised domain adaptation for medical image\ndiagnosis. TIP,29:7834–7844,2020. 1\n[44] ZiyuanZhao,FangchengZhou,KaixinXu,ZengZeng,Cun-\ntaiGuan,andSKevinZhou. Le-uda: Label-efficientunsu-\nperviseddomainadaptationformedicalimagesegmentation.\nTMI,42(3):633–646,2022. 1\n[45] Yixuan Zhou, Yi Qu, Xing Xu, and Hengtao Shen. Imb-\nsam: A closer look at sharpness-aware minimization in\nclass-imbalancedrecognition.InICCV,pages11345–11355,\n2023. 3\n[46] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima\nTajbakhsh,andJianmingLiang. Unet++: Redesigningskip\nconnectionstoexploitmultiscalefeaturesinimagesegmen-\ntation. TMI,39(6):1856–1867,2019. 1",
    "pdf_filename": "DiM_$f$-Divergence_Minimization_Guided_Sharpness-Aware_Optimization_for_Semi-supervised_Medical_Imag.pdf"
}