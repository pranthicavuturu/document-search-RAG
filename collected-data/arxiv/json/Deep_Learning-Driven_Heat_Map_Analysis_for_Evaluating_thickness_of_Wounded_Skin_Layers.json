{
    "title": "Deep Learning-Driven Heat Map Analysis for Evaluating thickness of Wounded Skin Layers",
    "abstract": "Understanding the appropriate skin layer thickness in wounded sites is an important tool to move forward on wound healing practices and treatment protocols. Methods to measure depth often are invasive and less specific. This paper introduces a novel method that is non-invasive with deep learning techniques using classifying of skin layers that helps in measurement of wound depth through heatmap analysis. A set of approximately 200 labeled images of skin allows five classes to be distinguished: scars, wounds, and healthy skin, among others. Each image has annotated key layers, namely the stratum cornetum, the epidermis, and the dermis, in the software Roboflow. In the preliminary stage, the Heatmap generator VGG16 was used to enhance the visibility of tissue layers, based upon which their annotated images were used to train ResNet18 with early stopping techniques. It ended up at a very high accuracy rate of 97.67%. To do this, the comparison of the models ResNet18, VGG16, DenseNet121, and EfficientNet has been done where both EfficientNet and ResNet18 have attained accuracy rates of almost 95.35%. For further hyperparameter tuning, EfficientNet and ResNet18 were trained at six different learning rates to determine the best model configuration. It has been noted that the accuracy has huge variations with different learning rates. In the case of EfficientNet, the maximum achievable accuracy was 95.35% at the rate of 0.0001. The same was true for ResNet18, which also attained its peak value of 95.35% at the same rate. These facts indicate that the model can be applied and utilized in actual-time, non-invasive wound assessment, which holds a great promise to improve clinical diagnosis and treatment planning.",
    "body": "1 \nDeep Learning-Driven Heat Map Analysis for Evaluating thickness of Wounded Skin Layers \nDevakumar GR, JB Kaarthikeyan, Dominic Immanuel T, Sheena Christabel Pravin*  \nSchool of Electronics Engineering, \nVellore Institute of Technology, Chennai. \n*sheenachristabel.p@vit.ac.in \n \nAbstract \n \nUnderstanding the appropriate skin layer thickness in wounded sites is an important tool to move \nforward on wound healing practices and treatment protocols. Methods to measure depth often are \ninvasive and less specific. This paper introduces a novel method that is non-invasive with deep \nlearning techniques using classifying of skin layers that helps in measurement of wound depth \nthrough heatmap analysis. A set of approximately 200 labeled images of skin allows five classes \nto be distinguished: scars, wounds, and healthy skin, among others. Each image has annotated key \nlayers, namely the stratum cornetum, the epidermis, and the dermis, in the software Roboflow. In \nthe preliminary stage, the Heatmap generator VGG16 was used to enhance the visibility of tissue \nlayers, based upon which their annotated images were used to train ResNet18 with early stopping \ntechniques. It ended up at a very high accuracy rate of 97.67%. To do this, the comparison of the \nmodels ResNet18, VGG16, DenseNet121, and EfficientNet has been done where both EfficientNet \nand ResNet18 have attained accuracy rates of almost 95.35%. \nFor further hyperparameter tuning, EfficientNet and ResNet18 were trained at six different \nlearning rates to determine the best model configuration. It has been noted that the accuracy has \nhuge variations with different learning rates. In the case of EfficientNet, the maximum achievable \naccuracy was 95.35% at the rate of 0.0001. The same was true for ResNet18, which also attained \nits peak value of 95.35% at the same rate. These facts indicate that the model can be applied and \nutilized in actual-time, non-invasive wound assessment, which holds a great promise to improve \nclinical diagnosis and treatment planning. \n \n1. Introduction \n \nIt has emerged as a huge phenomenon of deep learning for medical image analysis, especially in \nwound analysis and tissue monitoring, with non-invasive accuracy and interpretability that makes \napproaches in clinical applications critical. Traditional methods involved have always been \ninvasive risks while it becomes highly impractical, such as real-time and individualized care in \nsensitive cases, like post-surgical monitoring, burn care, diabetic wound management, and \nrecovery after cancer treatment [1][2]. Continuous monitoring of wound healing with minimal \npatient discomfort has become a crying necessity henceforth, and especially in cases that demand \ndetailed skin layer analysis, as seen in reconstructive surgeries related to breast cancer patients \nwhere healing dynamics could be extremely complex and therefore must be monitored accurately. \n \n\n2 \nIn the following paper, we discuss how all these problems are overcome with a deep learning \npipeline. We used a dataset of 200 annotated skin tissue images into five different categories: \n“scar1”, “scar2”, “wound”, “healthy_men”, and “healthy_women”. VGG16 would be applied \nto create heatmaps of the OCT images of skin layers that have improved interpretability, with the \nmost important layers of the skin being lit. These heatmaps are then manually annotated into three \nlayers of human skin: stratum corneum, epidermis, and dermis, which we did using Roboflow. We \nfirst trained a ResNet18 model to classify these annotated layers, early-stopped to prevent \novertraining, which actually culminated with an impressive accuracy of 97.67%, and the \nclassification report illustrates high precision and recall across all classes. \n \nFurther, we performed an ablation study by making early stopping absent in ResNet18, VGG16, \nDenseNet121, and EfficientNet models. The results obtained show that optimal accuracies were \nreached with both the ResNet18 and EfficientNet models at 95.35%, while that of DenseNet121 \nwas at 90.70%, and that of VGG16 only at 72.09% accuracy. This therefore proves the strength of \nResNet18 in our task. Finally, for optimizing the two models, EfficientNet and ResNet18, we \nselected six values of learning rates; we concluded that for both models, 0.0001 would be the best \nlearning rate that could sustain high accuracies ranging to 95%. \n \nIn recent years, several studies have surfaced toward potential applicability and challenges in the \napplication of deep learning in wound analysis. For instance, Shenoy et al. [3] applied ensemble \nCNNs in the DeepWound model in monitoring postoperative wounds using images captured by \nsmartphones while attaining good accuracy despite the limitations of the dataset. He et al. [4] \ncombined a traditional with a LayerCAM-based approach to analyze the collagen fibers in the \nhistological wound images, which further rendered interpretability but was limited by a small \ndataset. Blanco et al. [5] proposed a superpixel-driven ResNet model for dermatological ulcers \nwith excellent accuracy but bears a high computational requirement. Zhang et al. [6] and Siregar \net al. [7] also proved the multifaceted and fine resolutions of deep learning but limitations in \ncomputation resources and dataset sizes. Zhao and Chen [8] studied hydrogel-based wound healing \nusing Optical Coherence Tomography, and Neto et al. [9] developed a predictive model for wound \nstate with very cheap sensors—high promise for real-time clinical applicability. \n \nThis approach leverages deep learning models, coupled with the mechanisms of heatmap \ninterpretability along with a robust classification mechanism, which overcomes shortcomings in \nproviding an accurate clinical tool for wound analysis. It is a pipeline that integrates CNNs and \nheatmaps outlining why interpretability in medical AI and non-invasive evaluation are critically \nimportant for widespread clinical adoption. \n \n \n \n \n\n3 \n2. Related work \nSignificant advancements in wound assessment and monitoring using deep learning have \ndemonstrated the potential of AI-driven techniques in medical diagnostics. Shenoy et al. (2018) \nintroduced \"DeepWound,\" an automated wound assessment tool leveraging convolutional neural \nnetworks (CNNs) and VGG16-based models to classify postoperative wounds. DeepWound \nachieved high accuracy using CLAHE preprocessing and data augmentation, and a mobile app \n(Theia) facilitated real-time monitoring. However, this approach faced challenges with privacy \nand dataset limitations [1]. \nHe et al. (2020) focused on wound healing progress by analyzing collagen fiber regions in \nhistological images using a fine-tuned VGG16 model with LayerCAM interpretability. This \napproach successfully combined deep learning with traditional statistical analyses, achieving 82% \naccuracy in wound stage classification. However, the complexity of LayerCAM limited \ngeneralizability due to a small dataset size [2]. \nBlanco et al. (2019) developed QTDU, a ResNet-based superpixel-driven approach for \ndermatological wounds, which segmented and quantified ulcer areas with an AUC of 0.986. \nQTDU's precise tissue quantification demonstrates its potential in dermatological wound analysis, \nthough its reliance on expert-labeled images limits scalability [3]. \nZhang et al. (2022) conducted a comprehensive survey on wound image analysis, implementing \nResNet101 with a learning rate scheduler on a breast cancer wound dataset. This study achieved \nan accuracy of 97% but noted challenges like computational costs and model overfitting [4]. \nSiregar et al. (2024) used U-Net and Mask R-CNN models to measure wound size in zebrafish, an \nethical and cost-effective alternative to rodent models. However, the transition to mammalian \nmodels posed challenges [5]. Zhao and Chen (2023) integrated hydrogels in skin wound healing, \nusing Optical Coherence Tomography (OCT) and deep learning to assess hydrogel performance. \nAlthough effective in wound healing, hydrogel preparation complexity and cytotoxicity remain \nlimitations [6]. \nLastly, Ribeiro Neto (2021) presented a cost-effective, sensor-based wound healing tracking \nmodel, achieving an 85.7% accuracy rate. The model demonstrated potential for real-time \nmonitoring, although further refinement is needed for clinical applications [7]. \nBuilding on these approaches, our study integrates heatmap-based VGG16 for image \npreprocessing and ResNet18 and EfficientNet for classification, employing early stopping and \nlearning rate optimization to improve accuracy and reduce overfitting. This model not only \nachieves high accuracy in wound classification and skin layer analysis but also provides a scalable \nand interpretable tool for real-time wound monitoring, addressing limitations in previous studies \nregarding data availability, computational efficiency, and generalizability. \n\n4 \n3. Proposed Framework \n \nThis proposed framework initiated with about 200 images of skin tissue, each belonging to five \nclasses: “scar1”, “scar2”, “wound”, “healthy_men”, and “healthy_women”. Better vision for \nthose features important for classification is achievable through using VGG16 first to transform \nthe images into heatmaps. The heatmaps provide detailed visualizations of skin tissue variations, \nmaking it clearer to identify tissue structures for further analysis. \n \nThe images were then imported into Roboflow for manual annotation on three important layers of \nthe skin, namely stratum cornetum and epidermis and dermis.  \n \n \n \n \nFig. 1: Generated Heatmap and Annotations of the different layers \nThe annotations were necessary to take apart small structural differences in the tissue needed to \nraise the accuracy of classification for the model. The dataset itself was well-structured under \nseparate folders for images and labels of the YOLOv8 format. \n\n5 \n \nDifferent architectures of deep learning for model development, such as VGG16, DenseNet121, \nand EfficientNet have been researched. Comparatively, the best model to apply to this problem is \nthe ResNet18 model, which has a unique residual learning mechanism to overcome the problems \nresulting from vanishing gradients, a very common problem in deep neural networks. Skip \nconnections\" or shortcuts between layers are the crux of residual networks. This facilitates passing \nthe input of one layer directly to the output of a deeper layer so that the network may learn more \ncomplex representations without degradation of performance. \n \nMathematically, the residual learning framework can be written as follows: \n \n 𝑦 =  𝐹(𝑥, {𝑊𝑖}) +  𝑥 \n \nwhere x represents the input, F the residual mapping function that was learned by the layer and Wi \nthe weights in the layer. This helps the network learn the residual function instead of trying directly \nto learn some complicated transformation, thereby making training for deep networks more \nefficient. \n \nResNet18 is composed of 18 layers arranged in blocks, where each block contains two \nconvolutional layers with batch normalization and ReLU activation between them and followed \nby a skip connection. It's this architecture that enables the preservation of integrity as well as \nfeatures throughout the layers. So, it would make it possible to have more stable and efficient \ntraining even with deeper networks. So, ResNet18 also maintained high accuracy for our dataset \nas well, achieving a final accuracy of 97.67% with early stopping to prevent overfitting. \n \nFig. 2: Resnet18 Architecture \n \n\n6 \n \nFig. 3: Model Flowchart \n \nFig. 4: Resnet architecture review table \n\n7 \nThe early stopping monitors the validation loss so that training is stopped when improvements \nbecome stagnant for a certain patience interval. So, this moved ResNet18 to further great heights \nby avoiding overfitting in the annotated dataset. This, combined with optimizing the learning rate, \nhelped in tuning the ability of the model to generalize effectively on unseen data, and the trained \nmodel retained high accuracy across different tissue types and layers for our objectives. Overall, \nthe proposed framework here highlights those architectures like ResNet18, particularly with a \nresidual learning mechanism, are effective in robust classification of skin tissues. With regard to \nour structured preprocessing combined with accurate annotation, the systemic approach used to \ntrain and test our framework gives great promise for scalable assessment of skin tissue, opening \nup possible future developments on self-aware wound analysis. \n4. Results and Discussions \nThis section summarizes the results of training, optimizing, and evaluation of the ResNet18 \narchitecture model for the classification of skin tissue images. The work discusses how architecture \nimpacts a choice for a model; annotations impact interactions of models; impacts of methodologies \napplied during the training process; and an analysis of errors. \n \n4.1 Overview of the ResNet18 Model \nResNet18 is one of the most advanced architectures of convolutional neural networks (CNN) with \nresidual learning using skip connections; this, in effect, allows gradients to bypass certain layers \nand hence mitigates the vanishing gradient problem to make deeper learning possible with \nrelatively lesser computation. The 18-layer architecture includes convolutional, pooling, and fully \nconnected layers to extract hierarchical features for useful applications by employing them in \ncomplicated datasets. Due to the lightweight architecture, it is computationally efficient in tasks \ndemanding high accuracy, such as in medical image analysis. \n \n4.2 Annotations Impact \nThe input of pre-labeled labels from Roboflow had a highly positive impact on the quality of this \ndataset. Every image was delineated into three well-defined layers of stratum cornetum, epidermis, \nand dermis as bounding boxes. This provided the model with the additional data above the original \nimages, thus allowing the model to focus completely on region-based features. This greatly \nimproved the precision for classification, and it stands to bear testament to the role of proper \nlabeling in a machine learning workflow. \n \n4.3 Training and Optimization \nResNet18 is trained using an 80-20 split of the dataset between training and test sets. The initial \nlearning rate is 0.001, while the optimizer used is Adam, which ultimately provided good gradient \nupdates with the categorical cross-entropy loss function.  \n \n \n\n8 \n𝑙𝑜𝑠𝑠(𝑥, 𝑐𝑙𝑎𝑠𝑠) = −𝑙𝑜𝑔(𝑒𝑥𝑝(𝑥[𝑐𝑙𝑎𝑠𝑠])\n𝛴𝑗 𝑒𝑥𝑝(𝑥[𝑗]) ) \nAn early stopping strategy was implemented with patience of 10 epochs where the validation loss \nis controlled in which training is stopped when it undergoes stagnation. This not only halts \noverfitting but also maximizes training efficiency. \n \nThe training stage consisted of 100 epochs at maximum. Dropout was used between each layer so \nthat the layers were exposed to reducing overfitting by randomly deactivating 50% of neurons per \nepoch. Training, validation loss, and accuracy are tracked and plotted as functions of epochs. \n \n \n \nFig. 5: Confusion matrix \n \n \n \nFig. 6: Training loss over epochs \n \n4.4 Model Performance \nOn the test set, ResNet18 accuracy reached an impressive 97.67% accuracy rate. Ablation studies \nshowed that the architectures outperform other variants such as VGG16, DenseNet121, and \nEfficientNet. Other metrics included precision, recall, F1-score, and a confusion matrix in the form \nof the following formulas: \n\n9 \n \nIt turns out that the confusion matrix reveals high accuracy in classification for all classes, and \nonly minor misclassifications between healthy_men and healthy_women might be due to \noverlapping features. \n \n4.5 Error Analysis \nAs it turns out, similar visual features between healthy_men and healthy_women were initial \ncauses for failure to differentiate between these classes. Including them in the dataset with the \nRoboflow annotations generally fixed this problem and permitted the model to rely on deciding \nkey characteristics differentiating classes. Aiding with the addition of the annotations as auxiliary \nparameters for input contributed to a noticeable gain in class-specific recall and overall accuracy. \n \nThese enhancements thus appear to suggest that if the labeling is exact, region-specific information \nis important for augmenting the performance of the model, especially in tasks requiring fine visual \ndifferences. \n \nAlso, the graph shows a gradually descending line, which shows that the model training is smooth \nwith a gradual loss drop, which shows no overfitting has taken place. In the upcoming section, the \ngraphs are very spiky, which shows how important triggers are in the proposed model. \n \n \n \n \n \n \n \n \n\n10 \n5. Ablation Study  \n \nIn the ablation study, four distinct pre-trained models: ResNet18, VGG16, DenseNet121, and \nEfficientNet, were used to see how these models performed without early-stopping triggers. Since \nthe goal was assessing how well the models performed on the task of classification of skin tissue \nby making use of annotations from Roboflow to enhance the general learning process, this \nexperiment was important with labeled data from Roboflow because it allows for better \nunderstanding of characteristics of each class, hence better classification accuracy. \n \nThe test set ResNet18, famous for its efficient residual learning framework, achieved 95.35% \naccuracy in the classification of skin tissue images. Due to the removal of the vanishing gradient \nproblem, the model architecture including residual connections enables it to learn better. These \nhelp gradients flow during backpropagation easier, hence a robust model for the classification of \nimages, ResNet18. Even if early stopping triggers were removed, the accuracy would have \ndropped by roughly about 2%. \n \nThe second most widely used model, VGG16, performed poorly with an accuracy of 72.09%. Its \narchitecture is based on simple yet deep networks of convolutional layers but is infamous for \nhaving large numbers of parameters that can make it sometimes prone to overfitting compared to \nsome complex architectures like ResNet. This overfitting was strongly reflected in the relatively \nlower accuracy that VGG16 achieved, especially when the training was performed without early \nstopping triggers that might have helped in preventing it. \n \nThe DenseNet121 model, which connects each layer to every other layer in a dense block, shows \nreasonably good performance at 90.70%. The architecture of DenseNet generally has a better \ngradient flow and therefore promotes feature reuse across the different layers. Therefore, it can be \nvery useful when dealing with very complex datasets. Still, it is not as effective as ResNet18 and \nEfficientNet, based probably on its more complex architecture, and it may need to undergo more \nfine-tuning to achieve its perfect performance. \n \nThe model EfficientNet scaled with both depth, width, and resolution achieved accuracy similar \nto that of ResNet18 at 95.35%. With the compound scaling approach, EfficientNet ensured the use \nof as few parameters as possible in the model, which made it one of the best models used in this \nstudy. It performed well in the beginning, but when the step of early stopping was removed, the \nresults showed that the model needed more careful optimizations for consistent high performance. \n \nBased on the above initial results, the best among the two models is ResNet18 and EfficientNet. \nWe selected these two models and further trained them with different learning rates for fine-tuning \nthe performance. \n \n\n11 \nThe learning rate values from very low to high are set as follows for both ResNet18 and \nEfficientNet that is [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]. For EfficientNet, the accuracy was 95.35%, \nwhich was identical with that of the first training, at a learning rate of 0.0001. The accuracy \nplunged sharply to as low as remarkably 4.65% when the learning rate was 0.1 and down to 30.23% \nat 1.0. \n \nFor ResNet18, the optimal results were also obtained using a learning rate of 0.0001, at which the \naccuracy has been reached at 95.35%. However, if a higher learning rate is used, then the model's \nresult is significantly poor. At learning rates of 0.01, 0.1, and 1.0, the accuracy has been reduced \nto 34.88%, meaning that learning rates at such high levels prevented the model from converging \nsuitably. \n \nThe study demonstrates, through results, that learning rate selection is one of the key factors for \nthe successful training of models. Notably, both ResNet18 and EfficientNet attained very high \naccuracy at lower learning rates, but very high values of learning rates led to serious loss in \naccuracy, which does highlight the need for fine-tuning hyperparameters for the best achievable \nperformance. \n \nBesides that, the accuracy results that have been attained for the models along with the \ncorresponding training graphs and confusion matrices further illustrate how hyperparameter \ntuning, including learning rate adjustment, heavily weighs on the models' performances. \nAnnotations from Roboflow also came in handy, proving to be important for achieving better \naccuracy by giving labeled data that was used as input to train the models concerned. \n \nResNet18:  \n \n \nFig. 7: Results obtained using ResNet \n\n12 \nVGG16: \n \nFig. 8: Results obtained using VGG16 \n \nDenseNet121: \n \nFig. 9: Results obtained using Densenet121 \n \n \n \n \n \n \n \n \n \n \n\n13 \nEfficientNet: \n \n \nFig. 10: Results obtained using Efficient net \n \nThis ablation study thereby brings out the substantial impact that model architecture choices and \nhyperparameter optimization would have on the final performance, reiterating the notion that \nprecise tuning and quality data annotations are essential. \n \n6. Conclusion \n \nIn this paper, we shall discuss and test the performance of four renowned deep learning models: \nResNet18, VGG16, DenseNet121, and EfficientNet on the classification of images of skin tissues \ninto five classes. We did not apply early stopping and have an ablation study on the optimization \nof some hyperparameters to keep optimal learning rates. In this research, the top two performers \nincluded ResNet18 and EfficientNet with an accuracy of 95.35%. \n \nThe best use of the labeled annotations from Roboflow was to make the models understand \nnuances in classes of skin tissue. Further analysis on how the models perform with different \nlearning rates shows that fine-tuning the hyperparameter is essential for best performance. Both \nResNet18 and EfficientNet showed the best result when fine-tuned at a learning rate of 0.0001, \nproving the need for a more precise fine-tuning factor to avoid overfitting or underfitting. \n \nFrom the results presented in this study, it can be concluded that the selection of model, effects of \nhyperparameter tuning, and quality labeled data are involved in training deep learning models \nbased on an image classification task. So, other further work may involve designing techniques \nsuch as transfer learning with domain-specific models, architectural changes, and higher \noptimization of learning rates to improve the classification accuracy even further. \n \n \n\n14 \n7. References \nShenoy, V. N., Foster, E., Aalami, L., Majeed, B., & Aalami, O. (2018). Deepwound: Automated \nPostoperative Wound Assessment and Surgical Site Surveillance through Convolutional Neural \nNetworks. \nHe, J., Wang, X., Chen, L., Cai, Y., & Wang, Z. (2020). Deep Learning Method to Predict Wound \nHealing Progress Based on Collagen Fibers in Wound Tissue. \nBlanco, G., et al. (2019). A Superpixel-Driven Deep Learning Approach for the Analysis of \nDermatological Wounds. \nZhang, R., et al. (2022). A Survey of Wound Image Analysis Using Deep Learning. \nSiregar, P., Liu, Y.-S., & Franelyn. (2024). Optimization of Laser-Based Method to Conduct Skin \nAblation in Zebrafish and Development of Deep Learning-Based Method for Skin Wound-Size \nMeasurement. \nZhao, Q., & Chen, L. (2023). Integrated Optical Coherence Tomography and Deep Learning for \nEvaluating the Injectable Hydrogel on Skin Wound Healing. \nRibeiro Neto, P. D. (2021). Smart Data-Driven Predictive Model Application for Wound Healing \nTracking.",
    "pdf_filename": "Deep_Learning-Driven_Heat_Map_Analysis_for_Evaluating_thickness_of_Wounded_Skin_Layers.pdf"
}