{
    "title": "Introduction to Machine Learning",
    "context": "Laurent Younes September 5, 2024 arXiv:2409.02668v1  [stat.ML]  4 Sep 2024",
    "body": "Introduction to Machine Learning\nLaurent Younes\nSeptember 5, 2024\narXiv:2409.02668v1  [stat.ML]  4 Sep 2024\n\n2\n\nContents\nPreface\n13\n1\nGeneral Notation and Background Material\n15\n1.1\nLinear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n1.2\nTopology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n1.3\nCalculus\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n1.4\nProbability theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n2\nA Few Results in Matrix Analysis\n25\n2.1\nNotation and basic facts\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n2.2\nThe trace inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n2.3\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n2.4\nSome matrix norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n3\nIntroduction to Optimization\n37\n3.1\nBasic Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n3.2\nUnconstrained Optimization Problems . . . . . . . . . . . . . . . . . .\n39\n3.2.1\nConditions for optimality (general case) . . . . . . . . . . . . .\n39\n3.2.2\nConvex sets and functions . . . . . . . . . . . . . . . . . . . . .\n40\n3.2.3\nRelative interior . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n3.2.4\nDerivatives of convex functions and optimality conditions . . .\n45\n3.2.5\nDirection of descent and steepest descent\n. . . . . . . . . . . .\n48\n3.2.6\nConvergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n3.2.7\nLine search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n3.3\nStochastic gradient descent . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n3.3.1\nStochastic approximation methods . . . . . . . . . . . . . . . .\n56\n3.3.2\nDeterministic approximation and convergence study . . . . . .\n56\n3.3.3\nThe ADAM algorithm . . . . . . . . . . . . . . . . . . . . . . . .\n62\n3.4\nConstrained optimization problems . . . . . . . . . . . . . . . . . . . .\n63\n3.4.1\nLagrange multipliers . . . . . . . . . . . . . . . . . . . . . . . .\n63\n3.4.2\nConvex constraints . . . . . . . . . . . . . . . . . . . . . . . . .\n65\n3.4.3\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n3.4.4\nProjected gradient descent . . . . . . . . . . . . . . . . . . . . .\n70\n3.5\nGeneral convex problems . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n3\n\n4\nCONTENTS\n3.5.1\nEpigraphs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n3.5.2\nSubgradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n3.5.3\nDirectional derivatives . . . . . . . . . . . . . . . . . . . . . . .\n74\n3.5.4\nSubgradient descent\n. . . . . . . . . . . . . . . . . . . . . . . .\n76\n3.5.5\nProximal Methods . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n3.6\nDuality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n3.6.1\nGeneralized KKT conditions . . . . . . . . . . . . . . . . . . . .\n84\n3.6.2\nDual problem . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n3.6.3\nExample: Quadratic programming . . . . . . . . . . . . . . . .\n88\n3.6.4\nProximal iterations and augmented Lagrangian . . . . . . . . .\n89\n3.6.5\nAlternative direction method of multipliers . . . . . . . . . . .\n91\n3.7\nConvex separation theorems and additional proofs . . . . . . . . . . .\n92\n3.7.1\nProof of proposition 3.44 . . . . . . . . . . . . . . . . . . . . . .\n92\n3.7.2\nProof of theorem 3.45 . . . . . . . . . . . . . . . . . . . . . . . .\n94\n3.7.3\nProof of theorem 3.46 . . . . . . . . . . . . . . . . . . . . . . . .\n95\n4\nIntroduction: Bias and Variance\n97\n4.1\nParameter estimation and sieves . . . . . . . . . . . . . . . . . . . . . .\n97\n4.2\nKernel density estimation . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n5\nPrediction: Basic Concepts\n107\n5.1\nGeneral Setting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n5.2\nConditional expectation\n. . . . . . . . . . . . . . . . . . . . . . . . . . 108\n5.3\nBayes predictor\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n5.4\nExamples: model-based approach . . . . . . . . . . . . . . . . . . . . . 113\n5.4.1\nGaussian models and naive Bayes . . . . . . . . . . . . . . . . . 113\n5.4.2\nKernel regression . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n5.4.3\nA classification example . . . . . . . . . . . . . . . . . . . . . . 115\n5.5\nEmpirical risk minimization . . . . . . . . . . . . . . . . . . . . . . . . 115\n5.5.1\nGeneral principles\n. . . . . . . . . . . . . . . . . . . . . . . . . 115\n5.5.2\nBias and variance . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n5.6\nEvaluating the error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n5.6.1\nGeneralization error\n. . . . . . . . . . . . . . . . . . . . . . . . 119\n5.6.2\nCross validation . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n6\nInner Products and Reproducing Kernels\n123\n6.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n6.2\nBasic Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n6.2.1\nInner-product spaces . . . . . . . . . . . . . . . . . . . . . . . . 123\n6.2.2\nFeature spaces and kernels . . . . . . . . . . . . . . . . . . . . . 124\n6.3\nFirst examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n6.3.1\nInner product . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n6.3.2\nPolynomial Kernels . . . . . . . . . . . . . . . . . . . . . . . . . 126\n\nCONTENTS\n5\n6.3.3\nFunctional Features . . . . . . . . . . . . . . . . . . . . . . . . . 127\n6.3.4\nGeneral construction theorems\n. . . . . . . . . . . . . . . . . . 129\n6.3.5\nOperations on kernels\n. . . . . . . . . . . . . . . . . . . . . . . 130\n6.3.6\nCanonical Feature Spaces\n. . . . . . . . . . . . . . . . . . . . . 132\n6.4\nProjection on a finite-dimensional subspace . . . . . . . . . . . . . . . 134\n7\nLinear Regression\n137\n7.1\nLeast-Square Regression\n. . . . . . . . . . . . . . . . . . . . . . . . . . 137\n7.1.1\nNotation and Basic Estimator . . . . . . . . . . . . . . . . . . . 137\n7.1.2\nLimit behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7.1.3\nGauss-Markov theorem . . . . . . . . . . . . . . . . . . . . . . . 141\n7.1.4\nKernel Version . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n7.2\nRidge regression and Lasso . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.2.1\nRidge Regression\n. . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.2.2\nEquivalence of constrained and penalized formulations . . . . 147\n7.2.3\nLasso regression . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n7.3\nOther Sparsity Estimators\n. . . . . . . . . . . . . . . . . . . . . . . . . 155\n7.3.1\nLARS estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n7.3.2\nThe Dantzig selector\n. . . . . . . . . . . . . . . . . . . . . . . . 157\n7.4\nSupport Vector Machines for regression\n. . . . . . . . . . . . . . . . . 161\n7.4.1\nLinear SVM\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n7.4.2\nThe kernel trick and SVMs . . . . . . . . . . . . . . . . . . . . . 165\n8\nModels for linear classification\n167\n8.1\nLogistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n8.1.1\nGeneral Framework . . . . . . . . . . . . . . . . . . . . . . . . . 167\n8.1.2\nConditional log-likelihood . . . . . . . . . . . . . . . . . . . . . 168\n8.1.3\nTraining algorithm\n. . . . . . . . . . . . . . . . . . . . . . . . . 171\n8.1.4\nPenalized Logistic Regression . . . . . . . . . . . . . . . . . . . 173\n8.1.5\nKernel logistic regression . . . . . . . . . . . . . . . . . . . . . . 176\n8.2\nLinear Discriminant analysis . . . . . . . . . . . . . . . . . . . . . . . . 177\n8.2.1\nGenerative model in classification and LDA . . . . . . . . . . . 177\n8.2.2\nDimension reduction . . . . . . . . . . . . . . . . . . . . . . . . 179\n8.2.3\nFisher’s LDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n8.2.4\nKernel LDA\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n8.3\nOptimal Scoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n8.3.1\nKernel optimal scoring . . . . . . . . . . . . . . . . . . . . . . . 192\n8.4\nSeparating hyperplanes and SVMs\n. . . . . . . . . . . . . . . . . . . . 194\n8.4.1\nOne-layer perceptron and margin . . . . . . . . . . . . . . . . . 194\n8.4.2\nMaximizing the margin . . . . . . . . . . . . . . . . . . . . . . . 195\n8.4.3\nKKT conditions and dual problem\n. . . . . . . . . . . . . . . . 197\n8.4.4\nKernel version . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\n\n6\nCONTENTS\n9\nNearest-Neighbor Methods\n201\n9.1\nNearest neighbors for regression . . . . . . . . . . . . . . . . . . . . . . 201\n9.1.1\nConsistency\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n9.1.2\nOptimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n9.2\np-NN classification\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\n9.3\nDesigning the distance . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\n10 Tree-based algorithms\n213\n10.1 Recursive Partitioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\n10.1.1 Binary prediction trees . . . . . . . . . . . . . . . . . . . . . . . 213\n10.1.2 Training algorithm\n. . . . . . . . . . . . . . . . . . . . . . . . . 214\n10.1.3 Resulting predictor . . . . . . . . . . . . . . . . . . . . . . . . . 215\n10.1.4 Stopping rule\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\n10.1.5 Leaf predictors\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 215\n10.1.6 Binary features\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 215\n10.1.7 Pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n10.2 Random Forests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n10.2.1 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n10.2.2 Feature randomization . . . . . . . . . . . . . . . . . . . . . . . 219\n10.3 Top-Scoring Pairs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220\n10.4 Adaboost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222\n10.4.1 General set-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222\n10.4.2 The Adaboost algorithm . . . . . . . . . . . . . . . . . . . . . . 223\n10.4.3 Adaboost and greedy gradient descent . . . . . . . . . . . . . . 227\n10.5 Gradient boosting and regression . . . . . . . . . . . . . . . . . . . . . 228\n10.5.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228\n10.5.2 Translation-invariant loss\n. . . . . . . . . . . . . . . . . . . . . 229\n10.5.3 General loss functions\n. . . . . . . . . . . . . . . . . . . . . . . 230\n10.5.4 Return to classification . . . . . . . . . . . . . . . . . . . . . . . 232\n10.5.5 Gradient tree boosting . . . . . . . . . . . . . . . . . . . . . . . 233\n11 Neural Nets\n237\n11.1 First definitions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n11.2 Neural nets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\n11.2.1 Transitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\n11.2.2 Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\n11.2.3 Image data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\n11.3 Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\n11.4 Objective function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\n11.4.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\n11.4.2 Differential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\n11.4.3 Complementary computations\n. . . . . . . . . . . . . . . . . . 244\n11.5 Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . 245\n\nCONTENTS\n7\n11.5.1 Mini-batches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\n11.5.2 Dropout\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\n11.6 Continuous time limit and dynamical systems . . . . . . . . . . . . . . 246\n11.6.1 Neural ODEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\n11.6.2 Adding a running cost . . . . . . . . . . . . . . . . . . . . . . . 249\n12 Monte-Carlo Sampling\n255\n12.1 General sampling procedures\n. . . . . . . . . . . . . . . . . . . . . . . 255\n12.2 Rejection sampling\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\n12.3 Markov chain sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n12.3.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\n12.3.2 Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259\n12.3.3 Invariance and reversibility\n. . . . . . . . . . . . . . . . . . . . 263\n12.3.4 Irreducibility and recurrence\n. . . . . . . . . . . . . . . . . . . 266\n12.3.5 Speed of convergence . . . . . . . . . . . . . . . . . . . . . . . . 268\n12.3.6 Models on finite state spaces . . . . . . . . . . . . . . . . . . . . 268\n12.3.7 Examples on Rd . . . . . . . . . . . . . . . . . . . . . . . . . . . 269\n12.4 Gibbs sampling\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273\n12.4.1 Definition\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273\n12.4.2 Example: Ising model . . . . . . . . . . . . . . . . . . . . . . . . 276\n12.5 Metropolis-Hastings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\n12.5.1 Definition\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\n12.5.2 Sampling methods for continuous variables . . . . . . . . . . . 278\n12.6 Perfect sampling methods\n. . . . . . . . . . . . . . . . . . . . . . . . . 282\n12.7 Markovian Stochastic Approximation . . . . . . . . . . . . . . . . . . . 286\n13 Markov Random Fields\n293\n13.1 Independence and conditional independence\n. . . . . . . . . . . . . . 293\n13.1.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n13.1.2 Fundamental properties . . . . . . . . . . . . . . . . . . . . . . 295\n13.1.3 Mutual independence\n. . . . . . . . . . . . . . . . . . . . . . . 297\n13.1.4 Relation with Information Theory . . . . . . . . . . . . . . . . . 298\n13.2 Models on undirected graphs\n. . . . . . . . . . . . . . . . . . . . . . . 301\n13.2.1 Graphical representation of conditional independence . . . . . 301\n13.2.2 Reduction of the Markov property\n. . . . . . . . . . . . . . . . 304\n13.2.3 Restricted graph and partial evidence\n. . . . . . . . . . . . . . 307\n13.2.4 Marginal distributions . . . . . . . . . . . . . . . . . . . . . . . 308\n13.3 The Hammersley-Clifford theorem\n. . . . . . . . . . . . . . . . . . . . 309\n13.3.1 Families of local interactions . . . . . . . . . . . . . . . . . . . . 309\n13.3.2 Characterization of positive G-Markov processes . . . . . . . . 312\n13.4 Models on acyclic graphs . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n13.4.1 Finite Markov chains . . . . . . . . . . . . . . . . . . . . . . . . 317\n13.4.2 Undirected acyclic graph models and trees\n. . . . . . . . . . . 319\n\n8\nCONTENTS\n13.5 Examples of general “loopy” Markov random fields . . . . . . . . . . . 324\n13.6 General state spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326\n14 Probabilistic Inference for MRF\n329\n14.1 Monte Carlo sampling\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 330\n14.2 Inference with acyclic graphs\n. . . . . . . . . . . . . . . . . . . . . . . 335\n14.3 Belief propagation and free energy approximation\n. . . . . . . . . . . 341\n14.3.1 BP stationarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\n14.3.2 Free-energy approximations . . . . . . . . . . . . . . . . . . . . 342\n14.4 Computing the most likely configuration . . . . . . . . . . . . . . . . . 348\n14.5 General sum-prod and max-prod algorithms . . . . . . . . . . . . . . . 351\n14.5.1 Factor graphs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\n14.5.2 Junction trees\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 356\n14.6 Building junction trees . . . . . . . . . . . . . . . . . . . . . . . . . . . 358\n14.6.1 Triangulated graphs\n. . . . . . . . . . . . . . . . . . . . . . . . 359\n14.6.2 Building triangulated graphs\n. . . . . . . . . . . . . . . . . . . 363\n14.6.3 Computing maximal cliques . . . . . . . . . . . . . . . . . . . . 366\n14.6.4 Characterization of junction trees . . . . . . . . . . . . . . . . . 367\n15 Bayesian Networks\n371\n15.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n15.2 Conditional independence graph\n. . . . . . . . . . . . . . . . . . . . . 372\n15.2.1 Moral graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372\n15.2.2 Reduction to d-separation . . . . . . . . . . . . . . . . . . . . . 374\n15.2.3 Chain-graph representation . . . . . . . . . . . . . . . . . . . . 376\n15.2.4 Markov equivalence . . . . . . . . . . . . . . . . . . . . . . . . . 378\n15.2.5 Probabilistic inference: Sum-prod algorithm\n. . . . . . . . . . 382\n15.2.6 Conditional probabilities and interventions . . . . . . . . . . . 387\n15.3 Structural equation models . . . . . . . . . . . . . . . . . . . . . . . . . 389\n16 Latent Variables and Variational Methods\n391\n16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391\n16.2 Variational principle\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 391\n16.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393\n16.3.1 Mode approximation . . . . . . . . . . . . . . . . . . . . . . . . 393\n16.3.2 Gaussian approximation . . . . . . . . . . . . . . . . . . . . . . 394\n16.3.3 Mean-field approximation . . . . . . . . . . . . . . . . . . . . . 394\n16.4 Maximum likelihood estimation . . . . . . . . . . . . . . . . . . . . . . 397\n16.4.1 The EM algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 397\n16.4.2 Application: Mixtures of Gaussian . . . . . . . . . . . . . . . . 399\n16.4.3 Stochastic approximation EM . . . . . . . . . . . . . . . . . . . 401\n16.4.4 Variational approximation . . . . . . . . . . . . . . . . . . . . . 403\n16.5 Remarks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405\n\nCONTENTS\n9\n16.5.1 Variations on the EM . . . . . . . . . . . . . . . . . . . . . . . . 405\n16.5.2 Direct minimization\n. . . . . . . . . . . . . . . . . . . . . . . . 406\n16.5.3 Product measure assumption\n. . . . . . . . . . . . . . . . . . . 407\n17 Learning Graphical Models\n409\n17.1 Learning Bayesian networks . . . . . . . . . . . . . . . . . . . . . . . . 409\n17.1.1 Learning a Single Probability\n. . . . . . . . . . . . . . . . . . . 409\n17.1.2 Learning a Finite Probability Distribution . . . . . . . . . . . . 411\n17.1.3 Conjugate Prior for Bayesian Networks . . . . . . . . . . . . . . 412\n17.1.4 Structure Scoring . . . . . . . . . . . . . . . . . . . . . . . . . . 413\n17.1.5 Reducing the Parametric Dimension . . . . . . . . . . . . . . . 414\n17.2 Learning Loopy Markov Random Fields\n. . . . . . . . . . . . . . . . . 415\n17.2.1 Maximum Likelihood with Exponential Models . . . . . . . . . 415\n17.2.2 Maximum likelihood with stochastic gradient ascent . . . . . . 417\n17.2.3 Relation with Maximum Entropy . . . . . . . . . . . . . . . . . 418\n17.2.4 Iterative Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . 420\n17.2.5 Pseudo likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . 423\n17.2.6 Continuous variables and score matching . . . . . . . . . . . . 424\n17.3 Incomplete observations for graphical models . . . . . . . . . . . . . . 426\n17.3.1 The EM Algorithm\n. . . . . . . . . . . . . . . . . . . . . . . . . 426\n17.3.2 Stochastic gradient ascent . . . . . . . . . . . . . . . . . . . . . 427\n17.3.3 Pseudo-EM Algorithm\n. . . . . . . . . . . . . . . . . . . . . . . 428\n17.3.4 Partially-observed Bayesian networks on trees . . . . . . . . . . 429\n17.3.5 General Bayesian networks . . . . . . . . . . . . . . . . . . . . . 431\n18 Deep Generative Methods\n433\n18.1 Normalizing flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433\n18.1.1 General concepts\n. . . . . . . . . . . . . . . . . . . . . . . . . . 433\n18.1.2 A greedy computation\n. . . . . . . . . . . . . . . . . . . . . . . 434\n18.1.3 Neural implementation . . . . . . . . . . . . . . . . . . . . . . . 435\n18.1.4 Time-continuous version . . . . . . . . . . . . . . . . . . . . . . 435\n18.2 Non-diffeomorphic models and variational autoencoders\n. . . . . . . 437\n18.2.1 General framework . . . . . . . . . . . . . . . . . . . . . . . . . 437\n18.2.2 Generative model for VAEs\n. . . . . . . . . . . . . . . . . . . . 437\n18.2.3 Discrete data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439\n18.3 Generative Adversarial Networks (GAN) . . . . . . . . . . . . . . . . . 440\n18.3.1 Basic principles . . . . . . . . . . . . . . . . . . . . . . . . . . . 440\n18.3.2 Objective function\n. . . . . . . . . . . . . . . . . . . . . . . . . 440\n18.3.3 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441\n18.3.4 Associated probability metric and Wasserstein GANs\n. . . . . 442\n18.4 Reversed Markov chain models\n. . . . . . . . . . . . . . . . . . . . . . 444\n18.4.1 General principles\n. . . . . . . . . . . . . . . . . . . . . . . . . 444\n18.4.2 Binary model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 445\n\n10\nCONTENTS\n18.4.3 Model with continuous variables . . . . . . . . . . . . . . . . . 447\n18.4.4 Continuous-time limit . . . . . . . . . . . . . . . . . . . . . . . 449\n18.4.5 Differential of neural functions . . . . . . . . . . . . . . . . . . 449\n19 Clustering\n451\n19.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451\n19.2 Hierarchical clustering and dendograms . . . . . . . . . . . . . . . . . 452\n19.2.1 Partition trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452\n19.2.2 Bottom-up construction\n. . . . . . . . . . . . . . . . . . . . . . 453\n19.2.3 Top-down construction . . . . . . . . . . . . . . . . . . . . . . . 456\n19.2.4 Thresholding\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 457\n19.3 K-medoids and K-mean . . . . . . . . . . . . . . . . . . . . . . . . . . . 457\n19.3.1 K-medoids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457\n19.3.2 Mixtures of Gaussian and deterministic annealing . . . . . . . 460\n19.3.3 Kernel (soft) K-means . . . . . . . . . . . . . . . . . . . . . . . . 462\n19.3.4 Convex relaxation . . . . . . . . . . . . . . . . . . . . . . . . . . 463\n19.4 Spectral clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468\n19.4.1 Spectral approximation of minimum discrepancy . . . . . . . . 468\n19.5 Graph partitioning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470\n19.6 Deciding the number of clusters . . . . . . . . . . . . . . . . . . . . . . 472\n19.6.1 Detecting elbows\n. . . . . . . . . . . . . . . . . . . . . . . . . . 472\n19.6.2 The Cali´nski and Harabasz index . . . . . . . . . . . . . . . . . 473\n19.6.3 The “silhouette” index . . . . . . . . . . . . . . . . . . . . . . . 475\n19.6.4 Comparing to homogeneous data . . . . . . . . . . . . . . . . . 476\n19.7 Bayesian Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478\n19.7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478\n19.7.2 Model with a bounded number of clusters . . . . . . . . . . . . 482\n19.7.3 Non-parametric priors . . . . . . . . . . . . . . . . . . . . . . . 488\n20 Dimension Reduction and Factor Analysis\n497\n20.1 Principal component analysis\n. . . . . . . . . . . . . . . . . . . . . . . 497\n20.1.1 General Framework . . . . . . . . . . . . . . . . . . . . . . . . . 497\n20.1.2 Computation of the principal components . . . . . . . . . . . . 502\n20.2 Kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504\n20.3 Statistical interpretation and probabilistic PCA . . . . . . . . . . . . . 506\n20.4 Generalized PCA\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509\n20.5 Nuclear norm minimization and robust PCA . . . . . . . . . . . . . . . 511\n20.5.1 Low-rank approximation . . . . . . . . . . . . . . . . . . . . . . 511\n20.5.2 The nuclear norm . . . . . . . . . . . . . . . . . . . . . . . . . . 513\n20.5.3 Robust PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514\n20.6 Independent component analysis . . . . . . . . . . . . . . . . . . . . . 516\n20.6.1 Identifiability\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 516\n20.6.2 Measuring independence and non-Gaussianity . . . . . . . . . 517\n\nCONTENTS\n11\n20.6.3 Maximization over orthogonal matrices\n. . . . . . . . . . . . . 523\n20.6.4 Parametric ICA\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 524\n20.6.5 Probabilistic ICA\n. . . . . . . . . . . . . . . . . . . . . . . . . . 526\n20.7 Non-negative matrix factorization . . . . . . . . . . . . . . . . . . . . . 530\n20.8 Variational Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . 535\n20.9 Bayesian factor analysis and Poisson point processes . . . . . . . . . . 535\n20.9.1 A feature selection model\n. . . . . . . . . . . . . . . . . . . . . 535\n20.9.2 Non-negative and count variables . . . . . . . . . . . . . . . . . 537\n20.9.3 Feature assignment model . . . . . . . . . . . . . . . . . . . . . 539\n20.10Point processes and random measures\n. . . . . . . . . . . . . . . . . . 542\n20.10.1Poisson processes . . . . . . . . . . . . . . . . . . . . . . . . . . 542\n20.10.2The gamma process . . . . . . . . . . . . . . . . . . . . . . . . . 545\n20.10.3The beta process . . . . . . . . . . . . . . . . . . . . . . . . . . . 546\n20.10.4Beta Process and feature selection . . . . . . . . . . . . . . . . . 547\n21 Data Visualization and Manifold Learning\n549\n21.1 Multidimensional scaling . . . . . . . . . . . . . . . . . . . . . . . . . . 549\n21.1.1 Similarity matching (Euclidean case) . . . . . . . . . . . . . . . 549\n21.1.2 Dissimilarity matching . . . . . . . . . . . . . . . . . . . . . . . 552\n21.2 Manifold learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556\n21.2.1 Isomap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556\n21.2.2 Local Linear Embedding . . . . . . . . . . . . . . . . . . . . . . 558\n21.2.3 Graph Embedding\n. . . . . . . . . . . . . . . . . . . . . . . . . 561\n21.2.4 Stochastic neighbor embedding . . . . . . . . . . . . . . . . . . 565\n21.2.5 Uniform manifold approximation and projection (UMAP) . . . 568\n22 Generalization Bounds\n573\n22.1 Notation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573\n22.2 Penalty-based Methods and Minimum Description Length . . . . . . . 574\n22.2.1 Akaike’s information criterion . . . . . . . . . . . . . . . . . . . 574\n22.2.2 Bayesian information criterion and minimum description length576\n22.3 Concentration inequalities . . . . . . . . . . . . . . . . . . . . . . . . . 581\n22.3.1 Cram´er’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 582\n22.3.2 Sub-Gaussian variables . . . . . . . . . . . . . . . . . . . . . . . 584\n22.3.3 Bennett’s inequality . . . . . . . . . . . . . . . . . . . . . . . . . 587\n22.3.4 Hoeffding’s inequality\n. . . . . . . . . . . . . . . . . . . . . . . 590\n22.3.5 McDiarmid’s inequality\n. . . . . . . . . . . . . . . . . . . . . . 592\n22.3.6 Boucheron-Lugosi-Massart inequality\n. . . . . . . . . . . . . . 593\n22.4 Bounding the empirical error with the VC-dimension\n. . . . . . . . . 594\n22.4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594\n22.4.2 Vapnik’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 596\n22.4.3 VC dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . 598\n22.4.4 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 601\n\n12\nCONTENTS\n22.4.5 Data-based estimates . . . . . . . . . . . . . . . . . . . . . . . . 602\n22.5 Covering numbers and chaining . . . . . . . . . . . . . . . . . . . . . . 604\n22.5.1 Covering, packing and entropy numbers . . . . . . . . . . . . . 605\n22.5.2 A first union bound . . . . . . . . . . . . . . . . . . . . . . . . . 605\n22.5.3 Evaluating covering numbers . . . . . . . . . . . . . . . . . . . 608\n22.5.4 Chaining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 609\n22.5.5 Metric entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612\n22.5.6 Application\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 613\n22.6 Other complexity measures\n. . . . . . . . . . . . . . . . . . . . . . . . 615\n22.6.1 Fat-shattering and margins . . . . . . . . . . . . . . . . . . . . . 615\n22.6.2 Maximum discrepancy . . . . . . . . . . . . . . . . . . . . . . . 622\n22.6.3 Rademacher complexity . . . . . . . . . . . . . . . . . . . . . . 624\n22.6.4 Algorithmic Stability . . . . . . . . . . . . . . . . . . . . . . . . 627\n22.6.5 PAC-Bayesian bounds . . . . . . . . . . . . . . . . . . . . . . . . 629\n22.7 Application to model selection . . . . . . . . . . . . . . . . . . . . . . . 632\n\nPreface\nMachine learning addresses the issue of analyzing, reproducing and predicting var-\nious mechanisms and processes observable through experiments and data acquisi-\ntion. With the impetus of large technological companies in need of leveraging in-\nformation included in the gigantic datasets that they produced or obtained through\nuser data, with the development of new data acquisition techniques in biology, physics\nor astronomy, with the improvement of storage capacity and high-performance com-\nputing, this field has experienced an explosive growth over the past decades, in\nterms of scientific production and technological impact.\nWhile it is being recognized in some places as a scientific discipline in itself, ma-\nchine learning (which has received a few almost synonymic denominations across\ntime, including artificial intelligence, machine intelligence or statistical learning),\ncan also be seen as an interdisciplinary field interfacing techniques from traditional\ndomains such as computer science, applied mathematics, and statistics. From statis-\ntics, and more specially nonparametric statistics, it borrows its main formalism,\nasymptotic results and generalization bounds. It also builds on many classical meth-\nods that have been developed for estimation and prediction. From computer science,\nit involves the construction and implementation of efficient algorithms, program-\nming design and architecture. Finally, machine learning leverages classical methods\nfrom linear algebra and functional analysis, as well as from convex and nonlinear\noptimization, fields within which it had also provided new problems and discover-\nies. It forms a significant part of the larger field commonly called “data science,”\nwhich includes methods for storing, sharing and managing data, the development\npowerful computer architectures for increasingly demanding algorithms, and, im-\nportantly, the definition of ethical limits and processes through which data should\nbe used in the modern world.\nThis book, which originates from lecture notes of a series of graduate course\ntaught in the Department of Applied Mathematics and Statistics at Johns Hopkins\nUniversity, adopts a viewpoint (or bias) mainly focused on the mathematical and sta-\ntistical aspects of the subject. Its goal is to introduce the mathematical foundations\nand techniques that lead to the development and analysis of many of the algorithms\nthat are used today. It is written with the hope to provide the reader with a deeper\n13\n\n14\nCONTENTS\nunderstanding of the algorithms made available to her in multiple machine learn-\ning packages and software, and that she will be able to assess their prerequisites and\nlimitations, and to extend them and develop new algorithms. Note that, while adopt-\ning a presentation with a strong mathematical flavor, we will still make explicit the\ndetails of many important machine learning algorithms.\nUnsurprisingly, the book will be more accessible to a reader with some back-\nground in mathematics and statistics. It assumes familiarity with basic concepts in\nlinear algebra and matrix analysis, in multivariate calculus and in probability and\nstatistics. We tried to place a limit at the use of measure theoretic tools, that are\navoided up to a few exceptions, which are be localized and be accompanied with\nalternative interpretations allowing for a reading at a more elementary level.\nThe book starts with an introductory chapter that describes notation used through-\nout the book and serve at a reminder of basic concepts in calculus, linear algebra and\nprobability. It also introduces some measure theoretic terminology, and can be used\nas a reading guide for the sections that use these tools. This chapter is followed by\ntwo chapters offering background material on matrix analysis and optimization. The\nlatter chapter, which is relatively long, provides necessary references to many algo-\nrithms that are used in the book, including stochastic gradient descent, proximal\nmethods, etc.\nChapter 4, which is also introductory, illustrates the bias-variance dilemma in\nmachine learning through the angle of density estimation and motivates chapter 5\nin which basic concepts for statistical prediction are provided. Chapter 6 provides\nan introduction to reproducing kernel theory and Hilbert space techniques that are\nused in many places, before tackling, with chapters 7 to 11, the description of vari-\nous algorithms for supervised statistical learning, including linear methods, support\nvector machines, decision trees, boosting, or neural networks.\nChapter 12, which presents sampling methods and an introduction to the theory\nof Markov chains, starts a series of chapters on generative models, and associated\nlearning algorithms. Graphical models and described in chapters 13 to 15. Chap-\nter 16 introduces variational methods for models with latent variables, with applica-\ntions to graphical models in chapter 17. Generative techniques using deep learning\nare presented in chapter 18.\nChapters 19 to 21 focus on unsupervised learning methods, for clustering, factor\nanalysis and manifold learning. The final chapter of the book is theory-oriented and\ndiscusses concentration inequalities and generalization bounds.\n\nChapter 1\nGeneral Notation and Background Material\n1.1\nLinear algebra\n1. The set of all subsets of a given set A is denoted P(A). If A and B are two sets, the\nnotation BA refers to the set of all functions f : A →B. In particular, RA is the space\nof real-valued functions, and forms a vector space. When A is finite, this space is\nfinite dimensional and can be identified with R|A|, where |A| denotes the cardinality\n(number of elements) of A.\nThe indicator function of a subset C of A will be denoted 1C : A →{0,1}, with 1C(x) =\n1 if x ∈C and 0 otherwise. We will sometimes write 1x∈C for 1C(x).\n2. Elements of the d-dimensional Euclidean space Rd will be denoted with letters\nsuch as x,y,z, and their coordinates will be indexed as parenthesized exponents, so\nthat\nx =\n\n\nx(1)\n...\nx(d)\n\n\n(we will always identify element of Rd with column vectors). We will not distinguish\nin the notation between “points” in Rd, seen as an affine space, and “vectors” in Rd,\nseen as a vector space. The vectors 0d and 1d will denote the d-dimensional vectors\nwith all coordinates equal to 0 and 1, respectively. The identity matrix in Rd will be\ndenoted IdRd. The canonical basis of Rd, provided by the columns of IdRd will be\ndenoted e1,...,ed.\n3. The Euclidean norm of a vector x ∈Rd is denoted |x| with\n|x| =\n\u0010\n(x(1))2 + ··· + (x(d))2\u00111/2.\nIt will sometimes be denoted |x|2, identifying it as a member of the family of ℓp\nnorms\n|x|p =\n\u0010\n(x(1))p + ··· + (x(d))p\u00111/p\n(1.1)\n15\n\n16\nCHAPTER 1. GENERAL NOTATION AND BACKGROUND MATERIAL\nfor p ≥1. One can also define |x|p for 0 < p < 1, using (1.1), but in this case one\ndoes not get a norm because the triangle inequality |x + y|p ≤|x|p + |y|p is not true\nin general. The family is interesting, however, because it approximates, in the limit\np →0, the number of non-zero components of x, denoted |x|0, which is a measure of\nsparsity. Note that we also use the notation |A| to denote the cardinality (number of\nelements) of a set A, hopefully without risk of confusion.\nWhile we use single bars (|x|) to represent norms of finite-dimensional vectors, we\nwill use double bars (∥h∥) for infinite-dimensional objects.\n4. The set of m × d real matrices with real entries is denoted Mm,d(R), or simply\nMm,d (Md,d will also be denoted Md). The set of invertible d × d matrices will be\ndenote GLd(R).\nGiven m column vectors x1,...,xm ∈Rd, the notation [x1,...,xm] refers to the d by m\nmatrix with jth column equal to xj, so that, for example, IdRd = [e1,...,ed].\nEntry (i,j) in a matrix A ∈Mm,d(R) will either be denoted A(i,j) or A(i)\nj . The rows of\nA will be denoted A(1),...,A(m) and the columns A1,...,Am.\nThe operator norm of a matrix A ∈Mm,d is defined by\n|A|op = max{|Ax| : x ∈Rd,|x| = 1}.\n5. The space of d × d real symmetric matrices is denoted Sd, and its subsets con-\ntaining positive semi-definite (resp. positive definite) matrices is denoted S+\nd (resp.\nS++\nd ). If m ≤d, Om,d denotes the set of m × d matrices A such that AAT = IdRm, and\none writes Od for Od,d, the space of d-dimensional orthogonal matrices. Finally, SOd\nis the subset Od containing orthogonal matrices with determinant 1, i.e., rotation\nmatrices.\n6. A k-multilinear mapping is a function a : (x1,...,xk) 7→a(x1,...,xk) defined on\n(Rd)k with values in Rq which is linear in each of its variables. The mapping is\nsymmetric if its value is unchanged after any permutation of the variables. If k =\n2 and q = 1, one also says that a is a bilinear form. The norm of a k-multilinear\nmapping is defined as\n|a| = max{a(x1,...,xk) : |xj| ≤1,j = 1,...,k}\nso that\n|a(x1,...,xk)| ≤|a|\nk\nY\nj=1\n|xj|\nfor all x1,...,xk ∈Rd.\nA symmetric bilinear form a is called positive semidefinite if a(x,x) ≥0 for all x ∈Rd,\nand positive definite if it is positive semi-definite and a(x,x) = 0 if and only if x = 0.\n\n1.2. TOPOLOGY\n17\nSymmetric bilinear forms can always be expressed in the form a(x,y) = xT Ay for\nsome symmetric matrix A, and a is positive (semi-)definite if and only A is also.\nAnalogous statements hold for negative (semi-)definite forms and matrices. We will\nuse the notation A ≻0 (resp. ⪰0) to indicate that A is positive definite (resp. positive\nsemidefinite). Note that, if a(x,y) = xT Ay for A ∈Sd, then |a| = |A|op.\n1.2\nTopology\n1. The open balls in Rd will be denoted\nB(x,r) = {y ∈Rd : |y −x| < r},\nwith x ∈Rd and r > 0. The closed balls are denoted ¯B(x,r) and contain all y’s such\nthat |y −x| ≤r. A set U ⊂Rd is open if and only if for any x ∈U, there exists r > 0\nsuch that B(x,r) ⊂U. A set Γ ⊂Rd is closed if its complement, denoted\nΓc = {x ∈Rd : x < Γ}\nis open. The topological interior of a set A ⊂Rd is the largest open set included in\nA. It will be denoted either by ˚A or int(A). A point x belongs to ˚A if and only if\nB(x,r) ⊂A for some r > 0.\n2. The closure of A is the smallest closed set that contains A and will be denoted\neither ¯A or cl(A). A point x belongs to ¯A if and only if B(x,r) ∩A , ∅for all r > 0.\nAlternatively, x belongs to ¯A if and only if there exists a sequence (xk) that converges\nto x with xk ∈A for all k.\n3. A compact set in Rd is a set Γ such that any sequence of points in Γ contains\na subsequence that converges to some point in Γ. An alternate definition is that,\nwhenever Γ is covered by a collection of open sets, there exists a finite subcollection\nthat still covers Γ.\nOne can show that compact subsets of Rd are exactly its bounded and closed subsets.\n4. A metric space is a space B equipped with a distance, i.e., a function ρ : B × B →\n[0,+∞) that satisfies the following three properties.\n∀x,y ∈B : ρ(x,y) = 0 ⇔x = y,\n(1.2a)\n∀x,y ∈B : ρ(x,y) = ρ(y,x),\n(1.2b)\n∀x,y,z ∈B : ρ(x,z) ≤ρ(x,y) + ρ(y,z).\n(1.2c)\nEquation (1.2c) is called the triangle inequality. The norm of the difference between\ntwo points: ρ(x,y) = |x −y|, is a distance on Rd. The definition of open and closed\nsubsets in metric spaces is the same as above, with ρ(x,y) replacing |x −y|, and one\nsays that (xn) converges to x if and only if ρ(xn,x) →0.\nCompact subsets are also defined in the same way, but are not necessarily character-\nized as bounded and closed.\n\n18\nCHAPTER 1. GENERAL NOTATION AND BACKGROUND MATERIAL\n1.3\nCalculus\n1. If x,y ∈Rd, we will denote by [x,y] the closed segment delimited by x and y, i.e.,\nthe set of all points (1 −t)x + ty for 0 ≤t ≤1. One denotes by [x,y), (x,y] and (x,y)\nthe semi-open or open segments, with appropriate strict inequality for t. (Similarly\nto the notation for open intervals, whether (x,y) denotes an open segment or a pair\nof points will always be clear from the context.)\n2. The derivative of a differentiable function f : t 7→f (t) from an interval I ⊂R to\nR will be denoted by ∂f , or ∂tf if the variable t is well identified. Its value at t0 ∈I\nis denoted either as ∂f (t0) or ∂f |t=t0. Higher derivatives are denoted as ∂kf , k ≥0,\nwith the usual convention ∂0f = f . Note that notation such as f ′,f ′′,f (3) will never\nrefer to derivatives.\nIn the following, U is an open subset of Rd. If f is a function from U to Rm, we let\nf (i) denote the ith component of f , so that\nf (x) =\n\n\nf (1)(x)\n...\nf (m)(x)\n\n\nfor x ∈U. If d = 1, and f is differentiable, the derivative of f at x is the column\nvector of the derivatives of its components,\n∂f (x) =\n\n\n∂f (1)(x)\n...\n∂f (m)(x)\n\n\nFor d ≥1 and j ∈{1,...,d}, the jth partial derivative of f at x is\n∂jf (x) = ∂(t 7→f (x + tej))|t=0 ∈Rm,\nwhere e1,...,ed form the canonical basis of Rd. If the notation for the variables on\nwhich f depends is well understood from the context, we will alternatively use ∂xjf .\n(For example, if f : (α,β) 7→f (α,β), we will prefer ∂αf to ∂1f .) The differential of f\nat x is the linear mapping from Rd to Rm represented by the matrix\ndf (x) = [∂1f (x),...,∂df (x)].\nIt is defined so that, for all h ∈Rd\ndf (x)h = ∂(t 7→f (x + th))|t=0\nwhere the right-hand side is the directional derivative of f at x in the direction h.\nNote that, if f : Rd →R (i.e., m = 1), df (x) is a row vector. If f is differentiable\n\n1.3. CALCULUS\n19\non U and df (x) is continuous as a function of x, one says that f is continuously\ndifferentiable, or C1.\nDifferentials obey the product rule and the chain rule. If f ,g : U →R, then\nd(f g)(x) = f (x)dg(x) + g(x)df (x).\nIf f : U →Rm, g : ˜U ⊂Rk →U, then\nd(f ◦g)(x) = df (g(x))dg(x).\nIf d = m (so that df (x) is a square matrix), we let ∇·f (x) = trace(df (x)), the divergence\nof f .\nThe Euclidean gradient of a differentiable function f : U →R is ∇f (x) = df (x)T .\nMore generally, one defines the gradient of f with respect to a tensor field x 7→A(x)\ntaking values in S++\nd , as the vector ∇Af (x) that satisfies the relation\ndf (x)h = ∇Af (x)T A(x)h\nfor all h ∈Rd, so that\n∇Af (x) = A(x)−1df (x)T .\n(1.3)\nIn particular, the Euclidean gradient is associated with A(x) = IdRd for all x. With\nsome abuse of notation, we will denote ∇Af = A−1∇f when A is a fixed matrix,\ntherefore identified with the constant tensor field x 7→A.\n3. We here compute, as an illustration and because they will be useful later, the\ndifferential of the determinant and the inversion in matrix spaces.\nRecall that, if A = [a1,...,ad] ∈Md is a d by d matrix,, with a1,...,ad ∈Rd, det(A) is a\nd-linear form δ(a1,...,ad) which vanishes when two columns coincide and such that\nδ(e1,...,ed) = 1. In particular δ changes signs when two of its columns are inverted.\nIt follows from this that\n∂aij det(A) = δ(a1,...,ai−1,ej,aj+1,...,ad)\n= (−1)i−1δ(ej,a1,...,ai−1,...,ad) = (−1)i+j detA(ij),\nwhere A(ij) is the matrix A with row i and column j removed. We therefore find that\nthe differential of A 7→det(A) is the mapping\nH 7→trace(cof(A)T H)\n(1.4)\nwhere cof(A) is the matrix composed of co-factors (−1)i+j detA(ij). As a consequence,\nif A is invertible, then the differential of log|det(A)| is the mapping\nH 7→trace(det(A)−1cof(A)T H) = trace(A−1H)\n(1.5)\n\n20\nCHAPTER 1. GENERAL NOTATION AND BACKGROUND MATERIAL\nConsider now the function I(A) = A 7→A−1 defined on GLd(R), which is an open\nsubset of Md(R). Using AI(A) = IdRd and the product rule, we get\nA(dI(A)H) + HI(A) = 0\nor\ndI(A)H = −A−1HA−1.\n(1.6)\n4. Higher-order partial derivatives ∂ik ···∂i1f : U →Rm are defined by iterating the\ndefinition of first-order derivatives, namely\n∂ik ···∂i1f (x) = ∂ik(∂ik−1 ···∂i1f )(x)\nIf all order k partial derivatives of f exist and are continuous, one says that f is\nk-times continuously differentiable, or Ck and, when true, the order in which the\nderivatives are taken does not matter. In this case, one typically groups derivatives\nwith the same order using a power notation, writing, for example\n∂1∂2∂1f = ∂2\n1∂2f\nfor a C3 function.\nIf f is Ck, its kth differential at x is a symmetric k-multilinear map that can also be\niteratively defined by (for h1,...,hk ∈Rd)\ndkf (x)(h1,...,hk) = d(dk−1f (x)(h1,...,hk−1))hk ∈Rm.\nIt is related to partial derivatives through the relation:\ndkf (x)(h1,...,hk) =\nd\nX\ni1,...,ik=1\nh(i1)\n1\n···h(ik)\nk\n∂ik ···∂i1f (x).\nWhen m = 1 and k = 2, one denotes by ∇2f (x) = (∂i∂jf (x),i,j = 1,...,n) the symmet-\nric matrix formed by partial derivatives of order 2 of f at x. It is called the Hessian\nof f at x and satisfies\nhT\n1 ∇2f (x)h2 = d2f (x)(h1,h2).\nThe Laplacian of f is the trace of ∇2f and denoted ∆f .\n5. Taylor’s theorem, in its integral form, generalizes the fundamental theorem of\ncalculus to higher derivatives. It expresses the fact that, if f is Ck on U and x,y ∈U\nare such that the closed segment [x,y] is included in U, then, letting h = y −x:\nf (x + h) = f (x) + df (x)h + 1\n2d2f (x)(h,h) + ··· +\n1\n(k −1)!dk−1f (x)(h,...,h)\n+\n1\n(k −1)!\nZ 1\n0\n(1 −t)k−1dkf (x + th)(h,...,h)dt\n(1.7)\n\n1.4. PROBABILITY THEORY\n21\nThe last term (remainder) can also be written as\n1\nk!\nR 1\n0 (1 −t)k−1dkf (x + th)(h,...,h)dt\nR 1\n0 (1 −t)k−1 dt\n.\nIf f takes scalar values, then dkf (x + th)(h,...,h) is real and the intermediate value\ntheorem implies that there exists some z in [x,y] such that\nf (x + h) = f (x) + df (x)h + 1\n2d2f (x)(h,h) + ··· +\n1\n(k −1)!dk−1f (x)(h,...,h)\n+ 1\nk!dkf (z)(h,...,h).\n(1.8)\nThis is not true if f takes vector values. However, for any M such that |dkf (z)| ≤M\nfor z ∈[x,y] (such M’s always exist because f is Ck), one has\n1\n(k −1)!\nZ 1\n0\n(1 −t)k−1dkf (x + th)(h,...,h)dt ≤M\nk! |h|k.\nEquation (1.7) can be written as\nf (x + h) = f (x) + df (x)h + 1\n2d2f (x)(h,h) + ··· + 1\nk!dkf (x)(h,...,h)\n+\n1\n(k −1)!\nZ 1\n0\n(1 −t)k−1(dkf (x + th)(h,...,h) −dkf (x)(h,··· ,h))dt .\n(1.9)\nLet\nϵx(r) = max\nn\n|dkf (x + h) −dkf (x)| : |h| ≤r\no\n.\nSince dkf is continuous, ϵx(r) tends to 0 when r →0 and we have\nZ 1\n0\n(1 −t)k−1|dkf (x + th)(h,...,h) −dkf (x)(h,··· ,h)|dt ≤|h|k\nk ϵx(|h|).\nThis shows that (1.7) implies that\nf (x + h) = f (x) + df (x)h + 1\n2d2f (x)(h,h) + ··· + 1\nk!dkf (x)(h,...,h) + |h|k\nk! ϵx(|h|)\n(1.10)\n= f (x) + df (x)h + 1\n2d2f (x)(h,h) + ··· + 1\nk!dkf (x)(h,...,h) + o(|h|k)\n(1.11)\n\n22\nCHAPTER 1. GENERAL NOTATION AND BACKGROUND MATERIAL\n1.4\nProbability theory\n1. When discussing probabilistic concepts, we will make the convenient assump-\ntion that all random variables are defined on a fixed probability space (Ω,P). This\nmeans that Ωis large enough to include enough randomness to generate all required\nvariables (and implicitly enlarged when needed).\nWe assume that the reader is familiar with concepts related to discrete random vari-\nables or continuous variables (with values in Rd for some d) and their probability\ndensity functions, or p.d.f.’s. In particular, X : Ω→Rd is a random variable with\np.d.f. f if and only if the expectation of ϕ(X) is given by\nE(ϕ(X)) =\nZ\nRd ϕ(x)f (x)dx\nfor all bounded and continuous functions ϕ : Rd →[0,+∞).\n2. With a few exceptions, we will use capital letters for random variables and small\nletters for scalars and vectors that represent realizations of these variables. One of\nthese exceptions will be our notation for training data, defined as an independent\nand identically distributed (i.i.d.) sample of a given random variable. A realization\nof such a sample will always be denoted T = (x1,...,xN), which is therefore a series\nof observations. We will use the notation T = (X1,...,XN) for the collection of i.i.d.\nrandom variables that generate the training set, so that T = (X1(ω),...,XN(ω)) = T (ω)\nfor some ω ∈Ω. Another exception will apply to variables denoted using Greek\nletters, for which we will use boldface fonts (such as α,β,...).\nFor a random variable X, the notation [X = x], or [X ∈A] refers to subsets of Ω, for\nexample,\n[X = x] = {ω ∈Ω: X(ω) = x}.\n3. As much as possible—but not always—we will avoid making explicit reference to\nmeasure theory, leaving to readers familiar with this theory the task to complete the\nnotation and sometimes assumption gaps in order to make some of our statements\nfully rigorous.\nHowever, there will be situations in which the flexibility of the measure-theoretic\nformalism is needed for the exposition. The following notions may help the reader\nnavigate through these situations (basic references in measure theory are Rudin\n[171], Dudley [66], Billingsley [32]).\nA measurable space is a pair (S,S) where S is a set and S ⊂P(S) contains S, is\nstable by complementation (if A ∈S, then Ac = S \\ A ∈S), by countable unions and\nintersections. Such an S is called a σ-algebra and elements of S form the measurable\nsubsets of S (relative to the σ-algebra).\nA (positive) measure µ on (S,S) in a mapping from S →[0,+∞) that associates to\nA ∈S its measure µ(A), such that the measure of a countable union of disjoint sets\n\n1.4. PROBABILITY THEORY\n23\nis the countable sum of their measures. A function f : Ω→Rd is called measurable\nif the inverse images by f of open subsets of Rd are mesurable.\nA measurable set A (or event) is negligible (for P) if P(A) = 0 and events are said to\nhappen almost surely if their complements are negligible, i.e., P(Ac) = 0.\n4. The integral of a function f : Ω→Rd with respect to a measure (such as P) is\ndenoted\nR\nS f (x)µ(dx). This integral is defined, using a limit argument, as a function\nwhich is linear in f and such that\nZ\nA\nµ(dx) =\nZ\nS\n1A(x)µ(dx) = µ(A).\nThe Lebesgue measure, Ld, on Rd provides an important example. For this measure\nS is the σ-algebra generated by open subsets,\nR\nRd f (x)Ld(dx) extends the Riemann\nintegral and is denoted\nR\nRd f (x)dx. Another important example, when S is finite or\ncountable, is the counting measure, denoted card, that return the number of ele-\nments of a set, so that card(A) = |A|. In this case, S = P(S) and the integral is simply\nthe sum:\nZ\nS\nf (x)card(dx) =\nX\nx∈F\nf (x).\n5. If µ and ν are measures on (S,S), one says that ν is absolutely continuous with\nrespect to µ and write ν ≪µ if,\n∀A ∈S : µ(A) = 0 ⇒ν(A) = 0.\n(1.12)\nThe Radon-Nikodym theorem states that ν ≪µ if and only if ν has a density with\nrespect to µ, i.e., there exists a measurable function ϕ : S →[0,+∞) such that\nZ\nS\nf (x)ν(dx) =\nZ\nS\nf (x)ϕ(x)µ(dx)\nfor all measurable f : S →[0,+∞).\n6. If µ1 is a measure on (S1,S1) and µ2 a measure on (S2,S2), their tensor product is\ndenoted µ1 ⊗µ2. It is a measure on S1 ×S2 defined by µ1 ⊗µ2(A1 ×A2) = µ1(A1)µ2(A2)\nfor A1 ∈S1 and A2 ∈S2 (the σ-algebra on S1 ×S2 is the smallest one that contains all\nsets A1 × A2, A1 ∈S1, A2 ∈S2).\nThe integral, with respect to the product measure, of a function f : S1 × S2 →Rd is\ndenoted\nZ\nS1×S2\nf (x1,x2)µ1(dx1)µ2(dx2) =\nZ\nS1×S2\nf (x1,x2)µ1 ⊗µ2(dx1,dx2).\nThe tensor product between more that two measures is defined similarly, with nota-\ntion\nµ1 ⊗··· ⊗µn =\nn\nO\nk=1\nµk.\n\n24\nCHAPTER 1. GENERAL NOTATION AND BACKGROUND MATERIAL\n7. When using measure-theoretic probability, we will therefore assume that the pair\n(Ω,P) is completed to a triple (Ω,A,P) where A is a σ-algebra and P a probability\nmeasure, that is a positive measure on (Ω,A) such that P(Ω) = 1. This triple is called\na probability space.\nA random variable X must then also take values in a measurable space, say (S,S),\nand must be such that, for all C ∈S, the set [X ∈C] belongs to A. This justify the\ncomputation of P(X ∈C), which will also be denoted PX(C).\nA random variable X taking values in Rd has a p.d.f. if and only if PX ≪Ld and\nthe p.d.f. is the density provided by the Radon-Nikodym theorem. For a discrete\nrandom variable (i.e., taking values in a finite or countable set), the p.m.f. of X is\nalso the density of PX with respect to the counting measure card.\nIf X is a random variable with values in Rd, the integral of X with respect to P is the\nexpectation of X, denoted E(X). More generally, if (S,S,P) is a probability space, we\nwill use the notation\nEP(f ) =\nZ\nS\nf (x)P(dx).\nIf P = PX for some random variable X : Ω→S, we will use EX rather than EPX.\n8. One more technical consideration. Whenever we will consider measurable spaces,\nand sometimes without additional mention, we will assume that these spaces are\ncomplete metric spaces that have a dense countable subset (i.e., that are separable).\nIf not specified otherwise, their σ-algebras are given by the smallest ones containing\nall open sets (the Borel σ-algebra).\n\nChapter 2\nA Few Results in Matrix Analysis\nThis chapter collects a few results in linear algebra that will be useful in the rest of\nthis book.\n2.1\nNotation and basic facts\nWe denote by Mn,d(R) the space of all n × d matrices with real coefficients1. For a\nmatrix A ∈Mn,d(R) and integer k ≤n and l ≤d, we let A⌈kl⌉∈Mk,l(R) denote the\nmatrix A restricted to its first k rows and first l columns. The i,j entry of A will be\ndenoted A(i,j) or A(ij).\nWe assume that the reader is familiar with elementary matrix analysis, including,\nin particular the fact that symmetric matrices are diagonalizable in an orthonormal\nbasis, i.e., if A ∈Md,d(R) is a symmetric matrix (whose space is denoted Sd), there\nexists an orthogonal matrix U ∈Od (i.e., satisfying UT U = UUT = IdRd) and a diag-\nonal matrix D ∈Md,d(R) such that\nA = UDUT .\nThe identity AU = UD then implies that the columns of U form an orthonormal\nbasis of eigenvectors of A.\nIf A ∈S+\nd is positive semi-definite (i.e., uT Au ≥0 for all u ∈Rd), the entries of\nD in the decomposition A = UDUT are non-negative, and one can define the matrix\nsquare root of A as S = UD⊙1/2UT where D⊙1/2 is the diagonal matrix formed taking\nthe square roots of all coefficients of D. We will use the notation S = A1/2. Note that\nD1/2 = D⊙1/2 if D is diagonal and positive semi-definite.\nIf A ∈S++\nd\nis positive definite (i.e., A is positive semi-definite and uT Au = 0 im-\nplies u = 0) and B is positive semi-definite, both being d×d matrices, the generalized\n1Unless mentioned otherwise, all matrices are assumed to be real.\n25\n\n26\nCHAPTER 2. A FEW RESULTS IN MATRIX ANALYSIS\neigenvalue problem associated with A and B consists in finding a diagonal matrix D\nand a matrix U such that BU = AUD and UT AU = IdRd. Letting ˜U = A1/2U, the\nproblem is equivalent to solving A−1/2BA−1/2 ˜U = ˜UD with ˜UT ˜U = IdRd, i.e., finding\nthe eigenvalue decomposition of the symmetric positive-definite matrix A−1/2BA−1/2.\nIf A ∈Mn,d(R), it can be decomposed as\nA = UDV T\nwhere U ∈On(R) and V ∈Od(R)) are orthogonal matrices and D ∈Mn,d(R) is diago-\nnal (i.e., such that D(i,j) = 0 whenever i , j) with non-negative diagonal coefficients.\nThese coefficients are called the singular values of A, and the procedure is called\na singular-value decomposition (SVD) of A. An equivalent formulation is that there\nexist orthonormal bases u1,...,un of Rn and v1,...,vd of Rd (forming the columns of\nU and V ) such that\nAvi = λiui\nfor i ≤min(n,d), where λ1,...,λmin(n,d) are the singular values. Of course, if A is\nsquare and symmetric positive semi-definite, an eigenvalue decomposition of A is\nalso a singular value decomposition (and the singular values coincide with the eigen-\nvalues). More generally, if A = UDV T , then AAT = UDDT UT and AT A = V DT DV T\nare eigenvalue decompositions of AAT and AT A. Singular values are uniquely de-\nfined, up to reordering. However, the matrices U and V are not unique up to column\nreordering in general.\nIf m = min(n,d), then, forming the matrices ˜U = U⌈n,m⌉(resp.\n˜V = V⌈d,m⌉) by\nremoving from U (resp. V ) its last n −m (resp. d −m) columns , and ˜D = D⌈m,m⌉by\nremoving from D its n −m rows and d −m columns, one has\nA = ˜U ˜D ˜V T\nwith ˜U, ˜D and ˜V having respectively size n×m, m×m and m×d, ˜UT ˜U = ˜V T ˜V = IdRm\nand ˜D diagonal with non-negative coefficients. This representation provides a re-\nduced SVD of A and one can create a full SVD from a reduced one by completing the\nmissing rows of ˜U and ˜V to form orthogonal matrices, and by adding the required\nnumber of zeros to ˜D.\n2.2\nThe trace inequality\nWe now descibe Von Neumann’s trace theorem. Its justification follows the proof\ngiven in Mirsky [137].\nTheorem 2.1 (Von Neumann) Let A,B ∈Mn,d(R) have singular values (λ1,...,λm) and\n(µ1,...,µm), respectively, where m = min(n,d). Assume that these eigenvalues are listed\n\n2.2. THE TRACE INEQUALITY\n27\nin decreasing order so that λ1 ≥··· ≥λm and µ1 ≥··· ≥µm. Then,\ntrace(AT B) ≤\nm\nX\ni=1\nλiµi .\n(2.1)\nMoreover, if trace(AT B) = Pm\ni=1 λiµi, then there exist n × n and d × d orthogonal ma-\ntrices U and V such that UT AV and UT BV are both diagonal, i.e., one can find SVDs of\nA and B in the same bases of Rn and Rd.\nProof We can assume without loss of generality that d ≤n because, if the result\nholds for A and B, it also holds for AT and BT . Let A = U1ΛV T\n1 and B = U2MV T\n2 be\nthe singular values decompositions of A and B (both Λ and M are n × d matrices).\nThen\ntrace(AT B) = trace(V1ΛT UT\n1 U2MV2) = trace(ΛT UMV T )\nwith U = UT\n1 U2 and V = V T\n1 V2. Let u(i,j),1 ≤i,j ≤n and v(i,j),1 ≤i,j ≤d be the\ncoefficients of the orthogonal matrices U and V . Then\ntrace(ΛT UMV T ) =\nd\nX\ni,j=1\nu(i,j)v(i,j)λiµj ≤1\n2\nd\nX\ni,j=1\nλiµju(i,j)2 + 1\n2\nd\nX\ni,j=1\nλiµjv(i,j)2 (2.2)\nLet us consider the first sum in the upper-bound. Let ξd = λd (resp. ηd = µd) and\nξi = λi −λi+1 (resp. ηi = µi −µi+1) for i = 1,...,d −1. Since singular values are non-\nincreasing, we have ξi,ηi ≥0 and\nλi =\nd\nX\nj=i\nξj,\nµi =\nd\nX\nj=i\nηj\nfor i = 1,...,d. We have\nd\nX\ni,j=1\nλiµju(i,j)2 =\nd\nX\ni,j=1\nd\nX\ni′=i\nξi′\nd\nX\nj′=j\nηj′u(i,j)2 =\nd\nX\ni′,j′=1\nξi′ηj′\ni′\nX\ni=1\nj′\nX\nj=1\nu(i,j)2\n≤\nd\nX\ni′,j′=1\nξi′ηj′ min(i′,j′)\n(2.3)\nwhere we used the fact that U is orthogonal, which implies that Pj′\nj=1 u(i,j)2 and\nPi′\ni=1 u(i,j)2 are both less than 1. Notice also that, when u(i,j) = δij (i.e., u(i,j) = 1 if\ni = j and zero otherwise), then\ni′\nX\ni=1\nj′\nX\nj=1\nu(i,j)2 = min(i′,j′),\n\n28\nCHAPTER 2. A FEW RESULTS IN MATRIX ANALYSIS\nso that the last inequality is an identity, and the chain of equalities leading to (2.3)\nimplies\nd\nX\ni′,j′=1\nξi′ηj′ min(i′,j′) =\nd\nX\ni=1\nλiµj .\nWe therefore obtain (for any U), the fact that\nd\nX\ni,j=1\nλiµju(i,j)2 ≤\nd\nX\ni=1\nλiµj.\nThe same identity obviously holds with v in place of u, and combining the two yields\n(2.1).\nWe now consider conditions for equality. Clearly, if one can find SVD decompo-\nsitions of A and B with U1 = U2 and V1 = V2, then U = IdRn, V = IdRd and (2.1) is an\nidentity. We want to prove the converse statement.\nFor (2.1) to be an equality, we first need (2.2) to be an identity, which requires that\nu(i,j) = v(i,j) as soon as λiµj > 0. We also need an equality in (2.3), which requires\ni′\nX\ni=1\nj′\nX\nj=1\nu(i,j)2 = min(i′,j′)\nas soon as λi′ > λi′+1 and µj′ > µj′+1. The same identity must be true with v(i,j)\nreplacing u(i,j)\nIn view of this, denote by i1 < ··· < ip (resp. j1 < ··· < jq) the indexes at which\nthe singular values of A (resp. B) differ form their successors, with the convention\nλd+1 = µd+1 = 0. Let, for k = 1,...,p and l = 1,...,q\nC(k,l) =\nik\nX\ni=1\njl\nX\nj=1\nu(i,j)2.\nThen, we must have C(k,l) = min(ik,jl) for all k,l and u(i,j) = v(i,j) for i = 1,...,ip\nand j = 1,...,jq.\nIf, for all i,j ≤d, we let U⌈ij⌉be the matrix formed by the first i rows and j\ncolumns of U, the condition Ckl = min(ik,jl) requires that U⌈ikjl⌉UT\n⌈ikjl⌉= IdRik if ik ≤jl\nand UT\n⌈ikjl⌉U⌈ikjl⌉= IdRjl if jl ≤ik. This shows that, if ik ≤jl, the rows of U⌈ikjl⌉form an\northonormal family, and necessarily, all elements u(i,j) for i ≤ik and j > jl vanish.\nThe symmetric situation holds if jl ≤ik.\n\n2.2. THE TRACE INEQUALITY\n29\nLet rk = ik −ik−1 and sl = jl −jl−1 (with i0 = j0 = 0). We now consider possible\nchanges in the SVDs of A and B. With our notation, the matrix Λ takes the form\nΛ =\n\n\nλi1IdRr1\n0\n0\n···\n0\n0\n...\n0\n0\nλi2IdRr2\n0\n···\n0\n0\n...\n0\n...\n...\n...\n...\n...\n0\n0\n...\nλipIdRrp\n0\n0\n...\n0\n0\n...\n0\n0\n...\n0\n...\n...\n...\n...\n0\n...\n0\n0\n...\n0\n\n\nLet W, ˜W be n × n and d × d orthogonal matrices taking the form\nW =\n\n\nW1\n0\n0\n···\n0\n0\nW2\n0\n···\n0\n...\n...\n...\n0\n0\n...\nWp\n0\n0\n...\nWp+1\n\n\n, ˜W =\n\n\nW1\n0\n0\n···\n0\n0\nW2\n0\n···\n0\n...\n...\n...\n0\n0\n...\nWp\n0\n0\n...\n˜Wp+1\n\n\nwhere W1,...,Wp are orthogonal with respective sizes r1,...,rp, Wp+1 is orthogonal\nwith size n −ip and ˜Wp+1 is orthogonal with size d −ip. Then we have\nWD ˜W = D\nproving that U1 can be replaced by U1W provided that V1 is replaced by V1 ˜W. Sim-\nilar transformations can be made on U1 and V2, with U2 replaced by U2Z and V2 by\nV2 ˜Z with\nZ =\n\n\nZ1\n0\n0\n···\n0\n0\nZ2\n0\n···\n0\n...\n...\n...\n0\n0\n...\nZq\n0\n0\n...\nZq+1\n\n\n,\n˜Z =\n\n\nZ1\n0\n0\n···\n0\n0\nZ2\n0\n···\n0\n...\n...\n...\n0\n0\n...\nZq\n0\n0\n...\n˜Zq+1\n\n\nwith a structure similar to W and ˜W, replacing r1,...,rp by s1,...,sq. As a conse-\nquence, U = UT\n1 U2 can be replaced by W T UZ and V by ˜W T V ˜Z. To complete the\nproof, we need to show that, when (2.1) is an equality, these matrices can be chosen\nso that W T UZ = IdRn and ˜W T V ˜Z = IdRd.\nLet us consider a first step in this direction, assuming that i1 ≤j1 so that\nU[i1j1]UT\n⌈i1j1⌉= IdRi1.\n\n30\nCHAPTER 2. A FEW RESULTS IN MATRIX ANALYSIS\nComplete UT\n⌈i1j1⌉into a orthogonal matrix Z1 = [UT\n⌈i1j1⌉, ˜U]. Build a matrix Z as above\nby taking Z2, . . . , Zq+1 equal to the identity. Then UZ has a first i1 ×i1 block equal to\nIdRi1, which implies that all coefficients on the right and below this block are zeros.\nIf j1 ≤i1, a similar construction can be made on the other side, letting W1 = [U⌈i1j1⌉˜U]\nwith the first j1 × j1 block of the new matrix U equal to the identity. Note that, since\nV⌈ipjq⌉= U⌈ipjq⌉, the same result is obtained on V at the same time.\nPursuing this way (and skipping the formal induction argument, which is a bit\ntedious), we can progressively introduce identity blocks into U and V and transform\nthem into new matrices (that we still denote by U and V ) taking the form (letting\nk = min(ip,jq))\nU =\n \nIdRk\n0\n0\n¯U\n!\nand V =\n \nIdRk\n0\n0\n¯V\n!\nIf k = ip (resp. k = jq), the final reduction can be obtained by choosing Wp+1 = ¯U\nand ˜Wp+1 = ¯V (resp. Zp+1 = ¯UT and ˜Zp+1 = ¯V T ), leading to SVDs for A and B with\nidentical matrices U1 = U2 and V1 = V2.\n■\nRemark 2.2 Note that, since the singular values of −A and of A coincide, theorem 2.1\nimplies\n\f\f\ftrace(AT B)\n\f\f\f ≤\nm\nX\ni=1\nλiµi .\n(2.4)\nfor all matrices A and B, with equality if either A and B or −A and B have an SVD\nusing the same bases.\n♦\n2.3\nApplications\nLet p and d be integers with p ≤d. Let A ∈Sd(R), B ∈Sp(R) be symmetric ma-\ntrices. We consider the following optimization problem: maximize, over matrices\nU ∈Md,p(R) such that UT U = IdRp, the function\nF(U) = trace(UT AUB) = trace(AUBUT ).\nWe first note that the singular values of UBUT , which is d × d, are the same as the\neigenvalues of B completed with zeros. Letting λ1 ≥··· ≥λd be the eigenvalues of A\nand µ1 ≥··· ≥µp those of B, we therefore have, from theorem 2.1,\nF(U) ≤\np\nX\ni=1\nλiµi.\n\n2.3. APPLICATIONS\n31\nIntroduce the eigenvalue decompositions of A and B in the form A = V ΛV T and\nB = WMW T . For F(U) to be equal to its upper-bound, we know that we must arrange\nUBUT to take the form\nUBUT = V\n \nM\n0\n0\n0\n!\nV T .\nUse, as before, the notation V⌈dp⌉to denote the matrix formed with the p first columns\nof V . Take U = V⌈dp⌉W T , which satisfies UT U = IdRp. We then have\nV⌈dp⌉W T BWV T\n⌈dp⌉= V⌈dp⌉MV T\n⌈dp⌉= V\n \nM\n0\n0\n0.\n!\nV T ,\nwhich shows that U is optimal. We summarize this discussion in the next theorem.\nTheorem 2.3 Let A ∈Sd(R) and B ∈Sp(R) be symmetric matrices, with p ≤d. Let\neigenvalue decompositions of A and B be given by A = V ΛV T and B = WMW T , where\nthe diagonal elements of Λ (resp. M) are λ1 ≥··· ≥λd (resp. µ1 ≥··· ≥µp).\nDefine F(U) = trace(AUBUT ), for U ∈Md,p(R). Then,\nmax\nn\nF(U) : UT U = IdRp\no\n=\np\nX\ni=1\nλiµi.\nThis maximum is attained at\nU = V⌈d,p⌉W T .\nThe following corollary applies theorem 2.3 with B = diag(µ1,...,µp).\nCorollary 2.4 Let A ∈Sd(R) be a symmetric matrix with eigenvalues λ1 ≥··· ≥λd. For\np ≤d, let µ1 ≥··· ≥µp > 0 and define\nF(e1,...,ep) =\np\nX\ni=1\nµieT\ni Aei.\nThen, the maximum of F over all orthonormal families e1,...,ep in Rd is Pp\ni=1 λiµi and is\nattained when e1,...,ep are eigenvectors of A with eigenvalues λ1,...,λp.\nThe minimum of F over all orthonormal families e1,...,ep in Rd is Pp\ni=1 λd−i+1µi and\nis attained when e1,...,ep are eigenvectors of A with eigenvalues λd,...,λd−p+1.\nProof The statement about the maximum is just a special case of theorem 2.3, with\nB = diag(µ1,...,µp), noting that the ith diagonal element of UT AU is eT\ni Aei where ei\nis the ith column of U.\nThe statement about the minimum is deduced by replacing A by −A.\n■\n\n32\nCHAPTER 2. A FEW RESULTS IN MATRIX ANALYSIS\nApplying this corollary with p = 1, we retrieve the elementary result that λ1 =\nmax{uT Au : |u| = 1} and λd = min{uT Au : |u| = 1}.\nTo complete this chapter, we quickly state and prove Rayleigh’s theorem.\nTheorem 2.5 Let A ∈Md,d(R) be a symmetric matrix with eigenvalues λ1 ≥··· ≥λd.\nThen\nλk =\nmax\nV :dim(V )=k min{uT Au,u ∈V ,|u| = 1} =\nmin\nV :dim(V )=d−k+1max{uT Au,u ∈V ,|u| = 1}\nwhere the min and max are taken over linear subspaces of Rd.\nProof Let e1,...,ed be an orthonormal basis of eigenvectors of A associated with\nλ1,...,λd. Let, for k ≤l, Wk,l = span(ek,...,el). Let V be a subspace of dimension k.\nThen V ∩Wk,d , ∅(because the sum of the dimensions of these two spaces is d + 1).\nTaking u0 with norm 1 in this intersection, we have\nmin{uT Au,u ∈V ,|u| = 1} ≤uT\n0 Au0 ≤max{uT Au,u ∈Wk,d,|u| = 1} = λk,\nwhere the last identity follows by considering the eigenvalues of A restricted to Wk,d.\nSo, the maximum of the right-hand side is indeed less than λk, and it is attained for\nV = W1,k. This proves the first identity, and the second one can be obtained by\napplying the first one to −A.\n■\n2.4\nSome matrix norms\nThe operator norm of a matrix A ∈Mn,d(R), is defined as\n|A|op = max{|Ax| : x ∈Rd,|x| = 1}.\nIt is equal to the square root of the largest eigenvalue of AT A, i.e., to the largest\nsingular value of A.\nThe Frobenius norm of A is\n|A|F =\nq\ntrace(AT A) =\nv\nu\nu\nt\nd\nX\ni,j=1\nA(i,j)2,\nso that\n|A|F =\n\n\nm\nX\nk=1\nσ2\nk\n\n\n1/2\n\n2.4. SOME MATRIX NORMS\n33\nwhere σ1,...,σm are the singular values of A (and m = min(n,d)).\nThe nuclear norm of A is defined by\n|A|∗=\nd\nX\nk=1\nσk .\nOne can prove that this is a norm using an equivalent definition, provided by the\nfollowing proposition.\nProposition 2.6 Let A be an n by d matrix. Then\n|A|∗= max\n\u001a\ntrace(UAV T ) : U ∈Mn,n and UT U = Id,V ∈Md,d and V T V = Id\n\u001b\n.\nProof The fact that trace(UAV T ) ≤|A|∗for any U and V is a consequence of the\ntrace inequality applied with B = [Id,0] or its transpose depending on whether n ≤d\nor not. The upper-bound being attained when U and V are the matrices forming the\nsingular value decomposition of A, the proof is complete.\n■\nThe fact that |A|∗is a norm, for which the only non-trivial fact was the triangular\ninequality, now is an easy consequence of this proposition, because the maximum\nof the sum of two functions is always less than the sum of their maximums. More\nprecisely, we have\n|A + B|∗= max{trace(UAV T ) + trace(UBV T ) :\nUT U = Id,V T V = Id}\n≤max{trace(UAV T ) : UT U = Id,V T V = Id}\n+ max{trace(UBV T ) : UT U = Id,V T V = Id}\n= |A|∗+ |B|∗\nThe nuclear norms is also called the Ky Fan norm of order d. Ky Fan norms of\norder k (for 1 ≤k ≤d) associate to a matrix A the quantity\n|A|(k) = λ1 + ··· + λk,\ni.e., the sum of its k largest singular values. One has the following proposition.\nProposition 2.7 The Ky Fan norms satisfy the triangular inequality.\nProof We prove this following the argument suggested in Bhatia [28]. For A ∈Md,d,\nand k = 1,...,d, let trace(k)(A) be the sum of the k largest diagonal elements of A. Let,\n\n34\nCHAPTER 2. A FEW RESULTS IN MATRIX ANALYSIS\nfor a symmetric matrix A, |A|′\n(k) denote the sum of the k largest eigenvalues of A (it\nis equal to |A|(k) if A is positive definite, but can also include negative values).\nThen, for any symmetric matrix A ∈Sd,\n|A|′\n(k) = max\nn\ntrace(k)(UAUT ) : U ∈COd\no\n.\n(2.5)\nTo show this, assume that V in Od diagonalizes A, so that D = V AV T is a diagonal\nmatrix. Assume, without loss of generality, that the coefficients λj = D(j,j) are non-\nincreasing. Fix U ∈Od, let B = UAUT and W = V UT so that D = WBW T , or B =\nW T DW. Then, for any j ≤d,\nB(j,j) =\nd\nX\ni=1\nW(i,j)2D(i,i).\nThen, for any 1 ≤j1 < ··· < jk ≤d\nk\nX\nl=1\nB(jl,jl) =\nd\nX\ni=1\nD(i,i)\nk\nX\nl=1\nW(i,jl)2\n=\nk\nX\ni=1\nD(i,i) +\nk\nX\ni=1\nD(i,i)\n\u0010\nk\nX\nl=1\nW(i,jl)2 −1\n\u0011\n+\nd\nX\ni=k+1\nD(i,i)\nk\nX\nl=1\nW(i,jl)2\n=\nk\nX\ni=1\nD(i,i) +\nk\nX\ni=1\n(D(i,i) −D(k,k))\n\u0010\nk\nX\nl=1\nW(i,jl)2 −1\n\u0011\n+\nd\nX\ni=k+1\n(D(i,i) −D(k,k))\nk\nX\nl=1\nW(i,jl)2 + D(k,k)\n\n\nn\nX\ni=1\nk\nX\nj=1\nW(i,jl)2 −k\n\n.\nBecause W is orthogonal, we have Pk\nl=1 W(i,jl)2 ≤1 and\nn\nX\ni=1\nk\nX\nj=1\nW(i,jl)2 = k.\nThis shows that the terms after Pk\ni=1 D(i,i) in the upper bound are negative or zero,\nso that\nk\nX\nl=1\nB(jl,jl) ≤\nk\nX\ni=1\nD(i,i).\nThe maximum of the left-hand side is trace(k)(B). Noting that we get an equality\nwhen choosing U = V , the proof of (2.5) is complete.\n\n2.4. SOME MATRIX NORMS\n35\nUsing the same argument as that made above for the nuclear norm, one deduces\nfrom this that\n|A + B|′\n(k) ≤|A|′\n(k) + |B|′\n(k)\nfor all A,B ∈Sd and all k = 1,...,d.\nNow, let A ∈Mn,d and consider the symmetric matrix\n˜A =\n \n0\nAT\nA\n0\n!\n∈Sn+d.\nWrite a vector u ∈Rn+d as u =\n \nu1\nu2\n!\nwith u1 ∈Rd and u2 ∈Rn. Then u is an eigen-\nvector of ˜A for an eigenvalue λ if and only if AT u2 = λu1 and Au1 = λu2, which\nimplies that AT Au1 = λ2u1 and λ2 is a singular value of A. Conversely, if µ is a\nnonzero singular value of A, associated with eigenvector u1, then 1/√µ and −1/√µ\nare eigenvalues of ˜A, associated with eigenvectors\n \nu1\n±Au1/√µ\n!\n. It follows from this\nthat |A|(k) = | ˜A|′\n(k) for k ≤min(n,d) and therefore satisfies the triangle inequality.\n■\nWe refer to [28] for more examples of matrix norms, including, in particular those\nprovided by taking pth powers in Ky Fan’s norms, defining\n|A|(k,p) = (λp\n1 + ··· + λp\nk)1/p.\n\n36\nCHAPTER 2. A FEW RESULTS IN MATRIX ANALYSIS\n\nChapter 3\nIntroduction to Optimization\nThis chapter summarizes some fundamental concepts in optimization that will be\nused later in the book. The reader is referred to textbooks, such as Beck [22], Eiselt\net al. [68], Nocedal and Wright [146], Boyd et al. [40] and many others for proofs and\ndeeper results.\n3.1\nBasic Terminology\n1. If I is a subset of R, a lower bound of I is an element u ∈[−∞,+∞] such that u ≤x\nfor all x ∈I. Among these lower bounds, there exists a largest element, denoted\ninfI ∈[−∞,+∞], called the infimum of I (by convention, the infimum of an empty\nset is +∞). Similarly, one defines the supremum of I, denoted supI, as the smallest\nupper bound of I (and the supremum of an empty set is −∞). Every set in R has an\ninfimum and a supremum, but these numbers do not necessarily belong to I. When\nthey do, they are respectively called minimal and maximal elements of I, and are\ndenoted minI and maxI. So, the statement “u = minI” means u ∈I and u ≤v for all\nv ∈I.\n2. If F : Ω→R is a real-valued function defined on a subset Ω⊂Rd, the infimum\nof F over Ωis defined by\ninf\nΩF = inf{F(x) : x ∈Ω}\nand its supremum is\nsup\nΩ\nF = sup{F(x) : x ∈Ω}.\nAs seen above both numbers are well defined, and can take infinite values. One says\nthat x ∈Ωis a (global) minimizer (resp. maximizer) of F if F(y) ≥F(x) (resp. F(y) ≤\nF(x)) for all y ∈Ω. One also says that F reaches its minimum (resp. maximum), or is\nminimized (resp. maximized) at x. Equivalently, x is a minimizer (resp. maximizer)\nof F if and only if x ∈Ωand\nF(x) = min{F(y) : y ∈Ω} (resp. max{F(y) : y ∈Ω}).\n37\n\n38\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nIn such cases, one also writes F(x) = minΩF or F(x) = maxΩF. In particular, the\nnotation u = minΩF indicates that u = infΩF and that there exists an x in Ωsuch\nthat F(x) = u (i.e., that the infimum of F over Ωis realized at some x ∈Ω). Note\nthat the infimum of a function always exists, but not necessarily its minimum. Also\nnote that minimizers, when they exist, are not necessarily unique. We will denote\nby argminΩF (resp. argmaxΩF) the (possibly empty) set of minimizers (resp. maxi-\nmizers) of F\n3. One says that x is a local minimizer (resp. maximizer) of F on Ωif there exists an\nopen ball B ⊂Rd such that x ∈B and F(x) = minΩ∩B F (resp. F(x) = maxΩ∩B F).\n4. An optimization problem consists in finding a minimizer or maximizer of an “ob-\njective function” F. Focusing from now on on minimization problems (statements\nfor maximization problems are symmetric), we will always implicitly assume that\na minimizer exists. The following provides some general assumptions on F and Ω\nthat ensure this fact.\nThe sublevel sets of F in Ωare denoted [F ≤u]Ω(or simply [F ≤u] when Ω= Rd)\nfor u ∈[−∞,+∞] with\n[F ≤u]Ω= {x ∈Ω: F(x) ≤u}.\nNote that\nargmin\nΩ\nF =\n\\\nu>infF\n[F ≤u]Ω.\nA typical requirement for F is that its sublevel sets are closed in Rd, which means\nthat, if a sequence (xn) in Ωsatisfies, for some u ∈R, F(xn) ≤u for all n and converges\nto a limit x, then x ∈Ωand F(x) ≤u. If this is true, one says that F is lower semi-\ncontinuous, or l.s.c, on Ω. If, in addition to being closed, the sublevel sets of F are\nbounded (at least for u small enough—larger than infF), then argminΩF is an inter-\nsection of nested compact sets, and is therefore not empty (so that the optimization\nproblem has at least one solution).\n5. Different assumptions on F and Ωlead to different types of minimization prob-\nlems, with specific underlying theory and algorithms.\n1. If F is C1 or smoother and Ω= Rd, one speaks of an unconstrained smooth\noptimization problem.\n2. For constrained problems, Ωis often specified by a finite number of inequali-\nties, i.e.,\nΩ= {x ∈Rd : γi(x) ≤0,i = 1,...,q}.\nIf F and all functions γ1,...,γq are C1 one speaks of smooth constrained problems.\n3. If Ωis a convex set (i.e., x,y ∈Ω⇒[x,y] ∈Ω, where [x,y] is the closed line\nsegment connecting x and y) and F is a convex function (i.e., F((1 −t)x + ty) ≤(1 −\nt)F(x) + tF(y) for all x,y ∈Ω), one speaks of a convex optimization problem.\n\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n39\n4. Non-smooth problems are often considered in data science, and lead to inter-\nesting algorithms and solutions.\n5. When both F and γ1,...,γq are affine functions, one speaks of a linear program-\nming problem (or a linear program). (An affine function is a mapping x 7→bT x + β,\nb ∈Rd, β ∈R.)\nIf F is quadratic (F(x) = 1\n2xT Ax −bT x), and all γi’s are affine, one speaks of a\nquadratic programming problem.\n6. Finally, some machine learning problems are specified over discrete or finite\nsets Ω(for example Zd, or {0,1}d), leading to combinatorial optimization problems.\n3.2\nUnconstrained Optimization Problems\n3.2.1\nConditions for optimality (general case)\nConsider a function F : Ω→R where Ωis an open subset of Rd. We first discuss the\nunconstrained optimization problem of finding\nx∗∈argmin\nΩ\nF.\n(3.1)\nThe following result summarizes (non-identical) necessary and sufficient conditions\nthat are applicable to such a solution.\nTheorem 3.1 Necessary conditions. Assume that F is differentiable over Ω, and that\nx∗is a local minimum of F. Then ∇F(x∗) = 0.\nIf F is C2, then, in addition, ∇2F(x∗) must be positive semidefinite.\nSufficient conditions. Assume that F ∈C2(Ω). If x∗∈Ωis such that ∇F(x∗) = 0 and\n∇2F(x∗) is positive definite, then x∗is a local minimum of F.\nProof Necessary conditions: Since Ωis open, it contains an open ball centered at\nx∗, with radius ϵ0 and therefore all segments [x∗,x∗+ϵh] for all ϵ ∈[0,ϵ0] and all unit\nnorm vectors h. Since x∗is a local minimum, we can choose ϵ0 so that F(x∗+ ϵh) ≥\nF(x∗) for all h with |h| = 1.\nUsing Taylor formula, we get (for ϵ ∈[0,ϵ0], |h| = 1)\n0 ≤F(x∗+ ϵh) −f (x∗) = ϵ\nZ 1\n0\ndF(x∗+ tϵh)hdt .\nIf dF(x∗)h , 0 for some h, then, for small enough ϵ, dF(x∗+ tϵh)h cannot change sign\nfor t ∈[0,1] and therefore\nR 1\n0 dF(x∗+ tϵh)hdt has the same sign as dF(x∗)(h) which\nmust therefore be positive. But the same argument can be made with h replaced by\n\n40\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\n−h, implying that dF(x∗)(−h) = −dF(x∗)h is also positive, and this gives a contradic-\ntion. We therefore have dF(x∗)(h) = 0 for all h, i.e., ∇F(x∗) = 0.\nAssume that F is C2. Then, making a second-order Taylor expansion, one gets\n0 ≤F(x∗+ ϵh) −F(x∗) = ϵ2\nZ 1\n0\n(1 −t)d2F(x∗+ tϵh)(h,h)dt.\nThe same argument as above shows that, if d2F(x∗)(h,h) , 0, then it must be posi-\ntive. This shows that d2F(x∗)(h,h) ≥0 for all h and d2F(x∗) (or its associated matrix\n∇2F(x∗)) is positive semidefinite.\nNow, assume that F is C2 and ∇2F(x∗) positive definite. One still has\nF(x∗+ ϵh) −F(x∗) = ϵ2\nZ 1\n0\n(1 −t)d2F(x∗+ tϵh)(h,h)dt\nIf ∇2F(x∗) ≻0, then ∇2F(x∗+ tϵh) ≻0 for small enough ϵ, showing the the r.h.s. of\nthe identity is positive for h , 0, and that F(x∗+ ϵh) > F(x∗).\n■\nBecause maximizing F is the same as minimizing −F, necessary (resp. sufficient)\nconditions for optimality in maximization problems are immediately deduced from\nthe above: it suffices to replace positive semidefinite (resp. positive definite) by\nnegative semidefinite (resp. negative definite).\n3.2.2\nConvex sets and functions\nDefinition 3.2 One says that a set Ω⊂Rd is convex if and only if, for all x,y ∈Ω, the\nclosed segment [x,y] also belongs to Ω.\nA function F : Rd →(−∞,+∞] is convex if, for all λ ∈[0,1] and all x,y ∈Rd, one has\nF((1 −λ)x + λy) ≤(1 −λ)F(x) + λF(y).\n(3.2)\nIf, whenever the lower bound is not infinite, the inequality above is strict for λ ∈(0,1),\none says that F is strictly convex.\nNote that, with our definition, convex functions can take the value +∞but not\n−∞. In order for the upper-bound to make sense when F takes infinite values, one\nmakes the following convention: a + (+∞) = +∞for any a ∈(−∞,+∞]; λ · (+∞) = +∞\nfor any λ > 0; 0 · (+∞) is not defined but 0 · (+∞) + (+∞) = +∞.\n\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n41\nDefinition 3.3 The domain of F, denoted dom(F) is the set of x ∈Rd such that F(x) < ∞.\nOne says that F is proper if dom(F) , ∅.\nWe will only consider proper convex functions in our discussions, which will simply\nbe referred to as convex functions for brevity.\nProposition 3.4 If F is a convex function, then dom(F) is a convex subset of Rd. Con-\nversely, if Ωis a convex set and F satisfies (3.2) for all x,y ∈Ω(i.e., F is convex on Ω),\nthen the extension ˆF defined by ˆF(x) = F(x) if x ∈Ωand ˆF(x) = +∞is a convex function\ndefined on Rd (such that dom( ˆF) = Ω).\nProof The first statement is a direct consequence of (3.2), which implies that F is\nfinite on [x,y] as soon as it is finite at x and at y. For the second statement, (3.2) for\nˆF is true for x,y ∈Ω, since it is true for F, and the uper-bound is +∞otherwise.\n■\nThis proposition shows that there was no real loss of generality in requiring convex\nfunctions to be defined on the full space Rd. Note also that the upper bound in (3.2)\nis infinite unless both x and y belong to dom(F), so that the inequality only needs to\nbe checked in that case.\nOne says that a function F is concave if and only if −F is convex. All definitions\nand properties made for convex functions then easily transcribe into similar state-\nments for concave functions. We say that a function f : I →(−∞,+∞] (where I is an\ninterval) is non-decreasing if, for all x,y ∈I, x < y implies f (x) ≤f (y). We say that f\nis increasing if if, for all x,y ∈I, x < y implies f (x) < f (y) if f (x) < ∞and f (y) = ∞\notherwise.\nInequality (3.2) has important consequences on minimization problems. For ex-\nample, it implies the following proposition.\nProposition 3.5 Let F be a convex (resp. strictly convex) function on Rd. If x ∈dom(F)\nand y ∈Rd, the function\nλ ∈(0,1] 7→1\nλ(F((1 −λ)x + λy) −F(x))\n(3.3)\nis non-decreasing (resp. increasing).\nConversely, let Ω⊂Rd be a convex set and F : Ω→(−∞,+∞) be a function such that\nthe expression in (3.3) is non-decreasing (resp. increasing) for all x ∈dom(F) and y ∈Rd.\nThen, the extension ˆF of F defined in proposition 3.4 is convex (resp. strictly convex).\nProof Let f (λ) = (F((1 −λ)x + λy) −F(x))/λ. Let µ ≤λ denote zλ = (1 −λ)x + λy,\nzµ = (1 −µ)x + µy. One has zµ = (1 −ν)x + νzλ, with ν = µ/λ, so that\nF(zµ) ≤(1 −µ/λ)F(x) + (µ/λ)F(zλ).\n\n42\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nSubtracting F(x) to both sides (which is allowed since F(x) < ∞) and dividing by µ\nyields\nf (µ) ≤f (λ).\nIf F is strictly convex, then, either F(zµ) = ∞, in which case f (µ) = f (λ) = ∞, or\nF(zµ) < (1 −µ/λ)F(x) + (µ/λ)F(zλ).\nas soon as 0 < µ < λ, yielding\nf (µ) < f (λ).\nNow consider the converse statement. By comparing the expression in (3.3) to\nthat obtained with λ = 1, we find, for all x,y ∈Ω\n1\nλ(F((1 −λ)x + λy) −F(x)) ≤F(y) −f (x)\nwhich is (3.2). Since ˆF satisfies (3.2) in its domain, it is convex. If the function in\n(3.3) is increasing, then the inequality is strict for 0 < λ < 1 as soon as the lower\nbound is finite, and F is strictly convex.\n■\nCorollary 3.6 If F is convex, any local minimum of F is a global minimum.\nProof If x is a local minimum of F, then, obviously, x ∈dom(F), and for any y ∈Rd\nand small enough µ > 0, F(x) ≤F((1 −µ)x + µy). Using the function in (3.3) for λ = µ\nand for λ = 1, we get\n0 ≤1\nµ(F((1 −µ)x + µy) −F(x)) ≤F(y) −F(x)\nso that x is a global minimum.\n■\n3.2.3\nRelative interior\nIf Ωis convex, then ˚Ωand ¯Ω(its topological interior and closure) are convex too (the\neasy proof is left to the reader). However, topological interiors of interesting convex\nsets are often empty, and a more adapted notion of relative interior is preferable.\nDefine the affine hull of a set Ω, denoted aff(Ω), as the smallest affine subset of\nRd that contains Ω. The vector space parallel to aff(Ω) (generated by all differences\nx −y, x,y ∈Ω) will be denoted −−→\naff (Ω). Their dimension k, is the largest integer such\nthat there exist x0,x1,...,xk ∈Ωsuch that x1 −x0,...,xk −x0 are linearly indepen-\ndent. Moreover, given these points, elements of the affine hull are defined through\nbarycentric coordinates, yielding\naff(Ω) = {x = λ(0)x0 + ··· + λ(k)xk :,λ(0) + ··· + λ(k) = 1}.\n\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n43\nThe coordinates (λ(0),...,λ(k)) are uniquely associated to x ∈aff(Ω) and depend con-\ntinuously on x. They are indeed obtained by solving the linear system\nx −x0 = λ(1)(x1 −x0) + ··· + λ(k)(xk −x0)\nwhich has a unique solution for x ∈aff(Ω) by linear independence. To see continuity,\none can introduce the k × k matrix G with entries G(ij) given by the inner products\n(xi −x0)T (xj −x0) and the vector h(x) ∈Rk with entries h(j)(x) = (x −x0)T (xj −x0).\nContinuity is then clear since λ = G−1h(x).\nDefinition 3.7 If Ωis a convex set, then its relative interior, denoted relint(Ω), is the\nset of all x ∈Ωsuch that there exists ϵ > 0 such that aff(Ω) ∩B(x,ϵ) ⊂Ω.\nWe have the following important property.\nProposition 3.8 Let Ωbe a nonempty convex set. If x ∈relint(Ω) and y ∈Ω, then\nxλ = (1 −λ)x + λy ∈relint(Ω) for all λ ∈[0,1).\nMoreover relint(Ω) is a nonempty convex set.\nProof Take ϵ such that B(x,ϵ) ∩aff(Ω) ⊂Ω. Take any z ∈B(xλ,(1 −λ)ϵ) ∩aff(Ω).\nDefine ˜z such that z = (1 −λ)˜z + λy, i.e.\n˜z = z −λy\n1 −λ .\nThen ˜z ∈aff(Ω) and\n|˜z −x| = |z −xλ|\n1 −λ < ϵ\nso that ˜z, and therefore z belongs to Ω. This proves that B(xλ,(1 −λ)ϵ) ∩aff(Ω) ⊂Ω\nso that xλ ∈relint(Ω).\nIf both x and y belong to relint(Ω), then xλ ∈relint(Ω) for λ ∈[0,1], showing that\nthis set is convex.\nWe now show that relint(Ω) , ∅. Let k be the dimension of aff(Ω), so that there\nexist x0,x1,...,xk ∈Ωsuch that x1 −x0,...,xk −x0 are linearly independent. Consider\nthe “simplex”\nS = {λ(0)x0 + ··· + λ(k)xk :,λ(0) + ··· + λ(k) = 1,λ(j) ≥0,j = 0,...,k},\nwhich is included in Ω.\nThen the average x = (x0 + ··· + xk)/(k + 1) is such that\nB(x,ϵ) ∩aff(Ω) ⊂S for small enough ϵ. Otherwise, there would exist a sequence\ny(n) = λ(0)(n)x0 + ··· + λ(k)(n)xk such that λ(0)(n) + ··· + λ(k)(n) = 1 and at least one\nλ(j)(n) < 0 that converges to x. Let yj be the set of elements in this sequence such\n\n44\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nthat λ(j)(n) < 0. This set is infinite for at least one j and provides a subsequence of\ny that also converges to x. But this would imply that the jth barycentric coordinate,\nwhich depends continuously on x, is non-positive, which is a contradiction.\nWe therefore have x ∈relint(Ω), which completes the proof.\n■\nThe following proposition provides an equivalent definition of the relative inte-\nrior.\nProposition 3.9 If Ωis a convex set, then\nrelint(Ω) = {x ∈Ω: ∀y ∈Ω,∃ϵ > 0 such that x −ϵ(y −x) ∈Ω}.\n(3.4)\nSo x belongs in the relative interior of Ωif, for all y ∈Ω, the segment [x,y] can be\nextended on the x side and still remain included in Ω.\nProof Let A be the set in the r.h.s. of (3.4). The proof that relint(Ω) ⊂A is straight-\nforward and left to the reader. We consider the reverse inclusion.\nLet x ∈A, and let y ∈relint(Ω), which is not empty. Then, for some ϵ > 0, we have\nz = x −ϵ(y −x) ∈Ω.\nSince\nx =\n1\n1 + ϵ(ϵy + z),\nproposition 3.8 implies that x ∈relint(Ω).\n■\nConvex functions have important regularity properties in the relative interior of\ntheir domain, that we will denote ridom(F). Importantly:\nridom(F) = relint(dom(F)) , int(dom(F)).\nA first such property is provided by the next proposition.\nProposition 3.10 Let F be a convex function. Then F is locally Lipschitz continuous on\nridom(F), i.e., for every compact subset C ⊂ridom(F), there exists a constant L > 0 such\nthat |F(x) −F(y)| ≤L|x −y| for all x,y ∈C.\nThis implies, in particular, that F is continuous on ridom(F).\nProof Take x ∈ridom(F). Let K =\n\u001a\nh ∈−−→\naff (dom(F)),|h| = 1\n\u001b\n. Then, the segment\n[x −ah,x + ah] is included in ridom(F) for small enough a and all h ∈K. Since F is\nconvex, we have, for t ≤a,\nF(x + th) −F(x) ≤t\na(F(x + ah) −F(x))\n\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n45\nWriting x = λ(x −ah) + (1 −λ)(x + th) with λ = t/(t + a), we also have\nF(x) ≤\nt\nt + a(F(x −ah) +\na\nt + aF(x + th))\nwhich can be rewritten as\nF(x) −F(x + th) ≤t\na(F(x −ah) −F(x)).\nThese two inequalities show that F is continuous at x along any direction in −−→\naff (dom(F)),\nwhich implies that F is continuous at x. Given this, the differences F(x+ah)−F(x) are\nbounded over the compact set C, by some constant M and, the previous inequalities\nshow that\n|F(y) −F(x)| ≤M\na |x −y|\nif y ∈ridom(F), |y −x| ≤a.\n■\n3.2.4\nDerivatives of convex functions and optimality conditions\nThe following theorem provides a stronger version of optimality conditions for the\nminimization of differentiable convex functions. Note that we have only defined\ndifferentiability of functions defined over open sets.\nTheorem 3.11 Let F be a convex function, with int(dom(F)) , ∅. Assume that x ∈\nint(dom(F)) and that F is differentiable at x. Then, for all y ∈Rd:\n∇F(x)T (y −x) ≤F(y) −F(x).\n(3.5)\nIf F is strictly convex, the inequality is strict for y , x. In particular, ∇F(x) = 0 implies\nthat x is a global minimizer of F. It is the unique minimizer if F is strictly convex.\nConversely, if F is C1 on an open convex set Ωand satisfies (3.5) for all x,y ∈Ω, then\nF is convex.\nProof Equation (3.3) implies\n1\nλ(F((1 −λ)x + λy) −F(x)) ≤F(y) −F(x),0 < λ ≤1.\nTaking the limit of the lower bound for λ →0, λ > 0 yields (3.5). If F is strictly\nconvex, the inequality is strict for λ < 1 and, since the l.h.s. is increasing in λ, it\nremains strict when λ ↓0.\nConversely, assuming (3.5) for all x,y ∈Ω, the derivative of λ 7→1\nλ(F((1 −λ)x +\nλy) −F(x)) is\n1\nλ2(λ∇F(x + λh)T h −F(x + λh) + F(x))\nwith h = y −x, which is non-negative by (3.5). This proves that F is convex. If (3.5)\nholds with a strict inequality, then the derivative is positive and 1\nλ(F((1 −λ)x + λy) −\nF(x)) is increasing.\n■\n\n46\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nThe next proposition describes C2 convex functions in terms of their second\nderivatives.\nProposition 3.12 Let F be convex and twice differentiable at x ∈int(dom(F)). Then\n∇2F(x) is positive semi-definite.\nConversely, assume that Ω= dom(F) is an open set and that F is C2 on Ωwith a\npositive semi-definite second derivative. Then F (or, rather, its extension ˆF) is convex. If\nthe second derivative is everywhere positive definite, then F is strictly convex.\nProof Using Taylor formula (1.10) at order 2, we get, for any h ∈Rd with |h| = 1,\n1\n2d2F(x)(h,h) = 1\n2t2d2F(x)(th,th) = 1\nt2(F(x + th) −F(x) −t∇F(x)T h) + ϵ(t) ≥ϵ(t)\nwith ϵ(t) →0 when t →0, the last inequality deriving from (3.5). This shows that\nd2F(x)(h,h) ≥0.\nTo prove the second statement, assume that F is C2 and ∇2F is positive semi-\ndefinite everywhere. Then (1.8) implies\nF(y) −F(x) −∇F(x)T (y −x) = 1\n2(y −x)T ∇2F(z)(y −x)\nfor some z ∈[x,y]. Since the r.h.s. is non-negative, (3.5) holds. If ∇2F is positive\ndefinite everywhere, then the r.h.s. is positive if y , x and (3.5) holds with a strict\ninequality.\n■\nIf F is C2 and ∇2F is positive definite and strictly convex, then (1.8) implies that,\nfor some z ∈[x,y],\nF(y) −F(x) −∇F(x)T (y −x) = 1\n2(y −x)T ∇2F(z)(y −x) ≥ρmin(∇2F(z))\n2\n|y −x|2\nwhere ρmin(A) denotes the smallest eigenvalue of A. If this smallest eigenvalue is\nbounded from below away from zero, there exists a constant m > 0 such that\nF(y) −F(x) −∇F(x)T (y −x) −m\n2 |y −x|2 ≥0.\n(3.6)\nThis property is captured by the following definition, which does not require F to be\nC2.\nDefinition 3.13 A C1 function F is strongly convex if\n1. int(dom(F)) , ∅\n2. There exists m > 0 such that (3.6) holds for all x ∈int(dom(F)) and y ∈Rd.\n\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n47\nWe have the following proposition.\nProposition 3.14 If F is strongly convex, then it is strictly convex, so that, in particular\nargminF has at most one element.\nIf dom(F) = Rd, then argminF is not empty.\nProof The first part is a direct consequence of (3.6) and theorem 3.11.\nFor the second part, (3.6) implies that\nF(x) −F(0) ≥∇F(0)T x + m\n2 |x|2 ≥|x|\n\u0012m\n2 |x|2 −|∇F(0)|\n\u0013\nThis shows that F(x) > F(0) if |x| > 2|∇F(0)|/m := r so that\nargminF = argmin\n¯B(0,r)\nF.\nThe set in the r.h.s. involves the minimization of a continuous function on a compact\nset, and is therefore not empty.\n■\nWe will use the following definition.\nDefinition 3.15 A function F : Ω→Rm is L-Ck, L being a positive number, if it is Ck\nand\n|dkF(x) −dkF(y)| ≤L|x −y|.\nIf F is L-Ck, then Taylor formula ((1.9)) implies\n\f\f\f\f\ff (x + h) −f (x) −df (x)h −1\n2d2f (x)(h,h) −··· −1\nk!dkf (x)(h,...,h)\n\f\f\f\f\f ≤L|h|k+1\n(k + 1)!\n(3.7)\nfor which we used the fact that\nZ 1\n0\nt(1 −t)k−1dt =\nZ 1\n0\n(1 −t)k−1dt −\nZ 1\n0\n(1 −t)kdt = 1\nk −\n1\nk + 1 =\n1\nk(k + 1).\nIf F is strongly convex and is, in addition, L-C1 for some L, then using (3.7), one\ngets the double inequality, for all x,y ∈int(dom(F)):\nm\n2 |y −x|2 ≤F(y) −F(x) −∇F(x)T (y −x) ≤L\n2|y −x|2.\n(3.8)\nThe following proposition will be used later.\n\n48\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nProposition 3.16 Assume that F is strongly convex, satisfying (3.6), and that argminF =\n{x∗} with x∗∈int(dom(F)). Then, for all x ∈int(dom(F)):\nm\n2 |x −x∗|2 ≤F(x) −F(x∗) ≤1\n2m|∇F(x)|2\n(3.9)\nProof Since ∇F(x∗) = 0, the first inequality is a consequence of (3.6) applied to x =\nx∗. Switching the role of x and x∗, we have\nF(x∗) −F(x) −∇F(x)T (x∗−x) ≥m\n2 |x −x∗|2\nso that\n0 ≤F(x) −F(x∗) ≤−∇F(x)T (x∗−x) −m\n2 |x −x∗|2 ≤|∇F(x)||x −x∗| −m\n2 |x −x∗|2\n(3.10)\nThe maximum of the r.h.s. with respect to |x −x∗| is attained at |∇F(x)|/m, showing\nthat\nF(x) −F(x∗) ≤1\n2m|∇F(x)|2,\nwhich is the second inequality.\n■\n3.2.5\nDirection of descent and steepest descent\nGradient-based algorithms for optimization iteratively update the variable x, creat-\ning a sequence governed by an equation taking the form xt+1 = xt + αtht with αt > 0\nand ht ∈Rd. To ensure that the objective function F decreases at each step, ht is cho-\nsen to be a direction of descent for F at xt, a notion which, as seen below, is closely\nconnected with the direction of ∇F(xt).\nDefinition 3.17 Let Ωbe open in Rd and F : Ω→R be a C1 function. A direction\nof descent for F at x ∈Ωis a vector h , 0 ∈Rd such that there exists ϵ0 > 0 such that\nF(x + ϵh) < F(x) for all ϵ ∈(0,ϵ0].\nProposition 3.18 Assume that F : Ω→R is C1 and take x ∈Ω. Then any direction h\nsuch that hT ∇F(x) < 0 is a direction of descent for F at x. Conversely, if h is a direction of\ndescent, then hT ∇F(x) ≤0.\nProof We have the first-order expansion F(x+ϵh)−F(x) = ϵhT ∇F(x)+o(ϵ). If hT ∇F(x) <\n0, the r.h.s. is negative for small enough ϵ and h is a direction of descent. Similarly,\nif hT ∇F(x) > 0, the r.h.s. is positive for small enough ϵ and h cannot be a direction of\ndescent.\n■\n\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n49\nIn particular, h = −∇F(x) is always a direction of descent. It is called the steep-\nest descent direction because it minimizes h 7→∂αF(x + αh)|α=0 over all h such that\n|h|2 = 1. However, this designation has a character of optimality that may be mis-\nleading, because using the Euclidean norm for the condition |h|2 = 1 is not neces-\nsarily adapted to the optimization problem at hand. In the absence of additional\ninformation on the problem, it does have a canonical nature, as it is (up to rescaling)\nthe only norm invariant to rotations (including permutations) of the coordinates.\nSuch invariance is not necessarily desirable when the variable x has a known struc-\nture (e.g., it is organized on a graph) which would be broken by permutation. Also,\nsteepest refers to a local “greedy” evaluation, but may not be optimal from a global\nperspective. A simple example to illustrate this is the case of a quadratic function\nF(x) = 1\n2xT Ax −bT x\nwhere A ∈S++\nn\nis a positive definite symmetric matrix. Then ∇F(x) = Ax −b, but one\nmay argue that ∇AF(x) = A−1∇F(x) (defined in (1.3)) is a better choice, because it\nallows the algorithm to reach the minimizer of F in one step, since x−∇AF(x) = A−1b\n(this statement disregards the cost associated in solving the system Ax = b, which\ncan be an important factor in large dimension). Importantly, if F is any C1 function,\nand A ∈S++\nn , the minimizer of h 7→∂αF(x + αh)|α=0 over all h such that hT Ah = 1 is\ngiven by −∇AF(x), i.e., −∇AF(x) is the steepest descent for the norm associated with\nA. This yields a general version of steepest descent methods, iterating\nxt+1 = xt −αt∇AtF(xt)\nwith αt > 0 and At ∈S++\nn .\nOne can also notice that ∇AF(x) is also a minimizer of\nF(x) + ∇F(x)T h + 1\n2hT Ah.\nWhen ∇2F(x) is positive definite, it is then natural to choose it as the matrix A, there-\nfore taking h = −∇2F(x)−1∇F(x). This provides Newton’s method for optimization.\nHowever, Newton method requires computing second derivatives of F, which can be\ncomputationally costly. It is, moreover, not a gradient-based method, which is the\nfocus of this discussion.\n3.2.6\nConvergence\nWe now consider a descent algorithm\nxt+1 = xt + αtht\n(3.11)\n\n50\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nwhere ht is a direction of descent at xt for the objective function F. To ensure con-\nvergence, suitable choices for the direction of descent and the step must be made at\neach iteration, and some assumptions on the objective function are needed.\nRegarding the direction of descent, which must satisfy hT\nk ∇F(xk) ≤0, we will as-\nsume a uniform control away from orthogonality to the gradient, with the condition\n−hT\nt ∇F(xt) ≥ϵ|ht||∇F(xt)|\n(3.12a)\nfor some fixed ϵ > 0. Without loss of generality (given that a multiplicative step αt\nmust also be chosen), we assume that ht is commensurable to the gradient, namely,\nthat\nγ1|∇F(xt)| ≤|ht| ≤γ2|∇F(xt)|\n(3.12b)\nfor fixed 0 < γ1 ≤γ2. If ht = ∇AtF, these assumptions are satisfied as soon as the\nsmallest and largest eigenvalues of At are controlled along the trajectory.\nWe have the following proposition.\nProposition 3.19 Assume that F is L-C1. Assume that xt satisfies (3.11) and that (3.12a)\nand (3.12b) hold. Then, there exist constants ¯α > 0 and C > 0 that depends on γ1,γ2 and\nϵ, such that, for αt ≤¯α, one has\nF(xt+1) −F(xt) ≤−Cαt|∇F(xt)|2.\n(3.13)\nProof Applying (3.7) to xt and xt+1, we get\nF(xt+1) −F(xt) −αt∇F(xt)T ht ≤L\n2α2\nt |ht|2\nUsing (3.11) and (3.12a), this gives\nF(xt+1) −F(xt) + αtϵγ1|∇F(xt)|2 ≤L\n2α2\nt γ2\n2|∇F(xt)|2\nso that\nF(xt+1) −F(xt) ≤−αt\n\u0010\nϵγ1 −αtγ2\n2L/2\n\u0011\n|∇F(xt)|2.\nIt suffices to take ¯α = ϵγ1/Lγ2\n2 and C = ϵγ1/2 to obtain (3.13).\n■\nIterating (3.13) for t = 1 to t = T −1 yields\nT\nX\nt=1\nαt|∇F(xt)|2 ≤1\nC (F(x1) −F(xT )).\nIf F is bounded from below, and one takes αt = ¯α for all t, one deduces that\nmin\nn\n|∇F(xt)|2 : t = 1,...,T\no\n≤F(x1) −infF\nCT ¯α\n.\n\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n51\nWe can deduce from this, for example, that there exists a sequence t1 < ··· < tn < ···\nsuch that ∇F(xtk) →0 when k →∞. In particular, if one runs (3.11) until |∇F(xt)| is\nsmaller than a given tolerance level (which is standard), the procedure is guaranteed\nto terminate in a finite number of steps.\nStronger results may be obtained under stronger assumptions on F and on the\nalgorithm. The first assumption is an inequality similar to (3.13) and requires that,\nfor some constant C > 0,\nF(xt+1) −F(xt) ≤−C|∇F(xt)|2.\n(3.14)\nSuch an inequality can be deduced from (3.13) under the additional assumption that\nαt is bounded from below and we will discuss later line search strategies that ensure\nits validity. The second assumption is that F is convex.\nTheorem 3.20 Assume that F is convex and finite and that its sub-level set [F ≤F(x0)]\nis bounded. Assume that argminF is not empty and let x∗be a minimizer of F. If (3.14)\nis true, then\nF(xt) −F(x∗) ≤\nR2\nC(t + 1)\nwith R = max{|x −x∗| : F(x) ≤F(x0)}.\nProof Note that the algorithm never leaves [F ≤F(x0)]. We have\nF(xt+1) −F(x∗) ≤F(xt) −F(x∗) −C|∇F(xt)|2.\nMoreover, by convexity, F(x∗) −F(xt) ≥∇F(xt)T (x∗−xt), so that\nF(xt) −F(x∗) ≤∇F(xt)T (xt −x∗) ≤|∇F(xt)|R.\nCombining these two inequalities, we get\nF(xt+1) −F(x∗) ≤F(xt) −F(x∗) −C\nR2(F(xt) −F(x∗))2.\nIntroducing δt = (C/R2)(F(xt) −F(x∗)), this inequality implies\nδt+1 ≤δt(1 −δt).\nTaking inverses, we get\n1\nδt+1\n≥1\nδt\n+\n1\n1 −δt\n≥1\nδt\n+ 1\nwhich implies 1\nδt ≥t + 1 or δt ≤1/(t + 1), which in turn implies the statement of the\ntheorem.\n■\n\n52\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nA faster convergence rate can be obtained if F is assumed to be strongly convex.\nIndeed, if (3.6) and (3.14) are satisfied, then (using proposition 3.16),\nF(xt+1) −F(x∗) ≤F(xt) −F(x∗) −C|∇F(xt)|2\n≤F(xt) −F(x∗) −2Cm(F(xt) −F(x∗))\n= (1 −2Cm)(F(xt) −F(x∗)).\nWe therefore get the proposition:\nProposition 3.21 If F is finite and satisfies (3.6), and if the descent algorithm satisfies\n(3.14), then\nF(xt) −F(x∗) ≤(1 −2Cm)t(F(x0) −F(x∗)).\n3.2.7\nLine search\nProposition 3.19 states that, to ensure that (3.14) holds, it suffices to take a small\nenough step parameter α. However, the values of α that are acceptable depend on\nproperties of the objective function that are rarely known in practice. Moreover,\neven if a valid choice is determined (this can sometimes be done in practice by trial\nand error), setting a fixed value of α for the whole algorithm is often too conserva-\ntive, as the best α when starting the algorithm may be different from the best one\nclose to convergence.\nFor this reason, most gradient descent procedures select a parameter αt at each\nstep using a line search. Given a current position and direction of descent h, a line\nsearch explores the values of F(x + αh), α ∈(0,αmax] in order to discover some α∗\nthat satisfies some desirable properties. We will assume in the following that x and\nh satisfy (3.12a) and (3.12b) for fixed ϵ,γ1,γ2.\nOne possible strategy is to define α∗as a minimizer of the scalar function\nfh(α) = F(x + αh)\nover (0,αmax] for a given upper-bound ϵmax. This can be implemented using, e.g.,\nbinary or ternary search algorithms, but such algorithms would typically require a\nlarge number of number of evaluations of the function F, and would be too costly to\nbe run at each iteration of a gradient descent procedure.\nBased on the previous convergence study, we should be happy with a line search\nprocedure that ensures that (3.14) is satisfied for some fixed value of the constant C.\nOne such condition is the so-called Armijo rule that requires (with a fixed, typically\nsmall, value of c1 > 0):\nfh(α) ≤fh(0) + c1αhT ∇f (x).\n(3.15)\n\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n53\nWe know that, under the assumptions of proposition 3.19, this condition can always\nbe satisfied with a small enough value of α. Such a value can be determined using a\n“backtracking procedure,” which, given αmax and ρ ∈(0,1), takes α = ρkαmax where\nk is the smallest integer such that (3.15) is satisfied. This value of k is then deter-\nmined iteratively, trying αmax, ραmax, ρ2αmax,... until (3.15) is true (this provides\nthe “backtracking method”).\nA stronger requirement in the line search is to ensure that ∂fh(α) is not “too\nnegative” since one would otherwise be able to further reduce fh by taking a larger\nvalue of α. This leads to the weak Wolfe conditions, which combine the Armijo’s\nrule in (3.15) and\n∂fh(α) = hT ∇F(x + αh) ≥c2hT ∇F(x)\n(3.16a)\nfor some constant c2 ∈(c1,1). The strong Wolfe conditions require (3.15) and\n|hT ∇F(x + αh)| ≤c2|hT ∇F(x)|.\n(3.16b)\n(Since h is a direction of descent, (3.16b) requires (3.16a) and the fact that hT ∇F(x +\nαh) does not take too large positive values.) If F is L-C1, these conditions, with\n(3.12a) and (3.12b), imply (3.14). Indeed, (3.16a) and the L-C1 condition imply\n−(1 −c2)hT ∇F(x) ≤hT (∇F(x + αh) −∇F(x)) ≤Lα|h|2\nand (3.12a) and (3.12b) give\n(1 −c2)ϵ|∇F(x)|2 ≤αLγ2\n2|∇F(x)|2\nshowing that α ≥(1 −c2)ϵ/(Lγ2\n2). Moreover\nF(x + αh) ≤F(x) + c1αhT ∇f (x) ≤F(x) −c1αϵ|∇F(x)|2\nso that\nF(x + αh) ≤F(x) −c1(1 −c2)ϵ2\nLγ2\n2\n|∇F(x)|2.\nWe have just proved the following proposition.\nProposition 3.22 Assume that F is L-C1 and that (3.12a), (3.12b), (3.15) and (3.16a)\nare satisfied. Then there exists C > 0, depending only of L,ϵ,γ2,c1 and c2such that\nF(x + αh) ≤F(x) −C|∇F(x)|2.\nThe Wolfe conditions can always be satisfied by some α as soon as F is C1 and\nbounded from below, and hT ∇F(x) < 0. The next proposition shows this result for\nthe weak condition, while providing an algorithm finding an α that satisfies it in a\nfinite number of steps.\n\n54\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nProposition 3.23 Let f : α 7→f (α) be a C1 function defined on [0,+∞) such that f is\nbounded from below and ∂αf (0) < 0. Let 0 < c1 < c2 < 1.\nLet α0,0 = α0,1 = 0 and α0 > 0. Define recursively sequences αn,0,αn,1 and αn as\nfollows.\n(i) If f (αn) ≤f (0) + c1αn∂αf (0) and ∂f (αn) ≥c2∂αf (0) stop the construction.\n(ii) If f (αn) > f (0) + c1αn∂αf (0) let αn+1 = (αn + αn,0)/2, αn+1,1 = αn and αn+1,0 = αn,0.\n(iii) If f (αn) ≤f (0) + c1αn∂αf (0) and ∂f (αn) < c2∂αf (0):\n(a) If αn,1 = 0, let αn+1 = 2αn, αn+1,0 = αn and αn+1,1 = αn,1.\n(b) If αn,1 > 0, let αn+1 = (αn + αn,1)/2, αn+1,0 = αn and αn+1,1 = αn,1.\nThen the sequences are always finite, i.e., the algorithm terminates in a finite number of\nsteps.\nProof Assume, to get a contradiction, that the algorithm runs indefinitely, so that\ncase (i) never occurs. If case (ii) never occurs, then one runs step (iii-a) indefinitely,\nso that αn →∞with f (αn) ≤f (0) + c1αn∂αf (0), and f cannot be bounded from\nbelow, yielding a contradiction. As soon as case (ii) occurs, we have, at every step,\nαn,0 ≥αn−1,0, αn,1 ≤αn−1,1, αn ∈[αn,0,αn,1], f (αn,1) > f (0) + c1αn,1∂αf (0), f (αn,0) ≤\nf (0) + c1αn,0∂αf (0) and ∂f (αn,0) < c2∂αf (0). This implies that\nf (αn,1) −f (αn,0) > c1(αn,1 −αn,0)∂αf (0).\nMoreover, the updates imply that (αn+1,1 −αn+1,0) = (αn,1 −αn,0)/2. This requires that\nthe three sequences αn,αn,0 and αn,1 converge to the same limit, α. We have\n∂αf (α) = lim\nn→∞\nf (αn,1) −f (αn,0)\nαn,1 −αn,0\n≥c1∂αf (0)\nand\n∂αf (α) = lim\nn→∞∂αf (αn,0) ≤c2∂αf (0)\nyielding c1∂αf (0) ≤c2∂αf (0) which is impossible since c2 > c1 and ∂αf (0) < 0.\n■\nThe existence of α satisfying the strong Wolfe condition is a consequence of the\nfollowing proposition, which also provides an algorithm.\nProposition 3.24 Let f : α 7→f (α) be a C1 function defined on [0,+∞) such that f is\nbounded from below and ∂αf (0) < 0. Let 0 < c1 < c2 < 1.\nLet α0,0 = α0,1 = 0 and α0 > 0. Define recursively sequences αn,0,αn,1 and αn as\nfollows.\n\n3.3. STOCHASTIC GRADIENT DESCENT\n55\n(i) If f (αn) ≤f (0) + c1αn∂αf (0) and |∂αf (αn)| ≤c2|∂αf (0)| stop the construction.\n(ii) If f (αn) > f (0) + c1αn∂αf (0) let αn+1 = (αn + αn,0)/2, αn+1,1 = αn and αn+1,0 = αn,0.\n(iii) If f (αn) ≤f (0) + c1αn∂αf (0) and |∂αf (αn)| > c2|∂αf (0)|:\n(a) If αn,1 = 0 and ∂αf (αn) > −c2∂αf (0), let αn+1 = 2αn, αn+1,0 = αn,0 and αn+1,1 =\nαn,1.\n(b) If αn,1 = 0 and ∂αf (αn) < c2∂αf (0), let αn+1 = 2αn, αn+1,0 = αn and αn+1,1 =\nαn,1.\n(c) If αn,1 > 0 and ∂αf (αn) > −c2∂αf (0), let αn+1 = (αn + αn,0)/2, αn+1,1 = αn and\nαn+1,0 = αn,0.\n(d) If αn,1 > 0 and ∂αf (αn) < c2∂αf (0), let αn+1 = (αn + αn,1)/2, αn+1,0 = αn and\nαn+1,1 = αn,1.\nThen the sequences are always finite, i.e., the algorithm terminates in a finite number of\nsteps.\nProof Assume that the algorithm runs indefinitely in order to get a contradiction.\nIf the algorithm never enters case (ii), then αn,1 = 0 for all n, αn tends to infinity and\nf (αn) ≤f (0) + c1αn∂αf (0), which contradicts the fact that f is bounded from below.\nAs soon as the algorithm enter (ii), we have, for all subsequent iterations: αn,0 ≤\nαn ≤αn,1, αn+1,0 ≥αn,0, αn+1,1 ≤αn,1 and αn+1,1 −αn+1,0 = (αn,1 −αn,0)/2. This implies\nthat both αn,0 and αn,1 converge to the same limit α.\nMoreover, we have, at each step:\nf (αn,1) > f (0) + c1αn,1∂αf (0) or ∂αf (αn,1) > −c2∂αf (0)\nand\nf (αn,0) ≤f (0) + c1αn,0∂αf (0) and ∂αf (αn,0) ≤c2∂αf (0).\nThis implies that, at each step:\nf (αn,1) −f (αn,0)\nαn,1 −αn,0\n> c1∂αf (0) or ∂αf (αn,1) > −c2∂αf (0)\nand\n∂αf (αn,0) ≤c2∂αf (0).\nThere inequalities remain satisfied at the limit, and we must have\n∂αf (α) > c1∂αf (0) or ∂αf (α) > −c2∂αf (0)\nand\n∂αf (α) ≤c2∂αf (0),\nwhich is a contradiction since c2 > c1 and ∂αf (0) < 0.\n■\n\n56\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\n3.3\nStochastic gradient descent\n3.3.1\nStochastic approximation methods\nIn some situations, the computation of ∇F can be too costly, if not intractable, to run\ngradient descent updates while a low-cost stochastic approximation is available. For\nexample, if F is an average of a sum of many terms, the approximation may simply\nbe based on averaging over a randomly selected subset of the terms. This leads\nto a stochastic approximation algorithm [163, 113, 25, 67] called stochastic gradient\ndescent (SGD).\nA general stochastic approximation algorithm of the Robbins-Monro type up-\ndates a parameter, denoted x ∈Rd, using stochastic rules. One associates to each x a\nprobability distribution (πx) on some set S, and, for some function H : Rd ×S →Rd,\nconsiders the sequence of random iterations:\n(ξt+1 ∼πXt\nXt+1 = Xt + αt+1H(Xt,ξt+1)\n(3.17)\nwhere ξt+1 is a random variable and the notation ξt+1 ∼πXt should be interpreted\nas the more precise statement that the conditional distribution of ξt+1 given all past\nrandom variables Ut = (ξ1,X1,...,ξt,Xt) only depends on Xt and is given by πXt.\nIt is sometimes assumed in the literature that πx does not depend on x. This is\nno real loss of generality because under mild assumptions, a random variable ξ fol-\nlowing πx can be generated as function U(x, ˜ξ) where ˜ξ follows a fixed distribution\n(such as that of a family of independent uniformly distributed variables) and one\ncan replace H(x,ξ) by H(x,U(x, ˜ξ)). On the other hand, allowing π to depend on x\nbrings little additional complication in the notation, and corresponds to the natural\nform of many applications.\nMore complex situations can also be considered, in which ξt+1 is not condition-\nally independent of the past variables given Xt. For example, the conditional distri-\nbution of ξt+1 given the past may also depends on ξt, which allows for the combina-\ntion of stochastic gradient methods with Markov chain Monte-Carlo methods. This\nsituation is studied, for example, in M´etivier and Priouret [138], Benveniste et al.\n[25], and we will discuss an example in section 17.2.2.\n3.3.2\nDeterministic approximation and convergence study\nIntroduce the function\n¯H(x) = Eπx(H(x,·))\n\n3.3. STOCHASTIC GRADIENT DESCENT\n57\nand write\nXt+1 = Xt + αt+1 ¯H(Xt) + αt+1ηt+1\nwith ηt+1 = H(Xt,ξt+1) −¯H(Xt) in order to represent the evolution of Xt in (3.17) as\na perturbation of the deterministic algorithm\n¯xt+1 = ¯xt + αn+1 ¯H( ¯xt)\n(3.18)\nby the “noise term” αt+1ηt+1. In many cases, the deterministic algorithm provides\nthe limit behavior of the stochastic sequence, and one should ensure that this limit\nis as desired. By definition, the conditional expectation of ηt+1 given Ut (the past) is\nzero and one says that αt+1ηt+1 is a “martingale increment.” Then,\nMT =\nT\nX\nt=0\nαt+1ηt+1\n(3.19)\nis called a “martingale.” The theory of martingales offers numerous tools for con-\ntrolling the size of MT and is often a key element in proving the convergence of the\nmethod.\nMany convergence results have been provided in the literature and can be found\nin textbooks or lecture notes such as Bena¨ım [23], Kushner and Yin [113], Benveniste\net al. [25]. These results rely on some smoothness and growth assumptions made on\nthe function H, and on the dynamics of the deterministic equation (3.18). Depend-\ning on these assumptions, proofs may become quite technical. We will here restrict\nto a reasonably simple context and assume that\n(H1) There exists a constant C such that, for all x ∈Rd,\nEπx(|H(x,·)|2) ≤C(1 + |x|2).\n(H2) There exists x∗∈Rd and µ > 0 such that, for all x ∈Rd\n(x −x∗)T ¯H(x) ≤−µ|x −x∗|2.\nAssuming this, let At = |Xt −x∗|2 and at = E(At). Then, using (3.17),\nAt+1 = At + 2αt+1(Xt −x∗)T H(Xt,ξt+1) + α2\nt+1|H(Xt,ξt+1)|2 .\nTaking the conditional expectation given past variables yields\nE(At+1 | Ut) = At + 2αt+1(Xt −x∗)T ¯H(Xt) + α2\nt+1Eπxt (|H(Xt,·)|2)\n≤At −2αt+1µAt + α2\nt+1C(1 + |Xt|2)\n≤(1 −2αt+1µ + Cα2\nt+1)At + α2\nt+1 ˜C\n\n58\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nwith ˜C = 1 + |x∗|2. Taking expectations on both sides yields\nat+1 ≤(1 −2αt+1µ + Cα2\nt+1)at + α2\nt+1 ˜C.\n(3.20)\nWe state the next step in the computation as a lemma.\nLemma 3.25 Assume that the sequence at satisfies the recursive inequality\nat+1 ≤(1 −δt)at + ϵt\n(3.21)\nwith 0 ≤δt ≤1. Let vk,t = Qt\nj=k+1(1 −δj). Then\nat ≤a0v0,t +\nt\nX\nk=1\nϵkvk,t.\n(3.22)\nProof Letting bt = at/v0,t, we get\nbt+1 ≤bt + ϵt+1\nv0,t+1\nso that\nbt ≤b0 +\nt\nX\nk=1\nϵk\nv0,k\n,\nand\nat ≤a0v0,t +\nt\nX\nk=1\nϵkvk,t.\n■\nUsing (3.20), we can apply this lemma with ϵt = ˜Cα2\nt and δt = 2αtµ−Cα2\nt , making the\nadditional assumption that, for all t, αt < min( 1\n2µ, 2µ\nC ), which ensures that 0 < δt < 1.\nStarting with a simple case, assume that the steps γt are constant, equal to some\nvalue γ (yielding also constant δ and ϵ). Then, (3.22) gives\nat ≤a0(1 −δ)t + ϵ\nt\nX\nk=1\n(1 −δ)t−k−1 ≤a0(1 −δ)t + ϵ\nδ.\n(3.23)\nReturning to the expression of δ and ϵ as functions of α, this gives\nat ≤a0(1 −2αµ + α2C)t +\nα ˜C\n2µ −αC .\nThis shows that limsupat = O(α).\nReturn to the general case in which the steps depend on t, we will use the follow-\ning simple result, that we state as a lemma for future reference.\n\n3.3. STOCHASTIC GRADIENT DESCENT\n59\nLemma 3.26 Assume that the double indexed sequence wst, s ≤t of non-negative num-\nbers is bounded and such that, for all s, limt→∞wst = 0. Let β1,β2,... be such that\n∞\nX\nt=1\n|βt| < ∞.\nThen\nlim\nt→∞\nt\nX\ns=1\nβswst = 0.\nProof For any t0, we have\n\f\f\f\f\f\f\f\nt\nX\ns=1\nβswst\n\f\f\f\f\f\f\f\n≤max\ns\n|βs|\nt0\nX\ns=1\nwst + max\ns,t |wst|\nX\ns=t0+1\n|βs|\nso that\nlimsup\nt→∞\n\f\f\f\f\f\f\f\nt\nX\ns=1\nβswst\n\f\f\f\f\f\f\f\n≤max\ns,t |wst|\nX\ns=t0+1\n|βs|\nand since this upper bound can be made arbitrarily small, the result follows.\n■\nLemma 3.25 implies that\nat ≤a0v0,t + ˜C\nt\nX\ns=1\nα2\ns+1vs,t.\nAssume that\n(H3) P∞\nk=1 αk = ∞and P∞\nk=1 α2\nk < ∞,\nThen limt→∞vst = 0 for all s and lemma 3.26 implies that at tends to zero. So, we\nhave just proved that, if (H1), (H2) and (H3) are true, the sequence Xt converges in\nthe L2 sense to x∗. Actually, under these conditions, one can show that Xt converge\nto x∗almost surely, and we refer to Benveniste et al. [25], Chapter 5, for a proof (the\nargument above for an L2 convergence follows the one given in Nemirovski et al.\n[145]).\nUnder (H3), one can say much more on the asymptotic behavior of the algorithm\nby comparing it with an ordinary differential equation. The “ODE method,” intro-\nduced in Ljung [120], is indeed a fundamental tool for the analysis of stochastic\napproximation algorithms. The correspondence between discrete and continuous\n\n60\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\ntimes is provided by the sequence αt. More precisely, let τ0 = 0 and τt = τt−1 + αt,\nt ≥1. From (H3), τt →∞when t →∞. Define the piecewise linear interpolation\nxℓ(ρ) of the sequence xt by\nXℓ(ρ) = Xt + ρ −τt\nαt+1\n(Xt+1 −Xt),\nρ ∈[τt,τt+1).\nSwitching to continuous time allows us to interpret the average iteration ¯xt+1 = ¯xt +\nαt+1 ¯H( ¯xt) as an Euler discretization scheme for the ordinary differential equation\n(ODE)\n∂ρ ¯x = ¯H( ¯x).\n(3.24)\nMost of the insight on long-term behavior of stochastic approximations results\nfrom the fact that the random process x behaves asymptotically like solutions of\nthis ODE. One has, for example, the following result, for which we introduce some\nadditional notation.\nAssume that (3.24) has unique solutions for given initial conditions on any fi-\nnite interval, and denote by ϕ(ρ,ω) its solution at time ρ initialized with ¯x(0) = ω.\nLet αc(ρ) and ηc(ρ) be piecewise constant interpolations of (αt) and (ηt) defined by\nαc(ρ) = αt+1 and ηc(ρ) = ηt+1 on the interval [τt,τt+1). Finally, let\n∆(ρ,T ) =\nmax\ns∈[ρ,ρ+T ]\n\f\f\f\f\f\f\nZ s\nρ\nηc(u)du\n\f\f\f\f\f\f.\nThe following proposition (see [23]) compares the tails of the process xℓ(i.e., the\nfunctions xℓ(ρ + s), s ≥0) with the solutions of the ODE over finite intervals.\nProposition 3.27 (Benaim) Assume that ¯H is Lipschitz and bounded. Then, for some\nconstant C(T) that only depends on T and ¯H, one has, for all ρ ≥0\nsup\nh∈[0,T]\n|Xℓ(ρ + h) −ϕ(h,Xℓ(ρ))| ≤C(T )\n \n∆(ρ −1,T + 1) +\nmax\ns∈[ρ,ρ+T]αc(s)\n!\n.\n(3.25)\nRecall that ¯H being Lipschitz means that there exists a constant C such that\n| ¯H(w) −¯H(w′)| ≤C|w −w′|\nfor all w,w′ ∈Rp.\nIn the upper-bound in (3.25), the term ∆(ρ −1,T + 1) is a random variable. It can\nbe related to the variations\n∆′(t,N) = max\nk=0,...,N |Mt+k −Mt|,\n\n3.3. STOCHASTIC GRADIENT DESCENT\n61\nwhere M is defined in (3.19), because, if m(ρ) is the largest integer t such that τt ≤ρ,\nthen\n∆′(m(ρ) + 1,m(ρ + T) −m(ρ)) ≤∆(ρ,T ) ≤∆′(m(ρ),m(ρ + T) −m(t) + 1).\nIn the case we are considering, one can use martingale inequalities (called Doob’s\ninequalities) to control ∆′. One has, for example,\nP\n\u0012\nmax\n0≤k≤N |Mt+k −Mt| > λ\n\u0013\n≤E(|Mt+N −Mt|2)\nλ2\n.\n(3.26)\nFurthermore, using the fact that E(ηk+1ηl+1) = 0 if k , l, one has\nE(|Mt+N −Mt|2) =\nt+N\nX\nk=t\nα2\nk+1E(|ηt+1|2).\nIf we assume (to simplify) that H is bounded and P∞\nk=1 α2\nk < ∞then, for some con-\nstant C, we have\nE(|Mt+N −Mt|2) ≤C\n∞\nX\nk=t\nα2\nk+1 →0\nand inequality (3.26) can then be used in (3.25) to control the probability of devia-\ntion of the stochastic approximation from the solution of the ODE over finite inter-\nvals (a little more work is required under weaker assumptions on H, such as (H1)).\nProposition 3.27 cannot be used with T = ∞because the constant C(T ) typically\ngrows exponentially with T. In order to draw conclusions on the limit of the process\nW, one needs additional assumptions on the stability of the ODE. We refer to [23]\nfor a collection of results on the relationship between invariant sets and attractors of\nthe ODE and limit trajectories of the stochastic approximation. We here quote one\nof these results which is especially relevant for SGD.\nProposition 3.28 Assume that ¯H = −∇E is the gradient of a function E and that ∇E only\nvanishes at a finite number of points. Assume also that Xt is bounded. Then Xt converges\nto a point x∗such that ∇E(x∗) = 0.\nSome additional conditions on ¯H can ensure that stochastic approximation tra-\njectories remain bounded. The simplest one assumes the existence of a “Lyapunov\nfunction” that controls the ODE at infinity. The following result is a simplified ver-\nsion of Theorem 17 in Benveniste et al. [25].\n\n62\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nTheorem 3.29 In addition to the hypotheses previously made, assume that there exists a\nC2 function U with bounded second derivatives and K0 > 0 such that, for allx such that\n|x| ≥K0,\n∇U(x)T ¯H(x) ≤0,\nU(x) ≥γ|x|2,γ > 0.\nThen, the trajectories Xℓ(ρ) are almost surely bounded.\nNote that hypothesis (H2) above implies the theorem’s assumptions.\n3.3.3\nThe ADAM algorithm\nADAM (for adaptive moment estimation [102]) is a popular variant of stochastic\ngradient descent. When dealing with high-dimensional vectors W, using a single\n“gain” parameter (γn+1 in (11.2)) is a limiting assumption since all parameters do\nnot need to scale the same way. This can sometimes be handled by reweighting the\ncomponents of H, i.e., using iterations\nXt+1 = Xt + αtDtH(Xt,ξt+1)\nwhere Dt is a (typically diagonal) matrix. The previous theory can be applied to\nsituations in which D may be random, provided it converges almost surely to a fixed\nmatrix.\nThe ADAM algorithm provides such a construction (without the theoretical guar-\nantees) in which Dt is computed using past iterations of the algorithm. It requires\nseveral parameters, namely: α: the algorithm gain, taken as constant (e.g., α =\n0.001); Two parameters β1 and β2 for moment estimates (e.g. β1 = 0.9 and β2 =\n0.999); A small number ϵ (e.g., ϵ = 10−8) to avoid divisions by 0. In addition, ADAM\ndefines two vectors: a mean m and a second moment v, respectively initialized at\n0 and 1. The ADAM iterations are given below, in which g⊗2 denotes the vector\nobtained by squaring each coefficient of a vector g.\nAlgorithm 3.1 (ADAM)\n1. Let Xt be the current state, mt and vt the current mean and variance.\n2. Generate ξt+1 and let gt+1 = H(Xt,ξt+1).\n3. Update mt+1 = β1mt + (1 −β1)gt+1.\n4. Update vt+1 = β2vt + (1 −β2)g⊙2\nt+1.\n5. Let ˆmt+1 = mt+1/(1 −βt+1\n1\n) and ˆvt+1 = vt+1/(1 −βt+1\n2\n)\n\n3.4. CONSTRAINED OPTIMIZATION PROBLEMS\n63\n6. Set\nXt+1 = Xt −α\nˆmt+1\n√ˆvt+1 + ϵ\nNote that the iteration on mt and vt correspond to defining\nˆmt =\nβ1\n1 −βt\n1\nt\nX\nk=0\n(1 −β1)t−kgk\nand\nˆvt =\nβ2\n1 −βt\n2\nt\nX\nk=0\n(1 −β2)t−kg⊙2\nk .\n3.4\nConstrained optimization problems\n3.4.1\nLagrange multipliers\nA constrained optimization problem minimizes a function F over a closed subset\nΩof Rd, with Ω, Rd. This restriction invalidates, in a large part, the optimality\nconditions discussed in section 3.2. These conditions indeed apply to minimizers\nbelonging to the interior of Ω, and therefore do not hold when they lie at its bound-\nary, which is a very common situation in practice (Ωoften has an empty interior).\nIn this section, which follows the discussion given in Wright and Recht [204], we\nreview conditions for optimality for constrained minimization of smooth functions,\nin two cases. The first one, discussed in this section, is when Ωis defined by a finite\nnumber of smooth constraints, leading, under some assumptions, to the Karush-\nKuhn-Tucker (or KKT) conditions. The second one, in the next section, specializes\nto closed convex Ω.\nKKT conditions\nWe introduce some notation. Let γi, for i ∈C, be C1 functions γi : Rd →R, where C\nis a finite set of indices. We assume that C is divided into two non-intersecting parts,\nC = E ∪I and consider minimization problems searching for\nx∗∈argmin\nΩ\nF\n(3.27)\nwhere\nΩ= {x ∈Rd : γi(x) = 0,i ∈E and γi(x) ≤0,i ∈I}.\n(3.28)\n\n64\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nThe set Ωof all x that satisfy the constraints is called the feasible set for the consid-\nered problem. We will always assume that it is non-empty. If x ∈Ω, one defines the\nset A(x) of active constraints at x to be\nA(x) = {i ∈C : γi(x) = 0}.\nOne obviously has E ⊂A(x) for x ∈Ω.\nTo be valid, the KKT conditions require some additional assumptions on poten-\ntial minimizers, called “constraint qualifications.” An instance of such assumptions\nis provided by the next definition.\nDefinition 3.30 A point x ∈Ωsatisfies the Mangasarian-Fromovitz constraint qualifi-\ncations (MF-CQ) if the following two conditions are satisfied.\n(MF1) The vectors (∇γi(x),i ∈E) are linearly independent.\n(MF2) There exists a vector h ∈Rd such that hT ∇γi(x) = 0 for all i ∈E and hT ∇γi(x) < 0\nfor all i ∈A(x) ∩I.\nA sufficient (and easier to check) condition for x to satisfy these constraints is when\nthe vectors (∇γi(x),i ∈A(x)) are linearly independent [37]. Indeed, if the latter “LI-\nCQ” condition is true, then any set of values can be assigned to hT ∇γi(x) with the\nexistence of a vector h that achieves them.\nWe introduce the Lagrangian\nL(x,λ) = F(x) +\nX\ni∈C\nλiγi(x)\n(3.29)\nwhere the real numbers λi, i ∈C are called Lagrange multipliers. The following the-\norem (stated without proof, see, e.g., [146, 35]) provides necessary conditions satis-\nfied by solutions of the constrained minimization problem that satisfy the constraint\nqualifications.\nTheorem 3.31 Assume x∗∈Ωis a solution of (3.27), and that x∗satisfies the MF-CQ\nconditions. Then there exist Lagrange multipliers λi, i ∈C, such that\n(∂xL(x∗,λ) = 0\nλi ≥0 if i ∈I, with λi = 0 when i < A(x∗)\n(3.30)\nConditions (3.30) are the KKT conditions for the constrained optimization problem.\nThe second set of conditions is often called the complementary slackness conditions\nand state that λi = 0 for an inequality constraint unless this constraint is satisfied\nwith an equality. The next section provides examples in which the MF-CQ condi-\ntions are not satisfied and Theorem 3.31 does not hold. However, these conditions\nare not needed in the special case when the constraints are affine.\n\n3.4. CONSTRAINED OPTIMIZATION PROBLEMS\n65\nTheorem 3.32 Assume that for all i ∈A(x∗), the functions γi are affine, i.e., γi(x) =\nbT\ni x + βi for some b ∈Rd and β ∈R. Then (3.30) holds at any solution of (3.27).\nRemark 3.33 We have taken the convention to express the inequality constraints as\nγi(x) ≤0, i ∈I. With the reverse convention, i.e., γi(x) ≥0, i ∈I, one generally\ndefines the Lagrangian as\nL(x,λ) = F(x) −\nX\ni∈C\nλiγi(x)\nand the KKT conditions remain unchanged.\n♦\nExamples.\nConstraint qualifications are important to ensure the validity of the the-\norem. Consider a problem with equality constraints only, and replace it by\nx∗∈argmin\nΩ\nF\nsubject to ˜γi(x) = 0, i ∈E, with ˜γi = γ2\ni . We clearly did not change the problem.\nHowever, the previous theorem applied to the Lagrangian\nL(x,λ) = F(x) +\nX\ni∈C\nλi ˜γi(x)\nwould require an optimal solution to satisfy ∇F(x) = 0, because ∇˜γi(x) = 2γi(x)∇γi(x) =\n0 for any feasible solution. Minimizers of constrained problems do not necessarily\nsatisfy ∇F(x) = 0, however. This is no contradiction with the theorem since ∇˜γi(x) = 0\nfor all i shows that no feasible point satisfies the MF-CQ.\nTo take a more specific example, still with equality constraints, let d = 3, C = {1,2}\nwith F(x,y,z) = x/2+y and γ1(x,y,z) = x2−y2,γ2(x,y,z) = y−z2. Note that γ1 = γ2 = 0\nimplies that y = |x|, so that, for a feasible point, F(x,y,z) = |x| + x/2 ≥0 and vanishes\nonly when x = y = 0, in which case z = 0 also. So (0,0,0) is a global minimizer.\nWe have dF(0) = (1/2,1,0), dγ1(0) = (0,0,0) and dγ2(0) = (0,1,0) so that 0 does not\nsatisfy the MF-CQ. The equation\ndF(0) + λ1dγ1(0) + λ2dγ2(0) = 0\nhas no solution (λ1,λ2), so that the conclusion of the theorem does not hold.\n3.4.2\nConvex constraints\nWe now consider the case in which Ωis a closed convex set. To specify the optimality\nconditions in this case, we need the following definition.\n\n66\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nDefinition 3.34 Let Ω⊂Rd be convex and let x ∈Ω. The normal cone to Ωat x is the\nset\nNΩ(x) = {h ∈Rd : hT (y −x) ≤0 for all y ∈Ω}\n(3.31)\nThe normal cone is an example of convex cone. (A convex subset Γ of Rd is called\na convex cone, if it is such that λx ∈Γ for all x ∈Γ and λ ≥0, a property obviously\nsatisfied by NΩ(x).) It should also be clear from the definition that non-zero vectors\nin NΩ(x) always point outside Ω, i.e., x + h < Ωif h ∈NΩ(x), h , 0. Here are some\nexamples.\n• If x is in the interior of Ω, then NΩ(x) = {0}.\n• Assume that Ωis a half space, i.e., Ω= {x : bT x + β ≤0} with |b| = 1, and take\nx ∈∂Ω, i.e., bT x + β = 0. Then\nNΩ(x) = {h = µb : µ ≥0}.\nIndeed, any element of Rd can be written as y = x+λb+q with qT b = 0, and y ∈Ω\nif and only if λ ≤0. Fix such a y and take h ∈Rd, decomposed as h = µb + r, with\nrT b = 0. We have hT (y −x) = λµ + rT q. Clearly, if µ < 0, or if r , 0, one can find λ ≤0\nand q ⊥b such that hT (y −x) > 0. One the other hand, if µ ≤0 and r = 0, we have\nhT (y −x) ≤0 for all y ∈Ω, which proves the above statement.\n• With a similar argument, if Ω= {x : bT x +β = 0} is a hyperplane, one finds that\nNΩ(x) = {h = λb : λ ∈R}.\nOne can build normal cones to domains associated with multiple inequalities or\nequalities based on the following theorem.\nTheorem 3.35 Let Ω1 and Ω2 be two convex sets with relint(Ω1)∩relint(Ω2) , ∅. Then,\nif x ∈Ω1 ∩Ω2\nNΩ1∩Ω2(x) = NΩ1(x) + NΩ2(x)\nHere, the addition is the standard sum between sets in a vector space:\nA + B = {x + y : x ∈A,y ∈B}.\nFinally, we note that, if x ∈relint(Ω), then\nNΩ(x) = {h ∈Rd : hT (y −x) = 0,y ∈Ω}.\n(3.32)\nIndeed, if y ∈Ω, then x + ϵ(y −x) ∈Ωfor small enough ϵ (positive or negative). For\nh ∈NΩ(x), the condition ϵhT (y −x) ≤0 for small enough ϵ requires that hT (y −x) = 0.\nWith this definition in hand, we have the following theorem.\n\n3.4. CONSTRAINED OPTIMIZATION PROBLEMS\n67\nTheorem 3.36 Let F be a C1 function and Ωa closed convex set. If x∗∈argminΩF, then\n−∇F(x∗) ∈NΩ(x∗).\n(3.33)\nIf F is convex and (3.33) holds, then x∗∈argminΩF.\nProof Assume that x∗∈argminΩF. If y ∈Ω, then x∗+ t(y −x∗) ∈Ωfor all t ∈[0,1]\nand the function f (t) = F(x + t(y −x∗)) is C1 on [0,1], with a minimum at t = 0. This\nrequires that ∂tf (0) = ∇F(x∗)T (y −x∗) ≥0, because, if ∂tf (0) < 0, a Taylor expansion\nwould show that f (t) < f (0) for small enough t > 0.\nIf F is convex and (3.33) holds, we have F(y) ≥F(x∗)+∇F(x∗)T (y−x∗) by convexity,\nso that\nF(x∗) ≤F(y) + (−∇F(x∗))T (y −x∗) ≤F(y).\n■\n3.4.3\nApplications\nLagrange multipliers revisited. Consider Ωdefined by (3.28), with the additional\nassumptions that γi(x) = bT\ni x + βi for i ∈E and γi is convex for i ∈I, which ensure\nthat Ωis convex. Define\nN ′\nγ(x) =\n\ng =\nX\ni∈A(x)\nλi∇γi(x) : λi ≥0,i ∈A(x) ∩I\n\n.\nThen, the KKT conditions in (3.30) can be rewritten as\n−∇F(x∗) ∈N ′\nγ(x∗).\nNote that one always have N ′\nγ(x) ⊂NΩ(x) since, for g = P\ni∈A(x) λi∇γi(x) ∈N ′\nγ(x), one\nhas, for y ∈Ω,\ngT (y −x) =\nX\ni∈A(x)\nλi∇γi(x)T (y −x)\n=\nX\ni∈E\nλi(aT\ni y −aT\ni x) +\nX\ni∈A(x)∩I\nλi(γi(x) + ∇γi(x)T (y −x))\n=\nX\ni∈A(x)∩I\nλi(γi(x) + ∇γi(x)T (y −x))\n≤λiγi(y) ≤0,\nin which the have used the facts that aT\ni x = aT\ni y = −βi for x,y ∈Ω, i ∈E, γi(x) = 0 for\ni ∈A(x) and the convexity of γi. Constraint qualifications such as those considered\nabove are sufficient conditions that ensure the identity between the two sets.\n\n68\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nConsider now the situation of theorem 3.32, and assume that all constraints are\naffine inequalities, γi(x) = bT\ni x + β ≤0,i ∈I. Then, the statement NΩ(x) ⊂N ′\nγ(x) can\nbe reexpressed as follows. All h ∈Rd such that\nhT (y −x) ≤0\nas soon as bT\ni (y −x) ≤0 for all i ∈A(x) must take the form\nh =\nX\ni∈A(x)\nλibi\nwith λ(i) ≥0. This property is called Farkas’s lemma (see, e.g. [167]). Note that affine\nequalities bT\ni x + β = 0 can be included as two inequalities bT\ni x + β ≤0, −bT\ni x −β ≤0,\nwhich removes the sign constraint on the corresponding λ(i) and therefore yields\ntheorem 3.32.\nPositive semi-definite matrices. We now take an example in which theorem 3.32\ndoes not apply directly. Let Ω= S+\nn be the space of positive semidefinite n × n ma-\ntrices, considered as a subset of the space Mn of n ×n matrices, itself identified with\nRn2. With this identification, the Euclidean inner product between two matrices can\nbe expressed as (A,B) 7→trace(AT B).\nWe have A ∈S+\nn if and only if, for all u ∈Rd, uT Au ≥0, which provides an infinity\nof linear inequality constraints on A. Elements of NS+(A) are matrices H ∈Mn such\nthat\ntrace(HT (B −A)) ≤0\nfor all B ∈S+\nn , and we want to make this normal cone explicit. We first note that,\nevery square matrix H can be decomposed as the sum of a symmetric matrix, Hs and\nof a skew symmetric one, Ha (namely, Hs = (H + HT )/2 and Ha = (H −HT )/2). We\nhave moreover trace(HT\na (B −A)) = 0, so the condition is only on the symmetric part\nof H.\nFor any u ∈Rd, one can take B = A+uuT , which belongs to S+\nn , with trace(HT\ns (B−\nA)) = uT Hsu. This shows that, for H to belong to NS+n (A), one needs Hs ⪯0.\nNow, take an eigenvector u of A with eigenvalue ρ > 0. Then B = A −αuuT is\nalso in S+\nn as soon as 0 ≤α ≤ρ, and trace(HT\ns (B −A)) = −αuT Hsu. So, if H ∈NS+n (A),\nwe have uT Hsu ≥0, and since Hs ⪯0, this gives uT Hsu = 0. Still because Hs is\nnegative semi-definite, this implies Hsu = 0. (This can be shown, for example, using\nSchwarz’s inequality which says that (uT Hsv)2 ≤(uT Hsu)(vT Hsv) for all v ∈Rd.)\nDecomposing A with respect to its non-zero eigenvectors, i.e., writing\nA =\np\nX\nk=1\nρkukuT\nk\n\n3.4. CONSTRAINED OPTIMIZATION PROBLEMS\n69\nwhere p = rank(A), we get AHs = HsA = 0. We therefore obtained the proposition\nProposition 3.37 Let A ∈S+\nn . Then H ∈Mn belongs to NS+n (A) if and only if −Hs ∈S+\nn\nand HsA = 0, where Hs = (H + HT )/2.\nNow, if one wants to minimize a function F over positive semidefinite matrices,\nand A∗is a minimizer, we get the necessary condition that A∗(∇F(A∗))s = 0 with\n(∇F(A∗))s positive semidefinite. These conditions are sufficient if F is convex.\nFor example, take\nF(A) = 1\n2trace(A2) −trace(BA)\n(3.34)\nwith B ∈Sn. Then (∇F(A))s = A −B and the condition is A(A −B) = 0 with A ⪰B. If\nB is diagonalized in the form B = UT DU, with U orthogonal and D diagonal, then\nthe solution is A∗= UT D+U where D+ is deduced from U by replacing non-negative\nentries by zeros.\nProjection. Let Ωbe closed convex, x0 ∈Rd and F(x) = 1\n2|x −x0|2. We have\nmin\nΩF =\nmin\nΩ∩¯B(0,R)F\nfor large enough R (e.g., larger than F(x) for any fixed point in Ω), and since the\nlatter minimization is over a compact set, argminΩF is not empty. The function F\nbeing strongly convex, its minimizer over Ωis unique and called the projection of\nx0 on Ω, denoted projΩ(x0).\nSince ∇F(x) = x −x0, theorem 3.36 implies that projΩ(x0) is characterized by\nprojΩ(x0) ∈Ωand\nx0 −projΩ(x0) ∈NΩ(projΩ(x0))\n(3.35)\nor\n(x0 −projΩ(x0))T (y −projΩ(x0)) ≤0 for all y ∈Ω.\n(3.36)\nIf x0 < Ω, then projΩ(x0) ∈∂Ω, since otherwise we would have NΩ(projΩ(x0)) = {0}\nand x0 = projΩ(x0), a contradiction. Of course, if x0 ∈Ω, then projΩ(x0) = x0.\nHere are some important examples.\n1. Let Ω= z0+V , where z0 ∈Rd and V is a linear space (i.e., Ωis an affine subset\nof Rd). Then NΩ(x) = z0 + V ⊥= x + V ⊥for all x ∈Ω, where V ⊥is the vector space of\nvectors orthogonal to V , and projΩ(x0) is characterized by projΩ(x0) ∈Ωand\n(x0 −projΩ(x0)) ∈V ⊥\nwhich is the usual characterization of the orthogonal projection on an affine space\n(compare to section 6.4).\n\n70\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\n2. If Ω= ¯B(0,1), the closed unit sphere, then NΩ(x) = R+x for x ∈∂Ω(i.e., |x| = 1).\nOne can indeed note that, if h , 0 in normal to Ωat x, then h/|h| ∈Ωso that\nhT\n h\n|h| −x\n!\n≤0\nwhich yields |h| ≤hT x. The Cauchy-Schwartz inequality implying that hT x ≤|h||x| =\n|h|, we must have equality, hT x = |h||x|, which is only possible when x and h are\ncollinear.\nGiven x0 ∈Rd with x0 ≥1, we see that projΩ(x0) must satisfy the conditions\n|projΩ(x0)| = 1 (to be in ∂Ω) and x0 −projΩ(x0) = λx0 for some λ ≥0, which gives\nprojΩ(x0) = x0/|x0|.\n3. If Ω= S+\nn and B (taking the role of x0) is a symmetric matrix, then projΩ(B) was\nfound in the previous section, and is given by A = UT D+U where UT DU provides a\ndiagonalization of B.\nThe projection has the important property of being 1-Lipschitz.\nProposition 3.38 Let Ωbe a closed convex subset of Rd. Then, for all x,y ∈Rd\n|projΩ(x) −projΩ(y)| ≤|x −y|.\n(3.37)\nProof This proposition is a special case of proposition 3.55 below.\n■\n3.4.4\nProjected gradient descent\nThe projected gradient descent algorithm minimizes F over Ωby iterating\nxt+1 = projΩ(xt −αt∇F(xt)),\n(3.38)\nwhich provides a feasible method when projΩis easy to compute. An equivalent\nformulation is\nxt+1 = argmin\nΩ\nF(xt) + ∇F(xt)T (x −xt) + 1\n2αt\n|x −xt|2.\n(3.39)\nTo justify this last statement it suffices to notice that the function in the r.h.s. can be\nwritten as\n1\n2αt\n|x −xt + αt∇F(xt)|2 −αt\n2 |∇F(xt)|2 + F(xt)\nand apply the definition of the projection.\nThe convergence properties of this algorithm will be discussed in section 3.5.5,\nin a more general context.\n\n3.5. GENERAL CONVEX PROBLEMS\n71\n3.5\nGeneral convex problems\n3.5.1\nEpigraphs\nDefinition 3.39 Let F be a convex function. The epigraph of F is the set\nepi(F) =\nn\n(x,a) ∈Rd × R : F(x) ≤a\no\n.\n(3.40)\nOne says that F is closed if epi(F) is a closed subset of Rd × R, that is: if x = limn xn and\na = limn an with F(xn) ≤an, then F(x) ≤a.\nClearly, if (x,a) ∈epi(F), then x ∈dom(F). It should also be clear that epi(F) is always\nconvex when F is convex: If (x,a),(y,b) ∈epi(F), then\nF((1 −t)x + ty) ≤(1 −t)F(x) + tF(y) ≤(1 −t)a + tb\nso that (1 −t)(x,a) + t(y,b) ∈epi(F).\nTo illustrate the definition, consider a simple example. Let F be the function\ndefined on R by F(x) = |x| if |x| < 1 and F(x) = +∞otherwise. It is convex, but not\nclosed, as can be seen by taking the sequence (1 −1/n,1) ∈epi(F), with, at the limit,\nF(1) = +∞> 1. In contrast, the function defined by ˜F(x) = |x| if |x| ≤1 and ˜F(x) = +∞\notherwise is convex and closed.\nWe have the following proposition.\nProposition 3.40 A convex function F is closed if and only if all its sub-level sets\nΛa(F) =\nn\nx ∈Rd : F(x) ≤a\no\nare closed subsets of Rd.\nProof If F is closed, then Λa(F) is the intersection of the set {(x,a) : x ∈Rd}, which is\nobviously closed, and of epi(F). It is therefore a closed set.\nConversely, assume that all Λa(F) are closed and take a sequence (xn,an) in epi(F)\nthat converges to (x,a). Then, fixing ϵ > 0, xn ∈Λa+ϵ for large enough n, and since\nthis set is closed, F(x) ≤a + ϵ. Since this is true for all ϵ > 0, we have F(x) ≤a and\n(x,a) ∈epi(F).\n■\nNote that, if F is continuous, then it is closed, so that closedness generalizes con-\ntinuity for convex functions, but it also applies to the non-smooth case.\nIf Ωis a convex subset of Rd, its indicator function σΩ(such that σΩ(x) = 0 for\nx ∈Ωand σΩ(x) = +∞otherwise) is closed if and only if Ωis a closed subset of Rd.\nThis is obvious since Λa(σΩ) = Ωif a ≥0 and ∅otherwise.\n\n72\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\n3.5.2\nSubgradients\nSeveral machine learning problems involve convex functions that are not C1, requir-\ning a generalization of the notion of derivative provided by the following definition.\nDefinition 3.41 If F is a convex function and x ∈dom(F), a vector g ∈Rd such that\nF(x) + gT (y −x) ≤F(y)\n(3.41)\nfor all y ∈Rd is called a subgradient of F at x.\nThe set of subgradients of F at x is denoted ∂F(x) and called the subdifferential of F\nat x.\nIf x ∈int(dom(F)) and F is differentiable at x, (3.5) implies that ∇F(x) ∈∂F(x).\nThis is in this case the only element of ∂F(x).\nProposition 3.42 If F is differentiable at x ∈int(dom(F)), then ∂F(x) = {∇F(x)}.\nProof We need to prove that there is no other subgradient. Assume that ∇F(x) exists\nand take y = x + ϵu in (3.41) (u ∈Rd). One gets, for g ∈∂F(x),\nϵgT u ≤F(x + ϵu) −F(x) = ϵ∇F(x)T u + o(ϵ)\nDividing by |ϵ| and letting ϵ →0 gives (depending on the sign of ϵ)\ngT u ≤∇F(x)T u and −gT u ≤−∇F(x)T u\nThis is only possible if gT u = ∇F(x)T u for all u ∈Rd, which itself implies g = ∇F.\n■\nThe next theorem, which is an obvious consequence of definition 3.41, character-\nizes minimizers of convex functions in the general case.\nTheorem 3.43 Let F : Rd →R be convex. Then x is a (global) minimizer of F if and only\nif 0 ∈∂F(x).\nThe following result shows that subgradients exist under generic conditions. We\nnote that g ∈∂F(x) if and only if proj−−→\naff (dom(F))(g) ∈∂F, because (3.41) is trivial if\nF(y) = +∞. So ∂F cannot be bounded unless aff(dom(D)) = Rd. However, it is the\npart of this set that is included in the −−→\naff (dom(F)) that is of interest.\nProposition 3.44 For all x ∈Rd, ∂F(x) is a closed convex set (possibly empty, in par-\nticular for x < dom(F)). If x ∈ridom(F), then ∂F(x) , ∅and ∂F(x) ∩−−→\naff (dom(F)) is\ncompact.\n\n3.5. GENERAL CONVEX PROBLEMS\n73\nProof The convexity and closedness of ∂F(x) is clear from the definition. If x ∈\nridom(F), there exists ϵ > 0 such that x + ϵh ∈ridom(F) for all h ∈−−→\naff (dom(F)) with\n|h| = 1. For all g ∈∂F(x) ∩−−→\naff (dom(F)), one has\n|g| = max{gT h : h ∈−−→\naff (dom(F)),|h| = 1}\n≤max((F(x + ϵh) −F(x))/ϵ : h ∈−−→\naff (dom(F)),|h| = 1)\nand the upper bound is finite because it is the maximum of a continuous function\nover a bounded set. This shows that ∂F(x) is bounded. We defer the proof that\n∂F(x) , ∅to section 3.7.\n■\nSubdifferentials are, under mild conditions, additive. More precisely, we have\nthe following proposition.\nTheorem 3.45 Let F1 and F2 be convex functions such that\nridom(F1) ∩ridom(F2) , ∅.\nThen, for all x ∈Rd, ∂(F1 + F2)(x) = ∂F1(x) + ∂F2(x).\nNote that the inclusion\n∂F1(x) + ∂F2(x) ⊂∂(F1 + F2)(x)\nas can be immediately checked by summing the inequalities satisfied by subgradi-\nents. The reverse inclusion requires the use of separation theorems for convex sets\n(see section 3.7).\nAnother important point is how the chain rule works with compositions with\naffine functions.\nTheorem 3.46 Let F be a convex function on Rd, A a d × m matrix and b ∈Rd. Let\nG(x) = F(Ax + b), x ∈Rm. Assume that there exists x0 ∈Rm such that Ax0 ∈ridom(F).\nThen, for all x ∈Rm,\n∂G(x) = AT ∂F(Ax + b).\nOne direction is straightforward and does not require the condition on ridom(F). If\ng ∈∂F(Ax + b), then\nF(z) −F(Ax + b) ≥gT (z −Ax −b),z ∈Rd\nand applying this inequality to z = Ay + b for y ∈Rm yields\nG(y) −G(x) ≥gT A(y −x)\nso that AT g ∈∂G and AT ∂F ⊂∂G. The reverse inclusion is proved in section 3.7.\nSubdifferentials can be seen as generalizations of normal cones.\n\n74\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nProposition 3.47 Assume that Ωis a closed convex subset of Rd. Then σΩ(the indicator\nfunction of Ω) has a subdifferential everywhere on Ωwith\n∂σΩ(x) = NΩ(x), x ∈Ω\nProof For x ∈Ω, (3.41) is\ngT (y −x) ≤σΩ(y)\nfor y ∈Rd, but since σΩ(y) = +∞outside of Ω, g ∈∂σΩ(x) is equivalent to\ngT (y −x) ≤0\nfor y ∈Ω, which is exactly the definition of the normal cone.\n■\nGiven this proposition, it is also clear (after noting that σΩ1 + σΩ2 = σΩ1∩Ω2) that\ntheorem 3.45 is a generalization of theorem 3.35.\n3.5.3\nDirectional derivatives\nFrom proposition 3.5, applied with y = x + h, we see that\nt 7→1\nt (F(x + th) −F(x))\nis increasing as a function of t. This property allows us to define directional deriva-\ntives of F at x.\nDefinition 3.48 Let F be convex and x ∈dom(F). The directional derivative of F at x in\nthe direction h ∈Rd is defined by\ndF(x,h) = lim\nt↓0\n1\nt (F(x + th) −F(x)),\n(3.42)\nand belong to [−∞,+∞].\nNote that, still from proposition 3.5, one has, for all x ∈dom(F) and y ∈Rd:\nF(y) ≥F(x) + dF(x,y −x)\n(3.43)\nWe have the proposition:\nProposition 3.49 If F is convex, then x∗∈argmin(F) if and only if dF(x∗,h) ≥0 for all\nh ∈Rd.\n\n3.5. GENERAL CONVEX PROBLEMS\n75\nProof If dF(x∗,h) ≥0, then F(x∗+th)−F(x∗) ≥0 for all t > 0 and this being true for all\nh implies that x∗is a minimizer. Conversely, if x∗is a minimizer, dF(x∗,h) is a limit\nof non-negative numbers and is therefore non-negative.\n■\nProposition 3.50 If F is convex and x ∈dom(F), then dF(x,h) is positively homogeneous\nand subadditive (hence convex) as a function of h, namely\ndF(x,λh) = λdF(x,h),λ > 0\nand\ndF(x,h1 + h2) ≤dF(x,h1) + dF(x,h2).\nProof Positive homogeneity is straightforward and left to the reader. For the second\none, we can write\nF(x + th1 + th2) ≤1\n2(F(x + th1/2) + F(x + th2/2))\nby convexity so that\n1\nt (F(x + th1 + th2) −F(x)) ≤1\n2\n\u00121\nt (F(x + th1/2) −F(x)) + 1\nt (F(x + th2/2) −F(x))\n\u0013\n.\nTaking t ↓0,\ndF(x;h1 + h2) ≤1\n2(dF(x;h1/2) + dF(x,h2/2)) = dF(x,h1) + dF(x,h2).\n■\nProposition 3.51 If F is convex and x ∈dom(F), then\ndF(x,h) ≥sup{gT h,g ∈∂F(x)}.\nIf x ∈ridom(F), then\ndF(x,h) = max{gT h,g ∈∂F(x)}.\nProof If g ∈∂F(x), then for all t > 0\nF(x + th) −F(x) ≥tgT h.\nDividing by t and passing to the limit yields dF(x,h) ≥gT h.\nWe prove that the maximum is attained at some g ∈∂F(x) when x ∈ridom(F).\nIn this case, the domain of the convex function G : ˜h 7→dF(x, ˜h) is the vector space\nparallel to aff(dom(F)), namely\ndom(G) = {h : x + h ∈aff(dom(F))}.\n\n76\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nIndeed, for any h in this set, there exists ϵ > 0 such that x + th ∈dom(F) for 0 < t < ϵ\nand dF(x,h) ≤(F(x +th)−F(x))/t < ∞. Conversely, if h ∈dom(G), then F(x +th)−F(x)\nmust be finite for small enough t, so that x + th ∈dom(F) and x + h ∈aff(dom(F)).\nAs a consequence, for any h ∈aff(dom(F)), there exists ˆg ∈∂G(h), which therefore\nsatisfies\ndF(x, ˜h) ≥dF(x,h) + ˆgT (˜h −h)\nfor any ˜h ∈Rd (the upper bound is infinite if ˜h < dom(G)). Letting ˜h →0, we get\ndF(x,h) ≤ˆgT h.\nAlso, by positive homogeneity, we have\ntdF(x, ˜h) ≥dF(x,h) + ˆgT (t˜h −h)\nfor all t > 0, which requires dF(x, ˜h) ≥ˆgT ˜h for all ˜h, and in particular dF(x,h) = ˆgT h.\nSince\nF(x + ˜h) −F(x) ≥dF(x, ˜h) ≥ˆgT ˜h\nwe see that ˆg ∈∂F(x), with ˆgT h = dF(x,h), which concludes the proof.\n■\nThe next proposition gives a criterion for a vector g to belong to ∂F(x) based on\ndirectional derivatives.\nProposition 3.52 Assume that x ∈dom(F) where F is convex. If g ∈Rd is such that\ndF(x,h) ≥gT h\nfor all h ∈Rd, then g ∈∂F(x).\nProof Just use the fact that dF(x,h) ≤F(x + h) −F(x).\n■\n3.5.4\nSubgradient descent\nWhen F is a non-differentiable a convex function, directions g such that −g ∈∂F(x)\ndo not always provide directions of descent. Indeed, g ∈∂F(x) implies\nF(x −αg) ≥F(x) −α|g|2\nbut the inequality goes in the “wrong direction.” However, we know that, for any\nh ∈Rd, there exists gh ∈∂F(x) such that\ndF(x,−h) = −gT\nh h ≥−gT h\n\n3.5. GENERAL CONVEX PROBLEMS\n77\nfor all g ∈∂F(x). As a consequence, any non-vanishing solution of the equation\nh = gh will provide a direction of descent. This suggests looking for h ∈∂F(x) such\nthat h , 0 and |h|2 ≤gT h for all g ∈∂F(x). Since gT h ≤|g||h|, this requires that |h| ≤|g|\nfor all g ∈∂F(x), i.e.,\nh = argmin\n∂F(x)\n(g 7→|g|).\n(3.44)\nConversely, if h is the minimal-norm element of ∂F(x) (which is necessarily unique\nsince the norm is strictly convex and ∂F(x) is convex and compact), then |h|2 ≤|h +\nt(g −h)|2 for all g ∈∂F(x) and t ∈[0,1], and taking the difference yields\n2thT (g −h) + t2|g −h|2 ≥0.\nThe fact that this holds for all t ≥0 requires that hT (g −h) ≥0 as required. We have\ntherefore proved that h defined by (3.44) is a descent direction for F at x (it is actually\nthe steepest descent direction: see [204] for a proof), justifying the algorithm\nxt+1 = xt −αt argmin\n∂F(x)\n(g 7→|g|)\nas subgradient descent iterations.\nExample. Consider the minimization of\nF(x) = ψ(x) + λ\nn\nX\ni=1\n|x(i)|\nwhere ψ is a C1 convex function on Rd. Let A(x) = {i : x(i) = 0}. Then\n∂F(x) =\n\n∇ψ(x) + λ\nX\ni<A(x)\nsign(x(i)) + λ\nX\ni∈A(x)\nρiei : |ρi| ≤1,i ∈A(x)\n\nwhere ei is the ith vector of the canonical basis of Rd.\nFor g = ∇ψ(x) + λP\ni<A(x) sign(x(i)) + λP\ni∈A(x) ρiei, we have\n|g|2 =\nX\ni<A(x)\n(∂iF(x) + λsign(x(i)))2 +\nX\ni∈A(x)\n(∂iψ(x) −λρi)2.\nDefine\ns(t) = sign(t)min(|t|,1).\nThen h satisfying (3.44) is given by\nh(i) =\n(∂iψ(x) −λsign(x(i)) if i < A(x)\nλ s(∂iψ(x)/λ) if i ∈A(x).\n\n78\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nIn more complex situations, the extra minimization step at each iteration of the\nalgorithm can be challenging computationally. The following subgradient method\nuses an averaging approach to minimize F without requiring finding subgradients\nwith minimal norms. It simply defines\nxt+1 = xt −αtgt,\ngt ∈∂F(xt)\nand computes\n¯xt =\nPt\nj=1 αjxj\nPt\nj=1 αj\n.\nWe refer to [204] for a proof of convergence of this method.\n3.5.5\nProximal Methods\nProximal operator. We start with a few simple facts. Let F be a closed convex\nfunction and ψ be convex and differentiable, with dom(ψ) = Rd. Let G = F + ψ.\nThen G is a closed convex function. Indeed, consider the sub-level set Λa(G) = {x :\nG(x) ≤a} and assume that xn →x with xn ∈Λa(g). Then ψ(xn) →ψ(x) by continuity,\nand for all ϵ > 0, we have, for large enough n, F(xn) ≤a −ψ(x) + ϵ. This inequality\nremains true at the limit because F is closed, yielding G(x) ≤a + ϵ for all ϵ > 0, so\nthat x ∈Λa(G).\nWe have ridom(F) ∩ridom(ψ) , ∅so that (by theorem 3.45 and proposition 3.42)\n∂G(x) = ∇ψ(x) + ∂F(x). In particular, x∗is a minimizer of G if and only if −∇ψ(x∗) ∈\n∂F(x∗).\nIt one assumes that ψ is strongly convex, so that there exists m and L such that\nm\n2 |y −x|2 ≤ψ(y) −ψ(x) −∇ψ(x)T (y −x) ≤L\n2|y −x|2\nfor all x,y ∈Rd, then a minimizer of G exists and is unique. To see this, fix x0 ∈\nridom(F) and consider the closed convex set\nΩ0 = ΛG(x0)(G) = {x : G(x) ≤G(x0)}.\nAny minimizer of G must clearly belong to Ω0. If x ∈Ω0, we have\nF(x) + ψ(x0) + ∇ψ(x0)T (x −x0) + m\n2 |x −x0|2 ≤G(x) ≤G(x0).\nMoreover, there exists (from proposition 3.44) an element g ∈∂F(x0) so that F(x) ≥\nF(x0) + gT (x −x0) for all x ∈Rd. We therefore get\nF(x0) + ψ(x0) + (g + ∇ψ(x0))T (x −x0) + m\n2 |x −x0|2 ≤G(x0).\n\n3.5. GENERAL CONVEX PROBLEMS\n79\nfor all x ∈Ω0, which shows that Ω0 must be bounded and therefore compact. There\nexists a minimizer x∗of G on Ω0, and therefore on all Rd. This minimizer is unique,\nsince the sum of a convex function and a strictly convex function is strictly convex.\nIn particular, for any closed convex F, we can apply the previous remarks to\nG : v 7→F(v) + 1\n2|x −v|2\nwhere x ∈Rd is fixed. The function ψ : v 7→|v−x|2/2 is strongly convex (with L = m =\n1) and G therefore has a unique minimizer v∗. This is summarized in the following\ndefinition.\nDefinition 3.53 Let F be a closed convex function. The proximal operator associated to\nF is the mapping proxF : Rd →dom(F) defined by\nproxF(x) = argmin\nRd\n(v 7→F(v) + 1\n2|x −v|2).\n(3.45)\nFrom the previous discussion, we also deduce\nProposition 3.54 Let F be a closed convex function and α > 0. We have x′ = proxαF(x)\nif and only if x ∈x′ + α∂F(x′). In particular, x∗is a minimizer of F if and only if x∗=\nproxαF(x∗)\nLet us take a few examples.\n• Let F(x) = λ|x|, x ∈Rd, for some λ > 0. Then F is differentiable everywhere except\nat x = 0 and dom(F) = Rd. We have ∂F(x) = λx/|x| for x , 0. A vector g belongs to\n∂F(0) if and only if\ngT x ≤λ|x|\nfor all x ∈Rd, which is equivalent to |g| ≤λ so that ∂F(0) = ¯B(0,λ).\nWe have x′ = proxF(x) if and only if x′ , 0 and x = x′ + λx′/|x′| or x′ = 0 and |x| ≤λ.\nFor |x| > λ, the equation x = x′ + λx′/|x′| is solved by\nx′ = |x| −λ\n|x|\nx\nyielding\nproxF(x) =\n\n|x| −λ\n|x|\nx if |x| ≥λ\n0 otherwise\n(3.46)\n• Let Ωbe a closed convex set. Then proxσΩ= projΩ, the projection operator on Ω,\nas directly deduced from the definition.\n\n80\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nThe following proposition can then be compared to proposition 3.38.\nProposition 3.55 Let F be a closed convex function. Then proxF is 1-Lipschitz: for all\nx,y ∈Rd,\n|proxF(x) −proxF(y)| ≤|x −y|.\n(3.47)\nProof Let x′ = proxF(x) and y′ = proxF(y). Then, there exists g ∈∂F(x′) and h ∈\n∂F(y′) such that x = x′ + g and y = y′ + h. Moreover, we have\nF(y′) −F(x′) ≥gT (y′ −x′)\nF(x′) −F(y′) ≥hT (x′ −y′)\nfrom which we deduce gT (y′ −x′) ≤hT (y′ −x′) or (h −g)T (x′ −y′) ≥0. Expressing g,h\nin terms or x,x′,y,y′, we get (y −x −y′ + x′)T (y′ −x′) ≥0 or\n|y′ −x′|2 ≤(y −x)T (y′ −x′) ≤|y −x||y′ −x′|\nwhich is only possible if |y′ −x′| ≤[y −x|.\n■\nIf F is differentiable, then x′ = proxαF(x) satisfies\nx′ = x −α∇F(x′)\nso that x 7→proxαF(x) can be interpreted as an implicit version of the standard gra-\ndient step x 7→x−α∇F(x). The iterations x(t+1) = proxαtF(x(t)) provide an algorithm\nthat converges to a minimizer of F (this will be justified below). This algorithm is\nrarely practical, however, since the minimization required at each step is not nec-\nessarily much easier to perform than minimizing F itself. The proximal operator,\nhowever, is especially useful when combined with splitting methods.\nProximal gradient descent. Assume that the objective function F takes the form\nF(x) = G(x) + H(x)\n(3.48)\nwhere G is C1 on Rd and H is a closed convex function. We first note that\ndF(x,h) = lim\nt↓0\nF(x + th) −F(x)\nt\nis well defined (even if G is not convex, because it is smooth), with\ndF(x,h) = ∇G(x)T h + dH(x,h)\n\n3.5. GENERAL CONVEX PROBLEMS\n81\nIn particular, if x∗be a minimizer of F, then dF(x,h) ≥0 for all h so that dH(x,h) ≥\n−∇G(x)T h for all h. Using proposition 3.52, this shows that −∇G(x) ∈∂H(x), which\nis a necessary condition for optimality for F (which is sufficient if G is convex).\nProximal gradient descent implements the algorithm\nxt+1 = proxαtH(xt −αt∇G(xt)).\n(3.49)\nWe note that a stationary point of this algorithm, i.e. a point x such that x = proxαtH(x−\nαt∇G(x)) must be such that x −αt∇G(x) ∈x + αt∂H(x), so that −∇G(x) ∈∂H(x). This\nshows that the property of being stationary does not depend on αt > 0, and is equiv-\nalent to the necessary optimality condition that was just discussed.\nWe first study this algorithm under the assumption that G is L-C1, which implies\nthat, for all x,y ∈Rd.\nG(y) ≤G(x) + ∇G(x)T (y −x) + L\n2|x −y|2.\nAt iteration t, we have\nxt −αt∇G(xt) ∈xt+1 + αt∂H(xt+1)\nwhich implies, in particular\nαtH(xt)−αtH(xt+1) ≥(xt−xt+1)T (xt−xt+1−αt∇G(xt)) = |xt−xt+1|2+αt∇G(xt)T (xt+1−xt)\nDividing by αt and adding G(xt) −G(xt+1), we get\nF(xt) −F(xt+1) ≥1\nαt\n|xt −xt+1|2 + G(xt) + ∇G(xt)T (xt+1 −xt) −G(xt+1)\n≥\n 1\nαt\n−L\n2\n!\n|xt −xt+1|2\n(3.50)\nso that proximal gradient descent iterations reduce the objective function as soon as\nαt ≤2/L.\nAssuming that αt < 2/L, (3.50) can be rewritten as\n\f\f\f\f\f\nxt+1 −xt\nαt\n\f\f\f\f\f\n2\n≤\n2\nαt(2 −αtL)(F(xt) −F(xt+1).\nThis inequality should be compared to (3.11) in the unconstrained case. It yields, in\nparticular, the inequality\nmin\n(\f\f\f\f\f\nxt+1 −xt\nαt\n\f\f\f\f\f : t ≤T\n)\n≤\nF(x0) −minF\n2T min{αt(2 −αtL),t ≤T} .\n(3.51)\n\n82\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nAs a consequence, if one runs proximal gradient descent until |xt+1 −xt|/αt is small\nenough, the algorithm will terminate in finite time as soon as αt is bounded from\nbelow (and, in particular, if αt is constant).\nIf we assume that G is convex, in addition to being L-C1, then we have a stronger\nresult. Let x∗be a minimizer of F. Then, using again xt−αt∇G(xt) ∈xt+1+αt∂H(xt+1),\nwe have\nαtH(x∗) −αtH(xt+1) ≥(x∗−xt+1)T (xt −xt+1 −αt∇G(xt))\nand\nαtF(x∗) −αtF(xt+1) ≥(x∗−xt+1)T (xt −xt+1) −αt(x∗−xt+1)T ∇G(xt)) + αtG(x∗) −αtG(xt+1)\n≥(x∗−xt+1)T (xt −xt+1) −αt(x∗−xt)T ∇G(xt)) + αtG(x∗)\n+ αt(xt+1 −xt)T ∇G(xt) −αtG(xt+1)\n≥(x∗−xt+1)T (xt −xt+1) −αtL\n2 |xt −xt+1|2\nAssuming that αtL ≤1, then\nαtF(x∗) −αtF(xt+1) ≥(x∗−xt+1)T (xt −xt+1) −1\n2|xt −xt+1|2 = 1\n2(|xt+1 −x∗|2 −|xt −x∗|2),\nwhich we rewrite as\nαt(F(xt+1) −F(x∗)) ≤1\n2(|xt −x∗|2 −|xt+1 −x∗|2)\nNote that, from (3.50), we also have\nF(xt+1) ≤F(xt) −1\n2αt\n|xt −xt+1|2\nwhen αtL ≤1, which shows, in particular that F(xt) is decreasing. Fixing a time T,\nwe have, from these two observations\nαt(F(xT ) −F(x∗)) ≤1\n2(|xt −x∗|2 −|xt+1 −x∗|2)\nfor all t ≤T −1, and summing over T ,\n(F(xT ) −F(x∗))\nT −1\nX\nt=0\nαt ≤1\n2(|x0 −x∗|2 −|xT −x∗|2)\nyielding\nF(xT ) −F(x∗) ≤|x0 −x∗|2\n2PT−1\nt=0 αt\n.\n(3.52)\nWe summarize this in the following theorem, specializing to the case of constant\nstep αt.\n\n3.6. DUALITY\n83\nTheorem 3.56 Let G be am L-C1 function defined on Rd and H be closed convex. As-\nsume that F = G+H has a minimizer x∗. Then the algorithm (3.49) run with αt = α ≤1/L\nfor all t is such that, for all T > 0,\nF(xT ) −F(x∗) ≤|x0 −x∗|2\n2αT\n.\n(3.53)\nAlso, when G = 0, F = H, we retrieve the proximal iterations algorithm\nxt+1 = proxαF(xt),\n(3.54)\nand we have just proved that it converges for any α > 0 as soon as F is a closed convex\nfunction.\nOne gets a stronger result under the assumption that G is C2, and is such that the\neigenvalues of ∇2G(x) are included in a fixed interval [m,L] for all x ∈Rd with m > 0.\nSuch a G is strongly convex, which implies that F has a unique minimizer. We have\n|xt+1 −x∗| =\n\f\f\fproxαtH(xt −αt∇G(xt)) −proxαtH(x∗−αt∇G(x∗)\n\f\f\f\n≤|xt −x∗−αt(∇G(xt)) −∇G(x∗))|.\nWrite\n|xt −x∗−αt(∇G(xt)) −∇G(x∗)| =\n\f\f\f\f\f\f\nZ 1\n0\n(IdRn −αt∇2G(x∗+ t(xt −x∗)))(xt −x∗)dt\n\f\f\f\f\f\f\n≤\nZ 1\n0\n\f\f\f(IdRn −αt∇2G(x∗+ t(xt −x∗)))(xt −x∗)\n\f\f\fdt\n≤max(|1 −αtm|,|1 −αtL|)|xt −x∗|\nwhere we have use the fact that the eigenvalues of IdRn −αt∇2G(x) are included in\n[1−αtL,1−αtm] for all x ∈Rd. If one assumes that αt ≤1/L, so that max(|1−αtm|,|1−\nαtL|) ≤1 −αtm, one gets\n|xt+1 −x∗| ≤(1 −αtm)|xt −x∗|.\nIterating this inequality, we get the theorem that we state for constant αt.\nTheorem 3.57 Let F = G + H where G is a C2 convex function and H is a closed convex\nfunction. Assume that the eigenvalues of ∇2G are uniformly included in [m,L] with m > 0.\nLet x∗argminF.\nLet (xt) satisfy (3.49) with constant αt = α < 1/L. Then\n|xt −x∗| ≤(1 −αm)t|x0 −x∗|.\nNote that these results also apply to projected gradient descent (section 3.4.4),\nwhich is a special case (taking G = σΩ).\n\n84\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\n3.6\nDuality\n3.6.1\nGeneralized KKT conditions\nA constrained convex minimization problem consists in the minimization of a closed\nconvex function F over a closed convex set Ω⊂ridom(F). We have seen in theo-\nrem 3.36 that, for smooth F, any solution x∗of this problem had to satisfy −∇F(x∗) ∈\nNΩ(x) where\nNΩ(x) = {h : hT (y −x) ≤0 for all y ∈Ω}.\nThe next theorem generalizes this property to the non-smooth convex case, for\nwhich the necessary optimality condition is also sufficient.\nTheorem 3.58 Let F be a closed convex function, Ω⊂ridom(F) a nonempty closed con-\nvex set. Then x∗∈argminΩF if and only if\n0 ∈∂F(x∗) + NΩ(x∗)\nProof Introduce the indicator function σΩ. Then minimizing F over Ωis the same\nas minimizing G = F+σΩover Rd. The assumptions imply that ridom(σΩ) = relint(Ω) ⊂\nridom(F) and therefore\n∂G(x) = ∂F(x) + ∂σΩ(x)\nfor all x ∈Ω. Since\n∂σΩ(x) = NΩ(x)\nthe result follows for the characterization of minimum of convex functions.\n■\nIn the following, we will restrict to the situation in which F is finite (i.e., dom(F) =\nRd) and Ωis defined through a finite number of equalities and inequalities, taking\nthe form\nΩ=\nn\nx ∈Rd : γi(x) = 0,i ∈E and γi(x) ≤0,i ∈I\no\nfor functions (γi,i ∈C = E∪I) such that γi : x 7→bT\ni x+βt is affine for all i ∈E and γi is\nclosed convex for all i ∈I. This is similar to the situation considered in section 3.4.1,\nwith additional convexity assumptions, but without assuming smoothness. We re-\ncall the definition of active constraints from section 3.4.1, namely, for x ∈Ω,\nA(x) = {i ∈C : γi(x) = 0}.\nFollowing the discussion in the smooth case, define the set N ′\nγ(x) ⊂Rd by\nN ′\nγ(x) =\n\nX\ni∈A(x)\nλisi : si ∈∂γi(x),i ∈A(x),λi ≥0,i ∈A(x) ∩I\n\n.\n\n3.6. DUALITY\n85\nThe property 0 ∈∂F(x∗) + Nγ(x∗) is the expression of the KKT conditions in the non-\nsmooth case. It holds for x∗∈argminΩF as soon as NΩ(x∗) = N ′\nγ(x∗), which is true\nunder appropriate constraint qualifications. We here replace the MF-CQ in defini-\ntion 3.30 by the following conditions that do not involve gradients.\nDefinition 3.59 Let (γi,i ∈C = E ∪I) be a set of equality and inequality constraints,\nwith γi : x 7→bT\ni x + βi, i,∈E and γi closed convex, i ∈I. One says that these constraints\nsatisfy the Slater constraint qualifications (Sl-CQ) if and only if:\n(Sl 1) The vectors (bi,i ∈E) are linearly independent.\n(Sl 2) There exists x ∈Rd such that γi(x) = 0 for i ∈E and γi(x) < 0 for i ∈I.\nThe first constraint is a very mild condition. When it is not satisfied, this means that\nsome bi’s are linear combinations of others, and equality constraints for the latter\nimplies equality constraints for the former. These redundancies can therefore be\nremoved without changing the problem.\nNote that (Sl2) can be replaced by the apparently weaker condition that, for all\ni ∈I, there exists xi ∈Rd satisfying all the constraints and γi(xi) < 0. Indeed, if\nthis is true, then the average, ¯x, of (xi,i ∈I) also satisfies the equality constraints by\nlinearity, and if i ∈I,\nγi( ¯x) ≤1\n|I|\nX\nj∈I\nγi(x(j)) ≤1\n|I|γi(x(i)) < 0.\nThe following proposition makes a connection between the Slater conditions and\nthe MF-CQ in definition 3.30.\nProposition 3.60 Assume that γi, i ∈I are convex C1 functions. Then, if there exists a\nfeasible point x∗that satisfies the MF-CQ, there exists another point x satisfying the Sl-\nCQ. Conversely, if there exists x satisfying the Sl-CQ, then every feasible point x∗satisfies\nthe MF-CQ.\nProof The linear independence conditions on equality constraints are the same in\nMF-CQ and Sl-CQ, so that we only need to consider inequality constraints.\nLet x∗satisfy MF-CQ, and take h , 0 such that bT\ni h = 0 for all i ∈E, and ∇γi(x∗)T h <\n0, i ∈A(x) ∩I. Then x∗+ th satisfies the equality constraints for all t ∈R. If i ∈I is\nnot active, then γi(x∗) < 0 and this will remain true at x∗+ th for small t by continu-\nity. If i ∈A(x) ∩I, then a first order expansion gives γi(x∗+ th) = t∇γi(x∗)T h + o(|h|),\nwhich is also negative for small enough t > 0. So, x∗+th satisfies the Sl-CQ for small\nenough t > 0.\n\n86\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nConversely, let x satisfy the Sl-CQ. Take a feasible point x∗. If x∗= x, then there\nis no active inequality constraint and x∗satisfies MF-CQ. Assume x∗, x and let\nh = x −x∗. Then bT\ni h = 0 for all i ∈E, and if i ∈I ∩A(x∗),\n0 > γi(x) = γi(x∗+ h) ≥γi(x∗) + ∇γi(x∗)T h = ∇γi(x∗)T h\nso that x∗satisfies MF-CQ.\n■\nThe following theorem, that we give without proof, states that the Slater condi-\ntions implies that the KKT conditions are satisfied for a minimizer.\nTheorem 3.61 Assume that all the constraints are affine, or that they satisfy the Sl-CQ\nin definition 3.59. Let x∗∈argminΩF. Then NΩ(x∗) = N ′\nγ(x∗), so that there exist s0 ∈\n∂F(x∗), si ∈∂γi(x∗), i ∈A(x∗), (λi,i ∈A(x∗)) with λi ≥0 if i ∈I ∩A(x∗), such that\ns0 +\nX\ni∈A(x)\nλisi = 0\n(3.55)\n3.6.2\nDual problem\nConsider the Lagrangian\nL(x,λ) = F(x) +\nX\ni∈C\nλiγi(x)\ndefined in (3.29) and let D = {λ : λi ≥0,i ∈I}. Because the functions γi are non-\npositive on Ω, we have\nL(x,λ) ≤F(x)\nfor all x ∈Ωand λ ∈D, which implies that\nL∗(λ) = inf{L(x,λ) : x ∈Rd}\nis such that L∗(λ) ≤F(x) for all λ ∈D and x ∈Ω. Define\nˆd = sup{L∗(λ) : λ ∈D}\nand\nˆp = inf{F(x) : x ∈Ω},\nwhose computations respectively represent the dual and primal problems. Then, we\nhave ˆd ≤ˆp.\nWe did not need much of our assumptions (not even F to be convex) to reach\nthis conclusion. When the converse inequality is true (so that the duality gap ˆp −ˆd\nvanishes), the dual problem provides important insights on the primal problem, as\nwell as alternative ways to solve it. This is true under the Slater conditions.\n\n3.6. DUALITY\n87\nTheorem 3.62 The duality gap vanishes when the constraints are all affine, or when they\nsatisfy the Sl-CQ in definition 3.59. In this case, any solution λ∗of the dual problem\nprovides Lagrange multipliers in theorem 3.61 and conversely.\nWe justify this statement, as a consequence of theorem 3.61 and the following\nanalysis. The Lagrangian L(x,λ) is linear in λ, and when λ ∈D, is a convex func-\ntion of x. Moreover, one can use subdifferential calculus (theorem 3.45) to con-\nclude that, for any λ ∈D, (3.55) expresses the fact that 0 ∈∂xL(x∗,λ), i.e., that\nx∗∈argminRd L(·,λ).\nFixing x ∈Rd, one can also consider the maximization of L in λ ∈D. Clearly, if\nx < Ω, so that γi(x) , 0 for some i ∈E or γi(x) > 0 for some i ∈I, then maxD L(x,λ) =\n+∞. If x ∈Ω, then the slackness conditions, which require λ(i)γi(x) = 0, i ∈I, ensure\nthat λ ∈argmaxD L(x,·).\nAs a consequence, any pair x∗∈Ω, λ∗∈D satisfying the KKT conditions is such\nthat\nL(x∗,λ) ≤L(x∗,λ∗) ≤L(x,λ∗)\n(3.56)\nfor all x ∈Rd and λ ∈D. Such a pair (x∗,λ∗) is called a saddle point of the function L.\nConversely, any saddle point of L, i.e., any (x∗,λ∗) ∈Rd ×D satisfying (3.56), must be\nsuch that x∗∈Ω(to ensure that L(x∗,·) is bounded), and satisfies the KKT conditions.\nWe therefore obtain the equivalence of the two properties, for (x∗,λ∗) ∈Rd × D:\n(i) x∗∈Ωand (x∗,λ∗) satisfies the KKT conditions.\n(ii) Equation (3.56) holds for all (x,λ) ∈Rd × D.\nConsider now the additional condition that\n(iii) x∗∈argminΩF and λ∗∈argmaxD L∗.\nWe already know that, if (x∗,λ∗) satisfy the KKT conditions, then x∗∈argminΩF\n(because N ′\nγ(x∗) ⊂NΩ(x∗)). Moreover, if (3.56) holds, then the inequality L(x∗,λ) ≤\nL(x∗,λ∗) implies that L∗(λ) ≤L(x∗,λ∗) for all λ ∈D. The inequality L(x∗,λ∗) ≤L(x,λ∗)\nfor all x implies that L(x∗,λ∗) ≤L∗(λ∗). We therefore obtain the fact that λ∗∈argmaxL∗(λ).\nTo summarize, we have\n(i) ⇔(ii) ⇒(iii).\nTo obtain the final equivalence, we need to assume constraints qualifications,\nsuch as Slater’s conditions, to ensure that N ′\nγ(x∗) = NΩ(x∗). If this holds, then (iii)\nimplies (via theorem 3.61) that there exists ˜λ such that (i) and (ii) are satisfied for\n\n88\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\n(x∗, ˜λ), with L(x∗, ˜λ) = L∗( ˜λ) and ˜λ ∈argminD L∗. This shows that L∗( ˜λ) = L∗(λ∗). More-\nover, from (3.56), we have\nL(x∗,λ∗) ≤L(x∗, ˜λ) = L∗( ˜λ),\nand, by definition of L∗, L(x∗,λ∗) ≥L∗(λ∗). This shows that L(x∗,λ∗) = L(x∗, ˜λ). As a\nconsequence, for all (x,λ) ∈Rd × D:\nL(x∗,λ) ≤L(x∗, ˜λ) = L(x∗,λ∗) = L∗(λ∗) = inf\nRd L(·,λ∗) ≤L(x,λ∗)\nso that (x∗,λ∗) satisfies (ii).\n3.6.3\nExample: Quadratic programming\nQuadratic programming problems minimize F(x) = 1\n2xT Ax −bT x, where A is a pos-\nitive semidefinite matrix and b ∈Rd, subject to affine constraints cT\ni x −di = 0, i ∈E\nand cT\ni x −di ≤0, i ∈I.\nWe here consider the following objective function. Introduce variables x ∈Rd,\nx0 ∈R and ξ ∈RN and minimize, for a fixed parameter γ,\nF(x,x0,ξ) = 1\n2|x|2 + γ\nN\nX\nk=1\nξ(k)\nsubject to constraints, for k = 1,...,N ξ(k) ≥0 and\nbk(x0 + xT ak) + ξ(k) ≥1\nwhere bk ∈{−1,1} and ak ∈Rn respectively denote the kth output and input train-\ning sample. This algorithm minimizes a quadratic function of the input variables\n(x,x0,ξ) subject to linear constraints, and is an instance of a quadratic program-\nming problem (this is actually the support vector machine problem for classification,\nwhich will be described in section 8.4.1).\nIntroduce Lagrange multipliers ηk for the constraint ξ(k) ≥0 and αk for bk(x0 +\nxT ak) + ξ(k) ≥1. The Lagrangian then takes the form\nL(x,x0,ξ,α,η) = 1\n2|x|2 + γ\nN\nX\nk=1\nξ(k) −\nN\nX\ni=1\nηkξ(k) −\nN\nX\nk=1\nαk(bk(x0 + xT ak) + ξ(k) −1)\n= 1\n2|x|2 +\nN\nX\nk=1\n(γ −ηk −αk)ξ(k) −x0\nN\nX\nk=1\nαkbk −xT\nN\nX\nk=1\nαkbkak +\nN\nX\nk=1\nαk.\n\n3.6. DUALITY\n89\nWe compute the dual Lagrangian L∗by minimizing with respect to the primal vari-\nables. We note that L∗(α,η) = −∞when PN\nk=1\nalphakbk , 0, so that PN\nk=1 αkbk = 0 is a constraint for the dual problem. The min-\nimization in ξ(k) also gives −∞unless γ −ηk −αk = 0, which is therefore another\nconstraint. Finally, the optimal values of x is\nx =\nN\nX\nk=1\nαkbkak\nand we obtain the expression of the dual problem, which maximizes\n−1\n2\nN\nX\nk,l=1\nαkαlbkblaT\nk al +\nN\nX\nk=1\nαk\nsubject to ηk,αk ≥0, γ −ηk −αk = 0 and PN\nk=1 αkbk = 0. The conditions on ηk and αk\ncan be rewritten as 0 ≤αk ≤γ, ηk = γ −αk, and since the rest of the problem does\nnot depends on η, the dual problem can be reduced to maximizing\nL∗(α) = −1\n2\nN\nX\nk,l=1\nαkαlaT\nk al +\nN\nX\nk=1\nαk\nsubject to 0 ≤αk ≤γ and PN\nk=1 αkbk = 0.\n3.6.4\nProximal iterations and augmented Lagrangian\nThe concave function L∗can be maximized by minimizing −L∗using proximal itera-\ntions ((3.54)):\nλ(t + 1) = prox−αtL∗(λ(t)) = argmax\nD\n(λ 7→L∗(λ) −1\n2αt\n|λ −λ(t)|2).\nIntroduce the function\nϕ(x,λ) = F(x) +\nX\ni∈C\nλ(i)γi(x) −1\n2αt\n|λ −λ(t)|2\nso that\nλ(t + 1) = argmax\nµ∈D\ninf\nx∈Rn ϕ(x,µ).\nThe function ϕ is convex in x and strongly concave in µ. Results in “minimax\ntheory” [27] implies that one has the equality\nmax\nµ∈D inf\nx∈Rn ϕ(x,µ) = inf\nx∈Rd sup\nµ∈D\nϕ(x,µ)\n(3.57)\n\n90\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\n(Note that the left-hand side of this equation is never larger than the right-hand\nside, but their equality requires additional hypotheses—which are satisfied in our\ncontext—in order to hold.)\nImportantly, the maximization in µ in the right-hand side has a closed form so-\nlution. It requires to maximize\nX\ni∈C\n\u0012\nµiγi(x) −1\n2αt\n(µi −λi(t))2\u0013\nsubject to µi ≥0 for i ∈I, and each µi can be computed separately. For i ∈E, there is\nno constraint on µi, and one finds\nµi = λi(t) + αtγi(x),\nand\nµiγi(x) −1\n2αt\n(µi −λi(t))2 = λi(t)γi + αt\n2 γi(x)2 =\n1\n2αt\n(λi(t) + αtγi(x))2 −λi(t)2\n2αt\n.\nFor i ∈I, the solution is\nµi = max(0,λi(t) + αtγi(x))\nand one can check that, in this case:\nµiγi(x) −1\n2αt\n(µi −λi(t))2 =\n1\n2αt\nmax(0,λi(t) + αtγi(x))2 −λi(t)2\n2αt\nAs a consequence, the right-hand side of (3.57) requires to minimize\nG(x) = F(x) + 1\n2αt\nX\ni∈E\n(λi(t) + αtγi(x))2 + 1\n2αt\nX\ni∈I\nmax(0,λi(t) + αtγi(x)))2\n−1\n2αt\nX\ni∈C\nλi(t)2.\nIf we assume that the sub-level sets {x ∈Ω: F(x) ≤ρ} are bounded (or empty) for any\nρ ∈R, then so are the sets {x ∈Rn : G(x) ≤ρ}, and this is a sufficient condition for the\nexistence of a saddle point for ϕ, which is a pair (x∗,λ∗) such that, for all (x,λ) ∈Rn×D,\nϕ(x∗,λ) ≤ϕ(x∗,λ∗) ≤ϕ(x,λ∗).\nOne can then check that this implies that x∗∈argminRn G while λ∗= λ(t + 1), so that\n\n3.6. DUALITY\n91\nthe latter can be computed as follows:\n\nx(t) = argmin\nx∈Rn\n(\nF(x) + 1\n2αt\nX\ni∈E\n(λi(t) + αtγi(x))2\n+ 1\n2αt\nX\ni∈I\nmax(0,λi(t) + αtγi(x)))2\n)\nλi(t + 1) = λi(t) + αtγi(x(t)), i ∈E\nλi(t + 1) = max(0,λi(t) + αtγi(x(t))), i ∈I\n(3.58)\nThese iterations define the augmented Lagrangian algorithm. Starting this algorithm\nwith some λ(0) ∈R|C|, and constant α, λ(t) will converge to a solution ˆλ of the dual\nproblem. The last two iterations stabilizing imply that γi(x(t)) converges to 0 for\ni ∈E, and also for i ∈I such that ˆλi > 0, and that limsupγi(x(t)) = 0 otherwise. This\nshows that, if x(t) converges to a limit ˜x, then G( ˜x) = F( ˜x). However, for any x ∈Ω,\nwe have\nG(x(t)) ≤G(x) ≤F(x)\n(the proof being left to the reader), showing that ˜x ∈argminΩF.\nNote that the augmented Lagrangian method can also be used in non-convex\noptimization problems [146], requiring in that case that α is small enough.\n3.6.5\nAlternative direction method of multipliers\nWe return to a situation considered in section 3.5.5 where the function to minimize\ntakes the form F(x) = G(x) + H(x). Here, we do not assume that G or H is smooth,\nbut we will need their respective proximal operators to be easy to compute.\nThe problem can be reformulated as a minimization with equality constraints,\nnamely that of minimizing ˜F(x,z) = G(x) + F(z) subject to x = z. We will actually\nconsider a more general situation, namely the problem minimizing a function ˜F(x,z)\nsubject to constraints Ax + Bz = c where A and B are respectively d × n and d × m\nmatrices, x ∈Rn, z ∈mRm, c ∈Rd. The augmented Lagrangian algorithm applied to\nthis problem leads to iterate (with only equality constraints)\n\nxt,zt = argmin{G(x) + F(z) + 1\n2αt\n|λt + αt(Ax + Bz −c)|2}\nλt+1 = λt + αt(Axt + Bzt −c)\nwith λt ∈Rd.\n\n92\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nOne can now consider splitting the first step in two and iterate:\n\nxt = argmin{G(x) + F(zt−1) + 1\n2αt\n|λt + αt(Ax + Bzt−1 −c)|2}\nzt = argmin{G(xt) + F(z) + 1\n2αt\n|λt + αt(Axt + Bz −c)|2}\nλt+1 = λt + αt(Axt + Bzt −c)\n(3.59)\nThese iterations constitute the “alternative direction method of multipliers,” or ADMM\n(it is also sometimes called Douglas-Rachford splitting). It is not equivalent to the\naugmented Lagrangian algorithm (one would need to iterate a large number of times\nover the first two steps before applying the third one for this), but still satisfies good\nconvergence properties. The reader can refer to Boyd et al. [40] for a relatively el-\nementary proof that shows that this algorithm converges as soon as, in addition to\nthe hypotheses that were already made, the Lagrangian\nL(x,z,λ) = G(x) + H(z) + λT (Ax + Bz −c)\nhas a saddle point: there exists x∗,z∗,y∗such that\nmax\ny\nL(x∗,z∗,λ) = L(x∗,z∗,λ∗) = min\nx,z L(x,z,λ∗).\n3.7\nConvex separation theorems and additional proofs\nWe conclude this chapter by completing some of the proofs left aside when discus-\nsion convex functions. These proofs use convex separation theorems, stated below\n(without proof).\nTheorem 3.63 (c.f., Rockafellar [167]) Let Ω1 and Ω2 be two nonempty convex sets\nwith relint(Ω1) ∩relint(Ω2) = ∅. Then there exists b ∈Rd and β ∈R such that b , 0,\nbT x ≤β for all x ∈Ω1 and bT x ≥β for all x ∈Ω2, with a strict inequality for at least one\nx ∈Ω1 ∪Ω2.\nTheorem 3.64 Let Ω1 and Ω2 be two nonempty convex sets with Ω1 ∩Ω2 = ∅and Ω1\ncompact. Then there exists b ∈Rn, β ∈R and ϵ < 0 such that bT x ≤β −ϵ for all x ∈Ω1\nand bT x ≥β + ϵ for all x ∈Ω2.\n3.7.1\nProof of proposition 3.44\nWe start with a few general remarks. If x ∈Rd, the set {x} is convex and relint({x}) =\n{x}. If Ωis any convex set such that x < relint(Ω), then theorem 3.63 implies that\nthere exist b ∈Rd and β ∈R such that bT y ≥β ≥bT x for all y ∈Ω(with bT y > bT x for\n\n3.7. CONVEX SEPARATION THEOREMS AND ADDITIONAL PROOFS\n93\nat least one y). If x is in Ω\\ (relint(Ω)) (so that x is a point on the relative boundary\nof Ω), then, necessarily bT x = β and we can write\nbT y ≥bT x\nfor all y ∈Ωwith a strict inequality for some y ∈Ω. One says that b and β provide a\nsupporting hyperplane for Ωat x.\nNow, if F is a convex function, with\nepi(F) = {(y,a) ∈Rd × R : F(y) ≤a}\nthen\nrelint(epi(F)) = {(y,a) ∈ridom(F) × R : F(y) < a}\n(this simple fact is proved in lemma 3.65 below). In particular, if x ∈dom(F), then\n(x,F(x)) must be in the relative boundary of epi(F). This implies that there exists\n(b,b0) , (0,0) ∈Rd × R such that, for all (y,a) ∈epi(F):\nbT y + b0a ≥bT x + b0F(x).\nIf one assumes that x ∈ridom(F), then, necessarily, b0 , 0. To show this, assume\notherwise, so that bT y ≥bT x for all y ∈dom(F), with b , 0. We get a contradiction\nusing the fact that, for some ϵ > 0, [y,x−ϵ(y−x)] belongs to dom(Ω), because bT (y−x)\ncannot have a constant sign on this segment.\nSo b0 , 0 and necessarily b0 > 0 to ensure that bT y + b0a is bounded from below\nfor all a ≥F(y). Without loss of generality, we can assume b0 = 1 and we get, for all\ny ∈dom(F)\nF(y) + bT y ≥F(x) + bT x\nwhich shows that −b ∈∂F(x), justifying the fact that ∂F(x) , ∅for x ∈ridom(F).\nWe now state and prove the result announced above on the relative interior of\nthe epigraph of a convex function.\nLemma 3.65 Let F be a convex function with epigraph\nepi(F) = {(y,a) : y ∈dom(F),F(y) ≤a}.\nThen\nrelint(epi(G)) = {(y,a) : y ∈ridom(F),F(y) < a}.\nProof Let Γ = {(y,a) : y ∈ridom(F),F(y) < a}. Assume that (y,a) ∈relint(epi(F)).\nThen (y,b) ∈epi(F) for all b > a and there exists ϵ > 0 such that (y,a) −ϵ((y,b) −\n(y,a)) ∈epi(F) which requires that F(y) ≤a −ϵ(b −1) < a. Now, take x ∈dom(F).\n\n94\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nThen, (x,F(x)) ∈epi(dom(F)) and (y,a) −ϵ((x,F(x)) −(y,a)) ∈epi(F) for small enough\nϵ, showing that F(y −ϵ(x −y)) ≤(1+ϵ)a−ϵF(x) and y −ϵ(x −y) ∈dom(F). This proves\nthat y ∈ridom(F) and the fact that relint(epi(F)) ⊂Γ.\nTake (y,a) ∈Γ, and (x,b) ∈epi(F). We need to show that (y −ϵ(x −y),a −ϵ(b −a)) ∈\nepi(F) for small enough ϵ, i.e., that\nF(y −ϵ(x −y)) ≤a −ϵ(b −a)\nfor small enough ϵ. But this is an immediate consequence of the facts that F is\ncontinuous at y ∈ridom(G) and F(y) < a.\n■\n3.7.2\nProof of theorem 3.45\nAssume that there exists ¯x ∈ridom(F1)∩ridom(F2). Take x ∈dom(F1)∩dom(F2) and\ng ∈∂(F1 + F2)(x). We want to show that g = g1 + g2 with g1 ∈∂F1(x) and g2 ∈∂F2(x).\nBy definition, we have\nF1(y) + F2(y) ≥F1(x) + F2(x) + gT (y −x)\nfor all y. We want to decompose g as g = g1 + g2 with g1 ∈∂F1(x) and g2 ∈∂F2(x).\nEquivalently, we want to find g2 ∈Rd such that, for all y ∈Rd,\nF1(y) ≥F1(x) + (g −g2)T (y −x)\nF2(y) ≥F2(x) + gT\n2 (y −x)\nFirst note that we can replace F1 by y 7→F1(y) −F1(x) −gT (y −x) and F2 by y 7→\nF2(y) −F2(x) and assume with loss of generality that F1(x) = F2(x) = 0 and g = 0.\nMaking this assumption, we need to find g2 such that\nF1(y) ≥−gT\n2 (y −x)\nF2(y) ≥gT\n2 (y −x)\nfor all y ∈Rd and some g2 ∈Rd, under the assumption that F1(y) + F2(y) ≥0 for all\ny. Introduce the two convex sets in Rd × R\nΩ1 = epi(F1) = {(y,a) ∈Rd × R : F1(y) ≤a}\nΩ2 = {(y,a) ∈Rd × R : F2(y) ≤−a}.\nThe set Ω2 is the image of epi(F2) by the transformation (y,a) 7→(y,−a). We have\nrelint(Ω1) = epi(F1) = {(y,a) ∈ridom(F1) × R : F1(y) < a}\nrelint(Ω2) = {(y,a) ∈ridom(F2) × R : F2(y) < −a}.\n\n3.7. CONVEX SEPARATION THEOREMS AND ADDITIONAL PROOFS\n95\nSince F1 + F2 ≥0, Ω1 and Ω2 have non-intersecting relative interiors. We can apply\nthe first separation theorem, providing ¯b = (b,b0) ∈Rd × R and β ∈R such that\n¯b , (0,0), bT y + b0a −β ≤0 for (y,a) ∈Ω1 and bT y + b0a −β ≥0 for (y,b) ∈Ω2, with a\nstrict inequality for at least one point in Ω1 ∪Ω2. We therefore obtain the fact that,\nfor all y and a,\nF1(y) ≤a ⇒bT y + b0a −β ≤0\nF2(y) ≤−a ⇒bT y + b0a −β ≥0.\nWe claim that b0 , 0. Indeed, if b0 = 0, the statement for F1 would imply that bT y −\nβ ≤0 for all y ∈dom(F1) and the one on F2 that bT y −β ≥0 for y ∈dom(F2). The\npoint ¯x ∈relint(Ω1) ∩relint(Ω2) should then satisfy bT ¯x −β = 0. We know that there\nexists a point y ∈Ω1 ∪Ω2 such that bT y , β. Assume that y ∈Ω1, so that bT y −β < 0\nand take ϵ > 0 such that ˜y = ¯x −ϵ(y −¯x) ∈Ω1. Then\nbT ˜y −β = −ϵ(bT y −β) < 0,\nwhich is a contradiction. A similar contradiction is obtained when y belongs to Ω2,\nyielding the fact that b0 cannot vanish.\nMoreover, we clearly need b0 < 0 to ensure that bT y + b0a −β ≤0 for all large\nenough a if y ∈dom(Ω1). There is then no loss of generality in assuming b0 = −1 and\nwe get\nF1(y) ≤a ⇒bT y −β ≤a\nF2(y) ≤−a ⇒bT y −β ≥a,\nwhich is equivalent to\n−F2(y) ≤bT y −β ≤F1(y)\nTaking y = x gives β = bT x and we get the desired inequality with g2 = −b.\n3.7.3\nProof of theorem 3.46\nLet ¯x ∈Rm such that A ¯x ∈ridom(F). We need to prove that ∂G(x) ⊂AT ∂F(Ax + b)\nwhen G(x) = F(Ax + b). We assume in the following that b = 0, since the theorem\nwith G(x) = F(x + b) is obvious. If g ∈∂G(x), we have\nF(Ay) ≥F(Ax) + gT (y −x)\nfor all y ∈Rm. We want to show that there exists h ∈Rd such that g = AT h and, for\nall z ∈Rd,\nF(z) ≥F(Ax) + hT (z −Ax) = F(Ax) + hT z −gT x.\n\n96\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nLet Ω1 = epi(F) = {(z,a) :,z ∈Rd,F(z) ≤a} and\nΩ2 = {(Ay,a) : y ∈Rm,a = gT (y −x) + G(x)} ⊂Rd × R.\nNote that Ω2 is an affine space with relint(Ω2) = Ω2. If (z,a) ∈relint(Ω1) ∩Ω2, then\nz = Ay for some y ∈Rm and gT (y −x) + G(x) > F(z) = G(y). This contradicts the fact\nthat g ∈∂G(x) and shows that relint(Ω1) ∩Ω2 = ∅. As a consequence, there exist\n(b,b0) , (0,0) and β such that\nF(z) ≤a ⇒bT z + b0a ≤β\nz = Ay,a = gT (y −x) + G(x) ⇒bT z + b0a ≥β\nAssume, to get a contradiction, that b0 = 0 (so that b , 0). Then bT Ay ≥β for all y,\nwhich is only possible if b is perpendicular to the range of A and β ≤0. On the other\nhand, F(A ¯x) < ∞implies that 0 = bT A ¯x + b0F(A ¯x) ≤β, so that β = 0. Furthermore,\nwe know that one of the inequalities above has to be strict for at least one element\nof Ω1 ∪Ω2, but this cannot be true on Ω2, so there exists z ∈dom(F) such that\nbT z < 0. Since bT A ¯x = 0 and A ¯x ∈ridom(F), we have A ¯x −ϵ(z −A ¯x) ∈dom(F), so that\nbT (−ϵz) ≤0, yielding a contradiction.\nSo, we need b0 , 0, and the first pair of inequalities clearly requires b0 < 0, so that\nwe can take b0 = −1. This shows that\nbT z −β ≤F(z)\nfor all z and\nbT Ay −β ≥gT (y −x) + F(Ax)\nfor all y. Taking y = x, z = Ax, we find that β = bT Ax −F(Ax) yielding\nF(z) −F(Ax) ≥bT (z −x)\nfor all z and bT A(y −x) ≥gT (y −x) for all y. This last inequality implies that g = AT b\nand the first one that b ∈∂F(Ax), therefore concluding the proof.\n\nChapter 4\nIntroduction: Bias, Variance and Density Esti-\nmation\nIn this chapter, we illustrate the bias variance dilemma in the context of density es-\ntimation, in which problems are similar to those encountered in classical parametric\nor non-parametric statistics [159, 60, 154].\nFor density estimation, one assumes that a random variable X is given with un-\nknown p.d.f. f and we want to build an estimator, i.e., a mapping (x,T ) 7→ˆf (x;T)\nthat provides an estimation of f (x) based on a training set T = (x1,...,xN) containing\nN i.i.d. realizations of X (i.e., T is a realization of T = (X1,...,XN), N independent\ncopies of X). Alternatively, we will say that the mapping T 7→ˆf (·;T) is an estimator\nof the full density f . Note that, to further illustrate our notation, ˆf (x;T ) is a number\nwhile ˆf (x;T ) is a random variable.\n4.1\nParameter estimation and sieves\nParameter estimation is the most common density estimation method, in which one\nrestrict ˆf to belong to a finite-dimensional parametric class, denoted (fθ,θ ∈Θ), with\nΘ ⊂Rp. For example, fθ can be a family of Gaussian distributions on Rd. With our\nnotation, a parametric model provides estimators taking the form\nˆf (x;T ) = f ˆθ(T )(x)\nand the problem becomes the estimation of the parameter ˆθ.\nThere are several, well-known methods for parameter estimation, and, since this\nis not the focus of the book, we only consider the most common one, maximum\n97\n\n98\nCHAPTER 4. INTRODUCTION: BIAS AND VARIANCE\nlikelihood, which consists in computing ˆθ that maximizes the log-likelihood\nC(θ) = 1\nN\nN\nX\nk=1\nlogfθ(xk).\n(4.1)\nThe resulting ˆθ (when it exists) is called the maximum likelihood estimator of θ, or\nm.l.e.\nIf the true f belongs to the parametric class, so that f = fθ∗for some θ∗∈Θ, stan-\ndard results in mathematical statistics [29, 118] provide sufficient conditions for ˆθ\nto converge to θ∗when N tends to infinity. However, the fact that the true p.d.f. be-\nlongs to the finite dimensional class (fθ) is an optimistic assumption that is generally\nfalse. In this regard, the standard theorems in parametric statistics may be regarded\nas analyzing a “best case scenario,” or as performing a “sanity check,” in which one\nasks whether, in the ideal situation in which f actually belongs to the parametric\nclass, the designed estimator has a proper behavior. In non-parametric statistics, a\nparametric model can still be a plausible approach in order to approximate the true\nf , but the relevant question should then be whether ˆf provides (asymptotically), the\nbest approximation to f among all fθ, θ ∈Θ. The maximum likelihood estimator can\nbe analyzed from this viewpoint, if one measures the difference between two density\nfunctions by the Kullback-Liebler divergence (also called differential entropy):\nKL(f ∥fθ) =\nZ\nRd log f (x)\nfθ(x)f (x)dx\n(4.2)\nwhich is positive unless f = fθ (and may be equal to +∞).\nThis expression of the divergence is a simplification of its general measure-theo-\nretic definition, that we now provide for completeness—and future use. Let µ and\nν be two probability measures on a set eΩ. One says that µ is absolutely continuous\nwith respect to ν, with notation µ ≪ν, if, for every (measurable) subset A ⊂eΩ,\nν(A) = 0 implies µ(A) = 0. The Radon-Nikodym theorem [31] states that µ ≪ν is\nand only if there exists a non-negative function g defined on eΩsuch that\nµ(A) =\nZ\nA\ng(x)dν(x).\nIn terms of random variables, this says that, if X : Ω→eΩand Y : Ω→eΩare two ran-\ndom variables with respective distributions µ and ν, and ϕ : eΩ→R is measurable,\nthen E(ϕ(X)) = E(g(Y)ϕ(Y)). The function g is called the Radon-Nikodym derivative\nof µ with respect to ν and is denoted dµ/dν (it is defined up to a modification on a\nset of ν-probability zero). The general definition of the Kullback-Liebler divergence\n\n4.1. PARAMETER ESTIMATION AND SIEVES\n99\nbetween µ and ν is then:\nKL(µ∥ν) =\n\nZ\n˜Ω\n \nlog dµ\ndν\n! dµ\ndν dν\nif µ ≪ν\n+ ∞\notherwise\n(4.3)\nIn the case when µ = f dx and ν = ˜f dx are both probability measures on Rd with\nrespective p.d.f.’s f and ˜f , µ ≪ν means that f / ˜f is well defined everywhere except\non a set of ν-probability zero. It is then equal to dµ/dν. If µ ≪ν, we can therefore\nwrite\nKL(µ∥ν) =\nZ\nRd\nf (x)\n˜f (x)\nlog\n f (x)\n˜f (x)\n!\n˜f (x)dx =\nZ\nRd log\n f (x)\n˜f (x)\n!\nf (x)dx\nand we will make the abuse of notation of writing KL(f ∥˜f ) for KL(f dx∥˜f dx), which\ngives the expression provided in (4.2).\nThe general definition also gives a simple expression when eΩis a finite set, with\nKL(µ∥ν) =\nX\nx∈eΩ\nlog µ(x)\nν(x)µ(x),\nthat we will use later in these notes (if there exists x such that µ(x) > 0 and ν(x) =\n0, then KL(µ∥ν) = ∞). The most important property for us is that the Kullback-\nLiebler divergence can be used as a measure of discrepancy between two probability\ndistribution, based on the following proposition.\nProposition 4.1 Let µ and ν be two probability measures on eΩ. Then KL(µ∥ν) ≥0 and\nvanishes if and only if µ = ν.\nProof Assume that µ ≪ν since the statement is obvious otherwise and let g =\ndµ/dν. We have\nR\neΩgdν = 1 (since, by definition, it is equal to µ(eΩ)) so that\nKL(µ∥ν) =\nZ\neΩ\n(g logg + 1 −g)dν.\nWe have t logt + 1 −t ≥0 with equality if and only t = 1 (the proof being left to the\nreader) so that KL(µ∥ν) = 0 if and only if g = 1 with ν-probability one, i.e., if and\nonly if µ = ν.\n■\nMinimizing KL(f ∥fθ) with respect to θ is equivalent to maximizing\nEf (logfθ) =\nZ\nRd logfθ(x)f (x)dx,\n\n100\nCHAPTER 4. INTRODUCTION: BIAS AND VARIANCE\nand an empirical evaluation of this expectation is 1\nN\nPN\nk=1 logfθ(xk), which provides\nthe maximum likelihood method. Seen in this context, consistency of the maximum\nlikelihood estimator states that this estimator almost surely converges to a best ap-\nproximator of the true f in the class (fθ,θ ∈Θ). More precisely, if one assumes that\nthe function θ 7→logfθ(x) is continuous1 in θ for almost all x and that, for all θ ∈Θ,\nthere exists a small enough δ > 0 such that\nZ\nRd\n\u0012\nsup\n|θ′−θ|<δ\nlogfθ′(x)\n\u0013\nf (x)dx < ∞\nthen, letting Θ∗denote the set of maximizers of Ef (logfθ), and assuming that it is\nnot empty, the maximum likelihood estimator ˆθN is such that, for all ϵ > 0 and all\ncompact subsets K ⊂Θ,\nlim\nN→∞P\n\u0010\nd( ˆθN,Θ∗) > ϵ and ˆθN ∈K\n\u0011\n→0\nwhere d( ˆθN,Θ∗) is the Euclidean distance between ˆθN and the set Θ∗. The interested\nreader can refer to Van der Vaart [194], Theorem 5.14, for a proof of this statement.\nNote that this assertion does not exclude the situation in which ˆθN goes to infinity\n(i.e., steps out of ever compact subset K in Θ), and the boundedness of the m.l.e. is\neither asserted from additional properties of the likelihood, or by simply restricting\nΘ to be a compact set.\nIf Θ∗= {θ∗} and the m.l.e. almost surely converges to θ∗, the speed of conver-\ngence can also be quantified by a central limit theorem (see Van der Vaart [194],\nTheorem 5.23) ensuring that, in standard cases\n√\nN( ˆθN −θ∗) converges to a normal\ndistribution.\nEven though these results relate our present subject to classical parametric statis-\ntics, they are not sufficient for our purpose, because, when f , fθ∗, the convergence\nof the m.l.e. to the best approximator in Θ still leaves a gap in the estimation of f .\nThis gap is often called the bias of the class (fθ,θ ∈Θ). One can reduce it by con-\nsidering larger classes (e.g., with more dimensions), but the larger the class, the less\naccurate the estimation of the best approximator becomes for a fixed sample size\n(the estimator has a larger variance). This issue is known as the “bias vs. variance\ndilemma,” and to address it, it is necessary to adjust the class Θ to the sample size\nin order to optimally balance the two types of error (and all non-parametric estima-\ntion methods have at least one mechanism that allows for this). When the “tuning\nparameter” is the dimension of Θ, the overall approach is often referred to as the\nmethod of sieves [83, 80], in which the dimension of Θ is increased as a function of N\nin a suitable way.\n1Upper-semi continuous is sufficient.\n\n4.2. KERNEL DENSITY ESTIMATION\n101\nGaussian mixture models provide one of the most popular choices with the me-\nthod of sieves. Modeling in this setting typically follows some variation of the fol-\nlowing construction. Fix a sequence (mN,N ≥1) and let\nΘN =\n\u001a\nf : f (x) =\nmN\nX\nj=1\nαj\ne−|x−µj|2/2σ2\n(2πσ2)d/2 ,\nµ1,...,µmN ∈Rd,α1 + ··· + αmN = 1,α1,...,αmN ∈[0,+∞),σ > 0\n\u001b\n.\n(4.4)\nThere are therefore (d + 1)mN free parameters in ΘN. The integer mN allows one\nto adjust the dimension of ΘN and therefore controls the bias-variance trade-off. If\nmN tends to infinity “slowly enough,” the m.l.e. will converges (almost surely) to\nthe true p.d.f. f [80]. However, determining optimal sequences N →mN remains a\nchallenging and largely unsolved problem.\nIn practice the computation of the m.l.e. in this context uses an algorithm called\nEM, for expectation-maximization. This algorithm will be described later in chap-\nter 16.\n4.2\nKernel density estimation\nKernel density estimators [150, 177, 178] provide alternatives to the method of\nsieves. They also lend themselves to some analytical developments that provide\nelementary illustrations of the bias-variance dilemma.\nDefine a kernel function as a function K : Rd →[0,+∞) such that\nZ\nRd K(x)dx = 1,\nZ\nRd |x|K(x)dx < ∞,\nZ\nRd xK(x)dx = 0.\n(4.5)\nNote that the third equation is satisfied, in particular, when K is an even function,\ni.e., K(−x) = K(x).\nGiven K and a scalar σ > 0, the rescaled kernel is defined by\nKσ(x) = 1\nσd K\n\u0012 x\nσ\n\u0013\n.\nUsing the change of variable y = x/σ (so that dy = dx/σd) one sees that Kσ satisfies\n(4.5) as soon as K does.\nBased on a training set T = (x1,...,xN), the kernel density estimator defines the\nfamily of densities\nˆfσ(x;T) = 1\nN\nN\nX\nk=1\nKσ(x −xk)\n\n102\nCHAPTER 4. INTRODUCTION: BIAS AND VARIANCE\nOne has\nZ\nRd Kσ(x −xk)dx = 1\nso that it is clear that ˆfσ is a p.d.f. In addition,\nZ\nRd xKσ(x −xk)dx =\nZ\nRd(y + xk)Kσ(y)dy = xk\nso that\nZ\nRd x ˆfσ(x;T)dx = ¯x\nwhere ¯x = (x1 + ··· + xN)/N.\nA typical choice for K is a Gaussian kernel, K(y) = e−|y|2/2/(2π)d/2. In this case, the\nestimated density is a sum of bumps centered at the data points x1,...,xN. The width\nof the bumps is controlled by the parameter σ. A small σ implies less rigidity in the\nmodel, which will therefore be more affected by changes in the data: the estimated\ndensity will have a larger variance. The converse is true for large σ, at the cost of\nbeing less able to adapt to variations in the true density: the model has a larger bias\n(see fig. 4.1 and fig. 4.2).\nAs we now show, in order to get a consistent estimator, one needs to let σ = σN\ndepend on the size of the training set. We have, taking expectations with respect to\ntraining data,\nE( ˆfσ(x;T ))\n=\n1\nNσd\nN\nX\nk=1\nE\n\u0010\nK((x −Xk)/σ)\n\u0011\n=\n1\nσd\nZ\nRd K((x −y)/σ)f (y)dy\n=\nZ\nRd K(z)f (x −σz)dz\nThe bias of the estimator, i.e., the average difference between ˆfσ(x;T ) and f (x) is\ntherefore given by\nE( ˆfσ(x;T )) −f (x) =\nZ\nRd K(z)(f (x −σz) −f (x))dz.\nInterestingly, this bias does not depend on N, but only on σ, and it is clear that,\nunder mild continuity assumptions on f , it will go to zero with σ.\nThe variance of ˆfσ(x;T ) is given by\nvar( ˆfσ(x;T )) =\n1\nNσ2d var(K((x −X)/σ))\n\n4.2. KERNEL DENSITY ESTIMATION\n103\nσ = 0.1\nσ = 0.25\nσ = 0.5\nσ = 1.0\nFigure 4.1: Kernel density estimators using a Gaussian kernel and various values of σ when\nthe true distribution of the data is a standard Gaussian (Orange: true density; Blue: esti-\nmated density, Red dots: training data).\n\n104\nCHAPTER 4. INTRODUCTION: BIAS AND VARIANCE\nσ = 0.1\nσ = 0.25\nσ = 0.5\nσ = 1.0\nFigure 4.2: Kernel density estimators using a Gaussian kernel and various values of σ when\nthe true distribution of the data is a Gamma distribution with parameter 2 (Orange: true\ndensity; Blue: estimated density, Red dots: training data).\n\n4.2. KERNEL DENSITY ESTIMATION\n105\nwith\n1\nNσ2d var(K((x −X)/σ))\n=\n1\nNσ2d\nZ d\nR\nK((x −y)/σ)2f (y)dy\n−\n1\nNσ2d\n Z\nRd K((x −y)/σ)f (y)dy\n!2\n=\n1\nNσd\nZ\nRd K(z)2f (x −σz)dz −1\nN\n Z\nRd K(z)f (x −σz)dz\n!2\nThe total mean-square error of the estimator is\nE(( ˆfσ(x) −f (x))2) = var( ˆfσ(x)) + (E( ˆfσ(x)) −f (x))2.\nClearly, this error cannot go to zero unless we allow σ = σN to depend on N. For the\nbias term to go to zero, we know that we need σN →0, in which case we can expect\nthe second term in the variance to decrease like 1/N, while, for the first term to go to\nzero, we need Nσd\nN to go to infinity. This illustrates the bias-variance dilemma: σN\nmust go to zero in order to cancel the bias, but not too fast in order to also cancel the\nvariance. There is, for each N, an optimal value of σ that minimizes the error, and\nwe now proceed to a more detailed analysis and make this statement a little more\nprecise.\nLet us make a Taylor expansion of both bias and variance, assuming that f has at\nleast three bounded derivatives and that\nR\nRd |x|3K(x)dx < ∞. We can write\nf (x −σz) = f (x) −σzT ∇f (x) + σ2\n2 zT ∇2f (x)z + O(σ3|z|3),\nwhere ∇2f (x) denotes the matrix of second derivatives of f at x. Since\nR\nzK(z)dz = 0,\nthis gives\nE( ˆfσ(x;T )) −f (x) = σ2\n2 Mf (x) + o(σ2)\nwith Mf =\nR\nK(z)zT ∇2f (x)zdz. Similarly, letting S =\nR\nK2(z)dz,\nvar( ˆfσ(x)) =\n1\nNσd\n\u0010\nSf (x) + o(σd + σ2)\n\u0011\n.\nAssuming that f (x) > 0, we can obtain an asymptotically optimal value for σ by\nminimizing the leading terms of the mean square error, namely\nσ4\n4 M2\nf +\nS\nNσd f (x)\nwhich yields σN = O(N −1/(d+4)) and\nE(( ˆfσN(x;T ) −f (x))2) = O(N −4/(d+4)).\n\n106\nCHAPTER 4. INTRODUCTION: BIAS AND VARIANCE\nIf f has r + 1 derivatives, and K has r −1 vanishing moments (this excludes the\nGaussian kernel) one can reduce this error to N −2r\n2r+d . These rates can be shown to\nbe “optimal,” in the “min-max” sense, which roughly expresses the fact that, for any\nother estimator, there exists a function f for which the convergence speed is at least\nas “bad” as the one obtained for kernel density estimation.\nThis result says that, in order to obtain a given accuracy ϵ in the worst case sce-\nnario, N should be chosen of order (1/ϵ)1+(d/2r) which grows exponentially fast with\nthe dimension. This is the curse of dimensionality which essentially states that the\nissue of density estimation may be intractable in large dimensions. The same state-\nment is true also for most other types of machine learning problems. Since machine\nlearning essentially deals with high-dimensional data, this issue can be problematic.\nObviously, because the min-max theory is a worst-case analysis, not all situations\nwill be intractable for a given estimator, and some cases that are challenging for one\nof them may be quite simple for others: even though all estimators are “cursed,” the\nway each of them is cursed differs. Moreover, while many estimators are optimal\nin the min-max sense, this theory does not give any information on “how often” an\nestimator performs better than its worst case, or how it will perform on a given class\nof problems. (For kernel density estimation, however, what we found was almost\nuniversal with respect to the unknown density f , which indicates that this estimator\nis not a good choice in large dimensions.)\nAnother important point with this curse of dimensionality is that data may very\noften appear to be high dimensional while it has a simple, low-dimensional struc-\nture, maybe because many dimensions are irrelevant to the problem (they contain,\nfor example, just random noise), or because the data is supported by a non-linear\nlow-dimensional space, such as a curve or a surface. This information is, of course,\nnot available to the analysis, but can sometimes be inferred using some of the dimen-\nsion reduction methods that will be discussed later in chapter 20. Sometimes, and\nthis is also important, information on the data structure can be provided by domain\nknowledge, that is, by elements, provided by experts, that specify how the data has\nbeen generated (such as underlying equations) and reasonable hypotheses that are\nmade in the field. This source of information should never be ignored in practice.\n\nChapter 5\nPrediction: Basic Concepts\n5.1\nGeneral Setting\nThe goal of prediction is to learn, based on training data, an input-output relation-\nship between two random variables X and Y, in the sense of finding, for a specified\ncriterion, the best function of the input X that predicts the output Y. (In statistics, Y\nis often called the dependent variable, and X the independent variable.) We will, as\nalways, assume that all the variables mentioned in this chapter are defined on a fixed\nprobability space (Ω,P). We assume that X : Ω→RX, where RX is the input space,\nand Y : Ω→RY, where RY is the output space. The input-output relationship is\ntherefore captured by an unknown function f : RX →RY, the predictor.\nThe following two subclasses of prediction problems are important enough to\nhave learned their own names and specific literature.\n• Quantitative output: RY = Rq (often with q = 1). One then speaks of a regres-\nsion problem.\n• Categorical output: RY = {g1,...,gq} is a finite set. One then speaks of a classi-\nfication problem.\nIn most cases, the input space is Euclidean, i.e., RX = Rd. Note also that, in clas-\nsification, instead of a function f : R →RY, one sometimes estimates a function\nf : RX →Π(RY), where Π(RY) is the space of probability distributions on RY. We\nwill return to this in remark 5.4.\nThe quality of a prediction is assessed through the definition of a risk function.\nSuch a function, denoted r, is defined on RY ×RY, takes values in [0,+∞) and should\nbe understood as\nr(True output,Predicted output),\n(5.1)\n107\n\n108\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nso that r(y,y′) assigns a cost to the situation in which a true y is predicted by y′.\nNote that this definition is asymmetric, and there is no requirement that r(y,y′) =\nr(y′,y). It is important to remember our convention that the first variable is the true\nobservation and the second one is a place-holder for a prediction. Risk functions\nare also called loss functions, or simply cost functions and we will use these terms as\nsynonyms.\nThe goal in prediction is to minimize the expected risk, also called the generaliza-\ntion error:\nR(f ) = E(r(Y,f (X))).\nWe will prove that an optimal f can be easily described based on the joint distri-\nbution of X and Y (which is, unfortunately, never available). We will need for this\nto use conditional expectations and conditional probabilities and proceed first to a\nreminder of their definitions and properties.\n5.2\nConditional expectation\nIf ξ : Ω→Rξ and η : Ω→Rη ⊂Rd are discrete random variables, then\nP(η = η | ξ = ξ) = P(η = η,ξ = ξ)/P(ξ = ξ)\nif P(ξ = ξ) > 0 and is undefined otherwise. Then, if η is real-valued and discrete,\none defines the conditional expectation of η given ξ, denoted E(η | ξ), by\nE(η | ξ)(ω) =\nX\nη∈Rη\nηP(η = η | ξ = ξ(ω))\nfor all ω such that P(ξ = ξ(ω)) > 0. Note that E(η | ξ) is a random variable, defined\nover Ω. It however only depends on the values of ξ, in the sense that E(η | ξ)(ω) =\nE(η | ξ)(ω′) if ξ(ω) = ξ(ω′). We will use the notation\nE(η | ξ = ξ) =\nX\nη∈Rη\nηP(η = η | ξ = ξ),\nwhich is now a function defined on Rξ. One has E(η | ξ)(ω) = E(η | ξ = ξ(ω)).\nOne can characterize E(η | ξ) by the properties\n\nE(η | ξ) is a function of ξ\n∀f : Rη →R,E(E(η | ξ)f (ξ)) = E(ηf (ξ)).\n(5.2)\nThe proof that our definition of E(η | ξ) for discrete random variables is the only\none satisfying these properties is left to the reader. The interest of reformulating the\n\n5.2. CONDITIONAL EXPECTATION\n109\ndefinition of the conditional expectation via (5.2) is that this provides a definition\nthat works for general random variables (with the additional assumption that f is\nmeasurable), not only for discrete ones. We assume below that (Rξ,Sξ) and (Rη,Sη)\nare measurable spaces.\nDefinition 5.1 Assume that Rη = Rd. Let ξ : Ω→Rξ and η : Ω→Rη be two random\nvariables with E(|η|) < ∞. The conditional expectation of η given ξ is a random variable\nζ : Ω→Rη such that\n(i) There exists a function h : Rξ →R such that ζ = h ◦ξ almost surely.\n(ii) For any measurable function g : Rξ →[0,+∞), one has\nE(ηg ◦ξ) = E(ζg ◦ξ).\nThe variable ζ is then denoted E(η|ξ) and the function h in (i) is denoted E(η|ξ = ·).\nImportantly, functions ζ satisfying conditions (i) and (ii) always exists and are\nalmost surely unique, in the sense that if another function ζ′ satisfies these condi-\ntions, then ζ = ζ′ with probability one. One obtains an equivalent definition if one\nrestricts functions g in (ii) to indicators of measurable sets, yielding the condition\nthat, if A ⊂Rξ is measurable,\nE(η1ξ∈B) = E(ζ1ξ∈B).\nTaking g(ξ) = 1 for all ξ ∈Rξ in condition (ii), one gets the well-known identity\nE(E(η|ξ)) = E(η).\nMoreover, for any function g defined on Rξ we have E(ηg ◦ξ|ξ) = (g ◦ξ)E(η|ξ),\nwhich can be checked by proving that the right-hand side satisfies the conditions (i)\nand (ii).\nConditional expectations share many of the properties of simple expectations.\nFor example, if η ≤η′, both taking scalar values, then E(η | ξ) ≤E(η′ | ξ) almost\nsurely. Jensen’s inequality also holds: if γ : Rd →R is convex and γ ◦η is integrable,\nthen\nγ ◦E(η | ξ) ≤E(γ ◦η | ξ).\nWe will discuss convex functions in chapter 3, but two important examples for this\nsection are γ(η) = |η| and γ(η) = |η|2. The first one implies that |E(η | ξ)| ≤E(|η| |\nbf xi) and, taking expectations on both sides: E(|E(η | ξ)|) ≤E(|η|), the upper bound\nbeing finite by assumption. For the square norm, we find that, if η is square inte-\ngrable, then so is E(η | ξ) and\nE(|E(η | ξ)|2) ≤E(|η|2).\n\n110\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nIf η is square integrable, then this inequality shows that E(η | ξ) minimizes\nE[|η −ζ|2] among all square integrable functions ζ : Ω→Rη that satisfy (i). In\nother terms, the conditional expectation is the optimal least-square approximation\nof η by a function of ξ. To see this, just write\nE[|η −ζ|2 | ξ] = E[|η|2 | ξ] −2E[ηT ζ | ξ] + |ζ|2\n= E[|η|2 | ξ] −2E(η | ξ)T ζ + |ζ|2\n= E[|η|2 | ξ] −|E(η | ξ)|2 + |E(η | ξ) −ζ|2\n= E[|η −E(η | ξ)|2 | ξ] + |E(η | ξ) −ζ|2\n≥E[|η −E(η | ξ)|2 | ξ]\nand taking expectations on both sides yields the desired result.\nIf A is a measurable subset of Rη, the conditional expectation E(1A | ξ) (resp.\nE(1A | ξ = ξ)) is denoted P(η ∈A | ξ) (resp. P(η ∈A | ξ = ξ)), or Pη(A | ξ) (resp.\nPη(A | ξ = ξ)). While these functions are defined separately up to modifications on\nsets of probability zero. Under general assumptions on the set Rη and its σ-algebra\n(always satisfied in our discussions), these conditional probabilities can be defined\ntogether so that, for all ω ∈ΩA 7→Pη(A | ξ)(ω) is a probability distribution on Rη\nsuch that\nE(η | ξ) =\nZ\nRη\nηP(dη | ξ).\nAssume that the the sets Rξ and Rη are equipped with measures, say µξ and\nµη such that the joint distribution of (ξ,η) is absolutely continuous with respect to\nµξ ⊗µη, so that there exists a function ϕ : Rξ × Rη →R (the p.d.f. of (ξ,η) with\nrespect to µξ ⊗µη) such that\nP(ξ ∈A,η ∈B) =\nZ\nA×B\nϕ(ξ,η)µξ ⊗µη(dx,dη).\nThen Pη(· | ξ) is absolutely continuous with respect to µη, with density given by the\nconditional p.d.f. of η given ξ, namely,\nϕ(· | ξ) : (η,ω) 7→\nϕ(η,ξ(ω))\nR\nRη ϕ(η′,ξ(ω))µη(dη′)\n= ϕ(η | ξ = ξ(ω)).\n(5.3)\nNote that\nP\nω :\nZ\nRη\nϕ(η′,ξ(ω))µη(dη′) = 0\n= 0\n\n5.3. BAYES PREDICTOR\n111\nso that the conditional density can be defined arbitrarily when the numerator van-\nishes1.\nThe most common example is when Rξ and Rη are Euclidean spaces and µξ, µη\nare Lebesgue’s measures, in which case (5.3) is the usual definition of conditional\np.d.f.’s. Note also that, for discrete random variables, (5.3) coincides with the defi-\nnition of conditional probabilities P(η = η | ξ = ξ(ω)) when µx and µη are counting\nmeasures. As a last example, if Rξ = Rd, µξ is Lebesgue’s measure and η is discrete,\nthen\nϕ(η | ξ = ξ(ω)) =\nϕ(η,ξ(ω))\nP\nη′∈Rη ϕ(η′,ξ(ω)).\n5.3\nBayes predictor\nRecall that r : (y,y′) 7→r(y,y′) denotes the risk function and that we want to minimize\nR(f ) = E(r(Y,f (X)) over all possible predictors f .\nDefinition 5.2 A Bayes predictor is a measurable function f : RX →RY such that, for\nall x ∈RX,\nE\n\u0010\nr(Y,f (x)) | X = x\n\u0011\n= min\nn\nE\n\u0010\nr(Y,y′) | X = x\n\u0011\n: y′ ∈RY\no\nThere can be multiple Bayes predictors if the minimum in the proposition is not\nuniquely attained. Note that, if f ∗is a Bayes predictor and ˆf any other predictor, we\nhave, by definition\nE\n\u0010\nr(Y,f ∗(X)) | X\n\u0011\n≤E\n\u0010\nr(Y, ˆf (X)) | X\n\u0011\n.\nPassing to expectations, this implies R(f ∗) ≤R( ˆf ). We therefore have the following\nresult:\nTheorem 5.3 Any Bayes predictor f ∗is optimal, in the sense that it minimizes the gen-\neralization error R.\nExample 1. Regression with mean-square error. When RX = Rd and RY = Rq, the\nmost common risk function is the squared norm r(y,y′) = |y −y′|2. The resulting\ngeneralization error is called the MSE (mean square error) and given by R(f ) = E(|Y −\nf (X)|2). The Bayes predictor is such that f ∗(x) minimizes\nt 7→E(|Y −t|2 | X = x).\n1Letting ϕξ(ξ) =\nR\nRη ϕ(η′,ξ)µη(dη′), which is the marginal p.d.f. of ξ with respect to µξ, we have\nP(ϕξ(ξ) = 0) =\nZ\nRξ\n1ϕξ(ξ)=0ϕξ(ξ)µξ(dξ) = 0.\n\n112\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nLet f ∗(x) = E(Y | X = x) and write\nE(|Y −t|2 | X = x) =E(|Y −f ∗(x)|2 | X = x) + 2E((Y −f ∗(x))T (f ∗(x) −t) | X = x)\n+ |f ∗(x) −t|2\n=E(|Y −f ∗(x)|2 | X = x) + 2E((Y −f ∗(x))T | X = x)(f ∗(x) −t)\n+ |f ∗(x) −t|2\n=E(|Y −f ∗(x)|2 | X = x) + |f ∗(x) −t|2.\nThis proves that E(Y | X = x) is the unique Bayes classifier (up to a modification on\na set of probability 0).\nExample 2. Classification with zero-one loss. Let RX = Rd and RY be a finite set. The\nzero-one loss function is defined by r(y,y′) = 1 if y , y′ and 0 otherwise. From\nthis, it results that the generalization error is the probability of misclassification\nR(f ) = P(Y , f (X)) (also called the misclassification error).\nThe Bayes predictor is such that f ∗(x) minimizes\ng 7→P(Y , g | X = x) = 1 −P(Y = g | X = x).\nIt is therefore given by the so-called posterior mode:\nf ∗(x) = argmaxgP(Y = g | X = x).\nRemark 5.4 As mentioned at the beginning of the chapter, one sometimes replaces a\npointwise prediction of the output by a probabilistic one, so that f (x) is a probability\ndistribution on RY. If A is a (measurable) subset of RY, we will write f (x,A) rather\nthan f (x)(A).\nIn such a case, the loss function, r, is defined on RY × Π(RY), and the expected\nrisk is still defined by E(r(Y,f (X))).\nIt is quite natural to require that π 7→r(y,π) is minimized. For classification\nproblems, where RY is finite, one can choose\nr(y,π) = −logπ(y)\n(5.4)\nThe Bayes estimator is then a minimizer of π 7→−E(logπ(Y) | X = x). The solution is\n(unsurprisingly) f (x,y) = P(Y = y | X = x) since we always have\n−E(logπ(Y) | X = x) = −\nX\ny∈RY\nlogπ(y)f (x,y) ≥−\nX\ny∈RY\nlogf (x,y)f (x,y).\nThe difference between these terms is indeed\nX\ny∈RY\nlog f (x,y)\nπ(y) f (x,y) = KL(f (x,·),π) ≥0.\n\n5.4. EXAMPLES: MODEL-BASED APPROACH\n113\nFor regression problems, with RY = Rq, one can choose\nr(y,π) =\nZ\nRq |z −y|2π(dz)\nwhich is indeed minimum when π is concentrated on y. Here, the Bayes estimator\nminimizes (with respect to π)\nZ\nRq\nZ\nRq |z −y|2π(dz)PY(x,dy) =\nZ\nRq\n Z\nRq |z −y|2PY(x,dy)\n!\nπ(dz)\nwhere PY(x,·) is the conditional distribution of Y given X = x. For any z, one has\nZ\nRq |y −z|2f (x,dy) ≥\nZ\nRq |y −E(Y | X = x)|2f (x,dy)\n♦\nwhich shows that the Bayes estimator is, in this case, the Dirac measure concentrated\nat E(Y | X = x).\n5.4\nExamples: model-based approach\nBayes predictors are never available in practice, because the true distribution of\n(X,Y), or that of Y given X, are unknown. These distributions can only be inferred\nfrom observations, i.e., from a training set: T = (x1,y1,...,xN,yN).\nThis is the approach followed by model-based, or generative methods, namely us-\ning training data to approximate the joint distribution of X and Y with a statistical\nmodel estimated from data before using the Bayes estimator derived from this model\nfor prediction. We now illustrate this approach with a few examples.\n5.4.1\nGaussian models and naive Bayes\nConsider a regression problem with RY = R, and model the joint distribution of\n(X,Y) as a (d + 1)-dimensional Gaussian distribution with mean µ and covariance\nmatrix Σ, which must be estimated from data. Write µ =\n \nm\nµ0\n!\n, with µ0 ∈R, m ∈Rd\nand Σ in the form, for some symmetric matrix S and d-dimensional vector u\nΣ =\n \nS\nu\nuT\nσ2\n00\n!\n.\nThen, letting ∆= σ2\n00 −uT S−1u,\nΣ−1 = 1\n∆\n \n∆S−1 + S−1uuT S−1\n−S−1u\n−uT S−1\n1\n!\n.\n\n114\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nThis shows that the joint p.d.f. of (X,Y) is proportional to\nexp\n\u0012\n−\n1\n2∆\n\u0010\n(y −µ0)2 −2uT S−1(x −m)(y −µ0) + (terms not depending on y)\n\u0011\u0013\n.\nIn particular\nE(Y|X) = µ0 + uT S−1(x −m),\nwhich provides the least-square linear regression predictor. (In this expression, u is\nthe covariance between X and Y and S is the covariance matrix of X.)\nIf one restricts the model to having a diagonal covariance matrix S, then\nE(Y|X) = µ0 +\nd\nX\nj=1\nu(j)\nsjj\n(x(j) −m(j)).\nThis predictor is often called the naive Bayes predictor for regression.\n5.4.2\nKernel regression\nLet RX = Rd and RY = R. Let K1 : Rd →R and K2 : R →R be two kernels, therefore\nsatisfying\nZ\nRd K1(x)dx =\nZ\nR\nK2(x)dx = 1;\nZ\nRd xK1(x)dx =\nZ\nR\nyK2(y)dy = 0.\nLet K(x,y) = K1(x)K2(y) so that\nZ\nRd+1 K(x,y)dydx = 1\nZ\nRd+1 yK(y,x)dydx = 0\nZ\nRd+1 xK(y,x)dydx = 0.\nThe kernel estimator of the joint p.d.f., ϕ, of (X,Y) at scale σ is, in this case:\nˆϕ(x,y) = 1\nN\nN\nX\nk=1\n1\nσd+1K1\n\u0012x −xk\nσ\n\u0013\nK2\n\u0012y −yk\nσ\n\u0013\n.\nBased on ˆϕ, the conditional expectation of Y given X is\nˆf (x) =\n1\nN\nPN\nk=1\n1\nσd+1\nR\nR yK1\n\u0010x−xk\nσ\n\u0011\nK2\n\u0010y−yk\nσ\n\u0011\ndy\n1\nN\nPN\nk=1\n1\nσd+1\nR\nR K1\n\u0010x−xk\nσ\n\u0011\nK2\n\u0010y−yk\nσ\n\u0011\ndy\n.\n\n5.5. EMPIRICAL RISK MINIMIZATION\n115\nUsing the fact that σ−1 R\nR yK2\n\u0010y−yk\nσ\n\u0011\ndy = yk, we can simplify this expression to\nobtain\nˆf (x) =\nPN\nk=1 ykK1\n\u0010x−xk\nσ\n\u0011\nPN\nk=1 K1\n\u0010x−xk\nσ\n\u0011 .\nThis the kernel-density regression estimator [139, 202].\n5.4.3\nA classification example\nLet RY = {0,1} and assume RX = N = {0,1,2,...}. Let p = P(Y = 1) and assume that\nconditionally to Y = g, X follows a Poisson distribution with mean µg. Assume that\nµ0 < µ1.\nThe posterior distribution of Y given X = x is 2\nP(Y = g | X = x) ∝\n((1 −p)µx\n0e−µ0 if g = 0\npµx\n1e−µ1 if g = 1\nA Bayes classifier is then provided by taking f (x) = 1 if\nlogp + xlogµ1 −µ1 ≥log(1 −p) + xlogµ0 −µ0\nthat is:\nxlog µ1\nµ0\n≥log 1 −p\np\n+ µ1 −µ0\nSince we are assuming that µ1 > µ0, we find that f (x) = 1 if 3\nx ≥\n&log((1 −p)/p) + µ1 −µ0\nlog(µ1/µ0)\n'\nand 0 otherwise.\n5.5\nEmpirical risk minimization\n5.5.1\nGeneral principles\nModel-based approaches for prediction are based on the estimation of the joint dis-\ntribution of the input and output variables, which is arguably a harder problem\nthan prediction [196].\nSince the goal is to find f minimizing the expected risk\n2∝is the notation for “proportional to”\n3⌈x⌉is the smallest integer larger than x (ceiling).\n\n116\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nR(f ) = E(r(Y,f (X)), one may prefer a direct approach and consider the minimization\nof an empirical estimate of this risk, based on training data T = (x1,y1,...,xN,yN),\nnamely\nˆR(f ) = 1\nN\nN\nX\nk=1\nr(yk,f (xk)).\nThis strategy is called empirical risk minimization.\nImportantly, ˆR must be minimized over a restricted class, F , of predictors to\navoid overfitting. For example, with RY = R and R = Rd, one can take\nF =\nf : f (x) = β0 +\nd\nX\ni=1\nb(i)x(i) : β0,b(1) ...,b(d) ∈R\n.\nMinimizing the empirical mean-square error\nˆR(f ) = 1\nN\nN\nX\nk=1\n(yk −f (xk))2\nover f ∈F leads to the standard least-square regression estimator.\nAs another example, consider\nF =\n\nf : f (x) =\np\nX\nj=1\nwjψ\n\nβj0 +\nd\nX\ni=1\nβjix(i)\n\n,wj,βji ∈R\n\n.\nwith a fixed function ψ. This corresponds to a two-layer perceptron model.\nAs a last example for now (we will see many others in the rest of this book),\ntaking d = 1, the set\nF =\n(\nf :\nZ\nR\nf ′′(x)2dx < µ\n)\n(with µ > 0) provides an infinite dimensional space of predictors, which leads to\nspline regression.\n5.5.2\nBias and variance\nWe give a further illustration of the bias-variance dilemma in the regression case,\nusing the mean-square error and taking q = 1 to simplify. Denote the Bayes predictor\nby f ∗(x) = E(Y | X = x).\n\n5.6. EVALUATING THE ERROR\n117\nFix a function space F , and let ˆf ∗be the optimal predictor in F , in the sense that\nit minimizes E(|Y −f (X)|2) over f ∈F . Then, letting ˆfN ∈F denote an estimated\npredictor,\nR( ˆfN)\n= E(|Y −ˆfN(X)|2)\n= E(|Y −ˆf ∗(X)|2) + E(| ˆfN(X) −ˆf ∗(X)|2)\n+2E((Y −ˆf ∗(X))( ˆf ∗(X) −ˆfN(X))\nLet us make the assumption that there exists ϵ > 0 such that fλ = ˆf ∗+ λ( ˆfN −ˆf ∗)\nbelongs to F for λ ∈[−ϵ,ϵ]. This happens when F is a linear space, or more generally\nwhen F is convex and ˆf ∗is in its relative interior (see chapter 3). Let ψ : λ 7→\nE(|Y −fλ(X)|2), which is minimal at λ = 0. We have\nψ(λ) =E(|Y −ˆf ∗(X) −λ( ˆfN(X) −ˆf ∗(X))|2)\n=E(|Y −ˆf ∗(X)|2) −2λE((Y −ˆf ∗(X))( ˆfN(X) −ˆf ∗(X))) + λ2E(| ˆfN(X) −ˆf ∗(X))|2)\nand\n0 = ψ′(0) = 2E((Y −ˆf ∗(X))( ˆf ∗(X) −ˆfN(X)))\nWe therefore get the identity\nR( ˆfN) = E(|Y −ˆf ∗(X)|2) + E(| ˆfN(X) −ˆf ∗(X)|2) = “Bias” + “Variance”.\nThe bias can be further decomposed as\nE(|Y −ˆf ∗(X)|2) = E(|Y −f ∗(X)|2) + E(|f ∗(X) −ˆf ∗(X)|2)\nbecause f ∗is the conditional expectation. As a result, we obtain an expression the\ngeneralization error with three contributions, namely,\nR( ˆfN) ≤E(|Y −f ∗(X)|2) + E(|f ∗−ˆf ∗(X)|2) + E(| ˆfN(X) −ˆf ∗(X)|2).\nThe first term is the Bayes error. It is fixed by the joint distribution of X and Y\nand measures how well Y can be approximated by a function of X. The second term\ncompares f ∗to its best approximation in F , and is therefore reduced by taking larger\nmodel spaces. The last term is the error caused by using the data to estimate ˆf ∗. It\nincreases with the size of F . This is illustrated in Figure 5.1.\nRemark 5.5 If the assumption made on ˆf ∗is not valid, one can write\nR( ˆfN) = E(|Y −ˆfN(X)|2) ≤2\n\u0010\nE(|Y −ˆf ∗(X)|2) + E(| ˆfN(X) −ˆf ∗(X)|2)\n\u0011\nand still obtain a control (as an inequality) of the generalization error by a bias-plus-\nvariance sum.\n♦\n\n118\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nˆP\nP∗\nF\nˆf\nˆf ∗\nf ∗\nProbability space\nPredictor space\nFigure 5.1: Sources of errors in statistical Learning: When P∗is the distribution of the data,\nthe optimal predictor f ∗minimizes the expected loss function. Based on data Z1,...,ZN,\nthe sample-based distribution is ˆP = (δZ1 + ··· + δZN )/N and the empirical loss is minimized\nover a subset S of the space of all possible estimators. The expected discrepancy between\nthe resulting estimator and the one minimizing the true expected loss on the subspace is the\n“variance” of the method, and the expected discrepancy between this subspace-constrained\nestimator and and the optimal one is the “bias.”\n\n5.6. EVALUATING THE ERROR\n119\n5.6\nEvaluating the error\n5.6.1\nGeneralization error\nGiven input and output variables X : Ω→RX and Y : Ω→RY and a risk function\nr : RY × RY →[0. + ∞), we have defined the generalization (or prediction) error as\nR(f ) = E(r(Y,f (X))).\nRecall that a training set T = ((x1,y1),...,(xN,yN)) is a realization T = T (ω) of the\nrandom variable T = ((X1,Y1),...,(XN,YN)), an i.i.d. sample of the joint distribution\nof (X,Y). A learning algorithm is a function T 7→ˆfT defined on the set of training\nsets, namely, S∞\nN=1(R × RY)N and taking values in F .\nFor a given T and a specific algorithm, one is primarily interested in evaluating\nR( ˆfT ), the generalization error of the predictor estimated from observed data. To\nemphasize the fact that the training set is fixed in this expression, one often writes:\nR( ˆfT ) = E(r(Y, ˆf T (X))|T = T)\nIf we also take the expectation with respect to T (for fixed N), we obtain the\naveraged generalization risk as\nE(R( ˆf T )) = E(r(Y, ˆf T (X))),\nwhich provides an evaluation of the average quality of the algorithm when evaluated\non random training sets of size N. If A : T 7→ˆfT denotes the learning algorithm, we\nwill denote RN(A) = E(R( ˆf T )).\nSince their computation requires the knowledge of the joint distribution of X and\nY, these errors are not available in practice. Given a training set T and a predictor\nf , one can compute the empirical error\nˆRT (f ) = 1\nN\nN\nX\nk=1\nr(yk,f (xk)).\nUnder the usual moment conditions, the law of large numbers implies that ˆRT (f ) →\nR(f ) with probability one for any given predictor f . However, the law of large num-\nbers cannot be applied to assess whether the in-sample error,\nET\n∆= ˆRT ( ˆfT ) = 1\nN\nN\nX\nk=1\nr(yk, ˆfT (xk)),\n\n120\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nis a good approximation of the generalization error R( ˆfT ). This is because each term\nin the sum depends on the full data set, so that ET is not a sum of independent terms.\nThe in-sample error typically under-estimates the generalization error, sometimes\nwith a large discrepancy.\nWhen one has enough data, however, it is possible to set some of it aside to form\na test set. Formally, a test set is a collection T ′ = (x′\n1,y′\n1,...,x′\nN′,y′\nN′) considered as a\nrealization of an i.i.d. sample of (X,Y), T ′ = (X′\n1,Y ′\n1,...,X′\nN′,Y ′\nN′), independent of T .\nThe test set error is then given by\nET,T ′ = ˆRT ′( ˆfT ) = 1\nN ′\nN′\nX\nk=1\nr(y′\nk, ˆfT (x′\nk)).\nThe law of large numbers (applied conditionally to T = T) implies that ET,T ′ con-\nverges to R( ˆfT ) with probability one when N ′ →∞.\nHowever, in many applications, data acquisition is difficult or expensive (e.g., in\nthe medical field) and sparing a part of it in order to form a test set is not a reasonable\noption. In such situations, cross-validation is generally a preferred alternative.\n5.6.2\nCross validation\nCross-validation error\nThe n-fold cross-validation method (see, e.g., Stone [184]) separates the training set\ninto n non-overlapping sets of equal sizes, and estimates n predictors by leaving out\none of these subsets as a temporary test set. A generalization error is estimated from\neach test set and averaged over the n results.\nLet us formalize this computation after introducing some notation. We represent\ntraining data in the form T = (z1,...,zN), a sample of a random variable Z. With\nthis notation, we can include supervised problems, such as prediction (taking Z =\n(X,Y)) and unsupervised ones such as density estimation (taking Z = X). One tries\nto estimate a function f within a given class (e.g., a predictor, or a density) and\none has a measure of “loss”, denoted ℓ(f ,z) ≥0 measuring how badly f performs\non the data z. For prediction, one takes ℓ(f ,z) = r(y,f (x)) with z = (x,y) and for\ndensity estimation, e.g., ℓ(f ,z) = −logf (z), the negative log-likelihood. One then\nlets R(f ) = E(ℓ(f ,Z)). For an algorithm A : T 7→ˆfT , the loss ¯R(A) is the quantity of\ninterest.\n\n5.6. EVALUATING THE ERROR\n121\nGiven another set T ′ = (z′\n1,...,z′\nN′), the empirical loss is\nˆRT ′(f ) = 1\nN ′\nN′\nX\nk=1\nℓ(f ,z′\nk)\nand, using T as a training set and T ′ as a test set, we let\nET,T ′ = ˆRT ′( ˆfT ).\nTo define an n-fold cross-validation estimator of the error, one assumes that the\ntraining set T is partitioned into n subsets of equal sizes (up to one element if N is\nnot a multiple of n), T1, . . . , Tn, so that Ti and Tj are non-intersecting if i , j, and\nT = Sn\ni=1 Ti. For each i, let T (i) = T \\ Ti, which provides the training data with the\nelements of Ti removed. Then, the n-fold cross-validation error is defined by\nECV(T) = 1\nn\nn\nX\ni=1\nET (i),Ti .\nAssuming, to simplify, that N is a multiple of n, the expectation of the cross-\nvalidation error is E(R( ˆfTN′)), where the average is made over training sets TN′ of\nsize N ′ = N −N/n. Note that the cross-validation error is an estimate of the average\nerror of the algorithm over random training sets, not necessarily that of the current\nestimator ˆfT . It returns an evaluation of the algorithm A : T 7→ˆfT . When needed,\none can emphasize this and write ¯RCV,T (A).\nThe limit case when n = N is called leave-one-out (LOO) cross validation. In this\ncase ECV is an almost unbiased estimator of E(R( ˆfT )), but, because it is an average of\nfunctions of the training set that are quite similar (and that will therefore be posi-\ntively correlated), its variance (as a function of T) may be quite large. Conversely,\nsmaller values of n will have smaller variances, but larger biases. In practice, it is dif-\nficult to assess which choice of n is optimal, although 5- or 10-fold cross-validation\nis quite popular. LOO cross-validation is also often used, especially when N is small.\nModel selection using cross validation\nBecause it evaluates the quality of an algorithm, cross-validation is often used to per-\nform model selection. Indeed, many learning algorithms depends on a parameter,\nthat we will denote λ. In kernel density estimation, for example, λ = σ is the ker-\nnel width. For mixtures of Gaussian, λ = m is the number of Gaussian terms in the\nmixtures. Formally, this means that one has, for every λ, an algorithm Aλ : T 7→ˆfT ,λ.\n\n122\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nFixing a training set T , one can compute, for each λ, the cross-validation error\neT (λ) = ¯RCV,T (Aλ). Model selection is then performed by finding\nλ∗(T) = argmin\nλ\neT (λ).\nOnce this λ∗is obtained, the final estimator is ˆfT,λ∗(T ), obtained by rerunning the\nalgorithm one more time on the full training set.\nThis defines a new training algorithm, A∗: T 7→ˆfT,λ∗(T ). It is a common mistake\nto consider that the cross-validation error associated to this algorithm is still given\nby e(λ∗(T)). This is false, because the computation of λ∗uses the full training set.\nTo compute the cross-validation error of A∗, one needs to encapsulate this model\nselection procedure in an other cross-validation loop. So, one needs to compute,\nusing the previous notation,\nE∗\nCV(T) = 1\nn\nn\nX\ni=1\nˆRTi( ˆfT (i),λ∗(T (i)))\nwhere each ˆfT (i),λ∗(T (i)) is computed by running a cross-validated model selection pro-\ncedure restricted to T (i). This is often called a double-loop cross-validation proce-\ndure (the number of folds in the inner and outer loops do not have to coincide). Note\nthat each λ∗(T (i)) that does not necessarily coincide with the optimal λ∗(T) obtained\nwith the full training set.\n\nChapter 6\nInner Products and Reproducing Kernels\n6.1\nIntroduction\nWe will discuss later in this book various methods that specify the prediction is as a\nlinear function of the input. These methods are often applied after taking transfor-\nmations of the original variables, in the form x 7→h(x) (i.e., the prediction algorithm\nis applied to h(x) instead of x). We will refer to h as a “feature function,” which typi-\ncally maps the initial data x ∈R to a vector space, sometimes of infinite dimensions,\nthat we will denote H (the “feature space”).\nThe present chapter provides a formal description of this framework, focusing,\nin particular, on situations in which H has an inner product, as this inner product\nis often instrumental in the design of linear methods on H. Many machine learning\nmethods can indeed be expressed either as functions of the coordinates of the input\ndata in some space, or as functions of the inner products between the input samples.\nSuch methods can bypass the difficulty of using high-dimensional features with the\nhelp of the theory of “reproducing kernels,” [12, 201] which ensures that the inner\nproduct between special classes of feature functions h(x) and h(x′) can be explicitly\ncomputed as a function of x and x′.\n6.2\nBasic Definitions\n6.2.1\nInner-product spaces\nWe recall that a real vector space 1 is a set, H, on which an addition and a scalar\nproduct are defined, namely (h,h′) ∈H × H 7→h + h′ ∈H and (λ,h) ∈R × H 7→λh ∈\nH, and we assume that the reader is familiar with the theory of finite-dimensional\n1All vector spaces in these notes will be real, and will therefore only be referred as vector spaces.\n123\n\n124\nCHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS\nspaces.\nAn inner product on a vector space H is a bilinear function, typically denoted\n(ξ,η) 7→⟨ξ , η⟩such that ⟨ξ , ξ⟩≥0 with ⟨ξ , ξ⟩= 0 if and only if ξ = 0. A vector\nspace equipped with an inner product is called an inner-product space. We will\noften denote the inner product with a subscript referring to the space (e.g., ⟨· , ·⟩H).\nGiven such a product, the function\nξ 7→∥ξ∥H =\np\n⟨ξ , ξ⟩H\nis a norm, so that H is also a normed space (but not all normed spaces are inner-\nproduct spaces) 2.\nWhen a normed space is complete with respect to the topology induced by its\nnorm, it is called a Banach space, or a Hilbert space when the norm is associated with\nan inner product. Completeness means that Cauchy sequences in this space always\nhave a limit, i.e., if the sequence (ξn) is such that, for any ϵ > 0, there exists n0 > 0\nsuch that ∥ξn −ξm∥H < ϵ for all n,m ≥n0, then there exists ξ such that ∥ξn −ξ∥H →0.\nCompleteness is a very natural property. It allows, for example, for the definition of\nintegrals such as\nR\nh(t)dt as limits of Riemann sums for suitable functions h : R →H,\nleading (with more general notions of integrals) to proper definitions of expectations\nof H-valued random variables. Using a standard (abstract) construction, one can\nprove that any normed space (resp. inner-product) can be extended to a Banach\n(resp. Hilbert) space within which it is dense.\nNote that finite-dimensional normed spaces are always complete.\n6.2.2\nFeature spaces and kernels\nNow, consider an input set, say R, and a mapping h from R to H, where H is an inner\nproduct space. For us, R is the set over which the original input data is observed,\ntypically Rd, and H is the feature space. One can define the function Kh : R×R →R\nby\nKh(x,y) = ⟨h(x) , h(y)⟩H.\nThe function Kh satisfies the following two properties.\n[K1] Kh is symmetric, namely Kh(x,y) = Kh(y,x) for all x and y in R.\n[K2] For any n > 0, for any choice of scalars λ1,...,λn ∈R and any x1,...,xn ∈R, one\nhas\nn\nX\ni,j=1\nλiλjKh(xi,xj) ≥0.\n(6.1)\n2Note that we are using double bars for the norm in H, which, in most applications, is infinite\ndimensional\n\n6.2. BASIC DEFINITIONS\n125\nThe first property is obvious, and the second one results from the fact that one can\nwrite\nn\nX\ni,j=1\nλiλjKh(xi,xj) =\nn\nX\ni,j=1\nλiλj⟨h(xi) , h(xj)⟩H =\n\r\r\r\r\nn\nX\ni=1\nλih(xi)\n\r\r\r\r\n2\nH ≥0.\n(6.2)\nThis leads us to the following definition.\nDefinition 6.1 A function K : R × R 7→R satisfying properties [K1] and [K2] is called\na positive kernel.\nOne says that the kernel is positive definite if the sum in (6.1) cannot vanish unless (i)\nλ1 = ··· = λn = 0 or (ii) xi = xj for some i , j.\nAn equivalent definition of positive kernels can be given using kernel matrices,\nfor which we introduce a notation.\nDefinition 6.2 If K : R × R 7→R is given, we define, for every x1,...,xn ∈R, the kernel\nmatrix KK(x1,...,xn) with entries K(xi,xj), for i,j = 1,...,n. (If K is understood from the\ncontext, we will simply write K(x1,...,xn) instead of KK(x1,...,xn).)\nGiven this notation, it is clear that K is a positive kernel if and only if for all x1,...,xn ∈\nR, the matrix KK(x1,...,xn) is symmetric, positive semidefinite. It is a positive def-\ninite kernel if KK(x1,...,xn) is positive definite as soon as all xj’s are distinct. This\nlatter condition is obviously needed since, if xi = xj, the ith and jth columns of K\ncoincide and this matrix cannot be full-rank.\nRemark 6.3 It is important to point out that K being a positive kernel does not require\nthat K(x,y) ≥0 for all x,y ∈R (see examples in the next section). However, it does\nimply that K(x,x) ≥0 for all x ∈R, since diagonal elements of positive semi-definite\nmatrices are non-negative.\n♦\nThe function Kh defined above is therefore always a positive kernel, but not al-\nways positive definite, as seen below. We will also see later that the converse state-\nment is true: any positive kernel K : R × R 7→R can be expressed as Kh for some\nfeature function h between R and some feature space H.\nGiven a feature function h : R →H, we will denote by Vh = span(h(x),x ∈R) the\nvector space generated by the features, which, by definition, is the space of all linear\ncombinations\nξ =\nn\nX\ni=1\nλih(xi)\n\n126\nCHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS\nwith λ1,...,λm ∈R, x1,...,xn ∈R and n ≥0 (by convention, ξ = 0 if n = 0). Then Kh is\npositive definite if and only if any family (h(x1),...,h(xn)) with distinct xi’s is linearly\nindependent. This is a direct consequence of (6.2).\nn\nX\ni,j=1\nλiλjKh(xi,xj) =\n\r\r\r\r\r\r\r\nn\nX\ni=1\nλih(xi)\n\r\r\r\r\r\r\r\n2\nH\n.\nThis implies in particular that positive-definite kernels over infinite input spaces R\ncan only be associated to infinite-dimensional spaces H, since Vh ⊂H.\n6.3\nFirst examples\n6.3.1\nInner product\nClearly, if R is an inner product space, it has an associated reproducing kernel, de-\nfined by\nK(x,y) = ⟨x , y⟩R .\nThis kernel is equal to Kh with H = R and h = id (the identity mapping). In par-\nticular K(x,y) = xT y is a positive kernel if R = Rd. This kernel can obviously take\npositive and negative values.\nNotice that this kernel is not positive definite, because the rank of K(x1,...,xn) is\nequal to the dimension of span(x1,...,xn), which can be less than n even when the\nxi’s are distinct.\n6.3.2\nPolynomial Kernels\nConsider R = Rd and define\nh(x) = (x(i1) ...x(ik),1 ≤i1,...,ik ≤d),\nwhich contains all products of degree k formed from variables x(1),...,x(d), i.e., all\nmonomials of degree k in x. This function takes its values in the space H = RNk,\nwhere Nk = dk. Using, in H, the inner product ⟨ξ , η⟩H = ξT η, we have\nKh(x,y)\n=\nX\n1≤i1,...,ik≤d\n(x(i1)y(i1))···(x(ik)y(ik))\n=\n(xT y)k.\nThis provides the homogeneous polynomial kernel of order k.\n\n6.3. FIRST EXAMPLES\n127\nIf one now takes all monomials of order less than or equal to k, i.e.,\nh(x) = (x(i1) ...x(il),1 ≤i1,...,il ≤d,0 ≤l ≤k),\nwhich now takes values in a space of dimension 1 + d + ··· + dk, the corresponding\nkernel is\nKh(x,y) = 1 + (xT y) + ··· + (xT y)k = (xT y)k+1 −1\nxT y −1\n.\nThis provides a polynomial kernel of order k. It is important to notice here that, even\nthough the dimension of the feature space increases exponentially in k, so that the\ncomputation of the feature function rapidly becomes intractable, the computation\nof the kernel itself remains a relatively mild operation.\nOne can make variations on this construction. For example, choosing any family\nc0,c1,...,ck of positive numbers, one can take\nh(x) = (clx(i1) ...x(il),1 ≤i1,...,il ≤d,0 ≤l ≤k)\nyielding\nKh(x,y) = c2\n0 + c2\n1(xT y) + ··· + c2\nk(xT y)k.\nTaking cl =\n \nk\nl\n!1/2\nαl for some α > 0, we get another form of polynomial kernel,\nnamely,\nKh(x,y) = (1 + α2xT y)k.\n6.3.3\nFunctional Features\nWe now consider an example in which H is infinite dimensional. Let R = Rd. We as-\nsume that a function s : Rd →R is chosen, such that s is both (absolutely) integrable\nand square integrable. We also fix a scaling parameter ρ > 0. Associate to x ∈Rd the\nfunction\nξx : y 7→s((y −x)/ρ),\nwhich is also square integrable (as a function of y). We define the feature function\nh : x 7→ξx from Rd to H = L2(Rd), the space of square integrable functions on Rd\nwith inner product\n⟨ξ , η⟩H =\nZ\nRd ξ(z)η(z)dz.\nThe resulting kernel is\nKh(x,y) =\nZ\nRd s(z/ρ −x)s(z/ρ −y)dz = ρd\nZ\nRd s(z)s(z −(y −x)/ρ)dz.\n\n128\nCHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS\nNote that Kh(x,y) is “translation-invariant,” which means that it only depends on\nx −y. It takes the form Kh(x,y) = ρdΓ((y −x)/ρ) where\nΓ(u) =\nZ\nRd s(z)s(z −u)dz.\nis the convolution3 of s with ˜s : z 7→s(−z).\nLet σ be the Fourier transform of s, i.e.,\nσ(ω) =\nZ\nRd e−2iπωT us(u)du.\nBecause s is real-valued, we have σ(−ω) = ¯σ(ω), the complex conjugate of σ. More-\nover, ¯σ is also the Fourier transform of ˜s. Using the fact that the Fourier transform of\nthe convolution of two functions is the product of their Fourier transforms, we see\nthat the Fourier transform of Γ = s∗˜s is equal to |σ|2. Applying the inverse transform,\nwe find\nΓ(u) =\nZ\nRd e2iπωT u|σ(ω)|2 dω =\nZ\nRd e−2iπωT u|σ(−ω)|2 dω.\nThis form is (almost) characteristic of translation-invariant kernels.\nLet us consider a few examples of kernels that can be obtained in this way.\n(1) Take d = 1 and let s be the indicator function of the interval [−1\n2, 1\n2]. Then, one\nfinds\nΓ(t) = max(1 −|t|,0).\nIn this case, the space Vh is the space of all functions expressed as finite sums\nz 7→\nn\nX\nj=1\nλj1[xj−ρ/2,xj+ρ/2](z),\nand therefore is a space of compactly-supported piecewise constant functions. Such\na function computed with distinct xj’s cannot vanish everywhere unless all λj’s van-\nish, so that Kh is positive definite. Indeed, let\nf (z) =\nn\nX\nj=1\nλj1[xj−ρ/2,xj+ρ/2](z)\nand assume without loss of generality that x1 < x2 < ··· < xn and let xn+1 = ∞. Let i0\nbe the smallest index j such that λj , 0, assuming that such an index exists. Then\nf (z) = λi0 > 0 for all z ∈[xi0 −ρ/2,xi0+1 −ρ/2) which is a non-empty interval. So, if f\nvanishes almost everywhere, we must have λj = 0 for all j = 1,...,n.\n3The convolution between two absolutely integrable functions f and g is defined by f ∗g(u) =\nR\nRd f (z)g(u −z)dz\n\n6.3. FIRST EXAMPLES\n129\n(2) Still with d = 1, let s(z) = e−|z|. Then, for t > 0,\nΓ(t) =\nZ ∞\n−∞\ne−|z|e−|z−t| dz\n=\nZ 0\n−∞\nezez−t dz +\nZ t\n0\ne−zez−t dz +\nZ ∞\nt\ne−ze−z+t dz\n= e−t\n2 + te−t + e−t\n2\n= (1 + t)e−t\nUsing the fact that Γ(−t) = Γ(t) (make the change of variable z →−z in the integral),\nwe get\nΓ(t) = (1 + |t|)e−|t|.\nfor all t. This shows that\nK(x,y) = (1 + |x −y|)e−|x−y|\nis a positive kernel on Rd.\n(3) Take s(z) = e−|z|2/2, z ∈Rd. Then\nΓ(u) =\nZ\nRd e−|z|2+|u−z|2\n2\ndz = e−|u|2\n4\nZ\nRd e−|z−u/2|2 dz\n= (4π)d/2e−|u|2\n4 .\nThis provides a special case of Gaussian kernel.\n6.3.4\nGeneral construction theorems\nTranslation invariance\nAs introduced above, a kernel K is translation invariant if it takes the form K(x,y) =\nΓ(x−y) for some continuous function Γ defined on Rd. Bochner’s theorem [33] states\nthat such a K is a positive kernel if and only if Γ is the Fourier transform of a positive\nmeasure, namely,\nΓ(x) =\nZ\nRd e−2iπ⟨x,ω⟩dµ(ω)\nwhere µ is a positive and symmetric (invariant by sign change) measure on Rd. For\nexample one can take dµ(ω) = ν(ω)dω, where ν is a integrable, positive and even\nfunction.\nThis theorem provides an at least numerical, and sometimes analytical, method\nfor constructing kernels. The previous section exhibited a special case of translation-\ninvariant kernel for which ν = |σ|2.\n\n130\nCHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS\nRadial kernels\nA radial kernel takes the form K(x,y) = γ(|x −y|2), for some continuous function\nγ defined on [0,+∞). Shoenberg’s theorem [173] states that, if this function γ is\nuniversally valid, i.e., K is a kernel for all dimensions d, then, it must take the form\nγ(t) =\nZ ∞\n0\ne−λtdµ(λ)\nfor some positive finite measure µ on [0,+∞).\nFor example, when µ is a Dirac measure, i.e., µ = δ(2a)−1 for some a > 0, then\nK(x,y) = exp(−|x −y|2/2a), which is the Gaussian kernel. Taking dµ = e−aλdλ yields\nγ(t) = 1/(t + a), and dµ = λe−aλdλ yields γ(t) = 1/(a + t)2.\nThere is also, in Schoenberg [173], a characterization of radial kernels for a fixed\ndimension d. Such kernels must take the form\nγ(t) =\nZ +∞\n0\nΩd(tλ)dµ(λ)\nwith Ωd(t) = Γ(d/2)(2/t)(d−2)/2J(d−2)/2(t) where J(d−2)/2 is Bessel’s function of the first\nkind.\n6.3.5\nOperations on kernels\nKernels can be combined in several ways as described in the next proposition.\nProposition 6.4 Let K1 : R × R →R and K2 : R × R →R be positive kernels. Then the\nfollowing assertions hold.\n(i) If λ1,λ2 > 0, λ1K1 + λ2K2 is a positive kernel. It is positive definite as soon as either\nK1 or K2 is positive definite.\n(ii) For any function f : R′ →R, K′\n1(x′,y′)\n∆= K1(f (x′),f (y′)) is a positive kernel. It is\npositive definite as soon as K1 is positive definite and f is one-to-one.\n(iii) K(x,y) = K1(x,y)K2(x,y) is a positive kernel. It is positive definite as soon as K1 and\nK2 are positive definite.\n(iv) Let K1 and K2 be translation-invariant with R = Rd, taking the form Ki(x,y) =\nΓi(x −y), where Γi is continuous ( i = 1,2). Assume that one of the two functions\nΓ1,Γ2 is integrable on Rd. Then\nK(x,y) =\nZ\nRd K1(x,z)K2(z,y)dz\nis also a positive kernel.\n\n6.3. FIRST EXAMPLES\n131\nProof Point (i) is obvious. Point (ii) is almost as simple, because, for any λ1,...,λn ∈\nR and x′\n1,...,x′\nn ∈R′,\nn\nX\ni,j=1\nλiλjK′\n1(x′\ni,x′\nj) =\nn\nX\ni,j=1\nλiλjK1(f (x′\ni),f (x′\nj)) ≥0.\nIf K1 is positive definite, then the latter sum can only vanish if all λi are zero, or\nsome of the points in (f (x′\n1),...,f (x′\nn)) coincide. If, in addition, f is one-to-one, then\nthis is equivalent to all λi are zero, or some of the points in (x′\n1,...,x′\nn) coincide, so\nthat K′\n1 is positive definite.\nTo prove point (iii), take x1,...,xN ∈Rd and form the matrices Ki = Ki(x1,...,xN),\ni = 1,2, which are, by assumption positive semi-definite. The matrix K = K(x1,...,xN)\nis the element-wise (or Hadamard) product of K1 and K2, and the conclusion fol-\nlows from the linear algebra result stating that the Hadamard product of two pos-\nitive semi-definite (resp. positive definite) matrices A = (a(i,j),1 ≤i,j ≤N) and\nB = (b(i,j),1 ≤i,j ≤N) is positive semi-definite (resp. positive definite). This is\nproved by diagonalizing, say, A in an orthonormal basis u1,...,uN, with eigenvalues\nλ1,...,λN and writing\nN\nX\ni,j=1\nα(i)a(i,j)b(i,j)α(j) =\nN\nX\ni,j,k=1\nα(i)u(k)\ni u(k)\nj λkb(i,j)α(j)\n=\nN\nX\nk=1\nλk\nN\nX\ni,j=1\n(α(i)u(k)\ni )(α(j)u(k)\nj ))b(i,j) ≥0\nIf B is positive definite, then the sum above can be zero only if, for each k, either\nλk = 0 or α(i)u(k)\ni\n= 0 for all i. If A is also positive definite, then the only possibility\nis α(i)u(k)\ni\n= 0 for all i and k, which implies α(i) = 0 for all i since ui , 0.\nTo prove point (iv) 4, we first note that a translation invariant kernel K′(x,y) =\nΓ′(x −y) is always bounded. Indeed, the matrix K′(x,0) is positive semi-definite,\nwith determinant Γ′(0)2 −Γ′(x)2 > 0, showing that |Γ′(x)| < Γ′(0). This shows that\nthe integral defining K(x,y) converges as soon as one of the two functions Γ1 or Γ2 is\nintegrable. Moreover, we have K(x,y) = Γ(x −y) with\nΓ(x) =\nZ\nRd Γ1(x −z)Γ2(z)dz =\nZ\nRd Γ1(x −u)Γ2(u −y)du\nUsing the fact that both Γ1 and Γ2 are even, and making the change of variable z 7→−z,\none easily shows that Γ(x) = Γ(−x), which implies that K is symmetric.\n4This part of the proof uses some measure theory.\n\n132\nCHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS\nWe proceed with the assumption that Γ2 is integrable and use Bochner’s theorem\nto write\nΓ1(y) =\nZ\nRd e−iξT ydµ1(ξ)\nfor some positive finite measure µ1. Then\nΓ(x) =\nZ\nRd\n Z\nRd e−2iπξT (x−z)dµ1(ξ)\n!\nΓ2(z)dz\n=\nZ\nRd e−2iπξT x\n Z\nRd e2iπξT zΓ2(z)dz\n!\ndµ1(ξ)\nThe shift in the order of the variables ξ and z uses Fubini’s theorem. The function\nψ(ξ) =\nZ\nRd e2iπξT zΓ2(z)dz\nis the inverse Fourier transform of Γ2. Because Γ2 is bounded and integrable, it is also\nsquare integrable, which implies that its inverse Fourier transform is also a square\nintegrable function. Since Bochner’s theorem implies that Γ2 is the Fourier transform\nof a positive measure µ2, we find, using the injectivity of the Fourier transform, that\nψ is non-negative. So Γ is the Fourier transform of the finite positive measure ψdµ1,\nwhich implies that K is a positive kernel.\n■\nPoint (iv) can be related to the following discrete statement on symmetric ma-\ntrices: assume that A and B are positive semi-definite and that they commute, so\nthat AB = BA: then AB is positive semi-definite (see ??). In the case of kernels, one\nmay consider the symmetric linear operators Ki : f 7→\nR\nRd Ki(·,y)f (y)dy which maps\nthe space of square integrable functions into itself. Then K1 and K2 commute and\nK = K1K2.\n6.3.6\nCanonical Feature Spaces\nLet K be a positive kernel on a set R. The following construction, which is fun-\ndamental, shows that K can always be associated with a feature function h taking\nvalues in a suitably chosen inner-product space H.\nAssociate to each x ∈R the function ξx : y 7→K(y,x) (we will also write ξx =\nK(·,x)), and let HK = span(ξx,x ∈R), a subspace of the vector space of all functions\nfrom R to R. Define the feature function h : x 7→ξx from R to HK. There is a unique\ninner product on HK such that K = Kh. Indeed, by definition, this requires\n⟨K(·,x) , K(·,y)⟩HK = K(x,y).\n(6.3)\n\n6.4. PROJECTION ON A FINITE-DIMENSIONAL SUBSPACE\n133\nMoreover, by linearity, for any ξ = Pn\ni=1 λiK(·,xi) and η = Pm\ni=1 µiK(·,yi), one needs\n⟨ξ , η⟩HK =\nn\nX\ni=1\nm\nX\nj=1\nλiµjK(xi,yj),\nso that the inner product is uniquely specified on HK. To make sure that this inner-\nproduct is well defined, we must check that there is no ambiguity, in the sense that,\nif ξ has an alternative decomposition ξ = Pn′\ni=1 λ′\niK(·,x′\ni), then, the value of ⟨ξ , η⟩HK\nremains unchanged. But this is clear, because one can also write\n⟨ξ , η⟩HK =\nm\nX\nj=1\nµjξ(yj),\nwhich only depends on ξ and not on its decomposition. The linearity of the product\nwith respect to ξ is also clear from this expression, and the bilinearity by symmetry.\nThe Schwartz inequality implies that\n|⟨ξ , η⟩HK| ≤∥ξ∥HK ∥η∥HK\nFrom which we deduce that ∥ξ∥HK = 0 implies that ⟨ξ , η⟩HK = 0 for η ∈HK. Since\n⟨ξ , K(·,y)⟩HK = ξ(y) for all y, this also implies that ξ = 0, completing the proof that\nHK is an inner-product space.\nEquation (6.3) is the “reproducing property” of the kernel for the inner-product\non HK. In functional analysis, the completion, ˆHK, of HK for the topology associated\nto its norm is then a Hilbert space, and is referred to as a “reproducing kernel Hilbert\nspace,” or RKHS.\nMore generally, an inner-product space H of functions h : R →R is a reproducing\nkernel Hilbert space if H is a complete space (which makes it Hilbert) and there\nexists a positive kernel K such that,\n[RKHS1] For all x ∈R, K(·,x) belongs to H,\n[RKHS2] For all h ∈H and x ∈R,\n⟨h , K(·,x)⟩H = h(x).\nReturning to the example of functional features in section 6.3.3, we have two dif-\nferent representations of the kernel in feature space, namely in H = L2(Rd), or in HK,\nwith a different inner product. There is not a contradiction, and simply shows that\nthe representation of a positive kernel in terms of a feature function is not unique.\n\n134\nCHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS\n6.4\nProjection on a finite-dimensional subspace\nIf H is an inner-product space and V is a subspace of H, one defines the orthogonal\nprojection of an element ξ ∈H on V as its closest point in V , that is, the element\nη∗of V minimizing the function F : η 7→∥η −ξ∥2\nH over all η ∈V . This closest point\ndoes not always exist, but it does in the special case in which V is finite dimensional\n(or, more generally, when V is a closed subspace of H; see Yosida [205]). We state,\nwithout proof, some of the properties of this operation.\nAssuming that V is closed, this minimizer is unique and will be denoted η∗=\nπV (ξ). Moreover, πV is a linear transformation from H to V , and η∗is characterized\nby the properties\n(η∗∈V\nξ −η∗⊥V ,\nthe last condition meaning that ⟨ξ −η∗, η⟩H = 0 for all η ∈V .\nBecause ∥ξ∥2\nH = ∥πV (ξ)∥2\nH + ∥ξ −πV (ξ)∥2\nH, one always has ∥πV (ξ)∥H ≤∥ξ∥H, with\ninequality if and only if πV (ξ) = ξ, i.e., if and only if ξ ∈V .\nIf V is finite-dimensional and η1,...,ηn is a basis of V , then πV (ξ) is given by\nπV (ξ) =\nn\nX\ni=1\nα(i)ηi\nwith α (considered as a column vector in Rn) given by\nα = Gram(η1,...,ηn)−1λ,\nwhere λ ∈Rn is the vector with coordinates λ(i) = ⟨ξ , ηi⟩H, i = 1,...,n. The Gram ma-\ntrix of η1,...,ηn, denoted Gram(η1,...,ηn), is the n by n matrix with entries ⟨ηi , ηj⟩H\nfor i,j = 1,...,n.\nIf A is a subset of H, the set A⊥consists of all vectors perpendicular to A, namely\nA⊥=\nn\nh ∈H : ⟨h , ˜h⟩H = 0 for all ˜h ∈A\no\n.\nIf V is a finite-dimensional (or, more generally, closed) subspace of H, then any point\nin h is decomposed as h = πV (h)+h−πV (h) with h−πV (h) ∈V ⊥. This shows that πV ⊥\nis well defined and equal to idH −πV .\nOrthogonal projections can be applied to function interpolation in an RKHS. In-\ndeed, assuming that H is an RKHS, as described at the end of the previous sec-\ntion, with a positive-definite kernel. Given distinct points x1,...,xN ∈R and values\n\n6.4. PROJECTION ON A FINITE-DIMENSIONAL SUBSPACE\n135\nα1,...,αN ∈R, the interpolation problem consists in finding h ∈H with minimal\nnorm satisfying h(xk) = αk, k = 1,...,N. Consider the finite dimensional space\nV = span{K(·,xk),k = 1,...N}.\nThen there exists an element h0 ∈V that satisfies the constraints. Indeed, looking\nfor h0 in the form\nh0(x) =\nN\nX\nl=1\nK(x,xl)λl\none has\nh0(xk) =\nN\nX\nl=1\nK(xk,xl)λl\nso that\n\n\nλ1...\nλN\n\n\n= K(x1,...,xN)−1\n\n\nα1...\nαN\n\n\nAny other function h satisfying the constraints satisfies h(xk) −h0(xk) = 0, which,\nusing RKHS2, is equivalent to ⟨h −h0 , K(·,xk)⟩H = 0, i.e., to h −h0 ∈V ⊥. This shows\nthat h0 = πV (h), so that ∥h∥H ≥∥h0∥H and h0 provides the optimal interpolation. We\nsummarize this in the proposition:\nProposition 6.5 Let H is an RKHS with a positive-definite kernel. Let x1,...,xN ∈R be\ndistinct points and α1,...,αN ∈R. Then the function h ∈H with minimal norm satisfying\nh(xk) = αk, k = 1,...,N takes the form\nh(xk) =\nN\nX\nl=1\nK(xk,xl)λl\n(6.4a)\nwith\n\n\nλ1...\nλN\n\n\n= K(x1,...,xN)−1\n\n\nα1...\nαN\n\n\n.\n(6.4b)\nA variation of this problem replaces the constraint by a penalty that complete\nthe minimization associated with the orthogonal projection, namely, minimizing (in\nh ∈H)\n∥h∥2\nH + σ2\nN\nX\nk=1\n|h(xk) −αk|2.\n\n136\nCHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS\nLetting h0 = πV (h), so that h0(xk) = h(xk) for all k, this expression can be rewritten as\n∥h0∥2\nH + ∥h −h0∥2\nH + σ2\nN\nX\nk=1\n|h0(xk) −αk|2.\nThis shows that the optimal h must coincide with its projection on V , and therefore\nbelong to that subspace. Looking for h in the form\nh(·) =\nN\nX\nl=1\nK(·,xl)λl,\nthe objective function is rewritten as\nN\nX\nk,l=1\nK(xk,xl)λkλl + σ2\nN\nX\nk=1\n\f\f\f\f\f\f\f\nN\nX\nl=1\nK(xk,xl)λl −αk\n\f\f\f\f\f\f\f\n2\n,\nwhich, in vector notation gives, writing λ =\n\n\nλ1...\nλN\n\n\nand α =\n\n\nα1...\nαN\n\n\n,\nλT K(x1,...,xN)λ + σ2 (K(x1,...,xN)λ −α)T (K(x1,...,xN)λ −α).\nThe differential of this expression in λ is\nK(x1,...,xN)λ + 2σ2K(x1,...,xN)(K(x1,...,xN)λ −α).\nAssuming that x1,...,xN are distinct, this vanishes if and only if\nλ = (K(x1,...,xN) + (1/σ2)IdRN)−1α.\nWe have just proved the proposition:\nProposition 6.6 Let H is an RKHS with a positive-definite kernel. Let x1,...,xN ∈R be\ndistinct points and α1,...,αN ∈R. Then the unique minimizer of\nh 7→∥h∥2\nH + σ2\nN\nX\nk=1\n|h(xk) −αk|2\non H is given by\nh(xk) =\nN\nX\nl=1\nK(xk,xl)λl\n(6.5a)\nwith\n\n\nλ1...\nλN\n\n\n= (K(x1,...,xN) + (1/σ2)IdRN)−1\n\n\nα1...\nαN\n\n\n.\n(6.5b)\n\nChapter 7\nLinear Models for Regression\nIn regression, linear models refer to situations in which one tries to predict the de-\npendent variable Y ∈RY = Rq by a function ˆf (X) of the dependent variable X ∈RX,\nwhere ˆf is optimized over a linear space F . The most common situation is the “stan-\ndard linear model,” for which RX = Rd and\nF = {f (x) = a0 + bT x : a0 ∈Rq,b ∈Md,q(R)}.\n(7.1)\nMore generally, with q = 1, given a mapping h : R →H, where H is an inner-\nproduct space, one can take:\nF = {f (x) = a0 + ⟨b , h(x)⟩H : a0 ∈R,b ∈H}.\n(7.2)\nNote that h can be nonlinear, and F can be infinite dimensional. Such sets corre-\nsponds to linear models using feature functions, and will be addressed using kernel\nmethods in this chapter.\nNote also that, even if the model is linear, the associated training algorithms\ncan be nonlinear, and we will review in fact several situations in which solving the\nestimation problem requires nonlinear optimization methods.\n7.1\nLeast-Square Regression\n7.1.1\nNotation and Basic Estimator\nWe denote by Y and X the dependent and independent variables of the regression\nproblem. We will assume that Y takes values in Rq and that X takes values in a set\nRX, which will, by default, be equal to Rd, except when discussing kernel methods,\nfor which this set can be arbitrary (provided that there is a mapping h from RX to\nan inner product space H with an easily computable kernel).\n137\n\n138\nCHAPTER 7. LINEAR REGRESSION\nLeast-square regression uses the risk function r(y,y′) = |y −y′|2. The prediction\nerror is then R(f ) = E(|Y −f (X)|2) for any predictor f and the Bayes predictor is the\nconditional expectation x 7→E(Y | X = x) (see item Example 1. in section 5.3). We\nalso start with the standard setting where RX = Rd and F given by (7.1).\nWe will use the following notation, which sometimes simplifies the computation.\nIf x ∈Rd, we let ˜x =\n \n1\nx\n!\n, which belongs to Rd+1. The linear predictor f (x) = a0 +\nbT x with a0 ∈Rq,b ∈Md,q(R) can then be written as f (x) = βT ˜x with β =\n \naT\n0\nb\n!\n∈\nMd+1,q(R).\nIn a model-based approach, the linear model is a Bayes predictor under the\ngenerative assumption that Y = a0 + bT X + ϵ where ϵ is a residual noise satisfying\nE(ϵ | X) = 0, which is true, for example, when ϵ is centered and independent of\nX. If one further specifies the model so that ϵ is Gaussian, centered and indepen-\ndent of X, and one assumes that the distribution of X does not depend on a0 and b,\nthen the maximum likelihood estimator of these parameters based on a training set\nT = ((x1,y1),...,(xN,yN)) must minimize the “residual sum of squares:”\nRSS(β)\n∆= N ˆR(f )\n∆=\nN\nX\nk=1\n|yk −f (xk)|2 =\nN\nX\nk=1\n|yk −βT ˜xk|2 .\nIn other terms, the model-based approach is identical, under these (standard) as-\nsumptions, to empirical risk minimization (section 5.5), on which we now focus.\n(Recall that, even when using a model-based approach, one does not make assump-\ntions on the true distribution of X and Y; one rather treats the model as an approxi-\nmation of these distributions, estimated by maximum likelihood, and uses the Bayes\npredictor for the estimated model.)\nThe computation of the optimal regression parameters is made easier by the in-\ntroduction of the following matrices. Introduce the N × (d + 1) matrix X with rows\n˜xT\n1 ,..., ˜xT\nN and the N × q matrix Y with rows yT\n1 ,...,yT\nN, that is:\nX =\n\n\n1\nx(1)\n1\n···\nx(d)\n1\n...\n...\n...\n1\nx(1)\nN\n···\nx(d)\nN\n\n\n,\nY =\n\n\ny(1)\n1\n···\ny(q)\n1\n...\n...\ny(1)\nN\n···\ny(q)\nN\n\n\n.\nWith this notation, we have\nRSS(β) = |Y −X β|2\n2.\nwith |A|2\n2 = trace(AT A) for a rectangular matrix A. The solution of the problem is\nthen provided by the following theorem.\n\n7.1. LEAST-SQUARE REGRESSION\n139\nTheorem 7.1 Assume that the matrix X has rank d + 1. Then the RSS is minimized for\nˆβ = (X T X )−1X T Y\nProof We provide two possible proofs of this elementary problem. The first one is\nan optimization argument noting that F(β)\n∆= RSS(β) is a convex function defined\non Md+1,q(R) and with values in R. Since F is quadratic, we have, for any matrix\nh ∈Md+1,q(R),\ndF(β)h = ∂ϵF(β + ϵh)|ϵ=0 = −2trace(hT X T (Y −X β))\nand\ndF(β) = 0 ⇔X T (Y −X β) = 0 ⇔β = ˆβ.\nOne can alternatively proceed with a direct computation. We have\nRSS(β) = |Y|2\n2 −2trace(βT X T Y) + trace(βT X T X β)\n= |Y|2\n2 −2trace(βT X T X ˆβ) + trace(βT X T X β).\nReplacing β by ˆβ and simplifying yields\nRSS( ˆβ) = |Y|2\n2 −trace( ˆβT X T X ˆβ)\nIt follows that\nRSS(β) =RSS( ˆβ) + trace( ˆβT X T X ˆβ) −2trace(βT X T X ˆβ) + trace(βT X T X β)\n=RSS( ˆβ) + |X ( ˆβ −β)|2\n2\nso that the left-hand side is minimized at β = ˆβ.\n■\nRemark 7.2 If X does not have rank d + 1, then optimal solutions exist, but they\nare not unique. By convexity, the solutions are exactly the vectors β at which the\ngradient vanishes, i.e., those that satisfy X T X β = X T Y. The set of solutions can be\nobtained by introducing the SVD of X in the form X = UDV T and letting γ = V T β\nand Z = UT Y. Then\nX T X β = X T Y ⇔DT Dγ = DT Z.\nLetting d(1),...,d(m) denote the nonzero diagonal entries of D (so that m ≤d + 1), we\nfind γ(i) = z(i)/d(i) for i ≤m (the other equalities being 0 = 0). So, the d + 1 −m last\nentries of γ can be chosen arbitrarily (and β = V γ).\n♦\nAn alternate representation of the solution use a two-step computation that esti-\nmates b first, then a0. Indeed, for fixed ˆb, the minimum of\nN\nX\nk=1\n|yk −a0 −xT\nk ˆb|2\n\n140\nCHAPTER 7. LINEAR REGRESSION\nis attained at ˆa0 = ¯y −¯xT ˆb with the usual definitions\n¯y = 1\nN\nN\nX\nk=1\nyk and ¯x = 1\nN\nN\nX\nk=1\nxk.\nThis shows that ˆb itself must be a minimizer of\nN\nX\nk=1\n|yk −¯y −(xk −¯x)T b|2.\nDenote by Yc and Xc the matrices\nXc =\n\n\nx(1)\n1 −x(1)\n···\nx(d)\n1 −x(d)\n...\n...\nx(1)\nN −x(1)\n···\nx(d)\nN −x(d)\n\n\n, Yc =\n\n\ny(1)\n1 −y(1)\n···\ny(q)\n1 −y(q)\n...\n...\ny(1)\nN −y(1)\n···\ny(q)\nN −y(q)\n\n\n.\nThen ˆb must minimize |Yc −Xcb|2, yielding\nˆb = (X T\nc Xc)−1X T\nc Yc,\nˆa0 = ¯y −¯xT ˆb.\nThe reader may want to double-check that this solution coincides with the one pro-\nvided in theorem 7.1.\n7.1.2\nLimit behavior\nThe matrix\nˆΣXX = 1\nN X T\nc Xc = 1\nN\nN\nX\nk=1\n(xk −x)(xk −x)T\nis a sample estimate of the covariance matrix of X, that we will denote ΣXX. Simi-\nlarly, ˆΣXY = X T\nc Yc/N is a sample estimate of ΣXY, the covariance between X and Y.\nWith this notation, we have\nˆb = ˆΣ−1\nXX ˆΣXY,\nwhich, by the law of large numbers, converges to b∗= Σ−1\nXXΣXY.\nLet a∗\n0 = mY−mT\nXb∗. Then f ∗(x) = a∗\n0+(b∗)T x is the least-square optimal approxima-\ntion of Y by a linear function of X, and the linear predictor ˆf (x) = ˆa0 + ˆbT x converges\na.s. to f ∗(x). Of course, f ∗generally differs from f : x 7→E(Y | X = x), which is the\nleast-square optimal approximation of Y by any (square-integrable) function of X,\nso that the linear estimator will have a residual bias.\n\n7.1. LEAST-SQUARE REGRESSION\n141\n7.1.3\nGauss-Markov theorem\nIf one makes the (unlikely) assumption that the linear model is exact, i.e., f (x) =\nf ∗(x), one has:\nE( ˆβ) = E(E( ˆβ | X )) = E((X T X )−1X TE(Y | X )) = E((X T X )−1X T X β) = β\nand the estimator is “unbiased.” Under this parametric assumption, many other\nproperties of linear estimators can be proved, among which the well-known Gauss-\nMarkov theorem on the optimality of least-square estimation that we now state and\nprove. For this theorem, for which we take (for simplicity) q = 1, we also assume\nthat var(Y | X = x), the variance of Y for its conditional distribution given X does\nnot depend on x, and denote it by σ2. This typically correspond to the standard\nregression model in which one assumes that Y = f (X) + ϵ where ϵ is independent of\nX with variance σ2.\nRecall that a symmetric matrix A is said to be larger than or equal to another\nsymmetric matrix, B, writing A ⪰B, if and only if A −B is positive semi-definite.\nTheorem 7.3 (Gauss-Markov) Assume that an estimator ˜β takes the form ˜β = A(X )Y\n(it is linear) and is unbiased conditionally to X : Eβ( ˜β | X ) = β (for all β). Then (under\nthe assumptions above) the covariance matrix of ˜β cannot be smaller than that of the least\nsquare estimate, ˆβ.\nProof We write A = A(X ) for short. The condition that E(AY | X ) = β for all β yields\nAX β = β for all β, or AX = IdRd+1 (A is a (d + 1) × N matrix). Since ˜β is unbiased, its\ncovariance matrix is\nE(AYY T AT ) −ββT\nand\nE(AYY T AT ) = E(E(AYY T AT | X )) = σ2E(AAT ).\nFor ˜β = ˆβ, for which A = (X T X )−1X T , we get E(AYY T AT ) = σ2E((X T X )−1). We\ntherefore need to show that E(AAT ) ⪰E(X T X ), i.e., that for any u ∈Rd+1,\nuTE(AAT )u ≥uTE((X T X )−1)u\nas soon as AX = IdRd+1. We in fact have the stronger result (without expectations):\nAX = IdRd+1 ⇒AAT ⪰(X T X )−1.\nTo see this, fix u and consider the problem of minimizing Fu(A) = A 7→uT AAT u\nsubject to the linear constraint AX = IdRd+1. The Lagrange multipliers for this affine\nconstraint can be organized in a matrix C and the Lagrangian is\nuT AAT u + trace(CT (AX −IdRd+1)).\n\n142\nCHAPTER 7. LINEAR REGRESSION\nTaking the derivative in A, we find that optimal solutions must satisfy\n2uT AHT u + trace(CT HX ) = 0\nfor all H, which yields trace(HT (2uuT A + CX T )) = 0 for all H. This is only possible\nwhen 2uuT A + CX T = 0, which in turn implies that 2uuT AX = −CX T X . Using the\nconstraint, we get\nC = −2uuT (X T X )−1\nso that uuT A = uuT (X T X )−1X T . This implies that A = (X T X )−1X T (the least-square\nestimator) is a minimizer of Fu(A) for all u.\nAny other solution that satisfies uuT A = uuT (X T X )−1X T for all u. Taking u = ei\nand summing over i (with Pd+1\ni=1 eieT\ni = IdRd+1) yields A = (X T X )−1X T .\n■\n7.1.4\nKernel Version\nWe now assume that X takes its values in an arbitrary set RX, with a representation\nh : RX →H into an inner-product space. This representation does not need to be\nexplicit or computable, but the associated kernel K(x,y) = ⟨h(x) , h(y)⟩H is assumed\nto be known and easy to compute. (Recall that, from chapter 6, a positive kernel is\nalways associated with an inner-product space.) In particular, any algorithm in this\ncontext should only rely on the kernel, and the function h only has a conceptual role.\nAssume that q = 1 to lighten the notation, so that the dependent variable is scalar-\nvalued. We here let the space of predictors be\nF = {f (x) = a0 + ⟨b , h(x)⟩H : a0 ∈R,b ∈H}.\nThe residual sum of squares associated with this function space is\nRSS(a0,b) =\nN\nX\nk=1\n(yk −a0 −⟨b , h(xk)⟩)2.\nThe following result (or results similar to it) is a key step in almost all kernel\nmethods in machine learning.\nProposition 7.4 Let V = span(h(x1),...,h(xN)) be the finite-dimensional subspace of H\ngenerated by the feature functions evaluated on training input data. Then\nRSS(a0,b) = RSS(a0,πV (b)).\nwhere πV is the orthogonal projection on V .\n\n7.2. RIDGE REGRESSION AND LASSO\n143\nProof The justification is immediate: since h(xk) ∈V , we have\n⟨b , h(xk)⟩H = ⟨πV (b) , h(xk)⟩H\nfor all b ∈H.\n■\nThis shows that there is no loss of generality in restricting the minimization of\nthe residual sum of squares to b ∈V . Such a b takes the form\nb =\nN\nX\nk=1\nαkh(xk)\n(7.3)\nand the regression problem can be reformulated as a function of the coefficients\nα1,...,αN ∈R, with\nf (x) = a0 +\nN\nX\nk=1\nαk⟨h(x) , h(xk)⟩H = a0 +\nN\nX\nk=1\nαkK(x,xk),\nwhich only depends on the kernel. (This reduction is often referred to as the “kernel\ntrick.”)\nHowever, the solution of the problem is, in this context, not very interesting.\nIndeed, assume that K is positive definite and that all observations in the training set\nare distinct. Then the matrix K(x1,...,xN) formed by the kernel evaluations K(xi,xj)\nis invertible, and one can solve exactly the equations\nyk =\nN\nX\nj=1\nαjK(xk,xj),\nk = 1,...,N\nto get a zero RSS with a0 = 0. Unless there is no noise, such a solution will certainly\noverfit the data. If K is not positive definite, and the dimension of V is less than\nN (since this would place us in the previous situation otherwise), then it is more\nefficient to work directly in a basis of V rather than using the over-parametrized ker-\nnel representation. We will see however, starting with the next section, that kernel\nmethods become highly relevant as soon as the regression is estimated with some\ncontrol on the size of the regression coefficients, b.\n7.2\nRidge regression and Lasso\n7.2.1\nRidge Regression\nMethod.\nWhen the set F of possible predictors is too large, some additional com-\nplexity control is needed to reduce the estimation variance. One simple approach\n\n144\nCHAPTER 7. LINEAR REGRESSION\nis to limit the number of parameters to be estimated, which, for regression, corre-\nsponds to limiting the number of possible predictors. This is related to the methods\nof Sieves mentioned in section 4.1. In contrast, ridge regression and lasso control the\nsize of the parameters, as captured by their norm.\nIn both cases, one assigns a measure of complexity, denoted f 7→γ(f ) ≥0, to each\nelement f ∈F . Given γ, one can either optimize this predictor (using, for example,\nthe RSS) with the constraint that γ(f ) ≤C for some constant C, or add a penalty\nλγ(f ) to the objective function for some λ > 0. In general, the two approaches (con-\nstraint or penalty) are equivalent.\nIn linear spaces, complexity measures are often associated with a norm, and ridge\nregression uses the sum of squares of coefficients of the prediction matrix b, mini-\nmizing\nN\nX\nk=1\n|yk −a0 −bT xk|2 + λtrace(bT b),\n(7.4)\nwhich can be written in vector form as\n|Y −X β|2\n2 + λtrace(βT ∆β),\nwhere ∆= diag(0,1,...,1). In the following, we will work with an unspecified (d +\n1) × (d + 1) symmetric positive semi-definite matrix ∆. Various choices are indeed\npossible, for example, ∆= diag(0, ˆσ2(1),..., ˆσ2(d)), where ˆσ2(i) is the empirical vari-\nance of the ith coordinate of X in the training set. This last choice is quite natural,\nbecause it ensures that, whenever one of the variable X(i) is rescaled by a factor c,\nthe corresponding optimal ith row of bT is rescaled by 1/c, leaving the predictor\nunchanged.\nUnder this assumption, the optimal parameter is\nˆβλ = (X T X + λ∆)−1X T Y ,\nwith a proof similar to that made for least-square regression. We obviously retrieve\nthe original formula for regression when λ = 0.\nAlternatively, assuming that ∆=\n \n0\n0\n0\n∆′\n!\n, so that no penalty is imposed on the\nintercept, we have\nˆbλ = (X T\nc Xc + λ∆′)−1X T\nc Yc\n(7.5)\nand ˆa0λ = ¯y −(ˆbλ)T ¯x. The proof of these statements is left to the reader.\n\n7.2. RIDGE REGRESSION AND LASSO\n145\nAnalysis in a special case\nTo illustrate the impact of the penalty term on balancing\nbias and variance, we now make a computation in the special case when Y = ˜Xβ + ϵ,\nwhere var(ϵ) = σ2 and ϵ is independent of X. In the following computation, we as-\nsume that the training set is fixed (or rather, compute probabilities and expectations\nconditionally to it). Also, to simplify notation, we denote\nSλ = X T X + λ∆=\nN\nX\nk=1\n˜xT\nk ˜xk + λ∆\nand Σ = E( ˜XT ˜X) for a single realization of X. Finally, we assume that q = 1, also to\nsimplify the discussion.\nThe mean-square prediction error is\nR(λ)\n=\nE((Y −˜XT ˆβλ)2)\n=\nE(( ˜XT (β −ˆβλ) + ϵ)2)\n=\n( ˆβλ −β)T Σ( ˆβλ −β) + σ2.\nDenote by ϵk the (true) residual ϵk = yk −˜xT\nk β on training data and by ϵ the vector\nstacking these residuals. We have, writing S0 = Sλ −λ∆,\nˆβλ\n=\nS−1\nλ X T Y\n=\nS−1\nλ S0β + S−1\nλ X T ϵ\n=\nβ −λS−1\nλ ∆β + S−1\nλ X T ϵ\nSo we can rewrite\nR(λ) = λ2βT ∆S−1\nλ ΣS−1\nλ ∆β −2λϵT X S−1\nλ ΣS−1\nλ ∆β + ϵT X S−1\nλ ΣS−1\nλ X T ϵ + σ2.\nLet us analyze the quantities that depend on the training set in this expression. The\nfirst one is Sλ = S0 + λ∆. From the law of large numbers, S0/N →Σ when N tends\nto infinity, so that, assuming in addition that λ = λN = O(N), we have S−1\nλ = O(1/N).\nThe second one is\nϵT X =\nN\nX\nk=1\nϵk ˜xk\nwhich, according to the central limit theorem, is such that\nN −1/2ϵT X ∼N (0,σ2Var( ˜X))\nwhen N →∞. So, we can expect the coefficient of λ2 in R(λ) to have order N −2, the\ncoefficient of λ to have order N −3/2 and the constant coefficient of have order N −1.\nThis suggests taking λ = µ\n√\nN so that all coefficients have roughly the same order\nwhen expanding in powers of µ.\n\n146\nCHAPTER 7. LINEAR REGRESSION\nThis gives Sλ = N(S0/N +µ∆/\n√\nN) ≃NΣ and we make the approximation, letting\nξ = N −1/2σ−1/2ϵX T and γ = Σ−1/2∆β, that\nN(R(λ) −σ2) ≃µ2|γ|2 −2µξT γ + ξT ξ.\nWith this approximation, the optimal µ should be\nµ = ξT γ\n|γ|2 .\nOf course, this µ cannot be computed from data, but we can see that, since ξ con-\nverges to a centered Gaussian random variable, its value cannot be too large. It is\ntherefore natural to choose µ to be constant and use ridge regression in the form\nN\nX\nk=1\n(yk −˜xT\nk β)2 +\n√\nNµβT ∆β.\nIn all cases, the mere fact that we find that the optimal µ is not 0 shows that, under\nthe simplifying (and optimistic) assumptions that we made for this computation,\nallowing for a penalty term always reduces the prediction error. In other terms,\nintroducing some estimation bias in order to reduce the variance is beneficial.\nKernel Ridge Regression\nWe now return to the feature-space situation and take h :\nRX →H with associated kernel K. We still take q = 1 for simplicity. One formulates\nthe ridge regression problem in this context as the minimization of\nN\nX\nk=1\n(yl −a0 −⟨b , h(xl)⟩H)2 + λ∥b∥2\nH\nwith respect to β = (a0,b). Introducing the space V generated by the feature function\nevaluated on the training set, we know from proposition 7.4 that replacing b by\nπV (b) leaves the residual sum of squares invariant. Moreover, one has ∥πV (b)∥2\nH ≤\n∥b∥2\nH with equality if and only if b ∈V . This shows that the solution b must belong\nto V and therefore take the form (7.3).\nUsing this expression, one finds that the problem is reduced to finding the mini-\nmum of\nN\nX\nk=1\n\nyk −a0 −\nN\nX\nl=1\nK(xl,xk)αl\n\n\n2\n+ λ\nN\nX\nk,l=1\nαkαlK(xk,xl)\nwith respect to a0,α1,...,αN. Recall that we have denoted by K = K(x1,...,xN) the\nkernel matrix with entries K(xi,xj), i,j = 1,...,N. We will assume in the following\nthat K is invertible.\n\n7.2. RIDGE REGRESSION AND LASSO\n147\nIntroduce the vector 1N ∈RN with all coordinates equal to one. Let\n˜K =\n\u0010\n1N\nK\n\u0011\nand K′ =\n \n0\n0\n0\nK\n!\n.\nLet α ∈RN be the vector with coefficients α1,...,αN and ˜α =\n \na0\nα\n!\n. With this\nnotation, the function to minimize is\nF(α) = |Y −˜K ˜α|2 + λ ˜αT K′ ˜α.\nThis takes the same form as standard ridge regression, replacing β by ˜α, X by ˜K and\n∆by K′. The solution therefore is\n˜αλ = ( ˜KT ˜K + λK′)−1 ˜KT Y.\nNote that K being invertible implies that ˜KT ˜K + λK′ is invertible. 1\nTo write the equivalent of (7.5), we need to use the equivalent of the matrix Xc,\nthat is, the matrix K with the average of the jth column subtracted to each (i,j) entry,\ngiven by:\nKc = K −1\nN 1N1T\nNK.\nIntroduce the matrix P = Id −1N1T\nN/N. It is easily checked that P2 = P (P is a pro-\njection matrix). Since Kc = PK, we have KT\nc Kc = KPK. One deduces from this the\nexpression of the optimal vector αλ, namely,\nαλ = (KPK + λK)−1KPYc = (PK + λIdRN)−1Yc\nwhere we have, in addition, used the fact that PYc = Yc. Finally, the intercept is\ngiven by\na0 = y −1\nN (αλ)T K1N.\n7.2.2\nEquivalence of constrained and penalized formulations\nCase of ridge regression. Returning to the basic case (without feature space), we\nnow introduce an alternate formulation of ridge regression.\nLet ridge(λ) denote\nthe ridge regression problem that we have considered so far, for some parameter\n1Indeed, let u =\n \nw0\nw\n!\nwith w0 ∈R and w ∈RN be such that uT ( ˜KT ˜K + λK′)u = 0. This requires\n˜Ku = 0 and uT K′u = 0. The latter quantity is wT Kw, which shows that w = 0 since K has rank N.\nThen ˜K = 1Nw0 so that w0 = 0 also.\n\n148\nCHAPTER 7. LINEAR REGRESSION\nλ. Consider now the following problem, which will be called ridge′(C): minimize\nPN\nk=1 |yk −˜xT\nk β|2 subject to the constraint βT ∆β ≤C. We claim that this problem is\nequivalent to the ridge regression problem, in the following sense: for any C, there\nexists a λ such that the solution of ridge′(C) coincides with the solution of ridge(λ)\nand vice-versa.\nIndeed, fix a C > 0. Consider an optimal β for ridge′(C). Assuming as above that ∆\nis symmetric positive semi-definite, we let V be its null space and PV the orthogonal\nprojection on V . Write β = β1 + β2 with β1 = PV β. Let d1 and d2 be the respective\ndimensions of V and V ⊥so that d1 + d2 = d. Identifying Rd with the product space\nV ×V ⊥(i.e., making a linear change of coordinates), the problem can be rewritten as\nthe minimization of\n|Y −X1β1 −X2β2|2\nsubject to βT\n2 ∆β2 ≤C, where X1 (resp. X2) is N × d1 (resp. N × d2).\nThe gradient of the constraint γ(β2) = βT\n2 ∆β2 −C is ∇γ(β2) = 2∆β2. Assume first\nthat ∆β2 , 0. Then the solution must satisfy the KKT conditions, which require that\nthere exists µ ≥0 such that β is a stationary point of the Lagrangian\n|Y −X1β1 −X2β2|2 + µβT\n2 ∆β2,\nwith µ > 0 only possible if βT ∆β = C. This requires that\nX T\n1 X1β1 + X T\n1 X2β2 = X T Y,\nX T\n2 X1β1 + X T\n2 X2β2 + µ∆β2 = X T Y.\nSince ∆β1 = 0, and using X = (X1,X2), we have\nβ = (X T X + µ∆)−1X T Y,\nwhich is the only solution of ridge(µ).\nIf ∆β2 = 0, then, necessarily, β2 = 0. Since C > 0, β must then be the solution of\nthe unconstrained problem, which is ridge(0).\nConversely, any solution β of ridge(λ) satisfies the first-order optimality condi-\ntions for ridge′(C) for C = βT ∆β (or any C ≥βT ∆β if λ = 0). This shows the equiva-\nlence of the two problems.\nGeneral case.\nWe now consider this equivalence in a more general setting. Con-\nsider a penalized optimization problem, denoted var(λ) which consists in minimiz-\ning in β some objective function of the form U(β) + λϕ(β),λ ≥0. Consider also\nthe family of problems var′(C), with C > inf(ϕ), which minimize U(β) subject to\nϕ(β) ≤C.\n\n7.2. RIDGE REGRESSION AND LASSO\n149\nWe make the following assumptions.\n(i) U and ϕ are continuous functions from Rn to R.\n(ii) ϕ(β) →∞when β →∞.\n(iii) For any λ ≥0, there is a unique solution of var(λ), denoted βλ.\n(iv) For any C, there is a unique solution of var′(C). denoted β′\nC.\nAssumptions (ii) and (iv) are true, in particular, when U is strictly convex, ϕ is\nconvex and U has compact level sets. We show that, with these assumptions, the\ntwo families of problems are equivalent.\nWe first discuss the penalized problems and prove the following proposition,\nwhich has its own interest.\nProposition 7.5 The function λ 7→U(βλ) is nondecreasing, and λ 7→ϕ(βλ) is non-\nincreasing, with\nlim\nλ→∞ϕ(βλ) = inf(ϕ).\nMoreover, βλ varies continuously as a function of λ.\nProof Consider two parameters λ and λ′. We have\nU(βλ) + λϕ(βλ)\n≤\nU(βλ′) + λϕ(βλ′)\nand U(βλ′) + λ′ϕ(βλ′)\n≤\nU(βλ) + λ′ϕ(βλ)\nsince both left-hand sides are minimizers. This implies\nλ(ϕ(βλ) −ϕ(βλ′)) ≤U(βλ′) −U(βλ) ≤λ′(ϕ(βλ) −ϕ(βλ′)).\n(7.6)\nIn particular: (λ′ −λ)(ϕ(βλ) −ϕ(βλ′)) ≥0. Assume that λ < λ′. Then this last\ninequality implies ϕ(βλ) ≥ϕ(βλ′) and (7.6) then implies that U(βλ) ≤U(βλ′), which\nproves the first part of the proposition.\nNow assume that there exists ϵ > 0 such that ϕ(βλ) > infϕ + ϵ for all λ ≥0. Take\n˜β such that ϕ( ˜β) ≤infϕ + ϵ/2. For any λ > 0, we have\nU(βλ) + λϕ(βλ) ≤U( ˜β) + λϕ( ˜β)\nso that U(βλ) < U( ˜β) −λϵ/2. Since U(βλ) ≥U(a0), we get U(a0) = −∞, which is a\ncontradiction. This shows that ϕ(βλ) tends to inf(ϕ) when λ tends to infinity.\nWe now prove that λ 7→βλ is continuous. Define G(β,λ) = U(β)+λϕ(β). Since we\nassume that ϕ(β) →∞when β →∞, and we have just proved that ϕ(βλ) ≤ϕ(a0) for\nany λ, we obtain the fact that the set (|βλ|,λ ≥0) is bounded, say by a constant B ≥0.\n\n150\nCHAPTER 7. LINEAR REGRESSION\nConsider a sequence λn that converges to λ. We want to prove that βλn →βλ, for\nwhich (because βλ is bounded) it suffices to show that if any subsequence of (βλn)\nconverges to some ˜β, then ˜β = βλ.\nSo, consider such a converging subsequence, that we will still denote by βλn for\nconvenience. Since G is continuous, one has G(βλn,λn) →G( ˜β,λ) when n tends to\ninfinity. Let us prove that G(βλ,λ) is continuous in λ. For any pair λ,λ′ and any β,\nwe have\nG(βλ′,λ′) ≤G(βλ,λ′) = G(βλ,λ) + (λ′ −λ)ϕ(βλ) ≤G(βλ,λ) + |λ′ −λ|ϕ(a0).\nThis yields, by symmetry, |G(βλ′,λ′)−G(βλ,λ)| ≤ϕ(a0)|λ−λ′|, proving the continuity\nin λ.\nSo we must have G( ˜β,λ) = G(βλ,λ). This implies that both ˜β and βλ are solutions\nof var(λ), so that βλ = ˜β because we assume that the solution is unique.\n■\nWe now prove that the classes of problems var(λ) and var′(C) are equivalent.\nFirst, βλ is a minimizer of U(β) subject to the constraint ϕ(β) ≤C, with C = ϕ(βλ).\nIndeed, if U(β) < U(βλ) for some β with ϕ(β) ≤ϕ(βλ), then U(β) + λϕ(β) < U(βλ) +\nλϕ(βλ) which is a contradiction. So βλ = β′\nϕ(βλ). Using the continuity of βλ and ϕ,\nthis proves the equivalence of the problems when C is in the interval (a,ϕ(a0)) where\na = limλ→∞ϕ(βλ) = inf(ϕ).\nSo, it remains to consider the case C > ϕ(a0). For such a C, the solution of var′(C)\nmust be a0 since it is a solution of the unconstrained problem, and satisfies the con-\nstraint.\n7.2.3\nLasso regression\nProblem statement\nAssume that the output variable is scalar, i.e., q = 1. Let ˆσ2(i)\nbe the empirical variance of the ith variable X(i). Then, the lasso estimator is defined\nas a minimizer of PN\nk=1(yk −˜xT\nk β)2 subject to the constraint Pd\ni=1 ˆσ(i)|β(i)| ≤C. Com-\npared to ridge regression, the sum of squares for β is simply replaced by a weighted\nsum of absolute values, but we will see that this change may significantly affect the\nnature of the solutions.\nAs we have just seen, the penalized formulation, minimizing\nN\nX\nk=1\n(yk −˜xT\nk β)2 + λ\nd\nX\ni=1\nˆσ(i)|β(i)|\nprovides an equivalent family of problems, on which we will focus (because it is\neasier to analyze). Since one uses a non-Euclidean norm in the penalty, there is no\n\n7.2. RIDGE REGRESSION AND LASSO\n151\nkernel version of the lasso and we only discuss the method in the original input\nspace R = Rd.\nFor a vector a ∈Rk, we let |a|1 = |a(1)| + ··· + |a(k)|, the ℓ1 norm of a. Using the\nprevious notation for Y and X , the quantity to minimize can be rewritten as\n|Y −X β|2 + λ|Dβ|1\nwhere D is the d × (d + 1) matrix with d(i,i + 1) = ˆσ(i) for i = 1,...,d and all other\ncoefficients equal to 0. This is a convex optimization problem which, unlike ridge\nregression, does not have a closed form solution.\nADMM.\nThe alternating direction method of multipliers (ADMM) that was de-\nscribed in section 3.6, (3.59) is one of the state-of-the-art algorithm to solve the lasso\nproblem, especially in large dimensions. Other iterative methods include subgradi-\nent descent (see the example in section 3.5.4) and proximal gradient descent. Since\nx has a different meaning here, we change the notation in (3.59) by replacing x,z,u\nby β,γ,τ, and rewrite the lasso problem as the minimization of\n|Y −X β|2 + λ|γ|1\nsubject to Dβ −γ = 0. Applying (3.59) with A = D, B = −Id and c = 0, the ADMM\niterations are\n\nβ(n + 1)\n= argminβ\n \n|Y −X β|2 + 1\n2ρ|Dβ −γ(n) + τ(n)|2\n!\nγ(i)(n + 1)\n= argmint\n \nλ|t| + 1\n2ρ(t −Dβ(i)(n + 1) −τ(i)(n))2\n!\n, i = 1,...,d\nτ(n + 1)\n= τ(n) + Dβ(n + 1) −γ(n + 1)\nThe solutions of both minimization problems are explicit, yielding the following\nalgorithm, which converges to a solution if ρ is small enough.\nAlgorithm 7.1 (ADMM for lasso)\nLet ρ > 0 be chosen. Starting with initial values β(0), γ(0) and τ(0), the ADMM algo-\nrithm for lasso iterates:\n\nβ(n + 1)\n=\n \nX T X + DT D\n2ρ\n!−1  \nX T Y + DT\n2ρ (γ(n) −τ(n))\n!\nγ(i)(n + 1)\n= Sλρ\n\u0010\nDβ(i)(n + 1) + τ(i)(n)\n\u0011\n, i = 1,...,d\nτ(n + 1)\n= τ(n) + Dβ(n + 1) −γ(n + 1)\n\n152\nCHAPTER 7. LINEAR REGRESSION\nuntil the difference between the variables at steps n and n + 1 is below a small toler-\nance level. Here, Sλρ is the so-called shrinkage operator\nSλρ(v) =\n\nv −λρ\nif v ≥λρ\n0\nif |v| ≤λρ\nv + λρ\nif v ≤−λρ\nNote that the ADMM algorithm makes an iterative approximation of the constraints,\nso that they are only satisfied at some precision level when the algorithm is stopped.\nExact computation.\nWe now provide a more detailed characterization of the solu-\ntion of the lasso problem and analyze, in particular, how this solution changes when\nλ (or C) varies. To simplify the exposition, and without loss of generality, we will as-\nsume that the variables have been normalized so that ˆσ(i) = 1 and the penalty simply\nis the sum of absolute values. Let\nGλ(β) =\nN\nX\nk=1\n(yk −a0 −xT\nk b)2 + λ\nd\nX\ni=1\n|b(i)|.\nThe following proposition, in which we let\nrb = 1\nN\nN\nX\nk=1\n(yk −a0 −xT\nk b)xk,\ncharacterizes the solution of the lasso.\nProposition 7.6 The pair (a0,b) is the optimal solution of the lasso problem with param-\neter λ if and only if a0 = ¯y −¯xT b and, for all i = 1,...,d,\n|r(i)\nb | ≤λ\n2N\n(7.7)\nwith\nr(i)\nb = sign(b(i)) λ\n2N if b(i) , 0.\n(7.8)\nIn particular |r(i)\nb | < λ/(2N) implies b(i) = 0.\nProof Using the subdifferential calculus in theorem 3.45, one can compute the sub-\ngradients of G by adding the subdifferentials of the terms that compose it. All these\nterms are differentiable except |b(i)| when b(i) = 0, and the subdifferential of t 7→|t| at\nt = 0 is the interval [−1,1].\n\n7.2. RIDGE REGRESSION AND LASSO\n153\nThis shows that g ∈∂Gλ(β) if and only if\ng = −2Nrb + λz\nwith z(i) = sign(b(i)) if b(i) , 0 and |z(i)| ≤1 otherwise. Proposition 7.6 immediately\nfollows by taking g = 0.\n■\nLet ζ = sign(b), the vector formed by the signs of the coordinates of b, with\nsign(0) = 0. Then proposition 7.6 uniquely specifies a0 and b once λ and ζ are\nknown. Indeed, let J = Jζ denote the ordered subset of indices j ∈{1,...,d} such\nthat ζ(j) , 0, and let b(J), xk(J), ζ(J), etc., denote the restrictions of vectors to these\nindices. Equation (7.8) can be rewritten as (after replacing a0 by its optimal value)\nXc(J)T Xc(J)b(J) = Xc(J)T Yc −λ\n2ζ(J)\nwhere\nXc(J) =\n\n\n(x1(J) −x(J))T\n...\n(xN(J) −x(J))T\n\n\n.\nThis yields\nb(J) = (Xc(J)T Xc(J))−1\u0010\nXc(J)T Yc −λ\n2ζ(J)\n\u0011\n,\n(7.9)\nwhich fully determine b since b(j) = 0 if j < J, by definition.\nFor given λ, only one sign configuration ζ will provide the correct solution, with\ncorrect signs for nonzero values of b above, and correct inequalities on rb. Call-\ning this configuration ζλ, one can note that if ζλ is known for a given value of λ,\nit remains valid if we increase or decrease λ until one of the optimality conditions\nchanges, i.e., either one of the coordinates b(i),i ∈Jζλ, vanishes, or one of the inequal-\nities for i < Jζλ becomes an equality. Moreover, proposition 7.6 shows that between\nthese events both b and therefore rb depend linearly on λ, which makes easy the\ntask of determining maximal intervals around a given λ over which ζ remains un-\nchanged.\nNote that solutions are known for λ = 0 (standard least squares) and for λ large\nenough (for which b = 0). Indeed, for b = 0 to be a solution, it suffices that\nλ > λ0\n∆= 2max\ni\n\f\f\f\f\f\f\f\nN\nX\nk=1\n(yk −y)(x(i)\nk −x(i))\n\f\f\f\f\f\f\f\n.\nThese remarks set the stage for an algorithm computing the optimal solution of\nthe lasso problem for all values of λ, starting either from λ = 0 or λ > λ0. We will de-\nscribe this algorithm starting for λ > λ0, which has the merit to avoid complications\n\n154\nCHAPTER 7. LINEAR REGRESSION\ndue to underconstrained least squares when d is large. For this purpose, we need a\nlittle more notation. For a given ζ, let\nbζ = (Xc(Jζ)T Xc(Jζ))−1Xc(Jζ)T Yc\nand\nuζ = 1\n2(Xc(Jζ)T Xc(Jζ))−1ζ(Jζ),\nso that b(Jζ) = bζ −λuζ. The residuals then take the form\nr(i)\nb = 1\nN\nN\nX\nk=1\n(yk −a0 −bT\nζ xk)x(i)\nk + λ\nN\nN\nX\nk=1\n(xT\nk uζ)(x(i)\nk −x(i))\n= ρ(i)\nζ + λd(i)\nζ ,\nwhere the last equation defines ρζ and dζ.\nAssume that one wants to minimize Gλ∗for some λ∗> 0. We need to describe\nthe sequence of changes to the minimizers of Gλ when λ decreases from some value\nlarger than λ0 to the value λ∗.\nIf λ∗≥λ0, then the optimal solution is b = 0, so we can assume that λ∗< λ0.\nWhen λ is slightly smaller than λ0, one needs to introduce some non-zero values in\nζ. Those values are at the indexes i such that\nλ0 = 2\n\f\f\f\f\f\f\nN\nX\nk=1\n(yk −y)(x(i)\nk −x(i))\n\f\f\f\f\f\f\nThe sign of ζ(i) is also determined since sign(b(i)) = sign(r(i)\nb ) when b(i) , 0.\nThe algorithm will then continue by progressively adding non-zero entries to ζ\nwhen the covariance between some unused variables and the residual becomes too\nlarge, or by removing non-zero values when the optimal b crosses a zero. We now\ndescribe it in detail.\nAlgorithm 7.2 (Exact minimization for lasso)\n1. Initialization: let λ(0) = 1 + λ0, σ(0) = 0 and the corresponding values a0(0) = y\nand b(0) = 0.\n2. Assume that the algorithm has reached step n with current variables λ(n), σ(n),\na0(n) and b(n).\n3. Determine the first λ′ < λ(n) for which either\n(i) For some i, ζ(i)(n) , 0 and b(i)\nζ(n) −λ′u(i)\nζ(n) = 0.\n\n7.3. OTHER SPARSITY ESTIMATORS\n155\n(ii) For some i, ζ(i)(n) = 0 and (1 −2Nd(i)\nζ(n))λ′ −2Nρζ(n) = 0.\n(iii) For some i, ζ(i)(n) = 0 and (1 + 2Nd(i)\nζ(n))λ′ + 2Nρζ(n) = 0.\n4. Then, there are two cases:\n(a) If λ′ ≥λ∗, set λ(n +1) = λ′. Let ζ(i)(n +1) = ζ(i)(n) if i does not satisfy (i), (ii) or\n(iii). If i is in case (i), set ζ(i)(n+1) = 0. For i in case (ii) (resp. (iii)), set ζ(i)(n+1) = 1\n(resp. −1).\n(b) If λ′ < λ∗, terminate the algorithm without updating ζ and set\nb(i) = b(i)\nζ(n) −λ∗u(i)\nζ(n),\nζ(i)(n) , 0\nand a0 = ¯y −bT ¯x to obtain the final solution.\n7.3\nOther Sparsity Estimators\n7.3.1\nLARS estimator\nAlgorithm.\nThe LARS algorithm can be seen as a simplification of the previous\nlasso algorithm in which one always adds active variables at each step. We assume\nas above that input variables are normalized such that ˆσ(i) = 1.\nGiven a current set J of selected variables, the algorithm will decide either to stop\nor to add a new variable to J according to a criterion that depends on a parameter\nλ > 0. Let b(J) ∈R|J| be the least-square estimator based on variables in J\nb(J) = (Xc(J)T Xc(J))−1Xc(J)T Yc.\nLet bJ ∈Rd such that b(i)\nJ\n= b(i)\n(J) for i ∈J and 0 otherwise. The covariances between\nthe remaining variables and the residuals are given by\nr(i)\nJ\n= 1\nN\nN\nX\nk=1\n(yk −y −(xk −x)T bJ)(x(i)\nk −x(i)),\ni < J.\nIf, for all i ∈J, |r(i)\nJ | ≤\n√\nλ/N, the procedure is stopped. Otherwise, one adds to J the\nvariable i such that |r(i)\nJ | is largest and continues.\n\n156\nCHAPTER 7. LINEAR REGRESSION\nJustification.\nRecall the notation |b|0 for the number of non-zero entries of b. Con-\nsider the objective function\nL(b) = |Yc −Xcb|2 + λ|b|0.\nLet J be the set currently selected by the algorithm, and bJ defined as above. We\nconsider the problem of adding one non-zero entry to b. Fix i < J, and let ˜b ∈Rd\nhave all coordinates equalt to those of bJ for all except the ith one, which is therefore\nallowed to be non-zero. Then\nL(˜b) =\nN\nX\nk=1\n\u0012\nyk −y −\nX\nj∈J\n(x(j)\nk −x)b(j) −(x(i)\nk −x)˜b(i)\u00132\n+ λ|J| + λ,\nso that (using ˆσ(i) = 1)\nL(˜b) = L(bJ) −2Nr(i)\nJ ˜b(i) + N(˜b(i))2 + λ\nNow, L(˜b) is an upper-bound for L(bJ∪{i}), and so is its minimum with respect to ˜b(i).\nThis yields:\nL(bJ∪{i}) ≤L(bJ) −N(r(i)\nJ )2 + λ\nThe LARS algorithm therefore finds the value of i that minimizes this upper-bound,\nprovided that the resulting minimum is less that L(bJ).\nVariant.\nThe same argument can be made with |b|0 replaced by |b|1 and one gets\nL(˜b) = L(bJ) −2Nr(i)\nJ ˜b(i) + N(˜b(i))2 + λ|˜b(i)|\nMinimizing this expression with respect to ˜b(i) yields the upper bound:\nL(bJ∪{i}) ≤\n\nL(bJ) −N\n\u0010\n|r(i)\nJ | −λ\n2N\n\u00112\nif |r(i)\nJ | ≥λ\n2N\nL(bJ)\nif |r(i)\nJ | ≤λ\n2N\nThis leads to the following alternate form of LARS. Given a current set J of se-\nlected variables, compute\nr(i)\nJ\n= 1\nN\nN\nX\nk=1\n(yk −y −(xk −x)T bJ)(x(i)\nk −x(i)),\ni < J .\nIf, for all i < J, |r(i)\nJ | ≤λ/2N, stop the procedure. Otherwise, add to J the variable i\nsuch that |r(i)\nJ | is largest and continue. This form tends to add more variables since\nthe stopping criterion decreases in 1/N instead of 1/\n√\nN.\n\n7.3. OTHER SPARSITY ESTIMATORS\n157\nWhy “least angle”?\nLet µJ,k = yk−y−(xk−x)T bJ denote the residual after regression.\nThe empirical correlation between µ and x(i) is equal to the cosine of the angle, say\nθ(i)\nJ\nbetween µJ ∈RN and x(i)−x both considered as vectors in RN. This cosine is also\nequal to\ncosθ(i)\nJ\n=\nµT\nJ (x(i) −x(i))\n|x(i) −x(i)||µJ|\n=\n√\nN\nr(i)\nJ\n|µJ|\nwhere we have used the fact that |x(i) −x(i)|/\n√\nN = ˆσ(i) = 1. Since |µJ| does not depend\non i, looking for the largest value of |r(i)\nJ | is equivalent to looking for the smallest\nvalue of |θ(i)\nJ |, so that we are looking for the unselected variable for which the angle\nwith the current residual is minimal.\n7.3.2\nThe Dantzig selector\nNoise-free case.\nAssume that one wants to solve the equation X β = Y when the\ndimension, N, of Y is small compared to number of columns, d, in X . Since the\nsystem is under-determined, one needs additional constraints on β and a natural one\nis to look for sparse solutions, i.e., find solutions with a maximum number of zero\ncoefficients. However, this is numerically challenging, and it is easier to minimize\nthe ℓ1 norm of β instead (as seen when discussing the lasso, using this norm often\nprovides sparse solutions). In the following, we assume that the empirical variance\nof each variable is normalized, so that, denoting X (i) the ith column of X , we have\n|X (i)| = 1.\nThe Dantzig selector [47] minimizes\nd\nX\ni=1\n|β(i)|\nsubject to the constraint X β = Y. This results in a linear program (therefore easy to\nimplement). More precisely, introducing slack variables, it is indeed equivalent to\nminimize\nd\nX\ni=1\nξ(i) +\nd\nX\ni=1\nξ∗(i)\nsubject to constraints ξ(i) ≥β(i), ξ(i)∗≥−β(i), ξ(i) ≥0, ξ∗(i) ≥0 and X β = Y.\nSparsity recovery\nUnder some assumptions, this method does recover sparse solu-\ntions when they exist. More precisely, let ˆβ be the solution of the linear programming\nproblem above. Assume that there is a set J∗⊂{1,...,d} such that X β = Y for some\n\n158\nCHAPTER 7. LINEAR REGRESSION\nβ ∈Rd with β(i) = 0 if i < J∗. Conditions under which ˆβ is equal to β are provided\nin Candes and Tao [47] and involve the correlations between pairs of columns of X ,\nand the size of J.\nThat the size of J∗must be a factor is clear, since, for the statement to make sense,\nthere cannot exist two β’s satisfying X β = Y and β(i) = 0 for i < J∗. Uniqueness is\nobviously not true if |J| > N, because, even if one knew J, the condition would be\nunder-constrained for β. Since the set J∗is not known, and we also want to avoid\nany other solution associated to a set of same size. So, there cannot exist β and ˜β\nrespectively vanishing outside of J∗and ˜J∗, where J∗and ˜J∗have same cardinality,\nsuch that X β = Y = X ˜β. The equation X (β −˜β) = 0 would be under-constrained as\nsoon as the number of non-zero coefficients of β −˜β is larger than N, and since this\nnumber can be as large as |J∗| + |˜J∗| = 2|J∗|, we see that one should impose at least\n|J∗| ≤N/2.\nGiven this restriction, another obvious remark is that, if the set J on which β does\nnot vanish is known, with |J| small enough, then X β = Y is over-constrained and any\nsolution is (typically) unique. So the issue really is whether the set Jβ listing the\nnon-zero indexes of a solution β is equal to y J∗.\nAs often, precious insight on the solution of this minimization problem is ob-\ntained by considering the dual problem. Introducing Lagrange multipliers λ(i) ≥\n0,i = 1,...,d for the constraints ξ(i) −β(i) ≥0, λ∗(i) ≥0, i = 1,...,d for ξ∗(i) + β(i) ≥0,\nγ(i),γ∗(i) ≥0 for ξ(i) ≥0 and ξ∗(i) ≥0, and α ∈RN for X β = Y, the Lagrangian is\nL(β,ξ,λ,λ∗,α) = (1d −λ −γ)T ξ + (1d −λ∗−γ∗)T ξ∗+ (λ −λ∗+ X T α)T β −αT Y.\nThe KKT conditions require γ = 1d −λ, γ∗= 1d −λ∗, X α = λ∗−λ and the comple-\nmentary slackness conditions give (1 −λ(i))ξ(i) = (1 −λ∗\ni)ξ∗(i) = 0, λ(i)(β(i) −ξ(i)) =\nλ∗(i)(β(i) + ξ∗(i)) = 0.\nThe dual problem requires to minimize αT Y subject to the constraints X T α =\nλ∗−λ and 0 ≤λ(i),λ∗(i) ≤1. Assume that (α,λ,λ∗) is a solution of this dual problem.\nOne has the following cases.\n(1) If λ(i) ∈(0,1), then ξ(i) = β(i) −ξ(i) = 0, which implies ξ(i) = β(i) = 0, and, as a\nconsequence (1 −λ∗(i))ξ∗(i) = λ∗(i)ξ∗(i) = 0, so that also ξ∗(i) = 0 .\n(2) Similarly, λ∗(i) ∈(0,1) implies ξ(i) = ξ∗(i) = β(i) = 0.\n(3) If λ(i) = λ∗(i) = 1, then β(i) −ξ(i) = β(i) + ξ(i) = 0 with ξ(i),ξ∗(i) ≥0, so that also\nξ(i) = ξ∗(i) = β(i) = 0.\n(4) If λ(i) = λ∗(i) = 0, then ξ(i) = ξ∗(i) = 0 and since β(i) ≤ξ(i) and β(i) ≤−ξ∗(i), we\nget β(i) = 0.\n\n7.3. OTHER SPARSITY ESTIMATORS\n159\n(5) The only remaining situation, in which β(i) can be non-zero, is when λ(i) = 1 −\nλ∗(i) ∈{0,1}, or, equivalently, when |λ(i) −λ∗(i)| = 1.\nThis discussion allows one to reconstruct the set Jβ associated with the primal prob-\nlem given the solution of the dual problem. Note that |λ(i) −λ∗(i)| = |αT X (i)|, so that\nthe set of indexes with |λ(i) −λ∗(i)| = 1 is also\nIα\n∆=\nn\ni : |αT X (i)| = 1\no\n.\nOne has\nαT Y = αT X β =\nd\nX\ni=1\nβ(i)αT X (i) ≤\nX\ni∈Jβ\n|β(i)||αT X (i)| ≤\nX\ni∈Jβ\n|β(i)|.\nThe upper-bound is achieved when αT X (i) = sign(β(i)) for i ∈Jβ. So, if a vector α can\nbe found such that\n(i) αT X (i) = sign(β(i)) for i ∈J∗,\n(ii) |αT X (j)| < 1 for j < J∗,\nthen it is a solution of the dual problem with Jα = J∗.\nLet sJ = (s(j),j ∈J) be defined by s(j) = sign(β(j)). One can always decompose\nα ∈RN in the form\nα = XJ∗ρ + w\nwhere ρ ∈R|J∗| and w ∈RN is perpendicular to the columns of XJ∗. From X T\nJ∗α = sJ,\nwe get\nρ = (X T\nJ∗XJ∗)−1sJ∗.\nLetting αJ∗be the solution with w = 0, the question is therefore whether one can find\nw such that\n(\nwT X (j) = 0,\nj ∈J∗\n|αT\nJ X (k) + wT X (k)| < 1,\nk < J∗\nDenote for short ΣJJ′ = X T\nJ XJ′. One can show that such a solution exists when\nthe matrices ΣJJ are close to the identity as soon as |J| is small enough [47]. More\nprecisely, denote, for q ≤d\nδ(q) = max\n|J|≤q max(∥ΣJJ∥,∥Σ−1\nJJ ∥−1) −1,\n\n160\nCHAPTER 7. LINEAR REGRESSION\nin which one uses the operator norm on matrices, and\nθ(q,q′) = max\nn\nzT ΣTT ′z′ : |J|,|J′| ≤q.J ∩J′ = ∅,|z| = |z′| = 1\no\n.\nThen, the following proposition is true.\nProposition 7.7 (Candes-Tao) Let q = |J∗| and s = (s(j),j ∈J∗) ∈Rq.\nAssume that\nδ(2q) + θ(q,2q) < 1. Then there exists α ∈RN such that αT X (j) = s(j) for j ∈J∗and\n|αT X (j)| ≤\nθ(q,q)\n1 −δ(2q) −θ(q,2q) if j < J∗.\nSo α has the desired property as soon as δ(2q) + θ(q,q) + θ(q,2q) ≤1. to control\nsubsets of variables of size less than 3q to obtain the conclusion, which is important,\nof course, when q is small compared to d.\nNoisy case\nConsider now the noisy case. We here again introduce quantities that\nwere pivotal for the lasso and LARS estimators, namely, the covariances between the\nvariables and the residual error. So, we define, for a given β\nr(i)\nβ = X (i)T (Y −X β)\nwhich depends linearly on β. Then, the Dantzig selector is defined by the linear\nprogram: Minimize:\nd\nX\nj=1\n|β(j)|\nsubject to the constraint:\nmax\nj=1,...,d |r(j)\nβ | ≤C.\nThe explicit expression of this problem as a linear program is obtained as before by\nintroducing slack variables ξ(j),ξ∗(j),j = 1,...,d and minimizing\nd\nX\nj=1\nξ(j) +\nd\nX\nj=1\nξ∗(j)\nwith constraints ξ(j),ξ∗(j) ≥0, ξ ≥β, ξ∗≥−β, max\nj=1,...,d |r(j)\nβ | ≤C.\nSimilar to the noise-free case, the Dantzig selector can identify sparse solutions\n(up to a small error) if the columns of X are nearly orthogonal, with the same type of\nconditions [48]. Interestingly enough, the accuracy of this algorithm can be proved\nto be comparable to that of the lasso in the presence of a sparse solution [30].\n\n7.4. SUPPORT VECTOR MACHINES FOR REGRESSION\n161\n7.4\nSupport Vector Machines for regression\n7.4.1\nLinear SVM\nProblem formulation\nWe start by discussing support vector machines (SVM) [196,\n197] with RX = Rd equipped with the standard inner product (generally referred to\nas linear SVM) and will extend the theory to kernel methods in the next section.\nSVMs solve a linear regression problem, but replace the least-squares loss function\nby (y,y′) 7→V (y −y′) with\nV (t) =\n(0\nif |t| < ϵ\n|t| −ϵ\nif |t| ≥ϵ\n(7.10)\nFigure 7.1: The function V defining the SVM risk function.\nA plot of the function V is provided in fig. 7.1. This function is an example\nof what is often called a robust loss function. The quadratic error used in linear\nregression had the advantage of providing closed form expressions for the solution,\nbut is quite sensitive to outliers. For robustness, it is preferable to use loss functions\nthat, like V , increase at most linearly at infinity. One sometimes choose them as\nsmooth convex functions, for example V (t) = (1 −cosγt)/(1 −cosγϵ) for |t| < ϵ and\nf (t) = |t| for t ≥ϵ, where γ is chosen so that γϵsinγϵ/(1 −cosγϵ) = 1. In such a case,\nminimizing\nF(β) =\nN\nX\nk=1\nV (yk −a0 −xT\nk b)\ncan be done using gradient descent methods. Using V in (7.10) will require a little\nmore work, as we see now.\n\n162\nCHAPTER 7. LINEAR REGRESSION\nThe SVM regression problem is generally formulated as the minimization of\nN\nX\nk=1\nV (yk −a0 −xT\nk b) + λ|b|2 ,\nand we will study a slightly more general problem, minimizing\nF(a0,b) =\nN\nX\nk=1\nV (yk −a0 −xT\nk b) + λbT ∆b,\nwhere ∆is a symmetric positive-definite matrix. This objective function exhibits the\nfollowing features:\n• A penalty on the coefficients of b, similar to ridge regression.\n• A linear penalty (instead of quadratic) for large errors in the prediction.\n• An ϵ-tolerance for small errors, often referred to as the margin of the regression\nSVM.\nWe now describe the various steps in the analysis and reduction of the problem.\nThey will lead to simple minimization algorithms, and possible extensions to non-\nlinear problems.\nReduction to a quadratic programming problem\nIntroduce slack variables ξk,ξ∗\nk,k =\n1,...,N. The original problem is equivalent to the minimization, with respect to\n(a0,b,ξ,ξ∗), of\nN\nX\nk=1\n(ξk + ξ∗\nk) + λbT ∆b\nunder the constraints:\n\nξk,ξ∗\nk ≥0\nξk −yk + a0 + xT\nk b + ϵ ≥0\nξ∗\nk + yk −a0 −xT\nk b + ϵ ≥0\n(7.11)\nThe simple proof of this equivalence, which results in a quadratic programming\nproblem, is left to the reader. As often, one gains additional insight by studying the\ndual problem.\nDual problem\nIntroduce 4N non-negative Lagrange multipliers for the 4N con-\nstraints in the problem, namely, ηk,η∗\nk ≥0 for the positivity constraints, and αk,α∗\nk ≥\n\n7.4. SUPPORT VECTOR MACHINES FOR REGRESSION\n163\n0 for the last two in (7.11). The resulting Lagrangian is\nL(a0,b,ξ,ξ∗,α,α∗,η,η∗) =\nN\nX\nk=1\n(ξk + ξ∗\nk) + λbT ∆b −\nN\nX\nk=1\n(ηkξk + η∗\nkξ∗\nk)\n−\nN\nX\nk=1\nαk(ξk −yk + a0 + xT\nk b + ϵ) −\nN\nX\nk=1\nα∗\nk(ξ∗\nk + yk −a0 −xT\nk b + ϵ).\nIn this formulation, (a0,b,ξ,ξ∗) are the primal variables, and α,α∗,η,η∗the dual vari-\nables.\nThe KKT conditions are provided by the system:\n\nN\nX\nk=1\n(αk −α∗\nk) = 0\n2λ∆b −\nN\nX\nk=1\n(αk −α∗\nk)xk = 0\n1 −ηk −αk = 0\n1 −η∗\nk −α∗\nk = 0\nαk(ϵ + ξk −yk + a0 + xT\nk b) = 0\nα∗\nk(ϵ + ξ∗\nk + yk −a0 −xT\nk b) = 0\nηkξk = η∗\nkξ∗\nk = 0\n(7.12)\nThe first four equations are the derivatives of the Lagrangian with respect to a0,b,ξk,ξ∗\nk\nin this order and the last three are the complementary slackness conditions.\nThe dual problem maximizes the function\nL∗(α,α∗,η,η∗) = inf\nβ,ξ,ξ∗L.\nunder the previous positivity constraints. Since the Lagrangian is linear in a0, ξk\nand ξ∗\nk, its minimum is −∞unless the coefficients vanish. The linear terms must\ntherefore vanish for L∗to be finite. With these conditions plus the fact that ∂bL = 0,\nwe retrieve the first four equations of system (7.12). Using ηk = 1 −αk, η∗\nk = 1 −α∗\nk\nand\nb = 1\n2λ\nN\nX\nk=1\n(αk −α∗\nk)∆−1xk\n(7.13)\none can express L∗uniquely as a function of α,α∗, yielding\nL∗(α,α∗) = −1\n4λ\nN\nX\nk,l=1\n(αk −α∗\nk)(αl −α∗\nl )xT\nk ∆−1xl −ϵ\nN\nX\nk=1\n(αk + α∗\nk) +\nN\nX\nk=1\n(αk −α∗\nk)yk .\n\n164\nCHAPTER 7. LINEAR REGRESSION\nThis quantity must be maximized subject to the constraints 0 ≤αk,α∗\nk ≤1 and\nPN\nk=1(αk −α∗\nk) = 0. This still is a quadratic programming problem, but it now has\nnice additional features and interpretations.\nStep 3: Analysis of the dual problem\nThe dual problem only depends on the xk’s\nthrough the matrix with coefficients xT\nk ∆−1xl, which is the Gram matrix of x1,...,xN\nfor the inner product associated with ∆−1. This property will lead to the the kernel\nversion of SVMs discussed in the next section. The obtained predictor can also be\nexpressed as a function of these products, since\ny = a0 + xT b = a0 + 1\n2λ\nN\nX\nk=1\n(αk −α∗\nk)(xT\nk ∆−1x).\nMoreover, the dimension of the dual problem is 2N, which allows the method to be\nused in large (possibly infinite) dimensions with a bounded cost.\nWe now analyze the solutions α,α∗of the dual problem. The complementary\nslackness conditions reduce to:\n\nαk(ϵ + ξk −yk + a0 + xT\nk b) = 0\nα∗\nk(ϵ + ξ∗\nk + yk −a0 −xT\nk b) = 0\n(1 −αk)ξk = (1 −α∗\nk)ξ∗\nk = 0\n(7.14)\nThese conditions have the following consequences, based on the prediction error\nmade for each training sample.\n(i) First consider indexes k such that the error is strictly within the tolerance mar-\ngin ϵ: |yk −a0 −xT\nk b| < ϵ. Then the terms between parentheses in first two equations\nof (7.14) are strictly positive, which implies that αk = α∗\nk = 0. The last two equations\nin (7.14) then imply ξk = ξ∗\nk = 0.\n(ii) Consider now the case when the prediction is strictly less accurate than the\ntolerance margin. Assume that yk −a0 −xT\nk b > ϵ. The second and third equations in\n(7.14) imply that α∗\nk = ξ∗\nk = 0. The assumption also implies that\nξk = yk −a0 −xT\nk b −ϵ > 0\nand αk = 1. The case yk −a0 −xT\nk b < −ϵ is symmetric and provides αk = ξk = 0, ξ∗\nk > 0\nand α∗\nk = 1.\n(iii) Finally, consider samples for which the prediction error is exactly at the toler-\nance margin. If yk −a0 −xT\nk b = ϵ, we have α∗\nk = ξk = ξ∗\nk = 0. The fact that α∗\nk = ξ∗\nk = 0 is\nclear. To prove that ξk = 0, we note that would have otherwise ξk−yk+a0+xT\nk b+ϵ > 0,\nwhich would imply that αk = 0 and we reach a contradiction with (1−αk)ξk = 0. Sim-\nilarly, yk −a0 −xT\nk b = −ϵ implies that αk = ξk = ξ∗\nk = 0.\nThe points for which |yk −a0 −xT\nk b| = ϵ are called support vectors.\n\n7.4. SUPPORT VECTOR MACHINES FOR REGRESSION\n165\nOne important information deriving from this discussion is that the variables\n(αk,α∗\nk) have prescribed values as long as the error yk −a0 −xT\nk b is not exactly ϵ in\nabsolute value: (1,0) if the error is larger than ϵ, (0,0) if it is strictly between −ϵ and ϵ\nand (0,1) if it is less than −ϵ. Also in all cases, at least one of αk and α∗\nk must vanish.\nOnly in the case of support vectors does the previous discussion fail to provide a\nvalue for one of these variables.\nNow, we want to reverse the discussion and assume that the dual problem is\nsolved to see how the variables a0 and b of the primal problem can be retrieved. For\nb, this is easy, thanks to (7.13). For a0 a direct computation can be made if a support\nvector is identified, either because 0 < αk < 1, which implies that a0 = yk −xT\nk b −ϵ, or\nbecause 0 < α∗\nk < 1, which yields a0 = yk −xT\nk b + ϵ.\nIf no support vector can be identified, a0 is not uniquely determined (note that\nthe objective function is not strictly convex in a0). However, the coefficients αk,α∗\nk\nprovide some information on this intercept, in the form of inequalities. More pre-\ncisely, let J+ = {k : αk = 1}, J−= {k : α∗\nk = 1} and J0 = {k : αk = α∗\nk = 0}. Then k ∈J+\nimplies that yk −a0 −bT xk ≥ϵ, so that a0 ≤yk −bT xk −ϵ. Similarly, k ∈J−implies that\na0 ≥yk −bT xk + ϵ. Finally, k ∈J0 implies that a0 ≥yk −bT xk −ϵ and a0 ≤yk −bT xk + ϵ.\nAs a consequence, one can take a0 to be any point in the interval [a0−,a0+], where\na0\n−= max\n \nmax\nk∈J−(yk −xT\nk b + ϵ),max\nk∈J0\n(yk −xT\nk b −ϵ)\n!\na0\n+ = min\n \nmin\nk∈J+ (yk −xT\nk b −ϵ),min\nk∈J0\n(yk −xT\nk b + ϵ)\n!\n.\n7.4.2\nThe kernel trick and SVMs\nReturning to our feature space notation, let X take values in RX and h : RX →H be\na feature function with values in an inner-product space H with associated kernel K.\nSVMs in feature space must minimize, with a0 ∈R and b ∈H\nF(a0,b) =\nN\nX\nk=1\nV\n\u0010\nyk −a0 −⟨h(xk) , b⟩H\n\u0011\n+ λ∥b∥2\nH .\nLetting as before V = span(h(x1),...,h(xN)), the same argument as that made for\nridge regression works, namely that the first term in F is unchanged if b is replaced\nby πV (b) and the second one is strictly reduced unless b ∈V , leading to a finite-\ndimensional formulation in which\nb =\nN\nX\nk=1\nckh(xk)\n\n166\nCHAPTER 7. LINEAR REGRESSION\nand one minimizes\nF(a0,c) =\nN\nX\nk=1\nV\n\u0012\nyk −a0 −\nN\nX\nl=1\nK(xk,xl)cl\n\u0013\n+ λ\nN\nX\nk,l=1\nK(xk,xl)ckcl .\nThis function has the same form as the one studied in the linear case with b replaced\nby c ∈RN, xk replaced by the vector with coefficients K(xk,xl),l = 1,...,N, that we\nwill denote K(k) and ∆= K = K(x1,...,xN). Note that K(k) is the kth column of K, so\nthat\n\u0010\nK(k)\u0011T K−1K(l) = K(xk,xl).\nUsing this, we find that the dual problem requires to maximize\nL∗(α,α∗) = −1\n4λ\nN\nX\nk,l=1\n(αk −α∗\nk)(αl −α∗\nl )K(xk,xl) −ϵ\nN\nX\nk=1\n(αk + α∗\nk) +\nN\nX\nk=1\n(αk −α∗\nk)yk .\nwith\n\n0 ≤αk ≤1\n0 ≤α∗\nk ≤1\nN\nX\nk=1\n(αk −α∗\nk) = 0\nThe associated vector c satisfies\n2λc =\nN\nX\nk=1\n(αk −α∗\nk)K−1K(k) = α −α∗.\nand the regression function is\nf (x) = a0 + ⟨b , h(x)⟩H = a0 + 1\n2λ\nN\nX\nk=1\n(αk −α∗\nk)K(x,xk).\nFinally, the discussions on the values of α,α∗and on the computation of a0 remain\nunchanged.\n\nChapter 8\nModels for linear classification\nIn this chapter, Y is categorical and takes values in the finite set RY =\nn\ng1,...,gq\no\n.\nThe goal is to predict this class variable from the input X, taking values in a set RX.\nUsing the same progression as in the regression case, we will first discuss basic linear\nmethods, for which RX = Rd before extending them, whenever possible, to kernel\nmethods, for which RX can be arbitrary as soon as a feature space representation is\navailable.\nClassifiers will be based on a training set T = ((x1,y1),...,(xN,yN)) with xk ∈RX\nand yk ∈RY for k = 1,...,N. For g ∈RY, we will also let Ng denote the number of\nsamples in the training set such that yk = g, i.e.,\nNg =\n\f\f\f{k : yk = g}\n\f\f\f =\nN\nX\nk=1\n1yk=g.\n8.1\nLogistic regression\n8.1.1\nGeneral Framework\nLogistic regression uses the fact that, in order to apply Bayes’s rule, only the condi-\ntional distribution of the class variables Y given X is needed, and trains a parametric\nmodel of this distribution. More precisely, if one denotes by p(g|x) the probability\nthat Y = g conditional to X = x, logistic regression assumes that, for some parameters\n(a0(g),b(g),g ∈RY) with a0(g) ∈R and b(g) ∈Rd, one has p = pa0,b with\nlogpa0,b(g | x) = a0(g) + xT b(g) −log(C(a0,b,x)),\nwhere C(a0,b,x) = P\ng∈RY exp(a0(g) + xT b(g)).\n167\n\n168\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nIntroduce the functions, defined over mappings µ : RY →R (which can be iden-\ntified with vectors in Rq)\nFg(µ) = µ(g) −log\nX\ng′∈RY\neµ(g′) .\n(8.1)\nWith this notation, letting β(g) =\n \na0(g)\nb(g)\n!\nand ˜x =\n \n1\nx\n!\n, one has logpβ(g|x) = Fg(βT ˜x),\nwhere βT ˜x is the function (g′ 7→β(g′)T ˜x).\nFor any constant function (g 7→µ0 ∈R) one has\nFg(µ + µ0) = µ(g) + µ0 −log\nX\ng′∈RY\neµ(g′)+µ0 = µ(g) + µ0 −µ0 −log\nX\ng′∈RY\neµ(g′) = Fg(µ).\nAs a consequence, if one replaces, for all g, β(g) by ˜β(g) = β(g) + γ, with γ ∈Rd+1,\nthen ˜βT ˜x = βT ˜x + γT ˜x and\nlogp ˜β(g | x) = logpβ(g | x).\nThis shows that the model is over-parametrized.\nOne therefore needs a (d + 1)-\ndimensional constraint to ensure uniqueness, and we will enforce a linear constraint\nin the form\nX\ng∈RY\nρgβ(g) = c\nwith P\ng ρg , 0.\n8.1.2\nConditional log-likelihood\nThe conditional log-likelihood computed from the training set is:\nℓ(β) =\nN\nX\nk=1\nlogpβ(yk | xk).\nLogistic regression computes a maximizer ˆβ of this log-likelihood. The classification\nrule given a new input x then chooses the class g for which p ˆβ(g | x) is largest, or,\nequivalently, the class g that maximizes ˜xT β(g).\nProposition 8.1 Let ˜mg = P\nk:yg=k xk/Ng. The conditional log-likelihood ℓis concave\nwith first derivative\n∂β(g)ℓ= Ng ˜mT\ng −\nN\nX\nk=1\n˜xT\nk pβ(g|xk)\n(8.2)\n\n8.1. LOGISTIC REGRESSION\n169\nand negative semi-definite second derivative\n∂β(g)∂β(g′)ℓ= −1[g=g′]\nN\nX\nk=1\n˜xk ˜xT\nk pβ(g|xk) +\nN\nX\nk=1\n˜xk ˜xT\nk pβ(g|xk)pβ(g′|xk).\n(8.3)\nRemark 8.2 In this discussion, we consider ℓas a function defined over collections\n(β(g),g ∈RY), or, if one prefers, on the q(d + 1)-dimensional linear space, F , of\nfunctions β : RY →Rq+1. With this in mind, the differential dℓ(β) is a linear form\nfrom F to R, therefore associating to any family u = (u(g),g ∈RY), the expression\ndℓ(β)u =\nX\ng∈RY\n∂β(g)ℓu(g).\nSimilarly, the second derivative is the bilinear form\nd2ℓ(β)(u,u′) =\nX\ng,g′∈RY\nu(g)T ∂β(g)∂β(g′)ℓu(g′).\nThe last statement in the proposition expresses the fact that d2ℓ(β)(u,u) ≤0 for all\nu ∈F .\n♦\nProof First consider the function Fg in (8.1), so that\nℓ(β) =\nN\nX\nk=1\nFyk(βT ˜xk).\nWe have, for ζ : RY →R,\ndFg(µ)ζ = ζ(g) −\nP\ng′∈RY eµ(g′)ζ(g′)\nP\ng′∈RY eµ(g′)\nas can be easily computed by evaluating the derivative of F(µ + ϵu) at ϵ = 0. Intro-\nducing the notation\nqµ(g) =\neµ(g)\nP\ng∈RY eµ(g)\nand\n⟨ζ⟩µ =\nX\ng∈RY\nζ(g)qµ(g),\nwe have dFg(µ)ζ = ζ(g) −⟨ζ⟩µ. Evaluating the derivative of dFg(µ + ϵu′)(ζ) at ϵ = 0,\none gets (the computation being left to the reader)\nd2Fg(µ)(ζ,ζ′) = −⟨ζζ′⟩µ + ⟨ζ⟩µ ⟨ζ′⟩µ.\n(8.4)\n\n170\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nNote that −d2Fg(µ)(ζ,ζ) is the variance of ζ for the probability mass function qµ and\nis therefore non-negative (so that Fµ is concave). This immediately shows that ℓis\nconcave as a sum of concave functions.\nUsing the chain rule, we have, for u : RY →Rq,\ndℓ(β)u =\nN\nX\nk=1\ndFyk(βT ˜xk) ˜xT\nk u(·) =\nN\nX\nk=1\n˜xT\nk u(yk) −\nN\nX\nk=1\n⟨˜xT\nk u(·)⟩βT ˜xk.\nReordering the first sum in the right-hand side according to the values of yk gives\nN\nX\nk=1\nu(yk)T ˜xk =\nX\ng∈RY\nNgu(g)T ˜mg.\nNoting that qβT ˜x = pβ(·|x), we find\ndℓ(β)(u) =\nX\ng∈RY\nNg ˜mT\ng u(g) −\nX\ng∈RY\n˜xT\nk u(g)pβ(g|xk),\nyielding (8.2). Applying the chain rule again, we have\nd2ℓ(β)(u,u′) =\nN\nX\nk=1\nd2Fyk(βT ˜xk)( ˜xT\nk u(·), ˜xT\nk u′(·))\n(8.5)\nwith\nd2Fyk(βT ˜xk)( ˜xT\nk u(·), ˜xT\nk u′(·)) = −⟨u(·)T ˜xk ˜xT\nk u′(·)⟩βT ˜xk + ⟨˜xT\nk u(·)⟩βT ˜xk ⟨˜xT\nk u′(·)⟩βT ˜xk\n= −\nX\ng∈RY\nu(g)T ˜xk ˜xT\nk u′(g)pβ(g|xk)\n+\nX\ng,g′∈RY\nu(g)T ˜xk ˜xT\nk u′(g′)pβ(g|xk)pβ(g′|xk)\nfrom which (8.3) follows.\n■\nRemark 8.3 From Fg(µ+µ0) = Fg(µ) when µ0 is constant on RY, one deduces (taking\nthe derivative at µ0 = 0) that dFg(µ)1 = 0 for all µ, where 1 denotes the constant\nfunction equal to 1 on RY. For h ∈Rd+1, let ch denote the constant function ch(g) = h,\ng ∈RY. We have\ndℓ(β)ch =\nN\nX\nk=1\ndFyk(βT ˜xk) ˜xT\nk ch =\nN\nX\nk=1\ndFyk(βT ˜xk)( ˜xT\nk h1) =\nN\nX\nk=1\n( ˜xT\nk h)dFyk(βT ˜xk)1 = 0.\nTaking one extra derivative we see that\ndℓ(β)(ch,u) = 0\nfor all functions u : RY →Rq.\n♦\n\n8.1. LOGISTIC REGRESSION\n171\nWe now discuss whether there are other elements in the null space of the second\nderivative of ℓ. We will use notation introduced in the proof of proposition 8.1.\nFrom (8.4), we have d2Fg(µ)(ζ,ζ) = 0 if and only if the variance of ζ for qµ vanishes,\nwhich, since qµ > 0, is equivalent to ζ being constant. So, the null space of d2Fg(µ)\nis one-dimensional, and composed of scalar multiples of 1. Using (8.5), we see that\nd2ℓ(u,u) = 0 if and only if , for all k = 1,...,N, (g 7→˜xT\nk u(g)) is a constant function.\nAssume that this is true. Then, letting ¯u = 1\nq\nP\ng∈RY u(g), one has, for all g ∈RY\nand k = 1,...,N,\n˜xT\nk u(g) = ˜xT\nk ¯u\nso that u(g) −¯u is in the null space of the matrix X . This leads to the following\nproposition.\nProposition 8.4 Assume that X has rank d + 1. Then the null space of d2ℓ(β) is the set\nof all vectors u = ch for h ∈Rd+1. In particular, for any c ∈Rd+1, the function ℓrestricted\nto the space\nM =\n\nβ :\nX\ng∈RY\nρgβ(g) = c\n\nis strictly concave as soon as the scalar coefficients (ρg,g ∈RY) are such that P\ng∈RY ρg , 0.\nProof From the discussion before the proposition, u ∈Null(d2ℓ) implies that X (u(g)−\n¯u) = 0 for all g, and since we assume that X has rand d +1, this requires that u(g) = ¯u\nfor all g, i.e., u = c ¯u. This proves the first point.\nIf one restricts ℓto M, then we must restrict d2ℓ(β) to those u’s such that P\ng∈RY ρgu(g) =\n0. But if d2ℓ(β)(u,u) = 0 for such an u, then u = c ¯u and\nX\ng∈RY\nρgu(g) =\n\u0012 X\ng∈RY\nρg\n\u0013\n¯u.\nSince we assume that P\ng∈RY ρg , 0, this requires ¯u = 0, and therefore u = 0.\nThis shows that the second derivative of the restriction of ℓto M is negative\ndefinite, so this restriction is strictly concave.\n■\n8.1.3\nTraining algorithm\nGiven that we have expressed the first and second derivatives of ℓin closed form1,\nwe can use Newton-Raphson gradient ascent to maximize ℓover the affine space:\nM =\n\nβ :\nX\ng∈RY\nρgβ(g) = c\n\n1Their computation is feasible unless N is very large, and the matrix inversion in Newton’s itera-\ntion also requires d to be not too large.\n\n172\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nwith P\ng∈RY ρg , 0. We assume in the following that the matrix X has rand d + 1 so\nthat proposition 8.4 applies. Since the constraint is affine, it is easy to express one of\nthe parameters β(g) as a function of the others and solve the strictly concave problem\nas a function of the remaining variables. It is not much harder, and arguably more\nelegant to solve the problem without breaking its symmetry with respect to the class\nindexes, as described below.\nLet\nM0 =\n\nβ :\nX\ng∈RY\nρgβ(g) = 0\n\n.\nWe still have the second order expansion\nℓ(β + u) = ℓ(β) + dℓ(β)u + 1\n2d2ℓ(β)(u,u) + o(|u|2)\nand we consider the maximization of the first three terms, simply restricting to vec-\ntors u ∈M0. To allow for matrix computation, we use our ordering RY = (g1,...,gq)\nand identify a with the column vector\n\n\nu(g1)\n...\nu(gq)\n\n\n∈Rq(d+1)\nSimilarly, we let\n∇ℓ(β) =\n\n\n∂β(g1)ℓ\n...\n∂β(gq)ℓ\n\n\nand let ∇2(ℓ)(β) be the block matrix with i,j block given by ∂β(gi)∂β(gj)ℓ(β). We let ˆρ\nbe the (d + 1) × q(d + 1) row block matrix\n\u0010\nρ(g1)IdRd+1\n···\nρ(gq)IdRd+1\n\u0011\nso that u ∈M0 is just ˆρu = 0 in vector notation. Given this we have\nℓ(β + u) = ℓ(β) + ∇ℓ(β)T u + 1\n2uT ∇2(ℓ)(β)u + o(|u|2).\nThe maximum of ℓ(β) + uT ∇ℓ(β) + 1\n2uT ∇2(ℓ)(β)u subject to ˆρu = 0 is a stationary\npoint of the Lagrangian\nL = ℓ(β) + uT ∇ℓ(β) + 1\n2uT ∇2(ℓ)(β)u + λT ˆρu\n\n8.1. LOGISTIC REGRESSION\n173\nfor some λ ∈Rd+1 and is characterized by\n(∇2(ℓ)(β)u + ∇ℓ(β) + ˆρT λ = 0\nˆρu = 0\nThis shows that the Newton-Raphson iterations can be implemented as\nβn+1 = βn −ϵn+1un+1\n(8.6)\nwith\n \nun+1\nλ\n!\n=\n \n∇2(ℓ)(βn)\nˆρT\nˆρ\n0\n!−1  \n∇ℓ(βn)\n0\n!\n.\n(8.7)\nWe summarize this discussion in the following algorithm.\nAlgorithm 8.1 (Logistic regression with Newton’s gradient ascent)\n(1) Input: (i) training data (x1,y1,...,xN,yN) with xi ∈Rd and yi ∈RY; (ii) coefficients\nρg,g ∈RY with non-zero sum and target value c ∈R; (iii) algorithm step ϵ small\nenough.\n(2) Initialize the algorithm with β0 such that P\ng ρgβ0(g) = c.\n(3) At iteration n, compute ∇ℓ(βn) and ∇2(ℓ)(βn) as provided by proposition 8.1.\n(4) Update βn using (8.6) and (8.7), with ϵn+1 = ϵ. Alternatively, optimize ϵn+1 using\na line search.\n(5) Stop the procedure if the change in the parameter is below a small tolerance\nlevel. Otherwise, return to step 2.\n8.1.4\nPenalized Logistic Regression\nLogistic regression can be combined with a penalty term, e.g., maximizing\nℓ2(β) = ℓ(β) −λ\nd\nX\ni=1\n|b(i)|2\n(8.8)\nor\nℓ1(β) = ℓ(β) −λ\nd\nX\ni=1\n|b(i)|\n(8.9)\nwhere b(i) is the q-dimensional vector formed with the ith coefficients of b(g) for\ng ∈RY. Similarly to penalized regression, one generally normalizes the x variables\nto have unit standard deviation before applying the method.\n\n174\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nMaximization with the ℓ2 norm\nThe problem in (8.8) relates to ridge regression\nand can be solved using a Newton-Raphson method (Algorithm 8.1) with minor\nchanges. More precisely, letting\n∆=\n \n0\n0\n0\nIdRd\n!\nwe have, considering β as a d + 1 by q matrix,\nℓ2(β) = ℓ(β) −λtrace(βT ∆β)\nand\ndℓ2(β)u = dℓ(β)u −2λtrace(βT ∆u),\nd2ℓ2(β)(u,u′) = dℓ(β)(u,u′) −2λtrace(uT ∆u′).\nIn addition, when λ > 0, the problem is over-parametrized only up to the addition\nof a constant to (g 7→a0(g)), so that one only needs a single constraint P\ng ρga0(g) = 0\nand the Lagrange coefficient in (8.7) is one dimensional.\nMaximization with the ℓ1 norm\nThe maximization in (8.9) can be run using prox-\nimal gradient ascent (section 3.5.5), writing the objective function in the form\nℓ1(a0,b) = ℓ(a0,b) −λγ(a0,b)\nwith\nγ(a0,b) =\nd\nX\ni=1\ns X\ng∈RY\nb(i)(g)2.\nHere, ℓis concave and γ is convex and the proximal gradient iterations are\nβn+1 = proxϵλγ(βn + ϵ∇ℓ(βn))\n(8.10)\nwhere ∇ℓis the gradient of ℓprojected on the set of functions u : RY →Rd+1 satis-\nfying\nX\ng∈RY\nρgu(0)(g) = 0\nwhere u(0)(g) is the first coordinate of u(g). This projection can be computed by\nsubtracting\nP\ng′∈RY ρ(g′)∂a0(g′)ℓ\nP\ng′∈RY ρ(g′)2\nρ(g)\nto ∂a0(g)ℓ. This algorithm will converge for small enough ϵ.\n\n8.1. LOGISTIC REGRESSION\n175\nWe already know the gradient of ℓ, so it only remains to determine the proximal\noperator of γ to make (8.10) explicit. Let us denote the coordinates of a function\nu : RY →Rd+1 as u(i)(g) for i = 0,...,d and g ∈RY.\nproxϵλγ(u) = argmin\n˜u\n\nγ( ˜u) +\n1\n2λϵ\nd\nX\ni=0\nX\ng∈RY\n(u(i)(g) −˜u(i)(g))2\n\n.\nSince γ does not depend on ˜u(0)(·), the optimal ˜u(0)(˙) is ˜u(0)(·) = u(0)(·). One can\noptimize separately each group ( ˜u(i)(g),g ∈RY), which must minimize\ns X\ng∈RY\n˜u(i)(g)2 +\n1\n2λϵ\nX\ng∈RY\n(u(i)(g) −˜u(i)(g))2.\nThe function t 7→\n√\nt being differentiable everywhere except at 0, we first search for\na solution for which at least one ˜u(i)(g) does not vanish. If such a solution exists, it\nmust satisfy, for all g ∈RY\n˜u(i)(g)\nqP\ng′∈RY ˜u(i)(g′)2\n+ 1\nλϵ( ˜u(i)(g) −u(i)(g)) = 0\nLetting | ˜u(i)(·)| =\nqP\ng∈RY ˜u(i)(g)2 we get\n˜u(i)(·)(| ˜u(i)(·)| + λϵ) = u(i)(·)| ˜u(i)(·)|\nTaking the norm on both sides and dividing by | ˜u(i,·)| (which is assumed not to\nvanish) yields\n| ˜u(i)(·)| + λϵ = |u(i)(·)|,\nwhich has a positive solution only if |u(i)(·)| > λϵ, and gives in that case\n˜u(i)(·) = |u(i)(·)| −ϵλ\n|u(i)(·)|\nu(i)(·)\nIf |u(i)(·)| ≤λϵ, then we must take ˜u(i)(·) = 0. We have therefore obtained:\nproxϵλg(a) = ˜u\nwith ˜u(0)(·) = u(0)(·) and\n˜u(i)(·) = max\n\u0012|u(i)(·)| −ϵλ\n|u(i)(·)|\n,0\n\u0013\nu(i)(·)\nfor i ≥1. We summarize this discussion in the next algorithm.\n\n176\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nAlgorithm 8.2 (Logistic lasso)\n(1) Input: (i) training data (x1,y1,...,xN,yN) with xi ∈Rd and yi ∈RY; (ii) coefficients\nρg,g ∈RY with non-zero sum and target value c ∈R; (iii) algorithm step ϵ; (iv)\npenalty coefficient λ.\n(2) Initialize the algorithm with β0 = (a00,b0) such that P\ng ρga00(g) = c.\n(3) At iteration n, compute u = βn + ϵ∇ℓ(βn), with βn = (a0,n,bn).\n(4) Let an+1,0(·) = u(0)(·) and for i ≥1,\nb(i)\nn+1(·) = max\n\u0012|u(i)(·)| −ϵλ\n|u(i)(·)|\n,0\n\u0013\nu(i)(·)\n(5) Stop the procedure if the change in the parameter is below a small tolerance\nlevel. Otherwise, return to step 2.\n8.1.5\nKernel logistic regression\nLet h : RX →H be a feature function with values in a Hilbert space H with K(x,x′) =\n⟨h(x) , h(x′)⟩H. The kernel version of logistic regression uses the model:\nlogpa0,b(g | x) = a0(g) + ⟨h(x) , b(g)⟩H −log\nX\n˜g∈RY\nexp(a0(˜g) + ⟨h(x) , b(˜g)⟩H)\nwith b(g) ∈H for g ∈RY.\nUsing the usual kernel argument, one sees that, when maximizing the log-likelihood,\nthere is no loss of generality is assuming that each b(g) belongs to V = span(h(x1),...,h(xN)).\nTaking\nb(g) =\nN\nX\nk=1\nαk(g)h(xk),\nwe have\nlogpα(g | x) = a0(g) +\nN\nX\nk=1\nαk(g)K(x,xk) −log\n\n\nX\n˜g∈RY\nexp(a0(˜g) +\nN\nX\nk=1\nαk(˜g)K(x,xk))\n\n.\nTo avoid overfitting, one must include a penalty term in the likelihood, and (in order\nto take advantage of the kernel), one can take this term proportional to P\ng ∥b(g)∥2\nH.\nThe complete learning procedure then requires to maximize the concave penalized\nlikelihood\nℓ(α) =\nN\nX\nk=1\nlogpα(yk | xk) −λ\nX\ng∈RY\nN\nX\nk,l=1\nαk(g)αl(g)K(xk,xl).\n\n8.2. LINEAR DISCRIMINANT ANALYSIS\n177\nThe computation of the first and second derivatives of this function is similar to that\nfor the original version, and we skip the details.\n8.2\nLinear Discriminant analysis\n8.2.1\nGenerative model in classification and LDA\nGenerative model\nIn classification, the class variable Y generally has a causal role\nupon which the variable X is produced. Prediction can therefore be seen as an in-\nverse problem where the cause is deduced from the result. In terms of generative\nmodeling, one should therefore model the distribution of Y, followed by the the\nconditional distribution of X given Y.\nTaking RX = Rd, denote by fg the conditional p.d.f. of X given Y = g and let πg =\nP(Y = g). The Bayes estimator for the 0–1 loss maximizes the posterior probability\nP(Y = g | X = x) =\nπgfg(x)\nP\ng′∈RY πg′fg′(x) .\nSince the denominator does not depend on g the Bayes estimator equivalently max-\nimizes (taking logarithms)\nlogfg(x) + logπg .\nOne generally speaks of a linear classification method when the prediction is\nbased on the maximization in g of a function U(g,x) where U is affine in x. In this\nsense, logistic regression is linear, and kernel logistic regression is linear in feature\nspace. For the generative approach, this occurs when one uses the following model,\nwhich provides the generative form of linear discriminant analysis (LDA). Assume\nthat the distributions fg are all Gaussian with mean mg and common variance S, so\nthat\nfg(x) =\n1\np\n(2π)d detΣ\ne−1\n2(x−mg)T S−1(x−mg).\n(8.11)\nIn this case, the optimal predictor must maximize (in g)\n−1\n2(x −mg)T S−1(x −mg) + logπg .\nIntroduce m = E(X) = P\ng∈RY πgmg. Then the optimal classifier must maximize\n−1\n2(x −m)T S−1(x −m) + (x −m)T S−1(mg −m) −1\n2(mg −m)T S−1(mg −m) + logπg.\nSince the first term does not depend on g, it is equivalent to maximize\n(x −m)T S−1(mg −m) −1\n2(mg −m)T S−1(mg −m) + logπg\n(8.12)\nwith respect to the class g, which provides an affine function of x.\n\n178\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nTraining\nTraining for LDA simply consists in estimating the class means and com-\nmon variance in (8.11) from data. We introduce some notation for this purpose (this\nnotation will be reused through the rest of this chapter).\nRecall that Ng,g ∈RY denotes the number of samples with class g in the train-\ning set T = (x1,y1,...,xN,yN). We let cg = Ng/N and C be the diagonal matrix with\ndiagonal coefficients cg1,...,cgq. We also let ζ ∈Rq denote the vector with the same\ncoordinates. For g ∈RY, µg denotes the class average\nµg = 1\nNg\nn\nX\nk=1\nxk1yk=g\nand µ the global average\nµ = 1\nN\nN\nX\nk=1\nxk =\nX\ng∈RY\ncgµg.\nLet Σg denote the sample covariance matrix in class g, defined by\nΣg = 1\nNg\nN\nX\nk=1\n(xk −µg)(xk −µg)T 1yk=g,\nand Σw the pooled class covariance (also called within-class covariance) defined by\nΣw = 1\nN\nN\nX\nk=1\n(xk −µyk)(xk −µyk)T =\nX\ng∈RY\ncgΣg.\nLet, in addition, Σb denotes the “between-class” covariance matrix, given by\nΣb =\nX\ng∈RY\ncg(µg −µ)(µg −µ)T\nThe global covariance matrix, given by,\nΣXX = 1\nN\nN\nX\nk=1\n(xk −µ)(xk −µ)T\nsatisfies ΣXX = Σw + Σb. This identity is proved by noting that, for any g ∈RY,\n1\nNg\nN\nX\nk=1\n(xk −µ)(xk −µ)T 1yk=g = Σg + (µg −µ)(µg −µ)T .\n\n8.2. LINEAR DISCRIMINANT ANALYSIS\n179\nWe will finally denote by M the matrix\nM =\n\n\n(µg1 −µ)T\n...\n(µgq −µ)T\n\n\n.\nNote that Σb = MT CM.\nGiven this notation, one can in particular take ˆmg = µg, m = µ and ˆS = Σw in\n(8.12). The class probabilities, πg, can be deduced from the normalized frequencies\nof y1,...,yN. However, in many applications, one prefers to simply fix πg = 1/q, in\norder to balance the importance of each class.\nRemark 8.5 If one relaxes the assumption of common class variances, one needs to\nuse Σg in place of Σw for class g. The decision boundaries are not linear in this\ncase, but provided by quadratic equations (and the resulting method if often called\nquadratic discriminant analysis, or QDA). QDA requires the estimation of qd(d +\n3)/2 coefficients, which may be overly ambitious when the sample size is not large\ncompared to the dimension, in which case QDA is prone to overfitting. (Even LDA,\nwhich involves qd +d(d +1)/2 parameters, may be unrealistic in some cases.) We also\nnote a variant of QDA that uses class covariance matrices given by\n˜Σg = αΣw + (1 −α)Σg.\n8.2.2\nDimension reduction\nOne of the interests of LDA is that it can be combined with a rank reduction proce-\ndure. LDA with q classes can always be seen as a (q −1)-dimensional problem after\nsuitable projection on a data-dependent affine space. Recall that the classification\nrule after training requires to maximize w.r.t. g ∈RY the function\n(x −µ)T Σ−1\nw (µg −µ) −1\n2(µg −µ)T Σ−1\nw (µg −µ) + logπg.\nDefine the “spherized” data 2 by ˜xk = Σ−1/2\nw\n(xk −µ), where Σ1/2\nw\nis the positive sym-\nmetric square root of Σw. Also let ˜µg = Σ−1/2\nw\n(µg −µ).\nWith this notation, the predictor chooses the class g that maximizes\n˜xT ˜µg −1\n2| ˜µg|2 + logπg\nwith ˜x = Σ−1/2\nw\n(x −¯µ).\n2In this section only, the notation ˜x does not refer to (1,xT )T .\n\n180\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nNow, let V = span{ ˜µg,g ∈RY}. Since P\ng cg ˜µg = 0, this space is at most (q −1)-\ndimensional. Let PV denote the orthogonal projection on V . We have ˜xT z = (PV ˜x)T z\nfor any z ∈V and ˜x ∈Rd.\nThe classification rule can then be replaced by maximizing\n(PV ˜x)T ˜µg −1\n2| ˜µg|2 + logπg\nwith ˜x = Σ−1/2\nw\n(x −¯µ).\nRecall that M =\n\n\n(µg1 −µ)T\n...\n(µgq −µ)T\n\n\nand let e\nM =\n\n\n˜µT\ng1...\n˜µT\ngq\n\n\n. The dimension, denoted r, of V is\nequal to the rank of e\nM. Let (˜e1,..., ˜er) be an orthonormal basis of V . One has\nPV ˜x =\nr\nX\nj=1\n( ˜xT ˜ej) ˜ej.\nGiven an input x, one must therefore compute the “scores” γj(x) = ˜xT ˜ej and maxi-\nmize\nr\nX\nj=1\nγj(x)γj(µg) −1\n2\nr\nX\nj=1\nγj(µg)2 + logπg .\nThe following proposition is key to the practical implementation of LDA with\ndimension reduction.\nProposition 8.6 An orthonormal basis of V = span( ˜µg,g ∈CG) is provided by the the\nfirst r eigenvectors of e\nMT C e\nM associated with eigenvalues λ1 ≥··· ≥λr > 0 (all other\neigenvalues being zero).\nProof Indeed, if ˜x is perpendicular to V , we have\n˜MT C ˜M ˜x =\nX\ng∈RY\ncg( ˜µT\ng ˜x) ˜µg = 0\nso that V ⊥⊂Null( ˜MT C ˜M), and both spaces coincide because they have the same\ndimension (d −r). This shows that V = Null( e\nMT C e\nM)⊥= Range( ˜MT C e\nM). Since\ne\nMT C e\nM is symmetric, Null( e\nMT C e\nM)⊥is generated by eigenvectors with non-zero\neigenvalues.\n■\nReturning to the original variables, we have e\nM = MΣ−1/2\nw\nand MT CM = Σb, the\nbetween class covariance matrix. This implies that e\nMT C e\nM = Σ−1/2\nw\nΣbΣ−1/2\nw\nand each\n\n8.2. LINEAR DISCRIMINANT ANALYSIS\n181\nFigure 8.1: Left: Original (training) data with three classes. Right: LDA scores, where the x\naxis provides γ1 and the y axis γ2.\neigenvector ˜ej therefore satisfies\nΣbΣ−1/2\nw\n˜ej = λjΣ1/2\nw ˜ej = λjΣw(Σ−1/2\nw\n˜ej).\nTherefore, letting ej = Σ−1/2\nw\n˜ej, (e1,...,er) are the solutions of the generalized eigen-\nvalue problem Σbe = λΣwe that are associated with non-zero eigenvalues (they are\nhowever normalized so that eT\nj Σwej = 1). Moreover, the scores are given by\nγj(x) = ˜xT ˜ej = (x −µ)T Σ−1/2\nw\n˜ej = (x −µ)T ej\nand can therefore be computed directly from the original data and the vectors e1,...,er.\nAn example of training data and its representation in the LDA space (associated with\nthe scores) in provided in fig. 8.1.\nWe can now describe the LDA learning algorithm with dimension reduction.\nAlgorithm 8.3 (LDA with dimension reduction)\n1. Compute µg,g ∈RY, Σw and Σb from training data.\n2. Estimate (if needed) πg,g ∈RY\n3. Solve the generalized eigenvalue problem Σbe = λΣwe.\nLet e1,...,er be the\neigenvectors associated with non-zero eigenvalues, normalized so that eT\nj Σwej = 1.\n4. Choose a reduced dimension r0 ≤r.\n5. Precompute mean scores γj(µg) = (µg −µ)T ej, g ∈RY,j = 1,...,r0.\n6. To classify a new example x, compute γj(x) = (x −µ)T ej and choose the class\nthat maximizes\nr0\nX\nj=1\nγj(x)γj(µg) −1\n2\nr0\nX\nj=1\nγj(µg)2 + logπg .\n\n182\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\n8.2.3\nFisher’s LDA\nThis characterization leads to the discriminative interpretation of LDA, also called\nFisher’s LDA. Indeed, the generalized eigenvalue problem Σbe = λΣwe is directly\nrelated to the maximization of the ratio eT Σbe subject to eT Σwe = 1, which provides\ndirections that have a large between-class variance for within class variance equal to\n1. More precisely, e1 is the direction that achieves the maximum; e2 is the second best\ndirection, constrained to being perpendicular to e1, and so on until er which is the\noptimal constrained to be perpendicular to (e1,...,er−1). We are therefore looking\nfor directions that have the largest ratio of between-class variance to within-class\nvariance.\n8.2.4\nKernel LDA\nMean and covariance in feature space\nWe assume the usual construction where h :\nRX →H is a feature function, H a Hilbert space with kernel K(x,x′) = ⟨h(x) , h(x′)⟩H.\n(The assumption that H is a complete space is here required for the expectations\nbelow to be meaningful.)\nWe now discuss the kernel version of LDA by plugging the feature space repre-\nsentation directly in the classification rule. So, consider h : R →H. Let X : Ω→R be\na random variable such that E(∥h(X)∥2\nH) < ∞. Then, its mean feature m = E(h(X)) is\nwell defined as an element of H , and so are the class averages, mg = E(h(X) | Y = g).\nIn this possibly infinite-dimensional setting, the covariance “matrix” is defined\nas a linear operator S : H →H such that, for all ξ,η ∈H:\n⟨ξ , Sη⟩H = E\n\u0010\n⟨h(X) −m , ξ⟩H⟨h(X) −m , η⟩H\n\u0011\n,\n(8.13)\nwhich is equivalent to defining\nSη = E(⟨h(X) −m , η⟩H(h(X) −m))\nfor η ∈H. This definition generalizes the identity for a random variable U : Ω→Rd:\nSUw = E((U −E(U))(U −E(U))T )w = E(((U −E(U))T w)(U −E(U)))\nOne can similarly define the covariance matrix in class g, Sg, by conditioning the\nright-hand side in (8.13) by Y = g and replacing m by mg.\nLDA in feature space\nFollowing the LDA model, we assume that the operators Sg are all equal to a fixed\noperator, the within-class covariance operator denoted S.\n\n8.2. LINEAR DISCRIMINANT ANALYSIS\n183\nAssuming that S is invertible, one can generalize the LDA classification rule to\ndata represented in feature space by classifying a new input x in class g when\n⟨h(x) −m , S−1(mg −m)⟩H −1\n2⟨mg −m , S−1(mg −m)⟩H + logπg\n(8.14)\nis maximal over all classes. Notice that this is a transcription of the finite-dimensional\nBayes rule, but cannot be derived from a generative model, because the assumption\nthat h(X) is Gaussian is not valid in general. (It would require that h takes values\nin a d-dimensional linear space, which would eliminate all interesting kernel repre-\nsentations.)\nLet, as before, T = (x1,y1,...,xN,yN) be the training set, Ng denote the number\nof examples in class g and cg = Ng/N. When h is known (which, we recall, is not a\npractical assumption, but we will fix this later), one can estimate the class averages\nfrom training data by\nµg = 1\nNg\nN\nX\nk=1\nh(xk)1yk=g\nand the within-class covariance operator by\n⟨ξ , Σwη⟩H = 1\nN\nN\nX\nk=1\n⟨h(xk) −µyk , ξ⟩H⟨h(xk) −µyk , η⟩H.\nUnfortunately, the resulting variance estimator cannot be directly used in (8.14),\nbecause it is not invertible if dim(H) > N. Indeed, one has Σwη = 0 as soon as η is\nperpendicular to V\n∆= span(h(x1),...,h(xN)).\nOne way to address the degeneracy of the estimated covariance operator is to\nadd to Σw a small multiple of the identity, say ρIdH,3 and let the classification rule\nmaximize in g:\n⟨h(x) −µ , (Σw + ρIdH)−1(µg −µ)⟩H −1\n2⟨µg −µ , (Σw + ρIdH)−1(µg −µ)⟩H + logπg .\n(8.15)\nwhere µ is the average of h(x1),...,h(xN). Taking this option, we still need to make\nthis expression computable and remove the dependency in the feature function h.\nReduction\nWe have µg ∈V for all g ∈RY and, since\nΣwη = 1\nN\nN\nX\nk=1\n⟨h(xk) −µyk , η⟩H(h(xk) −µyk),\n3The operator A + ρIdH is invertible as soon as A is symmetric positive semi-definite.\n\n184\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nthis operator maps H to V , which implies that Σw + ρIdH maps V into itself. More-\nover, this mapping is onto: If v ∈V and u = (Σw + ρIdH)−1v, then, u ∈V . Indeed,\nfor any z ⊥V , we have ⟨z , Σwu + ρu⟩H = ⟨z , v⟩H. We have ⟨z , Σwu⟩H = 0 (because\nΣw maps H to V ) and ⟨z , v⟩H = 0 (because v ∈V ), so that we can conclude that\n⟨z , u⟩H = 0. Since this is true for all z ⊥V , this requires that u ∈V .4\nWe now express the classification rule in (8.15) as a function of the kernel asso-\nciated with the feature-space representation. Denote, for any vector u ∈RN,\nξ(u) =\nN\nX\nk=1\nu(k)h(xk),\ntherefore defining a mapping ξ from RN onto V . Letting as usual K = K(x1,...,xN)\nbe the matrix formed by pairwise evaluations of K on training inputs, we have the\nidentity\n⟨ξ(u) , ξ(u′)⟩H = uT Ku′.\nfor all u,u′ ∈RN. For simplicity, we will assume in the rest of the discussion that K\nis invertible.\nWe have µg = ξ(1g/Ng), where 1g ∈RN is the vector with kth coordinate equal\nto 1 if yk = g and 0 otherwise. Also µ = ξ(1/N) (recall that 1 is the vector with all\ncoordinates equal to 1).\nFor u ∈RN, we want to characterize v ∈RN such that Σwξ(u) = ξ(v). Let δk\ndenote the vector with 1 at the kth entry and 0 otherwise. We have\nΣwξ(u) = 1\nN\nN\nX\nk=1\n⟨ξ(u) , h(xk) −µyk⟩H(h(xk) −µyk)\n= 1\nN\nN\nX\nk=1\n⟨ξ(u) , ξ(δk −1yk/Nyk)⟩Hξ(δk −1yk/Nyk)\n= 1\nN\nN\nX\nk=1\n((δk −1yk/Nyk)T Ku)ξ(δk −1yk/Nyk)\n= ξ\n\n\n1\nN\nN\nX\nk=1\n((δk −1yk/Nyk)T Ku)(δk −1yk/Nyk)\n\n\nso that Σwξ(u) = ξ(PKu) with\nP = 1\nN\nN\nX\nk=1\n(δk −1yk/Nyk)(δk −1yk/Nyk)T\n4One has (V ⊥)⊥= V for finite-dimensional—or more generally closed—subspaces of H\n\n8.2. LINEAR DISCRIMINANT ANALYSIS\n185\nNote that one has\n•\nN\nX\nk=1\nδkδT\nk = IdRN,\n•\nN\nX\nk=1\n 1yk\nNyk\n!\nδT\nk =\nX\ng∈RY\n1g\nNg\nX\nk:yk=g\nδT\nk =\nX\ng∈RY\n1g1T\ng\nNg\n=\nN\nX\nk=1\nδk\n 1yk\nNyk\n!T\n,\n•\nN\nX\nk=1\n 1yk\nNyk\n! 1yk\nNyk\n!T\n=\nX\ng∈RY\n1g1T\ng\nNg\n.\nThis shows that P can be expressed as\nP = 1\nN\n\nIdRN −\nX\ng∈RY\n1g1T\ng /Ng\n\n.\nWe have therefore proved that\n•\n(Σw + ρIdH)ξ(u) = ξ ((PK + ρIdRN)u)\n•\n(Σw + ρIdH)−1ξ( ˜u) = ξ\n\u0010\n(PK + ρIdRN)−1 ˜u\n\u0011\n.\nRecall that the feature-space LDA classification rule maximizes\n⟨h(x) −µ , (Σw + ρIdH)−1(µg −µ)⟩H −1\n2⟨µg −µ , (Σw + ρIdH)−1(µg −µ)⟩H + logπg .\nAll terms belong to V , except h(x), but this term can be replaced by its orthogonal\nprojection on V without changing the result. This projection can be made explicit\nin terms of the representation ξ as follows. For x ∈R, let ξ(ψ(x)) denote the orthog-\nonal projection of h(x) on V (this defines the function ψ). If v(x) denotes the vector\nwith coordinates K(x,xk), k = 1,...,N, then ψ(x) = K−1v(x), as can be obtained by\nidentifying the inner products ⟨h(x) , h(xk)⟩H and ⟨ξ(ψ(x)) , h(xk)⟩H.\nWe are now ready to rewrite the kernel LDA classification rule in terms of quan-\ntities that only involve K. We have\n⟨h(x) −µ , (Σw + ρIdH)−1(µg −µ)⟩H\n= ⟨ξ(ψ(x) −1/N) , ξ((PK + ρIdRN)−1(1g/Ng −1/N))⟩H\n= (ψ(x) −1/N)T K(PK + ρIdRN)−1(1g/Ng −1/N)\nGiven this, the classification rule must maximize\n(ψ(x) −1/N)T K(PK + ρIdRN)−1(1g/Ng −1/N)\n−1\n2(1g/Ng −1/N)T K(PK + ρIdRN)−1(1g/Ng −1/N) + logπg.\n(8.16)\n\n186\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nDimension reduction\nNote that K(PK+ρIdRN)−1 = K(KPK+ρK)−1K is a symmet-\nric matrix. So, the expression in (8.16) can be written as\n(v(x) −ν)T R−1(νg −ν) −1\n2(νg −ν)T R−1(νg −ν) + logπg.\nwith R = KPK + ρK, νg = K1g/Ng and ¯ν = K1/N. Clearly, if v1,...,vN are the column\nvectors K, we have\nνg = 1\nNg\nN\nX\nk=1\nvk1yk=g,\n¯ν = 1\nN\nN\nX\nk=1\nvk .\nWe therefore retrieve an expression similar to finite-dimensional LDA, provided\nthat one replaces x by v(x), xk by vk and Σw by R. Letting\nQ = 1\nN\nX\ng∈RY\nNg(νg −¯ν)(νg −¯ν)T\nbe the between-class covariance matrix, the discriminant directions are therefore\nsolutions of the generalized eigenvalue problem\nQfj = λjRfj\nwith f T\nj Rfj = 1 with R = (KPK + ρK). Note that\nKPK = 1\nN\nN\nX\nk=1\n(vk −¯νyk)(vk −¯νyk)T\nis the within-class covariance matrix for the training data (v1,y1,...,vN,yN).\nThe following summarizes the kernel LDA classification algorithm.\nAlgorithm 8.4 (Kernel LDA)\n(1) Select a positive kernel K and a coefficient ρ > 0.\n(2) Given T = (x1,y1,...,xN,yN) and a kernel K, compute the kernel matrix K =\nK(x1,...,xN) and the matrix R = KPK+ρK. Let v1,...,vN be the column vectors of K.\n(3) Compute, for g ∈RY,\nνg = 1\nNg\nN\nX\nk=1\nvk1yk=g ,\n¯ν = 1\nN\nN\nX\nk=1\nvk\nand let Q = 1\nN\nP\ng∈RY Ng(νg −¯ν)(νg −¯ν)T .\n\n8.3. OPTIMAL SCORING\n187\n(4) Fix r0 ≤q−1 and compute the eigenvectors f1,...,fr0 associated with the r0 largest\neigenvalues for the generalized eigenvalue problem Qf = λRf , normalized such that\nf T\nj Rfj = 1.\n(5) Compute the scores γjg = (νg −¯ν)T fj.\n(6) Given a new observation x, let v(x) be the vector with coordinates K(x,xk), k =\n1,...,N. Compute the scores γj(x) = (v(x) −¯ν)T fj, j = 1,...,r0. Classify x in the class\ng maximizing\nr\nX\ni=1\nγi(x)γig −1\n2\nr\nX\ni=1\nγ2\nig + logπg.\n(8.17)\n8.3\nOptimal Scoring\nIt is possible to apply linear regression (chapter 7) to solve a classification problem\nby mapping the set RY to a collection of r-dimensional row vectors, or “scores.”\nThese scores (which have a different meaning from the LDA scores) will be repre-\nsented by a function θ : RY 7→Rr. As an example, one can take r = q and\nθ(g1) =\n\n\n1\n0\n0\n...\n0\n\n\n, θ(g2) =\n\n\n0\n1\n0\n...\n0\n\n\n, ... , θ(gq) =\n\n\n0\n0\n...\n0\n1\n\n\n.\nGiven a training set T = (x1,y1,...,xN,yN) and a score function θ, a linear model can\nthen be estimated from data by minimizing\nN\nX\nk=1\n|θyk −a0 −bT xk|2\nwhere b is a d×q matrix and a0 ∈Rq. Letting as before β be the matrix with aT\n0 added\nas first row to b and X the matrix with first row containing only ones and subsequent\nrows given by xT\n1 ,...,xT\nN, one gets the least square estimator ˆβ = (X T X )−1X T Y, where\nY is the N × q matrix of stacked θT\nyk row vectors.\nGiven an input vector x, the row vector ˜xT β will generally not coincide with\none of the score vectors. Assignment to a class can then be made by minimizing\n|a0 + bT x −θg| over all g in RY.\nSince the scores θ are free to choose, one may also try to optimize them, resulting\nin the optimal scoring algorithm. To describe it, we will need the notation already\n\n188\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nintroduced for LDA, plus the following. We will write, for short, θj = θ(gj) and\nintroduce the q × r matrix Θ =\n\n\nθT\n1...\nθT\nq\n\n\n. We also denote by ρ1,...,ρr the column vectors\nof Θ, so that Θ = [ρ1,...,ρr]. Let ugi, for i = 1,...,q, denote the q-dimensional vector\nwith ith coordinate equal to 1 and all others equal to 0. As before, Ng denote the class\nsizes, cg = Ng/N, C is the diagonal matrix with coefficients cg1,...,cgq and ζ =\n\n\ncg1...\ncgq\n\n\n.\nThe goal of optimal scoring is to minimize, now with respect to θ, a0 and b, the\nfunction\nF(θ,a0,b) =\nN\nX\nk=1\n|θ(yk) −a0 −bT xk|2 .\nSome normalizing conditions are clearly needed, because this problem is under-\nconstrained. (In the form above, the optimal choice is to take all free parameters\nequal to 0.) We now discuss the various indeterminacies and redundancies in the\nmodel,\n(a) If R is an r × r orthogonal matrix, then F(Rθ,Ra0,bRT ) = F(θ,a0,b), yielding an\ninfinity of possible equivalent solutions (that all lead to the same classification rule).\nThis implies that there is no loss of generality in assuming that ΘT CΘ is diagonal\n(introducing C here will turn out to be convenient). Indeed, given any (θ,a0,b), one\ncan just take R such that RΘT CΘRT is diagonal and replace Θ by RΘ, a0 by Ra0 and\nb by bRT to get an equivalent solution satisfying the constraint.\n(b) Let D be an r by r diagonal matrix with positive entries. Replace θ, a0 and b\nrespectively by Dθ, Da0 and bD. The resulting objective function is\nF(Dθ,Da0,bDT ) =\nN\nX\nk=1\n|Dθ(yk) −Da0 −DbT xk|2\n=\nr\nX\nj=1\nN\nX\nk=1\nd2\njj\n\u0012\nθ(yk,j) −a0(j) −\nd\nX\ni=1\nb(i,j)xk(i)\n\u00132\nIf the coefficient djj is free to chose, then the objective function can always be re-\nduced by letting djj →0, which removes one of the dimensions in θ. In order to\navoid this, one needs to fix the diagonal values of ΘT CΘ, and, by symmetry, it is\nnatural to require ΘT CΘ = IdRr.\n(c) Given any δ ∈Rr, one has F(θ,a0,b) = F(θ −δ,a0 + δ,b), with identical classifica-\ntion rule. One can therefore without loss of generality introduce r linear constraints,\n\n8.3. OPTIMAL SCORING\n189\nand a convenient choice is\nΘT ζ =\nX\ng∈RY\ncgθg = 0.\nGiven this reduction, we can now describe the optimal scoring problem as the\nminimization of\nN\nX\nk=1\n|θyk −a0 −bT xk|2\nsubject to ΘT CΘ = IdRr and ΘT ζ = 0.\nThe optimal a0 is given by\nˆa0 = 1\nN\nN\nX\nk=1\nθyk −bT µ = −bT µ,\nso that the problem is reduced to minimizing\nN\nX\nk=1\n|θyk −bT (xk −µ)|2\nsubject to the same constraints. Using the facts that θyk = ΘT uyk, that\nN\nX\nk=1\nuykuT\nyk =\nX\ng∈RY\nNguguT\ng = NC\nand that\nN\nX\nk=1\nuyk(xk −µ)T =\nN\nX\ng∈RY\nug\nX\nk:yk=g\n(xk −µ)T =\nN\nX\ng∈RY\nugNg(µg −µ)T = NCM,\none can write\nN\nX\nk=1\n|θyk −bT (xk −µ)|2 =\nN\nX\nk=1\n|ΘT uyk −bT (xk −µ)|2\n=\nN\nX\nk=1\nuT\nykΘΘT uyk −2\nN\nX\nk=1\n(xk −µ)T bΘT uyk +\nN\nX\nk=1\n(xk −µ)T bbT (xk −µ)\n=\nN\nX\nk=1\ntrace(ΘT uykuT\nykΘ) −2\nN\nX\nk=1\ntrace(ΘT uyk(xk −µ)T b)\n+\nN\nX\nk=1\ntrace(bT (xk −µ)(xk −µ)T b)\n= Ntrace(ΘT CΘ) −2Ntrace(ΘT CMb) + Ntrace(bT ΣXXb).\n\n190\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nNote that, since ΘT CΘ = IdRr, then trace(ΘT CΘ) = r. We therefore obtain a\nconcise form of the optimal scoring problem: minimize\n−2trace(ΘT CMb) + trace(bT ΣXXb).\nsubject to ΘT CΘ = IdRr and ΘT ζ = 0.\nGiven Θ, the optimal b is Σ−1\nXXMT CΘ, and replacing it in the objective function,\none finds that Θ must minimize\n−2trace(ΘT CMΣ−1\nXXMT CΘ) + trace(ΘT CMΣ−1\nXXMT CΘ)\ni.e., maximize\ntrace(ΘT CMΣ−1\nXXMT CΘ)\nsubject to ΘT CΘ = IdRr and ΘT ζ = 0. We now recall the following linear algebra\nresult (see chapter 2).\nProposition 8.7 Let A and B be respectively positive definite and non-negative semi-\ndefinite symmetric q by q matrices. Then, the maximum, over all q by r matrices S such\nthat trace(ST AS) = IdRr, of trace(ST BS) is attained at S = [σ1,...,σr], where the columns\nvectors σ1,...,σr are the solutions of the generalized eigenvalue problem\nBσ = λAσ\nassociated with the largest eigenvalues, normalized so that σT\ni Aσi = 1 for i = 1,...,r..\nGiven this proposition, let ρ1,...,ρr be the r first eigenvectors for the problem\nCMΣ−1\nXXMT Cρ = λCρ.\n(8.18)\nAssume that r is small enough so that the associated eigenvalues are not zero. Let\nΘ = [ρ1,...,ρr]. We now prove that Θ is indeed a solution of the optimal scoring\nproblem, and the only point to show to complete the statement is that this Θ satisfies\nthe constraints ΘT ζ = 0. But we have\nMT C1q =\nX\ng\ncg(µg −¯µ) = 0,\nwhich implies that 1q is a solution of the generalized eigenvalue problem associated\nwith λ = 0. This in turn implies that 1T\nq Cρi = ζT ρi = 0, which is exactly ΘT ζ = 0.\nTo summarize, we have found that the solution θ,b minimizing\n−2trace(ΘT CMb) + trace(bT ΣXXb)\nsubject to ΘT CΘ = IdRr and ΘT ζ = 0 is given by\n\n8.3. OPTIMAL SCORING\n191\n(i) Θ = [ρ1,...,ρr] where ρ1,...,ρr are the eigenvectors for the problem\nCMΣ−1\nXXMT Cρ = λCρ\nassociated with the r largest eigenvalues, normalized so that ρT Cρ = 1.\n(ii) b = Σ−1\nXXMT CΘ.\nThe computation can, however, be further simplified. Let λ1,...,λr be the eigen-\nvalues associated with ρ1,...,ρr. Letting D be the associated diagonal matrix, one\ncan write\nCMΣ−1\nXXMT CΘ = CΘD.\nThis yields\nΘ = MΣ−1\nXXMT CΘD−1 = MbD−1,\nfrom which we deduce that θg = ΘT ug = D−1bT (µg −¯µ). So, given a new input vector\nx, the decision rule is to assign it to the class g for which\n|θg −bT (x −¯µ)|2 = |ΘT ug −bT (x −¯µ)|2 = |D−1bT (µg −¯µ) −bT (x −¯µ)|2\nis minimal. Letting b1,...,br denote the r columns of b, this is equivalent to mini-\nmizing, in g\nr\nX\nj=1\n(bT\nj (µg −¯µ))2/λ2\nj −2\nr\nX\nj=1\n(bT\nj (x −¯µ))(bT\nj (µg −¯µ))/λj.\n(8.19)\nFrom b = Σ−1\nXXMT CΘ and Θ = MbD−1 we see that\nbD = Σ−1\nXXMT CMb,\nso that Σbb = ΣXXbD. This shows that the columns of b are solution of the eigenvalue\nproblem Σbu = λΣXXu. Moreover, from ΘT CΘ = IdRr, we get bT Σbb = D2. Since\nbT Σbb = bT ΣXXbD, we get that b must be normalized to that bT ΣXXb = D.\nThis shows that the solution of the optimal scoring problem can be reformulated\nuniquely in terms of b: if b1,...,br are the r principal solutions of the eigenvalue\nproblem Σbu = λΣXXu, normalized so that uT ΣXXu = λ, a new input is classified\ninto the class g minimizing\nr\nX\nj=1\nγj(µg)2/λ2\nj −2\nr\nX\nj=1\nγj(x)γj(µg)/λj.\nwith γj(x) = bT\nj (x −¯µ).\n\n192\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nRemark 8.8 The following computation shows that optimal scoring is closely re-\nlated to LDA. Recall the identity ΣXX = Σw + Σb. It implies that a solution of Σbu =\nλΣXXu is also a solution of Σbu = ˜λΣwu with ˜λ = λ/(1 −λ). If uT ΣXXu = λ, then\nuT Σwu = λ −uT Σbu = λ −λ2 =\n˜λ\n(1 + ˜λ)2,\nwhich shows that\n˜u = 1 + ˜λ\n√˜λ\nu\nsatisfies ˜uT Σw ˜u = 1. So,\nej =\n1 + ˜λj\nq\n˜λj\nbj\ncoincide with the LDA directions. We have, letting ˜γj(x) = eT\nj (x −¯µ) =\nq\n˜λjγj(x)/(1 +\n˜λj):\nr\nX\nj=1\nγj(µg)2/λ2\nj −2\nr\nX\nj=1\nγj(x)γj(µg)/λj =\nr\nX\nj=1\n˜γj(µg)2/ ˜λj −2\nr\nX\nj=1\nγj(x)γj(µg)/(1 + ˜λj)\nwhich relates the classification rules for the two methods.\n♦\nRemark 8.9 Optimal scoring can be modified by adding a penalty in the form\nγ\nr\nX\ni=1\nbT\ni Ωbi = γtrace(bT Ωb)\n(8.20)\nwhere Ωis a weight matrix. This only modifies the previous discussion by adding\nγΩ/N to both ΣXX and Σw.\n♦\n8.3.1\nKernel optimal scoring\nLet h : RX →H be the feature function and K the associated kernel, as usual. Opti-\nmal scoring in feature space requires to minimize\nN\nX\nk=1\n|θyk −a0 −b(h(xk))|2 + γ∥b∥2\nH,\n\n8.3. OPTIMAL SCORING\n193\nwhere we have introduced a penalty on b. Here, b is a linear operator from H to Rr,\ntherefore taking the form\nb(h) =\n\n\n⟨b1 , h⟩H\n...\n⟨br , h⟩H\n\n\nwith b1,...,br ∈H, and we take\n∥b∥2\nH =\nr\nX\ni=1\n∥bi∥2\nH.\nIt is once again clear (and the argument is left to the reader) that the problem\ncan be reduced to the finite dimensional space V = span(h(x1),...,h(xN)), and that\nthe optimal b1,...,br must take the form\nbj =\nN\nX\nl=1\nαlih(xl).\nIntroduce the kernel matrix K = K(x1,...,xN) with kth column denoted K(k). Let α\nbe the N by r matrix with entries αkj, k = 1,...,N,j = 1,...,r. Then b(h(xk)), which is\nthe vector with coordinates\n⟨bj , h(xk)⟩=\nN\nX\nl=1\nαliK(xk,xl),\nis equal to αT K(k). Moreover\n∥b∥2\nH =\nr\nX\nj=1\nN\nX\nk,l=1\nαkjK(xk,xl)αlj = trace(αT Kα).\nWe therefore need to minimize\nN\nX\nk=1\n|θyk −a0 −αT K(k)|2 + γtrace(αT Kα),\nso that the problem is reduced to penalized optimal scoring, with xk replaced by K(k),\nb replaced by α and the matrix Ωin (8.20) replaced by K. Introducing the matrix P =\nIdRN −11T /N and Kc = PK, the covariance matrix ΣXX becomes KT\nc Kc/N = KPK/N.\nThe class averages µg are equal to K1(g)/Ng while µ = K1/N, so that the matrix\nM is equal to\n\n\n1(g1)T /Ng1 −1T /N\n...\n1(gq)T /Ngq −1T /N\n\n\nK\n\n194\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nwhich gives Σb = MT CM = KQK, where Q is\nQ = PCP =\nX\ng∈RY\nNg\nN\n 1(g)\nNg\n−1\nN\n! 1(g)\nNg\n−1\nN\n!T\nSo, the columns of α are the r principal eigenvectors ρ1,...,ρr of the problem\nKQKρ = 1\nN (KPK + γK)ρ.\nGiven α, one then has, for any x ∈Rd,\n⟨bi , h(x)⟩H =\nN\nX\nk=1\nαkiK(x,xk)\nand\na0\n(i) = 1\nN\nN\nX\nk,l=1\nαkiK(xl,xk).\n8.4\nSeparating hyperplanes and SVMs\n8.4.1\nOne-layer perceptron and margin\nIn this whole section, we restrict to two-class problems, and let RY = {−1,1}. Given\na0 ∈R and b , 0 ∈Rd, the equation a0 + bT x = 0 defines a hyperplane in Rd. The\nfunction f (x) = sign(a0 + xT b) defines a classifier that attributes a class ±1 to x ac-\ncording to which side of the hyperplane it belongs (we ignore the ambiguity when\nx is on the hyperplane). With this notation, a pair (x,y), where y is the true class, is\ncorrectly classified if and only if y(a0 + xT b) > 0.\nLet T = (x1,y1),...,(xN,yN) denote, as usual, the training data. A hyperplane,\nrepresented by the parameters (a0,b) is separating for T if it correctly classifies all its\nsamples, i.e., if yk(a0 + xT\nk b) > 0 for k = 1,...,N. If such a hyperplane exists, one says\nthat T is linearly separable.\nThe perceptron algorithm computes a0 and b by minimizing\nL(β) =\nN\nX\nk=1\n[yk(a0 + xT\nk b)]−\n\n8.4. SEPARATING HYPERPLANES AND SVMS\n195\nwith u−= max(0,−u), , or more precisely, fixing a small positive number δ:\nL(β) =\nN\nX\nk=1\n[δ −yk(a0 + xT\nk b)]+ .\nThe problem can be recast as a linear program, i.e., minimize\nN\nX\nk=1\nξk\nsubject to ξk ≥0,ξk + yk(a0 + xT\nk b) −δ ≥0 for k = 1,...,N.\nHowever, when T is linearly separable, separating hyperplanes are not uniquely\ndefined, and there is in general (depending on the choice made for δ) an infinity\nof solutions to the perceptron problem. Intuitively, one should prefers a solution\nthat classifies the training data with some large margin, rather than one for which\ntraining points may be very close to the separating boundary (see fig. 8.2).\nFigure 8.2: The green line is preferable to the purple one in order to separate the data.\nThis leads to the maximum margin separating hyperplane classifier, also called\nlinear SVM, introduced by Vapnik and Chervonenkis [196, 197].\n8.4.2\nMaximizing the margin\nWe will use the following result.\nProposition 8.10 The distance of a point x ∈Rd to the hyperplane M : a0 + bT x = 0 is\ngiven by |a0 + xT b|/|b|.\n\n196\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nProof By definition, distance(x,M) = |x −πM(x)| where πM is the orthogonal projec-\ntion on M. Since b is normal to M and letting h = πM(x), we have x = λb + h so that\n|λb| = distance(x,M). Writing a0 +bT h = 0 in this equation implies a0 +bT x = λ|b|2 so\nthat |λ||b| = |a0 + xT b|/|b|.\n■\nAssume that T is linearly separable and let M : a0 + bT x = 0 be a separating\nhyperplane. The classification margin is defined as the minimal distance of the input\nvectors x1,...,xN to this hyperplane, i.e.,\nm(a0,b) = min{|a0 + xT\nk b|/|b| : k = 1,...,N}.\nBecause the hyperplane is separating, we have yk(a0 + xT\nk b) = |a0 + xT\nk b| for all k, so\nthat we also have\nm(a0,b) = min{yk(a0 + xT\nk b)/|b| : k = 1,...,N}.\nWe want to maximize this margin among all separating hyperplanes. This can be\nexpressed as maximizing, with respect to a0,b, the quantity\nmin{yk(a0 + xT\nk b)/|b| : k = 1,...,N}\nsubject to the constraint that the hyperplane is separating, namely\nyk(a0 + xT\nk b) ≥0, k = 1,...,N.\nIntroducing a new variable C representing the margin, the previous problem is\nequivalent to maximizing C subject to\nyk(a0 + xT\nk b) ≥C|b|, k = 1,...,N.\nThe problem is now overparametrized, and there is no loss of generality in en-\nforcing the additional constraint C|b| = 1. Noting that maximizing C is the same as\nminimizing |b|2, we can now reformulate the maximum margin hyperplane problem\nas minimizing |b|2/2 subject to\nyk(a0 + xT\nk b) ≥1, k = 1,...,N,\nwith C (the margin) given by C = 1/|b|. This results in a quadratic programming\nproblem.\nIf the data is not separable, there is no feasible point for this problem. To also\naccount for this situation (which is common), we can replace the constraint by a\npenalty and minimize, with respect to a0 and b:\n|b|2\n2 + γ\nN\nX\nk=1\n(1 −yk(a0 + xT\nk b))+\n\n8.4. SEPARATING HYPERPLANES AND SVMS\n197\nfor some γ > 0. (Recall that x+ = max(x,0).) This is equivalent to minimizing the\nperceptron objective function, with δ = 1, and with an additional penalty term equal\nto |b|2/(2γ). This minimization problem is equivalent to a quadratic programming\nproblem obtained by introducing slack variables ξk, k = 1,...,N and minimizing\n1\n2|b|2 + γ\nN\nX\nk=1\nξk ,\nsubject to the constraints ξk ≥0, yk(a0 + xT\nk b) + ξk ≥1, for k = 1,...,N.\n8.4.3\nKKT conditions and dual problem\nIntroduce Lagrange multipliers ηk ≥0 for ξk ≥0 and αk ≥0 for yk(a0 + xT\nk b) + ξk ≥1.\nThe Lagrangian is then given by\nL = 1\n2|b|2 + γ\nN\nX\nk=1\nξk −\nN\nX\nk=1\nηkξk −\nN\nX\nk=1\nαk\n\u0010\nyk(a0 + xT\nk b) + ξk −1\n\u0011\n.\nThe KKT conditions are\n\nb −\nN\nX\nk=1\nαkykxk = 0\nN\nX\nk=1\nαkyk = 0\nγ −ηk −αk = 0,\nk = 1,...,N\nξkηk = 0,\nk = 1,...,N\nαk\n\u0010\nyk(a0 + xT\nk b) + ξk −1\n\u0011\n= 0,\nk = 1,...,N\n(8.21)\nMinimizing L with respect to a0, b and ξ1,...,ξN and ensuring that the minimum\nis finite provides the first three KKT conditions. The resulting dual formulation\ntherefore requires to maximize\nN\nX\nk=1\nαk −1\n2\nN\nX\nk,l=1\nαkαlykylxT\nk xl\nsubject to the constraints 0 ≤αk ≤γ, PN\nk=1 αkyk = 0.\nWe now discuss the consequences of the complementary slackness conditions\nbased on the position of training sample relative to the separating hyperplane.\n\n198\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\n(i) First consider indices k such that (xk,yk) is correctly classified beyond the margin,\ni.e., yk(a0+xT\nk b) > 1. The last KKT condition and the constraint ξk ≥0 require αk = 0,\nand the third one then gives ξk = 0.\n(ii) For samples that are misclassified or correctly classified below the margin 5, i,e.,\nyk(a0 +xT\nk b) < 1, the constraint yk(a0 +xT\nk b)+ξk ≥1 implies ξk > 0, so that αk = γ and\nyk(a0 + xT\nk b) + ξk = 1.\n(iii) If (xk,yk) is correctly classified exactly at the margin, then ξk = 0 and there is\nno constrain on αk beside belonging to [0,γ]. Training samples that lie exactly at the\nmargin are called support vectors.\nGiven a solution α1,...,αN of the dual problem, one immediately recovers b via\nthe first equation in (8.21). For a0, one must, similarly to the regression case, rely on\nsupport vectors, which can be identified when 0 < αk < γ. In this case, one can take\na0 = yk −xT\nk b.\nIf no support vector is found, then a0 is not uniquely determined, and can be any\nvalue such that yk(a0 + bT xk) ≥1 if αk = 0 and yk(a0 + bT xk) ≤1 if αk = γ. This shows\nthat a0 can be any point in the interval [β−\n0,β+\n0 ] with\na0\n−= max{yk −xT\nk b : (yk = 1 and αk = 0) or (yk = −1 and αk = γ)}\na0\n+ = min{yk −xT\nk b : (yk = −1 and αk = 0) or (yk = 1 and αk = γ)}.\n8.4.4\nKernel version\nWe make the usual assumptions: h : RX →H is a feature map with values in an\ninner-product space with K(x,y) = ⟨h(x) , h(y)⟩H. The predictors take the form f (x) =\nsign(a0 + ⟨b , h(x)⟩H), a0 ∈R and b ∈H, and the goal is to minimize\n1\n2∥b∥2\nH + γ\nN\nX\nk=1\nξk ,\nsubject to ξk ≥0, yk(a0 + ⟨h(xk) , b⟩H) + ξk ≥1, k = 1,...,N.\nLet V = span(h(x1),...,h(xN)). The usual projection argument implies that the\noptimal b must belong to V and therefore take the form\nb =\nN\nX\nk=1\nukh(xk).\n5Note that, even if the training data is linearly separable, there are generally samples that are on\nthe right side of the hyperplane, but at a distance to the hyperplane strictly lower that the “nominal\nmargin” C = 1/|b|. This is due to our relaxation of the original problem of finding a separating\nhyperplane with maximal margin.\n\n8.4. SEPARATING HYPERPLANES AND SVMS\n199\nWe therefore need to minimize\n1\n2\nN\nX\nk,l=1\nukulK(xk,xl) + γ\nN\nX\nk=1\nξk ,\nsubject to\nyk\n\u0012\na0 +\nN\nX\nl=1\nK(xk,xl)al\n\u0013\n+ ξk ≥1\nfor k = 1,...,N. Introducing the same Lagrange multipliers as before, the Lagrangian\nis\nL = 1\n2\nN\nX\nk,l=1\nukulK(xk,xl) + γ\nN\nX\nk=1\nξk\n−\nN\nX\nk=1\nηkξk −\nN\nX\nk=1\nαk\n\u0012\nyk\n\u0012\na0 +\nN\nX\nl=1\nK(xk,xl)ul\n\u0013\n+ ξk −1\n\u0013\n.\nUsing vector notation, we have\nL = 1\n2uT Ku + ξT (γ1 −η −α) −a0αT y −(α ⊙y)T Ku + αT1\nwhere y ⊙α is the vector with coordinates ykαk. The infimum of L is −∞unless\nγ1−η −α = 0 and αT y = 0. If these identities are true, then the optimal u is u = α ⊙y\nand the minimum of L is\n−1\n2(α ⊙y)T K(α ⊙y) + αT1\nThe dual problem therefore requires to minimize\n1\n2(α ⊙y)T K(α ⊙y) −αT1 = αT (K ⊙yyT )α −αT1\nsubject to γ1 −η −α = 0 and αT y = 0.\nThis is exactly the same problem as the one we obtained in the linear case, up\nto the replacement of the Euclidean inner products xT\nk xl by the kernel evaluations\nK(xk,xl). Given the solution of the dual problem, the optimal b is\nb =\nX\nk\nukh(xk) =\nN\nX\nk=1\nαkykh(xk).\n\n200\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nIt is no computable, but the classification rule is explicit and given by\nf (x) = sign\n\na0 +\nN\nX\nk=1\nαkykK(xk,x)\n\n.\nSimilarly to the linear case, the coefficient a0 can be identified using a support\nvector, or is otherwise not uniquely determined. More precisely, if one of the αk’s is\nstrictly between 0 and γ, then a0 is given by a0 = yk −P\nl αlylK(xk,xl). Otherwise, a0\nis any number between\nβ−\n0 = max\nyk −\nX\nl\nαlylK(xk,xl) : (yk = 1 and αk = 0) or (yk = −1 and αk = γ)\n\nand\nβ+\n0 = min\nyk −\nX\nl\nαlylK(xk,xl) : (yk = −1 and αk = 0) or (yk = 1 and αk = γ)\n.\n\nChapter 9\nNearest-Neighbor Methods\nUnlike linear models, nearest-neighbor methods are completely non-parametric and\nassume no regularity on the decision rule or the regression function. In their sim-\nplest version, they require no training and rely on the proximity of a new observa-\ntion to those that belong to the training set. We will discuss in this chapter how\nthese methods are used for regression and classification, and study some of their\ntheoretical properties.\n9.1\nNearest neighbors for regression\n9.1.1\nConsistency\nWe let RX denote the input space, and RY = Rq be the output space. We assume that\na distance, denoted dist is defined on RX. This means that dist : RX × RX →[0,+∞]\n(we allow for infinite values) is a symmetric function such that dist(x,x′) = 0 if and\nonly if x = x′ and, for all x,x′,x′′ ∈RX\ndist(x,x′) ≤dist(x,x′′) + dist(x′′,x′),\nwhich is the triangle inequality.\nLet T = (x1,y1,...,xN,yN) be the training set. For x ∈RX, let\nDT (x) = (dist(x,xk),k = 1,...,N)\nbe the collection of all distances between x and the inputs in the training set. We\nconsider regression estimators taking the form\nˆf (x) =\nN\nX\nk=1\nWk(x)yk\n(9.1)\n201\n\n202\nCHAPTER 9. NEAREST-NEIGHBOR METHODS\nwhere W1(x),...,WN(x) is a family of coefficients, or weights, that only depends on\nDT (x).\nWe will, more precisely, use the following construction [183]. Assume that a\nfamily of numbers w1 ≥w2 ≥··· ≥wN ≥0 is chosen, with PN\nj=1 wj = 1. Given x ∈Rd\nand k ∈{1,...,N}, we let r+\nk (x) denote the number of indexes k′ such that dist(x,xk′) ≤\ndist(x,xk) and r−\nk (x) the number of such indexes such that d(x,xk′) < d(x,xk). The\ncoefficients defining ˆf in (9.1) are then chosen as:\nWk(x) =\nPr+\nk (x)\nk′=r−\nk (x)+1 wk′\nr+\nk (x) −r−\nk (x) .\n(9.2)\nTo emphasize the role of (w1,...,wN) is this definition, we will denote the resulting\nestimation as ˆfw. If there is no tie in the sequence of distances between x and ele-\nments of the training set, then r+\nk (x) = r−\nk (x)+1 is the rank of xk when training data is\nordered according to their proximity to x, and Wk(x) = wr+\nk (x). In this case, defining\nl1,...,lN such that d(x,xl1) < ··· < d(x,xlN), we have\nˆfw(x) =\nN\nX\nj=1\nwjylj.\nIn the general case, the weights wj associated with tied observations are averaged.\nIf p is an integer, the p-nearest neighbor (p-NN) estimator (that we will denote\nˆfp) is associated to the weights wj = 1/p for j = 1,...,p and 0 otherwise. If there is no\ntie for the definition of the pth nearest neighbor of x, Wk(x) = 1/p if k is among the\np nearest-neighbors and Wk(x) = 0 otherwise, so that ˆfp is the average of the output\nvalues over these p nearest neighbors. If the pth nearest neighbors are tied, their\noutput value is averaged before being used in the sum. For example, assume that\nN = 5 and p = 2 and let the distances between x and xk for k = 1,...,5 be respectively\n9,3,2,4,6. Then ˆf2(x) = (y2 + y3)/2. If the distances were 9,3,2,3,6, then we would\nhave ˆf2(x) = (y2 + y4)/4 + y3/2.\nWhen RX = Rd and d(x,x′) = |x −x′|, the following result is true.\nTheorem 9.1 ([183]) Assume that E(Y 2) < ∞. Assume that, for each N, a sequence\nw(N) = w(N)\n1\n≥··· ≥w(N)\nN\n≥0 is chosen with PN\nj=1 w(N)\nj\n= 1. Assume, in addition, that\n(i) limN→∞w(N)\n1\n= 0\n(ii) limN→∞\nP\nj≥αN w(N)\nj\n→0, for some α ∈(0,1).\n\n9.1. NEAREST NEIGHBORS FOR REGRESSION\n203\nThen the corresponding classifier ˆfw(N) converges in the L2 norm to E(Y | X):\nE\n\u0010\n| ˆfw(N)(X) −E(Y | X)|2\u0011\n→0.\nFor nearest-neighbor regression, (i) and (ii) mean that the number of nearest neighbors\npN must be chosen such that pN →∞and pN/N →0.\nProof We give a proof under the assumption that f : x 7→E(Y | X = x) is uniformly\ncontinuous and bounded (one can, in fact, prove that it is always possible to reduce\nto this case).\nTo lighten the notation, we will not make explicit the dependency on N in of\nquantities such as W or w. One has\nˆfw(X) −E(Y | X) =\nN\nX\nk=1\nWk(X)(f (Xk) −f (X)) +\nN\nX\nk=1\nWk(X)(Yk −f (Xk))\n(9.3)\nand the two sums can be addressed separately.\nWe start with the first sum and write, by Schwartz’s inequality:\n\n\nX\nk\nWk(X)(f (Xk) −f (X))\n\n\n2\n≤\nX\nk\nWk(X)(f (Xk) −f (X))2.\nIt therefore suffices to study the limit of E(P\nk Wk(X)(f (Xk) −f (X))2. Fix ϵ > 0. By\nassumption, there exists M,a > 0 such that |f (x)| ≤M for all x and |x −y| ≤a ⇒\n|f (x) −f (y)|2 ≤ϵ. Then\nE\n\n\nX\nk\nWk(X)(f (Xk) −f (X))2\n\n=E\n\n\nX\nk\nWk(X)(f (Xk) −f (X))2 1|Xk−X|≤a\n\n\n+ E\n\n\nX\nk\nWk(X)(f (Xk) −f (X))2 1|Xk−X|>a\n\n\n≤ϵ2 + 4M2E\n\n\nX\nk\nWk(X)1|Xk−X|>a\n\n.\nSince ϵ can be made arbitrarily small, we need to show that, for any positive a, the\nsecond term in the upper-bound tends to 0 when N →∞. We will use the follow-\ning fact, which requires some minor measure theory argument to prove rigorously.\nDefine\nS = {x : ∀δ > 0,P(|X −x| < δ) > 0}.\n\n204\nCHAPTER 9. NEAREST-NEIGHBOR METHODS\nThis set is called the support of X. Then, one can show that P(X ∈S) = 1. This\nmeans that, if ˜X is independent from X with the same distribution, then, for any\nδ > 0, P(|X −˜X| < δ|X) > 0 with probability one. 1\nLet Na(x) = |{k : |Xk −x| ≤a}|. We have, for all x ∈S and a > 0, and using the law\nof large numbers,\nNa(x)\nN\n= 1\nN\nN\nX\nk=1\n1|Xk−x|≤a →P(|X −x| ≤a) > 0.\nIf |X −Xk| > a, then r−\nk (X) > Na(x) so that\nX\nk\nWk(X)1|Xk−X|>a ≤\nX\nj≥Na(X)\nwj,\nand we have, taking 0 < α < P(|X −x| ≤a),\nE\n\n\nX\nj≥Na(X)\nwj\n\n≤\nX\nj≥αN\nwj + P(Na(X) < αN)\nand both terms in the upper bound converge to 0. This shows that the first sum in\n(9.3) tends to 0.\nWe now consider the second sum in (9.3). Let Zk = Yk −E(Y | Xk). We have\nE(Zk | Xk) = 0 and E(Z2\nk ) < ∞. We can write\nE\n\n\n\f\f\f\f\f\f\f\nN\nX\nk=1\nWk(X)Zk\n\f\f\f\f\f\f\f\n2\n\n= E\n\nE\n\n\n\f\f\f\f\f\f\f\nN\nX\nk=1\nWk(X)Zk\n\f\f\f\f\f\f\f\n2 \f\f\f\fX,X1,...,XN\n\n\n\n\n= E\n\n\nN\nX\nk=1\nWk(X)2E(Z2\nk | Xk)\n\n\n+\nN\nX\nk,l=1\nE(Wk(X)Wl(X)E(ZkZl | Xi,Xj))\n1This statement is proved as follows (with the assumption that X is Borel measurable). Let Sc\ndenote the complement of S. Then Sc is open. Indeed if x < S, there exists δx > 0 such that, letting\nB(x,δx) denote the open ball with radius δx, P(X ∈B(x,δx)) = 0. Then P(X ∈B(x′,δx/3)) = 0 as soon as\n|x −x′| < δx/3, so that B(x,δx/3) ⊂Sc.\nIf K ⊂Sc is compact, then K ⊂S\nx∈K B(x,δx) and one can find a finite subset M ⊂K such that\nK ⊂S\nx∈M B(x,δx), which proves that P(X ∈K) = 0. Since P(X ∈Sc) = maxK P(X ∈K) where the\nmaximum is over all compact subsets of Sc, we find P(X ∈Sc) = 0 as required.\n\n9.1. NEAREST NEIGHBORS FOR REGRESSION\n205\nThe cross products in the last term vanish because E(Zk | Xk) = 0 and the samples\nare independent. So it only remains to consider\nE\n\n\nN\nX\nk=1\nWk(X)2E(Z2\nk | Xk)\n\n\nThe random variable E(Zk | Xk) = E(Y 2\nk | Xk) −E(Yk | Xk)2 is a fixed non-negative\nfunction of Xk, that we will denote h(Xk). We have\nE\n\n\nN\nX\nk=1\nWk(X)2h(Xk)\n\n≤w1E\n\n\nN\nX\nk=1\nWk(X)h(Xi)\n\n\nwith w1 →0 and the proof is concluded by showing that E\n\u0010PN\nk=1 Wk(X)h(Xk)\n\u0011\nis\nbounded.\nRecall that the weights Wk are functions of X and of the whole training set,\nand we will need to make this dependency explicit and write Wi(X,TX) where TX =\n(X1,...,XN). Similarly, the ranks in (9.2) will be written r+\nj (X,TX) and r−\nj (X,TX).\nBecause X,X1,...,XN are i.i.d., we can switch the role of X and Xk in the kth term\nof the sum, yielding\nE\n\n\nN\nX\nk=1\nWk(X,TX)h(Xk)\n\n= E\n\n\n\n\nN\nX\ni=1\nWk(Xk,T (k)\nX )\n\nh(X)\n\n\nwith T (k)\nX = (X1,...,Xk−1,X,Xk+1,...,XN). We now show that PN\nk=1 Wk(Xk,T (k)\nX ) is bounded\nindependently of X,X1,...,XN.\nFor this purpose, we group X1,...,XN according to approximate alignment with\nX. For u ∈Rd with |u| = 1 and for δ ∈(0,π/4), denote by Γ(u,δ) the cone formed by\nall vectors v in Rd such that ⟨v , u⟩> |v|cosδ (i.e., the angle between v and u is less\nthan δ). Notice that if v,v′ ∈Γ(u,δ), then ⟨v , v′⟩≥cos(2δ)|v||v′| and if |v′| ≤|v|, then\n|v|2 −|v −v′|2 = |v′|(2|v|cos(2δ) −|v′|) > 0\n(9.4)\nbecause cos(2δ) > 1/2.\nFixing δ, let Cd(δ) be the minimal number of such cones needed to cover Rd.\nChoosing such a covering Γ(u1,δ),...,Γ(uM,δ) where M = Cd(δ), we define the fol-\nlowing subsets of {1,...,M}:\nI0 = {k : Xk = X}\nIq =\nn\nk < I0 : Xk −X ∈Γ(uq,δ)\no\n,\nq = 1,...,M\n\n206\nCHAPTER 9. NEAREST-NEIGHBOR METHODS\n(these sets may overlap). We have\nN\nX\nk=1\nWk(Xk,T (k)\nX ) ≤\nM\nX\nq=0\nX\nk∈Iq\nWk(Xk,T (k)\nX )\nIf k ∈I0, then r−\nk (Xk,T (k)\nX ) = 0 and r+\nk (Xk,T (k)\nX ) = c with c = |I0|. This implies that, for\nk ∈I0, we have Wk(Xk,T (k)\nX ) = Pc\nj=1 wj/c and\nX\nk∈I0\nWk(Xk,T (k)\nX ) =\nc\nX\nj=1\nwj.\nWe now consider Iq with q ≥1. Write Iq = {i1,...,ir} ordered so that |Xij −X| is\nnon-decreasing. If j′ < j, we have (using (9.4)) |Xij −Xij′| < |X −Xij|. This implies that\nr−\nij(Xij,T\n(ij)\nX ) ≥j −1 and r+\nij (Xij,T\n(ij)\nX ) −r−ij(Xij,T\n(ij)\nX ) ≥c + 1. Therefore,\nWij(Xij,T\n(ij)\nX ) ≤\n1\nc + 1\nc+j\nX\nj′=j\nwj′\nand\nX\nk∈Iq\nWk(Xk,T (k)\nX ) ≤\n1\nc + 1\nN\nX\nj=1\nc+j\nX\nj′=j\nwj′ =\n1\nc + 1\n\n\nc\nX\nj′=1\nj′wj′ + (c + 1)\nN\nX\nj′=c+1\nwj′\n\n.\nThis yields\nN\nX\nk=1\nWk(Xk,T (k)\nX ) ≤\nc\nX\nj=1\nwj + Cd(δ)\n\n\n1\nc + 1\nc\nX\nj′=1\nj′wj′ +\nN\nX\nj′=c+1\nwj′\n\n≤Cd(δ) + 1.\nWe therefore have\nE\n\n\nN\nX\nk=1\nWk(X)2E(Z2\nk | Xk)\n\n≤w1(Cd(δ) + 1)E(h(X)) →0,\nwhich concludes the proof.\n■\nTheorem 9.1 is proved in Stone [183] with weaker hypotheses allowing for more\nflexibility in the computation of distances, in which, for example, differences X −Xi\ncan be normalized by dividing them by a factor σi that may depend on the training\nset. These relaxed assumptions slightly complicate the proof, and we refer the reader\nto Stone [183] for a complete exposition.\n\n9.1. NEAREST NEIGHBORS FOR REGRESSION\n207\n9.1.2\nOptimality\nThe NN method can be shown to be optimal over some classes of functions. Opti-\nmality is in the min-max sense, and works as follows. We assume that the regression\nfunction f (x) = E(Y | X = x) belongs to some set F of real-valued functions on Rd.\nMost of the time, the estimation methods must be adapted to a given choice of F ,\nand various choices have arisen in the literature: classes of functions with r bounded\nderivatives, Sobolev or related spaces, functions whose Fourier transforms has given\nproperties, etc.\nConsider now an estimator of f , denoted ˆfN, based on a training set of size N.\nWe can measure the error by, say:\n\r\r\r ˆfN −f\n\r\r\r2 =\n Z\nRd( ˆfN(x) −f (x))2dx\n!1/2\nSince ˆfN is computed from a random sample, this error is a random variable. One\ncan study, when bN →0, the probability\nPf\n\u0010\n∥ˆfN −f ∥2\n2 ≥cbN\n\u0011\nfor some constant c and, for example, for the model: Y = f (X) + noise. Here, the\nnotation Pf refers to the model assumption indicating the unobserved function f .\nThe min-max method considers the worst case and computes\nMN(c) = sup\nf ∈F\nPf\n\u0010\n∥ˆfN −f ∥2\n2 ≥cbN\n\u0011\n.\nThis quantity now only depends on the estimation algorithm. One defines the no-\ntion of “lower convergence rate” as a sequence bN such that, for any choice of the\nestimation algorithm, MN(c) can be found arbitrarily close to 1 (i.e., ∥ˆfN −f ∥2\n2 ≥cbN\nwith arbitrarily high probability for all f ∈F ), for arbitrarily large N (and for some\nchoice of c). The mathematical statement is\n∃c > 0 : liminf\nN→∞MN(c) = 1.\nSo, if bN is a lower convergence rate, then, for every estimator, there exists a constant\nc such that the accuracy cbN cannot be achieved.\nOn the other hand, one says that bN is an achievable rate of convergence if there\nexists an estimator such that, for some c′,\nlimsup\nN→∞\nMN(c′) = 0.\n\n208\nCHAPTER 9. NEAREST-NEIGHBOR METHODS\nThis says that for large N, and for some c′, the accuracy is higher than c′bN for the\ngiven estimator. Notice the difference: a lower rate holds for all estimators, and an\nachievable rate for at least one estimator.\nThe final definition of a min-max optimal rate is that it is both a lower rate and\nan achievable rate (obviously for different constants c and c′). And an estimator is\noptimal in the min-max sense if it achieves an optimal rate.\nOne can show that the p-NN estimator is optimal (under some assumptions on\nthe ratio pN/N) when F is the class of Lipschitz functions on Rd, i.e., the class of\nfunctions such that there exists a constant K with\n|f (x) −f (y)| ≤K|x −y|\nfor all x,y ∈Rd. In this case, the optimal rate is bN = N −1/(2+d) (notice again the\n“curse of dimensionality”: to achieve a given accuracy in the worst case, the number\nof data points must grow exponentially with the dimension).\nIf the function class consists of smoother functions (for example, several deriva-\ntives), the p-NN method is not optimal. This is because the local averaging method\nis too crude when one knows already that the function is smooth. But it can be\nmodified (for example by fitting, using least squares, a polynomial of some degree\ninstead of computing an average) in order to obtain an optimal rate.\n9.2\np-NN classification\nLet (x1,y1,...,xN,yN) be the training set, with xi ∈Rd and yi ∈RY where RY is a\nfinite set of classes. Using the same notation as in the previous section, define\nbπw(y|x) =\nN\nX\nk=1\nWk(x)1yk=y.\nLet the corresponding classifier be\nˆfw(x) = argmax\ny∈RY\nbπw(y|x).\nTheorem 9.1 may be applied, for y ∈RY, to the function fy(x) = π(y | x) = E(1Y=y |\nX = x), which allows one to interpret the estimator bπ(y | x) as a nearest-neighbor\npredictor of the random variable 1Y=y as a function of X. We therefore obtain the\nconsistency of the estimated posteriors when N →∞under the same assumption as\nthose of theorem 9.1. This implies that, for large N, the classification will be close to\nBayes’s rule.\n\n9.2. p-NN CLASSIFICATION\n209\nAn asymptotic comparison with Bayes’s rule can already be made with p = 1. Let\nˆyN(x) be the 1-NN estimator of Y given x and a training set of size N, and let ˆy(x) be\nthe Bayes estimator. We can compute the Bayes error by\nP( ˆy(X) , Y)\n=\n1 −P( ˆy(X) = Y)\n=\n1 −E(P( ˆy(X) = Y|X))\n=\n1 −E(max\ny∈RY\nπ(y|X))\nFor the 1-NN rule, we have\nP( ˆyN(X) , Y)\n=\n1 −P( ˆyN(X) = Y)\n=\n1 −E(P( ˆyN(X) = Y|X))\nLet us make the assumption that nearest neighbors are not tied (with probability\none). Let k∗(x,T ) denote the index of the nearest neighbor to x in the training set T.\nWe have\nP( ˆyN(X) = Y | X)\n= E(P( ˆyN(X) = Y | X,T ))\n= E\n\u0012 N\nX\nk=1\nP(Y = Yk | X,T )1k∗(X,T )=k\n\u0013\n= E\n\u0012 N\nX\nk=1\nP(Y = Yk | X,Xk)1k∗(X,T )=k\n\u0013\n= E\n\u0012 N\nX\nk=1\nX\ng∈RY\nP(Y = g,Yk = g | X,Xk)1k∗(X,T )=k\n\u0013\n= E\n\u0012 N\nX\nk=1\nX\ng∈RY\nπ(g | X)π(g | Xk)χk∗(X,T )=k\n\u0013\n= E\n\u0012 X\ng∈RY\nπ(g | X)π(g | Xk∗(X,T ))\n\u0013\nNow, assume the continuity of x 7→π(g | x) (although the result can be proved\nwithout this simplifying assumption). We know that Xk∗(X,T ) →X when N →∞(see\nthe proof of theorem 9.1), which implies that π(g | Xk∗(X,T )) →π(x | X) and at the\nlimit\nP( ˆyN(X) = Y | X) →\nX\ng∈RY\nπ(g | X)2.\n\n210\nCHAPTER 9. NEAREST-NEIGHBOR METHODS\nThis implies that the asymptotic 1-NN misclassification error is always smaller\nthan 2 times the Bayes error, that is\n1 −E\n\u0012 X\ng∈RY\nπ(g | X)2\u0013\n≤2(1 −E(max\ng\nπ(g | X)))\nIndeed, the left-hand term is smaller than 1 −E(maxg π(g|x)2) and the result comes\nfrom the fact that for any t ∈R. 1 −t2 ≤2 −2t.\nRemark 9.2 Nearest neighbor methods may require large computation time, since,\nfor a given x, the number of comparisons which are needed is the size of the training\nset. However, efficient (tree-based) search algorithms can be used in many cases to\nreduce it to a logarithm in the size of the database, which is acceptable. A reduction\nof the size of the training set by clustering also is a possibility for improving the\nefficiency.\nThe computation time is also generally proportional to the dimension d of the\ninput x. When d is large, a reduction of dimension is often a good idea. Principal\ncomponents (see chapter 20), or LDA directions (see chapter 8) can be used for this\npurpose.\n♦\n9.3\nDesigning the distance\nLDA-based distance\nThe most important factor in the design of a NN procedure\nprobably is the choice of the distance, something we have not discussed so far. In-\ntuitively, the distance should increase fast in the directions “perpendicular” to the\nregions of constancy of the class variables, and slowly (ideally not at all) within these\nregions. The following construction uses discriminant analysis [87].\nFor g ∈RY, let Σg be the covariance matrix in class g, and Σw = P\ng∈RY πgΣg be\nthe within-class variance, where πg is the frequency of class g. Let Σb denote the\nbetween-class covariance matrix (see section 8.2).\nFor x ∈Rd, define the spherized vector x∗= Σ−1/2\nw\nx. The between-class variance\ncomputed for spherized data is Σ∗\nb = Σ−1/2\nw\nΣbΣ−1/2\nw\n. A direction is discriminant if it is\nclose to the principal eigenvectors of Σ∗\nb. This suggests the introduction of the norm\n|x|2\n∗= (x∗)T Σ∗\nbx∗= xT Σ−1/2\nw\n(Σ−1/2\nw\nΣbΣ−1/2\nw\n)Σ−1/2\nw\nx = xT Σ−1\nw ΣbΣ−1\nw x.\nThis replaces the standard Euclidean norm (the method can be made more robust\nby adding ϵIdRd to Σ∗\nb.)\n\n9.3. DESIGNING THE DISTANCE\n211\nTangent distance\nDesigning the distance, however, can sometimes be based on a\npriori knowledge on some invariance properties associated with the classes. A suc-\ncessful example comes from character recognition, where it is known that trans-\nforming images by slightly rotating, scaling, or translating the character should not\nchange its class. This corresponds to the following general framework.\nFor each input x ∈Rd, assume that one can make small transformations without\nchanging the class of x. We model these transformations as parametrized functions\nx 7→xθ = ϕ(x,θ) ∈Rd, such that ϕ(x,0) = x and ϕ is smooth in θ, which is a q-\ndimensional parameter. The assumption is that ϕ(x,θ) and x should be from the\nsame class, at least for small θ. This will be used to improve on the Euclidean dis-\ntance on Rd.\nTake x,x′ ∈Rd. Ideally, one would like to use the distance D(x,x′) = infθ,θ′ dist(xθ,xθ′)\nwhere θ and θ′ are restricted to a small neighborhood of 0. A more tractable expres-\nsion can be based on first-order approximations\nxθ ≃x + ∇θϕ(x,0)u = x +\nq\nX\ni=1\nui∂θiϕ(x,0)\nand\nx′\nθ ≃x′ + ∇θϕ(x′,0)u′ = x′ +\nq\nX\ni=1\nu′\ni∂θiϕ(x′,0)\nyielding the approximation (also called the tangent distance)\nD(x,x′)2 ≃\ninf\nu,u′∈Rq\n\r\r\rx −x′ + ∇θϕ(x,0)u −∇θϕ(x′,0)u′\r\r\r2 .\nThe computation now is a simple least-squares problem, for which the solution is\ngiven by the system\n \n∇θϕ(x,0)T ∇θϕ(x,0)\n−∇θϕ(x,0)T ∇θϕ(x′,0)\n−∇θϕ(x′,0)T ∇θϕ(x,0)\n∇θϕ(x′,0)T ∇θϕ(x′,0)\n! \nu\nv\n!\n=\n \n∇θϕ(x,0)T (x′ −x)\n∇θϕ(x′,0)T (x −x′)\n!\n.\nA slight modification, to ensure that the norms of u and u′ are not too large, is to\nadd a penalty λ(|u|2 + |u′|2), which results in adding λIdRq to the diagonal blocs of\nthe above matrix.\n\n212\nCHAPTER 9. NEAREST-NEIGHBOR METHODS\n\nChapter 10\nTree-based Algorithms, Randomization and Boost-\ning\n10.1\nRecursive Partitioning\nRecursive partitioning methods implement a “divide and conquer” strategy to ad-\ndress the prediction problem. They separate the input space RX into small regions\non which prediction is “easy,” i.e., such that the observed values of the output vari-\nable are (almost) constant for input values in these regions. The regions are esti-\nmated by recursive divisions until they become either too small or homogeneous.\nThese divisions are conveniently represented in the form of binary trees.\n10.1.1\nBinary prediction trees\nDefine a binary node to be a structure ν that contains the following information (note\nthat the definition is recursive):\n• A label L(ν) that uniquely identifies the node.\n• A set of children, C(ν), which is either empty or a pair of nodes (l(ν),r(ν)).\n• A binary feature, i.e., a function γν : RX →{0,1}, which is “None” (i.e., irrelevant)\nif the node has no children.\n• A predictor, fν : RX →RY, which is “None” if the node has children.\nA node without children is called a terminal node, or a leaf.\nA binary prediction tree T is a finite set of nodes, with the following properties:\n(i) Only one node has no parent (the root, denoted ρ or ρT);\n213\n\n214\nCHAPTER 10. TREE-BASED ALGORITHMS\n(ii) Each other node has exactly one parent;\n(iii) No node is a descendent of itself.\n10.1.2\nTraining algorithm\nAssume that a family Γ of binary features γ : RX →{0,1} is chosen, together with a\nfamily F of predictors f : RX →RY. Assume also the existence of two “algorithms”\nas follows:\n• Feature selection: Given the feature set Γ and a training set T, return an optimized\nbinary feature b\nγT,Γ ∈Γ.\n• Predictor optimization: Given the predictor set F and a training set T, return an\noptimized predictor ˆfT,F ∈F .\nFinally, assume that a stopping rule is defined, as a function of training sets σ : T 7→\nσ(T ) ∈{0,1}, where 0 means “continue”, and 1 means “stop”.\nGiven a training set T0, the algorithm builds a binary tree T using a recursive\nconstruction. Each node ν ∈T will be associated to a subset of T0, denoted Tν. We\ndefine below a recursive operation, denoted Node(T ,j) that adds a node ν to a tree\nT given a subset T of T0 and a label j. Starting with T = ∅, calling Node(T0,0) will\nthen create the desired tree.\nAlgorithm 10.1 (Node insertion: Node(T,j))\n(a) Given T and j, let Tν = T and L(ν) = j.\n(b) If σ(T) = 1, let C(ν) = ∅, γν = “None” and fν = ˆfT,F .\n(c) If σ(T ) = 0, let fν = “None”, γν = b\nγT,Γ and C(ν) = (l(ν),r(ν)) with\nl(ν) = Node(Tl,2j + 1),\nr(ν) = Node(Tr,2j + 2)\nwhere\nTl = {(x,y) ∈T : γν(x) = 0},\nTr = {(x,y) ∈T : γν(x) = 1}\n(d) Add ν to T and return.\nRemark 10.1 Note that, even though the learning algorithm for prediction trees can\nbe very conveniently described in recursive form as above, efficient computer im-\nplementations should avoid recursive calls, which may be inefficient and memory\ndemanding. Moreover, for large trees, it is likely that recursive implementations\nwill reach the maximal number of recursive calls imposed by compilers.\n♦\n\n10.1. RECURSIVE PARTITIONING\n215\n10.1.3\nResulting predictor\nOnce the tree is built, the predictor x 7→ˆfT(x) is recursively defined as follows.\n(a) Initialize the computation with ν = ρ.\n(b) At a given step of the algorithm, let ν be the current node.\n• If ν has no children: then let ˆfT(x) = fν(x).\n• Otherwise: replace ν by l(ν) if γν(x) = 0 and by r(ν) if γν(x) = 1 and go back to\n(b).\n10.1.4\nStopping rule\nThe function σ, which decides whether a node is terminal or not is generally defined\nbased on very simple rules. Typically, σ(T) = 1 when one the following conditions is\nsatisfied:\n• The number of training examples in T is small (e.g., less than 5).\n• The values yk in T have a small variance (regression) or are constant (classifica-\ntion).\n10.1.5\nLeaf predictors\nWhen one reaches a terminal node ν (so that σ(Tν) = 1), a predictor fν must be\ndetermined. This function can be optimized within any set F of predictors, using\nany learning algorithm, but in practice, one usually makes this fairly simple and\ndefines F to be the family of constant functions taking values in RY. The function\nˆfT ,F is then defined as:\n• the average of the values of yk, for (xk,yk) ∈T (regression);\n• the mode of the distribution of yk, for (xk,yk) ∈T (classification).\n10.1.6\nBinary features\nThe space Γ of possible binary features must be specified in order to partition non-\nterminal nodes. A standard choice, used in the CART model [43] with RX = Rd,\nis\nΓ =\nn\nγ(x) = 1[x(i)≥θ],i = 1,...,d,θ ∈R\no\n(10.1)\nwhere x(i) is the ith coordinate of x. This corresponds to splitting the space using a\nhyperplane parallel to one of the coordinate axes.\n\n216\nCHAPTER 10. TREE-BASED ALGORITHMS\nThe binary function b\nγT,Γ can be optimized over Γ using a greedy evaluation of\nthe risk, assuming that the prediction is based on the two nodes resulting from the\nsplit. For γ ∈Γ, f0,f1 ∈F , define\nFγ,f0,f1(x) =\n(f0(x) if γ(x) = 0\nf1(x) if γ(x) = 1\nGiven a risk function r, one then evaluates\nET (γ) = min\nf0,f1∈F\nX\n(x,y)∈T\nr(y,Fγ,f0,f1(x))\nOne then chooses b\nγT,Γ = argminγ∈Γ(ET (Γ)).\nExample 10.2 (Regression) Consider the regression case, taking squared differences\nas risk and letting F contain only constant functions. Then\nET (γ) = min\nm0,m1\nX\n(x,y)∈T\n\u0010\n(y −m0)21γ(x)=0 + (y −m1)21γ(x)=1\n\u0011\n.\nObviously, the optimal m0 and m1 are the averages of the output values, y, in each\nof the subdomains defined by γ. For CART (see (10.1)), this cost must be minimized\nover all choices (i,θ) with i = 1,...,d and θ ∈R where γi,θ(x) = 1 if x(i) > θ and 0\notherwise.\n♦\nExample 10.3 (Classification.) For classification, one can apply the same method,\nwith the 0/1 loss, letting\nET (γ) = min\ng0,g1\nX\n(x,y)∈T\n\u0010\n1y,g01γ(x)=0 + 1y,g11γ(x)=1\n\u0011\n.\nThe optimal g0 and g1 are the majority classes in T ∩{γ = 0} and T ∩{γ = 1}.\n♦\nExample 10.4 (Entropy selection for classification) For classification trees, other\nsplitting criteria may be used based on the empirical probability pT on the set T,\ndefined as\npT (A) = 1\nN |{k : (xk,yk) ∈A}|\nfor A ⊂RX × RY. The previous criterion, ET (γ), is proportional to\npT (γ = 0)(1 −max\ng\npT (g | γ = 0)) + pT (γ = 1)(1 −max\ng\npT (g | γ = 1)).\nOne can define alternative objectives in the form\npT (γ = 0)H(pT (g | γ = 0)) + pT (γ = 1)H(pT (g | γ = 1))\n\n10.1. RECURSIVE PARTITIONING\n217\nwhere π →H(π) associates to a probability distribution π a “complexity measure”\nthat is minimal when π is concentrated on a single class (which is the case for π 7→\n1 −maxg π(g)).\nMany such measures exists, and many of them are defined as various forms of\nentropy designed in information theory. The most celebrated is Shannon’s entropy\n[176], defined by\nH(p) = −\nX\ng∈RY\np(g)logp(g).\nIt is always positive, and minimal when the distribution is concentrated on a single\nclass. Other entropy measures include:\n• The Tsallis entropy: H(p) =\n1\n1−q\nP\ng∈RY (p(g)q−1), for q , 1. (Tsallis entropy for q = 2\nis sometimes called the Gini impurity index.)\n• The Renyi entropy: H(p) =\n1\n1−q logP\ng∈RY p(g)q, for q ≥0,q , 1.\n♦\n10.1.7\nPruning\nGrowing a decision tree to its maximal depth (given the amount of available data)\ngenerally leads to predictors that overfit the data. The training algorithm is usually\nfollowed by a pruning step that removes some some nodes based on a complexity\npenalty.\nLetting τ(T) denote the set of terminal nodes in the tree T and ˆfT the associated\npredictor, pruning is represented as an optimization problem, where one minimizes,\ngiven the training set T,\nUλ(T,T ) = ˆRT ( ˆfT) + λ|τ(T)|\nwhere ˆRT is as usual the in-sample error measured on the training set T.\nTo prune a tree, one selects one or more internal nodes and remove all their\ndescendants (so that these nodes become terminal). Associate to each node ν in T its\nlocal in-sample error ETν equal to the error made by the optimal classifier estimated\nfrom the training data associated with ν. Then,\nUλ(T,T ) =\nX\nν∈τ(T)\n|Tν|\n|T | ETν + λ|τ(T)|\nIf ν is a node in T (internal or terminal), let Tν be the subtree of T containing\nν as a root and all its descendants. Let T(ν) be the tree T will all descendants of ν\n\n218\nCHAPTER 10. TREE-BASED ALGORITHMS\nremoved (keeping ν). Then\nUλ(T,T ) = U0(T(ν),T ) −|Tν|\n|T | (ETν −U0(Tν,Tν)) + λ(|τ(Tν)| −1).\nNote also that, if ν is internal, and ν′, ν′′ are its children, then\nU0(Tν,Tν) = |Tν′|\n|Tν| U0(Tν′,Tν′) + |Tν′′|\n|Tν| U0(Tν′′,Tν′′)\nThis formula can be used to compute U0(Tν) recursively for all nodes, starting with\nleaves for which U0(Tν) = E(Tν). (We also have |τ(Tν)| = |τ(Tν′)| + |τ(Tν′′)|.) The\nfollowing algorithm converges to a global minimizer of Uλ.\nAlgorithm 10.2 (Pruning)\n(1) Start with a complete tree T(0) built without penalty.\n(2) Compute, for all nodes U0(Tν) and |τ(Tν)|. Let\nψν = |Tν|\n|T| (ETν −U0(Tν)) −λ(|τ(Tν)| −1).\n(3) Iterate the following steps.\n• If ψν < 0 for all internal nodes ν, exit the program and return the current T(n).\n• Otherwise choose an internal node ν such that ψν is largest.\n• Let T(n + 1) = T(ν)(n). Subtract λ(|τ(Tν(n))| −1) to ρν′ for all ν′ ancestor of ν.\n10.2\nRandom Forests\n10.2.1\nBagging\nA random forest [7, 42] is a special case of composite predictors (we will see other\nexamples later in this chapter when describing boosting methods) that train mul-\ntiple individual predictors under various conditions and combine them, through\naveraging, or majority voting. With random forests, one generates individual trees\nby randomizing the parameters of the learning process. One way to achieve this is\nto randomly sample from the training set before running the training algorithm.\nLetting as before T0 = (x1,y1,...,xN,yN) denote the original set, with size N, one\ncan create “new” training data by sampling with replacement from T0. More pre-\ncisely, consider the family of independent random variables ξ = (ξ1,...,ξN), with\n\n10.2. RANDOM FORESTS\n219\neach ξj following a uniform distribution over {1,...,N}. One can then form the ran-\ndom training set\nT0(ξ) = (xξ1,yξ1,...,xξN,yξN).\nRunning the training algorithm using T0(ξ) then provides a random tree, denoted\nT(ξ). Now, by sampling K realizations of ξ, say ξ(1),...,ξ(K), one obtains a collection\nof K random trees (a random forest) T∗= (T1,...,TK), with Tj = T(ξ(j)) that can\nbe combined to provide a final predictor. The simplest way to combine them is to\naverage the predictors returned by each tree (assuming, for classification, that this\npredictor is a probability distribution on classes), so that\nfT∗(x) = 1\nK\nK\nX\nj=1\nfTj(x).\n(10.2)\nFor classification, one can alternatively let each individual tree “vote” for their most\nlikely class.\nObviously, randomizing training data and averaging the predictors is a general\napproach that can be applied to any prediction algorithm, not only to decision trees.\nIn the literature, the approach described above has been called bagging [41], which is\nan acronym for “bootstrap aggregating” (bootstrap itself being a general resampling\nmethod in statistics that samples training data with replacement to determine some\nproperties of estimators). Another way to randomize predictors (especially when\nd, the input dimension is large), is to randomize input data by randomly removing\nsome of the coordinates, leading to a similar construction.\nWith decision trees one can in addition randomize the binary features use to\nsplit nodes, as described next. While bagging may provide some enhancement to\npredictors, feature randomization for decision trees often significantly improves the\nperformance, and is the typical randomization method used for random forests.\n10.2.2\nFeature randomization\nWhen one decides to split a node during the construction of a prediction tree, one\ncan optimize the binary feature γ over a random subset of Γ rather than exploring\nthe whole set. For CART, for example, one can select a small number of dimensions\ni1,...,iq ∈{1,...,d} with q ≪d, and optimize γ by thresholding one of the coordi-\nnates x(ij) for j ∈{1,...,q}. This results in a randomized version of the node insertion\nfunction.\nAlgorithm 10.3 (Randomized node insertion: RNode(T,j))\n(a) Given T and j, let Tν = T and L(ν) = j.\n(b) If σ(T) = 1, let C(ν) = ∅, γν = “None” and fν = ˆfT,CF.\n\n220\nCHAPTER 10. TREE-BASED ALGORITHMS\n(c) If σ(T ′) = 0, sample (e.g., uniformly without replacement) a subset Γν of Γ and let\nfν = “None”, γν = ˆγT,Γν and C(ν) = (l(ν),r(ν)) with\nl(ν) = Node(Tl,2j + 1)\nr(ν) = Node(Tr,2j + 2)\nwhere\nTl = {(x,y) ∈T : γν(x) = 0}\nTr = {(x,y) ∈T : γν(x) = 1}\n(d) Add ν to T and return.\nNow, each time the function RNode(T0,0) is run, it returns a different, random,\ntree. If it is called K times, this results in a random forest T∗= (T1,...TK), with a\npredictor FT∗given by (10.2). Note that trees in random forests are generally not\npruned, since this operation has been observed to bring no improvement in the con-\ntext of randomized tress.\n10.3\nTop-Scoring Pairs\nTop-Scoring Pair (TSP) classifiers were introduced in Geman et al. [78] and can be\nseen as forests formed with depth-one classification trees in which splitting rules are\nbased on the comparison of pairs of variables. More precisely, define\nγij(x) = 1x(i)>x(j).\nA decision tree based on these rules only relies on the order between the features,\nand is therefore well adapted to situations in which the observations are subject to\nincreasing transformations, i.e., when the observed variable X is such that X(j) =\nϕ(Z(j)), where ϕ : R →R is random and increasing and Z is a latent (unobserved)\nvariable. Obviously, in such a case, order-based splitting rules do not depend on ϕ.\nSuch an assumption is relevant, for example, when experimental conditions (such\nas temperature) may affect the actual data collection, without changing their order,\nwhich is the case when measuring high-throughput biological data, such as microar-\nrays, for which the approach was introduced.\nAssuming two classes, a depth-one tree in this context is simply the classifier\nfij = γij. Given a training set, the associated empirical error is\nEij = 1\nN\nN\nX\nk=1\n1γij(xk),yk = 1\nN\nN\nX\nk=1\n|yk −γij(xk)|\n\n10.4. ADABOOST\n221\nand the balanced error (better adapted to situations in which one class is observed\nmore often than the other) is\nEb\nij =\nN\nX\nk=1\nwk|yk −γij(xk)|\nwith wk = 1/(2Nyk), where N0, N1 are the number of observations with yk = 0, yk = 1.\nPairs (i,j) with small errors are those for which the order between the features switch\nwith high probability when passing from class 0 to class 1.\nIn its simplest form, the TSP classifier defines the set\nP = argmin\nij\nEb\nij\nof global minimizers of the empirical error (which may just be a singleton) and pre-\ndicts the class based on a majority vote among the family of predictors (fij,(i,j) ∈P).\nEquivalently, selected variables maximize the score ∆ij = 1 −Eb\nij, leading to the\nmethod’s name.\nSuch classifiers, which are remarkably simple, have been found to be competitive\namong a wide range of “advanced” classification algorithms for large-dimensional\nproblems in computational biology. The method has been refined in Tan et al. [190],\nleading to the k-TSP classifier, which addresses the following remarks. First, when\nj,j′ are highly correlated, and (i,j) is a high-scoring pair, then (i,j′) is likely to be\none too, and their associated decision rules will be redundant. Such cases should\npreferably be pruned from the classification rules, especially if one wants to select\na small number of pairs. Second, among pairs of features that switch with the same\nprobability, it is natural to prefer those for which the magnitude of the switch is\nlargest, e.g., when the pair of variables switches from a regime in which one of them\nis very low and the other very high to the opposite. In Tan et al. [190], a rank-based\ntie-breaker is introduced, defined as\nρij =\nN\nX\nk=1\nwk(Rk(i) −Rk(j))(2yk −1),\nwhere Rk(i) denotes the rank of x(i)\nk\nin x(1)\nk ,...,x(d)\nk . One can now order pairs (i,j)\nand (i′,j′) by stating that the former scores higher if (i) ∆ij > ∆i′j′, or (ii) ∆ij = ∆i′j′\nand ρij > ρi′j′. The k-TSP classifier is formed by selecting pairs, starting from the\nhighest scoring one, and use as lth pair (for l ≤k) the highest scoring ones among all\nthose that do not overlap with the previously selected ones. In [190], the value of k\nis optimized using cross-validation.\n\n222\nCHAPTER 10. TREE-BASED ALGORITHMS\n10.4\nAdaboost\nBoosting methods refer to algorithms in which classifiers are enhanced by recur-\nsively making them focus on harder data. We first address the issue of classification,\nand describe one of the earliest algorithms (Adaboost). We will then interpret it\nas a greedy gradient descent algorithm, as this interpretation will lead to further\nextensions.\n10.4.1\nGeneral set-up\nWe first consider binary classification problems, with RY = {−1,1}. We want to de-\nsign a function x 7→F(x) ∈{−1,1} on the basis of a training set T = (x1,y1,...,xN,yN).\nWith the 0-1 loss, minimizing the empirical error is equivalent to maximizing\nET (F) = 1\nN\nN\nX\nk=1\nykF(xk).\nBoosting algorithms build the function F as a linear combination of “base classi-\nfiers,” f1,...,fM, taking\nF(x) = sign\n\n\nM\nX\nj=1\nαjfj(x)\n\n.\nWe assume that each base classifier, fj, takes values in [−1,1] (the interval).\nThe sequence of base classifiers is learned by progressively focusing on the hard-\nest examples. We will therefore assume that the training algorithm for base clas-\nsifiers takes as input the training set T as well a family of positive weights W =\n(w1,...,wN). More precisely, letting\npW(k) =\nwk\nPN\nk=1 wk\n,\nthe weighted algorithm should implement (explicitly or implicitly) the equivalent\nof an unweighted algorithm on a simulated training set obtained by sampling with\nreplacement K ≫N elements of T according to pW (ideally letting K →∞). Let us\ntake a few examples.\n• Weighted LDA: one can use LDA as described in section 8.2 with\ncg =\nX\nk:yk=g\npW(k),\nµg = 1\ncg\nX\nk:yk=g\npW(k)xk,\nµ =\nX\ng∈RY\ncgµg\n\n10.4. ADABOOST\n223\nand the covariance matrices:\nΣw =\nN\nX\nk=1\npW(k)(xk −µyk)(xk −µyk)T ,\nΣb =\nX\ng∈RY\ncg(µg −¯µ)(µg −¯µ)T .\n• Weighted logistic regression: just maximize\nN\nX\nk=1\npW(k)logπθ(yk|xk)\nwhere πθ is given by the logistic model.\n• Empirical risk minimization algorithms can be modified in order to minimize\nˆRT,W(f ) =\nN\nX\nk=1\nwkr(yk,f (xk)).\n• Of course, any algorithm can be run on a training set resampled using pW.\n10.4.2\nThe Adaboost algorithm\nBoosting algorithms keep track of a family of weights and modify it after the jth\nclassifier fj is computed, increasing the importance of misclassified examples, before\ncomputing the next classifier. The following algorithm, called Adaboost [172, 73],\ndescribes one such approach.\nAlgorithm 10.4 (Adaboost)\n• Start with uniform weights, letting W(1) = (w1(1),...,wN(1)) with wk(1) = 1/N,\nk = 1,...,N. Fix a number ρ ∈(0,1] and an integer M > 0.\n• Iterate, for j = 1,...,M:\n(1) Fit a base classifier fj using the weights W(j) = (w1(j),...,wN(j)). Let\nS+\nw(j) =\nN\nX\nk=1\nwk(j)(2 −|yk −fj(xk)|)\n(10.3a)\nS−\nw(j) =\nN\nX\nk=1\nwk(j)|yk −fj(xk)|\n(10.3b)\nand define αj = ρlog\n\u0010\nS+\nw(j)/S−\nw(j)\n\u0011\n(2) Update the weights by\nwk(j + 1) = wk(j)exp\n\u0010\nαj|yk −fj(xk)|/2\n\u0011\n.\n\n224\nCHAPTER 10. TREE-BASED ALGORITHMS\n• Return the classifier:\nF(x) = sign\n\n\nM\nX\nj=1\nαjfj(x)\n\n.\nIf fj is binary, i.e., fj(x) ∈{−1,1}, then |yk −fj(xk)| = 21yk,fj(xk), so that S+\nW/2 is the\nweighted number of correct classifications and S−\nW/2 is the weighted number of in-\ncorrect ones.\nFor αj to be positive, the jth classifier must do better than pure chance on the\nweighted training set. If not, taking αj ≤0 reflects the fact that, in that case, −fj has\nbetter performance on training data.\nAlgorithms that do slightly better than chance with high probability are called\n“weak learners” [172]. The following proposition [73] shows that, if the base clas-\nsifiers reliably perform strictly better than chance (by a fixed, but not necessarily\nlarge, margin), then the boosting algorithm can make the training-set error arbitrar-\nily close to 0.\nProposition 10.5 Let ET be the training set error of the estimator F returned by Algo-\nrithm 10.4, i.e.,\nET = 1\nN\nN\nX\nk=1\n1yk,F(xk).\nThen\nET ≤\nM\nY\nj=1\n\u0012\nϵρ\nj (1 −ϵj)1−ρ + ϵ1−ρ\nj\n(1 −ϵj)ρ\u0013\nwhere\nϵj =\nS−\nW(j)\nS+\nW(j) + S−\nW(j).\nProof We note that example k is misclassified by the final classifier if and only if\nM\nX\nj=1\nαjykfj(xk) ≤0\nor\nM\nY\nj=1\ne−αjykfj(xk)/2 ≥1\n\n10.4. ADABOOST\n225\nNoting that |yk −fj(xk)| = 1 −ykfj(xk), we see that example k is misclassified when\nM\nY\nj=1\neαj|yk−fj(xk)|/2 ≥\nM\nY\nj=1\neαj/2.\nThis shows that\nET = 1\nN\nN\nX\nk=1\n1yk,F(xk)\n= 1\nN\nN\nX\nk=1\n1QM\nj=1 eαj |yk−fj (xk)|/2≥QM\nj=1 eαj /2\n≤1\nN\nN\nX\nk=1\nM\nY\nj=1\neαj|yk−fj(xk)|/2\nM\nY\nj=1\ne−αj/2.\nLet, for q ≤M,\nUq = 1\nN\nN\nX\nk=1\nq\nY\nj=1\neαj|yk−fj(xk)|/2.\nSince\nwk(q) = 1\nN\nq−1\nY\nj=1\neαj|yk−fj(xk)|/2,\nwe also have Uq = PN\nk=1 wk(q + 1) = (S+\nW(q + 1) + S−\nW(q + 1))/2.\nWe will use the inequality 1\neαt ≤1 −(1 −eα)t,\n1This inequality is clear for α = 0. Assuming α , 0, the difference between the upper and lower\nbound is\nq(t) = 1 −eαt −(1 −eα)t.\nThe function q is concave (its second derivative is −α2eαt) with q(0) = q(1) = 0 and therefore non-\nnegative over [0,1].\n\n226\nCHAPTER 10. TREE-BASED ALGORITHMS\nwhich is true for all α ∈R and t ∈[0,1], to write\nUq ≤1\nN\nN\nX\nk=1\nq−1\nY\nj=1\neαj|yk−fj(xk)|/2(1 −(1 −eαq)|yk −fq(xk)|/2)\n=\nN\nX\nk=1\nwk(q)(1 −(1 −eαq)|yk −fq(xk)|/2)\n=\nN\nX\nk=1\nwk(q) −(1 −eαq)\nN\nX\nk=1\nwk(q)|yk −fq(xk)|/2\n= Uq−1(1 −(1 −eαq)ϵq)\nThis gives (using U0 = 1)\nUM ≤\nM\nY\nj=1\n\u0012\n1 −(1 −eαj\n\u0013\nϵj)\nand\nET ≤\nM\nY\nj=1\n\u0012\n1 −(1 −eαj)ϵj\n\u0013\ne−αj/2.\nIt now suffices to replace eαj by (1 −ϵj)ρϵ−ρ\nj\nand note that\n\u0012\n1 −(1 −(1 −ϵj)ρϵ−ρ\nj )ϵj\n\u0013\n(1 −ϵj)−ρ/2ϵρ/2\nj\n= ϵρ\nj (1 −ϵj)1−ρ + ϵ1−ρ\nj\n(1 −ϵj)ρ\nto conclude the proof.\n■\nFor ϵ ∈[0,1], one has\nϵρ(1 −ϵ)1−ρ + ϵ1−ρ(1 −ϵ)ρ = 1 −(ϵρ −(1 −ϵ)ρ)(ϵ1−ρ −(1 −ϵ)−1−ρ) ≤1\nwith equality if and only if ϵ = 1/2, so that each term in the upper-bound reduces\nthe error unless the corresponding base classifier does not perform better than pure\nchance. The parameter ρ determines the level at which one increases the importance\nof misclassified examples for the next step. Let ˜S+\nW(j) and ˜S−\nW(j) denote the expres-\nsions in (10.3a) and (10.3b) with wk(j) replaced by wk(j + 1). Then, in the case when\nthe base classifiers are binary, ensuring that |yk −fj(xk)|/2 = 1yk,fj(xk), one can easily\ncheck that ˜S+\nW(j)/ ˜S−\nW(j) = (S+\nW(j)/S−\nW(j))1−ρ. So, the ratio is (of course) unchanged if\nρ = 0, and pushed to a pure chance level if ρ = 1. We provide below an interpretation\nof boosting as a greedy optimization procedure that will lead to the value ρ = 1/2.\n\n10.4. ADABOOST\n227\n10.4.3\nAdaboost and greedy gradient descent\nWe here restrict to the case of binary base classifiers and denote their linear combi-\nnation by\nh(x) =\nM\nX\nj=1\nαjfj(x).\nWhether an observation x is correctly classified in the true class y is associated to\nthe sign of the product yh(x), but the value of this product also has an important\ninterpretation, since, when it is positive, it can be thought of as a margin with which\nx is correctly classified.\nAssume that the function F is evaluated, not only on the basis of its classification\nerror, but also based on this margin, using a loss function of the kind\nΨ(h) =\nN\nX\nk=1\nψ(ykh(xk))\n(10.4)\nwhere ψ is decreasing. The boosting algorithm can then be interpreted as an classi-\nfier which incrementally improves this objective function.\nLet, for j < M,\nh(j) =\nj\nX\nq=1\nαqfq .\nThe next combination h(j+1) is equal to h(j) +αj+1fj+1, and we now consider the prob-\nlem of minimizing, with respect to fj+1 and αj+1, the function Ψ(h(j+1)), without\nmodifying the previous classifiers (i.e., performing a greedy optimization). So, we\nwant to minimize, with respect to the base classifier ˜f and to α ≥0, the function\nU(α, ˜f ) =\nN\nX\nk=1\nψ\n\u0010\nykh(j)(xk) + αyk ˜f (xk)\n\u0011\nUsing the fact that ˜f is a binary classifier, this can be written\nU(α, ˜f ) =\nN\nX\nk=1\nψ(ykh(j)(xk) + α)1yk= ˜f (xk) +\nN\nX\nk=1\nψ(ykh(j)(xk) −α)1yk, ˜f (xk)\n(10.5)\n=\nN\nX\nk=1\n(ψ(ykh(j)(xk) −α) −ψ(ykh(j)(xk) + α))1yk, ˜f (xk)\n+\nN\nX\nk=1\nψ(ykh(j)(xk) + α).\n\n228\nCHAPTER 10. TREE-BASED ALGORITHMS\nThis shows that α and ˜f have inter-dependent optimality conditions. For a given\nα, the best classifier ˜f must minimize a weighted empirical error with non-negative\nweights (since ψ is decreasing)\nwk = ψ(ykh(j)(xk) −α) −ψ(ykh(j)(xk) + α).\nGiven ˜f , α must minimize the expression in (10.5). One can use an alternative min-\nimization procedure to optimize both ˜f (as a weighted basic classifier) and α. How-\never, for the special choice ψ(t) = e−t, this optimization turns out to only require one\nstep.\nIn this case, we have\nU(α, ˜f ) =\nN\nX\nk=1\n(eα −e−α)e−ykh(j)(xk)1yk, ˜f (xk) + e−α\nN\nX\nk=1\ne−ykh(j)(xk)\n= e−α(j)(eα −e−α)\nN\nX\nk=1\nwk(j)1yk, ˜f (xk) + e−α(j)e−α\nN\nX\nk=1\nwk(j)\nwith wk(j+1) = eα(j)−ykh(j)(xk) and α(j) = α1+···+αj. This shows that ˜f should minimize\nN\nX\nk=1\nwk(j + 1)1yk, ˜f (xk).\nWe note that\nwk(j + 1) = wk(j)eαj(1−ykfj(xk)) = wk(j)eαj|yk−fk(xk)|,\nwhich is identical to the weight updates in algorithm Algorithm 10.4 (this is the\nreason why the term α(j) was introduced in the computation). The new value of α\nmust minimize (using the notation of Algorithm 10.4)\ne−αS+\nW(j) + eαS−\nW(j),\nwhich yields α = 1\n2 logS+\nW(j)/S−\nW(j). This is the value αj+1 in Algorithm 10.4 with\nρ = 1/2.\n10.5\nGradient boosting and regression\n10.5.1\nNotation\nThe boosting idea, and in particular its interpretation as a greedy gradient proce-\ndure, can be extended to non-linear regression problems [75]. Let us denote by F0\n\n10.5. GRADIENT BOOSTING AND REGRESSION\n229\nthe set of base predictors, therefore functions from RX = Rd to RY = Rq, since we\nare considering regression problems. The final predictor is a linear combination\nF(x) =\nM\nX\nj=1\nαjfj(x)\nwith α1,...,αM ∈R and f1,...,fM ∈F0. Note that the the coefficients αj are redundant\nwhen the class F0 is invariant by multiplication by a scalar. Replacing if needed F0 by\n{f = αg,α ∈R,g ∈F0}, we will assume that this property holds and therefore remove\nthe coefficients αj from the problem.\nIn accordance with the principle of performing greedy searches, we let\nF(j)(x) =\nj\nX\nq=1\nfq(x),\nand consider the problem of minimizing over f ∈F0,\nU(f ) =\nN\nX\nk=1\nr(yk,F(j)(xk) + f (xk)),\nwhere T = (x1,y1,...,xN,yN) is the training data and r is the loss function.\n10.5.2\nTranslation-invariant loss\nIn the case, which is frequent in regression, when r(y,y′) only depends on y −y′, the\nproblem is equivalent to minimizing\nU(f ) =\nN\nX\nk=1\nr(yk −F(j)(xk),f (xk)),\ni.e., to let fj+1 be the optimal predictor (in F0 and for the loss r) of the residuals\ny(j)\nk = yk −F(j)(xk). In this case, this provides a conceptually very simple algorithm.\nAlgorithm 10.5 (Gradient boosting for regression with translation-invariant loss)\n• Let T = (x1,y1,...,xN,yN) be a training set and r a loss function such that r(y,y′)\nonly depends on y −y′.\n• Let F0 be a function class such that f ∈F0 ⇒αf ∈F0 for all α ∈R.\n• Select an integer M > 0 and let F(0) = 0, y(0)\nk\n= yk, k = 1,...,N.\n• For j = 1,...,M:\n\n230\nCHAPTER 10. TREE-BASED ALGORITHMS\n(1) Find the optimal predictor fj ∈F0 for the training set (x1,y(j−1)\n1\n,...,xN,y(j−1)\nN\n).\n(2) Let y(j)\nk = y(j−1)\nk\n−fj(xk)\n• Return F = PM\nk=1 fj.\nRemark 10.6 Obviously, the class F0 should not be a linear class for the boosting\nalgorithm to have any effect. Indeed, if f ,f ′ ∈F0 implies f +f ′ ∈F0, no improvement\ncould be made to the predictor after the first step.\n♦\nA successful example of this algorithm uses regression trees as base predictors.\nRecall that the functions output by such trees take the form\nf (x) =\nX\nA∈C\nwA1x∈A\nwhere C is a finite partition of Rd. Each set in the partition is specified by the value\ntaken by a finite number of binary features (denoted by γ in our discussion of pre-\ndiction trees) and the maximal number of such features is the depth of the tree. We\nassume that the set Γ of binary features is shared by all regression trees in F0, and\nthat the depth of these trees is bounded by a fixed constant. These restrictions pre-\nvent F0 from forming a linear class.2 Note that the maximal depth of tree learnable\nfrom a finite training set is always bounded, since such trees cannot have more nodes\nthan the size of the training set (but one may want to restrict the maximal depth of\nbase predictors to be way less than N).\n10.5.3\nGeneral loss functions\nWe now consider situations in which the loss function is not necessarily a function\nof the difference between true and predicted output. We are still interested in the\nproblem of minimizing U(f ), but we now approximate this problem using the first-\norder expansion\nU(f ) =\nN\nX\nk=1\nr(yk,F(j)(xk)) +\nN\nX\nk=1\n∂2r(yk,F(j)(xk))T f (xk) + o(f ),\nwhere ∂2r denotes the derivative of r with respect to its second variable. This sug-\ngests (similarly to gradient descent) to choose f such that f (xk) = −α∂2r(yk,F(j)(xk))\n2If f and g are representable as trees, f + g can be represented as a tree whose depth is the sum as\nthose of the original trees, simply by inserting copies of g below each leaf of f .\n\n10.5. GRADIENT BOOSTING AND REGRESSION\n231\nfor some α > 0 and all k = 1,...,N. However, such an f may not exist in the class F0,\nand the next best choice is to pick f = α ˜f with ˜f minimizing\nN\nX\nk=1\n| ˜f (xk) + ∂2r(yk,F(j)(xk))|2\nover all ˜f ∈F0. This is similar to projected gradient descent in optimization, and α\nsuch that f = α ˜f should minimize\nN\nX\nk=1\nr(yk,F(j)(xk) + α ˜f (xk)).\nThis provides a generic “gradient boosting” algorithm [75], summarized below.\nAlgorithm 10.6 (Gradient boosting)\n• Let T = (x1,y1,...,xN,yN) be a training set and r a differentiable loss function.\n• Let F0 be a function class such that f ∈F0 ⇒αf ∈F0 for all α ∈R.\n• Select an integer M > 0 and let F(0) = 0.\n• For j = 1,...,M:\n(1) Find ˜fj ∈F0 minimizing\nN\nX\nk=1\n| ˜f (xk) + ∂2r(yk,F(j−1)(xk))|2\nover all ˜f ∈F0.\n(2) Let fj = αj ˜fj where αj minimizes\nN\nX\nk=1\nr(yk,F(j−1)(xk) + α ˜fj(xk)).\n(3) Let F(j) = F(j−1) + fj.\n• Return F = F(M).\nRemark 10.7 Importantly, the fact that F0 is stable by scalar multiplication implies\nthat the function ˜fj satisfies\nN\nX\nk=1\n˜f (xk)T ∂2r(yk,F(j−1)(xk)) ≤0,\n♦\nthat is, excepted in the unlikely case in which the above sum is zero, it is a direction\nof descent for the function U (because one could otherwise replace ˜fj by −˜fj and\nimprove the approximation of the gradient).\n\n232\nCHAPTER 10. TREE-BASED ALGORITHMS\n10.5.4\nReturn to classification\nA slight modification of this algorithm may also be applied to classification, pro-\nvided that the classifier f is obtained by learning the conditional distribution, de-\nnoted g 7→p(g|x), of the output variable (assumed to take values in a finite set RY)\ngiven the input (assumed to take values in RX = Rd).\nOur goal is to estimate an unknown target conditional distribution, µ, therefore\ntaking the form µ(g|x) for g ∈RY and x ∈Rd. We assume that a family µk,k =\n1,...,N of distributions on the set RY is observed, where each µk is assumed to be\nan approximation of the unknown µ(·|xk) (typically, µk(g) = 1g=yk, i.e., µk = δyk). The\nrisk function must take the form r(µ,µ′) where µ,µ′ ∈S(RY), the set of probability\ndistributions on RY. We will work with\nr(µ,µ′) = −\nX\ng∈RY\nµ(g)logµ′(g).\nOne can note that\nr(µ,µ′) = KL(µ∥µ′) + r(µ,µ),\nwhich is therefore minimal when µ′ = µ. Moreover, in the special case µk = δyk, the\nempirical risk is\nˆR(p) =\nN\nX\nk=1\nr(µk,p(·|xk)) = −\nN\nX\nk=1\nlogp(yk|xk),\nso that minimizing it is equivalent to maximizing the conditional likelihood that was\nused for logistic regression.\nBefore applying the previous algorithm, one must address the issue that prob-\nability distributions do not form a vector space, and cannot be added to form new\nprobability distributions. In Friedman [75], Hastie et al. [87], it is suggested to use\nthe representation, which can be associated with any function F : (g,x) 7→F(g|x) ∈R,\npF(g|x) =\neF(g|x)\nP\nh∈RY eF(h|x).\nBecause the representation if not unique (pF = pF′ if F −F′ only depends on x), we\nwill require in addition that\nX\nh∈RY\nF(h|x) = 0\nfor all x ∈Rd. The space formed by such functions F is now linear, and we can\nconsider the empirical risk\nˆR(F) = −\nN\nX\nk=1\nX\ng∈RY\nµk(g)logpF(g|xk) = −\nN\nX\nk=1\nX\ng∈RY\nµk(g)F(g|xk) +\nN\nX\nk=1\nlog\n\n\nX\ng∈RY\neF(g|xk)\n\n.\n\n10.5. GRADIENT BOOSTING AND REGRESSION\n233\nOne can evaluate the derivative of this risk with respect to a change on F(g|xk),\nand a short computation gives\n∂R\n∂F(g|xk) = −\nN\nX\nk=1\n(µk(g) −pF(g|xk)).\nNow assume that a basic space F0 of functions f : (g,x) 7→f (g|x) is chosen, such\nthat all function in F0 satisfy\nX\ng∈RY\nf (g|x) = 0\nfor all x ∈Rd. The gradient boosting algorithm then requires to minimize (in Step\n(1)):\nN\nX\nk=1\nX\ng∈RY\n(µk(g) −pF(j−1)(g|xk) −˜f (g|xk))2\nwith respect to all functions ˜f ∈F0. Given the optimal ˜fj, the next step requires to\nminimize, with respect to α ∈R:\n−α\nN\nX\nk=1\nX\ng∈RY\nµk(g) ˜fj(g|xk) +\nN\nX\nk=1\nlog\n\n\nX\ng∈RY\neF(j−1)(g|xk)+α ˜fj(g|xk)\n\n.\nThis is a scalar convex problem that can be solved, e.g., using gradient descent.\n10.5.5\nGradient tree boosting\nWe now specialize to the situation in which the set F0 contains regression trees. In\nthis situation, the general algorithm can be improved by taking advantage of the fact\nthat the predictors returned by such trees are piecewise constant functions, where\nthe regions of constancy are associated with partitions C of Rd defined by the leaves\nof the trees. In particular, ˜fj(x) in Step (1) takes the form\n˜fj(g|x) =\nJ\nX\nA∈C\n˜fj,A(g)1x∈A.\nThe final f at Step (2) should therefore take the form\nX\nA∈C\nα ˜fj,A(g)1x∈A\n\n234\nCHAPTER 10. TREE-BASED ALGORITHMS\nbut not much additional complexity is introduced by freely optimizing the values of\nfj on A, that is, by looking at f in the form\nX\nA∈C\nfj,A(g)1x∈A\nwhere the values fj,A(g) optimize the empirical risk. This risk becomes\n−\nN\nX\nk=1\nX\nA∈C\nX\ng∈RY\nµk(g)fj,A(g)1xk∈A +\nN\nX\nk=1\nX\nA∈C\nlog\n\n\nX\ng∈RY\neF(j−1)(g|xk)+fj,A(g)\n\n1xk∈A.\nThe values fj,A(g),g ∈RY can therefore be optimized separately, minimizing\n−\nX\nk=1:xk∈A\nX\ng∈RY\nµk(g)fj,A(g) +\nX\nk:xk∈A\nlog\n\n\nX\ng∈RY\neF(j−1)(g|xk)+fj,A(g)\n\n1xk∈A.\nThis is still a convex program, which has to be run at every leaf of the optimized\ntree. If computing time is limited (or for large-scale problems), the determination of\nfj,A(g) may be restricted to one step of gradient descent starting at fj,A = 0. A simple\ncomputation indeed shows that the first derivative of the function above with respect\nto fj,A(g) is\naA(g) = −\nX\nk:xk∈A\n(µk(g) −pF(g|xk)).\nThe derivative of this expression with respect to fj,A(g) (for the same g) is\nbA(g) =\nX\nk:xk∈A\npF(g|xk)(1 −pF(g|xk)).\nThe off-diagonal terms in the second derivative are, for g , h,\n−\nX\nk:xk∈A\npF(g|xk)pF(h|xk).\nIn Friedman et al. [74], it is suggested to use an approximate Newton step, where\nthe off-diagonal terms in the second derivative are neglected. This corresponds to\nminimizing\nX\ng∈RY\naA(g)fj,A(g) + 1\n2\nX\ng∈RY\nbA(g)fj,A(g)2.\nThe solution is (introducing a Lagrange multiplier for the constraint P\ng fj,A(g) = 0)\nfj,A(g) = −aA(g) −λ\nbA(g)\n\n10.5. GRADIENT BOOSTING AND REGRESSION\n235\nwith\nλ =\nP\ng∈RY aA(g)/bA(g)\nP\ng∈RY 1/bA(g)\n.\nA small value ϵ can be added to bA to avoid divisions by zero. We refer the reader\nto Friedman et al. [74], Friedman [75], Hastie et al. [87] for several variations on this\nbasic idea. Note that an approximate but highly efficient implementation of boosted\ntrees, called XGBoost, has been developed in Chen and Guestrin [53].\n\n236\nCHAPTER 10. TREE-BASED ALGORITHMS\n\nChapter 11\nIterated Compositions of Functions and Neural\nNets\n11.1\nFirst definitions\nWe now discuss a class of methods in which the predictor f is built using iterated\ncompositions, with a main application to neural nets. We will structure these mod-\nels using directed acyclic graphs (DAG). These graphs are composed with a set of\nvertexes (or nodes) V = {0,...,m + 1} and a collection L of directed edges i →j be-\ntween some vertexes. If an edge exists between i and j, one says that i is a parent of j\nand j a child of i and will use the notation pa(i) (resp. ch(i)) denote the set of parents\n(resp. children) of i. The graphs we consider must satisfy the following conditions:\n(i) No index is a descendant of itself, i.e., that the graph is acyclic.\n(ii) The only index without parent is i = 0 and the only one without children in\ni = m + 1.\nTo each node i in the graph, one associates a dimension di and a variable zi ∈Rdi.\nThe root node variable, z0 = x, is the input and zm+1 is the output. One also associates\nto each node i , 0 a function ψi defined on the product space\nN\nj∈pa(i)Rdj and taking\nvalues in Rdi. The input-output relation is then defined by the family of equations:\nzi = ψi(zpa(i))\nwhere zpa(i) = (zj,j ∈pa(i)). Since there is only one root and one terminal node, these\niterations implement a relationship y = zm+1 = f (x), with z0 = x. We will refer to the\nz1,...,zm as the latent variables of the network.\nEach function ψi is furthermore parametrized by an si-dimensional vector wi ∈\nRsi, so that we will write\nzi = ψi(zpa(i);wi).\n237\n\n238\nCHAPTER 11. NEURAL NETS\nWe let W denote the vector containing all parameters w1,...,wm+1, which therefore\nhas dimension s = s1 + ··· + sm+1. The network function f is then parametrized by W\nand we will write y = f (x;W).\n11.2\nNeural nets\n11.2.1\nTransitions\nMost neural networks iterate functions taking the form\nψi(z;w) = ρ(bz + β0),z ∈Rdj\nwhere b is a di × (P\nj∈pa(i) dj) matrix and β0 ∈Rdi (so that w = (b,β0) is si = di(1 +\nP\nj∈pa(i) dj)-dimensional); ρ is defined on and takes values in R, and we make the\nabuse of notation, for any d and u ∈Rd\nρ(u) =\n\n\nρ(u(1))\n...\nρ(u(d))\n\n\n.\nThe most popular choice for ρ is the positive part, or ReLU (for rectified linear\nunit), given by ρ(t) = max(t,0). Other common choices are ρ(t) = 1/(1+e−t) (sigmoid\nfunction), or ρ(t) = tanh(z).\nResidual neural networks (or ResNets [89]) are discussed in section 11.6. They\niterate transitions between inputs and outputs of same dimension, taking\nzi+1 = zi + ψ(zi;w).\n(11.1)\n11.2.2\nOutput\nThe last node of the graph provides the prediction, y. Its expression depends on the\ntype of predictor that is learned\n• For regression, y can be chosen as an affine function of is its parents: zm+1 =\nbzpa(m+1) + a0.\n• For classification, one can also use a linear model zm+1 = bzpa(m+1) + a0 where\nzm+1 is q-dimensional and let the classification be argmax(z(i)\nm+1,i = 1,...,q). Alterna-\ntively, one uses “softmax” transformation, with\nz(i)\nm+1 =\neζ(i)\nm+1\nPq\nj=1 eζ(j)\nm+1\nwith ζm+1 = bzpa(m+1) + a0.\n\n11.3. GEOMETRY\n239\n11.2.3\nImage data\nNeural networks have achieved top performance when working with organized struc-\ntures such as images. A typical problem in this setting is to categorize the content of\nthe image, i.e., return a categorical variable naming its principal element(s). Other\napplications include facial recognition or identification. In this case, the transition\nfunction can take advantage of the 2D structure, with some special terminology.\nInstead of speaking of the total dimension, say, d, of the considered variables,\nwriting z = (z(1),...,z(d)), images are better represented with three indices z(u,v,λ)\nwhere u = 1,...,U and U is the width of the image, v = 1,...,V and V is the height\nof the image, λ = 1,...,Λ and Λ is the depth of the image. (With this notation\nd = UV Λ.) Typical images have length and width of one or two hundred pixels,\nand depth Λ = 3 for the three color channels. This three-dimensional structure is\nconserved also for latent variables, with different dimensions. Deep neural networks\noften combine compression in width and height with expansion in depth while tran-\nsitioning from input to output.\nThe linear transformation b mapping one layer with dimensions Ui,Vi,Λi to an-\nother with dimensions Ui+1,Vi+1,Λi+1 is then preferably seen as a collection of num-\nbers: b(u′,v′,λ′,u,v,Λ) so that the transition from zi to zi+1 is given by\nzi+1(u′,v′,λ′) = ρ\n\nβ0(u′,v′,λ′) +\nUi\nX\nu=1\nVi\nX\nv=1\nΛi\nX\nλ=1\nb(u′,v′,λ′,u,v,λ)zi(u,v,λ)\n\n.\nFor images, it is often preferable to use convolutional transitions, providing con-\nvolutional neural networks ([116, 115], or CNNs. If Ui = Ui+1 and Vi = Vi+1, such\na transition requires that b(u′,v′,λ′,u,v,λ) only depends on λ, λ′ and on the differ-\nences u′ −u and v −v′. In general, one also requires that b(u′,v′,λ′,u,v,λ) is non-zero\nonly if |u′−u| and |v′−v| are both less than a constant, typically a small number. Also,\nthere is generally little computation across depths: each output at depth λ′ only uses\nvalues from a single input depth. These restrictions obviously reduce dramatically\nthe number of free parameters involved in the transition.\nAfter one or a few convolutions, the dimension is often reduced by a “pooling”\noperation, dividing the image into small non-overlapping windows and replacing\neach such window by a single value, either the max (max-pooling) or the average.\n11.3\nGeometry\nIn addition to the transitions between latent variables and resulting changes of di-\nmension, the structure of the DAG defining the network is an important element in\n\n240\nCHAPTER 11. NEURAL NETS\nx\nz1\n...\nzm\ny\nFigure 11.1: Linear net with increasing layer depths and decreasing layer width.\nx\nz1\nzm\nz2m−1\ny\nFigure 11.2: A sketch of the U-net architecture designed for image segmentation [168].\nthe design of a neural net. The simplest choice is a purely linear structure (as shown\nin Figure 11.1), as was, for example, used for image categorization in [110].\nMore complex architectures have been introduced in recent years. Their design\nis in a large part heuristic and based on an analysis of the kind of computation\nthat should be done in the network to perform a particular task. For example, an\narchitecture used for image segmentation in summarized in fig. 11.2.\nAn important feature of neural nets is their modularity, since “simple” architec-\ntures can be combined (e.g., by placing the output of a network as input of another\none) and form a more complex network that still follows the basic structure defined\nabove. One example of such a building block is the “attention module,” which take\nas input three vectors Q,K,V (for query, key, and value) and return\nsoftmax(QKT )V .\nThese modules are fundamental elements of “transformer networks” [198], that are\nused, among other tasks, for automatic translation.\n\n11.4. OBJECTIVE FUNCTION\n241\n11.4\nObjective function\n11.4.1\nDefinitions\nWe now return to the general form of the problem, with variables z0,...,zm+1 satis-\nfying\nzi = ψi(zpa(i);wi)\nLet T = (x1,y1,...,xN,yN) denote the training data.\nFor regression problems, the objective function minimized by the algorithm is\ntypically the empirical risk, the simplest choice being the mean square error, which\ngives\nF(W) = 1\nN\nN\nX\nk=1\n|yk −zk,m+1(W)|2.\nwith zk;m+1(W) = f (xk;W).\nFor classification, with the dimension of the output variable equal to the number\nof classes and the decision based on the largest coordinate, one can take (letting\nzk,m+1(i;W) denote the ith coordinate of zk,m+1(W)):\nF(W) = 1\nN\nN\nX\nk=1\n\n−zk,m+1(yk;W) + log\n\u0012\nq\nX\ni=1\nexp(zk,m+1(i;W))\n\u0013\n.\nThis objective function is similar to that minimized in logistic regression.\n11.4.2\nDifferential\nGeneral computation.\nThe computation of the differential of F with respect to W\nmay look daunting, but it has actually a simple structure captured by the back-\npropagation algorithm. Even if programming this algorithm can often be avoided\nby using an automatic differentiation software, it is important to understand how it\nworks, and why the implementation of gradient-descent algorithms remains feasi-\nble.\nConsider the general situation of minimizing a function G(W,z) over W ∈Rs\nand z ∈Rr, subject to a constraint γ(W,z) = 0 where γ is defined on Rs × Rr and\ntakes values in Rr (here, it is important that the number of constraints is equal to\nthe dimension of z). We will denote below by ∂W and ∂z the derivatives of these\nfunctions with respect to the multi-dimensional variables W and z. We make the\nassumptions that ∂zγ, which is an r ×r matrix, is invertible, and that the constraints\ncan be solved to express z as a function of W, that we will denote Z(W).\n\n242\nCHAPTER 11. NEURAL NETS\nThis allows us to define the function F(W) = G(W,Z(W)) and we want to com-\npute the gradient of F. (Clearly, the function F in the previous section satisfies these\nassumptions). Taking h ∈Rs, we have\ndF(W)h = ∂WGh + ∂zGdZh.\nMoreover, since γ(W,Z) = 0 by definition of Z, we have\n∂Wγh + ∂zγ dZh = 0,\nso that\ndF(W)h = ∂WGh −∂zG∂zγ−1 ∂Wγh.\nLet p ∈Rr be the solution of the linear system\n∂zγT p = ∂zGT .\nThen,\ndF(W)h = (∂WG −pT ∂Wγ)h\nor\n∇F = ∂WGT −∂WγT p.\nNote that, introducing the “Hamiltonian”\nH(p,z,W) = pT γ(W,z) −G(W,z),\none can summarize the previous computation with the system\n\n∂pH = 0\n∂zH = 0\n∇F = −∂WHT .\nApplication: back-propagation.\nIn our case, we are minimizing a function of the\nform\nG(W,z1,...,zN) = 1\nN\nN\nX\nk=1\nr(yk,zk,m+1)\nsubject to constraints zk,i+1 = ψi(zk,pa(i);wi), i = 0,...,m, zk,0 = xk. We focus on one\nof the terms in the sum, therefore fixing k, that we will temporarily drop from the\nnotation.\n\n11.4. OBJECTIVE FUNCTION\n243\nSo, we evaluate the gradient of G(W,z) = r(y,zm+1) with zi+1 = ψi(zpa(i);wi), i =\n0,...,m, z0 = x. With the notation of the previous paragraph, we take γ = (γ1,...,γm+1)\nwith\nγi(W,z) = ψi(zpa(i);wi) −zi\nThese constraints uniquely define z as a function of W, which was one of our as-\nsumptions. For the derivative, we have, for u = (u1,...,um+1) ∈Rr (with r = d1 + ··· +\ndm+1, ui ∈Rdi), and for i = 1,...,m + 1\n∂zγiu =\nX\nj∈pa(i)\n∂zjψi(zpa(i);wi)uj −ui\nTaking p = (p1,...,pm+1) ∈Rr, we get\npT ∂zγu =\nm+1\nX\ni=1\nX\nj∈pa(i)\npT\ni ∂zjψi(zpa(i);wi)uj −\nm+1\nX\ni=1\npT\ni ui\n=\nm+1\nX\nj=1\nX\ni∈ch(j)\npT\ni ∂zjψi(zpa(i);wi)uj −\nm+1\nX\nj=1\npT\nj uj\nThis allows us to identify ∂zγT p as the vector g = (g1,...,gm+1) with\ngj =\nX\ni∈ch(j)\n∂zjψi(zpa(i);wi)T pi −pj.\nFor j = m + 1 (which has no children), we get gm+1 = −pm+1, so that the equation\n∂zγT p = g can be solved recursively by taking pm+1 = −gm+1 and propagating back-\nward, with\npj = −gj +\nX\ni∈ch(j)\n∂zjψi(zpa(i);wi)T pi\nfor j = m,...,1.\nTo compute the gradient of G, the propagation has to be applied to g = ∂zG. Since\nG only depends on zm+1, we have gm+1 = ∂zm+1r(y,zm+1) and gj = 0 for j = 1,...,m.\nMoreover, G does not depend on W, so that ∂WG = 0. We have\n∂Wγi = ∂wiψi(zpa(i),wi)\nyielding ∂WγT p = (ζ1,...,ζm) with\nζj = ∂wjψj(zpa(j),wj)T pj.\nWe can now formulate an algorithm that computes the gradient of F with respect\nto W, reintroducing training data indexes in the notation.\n\n244\nCHAPTER 11. NEURAL NETS\nAlgorithm 11.1 (Back-propagation)\nLet (x1,y1,...,xN,yN) be the training set and Rk(z) = r(yk,z) so that\nF(W) = 1\nN\nN\nX\nk=1\nRk(zk,m+1(W))\nwith zk,m+1(W) = f (xk,W). Let W be a family of weights. The following steps com-\npute ∇F(W).\n1. For all k = 1,...,N and all i = 1,...,m + 1, compute zk,i(W) (forward computa-\ntion through the network).\n2. Initialize variables pk,m+1 = −∇Rk(zk,m+1(W)), k = 1,...,N.\n3. For all k = 1,...,N and all j = 1,...,m, compute pk,j using iterations\npk,j =\nX\ni∈ch(j)\n∂zjψi(zk,pa(i),wi)T pk,i.\n4. Let\n∇F(W) = −1\nN\nN\nX\nk=1\nm+1\nX\ni=1\nDT\ni ∂wiψi(zk,pa(i),wi)T pk,i,\nwhere Di is the si × s matrix such that Dih = hi.\n11.4.3\nComplementary computations\nThe back-propagation algorithm requires the computation of the gradient of the\ncosts Rk and of the derivatives of the functions ψi, and this can generally be done in\nclosed form, with relatively simple expressions.\n• If Rk(z) = |yk−z|2 (which is the typical choice for regression models) then ∇Rk(z) =\n2(z −yk).\n• In classification, with Rk(z) = −z(yk) + log\n\u0012Pq\ni=1 exp(z(i))\n\u0013\n, one has\n∇Rk(z) = −uyk +\nexp(z)\nPq\ni=1 exp(z(i))\nwhere uyk ∈Rd is the vector with 1 at position yk and zero elsewhere, and exp(z) is\nthe vector with coordinates exp(z(i)), i = 1,...,d.\n\n11.5. STOCHASTIC GRADIENT DESCENT\n245\n• For dense transition functions in the form ψ(z;w) = ρ(bz + β0) with w = (β0,b),\nthen ∂zψ(z,w) = diag(ρ′(β0 + bz))b so that\n∂zψ(z,w)T p = bT diag(ρ′(β0 + bz))p\n• Similarly\n∂wψ(z,w)T p =\nh\ndiag(ρ′(β0 + bz))p,diag(ρ′(β0 + bz))pzT i\n.\nNote that neural network packages implement these functions (and more) automat-\nically.\n11.5\nStochastic Gradient Descent\n11.5.1\nMini-batches\nFix ℓ≪N. Consider the set of Bℓof binary sequences ξ = (ξ1,...,ξN) such that\nξk ∈{0,1} and PN\nk=1 ξk = ℓ. Define\nH(W,ξ) = ∇W\n\n\n1\nℓ\nN\nX\nk=1\nξkr(yk,f (xk,W))\n\n= 1\nℓ\nN\nX\nk=1\nξk∇W r(yk,f (xk,W))\nwhere ξ follows the uniform distribution on Bℓ. Consider the stochastic approxima-\ntion algorithm:\nWn+1 = Wn −γn+1H(Wn,ξn+1).\n(11.2)\nBecause E(ξk) = ℓ/N, we have E(H(W,ξ)) = ∇WET (f (·,W)) and (11.2) provides a\nstochastic gradient descent algorithm to which the discussion in section 3.3 applies.\nSuch an approach is often referred to as “mini-batch” selection in the deep-learning\nliterature, since it correspond to sampling ℓexamples from the training set without\nreplacement and only computing the gradient of the empirical loss computed from\nthese examples.\n11.5.2\nDropout\nIntroduced for deep learning in Srivastava et al. [181], “dropout” is a learning paradigm\nthat brings additional robustness (and, maybe, reduces overfitting risks) to mas-\nsively parametrized predictors.\nAssume that a random perturbation mechanism of the model parameters has\nbeen designed. We will represent it using a random variable η (interpreted as noise)\nand a transformation W ′ = ϕ(W,η) describing how η affects a given weight con-\nfiguration W to form a perturbed one W ′. In order to shorten notation, we will\n\n246\nCHAPTER 11. NEURAL NETS\nwrite ϕ(W,η) = η · W, borrowing the notation for a group action from group the-\nory. As a typical example, η can be chosen as a vector of Bernoulli random variables\n(therefore taking values in {0,1}), with same dimension as W and one can simply let\nη ·W = η ⊙W be the pointwise multiplication of the two vectors. This corresponds to\nreplacing some of the parameters by zero (“dropping them out”) while keeping the\nothers unchanged. One generally preserves the parameters of the final layer (gm), so\nthat the corresponding η’s are equal to one, and let the other ones be independent,\nwith some probability p of being one, say, p = 1/2.\nReturning to the general case, in which η is simply assumed to be a random\nvariable with known probability distribution, the dropout method replaces the ob-\njective function F(W) = ET (f (·,W)) by its expectation over perturbed predictors\nG(W) = E(ET (f (·,η · W))) where the expectation is taken with respect to the random\nvariable η. While this expectation cannot be computed explicitly, its minimization\ncan be performed using stochastic gradient descent, with\nWn+1 = Wn −γn+1L(Wn,ηn+1),\nwhere η1,η2,... is a sequence of independent realizations of η and\nL(W,η) = ∇W (ET (f (·,η · W))) .\nThen, averaging in η\n¯L(W) = E(∇WF(η ⊙W)) = ∇G(W).\nIn the special case where η · W is just pointwise multiplication, then\nL(W,η) = η ⊙∇F(η ⊙W).\nSo this quantity can be evaluated by using back-propagation to compute ∇F(η · W)\nand multiplying the result by η pointwise. Obviously, random weight perturba-\ntion can be combined with mini-batch selection in a hybrid stochastic gradient de-\nscent algorithm, the specification of which being left to the reader. We also note\nthat stochastic gradient descent in neural networks is often implemented using the\nADAM algorithm (section 3.3.3).\n11.6\nContinuous time limit and dynamical systems\n11.6.1\nNeural ODEs\nEquation (11.1) expresses the difference of the input and output of a neural transi-\ntion as a non-linear function f (z;w) of the input. This strongly suggests passing to\n\n11.6. CONTINUOUS TIME LIMIT AND DYNAMICAL SYSTEMS\n247\ncontinuous time and replacing the difference by a derivative, i.e., replacing the neu-\nral network by a high-dimensional parametrized dynamical system. The continuous\nmodel then takes the form [52]\n∂tz(t) = ψ(z(t);w(t))\n(11.3)\nwhere t varies in a a fixed interval, say, [0,T ]. The whole process is parametrized by\nW = (w(t),t ∈[0,T ]). We need to assume existence and uniqueness of solutions of\n(11.3), which usually restricts the domain of admissibility of parameters W.\nTypical neural transition functions are Lipschitz functions whose constant de-\npend on the weight magnitude, i.e., are such that\n|ψ(z,w) −ψ(z′,w)| ≤C(w)|z −z′|\n(11.4)\nwhere C is a continuous function of W. For example, for ψ(z,w) = ρ(bz + β0), w =\n(b,β0), one can take C(w) = Cρ|b|op. The Caratheodory theorem [17] implies that\nsolutions are well-defined as soon as\nZ T\n0\nC(w(t))dt < ∞.\n(11.5)\nThis is a relatively mild requirement, on which we will return later. Assuming this,\nwe can consider z(T) as a function of the initial value, z(0) = x and of the parameters,\nwriting z(T ) = f (x,W).\nGiven a training set, we consider the problem of minimizing\nF(W) = 1\nN\nN\nX\nk=1\nr(yk,f (xk,W)).\n(11.6)\nThe discussion in section section 11.4.2 applies—formally, at least—to this continu-\nous case, and we can consider the equivalent problem of minimizing\nG(W,z1,...,zN) = 1\nN\nN\nX\nk=1\nr(yk,zk(T))\nwith ∂tzk(t) = ψ(zk(t);w(t)), zk(0) = xk. Once again, we consider each k separately,\nwhich boils down to considering N = 1 and we drop the index k from the notation,\nletting F(W) = r(y,f (x,W)) G(W,z) = r(y,z(T )).\nWe define γ(W,z) to return the function\nt 7→γ(W,z)(t) = ψ(z(t);w(t)) −∂tz(t).\n\n248\nCHAPTER 11. NEURAL NETS\nLet p : [0,T ] →Rd. We want to determine the expression of u = ∂zγT p, which\nsatisfies\nZ T\n0\nu(t)T δz(t)dt =\nZ T\n0\np(t)T (∂zψ(z(t),w(t))δz(t) −∂tδz(t))dt\nAfter an integration by parts, the r.h.s. becomes\n−p(T)T δz(T) +\nZ T\n0\n∂tp(t)T δz(t)dt +\nZ T\n0\np(t)T ∂zψ(z(t),w(t))δz(t))dt\nwhich gives\nu(t) = −p(T )δT + ∂tp(t) + ∂zψ(z(t),w(t))T p(t).\nThe equation ∂zγT p = ∂zGT therefore gives\n−p(T )δT + ∂tp(t) + ∂zψ(z(t),w(t))T p(t) = ∂2r(y,z(T ))δT ,\nso that p satisfies p(T) = −∂2r(y,z(T)) and\n∂tp(t) = −∂zψ(z(t),w(t))T p(t).\n(11.7)\nWe have ∂WG = 0 and v = ∂WγT p satisfies\nZ T\n0\nv(t)T δw(t)dt =\nZ T\n0\np(t)T ∂wψ(z(t),w(t))δw(t)dt\nso that\n∇F(W) = (t 7→−∂wψ(z(t),w(t))T p(t)).\nThis informal derivation (more work is needed to justify the existence of various\ndifferentials in appropriate function spaces) provides the continuous-time version\nof the back-propagation algorithm, which is also known as the adjoint method in\nthe optimal control literature [91, 123]. In that context, z represents the state of\nthe control system, w is the control and p is called the costate, or covector. We\nsummarize the gradient computation algorithm, reintroducing N training samples.\nAlgorithm 11.2 (Adjoint method for neural ODE)\nLet (x1,y1,...,xN,yN) be the training set and Rk(z) = r(yk,z) so that\nF(W) = 1\nN\nN\nX\nk=1\nRk(zk(T ,W))\nwith ∂tzk = ψ(zk,W), zk(0) = xk. Let W be a family of weights. The following steps\ncompute ∇F(W).\n\n11.6. CONTINUOUS TIME LIMIT AND DYNAMICAL SYSTEMS\n249\n1. For all k = 1,...,N and all t ∈[0,T ], compute zk(t,W) (forward computation\nthrough the dynamical system).\n2. Initialize variables pk(T) = −∇Rk(zk(T ,W))/N, k = 1,...,N.\n3. For all k = 1,...,N and all j = 1,...,m, compute pk(t) by solving (backwards in\ntime)\n∂tpk(t) = −∂zψ(zk(t),w(t))T pk(t).\n4. Let, for t ∈[0,T ],\n∇F(W)(t) = −\nN\nX\nk=1\n∂wψ(zk(t),w(t))T pk(t).\nOf course, in numerical applications, the forward and backward dynamical sys-\ntems need to be discretized, in time, resulting in a finite number of computation\nsteps. This can be done explicitly (for example using basic Euler schemes), or using\nODE solvers [52] available in every numerical software.\n11.6.2\nAdding a running cost\nOptimal control problems are usually formulated with a “running cost” that penal-\nizes the magnitude of the control, which in our case is provided by the function\nW : t 7→w(t). Penalties on network weights are rarely imposed with discrete neural\nnetworks, but, as discussed above, in the continuous setting, some assumptions on\nthe function W, such as (11.5), are needed to ensure that the problem is well defined.\nIt is therefore natural to modify the objective function in (11.6) by adding a\npenalty term ensuring the finiteness of the integral in (11.5), taking, for example,\nfor some λ > 0,\nF(W) = λ\nZ T\n0\nC(w(t))2dt +\nN\nX\nk=1\nr(yk,f (xk,W)).\n(11.8)\nThe finiteness of the integral of the squared C(w)2 implies, by Cauchy-Schwartz, the\nintegrability of C(w) itself, and usually leads to simpler computations.\nIf C(w) is known explicitly and is differentiable, the previous discussion and the\nback-propagation algorithm can be adapted with minor modifications for the mini-\nmization of (11.8). The only difference appears in Step 4 of Algorithm 11.2, with\n∇F(W)(t) = 2λ∇C(w(t)) −1\nN\nN\nX\nk=1\n∂wψ(zk(t),w(t))T pk(t).\n\n250\nCHAPTER 11. NEURAL NETS\nComputationally, one should still ensure that C and its gradient are not too costly to\ncompute. If ψ(z,w) = ρ(bz +β0), w = (b,β0), the choice C(w) = Cρ|b|op is valid, but not\ncomputationally friendly. The simpler choice C(w) = Cρ|b|2 is also valid, but cruder\nas an upper-bound of the Lipschitz constant. It leads however to straightforward\ncomputations.\nThe addition of a running cost to the objective is important to ensure that any\npotential solution of the problem leads to a solvable ODE. It does not guarantee that\nan optimal solution exists, which is a trickier issue in the continuous setting than\nin the discrete setting. This is an important theoretical issue, since it is needed, for\nexample, to ensure that various numerical discretization schemes lead to consistent\napproximations of a limit continuous problem. The existence of minimizers is not\nknown in general for ODE networks. It does hold, however, in the following non-\nparametric (i.e., weight-free) context that we now describe.\nThe function ψ in the r.h.s. of (11.3), is, for any fixed w, a function that maps\nz ∈Rd to a vector ψ(z,w) ∈Rd. Such functions are called vector fields on Rd, and the\ncollection ψ(·,w),w ∈Rs is a parametrized family of vector fields.\nThe non-parametric approach replaces this family of functions by a general vec-\ntor field, v so that the time-indexed parametrized family of vector fields (t 7→ψ(·,w(t)))\nbecomes an unconstrained family (t 7→f (t,·)). Following the general non-parametric\nframework in statistics, one needs to define a suitable function space for the vector\nfields, and use a penalty in the objective function.\nWe will assume that, at each time, f (t,·) belongs to a reproducing kernel Hilbert\nspace (RKHS), as introduced in chapter 6. However, because we are considering a\nspace of vector fields rather than scalar-valued functions, we need work with matrix-\nvalued kernels [5], for which we give a definition that generalizes definition 6.1\n(which corresponds to q = 1 below).\nDefinition 11.1 A function K : Rd × Rd 7→Mq(R) satisfying\n[K1-vec] K is symmetric, namely K(x,y) = K(y,x)T for all x and y in Rd.\n[K2-vec] For any n > 0, for any choice of vectors λ1,...,λn ∈Rq and any x1,...,xn ∈Rd,\none has\nn\nX\ni,j=1\nλT\ni K(xi,xj)λj ≥0.\n(11.9)\nis called a positive (matrix-valued) kernel.\nOne says that the kernel is positive definite if the sum in (6.1) cannot vanish, unless\n(i) λ1 = ··· = λn = 0 or (ii) xi = xj for some i , j.\n\n11.6. CONTINUOUS TIME LIMIT AND DYNAMICAL SYSTEMS\n251\nIf κ is a “scalar kernel” (satisfying definition 6.1), then K(x,y) = κ(x,y)IdRq is a\nmatrix-valued kernel.\nA reproducing kernel Hilbert space of vector-valued functions is a Hilbert space\nH of functions from Rd to Rq such that there exists a reproducing kernel K : Rd ×\nRd 7→Mq(R) with the following properties\n[RKHS1] For all x ∈Rd and λ ∈Rq, K(·,x)λ belongs to H,\n[RKHS2] For all h ∈H, x ∈Rd and λ ∈Rq,\n⟨h , K(·,x)λ⟩H = λT h(x).\nProposition 6.5 remains valid in the for vector-valued RKHS, with the following\nmodifications: λ1,...,λN and α1,...,αN are q-dimensional vectors and the matrix\nK(x1,...,xN) is now an Nq × Nq block matrix, with q × q blocks given by K(xk,xl),\nk,l = 1,...,N.\nReturning to the specification of the nonparametric control problem, we will as-\nsume that a vector-valued RKHS, H, has been chosen, with q = d in definition 11.1.\nWe further assume that elements of H are Lipschitz continuous, with\n|v(z) −v(˜z)| ≤C∥v∥H|z −˜z|\n(11.10)\nfor some constant C and all v ∈H. We note that, for every λ ∈Rd,\n|λT (v(z) −v(˜z))|2 = |⟨v , K(·,z)λ −K(·, ˜z)λ⟩H|2\n≤∥v∥2\nH ∥K(·,z)λ −K(·, ˜z)λ∥2\nH\n= ∥v∥2\nH (λT K(z,z)λ −2λT K(z, ˜z)λ + λT K(˜z, ˜z))\n≤|λ|2∥v∥2\nH |K(z,z) −2K(z, ˜z) + K(˜z, ˜z)|.\nThis shows that (11.10) can be derived from regularity properties of the kernel,\nnamely, that\n|K(z,z) −2K(z, ˜z) + K(˜z, ˜z)| ≤C|z −˜z|2\nfor some constant C and all z, ˜z ∈Rd. This property is satisfied by most of the kernels\nthat are used in practice.\nLet η : t 7→η(t) be a function from [0,1] to H. This means that, for each t, η(t) is\na vector field x 7→η(t)(x) on Rd, and we will write indifferently η(t) and η(t,·), with\na preference for η(t,x) rather than η(t)(x). We consider the objective function\n¯F(f ) = λ\nZ T\n0\n∥η(t)∥2\nHdt + 1\nN\nN\nX\nk=1\nr(yk,zk(1)),\n(11.11)\n\n252\nCHAPTER 11. NEURAL NETS\nwith ∂tzk(t) = η(t,zk(t)), zk(0) = xk. To compare with (11.8), the finite-dimensional\nw ∈Rs is now replaced with an infinite-dimensional parameter, η, and the transition\nψ(z,w) becomes η(z).\nUsing the vector version of proposition 6.5 (or the kernel trick used several times\nin chapters 7 and 8), one sees that there is no loss of generality in replacing η(t) by\nits projection onto the vector space\nV (t) =\n\nN\nX\nl=1\nK(·,zl(t))wl : w1,...,wN ∈Rd\n.\nNoting that, if η(t) takes the form\nη(t) =\nN\nX\nl=1\nK(·,zl(t))wl(t),\nthen\n∥η(t)∥2\nH =\nN\nX\nk,l=1\nwk(t)T K(zk(t),zl(t))wl(t).\nThis allows us to replace the infinite-dimensional parameter η by a family W =\n(w(t),t ∈[0,T ] with w(t) = (wk(t),k = 1,...,N). The minimization of ¯F in (11.11) can\nbe replaced by that of\nF(W) = λ\nZ T\n0\nN\nX\nk,l=1\nwk(t)T K(zk(t),zl(t))wl(t)dt + 1\nN\nN\nX\nk=1\nr(yk,zk(1)),\n(11.12)\nwith\n∂tzk(t) =\nN\nX\nl=1\nK(zk(t),zl(t))wl(t).\nThis optimal control problem has a similar form to that considered in (11.8),\nwhere the running cost C(w)2 is replaced by a cost that depends on the control (still\ndenoted w) and the state z. The discussion in section section 11.6.1 can be applied\nwith some modifications. Let K(z) be the dN × dN matrix formed with d × d blocks\nK(zk(t),zl(t)) and w(t) the dN-dimensional vector formed by stacking w1,...,wN. Let\nG(W,z) = λ\nZ T\n0\nw(t)T K(z(t))w(t)dt + 1\nN\nN\nX\nk=1\nr(yk,zk(1))\nand\nγ(W,z)(t) = K(z(t))w(t) −∂tz(t).\n\n11.6. CONTINUOUS TIME LIMIT AND DYNAMICAL SYSTEMS\n253\nThe backward ODE in step 3. of Algorithm 11.2 now becomes\n∂tpk(t) = −∂zk(w(t)T K(z(t))p(t)) + λ∂zk(w(t)T K(z(t))w(t))\nfor k = 1,...,N. Step 4. becomes (for t ∈[0,T ]),\n∇F(W)(t) = K(z(t))(2λ −p(t)).\nThe resulting algorithm was introduced in [209]. It has the interesting prop-\nerty (shared with neural ODE models with smooth controlled transitions) to de-\ntermine an implicit diffeomorphic transformation of the space, i.e., the function\nx 7→f (x;W,z) = ˜z(T) which returns the solution at time T of the ODE\n∂t ˜z(t) =\nN\nX\nl=1\nK(˜z(t),zl(t))wl(t)\n(or ∂˜z(t) = ψ(˜z(t);w(t)) for neural ODEs) is smooth, invertible, with a smooth inverse.\n\n254\nCHAPTER 11. NEURAL NETS\n\nChapter 12\nMonte-Carlo Sampling\nThe goal of this section is to describe how, from a basic random number generator\nthat provides samples from a uniform distribution on [0,1], one can generate sam-\nples that follow, or approximately follow, complex probability distributions on finite\nor general spaces. This, combined with the law of large numbers, permits to approx-\nimate probabilities or expectations by empirical averages over a large collection of\ngenerated samples.\nWe assume that as many as needed independent samples of the uniform dis-\ntribution are available, which is only an approximation of the truth. In practice,\ncomputer programs are only able to generate pseudo-random numbers, which are\nhighly chaotic recursive sequences, but still deterministic. Also, these numbers are\ngenerated as integers, which only provide, after normalization, a distribution on a\nfinite discretization of the unit interval. We will neglect these facts, however, and\nwork as if the output of the function random (or any similar name) in a computer\nprogram is a true realization of the uniform distribution.\n12.1\nGeneral sampling procedures\nReal-valued variables.\nWe will use the following notation for the left limit of a\nfunction F at a given point z\nF(z −0) =\nlim\ny→z,y<zF(y)\nassuming, of course that this limit exists (which is always true, for example when F is\nnon-decreasing). Recall that F is left continuous if and only if F = F(· −0). Moreover,\nit is easy to see that F(· −0) is left-continuous1. Note also that, if F is non-decreasing,\n1For every z and every ϵ > 0, there exists z′ < z such that for all z′′ ∈[z′,z), |F(z′′) −F(z −0)| < ϵ.\nMoreover, taking any y ∈(z′,z), there exists y′ < y such that for all y′′ ∈[y′,y), |F(y′′) −F(y −0)| < ϵ.\n255\n\n256\nCHAPTER 12. MONTE-CARLO SAMPLING\none always has F(z) ≤F(y −0) whenever z < y. The following proposition provides a\nbasic mechanism for Monte-Carlo sampling.\nProposition 12.1 Let Z be a real-valued random variable with c.d.f. FZ. For u ∈[0,1],\ndefine\nF−\nX(u) = max{z : FZ(z −0) ≤u}.\nLet U be uniformly distributed over [0,1]. Then F−\nZ(U) has the same distribution as Z.\nProof Let Az = {u ∈[0,1] : F−\nZ(u) ≤z}. Assume first that u < FZ(z). Then FZ(y−0) ≤u\nimplies that y ≤z, since y > z would imply that FZ(z) ≤FZ(y −0). This shows that\nsup{z′ : FZ(z′ −0) ≤u} ≤z, i.e., u ∈Az.\nNow, take u > FZ(z). Because c.d.f.’s are right continuous, there exists y > z such\nthat u > FZ(y), which implies that F−\nZ(u) ≥y and u < Az.\nWe have therefore shown that [0,FZ(z)) ⊂Az ⊂[0,FZ(z)]. If U is uniformly dis-\ntributed on [0,1], then P(U < FZ(z)) = P(U ≤FZ(z)) = FZ(z), showing that\nP(F−\nZ(U) ≤z) = P(U ∈Az) = FZ(z).\n■\nThis proposition shows that one can generate random samples of a real-valued\nrandom variable Z as soon as one can compute F−\nZ and generate uniformly dis-\ntributed variables. Note that, if FZ is strictly increasing, then F−\nZ = F−1\nZ , the usual\nfunction inverse.\nThe proposition also shows how to sample from random variables taking values\nin finite sets. Indeed, if Z takes values in eΩZ = {z1,...,zn} with pi = P(Z = zi), sam-\npling from Z is equivalent to sampling from the integer valued random variable eZ\nwith P(eZ = i) = pi. For this variable, F−\n˜Z(u) is the largest i such that p1 + ··· + pi−1 ≤u\n(this sum being zero if i = 1), which provides the standard sampling scheme for\ndiscrete probability distributions.\n12.2\nRejection sampling\nWhile the previous approach can be generalized to multivariate distributions, it\nquickly becomes unfeasible when the dimension gets large, excepting simple cases\nin which the variables are independent, or, say, Gaussian. Rejection sampling is a\nsimple algorithm that allows, in some cases, for the generation of samples from a\ncomplicated distribution based on repeated sampling of a simpler one.\nWithout loss of generality, we can assume that y′ ≥z′, yielding |F(z −0) −F(y −0)| ≤2ϵ, showing the\nleft continuity of F(· −0).\n\n12.3. MARKOV CHAIN SAMPLING\n257\nLet us assume that we want to sample from a variable Z taking values RZ, and\nthat there exists a measure µ on RZ with respect to which the distribution of Z is\nabsolutely continuous, i.e., so that this distribution has a density fZ with respect\nto µ. For example, RZ = Rd, and fZ is the p.d.f. of Z with respect to Lebesgue’s\nmeasire. Assume that g is another density functions (with respect to µ) from which\nit is “easy” to sample. Consider the following algorithm, which includes a function\na : z 7→a(z) ∈[0,1] that will be specified later.\nAlgorithm 12.1 (Rejection sampling with acceptance function a and base p.d.f. g)\n(1) Sample a realization z of a random variable with p.d.f. g.\n(2) Generate b ∈{0,1} with P(b = 1) = a(z).\n(3) If b = 1, return Z = z and exit.\n(4) Otherwise, return to step 1.\nThe probability of exiting at step 3 is ρ =\nR\nRd g(z)a(z)µ(dz). So, the algorithm\nsimulates a random variable with p.d.f.\n˜f (z) = g(z)a(z)(1 + (1 −ρ) + (1 −ρ)2 + ···) = g(z)a(z)\nρ\n.\nAs a consequence, in order to simulate fZ, one must choose a so that fZ(z) is pro-\nportional to g(z)a(z), which, (assuming that g(z) > 0 whenever fZ(z) > 0), requires\nthat a(z) is proportional to fZ(z)/g(z). Since a(z) must take values in [0,1], but should\notherwise be chosen as large as possible to ensure that fewer iterations are needed,\none should take\na(z) = fZ(z)\ncg(z)\nwhere c = max{fZ(z)/g(z) : z ∈Rd}, which must therefore be finite. This fully specifies\na rejection sampling algorithm for fZ. Note that g is free to choose (with the restric-\ntion that fZ(z)/g(z) must be bounded), and should be selected so that sampling from\nit is easy, and the coefficient c above is not too large.\n12.3\nMarkov chain sampling\nWhen dealing with high-dimensional distributions, the constant c in the previous\nprocedure is typically extremely large, and the rejection-sampling algorithm be-\ncomes unfeasible, because it keeps rejecting samples for very long times. In such\ncases, one can use alternative simulation methods that iteratively updates the vari-\nable Z by making small changes at each step, resulting in a procedure that asymp-\ntotically converges to a sample of the target distribution. Such sampling schemes\n\n258\nCHAPTER 12. MONTE-CARLO SAMPLING\nare usually described as Markov chains, leading to the name Markov-chain Monte\nCarlo (or MCMC) sampling.\nAssume that we want to sample from a random variable that takes values in\nsome (measurable) set B = RX.2 A Markov chain is the probabilistic analogous of a\nrecursive sequence Xn+1 = Φ(Xn), which is fully defined by the function Φ : B →B\nand the initial value X0 ∈B.\n12.3.1\nDefinitions\nFor Markov chains, X0 is a random variable, which therefore does not have a fixed\nvalue, but follows a probability distribution that we will generally denote µ0: P0(x) =\nP(X0 = x). The computation of Xn+1 given Xn is not deterministic either, but given\nthe conditional probabilities\nPn,n+1(x,A) = P(Xn+1 ∈A | Xn = x).\nwhere A ⊂B is measurable. The left-hand side of this equation, Pn,n+1 is called a\ntransition probability, according to the following definition.\nDefinition 12.2 Let F1 and F2 be two sets equipped with σ-algebras A1 and A2. A\ntransition probability from F1 to F2 is a function p : F1 × A2 →[0,1] such that, for all\nx ∈F1, the function A 7→p(x,A) is a probability on F2 and for all A ∈A2, the function\nx 7→p(x,A), x ∈F1, is measurable.\nWhen F2 is discrete, the probabilities are fully specified by their values on singleton\nsets, and we will write p(x,y) for p(x,{y}).\nWhen Pn,n+1(x,·) does not depend on n, the Markov chain is said to be homoge-\nneous. To simplify notation, we will restrict to homogeneous chains (and therefore\nonly write P(x,A)), although some of the chains used in MCMC sampling may be\ninhomogeneous. This is not a very strong loss of generality, however, because in-\nhomogeneous Markov chains can be considered as homogeneous by extending the\nspace Ωon which they are defined to Ω× N, and defining the transition probability\n˜p\n\u0010\n(x,n),A × {r}\n\u0011\n= 1r=n+1pn,n+1(x,A).\nAn important special case is when B is countable, in which case one only needs\nto specify transition probabilities for singletons A = {y}, and we will write\np(x,y) = P(x,{y}) = P(Xn+1 = y | Xn = x)\n2We will assume in this chapter that B is a complete metric space with a dense countable subset,\nwith the associated Borel σ-algebra.\n\n12.3. MARKOV CHAIN SAMPLING\n259\nfor the p.m.f. associated with P(x,·).\nAnother simple situation is when B = Rd and each P(x,·) has a p.d.f. that we will\nalso denote as p(x,·). In this latter case, assuming that P0 also have a p.d.f. that we\nwill denote by µ0, the joint p.d.f. of (X0,...,Xn) on (Rd)n+1 is given by\nf (x0,x1,...,xn) = µ0(x0)p(x0,x1)···p(xn−1,xn).\n(12.1)\nThe same expression holds for the joint p.m.f. in the discrete case.\nIn the general case (invoking measure theory), the joint distribution is also deter-\nmined by the transition probabilities, and we leave the derivation of the expression\nto the reader. An important point is that, in both special cases considered above,\nand under some very mild assumptions in the general case , these transition proba-\nbilities also uniquely define the joint distribution of the infinite process (X0,X1,...)\non B∞, which gives theoretical support to the consideration of asymptotic proper-\nties of Markov chains. In this discussion, we are interested in conditions ensuring\nthat the chain asymptotically samples from a target probability distribution Q, i.e.,\nwhether P(Xn ∈A) converges to Q(A) (one says that Xn converges in distribution to\nQ). In practice, Q is given or modeled, and the goal is to determine the transition\nprobabilities. Note that the marginal distribution of Xn is computed by integrating\n(or summing) (12.1) with respect to x0,...,xn−1, which is generally computationally\nchallenging.\nGiven a transition probability P on B, we will use the notation, for a function\nf : B →R:\nPf (x) =\nZ\nB\nf (y)P(x,dy).\nIf Q is a probability distribution on B, it will also be convenient to write\nQf (x) =\nZ\nB\nf (y)Q(dy).\n12.3.2\nConvergence\nWe will denote Px(·) the conditional distribution P(· | X0 = x) and Pn(x,A) = Px(Xn ∈\nA), which is a probability distribution on B. The goal of Markov Chain Monte Carlo\nsampling is to design the transition probabilities such that Pn\nx (A) converges to Q(A)\nwhen n tends to infinity. One furthermore wants to complete this convergence with\na law of large numbers, ensuring that\n1\nn\nn\nX\nk=1\nf (Xk) →\nZ\nB\nf (x)Q(dx)\n\n260\nCHAPTER 12. MONTE-CARLO SAMPLING\nwhen n →∞, where Xn is the generated Markov chain and f is Q-integrable.\nIntroduce the total variation distance between two probability measures on a\ngiven probability space,\nDvar(µ1,µ2) = sup\nA\n(µ1(A) −µ2(A)).\n(12.2)\nwhere the supremum is taken over all measurable sets A.\nWe will say that the\nMarkov chain with transition P asymptotically samples from Q if\nlim\nn→∞Dvar(Pn(x,·),Q) = 0\n(12.3)\nfor Q-almost all x ∈B. The chain must satisfy specific conditions for this to be\nguaranteed.\nWe now discuss some properties of the total variation distance that will be useful\nlater. First, we note that the supremum in the r.h.s. of (12.2) is achieved. Indeed,\nthere exists a set A0 such that, for all measurable sets A, µ1(A ∩A0) ≥µ2(A ∩A0) and\nµ1(A∩Ac\n0) ≤µ2(A∩Ac\n0). If B is a finite set, it suffices to let A0 = {x ∈B : µ1(x) ≥µ2(x)};\nif both µ1 and µ2 have p.d.f.’s ϕ1 and ϕ2 with respect to Lebesgue’s measure (with\nB = Rd), then one can take A0 = {x ∈B : ϕ1(x) ≥ϕ2(x)}. In the general case, one\ncan take µ = µ1 + µ2 so that µ1,µ2 ≪µ, and letting ϕi = dµi/dµ, also take A0 = {x ∈\nB : ϕ1(x) ≥ϕ2(x)}. (This is also a special case of the Hahn-Jordan decomposition of\nsigned measures [66]).\nNow, it is clear that, for any A\nµ1(A) −µ2(A) = µ1(A ∩A0) −µ2(A ∩A0) + µ1(A ∩Ac\n0) −µ2(A ∩Ac\n0)\n≤µ1(A ∩A0) −µ2(A ∩A0)\n≤µ1(A ∩A0) −µ2(A ∩A0) + µ1(Ac ∩A0) −µ2(Ac ∩A0)\n= µ1(A0) −µ2(A0)\nshowing that\nDvar(µ1,µ2) = µ1(A0) −µ2(A0).\nThe following proposition lists additional properties.\nProposition 12.3 (i) If µ1,µ2 have a densities ϕ1,ϕ2 with respect to some positive mea-\nsure µ (such as µ1 + µ2), then\nDvar(µ1,µ2) = 1\n2\nZ\nB\n|ϕ1(x) −ϕ2(x)|µ(dx).\nIn particular, if B is finite\nDvar(µ1,µ2) = 1\n2\nX\nx∈B\n|µ1(x) −µ2(x)|.\n\n12.3. MARKOV CHAIN SAMPLING\n261\n(ii) For general B,\nDvar(µ1,µ2) = sup\nf\n Z\nB\nf (x)µ1(dx) −\nZ\nB\nf (x)µ2(dx)\n!\n.\n(12.4)\nwhere the supremum is taken over all measurable functions f taking values in [0,1].\n(iii) If f : B →R is bounded, define the maximal oscillation of f by\nosc(f ) = sup{f (x) −f (y) : x,y ∈B}.\nThen\nDvar(µ1,µ2) = sup\n(Z\nB\nf (x)µ1(dx) −\nZ\nB\nf (x)µ2(dx) : osc(f ) ≤1\n)\n(iv) Conversely, for any bounded measurable f : B →R,\nosc(f ) = sup\n(Z\nB\nf (x)µ1(dx) −\nZ\nB\nf (x)µ2(dx) : Dvar(µ1,µ2) ≤1\n)\nProof If one takes A0 = {x ∈B : ϕ1(x) ≥ϕ2(x)}, then\nDvar(µ1,µ2) =\nZ\nA0\n(ϕ1(x) −ϕ2(x))µ(dx) =\nZ\nA0\n|ϕ1(x) −ϕ2(x)|µ(dx).\nBut, because both µ1 and µ2 are probability measures\nZ\nB\n(ϕ1(x) −ϕ2(x))µ(dx) = 0\nso that\nZ\nAc\n0\n(ϕ1(x) −ϕ2(x))µ(dx) = −\nZ\nA0\n(ϕ1(x) −ϕ2(x))µ(dx).\nHowever, the l.h.s. is also equal to\n−\nZ\nAc\n0\n|ϕ1(x) −ϕ2(x)|µ(dx)\nso that\nZ\nB\n|ϕ1(x) −ϕ2(x)|µ(dx) = 2\nZ\nA0\n(ϕ1(x) −ϕ2(x)) = 2Dvar(µ1,µ2),\nwhich proves (i).\nTo prove (ii), first notice that, for all A,\nµ1(A) −µ2(A) =\nZ\nB\nf (x)µ1(dx) −\nZ\nB\nf (x)µ2(dx)\n\n262\nCHAPTER 12. MONTE-CARLO SAMPLING\nfor f = 1A, so that\nDvar(µ1,µ2) ≤sup\nf\n Z\nB\nf (x)µ1(dx) −\nZ\nB\nf (x)µ2(dx)\n!\n.\nConversely, using A0 as above, and taking f with values in [0,1]\nZ\nB\nf (x)µ1(dx) −\nZ\nB\nf (x)µ2(dx) =\nZ\nA0\nf (x)(µ1 −µ2)(dx) +\nZ\nAc\n0\nf (x)(µ1 −µ2)(dx)\n≤\nZ\nA0\nf (x)(µ1 −µ2)(dx)\n≤\nZ\nA0\n(µ1 −µ2)(dx) = Dvar(µ1,µ2)\nThis shows (ii). For (iii), one can note that, if f takes values in [0,1], then osc(f ) ≤\n1 so that\nDvar(µ1,µ2) ≤sup\n(Z\nB\nf (x)µ1(dx) −\nZ\nB\nf (x)µ2(dx) : osc(f ) ≤1\n)\nConversely, take f such that osc(f ) ≤1, ϵ > 0 and y such that f (y) ≥infxf (x) + ϵ. Let\nfϵ(x) = (f (x) −f (y) + ϵ)/(1 + ϵ), which takes values in [0,1]. Then\nDvar(µ1,µ2) ≥\nZ\nB\nfϵ(x)µ1(dx) −\nZ\nB\nfϵ(x)µ2(dx)\n=\n1\n1 + ϵ\n Z\nB\nf (x)µ1(dx) −\nZ\nB\nf (x)µ2(dx)\n!\nand since this is true for all ϵ > 0, we get\nZ\nB\nf (x)µ1(dx) −\nZ\nB\nf (x)µ2(dx) ≤Dvar(µ1,µ2)\nwhich completes the proof of (iii).\nUsing (iii), we find, for any µ1,µ2 and any bounded f\nZ\nB\nf (x)µ1(dx) −\nZ\nB\nf (x)µ2(dx) ≤Dvar(µ1,µ2)osc(f )\nwhich shows that\nsup\n(Z\nB\nf (x)µ1(dx) −\nZ\nB\nf (x)µ2(dx) : Dvar(µ1,µ2) ≤1\n)\n≤osc(f ).\n\n12.3. MARKOV CHAIN SAMPLING\n263\nHowever, taking µ1 = δx and µ2 = δy, so that Dvar(µ1,µ2) = 0 is x = y and 1 otherwise,\nwe get\nf (x) −f (y) =\nZ\nB\nf (x)µ1(dx) −\nZ\nB\nf (x)µ2(dx)\n≤sup\n(Z\nB\nf (x)µ1(dx) −\nZ\nB\nf (x)µ2(dx) : Dvar(µ1,µ2) ≤1\n)\nwhich yields (iv) after taking the supremum with respect to x and y.\n■\n12.3.3\nInvariance and reversibility\nIf a Markov chain converges to Q, then Q must be an “invariant distribution,” in the\nsense that, if Xn ∼Q for some n, then so does Xn+1 and as a consequences all Xm for\nn ≥m. This can be seen by writing\nPn+1(x,A) = Px(Xn+1 ∈A) = Ex(P(Xn,A)) = EPn(x,·)(P(·,A))\nIf Pn(x,·) (and therefore also Pn+1(x,·)) converges to Q, then passing to the limit\nabove yields\nQ(A) = EQ(P(·,A))\nand this states that, if Xn ∼Q, then so does Xn+1. If Q has a p.d.f. (resp. p.m.f.), say,\nq, this gives\nq(y) =\nZ\nRd p(x,y)q(x)dx,(resp. q(y) =\nX\nx∈B\np(x,y)q(x)).\nSo, if one designs a Markov chain with a target asymptotic distribution Q, the first\nthing to ensure is that Q is invariant.\nWhile invariance leads to an integral equation for q, a stronger condition, called\nreversibility is easier to assess.\nAssume that Q is invariant by P. Make the assumption that P(x,·) has a density\np∗with respect to Q (this is, essentially, no loss of generality, see argument below),\nso that\nP(x,A) =\nZ\nA\np∗(x,y)Q(dy).\nTaking A = B above, we have\nZ\nB\np∗(x,y)Q(dy) = P(x,B) = 1\nbut we also have, because Q is invariant, that\nZ\nB\np∗(x,y)Q(dx) = Q(B) = 1.\n\n264\nCHAPTER 12. MONTE-CARLO SAMPLING\nOne says that the density is “doubly stochastic” with respect to Q.\nConversely, if a transition probability P has a doubly stochastic density p∗with\nrespect to some probability Q on B, then Q is invariant by P, since\nZ\nB\nP(x,A)Q(dx) =\nZ\nB\nZ\nA\np∗(x,y)Q(dy)Q(dx)\n=\nZ\nA\nZ\nB\np∗(x,y)Q(dx)Q(dy) =\nZ\nA\nQ(dy) = Q(A).\nThe property of being doubly stochastic can be reinterpreted in terms of time\nreversal for Markov chains. Let Q0 be an initial distribution for a Markov chain\nwith transition P (not necessarily invariant) so that, for any n ≥0, the distribution\nof Xn is Qn = Q0Pn. Fixing any m > 0, we are interested in the reversed process\n˜Xk = Xm−k. We first notice that the conditional distribution of Xn given its future\nXn+1,...,Xm (with n < m) only depends on Xn+1, so that the reversed process is also\nMarkov. Indeed, for any positive function f : B →R, g : Bm−n →R, one has, using\nthe fundamental properties of conditional expectations and the fact that (Xn) is a\nMarkov chain,\nE(f (Xn)g(Xn+1,...,Xm)) = E(E(f (Xn)g(Xn+1,...,Xm) | Xn,Xn+1))\n= E(f (Xn)E(g(Xn+1,...,Xm) | Xn,Xn+1))\n= E(f (Xn)E(g(Xn+1,...,Xm) | Xn+1))\n= E(E(f (Xn) | Xn+1)E(g(Xn+1,...,Xm) | Xn+1))\n= E(E(f (Xn) | Xn+1)g(Xn+1,...,Xm)).\nThis shows that\nE(f (Xn) | Xn+1,...,Xm) = E(f (Xn) | Xn+1),\nwhich is what we wanted. To identify the conditional distribution of Xn given Xn+1,\nwe note that for any x ∈B, the transition probability P(x,·) is absolutely continuous\nwith respect to Qn+1 since\nQn+1(A) =\nZ\nB\nP(x,A)Qn(dx)\nand the r.h.s. is zero only if P(x,A) = 0 Qn-almost everywhere 3. This shows that\nthere exists a function rn+1 : B × B →R such that, for all A\nP(x,A) =\nZ\nA\nrn+1(x,y)Qn+1(dy).\n3The “almost everywhere” statement a priori depends on A, but can be made independent of it\nunder the mild assumption (that we will always make) that B has a countable basis of open sets.\n\n12.3. MARKOV CHAIN SAMPLING\n265\nGiven this point, one can write\nE(f (Xn)g(Xn+1) =\nZ\nB2 f (xn)g(xn+1)P(xn,dxn+1)Qn(dxn)\n=\nZ\nB2 f (xn)g(xn+1)rn+1(xn,xn+1)Qn+1(dxn+1)Qn(dxn)\n=\nZ\nB\n Z\nB\nf (xn)rn+1(xn,xn+1)Qn(dxn)\n!\ng(xn+1)Qn+1(dxn+1)\nwhich shows that the conditional distribution of Xn given Xn+1 = xn+1 has density\nxn 7→rn+1(xn,xn+1) relatively to Qn. Note that, for discrete probabilities, one has\nrn+1(x,y) = P(x,y)\nQn+1(y)\nand\nP(Xn = x | Xn+1 = y) = Qn(x)P(x,y)\nQn+1(y)\n.\n(12.5)\nThe formula is identical if both Q0 and P(x,·) have p.d.f.’s with respect to a fixed\nreference measure µ on B (for example, Lebesgue’s measure when B = Rd), denoting\nthese p.d.f’s by q0 and p(x,·). Then, the p.d.f. of the distribution of Xn given Xn+1 = y\nis\n˜pn(y,x) = qn(x)p(x,y)\nqn+1(y)\n(12.6)\nwhere qn is the p.d.f. of Qn. Note that the transition probabilities of the reversed\nMarkov chain depend on n, i.e., the reversed chain is non-homogeneous in general.\nHowever, if one assumes that Q0 = Q is invariant by P, then Qn = Q for all n and\ntherefore rn(x,y) = p∗(x,y), using the previous notation. In this case, the reversed\nchain has transitions independent of n and its transition probability has density\n˜p∗(x,y) = p∗(y,x)\nwith respect to Q. In the discrete case, letting p(x,y) = P(Xn+1 = y | Xn = x), we have\np∗(x,y) = p(x,y)/Q(y), so that the reversed transition (call it ˜p) is such that\n˜p(x,y)\nQ(y) = p(y,x)\nQ(x) ,\ni.e.,\nQ(y)p(y,x) = Q(x) ˜p(x,y).\n(12.7)\nOne retrieves easily the fact that if p is such that there exists Q and ˜p such that (12.7)\nis satisfied, then (summing the equation over y) Q is an invariant probability for p.\n\n266\nCHAPTER 12. MONTE-CARLO SAMPLING\nLet Q be a probability on B. One says that the Markov chain (or the transition\nprobability p) is Q-reversible if and only if p(x,·) has a density p∗(x,·) with respect\nto Q such that p∗(x,y) = p∗(y,x) for all x,y ∈B. Since such a density is necessarily\ndoubly stochastic, Q is then invariant by p. Reversibility is equivalent to the prop-\nerty that, whenever Xn ∼Q, the joint distribution of (Xn,Xn+1) coincides with that of\n(Xn+1,Xn). Alternatively, Q-reversibility requires that for all A,B ⊂B,\nZ\nA\nP(z,B)dQ(z) =\nZ\nB\nP(z,A)dQ(z).\n(12.8)\nIn the discrete case, (12.8) is equivalent to the “detailed balance” condition:\nQ(y)p(y,x) = Q(x)p(x,y).\n(12.9)\nWhile Q can be an invariant distribution for a Markov chain without that chain\nbeing Q-reversible, the latter property is easier to ensure when designing transition\nprobabilities, and most sampling algorithms are indeed reversible with respect to\ntheir target distribution.\nRemark 12.4 A simple example of non-reversible Markov chain with invariant prob-\nability Q is often obtained in practice by alternating two or more Q-reversible transi-\ntion probabilities. Assume, to simplify, that B is discrete and that p1 and p2 are tran-\nsition probabilities that satisfy (12.9). Consider a composite Markov chain for which\nthe transition from Xn to Xn+1 consists in generating first Yn according to p1(Xn,·)\nand then Xn+1 according to p2(Yn,·). The resulting composite transition probability\nis\np(x,y) =\nX\nz∈B\np1(x,z)p2(z,y).\nTrivially, Q is invariant by p, since it is invariant by p1 and p2, but p is not Q-\nreversible. Indeed, p satisfies (12.7) with\n˜p(x,y) =\nX\nz∈B\np2(x,z)p1(z,y).\n♦\n12.3.4\nIrreducibility and recurrence\nWhile necessary, invariance is not sufficient for a Markov chain to converge to Q\nin distribution. However, it simplifies the general ergodicity conditions compared\nto the general theory of Markov chains [147, 160], as summarized below, following\n[192] (see also [13]). We therefore assume that the transition probability P is such\nthat Q is P-invariant.\n\n12.3. MARKOV CHAIN SAMPLING\n267\nOne says that the Markov chain is Q-irreducible (or, simply, irreducible in what\nfollows) if and only if, for all z ∈B and all (measurable) B ⊂B such that Q(B) > 0,\nthere exists n > 0 with Pz(Xn ∈B) > 0. (Irreducibility implies that Q is the only\ninvariant probability of the Markov chain.)\nA Markov chain is called periodic if there exists m > 1 such that B can be covered\nby disjoint subsets B0,...,Bm−1 that satisfy P(x,Bj) = 1 for all x ∈Bj−1 if j ≥1 and\nall x ∈Bm−1 if j = 0. In other terms, the chain loops between the sets B0,...,Bm−1. If\nsuch a decomposition does not exists, the chain is called aperiodic.\nA periodic chain cannot satisfy (12.3). Indeed, periodicity implies that Px(Xn ∈\nBi) = 0 for all x ∈Bi unless i = 0 (mod d). Since the sets Bi cover B, (12.3) is only pos-\nsible with Q = 0. Irreducibility and aperiodicity are therefore necessary conditions\nfor ergodicity. Combined with the fact that Q is an invariant probability distribu-\ntion, these conditions are also sufficient, in the sense that (12.3) is true for Q-almost\nall x. (See [192] for a proof.)\nWithout the knowledge that the chain has an invariant probability, showing con-\nvergence usually requires showing that the chain is recurrent, which means that, for\nany set B such that Q(B) > 0, the probability that, starting from x, Xn ∈B for an in-\nfinite number of n, written as Px(Xn ∈B i.o.) (for infinitely often) is positive for all\nx ∈E and equal to 1 Q-almost surely. The fact that irreducibility and aperiodicity\ncombined with Q-invariance imply recurrence (or, more precisely, Q-positive recur-\nrent [147]) is an important remark that significantly simplifies the theory for MCMC\nsimulation. Note that, by restricting B to a suitable set of Q-probability 1, one can\nassume that Px(Xn ∈B i.o.) = 1 for all x ∈B, which is called Harris recurrence. It the\nchain is Harris recurrent, then (12.3) holds with µ0 = δx for all x ∈B. 4\nOne says that C ⊂B is a “small” set if Q(C) > 0 and there exists a triple (m0,ϵ,ν),\nwith ϵ > 0 and ν a probability distribution on B, such that\nPm0(x,·) ≥ϵν(·)\nfor all x ∈C. A slightly different result, proved in [13], replaces irreducibility by the\nproperty that there exists a small set C ⊂B such that\nPx(∃n : Xn ∈C) > 0\n4Harris recurrence is also associated with the uniqueness of right eigenvectors of P, that is func-\ntions h : B →R such that\nPh(x)\n∆=\nZ\nB\nP(x,dy)h(y) = h(x).\nSuch functions are also called harmonic for P. Because P is a transition probability, constant functions\nare always harmonic. Harris recurrence, in the current context, is equivalent to the fact that every\nbounded harmonic function is constant.\n\n268\nCHAPTER 12. MONTE-CARLO SAMPLING\nfor Q-almost all x ∈B. One then replaces aperiodicity by the similar condition that\nthe greatest common divisor of the set of integers m such that there exists ϵm with\nPm(x,·) ≥ϵmν(·) for all x ∈C is equal to 1. These two conditions combined with\nQ-invariance also imply that (12.3) holds for Q-almost all x ∈B.\n12.3.5\nSpeed of convergence\nIt is also important to quantify the speed of convergence in (12.3). Efficient algo-\nrithms typically have a geometric convergence speed, namely\nDvar(Pn\nx ,Q) ≤M(x)rn\n(12.10)\nfor some 0 ≤r < 1 and some function M(x), or uniformly geometric convergence\nspeed, for which the function M is bounded (or, equivalently, constant).\nA sufficient condition for geometric ergodicity is provided in Nummelin [147,\nProposition 5.21]. Assume that the chain is Harris recurrent and that there exist\nr > 1, a small set C and a “drift function” h with\nsup\nx<C\n(rE(h(Xn+1) | Xn = x) −h(x)) < 0\n(12.11a)\nand\nsup\nx∈C\nE(h(Xn+1)1Xn+1<C | Xn = x) < ∞.\n(12.11b)\nThen the Markov chain is geometrically ergodic. Note that E(h(Xn+1) | Xn = x) =\nPh(x). Equations (12.11a) and (12.11b) can be summarized in a single equation\n[136], namely\nPh(x) ≤βh(x) + M1C(x)\n(12.12)\nwith β = 1/r < 1 and M ≥0.\n12.3.6\nModels on finite state spaces\nUniform geometric ergodicity is implied by the simple condition that the whole set\nB is small, requiring in a uniform lower bound, for some probability distribution ν,\nPm0(x,·) ≥ϵν(·)\n(12.13)\nfor all x ∈B.\nSuch uniform conditions usually require strong restrictions on the space B, such\nas compactness or finiteness. To illustrate this consider the case in which the set B\nis finite. Assume, to simplify, that Q(x) > 0 for all x ∈B (one can restrict the Markov\nchain to such x’s otherwise). Arbitrarily labeling elements of B as B = {x1,...,xN},\n\n12.3. MARKOV CHAIN SAMPLING\n269\nwe can consider p(x,y) as the coefficients of a matrix P = (p(xk,xl),k,l = 1,...,N).\nSuch a matrix, which has non-negative entries and row sums equal to 1, is called a\nstochastic matrix.\nWe will denote the nth power of P as Pn = (p(n)(xk,xl),k,l = 1,...,N). One im-\nmediately sees that irreducibility is equivalent to the fact that, for all x,y ∈B, there\nexists m (that may depend of x and y) such that p(m)(x,y) > 0. One can furthermore\nshow that the chain is irreducible and aperiodic if one can choose m independent of\nx and y above, that is, if there exists m such that Pm has positive coefficients. This\ncondition clearly implies uniformly geometric ergodicity, which is therefore valid\nfor all irreducible and aperiodic Markov chains on finite sets.\nThis result can also be deduced from properties of matrices with non-negative\nor positive coefficients. The Perron-Frobenius theorem [93] states that the eigen-\nvalue 1 (associated with the eigenvector 1) is the largest, in modulus, eigenvalue of\na stochastic matrix ˜P with positive entries, that it has multiplicity one and that all\nother eigenvalues have a modulus strictly smaller that one. If Pm has positive en-\ntries, this implies that all the eigenvalues of (Pm −1Q) (where Q is considered as a\nrow vector) have modulus strictly less than one. This fact can then be used to prove\nuniformly geometric ergodicity.\n12.3.7\nExamples on Rd\nTo take a geometrically ergodic example that is not uniform, consider the simple\nrandom walk provided by the iterations\nXn+1 = ρXn + τ2ϵn\nwhere ϵn ∼N (0,IdRd), τ2 > 0 and 0 < ρ < 1. One shows easily by induction that\nthe conditional distribution of Xn given X0 = x is Gaussian with mean mn = ρnx and\ncovariance matrix σ2\nnIdRd with\nσ2\nn = 1 −ρ2n\n1 −ρ2 τ2.\nIn particular, the distribution Q = N (0,σ2\n∞IdRd), with σ2\n∞= τ2/(1 −ρ2), is invariant.\nEstimates on the variational distances between Gaussian distributions, such as those\nprovided in Devroye et al. [61], can then be used to show that\nDvar(Pn\nx ,Q) ≤M(x)ρn\nwhere M grows linearly in x but is not bounded.\nSituations in which on can, as above, compute the probability distributions of Xn\nare rare, however, and proving geometric convergence is significantly more difficult\n\n270\nCHAPTER 12. MONTE-CARLO SAMPLING\nthan for finite-state chains. For chains on Rd (or, more generally, locally compact\nmetric spaces), the drift function criterion (12.12) can be used. Assume that Ph(·),\ngiven by\nPh(x) = E(h(Xn+1) | Xn = x) =\nZ\nRd h(y)P(x,dy)\nis continuous as soon as the function h : Rd →R is continuous (one says that the\nchain is weak Feller). This true, for example, if P(x,·) has a p.d.f. with respect\nto Lebesgue’s measure which is continuous in x. In such a situation, one can see\nthat compact sets are small sets, and (12.12) can be restated as the existence if a\npositive function h with compact sub-level sets and such that h(x) ≥1, of a compact\nset C ⊂Rd and of positive constants β < 1 and b such that, for all x ∈Rd,\nPh(x) ≤βh(x) + b1C(x).\n(12.14)\nAs an example, consider the Markov chain defined by\nXn+1 = Xn −δ∇H(Xn) + τϵn+1\nwhere ϵ2,ϵ2,... are i.i.d. standard d-dimensional Gaussian variables and H : Rd →R\nis C2. This chain is clearly irreducible (with respect to Lebesgue’s measure). One has\nPh(x) =\n1\n(2πτ2)d/2\nZ\nRd h(y)e−1\n2τ2 |y−x+δ∇H(x)|2\ndy =\n1\n(2π)d/2\nZ\nRd h(x−δ∇H(x)+τu)e−|u|2\n2 dy.\nLet us make the assumption that H is L-C1 for some L > 0 (c.f. definition 3.15)\nand furthermore assume that |∇H(x)| tends to infinity when x tends to infinity, en-\nsuring the fact that the sets {x : |∇H(x)| ≤c} are compact for c > 0. We want to show\nthat, if δ is small enough, (12.14) holds for h(x) = exp(mH(x)) and m small enough.\nWe first compute an upper bound of\ng(x,u) = mH(x −δ∇H(x) + τu) −|u|2\n2 .\nUsing the L-C1 property, we have\ng(x,u) ≤mH(x) + m(−δ∇H(x) + τu)T ∇H(x) + mL\n2 |δ∇H(x) −τu|2 −|u|2\n2\n= mH(x) −mδ(1 −δL/2)|∇H(x)|2 + mτ(1 −τL)∇H(x)T u −1 −mLτ2\n2\n|u|2\n= mH(x) −1 −mLτ2\n2\n\f\f\f\f\fu −mτ(1 −τL)\n1 −mLτ2 ∇H(x)\n\f\f\f\f\f\n2\n−m\n \nδ(1 −δL/2) −mτ2(1 −τL)2\n2(1 −mLτ2)\n!\n|∇H(x)|2\n\n12.3. MARKOV CHAIN SAMPLING\n271\nAssume that mLτ2 ≤1. It follows that\nPh(x) =\n1\n(2π)d/2\nZ\nRd eg(x,u) du ≤\nh(x)\n(1 −mLτ2)d/2\nexp\n \n−m\n \nδ(1 −δL/2) −mτ2(1 −τL)2\n2(1 −mLτ2)\n!\n|∇H(x)|2\n!\nUsing this upper bound, we see that (12.14) will hold if one first chooses δ such that\nδL < 2, then m such that mLτ2 < 1 and\nmτ2(1 −τL)2\n2(1 −mLτ2) < δ(1 −δL/2)\nand finally choose C = {x : |∇H(x)| ≤c} where c is large enough so that\n1\n(1 −mLτ2)d/2 exp\n \n−m\n \nδ(1 −δL/2) −mτ2(1 −τL)2\n2(1 −mLτ2)\n!\nc2\n!\n< 1.\nNote that this Markov chain is not in detailed balance. Since P(x,·) has a p.d.f.,\nbeing in detailed balance requires the ratio p(x,y)/p(y,x) to simplify as a ratio q(y)/q(x)\nfor some function q, which does not hold. However, we can identify the invariant\ndistribution approximately with small δ and τ, that we will assume to satisfy τ = a\n√\nδ\nfor a fixed a > 0, with δ a small number.\nWe can write\np(x,y) =\n1\n(2πτ2)d/2 exp\n\u0012\n−1\n2τ2|y −x + δ∇H(x)|2\u0013\n=\n1\n(2πτ2)d/2 exp\n \n−1\n2τ2|y −x|2 −δ\nτ2(y −x)T ∇H(x) −δ2\n2τ2|∇H(x)|2\n!\n.\nIf q is a density, we have\nqP(y) =\nZ\nRd q(x)p(x,y)dx\n=\n1\n(2π)d/2\nZ\nRd q(y + a\n√\nδu)exp\n \n−1\n2|u|2 +\n√\nδ\na uT ∇H(y + a\n√\nδu) −δ\n2a2|∇H(y + a\n√\nδu)|2\n!\ndu\nMake the expansions:\nq(y + a\n√\nδu) = q(y) + a\n√\nδ∇q(y)T u + a2δ\n2 uT ∇2q(y)u + o(δ|u|2)\n\n272\nCHAPTER 12. MONTE-CARLO SAMPLING\nand\nexp\n √\nδ\na uT ∇H(y + a\n√\nδu) −δ\n2a2|∇H(y + a\n√\nδu)|2\n!\n= 1 +\n√\nδ\na uT ∇H(y) −δ\n2a2|∇H(y)|2 + δuT ∇2H(u)u + δ\n2a2(uT ∇H(y))2 + o(δ|u|2).\nTaking the product and using the fact that (2π)−d/2 R\nRd u exp(−|u|2/2)du = 0 and\nthat (2π)−d/2 R\nRd uT Au exp(−|u|2/2)du = trace(A) for any symmetric matrix A, we can\nwrite, taking the product:\nqP(y) = q(y) + δ\n a2\n2 ∆q(y) + ∇H(y)T ∇q(y) + q(y)∆H(y)\n!\n+ o(δ)\nThis indicates that, if q is invariant by P, it should satisfy\na2\n2 ∆q(y) + ∇H(y)T ∇q(y) + q(y)∆H(y) = o(1).\nThe partial differential equation\na2\n2 ∆q(y) + ∇H(y)T ∇q(y) + q(y)∆H(y) = 0\n(12.15)\nis satisfied by the function y 7→e−2H(y)\na2 . Assuming that this function is integrable, this\ncomputation suggests that, for small δ, the Markov chain approximately samples\nfrom the probability distribution\nq0 = 1\nZ e−2H(x)\na2 .\nThis is further discussed in the next remark that involves stochastic differential\nequations. We will also present a correction of this Markov chain that samples from\nq0 for all δ in section 12.5.2.\nRemark 12.5 (Langevin equation) This chain is indeed the Euler discretization [106]\nof the stochastic differential equation,\ndxt = −∇H(xt)dt + adwt\n(12.16)\nwhere wt is a Brownian motion. Under general hypotheses, this stochastic diffusion\nequation, called a Langevin equation, indeed converges in distribution to q0(x). 5\n5Providing a rigorous account of the theory of stochastic differential equations is beyond our\nscope, and we refer the reader to the many textbooks on the subject, such as McKean [131], Ikeda\nand Watanabe [96], Ethier and Kurtz [69] (see also Berglund [26] for a short introduction).\n\n12.4. GIBBS SAMPLING\n273\nSuch diffusions are continuous-time Markov processes (Xt,t ≥0), which means\nthat the probability distribution of Xt+s given all events before and including time s\nonly depends on Xs and is provided by a transition probability Pt, with\nP(Xt+1 ∈A | Xs = x) = Pt(x,A).\nSimilarly to deterministic ordinary differential equations, one shows that under suf-\nficient regularity conditions (e.g., ∇H is C1), equations such as (12.16) have solutions\nup to some positive (random) explosion time, and that this explosion time is finite\nunder additional conditions that ensure that |∇H(x)| does not grow too fast when x\ntends to infinity.\nIf ϕ is a smooth enough function (say, C2, with compact support), the function\n(t,x) 7→Ptϕ(x) satisfies the partial differential equation, called Kolmogorov’s back-\nward equation,\n∂tPtϕ(x) = −∇H(x)T ∇Ptϕ(x) + a2\n2 ∆Ptϕ(x)\nwith initial condition P0ϕ(x) = ϕ(x). If Pt(x,·) has at all times t a p.d.f. pt(x,·), then\nthis p.d.f. must satisfy the forward Kolmogorov equation:\n∂tpt(x,y) = ∇2 · (∇H(y)pt(x,y)) + a2\n2 ∆2pt(x,y)\nwhere ∇2 and ∆2 indicate differentiation with respect to the second variable (y). (Re-\ncall that δf denotes the Laplacian of f .) Moreover, if Q is an invariant distribution\nwith p.d.f. q, it satisfies the equation\n∇· (q∇H) + a2\n2 ∆q(y) = 0.\nNoting that ∇· (q∇H) = ∇qT ∇H + q∆H, we retrieve (12.15). Convergence proper-\nties (and, in particular, geometric convergence) of the Langevin equation to its limit\ndistribution are studied in Roberts and Tweedie [166], using methods introduced in\nMeyn and Tweedie [134, 135, 136]\n♦\n12.4\nGibbs sampling\n12.4.1\nDefinition\nThe Gibbs sampling algorithm [79] was introduced to sample from a distribution\non large sets for which direct sampling is intractable and rejection samping is inef-\nficient. It generates a Markov chain that converges (under some hypotheses) in dis-\ntribution to this target probability. A general version of this algorithm is described\nbelow.\n\n274\nCHAPTER 12. MONTE-CARLO SAMPLING\nLet Q be a probability distribution on B. Consider a finite family U1,...,UK of\nrandom variables defined on B with values in measurable spaces B′\n1,...,B′\nK. Let\nQi = QUi denote the image of Q by Ui, defined by Qi(Bi) = Q(Ui ∈Bi) for Bi ⊂Bi.\nAlso, assume that there exists, for all i, a regular family of conditional probabilities\nfor Q given Ui, defined as a collection of transition probabilities (ui,A) 7→Qi(ui,A)\nfor ui ∈Bi and A ⊂B, that satisfy\nZ\nA\ng(Ui(x))Q(dx) =\nZ\nBi\nQi(ui,A)g(ui)Qi(dui)\nfor all nonnegative measurable functions g : Bi →R. In simpler terms, Qi(ui,A)\ndetermine a consistent set of conditional probabilities for Q(A | Ui = ui). For dis-\ncrete random variables (resp. variables with p.d.f.’s on Rd), they are just elementary\nconditional probabilities.\nWe then consider the following algorithm.\nAlgorithm 12.2 (Gibbs sampling)\nInitialize the algorithm with some z(0) = z0 ∈B and iterate the following two update\nsteps given a current z(n) ∈B:\n(1) Select j ∈{1,...,K} according to some pre-defined scheme, i.e., at random ac-\ncording to a probability distribution π(n) on the set {1,...,K}.\n(2) Sample a new value z(n+1) according to the probability distribution Qj(Uj(z(n)),·).\nOne typically chooses the probability distribution in Step 1 equal to the uniform\ndistribution on {1,...,K} (in which case it is independent on n), or to π(n) = δjn where\njn = 1+(n (mod) K) (periodic scheme). Strictly speaking, Gibbs sampling is a Markov\nchain if π(n) does not depend on n, and we will make this simplifying assumption in\nthe rest of our discussion (therefore replacing π(n) by π). One obvious requirement\nfor the feasibility of the method is that step (2) can be performed efficiently since it\nmust be repeated a very large number of times.\nOne can see that the Markov chain generated by this algorithm is Q-reversible.\nIndeed, assume that Xn ∼Q. For any (measurable) subsets A and B in B, one has,\nusing the definition of conditional expectations,\nP(Xn ∈A,Xn+1 ∈B) =\nK\nX\ni=1\nE\n\u0010\n1Z∈AQi(Ui(Z),B)\n\u0011\nπ(i).\n(12.17)\n\n12.4. GIBBS SAMPLING\n275\nNow, for any i\nE\n\u0010\n1Z∈AQi(Ui(Z),B)\n\u0011\n=\nZ\nA\nQi(Ui(z),B)Q(dz)\n=\nZ\nBi\nQi(ui,A)Qi(ui,B)Qi(dui)\nwhich is symmetric in A and B.\nNote that, in the discrete case\nP(z, ˜z) =\nn\nX\ni=1\nπ(i)\nQ(˜z)1Ui(˜z)=Ui(z)\nP\nz′:Ui(z′)=Ui(z) Q(z′)\n(12.18)\nand the relation Q(z)P(z, ˜z) = Q(˜z)P(˜z,z) is obvious.\nThe conditioning variables U1,...,UK should ensure, at least, that the associated\nMarkov chain is irreducible and aperiodic. For irreducibility, this requires that Z\ncan visits Q-almost all elements of B by a sequence of steps that lead one of the Ui’s\ninvariant.\nRemark 12.6 In the standard version of Gibbs sampling, B is a product space B1 ×\n··· × BK, and\nB′\nj = B1 × ··· × Bj−1 × Bj+1 × ··· × BK.\nOne then takes Uj(z(1),...,z(K)) = (z(1),...,z(i−1),z(i+1),...,z(K)). In other terms, step 2\nin the algorithm replaces the current value of z(j)(n) by a new one sampled from the\nconditional distribution of Z(j) given the current values of z(i)(n),i , j.\n♦\nRemark 12.7 We have considered a fixed number of conditioning variables, U1,...,UK,\nfor simplicity, but the same analysis can be carried on if one replaces Uj by a func-\ntion U : (x,θ) 7→Uθ(x) defined on a product space B × Θ, taking values in some\nspace ˜B, where Θ is a probability space equipped with a probability distribution\nπ and B is measurable. The previous discussion corresponds to Θ = {1,...,K} and\nB = SK\ni=1{i} × Bi (so that Ui(x) is replaced by (i,Ui(x))).\nOne may then define Qθ as the image of Q by Uθ and let Qθ(u,A) provide a\nversion of Q(A | Uθ = u). The only change in the previous discussion (besides using\nθ in index) is that (12.17) becomes\nP(Xn ∈A,Xn+1 ∈B) =\nZ\nΘ\nE\n\u0010\n1Z∈AQθ(Uθ(Z),B)\n\u0011\nπ(dθ).\n♦\n\n276\nCHAPTER 12. MONTE-CARLO SAMPLING\nRemark 12.8 Using notation from the previous remark, and allowing π = π(n) to\ndepend on n, it is possible to allow π(n) to depend on the current state z(n) using the\nfollowing construction.\nFor every step n, assume that there exists a subset Θn of Θ such that π(n)(z,Θn) = 1\nand that, for all θ ∈Θn, π(n) can be expressed in the form\nπ(n)(z,·) = ψ(n)\nθ (Uθ(z),·)\nfor some transition probability ψ(n)\nθ\nfrom Bθ to Θn. The resulting chain remains\nQ-reversible, since\nP(Xn ∈A,Xn+1 ∈B) =\nZ\nB\nZ\nΘn\n1z∈AQθ(Uθ(z),B)π(n)(z,dθ)Q(dz)\n=\nZ\nΘn\nZ\nB\n1z∈AQθ(Uθ(z),B)ψ(n)\nθ (Uθ(z),dθ)Q(dz)\n=\nZ\nΘn\nZ\n˜B\nQθ(u,A)Qθ(u,B)ψ(n)\nθ (u,dθ)Qθ(du).\n♦\n12.4.2\nExample: Ising model\nWe will see several examples of applications of Gibbs sampling in the next few chap-\nters. Here, we consider a special instance of Markov random field (see chapter 13)\ncalled the Ising model. For this example, B = {0,1}L, and\nq(z) = 1\nC exp\n\n\nL\nX\nj=1\nαz(j) +\nL\nX\ni,j=1,i<j\nβijz(i)z(j)\n\n.\nNote that, although B is a finite set, its cardinality, 2L, is too large for the enumerative\nprocedure described in section 12.1 to be applicable as soon as L is, say, larger than\n30. In practical applications of this model, L is orders of magnitude larger, typically\nin the thousands or tens of thousands.\nWe here apply standard Gibbs sampling, as described in remark 12.6, defining\nBj = {0,1} and\nUi(z(1),...,z(L)) = (z(1),...,z(i−1),z(i+1),...,z(L)).\nThe conditional distribution of Z(j) given Uj(z) is a Bernoulli distribution with pa-\nrameter\nqZ(j)(1|Uj(z)) =\nexp(α + PL\nj′=1,j′,j βjj′z(j′))\n1 + exp(α + PL\nj′=1,j′,j βijz(j))\n\n12.5. METROPOLIS-HASTINGS\n277\n(taking βjj′ = βj′j for j > j′). Gibbs sampling for this model will generate a sequence\nof variables Z(0),Z(1),... by fixing Z(0) arbitrarily and, given Z(n) = z, applying the\ntwo steps:\n1. Select j ∈{1,...,L} at random according to a probability distribution π(n) on the\nset {1,...,L}.\n2. Sample a new value ζ ∈{0,1} according to the Bernoulli distribution with pa-\nrameter qZ(j)\nn (1|Uj(z)), and set Z(j)(n + 1) = ζ and Z(j′)(n + 1) = Z(j′)(n) for j′ , j.\nLet us now consider the Ising model with fixed total activation, namely the pre-\nvious distribution conditional to S(z)\n∆= z(1) + ··· + z(L) = h where 0 < h < L. The\ndistribution one wants to sample from now is\nqh(z) = 1\nCh\nexp\n\n\nL\nX\nj=1\nαz(j) +\nL\nX\ni,j=1,i<j\nβijz(i)z(j)\n\n1S(z)=h.\nIn that case, the previous choice for the one-step transitions does not work, because\nfixing all but one coordinate of z also fixes the last one (so that the chain would not\nmove from its initial value and would certainly not be irreducible). One can however\nfix all but two coordinates, therefore defining\nUij(z(1),...,z(L)) = (z(1),...,z(i−1),z(i+1),...,z(j−1),z(j+1),...,z(L))\nand Bij = {0,1}2. If Uij(z) is fixed, the only acceptable configurations are z and the\nconfiguration z′ deduced from z by switching the value of z(i) and z(j). Thus, there\nis no possible change is z(i) = z(j). If z(i) , z(j), then the probability of flipping the\nvalues of z(i) and z(j) is qh(z′)/(qh(z) + qh(z′)).\n12.5\nMetropolis-Hastings\n12.5.1\nDefinition\nGibbs sampling is a special case of a generic MCMC algorithm called Metropolis-\nHastings that is defined as follows [133, 88]. Assume that the distribution Q has a\ndensity q with respect to a measure µ on B. Specify a transition probability on B,\nrepresented by a family of density functions with respect to µ, (g(z,·),z ∈B), and a\nfamily of acceptance functions (z,z′) 7→a(z,z′) ∈[0,1]. Two basic examples are when\nB is finite, µ is the counting measure, and q and g are probability mass functions, and\nwhen B = Rd, µ is Lebesgue’s measure and q and g are probability density functions.\nThe sampling algorithm is then defined as follows. It invokes a function a that\nwill be specified below.\n\n278\nCHAPTER 12. MONTE-CARLO SAMPLING\nAlgorithm 12.3 (Metropolis-Hastings)\nInitialize the algorithm with Z(0) = z(0) ∈B. At step n, the current value Z(n) = z is\nthen updated as follows.\n• “Propose” a new configuration z′ drawn according to g(z,·).\n• “Accept” z′ (i.e., set Z(n + 1) = z′) with probability a(z,z′). If the new value is\nrejected, keep the current one, i.e., let Z(n + 1) = z.\nThe transition probabilities for this process are p(x,y) = g(x,y)a(x,y) if x , y and\np(x,x) = 1 −P\ny,x p(x,y). The chain is Q-reversible is the detailed balance equation\nq(z)g(z,z′)a(z,z′) = q(z′)g(z′,z)a(z′,z)\n(12.19)\nis satisfied. The functions g and a are part of the design of the algorithm, but (12.19)\nsuggest that g should satisfy the “weak symmetry” condition:\n∀x,y ∈Ω: g(x,y) = 0 ⇔g(y,x) = 0.\n(12.20)\nNote that this condition is necessary to ensure (12.19) if q(z) > 0 for all z. If q(z) > 0,\nthe fact that acceptance probabilities are less than 1 requires that\na(z,z′) ≤min\n \n1, q(z′)g(z′,z)\nq(z)g(z,z′)\n!\n.\nIf one takes a(z,z′) equal to the r.h.s., so that\na(z,z′) = min\n \n1, q(z′)g(z′,z)\nq(z)g(z,z′)\n!\n,\n(12.21)\nthen (12.19) is satisfied as soon as q(z) > 0. If q(z) = 0, then this definition ensures\nthat a(z′,z) = 0 and (12.19) is also satisfied. Note also that the case g(z,z′) = 0 is not\nrelevant, since z′ is not attainable from z in one step in this case. This shows that\n(12.21) provides a Q-reversible chain. Obviously, if g already satisfies q(z)g(z,z′) =\nq(z′)g(z′,z), which is the case for Gibbs sampling, then one should take a(z,z′) = 1 for\nall z and z′.\n12.5.2\nSampling methods for continuous variables\nWhile the Gibbs sampling and Metropolis-Hastings methods can be (and were) for-\nmulated for general variables and probability distributions, proving that the re-\nlated chains are ergodic, and checking conditions for geometric convergence speed\nis much harder when dealing with general state spaces than with finite or com-\npact spaces (see, e.g., [164, 132, 6, 165]). On the other hand, interesting choices\n\n12.5. METROPOLIS-HASTINGS\n279\nof proposal transitions for Metropolis-Hastings are available when B = Rd and µ is\nLebesgue’s measure, taking advantage, in particular, of differential calculus. More\nprecisely, assume that q takes the form\nq(z) = 1\nC exp(−H(z))\nfor some smooth function H (at least C1), such that exp(−H) is integrable. We saw\nin section 12.3.7 that, under suitable assumptions, the Markov chain\nXn+1 = Xn −δ\n2∇H(Xn) +\n√\nδϵn+1\n(12.22)\nwith ϵn+1 ∼N (0,IdRd) has q as invariant distribution in the limit δ →0. Its transition\nprobability, such that g(z,·) is the p.d.f. of N (z−δ\n2∇H(z),δIdRd), is therefore a natural\nchoice for a proposal distribution in the Metropolis-Hastings algorithm. In addition\nto converging from the exact target distribution, this “Metropolis Adjusted Langevin\nAlgorithm” (or MALA) can also be proved to satisfy geometric convergence under\nless restrictive hypotheses than (12.22) [166].\nAnother approach, similar to MALA is the Hamiltonian Monte-Carlo methods\n(or hybrid Monte-Carlo) [65, 142]. Inspired by physics, the method introduces a\nnew variable, p ∈Rd, called “momentum,” and defines the “Hamiltonian:”\nH(z,m) = H(z) + 1\n2|m|2.\nFix a time θ > 0. The proposal transition g(z,·) is then defined as the value ζ(θ) that\nis obtained by solving the Hamiltonian dynamical system\n(∂tζ(t) = ∂pH(ζ(t),µ(t)) = µ(t)\n∂tµ(t) = −∂zH(ζ(t),µ(t)) = −∇H(ζ(t))\n(12.23)\nwith ζ(0) = z and µ(0) ∼N (0,IdRd). One can easily see that ∂tH(ζ(t),µ(t)) = 0, which\nimplies that\nH(ζ(t)) + 1\n2|µ(t)|2 = H(z) + 1\n2|µ(0)|2\nat all times t, or, denoting by ϕN the p.d.f. of the d-dimensional standard Gaussian,\nq(ζ(t))ϕN (µ(t)) = q(ζ(0))ϕN (µ(0)).\nMoreover, if one denotes by Φt(z,m) = (zt(z,m),mt(z,m)) the solution (ζ(t),µ(t)) of\nthe system started with ζ(0) = z and µ(0) = m, one can also see that det(dΦt(z,m)) = 1\nat all times. Indeed, applying (1.5) and the chain rule, we have\n∂t logdet(dΦt(z,m)) = trace(dΦt(z,m)−1∂tdΦt(t,m)).\n\n280\nCHAPTER 12. MONTE-CARLO SAMPLING\nFrom\n(∂tzt(z,m) = mt(z,p)\n∂tpt(z,m) = −∇H(zt(z,m))\nwe get\n∂tdΦt(z,m) =\n \n∂zmt(z,m)\n∂mmt(z,m)\n−∇2H(zt(z,m))∂zzt(z,m)\n−∇2H(zt(z,m))∂mzt(z,m)\n!\n=\n \n0\nIdRd\n−∇2H(zt(z,m))\n0\n!\ndΦt(z,m).\nWe therefore get\n∂t logdet(dΦt(z,m)) = trace\n \n0\nIdRd\n−∇2H(zt(z,m))\n0\n!\n= 0\nshowing that the determinant is constant. Since Φ0(z,m) = (z,m) by definition, we\nget det(dΦt(z,m)) = 1 at all times.\nLet ¯qt denote the p.d.f. of Φt(z,m) and assume that ¯q0(z,m) = q(z)ϕN (m). We\nhave, using the change of variable formula\n¯qt(Φt(z,m))|detdΦt(z,m)| = q(z)ϕN (m)\nbut the r.h.s. is, from the remarks above also equal to\nq(zt(z,m))ϕN (mt(z,m))|detdΦt(z,m)|\nyielding the identification\n¯qt(z′,m′) = q(z′)ϕN (m′)\nThis shows that Q (with p.d.f. q) is left invariant by this Markov chain. One can ac-\ntually show that chain is in detailed balance for the joint density ¯q(z,m) = q(z)ϕN (m).\nThis is due to the fact that the system (12.23) is reversible, in the sense that\nΦt(zt(z,m),−mt(z,m)) = (z,−m),\ni.e., the system solved from its end point after changing the sign of the momentum\nreturns to its initial state after changing the sign of the momentum a second time.\nIn other terms, letting J(z,m) = (z,−m), we have Φ−1\nt\n= JΦt ◦J. So, consider a function\nf : (Rd ×Rd)2 →R. Denoting the Markov chain by (Zn,Mn), we assume that the next\npair Zn+1,Mn+1 is computed by (i) sampling M′\nn ∼N (0,IdRd); (ii) solving (12.23),\nwith initial conditions ζ(0) = Zn and µ(0) = M′\nn; (iii) taking Zn+1 = ζ(θ) and sampling\nMn+1 ∼N (0,IdRd).\nWe have\nE(f (Zn,Mn,Zn+1,Mn+1)) =\nZ\nf (z, ˜m,z(z,m), ¯m)ϕN (m)ϕN ( ¯m)ϕN ( ˜m)q(z)dmd ¯md ˜mdz.\n\n12.5. METROPOLIS-HASTINGS\n281\nMake the change of variables z′ = z(z,m), m′ = m(z,m), which has Jacobian determi-\nnant 1, and is such that z = z(z′,−m′), m = −m(z′,−m′). We get\nE(f (Zn,Mn,Zn+1,Mn+1))\n=\nZ\nf (z(z′,−m′), ˜m,z′, ¯m)ϕN (−m(z′,−m′))ϕN ( ¯m)ϕN ( ˜m)q(z(z′,−m′))dm′d ¯md ˜mdz′\n=\nZ\nf (z(z′,−m′), ˜m,z′, ¯m)ϕN (m(z′,−m′))ϕN ( ¯m)ϕN ( ˜m)q(z(z′,−m′))dm′d ¯md ˜mdz′\n=\nZ\nf (z(z′,−m′), ˜m,z′, ¯m)ϕN (−m′)ϕN ( ¯m)ϕN ( ˜m)q(z′)dm′d ¯md ˜mdz′,\nusing the conservation of H. Making the change of variables m′ →−m′, we get\nE(f (Zn,Mn,Zn+1,Mn+1)\n=\nZ\nf (z(z′,m′), ˜m,z′, ¯m)ϕN (m′)ϕN ( ¯m)ϕN ( ˜m)q(z′)dm′d ¯md ˜mdz′\nwhich is equal to E(f (Zn+1,Mn+1,Zn,Mn)) showing the reversibility of the chain.\nThis simulation scheme can potentially make large moves in the current configu-\nration z while maintaining detailed balance (therefore not requiring an accept/reject\nstep). However, practical implementations require discretizing (12.23), which breaks\nthe conservation properties that were used in the argument above, therefore requir-\ning a Metropolis-Hastings correction. For example, a second-order Runge Kutta\n(RK2) scheme with time step α gives\n\nZn+1 = Zn + αMn −α2\n2 ∇H(Zn)\nMn+1 = Mn −α\n2 (∇H(Zn) + ∇H(Zn + hMn))\nOnly the update for Zn matters, however, since Mn+1 is discarded and resampled at\neach step. Importantly, if we let δ = √α the first equation in the system becomes\nZn+1 = Zn −δ\n2∇H(Zn) + δMn\nwith Mn ∼N (0,1), which is exactly (12.22). Note that one can, in principle, solve\n(12.23) for more that one discretization step (the continuous equation can be solved\nfor an arbitrary time), but one must then face the challenge of computing the Metropo-\nlis correction since the Hamiltonian is not conserved at each step.\nOne can however use schemes that are more adapted to solving Hamiltonian\n\n282\nCHAPTER 12. MONTE-CARLO SAMPLING\nsystems [119], such as the St¨ormer-Verlet scheme, which is\n\nMn+1/2 = Mn −α\n2 ∇H(Zn)\nZn+1 = Zn + αMn+1/2\nMn+1 = Mn+1/2 −α\n2 ∇H(Zn+1)\nThis scheme computes ψ1 ◦ψ2 ◦ψ1(z,m) with ψ1(z,m) = (z,m −(α/2)∇H(z)) and\nψ2(z,m) = (z + αm,m). Because both ψ1 and ψ2 have a Jacobian determinant equal to\n1, so does their composition. This scheme is also reversible, since we have\n\n−Mn+1/2 = −Mn+1 −α\n2 ∇H(Zn+1)\nZn = Zn+1 −αMn+1/2\n−Mn = −Mn+1/2 −α\n2 ∇H(Zn)\nThese properties are conserved if one applies the St¨ormer-Verlet scheme more than\nonce at each iteration, that is, fixing some N > 0 and letting Φ(z,m) = (ψ1◦ψ2◦ψ1)◦N,\nthen Φ−1 = JΦ ◦J, with J(z,m) = (z,−m) with detdΦ = 1. Considering again the aug-\nmented chain which, starting from (Zn,Mn), samples ˜M ∼N (0,IdRd), then computes\n(Z′, ˜M′) = Φ(Zn, ˜M) and finally samples M′ ∼N (0,IdRd) as a Metropolis-Hastings\nproposal to sample from (z,m) 7→q(z)ϕN (m), then, assuming that (Z,M) follows this\ntarget distribution and letting (Z′,M′) be the result of the proposal distribution, we\nhave, as computed above\nE(f (Z,M,Z′,M′))\n=\nZ\nf (z, ˜m,z(z,m), ¯m)ϕN (m)ϕN ( ¯m)ϕN ( ˜m)q(z)dmd ¯md ˜mdz\n=\nZ\nf (z(z′,m′), ˜m,z′, ¯m)ϕN (m(z′,m′))ϕN ( ¯m)ϕN ( ˜m)q(z(z′,m′))dm′d ¯md ˜mdz′\nThis shows that the acceptance probability in the Metropolis step is\na(z,m,z′,m′) = min\n \n1, ϕN (m(z′,m′))q(z(z′,m′))\nϕN (m)q(z)\n!\n= exp(−max(H(z(z′,m′),m(z′,m′)) −H(z,m)),0)\nWhile the Hamiltonian is not kept invariant by the St¨ormer-Verlet scheme, so that an\naccept-reject step is needed, it is usually quite stable over extended periods of time\nso that the acceptance probability is generally close to one.\n12.6\nPerfect sampling methods\nThe Markov chain simulation methods, provided in the previous sections do not\nprovide exact samples from the distribution q, but only increasingly accurate ap-\n\n12.6. PERFECT SAMPLING METHODS\n283\nproximations. Perfect sampling algorithms [156, 157, 71] use Markov chains “back-\nwards” to generate exact samples. To describe them, it is easier to describe a Markov\nchain as a stochastic recursive equation of the form\nXn+1 = f (Xn,Un+1)\n(12.24)\nwhere Un+1 is independent of Xn,Xn−1,..., and the Uk’s are identically distributed. In\nthe discrete case (assumed in this section), and given a stochastic matrix P, one can\ntake Un to be the uniformly distributed variable used to sample from (p(Xn,x),x ∈B).\nConversely, the transition probability associated to (12.24) is p(x,y) = P(f (x,U) = y).\nIt will be convenient to consider negative times also. For n > 0, recursively define\nF−n(x,u−n+1,...,u0) by\nF−n−1(x,u−n,...,u0) = F−n(f (x,u−n),u−n+1,...,u0)\nand F−1(x,u0) = f (x,u0). Denote, for short, U0\n−n = (U−n,...,U0). The function F−n(x,u0\n−n+1)\nprovides the value of X0 when X−n = x and U0\n−n+1 = u0\n−n+1.\nFor an infinite past sequence, u0\n−∞, let ν(u0\n−∞) denote the first integer n such that\nF−n(x,u0\n−n+1) does not depend on x (the function “coalesces”). Then, the following\ntheorem is true:\nTheorem 12.9 Assume that the chain defined by (12.24) is ergodic, with invariant dis-\ntribution Q. Then ν = ν(U0\n−∞) is finite with probability 1, and\nX∗:= F−ν(x,U0\n−ν+1)\n(12.25)\n(which is independent of x) has distribution Q.\nProof Because the chain is ergodic, we know that there exists an integer N such\nthat one can pass from any state to any other with positive probability. So the chain\ncan, starting from anywhere, coalesce with positive probability in N steps; ν being\ninfinite would imply that this event never occurs in an infinite number of trials, and\nthis has probability 0.\nFor any k > 0 and any x ∈B, we have\nX∗= F−ν(f−k(x,U−ν\n−ν−k+1),U0\n−ν+1) = F−ν−k(x,U0\n−ν−k+1).\n(12.26)\nBut, because the chain is ergodic, we have, for any x ∈B\nlim\nk→∞P(F−k(x,U0\n−k+1) = y) = Q(y).\nWe can write\nP(F−k(x,U0\n−k+1) = y) = P(F−k(x,U0\n−k+1) = y,ν ≤k) + P(F−k(x,U0\n−k+1) = y,ν > k)\n= P(X∗= y,ν ≤k) + P(F−k(x,U0\n−k+1) = y,ν > k)\n\n284\nCHAPTER 12. MONTE-CARLO SAMPLING\nThe right-hand side tends to P(X∗= y) when k tends to infinity (because P(ν > k)\ntends to 0), and the left-hand side tends to Q(y), which gives the second part of the\ntheorem.\n■\nFrom (12.26), which is the key step in proving that X∗follows the invariant distri-\nbution, one can see why it is important to consider sampling that expands backward\nin time rather than forward. More specifically, consider the coalescence time for the\nforward chain, letting ˜ν(u∞\n0 ) be the first index for which\n˜X∗:= F ˜ν(x,u ˜ν\n0)\nis independent from the starting point, x. For any k ≥0, one still has the fact that\nF ˜ν+k(x,u ˜ν+k\n0\n) does not depend on x, but its value depends on k and will not be equal\nto ˜X∗anymore, which prevents the rest of the proof of theorem 12.9 to carry on.\nAn equivalent algorithm is described in the next proposition (the proof is easy\nand left to the reader).\nProposition 12.10 Using the same notation as above, the following algorithm generates\na perfect sample, ξ∗, of the invariant distribution of an ergodic Markov chain.\nAssume that an infinite sample u0\n−∞of U is available. Given this sequence, the algo-\nrithm, starting with t0 = 2, is:\n1. For all x ∈B, define ξx\n−t,t = −t0,...,0 by ξx\n−t0 = x and ξx\n−t+1 = f (ξx\n−t,u−t+1).\n2. If ξx\n0 is constant (independent of x), let ξ∗be equal to this constant value and stop.\nOtherwise, return to step 1 replacing t0 with 2t0.\nIn practice, the u−k’s are only generated when they are needed. But it is important to\nconsider the sequence as fixed: once u−k is generated, it must be stored (or identically\nregenerated, using the same seed) for further use. It is important to strengthen the\nfact that this algorithm works backward in time, in the sense that the first states of\nthe sequence are not identical at each iteration, because they are generated using\nrandom numbers with indexes further in the past.\nSuch an algorithm is not feasible when |B| is too large, since one would have to\nconsider an intractable number of Markov chains (one for each x ∈B). However\nthere are cases in which the constancy of ξx\n0 over all B can be decided from its con-\nstancy over a small subset of B.\nOne situation in which this is true is when the Markov chain is monotone, ac-\ncording to the following definition. Assume that B can be partially ordered, and\nthat f in (12.24) is increasing in x, i.e.,\nx ≤x′ ⇒∀u,f (x,u) ≤f (x′,u).\n(12.27)\n\n12.6. PERFECT SAMPLING METHODS\n285\nLet Bmin and Bmax be the set of minimal and maximal elements in B. Then the\nsequence coalesces for the algorithm above if and only if it coalesces over Bmin ∪\nBmax. Indeed, any x ∈B is smaller than some maximal element, and larger than\nsome minimal element in B. By (12.27), these inequalities remain true at each step\nof the sampling process, which implies that when chains initialized with extremal\nelements coalesce, so do the other ones. Therefore, it suffices to run the algorithm\nwith extremal configurations only.\nOne can rewrite (12.27) in terms of transition probabilities p(x,y), assuming that\nU follows a uniform distribution on [0,1] and, for all x ∈B, there exists a partition\n(Ixy,y ∈B) of B, such that\nf (x,u) = y ⇔u ∈Ix,y\nand Ixy is an interval with length pxy. Condition (12.27) is then equivalent to\nx ≤x′ ⇒∀y ∈B,Ixy ⊂\n[\ny′≥y\nIx′y′.\nThis requires in particular that P\ny≥y0 p(x,y) ≤P\ny≥y0 p(x′,y) whenever x ≤x′ (one\nsays that p(x,·) is stochastically smaller than p(x′,·)).\nOne example in which this reduction works is with the ferromagnetic Ising model,\nfor which B = {−1,1}L and\nq(x) = 1\nC exp\n\u0010\nL\nX\ns,t=1,s<t\nβstx(s)x(t)\u0011\nwith βst ≥0 for all {s,t}. Then, the Gibbs sampling algorithm iterates the follow-\ning steps: take a random s ∈{1,...,L} and update x(s) according to the conditional\ndistribution\ngs(y(s) | x(sc)) =\ney(s)vs(x)\ne−vs(x) + evs(x)\nwith vs(x) = P\nt,s βstx(t). Order B so that x ≤˜x if and only if x(s) ≤˜x(s) for all s =\n1,...,L. The minimal and maximal elements are unique in this case, with x(s)\nmin ≡−1\nand x(s)\nmax ≡1. Moreover, because all βst are non-negative, vs is an increasing function\nof x so that, if x ≤˜x, then gs(1 | x(s)) ≤gs(1 | ˜x(s)).\nTo define the stochastic iterations, first introduce\nfs(x,u) =\n\n1(s) ∧x(sc) if u ≤qs(1 | x(s))\n(−1)(s) ∧x(sc) if u > qs(1 | x(s)),\nwhich satisfies (12.27). The whole updating scheme can then be implemented with\nthe function\nf (x,(u, ˜u)) =\nL\nX\ns=1\nδIs( ˜u)fs(x,u)\n\n286\nCHAPTER 12. MONTE-CARLO SAMPLING\nwhere (Is,s ∈V ) is any partition of [0,1] in intervals of length 1/L. This is still mono-\ntonic. The algorithm described in proposition 12.10 can therefore be applied to\nsample exactly, in finite time, from the ferromagnetic Ising model.\n12.7\nApplication: Stochastic approximation with Markovian tran-\nsitions\nUsing the material developed in this chapter, we now discuss the convergence of\nstochastic approximation methods (such as stochastic gradient descent) when the\nrandom random variable in the update term follows Markovian transitions. In sec-\ntion 3.3, we considered algorithms in the form\n(ξt+1 ∼πXt\nXt+1 = Xt + αt+1H(Xt,ξt+1)\nwhere ξt : Ω→Rξ is a random variable. We now want to addres situations in which\nthe random variable ξt+1 is obtained through a transition probability, therefore con-\nsidering the algorithm\n(ξt+1 ∼PXt(ξt,·)\nXt+1 = Xt + αt+1H(Xt,ξt+1)\n(12.28)\nHere Px is, for all x, a transition probability from Rξ to Rξ. We will assume that,\nfor all x ∈Rd, the Markov chain with transition Px is geometrically ergodic, and we\ndenote by πx its invariant distribution. We let, as in section 3.3, ¯H(x) = Eπx(H(x,·)).\nWe will use the notation for a function f : Rd × Rξ →R\nPxf : (x′,ξ) ∈Rd × Rξ 7→Pxf (x′,ξ) =\nZ\nRξ\nf (x′,ξ′)Px(ξ,dξ′)\nand\nπxf : x′ ∈Rd 7→πxf (x′) =\nZ\nRξ\nf (x′,ξ)πx(dξ).\nIn particular, ¯H(x) = πxH(x). We also define h(x,ξ) = H(x,ξ) −¯H(x) and ˜h(x,ξ) =\nPxh(x,ξ). We make the following assumptions.\n(H1) There exists constants C0,C1,c2 such that, for all x,y ∈Rd,\nsup\nξ∈Rξ\n|H(x,ξ)| ≤C0,\n(12.29a)\nsup\nξ∈Rξ\n|˜h(x,ξ)| ≤C1,\n(12.29b)\nsup\nξ∈Rξ\n|˜h(x,ξ) −˜h(y,ξ)| ≤C1|x −y|,\n(12.29c)\nDvar(πx,πy) ≤C2|x −y|\n(12.29d)\n\n12.7. MARKOVIAN STOCHASTIC APPROXIMATION\n287\n(H2) There exists x∗∈Rd and µ > 0 such that, for all x ∈Rd\n(x −x∗)T ¯H(x) ≤−µ|x −x∗|2.\n(12.30)\n(H3) We assume that there exists a constant M and a non-decreasing function ρ :\n[0,+∞) →[0,1) such that, for all probability distributions Q and Q′ on Rξ,\nDvar(QPn\nx ,Q′Pn\nx ) ≤Mρ(|x|)nDvar(Q,Q′).\n(12.31)\n(H4) The sequence α1,α2,... is non-increasing, with\n∞\nX\nt=1\nαt = +∞\nand\n∞\nX\nt=1\nα2\nt < +∞.\n(12.32a)\nLet σt = Pt\ns=1 αs. If C1 > 0, we also require that\nlim\nt→∞αtσt(1 −ρ(σt))−1 = 0\n(12.32b)\nand\nt\nX\ns=2\nα2\ns σs(1 −ρ(σs))−2 < ∞.\n(12.32c)\nGiven this, the following theorem holds.\nTheorem 12.11 Assuming (H1) to (H4), the sequence defined by (12.28) is such that\nlim\nt→∞E(|Xt −x∗|2) = 0\nRemark 12.12 Condition (H1) assumes that H is bounded and uniformly Lipschitz\nin x, which is more restrictive than what was assumed in section 3.3.2, but applies,\nfor example, to situations considered in Younes [206] and later in this book in sec-\ntion 17.2.2.\nCondition (H3) implies that the Markov chain with transition Px is uniformly\ngeometrically ergodic, but the ergodicity rate may depend on x and in particular\nconverge to 1 when x tends to ∞, which is the situation targeted in this theorem.\nThe reader may refer to [208] for a general discussion of this problem with re-\nlaxed hypotheses and almost sure convergence, at the expense of significantly longer\nproofs.\n♦\nProof We note that, from (12.29a), one has\n|Xt −x∗| ≤C0σt|X0 −x∗|.\n(12.33)\n\n288\nCHAPTER 12. MONTE-CARLO SAMPLING\nSimilarly to section 3.3.2, we let At = |Xt −x∗|2 and at = E(At). One can then write\nAt+1 = At+2αt+1(Xt−x∗)T ¯H(Xt)+2αt+1(Xt−x∗)T (H(Xt,ξt+1)−¯H(Xt))+α2\nt+1|H(Xt,ξt+1)|2\nbut we do not have\nE((Xt −x∗)T (H(Xt,ξt+1) −¯H(Xt)) | Ut) = 0\nanymore, where Ut is the σ-algebra of all past events up to time t (all events depend-\ning of Xs,ξs, s ≤t). Indeed the Markovian assumption implies that\nE((Xt −x∗)T (H(Xt,ξt+1) −¯H(Xt)) | Ut) = (Xt −x∗)T\n\n\nZ\nRξ\nH(Xt,ξ)PXt(ξt,dξ) −¯H(Xt)\n\n\n= (Xt −x∗)T ((PXtH(Xt,·))(ξt) −¯H(Xt)),\nwhich does not vanish in general. Following Benveniste et al. [25], this can be ad-\ndressed by introducing the solution g(x,·) of the “Poisson equation”\ng(x,·) −Pxg(x,·) = h(x,·).\n(12.34)\n(Recall that h(x,ξ) = H(x,ξ) −¯H(x).) One can then write\n(Xt −x∗)T h(Xt,ξt+1) = (Xt −x∗)T (g(Xt,ξt+1) −PXtg(Xt,ξt+1)\nand\nAt+1 ≤(1 −2αt+1µ)At + 2αt+1(Xt −x∗)T (g(Xt,ξt+1) −PXtg(Xt,ξt)))\n+ 2αt+1(Xt −x∗)T PXtg(Xt,ξt) −2αt+1(Xt −x∗)T PXtg(Xt,ξt+1) + α2\nt+1|H(Xt,ξt+1)|2\nIntroducing the notation\nηst = E((Xs −x∗)T PXsg(Xs,ξt)).\nUsing the fact that\nE\n\u0010\n(Xt −x∗)T (g(Xt,ξt+1) −PXtg(Xt,ξt))) | Ut\n\u0011\n= 0\nand and noting that |H(Xt,ξt+1)|2 ≤C2\n0, this gives, after taking expectations,\nat+1 ≤(1 −2αt+1µ)at + 2αt+1ηtt −2αt+1ηt,t+1 + α2\nt+1C2\n0.\nApplying lemma 3.25, and letting vs,t = Qt\nj=s+1(1 −2αj+1µ), we get\nat ≤a0v0,t + 2\nt\nX\ns=1\nvs,tαs+1(ηss −ηs,s+1) + C2\n0\nt\nX\ns=1\nvs,tα2\ns+1.\n\n12.7. MARKOVIAN STOCHASTIC APPROXIMATION\n289\nWe now want to ensure that each term in the upper bound converges to 0. Simi-\nlarly to section 3.3.2, (12.32a) implies that this holds the first and last terms and we\ntherefore focus on the middle one, writing\nt\nX\ns=1\nvs,tαs+1(ηss −ηs,s+1) = v1,tα2η11 −αt+1ηt,t+1 +\nt\nX\ns=2\n(vs,tαs+1 −vs−1,tαs)ηss\n(12.35)\n+\nt\nX\ns=2\nvs−1,tαs(ηss −ηs−1,s)\nWe will need the following estimates on the function g in (12.34), which is de-\nfined by\ng(x,ξ) =\n∞\nX\nn=0\nPn\nx h(x,ξ) = h(x,ξ) +\n∞\nX\nn=0\nPn\nx ˜h(x,ξ).\nLemma 12.13 We have\n|g(x,·)| ≤C0 + 2C1M(1 −ρ(x))−1,\n(12.36a)\n|Pxg(x,·)| ≤2C1M(1 −ρ(x))−1.\n(12.36b)\nand, for all x,y ∈Rd and ξ ∈Rξ\n|Pxg(x,ξ) −Py(g(y,ξ)| = M2C1C2(1 −¯ρ)−2 + MC1(1 + C2)(1 −¯ρ)−1.\n(12.37)\nwith ¯ρ = max(ρ(|x|),ρ(|y|)).\nUsing lemma lemma 12.13 (which is proved at the end of the section), we can\ncontrol the terms intervening in (12.35). Note that the first term, v1tα2η11, converges\nto 0 since (12.32a) implies that v1t converges to 0.\nWe have,\nαt+1|E((Xt −x∗)T PXtg(Xt,ξt+1))| ≤2MC1αt+1σt(1 −ρ(σt))−1,\nso that (12.32b) implies that αt+1ηt,t+1 →0.\nand since αs+1 ≤αs, we have\n\f\f\f\f\f\f\f\nt\nX\ns=2\n(vs,tαs+1 −vs−1,tαs)ηss\n\f\f\f\f\f\f\f\n≤\nt\nX\ns=2\n|vstαs −vs,tαs+1||ηss|\n≤MC1\nt\nX\ns=2\n|vs−1,tαs −vs,tαs+1|αs+1σs(1 −ρ(σs))−1\n≤C\nt\nX\ns=2\n|vs−1,tαs −vs,tαs+1|\n\n290\nCHAPTER 12. MONTE-CARLO SAMPLING\nfor some constant C, since αs+1σs(1 −ρ(σs))−1 is bounded. Writing\nvs,tαs+1 −vs−1,tαs = vst(αs+1 −αs + 2µα2\ns ),\nwe get (using αs+1 ≤αs)\nt\nX\ns=2\n|vs−1,tαs −vs,tαs+1| ≤\nt\nX\ns=2\nvst(αs −αs+1) +\nt\nX\ns=2\nvst2µα2\ns .\nSince both P\ns(αs −αs+1) and Pt\ns=2 α2\ns converge (the former is just α1), lemma 3.26\nimplies that\nt\nX\ns=2\n(vs,tαs+1 −vs−1,tαs)ηss\ntends to zero. The last term to consider is\nt\nX\ns=2\nvs−1,tαs(ηss −ηs−1,s) =\nt\nX\ns=2\nvs−1,tαsE((Xs −Xs−1)T PXsg(Xs,ξs))\n+\nt\nX\ns=2\nvs−1,tαsE((Xs−1 −x∗)T (PXsg(Xs,ξs)) −PXs−1g(Xs−1,ξs))).\nWe have\n\f\f\f\f\f\f\f\nt\nX\ns=2\nvs−1,tαsE((Xs −Xs−1)T PXsg(Xs,ξs))\n\f\f\f\f\f\f\f\n≤2C0C1M\nt\nX\ns=2\nvs−1,tα2\ns (1 −ρ(σs))−1\nand\n\f\f\f\f\f\f\f\nt\nX\ns=2\nvs−1,tαsE((Xs−1 −x∗)T (PXsg(Xs,ξs)) −PXs−1g(Xs−1,ξs)))\n\f\f\f\f\f\f\f\n≤2M2C0C1(1 + C2)|X0 −x∗|\nt\nX\ns=2\nvs−1,tα2\ns σs(1 −ρ(σs))−2\nand lemma 3.26 implies that both terms vanish at infinity. This concludes the proof\nof theorem 12.11.\n■\nProof (Proof of lemma 12.13) Condition (H3) and proposition 12.3 and imply that\n(since πx˜h = 0)\n|Pn\nx ˜h(x,ξ)| ≤Dvar(Pn\nx (ξ,·),πx)osc(˜h(x,·)) ≤2C1Mρ(x)n\nso that g is well defined with\n|g(x,·)| ≤C0 + 2C1M(1 −ρ(x))−1,\n|Pxg(x,·)| ≤2C1M(1 −ρ(x))−1.\n\n12.7. MARKOVIAN STOCHASTIC APPROXIMATION\n291\nWe will also need to control differences of the kind\nPxg(x,ξ) −Pyg(y,ξ).\nWe consider the nth term in the series, writing\nPn\nx ˜h(x,ξ) −Pn\ny ˜h(y,ξ) =\nn−1\nX\nk=0\n(Pn−k\nx\nPk\ny ˜h(y,ξ) −(Pn−k−1\nx\nPk+1\ny\n˜h(y,ξ))\n+ Pn\nx ˜h(x,ξ) −Pn\nx ˜h(y,ξ).\nThis gives\nPn\nx ˜h(x,ξ) −Pn\ny ˜h(y,ξ) =\nn−1\nX\nk=0\nPn−k−1\nx\n(PxPk\ny ˜h(y,ξ) −Pk+1\ny\n˜h(y,ξ) −πxPk\ny ˜h(y) + πxPk+1\ny\n˜h(y))\n+\nn−1\nX\nk=0\n(πxPk\ny ˜h(y) −πxPk+1\ny\n˜h(y)) + Pn\nx ˜h(x,ξ) −Pn\nx ˜h(y,ξ)\n=\nn−1\nX\nk=0\nPn−k−1\nx\n(PxPk\ny ˜h(y,ξ) −Pk+1\ny\n˜h(y,ξ) −πxPk\ny ˜h(y) + πxPk+1\ny\n˜h(y))\n+ πx˜h(y) −πxPn\ny ˜h(y) + Pn\nx ˜h(x,ξ) −Pn\nx ˜h(y,ξ)\nFinally\nPn\nx h(x,ξ) −Pn\ny h(y,ξ) =\nn−1\nX\nk=0\nPn−k−1\nx\n(PxPk\ny ˜h(y,ξ) −Pk+1\ny\n˜h(y,ξ) −πxPk\ny ˜h(y) + πxPk+1\ny\n˜h(y))\n+ Pn\nx (˜h(x,ξ) −˜h(y,ξ) + πx˜h(y)) −(πx −πy)Pn\ny ˜h(y)\nUsing proposition 12.3, we can write, letting ¯ρ = max(ρ(|x|),ρ(|y|)),\n|Pn−k−1\nx\n(PxPk\ny ˜h(y,ξ) −Pk+1\ny\n˜h(y,ξ) −πxPk\ny ˜h(y,ξ) + πxPk+1\ny\n˜h(y,ξ))|\n≤M ¯ρn−k−1osc(PxPk\ny ˜h(y,ξ) −Pk+1\ny\n˜h(y,ξ))\n≤C2M ¯ρn−k−1|x −y|osc(Pk\ny ˜h(y,ξ))\n≤C2C1M2 ¯ρn−1|x −y|\nWe also have\n|Pn\nx (˜h(x,ξ) −˜h(y,ξ) + πx˜h(y,ξ))| ≤MC1 ¯ρn|x −y|\nand\n|(πx −πy)Pn\ny ˜h(y,ξ)| ≤MC2C1 ¯ρn|x −y|\n\n292\nCHAPTER 12. MONTE-CARLO SAMPLING\nso that\n|Pn\nx h(x,ξ) −Pn\ny h(y,ξ)| ≤MC1 ¯ρn−1(nMC2 + (1 + C2) ¯ρ)|x −y|\nFrom this, it follows that\n|Pxg(x,ξ) −Py(g(y,ξ)| ≤MC1\n∞\nX\nn=1\n¯ρn−1(nMC2 + (1 + C2)|x −y|\n= M2C1C2(1 −¯ρ)−2 + MC1(1 + C2)(1 −¯ρ)−1.\n■\n\nChapter 13\nMarkov Random Fields\nWith this chapter, we start a discussion of large-scale statistical models in data sci-\nence, starting with graphical models (Markov random fields and Bayesian networks)\nbefore discussing more recent approaches using, notably, deep learning. Impor-\ntant textbook references for the present chapter include Pearl [151], Ancona et al.\n[8], Winkler [203], Lauritzen [114], Cowell et al. [56], Koller and Friedman [108].\n13.1\nIndependence and conditional independence\n13.1.1\nDefinitions\nWe consider random variables X,Y,Z ..., and denote by RX,RY,RZ ... the sets in\nwhich they take their values. We discuss in this section concepts of independence\nand conditional independence between random variables. To simplify the exposi-\ntion, we will work (unless mentioned otherwise) with discrete random variables (X\nis discrete if RX is finite or countable)1. We start with a basic definition.\nDefinition 13.1 Two discrete random variables X : Ω→RX and Y : Ω→RY are inde-\npendent if and only if\n∀x ∈RX,∀y ∈RY : P(X = x,Y = y) = P(X = x)P(Y = y).\nThe general definition for arbitrary r.v.’s is that\nE(f (X)g(Y)) = E(f (X))E(g(Y))\nfor any pair of (measurable) non-negative functions f : RX →[0,+∞) and g : RY →\n[0,+∞).\n1In the general case, RX,RY,... are metric spaces with a countable dense subset with σ-algebras\nSX,SY,...\n293\n\n294\nCHAPTER 13. MARKOV RANDOM FIELDS\nOne can easily check that X and Y are independent if and only if, for any non-\nnegative function g : RY →R, one has\nE(g(Y) | X) = E(g(Y)).\nNotation 13.2 Independence is a property that involves two variables X and Y and\nan underlying probability distribution P. Independence of X and Y relative to P will\nbe denoted (XyY)P. However we will only write XyY when there is no ambiguity\non P.\n♦\nMore than independence, the concept of conditional independence will be fun-\ndamental in this chapter. It requires three variables, say X,Y,Z. Returning to the\ndiscrete case, one says that X and Y are conditionally independent given Z is, for\nany x ∈RX, y ∈RY and z ∈RZ such that P(Z = z) > 0,\nP(X = x,Y = y | Z = z) = P(X = x | Z = z)P(Y = y | Z = z).\n(13.1)\nAn equivalent statement is that, for any z such that P(Z = z) , 0, X and Y are inde-\npendent when P is replaced by the conditional distribution P(· | Z = z).\nIn the general case conditional independence means that, for any pair of non-\nnegative measurable functions f and g,\nE(f (X)g(Y) | Z) = E(f (X) | Z)E(g(Y) | Z).\n(13.2)\nFrom now, we restrict our discussion to discrete random variables.\nMultiplying both terms in (13.1) by P(Z = z)2, we get the equivalent statement:\nX and Y are conditionally independent given Z if and only if,\n∀x,y,z : P(X = x,Y = y,Z = z)P(Z = z) = P(X = x,Z = z)P(Y = y,Z = z).\n(13.3)\nNote that the identity is meaningful, and always true, for P(Z = z) = 0, so that this\ncase does not need to be excluded anymore.\nConditional independence can be interpreted by the statement that X brings no\nmore information on Y than what is already provided by Z: one has\nP(Y = y | X = x,Z = z) = P(Y = y,X = x,Z = z)\nP(X = x,Z = z)\n= P(Y = y,Z = z)\nP(Z = z)\nas directly deduced from (13.3). (This computation being valid as soon as P(X =\nx,Z = z) > 0.)\nNotation 13.3 To indicate that X and Y are conditionally independent given Z for\nthe distribution P, we will write (XyY | Z)P or simply (XyY | Z).\n♦\n\n13.1. INDEPENDENCE AND CONDITIONAL INDEPENDENCE\n295\nSo we have the equivalence:\n(XyY | Z)P ⇔\n\u0010\n∀z : P(Z = z) > 0 ⇒(XyY)P(|Z=z)\n\u0011\n.\nAbsolute independence is like “independence conditional to no variable”, and we\nwill use the notation ∅for the “empty” random variable that contains no information\n(for example, a set-valued random variable that always returns the empty set, or any\nconstant variable). So we have the tautology\nXyY ⇔(XyY | ∅).\nNote that, dealing with discrete variables, all previous definitions automatically\nextend to groups of variables: for example, if Z1, Z2 are two discrete variables, so\nis Z = (Z1,Z2) and we immediately obtain a definition for the conditional indepen-\ndence of X and Y given Z1 and Z2, denoted (XyY | Z1,Z2).\n13.1.2\nFundamental properties\nProposition 13.5 below lists important properties of conditional independence that\nwill be used repeatedly in this chapter. Before stating this proposition, we need the\nfollowing definition.\nDefinition 13.4 One says that the joint distribution of the random variables (X1,...,XN)\nis positive if there exists subsets ˜Rk ⊂RXk, k = 1,...,N such that P(Xk ∈˜Rk) = 1 and:\nP(X1 = x1,...,XN = xN) > 0\nif xk ∈˜Rk, k = 1,...,N.\nNote that the condition implies P(Xk = xk) > 0 for all xk ∈˜Rk, so that ˜Rk = {xk ∈\nRXk : P(Xk = xk) > 0}, i.e., ˜Rk is the support of PXk. One can interpret the definition\nas expressing the fact that any conjunction of events for different Xk’s has positive\nprobability, as soon as each of them has positive probability (if all events may occur,\nthen they may occur together).\nNote that the sets ˜Rk depend on X1,...,XN. However, if this family of variables\nis fixed, there is no loss in generality in restricting the space RXk to ˜Rk and there for\nassume that P(X1 = x1,...,XN = xN) > 0 everywhere.\nProposition 13.5 Let X,Y,Z and W be random variables. The following properties are\ntrue.\n(CI1) Symmetry: (XyY | Z) ⇒(YyX | Z).\n\n296\nCHAPTER 13. MARKOV RANDOM FIELDS\n(CI2) Decomposition: (Xy(Y,W) | Z) ⇒(XyY | Z).\n(CI3) Weak union: (Xy(Y,W) | Z) ⇒(XyY | (Z,W)).\n(CI4) Contraction: (XyY | Z) and (XyW | (Z,Y)) ⇒(Xy(Y,W) | Z).\n(CI5) Intersection: assume that the joint distribution of W,Y and Z is positive. Then\n(XyW | (Z,Y)) and (XyY | (Z,W)) ⇒(Xy(Y,W) | Z).\nProof Properties (CI1) and (CI2) are easily deduced from (13.3) and left to the\nreader. To prove the last three, we will use the notation P(x),P(x,y) etc. instead\nof P(X = x),P(X = x,Y = y), etc. to save space. Identities are assumed to hold for all\nx,y,z,w unless stated otherwise.\nFor (CI3), we must prove, according to (13.3), that\nP(x,y,z,w)P(z,w) = P(x,z,w)P(y,z,w)\n(13.4)\nwhenever P(x,y,z,w)P(z) = P(x,z)P(y,z,w). Summing this last equation over y (or\napplying (CI2)) yields P(x,z,w)P(z) = P(x,z)P(z,w). We can note that all terms in\n(13.4) vanish when P(z) = 0, so that the identity is true in this case. When P(z) , 0,\nthe right-hand side of (13.4) becomes\n(P(x,z)P(z,w)/P(z))P(y,z,w) = (P(x,z)P(y,z,w)/P(z))P(z,w) = P(x,y,z,w)P(z,w),\nusing once again the hypothesis. This proves (CI3).\nFor (CI4), the hypotheses are\n\nP(x,y,z)P(z) = P(x,z)P(y,z)\nP(x,y,z,w)P(y,z) = P(x,y,z)P(y,z,w)\nand the conclusion must be\nP(x,y,z,w)P(z) = P(x,z)P(y,z,w).\n(13.5)\nSince (13.5) is true when P(y,z) = 0, we assume that this probability does not vanish\nand write\nP(x,y,z,w)P(z)\n=\nP(x,y,z)P(z)P(y,z,w)/P(y,z)\n=\nP(x,z)P(y,z)P(y,z,w)/P(y,z)\n=\nP(x,z)P(y,z,w)\nyielding (13.5).\n\n13.1. INDEPENDENCE AND CONDITIONAL INDEPENDENCE\n297\nFor (CI5), assuming\n\nP(x,y,z,w)P(y,z) = P(x,y,z)P(y,z,w)\nP(x,y,z,w)P(z,w) = P(x,z,w)P(y,z,w),\n(13.6)\nwe want to show that\nP(x,y,z,w)P(z) = P(x,z)P(y,z,w).\nSince this identity is true when any of the events W = w,Y = y or Z = z has zero\nprobability, we can assume that their probabilities are positive, which, by assump-\ntion, also implies that all joint probabilities are positive. From the two identities, we\nget\nP(x,y,z,w)/P(y,z,w) = P(x,y,z)/P(y,z) = P(x,z,w)/P(z,w)\nThis implies\nP(x,y,z) = P(y,z)P(x,z,w)/P(z,w)\nthat we can sum over y to obtain\nP(x,z) = P(z)P(x,z,w)/P(z,w)\nWe therefore get\nP(x,y,z,w)/P(y,z,w) = P(x,z,w)/P(z,w) = P(x,z)/P(z),\nwhich is what we wanted.\n■\nA counter-example of (CI5) when the positivity assumption is not satisfied can be\nbuilt as follows: let X be a Bernoulli random variable, and let Y = W = X. Let Z be\nany Bernoulli random variable, independent from X. Given Z and W, X and Y are\nconstant and therefore independent. Similarly, given Z and Y, X and W are constant\nand therefore independent. However, given Z, X and (Y,W) are not independent\n(they are equal and non constant).\n13.1.3\nMutual independence\nAnother concept of interest is the mutual (conditional) independence of more than\ntwo random variables. The random variables (X1,...,Xn) are mutually conditionally\nindependent given Z if and only if\nE(f1(X1)···fn(Xn) | Z) = E(f1(X1) | Z)···E(fn(Xn) | Z)\nfor any non-negative measurable functions f1,...,fn. In terms of discrete probabili-\nties, this can be written as\nP(X1 = x1,...,Xn = xn,Z = z)P(Z = z)n−1 =\nP(X1 = x1,Z = z)···P(Xn = xn,Z = z).\n\n298\nCHAPTER 13. MARKOV RANDOM FIELDS\nThis will be summarized with the notation\n(X1y···yXn | Z).\nWe have the proposition\nProposition 13.6 For variables X1,...,Xn and Z, the following properties are equivalent.\n(i) (X1y···yXn | Z);\n(ii) For all S,T ⊂{1,...,n} with S ∩T = ∅, we have: ((Xi,i ∈S)y(Xj,j ∈T) | Z);\n(iii) For all s ∈{1,...,n}, we have: (Xsy(Xt,t , s) | Z);\n(iv) For all s ∈{2,...,n}, we have: (Xsy(X1,...,Xs−1) | Z).\nProof It is clear that (i) ⇒··· ⇒(iv) so it suffices to prove that (iv) ⇒(i). For this,\nsimply write (applying (iv) repeatedly to s = n −1,n −2,...)\nE(f1(X1)···fn(Xn) | Z)\n= E(f1(X1)···fn−1(Xn−1) | Z)E(fn(Xn) | Z)\n= E(f1(X1)···fn−2(Xn−2) | Z)E(fn−1(Xn−1) | Z)\nE(fn(Xn) | Z)\n...\n= E(f1(X1) | Z)···E(fn(Xn) | Z).\n13.1.4\nRelation with Information Theory\nSeveral concepts in information theory are directly related to independence between\nrandom variables. Recall that the (Shannon) entropy of a discrete probability distri-\nbution over a finite set R is defined by\nH(P) = −\nX\nω∈R\nlogP(ω)P(ω).\n(13.7)\nSimilarly, the entropy of a random variable X : Ω→RX is defined by\nH(X)\n∆= H(PX) = −\nX\nx∈RX\nlogP(X = x)P(X = x).\n(13.8)\nThe entropy is always non-negative, and provides a measure of the uncertainty as-\nsociated to P. For a given finite set R, it is maximal when P is uniform over R, and\nminimal (and vanishes) when P is supported by a single ω ∈R (i.e. P(ω) = 1).\n\n13.1. INDEPENDENCE AND CONDITIONAL INDEPENDENCE\n299\nOne defines the entropy of two or more random variables as the entropy of their\njoint distribution, so that, for example,\nH(X,Y) = −\nX\n(x,y)∈RX×RY\nlogP(X = x,Y = y)P(X = x,Y = y).\nWe have the proposition:\nProposition 13.7 For random variables X1,...,Xn, one has\nH(X1,...,Xn) ≤H(X1) + ··· + H(Xn)\nwith equality if and only if (X1,...,Xn) are mutually independent.\nProof The proof of this proposition uses properties of the Kullback-Leibler diver-\ngence (c.f. (4.3)), given by, for two probability distributions π and π′ on a finite set\nB,\nKL(π∥π′) =\nX\nω∈B\nπ(ω)log π(ω)\nπ′(ω).\nwith the convention πlog(π/π′) = 0 if π = 0 and = ∞if π > 0 and π′ = 0. Returning to\nproposition 13.7, a straightforward computation (which is left to the reader) shows\nthat\nH(X1) + ··· + H(Xn) −H(X1,...,Xn) = KL(π∥π′)\nwith π(x1,...,xn) = P(X1 = x1,...,Xn = xn) and π′(x1,...,xn) = Qn\nk=1P(Xk = xk). This\nmakes proposition 13.7 a direct consequence of proposition 4.1.\n■\nThe mutual information between two random variables X and Y is defined by\nI(X,Y) = H(X) + H(Y) −H(X,Y).\n(13.9)\nFrom proposition 13.7, I(X,Y) is nonnegative and vanishes if and only if X and\nY are independent. Also from the proof of proposition 13.7, I(X,Y) is equal to\nKL(P(X,Y)∥PX ⊗PY) where the first probability is the joint distribution of X and Y and\nthe second one the product of the marginals of X and Y, which coincides with PX,Y\nif and only if X and Y are independent.\nIf X and Y are two random variables, and y ∈RY with P(Y = y) > 0, the entropy\nof the conditional probability x 7→P(X = x | Y = y) is denoted H(X | Y = y), and\nis a function of y. The conditional entropy of X given Y, denoted H(X | Y) is the\nexpectation of H(X | Y = y) for the distribution of Y, i.e.,\nH(X | Y) =\nX\ny∈RY\nH(X | Y = y)P(Y = y)\n= −\nX\nx∈RX\nX\ny∈RY\nlogP(X = x | Y = y)P(X = x,Y = y).\nSo, we have (with a straightforward proof)\n\n300\nCHAPTER 13. MARKOV RANDOM FIELDS\nProposition 13.8 Given two random variables X and Y, we have\nH(X | Y)\n=\n−EX,Y(logP(X = · | Y = ·))\n(13.10)\n=\nH(X,Y) −H(Y)\nThis proposition also immediately yields:\nI(X,Y) = H(X) −H(X | Y) = H(Y) −H(Y | X).\n(13.11)\nThe identity H(X,Y) = H(X | Y)+H(Y) that is deduced from proposition 13.8 can be\ngeneralized to more than two random variables (the proof being left to the reader),\nyielding, if X1,...,Xn are random variables:\nH(X1,...,Xn) =\nn\nX\nk=1\nH(Xk | X1,...,Xk−1).\n(13.12)\nIf Z is an additional random variable, the following identity is obtained by ap-\nplying the previous one to conditional distributions given Z = z and taking averages\nover z:\nH(X1,...,Xn | Z) =\nn\nX\nk=1\nH(Xk | X1,...,Xk−1,Z).\n(13.13)\nThe following proposition characterizes conditional independence in terms of\nentropy.\nProposition 13.9 Let X,Y and Z be three random variables. The following statements\nare equivalent.\n(i) X and Y are conditionally independent given Z.\n(ii) H(X,Y | Z) = H(X | Z) + H(Y | Z)\n(iii) H(X | Y,Z) = H(X | Y)\nMoreover, when (i) to (iii) are satisfied, we have:\n(iv) I(X,Y) ≤min(I(X,Z),I(Y,Z)).\nProof From proposition 13.7, we have, for any three random variables X,Y,Z, and\nany z such that P(Z = z) > 0,\nH(X,Y | Z = z) ≤H(X | Z = z) + H(Y | Z = z).\n\n13.2. MODELS ON UNDIRECTED GRAPHS\n301\nTaking expectations on both sides implies the important inequality\nH(X,Y | Z) ≤H(X | Z) + H(Y | Z)\n(13.14)\nand equality occurs if and only if P(X = x,Y = y | Z = z) = P(X = x | Z = z)P(Y =\ny | Z = z) whenever P(Z = z) > 0, that is, if and only if X and Y are conditionally\nindependent given Z. This proves that (i) and (ii) are equivalent. The fact that\n(ii) and (iii) are equivalent comes from (13.13), which gives, for any three random\nvariables\nH(X,Y | Z) = H(X | Y,Z) + H(Y | Z).\n(13.15)\nTo prove that (i)-(iii) implies (iv), we note that (13.14) and (13.15) imply that, for\nany three random variables:\nH(X | Y,Z) ≤H(X | Y).\nIf X and Y are conditionally independent given Z, then the right-hand side is equal\nto H(X | Z) and this yields\nI(X,Y) = H(X) −H(X | Y) ≤H(X) −H(X | Z) = I(X,Z).\nBy symmetry, we must also have I(X,Y) ≤I(Y,Z) so that (iv) is true.\n■\nStatement (iv) is often called the data-processing inequality, and has been used to\ninfer conditional independence within gene networks [125].\n13.2\nModels on undirected graphs\n13.2.1\nGraphical representation of conditional independence\nAn undirected graph is a collection of vertexes and edges, in which edges link pairs\nof vertexes without order. Edges can therefore be identified to subsets of cardinality\ntwo of the set of vertexes, V . This yields the definition:\nDefinition 13.10 An undirected graph G is a pair G = (V ,E) where V is a finite set of\nvertexes and elements e ∈E are subsets e = {s,t} ⊂V .\nNote that edges in undirected graphs are defined as sets, i.e., unordered pairs, which\nare delimited with braces in these notes. Later on, we will use parentheses to repre-\nsent ordered pairs, (s,t) , (t,s). We will write s ∼G t, or simply s ∼t to indicate that s\nand t are connected by an edge in G (we also say that s and t are neighbors in G).\n\n302\nCHAPTER 13. MARKOV RANDOM FIELDS\nDefinition 13.11 A path in an undirected graph G = (V ,E) is a finite sequence (s0,...,sN)\nof vertexes such that sk−1 ∼sk ∈E. (A sequence, (s0), of length 1 is also a path by exten-\nsion.)\nWe say that s and t are connected by a path if either s = t or there exists a path\n(s0,...,sN) such that s0 = s and sN = t.\nA subset S ⊂G is connected if any pair of elements in S can be connected by a path.\nA subset T ⊂G separates two other subsets S and S′ if all paths between S and S′\nmust pass in T. We will write (SyS′ | T) in such a case.\nOne of the goals of this chapter is to relate the notion of conditional indepen-\ndence within a set of variables to separation in a suitably chosen undirected graph\nwith vertexes in one-to-one correspondence with the variables. This will also justi-\nfies the similarity of notation used for separation and conditional independence.\nWe have the following simple fact:\nLemma 13.12 Let G = (V ,E) be an undirected graph, and S,S′,T ⊂V . Then\n(SyS′ | T) ⇒S ∩S′ ⊂T .\nIndeed, if (SyS′ | T) and s0 ∈S ∩S′, the path (s0) links S and S′ and therefore must\npass in T .\nProposition 13.5 translates into similar properties for separation:\nProposition 13.13 Let (V ,E) be an undirected graph and S,T ,U,R be subsets of V . The\nfollowing properties hold\n(i) (SyT |U) ⇔(T yS|U).\n(ii) (SyT ∪R|U) ⇒(SyT|U).\n(iii) (SyT ∪R|U) ⇒(SyT |U ∪R).\n(iv) (SyT | U) and (SyR | U ∪T) ⇔(SyT ∪R | U).\n(v) U ∩R = ∅,(SyR | U ∪T) and (SyU | T ∪R) ⇒(SyU ∪R | T).\nProof (i) is obvious, and for (ii) (and (iii)), if any path between S and T ∪R must\npass by U, the same is obviously true for a path between S and T.\nFor the ⇒part of (iv), if a path links S and T ∪R, then it either links S and T\nand must pass through U by the first assumption, or link S and R and therefore pass\nthrough U or T by the second assumption. But if the path passes through T, it must\n\n13.2. MODELS ON UNDIRECTED GRAPHS\n303\nalso pass through U before by the first assumption. In all cases, the path passes\nthrough U. The ⇐part of (iv) is obvious.\nFinally, consider (v) and take a path between two distinct elements in S and U∪R.\nConsider the first time the path hits U or R, and assumes that it hits U (the other\ncase being treated similarly by symmetry). Notice that the path cannot hit both U\nand R at the same point since U ∩R = ∅. From the assumptions, the path must hit\nT ∪R before passing by U, and the intersection cannot be in R, so it is in T , which is\nthe conclusion we wanted.\n■\nTo make a connection between separation in graphs and conditional indepen-\ndence between random variables, we consider a graph G = (V ,E) and a family of\nrandom variables (X(s),s ∈V ) indexed by V . Each variable is assumed to take values\nin a set Fs = RX(s). The collection of values taken by the random variables will be\ncalled configurations, and the sets Fs,s ∈V are called the state spaces.\nLetting F denote the collection (Fs,s ∈V ), we will denote the set of such configu-\nrations as F (V ,F ). Then F is clear from the context, we will just write F (V ). If S ⊂V\nand x ∈F (V ,F ), the restriction of x to S is denoted x(S) = (x(s),s ∈S). The set formed\nby those restrictions will be denoted F (S,F ) (or just F (S)).\nRemark 13.14 Some care needs to be given to the definition of the space of con-\nfigurations, to avoid ambiguities when two sets Fs coincide. The configuration x =\n(x(s),s ∈V ) should be understood, in an emphatic way, as the collection ˆx = ((s,x(s)),s ∈\nV ), which makes explicit the fact that x(s) is the value observed at vertex s. Similarly\nthe emphatic notation for x(S) ∈F (V ,F ) is ˆx(S) = ((s,x(s)),s ∈S).\nIn the following, we will not use the emphatic notation to avoid overly heavy\nexpressions, but its relevance should be clear with the following simple example.\nTake V = {1,2,3} and F1 = F2 = F3 = {0,1}. Let x(1) = 0, x(2) = 0 and x(3) = 1. Then\nthe sub-configurations x({1,3}) and x({2,3}) both corresponds to values (0,1), but we\nconsider them as distinct. In the same spirit, x(1) = x(2), but x({1}) , x({2})\n♦\nIf S,T ⊂V with S ∩T = ∅, x(S) ∈F (S,F ), y(T ) ∈F (T ,F ), we will denote their\nconcatenation by x(S) ∧y(T ), which is the configuration z = (zs,s ∈S ∪T) ∈F (T ∪S,F )\nsuch that z(s) = x(s) if s ∈S and z(s) = y(s) if s ∈T.\nWe define a random field over V as a random configuration X : Ω→F (V ,F ), that\nwe will denote for short X = (X(s),s ∈V ). If S ⊂V , the restriction X(S) will also be\ndenoted (X(s),s ∈S).\nWe can now write the definition:\n\n304\nCHAPTER 13. MARKOV RANDOM FIELDS\nDefinition 13.15 Let G = (V ,E) be an undirected graph and X = (X(s),s ∈V ) a random\nfield over V . We say that X is Markov (or has the Markov property) relative to G (or is\nG-Markov, or is a Markov random field on G) if and only if, for all S,T ,U ⊂V :\n(SyT | U) ⇒(X(S)yX(T) | X(U)).\n(13.16)\nLetting the observation over an empty set S be empty, i.e., X∅= ∅, this definition in-\ncludes the statement that, if S and T are disconnected (i.e., there is no path between\nthem: they are separated by the empty set), then (X(S)yX(T) | ∅): X(S) and X(T ) are\nindependent.\nWe will say that a probability distribution π on F (V ) is G-Markov if its associated\ncanonical random field X = (X(s),s ∈V ) defined on ˜Ω= F (V ) by X(s)(x) = x(s) is G-\nMarkov.\n13.2.2\nReduction of the Markov property\nWe now proceed, in a series of steps, to a simplification of definition 13.15 in order\nto obtain a minimal number of conditional independence statements. Note that, in\nits current form, definition 13.15 requires to check (13.16) for any three subsets of\nV , which provides a huge number of conditions. Fortunately, as we will see, these\nconditions are not independent, and checking a much smaller number of them will\nensure that all of them are true.\nThe first step for our reduction is provided by the following lemma.\nLemma 13.16 Let G = (V ,E) be an undirected graph and X = (Xs,s ∈V ) a set of random\nvariables indexed by V . Then X is G-Markov if and only if, for S,T ,U ⊂V ,\nS ∩U = T ∩U = ∅and (SyT | U) ⇒(X(S)yX(T) | X(U)).\n(13.17)\nProof Assume that (13.17) is true, and take any S,T ,U with (SyT | U). Let A =\nS ∩U, B = T ∩U and C = A ∪B. Partition S in S = S1 ∪A, T in T1 ∪B and U in\nU1 ∪C. From (SyT | U), we get (S1yT1 | U). Since S1 ∩U = T1 ∩U = ∅, this implies\n(X(S1)yX(T1) | X(U)). But this implies ((X(S1),X(A))y(X(T1),X(B)) | X(U)). Indeed, this\nproperty requires\nPX(x(S1) ∧x(A) ∧x(T1) ∧x(B) ∧x(U1) ∧y(C))PX(x(U1) ∧y(C))\n= PX(x(S1) ∧x(A) ∧x(U1) ∧y(C))PX(x(T1) ∧x(B) ∧x(U1) ∧y(C))\nIf the configurations x(A),x(B),y(C) are not consistent (i.e., x(t) , y(t) for some t ∈C),\nthen both sides vanish. So we can assume x(C) = y(C) and remove x(A) and x(B) from\nthe expression, since they are redundant. The resulting identity is true since it ex-\nactly states that (X(S1)yX(T1) | X(U)).\n■\n\n13.2. MODELS ON UNDIRECTED GRAPHS\n305\nDefine the set of neighbors of s ∈V (relative to the graph G) as the set of t , s\nsuch that {s,t} ∈E and denote this set by Vs. For S ⊂V define also\nVS = Sc ∩\n[\ns∈S\nVs\nwhich is the set of neighbors of all vertexes in S that do not belong to S. (Here Sc\ndenotes the complementary set of S, Sc = V \\ S.) Finally, let WS denote the vertexes\nthat are “remote” from S, WS = (S ∪VS)c.\nWe have the following important reduction of the condition in definition 13.15.\nProposition 13.17 X is Markov relative to G if and only if, for any S ⊂V ,\n(X(S)yX(WS) | X(VS)).\n(13.18)\nThis says that\nP(X(S) = x(S) | X(Sc) = x(Sc))\nonly depends (when defined) on variables x(t) for t ∈S ∪VS.\nProof First note that (SyWS | VS) is always true, since any path reaching S from Sc\nmust pass through VS. This immediately proves the “only if” part of the proposition.\nConsider now the “if” part. Take S,T ,U such that (SyT | U). We want to prove\nthat (XSyXT | XU). According to lemma 13.16, we can assume, without loss of gen-\nerality, that S ∩U = T ∩U = ∅.\nDefine R as the set of vertexes v in V such that there exists a path between S and\nv that does not pass in U. Then:\n1. S ⊂R: the path (s) for s ∈S does not pass in U since S ∩U = ∅.\n2. U ∩R = ∅by definition.\n3. VR ⊂U: assume that there exists a point r in VR which is not in U. Then r has a\nneighbor, say r′ in R. By definition of R, there exists a path from S to r′ that does not\nhit U, and this path can obviously be extended by adding r at the end to obtain a\npath that still does not hit U. But this implies that r ∈R, which contradicts the fact\nthat VR ∩R = ∅.\n4. T ∩(R ∪VR) = ∅: if t ∈T, then t < R from (SyT | U) and t < VR from T ∩U = ∅.\nWe can then write (each decomposition being a partition, implicitly defining the\nsets A, B and C, see Fig. 13.1) R = S ∪A, U = VR ∪C, (R ∪VR)c = T ∪C ∪B, and from\n(X(R)yX(WR) | X(VR)), we get\n((X(S),X(A))y(X(T),X(C),X(B)) | X(VR))\n\n306\nCHAPTER 13. MARKOV RANDOM FIELDS\nFigure 13.1: See proof of proposition 13.17 for details\nwhich implies\n((X(S),X(A))y(X(T),X(B)) | X(U))\nby (CI3), which finally implies (X(S)yX(T ) | X(U)) by (CI2).\n■\nFor positive probabilities, it suffices to consider singletons in proposition 13.17.\nProposition 13.18 If the joint distribution of (X(s),s ∈V ) is positive and, for any s ∈V ,\n(X(s)yX(Ws) | X(Vs)),\n(13.19)\nthen X is Markov relative to G. The converse statement is true without the positivity\nassumption.\nProof It suffices to prove that, if (13.18) is true for S and T ⊂V , with T ∩S = ∅, it is\nalso true for S ∪T. The result will then follow by induction.\nSo, let U = VS∪T and R = WS∪T = V \\ (S ∪T ∪U). Then, we have\n(X(S)yX(WS) | X(VS)) ⇒(X(S)yX(R) | (X(U),X(T)))\nbecause R ⊂WS (if s ∈VS, then it is either in U or in T and therefore cannot be in\nR). Similarly, (X(T )yX(R) | (X(U),X(S))), and (CI5) (for which we need P positive) now\nimplies ((X(T ),X(S))yX(R) | X(U)).\n■\nTo see that the positivity assumption is needed, consider the following example with\nsix variables X(1),...,X(6), and a graph linking consecutive integers and closing with\n\n13.2. MODELS ON UNDIRECTED GRAPHS\n307\nan edge between 1 and 6. Assume that X(1) = X(2) = X(4) = X(5), and that X(1),X(3)\nand X(6) are independent. Then (13.19) is true, since, for k = 1,2,4,5, X(k) is constant\ngiven its neighbors, and X(3) (resp. X(6)) is independent of the rest of the variables.\nBut (X(1),X(2)) is not independent of (X(4),X(5)) given the neighbors X(3),X(6).\nFinally, another statement equivalent to proposition 13.18 is the following:\nProposition 13.19 If the joint distribution of (X(s),s ∈V ) is positive and, for any s,t ∈V ,\ns ≁G t ⇒(X(s)yX(t) | X(V \\{s,t})),\nthen X is Markov relative to G. The converse statement is true without the positivity\nassumption.\nProof Fix s ∈V and assume that (X(s)yX(R) | X(V \\R)) for any R ⊂Ws with cardinality\nat most k (the statement is true for k = 1 by assumption). Consider a set ˜R ⊂Ws of\ncardinality k + 1, that we decompose into R ∪{t} for some t ∈˜R. We have (X(s)yX(t) |\nX(V \\ ˜R),XR) from the initial hypothesis and (X(s)yX(R) | X(V \\ ˜R),Xt) from the induction\nhypothesis. Using property (CI5), this yields (X(s)yX( ˜R) | X(V \\ ˜R)). This proves the\nproposition by induction.\n■\nRemark 13.20 It is obvious from the definition of a G-Markov process that, if X is\nMarkov for a graph G = (V ,E), it is automatically Markov for any richer graph, i.e.,\nany graph ˜G = (V , ˜E) with E ⊂˜E. This is because separation in ˜G implies separation\nin G. Moreover, any X is G-Markov for the complete graph on V , for which s ∼t for\nall s , t ∈V . This is because no pair of sets can be separated in a complete graph.\nAny graph with respect to which X is Markov must be richer than the graph\nGX = (V ,EX) defined by s ≁GX t if and only (X(s)yX(t) | X({s,t}c)). This is true because,\nfor any graph G for which X is Markov, we have\ns ≁G t ⇒(X(s)yX(t) | X({s,t}c)) ⇒s ≁GX t.\nInterestingly, proposition 13.19 states that X is GX-Markov as soon as its joint dis-\ntribution is positive. This implies that GX is the minimal graph over which X is\nMarkov in this case.\n♦\n13.2.3\nRestricted graph and partial evidence\nAssume that some variables X(T) = (X(t),t ∈T) (with T ⊂V ) have been observed,\nwith observed values x(T ) = (x(t),t ∈T ). One would like to use this partial evidence\nto get additional information on the remaining variables, X(S) where S = V \\T. From\nthe probabilistic point of view, this means computing the conditional distribution of\nX(S) given X(T) = x(T ).\n\n308\nCHAPTER 13. MARKOV RANDOM FIELDS\nOne important property of G-Markov models is that the Markov property is es-\nsentially conserved when passing to conditional distributions. We introduce for this\nthe following definitions.\nDefinition 13.21 If G = (V ,E) is an undirected graph, a subgraph of G is a graph G′ =\n(V ′,E′) with V ′ ⊂V and E′ ⊂E.\nIf S ⊂V , the restricted graph, GS, of G to S is defined by\nGS = (S,ES) with ES = {e = {s,t} : s,t ∈S and e ∈E}.\n(13.20)\nWe have the following proposition.\nProposition 13.22 Let G = (V ,E) be an undirected graph and X be G-Markov. Let S ⊂V\nand T = Sc. Given a partial evidence x(T ) such that P(X(T) = x(T )) > 0, X(S), conditionally\nto X(T ) = x(T ), is GS-Markov.\nProof The proof is straightforward once it is noticed that\n(AyB | C)GS ⇒(AyB | C ∪T )G\n■\nso that\n(AyB | C)GS\n⇒\n(X(A)yX(B) | X(C),X(T ))P\n⇒\n(X(A)yX(B) | X(C))P(·|X(T )=x(T ))\n13.2.4\nMarginal distributions\nThe effect of taking marginal distributions for a G-Markov model is, unfortunately,\nnot as much a mild operation as computing conditional distributions, in the sense\nthat the conditional independence structure of the marginal distribution may be\nmuch more complex than the original one.\nLet G = (V ,E) be an undirected graph, and let S be a subset of V . Define the\ngraph GS = (S,ES) by {s,t} ∈ES if and only if {s,t} ∈E or there exist u,u′ ∈Sc such\nthat {s,u} ∈E, {t,u′} ∈E and u and u′ are connected by a path in Sc. In other terms\nES links all s,t ∈S that can be connected by a path, all but the extremities of which\nare included in Sc. With this notation, the following proposition holds.\nProposition 13.23 Let G = (V ,E) be an undirected graph, and S ⊂V . Assume that X =\n(X(s),s ∈V ) is a family of random variables which is G-Markov. Then X(S) = (x(s),s ∈S)\nis GS-Markov.\n\n13.3. THE HAMMERSLEY-CLIFFORD THEOREM\n309\nProof It suffices to prove that, for A,B,C ⊂S,\n(AyB | C)GS ⇒(AyB | C)G.\nSo, assume that A and B are separated by C in GS. If a path connects A and B in G,\nwe can, by definition of ES, remove from this path any portion that passes in Sc and\nobtain a valid path in GS. By assumption, this path must pass in C, and therefore so\ndoes the original path.\n■\nThe graph GS can be much more complex than the restricted graph GS intro-\nduced in the previous section (note that, by definition, GS is richer than GS). Take,\nfor example, the graph that corresponds to “hidden Markov models,” for which (cf.\nfig. 13.2)\nV = {1,...,N} × {0,1}\nand edges {s,t} ∈E have either s = (k,0) and t = (l,0) with |k −l| = 1, or s = (k,0) and\nt = (k,1). Let S = {1,...,N} × {1}. Then, GS is totally disconnected (ES = ∅), since no\nedge in G links two elements of S. In contrast, any pair of elements in S is connected\nby a path in Sc, so that GS is a complete graph.\nFigure 13.2: In this graph, variables in the lower row are conditionally independent given\nthe first row, while their marginal distribution requires a completely connected graph.\n13.3\nThe Hammersley-Clifford theorem\nThe Hammersley-Clifford theorem, which will be proved in this section, gives a com-\nplete description of positive Markov processes relative to a given graph, G. It states\nthat positive G-Markov models are associated to families of positive local interac-\ntions indexed by cliques in the graph. We now introduce each of these concepts.\n13.3.1\nFamilies of local interactions\nDefinition 13.24 Let V be a set of vertexes and (Fs,s ∈V ) a collection of state spaces.\nA family of local interactions is a collection of non-negative functions Φ = (ϕC,C ∈C)\nindexed over some subset C of P(V ), such that each ϕC only depends on configurations\n\n310\nCHAPTER 13. MARKOV RANDOM FIELDS\nrestricted to C (i.e., it is defined on F (C)), with values in [0,+∞). (Recall that P(V ) is\nthe set of all subsets of V .)\nSuch a family has order p if no C ∈C has cardinality larger than p. A family of local\ninteractions of order 2 is also called a family of pair interactions.\nSuch a family is said to be consistent, if there exists an x ∈F (V ) such that\nY\nC∈C\nϕC(x(C)) , 0.\nTo a consistent family of local interactions, one associates the probability distribution πΦ\non F (V ) defined by\nπΦ(x) = 1\nZΦ\nY\nC∈C\nϕC(x(C))\n(13.21)\nfor all x ∈F (V ), where ZΦ is a normalizing constant.\nGiven C ⊂P(V ), define the graph GC = (V ,EC) by letting {s,t} ∈EC if and only if\nthere exists C ∈C such that {s,t} ∈C. We then have the following proposition.\nProposition 13.25 Let Φ = (ϕC,C ⊂C) be a consistent family of local interactions, asso-\nciated to some C ⊂P(V ). Then the associated distribution πΦ is GC-Markov.\nProof Let X be a random field associated with π = πΦ.\nAccording to proposi-\ntion 13.17, we must show that, for any S ⊂V , one has\n(X(S)yX(WS) | X(VS))\nwhere VS is the set of neighbors of S in GC and WS = V \\ (VS ∪S). Define the set US\nby\nUS =\n[\nC∈C,S∩C,∅\nC\nso that VS = US \\S and WS = V \\US. To prove conditional independence, we need to\nprove that, for any x ∈F:\nπ(x)πVS(x(VS)) = πUS(x(US))πV \\S(x(V \\S))\n(13.22)\n(where we denote πA the marginal distribution of π on F (A).)\nFrom the definition of π, we have\nπ(x)\n=\n1\nZ\nY\nC∈C\nϕC(x(C))\n=\n1\nZ\nY\nC:C∩S,∅\nϕC(x(C))\nY\nC:C∩S=∅\nϕC(x(C)).\n\n13.3. THE HAMMERSLEY-CLIFFORD THEOREM\n311\nThe first term in the last product only depends on x(US), and the second one only on\nx(V \\S). Introduce the notation\n\nµ1(x(VS)) =\nX\ny(US ):y(VS )=x(VS )\nY\nC:C∩S,∅\nϕC(x(C))\nµ2(x(VS)) =\nX\ny(V \\S):y(VS )=x(VS )\nY\nC:C∩S=∅\nϕC(x(C)).\nWith this notation, we have:\n\nπUS(x(US)) = (µ2(x(VS))/Z)\nY\nC:C∩S,∅\nϕC(x(C))\nπV \\S(x(V \\S)) = (µ1(x(VS))/Z)\nY\nC:C∩S=∅\nϕC(x(C))\nπVS(x(VS)) = µ1(x(VS))µ2(x(VS))/Z\nfrom which (13.22) can be easily obtained.\n■\nWe now discuss conditional distributions and marginals for processes associated\nwith local interactions. If T ⊂V , we let πT = πΦ\nT denote the marginal distribution of\nπ on T.\nWe start with a discussion of conditionals. Let π be associated with Φ, and let\nS ⊂V and T = V \\S. Assume that a configuration y(T) is given, such that πT (y(T)) > 0,\nand consider the conditional distribution\nπS|T (x(S) | y(T)) = π(x(S) ∧y(T))/πT (y(T )).\n(13.23)\nWe have the following proposition.\nProposition 13.26 With the notation above, πS|T (· | y(T )) is associated to the family of\nlocal interactions Φ|yT = (ϕ ˜C|y(T ), ˜C ∈CS) with\nCS =\nn ˜C : ˜C ⊂S,∃C ∈C : ˜C = C ∩S\no\nand\nϕ ˜C|y(T )(x( ˜C)) =\nY\nC∈C:C∩S= ˜C\nϕC(x( ˜C) ∧y(C∩T)).\nProof From (13.23) and the definition of π, it is easy to sees that\nπS|T (x(S) | y(T)) =\n1\nZ(y(T ))\nY\nC:C∩S,∅\nϕC(x(C∩S) ∧y(C∩T)),\nwhere Z(y(T )) is a constant that only depends on y(T ). The fact that πS|T (· | y(T )) is\nassociated to Φ|y(T) is then obtained by reorganizing the product over distinct S ∩\nC’s.\n■\n\n312\nCHAPTER 13. MARKOV RANDOM FIELDS\nThis result, combined with proposition 13.25, is consistent with proposition 13.22,\nin the sense that the restriction to GC to S coincides with the graph GCS. The easy\nproof is left to the reader.\nWe now consider marginals, and more specifically marginals when only one node\nis removed, which provides the basis for “node elimination.”\nProposition 13.27 Let π be associated to Φ = (ϕC,C ∈C) as above. Let t ∈V and\nS = V \\ {t}. Define Ct ∈P(V ) as the set\nCt = {C ∈C : t < C} ∪{ ˜Ct}\nwith\n˜Ct =\n[\nC∈C:t∈C\nC \\ {t}.\nDefine a family of local interactions Φt = ( ˜ϕ ˜C, ˜C ∈Ct) by ˜ϕ ˜C = ϕ ˜C if ˜C , ˜Ct and:\n• If ˜Ct < C:\n˜ϕ ˜Ct(x( ˜Ct)) =\nX\ny(t)∈Ft\nY\nC∈C,t∈C\nϕC(x( ˜Ct) ∧y(t)).\n• If ˜Ct ∈C:\n˜ϕ ˜Ct(x( ˜Ct)) = ϕCt(x( ˜Ct))\nX\ny(t)∈Ft\nY\nC∈C,t∈C\nϕC(x(Ct) ∧y(t))\nThen the marginal, πS, of π over S is the distribution associated to Φt.\nThe proof is almost straightforward by summing over possible values of yt in the\nexpression of π and left to the reader.\n13.3.2\nCharacterization of positive G-Markov processes\nUsing families of local interactions is a typical way to build graphical models in\napplications. The previous section describes a graph with respect to which the ob-\ntained process is Markov. Conversely, given a graph G, the Hammersley-Clifford\ntheorems states that families of local interactions over the cliques of G are the only\nways to build positive graphical models, which reinforces the importance of this\nconstruction. We now pass to the statement and proof of this theorem, starting with\nthe following definition.\nDefinition 13.28 Let G = (V ,E) be an undirected graph. A clique in G is a nonempty\nsubset C ⊂V such that s ∼G t whenever s,t ∈C, s , t. (In particular, subsets of cardinality\none are always cliques.) Cliques therefore form complete subgraphs of G.\n\n13.3. THE HAMMERSLEY-CLIFFORD THEOREM\n313\nThe set of cliques of a graph G will be denoted CG.\nA clique that cannot be strictly included in any other clique is called a maximal clique,\nand their set denoted C∗\nG.\n(Note that some authors call cliques what we refer to as maximal cliques.)\nGiven G = (V ,E), consider a random field X = (X(s),s ∈V ). We assume that X(s)\ntakes values in a finite set Fs with P(X(s) = a) > 0 for any a ∈Fs (this is no loss of\ngenerality since one can always restrict Fs to such a’s). If S ⊂V , we denote as before\nF (S) the set of restrictions of configurations to S. With this notation, X is positive,\naccording to definition 13.4, if and only if P(X = x) > 0 for all x ∈F (V ). We will let\nπ = PX be the probability distribution of X, so that π(x) = P(X = x) and use as above\nthe notation: for S,T ⊂V\n\nπS(x(S)) = P(X(S) = x(S))\nπS|T (x(S) | x(T )) = P(X(S) = x(s) | X(T) = x(T )).\n(13.24)\n(For the first notation, we will simply write π if S = V .)\nWe will also need to fix a reference, or “zero,” configuration in F (V ) that we will\ndenote 0 = (0(s),s ∈V ), with 0(s) ∈Fs for all s. We can choose it arbitrarily. Given this,\nwe have the theorem:\nTheorem 13.29 (Hammersley-Clifford) With the previous notation, X is a positive G-\nMarkov process if and only if its distribution, π, is associated to a family of local interac-\ntions Φ = (ϕC,C ⊂CG) such that ϕC(x(C)) > 0 for all x(C) ∈F (C).\nMoreover, Φ is uniquely characterized by the additional constraint: ϕC(x(C)) = 1 as\nsoon as there exists s ∈C such that x(s) = 0(s).\nLetting λC = −logϕC, we get an equivalent formulation of the theorem in terms\nof potentials, where a potential is defined as a family of functions\nΛ = (λC,C ∈C)\nindexed by a subset C of P(V ), such that λC only depends on x(C). The distribution\nassociated to Λ is\nπ(x) = 1\nZΛ\nexp\n\u0012\n−\nX\nC∈C\nλC(x(C))\n\u0013\n.\n(13.25)\nWith this terminology, we trivially have an equivalent formulation:\n\n314\nCHAPTER 13. MARKOV RANDOM FIELDS\nTheorem 13.30 X is a positive G-Markov process if and only if its distribution, π, is\nassociated to a potential Λ = (λC,C ⊂CG).\nMoreover, Λ is uniquely characterized by the additional constraint: λC(x(C)) = 0 as\nsoon as there exists s ∈C such that x(s) = 0(s).\nWe now prove this theorem.\nProof Let us start with the “if” part. If π is associated to a potential over CG, we\nhave already proved that π is GCG-Markov, so that it suffices to prove that GCG = G,\nwhich is almost obvious: If s ∼G t, then {s,t} ∈CG and s ∼GCG t by definition of GCG.\nConversely, if s ∼GCG t, there exists C ∈CG such that {s,t} ⊂C, which implies that\ns ∼G t, by definition of a clique.\nWe now prove the “only if” part, which relies on a combinatorial lemma, which\nis one of M¨obius’s inversion formulas.\nLemma 13.31 Let A be a finite set and f : P(A) →R, B 7→fB. Then, there is a unique\nfunction λ : P(A) →R such that\n∀B ⊂A, fB =\nX\nC⊂B\nλC,\n(13.26)\nand λ is given by\nλC =\nX\nB⊂C\n(−1)|C|−|B|fB.\n(13.27)\nTo prove the lemma, first notice that the space F of functions f : P(A) →R is a vector\nspace of dimension 2|A| and that the transformation ϕ : λ 7→f with fB = P\nC⊂B λC\nis linear. It therefore suffices to prove that, given any f , the function λ given in\n(13.27) satisfies ϕ(λ) = f , since this proves that ϕ is onto from F to F and therefore\nnecessarily one to one.\nSo consider f and λ given by (13.27). Then\nϕ(λ)(B) =\nX\nC⊂B\nλC =\nX\nC⊂B\nX\n˜B⊂C\n(−1)|C|−| ˜B|f ˜B =\nX\n˜B⊂B\n\n\nX\nC⊃˜B,C⊂B\n(−1)|C|−| ˜B|\n\nf ˜B = fB\nThe last identity comes from the fact that, for any finite set ˜B ⊂B, ˜B , B, we have\nX\nC⊃˜B,C⊂B\n(−1)|C|−| ˜B| = 0\n\n13.3. THE HAMMERSLEY-CLIFFORD THEOREM\n315\n(for ˜B = B, the sum is obviously equal to 1). Indeed, if s ∈B, s < ˜B, we have\nX\nC⊃˜B,C⊂B\n(−1)|C|−| ˜B|\n=\nX\nC⊃˜B,C⊂B,s∈C\n(−1)|C|−| ˜B| +\nX\nC⊃˜B,C⊂B,s<C\n(−1)|C|−| ˜B|\n=\nX\nC⊃˜B,C⊂B,s<C\n((−1)|C∪{s}|−| ˜B| + (−1)|C|−| ˜B|)\n=\n0.\nSo the lemma is proved. We now proceed to proving the existence and unique-\nness statements in theorem 13.30. Assume that X is G-Markov and positive. Fix\nx ∈F (V ) and consider the function, defined on P(V ) by\nfB(x(B)) = −log π(x(B) ∧0(Bc))\nπ(0)\n.\nThen, letting\nλC(x(C)) =\nX\nB⊂C\n(−1)|C|−|B|fB(x(B)),\nwe have fB(x(B)) = P\nC⊂B λC(x(C)). In particular, for B = V , this gives\nπ(x) = 1\nZ exp\n\u0012\n−\nX\nC⊂V\nλC(x(C))\n\u0013\nwith Z = P(0). We now prove that λC(x(C)) = 0 if x(s) = 0(s) for some s ∈V or if C < CG.\nThis will prove (13.25) and the existence statement in theorem 13.30.\nSo, assume x(s) = 0(s). Then, for any B such that s < B, we have fB(x(B)) = f{s}∪B(x({s}∪B)).\nNow take C with s ∈C. We have\nλC(x(C))\n=\nX\nB⊂C,s∈B\n(−1)|C|−|B|fB(x(B)) +\nX\nB⊂C,s<B\n(−1)|C|−|B|fB(x(B))\n=\nX\nB⊂C,s<B\n(−1)|C|−|B∪{s}|fB∪{s}(x(B∪{s})) +\nX\nB⊂C,s<B\n(−1)|C|−|B|fB(x(B))\n=\nX\nB⊂C,s<B\n((−1)|C|−|B∪{s}| + (−1)|C|−|B|)fB(x(B))\n= 0.\nNow assume that C is not a clique, and let s , t ∈C such that s ≁t. We can write,\nusing decompositions similar to the above,\nλC(x(C)) =\nX\nB⊂C\\{s,t}\n(−1)|C|−|B| \u0010\nfB∪{s,t}(x(B∪{s,t})) −fB∪{s}(x(B∪{s})) −fB∪{t}(x(B∪{t})) + fB(x(B))\n\u0011\n.\n\n316\nCHAPTER 13. MARKOV RANDOM FIELDS\nBut, for B ⊂C \\ {s,t}, we have\nfB∪{s,t}(x(B∪{s,t})) −fB∪{s}(x(B∪{s}))\n=\n−log π(x(B∪{s,t}) ∧0(Bc\\{s,t}))\nπ(x(B∪{s}) ∧0(Bc\\{s}))\n=\nlog πt(x(t) | x(B∪{s}) ∧0(Bc\\{s,t}))\nπt(0(t) | x(B∪{s}) ∧0(Bc\\{s,t}))\nand\nfB∪{t}(x(B∪{t})) −fB(x(B))\n=\n−log π(x(B∪{t}) ∧0(Bc\\{t}))\nπ(x(B) ∧0(Bc))\n=\nlog πt(x(t) | x(B) ∧0(Bc\\{t}))\nπt(0(t) | x(B) ∧0(Bc\\{t})).\nSo, we can write\nλC(x(C)) =\nX\nB⊂C\\{s,t}\n(−1)|C|−|B| log πt(x(t) | x(B∪{s}) ∧0(Bc\\{s,t}))πt(0(t) | x(B) ∧0(Bc\\{t}))\nπt(0(t) | xB∪{s} ∧0(Bc\\{s,t}))πt(x(t) | x(B) ∧0(Bc\\{t}))\nwhich vanishes, because\nπt(x(t) | x(B∪{s}) ∧0(Bc\\{s,t})) = πt(x(t) | x(B) ∧0(Bc\\{t}))\nwhen s ≁t.\nTo prove uniqueness, note that, for any zero-normalized Λ satisfying (13.25), we\nmust have π(0) = 1/Z and therefore, for any x,\n−log π(x(B) ∧0(Bc))\nπ(0)\n=\nX\nC⊂B\nλC(x(C))\n(extending Λ so that λC = 0 for C < CG). But, from lemma 13.31, this uniquely\ndefines Λ.\n■\nThe exponential form of the distribution in the Hammersley-Clifford theorem is\nrelated to what is called a Gibbs distribution in statistical mechanics. More precisely:\nDefinition 13.32 Let F be a finite set and W : F →R be a scalar function. The Gibbs\ndistribution with energy W at temperature T > 0 is defined by\nπ(x) = 1\nZT\ne−W(x)\nT , x ∈F\nThe normalizing constant ZT = P\ny∈F exp(−W(y)/T) is called the partition function.\nIf Λ = (λC,C ⊂V ) is a potential then its associated energy is\nW(x) =\nX\nC⊂V\nλC(x(C)).\n\n13.4. MODELS ON ACYCLIC GRAPHS\n317\nSo the Hammersley-Clifford theorem implies that any positive G-Markov model is\nassociated to a unique zero-normalized potential defined over the cliques of G. This\nrepresentation can also be used to provide an alternate proof of proposition 13.19,\nwhich is left to the reader. Finally, one can restate proposition 13.26 in terms of\npotentials, yielding:\nProposition 13.33 Let P be a Gibbs distribution associated with a zero-normalized po-\ntential λ = (λC,C ⊂V ). Let S ⊂V and T = Sc. Then the conditional distribution of X(S)\ngiven X(T ) = x(T ) is the Gibbs distribution associated with the zero-normalized potential\n˜λ = ( ˜λC,C ⊂S) where\n˜λC(y(C)) =\nX\nC′⊂V ,C′∩S=C\nλC′(y(C) ∧x(T ∩C′)).\n13.4\nModels on acyclic graphs\n13.4.1\nFinite Markov chains\nWe now review a few important examples of Markov processes X associated to spe-\ncific graphs G = (V ,E). We will always denote by Fs the space in which X(s) takes his\nvalues, for s ∈V .\nThe simplest example of G-Markov process (for any graph G) is the case when\nX = (X(s),s ∈V ) is a collection of independent random variables. In this case, we can\ntake GX = (V ,∅), the totally disconnected graph on V . Another simple fact is that, as\nalready remarked, any X is Markov for the complete graph (V ,P2(V )) where P2(V )\ncontains all subsets of V with cardinality 2.\nBeyond these trivial (but nonetheless important) cases, the simplest graph-Markov\nprocesses are those associated with linear graphs, providing finite Markov chains.\nFor this, we let V be a finite ordered set, say,\nV = {0,...,N}.\nWe say that X is a finite Markov chain if, for any k = 1,...,N\n(X(k)y(X(0),...,X(k−2)) | X(k−1)).\nSo we have the identity\nP(X(0) = x(0),...,X(k) = x(k))P(X(k−1) = x(k−1))\n= P(X(0) = x(0),...,X(k−1) = x(k−1))P(X(k−1) = x(k−1),X(k) = x(k)).\n\n318\nCHAPTER 13. MARKOV RANDOM FIELDS\nThe distribution of a Markov chain is therefore fully specified by P(X(0) = x(0)),x0 ∈\nF0 (the initial distribution) and the conditional probabilities\npk(x(k−1),x(k)) = P(X(k) = x(k) | X(k−1) = x(k−1))\n(13.28)\n(with an arbitrary choice when P(X(k−1) = x(k−1)) = 0). Indeed, assume that P(X(0) =\nx(0),...,X(k−1) = x(k−1)) is known (for all x(0),...,x(k−1)). Then, either:\n(i) P(X(0) = x(0),...,X(k−1) = x(k−1)) = 0, in which case\nP(X(0) = x(0),...,X(k) = x(k)) = 0\nfor any x(k), or:\n(ii) P(X(0) = x(0),...,X(k−1) = x(k−1)) > 0, in which case, necessarily, P(X(k−1) = x(k−1)) >\n0, and\nP(X(0) = x(0),...,X(k) = x(k)) = pk(x(k−1),x(k))P(X(0) = x(0),...,X(k−1) = x(k−1)).\nNote that pk in (13.28) is a transition probability (according to definition 12.2) be-\ntween Fk−1 and Fk.\nWe have the following identification of a finite Markov chain with a graph-Markov\nprocess:\nProposition 13.34 Let X = (X(0),...,X(N)) be a finite Markov chain, such that X is posi-\ntive. Then X is G-Markov for the linear graph G = (V ,E) with\nV\n=\n{1,...,N}\nE\n=\n{{1,2},...,{N −1,N}}.\nThe converse is true without the positivity assumption: a G-Markov process for the graph\nabove is always a finite Markov chain.\nProof We prove the direct statement (the converse one being obvious). Let s and t\nbe nonconsecutive distinct integers, with, say, s < t. From the Markov chain assump-\ntion, we have\n(X(t)y(X(s),X({1,t−2}\\{s,})) | X(t−1)),\nwhich, using (CI3), yields (X(t)yX(s) | X({1,...,t−1}\\{s})). Define Y (u) = X({1,...,u}\\{s,t}): what\nwe have proved is (X(t)yX(s) | Y (t)).\nWe now proceed by induction and assume that (X(t)yX(s) | Y (u)) for some u ≥t.\nThen, we have (X(u+1)y(X(s),X(t),Y (u−1)) | X(u)), which implies (from (CI3)) (X(u+1)yX(t) |\nX(s),Y (u)). Applying (CI4) to (X(t)yX(s) | Y (u)) and (X(t)yX(u+1) | X(s),Y (u)), we obtain\n(X(t)y(X(s),X(u+1)) | Y (u)) and finally, (X(t)yX(s) | Y (u+1)). By induction, this gives\n(X(t)yX(s) | Y (N)) and therefore proposition 13.19 now implies that X is G-Markov.\n(The proposition can also be proved as a consequence of the decomposition\nP(X(0) = x(0),...,X(N) = x(N)) = P(X(0) = x(0))p1(x(0),x(1))...pN(x(N−1),x(N)).)\n■\n\n13.4. MODELS ON ACYCLIC GRAPHS\n319\n13.4.2\nUndirected acyclic graph models and trees\nThe situation with acyclic graphs is only slightly more complex than with linear\ngraphs, but will require a few new definitions, including those of directed graphs\nand trees.\nThe difference between directed and undirected graphs is that the edges of the\nformer are ordered pairs, namely:\nDefinition 13.35 A (finite) directed graph G is a pair G = (V ,E) where V is a finite set\nof vertexes and E is a subset of\nV × V \\ {(s,s),s ∈V },\nwhich satisfies, in addition,\n(s,t) ∈E ⇒(t,s) < E.\nSo, for directed graphs, edges (s,t) and (t,s) have different meanings, and we allow\nat most one of them in E. We say that the edge e = (s,t) stems from s and points to t.\nThe parents of a vertex s are the vertexes t such that (t,s) ∈E, and its children are the\nvertexes t such that (s,t) ∈E. We will also use the notation s →G t to indicate that\n(s,t) ∈E (compare to s ∼G t for undirected graphs).\nDefinition 13.36 A path in a directed graph G = (V ,E) is a sequence (s0,...,sN) such\nthat, for all k = 1,...,N, sk →G sk+1 (this includes the “trivial”, one-vertex, paths (s0)).\n(The definition was the same for undirected graph, replacing sk →G sk+1 by sk ∼G sk+1.)\nFor both directed and undirected cases, one says that a path is closed if s0 = sN.\nIn an undirected graph, a path is folded if it can be written as (s0,...,sN−1,sN,sN−1,...,s0).\nIf G = (V ,E) is directed, one says that t ∈V is a descendant of s ∈V (or that s is an\nancestor of t) if there exists a path starting at s and ending at t. In particular, every vertex\nis both a descendant and an ancestor of itself.\nWe finally define acyclic graphs.\nDefinition 13.37 A loop in a directed (resp. an undirected) graph G is a path (s0,s1,...,sN),\nwith N ≥3, such that sN = s0, which passes only once through s0,...,sN−1 (no self-\nintersection except at the end).\nA (directed or undirected) graph G is acyclic if it contains no loop.\nThe following property will be useful.\n\n320\nCHAPTER 13. MARKOV RANDOM FIELDS\nProposition 13.38 In a directed graph, any non-trivial closed path contains a loop (i.e.,\none can delete vertexes from it to finally obtain a loop.)\nIn an undirected graph, any non-trivial closed path which is not a union of folded\npaths contains a loop.\nProof Take γ = (s0,s1,...,sN) with sN = s0. The path being non-trivial means N > 1.\nFirst take the case of a directed graph. Clearly, N ≥3 since a two-vertex path\ncannot be closed in an directed graph. Consider the first occurrence of a repetition,\ni.e., the first index for which\nsj ∈{s0,...,sj−1}.\nThen there is a unique j′ ∈{0,...,j −1} such that sj′ = sj, and the path (sj′,...,sj−1)\nmust be a loop (any repetition in the sequence would contradict the fact that j was\nthe first occurrence. This proves the result in the directed case.\nConsider now an undirected graph. We can recursively remove all folded sub-\npaths, by keeping everything but their initial point, since each such operation still\nprovide a path at the end. Assume that this is done, still denoting the remaining\npath (s0,s1,...,sN), which therefore has no folded subpath. We must have N ≥3\nsince N = 1 implies that the original path was a union of folded paths, and N = 2\nprovides a folded path. Let, 0 ≤j′ < j be as in the directed case. Note that one must\nhave j′ < j −2, since j′ = j −1 would imply an edge between j and itself and j′ = j −2\ninduces a folded subpath. But this implies that (sj′,...,sj−1) is a loop.\n■\nDirected acyclic graphs (DAG) will be important for us, because they are associ-\nated with Bayesian networks that we will discuss later. For now, we are interested\nwith undirected acyclic graphs and their relation to trees, which form a subclass of\ndirected acyclic graphs, defined as follows.\nDefinition 13.39 A forest is a directed acyclic graph with the additional requirement\nthat each of its vertexes has at most one parent.\nA root in a forest is a vertex that has no parent. A forest with a single root is called a\ntree.\nIt is clear that a forest has at least one root, since one could otherwise describe\na nontrivial loop by starting from a any vertex and passing to its parent until the\nsequence self-intersects (which must happen since V is finite). We will use the fol-\nlowing definition.\nDefinition 13.40 If G = (V ,E) is a directed graph, its flattened graph, denoted G♭=\n(V ,E♭) is the undirected graph obtained by forgetting the edge ordering, namely\n{s,t} ∈E♭⇔(s,t) ∈E or (t,s) ∈E.\n\n13.4. MODELS ON ACYCLIC GRAPHS\n321\nThe following proposition relates forests and undirected acyclic graphs.\nProposition 13.41 If G is a forest, then G♭is an undirected acyclic graph.\nConversely, if G is an undirected acyclic graph, there exists a forest ˜G such that ˜G♭= G.\nProof Let G = (V ,E) be a forest and, in order to reach a contradiction, assume that\nG♭has a loop, s0,...,sN−1,sN = s0. Assume that (s0,s1) ∈E; then, also (s1,s2) ∈E\n(otherwise s1 would have two parents), and this propagates to all (sk,sk+1) for k =\n0,...,N −1. But, since sN = s0, this provides a loop in G which is not possible. This\nproves thet G♭has no loop since the case (s1,s0) ∈E is treated similarly.\nNow, let G be an undirected acyclic graph. Fix a vertex s ∈V and consider the\nfollowing procedure, in which we recursively define sets Sk of processed vertexes,\nand ˜Ek of oriented edges, k ≥0, initialized with S0 = {s} and ˜E0 = ∅.\n– At step k of the procedure, assume that vertexes in Sk have been processed and\nedges in ˜Ek have been oriented so that (Sk, ˜Ek) is a forest, and that ˜E♭\nk is the set of edges\n{s,t} ∈E such that s,t ∈Sk (so, oriented edges at step k can only involve processed\nvertexes).\n– If Sk = V : stop, the proposition is proved.\n– Otherwise, apply the following construction. Let Fk be the set of edges in E that\ncontain exactly one element of Sk.\n(1) If Fk = ∅, take any s ∈V \\ Sk as a new root and let Sk+1 = Sk ∪{s}, ˜Ek+1 = ˜Ek.\n(2) Otherwise, add to ˜Ek the oriented edges (s,t) such that s ∈Sk and {s,t} ∈Fk,\nyielding ˜Ek+1, and add to Sk the corresponding children (t’s) yielding Sk+1.\nWe need to justify the fact that ˜Gk+1 = (Sk+1, ˜Ek+1) above is still a forest. This is\nobvious after Case (1), so consider Case (2). First ˜Gk+1 is acyclic, since any oriented\nloop is a fortiori an unoriented loop and G is acyclic. So we need to prove that no\nvertex in Sk+1 has two parents. Since we did not add any parent to the vertexes in Sk\nand, by assumption, (Sk, ˜Ek) is a forest, the only possibility for a vertex to have two\nparents in Sk+1 is the existence of t such that there exists s,s′ ∈Sk with {s,t} and {s′,t}\nin E. But, since s and s′ have unaccounted edges containing them, they cannot have\nbeen introduced in Sk before the previously introduced root has been added, so they\nare both connected to this root: but the two connections to t would create a loop in\nG which is impossible.\nSo the procedure carries on, and must end with Sk = V at some point since we\nkeep adding points to Sk at each step.\n■\nNote that the previous proof shows there is more than one possible orientation\nof a connected undirected tree into a tree is not unique, although uniquely specified\n\n322\nCHAPTER 13. MARKOV RANDOM FIELDS\nonce a root is chosen. The proof is constructive, and provides an algorithm building\na forest from an undirected acyclic graph.\nWe now define graphical models supported by trees, which constitute our first\nMarkov models associated with directed graphs. Define the depth of a vertex in a\ntree G = (V ,E) to be the number of edges in the unique path that links it to the\nroot. We will denote by Gd the set of vertexes in G that are at depth d, so that G0\ncontains only the root, G1 the children of the root and so on. Using this, we have the\ndefinition:\nDefinition 13.42 Let G = (V ,E) be a tree. A process X = (X(s),s ∈V ) is G-Markov if\nand only, for each d ≥1, and for each s ∈Gd, we have\n(X(s)y(X(Gd\\{s}),X(Gq\\{pa(s)}),q < d) | X(pa(s)))\n(13.29)\nwhere pa(s) is the parent of s.\nSo, conditional to its parent, X(s) is independent from all other variables at depth\nsmaller or equal to the depth of s.\nNote that, from (CI3), definition 13.42 implies that, for all s ∈Gd,\n(X(s)yX(Gd\\{s}) | X(Gq),q < d),\nwhich, using proposition 13.6, implies that the variables (X(s),s ∈Gd) are mutually\nindependent given X(Gq),q < d. This implies that, for d = 1 (letting s0 denote the root\nin G):\nP(X(G1) = x(G1),X(s0) = x(s0)) = P(X(s0) = x(s0))\nY\ns∈G1\nP(X(s) = x(s) | X(s0) = x(s0)).\n(If P(X(s0) = x(s0)) = 0, the choice for the conditional probabilities can be made ar-\nbitrarily without changing the left-hand side which vanishes.) More generally, we\nhave, letting G<d = G0 ∪··· ∪Gd−1,\nP(X(G≤d) = x(G≤d)) =\nY\ns∈Gd\nP(X(s) = x(s) | X(pa(s)) = x(pa(s)))P(X(G<d) = x(G<d))\n(with again an arbitrary choice for conditional probabilities that are not defined) so\nthat, we obtain, by induction, for x ∈F (V )\nP(X = x) = P(X(s0) = x(s0))\nY\ns,s0\nps(x(pa(s)),x(s))\n(13.30)\nwhere ps(x(pa(s)),x(s))\n∆= P(X(s) = x(s) | X(pa(s)) = x(pa(s))) are the tree transition probabil-\nities between a parent and a child. So we have the following proposition.\n\n13.4. MODELS ON ACYCLIC GRAPHS\n323\nProposition 13.43 A process X is Markov relative to a tree G = (V ,E) if and only if there\nexists a probability distribution p0 on Fs0 and a family (pst,(s,t) ∈E) such that pst is a\ntransition probability from Fs to Ft and\nPX(x) = p0(xs0)\nY\n(s,t)∈E\npst(x(s),x(t)), x ∈F (V ).\n(13.31)\nWe only have proved the “only if” part, but the “if” part is obvious from (13.31).\nAnother property that becomes obvious with this expression is the first part of the\nfollowing proposition.\nProposition 13.44 If a process X is Markov relative to a tree G = (V ,E) then it is G♭\nMarkov. Conversely, if G = (V ,E) is an undirected acyclic graph and X is G-Markov, then\nX is Markov relative to any tree ˜G such that ˜G♭= G.\nProof To prove the converse part, assume that G = (V ,E) is undirected acyclic and\nthat X is G-Markov. Take ˜G such that ˜G♭= G. For s ∈V and its parent pa(s) in ˜G, the\nsets {s} and ˜G≤d \\ {s,pa(s)} are separated by pa(s) in G. To see this, assume that there\nexists a t ∈˜G≤d \\ {s,pa(s)} with a path from t to s that does not pass through pa(s).\nThen we can complete this path with the path from t to the first common ancestor\n(in ˜G) of t and s and back to s to create a path from s to s that passes only once\nthrough {pa(s),s} and therefore contains a loop by proposition 13.38.\nThe G-Markov property now implies\n(X(s)y(X( ˜Gd\\{s}),X( ˜Gq\\{pa(s)}),q < d) | X(pa(s)))\nwhich proves that X is ˜G-Markov.\n■\nRemark 13.45 We see that there is no real gain in generality with passing from undi-\nrected to directed graphs when working with trees. This is an important remark,\nbecause directionality in graphs is often interpreted as causality. For example, there\nis a natural causal order in the statements\n(it rains) →(car windshields get wet) →(car wipers are on)\nin the sense that each event can be seen as a logical precursor to the next one. How-\never, because one can pass from this directed chain to an equivalent undirected chain\nand then back to a equivalent directed tree by choosing any of the three variables as\nroots, there is no way to infer, from the observation of the joint distribution of the\nthree events (it rains, car windshields get wet, wipers are on), any causal relationship\nbetween them: the joint distribution cannot resolve whether wipers are on because\n\n324\nCHAPTER 13. MARKOV RANDOM FIELDS\nit rains, or whether turning wipers on automatically wets windshields which in turn\ntriggers a shower !\nTo infer causal relationships, one needs a different kind of observation, that\nwould modify the distribution of the system. Such an operation (called an inter-\nvention), can be done, for example, by preventing the windshields from being wet\n(doing, for example, the observation in a parking garage), or forcing them to be wet\n(using a hose). Then, one can compare observations made with these new condi-\ntions, and those made with the original system, and check, for example, whether\nthey modified the probability that rain occurs outside. The answer (likely to be\nnegative !) would refute any causal relationship from “windshields are wet” to “it\nrains.” On the other hand, the intervention might modify how wipers are used,\nwhich would indicate a possible causal relationship from “windshields are wet” to\n“wipers are on.”\n♦\n13.5\nExamples of general “loopy” Markov random fields\nWe will see that acyclic models have very nice computational properties that make\nthem attractive in designing distributions. However, the absence of loops is a very\nrestrictive constraint, which is not realistic in many practical situations. Feedback\neffects are often needed, for example. Most models in statistical physics are sup-\nported by a lattice, in which natural translation/rotation invariance relations forbid\nusing any non-trivial acyclic model. As an example, we now consider the 2D Ising\nmodel on a finite grid, which is a model for (anti)-ferromagnetic interaction in a spin\nsystem.\nLet G = (V ,E). A (positive) G-Markov model is said to have only pair interactions\nif and only if can be written in the form\nπ(x) = 1\nZ exp\n\u0012\n−\nX\ns∈G\nhs(x(s)) −\nX\n{s,t}∈E\nh{s,t}(x(s,t))\n\u0013\n.\nRelating to theorem 13.30, this says that π is associated to a potential involving\ncliques of order 2 at most (note that this does not mean that the cliques of the asso-\nciated graph have order 2 at most; there can be higher-order cliques, which would\nthen have a zero potential). The functions in the potential are indexed by sets, as\nthey should be from the general definition. However, models with pair interactions\nare often written in the form\nπ(x) = 1\nZ exp\n\u0012\n−\nX\ns∈G\nhs(x(s)) −\nX\n{s,t}∈E\n˜hst(x(s),x(t))\n\u0013\nwith ˜hst(λ,µ) = ˜hts(µ,λ) (which is equivalent, taking ˜h = h/2).\n\n13.5. EXAMPLES OF GENERAL “LOOPY” MARKOV RANDOM FIELDS\n325\nThe Ising model is a special case of models with pair interactions, for which the\nstate space, Fs, is equal to {−1,1} for all s and\nhs(x(s)) = αsx(s), h{s,t}(x(s),x(t)) = βstx(s)x(t).\nIn fact, for binary variables, this is the most general pair interaction model.\nFigure 13.3: Graph forming a two-dimensional regular grid.\nThe Ising model is moreover usually defined on a regular lattice, which, in two\ndimensions, implies that V is a finite rectangle in Z2, for example V = {−N,...,N}2.\nThe simplest choice of a translation- and 90-degree rotation-invariant graph is the\nnearest-neighbor graph for which {(i,j),(i′,j′)} ∈E if and only if |i −i′| + |j −j′| = 1\n(see fig. 13.3). With this graph, one can furthermore simplify the model to obtain\nthe isotropic Ising model given by\nπ(x) = 1\nZ exp\n\u0012\n−α\nX\ns∈V\nx(s) −β\nX\ns∼t\nx(s)x(t)\u0013\n.\nWhen β < 0, the model is ferromagnetic: each pair of neighbors with identical signs\nbrings a negative contribution to the energy, making the configuration more likely\n(since lower energy implies higher probability).\n\n326\nCHAPTER 13. MARKOV RANDOM FIELDS\nThe Potts model generalizes the Ising model to finite, but non-necessarily binary,\nstate spaces, say, Fs = F = {1,...,n}. Define the function δ(λ,µ) = 1 if λ = µ and (−1)\notherwise. Then the Potts model is given by\nπ(x) = 1\nZ exp\n\u0012\n−α\nX\ns∈V\nh(x(s)) −β\nX\ns∼t\nδ(x(s),x(t))\n\u0013\n(13.32)\nfor some function h defined on F.\n13.6\nGeneral state spaces\nOur discussion of Markov random fields on graphs was done under the assumption\nof finite state spaces, which notably simplifies many of the arguments and avoids\nrelying too much on measure theory. While this situation does cover a large range of\napplication, there are cases in which one wants to consider variables taking values\nin continuous spaces, or in countable (infinite) spaces.\nThe results obtained for discrete variables can most of the time be extended to\nvariables whose distribution has a p.d.f. with respect to a product of measures on\nthe sets in which they take their values. For example, let X,Y,Z takes values in\nRX,RY,RZ, equipped with σ-algebras SX, SY, SZ and measures µX, µY, µZ. As-\nsume that PX,Y,Z is absolutely continuous with respect to µX ⊗µY ⊗µZ, with density\nϕXYZ. In such a situation, (13.3) remains valid, in that X is conditionally indepen-\ndent of Y given Z if and only if\nϕXYZ(x,y,z)ϕZ(z) = ϕXZ(x,z)ϕYZ(y,z)\n(13.33)\nalmost everywhere (relative to µX ⊗µY ⊗µZ). Here, ϕXZ,ϕYZ,ϕZ are marginal densi-\nties of the indexed random variables. The only difficulty in the argument, provided\nbelow for the interested reader, is dealing properly with sets of measure zero.\nProof (Proof of (13.33)) Introduce the conditional densities\nϕXY|Z(x,y | z) = ϕXYZ(x,y,z)\nϕZ(z)\nand similarly ϕX|Z and ϕY|Z, which are defined when z < MZ = {z ∈RZ : ϕZ(z) = 0}.\nBy definition of conditional independence, we have, for all A ∈SX, B ∈SX\nZ\nA×B\nϕXY|Z(x,y | z)µX(dx)µY(dy) =\nZ\nA×B\nϕX|Z(x | z)ϕY|Z(y | z)µX(dx)µY(dy)\nfor all z < MZ, which implies that, for all z < MZ, there exists a set Nz ⊂RX ×RY such\nthat µX × µY(Nz) = 0 and\nϕXY|Z(x,y | z) = ϕX|Z(x | z)ϕY|Z(y | z)\n\n13.6. GENERAL STATE SPACES\n327\nfor all z < MZ and (x,y) < Nz. This immediately implies (13.33) for those (x,y,z).\nIf z ∈MZ, then\n0 = ϕZ(z) =\nZ\nRX\nϕXZ(x,z)µX(dx) =\nZ\nRY\nϕYZ(x,z)µY(dy)\nimplying that ϕXZ(x,z) = ϕYZ(y,z) = 0 excepted on some set Nz such that µX ⊗\nµY(Nz) = 0, and (13.33) is therefore also true outside of this set. Now, letting N =\n{(x,y,z) : (x,y) ∈Nz}, we find that (13.33) is true for all (x,y,z) < N and\nµX⊗µY⊗µZ(N) =\nZ\nRX×RY ×RZ\n1(x,y)∈NzµX(dx)µY(dy)µZ(dz) =\nZ\nRZ\nµX⊗µY(Nz)µZ(dz) = 0.\n(This argument involves Fubini’s theorem [171].)\n■\nWith this definition, the proof of proposition 13.5 can be caried on without change,\nwith the positivity condition expressing the fact that there exists ˜RX ⊂RX, ˜RY ⊂RY\nand ˜RZ ⊂RX such that ϕXYZ(x,y,z) > 0 for all x,y,z ∈˜RX × ˜RY × ˜RZ. (This proposition\nis actually valid in full generality, with a proper definition of positivity.)\nWhen considering random fields with general state spaces, we will restrict to\nthe similar situation in which each state space Fs is equipped with a σ-algebra Ss\nand a measure µs, and the joint distribution, PX of the random field X = (Xs,s ∈V ) is\nabsolutely continuous with respect to µ\n∆=\nN\ns∈V µs, denoting by π the corresponding\np.d.f. We will says that π is positive if there exists ˜F = ( ˜Fs,s ∈V ) with measurable\n˜Fs ⊂Fs such that π(x) > 0 for all x ∈F (V , ˜F). Without loss of generality unless one\nconsiders multiple random fields with different supports, we will assume that ˜Fs = Fs\nfor all s.\nThe definition of consistent families of local interactions (definition 13.24) must\nbe modified by adding the condition that\nZ\nF (V )\nY\nC∈C\nϕC(x(C))µ(dx) < ∞.\n(13.34)\nThis requirement is obviously needed to ensure that the normalizing constant in\n(13.21) is finite. Proposition 13.25 is then true (with sums replaced by integrals in\nthe proof) and so are propositions 13.26 and 13.27. Finally, the Hammersley-Clifford\ntheorem (theorem 13.29) extends to this context.\nEven though it is a natural requirement, condition (13.34) may be hard to assess\nwith general families of local interactions. In the case of Gaussian distributions,\nhowever, one can provide relatively simple conditions. Assume that Fs = R for all\n\n328\nCHAPTER 13. MARKOV RANDOM FIELDS\ns ∈V , and condider a potential Λ = (λC,C ∈C) with only univariate and bivariate\ninteractions, such that, for some vector a ∈Rd (with d = |V |) and symmetric matrix\nb ∈Sd,\n\nλ{s}(x({s})) = −a(s)x(s) + 1\n2bss(x(s))2\nλ{s,t}(x({s,t})) = bstx(s)x(t)\nThen, considering x ∈F (V ) as a d-dimensional vector, we have\nπ(x) = 1\nZ exp\n\u0012\naT x −1\n2xT bx\n\u0013\n,\nwith the integrability requirement that b ≻0 (positive definite). The random field\nthen follows a Gaussian distribution with mean m = b−1a and covariance matrix\nΣ = b−1. The normalizing constant, Z, is given by\nZ = e−1\n2aT ba(2π)d/2\n√\ndetb\n.\nThis Markov random field parametrization of Gaussian distributions emphasizes\nthe conditional structure of the variables rather than their covariances. It is useful\nwhen the associated graph, represented by the matrix b is sparse. In particular,\nthe conditional distribution of X(s) given the other variables is Gaussian, with mean\n(a(s) −P\nt,s bstx(t))/bss and variance 1/bss.\n\nChapter 14\nProbabilistic Inference for Random Fields\nOnce the joint distribution of a family of variables has been modeled as a random\nfield, this model can be used to estimate the probabilities of specific events, or the\nexpectations of random variables of interest. For example, if the modeled variables\nrelate to a medical condition, in which variables such as diagnosis, age, gender, clin-\nical evidence can interact, one may want to compute, say, the probability of someone\nhaving a disease given other observable factors. Note that, being able to compute\nexpectations of the modeled variables for G-Markov processes also ensures that one\ncan compute conditional expectations of some modeled variables given others, since,\nby proposition 13.22, conditional G-Markov distributions are Markov over restricted\ngraphs.\nWe assume that X is G-Markov for a graph G = (V ,E) and restrict (unless spec-\nified otherwise) to finite state spaces. We condider the basic problem to compute\nP(X(S) = x(S)) when S ⊂V , starting with one-vertex marginals, P(X(s) = x(s)).\nThe Hammersley-Clifford theorem provides a generic form for general positive\nG-Markov processes, in the form\nP(X = x) = π(x) = 1\nZ exp\n\u0012\n−\nX\nC∈CG\nhC(x(C))\n\u0013\n.\n(14.1)\nSo, formally, marginal distributions are given by the ratio\nP(X(S) = x(S)) =\nP\ny∈F (V ),y(S)=x(S) exp\n\u0012\n−P\nC∈CG hC(y(C))\n\u0013\nP\ny∈F (V ) exp\n\u0012\n−P\nC∈CG hC(y(C))\n\u0013\n.\nThe problem is that the sums involved in this ratio involve a number of terms that\ngrows exponentially with the size of V . Unless V is very small, a direct computation\nof these sums is intractable. An exception to this is the case of acyclic graphs, as\n329\n\n330\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nwe will see in section 14.2. But for general, loopy, graphs, the sums can only be\napproximated, using, for example, Monte-Carlo sampling, as described in the next\nsection.\n14.1\nMonte Carlo sampling\nMarkov chain Monte Carlo methods are well adapted to sampling from Markov ran-\ndom fields, because conditional distributions used in Gibbs sampling, or, more gen-\nerally, ratios of probabilities used in the Metropolis-Hastings algorithm do not in\nrequire the computation of the normalizing constant Z in (14.1). The simplest use\nof Gibbs sampling generalizes the Ising model example of section 12.4.2. Using the\nnotation of Algorithm 12.2, one lets B′\ns = F (sc) (with the notation sc = V \\ {s}) and\nUs(x) = x(sc). The conditional distribution given Us is\nQs(Us(x),y) = P(X(s) = y(s) | X(sc) = x(sc))1y(sc)=x(sc).\nThe conditional probability in the r.h.s. of this equation takes the form\nπs(y(s) | x(sc))\n∆= P(X(s) = y(s) | X(sc) = x(sc)) =\n1\nZs(x(sc)) exp\n\n−\nX\nC∈C,s∈∈C\nhC(y(s) ∧x(C∩sc))\n\n\nwith\nZs(x(sc)) =\nX\nz(s)∈Fs\nexp\n\n−\nX\nC∈C,s∈∈C\nhC(z(s) ∧x(C∩sc))\n\n.\nThe Gibbs sampling algorithm samples from Qs by visiting all s ∈V infinitely of-\nten, as described in Algorithm 12.2. Metropolis-Hastings schemes are implemented\nsimilarly, the most common choice using a local update scheme in Algorithm 12.3\nsuch that g(x,·) only changes one coordinate, chosen at random, so that\ng(x,y) = 1\n|V |\nX\ns∈V\n1y(sc)=x(sc)gs(y(s))\nwhere gs is some probability distribution on Fs. The acceptance probability a(x,y) is\nequal to 1 when y = x. If y , x and g(x,y) > 0, there is a unique s for which y(sc) = x(sc)\nand\na(x,y) = min\n \n1, π(y)g(y,x)\nπ(x)g(x,y)\n!\nwith\nπ(y)g(y,x)\nπ(x)g(x,y) = πs(y(s) | x(sc))gs(x(s))\nπs(x(s) | x(sc))gs(y(s)).\n\n14.1. MONTE CARLO SAMPLING\n331\nNote that the latter equation avoids the computation of the local normalizing con-\nstant Zs(x(sc)), which simplifies in the ratio.\nBoth algorithms have a transition probability P that satisfies Pm(x,y) > 0 for all\nx,y ∈F (V ), with m = |V | (for Metropolis-Hastings, one must assume that gs(y(s)) > 0\nfor all y(s) ∈Fs. This ensures that the chain is uniformly geometrically ergodic, i.e.,\n(12.10) is satisfied with a constant M and some ρ < 1. However, in many practical\ncases (especially for strongly structured distributions and large sets V ), the conver-\ngence rate, ρ can be very close to 1, resulting in a slow convergence.\nAcceleration strategies have been designed to address this issue, which is often\ndue to the existence of multiple configurations that are local modes of the probabil-\nity π. Such configurations are isolated from other high-probability configurations\nbecause local updating schemes need to make multiple low-probability changes to\naccess them from the local mode. The following two approaches provide examples\ndesigned to address this issue.\na. Cluster sampling. To facilitate escaping from such local modes, it is sometimes\npossible to augment the state space by introducing a new configuration space, with\nvariable denoted ξ, and designing a joint distributions ˆπ(ξ,x) such that the marginal\ndistribution on F (V ) (summing over ξ) is the targeted π. The additional variable\ncan create high-probability bridges between local modes for π, and accelerate con-\nvergence.\nTo take an example, assume that all sets Fs are identical (letting F = Fs, s ∈V ) and\nthat the auxiliary variable ξ takes values in the set of functions from E to {0,1}, that\nwe will denote B(E), i.e., that it takes the form (ξ(st),{s,t} ∈E), with ξ(st) ∈{0,1}. For\nx ∈F (V ), introduce the set Bx containing all ξ ∈B(E), such that for all {s,t} ∈E,\nx(s) , x(t) ⇒ξ(st) = 1.\nAssume that the conditional distribution of ξ given x is supported by Bx, such that,\nfor ξ ∈Bx\nP(ξ = ξ | X = x) = ˆπ(ξ | x) =\n1\nζ(x) exp\n\n−\nX\n{s,t}∈E\nµstξ(st)\n\n.\nThe coefficients µst are free to choose (and one possible choice is to take µst = 0 for\nall {s,t} ∈E). For this distribution, all ξ(st) are independent conditionally to X = x,\nwith ξ(st) = 1 with probability 1 if x(s) , x(t), and\nP(ξ(st) = 1 | X = x) =\ne−µst\n1 + e−µst\n(14.2)\nif x(s) = x(t). This conditional distribution is, as a consequence, very easy to sample\n\n332\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nfrom. Moreover, the normalizing constant ζ(x) has closed form and is given by\nζ(x) =\nY\n{s,t}∈E\n(1x(s)=x(t) + e−µst) = exp\n\n\nX\n{s,t}∈E\nlog(1 + e−µst) +\nX\n{s,t}∈E\nlog(1 + eµst)1x(s),x(t)\n\n.\nNow consider the conditional probability that X = x given ξ = ξ. For this distribu-\ntion, one has, with probability 1, X(s) = X(t) when ξ(st) = 0. This implies that X is con-\nstant on the connected components of the subgraph (V ,Eξ) of (V ,E), where {s,t} ∈Eξ\nif and only if ξ(st) = 0. Let V1,...,Vm denote these connected components (these com-\nponents and their number depend on ξ). The conditional distribution of X given ξ\nis therefore supported by the configurations such that there exists c1,...,cm ∈F such\nthat x(s) = cj if and only if s ∈Vj, that we will denote, with some abuse of notation:\nc(V1)\n1\n∧··· ∧c(Vm)\nm\n.\nGiven this remark, the conditional distribution of X given ξ = ξ is equivalent to a\ndistribution on Fm, which may be feasible to sample from directly if |F| and m are not\ntoo large. To sample from π, one now needs to alternate between sampling ξ given\nX and the converse, yielding the following first version of cluster-based sampling.\nAlgorithm 14.1 (Cluster-based sampling: Version 1)\nThis algorithm samples from (14.1).\n(1) Initialize the algorithm some configuration x ∈F (V ).\n(2) Loop over the following steps:\na.\nGenerate a configuration ξ ∈Bx such that ξ(st) = 1 with probability given by\n(14.2) when x(s) = x(t).\nb.\nDetermine the connected components, V1,...,Vm, of the graph Gξ = (V ,Eξ)\nwith edges given by pairs {s,t} such that ξ(st) = 1.\nc.\nSample values c1,...,cm ∈F according to the distribution\nq(c1,...,cm) ∝π(c(V1)\n1\n∧··· ∧c(Vm)\nm\n)\nζ(c(V1)\n1\n∧··· ∧c(Vm)\nm\n)\n.\nd. Set x = c(V1)\n1\n∧··· ∧c(Vm)\nm\n.\nStep (2.c) takes a simple form in the special case when π is a non-homogeneous Potts\nmodel ((13.32)) with positive interactions, that we will write as\nπ(x) = exp\n\n−\nX\ns∈V\nαsx(s) −\nX\n{s,t}∈E\nβst1x(s),x(t)\n\n\n\n14.1. MONTE CARLO SAMPLING\n333\nwith βst ≥0. Then\nπ(x)\nζ(x) ∝exp\n\n−\nX\ns∈V\nαsx(s) −\nX\n{s,t}∈E\n(βst −β′\nst)1xs,st\n\n\nwith β′\nst = log(1 + eµst). If one chooses µst such that β′\nst = βst (which is possible since\nβst ≥0), then the interaction term disappears and the probability q in (2.c) is propor-\ntional to\nm\nY\nj=1\nexp\n\n−\nX\ns∈Vj\nαs\n\n\nso that c1,...,cm can be generated independently. The resulting algorithm is the\nSwendsen-Wang sampling algorithm for the Potts model [186]. The presentation\ngiven here adapts the one introduced in Barbu and Zhu [16].\nFor more general models, step (2.c) can be computationally costly, especially if the\nnumber of connected components is large. In this case, this step can be replaced by\na Gibbs sampling step for one of the c′\njs conditional to the others (and ξ) that we\nsummarize in the following variation of Algorithm 14.1.\nAlgorithm 14.2 (Cluster-based sampling: Version 2)\nThis algorithm samples from (14.1).\n(1) Initialize the algorithm some configuration x ∈F (V ).\n(2) Loop over the following steps:\na.\nGenerate a configuration ξ ∈Bx such that ξ(st) = 1 with probability given by\n(14.2) when x(s) = x(t).\nb.\nDetermine the connected components, V1,...,Vm, of the graph Gξ = (V ,Eξ)\nwith edges given by pairs {s,t} such that ξ(st) = 1. Note that x is constant on\neach of these connected components, i.e., there exists c1,...,cm ∈F such that\nx = c(V1)\n1\n∧··· ∧c(Vm)\nm\n.\nc.\nSelect at random one of the components, say, j0 ∈{1,...,m}.\nd. Sample the value ˜cj0 ∈F according to the distribution\nq(˜cj0) ∝π(˜c(V1)\n1\n∧··· ∧˜c(Vm)\nm\n)\nζ(˜c(V1)\n1\n∧··· ∧˜c(Vm)\nm\n)\n.\nwith ˜cj = cj if j , j0.\ne.\nSet x(s) = ˜cj0 for s ∈Vj0.\nUnlike single-variable updating schemes, these algorithms can update large chunks\nof the configurations at each step, and may result in significantly faster convergence\n\n334\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nof the sampling procedure. Note that step (2.d) in Algorithm 14.2 can be replaced\nby a Metropolis-Hastings update with a proper choice of proposal probability [16].\nb. Parallel tempering. We now consider a different kind of extension in which we\nallow π depends continuously on a parameter β > 0, writing πβ and, the goal is to\nsample from π1. For example, one can extend (14.1) by the family of probability\ndistributions\nπβ(x) = 1\nZβ\nexp\n\n−β\nX\nC∈CG\nhC(x(C))\n\n\nfor β ≥0. For small β, πβ gets close to the uniform distribution on F (V ) (achieved for\nβ = 0), so that it becomes easier to move from local mode to local mode. This implies\nthat sampling with small β is more efficient and the associated Markov chain moves\nmore rapidly in the configuration space.\nAssume given, for all β, two ergodic transition probabilities on F (V ), qβ and ˜qβ such\nthat (12.7) is satisfied with πβ as invariant probability, namely\nπβ(y)qβ(y,x) = πβ(x)˜qβ(x,y)\n(14.3)\nfor all x,y ∈F (V ) (as seen in (12.7), ˜qβ is the transition probability for the reversed\nchain). The basic idea is that qβ provides a Markov chain that converges rapidly\nfor small β and slowly when β is closer to 1. Parallel tempering (this algorithm\nwas introduced in Neal [140] based on ideas developed in Marinari and Parisi [126])\nleverages this fact (and the continuity of πβ in β) to accelerate the simulation of π1\nby introducing intermediate steps sampling at low β values.\nThe algorithm specifies a sequence of parameters 0 ≤β1 ≤··· ≤βm = 1. One simula-\ntion steps goes down, then up this scale, as described in the following algorithm.\nAlgorithm 14.3 (Parallel Tempering)\nStart with an initial configuration x0 ∈F (V ). This configuration is then updated at\neach step, using the following sequence of operations.\n(1) For j = 1,...,m, generate a configuration xj according to ˜qβj(xj−1,·).\n(2) Generate a configuration zm−1 according to qβm(xm,·).\n(3) For j = m −1,...,1, generate a configuration zj−1 according to qβj(zj,·).\n(4) Set x0 = z0 with probability\nmin\n\n1,\nπβ0(z0)\nπβ0(x0)\n\n\nm−1\nY\nj=1\nπβj(xj−1)\nπβj(xj)\n\n\nπβm(xm−1)\nπβm(zm−1)\n\n\nm−1\nY\nj=1\nπβj(zj)\nπβj(zj−1)\n\n\n\n.\n(Otherwise, keep x0 unchanged).\n\n14.2. INFERENCE WITH ACYCLIC GRAPHS\n335\nImportantly, the acceptance probability at step (4) only involves ratios of π′\nβs and\ntherefore no normalizing constant. We now show that this algorithm is πβ0-reversible.\nLet p(·,·) denote the transition probability of the chain. If z0 , x0, p(x0,z0) corre-\nsponds to steps (1) to (3), with acceptance at step(4), and is therefore given by the\nsum, over all x1,...,xm and z1,...,zm, of products\n˜qβ1(x0,x1)··· ˜qβm(xm−1,xm)qβm(xm,zm−1)···qβ1(z1,z0)\nmin\n\n1,\nπβ0(z0)\nπβ0(x0)\n\n\nm−1\nY\nj=1\nπβj(xj−1)\nπβj(xj)\n\n\nπβm(xm−1)\nπβm(zm−1)\n\n\nm−1\nY\nj=1\nπβj(zj)\nπβj(zj−1)\n\n\n\n\nApplying (14.3), this is equal to\nmin\n\u0012\n˜qβ1(x0,x1)··· ˜qβm(xm−1,xm)qβm(xm,zm−1)···qβ1(z1,z0),\nπβ0(z0)\nπβ0(x0)qβ1(x0,x1)···qβm(xm−1,xm)˜qβm(xm,zn−1)··· ˜qβ1(z1,z0)\n\u0013\nSo,\nπβ0(x0)p(x0,z0) =\nX\nmin\n\u0012\nπβ0(x0)˜qβ1(x0,x1)··· ˜qβm(xm−1,xm)qβm(xm,zm−1)···qβ1(z1,z0),\nπβ0(z0)qβ1(x1,x0)···qβm(xm,xm−1)˜qβm(zm−1,xm)··· ˜qβ1(z0,z1)\n\u0013\nwhere the sum is over all x1,...,xm,z1,...,zm−1 ∈F (V ). The sum is, of course, un-\nchanged if one renames x1,...,xm,z1,...,zm−1 to z1,...,zm,x1,...,xm−1, but doing so\nprovides the expression of πβ0(z0)p(z0,x0), proving the reversibility of the chain with\nrespect to πβ0.\n14.2\nInference with acyclic graphs\nWe now switch to deterministic methods to compute, or approximate, marginal\nprobabilities of Markov random fields. In this section, we consider a directed acyclic\ngraph G = (V ,E). As we have seen, Markov processes for acyclic graphs are also\nMarkov for any tree structure associated with the graph. Introducing such a tree,\n˜G = (V , ˜E) with ˜G♭= G, we know that a Markov process on G can be written in the\nform (letting s0 denote the root in ˜G):\nπ(x) = ps0(x(s0))\nY\n(s,t)∈˜E\npst(x(s),x(t))\n(14.4)\n\n336\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nwhere ps0 is a probability and pst a transition probability.\nWe now show how to compute marginal probabilities of configurations x(S), de-\nnoted πS(x(S)), for a set S ⊂V , starting with singletons S = {s}. The computation\ncan be done by propagating down the tree as follows. For s = s0, the probability is\nknown, with πs0 = ps0. Now take an arbitrary s , s0 and let pa(s) be its parent. Then\nπs(x(s)) = P(X(s) = x(s)) =\nX\ny(pa(s))∈Fpa(s)\nP(X(s) = x(s) | X(pa(s)) = y(pa(s)))P(x(pa(s)) = y(pa(s)))\n=\nX\ny(pa(s))∈Fpa(s)\nπpa(s)(y(pa(s)))ppa(s)(ypa(s),x(s))\nso that the marginal probability at any s , s0 can be computed given the marginal\nprobability of its parent. We can propagate the computation down the tree, with a\ntotal cost for computing πs proportional to Pn\nk=1 |Ftk−1||Ftk| where t0 = s0,t1,...,tn = s\nis the unique path between s0 and s. This is linear in the depth of the tree, and\nquadratic (not exponential) in the sizes of the state spaces. The computation of all\nsingleton marginals requires an order of P\n(s,t)∈E |Fs||Ft| operations.\nNow, assume that probabilities of singletons have been computed and consider\nan arbitrary set S ⊂V . Let s ∈V be an ancestor of every vertex in S, maximal in the\nsense that none of its children also satisfy this property. Consider the subtrees of\n˜G starting from each of the children of s, denoted ˜G1,..., ˜Gn with ˜Gk = (Vk, ˜Ek). Let\nSk = S ∩Vk. From the conditional independence,\nπS(x(S))\n=\nX\ny(s)∈Fs\nP(X(S\\{s}) = x(S\\{s}) | X(s) = y(s))πs(y(s))\n=\nX\ny(s)∈Fs\nn\nY\nk=1,Sk,∅\nP(X(Sk) = x(Sk) | X(s) = y(s))πs(ys)\nNow, for all k = 1,...,n, we have |Sk| < |S|: this is obvious if S is not completely\nincluded in one of the Vk’s. But if S ⊂Vk then the root, sk, of Vk is an ancestor of\nall the elements in S and is a child of s, which contradicts the assumption that s is\nmaximal. So we have reduced the computation of πS(xS) to the computations of n\nprobabilities of smaller sets, namely P(X(Sk) = x(Sk) | X(s) = y(s)) for Sk , ∅. Because\nthe distribution of X(Vk) conditioned at s is a ˜Gk-Markov model, we can reiterate\nthe procedure until only sets of cardinality one remain, for which we know how to\nexplicitly compute probabilities.\nThis provides a feasible algorithm to compute marginal probabilities with trees,\nat least when its distribution is given in tree-form, like in (14.4). We now address\n\n14.2. INFERENCE WITH ACYCLIC GRAPHS\n337\nthe situation in which one starts with a probability distribution associated with pair\ninteractions (cf. definition 13.24) over the acyclic graph G\nπ(x) = 1\nZ\nY\ns∈V\nϕs(x(s))\nY\n{s,t}∈E\nϕst(x(s),x(t)).\n(14.5)\nWe assume these local interactions to be consistent, still allowing for some vanishing\nϕst(x(s),x(t)).\nPutting π in the form (14.4) is equivalent to computing all joint probability dis-\ntributions πst(x(s),x(t)) for {s,t} ∈E, and we now describe this computation. Denote\nU(x) =\nY\ns∈V\nϕs(x(s))\nY\n{s,t}∈E\nϕst(x(s),x(t))\nso that Z = P\ny∈F (V ) U(y). For the tree ˜G = (V , ˜E), and t ∈V , we let ˜Gt = (Vt, ˜Et) be the\nsubtree of G rooted at t (containing t and all its descendants). For S ⊂V , define\nUS(x(S)) =\nY\ns∈S\nϕs(x(s))\nY\n{s,s′}∈E,s,s′∈D\nϕss′(x(s),x(s′))\nand\nZt(x(t)) =\nX\ny(V ∗t )∈F (V ∗\nt )\nUVt(x(t) ∧y(V ∗\nt )).\nwith V ∗\nt = Vt \\ {t}.\nLemma 14.1 Let G = (V ,E) be a directed acyclic graph and π = PX be the G-Markov\ndistribution given by (14.5). With the notation above, we have\nπs0(x(s0)) =\nZs0(x(s0))\nP\ny(s0)∈Fs0 Zs0(y(s0))\n(14.6)\nand, for (s,t) ∈˜E,\npst(x(s),x(t)) = P(X(t) = x(t) | X(s) = x(s)) =\nϕst(x(s),x(t))Zt(x(t))\nP\ny(t)∈Ft ϕst(x(s),y(t))Zt(y(t))\n(14.7)\nProof Let Wt = V \\Vt. Clearly, Z = P\nx(0)∈Fs0 Zs0(x(0)) and πs0(x(0)) = Zs0(x(0))/Z which\ngives (14.6). Moreover, if s ∈V , we have\nP(X(V ∗s ) = x(V ∗s ) | X(s) = x(s)) =\nP\ny(Ws) U(x(Vs) ∧y(Ws))\nP\ny(V ∗s ),y(Ws) U(x(s) ∧y(V ∗s ) ∧y(Ws)).\n\n338\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nWe can write\nU(x(s) ∧y(V ∗s ) ∧y(Ws)) = UVs(x(s) ∧y(V ∗s ))U{s}∪Ws(x(s) ∧y(Ws))ϕs(x(s))−1\nyielding the simplified expression\nP(X(V ∗s ) = x(V ∗s ) | X(s) = x(s)) =\nUVs(x(Vs))ϕs(x(s))−1 P\nyWs U{s}∪Ws(x(s) ∧y(Ws))\nϕs(x(s))−1\u0010P\ny(V ∗s ) UVs(x(s) ∧y(V ∗s ))\n\u0011\u0010P\ny(Ws) U{s}∪Ws(x(s) ∧y(Ws))\n\u0011\n= UVs(x(Vs))\nZs(x(s))\nNow, if t1,...,tn are the children of s, we have\nUVs(x(Vs)) = ϕs(x(s))\nn\nY\nk=1\nϕstk(x(s),x(tk))\nn\nY\nk=1\nUVtk (x(Vtk )),\nso that\nP(X(tk) = x(tk),k = 1,...,n | X(s) = x(s))\n=\n1\nZs(x(s))\nX\ny\n(V ∗tk ),k=1,...,n\nϕs(x(s))\nn\nY\nk=1\nϕstk(x(s),x(tk))\nn\nY\nk=1\nUVtk (x(tk) ∧y(V ∗\ntk ))\n= ϕs(x(s))Qn\nk=1 ϕstk(x(s),x(tk))Qn\nk=1 Ztk(x(tk))\nZs(x(s))\nThis implies that the transition probability needed for the tree model, pst1(x(s),x(t1)),\nmust be proportional to ϕst1(x(s),x(t1))Zt1(x(t1)) which proves the lemma.\n■\nThis lemma reduces the computation of the transition probabilities to the com-\nputation of Zs(x(s)), for s ∈V . This can be done efficiently, going upward in the\ntree (from terminal vertexes to the root). Indeed, if s is terminal, then Vs = {s} and\nZs(x(s)) = ϕs(x(s)). Now, if s is non-terminal and t1,...,tn are its children, then, it is\neasy to see that\nZs(x(s)) = ϕs(x(s))\nX\nx(t1)∈Ft1,...,x(tn)∈Ftn\nn\nY\nk=1\nϕstk(x(s),x(tk))Ztk(x(tk))\n= ϕs(x(s))\nn\nY\nk=1\n\u0012\nX\nx(tk)∈Ftk\nϕstk(x(s),x(tk))Ztk(x(tk))\n\u0013\n(14.8)\nSo, Zs(x(s)) can be easily computed once the Zt(x(t))’s are known for the children of s.\n\n14.2. INFERENCE WITH ACYCLIC GRAPHS\n339\nEquations (14.6) to (14.8) therefore provide the necessary relations in order to\ncompute the singleton and edge marginal probabilities on the tree. It is important\nto note that these relations are valid for any tree structure consistent with the acyclic\ngraph we started with. We now rephrase them with notation that only depend on\nthis graph and not on the selected orientation.\nLet {s,t} be an edge in E. Then s separates the graph G \\ {s} into two components.\nLet Vst be the component that contains t, and V ∗\nst = Vst \\ t. Define\nZst(xt) =\nX\ny(V ∗st)∈F (V ∗\nst)\nUVst(x(t) ∧y(V ∗\nst)).\nThis Zst coincides with the previously introduced Zt, computed with any tree in\nwhich the edge {s,t} is oriented from s to t. Equation (14.8) can be rewritten with\nthis new notation in the form:\nZst(x(t)) = ϕt(x(t))\nY\nt′∈Vt\\{s}\n\u0012 X\nx(t′)∈Ft′\nϕtt′(x(t),x(t′))Ztt′(x(t′))\n\u0013\n.\n(14.9)\nThis equation is usually written in terms of “messages” defined by\nmts(x(s)) =\nX\nx(t)∈Ft\nϕst(x(s),x(t))Zst(x(t))\nwhich yields\nZst(x(t)) = ϕt(x(t))\nY\nt′∈Vt\\{s}\nmt′t(x(t))\nand the message consistency relation\nmts(x(s)) =\nX\nx(t)∈Ft\nϕst(x(s),x(t))ϕt(x(t))\nY\nt′∈Vt\\{s}\nmt′t(x(t)).\n(14.10)\nAlso, because one can start building a tree from G♭using any vertex as a root,\n(14.6) is valid for any s ∈V , in the form (applying (14.8) to the root)\nπs(x(s)) = 1\nζs\nϕs(x(s))\nY\nt∈Vs\nmts(x(s))\n(14.11)\nwhere ζs is chosen to ensure that the sum of probabilities is 1. (In fact, looking at\nlemma 14.1, we have Zs = Z, independent of s.)\nSimilarly, (14.7) can be written\npst(x(s),x(t)) = mts(x(s))−1ϕst(x(s),x(t))ϕt(x(t))\nY\nt′∈Vt\\{s}\nmt′t(x(t))\n(14.12)\n\n340\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nwhich provides the edge transition probabilities. Combining this with (14.11), we\nget the edge marginal probabilities:\nπst(x(s),x(t)) = 1\nζ ϕst(x(s),x(t))ϕs(x(s))ϕt(x(t))\nY\nt′∈Vt\\{s}\nmt′t(x(t))\nY\ns′∈Vs\\{t}\nms′s(x(s)).\n(14.13)\nRemark 14.2 We can modify (14.10) by multiplying the right-hand side by an ar-\nbitrary constant qts without changing the resulting estimation of probabilities: this\nonly multiplies the messages by a constant, which cancels after normalization. This\nremark can be useful in particular to avoid numerical overflow; one can, for exam-\nple, define qts = 1/ P\nxs∈Fs mts(xs) so that the messages always sum to 1. This is also\nuseful when applying belief propagation (see next section) to loopy networks, for\nwhich (14.10) may diverge while the normalized version converges.\n♦\nThe following summarizes this message passing algorithm.\nAlgorithm 14.4 (Belief propagation on acyclic graphs)\nGiven a family of interactions ϕs : Fs →[0,+∞), ϕst : Fs × Ft →[0,+∞),\n(1) Initialize functions (messages) mts : Fs →R, e.g., taking mts(x(s)) = 1/|Fs|.\n(2) Compute unnormalized messages\n˜mts(·) =\nX\nx(t)∈Ft\nϕst(·,x(t))ϕt(x(t))\nY\nt′∈Vt\\{s}\nmt′t(x(t))\nand let mts(·) = qts ˜mts(·), for some choice of constant qts, which must be a fixed func-\ntion of ˜mts(·), such as\nqts =\n\n\nX\nx(s)∈Fs\n˜mts(x(s))\n\n\n−1\n.\n(3) Stop the algorithm when the messages stabilize (which happens after a finite\nnumber of updates). Compute the edge marginal distributions using (14.13).\nIt should be clear, from the previous analysis that messages stabilize in finite time,\nstarting from the outskirts of the acyclic graph. Indeed, messages starting from a\nterminal t (a vertex with only one neighbor) are automatically set to their correct\nvalue in (14.10),\nmts(xs) =\nX\nxt∈Ft\nϕst(xs,xt)ϕt(xt),\nat the first update. These values then propagate to provide messages that satisfy\n(14.10) starting from the next-to-terminal vertexes (those that have only one neigh-\nbor left when the terminals are removed) and so on.\n\n14.3. BELIEF PROPAGATION AND FREE ENERGY APPROXIMATION\n341\n14.3\nBelief propagation and free energy approximation\n14.3.1\nBP stationarity\nIt is possible to run Algorithm 14.4 on graphs that are not acyclic, since nothing in\nits formulation requires this property. However, while the method stabilizes in finite\ntime for acyclic graphs, this property, or even the convergence of the messages is not\nguaranteed for general, loopy, graphs. Convergence, however, has been observed in\na large number of applications, sometimes with very good approximations of the\ntrue marginal distributions.\nWe will refer to stable solutions of Algorithm 14.4 as BP-stationary points, as\nformally stated in the next definition, which allows for a possible normalization of\nmessages, which is particularly important with loopy networks.\nDefinition 14.3 Let G = (V ,E) be an undirected graph and Φ = (ϕst,{s,t} ∈E,ϕs,s ∈\nV ) a consistent family of pair interactions. We say that a family of joint probability\ndistributions (π′\nst,{s,t} ∈E) is BP-stationary for (G,Φ) if there exists messages xt ∈Ft 7→\nmst(xt), constants ζst for t ∼s and αs for s ∈V satisfying\nmts(x(s)) = αs\nζts\nX\nx(t)∈Ft\nϕst(x(s),x(t))ϕt(x(t))\nY\nt′∈Vt\\{s}\nmt′t(x(t))\n(14.14)\nsuch that\nπ′\nst(x(s),x(t)) = 1\nζst\nϕst(x(s),x(t))ϕs(x(s))ϕt(x(t))\nY\nt′∈Vt\\{s}\nmt′t(x(t))\nY\ns′∈Vs\\{t}\nms′s(x(s)). (14.15)\nThere is no loss of generality in the specific form chosen for the normalizing con-\nstants in (14.14) and (14.15), in the sense that, if the messages satisfy (14.15) and\nmts(x(s)) = qts\nX\nx(t)∈Ft\nϕst(x(s),x(t))ϕt(x(t))\nY\nt′∈Vt\\{s}\nmt′t(x(t))\nfor some constants qts, then\nζst\n=\nX\nx(s)∈Fs,x(t)∈Ft\nϕst(x(s),x(t))ϕs(x(s))ϕt(x(t))\nY\nt′∈Vt\\{s}\nmt′t(x(t))\nY\ns′∈Vs\\{t}\nms′s(x(s))\n=\n1\nqts\nX\nx(s)∈Fs\nϕs(x(s))\nY\ns′∈Vs\nms′s(x(s))\nso that ζstqts (which has been denoted αs) does not depend on t. Of course, the rele-\nvant questions regarding BP-stationarity is whether the collection of pairwise prob-\nability π′\nst exists, how to compute them, and whether π′\nst(x(s),x(t)) provides a good\n\n342\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\napproximation of the marginals of the probability distribution π that is associated\nto Φ, namely\nπ(x) = 1\nZ\nY\ns∈V\nϕs(x(s))\nY\n{s,t}∈E\nϕst(x(s),x(t)).\nA reassuring statement for BP-stationarity is that it is not affected when the func-\ntions in Φ are multiplied by constants, which does not affect the underlying proba-\nbility π. This is stated in the next proposition.\nProposition 14.4 Let Φ be as above a family of edge and vertex interactions. Let cst,{s,t} ∈\nE, cs,s ∈V be families of positive constants, and define ˜Φ = ( ˜ϕst, ˜ϕs) by ˜ϕst = cstϕst and\n˜ϕs = csϕs. Then,\nπ′ is BP-stationary for (G,Φ) ⇔π′ is BP-stationary for (G, ˜Φ).\nProof Indeed, if (14.14) and (14.15) are true for (G,Φ), it suffices to replace αs by\nαscs and ζst by ζstcstct to obtain (14.14) and (14.15) for (G, ˜Φ).\n■\nIt is also important to notice that, if G is acyclic, definition 14.3 is no more general\nthan the message-passing rule we had considered earlier. More precisely, we have\n(see remark 14.2),\nProposition 14.5 Let G = (V ,E) be undirected acyclic and Φ = (ϕst,{s,t} ∈E,ϕs,s ∈V )\na consistent family of pair interactions. Then, the only BP-stationary distributions are the\nmarginals of the distribution π associated to Φ.\n14.3.2\nFree-energy approximations\nA partial justification of the good behavior of BP with general graphs has been pro-\nvided in terms of a quantity introduced in statistical mechanics, called the Bethe free\nenergy. We let G = (V ,E) be an undirected graph and assume that a consistent family\nof pair interactions is given (denoted Φ = (ϕs,s ∈V ,ϕst,{s,t} ∈E)) and consider the\nassociated distribution, π, on F (V ), given by\nπ(x) = 1\nZ\nY\ns∈V\nϕs(x(s))\nY\n{s,t}∈E\nϕst(x(s),x(t)).\n(14.16)\nIt will also be convenient to use the function\nψst(x(s),x(t)) = ϕs(x(s))ϕt(x(t))ϕst(x(s),x(t))\nsuch that\nπ(x) = 1\nZ\nY\ns∈V\nϕs(x(s))1−|Vs| Y\n{s,t}∈E\nψst(x(s),x(t)).\n(14.17)\n\n14.3. BELIEF PROPAGATION AND FREE ENERGY APPROXIMATION\n343\nWe will consider approximations π′ of π that minimize the Kullback-Leibler di-\nvergence, KL(π′∥π) (see (4.3)), subject to some constraints. We can write\nKL(π′∥π)\n=\n−Eπ′(logπ) −H(π′)\n=\n−logZ −\nX\ns∈V\n(1 −|Vs|)Eπ′(logϕs) −\nX\n{s,t}∈E\nEπ′(logψst) −H(π′)\n(where H(π′) is the entropy of π′). Introduce the one- and two-dimensional marginals\nof π′, denoted π′\ns ad π′\nst. Then\nKL(π′∥π) = −logZ −\nX\ns∈V\n(1 −|Vs|)Eπ′(log ϕs\nπ′s\n) −\nX\n{s,t}∈E\nEπ′(log ψst\nπ′\nst\n)\n+\nX\ns∈V\n(1 −|Vs|)H(π′\ns) +\nX\n{s,t}∈E\nH(π′\nst) −H(π′).\nThe Bethe free energy is the function Fβ defined by\nFβ(π′) = −\nX\ns∈V\n(1 −|Vs|)Eπ′(log ϕs\nπ′s\n) −\nX\n{s,t}∈E\nEπ′(log ψst\nπ′\nst\n);\n(14.18)\nso that\nKL(π′∥π) = Fβ(π′) −logZ + ∆G(π′)\nwith\n∆G(π′) =\nX\ns∈V\n(1 −|Vs|)H(π′\ns) +\nX\n{s,t}∈E\nH(π′\nst) −H(π′).\nUsing this computation, one can consider the approximation problem: find ˆπ′\nthat minimizes KL(π′∥π) over a class of distributions π′ for which the computation\nof the first and second order marginals is easy. This problem has an explicit solution\nwhen the distribution π′ is such that all variables are independent, leading to what\nis called the mean-field approximation of π. Indeed, in this case, we have\n∆G(π′) =\nX\n{s,t}∈G\n(H(π′\ns) + H(π′\nt)) +\nX\ns∈S\n(1 −|Vs|)H(π′\ns) −\nX\ns∈S\nH(π′\ns) = 0\nand\nFβ(π′) = −\nX\ns∈V\n(1 −|Vs|)Eπ′(log ϕs\nπ′s\n) −\nX\n{s,t}∈E\nEπ′(log ψst\nπ′sπ′\nt\n).\nFβ must be minimized with respect to the variables π′\ns(x(s)),s ∈S,xs ∈FS subject to\nthe constraints P\nxs∈Fs π′\ns(x(s)) = 1. The corresponding necessary optimality condi-\ntions equations provide the mean-field consistency equations, described in the fol-\nlowing definition.\n\n344\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nProposition 14.6 A local minimum of Fβ(π′) over all probability distributions π′ of the\nform\nπ′(x) =\nY\ns∈V\nπ′\ns(x(s))\nmust satisfy the mean field consistency equations:\nπs(x(s)) = 1\nZs\nϕs(x(s))1−|Vs| Y\nt∼s\nexp\n\u0010\nEπt(logψst(x(,).))\n\u0011\n.\n(14.19)\nProof Since all constraints are affine, we can use Lagrange multipliers, denoted\n(λs,s ∈S) for each of the constraints, to obtain necessary conditions for a minimizer,\nyielding\n∂Fβ\n∂πs(xs) −λs = 0, s ∈S,xs ∈Fs.\nThis gives:\n−(1 −|Vs|)\n \nlog ϕs(xs)\nπs(xs) −1\n!\n−\nX\nt∼s\nX\nxt∈Ft\n \nlog ψst(xs,xt)\nπs(xs)πt(xt) −1\n!\nπt(xt) = λs.\nSolving this with respect to πs(xs) and regrouping all constant terms (independent\nfrom xs) in the normalizing constant Zs yields (14.19).\n■\nThe mean field consistency equations can be solved using a root-finding algo-\nrithm or by directly solving the minimization problem. We will retrieve this method,\nwith more details, in our discussion of variational approximations in chapter 16.\nIn the particular case in which G is acyclic and the approximation is made by\nG-Markov processes, the Kullback-Leibler distance is minimized with π′ = π (since\nπ belongs to the approximating class). A slightly non-trivial remark is that π is\noptimal also for the minimization of the Bethe free energy Fβ, because this energy\ncoincides, up to the constant term logZ, with the Kullback-Leibler divergence, as\nproved by the following proposition.\nProposition 14.7 If G is acyclic and π′ is G-Markov, then ∆G(π′) = 0.\nThis proposition is a consequence of the following lemma that has its own interest:\nLemma 14.8 If G is acyclic and π is a G-Markov distribution, then\nπ(x) =\nY\ns∈V\nπs(x(s))1−|Vs| Y\n{s,t}∈E\nπst(x(s),x(t)).\n(14.20)\n\n14.3. BELIEF PROPAGATION AND FREE ENERGY APPROXIMATION\n345\nProof (of lemma 14.8) We know that, if ˜G = (V , ˜E) is a tree such that ˜G♭= G, we\nhave, letting s0 be the root in ˜G\nπ(x)\n=\nπs0(x(s0))\nY\n(s,t)∈˜E\npst(x(s),x(t))\n=\nπs0(x(s0))\nY\n(s,t)∈˜E\n(πst(x(s),x(t))π(x(s))−1).\nEach vertex s in V has |Vs|−1 children in ˜G, except s0 which has |Vs0| children. Using\nthis, we get\nπ(x)\n=\nπs0(x(s0))πs0(x(s0))−|Vs0|\nY\ns∈V \\{s0}\nπs(x(s))1−|Vs| Y\n(s,t)∈˜E\nπst(x(s),x(t))\n=\nY\ns∈V\nπs(x(s))1−|Vs| Y\n{s,t}∈E\nπst(x(s),x(t)).\nProof (of proposition 14.7) If π′ is given by (14.20), then\nH(π′)\n=\n−Eπ′ logπ′\n=\n−\nX\ns∈V\n(1 −|Vs|)Eπ′ logπ′\ns −\nX\n{s,t}∈E\nEπ′ logπ′\nst\n=\nX\ns∈V\n(1 −|Vs|)H(π′\ns) +\nX\n{s,t}∈E\nH(π′\nst)\nwhich proves that ∆G(π′) = 0.\n■\nIn view of this, it is tempting to “generalize” the mean field optimization proce-\ndure and minimize Fβ(π′) over all possible consistent singletons and pair marginals\n(π′\ns and π′\nst), then use the optimal ones as an approximation of πs and πst. What\nwe have just proved is that this procedure provides the exact expression of the\nmarginals when G is acyclic. For loopy graphs, however, it is not justified, and is\nat best an approximation. A very interesting fact is that this procedure provides the\nsame consistency equations as belief propagation. To see this, we first start with the\ncharacterization of minimizers of Fβ.\nProposition 14.9 Let G = (V ,E) be an undirected graph and π be given by (14.16).\nConsider the problem of minimizing the Bethe free energy Fβ in (14.18) with respect to all\npossible choices of probability distributions (π′\nst,{s,t} ∈E), (π′\ns,s ∈V ) with the constraints\nπ′\ns(x(s)) =\nX\nx(t)∈Ft\nπ′\nst(x(s),x(t)),∀x(s) ∈Fs and t ∼s.\n\n346\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nThen a local minimum of this problem must take the form\nπ′\nst(x(s),x(t)) = 1\nZst\nψst(x(s),x(t))µst(x(t))µts(x(s))\n(14.21)\nwhere the functions µst : Ft →[0,+∞) are defined for all (s,t) such that {s,t} ∈E and\nsatisfy the consistency conditions:\nµts(x(s))−(|Vs|−1) Y\ns′∼s\nµs′s(x(s)) =\n\n\ne\nZst\nX\nx(t)∈Ft\nψst(x(s),x(t))ϕt(x(t))µst(x(t))\n\n\n|Vs|−1\n.\n(14.22)\nProof We introduce Lagrange multipliers: λts(x(s)) for the constraint\nπ′\ns(x(s)) =\nX\nx(t)∈Ft\nπ′\nst(x(s),x(t))\nand γst for\nX\nx(s),x(t)\nπ′\nst(x(s),x(t)) = 1,\nwhich covers all constraints associated to the minimization problem. The associated\nLagrangian is\nFβ(π′) −\nX\ns∈V\nX\nx(s)∈Fs\nX\nt∼s\nλts(x(s))\n\n\nX\nx(t)∈Ft\nπ′\nst(x(s),x(t)) −π′\ns(x(s))\n\n\n−\nX\n{s,t}∈E\nγst\n\n\nX\nx(s)∈Fs,x(t)∈Ft\nπ′\nst(x(s),x(t)) −1\n\n.\nThe derivative with respect to π′\nst(x(s),x(t)) yields the condition\nlogπ′\nst(x(s),x(t)) −logψst(x(s),x(t)) + 1 −λts(x(s)) −λst(x(t)) −γst = 0.\nwhich implies\nπ′\nst(x(s),x(t)) = ϕst(x(s),x(t))exp(γst −1)exp(λts(x(s)) + λst(x(t))).\nWe let Zst = exp(1 −γst), with γst chosen so that π′\nst is a probability. The derivative\nwith respect to π′\ns(x(s)) gives\n(1 −|Vs|)(logπ′\ns(x(s)) −logϕs(x(s)) + 1) +\nX\nt∼s\nλts(x(s)) = 0.\n\n14.3. BELIEF PROPAGATION AND FREE ENERGY APPROXIMATION\n347\nCombining this with the expression just obtained for π′\nst, we get, for t ∼s,\n(1 −|Vs|)log\nX\nx(t)∈Ft\nψst(x(s),x(t))eλst(x(t)) + (1 −|Vs|)λts(x(s))\n+ (1 −|Vs|)(1 −logZst −logϕs(x(s))) +\nX\ns′∼s\nλs′s(x(s)) = 0,\nwhich gives (14.22) with µst = exp(λst).\n■\nA family π′\nst satisfying conditions (14.21) and (14.22) of proposition 14.9 will be\ncalled Bethe-consistent. A very interesting remark states that Bethe-consistency is\nequivalent to BP-stationarity, as stated below.\nProposition 14.10 Let G = (V ,E) be an undirected graph and Φ = (ϕst,{s,t} ∈E,ϕs,s ∈\nV ) a consistent family of pair interactions. Then a family π′ of joint probability distribu-\ntions is BP-stationary if and only if it is Bethe-consistent.\nProof First assume that π′ is BP-stationary with messages mst, so that (14.14) and (14.15)\nare satisfied. Take\nµst = at\nY\nt′∈Vt,t′,s\nmt′t(x(t))\nfor some constant at that will be determined later. Then, the left-hand side of (14.22)\nis\nµts(x(s))−(|Vs|−1) Y\ns′∈Vs\nµs′s(x(s)) = as\n\n\nY\ns′∈Vs,s′,t\nms′s(x(s))\n\n\n−(|Vs|−1) Y\ns′∈Vs\nY\ns′′∈Vs,s′′,s′\nms′′s(x(s))\n= asmts(x(s))|Vs|−1.\nThe right-hand side is equal to (using (14.14))\n eatζst\nZstαs\nmts(x(s))\n!|Vs|−1\n,\nso that we need to have\nas =\n eatζst\nZstαs\n!|Vs|−1\n.\nWe also need\nZst =\nX\nx(s),x(t)\nψst(x(s),x(t))µst(x(t))µts(x(s)) = asatζst.\nSolving these equations, we find that (14.21) and (14.22) are satisfied with\n\nas = (e/αs)(|Vs|−1)/|Vs|\nZst = ζstasat\n\n348\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nwhich proves that π′ is Bethe-consistent.\nConversely, take a Bethe-consistent π′, and µst,Zst satisfying (14.21) and (14.22).\nFor s such that |Vs| > 1, define, for t ∈Vs,\nmts(x(s)) = µts(x(s))−1 Y\ns′∼s\nµs′s(x(s))1/(|Vs|−1).\n(14.23)\nDefine also, for |Vs| > 1,\nρts(x(s)) =\nY\ns′∈Vs,s′,t\nms′s(x(s)).\n(If |Vs| = 1, take ρts ≡1.) Using (14.23), we find ρts = µts when |Vs| > 1, and this\nidentity is still valid when |Vs| = 1, since in this case, (14.22) implies that µts(x(s)) = 1.\nWe need to find constants αt and ζst such that (14.14) and (14.15) are satisfied.\nBut (14.15) implies\nζts =\nX\nxt,xs\nψst(x(s),x(t))ρst(x(t))ρts(x(s))\nand (14.21) implies ζts = Zts.\nWe now consider (14.14), which requires\nmts(x(s)) = αs\nζst\nX\nx(t)\nϕst(x(s),x(t))ϕt(x(t))ρst(x(t)).\nIt is now easy to see that this identity to the power |Vs| −1 coincides with (14.22) as\nsoon as one takes αs = e.\n■\n14.4\nComputing the most likely configuration\nWe now address the problem of finding a configuration that maximizes π(x) (mode\ndetermination). This problem turns out to be very similar to the computation of\nmarginals, that we have considered so far, and we will obtain similar algorithms.\nAssume that G is undirected and acyclic and that π can be written as\nπ(x) = 1\nZ\nY\n{s,t}∈E\nϕst(x(s),x(t))\nY\ns∈V\nϕs(x(s)).\nMaximizing π(x) is equivalent to maximizing\nU(x) =\nY\n{s,t}∈E\nϕst(x(s),x(t))\nY\ns∈V\nϕs(x(s)).\n(14.24)\n\n14.4. COMPUTING THE MOST LIKELY CONFIGURATION\n349\nAssume that a root has been chosen in G, with the resulting edge orientation yielding\na tree ˜G = (V , ˜E) such that ˜G♭= G. We partially order the vertexes according to ˜G,\nwriting s ≤t if there exists a path from s to t in ˜G (s is an ancestor of t). Let V +\ns\ncontain all t ∈V with t ≥s, and define\nUs(x(V +\ns )) =\nY\n{t,u}∈EV +\ns\nϕtu(x(t),x(u))\nY\nt>s\nϕt(x(t))\nand\nU∗\ns (x(s)) = max\nn\nUs(y(V +\ns )),y(s) = x(s)o\n.\n(14.25)\nSince we can write\nUs(x(V +\ns )) =\nY\nt∈s+\nϕst(x(s),x(t))ϕt(x(t))Ut(x(V +\nt )),\n(14.26)\nwe have\nU∗\ns (x(s))\n=\nmax\nx(t),t∈s+\n\n\nY\nt∈s+\nϕt(x(t))ϕst(x(s),x(t))U∗\nt (x(t))\n\n\n=\nY\nt∈s+\nmax\nxt∈Ft\n(ϕt(x(t))ϕst(x(s),x(t))U∗\nt (x(t))).\n(14.27)\nThis provides a method to compute U∗\ns (x(s)) for all s, starting with the leaves and\nprogressively updating the parents. (When s is a leaf, U∗\ns (x(s)) = 1, by definition.)\nOnce all U∗\ns (x(s)) have been computed, it is possible to obtain a configuration x∗\nthat maximizes π. This is because an optimal configuration must satisfy U∗\ns (x(s)\n∗) =\nUs(x(V +\ns )\n∗\n) for all s ∈V , i.e., x(V +\ns \\{s})\n∗\nmust solve the maximization problem in (14.25).\nBut because of (14.26), we can separate this problem over the children of s and obtain\nthe fact that, it t ∈s+,\nx(t)\n∗= argmax\nx(t)\n\u0012\nϕt(x(t))ϕst(x(s)\n∗,x(t))U∗\nt (x(t))\n\u0013\n.\nThis procedure can be rewritten in a slightly different form using messages sim-\nilar to the belief propagation algorithm. It s ∈t+, define\nµst(x(t)) = max\nxs∈Fs\n(ϕt(x(t))ϕts(x(t),x(s))U∗\ns (x(s)))\nand\nξst(x(t)) = argmax\nx(s)∈Fs\n(ϕt(x(t))ϕts(x(t),x(s))U∗\ns (x(s))).\n\n350\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nUsing section 14.4, we get\nµst(x(t))\n=\nmax\nx(s)∈Fs\n\nϕts(x(t),x(s))ϕs(x(s))\nY\nu∈s+\nµus(x(s))\n\n,\nξst(xt)\n=\nargmax\nx(s)∈Fs\n\nϕts(x(t),x(s))ϕs(x(s))\nY\nu∈s+\nµus(x(s))\n\n.\nAn optimal configuration can now be computed using x(t)\n∗= ξts(x(s)\n∗), with s ∈pa(t).\nThis resulting algorithm therefore first operates upwards on the tree (from leaves\nto root) to compute the µst’s and ξst’s, then downwards to compute x∗. This is sum-\nmarized in the following algorithm.\nAlgorithm 14.5\nA most likely configuration for\nπ(x) = 1\nZ\nY\n{s,t}∈E\nϕst(xs,xt)\nY\ns∈V\nϕs(xs).\ncan be computed after iterating the following updates, based on any acyclic orienta-\ntion of G:\n(1) Compute, from leaves to root:\nµst(x(t)) = max\nx(s)∈Fs\n\nϕts(x(t),x(s))ϕs(x(s))\nY\nu∈s+\nµus(x(s))\n\n\nand ξst(x(t)) = argmax\nx(s)∈Fs\n\nϕts(x(t),x(s))ϕs(x(s))\nY\nu∈s+\nµus(x(s))\n\n.\n(2) Compute, from root to leaves: x(t)\n∗= ξts(x(s)\n∗), with s = pa(t).\nSimilar to the computation of marginals, this algorithm can be rewritten in an\norientation-independent form. The main remark is that the value of µst(x(t)) does\nnot depend on the tree orientation, as long as it is chosen such that s ∈t+, i.e., the\nedge {s,t} is oriented from t to s. This is because such a choice uniquely prescribes\nthe orientation of the edges of the descendants of s for any such tree, and µst only\ndepends on this structure. Since the same remark holds for ξst, this provides a def-\ninition of these two quantities for any pair s,t such that {s,t} ∈E. The updating rule\n\n14.5. GENERAL SUM-PROD AND MAX-PROD ALGORITHMS\n351\nnow becomes\nµst(x(t))\n=\nmax\nx(s)∈Fs\n\nϕts(x(t),x(s))ϕs(x(s))\nY\nu∈Vs\\{t}\nµus(x(s))\n\n,\n(14.28)\nξst(x(t))\n=\nargmax\nx(s)∈Fs\n\nϕts(x(t),x(s))ϕs(x(s))\nY\nu∈Vs\\{t}\nµus(x(s))\n\n\n(14.29)\nwith x(t)\n∗\n= ξts(x(s)\n∗) for any pair s ∼t. Like with the mts in the previous section,\nlooping over updating all µts in any order will finally stabilize to their correct values,\nalthough, if an orientation is given, going from leaves to roots is obviously more\nefficient.\nThe previous analysis is not valid for loopy graphs but section 14.4 and sec-\ntion 14.4 provide well defined iterations when G is an arbitrary undirected graph,\nand can therefore be used as such, without any guaranteed behavior.\n14.5\nGeneral sum-prod and max-prod algorithms\n14.5.1\nFactor graphs\nThe expressions we obtained for message updating with belief propagation and with\nmode determination respectively took the form\nmts(x(s)) ←\nX\nx(t)∈Ft\nϕst(x(s),x(t))ϕt(x(t))\nY\nt′∈Vt\\{s}\nmt′t(x(t))\nand\nµts(x(s)) ←max\nx(t)∈Ft\n\nϕst(x(s),x(t))ϕt(x(t))\nY\nt′∈Vt\\{s}\nµt′t(x(t))\n\n.\nThey first one is often referred to as the “sum-prod” update rule, and the second\nas the “max-prod”. In our construction, the sum-prod algorithm provided us with a\nmethod computing\nσs(x(s)) =\nX\ny(V \\{s})\nU(x(s) ∧y(V \\{s}))\nwith\nU(x) =\nY\ns\nϕs(x(s))\nY\n{s,t}∈E\nϕst(x(s),x(t)).\n\n352\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nIndeed, we have, according to (14.11)\nσs(x(s)) = ϕs(x(s))\nY\nt∈Vs\nmts(x(s)).\nSimilarly, the max-prod algorithm computes\nρs(x(s)) = max\nyV \\{s} U(x(s) ∧y(V \\{s}))\nvia the relation\nρs(x(s)) = ϕs(x(s))\nY\nt∈Vs\nµts(x(s)).\nWe now discuss generalizations of these algorithms to situations in which the\nfunction U does not decompose as a product of bivariate functions. More precisely,\nlet S be a subset of P(V ), and assume the decomposition\nU(x) =\nY\nC⊂S\nϕC(xC).\nThe previous algorithms can be generalized using the concept of factor graphs asso-\nciated with the decomposition. The vertexes of this graph are either indexes s ∈V or\nsets C ∈S, and the only edges link indexes and sets that contain them. The formal\ndefinition is as follows.\nDefinition 14.11 Let V be a finite set of indexes and S a subset of P(V ). The factor\ngraph associated to V and S is the graph G = (V ∪S,E), E being constituted of all pairs\n{s,C} with C ∈S and s ∈C.\nWe assign the variable x(s) to a vertex s ∈V of the factor graph, and the function ϕC\nto C ∈S. With this in mind, the sum-prod and max-prod algorithms are extended\nto factor graphs as follows.\nDefinition 14.12 Let G = (V ∪S,E) be a factor graph, with associated functions ϕC(xC).\nThe sum-prod algorithm on G updates messages msC(xs) and mCs(xs) according to the\nrules\n\nmsC(x(s)) ←\nY\n˜C,s∈˜C, ˜C,C\nm ˜Cs(x(s))\nmCs(x(s)) ←\nX\nyC:y(s)=x(s)\nϕC(y(C))\nY\nt∈C\\{s}\nmtC(y(t))\n(14.30)\n\n14.5. GENERAL SUM-PROD AND MAX-PROD ALGORITHMS\n353\nSimilarly, the max-prod algorithm iterates\n\nµsC(x(s)) ←\nY\n˜C,s∈˜C, ˜C,C\nµ ˜Cs(x(s))\nµCs(x(s)) ←\nmax\ny(C):y(s)=x(s) ϕC(y(C))\nY\nt∈C\\{s}\nµtC(y(t))\n(14.31)\nThese algorithms reduce to the original ones when only single vertex and pair in-\nteractions exist. Let us check this with sum-prod. In this case, the set S contains\nall singletons C = {s}, with associated function ϕs, and all edges {s,t} with associated\nfunction ϕst. We have links between s and {s} and s and {s,t} ∈E. For singletons, we\nhave\nms{s}(x(s)) ←\nY\nt∼s\nms{s,t}(x(s)) and m{s}s(x(s)) ←ϕs(x(s)).\nFor pairs,\nms{s,t}(x(s)) ←ϕs(x(s))\nY\n˜t∈Vs\\{t}\nm{s,˜t}s(x(s))\nand\nm{s,t}s(x(s)) ←\nX\ny(t)\nϕst(x(s),y(t))mt{s,t}(y(t))\nand, combining the last two assignments, it becomes clear that we retrieve the initial\nalgorithm with m{s,t}s taking the role of what we previously denoted mts.\nThe important question, obviously, is whether the algorithms converge. The fol-\nlowing result shows that this is true when the factor graph is acyclic.\nProposition 14.13 Let G = (V ∪S,E) be a factor graph with associated functions ϕC.\nAssume that G is acyclic. Then the sum-prod and max-prod algorithms converge in finite\ntime.\nAfter convergence, we have σs(x(s)) = Q\nC,s∈C mCs(x(s)) and ρs(x(s)) = Q\nC,s∈C µCs(x(s)).\nProof Let us assume that G is connected, which is without loss of generality, since\nthe following argument can be applied to each component of G separately. Since G\nis acyclic, we can arbitrarily select one of its vertexes as a root to form a tree. This\nbeing done, we can see that the messages going upward in the tree (from children\nto parent) progressively stabilize, starting with leaves. Leaves in the factor graph\nindeed are either singletons, C = {s}, or vertexes s ∈V that belong to only one set\nC ∈S. In the first case, the algorithm imposes (taking, for example, the sum-prod\ncase) m{s}s(x(s)) = ϕs(x(s)), and in the second case msC(x(s)) = 1. So the messages sent\nupward by the leaves are set at the first step. Since the messages going from a child\nto its parents only depend on the messages that it received from its other neighbors\n\n354\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nin the acyclic graph, which are its children in the tree, it is clear that all upward\nmessages progressively stabilize until the root is reached. Once this is done, mes-\nsages propagate downward from each parent to its children. This stabilizes as soon\nas all incoming messages to the parent are stabilized, since outgoing messages only\ndepend on those. At the end of the upward phase, this is true for the root, which\ncan then send its stable message to its children. These children now have all their\nincoming messages and can now send their messages to their own children and so\non down to the leaves.\nWe now consider the second statement, proceeding by induction, assuming that\nthe result is true for any smaller graph than the one considered. Let s0 be the selected\nroot, and consider all vertexes s , s0 such that there exists Cs ∈S such that s0 and s\nboth belong to Cs. Given s, there cannot be more than one such Cs since this would\ncreate a loop in the graph. For each such s, consider the part Gs of G containing all\ndescendants of s. Let Vs be the set of vertexes among the descendants of s and Cs the\nset of C’s below s. Define\nUs(x(Vs)) =\nY\nC∈Cs\nϕC(x(C)).\nSince the upward phase of the algorithm does not depend on the ancestors of s,\nthe messages incoming to s for the sum-prod algorithm restricted to Gs are the same\nas with the general algorithm, so that, using the induction hypothesis\nX\ny(Vs),y(s)=x(s)\nUs(y(Vs)) =\nY\nC∈Cs,s∈C\nmCs(x(s)) = msCs(x(s)).\nNow let C1,...,Cn list all the sets in C that contain s0, which must be non-intersecting\n(excepted at {s0}), again not to create loops. Write\nC1 ∪··· ∪Cn = {s0,s1,...,sq}.\nThen, we have\nU(x) =\nn\nY\nj=1\nϕCj(x(Cj))\nq\nY\ni=1\nUsi(x(Vsi ))\n\n14.5. GENERAL SUM-PROD AND MAX-PROD ALGORITHMS\n355\nand letting S = Sn\nj=1 Cj \\ {s0},\nσs0(x(s0))\n=\nX\ny(V ):y(s0)=x(s0)\nn\nY\nj=1\nϕCj(y(Cj))\nq\nY\ni=1\nUsi(y(Vsi ))\n=\nX\ny()S:y(s0)=x(s0)\nn\nY\nj=1\nϕCj(y(Cj))\nq\nY\ni=1\nmsiCsi (y(si))\n=\nn\nY\nj=1\nX\ny()Cj:y(s0)=x(s0)\nϕCj(y(Cj))\nY\ns∈Cj\\{s0}\nmsCs(y(s))\n=\nn\nY\nj=1\nmCjs0(x(s0))\nwhich proves the required result (note that, when factorizing the sum, we have used\nthe fact that the sets Cj \\ {s0} are non intersecting). An almost identical argument\nholds for the max-prod algorithm.\n■\nRemark 14.14 Note that these algorithms are not always feasible. For example, it is\nalways possible to represent a function U on F (V ) with the trivial factor graph in\nwhich S = {V } and E contains all {s,V },s ∈V (using ϕV = U), but computing mV s\nis identical to directly computing σs with a sum over all configurations on V \\ {s}\nwhich grows exponentially. In fact, the complexity of the sum-prod and max-prod\nalgorithms is exponential in the size of the largest C in S which should therefore\nremain small.\n♦\nRemark 14.15 It is not always possible to decompose a function so that the resulting\nfactor graph is acyclic with small degree (maximum number of edges per vertex).\nSum-prod and max-prod can still be used with loopy networks, sometimes with\nexcellent results, but without theoretical support.\n♦\nRemark 14.16 One can sometimes transform a given factor graph into an acyclic\none by grouping vertexes. Assume that the set S ⊂P(V ) is given. We will say that\na partition ∆= (D1,...Dk) of V is S-admissible if, for any C ∈S and any j ∈{1,...,k},\none has either Dj ∩C = ∅or Dj ⊂C.\nIf ∆is S-admissible, one can define a new factor graph ˜G as follows. We first let\n˜V = {1,...,k}. To define ˜S ⊂P( ˜V ) assign to each C ∈S the set JC of indexes j such\nthat Dj ⊂C. From the admissibility assumption,\nC =\n[\nj∈JC\nDj,\n(14.32)\n\n356\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nso that C 7→JC is one-to-one. Let ˜S = {JC,C ∈S}. Group variables using ˜x(k) = x(Dk),\nso that ˜Fk = F (Dk). Define ˜Φ = ( ˜ϕ ˜C, ˜C ∈˜S) by ˜ϕ ˜C = ϕC where C is given by (14.32).\nIn other terms, one groups variables (x(s),s ∈V ) into clusters, to create a simpler\nfactor graph, which may be acyclic even if the original one was not. For example, if\nV = {a,b,c,d}, S = {A,B} with A = {a,b,c} and B = {b,c,d}, then (A,c,B,b) is a cycle in\nthe associated factor graph. If, however, one takes D1 = {a}, D2 = {b,c} and D3 = {d},\nthen (D1,D2,D3) is S-admissible and the associated factor graph is acyclic. In fact, in\nsuch a case, the resulting factor graph, considered as a graph with vertexes given by\nsubsets of V , is a special case of a junction tree, which is defined in the next section.♦\n14.5.2\nJunction trees\nDefinition 14.17 Let V be a finite set. A junction tree on V is an undirected acyclic\ngraph G = (S,E) where S ⊂P(V ) is a family of subsets of V that satisfy the following\nproperty, called the running intersection constraint: if C,C′ ∈S and s ∈C ∩C′, then all\nsets C′′ in the (unique) path connecting C and C′ in G must also contain s.\nRemark 14.18 Let us check that the clustered factor graph ˜G defined in remark 14.16\nis equivalent to a junction tree when acyclic.\nUsing the same notation, let ˆS =\n{D1,...,Dk} ∪S, removing if needed sets C ∈S that coincide with one of the Dj’s.\nPlace an edge between Dj and C if and only if Dj ⊂C.\nLet (C1,Di1,...,Din−1,Cn) be a path in that graph. Assume that s ∈C1 ∩C2. Let Din\nbe the unique Dj that contains s. It is such that from the the admissibility assump-\ntion, Din ⊂C1 and Din ⊂Cn, which implies that (C1,Di1,...,Cn,Din,C1) is a path in ˜G.\nSince ˜G is acyclic, this path must be a union of folded paths. But it is easy to see that\nany folded path satisfies the running intersection constraint. (Note that there was\nno loss of generality in assuming that the path started and ended with a “C”, since\nany “D” must be contained in the C that follows or precedes it.)\n♦\nWe now consider a probability distribution written in the form\nπ(x) = 1\nZ\nY\nC∈S\nϕC(x(C))\nand we make the assumption that S can be organized as a junction tree.\nBelief propagation can be extended to junction trees. Fixing a root C0 ∈S, we\nfirst choose an orientation on G, which induces as usual a partial order on S. For\nC ∈S, define S+\nC as the set of all B ∈S such that B > C. Define also\nV +\nC =\n[\nB∈S+\nC\nB.\n\n14.5. GENERAL SUM-PROD AND MAX-PROD ALGORITHMS\n357\nWe want to compute sums\nσC(x(C)) =\nX\ny(V \\C)\nU(x(C) ∧y(V \\C)),\nwhere U(x) = Q\nC∈S ϕC(x(C)). We have\nσC(x(C)) =\nX\ny(V \\C)\nϕC(x(C))\nY\nB∈S\\{C}\nϕB(x(B∩C) ∧y(B\\C)).\nDefine\nσ+\nC(x(C)) =\nX\ny(V +\nC \\C)\nY\nB>C\nϕB(x(B∩C) ∧y(B\\C)).\nNote that we have σC0 = ϕC0σ+\nC0 at the root. We have the recursion formula\nσ+\nC(x(C))\n=\nX\ny(V +\nC \\C)\nY\nC→B\n\nϕB(x(B∩C) ∧y(B\\C))\nY\nB′>B\nϕB′(x(B′∩C) ∧y(B′\\C))\n\n\n=\nY\nC→B\nX\ny(B∪V +\nB \\C)\nϕB(x(B∩C) ∧y(B\\C))\nY\nB′>B\nϕB′(x(B′∩C) ∧y(B′\\C))\n=\nY\nC→B\nX\ny(B\\C)\nϕB(x(B∩C) ∧y(B\\C))σ+\nB (x(B∩C) ∧y(B\\C)).\nThe inversion between the sum and product in the second equation above was pos-\nsible because the sets B ∪V +\nB \\ C, C →B are disjoint. Indeed, if there existed B,B′\nsuch that C →B and C →B′, and descendants C′ of B′ and C′′ of B′′ with a non-\nempty intersection, then this intersection would have to be included in every set in\nthe (non-oriented) path connecting C′ and C′′ in G. Since this path contains C, the\nintersection must also be included in C, so that the sets B ∪V +\nB \\ C, with C →B are\ndisjoint.\nIntroduce messages\nm+\nB(x(C)) =\nX\ny(B\\C)\nϕB(x(B∩C) ∧y(B\\C))σ+\nB (x(B∩C) ∧y(B\\C))\nwhere C is the parent of B. Then\nm+\nB(x(C)) =\nX\ny(B\\C)\nϕB(x(B∩C) ∧y(B\\C))\nY\nB→B′\nm+\nB′(x(B∩C) ∧y(B\\C))\nwith\nσ+\nC(x(C)) =\nY\nC→B\nm+\nB(x(C))\n\n358\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nwhich provides σC at the root. Reinterpreting this discussion in terms of the undi-\nrected graph, we are led to introducing messages mBC(x(C)) for B ∼C in G, with the\nmessage-passing rule\nmBC(x(C)) =\nX\ny(B\\C)\nϕB(x(B∩C) ∧y(B\\C))\nY\nB′∼B,B′,C\nmB′B(x(B∩C) ∧y(B\\C)).\n(14.33)\nMessages progressively stabilize when applied in G, and at convergence, we have\nσC(x(C)) = ϕC(x(C))\nY\nB∼C\nmBC(x(C)).\n(14.34)\nNote that the complexity of the junction tree algorithm is exponential in the car-\ndinality of the largest C ∈S. This algorithm will therefore be unfeasible if S contains\nsets that are too large.\n14.6\nBuilding junction trees\nThere is more than one family of set interactions with respect to which a given prob-\nability π can be decomposed (notice that, unlike in the Hammersley-Clifford The-\norem, we do not assume that the interactions are normalized), and not all of them\ncan be organized as a junction tree. One can however extend any given family into a\nnew one on which one can build a junction tree.\nDefinition 14.19 Let V be a set of vertexes, and S0 ⊂P(V ). We say that a set S ⊂P(V )\nis an extension of S0 if, for any C0 ∈S0, there exists a C ∈S such that C0 ⊂C.\nA tree G = (S,E) is a junction-tree extension of S0 if S is an extension of S0 and G is\na junction tree.\nIf Φ0 = (ϕ0\nC,C ∈S0) is a consistent family of set interactions, and S is an extension\nof S0, one can build a new family, Φ = (ϕC,C ∈S), of set interactions which yields\nthe same probability distribution, i.e., such that, for all x ∈F (V ),\nY\nC∈S\nϕC(x(C)) ∝\nY\nC0∈S0\nϕ0\nC0(x(C0)).\nFor this, it suffices to build a mapping say T : S0 →S such that C0 ⊂T(C0) for\nall C0 ∈S0, which is always possible since S is an extension of S0 (for example,\narbitrarily order the elements of S and let T(S0) be the first element of S, according\nto this order, that contains C0). One can then define\nϕC(x(C)) =\nY\nC0:T (C0)=C\nϕ0\nC0(x(C0)).\n\n14.6. BUILDING JUNCTION TREES\n359\nGiven Φ0, our goal is to design a junction-tree extension which is as feasible\nas possible. So, we are not interested by the trivial extension G = (V ,∅), since the\nresulting junction-tree algorithm is unfeasible as soon as V is large. Theorem 14.24\nin the next section will be the first step in the design of an algorithm that computes\njunction trees on a given graph.\n14.6.1\nTriangulated graphs\nDefinition 14.20 Let G = (V ,E) be an undirected graph. Let (s1,s2,...,sn) be a path in\nG. One says that this path has a chord at sj, with j ∈{2,...,n} , if sj−1 ∼sj+1, and we will\nrefer to (sj−1,sj,sj+1) as a chordal triangle. A path in G is achordal if it has no chord.\nOne says that G is triangulated (or chordal) if it has no achordal loop.\nDefinition 14.21 The graph G is decomposable if it satisfies the following recursive con-\ndition: it is either complete, or there exists disjoint subsets (A,B,C) of V such that\n• V = A ∪B ∪C,\n• A and B are not empty,\n• C is clique in G, C separates A and B,\n• the restricted graphs, GA∪C and GB∪C are decomposable.\nThese definitions are in fact equivalent, as stated in the following proposition.\nProposition 14.22 An undirected graph is triangulated if and only if it is decomposable\nProof To prove the “if” part, we proceed by induction on n = |V |. Note that every\ngraph for n ≤3 is both decomposable and triangulated (we leave the verification\nto the reader). Assume that the statement “decomposable ⇒triangulated” holds\nfor graphs with less than n vertexes, and take G with n vertexes. Assume that G is\ndecomposable. If it is complete, it is obviously triangulated. Otherwise, there exists\nA,B,C such that V = A ∪B ∪C, with A and B non-empty such that GA∪C and GB∪C\nare decomposable, hence triangulated from the induction hypothesis, and such that\nC is a clique which separates A and B. Assume that γ is an achordal loop in G. Since\nit cannot be included in A ∪C or B ∪C, γ must go from A to B and back, which\nimplies that it passes at least twice in C. Since C is complete, the original loop can\nbe shortcut to form subloops in A ∪C and B ∪C. If one of (or both) these loops has\ncardinality 3, this would provide γ with a chord, which contradicts the assumption.\nOtherwise, the following lemma also provides a contradiction, since one of the two\nchords that it implies must also be a chord in the original γ.\nLemma 14.23 Let (s1,...,sn,sn+1 = s1) be a loop in a triangulated graph, with n ≥4.\nThen the path has a chord at two non-contiguous vertexes at least.\n\n360\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nTo prove the lemma, assume the contrary and let (s1,...,sn,sn+1 = s1) be a loop that\ndoes not satisfy the condition, with n as small as possible. If n > 4, the loop must\nhave a chord, say at sj, and one can remove sj from the loop to still obtain a smaller\nloop that must satisfy the condition in the lemma, since n was as small as possible.\nOne of the two chords must be at a vertex other than the two neighbors of sj, and\nthus provide a second chord in the original loop, which is a contradiction. Thus\nn = 4, but G being triangulated implies that this 4-point loop has a diagonal, so that\nthe condition in the lemma also holds, which provides a contradiction.\nFor the “only if” part of proposition 14.22, assume that G is triangulated. We\nprove that the graph is decomposable by induction on |G|. The induction will work if\nwe can show that, if G is triangulated, it is either complete or there exists a clique in\nG such that V \\C is disconnected, i.e., there exist two elements a,b ∈V \\C which are\nrelated by no path in V \\C. Indeed, we will then be able to decompose V = A∪B∪C,\nwhere A and B are unions of (distinct) connected components of V \\ C. Take, for\nexample, A to be the set of vertexes connected to A in G \\ C, and B = V \\ (A ∪C),\nwhich is not empty since it contains b. Note that restricted graphs from triangulated\ngraphs are triangulated too.\nSo, assume that G is triangulated, and not complete. Let C be a subset of V that\nsatisfies the property that V \\ C is disconnected, and take C minimal, so that V \\ C′\nis connected for any C′ ⊂C, C′ , C. We want to show that C is a clique, so take s and\nt in C and assume that they are not neighbors to reach a contradiction.\nLet A and B be two connected components of V \\ C. For any a ∈A, b ∈B, and\ns,t ∈C, we know that there exists a path between a and b in V \\ C ∪{s} and another\none in V \\C∪{t}, the first one passing by s (because it would otherwise connect a and\nb in V \\ C) and the second one passing by t. Any point before s (or t) in these paths\nmust belong to A, and any point after them must belong to B. Concatenating these\ntwo paths, and removing multiple points if needed, we obtain a loop passing in A,\nthen by s, then in B, then by t. We can recursively remove all points at which these\npaths have a chord. We can also notice that we cannot remove s nor t in this process,\nsince this would imply an edge between A and B, and that we must leave at least one\nelement in A and one in B because removing the last one would require s ∼t. So, at\nthe end, we obtain an achordal loop with at least four points, which contradicts the\nfact that G is triangulated.\n■\nWe can now characterize graphs that admit junction trees over the set of their\nmaximal cliques.\nTheorem 14.24 Let G = (V ,E) be an undirected graph, and C∗\nG be the set of all maximum\ncliques in G. The following two properties are equivalent.\n(i) There exists a junction tree over C∗\nG.\n\n14.6. BUILDING JUNCTION TREES\n361\n(ii) G is triangulated/decomposable.\nProof The proof works by induction on the number of maximal cliques, |C∗\nG|. If G\nhas only one maximal clique, then G is complete, because any point not included\nin this clique will have to be included in another maximal clique, which leads to a\ncontradiction. So G is decomposable, and, since any single node obviously provides\na junction tree, (i) is true also.\nNow, fix G and assume that the theorem is true for any graph with fewer maximal\ncliques. First assume that C∗\nG has a junction tree, T . Let C1 be a leaf in T , connected,\nsay, to C2, and let T2 be T restricted to C2 = C∗\nG\\{C1}. Let V2 be the unions of maximal\ncliques from nodes in T2. A maximal clique C in GV2 is a clique in GV and therefore\nincluded in some maximal clique C′ ∈CV . If C′ ∈C2, then C′ is also a clique in GV2,\nand for C to be maximal, we need C = C′. If C′ = C1, we note that we must also have\nC =\n[\n˜C∈C2\nC ∩˜C\nand whenever C∩˜C is not empty, this set must be included in any node in the path in\nT that links ˜C to C1. Since this path contains C2, we have C ∩˜C ⊂C2 so that C ⊂C2,\nbut, since C is maximal, this would imply that C = C2 = C1 which is impossible.\nThis shows that C∗\nG2 = C2. This also shows that T2 is a junction tree over C2. So,\nby the induction hypothesis, GV2 is decomposable. If s ∈V2 ∩C1, then s also belongs\nto some clique C′ ∈C2, and therefore belongs to any clique in the path between\nC′ and C1, which includes C2. So s ∈C1 ∩C2 and C1 ∩V2 = C1 ∩C2. So, letting\nA = C1 \\ (C1 ∩C2), B = V1 \\ (C1 ∩C2), S = C1 ∩C2, we know that GA∪S and GB∪S are\ndecomposable (the first one being complete), and that S is a clique. To show that G\nis decomposable, it remains to show that S separates A from B.\nIf a path connects A to B in G, it must contain an edge, say {s,t}, with s ∈V \\S and\nt ∈S; {s,t} must be included in a maximal clique in G. If this clique is C1, we have\ns ∈C1 ∩V2 = S. The same argument shows that this is the only possibility, because,\nif {s,t} is included in some maximal clique in C2, then we would find t ∈C1 ∩C2. So\nS separates A and B in G.\nLet us now prove the converse statement, and assume that G is decomposable.\nIf G is complete, it has only one maximal clique and we are done. Otherwise, there\nexists a partition V = A ∪B ∪S such that GA∪S and GB∪S are decomposable, A and B\nseparated by S which is complete. Let C∗\nA be the maximal cliques in GA∪S and C∗\nB the\nmaximal cliques in GB∪S. By hypothesis, there exist junction trees TA and TB over C∗\nA\nand C∗\nB.\nLet C be a maximal clique in GA∪S. Assume that C intersect A; C can be extended\nto a maximal clique, C′, in G, but C′ cannot intersect B (since this would imply a\n\n362\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\ndirect edge between A and B) and is therefore included in A ∪S, so that C = C′.\nSimilarly, all maximal cliques in GB∪S that intersect B also are maximal cliques in G.\nThe clique S is included in some maximal clique S∗\nA ∈C∗\nA. From the previous\ndiscussion, we have either S∗\nA = S or S∗\nA ∈C∗\nG. Similarly, S can be extended to a\nmaximal clique S∗\nB ∈C∗\nB, with S∗\nB = S or S∗\nB ∈C∗\nG. Notice also that at least one of S∗\nA\nor S∗\nB must be a maximal clique in G: indeed, assume that both sets are equal to S,\nwhich, as a clique, can extended to a maximal clique S∗in G; S∗must be included\neither in A ∪S or in B ∪S, and therefore be a maximal clique in the corresponding\ngraph which yields S∗= S. Reversing the notation if needed, we will assume that\nS∗\nA ∈C∗\nG.\nAll elements of C∗\nG must belong either to C∗\nA or C∗\nB since any maximal clique, say C,\nin G must be included in either A ∪S or B ∪S, and therefore also provide a maximal\nclique in the related graph. So the nodes in TA and TB enumerate all maximal cliques\nin G, and we can build a tree T over C∗\nG by identifying S∗\nA and S∗\nB to S∗and merging\nthe two trees at this node. To conclude our proof, it only remains to show that the\nrunning intersection property is satisfied. So consider two nodes C,C′ in T and\ntake s ∈C ∩C′. If the path between these nodes remain in C∗\nA, or in C∗\nB, then s will\nbelong to any set along that path, since the running intersection is true on TA and\nTB. Otherwise, we must have s ∈S, and the path must contain S∗to switch trees,\nand s must still belong to any clique in the path (applying the running intersection\nproperty between the beginning of the path and S∗, and between S∗and the end of\nthe path).\n■\nThis theorem delineates a strategy in order to build a junction tree that is adapted\nto a given family of local interactions Φ = (ϕC,C ∈C). Letting G be the graph in-\nduced by these interactions, i.e., s ∼G t if and only if there exists C ∈C such that\n{s,t} ⊂C, the method proceeds as follows.\n(JT1) Extend G by adding edges to obtain a triangulated graph G∗.\n(JT2) Compute the set C∗of maximal cliques in G∗, which therefore extend C.\n(JT3) Build a junction tree over C∗.\n(JT4) Assign interaction ϕC to a clique C∗∈C∗such that C ⊂C∗.\n(JT5) Run the junction-tree belief propagation algorithm to compute the marginal\nof π (associated to Φ) over each set C∗∈C∗.\nSteps (JT4) and (JT5) have already been discussed, and we now explain how the first\nthree steps can be implemented.\n\n14.6. BUILDING JUNCTION TREES\n363\n14.6.2\nBuilding triangulated graphs\nFirst consider step (JT1). To triangulate a graph G = (V ,E), it suffices to order its\nvertexes so that V = {s1,...,sn}, and then run the following algorithm.\nAlgorithm 14.6 (Graph triangulation)\nInitialize the algorithm with k = n and Ek = E. Given Ek, determine Ek−1 as follows:\n• Add an edge to any pair of neighbors of sk (unless, of course, they are already\nlinked).\n• Let Ek−1 be the new set of edges.\nThen the graph G∗= (V ,E0) is triangulated. Indeed taking any achordal loop,\nand selecting the vertex with highest index in the loop, say sk, brings a contradiction,\nsince the neighbors of sk have been linked when building Ek−1.\nHowever, the quality of the triangulation, which can be measured by the number\nof added edges, or by the size of the maximal cliques, highly depends on the way ver-\ntexes have been numbered. Take the simple example of the linear graph with three\nvertexes A ∼B ∼C. If the point of highest index is B, then the previous algorithm\nwill return the three-point loop A ∼B ∼C ∼A. Any other ordering will leave the\nlinear graph, which is already triangulated, invariant.\nSo, one must be careful about the order with which nodes will be processed.\nFinding an optimal ordering for a given global cost is an NP-complete problem.\nHowever, a very simple modification of the previous algorithm, which starts with\nsn having the minimal number of neighbors, and at each step defines sk to be the\none with fewest neighbors that haven’t been visited yet, provides an efficient way\nfor building triangulations. (It has the merit of leaving G invariant if it is a tree,\nfor example). Another criterion may be preferred to the number of neighbors (for\nexample, the number of new edges that would be needed if s is added).\nIf G is triangulated, there exists an ordering of V such that the algorithm above\nleaves G invariant. We now proceed to a proof of this statement and also show that\nsuch an ordering can be computed using an algorithm called maximum cardinality\nsearch, which, in addition, allows one to decide whether a graph is triangulated. We\nstart with a definition that formalizes the sequence of operations in the triangulation\nalgorithm.\nDefinition 14.25 Let G = (V ,E) be an undirected graph. A node elimination consists in\nselecting a vertex s ∈V and building the graph G(s) = (V (s),E(s)) with V (s) = V \\ {s}, and\nE(s) containing all pairs {t,t′} ⊂V (s) such that either {t,t′} ∈E or {t,t′} ⊂Vs.\n\n364\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nG(s) is called the s-elimination graph of G. The set of added edges, namely E(s)\\(E∩E(s))\nis called the deficiency set of s and denoted D(s) (or DG(s)).\nSo, the triangulation algorithm implements a sequence of node eliminations, suc-\ncessively applied to sn,sn−1, etc. One says that such an elimination process is perfect\nif, for all k = 1,...,n, the deficiency set of sk in the graph obtained after elimination\nof sn,...,sk+1 is empty (so that no edge is added during the process). We will also say\nthat (s1,...,sn) provides a perfect ordering for G.\nTheorem 14.26 An undirected graph G = (V ,E) admits a perfect ordering if and only if\nit is triangulated.\nProof The “only if” part is obvious, since, the triangulation algorithm following\na perfect ordering does not add any edge to G, which must therefore have been\ntriangulated to start with.\nWe now proceed to the “if” part. For this it suffices to prove that for any trian-\ngulated graph, there exists a vertex s such that DG(s) = ∅. One can then easily prove\nthe result by induction, since, after removing this s, the remaining graph G(s) is still\ntriangulated and would admit (by induction) a perfect ordering that completes this\nfirst step.\nTo prove that such an s exists, we take a decomposition V = A ∪S ∪B, in which\nS is complete and separates A and B, such that |A ∪S| is minimal (or |B| maximal).\nWe claim that A ∪S must be complete. Otherwise, since A ∪S is still triangulated,\nThere exists a similar decomposition A ∪S = A′ ∪S′ ∪B′. One cannot have S ∩A′\nand S ∩B′ non empty simultaneously, since this would imply a direct edge from A′\nto B′ (S is complete). Say that S ∩A′ = ∅, so that A′ ⊂A. Then the decomposition\nV = A′ ∪S′ ∪(B′ ∪B) is such that S′ separates A′ from B ∪B′. Indeed, a path from A′\nto b ∈B ∪B′ must pass in S′ if b ∈B′, and, if b ∈B, it must pass in S (since it links\nA and B). But S ⊂S′ ∪B′ so that the path must intersect S′. We therefore obtain\na decomposition that enlarges B, which is a contradiction and shows that A ∪S is\ncomplete. Given this, any element s ∈A can only have neighbors in A ∪S and is\ntherefore such that DG(s) = ∅, which concludes the proof.\n■\nIf a graph is triangulated, there is more than one perfect ordering of its vertexes.\nOne of these orderings is provided the maximum cardinality search algorithm, which\nalso allows one to decide whether the graph is triangulated. We start with a defini-\ntion/notation.\nDefinition 14.27 If G = (V ,E) is an undirected graph, with |V | = n, any ordering V =\n(s1,...,sn) can be identified with the bijection α : V →{1,...,n} defined by α(sk) = k. In\nother terms, α(s) is the rank of s in the ordering. We will refer to α as an ordering, too.\n\n14.6. BUILDING JUNCTION TREES\n365\nGiven an ordering α, we define incremental neighborhoods V α,k\ns\n, for s ∈V and k =\n1,...,n to be the intersections of Vs with the sets α−1({1,...,k}), i.e.,\nV α,k\ns\n= {t ∈V ,t ∼s,α(t) ≤k}.\nOne says that α satisfies the maximum cardinality property if, for all k = {2,...,n}\n|V α,k−1\nsk\n| = max\nα(s)≥k |V α,k−1\ns\n|.\n(14.35)\nwhere sk = α−1(k).\nGiven this, we have the proposition:\nProposition 14.28 If G = (V ,E) is triangulated, then any ordering that satisfies the max-\nimum cardinality property is perfect.\nEquation (14.35) immediately provides an algorithm that constructs an order-\ning satisfying the maximum cardinality property given a graph G. From proposi-\ntion 14.28, we see that, if for some k, the largest set V α,k−1\nsk\nis not a clique, then G is\nnot triangulated. We now proceed to the proof of this proposition.\nProof Let G be triangulated, and assume that α is an ordering that satisfies (14.35).\nAssume that α is not proper in order to reach a contradiction.\nLet k be the first index for which V α,k−1\nsk\nis not a clique, so that sk has two neigh-\nbors, say t and u, such that α(t) < k, α(u) < k and t ≁u. Assume that α(t) > α(u).\nThen t must have a neighbor that is not neighbor of s, say t′, such that α(t′) < α(t)\n(otherwise, s would have more neighbors than t at order less than α(t), which con-\ntradicts the maximum cardinality property). The sequence t′,t,s,u forms a path that\nis such that α increases from t′ to s, then decreases from s to u, and contains no\nchord. Moreover, t′ and u cannot be neighbors, since this would yield an achordal\nloop and a contradiction. The proof of proposition 14.28 consists in showing that\nthis construction can be iterated until a contradiction is reached.\nMore precisely, assume that an achordal path s1,...,sk has been obtained, such\nthat α(s) is first increasing, then decreasing along the path, and such that, at extrem-\nities one either has α(s1) < α(sk) < α(s2) or α(sk) < α(s1) < α(sk−1). In fact, one can\nswitch between these last two cases by reordering the path backwards. Both paths\n(u,s,t) and (u,s,t,t′) in the discussion above satisfy this property.\n• Assume, without loss of generality, that α(s1) < α(sk) < α(s2) and note that, in the\nconsidered path, s1 and sk cannot be neighbors (for, if j is the last index smaller than\nk −1 such that sj and sk are neighbors, then j must also be smaller than k −2 and the\nloop sj,...,sk−1,sk would be achordal).\n\n366\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\n• Since α(s2) > α(sk), and s1 and s2 are neighbors, sk must have a neighbor, say s′\nk,\nsuch that s′\nk is not neighbor of s2 and α(s′\nk) < α(sk).\n• Select the first index j > 2 such that sj ∼s′\nk, and consider the path (s1,...,sj,s′\nk).\nThis path is achordal, by construction, and one cannot have s1 ∼s′\nk since this would\ncreate an achordal loop. Let us show that α first increases and then decreases along\nthis path. Since s2 is in the path, α must first increase, and it suffices to show that\nα(s′\nk) < α(sj). If α increases from s1 to sj, then α(sj) > α(s2) > α(sk) > α(s′\nk). If α\nstarted decreasing at some point before sj, then α(sj) > α(sk) > α(s′\nk).\n• Finally, we need to show that the α-value at one extremity is between the first two\nα-values on the other end of the path. If α(s′\nk) < α(s1), and since we have just seen\nthat α(sj) > α(sk) > α(s1), we do get α(s′\nk) < α(s1) < α(sj). If α(s′\nk) > α(s1), then, since\nby construction α(s2) > α(sk) > α(s′\nk), we have α(s2) > α(s′\nk) > α(s1).\n• So, we have obtained a new path that satisfies the same property that the one we\nstarted with, but with a maximum value at end points smaller than the initial one,\ni.e.,\nmax(α(s1),α(s′\nk)) < max(α(s1),α(sk)).\nSince α takes a finite number of values, this process cannot be iterated indefinitely,\nwhich yields our contradiction.\n■\n14.6.3\nComputing maximal cliques\nAt this point, we know that a graph must be triangulated for its maximal cliques\nto admit junction trees, and we have an algorithm to decide whether a graph is\ntriangulated, and extend it into a triangulated one if needed. This provides the first\nstep, (JT1), of our description of the junction tree algorithm. The next step, (JT2),\nrequires computing a list of maximal cliques. Computing maximal cliques in general\ngraph is an NP complete problem, for which a large number of algorithms has been\ndeveloped (see, for example, [149] for a review). For graphs with a perfect ordering,\nhowever, this problem can always be solved in a polynomial time.\nIndeed, assume that a perfect ordering is given for G = (V ,E), so that V = {s1,...,sn}\nis such that, for all k, V ′\nsk := Vsk ∩{s1,...,sk−1} is a clique. Let Gk be G restricted to\n{s1,...,sk} and C∗\nk be the set of maximal cliques in Gk. Then the set Ck := {sk} ∪V ′\nsk is\nthe only maximal clique in Gk that contains sk: it is a clique because the ordering is\nperfect, and any clique that contains sk must be included in it (because its elements\nare either sk or neighbors of sk). It follows from this that the set C∗\nk can be deduced\nfrom C∗\nk−1 by\n\nC∗\nk = C∗\nk−1 ∪{Ck} if V ′\nk < C∗\nk−1\nC∗\nk = (C∗\nk−1 ∪{Ck}) \\ {V ′\nk} if V ′\nk ∈C∗\nk−1\nThis allows one to enumerate all elements in C∗\nG = C∗\nn, starting with C∗\n1 = {{s1}}.\n\n14.6. BUILDING JUNCTION TREES\n367\n14.6.4\nCharacterization of junction trees\nWe now discuss the last remaining point, (JT3). For this, we need to form the clique\ngraph of G, which is the undirected graph G = (C∗\nG,E) defined by (C,C′) ∈E if and\nonly if C ∩C′ , ∅. We then have the following fact:\nProposition 14.29 The clique graph G of a connected triangulated undirected graph G\nis connected.\nProof We proceed by induction, and assume that the result is true if |V | = n −1\n(the proposition obviously holds if |V | = 1). Assume that a perfect order on G has\nbeen chosen, say V = {s1,...,sn}. Let G′ be G restricted to {s1,...,sn−1}, and G′ the\nassociated clique graph. Because {sn} ∪Vsn is a clique, any path in G provides a valid\npath in G′ after removing all occurrences of sn (because any two neighbors of sn\nare linked). The induction hypothesis also implies that G′ is connected. Since G is\nconnected, Vsn is not empty. Moreover, C := {sn} ∪Vsn must be a maximal clique in\nG (since we assume that the order is perfect) and it is the only maximal clique in G\nthat contains sn (all other maximal cliques in G therefore are maximal cliques in G′\nalso). To prove that G is connected, it suffices to prove that C is connected to any\nother maximal clique, C′, in G by a path in G. If t ∈C, t , sn, there exists a maximal\nclique, say C′′, in G′ that contains t, and, since G′ is connected, there exists a path\n(C1 = C′,...,Cq = C′′) connecting C′ to C′′ in G′. Let j be the first integer such that\nCj = Vn (take j = q + 1 if this never happens). Then (C1,...,Cj−1,C) is a path linking\nC′ and C in G.\n■\nWe hereafter assume that G, and hence G, is connected. This is not real loss of\ngenerality because connected components in undirected graphs yields independent\nprocesses that can be handled separately. We assign weights to edges of the clique\ngraph of G by defining w(C,C′) = |C ∩C′|. A subgraph ˜T of any given graph ˜G is\ncalled a spanning tree if ˜T is a tree with set of vertexes equal to the set of vertexes of\n˜G. If T = (C∗\nG,E′) is a spaning tree of G, we define the total weight\nw(T ) =\nX\n{C,C′}∈E′\nw(C,C′).\nWe then have the proposition:\nProposition 14.30 [99] If G is a connected triangulated graph, the set of junction trees\nover C∗\nG coincides with the set of maximizers of w(T ) over all spanning trees of G.\n(Notice that G being connected implies that spanning trees over G exist.)\n\n368\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nBefore proving this proposition, we discuss some properties related to maximal\n(or maximum-weight) spanning trees over an undirected graph. For this discussion,\nwe let G = (V ,E) be any undirected graph with weight (w(e),e ∈E). We will then\napply these results to a clique graph when will switch back to the general notation\nof this section. Maximal spanning trees can be computed using the so-called Prim’s\nalgorithm [98, 155, 63].\nAlgorithm 14.7 (Prim’s algorithm)\nInitialize the algorithm with a single-node tree T1 = ({s1},∅), for some arbitrary s1 ∈\nV . Let Tk−1 = (Vk−1,Ek−1) be the tree obtained at step k −1 of the algorithm. If k ≤n,\nthe next tree is built as follows.\n(1) Let\nVk = {sk} ∪Vk−1 (sk < Vk−1.)\n(2) Let Ek = {ek} ∪Ek−1, such that ek = {sk,s} for some s ∈Vk−1 satisfying\nw(ek) = max\n\u0012\nw({t,t′}),{t,t′} ∈E,t < Vk−1,t′ ∈Vk−1\n\u0013\n.\n(14.36)\nThe ability of this algorithm to always build a maximal spanning tree is summa-\nrized in the following proposition [81, 129].\nProposition 14.31 If G = (V ,E,w) is a weighted, connected undirected graph, Prim’s\nalgorithm, as described above, provides a sequence Tk = (Vk,Ek), for k = 1,...,n of subtrees\nof G such that Vn = V and, for all k, Tk is a maximal spanning tree for the restriction GVk\nof G to Vk.\nMoreover, any maximal spanning tree of G, can be realized as Tn, where (T1,...,Tn) is\na sequence provided by Prim’s algorithm.\nProof We first prove that, for all k, Tk is a maximal spanning tree on the graph GVk.\nWe will prove a slightly stronger statement, namely, that, for all k, Tk can be\nextended to form a maximal spanning tree of G. This is stronger, because, if Tk =\n(Vk,Ek) can be extended to a maximal spanning tree T = (V ,E), and if T ′\nk = (Vk,E′\nk) is\na spanning tree for GVk such that w(Tk) < w(T ′\nk), then the graph T ′ = (V ,E′) with\nE′ = (E \\ Ek) ∪E′\nk\nwould be a spanning tree for G with w(T) < w(T ′), which is impossible. (To see that\nT ′ is a tree, notice that paths in T ′ are in one-to-one correspondence with paths in T\nby replacing any subpath within T ′\nk by the unique subpath in Tk that has the same\nextremities.)\n\n14.6. BUILDING JUNCTION TREES\n369\nClearly, T1, which only has one vertex, can be extended to a maximal spanning\ntree. Let k ≥1 be the last integer for which this property is true for all j = 1,...,k.\nIf k = n, we are done. Otherwise, take a maximum spanning tree, T , that extends\nTk. This tree cannot contain the new edge added when building Tk+1, namely ek+1 =\n{sk+1,s} as defined in Prim’s algorithm, since it would otherwise also extend Tk+1.\nConsider the path γ in T that links s to sk. This path must have an edge e = {t,t′}\nsuch that t ∈Vk and t′ < Vk, and by definition of ek+1, we must have w(e) ≤w(ek+1).\nNotice that e is uniquely defined, because a path leaving Vk cannot return in this set,\nsince one would be otherwise able to close it into a loop by inserting the only path\nin Tk that connects its extremities.\nReplace e by ek+1 in T. The resulting graph, say T ′, is still a spanning tree for\nG. From any path in T, one can create a path in T ′ with the same extremities by\nreplacing any occurrence of the edge, e, by the concatenation of the unique path in\nT going from t to s, followed by (s,sk+1), followed by the unique path in T going\nfrom sk+1 to t′. This implies that T ′ is connected. It is also acyclic, since any loop\nin T would have to contain ek+1 (since T is acyclic), but there is no other path than\n(s,sk+1) in T ′ that links s and sk, because this path would have to be in T, and we\nhave removed the only possible one from T by deleting the edge e.\nAs a conclusion, T ′ is an extension of Tk+1, and a spanning tree with total weight\nlarger or equal to the one of T , and must therefore be optimal, too. But this contra-\ndicts the fact that Tk+1 cannot be extended to a maximal tree, so that k = n and the\nsequence of trees provided by Prim’s algorithm is optimal.\nTo prove the second statement, let T be an optimal spanning tree. Let k be the\nlargest integer such that there exists a sequence (T1,...,Tk) generated by Prim’s algo-\nrithm, such that, for all j = 1,...,k, Tj is a subtree of T. One necessarily has j ≥1,\nsince T extends any one-vertex tree. If k = n, we are done. Assuming otherwise,\nlet Tk = (Vk,Ek) and make one more step of Prim’s algorithm, selecting an edge\nek+1 = (sk+1,s) satisfying (14.36). By assumption, ek+1 is not in T. Take as before\nthe unique path linking s and sk+1 in T and let e be the unique edge at which this\npath leaves Vk. Replacing e by ek+1 in T provides a new spanning tree, T ′. One\nmust have w(e) ≥w(ek+1) because T is optimal, and w(ek+1) ≥w(e) by (14.36). So\nw(e) = w(ek+1), and one can use e instead of ek+1 for the (k + 1)th step of Prim’s al-\ngorithm. But this contradicts the fact that k was the largest integer in a sequence of\nsubtrees of T that is generated by Prim’s algorithm, and one therefore has k = n.\n■\nThe proof of proposition 14.30, that we provide now, uses very similar “edge-\nswitching” arguments.\nProof (Proof of proposition 14.30) Let us start with a maximum weight spanning\ntree for G, say T , and show that it is a junction tree. Since T has maximum weight, we\n\n370\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nknow that it can be obtained via Prim’s algorithm, and that there exists a sequence\nT1,...,Tn = T of trees constructed by this algorithm. Let Tk = (Ck,Ek).\nWe proceed by contradiction. Let k be the largest index such that Tk can be ex-\ntended to a junction tree for C∗\nG, and let T ′ be a junction tree extension of Tk. Assume\nthat k < n, and let ek+1 = (Ck+1,C′) be the edge that has been added when building\nTk+1, with Ck+1 = {Ck+1} ∪Ck. This edge is not in T ′, so that there exists a unique\nedge e = (B,B′) in the path between Ck and C′ in T ′ such that B ∈Ck and B′ < Ck. We\nmust have w(e) = |B ∩B′| ≤w(ek+1) = |Ck+1 ∩C′|. But, since the running intersection\nproperty is true for T ′, both B and B′ must contain Ck+1∩C′ so that B∩B′ = Ck+1∩C′.\nThis implies that, if one modifies T ′ by replacing edge e by edge ek+1, yielding a new\nspanning tree T ′′, the running intersection property is still satisfied in T ′. Indeed if\na vertex s ∈V belongs to both extremities of a path containing B and B′ in T ′, then\nit must belong to B ∩B′, and hence to Ck+1 ∩C′, and therefore to any set in the path\nin T ′ that linked Ck+1 and C′. So we found a junction tree extension of Tk+1, which\ncontradicts our assumption that k was the largest. We must therefore have k = n and\nT is a junction tree.\nLet us now consider the converse statement and assume that T is a junction tree.\nLet k be the largest integer such that there exists a sequence of subgraphs of T that\nis provided by Prim’s algorithm. Denote such a sequence by (T1,...,Tk), with Tj =\n(Cj,Ej). Assume (to get a contradiction) that k < n, and consider a new step for\nPrim’s algorithm, adding a new edge ek+1 = {Ck+1,C′} to Tk. Take as before the path\nin T linking C′ to Ck+1 in T , and select the edge e at which this path leaves Ck. If e =\n(B,B′), we must have w(e) = |B∩B′| ≤w(ek) = |Ck+1∩C′|, and the running intersection\nproperty in T implies that Ck+1 ∩C′ ⊂B ∩B′, which implies that w(e) = w(ek+1).\nThis implies that adding e instead of ek+1 at step k + 1 is a valid choice for Prim’s\nalgorithm, and contradicts the fact that k was the largest number of such steps that\ncould provide a subtree of T . So k = n and T is maximal.\n■\n\nChapter 15\nBayesian Networks\n15.1\nDefinitions\nBayesian networks are graphical models supported by directed acyclic graphs (DAG),\nwhich provide them with an ordered organization (directed graphs were introduced\nin definition 13.35).\nWe first introduce some notation. Let G = (V ,E) be a directed acyclic graph. The\nparents of s ∈V are vertexes t such that (t,s) ∈E, and its children are t’s such that\n(s,t) ∈E. The set of parents of s is denoted pa(s), and the set of its children is ch(s),\nwith Vs = ch(s) ∪pa(s).\nSimilarly to trees, the vertexes of G can be partially ordered by s ≤G t if and only\nif there exists a path going from s to t. Unlike trees, however, there can be more\nthan one minimal element in V , and we still call roots vertexes that have no parent,\ndenoting\nV0 = {s ∈V : pa(s) = ∅}.\nWe also call leaves, or terminal nodes, vertexes that have no children. Unless other-\nwise specified, we assume that all graphs are connected.\nBayesian networks over G are defined as follows. We use the same notation as\nwith Markov random fields to represent the set of configurations F (V ) that contains\ncollections x = (xs,s ∈V ) with xs ∈Fs.\nDefinition 15.1 A random variable X with values in F (V ) is a Bayesian network over a\nDAG G = (V ,E) if and only if its distribution can be written in the form\nPX(x) =\nY\ns∈V0\nps(x(s))\nY\ns∈V \\V0\nps(x(pa(s)),x(s))\n(15.1)\nwhere ps is, for all s ∈V , a probability distribution with respect to x(s).\n371\n\n372\nCHAPTER 15. BAYESIAN NETWORKS\nUsing the convention that conditional distributions given the empty set are just ab-\nsolute distributions, we can rewrite (15.1) as\nPX(x) =\nY\ns∈V\nps(x(pa(s)),x(s)).\n(15.2)\nOne can verify that P\nx∈ΩPX(x) = 1. Indeed, when summing over x, we can start\nsumming over all x(s) with ch(s) = ∅(the leaves). Such x(s)’s only appear in the corre-\nsponding ps’s, which disappear since they sum to 1. What remains is the sum of the\nproduct over V minus the leaves, and the argument can be iterated until the remain-\ning sum is 1 (alternatively, work by induction on |V |). This fact is also a consequence\nof proposition 15.5 below, applied with A = ∅.\n15.2\nConditional independence graph\n15.2.1\nMoral graph\nBayesian networks have a conditional independence structure which is not exactly\ngiven by G, but can be deduced from it. Indeed, fixing S ⊂V , we can see, when\ncomputing the probability of X(S) = x(s) given X(Sc) = x(Sc), which is\nP(X(S) = x(S) | X(Sc) = x(Sc)) =\n1\nZ(x(Sc))\nY\ns∈V\nps(x(pa(s)),x(s)),\nthat the only variables x(t),t < S that can be factorized in the normalizing constant\nare those that are neither parent nor children of vertexes in S, and do not share a\nchild with a vertex in S (i.e., they intervene in no ps(x(pa(s)),x(s)) that involve elements\nof S). This suggests the following definition.\nDefinition 15.2 Let G be a directed acyclic graph. We denote G♯= (V ,E♯) the undirected\ngraph on V such that {s,t} ∈E♯if one of the following conditions is satisfied\n• Either (s,t) ∈E or (t,s) ∈E.\n• There exists u ∈V such that (s,u) ∈E and (t,u) ∈E.\nG♯is sometimes called the moral graph of G (because it forces parents to marry !). A\npath in G♯can be visualized as a path in G♭(the undirected graph associated with\nG) which is allowed to jump between parents of the same vertex even if they were\nnot connected originally.\nThe previous discussion implies:\n\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n373\nProposition 15.3 Let X be a Bayesian network on G. We have\n(SyT | U)G♯⇒(X(S)yX(T ) | X(U)),\ni.e., X is G♯-Markov.\nThis proposition can be refined by noticing that the joint distribution of X(S),\nX(T ) and X(U) can be deduced from a Bayesian network on a graph restricted to the\nancestors of S ∪T ∪U. Definition 13.21 for restricted graphs extends without change\nto directed graphs, and we repeat it below for convenience.\nDefinition 15.4 Let G = (V ,E) be a graph (directed or undirected), and A ⊂V . The\nrestricted graph GA = (A,EA) is such that the elements of EA are the edges (s,t) (or {s,t})\nin E such that both s and t belong to A.\nMoreover, for a directed acyclic graph G and s ∈V , we define the set of ancestors of\ns by\nAs = {t ∈V ,t ≤G s}\n(15.3)\nfor the partial order on V induced by G.\nIf S ⊂V , we denote AS = S\ns∈S As. Note that, by definition, S ⊂AS. The following\nproposition is true.\nProposition 15.5 Let X be a Bayesian network on G = (V ,E) with distribution given by\n(15.2). Let S ⊂V and A = AS. Then the distribution of X(A) is a Bayesian network over\nGA given by\nP(X(A) = x(A)) =\nY\ns∈A\nps(x(pa(s)),x(s)).\n(15.4)\nThere is no ambiguity in the notation pa(s), since the parents of s ∈A are the same in\nGA as in G.\nProof One needs to show that\nY\ns∈A\nps(x(pa(s)),x(s)) =\nX\nxAc\nY\ns∈V\nps(x(pa(s)),x(s)).\nThis can be done by induction on the cardinality of V . Assume that the result is true\nfor graphs of size n, and let |V | = n + 1 (the result is obvious for graphs of size 1).\nIf A = V , there is nothing to prove, so assume that Ac is not empty. Then Ac must\ncontain a leaf in G, since otherwise, A would contain all leaves and their ancestors\nwhich would imply that A = V .\nIf s ∈Ac is a leaf in G, one can remove the variable x(s) from the sum, since it\nonly appear in ps and transition probabilities sum to one. But one can now apply\nthe induction assumption to the restriction of G to V \\ {s}.\n■\n\n374\nCHAPTER 15. BAYESIAN NETWORKS\nGiven proposition 15.5, proposition 15.3 can therefore be refined as follows.\nProposition 15.6 Let X be a Bayesian network on G. We have\n(SyT | U)(GAS∪T ∪U )♯⇒(X(S)yX(T) | X(U)).\nProposition 15.5 is also used in the proof of the following proposition.\nProposition 15.7 Let G = (V ,E) be a directed acyclic graph, and X be a Bayesian net-\nwork over G. Then, for all s ∈S\nP(X(s) = x(s) | X(As\\{s}) = x(As\\{s})) = P(X(s) = x(s) | X(pa(s)) = x(s−)) = ps(x(pa(s)),x(s)).\nProof By proposition 15.5, we can without loss of generality assume that V = As.\nThen\nP(X(s) = x(s) | X(As\\{s}) = x(As\\{s}))\n∝P(X(As) = x(As))\n=\nps(x(pa(s)),x(s))Z(x(As\\{s}))\nwhere\nZ(x(As\\{s})) =\nY\nt∈As\\{s}\npt(x(pa(t)),x(t))\ndisappears when the conditional probability is normalized.\n■\n15.2.2\nReduction to d-separation\nWe now want to reformulate proposition 15.6 in terms of the unoriented graph G♭\nand specific features in G called v-junctions, that we now define.\nDefinition 15.8 Let G = (V ,E) be a directed graph. A v-junction is a triple of distinct\nvertexes, (s,t,u) ∈V × V × V such that {s,u} ⊂pa(t) (i.e., s and u are parents of t).\nWe will say that a path (s1,...,sN) in G♭passes at s = sk with a v-junction if (sk−1,sk,sk+1)\nis a v-junction in G.\nWe have the lemma:\nLemma 15.9 Two vertexes s and t in G are separated by a set U in (GA{s,t}∪U)♯if and only\nif any path between s and t in G♭must either\n(1) Pass at a vertex in U without a v-junction.\n(2) Pass in V \\ A{s,t}∪U at a v-junction.\n\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n375\nProof\nStep 1. We first note that the v-junction clause is redundant in (2). It can be removed\nwithout affecting the condition. Indeed, if a path in G♭passes in V \\A{s,t}∪U one can\nfollow this path downward (i.e., following the orientation in G) until a v-junction is\nmet. This has to happen before reaching the extremities of the path, since u would\nbe an ancestor of s or t otherwise. We can therefore work with the weaker condition\n(that we will denote (2)’) in the rest of proof.\nStep 2. Assume that U separates s and t in (GA{s,t}∪U)♯. Take a path γ between s and\nt in G♭. We need to show that the path satisfies (1) or (2)’. So assume that (2)’ is\nfalse (otherwise we are done) so that γ is included in A{s,t}∪U. We can modify γ by\nremoving all the central nodes in v-junctions and still keep a valid path in (GA{s,t}∪U)♯\n(since parents are connected in the moral graph). The remaining path must intersect\nU by assumption, and this cannot be at a v-junction in γ since we have removed\nthem. So (1) is true.\nStep 3. Conversely, assume that (1) or (2) is true for any path in G♭. Consider a path\nγ in (GA{s,t}∪U)♯between s and t. Any edge in γ that is not in G♭must involve parents\nof a common child in A{s,t}∪U. Insert this child between the parents every time this\noccurs, resulting in a v-junction added to γ. Since the added vertexes are still in\nA{s,t}∪U, the new path still has no intersection with V \\ A{s,t}∪U and must therefore\nsatisfy (1). So there must be an intersection with U without a v-junction, and since\nthe new additions are all at v-junctions, the intersection must have been originally\nin γ, which therefore passes in U. This shows that U separates s and t in (GA{s,t}∪U)♯.■\nCondition (2) can be further restricted to provide the notion of d-separation.\nDefinition 15.10 One says that two vertexes s and t in G are d-separated by a set U if\nand only if any path between s and t in G♭must either\n(D1) Pass at a vertex in U without a v-junction.\n(D2) Pass in V \\ AU with a v-junction.\nThen we have:\nTheorem 15.11 Two vertexes s and t in G are separated by a set U in (GA{s,t}∪U)♯if and\nonly if they are d-separated by U.\nProof It suffices to show that if condition ((D1) or (D2)) holds for any path between\ns and t in G♭, then so does ((1) or (2)). So take a path between s and t: if (D1) is true\n\n376\nCHAPTER 15. BAYESIAN NETWORKS\nfor this path, the conclusion is obvious, since (D1) and (1) are the same. So assume\nthat (D1) (and therefore (1)) is false and that (D2) is true. Let u be a vertex in V \\AU\nat which γ passes with a v-junction.\nAssume that (2) is false. Then u must be an ancestor of either s or t. Say it is an\nancestor of s: there is a path in G going from u to s without passing by U (otherwise\nu would be an ancestor of U); one can replace the portion of the old path between\ns and u by this new one, which does not pass by u with a v-junction anymore. So\nthe new path still does not satisfy (D1) and must satisfy (D2). Keep on removing\nall intersections with ancestors of s and t that have v-junctions to finally obtain a\npath that satisfies neither (D1) or (D2) and a contradiction to the fact that s and t are\nd-separated by U.\n■\n15.2.3\nChain-graph representation\nThe d-separability property involves both unoriented and oriented edges. It is in\nfact a property of the hybrid graph in which the orientation is removed from the\nedges that are not involved in a v-junction, and retained otherwise. Such graphs are\nparticular instances of chain graphs.\nDefinition 15.12 A chain graph G = (V ,E, ˜E) is composed with a finite set V of vertexes,\na set E ⊂P2(V ) of unoriented edges and a set ˜E ⊂E × E \\ {(t,t),t ∈E} of oriented edges\nwith the property that E ∩˜E♭= ∅, i.e., two vertexes cannot be linked by both an oriented\nand an unoriented edge.\nA path in a chain graph is a a sequence of vertexes s0,...,sN such that for all k ≥1,\nsk−1 and sk form an edge, which means that either {sk−1,sk} ∈E or (sk−1,sk) ∈˜E.\nA chain graph is acyclic if it contains no loop. It is semi-acyclic if it contains no loop\ncontaining oriented edges.\nWe start with the following equivalence relation within vertexes in a semi-acyclic\nchain graph.\nProposition 15.13 Let G = (V ,E, ˜E) be a semi-acyclic chain graph. Define the relation\nsRt if and only if there exists a path in the unoriented subgraph (V ,E) that links s and t.\nThen R is an equivalence relation.\nThe proposition is obvious. This relation partitions V in equivalence classes, the\nset of which being denoted VR. If S ∈VR, then any pair s,t in S is related by an\nunoriented path, and if S , S′ ∈VR, no elements s ∈S and t ∈S′ can be related by\nsuch a path.\n\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n377\nMoreover, no path in G between two elements of S ∈VR, can contain a directed\nedge, since these elements must also be related by an undirected path, and this\nwould create a loop in G containing an undirected edge. So the restriction of G\nto S is an undirected graph.\nOne can define a directed graph over equivalence classes as follows. Let GR =\n(VR,ER) be such that (S,S′) ∈ER if and only if there exists s ∈S and t ∈S′ such\nthat (s,t) ∈˜E. The graph GR is acyclic: any loop in GR would induce a loop in G\ncontaining at least one oriented edge.\nWe now can formally define a probability distribution on a semi-acyclic chain\ngraph.\nDefinition 15.14 Let G = (V ,E, ˜E) be a semi-acyclic chain graph. One says that a ran-\ndom variable X decomposes on G if and only if: (X(S),S ∈VR) is a Bayesian network on\nGR and the conditional distribution of X(S) given X(S′),S′ ∈pa(S) is GS-Markov, such\nthat, for s ∈S, P(X(s) = x(s) | X(t),t ∈S,XS′,S′ ∈pa(S)) only depends on x(t) with {s,t} ∈E\nor (t,s) ∈˜E.\nReturning to our discussion on Bayesian networks, we have the following. Asso-\nciate to a DAG G = (V ,E) the chain graph G† = (V ,E†, ˜E†) defined by: {s,t} ∈E† if and\nonly if (s,t) or (t,s) ∈E and is not involved in a v-junction, and (s,t) ∈˜E† if (s,t) ∈E\nand is involved in a v-junction. This graph is acyclic; indeed, take any loop in G†:\nwhen its edges are given their original orientations in E, the sequence cannot contain\na v-junction, since the orientation in v-junctions are kept in G†; the path therefore\nconstitutes a loop in G which is a contradiction.\nAll, excepted at most one, vertexes in an equivalence class S ∈G†\nR have all their\nparents in S. Indeed, assume that two vertexes, s and t, in S have parents outside of\nS. There exists an unoriented path, s0 = s,s1,...,sN = t, in G† connecting them, since\nthey belong to the same equivalence class. The edge at s must be oriented from s to\ns1 in G, since otherwise s1 would be a second parent to s in G, creating a v-junction,\nand the edge would have remained oriented in G†. Similarly, the last edge in the\npath must be oriented from t to sN−1 in G. But this implies that there exists a v-\njunction in the original orientation along the path, which cannot be constituted with\nonly unoriented edges in G†. So we get a contradiction.\nThus, random variables that decompose on G† are “Bayesian networks” of acyclic\ngraphs, or trees since we know these are equivalent. The root of each tree must have\nmultiple (vertex) parents in the parent tree in GR. The following theorem states that\nall Bayesian networks are equivalent to such a process.\nTheorem 15.15 Let G = (V ,E) be a DAG. The random variable X is a Bayesian network\non G if and only if it decomposes over G†.\n\n378\nCHAPTER 15. BAYESIAN NETWORKS\nProof Assume that X is a Bayesian network on G. We can obviously rewrite the\nprobability distribution of X in the form\nπ(x) =\nY\nS∈G†\nR\nY\ns∈S\nps(x(pa(s)),x(s)).\nSince every vertex in S has its parents in S or in S\nT∈pa(S) T , this a fortiori takes the\nform\nπ(x) =\nY\nS∈G†\nR\npS((x(T),T ∈S−),x(s)).\nSo X(S),S ∈VR is a Bayesian network. Moreover,\npS((x(T ),T ∈S−),x(s)) =\nY\ns∈S\nps(x(pa(s)),x(s))\nis a tree distribution with the required form of the individual conditional distribu-\ntions.\nNow assume that X decomposes on G†. Then the conditional distribution of\nX(S) given X(T),T ∈pa(S) is Markov for the acyclic undirected graph GS, and can\ntherefore be expressed as a tree distribution consistent with the orientation of G.\n■\n15.2.4\nMarkov equivalence\nWhile the previous discussion provides a rather simple description of Bayesian net-\nworks in terms of chain graphs, it does not go all the way in reducing the number\nof oriented edges in the definition of a Bayesian network. The issue is, in some way,\naddressed by the notion of Markov equivalence, which is defined as follows.\nDefinition 15.16 Two directed acyclic graphs on the same set of vertexes G = (V ,E) and\n˜G = (V , ˜E) are Markov-equivalent if any family of random variables that decomposes as a\n(positive) Bayesian network over one of them also decomposes as a Bayesian network over\nthe other.\nThe notion of Markov equivalence is exactly described by d-separation. This\nis stated in the following theorem, due to Geiger and Pearl [77, 76], that we state\nwithout proof.\nTheorem 15.17 G and ˜G are Markov equivalent if and only if, whenever two vertexes\nare d-separated by a set in one of them, the same separation is true with the other.\nThis property can be expressed in a strikingly simple condition. One says that a\nv-junction (s,t,u) in a DAG is unlinked if s and u are not neighbors.\n\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n379\nTheorem 15.18 G and ˜G are Markov equivalent if and only if G♭= ˜G♭and G and ˜G have\nthe same unlinked v-junctions.\nProof Step 1. We first show that a given pair of vertexes in a DAG is unlinked if\nand only if it can be d-separated by some set in the graph. Clearly, if they are linked,\nthey cannot be d-separated (which is the “if” part), so what really needs to be proved\nis that unlinked vertexes can be d-separated. Let s and t be these vertexes and let\nU = A{s,t} \\ {s,t}. Then U d-separates s and t since any path between s and t in\n(GA{s,t}∪U)♯= (GA{s,t})♯must obviously pass in U.\nStep 2. We now prove the only-if part of theorem 15.18 and therefore assume that\nG and ˜G are Markov equivalent, or, as stated in theorem 15.17, that d-separation\ncoincides in G and ˜G. We want to prove that G♭= ˜G♭and unlinked v-junctions are\nthe same.\nStep 2.1. The first statement is obvious from Step 1: d-separation determines the\nexistence of a link, so if d-separation coincides in the two graphs, then the same\nholds for links and G♭= ˜G♭.\nStep 2.2. So let us proceed to the second statement and let (s,t,u) be an unlinked v-\njunction in G. We want to show that it is also a v-junction in ˜G (obviously unlinked\nsince links coincide).\nWe will denote by ˜\nAS the ancestors of some set S ⊂V in ˜G (while AS still denotes\nits ancestors in G). Let U = A{s,u} \\ {s,u}. Then, as we have shown in Step 1, U\nd-separates s and u in G, so that, by assumption it also d-separates them in ˜G.\nWe know that t < U, because it cannot be both a child and an ancestor of {s,u} in G\n(this would induce a loop). The path (s,t,u) links s and u and does not pass in U,\nwhich is only possible (since U d-separates s and t in ˜G) if it passes in V −˜\nAU at a\nv-junction: so (s,t,u) is a v-junction in ˜G, which is what we wanted to prove.\nStep 3. We now consider the converse statement and assume that G♭= ˜G♭and un-\nlinked v-junctions coincide. We want to show that d-separation is the same in G and\n˜G. So, we assume that U d-separates s and t in G, and we want to show that the same\nis true in ˜G. Thus, what we need to prove is:\nClaim 1. Consider a path γ between s and t in ˜G♭= G♭. Then γ either (D1) passes in\nU without a v-junction in ˜G, or (D2) in V \\ ˜\nAU with a v-junction in ˜G.\nWe will prove Claim 1 using a series of lemmas. We say that γ has a three-point loop\nat u if (v,u,w) are three consecutive points in γ such that v and w are linked. So\n(v,u,w,v) forms a loop in the undirected graph.\nLemma 15.19 If γ is a path between s and t that does not satisfy (D2) for G and passes\nin U without three-point loops, then γ satisfies (D1) for ˜G.\nThe proof is easy: since γ does not satisfy (D2) in G, it satisfies (D1) and passes in\nU without a v-junction in G. But this intersection cannot be a v-junction in ˜G since\n\n380\nCHAPTER 15. BAYESIAN NETWORKS\nit would otherwise have to be linked and constitute a three-point loop in γ, which\nproves that (D1) is true for γ in ˜G.\nThe next step is to remove the three-point loop condition in lemma 15.19. This will\nbe done using the next two results.\nLemma 15.20 Let γ be a path with a three-point loop at u ∈U for G. Assume that γ \\ u\n(which is a valid path in G♭) satisfies (D1) or (D2) in ˜G. Then γ satisfies (D1) or (D2) in\n˜G.\nTo prove the lemma, let v and w be the predecessor and successor of u in γ. First\nassume that γ \\ u satisfies (D1) in ˜G. If this does not happen at v or at w, then this\nwill apply also to γ and we are done, so let us assume that v ∈U and that (v′,v,w) is\nnot a v-junction in ˜G, where v′ is the predecessor of v. If (v′,v,u) is not a v-junction\nin ˜G, then (D1) is true for γ in ˜G. If it is a v-junction, then (v,u,w) is not and (D1) is\ntrue too.\nAssume now that (D2) is true for γ \\u in ˜G. Again, there is no problem if (D2) occurs\nfor some point other than v or w, so let us consider the case for which it happens at\nv. This means that v < ˜\nAU and (v′,v,w) is a v-junction. But, since u ∈U, the link\nbetween u and v must be from u to v in ˜G so that there is no v-junction at u and (D1)\nis true in ˜G. This proves lemma 15.20.\nLemma 15.21 Let γ be a path with a three-point loop at u ∈U for G. Assume that γ\ndoes not satisfy (D2) in G. Then γ \\ u does not satisfy this property either.\nLet us assume that γ \\ u satisfies (D2) and reach a contradiction. Letting (v,u,w) be\nthe three-point loop, (D2) can only happen in γ \\ u at v or w, and let us assume that\nthis happens at v, so that, v′ being the predecessor of v, (v′,v,w) is a v-junction in G\nwith v < AU. Since v < AU, the link between u and v in G must be from u to v, but\nthis implies that (v′,v,u) is a v-junction in G with v < AU which is a contradiction:\nthis proves lemma 15.21.\nThe previous three lemmas directly imply the next one.\nLemma 15.22 If γ is a path between s and t that does not satisfy (D2) for G, then γ\nsatisfies (D1) or (D2) for ˜G.\nIndeed, if we start with γ that does not satisfy (D2) for G, lemma 15.21 allows us\nto progressively remove three-point loops from γ until none remains with a final\npath that satisfies the assumptions of lemma 15.19 and therefore satisfies (D1) in ˜G,\nand lemma 15.20 allows us to add the points that we have removed in reverse order\nwhile always satisfying (D1) or (D2) in ˜G.\nWe now partially relax the hypothesis that (D2) is not satisfied with the next lemma.\nLemma 15.23 If γ is a path between s and t that does not pass in V \\ AU at a linked\nv-junction for G, then γ satisfies (D1) or (D2) for ˜G.\n\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n381\nAssume that γ does not satisfy (D2) for ˜G (otherwise the result is proved).\nBy\nlemma 15.22, γ must satisfy (D2) for G. So, take an intersection of γ with V \\AU that\noccurs at a v-junction in G, that we will denote (v,u,w). This is still a v-junction in\n˜G since we assume it to be unlinked. Since (D2) is false in ˜G, we must have u ∈˜\nAU,\nand there is an oriented path, τ, from u to U in ˜G.\nWe can assume that τ has no v-junction in G. If a v-junction exists in τ, then this\nv-junction must be linked (otherwise this would also be a v-junction in ˜G and con-\ntradict the fact that τ is consistently oriented in ˜G), and this link must be oriented\nfrom u to U in ˜G to avoid creating a loop in this graph. This implies that we can\nbypass the v-junction while keeping a consistently oriented path in ˜G, and iterate\nthis until τ has no v-junction in G. But this implies that τ is consistently oriented in\nG, necessarily from U to u since u < AU.\nDenote τ = (u0 = u,v1,...,un ∈U). We now prove by induction that each (v,uk,w) is\nan unlinked v-junction. This is true when k = 0, and let us assume that it is true for\nk −1. Then (uk,uk−1,v) is a v-junction in G but not in ˜G: so it must be linked and\nthere exists an edge between v and uk. In ˜G, this edge must be oriented from v to uk,\nsince (v,uk−1,uk,v) would form a loop otherwise. For the same reason, there must be\nan edge in ˜G from w to uk so that (v,uk,w) is an unlinked v-junction.\nSince this is true for k = n, we can replace u by un in γ and still obtain a valid path.\nThis can be done for all intersections of γ with V \\AU that occur at v-junctions. This\nfinally yields a path (denote it ¯γ) which does not satisfy (D2) in G anymore, and\ntherefore satisfies (D1) or (D2) in ˜G: so ¯γ must either pass in U without a v-junction\nor in V \\ ˜AU at a v-junction. None of the nodes that were modified can satisfy any of\nthese conditions, since they were all in U with a v-junction, so that the result is true\nfor the original γ also. This proves lemma 15.23.\nSo the only unsolved case is when γ is allowed to pass in V \\AU at linked v-junctions.\nWe define an algorithm that removes them as follows. Let γ0 = γ and let γk be the\npath after step k of the algorithm. One passes from γk to γk+1 as follows.\n• If γk has no linked v-junctions in V \\ AU for G, stop.\n• Otherwise, pick such a v-junction and let (v,u,w) be the three nodes involved in\nit.\n(i) If v ∈U,v′ < U and (v′,v,u) is a v-junction in ˜G, remove v from γk to define\nγk+1.\n(ii) Otherwise, if w ∈U,w′ < U and (u,w,w′) is a v-junction in ˜G, remove w from\nγk to define γk+1.\n(iii) Otherwise, remove u from γk to define γk+1.\nNone of the considered cases can disconnect the path. This is clear for case (iii) since\nv and w are linked. For case (i), note that, in G, (v′,v,u) cannot be a v-junction since\n(v,u,w) is one. This implies that the v-junction in ˜G must be linked and that v′ and\nu are connected.\n\n382\nCHAPTER 15. BAYESIAN NETWORKS\nThe algorithm will stop at some point with some γn that does not have any linked\nv-junction in V \\AU anymore, which implies that (D1) or (D2) is true in ˜G for γn. To\nprove that this statement holds for γ, it suffices to show that if (D1) or (D2) is true\nin ˜G with γk+1, it must have been true with γk at each step of the algorithm. So let’s\nassume that γk+1 satisfies (D1) or (D2) in ˜G.\nFirst assume that we passed from γk to γk+1 via case (iii). Assume that (D2) is true\nfor γk+1, with as usual the only interesting case being when this occurs at v or w.\nAssume it occurs at v so that (v′,v,w) is a v-junction and v < ˜\nAU. If (v′,v,u) is a\nv-junction, then (D2) is true with γk. Otherwise, there is an edge from v to u in ˜G\nwhich also implies an edge from w to u since (v,u,w,v) would be a loop otherwise.\nSo (v,u,w) is a v-junction in ˜G, and u cannot be in ˜\nAU since its parent, v would be in\nthat set also. So (D2) is true in ˜G. Now, assume that (D1) is true at v, so that (v′,v,w)\nis not a v-junction and v ∈U. If (v′,v,u) is not a v-junction either, we are done, so\nassume the contrary. If v′ ∈U, then we cannot have a v-junction at v′ and (D1) is\ntrue. But v′ < U is not possible since this leads to case (i).\nNow assume that we passed from γk to γk+1 via case (i). Assume that (D1) is true for\nγk: this cannot be at v′ since v′ < U, neither at u since u < AU, so it will also be true\nfor γk+1. The same statement holds with (D2) since (v′,v,u) is a v-junction in ˜G with\nv ∈U which implies that both v′ and u are in ˜\nAU. Case (ii) is obviously addressed\nsimilarly.\nWith this, the proof of theorem 15.18 is complete.\n■\n15.2.5\nProbabilistic inference: Sum-prod algorithm\nWe now discuss the issue of using the sum-prod algorithm to compute marginal\nprobabilities, P(X(s) = x(s)) for s ∈V when X is a Bayesian network on G = (V ,E). By\ndefinition, P(X = x) can be written in the form\nP(X = x) =\nY\nC∈C\nϕC(x(C))\nwhere C contains all subsets Cs := {s}∪pa(s), s ∈V . Marginal probabilities can there-\nfore be computed easily when the factor graph associated to C is acyclic, according\nto proposition 14.13. However, because of the specific form of the ϕC’s (they are\nconditional probabilities), the sum-prod algorithm can be analyzed in more detail,\nand provide correct results even when the factor graph is not acyclic.\nThe general rules for the sum-prod algorithm are\n\nmsC(x(s)) ←\nY\n˜C,s∈˜C, ˜C,C\nm ˜Cs(x(s))\nmCs(x(s)) ←\nX\ny(C):y(s)=x(s)\nϕC(y(C))\nY\nt∈C\\{s}\nmtC(y(t))\n\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n383\nThey take a particular form for Bayesian networks, using the fact that a vertex s\nbelongs to Cs, and to all Ct for t ∈ch(s).\nmsCs(x(s))\n←\nY\nt∈ch(s)\nmCts(x(s)),\nmsCt(x(s))\n←\nmCss(x(s))\nY\nu∈ch(s),u,t\nmCus(x(s)), for t ∈ch(s),\nmCss(x(s))\n←\nX\ny(Cs),y(s)=x(s)\nps(y(pa(s)),x(s))\nY\nt∈pa(s)\nmtCs(y(t)),\nmCts(x(s))\n←\nX\ny(Ct),y(s)=x(s)\npt(x(s) ∧y(pa(s)\\{t}),y(t))mtCt(y(t))\nY\nu∈pa(s),u,t\nmuCt(y(u)),\nfor t ∈ch(s).\nThese relations imply that, if pa(s) = ∅(s is a root), then mCss = ps(x(s)). Also, if\nch(s) = ∅(s is a leaf) then msCs = 1. The following proposition shows that many of the\nmessages become constant over time.\nProposition 15.24 All upward messages, msCs and mCts with t ∈ch(s) become constant\n(independent from x(s)) in finite time.\nProof This can be shown recursively as follows. Assume that, for a given s, mtCt is\nconstant for all t ∈ch(s) (this is true if s is a leaf). Then,\nmCts(x(s))\n←\nX\npeyCt,y(s)=x(s)\npt(x(s) ∧y(pa(s)\\{t}),y(t))mtCt(y(t))\nY\nu∈pa(s),u,t\nmuCt(y(u)),\n=\nmtCt\nX\ny(Ct),y(s)=x(s)\npt(x(s) ∧y(pa(s)\\{t}),y(t))\nY\nu∈pa(s),u,t\nmuCt(y(u))\n=\nmtCt\nX\ny(Ct\\{t}),y(s)=x(s)\nY\nu∈pa(s),u,t\nmuCt(y(u))\n=\nmtCt\nY\nu∈pa(s),u,t\nX\ny(u)\nmuCt(y(u))\nwhich is constant. Now\nmsCs(x(s)) ←\nY\nt∈ch(s)\nmCts(x(s))\nis also constant. This proves that all msCs progressively become constant, and, as we\nhave just seen, this implies the same property for mCts, t ∈ch(s).\n■\n\n384\nCHAPTER 15. BAYESIAN NETWORKS\nThis proposition implies that, if initialized with constant messages (or after a\nfinite time), the sum-prod algorithm iterates\nmsCs\n←\nY\nt∈ch(s)\nmCts\nmCss(x(s))\n←\nX\ny(Cs),y(s)=x(s)\nps(y()pa(s),x(s))\nY\nt∈pa(s)\nmtCs(y(t))\nmsCt(x(s))\n←\nmCss(x(s))\nY\nu∈ch(s),u,t\nmCus, t ∈ch(s)\nmCts\n←\nmsCs\nY\nu∈pa(t),u,s\nX\ny(u)\nmuCs(y(u)), t ∈ch(s).\nFrom this expression, we can conclude\nProposition 15.25 If the previous algorithm is first initialized with upward messages,\nmsCs = mCts all equal to 1, and if downward messages are computed top down from the\nroots to the leaves, the obtained configuration of messages is invariant for the sum-prod\nalgorithm.\nProof If all upward messages are equal to 1, then clearly, the downward messages\nsum to 1 once they are updated from roots to leaves, and this implies that the upward\nmessages will remain equal to 1 for the next round. The obtained configuration is\ninvariant since the downward messages are recursively uniquely defined by their\nvalue at the roots.\n■\nThe downward messages, under the previous assumptions, satisfy msCt(x(s)) = mCss(x(s))\nfor all t ∈ch(s) and therefore\nmCss(x(s)) =\nX\ny(Cs),y(s)=x(s)\nπ(y(pa(s)),x(s))\nY\nt∈pa(s)\nmCtt(y(t)).\n(15.5)\nNote that the associated “marginals” inferred by the sum-prod algorithm are\nσs(x(s)) =\nY\nC,s∈C\nmCs(x(s)) = mCss(x(s))\nsince mCts(x(s)) = 1 when t ∈ch(s).\nAlthough the sum-prod algorithm initialized with unit messages converges to a\nstable configuration if run top-down, the obtained σs’s do not necessarily provide\nthe correct single site marginals. There is a situation for which this is true, however,\nwhich is when the initial directed graph is singly connected, as we will see below.\n\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n385\nBefore this, let us analyze the complexity resulting from an iterative computation of\nthe marginal probabilities, similar to what we have done with trees.\nWe define the depth of a vertex in G as follows.\nDefinition 15.26 Let G = (V ,E) be a DAG. The depth of a vertex s in V is defined recur-\nsively by\n- depth(s) = 0 if s has no parent.\n- depth(s) = 1 + max\n\u0010\ndepth(t),t ∈pa(s)\n\u0011\notherwise.\nThe recursive computation of marginal distributions is made possible (although\nnot always feasible) with the following remark.\nLemma 15.27 Let X be a Bayesian network on the DAG G = (V ,E), and S ⊂V , such\nthat all elements in S have the same depth. Let pa(S) be the set of parents of elements\nin S, and T = depth−(S) the set of vertexes in V with depth strictly smaller than the\ndepth of S. Then (X(S)yX(T\\pa(S)) | X(pa(S))) and the variables X(s),s ∈S are conditionally\nindependent given X(pa(S)).\nProof It suffices to show that vertexes in S are separated from T \\ pa(S) and from\nother elements of S by pa(S) for the graph (GS∪T )♯. Any path starting at s ∈S must\neither pass by a parent of s (which is what we want), or by one of its children, or\nby another vertex that shares a child with s in GS∪T . But s cannot have any child\nin GS∪T , since this child cannot have a smaller depth than s, and it cannot be in S\neither since all elements in S have the same depth.\n■\nThis lemma allows us to work recursively as follows. Assume that we can compute\nmarginal distributions over sets S with maximal depth no larger than d. Take a set\nS of maximal depth d +1, and let S0 be the set of elements of depth d +1 in S. Then,\nletting T = depth−(S) = depth−(S0), and S1 = S \\ S0,\nP(X(S) = x(S))\n=\nX\ny(T \\S1)\nP(X(S0) = x(S0) | X(T ) = y(T\\S1) ∧x(S1))P(X(T ∪S1) = y(T \\S1) ∧x(S1))\n=\nX\ny(pa(S)\\S1)\nY\ns∈S0\nps((y ∧x)(pa(s)) ∧x(S1),x(s))P(X(pa(S0)∪S1) = y(pa(S0)\\S1) ∧x(S1))\n(15.6)\nSince pa(S)∪S1 has maximal depth strictly smaller than the maximal depth of S, this\nindeed provides a recursive formula for the computation of marginal over subsets\nof V with increasing maximal depths. However, because one needs to add parents\nto the considered set when reducing the depth, one may end up having to compute\nmarginals over very large sets, which becomes intractable without further assump-\ntions.\n\n386\nCHAPTER 15. BAYESIAN NETWORKS\nA way to reduce the complexity is to assume that the graph G is singly connected,\nas defined below.\nDefinition 15.28 A DAG G is singly connected if there exists at most one path in G that\nconnects any two vertexes.\nSuch a property is true for a tree, but also holds for some networks with multiple\nparents. We have the following nice property in this case.\nProposition 15.29 Let G be a singly connected DAG and X a Bayesian network on G. If\ns is a vertex in G, the variables (X(t),t ∈pa(s)) are mutually independent.\nProof We have, using proposition 15.5,\nP(X(pa(s)) = x(pa(s))) =\nX\ny(Apa(s)),y(pa(s))=x(pa(s))\nY\nu∈Apa(s)\npu(y(pa(u)),y(u)).\nBecause the graph is singly connected, two parents of s cannot have a common an-\ncestor (since there would then be two paths from this ancestor to S). So Apa(s) is the\ndisjoint union of the At’s for t ∈pa(s) and we can write\nP(X(pa(s)) = x(pa(s)))\n=\nX\ny(Apa(s)),y(pa(s))=x(pa(s))\nY\nt∈pa(s)\nY\nu∈At\npu(y(pa(u)),y(u))\n=\nY\nt∈pa(s)\nX\ny(At),y(t)=x(t)\nY\nu∈At\npu(y(pa(u)),y(u))\n=\nY\nt∈pa(s)\nP(X(t) = x(t))\nThis proves the lemma.\n■\nSection 15.2.5 can be simplified under the assumption of a singly connected\ngraph, at least for the computation of single vertex marginals; we have, if s ∈V\nand G is singly connected\nP(X(s) = x(s)) =\nX\ny(pa(s))\nps(y(pa(s)),x(s))\nY\nt∈pa(s)\nP(X(t) = y(t)).\n(15.7)\nThis is now recursive in single vertex marginal probabilities. It moreover coincides\nwith the recursive equation that defines the messages mCss in (15.5), which shows\nthat the sum-prod algorithm provides the correct answer in this case.\n\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n387\n15.2.6\nConditional probabilities and interventions\nOne of the main interests of graphical models is to provide an ability to infer the be-\nhavior of hidden variables of interest given other, observed, variables. When dealing\nwith oriented graphs the way this should be analyzed is, however, ambiguous.\nLet’s consider an example, provided by the graph in fig. 15.1. The Bayesian net-\nNo school\nBad weather\nBroken HVAC\nFigure 15.1: Example of causal graph.\nwork interpretation of this graph is that both events (which may be true or false)\n“Bad weather” and “Broken HVAC” happen first, and that they are independent.\nThen, given their observation, the “No school” event may occur, probably more\nlikely if the weather is bad or the HVAC is broken or snow, and even more likely\nif both happened at the same time.\nNow consider the following passive observation: you wake up, you haven’t checked\nthe weather yet or the news yet, and someone tells you that there is no school today.\nThen you may infer that there is more chances than usual for bad weather or the\nHVAC broken at school. Conditionally to this information, these two events become\ncorrelated, even if they were initially independent. So, even if the “No school” event\nis considered as a probabilistic consequence of its parents, observing it influences\nour knowledge on them.\nNow, here is an intervention, or manipulation: the school superintendent has\ndeclared that he has given enough snow days for the year and declared that there\nwould be school today whatever happens. So you know that the “no-school” event\nwill not happen. Does it change the risk of bad weather of broken HVAC? Obviously\nnot: an intervention on a node does not affect the distribution of the parents.\nManipulation and passive observation are two very different ways of affecting\nunobserved variables in Bayesian networks. Both of them may be relevant in appli-\ncations. Of the two, the simplest to analyze is intervention, since it merely consists\nin clamping one of the variables while letting the rest of the network dynamics un-\nchanged. This leads to the following formal definition of manipulation.\nDefinition 15.30 Let G = (V ,E) be a directed acyclic graph and X a Bayesian network on\n\n388\nCHAPTER 15. BAYESIAN NETWORKS\nG. Let S be a subset of G and x(S) ∈FS a given configuration on S. Then the manipulated\ndistribution of X with fixed values x(S) on S is the Bayesian network on the restricted\ngraph GS, with the same conditional probabilities, using the value x(s) every time a vertex\ns ∈S is a parent of t ∈V \\ S in G.\nSo, if the distribution of X is given by (15.2), then its distribution after manipulation\non S is\n˜π(y(V \\S)) =\nY\nt∈V \\S\npt(y(pa(t)),y(t))\nwhere pa(t) is the set of parents of t in G, and y(s) = x(s) whenever s ∈pa(t) ∩S.\nThe distribution of a Bayesian network X after passive observation X(S) = x(S) is\nnot so easily described. It is obviously the conditional distribution P(X(V \\S) = y(V \\S) |\nX(S) = x(S)) and therefore requires using the conditional dependency structure, in-\nvolving the moral graph and/or d-separation.\nLet us discuss this first in the simpler case of trees, for which the moral graph is\nthe undirected acyclic graph underlying the tree, and d-separation is simple separa-\ntion on this acyclic graph. We can then use proposition 13.22 to understand the new\nstructure after conditioning: it is a G♭\nV \\S-Markov random field, and, for t ∈V \\S, the\nconditional distribution of X(t) = y(t) given its neighbors is the same as before, using\nthe value x(s) when s ∈S. But note that when doing this (passing to G♭), we broke the\ncausality relation between the variables. We can however always go back to a tree (or\nforest, since connectedness may have been broken) with the same edge orientation as\nthey initially were, but this requires reconstituting the edge joint probabilities from\nthe new acyclic graph, and therefore using (acyclic) belief propagation.\nWith general Bayesian networks, we know that the moral graph can be loopy and\ntherefore a source of difficulties. The following proposition states that the damage\nis circumscribed to the ancestors of S.\nProposition 15.31 Let G = (V ,E) be a directed acyclic graph, X a Bayesian network\non G, S ⊂V and x(AS) ∈F (AS). Then the conditional distribution of X(Ac\nS) given by\nX(AS) = x(AS) coincides with the manipulated distribution in definition 15.30.\nProof The conditional distribution is proportional to\nY\ns∈V\np(y(pa(s)),y(s))\nwith y(t) = x(t) if t ∈AS. Since s ∈AS implies pa(s) ⊂AS, all terms with s ∈AS are\nconstant in the sum and can be factored out after normalization. So the conditional\n\n15.3. STRUCTURAL EQUATION MODELS\n389\ndistribution is proportional to\nY\ns∈Ac\nS\np(y(pa(s)),y(s))\nwith y(t) = x(t) if t ∈AS. But we know that such products sum to 1, so that the\nconditional distribution is equal to this expression and therefore provides a Bayesian\nnetwork on GAc\nS.\n■\n15.3\nStructural equation models\nStructural equation models (SEM’s) provides an alternative (and essentially equiva-\nlent) formulation of Bayesian networks, which may be more convenient to use, espe-\ncially when dealing with variables taking values in general state spaces.\nLet G = (V ,E) be a directed acyclic graph. SEMs are associated to families of\nfunctions Φs : F (pa(s)) × Bs →Fs and random variables ξs : Ω→Bs (where Bs is\nsome measurable set), for s ∈V . The random field X : Ω→F (V ) associated to the\nSEM satisfies the equations\nXs = Φ(s)(X(s−),ξ(s)).\n(15.8)\nBecause of the DAG structure, these equations uniquely define X once ξ is specified.\nAs a consequence, there exists a function Ψ such that X = Ψ(ξ).\nThe model is therefore fully specified by the functions Φ(s) and the probability\ndistributions of the variables ξ(s). We will assume that they have a density, denoted\ng(s),s ∈V , with respect to some measure µs on Bs. They are typically chosen as\nuniform distributions on Bs (continuous and compact, or discrete) or as standard\nGaussian when Bs = Rds for some ds. One also generally assumes that the variables\n(ξ(s),s ∈V ) are jointly independent, and we make this assumption below.\nLet Vk, k ≥0, be the set of vertexes in V with depth k (c.f. definition 15.26) and\nV<k = V0∪···∪Vk−1. Then (using the independence of (ξ(s),s ∈V ), for s ∈Vk, the con-\nditional distribution of X(s) given X(V<k) = x(V<k) is the distribution of Φ(s)(x(s−),ξ(s)).\nFormally this is given by\nΦ(s)(x(s−),·)♯(g(s)µs),\nthe pushforward of the distribution of ξ(s) by Φ(s)(x(s−),·).\nMore concretely, assume that ξs follows a uniform distribution on Bs = [0,1]h for\nsome h, and assume that Fs is finite for all s. Then,\nP(X(s) = x(s) | X(V<k) = x(V<k)) = Volume(Us(x(pa(s)),x(s)))\n∆= ps(x(pa(s)),x(s))\n\n390\nCHAPTER 15. BAYESIAN NETWORKS\nwhere\nUs(x(pa(s)),x(s)) =\nn\nξ ∈[0,1]h : Φ(s)(x(s−),ξ) = x(s)o\n.\nSince variables X(s), s ∈Vk are conditionally independent given X(V <k), we find that\nX decomposes as a Bayesian network over G,\nP(X = x) =\nY\ns∈V\nps(x(pa(s)),x(s)).\nSimilarly, if Fs = Bs = Rds, ξ(s) ∼N (0,IdRds), and ξ(s) 7→Φ(s)\nθ (x(pa(s)),ξ(s)) is invertible,\nwith C1 inverse x(s) 7→Ψ(s)\nθ (x(pa(s)),x(s)), then X is a Bayesian network, with continu-\nous variables, and, using the change of variable formula, the conditional distribution\nof X(s) given X(pa(s)) = x(s−) has p.d.f.\nps(x(pa(s)),x(s)) =\n1\n(2π)ds/2 exp\n\u0012\n−1\n2|xs −Ψ(s)\nθ (x(pa(s)),x(s))|2\u0013\f\f\f\fdet(∂x(s)Ψ(s)\nθ (x(pa(s)),x(s)))\n\f\f\f\f.\nA simple and commonly used special case for this example are linear SEMs, with\nX(s) = as + bT\ns X(s) + σsξ(s).\nIn this case, the inverse mapping is immediate and the Jacobian determinant in the\nchange of variables is 1/σds\ns .\n\nChapter 16\nLatent Variables and Variational Methods\n16.1\nIntroduction\nWe will describe, in the next chapters, methods that fit a parametric model to the\nobservation while introducing unobserved, or “latent,” components in their models,\nwhose inference typically attaches interpretable information or structure to the data.\nWe have seen one such example in the form of the mixture of Gaussian in chapter 4,\nthat we will revisit in chapter 19. We now provide a presentation of the variational\nBayes paradigm that provides a general strategy to address latent variable problems\n[143, 97, 14, 100].\nThe general framework is as follows. Variables in the model are divided in two\ngroups: the observable part, that we denote X, and the latent part, denoted Z. In\nmany models Z represents some unobservable structure, such that X conditional to\nZ has some relatively simple distribution (in a Bayesian estimation context, Z often\ncontains model parameters). The quantity of interest, however, is the conditional\ndistribution of Z given X (also called the “posterior distribution”), which allows one\nto infer the latent structure from the observations, and will also have an important\nrole in maximum likelihood parametric estimation, as we will see below. This condi-\ntional distribution is not always easy to compute or simulate, and variational Bayes\nprovides a framework under which it can be approximated.\n16.2\nVariational principle\nWe consider a pair of random variables X and Z, where X is considered as “ob-\nserved” and Z is hidden, or “latent”. We will use U = (X,Z) to denote the two\nvariables taken together. We denote as usual by PU the probability law of U, defined\non RU = RX × RZ by PU(A) = P(U ∈A). We will also assume that there exists a mea-\nsure µ on RU that decomposes as a product measure µ = µX × µZ (where µX and µZ\n391\n\n392\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\nare measures on RX and RZ), such that PU ≪µ (πU is absolutely continuous with\nrespect to µ). This implies that PU has a density with respect to µ that we will denote\nfU. If both RX and RZ are discrete, µ is typically the counting measure, and if they\nare both Euclidean space, µ can be the Lebesgue measure on the product.1\nThe variables X and Z then have probability density functions with respect to µX\nad µZ, given by\nfX(x) =\nZ\nRZ\nfU(x,z)µZ(dz)\nand\nfZ(z) =\nZ\nRX\nfU(x,z)µX(dx).\nThe conditional distribution of X given Z = z, denoted PX(· | Z = z), has density\nfX(x | z) = fU(x,z)/fZ(z) with respect to µX and that of Z given X = x, denoted PZ(· |\nX = x), has density fZ(z | x) = fU(x,z)/fX(x) with respect to µZ. We will be mainly\ninterested by approximations of PZ(· | X = x), assuming that PZ and PX(· | Z = z) (and\nhence PU) are easy to compute or simulate.\nWe will use the Kullback-Liebler divergence to quantify the accuracy of the ap-\nproximation. As stated in proposition 4.1, we have\nPZ(· | X = x) = argmin\nν∈M1(RZ)\nKL(ν ∥PZ(· | X = x))\nwhere M1(RZ) denotes the set of all probability distributions on RZ. Note that all\ndistributions ν for which KL(ν ∥πZ(·|X = x)) is finite must be absolutely continuous\nwith respect to µZ and therefore take the form ν = gµZ. One has\nKL(gdµZ∥PZ(·|X = x)) =\nZ\nRZ\nlog g(z)\nfZ(z|x)g(z)µZ(dz)\n=\nZ\nRZ\nlog\ng(z)\nfU(x,z)g(z)µZ(dz) + logfX(x).\n(16.1)\nWe will denote by P(µZ), or just P when there is no ambiguity, the set of all p.d.f.’s g\nwith respect to µZ, i.e., the set of all non-negative measurable functions on RZ with\nR\nRZ g(z)µZ(dz) = 1.\nThe basic principle of variational Bayes methods is to replace P by a subset b\nP\nand to define the approximation\nbPZ(·|X = x) = argmin\ng∈b\nP\nKL(gµZ∥PZ(·|X = x)).\n1The reader unfamiliar with measure theory may want to read this discussion by replacing dµX\nby dx, dµZ by dz and dµU by dxdz, i.e., in the context of continuous probability distributions having\np.d.f.’s with respect to the Lebesgue’s measure.\n\n16.3. EXAMPLES\n393\nFor the approximation to be practical, the set b\nP must obviously be chosen so that\nthe computation of bPZ(·|X = x) is computationally feasible. We now review a few\nexamples, before passing to the EM algorithm and its approximations.\n16.3\nExamples\n16.3.1\nMode approximation\nAssume that RZ is discrete and µZ is the counting measure so that\nKL(gdµZ ∥PZ(· | X = x)) −logfX(x) =\nX\nz∈RZ\nlog\ng(z)\nfU(x,z)g(z),\nthe sum being infinite if there exists z such that ν(z) > 0 and fU(x,z) = 0. Take\nb\nP = {1z : z ∈RZ},\nthe family of all Dirac functions on RZ. Then,\nKL(1z ∥PZ(·|X = x)) −logfX(x) = −logfU(x,z).\nThe variational approximation of PZ(· | X = x) over b\nP therefore is the Dirac measure\nat point(s) z ∈RZ at which fU(x,z) is largest, i.e., the mode(s) of the posterior distri-\nbution. This approximation is often called the MAP approximation (for maximum a\nposteriori).\nIf RZ is, say, Rq and µZ = dz is Lebesgue’s measure, then the previous construc-\ntion does not work because 1z is not a p.d.f. with respect to µZ. In place of Dirac\nfunctions, one can use constant functions on small balls. Let B(z,ϵ) denote the open\nball with radius ϵ, and let |B(z,ϵ)| denote its volume. Let uz,ϵ = 1B(z,ϵ)/|B(z,ϵ)|. Fixing\nϵ, we can consider the set\nb\nP = \buz,ϵ : z ∈Rq\t.\nNow, one has (leaving the computation to the reader)\nKL(uz,ϵdz∥PZ(·|X = x)) −logfX(x) = −log\n \n1\n|B(z,ϵ)\nZ\nB(z,ϵ)\nfU(x,z′)dz′\n!\n.\nThe limit for small ϵ (assuming that fU(x,·) is continuous at z, or defining the limit\nup to sets of measure zero) is −logfU(x,z), justifying again choosing the mode of the\nposterior distribution of Z for the approximation.\nThe mode approximation has some limitations. First, it is in general a very crude\napproximation of the posterior distribution. Second, even with the assumption that\nfU has closed form, this p.d.f. is often difficult to maximize (for example when defin-\ning models over large discrete sets). In such cases, the mode approximation has\nlimited practical use.\n\n394\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\n16.3.2\nGaussian approximation\nLet us still assume that RZ = Rq and that µZ = dz. Let b\nP be the family of all Gaussian\ndistributions N (m,Σ) on Rq. Then, denoting by ϕ(·;m,Σ) the density of N (m,Σ),\nKL(ϕ(·; m,Σ)∥PZ(· | X = x)) −logfX(x) = −q\n2 log2π −q\n2 −1\n2 logdet(Σ)\n−\nZ\nRq logfU(x,z)ϕ(z;m,Σ)dz.\nIn order to provide the best approximation, m and Σ must therefore maximize\nZ\nRq logfU(x,z)ϕ(z;m,Σ)dz + 1\n2 logdet(Σ).\n(16.2)\nThe resulting optimization problem does not have a closed form solution in general\n(see section 18.2.2 for an example in which stochastic gradient methods are used to\nsolve this problem). Another approach that is commonly used in practice is to push\nthe approximation further by replacing logfU(x,z) by its second order expansion\naround its maximum as a function of z. Let m(x) be the posterior mode, i.e., the\nvalue of z at which x 7→logfU(x,z) is maximal, that we will assume to be unique.\nLet H(x) denote the q × q Hessian matrix formed by the second partial derivatives\nof −logfU(x,z) (with respect to z) at z = m(x). This matrix is positive semidefinite\naccording to the choice made for m(x), and we will assume that it is positive definite.\nSince the first derivatives of logfU(x,z) at m(x) must vanish, we have the expansion:\nlogfU(x,z) = logfU(x,m(x)) −1\n2(z −m(x))T H(x)(z −m(x)) + ···\nPlugging the expansion into the integral in (16.2) yields\n−1\n2trace(H(x)Σ) −1\n2(m −m(x))T H(x)(m −m(x)) + 1\n2 logdetΣ.\nTo maximize this expression, one must clearly take m = m(x). Moreover,\n∂Σ (−trace(H(x)Σ) + logdetΣ) = −H(x)T + (ΣT )−1 = −H(x) + Σ−1,\nand we see that one must take Σ = H(x)−1. This provides the Laplace approxima-\ntion [62] of the posterior, N (m(x),H(x)−1), which is practical when the mode and\ncorresponding second derivatives are feasible to compute.\n16.3.3\nMean-field approximation\nThis section generalizes the approach discussed in proposition 14.6 for Markov ran-\ndom fields. Assume that RZ can be decomposed into several components R[1]\nZ ,...,R[K]\nZ ,\n\n16.3. EXAMPLES\n395\nwriting z = (z[1],...,z[K]) (for example, taking K = q and z[i] = z(i), the ith coordinate\nof z if RZ = Rq). Also assume that µZ splits into a product measure µ[1]\nZ ⊗··· ⊗µ[K]\nZ .\nMean-field approximation consists in assuming that probabilities ν in b\nP split into\nindependent components, i.e., their densities g take the form:\ng(z) = g[1](z[1])···g[K](z[K]).\nThen,\nKL(ν ∥PZ(· | X = x)) −logfX(x) =\nK\nX\nj=1\nZ\nR[j]\nZ\nlogg[j](z[j])g[j](z[j])µ[j]\nZ (dz[j])\n−\nZ\nRZ\nlogfU(x,z)\nq\nY\nj=1\ng[j](z[j])µZ(dz).\n(16.3)\nThe mean-field approximation may be feasible when logfU(x,z) can be written as a\nsum of products of functions of each z[j]. Indeed, assume that\nlogfU(x,z) =\nX\nα∈A\nK\nY\nj=1\nψα,j(z[j],x)\n(16.4)\nwhere A is a finite set. To shorten notation, let us denote by ⟨ψ⟩the expectation of a\nfunction ψ with respect to the product p.d.f. g. Then, (16.3) can be written as\nKL(ν∥PZ(· | X = x)) −logfX(x) =\nK\nX\nj=1\n⟨logg(j)(z[j])⟩−\nX\nα∈A\nK\nY\nj=1\n⟨ψα,j(z[j],x)⟩.\nThe following lemma will allow us to identify the form taken by the optimal\np.d.f. g[j].\nLemma 16.1 Let Q be a set equipped with a positive measure µ. Let ψ : Q →R be a\nmeasurable function such that\nCψ\n∆=\nZ\nQ\nexp(ψ(q))µ(dq) < ∞.\nLet\ngψ(q) = 1\nCψ\nexp(ψ(q)).\nLet g be any p.d.f. with respect to µ, and define\nF(g) =\nZ\nQ\n(logg(q) −ψ(q))g(q)µ(dq).\nThen F(gψ) ≤F(g).\n\n396\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\nProof We note that gψ > 0, and that\nKL(g∥gψ) = F(g) + logCψ = F(g) −F(gψ),\nwhich proves the result, since KL divergences are always non-negative.\n■\nApplying this lemma separately to each function g[j] implies that any optimal g\nmust be such that\ng[j](z[j]) ∝exp\n\n\nX\nα∈A\nMα,jψα,j(z[j],x)\n\n\nwith\nMα,j =\nK\nY\nj′=1,j′,j\n⟨ψα,j′(z[j′],x)⟩.\nWe therefore have\n⟨ψα,j(z[j],x)⟩=\nR\nR[j]\nZ ψα,j(z[j],x)exp\n\u0010P\nα′∈A Mα′,jψα′,j(z[j],x)\n\u0011\nµ[j]\nZ (dz[j])\nR\nR[j]\nZ exp\n\u0010P\nα′∈A Mα′,jψα′,j(z[j],x)\n\u0011\nµ[j]\nZ (dz[j])\n(16.5)\nThis specifies a relationship expressing ⟨ψα,j(z[j],x)⟩as a function of the other\nexpectations ⟨ψα′,j′(z(j′),x)⟩for j , j′. These equations put together are called the\nmean-field consistency equations. When these equations can be written explicitly, i.e.,\nwhen the integrals in (16.5) can be evaluated analytically (which is generally the case\nwhen the p.d.f.’s g[j] can be associated with standard distributions), one obtains an\nalgorithm that iterates (16.5) over all α and j until stabilization (each step reducing\nthe objective function in (16.3)).\nLet us retrieve the result obtained in proposition 14.6 using the current formal-\nism. Assume that RX finite and RZ = {0,1}L, where L can be a large number, with\nfU(x,z) = 1\nC exp\n\n\nL\nX\nj=1\nαj(x)z(j) +\nL\nX\ni,j=1,i<j\nβij(x)z(i)z(j)\n\n.\nTake K = L, z[j] = z(j). Applying the previous discussion, we see that g[j] must take\nthe form\ng[j](z(j)) =\nexp\n\u0010\nαj(x)z(j) + P\ni,j βij(x)⟨z(i)⟩z(j)\u0011\n1 + exp\n\u0010\nαj(x) + P\ni,j βij(x)⟨z(i)⟩\n\u0011\nIn particular\n⟨z(j)⟩=\nexp\n\u0010\nαj(x) + P\ni,j βij(x)⟨z(i)⟩\n\u0011\n1 + exp\n\u0010\nαj(x) + P\ni,j βij(x)⟨z(i)⟩\n\u0011\n\n16.4. MAXIMUM LIKELIHOOD ESTIMATION\n397\nproviding the mean-field consistency equations.\nIn this special case, it is also possible to express the objective function as a simple\nfunction of the expectations ⟨z(j)⟩’s. We indeed have, letting ρj = ⟨z(j)⟩,\nX\nz∈RZ\nlogfU(x,z)\nL\nY\nj=1\ng[j](z(j)) = −logC +\nL\nX\nj=1\nαj(x)ρj +\nL\nX\ni,j=1,i<j\nβij(x)ρiρj.\nThe values of ρ1,...,ρL are then obtained by maximizing\nL\nX\nj=1\nαj(x)ρj +\nL\nX\ni,j=1,i<j\nβij(x)ρiρj −\nL\nX\nj=1\n\u0010\nρj logρj + (1 −ρj)log(1 −ρj)\n\u0011\n.\nThe consistency equations express the fact that the derivatives of this expression\nwith respect to each ρj vanish.\n16.4\nMaximum likelihood estimation\n16.4.1\nThe EM algorithm\nWe now consider maximum likelihood estimation with latent variables and use the\nnotation of section 16.2. The main tool is the following obvious consequence of\n(16.1).\nProposition 16.2 One has\nlogfX(x) = max\ng∈P(µZ)\nZ\nRZ\nlog\n fU(x,z)\ng(z)\n!\ng(z)dµZ(z)\nand the maximum is achieved for g(z) = fZ(z | x), the conditional p.d.f. of Z given X = x.\nProof Equation (16.1) implies that\nZ\nRZ\nlog\n fU(x,z)\ng(z)\n!\ng(z)dµZ(z) = logfX(x) −KL(g µZ∥PZ(·|X = x))\nand the r.h.s. is indeed maximum when the Kullback-Liebler divergence vanishes,\nthat is, when g is the p.d.f. of PZ(· | X = x).\n■\nWe will use this proposition for the derivation of the expectation-maximization\n(or EM) algorithm for maximum likelihood with latent variables. We now assume\nthat PU, and therefore fU, is parametrized by θ ∈Θ, and that a training set T =\n\n398\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\n(x1,...,xN) of X is observed. To indicate the dependence in θ, we will write fU(x,z; θ),\nor fZ(z | x; θ). The maximum likelihood estimator (m.l.e.) then maximizes\nℓ(θ) =\nX\nx∈T\nlogfX(x; θ).\nThe EM algorithm is useful when the computation of the m.l.e. for complete obser-\nvations, i.e., the maximization of\nlogfU(x,z; θ)\nwhen both x and z are given, is easy, whereas the same problem with the marginal\ndistribution is hard.\nFrom the proposition, we have:\nX\nx∈T\nlogfX(x; θ) =\nX\nx∈T\nmax\ngx∈P(µZ)\nZ\nRZ\nlog\n fU(x,z; θ)\ngx(z)\n!\ngx(z)µZ(dz)\nTherefore the maximum likelihood requires to compute\nmax\nθ,gx,x∈T\nX\nx∈T\nZ\nRZ\nlog\n fU(x,z; θ)\ngx(z)\n!\ngx(z)µZ(dz).\n(16.6)\nThe maximization can therefore be done by iterating the following two steps.\n1. Given θn, compute\nargmax\ngx,x∈T\nX\nx∈T\nZ\nRZ\nlog\n fU(x,z; θ)\ngx(z)\n!\ngx(z)µZ(dz).\n2. Given g1,...,gN, compute\nargmax\nθ\nX\nx∈T\nZ\nRZ\nlog\n fU(x,z; θ)\ngx(z)\n!\ngx(z)µZ(dz)\n= argmax\nθ\nX\nx∈T\nZ\nRZ\nlog(fU(x,z; θ))gx(z)µZ(dz).\nStep 1. is explicit and its solution is gx(z) = fZ(z | x; θ). Using this, both steps can\nbe grouped together, yielding the EM algorithm.\n\n16.4. MAXIMUM LIKELIHOOD ESTIMATION\n399\nAlgorithm 16.1 (EM algorithm)\nLet a statistical model with density fU(x,z; θ) modeling an observable variable X\nand a latent variable Z be given, and a training set T = (x1,...,xN). Starting with an\ninitial guess of the parameter, θ(0), the EM algorithm iterate the following equation\nuntil numerical stabilization, .\nθn+1 = argmax\nθ′\nX\nx∈T\nZ\nRZ\nlog(fU(x,z; θ′))fZ(z | x; θn)µZ(dz).\n(16.7)\nEquation (16.7) maximizes (in θ′) a function defined as an expectation (for θn), jus-\ntifying the name ”Expectation-Maximization.”\n16.4.2\nApplication: Mixtures of Gaussian\nA mixture of Gaussian (MoG) model was introduced in chapter 4 ((4.4)). We now\nreinterpret it (in a slightly generalized version) as a model with partial observations\nand show how the EM algorithm can be applied. Let ϕ(x; m,Σ) denote the p.d.f. of\nthe d-dimensional multivariate Gaussian distribution with mean m and covariance\nmatrix Σ. We model fX(x; θ) as\nfX(x; θ) =\np\nX\nj=1\nαjϕ(x,; cj,Σj).\nHere, θ contains all sequences α1,...,αp (non-negative numbers that sum to one),\nc1,...,cp ∈Rd and Σ1,...,Σp (d × d positive definite matrices).\nUsing the previous notation, we therefore have RX = Rd, and µX the Lebesgue\nmeasure on that space. The variable Z will take values in RZ = {1,...,p}, with µZ\nbeing the counting measure. We model the joint density function for (X,Z) as\nfU(x,z; θ) = αzϕ(x; cz,Σz).\n(16.8)\nClearly fX is the marginal p.d.f. of fU. One can therefore consider Z as a latent\nvariable, and therefore estimate θ using the EM algorithm.\nWe now make (16.7) explicit for mixtures of Gaussian. For given θ and θ′ and\nx ∈R, let\nUx(θ,θ′) = d\n2 log2π +\nZ\nRZ\nlog(fU(x,z; θ′))fZ(z|x; θ)dµZ(z)\n=\np\nX\nz=1\n\u0012\nlogα′\nz −1\n2 logdetΣ′\nz −1\n2(x −c′\nz)T Σ′\nz\n−1(x −c′\nz)\n\u0013\nfZ(z|x; θ)\n\n400\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\nwith\nfZ(z | x; θ) =\n(detΣz)−1\n2αze−1\n2(x−cz)T Σ−1\nz (x−cz)\nPp\nj=1(detΣj)−1\n2αje−1\n2(x−cj)T Σ−1\nj (x−cj).\nIf θn is the current parameter in the EM, the next one, θn+1 must maximize\nPN\nx∈T Ux(θn,θ′). This can be solved in closed form. To compute α′\n1,...,α′\np, one must\nmaximize\nX\nx∈T\np\nX\nz=1\n(logα′\nz)fZ(z|x; θ)\nsubject to the constraint that P\nz α′\nz = 1. This yields\nα′\nz =\nX\nx∈T\nfZ(z|x; θ)\n.\np\nX\nj=1\nX\nx∈T\nfZ(j|x; θ) = ζz / N\nwith ζz = P\nx∈T fZ(z|x; θ).\nThe centers c′\n1,...,c′\np must minimize P\nx∈T (x −c′\nz)T Σ′\nz\n−1(x −c′\nz)fZ(z|x; θ), which\nyields\nc′\nz = 1\nζz\nX\nx∈T\nxfZ(z|x; θ).\nFinally, Σ′\nz must minimize\nζz\n2 logdetΣ′\nz + 1\n2\nX\nx∈T\n(x −c′\nz)T Σ′\nz\n−1(x −c′\nz)fZ(z|x; θ),\nwhich yields\nΣ′\nz = 1\nζz\nX\nx∈T\n(x −c′\nz)(x −c′\nz)T fZ(z|x; θ).\nWe can now summarize the algorithm.\nAlgorithm 16.2 (EM for Mixture of Gaussian distributions)\n1. Initialize the parameter θ(0) = (α(0),c(0),Σ(0)). Choose a small constant ϵ and\na maximal number of iterations M.\n2. At step n of the algorithm, let θ = θ(n) be the current parameter, writing for\nshort θ = (α,c,Σ).\n3. Compute, for x ∈T and i = 1,...,p\nfZ(i | x; θ) =\n(detΣi)−1\n2αie−1\n2(x−ci)T Σ−1\ni (x−ci)\nPp\nj=1(detΣj)−1\n2αje−1\n2(x−cj)T Σ−1\nj (x−cj)\nand let ζi = P\nx∈T fZ(i|x; θ), i = 1,...,p.\n\n16.4. MAXIMUM LIKELIHOOD ESTIMATION\n401\n4. Let α′\ni = ζi/N.\n5. For i = 1,...,p, let\nc′\ni = 1\nζi\nX\nx∈T\nxfZ(i | x; θ).\n6. For i = 1,...,p, let\nΣ′\ni = 1\nζi\nX\nx∈T\n(x −c′\ni)(x −c′\ni)T fZ(i | x; θ).\n7. Let θ′ = (µ′,c′,Σ′). If |θ′ −θ| < ϵ or n + 1 = M: return θ′ and exit the algorithm.\n8. Set θ(n + 1) = θ′ and return to step 2.\nRemark 16.3 Algorithm 16.2 can be simplified by making restrictions on the model.\nHere are some examples.\n(i) One may restrict to Σi = σ2\ni IdRd to reduce the number of free parameters. Then,\nStep 7 of the algorithm needs to be replaced by:\n(σ′\ni )2 = 1\ndζi\nX\nx∈T\n|x −c′\ni|2fZ(i | x; θ).\n(ii) Alternatively, the model may be simplified by assuming that all covariance ma-\ntrices coincide: Σi = Σ for i = 1,...,p. Then, Step 7 becomes\nΣ′\ni = 1\nN\np\nX\ni=1\nX\nx∈T\n(x −c′\ni)(x −c′\ni)T fZ(i | x; θ).\n♦\n(iii) Finally, one may assume that Σ is known and fixed in the algorithm (usually in\nthe form Σ = σ2IdRd for some σ > 0) so that Step 7 of the algorithm can be removed.\n(iv) One may also assume also that the (prior) class probabilities are known, typi-\ncally set to αi = 1/p for all i, so that Step 4 can be skipped.\n16.4.3\nStochastic approximation EM\nThe stochastic approximation EM (or SAEM) algorithm has been proposed by De-\nlyon et al. [58] (see this reference for convergence results) to address the situation in\nwhich the expectations for the posterior distribution cannot be computed in closed\nform, but can be estimated using Monte-Carlo simulations. SAEM uses a special\n\n402\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\nform of stochastic approximation, different from the SGD algorithm described in\nsection 3.3. It updates, at each step n, an approximate objective function that we\nwill denote λn and a current parameter θ(n). It implements the following iterations:\n\nξ(x)\nn+1 ∼PZ(· | X = x; θn),\nx ∈T\nλn+1(θ′) =\n\u0012\n1 −\n1\nn + 1\n\u0013\nλn(θ′) +\n1\nn + 1\n\u0012X\nx∈T\nlogfU(x,ξ(x)\nn+1 ; θ′) −λn(θ′)\n\u0013\n, θ′ ∈Θ\nθn+1 = argmax\nθ′\nλn+1(θ′)\n(16.9)\nThe second step means that\nλn(θ′) =\nN\nX\nx∈T\n\n\n1\nn\nn\nX\nj=1\nlogfU(x,ξ(x)\nj\n; θ′)\n\n.\nGiven that ξ(x)\nn+1 ∼PZ(· | X = x; θn), one expects this expression to approximate\nX\nx∈T\nZ\nRZ\nlog(fU(x,z; θ′))fZ(z | x; θ)dµZ(z)\nso that the third step of (16.9) can be seen as an approximation of (16.7). Suffi-\ncient conditions under which this actually happens (and θ(n) converges to a local\nmaximizer of the likelihood) are provided in Delyon et al. [58] (see also Kuhn and\nLavielle [112] for a convergence result under more general hypotheses on how ξ is\nsimulated).\nTo be able to run this algorithm efficiently, one needs the simulation of the pos-\nterior distribution to be feasible. Importantly, one also needs to be able to update\nefficiently the function λn. This can be achieved when the considered model belongs\nto an exponential family, which corresponds to assuming that the p.d.f. of U takes\nthe form\nfU(x,z; θ) =\n1\nC(θ) exp\n\u0010\nψ(θ)T H(x,z)\n\u0011\nfor some functions ψ and H. For example, the MoG model of equation (4.4) takes\n\n16.4. MAXIMUM LIKELIHOOD ESTIMATION\n403\nthis form, with\nψ(θ)T =\n\u0012\nlogα1 −1\n2mT\n1 Σ−1\n1 m1 −1\n2 logdetΣ1,...,logαp −1\n2mT\np Σ−1\np mp −1\n2 logdetΣp,\nΣ−1\n1 m1,...,Σ−1\np mp,\nΣ−1\n1 ,...,Σ−1\np\n\u0013\n,\nH(x,z)T =\n\u0012\n1z=1,...,1z=p,\nx1z=1,...,x1z=p,\n−1\n2xxT 1z=1,...,−1\n2xxT 1z=p\n\u0013\nand C(θ) = (2π)pd/2.\nFor such a model, we can replace the algorithm in (16.9) by the more manageable\none:\n\nξ(x)\nn+1 ∼PZ(· | X = x; θn), x ∈T\nη(x)\nn+1 =\n\u0012\n1 −\n1\nn + 1\n\u0013\nη(x)\nn +\n1\nn + 1(H(x,ξ(x)\nn+1) −η(x)\nn )\nλn+1(θ′) = ψ(θ′)T \u0012X\nx∈T\nη(x)\nn+1\n\u0013\n−logC(θ′)\nθn+1 = argmax\nθ′\nλn+1(θ′)\n(16.10)\nWe leave as an exercise the computation leading to the implementation of this algo-\nrithm for mixtures of Gaussian.\n16.4.4\nVariational approximation\nReturning to proposition 16.2 and (16.6), we see that one can make a variational\napproximation of the maximum likelihood by computing\nmax\nθ∈Θ,gx∈b\nP,x∈T\nX\nx∈T\nZ\nRZ\nlog\n fU(x,z; θ)\ngx(z)\n!\ngx(z)µZ(dz),\n(16.11)\nwhere b\nP ⊂P is a class of p.d.f. with respect to µZ. The resulting algorithm is then\nimplemented by iterating the computation of gx, x ∈T, using approximations sim-\nilar to those provided in section 16.3, and maximization in θ for given gx, x ∈T.\nThis variational approximation of the maximum likelihood estimator is therefore\nprovided by the following algorithm.\n\n404\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\nAlgorithm 16.3 (Variational Bayes approximation of the m.l.e.)\nLet a statistical model with density fU(x,z; θ) modeling an observable variable X\nand a latent variable Z be given, and a training set T = (x1,...,xN) be observed. Let\nb\nP be a set of p.d.f. on RZ and define\nbg(·; x,θ) = argmin\ng∈b\nP\nZ\nRZ\nlog\n \ng(z)\nfU(x,z; θ)\n!\ng(z)µZ(dz)\n(assuming that this minimizer is uniquely defined).\nStarting with an initial guess of the parameter, θ0, iterate the following equation\nuntil numerical stabilization:\nθ(n + 1) = argmax\nθ′\nX\nx∈T\nZ\nRZ\nlog(fU(x,z; θ′))bg(z|x; θ(n))µZ(dz).\n(16.12)\nAssume that the distributions in b\nP are also parametrized, denoting their param-\neter by η, belonging to some Euclidean domain H. Let g(·;η) denote the p.d.f. in\nb\nP with parameter η. Letting η = (ηx,x ∈T) denote an element of HT (parameters\nin H indexed by elements of the training set), (16.11) can then be written as the\nmaximization of\nF(θ,η) =\nX\nx∈T\nZ\nRZ\nlog\n fU(x,z; θ)\ng(z;ηx)\n!\ng(z;ηx)µZ(dz).\n(16.13)\nThis expression is amenable to a stochastic gradient ascent implementation. We\nhave\n∂θ\nZ\nRZ\nlog\n fU(x,z; θ)\ng(z;ηx)\n!\ng(z;ηx)µZ(dz) =\nZ\nRZ\n∂θ logfU(x,z; θ)g(z;ηx)µZ(dz)\nand\n∂ηx\nZ\nRZ\nlog\n fU(x,z; θ)\ng(z;ηx)\n!\ng(z;ηx)µZ(dz)\n=\nZ\nRZ\n \n−∂η logg(z;ηx)g(z;ηx) + log\n fU(x,z; θ)\ng(z;ηx)\n!\n∂ηg(z;ηx)\n!\nµZ(dz)\n=\nZ\nRZ\nlog\n fU(x,z; θ)\ng(z;ηx)\n!\n∂η logg(z;ηx)g(z;ηx)µZ(dz)\nHere, we have used the fact that, for all η,\nZ\nRZ\n∂η logg(z;η)g(z;η)µZ(dz) =\nZ\nRZ\n∂ηg(z;η)µZ(dz) = 0\n\n16.5. REMARKS\n405\nsince\nR\nRZ g(x,η)µZ(dz) = 1.\nDenote by πη the probability distribution of the random variable Z taking val-\nues in R|T|\nZ obtained by sampling Z = (Zx,x ∈T ) such that the components Zx are\nindependent and with p.d.f. g(·;ηx) with respect to µZ. Define\nΦ1(θ,z) =\nX\nx∈T\n∂θ logfU(x,zx ; θ)\nand\nΦ2(theta,η,z) =\nX\nx∈T\nlog\n fU(x,z; θ)\ng(z;ηx)\n!\n∂η logg(zx;ηx).\nThen, following section 3.3, one can maximize (16.13) using the algorithm\n(θn+1 = θn + γn+1Φ1(θn,Z n+1)\nηn+1 = ηn + γn+1Φ2(θn,ηn,Z n+1)\n(16.14)\nwhere Z n+1 ∼πηn.\nAlternatively (for example when T is large), one can also sample from x ∈T at\neach update. This would require defining πη as the distribution on T ×RZ with p.d.f.\nϕη(x,z) = g(z;ηx)/N, where N = |T|. One can now use\nΦ1(θ,x,z) = ∂θ logfU(x,z; θ)\nand\nΦ2(θ,η,z) = log\n fU(x,z; θ)\ng(z;η)\n!\n∂η logg(z;η),\none can use\n\nθn+1 = θn + γn+1∂θ logfU(Xn+1,Zn+1 ; θn)\nηn+1,Xn+1 = ηn,Xn+1 + γn+1 log\n fU(Xn+1,Zn+1 ; θn)\ng(Zn+1;ηn,Xn+1)\n!\n∂η logg(Zn+1;ηn,Xn+1)\n(16.15)\nwith (Xn+1,Zn+1) ∼πηn. Sampling from a single training sample at each step can be\nreplaced by sampling from a minibatch with obvious modifications.\n16.5\nRemarks\n16.5.1\nVariations on the EM\nBased on the formulation of the EM as the solution of (16.6), it should be clear that\nsolving (16.7) at each step can be replaced by any update of the parameter that in-\ncreases (16.6). For example, (16.7) can be replaced by a partial run of a gradient\n\n406\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\nascent algorithm, stopped before convergence. One can also use a coordinate as-\ncent strategy. Assume that θ can be split into several components, say two, so that\nθ = (θ(1),θ(2)). Then, (16.7) may then be split into\nθ(1)\nn+1 = argmax\nθ(1)\nX\nx∈T\nZ\nRZ\nlog\n\u0012\nfU(x,z; θ(1),θ(2)\nn )\n\u0013\nfZ(z | x; θ(n))µZ(dz)\nθ(2)\nn+1 = argmax\nθ(2)\nX\nx∈T\nZ\nRZ\nlog\n\u0012\nfU(x,z; θ(1)\nn+1,θ(2))\n\u0013\nfZ(z | x; θ(n))µZ(dz).\nDoing so is, in particular, useful when both these steps are explicit, but not (16.7).\n16.5.2\nDirect minimization\nWhile the EM algorithm is widely used in the context of partial observations, it is\nalso possible to make explicit the derivative of\nlogfX(x; θ) = log\nZ\nRZ\nfU(x,z; θ)µZ(dz)\nwith respect to the parameter θ. Indeed, differentiating the integral and writing\n∂θfU = fU∂θ logfU, we have\n∂θ logfX(x; θ) =\nZ\nRZ\n∂θ logfU(x,z; θ)fU(x,z; θ)\nfX(x; θ) µZ(dz)\n=\nZ\nRZ\n∂θ logfU(x,z; θ)fZ(z|x,θ)µZ(dz).\nIn other terms, the derivative of the log-likelihood of the observed data is the con-\nditional expectation of the derivative of the log-likelihood of the full data given\nthe observed data. When computable, this expression can be used with standard\ngradient-based optimization methods, such as those described in chapter 3. This\nexpression is also amenable to a stochastic gradient ascent algorithm, namely\nθn+1 = θn + γn+1\nX\nx∈T\n∂θfU(x,Zn+1,x,θn)\n(16.16)\nwhere Zn+1,x follows the distribution with density fZ(·|x,θn) with respect to µZ. An\nalternative SGA implementation can use the discussion in section 16.4.4, with the\ndensity gηx replaces by fZ(·|x,ηx), which leads to\n\nθn+1 = θn + γn+1\nX\nx∈T\n∂θ logfU(x,Zn+1,x,θn)\nηn+1,x = ηn,x −γn+1∂ηx logfZ(Zn+1,x |x,ηx),\nx ∈T\nwhere Zn+1,x follows the distribution with density fZ(·|x,ηn,x).\n\n16.5. REMARKS\n407\n16.5.3\nProduct measure assumption\nWe have worked, in this chapter, under the assumption that πU was absolutely con-\ntinuous with respect to a product measure µU = µX ⊗µZ. This is not a mild as-\nsumption, as it fails to include some important cases, for example when X and Z\nhave some deterministic relationship, the simplest instance being when X = F(Z)\nfor some function F. In many cases, however, one can make simple transformations\non the model that will make it satisfy this working assumption. For example, if\nX = F(Z), one can generally split Z into Z = (Z(1),Z(2)) so that the equation X = F(Z)\nis equivalent to Z(2) = G(X,Z(1)) for some function G. One can then apply the dis-\ncussion above to U = (X,Z(1)) instead of U = (X,Z).\nIf one is ready to step further into measure theoretic concepts, however, one can\nsee that this product decomposition assumption was in fact unnecessary. Indeed,\none can assume that the measure µU can “disintegrate” in the following sense: there\nexists, a measure µX on RX and, for all x ∈RX, a measure µZ(·|x) on RZ such that,\nfor all functions ψ defined on RU,\nZ\nRU\nψ(x,z)µU(dx,dz) =\nZ\nRX\nZ\nRZ\nψ(x,z)µZ(dz|x)µX(dx).\nThis is in fact a fairly general situation [34] as soon as one assumes that µU(R) is\nfinite (which is not a real loss of generality as one can reduce to this case by replacing\nif needed µU by an equivalent probability distribution).\nWith this assumption, the marginal distribution of X had a p.d.f. with respect to\nµX given by\nfX(x) =\nZ\nRZ\nfU(x,z)µZ(dz|x)\nand the conditional distributions PZ(· | x) have a p.d.f. relative to µZ(· | x) given by\nfZ(z | x) = fU(x,z)\nfX(x) .\nThe computations and approximations made earlier in this chapter can then be ap-\nplied with essentially no modification.\n\n408\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\n\nChapter 17\nLearning Graphical Models\nWe discuss, in this chapter, several methods designed to learn parameters of graph-\nical models, starting with the somewhat simpler case of Bayesian networks, than\npassing to Markov random fields on loopy graphs.\n17.1\nLearning Bayesian networks\n17.1.1\nLearning a Single Probability\nSince Bayesian networks are specified by probabilities and conditional probabilities\nof configurations of variables, we start with a discussion of the basic problem of\nestimating discrete probability distributions.\nThe obvious way to estimate the probability of an event A based on a series of N\nindependent experiments is by using relative frequencies\nfA = #{A occurs}\nN\n.\nThis estimation is unbiased (E(fA) = P(A)) and its variance is P(A)(1−P(A))/N. This\nimplies that the relative error δA = fA/P(A) −1 has zero mean and variance\nσ2 = 1 −P(A)\nNP(A) .\nThis number can clearly become very large when P(A) ≃0. In particular, when\nP(A) is small compared to 1/N, the relative frequency will often be fA = 0, leading\nto the false conclusion that A is not just rare, but impossible. If there are reasons to\nexpect beforehand that A is indeed possible, it is important to inject this prior belief\nin the procedure, which suggest using Bayesian estimation methods.\n409\n\n410\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nThe main assumption for these methods is to consider the unknown probability,\np = P(A), as a random variable, yielding a generative process in which a random\nprobability is first obtained, and then N instances of A or not-A are generated using\nthis probability.\nAssume that the “prior distribution” of p (which determines a prior belief) has\na p.d.f. q (with respect to Lebesgue’s measure) on the unit interval. Given on N in-\ndependent observations of occurrences of A, each following a Bernoulli distribution\nb(p), the joint likelihood of all involved variables is given by\n N\nk\n!\npk(1 −p)N−kq(p),\nwhere k is the number of times the event A has been observed.\nThe conditional density of p given the observation (k occurrences of A) is called\nthe posterior distribution. Here, it is given by\nq(p | k) = q(p)\nCk\npk(1 −p)N−k\nwhere Ck is a normalizing constant. If there was no specific prior knowledge on p\n(so that q(p) = 1), the resulting distribution is a beta distribution with parameters\nk + 1 and N −k + 1, the beta distribution being defined as follows.\nDefinition 17.1 The beta distribution with parameters a and b (abbreviated β(a,b)) has\ndensity with respect to Lebesgue’s measure\nρ(t) = Γ(a + b)\nΓ(a)Γ(b)ta−1(1 −t)b−1 if t ∈[0,1]\nand ρ(t) = 0 otherwise, with\nΓ(x) =\nZ ∞\n0\ntx−1e−tdt.\nFrom the definition of a beta distribution, it is clear also that, if we choose the\nprior to be β(a + 1,ν −a + 1) then the posterior is β(k + a + 1,N + ν −(k + a) + 1).\nThe posterior therefore belongs to the same family of distributions as the prior, and\none says that the beta distribution is a conjugate prior for the binomial distribution.\nThe mode of the posterior distribution (which is the maximum a posteriori (MAP)\nestimator) is given by\nˆp = k + a\nN + ν .\nThis estimator now provides a positive value even if k = 0. By selecting a and ν,\none therefore includes the prior belief that p is positive.\n\n17.1. LEARNING BAYESIAN NETWORKS\n411\n17.1.2\nLearning a Finite Probability Distribution\nNow assume that F is a finite space and that we want to estimate a probability distri-\nbution p = (p(x),x ∈F) using a Bayesian approach as above. We cannot use the previ-\nous approach to estimate each p(x) separately, since these probabilities are linked by\nthe fact that they sum to 1. We can however come up with a good (conjugate) prior,\nidentified, as done above, by computing the posterior associated to a uniform prior\ndistribution.\nLetting Nx be the number of times x ∈F is observed among N independent sam-\nples of a random variable X with distribution PX(·) = p(·), the joint distribution of\n(Nx,x ∈F) is multinomial, given by\nP(Nx,x ∈F | p(·)) =\nN!\nQ\nx∈F Nx!\nY\nx∈F\np(x)Nx.\nThe posterior distribution of p(·) given the observations with a uniform prior is pro-\nportional to Q\nx∈F p(x)Nx. It belongs to the family of Dirichlet distributions, described\nin the following definition.\nDefinition 17.2 Let F be a finite set and SF be the simplex defined by\nSF =\n(p(x),x ∈F) : p(x) ≥0,x ∈F and\nX\nx∈F\np(x) = 1\n.\nThe Dirichlet distribution with parameters a = (a(x),x ∈F) (abbreviated Dir(a)) has den-\nsity\nρ(p(·)) =\nΓ(ν)\nQ\nx∈F Γ(a(x))\nY\nx∈F\np(x)a(x)−1, if x ∈SF\nand 0 otherwise, with ν = P\nx∈F a(x).\nNote that, if F has cardinality 2, the Dirichlet distribution coincides with the beta\ndistribution. Similarly to the beta for the binomial, and almost by construction, the\nDirichlet distribution is a conjugate prior for the multinomial. More precisely, if the\nprior distribution for p(·) is Dir(1+a(x),x ∈F), then the posterior after N observations\nof X is Dir(1 + Nx + a(x),x ∈F), and the MAP estimator is given by\nˆp(x) = Nx + a(x)\nN + ν\nwith ν = P\nx∈F a(x).\n\n412\nCHAPTER 17. LEARNING GRAPHICAL MODELS\n17.1.3\nConjugate Prior for Bayesian Networks\nWe now consider a Bayesian network on the set F (V ) containing configurations x =\n(x(s),s ∈V ) with x(s) ∈Fs. We want to estimate the conditional probabilities in the\nrepresentation\nP(X = x) =\nY\ns∈V\nps(x(pa(s)),x(s)).\nAssume that N independent observations of X have been made. Define the counts\nNs(x(s),x(pa(s))) to be the number of times the observation x({s}∪pa(s)) has been made.\nThen, it is straightforward to see that, assuming a uniform prior for the ps, their\nposterior distribution is proportional to\nY\ns∈V\nY\nx(pa(s))∈Fpa(s)\nY\nx(s)∈Fs\nps(x(pa(s)),x(s))Ns(x(s),x(s−)).\nThis implies that, for the posterior distribution, the conditional probabilities\nps(x(pa(s)),·) are independent and follow a Dirichlet distribution with parameters\n1 + Ns(x(s),x(pa(s))), x(s) ∈Fs.\nSo, independent Dirichlet distributions indexed by configurations of parents of\nnodes provide a conjugate prior for the general Bayesian network model. This prior\nis specified by a family of positive numbers\n\u0010\nas(x(s),x(pa(s))),s ∈V ,x(s) ∈Fs,x(pa(s)) ∈F (pa(s))\n\u0011\n,\n(17.1)\nyielding a prior probability proportional to\nY\ns∈V\nY\nx(pa(s))∈Fpa(s)\nY\nx(s)∈Fs\nps(x(pa(s)),x(s))as(x(s),x(s−))−1.\nand a MAP estimator\nˆps(x(pa(s)),x(s)) = Ns(x(s),x(pa(s))) + as(x(s),x(s−))\nNs(x(s−)) + νs(x(s−))\n(17.2)\nwhere Ns(x(pa(s))) = P\nx(s)∈Fs Ns(x(s),x(pa(s))) and νs(x(pa(s))) = P\nx(s)∈Fs as(x(s),x(pa(s))).\nOne can restrict the huge class of coefficients described by (17.1) to a smaller\nclass by imposing the following condition.\nDefinition 17.3 One says that the family of coefficients\na = (as(x(s),x(pa(s))),s ∈V ,x(s) ∈Fs,x(pa(s)) ∈F (pa(s))),\nis consistent if there exists a positive scalar ν and a probability distribution P′ on F (V )\nsuch that\nas(x(s),x(pa(s))) = νP′\n{s}∪pa(s)(x({s}∪pa(s))).\n\n17.1. LEARNING BAYESIAN NETWORKS\n413\nThe class of products of Dirichlet distributions with consistent families of coeffi-\ncients still provides a conjugate prior for Bayesian networks (the proof being left to\nthe reader). Within this class, the simplest choice (and most natural in the absence\nof additional information) is to assume that P′ is uniform, so that\nas(x(s),x(pa(s))) =\nν′\n|F ({s} ∪pa(s))|.\n(17.3)\nWith this choice, ν′ is the only parameter that needs to be specified. It is often called\nthe equivalent sample size for the prior distribution.\nWe can see from (17.2) that using a prior distribution is quite important for\nBayesian networks, since, when the number of parents increases, some configura-\ntions on F (pa(s)) may not be observed, resulting in an undetermined value for the\nratio\nNs(x(s),x(pa(s)))/Ns(x(s−)),\neven though, for the estimated model, the probability of observing x(pa(s)) may not\nbe zero.\n17.1.4\nStructure Scoring\nGiven a prior defined as a family of Dirichlet distributions associated to a = (as(x(s),x(pa(s)))\nfor s ∈V ,x(s) ∈Fs,x(pa(s)) ∈F (pa(s)), the joint density of the observations and param-\neters is given by\nP(x,θ) =\nY\ns,x(pa(s))\nD(as(·,x(pa(s))))\nY\ns,x(s),x(pa(s))\np(x(pa(s)),x(s))Ns(x(s),x(pa(s)))+as(x(s),x(pa(s)))−1\nwith\nD(a(λ),λ ∈F) =\nΓ(ν)\nQ\nλ Γ(a(λ))\nand ν = P\nλ a(λ). Here, θ represents the parameters of the model, i.e., the conditional\ndistributions that specify the Bayesian network. Note that P(x,θ) is a density over\nthe product space F (V ) × Θ where Θ is the space of all these conditional distribu-\ntions. The marginal of this likelihood over all possible parameters, i.e.,\nP(x) =\nZ\nP(x,θ)dθ\nprovides the expected likelihood of the sample relative to the distribution of the pa-\nrameters, and only depends on the structure of the network. In our case, integrating\nwith respect to θ yields\nlogP(x) =\nX\ns,xpa(s)\nlog\nD(as(·,x(pa(s))))\nD(as(·,x(pa(s))) + Ns(·,x(pa(s)))).\n\n414\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nLetting\nγ(s,pa(s)) =\nX\nx(pa(s))\nlog\nD(as(·,x(pa(s))))\nD(as(·,x(pa(s))) + Ns(·,x(pa(s)))),\nthe decomposition\nlogP(x) =\nX\ns∈V\nγ(s,pa(s))\nexpresses this likelihood as a sum of “scores” (associated to each node and its par-\nents), which depends on the observed sample. The scores that are computed above\nare often called Bayesian scores because they derive from a Bayesian construction.\nOne can also consider simpler scores, such as penalized likelihood:\nγ(s,pa(s)) = −\nX\nx(pa(s))\nˆH(X(s) | X(pa(s)))|F (pa(s))| −ρ|pa(s)|,\nwhere ˆH is the conditional entropy for the empirical distribution based on observed\nsamples. Structure learning algorithms [144, 108] are designed to optimize such\nscores.\n17.1.5\nReducing the Parametric Dimension\nIn the previous section, we estimated all conditional probabilities intervening in the\nnetwork. This is obviously a lot of parameters and, even with a regularizing prior,\nthe estimated values are likely to be be inaccurate for small sample sizes. It then\nbecomes desirable to simplify the parametric complexity of the model.\nWhen the sets Fs are not too large, which is common in practice, the paramet-\nric explosion is due to the multiplicity of parents, since the number of conditional\nprobabilities ps(x(pa(s)),·) grows exponentially with |pa(s)|. One way to simplify this\nis to assume that the conditional probability at s only depends on x(pa(s)) via some\n“global-effect” statistic gs(x(pa(s))). The idea, of course, is that the number of values\ntaken by gs should remain small, even if the number of parents is large.\nExamples of some functions gs can be max(x(t),t ∈pa(s)), or the min, or some\nsimple (quantized) function of the sum. With binary variables (Fs = {0,1}), logical\noperators are also available (“and”, “or”, “xor”), as well as combinations of them.\nThe choice made for the functions gs is part of building the model, and would rely on\nthe specific context and prior information on the process, which is always important\nto account for, in any statistical problem.\nOnce the gs’s are fixed, learning the network distribution, which is now given by\nπ(x) =\nY\ns∈V\nps(gs(x(pa(s))),x(s))\n\n17.2. LEARNING LOOPY MARKOV RANDOM FIELDS\n415\ncan be done exactly as before, the parameters being all ps(w,λ),λ ∈Fs,w ∈Ws, where\nWs is the range of gs, and Dirichlet priors can be associated to each ps(w,·) for s ∈V\nand w ∈Ws. The counts provided in (17.3) now can be chosen as\nas(xs,w) =\nν′\n|F||g−1\ns (w)|\n.\n(17.4)\n17.2\nLearning Loopy Markov Random Fields\nLike everything else, parameter estimation for loopy networks is much harder than\nwith trees or Bayesian networks. There is usually no closed form expression for\nthe estimators, and their computation relies on more or less tractable numerical\nprocedures.\n17.2.1\nMaximum Likelihood with Exponential Models\nIn this section, we consider a parametrized model for a Gibbs distribution\nπθ(x) = 1\nZθ\nexp(−θT U(x))\n(17.5)\nwhere θ is a d-dimensional parameter and U is a function from F (V ) to Rd. For\nexample, if π is an Ising model with\nπ(x) = 1\nZ exp\n\u0010\nα\nX\ns∈V\nx(s) + β\nX\ns∼t\nx(s)x(t)\u0011\n,\nthen θ = (α,β) and U(x) = −(P\ns x(s),P\ns∼t x(s)x(t)). Most of the Markov random fields\nmodels that are used in practice can be put in this form. The constant Zθ in (17.5) is\nZθ =\nX\nx∈F (V )\nexp(−θT U(x))\nand is usually not computable.\nNow, assume that an N-sample, x1,...,xN, is observed for this distribution. The\nmaximum likelihood estimator maximizes\nℓ(θ) = 1\nN\nN\nX\nk=1\nlogπθ(xk) = −θT ¯UN −logZθ\nwith ¯UN = (U(x1) + ··· + U(xN))/N.\nWe have the following proposition, which is a well-known property of exponen-\ntial families of probabilities.\n\n416\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nProposition 17.4 The log-likelihood, ℓ, is a concave function of θ, with\n∇ℓ(θ) = Eθ(U) −¯UN\n(17.6)\nand\n∇2ℓ(θ) = −Varθ(U)\n(17.7)\nwhere Eθ denotes the expectation with respect to πθ and Varθ the covariance matrix under\nthe same distribution.\nWe skip the proof, which is just computation. This proposition implies that a\nlocal maximum of θ 7→ℓ(θ) must also be global. Any such maximum must be a\nsolution of\nEθ(U) = ¯UN(x0)\nand conversely. There are some situations in which the maximum does not exist, or\nis not unique. Let us first discuss the second case.\nIf several solutions exist, the log-likelihood cannot be strictly concave: there must\nexist at least one θ for which Varθ(U) is not definite. This implies that there exists a\nnonzero vector u such that varθ(uT U) = uT Varθ(U)u = 0. This is only possible when\nuT U(x) = cst for all x ∈FV . Conversely, if this is true, Varθ(U) is degenerate for all θ.\nSo, the non-uniqueness of the solutions is only possible when a deterministic\naffine relation exists between the components of U, i.e., when the model is over-\ndimensioned. Such situations are usually easily dealt with by removing some pa-\nrameters. In all other cases, there exists at most one maximum.\nFor a concave function like ℓto have no maximum, there must exist what is called\na direction of recession [167], which is a direction α ∈Rd such that, for all θ, the\nfunction t 7→ℓ(θ + tα) is increasing. In this case the maximum is attained “at infin-\nity”. Denoting Uα(x) = αT U(x), the derivative in t of ℓ(θ + tα) is\nEθ+tα(Uα) −¯Uα\nwhere ¯Uα = αT ¯UN. This derivative is positive for all t if and only if\n¯Uα = U∗\nα := min{Uα(x),x ∈F (V )}\n(17.8)\nand Uα is not constant. To prove this, assume that the derivative is positive. Then\nUα is not constant (otherwise, the derivative would be zero). Let F ∗\nα ⊂F (V ) be the\n\n17.2. LEARNING LOOPY MARKOV RANDOM FIELDS\n417\nset of configurations x for which Uα(x) = U∗\nα. Then\nEθ+tα(Uα)\n=\nP\nx∈F (V ) Uα(x)exp(−θT U(x) −tUα(x))\nP\nx∈F (V ) exp(−θT U(x) −tUα(x))\n=\nP\nx∈F (V ) Uα(x)exp(−θT U(x) −t(Uα(x) −U∗\nα))\nP\nx∈F (V ) exp(−θT U(x) −t(Uα(x) −U∗α))\n=\nU∗\nα\nP\nx∈F ∗α exp(−θT U(x)) + P\nx<F ∗α Uα(x)exp(−θT U(x) −t(Uα(x) −U∗\nα))\nP\nx∈F ∗α exp(−θT U(x)) + P\nx<F ∗α exp(−θT U(x) −t(Uα(x) −U∗α))\n.\nWhen t tends to +∞, the sums over x < F ∗\nα tend to 0, which implies that Eθ+tα(Uα)\ntends to U∗\nα. So, if Eθ+tα(Uα) −¯Uα > 0 for all t, then ¯Uα = U∗\nα and Uα is not constant.\nThe converse statement is obvious.\nAs a conclusion, the function ℓhas a finite maximum if and only if there is no\ndirection α ∈Rd such that αT (U(x)−¯UN) ≤0 for all x ∈F (V ). Equivalently, ¯UN must\nbelong to the interior of the convex hull of the finite set\n{U(x),x ∈F (V )} ⊂Rd.\nIn such a case, that we hereafter assume, computing the maximum likelihood\nestimator boils down to solving the equation\nEθ(U) = ¯UN.\nBecause the maximization problem is concave, we know that numerical algorithms\nsuch as gradient ascent,\nθ(t + 1) = θ(t) + ϵ(Eθ(t)(U) −¯UN),\n(17.9)\nconverge to the optimal parameter. Unfortunately, the computation of the expec-\ntations and covariance matrices can only be made explicitly for acyclic models, for\nwhich parameter estimation is not a problem anyway. For general loopy graphical\nmodels, the expectation can be estimated iteratively using Monte-Carlo methods. It\nturns out that this estimation can be synchronized with gradient descent to obtain a\nconsistent algorithm, which is described in the next section.\n17.2.2\nMaximum likelihood with stochastic gradient ascent\nAs remarked above, for fixed θ, we have designed, in chapter 14, Markov chain\nMonte Carlo algorithms that asymptotically sample form πθ. Select one of these\nalgorithms, and let pθ be the corresponding transition probabilities for a given θ, so\n\n418\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nthat pθ(x,y) = P(Xn+1 = y | Xn = x) for the sampling chain. Then, define the iterative\nalgorithm, initialized with arbitrary θ0 and x0 ∈F (V ), that loops over the following\ntwo steps.\n(SG1) Sample from the distribution pθt(xt,·) to obtain a new configuration xt+1.\n(SG2) Update the parameter using\nθt+1 = θt + γt+1(U(xt+1) −¯UN).\n(17.10)\nThis algorithm differs from the situation considered in section 3.3 in that the\ndistribution of the sampled variable xt+1 depends on both the current parameter θt\nand on the current variable xt. Convergence requires additional constraints on the\nsize of the gains γ(t) and we have the following theorem [206].\nTheorem 17.5 If pθ corresponds to the Gibbs sampler or Metropolis algorithm, and γt+1 =\nϵ/(t+1) for small enough ϵ, the algorithm that iterates (SG1) and (SG2) converges almost\nsurely to the maximum likelihood estimator.\nThe speed of convergence of such algorithms depends both on the speed of con-\nvergence of the Monte-Carlo sampling and of the original gradient ascent. The latter\ncan be improved somewhat with variants similar to those discussed in section 3.3,\nfor example by choosing data-adaptive gains as in the ADAM algorithm.\n17.2.3\nRelation with Maximum Entropy\nThe maximum likelihood estimator is closely related to what is called the maximum\nentropy extension of a set of constraints. Let the function U from F (V ) to Rd be\ngiven. An element u ∈Rd is said to be a consistent assignment for U if there exists\na probability distribution π on F (V ) such that Eπ(U) = u. An example of consistent\nassignment is any empirical average ¯U based on a sample (x(1),...,x(N)), since ¯U =\nEπ(U) for\nπ = 1\nN\nN\nX\nk=1\nδx(k).\nGiven U and a consistent assignment, u, the associated maximum entropy exten-\nsion is defined as a probability distribution π maximizing the entropy, H(π), subject\nto the constraint Eπ(U) = u. This is a convex optimization problem, with constraints\n\nX\nx∈F (V )\nπ(x) = 1\nX\nx∈F (V )\nUj(x)π(x) = uj,j = 1,...,d\nπ(x) ≥0,x ∈F (V )\n(17.11)\n\n17.2. LEARNING LOOPY MARKOV RANDOM FIELDS\n419\nBecause the entropy is strictly convex, there is a unique solution to this problem.\nWe first discuss non-positive solutions, i.e., solutions for which π(x) = 0 for some x.\nAn important fact is that, if, for a given x, there exists π1 such that Eπ1(U) = u and\nπ1(x) > 0, then the optimal π must also satisfy π(x) > 0. This is because, if π(x) = 0,\nthen, letting πϵ = (1 −ϵ)π + ϵπ1, we have Eπϵ(U) = u since this constraint is linear,\nπϵ(x) > 0 and\nH(πϵ) −H(π)\n=\n−\nX\ny,π(y)>0\n(πϵ(y)logπϵ(y) −π(y)logπ(y))\n−\nX\ny,π(y)=0\nϵπ1(y)(log(ϵ) + logπ1(y))\n=\n−ϵlogϵ\nX\ny,π(y)=0\nπ1(y) + O(ϵ)\nwhich is positive for small enough ϵ, contradicting the fact that π is a maximizer.\nIntroduce the set Nu containing all configurations x ∈F (V ) such that π(x) = 0\nfor all π such that Eπ(U) = u. Then we know that the maximum entropy extension\nsatisfies π(x) > 0 if x < Nu. Introduce Lagrange multipliers θ0,θ1,...,θd for the d + 1\nequality constraints in (17.11), and the Lagrangian\nL = H(π) +\nX\nx∈F (V )\\Nu\n(θ0 + θT U(x))π(x)\nin which we have set θ = (θ1,...,θd), we find that the optimal π must satisfy\n\nlogπ(x) = −θ0 −1 −θT U(x)\nX\nx\nπ(x) = 1\nEπ(U) = ¯u\nIn other terms, the maximum entropy extension is characterized by\nπ(x) = 1\nZθ\nexp(−θT U(x))1N cu(x)\nand Eπ(U) = u.\nIn particular, if Nu = ∅, then the maximum entropy extension is positive. If,\nin addition, u = ¯U for some observed sample, then it coincides with the maximum\nlikelihood estimator for (17.5). Notice that, in this case, the condition Nu , ∅coin-\ncide with the condition that there exists α such that αT U(x) ≥αT u for all x, with\nαT U(x) not constant. Indeed, assume that the latter condition is true. Then, if\nEπ(U) = u, then Eπ(αT U) = αT u, which is only possible if π(x) = 0 for all x such\n\n420\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nthat αT U(x) < αT u. Such x’s exist by assumption, and therefore Nu , ∅. Conversely,\nassume Nu , ∅. If condition (17.8) is not satisfied, then we have shown when dis-\ncussing maximum likelihood that an optimal parameter for the exponential model\nwould exist, leading to a positive distribution for which Eπ(U) = u, which is a con-\ntradiction.\n17.2.4\nIterative Scaling\nIterative scaling is a method that is well-adapted to learning distributions given by\n(17.5), when U can be interpreted as a random histogram, or a collection of them.\nMore precisely, assume that for all x ∈F (V ), one has\nU(x) = (U1(x),...,Uq(x))\nwith\nq\nX\nj=1\nUj(x) = 1 and Uj(x) ≥0.\nLet the parameter be given by θ = (θ1,...,θq). Assume that x1,...,xN have been\nobserved, and let u ∈Rd be a consistent assignment for U, with uj > 0 for j = 1,...,d\nand such that Nu = ∅. Iterative scaling computes the maximum entropy extension of\nEπ(U) = u, that we will denote π∗. It is supported by the following lemma.\nLemma 17.6 Let π be a probability on F (V ) with π > 0 and define\nπ′(x) = π(x)\nζ\nd\nY\nj=1\n \nuj\nEπ(Uj)\n!Uj(x)\nwhere ζ is chosen so that π′ is a probability. Then π′ > 0 and\nKL(π∗∥π′) −KL(π∗∥π) ≤−KL(u∥Eπ(U)) ≤0\n(17.12)\nProof Note that, since π > 0, Eπ(Uj) must also be positive for all j, since Eπ(Uj) = 0\nwould otherwise imply Uj = 0 and uj = 0 for u to be consistent. So, π′ is well defined\nand obviously positive.\nWe have\nKL(π∗∥π′) −KL(π∗∥π)\n=\nlogζ −\nX\nx∈F (V )\nπ∗(x)\nd\nX\nj=1\nUj(x)log\nuj\nEπ(Uj)\n=\nlogζ −\nd\nX\nj=1\nuj log\nuj\nEπ(Uj)\n=\nlogζ −KL(u∥Eπ(U)).\n\n17.2. LEARNING LOOPY MARKOV RANDOM FIELDS\n421\n(We have used the identity Eπ∗(U) = u.) So it suffices to prove that ζ ≤1. We have\nζ\n=\nX\nx∈F (V )\nπ(x)\nd\nY\nj=1\n \nuj\nEπ(Uj)\n!Uj(x)\n≤\nd\nX\nj=1\nX\nx∈F (V )\nπ(x)Uj(x)\nuj\nEπ(Uj)\n=\nd\nX\nj=1\nEπ(Uj)\nuj\nEπ(Uj) = 1,\nwhich proves the lemma (we have used the fact that, for xi, wi positive numbers with\nP\ni wi = 1, one has Q\ni xwi\ni\n≤P\ni wixi, which is a consequence of the concavity of the\nlogarithm).\n■\nConsider the iterative algorithm\nπn+1(x) = πn(x)\nζn\nd\nY\nj=1\n \nuj\nEπn(Uj)\n!Uj(x)\ninitialized with a uniform distribution. Equivalently, using the exponential formu-\nlation, define, for j = 1,...,d,\nθn+1,j = θn,j + log\nEθn(Uj)\nuj\n+ KL(u∥Eθn(U)),\n(17.13)\nwith πθ given by (17.5), initialized with θ0 = 0. Note that adding a term that is\nindependent of j to θ does not change the value of πθ, because the Uj’s sum to 1.\nThe model is in fact overparametrized, and the addition of the KL divergence in\n(17.13) ensures that Pd\ni=1 uiθi = 0 at all steps.\nThis algorithm always reduces the Kullback-Leibler distance to the maximum en-\ntropy extension. This distance being always positive, it therefore converges to a limit,\nwhich, still according to lemma 17.6, is only possible if KL(u∥Eπn(U)) also tends to\n0, that is Eπn(U) →u. Since the space of probability distributions is compact, the\nHeine-Borel theorem implies that the sequence πθn has at least one accumulation\npoint, that we now identify. If π is such a point, one must have Eπ(U) = u. More-\nover, we have π > 0, since otherwise KL(π∗∥π) = +∞. To prove that π = π∗(and\ntherefore the limit of the sequence), it remains to show that it can be put in the form\n(17.5). For this, define the vector space V of functions v : F (V ) →R which can be\nwritten in the form\nv(x) = α0 +\ng\nX\nj=1\nαjUj(x).\n\n422\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nSince logπθn ∈V for all n, so is its limit, and this proves that logπ belongs to V. We\nhave obtained the following proposition.\nProposition 17.7 Assume that for all x ∈F (V ), one has U(x) = (U1(x),...,Ud(x)) with\nd\nX\nj=1\nUj(x) = 1 and Uj(x) ≥0.\nLet u be a consistent assignment for the expectation of U such thatNu = ∅. Then, the\nalgorithm described in (17.13) converges to the maximum entropy extension of u.\nThis is the iterative scaling algorithm. This method can be extended in a straight-\nforward way to handle the maximum entropy extension for a family of functions\nU(1),...,U(K), such that, for all x and for all k, U(k)(x) is a dk-dimensional vector such\nthat\ndk\nX\nj=1\nU(k)\nj (x) = 1.\nThe maximum entropy extension takes the form\nπθ(x) = 1\nZθ\nexp\n\u0012\n−\nK\nX\nk=1\n(θ(k))T U(k)(x)\n\u0013\n,\nwhere θ(k) is dk-dimensional, and iterative scaling can then be implemented by up-\ndating only one of these vectors at a time, using (17.13) with U = U(k).\nThe restriction to U(x) providing a discrete probability distribution for all x is,\nin fact, no loss of generality. This is because adding a constant to U does not change\nthe resulting exponential model in (17.5), and multiplying U by a constant can be\nalso compensated by dividing θ by the same constant in the same model. So, if u−is\na lower bound for minj,x Uj(x), one can replace U by (U −u−), and therefore assume\nthat U ≥0, and if u+ is an upper bound for P\nj Uj(x), we can replace U by U/u+ and\ntherefore assume that P\nj Uj(x) ≤1. Define\nUd+1(x) = 1 −\nd\nX\nj=1\nUj(x) ≥0.\nThen, the maximum entropy extension for (U1,...,Ud) with assignment (u1,...,ud) is\nobviously also the extension for (U1,...,Ud+1), with assignment (u1,...,ud+1), where\nud+1 = 1 −\nd\nX\nj=1\nuj,\n\n17.2. LEARNING LOOPY MARKOV RANDOM FIELDS\n423\nand the latter is in the form required in proposition 17.7. Note that iterative scaling\nrequires to compute the expectation of U1,...,Ud before each update. These are not\nnecessarily available in closed form and may have to be estimated using Monte-Carlo\nsampling.\n17.2.5\nPseudo likelihood\nMaximum likelihood estimation is a special case of minimal contrast estimators. These\nestimators are based on the definition of a measure of dissimilarity, say C(π∥˜π), be-\ntween two probability distributions π and ˜π. The usual assumptions on C are that\nC(π∥˜π) ≥0, with equality if and only if π = ˜π, and that C is — at least — continuous\nin π and ˜π. Minimal contrast estimators approximate the problem of minimizing\nθ 7→C(πtrue∥πθ) over a parameter θ ∈Θ, (which is not feasible, since πtrue, the true\ndistribution of the data, is unknown) by the minimization of θ 7→C( ˆπ∥πθ) where\nˆπ is the empirical distribution computed from observed data. Under mild condi-\ntions on C, these estimators are generally consistent when N tends to infinity, which\nmeans that the estimated parameter asymptotically (in the sample size N) provides\nthe best (according to C) approximation of πtrue by the family πθ,θ ∈Θ.\nThe contrast that is associated with maximum likelihood is the Kullback-Leibler\ndivergence. Indeed, given a sample x1,...,xN, we have\nKL( ˆπ∥πθ)\n=\nE ˆπ log ˆπ −E ˆπ logπθ\n=\nE ˆπ log ˆπ −\nN\nX\nk=1\nlogπθ(xk).\nSince E ˆπ log ˆπ does not depend on θ, minimizing KL( ˆπ∥πθ) is equivalent to maxi-\nmizing PN\nk=1 logπθ(xk) which is the log-likelihood.\nMaximum pseudo-likelihood estimators form another class of minimal contrast\nestimators for graphical models. Given a distribution π on F (V ), define the local\nspecifications πs(x(s) | x(t),t , s) to be conditional distributions at one vertex given\nthe others, and the contrast\nC(π∥˜π) =\nX\ns∈V\nEπ(log πs\n˜πs\n).\nBecause we can write, using standard properties of conditional expectations,\nC(π∥˜π) =\nX\ns∈V\nEπ\n \nEπs(log πs\n˜πs\n)\n!\n=\nX\ns∈V\nE(KL(πs(· | X(t),t , s)∥˜πs(· | X(t),t , s)),\nwe see that C(π, ˜π) is always positive, and vanishes (under the assumption of positive\nπ) only if all the local specifications for π and ˜π coincide, and this can be shown\n\n424\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nto imply that π = ˜π. Indeed, for any x,y ∈F (V ), and choosing some order V =\n{s1,...,sn} on V , one can write\nπ(x)\nπ(y) =\nn\nY\nk=1\nπ(x(sk)|x(s1),...,x(sk−1),y(sk+1),...,y(sn))\nπ(x(sk)|x(s1),...,x(sk−1),y(sk+1),...,y(sn))\nand the ratios π(x)/π(y), for x ∈F (V ), combined with the constraint that P\nx π(x) = 1\nuniquely define π.\nSo C is a valid contrast and\nC( ˆπ∥πθ) =\nX\ns∈V\nE ˆπ log ˆπs −\nX\ns∈V\nN\nX\nk=1\nlogπθ,s(x(s)\nk |x(t)\nk ,t , s).\nThis yields the maximum pseudo-likelihood estimator (or pseudo maximum likeli-\nhood) defined as a maximizer of the function (called log-pseudo-likelihood)\nθ 7→\nX\ns∈V\nN\nX\nk=1\nlogπθ,s(x(s)\nk |x(s)\nk ,t , s).\nAlthough maximum likelihood is known to provide the most accurate approxi-\nmations in many cases, maximum of pseudo likelihood has the important advantage\nto be, most of the time, computationally feasible. This is because, for a model like\n(17.5), local specifications are given by\nπθ,s(x(s) | x(t),t , s) =\nexp(−θT U(x))\nP\ny(s)∈Fs exp(−θT U(y(s) ∧x(V \\s))).\nand therefore include no intractable normalizing constant. Maximum of pseudo-\nlikelihood estimators can be computed using standard maximization algorithms.\nFor exponential models such as (17.5), the log-pseudo-likelihood is, like the log-\nlikelihood, a concave function.\n17.2.6\nContinuous variables and score matching\nThe methods that were presented so far for discrete variables formally generalize to\nmore general state spaces, even though consistency or convergence issues in non-\ncompact cases can be significantly harder to address. Score matching is a parameter\nestimation method that was introduced in [95] and was designed, in its original\nversion, to estimate parameters for statistical models taking the form\nπθ(x) =\n1\nC(θ) exp(−F(x,θ))\n\n17.2. LEARNING LOOPY MARKOV RANDOM FIELDS\n425\nwith x ∈Rd. We assume below suitable integrability and differentiability conditions,\nin order to justify differentiation under integrals whenever they are needed. The\n“score function” is defined as\ns(x,θ) = −∇x logπθ(x) = ∇xF(x,θ)\nwhere ∇x denotes the gradient with respect to the x variable. Letting πtrue denote\nthe p.d.f. of the true data distribution (not necessarily part of the statistical model),\nscore matching minimizes\nf (θ) =\nZ\nRd |s(x,θ) −strue(x)|2πtrue(x)dx\nwhere strue = −∇logπtrue. This integral can be restricted to the support of πtrue, if\nwe don’t want to assume that πtrue is non-vanishing. Note, however that f (θ) = 0\nimplies that logπθ(·,θ) = logπtrue πtrue-almost everywhere, so that πθ(x) = cπtrue(x)\nfor some constant c and x in the support of πtrue. Only if πtrue(x) > 0 for all x ∈Rd,\ncan we conclude that this requires πθ = πtrue.\nExpanding the squared norm and applying the divergence theorem yield\nf (θ) =\nZ\nRd |∇x logπθ(x)|2πtrue(x)dx −2\nZ\nRd ∇x logπθ(x)T ∇πtrue(x)dx\n+\nZ\nRd |strue(x)|2πtrue(x)dx\n=\nZ\nRd |∇x logπθ(x)|2πtrue(x)dx + 2\nZ\nRd ∆logπθ(x)T πtrue(x)dx +\nZ\nRd |strue(x)|2dx\nTo justify the use of the divergence theorem, one needs to assume two derivatives in\nthe log-likelihoods with sufficient decay at infinity (see Hyv¨arinen and Dayan [95]\nfor details). This shows that minimizing f is equivalent to minimizing\ng(θ) =\nZ\nRd |∇x logπθ(x)|2πtrue(x)dx + 2\nZ\nRd ∆logπθ(x)T πtrue(x)dx\n= E(|∇x logπθ(X)|2 + 2∆logπθ(X)).\nIn this form, the objective function can be approximated by a sample average, so\nthat, given observed data x1,...,xN, one can define the score-matching estimator as\na minimizer of\nN\nX\nk=1\n\u0010\n|∇x logπθ(xk)|2 + 2∆logπθ(xk)\n\u0011\n.\n(17.14)\nRemark 17.8 The method can be adapted to deal with discrete variables replacing\nderivatives with differences. Let X take values in a finite set, RX, on which a graph\n\n426\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nstructure can be defined, writing x ∼y if x and y are connected by an edge. For\nexample, if X is itself a Markov random field on a graph G = (V ,E), so that RX =\nF (V ), one can define x ∼y if and only if x(s) = y(s) for all but one s ∈V . One can then\ndefine the score function\nsθ(x,y) = 1 −πθ(y)\nπθ(x)\ndefined over all x,y ∈RX such that x ∼y. Now the score matching functional is\nf (θ) =\nX\nx∈RX\n\n\nX\ny∼x\n|sθ(x,y) −s∗(x,y)|2\n\nπ∗(x),\nwhose minimization is, after reordering terms, equivalent to that of\ng(θ) =\nX\nx∈RX\nX\ny∼x\n\f\f\f\f\f1 −πθ(y)\nπθ(x)\n\f\f\f\f\f\n2\nπ∗(x) + 2\nX\nx∈RX\nX\ny∼x\n πθ(x)\nπθ(y) −πθ(y)\nπθ(x)\n!\nπ∗(x).\nBased on training data, a discrete score matching estimator is a minimizer of\nN\nX\nk=1\nX\ny∼xk\n\f\f\f\f\f1 −πθ(y)\nπθ(xk)\n\f\f\f\f\f\n2\n+ 2\nN\nX\nk=1\nX\ny∼xk\n πθ(xk)\nπθ(y) −πθ(y)\nπθ(xk)\n!\n.\n(17.15)\n♦\n17.3\nIncomplete observations for graphical models\n17.3.1\nThe EM Algorithm\nMissing variable sin the context of graphical models may correspond to real pro-\ncesses that cannot be measured, which is common, for example, with biological data.\nThey may be more conceptual objects that are interpretable but are not parts of the\ndata acquisition process, like phonemes in speech recognition, or edges and labels\nin image processing and object recognition. They may also be variables that have\nbeen added to the model to increase its parametric dimension without increasing\nthe complexity of the graph. However, as we will see, dealing with incomplete or\nimperfect observations brings the parameter estimation problem to a new level of\ndifficulty.\nSince it is the most common approach to address incomplete or noisy observa-\ntions, we start with a description of how the EM algorithm (Algorithm 16.1) applies\nto graphical models, and of its limitations. We assume a graphical model on an\nundirected graph G = (V ,E), in which we assume that V is separated in two non-\nintersecting subsets, V = S ∪H. Letting X be a G-Markov random field, the part X(S)\nis assumed to be observable, and X(H) is hidden.\n\n17.3. INCOMPLETE OBSERVATIONS FOR GRAPHICAL MODELS\n427\nWe assume that X takes values in F (V ), where we still denote by Fs the sets in\nwhich Xs takes values for s ∈V . We let the model distribution belong to an expo-\nnential family, with\nπθ(x) =\n1\nZ(θ) exp\n\u0010\n−θT U(x)\n\u0011\n, x ∈F (V ).\n(17.16)\nAssume that an N-sample x(S)\n1 ,...,x(S)\nN is observed over S. Since\nlogπθ(x) = −logZ(θ) −θT U(x),\nthe transition from θn to θn+1 in Algorithm 16.1 is done by maximizing\n−logZ(θ) −θT ¯Un\n(17.17)\nwhere\n¯Un = 1\nN\nN\nX\nk=1\nEθn(U(X) | X(S) = x(S)\nk ).\n(17.18)\nSo, the M-step of the EM, which maximizes (17.17), coincides with the complete-\ndata maximum-likelihood problem for which the empirical average of U is replaced\nby the average of its conditional expectations given the observations, as given in\n(17.18), which constitutes the E-step. As a consequence, a strict application of the\nEM algorithm for graphical models is unfeasible, since each step requires running\nan algorithm of similar complexity maximum likelihood for complete data, that we\nalready identified as a challenging, computationally costly problem. The same re-\nmark holds for the SAEM algorithm of section 16.4.3, which also requires solving a\nmaximum likelihood problem at each iteration.\n17.3.2\nStochastic gradient ascent\nThe stochastic gradient ascent described in section 17.2.2 can be extended to partial\nobservations [207], even though it loses the global convergence guarantee that re-\nsulted from the concavity of the log-likelihood for complete observations. Indeed,\napplying the computation of section 16.5.2, to a model given by (17.16), we get using\nproposition 17.4,\n∂θ logψθ = Eθ(Eθ(U) −U | X(S) = x(S)) = Eθ(U) −Eθ(U | X(S) = x(S))\nwhere we ψθ(x(S)) denotes the marginal distribution of πθ on S.\nLet πθ(x(H) | x(S)) denotes the conditional probability P(X(H) = x(H) | X(S) = s(S))\nfor the distribution πθ, therefore taking the form\nπθ(x(H) | x(S)) =\n1\n˜Z(θ,x(S)) exp\n\u0010\n−θT U(x(S) ∧x(H))\n\u0011\n.\n\n428\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nAssume given an ergodic transition probability pθ on F (V ), and a family of ergodic\ntransition probabilities px(S)\nθ\n, x(S) ∈F (S), such that the invariant distribution of pθ is\nπθ, and the one of px(S)\nθ\nis πθ(· | x(S)). Then the following SGA algorithm can be used\nto estimate θ\nAlgorithm 17.1\nStart the algorithm with an initial parameter θ(0) and initial configurations x(0) and\nx(H)\nk\n(0), k = 1,...,N. Then, at step n,\n(SGH1) Sample from the distribution pθ(n)(x(n),·) to obtain new configurations x(n+\n1) ∈F (V ).\n(SGH2) For k = 1,...,N, sample from the distribution p\nx(S)\nk\nθ(n)(x(H)\nk\n(n),·) to obtain a new\nconfiguration x()\nk H(n + 1) over the hidden vertexes.\n(SGH3) Update the parameter using\nθ(n + 1) = θ(n) + γ(n + 1)\n\nU(x(n + 1)) −1\nN\nN\nX\nk=1\nU(x(S)\nk\n∧x(H)\nk\n(n + 1))\n\n.\n(17.19)\n17.3.3\nPseudo-EM Algorithm\nThe EM update\nθn+1 = argmax\nθ\n\u0012 N\nX\nk=1\nEθn\n\u0012\nlogπθ(X) | X(S) = x(S)\nk\n\u0013\u0013\n.\nbeing challenging for Markov random fields, it is tempting to replace the log-likelihood\nin the expectation by an other contrast, such as the log-pseudo-likelihood. A simi-\nlar approach to that described here was introduced in Chalmond [51], for situations\nwhen the conditional distribution of X(S) given X(H) is “simple enough” (for exam-\nple, if the variables Xs,s ∈S are conditionally independent given X(H)) and when the\ncardinality of the sets Fs, s ∈H is small (binary, or ternary, variables).\nThe algorithm has the following variational interpretation. Fix x(S) ∈F (S) and\ns ∈H. Also denote µs = 1/|F (H \\ {s})|. If q is a transition probability from F (H \\ {s})\nto Fs, let\n∆(s)\nθ (q,x(S)) =\nX\ny∈F (H)\n \nlog\n πθ,s(y(s) ∧x(S) | y(H\\{s}))\nq(y(H\\{s}),y(s))µs\n!\nq(y(H\\{s}),y(s))µs\n!\n.\n(17.20)\n\n17.3. INCOMPLETE OBSERVATIONS FOR GRAPHICAL MODELS\n429\nThis function is concave in q, since its first partial derivative with respect to q(y(H\\{s}),y(s))\n(for each y ∈F (H)) is given by\nµs logπθ,s(y(s) ∧x(S) | y(H\\{s}))µs(y(H\\{s})) −µs log(q(y(H\\{s}),y(s))µs) −µs\nso that its Hessian is the diagonal matrix with negative entries −µs/q(y(H\\{s}),y(s)).\nUsing Lagrange multipliers to express the constraints P\ny(s)∈Fs q(y(H\\{s}),y(s)) = 1 for\nall y(H\\{s}), we find that ∆(s)\nθ (q,x(S)) is maximized when q(y(H\\{s}),y(s)) is proportional\nto πθ,s(y(s) ∧x(S) | y(H\\{s})), yielding\nq(y(H\\{s}),y(s)) = πθ,s(y(s) | x(S) ∧y(H\\{s})).\nNow, consider the problem of maximizing\nN\nX\nk=1\nX\ns∈H\n∆(n)\nθ (q(s)\nk ,x(s)\nk )\n(17.21)\nwith respect to θ and q(s)\nk , k = 1,...,N, s ∈H. Consider an iterative maximization\nscheme in which, from a current parameter θn, one first, maximizes (17.21) with\nrespect to transition probabilities q(s)\nk , then with respect to θ to obtain θn+1. This\nscheme provides the iteration\nθn+1 =\nargmax\nθ\nN\nX\nk=1\nX\ns∈H\nX\ny∈F (H)\n\u0012\nlogπθ,s(y(s) ∧x(S)\nk\n| y(H\\{s}))\n\u0013\nπθn,s(y(s) | x(S)\nk\n∧y(H\\{s}))µs.\n17.3.4\nPartially-observed Bayesian networks on trees\nWe now consider the situation in which the joint distribution of X = X(S) ∧X(H) is a\nBayesian network over a directed acyclic graph G = (V ,E).\nAssume that x(S)\n1 ,...,x(S)\nN are observed. The parameter θ is the collection of all\np(x(pa(s)),x(s)) for s ∈V . Define the random variables Is,x(y) equal to one if y({s}∪pa(s)) =\nx({s}∪pa(s)) and zero otherwise. We can write\nlogπ(y) =\nX\ns∈S\nlogps(y(pa(s)),y(s)) =\nX\ns∈S\nX\nx({s}∪pa(s))∈F ({s}∪pa(s))\nlogps(x(pa(s)),x(s))Is,x(y)\n\n430\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nThis implies that\nN\nX\nk=1\nEθn\n\u0012\nlogπ(x(S)\nk ,X(H)) | X(S) = x(S)\nk\n\u0013\n=\nX\nx({s}∪pa(s))∈F ({s}∪pa(s))\nlogps(x(pa(s)),x(s))\nN\nX\nk=1\nEθn(Is,x(X) | X(S) = x(S)\nk )\n=\nX\nx({s}∪pa(s))∈F ({s}∪pa(s))\nlogps(x(pa(s)),x(s))\nN\nX\nk=1\nπθn(x({s}∪pa(s)) | X(S) = x(S)\nk ).\nThe EM iteration at step n then is\np(n+1)\ns\n(x(pa(s)),x(s)) =\n1\nZs(x(s−))\nN\nX\nk=1\nπθn(x({s}∪pa(s)) | X(S) = x(S)\nk )\nwith\nπθn(x) =\nY\ns∈V\np(n)(x(pa(s)),x(s)),\nZs being a normalization constant.\nIf the estimation is solved with a Dirichlet prior Dir(1+as(x(s),x(pa(s)))), the update\nformula becomes\np(n+1)\ns\n(x(pa(s)),x(s)) =\n1\nZs(x(s−))\n\nas(x(s),x(pa(s))) +\nN\nX\nk=1\nπθn(x({s}∪pa(s)) | X(S) = x(S)\nk )\n\n.\n(17.22)\nThis algorithm is very simple when the conditional distributions πθn(x(s∪pa(s)) |\nX(S) = x(S)\nk ) can be easily computed, which is not always the case for a general\nBayesian network, since conditional distributions do not always have a structure of\nBayesian network. The computation is simple enough for trees, however, since con-\nditional tree distributions are still trees (or forests). More precisely, the conditional\ndistribution given the observed variables can be written in the form\nπ(y(H) | x(S)) =\n1\nZ(x(S))\nY\ns∈H\nϕs,x(y(s))\nY\nt∼s,{s,t}⊂H\nϕst(y(s),y(t))\nwith ϕs,pa(s)(y(s),y(pa(s))) = ps(y(pa(s)),y(s)) and, letting ϕs(y(s)) = ps(y(s)) if pa(s) = ∅and\n1 otherwise,\nϕs,x(y(s)) = ϕs(y(s))\nY\nt∼s,t∈S\nϕst(y(s),x(t)).\n\n17.3. INCOMPLETE OBSERVATIONS FOR GRAPHICAL MODELS\n431\nSo, the marginal joint distribution of a vertex and its parents are directly given by\nbelief propagation, using the just defined interactions. This training algorithm is\nsummarized below.\nAlgorithm 17.2 (Learning tree distributions with hidden variables)\nStart with some initial guess of the conditional probabilities (for example, those\ngiven by the prior). The iterate the following two steps providing the transition\nfrom θn to step θn+1.\n(1) For k = 1,...,N, use belief propagation (or sum-prod) to compute all πθn(x({s}∪pa(s)) |\nX(S) = x(S)\nk ). Note that these probabilities can be 0 or 1 when s ∈S and/or pa(s) ⊂S.\n(2) Use (17.22) to compute the next set of parameters.\nThe tree case includes the important example of hidden Markov models, which\nare defined as follows. S and H are ordered, with same cardinality, say S = {s1,...,sq}\nand H = {h1,...,hq}. Edges are (h1,h2),...,(hq−1,hq) and (h1,s1),...,(hq,sq). The in-\nterpretation generally is that the hidden variables, hs, are the variables of interest,\nand behave like a Markov chain, and that the observations, xs, are either noisy or\ntransformed versions of them. A major application is in speech recognition, where\nthe hs’s are labels that represent specific phonemes (little pieces of spoken words)\nand the xs’s are measured signals. The transitions between hidden variables then\ndescribe how phonemes are likely to appear in sequence for a given language, and\nthose between hidden and observed variables describe how each phoneme is likely\nto be pronounced and heard.\n17.3.5\nGeneral Bayesian networks\nThe algorithm in the general case can move from tractable to intractable depending\non the situation. This must generally be handled in a case by case basis, by analyzing\nthe conditional structure, for a given model, knowing the observations.\nIn practice, it is always possible to use loopy belief propagation to obtain some\napproximation of the conditional probabilities, even if it is not sure that the algo-\nrithm will converge to the correct marginals. When feasible, junction trees can be\nused, too. Monte-Carlo sampling is also an option, although quite computational.\n\n432\nCHAPTER 17. LEARNING GRAPHICAL MODELS\n\nChapter 18\nDeep Generative Methods\n18.1\nNormalizing flows\n18.1.1\nGeneral concepts\nWe develop, in this chapter, methods that model stochastic processes using a feed-\nforward approach that generates complex random variables using non-linear trans-\nformations of simpler ones. Many of these methods can be seen as instances of struc-\ntural equation models (SEMs), described in section 15.3, with, for deep-learning im-\nplementations, high-dimensional parametrizations of (15.8).\nWith start with the formally simple case where the modeled variable takes values\nin Rd and is modeled as\nX = g(Z)\nwhere Z also takes values in Rd, with a known distribution and g is C1, invertible,\nwith a C1 inverse on Rd, i.e., is a diffeomorphism of Rd. Let us denote by h the inverse\nof g.\nIf Z has a p.d.f. fZ with respect to Lebesgue’s measure, then, using the change of\nvariable formula, the p.d.f. of X is\nfX(x) = fZ(h(x)) |det∂xh(x)|.\nNow, given a training set T = (x1,...,xN), the log-likelihood, considered as a func-\ntion of h, is given by\nℓ(h) =\nN\nX\nk=1\nlogfZ(h(xk)) +\nN\nX\nk=1\nlog|det∂xh(xk)|.\n(18.1)\nThis expression should then be maximized with respect to h, subject to some restric-\ntions or constraints to avoid overfitting.\n433\n\n434\nCHAPTER 18. DEEP GENERATIVE METHODS\n18.1.2\nA greedy computation\nOne can define a rich class of diffeomorphisms through iterative compositions of\nsimple transformations. This framework was introduced in [187], where a greedy\napproach was suggested to build such compositions. The method was termed “nor-\nmalizing flows,” since it create a discrete flow of diffeomorphisms that transform the\ndata into a sample of a normal distribution.\nWe quickly describe the basic principles of the algorithm. One starts with a\nparametrized family, say (ψα,α ∈A) of diffeomorphisms of R. Such families are\nrelatively easy to design, one example proposed in [187] being a smoothed version\nof the piecewise linear function\nu 7→v0 + (1 −σ)u + γ|(1 −σ)u −u0|\nwhich is increasing as soon as 0 ≤max(σ,γ) < 1. The smoothed version has an\nadditional parameter, ϵ, and takes the form\nu 7→v0 + (1 −σ)u + γ\nq\nϵ2 + ((1 −σ)u −u0)2.\nThis transformation is parametrized by α = (v0,σ,γ,u0,ϵ). Other families of parametrized\ntransformations can be designed. A multivariate transformation ϕα,U : Rd →Rd can\nthen be associated to families α = (α1,...,αd) and orthogonal matrices U by taking\nϕα,U(x) =\n\n\nψα1(y(1))\n...\nψαd(y(d))\n\n\nwith y = Ux.\nThe algorithm in [187] is initialized with h0 = id[d] and update the transforma-\ntion at step n according to\nhn = ϕαn,Un ◦hn−1.\nIn this update, Un is generated as a random rotation matrix, and αn is determined\nas a gradient ascent update (starting from α = 0) for the maximization of\nα 7→ℓ(ϕα,Un ◦hn−1).\n(Here, the current value hn−1 is not revisited, therefore providing a “greedy” opti-\nmization method.)\n\n18.1. NORMALIZING FLOWS\n435\nLetting zn,k = hn(xk), the chain rule implies that\nℓ(ϕα,Un ◦hn−1)) =\nN\nX\nk=1\nlogfZ(ϕα,Un(zn−1,k)) +\nN\nX\nk=1\nlog|detϕα,Un(zn−1,k)|\n+\nN\nX\nk=1\nlog|det∂xhn−1(xk)|.\nSince the last term does not depend on α, we see that it suffices to keep track of the\n“particle” locations, zn−1,k to be able to compute αn. Note also that these locations\nare easily updated with zn,k = ϕαn,Un(zn−1,k).\n18.1.3\nNeural implementation\nThis iterated composition of diffeomorphisms obviously provides a neural architec-\nture similar to those discussed in chapter 11. Fixing the number of iterations to be,\nsay, m, one can consider families of diffeomorphisms (ϕθ) indexed by a parameter w\n(we had w = (α,U) in the previous discussion), and optimize (18.1) over all functions\nh taking the form h = ϕwm ◦··· ◦ϕw1. Letting zj,k = ϕwj ◦··· ◦ϕw1(xk) for j ≤m (with\nz0,k = xk), we can write\nℓ(h) =\nN\nX\nk=1\nlogfZ(zm,k) +\nN\nX\nk=1\nm\nX\nj=1\nlog|det∂xϕwj(zj−1,k)|.\nNormalizing flows in this form are described in [161, 107, 148]. The gradient of ℓ\nwith respect to the parameters w1,...,wm can be computed by backpropagation. We\nnote however that, unlike typical neural implementations, the parameters may come\nwith specific constraints, such as U ∈Od(R) when w = (α,U), so that the gradient\nand associated displacement may have to be adapted compared to standard gradient\nascent implementations (see section 20.6.3 for a discussion of first-order implemen-\ntations of gradient methods for functions of orthogonal matrices, and [1] for more\ngeneral methods on optimization over matrix groups).\n18.1.4\nTime-continuous version\nIn section 11.6, we described how diffeomorphisms could be generated as flows of\ndifferential equations, and this remark can be used to provide a time-continuous\nversion of normalizing flows. Using (11.3), one generates trajectories z(·) by solving\nover, say, [0,T ]\n∂tz(t) = ψw(t)(z(t))\n\n436\nCHAPTER 18. DEEP GENERATIVE METHODS\nwith z(0) = x for some function w : t 7→w(t). Letting z(t) = hw(t,x) (which defines\nhw), we know that, under suitable assumptions on ψ, the mapping x 7→hw(t,x) is a\ndiffeomorphism of Rd. One can then maximize\nℓ(hw(T,·)) =\nN\nX\nk=1\nlogfZ(hw(T ,xk)) +\nN\nX\nk=1\nlog|det∂xhw(T ,xk)|\nwith respect to the function w. Let zk(t) = hw(t,xk) and Jk(t) = log|det∂xhw(t,xk)|. We\nhave, by definition\n∂tzk(t) = ψw(t)(zk(t))\nwith zk(0) = xk. One can also show that\n∂tJk(t) = ∇· ψw(t)(zk(t))\nwith Jk(0) = 0, where the r.h.s. is the divergence of ψw(t) evaluated at zk(t). We\nprovide a quick (and formal) justification of this fact. First note that differentiating\n∂thw(t,x) = ψw(t)(hw(t,x)) with respect to x yields\n∂t∂xhw(t,x) = ∂xψw(t)(hw(t,x))∂xhw(t,x).\nThe mapping J : A 7→log|det(A)| is differentiable on the set of invertible matrices\nand is such that dJ (A)H = trace(A−1H). Applying the chain rule, we find\n∂t log|det∂xhw(t,x)| = trace(∂xhw(t,x)−1∂xψw(t)(hw(t,x))∂xhw(t,x))\n= trace(∂xψw(t)(hw(t,x))) = ∇· ψw(t)(hw(t,x)).\nFrom this, it follows that the time-continuous normalizing flow problem can be\nreformulated as maximizing\nN\nX\nk=1\nlogfZ(zk(T )) +\nN\nX\nk=1\nJk(T)\nsubject to ∂tzk(t) = ψw(t)(zk(t)), ∂tJk(t) = ∇· ψw(t)(zk(t)), zk(0) = xk, Jk(0) = 0. This is\nan optimal control problem, whose analysis can be done similarly to that made in\nsection 11.6.1, provided that ∇· ψw(t) can be expressed in closed form.\nNote that the inverse of hw(T,·), which provides the generative model going from\nZ to X can also be obtained as the solution of an ODE. Namely, if one solves the\ndifferential equation\n∂tx(t) = −ψw(T−t)(x(t))\nwith initial condition x(0) = z, then x(T ) solves the equation hw(T ,·) = z.\n\n18.2. NON-DIFFEOMORPHIC MODELS AND VARIATIONAL AUTOENCODERS437\n18.2\nNon-diffeomorphic models and variational autoencoders\n18.2.1\nGeneral framework\nThe previous discussion addressed the situation X = g(Z) when g is a diffeomor-\nphism, which required, in particular, that X and Z are real vectors with identical di-\nmensions. This may not always be desirable, as one may prefer a small-dimensional\nvariable Z (in the spirit of the factor analysis methods discussed in chapter 20), or\na high-dimensional Z to increase, for example the modeling power. In addition, the\nobservation variables may be discrete, which precludes the use of the change of vari-\nables formula. In such cases, Z has to be treated as a hidden variable using one of\nthe methods discussed in chapter 16.\nIt will convenient to model the generative process in the form of a conditional\ndistribution of X given Z rather than a deterministic function. We place ourselves\nin the framework of chapter 16 (with slightly modified notation) and let RX and RZ\ndenote the measured spaces over where X and Z take their values, with measures\nµX and µZ, and assume that the conditional distribution of X given Z = z has den-\nsity fX(x | z,θ) with respect to µX, for some parameter θ. We also assume that Z\nhas a distribution with density fZ with respect to µZ, that we assume given and un-\nparametrized. One can then directly apply the algorithms provided in chapter 16,\nand in particular the variational methods described in section 16.4.4 with an appro-\npriate definition of the approximation of the conditional density of Z given X. An\nimportant example in this context is provided by variational autoencoders (VAEs)\nthat we now present.\n18.2.2\nGenerative model for VAEs\nVAEs [103, 104] model X ∈Rd as X = g(Z,θ)+ϵ where ϵ is a centered Gaussian noise\nwith covariance matrix Q. The function g is typically non-linear, and VAEs have\nbeen introduced with this function modeled as a deep neural network (see chap-\nter 11). Letting ϕN (·; 0,Q) denote the p.d.f. of the Gaussian distribution N (0,Q),\nthe conditional distribution of X given Z = z has density\nfX(x | z,θ) = ϕN (x −g(z,θ)); 0,Q)\nwith respect to Lebesgue’s measure on Rd.\nFollowing the procedure in section 16.4.4, we define an approximation of the\nconditional distribution of Z given X. Assuming that Z ∈Rp, we let this distri-\nbution be N (µ(x,w),Σ(x,w)) for some functions µ and Σ, w being a parameter. To\nensure that Σ ⪰0, we will represent it in the form Σ(x,w) = S(x,w)2 where S is a sym-\nmetric matrix. In [103], both functions µ and S are represented as neural networks\n\n438\nCHAPTER 18. DEEP GENERATIVE METHODS\nparametrized by w. The joint density of X and Z is such that\nlogfX,Z(x,z; θ,Q) = logϕN (x −g(z,θ)); 0,Q) + logfZ(z)\n= −1\n2(x −g(z,θ))T Q−1(x −g(z,θ)) −1\n2 logdetQ −d\n2 log2π + logfZ(z)\nWe also have\nlogϕN (z; µ(x,w),S(x,w)2) = −1\n2(z−µ(x,w))T S(x,w)−2(z−µ(x,w))−logdetS(x,w)−p\n2 log2π.\nWe can then rewrite the algorithm in (16.15) as\n\nθn+1 = θn + γn+1∂θ logfX,Z(Xn+1,Zn+1;θn,Qn)\nQn+1 = Qn + γn+1∂Q logfX,Z(Xn+1,Zn+1;θn,Qn)\nwn+1 = wn + γn+1 log\n \nfX,Z(Xn+1,Zn+1;θn,Qn)\nϕN (Xn+1 ; µ(Xn+1,wn),S(Xn+1,wn)2)\n!\n× ∂w logϕN (Xn+1 ; µ(Xn+1,wn),S(Xn+1,wn)2)\n(18.2)\nwhere Xn+1 is drawn uniformly from the training data and\nZn+1 ∼N (µ(Xn+1,wn),S(Xn+1,wn)2).\nThe derivatives in this system can be computed from those of g,µ and S (typically\ninvolving back-propagation) and the expression of the derivatives of the determinant\nand inverse of a matrix provided in (1.4) and (1.6).\nThe computation can be simplified if one assumes that fZ is the p.d.f. of a stan-\ndard Gaussian, i.e., fZ = ϕN (·;0,IdRp). Indeed, in that case, the integral in (16.11),\nwhich is, using the current notation\nZ\nRp log ϕN (x −g(z,θ); 0,Q)ϕN (z; 0,IdRp)\nϕN (z; µ(x,w),S(x,w)2)\nϕN (z; µ(x,w),S(x,w)2)dz,\n(18.3)\ncan be partially computed. For any two p-dimensional Gaussian p.d.f.’s, one has\nZ\nRp logϕN (z; µ1,Σ1) ϕN (z; µ2,Σ2)dz = −1\n2trace(Σ−1\n1 Σ2) −1\n2(µ2 −µ1)T Σ−1\n1 (µ2 −µ1)\n−1\n2 logdet(Σ1) −p\n2 log(2π).\n(18.4)\nAs a consequence, (18.3) becomes\n−1\n2Ew\n\u0010\n(X −g(Z,θ))T Q−1(X −g(Z,θ))\n\u0011\n−1\n2 logdetQ −d\n2 log2π\n−Ew\n\u00121\n2trace(S(X,w)2) + 1\n2|µ(X,w)|2 −logdet(S(X,w))\n\u0013\n+ p\n2,\n(18.5)\n\n18.2. NON-DIFFEOMORPHIC MODELS AND VARIATIONAL AUTOENCODERS439\nwhere Ew denotes the expectation for the random variable (X,Z) where X follows a\nuniform distribution over training data and the conditional distribution of Z given\nX = x is N (µ(x,w), S(x,w)2).\nThe algorithm proposed in Kingma and Welling [103] introduces a change of\nvariable Z = µ(X,w) + S(X,w)U where U ∼N (0,IdRp), rewriting (18.5) as\n−1\n2E\n\u0010\n(X −g(µ(X,w) + S(X,w)U,θ))T Q−1(X −g(µ(X,w) + S(X,w)U,θ))\n\u0011\n−Ew\n\u00121\n2trace(S(x,w)2) + 1\n2|µ(X,w)|2 −logdet(S(X,w))\n\u0013\n−1\n2 logdetQ −d\n2 log2π + p\n2,\n(18.6)\nwith a modified version of (18.2). Letting\nF(θ,Q,w,x,u) = −1\n2(x −g(µ(x,w) −S(x,w)U,θ))T Q−1(x −g(µ(x,w) −S(x,w)U,θ))\n−1\n2 logdetQ −1\n2trace(S(x,w)2) −1\n2|µ(x,w)|2 + logdet(S(x,w))\nthe resulting algorithm is\n\nθn+1 = θn + γn+1∂θF(θn,Qn,wn,Xn+1,Un+1)\nQn+1 = Qn + γn+1∂QF(θn,Qn,wn,Xn+1,Un+1)\nwn+1 = wn −γn+1∂wF(θn,Qn,wn,Xn+1,Un+1)\n(18.7)\nwhere Xn+1 is drawn uniformly from the training data and Un+1 ∼N (0,IdRp).\n18.2.3\nDiscrete data\nThis framework can be easily adapted to situations in which the observations are\ndiscrete. Consider, as an example, the situation in which X takes values in {0,1}V ,\nwhere V is a set of vertexes, i.e., X is a binary Markov random field on V . Assume,\nas a generative model, that conditionally to the latent variable Z ∈Rp, the variables\nX(s),s ∈V are independent and X(s) follows a Bernoulli distribution with parame-\nter gs(z,θ), where g : Rp →[0,1]V . Assume also that Z ∼N (0,IdRp), and define, as\nabove, an approximation of the conditional distribution of Z given X = x as a Gaus-\nsian with mean µ(x,w) and covariance matrix S(x,w)2. Then, the joint density of X\nand Z (with respect to the product of the counting measure on {0,1}V and Lebesgue’s\nmeasure on Rp) is\nlogfX,Z(x,z; θ) = xlogg(z,θ) + (1 −x)log(1 −g(z,θ)) + logϕN (z; 0,IdRp)\n\n440\nCHAPTER 18. DEEP GENERATIVE METHODS\nand (18.2) becomes\n\nθn+1 = θn + γn+1∂θ logfX,Z(Xn+1,Zn+1;θn,Qn)\nwn+1 = wn + γn+1 log\n \nfX,Z(Xn+1,Zn+1;θn)\nϕN (Xn+1 ; µ(Xn+1,wn),S(Xn+1,wn)2)\n!\n× ∂w logϕN (Xn+1 ; µ(Xn+1,wn),S(Xn+1,wn)2)\n(18.8)\n18.3\nGenerative Adversarial Networks (GAN)\n18.3.1\nBasic principles\nSimilarly to the methods discussed so far, GANs [82], use a one-step nonlinear gen-\nerator X = g(Z,θ), with θ ∈RK, to model observed data (we here switch back to a\ndeterministic relation), where Z has a known distribution, with p.d.f. fZ, for exam-\nple Z ∼N (0,IdRp). However, unlike the exact or approximate likelihood maximiza-\ntion that were discussed in sections 18.1 and 18.2, GANs us a different criterion for\nestimating the parameter θ by minimizing metrics that can be approximated by opti-\nmizing a classifier. The classifier is a function x 7→f (x,w), parametrized by w ∈RM,\nwhose goal is to separate simulated samples from real ones: it takes values in [0,1]\nand estimates the (posterior) probability that its input x is real. GANs’ adversarial\nparadigm consists in estimating θ and w together so that generated data, using θ,\nare indistinguishable from real ones using the optimal w. Their basic structure is\nsummarized in Figure 18.1.\nClassifier\nPrediction\nW\nData\nSimulation\nGenerator\nθ\nNoise\nFigure 18.1: Basic structure of GANs: W is optimized to improve the prediction problem:\n“real data” vs. “simulation”. Given W, θ is optimized to worsen the prediction.\n18.3.2\nObjective function\nLet Pθ denote the distribution of g(Z,θ), and Ptrue the target distribution of “real\ndata.” One can formalize the “real data” vs. “simulation” problem with a pair of\n\n18.3. GENERATIVE ADVERSARIAL NETWORKS (GAN)\n441\nrandom variables (X,Y) where Y follows a Bernoulli distribution with parameter\n1/2, and the conditional distribution of X given Y is Ptrue when Y = 1 and Pθ when\nY = 0. Given a loss function r : {0,1} × [0,1] →[0,+∞), one can define\nU(θ,w) = Eθ(r(Y,f (X,w)))\nand\nU∗(θ) = min\nw∈RM U(θ,w).\nWe want to maximize U∗or, equivalently, solve the optimization problem\nθ∗= argmaxθ min\nw∈RM U(θ,w).\nNote that\n2U(θ,w) = Etrue(r(1,f (X,w))) + Eθ(r(0,f (X,w)))\nso that choosing the cost requires to specify the two functions t 7→r(1,t) and t 7→\nr(0,t). In Goodfellow et al. [82], they are:\nr(1,t) = −logt\nr(0,t) = −log(1 −t).\n(18.9)\n18.3.3\nAlgorithm\nUsing costs in (18.9), one must compute\nθ∗= argmin max\nw∈RM\n\u0012\nEtrue(logf (X,w)) + Eθ(log(1 −f (X,w)))\n\u0013\n= argmin max\nw∈RM\n\u0012\nEtrue(logf (X,w)) + E(log(1 −f (g(Z,θ),w)))\n\u0013\n.\nSuch min-max, or saddle-point problem are numerically challenging. The fol-\nlowing algorithm was proposed in Goodfellow et al. [82], and also includes a stochas-\ntic approximation component. Indeed, in practice, Etrue is only known through the\nobservation of training data, say x1,...,xN. Moreover, Eθ is only accessible through\nMonte-Carlo simulation, so that both expectations can only be approximated through\nfinite-sample averaging.\nAlgorithm 18.1 (GAN training algorithm)\n1. Extract a batch of m examples from training data, simulate m samples accord-\ning to Pθ and run a few (stochastic) gradient ascent steps with fixed θ to update w,\nreplacing expectations by averages.\n2. Generate m new samples of Z and update θ with fixed w by iterating a few\nsteps of (stochastic) gradient descent.\n\n442\nCHAPTER 18. DEEP GENERATIVE METHODS\n18.3.4\nAssociated probability metric and Wasserstein GANs\nLet F be the family of all measurable functions: f : Rd →[0,1]. Given two possi-\nble probability distributions P1,P2 (with associated expectations denoted E1,E2) of a\nrandom variable X taking values in Rd, consider the function\nD(P1,P2) = 2log2 + max\nf ∈F\n\u0012\nE1(logf (X)) + E2(log(1 −f (X)))\n\u0013\nAssume that X under P1 (resp. under P2) has p.d.f. g1 (resp. g2) with respect to\nLebesgue’s measure (this assumption is not needed for the following to hold, but\nmakes the discussion more elementary). Then\nE1(logf (X)) + E2(log(1 −f (X))) =\nZ\nRd(g1 logf + g2 log(1 −f ))dx\nwhich is maximal at f ∗= g1/(g1 + g2). For this f ∗,\n2log2 + E1(logf ∗(X)) + E2(log(1 −f ∗(X))) =\nZ\nRd g1 log\n2g1\ng1 + g2\ndx +\nZ\nRd g2 log\n2g2\ng1 + g2\ndx\n= KL\n\u0012g1 + g2\n2\n,g1\n\u0013\n+ KL\n\u0012g1 + g2\n2\n,g2\n\u0013\nThis expression is called the Jensen-Shannon divergence between g1 and g2. It is al-\nways non-negative, and vanishes only when g1 = g2.\nSo, D : (P1,P2) 7→D(P1,P2) can be interpreted as a way to evaluate the difference\nbetween two probability distributions on Rd. One can then define\nˆD(P1,P2) = max\nw∈RM\n\u0012\nE1(logf (X,w)) + E2(log(1 −f (X,w)))\n\u0013\nas an approximation of D in which the set of all possible functions with values in\n[0,1] is replaced by those arising from the GAN classification network, parametrized\nby w. This approximation is useful when g1,g2 are only observable through random\nsampling or simulation. With this interpretation, GANs minimize ˆD(Ptrue,Pθ).\nThis discussion suggests that new types of GAN may be designed using other\ndiscrepancy functions between probability distributions, provided they can be ex-\npressed in terms of the maximization of some quantity over some space of functions.\nConsider, for example the norm in total variation, defined by (for discrete distribu-\ntions)\nDvar(P1,P2) = 1\n2\nX\nx\n|P1(x) −P2(x)|.\nor, in the general case Dvar(P1,P2) = maxA(P1(A) −P2(A)).\n\n18.3. GENERATIVE ADVERSARIAL NETWORKS (GAN)\n443\nIf F is the space of continuous functions f : Rd →[0,1], then we also have (under\nmild assumptions on P1 and P2)\nDvar(P1,P2) = max\nf ∈F (E1(f ) −E2(f )).\nSince neural nets typically generate continuous functions with values in [0,1], one\ncould train GANs by maximizing\nˆDvar(P1,P2) = max\nw∈RM\n\u0012\nE1(f (X,w)) −E2(f (X,w))\n\u0013\nHowever, the total variation distance is too crude to allow for meaningful compar-\nisons between distributions. For example, the distance between two Dirac distri-\nbutions at, say, x1 and x2 in Rd is always 1, whatever the distance between x1 and\nx2, unless x1 = x2. A more sensitive distance can be defined based on the notion of\noptimal transport.\nThe Monge-Kantorovich, also called Wasserstein, and sometimes also called “earth-\nmover”, distance evaluates the minimal total distance along which “mass” needs to\nbe transported to transform a distribution, P1, into another, P2. Its mathematical\ndefinition is\nDw(P1,P2) = inf\nQ\nZ\nRd×Rd |x1 −x2|Q(dx1,dx2)\nwhere the inf is computed over all joint distributions on Rd×Rd whose first marginal\nis P1 and second marginal P2. Note that the distance Dw between δx1 and δx2 now is\n|x1 −x2|.\nThe Wasserstein distance can also be defined by\nDw(P1,P2) = max\nf ∈F (E1(f ) −E2(f ))\nwhere F is now the space of contractive (or 1-Lipschitz) functions, i.e., f ∈F if and\nonly if, for all x1,x2 ∈Rd, |f (x1) −f (x2)| ≤|x1 −x2|.\nUsing the fact that a neural network with all weights bounded by a constant K\ngenerates a function whose Lipschitz constant is controlled solely by K, one can then\napproximate (up to a multiplicative constant) the Wasserstein distance by\nˆDw(P1,P2) = max\nw∈W\n\u0012\nE1(f (X,w)) −E2(f (X,w))\n\u0013\nwhere W is the set of all weights bounded by a fixed constant. Given the distribution\nPtrue and the model Pθ, Wasserstein GANs (WGANs [11]) must then solve the saddle-\npoint problem\nU(θ,w) = max\nw∈W\n\u0012\nEtrue(f (X,w)) −Eθ(f (X,w))\n\u0013\n\n444\nCHAPTER 18. DEEP GENERATIVE METHODS\nand\nU∗(θ) = min\nw∈RM U(θ,w),\nwith an algorithm similar to that described earlier.\nAs a final reference, we note the improved WGAN algorithm introduced in Gul-\nrajani et al. [84] in which the boundedness constraint in the weights is replaced by\nan explicit control of the derivative in x of the function f . More precisely, introduce\na random variable Z with distribution ˜Pθ equal (1 −U)X + UX′ where U is uni-\nformly distributed over [0,1] and X and X′ are independent respectively following\nthe distribution Ptrue and Pθ. Then, the following approximation of the Wasserstein\ndistance between Ptrue and Pθ that is used in Gulrajani et al. [84]:\nˆDw(Ptrue,Pθ) = max\nw∈W\n\u0012\nEtrue(f (X,w)) −Eθ(f (X,w)) −˜Eθ((|∂zf (Z,w)| −1)2)\n\u0013\n.\n18.4\nReversed Markov chain models\n18.4.1\nGeneral principles\nThe discussions in sections 18.2 and 18.3 can be applied to sequences of structural\nequations (describing finite Markov chains) in the form\n\nZ0 = ξ0\nZk+1 = g(Zk,ξk;θk), k = 0,...,m −1\nX = Zm\nwhere ξ0,...,ξm−1 are random variables with fixed distribution.\nIndeed, letting ˜Z = (ξ0,...,ξn−1) and ˜θ = (θ0,...,θm−1) the whole system can be\nconsidered as a function X = G( ˜Z, ˜θ) as considered in these sections. This repre-\nsentation, however, includes a large number of hidden variables, and it is unclear\nwhether much improvement can be added to the case m = 1 to justify the additional\ncomputational load.\nReversed Markov chain models use a different generative approach in that they\nfirst model a forward Markov chain Zn,n ≥0 which is ergodic with known (and\neasy to sample from) limit distribution Q∞, and initial distribution Qtrue, the true\ndistribution of the data. If one fixes a large enough number of steps, say, τ, then it is\nreasonable to assume that Zτ approximately follows the limit distribution, Q∞. One\ncan then (approximately) sample from Qtrue by sampling ˜Z0 according to Q∞and\nthen applying τ steps of the time-reversed Markov chain.\n\n18.4. REVERSED MARKOV CHAIN MODELS\n445\nReversed chains were discussed in section 12.3.3. Assuming that Qtrue and P(z,·)\nhave a density with respect to a fixed measure µ on RZ, we found that ˜Zk = Zτ−k is a\nnon-homogeneous Markov chain whose transition probability ˜Pk(x,A) = P( ˜Zk+1 ∈A |\n˜Zk = x) has density\n˜pk(x,y) = p(y,x)qτ−k−1(y)\nqτ−k(x)\nwith respect to µ, where qn is the p.d.f. of Qn = QtruePn, the distribution of Zn.\nThe distributions Qn,n ≥0 are unknown, since they depend on the data dis-\ntribution Ptrue, and the transition probabilities above must be estimated from data\nto provide a sampling algorithm from the reversed Markov chain. While, at first\nglance, this does not seem like a simplification of the problem, because one now has\nto sample from a potentially large number (τ) of distributions instead of one, this\nleads, with proper modeling and some intensive learning, to efficient and accurate\nsampling algorithms.\nSeveral factors can indeed make this approach achievable. First, the forward\nchain should be making small changes to the current configuration at each step (e.g.,\nadding a small amount of noise). This ensures that the reversed transition probabili-\nties ˜pk(x,·) are close to Dirac distributions and are therefore likely to be well approx-\nimated by simple unimodal distributions such as Gaussians. Second, the estimation\nproblem does not have hidden data: given an observed sample, one can simulate τ\nsteps of the forward chain to obtain, after reversing the order, a full observation of\nthe reversed chain. Third, in some cases, analytical considerations can lead to partial\ncomputations that facilitate the modeling of the reversed transitions.\n18.4.2\nBinary model\nWe now take some examples, starting with a discrete one. Let Qtrue be the distribu-\ntion of a binary random field with state space {0,1} over a set of vertexes V , i.e., with\nthe notation of section 13.2, RX = F (V ) with F = {0,1}. Fix a small ϵ > 0 and define\nthe transition probability p(x,y) for x,y ∈F (V ) by\np(x,y) =\nY\ns∈V\n\u0010\n(1 −ϵ)1y(s)=x(s) + ϵ1y(s)=1−x(s)\n\u0011\n.\nSince p(x,y) > 0 for all x and y, the chain converges (uniformly geometrically) to its\ninvariant probability Q∞and one easily checks that this probability is such that all\nvariables are independent Bernoulli random variables with success probability 1/2.\nAssuming that τ is large enough so that Qτ ≃Q∞, the sampling algorithm initializes\nthe reversed chain as independent Bernoulli(1/2) variables and runs τ steps using\nthe transitions ˜pk which must be learned from data.\n\n446\nCHAPTER 18. DEEP GENERATIVE METHODS\nFor this model, we have\nqk(x) =\nX\ny∈F (V )\nqk−1(y)p(y,x).\nFor this transition, the probabililty of flipping two or more values of y is\n1 −(1 −ϵ)N −Nϵ(1 −ϵ)N−1 = N(N −1)\n2\nϵ2 + o(ϵ2)\nwith N = |V |. We will write x ∼s y if y(s) = 1 −x(s) and y(t) = x(t) for s , t, and we will\nwrite x ∼y if x ∼s y for some s. With this notation, we have\nqk(x) = (1 −Nϵ)qk−1(x) + ϵ\nX\ny:y∼x\nqk−1(y) + O(ϵ2)\nSince it implies that qk(x) = qk−1(x) + o(ϵ), this expression can be reversed as\nqk−1(y) = (1 + Nϵ)qk(y) −ϵ\nX\nx:x∼y\nqk(x) + O(ϵ2)\nSimilarly, we have\np(y,x) = (1 −Nϵ)1x=y + ϵ1x∼y + O(ϵ2).\nThis gives\np(y,x)qk−1(y) = qk(x)1x=y −ϵ1x=y\nX\nx′:x′∼y\nqk(x′) + ϵqk(y)1x∼y + O(ϵ2),\nand we finally get\n˜pk(x,y) =\n\n1 −ϵ\nX\nx′:x′∼y\nqτ−k(x′)\nqτ−k(x)\n\n1x=y + ϵqτ−k(y)\nqτ−k(x)1x∼y + O(ϵ2)\nIf one lets σ(s)\nk (x) = qτ−k(y)\nqτ−k(x) with y ∼s x, and defines\nˆpk(x,y) =\nY\ns∈V\n\u0012\n(1 −ϵσ(s)\nk (x))1y(s)=x(s) + ϵσ(s)\nk (x)1y(s)=1−x(s)\n\u0013\n,\none checks easily that ˆpk(x,y) = ˜pk(x,y)+O(ϵ2). This suggests modeling the reversed\nchain using transitions ˆpk, for which the mapping x 7→(σ(s)\nk (x),s ∈V ) needs to be\nlearned from data (for example using a deep neural network). Note that 1 −σk(x) is\nprecisely the score function introduced for discrete distributions in remark 17.8.\n\n18.4. REVERSED MARKOV CHAIN MODELS\n447\n18.4.3\nModel with continuous variables\nWe now switch to an example with vector-valued variables, RX = Rd, and assume\nthat the forward Markov chain is such that, conditionally to Xn = x,\nXn+1 ∼N (x + hf (x),\n√\nhIdRd),\nwhere f is C1. We saw in section 12.3.7 that, when f = −∇H/2 for a C2 function\nH such that exp(−H) is integrable, this chain converges (approximately for small h)\nto a limit distribution with p.d.f. (with respect to Lebesgue’s measure) proportional\nto exp(−H). In the linear case, in which f (x) = −Ax/2 for some positive-definite\nsymmetric matrix A, so that H(x) = 1\n2xT Ax, the limit distribution can be identified\nexactly as N (0,Σh) where Σh satisfies the equation\nAΣh + ΣhA −h\n2A2 −2IdRd = 0\nwhose solution is Σh = (A −hA2/4)−1 (details being left to the reader). This implies\nthat this limit distribution can be easily sampled from for any choice of A.\nWe now return to general f ’s and make, like in the discrete case, a first-order\nidentification of the reversed chain. We note that, for any smooth function γ,\nE(γ(Xn+1) | Xn = x) = E(γ(x + hf (x) +\n√\nhU))\nwhere U ∼N (0,IdRd). Making the second order expansion\nγ(x + hf (x) + hU) = γ(x) +\n√\nh∇γ(x)T U + h∇γ(x)T f (x) + h\n2UT ∇2γ(x)U + o(h)\nand taking the expectation gives\nE(γ(Xn+1) | Xn = x) = γ(x) + h∇γ(x)T f (x) + h\n2∆γ(x) + o(h).\n(18.10)\nConsidering the reversed chain, and letting qk denote the p.d.f. of Xk for the\nforward chain, we have\nE(γ(Xk−1) | Xk = x) =\nZ\nRd γ(y) ˜pk(x,y)dy\n=\nZ\nRd γ(y)p(y,x)qk−1(y)\nqk(x) dy\n=\n1\n(2πh)d/2\nZ\nRd γ(y)qk−1(y)\nqk(x) e−1\n2h|x−y−hf (y)|2dy\n=\n1\n(2π)d/2\nZ\nRd γ(x −\n√\nhu)qk−1(x −\n√\nhu)\nqk(x)\ne−1\n2|u−\n√\nhf (x−\n√\nhu)|2dy,\n\n448\nCHAPTER 18. DEEP GENERATIVE METHODS\nwith the change of variable u = (x −y)/\n√\nh. We make a first-order expansion of the\nterms in this integral, with\nγ(x −\n√\nhu)qk−1(x −\n√\nhu) = γ(x)qk−1(x) −\n√\nh∇(γqk−1)(x)T u + h\n2uT ∇2(γqk−1)(x)u + o(h)\nand\ne−1\n2|u−\n√\nhf (x−\n√\nhu)|2 = e−1\n2|u|2e\n√\nhuT f (x)−huT df (x)u−1\n2|f (x)|2+o(h)\n= e−1\n2|u|2  \n1 +\n√\nhuT f (x) −huT df (x)u −h\n2|f (x)|2 + h\n2|uT f (x)|2 + o(h)\n!\n.\nTaking products\nγ(x −\n√\nhu)qk−1(x −\n√\nhu)e−1\n2|u−\n√\nhf (x−\n√\nhu)|2\n= e−1\n2|u|2γ(x)qk−1(x)\n \n1 +\n√\nhuT f (x) −huT df (x)u −h\n2|f (x)|2 + h\n2|uT f (x)|2\n!\n+ e−1\n2|u|2  \n−\n√\nh∇(γqk−1)(x)T u −h(∇(γqk−1)(x)T u)(f (x)T u) + h\n2uT ∇2(γqk−1)(x)u\n!\n+ o(h)\nWe now take the integral with respect to u (recall that E(UT AU) = trace(A) if A is\nany square matrix and U is standard Gaussian), so that\n1\n(2π)d/2\nZ\nRd γ(x −\n√\nhu)qk−1(x −\n√\nhu)e−1\n2|u−\n√\nhf (x−\n√\nhu)|2du\n= γ(x)qk−1(x) + h\n\u0012\n−γ(x)qk−1(x)∇· f (x) −∇(γqk−1)(x)T f (x) + 1\n2∆(γqk−1)(x)\n\u0013\n+ o(h)\n= qk−1(x)\n\nγ(x) + h\n\n−γ(x)∇· f (x) −\n ∇(γqk−1)(x)\nqk−1(x)\n!T\nf (x) + 1\n2\n∆(γqk−1)(x)\nqk−1(x)\n\n\n\n+ o(h)\nTo compute an expansion of qk(x), it suffices to take γ = 1 above, so that\nqk(x) = qk−1(x)\n\n1 + h\n\n−∇· f (x) −\n ∇qk−1(x)\nqk−1(x)\n!T\nf (x) + 1\n2\n∆qk−1(x)\nqk−1(x)\n\n\n\n+ o(h).\nWe now take the first-order expansion of the ratio, removing terms that cancel, and\nget\nE(γ(Xk−1) | Xk = x) = γ(x) −h∇γ(x)T f (x) + h∇γ(x)T\n ∇qk−1(x)\nqk−1(x)\n!\n+ h\n2∆γ(x) + o(h)\nComparing with (18.10), we find that ˜Xk = Xτ−k behaves, for small h, like the\nnon-homogeneous Markov chain such that the conditional distribution of ˜Xk+1 given\n˜Xk = x is N (x −hf (x) −hsτ−k−1(x),\n√\nhIdRd), with sτ−k−1(x) = −∇logqτ−k−1, the score\nfunction introduced in section 17.2.6, and score-matching methods from that section\ncan be used to estimate it from observations of the forward chain initialized with\ntraining data.\n\n18.4. REVERSED MARKOV CHAIN MODELS\n449\n18.4.4\nContinuous-time limit\nThe forward schemes described in the previous examples can be interpreted as con-\ntinuous time processes over discrete or continuous variables. In the latter case, the\nexample Xk+1 ∼N (x+hf (x),\n√\nhIdRd) conditionally to Xk = x is a discretization of the\nstochastic differential equation\ndxt = f (xt)dt + dwt\n(see remark 12.5), where wt is a Brownian motion and the diffusion is initialized\nwith Qtrue. We found that going backward meant (at first order and conditionally to\nXk = x)\nXk−1 ∼N (x −hf (x) −hsk−1(x),\n√\nhId)\nthat we can rewrite as\nxτ −Xk−1 ∼N (xτ −x + hf (x) + hsk−1(x),\n√\nhId).\nFollowing the definition in Anderson [9], this corresponds to a first-order discretiza-\ntion of the reverse diffusion\ndxt = (f (xt) + st(xt))dt + d ˜wt, t ≤τ\nwhere ˜wt is also a Brownian motion. This reverse diffusion with Xτ ∼Q∞will there-\nfore approximately sample from Qtrue. (With this terminology, forward and reverse\ndiffusions have similar differential notation, but mean different things.) Note that,\nin the continuous-time limit, the reverse Markov process follows the distribution of\nthe reversed diffusion exactly.\n18.4.5\nDifferential of neural functions\nAs we have seen in the previous two examples, estimating the reversed Markov chain\nrequires computing the score functions of the forward probabilities. In the case\nof continuous variables, this score function is typically parametrized as a neural\nnetwork, so that the function sk(x) = −∇logqk(x) is computed as sk(x) = F(x;Wk),\nwith the usual definition F(x,Wk) = zm+1 with zj+1 = ϕj(zj,wjk), z0 = x and Wk =\n(w0k,...,wmk).\nAssume that a training set T is observed. Running the forward Markov chain\ninitialized with elements of T generates a new training step at each time step, that\nwe will denote Tk at step k. We have seen in section 17.2.6 that the score function sk\ncould be estimated by minimizing, with respect to W\nX\nx∈Tk\n\u0010\n|F(x,W)|2 −2∇· F(x,W)\n\u0011\n.\n\n450\nCHAPTER 18. DEEP GENERATIVE METHODS\nThis term involves the differential of F, which is defined recursively by (simply tak-\ning the derivative at each step)\ndF(x,W) = ζm+1, ζj+1 = dϕj(zj,wj)ζj,\nwith ζ0 = IdRd. From this recursive definition, back-propagation can be applied, in\nprinciple, to compute the derivative of dF(x,W) with respect to W. The feasibility of\nthis computation, however, is limited when d is large (d could be tens of thousands\nif one models images) computing the d × d matrix dF(x,W) is intractable.\nWe can note that, for any h ∈Rd, the vector dF(x,W)h also satisfies the recursion\ndF(x,W)h = ζm+1h, ζj+1h = dϕj(zj,wj)ζjh,\nwith ζ0h = h and\n∇· F(x,W) =\nd\nX\ni=1\neT\ni dF(x,W)ei\nwhere e1,...,ed is the canonical basis of Rd. Putting the divergence of F in this\nform does not reduce the computation cost (which is, roughly d2m, assuming that\nall zj’s have the same dimension), but expresses the divergence term in a form that is\namenable to stochastic gradient descent (which is typically already used to approx-\nimate the sum over x). Indeed, if U follows any distribution with zero mean and\ncovariance matrix equal to the identity (such as a standard Gaussian, or the uniform\ndistribution on the unit sphere), then\n∇· F(x,W) = E(UT dF(x,W)U)\nso that U can be sampled from in minibatches in SGD implementations (see [180],\nwhere this approach is called “sliced score matching”).\n\nChapter 19\nClustering\n19.1\nIntroduction\nWe now describe a collection of methods designed to divide a training set into ho-\nmogeneous subsets, or clusters. This grouping operation is a key problem in many\napplications for which it is important to categorize the data in order to obtain im-\nproved understanding of the sampled phenomenon, and sometimes to be able to\napply a different approach to subsequent processing or analysis adapted to each\ncluster.\nWe will assume that the variables of interest belong a set R = RX where R is\nequipped with a discrepancy function α : R × R →[0,+∞). Often, α is derived from\na distance ρ on R, but this is not always the case. We will assume that the data results\nfrom a training set T = (x1,...,xN). However, it may happen that only the discrep-\nancy matrix A = (α(x,y),x,y ∈T) is observed, while a coordinate representation of\nthe elements of T is not available.\nLet us consider a few examples.\n(i) The simplest case is when R = Rd with the standard Euclidean metric. Slightly\nmore generally, a metric may be defined by ρ2(x,y) = ∥h(x) −h(y)∥2\nH, where H is an\ninner-product space and the feature function h : R 7→H may be unknown, while its\nassociated “kernel”, K(x,y) = ⟨h(x) , h(y)⟩H is known (this is a metric if h is one-to-\none). In this case\nρ2(x,y) = K(x,x) −2K(x,y) + K(y,y).\nTypically, one then takes α = ρ or α = ρ2.\n(ii) Very often, however, the data is not Euclidean, and the distance does not cor-\nrespond to a feature space representation. This is the case, for example, for data be-\nlonging to “curved spaces” (manifolds), for which one may use the intrinsic distance\n451\n\n452\nCHAPTER 19. CLUSTERING\nprovided by the length of shortest paths linking two points (assuming of course that\nthis notion can be given a rigorous meaning). The simplest example is data on the\nunit sphere, where the distance ρ(x,y) between two points x and y is the length of\nthe shortest large circle that connects them, satisfying\n|x −y|2 = 2 −2cosρ(x,y).\nOnce again, α = ρ or ρ2 is a typical choice.\n(iii) A more complex example is provided by R being the space of symmetric\npositive-definite matrices on Rd, for which one defines the length of a differentiable\ncurve (S(t),t ∈[a,b]) in this space by\nZ b\na\nq\ntrace((S(t)−1∂tS)(S(t)−1∂tS)T )dt\nand for which\nρ2(S1,S2) =\nd\nX\ni=1\n(logλi)2\nwhere λ1,...,λd are the eigenvalues of S−1/2\n1\nS2S−1/2\n1\nor, equivalently, solutions of the\ngeneralized eigenvalue problem S2u = λS1u (see, for example, [72]).\n(iv) Another common assumption is that the elements of R are vertices of a weighted\ngraph of which T is a subgraph; ρ may then be, e.g., the geodesic distance on the\ngraph.\n19.2\nHierarchical clustering and dendograms\n19.2.1\nPartition trees\nThis method builds clusters by organizing them in a binary hierarchy in which the\ndata is divided into subsets, starting with the full training set, and iteratively split-\nting each subset into two parts until reaching singletons. This results in a binary\ntree structure, called a dendogram, or partition tree, which is defined as follows.\nDefinition 19.1 A partition tree of a finite set A is a finite collection of nodes T with the\nfollowing properties.\n(i) Each node has either zero or exactly two children. (We will use the notation v →v′\nto indicate that v′ is a child of v.\n(ii) All nodes but one have exactly one parent. The node without parent is the root of\nthe tree.\n(iii) To each node v ∈T is associated a subset Av ⊂A.\n\n19.2. HIERARCHICAL CLUSTERING AND DENDOGRAMS\n453\n1: {a,b,c,d,e,f }\n2: {a,c,f }\n3: {b,d,e}\n4: {a,f }\n5: {c}\n6: {d}\n7: {b,e}\n8: {a}\n9: {f }\n10: {b}\n11: {e}\nFigure 19.1: A partition tree of the set {a,b,c,d,e,f }.\n(iv) If v′ and v′′ are the children of v, then (Av′,Av′′) forms a partition of Av.\nNodes without children are called leaves, or terminal nodes. We will say that the hierarchy\nis complete if Av = A if v is the root, and |Av| = 1 for all terminal nodes.\nAn example of partition tree is provided in fig. 19.1.\nThe construction of the tree can follow two directions, the first one being bottom-\nup, or agglomerative, in which the algorithm starts with the collection of all single-\ntons and merges subsets one pair at a time until everything is merged into the full\ndataset. The second approach is top-down, or divisive, and initializes the algorithm\nwith the full training set which is recursively split until singletons are reached. The\nfirst approach, on which we now focus, is more common, and computationally sim-\npler.\nWe let T denote the training set and assume that a matrix of dissimilarities\n(α(x,y), x,y ∈T)\nis given. We will make the abuse of notation of considering that T is a set even\nthough some of its elements may be repeated. This is no loss of generality, since\nT = (x1,...,xN) can always be replaced by the subset {(k,xk),k = 1,...,N} of N × R.\n19.2.2\nBottom-up construction\nWe will extend α to a dissimilarity measure between subsets A,A′ ⊂T that we will\ndenote (A,A′) 7→ϕ(A,A′). Once ϕ is defined, agglomeration works along the follow-\ning algorithm.\nAlgorithm 19.1\n1. Start with the collection T1,...,TN of all single-node trees associated to each\nelement of T . Let n = 0 and m = N.\n\n454\nCHAPTER 19. CLUSTERING\n2. Assume that, at step n of the algorithm, one has a collection of partition trees\nT1,...,Tm with root nodes r1,...,rm associated with subsets Ar1,...,Arm of T. Let the\ntotal collection of nodes be indexed as Vn = {v1,...,vN+n}, so that {r1,...,rm} ⊂Vn.\n3. If m = 1, stop the algorithm.\n4. Select indices i,j ∈{1,...,m} such that ϕ(Ari,Arj) is minimal, and merge the\ncorresponding trees by creating a new node vn+1+N with the root nodes of Ti and Tj\nas children (so that vn+1+N is associated with Ari ∪Arj). Add vn+1+N to the collection\nof root nodes, and remove ri and rj.\n5. Set n →n + 1 and m →m −1 and return to step 2.\nClearly, the specification of the extended dissimilarity measure (ϕ) is a key ele-\nment of the method. Some of most commonly used extensions are:\n• Minimum gap: ϕmin(A,A′) = min(α(x,x′) : x ∈A,x′ ∈A′).\n• Maximum dissimilarity: ϕmax(A,A′) = max(α(x,x′) : x ∈A,x′ ∈A′).\n• Sum of dissimilarities:\nϕsum(A,A′) =\nX\nx∈A\nX\nx′∈A′\nα(x,x′)\n• Average dissimilarity:\nϕavg(A,A′) =\n1\n|A||A′|\nX\nx∈A\nX\nx′∈A′\nα(x,x′).\nAs shown in the next two propositions, the maximum distance favors clusters\nwith small diameters, while using minimum gaps tends to favor connected clusters.\nProposition 19.2 Let diam(A) = max(α(x,y),x,y ∈A). The agglomerative algorithm\nusing ϕmax is identical to that using ϕ(A,A′) = diam(A ∪A′).\nProof Call Algorithm 1 the agglomerative algorithm using ϕmax, and Algorithm 2\nthe one using ϕ. At initialization, we have (because all sets are singletons),\nϕmax(Ak,Al) = diam(Ak ∪Al) for all 1 ≤k , l ≤m.\n(19.1)\nWe show that this property remains true at all steps of the algorithms. Pro-\nceeding by induction, assume that, up to the step n, Algorithms 1 and 2 have been\nidentical and result in sets (A1,...,Am) satisfy (19.1). Then the next steps of the\ntwo algorithms coincide and assume, without loss of generality, that this next step\n\n19.2. HIERARCHICAL CLUSTERING AND DENDOGRAMS\n455\nmerges Am−1 with Am. Let A′\nm−1 = Am−1 ∪Am so that diam(A′\nm−1) ≤diam(Ai ∪Aj) for\nall 1 ≤i , j ≤m.\nWe need to show that the new partition satisfies (19.1), which requires that\nϕmax(A′\nm−1,Ak) = diam(A′\nm−1 ∪Ak)\nfor k = 1,...,m −2.\nWe have\ndiam(A′\nm−1 ∪Ak) = max(diam(A′\nm−1),diam(Ak),ϕmax(A′\nm−1,Ak)),\nso that we must show that\nmax(diam(A′\nm−1),diam(Ak)) ≤ϕmax(A′\nm−1,Ak).\nWrite\nϕmax(A′\nm−1,Ak) = max(ϕmax(Am,Ak),ϕmax(Am−1,Ak))\n= max(diam(Am ∪Ak),diam(Am−1 ∪Ak))\nwhere the last identity results from the induction hypothesis.\nThe fact that\ndiam(Ak) ≤max(diam(Am ∪Ak),diam(Am−1 ∪Ak))\nis obvious, and the inequality\ndiam(A′\nm−1) ≤max(diam(Am ∪Ak),diam(Am−1 ∪Ak))\nresults from the fact that Am and Am−1 was an optimal pair. This shows that the\ninduction hypothesis remains true at the next step and concludes the proof of the\nproposition.\n■\nWe now analyze ϕmin and, more specifically, the equivalence between the result-\ning algorithm and the one using the following measure of connectedness. For a given\nset A and x,y ∈A, let\n˜αA(x,y) = inf\nn\nϵ : ∃n > 0,∃(x = x0,x1,...,xn−1,xn = y) ∈An+1 :\nα(xi,xi−1) ≤ϵ for 1 ≤i ≤n\no\n.\nSo ˜αA is the smallest ϵ such that there exists a sequence of steps of size less than ϵ in\nA going from x to y. The function\nconn(A) = max{ ˜αA(x,y) : x,y ∈A}\nmeasures how well the set A is connected relative to the dissimilarity measure α.\nand we have:\n\n456\nCHAPTER 19. CLUSTERING\nProposition 19.3 The agglomerative algorithm using ϕmin is identical to that using\nϕ(A,A′) = conn(A ∪A′).\nProof The proof is similar to that of proposition 19.2. Indeed one can note that\nconn(A ∪A′) = max(conn(A),conn(A′),ϕmin(A,A′)).\nGiven this we can proceed by induction and prove that, if the current decomposi-\ntion is A1,...,Am such that ψ(Ak ∪Al) = ϕmin(Ak,Al) for all 1 ≤k , l ≤m, then this\nproperty is still true after merging using ϕmin and ϕ.\nAssuming again that Am−1 and Am are merged, and letting A′\nm−1 = Am ∪Am−1, we\nneed to show that conn(Ak ∪A′\nm−1) = ϕmin(Ak,A′\nm−1) for all k = 1,...,m −2, which is\nthe same as showing that:\nmax(conn(Ak),conn(A′\nm−1)) ≤ϕmin(Ak,A′\nm−1) = min(ϕmin(Ak,Am−1),ϕmin(Ak,Am)).\nFrom the induction hypothesis, we have\nmin(ϕmin(Ak,Am−1),ϕmin(Ak,Am)) = min(conn(Ak ∪Am−1),conn(Ak ∪Am))\nand both terms in the right-hand side are larger than conn(Ak) and also larger than\nconn(A′\nm−1) which was a minimizer.\n■\n19.2.3\nTop-down construction\nThe agglomerative method is the most common way to build dendograms, mostly\nbecause of the simplicity of the construction algorithm. The divisive approach is\nmore complex, because the division step, which requires, given a set A, to optimize\na splitting criterion over all two-partitions of A, may be significantly more expensive\nthan the merging steps in the agglomerative algorithm. The top-down construction\ntherefore requires the specification of a “splitting algorithm” σ : A 7→(A′,A′′) such\nthat (A′,A′′) is a partition of A. We assume that, if |A| > 1, then the partition A,A′′ is\nnot trivial, i.e., neither set is empty.\nGiven σ, the top-down construction is as follows.\nAlgorithm 19.2\n1. Start with the one-node partition tree T0 = (T ).\n2. Assume that at a given step of the algorithm, the current partition is T .\n3. If T is complete, stop the algorithm.\n4. For each terminal node v in T such that |Av| > 1, compute (A′\nv,A′′\nv ) = σ(Av) and\nadd two children v′ and v′′ to v with Av′ = A′\nv and Av′′ = A′′\nv .\n5. Return to step 2.\nThe division of a set into two parts is itself a clustering algorithm, and one may apply\nany of those described in the rest of this chapter.\n\n19.3. K-MEDOIDS AND K-MEAN\n457\n19.2.4\nThresholding\nOnce a complete hierarchy is built, it provides a complete binary partition tree T .\nThis tree provides in turn a collection of partitions of V, each of them obtained\nthrough pruning. We now formalize this operation.\nLet VT denote the set of terminal nodes in T and V0 = V \\ VT contain the interior\nnodes. Define a pruning set to be a subset D ⊂V0 that contains no pair of nodes v,v′\nsuch that v′ is a descendant of v. To any pruning set D, one can associate the pruned\nsubtree T (D) of T consisting of T from which all the vertices that are descendants\nof elements of D are removed. From any such pruned subtree, one obtain a partition\nS(D) of T formed by the collection of sets Av for v in the terminal nodes of T (D).\nBetween the extreme case S(v0) = {V} (where v0 is the root of T ) and S(∅) = ({x},x ∈\nVT ), there exists a huge number of possible partitions obtained in this way.\nIt is often convenient to organize these partitions according to the level sets of\na well-chosen score function v 7→h(v) defined over V0. For D ⊂V, we denote by\nmax(D) the set of its deepest elements, i.e., the set formed by those v ∈D that have\nno descendant in D. Then, for any λ ∈R, one can define D+\nλ = max{v : h(v) ≥λ} (resp.\nD−\nλ = max{v : h(v) ≤λ}) and the associated partition S(D+\nλ) (resp. S(D−\nλ)). The score\nfunction h can be linked to the construction algorithm. For example, if one uses a\nbottom-up construction using an extended dissimilarity ϕ, one can associate to each\nnode v with v ∈V0 the value of ϕ(Av′,Av′′) where v′ and v′′ are the children of v.\nAnother way to define such scores functions is by assigning weights to edges in\nT . Indeed, given a collection w of positive numbers w(v,v′) for v →v′ in T , one can\ndefine a score hw recursively by letting hw(v0) = 0 and hw(v′) = hw(v)+w(v,v′) if v′ is\na child of v. The choice w(v,v′) = 1 for all v,v′ provide the usual notion of depth in\nthe tree.\nScores can also be built bottom-up, letting h(v) = 0 for terminal nodes and, for\nv ∈V0,\nhw(v) = max(hw(v′) + w(v,v′),hw(v′′) + w(v,v′′))\nwhere v′,v′′ are the children of v Here, taking w = 1 provides the height of each\nnode.\n19.3\nK-medoids and K-mean\n19.3.1\nK-medoids\nOne of the limitation of hierarchical clustering is that it is a greedy approach that\ndoes not optimize a global quality measure associated to the partition. Such qual-\n\n458\nCHAPTER 19. CLUSTERING\nity measures can indeed be defined based on the heuristic that clusters should be\nhomogeneous (for some criterion) and far apart from each other.\nIn centroid-based methods, the homogeneity criterion is the minimum, over all\npossible points in R, of the sum of dissimilarities between elements of the cluster\nand that point. More precisely, for any A ⊂R, and any dissimilarity measure α,\ndefine the central dispersion index\nVα(A) = inf\n\nX\nx∈A\nα(x,c) : c ∈R\n.\n(19.2)\nIf c achieves the minimum in the definition of Vα, it is called a centroid of A for the\ndissimilarity α.\nThe most common choice is α = ρ2, where ρ is a metric on R, and in this case,\nwe will just use V in place of Vρ2. Note also that it is always possible to limit R to\nthe training set T, in which case the optimization in (19.2) is over a finite number of\ncenters. This makes centroid-based methods also applicable to the situation when\nthe matrix of dissimilarities is the only input provided to the algorithm, or when the\nset R and the function α are too complex for the optimization in (19.2) to be feasible.\nA centroid, c, in (19.2) may not always exists, and when it exists it may not always\nbe unique. For α = ρ2, a point c such that\nV (A) =\nX\nx∈A\nρ2(x,c)\nis called a Fr´echet mean of the set A. Returning to the examples provided in the\nbeginning of this chapter, two antipodal points on the sphere (whose distance is π)\nhave an infinity of Fr´echet means (or midpoints in this case) provided by every point\nin the equator between them. In contrast, the example provided with symmetric\nmatrices provides a so-called Hadamard space [44] and the Fr´echet mean in that\ncase is unique. Of course, for Euclidean metrics, the Fr´echet mean is just the usual\none.\nReturning to our general discussion, the K-medoids method optimizes the sum\nof central dispersions with a fixed number of clusters. Note that the letter K in K-\nmedoids originally refers to this number of clusters, but this notation conflicts with\nother notation in this book (e.g., reproducing kernels) and we shall denote by p this\n\n19.3. K-MEDOIDS AND K-MEAN\n459\ntarget number1. So the K-medoids method minimizes\nWα(A1,...,Ap) =\np\nX\ni=1\nVα(Ai)\nover all partitions A1,...,Ap of the training set T. Equivalently, it minimizes\nWα(A1,...,Ap,c1,...,cp) =\np\nX\ni=1\nX\nx∈Ai\nα(x,ci)\n(19.3)\nover all partitions of T and c1,...,cp ∈R. Finally, taking first the minimum with\nrespect to Ai, which corresponds to associating each x to the subset with closest\ncenter, K-medoids, an equivalent formulation minimizes\n˜Wα(c1,...,cp) =\nX\nx∈T\nmin\nn\nα(x,ci),i = 1,...,p\no\n.\nThe standard implementation of K-medoids solves this problem using an alter-\nnate minimization, as defined in the following algorithm.\nAlgorithm 19.3 (K-medoids)\nLet T ⊂R be the training set. Start with an initial choice of c1,...,cp ∈R and iterate\nover the following two steps until stabilization:\n(1) For i = 1,...,p, let Ai contain points x ∈T such that α(x,ci) = min{α(x,cj),j =\n1,...,p}. In case of a tie in this minimum, assign x to only one of the tied sets\n(e.g., at random) to ensure that A1,...,Ap is a partition.\n(2) For i = 1,...,p, let ci be a minimizer of P\nx∈Ai α(x,ci) if Ai is not empty, or ci be a\nrandom point in T otherwise.\nIt should be clear that each step reduces the total cost Wα and that this cost\nshould stabilize at some point (which provides the stopping criterion) because there\nis only a finite number of possible partitions of T. However, there can be many\npossible limit points that are stable under the previous iterations, and some may\ncorrespond to poor “local minima” of the objective function. Since the end-point of\nthe algorithm depends on the initialization, this step requires extra care. One may\ndesign ad-hoc heuristics in order to start the algorithm with a good initial point that\nis likely to provide a good solution at the end. These heuristics may depend on the\n1We still call the method K-medoids rather than p-medoids, to keep the name universally used in\nthe literature.\n\n460\nCHAPTER 19. CLUSTERING\nproblem at hand, or use a generic strategy. As a common example of the latter, one\nmay ensure that the initial centers are sufficiently far apart by picking c1 at random,\nc2 as far as possible from c1, c3 maximizing the sum of distances to c1 and c2 etc.\nOne also typically runs the algorithm several times with random initial conditions\nand select the best solution over these multiple runs.\nThe second step of Algorithm 19.3 can be computationally challenging depend-\ning on the set R and the dissimilarity measure α. When R = Rd and α = ρ2 is the\nsquare Euclidean distance, the solution is explicit and ci is simply the average of all\npoints in Ai. The resulting algorithm is the original incarnation of K-medoids, and\ncalled K-means [182, 121, 124]. K-means is probably the most popular clustering\nmethod and is often a step in more advanced approaches, as we will discuss later.\nThe two steps of Algorithm 19.3 are then simplified as follows.\nAlgorithm 19.4 (K-means)\nLet T ⊂Rd be the training set. Start with an initial choice of c1,...,cp ∈Rd and iterate\nover the following two steps until stabilization:\n(1) For i = 1,...,p, let Ai contain points x ∈T such that |x −ci|2 = min{|x −cj|2,j =\n1,...,p}. In case of tie in this minimum, assign x to only one of the tied sets\n(e.g., at random) to ensure that A1,...,Ap is a partition.\n(2) For i = 1,...,p, let\nci = 1\n|Ai|\nX\nx∈Ai\nx\nif Ai is not empty, or ci be a random point in T otherwise.\n19.3.2\nMixtures of Gaussian and deterministic annealing\nMixtures of Gaussian (MoG) were discusssed in chapter 16 and in Algorithm 16.2.\nRecall that they model the observed data X together with a latent class variable\nZ ∈{1,...,p} with joint distribution\nf (x,z;θ) = (2π)−d\n2 (detΣz)−1\n2αze−1\n2(x−cz)T Σ−1\nz (x−cz)\nwhere θ contains the weights, α1,...,αp, the means, c1,...,cp and the covariance ma-\ntrices Σ1,...,Σp (we create, hopefully without risk of confusion, a short-lived conflict\nof notation between the weights and the dissimilarity function). The posterior class\nprobabilities\nfZ(i|x; θ) =\n(detΣi)−1\n2αie−1\n2(x−ci)T Σ−1\ni (x−ci)\nPp\nj=1(detΣj)−1\n2αje−1\n2(x−cj)T Σ−1\nj (x−cj),\ni = 1,...,p,\n\n19.3. K-MEDOIDS AND K-MEAN\n461\nwhich are computed in step 3 of Algorithm 16.2 can be interpreted as a likelihood\nthat observation x belongs to group i. As a consequence, the mixture of Gaussian\nalgorithm can also be seen as a clustering method, in which one assigns each x ∈T\nto cluster i when i = argmax{fZ(j|x,θ) : j = 1,...,p}, making an arbitrary decision in\ncase of a tie.\nIn the special case in which all variances are fixed and equal to σ2IdRd, and all\nprior class probabilities are equal to 1/p (see remark 16.3), the EM algorithm for mix-\ntures of Gaussian is also called “soft K-means”, because it replaces the “hard” cluster\nassignments in K-means by “soft” ones represented by the update of the posterior\ndistribution. We repeat its definition here for completeness (where θ = (c1,...,cp)).\nAlgorithm 19.5 (Soft K-means)\n1. Choose a number σ2 > 0, a small constant ϵ and a maximal number of itera-\ntions M. Initialize the centers c = (c1,...,cp).\n2. At step n of the algorithm, let c be the current centers.\n3. Compute, for x ∈T and i = 1,...,p\nfZ(i|x,θ) =\ne−\n1\n2σ2 |x−ci|2\nPp\nj=1 e−\n1\n2σ2 |x−cj|2\nand let ζi = PN\nk=1 fZ(i|x,θ), i = 1,...,p.\n4. For i = 1,...,p, let\nc′\ni = 1\nζi\nX\nx∈T\nxfZ(i|x,θ).\n5. If |c′ −c| < ϵ or n = M: stop the algorithm.\n6. Replace c by c′ and n by n + 1 and return to step 2.\nWhen σ2 →0, fZ(·|xk,θ) converges to the uniform probability on indexes j such\nthat cj is closest to xk, which is a Dirac measure unless there are ties. Class allo-\ncation and center updating become then asymptotically identical to the K-means\nalgorithm. A variant of soft K-means, called deterministic annealing [169], applies\nAlgorithm 19.5 while letting σ slowly tend to 0. This new algorithm is experimen-\ntally more robust than K-means, in that it is less likely to be trapped in bad local\nminimums.\nRemark 19.4 The soft K-means algorithm can also be defined directly as an alternate\nminimization method for the objective function\nF(c,fZ) = 1\n2\nX\nx∈T\np\nX\nj=1\nfZ(j|x)|x −cj|2 + σ2 X\nx∈T\np\nX\nj=1\nfZ(j|x)logfZ(j|x),\n\n462\nCHAPTER 19. CLUSTERING\nwith the constraints fZ(j|x) ≥0 for all j and x and Pp\nj=1 fZ(j|x) = 1. One can check\n(we leave this as an exercise) that Step 3 in Algorithm 19.5 provides the optimal fZ\nfor F when c is fixed, and that Step 4 gives the optimal c when fZ is fixed (see ??). ♦\nRemark 19.5 We note that, if a K-means, soft K-means or MoG algorithm has been\ntrained on a training set T, it is then easy to assign a new sample ˜x to one of the\nclusters. Indeed, for K-means, it suffices to determine the center closest to ˜x, and\nfor the other methods to maximize fZ(j| ˜x,θ), which is computable given the model\nparameters. In contrast, there was no direct way to do so using hierarchical cluster-\ning.\n♦\n19.3.3\nKernel (soft) K-means\nWe now consider the soft K-means algorithm in feature space, and introduce fea-\ntures hk = h(xk) in an inner product space H such that ⟨hk , hl⟩H = K(xk,xl) for some\npositive definite kernel. As usual, the underlying assumption is that the computa-\ntion of h(x) does not need to be feasible, while evaluations of K(x,y) are easy. Let us\nconsider the minimization of\n1\n2\nX\nx∈T\np\nX\nj=1\nfZ(j|x)∥h(x) −cj∥2\nH + σ2 X\nx∈T\np\nX\nj=1\nfZ(j|x)logfZ(j|x)\nfor some σ2 > 0 (kernel K-means corresponds to taking the limit σ2 →0). Given fZ,\nthe optimal centers are\ncj = 1\nζ j\nX\nx∈T\nfZ(j|x)h(x)\nwith ζ = P\nx∈T fZ(j|x). They belong to the feature space, H, and are therefore not\ncomputable in general. However, the distance between them and a point h(y) ∈H is\nexplicit and given by\n∥h(y) −cj∥2\nH = K(y,y) −2\nζj\nX\nx∈T\nfZ(j|x)K(y,x) + 1\nζ2\nj\nX\nx,x′∈T\nfZ(j|x)fZ(j|x′)K(x,x′).\nThe class probabilities at each iteration can therefore be updated using\nfZ(j|x) =\ne−∥h(x)−cj∥2\nH / 2σ2\nPp\nj′=1 e−∥h(y)−cj′∥2\nH / 2σ2 .\nThis yields the soft kernel K-means algorithm, that we repeat below.\n\n19.3. K-MEDOIDS AND K-MEAN\n463\nAlgorithm 19.6 (Kernel soft K-means)\nLet T ⊂Rd be the training set. Initialize the algorithm with some choice for fZ(j|x),\nj = 1,...,p, x ∈T (for example: fZ(j|x) = 1/p for all j and x).\n(1) For j = 1,...,p and x ∈T compute\n∥h(x) −cj∥2\nH = K(x,x) −2\nζj\nX\nx′∈T\nfZ(j|x′)K(x,x′) + 1\nζ2\nj\nX\nx′,x′′∈T\nfZ(j|x′)fZ(j|x′′)K(x′,x′′)\nwith ζj = P\nx′∈T fZ(j|x′).\n(2) Compute, for x ∈T and j = 1,...,p,\nfZ(j|x) =\ne−∥h(x)−cj∥2\nH/2σ2\nPp\nj′=1 e−∥h(y)−cj′∥2\nH/2σ2 .\n(3) If the variation of fZ compared to the previous iteration is small, or if a maximum\nnumber of iterations has been reached, exit the algorithm.\n(4) Return to step 1.\nAfter convergence, the clusters are computed by assigning x to Ai when i = argmax{fZ(j|x) :\nj = 1,...,p}, making an arbitrary decision in case of a tie.\nFor “hard” K-means (with σ2 →0), step 2 simply updates fZ(j|x) as the uniform\nprobability on the set of indexes j at which ∥h(x) −cj∥2\nH is minimal.\n19.3.4\nConvex relaxation\nWe return to the initial formulation of K-means for Euclidean data, as a minimiza-\ntion, over all partitions A = {A1,...,AK} of {1,...,N} of\nW(A) =\nK\nX\nj=1\nX\nk∈Aj\n|xk −cj|2\nwhere cj is the average of the points xj such that j ∈Aj. We start with a simple\ntransformation expressing this function in terms of the matrix Sα of square distances\n\n464\nCHAPTER 19. CLUSTERING\nα(xk,xl) = |xk −xl|2. Indeed, we have\nX\nk∈Aj\n|xk −cj|2 =\nX\nk∈Aj\n|xk|2 −1\n|A|\n\f\f\f\f\f\f\f\f\nX\nk∈Aj\nxk\n\f\f\f\f\f\f\f\f\n=\nX\nk∈Aj\n|xk|2 −1\n|A|\nX\nk,l∈Aj\nxT\nk xl\n=\n1\n2|Aj|\nX\nk,l∈Aj\n(|xk|2 + |xl|2 −2xT\nk xl)\n=\n1\n2|Aj|\nX\nk,l∈Aj\n|xk −xl|2\nIntroduce the vector uj ∈RN with coordinates u(k)\nj\n= 1/\nq\n|Aj| for k ∈Aj and 0 other-\nwise. Then\n1\n2|Aj|\nX\nk,l∈Aj\n|xk −xl|2 = 1\n2uT\nj SαuJ = 1\n2trace(SαujuT\nj ).\n(19.4)\nLet\nZ(A) =\np\nX\nj=1\nujuT\nj ,\nso that Z(A) has entries Z(k,l)(A) = 1/|Aj| for k,l ∈Aj, j = 1,...p and 0 for all other\nk,l. Summing (19.4) over j, we get\nW(A) = 1\n2trace(SαZ(A)).\nThe matrix Z(A) is symmetric, has non-negative entries. It moreover satisfies\nZ(A)1N = 1N and Z(A)2 = Z(A). Interestingly, these properties characterize matri-\nces Z associated with partitions, as stated in the next proposition [153, 152].\nProposition 19.6 Let Z ∈MN(R) be a symmetric matrix with non-negative entries sat-\nisfying Z1N = 1N and Z2 = Z. The there exists a partition A of {1,...,N} such that\nZ = Z(A).\nProof Note that Z being symmetric and satisfying Z2 = Z imply that it is an orthog-\nonal projection with eigenvalues 0 and 1. In particular Z is positive semidefinite.\nThis implies that, for all i,j ∈{1,...,N}, one has\nZ(i,j)2 ≤Z(i,i),Z(j,j).\nThis inequality combined with PN\nj=1 Z(k,j) = 1 (expressing Z1N = 1N) shows that all\ndiagonal entries of Z are positive.\n\n19.3. K-MEDOIDS AND K-MEAN\n465\nDefine on {1,...,N} the relation k ∼j if and only if Z(j,k) > 0. The relation is\nsymmetric and we just checked that k ∼k for all k. It is also transitive, from the\nrelation (deriving from Z2 = Z)\nZ(k,j) =\nN\nX\ni=1\nZ(k,i)Z(i,j)\nwhich shows (since all terms in the sum are non-negative) that k ∼i and j ∼i imply\nk ∼j.\nLet A = {A1,...,Aq} be the partition of {1,...,N} formed by the equivalence classes\nfor this relation. We now show that Z = Z(A).\nWe have, for all k,j ∈{1,...,N}\nN\nX\ni=1\nZ(k,i)(Z(k,j) −Z(i,j)) = Z(k,j)\nN\nX\ni=1\nZ(k,i) −\nN\nX\ni=1\nZ(k,i)Z(i,j)\n= Z(k,j) −\nN\nX\ni=1\nZ(k,i)Z(i,j) = 0\nNow, if k,j ∈As for some s, the identity reduces to\nX\ni∈As\nZ(k,i)(Z(k,j) −Z(i,j)) = 0.\n(19.5)\nChoose k such that Z(k,k) = max{Z(i,i) : i ∈As}. Then, for all i,j ∈As, Z(i,j) ≤\np\nZ(i,i)Z(j,j) ≤Z(k,k) and (19.5) for j = k yields\nX\ni∈As\nZ(k,i)(Z(k,k) −Z(k,i)) = 0,\nwhich is only possible (since all Z(k,i) are positive) if Z(k,i) = Z(k,k) for all i ∈As.\nFrom Z(k,i) ≤\np\nZ(i,i)Z(k,k), we get Z(i,i) = Z(k,k) for all i, and therefore (reapply-\ning what we just found to i insteand of k) Z(i,j) = Z(i,i) = Z(k,k) for all i,j ∈As.\nFinally, we have\n1 =\nX\ni∈As\nZ(k,i) = |As|Z(k,k)\nshowing that Z(k,k) = 1/|As| and completing the proof that Z = Z(A).\n■\nNote that the number of clusters, |A| is equal to the trace of Z(A). This shows that\nminimizing W(A) over partitions with p clusters is equivalent to the constrained\noptimization problem minimizing\nG(Z) = trace(SαZ)\n(19.6)\n\n466\nCHAPTER 19. CLUSTERING\nover all matrices Z such that Z ≥0, ZT = Z, Z1N = 1N, trace(Z) = p and Z2 = Z.\nThis is still a difficult problem, since it is equivalent to K-means, which is NP hard.\nSeeing the problem in this form, however, is more amenable to approximations and,\nin particular, convex relaxations.\nIn [152], it is proposed to use a semidefinite program (SDP) as a relaxation. The\nconditions Z = ZT and Z2 = Z require that all eigenvalues of Z are either 0 or 1, and a\ndirect relaxation is to replace these constraints by ZT = Z and 0 ⪯Z ⪯IdRN. The last\ninequality is however redundant if we add the conditions 2 Z ≥0 and Z1 = 1. This\nis a consequence of the Perron-Frobenius theorem which states that a matrix ˜Z with\npositive entries has a largest (in modulus) real eigenvalue, which has multiplicity\none and is associated with an eigenvector with positive coordinates, the latter eigen-\nvector being (up to multiplication by a constant) the unique eigenvector of ˜Z with\npositive coordinates. So, if a matrix ˜Z is symmetric, satisfies ˜Z > 0 and ˜Z1N = 1N,\nthen ˜Z ⪯IdRN. Applying this result to ˜Z = (1 −ϵ)Z + (ϵ/N)1N1T\nN and letting ϵ tend\nto 0 shows that any matrix Z with non-negative entries satisfying Z1N = 1N also\nsatisfies Z ⪯IdRN.\nThis provides the following SDP relaxation of K-means [152]: minimize\nG(Z) = trace(SαZ)\n(19.7)\nsubject to ZT = Z, Z1N = 1N, trace(Z) = p, Z ≥0, Z ⪰0.\nClusters can be immediately inferred from the columns of the matrix Z(A), since\nthey are identical for two indices in the same cluster, and orthogonal to each other\nfor two indices in different clusters. Let z1(A),...,zN(A) denote the columns of Z(A)\nand ¯zk(A) = zk(A)/|zk(A)|. One has |¯zk(A) −¯zl(A)| = 0 if k and l belong to the same\ncluster and\n√\n2 otherwise.\nThese properties will not necessarily be satisfied by a solution, say, Z∗, of the\nSDP relaxation, but, assuming that the approximation is good enough, one may still\nconsider the normalized columns of Z∗and expect them to be similar for indices in\nthe same cluster, and away from each other otherwise. Denoting by ¯z∗\n1,..., ¯z∗\nN these\nnormalized columns, one can then run on them the standard K-means algorithm, or\na spectral clustering method such as those described in the next sections, to infer\nclusters.\nRemark 19.7 Clearly, one can use any symmetric matrix S in the definition of G in\n(19.6) and (19.7). The method is equivalent to, or to a relaxation of, K-means only\nwhen S is formed with squared norms in inner-product spaces, which does include\nkernel K-means, for which\nα(xk,xl) = K(xk,xk) −2K(xk,xl) + K(xl,xl).\n2Recall that Z ⪰0 means that Z is positive definite, while Z ≥0 indicates that all its entries are\nnon-negative.\n\n19.4. SPECTRAL CLUSTERING\n467\nIf α is an arbitrary discrepancy measure, the minimization of G(Z) still makes sense,\nsince it is equivalent to minimizing\nG(Z(A)) =\np\nX\nj=1\nDα(Aj).\nwhere\nDα(A) = 1\n|A|\nX\nx,y∈A\nα(x,y).\n(19.8)\nis a (normalized) measure of size, that we will call the α-dispersion of a finite set A.♦\nRemark 19.8 Instead of using dissimilarities, some algorithms are more naturally\ndefined in terms of similarities. Given such a similarity measure, say, β, one must\nmaximize rather than minimize the index ∆β (which becomes, rather than a measure\nof dispersion, a measure of concentration).\nOne passes from a dissimilarity α to a similarity β by applying a decreasing func-\ntion to the former, a common choice being\nβ(x,x′) = exp(−α(x,x′)/τ)\nfor some τ > 0.\nAlternatively, one can fix an element x0 ∈R and let\nβ(x,y) = α(x,x0) + α(y,x0) −α(x,y) −α(x0,x0),\n(note that the last term, α(x0,x0) is generally equal to 0). For example, if α(x,y) =\n|x −y|2, then β(x,y) = 2(x −x0)T (y −x0) (for which it is natural to take x0 = 0). If α is a\ndistance (not squared!), then β ≥0 by the triangular inequality. In this case, we have\n∆β(A1,...,Ap) =\nn\nX\nk=1\nDβ(Ak)\n=\np\nX\nk=1\n1\n|Ap|\nX\nx,y∈Ak\nα(x,x0) +\np\nX\nk=1\n1\n|Ap|\nX\nx,y∈Ak\nα(y,x0)\n−\np\nX\nk=1\n1\n|Ap|\nX\nx,y∈Ak\nα(x0,x0) −\np\nX\nk=1\n1\n|Ap|\nX\nx,y∈Ak\nα(x,x0)\n= 2\np\nX\nk=1\nX\nx∈Ak\nα(x,x0) −\np\nX\nk=1\n|Ak|α(x0,x0) −∆α(A1,...,Ap)\n= 2\nX\nx∈T\nα(x,x0) −|T |α(x0,x0) −∆α(A1,...,Ap)\n♦\nso that minimizing ∆α is equivalent to maximizing ∆β.\n\n468\nCHAPTER 19. CLUSTERING\n19.4\nSpectral clustering\n19.4.1\nSpectral approximation of minimum discrepancy\nOne refers to spectral methods algorithms that rely on computing eigenvectors and\neigenvalues (the spectrum) of data-dependent matrices. In the case of minimizing\ndiscrepancies, they can be obtained by further simplifying (19.7), essentially by re-\nmoving constraints.\nOne indeed gets a simpler problem if the non-negativity constraint, Z ≥0, is\nremoved. Doing so, one cannot guarantee anymore that Z ⪯IdRN, so we need to\nreinstate this constraint. We will first make the further simplification to remove\nthe constraint Z1N = 1N, the problem becoming minimizing trace(SαZ) over all Z ∈\nS+\nN(R) such that 0 ⪯Z ⪯IdRN and trace(Z) = p. Decomposing Z in an eigenbasis,\ni.e., looking for it in the form\nZ =\nN\nX\nj=1\nξjejeT\nj ,\nthis is equivalent to minimizing\nN\nX\nj=1\nξjeT\nj Sαej\n(19.9)\nsubject to 0 ≤ξj ≤1, PN\nj=1 ξj = p and u1,...,uN orthonormal basis of RN. First con-\nsider minimization with respect to the basis, fixing ξ. There is obviously no loss of\ngenerality in requiring that ξ1 ≤ξ2 ≤··· ≤ξN, and using corollary 2.4 (adapted\nto minimizing (19.9) rather than maximizing it) we know that an optimal basis\nis given by the eigenvectors of Sα, ordered with non-decreasing eigenvalues. Let-\nting λ1 ≤··· ≤λN denote these eigenvalues, we find that ξ1,...,xN must be a non-\ndecreasing sequence minimizing\nN\nX\nj=1\nλjξj\nsubject to 0 ≤ξk ≤1 and PN\nj=1 ξj = p. The optimal solution is obtained by taking\n\n19.4. SPECTRAL CLUSTERING\n469\nξ1 = ··· = ξp = 1, since, for any other solution\nN\nX\nj=1\nλjξj −\np\nX\nj=1\nλj ≥λp+1\nN\nX\nj=p+1\nξj +\np\nX\nj=1\nλj(ξj −1)\n= λp+1\np\nX\nj=1\n(1 −ξj) +\np\nX\nj=1\nλj(ξk −1)\n=\np\nX\nj=1\n(λp+1 −λj)(1 −ξj)\n≥0.\nThe following algorithm (similar to that discussed in [64]) summarizes this dis-\ncussion.\nAlgorithm 19.7 (Spectral clustering: version 1)\nLet Sα be an N × N discrepancy matrix. Let p denote the number of clusters.\n(1) Compute the eigenvectors of Sα associated with the p smallest eigenvalues.\n(2) Denoting these eigenvectors by e1,...,ep, define y1,...,yN ∈Rp by y(j)\nk = e(k)\nj .\n(3) Run K-means on (y1,...,yN) to determine a partition.\nThis algorithm needs to be slightly modified if one also wants Z to satisfy Z1 = 1.\nIn that case, 1 is one of the eigenvectors (with eigenvalue 1), and the others are\northogonal to it. As a consequence, one now looks for Z in the form\nZ =\nN−1\nX\nk=1\nξjejeT\nj + 1\nN 11T\nleading to the minimization of\nN−1\nX\nj=1\nξjeT\nj Sαej + 1\nN 1T Sα1\nover all ξ1,...,ξN−1 such that 0 ≤ξj ≤1 and PN\nj=1 ξj = p −1, and over all e1,...,eN−1\nsuch that e1,...,eN−1,1/\n√\nN form an orthonormal basis. The main difference with the\nprevious problem is that we now need to ensure that all ej are perpendicular to 1.\n\n470\nCHAPTER 19. CLUSTERING\nTo achieve this, introduce the projection matrix P = IdRN −11T /N and let ˜Sα =\nPSαP. Then, since uT1 = 0 implies uT ˜Sαu = uT Sαu, it is equivalent to minimize\nN−1\nX\nj=1\nξjeT\nj ˜Sαej\nover all ξ1,...,ξN−1 such that 0 ≤ξj ≤1 and PN\nj=1 ξj = p −1, and over all e1,...,eN−1\nsuch that e1,...,eN−1,1/\n√\nN form an orthonormal basis. Because ˜Sα1 = 0, we know\nthat ˜Sα can be diagonalized in an orthonormal basis (e1,...,eN−1,1/\n√\nN), and we ob-\ntain an optimal solution by selecting the p−1 vectors associated with smallest eigen-\nvalues, with associated ξj = 1. We therefore get a modified version of the spectral\nclustering algorithm.\nAlgorithm 19.8 (Spectral clustering: version 2)\nLet Sα be an N × N discrepancy matrix. Let p denote the number of clusters. Let\nP = IdRN −1N1T\nN/N.\n(1) Compute ˜Sα = PSαP\n(2) Compute the eigenvectors of ˜Sα associated with the p −1 smallest eigenvalues.\n(3) Denoting these eigenvectors by e1,...,ep−1, define y1,...,yN ∈Rp−1 by y(j)\nk = e(k)\nj .\n(4) Run K-means on (y1,...,yN) to determine a partition.\n19.5\nGraph partitioning\nSimilarity measures are often associated with graph structures, with a goal of finding\na partition of their set of vertices. So, let T denote the set of these vertices and\nassume that to all pairs x,y ∈T, one attribute a weight given by β(x,y), where β is\nassumed to be non-negative. We define β for all x,y ∈T, but we interpret β(x,y) = 0\nas marking the absence of an edge between x and y.\nLet V denote the vector space of all functions f : T →R (we have dim(V ) = |T|).\nThis space can be equipped with the standard Euclidean norm, that we will call\nin this section the L2 norm (by analogy with general spaces of square integrable\nfunctions), letting,\n|f |2\n2 =\nX\nx∈T\nf (x)2.\nOne can also associate a measure of smoothness for a function f ∈V by computing\nthe discrete “H1” semi-norm,\n|f |2\nH1 =\nX\nx,y∈T\nβ(x,y)(f (x) −f (y))2.\n\n19.5. GRAPH PARTITIONING\n471\nWith this definition, “smooth functions” tend to have similar values at points x,y\nin T such that β(x,y) is large while there is less constraint when β(x,y) is small. In\nparticular, |f |H1 = 0 if and only if f is constant on connected components of the\ngraph.3\nThe notion of connected components, combined with thresholding, can be used\nto build a hierarchical family of partitions of the graph. Define, for all t > 0, the\nthresholded weights β(t)(x,y) = max(β(x,y) −t,0). The set of connected components\nassociated with the pair (V ,β(t)) forms a partition, say, A(t), of T . The resulting set\nof partitions is nested in the sense that, if s < t, the sets forming the partition A(s) are\nunions of sets forming A(t). This thresholding procedure is not always satisfactory,\nhowever, because there does not always exist a fixed value of t that produces a good\nquality cluster decomposition.\nIf there exists p connected components, then the subspace of all functions f ∈V\nsuch that |f |H1 = 0 has dimension p. If C1,...,Cp are the connected components,\nthis space is generated by the functions δCk, k = 1,...,p, with δCk(x) = 1 if x ∈Ck\nand 0 otherwise. These functions form, in addition, an orthogonal system for the\nEuclidean inner product: ⟨δCk , δCl⟩2 = 0 if k , l.\nOne can write 1\n2|f |2\nH1 = f T Lf where L, called the Laplacian operator associated to\nthe considered graph, is defined by\nLf (x) =\nX\ny∈T\nL(x,y)f (y)\nand\nL(x,y) =\n\n\nX\nz∈T\nβ(x,z)\n\n1x=y −β(x,y).\n(19.10)\nThe vectors δCk, k = 1,...,p are then an orthogonal basis of the null space of L. Con-\nversely, let (e1,...,ep) be any basis of this null space. Then, there exists an invertible\nmatrix A = (aij,i,j = 1,...,p) such that\nei(x) =\np\nX\nj=1\naijδCj(x).\nAssociate to each x ∈T the vector e(x) =\n\n\ne1(x)\n...\nep(x)\n\n\n∈Rp. Then, for any x,y ∈T, we have\ne(x) = e(y) if and only if δCj(x) = δCj(y) for all j = 1,...,p (because A is invertible),\n3Two nodes x and y are connected in the graph if there is a sequence z0,...,zn in T such that z0 = x,\nzn = y and β(zi,zi−1) > 0 for i = 1,...,n. This provides an equivalence relation and equivalent classes\nare called connected components.\n\n472\nCHAPTER 19. CLUSTERING\nthat it, if and only if x and y belong to the same connected component. So, given\nany basis of the null space of L, the function x 7→e(x) determines these connected\ncomponents. So, a—not very efficient—way of determining the connected compo-\nnents of the graph can be to diagonalize the operator L (written as an N by N matrix,\nwhere N = |T|), extract the p eigenvectors e1,...,ep associated with eigenvalue zero\nand deduce from the function e(x) above the set of connected components.\nNow, in practice, the graph associated to T and β will not separate nicely into\nconnected components in order to cluster the training set. Most of the time, because\nof noise or some weak connections, there will be only one such component, or in any\ncase much less than what one would expect when clustering the data. The previous\ndiscussion suggests, however, that in the presence of moderate noise in the con-\nnection weights, one may expect that the eigenvectors associated to the p smallest\neigenvalues of L provide vectors e(x),x ∈T such that e(x) and e(y) have similar values\nif x and y belong to the same cluster (see 19.2). In such cases, these clusters should\nbe easy to determine using, say, K-means on the transformed dataset ˜T = (e(x),x ∈T ).\nThis is summarized in the following algorithm.\nAlgorithm 19.9 (Spectral Graph Partitioning)\nLet T ⊂R be the training set and (x,y) 7→β(x,y) a similarity measure defined on\nT × T . Let p be the desired number of clusters.\n(1) Form the Laplacian operator described in (19.10) and let e1,...,ep be its eigen-\nvectors associated to the p lowest eigenvalues. For x ∈T, let e(x) ∈Rp be given\nby\ne(x) = (e1(x),...,ep(x))T ∈Rp.\n(2) Apply the K-means algorithm (or one of its variants) with p clusters to ˜T =\n(e(x),x ∈T).\n19.6\nDeciding the number of clusters\n19.6.1\nDetecting elbows\nThe number, p, of subsets with respect to which the population should be parti-\ntioned is rarely known a priori, and several methods have been introduced in the\nliterature in order to assess the ideal number of clusters. We now review some of\nthese methods, and denote, for this purpose, by L∗(p) the minimized cost function\nobtained with p clusters, e.g., using (19.3),\nL∗(p) = min{Wα(A1,...,Ap,c1,...,cp) : A1,...,Ap partition of T ,c1,...,cp ∈R},\n\n19.6. DECIDING THE NUMBER OF CLUSTERS\n473\nFigure 19.2:\nExample of data transformed using the eigenvectors of the graph Laplacian.\nLeft: Original data. Center: Result of a Kmeans algorithm with three clusters applied to the\ntransformed data (2D projection). Right: Visualization of the cluster labels on the original\ndata.\nin the case of K-medoids (this definition is algorithm dependent). It is clear that L∗\nis a decreasing function of p. It is also natural to expect that L∗should decrease\nsignificantly when p is smaller than the correct number of clusters, while the varia-\ntion should be more marginal when p is overestimated, because the cost in putting\ntogether two sets of points that are far apart (which happens when p is too small) is\ntypically larger than the gain in splitting a homogeneous region in two.\nThe simplest approach in this context is to visualize L∗(p) as a function of p and\ntry to locate at which value the resulting curve makes an “elbow,” i.e., switches from\na sharply decreasing slope to a milder one. Figure 19.3 provides an illustration of\nthis visualization when the true number of clusters is three (the data in each cluster\nfollowing a normal distribution). When the clusters are well separated, an elbow\nclearly appears on the graph of Γ∗\nα, but this situation is harder to observe when clus-\nters overlap with each other.\nOne can measure the “curvature” at the elbow using the distance between each\npoint in the graph of (p,W ∗\nα(p)) and the line between its predecessor and successor.\nThe result gives the criterion\nC(p) = L∗(p + 1) + L∗(p −1) −2L∗(p)\np\n(L∗(p + 1) −L∗(p −1))2 + 4\n,\nspecifying the elbow point as the value of p at which C attains its maximum. For\nboth examples in fig. 19.3, this method returns the correct number of clusters (3).\n19.6.2\nThe Cali´nski and Harabasz index\nSeveral other criteria have been introduced in the literature. Cali´nski and Harabasz\n[46] propose to minimize the ratio of normalized between-group and within-groups\nsums of squares associated with K-means. For a given p, let c1,...,cp denote the\noptimal centers, and A1,...,Ap the optimal partition, with Nk = |Ak|. The normalized\n\n474\nCHAPTER 19. CLUSTERING\nFigure 19.3: Elbow graphs for K-means clustering for two populations generated as mixtures\nof Gaussian.\nbetween-group sum of squares is\nhα(p) =\n1\np −1\np\nX\nk=1\nNk|ck −x|2\nand the normalized within-group sum of squares is\nwα(p) =\n1\nN −p\np\nX\nk=1\nX\nx∈Ak\n|x −ck|2\nCali´nski and Harabasz [46] suggest to maximize γCH(p) = hα(p)/wα(p).\nThis criterion can be extended to other types of cluster analysis. We have seen in\nsection 19.4 that, when α(x,y) = |x −y|2,\n1\n2\np\nX\nk=1\nX\nx,y∈Ak\nα(x,y)/Nk =\np\nX\nk=1\nX\nx∈Ak\n|x −ck|2.\nWe also have\nX\nx∈T\n|x −x|2 =\np\nX\nk=1\nX\nx∈Ak\n|x −ck|2 +\np\nX\nk=1\nNk|ck −x|2\n\n19.6. DECIDING THE NUMBER OF CLUSTERS\n475\nand the left-hand side is also equal to\n1\n2N\nX\nx,y∈T\nα(x,y).\nIt follows that, when α(x,y) = |x −y|2,\nhα(p) =\n1\n2(p −1)\n\n\n1\nN\nX\nx,y∈T\nα(x,y) −\np\nX\nk=1\nX\nx,y∈Ak\nα(x,y)/Nk\n\n\nand\nwα(p) =\n1\n2(N −p)\np\nX\nk=1\nX\nx,y∈Ak\nα(x,y)/Nk.\nThese expressions can obviously be applied to any dissimilarity measure, extending\nγCH to general clustering problems.\n19.6.3\nThe “silhouette” index\nFor x ∈T , let\ndα(x,Ak) = 1\nNk\nX\ny∈Ak\nα(x,y).\nLet aα(x,p) = dα(x,A(x)) and b(x,p) = min{dα(x,Ak) : Ak , A(x)}. Define the silhouette\nindex of x in the segmentation [170]by\nsα(x,p) =\nbα(x,p) −aα(x,p)\nmax(bα(x,p),aα(x,p)) ∈[−1,1].\nThis index measures how well x is classified in the partitioning. It is large when\nthe mean distance between x and other objects in its class is small compared to the\nminimum mean distance between x and any other class. In order to estimate the best\nnumber of clusters with this criterion, one then can maximize the average index:\nγR(p) = 1\nN\nX\nx∈T\nsα(x,p).\nRemark 19.9 One can rewrite the Cali´nski and Harabasz index using the notation\nintroduced for the silhouette index. Indeed, let A(x) be the cluster Ak to which x\nbelongs. Then\nhα(p) =\n1\n2(p −1)\nX\nx∈T\np\nX\nk=1\nNk\nN (dα(x,Ak) −dα(x,A(x)))\nand\nwα(p) =\n1\n2(N −p)\np\nX\nk=1\nX\nx∈Ak\ndα(x,Ak).\n♦\n\n476\nCHAPTER 19. CLUSTERING\nFigure 19.4: Division of the unit square into clusters for uniformly distributed data.\n19.6.4\nComparing to homogeneous data\nSeveral selection methods choose p based on the comparison of the data to a “null\nhypothesis” of no cluster. For example, assume that K-means is applied to a training\nset T where samples are drawn uniformly according to the uniform distribution on\n[0,1]d. Given centers, c1,...,cp, let ¯Ak be the set of points in [0,1]d that are closer\nto ck than to any other point. Then the segmentation of T is formed by the sets\nAk = {x ∈T : x ∈¯Ak} and, for large enough N, we can approximate |Ak|/N (by the\nLaw of Large Numbers) by the volume of the set ¯Ak, that we will denote by vol( ¯Ak).\nLet us assume that c1,...,cp are uniformly spaced, so that the sets ¯Ak have similar\nvolumes (close to 1/p) and have roughly spherical shapes (see fig. 19.4). This implies\nthat\nZ\n¯Ak\n|x −ck|2dx ≃vol(Ak)\nr2\npd\nd + 2\nwhere rp is the radius of a sphere of volume 1/p, i.e., prd\np ≃d/Γd−1 where Γd−1 is the\nsurface area of the unit sphere in Rd. So, we should have, for some constant C that\nonly depends on d,\nX\nx∈Ak\n|x −ck|2 ≃Nk\nZ\n¯Ak\n|x −ck|2dx ≃C(d)(pN)p−2/d−1 = C(d)Np−2/d.\nThis suggests that, for fixed N and d, p2/dL∗(p) should vary slowly when p overesti-\n\n19.6. DECIDING THE NUMBER OF CLUSTERS\n477\nmate the number of clusters (assuming that this operation divides an homogeneous\ncluster). Based on this analysis, Krzanowski and Lai [111] introduced the difference-\nratio criterion, namely,\nγKL(p) =\n\f\f\f\f\f\f\f\n(p −1)\n2\nd L∗(p −1) −p\n2\nd L∗(p)\np\n2\nd L∗(p) −(p + 1)\n2\nd L∗(p + 1)\n\f\f\f\f\f\f\f\n,\nand estimate the number of clusters by taking p maximizing γKL.\nAnother similar approach, introduced by Sugar and James [185], is based on an\nanalysis of mixtures of Gaussian, namely assuming an underlying model with p0\ngroups, where data in group k follow a Gaussian distribution N (µk,Id) (possibly\nafter standardizing the covariance matrix). In that work, the authors show that,\nif d (the dimension) tends to infinity, with the minimal distance between centers\ngrowing proportionally to\n√\nd, then L∗(p)/d tends to infinity when p < p0. They also\nshow that, with similar assumptions, L∗(p)/d behaves like p−2/d for p ≥p0, still for\nlarge dimensions. Based on this, they suggest using the criterion\nγSJ(p) =\n L∗(p)\nd\n!−ν\n−\n L∗(p −1)\nd\n!−ν\n(with the convention that L∗(0) = 0) for some positive number ν and select the value\nof p that maximizes γSJ. Indeed, in the case of Gaussian mixtures, the choice ν = d/2\nensures that, in large dimensions, γSJ(p) is small for p < p0, that it is close to 1 for\np > p0 and close to p0 for p = p0.\nA more computational approach, based on Monte-Carlo simulations has been\nintroduced in Tibshirani et al. [191], defining the gap index\nγTWH(p) = E(L∗(p,T ♯)) −L∗(p,T )\nwhere the L∗(p,T ) denotes the optimal value of the optimized cost with p clusters\nfor a training set T. The notation T ♯represent a random training set, with same\nsize and dimension as T , generated using an unclustered probability distribution\nused as a reference. In Tibshirani et al. [191], this distribution is taken as uniform\n(over the smallest hypercube containing the observed data), or uniform on the co-\nefficients of a principal component decomposition of the data (see chapter 20). The\nexpectation E(L∗(p,T ♯)) is computed by Monte-Carlo simulation, by sampling many\nrealizations of the training set T , running the clustering algorithm for each of them\nand averaging the optimal costs.\nOne can expect L∗(p,T ) (for observed data) to decrease much faster (when adding\na cluster) than its expectation for homogeneous data when p < p0, and the decrease of\n\n478\nCHAPTER 19. CLUSTERING\nboth terms to be comparable when p ≥p0. So the number of clusters can in principle\nbe estimated by detecting an elbow in the graph of γTWH(p) as a function of p. The\nprocedure suggested in Tibshirani et al. [191] in order to detect this elbow if to look\nfor the first index p such that\nγTWH(p + 1) ≤γTWH(p) + σ(p + 1)\nwhere σ(p + 1) is the standard deviation of L∗(p + 1,T ♯) for homogeneous data, also\nestimated via Monte-Carlo simulation.\nFigures figs. 19.5 to 19.7 provide a comparative illustration of some of these in-\ndexes.\n19.7\nBayesian Clustering\n19.7.1\nIntroduction\nWe have seen an example of model-based clustering with mixtures of Gaussian dis-\ntributions. The main parameters in this model were the number of classes, p, and\nthe probabilities αj associated to each cluster, and the parameter of the conditional\ndistribution (e.g., N (cj,σ2IdRd)) of X conditionally to being in the jth cluster. In the\napproach we described, these parameters were estimated from data using maximum\nlikelihood (through the EM algorithm) and probabilities fZ(j|x) were then estimated\nin order to compute the most likely clustering.We interpreted fZ(j|x) as the condi-\ntional probability P(Z = z|X = x), where Z ∈{1,...,p} represents the group variable.\nThe natural generative order is Z →X: first decide to which group the observation\nbelongs to, then sample the value of X conditional to this group. Clustering is in this\ncase reversing the order, i.e., computing the posterior distribution of Z given X.\nIn a Bayesian approach, the parameters p,α,c and σ2 are also considered as ran-\ndom variables, so that (letting θ denote the vector formed by these parameters), the\ngenerative random sequence becomes θ →Z →X. Importantly, θ is assumed to\nbe generated once for all, even if several samples of X are observed, yielding the\ngenerative sequence for an N-sample,\nθ →(Z1,...,ZN) →(X1,...,XN).\nWe use below underlined letters to denote configurations of points, Z = (Z1,...,ZN),\nX = (X1,...,XN), etc. We also use capital letters or boldface letters (for Greek sym-\nbols) to differentiate random variable from realizations.\nClusters are still evaluated based on the conditional distribution of Z given X,\nbut this distribution must be evaluated by averaging the conditional distribution of\n\n19.7. BAYESIAN CLUSTERING\n479\nFigure 19.5: Comparison of cluster indices for Gaussian clusters. First row: original data\nand ground truth. Second panel: plots of four indices as functions of p (Elbow; Cali´nski and\nHarabasz; silhouette; Sugar and James)\n\n480\nCHAPTER 19. CLUSTERING\nFigure 19.6: Comparison of cluster indices for Gaussian clusters. First row: original data\nand ground truth. Second panel: plots of four indices as functions of p (Elbow; Cali´nski and\nHarabasz; silhouette; Sugar and James).\n\n19.7. BAYESIAN CLUSTERING\n481\nFigure 19.7: Comparison of cluster indices for Gaussian clusters. First row: original data\nand ground truth. Second panel: plots of four indices as functions of p (Elbow; Cali´nski and\nHarabasz; silhouette; Sugar and James).\n\n482\nCHAPTER 19. CLUSTERING\nZ and θ given X with respect to θ, formally4,\nP(z|x) =\nZ\nP(z,θ|x)P(θ)dθ\n∝\nZ\nN\nY\nk=1\nP(xk|zk,θ)P(zk|θ)P(θ)dθ.\nIn this expression, P(θ)dθ implies an integration with respect to the prior distribu-\ntion of the parameters. This distribution is part of the design of the method, but one\nusually chooses it so that it leads to simple computations, using so-called conjugate\npriors, which are such that posterior distributions belong to the same parametric\nfamily as the prior. For example, the conjugate prior for the mean of a Gaussian\ndistribution (such as ci in our model) is also a Gaussian distribution. The conjugate\nprior for a scalar variance is the inverse gamma distribution, with density\nvu\nΓ(u)s−u−1 exp(−v/s)\nfor some parameters u,v. A conjugate prior for the class probabilities α = (α1,...,αp)\nis the Dirichlet distribution, with density\nD(α1,...,αp) =\nΓ(a1 + ··· + ap)\nΓ(a1)···Γ(ap)\np\nY\nj=1\nα\naj−1\nj\non the simplex\nSp = {(α1,...,αp) ∈Rp : αi ≥0,α1 + ··· + αp = 1}.\nNote that these conjugate priors have the same form (up to normalization) as the\nparametric model densities when considered as functions of the parameters.\n19.7.2\nModel with a bounded number of clusters\nWe first discuss the Bayesian approach assuming that the number of clusters is\nsmaller than a fixed number, p. In this example, we assume that c1,...,cp are mod-\neled as independent Gaussian variables N (0,τ2IdRd), σ2 with an inverse gamma\ndistribution with parameters u and v and (α1,...,αp) using a Dirichlet distribution\nwith parameters (a,...,a).\n4The symbol ∝means “equal up to a multiplicative constant”.\n\n19.7. BAYESIAN CLUSTERING\n483\nAnalytical example.\nThe joint probability density of (X,Z) and θ is proportional\nto\n(σ2)−u−1e−v/σ2e−Pp\nj=1 |cj|2/2τ2\np\nY\nj=1\nαa−1\nj\nN\nY\nk=1\ne−|xk−czk |2/2σ2\n(σ2)d/2\nN\nY\nk=1\nαzk\n= (σ2)−u−dN/2−1 exp\n\n−(v + 1\n2\nN\nX\nk=1\n|xk −czk|2)/σ2\n\n\np\nY\nj=1\nα\na+Nj−1\nj\n.\nOne can explicitly integrate this last expression with respect to σ2 and α, using\nthe expressions of the normalizing constants in the inverse gamma and Dirichlet\ndistributions, yielding (after integration and ignoring constant terms)\nΓ(a + N1)···Γ(a + Np)\n(v + 1\n2\nPN\nk=1 |xk −czk|2)u+dN/2 exp\n\n−\np\nX\nj=1\n|cj|2/2τ2\n\n\n=\nΓ(a + N1)···Γ(a + Np)\n(v + 1\n2Sw + 1\n2\nPp\nj=1 Nj|cj −¯xj|2)u+dN/2 exp\n\n−\np\nX\nj=1\n|cj|2/2τ2\n\n\nwhere Sw = PN\nk=1 |xk −¯xzk|2 is the within group sum of squares. Note that this sum of\nsquares depends on x and z, and that (N1,...,Np), the group sizes, depend on z.\nLet us assume a “non-informative prior” on the centers, which corresponds to\nletting τ tend to infinity and neglecting the last exponential. The remaining expres-\nsion can now be integrated with respect to c1,...,cp by making a change of variables\nµj =\nq\nNj/(2v + Sk)(cj −xj) and using the fact that\nZ\n(Rd)p\ndc1 ...dcp\n(v + 1\n2Sw + 1\n2\nPp\nj=1 Nj|cj −¯xj|2)u+dN/2 =\n(2v + Sw)(p−N)d/2−u)\np\nY\nj=1\nN −d/2\nj\nZ\n(Rd)p\ndµ1 ...dµp\n(1\n2 + 1\n2\nPp\nj=1 |µj|2)u+dN/2\nand the final integral does not depend on x or z. It follows from this that the condi-\ntional distribution of Z given x takes the form\nP(z|x) = C(x)\nQp\nj=1 Γ(a + Nj)\n(2v + Sw)(N−p)d/2+u) Qp\nj=1 N d/2\nj\nwhere C(x) is a normalization constant ensuring that the right-hand side is a proba-\nbility distribution over configurations z = (z1,...,zN) ∈{1,...,p}N. In order to obtain\n\n484\nCHAPTER 19. CLUSTERING\nthe most likely configuration for this posterior distribution, one should therefore\nminimize in z the function\n((N −p)d\n2 + u)log(2v + Sw) + d\n2\np\nX\nj=1\nlogNj −\np\nX\nj=1\nlogΓ(a + Nj).\nThis final optimization problem cannot be solved in closed form, but this can be\nperformed numerically. One can simplify it a little by only keeping the main order\nterms in the last two sums (using Stirling formula for the Gamma function) and\nminimize\n((N −p)d\n2 + u)log(2v + Sw) −\np\nX\nj=1\n(a + Nj)log(a + Nj).\nThis expression has a nice interpretation, since the first term minimizes the within-\ngroup sum of squares, the same objective function as in K-means, and the second\none is an entropy term that favors clusters with similar sizes.\nMonte-Carlo simulation.\nAn alternative to this analytical approach is to use Monte-\nCarlo simulations to estimate some properties of the posterior distribution numeri-\ncally. While they are often computationally demanding, Monte-Carlo methods are\nmore flexible and can be used in situations when analytic computations are intrac-\ntable. In order to sample from the distribution of Z given x, it is actually easier to\nsample from the joint distribution of (Z,θ) given x, because this distribution has a\nsimpler form. Of course, if the pair (Z,θ) is sampled from the conditional distri-\nbution given x, the first component, Z will follow the posterior distribution we are\ninterested in.\nIn the context of the discussed example, this reduces to sampling from a distri-\nbution proportional to\n(σ2)−u−1e−v/σ2e−Pp\nj=1 |cj|2/2τ2\np\nY\nj=1\nαa−1\nj\nN\nY\nk=1\ne−|xk−czk |2/2σ2\n(σ2)d/2\nN\nY\nk=1\nαzk .\n(19.11)\nSampling from all these variables at once is not tractable, but it is easy to sample\nfrom them in sub-groups, conditionally to the rest of the variables. We can, for\nexample, deduce from the expression above the following conditional distributions.\n(i) Given (α,c,z), σ2 follows an inverse gamma distribution with parameters u +\ndN/2 and v + 1\n2\nPN\nk=1 |xk −czk|2.\n(ii) Given (z,z,σ2), α follows a Dirichlet distribution with parameters a+N1,...,a+\nNp.\n\n19.7. BAYESIAN CLUSTERING\n485\n(iii) Given (z,σ2,α), c1,...,cp are independent and follow a Gaussian distribution,\nrespectively with mean (1 + σ2/(Njτ2))−1 ¯xj and variance (Nj/σ2 + 1/τ2)−1.\n(iv) Given (σ2,α,c), z1,...,zN are independent and\nP(zk = j|σ2,α,c,x) ∝αje−|xk−cj|2/2σ2.\nAlgorithm 19.10 (Gibbs sampling for mixture of Gaussian (Bayesian case))\n(1) Initialize with variables α,c,σ and z, for example generated according to the\nprior distribution.\n(2) Loop a large number of times over the following steps.\n(i) Simulate a new value of σ2 according to an inverse gamma distribution with\nparameters u + dN/2 and v + 1\n2\nPN\nk=1 |xk −czk|2.\n(ii) Simulate new values for α1,...,αp according to a Dirichlet distribution with\nparameters a + N1,...,a + Np.\n(iii) Simulate new values for c1,...,cp independently, sampling ci according to\na Gaussian distribution with mean (1+σ2/(Njτ2))−1 ¯xj and variance (Nj/σ2 +1/τ2)−1.\n(iv) Simulate new values of z1,...,zN independently such that\nP(zk = j|σ2,α,c,x) ∝αje−|xk−cj|2/2σ2.\nNote that this algorithm is only asymptotically providing a sample of the poste-\nrior distribution (it has to be stopped at some point, of course). Note also that, at\neach step, the labels z1,...,zN provide a random partition of the set {1,...,N}, and\nthis partition changes at every step.\nTo estimate one single partition out of this simulation, several strategies are pos-\nsible. Using the simulation, one can estimate the probability wkl that xk and xl be-\nlong to the same cluster. This can be dome by averaging the number of times that\nzk = zl was observed along the Gibbs sampling iterations (from which one usually\nexcludes a few early “burn-in” iterations). These weights, wkl can then be used as\nsimilarity measures in a clustering algorithm.\nAlternatively, one can average for each k, the values of the class center czk associ-\nated to k, still along the Gibbs sampling iterations. These average values can then be\nused as input of, say, a K-means algorithm to estimate final clusters.\n\n486\nCHAPTER 19. CLUSTERING\nMean-field approximation.\nWe conclude this section with a variational Bayes ap-\nproximation of the posterior distribution. We will make a mean-field approxima-\ntion, in which all parameters and latent variables are independent, therefore ap-\nproximating the distribution in (19.11) by a product distribution taking the form\ng(σ2,α,c,z) = g(σ2)(σ2)g(α)(α)\np\nY\nj=1\ng(c)\nj (cj)\nN\nY\nk=1\ng(z)\nk (zk).\nHere c = (c1,...,cp), z = (z1,...,zN) and α = (α1,...,αp). We have σ2 ∈(0,+∞), c ∈\n(Rd)p, α ∈S, the set of all non-negative α1,...,αp that sum to one, and z ∈{1,...,p}N\n(so that g(x)\nk\nis a p.m.f. on {1,...,p}. We will use the discussion in section 16.3.3 and\nlemma 16.1, and use the notation introduced in that section to denote as \nϕ\u000b the\nexpectation a variable ϕ of the variables above for the p.d.f. g.\nThe log-likelihood for a mixture of Gaussian takes the form (ignoring contant\nterms)\nℓ(σ2,α,c,z) = −(u + 1)logσ2 −vσ−2 −1\n2τ2\np\nX\nk=1\n|cj|2 +\np\nX\nj=1\n(a −1)logαj\n−Nd\n2 logσ2 −1\n2σ−2\nN\nX\nk=1\n|xk −czk|2 +\nN\nX\nk=1\nlogαzk\n= −(u + 1)logσ2 −vσ−2 −1\n2τ2\np\nX\nk=1\n|cj|2 +\np\nX\nk=1\n(a −1)logαj\n−Nd\n2 logσ2 −1\n2σ−2\nN\nX\nk=1\np\nX\nj=1\n|xk −cj|21zk=j +\nN\nX\nk=1\np\nX\nj=1\nlogαj1zj=k\nand can therefore be decomposed as a sum of products of functions of single vari-\nables, as assumed in section 16.3.3. Using lemma 16.1, we can identify each of the\ndistributions composing g, namely:\n• g(σ2) is the p.d.f. of an inverse gamma with parameters ˜u = u + Nd/2 and\n˜v = ν + 1\n2\nN\nX\nk=1\np\nX\nj=1\nD\n|xk −Cj|2E \nZk = j\u000b.\n• g(c)\nj\nis the p.d.f. of a Gaussian, with parameters N ( ˜mj, ˜σ2\nj IdRd), with, letting\n˜ζ(j) =\nN\nX\nk=1\n\nZk = j\u000b =\nN\nX\nk=1\ng(z)\nk (j),\n\n19.7. BAYESIAN CLUSTERING\n487\n˜σ2\nj =\n\u0010 1\nτ2 +\nD\nσ−2E ˜ζ(j)\n\u0011−1 and ˜mi =\nD\nσ−2E\n˜σ2\nj\nPN\nk=1\n\nZk = j\u000bxk.\n• g(α) of a Dirichlet distribution, with parameters ˜a1,..., ˜ak, with ˜ai = a + ˜ζ(j).\n• Finally g(z)\nk\nis a p.m.f. on {1,...,p} with\ng(z)\nk (j) ∝exp\n\u0012\n−1\n2\nD\nσ−2ED\n|xk −Cj|2E\n+\nD\nlogαj\nE\u0013\n.\nTo complete the consistency equations, it now suffices to evaluate the expecta-\ntions in the formula above as functions of the other parameters. We leave to the\nreader the verification of the following statements.\n• If σ2 follows an inverse gamma distribution with parameters ˜u and ˜v, then\nD\nσ−2E\n=\n˜u/ ˜v.\n• If Cj ∼N ( ˜mj, ˜σ2\nj IdRd), then\nD\n|xk −Cj|2E\n= |xk −˜mj|2 + d ˜σ2\nj .\n• If α follows a Dirichlet distribution with parameters ˜a1,..., ˜ap, then\nD\nlogαj\nE\n=\nψ(˜aj) −ψ(˜a1 + ··· + ˜ap) where ψ is the digamma function (derivative of the logarithm\nof the gamma function).\nCombining these facts with the expression of the mean-field parameters, we can\nnow formulate a mean-field estimation algorithm for mixtures of Gaussian that iter-\natively applies the consistency equations.\nAlgorithm 19.11 (Mean-field algorithm for mixtures of Gaussian)\n(1) : Input: training set (x1,...,xN), number of clusters p, prior parameters u,v,τ2\nand a .\n(2) Initialize variables ˜σ2\n1 ,..., ˜σ2\np , ˜m1,..., ˜mp, ˜a1,..., ˜ap, ˜gk(j), k = 1,...,N, j = 1,...,p.\n(3) Let ˜ζ(j) = PN\nk=1 ˜gk(j), j = 1,...,p.\n(4) Let\n˜ρ2 =\n1\nu + Nd/2\n\nv + 1\n2\nN\nX\nk=1\np\nX\nj=1\n˜gk(j)|xk −˜mj|2 + d\n2\np\nX\nj=1\n˜σ2\nj ˜ζ(j)\n\n.\n(5) For j = 1,...,p, let ˜σ2\ni =\n\u0012\n1\nτ2 +\n˜ζ(j)\n˜ρ2\n\u0013−1\nand ˜mi =\n˜σ2\nj\n˜ρ2\nPN\nk=1 ˜gk(j)xk.\n(6) Let ˜ai = a + ˜ζ(j), j = 1,...,p.\n(7) For k = 1,...,N, j = 1,...,p, let\n˜gk(j) ∝exp\n \n−1\n2 ˜ρ2\n\u0010\n|xk −˜mj|2 + d ˜σ2\nj\n\u0011\n+ ψ(˜aj)\n!\n.\n\n488\nCHAPTER 19. CLUSTERING\n(8) Compare the updated variables with their previous values and stop if the dif-\nference is below a tolerance level. Otherwise, return to (3).\nAfter convergence g(z)\nk\nprovides the mean-field approximation of the posterior prob-\nability of classes for observation k and can be used to determine clusters.\n19.7.3\nNon-parametric priors\nThe Polya urn\nIn the previous model with p clusters or less, the joint distribution\nof Z1,...,ZN is given by\nπ(z1,...,zN) = Γ(pa)\nΓ(a)p\nZ\nSp\np\nY\nj=1\nα\na+Nj−1\nj\ndα =\nΓ(pa)\nΓ(pa + N)\np\nY\nj=1\nΓ(a + Nj)\nΓ(a)\n.\nConditional to z1,...,zN, the data model was completed by sampling p sets of pa-\nrameters, say, θ1,...,θp, each belonging to a parameter space Θ and following a prior\nprobability distribution with density, say, ψ and variables X1,...,XN, where Xk ∈R\nwas drawn according to a law dependent on its cluster, that we will denote ϕ(· | θzk).\nThe complete likelihood of the data is now\nL(z,θ,x) =\nΓ(pa)\nΓ(pa + N)\np\nY\nj=1\nΓ(a + Nj)\nΓ(a)\np\nY\nj=1\nψ(θj)\nN\nY\nk=1\nϕ(xk|θzk).\nNote that the right-hand side does not change if one relabels the values of z1,...,zN,\ni.e., if one replaces each zk by s(zk) where s is a permutation of {1,...,p}, creating a\nnew configuration denoted s · z. Let [z] denote the equivalence class of z, containing\nall z′ = s · z,s ∈SN: all the labelings in [z] provide the same partition of {1,...,N}\nand can therefore be identified. One defines a probability distribution ¯π over these\nequivalence classes by letting\n¯π([z]) = |[z]|\nΓ(pa)\nΓ(pa + N)\np\nY\nj=1\nΓ(a + Nj)\nΓ(a)\n.\nThe first term on the right-hand side is the number of elements in the equivalence\nclass of [z]. To compute it, let p0 = p0(z) denote the number of different values\ntaken by z1,...,zN, i.e., the “true” number of clusters (ignoring the empty ones),\nwhich now is a function of z. Let A1,...,Ap0 denote the partition associated with z.\nNew labelings equivalent to z can be obtained by assigning any index i1 ∈{1,...,p}\n\n19.7. BAYESIAN CLUSTERING\n489\nto elements of A1, then any index i2 , i1 to elements of A2, etc., so that there are\n|[z]| = p!/(p −p0)! choices. We therefore find:\n¯π([z]) =\np!\n(p −p0)!\nΓ(pa)\nΓ(pa + N)\np\nY\nj=1\nΓ(a + Nj)\nΓ(a)\n.\nLetting λ = pa and using the formula Γ(x + 1) = xΓ(x), this can be rewritten as\n¯π([z]) = p(p −1)···(p −p0 + 1)\nλ(λ + 1)...(λ + N −1)\np\nY\nj=1\nNj−1\nY\ni=0\n(λ/p + i).\nNow, the class [z] contains exactly one element ˆz with the following properties\n• ˆz1 = 1,\n• ˆzk ≤max(zj,j < k) + 1 for all k > 1.\nThis means that the kth label is either one of those already appearing in (ˆz1,..., ˆzk−1)\nor the next integer in the enumeration. We will call such a ˆz admissible. If we assume\nthat z is admissible in the expression of ¯π, we can write\n¯π([z]) =\nQp0\nj=1\n\u0012\nλ(1 −j/p)QNj−1\ni=1 (λ/p + i)\n\u0013\nλ(λ + 1)...(λ + N −1)\n.\nIf one takes the limit p →∞in this expression, one still gets a probability distribu-\ntion on admissible labelings, namely\n¯π([z]) =\nλp0 Qp0\nj=1(Nj −1)!\nλ(λ + 1)...(λ + N −1).\n(19.12)\nRecall that, in this equation, p0 is a function of z, equal, for admissible labelings, to\nthe largest j such that Nj > 0.\nThe probability ¯π is generated by the following sampling scheme, called the\nPolya urn process simulating admissible labelings.\nAlgorithm 19.12 (Polya Urn)\n1 Initialize k = 1, z1 = 1, j = 1. Let N1 = 1\n2 At step k, assume that z1,...,zk have been generated, with associated number of\nclusters equal to j and N1,...,Nj elements per cluster. Generate zk+1 such that\nzk+1 =\n\ni\nwith probability\nNi\nλ + k , for i = 1,...,j\nj + 1\nwith probability\nλ\nλ + k\n(19.13)\n\n490\nCHAPTER 19. CLUSTERING\n3 If zk+1 = i ≤j, then replace Ni by Ni + 1, k by k + 1.\n4 If zk+1 = j + 1, let Nj+1 = 1, replace j by j + 1 and k by k + 1.\n5 If k < N, return to step 2, otherwise, stop.\nUsing this prior, the complete model for the distribution of the observed data is\nL(z,θ,x) =\nλp0 Qp0\nj=1(Nj −1)!\nλ(λ + 1)...(λ + N −1)\np0\nY\nj=1\nψ(θj)\nN\nY\nk=1\nϕ(xk|θzk)\nRecall that, in this expression, z is restricted to the set of admissible labelings. We\nalso note that admissible labelings are in one-to-one correspondence with the par-\ntitions of {1,...,N}, so that the latent variable z in this expression can also be inter-\npreted as representing a random partition of this set.\nDirichlet processes.\nAs we will see later, the expression of the global likelihood\nand the Polya urn model will suffice for us to develop non-parametric clustering\nmethods for a set of observations x1,...,xN. However, this model is also associated\nto an important class of random probability distributions (i.e., random variables\ntaking values in some set of probability distributions) called Dirichlet processes for\nwhich we provide a brief description.\nThe distribution in (19.12) was obtained by passing to the limit from a model\nthat first generates p numbers α1,...,αp, then generates the labels z1,...,zN ∈{1,...,p}\nidentified modulo relabeling. This distribution can also be defined directly, by first\ndefining an infinity of positive numbers (αj,j ≥1) such that P∞\ni=1 αi = 1, followed by\nthe generation of random labels Z1,...,ZN such that P(Zk = j) = αj, followed once\nagain with an identification up to relabeling.\nThe distribution of α that leads to the Polya urn is called the stick breaking process.\nThis process is such that\nαj = Uj\nj−1\nY\ni=1\n(1 −Ui)\nwhere U1,U2,... is a sequence of i.i.d. variables following a Beta(1,λ) distribution,\ni.e., with p.d.f. λ(1 −u)λ−1 for u ∈[0,1]. The stick breaking interpretation comes\nfrom the way α1,α2,... can be simulated: let α1 ∼Beta(1,λ); given α1,...,αj−1, let\nαj = (1 −α1 −··· −αj−1)Uj where Uj ∼Beta(1,λ) and is independent from the past.\nEach step can be thought of as breaking the remaining length, (1 −α1 −··· −αj−1),\nof an original stick of length 1 using a beta-distributed variable, Uj. This process\nleads to the distribution (19.12) over admissible distributions, i.e., if α is generated\naccording to the stick breaking process, and Z1,...,ZN are independent, each such\n\n19.7. BAYESIAN CLUSTERING\n491\nthat P(Zk = j) = αj, then the probability that (Z1,...,ZN) is identical, after relabeling,\nto the admissible configuration z is given by (19.12). (We skip the proof of this result,\nwhich is not straightforward.)\nNow, take a realization α = (α1,α2,...) of the stick-breaking process, and inde-\npendent realizations η = (η1,η1,...) drawn according to the p.d.f. ψ. Define\nρ =\n∞\nX\nj=1\nαjδηj .\n(19.14)\nFor any realization of α and of η, ρ is a probability distribution on the parameter\nspace Θ (in which one chooses ηi with probability αi). Since α and η are both random\nvariables, this defines a random variable ρ with values in the space of probability\nmeasures on Θ.\nThis process has the following characteristic property. For any family V1,...,Vk ⊂\nΘ forming a partition of that set, the random variable (ρ(U1),...,ρ(Uk)) follows a\nDirichlet distribution with parameters\n \nλ\nZ\nU1\nψ dη,...,λ\nZ\nU1\nψ dη\n!\n.\nThis is the definition of a Dirichlet process with parameters (λ,ψ), or, simply, with\nparameter λψ. Conversely, one can also show that any Dirichlet process can be de-\ncomposed as in (19.14) where α is a stick-breaking process and η independent real-\nizations of ψ.\nMonte-Carlo simulation.\nThe joint distribution of labels, parameters and observed\nvariables can also be deduced from (19.12), with a joint p.d.f. given by\nλp0−1 Qp0\nj=1(Nj −1)!\n(λ + 1)···(λ + N −1)\np0\nY\nj=1\nψ(ηj)\nN\nY\nk=1\nϕ(xk|ηzk).\n(19.15)\nThe forward simulation of this distribution is a straightforward extension of Algo-\nrithm 19.12, namely:\nAlgorithm 19.13\n1 Initialize k = 1, z1 = 1, j = 1. Let N1 = 1.\n2 Sample η1 ∼ψ and x1 ∼ϕ(·|η1).\n3 At step k, assume that z1,...,zk has been generated, with associated number of\nclusters equal to j and N1,...,Nj elements per cluster. Generate zk+1 such that\nzk+1 =\n\ni\nwith probability\nNi\nλ + k , for i = 1,...,j\nj + 1\nwith probability\nλ\nλ + k\n\n492\nCHAPTER 19. CLUSTERING\n4 If zk+1 = i ≤j, sample xk+1 ∼ϕ(·|ηi). Replace Ni by Ni + 1, k by k + 1.\n5 If zk+1 = j + 1, let Nj+1 = 1, sample ηj+1 ∼ψ and xk+1 ∼ϕ(·|ηj+1). Replace j by\nj + 1 and k by k + 1.\n6 If k < N, return to step 2, otherwise, stop.\nThis algorithm cannot be used, of course, to sample from the conditional distri-\nbution of Z and η given X = x, and Markov-chain Monte-Carlo must be used for this\npurpose. In order to describe how Gibbs sampling may be applied to this problem,\nwe use the fact that, as previously remarked, using admissible labelings z is equiv-\nalent to using partitions A = (A1,...,Ap0) of {1,...,N}, and we will use the latter\nformalism to describe the algorithm. We will also use the notation ηA to denote the\nparameter associated to A ∈A so our new notation for the variables is (A,η) where\nA is a partition of {1,...,N} and η is a collection (ηA,A ∈A) with ηA ∈Θ. Given this,\nwe want to sample from a conditional p.d.f.\nΦ(A,η|x) ∝λ|A|−1 Q\nA∈A(|A| −1)!\n(λ + 1)···(λ + N −1)\nY\nA∈A\nψ(ηA)\nY\nk∈A\nϕ(xk|ηA).\n(19.16)\nAs an additional notation, given a partition A and an index k ∈{1....,N}, we let Ak\ndenote the set A in A that contains k.\nThe following points are relevant for the design of the sampling algorithm.\n(1) The conditional distribution of η given A and the training data is proportional\nto\nY\nA∈A\n\nψ(ηA)\nY\nk∈A\nϕ(xk|ηA)\n\n\nThis shows that the parameters ηA,A ∈A are independent of each other, with ηA\nfollowing a distribution proportional to\nη 7→ψ(η)\nY\nk∈Aj\nϕ(xk|η).\nSampling from this distribution generally offers no special difficulty, especially if\nthe prior ψ is conjugate to ϕ. Importantly, one does not need to sample exactly from\nηA, and it is often more convenient to separate ηA into several components (such as\nmean and variance for mixtures of Gaussian) and sample from them alternatively,\ncreating another level of Gibbs sampling.\n(2) We now consider the issue of updating A. We will use for this purpose the\nformalism of Algorithm 12.2. In particular, for each k ∈{1,...,N}, we associate to\n\n19.7. BAYESIAN CLUSTERING\n493\nthe variable (A,η) the pair (A(k),η(k)), where A(k) is the partition of {1,...,N} \\ {k}\nformed by the sets A(k) = A \\ {k} and η(k)\nA = ηA, unless A = {k}, in which case the set\nand the corresponding ηA are dropped.\nWe can write Φ(A,η|x) in the form\nΦ(A,η|x) ∝q(Ak,ηAk)ϕ(xk|ηAk)λ|A(k)|−1 Q\nB∈A(k)(|B| −1)!\n(λ + 1)···(λ + N −1)\nY\nB∈A(k)\nψ(ηB)\nY\nl∈B\nϕ(xl|ηB)\n(19.17)\nwith\nq(A,θ) =\nX\nB∈A(k)\n|B|1A=B∪{k} + λψ(θ)1A={k}\nPartitions A′ that are consistent with A(k) allocate k to one of the clusters in A(k) or\ncreate a new cluster with a new parameter η′\nk. If one replaces (A,η) by (A′,η′), only\nthe first two terms in (19.17) will be affected, so that the conditional probability of\nA′ given A(k) is proportional to q(A′\nk,ηA′\nk)ϕ(xk|ηA′\nk) and given by\n\n|B|ϕ(xk|ηB)\nC1 + λC2\nif A′\nk = B ∪{k},η′\nB = ηB,B ∈A(k)\nλϕ(xk|η′\nk)ψ(η′\nk)\nC1 + λC2\nif A′\nk = {k},\nwhere\nC1 =\nX\nB∈Ak\n|B|ϕ(xk|ηB)\nand\nC2 =\nZ\nΘ\nϕ(xk|θ)ψ(θ)dθ.\nConcretely, this means that one first decides to allocate k to a set B in A(k) with\nprobability |B|ϕ(xk|ηB)/(C1+λC2) and to create a new set with probability λC2/(C1+\nλC2). If a new set is created, then the associated parameter η′\n{k} is sampled according\nto the p.d.f. ϕ(xk|θ)ψ(θ/C2.\n(3) However, sampling using this conditional probability requires the computa-\ntion of the integral C2, which can represent a significant computational burden,\nsince this has to be done many times in a Gibbs sampling algorithm. A modification\nof this algorithm, introduced in Neal [141], avoids this computation by adding new\nauxiliary variables at each step of the computation. These variables are m parameters\nη∗\n1,...,η∗\nm ∈Θ where m is a fixed integer. To define the joint distribution of A,η,η∗,\none lets the marginal distribution of (A,η) be given by (19.16) and conditionally to\nA,η, let η∗\n1,...,η∗\nm be:\n(i) independent with density ψ if |Ak| > 1;\n(ii) such that η∗\nj = ηAk and the other m −1 starred parameters are independent\nwith distribution ψ, where j is randomly chosen in {1,...,m} if Ak = {k}.\n\n494\nCHAPTER 19. CLUSTERING\nWith this definition, the joint conditional distribution of (A,η,η∗) takes the form\nbΦ(A,η,η∗|x) ∝ˆq(Ak,ηAk,η∗)ϕ(xk|ηAk)\nλ|A(k)|−1 Q\nB∈A(k)(|B| −1)!\n(λ + 1)···(λ + N −1)\nY\nB∈A(k)\nψ(ηB)\nY\nl∈B\nϕ(xl|ηB)\n(19.18)\nwith\nˆq(A,θ,η∗\n1,...,η∗\nm) =\nX\nB∈A(k)\n|B|1θ=ηB,A=B∪{k}\nm\nY\nj=1\nψ(η∗\nj) + λ\nm\nm\nX\nj=1\n1θ=η∗\nj ,A={k}ψ(θ)\nm\nY\ni=1,i,j\nψ(η∗\ni ).\nNote that bΦ depends on k, so that the definition of the auxiliary variables will change\nat each step of Gibbs sampling. The conditional distribution, for bΦ, of A′,η′ given\nA(k),η(k),η∗is such that\n• A′\nk = B ∪{k} and η′\nA′\nk = ηB with probability |B|ϕ(xk|ηB)/C, for B ∈A(k).\n• A′\nk = {k} and ηA′\nk = η∗\nj with probability (λ/m)ϕ(xk|η∗\nj)/C, j = 1,...,m.\nThe constant C is given by\nC =\nX\nB∈Ak\n|B|ϕ(xk|ηB) + λ\nm\nm\nX\nj=1\nϕ(xk|η∗\nj)\nand is therefore easy to compute.\nWe can now summarize this discussion with Neal’s version of the Gibbs sampling\nalgorithm.\nAlgorithm 19.14 (Neal)\nInitialize the algorithm with some arbitrary partition and parameters (A,η) (for ex-\nample, generated using the Dirichlet prior). Use the same notation to denote these\nvariables at the end of the previous iteration of the algorithm. The next iteration is\nthen run as follows.\n(1) For k = 1,...,N, reallocate k to a cluster as follows.\n(i) Form the new family of sets A(k) and labels η(k) by removing k from the parti-\ntion A.\n(ii) If |Ak| > 1, generate m variables η∗\n1,...,η∗\nm according to ψ. If Ak = {k}, generate\nonly m −1 such variables and let the last one be equal to ηAk.\n\n19.7. BAYESIAN CLUSTERING\n495\n(iii) Allocate k to a new cluster A′ with parameter η′\nA′ according to probabilities\nproportional to\n\n|B|ϕ(xk|η(k)\nB ) if A′ = B ∪{k} and η′\nA′ = η(k)\nB\nλ\nmϕ(xk|η∗\nj) if A = {k} and η′\nA′ = η∗\nj,j = 1,...,m\n(2) For A ∈A, update ηA,A ∈A according to the distribution proportional to\nψ(η)\nY\nk∈A\nϕ(xk|η)\neither directly, or via one step of Gibbs sampling visiting each of the variables that\nconstitute ηA.\n(3) Loop a sufficient number of times over the previous two steps.\nAfter running this algorithm, the set of clusters should be finalized by using\nstatistics computed along the simulation, as discussed after Algorithm 19.10.\nFull example: Mixture of Gaussian.\nTo conclude this section, we summarize the\nMonte-Carlo sampling algorithm for mixtures of Gaussian using a non-parametric\nBayesian prior. Here, η ∈Θ is the center c ∈Rd, with prior distribution ψ = N (0,τ2IdRd).\nThe previous algorithm must be modified because an additional parameter σ2 is\nshared by all classes, with prior given by an inverse gamma distribution with pa-\nrameters u and v. The conditional distribution of the data is ϕ(x|c,σ) ∼N (c,σ2IdRd).\nAlgorithm 19.15 (Gibbs sampling for non-parametric mixture of Gaussian)\n(1) Initialize the algorithm with some arbitrary partition and parameters (A,η).\n(2) For k = 1,...,N, reallocate k to a cluster as follows.\n(i) Form the new family of sets A(k) and labels η(k) by removing k from the parti-\ntion A.\n(ii) If |Ak| > 1, generate m variables c∗\ni, i = 1,...,m independently with c∗\ni ∼N (0,τ2IdRd).\nIf Ak = {k}, generate only m −1 such pairs of variables and let the last one be equal\nto cAk.\n(iii) Allocate k to a new cluster A′ with parameter c′\nA′ according to probabilities\nproportional to\n\n|B|exp\n\u0010\n−|xk −c(k)\nB |\n2σ2\n\u0011\nif A′ = B ∪{k} and c′\nA′ = c(k)\nB\nλ\nm exp\n\u0010\n−|xk −c∗\nB|\n2σ2\n\u0011\nif A = {k} and c′\nA′ = c∗\nj,j = 1,...,m.\n\n496\nCHAPTER 19. CLUSTERING\n(3) Simulate a new value of σ2 according to an inverse gamma distribution with\nparameters u + dN/2 and v + 1\n2\nPN\nk=1 |xk −cAk|2.\n(4) Simulate new values for cA,A ∈A independently, sampling cA according to a\nGaussian distribution with mean (1 + σ2/(Njτ2))−1 ¯xA and variance (|A|/σ2 + 1/τ2)−1,\nwhere\n¯xA = 1\n|A|\nX\nk∈A\nxk.\n\nChapter 20\nDimension Reduction and Factor Analysis\n20.1\nPrincipal component analysis\n20.1.1\nGeneral Framework\nFactor analysis aims at representing potentially high-dimensional data as functions\nof a (generally) small number of “factors,” with a representation taking the general\nform\nX = Φ(Y,θ) + residual,\n(20.1)\nwhere X is the observation, Y provide the factors and Φ is a function parametrized\nby θ. A factor analysis model must therefore specify Φ (often, a linear function of Y),\nadd hypotheses on Y (such as its dimension, or properties of its distribution) and on\nthe residuals. The transformation Φ is estimated from training data, but, ideally, the\nmethod should also provide an algorithm that infers Y from a new observation of X.\nMost of the time, Y is small dimensional so that the model also implies a reduction\nof dimension.\nWe start our discussion with principal component analysis (or PCA). This meth-\nods can be characterized in multiple ways, and we introducing through the angle of\ndata approximation. In the following, the random variable X takes values in a finite-\nor infinite-dimensional inner-product space H. We will denote, as usual, by ⟨. , .⟩H\nthe product in this space.\nAssume that N independent realization of X, denoted x1,...,xN, are observed,\nforming our training set T. Our goal is to obtain a small-dimensional representation\nof these data, while loosing a minimal amount of relevant information. PCA, is the\nsimplest and most commonly used approach developed for this purpose.\nIf V is a finite-dimensional subspace of H, we denote by PV (y) the orthogonal\nprojection of y ∈H on V , i.e., the element ξ ∈V such that ∥y −ξ∥2\nH is minimal\n497\n\n498\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\n(see section 6.4). Recall that this orthogonal projection if characterized by the two\nproperties: (i) PV (y) ∈V and (ii) (y −PV (y)) ⊥V .\nGiven a target dimension p, PCA determines a p-dimensional subspace of H, say,\nV and a point c ∈H, such that, letting\nRk = xk −c −PV (xk −c)\nfor k = 1,...,N, the residual sum of squares\nS =\nN\nX\nk=1\n∥Rk∥2\nH\n(20.2)\nis as small as possible.\nAn optimal choice for c is c = x = PN\nk=1 xk/N. Indeed, using the linearity of the\northogonal projection, we have\nS =\nN\nX\nk=1\n∥xk −PV (xk) −(c −PV (c))∥2\nH\n=\nN\nX\nk=1\n∥xk −PV (xk) −(x −PV (x))∥2\nH + N∥x −PV (x) −(c −PV (c))∥2\nH.\nGiven this, there would be no loss of generality in assuming that all xk’s have been\nreplaced by xk −x and taking c = 0. While this is often done in the literature, there\nare some advantages (especially when discussing kernel methods) in keeping the\naverage explicit in the notation, as we will continue to do.\nIntroducing an orthonormal basis (e1,...,ep) of V , one has\nPV (xk −x) =\np\nX\ni=1\nρk(i)ei\nwith ρki = ⟨xk −x , ei⟩H. One can then reformulate the problem in terms of (e1,...,ep),\nwhich must minimize\nS\n=\nN\nX\nk=1\n∥xk −x −\np\nX\ni=1\n⟨xk −x , ei⟩ei∥2\nH\n=\nN\nX\nk=1\n∥xk −x∥2\nH −\np\nX\ni=1\nN\nX\nk=1\n⟨xk −x , ei⟩2\nH.\n\n20.1. PRINCIPAL COMPONENT ANALYSIS\n499\nFor u,v ∈H, define\n⟨u , v⟩T = 1\nN\nN\nX\nk=1\n⟨xk −x , u⟩H⟨xk −x , v⟩H\nand ∥u∥T = ⟨u , u⟩1/2\nT\n(the index T refers to the fact that this norm is associated with\nthe training set). This provides a new quadratic form on H. The formula above\nshows that minimizing S is equivalent to maximizing\np\nX\ni=1\n∥ei∥2\nT\nsubject to the constraint that (e1,...,ep) is orthonormal in H.\nLet us consider a slightly more general problem. If H is a separable Hilbert\nspace1 and µ is a square-integrable probability measure on H, such that\nZ\nH\n∥x∥2\nH dµ(x) < ∞,\none can define m =\nR\nH xdµ(x) and σ2\nµ =\nR\nH ∥x −m∥2\nHdµ. One can then define the\ncovariance bilinear form\nΓµ(u,v) =\nZ\nH\n⟨u , x −m⟩H ⟨v , x −m⟩H dµ(x),\nwhich satisfies Γµ(u,v) ≤σ2\nµ∥u∥H ∥v∥H.\nWith this notation, we have\n⟨u , v⟩T = ΓˆµT (u,v),\nwhere ˆµT = (1/N)PN\nk=1 δxk is the empirical measure (and in that case m = ¯x). We can\ntherefore generalize the PCA problem by considering the maximization of\np\nX\nk=1\nΓµ(ek,ek)\n(20.3)\nover all orthonormal families (e1,...,ep) in H.\nWhen µ is square integrable, the associated operator, Aµ defined by\n⟨u , Aµv⟩H = Γµ(u,v)\n(20.4)\n1A Hilbert space is an inner-product space which is complete for its norm. A separable Hilbert\nspace must have a dense countable subset, which, in particular, implies that it has orthonormal bases.\n\n500\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nfor all u,v ∈H, is a Hilbert-Schmidt operator [205]. Such an operator can, in partic-\nular, be diagonalized in an orthonormal basis of H, i.e., there exists an orthonormal\nbasis, (f1,f2,...) of H such that Aµfi = λ2\ni fi for a non-increasing sequence of eigenval-\nues (with λ1 ≥λ2 ≥··· ≥0) such that\nσ2\nµ =\n∞\nX\nk=1\nλ2\ni .\nThe main statement of the following result is in finite dimensions, a simple ap-\nplication of corollary 2.4. We here give a direct proof that also works in infinite\ndimensions.\nTheorem 20.1 Let (f1,f2,...) be an orthonormal basis of eigenvectors of Aµ with associ-\nated eigenvalues λ2\n1 ≥λ2\n2 ≥··· ≥0. Then an orthonormal family (e1,...,ep) in H maxi-\nmizes (20.3) if and only if,\nspan(fj : λ2\nj > λ2\np) ⊂span(e1,...,ep) ⊂span(fj : λ2\nj ≥λ2\np).\n(20.5)\nIn particular f1,...,fp always provide a solution and span(e1,...,ep) = span(f1,...,fp) for\nany other solution as soon as λ2\np > λ2\np+1.\nDefinition 20.2 When µ = ˆµT , the vectors (f1,...,fp) are called (with some abuse when\neigenvalues coincide) the first p principal components of the training set (x1,...,xN).\nProof If (e1,...,ep) is an orthonormal family in H, let\nF(e1,...,ep) =\np\nX\nk=1\nΓµ(ek,ek).\nNote that F(f1,...,fp) = λ2\n1 + ··· + λ2\np. Write ek = P∞\nj=1 α(j)\nk fj (so that α(j)\nk\n= ⟨fj , ek⟩H).\nThese coefficients satisfy P∞\nj=1 α(j)\nk α(j)\nl\n= 1 if k = l and 0 otherwise. Then\nΓµ(ek,ek) =\n∞\nX\nj=1\nλ2\nj (α(j)\nk )2.\n\n20.1. PRINCIPAL COMPONENT ANALYSIS\n501\nWe have\nF(e1,...,ep) =\np\nX\nk=1\n∞\nX\nj=1\nλ2\nj (α(j)\nk )2\n=\np\nX\nk=1\np\nX\nj=1\nλ2\nj (α(j)\nk )2 +\np\nX\nk=1\n∞\nX\nj=p+1\nλ2\nj (α(j)\nk )2\n≤\np\nX\nk=1\np\nX\nj=1\nλ2\nj (α(j)\nk )2 +\np\nX\nk=1\n∞\nX\nj=p+1\nλ2\np+1(α(j)\nk )2\n=\np\nX\nj=1\n(λ2\nj −λ2\np+1)\np\nX\nk=1\n(α(j)\nk )2 + pλ2\np+1.\nLet P denote the orthogonal projection operator from H to span(e1,...,ep). We have,\nfor any h ∈H, ∥Ph∥2\nH ≤∥h∥2\nH with equality if and only if h ∈span(e1,...,ep). Applying\nthis to h = fj, with P(fj) = Pp\nk=1 α(j)\nk ek, we get Pp\nk=1(α(j)\nk )2 ≤1 with equality if and only\nif fj ∈span(e1,...,ep).\nAs a consequence, the previous upper bound on F(e1,...,ep) implies\nF(e1,...,ep) ≤\np\nX\nj=1\nλ2\nj .\nThis upper bound is attained at (e1,...,ep) = (f1,...,fp), which is therefore a maxi-\nmizer. Also, inspecting the argument above, we see that F(e1,...,ep) < λ2\n1 + ··· + λ2\np\nunless\n(a) for all k ≤p and j ≥p + 1: α(j)\nk = 0 if λ2\nj > λ2\np+1, and\n(b) for all j ≤p: Pp\nk=1(α(j)\nk )2 = 1 unless λ2\nj = λ2\np+1.\nCondition (a) implies that span(e1,...,ep) ⊂span(fj : λ2\nj ≤λ2\np+1). If λ2\np = λ2\np+1, the\ninclusion span(e1,...,ep) ⊂span(fj : λ2\nj ≤λ2\np) therefore holds. If λ2\np < λ2\np+1, condition\n(b) requires Pp\nk=1(α(j)\nk )2 = 1 for all j ≤p, which implies fj ∈span(e1,...,ep) for j ≤p,\nso that span(e1,...,ep) = span(f1,...,fp) and the inclusion also hold.\nCondition (b) always requires Pp\nk=1(α(j)\nk )2 = 1, hence fj ∈span(f1,...,fp), when\nλj < λp, showing that span(fj : λ2\nj < λ2\np) ⊂span(e1,...,ep). Equation (20.5) therefore\nalways holds for (e1,...,ep) such that F(e1,...,ep) = λ2\n1 + ··· + λ2\np. Furthermore, condi-\ntions (a) and (b) always hold for any orthonormal family that satisfy (20.5), showing\nthat any such solution is optimal.\n■\n\n502\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nNotice that the optimal S in (20.2) is such that\nS = N\nX\ni>p\nλ2\ni .\nRemark 20.3 The interest of discussing PCA associated with a covariance operator\nfor a square integrable measure (in which case it is often called a Karhunen-Loeve\n(KL) expansion) is that this setting is often important when discussing infinite-\ndimensional random processes (such as Gaussian random fields). Moreover, these\noperators quite naturally provide asymptotic versions of sample-based PCA. In-\nteresting issues, that are part of functional data analysis [158], address the design\nof proper estimation procedures to obtain converging estimators of KL expansions\nbased on finite samples for stochastic processes in infinite-dimensional spaces.\n♦\n20.1.2\nComputation of the principal components\nSmall dimension.\nAssume that H has finite dimension, d, i.e., H = Rd, and repre-\nsent x1,...,xN ∈Rd as column vectors. Let the inner product on H be associated to a\npositive-definite symmetric matrix Q:\n⟨u , v⟩H = uT Qv.\nIntroduce the covariance matrix of the data\nΣT = 1\nN\nN\nX\nk=1\n(xk −x)(xk −x)T ,\nWrite AT = A ˆµT , for short, in (20.4). We have:\n⟨u , AT v⟩H = 1\nN\nN\nX\nk=1\n(uT Q(xk −x))(vT Q(xk −x))\n= 1\nN\nN\nX\nk=1\nuT Q(xk −x)(xk −x)T Qv\n= ⟨u , ΣT Qv⟩H ,\nso that AT = ΣT Q.\nThe eigenvectors, f , of AT are such that Q1/2f are eigenvectors of the symmetric\nmatrix Q1/2ΣT Q1/2, which shows that they form an orthogonal system in H, which\nwill be orthonormal if the eigenvectors are normalized so that f T Qf = 1. Equiva-\nlently, they solve the generalized eigenvalue problem QΣT Qf = λ2Qf , which may\nbe preferred numerically to diagonalizing the non-symmetric matrix ΣT Q.\n\n20.1. PRINCIPAL COMPONENT ANALYSIS\n503\nRemark 20.4 Sometimes, the metric is specified by giving Q−1 instead of Q (or Q−1\nis easy to compute). Then, one can directly solve the generalized eigenvalue problem\nΣT ˜f = λ2Q−1 ˜f and set f = Q−1 ˜f . The normalization f T Qf = 1 is then obtained by\nnormalizing ˜f so that ˜f T Q−1 ˜f = 1.\n♦\nRemark 20.5 The “standard” version of PCA applies this computation using the Eu-\nclidean inner product, with Q = IdRd, and the principal components are the eigen-\nvectors of the covariance matrix of T associated with the largest eigenvalues.\n♦\nLarge dimension.\nIt often happens that the dimension of H is much larger than the\nnumber of observations, N. In such a case, the previous approach is quite inefficient\n(especially when the dimension of H is infinite!) and one should proceed as follows.\nReturning to the original problem, one can remark that there is no loss of gener-\nality in assuming that V is a subspace of W := span{x1 −x,...,xN −x}. Indeed, letting\nV ′ = PW(V ) (the projection of V on W), we have, for ξ ∈W,\n∥ξ −PV ξ∥2\nH = ∥ξ∥2\nH −2⟨ξ , PV ξ⟩H + ∥PV ξ∥2\nH\n= ∥ξ∥2\nH −2⟨PWξ , PV ξ⟩H + ∥PV ξ∥2\nH\n= ∥ξ∥2\nH −2⟨ξ , PWPV ξ⟩H + ∥PV ξ∥2\nH\n≥∥ξ∥2\nH −2⟨ξ , PWPV x⟩H + ∥PWPV ξ∥2\nH\n= ∥ξ −PWPV ξ∥2\nH\n≥∥ξ −PV ′ξ∥2\nH .\nIn this computation, we have used the facts that PWξ = ξ (since ξ ∈W), that ∥PWPV ξ∥H ≤\n∥PV ξ∥H, that PWPV ξ ∈V ′ and that PV ′(ξ) is the best approximation of ξ by an ele-\nment of V ′. This shows that (since xk −x ∈W for all k)\nN\nX\nk=1\n∥xk −x −PV (xk −x)∥2\nH ≥\nN\nX\nk=1\n∥xk −x −PV ′(xk −x)∥2\nH\nwith V ′ a subspace of W of dimension less than p, proving the result. This computa-\ntion also shows that no improvement in PCA can be obtained by looking for spaces\nof dimension p ≥dim(W) (with dim(W) ≤N −1 because the data is centered).\nIt therefore suffices to look for f1,...,fp in the form\nfi =\nN\nX\nk=1\nα(i)\nk (xk −x).\nfor some α(i)\nk , 1 ≤k ≤N,1 ≤i ≤p.\n\n504\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nWith this notation, we have ⟨fi , fj⟩H = PN\nk,l=1 α(i)\nk α(j)\nl ⟨xk −x , xl −x⟩H and\n⟨fi , fj⟩T = 1\nN\nN\nX\nl=1\n⟨fi , xl −x⟩H⟨fj , xl −x⟩H\n= 1\nN\nN\nX\nk,k′=1\nα(i)\nk α(j)\nk′\nN\nX\nl=1\n⟨xk −x , xl −x⟩H⟨xk′ −x , xl −x⟩H.\nLet S be the Gram matrix of the centered data, formed by the inner products ⟨xk −x , xl −x⟩H,\nfor k,l = 1,...,N. Let α(i) be the column vector with coordinates α(i)\nk , k = 1,...,N. We\nhave ⟨fi , fj⟩H = (α(i))T Sα(j) and ⟨fi , fj⟩T = (α(i))T S2α(j)/N, which implies that, in this\nrepresentation, the operator AT is given by S/N. Thus, the previous simultaneous\northogonalization problem can be solved in terms of the α’s by diagonalizing S and\ntaking the first eigenvectors, normalized so that (α(i))T Sα(i) = 1. Let λ2\nj , j = 1,...,N\nbe the eigenvalues of S/N (of which only the first min(d,N −1) may be non-zero).\nIn this representation, the decomposition of the projection of xk on the PCA basis is\ngiven by\nxk =\np\nX\nj=1\nβ(j)\nk fj\nwith\nβ(j)\nk = ⟨xk −x , fj⟩H =\nN\nX\nl=1\nα(j)\nl ⟨xl −x , xk −x⟩H = Nλ2\nj α(j)\nk .\n20.2\nKernel PCA\nSince the previous computation only depended on the inner products ⟨xk −x , xl −x⟩H,\nPCA can be performed in reproducing kernel Hilbert spaces, and the resulting method\nis called kernel PCA. In this framework, X may take values in any set R with a rep-\nresentation h : R →H. The associated kernel, K(x,x′) = ⟨h(x) , h(x′)⟩H, provides a\nclosed form expression of the inner products in terms of the original variables. The\nfeature function itself is most of the time unnecessary.\nThe kernel version of PCA consists in replacing xk −x with h(xk) −¯h where ¯h is\nthe average feature. This leads to defining a “centered kernel:”\nKc(x,x′)\n=\n⟨h(x) −¯h , h(x′) −¯h⟩H\n=\n⟨h(x) , h(x′)⟩H −⟨h(x) + h(x′) , ¯h⟩+\n\r\r\r¯h\n\r\r\r2\nH\n=\nK(xk,xl) −1\nN\nN\nX\nk=1\n(K(x,xk) + K(x′,xk)) + 1\nN 2\nN\nX\nk,l=1\nK(xk,xl).\n\n20.2. KERNEL PCA\n505\nThen the Gram matrix in feature space is S with skl = Kc(xk,xl) and the computation\ndescribed in the previous section can be applied. Note that, if one denotes, as usual\nK = K(x1,...,xN) the matrix formed by kernel evaluations K(xk,xl), and if one lets\nP = IdRN −1N1N/N, then we have the simple matrix expression S = PKP.\nLetting α(1),...,α(p) ∈RN be the first p eigenvectors of S, normalized so that\n(α(i))T Sα(i) = 1, the principal directions are vectors in feature space given by (us-\ning the notation in the previous section in which the kth coordinate of α(i) is α(i)\nk )\nfi =\nN\nX\nk=1\nα(i)\nk (h(xk) −¯h),\nand they are not computable when the features not known explicitly. However, a\nfew geometric features associated with these directions can be characterized using\nthe kernel only.\nConsider the line in feature space Di =\nn¯h + λfi,λ ∈R\no\n. Let Ωi denote the points\nx ∈R such that h(x) ∈Di. Then x ∈Ωi if and only if h(x) coincides with its orthogonal\nprojection on Di, which is equivalent to\n⟨h(x) −¯h , fi⟩2\nH =\n\r\r\rh(x) −¯h\n\r\r\r2\nH ,\nwhich can be expressed with the kernel as\nKc(x,x) −\n\n\nN\nX\nk=1\nα(i)\nk Kc(x,xk)\n\n\n2\n= 0.\n(20.6)\nThis provides a nonlinear equation in x. In particular, Ωi is generally nonlinear,\npossibly with several connected components. Note that, by definition, the difference\nin (20.6) is always non-negative, so that a way to visualize Ωi is to compute its sub-\nlevel sets, i.e., the set of all x such that\nKc(x,x) −\n\n\nN\nX\nk=1\nα(i)\nk Kc(x,xk)\n\n\n2\n≤ϵ\nfor small ϵ.\nSimilarly, the feature vector h(x) −¯h belongs to the space generated by the first p\ncomponents if and only if\np\nX\ni=1\n⟨h(x) −¯h , fi⟩2\nH =\n\r\r\rh(x) −¯h\n\r\r\r2\nH\n\n506\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\ni.e.,\np\nX\ni=1\n\n\nN\nX\nk=1\nα(i)\nk Kc(x,xk)\n\n\n2\n= Kc(x,x).\nOne can also compute the finite-dimensional coordinates of h(x) in the PCA basis,\nand this computation is easier. The representation is\nx 7→(u1(x),...,up(x))\nwith\nui = ⟨h(x) −¯h , fi⟩H =\nN\nX\nk=1\nα(i)\nk Kc(x,xk).\nThis provides an explicit nonlinear transformation that maps each data point x into\na p-dimensional point. This representation allows one to easily exploit the reduction\nof dimension.\n20.3\nStatistical interpretation and probabilistic PCA\nThere is a simple probabilistic interpretation of linear PCA. Assume that H = Rd\nwith the standard inner product and that X is a centered random vector with covari-\nance matrix Σ. Consider the problem that consists in finding a factor decomposition\nX =\np\nX\ni=1\nY (i)ei + R\nwhere Y = (Y (1),...,Y (p))T forms a p-dimensional centered vector, e1,...,ep is an or-\nthonormal system, and R is a random vector, independent of Y and as small as pos-\nsible, in the sense that E(|R|2) is minimal.\nOne can see that, in an optimal decomposition, one needs RT ei = 0 for all i,\nbecause one can always write\np\nX\ni=1\nY (i)ei + R =\np\nX\ni=1\n(Y (i) + RT ei)ei + R −\np\nX\ni=1\nRT eiei .\nIf R is centered, then so is R −Pp\ni=1 RT eiei and the latter provides a better solution\nsince |R −Pp\ni=1 RT eiei| ≤|R|. Also, there is no loss of generality in requiring that\n(Y (1),...,Y (p)) are uncorrelated, as this can always be obtained after a change of basis\nin span(e1,...,ep).\n\n20.3. STATISTICAL INTERPRETATION AND PROBABILISTIC PCA\n507\nAssuming this, we can write\nE(|X|2) =\np\nX\ni=1\nE((Y (i))2) + E(|R|2)\nwith Y (i) = eT\ni X. So, to minimize E(|R|2), one needs to maximize\np\nX\ni=1\nE((eT\ni X)2)\nwhich is equal to (letting Σ be the covariance matrix of X)\np\nX\ni=1\neT\ni Σei.\nThe solution of this problem is given by the first p eigenvectors of Σ. PCA (with a\nEuclidean metric) exactly applies this procedure, with Σ replaced by the empirical\ncovariance.\n“Probabilistic PCA” is based on a slightly different statistical model in which it is\nassumed that X can be decomposed as\nX =\np\nX\ni=1\nλiY (i)ei + σR,\nwhere R is a d dimensional standard Gaussian vector and Y = (Y (1),...,Y (p))T a p-\ndimensional standard Gaussian vector, independent of R. The main difference with\nstandard PCA is that the total variance of the residual, here dσ2, is a model param-\neter and not a quantity to minimize.\nIn addition to σ2, the model is parametrized by the coordinates of e1,...,ep and\nthe values of λ1,...,λp. Introduce the d × p matrix\nW = [λ1e1,...,λpep].\nWe can rewrite this model in the form\nX = WY + σ2R\nwhere the parameters are W and σ2, with the constraint that W T W is a diagonal\nmatrix. As a linear combination of independent Gaussian random variables, X is\n\n508\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nGaussian with covariance matrix WW T + σ2Id. The log-likelihood of the observa-\ntions x1,...,xN therefore is\nL(W,σ) = −N\n2\n\u0012\nd log2π + logdet(WW T + σ2Id) + trace((WW T + σ2Id)−1ΣT )\n\u0013\n(20.7)\nwhere ΣY is the empirical covariance matrix of x1,...,xN. This function can be max-\nimized explicitly in W and σ, as stated in the following proposition.\nProposition 20.6 Assume that the matrix ΣT is invertible. The log-likelihood in (20.7)\nis maximized by taking\n(i) W = [λ1e1,...,λpep] where e1,...,ep are the eigenvectors of ΣT associated to the p\nlargest eigenvalues, and λi =\nq\nδ2\ni −σ2, where δ2\ni is the eigenvalue of Σ associated to ei;\n(ii) and\nσ2 =\n1\nd −p\nd\nX\ni=p+1\nδ2\ni .\nProof We make the following change of variables: let ρ2 = 1/σ2 and\nµ2\ni = 1\nσ2 −\n1\nλ2\ni + σ2.\nLet Q = [µ1e1,...,µpep]. We have\n(WW T + σ2Id)−1 = ρ2Id −QQT .\nTo see this, complete (e1,...,ep) into an orthonormal basis of Rd, letting ep+1,...,ed\ndenote the added vectors. Then\nWW T + σ2Id =\np\nX\ni=1\n(λ2\ni + σ2)eieT\ni +\nd\nX\ni=p+1\nσ2eieT\ni\nso that\n(WW T + σ2Id)−1 =\np\nX\ni=1\n(λ2\ni + σ2)−1eieT\ni +\nd\nX\ni=p+1\nσ−2eieT\ni = ρ2Id −QQT .\nUsing these variables, we can reformulate the problem as the minimization of\n−\np\nX\ni=1\nlog(ρ2 −µ2\ni ) −(d −p)logρ2 + ρ2trace(Σ) −\np\nX\nj=1\nµ2\nj eT\nj Σej.\n\n20.4. GENERALIZED PCA\n509\nFrom theorem 2.3, we have\np\nX\nj=1\nµ2\nj eT\nj Σej ≤\np\nX\nj=1\nµ2\nj δ2\nj\nand this upper bound is attained by letting e1,...,ep be the first p eigenvectors of Σ.\nUsing this, we see that σ2,µ2\n1,...,µ2\np must minimize\n−\np\nX\ni=1\nlog(ρ2 −µ2\ni ) −(d −p)logρ2 + ρ2\nd\nX\nj=1\nδ2\nj −\np\nX\nj=1\nµ2\nj δ2\nj .\nComputing the solution is elementary and left to the reader, and yields, when ex-\npressed as functions of σ2,λ2\n1,...,λ2\np, the expressions given in the statement of the\ntheorem.\n■\n20.4\nGeneralized PCA\nWe now discuss a dimension reduction method called generalized PCA (GPCA) [200]\nthat, instead of looking for the best linear approximation of the training set by one\nspecific subspace, provides an approximation by a finite union of such spaces.\nAs a motivation, consider the situation in fig. 20.1 in which part of the data\nis aligned along one direction in space, and another part along another direction.\nThen, the only information that PCA can retrieve (provided that the two directions\nintersect) is the plane generated by the two directions, which will be captured by\nthe two principal components. PCA will not be able to determine the individual\ndirections. GPCA addresses this type of situation as follows.\nFigure 20.1: PCA cannot distinguish between the situations depicted in the two datasets.\n\n510\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nFor simplicity, assume that we are trying to decompose the data along unions of\nhyperplanes in Rd. Such hyperplanes have equations of the form uT ˜x = 0 where ˜x is\nour notation for the vector (1,xT )T . If we have two hyperplanes, specified by u1 and\nu2 and all the training samples approximately belong to one of them, then one has,\nfor all k = 1,...,N:\n(uT\n1 ˜xk)(uT\n2 ˜xk) = ˜xT\nk u1uT\n2 ˜xk ≃0.\nSimilarly, for n hyperplanes, the identity is, for k = 1,...,N:\nn\nY\nj=1\n(uT\nj ˜xk) ≃0.\nWrite\nn\nY\nj=1\n(uT\nj x) =\nX\n1≤i1,...,in≤d\nu1(i1)···un(in)x(i1) ···x(in)\nin the form (by regrouping the terms associated with the same powers of x)\nF(x) =\nX\np1+...+pd=n\nqp1...pd (x(1))p1 ...(x(d))pd .\n(20.8)\nThe collection of \u0000n+d−1\nn\n\u0001 numbers Q = (qp1...pn,p1 + ··· + pd = n) takes a specific form\n(that we will not need to make explicit) as a function of the unknown u1,...,un, but\nthe first step of GPCA ignores this constraint and estimates Q minimizing\nN\nX\nk=1\n\n\nX\np1+...+pd=n\nqp1...pd (x(1)\nk )p1 ...(x(d)\nk )pd\n\n\n2\nunder the constraint Pq2\np1...pn = 1 (to avoid trivial solutions). Choosing an ordering\non the set of indices (p1,...,pd) such that p1 +···pd = n, one can stack the coefficients\nin Q and the monomials (x(1)\nk )p1 ...(x(d)\nk )pd to form two vectors denoted Q (with some\nabuse of notation) and V (xk). One can then rewrite the problem of determining Q\nas minimizing QT ΣQ subject to |Q|2 = 1, where\nΣ =\nN\nX\nk=1\nV (xk)V (xk)T .\nThe solution is given by the eigenvector associated with the smallest eigenvalue of Σ.\nIf the model is exact, this eigenvalue should be zero, and if only one decomposition\nof the data in a set of distinct hyperplanes exists (i.e., if n is not chosen too large),\nthen Q is the unique solution up to a multiplicative constant.\n\n20.5. NUCLEAR NORM MINIMIZATION AND ROBUST PCA\n511\nOnce Q is found, it remains to identify the vectors u1,...,un. This identification\ncan be obtained by inspecting the gradient of F on the union of hyperplanes. Indeed,\none has, for x ∈Rd,\n∇F(x) =\nn\nX\nj=1\n\n\nY\nj′,j\nuT\nj′ x\n\nuj\nHowever, if x belong in one and only one of the hyperplanes, say xT uj = 0, then all\nterms in the sum vanish but one and ∇F(x) is proportional to uj. So, if the model is\nexact, one has, for each k = 1,...,N, either ∇F(xk) = 0 (if xk belongs to the intersection\nof two hyperplanes) or ∇F(xk)/|∇F(xk)| = ±uj for some j, and the sign ambiguity can\nbe removed by ensuring, for example, that the first non-vanishing coordinate of uj is\npositive. (The gradient of F can be computed from Q using (20.8).) The computation\nof ∇F on training data therefore allows for an exact computation of the hyperplanes.\nIn practice, when noise is present, one cannot expect this computation to be\nexact. The vectors u1,...,un can be estimated by clustering the collection of non-\nvanishing gradients ∇F(xk), k = 1,...,N. For example, one can compute a dissimi-\nlarity matrix such as dkl = 1 −cos2(θkl), where θkl is the angle between ∇F(xk) and\n∇F(xl), and apply one of the methds discussed in section 19.4.1.\nThis analysis provides a decomposition of the training set into n (or fewer) hyper-\nplanes. The computation can then be recursively refined in order to obtain smaller\ndimensional subspaces by applying the same method separately to each hyperplane.\n20.5\nNuclear norm minimization and robust PCA\n20.5.1\nLow-rank approximation\nOne can also interpret PCA in terms of low-rank matrix approximations. Let Xc be\nthe N by d matrix (x1 −x,...,xN −x)T , which, in generic situations, has rank d −1.\nThen PCA with p components is equivalent to minimizing, over all N by d matrices\nZ of rank p, the norm of the difference\n|Xc −Z|2 = trace((Xc −Z)T (Xc −Z)).\n(20.9)\nThe quantity |A|2 = trace(AT A) is the sum of square of the entries of A, which is often\nreferred to as the (squared) Frobenius norm. We have\n|A|2 =\nd\nX\nk=1\nσ2\nk\nwhere σ1,...,σd are the singular values of A, i.e., the square roots of the eigenvalues\nof AT A.\n\n512\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nWe first note the following characterization of rank-p matrices.\nProposition 20.7 A matrix Z has rank p if and only if it can be written in the form\nZ = AW T where A is N ×p, and W is d ×p with W T W = IdRp, i.e., W = [e1,...,ep] where\nthe columns form an orthonormal family of Rd.\nProof The “if” part is obvious and we prove the “only if” part. Assume that Z has\nrank p. Take W = [e1,...,ep], where (e1,...,ep) is an orthonormal family in Null(Z)⊥.\nLetting ep+1,...,ed denote an orthonormal basis of Null(Z), we have Pd\ni=1 eieT\ni = IdRd\nand\nZ = Z\nd\nX\ni=1\neieT\ni = Z\np\nX\ni=1\neieT\ni = ZWW T\nso that one can take A = ZW.\n■\nUsing this representation and letting zT\nk be the kth row vector of Z, we have\n|Xx −Z|2 =\nN\nX\nk=1\n|xk −x −zk|2 =\nN\nX\nk=1\n\f\f\f\fxk −x −\np\nX\nj=1\na(j)\nk ej\n\f\f\f\f\n2\n.\nWith fixed e1,...,ep, the optimal matrix A has coefficients a(j)\nk = (xk −x)T ej. In matrix\nform, this is:\nZ = Xc\n\n\np\nX\nj=1\nejeT\nj\n\n.\nWe therefore retrieve the PCA formulation that we gave in section 20.1, in the\nspecial case of H = Rd with the standard Euclidean product.\nThe lowest value\nachieved by the PCA solution is\n|Xc −Z|2 = N\nd\nX\nk=p+1\nλ2\nk\nwhere λ2\n1,...,λ2\nd are the eigenvalues of the covariance matrix computed from x1,...,xN,\nwho are also the squared singular values of the matrix Xc divided by N.\nIn this section, we will explore variations on PCA in which the minimization\nof |Xc −Z|2 is completed with a penalty that depends on the singular values of the\nmatrix Z. As a first example, one can modify PCA by adding a penalty on the rank\n(i.e., on the number of non-zero singular values), minimizing:\nγ|Xc −Z|2 + rank(Z)\n\n20.5. NUCLEAR NORM MINIMIZATION AND ROBUST PCA\n513\nfor some parameter γ > 0. However, the solution to this problem is a small variation\nof that of standard PCA. It is indeed given by standard PCA with p components\nwhere p minimizes\nNγ\nd\nX\nk=p+1\nλ2\nk + p = Nγ\nd\nX\nk=p+1\n(λ2\nk −(Nγ)−1) + d,\ni.e., p is the index of the last eigenvalue that is larger than (Nγ)−1.\n20.5.2\nThe nuclear norm\nBased on the fact that rank(Z) is the number of non-zero singular values of Z, one\ncan use the same heuristic as in the development of the lasso, and replace counting\nthe non-zero values by the sum of the absolute values of the singular values, which\nis just the sum of singular values since they are non-negative. This provides the\nnuclear norm of A, defined in section 2.4 by\n|A|∗=\nd\nX\nk=1\nσk\nwhere σ1,...,σd are the singular values of A. We will consider below the problem of\nminimizing\nγ|Xc −Z|2 + |Z|∗\n(20.10)\nand show that its solution is once again similar to PCA.\nWe recall the characterization of the nuclear norm proposition 2.6. If A is an N\nby d matrix,\n|A|∗= max\n\u001a\ntrace(UAV T ) : U is N × N and UT U = Id,V is d × d and V T V = Id\n\u001b\n.\nIn Cai et al. [45], the authors consider the minimization of (20.10) and prove\nthe following result. Recall that we have defined the shrinkage function Sτ : t 7→\nsign(t)max(|t|−τ,0) (with τ ≥0), using the same notation Sτ(X) when applying Sτ to\nevery entry of a vector or matrix X. Following Cai et al. [45], we define the singular\nvalue thresholding operator A 7→Sτ(A), where A is any rectangular matrix, by\nSτ(A) = USτ(∆)V T\nwhen A = U∆V T is a singular value decomposition of A.\nProposition 20.8 Let us assume without loss of generality that N ≥d. The function\nZ 7→γ|Xc −Z|2 + |Z|∗is minimized by Z = S1/2γ(X ).\n\n514\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nProof Representing Z by its singular value decomposition, we have the equivalent\nformulation of minimizing\nF(U,V ,D) = γ|Xc −UDV T |2 + |D|∗\n= γ|Xc|2 −2γtrace(X T\nc UDV T ) + γ|D|2 + |D|∗\nover all orthonormal matrices U and V and diagonal matrices with non-negative\ncoefficients D. From theorem 2.1, we know that trace(X T\nc UDV T ) is less than the\nsum of the products of the non-increasingly ordered singular values of Xc and D\nand this upper bound is attained by taking U = ¯U and V = ¯V where ¯U and ¯V are\nthe matrices providing the SVD of Xc, i.e., such that Xc = ¯U∆¯V T where ∆is diagonal\nwith non-decreasing coefficients along the diagonal. So, letting λ1 ≥··· ≥λd ≥0 and\nµ1 ≥··· ≥µd ≥0 be the singular values of Xc and Z, we have just proved that, for any\nD,\nF(U,V ,D) ≥F( ¯U, ¯V ,D) = −2γ\nd\nX\ni=1\nµiλi + γ\nd\nX\ni=1\nµ2\ni +\nd\nX\ni=1\nµi .\nThe lower bound is minimized when µi = max(λi −1/2γ,0). This proves the propo-\nsition.\n■\n20.5.3\nRobust PCA\nAs a consequence, the nuclear norm penalty provides the same principal directions\n(after replacing γ by 2γ) as the rank penalty, but applies a shrinking operation rather\nthan thresholding on the singular values. The difference is however more fundamen-\ntal if, in addition to using the nuclear norm as a penalty, on replaces the squared\nFrobenius norm on the approximation error by the ℓ1 norm, where, for an n by m\nmatrix A with coefficients (a(i,j)),\n|A|ℓ1 =\nX\ni,j\n|a(i,j)|.\nThis is the formulation of robust PCA [49], which minimizes\nγ|Xc −Z|ℓ1 + |Z|∗\n(20.11)\nwith respect to Z.\nRobust PCA (which was initially named Principal Component Pursuit by the au-\nthors in Cand`es et al. [49]) is designed for situations in which Xc can be decomposed\nas the sum of a low-rank matrix Z and of a sparse residual S. Some theoretical justi-\nfication was provided in the original paper, stating that if Xc = Z+S, with Z = UDV T\n(its singular value decomposition) such that U and V are sufficiently “diffuse” and\n\n20.6. INDEPENDENT COMPONENT ANALYSIS\n515\nrank(Z) is small enough, with the residual’s sparsity pattern taken uniformly at ran-\ndom over the subsets of entries of S with a sufficiently small cardinality, then robust\nPCA is able to reconstruct the decomposition exactly with high probability (relative\nto the random selection of the sparsity pattern of S). We refer to Cand`es et al. [49]\nfor the long proof that justifies this statement.\nRobust PCA can be solved using the ADMM algorithm (section 3.5.5) after refor-\nmulating the problem as the minimization of\nγ|R|ℓ1 + |Z|∗\nsubject to R + Z = Xc. The algorithm therefore iterates over the following steps.\n\nZ(k+1) = argmin\nZ\n\u0012\n|Z|∗+ 1\n2α |Z + R(k) −Xx + U(k)|2\u0013\nR(k+1) = argmin\nR\n\u0012\nγ|R|ℓ1 + 1\n2α|Z(k+1) + R −Xc + U(k)|2\u0013\nU(k+1) = U(k) + Z(k+1) + R(k+1) −Xc\n(20.12)\nThe first minimization is covered by proposition 2.6 and yields\nZ(k+1) = Sα(Xc −R(k) −U(k)).\nThe second minimization is solved by a standard shrinking operation, i.e.,\nR(k+1) = Sγα(Xc −Z(k+1) −U(k)).\nUsing this, we can rewrite the robust PCA algorithm as the sequence of fairly simple\niterations.\nAlgorithm 20.1\n(1) Choose a small enough constant α and a very small tolerance level ϵ.\n(2) Initialize the algorithm with N by d matrices R(0) and U(0) (e.g., equal to zero).\n(3) At step n, apply the iteration:\n\nZ(k+1) = Sα(Xc −R(k) −U(k))\nR(k+1) = Sγα(Xc −Z(k+1) −U(k))\nU(k+1) = U(k) + Z(k+1) + R(k+1) −Xc\n(20.13)\n(4) Stop the algorithm is the variation compared to variables at the previous step is\nbelow the tolerance level. Otherwise, apply step n + 1.\n\n516\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\n20.6\nIndependent component analysis\nIndependent component analysis (ICA) is a factor analysis method that represents a\nd-dimensional random variable X in the form X = AY where A is a fixed d ×d invert-\nible matrix and Y is a d-dimensional random vector with independent components.\nThere are two main approaches in this setting. The first one optimizes the matrix\nW = A−1 so that the components of WX are “as independent as possible” according\nto a suitable criterion. The second one is model-based, where a statistical model is\nassumed for Y, and its parameters, together with the entries of the matrix A, are es-\ntimated via maximum likelihood. Before describing each of these methods, we first\ndiscuss the extent to which the coefficients of A are identifiable.\n20.6.1\nIdentifiability\nA statistical model is identifiable if its parameters (which could be finite- of infinite-\ndimensional) are uniquely defined by the distribution of the observable variables. In\nthe case of ICA, this question boils down to deciding whether AY ∼A′Y ′ (i.e., they\nhave the same probability distribution) implies that A = A′ (where Y and Y ′ are two\nrandom vectors with independent components).\nIt should be clear that the answer to this question is negative, because there are\ntrivial transformations of the matrix A that do not break the ICA model. One can,\nfor example, take any invertible diagonal matrix, D, and let A′ = AD−1 and Y ′ = DY.\nThe same statement can be made if D is replaced by a permutation matrix, P, which\nreorders the components of Y. So we know that AY ∼A′Y ′ is possible already when\nA′ = ADP where D is diagonal and invertible and P is a permutation matrix. Note\nthat iterating such matrices (i.e., letting A′ = ADPD′P′) does not extend the class\nof transformations because one has DP = PP−1DP and one can easily check that\nP−1DP is diagonal, so that one can rewrite any product of permutations and diagonal\nmatrices as a single diagonal matrix multiplied by a single permutation.\nIt is interesting, and fundamental for the well-posedness of ICA, that, under one\nimportant additional assumption, the indeterminacy in the identification of A stops\nat these transformations. The additional assumption is that at most one of the com-\nponents of Y follows a Gaussian distribution. That such a restriction is needed is\nclear from the fact that one can transform any Gaussian vector Y with independent\ncomponents into another, BY, one as soon as BBT is diagonal. If two or more com-\nponents of Y are Gaussian, one can restrict these matrices B to only affect those\ncomponents. If only one of them is Gaussian, such an operation has no effect.\nThe following theorem is formally stated in Comon [54], and is a rephrasing\nof the Darmois-Skitovitch theorem [57, 179]. The proof of this theorem relies on\ncomplex analysis arguments on characteristic functions and is beyond the scope of\n\n20.6. INDEPENDENT COMPONENT ANALYSIS\n517\nthese notes (see Kagan et al. [101] for more details).\nTheorem 20.9 Assume that Y is a random vector with independent components, such\nthat at most one of its components is Gaussian. Let A be an invertible linear transforma-\ntion and ˜Y = CY. Then the following statements are independent.\n(i) For all i , j, the components ˜Y (i), ˜Y (j) are independent.\n(ii) ˜Y (1),..., ˜Y (d) are mutually independent.\n(iii) C = DP is the product on a diagonal matrix and of a permutation.\nThe equivalence of (ii) and (iii) implies that the ICA model is identifiable up\nto multiplication on the right by a permutation and a diagonal matrix. Indeed, if\nX = AY = A′Y ′ are two decompositions, then it suffices to apply the theorem to\nC = (A′)−1A to conclude. The equivalence of (i) and (ii) is striking, and has the\nimportant consequence that, if the data satisfies the ICA model, then, in order to\nidentify A (up to the listed indeterminacy), it suffices to look for Y = A−1X with\npairwise independent components, which is a much lesser constraint than full mu-\ntual independence.\nAs a final remark on the Gaussian indeterminacy, we point out that, if the mean\n(m) and covariance matrix (Σ) of X are known (or estimated from data), the ICA\nproblem can be reduced to looking for orthogonal transformations A. Indeed, as-\nsuming X = AY and letting ˜X = Σ−1/2(X −m) and ˜Y = D−1/2(Y −A−1m), where D is\nthe (diagonal) covariance matrix of Y, we have\n˜X = Σ−1/2(AY −m) = Σ−1/2AD1/2 ˜Y.\nLetting ˜A = Σ−1/2AD1/2, we have IdRd = E( ˜X ˜XT ) = ˜A ˜AT so that ˜A is orthogonal.\nThis shows that the ICA problem for ˜X in the form ˜X = ˜A ˜Y with the restriction\nthat ˜A is orthogonal has a solution, and also provides a solution of the original ICA\nproblem by letting A = Σ1/2 ˜A and Y = ˜Y −˜A−1Σ−1/2m. Therefore, the indeterminacy\nassociated with Gaussian vectors is as general as possible up to a normalization of\nfirst and second moments.\n20.6.2\nMeasuring independence and non-Gaussianity\nIndependence between d variables is a very strong property and its complete char-\nacterization is computationally challenging. The fact that the joint p.d.f.of the d\nvariables (we will restrict, to simplify our discussion, to variables that are absolutely\ncontinuous) factorizes into the product of the marginal p.d.f.’s of each variable can\nbe measured by computing the mutual information between the variables, defined\n\n518\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nby (letting ϕZ denote the p.d.f. of a variable Z)\nI(Y) =\nZ\nϕY(y)\nQd\ni=1 ϕY (i)(y(i))\nϕY(y)dy.\nThe mutual information is always non-negative and vanishes only if the components\nof Y are mutually independent. Therefore, one can represent ICA as an optimization\nproblem minimizing I(WX) with respect to all invertible matrices W (so that W =\nA−1). Letting\nh(Y) = −\nZ\nlogϕY(y)ϕY(y)dy\ndenote the “differential entropy” of Y, we can write\nI(Y) =\nd\nX\ni=1\nh(Y (i)) −h(Y).\nIf Z = WX, then ϕZ(z) = ϕX(W −1x)|det(W)|−1. Using this expression in h(Z) and\nmaking a change of variables yields h(WZ) = h(X) + log|detW| and\nI(WX) =\nd\nX\ni=1\nh(Z(i)) −log|det(W)| −h(X).\nThis shows that the optimal W can be obtained by minimizing\nF(W) =\nd\nX\ni=1\nh(W (i)X) −log|det(W)|\nwhere W (i) is the ith row of W. This brings a notable simplification, since this ex-\npression only involves differential entropies of scalar variables, but still remains a\nchallenging problem.\nIn Comon [54], it is proposed to use cumulant expansions of the entropy around\nthat of a Gaussian with identical mean and variance to approximate the differential\nentropy. If ξ ∼N (m,σ2) , then\nh(ξ) = 1\n2 + 1\n2 log(2πσ2).\nDefine, for a general random variable U with standard deviation σU, the non-Gaussian\nentropy, or negentropy, defined by\nν(U) = 1\n2 + 1\n2 log(2πσ2\nU) −h(U).\n\n20.6. INDEPENDENT COMPONENT ANALYSIS\n519\nOne can shows that ν(U) ≥0 and is equal to 0 if and only if U is Gaussian. One can\nrewrite F(W) as\nF(W) = d\n2 + d\n2 log(2π) +\nd\nX\ni=1\nlog(σ2\nW (i)X) −\nd\nX\ni=1\nν(W (i)X) −log|det(W)|\nAs we remarked earlier, if we replace X by Σ−1/2(X −m) (after estimating the\ncovariance matrix of X), there is no loss of generality in requiring that W is an or-\nthogonal matrix, in which case both σ2\nW (i)X and |detW| are equal to 1. Assuming\nsuch a reduction is done, we see that the problem now requires to maximize\nd\nX\ni=1\nν(W (i)X)\n(20.14)\namong all orthogonal matrices W. Still in Comon [54], an approximation of the\nnegentropy ν(U) is provided as a function of the third and fourth cumulants of the\ndistribution of U. These are given by\nκ3 = E((U −E(U))3)\nand\nκ4 = E((U −E(U))4) −3σ4\nU.\nIn particular, when U is normalized, i.e., E(U) = 0 and σ2\nU = 1, we have κ3 = E(U3)\nand κ4 = E(U4)−3. Under the same assumption, it is proposed in Comon [54] to use\nthe approximation\nν(U) ∼κ2\n3\n12 + κ2\n4\n48 + 7κ4\n3\n48 −κ2\n3κ4\n8\n.\nThis approximation was derived from an Edgeworth expansion of the p.d.f. of U,\nwhich can be seen as a Taylor expansion around a Gaussian distribution. Plugging\nthis expression into (20.14) provides an expression that can be maximized in W\nwhere the cumulants are replaced by their sample estimates. However, the maxi-\nmized function involves high-degree polynomials in the unknown coefficients of W,\nand this simplified problem still presents numerical challenges.\nAn alternative approximation of the negentropy has been proposed in Hyv¨arinen\n[94] relying on the maximum entropy principle, described in the following theorem.\nAssociate to any random variable Y : G →R the differential entropy\nhµ(Y) = −\nZ\nG\nlogϕY(x)ϕY(x)dµ(x)\n\n520\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nif the distribution of Y has a density, denoted ϕY, with respect to µ and hµ(Y) = −∞\notherwise. Use also the same notation\nhµ(ϕ) = −\nZ\nG\nlogϕ(x)ϕ(x)dµ(x)\nfor a p.d.f. ϕ with respect to µ (i.e., such that ϕ is non-negative and has integral 1).\nThen, the following is true.\nTheorem 20.10 Let g = (g(1),...,g(p))T be a function defined on a measurable space G,\ntaking values in Rp, and let µ be a measure on G. Let Γµ be the set of all λ = (λ(1),...,λ(p)) ∈\nRp such that\nZ\nG\nexp\n\u0010\nλT g(y)\n\u0011\ndµ(y) < ∞.\n(20.15)\nThen\nhµ(Y) ≤inf\n(\n−λT E(g(Y)) + log\nZ\nG\nexp\n\u0010\nλT g(y)\n\u0011\ndµ(y) : λ ∈Γµ\n)\n.\n(20.16)\nDefine, for λ ∈Γµ,\nψλ(x) =\nexp\n\u0010\nλT g(x)\n\u0011\ndµ(x)\nR\nG exp\n\u0010\nλT g(y)\n\u0011\ndµ(y)\n.\n(20.17)\nAssume that the infimum in (20.16) is attained at an interior point λ∗of Γµ. Then\nh(ϕλ∗) = max{h( ˜Y) : E ˜Y(g) = EY(g),i = 1,...,p}.\n(20.18)\nProof Let Y be a random variable with p.d.f. ϕY with respect to µ (otherwise the\nlower bound in (20.16) is −∞). Then\nhµ(Y) + λE(g(Y)) −log\nZ\nexp(λg(y))dµ(y) = −\nZ\nG\nϕY(x)log ϕY(x)\nψλ(x)dµ(x) ≤0\nsince\nR\nG ϕY(x)log ϕY (x)\nψλ(x)dµ(x) is a KL divergence and is always non-negative.\nAssume that λ is in ˚Γµ. Then, there exists ϵ > 0 such that, for any u ∈Rp, |u| = 1,\nλ + ϵu ∈Γµ. Using the fact that eβ ≥eα + (β −α)eα, we can write\nϵuT geλT g ≤e(λ+ϵu)T g −eλT g\n−ϵuT geλT g ≤e(λ−ϵu)T g −eλT g\nyielding\nϵ|uT g|eλT g ≤max(e(λ+ϵu)T g,e(λ−ϵu)T g) −eλT g ≤e(λ+ϵu)T g + e(λ−ϵu)T g −eλT g.\n\n20.6. INDEPENDENT COMPONENT ANALYSIS\n521\nSince the upper-bound is integrable with respect to µ, so is the lower bound, showing\nthat (taking u in the canonical basis of Rp)\nZ\nG\n|g(i)(y)|eλT g(y)dµ(y) < ∞\nfor all i, or\nZ\nG\n|g(y)|eλT g(y)dµ(y) < ∞.\nLet c = E(g(Y)) and define\nΨc(λ) = −cT λ + log\nZ\nexp(λT g(y))dy.\n(20.19)\nThen\n∂λΨc = −cT +\nR\ng(x)T exp(λT g(x))dx\nR\nexp(λT g(y))dy\n= −cT +\nZ\nG\ng(x)T ψλ(x)dx.\nSince λ∗is a minimizer, we find that, if ˜Y is a random variable with p.d.f. λ∗, then\nE( ˜Y) = c = E(Y). In that case, the upper-bound in (20.16) is hµ( ˜Y), proving (20.18). ■\nRemark 20.11 The previous theorem is typically applied with µ equal to Lebesgue’s\nmeasure on G = Rd or to a counting measure with G finite. To rewrite the statement\nof theorem 20.10 in those cases, it suffices to replace dµ(x) by dx for the former, and\nintegrals by sums over G for the latter. In the rest of the discussion, we restrict to the\ncase when µ is Lebesgue’s measure, using h(Y) instead of hµ(Y).\n♦\nRemark 20.12 This principle justifies, in particular, that the negentropy is always\nnon-negative since it implies that a distribution that maximizes the entropy given\nits first and second moments must be Gaussian.\n♦\nThe right-hand side of (20.16) provides a variational approximation of the en-\ntropy. If one uses this approximation when minimizing h(W (1)X) + ··· + h(W (d)X),\nthe resulting problem can be expressed as a minimization, with respect to W and\nλ1,...,λd ∈Rp of\n−\nd\nX\nj=1\nλT E(g(W (j)X)) +\nd\nX\nj=1\nlog\nZ\nexp\n\u0010\nλT g(y)\n\u0011\ndy .\nWhile it would be possible to solve this optimization problem directly, a further\napproximation of the upper bound can be developed leading to a simpler procedure.\n\n522\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nWe have seen in the previous proof that, defining Ψc by (20.19) and denoting by Eλ\nthe expectation with respect to ϕλ, one has\n∇Ψc(λ) = −c + Eλ(g).\nTaking the second derivative, one finds\n∇2Ψc(λ) = Eλ((g −Eλ(g))(g −Eλ(g))T ).\nNow choose c0 such that a maximizer of Ψc0(λ), say, λc0, is known. If c is close to c0,\na first order expansion indicates that, for λc maximizing Ψc, one should have\nλc ≃λc0 + ∇2Ψc(λc0)−1(c −c0)\nwith\nΨc(λc) ≃Ψc(λc0) −(c −c0)T ∇2Ψc(λc0)−1(c −c0).\nOne can then use the right-hand side as an approximation of the optimal entropy.\nThis leads to simple computations under the following assumptions. First, as-\nsume that the first two functions g(1) and g(2) are u and u2/\n√\n3. Let ϕ0 be the p.d.f.\nof a standard Gaussian. Assume that the functions g(j) are chosen so that\nZ\ng(i)(u)g(j)(u)ϕ0(y)dy = δij\nfor i,j = 1,...,p and such that\nR\ng(i)(u)ϕ0(y)dy = 0 for i , 2. Take\nc0 =\nZ\ngϕ0(u)du\nso that c(1)\n0 = 0, c(2)\n0 = 1/\n√\n3 and c(i)\n0 = 0 for i ≥2.\nThen λc0 provides, by construction, the distribution ϕ0 and for any c, ∇2Ψc(λc0) =\nIdRp. With these assumptions, the approximation is\nΨc(λ) = h(ϕ0) −|c −c0|2\n= 1\n2(1 + log2π) −\nX\nj≥3\n(c(j))2\n(assuming that the data is centered and normalized so that c(1) = 0 and c(2) = 1/\n√\n3).\nThe ICA problem can then be solved by maximizing\nd\nX\nj=1\np\nX\ni=1\nE(g(i)(W (j)X))2\n(20.20)\nover orthogonal matrices W.\n\n20.6. INDEPENDENT COMPONENT ANALYSIS\n523\nRemark 20.13 Without the assumption made on the functions g(j), one needs to\ncompute S = Cov(g(U))−1 where U ∼N (0,1) and maximize\nd\nX\nj=1\n(E(g(W (j)X)) −E(g(U)))T S(E(g(W (j)X)) −E(g(U))).\nClearly, this expression can be reduced to (20.20) by replacing g by S−1/2(g−E(g(U))).\nNote also that we retrieve here a similar idea to the negentropy, maximizing a devi-\nation to a Gaussian.\n♦\n20.6.3\nMaximization over orthogonal matrices\nIn the previous discussion, we reached a few times a formulation of ICA which re-\nquired optimizing a function W 7→F(W) over all orthogonal matrices. We now dis-\ncuss how such a problem may be implemented.\nIn all the examples that were considered, there would have been no loss of gen-\nerality in requiring that W is a rotation, i.e., det(W) = 1. This is because one can\nchange the sign of this determinant by simply changing the sign of one of the in-\ndependent components, which is always possible. (In fact, the indeterminacy in W\nis by right multiplication by the product of a permutation matrix and a diagonal\nmatrix with ±1 entries.)\nLet us assume that F(W) is actually defined and differentiable over all invertible\nmatrices, which form an open subset of the linear space Md(R) of d by d matrices.\nOur optimization problem can therefore be considered as the minimization of F with\nthe constraint that WW T = IdRd.\nGradient descent derives from the analysis that a direction of descent should be\na matrix H such that F(W + ϵH) < F(W) for small enough ϵ > 0 and on the remark\nthat H = −∇F(W) provides such a direction. This analysis does not apply to the con-\nstrained optimization setting because, unless the constraints are linear, W + ϵH will\ngenerally stop to satisfy the constraint when ϵ > 0, requiring the use of more complex\nprocedures. In our case, however, one can take advantage of the fact that orthogo-\nnal matrices form a group to replace the perturbation W 7→W + ϵH by W 7→WeϵH\n(using the matrix exponential) where H is moreover required to be skew symmetric\n(H + HT = 0), which guarantees that eϵH is an orthogonal matrix with determinant\n1. Now, using the fact that eϵH = Id + ϵH + o(ϵ), we can write\nF(WeϵH) = F(W) + ϵtrace(∇F(W)T WH) + o(ϵ).\nLet ∇sF(W) be the skew symmetric part of W T ∇F(W), i.e.,\n∇sF(W) = 1\n2(W T ∇F(W) −∇F(W)T W).\n\n524\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nThen, if H is skew symmetric,\ntrace(∇sF(W)T H) = 1\n2trace(∇F(W)T WH) −1\n2trace(W T ∇F(W)H)\n= 1\n2trace(∇F(W)T WH) + 1\n2trace(W T ∇F(W)HT )\n= trace(∇F(W)T WH)\nso that\nF(WeϵH) = F(W) + ϵtrace(∇sF(W)T H) + o(ϵ).\nThis show that H = −∇sF(W) provides a direction of descent in the orthogonal group,\nin the sense that, if ∇sF(W) , 0,\nF(We−ϵ∇sF(W)) < F(W)\nfor small enough ϵ > 0. As a consequence, the algorithm\nWn+1 = Wne−ϵn∇sF(Wn)\ncombined with a line search for ϵn implements gradient descent in the group of\northogonal matrices, and therefore converges to a local minimizer of F.\nIf one linearizes the r.h.s. as a function of ϵ, one gets\nWne−ϵn∇sF(Wn) = Wn + ϵn\n2 Wn((Wn)T ∇F(Wn) −∇F(Wn)T Wn) + o(ϵ)\n= Wn + ϵn\n2 (∇F(Wn) −Wn∇F(Wn)T Wn) + o(ϵ).\nAs already argued, this linearized version cannot be used when optimizing over the\northogonal group. However, if one denotes by ω(A) the unitary part of the polar\ndecomposition of A, i.e., ω(A) = (AAT )−1/2A, then the algorithm\nWn+1 = ω\n\u0012\nWn + ϵn\n2 (∇F(Wn) −Wn∇F(Wn)T Wn)\n\u0013\nalso provides a valid gradient descent algorithm.\n20.6.4\nParametric ICA\nWe now describe a parametric version of ICA in which a model is chosen for the in-\ndependent components of Y. The simplest version of to assume that all Y (j) are i.i.d.\nwith some prescribed p.d.f., say, ψ. A typical example for ψ is a logistic distribution\nwith\nψ(t) =\n2\n(et + e−t)2.\n\n20.6. INDEPENDENT COMPONENT ANALYSIS\n525\nIf y is a vector in Rd, we will use, as usual, the notation ψ(y) = (ψ(y(1)),...,ψ(y(d)))T\nfor ψ applied to each component of y.\nThe model parameter is then the matrix A, or preferably W = A−1, and it may be\nestimated using maximum likelihood. Indeed, the p.d.f. of X is\nfX(x) = |detW|\nd\nY\nj=1\nψ(W (j)x)\nwhere W (j) is the jth row of W, so that W can be estimated by maximizing\nℓ(W) = N log|det(W)| +\nN\nX\nk=1\nd\nX\nj=1\nlogψ(W (j)xk).\nIf we denote by Γ(W) the matrix with coefficients\nγij(W) =\nN\nX\nk=1\nxk(i)ψ′(W (j)xk)\nψ(W (j)xk)\nand use the fact that the gradient of W 7→log|detW| is W −T (the inverse transpose\nof W), we can write\n∇ℓ(W) = NW −T + Γ(W).\nWe need however the maximization to operate on sets of invertible matrices, and\nit is more natural to move in this set through multiplication than through addition,\nbecause the product of two invertible matrices is always invertible, but not necessar-\nily their sum. So, similarly to the previous section, we will look for small variations\nin the form W 7→WeϵH, or simply, in this case, W 7→W(IdRd +ϵH). In both case, the\nfirst order expansion of the log-likelihood gives\nℓ(W) + ϵtrace((NW −T + Γ(W))T WH)\nwhich suggests taking\nH = W T (NW −T + Γ(W)) = NId + W T Γ(W).\nDividing H by N, we obtain the following variant of gradient ascent for maxi-\nmum likelihood\nWn+1 = (1 + ϵn)Wn + ϵnWnW T\nn Γ(Wn).\nThis algorithm numerically performs much better than standard gradient ascent. It\nmoreover presents the advantage of avoiding computing the inverse of W at each\nstep.\n\n526\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\n20.6.5\nProbabilistic ICA\nNote that the algorithms that we discussed concerning ICA were all formulated in\nterms of the matrix W = A−1, which “filters” the data into independent components.\nAs a result, ICA requires as many independent components as the dimension of X.\nMoreover, because the components are typically normalized to have equal variance,\nthere is no obvious way to perform dimension reduction using this method. Indeed,\nICA is typically run after the data is preprocessed using PCA, this preprocessing\nstep providing the reduction of dimension.\nIt is however possible to define a model similar to probabilistic PCA, assuming a\nlimited number of components to which a Gaussian noise is added, in the form\nX =\np\nX\nj=1\najY (j) + σR\nwith p < d, a1,...,ap ∈Rd, Y (1),...,Y (p) independent variables as before, and R ∼\nN (0,IdRd). This model is identifiable (up to permutation and scalar multiplication\nof the components) as soon as none of the variables Y (j) is Gaussian.\nLet us assume a parametric setting similar to that of the previous section, so that\nY (1),...,Y (p) are explicitly modeled as independent variables with p.d.f. ψ. Introduce\nthe matrix A = [a1,...,ap], so that the model can also be written X = AY + σR, where\nA and σ2 are unknown model parameters.\nThe p.d.f. of X is now given by\nfX(x;A,σ2) =\n1\n(2πσ2)d/2\nZ\nRp e−|x−Ay|2\n2σ2\n\n\np\nY\ni=1\nψ(y(i))\n\ndy(1) ...dy(p),\nwhich is definitely not a closed form. Since we are in a situation in which the pair of\nrandom variables is imperfectly observed through X, using the EM algorithm (chap-\nter 16) is an option, but it may, as we shall see below, lead to heavy computation. The\nbasic step of the EM is, given current parameters A0,σ0, to maximize the conditional\nexpectation (knowing X, for the current parameters) of the joint log-likelihood of\n(X,Y) with respect to the new parameters. In this context, the joint distribution of\n(X,Y) has density\nfX,Y(x,y;A,σ2) =\n1\n(2πσ2)d/2e−|x−Ay|2\n2σ2\np\nY\ni=1\nψ(y(i))\n\n20.6. INDEPENDENT COMPONENT ANALYSIS\n527\nso that, the conditional joint likelihood over the training set is\n−Nd\n2 log(2πσ2)−1\n2σ2\nN\nX\nk=1\nEA0,σ0(|xk −AY|2 |X = xk)−\nN\nX\nk=1\np\nX\nj=1\nEA0,σ2\n0 (logψ(Y (j))|X = xk).\nNotice that the last term does not depend on A,σ2, and that, given A, the optimal\nvalue of σ2 is given by\nσ2 = 1\nNd\nN\nX\nk=1\nEA0,σ0(|xk −AY|2 |X = xk)\nThe minimization of\nN\nX\nk=1\nEA0,σ0(|xk −AY|2 |X = xk)\nwith respect to A is a least square problem. Let b(j)\nk = EA0,σ0(Y (j)|X = xk) and sk(i,j) =\nEA0,σ0(Y (i)Y (j)|X = xk): the gradient of the previous term is\n−2\nN\nX\nk=1\nEA0,σ2\n0 ((xk −AY)Y T |X = xk) = −2\nN\nX\nk=1\n(xkbT\nk −ASk),\nbk being the column vector with coefficients b(j)\nk\nand Sk the matrix with coefficients\nsk(i,j). The result therefore is\nA =\n\n\nN\nX\nk=1\nxkbT\nk\n\n\n\n\nN\nX\nk=1\nSk\n\n\n−1\n.\nUnfortunately, the computation of the moments of the conditional distribution\nof Y given xk (needed in bk and Sk) is a difficult task. The conditional density of Y\ngiven X = xk is\ng(y|xk) = ψ(y)e\n−|A0y−x|2\n2σ2\n0\n/Z(A0,σ0)\nfrom which moments cannot be computed analytically in general. Monte-Carlo sam-\npling algorithms can be used however to approximate these moments, but they are\ncomputationally demanding. And they must be run at every step of the EM.\nIn place of the exact EM, one may use a mode approximation (section 16.3.1),\nwhich replaces the conditional likelihood of Y given X = xk by a Dirac distribution\n\n528\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nat the mode:\nˆyA0,σ0(xk) = argmaxy\n\nψ(y)e\n−|A0y−xk|2\n2σ2\n0\n\n.\nThe maximization step then reduces to maximizing in A,σ2\n−Nd\n2 log(2πσ2) −\n1\n2σ2\nN\nX\nk=1\n\f\f\fxk −A ˆyA0,σ0(xk)\n\f\f\f2 .\n(20.21)\nThis therefore provides a two-step procedure.\nAlgorithm 20.2 (Probabilistic ICA: mode approximation)\n(1) Initialize the algorithm with A0,σ0.\n(2) At step n:\n(i) For k = 1,...,N, maximize Qp\ni=1 ψ(y(i))e\n−|Any−xk|2\n2σ2n\nto obtain ˆyAn,σn(xk). This\nrequires a numerical optimization procedure, such as gradient ascent. The problem\nis concave when logψ is concave.\n(ii) Minimize (20.21) with respect to A,σ2, yielding\nAn+1 =\n\n\nN\nX\nk=1\nxkbT\nk\n\n\n\n\nN\nX\nk=1\nSk\n\n\n−1\nwith bk = ˆyAn,σn(xk), Sk = ˆyAn,σn(xk) ˆyAn,σn(xk)T , and\nσ2\nn+1 = 1\nNd\nN\nX\nk=1\n\f\f\fxk −A ˆyAn,σn\n\f\f\f2 .\n(3) Stop if the variation of the parameter is below a tolerance level. Otherwise,\niterate to the next step.\nOnce A and σ2 have been estimated, the y components associated to a new obser-\nvation x can be estimated by ˆyA,σ(x), therefore minimizing\n1\n2σ2\n\f\f\fxk −Ay\n\f\f\f2 +\np\nX\nj=1\nlogψ(y(j)),\nyielding the map estimate, the same convex optimization problem as in step (1)\nabove. Now we can see how the method takes from both PCA and ICA: the columns\n\n20.6. INDEPENDENT COMPONENT ANALYSIS\n529\nof A, a1,...,ap can be considered as p principal directions, and are fixed after learn-\ning; they are not orthonormal, and do not satisfy the nesting properties of PCA (that\nthose p contain those for p −1). The coordinates of x with respect to this basis is not\na projection, as would be provided by PCA, but the result of a penalized estimation\nproblem. The penalty associated to the logistic case is\nlogψ(y(j)) = log2 −2log(ey(j) + e−y(j)).\nThis distribution with “exponential tails” has the interest of allowing large values of\ny(j), which generally entails sparse decompositions, in which y has a few large coeffi-\ncients, and many zeros.\nAs an alternative to the mode approximation of the EM, which may lead to bi-\nased estimators, one may use the SAEM algorithm (section 16.4.3), as proposed in\nAllassonniere and Younes [3]. Recall that the EM algorithm replaces the parameters\nA0,σ2\n0 by minimizers of\nNd\n2 log(σ2) +\n1\n2σ2\nN\nX\nk=1\nEA0,σ0(|xk −AY|2 |X = xk)\n= Nd\n2 log(σ2) +\n1\n2σ2\nN\nX\nk=1\n|xk|2 −1\nσ2\nN\nX\nk=1\nxT\nk Abk +\n1\n2σ2\nN\nX\nk=1\ntrace(AT ASk),\nwhere the computation of b(j)\nk = EA0,σ0(Y (j)|X = xk) and sk(i,j) = EA0,σ0(Y (i)Y (j)|X = xk)\nwas the challenging issue. In the SAEM algorithm, the statistics bk and Sk are part of\na stochastic approximation scheme, and are estimated in parallel with EM updates\nas follows.\nAlgorithm 20.3 (SAEM for probabilistic ICA)\nInitialize the algorithm with parameters A, σ2. Define a sequence of decreasing\nsteps, γt.\nLet, for k = 1,...,N, bk = 0 and Sk = Id. Iterate the following steps.\n(1) For k = 1,...,N, sample yk according to the conditional distribution of Y given\nX = xk, using the current parameters A and σ2.\n(2) Update bk and Sk, letting (assuming step t of the algorithm)\n(bk →bk + γt(Yk −bk)\nSk →Sk + γt(YkY T\nk −Sk)\n\n530\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\n(3) Replace A and σ2 by\nA =\n\n\nN\nX\nk=1\nxkbT\nk\n\n\n\n\nN\nX\nk=1\nSk\n\n\n−1\nand\nσ2 = 1\nNd\nN\nX\nk=1\n\f\f\fxk −A ˆyA0,σ0\n\f\f\f2 .\nThe parameter γt should be decreasing with t, typically so that P\nt γt = +∞and\nP\nt γ2\nt < ∞(e.g., γt ∝1/t). One way to sample from Yk is to uses a rejection scheme,\niterating the procedure which samples y according to the prior and accepts the result\nwith probability M exp(−|xk −Ay|2/2σ2) until acceptance. Here M must be chosen so\nthat M maxy exp(−|xk −Ay|2/2σ2) ≤1 (e.g., M = 1).\nThis method will work for small p, but for large p, the probability of acceptance\nmay be very small. In such cases, Yk can be sampled changing one component at a\ntime using a Metropolis-Hastings scheme. If component j is updated, this scheme\nsamples a new value of y (call it y′) by changing only y(j) according to the prior\ndistribution ψ and accept the change with probability\nmin\n \n1, exp(−|xk −Ay′|2/2σ2)\nexp(−|xk −Ay|2/2σ2)\n!\n.\n20.7\nNon-negative matrix factorization\nIn this section, we consider factor analysis methods that approximate a random vari-\nable X in the form X = Pp\nj=1 a(j)Y (j) with the constraint that the scalars a(1), . . . ,\na(p) ∈R and the vectors Y (1),...,Y (p) ∈Rd are respectively non-negative and with\nnon-negative entries. This model makes sense, for example, when X represents the\ntotal multivariate production (e.g., in terms of number of molecules of various types)\nresulting of several chemical reactions that operate together. Another application\nis when X is a list of preference scores associated with a person for, say, books or\nmovies, and each person is modeled as a positive linear combination of p “typical\nscorers,” represented by the vector Y (j) for j = 1,...,p.\nWhen training data (x1,...,xN) is observed and stacked in an N by d matrix X ,\nthe decomposition can be summarized for all observations together in the matrix\nform\nX = AY T\n\n20.7. NON-NEGATIVE MATRIX FACTORIZATION\n531\nwhere A is N by p and provides the coefficients a(j)\nk associated with each observation\nand Y = [y(1),...,y(p)] is d by p and provides the p typical profiles. The matrices A\nand Y are unknown and their estimation subject to the constraint of having non-\nnegative components represent the non-negative matrix factorization (NMF) prob-\nlem.\nNMF is often implemented by solving the constrained optimization problem of\nminimizing |X −AY T |2 subject to A and Y having non-negative entries. This problem\nis non-convex in general but the sub-problems of optimizing either A or Y when the\nother matrix is fixed are simple quadratic programs.\nThis suggests using an alternating minimization method, iterating steps in which\nA is updated with Y fixed, followed by an update of Y with A fixed. However,\nsolving a full quadratic program at each step would be computationally prohibitive\nwith large datasets, and simpler update rules have been suggested, updating each\nmatrix in turn with a guarantee of reducing the objective function.\nIf Y is considered as fixed and A is the free variable, we have\n|X −AY T |2 = |X |2 −2trace(X T AY T ) + trace(AY T YAT )\n= trace(AT AY T Y) −2trace(AT (X Y)) + |X |2 .\nThe next lemma will provide update steps for A.\nLemma 20.14 Let M be an n by n symmetric matrix and b ∈Rn, both assumed to have\nnon-negative entries. Let u ∈Rn, also with non-negative coefficients, and let\nv(i) = u(i)\n\n\nb(i)\nPd\nj=1 m(i,j)u(j)\n\n.\nThen\nvT Mv −2bT v ≤uT Mu −2bT u .\nMoreover, v = u if and only if u minimizes uT Mu −2bT u subject to u(i) = 0, i = 1,...,n.\nProof Let F(u) = uT Mu −2bT u. We look for v(i) = β(i)u(i) with β(i) ≥0 such that\nF(v) ≤F(u). We have\nF(v) =\nn\nX\ni,j=1\nβ(i)β(j)u(i)u(j)m(i,j) −2\nn\nX\ni=1\nb(i)β(i)\n≤1\n2\nn\nX\ni,j=1\n((β(i))2 + (β(j))2)u(i)u(j)m(i,j) −2\nn\nX\ni=1\nb(i)β(i)u(i)\n=\nn\nX\ni,j=1\n(β(i))2u(i)u(j)m(i,j) −2\nn\nX\ni=1\nb(i)β(i)u(i)\n\n532\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nWhen β = 1n, this upper-bound is equal to F(u). So, if we choose β minimizing the\nupper-bound, we will indeed find v such that F(v) ≤F(u). Rewriting the upper-\nbound as\nn\nX\ni=1\nu(i)\n\n(β(i))2\n\n\nn\nX\nj=1\nm(i,j)u(j)\n\n−2b(i)β(i)\n\n\nwe see that β(i) = b(i)/ Pn\nj=1 m(i,j)u(j) provides such a minimizer, which proves the\nfirst statement of the lemma. For the second statement, we have v(i) = u(i) if and\nonly if u(i) = 0 or Pn\nj=1 m(i,j)u(j) = b(i), and one directly checks that these are exactly\nthe KKT conditions for a minimizer of F over vectors with non-negative entries.\n■\nTo apply the lemma to the minimization in A, let M : A 7→AY T Y and b = X Y\n(we are working in the linear space of N by p matrices). Then the update\na(i)\nk 7→a(i)\nk\n(X Y)(i,k)\n(AY T Y)(i.k)\ndecreases the objective function.\nSimilarly, applying the lemma with the operator Y 7→YAT A and b = X T A gives\nthe update for Y, namely\ny(i)\nj\n7→y(i)\nj\n(X T A)(i,j)\n(YAT A)(i,j).\nWe have therefore obtained the following algorithm.\nAlgorithm 20.4 (NMF, quadratic cost)\n1. Fix p > 0 and let X be the N by d matrix containing the observed data. Initialize\nthe procedure with matrices A and Y, respectively of size N by p and d by p, with\npositive coefficients.\n2. At a given stage of the algorithm, let A and Y be the current matrices providing\nan approximate decomposition of X .\n3. For the next step, let ˜\nA be the matrix with coefficients\n˜a(i)\nk = a(i)\nk\n(X Y)(i,k)\n(AY T Y)(i,k)\nand ˜Y the matrix with coefficients\n˜y(i)\nj\n= y(i)\nj\n(X T ˜\nA)(i,j)\n(Y ˜\nAT ˜\nA)(i,j)\n.\n\n20.7. NON-NEGATIVE MATRIX FACTORIZATION\n533\n4. Replace A by ˜\nA and Y by ˜Y, iterating until numerical convergence.\nAn alternative version of the method has been proposed, where the objective\nfunction is Φ(AY T ), where, for an N by d matrix Z = [z1,...,zN]T ,\nΦ(Z) =\nN\nX\nk=1\nd\nX\ni=1\n(z(i)\nk −x(i)\nk logz(i)\nk )\nwhich is indeed minimal for Z = X . We state and prove a second lemma that will\nallow us to address this problem.\nLemma 20.15 Let M be an n by q matrix and x ∈Rn, b ∈Rq, all assumed to have positive\nentries. For u ∈(0,+∞)q, define\nF(u) =\nq\nX\nj=1\nb(j)u(j) −\nn\nX\ni=1\nx(i) log\nq\nX\nj=1\nm(i,j)u(j).\nDefine v ∈(0,+∞)q by\nv(j) = u(j)\n Pn\ni=1 m(i,j)x(i)/α(i)\nb(j)\n!\nwith α(i) = Pq\nk=1 m(i,k)u(k). Then F(v) ≤F(u). Moreover, v = u if and only if u minimizes\nF subject to u(i) ≥0,i = 1,...,n.\nProof Introduce a variable β(j) > 0 for j = 1,...,q an let w(j) = u(j)β(j). Then\nF(w) =\nq\nX\nj=1\nb(j)u(j)β(j) −\nn\nX\ni=1\nx(i) log\nq\nX\nj=1\nm(i,j)u(j)β(j)\n=\nq\nX\nj=1\nb(j)u(j)β(j) −\nn\nX\ni=1\nx(i) log\nPq\nj=1 m(i,j)u(j)β(j)\nPq\nj=1 m(i,j)u(j)\n−\nn\nX\ni=1\nx(i) log\nq\nX\nj=1\nm(i,j)u(j)\nLet ρ(i,j) = m(i,j)u(j)/α(i). Since the logarithm is concave, we have\nlog\nq\nX\nj=1\nρ(i,j)β(j) ≥\nq\nX\nj=1\nρ(i,j)logβ(j)\nso that\nF(w) ≤\nq\nX\nj=1\nb(j)u(()jβ(j) −\nn\nX\ni=1\nq\nX\nj=1\nx(i)ρ(i,j)logβ(j) −\nn\nX\ni=1\nx(i) log\nq\nX\nj=1\nm(i,j)u(j).\n\n534\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nThe upper bound with β(j) ≡1 gives F(u), so minimizing this expression in β will\ngive F(w) ≤F(u). This minimization is straightforward and gives\nβ(j) =\nPn\ni=1 x(i)ρ(i,j)\nb(j)u(j)\n=\nPn\ni=1 m(i,j)x(i)/α(i)\nb(j)\nand the optimal w is the vector v provided in the lemma. Finally, one checks that\nv = u if and only if u satisfies the KKT conditions for the considered problem.\n■\nWe can now apply this lemma to derive update rules for Y and A, where the\nobjective is\nN\nX\nk=1\nd\nX\ni=1\np\nX\nj=1\ny(i)\nj a(j)\nk −\nN\nX\nk=1\nd\nX\ni=1\nx(i)\nk log\np\nX\nj=1\ny(i)\nj a(j)\nk .\nStarting with the minimization in A, we apply the lemma to each index k separately,\ntaking n = d and q = p, with b(j) = Pd\ni=1 y(i)\nj\nand m(i,j) = y(j)\ni . Then the update is\nak(j) 7→ak(j)\nPd\ni=1 x(i)\nk y(i)\nj /α(i)\nk\nPd\ni=1 y(i)\nj\nwith α(i)\nk = Pp\nj=1 y(i)\nj a(j)\nk .\nFor Y, we can work with fixed i and apply the lemma with n = N, q = p, b(j) =\nPN\nk=1 a(j)\nk and m(k,j) = a(j)\nk . This gives the update:\ny(i)\nj\n7→y(i)\nj\nPN\nk=1 x(i)\nk a(j)\nk /α(i)\nk\nPN\nk=1 a(j)\nk\n,\nstill with α(i)\nk = Pp\nj=1 y(i)\nj a(j)\nk .\nWe summarize this in our second algorithm for NMF.\nAlgorithm 20.5 (NMF, logarithmic cost)\n1. Fix p > 0 and let X be the N by d matrix containing the observed data.\n2. Initialize the procedure with matrices Y and A, respectively of size N by p and\nd by p, with positive coefficients.\n3. At a given stage of the algorithm, let A and Y be the current matrices decom-\nposing X .\n\n20.8. VARIATIONAL AUTOENCODERS\n535\n4. Let ˜\nA be the matrix with coefficients\n˜a(j)\nk = a(j)\nk\nPd\ni=1 x(i)\nk y(i)\nj /α(i)\nk\nPd\ni=1 y(i)\nj\nwith α(i)\nk = Pp\nj=1 y(i)\nj a(j)\nk .\n5. Let ˜Y the matrix with coefficients\n˜y(i)\nj\n= y(i)\nj\nPN\nk=1 x(i)\nk ˜a(j)\nk / ˜α(i)\nk\nPp\nj=1 ˜a(j)\nk\nwith ˜α(i)\nk = Pp\nj=1 y(i)\nj ˜a(j)\nk .\n6. Replace A by ˜\nA and Y by ˜Y, iterating until numerical convergence.\n20.8\nVariational Autoencoders\nVariational autoencoders, which were described in section 18.2.2, can be ineter-\npreted as a non-linear factor model in which X = g(θ,Y) + ϵ where ϵ is a centered\nGaussian noise with covariance matrix Q and Y ∈Rp has a known probability dis-\ntribution, such as Y ∼N (0,IdRp). In this framework, the conditional distribution of\nY given X = x was approximated as a Gaussian distribution with mean µ(x,w) and\ncovariance matrix S(x,w)2. The implementation in Kingma and Welling [103, 104]\nuse neural networks for the three functions g, µ and S.\n20.9\nBayesian factor analysis and Poisson point processes\n20.9.1\nA feature selection model\nThe expectation in many factor models is that individual observations are obtained\nby mixing pure categories, or topics, and represented as a weighted sum or linear\ncombination of a small number of uncorrelated or independent variables. Denote p\nthe number of possible categories, which, in this section, can be assumed to be quite\nlarge.\nWe will assume that each observation randomly selects a small number among\nthese categories before combining them. Let us consider (as an example) the follow-\ning model.\n\n536\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\n• The observations X1,...,XN take the form of a probabilistic ICA model\nXk =\np\nX\nj=1\nak(j)bk(j)Y (j) + σRk,\nwhere:\n• Rk follows a standard Gaussian distribution,\n• ak(1),...,ak(p) are independent with ak(j) ∼N (mj,τ2\nj ),\n• bk(1),...,bk(p) are independent and follow a Bernoulli distribution with param-\neter πj,\n• Y (1),...,Y (p) are independent standard Gaussian random variables.\n• σ2 follows an inverse gamma distribution with parameters α0,β0.\n• τ2\n1,...,τ2\np follow independent inverse gamma distributions with parameters α1,β1.\n• mj follow a Gaussian N (0,ρ2) and,\n• πj follow a beta distribution with parameters (u,v).\nThe priors are, as usual, chosen so that the computation of posterior distributions\nis easy, i.e., they are conjugate priors. The observed data is therefore obtained by\nselecting components Yj with probability πj and weighted with a Gaussian random\ncoefficient, then added before introducing noise.\nLet nj = PN\nk=1 bk(j). Ignoring constant factors, the joint likelihood of all variables\ntogether is proportional to:\nL ∝σ−Nd exp\n\n−1\n2σ2\nN\nX\nk=1\n|Xk −\np\nX\nj=1\nak(j)bk(j)Y (j)|2\n\n\np\nY\nj=1\n\nτ−N\nj\nexp\n\n−1\n2τ2\nj\nN\nX\nk=1\n(ak(j) −mj)2\n\n\n\nexp\n\n−1\n2ρ2\np\nX\nj=1\nm2\nj\n\n\np\nY\nj=1\n\u0010\nπ\nnj\nj (1 −πj)N−nj\u0011\np\nY\nj=1\n\u0010\n(τ2\nj )−α1−1 exp(−β1/τ2\nj )\n\u0011\n(σ2)α0−1 exp(−β0/σ2)\np\nY\nj=1\n\u0010\nπu−1\nj\n(1 −πj)v−1\u0011\nexp\n\n−1\n2\np\nX\ni=1\n|Y (i)|2\n\n\nIn spite of the complexity of this expression, it is relatively straightforward (by\nconsidering each variable in isolation) to see that\n\n20.9. BAYESIAN FACTOR ANALYSIS AND POISSON POINT PROCESSES\n537\n• The conditional distribution of σ2,τ2\n1,...,τ2\np given all other variables remains a\nproduct of inverse gamma distributions.\n• The conditional distribution of Y (1),...,Y (p) given the other variables is Gaus-\nsian.\n• The conditional distribution of π1,...,πp given the other variables is a product\nof beta distributions.\n• The conditional distribution of m1,...,mp given the other variables remain in-\ndependent Gaussian.\n• The posterior distribution of a1,...,aN (considered as p-dimensional vectors)\ngiven the other variables is a product of independent Gaussian (but the components\nak(j), j = 1,...,p are correlated).\n• For the posterior distribution given the other variables, b1,...,bN (considered\nas p-dimensional vectors) are independent. The components of each bk are not inde-\npendent but each bk(j) being a binary variable follows a Bernoulli distribution given\nthe other ones.\nThese remarks provide the basis of a Gibbs sampling algorithm for the simulation\nof the posterior distribution of all unobserved variables (the computation of the pa-\nrameters of each of the conditional distribution above requires some work, of course,\nand these details are left to the reader). This simulation does not explicitly provide a\nmatrix factorization of the data (in the sense of a single matrix A such that X = AY,\nas considered in the previous section), but a probability distribution on such matri-\nces, expressed as A(k,j) = ak(j)bk(j). One can however use the average of the matri-\nces obtained through the simulation for this purpose. Additional information can\nbe obtained through this simulation. For example, the expectation of bk(j) provides\na measure of proximity of observation k to category j.\n20.9.2\nNon-negative and count variables\nPoisson factor analysis.\nMany variations can be made on the previous construc-\ntion. When the observations are non-negative, for example, an additive Gaussian\nnoise may not be well adapted. Alternative models should model the conditional\ndistribution of X given a, b and Y as a distribution over non-negative numbers with\nmean (a ⊙b)T Y (for example a gamma distribution with appropriate parameters).\nThe posterior sampling generally is more challenging in this case because simple\nconjugate priors are not always available.\nAn important special case is when X is a count variable taking values in the set of\nnon-negative integers. In this case (starting with a model without feature selection),\nmodeling X as a Poisson variable with mean a(1)Y (1) +···+a(p)Y (p) leads to tractable\ncomputations, once it is noticed that X can be seen as a sum of random variables\n\n538\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nZ[1],...,Z[p] where Z[i] follows a Poisson distribution with parameter a(i)Y (i). This\nsuggests introducing new latent variables (Z[1],...,Z[p]), which are not observed but\nfollow, conditionally to their sum, which is X and is observed, a multinomial distri-\nbution with parameters X,q1,...,qp, with qi = a(i)Y (i)/(Pp\nj=1 a(j)Y (j)).\nThis provides what is referred to as a Poisson factor analysis (PFA). As an exam-\nple, consider a Bayesian approach where, for the prior distribution, a(1),...,a(p) are\nindependent and follow as a gamma distribution with parameters α0 and β0, and\nY (1),...,Y (p) are independent, exponentially distributed with parameter 1. The joint\nlikelihood of all data then is (up to constant factors):\nL ∝exp\n\n−\nN\nX\nk=1\n(ak(1)Y (1) + ··· + ak(p)Y (p))\n\n\n\n\nN\nY\nk=1\np\nY\ni=1\n(ak(i)Y (i))z[i]\nk\nz[i]\nk !\n\n\n\n\nN\nY\nk=1\np\nY\ni=1\nak(i)α−1\n\nexp\n\n−β\nN\nX\nk=1\np\nX\ni=1\nak(i)\n\nexp\n\n−\np\nX\ni=1\nY (i)\n\n.\nThis is the GaP (For Gamma-Poisson) model introduced in Canny [50]. The condi-\ntional distribution of the variables (ak(i)) given (Z[i]\nk ) and (Yi) are independent and\ngamma-distributed, and so are (Y (i)) given the other variables. Finally, for each k, the\nfamily (Z[1]\nk ,...,Z[p]\nk ) follows a multinomial distribution conditionally to their sum,\nXk, and the rest of the variables, and these variables are conditionally independent\nacross k.\nGaP with feature selection\nOne can include a feature selection step in this model\nby introducing binary variables b(1),...,b(p), with selection probabilities π1,...,πp,\nwith a Beta(u,v) prior distribution on πi. Doing so, the likelihood of the extended\nmodel is:\nL ∝exp\n\n−\nN\nX\nk=1\n(ak(1)bk(1)Y (1) + ··· + ak(p)bk(p)Y (p))\n\n\n\n\nN\nY\nk=1\np\nY\ni=1\n(ak(i)bk(i)Y (i))z[i]\nk\nz[i]\nk !\n\n\n\n\nN\nY\nk=1\np\nY\ni=1\nak(i)α−1\n\nexp\n\n−β\nN\nX\nk=1\np\nX\ni=1\nak(i)\n\nexp\n\n−\np\nX\ni=1\nY (i)\n\n\np\nY\nj=1\n\u0010\nπ\nnj\nj (1 −πj)N−nj\u0011\np\nY\nj=1\n\u0010\nπu−1\nj\n(1 −πj)v−1\u0011\n.\nwhere, as before, nj = PN\nk=1 bk(j). The conditional distribution of π1,...,πp given\nthe other variables is therefore still that of a family of independent beta-distributed\nvariables. The binary variables bk(1),...,bk(p) are also conditionally independent\ngiven the other variables, with bk(i) = 1 with probability one if z[i]\nk\n> 0 and with\nprobability πj exp(−ak(j)Y (j)) if z[j]\nk = 0.\n\n20.9. BAYESIAN FACTOR ANALYSIS AND POISSON POINT PROCESSES\n539\n20.9.3\nFeature assignment model\nThe previous models assumed that p features were available, modeled as p random\nvariables with some prior distribution, and that each observation picks a subset of\nthem, drawing feature j with probability πj. We denoted by bk(j) the binary variable\nindicating whether feature j was selected for observation k, and nj was the number\nof times that feature was selected. Finally, we modeled πj as a beta variable with\nparameters u and v.\nOne can compute, using this model, the probability distribution of of the feature\nselection variables, b = (bk(j),j = 1,...,p,k = 1,...,N). From the model definition, the\nprobability of observing such a configuration is given by\nQ(b) = Γ(u + v)p\nΓ(u)pΓ(v)p\nZ\np\nY\nj=1\nπ\nnj+u−1\nj\n(1 −πj)N−nj+v−1dπ1 ...dπp\n=\np\nY\nj=1\nΓ(u + v)Γ(u + nj)Γ(v + N −nj)\nΓ(u)Γ(v)Γ(u + v + N)\n=\np\nY\nj=1\nu(u + 1)···(u + nj −1)v(v + 1)···(v + N −nj −1)\n(u + v)(u + v + 1)···(u + v + N −1)\nDenote by njk = Pk−1\nl=1 bl(j) the number of observations with index less than k that\npick feature j. Using this notation, we can write, using the fact that\nu(u + 1)···(u + nj −1) =\nN\nY\nk=1\n(u + njk)bk(j)\nand a similar identity for v(v + 1)···(v + N −nj −1),\nQ(b) =\np\nY\nj=1\nN\nY\nk=1\n(u + njk)bk(j)(v + k −1 −njk)1−bk(j)\nu + v + k −1\n=\nN\nY\nk=1\np\nY\nj=1\n\u0012\nu + njk\nu + v + k −1\n\u0013bk(j)  v + k −1 −njk\nu + v + k −1\n!1−bk(j)\n.\nUsing this last equation, we can interpret the probability Q as resulting from a pro-\ngressive feature assignment process. The first observation, k = 1, for which njk = 0\nfor all j, chooses each feature with probability u/(u +v). When reaching observation\nk, feature j is chosen with probability (u + njk)/(u + v + k −1). At all steps, features\nare chosen independently from each other.\n\n540\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nLet Fk be the set of features assigned to observation k, i.e., Fk = {j : bk(j) = 1} and\nGk = Fk \\\nk−1\n[\nl=1\nFl\nbe the set of features used in observation k but in no previous observation. Let\nCk = Fk \\ Gk and Uk = G1 ∩··· ∩Gk−1. Instead of considering configurations b =\n(bk(j),i = 1,...,d,k = 1,...,N) we may alternatively consider the family of sets S =\n(Gk,Ck,1 ≤k ≤N). Such a family must satisfy the property that the sets Gk and\nCk are non-intersecting, Ck ⊂Uk and Gl ∩Gk = ∅for l < k. It provide a unique\nconfiguration b by letting bk(j) = 1 if and only if j ∈Gk ∪Ck. We will let, in the\nfollowing, qk = |Gk| and pk = |Uk|. The probability Q(b) can be re-expressed in terms\nof S, letting (with some abuse of notation)\nQ(S) =\nN\nY\nk=1\n\n\n\u0012\nu\nu + v + k −1\n\u0013qk  \nv + k −1\nu + v + k −1\n!p−pk+1\nY\nj∈Uk\n\u0012\nu + njk\nu + v + k −1\n\u00131j∈Ck  v + k −1 −njk\nu + v + k −1\n!1j<Ck \n.\nLet Sk = (Gl,Cl,l ≤k). Then the expression of Q shows that, conditionally to Sk−1, Gk\nand Ck are independent. Elements in Ck are chosen independently for each feature\nj ∈Uk with probability (u +njk)/(u +v +k −1). Moreover, the conditional distribution\nof qk given Sk−1 is proportional to\n\u0012\nu\nu + v + k −1\n\u0013qk  \nv + k −1\nu + v + k −1\n!p−pk−qk\ni.e., it is a binomial distribution with parameters p −pk and u/(u + v + k −1). Finally,\ngiven sk−1 and qk, the distribution of Gk is uniform among all \u0000p−pk\nqk\n\u0001 subsets of\n{1,...,p} \\ (G1 ∪··· ∪Gk−1)\nwith cardinality qk.\nIf there is no special meaning in the feature label, which is the case in our discus-\nsion of prior models in which all features are sampled independently with the same\ndistribution, we may identify configurations that can be deduced from each other by\nrelabeling (note that relabeling features does not change the value of Q).\nCall a configuration normal if Gk = {pk + 1,...,pk+1}. Given S, it is always pos-\nsible to relabel the features with a permutation σ so that, for each k, σ(Gk) = {pk +\n1,...,pk+1}. There are, in fact, q1!...qN! such permutations. We can complete the pro-\ncess generating S by adding at the end a transformation into a normal configuration\n\n20.9. BAYESIAN FACTOR ANALYSIS AND POISSON POINT PROCESSES\n541\n(picking uniformly at random one of the possible ones). The probability of a normal\nconfiguration S obtained through this process is (using a simple counting argument)\nQ(S) =\nN\nY\nk=1\n\n\n p −pk\nqk\n!\u0012\nu\nu + v + k −1\n\u0013qk  \nv + k −1\nu + v + k −1\n!p−pk+1\nY\nj∈Uk\n\u0012\nu + njk\nu + v + k −1\n\u00131j∈Ck  v + k −1 −njk\nu + v + k −1\n!1j<Ck \n,\nThis provides a new incremental procedure that directly samples normalized as-\nsignments. First let q1 follow a binomial distribution bin(p,u/(u + v)) and assign the\nfirst observation to features 1 to q1. Assume that pk labels have been created before\nstep k. Then select for observation k some of the already labeled features, label j\nbeing selected with probability (u + njk)/(u + v + k −1) as above. Finally, add qk new\nfeatures where qk follows a binomial distribution bin(p −pk,u/(u + v + k −1)).\nThis discussion is clearly reminiscent of the one that was made in section 19.7.3\nleading to the Polya urn process, and we want here also to let p tend to infinity\n(with fixed N) with proper choices of u and v as functions of p in the expression\nabove. Choose two positive numbers c and γ and let u = cγ/p and v = c −u. Note\nthat, with the incremental simulation process that we just described, the conditional\nexpectation of the next number of labels, pk+1 given the current one, pk is\nE(pk+1|pk) =\n(p −pk)u\nu + v + k −1 + pk =\ncγ\nc + k −1 +\n \n1 −\ncγ\np(c + k −1)\n!\npk ≤\ncγ\nc + k −1 + pk\nTaking expectations on both sides, we get\nE(pk+1) ≤\nk\nX\nl=1\ncγ\nc + l −1 ≤\nN\nX\nl=1\ncγ\nc + l −1\nso that this expectation is bounded independently of k. This shows in particular\nthat pk/p tends to 0 in probability (just applying Markov’s inequality) and that the\nbinomial distribution bin(p −pk,u/(u + v + k −1)) can be approximated by a Poisson\ndistribution with parameter cγ/(c + k −1).\nSo, when p →∞, we obtain the following incremental simulation process for the\nfeature labels, that we combine with the actual simulation of the features, assumed\nto follow a prior distribution with p.d.f. ψ. This process is called the Indian buffet\nprocess in the literature, the analogy being that a buffet offers an infinite variety of\ndishes, and each observation is a customer who tastes a finite number of them.\n\n542\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nAlgorithm 20.6 (Indian buffet process)\n1. Initialization:\n(i) Sample an integer q1 according to a Poisson distribution with parameter γ.\n(ii) Sample features y(1),...,y(q1) according to ψ.\n(iii) Assign these features to observation 1, and let n2,j = 1 for j = 1,...,q1.\n2. Assume that observations 1 to k−1 have been obtained, with pk features y(1),...,y(pk)\nsuch that the jth feature has been chosen nk,j times.\n(i) For j = 1,...,pk, assign feature j to sample k with probability\nnk,j\nc+k−1. If j is\nselected, let nk+1,j = nk,j + 1, otherwise let nk+1,j = nk,j.\n(ii) Sample an integer qk according to a Poisson distribution with parameter\ncγ\nc+k−1 and let pk+1 = pk + qk.\n(iii) Sample features y(pk+1),...,y(pk+1) according to ψ.\n(iv) Assign these features to observation k, and let nk+1,j = 1 for j = pk+1,...,pk.\n3. If k = N, stop, otherwise replace k by k + 1 and return to Step 2.\n20.10\nPoint processes and random measures\nThis section assumes that the reader is familiar with measure theory. It can however safely\nbe skipped as it is not reused in the rest of the book.\n20.10.1\nPoisson processes\nIf Z is a set, we will denote by Pc(Z) the set composed with all finite or countable\nsubsets of Z. A point process over Z is a random variable S : Ω→Pc(Z), i.e., a\nvariable that provides a countable random subset of Z. If B ⊂Z one can then define\nthe counting function νS(B) = |S ∩B| ∈Z ∪{+∞}.\nA proper definition of such point processes requires some measure theory. Equip\nZ with a σ-algebra A and consider the set N0 of integer-valued measures µ on (Z,A)\nsuch that µ(Z) < ∞. Let N be the set formed with all countable sums of measures\nin N0. Then a general point process is a mapping ν : Ω→N such that for all\nk ∈N ∪{+∞} and all B ∈A, the event {ν(B) = k} is measurable. Recall that, for each\nB ∈A, ν(B) is itself a random variable, that we may denote ω 7→νω(B). One then\ndefine the intensity of the process as the the function µ : B 7→E(ν(B)).\nThe following proposition provides an important identity satisfied by such mod-\nels.\n\n20.10. POINT PROCESSES AND RANDOM MEASURES\n543\nTheorem 20.16 (Campbell identity) Let ν be a point process with intensity µ. For\nω ∈Ω, let Xω : Ω′ →Z be a random variable with distribution νω (defined, if needed, on\na different probability space (Ω′,P′)). Then, for any µ-integrable function f :\nE(f (X)) =\nZ\nZ\nf (z)dµ(z).\n(20.22)\nHere, the expectation of f (X) is over both spaces Ωand Ω′ and corresponds to the\naverage of f . The identity is an immediate consequence of Fubini’s theorem.\nWe will be mainly interested in the family of Poisson point processes. These\nprocesses are themselves parametrized by a measure, say µ, on Z such that µ is σ-\nfinite and µ(B) = 0 if B is a singleton. A Poisson process with intensity measure µ is\na point process ν such that:\n(i) If B1,...,Bn are non-intersecting pairwise, then ν(B1),...,ν(Bn) are mutually in-\ndependent.\n(ii) for all B, ν(B) ∼Poisson(µ(B)).\nWe take the convention that ν(B) = 0 (resp. = ∞) almost surely if µ(B) = 0 (resp.\n= ∞). Note that property (i) also implies that if g1,...,gn are measurable functions\nfrom Z to (0,+∞) such that gigj = 0 for i , j, then the variables ν(gi) =\nR\nZ gi(z)dν(z)\nare independent.\nIf µ(Z) < ∞(i.e., µ is finite), one can represent the distribution of a Poisson point\nprocess as follows:\nν =\nν(Z)\nX\nk=1\nδXk\nwith ν(Z) ∼Poisson(µ(Z)) and, conditional to ν(Z) = N, X1,...,XN are i.i.d. and fol-\nlow the probability distribution ¯µ = µ/µ(Z). This measure can also be identified with\nthe random set S = {X1,...,Xν(Z)}. The assumption that µ({z}) = 0 for any singleton\nimplies that ν({z}) = 0 almost surely. It also ensures that the points X1,...,XN are\ndistinct with probability one.\nIf µ is σ-finite, then (by definition), it is a countable sum of finite measures\nµ1,µ2,.... Then ν can be generated as the sum of independent ν1,ν2,..., where νi\nis a Poisson process with intensity µi. It can moreover be identified with the count-\nable random set S = S∞\ni=1 Si, where Si is the random set associated with νi. Note that,\nin this construction, one can always assume that the measures µ1,µ2,... are mutually\nsingular (i.e., µi(B) > 0 for some i implies that µj(B) = 0 for j , i).\n\n544\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nIf we consider a Poisson process on (0,+∞) × Z, we can define weighted random\nmeasures. Indeed, such a point process takes values in the collection of all sets of the\nform {(wk,zk),k ∈I} where I is finite or countable. These subsets can be represented\nas the sum of weighted Dirac masses,\nξ =\nX\nk∈I\nwkδzk .\nTo ensure that the points (zk,k ∈I) generated by this process are all different, we need\nto assume that the intensity µ of this random process is such that µ((0,+∞) × {z}) = 0\nfor all z ∈Z. We will refer to ξ as a weighted Poisson process.\nIn the following, we will consider this class of random measures, with the small\naddition of allowing for an extra term including a measure supported by a fixed set.\nMore precisely, given a (deterministic) countable subset I ⊂Z, a family of indepen-\ndent random variables (ρz,z ∈I) and a σ-finite measure µo such that µo((0,+∞) ×\n{z}) = 0 for all z ∈Z, we can define the random measure\nξ = ξf + ξo\nwhere ξo is a weighted Poisson process with intensity µo, assumed independent of\n(ρz,z ∈I) and\nξf =\nX\nz∈I\nρzδz.\nThe subscripts o and f come from the terminology introduced in Kingman [105],\nwhich studies “completely random measures,” which are a random measures that\nsatisfy point (i) in the definition of a Poisson process. Under mild assumptions, such\nmeasures can be decomposed as a sum of a weighted Poisson process (here, ξo, the\nordinary part), of a process with fixed support, (here, ξf , the fixed part) and of a\ndeterministic measure (which is here taken to be 0).\nLet us rapidly check that ξ satisfies property (i). Let B1,...,Bn be non-overlapping\nelements of A. Get gi(w,z) = w1Bi(z). Then\nξ(Bi) = ξf (Bi) + νo(gi)\nwhere νo is a Poisson process with intensity µo. Since the sets do not overlap, the\nvariables (ξf (Bi),i = 1,...,n) are independent, and so are (νo(gi),i = 1,...,n) since\ngigj = 0 for i , j. Since ξf and νo are, in addition independent, we see that (ξ(Bi),i =\n1,...,n) are independent.\nThe intensity measure of such a process is still defined by\nη(B) = E(ξ(B)) =\nX\nz∈I\nP((ρz,z) ∈B) +\nZ\n(0,+∞)×B\nwdµo(w,z)\nwhere the last term is an application of Campbell’s inequality to the Poisson process\nνo and the function g(w,x) = w1B(x).\n\n20.10. POINT PROCESSES AND RANDOM MEASURES\n545\n20.10.2\nThe gamma process\nThe main example of such processes in factor analysis is the beta process that will be\ndiscussed in the next section. We start, however, with a first example that is closely\nrelated with the Dirichlet process, called the gamma process.\nIn this process, one fixes a finite measure π0 on Z and defines µ on (0,+∞)×Z by\nµ(dw,dz) = cw−1e−cwπ0(dz)dw.\nBecause µ is σ-finite but not finite (the integral over t diverges at t = 0), every real-\nization of ξ is an infinite sum\nξ =\n∞\nX\nk=1\nwkδzk.\nThe intensity measure of ξ is\nη(B) = cπ0(B)\nZ +∞\n0\ne−cwdw = π0(B).\nIn particular,\n∞\nX\nk=1\nwk = η(Z) = π0(Z) < ∞.\nFor fixed B, the variable ξ(B) follows a Gamma distribution. This can be proved\nby computing the Laplace transform of ξ, E(e−λξ(B)), and identify it to that of a\nGamma. To make this computation, consider the point process νJ restricted to a\ninterval J ⊂(0,+∞) with min(J) > 0, and ξJ the corresponding weighted process.\nLet mJ(t) =\nR\nJ w−1ce−(c+t)w dw. Then a realization of νJ can be obtained by first sam-\npling N from a Poisson distribution with parameter µ(J × Z) = mJ(0)π0(Z) and then\nsampling N points (wi,zi) independently from the distribution µ/(mJ(0)π0(Z)). This\nimplies that\nE(e−tξJ(B)) =\n∞\nX\nn=0\ne−mJ(0)π0(Z)(mJ(0)π0(Z))n\nn!\n\n\nR ∞\n0 e−tw1B(z)w−1ce−cwdwdπ0(z)\nmJ(0)π0(Z)\n\n\nn\n=\n∞\nX\nn=0\ne−mJ(0)π0(Z)\nn!\n\u0010\nπ0(B)mJ(t) + (π0(Z) −π0(B))mJ(0)\n\u0011n\n= eπ0(B)(mJ(t)−mJ(0)) .\nNow,\nmJ(t) −mJ(0) = c\nZ\nJ\necw e−tw −1\nw\ndw\n\n546\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nis finite even when J = (0,+∞). With a little more work justifying passing to the\nlimit, one finds that, for J = (0,+∞),\nE(e−tξJ(B)) = exp\n \nπ0(B)\nZ +∞\n0\ne−cw e−tw −1\nw\ndw\n!\n.\nFinally, write\nc\nZ +∞\n0\ne−cw e−tw −1\nw\ndw = −c\nZ +∞\n0\ne−cw\nZ t\n0\ne−swdsdw\n= −c\nZ t\n0\nZ +∞\n0\ne−(s+c)wdwds\n= −\nZ t\n0\nc(s + c)−1ds = −clog(1 + t\nc).\nThis shows that\nE(e−tξJ(B)) =\n\u0012\n1 + t\nc\n\u0013−cπ0(B)\nwhich is the Laplace transform of a Gamma distribution with parameters cπ0(B) and\nc, i.e., with density proportional to wcπ0(B)−1e−cw.\nAs a consequence, the normalized process δ = ξ/ξ(Z) is a Dirichlet process with\nintensity cπ0. Indeed, if B1,...,Bn is a partition of Z the family (δ(B1),...,δ(Bn)) is\nthe ratio of n independent gamma variables to their sum, which provides a Dirichlet\ndistribution, and this property characterizes Dirichlet processes.\n20.10.3\nThe beta process\nThe definition of the beta process parallels that of the gamma process, with weights\ntaking this time values in (0,1). Fix again a finite measure π0 on Z and let µo on\n(0,+∞) × Z be defined by\nµo(dw,dz) = cw−1(1 −w)c−1π0(dz)dw.\nThe associated weighted Poisson process can therefore be represented as a sum\nξo =\n∞\nX\nk=1\nwkδzk,\nand its intensity measure is\nηo(B) = cπ0(B)\nZ 1\n0\n(1 −t)c−1dw = π0(B).\n\n20.10. POINT PROCESSES AND RANDOM MEASURES\n547\nIn particular, since π0 is finite, we have P∞\nk=1 wk < ∞almost surely. A beta process is\nthe sum of the process ξo and of a fixed set process\nξf =\nX\nz∈I\nwzδz\nwhere I is a fixed finite set and (wz,z ∈I) are independent and follow a beta distri-\nbution with parameters (a(z),b(z)).\nIf Z is a space of features, such a process provides a prior distribution on fea-\nture selections. It indeed provides, in addition to the deterministic set I, a random\ncountable set J ⊂Z, with a set of random weights wz,z ∈F := I ∪J . Given this,\none defines the feature process as the selection of a subset A ⊂F where each feature\nz is selected with probability wz. Because E(|A|) = P\nz∈F wz is finite, A is finite with\nprobability 1.\nIn the same way the Polya urn could be used to sample from a realization of a\nDirichlet process without actually sampling the whole process, there exists an algo-\nrithm that samples a sequence of feature sets (A1,...,An) from this feature selection\nprocess without needing the infinite collection of weights and features associated\nwith a beta process. We assume in the following that the prior process has an empty\nfixed set. (Non-empty fixed sets will appear in the posterior.)\nThe first set of features, A1, is obtained as follows according to a Poisson process\nwith intensity π0: choose the number N of features in A1 according to a Poisson dis-\ntribution with parameter π0(Z). Then sample N features independently according\nto the distribution π0/π0(Z).\nNow assume that n−1 sets of features A1,...,An have been obtained and we want\nto sample a new set An+1 conditionally to their observation. Let Jn be the union of\nall random features obtained up to this point and n(z), for z ∈Jn the number of times\nthis feature was observed in A1,...,An. Then the conditional distribution of the beta\nprocess ξ given this observation is still a beta process, with fixed set given by I = Jn,\n(a(z),b(z)) = (n(z),c + n −n(z)) for z ∈Jn−1 and base measure πn = cπ0/(c + n). This\nimplies that the next set An+1 can be obtained by sampling from the associated fea-\nture process. To do this, one first selects features z ∈Jn with probability n(z)/(c + n),\nthen selects additional features z1,...,zN independently with distribution π0/π0(Z)\nwhere N follows a Poisson distribution with parameter cπ0(Z)/(c + n). This is the\nIndian buffet process, described in Algorithm 20.6 (taking π0 = γψ).\n20.10.4\nBeta Process and feature selection\nThe beta process can be used as a prior for feature selection within a factor analysis\nmodel, as described in the previous paragraph. It is however easier to approximate\n\n548\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nit with a model with almost surely finite support. Indeed, letting, for ϵ > 0\nµ(dw,dz) =\nΓ(c + 1)\nΓ(ϵ + 1)Γ(c −ϵ)wϵ−1(1 −w)c−ϵπ0(dz)dw,\none obtains a finite measure since\nZ +∞\n0\nZ\nZ\nµo(dw,dz) = cγ\nϵ\nwhere γ = π0(Z). Note that µ is normalized so that E(ξ(B)) = π0(B) for B ⊂Z.\nIn this case, the prior generates features by first sampling their number, p, ran-\ndomly according to a Poisson distribution with mean cγ/ϵ, then select p probabilities\nw1,...,wp independently using a beta distribution with parameters ϵ and c −ϵ, and\nfinally attach to each i a feature zi with distribution π0/γ. The features associated\nwith a given sample are then obtained by selecting each zi with probability wi.\nWe note also that the model described in section 20.9.3 provides an approxima-\ntion of this prior using a finite number of features. With our notation here, this\ncorresponds to taking p ≫1 and ϵ = cγ/p.\n\nChapter 21\nData Visualization and Manifold Learning\n21.1\nMultidimensional scaling\nThe methods described in this chapter aim at representing a dataset in low dimen-\nsion, allowing for its visual exploration by summarizing its structure in a user-\naccessible interface. Unlike factor analysis methods, they do not necessarily attempt\nat providing a causal model expressing the data as a function of a small number of\nsources, and generally do not provide a direct mechanism for adding new data to the\nrepresentation. In addition, all these methods take as input similarity dissimilarity\nmatrices between data points and do not require, say, Euclidean coordinates.\nAssuming that a dissimilarity matrix D = (dkl, k,l = 1,...,N) is given, the goals of\nmultidimensional scaling (or MDS) is to determine a small-dimensional Euclidean\nrepresentation, say y1,...,yN ∈Rp, such that\n\f\f\fyk −yl\n\f\f\f2 ≃d2\nkl. We review below two\nversions of this algorithm, referred to as “similarity” and “dissimilarity” matching.\n21.1.1\nSimilarity matching (Euclidean case)\nWe start with the standard hypotheses of MDS, assuming that the distances dkl de-\nrive from a representation in feature space, so that d2\nkl = ∥hk −hl∥2\nH for some inner-\nproduct space H and (possibly unknown) features h1,...,hN. Note that, since the\nEuclidean distance is invariant by translation, there is no loss of generality in as-\nsuming h1 + ··· + hN = 0, which will be done in the following.\nWe look for a p-dimensional representation in the form yk = Φhk where Φ is a lin-\near transformation (and we want yk to be computable directly from dissimilarities,\nsince we do not assume that hk is known). Since we are only interested in a trans-\nformation of the h1,...,hN, it suffices to compute Φ in the vector space generated by\n549\n\n550\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nthem, so that we let\nΦ : span(h1,...,hN) →Rp ,\nand we want Φ to (approximately) conserve the norm, i.e., be close to being an isom-\netry.\nBecause isometries are one-to-one and onto, the existence of an exact isometry\nwould require V\n∆= span(h1,...,hN) to be p-dimensional. The mapping Φ could then\nbe defined as Φ(h) = (⟨h , e1⟩H,...,⟨h , ep⟩H) where e1,...,ep is any orthonormal basis\nof V . In the general case, however, where V is not p-dimensional or less, one can\nreplace it by a best p-dimensional approximation of the training data, leading to a\nproblem similar to PCA in feature space.\nIndeed, as we have seen in section 20.1.2, this best approximation can be ob-\ntained by diagonalizing the Gram matrix S of h1,...,hN, which is such that skl =\n⟨hk , hl⟩H. (Recall that we assume that ¯h = 0, so we do not center the data here.) Us-\ning the notation in section 20.1.2, let α(1),...,α(p) denote the eigenvectors associated\nwith the p largest eigenvalues, normalized so that (α(i))T Sα(i) = 1 for i = 1,...,p. One\ncan then take\nei =\nN\nX\nl=1\nα(i)\nl hl\nand, for k = 1,...,N, j = 1,...,p:\ny(i)\nk = λ2\ni α(i)\nk\n(21.1)\nwhere λ2\ni is the eigenvalue associated with α(i).\nThis does not entirely address the original problem, since the inner products skl\nare not given, but only the distances dkl, which satisfy\nd2\nkl = −2skl + skk + sll .\n(21.2)\nThis provides a linear system of equations in the unknown skl. This system is under-\ndetermined, because D is invariant by any transformation hk 7→hk + h0 (for a fixed\nh0), and S is not. However, the assumption h1 + ··· + hN = 0 provides the additional\nconstraint needed to provide a unique solution.\nSumming (21.2) over l, we then get\nN\nX\nl=1\nd2\nkl = Nskk +\nN\nX\nl=1\nsll .\n(21.3)\nSumming this equation over k, we find\nN\nX\nk,l=1\nd2\nkl = 2N\nN\nX\nl=1\nsll.\n\n21.1. MULTIDIMENSIONAL SCALING\n551\nUsing this in (21.3), we get\nskk = 1\nN\nN\nX\nl=1\nd2\nkl −\n1\n2N 2\nN\nX\nk,l=1\nd2\nkl,\nand, from (21.2)\nskl = −1\n2\n\nd2\nkl −1\nN\nN\nX\nk′=1\nd2\nk′l −1\nN\nN\nX\nl′=1\nd2\nkl′ + 1\nN 2\nN\nX\nk′,l′=1\nd2\nk′l′\n\n.\nIf we denote by D⊙2 the matrix formed with the squared distances d2\nkl, this identity\ncan we rewritten in the simpler form\nS = −1\n2PD⊙2P\n(21.4)\nwith P = IdRN −1N1T\nN/N.\nWe now show that this PCA approach to MDS is equivalent to the problem of\nminimizing\nF(y) =\nN\nX\nk,l=1\n(yT\nk yl −skl)2\n(21.5)\nover all y1,...,yN ∈Rp such that y1+···+yN = 0, which can be interpreted as matching\n“similarities” skl rather than distances. Indeed, letting Y denote the N by p matrix\nwith rows yT\n1 ,...,yT\nN, we have\nF(y) = trace((YY T −S)2).\nFinding Y is equivalent to finding a symmetric matrix M of rank p minimizing\ntrace((M −S)2). We have, using the trace inequality (theorem 2.1), and letting λ2\n1 ≥\n··· ≥λ2\nN (resp. µ2\n1 ≥··· ≥µ2\np) denote the eigenvalues of S (resp. M)\ntrace((M −S)2) = trace(M2) −2trace(MS) + trace(S2)\n=\np\nX\nk=1\nµ4\nk −2trace(MS) +\nN\nX\nk=1\nλ4\nk\n≥\np\nX\nk=1\nµ4\nk −2\np\nX\nk=1\nλ2\nkµ2\nk +\nN\nX\nk=1\nλ2\nk\n=\np\nX\nk=1\n(λ2\nk −µ2\nk)2 +\nN\nX\nk=p+1\nλ4\nk\n≥\nN\nX\nk=p+1\nλ4\nk\n\n552\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nThis lower bound is attained when M and S can be diagonalized in the same or-\nthonormal basis with λ2\nk = µ2\nk for k = 1,...,p. So, letting S = UDUT , where U is\northogonal and D is diagonal with decreasing numbers on the diagonal, an optimal\nM is given by M = UpDpUT\np , where Up is formed with the first p columns of A and Dp\nis the first p × p block of D. This shows that the matrix Y = UpD1/2 provides a min-\nimizer of F. The matrix U = [u(1),...,u(N)] differs from the matrix A = [α(1),...,α(N)]\nabove through the normalization of its column vectors: we have Sα(i) = λ2\ni α(i) with\n(α(i))T Sα(i) = 1 while Su(i) = λ2\ni u(i) with (u(i))T Su(i) = λ2\ni showing that α(i) = λ−1\ni u(i).\nThis shows that Ap = UpD−1/2\np\nso that Y can also be rewritten as Y = ApDp, i.e.,\ny(i)\nk = λ2\ni α(i)\nk , the same expression that was obtained before.\nThe minimization of F is called similarity matching. Clearly, this method can\nbe applied when one starts directly with a matrix of dissimilarities S, provided it\nsatisfies PN\nl=1 skl = 0 for all k. If this is not the case, then interpreting skl as an inner\nproduct hT\nk hl, it is natural to replace skl by what would give (hk −¯h)T (hl −¯h), namely,\nby\ns′\nkl = skl −1\nN\nN\nX\nl′=1\nskl′ −1\nN\nN\nX\nk′=1\nsk′l + 1\nN 2\nN\nX\nk′,l′=1\nsk′l′.\nInterestingly, this discussion provides us with yet another interpretation of PCA.\n21.1.2\nDissimilarity matching\nWhile the minimization of (21.5) did not provide us with a new way of analyzing the\ndata (since it was equivalent to PCA), the direct comparison of dissimilarities, that\nis, the minimization of\nG(y) =\nN\nX\nk,l=1\n(|yk −yl| −dkl)2\nover y1,...,yN ∈Rp, provides a different approach. Since this may be useful in prac-\ntice and does not bring in much additional difficulty, we will allow for the possibility\nof weighting the differences in G and consider the minimization of\nG(y) =\nN\nX\nk,l=1\nwkl(|yk −yl| −dkl)2\nwhere W = (wkl) is a symmetric matrix of non-negative weights. The only additional\ncomplexity resulting by adding weights is that the indeterminacy on y1,...,yN is that\nG(y) = G(y′) as soon as y −y′ is constant on every connected component of the graph\nassociated with the weight matrix W, so that the constraint on y should be replaced\nby\nX\nk∈Γ\nyk = 0\n\n21.1. MULTIDIMENSIONAL SCALING\n553\nfor any connected component Γ of this graph. (If all weights are positive, then the\nonly non-empty connected component is {1,...,N} and we retrieve our previous con-\nstraint PN\nk=1 yk = 0.)\nStandard nonlinear optimization methods, such as projected gradient descent,\nmay be used to minimize G, but the preferred algorithm for MDS uses a stepwise\nprocedure resulting from the addition of an auxiliary variable. Rewrite\nG(y) =\nN\nX\nk,l=1\nwkl|yk −yl|2 −2\nN\nX\nk,l=1\nwkldkl|yk −yl| +\nN\nX\nk,l=1\nd2\nkl .\nWe have, for u ∈Rp:\n|u| = max{zT u : z ∈Rp,|z| = 1u,0}.\nUsing this identity, we can introduce auxiliary variables zkl, k,l = 1,...,N in Rp, with\n|zkl| = 1 if yk , yl and define\nˆG(y,z) =\nN\nX\nk,l=1\nwkl|yk −yl|2 −2\nN\nX\nk,l=1\nwkldkl(yk −yl)T zkl +\nN\nX\nk,l=1\nd2\nkl .\nWe then have\nG(y) =\nmin\nz:|zkl|=1 if yk,yk\nˆG(y,z).\nAs a consequence, minimizing G in y can be achieved by minimizing ˆG in y and\nz and discarding z when this is done. One can minimize ˆG iteratively, alternating\nminimization in y given z and in z given y, both steps being elementary. In order to\ndescribe these steps, introduce some matrix notation.\nLet L denote the Laplacian matrix of the weighted graph on {1,...,N} associated\nwith the weight matrix W, namely L = (ℓkl,k,l = 1,...,N) with ℓkk = PN\nk=1 wkl −wkk\nand ℓkl = −wkl when k , l. Then,\nN\nX\nk,l=1\nwkl|yk −yl|2 = 2trace(Y T LY).\nDefining uk ∈Rp by\nuk =\nN\nX\nl=1\nwkldkl(zkl −zlk),\nand U =\n\n\nuT\n1...\nuT\nN\n\n\n, we have\nN\nX\nk,l=1\nwkldkl(yk −yl)T zkl = trace(U T Y).\n\n554\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nWith this notation, the optimal matrix Y must minimize\n2trace(Y T LY) −2trace(U T Y).\nLet m be the number of connected components of the weighted graph. Recall that\nthe matrix L is positive semi-definite and that an orthonormal basis of its null space\nis provided by vectors, say e1,...,em, that are constant on each of the m connected\ncomponents of the graph, so that the constraint on Y can be written as eT\nj Y = 0 for\nj = 1,...,m. Introduce the matrix\nˆL = L +\nm\nX\nk=1\nekeT\nk\nwhich is positive definite. Our minimization problem is then equivalent to minimiz-\ning\n2trace(Y T ˆLY) −2trace(U T Y),\nsubject to eT\nj Y = 0 for j = 1,...,m. The derivative of this function is\n4ˆLY −2U\nso that an optimal Y must satisfy\n4ˆLY −2U +\nm\nX\nj=1\nejµT\nj = 0\nfor Lagrange multipliers µ1,...,µm ∈Rp. This shows that\nY = 1\n2\nˆL−1\u0012\nU −1\n2\nm\nX\nj=1\nejµT\nj\n\u0013\n= 1\n2\nˆL−1U −1\n4\nm\nX\nj=1\nejµT\nj\nwhere we have used the fact that ˆL−1ej = ej. We can now identify µj since\n0 = eT\nj Y = 1\n2eT\nj ˆL−1U −1\n4\nm\nX\nj′=1\neT\nj ej′µT\nj = 1\n2eT\nj U −1\n4µT\nj\nso that µT\nj = 2eT\nj U and the optimal Y is\nY = 1\n2\nˆL−1U −1\n2\nm\nX\nj=1\nejeT\nj U.\n\n21.2. MANIFOLD LEARNING\n555\nNote that this expression can be rewritten as\nY = 1\n2PL ˆL−1U\nwhere PL = IdRN −PN\nk=1 ejeT\nj is the projection onto the space perpendicular to the null\nspace of L (i.e., the range of L). In the case where the graph has a single connected\ncomponent, one has m = 1 and e1 = 1N/\n√\nN yielding\nPL = IdRN −1\nN 1N1T\nN.\nThe minimization in z given y is straightforward: if yk , yk, then zkl = (yk −\nyl)/|yk −yl|. If yk = yl, then one can take any value for zkl and the simplest if of course\nzkl = 0. Using the previous computation, we can summarize a training algorithm for\nmulti-dimensional scaling, called SMACOF for “Scaling by Maximizing a Convex\nFunction” (see, e.g., Borg and Groenen [36] for more details and references).\nAlgorithm 21.1 (SMACOF)\nAssume that a symmetric matrix of dissimilarities (dkl,k,l = 1,...,N) is given, to-\ngether with a matrix of weights (wkl,k,l = 1,...,N). Fix a target dimension, p. Fix a\ntolerance constant ϵ.\n1. Compute the Laplacian matrix L of the graph associated with the weights, the\nprojection matrix PL onto the range of L and the matrix M = (L + IdRN −PL)−1.\n2. Initialize the algorithm with some family y1,...,yN ∈Rp and let Y =\n\n\nyT\n1...\nyT\nN\n\n\n.\n3. At a given step of the algorithm, let Y be the current solution and compute, for\nk = 1,...,N:\nuk = 2\nN\nX\nl=1\nwkldkl\nyk −yl\n|yk −yl|1yk,yl\nto form the matrix U =\n\n\nuT\n1...\nuT\nN\n\n\n.\n4. Compute Y ′ = 1\n2PLMU.\n5. If |Y −Y ′| ≤ϵ, exit and return Y ′.\n6. Return to step 3.\n\n556\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nFigure 21.1:\nLeft: Multidimensional scaling applied to a 3D curve embedded in a 10-\ndimensional space retrieves the Euclidean structure. Right: Isomap, in contrasts, identifies\nthe one-dimensional nature of the data.\n21.2\nManifold learning\nThe goal of MDS is to map a full matrix of distances into a low-dimensional Eu-\nclidean space. Such a representation, however, cannot address the possibility that\nthe data is supported by a low-dimensional, albeit nonlinear, space. For example,\npeople leaving on Earth live, for all purposes, on a two-dimensional structure (a\nsphere), but any faithful Euclidean representation of the world population needs\nto use the three spatial dimensions. One may also argue that the relevant distance\nbetween points on Earth is not the Euclidean one either (because one would never\ntravel through Earth to go from one place to another), but the distance associated to\nthe shortest path on the sphere, which is measured along great circles.\nTo take another example, the left panel in fig. 21.1 provides the result of applying\nMDS to a ten-dimensional dataset obtained by applying a random ten-dimensional\nrotation to a curve supported by a three-dimensional torus. MDS indeed retrieves\nthe correct curve structure in space, which is three dimensional. However, for a\nperson “living” on the curve, the data is one-dimensional, a fact that is captured by\nthe Isomap method that we now describe.\n21.2.1\nIsomap\nLet us return to the example of people living on the spherical Earth. One can de-\nfine the distance between two points on Earth either as the shortest length a person\nwould have to travel (say, by plane) to go from one point to the other (that we can\ncall the intrinsic distance), or simply the chordal distance in 3D space between the two\npoints. The first one is obviously the most relevant to the spherical structure of the\nEarth, but the second one is easier to compute given the locations of the points in\n\n21.2. MANIFOLD LEARNING\n557\nspace.\nFor typical datasets, the geometric structure of the data (e.g., that it is supported\nby a sphere) is unknown, and the only information that is available is their chordal\ndistance in an ambient space (which can be very large). An important remark, how-\never is that, when the points are close to each other, the two distances can be ex-\npected to be similar, if we assume that the geometry of the set supporting the data is\nlocally linear (e.g., that it is, like the sphere, a “submanifold” of the ambient space,\nwith small neighborhoods of any data point well approximated, at first order, by\npoints on a tangent space). Isomap uses this property, only trusting small distances\nin the matrix D, and infers large distances by adding the costs resulting from travel-\ning from data points to nearby data points.\nFix an integer c. Given D, the c-nearest neighbor graph on V = {1,...,N} places an\nedge between k and l if and only if dk,l is among the c smallest values in {dkl′,l′ , k}\nneighbors or xl among the c smallest values in {dk′l,k′ , l}. We will write k ∼c l to\nindicate that there exists an edge between k and l in this graph. One then defines\nthe geodesic distance on the graph as\nd(∗)\nkl = min\n\nm\nX\nj=1\ndkj−1kj : k0,...,km ∈{1,...,N},k0 = k ∼c k1 ∼c ··· ∼c km−1 ∼c km = l,m ≥0\n\n.\nThis geodesic distance can be computed incrementally as follows. First define\nd(1)\nkl = |xk −xl| if k ∼c l and d(1)\nkl = +∞otherwise (and also let d(1)\nkk = 0). Then, given\nd(n−1), define\nd(n)\nkl = min\n\u001a\nd(n−1)\nkl′\n+ d(1)\nll′ l′ = 1,...,N\n\u001b\nuntil the entries stabilize, i.e., d(n+1) = d(n), in which case one has d(∗) = d(n). The\nvalidity of the statement can be easily proved by checking that\nd(n)\nkl = min\n\nn\nX\nj=1\nd(1)\nkj−1kj : k0,...,kn ∈{1,...,N},k0 = k,kn = l\n\n,\nwhich can be done by induction, the details being left to the reader. It should also\nbe clear that the procedure will stabilize after no more than N steps.\nOnce the distance is computed, Isomap then applies standard MDS, resulting\nin a straightened representation of the data like in fig. 21.1. Another example is\nprovided in fig. 21.2, where, this time, the input curve is closed and cannot therefore\nbe represented as a one-dimensional structure. One can note, however, that, even in\nthis case, Isomap still provides some simplification of the initial shape of the data.\n\n558\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nFigure 21.2:\nLeft: Multidimensional scaling applied to a 3D curve embedded in a 10-\ndimensional space retrieves the Euclidean structure. Right: Isomap, in contrasts, identifies\nthe one-dimensional nature of the data.\n21.2.2\nLocal Linear Embedding\nLocal linear embedding (LLE) exploits in a different way the fact that manifolds are\nlocally well approximated by linear spaces. Like Isomap, it starts also with build-\ning a c-nearest-neighbor graph on {1,...,k}. Assume, for the sake of the discussion,\nthat the distance matrix is computed for possibly unobserved data T = (x1,...,xN).\nLetting Nk denote the indices of the nearest neighbors of k (excluding k itself), the\nbasic assumption is that xk should approximately lie in the affine space generated by\nxl,l ∈Nk. Expressed in barycentric coordinates, this space is defined by\nTk =\n\nX\nl∈Nk\nρ(l)xl : ρ ∈RNk,\nX\nl∈Nk\nρ(l) = 1\n\n,\nand Tk can be interpreted as an approximation of the tangent space at xk to the data\nmanifold. Optimal coefficients (ρ()kl,k = 1,...,N,l ∈Nk) providing the representa-\ntion of xk in that space can be estimated by minimizing, for all k\n\f\f\f\f\f\f\f\f\nxk −\nX\nl∈Nk\nρ(l)\nk xl\n\f\f\f\f\f\f\f\f\n2\nsubject to P\nl∈Nk ρ(l)\nk = 1. This is a simple least-square program. Let ck = |Nk| (ck = c\nin the absence of ties). Order the elements of Nk to represent ρ(l)\nk ,l ∈Nk as a vector\ndenoted ρk ∈Rck. Similarly, let Sk be the Gram matrix associated with xl,l ∈Nk\nformed with all inner products xT\nl′ xl, l,l′ = 1,...,N and let rk be the vector composed\nwith products xT\nk xl,l ∈Nk. Assume that Sk is invertible, which is generally true if\n\n21.2. MANIFOLD LEARNING\n559\nc < d, unless the neighbors are exactly linearly aligned. Then, the optimal ρk and the\nLagrange multiplier λ for the constraint are given by\n \nρk\nλ\n!\n=\n \nSk\n1ck\n1T\nck\n0\n!−1  \nrk\n1\n!\n.\n(21.6)\nIf Sk is not invertible, the problem is under-constrained and one of its solutions can\nbe obtained by replacing the inverse above by a pseudo-inverse.\nThe low-dimensional representation of the data, still denoted (y1,...,yN) with\nyk ∈Rp is then estimated so that the relative position of yk to its neighbors is the\nsame as that of xk, i.e., so that\nyk ≃\nX\nl∈Nk\nρ(l)\nk yl.\nThese vectors are estimated by minimizing\nF(y) =\nN\nX\nk=1\n\f\f\f\f\f\f\f\f\nyk −\nX\nl∈Nk\nρ(l)\nk yl\n\f\f\f\f\f\f\f\f\n2\n.\nObviously, some additional constraints are needed to avoid the trivial solution yk = 0\nfor all k. Also, replacing all yk’s by y′\nk = Ryk +b where R is an orthogonal transforma-\ntion in Rp and b is a translation does not change the value of F, so there is no loss of\ngenerality in assuming that PN\nk=1 yk = 0 and that PN\nk=1 ykyT\nk = D0, a diagonal matrix.\nHowever, if one lets y′\nk = Dyk where D is diagonal, then\nF(y) =\np\nX\ni=1\nD2\nii\nN\nX\nk=1\n\ny(i)\nk −\nX\nl∈Nk\nρ(l)\nk y(i)\nl\n\n\n2\n.\nThis shows that one should not allow the diagonal coefficients of D0 to be chosen\nfreely, since otherwise the optimal solution would require to take this coefficient to 0.\nSo D0 should be a fixed matrix, and by symmetry, it is natural to take D0 = IdRp. (Any\nother solution—for a different D0—can then be obtained by rescaling independently\nthe coordinates of y1,...,yN.)\nExtend ρ(l)\nk to an N-dimensional vector by taking ρ(k)\nk\n= −1 and ρ(l)\nk = 0 if l , k and\nl < Nk. We can write\nF(y) =\nN\nX\nk=1\n\f\f\f\f\f\f\f\nN\nX\nl=1\nρ(l)\nk yl\n\f\f\f\f\f\f\f\n2\n.\nExpanding the square, this is\nF(y) =\nN\nX\nl,l′=1\nwll′yT\nl yl′\n\n560\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nwith wll′ = PN\nk=1 ρ(l)\nk ρ(l′)\nk . Introducing the matrix W with entries wkl and the N × p\nmatrix Y =\n\n\nyT\n1...\nyT\nN\n\n\n, we have the simple expression\nF(y) = trace(Y T WY).\nNote that the constraints are Y T Y = IdRp and Y T1N = 0. Without this last constraint,\nwe know that an optimal solution is provided by Y = [e1,...,ep] where e1,...,ep pro-\nvide an orthonormal family of eigenvectors associated to the p smallest eigenvalues\nof W (this is a consequence of corollary 2.4). To handle the additional constraint, it\nsuffices to note that W1N = 0, so that 1N is a zero eigenvector. Given this, it suffices\nto compute p + 1 eigenvectors associated to smallest eigenvalues of W, e1,...,ep+1,\nwith the condition that e1 = ±1N/\n√\nN (which is automatically satisfied unless 0 is a\nmultiple eigenvalue of W) and let\nY = [e2,...,ep+1].\nNote that e2,...,ep+1 are also the p smallest eigenvectors of W + λ11T for any large\nenough λ, e.g., λ > trace(W)/N.\nLLE is summarized in the following algorithm.\nAlgorithm 21.2 (Local linear embedding)\nThe input of the algorithm is\n(i) Either a training set T = (x1,...,xN), or its Gram matrix S containing all\ninner products xT\nk xl (or more generally inner products in feature space), or a dissim-\nilarity matrix D = (dkl).\n(ii) An integer c for the graph construction.\n(iii) An integer p for the target dimension.\n(1) If not provided in input, compute the Gram matrix S and distance matrix D\n(using (21.2) and (21.4)).\n(2) Build the c-nearest-neighbor graph associated with the distances. Let Nk be the\nset of neighbors of k, with ck = |Nk|.\n(3) For k = 1,...,N, let Sk be the sub-matrix of S matrix associated with xl,l ∈Nk\nand compute coefficients ρ(l)\nk ,l ∈Nk stacked in a vector ρk ∈Rck by solving (21.6).\n(4) Form the matrix W with entries wll′ = PN\nk=1 ρ(l)\nk ρ(l′)\nk\nwith ρ extended so that ρ(k)\nk\n=\n−1 and ρ(l)\nk = 0 if l , k and l < Nk.\n\n21.2. MANIFOLD LEARNING\n561\nFigure 21.3: Local linear embedding with target dimension 3 applied to the data in fig. 21.1\nand fig. 21.2.\n(5) Compute the first p + 1 eigenvectors, e1,...,ep+1, of W (associated with smallest\neigenvalues) arranging for e1 to be proportional to 1N.\n(6) Set y(i)\nk = e(k)\ni+1 for i = 1,...,p and k = 1,...,N.\nThe results of LLE applied to the datasets described in fig. 21.1 and fig. 21.2 are\nprovided in fig. 21.3.\nRemark 21.1 We note that, for both Isomap and LLE, the c-nearest-neighbors graph\ncan be replaced by the graph formed with edges between all pairs of points that are\nat distance less than ϵ from each other, for a chosen ϵ > 0, with no change in the\nalgorithms.\nThese parameters (c or ϵ) must be chosen carefully and may have an important\nimpact on the output of the algorithm. Choosing them too small would not allow\nfor a correct estimation of distances in Isomap (with possibly some of them being\ninfinite if the graph has more than one connected component), or of the linear ap-\nproximations in LLE. However, choosing them too large may break the basic hypoth-\nesis that the data is locally Euclidean or linear that form the basic principles of these\nalgorithms.\n♦\n21.2.3\nGraph Embedding\nBoth Isomap and LLE are based on the construction of a nearest-neighbor graph\nbased on dissimilarity data and the conservation of some of its geometric features\nwhen deriving a small-dimensional representation. For LLE, a weight matrix W was\nfirst estimated based on optimal linear approximations of xk by its neighbors, and\n\n562\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nthe representation was computed by estimating the eigenvectors associated with the\nsmallest eigenvalues of W (excluding the eigenvector proportional to 1). However,\nboth methods were motivated by the intuition that the dataset was supported by\na continuous small-dimensional manifold. We now discuss methods that are solely\nmotivated by the discrete geometry of a graph, for which we use tools that are similar\nto our discussion of graph clustering in section 19.5.\nAdapting the notation in that section to the present one, we start with a graph\nwith N vertices and weights βkl between these vertices (such that βll = 0) and we\nform the Laplacian operator defined by, for any vector u ∈RN:\n1\n2∥u∥H1 = 1\n2\nN\nX\nl,l′=1\nβll′(u(l) −u(l′))2 = uT Lu,\nso that L is identified as the matrix with coefficients ℓll′ = −βll′ for l , l′ and ℓll =\nPN\nl′=1 βll′. The matrix W that was obtained for LLE coincides with this graph Lapla-\ncian if one lets βll′ = −wll′ for l , l′, since we have PN\nl′=1 wll′ = 0. The usual require-\nment that weights are non-negative is no real loss of generality, because in LLE (and\nin the Graph embedding method above), one is only interested in eigenvectors of W\n(or L below) that are perpendicular to 1, and those remain the same if one replaces\nW by\nW −a1N1T\nN + NaIdRN\nwhich has negative off-diagonal coefficients ˜wll′ = wll′ −a for large enough a.\nIn graph (or Laplacian) embedding, the starting point is a weighted graph on\n{1,...,N} with edge weights βll′ interpreted as similarities between vertexes. These\nweights may or may not be deduced from measures of dissimilarity (dll′,k,l = 1,...,N)\nwhich themselves may or may not be computed as distances between training data\nx1,...,xN. If one starts with dissimilarities, it is typical to use simple transformations\nto compute edge weights, and one the most commonly used is\nβll′ = exp(−d2\nll′/2τ2)\nfor some constant τ. These weights are usually truncated, replacing small values\nby zeros (or the computation is restricted to nearest neighbors), to ensure that the\nresulting graph is sparse, which speeds up the computation of eigenvectors for large\ndatasets.\nGiven a target dimension p, the graph is then represented as a collection of points\ny1,...,yN ∈Rp, where yk is associated to vertex k. For this purpose, one needs to com-\npute the first p +1 eigenvectors, e1,...,ep+1, of the graph Laplacian, with the require-\nment that e1 = ±1N/\n√\nN. (This is always possible and can be achieved numerically\nby computing eigenvectors of L+c11T for large enough c.) The graph representation\n\n21.2. MANIFOLD LEARNING\n563\nis then given by y(|)\nk i = e(k)\ni+1 for i = 1,...,p and k = 1,...,N. Note that these are exactly\nthe same operations as those described in steps 4 and 5 of the LLE algorithm.\nOne way to interpret this construction is that e2,...,ep+1 (the coordinate functions\nfor the representation y1,...,yN) minimize\np\nX\nj=1\n∥ei∥2\nH1\nsubject to e2,...,ep+1 being perpendicular to each other and perpendicular to the\nconstant functions (these constraints being justified for the same reasons as those\ndiscussed for LLE). Small H1 semi-norms being associated with smoothness on the\ngraph, we see that we are looking for the smoothest zero-mean representation of the\ndata.\nBased on our discussion of LLE, we can make an alternative interpretation by\nintroducing a symmetric square root R of the Laplacian matrix L or any matrix such\nthat RRT = L. Writing R = [ρ1,...,ρN], one has\nL =\nN\nX\nk=1\nρkρT\nk\nand PN\nk=1 ρk = 0. With this notation, we can interpret Laplacian embedding as the\nminimization of\nN\nX\nk=1\n\f\f\f\f\f\f\f\nN\nX\nl=1\nρ(l)\nk yl\n\f\f\f\f\f\f\f\n2\n(subject to previous orthogonality constraints). In other terms, y1,...,yN are deter-\nmined so that the linear relationships\nρkkyk = −\nX\nl,k\nρ(l)\nk yl\nare satisfied, which is similar to the LLE condition, without the requirement that\nρk(k) = 1.\nAn alternate requirement that could have been made for LLE is that PN\nl=1(ρ(l)\nk )2 =\n1 for all k. Instead of having to solve a linear system in step 2 of Algorithm 21.2, one\nwould then compute an eigenvector with smallest eigenvalue of Sk. For graph em-\nbedding, this constraint can be enforced by modifying the Laplacian matrix, since\nPN\nl=1(ρ(l)\nk )2 is just the (k,k) coefficient of RRT . Given this, let D be the diagonal matrix\nformed by the diagonal elements of L, and define the so-called “symmetric Lapla-\ncian” ˜L = D−1/2LD−1/2. One obtain an alternative, and popular, graph embedding\nmethod by replacing e1,...,ep+1 above by the first p eigenvectors of ˜L.\n\n564\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nAnother interpretation of this representation can be based on the random walk\nassociated with the graph structure. Consider the random process t 7→q(t) defined\nas follows. The initial position, q(0) is selected according to some arbitrary dis-\ntribution, say π0. Conditional to q(t) = k, the next position is determined by set-\nting random waiting times τkl, each distributed as an exponential distribution with\nrate βkl (or expectation 1/βkl), and the process moves to the position l for which\nτkl is smallest after waiting for that time. Let P(t) be the matrix with coefficients\nP(t,k,l) = P(q(t + s) = l | q(s) = k). Then, one has\nP(t) = e−tL\nwhere the right-hand side is the matrix exponential. If λ1 = 0 ≤λ2 ≤··· ≤λN are the\neigenvalues of L with corresponding eigenvectors e1,...,eN, then\nP(t) =\nN\nX\ni=1\ne−tλieieT\ni\nIn particular, restricting the first eigenvectors of L provides an approximation of this\nstochastic process, i.e.,\nP(t) ≃11T\nN +\np\nX\ni=1\ne−tλi+1y(i)y(i)T .\nWe could also have considered the discrete-time version of the walk, for which,\nconsidering integer times t ∈N,\nP(q(t + 1) = l | q(t) = k) =\n\nβkl\nPN\nl′=1,l′,k βkl′\nif l , k\n0 if l = k\nIntroducing the matrix B of similarities βkl (with zero on the diagonal) and the diag-\nonal matrix D with coefficients dkk = PN\nl=1,l,k βkl, the r.h.s. of the previous equation\nis the k,l entry of the matrix ˜P = D−1B. Then, for any integer s, P(q(t+s) = l | q(t) = k)\nis the k,l entry if ˜Ps = D−1/2(D−1/2BD−1/2)sD1/2.\nThe Laplacian matrix L is given by L = D −B. The normalized Laplacian is\n¯L = D−1/2LD−1/2 = IdRN −D−1/2BD−1/2\nso that\n˜Ps = D−1/2(IdRN −¯L)sD1/2.\n\n21.2. MANIFOLD LEARNING\n565\nIf one introduces the eigenvectors ¯e1,..., ¯eN of the normalized Laplacian, still asso-\nciated with non-decreasing eigenvalues ¯λ1 = 0,..., ¯λN, and arranges without loss of\ngenerality that ¯e1 ∝D1/21N, then\n˜Ps = D−1/2\n\n\nN\nX\ni=1\n(1 −¯λi)s ¯ei ¯eT\ni\n\nD1/2.\nThis shows that, for s large enough, the transitions of this Markov chain are well ap-\nproximated by its first terms, suggesting using the alternative representation based\non the normalized Laplacian:\n¯yk(i) = ¯ei+1(k).\nBoth representations (using normalized or un-normalized Laplacians) are commonly\nused in practice.\n21.2.4\nStochastic neighbor embedding\nGeneral algorithm\nStochastic neighbor embedding (SNE, Hinton and Roweis [90]), and its variant (t-\nSNE, Maaten and Hinton [122]) have become a popular tool for the visualization\nof high-dimensional data based on dissimilarity matrices. One of the key contri-\nbutions of this algorithm is to introduce a local data rescaling step, that allows for\nvisualization of more homogeneous point clouds.\nAssume that dissimilarities D = (dkl,k,l = 1,...,N) are observed. The basic prin-\nciple in SNE is to deduce from the dissimilarities a family of N probability distribu-\ntions on {1,...,N}, that we will denote πk, k = 1,...,N, with the property that πk(k) =\n0. The computation of these probabilities include the local normalization step, and\nwe will return to this later. Given the πk’s, one then estimate low-dimensional rep-\nresentations y = (y1,...,yN) such that πk ≃ψk where ψk is given by\nψk(l;y) =\nexp\n\u0010\n−β\n\u0010\n|yk −yl|2\u0011\u0011\nPN\nl′=1,l′,k exp\n\u0010\n−β\n\u0010\n|yk −yl′|2\u0011\u00111l,k.\nHere, β : [0,+∞) →[0,+∞) is an increasing differentiable function that tends to +∞\nat infinity. The derivative is denoted ∂β. The original version of SNE [90] uses\nβ(t) = t and t-SNE [122] takes β(t) = log(1 + t).\nThe determination of the representation can then be performed by minimizing a\nmeasure of discrepancy between the probabilities πk and ψk. In Hinton and Roweis\n[90], it is suggested to minimize the sum of Kullback-Liebler divergences, namely\nN\nX\nk=1\nKL(πk∥ψk(·;y))\n\n566\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nor, equivalently, to maximize\nF(y) =\nN\nX\nk,l=1\nπk(l)logψk(l;y)\n= −\nN\nX\nk,l=1\nβ(|yk −yl|2)πk(l) +\nN\nX\nk=1\nlog\n\n\nN\nX\nl=1,l,k\nexp(−β(|yk −yl|2))\n\n\nThe gradient of this function can be computed by evaluating the derivative at ϵ = 0\nof f : ϵ 7→F(y + ϵh). This computation gives\nf ′(0) = −2\nN\nX\nk,l=1\n∂β(|yk −yl|2)(yk −yl)T (hk −hl)πk(l)\n+ 2\nN\nX\nk=1\nN\nX\nl=1\n∂β(|yk −yl|2)(yk −yl)T (hk −hl)ψk(l;y)\n= −2\nN\nX\nk=1\nhT\nk\nN\nX\nl=1\n∂β(|yk −yl|2)(yk −yl)(πk(l) + πl(k) −ψk(l;y) −ψl(k;y))\nThis shows that\n∂ykF(y) = −2\nN\nX\nl=1\nβ(|yk −yl|2)(yk −yl)(πk(l) + πl(k) −ψk(l;y) −ψl(k;y)).\nThis is a rather simple expression that can be used with any first-order opti-\nmization algorithm to maximize F. The algorithm in Hinton and Roweis [90] uses\ngradient ascent with momentum, namely iterating\ny(n+1) = y(n) + γ∇F(y(n)) + α(n)(y(n) −y(n−1))\nChoosing α(n) = 0 provides standard gradient ascent with fixed gain γ (of course,\nother optimization methods may be used). The momentum can be interpreted, in a\nloose sense, as a “friction term”.\nA variant of the algorithm replaces the node-dependent probabilities πk by a sin-\ngle, symmetric, joint distribution ¯π on {1,...,N}2, (k,l) 7→¯π(k,l), satisfying ¯π(k,k) = 0\nand ¯π(k,l) = ¯π(l,k). The target distribution ¯ψ then becomes\n¯ψ(k,l;y) =\nexp(−β(|yk −yl|2))\nPN\nk′,l′=1 exp(−β(|yk′ −yl′|2))\n.\n\n21.2. MANIFOLD LEARNING\n567\nWith such a choice, the objective function has a simpler form, namely minimizing\nKL( ¯π∥¯ψ(·,y)) or maximizing the expected likelihood\n¯F(y) =\nN\nX\nk,l=1\n¯π(k,l)log ¯ψ(k,l;y) = −\nN\nX\nk,l=1\nβ(|yk −yl|2) ¯π(k,l) + log\n\n\nN\nX\nk,l=1\nexp(−β(|yk −yl|2))\n\n.\nThe gradient of this symmetric version of F can be computed similarly to the previ-\nous one and is given by\n∂yk ¯F(y) = −4\nN\nX\nl=1\n∂β(|yk −yl|2)(yk −yl)( ¯π(k,l) −¯ψ(k,l;y)).\nSetting initial probabilities\nThe probabilities πk(l) or ¯π(k,l) are deduced from the dissimilarities as\nπk(l) =\ne−d2\nkl/2σ2\nk\nPN\nl′=1,l′,k e−d2\nkl′/2σ2\nk\nfor l , k and\n¯π(k,l) = πk(l) + πl(k)\n2n\n.\nThe coefficients σ2\nk , k = 1,...,N operate the local normalization, justifying, in\nparticular, the parameter-free expression chosen for ψ and ¯ψ. These coefficients are\nestimated so as to adjust the entropies of all πk to a fixed value, which is a parameter\nof the algorithm. Note that, letting t = 1/2σ2\nk and H(πk) = −PN\nl=1 πk(l)logπk(l),\n∂tH(πk) = −\nN\nX\nl=1\n∂tπk(l)logπk(l) −\nN\nX\nl=1\n∂tπk(l)\n= −\nN\nX\nl=1\n∂tπk(l)logπk(l)\nNow\n∂t logπk(l) = −d2\nkl + ¯d2\nk\nwith ¯d2\nk = PN\nl′=1 d2\nkl′πk(l′). Writing ∂tπk(l) = πk(l)∂t logπk(l), we have\n∂tH(πk) =\nN\nX\nl=1\n(dkl logπk(l))πk(l) −¯dk\nN\nX\nl=1\nπk(l)logπk(l).\n\n568\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nUsing Schwartz inequality, we see that ∂tH(πk) ≤0 so that H(πk) is decreasing as\na function of t, i.e., increasing as a function of σ2\nk . When σ2\nk →0, πk converges to\nthe uniform distribution on the set of nearest neighbors of k (the indexes l , k such\nthat d2\nkl is minimal) and, letting νk denote their number, which is typically equal to\n1, H(πk) converges to logνk. When σ2\nk tends to infinity, πk converges to the uniform\ndistribution over indexes l , k, whose entropy is log(N −1). This shows that eH(πk),\nwhich is called the perplexity of πk can take any value between νk and N −1. The\ncommon target value of the perplexity can therefore be taken anywhere between\nmaxk νk and N −1. In Maaten and Hinton [122], it is recommended to choose a value\nbetween 5 and 50.\nRemark 21.2 The complexity of the computation of the gradient of the objective\nfunction (either F or ¯F ) scales like the square of the size of the training set, which\nmay be prohibitive when N is large. In Van Der Maaten [193], an accelerated proce-\ndure, that involves an approximation of the gradient is proposed. (This procedure is\nhowever limited to representations in dimensions 2 or 3.)\n♦\n21.2.5\nUniform manifold approximation and projection (UMAP)\nUMAP is similar in spirit to t-SNE, with a few important differences that result in\na simpler optimization problem and faster algorithms. Like Isomap, the approach\nis based on matching distances between the high-dimensional data and the low-\ndimensional representation. But while Isomap estimates a unique distance on the\nwhole training set (the geodesic distance on the nearest-neighbor graph), UMAP\nestimates as many “local distances” as observations before “patching” them to form\nthe final representation.\nThe goal of transporting possibly non-homogeneous locally defined objects on\ninitial data to a homogeneous low-dimensional visualization is what makes UMAP\nsimilar to t-SNE. The difference is that t-SNE transports local probability distri-\nbutions, while UMAP transports metric spaces.\nMore precisely, given distances\n(dkl,k,l = 1,...,N) and an integer m provided as input, the algorithm builds, for\neach k = 1,...,N a (pseudo-)metric δk on the associated data graph by letting\nδ(k)(k,l) = δ(k)(l,k) = 1\nσk\n\u0012\ndkl −min\nl′,k dkl′\n\u0013\nif l is among the m nearest neighbors of k, where m is a parameter of the algorithm,\nwith all other distances being infinite. The normalization parameter σk has a role\nsimilar to that of the same parameter in t-SNE in that it tends to make the represen-\ntation homogeneous. Here, it is computed such that\nX\nl\nexp(−δ(k)(l,l′)) = log2 m.\n\n21.2. MANIFOLD LEARNING\n569\nEach such metric provides a weighted graph structure on {1,...,N} by defining\nweights w(k)\nll′ = exp(−δ(k)(l,l′)). In UMAP, these weights are interpreted in the frame-\nwork of fuzzy sets, where a fuzzy set is defined by a pair (A,µ) where A is a set and\nµ a function µ : A →[0,1] [210]. The function µ is called the membership func-\ntion and µ(x) for x ∈A is the membership strength of x to A. Letting V = {1,...,N}\nand E = V × V, one then interprets the weights as defining the membership strength\nof edges to the graph, i.e., one defines the “fuzzy graph” G(k) = (V,E,µ(k)) where\nµ(k)(l,l′) = w(k)\nll′ is the membership strength of edge (l,l′) to G(k).\nThis is, of course, just a reinterpretation of weighted graphs in terms of fuzzy\nsets, but it allows one to combine the collection (G(k),k = 1,...,N) using simple fuzzy\nsets operations, namely, defining the combined (fuzzy) graph G = (V,E,µ) with\n(E,µ) =\nN\n[\nk=1\n(E,µ(k))\nbeing the fuzzy union of the edge sets. There are, in fuzzy logic, multiple ways\nto define set unions [85], and the one selected for UMAP define (A,µ) ∪(A′,µ′) =\n(A∪A′,ν) with ν(x) = µ(x)+µ′(x)−µ(x)µ′(x) (µ(x) and µ′(x) being defined as 0 is x < A\nor x < A′ respectively). In UMAP, each edge µ(k)(l,l′) is non-zero only is k = l or l′ so\nthat\nµ(l,l′) = w(l)\nll′ + w(l′)\nll′ −w(l)\nll′w(l′)\nll′ .\nThis defines an input fuzzy graph structure on {1,...,N} that serves as target for\nan optimized similar structured associated with the representation y = (y1,...,yN).\nThis representation, since it is designed as a homogeneous representation of the\ndata, provides a unique fuzzy graph H(y) = (V,E,ν(·;y)) and the edge membership\nfunction is defined by ν(l,l′;y) = ϕa,b(yl,yl′) with\nϕa,b(y,y′) =\n1\n1 + a|y −y′|b .\nThe parameters a and b are adjusted so that ϕa,b provides a differentiable approxi-\nmation of the function\nψρ0(y,y′) = exp(−max(0,|y −y′| −ρ0))\nwhere ρ0 is an input parameter of the algorithm. This function ψρ0 takes the same\nform as the membership function defined for local graphs G(k), and its replacement\nby ϕa,b makes possible the use of gradient-based methods for the determination of\nthe optimal y (ψρ0 is not differentiable everywhere).\nThe representation y is optimized by minimizing the “fuzzy set cross-entropy”\nC(µ∥ν(·,y)) =\nX\n(k,l)∈E\n \nµ(k,l)log µ(k,l)\nν(k,l|y) + (1 −µ(k,l))log 1 −µ(k,l)\n1 −ν(k,l|y)\n!\n\n570\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nor, equivalently, maximizing (using, for short, ϕ = ϕa,b)\nF(y) =\nX\n(k,l)∈E\n(µ(k,l)logν(k,l|y) + (1 −µ(k,l))log(1 −ν(k,l|y)))\n=\nX\n(k,l)∈E\n(µ(k,l)logϕ(yk,yl) + (1 −µ(k,l))log(1 −ϕ(yk,yl)))\nNote the important simplification compared to the similar function F is t-SNE,\nin that the logarithm of a potentially large sum is avoided. We have\n∂ykF(y) =2\nN\nX\nl=1\nµ(k,l)∂yk logϕ(yk,yl) + 2\nN\nX\nl=1\n(1 −µ(k,l))∂yk log(1 −ϕ(yk,yl))\n=2\nN\nX\nl=1\nµ(k,l)∂yk log\nϕ(yk,yl)\n1 −ϕ(yk,yl) + 2\nN\nX\nl=1\n∂yk log(1 −ϕ(yk,yl)).\nThe optimization can be implemented using stochastic gradient ascent. Introduce\nrandom variables ξkl and ξ′\nkl both taking value in {0,1}, all independent of each\nother and such that P(ξkl = 1) = µkl and P(ξ′\nkl = 1) = ϵ. Define\nHk(y,ξ,ξ′) = 2\nN\nX\nl=1\nξkl∂yk log\nϕ(yk,yl)\n1 −ϕ(yk,yl) + 2ck\nN\nX\nl=1\nN\nX\nl′=1\nξklξ′\nkl′∂yk log(1 −ϕ(yk,yl′)).\nThen, if one takes ck = 1/(ϵP\nl µ(k,l)) one has\nE(Hk(y,ξ,ξ′)) = ∂ykF(y).\nThis corresponds to SGA iterations in which:\n(1) Each edge (k,l) is selected with probability µ(k,l) (which are zero for unless k\nand l are neighbors);\n(2) If (k,l) is selected, one selects an additional edges (k,l′) each with probability ϵ.\nLetting l1,...,lm be the number of edges selected, yk is updated according to\nyk ←yk + 2γ\n\n∂yk log\nϕ(yk,yl)\n1 −ϕ(yk,yl) + ck\nm\nX\nj=1\n∂yk log(1 −ϕ(yk,yl′))\n\n.\nRemark 21.3 If one prefers using probability rather than fuzzy set theory, the graphs\nG(k) may also be interpreted as random graphs in which edges are added indepen-\ndently from each other and each edge (l,l′) is drawn with probability µ(k)(l,l′). The\n\n21.2. MANIFOLD LEARNING\n571\ncombined graph G is then the random graph in which (l,l′) is present if and only\nif it is in at least one of the G(k) and the objective function C coincides with the KL\ndivergence between this random graph and the random graph similarly defined for\ny.\nHowever, this fuzzy/random graph formulation of UMAP—which corresponds\nto current practical implementations—is only a special case of the theoretical con-\nstruction made in McInnes et al. [130] which builds on the theory of (fuzzy) simpli-\ncial sets and their representation of metric spaces. We refer the interested reader to\nthis reference, which requires a mathematical background beyond the scope of these\nnotes.\n♦\n\n572\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\n\nChapter 22\nGeneralization Bounds\nWe provide, in this chapter, an introduction to some theoretical aspects of statistical\n(or machine) learning, mostly focusing on the derivation of “generalization bounds”\nthat provide high-probability guarantees on the generalization error of predictors\nusing training data. While these bounds are not always of practical use, because\nmaking them small in realistic situations would require an enormous amount of\ntraining data, their derivations and the form they take for specific model classes\nbring important insight on the structure of the learning problem, and help under-\nstand why some methods may perform well while others do not.\n22.1\nNotation\nWe here recall some notation introduced in chapter 5. We consider a pair of random\nvariables (X,Y), with X : Ω→RX and Y : Ω→G. Regression problems correspond\nto RY = R (or Rq if multivariate) and classification to RY being a finite set. A pre-\ndictor is a function f : RX →RY. The general prediction problem is to find such\na predictor within a class of functions, denoted F , minimizing the prediction (or\ngeneralization error)\nR(f ) = E(r(Y,f (X)))\nwhere r : RY × RY →[0. + ∞) is a risk function.\nA training set is a family T = ((x1,y1),...,(xN,yN)) ∈(RX × RY)N, the set T of all\npossible training sets therefore being the set of all finite sequences in RX × RY. A\ntraining algorithm can then be seen as a function A : T →F which associates to\neach training set T a function A(T) = ˆfT .\n573\n\n574\nCHAPTER 22. GENERALIZATION BOUNDS\nGiven T ∈T , The training set error associated to a function f ∈F is\nˆRT (f ) = 1\n|T |\nX\n(x,y)∈T\nr(y,f (x)))\nand the in-sample error associated to a learning algorithm is the function T 7→ET\n∆=\nˆRT ( ˆfT ). Fixing the size (N) of T, one also considers the random variable T with\nvalues in T distributed as an N-sample of the distribution of (X,Y).\nA good learning algorithm should be such that the generalization error R( ˆfT ) is\nsmall, at least in average (i.e., E(R( ˆfT )) is small). Our main goal in this chapter is to\ndescribe generalization bounds trying to find upper-bounds for R( ˆfT ) based on ET\nand properties of the function class F . These bounds will reflect the bias-variance\ntrade-off, in that, even though large function classes provide smaller in-sample er-\nrors, they will also induce a large additive term in the upper-bound, accounting for\nthe “variance” associated to the class.\nRemark 22.1 Both variables X and Y are assumed to be random in the previous\nsetting, but there are often situations when one of them is “more random” than the\nother. Randomness in Y is associated to measurement errors, or ambiguity in the\ndecision. Randomness in X more generally relates to the issue of sampling a dataset\nin a large dimensional space. In some cases, Y is not random at all: for example,\nin object recognition, the question of assigning categories for images such as those\ndepicted in fig. 22.1 has a quasi-deterministic answer. Sometimes, it is X who is\nnot random, for example when observing noisy signals where X is a deterministic\ndiscretization of a time interval and Y is some function of X perturbed by noise.\n♦\n22.2\nPenalty-based Methods and Minimum Description Length\n22.2.1\nAkaike’s information criterion\nWe make a computation under the following assumptions. We assume a regression\nmodel Y = fθ(X) + ϵ where ϵ ∼N (0,σ2) and f is some function parametrized by\nθ ∈Rm. We also assume that the true distribution is actually covered by this model\nand represented by a parameter θ0. Let ˆθT denote the parameter estimated by least\nsquares using a training set T, and denote for short ˆfT = f ˆθT .\nThe in-sample error is\nET = 1\nN\nN\nX\nk=1\n(yk −ˆfT (xk))2.\n\n22.2. PENALTY-BASED METHODS AND MINIMUM DESCRIPTION LENGTH575\nFigure 22.1: Images extracted from the PASCAL challenge 2007 dataset [70], in which cate-\ngories must be associated with images. There is little ambiguity on correct answers based on\nobserving the image, i.e., little randomness in the variable Y.\nWe want to compare the training-set-averaged prediction error and the average in-\nsample error, namely compute the error bias\n∆N = E(R(fT )) −E(ET ).\nWrite\n∆N = E(R(fT )) −R(fθ0) + R(fθ0) −E(ET ).\nWe make a heuristic argument to evaluate ∆N. We can use the fact that ˆθT mini-\nmizes the empirical error and write\n1\nN\nN\nX\nk=1\n(Yk −fθ0(Xk))2 = ET + σ2( ˆθT −θ0)T JT ( ˆθT −θ0) + o(| ˆθT −θ0|2)\nwith\nJT =\n1\n2σ2N\nN\nX\nk=1\n∂2\nθ((yk −fθ(xk))2)|θ−ˆθT ,\nwhich is an m by m symmetric matrix.\nNow, using the fact that θ0 minimizes the mean square error (since fθ0(x) =\nE(Y|X = x)), we can write, for any T :\nR(fT ) = R(fθ0) + σ2( ˆθT −θ0)T I( ˆθT −θ0) + o(| ˆθT −θ0|2)\n\n576\nCHAPTER 22. GENERALIZATION BOUNDS\nwith\nI =\n1\n2σ2E(∂2\nθ(Y −fθ(X))2\n|θ−θ0).\nAs a consequence, we can write (taking expectations in both Taylor expansions)\n∆N = σ2E\n\u0010\n( ˆθT −θ0)T JT ( ˆθT −θ0)\n\u0011\n+ σ2E\n\u0010\n( ˆθT −θ0)T I( ˆθT −θ0)\n\u0011\n+ o(E(| ˆθT −θ0|2)).\n(We skip hypotheses and justification for the analysis of the residual term.)\nWe now note that, because we are assuming a Gaussian noise, and that the true\ndata distribution belongs to the parametrized family, the least-square estimator is\nalso a maximum likelihood estimator. Indeed, the likelihood of the data is\n1\n(2πσ2)N/2 exp\n\n−1\n2σ2\nN\nX\nk=1\n(Yk −fθ(Xk))2\n\n\nN\nY\nk=1\nϕX(Xk)\nwhere ϕX is the p.d.f. of X and does not depend on the unknown parameter.\nWe can therefore apply classical results from mathematical statistics [194]. Un-\nder some mild smoothness assumptions on the mapping θ 7→fθ, ˆθT converges to\nθ0 in probability when N tends to infinity, the matrix JT converges to I, which\nis the model’s Fisher information matrix, and\n√\nN( ˆθT −θ0) converges in distribu-\ntion to a Gaussian N (0,I−1) . This implies that both N( ˆθT −θ0)T JT ( ˆθT −θ0) and\nN( ˆθT −θ0)T I( ˆθT −θ0) converge to a chi-square distribution with m degrees of free-\ndom, whose expectation is m, which indicates that ∆N has order 2σ2m/N.\nThis analysis can be used to develop model selection rules, in which one chooses\nbetween models of dimensions k1 < k2 < ··· < kq = m (e.g., by truncating the last\ncoordinates of X). The rule suggested by the previous computation is to select j\nminimizing\nE(j)\nT ( ˆfT ) +\n2σ2kj\nN\n,\nwhere E(j) is the in-sample error computed using the kj-dimensional model. This\nis an example of a penalty-based method, using the so-called Akaike’s information\ncriterion (AIC) [2].\n22.2.2\nBayesian information criterion and minimum description length\nOther penalty-based methods are more size-averse and replace the constant, 2, in\nAIC by a function of N = |T |, for example logN. Such a change can be justified by a\nBayesian analysis, yielding the Bayesian information criterion (BIC) [174]. The ap-\nproach in this case is not based on an evaluation of the error, but on an asymptotic\n\n22.2. PENALTY-BASED METHODS AND MINIMUM DESCRIPTION LENGTH577\nestimation of the posterior distribution resulting from a Bayesian model selection\nprinciple. Like in the previous section, we content ourselves with a heuristic discus-\nsion.\nLet us consider a statistical model parametrized by θ ∈Θ, where Θ is an open\nconvex subset of Rm with p.d.f. given by\nf (z;θ) = exp(θT U(z) −C(θ)),\nwith U : Rd →Rmand z = (x,y). We are given a family of sub-models represented\nby M1,...,Mq, where, for each j, Mj is the intersection of Θ with a kj-dimensional\naffine subspace of Rm. We are also given a prior distribution for θ in which a sub-\nmodel is first chosen, with probabilities α1,...,αq, and given that, say, Mj is se-\nlected, θ ∈Mj is chosen with a probability distribution with density ϕj with respect\nto Lebesgue’s measure on Mj (denoted dmj). Given training data T = (z1,...,zN),\nBayesian model selection consists in choosing the model Mj where j maximizes the\nposterior log-likelihood\nµ(Mj|T ) = log\nZ\nRm αjeN(θT ¯UT −C(θ))ϕjdmj(θ)\nwhere ¯UT = (U(z1) + ··· + U(zN))/N.\nConsider the maximum likelihood estimator ˆθj within Mj, maximizing ℓ(θ, ¯UT ) =\nθT ¯UT −C(θ) over Mj. Then one has\nℓ(θ, ¯UT ) = ℓ( ˆθj, ¯UT ) + 1\n2(θ −ˆθj)T ∂2\nθℓ( ˆθj, ¯UT )(θ −ˆθj) + Rj(θ, ˆθj)|θ −ˆθj|3\nNote that the first derivative of ℓis ∂θℓ= ¯U −Eθ(U) where Eθ is the expectation for\nf (·,θ). The second derivative is −varθ(U) (showing that ℓis concave) and the third\nderivative involves third-order moments of U for Eθ and (like the second derivative)\ndoes not depend on ¯UT . In particular, we can assume that, for any M > 0, there exists\na constant CM such that whenever max(|θ|,| ˆθj|) ≤M, we have Rj(θ, ˆθj) ≤CM.\nThe law of large numbers implies that ¯UT converges to a limit when N tends to\ninfinity, and our assumptions imply that ˆθj converges to the parameter providing\nthe best approximation of the distribution of Z for the Kullback-Leibler divergence.\nIn particular, with probability 1, there exists an N such that ˆθj belongs to any large\nenough, but fixed, compact set. Moreover, the second derivative ℓ( ˆθj) will also con-\nverge to a limit, −Σj.\n\n578\nCHAPTER 22. GENERALIZATION BOUNDS\nFor any ϵ > 0, write\nZ\nRm αjeN(θT ¯UT −C(θ))ϕjdmj(θ)\n=\nZ\n|θ−ˆθj|≤ϵ\neN(θT ¯UT −C(θ))ϕjdmj(θ) +\nZ\n|θ−ˆθj|≥ϵ\neN(θT ¯UT −C(θ))ϕjdmj(θ).\nThe second integral converges to 0 exponentially fast when N tends to ∞. The first\none behaves essentially like\nZ\nMj\ne−1\n2N(θ−ˆθj)T Σ−1\nj (θ−ˆθj)+logϕj(θ)dmj(θ).\nNeglecting logϕj(θ), this integral behaves like (2πdet(Σj/N))−1/2, whose logarithm\nis (−kj(logN)/2) plus constant terms. As a consequence, we find that\nµ(Mj | T ) = max\nθ∈Mj\nℓ(θ) −\nkj\n2 logN + bounded terms.\nConsider, as an example, linear regression with Y = β0 + bT x + σ2ν where ν is a\nstandard Gaussian random variable. Assume that the distribution of X is known, or,\npreferably, make the previous discussion conditional to X1,...,XN. Let sub-models\nMj correspond to the assumption that all but the first kj −1 coefficients of b van-\nish. Then, up to bounded terms, the Bayesian estimator must minimize (over such\nparameters b)\n1\n2σ2\nN\nX\nk=1\n(yk −β0 −bT xk)2 +\nkj\n2 logN .\nor\nE(j)\nT +\nkjσ2\nN\nlogN .\nWe now turn to another interesting point of view, which provides the same penalty,\nbased on maximum description length principle (MDL; Rissanen [162]) measuring\nthe coding efficiency of a model.\nLet us fix some notation. We assume that one has q competing models for pre-\ndicting Y from X, for example, linear regression models based on different subsets\nof the explanatory variables. Denote these models M1,...,Mq. Each model will be\nseen, not as an assumption on the true joint distribution of X and Y, but rather as\na tool to efficiently encode the training set ((x1,y1),...,(xN,yN)). To describe MDL,\n\n22.2. PENALTY-BASED METHODS AND MINIMUM DESCRIPTION LENGTH579\nwhich selects the model that provides the most efficient code, we need to reintroduce\na few basic concepts of information theory.\nThe entropy of a discrete probability P over a set Ωis\nH2(P) = −\nX\nx∈Ω\npx log2 px.\n(The logarithm in base 2 is used because of the tradition of coding with bits in infor-\nmation theory.)\nFor a discrete random variable X, the entropy H2(X) is H2(PX) where PX is the\nprobability distribution of X. The relation between the entropy and coding theory\nis as follows: a code is a function which associates to any element ω ∈Ωa string of\nbits c(ω). The associated code-length is denoted lc(ω), which is simply the number\nof bits in c(ω). When P is a probability on Ω, the efficiency of a code is measured by\nthe average code-length:\nEP(lc) =\nX\nω∈Ω\nlc(ω)P(ω).\nShannon’s theorem [175, 55] states that, under some conditions on the code (en-\nsuring that any sequence of words can be recognized as soon as it is observed:\none says that it is instantaneously decodable) the average code length can never\nbe larger than the entropy of P.\nMoreover, it states that there exists codes that\nachieve this lower bound with no more than one bit loss, such that for all ω, lc(ω) ≤\n−log2(P(ω))+1. These optimal codes, such as the Huffman code [55], can completely\nbe determined from the knowledge of P. This allows one to interpret a probability P\non Ωas a tool for designing codes with code-lengths essentially equal to (−log2 P).\nThis statement can be generalized to continuous random variables (replacing the\ndiscrete probability P by a probability density function, say ϕ) if one introduces a\ncoding precision level, denoted δ0, meaning that the decoded values may differ by\nno more than δ0 from the encoded ones. The result is that the optimal code-length\nat precision δ0 can be estimated (up to one extra bit) by −log2 ϕ −log2 δ0.\nIn our context, each model of the conditional distribution of Y given X, with\nconditional density ϕ(y|x), provides a way to encode the training set with a total\ncode length, for (y1,...,yN), of\n−\nN\nX\nk=1\nlog2 ϕ(yk | xk) −N log2 δ0\n(working, as before, conditionally to x1,...,xN). We assume that the precision at\nwhich the data is encoded is fixed, which implies that the last term does not affect the\n\n580\nCHAPTER 22. GENERALIZATION BOUNDS\nmodel choice. Now, assume a sequence of m parametrized model classes, M1,...,Mm\nand let ϕ(y | x,θ,Mj) denote the conditional distribution with parameter θ in the\nclass Mj. Within model Mj, the optimal code length corresponds to the maximum\nlikelihood:\n−\nN\nX\nk=1\nlog2 ϕ(x,y | ˆθj,Mj) = −max\nθ\n\n\nN\nX\nk=1\nlog2 ϕ(x,y;θ,Mj)\n\n.\nIf the models are nested, which is often the case, the most efficient will always be\nthe largest model, since the maximization is on a larger set. However, the minimum\ndescription length (MDL) principle uses the fact that, in order to decode the com-\npressed data, the model, including its optimal parameters, has to be known, so that\nthe complete code needs to include a model description. The decoding algorithm\nwill then be: decode the model, then use it to decode the data.\nSo assume that a model (one of the Mj’s) has a kj-dimensional parameter θ. Also\nassume that a probability distribution, π(θ | Mj), is used to encode θ. Also choose a\nprecision level, δij, for each coordinate in θ, i = 1,...,kj. (Previously, we could con-\nsider the precision of the yk, δ0, as fixed, but now, the precision level for parameters\nis a variable that will be optimized.) The total description length using this model\nnow becomes\n−\nN\nX\nk=1\nlog2 ϕ(yk | xk;θ,Mj) −log2 π(θ | Mj) −\nkj\nX\ni=1\nlog2(δij).\nLet ˆθ(j) be the parameter that maximizes\nL(θ | Mj) =\nN\nX\nk=1\nlog2 ϕ(yk | xk;θ,Mj) + log2 π(θ | Mj)\nIf π is interpreted as a prior distribution of the parameters, ˆθ(j) is the maximum\na posteriori Bayes estimator. We now take the correction caused by (δij,i = 1,...,kj)\ninto account, by assuming that the ith coordinate in ˆθ(j) is truncated to −log2 δi bits.\nLet θ\n(j) denote this approximation. A second-order expansion of L(θ|Mj) around\nˆθ(j) yields (assuming sufficient differentiability)\nL(θ\n(j) | Mj) = L( ˆθ(j) | Mj) + 1\n2(θ\n(j) −ˆθ(j))T S ˆθ(j)(θ\n(j) −ˆθ(j)) + o(|θ\n(j) −ˆθ(j)|2)\nwhere Sθ is the matrix of second derivatives of L(· | Mj) at θ. Approximating θ\n(j)−ˆθ(j)\nby δ(j) (the kj-dimensional vector with coordinates δij, i = 1,...,kj), we see that the\n\n22.3. CONCENTRATION INEQUALITIES\n581\nprecision should maximize\n1\n2(δ(j))T S ˆθ(j)δ(j) +\nkj\nX\ni=1\nlog2 δij.\nNote that S ˆθ(j) must be negative semi-definite, since ˆθ is a local maximum. Assuming\nit is non-singular, the previous expression can be maximized and yields\nS ˆθ(j)δ(j) = −\n1\nlog2\n1\nδ(j)\n(22.1)\nwhere 1/δ(j) is the vector with coordinates (1/δij).\nLet us now make an asymptotic evaluation. Because L(θ | Mj) includes a sum\nover N independent terms, it is reasonable to assume that S ˆθ(j) has order N, and\nmore precisely, that S ˆθ(j)/N has a limit. Rewrite (22.1) as\nS ˆθ(j)\nN\n√\nNδ(j) = −\n1\nlog2\n1\n√\nNδ(j) .\nThis implies that\n√\nNδ(j) is the solution of an equation which stabilizes with N, and\nit is therefore reasonable to assume that the optimal δij takes the form δij = ci(N |\nMj)/\n√\nN, with ci(N | Mj) converging to some limit when N tends to infinity. The\ntotal cost can therefore be estimated by\n−L( ˆθ(j) | Mj) +\nkj\n2 log2 N −\nkj\n2 −\nkj\nX\ni=1\nlog2 ci(N | Mj)\nThe last two terms are O(1), and can be neglected, at least when N is large compared\nto kj. The final criterion becomes the penalized likelihood\nld(θ | Mj) = L(θ|Mj) −\nkj\n2 log2 N\nin which we see that the dimension of the model appears with a factor log2 N as\nannounced (one needs to normalize both terms by N to compare with the previous\nparagraph).\n22.3\nConcentration inequalities\nThe discussion of the AIC was a first attempt at evaluating a prediction error. It was\nhowever done under very specific parametric assumptions, including the fact that\nthe true distribution of the data was within the considered model class. It was, in\n\n582\nCHAPTER 22. GENERALIZATION BOUNDS\naddition, a bias evaluation, i.e., we estimated how much, in average, the in-sample\nerror was less than the generalization error. We would like to obtain upper bounds to\nthe generalization error that hold with high probability, and rely as little as possible\non assumptions on the true data distribution.\nOne of the main tools used in this context are concentration inequalities, which\nprovide upper bounds on the various probabilities of events involving a large num-\nber of random variables. The current section provides a review of some of these\ninequalities.\n22.3.1\nCram´er’s theorem\nIf X1,X2,... are independent, integrable random variables with identical distribu-\ntions (to that of a random variable X), the law of large numbers tells us that the\nempirical mean ¯XN = (X1 + ··· + XN)/N converges with probability one to m = E(X).\nWhen the variables are square integrable, Chebychev’s inequality provides an easy\nproof of the weak law of large numbers. Indeed,\nP\n\u0010\n| ¯Xn −m| > ϵ\n\u0011\n≤1\nϵ2E\n\u0010\n( ¯XN −m)21| ¯Xn−m|>ϵ\n\u0011\n≤var( ¯XN)\nϵ2\n= var(X)\nNϵ2 .\nA stronger assumption on the moments of X yields a stronger inequality. One\nsays that X has exponential moments if there exists λ0 > 0 such that E(eλ0|X|) < ∞. In\nthis case, the cumulant-generating function, defined, for λ ∈R, by\nMX(λ) = logE(eλX) ∈[0,+∞],\n(22.2)\nis finite for λ ∈[−λ0,λ0].\nHere are a few straightforward properties of the cumulant-generating function.\n(i) One has MX(0) = 0.\n(ii) For any a ∈R, one has MaX(λ) = MX(aλ).\n(iii) If X1 and X2 are independent variables, one also has\nMX1+X2(λ) = MX1(λ) + MX2(λ).\nIn particular, MX+a(λ) = MX(λ) + λa, so that MX−E(X)(λ) = MX(λ) −λE(X).\n(iv) Finally, Markov’s inequality (which states that, for any non-negative variable Y,\nP(Y > t) ≤E(Y)/t) applied to Y = eλX for λ > 0 yields\nP(X > t) = P(eλX > eλt) ≤eMX(λ)−λt .\n(22.3)\n(Note that this inequality is trivially true for λ = 0.)\n\n22.3. CONCENTRATION INEQUALITIES\n583\nFrom these properties, one can easily derive a concentration inequality for the\nmean of independent random variables. We have M ¯XN(λ) = NMX(λ/N) and apply-\ning (22.3) we get, for any λ ≥0 and t > 0\nP( ¯XN −m > t) ≤e−λ(m+t)+M ¯XN (λ) = e−N\n\u0010 λ(m+t)\nN\n−MX( λ\nN )\n\u0011\nwhere the right-hand side may be infinite. Because this inequality is true for any λ,\nwe have\nP( ¯XN −m > t) ≤e−NM∗\nX,+(m+t)\nwhere M∗\nX,+(u) = supλ≥0(λu −MX(λ)), which is non-negative since the maximized\nquantity vanishes for λ = 0. A symmetric computation yields\nP( ¯XN −m < −t) ≤e−NM∗\nX,−(m−t)\nwhere M∗\nX,−(t) = supλ≤0(λt −MX(λ)), which is also non-negative.\nLet\nM∗\nX(t) = sup\nλ∈R\n(λt −MX(λ)) ≥0\n(22.4)\n(this is the Fenchel-Legendre transform of the cumulant generating function, some-\ntimes called the Cram´er transform of X). One has M∗\nX(m + t) = M∗\nX,+(m + t) for t > 0.\nIndeed, because x 7→eλx is convex, Jensen’s inequality implies that\nE(eλX) ≥eλm\nso that λ(m +t)−MX(λ) ≤λt < 0 if λ < 0. Similarly, M∗\nX(m −t) = M∗\nX,−(m −t) for t > 0.\nWe therefore have the following result.\nTheorem 22.2 Let X1,...,XN be independent and identically distributed random vari-\nables. Assume that these variables are integrable and let m = E(X1). Then, for all t > 0,\nP( ¯XN −m > t) ≤e−NM∗\nX(m+t)\nand\nP(| ¯XN −m| > t) ≤2e−N min(M∗\nX(m+t),M∗\nX(m−t))\nThe last inequality derives from\nP(| ¯XN −m| > t) = P( ¯XN −m > t) + uP( ¯XN −m < −t).\nThis is our first example of concentration inequality that shows that, when\nmin(M∗\nX(m + t),M∗\nX(m −t)) > 0,\n\n584\nCHAPTER 22. GENERALIZATION BOUNDS\nthe probability of a deviation by t at least of ¯Xn from its mean decays exponen-\ntially fast. The derivation of the inequality above was quite easy: apply Markov’s\ninequality in a parametrized form and optimize over the parameter. It is therefore\nsurprising that this inequality is sharp, in the sense that a similar lower bound also\nholds. Even though we are not going to use it in the rest of this chapter, it is worth\nsketching the argument leading to this lower bound, which involves an interesting\nstep making a change of measure.\nAssume (without loss of generality) that m = 0 and consider P( ¯Xn > t). Assume,\nto simplify the discussion, that the supremum of λ 7→ϵλ−MX(λ) is attained at some\nλt. We have\n∂λMX(λ) = E(XeλX)\nE(eλX) .\nLet qλ(x) =\neλx\nE(eλX) and Pλ (with expectation Eλ) the probability distribution on Ω\nwith density qλ(X) with respect to P, so that ∂λMX(λ) = Eλ(X). We have, since λt is\na maximizer, Eλt(X) = t. Moreover, fixing δ > 0,\nP( ¯XN > t) = E(1 ¯XN>t)\n≥E(1| ¯Xn−t−δ|<δ)\n≥E\n\u0010\n1| ¯Xn−t−δ|<δeNλ ¯XN−Nt−2Nδ\u0011\n= e−N(t+2δ)MX(λ)NPλ(| ¯XN −t −δ| < δ)\nIf one takes λ = λt+δ, this implies that\nP( ¯XN > t) ≥e−NM∗\nX(t+δ)e−NδPλt+δ(| ¯XN −t −δ| < δ).\nBy the law of large numbers (applied to Pλt+δ), Pλt+δ(| ¯XN −t −δ| < δ) tends to 1 when\nN tends to infinity. This implies that the logarithmic rate of convergence to 0 of\nP( ¯XN > t) is larger than N(M∗\nX(t + δ) + δ), for any δ > 0, to be compared with the\nrate NM∗\nX(t) for the upper bound. In Large Deviation theory, the upper and lower\nbounds are often simplified by considering the limit of logP( ¯XN > t)/N, which, in\nthis case, is M∗\nX(t) (and this result is called Cram´er’s therorem).\nWhile Cram´er’s upper bound is sharp, its computation requires an exact knowl-\nedge of the distribution of X, which is not a common situation. The following sec-\ntions optimize the upper bound in situations where only partial information on the\nvariable is known, such as its moments or its range. As a first example, we consider\nconcentration of the mean for sub-Gaussian variables.\n22.3.2\nSub-Gaussian variables\nIf X has exponential moments, then, (applying again Markov’s inequality)\nP(|X| > x) ≤Ce−λx\n\n22.3. CONCENTRATION INEQUALITIES\n585\nfor some positive constants C and λ. Reducing if needed the value of λ, one can\nassume that C takes some predetermined (larger than 1) value, say, C = 2, the simple\nargument being left to the reader. A random variable such that, for some λ > 0\nP(|X| > x) ≤2e−λx\nis called sub-exponential (and this property is equivalent to X having exponential\nmoments). Similarly, one says that X is sub-Gaussian if, some σ > 0,\nP(|X| > x) ≤2e−x2\n2σ2 .\n(22.5)\nSub-Gaussian random variables are such that M(λ) < ∞for all λ ∈R. Indeed, for\nλ > 0\nE(eλ|X|) =\nZ ∞\n0\nP(eλ|X| > z)dz\n= 1 +\nZ ∞\n1\nP(|X| > λ−1 logz)dz\n≤1 + 2\nZ ∞\n1\ne−(logz)2\n2σ2λ2 dz\n≤1 + 2\nZ ∞\n1\nex−\nx2\n2λ2σ2 dx\n≤1 + 2\n√\n2πλσe\nλ2σ2\n2\n.\nProposition 22.3 Assume that X is sub-Gaussian, so that (22.5) holds for some σ2 > 0.\nThen, for any t > 0, we have\nP( ¯Xn −E(X) > t) ≤\n \n1 + 4t2\nσ2\n!N\ne−Nt2\n2σ2 .\nProof Let us assume, without loss of generality, that E(X) = 0. For λ > 0, we then\nhave\nE(eλX) = 1 + E(eλX −λX −1).\nLet ϕ(t) = et −t −1. We have ϕ(t) ≥0 for all t, ϕ(0) = 0 and, for z > 0, the equation\nz = ϕ(t) has two solutions, one positive and one negative that we will denote g+(z) >\n0 > g−(z). We have\nE(ϕ(λX)) =\nZ ∞\n0\nP(ϕ(λX) > z)dz\n=\nZ ∞\n0\nP(λX > g+(z))dz +\nZ ∞\n0\nP(λX < g−(z))dz\n\n586\nCHAPTER 22. GENERALIZATION BOUNDS\nThe change of variable u = g+(z) in the first integral is equivalent to u > 0, ϕ(u) = z\nwith dz = (eu−1)du. Similarly, u = −g−(z) in the second integral gives u > 0, ϕ(−u) = z\nand dz = (1 −e−u)du so that\nE(ϕ(λX)) =\nZ ∞\n0\nP(λX > u)(eu −1)du +\nZ ∞\n0\nP(λX < −u)(1 −e−u)du\n≤\nZ ∞\n0\nP(λ|X| > u)(eu −e−u)du.\n(Using the fact that max(P(λX > u),P(λX < −u)) ≤P(λ|X| > u).) We have\nZ ∞\n0\nP(λ|X| > u)(eu −e−u)du ≤2\nZ +∞\n0\n(eu −e−u)e−\nu2\n2λ2σ2 du\n= 2λσ\nZ +∞\n0\n(eλσv −e−λσv)e−v2\n2 dv\n= 2λσe\nλ2σ2\n2 √\n2π(Φ(−σλ) −Φ(σλ))\n≤4λ2σ2e\nλ2σ2\n2\nwhere Φ is the cumulative distribution function of the standard Gaussian and we\nhave used Φ(−t) −Φ(t) ≤2t/\n√\n2π. We therefore have\nMX(λ) ≤log\n\u0012\n1 + 4λ2σ2e\nλ2σ2\n2\n\u0013\n≤λ2σ2\n2\n+ log(1 + 4λ2σ2).\nThis implies\nM∗\nX(t) = sup\nλ>0\n(λt −MX(λ)) ≥t2\nσ2 −MX(t/σ2) ≥t2\n2σ2 −log(1 + 4t2\nσ2 )\nso that\nP( ¯Xn > t) ≤\n \n1 + 4t2\nσ2\n!N\ne−Nt2\n2σ2 .\n■\nThe following result allows one to control the expectation of a non-negative sub-\nGaussian random variable.\nProposition 22.4 Let X be a non-negative random variable such that\nP(X > t) ≤Ce−t2/2σ2\nfor some constants C and σ2. Then,\nE(X) ≤3σ\np\nlogC.\n\n22.3. CONCENTRATION INEQUALITIES\n587\nProof For any α ∈(1,C], one has\nmin(1,Ce−t2/2σ2) ≤αe\n−t2 logα\n2σ2 logC ,\nwhich implies that\nE(X) =\nZ +∞\n0\nP(X > t)dt ≤\nα\n2logα\n√\n2πσ\np\nlogC\nTaking α = √e gives\nE(X) ≤\n√\nπeσ\np\nlogC ≤3σ\np\nlogC.\n■\n22.3.3\nBennett’s inequality\nThe following proposition (see [24]) provides an upper bound for MX(λ) as a func-\ntion of E(X) and var(X) under the additional assumption that X is bounded from\nabove.\nProposition 22.5 Let m = E(X) and assume that for some constant b, one has X ≤b with\nprobability one. Then, for any σ2 > 0 such that var(X) ≤σ2, one has\nE(eλX) ≤eλm\n \n(b −m)2\n(b −m)2 + σ2e−λσ2\n(b−m) +\nσ2\n(b −m)2 + σ2eλ(b−m)\n!\n(22.6)\nfor any λ ≥0.\nProof There is no loss of generality in assuming that m = 0 and λ = 1, in which case\none must show that\nE(eX) ≤\nb2\nb2 + σ2e−σ2\nb +\nσ2\nb2 + σ2eb\n(22.7)\nif X < b and E(X2) ≤σ2. Indeed, if this inequality is true for m = 0 and λ = 1, (22.6)\nin the general case will result from letting X = Y/λ+m and applying the special case\nto Y.\nThe right-hand side of (22.7) is exactly E(eX) when X follows the discrete distri-\nbution P0 supported by two points x0 and b, and such that E(X) = 0 and E(X2) = σ2,\nwhich requires x0 = −σ2/b and P(X = x0) = b2/(σ2 + b2).\nNow consider the quadratic function v(x) = αx2 + βx + γ which intersects x 7→ex\nat x = x0 and x = b, and is tangent to it at x = x0, i.e., v(b) = eb and v(x0) = v′(x0) = ex0\n(this uniquely defines v). Then ex ≤v(x) for x < b, yielding\nE(eX) ≤ασ2 + γ.\nHowever, since v(X) = eX almost surely when X ∼P0, this upper bound is attained\nand equal to that provided in (22.7).\n■\n\n588\nCHAPTER 22. GENERALIZATION BOUNDS\nIf F(λ) denotes the right-hand side of (22.6), we have, for m ≤u < b,\nM∗\nX(t) ≥sup\nλ≥0\n(λu −logF(λ))\nand we now estimate this lower bound. Maximizing λy −logF(λ) is equivalent to\nminimizing\nλ 7→(b −m)2e−λ(σ2+(u−m))\nb−m\n+ σ2eλ(b−u)\n(b −m)2 + σ2\n.\nIntroduce the notation ρ = σ2/(b −m)2, µ = λ(b −m) and x = (u −m)/(b −m), so that\nthe function to minimize is\nµ 7→e−µ(ρ+x) + ρeµ(1−x)\n1 + ρ\n.\nComputing the derivative in µ and equating it to 0 gives\nµ =\n1\n1 + ρ log\nρ + x\nρ(1 −x),\nwhich is non-negative since ρ + x −ρ(1 −x) = (1 + ρ)x. For this value of µ, we have\ne−µ(ρ+x) + ρeµ(1−x)\nρ + 1\n= e−µ(ρ+x)1 + ρeµ(1+ρ)\nρ + 1\n= e−µ(ρ+x)1 + ρ ρ+x\nρ(1−x)\nρ + 1\n= e−µ(ρ+x)\n1 −x\nand\n−log e−µ(ρ+x) + ρeµ(1−x)\nρ + 1\n= µ(ρ + x) + log(1 −x)\n= ρ + x\n1 + ρ log\nρ + x\nρ(1 −x) + log(1 −x)\n= ρ + x\n1 + ρ log ρ + x\nρ\n+ 1 −x\n1 + ρ log(1 −x).\nThis provides a lower bound for M∗\nX(m+(b−m)x), and yields the following corollary.\nCorollary 22.6 Assume that X satisfy the conditions of proposition 22.5. Then\nP( ¯XN > m + t) ≤exp\n \n−N\n ρ + x\n1 + ρ log ρ + x\nρ\n+ 1 −x\n1 + ρ log(1 −x)\n!!\n(22.8)\nwith x = t/(b −m) and ρ = σ2/(b −m)2.\n\n22.3. CONCENTRATION INEQUALITIES\n589\nBennett’s inequality is sometimes stated in a slightly weaker, but simpler form\n[127]. Returning to the proof of proposition 22.5 and using the fact that logu ≤u−1,\nequation (22.7) implies\nlogE(eX) ≤\nb2\nb2 + σ2e−σ2\nb +\nσ2\nb2 + σ2eb −1\n=\nb2\nb2 + σ2(e−σ2\nb + σ2\nb −1) +\nσ2\nb2 + σ2(eb −b −1).\nWe will use the following lemma.\nLemma 22.7 The function ϕ : u 7→(eu −u −1)/u2 is non-decreasing.\nProof We have ϕ′(u) = ψ(u)/u3 where ψ(u) = ueu −2eu + u + 2, yielding ψ′(u) =\nueu −eu + 1, ψ′′(u) = ueu. Therefore, ψ′ is has its minimum at u = 0 with ψ′(0) = 0 so\nthat ψ is increasing. Since ψ(0) = 0, we have ψ(u)/u3 ≥0.\n■\nWe therefore have\nlogE(eX) ≤\nb2\nb2 + σ2(e−σ2\nb + σ2\nb −1) +\nσ2\nb2 + σ2(eb −b −1)\n=\nb2\nb2 + σ2\nσ4\nb2 ϕ(−σ2/b) +\nσ2\nb2 + σ2b2ϕ(b)\n≤\n \nσ4\nb2 + σ2 + σ2b2\nb2 + σ2\n!\nϕ(b)\n= σ2\nb2 (eb −b −1)\nThis shows that\nlogE(eλX) ≤σ2\nb2 (eλb −λb −1)\nand\nM∗\nX(t) ≥σ2\nb2 max\nλ (λb2t/σ2 −eλb + λb + 1) = σ2\nb2 h(bt/σ2)\nwhere h(u) = (1 + u)log(1 + u) −u.\nWe summarize this in the following corollary.\nCorollary 22.8 Assume that X satisfy the conditions of proposition 22.5. Then, for t > 0,\nP( ¯XN > m + t) ≤exp\n \n−\nNσ2\n(b −m)2h\n\u0012(b −m)t\nσ2\n\u0013!\n(22.9)\nwhere h(u) = (1 + u)log(1 + u) −u.\n\n590\nCHAPTER 22. GENERALIZATION BOUNDS\nThis estimate can be further simplified as follows. Let g be such that g′′(u) =\n(1+u/3)−3 and g(0) = g′(0) = 0, which gives g(u) = u2/(2+2u/3). Noting that h′′(u) =\n(1 + u)−1 and that (1 + u)−1 ≥(1 + u/3)−3, for u ≥0 we find, integrating twice, that\nh(u) ≥g(u) for u ≥0. This shows that the following upper-bound is also true:\nP( ¯XN > m + t) ≤exp\n \n−\nNt2\n2σ2 + 2t(b −m)/3\n!\n.\n(22.10)\nThis upper bound is known as Bernstein’s inequality.\nRemark 22.9 It should be clear that, in the previous discussion, one may relax the\nassumption that X1,...,XN are identically distributed as long as there is a common\nfunction M such that MXk(λ) ≤mk + M(λ) for all k, with mk = E(Xk). We have in this\ncase\nP( ¯XN > ¯mN + t) ≤exp(−NM∗(t))\nwith ¯mN = (m1 + ··· + mN)/N and M∗(t) = supλ(λt −M(λ)). This remark can be,\nin particular, applied to the situation in which X1,...,XN satisfy the conditions of\nproposition 22.5 with the same constants b and σ2, yielding the same upper bound\nas in equation (22.8).\n♦\n22.3.4\nHoeffding’s inequality\nWe now consider the case in which the random variables X1,...,XN are bounded\nfrom above and from below, and start with the following consequence of proposi-\ntion 22.5.\nProposition 22.10 Let X be a random variable taking values in the interval [a,b]. Let\nm = E(X). Then\nE(eλX) ≤b −m\nb −a eλa + m −a\nb −a eλb ≤eλme\nλ2(b−a)2\n8\n(22.11)\nfor all λ ∈R.\nProof We first note that, if X takes values in [a,b], then var(X) ≤(b−m)(m−a) (using\nσ2 = (b −m)(m −a) in (22.6)). To prove the upper bound on the variance, introduce\nthe function g(x) = (x −a)(x −b) so that g(x) ≤0 on [a,b]. Noting that one can write\ng(x) = (x −m)2 + (2m −a −b)(x −m) + (a −m)(b −m), we have\nE(g(X)) = var(X) −(b −m)(m −a) ≤0,\nwhich proves the inequality.\nThis shows that, if λ ≥0, we can apply proposition 22.5 with σ2 = (b −m)(m −a),\nwhich provides the first inequality in (22.11). To handle the case λ ≤0, it suffices to\napply this inequality with ˜λ = −λ, ˜X = −X, ˜a = −b, ˜b = −a and ˜m = −m.\n\n22.3. CONCENTRATION INEQUALITIES\n591\nThe second inequality, namely\n b −m\nb −a eλa + m −a\nb −a eλb\n!\n≤eλme\nλ2(b−a)2\n8\nrequires a little additional work. Letting u = (m −a)/(b −a), α = λ(b −a) and taking\nlogarithms, we need to prove that\nlog(1 −u + ueα) −uα ≤α2\n8\nLet f (α) denote the difference between the right-hand side and left-hand side. Then\nf (0) = 0,\nf ′(α) = α\n4 −\nueα\n1 −u + ueα + u,\n(so that f ′(0) = 0) and\nf ′′(α) = 1\n4 −\nu(1 −u)eα\n(1 −u + ueα)2 .\nFor positive numbers x = 1 −u and y = ueα, one has (x + y)2 ≥4xy, which shows that\nf ′′(α) ≥0. This proves that f ′ is non-decreasing with f ′(0) = 0, proving that f is\nminimized at α = 0, so that f (α) ≥0 as needed.\n■\nWe can then deduce the following theorem [92].\nCorollary 22.11 (Hoeffding Inequality) If X1,...,XN are independent, taking values,\nrespectively, in intervals of length, c1,...,cN and Y = X1 + ··· + XN, then\nP(Y > E(Y) + t) ≤exp\n \n−2t2\n|c|2\n!\n(22.12)\nand\nP(Y < E(Y) −t) ≤exp\n \n−2t2\n|c|2\n!\n(22.13)\nwhere |c|2 = PN\nk=1 c2\nk.\nProof We have, by proposition 22.10, for any λ > 0\nP(Y > E(Y) + t) ≤e\n−\n\u0010\nλt−PN\nk=1 MXk (λ)\n\u0011\n≤e−(λt−λ2\n8 |c|2)\nThe upper bound is minimized for λ = 4t/|c|2, yielding (22.12). Equation (22.13) is\nobtained by applying (22.12) to −X.\n■\n\n592\nCHAPTER 22. GENERALIZATION BOUNDS\nAn important special case of this inequality is when X1,...,XN are i.i.d. taking\nvalues in an interval of length δ. Then\nP( ¯XN > E(X) + t) ≤exp\n \n−2Nt2\nδ2\n!\n.\n(22.14)\nThis inequality is obtained after applying Hoeffding’s inequality to X1/N,...,XN/N,\ntherefore taking c1 = ··· = cN = δ/N and |c|2 = δ2/N.\n22.3.5\nMcDiarmid’s inequality\nOne can relax the assumption that the random variables X1,...,XN are independent\nand only assume that these variables behave like “martingale increments,” as stated\nin the following proposition [59].\nProposition 22.12 Let X1,...,XN, Z1,...,ZN be two sequences of N random variables\nsuch that\nE(Zk | X1,Z1,...,Xk−1,Zk−1) = mk\nis constant and |Zk −mk| ≤ck for some constants c1,...,cN. Then\nP(Y > E(Y) + t) ≤e−2t2/|c|2\nwith Y = Z1 + ··· + ZN and |c|2 = PN\nk=1 c2\nk.\nProof Proposition 22.10 applied to the conditional distribution implies that, for\nλ ≥0:\nlogE(eλ(Zk−mk) | X1,Z1,...,Xk−1,Zk−1) ≤logE(eλ|Zk−mk| | X1,Z1 ...,Xk−1,Zk−1) ≤λ2c2\nk\n8\n.\nLet Sk = Pk\nj=1(Zj −mj). Then\nE(eλSk) = E(eλSk−1E(eλ(Zk−mk) | X1,Z1,...,Xk−1,Zk−1)) ≤e\nλ2c2\nk\n8 E(eλSk−1)\nso that\nE(eλSN) ≤e\nλ2\n8\nPN\nk=1 c2\nk\nand the result follows from Markov’s inequality optimized over λ.\n■\nWe will use this proposition to prove the “bounded difference,” or McDiarmid’s\ninequality.\n\n22.3. CONCENTRATION INEQUALITIES\n593\nTheorem 22.13 (McDiarmid’s inequality) Let X1,...,XN be independent random vari-\nables and g : RN →R a function such that there exists c1,...,cN such that\n|g(x1,...,xk−1,xk,xk+1,...,xN) −g(x1,...,xk−1, ˜xk,xk+1,...,xN)| ≤ck\n(22.15)\nfor all k = 1,...,N and x1,...,xk−1,xk, ˜xk,xk+1,...,xN. Then\nP(g(X1,...,XN) > E(g(X1,...,XN)) + t) ≤e−2t2/|c|2\nwith |c|2 = c2\n1 + ··· + c2\nN.\nProof Let m = E(g(X1,...,XN)). Let Z0 = 0,\nYk = E(g(X1,...,XN) | X1,...,Xk) −m\nand Zk = Yk −Yk−1. Note that Zk is a function of X1,...,Xk and can therefore be\nomitted from the conditional expectation given (X1,Z1,...,Xk−1,Zk−1).\nWe have E(Yk) = 0 and E(Yk | X1,...,Xk−1) = Yk−1 so that E(Zk | X1,...,Xk−1) = 0.\nBecause the variables are independent, we have, letting ˜X1,..., ˜XN be independent\ncopies of X1,...,XN,\nZk = E(g(X1,...,Xk−1,Xk, ˜Xk+1,..., ˜XN) | X1,...,Xk)\n−E(g(X1,...,Xk−2, ˜Xk−1, ˜Xk,..., ˜XN) | X1,...,Xk−1).\nFor fixed X1,...,Xk−1, (22.15) implies that Zk varies in an interval of length ck at most\n(whose bounds depend on X1,...,Xk−1) so that |Zk −E(Zk)| ≤ck. Proposition 22.12\nimplies that\nP(Z1 + ··· + ZN ≥t) ≤e−2t2/|c|2,\nwhich concludes the proof since\nZ1 + ··· + ZN = g(X1,...,XN) −E(g(X1,...,XN)).\n■\n22.3.6\nBoucheron-Lugosi-Massart inequality\nThe following result [38], that we state without proof, extends on the same idea.\nTheorem 22.14 Let X1,...,XN be independent random variables. Let\nZ = g(X1,...,XN)\nwith g : RN →[0,+∞) and for k = 1,...,N,\nZk = gk(X1,...,Xk−1,Xk+1,...,XN)\n\n594\nCHAPTER 22. GENERALIZATION BOUNDS\nwith gk : RN−1 →R. Assume that, for all k = 1,...,N, one has 0 ≤Z −Zk ≤1 and that\nN\nX\nk=1\n(Z −Zk) ≤Z.\nThen\nP(Z −E(Z) > t) ≤exp(−E(Z)h(t/E(Z))) ≤exp\n \n−\nt2\n2E(Z) + 2t/3\n!\nwhere h(u) = (1 + u)log(1 + u) −u. Moreover, for t < E(Z),\nP(Z −E(Z) < −t) ≤exp(−E(Z)h(−t/E(Z))) ≤exp(−t2/2E(Z)).\nFinally, for all λ ∈R\nlogE(eλ(Z−E(Z))) ≤E(Z)(eλ −λ −1).\n(22.16)\n22.4\nBounding the empirical error with the VC-dimension\n22.4.1\nIntroduction\nSection 22.3 provides some of the most important inequalities used to evaluate the\ndeviation of various combinations of independent random variables (e.g., their em-\npirical mean) from their expectations (the reader may refer to Ledoux and Talagrand\n[117], Devroye et al. [60], Talagrand [188], Dembo and Zeitouni [59], Vershynin [199]\nand other textbooks on the subject for further developments).\nWe now return to the problem of estimating the generalization error based on\ntraining data. For a given predictor f , concentration bounds allow us to control the\nprobability\nP(R(f ) −ˆRT (f ) > t)\nwhere\nR(f ) = E(r(Y,f (X))\nand\nˆRT (f ) = 1\nN\nN\nX\nk=1\nr(yk,f (xk))\nfor a training set T = (x1,y1,...,xN,yN).\nIf this probability is small, then R(f ) ≤ˆRT (f )+t with high probability, providing\na likely upper bound to the generalization error of f . For example, if r is the 0–1\n\n22.4. BOUNDING THE EMPIRICAL ERROR WITH THE VC-DIMENSION\n595\nloss in a classification problem, Hoeffding’s inequality implies, for training sets of\nsize N,\nP(R(f ) −ˆRT (f ) > t) ≤e−2Nt2 .\nNow corollary 22.11 does not hold if we replace f by ˆfT , i.e., if f is estimated\nfrom the training set T , which is, unfortunately, the situation we are interested in.\nBefore addressing this problem, we point out that this inequality does apply to the\ncase in which f = ˆfT0 where T0 is another training set, independent from T , so that\nP(R( ˆfT0) −ˆRT ( ˆfT0) > t) ≤e−2Nt2 ,\nwhich is proved by writing\nP(R( ˆfT0) −ˆRT ( ˆfT0) > t) = E(P(R( ˆfT ) −ˆRT ( ˆfT ) |> t|T0 = T )).\nIn this situation, the empirical risk is computed on a test or validation set (T ) inde-\npendent of the set used to estimate f (T0).\nIf one does not have a test set, and ˆfT is optimized over a set F of possible pre-\ndictors, one can rarely do much better than starting from a variation of the trivial\nupper bound\nP(R( ˆfT ) −ET > t) ≤P\n\u0012\nsup\nf ∈F\n(R(f ) −ET (f )) > t\n\u0013\n(with ET = ˆRT ( ˆfT )) and the concentration inequalities discussed in section 22.3 need\nto be extended to provide upper bounds to the right-hand side.\nRemark 22.15 Computing supremums of functions over non countable sets may\nbring some issues regarding measurability. To avoid complications, we will always\nassume, when computing supremums over infinite sets, that such supremums can\nbe reduced to maximizations over finite sets, i.e., when considering supf ∈F Φ(f ) for\nsome function Φ, we will assume that there exists a nested sequence of finite subsets\nFn ⊂F such that\nsup{Φ(f ) : f ∈F } = lim\nn→∞sup{Φ(f ) : f ∈Fn}.\n(22.17)\nThis is true, for example, when F has a topology that admits a countable dense\nsubset, with respect to which Φ is continuous.\n♦\nWhen F is a finite set, one can use a ”union bound” with\nP(sup\nf ∈F\n(R(f ) −ET (f )) > t) ≤\nX\nf ∈F\nP(R(f ) −ET (f ) > t) ≤|F |max\nf ∈F P(R(f ) −ET (f ) > t).\n\n596\nCHAPTER 22. GENERALIZATION BOUNDS\nSuch bounds cannot be applied to the typical case in which F is infinite, and is\nlikely to provide very poor estimates even when F is finite, but |F | is large. How-\never, all proofs of concentration inequalities applied to such supremums require us-\ning a union bound at some point, often after considerable preparatory work. Union\nbounds will in particular appear in conjunction with the Vapnik-Chervonenkis di-\nmension that we now discuss.\n22.4.2\nVapnik’s theorem\nWe consider a classification problem with two classes, 0 and 1, and therefore let F\nbe a set of binary functions, i.e., taking values in {0,1}. We also assume that the risk\nfunction r takes values in the interval [0,1] (using, for example, the 0–1 loss). Let\nU(t) = P\n\u0012\nsup\nf ∈F\n(R(f ) −ET (f )) > t\n\u0013\n.\n(22.18)\nA fundamental theorem of Vapnik provides an estimate of U(t) based on the\nnumber of possible ways to split a training set of 2N points into two classes using\nfunctions in F . The rest of this section is devoted to a discussion of this result and\nrelated notions.\nIf A is a finite subset of R, we let F (A) denote the set {f|A : f ∈F } of restrictions\nof elements of F to the set A. As a convention, we let F (∅) = {f∅}, containing the so-\ncalled empty function. Since F only contains binary functions, we have |F (A)| ≤2|A|.\nIf x1,...,xM ∈R, we let, with a slight abuse of notation,\nF (x1,...,xM) = F (A)\nwhere A = {xi,i = 1,...,M}. This provides the number of possible splits of a training\nset T = (x1,...,xM) using classifiers in F . Fixing in this section a random variable X,\nwe let\nSF (M) = E(|F (X1,...,XM)|)\nwhere the expectation is taken over all M i.i.d. realizations from X. We also let\nS∗\nF (M) = max{|F (A)| : A ⊂R,|A| ≤M}.\nThe following theorem controls U in (22.18) in terms of SF .\nTheorem 22.16 (Vapnik) With the notation above, one has, for t ≥\n√\n2/N:\nP\n\u0012\nsup\nf ∈F\n(R(f ) −ET (f )) > t\n\u0013\n≤2SF (2N)e−Nt2/8,\n(22.19)\n\n22.4. BOUNDING THE EMPIRICAL ERROR WITH THE VC-DIMENSION\n597\nwhich implies that, with probability at least 1 −δ, we have\n∀f ∈F : R(f ) ≤ET (f )) +\nr\n8\nN\n\u0012\nlogSF (N) + log 2\nδ\n\u0013\n(22.20)\n(The requirement that t ≥\n√\n2/N does not really reduce the range of applicability\nof (22.19), since, for t ≤\n√\n2/N, the upper bound in that equation is typically much\nlarger than 1.)\nProof We first show that the problem can be symmetrized with the inequality, valid\nif Nt2 ≥2,\nP\n\u0012\nsup\nf ∈F\n(R(f ) −ET (f )) ≥t\n\u0013\n≤2P\n\u0012\nsup\nf ∈F\n(ET ′(f ) −ET (f )) ≥t\n2\n\u0013\n(22.21)\nin which T ′ is a second training set (independent of T ) with N samples also. In view\nof assumption (22.17), there is no loss of generality in assuming that F is finite.\nAssociate to any training set T , a classifier fT ∈F maximizing R(fT ) −E(fT ). One\nthen has\nP\n\nsup\nf ∈F\n(ET ′(f ) −ET (f )) ≥t\n2\n\n≥P\n\u0012\n(ET ′(fT ) −ET (fT )) ≥t\n2\n\u0013\n≥P\n\u0012\n(R(fT ) −ET ′(fT ) ≤t\n2 and R(fT ) −ET (fT )) ≥t\n\u0013\n= E\n\u0012\n1R(fT )−ET (fT ))≥tP\n\u0012\nR(fT ) −ET ′(fT ) ≤t\n2\n\f\f\f T\n\u0013\u0013\nConditional to T , ET ′(fT ) is the average of M i.i.d. Bernoulli random variables, with\nvariance bounded from above by 1/4 and\nP\n\u0012\nR(fT ) −ET ′(fT ) ≤t\n2\n\f\f\f T\n\u0013\n≥1 −\n1/4\nNt2/4 ≥1\n2 .\nIt follows that\nP\n\u0012\nsup\nf ∈F\n(ET ′(f ) −ET (f )) > t\n2\n\u0013\n≥1\n2P\n\u0012\nR(fT ) −ET (fT )) ≥t\n\u0013\n= 1\n2P\n\u0012\nsup\nf ∈F\n(R(f ) −ET (f )) ≥t\n\u0013\n.\nThis justifies (22.21).\nNow consider a family of independent Rademacher random variables ξ1,...,ξN,\nalso independent of T and T ′, taking values −1 and +1 with equal probability. By\nsymmetry,\nsup\nf ∈F\n(ET ′(f ) −ET (f )) = sup\nf ∈F\nN\nX\nk=1\n(r(Yk,f (Xk)) −r(Y ′\nk,f (X′\nk)))/N\n\n598\nCHAPTER 22. GENERALIZATION BOUNDS\nhas the same distribution as\nsup\nf ∈F\nN\nX\nk=1\nξk(r(Yk,f (Xk)) −r(Y ′\nk,f (X′\nk)))/N .\nNow, there are at most |F (X1,...,XN,X′\n1,...,X′\nN)| different sets of coefficients in front\nof ξ1,...,ξN in the above sum when f varies in F , so that, conditioning on T ,T ′ and\ntaking a union bound , we have\nP\n\u0012\nsup\nf ∈F\nN\nX\nk=1\nξk(r(Yk,f (Xk)) −r(Y ′\nk,f (X′\nk)))/N ≥t/2\n\f\f\f\f T ,T ′\u0013\n≤|F (X1,...,XN,X′\n1,...,X′\nN)|\nsup\nf ∈F\nP\n\u0012 N\nX\nk=1\nξk(r(Yk,f (Xk)) −r(Y ′\nk,f (X′\nk)))/N ≥t/2\n\f\f\f\f T ,T ′\u0013\nThe variables ξk(r(Yk,f (Xk))−r(Y ′\nk,f (X′\nk)) are centered and belong to the interval\n[−1,1], which has length 2, so that Hoeffding’s inequality implies\nP\n\u0012 N\nX\nk=1\nξk(r(Yk,f (Xk)) −r(Y ′\nk,f (X′\nk)))/N ≥t/2\n\f\f\f\f T ,T ′\u0013\n≤e−2N(t/2)2/4 = e−Nt2/8\nand taking expectation over T and T ′ yields\nP\n\nsup\nf ∈F\n(R(f ) −ET (f )) ≥t\n\n= 2SF (2N)e−Nt2/8.\nEquation (22.20) is then obtained from letting δ = 2SF (2N)e−Nt2/8 so that t =\nq\n8\nN log 2SF (2N)\nδ\nwith R(f ) ≤ET (f ) + t for all f with probability 1 −δ or more.\n■\n22.4.3\nVC dimension\nTo obtain a practical bound, the quantity SF (2N), or its upper-bound S∗\nF (2N), needs\nto be estimated. We prove below an important property of S∗\nF , namely that, either\nS∗\nF (M) = 2M for all M, or there exists an M0 for which S∗\nF (M0) < 2M0, and taking\nM0 to be the largest one for which an equality occurs, S∗\nF (M) has order MM0 for all\nM ≥M0. This motivates the following definition of the VC-dimension of the model\nclass.\n\n22.4. BOUNDING THE EMPIRICAL ERROR WITH THE VC-DIMENSION\n599\nDefinition 22.17 The Vapnik-Chervonenkis dimension (or VC dimension) of the model\nclass F is\nVC-dim(F ) = max{M : S∗\nF (M) = 2M}.\n(where the infimum of an empty set is +∞).\nRemark 22.18 If, for a finite set A ⊂R, one has |F (A)| = 2|A|, one says that A is\nshattered by F . So VC-dim(F ) is the largest integer M such that there exists a set of\ncardinality M in R that is shattered by F .\n♦\nWe now evaluate the growth of S∗\nF (M) in terms of the VC-dimension, starting\nwith the following lemma, which states that, if A is a finite subset of R, there are at\nleast |F (A)| subsets of A that are shattered by F .\nLemma 22.19 (Pajor) Let A be a finite subset of R. Then\n|F (A)| ≤|{B ⊂A : |F (B)| = 2B}|.\nProof The statement holds for A = ∅, for which |F∅| = 1 = 20. For |A| = 1, the upper-\nbound is either 1 if |F (A)| = 1, or 2 if |F (A)| = 2, and the collection of sets B ⊂A such\nthat |F (B)| = 2B is {∅} in the first case and {∅,A} in the second one. So, the statement\nis true for |A| = 0 or 1.\nProceeding by induction, assume that the result is true if |A| ≤N, and consider a\nset A′ with |A′| = N +1. Assume that |F (A′)| ≥2 (otherwise there is nothing to prove),\nwhich implies that there exists x ∈A′ such that |F (x)| = 2. Take such an x and write\nA′ = A ∪{x} with x < A. Let\nF0 = {f ∈F : f (x) = 0} and F1 = {f ∈F : f (x) = 1}.\nSince F0 ∩F1 = ∅, we have\n|F (A′)| = |F0(A′)| + |F1(A′)|.\nSince f (x) is constant on F0 (resp. F1), we have |F0(A′)| = |F0(A)| (resp. |F1(A′)| =\n|F1(A)|), and the induction hypothesis implies\n|F (A′)| ≤|{B ⊂A : |F0(B)| = 2B}| + |{B ⊂A : |F1(B)| = 2B}|\n= |{B ⊂A : |F0(B)| = 2B or |F0(B)| = 2B}|\n+ |{B ⊂A : |F0(B)| = |F1(B)| = 2B}|.\nIf B ⊂A is shattered by F0 or F1, it is obviously shattered by F . Moreover, if B is\nshattered by both, then B ∪{x} is shattered by F . The upper bound in the equation\nabove is therefore less than the total number of sets shattered by F , which proves\nthe lemma.\n■\n\n600\nCHAPTER 22. GENERALIZATION BOUNDS\nFrom this lemma, it results that if VC-dim(F ) = D < ∞, then S∗\nF (M) is bounded\nby the total number of subsets of cardinality D or less in a set of cardinality M. This\nprovides the following result, which implies that the term in front of the exponential\nin (22.18) grows polynomially in N if F have finite VC-dimension.\nProposition 22.20 (Sauer-Shelah’s lemma) If D is the VC-dimension of F , then, for\nN ≥D,\nS∗\nF (N) ≤\n\u0012eN\nD\n\u0013D\n.\nProof Pajor’s lemma implies that\nS∗\nF (N) ≤\nD\nX\nk=0\n N\nk\n!\nand the statement of the proposition derives from the standard upper bound\nD\nX\nk=0\n N\nk\n!\n≤\n\u0012eN\nD\n\u0013D\nthat we now justify for completeness. We have\n N\nk\n!\n=\nN!\n(N −k)!k! ≤N k\nk! ≤N D\nDD\nDk\nk!\nif k ≤D ≤N. This yields\nD\nX\nk=0\n N\nk\n!\n≤N D\nDD\nD\nX\nk=0\nDk\nk! ≤N DeD\nDD\nas required.\n■\nWe can therefore state a corollary to theorem 22.16 for model classes with finite\nVC-dimension.\nCorollary 22.21 Assume that VC-dim(F ) = D < ∞. Then, for t ≥\n√\n2/N and N ≥D,\nP\n\u0012\nsup\nf ∈F\n(R(f ) −ET (f )) > t\n\u0013\n≤2\n\u00122eN\nD\n\u0013D\ne−Nt2/8.\n(22.22)\nand\nP\n\u0012\nsup\nf ∈F\n(R(f ) −ET (f )) ≤\nr\n8\nN\nr\nD log eN\nD + log 2\nδ\n\u0013\n≥1 −δ.\n(22.23)\n\n22.4. BOUNDING THE EMPIRICAL ERROR WITH THE VC-DIMENSION\n601\n22.4.4\nExamples\nThe following result provides the VC-dimension of the collection of linear classifiers.\nProposition 22.22 Let R = Rd and F =\nn\nx 7→sign(a0 + bT x) : β0 ∈R,b ∈Rdo\n. Then\nVC-dim(F ) = d + 1.\nProof Let us show that no set of d+2 points can be shattered by F . Use the notation\n˜x = (1,xT )T and β = (a0,bT )T , and consider d + 2 points x1,...,xd+2. Then ˜x1,..., ˜xd+2\nare linearly dependent and one of them, say, ˜xd+2 can be expressed as a linear com-\nbination of the others. Write\n˜xd+2 =\nd+1\nX\nk=1\nαk ˜xk .\nThen there is no function f ∈F (taking the form ˜x 7→sign(β ˜x)) that maps (x1,...,xd+2)\nto (sign(α1),...,sign(αd+1),−1) (where the definition of sign(0) = ±1 is indifferent),\nsince any such function satisfies\nβT ˜xd+2 =\nd+1\nX\nk=1\nαkβT ˜xk > 0.\nThis proves VC-dim(F ) < d +2. To prove that VC-dim(F ) = d +1, it suffices to exhibit\na set of d + 1 vectors in Rd that can be shattered by F . Choose x1,...,xd+1 such\nthat ˜x1,..., ˜xd+1 are linearly independent (for example xi = Pi−1\nk=1 ei, where (e1,...,ed)\nis the canonical basis of Rd). This linear independence implies that, for any vector\nα = (α1,...,αd+1)T ∈Rd+1, there exists a vector β ∈Rd+1 such that ˜xT\ni β = αi for all\ni = 1,...,d + 1. This shows that any combination of signs for ˜xT\ni β can be achieved, so\nthat (x1,...,xd+1) is shattered.\n■\nUpper-bounds on VC dimensions of more complex models have also been pro-\nposed in the literature. As an example, the following theorem, that we provide with-\nout proof, considers feed-forward neural networks with piecewise linear units (such\nas ReLU, see chapter 11). This theorem is a special case of Theorem 7 in Bartlett et al.\n[21], in which the more general case of networks with piecewise polynomial units is\nprovided. Given integers L, U1,...,UL and W1,...,WL, define the function class\nF (L,(Ui),(Wi),p)\nthat consists of feed-forward neural networks with L layers, Ui piecewise linear com-\nputational units with less than p pieces in the ith layer, and such that the total num-\nber of parameters involved in layers 1,2,...,j is less than Wj.\n\n602\nCHAPTER 22. GENERALIZATION BOUNDS\nTheorem 22.23\nVC-dim(F (L,(Ui),(Wi),p)) = O(¯LWL log(pU)).\nwhere U = U1 + ··· + UL and\n¯L = 1\nWL\nL\nX\nj=1\nWj.\nNote that p = 2 for ReLU networks. Theorem 7 in Bartlett et al. [21] also provides a\nmore explicit upper bound, namely\nVC-dim(F (L,(Ui),(W),p)) ≤L + ¯LWL log2\n\n4ep\nL\nX\ni=1\niUi log2\n\n\nL\nX\ni=1\n(2epiUi)\n\n\n\n.\n22.4.5\nData-based estimates\nApproximations of the shattering numbers can be computed using training data.\nOne can, in particular, prove a concentration inequality [38] on logSF (X1,...,XN),\nwhich may in turn be used to estimate log(SF (2N)). In the following, we let HVC(N,F )\ndenote the expectation of logSF (X1,...,XN). It is often referred to as the VC entropy\nof F .\nTheorem 22.24 One has, letting HVC = HVC(N,F ):\nP(logSF (X1,...,XN) ≥HVC + t) ≤exp\n \n−\nt2\n2HVC + 2t/3\n!\nand\nP(logSF (X1,...,XN) ≤HVC −t) ≤exp\n \n−\nt2\n2HVC\n!\nProof We show that the random variable Z = log2 SF (X1,...,XN) satisfies the as-\nsumptions of theorem 22.14, with\nZk = log2 SF (X1,...,Xk−1,Xk+1,...,XN).\nClearly, 0 ≤Z, 0 ≤Z −Zk ≤1, because one can do no more than double SF by adding\none point. We need to show that\nN\nX\nk=1\n(Z −Zk) ≤Z.\n(22.24)\nNote that Z is the base-two entropy of the uniform distribution, π, on the set\nF (X1,...,XN) ⊂{−1,1}N.\nWe will use the following lemma.\n\n22.4. BOUNDING THE EMPIRICAL ERROR WITH THE VC-DIMENSION\n603\nLemma 22.25 Let A be a finite set and ψ a probability distribution on AN. Let ψk be its\nmarginal when the kth variable is removed. Then:\nN\nX\nk=1\nH2(ψk) −(N −1)H2(ψ) ≥0.\n(22.25)\nThis lemma is a special case of a collection of results on non-negative entropy mea-\nsures developed in Han [86], and we provide a direct proof below for completeness.\nGiven the lemma, let πk denote the marginal distribution of π when the kth\nvariable is removed, i.e.,\nπk(ϵ1,...ϵk−1,ϵk+1,...,ϵN)\n= π(ϵ1,...ϵk−1,−1,ϵk+1,...,ϵN) + π(ϵ1,...ϵk−1,1,ϵk+1,...,ϵN).\nWe have:\nN\nX\nk=1\n(H2(π) −H2(πk)) ≤H(π)\nfrom which (22.24) derives since Z = H2(π) and Zk ≥H2(πk). The result then follows\nfrom theorem 22.14.\nWe now prove lemma 22.25 by induction (this proof requires some basic notions\nof information theory). For convenience, introduce random variables (ξ1,...,ξN)\nsuch that ξk ∈A, with joint probability distribution given by ψ. Let Y = (ξ1,...,ξN),\nY (k) the (N −1)-tuple formed from Y by removing ξk, Y (k,l) the (N −2)-tuple obtained\nby removing ξk and ξl, etc. Inequality (22.25) can then be rewritten\nN\nX\nk=1\nH2(Y (k)) −(N −1)H2(Y) ≥0.\nThis inequality is obviously true for N = 1, and it is true also for N = 2 since it gives\nin this case the well-known inequality H2(Y1,Y2) ≤H2(Y1) + H2(Y2). Fix M > 2 and\nassume that the lemma is true for any N < M. To prove the statement for N = M,\nwe will use the following inequality, which holds for any three random variables\nU1,U2,U3:\nH2(U1,U3) + H2(U2,U3) ≥H2(U1,U2,U3) + H2(U3).\nThis inequality is equivalent to the statement on conditional entropies that H2(U1,U2 |\nU3) ≤H2(U1 | U3) + H2(U2 | U3). We apply it, for given k , l, to U1 = Yl, U2 = Yk,\nU3 = Y (k,l), yielding\nH2(Y (k)) + H2(Y (l)) ≥H2(Y) + H2(Y (k,l)).\n\n604\nCHAPTER 22. GENERALIZATION BOUNDS\nWe now sum over all pairs k , l, yielding\n2(N −1)\nN\nX\nk=1\nH2(Y (k)) ≥N(N −1)H2(Y) +\nX\nk,l\nH2(Y (k,l)).\nWe finally use the induction hypothesis to write that, for all k\nX\nl,k\nH2(Y (k,l)) ≥(N −2)H2(Y (k))\nand obtain\n2(N −1)\nN\nX\nk=1\nH2(Y (k)) ≥N(N −1)H2(Y) + (N −2)\nN\nX\nk=1\nH2(Y (k)),\nwhich provides the desired result after rearranging the terms.\n■\nNote that theorem 22.16 involves SF (2N), with:\nlog2(SF (2N)) = log2E(SF (X1,...,X2N)) ≥HVC(2N,F )\nfrom Jensen’s inequality. This implies that the high-probability upper bound on\nHVC(2N,F ) that results from the previous theorem is not necessarily an upper bound\non log(SF (2N)). It is however proved in Boucheron et al. [38] that\nlog2E(SF (X1,...,X2N)) ≤\n1\nlog2HVC(2N,F )\nalso holds (as a consequence of (22.16)). A little more work (see Boucheron et al. [38])\ncombining theorem 22.16 and theorem 22.24 implies the following bound, which\nholds with probability 1 −δ at least:\n∀f ∈F : R(f ) ≤E(f ) +\nr\n6logSF (X1,...,XN)\nN\n+ 4\nr\nlog(2/δ)\nN\n.\n22.5\nCovering numbers and chaining\nThe upper bounds using the VC dimension relied on the number of different values\ntaken by a set of functions when evaluated on a finite set, this number being used to\napply a union bound. A different point of view may be applied when one relies on\nsome notion of continuity of the family of functions on which a uniform concentra-\ntion bound is needed, with respect to a given metric. This viewpoint is furthermore\napplicable when the sets F (X1,...,XN) are infinite. To develop these tools, we will\nneed some new concepts measuring the size of sets in a metric space.\n\n22.5. COVERING NUMBERS AND CHAINING\n605\n22.5.1\nCovering, packing and entropy numbers\nDefinition 22.26 Let (G,ρ) be a metric space and let ϵ > 0. The ϵ-covering number of\n(G,ρ). denoted N (G,ρ,ϵ), is the smallest integer n such that there exists a subset G ⊂G\nsuch that |G| = n and maxg∈G ρ(g,G) ≤ϵ.\nLet γ > 0. The γ-packing number M(G,ρ,γ), is the largest number n such that there\nexists a subset A ⊂G with cardinality n such that any two distinct elements of A are at\ndistance strictly larger than γ (such sets are called γ-nets).\nWhen G and ρ are well understood from the context, we will write simply N (ϵ) and\nM(γ).\nProposition 22.27 One has, for any γ > 0:\nM(G,ρ,2γ) ≤N (G,ρ,γ) ≤M(G,ρ,γ).\nProof Let A be a maximal γ-net. Then, for all x ∈G, there exists y ∈A such that\nρ(x,y) ≤γ: otherwise A∪{x} would also be a γ −net. This shows that max(ρ(x,A),x ∈\nG) ≤γ and N (G,ρ,γ) ≤|A|.\nConversely, let A be a 2γ-net. Let G be an optimal γ-covering. Associate to each\ny ∈A a point x ∈G at distance less than γ: at least one exists because G is a covering.\nThis defines a function f : A →G, which is necessarily one-to-one, because if two\npoints in A map to the same point in G, the distance between these two points would\nbe less than or equal to 2γ. This shows that M(G,ρ,2γ) ≤N (G,ρ,γ).\n■\nThe entropy numbers of (G,ρ), denoted, for an integer N, e(G,ρ,N) (or just e(N))\nrepresent the best accuracy that can be achieved by subsets of G of size N, namely\ne(G,ρ,N) =\nmin\nG⊂G,|G|=N max{ρ(g,G) : g ∈G}.\n(22.26)\nWe have:\ne(G,ρ,N) = inf{ϵ : N(G,ρ,ϵ) ≤N}\n(22.27a)\nand\nN(G,ρ,ϵ) = min{N : e(G,ρ,N) ≤ϵ}.\n(22.27b)\n22.5.2\nA first union bound\nLet Z be a random variable Z : Ω→Z. We will consider a space G of functions\ng : Z →R, such that (to simplify the discussion) E(g(Z)) = 0 for all g ∈G. In this\nsection, we assume that functions in G are bounded and let\nρ∞(g,g′) = sup\nz∈Z\n|g(z) −g′(z)|.\n\n606\nCHAPTER 22. GENERALIZATION BOUNDS\nAssume that N (G,ρ∞,ϵ) < ∞, for all ϵ > 0 (which requires the set G to be pre-\ncompact for the ρ∞metric). Take t > 0, 0 < ϵ < t and choose a set G ⊂G such that\n|G| = N (G,ρ∞,ϵ). Then, using a union bound,\nP(sup\ng∈G\ng(Z) ≥t) ≤P(sup\ng∈G\ng(Z) ≥t −ϵ)\n(22.28)\n≤N (G,ρ∞,ϵ) sup\ng∈G\nP(g(Z) ≥t −ϵ).\nNow, if each function in G satisfies a concentration inequality, say,\nP(g(Z) ≥u) ≤e−u2\n2µ(g)\nfor some µ(g) > 0, then, assuming that µ(G)\n∆= maxg∈G µ(g) is finite, we find that, for\n0 < ϵ < t,\nP(sup\ng∈G\ng(Z) ≥t) ≤N (G,ρ∞,ϵ)e−(t−ϵ)2\n2µ(G) .\nWe now apply this inequality to the case of binary classification, where a binary\nvariable Y is predicted by an input variable X, with a model class of classifiers F\nand the 0–1 loss function. If A is a finite family of elements of R, we define, for\nf ,f ′ ∈F\nρA(f ,f ′) = 1\n|A|\nX\nx∈A\n1f (x),f ′(x).\nLet\n¯\nN (F ,ϵ,N) = E\n\u0010\nN (F ,ρ{X1,...,XN},ϵ)\n\u0011\nwhere X1,...,XN is an i.i.d. sample of X. We then have the following proposition.\nProposition 22.28 For all ϵ > 0, one has\nP\n\u0012\nsup\nf ∈F\n(R(f ) −ET (f )) ≥t\n\u0013\n≤2 ¯\nN (F ,ϵ/2,N)e−N(t/2−ϵ)2\n4\n.\n(22.29)\nProof A key step in the proof of theorem 22.16, was to show that\nP\n\u0012\nsup\nf ∈F\n(R(f )−ET (f )) ≥t\n\u0013\n≤2P\n\u0012\nsup\nf ∈F\nN\nX\nk=1\nξk(r(Y ′\nk,f (X′\nk))−r(Yk,f (Xk))) ≥Nt/2\n\u0013\n. (22.30)\nwhere ξ1,...,ξN are Rademacher random variables and T ,T ′ are two independent\ntraining sets of size N. We start from this inequality and bound the conditional\nexpectation\nP\n\u0012\nsup\nf ∈F\nN\nX\nk=1\nξk(r(Y ′\nk,f (X′\nk)) −r(Yk,f (Xk))) ≥Nt/2\n\f\f\f\f T ,T ′\u0013\n(22.31)\n\n22.5. COVERING NUMBERS AND CHAINING\n607\nand therefore consider r(Y ′\nk,f (X′\nk)) −r(Yk,f (Xk)) as constants that we will denote\nck(f ). Since we are using a 0–1 loss, we have ck(f ) ∈{−1,0,1} and, for f ,f ′ ∈F ,\n|ck(f ) −ck(f ′)| ≤1f (Xk),f ′(Xk) + 1f (X′\nk),f ′(X′\nk) .\n(22.32)\nConsider the random variable Z = (ξ1,...,ξN), and let\nG =\nn\ngf ,f ∈F\no\nwith\ngf (ξ1,...,ξN) = 1\nN\nN\nX\nk=1\nck(f )ξk .\nWe have\nρ∞(gf ,gf ′) = 1\nN\nN\nX\nk=1\n|ck(f ) −ck(f ′)|.\nApplying Hoeffding’s inequality, we have, for u > 0 and using the fact that ck ∈[−1,1]\nP(gf (Z) > u | T ,T ′) ≤e−2Nu2\n4\n= e−Nu2\n2\nand the discussion preceding the theorem yields the fact that, for any ϵ > 0:\nP(sup\nf ∈F\ngf (Z) > t/2 | T ,T ′) ≤N (G,ϵ,ρ∞)e−N(t/2−ϵ)2\n2\n.\n(22.33)\nLet A = (X1,...,XN,X′\n1,...,X′\nN) so that\nρA(f ,f ′) = 1\n2N\nN\nX\nk=1\n\u0010\n1f (Xk),f ′(Xk) + 1f (X′\nk),f ′(X′\nk)\n\u0011\n.\nUsing (22.32), we have ρ∞(gf ,gf ′) ≤2ρA(f ,f ′), which implies\nN (G,ϵ,ρ∞) ≤N (F ,ϵ/2,ρA).\nUsing this in (22.33) and taking the expectation in (22.31), we get\nP\n\u0012\nsup\nf ∈F\n(R(f ) −ET (f )) ≥t\n\u0013\n≤2 ¯\nN (F ,ϵ/2,N)e−N(t/2−ϵ)2\n2\n(22.34)\nwhich is valid for all ϵ > 0.\n■\nOne can retrieve the bound obtained in theorem 22.16 using the obvious fact that\nN (F ,ϵ,ρA) ≤|F (A)|,\n\n608\nCHAPTER 22. GENERALIZATION BOUNDS\nfor any A ⊂R, so that\nP\n\u0012\nsup\nf ∈F\n(R(f ) −ET (f )) ≥t\n\u0013\n≤2S(F ,2N)e−N(t/2−ϵ)2\n2\nfor any ϵ > 0, and letting ϵ go to zero,\nP\n\u0012\nsup\nf ∈F\n(R(f ) −ET (f )) ≥t\n\u0013\n≤2S(F ,2N)e−Nt2\n8 .\nSo (22.29) provides a family of equations that depend on a parameter ϵ which, in\nthe limit ϵ →0, includes theorem 22.16 as a particular case. For a given N, optimiz-\ning (22.29) over ϵ may give a better upper bound, provided one has a good way to\nestimate ¯\nN (F ,ϵ/2,N) (which is, of course, far from obvious).\n22.5.3\nEvaluating covering numbers\nCovering numbers can be evaluated in some simple situations. The following propo-\nsition provides an example in finite dimensions.\nProposition 22.29 Assume that G is a parametric family of functions, so that G = {gθ,θ ∈Θ}\nwhere Θ ⊂Rm. Assume also that, for some constant C, ρ∞(gθ,gθ′) ≤C|θ −θ′| for all\nθ,θ′ ∈Θ. Let G(M) = {gθ : θ ∈Θ,|θ| ≤M}. Then\nN (G,ρ∞,ϵ) ≤\n\u0012\n1 + 2CM\nϵ\n\u0013m\nProof Letting ρ denote the Euclidean distance in Rm, our hypotheses imply that\nN (G(M),ρ∞,ϵ) is bounded by N (BM,ρ,ϵ/C) where BM is the ball with radius M in\nRm. Now, if θ1,...,θn is an α-covering of BM, then θ1/M,...,θn/M is an (α/M)-\ncovering of B1, which shows (together with a symmetric argument) that N (BM,ρ,α) =\nN (B1,ρ,α/M) and we get\nN (G(M),ρ∞,ϵ) ≤N (B1,ρ,ϵ/MC)\nand we only need to evaluate N (B1,ρ,α) for α > 0. Using proposition 22.27, one can\ninstead evaluate M(B1,ρ,α). So let A be an α-net in B1. Then\n[\nx∈A\nBρ(x,α/2) ⊂Bρ(0,1 + α/2)\nand, since the sets in the union are disjoint,\nX\nx∈A\nvolume(Bρ(x,α/2)) = |A|volume(Bρ(0,α/2)) ≤volume(Bρ(0,1 + α/2)).\n\n22.5. COVERING NUMBERS AND CHAINING\n609\nLetting Cm denote the volume of the unit ball in Rm, this shows\n|A|Cm\n\u0012α\n2\n\u0013m\n≤Cm\n\u0012\n1 + α\n2\n\u0013m\nand\n|A| ≤\n\u0012\n1 + 2\nα\n\u0013m\n,\nwhich concludes the proof.\n■\nOne can also obtain entropy number estimates in infinite dimensions. Here, we\nquote a result applicable to spaces of smooth functions, referring to Van der Vaart\nand Wellner [195] for a proof.\nTheorem 22.30 Let Z be a bounded convex subset of Rd with non-empty interior. For\np ≥1 and f ∈Cp(Z), let\n∥f ∥p,∞= max\nn\n|Dk(f (x)| : k = 0,...,p,x ∈Z\no\n.\nLet G be the unit ball for this norm,\nG =\nn\nf ∈Cp(Z) : ∥f ∥p,∞≤1\no\n.\nLet Z(1) be the set of all x ∈Rd at distance less than 1 from R.\nThen there exists a constant K depending only on p and d such that\nlogN (ϵ,G,ρ∞) ≤Kvolume(Z(1))\n\u00121\nϵ\n\u0013d/p\n22.5.4\nChaining\nThe distance ρ∞may not always be the best one to analyze the set of functions, G. For\nexample, if G is a class of functions with values in {−1,1}, then ρ∞(g,g′) = 2 unless\ng = g′. In such contexts, it is often preferable to use distances that compute average\ndiscrepancies, such as\nρp(g,g′) = E(|g(Z) −g′(Z)|p)1/p ,\n(22.35)\nfor some random variable Z. Such distances, by definition, do not provide uniform\nbounds on differences between functions (that we used to write (22.28)), but can\nrather be used in upper-bounds on the probabilities of deviations from zero, which\nhave to be handled somewhat differently. We here summarize a general approach\ncalled “chaining,” following for this purpose the presentation made in Talagrand\n[189] (see also Audibert and Bousquet [15]). From now on, we assume that (G,ρ) is a\n\n610\nCHAPTER 22. GENERALIZATION BOUNDS\n(pseudo-)metric space of functions g : Z →R and Z a random variable taking values\nin Z. We will make the basic assumption that, for all g,g′ ∈G and t > 0,\nP(|g(Z) −g′(Z)| > t) ≤2e\n−\nt2\n2ρ(g,g′)2 .\nNote that this assumption includes cases in which\nP(|g(Z) −g′(Z)| > t) ≤2e\n−\nt2\n2ρ(g,g′)α .\nfor some α ∈(0,2], because, if ρ is a distance, then so is ρα/2 if α ≤2. We will also\nassume that E(g(Z)) = 0 in order to avoid centering the variables at every step.\nWe are interested in upper bounds for P(supg∈G g(Z) > t). To build a chaining\nargument, consider a family (G0,G1,...) of subsets of G. Assume that |Gk| ≤Nk with\nNk chosen, for future simplicity, so that Nk−1Nk ≤Nk+1. For g ∈G, let πk(g) denote a\nclosest point to g in Gk. Also assume that G0 = {g0} is a singleton, so that π0(g) = g0\nfor all g ∈G. (One can generally assume without harm that 0 ∈G, in which case one\nshould choose g0 = 0 in the following discussion.) For g ∈Gn, we therefore have\ng −g0 =\nn\nX\nk=1\n(πk(g) −πk−1(g)).\nLet (t1,t2,...) be a sequence of numbers that will be determined later. Let\nSn = max\ng∈Gn\nn\nX\nk=1\ntkρ(πk(g),πk−1(g)).\n(22.36)\nThen, for any t,\nP(sup\ng∈Gn\ng(Z) −g0(Z) > tSn)\n≤P(∃g ∈Gn,∃k ≤n : πk(g)(Z) −πk−1(g)(Z) > ttkρ(πk(g),πk−1(g)))\n≤P(∃k ≤n,∃g ∈Gk,g′ ∈Gk−1 : g(Z) −g′(Z) > ttkρ(g,g′))\n≤\nn\nX\nk=1\nNkNk−1\nsup\ng∈Gk,g′∈Gk−1\nP(g(Z) −g′(Z) > ttkρ(g,g′))\n≤2\nn\nX\nk=1\nNk+1e−\nt2t2\nk\n2\nIf one takes Nk = 22k, which satisfies NkNk−1 = 22k+2k−1 ≤Nk+1, and tk = 2k/2, one\nfinds that\nP(sup\ng∈Gn\ng(Z) −g0(Z) > tSn) ≤2\nn\nX\nk=1\n22k+1e−2k−1t2.\n\n22.5. COVERING NUMBERS AND CHAINING\n611\nThe upper bound converges (as a function of n) as soon as t > 2\np\nlog2. Moreover,\none has\n2\nn\nX\nk=1\n22k+1e−2k−1t2 = 2e−t2\n2\nn\nX\nk=1\ne−2k−2(t2−8log2) ≤2e−t2\n2\n∞\nX\nk=1\ne−2k−2\nwhen t >\np\n1 + 8log2. This provides a concentration bound for P(supg∈Gn g(Z) −\ng0(Z) > tSn), that we may rewrite as\nP(sup\ng∈Gn\ng(Z) −g0(Z) > t) ≤Ce\n−t2\n2S2n\n(22.37)\nfor t > 2Sn\np\nlog2, C = 2P∞\nk=1 e−2k−2 and Sn given by (22.36), with tk = 2k/2. Moreover,\nwe have\nSn = max\ng∈Gn\nn\nX\nk=1\n2k/2ρ(πk(g),πk−1(g))\n≤max\ng∈Gn\nn\nX\nk=1\n2k/2(ρ(g,Gk) + ρ(g,Gk−1))\n≤2max\ng∈Gn\nn\nX\nk=0\n2k/2ρ(g,Gk)\nand this simpler upper bound can be used in (22.37).\nWe haven’t made many assumptions so far on the sequence G0,G1,..., beyond\nbounding their cardinality, but it is natural to require that they are built in order to\nbehave like a dense subset of G, so that\nlim\nn→∞max\ng∈G ρ(x,Gn) = 0.\n(22.38)\nNote that this requires that the set G is precompact for the distance ρ. We will also\nassume that\nlim\nn→∞sup\ng∈Gn\ng(x) = sup\ng∈G\ng(x).\n(22.39)\nThen, we have proved the following result [188].\nTheorem 22.31 Let G0,G1,... be a family of subsets of G satisfying (22.38) and (22.39)\nand such that G0 = {g0} and |Gn| ≤22n for n ≥0. Let\nS = 2sup\ng∈G\n∞\nX\nn=0\n2n/2ρ(g,Gn)\n(22.40)\n\n612\nCHAPTER 22. GENERALIZATION BOUNDS\nThen, for t > S\np\n1 + 8log2,\nP(sup\ng∈G\ng(Z) −g0(Z) > t) ≤Ce−t2\n2S2\n(22.41)\nwith C = 2P∞\nk=1 e−2k−2.\nThe exponential rate of convergence in the right-hand side of (22.41) is the quan-\ntity S, and the upper bound will be improved when building the sequence (G0,G1,...)\nso that S is as small as possible. Such an optimization for a given family of functions\nis however a formidable problem. It is however interesting to see (still following\n[188]) that theorem 22.31 implies a classical inequality in terms of what is called the\nmetric entropy of the metric space (G,ρ).\n22.5.5\nMetric entropy\nIf S is given by (22.40), we have\nS = 2sup\n\u0012 ∞\nX\nn=0\n2n/2ρ(g,Gn) : g ∈G\n\u0013\n≤2\n∞\nX\nn=0\n2n/2 sup{ρ(g,Gn) : g ∈G}\nTake Gn achieving the minimum in the entropy number e(G,ρ,22n). Then, (22.41)\nholds with S replaced by\nˆS = 2\n∞\nX\nn=0\n2n/2e(G,ρ,22n).\nConsider the function\nh(G,ρ) =\nZ ∞\n0\nq\nlogN (G,ρ,ϵ)dϵ,\n(22.42)\nwhich is known as Dudley’s metric entropy of the space (G,ρ). We have\nh(G,ρ) =\nZ e(2)\n0\nq\nlogN (ϵ)dϵ +\n∞\nX\nn=1\nZ e(22n)\ne(22n−1)\nq\nlogN (ϵ)dϵ.\nIf ϵ ∈[e(22n−1),e(22n)), we have N (ϵ) > 22n so that\nh(G,ρ) ≥e(2)\np\nlog3 +\n∞\nX\nn=1\n2n/2(e(22n) −e(22n−1))\n≥\n\u0010\n1 −\n√\n2\n2\n\u0011 ∞\nX\nn=1\n2n/2e(22n).\n\n22.5. COVERING NUMBERS AND CHAINING\n613\nTherefore,\nˆS ≤\n4\n2 −\n√\n2\nh(G,ρ) ≤7h(G,ρ)\nand this upper bound can also be used to obtain a simpler (but weaker) form of\ntheorem 22.31.\nRemark 22.32 The covering numbers of a class G of binary functions g with values\nin {−1,1} can be controlled by the VC dimension of the class. Here, we consider\nρ(g,g′) = P(g , g′) = ρ1(g,g′)/2. Then, the following theorem holds.\nTheorem 22.33 Let G be a class of binary functions such that D = VC-dim(G) < ∞.\nThen, there is a universal constant K such that, for any ϵ ∈(0,1),\nN (G,ρ,ϵ) ≤KD(4e)D \u00121\nϵ\n\u0013D−1\nwith ρ(g,g′) = P(g , g′).\nWe refer to Van der Vaart and Wellner [195], Theorem 2.6.4 for a proof, which is\nrather long and technical.\n♦\n22.5.6\nApplication\nWe quickly show how this discussion can be turned into results applicable to the\nclassification problem. If F is a function class of binary classifiers and r is the risk\nfunction, one can consider the class\nG = {(x,y) 7→r(y,f (x)) : f ∈F }.\nIf r is the 0–1 loss, we have VC-dim(G) ≤VC-dim(F ). Indeed, if one considers N\npoints in R × {−1,1}, say (x1,y1,...,xN,yN), then\nG(x1,y1,...,xN,yN)\n= {r(1,f (xk)) : k = 1,...,N,yk = 1} ∪{r(−1,f (xk)) : k = 1,...,N,yk = −1}.\nIf the two sets in the right-hand side are not empty, i.e., the numbers N(1) and N(−1)\nof k’s such that yk = 1 or yk = −1 are not zero, then\n|G(x1,y1,...,xN,yN)| ≤2N(1) + 2N(−1),\nwhich is less that 2N as soon as N > 2. So, taking N > 2, for (x1,y1,...,xN,yN) to be\nshattered by G, we need N(1) = N or N(−1) = N and in this case, the inequality:\n|G(x1,y1,...,xN,yN)| ≤|F (x1,...,xN)|\n\n614\nCHAPTER 22. GENERALIZATION BOUNDS\nis obvious. The same inequality will be true for some x1,...,xN with N = 2, except in\nthe uninteresting case where f (x) = 1 (or −1) for every x ∈R.\nA similar inequality holds for entropy numbers with the ρ1 distance (cf. (22.35))\nbecause\nE(|r(Y,f (X)) −r(Y,f ′(X))|) ≤P(f (X) , f ′(X))\nwhenever r takes values in [0,1], which implies that\nN (G,ρ1,ϵ) ≤N (F ,ρ1,ϵ)\nfor all ϵ > 0. Note however that evaluating this upper bound may still be challenging\nand would rely on strong assumptions on the distribution of X allowing to control\nP(f (X) , f ′(X)).\nWe now assume that functions in F define “posterior probabilities” on G. More\nprecisely, given λ ∈R we can define the probability πλ on {−1,1} by\nπλ(y) =\neλy\ne−λ + eλ.\nNow, if F is a class of real-valued functions, we can define the risk function\nr(y,f (x)) = log\n1\nπf (x)(y) .\nSince |∂λ logπλ(y)| = |y −tanhλ| ≤2 for y ∈{−1,1}, we have\n|r(y,f (x)) −r(y,f ′(x))| ≤2|f (x) −f ′(x)|\nso that entropy numbers in G can be estimated from entropy numbers in F . As an\nexample, let F be a space of affine functions x 7→a0 + bT x, x ∈Rd. Assume that the\nrandom variable X is bounded, so that one can take R to be an open ball centered at\n0 with radius, say, U. For M > 0, let\nFM = {f : x 7→a0 + bT x : |b| ≤M,|a0| ≤UM}.\nThe restriction |b| ≤M is equivalent to using a penalty method, such as, for example,\nridge logistic regression. Moreover, if |b| ≤M, it is natural to assume that |a0| ≤UM\nbecause otherwise f would have a constant sign on R. In this case, we get\nρ∞(r(y,f (x)),r(y,f ′(x))) ≤|a0 −a′\n0| + U|b −b′|\nand a small modification of the proof of proposition 22.29 shows that\nN (F ,ρ∞,ϵ) ≤\n\u0012\n1 + 4CU\nϵ\n\u0013d+1\n\n22.6. OTHER COMPLEXITY MEASURES\n615\n22.6\nOther complexity measures\n22.6.1\nFat-shattering and margins\nVC-dimension and metric entropy are measures that control the complexity of a\nmodel class, and can therefore be evaluated a priori without observing any data.\nThese bounds can be improved, in general, by using information derived from the\ntraining set, and, particular the classification margin that has been obtained [18].\nFor this discussion, we need to return to the definition of covering numbers. If F\nis a function class, ρ∞the supremum metric on F , ϵ > 0 and N is an integer, we let\nN (F ,ρ∞,ϵ,N) = max{N (F (A),ρ∞,ϵ) : A ⊂R,|A| = N}\nthat we will abbreviate in N∞(ϵ,N) when F is known from the context. We will\nassume that functions in F take values values in [−1,1], and we define for γ ≥0,\ny ∈{0,1}, u ∈R:\nrγ(y,u) =\n\n0 if u < −γ and y = 0\n0 if u > γ and y = 1\n1 otherwise\nSo, rγ(y,f (x)) is equal to 0 if f (x) correctly predicts y with margin γ and to 1 other-\nwise. We then define the classification error with margin γ as\nRγ(f ) = E(rγ(Y,f (X)))\nand, given a training set T of size N\nEγ,T = 1\nN\nN\nX\nk=1\nrγ(yk,f (xk)).\nWe then have the following theorem [10].\nTheorem 22.34 If t ≥\n√\n2/N\nP(sup\nf ∈F\n(R0(f ) −Eγ,T (f )) > t) ≤2N∞(γ/2,2N)e−Nt2/8,\n(22.43)\nor, equivalently, with probability larger than 1 −δ, one has, for all f ∈F ,\nR0(f ) −Eγ,T (f )) ≤\nr\n8\nN\n\u0012\nlogN∞(γ/2,2N) + log 2\nδ\n\u0013\n.\n(22.44)\nProof We first note that, for Nt2 > 2,\nP\n\u0012\nsup\nf ∈F\n(R0(f ) −Eγ,T (f )) > t\n\u0013\n≤2P\n\u0012\nsup\nf ∈F\n(ET ′(f ) −Eγ,T (f )) > ϵ\n2\n\u0013\n,\n\n616\nCHAPTER 22. GENERALIZATION BOUNDS\nwhich is proved exactly the same way as (22.21) in theorem 22.16, and we skip the\nargument.\nWe have\nET ′(f ) −Eγ,T (f ) = 1\nN\nN\nX\nk=1\n(r0(Y ′\nk,f (X′\nk)) −rγ(Yk,f (Xk)))\nand because (Xk,Yk) and (X′\nk,Y ′\nk) have the same distribution, supf ∈F (ET ′(f )−Eγ,T (f ))\nhas the same distribution as\n∆T ,T ′(ξ1,...,ξN) =\nsup\nf ∈F\n1\nN\nN\nX\nk=1\n\u0010\n(r0(Y ′\nk,f (X′\nk)) −rγ(Yk,f (Xk)))ξk + (r0(Yk,f (Xk)) −rγ(Y ′\nk,f (X′\nk)))(1 −ξk)\n\u0011\nwhere ξ1,...,ξN is a sequence of Bernoulli random variables with parameter 1/2.\nWe now estimate P(∆T ,T ′(ξ1,...,ξN) > t/2 | T ,T ′) and we therefore consider T and\nT ′ as fixed. Let F be a subset of F , with cardinality N∞(γ/2,2N), such that for all f ∈\nF there exists an f ′ ∈F such that |f (x)−f ′(x)| ≤γ/2 for all x ∈{X1,...,XN,X′\n1,...,X′\nN}.\nThen we claim that\n∆T ,T ′(ξ1,...,ξN) ≤∆′\nT ,T ′(ξ1,...,ξN)\nwhere\n∆′\nT ,T ′(ξ1,...,ξN) = max\nf ∈F\n1\nN\nN\nX\nk=1\n(2ξk −1)\n\u0010\nr γ\n2 (Y ′\nk,f (X′\nk)) −r γ\n2 (Yk,f (Xk))\n\u0011\n.\nThis is because, for any (x,y) ∈R × {0,1}, and f ,f ′ such that |f (x) −f ′(x)| < γ/2, we\nhave r0(y,f (x)) ≤rγ/2(y,f ′(x)) and rγ/2(y,f ′(x)) ≤rγ(y,f (x)): if an example is misclas-\nsified by f (resp. f ′) at a given margin, it must be misclassified by f ′ (resp. f ) at this\nmargin plus γ/2.\nNow,\nP(∆′\nT ,T ′(ξ1,...,ξN) > t\n2)\n≤|F|max\nf ∈F P\n\u0012 1\nN\nN\nX\nk=1\n(2ξk −1)(r γ\n2 (Y ′\nk,f (X′\nk)) −r γ\n2 (Yk,f (Xk))) > t\n2\n\u0013\nto which we can apply Hoeffding’s inequality, yielding\nP\n\u0012\n∆′\nT ,T ′(ξ1,...,ξN) > t\n2\n\u0013\n≤|F|e−Nt2/8,\n■\nwhich concludes the proof, since, by proposition 22.27, |F| ≤N∞(γ/2,2N).\n\n22.6. OTHER COMPLEXITY MEASURES\n617\nIn order to evaluate the covering numbers N∞(ϵ,N) using quantities similar\nto VC-dimensions, a different type of set decomposition and shattering has been\nproposed. Following Alon et al. [4], we introduce the following notions. Recall\nthat a family of functions F : R →{0,1} shatters a finite set A ⊂R if and only if\n|F (A)| = 2|A|. The following definitions are adapted to functions taking values in a\ncontinuous set.\nDefinition 22.35 Let F be a family of functions f : R →[−1,1] and A a finite subset of\nR.\n(i) One says that F P-shatters A if there exists a function gA : R →R such that, for each\nB ⊂A, there exists a function f ∈F such that f (x) ≥gA(x) if x ∈B and f (x) < gA(x) if\nx ∈A \\ B.\n(ii) Let γ be a positive number. One says that F Pγ-shatters A if there exists a function\ngA : R →R such that, for each B ⊂A, there exists a function f ∈F such that f (x) ≥\ngA(x) + γ if x ∈B and f (x) ≤gA(x) −γ if x ∈A \\ B.\nNote that only the restriction of gA to A matters in this definition. This function\nacts as a threshold for binary classification. More precisely, given a function g : A →\nR, one can associate to every f ∈F the binary function fg with fg(x) equal to 1 if\nf (x) ≥g(x) and to 0 otherwise. Letting Fg = {fg : f ∈F } we see that F P-shatters A\nif there exists a function gA such that FgA shatters A. The definition of Pγ-shattering\nintroduces a margin in the definition of fg (with fg(x) equal to 1 if f (x) ≥g(x) + γ, to\n0 if f (x) ≤g(x) −γ and is ambiguous otherwise), and A is Pγ-shattered by F if, for\nsome gA, the corresponding FgA shatters A without ambiguities.\nDefinition 22.36 One then defines the P-dimension of F by\nP-dim(F ) = max{|A| : A ⊂R,F P-shatters A},\nand similarly the Pγ-dimension of F is\nPγ-dim(F ) = max{|A| : A ⊂R,F Pγ-shatters A}.\nThe Pγ-dimension of F will replace the VC-dimension in order to control the\ncovering numbers. More precisely, we have the following theorem [4].\nTheorem 22.37 Let γ > 0 and assume that F has Pγ/4-dimension D < ∞. Then,\nN∞(γ,N) ≤2\n 16N\nγ2\n!D log(4eN/(Dγ))\n.\n\n618\nCHAPTER 22. GENERALIZATION BOUNDS\nProof The proof is quite technical and relies on a combinatorial argument in which\nF is first assumed to take integer values before addressing the continuous case.\nStep 1. We first assume that functions in F take values in the finite set {1,...,r}\nwhere r is an integer. For the time of this proof, we introduce yet another notion of\nshattering called S-shattering (for strong shattering) which is essentially the same\nas P1-shattering, except that functions g are restricted to take values in {1,...,r}. Let\nA be a finite subset of R. Given a function g : R →{1,...,r}, we say that (F ,g) S-\nshatters A if, for any B ⊂A, there exist f ∈F satisfying f (x) ≥g(x) + 1 for x ∈B and\nf (x) ≤g(x) −1 if x ∈A \\ B. We say that F S-shatters A if (F ,g) S-shatters A for some\ng. The S-dimension of F is the cardinality of the largest subset of R that can be\nS-shattered and will be denoted S-dim(F ). The first, and most difficult, part of the\nproof is to show that, if S-dim(F ) = D, then\nM(F (A),ρ∞,2) ≤2(|A|r2)⌈log2 y⌉\nwith\ny =\nD\nX\nk=1\n |A|\nk\n!\nrk\nand ⌈i⌉denotes the smallest integer larger than u ∈R. Here, M is the packing\nnumber defined in section 22.5.1.\nTo prove this, we can assume that r ≥3, since, for r ≤2, M(F (A),ρ∞,2) = 1 (the\ndiameter of F for the ρ∞distance is 0 or 1). Let G(A) = {1,...,r}A be the set of all\nfunctions f : A →{1,...,r} and let\nUA = \bF ⊂G(A) : ∀f ,f ′ ∈F,∃x ∈A with |f (x) −f ′(x)| ≥2\t .\nFor F ∈UA, let\nSA(F) = {(B,g) : B ⊂A,B , ∅,g : B →{1,...,r},(F,g) S-shatters B}.\nLet tA(h) = min{|SA(F)| : F ∈UA,|F| = h} (where the minimum of the empty set is +∞).\nSince we are considering in UA all possible functions from A to {1,...,r}, it is clear\nthat tA(h) only depends on |A|, and we will also denote it by t(h,|A|).\nNote that, by definition, if (B,g) ∈SA(F), and F ⊂F , then |B| ≤D. So, the number\nof elements in SA(F) for such an F is less or equal than the number of possible such\npairs (B,g), which is strictly less than y = PD\nk=1\n\u0000|A|\nk\n\u0001rk. So, if t(h,|A|) ≥y, then there\ncannot be any F ⊂F in the set UA and M(F (A),ρ∞,2) < h. The rest of the proof\nconsists in showing that t(h,|A|) ≥y.\nFor any n ≥1, we have t(2,n) = 1: fix x ∈A, and F = {f1,f2} ∈G such that f1(x) = 1,\nf2(x) = 3 and f1(y) = f2(y) if y , x. Then only ({x},g) is S-shattered by F, with g such\nthat g(x) = 2.\n\n22.6. OTHER COMPLEXITY MEASURES\n619\nNow, assume that, for some integer m, t(2mnr2,n) < ∞, so that there exists F ∈UA\nsuch that |F| = 2mnr2. Arrange the elements of F into mnr2 pairs {fi,f ′\ni }. For each\nsuch pair, there exists xi ∈A such that |fi(xi) −f ′\ni (xi)| > 1. Since there are at most n\nselected xi, one of them must be appearing at least mr2 times. Call it x and keep (and\nreindex) the corresponding mr2 pairs, still denoted {fi,f ′\ni }. Now, there are at most\nr(r −1)/2 possible distinct values for the unordered pairs {fi(x),f ′\ni (x)}, so that one of\nthem must be appearing at least 2mr2/r(r −1) > 2m times. Select these functions,\nreindex them and exchange the role of fi and f ′\ni if needed to obtain 2m pairs {fi,f ′\ni }\nsuch that fi(x) = k and f ′\ni (x) = l for all i and fixed k,l ∈{1,...,r} such that k + 1 < l.\nLet F1 = {f1,...,f2m} and F′\n1 = {f ′\n1,...,f ′\n2m}. Let A′ = A \\ {x}. Then both F1 and F′\n1\nbelong to UA′, which implies that both SA′(F1) and SA′(F′\n1) have cardinality at least\nt(2m,n−1). Moreover, both sets are included in SA(F), and if (B,g) ∈SA′(F1)∩SA′(F′\n1),\nthen (B ∪{x},g′) ∈SA(F), with g′(y) = g(y) for y ∈B and g′(x) = k + 1. This provides\n2t(2m,n−1) elements in SA(F) and shows the key inequality (which is obviously true\nwhen the left-hand side is infinite)\nt(2mnr2,n) ≥2t(2m,n −1).\nThis inequality can now be used to prove by induction that for all 0 ≤k < n, one\nhas t(2(nr2)k,n) ≥2k, since\nt(2((n + 1)r2)k+1,n + 1) ≥2t(2((n + 1)r2)k,n) ≥2t(2(nr2)k,n).\nFor k ≥n, one has 2(nr2)k > rn, where rn is the number of functions in G(A), so\nthat t(2(nr2)k,n) = +∞. So, t(2(nr2)k,n) ≥2k is valid for all k and it suffices to take\nk = ⌈log2 y⌉to obtain the desired result.\nStep 2. The next step uses a discretization scheme to extend the previous result to\nfunctions with values in [−1,1]. More precisely, given f : R →[0,1], and η > 0, let\nf η(x) = max{k ∈N : 2kη −1 ≤f (x)}\nwhich takes values in {0,...,r} for r = ⌊η−1⌋. If F is a class of functions with values\nin [−1,1], define F η = {f η : f ∈F }. With this notation, the following holds.\n(a) For all γ ≤η: S-dim(F η) ≤Pγ-dim(F )\n(b) For all ϵ ≥4η and A ⊂R: M(F (A),ρ∞,ϵ) ≤M∞(F η(A),ρ∞,2).\nTo prove (a), assume that F η S-shatters A, so that there exists g such that, for all\nB ⊂A, there exists f ∈F such that f η(x) ≥g(x) + 1 for x ∈B and f η(x) ≤g(x) −1\nfor x ∈A \\ B. Using the fact that 2ηf η(x) −1 ≤f (x) < 2ηf η(x) + 2η −1, we get f (x) ≥\n2ηg(x)+2η−1 for x ∈B and f (x) ≤2ηg(x)−1 for x ∈A\\B. So taking ˜g(x) = 2ηg(x)+η−1\n\n620\nCHAPTER 22. GENERALIZATION BOUNDS\nas threshold function (which does not depend on B), we see that F Pγ-shatters A if\nγ ≤η.\nFor (b), we deduce from the definition of f η that |f η(x) −˜f η(x)| > (2η)−1|f (x) −\n˜f (x)| −1 so that, if ϵ = 4η, |f (x) −˜f (x)| ≥ϵ implies |f η(x) −˜f η(x)| > 1, or, equivalently\n|f η(x) −˜f η(x)| ≥2.\nStep 3. We can now conclude. Taking γ > 0, we have, if |A| = N\nN (F (A),ρ∞,γ) ≤M(F (A),ρ∞,γ) ≤M(F γ/4(A),ρ∞,2) ≤2\n 16N\nγ2\n!⌈logy⌉\nwith\ny =\nD\nX\nk=1\n N\nk\n!\n(γ/4)−k ≤(γ/4)−D\nD\nX\nk=1\n N\nk\n!\n≤\n 4Ne\nDγ\n!D\n.\nSince the maximum of N (F (A),ρ∞,γ) over A with cardinality N is N∞(γ,N), the\nproof is complete.\n■\nOne can use this result to evaluate margin bounds on linear classifiers with\nbounded data. Let R be the ball with radius Λ in Rd and consider the model class\ncontaining all functions f (x) = a0 + bT x with a0 ∈[−Λ,Λ] and b ∈Rd, |b| ≤1. Let\nA = {x1,...,xN} be a finite subset of R. Then, F Pγ-shatters A if and only if there\nexists g1,...,gN ∈R such that, for any sequences ξ = (ξ1,...,ξN) ∈{−1,1}N , there\nexists aξ\n0 ∈[−Λ,Λ] and bξ ∈Rd, |bξ| ≤1 with ξk(aξ\n0 + (bξ)T xk −gk) ≥γ for k = 1,...,N.\nSumming over N, we find that\nNγ +\nN\nX\nk=1\ngkξk ≤aξ\n0\nN\nX\nk=1\nξk + (bξ)T\nN\nX\nk=1\nξkxk .\nThis shows that, for any sequence ξ1,...,ξN,\nNγ +\nN\nX\nk=1\ngkξk ≤Λ\n\f\f\f\f\nN\nX\nk=1\nξk\n\f\f\f\f +\n\f\f\f\f\nN\nX\nk=1\nξkxk\n\f\f\f\f\nApplying the same inequality after changing the signs of ξ1,...,ξN yields\nNγ ≤Nγ +\n\f\f\f\f\nN\nX\nk=1\ngkξk\n\f\f\f\f ≤Λ\n\f\f\f\f\nN\nX\nk=1\nξk\n\f\f\f\f +\n\f\f\f\f\nN\nX\nk=1\nξkxk\n\f\f\f\f.\nThis shows, in particular, that (letting ξ1,...,ξN be independent Rademacher ran-\ndom variables)\nP\n\nΛ\n\f\f\f\f\nN\nX\nk=1\nξk\n\f\f\f\f +\n\f\f\f\f\nN\nX\nk=1\nξkxk\n\f\f\f\f ≥Nγ\n\n= 1.\n\n22.6. OTHER COMPLEXITY MEASURES\n621\nHowever, using the identity (A + B)2 ≤2A2 + 2B2, we have\nP\n\nΛ\n\f\f\f\f\nN\nX\nk=1\nξk\n\f\f\f\f +\n\f\f\f\f\nN\nX\nk=1\nξkxk\n\f\f\f\f ≥Nγ\n\n≤P\n\n2Λ2\f\f\f\f\nN\nX\nk=1\nξk\n\f\f\f\f\n2\n+ 2\n\f\f\f\f\nN\nX\nk=1\nξkxk\n\f\f\f\f\n2\n≥N 2γ2\n\n.\nSince\nE\n\n2Λ2\f\f\f\f\nN\nX\nk=1\nξk\n\f\f\f\f\n2\n+ 2\n\f\f\f\f\nN\nX\nk=1\nξkxk\n\f\f\f\f\n2\n\n= 2NΛ2 + 2\nn\nX\nk=1\n|xk|2 ≤4NΛ2,\nMarkov’s inequality implies\nP\n\n2Λ2\f\f\f\f\nN\nX\nk=1\nξk\n\f\f\f\f\n2\n+ 2\n\f\f\f\f\nN\nX\nk=1\nξkxk\n\f\f\f\f\n2\n≥N 2γ2\n\n≤4Λ2\nNγ2 .\nWe get a contradiction unless N ≤4Λ2/γ2, which shows that Pγ-dim(F ) ≤4Λ2/γ2.\nTheorem 22.37 then implies that\nN∞(γ,N) ≤2\n 16N\nγ2\n! 63Λ2\nγ2 log\n\u0010 16eNγ\nΛ2\n\u0011\nand this upper bound can then be plugged into equations (22.43) or (22.44) to esti-\nmate the generalization error.\nBeyond the explicit expression of the upper bound, the important point in the\nprevious argument is that the Pγ-dimension is bounded independently from the di-\nmension d of X (and therefore also applies in the infinite-dimensional case). This\nshould be compared to what we found for the VC-dimension of separating hyper-\nplanes, which was d + 1 (cf. proposition 22.22).\nRemark 22.38 Note that the upper-bound obtained in theorem 22.34 depends on a\nparameter (γ) and the result is true for any choice of this parameter. It is tempting at\nthis point to optimize the bound with respect to γ, but this would be a mistake since\na family of events being likely does not imply that their intersection is likely too.\nHowever, with a little work, one can ensure that an intersection of slightly weaker\ninequalities holds. Indeed, assume that an estimate similar to (22.43) holds, in the\nform\nP(R0( ˆfT ) > UT (γ) + t) ≤C(γ)e−mt2/2\nor, equivalently\nP\n\u0012\nR0( ˆfT ) > UT (γ) +\nq\nt2 + 2logC(γ)\n\u0013\n≤e−mt2/2 ,\n\n622\nCHAPTER 22. GENERALIZATION BOUNDS\nwhere UT (γ) depends on data and is increasing (as a function of γ), and C(γ) is a\ndecreasing function of γ. Consider a decreasing sequence (γk) that converges to 0\n(for example γk = L2−k). Choose also an increasing function ϵ(γ). Then\nP\n\u0012\nR0( ˆfT ) > min{UT (γ) +\nq\nt2 + 2logC(γ) + ϵ2(γ) : 0 ≤γ ≤L}\n\u0013\n≤P\n\u0012\nR0( ˆfT ) > min{UT (γk) +\nq\nt2 + 2logC(γk−1) + ϵ2(γk) : k ≥1}\n\u0013\n.\nMoreover\nP\n\u0012\nR0( ˆfT ) > min{UT (γk) +\nq\nt2 + 2logC(γk−1) + ϵ2(γk) : k ≥1}\n\u0013\n≤\n∞\nX\nk=0\nP\n\u0012\nR0( ˆfT ) > UT (γk) +\nq\nt2 + 2logC(γk−1) + ϵ(γk)\n\u0013\n≤\n∞\nX\nk=0\nC(γk)\nC(γk−1)e−mϵ2(γk)/2−mt2/2 .\nSo, it suffices to choose ϵ(γ) so that\nC0 =\n∞\nX\nk=1\nC(γk)\nC(γk−1)e−mϵ2(γk)/2 < ∞\nto ensure that\nP\n\u0012\nR0( ˆfT ) > min{UT (γ) +\nq\nt2 + 2logC(γ) + ϵ2(γ) : γ0 ≤γ ≤L}\n\u0013\n≤C0e−mt2/2.\nFor example, if γk = L2−k, one can take\nϵ(γ) =\ns\n2\nm\n \nlog C(γ)\nC(γ/2) + logγ−1\n!\nwhich yields C0 ≤L.\n♦\n22.6.2\nMaximum discrepancy\nLet T be a training set and let T1 and T2 form a fixed partition of the training set\nin two equal parts. Assume, for simplicity, that N is even and that the method\nfor selecting the two parts is deterministic, e.g., place the first half of T in T1 and\nsecond one in T2. Following Bartlett et al. [20], one can then define the maximum\ndiscrepancy on T by\nCT = sup\nf ∈F\n(ET1(f ) −ET2(f ))\n\n22.6. OTHER COMPLEXITY MEASURES\n623\nThis discrepancy measures the extent to which estimators may differ when trained\non two independent half-sized training sets. For a binary classification problem, the\nestimation of CT can be made with the same algorithm as the initial classifier, since\nET1(f ) −ET2(f ) is, up to a constant, exactly the classification error for the training set\nin which the class labels are flipped for the data in T2.\nFollowing [20], we now discuss concentration bounds that rely on CT and start\nwith the following Lemma.\nLemma 22.39 Introduce the function\nΦ(T) = sup\nf ∈F\n(R(f ) −ET (f )) −sup\nf ∈F\n(ET1(f ) −ET2(f )).\nThen E(Φ(T )) ≤0.\nProof Note that, if T ′ is a training set, independent of T with identical distribution,\nthen, for any f0 ∈F ,\nR(f0) −ET (f0) = E(ET ′(f0) −ET (f0) | T ) ≤E(sup\nf ∈F\n(ET ′(f ) −ET (f )) | T )\nso that\nE(sup\nf ∈F\n(R(f ) −ET (f ))) ≤E(sup\nf ∈F\n(ET ′(f ) −ET (f ))).\nNow, for a given f , we have ET (f ) = 1\n2(ET1(f ) + ET2(f )) and splitting T ′ the same way,\nwe have ET ′(f ) = 1\n2(ET ′\n1(f ) + ET ′\n2(f )).\nWe can therefore write\nE(sup\nf ∈F\n(R(f ) −ET (f ))) ≤1\n2E(sup\nf ∈F\n(ET ′\n1(f ) −ET1(f )) + (ET ′\n2(f ) −ET2(f )))\n≤1\n2\n\nE(sup\nf ∈F\n(ET ′\n1(f ) −ET1(f ))) + E(sup\nf ∈F\n(ET ′\n2(f ) −ET2(f ))\n\n\n= E(sup\nf ∈F\n(ET1(f ) −ET2(f )))\nwhere we have used the fact that both (T ′\n1,T1) and (T ′\n2,T2) form random training sets\nwith identical distribution to (T1,T2).\nThis proves that E(Φ(T )) ≤0.\n■\nUsing the lemma, one can write\nP(sup\nf ∈F\n(R(f ) −ET (f )) ≥CT + ϵ) = P(Φ(T ) ≥ϵ) ≤P(Φ(T ) −E(Φ(T )) ≥ϵ).\n\n624\nCHAPTER 22. GENERALIZATION BOUNDS\nOne can then use McDiarmid’s inequality (theorem 22.13) after noticing that, letting\nzk = (xk,yk) for k = 1,...,N,\nmax\nz1,...,zN,z′\nk\n\f\f\fΦ(z1,...,zN) −Φ(z1,...,zk−1,z′\nk,zk+1,...,zN)\n\f\f\f ≤3\nN\nyielding\nP(sup\nf ∈F\n(R(f ) −ET (f )) ≥CT + ϵ) ≤e−2Nϵ2\n9 .\n22.6.3\nRademacher complexity\nWe now extend the previous definition by computing discrepancies over random\ntwo-set partitions of the training set, which have equal size in average. This leads\nto the empirical Rademacher complexity of the function class. Let ξ1,...,ξN be a\nsequence of Rademacher random variables (equal to -1 and +1 with equal probabil-\nity 1/2). Then, the (empirical) Rademacher complexity of the training set T for the\nmodel class F is\nrad(T) = E\n\u0012\nsup\nf ∈F\n1\nN\nN\nX\nk=1\nξkr(Yk,f (Xk))\n\f\f\f\f T = T\n\u0013\n.\nThe mean Rademacher complexity is then the expectation of this quantity over\nthe training set distribution. The Rademacher complexity can be computed with\na—costly—Monte-Carlo simulation, in which the best estimator is computed with\nrandomly flipped labels corresponding to the values of k such that ξk = −1.\nThis measure of complexity was introduced to the machine learning framework\nin Koltchinskii and Panchenko [109], Bartlett and Mendelson [19], and Rademacher\nsums have been extensively studied in relation to empirical processes (cf. Ledoux\nand Talagrand [117], chapter 4).\nOne can bound the Rademacher complexity in terms of VC dimension.\nProposition 22.40 Let F be a function class such that D = VC-dim(F ) < ∞. Then\nrad(T) ≤\n3\n√\nN\np\n2D log(eN/D).\nProof One has, using Hoeffding’s inequality\nP\n\u0012\nsup\nf ∈F\n1\nN\nN\nX\nk=1\nξkr(yk,fk) > t\n\u0013\n≤|F (T )|sup\nf ∈F\nP\n\u0012 1\nN\nN\nX\nk=1\nξkr(yk,fk) > t\n\u0013\n≤|F (T )|e−Nt2/2.\n\n22.6. OTHER COMPLEXITY MEASURES\n625\nThis implies that\nP\n\u0012\nsup\nf ∈F\n\f\f\f\f\n1\nN\nN\nX\nk=1\nξkr(yk,fk)\n\f\f\f\f > t\n\u0013\n≤2|F (T )|e−Nt2/2\nand proposition 22.4 implies\nrad(T) ≤3\np\n2|F (T)|\n√\nN\n.\nTherefore if D = VC-dim(F ) < ∞, proposition 22.20 implies\nrad(T ) ≤\n3\n√\nN\np\n2D log(eN/D).\n■\nWe now discuss generalization bounds using Rademacher’s complexity. While we\nstill consider binary classification problems (with RY = {−1,1}), we will assume that\nF contains functions that can take arbitrary scalar values, and the 0–1 loss function\nbecomes r(y,y′) = 1yy′≤0 with y ∈{−1,1} and y′ ∈R. We will also consider functions\nthat dominate this loss, i.e., functions ρ : RY × R →[0,1] such that\nr(y,y′) ≤ρ(y,y′)\nfor all y ∈RY, y′ ∈R. Some examples are the margin loss ρ∗\nh(y,y′) = 1yy′≤h for h ≥0,\nor the piecewise linear function\nρh(y,y′) =\n\n1\nif yy′ ≤0\n1 −yy′/h\nif 0 ≤yy′ ≤h\n0\nif yy′ ≥h\nIf G is a class of functions g : Z →R, we will denote\nRadG(z1,...,zN) = 1\nN E\n\nsup\ng∈G\nN\nX\nk=1\nξkg(zk)\n\n\nand\nRadG(N) = E\n\u0010\nRadG(Z1,...,ZN)\n\u0011\n.\nOur previous notation can then be rewritten as rad(T ) = RadG(z1,...,zn) where zi =\n(xi,yi) and G is the space of functions: g : (x,y) 7→r(y,f (x)) for f ∈F . The following\ntheorem is proved in Koltchinskii and Panchenko [109], Bartlett and Mendelson [19].\n\n626\nCHAPTER 22. GENERALIZATION BOUNDS\nTheorem 22.41 Let ρ be a function dominating the risk function r(y,y′) = 1yy′≤0. Let\nGρ = {(x,y) 7→ρ(y,f (x)) −1 : f ∈F }\nand\nEρ\nT (f ) = 1\nN\nN\nX\nk=1\nρ(yk,f (xk)).\nThen\nP(sup\nf ∈F\nR(f ) ≥Eρ\nT (f ) + 2RadGρ(N) + t) ≤e−Nt2/2\nProof For f ∈F , we have\nR(f ) −Eρ(f ) ≤E(ρ(Y,f (X))) −Eρ(f ) ≤Φ(Z1,...,ZN)\nwhere\nΦ(Z1,...,ZN) = sup\ng∈Gρ\n\u0012\nE(g(Z)) −1\nN\nN\nX\nk=1\ng(Zk)\n\u0013\n.\nSince changing one variable among Z1,...,ZN changes Φ by at most 2/N, McDi-\narmid’s inequality implies that\nP(Φ(Z1,...,ZN) −E(Φ(Z1,...,ZN)) ≥t) ≤e−Nt2/2.\nNow we have\nE(Φ(Z1,...,ZN)) = E\n\u0012\nsup\ng∈Gρ E\n\u0012 1\nN\nN\nX\nk=1\ng(Z′\nk) −1\nN\nN\nX\nk=1\ng(Zk)\n\f\f\f\fZ1,...,ZN\n\u0013\u0013\n≤E\n\u0012\nsup\ng∈Gρ\n\u0012 1\nN\nN\nX\nk=1\ng(Z′\nk) −1\nN\nN\nX\nk=1\ng(Zk)\n\u0013\u0013\n≤E\n\u0012\nE\n\u0012\nsup\ng∈Gρ\n\u0012 1\nN\nN\nX\nk=1\nξk(g(Z′\nk) −g(Zk))\n\u0013\f\f\fZ,Z′\u0013\u0013\n≤2E\n\u0012\nE\n\u0012\nsup\ng∈Gρ\n\u0012 1\nN\nN\nX\nk=1\nξkg(Zk)\n\u0013\f\f\fZ\n\u0013\u0013\n≤2RadGρ(N),\nof which the statement of the theorem is a direct consequence.\n■\n\n22.6. OTHER COMPLEXITY MEASURES\n627\n22.6.4\nAlgorithmic Stability\nAnother result using McDiarmid’s inequality is proved in Bousquet and Elisseeff\n[39], and is based on the stability of a classifier when one removes a single example\nfrom the training set. As before, we consider training sets T of size N, where T is a\nrandom variable.\nFor k ∈{1,...,N}, and a training set T = (x1,y1,...,xN,yN), we let T (k) be the train-\ning set with sample (xk,yk) removed. One says that the predictor (T 7→ˆfT ) has uni-\nform stability βN for the loss function r if, for all T of size N, all k ∈{1,...,N}, and\nall x,y:\n|r( ˆfT (x),y) −r( ˆfT (k)(x),y)| ≤βN .\n(22.45)\nWith this definition, the following theorem holds.\nTheorem 22.42 (Bousquet and Elisseeff [39]) Assume that ˆfT has uniform stability\nβN for training sets of size N and that the loss function r(Y,f (X)) is almost surely\nbounded by M > 0. Then, for all ϵ > 2βN, one has\nP(R( ˆfT ) ≥ET ( ˆfT ) + ϵ) ≤e−2N\n\u0010\nϵ−2βN\n4NβN +M\n\u00112\n.\nOf course, this theorem is interesting only when βN is small as a function of N, i.e.,\nwhen NβN is bounded.\nProof Let Zi = (Xi,Yi) and F(Z1,...,ZN) = R( ˆfT ) −ET ( ˆfT ). We want to apply McDi-\narmid inequality (theorem 22.13) to F, and therefore estimate\nδk(F)\n∆=\nmax\nz1,...,zN,z′\nk\n\f\f\fF(z1,...,zN) −F(z1,...,zk−1,z′\nk,zk+1,...,zN)\n\f\f\f.\nIntroduce a training set ˜Tk in which the variable zk is replaced by z′\nk = (x′\nk,y′\nk).\nBecause ˜T (k)\nk\n= T (k), we have\n|R( ˆfT ) −R( ˆf ˜Tk)|\n≤\nE(|r(Y, ˆfT (X)) −r(Y, ˆf ˜Tk(X)))|\n≤\nE(|r(Y, ˆfT (X)) −r(Y, ˆfT (k)(X))|)\n+E(|r(Y, ˆf ˜Tk(X)) −E(r(Y, ˆf ˜T (k)\nk (X)))|)\n≤\n2βN\n\n628\nCHAPTER 22. GENERALIZATION BOUNDS\nSimilarly, we have\n|ET ( ˆfT ) −E ˜Tk)( ˆf ˜Tk)|\n≤\n1\nN\nX\nl,k\n|r(yl, ˆfT (xl),) −r(yl, ˆf ˜Tk(xl))|\n+ 1\nN |r(yk, ˆfT (xk)) −r(y′\nk, ˆf ˜Tk(x′\nk))|\n≤\n1\nN\nX\nl,k\n|r(yl, ˆfT (xl)) −r(yl, ˆfT (k)(xl))|\n+ 1\nN\nX\nl,k\n|r(yl, ˆf ˜Tk(xl)) −r(yl, ˆf ˜T (k)\nk (xl))| + M\nN\n≤\n2βN + M\nN\nCollecting these results, we find that δk(F) ≤4βN + M\nN , so that, by theorem 22.13,\nP\n\u0010\nR( ˆfT ) ≥ET ( ˆfT ) + E(R( ˆfT ) −ET ( ˆfT ))\n\u0011\n+ ϵ) ≤exp\n \n−\n2Nϵ2\n(4NβN + M)2\n!\n.\nIt remains to evaluate the expectation in this formula. Introducing as above vari-\nables Z′\n1,...,Z′\nN and using the same notation for ˜Tk, we have\nE(R( ˆfT )) = E(r(Y ′\nk,fT (X′\nk))) = E(r(Yk,f ˜Tk(Xk))).\nUsing this, we have\nE(R( ˆfT ) −ET ( ˆfT ))\n=\n1\nN\nN\nX\nk=1\nE(r(Yk,f ˜Tk(Xk)) −r(Yk,fT (Xk)))\n=\n1\nN\nN\nX\nk=1\nE(r(Yk,f ˜Tk(Xk)) −r(Yk,f ˜T (k)\nk (Xk)))\n+ 1\nN\nN\nX\nk=1\nE(r(Yk,fT (k)\nk (Xk)) −r(Yk,fT (Xk)))\nfrom which one deduces that\n|E(R( ˆfT ) −ET ( ˆfT ))| ≤2βN.\nWe therefore obtain\nP\n\u0010\nR( ˆfT ) ≥ET ( ˆfT ) + ϵ + 2βN\n\u0011\n≤exp\n \n−\n2Nϵ2\n(4NβN + M)2\n!\n.\nas required.\n■\n\n22.6. OTHER COMPLEXITY MEASURES\n629\n22.6.5\nPAC-Bayesian bounds\nOur final discussion of concentration bounds for the empirical error uses a slightly\ndifferent paradigm from that discussed so far. The main difference is that, instead of\ncomputing one predictor ˆfT from a training set T, it would return a random variable\nwith values in F , or, equivalently, a probability distribution on F (therefore assum-\ning that this space is measurable) that we will denote ˆµT . The training set error is\nnow defined by:\n¯ET (µ) =\nZ\nET (f )dµ(f ),\nfor any probability distribution µ on F , while the generalization error is:\n¯R(µ) =\nZ\nF\nR(f )dµ(f ).\nOur goal is to obtain upper bounds on ¯R(µT )−¯ET (µT ) that hold with high probability.\nIn this framework, we have the following result, in which Q denotes the space of\nprobability distributions on F .\nAssume that the loss function r takes its values in [0,1]. Recall that KL(µ∥π) is\nthe Kullback-Leibler divergence from µ to π, defined by\nKL(µ∥π) =\nZ\nF\nlog(ϕ(f ))ϕ(f )dπ(f )\nif µ has a density ϕ with respect to π and +∞otherwise. Then, the following theorem\nholds.\nTheorem 22.43 (McAllester [128]) With the notation above, for any fixed probability\ndistribution π ∈Q,\nP\n\u0012\nsup\nµ∈Q\n( ¯R(µ) −¯ET (µ)) >\nr\nt + KL(µ∥π)\n2N\n\u0013\n≤2Ne−Nt.\n(22.46)\nTaking t = log(2N/δ)/2N, the theorem is equivalent to the statement that, with prob-\nability 1 −δ, one has\n¯R(µ) −¯ET (µ) ≤\nr\nlog2N/δ + KL(µ∥π)\n2N\n.\n(22.47)\nProof We first show that, for any probability distributions π,µ on F , and any func-\ntion H on F ,\nZ\nF\nH(f )dµ −log\nZ\nF\neH(f )dπ ≤KL(µ∥π).\n\n630\nCHAPTER 22. GENERALIZATION BOUNDS\nIndeed, assume that µ has a density ϕ with respect to π (otherwise the upper bound\nis infinite) and let\nϕH =\neH\nR\nF eH(f )dπ\n.\nThen\nKL(µ∥π) −\nZ\nF\nH(f )dµ + log\nZ\nF\neH(f )dπ =\nZ\nF\nϕ logϕdπ −\nZ\nF\nϕ logϕHdπ\n=\nZ\nF\nϕ\nϕH\n \nlog ϕ\nϕH\n!\nϕHdπ\n= KL(µ∥ϕHπ) ≥0,\nwhich proves the result (and also shows that one can only have equality when ϕ =\nϕH π-almost surely.)\nLet χ(u) = max(u,0)2. We can use this inequality to show that, for any probability\nQ ∈Q and λ > 0,\nλχ( ¯R(µ) −¯ET (µ)) ≤λ\nZ\nF\nχ(R(f ) −ET (f ))dµ(f ) ≤KL(µ∥π) + log\nZ\nF\neλχ(R(f )−ET (f ))dπ\nwhere we have applied Jensen’s inequality to the convex function χ. This yields\neλχ( ¯R(µ)−¯ET (Q)) ≤eKL(µ∥π)\nZ\nF\neλχ(R(f )−ET (f ))dπ.\nHoeffding’s inequality implies that, for all f ∈F and t ≥0\nP(χ(R(f ) −ET (f )) > t) = P(R(f ) −ET (f ) >\n√\nt) ≤e−2Nt\nso that\nE\n\u0010\neλχ(R(f )−ET (f ))\u0011\n=\nZ ∞\n0\nP(λχ(R(f ) −ET (f )) > logt)dt\n≤1 +\nZ eλ\n1\ne−2N logt\nλ\ndt\n= 1 +\nZ λ\n0\ne−2Nu\nλ +udu\n= 1 + λeλ−2N −1\nλ −2N\n.\nFrom this and Markov’s inequality, we get, for any λ > 0:\nP(sup\nµ∈Q\nχ( ¯R(µ) −¯ET (µ)) > t + KL(µ∥π)/λ) ≤e−λt\n \n1 + λeλ−2N −1\nλ −2N\n!\n.\n\n22.7. APPLICATION TO MODEL SELECTION\n631\nTaking λ = 2N yields\nP(sup\nµ∈Q\nχ( ¯R(µ) −¯ET (µ)) > t + KL(µ∥π)/2N) ≤2Ne−2Nt,\nwhich implies\nP\n\u0012\nsup\nµ∈Q\n¯R(µ) −¯ET (µ) >\nq\nt + KL(µ∥π)/2N\n\u0013\n≤2Ne−2Nt,\nconcluding the proof.\n■\nRemark 22.44 Note that the proof, which follows that given in Audibert and Bous-\nquet [15], provides a family of inequalities obtained by taking λ = 2N/c in the final\nstep, with c > 1. In this case\n1 + λeλ−2N −1\nλ −2N\n≤1 +\nλ\n2N −λ =\nc\nc −1\nand one gets\nP\n\u0012\nsup\nµ∈Q\n¯R(µ) −¯ET (µ) >\nq\nt + cKL(µ∥π)/2N\n\u0013\n≤\nc\nc −12Ne−2Nt.\n♦\nRemark 22.45 One special case of theorem 22.43 is when π is a discrete probability\nmeasure supported by a subset F0 of F and µ corresponds to a deterministic pre-\ndictor optimized over F0, and is therefore a Dirac measure on some element f ∈F0.\nBecause δf has density ϕ(g) = 1/π(g) if g = f and 0 otherwise with respect to π, we\nhave KL(δf ∥π) = −logπ(f ) and theorem 22.43 implies that, with probability larger\nthan 1 −δ,\nR(f ) −ET (f ) ≤\nr\nlog2N/δ −logπ(f )\n2N\n.\nThe term log2N is however superfluous in this simple context, because one can\nwrite, for any t > 0\nP\n\u0012\nsup\nf ∈F0\nR(f ) −ET (f ) ≥\nr\nt −log(π(f ))\n2N\n\u0013\n≤\nX\nf ∈F0\ne−2N(t log(π(f ))\n2N\n) = e−2Nt\nso that, with probability 1 −δ (letting t = log(1/δ)/2N), for all f ∈F0:\nR(f ) −ET (f ) ≤\nr\n−logδ −logπ(f )\n2N\n.\n♦\n\n632\nCHAPTER 22. GENERALIZATION BOUNDS\n22.7\nApplication to model selection\nWe now describe how the previous results can, in principle, be applied to model\nselection [20]. We assume that we have a countable family of nested models classes\n(F (j),j ∈J ). Denote, as usual, by ET (f ) the empirical prediction error in the training\nset for a given function f . We will denote by ˆf (j)\nT\na minimizer of the in-sample error\nfor F (j), such that\nET ( ˆf (j)\nT ) = min\nf ∈F (j) ET (f ).\nIn the model selection problem, one would like to determine the best model class,\nj = j(T ), such that the prediction error R( ˆf (j)\nT ) is minimal, or, more realistically, de-\ntermine j∗such that R( ˆf (j∗)\nT\n) is not too far from the optimal one.\nWe will consider penalty-based methods in which one minimizes ˜ET (f ) = ET (f )+\nCT (j) to determine j(T ). The penalty, CT , may also be data-dependent, and will\ntherefore be a random variable. The previous concentration inequalities provided\nhighly probable upper-bounds for R( ˆf (j)\nT ), each exhibiting a random variable Γ(j)\nT\nthat\nis larger than R( ˆf (j)\nT ) with probability close to one. More precisely, we obtained in-\nequalities taking the form (when applied to F (j))\nP(RT ( ˆf (j)) ≥Γ(j)\nT + t) ≤cje−mt2\n(22.48)\nfor some known constants cj and m. For example, the VC-dimension bounds have\nΓ(j)\nT\n= ET ( ˆf (j)\nT ), cj = 2SF (j)(2N) and m = N/8.\nGiven such inequalities, one can develop a model selection strategy that relies on\na priori weights, provided by a sequence πj of positive numbers such that P\nj∈J πj =\n1. Define\n˜πj =\nπj/cj\nP∞\nj′=1 πj′/cj′ ,\nand let\nC(j)\nT = Γ(j)\nT −ET ( ˆf (j)\nT ) +\nr\n−\nlog ˜πj\nm\nyielding a penalty-based method that requires the minimization of\n˜ET (f ) = (ET (f ) −ET ( ˆf (j)\nT )) + Γ(j)\nT +\nr\n−\nlog ˜πj\nm\n.\nThe selected model class is then F (j∗) where j∗minimizes Γ(j)\nT +\nq\n−\nlog ˜πj\n2m .\n\n22.7. APPLICATION TO MODEL SELECTION\n633\nThe same proof as that provided at the end of section 22.6.5 justifies this proce-\ndure. Indeed, for t > 0,\nP\n\u0010\nR( ˆfT ) −˜ET ( ˆfT ) ≥t\n\u0011\n≤P\n \nmax\nj\n(R( ˆf (j)\nT ) −˜ET ( ˆf (j)\nT )) ≥t\n!\n≤P\n\nmax\nj\n(R( ˆf (j)\nT ) ≥R∗\nj + t +\nr\n−\nlog ˜πj\nm\n\n\n≤˜c\nX\nj\nπje−mt2\n≤˜ce−mt2\nwith ˜c = P∞\nj=1 πj/cj.\n\n634\nCHAPTER 22. GENERALIZATION BOUNDS\n\nBibliography\n[1] Pierre-Antoine Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization\nalgorithms on matrix manifolds. Princeton University Press, 2008.\n[2] Hirotugu Akaike. Information theory and an extension of the maximum like-\nlihood principle. In 2nd International Symposium on Information Theory, 1973.\nAkademiai Kaido, 1973.\n[3] St´ephanie Allassonniere and Laurent Younes. A stochastic algorithm for prob-\nabilistic independent component analysis. The Annals of Applied Statistics, 6\n(1):125–160, 2012.\n[4] Noga Alon, Shai Ben-David, Nicolo Cesa-Bianchi, and David Haussler. Scale-\nsensitive dimensions, uniform convergence, and learnability. Journal of the\nACM (JACM), 44(4):615–631, 1997.\n[5] Mauricio A. ´Alvarez, Lorenzo Rosasco, and Neil D. Lawrence. Kernels for\nvector-valued functions: A review. Foundations and Trends in Machine Learn-\ning, 4(3):195–266, 2012. ISSN 1935-8237. doi: 10.1561/2200000036.\n[6] Yali Amit. Convergence properties of the gibbs sampler for perturbations of\ngaussians. The Annals of Statistics, 24(1):122–140, 1996.\n[7] Yali Amit and Donald Geman. Shape quantization and recognition with ran-\ndomized trees. Neural computation, 9(7):1545–1588, 1997.\n[8] Alano Ancona, Donald Geman, Nobuyuki Ikeda, and D Geman.\nRandom\nfields and inverse problems in imaging. In Ecole d’ete de Probabilites de Saint-\nFlour XVIII-1988, pages 115–193. Springer, 1990.\n[9] Brian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Pro-\ncesses and their Applications, 12(3):313–326, 1982.\n[10] Martin Anthony and Peter L. Bartlett.\nNeural network learning: Theoretical\nfoundations. cambridge university press, 2009.\n635\n\n636\nBIBLIOGRAPHY\n[11] Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein Genera-\ntive Adversarial Networks. In Proceedings of the 34th International Conference\non Machine Learning - Volume 70, ICML’17, pages 214–223. JMLR.org, 2017.\nevent-place: Sydney, NSW, Australia.\n[12] Nachman Aronszajn. Theory of Reproducing Kernels. Trans. Am. Math. Soc.,\n68:337–404, 1950.\n[13] Krishna B. Athreya, Hani Doss, and Jayaram Sethuraman. On the convergence\nof the markov chain simulation method. The Annals of Statistics, 24(1):69–100,\n1996.\n[14] Hagai Attias.\nA Variational Baysian Framework for Graphical Models.\nIn\nNIPS, volume 12. Citeseer, 1999.\n[15] Jean-Yves Audibert and Olivier Bousquet.\nCombining pac-bayesian and\ngeneric chaining bounds. Journal of Machine Learning Research, 8(Apr):863–\n889, 2007.\n[16] Adrian Barbu and Song-Chun Zhu. Generalizing swendsen-wang to sampling\narbitrary posterior probabilities.\nIEEE Transactions on Pattern Analysis and\nMachine Intelligence, 27(8):1239–1253, 2005.\n[17] Viorel Barbu. Differential equations. Springer, 2016.\n[18] Peter Bartlett and John Shawe-Taylor. Generalization performance of support\nvector machines and other pattern classifiers. Advances in Kernel methods—\nsupport vector learning, pages 43–54, 1999.\n[19] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexi-\nties: Risk bounds and structural results. Journal of Machine Learning Research,\n3(Nov):463–482, 2002.\n[20] Peter L. Bartlett, St´ephane Boucheron, and G´abor Lugosi. Model selection and\nerror estimation. Machine Learning, 48:85–113, 2002.\n[21] Peter L. Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian.\nNearly-tight vc-dimension and pseudodimension bounds for piecewise linear\nneural networks. Journal of Machine Learning Research, 20(63):1–17, 2019.\n[22] Amir Beck. Introduction to nonlinear optimization: Theory, algorithms, and ap-\nplications with MATLAB. SIAM, 2014.\n[23] Michel Bena¨ım. Dynamics of stochastic approximation algorithms. In Semi-\nnaire de probabilites XXXIII, pages 1–68. Springer, 1999.\n[24] George Bennett. Probability inequalities for the sum of independent random\nvariables. Journal of the American Statistical Association, 57(297):33–45, 1962.\n\nBIBLIOGRAPHY\n637\n[25] Albert Benveniste, Michel M´etivier, and Pierre Priouret. Adaptive algorithms\nand stochastic approximations, volume 22. Springer Science & Business Media,\n2012.\n[26] Nils Berglund. Long-time dynamics of stochastic differential equations. arXiv\npreprint arXiv:2106.12998, 2021.\n[27] Dimitri Bertsekas. Convex optimization theory, volume 1. Athena Scientific,\n2009.\n[28] Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business\nMedia, 2013.\n[29] Peter J. Bickel and Kjell A. Doksum. Mathematical statistics: basic ideas and\nselected topics, volume I, volume 117. CRC Press, 2015.\n[30] Peter J. Bickel, Ya’acov Ritov, and Alexandre B. Tsybakov. Simultaneous anal-\nysis of lasso and dantzig selector. 2009.\n[31] Patrick Billingsley. Probability and measure. John Wiley & Sons, 2008.\n[32] Patrick Billingsley. Convergence of probability measures. John Wiley & Sons,\n2013.\n[33] Salomon Bochner. Vorlesungen ¨uber fouriersche integrale. Bull Amer Math\nSoc, 39:184, 1933.\n[34] Vladimir I. Bogachev. Measure Theory. Springer, 2007.\n[35] Joseph-Fr´ed´eric Bonnans, Jean Charles Gilbert, Claude Lemar´echal, and Clau-\ndia A. Sagastiz´abal. Numerical optimization: theoretical and practical aspects.\nSpringer Science & Business Media, 2006.\n[36] Ingwer Borg and Patrick J.F. Groenen. Modern multidimensional scaling: Theory\nand applications. Springer Science & Business Media, 2005.\n[37] Jonathan Borwein and Adrian S. Lewis. Convex analysis and nonlinear opti-\nmization: theory and examples. Springer Science & Business Media, 2010.\n[38] St´ephane Boucheron, G´abor Lugosi, and Pascal Massart. A sharp concentra-\ntion inequality with applications. Random Structures & Algorithms, 16(3):277–\n292, 2000.\n[39] Olivier Bousquet and Andr´e Elisseeff. Stability and generalization. Journal of\nmachine learning research, 2(Mar):499–526, 2002.\n\n638\nBIBLIOGRAPHY\n[40] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.\nDistributed optimization and statistical learning via the alternating direction\nmethod of multipliers. Foundations and Trends® in Machine learning, 3(1):1–\n122, 2011.\n[41] Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.\n[42] Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001.\n[43] Leo Breiman, Jerome Friedman, Charles J Stone, and Richard A. Olshen. Clas-\nsification and regression trees. CRC press, 1984.\n[44] Dmitri Burago, Iu D. Burago, Yuri Burago, Sergei A. Ivanov, and Sergei Ivanov.\nA course in metric geometry, volume 33. American Mathematical Soc., 2001.\n[45] Jian-Feng Cai, Emmanuel J. Cand`es, and Zuowei Shen.\nA singular value\nthresholding algorithm for matrix completion. SIAM Journal on optimization,\n20(4):1956–1982, 2010.\n[46] Tadeusz Cali´nski and Jerzy Harabasz. A dendrite method for cluster analysis.\nCommunications in Statistics-theory and Methods, 3(1):1–27, 1974.\n[47] Emmanuel J. Candes and Terence Tao. Decoding by linear programming. IEEE\nTrans. information theory, 51(12):4203–4215, 2005.\n[48] Emmanuel J. Candes and Terence Tao. The dantzig selector: statistical esti-\nmation when p is much larget. Annals of statistics, 35, 2007.\n[49] Emmanuel J. Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal\ncomponent analysis? Journal of the ACM (JACM), 58(3):11, 2011.\n[50] John Canny. Gap: a factor model for discrete data. In Proceedings of the 27th\nannual international ACM SIGIR conference on Research and development in in-\nformation retrieval, pages 122–129, 2004.\n[51] B. Chalmond. An iterative Gibbsian technique for reconstruction of m-ary\nimages. Pattern recognition, 22(6):747–761, 1989. ISSN 0031-3203.\n[52] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duve-\nnaud.\nNeural ordinary differential equations.\nIn S. Bengio, H. Wallach,\nH. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 31, pages 6571–6583. Curran Asso-\nciates, Inc., 2018.\n[53] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system.\nIn Proceedings of the 22nd acm sigkdd international conference on knowledge dis-\ncovery and data mining, pages 785–794, 2016.\n\nBIBLIOGRAPHY\n639\n[54] Pierre Comon. Independent component analysis, a new concept? Signal pro-\ncessing, 36(3):287–314, 1994.\n[55] Thomas M. Cover and Joy A. Thomas. Elements of information theory. John\nWiley & Sons, 2012.\n[56] Robert G. Cowell, A. Philip Dawid, Steffen L. Lauritzen, and David J. Spiegel-\nhalter. Probabilistic networks and expert systems. Springer, 2007.\n[57] George Darmois. Analyse g´en´erale des liaisons stochastiques: etude partic-\nuli`ere de l’analyse factorielle lin´eaire. Revue de l’Institut international de statis-\ntique, pages 2–8, 1953.\n[58] Bernard Delyon, Marc Lavielle, and Eric Moulines. Convergence of a stochas-\ntic approximation version of the em algorithm. Annals of statistics, pages 94–\n128, 1999.\n[59] Amir Dembo and Ofer Zeitouni. Large deviations techniques and applica-\ntions. 1998. Applications of Mathematics, 38, 2011.\n[60] Luc Devroye, L´azl´o Gy¨orfi, and G´abor Lugosi. A Probabilistic Theory of Pattern\nRecognition. Springer, 1996.\n[61] Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation dis-\ntance between high-dimensional gaussians with the same mean. arXiv preprint\narXiv:1810.08693, 2018.\n[62] Jean Dieudonn´e. Infinitesimal Calculus. Houghton Mifflin, 1971.\n[63] Edsger W. Dijkstra. A note on two problems in connexion with graphs. Nu-\nmerische mathematik, 1(1):269–271, 1959. ISSN 0029-599X.\n[64] Petros Drineas, Alan Frieze, Ravi Kannan, Santosh Vempala, and Vish-\nwanathan Vinay. Clustering large graphs via the singular value decomposi-\ntion. Machine learning, 56:9–33, 2004.\n[65] Simon Duane, Anthony D. Kennedy, Brian J. Pendleton, and Duncan Roweth.\nHybrid monte carlo. Physics letters B, 195(2):216–222, 1987.\n[66] Richard M. Dudley. Real analysis and probability. Chapman and Hall/CRC,\n2018.\n[67] Marie Duflo. Random iterative models, volume 34. Springer Science & Business\nMedia, 2013.\n[68] HA Eiselt, Carl-Louis Sandblom, et al. Nonlinear optimization: Methods and\napplications. Springer, 2019.\n\n640\nBIBLIOGRAPHY\n[69] Stewart N. Ethier and Thomas G. Kurtz. Markov processes: Characterization\nand convergence. 1986.\n[70] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and\nAndrew Zisserman. The pascal visual object classes (voc) challenge. Interna-\ntional journal of computer vision, 88(2):303–338, 2010.\n[71] James A. Fill. An interruptible algorithm for perfect sampling via Markov\nchains. The Annals of Applied Probability, 8(1):131–162, 1998.\n[72] P. Thomas Fletcher and Sarang Joshi. Principal geodesic analysis on symmetric\nspaces: Statistics of diffusion tensors. In Computer vision and mathematical\nmethods in medical and biomedical image analysis, pages 87–98. Springer, 2004.\n[73] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of\non-line learning and an application to boosting. Journal of computer and system\nsciences, 55(1):119–139, 1997. Publisher: Elsevier.\n[74] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic re-\ngression: a statistical view of boosting (with discussion and a rejoinder by the\nauthors). The annals of statistics, 28(2):337–407, 2000.\n[75] Jerome H. Friedman. Greedy function approximation: a gradient boosting\nmachine. Annals of statistics, pages 1189–1232, 2001. Publisher: JSTOR.\n[76] Dan Geiger and Judea Pearl. On the logic of causal models. In Machine intel-\nligence and pattern recognition, volume 9, pages 3–14. Elsevier, 1990.\n[77] Dan Geiger, Thomas Verma, and Judea Pearl. Identifying independence in\nbayesian networks. Networks, 20(5):507–534, August 1990. ISSN 00283045.\ndoi: 10.1002/net.3230200504.\n[78] Donald Geman, Christian d’Avignon, Daniel Q. Naiman, and Raimond L\nWinslow. Classifying gene expression profiles from pairwise mrna compar-\nisons. Statistical applications in genetics and molecular biology, 3(1):1–19, 2004.\n[79] Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions,\nand the bayesian restoration of images. IEEE Transactions on pattern analysis\nand machine intelligence, (6):721–741, 1984.\n[80] Stuart Geman and Chii-Ruey Hwang. Nonparametric maximum likelihood\nestimation by the method of sieves. The Annals of Statistics, pages 401–414,\n1982.\n[81] M. Gondran and M. Minoux. Graphs and algorithms. John Wiley & Sons, 1983.\n\nBIBLIOGRAPHY\n641\n[82] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-\nFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adver-\nsarial nets. In Advances in neural information processing systems, pages 2672–\n2680, 2014.\n[83] Ulf Grenander. Abstract Inference. Wiley, 1981.\n[84] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and\nAaron C. Courville. Improved training of Wasserstein GANs. In Advances\nin neural information processing systems, pages 5767–5777, 2017.\n[85] Madan M. Gupta and J. Qi. Theory of T-norms and fuzzy inference methods.\nFuzzy Sets and Systems, 40(3):431–450, April 1991. ISSN 0165-0114.\n[86] Te Sun Han. Nonnegative entropy measures of multivariate symmetric corre-\nlations. Information and Control, 36:133–156, 1978.\n[87] Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman. The elements of\nstatistical learning. Springer, 2003.\n[88] W. Keith Hastings. Monte carlo sampling methods using markov chains and\ntheir applications. 1970.\n[89] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual\nlearning for image recognition. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 770–778. IEEE, 2016.\n[90] Geoffrey E. Hinton and Sam Roweis. Stochastic neighbor embedding. Ad-\nvances in neural information processing systems, 15:857–864, 2002.\n[91] Leslie M. Hocking. Optimal Control: An Introduction to the Theory with Appli-\ncations. Oxford University Press, 1991.\n[92] Wassily Hoeffding. Probability inequalities for sums of bounded random vari-\nables. In The Collected Works of Wassily Hoeffding, pages 409–426. Springer,\n1994.\n[93] Roger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge university\npress, 2012.\n[94] Aapo Hyv¨arinen. New approximations of differential entropy for independent\ncomponent analysis and projection pursuit. In Advances in neural information\nprocessing systems, pages 273–279, 1998.\n[95] Aapo Hyv¨arinen and Peter Dayan. Estimation of non-normalized statistical\nmodels by score matching. Journal of Machine Learning Research, 6(4), 2005.\n\n642\nBIBLIOGRAPHY\n[96] Nobuyuki Ikeda and Shinzo Watanabe.\nStochastic differential equations and\ndiffusion processes. Elsevier, 1981.\n[97] Tommi Sakari Jaakkola.\nVariational methods for inference and estimation in\ngraphical models. PhD Thesis, Massachusetts Institute of Technology, 1997.\n[98] Vojtech Jarnik. O jistem problemu minimalnim (about a certain minimal prob-\nlem). Prace Moravske Prirodovedecke Spolecnosti, 6:57–63, 1930.\n[99] Finn Jensen and Frank Jensen. Optimal junction trees. In Proceedings of the\nTenth Conference on Uncertainty in Artificial Intelligence, pages 360–366, 1994.\n[100] Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K.\nSaul. An introduction to variational methods for graphical models. Machine\nlearning, 37(2):183–233, 1999.\n[101] Abram M. Kagan, Calyampudi Radhakrishna Rao, and Yurij Vladimirovich\nLinnik. Characterization problems in mathematical statistics. 1973.\n[102] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimiza-\ntion. arXiv preprint arXiv:1412.6980, 2014.\n[103] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. 2014.\n[104] Diederik P. Kingma and Max Welling. An Introduction to Variational Autoen-\ncoders. Foundations and Trends® in Machine Learning, 12(4):307–392, 2019.\nPublisher: Now Publishers, Inc.\n[105] John Kingman. Completely random measures. Pacific Journal of Mathematics,\n21(1):59–78, 1967.\n[106] Peter E. Kloeden and Eckhard Platen. Numerical solutions of stochastic differen-\ntial equations. Springer, 1992.\n[107] Ivan Kobyzev, Simon J.D. Prince, and Marcus A. Brubaker. Normalizing flows:\nAn introduction and review of current methods. IEEE transactions on pattern\nanalysis and machine intelligence, 43(11):3964–3979, 2020.\n[108] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and\ntechniques. The MIT Press, 2009.\n[109] Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions\nand bounding the generalization error of combined classifiers. The Annals of\nStatistics, 30(1):1–50, 2002.\n[110] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classifica-\ntion with deep convolutional neural networks. Communications of the ACM,\n60(6):84–90, 2017.\n\nBIBLIOGRAPHY\n643\n[111] Wojtek J. Krzanowski and Y.T. Lai. A criterion for determining the number of\ngroups in a data set using sum-of-squares clustering. Biometrics, pages 23–34,\n1988.\n[112] Estelle Kuhn and Marc Lavielle. Coupling a stochastic approximation version\nof EM with an MCMC procedure. ESAIM: Probability and Statistics, 8:115–131,\n2004. Publisher: EDP Sciences.\n[113] Harold Kushner and G. George Yin.\nStochastic approximation and recursive\nalgorithms and applications, volume 35. Springer Science & Business Media,\n2003.\n[114] Steffen L Lauritzen. Graphical models, volume 17. Clarendon Press, 1996.\n[115] Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech,\nand time series. The handbook of brain theory and neural networks, 3361(10):\n1995, 1995.\n[116] Yann LeCun, Bernhard Boser, John S. Denker, Donnie Henderson, Richard E.\nHoward, Wayne Hubbard, and Lawrence D. Jackel. Backpropagation applied\nto handwritten zip code recognition. Neural computation, 1(4):541–551, 1989.\n[117] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperime-\ntry and processes. Springer Science & Business Media, 1991.\n[118] Erich L. Lehmann and George Casella. Theory of point estimation. Springer\nScience & Business Media, 2006.\n[119] Benedict Leimkuhler and Sebastian Reich. Simulating Hamiltonian Dynamics.\nCambridge Monographs on Applied and Computational Mathematics. Cam-\nbridge University Press, 2005.\n[120] Lennart Ljung. Analysis of recursive stochastic algorithms. IEEE Transactions\non Automatic Control, 22(4):551–575, August 1977. ISSN 1558-2523. Confer-\nence Name: IEEE Transactions on Automatic Control.\n[121] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on infor-\nmation theory, 28(2):129–137, 1982.\n[122] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE.\nJournal of machine learning research, 9(Nov):2579–2605, 2008.\n[123] Jack Macki and Aaron Strauss.\nIntroduction to Optimal Control Theory.\nSpringer Science & Business Media, 2012.\n[124] James MacQueen. Some methods for classification and analysis of multivari-\nate observations. In Proceedings of the fifth Berkeley symposium on mathematical\nstatistics and probability, volume 1, pages 281–297. Oakland, CA, USA, 1967.\n\n644\nBIBLIOGRAPHY\n[125] Adam a Margolin, Ilya Nemenman, Katia Basso, Chris Wiggins, Gustavo\nStolovitzky, Riccardo Dalla Favera, and Andrea Califano. ARACNE: an al-\ngorithm for the reconstruction of gene regulatory networks in a mammalian\ncellular context. BMC bioinformatics, 7 Suppl 1:S7, January 2006. ISSN 1471-\n2105. doi: 10.1186/1471-2105-7-S1-S7.\n[126] Enzo Marinari and Giorgio Parisi. Simulated tempering: a new monte carlo\nscheme. Europhysics letters, 19(6):451, 1992.\n[127] Pascal Massart.\nConcentration inequalities and model selection, volume 6.\nSpringer, 2007.\n[128] David A. McAllester. Pac-bayesian model averaging. In COLT, volume 99,\npages 164–170. Citeseer, 1999.\n[129] James A. McHugh. Algorithmic graph theory. New Jersey: Prentice-Hall Inc,\n1990.\n[130] Leland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold\nApproximation and Projection for Dimension Reduction. arXiv:1802.03426\n[cs, stat], September 2020. arXiv: 1802.03426.\n[131] Henry P. McKean. Stochastic integrals, volume 353. American Mathematical\nSociety, 1969.\n[132] Kerrie L. Mengersen and Richard L. Tweedie.\nRates of convergence of the\nhastings and metropolis algorithms. The annals of Statistics, 24(1):101–121,\n1996.\n[133] Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Au-\ngusta H. Teller, and Edward Teller. Equation of state calculations by fast com-\nputing machines. The journal of chemical physics, 21(6):1087–1092, 1953.\n[134] Sean P. Meyn and Richard L. Tweedie. Stability of markovian processes ii:\nContinuous-time processes and sampled chains. Advances in Applied Probabil-\nity, 25(3):487–517, 1993.\n[135] Sean P. Meyn and Richard L. Tweedie. Stability of markovian processes iii:\nFoster–lyapunov criteria for continuous-time processes. Advances in Applied\nProbability, 25(3):518–548, 1993.\n[136] Sean P. Meyn and Richard L. Tweedie. Markov chains and stochastic stability.\nSpringer Science & Business Media, 2012.\n[137] Leon Mirsky. A trace inequality of john von neumann. Monatshefte f¨ur mathe-\nmatik, 79(4):303–306, 1975.\n\nBIBLIOGRAPHY\n645\n[138] Michel M´etivier and Pierre Priouret. Th´eor`emes de convergence presque sure\npour une classe d’algorithmes stochastiques `a pas d´ecroissant. Probability The-\nory and related fields, 74(3):403–428, 1987. Publisher: Springer.\n[139] Elizbar A. Nadaraya. On estimating regression. Theory of Probability & Its\nApplications, 9(1):141–142, 1964.\n[140] Radford M. Neal. Sampling from multimodal distributions using tempered\ntransitions. Statistics and computing, 6:353–366, 1996.\n[141] Radford M. Neal. Markov chain sampling methods for dirichlet process mix-\nture models. Journal of computational and graphical statistics, 9(2):249–265,\n2000.\n[142] Radford M. Neal.\nMcmc using hamiltonian dynamics.\narXiv preprint\narXiv:1206.1901, 2012.\n[143] Radford M. Neal and Geoffrey E. Hinton. A view of the EM algorithm that jus-\ntifies incremental, sparse, and other variants. In Learning in graphical models,\npages 355–368. Springer, 1998.\n[144] R. E. Neapolitan. Learning Bayesian networks. Prentice Hall, 2004.\n[145] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.\nRobust Stochastic Approximation Approach to Stochastic Programming.\nSIAM Journal on Optimization, 19(4):1574–1609, January 2009. ISSN 1052-\n6234. Publisher: Society for Industrial and Applied Mathematics.\n[146] Jorge Nocedal and Stephen J. Wright. Nonlinear Equations. Springer, 2006.\n[147] Esa Nummelin. General irreducible Markov chains and non-negative operators.\nNumber 83. Cambridge University Press, 2004.\n[148] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mo-\nhamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic\nmodeling and inference. Journal of Machine Learning Research, 22(57):1–64,\n2021.\n[149] Panos M. Pardalos and Jue Xue. The maximum clique problem. Journal of\nGlobal Optimization, 4(3):301–328, 1994. ISSN 0925-5001.\n[150] Emanuel Parzen. On estimation of a probability density function and mode.\nThe annals of mathematical statistics, 33(3):1065–1076, 1962.\n[151] Judea Pearl. Probabilistic reasoning in intelligent systems. Morgan Kaufmann,\n1988, 2012.\n\n646\nBIBLIOGRAPHY\n[152] Jiming Peng and Yu Wei. Approximating k-means-type clustering via semidef-\ninite programming. SIAM journal on optimization, 18(1):186–205, 2007.\n[153] Jiming Peng and Yu Xia. A new theoretical framework for k-means-type clus-\ntering. Foundations and advances in data mining, pages 79–96, 2005. Publisher:\nSpringer.\n[154] Odile Pons. Functional estimation for density, regression models and processes.\nWorld scientific, 2011.\n[155] Robert C. Prim. Shortest connection networks and some generalizations. Bell\nsystem technical journal, 36(6):1389–1401, 1957.\n[156] James G. Propp and David B. Wilson. Exact sampling with coupled Markov\nchains and applications to statistical mechanics. Random Structures and Algo-\nrithms, 9(1&2):223–252, 1996.\n[157] James G. Propp and David B. Wilson. How to get a perfectly random sam-\nple from a generic Markov chain and generate a random spanning tree of a\ndirected graph. Journal of Algorithms, 27:170–217, 1998.\n[158] Jim O. Ramsay and Bernard W. Silverman. Functional Data Analysis. Springer-\nVerlag, 1997.\n[159] BLS Prakasa Rao. Nonparametric functional estimation. Academic press, 1983.\n[160] Daniel Revuz. Markov chains. Elsevier, 2008.\n[161] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing\nflows. In International conference on machine learning, pages 1530–1538. PMLR,\n2015.\n[162] Jorma Rissanen. Stochastic complexity in statistical inquiry. World Scientific,\n1989.\n[163] Herbert Robbins and Sutton Monro. A stochastic approximation method. In\nHerbert Robbins Selected Papers, pages 102–109. Springer, 1985.\n[164] Gareth O. Roberts and Nicholas G. Polson. On the geometric convergence of\nthe gibbs sampler. Journal of the Royal Statistical Society Series B: Statistical\nMethodology, 56(2):377–384, 1994.\n[165] Gareth O. Roberts and Jeffrey S. Rosenthal. General state space markov chains\nand mcmc algorithms. Probability Surveys, 1:20–71, 2004.\n[166] Gareth O. Roberts and Richard L. Tweedie.\nExponential convergence of\nlangevin distributions and their discrete approximations. Bernoulli, 2(4):341–\n363, 1996.\n\nBIBLIOGRAPHY\n647\n[167] R. Tyrrell Rockafellar. Convex analysis, volume 18. Princeton university press,\n1970.\n[168] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional\nNetworks for Biomedical Image Segmentation.\nIn Nassir Navab, Joachim\nHornegger, William M. Wells, and Alejandro F. Frangi, editors, Medical Im-\nage Computing and Computer-Assisted Intervention – MICCAI 2015, Lecture\nNotes in Computer Science, pages 234–241, Cham, 2015. Springer Interna-\ntional Publishing. ISBN 978-3-319-24574-4.\n[169] Kenneth Rose, Eitan Gurewitz, and Geoffrey Fox. A deterministic annealing\napproach to clustering. Pattern Recognition Letters, 11(9):589–594, 1990.\n[170] Peter J. Rousseeuw. Silhouettes: a graphical aid to the interpretation and val-\nidation of cluster analysis. Journal of computational and applied mathematics,\n20:53–65, 1987.\n[171] Walter Rudin. Real and Complex Analysis. Tata McGraw Hill, 1966.\n[172] Robert E. Schapire. The strength of weak learnability. Machine learning, 5(2):\n197–227, 1990.\n[173] Isaac J. Schoenberg. Metric spaces and completely monotone functions. An-\nnals of Mathematics, pages 811–841, 1938.\n[174] Gideon Schwarz. Estimating the dimension of a model. The annals of statistics,\n6(2):461–464, 1978.\n[175] Claude E. Shannon. A mathematical theory of communication. The Bell system\ntechnical journal, 27(3):379–423, 1948.\n[176] Claude E. Shannon. Communication in the presence of noise. Proc. Institute\nof Radio Engineers, 37(1):10–21, 1949.\n[177] Simon J. Sheather and Michael C. Jones.\nA reliable data-based bandwidth\nselection method for kernel density estimation. Journal of the Royal Statistical\nSociety: Series B (Methodological), 53(3):683–690, 1991.\n[178] Bernard W. Silverman. Density estimation for statistics and data analysis. Chap-\nman et Hall, 1998.\n[179] Viktor Pavlovich Skitovich. Linear forms of independent random variables\nand the normal distribution law. Izvestiya Rossiiskoi Akademii Nauk. Seriya\nMatematicheskaya, 18(2):185–200, 1954.\n[180] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score match-\ning: A scalable approach to density and score estimation. In Uncertainty in\nArtificial Intelligence, pages 574–584. PMLR, 2020.\n\n648\nBIBLIOGRAPHY\n[181] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Rus-\nlan Salakhutdinov. Dropout: a simple way to prevent neural networks from\noverfitting. The Journal of Machine Learning Research, 15(1):1929–1958, 2014.\n[182] Hugo Steinhaus. Sur la division des corp materiels en parties. Bull. Acad.\nPolon. Sci, 1(804):801, 1956.\n[183] Charles J. Stone. Consistent nonparametric regression. The annals of statistics,\npages 595–620, 1977.\n[184] Mervyn Stone. Cross-validatory choice and assessment of statistical predic-\ntions. Journal of the royal statistical society: Series B (Methodological), 36(2):\n111–133, 1974.\n[185] Catherine A. Sugar and Gareth M. James. Finding the number of clusters in a\ndataset: An information-theoretic approach. Journal of the American Statistical\nAssociation, 98(463):750–763, 2003.\n[186] Robert H. Swendsen and Jian-Sheng Wang. Nonuniversal critical dynamics in\nmonte carlo simulations. Physical review letters, 58(2):86, 1987.\n[187] Esteban G. Tabak and Eric Vanden-Eijnden. Density estimation by dual ascent\nof the log-likelihood. 2010.\n[188] Michel Talagrand. The generic chaining: upper and lower bounds of stochastic\nprocesses. Springer Science & Business Media, 2006.\n[189] Michel Talagrand. Upper and lower bounds for stochastic processes: modern meth-\nods and classical problems, volume 60.\nSpringer Science & Business Media,\n2014.\n[190] Aik Choon Tan, Daniel Q. Naiman, Lei Xu, Raimond L. Winslow, and Don-\nald Geman. Simple decision rules for classifying human cancers from gene\nexpression profiles. Bioinformatics, 21(20):3896–3904, 2005.\n[191] Robert Tibshirani, Guenther Walther, and Trevor Hastie. Estimating the num-\nber of clusters in a data set via the gap statistic. Journal of the Royal Statistical\nSociety: Series B (Statistical Methodology), 63(2):411–423, 2001.\n[192] Luke Tierney. Markov Chains for Exploring Posterior Distributions. Annals\nof Statistics, 22(4):1701–1728, December 1994. ISSN 0090-5364, 2168-8966.\nPublisher: Institute of Mathematical Statistics.\n[193] Laurens Van Der Maaten. Accelerating t-sne using tree-based algorithms. The\nJournal of Machine Learning Research, 15(1):3221–3245, 2014.\n[194] Aad W. Van der Vaart. Asymptotic statistics, volume 3. Cambridge university\npress, 2000.\n\nBIBLIOGRAPHY\n649\n[195] Aad W. Van der Vaart and John A. Wellner. Weak convergence and empirical\nprocesses with applications to statistics. Springer, 1996.\n[196] Vladimir Vapnik. Statistical learning theory. 1998. Wiley, New York, 1998.\n[197] Vladimir Vapnik. The nature of statistical learning theory. Springer science &\nbusiness media, 2013.\n[198] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neural Information Processing\nSystems, volume 30. Curran Associates, Inc., 2017.\n[199] Roman Vershynin. High-dimensional probability: An introduction with applica-\ntions in data science, volume 47. Cambridge University Press, 2018.\n[200] Rene Vidal, Yi Ma, and Shankar Sastry.\nGeneralized principal component\nanalysis (gpca). IEEE transactions on pattern analysis and machine intelligence,\n27(12):1945–1959, 2005.\n[201] Grace Wahba. Spline Models for Observational Data. SIAM, 1990.\n[202] Geoffrey S. Watson. Smooth regression analysis. Sankhy¯a: The Indian Journal\nof Statistics, Series A, pages 359–372, 1964.\n[203] Gerhard Winkler. Image analysis, random fields and Markov chain Monte Carlo\nmethods. Springer, 1995,2003.\n[204] Stephen J. Wright and Benjamin Recht. Optimization for data analysis. Cam-\nbridge University Press, 2022.\n[205] Kˆosaku Yosida. Functional Analysis. Springer, 1970.\n[206] Laurent Younes. Estimation and annealing for gibbsian fields. Ann. de l’Inst.\nHenri Poincar´e, 2, 1988.\n[207] Laurent Younes. Parametric inference for imperfectly observed gibbsian fields.\nProb. Thry. Rel. Fields, 82:625–645, 1989.\n[208] Laurent Younes. On the convergence of markovian stochastic algorithms with\nrapidly decreasing ergodicity rates.\nStochastics: An International Journal of\nProbability and Stochastic Processes, 65(3-4):177–228, 1999.\n[209] Laurent Younes. Diffeomorphic learning. Journal of Machine Learning Research,\n21:1 – 28, 2020.\n[210] Lotfi A. Zadeh. Fuzzy sets. In Fuzzy sets, fuzzy logic, and fuzzy systems: selected\npapers by Lotfi A Zadeh, pages 394–432. World Scientific, 1996.",
    "pdf_filename": "Introduction to Machine Learning.pdf"
}