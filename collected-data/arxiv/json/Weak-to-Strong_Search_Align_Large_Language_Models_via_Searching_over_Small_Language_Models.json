{
    "title": "Weak-to-Strong Search Align Large Language Models via Searching over Small Language Models",
    "abstract": "Large language models are usually fine-tuned to align with human preferences. However, fine-tuning a large language model can be challenging. In this work, we introduce weak-to-strong search, framing the alignment of a large language model as a test-time greedy search to maximize the log-probability difference between small tuned and untuned models while sampling from the frozen large model. This method serves both as (1) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (2) an instance of weak-to-strong generalization that enhances a strong model with weak test-time guidance. Empirically, we demonstrate the flexibility of weak-to-strong search across different tasks. In controlled-sentiment generation and summarization, we use tuned and untuned gpt2s to improve the alignment of large models without additional training. Crucially, in a more difficult instruction-following bench- mark, AlpacaEval 2.0, we show that reusing off-the-shelf small models (e.g., zephyr-7b-beta and its untuned version) can improve the length-controlled win rates of both white-box and black-box large models against gpt-4-turbo (e.g., 34.4% →37.9% for Llama-3-70B-Instruct and 16.0% →20.1% for gpt-3.5-turbo-instruct), despite the small models’ low win rates ≈10.0%. 1 Introduction Learning-based algorithms [1, 2, 3, 4, 5] have become the standard approach for aligning large language models (LLMs) with human preferences [3, 6, 7, 8, 9, 10]. However, fine-tuning large language models is resource-intensive and difficult to implement [4]. These challenges have motivated recent studies on search-based algorithms that keep the large language models frozen and steer their decoding with test-time guidance [11, 12, 13, 14, 15, 16, 17]. Typical examples of search-based algorithms include rejection sampling [16, 17] and Monte Carlo Tree Search [18, 19]. These search- based algorithms are promising as they can reuse the same guiding signal to steer the decoding of any large language model without additional training. However, existing search-based methods either simplify the search over tokens as a bandit problem [16, 17], which limits their steerability, or require a value function learned from scratch to address preference reward sparsity and prune search space [13, 18, 14], which can be as difficult as fine-tuning a large language model. To make search-based algorithms better suited for aligning large language models, we introduce weak-to-strong search, a simple algorithm that frames the alignment of a large model as a test-time search over the log-probabilities of small language models. This algorithm makes two contributions: (1) First, it builds on the theoretical foundation of the token-level MDP for alignment [20], using the 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2405.19262v3  [cs.CL]  19 Nov 2024",
    "body": "Weak-to-Strong Search:\nAlign Large Language Models via\nSearching over Small Language Models\nZhanhui Zhou∗†, Zhixuan Liu∗, Jie Liu, Zhichen Dong, Chao Yang, Yu Qiao\nShanghai Artificial Intelligence Laboratory\n∗Core Contribution, †Corresponding Author\nasap.zzhou@gmail.com\nCode: https://github.com/ZHZisZZ/weak-to-strong-search\nAbstract\nLarge language models are usually fine-tuned to align with human preferences.\nHowever, fine-tuning a large language model can be challenging. In this work,\nwe introduce weak-to-strong search, framing the alignment of a large language\nmodel as a test-time greedy search to maximize the log-probability difference\nbetween small tuned and untuned models while sampling from the frozen large\nmodel. This method serves both as (1) a compute-efficient model up-scaling\nstrategy that avoids directly tuning the large model and as (2) an instance of\nweak-to-strong generalization that enhances a strong model with weak test-time\nguidance. Empirically, we demonstrate the flexibility of weak-to-strong search\nacross different tasks. In controlled-sentiment generation and summarization, we\nuse tuned and untuned gpt2s to improve the alignment of large models without\nadditional training. Crucially, in a more difficult instruction-following bench-\nmark, AlpacaEval 2.0, we show that reusing off-the-shelf small models (e.g.,\nzephyr-7b-beta and its untuned version) can improve the length-controlled\nwin rates of both white-box and black-box large models against gpt-4-turbo\n(e.g., 34.4% →37.9% for Llama-3-70B-Instruct and 16.0% →20.1% for\ngpt-3.5-turbo-instruct), despite the small models’ low win rates ≈10.0%.\n1\nIntroduction\nLearning-based algorithms [1, 2, 3, 4, 5] have become the standard approach for aligning large\nlanguage models (LLMs) with human preferences [3, 6, 7, 8, 9, 10]. However, fine-tuning large\nlanguage models is resource-intensive and difficult to implement [4]. These challenges have motivated\nrecent studies on search-based algorithms that keep the large language models frozen and steer their\ndecoding with test-time guidance [11, 12, 13, 14, 15, 16, 17]. Typical examples of search-based\nalgorithms include rejection sampling [16, 17] and Monte Carlo Tree Search [18, 19]. These search-\nbased algorithms are promising as they can reuse the same guiding signal to steer the decoding\nof any large language model without additional training. However, existing search-based methods\neither simplify the search over tokens as a bandit problem [16, 17], which limits their steerability, or\nrequire a value function learned from scratch to address preference reward sparsity and prune search\nspace [13, 18, 14], which can be as difficult as fine-tuning a large language model.\nTo make search-based algorithms better suited for aligning large language models, we introduce\nweak-to-strong search, a simple algorithm that frames the alignment of a large model as a test-time\nsearch over the log-probabilities of small language models. This algorithm makes two contributions:\n(1) First, it builds on the theoretical foundation of the token-level MDP for alignment [20], using the\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2405.19262v3  [cs.CL]  19 Nov 2024\n\nLlama2-7B\nLlama2-70B\nLlama3-8B\nLlama3-70B\nGPT3.5\n10\n15\n20\n25\n30\n35\n40\nAlpacaEval 2.0 LC Win Rate (%)\n10.1\n16.2\n22.9\n34.4\n16.0\n13.7\n19.1\n27.2\n37.9\n20.1\nzephyr-7b-beta (tuned)\nmistral-7b-sft-beta (untuned)\nBase\nWeak-to-strong search\nFigure 1: Weak-to-strong search enhances the alignment of large models through test-time guidance\nfrom small models (dashed lines). This method is applicable to white-box models that use the same\nor different vocabularies as the small models, as well as to black-box models. We present the results\nfor the instruction-tuned models from each family (e.g., Llama2-7B denotes Llama-2-7b-chat).\nlog-probability difference between small tuned and untuned language models as both reward and\nvalue [4, 20] to guide the decoding of a large model (Section 4.1). Theoretically, this formulation is\nsuitable for search as it converts the otherwise sequence-level sparse preference reward function to a\nper-token dense reward function, which can be summed up as a value function [20]. Practically, this\nformulation allows the reuse of off-the-shelf small tuned and untuned language model pairs as steering\nforces, avoiding the need to train a reward or value model from scratch. (2) Second, it introduces a\nbeam search variant, Chunk-level Beam Search (CBS), tailored for optimizing the proposed search\nobjective. CBS guides the large language model towards high-reward regions by alternating between\nsampling from the frozen large model and expanding promising states as evaluated by the small\ntuned and untuned models (Section 4.2). Especially, when the small models are weaker than the large\nmodel, our method can be viewed as an instance of weak-to-strong generalization [21] that makes the\nstrong model stronger with weak test-time guidance (Figure 1).\nEmpirically, we verify weak-to-strong search’s flexibility in various tasks (Section 5). First, in\ncontrolled-sentiment generation [22] and summarization [2], our method uses small language models\nof 124M parameters (i.e., gpt2) to effectively steer much larger language models from the GPT-2\n(e.g., gpt2-xl) [23], Llama-2 [7] and Llama-3 [24] families, at least as effective as existing methods.\nThen, in a more difficult instruction-following benchmark, AlpacaEval 2.0 [25], we show reusing\noff-the-shelf small models (e.g., zephyr-7b-beta and its untuned version) as test-time guidance can\nsignificantly improve the length-controlled win rates of both white-box and black-box large models\nagainst gpt-4-turbo (e.g, 34.4% →37.9% for Llama-3-70B-Instruct, and 16.0% →20.1%\nfor gpt-3.5-turbo-instruct), despite the small models’ low win rates ≈10.0% (Figure 1).\n2\nRelated Work\nLarge unsupervised language models trained on internet-scale corpus acquire broad knowledge and\nabilities [26, 27, 28]. However, these large pre-trained language models may not always align with\nhuman values. To instill the desired behaviors into language models, most existing methods fine-tune\nthese pre-trained language models on human comparisons of model-generated responses [1, 2, 3,\n4, 7, 24, 6, 29]. Despite these successes, fine-tuning a large language model requires substantial\ncomputational resources and engineering effort. These problems are compounded by the reality that\ndifferent humans have different values [3, 30, 13, 31, 32, 33], as it is nearly impossible to train a new\nlarge language model from scratch for individual preference. In light of these issues, our work takes a\nsearch-based approach, folding as much of the complexity of alignment as possible into the decoding\nphase. This allows us to keep the large pre-trained language models frozen, steering their outputs at\ntest time with only small models that are easier to obtain.\nFraming alignment as a test-time search to maximize a reward function is not a novel formulation.\nHowever, most existing works either simplify autoregressive decoding as a bandit problem [16, 17],\nwhich limits their steerability, or require a value function learned from scratch to handle sparse\n2\n\npreference rewards and prune search space [13, 18], which can be as difficult as training a large\nlanguage model from scratch. Our work avoids these issues by parametrizing the sparse preference\nreward function with the log-probability difference between small tuned and untuned language\nmodels [4]. This parametrization not only simplifies the search objective, allowing a simple greedy\nsearch algorithm to generate good results, but also reuses off-the-shelf models as steering forces,\neliminating the need to train a reward or critic model from scratch.\nConcurrently with our work, Rafailov et al. [20] proposes a token-level MDP interpretation for\nlanguage model alignment, demonstrating that a greedy probability search over a trained language\nmodel can achieve improvements over regular decoding. Our work builds on their theoretical\nfoundations and proposes a practical greedy search algorithm designed for weak-to-strong guidance.\nThe idea of using small language models to align large language models has arisen in many recent\nworks. The most related is proxy or emulated fine-tuning [34, 12, 11, 35], which uses the distributional\ndifference of a small tuned and untuned model pair to modify the output distribution of a large model,\napproximating the output of the directly tuned large model. However, these methods require that both\nsmall and large models share the same vocabulary, limiting their practical applications. In contrast,\nour approach does not modify the sampling distribution of the large model at the token level. Instead,\nwe perform a tree search that periodically prioritizes the most promising states for further expansion\n(as evaluated by the small models) while sampling from the frozen large model’s distribution. Thus\nour approach does not require shared vocabulary and is applicable to black-box language models.\n3\nPreliminaries\nIn this section, we introduce the mathematical formulation of alignment (Section 3.1) and describe\nthe duality between language models and reward functions (Section 3.2).\n3.1\nAligning Language Models with Human Preferences\nThe alignment of language models is typically cast as a KL-constrained optimization problem [1]:\narg max\nπ\nEx∼p(x),y∼π(y|x) [r(x, y)]\n(1a)\ns.t.\nEx∼p(x) [DKL (π(y | x) ∥πref(y | x))] ≤ϵ,\n(1b)\nwhere p(x) is a distribution of prompts, y is the complete language model response, r is a preference\nreward function that encourages human-preferred responses, and DKL limits how far the optimized\nlanguage model π can deviate from the reference (untuned) model πref. There are two main categories\nof alignment algorithms: (1) search-based algorithms that optimize Eq. 1 with graph-based search\nduring inference [16, 13, 18, 19, 11, 12], and (2) learning-based algorithms that optimize Eq. 1\nthrough gradient descent, aiming for a parametrized optimal language model [1, 36, 4, 37]. Our work\nfalls in the first category, proposing a search-based algorithm capable of using small language models\nto guide the decoding of a large language model to align with human preferences.\n3.2\nDuality between Language Models and Reward Functions\nThe analytical solution to Eq. 1 can be obtained through the following Lagrangian [38, 39]:\nL(π, β) = Ex∼p(x),y∼π(y|x) [r(x, y) + β (ϵ −DKL (π(y | x) ∥πref(y | x)))] ,\n(2)\nwhich has a well-known closed-form solution that expresses a duality between the reward function\nr(x, y) and the optimal language model π∗(y | x) [40, 41]:\nr(x, y) = β log π∗(y | x)\nπref(y | x) + β log Z(x),\n(3)\nwhere Z(x) = P\ny πref(y | x) exp\n\u0010\n1\nβ r(x, y)\n\u0011\ndenotes the partition function. One takeaway from\nthis duality is that we can always express a reward function using tuned and untuned language models:\n(1) If a reward function is given [1, 2, 3], we can first obtain the optimally tuned language model\nunder this reward function with any learning-based algorithms, and then use the tuned and untuned\nmodels (π∗, πref) to reparametrize the reward function [42]; (2) If a dataset is given from which the\nreward function can be derived, we can then directly parametrize the reward function with the tuned\nand untuned language models (π∗, πref) during reward modeling [4].\n3\n\n4\nWeak-to-Strong Search\nIn this section, we introduce weak-to-strong search, a search-based algorithm that aligns a large\nlanguage model by searching over the log-probability difference between small tuned and untuned\nlanguage models. First, we discuss how using language models to parametrize the preference\nreward function (Eq. 1) makes the reward-maximization problem solvable by a simple greedy\nsearch algorithm (e.g., beam search) (Section 4.1). Then, we introduce a practical beam search\nmethod, Chunk-level Beam Search (CBS) (Section 4.2), that balances reward maximization and KL\nminimization, which is applicable to steering both white-box and black-box large language models.\n4.1\nLanguage Models as Both Reward and Value Functions\nOne practical challenge for search-based alignment algorithms is the sparsity of the preference reward\nsignal. The preference reward function r(x, y), based on the Bradley-Terry model [43], only emits\na terminal reward when the model response is complete. Search-based algorithms often struggle\nwithout any intermediate rewards or a value function providing intermediate guidance [44, 45].\nHowever, if we parameterize this sparse reward function with language models (Section 3.2), we can\nobtain both a dense reward function and a value function simultaneously.\nLanguage models as a dense reward function.\nTo obtain a dense reward function, we leverage\nthe duality between the sparse preference reward function and the dense language model probability\n(Eq. 3). By explicitly factorizing the log-probability of a complete response y under the language\nmodels, we obtain a sum-of-rewards style formulation for Eq. 3:\nr(x, y) = β\n\n\n|y|\nX\nt=1\nlog π∗(yt | x, y<t)\nπref(yt | x, y<t)\n\n+ β log Z(x),\n(4)\nwhere y<t denotes the response tokens from 1 to t −1, and the last response token y|y| is the EOS\ntoken. Combining Eq. 1 and 4, we rewrite the original objective with a per-token reward function:\narg max\nπ\nEx∼p(x),y∼π(y|x)\n\n\n|y|\nX\nt=1\nlog π∗(yt | x, y<t)\nπref(yt | x, y<t)\n\n\ns.t.\nEx∼p(x) [DKL (π(y | x) ∥πbase(y | x))] ≤ϵ,\n(5a)\n(5b)\nwhere β and Z(x) are omitted as they do not influence the optimal solution. It is important to note\nthat the reference model that parametrizes the reward function (πref) (Eq. 5a) and the reference model\nthat constrains the test-time search space (πbase) (Eq. 5b) can be different. Practically, decoupling\nthe reference models is useful as it allows using a tuned and untuned language model pair -\nnamely (π∗, πref) - to steer the decoding of any base language model πbase without retraining.\nSetting aside the KL constraint (Eq. 5b) for now, we can apply existing search algorithms like\nbeam search [45, 20] to optimize Eq. 5a. Beam search is often criticized for leading to myopic\nsolutions [45], as it tends to greedily prioritize states (x, y′) (y′ is incomplete)1 with high cumulative\nreward log π∗(y′ | x) −log πref(y′ | x) midway through generation, which is generally viewed as\npoorly correlated with the overall return we care about. While this criticism is valid for most MDPs,\nwe argue that in the token-level MDP [20] of our case, the cumulative reward mid-generation is\nactually a reliable indicator of the long-term value, making beam search less myopic.\nCumulative reward under language models as a value function [20].\nAppendix A shows that:\nlog π∗(y′ | x)\nπref(y′ | x) ∝\n\u001a−V ∗(x) + V ∗(x, y′)\nif y′ is incomplete\n−V ∗(x) + r(x, y′)\nif y′ is complete,\n(6)\nwhere V ∗(x, y′) denotes the value function, predicting the expected terminal reward under the optimal\nπ∗in the original KL-constrained sparse reward problem. Although V ∗(x, y′) is not necessarily\nachievable by the searched policy, it approximates how good the state (x, y′) is in the long run. In\nother words, continuing from the state (x, y′) of high cumulative reward log π∗(y′ | x)−log πref(y′ |\nx) is likely to generate a complete response y with high overall return log π∗(y | x) −log πref(y | x).\n1y denotes a complete response, while y′ denotes a response that can be either incomplete or complete.\n4\n\nhate the boring plot\nMy wife and I really\n0.8\nHypothesis Set\nSuccessor Chunk\nlike the fun story\n5.1\nfound it too long\n0.4\nliked the nice play\n4.7\nMy wife and I indeed\nMy wife and I indeed\n              liked the nice play\nMy wife and I really\n like the fun story\nHypothesis Set\nTop-W\nK successors per state;\nSampled i.i.d. from \nScore = \nPlease write a movie review\nPrompt\nFigure 2: Illustration of Chunk-level Beam Search with W, K = 2, 2.\n4.2\nChunk-level Beam Search (CBS)\nAfter analyzing the feasibility of optimizing Eq. 5a with greedy search algorithms (e.g., beam search),\nwe introduce a practical beam search variant that optimizes the dense reward objective (Eq. 5a) while\nensuring the KL-constraint from πbase (Eq. 5b).\nThe core algorithm providing the foundation of our method, Chunk-level Beam Search (CBS),\nis detailed in Algorithm 1 and illustrated in Figure 2. The key insight is that our beam search\noperates at the level of chunk. The search starts at the prompt and always maintains a hypothesis set\nH = {(x, y′)i}W\ni=1 of W states. For each state (x, y′) in H, CBS samples K continuation chunks\nyL of L tokens from πbase. This results in WK successor states. Among these successors, only the\ntop-W successors with the highest partial return log π∗(y′ ◦yL | x)−log πref(y′ ◦yL | x) are stored\nin H and expanded further. Finally, the terminal state (x, y) with the highest intermediate return\nlog π∗(y | x) −log πref(y | x) is selected, from which the complete response y is extracted. Notably,\nif the model to steer πbase has a different vocabulary from the tuned and unturned models (π∗, πref),\nwe should first decode the sampled tokens into natural language using πbase’s vocabulary and then\nre-encode using (π∗, πref)’s vocabulary for evaluation.\nAlgorithm 1 Chunk-level Beam Search (CBS)\n1: Input: prompt x, beam width W, successors per state K, chunk length L,\n2:\nmodel to steer πbase, tuned model π∗, and untuned model πref.\n3: Output: optimal terminal state (x, y)\n4: Initialize H = {(x, y′ = ∅)i}W\ni=1\n5: while ∃(x, y′) ∈H such that y′ is incomplete do\n6:\nInitialize C = {}\n7:\nfor each (x, y′) ∈H do\n8:\nY ←{(yL)i}K\ni=1\ni.i.d.\n∼πbase(· | x, y′)\n// yL = ∅if y′ is complete\n9:\nC ←C ∪{(x, y′ ◦yL) | yL ∈Y}\n10:\nend for\n11:\nH ←Top-W(x,y′◦yL)∈C(log π∗(y′ ◦yL | x) −log πref(y′ ◦yL | x))\n12: end while\n13: return arg max(x,y)∈H(log π∗(y | x) −log πref(y | x))\nCBS is a unified framework that encompasses several search-based algorithms: (1) CBS\nwith W = 1, K = N, L = ∞(i.e., infinite chunk length) is equivalent to BoN sampling with\nlog π∗(y | x) −log πref(y | x) as the scoring function, and (2) CBS with K = ∞, L = 1 (i.e.,\nexploring all possible next tokens from the vocabulary) is equivalent to vanilla token-level beam\n5\n\nsearch. However, we always ensure finite chunk length and limited successor exploration via sampling\nto achieve the best of both worlds: (1) Using a finite chunk length allows CBS to prune bad states\nduring generation, enhancing steerability more efficiently compared to BoN. (2) Sampling from\nπbase with limited successor exploration implicitly enforces the KL-constraint from πbase (Eq. 5b);\notherwise, integrating the KL-constraint into the objective (Eq. 5a) would be necessary for token-level\nsearch, but this can be challenging, especially when vocabularies of models differ or with black-box\nlanguage base models πbase whose log-probabilities are inaccessible.\nComputation costs. In practice, CBS samples WK continuation chunks in parallel from the frozen\nbase model πbase and prune states by calling tuned and untuned model pair (π∗, πref) every L tokens.\nLarger WK and smaller L enhance steerability at the cost of increased computations. Note that\nhigh steerability, while beneficial, is not always ideal as it may lead to large KL deviation and\nover-optimization [16].\n4.3\nApplication: Model Up-Scaling and Weak-to-Strong Generalization\nThe most practical use of CBS occurs when the tuned and untuned models, (π∗, πref), are smaller\nthan the model to steer, πbase. (1) First, this instance serves as a model up-scaling strategy, directly\ntuning a small model πref →π∗, by which the large model decoding can then be guided, to achieve\nsimilar outcomes as directly tuning the large model. (2) Second, since the small models (π∗, πref)\nare usually weaker than the large model to steer πbase, this instance also exemplifies weak-to-strong\ngeneralization [21], enhancing the strong model with only weak test-time guidance. We refer to this\ninstance of CBS as weak-to-strong search, which is the main focus of our study.\n5\nExperiments\nIn this section, we empirically evaluate weak-to-strong search’s ability to align large language\nmodels using only test-time guidance from small language models. First, in controlled-sentiment\ngeneration [22] and summarization [2], we tune gpt2 to model the desired behaviors in each task\nand then use tuned and untuned gpt2 to steer larger models of various scales (Section 5.1). Next,\nin a more difficult instruction-following benchmark, AlpacaEval 2.0 [25], instead of tunning small\nmodels, we reuse off-the-shelf open-source 7B models and their untuned versions to steer a series of\nlarge models, including open-source 70B models and a black-box model (Section 5.2).\nBaselines.\nIn addition to weak-to-strong search, we evaluate several existing test-time approaches\nthat steer a large language model πbase using small tuned and untuned language models (π∗, πref):\n(1) Base: we explore regular decoding from the frozen large language model with n-shot prompting\n(see Appendix B.1.6 for prompt details). (2) Best-of-N Sampling (BoN) [16, 17]: BoN uses\nr = log π∗(y | x) −log πref(y | x) to select the highest-scoring responses among the N independent\nresponses from the frozen large language model. Since weak-to-strong search (CBS) samples WK\nresponse chunks in parallel, for fair computational comparisons, we always ensure N = WK.\n(3) Emulated Fine-Tuning (EFT) [34, 12, 11, 35]: EFT approximates the results of directly fine-\ntuning the large language model by sampling from log πEFT(yt | x, y<t) ∝log πbase(yt | x, y<t) +\nβ−1(log π∗(yt | x, y<t)−log πref(yt | x, y<t)), where β is the hyperparameter from Eq. 2. Note that\nEFT is only applicable when all models share the same vocabulary (which is necessary for composing\noutput distributions from different models). Whenever possible, we also compare test-time methods\nagainst directly fine-tuning the large models in the same way small models are tuned.\n5.1\nControlled-Sentiment Generation & Summarization\nSetup.\nFor these two tasks, we follow the synthetic setups from [16, 46, 4], assuming access to a\ngold reward model rgold. For controlled-sentiment generation, rgold encourages positive continuations\nof movie reviews, while for summarization, it encourages high-quality summaries of Reddit posts\n(details in Appendix B.1.4). We generate synthetic preference datasets D = {(x, yw, yl)i}N\ni=1 from\nrgold with p(y1 ≻y2 | x) = σ(rgold(x, y1) −rgold(x, y2)) to mimic human feedback [47].\nTo obtain the small language models, we optimize gpt2 (124M parameters) using the standard DPO\npipeline [4]: (1) we first obtain the reference model πref through supervised fine-tuning on both\nchosen and rejected responses from the synthetic preference dataset, then (2) we apply DPO on the\n6\n\nGPT2-large\nGPT2-xl\nLlama2-7B\nLlama3-8B\n0\n1\n2\n3\n4\n5\nGold RM (rgold)\nControlled-Sentiment Generation\nGPT2-large\nGPT2-xl\nLlama2-7B\nLlama3-8B\n−2\n−1\n0\n1\n2\nSummarization\nGPT2-DPO (π∗)\nGPT2-SFT (πref)\nBase (πbase)\nWeak-to-strong search (4, 4, 5)\nBoN (16)\nEFT (β∗)\nDirectly tuned\nFigure 3: The gold reward achieved for different large pre-trained models under the gpt2\nguidance. We show the mean reward (± standard deviations) across three random seeds. EFT (β∗)\ndenotes the best EFT results among β ∈{1/4, 1/2, 1, 2, 4}; Weak-to-strong search (4, 4, 5) denotes\nCBS with W, K, L = 4, 4, 5; BoN (16) denotes BoN with N = 16.\nsynthetic preference dataset with πref as the reference policy to obtain the optimal language model\nπ∗. Note that the first stage primarily informs the language model of the desired response format,\nwith most of the tuning occurring in the second DPO stage.\nGiven the tuned and untuned (un-DPO-tuned) gpt2 pair (π∗, πref), we use them to steer the large\npre-trained language models without additional training. The large pre-trained language models\nwe study fall into two categories based on whether they share the same vocabulary as the small\nmodels: (1) same vocabulary: gpt2-large (774M), gpt2-xl (1.5B) and (2) cross vocabulary:\nLlama-2-7b, Llama-3-8B. Eventually, since we have access to the gold reward model, language\nmodel responses can be fairly evaluated on the test split of prompts using this gold reward model.\nResults.\nFigure 3 demonstrates weak-to-strong search’s great flexibility and steerability in both\ntasks. For summarization, weak-to-strong search consistently outperforms other test-time methods\nby large margins. For controlled-sentiment generation, weak-to-strong search is second only to\nEFT with a carefully selected hyperparameter (β∗= 1/4) when EFT is applicable. We hypothesize\nthat token-level adjustments from EFT are sufficient for controlled-sentiment generation, which\nprimarily requires minor stylistic changes at the token level (e.g., “hate” →“love”). However, in the\nmore complex task of summarization, where broader subsequence-level manipulations are essential,\nweak-to-strong search excels. Please refer to Appendix D for quantitative comparisons of samples\nfrom different methods. We need to mention that we do not meaningfully tune weak-to-strong search\n(CBS)’s hyperparameters to obtain the results in Figure 3 (we use a fixed set of hyperparameters of\n(4, 4, 5) for W, K, L across all models), which may underestimate the performance of our method.\nIn addition, our method enables consistent weak-to-strong generalization in the harder task of\nsummarization: most large pre-trained models (except for gpt2-large) are stronger than the tuned\ngpt2 in summarizing long text, but the weak models are still able to improve the strong models\nthrough test-time guidance, nearly matching the results of direct fine-tuning. The phenomenon of\nweak-to-strong generalization will be further studied in Section 5.2.\nChunk-level Beam Search ablations.\nWe perform additional ablations to understand how CBS\nhyperparameters (beam width W, successors per state K, and chunk length L) influence performance.\nFigure 4 displays the ablation results for W and K. With the same computation budget (i.e., WK),\nthe optimal trade-off between W and K varies by tasks: for controlled-sentiment generation, the\nbest results come from retaining the most promising state and concentrating computational efforts on\nexpanding from it (W, K = 1, 16); in contrast, for summarization, maintaining multiple hypotheses\n(W, K = 8, 2) yields the best results probably because it helps avoid local optima. Figure 5\ndisplays the ablation results for L where smaller L benefits controlled-sentiment generation, while\nan intermediate L is optimal for summarization. These results are consistent with our findings from\n7\n\nFigure 3, suggesting that the simple nature of controlled-sentiment generation makes token-level\nmanipulation sufficient and cumulative reward mid-generation a more reliable indicator of overall\nreturn. See Appendix C.1 for extended ablations on more models.\n1\n2\n4\n8\n16\nbeam width (W)\n16\n8\n4\n2\n1\nsuccessors per state (K)\n4.61\n4.31\n4.62\n3.85\n4.30\n4.52\n3.13\n3.75\n4.10\n4.36\n1.77\n2.43\n2.96\n3.35\n3.57\ngpt2-xl\n1\n2\n4\n8\n16\nbeam width (W)\n16\n8\n4\n2\n1\n4.09\n3.91\n4.12\n3.49\n3.87\n4.05\n2.81\n3.24\n3.63\n3.75\n1.86\n2.26\n2.65\n2.91\n3.12\nLlama-2-7b\n(a) controlled-sentiment generation\n1\n2\n4\n8\n16\nbeam width (W)\n16\n8\n4\n2\n1\nsuccessors per state (K)\n-0.20\n0.02\n0.22\n0.15\n0.56\n0.63\n-0.11\n0.42\n0.81\n1.01\n-0.87\n-0.44\n-0.29\n-0.22\n-0.22\ngpt2-xl\n1\n2\n4\n8\n16\nbeam width (W)\n16\n8\n4\n2\n1\n1.35\n1.49\n1.52\n1.50\n1.57\n1.61\n1.24\n1.56\n1.63\n1.65\n0.65\n0.80\n0.90\n1.02\n1.08\nLlama-2-7b\n(b) summarization\nFigure 4: W, K ablations for CBS (L = 5). We show the mean rewards across three random seeds.\nWith the same computation budget (i.e., same WK), the optimal hyperparameters differ by tasks.\n0\n10\n20\n30\n40\n50\nchunk length (L)\n3.0\n3.5\n4.0\n4.5\n5.0\nGold RM\ngpt2-xl\nLlama-2-7b\n(a) controlled-sentiment generation\n0\n10\n20\n30\n40\n50\nchunk length (L)\n−0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nGold RM\ngpt2-xl\nLlama-2-7b\n(b) summarization\nFigure 5: L ablations for CBS (W, K = 4, 4). We show the mean rewards (± standard deviations)\nacross three random seeds.\n5.2\nInstruction Following\nSetup.\nNext, we evaluate weak-to-strong search on a standard single-turn instruction-following\nbenchmark, AlpacaEval 2.0 [25], which consists of 805 prompts from various open-source datasets.\nUnlike the previous section where we steer large pre-trained language models (e.g., Llama-2-7b),\nwe now steer large instruction-tuned language models (e.g., Llama-2-7b-chat). This is because\n(1) instruction-tuned models often require further alignment to match human preferences [48], and\n(2) to study weak-to-strong generalization in instruction-following, the models must be proficient at\nfollowing instructions before steering.\nFor small language models, we reuse two high-ranking 7B model pairs from the AlpacaEval\n2.0 leaderboard as guidance: (1) Zephyr guidance: zephyr-7b-beta and its untuned version\nmistral-7b-sft-beta; (2) Tulu guidance: tulu-2-dpo-7b and its untuned version tulu-2-7b.\nAll four models use the Llama-2 tokenizer. The large instruction-tuned language models we aim to fur-\nther align fall into three categories: (1) same vocabulary: Llama-2-7b-chat, Llama-2-70b-chat;\n(2) cross vocabulary: Llama-3-8B-Instruct, Llama-3-70B-Instruct; and (3) black box:\ngpt-3.5-turbo-instruct. As it is nearly impossible to reproduce the exact training pipeline for\nthese small models (πref →π∗), we do not test the baseline results of directly fine-tuning the large\nmodels as in Figure 3. Language model responses are evaluated by their length-controlled win rates\n(LC WR) against gpt-4-turbo, with gpt-4-turbo serving as the judge.\nResults.\nExperimental results with Zephyr and Tulu guidance are shown in Figure 6 (detailed\nhyperparameters in Appendix B.2.2). Weak-to-strong search consistently outperforms other test-time\n8\n\nLlama2-7B Llama2-70B Llama3-8B Llama3-70B\nGPT3.5\n10\n15\n20\n25\n30\n35\n40\nAlpacaEval 2.0 LC Win Rate (%)\nInstruction Following (Zephyr)\nzephyr-7b-beta (π∗)\nmistral-7b-sft-beta (πref)\nLlama2-7B Llama2-70B Llama3-8B Llama3-70B\nGPT3.5\n10\n15\n20\n25\n30\n35\n40\nInstruction Following (Tulu)\ntulu-2-dpo-7b (π∗)\ntulu-2-7b (πref)\nBase (πbase)\nWeak-to-strong search\nBoN\nEFT (β∗)\nFigure 6: The length-controlled win rates against gpt-4-turbo for various instruction-tuned\nmodels under Zephyr (left) or Tulu (right) guidance. Hyperparameters are in Appendix B.2.2.\nbaselines with great margins. There are two crucial takeaways worth mentioning: (1) Weak-to-strong\nsearch makes strong models stronger with only weak test-time guidance. Take Zephyr guidance\nfor an example (Figure 6, left), even if most large instruction-tuned models πbase are stronger than\nzephyr-7b-beta before steering, weak-to-strong search is still able to enhance their performances\nusing weak models as guidance. Conversely, EFT and BoN mainly interpolate between weak and\nstrong models, resulting in limited, if any, improvements over the strong models. We also tested beam\nsearch over the strong models without external guidance [20] but we found no obvious improvements\ncompared with regular decoding (Table 2), probably because the latent reward functions behind these\nlanguage models are not well aligned with the human preference that gpt-4-turbo approximates.\nThe same observations apply to Tulu guidance, even though the tuned tulu-2-dpo-7b is weaker than\nall the large instruction-tuned language models by significant margins (Figure 6, right). (2) Weak-to-\nstrong search applies to black-box language models. Our method, requiring only sampling from\nlarge language models, is also effective for black-box models like gpt-3.5-turbo-instruct. For\nweak-to-strong search with gpt-3.5-turbo-instruct, we use a relatively long chunk length of\n100, as the black-box language model APIs are stateless and do not retain activation caches, making\nrepeated context embedding costly. Despite the long chunk length, our method still effectively\nimproves the alignment of black-box models, significantly outperforming BoN, a special case of\nweak-to-strong search (CBS) with infinite chunk length.\n6\nDiscussion\nWe have presented weak-to-strong search, an alignment method that keeps the large language model\nfrozen while steering its decoding through a test-time greedy search over small language models.\nThis method builds on the insight that the log-probability difference between small tuned and untuned\nlanguage models can serve both as a dense reward function and a value function, and then introduces\na novel beam search algorithm designed for balancing reward maximization and KL minimization.\nThis method offers a compute-efficient model up-scaling strategy that eliminates the complexity of\ndirectly fine-tuning the large models, and exemplifies weak-to-strong generalization [21] that makes\nstrong models stronger with only weak test-time guidance. Empirically, this approach is effective in\ncontrolled-sentiment generation, summarization, and instruction following.\nLimitations & Future Work.\nWhile our work focuses on aligning with human preferences, weak-\nto-strong search could also apply to tasks like reasoning [49, 50] and coding [51], where ground truth\nanswers exist. This is because any pair of tuned and untuned language models can act as test-time\nsteering forces, without necessarily being trained on preferences. This then raises several questions\nbeyond the scope of our current study: (1) In our study, we consistently use SFTed policy as the\nuntuned model πref due to the two-stage nature of preference learning; however, in single-stage\nfine-tuning tasks, does weak-to-strong search still work with a pre-trained model serving as the\nuntuned model πref? (2) Although our method shows consistent weak-to-strong generalization across\n9\n\ndiverse alignment tasks, it is also critical to probe its potential failure modes [52]. Can weak-to-strong\nsearch enhance language models in tasks where ground truth answers exist, beyond merely tailoring\ntheir knowledge and skills to human preferences? (3) Additionally, while our work mainly focuses on\nhow dense language model reward function (Eq. 4) benefits language model decoding at test time, it’s\nalso worth exploring the potential benefits of this reward parametrization for RL tuning. Although\nAppendix C.3 presents some promising preliminary results, we leave further analysis for future work.\nAcknowledgements\nThis work was supported in part by the National Key R&D Program of China (NO.2022ZD0160201).\nWe would like to thank anonymous reviewers for their valuable feedback and helpful discussions.\nAuthor Contributions\nZhanhui Zhou led the project, proposed the research idea, wrote the codebase, designed the experi-\nments, conducted most of the initial experiments, and wrote the paper. Zhixuan Liu assisted with\nrunning many of the ablation studies throughout the experiments presented in the paper. All other\nauthors provided feedback throughout the project.\n10\n\nReferences\n[1] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei,\nPaul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences.\narXiv preprint arXiv:1909.08593, 2019.\n[2] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.\nAdvances in Neural Information Processing Systems, 33:3008–3021, 2020.\n[3] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in neural information processing systems,\n35:27730–27744, 2022.\n[4] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\nAdvances in Neural Information Processing Systems, 36, 2024.\n[5] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland,\nMichal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning\nfrom human preferences. In International Conference on Artificial Intelligence and Statistics,\npages 4447–4455. PMLR, 2024.\n[6] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless\nassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,\n2022.\n[7] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[8] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n[9] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes\nBelkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. Zephyr:\nDirect distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023.\n[10] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith\nStevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. Openassistant\nconversations-democratizing large language model alignment. Advances in Neural Information\nProcessing Systems, 36, 2024.\n[11] Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D Manning. An\nemulator for fine-tuning large language models using small language models. arXiv preprint\narXiv:2310.12962, 2023.\n[12] Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A Smith.\nTuning language models by proxy. arXiv preprint arXiv:2401.08565, 2024.\n[13] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang,\nZhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, et al. Controlled decoding\nfrom language models. arXiv preprint arXiv:2310.17022, 2023.\n[14] Minbeom Kim, Hwanhee Lee, Kang Min Yoo, Joonsuk Park, Hwaran Lee, and Kyomin Jung.\nCritic-guided decoding for controlled text generation. In Findings of the Association for\nComputational Linguistics: ACL 2023, pages 4598–4612, 2023.\n[15] James Y Huang, Sailik Sengupta, Daniele Bonadiman, Yi-an Lai, Arshit Gupta, Nikolaos\nPappas, Saab Mansour, Katrin Kirchoff, and Dan Roth. Deal: Decoding-time alignment for\nlarge language models. arXiv preprint arXiv:2402.06147, 2024.\n11\n\n[16] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization.\nIn International Conference on Machine Learning, pages 10835–10866. PMLR, 2023.\n[17] Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D’Amour, Jacob Eisenstein,\nChirag Nagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment\npolicy. arXiv preprint arXiv:2401.01879, 2024.\n[18] Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli\nCelikyilmaz. Making ppo even better: Value-guided monte-carlo tree search decoding. arXiv\npreprint arXiv:2309.15028, 2023.\n[19] Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-\nlike tree-search can guide large language model decoding and training.\narXiv preprint\narXiv:2309.17179, 2023.\n[20] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From r to q∗: Your language model\nis secretly a q-function. arXiv preprint arXiv:2404.12358, 2024.\n[21] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschen-\nbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong gener-\nalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390,\n2023.\n[22] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Human Language Technologies, pages\n142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.\n[23] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners.\n[24] AI@Meta. Llama 3 model card. 2024.\n[25] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled\nalpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475,\n2024.\n[26] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–\n113, 2023.\n[27] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n[28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[29] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai:\nHarmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.\n[30] Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and\nYu Qiao. Beyond one-preference-for-all: Multi-objective direct preference optimization. arXiv\npreprint arXiv:2310.03708, 2023.\n[31] Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor,\nLaure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by\ninterpolating weights fine-tuned on diverse rewards. Advances in Neural Information Processing\nSystems, 36, 2024.\n12\n\n[32] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer,\nHannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized soups: Per-\nsonalized large language model alignment via post-hoc parameter merging. arXiv preprint\narXiv:2310.11564, 2023.\n[33] Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and\nTong Zhang. Arithmetic control of llms for diverse user preferences: Directional preference\nalignment with multi-objective rewards. arXiv preprint arXiv:2402.18571, 2024.\n[34] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A\nSmith, and Yejin Choi. Dexperts: Decoding-time controlled text generation with experts and\nanti-experts. arXiv preprint arXiv:2105.03023, 2021.\n[35] Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli Ouyang, and Yu Qiao.\nEmulated disalignment: Safety alignment for large language models may backfire!\narXiv\npreprint arXiv:2402.12343, 2024.\n[36] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-\nhf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425,\n2023.\n[37] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto:\nModel alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.\n[38] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:\nSimple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.\n[39] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online\nreinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.\n[40] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy\ninverse reinforcement learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008.\n[41] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and\nreview. arXiv preprint arXiv:1805.00909, 2018.\n[42] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi\nChandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating\nreward models for language modeling. arXiv preprint arXiv:2403.13787, 2024.\n[43] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the\nmethod of paired comparisons. Biometrika, 39(3/4):324–345, 1952.\n[44] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-\ntering the game of go with deep neural networks and tree search. nature, 529(7587):484–489,\n2016.\n[45] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big\nsequence modeling problem. Advances in neural information processing systems, 34:1273–\n1286, 2021.\n[46] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint\narXiv:2305.20050, 2023.\n[47] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the\nmethod of paired comparisons. Biometrika, 39(3/4):324–345, 1952.\n[48] Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai,\nand Yaodong Yang. Aligner: Achieving efficient alignment through weak-to-strong correction.\narXiv preprint arXiv:2402.02416, 2024.\n[49] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with\nreasoning. Advances in Neural Information Processing Systems, 35:15476–15488, 2022.\n13\n\n[50] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos\nGuestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for\nmethods that learn from human feedback. Advances in Neural Information Processing Systems,\n36, 2024.\n[51] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\nYossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models\nfor code. arXiv preprint arXiv:2308.12950, 2023.\n[52] Wenkai Yang, Shiqi Shen, Guangyao Shen, Zhi Gong, and Yankai Lin. Super (ficial)-alignment:\nStrong models may deceive weak models in weak-to-strong generalization. arXiv preprint\narXiv:2406.11431, 2024.\n[53] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis. In Dekang Lin, Yuji Matsumoto, and Rada\nMihalcea, editors, Proceedings of the 49th Annual Meeting of the Association for Computational\nLinguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA, June\n2011. Association for Computational Linguistics.\n[54] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep\nDasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. Camels in a changing\nclimate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702, 2023.\n[55] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan\nLiu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback.\narXiv preprint arXiv:2310.01377, 2023.\n[56] Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving\nllm helpfulness & harmlessness with rlaif, November 2023.\n[57] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n14\n\nA\nMathematical Derivations for Eq. 6\nAs introduced in Section 3, the alignment objective under the traditional contextual bandit framing is\ngiven by (Eq. 2):\narg max\nπ\nEx∼p(x),y∼π(y|x) [r(x, y) −βDKL (π(y | x) ∥πref(y | x))] ,\n(7)\nwhere p(x) is a distribution of prompts, y is the complete language model response, r is a reward\nfunction that encourages human-preferred responses, and DKL limits how far the optimized language\nmodel π can deviate from the reference model πref.\nGiven the autoregressive nature of language models, we can frame language model alignment as\nsolving a token-level MDP [20]. This token-level MDP is define by the tuple (S, A, f, r(st, at)).\nHere, the state st := (x, y<t) ∈S consists of the prompt and all response tokens generated so\nfar; the action a := yt determines the next token to generate from the vocabulary A; the dynamics\nf is a deterministic function that updates the state by concatenating the current state and action\nst+1 := (st, at); r(st, at) is a sparse reward that equals r(x, y) if at is EOS and 0 otherwise. We use\nρπ(st) to denote the state marginals of the trajectory distribution induced by the policy π. Under the\ntoken-level MDP, the objective from Eq. 7 can be written as\nmax\nπ\nT\nX\nt=1\nEst∼ρπ(st),at∼π(at|st)\n\nr(st, at) + β log πref(at | st) + βH(π(at | st))\n|\n{z\n}\n−βDKL[π(at|st) ∥πref(at|st)]\n\n,\n(8)\nwhere T specifies the response length. The solution of Eq. 8 is given by [40] as:\nπ∗(at | st) = exp\n\u0012 1\nβ (Q∗(st, at) −V ∗(st))\n\u0013\n,\n(9)\nwhere the optimal Q-function and V-function satisfies\nQ∗(st, at) = r(st, at) + β log πref(at | st) + V ∗(st+1),\n(10)\nV ∗(st) =\nT\nX\ni=t\nEsi∼ρπ∗(si|st),ai∼π∗(ai|si) [r(si, ai) + β log πref(ai | si) + βH(π∗(ai | si))] . (11)\nHere, V ∗predicts the expected future return (the terminal reward in this sparse reward setting)\npenalized with the future KL constraint starting from the state st, under the optimal language model\nπ∗. Combining Eq. 9 and Eq. 10, we have\nβ log π∗(at | st)\nπref(at | st) = r(st, at) + V ∗(st+1) −V ∗(st).\n(12)\nNote that (1) r(st, at) is a sparse reward that is non-zero if at is EOS, and (2) V ∗(st+1) = 0 if at is\nEOS. Then, summing Eq. 12 from timestep 1 to H yield\nH\nX\nt=1\nβ log π∗(at | st)\nπref(at | st) =\n\u001a−V ∗(s1) + V ∗((sH, aH))\nif aH is not EOS\n−V ∗(s1) + r(sH, aH)\nif aH is EOS,\n(13)\nwhere V ∗(sH+1) = V ∗((sH, aH)) due to the deterministic transition. Now, returning back to the\nsequence-level MDP (Section 4), where we define y as a complete response, y′ as a response that\ncan be either complete or incomplete, we have that s1 = (x, y<1) = (x, ∅) = x and (sH, aH) =\n((x, y<H), yH) = (x, y<H+1). Thus, we can rewrite Eq. 13, with a slight abuse of notations, as\nlog π∗(y′ | x)\nπref(y′ | x) ∝\n(\n−V ∗(x) + V ∗(x, y′)\nif y′\n|y′| ̸= EOS (y′ is incomplete)\n−V ∗(x) + r(x, y′)\nif y′\n|y′| = EOS (y′ is complete).\n(14)\n15\n\nB\nFurther Details on the Experimental Setup\nB.1\nControlled-Sentiment Generation & Summarization\nB.1.1\nModel Specification\nThe following table lists the models and their corresponding links.\nModels\nLinks\ngpt2 (124M) [23]\nhttps://huggingface.co/openai-community/gpt2\ngpt2-large (774M) [23]\nhttps://huggingface.co/openai-community/gpt2-large\ngpt2-xl (1.5B) [23]\nhttps://huggingface.co/openai-community/gpt2-xl\nLlama-2-7b [7]\nhttps://huggingface.co/meta-llama/Llama-2-7b-hf\nLlama-3-8B [24]\nhttps://huggingface.co/meta-llama/Meta-Llama-3-8B\nB.1.2\nHyperparameters Specification\nWe use fixed hyperparameters across all tested models. We use temperature T = 0.7, top-k = 50\nand top-p = 1.0 when sampling from the language models. For weak-to-strong search (CBS), we\nuse W, K, L = 4, 4, 5 (W: beam width, K: successors per state, L: chunk length). For BoN, we use\nN = 16 for fair computational comparison with weak-to-strong search (i.e., WK = N). For EFT,\nwe report the best results among β ∈{1/4, 1/2, 1, 2, 4}.\nB.1.3\nCompute Resources Specification\nModels are evaluated over 1000 test prompts, on one single NVIDIA A100 GPU.\nB.1.4\nGold Reward Model Details\nWe follow the synthetic setup in which we use the gold reward models to play the roles of humans\nand provide binary preference labels [16, 46, 4].\nFor controlled-sentiment generation, we reuse the publicly available distilbert-imdb to define\nthe gold reward model rgold. Distilbert-imdb is a fine-tuned classifier p on the imdb dataset [53]\nto classify movie review sentiments. We define the gold reward rgold as log p(positive | x, y) −\nlog p(negative | x, y) to encourage positive review. Synthetic preferences are collected using the\ntruncated movie reviews as prompts x, and pairwise completions from gpt2-imdb, ranked with\np(y1 ≻y2 | x) = σ(rgold(x, y1) −rgold(x, y2)), as preferences.\nFor summarization, we fit a reward model on the summarize_from_feedback dataset [2] as the\ngold reward model rgold. Specifically, this reward model is fine-tuned from Llama-2-7b with a linear\nprojection head and binary cross entropy loss, using a batch size of 32, a learning rate of 1e-5 for the\nprojection head, and 5e-6 for other parameters, over one epoch with a cosine learning rate schedule.\nSynthetic preferences are generated by relabeling pairwise responses in the original dataset with\np(y1 ≻y2 | x) = σ(rgold(x, y1) −rgold(x, y2)).\nBoth gold reward models show high validation accuracies, 0.928 and 0.736, demonstrating strong\ncorrelation with human judgments.\nB.1.5\nDirect Tuning Details\nDirect tuning on the synthetic preferences D = {(x, yw, yl)i}N\ni=1 involves two stages: Supervised\nFine-Tuning (SFT) and Direct Preference Optimization (DPO) [4]. During SFT, models are trained on\nboth selected and rejected responses using a batch size of 64, a learning rate of 2e-5, and a cosine\nlearning rate schedule over one epoch. During DPO, we use a β = 0.1, batch size of 256, a learning\nrate of 1e-6, and a cosine learning rate schedule over one epoch.\nB.1.6\nPrompt Template for Sampling from Base Models\nWhen sampling from large pre-trained models, it is crucial to provide clear task-specific instructions.\nFor sentiment-controlled generation, we use a zero-shot prompt:\n16\n\nHere is a movie review from imdb: {prompt}\nFor summarization, we use a two-shot prompt (the exemplars are selected arbitrarily):\n{examplar[1].prompt}TL;DR: {examplar[1].response}\n{examplar[2].prompt}TL;DR: {examplar[2].response}\n{prompt}TL;DR:\nB.2\nInstruction Following\nB.2.1\nModel Specification\nThe following table lists the models and their corresponding links.\nModels\nLinks\nzephyr-7b-beta [9]\nhttps://huggingface.co/HuggingFaceH4/zephyr-7b-beta\nmistral-7b-sft-beta [9]\nhttps://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta\ntulu-2-dpo-7b [54]\nhttps://huggingface.co/allenai/tulu-2-dpo-7b\ntulu-2-7b [54]\nhttps://huggingface.co/allenai/tulu-2-7b\nLlama-2-7b-chat [7]\nhttps://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nLlama-2-70b-chat [7]\nhttps://huggingface.co/meta-llama/Llama-2-70b-chat-hf\nLlama-3-8B-Instruct [24]\nhttps://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\nLlama-3-70B-Instruct [24]\nhttps://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\ngpt-3.5-turbo-instruct [3]\nhttps://platform.openai.com/docs/models/gpt-3-5-turbo\nB.2.2\nHyperparameters Specification\nWhen sampling from Llama-3-8B-Instruct and Llama-3-70B-Instruct, we use the T = 0.6,\ntop-k = 1.0 and top-p = 0.9 as per the official Llama-3 generation configuration. For other models,\nwe default to temperature T = 0.7, top-k = 50 and top-p = 1.0. Specific hyperparameters for each\nmethod are detailed in Tables 3 and 4.\nB.2.3\nCompute Resources Specification\nModels are evaluated on 805 test prompts. Model inference takes place on one single NVIDIA A100\nGPU for 7B&8B and black-box models, and on four NVIDIA A100 GPUs for 70B models.\nC\nExtended Experimental Results\nC.1\nChunk-level Beam Search Ablations\nWe show the extended CBS hyperparameters (W, K, L) ablations in Figures 8 and 7.\n0\n10\n20\n30\n40\n50\nchunk length (L)\n3.0\n3.5\n4.0\n4.5\n5.0\nGold RM\ngpt2-large\ngpt2-xl\nLlama-2-7b\nLlama-3-7b\n(a) controlled-sentiment generation\n0\n10\n20\n30\n40\n50\nchunk length (L)\n−1.5\n−1.0\n−0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nGold RM\ngpt2-large\ngpt2-xl\nLlama-2-7b\nLlama-3-7b\n(b) summarization\nFigure 7: L ablations for CBS (W, K = 4, 4). We show the mean rewards (± standard deviations).\n17\n\n1\n2\n4\n8\n16\nbeam width (W)\n16\n8\n4\n2\n1\nsuccessors per state (K)\n4.89\n4.64\n4.87\n4.29\n4.60\n4.84\n3.55\n4.19\n4.45\n4.63\n2.13\n2.89\n3.46\n3.76\n4.04\ngpt2-large\n1\n2\n4\n8\n16\nbeam width (W)\n16\n8\n4\n2\n1\n4.61\n4.31\n4.62\n3.85\n4.30\n4.52\n3.13\n3.75\n4.10\n4.36\n1.77\n2.43\n2.96\n3.35\n3.57\ngpt2-xl\n1\n2\n4\n8\n16\nbeam width (W)\n16\n8\n4\n2\n1\n4.09\n3.91\n4.12\n3.49\n3.87\n4.05\n2.81\n3.24\n3.63\n3.75\n1.86\n2.26\n2.65\n2.91\n3.12\nLlama-2-7b\n1\n2\n4\n8\n16\nbeam width (W)\n16\n8\n4\n2\n1\n4.20\n4.05\n4.25\n3.76\n4.04\n4.20\n3.04\n3.70\n3.93\n4.02\n1.92\n2.49\n2.89\n3.10\n3.16\nLlama-3-8B\n(a) controlled-sentiment generation\n1\n2\n4\n8\n16\nbeam width (W)\n16\n8\n4\n2\n1\nsuccessors per state (K)\n-0.73\n-0.52\n-0.15\n-0.58\n0.03\n0.27\n-0.90\n-0.21\n0.45\n0.73\n-1.70\n-1.30\n-0.99\n-0.87\n-0.76\ngpt2-large\n1\n2\n4\n8\n16\nbeam width (W)\n16\n8\n4\n2\n1\n-0.20\n0.02\n0.22\n0.15\n0.56\n0.63\n-0.11\n0.42\n0.81\n1.01\n-0.87\n-0.44\n-0.29\n-0.22\n-0.22\ngpt2-xl\n1\n2\n4\n8\n16\nbeam width (W)\n16\n8\n4\n2\n1\n1.35\n1.49\n1.52\n1.50\n1.57\n1.61\n1.24\n1.56\n1.63\n1.65\n0.65\n0.80\n0.90\n1.02\n1.08\nLlama-2-7b\n1\n2\n4\n8\n16\nbeam width (W)\n16\n8\n4\n2\n1\n1.68\n1.95\n1.93\n1.95\n2.08\n2.01\n1.73\n2.00\n2.06\n1.98\n1.14\n1.18\n1.30\n1.39\n1.46\nLlama-3-8B\n(b) summarization\nFigure 8: W, K ablations for CBS (L = 5). We show the mean rewards across three random seeds.\nWith the same computation budget (i.e., same WK), the optimal hyperparameters differ by tasks.\nC.2\nEvaluation Results for Instruction Following\nIn addition to gpt-4-turbo evaluations, we assess language model responses using two top-ranking\nreward models from RewardBench [42]: UltraRM-13b [55] and Starling-RM-34B [56]. Table 3\nand 4 show that weak-to-strong search consistently outperform other methods across all metrics. We\nalso test vanilla beam search without any external guidance [20], which does not consistently improve\nover direct sampling for instruction following (Table 2).\nModels\nLC WR (%)\nWR (%)\nURM (↑)\nSRM (↑)\nLlama-2-7b-chat\n10.08\n10.30\n1.183\n-5.849\nw/ beam search (16)\n10.23\n10.07\n1.141\n−5.935\nLlama-2-70b-chat\n16.18\n14.98\n1.902\n-5.641\nw/ beam search (4)\n16.01\n14.54\n1.873\n−5.655\nLlama-3-8B-Instruct\n22.92\n22.57\n2.682\n-5.156\nw/ beam search (16)\n21.93\n21.66\n2.432\n−5.655\nLlama-3-70B-Instruct\n34.42\n32.18\n3.833\n-4.674\nw/ beam search (4)\n34.91\n35.07\n3.678\n−4.754\nTable 2: Vanilla beam search [20], without external guidance, shows limited improvements over\nregular decoding. ‘w/ beam search (16)’ denotes beam search with a beam width of 16. LC WR\nand WR denote length-controlled and raw win rates against gpt-4-turbo; URM and SRM denote\nscores by UltraRM-13b [55] and Starling-RM-34B [56].\n18\n\nModels\nLC WR (%)\nWR (%)\nURM (↑)\nSRM (↑)\nweak supervision\nzephyr-7b-beta (π∗)\n13.20\n11.00\n1.138\n−6.143\nmistral-7b-sft-beta (πref)\n7.54\n4.77\n−1.274\n−7.618\nsame vocabulary\nLlama-2-7b-chat\nBase (πbase)\n10.08\n10.30\n1.183\n−5.849\nEFT (β∗= 0.25)\n9.05\n10.08\n1.244\n−5.819\nBoN (16)\n13.11\n13.23\n1.650\n−5.738\nWeak-to-strong search (4, 4, 30)\n13.65\n14.11\n2.234\n-5.424\nLlama-2-70b-chat\nBase (πbase)\n16.18\n14.98\n1.902\n−5.641\nEFT (β∗= 0.25)\n15.54\n14.45\n1.905\n−5.581\nBoN (4)\n17.57\n16.91\n2.061\n−5.576\nWeak-to-strong search (2, 2, 30)\n19.10\n18.14\n2.290\n-5.425\ncross vocabulary\nLlama-3-8B-Instruct\nBase (πbase)\n22.92\n22.57\n2.682\n−5.156\nEFT (β∗)\nNA\nNA\nNA\nNA\nBoN (16)\n25.35\n24.32\n3.000\n−5.070\nWeak-to-strong search (4, 4, 30)\n27.17\n27.43\n3.407\n-4.862\nLlama-3-70B-Instruct\nBase (πbase)\n34.42\n32.18\n3.833\n−4.674\nEFT (β∗)\nNA\nNA\nNA\nNA\nBoN (4)\n36.60\n36.38\n3.869\n−4.676\nWeak-to-strong search (2, 2, 30)\n37.92\n38.43\n4.019\n-4.616\nblack box\ngpt-3.5-turbo-instruct\nBase (πbase)\n16.00\n10.58\n0.771\n−6.556\nEFT (β∗)\nNA\nNA\nNA\nNA\nBoN (4)\n19.59\n12.51\n1.017\n−6.455\nWeak-to-strong search (2, 2, 100)\n20.07\n12.61\n1.212\n-6.391\nTable 3: Instruction following performance under the Zephyr guidance. EFT (β∗) denotes the\nbest EFT results among β ∈{0.25, 0.5, 1, 1.5}; Weak-to-strong search (2, 2, 30) denotes CBS with\nW = 2, K = 2, L = 30. LC WR and WR denote length-controlled and raw win rates against\ngpt-4-turbo; URM and SRM denote scores by UltraRM-13b [55] and Starling-RM-34B [56].\n19\n\nModels\nLC WR (%)\nWR (%)\nURM (↑)\nSRM (↑)\nweak supervision\ntulu-2-dpo-7b (π∗)\n9.46\n8.10\n0.743\n−6.310\ntulu-2-7b (πref)\n9.03\n5.38\n−1.070\n−7.362\nsame vocabulary\nLlama-2-7b-chat\nBase (πbase)\n10.08\n10.30\n1.183\n−5.849\nEFT (β∗= 1)\n10.07\n11.63\n1.924\n−5.535\nBoN (16)\n11.60\n11.67\n1.536\n−5.721\nWeak-to-strong search (4, 4, 30)\n13.16\n14.20\n2.115\n-5.451\nLlama-2-70b-chat\nBase (πbase)\n16.18\n14.98\n1.902\n−5.641\nEFT (β∗= 1)\n16.58\n16.85\n2.370\n-5.381\nBoN (4)\n16.73\n15.99\n2.145\n−5.515\nWeak-to-strong search (2, 2, 30)\n19.04\n18.15\n2.300\n-5.438\ncross vocabulary\nLlama-3-8B-Instruct\nBase (πbase)\n22.92\n22.57\n2.682\n−5.156\nEFT (β∗)\nNA\nNA\nNA\nNA\nBoN (16)\n22.42\n22.54\n3.039\n−5.020\nWeak-to-strong search (4, 4, 30)\n25.96\n26.73\n3.431\n-4.859\nLlama-3-70B-Instruct\nBase (πbase)\n34.42\n32.18\n3.833\n−4.674\nEFT (β∗)\nNA\nNA\nNA\nNA\nBoN (4)\n35.96\n36.43\n3.876\n−4.668\nWeak-to-strong search (2, 2, 30)\n39.09\n39.81\n4.068\n-4.583\nblack box\ngpt-3.5-turbo-instruct\nBase (πbase)\n16.00\n10.58\n0.771\n−6.556\nEFT (β∗)\nNA\nNA\nNA\nNA\nBoN (4)\n18.60\n13.15\n1.202\n−6.327\nWeak-to-strong search (2, 2, 100)\n19.80\n13.23\n1.285\n-6.295\nTable 4: Instruction following performance under the Tulu guidance. EFT (β∗) denotes the\nbest EFT results among β ∈{0.25, 0.5, 1, 1.5}; Weak-to-strong search (2, 2, 30) denotes CBS with\nW = 2, K = 2, L = 30. LC WR and WR denote length-controlled and raw win rates against\ngpt-4-turbo; URM and SRM denote scores by UltraRM-13b [55] and Starling-RM-34B [56].\n20\n\nC.3\nRL Fine-tuning with Language Model Reward\nIn Section 5, we have demonstrated that by converting weak language models to a per-token dense\nreward function (Eq. 4), we can further improve strong models at test time without additional training.\nThis section presents preliminary experiments showing that this reparametrized dense reward function\ncan also benefit RL fine-tuning. Using the same setup from Section 5, we start with a dense reward\nfunction parametrized by tuned and unturned gpt2 pair (π∗, πref) and then we tune two larger models\nunder this dense reward on sentiment-controlled generation using PPO [57]. Figure 9 shows the\nfine-tuning results with different reward sparsity, where we find that dense reward function does\nbenefit RL fine-tuning, and weak-to-strong generalization [21] also occurs consistently during training\nwhen the reward signal comes from weaker language models (note that π∗only achieve a 4.6 gold\nreward on sentiment-controlled generation from Figure 3).\n(a) PPO fine-tuning results for gpt2-large.\n(b) PPO fine-tuning results for Llama-2-7b w/ LoRA (r = 128, α = 128).\nFigure 9: PPO fine-tuning with chunk-level dense rewards for controlled-sentiment generation.\nUsing dense reward parameterized by tuned and untuned gpt2 models (see Section 5 for details), we\ntrain larger base models with PPO. The chunk length L controls the reward sparsity. For example,\nL = 5 means rewards are accumulated and emitted every 5 tokens (delayed to the last token of\neach chunk) while L = inf corresponds to vanilla PPO with sequence-level sparse rewards. Denser\nrewards facilitate credit assignment and accelerate training, improving both the achieved return\non training prompts (cumulative dense rewards over the complete responses) (left) and the\nachieved gold reward on validation prompts (right). We plot mean ± std over three random seeds.\n21\n\nD\nSample Generations\nD.1\nControlled-Sentiment Generation Sample Generations\nPrompt\nWhile I don’t consider myself\nweak supervision\nGPT2-DPO\na fan of the ¨real¨horror genre, I enjoyed this movie a lot. If you have seen horror movies, I would\nrecommend it for those interested in the genre. However, I do recommend that you take a look\n[truncated for brevity] 4.54\nGPT2-SFT\na movie fan, I did search for the trailer for this movie and couldn’t find it. I was wondering if\nanyone had seen it. I might have to watch the movie, but I’m not sure if I want to. I’m [truncated\nfor brevity] −3.26\ngpt2-large\nBase\na classic horror fan, reading this book has made me feel rather old. I’m not sure if I’m a fan\nof Lovecraft’s work or not, but this book has made me want to read all his stories. I would\nrecommend this book to [truncated for brevity] 1.96\nEFT (β∗= 1/4)\na great comedy fan, I do enjoy a good comedy. I have seen many films over the years and have\nfound that a lot of the best comedies have been comedies that I enjoyed as a child. I think that this\nis one of those [truncated for brevity] 4.85\nBoN (16)\na classic horror fan, reading this book has made me feel rather old. I’m not sure if I’m a fan\nof Lovecraft’s work or not, but this book has made me want to read all his stories. I would\nrecommend this book to [truncated for brevity] 5.03\nWeak-to-strong search (4, 4, 5)\na huge fan of the series, I do like to see it on a regular basis. It is a great example of a show that\nhas its own unique style and style of storytelling that works. It is a show that is very much a work\nin [truncated for brevity] 5.19\nDirect Tuning\na huge fan of the genre, I still find this film a very enjoyable movie. 5.40\ngpt2-xl\nBase\na huge Mayweather fan, I do like the movie. I like Mayweather, I like movies with Mayweather,\nand I like boxers. It’s definitely worth checking out. This movie is not that good. It’s not that\ngreat. It [truncated for brevity] 0.49\nEFT (β∗= 1/4)\na great comedy fan, I do enjoy a good comedy every now and then. I loved watching this movie\nbecause it was funny. The humor, the characters, the story, all of it was very funny. It is a very\nfunny film and it [truncated for brevity] 5.29\nBoN (16)\na fan of the original Star Trek, I do often enjoy the movies. I have seen them all. I have seen all\nthe movies, and while I have enjoyed them, I have not been able to find a single one that I would\nrecommend to [truncated for brevity] 3.75\nWeak-to-strong search (4, 4, 5)\n22\n\na huge fan of the Godzilla series, I do enjoy watching all of the films in the franchise, and I\nenjoyed watching this one with a good friend. The film has a very good story, with some great\naction scenes. The movie was very well [truncated for brevity] 5.16\nDirect Tuning\nto be “old” at this point in my life, I still love this movie. It’s a very entertaining film. 5.52\nLlama-2-7b\nBase\nto be a fan of the James Bond series, I do like the movie series. The most recent is a film with\na highly complex plot, but it is not the best of the series, and it’s not a good idea to watch it\n[truncated for brevity] 2.04\nEFT (β∗)\nNA\nBoN (16)\na fan of the franchise, I have seen most of the movies in this series, so I could appreciate this one\nto a certain extent. However, I’m not sure if it is a good movie. The characters are not as well\n[truncated for brevity] −1.65\nWeak-to-strong search (4, 4, 5)\na fan of the original trilogy, I do think this is a fantastic remake. It’s essentially the same story, yet\nit’s different enough to be enjoyable. There’s also a lot of nods to the [truncated for brevity] 4.89\nDirect Tuning\na huge fan of ¨romantic comedies,Ï do consider this film to be a delightful movie. While I can see\nhow people might not like this film, I can also see why people would like it. I think it’s [truncated\nfor brevity] 4.73\nLlama-3-7B\nBase\nan intellectual, I do consider myself a fan of cerebral movies, and this is easily the most intelligent\nmovie I’ve ever seen. I’m not sure if I’d call this film uplifting, but it is without a doubt inspiring\nand thought-provoking [truncated for brevity] 5.16\nEFT (β∗)\nNA\nBoN (16)\na huge fan of either comic book or of super heroes, I have been very impressed by the overall\nquality of the Marvel’s cinematic universe. This movie is a great addition to the series. The story\nis complex and multi-layered, with the characters [truncated for brevity] 5.33\nWeak-to-strong search (4, 4, 5)\na horror fan, I do love a good story which is why I thoroughly enjoyed this movie. The story is\ntold well, very well. The acting is great, and the special effects are great too. I thought the story\nwas very well done. [truncated for brevity] 5.41\nDirect Tuning\na ¨horror¨fan, I enjoy a very wide variety of genres. However, it is my belief that few genre films\nare ever truly ¨horrorör ¨cinematic¨films; they are all variations on a theme. I think [truncated for\nbrevity] 3.11\n23\n\nD.2\nSummarization Sample Generations\nPrompt\nSUBREDDIT: rrelationships\nTITLE: Me [23 F] with my boyfriend [30 M] of 9 months, hurt and I don’t know what to do.\nPOST: Sorry for the long post. I’m really hurting right now and could use some advice or wise\nwords.\nTo give a brief background, my boyfriend and I have been dating for about 9 months. He’s a\nphysician in an intense fellowship program and generally very stressedtiredbusy, I’m currently in\nschool, and stay pretty busy with an internship and working. We spend almost everyday together,\nsupport each other, have a lot of fun and both refer to the other as our best friend (that we enjoy\ntouching inappropriately.)\nFor the first couple months we were very happy together. The issues didn’t start until I brought\nup the dreaded question, ¨So what are we?¨. Then began a bit of a struggle. I always wanted more\nfrom the relationship and was greeted by hesitancy, by 3 months I asked for exclusivity and he\neventually agreed but only because I was going to leave if he didn’t (I wanted something serious.)\nby 6-7 months I said I love you, he didn’t say it back. This is totally fine, painful, but fine. I said\nI’d be patient with him until he is ready. He still hasn’t said it back, but now at 9 months he said\nthat he feels like he is falling in love, but isn’t in love yet. This progress has given me hope, but\nnot being on the same page has made me feel a bit insecure about our relationship.\nHe has mentioned that he doesn’t want to fall in love unless he knows that we will have a serious\nfuture together, so he doesn’t get hurt like he did in his 2 prior relationships. :(\nWe have been fighting a lot recently because: I still haven’t met his family (they live 5 minutes\nfrom him), he doesn’t want to share our relationship on Facebook (not the biggest deal!), he\nisn’t in love with me, we don’t really talk about the future, and the quality of time spent together\nisn’t the best due to the nature of his work and how stressed and tired he is all of the time. After\nexpressing my concerns about these things, he said he doesn’t want to lose me and will try to open\nup to me. The fighting is really getting to us both.\nweak supervision\nGPT2-DPO\nBoyfriend of 9 months insists on exclusivity, refuses to talk about future but I don’t know how\nto approach this. What are my options for dealing with the pressure he is feeling now and what\nshould I do? −1.05\nGPT2-SFT\nI don’t know what I should do, and I’m feeling insecure and don’t know what to do. −2.67\ngpt2-large\nBase\nI love him and he doesn’t want to be in a relationship unless he knows we will be together. He has\nbeen fighting recently because I’m in love with him, I’m not in love with him, he wants to be in a\nrelationship with someone other than me, I’m not in love with him, I’m not in love with him, I’m\nnot in love with him because I haven’t met his family, he doesn’t want to share our relationship on\nFacebook, I’m not in love with him, I’m not in love with him because he doesn’t want to share\nour relationship on Facebook, I’m not in [truncated for brevity] −3.56\nEFT (β∗= 2)\nGirlfriend has been having fights with boyfriend over not sharing his relationship on Facebook\nand they are not in love. Girlfriend has expressed her fears about their relationship and is willing\nto commit time to them. He is trying to open up to her because he is getting his family to open\nup to him. Girlfriend is hurting. I want to give him the benefit of the doubt and believe that he\ndoes genuinely want to be with me and will be open to us, but I am concerned that if he doesn’t,\nthen he won’t be able to open up to me in a healthy way. I believe this to be true because he said\n[truncated for brevity] −2.02\n24\n\nBoN (16)\nHe doesn’t want to share our relationship on Facebook, he doesn’t want to get me hurt, and the\nquality of time we spend together isn’t the best due to our work and stress. −1.03\nWeak-to-strong search (4, 4, 5)\nWe have been fighting because we have not had a serious relationship that we can feel happy and\nin love with yet. I want more from the relationship and am very insecure about our relationship.\nHe is trying to open up to me but I don’t really do well with that approach. I want more from our\nrelationship and I am feeling pretty insecure about it. 0.49\nDirect Tuning\nI feel like I have been hurt by my boyfriends lack of interest in my feelings for him. I don’t know\nhow to proceed with our relationship and I’m not sure if he is ready for a serious relationship.\n0.33\ngpt2-xl\nBase\nI still haven’t met his family (they live 5 minutes from him), he doesn’t want to share our\nrelationship on Facebook (not the biggest deal!), he isn’t in love with me, he doesn’t really talk\nabout the future, and the quality of time spent together isn’t the best due to the nature of his work\nand how stressed and tired he is all of the time. After expressing my concerns about these things,\nhe said he doesn’t want to lose me and will try to open up to me. The fighting is really getting to\nus both. 1.77\nEFT (β∗= 1)\nBoyfriend has been talking about his love life for the better half of 9 months and has mentioned\nwanting exclusivity within the next 6 months. We are fighting a lot because I still haven’t met his\nfamily (they are 5 minutes from him), he doesn’t want to list our relationship on Facebook (not\nthe biggest deal), he isn’t in love with me, we don’t really talk about the future, and the quality of\ntime spent together isn’t the best due to the nature of his work and how stressed and tired he is all\nof the time. 1.61\nBoN (16)\nBoyfriend has been with me for 9 months, we have been fighting about a lot of things, he doesn’t\nwant to share our relationship on Facebook, he is not in love with me, we don’t really talk about\nthe future, and the quality of our time together isn’t the best. He has said he doesn’t want to lose\nme and will try to open up to me. 2.51\nWeak-to-strong search (4, 4, 5)\nBoyfriend is a very talented (and really stressedtired) physician. He has mentioned that he doesn’t\nwant to fall in love unless he is 100% sure we can have a long term relationship. He’s said he feels\nlike he is falling in love, but isn’t in love yet. We are fighting a lot because we don’t talk about the\nfuture and he doesn’t want to share our relationship on social media. 3.04\nDirect Tuning\nMy boyfriend and I have been together for 9 months and I’ve mentioned I want more from the\nrelationship, he said he’s falling in love but isn’t in love yet, and now he won’t address my\nconcerns. 1.85\nLlama-2-7b\nBase\nHave been dating for 9 months, boyfriend is a physician in an intense fellowship program, we are\nboth in school. He’s hesitant about saying I love you, and I’m getting hurt. 1.98\nEFT (β∗)\nNA\nBoN (16)\nBoyfriend and I have been dating for 9 months, we are still not sure if we are together or not. I\nlove him but he is worried about falling in love and getting his heart broken. We fight a lot and I\nam hurting :( 2.03\n25\n\nWeak-to-strong search (4, 4, 5)\nBeen dating 9 months, he’s stressed, tired, and busy. I’m managing a bit of an internship and\nworking. He doesn’t want to fall in love if it’s not serious. We fight a lot lately and he’s having\ntrouble opening up. Don’t know if we have a future together, but I want it, can’t imagine my life\nwithout him. I love him dearly. I’m really hurt and I don’t know how to move forward. 2.80\nDirect Tuning\nMy boyfriend and I have been together for 9 months and I’ve mentioned I want more from\nthe relationship, he 9 month relationship with bf, he isn’t completely in love with me, hasn’t\nintroduced me to his family, we don’t talk about the future, and now we are fighting a lot. 2.59\nLlama-3-7B\nBase\nI want a serious relationship with my boyfriend. I don’t know how to move forward with him. He\nisn’t in love and doesn’t know if he ever will be. I don’t want to lose him, but I’m not sure how to\nmake this work. 2.27\nEFT (β∗)\nNA\nBoN (16)\nBoyfriend and I have been dating 9 months. I want more from him than he is willing to give. We\nfight a lot. He says he is falling in love but isn’t there yet. I don’t know what to do about it. 2.67\nWeak-to-strong search (4, 4, 5)\nMe, 23 F, with my boyfriend, 30 M, for 9 months. Boyfriend hasn’t said love yet, I want exclusivity\nand to meet his family. We have been fighting a lot since I brought these issues up. I just don’t\nknow what to do anymore. 2.88\nDirect Tuning\n9 month relationship, boyfriend doesn’t say he loves me, doesn’t want to post us on FB, doesn’t\nwant to meet my parents, doesn’t want to plan for the future (not that I have any expectations for\nhim), and doesn’t spend a ton of quality time with me due to work stresses. I am getting insecure\nand we have been fighting a lot. I love him and don’t want to lose him. What can I do? I’m really\nhoping for someone to give me some insight into the male psyche here and tell me the best things\nI can do to help this relationship progress and be as happy as possible 2.61\n26",
    "pdf_filename": "Weak-to-Strong_Search_Align_Large_Language_Models_via_Searching_over_Small_Language_Models.pdf"
}