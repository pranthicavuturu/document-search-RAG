{
    "title": "Weak-to-Strong Search:",
    "abstract": "Large language models are usually fine-tuned to align with human preferences. However, fine-tuning a large language model can be challenging. In this work, weintroduceweak-to-strongsearch, framingthealignmentofalargelanguage model as a test-time greedy search to maximize the log-probability difference betweensmalltunedanduntunedmodelswhilesamplingfromthefrozenlarge model. This method serves both as (1) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (2) an instance of weak-to-stronggeneralizationthatenhancesastrongmodelwithweaktest-time guidance. Empirically, we demonstrate the flexibility of weak-to-strong search acrossdifferenttasks. Incontrolled-sentimentgenerationandsummarization,we usetunedanduntunedgpt2stoimprovethealignmentoflargemodelswithout additional training. Crucially, in a more difficult instruction-following bench- mark, AlpacaEval 2.0, we show that reusing off-the-shelf small models (e.g., zephyr-7b-beta and its untuned version) can improve the length-controlled win rates of both white-box and black-box large models against gpt-4-turbo (e.g., 34.4% 37.9% for Llama-3-70B-Instruct and 16.0% 20.1% for → → gpt-3.5-turbo-instruct),despitethesmallmodels’lowwinrates 10.0%. ≈ 1 Introduction Learning-based algorithms [1, 2, 3, 4, 5] have become the standard approach for aligning large language models (LLMs) with human preferences [3, 6, 7, 8, 9, 10]. However, fine-tuning large languagemodelsisresource-intensiveanddifficulttoimplement[4].Thesechallengeshavemotivated recentstudiesonsearch-basedalgorithmsthatkeepthelargelanguagemodelsfrozenandsteertheir decoding with test-time guidance [11, 12, 13, 14, 15, 16, 17]. Typical examples of search-based algorithmsincluderejectionsampling[16,17]andMonteCarloTreeSearch[18,19]. Thesesearch- based algorithms are promising as they can reuse the same guiding signal to steer the decoding ofanylargelanguagemodelwithoutadditionaltraining. However,existingsearch-basedmethods eithersimplifythesearchovertokensasabanditproblem[16,17],whichlimitstheirsteerability,or requireavaluefunctionlearnedfromscratchtoaddresspreferencerewardsparsityandprunesearch space[13,18,14],whichcanbeasdifficultasfine-tuningalargelanguagemodel. To make search-based algorithms better suited for aligning large language models, we introduce weak-to-strongsearch,asimplealgorithmthatframesthealignmentofalargemodelasatest-time searchoverthelog-probabilitiesofsmalllanguagemodels. Thisalgorithmmakestwocontributions: (1)First,itbuildsonthetheoreticalfoundationofthetoken-levelMDPforalignment[20],usingthe 38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024). 4202 voN 91 ]LC.sc[ 3v26291.5042:viXra",
    "body": "Weak-to-Strong Search:\nAlign Large Language Models via\nSearching over Small Language Models\nZhanhuiZhou∗†,ZhixuanLiu∗,JieLiu,ZhichenDong,ChaoYang,YuQiao\nShanghaiArtificialIntelligenceLaboratory\n∗CoreContribution,†CorrespondingAuthor\nasap.zzhou@gmail.com\nCode: https://github.com/ZHZisZZ/weak-to-strong-search\nAbstract\nLarge language models are usually fine-tuned to align with human preferences.\nHowever, fine-tuning a large language model can be challenging. In this work,\nweintroduceweak-to-strongsearch, framingthealignmentofalargelanguage\nmodel as a test-time greedy search to maximize the log-probability difference\nbetweensmalltunedanduntunedmodelswhilesamplingfromthefrozenlarge\nmodel. This method serves both as (1) a compute-efficient model up-scaling\nstrategy that avoids directly tuning the large model and as (2) an instance of\nweak-to-stronggeneralizationthatenhancesastrongmodelwithweaktest-time\nguidance. Empirically, we demonstrate the flexibility of weak-to-strong search\nacrossdifferenttasks. Incontrolled-sentimentgenerationandsummarization,we\nusetunedanduntunedgpt2stoimprovethealignmentoflargemodelswithout\nadditional training. Crucially, in a more difficult instruction-following bench-\nmark, AlpacaEval 2.0, we show that reusing off-the-shelf small models (e.g.,\nzephyr-7b-beta and its untuned version) can improve the length-controlled\nwin rates of both white-box and black-box large models against gpt-4-turbo\n(e.g., 34.4% 37.9% for Llama-3-70B-Instruct and 16.0% 20.1% for\n→ →\ngpt-3.5-turbo-instruct),despitethesmallmodels’lowwinrates 10.0%.\n≈\n1 Introduction\nLearning-based algorithms [1, 2, 3, 4, 5] have become the standard approach for aligning large\nlanguage models (LLMs) with human preferences [3, 6, 7, 8, 9, 10]. However, fine-tuning large\nlanguagemodelsisresource-intensiveanddifficulttoimplement[4].Thesechallengeshavemotivated\nrecentstudiesonsearch-basedalgorithmsthatkeepthelargelanguagemodelsfrozenandsteertheir\ndecoding with test-time guidance [11, 12, 13, 14, 15, 16, 17]. Typical examples of search-based\nalgorithmsincluderejectionsampling[16,17]andMonteCarloTreeSearch[18,19]. Thesesearch-\nbased algorithms are promising as they can reuse the same guiding signal to steer the decoding\nofanylargelanguagemodelwithoutadditionaltraining. However,existingsearch-basedmethods\neithersimplifythesearchovertokensasabanditproblem[16,17],whichlimitstheirsteerability,or\nrequireavaluefunctionlearnedfromscratchtoaddresspreferencerewardsparsityandprunesearch\nspace[13,18,14],whichcanbeasdifficultasfine-tuningalargelanguagemodel.\nTo make search-based algorithms better suited for aligning large language models, we introduce\nweak-to-strongsearch,asimplealgorithmthatframesthealignmentofalargemodelasatest-time\nsearchoverthelog-probabilitiesofsmalllanguagemodels. Thisalgorithmmakestwocontributions:\n(1)First,itbuildsonthetheoreticalfoundationofthetoken-levelMDPforalignment[20],usingthe\n38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).\n4202\nvoN\n91\n]LC.sc[\n3v26291.5042:viXra\n40\nzephyr-7b-beta(tuned) 37.9\nmistral-7b-sft-beta(untuned)\n35 34.4\nBase\nWeak-to-strongsearch\n30\n27.2\n25\n22.9\n20.1\n20 19.1\n16.2 16.0\n15 13.7\n10.1\n10\nLlama2-7B Llama2-70B Llama3-8B Llama3-70B GPT3.5\nFigure1: Weak-to-strongsearchenhancesthealignmentoflargemodelsthroughtest-timeguidance\nfromsmallmodels(dashedlines). Thismethodisapplicabletowhite-boxmodelsthatusethesame\nordifferentvocabulariesasthesmallmodels,aswellastoblack-boxmodels. Wepresenttheresults\nfortheinstruction-tunedmodelsfromeachfamily(e.g.,Llama2-7BdenotesLlama-2-7b-chat).\nlog-probabilitydifferencebetweensmalltunedanduntunedlanguagemodelsasbothrewardand\nvalue[4,20]toguidethedecodingofalargemodel(Section4.1). Theoretically,thisformulationis\nsuitableforsearchasitconvertstheotherwisesequence-levelsparsepreferencerewardfunctiontoa\nper-tokendenserewardfunction,whichcanbesummedupasavaluefunction[20]. Practically,this\nformulationallowsthereuseofoff-the-shelfsmalltunedanduntunedlanguagemodelpairsassteering\nforces,avoidingtheneedtotrainarewardorvaluemodelfromscratch. (2)Second,itintroducesa\nbeamsearchvariant,Chunk-levelBeamSearch(CBS),tailoredforoptimizingtheproposedsearch\nobjective. CBSguidesthelargelanguagemodeltowardshigh-rewardregionsbyalternatingbetween\nsampling from the frozen large model and expanding promising states as evaluated by the small\ntunedanduntunedmodels(Section4.2). Especially,whenthesmallmodelsareweakerthanthelarge\nmodel,ourmethodcanbeviewedasaninstanceofweak-to-stronggeneralization[21]thatmakesthe\nstrongmodelstrongerwithweaktest-timeguidance(Figure1).\nEmpirically, we verify weak-to-strong search’s flexibility in various tasks (Section 5). First, in\ncontrolled-sentimentgeneration[22]andsummarization[2],ourmethodusessmalllanguagemodels\nof124Mparameters(i.e.,gpt2)toeffectivelysteermuchlargerlanguagemodelsfromtheGPT-2\n(e.g.,gpt2-xl)[23],Llama-2[7]andLlama-3[24]families,atleastaseffectiveasexistingmethods.\nThen,inamoredifficultinstruction-followingbenchmark,AlpacaEval2.0[25],weshowreusing\noff-the-shelfsmallmodels(e.g.,zephyr-7b-betaanditsuntunedversion)astest-timeguidancecan\nsignificantlyimprovethelength-controlledwinratesofbothwhite-boxandblack-boxlargemodels\nagainstgpt-4-turbo(e.g,34.4% 37.9%forLlama-3-70B-Instruct,and16.0% 20.1%\n→ →\nforgpt-3.5-turbo-instruct),despitethesmallmodels’lowwinrates 10.0%(Figure1).\n≈\n2 RelatedWork\nLargeunsupervisedlanguagemodelstrainedoninternet-scalecorpusacquirebroadknowledgeand\nabilities[26,27,28]. However,theselargepre-trainedlanguagemodelsmaynotalwaysalignwith\nhumanvalues. Toinstillthedesiredbehaviorsintolanguagemodels,mostexistingmethodsfine-tune\nthesepre-trainedlanguagemodelsonhumancomparisonsofmodel-generatedresponses[1,2,3,\n4, 7, 24, 6, 29]. Despite these successes, fine-tuning a large language model requires substantial\ncomputationalresourcesandengineeringeffort. Theseproblemsarecompoundedbytherealitythat\ndifferenthumanshavedifferentvalues[3,30,13,31,32,33],asitisnearlyimpossibletotrainanew\nlargelanguagemodelfromscratchforindividualpreference. Inlightoftheseissues,ourworktakesa\nsearch-basedapproach,foldingasmuchofthecomplexityofalignmentaspossibleintothedecoding\nphase. Thisallowsustokeepthelargepre-trainedlanguagemodelsfrozen,steeringtheiroutputsat\ntesttimewithonlysmallmodelsthatareeasiertoobtain.\nFramingalignmentasatest-timesearchtomaximizearewardfunctionisnotanovelformulation.\nHowever,mostexistingworkseithersimplifyautoregressivedecodingasabanditproblem[16,17],\nwhich limits their steerability, or require a value function learned from scratch to handle sparse\n2\n)%(etaRniWCL0.2lavEacaplA\npreference rewards and prune search space [13, 18], which can be as difficult as training a large\nlanguagemodelfromscratch. Ourworkavoidstheseissuesbyparametrizingthesparsepreference\nreward function with the log-probability difference between small tuned and untuned language\nmodels[4]. Thisparametrizationnotonlysimplifiesthesearchobjective,allowingasimplegreedy\nsearchalgorithmtogenerategoodresults,butalsoreusesoff-the-shelfmodelsassteeringforces,\neliminatingtheneedtotrainarewardorcriticmodelfromscratch.\nConcurrently with our work, Rafailov et al. [20] proposes a token-level MDP interpretation for\nlanguagemodelalignment,demonstratingthatagreedyprobabilitysearchoveratrainedlanguage\nmodel can achieve improvements over regular decoding. Our work builds on their theoretical\nfoundationsandproposesapracticalgreedysearchalgorithmdesignedforweak-to-strongguidance.\nTheideaofusingsmalllanguagemodelstoalignlargelanguagemodelshasariseninmanyrecent\nworks.Themostrelatedisproxyoremulatedfine-tuning[34,12,11,35],whichusesthedistributional\ndifferenceofasmalltunedanduntunedmodelpairtomodifytheoutputdistributionofalargemodel,\napproximatingtheoutputofthedirectlytunedlargemodel. However,thesemethodsrequirethatboth\nsmallandlargemodelssharethesamevocabulary,limitingtheirpracticalapplications. Incontrast,\nourapproachdoesnotmodifythesamplingdistributionofthelargemodelatthetokenlevel. Instead,\nweperformatreesearchthatperiodicallyprioritizesthemostpromisingstatesforfurtherexpansion\n(asevaluatedbythesmallmodels)whilesamplingfromthefrozenlargemodel’sdistribution. Thus\nourapproachdoesnotrequiresharedvocabularyandisapplicabletoblack-boxlanguagemodels.\n3 Preliminaries\nInthissection,weintroducethemathematicalformulationofalignment(Section3.1)anddescribe\nthedualitybetweenlanguagemodelsandrewardfunctions(Section3.2).\n3.1 AligningLanguageModelswithHumanPreferences\nThealignmentoflanguagemodelsistypicallycastasaKL-constrainedoptimizationproblem[1]:\nargmax E [r(x,y)] (1a)\nx∼p(x),y∼π(y|x)\nπ\ns.t. E [D (π(y x) π (y x))] ϵ, (1b)\nx∼p(x) KL ref\n| ∥ | ≤\nwherep(x)isadistributionofprompts,yisthecompletelanguagemodelresponse,risapreference\nrewardfunctionthatencourageshuman-preferredresponses,andD limitshowfartheoptimized\nKL\nlanguagemodelπcandeviatefromthereference(untuned)modelπ . Therearetwomaincategories\nref\nofalignmentalgorithms: (1)search-basedalgorithmsthatoptimizeEq.1withgraph-basedsearch\nduring inference [16, 13, 18, 19, 11, 12], and (2) learning-based algorithms that optimize Eq. 1\nthroughgradientdescent,aimingforaparametrizedoptimallanguagemodel[1,36,4,37]. Ourwork\nfallsinthefirstcategory,proposingasearch-basedalgorithmcapableofusingsmalllanguagemodels\ntoguidethedecodingofalargelanguagemodeltoalignwithhumanpreferences.\n3.2 DualitybetweenLanguageModelsandRewardFunctions\nTheanalyticalsolutiontoEq.1canbeobtainedthroughthefollowingLagrangian[38,39]:\n(π,β)=E [r(x,y)+β(ϵ D (π(y x) π (y x)))], (2)\nx∼p(x),y∼π(y|x) KL ref\nL − | ∥ |\nwhichhasawell-knownclosed-formsolutionthatexpressesadualitybetweentherewardfunction\nr(x,y)andtheoptimallanguagemodelπ∗(y x)[40,41]:\n|\nπ∗(y x)\nr(x,y)=βlog | +βlogZ(x), (3)\nπ (y x)\nref\n|\n(cid:16) (cid:17)\nwhereZ(x)=(cid:80) π (y x)exp 1r(x,y) denotesthepartitionfunction. Onetakeawayfrom\ny ref | β\nthisdualityisthatwecanalwaysexpressarewardfunctionusingtunedanduntunedlanguagemodels:\n(1)Ifarewardfunctionisgiven[1,2,3],wecanfirstobtaintheoptimallytunedlanguagemodel\nunderthisrewardfunctionwithanylearning-basedalgorithms,andthenusethetunedanduntuned\nmodels(π∗,π )toreparametrizetherewardfunction[42];(2)Ifadatasetisgivenfromwhichthe\nref\nrewardfunctioncanbederived,wecanthendirectlyparametrizetherewardfunctionwiththetuned\nanduntunedlanguagemodels(π∗,π )duringrewardmodeling[4].\nref\n3\n4 Weak-to-StrongSearch\nIn this section, we introduce weak-to-strong search, a search-based algorithm that aligns a large\nlanguagemodelbysearchingoverthelog-probabilitydifferencebetweensmalltunedanduntuned\nlanguage models. First, we discuss how using language models to parametrize the preference\nreward function (Eq. 1) makes the reward-maximization problem solvable by a simple greedy\nsearch algorithm (e.g., beam search) (Section 4.1). Then, we introduce a practical beam search\nmethod,Chunk-levelBeamSearch(CBS)(Section4.2),thatbalancesrewardmaximizationandKL\nminimization,whichisapplicabletosteeringbothwhite-boxandblack-boxlargelanguagemodels.\n4.1 LanguageModelsasBothRewardandValueFunctions\nOnepracticalchallengeforsearch-basedalignmentalgorithmsisthesparsityofthepreferencereward\nsignal. Thepreferencerewardfunctionr(x,y),basedontheBradley-Terrymodel[43],onlyemits\naterminalrewardwhenthemodelresponseiscomplete. Search-basedalgorithmsoftenstruggle\nwithout any intermediate rewards or a value function providing intermediate guidance [44, 45].\nHowever,ifweparameterizethissparserewardfunctionwithlanguagemodels(Section3.2),wecan\nobtainbothadenserewardfunctionandavaluefunctionsimultaneously.\nLanguagemodelsasadenserewardfunction. Toobtainadenserewardfunction,weleverage\nthedualitybetweenthesparsepreferencerewardfunctionandthedenselanguagemodelprobability\n(Eq.3). Byexplicitlyfactorizingthelog-probabilityofacompleteresponseyunderthelanguage\nmodels,weobtainasum-of-rewardsstyleformulationforEq.3:\n \nr(x,y)=β(cid:88)|y|\nlog\nπ∗(y\nt\n|x,y <t)\n+βlogZ(x), (4)\nπ (y x,y )\nt=1 ref t | <t\nwherey denotestheresponsetokensfrom1tot 1,andthelastresponsetokeny istheEOS\n<t − |y|\ntoken. CombiningEq.1and4,werewritetheoriginalobjectivewithaper-tokenrewardfunction:\n \narg πmax E x∼p(x),y∼π(y|x)(cid:88) t| =y| 1log ππ r∗ ef( (y yt t| |x x, ,y y< <t t) ) (5a)\ns.t. E [D (π(y x) π (y x))] ϵ, (5b)\nx∼p(x) KL base\n| ∥ | ≤\nwhereβ andZ(x)areomittedastheydonotinfluencetheoptimalsolution. Itisimportanttonote\nthatthereferencemodelthatparametrizestherewardfunction(π )(Eq.5a)andthereferencemodel\nref\nthatconstrainsthetest-timesearchspace(π )(Eq.5b)canbedifferent. Practically,decoupling\nbase\nthereferencemodelsisusefulasitallowsusingatunedanduntunedlanguagemodelpair-\nnamely(π∗,π )-tosteerthedecodingofanybaselanguagemodelπ withoutretraining.\nref base\nSetting aside the KL constraint (Eq. 5b) for now, we can apply existing search algorithms like\nbeam search [45, 20] to optimize Eq. 5a. Beam search is often criticized for leading to myopic\nsolutions[45],asittendstogreedilyprioritizestates(x,y′)(y′isincomplete)1withhighcumulative\nrewardlogπ∗(y′ x) logπ (y′ x)midwaythroughgeneration,whichisgenerallyviewedas\nref\n| − |\npoorlycorrelatedwiththeoverallreturnwecareabout. WhilethiscriticismisvalidformostMDPs,\nwe argue that in the token-level MDP [20] of our case, the cumulative reward mid-generation is\nactuallyareliableindicatorofthelong-termvalue,makingbeamsearchlessmyopic.\nCumulativerewardunderlanguagemodelsasavaluefunction[20]. AppendixAshowsthat:\nπ∗(y′ x) (cid:26) V∗(x)+V∗(x,y′) ify′isincomplete\nlog | − (6)\nπ (y′ x) ∝ V∗(x)+r(x,y′) ify′iscomplete,\nref\n| −\nwhereV∗(x,y′)denotesthevaluefunction,predictingtheexpectedterminalrewardundertheoptimal\nπ∗ intheoriginalKL-constrainedsparserewardproblem. AlthoughV∗(x,y′)isnotnecessarily\nachievablebythesearchedpolicy,itapproximateshowgoodthestate(x,y′)isinthelongrun. In\notherwords,continuingfromthestate(x,y′)ofhighcumulativerewardlogπ∗(y′ x) logπ (y′\nref\nx)islikelytogenerateacompleteresponseywithhighoverallreturnlogπ∗(y x)| l− ogπ (y x)| .\nref\n| − |\n1ydenotesacompleteresponse,whiley′denotesaresponsethatcanbeeitherincompleteorcomplete.\n4\nPrompt\nPlease write a movie review\nHypothesis Set Successor Chunk Top-W Hypothesis Set\nhate the boring plot 0.8\nMy wife and I really\nMy wife and I really\nlike the fun story\nlike the fun story 5.1\nfound it too long 0.4\nMy wife and I indeed\nMy wife and I indeed\nliked the nice play\nliked the nice play 4.7\nK successors per state;\nScore =\nSampled i.i.d. from\nFigure2: IllustrationofChunk-levelBeamSearchwithW,K =2,2.\n4.2 Chunk-levelBeamSearch(CBS)\nAfteranalyzingthefeasibilityofoptimizingEq.5awithgreedysearchalgorithms(e.g.,beamsearch),\nweintroduceapracticalbeamsearchvariantthatoptimizesthedenserewardobjective(Eq.5a)while\nensuringtheKL-constraintfromπ (Eq.5b).\nbase\nThe core algorithm providing the foundation of our method, Chunk-level Beam Search (CBS),\nis detailed in Algorithm 1 and illustrated in Figure 2. The key insight is that our beam search\noperatesatthelevelofchunk. Thesearchstartsatthepromptandalwaysmaintainsahypothesisset\n= (x,y′) W ofW states. Foreachstate(x,y′)in ,CBSsamplesK continuationchunks\nH { i }i=1 H\ny ofLtokensfromπ . ThisresultsinWK successorstates. Amongthesesuccessors,onlythe\nL base\ntop-W successorswiththehighestpartialreturnlogπ∗(y′ y x) logπ (y′ y x)arestored\nL ref L\n◦ | − ◦ |\nin andexpandedfurther. Finally,theterminalstate(x,y)withthehighestintermediatereturn\nlogH π∗(y x) logπ (y x)isselected,fromwhichthecompleteresponseyisextracted. Notably,\nref\nifthemod| elto− steerπ h| asadifferentvocabularyfromthetunedandunturnedmodels(π∗,π ),\nbase ref\nweshouldfirstdecodethesampledtokensintonaturallanguageusingπ ’svocabularyandthen\nbase\nre-encodeusing(π∗,π )’svocabularyforevaluation.\nref\nAlgorithm1Chunk-levelBeamSearch(CBS)\n1: Input: promptx,beamwidthW,successorsperstateK,chunklengthL,\n2: modeltosteerπ ,tunedmodelπ∗,anduntunedmodelπ .\nbase ref\n3: Output: optimalterminalstate(x,y)\n4: Initialize H= {(x,y′ =∅) i }W i=1\n5: while (x,y′) suchthaty′isincompletedo\n∃ ∈H\n6: Initialize =\nC {}\n7: foreach(x,y′) do\n∈H\n8: Y ←{(y L) i }K i=1 i. ∼i.d. π base( ·|x,y′) //y L =∅ify′iscomplete\n9: (x,y′ y L) y L\nC ←C∪{ ◦ | ∈Y}\n10: endfor\n11: H←Top-W (x,y′◦yL)∈C(logπ∗(y′ ◦y L |x) −logπ ref(y′ ◦y L |x))\n12: endwhile\n13: returnargmax (logπ∗(y x) logπ (y x))\n(x,y)∈H | − ref |\nCBS is a unified framework that encompasses several search-based algorithms: (1) CBS\nwith W = 1, K = N, L = (i.e., infinite chunk length) is equivalent to BoN sampling with\nlogπ∗(y x) logπ (y x∞ ) as the scoring function, and (2) CBS with K = , L = 1 (i.e.,\nref\n| − | ∞\nexploringallpossiblenexttokensfromthevocabulary)isequivalenttovanillatoken-levelbeam\n5\nsearch.However,wealwaysensurefinitechunklengthandlimitedsuccessorexplorationviasampling\ntoachievethebestofbothworlds: (1)UsingafinitechunklengthallowsCBStoprunebadstates\nduring generation, enhancing steerability more efficiently compared to BoN. (2) Sampling from\nπ withlimitedsuccessorexplorationimplicitlyenforcestheKL-constraintfromπ (Eq.5b);\nbase base\notherwise,integratingtheKL-constraintintotheobjective(Eq.5a)wouldbenecessaryfortoken-level\nsearch,butthiscanbechallenging,especiallywhenvocabulariesofmodelsdifferorwithblack-box\nlanguagebasemodelsπ whoselog-probabilitiesareinaccessible.\nbase\nComputationcosts. Inpractice,CBSsamplesWK continuationchunksinparallelfromthefrozen\nbasemodelπ andprunestatesbycallingtunedanduntunedmodelpair(π∗,π )everyLtokens.\nbase ref\nLarger WK and smaller L enhance steerability at the cost of increased computations. Note that\nhigh steerability, while beneficial, is not always ideal as it may lead to large KL deviation and\nover-optimization[16].\n4.3 Application: ModelUp-ScalingandWeak-to-StrongGeneralization\nThemostpracticaluseofCBSoccurswhenthetunedanduntunedmodels,(π∗,π ),aresmaller\nref\nthanthemodeltosteer,π . (1)First,thisinstanceservesasamodelup-scalingstrategy,directly\nbase\ntuningasmallmodelπ π∗,bywhichthelargemodeldecodingcanthenbeguided,toachieve\nref\nsimilaroutcomesasdirec→ tlytuningthelargemodel. (2)Second,sincethesmallmodels(π∗,π )\nref\nareusuallyweakerthanthelargemodeltosteerπ ,thisinstancealsoexemplifiesweak-to-strong\nbase\ngeneralization[21],enhancingthestrongmodelwithonlyweaktest-timeguidance. Werefertothis\ninstanceofCBSasweak-to-strongsearch,whichisthemainfocusofourstudy.\n5 Experiments\nIn this section, we empirically evaluate weak-to-strong search’s ability to align large language\nmodelsusingonlytest-timeguidancefromsmalllanguagemodels. First,incontrolled-sentiment\ngeneration[22]andsummarization[2],wetunegpt2tomodelthedesiredbehaviorsineachtask\nandthenusetunedanduntunedgpt2tosteerlargermodelsofvariousscales(Section5.1). Next,\ninamoredifficultinstruction-followingbenchmark,AlpacaEval2.0[25],insteadoftunningsmall\nmodels,wereuseoff-the-shelfopen-source7Bmodelsandtheiruntunedversionstosteeraseriesof\nlargemodels,includingopen-source70Bmodelsandablack-boxmodel(Section5.2).\nBaselines. Inadditiontoweak-to-strongsearch,weevaluateseveralexistingtest-timeapproaches\nthatsteeralargelanguagemodelπ usingsmalltunedanduntunedlanguagemodels(π∗,π ):\nbase ref\n(1)Base: weexploreregulardecodingfromthefrozenlargelanguagemodelwithn-shotprompting\n(see Appendix B.1.6 for prompt details). (2) Best-of-N Sampling (BoN) [16, 17]: BoN uses\nr =logπ∗(y x) logπ (y x)toselectthehighest-scoringresponsesamongtheN independent\nref\n| − |\nresponsesfromthefrozenlargelanguagemodel. Sinceweak-to-strongsearch(CBS)samplesWK\nresponse chunks in parallel, for fair computational comparisons, we always ensure N = WK.\n(3)EmulatedFine-Tuning(EFT)[34,12,11,35]: EFTapproximatestheresultsofdirectlyfine-\ntuningthelargelanguagemodelbysamplingfromlogπ (y x,y ) logπ (y x,y )+\nβ−1(logπ∗(y x,y ) logπ (y x,y )),whereβE isFT thet hy| perp< art am∝ eterfrob mase Eq.t 2| .Not< et that\nt | <t − ref t | <t\nEFTisonlyapplicablewhenallmodelssharethesamevocabulary(whichisnecessaryforcomposing\noutputdistributionsfromdifferentmodels). Wheneverpossible,wealsocomparetest-timemethods\nagainstdirectlyfine-tuningthelargemodelsinthesamewaysmallmodelsaretuned.\n5.1 Controlled-SentimentGeneration&Summarization\nSetup. Forthesetwotasks,wefollowthesyntheticsetupsfrom[16,46,4],assumingaccesstoa\ngoldrewardmodelr . Forcontrolled-sentimentgeneration,r encouragespositivecontinuations\ngold gold\nofmoviereviews,whileforsummarization,itencourageshigh-qualitysummariesofRedditposts\n(detailsinAppendixB.1.4). Wegeneratesyntheticpreferencedatasets = (x,y ,y ) N from\nD { w l i }i=1\nr withp(y y x)=σ(r (x,y ) r (x,y ))tomimichumanfeedback[47].\ngold 1 2 gold 1 gold 2\n≻ | −\nToobtainthesmalllanguagemodels,weoptimizegpt2(124Mparameters)usingthestandardDPO\npipeline [4]: (1) we first obtain the reference model π through supervised fine-tuning on both\nref\nchosenandrejectedresponsesfromthesyntheticpreferencedataset,then(2)weapplyDPOonthe\n6\nControlled-Sentiment Generation Summarization\n5\n2\n4\n1\n3\n0\n2\n1\n1 −\n0 2\nGPT2-large GPT2-xl Llama2-7B Llama3-8B − GPT2-large GPT2-xl Llama2-7B Llama3-8B\nGPT2-DPO(π∗) Base(π base) BoN(16) Directlytuned\nGPT2-SFT(π ref) Weak-to-strongsearch(4,4,5) EFT(β∗)\nFigure 3: The gold reward achieved for different large pre-trained models under the gpt2\nguidance. Weshowthemeanreward( standarddeviations)acrossthreerandomseeds. EFT(β∗)\n±\ndenotesthebestEFTresultsamongβ 1/4,1/2,1,2,4 ;Weak-to-strongsearch(4,4,5)denotes\n∈{ }\nCBSwithW,K,L=4,4,5;BoN(16)denotesBoNwithN =16.\nsyntheticpreferencedatasetwithπ asthereferencepolicytoobtaintheoptimallanguagemodel\nref\nπ∗. Notethatthefirststageprimarilyinformsthelanguagemodelofthedesiredresponseformat,\nwithmostofthetuningoccurringinthesecondDPOstage.\nGiventhetunedanduntuned(un-DPO-tuned)gpt2pair(π∗,π ),weusethemtosteerthelarge\nref\npre-trained language models without additional training. The large pre-trained language models\nwe study fall into two categories based on whether they share the same vocabulary as the small\nmodels: (1)samevocabulary: gpt2-large(774M),gpt2-xl(1.5B)and(2)crossvocabulary:\nLlama-2-7b,Llama-3-8B.Eventually,sincewehaveaccesstothegoldrewardmodel,language\nmodelresponsescanbefairlyevaluatedonthetestsplitofpromptsusingthisgoldrewardmodel.\nResults. Figure3demonstratesweak-to-strongsearch’sgreatflexibilityandsteerabilityinboth\ntasks. Forsummarization,weak-to-strongsearchconsistentlyoutperformsothertest-timemethods\nby large margins. For controlled-sentiment generation, weak-to-strong search is second only to\nEFTwithacarefullyselectedhyperparameter(β∗ =1/4)whenEFTisapplicable. Wehypothesize\nthat token-level adjustments from EFT are sufficient for controlled-sentiment generation, which\nprimarilyrequiresminorstylisticchangesatthetokenlevel(e.g.,“hate” “love”). However,inthe\n→\nmorecomplextaskofsummarization,wherebroadersubsequence-levelmanipulationsareessential,\nweak-to-strongsearchexcels. PleaserefertoAppendixDforquantitativecomparisonsofsamples\nfromdifferentmethods. Weneedtomentionthatwedonotmeaningfullytuneweak-to-strongsearch\n(CBS)’shyperparameterstoobtaintheresultsinFigure3(weuseafixedsetofhyperparametersof\n(4,4,5)forW,K,Lacrossallmodels),whichmayunderestimatetheperformanceofourmethod.\nInaddition,ourmethodenablesconsistentweak-to-stronggeneralizationinthehardertaskof\nsummarization: mostlargepre-trainedmodels(exceptforgpt2-large)arestrongerthanthetuned\ngpt2 in summarizing long text, but the weak models are still able to improve the strong models\nthroughtest-timeguidance,nearlymatchingtheresultsofdirectfine-tuning. Thephenomenonof\nweak-to-stronggeneralizationwillbefurtherstudiedinSection5.2.\nChunk-levelBeamSearchablations. WeperformadditionalablationstounderstandhowCBS\nhyperparameters(beamwidthW,successorsperstateK,andchunklengthL)influenceperformance.\nFigure4displaystheablationresultsforW andK. Withthesamecomputationbudget(i.e.,WK),\ntheoptimaltrade-offbetweenW andK variesbytasks: forcontrolled-sentimentgeneration,the\nbestresultscomefromretainingthemostpromisingstateandconcentratingcomputationaleffortson\nexpandingfromit(W,K =1,16);incontrast,forsummarization,maintainingmultiplehypotheses\n(W,K = 8,2) yields the best results probably because it helps avoid local optima. Figure 5\ndisplaystheablationresultsforLwheresmallerLbenefitscontrolled-sentimentgeneration,while\nanintermediateLisoptimalforsummarization. Theseresultsareconsistentwithourfindingsfrom\n7\n)\nr(MRdloG\ndlog\nFigure3,suggestingthatthesimplenatureofcontrolled-sentimentgenerationmakestoken-level\nmanipulationsufficientandcumulativerewardmid-generationamorereliableindicatorofoverall\nreturn. SeeAppendixC.1forextendedablationsonmoremodels.\ngpt2-xl Llama-2-7b gpt2-xl Llama-2-7b\n16 4.61 16 4.09 16 -0.20 16 1.35\n8 4.31 4.62 8 3.91 4.12 8 0.02 0.22 8 1.49 1.52\n4 3.85 4.30 4.52 4 3.49 3.87 4.05 4 0.15 0.56 0.63 4 1.50 1.57 1.61\n2 3.13 3.75 4.10 4.36 2 2.81 3.24 3.63 3.75 2 -0.11 0.42 0.81 1.01 2 1.24 1.56 1.63 1.65\n1 1.77 2.43 2.96 3.35 3.57 1 1.86 2.26 2.65 2.91 3.12 1 -0.87 -0.44 -0.29 -0.22 -0.22 1 0.65 0.80 0.90 1.02 1.08\n1 2 4 8 16 1 2 4 8 16 1 2 4 8 16 1 2 4 8 16\nbeamwidth(W) beamwidth(W) beamwidth(W) beamwidth(W)\n(a)controlled-sentimentgeneration (b)summarization\nFigure4: W,KablationsforCBS(L=5). Weshowthemeanrewardsacrossthreerandomseeds.\nWiththesamecomputationbudget(i.e.,sameWK),theoptimalhyperparametersdifferbytasks.\n2.0\n5.0 gpt2-xl\nLlama-2-7b 1.5\n4.5\n1.0\n4.0\n0.5\n3.5\n0.0\ngpt2-xl\n3.0 0.5 Llama-2-7b\n−\n0 10 20 30 40 50 0 10 20 30 40 50\nchunklength(L) chunklength(L)\n(a)controlled-sentimentgeneration (b)summarization\nFigure5: LablationsforCBS(W,K=4,4). Weshowthemeanrewards( standarddeviations)\n±\nacrossthreerandomseeds.\n5.2 InstructionFollowing\nSetup. Next,weevaluateweak-to-strongsearchonastandardsingle-turninstruction-following\nbenchmark,AlpacaEval2.0[25],whichconsistsof805promptsfromvariousopen-sourcedatasets.\nUnliketheprevioussectionwherewesteerlargepre-trainedlanguagemodels(e.g.,Llama-2-7b),\nwenowsteerlargeinstruction-tunedlanguagemodels(e.g.,Llama-2-7b-chat). Thisisbecause\n(1)instruction-tunedmodelsoftenrequirefurtheralignmenttomatchhumanpreferences[48],and\n(2)tostudyweak-to-stronggeneralizationininstruction-following,themodelsmustbeproficientat\nfollowinginstructionsbeforesteering.\nFor small language models, we reuse two high-ranking 7B model pairs from the AlpacaEval\n2.0 leaderboard as guidance: (1) Zephyr guidance: zephyr-7b-beta and its untuned version\nmistral-7b-sft-beta;(2)Tuluguidance: tulu-2-dpo-7banditsuntunedversiontulu-2-7b.\nAllfourmodelsusetheLlama-2tokenizer.Thelargeinstruction-tunedlanguagemodelsweaimtofur-\ntheralignfallintothreecategories:(1)samevocabulary:Llama-2-7b-chat,Llama-2-70b-chat;\n(2) cross vocabulary: Llama-3-8B-Instruct, Llama-3-70B-Instruct; and (3) black box:\ngpt-3.5-turbo-instruct. Asitisnearlyimpossibletoreproducetheexacttrainingpipelinefor\nthesesmallmodels(π π∗),wedonottestthebaselineresultsofdirectlyfine-tuningthelarge\nref\n→\nmodelsasinFigure3. Languagemodelresponsesareevaluatedbytheirlength-controlledwinrates\n(LCWR)againstgpt-4-turbo,withgpt-4-turboservingasthejudge.\nResults. Experimental results with Zephyr and Tulu guidance are shown in Figure 6 (detailed\nhyperparametersinAppendixB.2.2). Weak-to-strongsearchconsistentlyoutperformsothertest-time\n8\n)K(etatsrepsrosseccus\nMRdloG\n)K(etatsrepsrosseccus\nMRdloG\nInstruction Following (Zephyr) Instruction Following (Tulu)\n40 40\nzephyr-7b-beta(π∗) tulu-2-dpo-7b(π∗)\n35\nmistral-7b-sft-beta(πref)\n35\ntulu-2-7b(πref)\n30 30\n25 25\n20 20\n15 15\n10 10\nLlama2-7B Llama2-70B Llama3-8B Llama3-70B GPT3.5 Llama2-7B Llama2-70B Llama3-8B Llama3-70B GPT3.5\nBase(π base) Weak-to-strongsearch BoN EFT(β∗)\nFigure 6: The length-controlled win rates against gpt-4-turbo for various instruction-tuned\nmodelsunderZephyr(left)orTulu(right)guidance. HyperparametersareinAppendixB.2.2.\nbaselineswithgreatmargins.Therearetwocrucialtakeawaysworthmentioning:(1)Weak-to-strong\nsearchmakesstrongmodelsstrongerwithonlyweaktest-timeguidance. TakeZephyrguidance\nforanexample(Figure6,left),evenifmostlargeinstruction-tunedmodelsπ arestrongerthan\nbase\nzephyr-7b-betabeforesteering,weak-to-strongsearchisstillabletoenhancetheirperformances\nusingweakmodelsasguidance. Conversely,EFTandBoNmainlyinterpolatebetweenweakand\nstrongmodels,resultinginlimited,ifany,improvementsoverthestrongmodels. Wealsotestedbeam\nsearchoverthestrongmodelswithoutexternalguidance[20]butwefoundnoobviousimprovements\ncomparedwithregulardecoding(Table2),probablybecausethelatentrewardfunctionsbehindthese\nlanguagemodelsarenotwellalignedwiththehumanpreferencethatgpt-4-turboapproximates.\nThesameobservationsapplytoTuluguidance,eventhoughthetunedtulu-2-dpo-7bisweakerthan\nallthelargeinstruction-tunedlanguagemodelsbysignificantmargins(Figure6,right). (2)Weak-to-\nstrongsearchappliestoblack-boxlanguagemodels. Ourmethod,requiringonlysamplingfrom\nlargelanguagemodels,isalsoeffectiveforblack-boxmodelslikegpt-3.5-turbo-instruct. For\nweak-to-strongsearchwithgpt-3.5-turbo-instruct,weusearelativelylongchunklengthof\n100,astheblack-boxlanguagemodelAPIsarestatelessanddonotretainactivationcaches,making\nrepeated context embedding costly. Despite the long chunk length, our method still effectively\nimprovesthealignmentofblack-boxmodels, significantlyoutperformingBoN,aspecialcaseof\nweak-to-strongsearch(CBS)withinfinitechunklength.\n6 Discussion\nWehavepresentedweak-to-strongsearch,analignmentmethodthatkeepsthelargelanguagemodel\nfrozenwhilesteeringitsdecodingthroughatest-timegreedysearchoversmalllanguagemodels.\nThismethodbuildsontheinsightthatthelog-probabilitydifferencebetweensmalltunedanduntuned\nlanguagemodelscanservebothasadenserewardfunctionandavaluefunction,andthenintroduces\nanovelbeamsearchalgorithmdesignedforbalancingrewardmaximizationandKLminimization.\nThismethodoffersacompute-efficientmodelup-scalingstrategythateliminatesthecomplexityof\ndirectlyfine-tuningthelargemodels,andexemplifiesweak-to-stronggeneralization[21]thatmakes\nstrongmodelsstrongerwithonlyweaktest-timeguidance. Empirically,thisapproachiseffectivein\ncontrolled-sentimentgeneration,summarization,andinstructionfollowing.\nLimitations&FutureWork. Whileourworkfocusesonaligningwithhumanpreferences,weak-\nto-strongsearchcouldalsoapplytotaskslikereasoning[49,50]andcoding[51],wheregroundtruth\nanswersexist. Thisisbecauseanypairoftunedanduntunedlanguagemodelscanactastest-time\nsteeringforces,withoutnecessarilybeingtrainedonpreferences. Thisthenraisesseveralquestions\nbeyondthescopeofourcurrentstudy: (1)Inourstudy, weconsistentlyuseSFTedpolicyasthe\nuntuned model π due to the two-stage nature of preference learning; however, in single-stage\nref\nfine-tuning tasks, does weak-to-strong search still work with a pre-trained model serving as the\nuntunedmodelπ ? (2)Althoughourmethodshowsconsistentweak-to-stronggeneralizationacross\nref\n9\n)%(etaRniWCL0.2lavEacaplA\ndiversealignmenttasks,itisalsocriticaltoprobeitspotentialfailuremodes[52]. Canweak-to-strong\nsearchenhancelanguagemodelsintaskswheregroundtruthanswersexist,beyondmerelytailoring\ntheirknowledgeandskillstohumanpreferences? (3)Additionally,whileourworkmainlyfocuseson\nhowdenselanguagemodelrewardfunction(Eq.4)benefitslanguagemodeldecodingattesttime,it’s\nalsoworthexploringthepotentialbenefitsofthisrewardparametrizationforRLtuning. Although\nAppendixC.3presentssomepromisingpreliminaryresults,weleavefurtheranalysisforfuturework.\nAcknowledgements\nThisworkwassupportedinpartbytheNationalKeyR&DProgramofChina(NO.2022ZD0160201).\nWewouldliketothankanonymousreviewersfortheirvaluablefeedbackandhelpfuldiscussions.\nAuthorContributions\nZhanhuiZhouledtheproject,proposedtheresearchidea,wrotethecodebase,designedtheexperi-\nments,conductedmostoftheinitialexperiments,andwrotethepaper. ZhixuanLiuassistedwith\nrunningmanyoftheablationstudiesthroughouttheexperimentspresentedinthepaper. Allother\nauthorsprovidedfeedbackthroughouttheproject.\n10\nReferences\n[1] DanielMZiegler,NisanStiennon,JeffreyWu,TomBBrown,AlecRadford,DarioAmodei,\nPaulChristiano,andGeoffreyIrving. Fine-tuninglanguagemodelsfromhumanpreferences.\narXivpreprintarXiv:1909.08593,2019.\n[2] NisanStiennon,LongOuyang,JeffreyWu,DanielZiegler,RyanLowe,ChelseaVoss,Alec\nRadford,DarioAmodei,andPaulFChristiano. Learningtosummarizewithhumanfeedback.\nAdvancesinNeuralInformationProcessingSystems,33:3008–3021,2020.\n[3] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,\nChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelsto\nfollowinstructionswithhumanfeedback. Advancesinneuralinformationprocessingsystems,\n35:27730–27744,2022.\n[4] RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,StefanoErmon,and\nChelseaFinn. Directpreferenceoptimization: Yourlanguagemodelissecretlyarewardmodel.\nAdvancesinNeuralInformationProcessingSystems,36,2024.\n[5] MohammadGheshlaghiAzar,ZhaohanDanielGuo,BilalPiot,RemiMunos,MarkRowland,\nMichalValko,andDanieleCalandriello. Ageneraltheoreticalparadigmtounderstandlearning\nfromhumanpreferences. InInternationalConferenceonArtificialIntelligenceandStatistics,\npages4447–4455.PMLR,2024.\n[6] YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,NovaDasSarma,Dawn\nDrain,StanislavFort,DeepGanguli,TomHenighan,etal. Trainingahelpfulandharmless\nassistantwithreinforcementlearningfromhumanfeedback. arXivpreprintarXiv:2204.05862,\n2022.\n[7] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,\nNikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open\nfoundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.\n[8] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile\nSaulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023.\n[9] LewisTunstall,EdwardBeeching,NathanLambert,NazneenRajani,KashifRasul,Younes\nBelkada,ShengyiHuang,LeandrovonWerra,ClémentineFourrier,NathanHabib,etal.Zephyr:\nDirectdistillationoflmalignment. arXivpreprintarXiv:2310.16944,2023.\n[10] AndreasKöpf,YannicKilcher,DimitrivonRütte,SotirisAnagnostidis,ZhiRuiTam,Keith\nStevens,AbdullahBarhoum,DucNguyen,OliverStanley,RichárdNagyfi,etal. Openassistant\nconversations-democratizinglargelanguagemodelalignment. AdvancesinNeuralInformation\nProcessingSystems,36,2024.\n[11] EricMitchell,RafaelRafailov,ArchitSharma,ChelseaFinn,andChristopherDManning. An\nemulatorforfine-tuninglargelanguagemodelsusingsmalllanguagemodels. arXivpreprint\narXiv:2310.12962,2023.\n[12] AlisaLiu,XiaochuangHan,YizhongWang,YuliaTsvetkov,YejinChoi,andNoahASmith.\nTuninglanguagemodelsbyproxy. arXivpreprintarXiv:2401.08565,2024.\n[13] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang,\nZhifengChen,Heng-TzeCheng,MichaelCollins,TrevorStrohman,etal. Controlleddecoding\nfromlanguagemodels. arXivpreprintarXiv:2310.17022,2023.\n[14] MinbeomKim,HwanheeLee,KangMinYoo,JoonsukPark,HwaranLee,andKyominJung.\nCritic-guided decoding for controlled text generation. In Findings of the Association for\nComputationalLinguistics: ACL2023,pages4598–4612,2023.\n[15] James Y Huang, Sailik Sengupta, Daniele Bonadiman, Yi-an Lai, Arshit Gupta, Nikolaos\nPappas,SaabMansour,KatrinKirchoff,andDanRoth. Deal: Decoding-timealignmentfor\nlargelanguagemodels. arXivpreprintarXiv:2402.06147,2024.\n11\n[16] LeoGao,JohnSchulman,andJacobHilton. Scalinglawsforrewardmodeloveroptimization.\nInInternationalConferenceonMachineLearning,pages10835–10866.PMLR,2023.\n[17] Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D’Amour, Jacob Eisenstein,\nChiragNagpal,andAnandaTheerthaSuresh. Theoreticalguaranteesonthebest-of-nalignment\npolicy. arXivpreprintarXiv:2401.01879,2024.\n[18] JiachengLiu,AndrewCohen,RamakanthPasunuru,YejinChoi,HannanehHajishirzi,andAsli\nCelikyilmaz. Makingppoevenbetter: Value-guidedmonte-carlotreesearchdecoding. arXiv\npreprintarXiv:2309.15028,2023.\n[19] XidongFeng,ZiyuWan,MuningWen,YingWen,WeinanZhang,andJunWang. Alphazero-\nlike tree-search can guide large language model decoding and training. arXiv preprint\narXiv:2309.17179,2023.\n[20] RafaelRafailov,JoeyHejna,RyanPark,andChelseaFinn. Fromrtoq∗: Yourlanguagemodel\nissecretlyaq-function. arXivpreprintarXiv:2404.12358,2024.\n[21] CollinBurns,PavelIzmailov,JanHendrikKirchner,BowenBaker,LeoGao,LeopoldAschen-\nbrenner,YiningChen,AdrienEcoffet,ManasJoglekar,JanLeike,etal. Weak-to-stronggener-\nalization: Elicitingstrongcapabilitieswithweaksupervision. arXivpreprintarXiv:2312.09390,\n2023.\n[22] AndrewL.Maas,RaymondE.Daly,PeterT.Pham,DanHuang,AndrewY.Ng,andChristopher\nPotts. Learningwordvectorsforsentimentanalysis. InProceedingsofthe49thAnnualMeeting\nof the Association for Computational Linguistics: Human Language Technologies, pages\n142–150,Portland,Oregon,USA,June2011.AssociationforComputationalLinguistics.\n[23] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguagemodelsareunsupervisedmultitasklearners.\n[24] AI@Meta. Llama3modelcard. 2024.\n[25] YannDubois,BalázsGalambosi,PercyLiang,andTatsunoriBHashimoto. Length-controlled\nalpacaeval: Asimplewaytodebiasautomaticevaluators. arXivpreprintarXiv:2404.04475,\n2024.\n[26] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,Adam\nRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. Palm:\nScalinglanguagemodelingwithpathways. JournalofMachineLearningResearch,24(240):1–\n113,2023.\n[27] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,\nArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare\nfew-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.\n[28] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-\nthéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open\nandefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.\n[29] YuntaoBai,SauravKadavath,SandipanKundu,AmandaAskell,JacksonKernion,AndyJones,\nAnnaChen, AnnaGoldie, AzaliaMirhoseini, CameronMcKinnon, etal. Constitutionalai:\nHarmlessnessfromaifeedback. arXivpreprintarXiv:2212.08073,2022.\n[30] Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and\nYuQiao. Beyondone-preference-for-all: Multi-objectivedirectpreferenceoptimization. arXiv\npreprintarXiv:2310.03708,2023.\n[31] AlexandreRame,GuillaumeCouairon,CorentinDancette,Jean-BaptisteGaya,MustafaShukor,\nLaure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by\ninterpolatingweightsfine-tunedondiverserewards.AdvancesinNeuralInformationProcessing\nSystems,36,2024.\n12\n[32] JoelJang,SeungoneKim,BillYuchenLin,YizhongWang,JackHessel,LukeZettlemoyer,\nHannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized soups: Per-\nsonalized large language model alignment via post-hoc parameter merging. arXiv preprint\narXiv:2310.11564,2023.\n[33] HaoxiangWang,YongLin,WeiXiong,RuiYang,ShizheDiao,ShuangQiu,HanZhao,and\nTongZhang. Arithmeticcontrolofllmsfordiverseuserpreferences: Directionalpreference\nalignmentwithmulti-objectiverewards. arXivpreprintarXiv:2402.18571,2024.\n[34] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A\nSmith,andYejinChoi. Dexperts: Decoding-timecontrolledtextgenerationwithexpertsand\nanti-experts. arXivpreprintarXiv:2105.03023,2021.\n[35] ZhanhuiZhou,JieLiu,ZhichenDong,JiahengLiu,ChaoYang,WanliOuyang,andYuQiao.\nEmulated disalignment: Safety alignment for large language models may backfire! arXiv\npreprintarXiv:2402.12343,2024.\n[36] YaoZhao,RishabhJoshi,TianqiLiu,MishaKhalman,MohammadSaleh,andPeterJLiu. Slic-\nhf: Sequencelikelihoodcalibrationwithhumanfeedback. arXivpreprintarXiv:2305.10425,\n2023.\n[37] KawinEthayarajh, WinnieXu, NiklasMuennighoff, DanJurafsky, andDouweKiela. Kto:\nModelalignmentasprospecttheoreticoptimization. arXivpreprintarXiv:2402.01306,2024.\n[38] XueBinPeng,AviralKumar,GraceZhang,andSergeyLevine. Advantage-weightedregression:\nSimpleandscalableoff-policyreinforcementlearning. arXivpreprintarXiv:1910.00177,2019.\n[39] AshvinNair,AbhishekGupta,MurtazaDalal,andSergeyLevine. Awac: Acceleratingonline\nreinforcementlearningwithofflinedatasets. arXivpreprintarXiv:2006.09359,2020.\n[40] BrianDZiebart,AndrewLMaas,JAndrewBagnell,AnindKDey,etal. Maximumentropy\ninversereinforcementlearning. InAaai,volume8,pages1433–1438.Chicago,IL,USA,2008.\n[41] SergeyLevine. Reinforcementlearningandcontrolasprobabilisticinference: Tutorialand\nreview. arXivpreprintarXiv:1805.00909,2018.\n[42] NathanLambert,ValentinaPyatkin,JacobMorrison,LJMiranda,BillYuchenLin,Khyathi\nChandu,NouhaDziri,SachinKumar,TomZick,YejinChoi,etal. Rewardbench: Evaluating\nrewardmodelsforlanguagemodeling. arXivpreprintarXiv:2403.13787,2024.\n[43] RalphAllanBradleyandMiltonE.Terry. Rankanalysisofincompleteblockdesigns: I.the\nmethodofpairedcomparisons. Biometrika,39(3/4):324–345,1952.\n[44] DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,LaurentSifre,GeorgeVanDenDriess-\nche,JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,etal. Mas-\nteringthegameofgowithdeepneuralnetworksandtreesearch. nature,529(7587):484–489,\n2016.\n[45] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big\nsequencemodelingproblem. Advancesinneuralinformationprocessingsystems,34:1273–\n1286,2021.\n[46] HunterLightman,VineetKosaraju,YuraBurda,HarriEdwards,BowenBaker,TeddyLee,Jan\nLeike,JohnSchulman,IlyaSutskever,andKarlCobbe. Let’sverifystepbystep. arXivpreprint\narXiv:2305.20050,2023.\n[47] RalphAllanBradleyandMiltonE.Terry. Rankanalysisofincompleteblockdesigns: I.the\nmethodofpairedcomparisons. Biometrika,39(3/4):324–345,1952.\n[48] JiamingJi,BoyuanChen,HantaoLou,DonghaiHong,BorongZhang,XuehaiPan,JuntaoDai,\nandYaodongYang. Aligner: Achievingefficientalignmentthroughweak-to-strongcorrection.\narXivpreprintarXiv:2402.02416,2024.\n[49] EricZelikman,YuhuaiWu,JesseMu,andNoahGoodman. Star: Bootstrappingreasoningwith\nreasoning. AdvancesinNeuralInformationProcessingSystems,35:15476–15488,2022.\n13\n[50] YannDubois,ChenXuechenLi,RohanTaori,TianyiZhang,IshaanGulrajani,JimmyBa,Carlos\nGuestrin,PercySLiang,andTatsunoriBHashimoto. Alpacafarm: Asimulationframeworkfor\nmethodsthatlearnfromhumanfeedback. AdvancesinNeuralInformationProcessingSystems,\n36,2024.\n[51] BaptisteRoziere,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,\nYossiAdi,JingyuLiu,TalRemez,JérémyRapin,etal. Codellama: Openfoundationmodels\nforcode. arXivpreprintarXiv:2308.12950,2023.\n[52] WenkaiYang,ShiqiShen,GuangyaoShen,ZhiGong,andYankaiLin. Super(ficial)-alignment:\nStrong models may deceive weak models in weak-to-strong generalization. arXiv preprint\narXiv:2406.11431,2024.\n[53] AndrewL.Maas,RaymondE.Daly,PeterT.Pham,DanHuang,AndrewY.Ng,andChristopher\nPotts. Learningwordvectorsforsentimentanalysis. InDekangLin,YujiMatsumoto,andRada\nMihalcea,editors,Proceedingsofthe49thAnnualMeetingoftheAssociationforComputational\nLinguistics: HumanLanguageTechnologies, pages142–150, Portland, Oregon, USA,June\n2011.AssociationforComputationalLinguistics.\n[54] HamishIvison,YizhongWang,ValentinaPyatkin,NathanLambert,MatthewPeters,Pradeep\nDasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. Camels in a changing\nclimate: Enhancinglmadaptationwithtulu2. arXivpreprintarXiv:2311.10702,2023.\n[55] GanquCui,LifanYuan,NingDing,GuanmingYao,WeiZhu,YuanNi,GuotongXie,Zhiyuan\nLiu,andMaosongSun. Ultrafeedback: Boostinglanguagemodelswithhigh-qualityfeedback.\narXivpreprintarXiv:2310.01377,2023.\n[56] BanghuaZhu,EvanFrick,TianhaoWu,HanlinZhu,andJiantaoJiao. Starling-7b: Improving\nllmhelpfulness&harmlessnesswithrlaif,November2023.\n[57] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximal\npolicyoptimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.\n14\nA MathematicalDerivationsforEq.6\nAsintroducedinSection3,thealignmentobjectiveunderthetraditionalcontextualbanditframingis\ngivenby(Eq.2):\nargmaxE [r(x,y) βD (π(y x) π (y x))], (7)\nx∼p(x),y∼π(y|x) KL ref\nπ − | ∥ |\nwherep(x)isadistributionofprompts,yisthecompletelanguagemodelresponse,risareward\nfunctionthatencourageshuman-preferredresponses,andD limitshowfartheoptimizedlanguage\nKL\nmodelπcandeviatefromthereferencemodelπ .\nref\nGiventheautoregressivenatureoflanguagemodels,wecanframelanguagemodelalignmentas\nsolvingatoken-levelMDP[20]. Thistoken-levelMDPisdefinebythetuple( , ,f,r(s ,a )).\nt t\nS A\nHere, the state s := (x,y ) consists of the prompt and all response tokens generated so\nt <t\n∈ S\nfar;theactiona:=y determinesthenexttokentogeneratefromthevocabulary ;thedynamics\nt\nA\nf is a deterministic function that updates the state by concatenating the current state and action\ns :=(s ,a );r(s ,a )isasparserewardthatequalsr(x,y)ifa isEOSand0otherwise. Weuse\nt+1 t t t t t\nρ (s )todenotethestatemarginalsofthetrajectorydistributioninducedbythepolicyπ. Underthe\nπ t\ntoken-levelMDP,theobjectivefromEq.7canbewrittenas\n \nT\n(cid:88)\nmax E r(s ,a )+βlogπ (a s )+β (π(a s )), (8)\nπ st∼ρπ(st),at∼π(at|st) t t (cid:124) ref t | t (cid:123)(cid:122) H t | t (cid:125)\nt=1 −βD KL[π(at|st)∥πref(at|st)]\nwhereT specifiestheresponselength. ThesolutionofEq.8isgivenby[40]as:\n(cid:18) (cid:19)\n1\nπ∗(a s )=exp (Q∗(s ,a ) V∗(s )) , (9)\nt t t t t\n| β −\nwheretheoptimalQ-functionandV-functionsatisfies\nQ∗(s ,a )=r(s ,a )+βlogπ (a s )+V∗(s ), (10)\nt t t t ref t t t+1\n|\nT\n(cid:88)\nV∗(s )= E [r(s ,a )+βlogπ (a s )+β (π∗(a s ))]. (11)\nt si∼ρπ∗(si|st),ai∼π∗(ai|si) i i ref i\n|\ni\nH\ni\n|\ni\ni=t\nHere, V∗ predicts the expected future return (the terminal reward in this sparse reward setting)\npenalizedwiththefutureKLconstraintstartingfromthestates ,undertheoptimallanguagemodel\nt\nπ∗. CombiningEq.9andEq.10,wehave\nπ∗(a s )\nβlog t | t =r(s ,a )+V∗(s ) V∗(s ). (12)\nπ (a s ) t t t+1 − t\nref t t\n|\nNotethat(1)r(s ,a )isasparserewardthatisnon-zeroifa isEOS,and(2)V∗(s )=0ifa is\nt t t t+1 t\nEOS.Then,summingEq.12fromtimestep1toH yield\n(cid:88)H\nβlog\nπ∗(a\nt\n|s t) =(cid:26) −V∗(s 1)+V∗((s H,a H)) ifa\nH\nisnotEOS\n(13)\nπ (a s ) V∗(s )+r(s ,a ) ifa isEOS,\nref t t 1 H H H\nt=1 | −\nwhereV∗(s ) = V∗((s ,a ))duetothedeterministictransition. Now,returningbacktothe\nH+1 H H\nsequence-levelMDP(Section4),wherewedefineyasacompleteresponse,y′ asaresponsethat\ncanbeeithercompleteorincomplete,wehavethats = (x,y ) = (x,∅) = xand(s ,a ) =\n1 <1 H H\n((x,y ),y )=(x,y ). Thus,wecanrewriteEq.13,withaslightabuseofnotations,as\n<H H <H+1\n(cid:40)\nπ∗(y′ x) V∗(x)+V∗(x,y′) ify′ =EOS(y′isincomplete)\nlog | − |y′| ̸ (14)\nπ (y′ x) ∝ V∗(x)+r(x,y′) ify′ =EOS(y′iscomplete).\nref | − |y′|\n15\nB FurtherDetailsontheExperimentalSetup\nB.1 Controlled-SentimentGeneration&Summarization\nB.1.1 ModelSpecification\nThefollowingtableliststhemodelsandtheircorrespondinglinks.\nModels Links\ngpt2(124M)[23] https://huggingface.co/openai-community/gpt2\ngpt2-large(774M)[23] https://huggingface.co/openai-community/gpt2-large\ngpt2-xl(1.5B)[23] https://huggingface.co/openai-community/gpt2-xl\nLlama-2-7b[7] https://huggingface.co/meta-llama/Llama-2-7b-hf\nLlama-3-8B[24] https://huggingface.co/meta-llama/Meta-Llama-3-8B\nB.1.2 HyperparametersSpecification\nWeusefixedhyperparametersacrossalltestedmodels. WeusetemperatureT = 0.7,top-k = 50\nandtop-p=1.0whensamplingfromthelanguagemodels. Forweak-to-strongsearch(CBS),we\nuseW,K,L=4,4,5(W: beamwidth,K: successorsperstate,L: chunklength). ForBoN,weuse\nN =16forfaircomputationalcomparisonwithweak-to-strongsearch(i.e.,WK =N). ForEFT,\nwereportthebestresultsamongβ 1/4,1/2,1,2,4 .\n∈{ }\nB.1.3 ComputeResourcesSpecification\nModelsareevaluatedover1000testprompts,ononesingleNVIDIAA100GPU.\nB.1.4 GoldRewardModelDetails\nWefollowthesyntheticsetupinwhichweusethegoldrewardmodelstoplaytherolesofhumans\nandprovidebinarypreferencelabels[16,46,4].\nForcontrolled-sentimentgeneration,wereusethepubliclyavailabledistilbert-imdbtodefine\nthegoldrewardmodelr . Distilbert-imdbisafine-tunedclassifierpontheimdbdataset[53]\ngold\nto classify movie review sentiments. We define the gold reward r as logp(positive x,y)\ngold\n| −\nlogp(negative x,y) to encourage positive review. Synthetic preferences are collected using the\n|\ntruncated movie reviews as prompts x, and pairwise completions from gpt2-imdb, ranked with\np(y y x)=σ(r (x,y ) r (x,y )),aspreferences.\n1 2 gold 1 gold 2\n≻ | −\nForsummarization,wefitarewardmodelonthesummarize_from_feedbackdataset[2]asthe\ngoldrewardmodelr . Specifically,thisrewardmodelisfine-tunedfromLlama-2-7bwithalinear\ngold\nprojectionheadandbinarycrossentropyloss,usingabatchsizeof32,alearningrateof1e-5forthe\nprojectionhead,and5e-6forotherparameters,overoneepochwithacosinelearningrateschedule.\nSyntheticpreferencesaregeneratedbyrelabelingpairwiseresponsesintheoriginaldatasetwith\np(y y x)=σ(r (x,y ) r (x,y )).\n1 2 gold 1 gold 2\n≻ | −\nBothgoldrewardmodelsshowhighvalidationaccuracies,0.928and0.736,demonstratingstrong\ncorrelationwithhumanjudgments.\nB.1.5 DirectTuningDetails\nDirecttuningonthesyntheticpreferences = (x,y ,y ) N involvestwostages: Supervised\nD { w l i }i=1\nFine-Tuning(SFT)andDirectPreferenceOptimization(DPO)[4]. DuringSFT,modelsaretrainedon\nbothselectedandrejectedresponsesusingabatchsizeof64,alearningrateof2e-5,andacosine\nlearningratescheduleoveroneepoch. DuringDPO,weuseaβ =0.1,batchsizeof256,alearning\nrateof1e-6,andacosinelearningratescheduleoveroneepoch.\nB.1.6 PromptTemplateforSamplingfromBaseModels\nWhensamplingfromlargepre-trainedmodels,itiscrucialtoprovidecleartask-specificinstructions.\nForsentiment-controlledgeneration,weuseazero-shotprompt:\n16\nHere is a movie review from imdb: {prompt}\nForsummarization,weuseatwo-shotprompt(theexemplarsareselectedarbitrarily):\n{examplar[1].prompt}TL;DR: {examplar[1].response}\n{examplar[2].prompt}TL;DR: {examplar[2].response}\n{prompt}TL;DR:\nB.2 InstructionFollowing\nB.2.1 ModelSpecification\nThefollowingtableliststhemodelsandtheircorrespondinglinks.\nModels Links\nzephyr-7b-beta[9] https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\nmistral-7b-sft-beta[9] https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta\ntulu-2-dpo-7b[54] https://huggingface.co/allenai/tulu-2-dpo-7b\ntulu-2-7b[54] https://huggingface.co/allenai/tulu-2-7b\nLlama-2-7b-chat[7] https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nLlama-2-70b-chat[7] https://huggingface.co/meta-llama/Llama-2-70b-chat-hf\nLlama-3-8B-Instruct[24] https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\nLlama-3-70B-Instruct[24] https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\ngpt-3.5-turbo-instruct[3] https://platform.openai.com/docs/models/gpt-3-5-turbo\nB.2.2 HyperparametersSpecification\nWhensamplingfromLlama-3-8B-InstructandLlama-3-70B-Instruct,weusetheT =0.6,\ntop-k=1.0andtop-p=0.9aspertheofficialLlama-3generationconfiguration. Forothermodels,\nwedefaulttotemperatureT =0.7,top-k=50andtop-p=1.0. Specifichyperparametersforeach\nmethodaredetailedinTables3and4.\nB.2.3 ComputeResourcesSpecification\nModelsareevaluatedon805testprompts. ModelinferencetakesplaceononesingleNVIDIAA100\nGPUfor7B&8Bandblack-boxmodels,andonfourNVIDIAA100GPUsfor70Bmodels.\nC ExtendedExperimentalResults\nC.1 Chunk-levelBeamSearchAblations\nWeshowtheextendedCBShyperparameters(W,K,L)ablationsinFigures8and7.\ngpt2-large\n5.0 2.0\ngpt2-xl\n1.5\nLlama-2-7b\n4.5\nLlama-3-7b 1.0\n4.0 0.5\n0.0 gpt2-large\n3.5 gpt2-xl\n0.5\n− Llama-2-7b\n3.0 −1.0 Llama-3-7b\n1.5\n−\n0 10 20 30 40 50 0 10 20 30 40 50\nchunklength(L) chunklength(L)\n(a)controlled-sentimentgeneration (b)summarization\nFigure7: LablationsforCBS(W,K=4,4). Weshowthemeanrewards( standarddeviations).\n±\n17\nMRdloG MRdloG\ngpt2-large gpt2-xl Llama-2-7b Llama-3-8B\n16 4.89 16 4.61 16 4.09 16 4.20\n8 4.64 4.87 8 4.31 4.62 8 3.91 4.12 8 4.05 4.25\n4 4.29 4.60 4.84 4 3.85 4.30 4.52 4 3.49 3.87 4.05 4 3.76 4.04 4.20\n2 3.55 4.19 4.45 4.63 2 3.13 3.75 4.10 4.36 2 2.81 3.24 3.63 3.75 2 3.04 3.70 3.93 4.02\n1 2.13 2.89 3.46 3.76 4.04 1 1.77 2.43 2.96 3.35 3.57 1 1.86 2.26 2.65 2.91 3.12 1 1.92 2.49 2.89 3.10 3.16\n1 2 4 8 16 1 2 4 8 16 1 2 4 8 16 1 2 4 8 16\nbeamwidth(W) beamwidth(W) beamwidth(W) beamwidth(W)\n(a)controlled-sentimentgeneration\ngpt2-large gpt2-xl Llama-2-7b Llama-3-8B\n16 -0.73 16 -0.20 16 1.35 16 1.68\n8 -0.52 -0.15 8 0.02 0.22 8 1.49 1.52 8 1.95 1.93\n4 -0.58 0.03 0.27 4 0.15 0.56 0.63 4 1.50 1.57 1.61 4 1.95 2.08 2.01\n2 -0.90 -0.21 0.45 0.73 2 -0.11 0.42 0.81 1.01 2 1.24 1.56 1.63 1.65 2 1.73 2.00 2.06 1.98\n1 -1.70 -1.30 -0.99 -0.87 -0.76 1 -0.87 -0.44 -0.29 -0.22 -0.22 1 0.65 0.80 0.90 1.02 1.08 1 1.14 1.18 1.30 1.39 1.46\n1 2 4 8 16 1 2 4 8 16 1 2 4 8 16 1 2 4 8 16\nbeamwidth(W) beamwidth(W) beamwidth(W) beamwidth(W)\n(b)summarization\nFigure8: W,KablationsforCBS(L=5). Weshowthemeanrewardsacrossthreerandomseeds.\nWiththesamecomputationbudget(i.e.,sameWK),theoptimalhyperparametersdifferbytasks.\nC.2 EvaluationResultsforInstructionFollowing\nInadditiontogpt-4-turboevaluations,weassesslanguagemodelresponsesusingtwotop-ranking\nrewardmodelsfromRewardBench[42]: UltraRM-13b[55]andStarling-RM-34B[56]. Table3\nand4showthatweak-to-strongsearchconsistentlyoutperformothermethodsacrossallmetrics. We\nalsotestvanillabeamsearchwithoutanyexternalguidance[20],whichdoesnotconsistentlyimprove\noverdirectsamplingforinstructionfollowing(Table2).\nModels LCWR(%) WR(%) URM( ) SRM( )\n↑ ↑\nLlama-2-7b-chat 10.08 10.30 1.183 -5.849\nw/beamsearch(16) 10.23 10.07 1.141 5.935\n−\nLlama-2-70b-chat 16.18 14.98 1.902 -5.641\nw/beamsearch(4) 16.01 14.54 1.873 5.655\n−\nLlama-3-8B-Instruct 22.92 22.57 2.682 -5.156\nw/beamsearch(16) 21.93 21.66 2.432 5.655\n−\nLlama-3-70B-Instruct 34.42 32.18 3.833 -4.674\nw/beamsearch(4) 34.91 35.07 3.678 4.754\n−\nTable2: Vanillabeamsearch[20],withoutexternalguidance,showslimitedimprovementsover\nregulardecoding. ‘w/beamsearch(16)’denotesbeamsearchwithabeamwidthof16. LCWR\nandWRdenotelength-controlledandrawwinratesagainstgpt-4-turbo;URMandSRMdenote\nscoresbyUltraRM-13b[55]andStarling-RM-34B[56].\n18\n)K(etatsrepsrosseccus\n)K(etatsrepsrosseccus\nModels LCWR(%) WR(%) URM( ) SRM( )\n↑ ↑\nweaksupervision\nzephyr-7b-beta(π∗) 13.20 11.00 1.138 6.143\n−\nmistral-7b-sft-beta(π ) 7.54 4.77 1.274 7.618\nref\n− −\nsamevocabulary\nLlama-2-7b-chat\nBase(π ) 10.08 10.30 1.183 5.849\nbase\n−\nEFT(β∗ = 0.25) 9.05 10.08 1.244 5.819\n−\nBoN(16) 13.11 13.23 1.650 5.738\n−\nWeak-to-strongsearch(4,4,30) 13.65 14.11 2.234 -5.424\nLlama-2-70b-chat\nBase(π ) 16.18 14.98 1.902 5.641\nbase\n−\nEFT(β∗ = 0.25) 15.54 14.45 1.905 5.581\n−\nBoN(4) 17.57 16.91 2.061 5.576\n−\nWeak-to-strongsearch(2,2,30) 19.10 18.14 2.290 -5.425\ncrossvocabulary\nLlama-3-8B-Instruct\nBase(π ) 22.92 22.57 2.682 5.156\nbase\n−\nEFT(β∗) NA NA NA NA\nBoN(16) 25.35 24.32 3.000 5.070\n−\nWeak-to-strongsearch(4,4,30) 27.17 27.43 3.407 -4.862\nLlama-3-70B-Instruct\nBase(π ) 34.42 32.18 3.833 4.674\nbase\n−\nEFT(β∗) NA NA NA NA\nBoN(4) 36.60 36.38 3.869 4.676\n−\nWeak-to-strongsearch(2,2,30) 37.92 38.43 4.019 -4.616\nblackbox\ngpt-3.5-turbo-instruct\nBase(π ) 16.00 10.58 0.771 6.556\nbase\n−\nEFT(β∗) NA NA NA NA\nBoN(4) 19.59 12.51 1.017 6.455\n−\nWeak-to-strongsearch(2,2,100) 20.07 12.61 1.212 -6.391\nTable3: InstructionfollowingperformanceundertheZephyrguidance. EFT(β∗)denotesthe\nbestEFTresultsamongβ 0.25,0.5,1,1.5 ;Weak-to-strongsearch(2,2,30)denotesCBSwith\n∈{ }\nW = 2,K = 2,L = 30. LC WR and WR denote length-controlled and raw win rates against\ngpt-4-turbo;URMandSRMdenotescoresbyUltraRM-13b[55]andStarling-RM-34B[56].\n19\nModels LCWR(%) WR(%) URM( ) SRM( )\n↑ ↑\nweaksupervision\ntulu-2-dpo-7b(π∗) 9.46 8.10 0.743 6.310\n−\ntulu-2-7b(π ) 9.03 5.38 1.070 7.362\nref\n− −\nsamevocabulary\nLlama-2-7b-chat\nBase(π ) 10.08 10.30 1.183 5.849\nbase\n−\nEFT(β∗ = 1) 10.07 11.63 1.924 5.535\n−\nBoN(16) 11.60 11.67 1.536 5.721\n−\nWeak-to-strongsearch(4,4,30) 13.16 14.20 2.115 -5.451\nLlama-2-70b-chat\nBase(π ) 16.18 14.98 1.902 5.641\nbase\n−\nEFT(β∗ = 1) 16.58 16.85 2.370 -5.381\nBoN(4) 16.73 15.99 2.145 5.515\n−\nWeak-to-strongsearch(2,2,30) 19.04 18.15 2.300 -5.438\ncrossvocabulary\nLlama-3-8B-Instruct\nBase(π ) 22.92 22.57 2.682 5.156\nbase\n−\nEFT(β∗) NA NA NA NA\nBoN(16) 22.42 22.54 3.039 5.020\n−\nWeak-to-strongsearch(4,4,30) 25.96 26.73 3.431 -4.859\nLlama-3-70B-Instruct\nBase(π ) 34.42 32.18 3.833 4.674\nbase\n−\nEFT(β∗) NA NA NA NA\nBoN(4) 35.96 36.43 3.876 4.668\n−\nWeak-to-strongsearch(2,2,30) 39.09 39.81 4.068 -4.583\nblackbox\ngpt-3.5-turbo-instruct\nBase(π ) 16.00 10.58 0.771 6.556\nbase\n−\nEFT(β∗) NA NA NA NA\nBoN(4) 18.60 13.15 1.202 6.327\n−\nWeak-to-strongsearch(2,2,100) 19.80 13.23 1.285 -6.295\nTable 4: Instruction following performance under the Tulu guidance. EFT (β∗) denotes the\nbestEFTresultsamongβ 0.25,0.5,1,1.5 ;Weak-to-strongsearch(2,2,30)denotesCBSwith\n∈{ }\nW = 2,K = 2,L = 30. LC WR and WR denote length-controlled and raw win rates against\ngpt-4-turbo;URMandSRMdenotescoresbyUltraRM-13b[55]andStarling-RM-34B[56].\n20\nC.3 RLFine-tuningwithLanguageModelReward\nInSection5,wehavedemonstratedthatbyconvertingweaklanguagemodelstoaper-tokendense\nrewardfunction(Eq.4),wecanfurtherimprovestrongmodelsattesttimewithoutadditionaltraining.\nThissectionpresentspreliminaryexperimentsshowingthatthisreparametrizeddenserewardfunction\ncanalsobenefitRLfine-tuning. UsingthesamesetupfromSection5,westartwithadensereward\nfunctionparametrizedbytunedandunturnedgpt2pair(π∗,π )andthenwetunetwolargermodels\nref\nunder this dense reward on sentiment-controlled generation using PPO [57]. Figure 9 shows the\nfine-tuning results with different reward sparsity, where we find that dense reward function does\nbenefitRLfine-tuning,andweak-to-stronggeneralization[21]alsooccursconsistentlyduringtraining\nwhentherewardsignalcomesfromweakerlanguagemodels(notethatπ∗onlyachievea4.6gold\nrewardonsentiment-controlledgenerationfromFigure3).\n(a)PPOfine-tuningresultsforgpt2-large.\n(b)PPOfine-tuningresultsforLlama-2-7bw/LoRA(r=128,α=128).\nFigure9: PPOfine-tuningwithchunk-leveldenserewardsforcontrolled-sentimentgeneration.\nUsingdenserewardparameterizedbytunedanduntunedgpt2models(seeSection5fordetails),we\ntrainlargerbasemodelswithPPO.ThechunklengthLcontrolstherewardsparsity. Forexample,\nL = 5 means rewards are accumulated and emitted every 5 tokens (delayed to the last token of\neachchunk)whileL=infcorrespondstovanillaPPOwithsequence-levelsparserewards. Denser\nrewardsfacilitatecreditassignmentandacceleratetraining,improvingboththeachievedreturn\nontrainingprompts(cumulativedenserewardsoverthecompleteresponses)(left)andthe\nachievedgoldrewardonvalidationprompts(right). Weplotmean stdoverthreerandomseeds.\n±\n21\nD SampleGenerations\nD.1 Controlled-SentimentGenerationSampleGenerations\nPrompt\nWhileIdon’tconsidermyself\nweak supervision\nGPT2-DPO\nafanofthe¨realh¨orrorgenre,Ienjoyedthismoviealot. Ifyouhaveseenhorrormovies,Iwould\nrecommenditforthoseinterestedinthegenre. However,Idorecommendthatyoutakealook\n[truncatedforbrevity] 4.54\nGPT2-SFT\namoviefan,Ididsearchforthetrailerforthismovieandcouldn’tfindit. Iwaswonderingif\nanyonehadseenit. Imighthavetowatchthemovie,butI’mnotsureifIwantto. I’m[truncated\nforbrevity] 3.26\n−\ngpt2-large\nBase\na classic horror fan, reading this book has made me feel rather old. I’m not sure if I’m a fan\nof Lovecraft’s work or not, but this book has made me want to read all his stories. I would\nrecommendthisbookto[truncatedforbrevity] 1.96\nEFT(β∗ =1/4)\nagreatcomedyfan,Idoenjoyagoodcomedy. Ihaveseenmanyfilmsovertheyearsandhave\nfoundthatalotofthebestcomedieshavebeencomediesthatIenjoyedasachild. Ithinkthatthis\nisoneofthose[truncatedforbrevity] 4.85\nBoN(16)\na classic horror fan, reading this book has made me feel rather old. I’m not sure if I’m a fan\nof Lovecraft’s work or not, but this book has made me want to read all his stories. I would\nrecommendthisbookto[truncatedforbrevity] 5.03\nWeak-to-strongsearch(4,4,5)\nahugefanoftheseries,Idoliketoseeitonaregularbasis. Itisagreatexampleofashowthat\nhasitsownuniquestyleandstyleofstorytellingthatworks. Itisashowthatisverymuchawork\nin[truncatedforbrevity] 5.19\nDirectTuning\nahugefanofthegenre,Istillfindthisfilmaveryenjoyablemovie. 5.40\ngpt2-xl\nBase\nahugeMayweatherfan,Idolikethemovie. IlikeMayweather,IlikemovieswithMayweather,\nandIlikeboxers. It’sdefinitelyworthcheckingout. Thismovieisnotthatgood. It’snotthat\ngreat. It[truncatedforbrevity] 0.49\nEFT(β∗ =1/4)\nagreatcomedyfan,Idoenjoyagoodcomedyeverynowandthen. Ilovedwatchingthismovie\nbecauseitwasfunny. Thehumor,thecharacters,thestory,allofitwasveryfunny. Itisavery\nfunnyfilmandit[truncatedforbrevity] 5.29\nBoN(16)\nafanoftheoriginalStarTrek,Idooftenenjoythemovies. Ihaveseenthemall. Ihaveseenall\nthemovies,andwhileIhaveenjoyedthem,IhavenotbeenabletofindasingleonethatIwould\nrecommendto[truncatedforbrevity] 3.75\nWeak-to-strongsearch(4,4,5)\n22\na huge fan of the Godzilla series, I do enjoy watching all of the films in the franchise, and I\nenjoyedwatchingthisonewithagoodfriend. Thefilmhasaverygoodstory,withsomegreat\nactionscenes. Themoviewasverywell[truncatedforbrevity] 5.16\nDirectTuning\ntobe“old”atthispointinmylife,Istilllovethismovie. It’saveryentertainingfilm. 5.52\nLlama-2-7b\nBase\ntobeafanoftheJamesBondseries,Idolikethemovieseries. Themostrecentisafilmwith\na highly complex plot, but it is not the best of the series, and it’s not a good idea to watch it\n[truncatedforbrevity] 2.04\nEFT(β∗)\nNA\nBoN(16)\nafanofthefranchise,Ihaveseenmostofthemoviesinthisseries,soIcouldappreciatethisone\ntoacertainextent. However,I’mnotsureifitisagoodmovie. Thecharactersarenotaswell\n[truncatedforbrevity] 1.65\n−\nWeak-to-strongsearch(4,4,5)\nafanoftheoriginaltrilogy,Idothinkthisisafantasticremake. It’sessentiallythesamestory,yet\nit’sdifferentenoughtobeenjoyable. There’salsoalotofnodstothe[truncatedforbrevity] 4.89\nDirectTuning\nahugefanof¨romanticcomedies,Ïdoconsiderthisfilmtobeadelightfulmovie. WhileIcansee\nhowpeoplemightnotlikethisfilm,Icanalsoseewhypeoplewouldlikeit. Ithinkit’s[truncated\nforbrevity] 4.73\nLlama-3-7B\nBase\nanintellectual,Idoconsidermyselfafanofcerebralmovies,andthisiseasilythemostintelligent\nmovieI’veeverseen. I’mnotsureifI’dcallthisfilmuplifting,butitiswithoutadoubtinspiring\nandthought-provoking[truncatedforbrevity] 5.16\nEFT(β∗)\nNA\nBoN(16)\nahugefanofeithercomicbookorofsuperheroes,Ihavebeenveryimpressedbytheoverall\nqualityoftheMarvel’scinematicuniverse. Thismovieisagreatadditiontotheseries. Thestory\niscomplexandmulti-layered,withthecharacters[truncatedforbrevity] 5.33\nWeak-to-strongsearch(4,4,5)\nahorrorfan,IdoloveagoodstorywhichiswhyIthoroughlyenjoyedthismovie. Thestoryis\ntoldwell,verywell. Theactingisgreat,andthespecialeffectsaregreattoo. Ithoughtthestory\nwasverywelldone. [truncatedforbrevity] 5.41\nDirectTuning\nah¨orror¨fan,Ienjoyaverywidevarietyofgenres. However,itismybeliefthatfewgenrefilms\nareevertrulyh¨orrorörc¨inematic¨films;theyareallvariationsonatheme. Ithink[truncatedfor\nbrevity] 3.11\n23\nD.2 SummarizationSampleGenerations\nPrompt\nSUBREDDIT:rrelationships\nTITLE:Me[23F]withmyboyfriend[30M]of9months,hurtandIdon’tknowwhattodo.\nPOST:Sorryforthelongpost. I’mreallyhurtingrightnowandcouldusesomeadviceorwise\nwords.\nTogiveabriefbackground, myboyfriendandIhavebeendatingforabout9months. He’sa\nphysicianinanintensefellowshipprogramandgenerallyverystressedtiredbusy,I’mcurrentlyin\nschool,andstayprettybusywithaninternshipandworking. Wespendalmosteverydaytogether,\nsupporteachother,havealotoffunandbothrefertotheotherasourbestfriend(thatweenjoy\ntouchinginappropriately.)\nForthefirstcouplemonthswewereveryhappytogether. Theissuesdidn’tstartuntilIbrought\nupthedreadedquestion,S¨owhatarewe?. Thenbeganabitofastruggle. Ialwayswantedmore\n¨\nfromtherelationshipandwasgreetedbyhesitancy,by3monthsIaskedforexclusivityandhe\neventuallyagreedbutonlybecauseIwasgoingtoleaveifhedidn’t(Iwantedsomethingserious.)\nby6-7monthsIsaidIloveyou,hedidn’tsayitback. Thisistotallyfine,painful,butfine. Isaid\nI’dbepatientwithhimuntilheisready. Hestillhasn’tsaiditback,butnowat9monthshesaid\nthathefeelslikeheisfallinginlove,butisn’tinloveyet. Thisprogresshasgivenmehope,but\nnotbeingonthesamepagehasmademefeelabitinsecureaboutourrelationship.\nHehasmentionedthathedoesn’twanttofallinloveunlessheknowsthatwewillhaveaserious\nfuturetogether,sohedoesn’tgethurtlikehedidinhis2priorrelationships. :(\nWehavebeenfightingalotrecentlybecause: Istillhaven’tmethisfamily(theylive5minutes\nfrom him), he doesn’t want to share our relationship on Facebook (not the biggest deal!), he\nisn’tinlovewithme,wedon’treallytalkaboutthefuture,andthequalityoftimespenttogether\nisn’tthebestduetothenatureofhisworkandhowstressedandtiredheisallofthetime. After\nexpressingmyconcernsaboutthesethings,hesaidhedoesn’twanttolosemeandwilltrytoopen\nuptome. Thefightingisreallygettingtousboth.\nweak supervision\nGPT2-DPO\nBoyfriendof9monthsinsistsonexclusivity,refusestotalkaboutfuturebutIdon’tknowhow\ntoapproachthis. Whataremyoptionsfordealingwiththepressureheisfeelingnowandwhat\nshouldIdo? 1.05\n−\nGPT2-SFT\nIdon’tknowwhatIshoulddo,andI’mfeelinginsecureanddon’tknowwhattodo. 2.67\n−\ngpt2-large\nBase\nIlovehimandhedoesn’twanttobeinarelationshipunlessheknowswewillbetogether. Hehas\nbeenfightingrecentlybecauseI’minlovewithhim,I’mnotinlovewithhim,hewantstobeina\nrelationshipwithsomeoneotherthanme,I’mnotinlovewithhim,I’mnotinlovewithhim,I’m\nnotinlovewithhimbecauseIhaven’tmethisfamily,hedoesn’twanttoshareourrelationshipon\nFacebook,I’mnotinlovewithhim,I’mnotinlovewithhimbecausehedoesn’twanttoshare\nourrelationshiponFacebook,I’mnotin[truncatedforbrevity] 3.56\n−\nEFT(β∗ =2)\nGirlfriendhasbeenhavingfightswithboyfriendovernotsharinghisrelationshiponFacebook\nandtheyarenotinlove. Girlfriendhasexpressedherfearsabouttheirrelationshipandiswilling\ntocommittimetothem. Heistryingtoopenuptoherbecauseheisgettinghisfamilytoopen\nuptohim. Girlfriendishurting. Iwanttogivehimthebenefitofthedoubtandbelievethathe\ndoesgenuinelywanttobewithmeandwillbeopentous,butIamconcernedthatifhedoesn’t,\nthenhewon’tbeabletoopenuptomeinahealthyway. Ibelievethistobetruebecausehesaid\n[truncatedforbrevity] 2.02\n−\n24\nBoN(16)\nHedoesn’twanttoshareourrelationshiponFacebook,hedoesn’twanttogetmehurt,andthe\nqualityoftimewespendtogetherisn’tthebestduetoourworkandstress. 1.03\n−\nWeak-to-strongsearch(4,4,5)\nWehavebeenfightingbecausewehavenothadaseriousrelationshipthatwecanfeelhappyand\ninlovewithyet. Iwantmorefromtherelationshipandamveryinsecureaboutourrelationship.\nHeistryingtoopenuptomebutIdon’treallydowellwiththatapproach. Iwantmorefromour\nrelationshipandIamfeelingprettyinsecureaboutit. 0.49\nDirectTuning\nIfeellikeIhavebeenhurtbymyboyfriendslackofinterestinmyfeelingsforhim. Idon’tknow\nhowtoproceedwithourrelationshipandI’mnotsureifheisreadyforaseriousrelationship.\n0.33\ngpt2-xl\nBase\nI still haven’t met his family (they live 5 minutes from him), he doesn’t want to share our\nrelationshiponFacebook(notthebiggestdeal!),heisn’tinlovewithme,hedoesn’treallytalk\naboutthefuture,andthequalityoftimespenttogetherisn’tthebestduetothenatureofhiswork\nandhowstressedandtiredheisallofthetime. Afterexpressingmyconcernsaboutthesethings,\nhesaidhedoesn’twanttolosemeandwilltrytoopenuptome. Thefightingisreallygettingto\nusboth. 1.77\nEFT(β∗ =1)\nBoyfriendhasbeentalkingabouthislovelifeforthebetterhalfof9monthsandhasmentioned\nwantingexclusivitywithinthenext6months. WearefightingalotbecauseIstillhaven’tmethis\nfamily(theyare5minutesfromhim),hedoesn’twanttolistourrelationshiponFacebook(not\nthebiggestdeal),heisn’tinlovewithme,wedon’treallytalkaboutthefuture,andthequalityof\ntimespenttogetherisn’tthebestduetothenatureofhisworkandhowstressedandtiredheisall\nofthetime. 1.61\nBoN(16)\nBoyfriendhasbeenwithmefor9months,wehavebeenfightingaboutalotofthings,hedoesn’t\nwanttoshareourrelationshiponFacebook,heisnotinlovewithme,wedon’treallytalkabout\nthefuture,andthequalityofourtimetogetherisn’tthebest. Hehassaidhedoesn’twanttolose\nmeandwilltrytoopenuptome. 2.51\nWeak-to-strongsearch(4,4,5)\nBoyfriendisaverytalented(andreallystressedtired)physician. Hehasmentionedthathedoesn’t\nwanttofallinloveunlessheis100%surewecanhavealongtermrelationship. He’ssaidhefeels\nlikeheisfallinginlove,butisn’tinloveyet. Wearefightingalotbecausewedon’ttalkaboutthe\nfutureandhedoesn’twanttoshareourrelationshiponsocialmedia. 3.04\nDirectTuning\nMyboyfriendandIhavebeentogetherfor9monthsandI’vementionedIwantmorefromthe\nrelationship, he said he’s falling in love but isn’t in love yet, and now he won’t address my\nconcerns. 1.85\nLlama-2-7b\nBase\nHavebeendatingfor9months,boyfriendisaphysicianinanintensefellowshipprogram,weare\nbothinschool. He’shesitantaboutsayingIloveyou,andI’mgettinghurt. 1.98\nEFT(β∗)\nNA\nBoN(16)\nBoyfriendandIhavebeendatingfor9months,wearestillnotsureifwearetogetherornot. I\nlovehimbutheisworriedaboutfallinginloveandgettinghisheartbroken. WefightalotandI\namhurting:( 2.03\n25\nWeak-to-strongsearch(4,4,5)\nBeendating9months,he’sstressed,tired,andbusy. I’mmanagingabitofaninternshipand\nworking. Hedoesn’twanttofallinloveifit’snotserious. Wefightalotlatelyandhe’shaving\ntroubleopeningup. Don’tknowifwehaveafuturetogether,butIwantit,can’timaginemylife\nwithouthim. Ilovehimdearly. I’mreallyhurtandIdon’tknowhowtomoveforward. 2.80\nDirectTuning\nMy boyfriend and I have been together for 9 months and I’ve mentioned I want more from\nthe relationship, he 9 month relationship with bf, he isn’t completely in love with me, hasn’t\nintroducedmetohisfamily,wedon’ttalkaboutthefuture,andnowwearefightingalot. 2.59\nLlama-3-7B\nBase\nIwantaseriousrelationshipwithmyboyfriend. Idon’tknowhowtomoveforwardwithhim. He\nisn’tinloveanddoesn’tknowifheeverwillbe. Idon’twanttolosehim,butI’mnotsurehowto\nmakethiswork. 2.27\nEFT(β∗)\nNA\nBoN(16)\nBoyfriendandIhavebeendating9months. Iwantmorefromhimthanheiswillingtogive. We\nfightalot. Hesaysheisfallinginlovebutisn’tthereyet. Idon’tknowwhattodoaboutit. 2.67\nWeak-to-strongsearch(4,4,5)\nMe,23F,withmyboyfriend,30M,for9months.Boyfriendhasn’tsaidloveyet,Iwantexclusivity\nandtomeethisfamily. WehavebeenfightingalotsinceIbroughttheseissuesup. Ijustdon’t\nknowwhattodoanymore. 2.88\nDirectTuning\n9monthrelationship,boyfrienddoesn’tsayhelovesme,doesn’twanttopostusonFB,doesn’t\nwanttomeetmyparents,doesn’twanttoplanforthefuture(notthatIhaveanyexpectationsfor\nhim),anddoesn’tspendatonofqualitytimewithmeduetoworkstresses. Iamgettinginsecure\nandwehavebeenfightingalot. Ilovehimanddon’twanttolosehim. WhatcanIdo? I’mreally\nhopingforsomeonetogivemesomeinsightintothemalepsychehereandtellmethebestthings\nIcandotohelpthisrelationshipprogressandbeashappyaspossible 2.61\n26",
    "pdf_filename": "Weak-to-Strong_Search_Align_Large_Language_Models_via_Searching_over_Small_Language_Models.pdf"
}