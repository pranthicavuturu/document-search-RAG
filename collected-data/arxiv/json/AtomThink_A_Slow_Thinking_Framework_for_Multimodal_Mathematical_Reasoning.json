{
    "title": "AtomThink: A Slow Thinking Framework for Multimodal Mathematical",
    "abstract": "\u00009\u0000D\u0000U\u0000L\u0000D\u0000E\u0000O\u0000H \u0000*\u0000U\u0000D\u0000S\u0000K\u0000V \u0000'\u0000H\u0000I\u0000L\u0000Q\u0000L\u0000W\u0000L\u0000R\u0000Q \u0000$\u0000Q\u0000D\u0000O\u0000\\\u0000V\u0000L\u0000V In this paper, we address the challenging task of multi- modalmathematicalreasoningbyincorporatingtheability \u0000\u0014\u0000\u0013\u0000\u0013\u0000\b \u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000,\u0000P\u0000D\u0000J\u0000H \u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000'\u0000H\u0000V\u0000F\u0000U\u0000L\u0000S\u0000W\u0000L\u0000R\u0000Q of “slow thinking” into multimodal large language mod- \u00009\u0000H\u0000U\u0000L\u0000I\u0000L\u0000F\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u001b\u0000\u0013\u0000\b els (MLLMs). Contrary to existing methods that rely on \u0000\u0019\u0000\u0013\u0000\b direct or fast thinking, our key idea is to construct long \u0000\u0017\u0000\u0013\u0000\b \u0000\u0015\u0000\u0013\u0000\b chains of thought (CoT) consisting of atomic actions in \u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000.\u0000Q\u0000R\u0000Z\u0000O\u0000H\u0000G\u0000J\u0000H \u0000\u0013\u0000\b \u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000,\u0000Q\u0000W\u0000U\u0000R\u0000G\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q a step-by-step manner, guiding MLLMs to perform com- \u0000,\u0000Q\u0000I\u0000R\u0000U\u0000P\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003 \u0000(\u0000[\u0000W\u0000U\u0000D\u0000F\u0000W\u0000L\u0000R\u0000Q\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003 plex reasoning. To this end, we design a novel AtomThink framework composed of three key modules: (i) a CoT an- notation engine that automatically generates high-quality \u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000(\u0000T\u0000X\u0000D\u0000W\u0000L\u0000R\u0000Q \u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000)\u0000R\u0000U\u0000P\u0000X\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q CoT annotations to address the lack of high-quality visual \u0000&\u0000D\u0000O\u0000F\u0000X\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003 mathematical data; (ii) an atomic step fine-tuning strat- egy that jointly optimizes an MLLM and a policy reward \u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000*\u0000H\u0000R\u0000P\u0000H\u0000W\u0000U\u0000L\u0000F model (PRM) for step-wise reasoning; and (iii) four dif- \u0000$\u0000S\u0000S\u0000U\u0000R\u0000[\u0000L\u0000P\u0000D\u0000W\u0000L\u0000R\u0000Q \u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u00005\u0000H\u0000D\u0000V\u0000R\u0000Q\u0000L\u0000Q\u0000J ferent search strategies that can be applied with the PRM \u0000/\u0000/\u0000D\u00009\u0000$\u0000\u0010\u0000/\u0000O\u0000D\u0000P\u0000D\u0000\u0016\u0000\u0010\u0000\u001b\u0000% \u0000$\u0000W\u0000R\u0000P\u00007\u0000K\u0000L\u0000Q\u0000N\u0000\u0010\u0000/\u0000/\u0000D\u00009\u0000$ \u0000(\u00000\u00002\u00009\u0000$\u0000\u0010\u0000\u001b\u0000% \u0000$\u0000W\u0000R\u0000P\u00007\u0000K\u0000L\u0000Q\u0000N\u0000\u0010\u0000(\u00000\u00002\u00009\u0000$ to complete reasoning. Additionally, we propose Atom- Figure1.Atomiccapabilityevaluationofdifferentmodels.Exist- MATH,alarge-scalemultimodaldatasetoflongCoTs,and ingopen-sourcemodelsexhibitsignificantshortcomingsincapa- an atomic capability evaluation metric for mathematical bilitiessuchasvariabledefinition, approximationandimagede- tasks. Extensive experimental results show that the pro- scription. posedAtomThinksignificantlyimprovestheperformanceof baselineMLLMs,achievingapproximately50%relativeac- curacy gains on MathVista and 120% on MathVerse. To lemsandcodewriting[44]. supporttheadvancementofmultimodalslow-thinkingmod- While CoT-based methods show clear improvements els, we will make our code and dataset publicly available over direct predictions, they still rely heavily on greedy onhttps://github.com/Quinn777/AtomThink. decoding strategies. More recently, the introduction of OpenAI’s o1 [23] marks a substantial advancement in the ability of artificial intelligence systems to perform high- 1.Introduction level reasoning. Unlike traditional models, o1 excels in solving complex problems by utilizing extended reason- Chain-of-thought (CoT) reasoning [34] has provided a ingchainsandadoptingtest-timescaling,i.e.,“slowthink- novel scheme for large language models (LLMs) to tackle ing”. In addition to o1, several concurrent works have ex- complex reasoning tasks. By utilizing a small number of plored methods for incorporating slow thinking capabili- speciallydesignedinstructions,CoTenablesLLMstogen- ties into open-source LLMs, such as Thought Trees [35] erate intermediate reasoning steps, significantly enhancing and Monte Carlo tree search (MCTS) based tree search performanceonsymbolictaskssuchasmathematicalprob- techniques [6, 25, 30, 31]. The success of o1 and its variants demonstrate that incorporating slow thinking into *Theseauthorscontributedequallytothiswork. †Correspondingauthor.Email:xdliang328@gmail.com LLMs significantly enhances their performance on com- 4202 voN 81 ]VC.sc[ 1v03911.1142:viXra",
    "body": "AtomThink: A Slow Thinking Framework for Multimodal Mathematical\nReasoning\nKunXiang1*,ZhiliLiu2∗,ZihaoJiang3∗,YunshuangNie1,RunhuiHuang4,HaoxiangFan5,\nHanhuiLi1,WeiranHuang3,YihanZeng5,JianhuaHan5,LanqingHong5,HangXu5,XiaodanLiang1†\n1 SunYat-senUniversity2 HongKongUniversityofScienceandTechnology\n3 ShanghaiJiaotongUniversity4 UniversityofHongKong5 HuaweiNoah’sArkLab\nAbstract\n\u00009\u0000D\u0000U\u0000L\u0000D\u0000E\u0000O\u0000H \u0000*\u0000U\u0000D\u0000S\u0000K\u0000V \u0000'\u0000H\u0000I\u0000L\u0000Q\u0000L\u0000W\u0000L\u0000R\u0000Q\n\u0000$\u0000Q\u0000D\u0000O\u0000\\\u0000V\u0000L\u0000V\nIn this paper, we address the challenging task of multi-\nmodalmathematicalreasoningbyincorporatingtheability \u0000\u0014\u0000\u0013\u0000\u0013\u0000\b \u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000,\u0000P\u0000D\u0000J\u0000H\n\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000'\u0000H\u0000V\u0000F\u0000U\u0000L\u0000S\u0000W\u0000L\u0000R\u0000Q of “slow thinking” into multimodal large language mod- \u00009\u0000H\u0000U\u0000L\u0000I\u0000L\u0000F\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u001b\u0000\u0013\u0000\b\nels (MLLMs). Contrary to existing methods that rely on \u0000\u0019\u0000\u0013\u0000\b\ndirect or fast thinking, our key idea is to construct long \u0000\u0017\u0000\u0013\u0000\b\n\u0000\u0015\u0000\u0013\u0000\b\nchains of thought (CoT) consisting of atomic actions in \u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000.\u0000Q\u0000R\u0000Z\u0000O\u0000H\u0000G\u0000J\u0000H\n\u0000\u0013\u0000\b \u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000,\u0000Q\u0000W\u0000U\u0000R\u0000G\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q\na step-by-step manner, guiding MLLMs to perform com- \u0000,\u0000Q\u0000I\u0000R\u0000U\u0000P\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\n\u0000(\u0000[\u0000W\u0000U\u0000D\u0000F\u0000W\u0000L\u0000R\u0000Q\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\nplex reasoning. To this end, we design a novel AtomThink\nframework composed of three key modules: (i) a CoT an-\nnotation engine that automatically generates high-quality \u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000(\u0000T\u0000X\u0000D\u0000W\u0000L\u0000R\u0000Q\n\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000)\u0000R\u0000U\u0000P\u0000X\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\nCoT annotations to address the lack of high-quality visual\n\u0000&\u0000D\u0000O\u0000F\u0000X\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\nmathematical data; (ii) an atomic step fine-tuning strat-\negy that jointly optimizes an MLLM and a policy reward\n\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000*\u0000H\u0000R\u0000P\u0000H\u0000W\u0000U\u0000L\u0000F\nmodel (PRM) for step-wise reasoning; and (iii) four dif- \u0000$\u0000S\u0000S\u0000U\u0000R\u0000[\u0000L\u0000P\u0000D\u0000W\u0000L\u0000R\u0000Q \u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u0000\u0003\u00005\u0000H\u0000D\u0000V\u0000R\u0000Q\u0000L\u0000Q\u0000J\nferent search strategies that can be applied with the PRM \u0000/\u0000/\u0000D\u00009\u0000$\u0000\u0010\u0000/\u0000O\u0000D\u0000P\u0000D\u0000\u0016\u0000\u0010\u0000\u001b\u0000% \u0000$\u0000W\u0000R\u0000P\u00007\u0000K\u0000L\u0000Q\u0000N\u0000\u0010\u0000/\u0000/\u0000D\u00009\u0000$ \u0000(\u00000\u00002\u00009\u0000$\u0000\u0010\u0000\u001b\u0000% \u0000$\u0000W\u0000R\u0000P\u00007\u0000K\u0000L\u0000Q\u0000N\u0000\u0010\u0000(\u00000\u00002\u00009\u0000$\nto complete reasoning. Additionally, we propose Atom-\nFigure1.Atomiccapabilityevaluationofdifferentmodels.Exist-\nMATH,alarge-scalemultimodaldatasetoflongCoTs,and\ningopen-sourcemodelsexhibitsignificantshortcomingsincapa-\nan atomic capability evaluation metric for mathematical\nbilitiessuchasvariabledefinition, approximationandimagede-\ntasks. Extensive experimental results show that the pro- scription.\nposedAtomThinksignificantlyimprovestheperformanceof\nbaselineMLLMs,achievingapproximately50%relativeac-\ncuracy gains on MathVista and 120% on MathVerse. To\nlemsandcodewriting[44].\nsupporttheadvancementofmultimodalslow-thinkingmod-\nWhile CoT-based methods show clear improvements\nels, we will make our code and dataset publicly available\nover direct predictions, they still rely heavily on greedy\nonhttps://github.com/Quinn777/AtomThink.\ndecoding strategies. More recently, the introduction of\nOpenAI’s o1 [23] marks a substantial advancement in the\nability of artificial intelligence systems to perform high-\n1.Introduction\nlevel reasoning. Unlike traditional models, o1 excels in\nsolving complex problems by utilizing extended reason-\nChain-of-thought (CoT) reasoning [34] has provided a\ningchainsandadoptingtest-timescaling,i.e.,“slowthink-\nnovel scheme for large language models (LLMs) to tackle\ning”. In addition to o1, several concurrent works have ex-\ncomplex reasoning tasks. By utilizing a small number of\nplored methods for incorporating slow thinking capabili-\nspeciallydesignedinstructions,CoTenablesLLMstogen-\nties into open-source LLMs, such as Thought Trees [35]\nerate intermediate reasoning steps, significantly enhancing\nand Monte Carlo tree search (MCTS) based tree search\nperformanceonsymbolictaskssuchasmathematicalprob-\ntechniques [6, 25, 30, 31]. The success of o1 and its\nvariants demonstrate that incorporating slow thinking into\n*Theseauthorscontributedequallytothiswork.\n†Correspondingauthor.Email:xdliang328@gmail.com LLMs significantly enhances their performance on com-\n4202\nvoN\n81\n]VC.sc[\n1v03911.1142:viXra\nplex, multi-step tasks, improving their overall problem- accuracy of LLaVA-Llama3-8B on MathVista and Math-\nsolvingcapabilities. Verseby9.6%and18.8%,respectively. WithEMOVA(8B)\nHowever, adopting the slow-thinking technique into as the base model, AtomThink achieved the highest accu-\nmultimodal large language models (MLLMs) is challeng- racy of 40.5% on MathVerse, surpassing the cutting-edge\ning, due to the increased data and computational resource GPT-4V.\ndemandforinformationmodelinginvisualtasks [24,42]. Insummary,ourprimarycontributionsareasfollows:\nAlthoughmanyeffortshavebeenconductedtoalleviatethis • We introduce AtomThink, a comprehensive framework\nissue,suchasincorporatinginterleavedimage-textdata [1], that guides MLLMs to focus on atomic step reason-\npromptengineering [20,26],theyarestillconfinedtostim- ing,whichobtainsconsistentperformanceimprovements\nulating the inherent CoT capabilities of MLLMs, without acrossmultiplebaselineMLLMs.\nconsidering the quality of each intermediate step in the • By designing an atomic capability evaluation based on\nreasoningchain. Hence,existingmethodsarehardtoapply outcomesupervision,werevealthecapabilitydistribution\ntest-timescalinglawstoguaranteetheirperformance. ofMLLMsingeneratingeachtypeofatomicstep.\nTo validate the importance of the quality of each inter- • A multimodal long CoT dataset specifically focused on\nmediate step in CoT, we first design a capability evalua- multimodal mathematical tasks, AtomMATH, is first in-\ntion method to perform a fine-grained quality assessment troduced.\nof each atomic step generated by MLLMs. Here we de-\nfine an atomic step as the minimal prediction node in the 2.RelatedWork\nslowthinkingprocess.Consideringthathumansmayutilize\nSlow Thinking in Multimodal Reasoning Tasks Com-\ndistinct cognitive abilities for solving mathematical prob-\nplex reasoning tasks such as mathematical computation\nlems, we utilize one of the current most advanced LLMs,\nand code programming have long been challenging for\ni.e., GPT-4o [21] to construct an ability set and estimate\nMLLMs[15,36,44]. Somepriorworkhasapproachedthis\nscores of atomic steps with outcome supervision. The re-\nissue from the perspective of prompt engineering, encour-\nsultsshowninFigure1indicatethatthestepqualityofex-\naging models to generate Chain-of-Thought(CoT), which\nisting open-source models is significantly lower than that\nis widely believed to enhance model’s reasoning [33, 34].\nofGPT-4o,particularlyinareassuchasimagerecognition,\nThey carefully modify the input distribution to enable the\nvariabledefinition,andcalculationability. Thisfindingfur-\nmodeltomimichumanstep-by-stepoutputwithoutfinetun-\nther motivates our focus on the capability gaps in existing\ning parameters. Other recent studies have explored under-\nmodels, prompting us to improve performance by enhanc-\nstandingvisualambiguitybyintroducingmulti-turnchain-\ningthequalityofatomicreasoningsteps.\nof-thoughts [20]. Shao et al. [26] have considered incor-\nTherefore, to fully leverage the advantages of CoT\nporatingadditionalvisualtokensintoCoTs,suchasobject\nand address the aforementioned challenges, we propose a\nregions and precise localization. However, due to the lack\nfull-process slow-thinking framework called AtomThink.\nofmultimodalprocesssupervisiondata,currentworkshave\nAtomThink introduces a multimodal CoT annotation en-\nnot explored reward model-based search strategies, which\ngine,anatomicstepfinetuningstrategy,andpolicysearch-\narewidelyusedinLLMs[3,12,28,29,38].\ning strategies to generate high-quality atomic steps. It\naims to enhance the decoding capabilities of MLLMs\nthrough careful training, combined with post-sampling Long CoT Annotation for Mathematical Data The in-\nsearch strategies to identify the optimal prediction nodes. troduction of slow thinking relies heavily on the availabil-\nTo begin with, the proposed annotation engine is used to ity of high-quality step-level annotations. In 2023, Light-\ncreate a novel multimodal long CoT dataset including 26k man et al. [11] constructed a process supervision dataset\nhigh-levelmathematicalproblems,157katomic-granularity composedofextensivehumanannotations,whichhasbeen\nsteps, and130kprocesssupervisionannotations. Thecon- widely used for mathematical reasoning. Recent advance-\nstruction of this dataset does not require manual labeling mentshavefocusedonautomatingthedataacquisitionpro-\nand effectively leverages existing short labels. Secondly, cess, allowing models to generate their own CoT. Tech-\nouratomicstepfinetuningstrategyappliesstep-levelmask- niques like Quiet-STaR [39] have demonstrated how self-\ning to the training set, forcing our models to learn multi- generatedreasoningcanenhancemodelperformancewith-\nturnself-dialogueabilityandgeneratereasoningfocusedon out requiring manually labels. Moreover, some methods\nindividual inference actions. Thirdly, we explore different based on Monte Carlo estimation have automated the pro-\nsearch strategies along both the path and step dimensions cessdatacollection,buttheyalsointroduceadditionalcom-\nduringtheinferencephasetofindoptimalpredictionnodes. putational cost [19, 32]. In multimodal domain, MAVIS\nTovalidatetheeffectivenessofourmethod,weconductex- [41],adatasetconsistingof834kvisualmathproblemsan-\ntensive experiments on public datasets. We improved the notated with short CoT, has been proposed. Other stud-\nies have distilled reasoning processes from short answers\n[42].However,thesemachine-generatedannotationsareof- promptingstrategyandasemantic-levelaugmentationstrat-\ntentoobriefandchallengingtosegmentsemantically. egytoproducemulti-stepreasoningpaths.\n3.Method\nDynamicPromptingStrategy. Toovercomethecompu-\nIn this section, we present the details of AtomThink for tational cost limitations associated with previous methods\npromoting MLLMs for mathematical reasoning with slow thatreliedonmanualannotationorprocesssupervision,we\nthinking. As shown in Figure 2, AtomThink consists of explore the possibility of driving existing models to au-\nthree key components, including a multimodal CoT anno- tonomously generate high-quality reasoning data through\ntationengine(Sec. 3.1),atomicstepfine-tuning(Sec. 3.2), simple prompting. Inspired by recent research [9] on us-\nand policy searching (Sec. 3.3). The annotation engine ingpromptingstrategiestoimprovethereasoningcapabil-\nis designed to efficiently generate long CoTs to address ities of LLMs, we propose a dynamic prompt strategy for\ndata scarcity. With sufficient data, we fine-tune MLLMs generatingatomicinferencesteps.Specifically,ourstrategy\nand train a process reward model (PRM) for incorporating drives LLMs to iteratively construct state-reasoning paths.\nslowthinkingabilityintomodels. Furthermore,weexplore Each path node represents a reasoning step and encom-\nfourdifferentpath-wiseandstep-wise strategies forpolicy passes the previous stage, the current state, and a possible\nsearching, allowing the fine-tuned MLLM to ensure that action. The possible action includes continuing reasoning,\neach decision made during its inference contributes to the verifying and drawing conclusion, which is determined by\noverallaccuracyandconsistencyofreasoning. Finally,we LLMitself. UnlikepreviousmethodssuchasOmegaPRM\nproposeanatomiccapabilityevaluationmetricinSec. 3.4 [19] and Math-Shepherd [32] that generate a whole rea-\ntomeasurethereasoningqualityofmodels. soning tree at once, our approach implicitly integrates the\nsearchoverthestepdimensionintoexistingreasoningpro-\nSource MetaSamples AMATH-SFT AMATH-PRM cess through prompt engineering. For each problem in-\nstance,onlyasinglevalidpathisexplored,eliminatingthe\nCLEVR 1929 11.2k 25k\nGeometry3K 1201 11.1k 15.6k needforadditionalprocesssupervisioncomputation.\nMAVIS 3654 17.7k 30.5k\nTabMWP 2463 15.7k 25.7k\nShort CoT Augmentation. To fully leverage existing\nGeomVerse 1347 9.9k 17k\nMathV360K 10157 53.5k 24.8k short CoT annotations of VQA datasets, we also employ\nMMMU 76 0.6k 1.2k LLMs to atomize and augment these annotations. An ex-\nGeoQA+ 2082 19.5k 0 ampleofshortCoTaugmentationisprovidedinthesupple-\nIconQA 3199 18.1k 0\nmental material. This approach allows us to semantically\nTotal 26108 157k 130k segmentanoriginalreasoningprocessintomultiplediscrete\nsteps,andfocusonsolvingasingleatomicproblemateach\nTable1.DatacompositionofAtomMATH.\nstageofthereasoningprocess,therebyensuringtheclarity\nandprecisionofourmodel.\nData GPTScore Avg. Length\nAtomMATH Dataset. We sample mathematical data\nPRM800k 84.1 1245.4\nfrom Geo3k [17], Mathv360k [27], MMMU-dev [37],\nDirect 1.5 3.6\nTabMWP [16], CLEVR [7], Geomverse-Cauldron [8] and\nCoT 79.6 670.5\nMAVIS[41].ForGeomverseandMAVIS,weconductshort\nAtomMATH(Ours) 89.4 849.8\nCoTaugmentation,whiletherestaregeneratedbydynamic\nTable 2. Comparison of different data styles. Our AtomMATH prompts to produce multi-step reasoning. Both short CoT\nachievesthehighestGPT-4opreferencescore. augmentationanddynamicpromptingareimplementedby\nGPT-4o in this paper. After generating long CoTs, we\nalso use GPT-4o to double-check the answers and remove\n3.1.MultimodalCoTAnnotationEngine\nrollouts with incorrect responses. To enable our model\nGuiding MLLMs toward deep reasoning requires a sub- to learn atomic step-based reasoning patterns, we progres-\nstantial amount of high-quality CoT data. However, in the sivelymaskeachnodealongitsreasoningpathtogenerate\nfield of visual mathematics, the scarcity of publicly avail- 157k atomic steps. We refer to this database as AMATH-\nable datasets presents a considerable challenge. To over- SFT.Meanwhile,wesampledapproximately65kexamples\ncome this, we develop an automated data engine capa- withcorrectstepsfromAMATH-SFT,andgeneratednega-\nble of generating step-by-step long CoTs, resulting in our tive samples using GPT-4o to serve as PRM training data.\nown atomic mathematical problem dataset, dubbed Atom- Table 1 illustrates the distribution of our data. In Table 2,\nMATH.Specifically,ourdataengineintroducesadynamic wealsoevaluatethequalityinasubsetof500AtomMATH\nMultimodal CoTAnnotation Atomic Step Finetuning Policy Searching\nImages with human solution Images with short CoT\nGeneral VQA\ninstructions with\n130k math-targeted\nshort annotations positive and Question\nnegative step pairs\n157k math-targeted\nQ: In rhombus ABCD, if AC = Q:WhatisthelengthofAC? instructions with\n8 and BD = 6, what is the length A:AB=AC*sin(30)=AC* atomic CoT Path-wise Search\nof side AB? A: 5. 0.5,thusAC=AB*2=20.\nS_0 A_0 A_1 A_2 A_3\nGPT-4o Text base PRM\nDynamic Prompting Short CoT Augmentation\nS_0 S_1 A_1 A_2 A_3\nStep 1: The image shows a rhombus ABCD…\ndiagonals bisect each other at right angles. Question\nStep 2: Diagonals bisect each other. Therefore, Atomic S_0 S_1 S_2 A_2 A_3\nAO = CO = 4 and BO = DO = 3.\nData\nStep 5: Applying the Pythagorean theorem: S_0 S_1 S_2 S_3 A_3\nAB2= AO2+ BO2= 42+ 32= 16 + 9 = 25.\nStep 6: Takethe square root of 25, thus AB = 5. MaskedQueries Step-wise Search\nFigure2.TheoverviewofAtomThinkframework.Weautomaticallyannotatetheopen-sourcedatawithCoTtogenerateatomicstepsfor\nfine-tuningandPRMtraining.Duringinference,step-wiseorpath-wisesearchingstrategiescanbeappliedtofindoptimalpolicies.\nsamples with GPT-4o. Result shows that our method ex- selectinganactionagiventhepreviousstatess asfol-\n1:t−1\nhibits longer information content than general CoT. Even lows:\ncompared to the PRM800k with golden annotations, our\ndataobtainedahigherpreferencescore. p t(a)=PRM([q,s 1:t−1],a). (1)\n3.2.AtomicStepFine-Tuning WeproposetotrainthePRMbyminimizingthefollowing\nbinarycross-entropyloss:\nTofullyexploitMLLMsforaddressingmulti-modalmathe-\nmaticalproblems,weconductfine-tuningwithatomicstep- T\n(cid:88)\nwise reasoning. Particularly, this process includes fine- L = y (a)logp (a)+(1−y (a))log(1−p (a)),\nPRM t t t t\ntuningtheMLLMonourAtomMATHdatasetandlearning\nt=1\nthePRMtoestimaterewardscoresduringtheinference. (2)\nwhere y (a) denotes the ground-truth CoT annotation that\nMLLM Fine-Tuning. To transfer MLLM to step-wise t\ny = 1 if the action a is selected, otherwise y (a) = 0.\nmathematical reasoning, we first fine-tune it within the t t\nT is the maximum number of steps. Note that we omit to\nframework of Markov decision process (MDP) learn-\nenumerate all possible actions in Eq. (2) for the concise\ning. Specifically, we consider the reasoning process of\npresentation. After selecting the action at the current step\nMLLM as an MDP, which can be formulated as M =\na , we concatenate it with the previous states to construct\n(V,S,A,R,π). Here, V denotes the vocabulary, S rep- t\ns ,i.e.,s =s ∪a .\nresents historical reasoning steps, and A corresponds to t t 1:t−1 t\nIn this subsection, we perform atomic step fine-tuning\nnextatomicsteppredictedbyMLLM.π(a|s)representsthe\nontheAtomMATH(includingA-PRMandA-SFTsubsets)\nprobability of selecting an action a ∈ A conditioned on a\nandPRM800kdataset[11]. Moreover, weincorporateim-\nstates∈S,whichisestimatedbyPRMtoguidereasoning\nagecaptionsintothegenerationoflongCoTdata,thuswe\nprocess. Herebywecanadoptthevisualinstructiontuning\ncan alleviate the expensive computation burden of image\ntechnique[14]tofine-tuneMLLM.\nunderstandinginMLLMsandfocusontextsforsupervised\nfine-tuning. Therefore, we post-train an LLM based on\nPRM Training. In a slow thinking process, reasoning is Math-psa [31] to evaluate the consistency of atomic texts\ncarried out step by step, where each atomic step provides andsupervisefine-tuning.\nan intermediate conclusion. We train the PRM to imple-\n3.3.ActionSearchwithPRM\nment π(a|s) and provide feedback for every step, allow-\ningMLLMstorefineandimproveitsreasoning. Formally, Withthefine-tunedMLLMcapableofatomicstepreason-\ngiventhedescriptionofmathematicalproblemq,foranar- ing and the well-trained PRM providing feedback, we can\nbitrary step t ≥ 1, the PRM predicts a probability p of nowbeginthereasoningprocess. Astherearemanysearch\nt\nstrategies to generate candidate actions, we categorize the\nexisting strategies into path-wise searching and step-wise\nsearchingandexploretheminourAtomThinkframework.\nGPT-4o\nPath-wiseSearch. Inpath-wisesearching,webuildupon\npriorwork[28,31]byparallelsamplingmultiplepathsand Behaviour Distribution Capbility Clustering\naggregatingscorestofindoptimalsolutions. Weinvestigate\nthefollowingtwostrategies: ·Mathematical Understanding : Variable Definition, …\n·Mathematical Operations and Reasoning:Calculations, …\n• MajorityVoting: Itcombinesmultiplereasoningpaths\n·Interpretation and Conclusion: Graphs Analysis, …\nby selecting the most frequent outcome across them. It\n·Verification and Synthesis: Verification, …\nassumesthattheconsensusacrossdifferentpathsismore\nMathematical Capbilities\nlikelytoleadtothecorrectanswer.\n• Best-of-N: Given a generative MLLM, the best-of-N <image> Question: How many squares are there? S0 S1\nsampling method generates C candidate rollouts simul-\ntaneouslyandselectsthesolutionwiththehighestscore. A2-1 A3-1 A4-1 … An-1\nThe evaluation of candidate reasoning processes is de-\ntermined by the PRM, which employs three aggregation A1 A2-2 A3-2 A4-2 … An-2\nmethods to map the dense scores to the overall value of\ntheentirepath:1)Theworstaction: Comparetheworst A2-3 A3-3 A4-3 … An-3\nactionamongallcandidaterollouts.Itpenalizessolutions\nQuality of A_1 = 2/3\nwith any weak action and is used to search a reasoning\nthatissensitivetoerrors. 2)Thelastaction: Thescore Atomic Action Quality Evaluation\nis derived from the prediction of the final answer in in-\nference. 3)Averagescore: Itiscalculatedbyaveraging Figure3. Atomiccapabilityevaluation. Thecapabilitycategories\ntherewardsofalltheactionsinachain. Theexplainabil- arederivedfromtheclusteringofGPT-4o’sbehavior.Bysampling\nityandconsistencyofintermediatereasoningareempha- eachatomicstepandevaluatingtheaccessibilityoftheresults,we\nsizedhereasimportantastheoutcome. assignasoftlabelthatrepresentsqualityofanatomicstep.\nStep-wise Search. Searching strategies of this type start pabilities. As shown in Figure 3, we collect the behavior\nwithaninitialpathandincrementallyexpandthesampling distributionofGPT-4oonMathVerse[43]andperformclus-\nspace for each atomic action. Beam search and greedy tering,yieldingclustersthateachofthemrepresentscertain\nstrategiesareappliedtoprunebrancheswithlowquality. abilitiesutilizedbyhigh-levelintelligentmodelsinsolving\n• GreedyAlgorithm: Itfocusesonmakingthelocallyop- mathematicalproblems. Afterward,weevaluateanatomic\ntimalchoiceateachstepofthereasoningprocess. Itse- actionapredictedbythetargetmodelbasedonoutcomesu-\nlectsthebestimmediateaction(step)basedonthecurrent pervision,namely,weconsiderthatthequalityofanaction\nstate,withoutconsideringfutureconsequences. canbereflectedbytheprobabilityofitleadingtoacorrect\n• BeamSearch: Itexploresmultiplebranchesateachac- answer. This can be formulated as the following soft esti-\ntion and maintains a fixed number of top candidates for mationwithK roundsofoutcomesupervision[11]:\neach stage of reasoning. It balances between exploring\ndifferentpathsandexploitingthemostpromisingones. c soft(a)=\n(cid:80)K\nk=1 (cid:74)aˆ k Kiscorrect (cid:75), (3)\n3.4.AtomicCapabilityEvaluation\nwhere aˆ denotes the final answer predicted in the k-th\nk\nSimilar to human problem-solving processes, a CoT may roundand · istheIversonbracket. Consequently,wecan\ninvolve the use of multiple reasoning abilities. However, evaluate al(cid:74)l t(cid:75)he actions of the target model and map them\ntraditional CoT methods do not focus on the quality of in- backtothecapabilityset, toevaluatethemathematicalca-\ndividualreasoningstepsorprovidefine-grainedanalysesof pabilitiesofthetargetmodelthoroughly.\nthe underlying abilities. To address this gap, we have de-\nvelopedanatomiccapabilityevaluationstrategy,offeringa 4.Experiment\nnewanalyticalperspectiveforslowthinking.\n4.1.Setup\nOur evaluation method aims to assess the mathemati-\ncalcapabilitiesofatargetmodelfromvariousperspectives, Baselines. Our experiments utilize two open-source\nsuch as understanding, operations, and certifications. To MLLMs, includingLLaVA-Llama3-8B[14]andEMOVA-\nthis end, we first need to construct a canonical set of ca- 8B[4]. Wefine-tuneonlytheparametersoftheirlanguage\nQuestion: What is the length AtomThink from EMOVA-8B\nof the radius of the circle in\nStep 1: The image shows a sector of a circle, labeled\nwhich sector ABC is inscribed? ABC, with a central angle of 45 degrees. The length\nChoices: of the arc AB is given as 7.0 units.\nA. 7.5\nB. 8.5 Step 2: The formula for the length of an arc in a circle\nC. 8.9 is given by , where is the radius of the circle\nD. 9.3 and is the central angle in radians. 𝐿𝐿\n𝑳𝑳=𝒓𝒓 ∗𝜽𝜽 𝑟𝑟\nAnswer: The final answer is C.\nStep𝜃𝜃 3: Convert the central angle from degrees to\nradians. The conversion factor is . Therefore,\nCoT Answer from GPT-4o\nin radians is = .\nTo find the radius of the circle in which sector ABC is inscribed, 𝜋𝜋/180\nwe can use the properties of a 45-degree righttriangle. In this 4S5te°p 4: Substitute𝟏𝟏 𝟒𝟒the∗ kn𝝅𝝅o/w𝟏𝟏n𝟐𝟐 𝟏𝟏valu𝝅𝝅es/ 𝟏𝟏into the arc length\nformula: .\ncase, triangle AB is a right triangle with angle B being 45\ndegreesand AB being the radius of the circle.\n𝟕𝟕.𝟏𝟏=𝒓𝒓 ∗ 𝝅𝝅/𝟏𝟏\nStep 5: Solve for the radius . Rearrange the equation\nSince triangle AB is a 45-degree right triangle, the sides\nto find : .\nopposite the 45-degree angles are equal. Therefore, if AB = 7.0,\n𝑟𝑟\nthen the radius of the circle (which is also the hypotenuse of the 𝑟𝑟 𝒓𝒓=(𝟕𝟕.𝟏𝟏∗𝟏𝟏)/𝝅𝝅=𝟐𝟐𝟐𝟐/𝝅𝝅\nStep 6: Calculate using the approximate value of\ntriangle) can be found using the Pythagorean theorem:\n: .\nLet be the radius of the circle. 28/𝜋𝜋\n𝜋𝜋Ste≈p 37.:1 C4om𝐫𝐫p≈ar𝟐𝟐e𝟐𝟐 t/h𝟑𝟑e .c𝟏𝟏a𝟏𝟏lcu≈la𝟐𝟐te.d𝟗𝟗 radius with the given\n𝑟𝑟\n2 2 2 choices: A. 7.5; B. 8.5; C. 8.9; D. 9.3.\n𝑟𝑟 =7.0 +7.0\n2\n𝑟𝑟 =49+49=98 Step 8: The calculated radius matches choice C.\nHowever, since the choices provided are slightly different, we\n𝑟𝑟= 98≈9.9\nshould check the closest value to 9.9. The closest value is 9.3. 𝟐𝟐.𝟗𝟗\nStep 9: To sum up, the final answer is: C.\nAnswer: 9.3\nFigure4. AcasestudyofAtomThinkCoTandGPT-4ogeneratedCoT.Redandgreencharactersdenoteincorrectandcorrectresponses,\nrespectively.Ourmodelexhibitsfewerhallucinationsandstrongerreasoningabilitiesasitfocusesonthequalityofatomicsteps.Moreover,\nourmodelautomaticallydecomposesthereasoningprocesssemantically,leadingtoimprovedreadability.\nmodelsandprojectorswithlearningratesof2e-5and2e-6, targeteddomains.Additionally,toassessthemodel’sability\nrespectively,andabatchsizeof128.Weselectninecutting- tointerpretmathematicalgraphs,weuseamorechallenging\nedgeMLLMsforcomparison,includingOpenAI’so1[23], multimodalbenchmark,MathVerse[43]forfurtherevalua-\n4o [21], and 4v [22], as well as LLava-NeXT-34B [13], tion. It contains five categories including Text Lite (TL),\nInternLM-XComposer2 [41], Qwen-VL-Plus [2], LLaVA- Text Dominant (TD), Vision Intensive (VI), Vision Domi-\n7B[14],G-LLaVA-7B[5],andMAVIS-7B[41]. nant(VD),VisionOnly(VO).\nOut evaluations include four inference settings, includ-\nDatasets. For LLaVA-Llama3-8B, we use LLaVA- ing Direct, CoT, Quick Think, and Slow Think. In the\n665k [14] for supervised fine-tuning (SFT) as a baseline. Direct setting, we prompt the model to generate a concise\nAdditionally, in LLaVA w/. Formatted and EMOVA w/. final answer. In CoT, the model is instructed to answer\nFormatted,wetransferthesourcedataofAtomMATHinto the question through step-by-step reasoning. For the Di-\nan aligned CoT format for incremental training, ensuring rectandCoTevaluations,weusepromptsfromlmms-eval\na fair comparison without introducing bells and whistles. [10, 40]. Our AtomThink-models support two additional\nFor EMOVA-8B, we downsampled its publicly available settings: QuickThinkandSlowThink. InQuickThink,\nSFTdata[4]toobtainabasicpost-trainingdatasetcontain- our models follow a single, atomic reasoning path based\ningabout200ksamples. FormodelswithAtomThink, the purelyontheirlearnedpolicies,withoutemployinganysup-\nAMATH-SFTdatasetintroducedinSection3.1,isincorpo- plementarysearchstrategies. InSlowThink,enhancedby\nratedtointroduceatomicreasoningcapabilities. thePRM,weutilizebeamsearchwithbeamwidthof2and\ntemperature of 1.0, encouraging our models to engage in\nEvaluationSetting. Weevaluatedtheperformanceofour moreextensivereasoning.\nmethodonMathVista[18],apubliclyavailablebenchmark\nencompassing both general-targeted and mathematics-\nMathVista MathVerse\nModel Inference General Math Total TL TD VI VD VO Total\nRandomChoice - - - 17.9 12.4 12.4 12.4 12.4 12.4 12.4\nHuman - - - - 70.9 71.2 61.4 68.3 66.7 66.7\nOpenAIo1 SlowThink* - - 73.9 - - - - - -\nGPT-4o CoT - - 63.8 - - - - - -\nGPT-4V CoT - - 49.9 56.6 63.1 51.4 50.8 50.3 54.4\nLLaVA-NeXT-34B Direct - - 46.5 25.5 33.8 23.5 20.3 15.7 23.8\nInternLM-XComposer2 Direct - - 57.6 17.0 22.3 15.7 16.4 11.0 16.5\nQwen-VL-Plus Direct - - 43.3 11.1 15.7 9.0 13.0 10.0 11.8\nLLaVA-1.5-13B Direct - - 27.6 15.2 19.4 16.8 15.2 11.3 15.6\nG-LLaVA-7B Direct - - 53.4 20.7 20.9 17.2 14.6 9.4 16.6\nMAVIS-7B Direct - - - 29.1 41.4 27.4 24.9 14.6 27.5\nLLaVA-Llama3-8B Direct 34.1 25.6 29.5 16.0 19.3 16.4 13.1 15.0 15.9\nLLaVAw/. Formatted CoT 30.2 22.9 26.3 14.3 18.4 15.7 10.0 7.7 13.2\nAtomThink-LLaVA Direct 34.4 27.2 30.5 16.0 19.3 16.2 13.1 15.0 15.9\nAtomThink-LLaVA QuickThink 36.9 37.0 36.6 22.2 26.6 24.1 20.9 17.9 22.4\nAtomThink-LLaVA SlowThink 36.5 41.3 39.1 36.1 42.4 30.0 36.8 28.6 34.7\nEMOVA-8B-200k Direct 52.4 51.1 51.7 34.4 39.0 33.4 30.1 23.5 32.1\nEMOVAw/. Formatted CoT 30.9 31.3 31.1 26.5 36.5 25.3 20.4 19.8 25.7\nAtomThink-EMOVA Direct 53.9 52.4 53.1 33.6 39.0 33.8 28.0 24.4 31.8\nAtomThink-EMOVA QuickThink 48.7 54.4 51.8 36.5 42.4 34.1 32.9 29.7 35.1\nAtomThink-EMOVA SlowThink 48.9 57.0 53.3 42.1 51.5 39.0 36.7 33.1 40.5\nTable3. Comparisonofaccuracywithstate-of-the-artmethodsonMathVistaandMathVerse. OurAtomThinkLLaVAoutperformsthe\nbaselineinallsub-tasksacrosstwobenchmarks,achievinganaverageimprovementof14.2%.Meanwhile,AtomThinkEMOVA,withonly\n8Bparameters,surpassesthelargerLLaVA-NEXT-34BandeveniscomparabletoGPT-4V.\n4.2.MainResults 38.3% on MathVerse, outperforming existing open-source\nMLLMs.Theseresultsdemonstratethatwhenamodelpos-\nComparison with existing MLLMs. In Table 3, our\nsesses atomic reasoning capabilities, it can leverage rapid\nAtomThinkframeworkisappliedtotrainLLaVA-Llama3-\nintuitiontoperformmoreaccuratemathematicalreasoning.\n8B and EMOVA-8B, yielding consistent performance im-\nprovementsovertheoriginalmodels. Whencombinedwith\nPRM,AtomThink-EMOVAachievesanewstate-of-the-art LLMEffectivelySuperviseVisualReasoningProcesses.\non MathVerse, surpassing GPT-4o and narrowing the gap Previous work has shown that process supervision reward\nbetweenMLLMsandhumanperformance. OnMathVista, models are effective in evaluating intermediate reasoning\nitalsoachievesperformanceclosetothatofGPT-4o. These steps, though these methods have been primarily applied\nresults demonstrate the framework’s strong generalization within the domain of language models. We fine-tuned an\ncapabilityandpracticalusability. LLMwithA-MATH-PRMandapplieditfortest-timescal-\ning. Asshowninthetable,AtomThink-EMOVA,whenuti-\nlizing PRM with beam search, achieved an additional 2%\nQuick Think with Intuition. Unlike traditional CoT improvementonMathVista[18]comparedtoQuickThink.\nmethods,QuickThinkgeneratesastepwisereasoningpath InMathVerse[43], itevenoutperformedtheclosed-source\nthroughmulti-turnconversations,bypassingtheneedforan model GPT-4V by 1%. Additionally, increasing test-time\nadditional verifier. This approach offers a computational scalinginLLAVAresultedinsubstantialimprovements,po-\nadvantage over Slow Think and highlights the model’s sitioningitwellaboveitssiblingmodel,LLAVA-1.5-13B.\nintuitive reasoning capabilities. For LLaVA-Llama3-8B, Wefindthatevenwhenthereasoningprocessheavilyre-\nour AtomThink framework surpasses the baseline model, lies on visual dominant inputs, our models can avoid tak-\nachieving approximately a 10% improvement on Math- ing incorrect paths by improving text decoding. On the\nVista [18] and a 19% improvement on MathVerse [43]. onehand,itisattributedtotheAtomThinktrainingprocess,\nForAtomThink-EMOVA,QuickThinkachievedascoreof whichencouragesMLLMtofirstunderstandimagebefore\nreasoning. Ontheotherhand,italsoconfirmstheeffective- 80\nGPT-4o\nnessoftest-timeextensioninmultimodaltasks. +CoT 60 +g1\n40\nTrade-off between General and Math Ability. Similar 20\ntotheconclusionsreportedino1,weobservethatMLLMs\n0\nStatistical Scientific Algebraic Arithmetic Geometry Numeric Logical\nbecomeweakerongeneraltasksthatrelyonworldknowl-\n80\nEMOVA\nedgeduringdeepcontemplation,demonstratingatrade-off +g1 60 +AtomThink\nbetweenhigher-levelreasoninganddirectthinking. Forin-\nstance,LLaVA-Llama3-8Bpresentsadeclineinaccuracyof 40\n7%comparedtothebaselineonthegeneralsubsetofMath- 20\nVista, while EMOVA experiences a 17% reduction. How-\n0\nStatistical Scientific Algebraic Arithmetic Geometry Numeric Logical\never,afterapplyingPRM-basedactionsearch,bothmodels\n80\nare able to narrow this generalization gap and improve ac- L +L ga 1Va-LLaMa3\n60 +AtomThink\ncuraciesby4%and16%,respectively.\n40\n4.3.AtomicAbilityAnalysis 20\nWe first cluster the reasoning behaviors of GPT-4o into 0\nStatistical Scientific Algebraic Arithmetic Geometry Numeric Logical\na set of capabilities S, including Approximation, Verifi-\ncation, Calculation, Variable Definition, Geometric Rea- Figure 5. Comparison to CoT and g1 in MathVista subsets. In\nsoning, Conclusion Drawing, Graphs Analysis, Equation contrasttothedecliningtrendobserveding1,AtomThinkoutper-\nformsthebaselineacrossmostsubsets.\nFormulation, Image Description, Knowledge Introduction,\nInformation Extraction, and Formula Derivation. Using\nqueries in MathVerse [43], we constructed 500 current\n4.5.ActionSearchAblation\nstatesass withhigh-qualityresponsesgeneratedbyGPT-\ni\n4o. Subsequently, soft estimations of atomic actions are In Table 4, we evaluate the impact of direct output, path-\nmappedtoS foranalysis. wise search and action-wise search strategies on Math-\nVista [18] and MathVerse [43] using a subset of 200 sam-\nples from each. Results show that even without addi-\nAbility Analysis. Figure 3 illustrates the distribution tionalcomputation,AtomThink-EMOVA’sdirectprediction\nof atomic behaviors and capability differences between accuracy outperforms the original, with improvements of\nLLAVA-llama3-8bandEMOVA-8BwiththeirAtomThink- 1.3%,1.52%,and2.4%,respectively. Thepath-wisesearch\nversions. TheanalysisrevealsthatAtomThink-Modelgen- method,BoN-Avg,achievesthehighestaccuracyof58.68%\nerally outperforms baseline across most abilities, demon- on the MathVista [18] mathematical tasks, although it ex-\nstrating higher scores in areas such as Image Description perienced a drop on general problems. Meanwhile, both\nand Verification. It suggests that our model is capable of greedy algorithm and beam search show balanced perfor-\nmore accurate analysis of visual information and demon- mance across all benchmarks, with the generalization gap\nstratesadegreeofself-checkingandreflectivecapability. betweenmathandgeneraltasksbeingnotablysmallerthan\nthatofpath-wisesearch.\n4.4.Comparisonwithg1\nModel Method MathVista-M MathVista-G MathVerse\nInFigure5, wecompareAtomThinkwiththestate-of-the-\nEMOVA-200k Direct 51.1 52.4 33.3\nartopen-sourceinferencestrategy,g11,whichemploysdy-\nDirect 52.4 53.9 35.7\nnamic prompting to make model focus on single step re- AtomThink QuickThink 54.2 46.7 38.1\nflection. In GPT-4o, direct application of g1 for multi- MajorityVoting 48.8 49.4 39.0\nturnreasoningyieldsagreaterimprovementoverChain-of- w/.Path-wise BoN-Last 51.2 46.8 41.3\nBoN-Avg 58.7 40.5 38.7\nThought,particularlyinnumericandgeometrictasks.How- BoN-Min 53.7 53.2 40.0\never,duetotherelianceontheinherentreasoningcapabili- Greedy 46.3 45.6 38.3\nw/.Step-wise\nBeamSearch 57.1 53.2 45.3\ntiesoflarge-scalelanguagemodels,itsperformancesignifi-\ncantlydegradesonsmallermodelssuchasEMOVA-8Band\nTable4.AblationstudyonPath-wiseandstep-wisesearch.There-\nLLaVA-Llama3-8B.Incontrast,ourAtomThinkframework\nsultsshowthatbothBest-of-N-Min(BoN-Min)andBeamSearch\nconsistentlyenhancestheperformanceoftheseMLLMs. exhibitconsistentperformanceimprovements.\n1https://github.com/bklieger-groq/g1 Limitation. Duetothelimitationsincomputinginfras-\n)%(\nycaruccA\n)%(\nycaruccA\n)%(\nycaruccA\ntructure, we are unable to validate our method on larger [8] MehranKazemi,HamidrezaAlvari,AnkitAnand,JialinWu,\nMLLMs. Additionally, despite undergoing small-scale XiChen,andRaduSoricut. Geomverse: Asystematiceval-\nmanualreview, ourdatasetstilllacksstep-levelgoldenan- uationoflargemodelsforgeometricreasoning. InCVPRW.\nswers,whichmayintroducenoiseintotraining. 3\n[9] BenjaminKlieger.g1:Usingllama-3.170bongroqtocreate\n5.Conclusion o1-likereasoningchains,2024. 3\n[10] BoLi,PeiyuanZhang,KaichenZhang,XinrunDuFanyiPu,\nThispaperintroducesatomthinkingcapabilitiestoMLLMs Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang,\nfor solving visual mathematics problems. We release ChunyuanLi,andZiweiLiu. Lmms-eval: Acceleratingthe\na high-quality, human-free annotated long-CoT dataset, developmentoflargemultimodalmodels,2024. 6\nAtomMATH,consistingof157katomicreasoningstepsand [11] HunterLightman,VineetKosaraju,YuriBurda,HarrisonEd-\n130kcorrespondingannotations. Furthermore,wepropose wards,BowenBaker,TeddyLee,JanLeike,JohnSchulman,\nIlyaSutskever,andKarlCobbe. Let’sverifystepbystep. In\nAtomThink,anovelframeworkthatfocusesonthequality\nICLR. 2,4,5\nofatomicsteps. Theexperimentalresultsdemonstratethat\n[12] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Ed-\nour method consistently enhances the model’s diverse be-\nwards,BowenBaker,TeddyLee,JanLeike,JohnSchulman,\nhaviorsduringtheproblem-solvingprocess,leadingtoim-\nIlya Sutskever, and Karl Cobbe. Let’s verify step by step.\nproved reasoning performance across various multimodal\narXivpreprintarXiv:2305.20050,2023. 2\nmathematicaltasks. Thisworkpavesthewayfordevelop- [13] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\ninggeneralizedslow-thinkingmodels. Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im-\nprovedreasoning,ocr,andworldknowledge,2024. 6\nReferences [14] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.\nVisualinstructiontuning. NeurIPS,36,2024. 4,5,6\n[1] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, Antoine\n[15] Wentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-\nLi, JiayiZeng, MengliangHe, QinChen, BoJiang, Aimin\nsch,KatherineMillican,MalcolmReynolds,etal.Flamingo:\nZhou,etal.Mathematicallanguagemodels:Asurvey.arXiv\na visual language model for few-shot learning. Advances\npreprintarXiv:2312.07622,2023. 2\ninneuralinformationprocessingsystems,35:23716–23736,\n[16] PanLu, LiangQiu, Kai-WeiChang, YingNianWu, Song-\n2022. 2\nChun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nKalyan. Dynamic prompt learning via policy gradient for\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nsemi-structuredmathematicalreasoning. InICLR. 3\nZhou. Qwen-vl: Aversatilevision-languagemodelforun-\n[17] PanLu,RanGong,ShibiaoJiang,LiangQiu,SiyuanHuang,\nderstanding, localization, text reading, and beyond. arXiv\nXiaodanLiang,andSong-chunZhu.Inter-gps:Interpretable\npreprintarXiv:2308.12966,1(2):3,2023. 6\ngeometry problem solving with formal language and sym-\n[3] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald bolicreasoning. InACL,pages6774–6786,2021. 3\nClark, QuocVLe, ChristopherRe´, andAzaliaMirhoseini.\n[18] PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi,\nLarge language monkeys: Scaling inference compute with\nHannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel\nrepeatedsampling. arXivpreprintarXiv:2407.21787,2024.\nGalley,andJianfengGao. Mathvista: Evaluatingmathrea-\n2\nsoninginvisualcontextswithgpt-4v, bard,andotherlarge\n[4] Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin multimodalmodels.arXive-prints,pagesarXiv–2310,2023.\nTan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo 6,7,8\nYang, et al. Emova: Empowering language models to [19] LiangchenLuo,YinxiaoLiu,RosanneLiu,SamratPhatale,\nsee, hear and speak with vivid emotions. arXiv preprint Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng,\narXiv:2409.18042,2024. 5,6 Jiao Sun, et al. Improve mathematical reasoning in lan-\n[5] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun guage models by automated process supervision. arXiv\nZhong,YufeiWang,LanqingHong,JianhuaHan,HangXu, preprint:2406.06592,2024. 2,3\nZhenguoLi,andLingpengKong. G-llava: Solvinggeomet- [20] Minheng Ni, Yutao Fan, Lei Zhang, and Wangmeng\nric problem with multi-modal large language model, 2023. Zuo. Visual-o1: Understandingambiguousinstructionsvia\n6 multi-modal multi-turn chain-of-thoughts reasoning. arXiv\n[6] ZitianGao,BoyeNiu,XuzhengHe,HaotianXu,Hongzhang preprintarXiv:2410.03321,2024. 2\nLiu, Aiwei Liu, Xuming Hu, and Lijie Wen. Interpretable [21] OpenAI. Gpt-4osystemcard,. 2,6\ncontrastivemontecarlotreesearchreasoning.arXivpreprint [22] OpenAI. Gpt-4v(ision)systemcard,. 6\narXiv:2410.01707,2024. 1 [23] OpenAI. Openaio1systemcard,. 1,6\n[7] Justin Johnson, Bharath Hariharan, Laurens Van [24] Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv,\nDer Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong,\nGirshick. Clevr: A diagnostic dataset for compositional et al. Cogcom: Train large vision-language models diving\nlanguageandelementaryvisualreasoning. InCVPR,pages intodetailsthroughchainofmanipulations. arXivpreprint\n2901–2910,2017. 3 arXiv:2402.04236,2024. 2\n[25] YiweiQin,XuefengLi,HaoyangZou,YixiuLiu,ShijieXia, [38] EricZelikman,YuhuaiWu,JesseMu,andNoahGoodman.\nZhenHuang, YixinYe, WeizheYuan, HectorLiu, Yuanzhi Star: Bootstrappingreasoningwithreasoning. Advancesin\nLi,etal.O1replicationjourney:Astrategicprogressreport– Neural Information Processing Systems, 35:15476–15488,\npart1. arXivpreprintarXiv:2410.18982,2024. 1 2022. 2\n[26] HaoShao, ShengjuQian, HanXiao, GuangluSong, Zhuo- [39] EricZelikman,GeorgesHarik,YijiaShao,VarunaJayasiri,\nfanZong,LetianWang,YuLiu,andHongshengLi. Visual NickHaber, andNoahDGoodman. Quiet-star: Language\ncot: Unleashingchain-of-thoughtreasoninginmulti-modal modelscanteachthemselvestothinkbeforespeaking.arXiv\nlanguagemodels.arXivpreprintarXiv:2403.16999,2024.2 preprintarXiv:2403.09629,2024. 2\n[27] WenhaoShi,ZhiqiangHu,YiBin,JunhuaLiu,YangYang, [40] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu,\nSeeKiongNg,LidongBing,andRoyLee.Math-llava:Boot- Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan\nstrappingmathematicalreasoningformultimodallargelan- Zhang,JingkangYang,ChunyuanLi,andZiweiLiu.Lmms-\nguagemodels. pages4663–4680,2024. 3 eval: Reality check on the evaluation of large multimodal\n[28] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Ku- models,2024. 6\nmar. Scalingllmtest-timecomputeoptimallycanbemore [41] RenruiZhang,XinyuWei,DongzhiJiang,YichiZhang,Ziyu\neffective than scaling model parameters. arXiv preprint Guo,ChengzhuoTong,JiamingLiu,AojunZhou,BinWei,\narXiv:2408.03314,2024. 2,5 Shanghang Zhang, et al. Mavis: Mathematical visual in-\n[29] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis structiontuning. arXive-prints,pagesarXiv–2407,2024. 2,\nSong, Noah Siegel, Lisa Wang, Antonia Creswell, Geof- 3,6\nfreyIrving,andIrinaHiggins. Solvingmathwordproblems [42] RuohongZhang,BowenZhang,YanghaoLi,HaotianZhang,\nwith process-and outcome-based feedback. arXiv preprint Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and\narXiv:2211.14275,2022. 2 Yiming Yang. Improve vision language model chain-of-\n[30] ChaojieWang,YanchenDeng,ZhiyiLyu,LiangZeng,Jujie thoughtreasoning. arXivpreprintarXiv:2410.16198,2024.\nHe,ShuichengYan,andBoAn. Q*: Improvingmulti-step 2,3\nreasoningforllmswithdeliberativeplanning.arXivpreprint [43] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin,\narXiv:2406.14283,2024. 1 Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei\n[31] Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Chang,YuQiao,etal. Mathverse: Doesyourmulti-modal\nZhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Li- llm truly see the diagrams in visual math problems? In\nonelMNi,etal. Openr: Anopensourceframeworkforad- EuropeanConferenceonComputerVision,pages169–186.\nvancedreasoningwithlargelanguagemodels.arXivpreprint Springer,2025. 5,6,7,8\narXiv:2410.09671,2024. 1,4,5 [44] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,Xiaolei\n[32] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\nDai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math- Zhang,ZicanDong,etal. Asurveyoflargelanguagemod-\nshepherd:Verifyandreinforcellmsstep-by-stepwithouthu- els. arXivpreprintarXiv:2303.18223,2023. 1,2\nmanannotations. InACL,pages9426–9439,2024. 2,3\n[33] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed\nChi, Sharan Narang, Aakanksha Chowdhery, and Denny\nZhou. Self-consistency improves chain of thought reason-\ning inlanguage models. arXiv preprintarXiv:2203.11171,\n2022. 2\n[34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in neural information processing\nsystems,35:24824–24837,2022. 1,2\n[35] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom\nGriffiths, Yuan Cao, and Karthik Narasimhan. Tree of\nthoughts: Deliberate problem solving with large language\nmodels. Advances in Neural Information Processing Sys-\ntems,36,2024. 1\n[36] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,\nTongXu,andEnhongChen. Asurveyonmultimodallarge\nlanguagemodels.arXivpreprintarXiv:2306.13549,2023.2\n[37] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,Ruoqi\nLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen,YuxuanSun,etal. Mmmu:Amassivemulti-discipline\nmultimodalunderstandingandreasoningbenchmarkforex-\npertagi. InCVPR,pages9556–9567,2024. 3",
    "pdf_filename": "AtomThink_A_Slow_Thinking_Framework_for_Multimodal_Mathematical_Reasoning.pdf"
}