{
    "title": "AtomThink A Slow Thinking Framework for Multimodal Mathematical Reasoning",
    "context": "In this paper, we address the challenging task of multi- modal mathematical reasoning by incorporating the ability of “slow thinking” into multimodal large language mod- els (MLLMs). Contrary to existing methods that rely on direct or fast thinking, our key idea is to construct long chains of thought (CoT) consisting of atomic actions in a step-by-step manner, guiding MLLMs to perform com- plex reasoning. To this end, we design a novel AtomThink framework composed of three key modules: (i) a CoT an- notation engine that automatically generates high-quality CoT annotations to address the lack of high-quality visual mathematical data; (ii) an atomic step fine-tuning strat- egy that jointly optimizes an MLLM and a policy reward model (PRM) for step-wise reasoning; and (iii) four dif- ferent search strategies that can be applied with the PRM to complete reasoning. Additionally, we propose Atom- MATH, a large-scale multimodal dataset of long CoTs, and an atomic capability evaluation metric for mathematical tasks. Extensive experimental results show that the pro- posed AtomThink significantly improves the performance of baseline MLLMs, achieving approximately 50% relative ac- curacy gains on MathVista and 120% on MathVerse. To support the advancement of multimodal slow-thinking mod- els, we will make our code and dataset publicly available on https://github.com/Quinn777/AtomThink. Chain-of-thought (CoT) reasoning [34] has provided a novel scheme for large language models (LLMs) to tackle complex reasoning tasks. By utilizing a small number of specially designed instructions, CoT enables LLMs to gen- erate intermediate reasoning steps, significantly enhancing performance on symbolic tasks such as mathematical prob- *These authors contributed equally to this work. †Corresponding author. Email: xdliang328@gmail.com Graphs Analysis Verification Information Extraction Calculation Approximation Geometric Reasoning Equation Formulation Knowledge Introduction Image Description Variable Definition 0% 20% 40% 60% 80% 100% LLaVA-Llama3-8B AtomThink-LLaVA EMOVA-8B AtomThink-EMOVA Figure 1. Atomic capability evaluation of different models. Exist- ing open-source models exhibit significant shortcomings in capa- bilities such as variable definition, approximation and image de- scription. lems and code writing [44]. While CoT-based methods show clear improvements over direct predictions, they still rely heavily on greedy decoding strategies. More recently, the introduction of OpenAI’s o1 [23] marks a substantial advancement in the ability of artificial intelligence systems to perform high- level reasoning. Unlike traditional models, o1 excels in solving complex problems by utilizing extended reason- ing chains and adopting test-time scaling, i.e., “slow think- ing”. In addition to o1, several concurrent works have ex- plored methods for incorporating slow thinking capabili- ties into open-source LLMs, such as Thought Trees [35] and Monte Carlo tree search (MCTS) based tree search techniques [6, 25, 30, 31]. The success of o1 and its variants demonstrate that incorporating slow thinking into LLMs significantly enhances their performance on com- arXiv:2411.11930v1  [cs.CV]  18 Nov 2024",
    "body": "AtomThink: A Slow Thinking Framework for Multimodal Mathematical\nReasoning\nKun Xiang1*, Zhili Liu2∗, Zihao Jiang3∗, Yunshuang Nie1, Runhui Huang4, Haoxiang Fan5,\nHanhui Li1, Weiran Huang3, Yihan Zeng5, Jianhua Han5, Lanqing Hong5, Hang Xu5, Xiaodan Liang1†\n1 Sun Yat-sen University 2 Hong Kong University of Science and Technology\n3 Shanghai Jiaotong University 4 University of Hong Kong 5 Huawei Noah’s Ark Lab\nAbstract\nIn this paper, we address the challenging task of multi-\nmodal mathematical reasoning by incorporating the ability\nof “slow thinking” into multimodal large language mod-\nels (MLLMs). Contrary to existing methods that rely on\ndirect or fast thinking, our key idea is to construct long\nchains of thought (CoT) consisting of atomic actions in\na step-by-step manner, guiding MLLMs to perform com-\nplex reasoning. To this end, we design a novel AtomThink\nframework composed of three key modules: (i) a CoT an-\nnotation engine that automatically generates high-quality\nCoT annotations to address the lack of high-quality visual\nmathematical data; (ii) an atomic step fine-tuning strat-\negy that jointly optimizes an MLLM and a policy reward\nmodel (PRM) for step-wise reasoning; and (iii) four dif-\nferent search strategies that can be applied with the PRM\nto complete reasoning.\nAdditionally, we propose Atom-\nMATH, a large-scale multimodal dataset of long CoTs, and\nan atomic capability evaluation metric for mathematical\ntasks.\nExtensive experimental results show that the pro-\nposed AtomThink significantly improves the performance of\nbaseline MLLMs, achieving approximately 50% relative ac-\ncuracy gains on MathVista and 120% on MathVerse. To\nsupport the advancement of multimodal slow-thinking mod-\nels, we will make our code and dataset publicly available\non https://github.com/Quinn777/AtomThink.\n1. Introduction\nChain-of-thought (CoT) reasoning [34] has provided a\nnovel scheme for large language models (LLMs) to tackle\ncomplex reasoning tasks. By utilizing a small number of\nspecially designed instructions, CoT enables LLMs to gen-\nerate intermediate reasoning steps, significantly enhancing\nperformance on symbolic tasks such as mathematical prob-\n*These authors contributed equally to this work.\n†Corresponding author. Email: xdliang328@gmail.com\nGraphs\nAnalysis\nVerification                  \nInformation                  \nExtraction                  \nCalculation                \nApproximation\n        Geometric\n        Reasoning\n                Equation\n                Formulation\n                  Knowledge\n                  Introduction\n                    Image\n                    Description\nVariable\nDefinition\n0%\n20%\n40%\n60%\n80%\n100%\nLLaVA-Llama3-8B\nAtomThink-LLaVA\nEMOVA-8B\nAtomThink-EMOVA\nFigure 1. Atomic capability evaluation of different models. Exist-\ning open-source models exhibit significant shortcomings in capa-\nbilities such as variable definition, approximation and image de-\nscription.\nlems and code writing [44].\nWhile CoT-based methods show clear improvements\nover direct predictions, they still rely heavily on greedy\ndecoding strategies.\nMore recently, the introduction of\nOpenAI’s o1 [23] marks a substantial advancement in the\nability of artificial intelligence systems to perform high-\nlevel reasoning.\nUnlike traditional models, o1 excels in\nsolving complex problems by utilizing extended reason-\ning chains and adopting test-time scaling, i.e., “slow think-\ning”. In addition to o1, several concurrent works have ex-\nplored methods for incorporating slow thinking capabili-\nties into open-source LLMs, such as Thought Trees [35]\nand Monte Carlo tree search (MCTS) based tree search\ntechniques [6, 25, 30, 31].\nThe success of o1 and its\nvariants demonstrate that incorporating slow thinking into\nLLMs significantly enhances their performance on com-\narXiv:2411.11930v1  [cs.CV]  18 Nov 2024\n\nplex, multi-step tasks, improving their overall problem-\nsolving capabilities.\nHowever, adopting the slow-thinking technique into\nmultimodal large language models (MLLMs) is challeng-\ning, due to the increased data and computational resource\ndemand for information modeling in visual tasks [24, 42].\nAlthough many efforts have been conducted to alleviate this\nissue, such as incorporating interleaved image-text data [1],\nprompt engineering [20, 26], they are still confined to stim-\nulating the inherent CoT capabilities of MLLMs, without\nconsidering the quality of each intermediate step in the\nreasoning chain. Hence, existing methods are hard to apply\ntest-time scaling laws to guarantee their performance.\nTo validate the importance of the quality of each inter-\nmediate step in CoT, we first design a capability evalua-\ntion method to perform a fine-grained quality assessment\nof each atomic step generated by MLLMs. Here we de-\nfine an atomic step as the minimal prediction node in the\nslow thinking process. Considering that humans may utilize\ndistinct cognitive abilities for solving mathematical prob-\nlems, we utilize one of the current most advanced LLMs,\ni.e., GPT-4o [21] to construct an ability set and estimate\nscores of atomic steps with outcome supervision. The re-\nsults shown in Figure 1 indicate that the step quality of ex-\nisting open-source models is significantly lower than that\nof GPT-4o, particularly in areas such as image recognition,\nvariable definition, and calculation ability. This finding fur-\nther motivates our focus on the capability gaps in existing\nmodels, prompting us to improve performance by enhanc-\ning the quality of atomic reasoning steps.\nTherefore, to fully leverage the advantages of CoT\nand address the aforementioned challenges, we propose a\nfull-process slow-thinking framework called AtomThink.\nAtomThink introduces a multimodal CoT annotation en-\ngine, an atomic step finetuning strategy, and policy search-\ning strategies to generate high-quality atomic steps.\nIt\naims to enhance the decoding capabilities of MLLMs\nthrough careful training, combined with post-sampling\nsearch strategies to identify the optimal prediction nodes.\nTo begin with, the proposed annotation engine is used to\ncreate a novel multimodal long CoT dataset including 26k\nhigh-level mathematical problems, 157k atomic-granularity\nsteps, and 130k process supervision annotations. The con-\nstruction of this dataset does not require manual labeling\nand effectively leverages existing short labels. Secondly,\nour atomic step finetuning strategy applies step-level mask-\ning to the training set, forcing our models to learn multi-\nturn self-dialogue ability and generate reasoning focused on\nindividual inference actions. Thirdly, we explore different\nsearch strategies along both the path and step dimensions\nduring the inference phase to find optimal prediction nodes.\nTo validate the effectiveness of our method, we conduct ex-\ntensive experiments on public datasets. We improved the\naccuracy of LLaVA-Llama3-8B on MathVista and Math-\nVerse by 9.6% and 18.8%, respectively. With EMOVA (8B)\nas the base model, AtomThink achieved the highest accu-\nracy of 40.5% on MathVerse, surpassing the cutting-edge\nGPT-4V.\nIn summary, our primary contributions are as follows:\n• We introduce AtomThink, a comprehensive framework\nthat guides MLLMs to focus on atomic step reason-\ning, which obtains consistent performance improvements\nacross multiple baseline MLLMs.\n• By designing an atomic capability evaluation based on\noutcome supervision, we reveal the capability distribution\nof MLLMs in generating each type of atomic step.\n• A multimodal long CoT dataset specifically focused on\nmultimodal mathematical tasks, AtomMATH, is first in-\ntroduced.\n2. Related Work\nSlow Thinking in Multimodal Reasoning Tasks\nCom-\nplex reasoning tasks such as mathematical computation\nand code programming have long been challenging for\nMLLMs [15, 36, 44]. Some prior work has approached this\nissue from the perspective of prompt engineering, encour-\naging models to generate Chain-of-Thought(CoT), which\nis widely believed to enhance model’s reasoning [33, 34].\nThey carefully modify the input distribution to enable the\nmodel to mimic human step-by-step output without finetun-\ning parameters. Other recent studies have explored under-\nstanding visual ambiguity by introducing multi-turn chain-\nof-thoughts [20]. Shao et al. [26] have considered incor-\nporating additional visual tokens into CoTs, such as object\nregions and precise localization. However, due to the lack\nof multimodal process supervision data, current works have\nnot explored reward model-based search strategies, which\nare widely used in LLMs [3, 12, 28, 29, 38].\nLong CoT Annotation for Mathematical Data\nThe in-\ntroduction of slow thinking relies heavily on the availabil-\nity of high-quality step-level annotations. In 2023, Light-\nman et al. [11] constructed a process supervision dataset\ncomposed of extensive human annotations, which has been\nwidely used for mathematical reasoning. Recent advance-\nments have focused on automating the data acquisition pro-\ncess, allowing models to generate their own CoT. Tech-\nniques like Quiet-STaR [39] have demonstrated how self-\ngenerated reasoning can enhance model performance with-\nout requiring manually labels. Moreover, some methods\nbased on Monte Carlo estimation have automated the pro-\ncess data collection, but they also introduce additional com-\nputational cost [19, 32]. In multimodal domain, MAVIS\n[41], a dataset consisting of 834k visual math problems an-\nnotated with short CoT, has been proposed. Other stud-\nies have distilled reasoning processes from short answers\n\n[42]. However, these machine-generated annotations are of-\nten too brief and challenging to segment semantically.\n3. Method\nIn this section, we present the details of AtomThink for\npromoting MLLMs for mathematical reasoning with slow\nthinking. As shown in Figure 2, AtomThink consists of\nthree key components, including a multimodal CoT anno-\ntation engine (Sec. 3.1), atomic step fine-tuning (Sec. 3.2),\nand policy searching (Sec. 3.3).\nThe annotation engine\nis designed to efficiently generate long CoTs to address\ndata scarcity. With sufficient data, we fine-tune MLLMs\nand train a process reward model (PRM) for incorporating\nslow thinking ability into models. Furthermore, we explore\nfour different path-wise and step-wise strategies for policy\nsearching, allowing the fine-tuned MLLM to ensure that\neach decision made during its inference contributes to the\noverall accuracy and consistency of reasoning. Finally, we\npropose an atomic capability evaluation metric in Sec. 3.4\nto measure the reasoning quality of models.\nSource\nMeta Samples\nAMATH-SFT\nAMATH-PRM\nCLEVR\n1929\n11.2k\n25k\nGeometry3K\n1201\n11.1k\n15.6k\nMAVIS\n3654\n17.7k\n30.5k\nTabMWP\n2463\n15.7k\n25.7k\nGeomVerse\n1347\n9.9k\n17k\nMathV360K\n10157\n53.5k\n24.8k\nMMMU\n76\n0.6k\n1.2k\nGeoQA+\n2082\n19.5k\n0\nIconQA\n3199\n18.1k\n0\nTotal\n26108\n157k\n130k\nTable 1. Data composition of AtomMATH.\nData\nGPT Score\nAvg. Length\nPRM800k\n84.1\n1245.4\nDirect\n1.5\n3.6\nCoT\n79.6\n670.5\nAtomMATH(Ours)\n89.4\n849.8\nTable 2. Comparison of different data styles. Our AtomMATH\nachieves the highest GPT-4o preference score.\n3.1. Multimodal CoT Annotation Engine\nGuiding MLLMs toward deep reasoning requires a sub-\nstantial amount of high-quality CoT data. However, in the\nfield of visual mathematics, the scarcity of publicly avail-\nable datasets presents a considerable challenge. To over-\ncome this, we develop an automated data engine capa-\nble of generating step-by-step long CoTs, resulting in our\nown atomic mathematical problem dataset, dubbed Atom-\nMATH. Specifically, our data engine introduces a dynamic\nprompting strategy and a semantic-level augmentation strat-\negy to produce multi-step reasoning paths.\nDynamic Prompting Strategy.\nTo overcome the compu-\ntational cost limitations associated with previous methods\nthat relied on manual annotation or process supervision, we\nexplore the possibility of driving existing models to au-\ntonomously generate high-quality reasoning data through\nsimple prompting. Inspired by recent research [9] on us-\ning prompting strategies to improve the reasoning capabil-\nities of LLMs, we propose a dynamic prompt strategy for\ngenerating atomic inference steps. Specifically, our strategy\ndrives LLMs to iteratively construct state-reasoning paths.\nEach path node represents a reasoning step and encom-\npasses the previous stage, the current state, and a possible\naction. The possible action includes continuing reasoning,\nverifying and drawing conclusion, which is determined by\nLLM itself. Unlike previous methods such as OmegaPRM\n[19] and Math-Shepherd [32] that generate a whole rea-\nsoning tree at once, our approach implicitly integrates the\nsearch over the step dimension into existing reasoning pro-\ncess through prompt engineering.\nFor each problem in-\nstance, only a single valid path is explored, eliminating the\nneed for additional process supervision computation.\nShort CoT Augmentation.\nTo fully leverage existing\nshort CoT annotations of VQA datasets, we also employ\nLLMs to atomize and augment these annotations. An ex-\nample of short CoT augmentation is provided in the supple-\nmental material. This approach allows us to semantically\nsegment an original reasoning process into multiple discrete\nsteps, and focus on solving a single atomic problem at each\nstage of the reasoning process, thereby ensuring the clarity\nand precision of our model.\nAtomMATH Dataset.\nWe sample mathematical data\nfrom Geo3k [17], Mathv360k [27], MMMU-dev [37],\nTabMWP [16], CLEVR [7], Geomverse-Cauldron [8] and\nMAVIS [41]. For Geomverse and MAVIS, we conduct short\nCoT augmentation, while the rest are generated by dynamic\nprompts to produce multi-step reasoning. Both short CoT\naugmentation and dynamic prompting are implemented by\nGPT-4o in this paper.\nAfter generating long CoTs, we\nalso use GPT-4o to double-check the answers and remove\nrollouts with incorrect responses.\nTo enable our model\nto learn atomic step-based reasoning patterns, we progres-\nsively mask each node along its reasoning path to generate\n157k atomic steps. We refer to this database as AMATH-\nSFT. Meanwhile, we sampled approximately 65k examples\nwith correct steps from AMATH-SFT, and generated nega-\ntive samples using GPT-4o to serve as PRM training data.\nTable 1 illustrates the distribution of our data. In Table 2,\nwe also evaluate the quality in a subset of 500 AtomMATH\n\nImages with human solution\nQ: What is the length of AC?\nA: AB = AC * sin(30) = AC *\n0.5, thus AC = AB * 2 = 20.\nQ: In rhombus ABCD, if AC = \n8 and BD = 6, what is the length \nof side AB?   A: 5.\nImages with short CoT\nGPT-4o\nDynamic Prompting\nShort CoT Augmentation\nS_0\nA_0\nA_1\nA_2\nA_3\nS_0\nS_1\nA_1\nA_2\nA_3\nS_0\nS_1\nA_2\nA_3\nS_2\nS_0\nS_1\nA_3\nS_2\nS_3\nMasked Queries\nAtomic Step Finetuning\nGeneral VQA \ninstructions with \nshort annotations\n157k math-targeted \ninstructions with \natomic CoT\nStep-wise Search\nPath-wise Search\nText base PRM\nMultimodal CoT Annotation\nPolicy Searching\nStep 1: The image shows a rhombus ABCD … \ndiagonals bisect each other at right angles.\nStep 2: Diagonals bisect each other. Therefore, \nAO = CO = 4 and BO = DO = 3.\nStep 5: Applying the Pythagorean theorem: \nAB2 = AO2 + BO2 = 42 + 32 = 16 + 9 = 25.\nQuestion\nQuestion\nStep 6: Take the square root of 25, thus AB = 5. \nAtomic \nData \n130k math-targeted  \npositive and \nnegative step pairs\nFigure 2. The overview of AtomThink framework. We automatically annotate the open-source data with CoT to generate atomic steps for\nfine-tuning and PRM training. During inference, step-wise or path-wise searching strategies can be applied to find optimal policies.\nsamples with GPT-4o. Result shows that our method ex-\nhibits longer information content than general CoT. Even\ncompared to the PRM800k with golden annotations, our\ndata obtained a higher preference score.\n3.2. Atomic Step Fine-Tuning\nTo fully exploit MLLMs for addressing multi-modal mathe-\nmatical problems, we conduct fine-tuning with atomic step-\nwise reasoning.\nParticularly, this process includes fine-\ntuning the MLLM on our AtomMATH dataset and learning\nthe PRM to estimate reward scores during the inference.\nMLLM Fine-Tuning.\nTo transfer MLLM to step-wise\nmathematical reasoning, we first fine-tune it within the\nframework of Markov decision process (MDP) learn-\ning.\nSpecifically, we consider the reasoning process of\nMLLM as an MDP, which can be formulated as M =\n(V, S, A, R, π).\nHere, V denotes the vocabulary, S rep-\nresents historical reasoning steps, and A corresponds to\nnext atomic step predicted by MLLM. π(a|s) represents the\nprobability of selecting an action a ∈A conditioned on a\nstate s ∈S, which is estimated by PRM to guide reasoning\nprocess. Hereby we can adopt the visual instruction tuning\ntechnique [14] to fine-tune MLLM.\nPRM Training.\nIn a slow thinking process, reasoning is\ncarried out step by step, where each atomic step provides\nan intermediate conclusion. We train the PRM to imple-\nment π(a|s) and provide feedback for every step, allow-\ning MLLMs to refine and improve its reasoning. Formally,\ngiven the description of mathematical problem q, for an ar-\nbitrary step t ≥1, the PRM predicts a probability pt of\nselecting an action a given the previous states s1:t−1 as fol-\nlows:\npt(a) = PRM ([q, s1:t−1], a) .\n(1)\nWe propose to train the PRM by minimizing the following\nbinary cross-entropy loss:\nLP RM =\nT\nX\nt=1\nyt(a) log pt(a) + (1 −yt(a)) log(1 −pt(a)),\n(2)\nwhere yt(a) denotes the ground-truth CoT annotation that\nyt = 1 if the action a is selected, otherwise yt(a) = 0.\nT is the maximum number of steps. Note that we omit to\nenumerate all possible actions in Eq. (2) for the concise\npresentation. After selecting the action at the current step\nat, we concatenate it with the previous states to construct\nst, i.e., st = s1:t−1 ∪at.\nIn this subsection, we perform atomic step fine-tuning\non the AtomMATH (including A-PRM and A-SFT subsets)\nand PRM800k dataset [11]. Moreover, we incorporate im-\nage captions into the generation of long CoT data, thus we\ncan alleviate the expensive computation burden of image\nunderstanding in MLLMs and focus on texts for supervised\nfine-tuning.\nTherefore, we post-train an LLM based on\nMath-psa [31] to evaluate the consistency of atomic texts\nand supervise fine-tuning.\n3.3. Action Search with PRM\nWith the fine-tuned MLLM capable of atomic step reason-\ning and the well-trained PRM providing feedback, we can\nnow begin the reasoning process. As there are many search\n\nstrategies to generate candidate actions, we categorize the\nexisting strategies into path-wise searching and step-wise\nsearching and explore them in our AtomThink framework.\nPath-wise Search.\nIn path-wise searching, we build upon\nprior work [28, 31] by parallel sampling multiple paths and\naggregating scores to find optimal solutions. We investigate\nthe following two strategies:\n• Majority Voting: It combines multiple reasoning paths\nby selecting the most frequent outcome across them. It\nassumes that the consensus across different paths is more\nlikely to lead to the correct answer.\n• Best-of-N: Given a generative MLLM, the best-of-N\nsampling method generates C candidate rollouts simul-\ntaneously and selects the solution with the highest score.\nThe evaluation of candidate reasoning processes is de-\ntermined by the PRM, which employs three aggregation\nmethods to map the dense scores to the overall value of\nthe entire path: 1) The worst action: Compare the worst\naction among all candidate rollouts. It penalizes solutions\nwith any weak action and is used to search a reasoning\nthat is sensitive to errors. 2) The last action: The score\nis derived from the prediction of the final answer in in-\nference. 3) Average score: It is calculated by averaging\nthe rewards of all the actions in a chain. The explainabil-\nity and consistency of intermediate reasoning are empha-\nsized here as important as the outcome.\nStep-wise Search.\nSearching strategies of this type start\nwith an initial path and incrementally expand the sampling\nspace for each atomic action.\nBeam search and greedy\nstrategies are applied to prune branches with low quality.\n• Greedy Algorithm: It focuses on making the locally op-\ntimal choice at each step of the reasoning process. It se-\nlects the best immediate action (step) based on the current\nstate, without considering future consequences.\n• Beam Search: It explores multiple branches at each ac-\ntion and maintains a fixed number of top candidates for\neach stage of reasoning. It balances between exploring\ndifferent paths and exploiting the most promising ones.\n3.4. Atomic Capability Evaluation\nSimilar to human problem-solving processes, a CoT may\ninvolve the use of multiple reasoning abilities. However,\ntraditional CoT methods do not focus on the quality of in-\ndividual reasoning steps or provide fine-grained analyses of\nthe underlying abilities. To address this gap, we have de-\nveloped an atomic capability evaluation strategy, offering a\nnew analytical perspective for slow thinking.\nOur evaluation method aims to assess the mathemati-\ncal capabilities of a target model from various perspectives,\nsuch as understanding, operations, and certifications. To\nthis end, we first need to construct a canonical set of ca-\nGPT-4o\nBehaviour Distribution\nCapbility Clustering\n· Mathematical Understanding : Variable Definition, …\n· Mathematical Operations and Reasoning: Calculations, …\n· Interpretation and Conclusion: Graphs Analysis, …\n· Verification and Synthesis: Verification, …\nMathematical Capbilities\n<image>  Question: How many squares are there? \nS0\nAtomic Action Quality Evaluation\nS1\nA1\nA2-1\nA2-2\nA2-3\n…\n…\n…\nQuality of A_1 = 2/3\nA3-1\nA3-2\nA3-3\nA4-1\nA4-2\nA4-3\nAn-1\nAn-2\nAn-3\nFigure 3. Atomic capability evaluation. The capability categories\nare derived from the clustering of GPT-4o’s behavior. By sampling\neach atomic step and evaluating the accessibility of the results, we\nassign a soft label that represents quality of an atomic step.\npabilities. As shown in Figure 3, we collect the behavior\ndistribution of GPT-4o on MathVerse [43] and perform clus-\ntering, yielding clusters that each of them represents certain\nabilities utilized by high-level intelligent models in solving\nmathematical problems. Afterward, we evaluate an atomic\naction a predicted by the target model based on outcome su-\npervision, namely, we consider that the quality of an action\ncan be reflected by the probability of it leading to a correct\nanswer. This can be formulated as the following soft esti-\nmation with K rounds of outcome supervision [11]:\ncsoft(a) =\nPK\nk=1Jˆak is correctK\nK\n,\n(3)\nwhere ˆak denotes the final answer predicted in the k-th\nround and J·K is the Iverson bracket. Consequently, we can\nevaluate all the actions of the target model and map them\nback to the capability set, to evaluate the mathematical ca-\npabilities of the target model thoroughly.\n4. Experiment\n4.1. Setup\nBaselines.\nOur\nexperiments\nutilize\ntwo\nopen-source\nMLLMs, including LLaVA-Llama3-8B [14] and EMOVA-\n8B [4]. We fine-tune only the parameters of their language\n\nStep 6: Calculate 28/𝜋𝜋using the approximate value of \n𝜋𝜋≈3.14: 𝐫𝐫≈𝟐𝟐𝟐𝟐/𝟑𝟑. 𝟏𝟏𝟏𝟏≈𝟖𝟖. 𝟗𝟗.\nStep 2: The formula for the length of an arc 𝐿𝐿in a circle \nis given by 𝑳𝑳= 𝒓𝒓∗𝜽𝜽, where 𝑟𝑟is the radius of the circle \nand 𝜃𝜃is the central angle in radians.\nQuestion: What is the length \nof the radius of the circle in \nwhich sector ABC is inscribed?\nChoices:\nA. 7.5\nB. 8.5\nC. 8.9\nD. 9.3\nAnswer: The final answer is C.\nTo find the radius of the circle in which sector ABC is inscribed, \nwe can use the properties of a 45-degree right triangle. In this \ncase, triangle AB is a right triangle with angle B being 45 \ndegrees and AB being the radius of the circle.\nSince triangle AB is a 45-degree right triangle, the sides \nopposite the 45-degree angles are equal. Therefore, if AB = 7.0, \nthen the radius of the circle (which is also the hypotenuse of the \ntriangle) can be found using the Pythagorean theorem:\nLet 𝑟𝑟be the radius of the circle.\n𝑟𝑟2 = 7.02 + 7.02\n𝑟𝑟2 = 49 + 49 = 98\n𝑟𝑟=\n98 ≈9.9\nHowever, since the choices provided are slightly different, we \nshould check the closest value to 9.9. The closest value is 9.3. \nAnswer: 9.3\nCoT Answer from GPT-4o\nAtomThink from EMOVA-8B\nStep 1: The image shows a sector of a circle, labeled \nABC, with a central angle of 45 degrees. The length \nof the arc AB is given as 7.0 units.\nStep 3: Convert the central angle from degrees to \nradians. The conversion factor is 𝜋𝜋/180. Therefore, \n45° in radians is 𝟒𝟒𝟒𝟒∗𝝅𝝅/𝟏𝟏𝟏𝟏𝟏𝟏= 𝝅𝝅/𝟒𝟒.\nStep 4: Substitute the known values into the arc length \nformula: 𝟕𝟕. 𝟎𝟎= 𝒓𝒓∗𝝅𝝅/𝟒𝟒.\nStep 5: Solve for the radius 𝑟𝑟. Rearrange the equation \nto find 𝑟𝑟: 𝒓𝒓= (𝟕𝟕. 𝟎𝟎∗𝟒𝟒)/𝝅𝝅= 𝟐𝟐𝟐𝟐/𝝅𝝅.\nStep 7: Compare the calculated radius with the given \nchoices: A. 7.5; B. 8.5; C. 8.9; D. 9.3.\nStep 8: The calculated radius 𝟖𝟖. 𝟗𝟗matches choice C.\nStep 9: To sum up, the final answer is: C.\nFigure 4. A case study of AtomThink CoT and GPT-4o generated CoT. Red and green characters denote incorrect and correct responses,\nrespectively. Our model exhibits fewer hallucinations and stronger reasoning abilities as it focuses on the quality of atomic steps. Moreover,\nour model automatically decomposes the reasoning process semantically, leading to improved readability.\nmodels and projectors with learning rates of 2e-5 and 2e-6,\nrespectively, and a batch size of 128. We select nine cutting-\nedge MLLMs for comparison, including OpenAI’s o1 [23],\n4o [21], and 4v [22], as well as LLava-NeXT-34B [13],\nInternLM-XComposer2 [41], Qwen-VL-Plus [2], LLaVA-\n7B [14], G-LLaVA-7B [5], and MAVIS-7B [41].\nDatasets.\nFor LLaVA-Llama3-8B, we use LLaVA-\n665k [14] for supervised fine-tuning (SFT) as a baseline.\nAdditionally, in LLaVA w/. Formatted and EMOVA w/.\nFormatted, we transfer the source data of AtomMATH into\nan aligned CoT format for incremental training, ensuring\na fair comparison without introducing bells and whistles.\nFor EMOVA-8B, we downsampled its publicly available\nSFT data [4] to obtain a basic post-training dataset contain-\ning about 200k samples. For models with AtomThink, the\nAMATH-SFT dataset introduced in Section 3.1, is incorpo-\nrated to introduce atomic reasoning capabilities.\nEvaluation Setting.\nWe evaluated the performance of our\nmethod on MathVista [18], a publicly available benchmark\nencompassing both general-targeted and mathematics-\ntargeted domains. Additionally, to assess the model’s ability\nto interpret mathematical graphs, we use a more challenging\nmultimodal benchmark, MathVerse [43] for further evalua-\ntion. It contains five categories including Text Lite (TL),\nText Dominant (TD), Vision Intensive (VI), Vision Domi-\nnant (VD), Vision Only (VO).\nOut evaluations include four inference settings, includ-\ning Direct, CoT, Quick Think, and Slow Think. In the\nDirect setting, we prompt the model to generate a concise\nfinal answer. In CoT, the model is instructed to answer\nthe question through step-by-step reasoning. For the Di-\nrect and CoT evaluations, we use prompts from lmms-eval\n[10, 40]. Our AtomThink-models support two additional\nsettings: Quick Think and Slow Think. In Quick Think,\nour models follow a single, atomic reasoning path based\npurely on their learned policies, without employing any sup-\nplementary search strategies. In Slow Think, enhanced by\nthe PRM, we utilize beam search with beam width of 2 and\ntemperature of 1.0, encouraging our models to engage in\nmore extensive reasoning.\n\nMathVista\nMathVerse\nModel\nInference\nGeneral\nMath\nTotal\nTL\nTD\nVI\nVD\nVO\nTotal\nRandom Choice\n-\n-\n-\n17.9\n12.4\n12.4\n12.4\n12.4\n12.4\n12.4\nHuman\n-\n-\n-\n-\n70.9\n71.2\n61.4\n68.3\n66.7\n66.7\nOpenAI o1\nSlow Think*\n-\n-\n73.9\n-\n-\n-\n-\n-\n-\nGPT-4o\nCoT\n-\n-\n63.8\n-\n-\n-\n-\n-\n-\nGPT-4V\nCoT\n-\n-\n49.9\n56.6\n63.1\n51.4\n50.8\n50.3\n54.4\nLLaVA-NeXT-34B\nDirect\n-\n-\n46.5\n25.5\n33.8\n23.5\n20.3\n15.7\n23.8\nInternLM-XComposer2\nDirect\n-\n-\n57.6\n17.0\n22.3\n15.7\n16.4\n11.0\n16.5\nQwen-VL-Plus\nDirect\n-\n-\n43.3\n11.1\n15.7\n9.0\n13.0\n10.0\n11.8\nLLaVA-1.5-13B\nDirect\n-\n-\n27.6\n15.2\n19.4\n16.8\n15.2\n11.3\n15.6\nG-LLaVA-7B\nDirect\n-\n-\n53.4\n20.7\n20.9\n17.2\n14.6\n9.4\n16.6\nMAVIS-7B\nDirect\n-\n-\n-\n29.1\n41.4\n27.4\n24.9\n14.6\n27.5\nLLaVA-Llama3-8B\nDirect\n34.1\n25.6\n29.5\n16.0\n19.3\n16.4\n13.1\n15.0\n15.9\nLLaVA w/. Formatted\nCoT\n30.2\n22.9\n26.3\n14.3\n18.4\n15.7\n10.0\n7.7\n13.2\nAtomThink-LLaVA\nDirect\n34.4\n27.2\n30.5\n16.0\n19.3\n16.2\n13.1\n15.0\n15.9\nAtomThink-LLaVA\nQuick Think\n36.9\n37.0\n36.6\n22.2\n26.6\n24.1\n20.9\n17.9\n22.4\nAtomThink-LLaVA\nSlow Think\n36.5\n41.3\n39.1\n36.1\n42.4\n30.0\n36.8\n28.6\n34.7\nEMOVA-8B-200k\nDirect\n52.4\n51.1\n51.7\n34.4\n39.0\n33.4\n30.1\n23.5\n32.1\nEMOVA w/. Formatted\nCoT\n30.9\n31.3\n31.1\n26.5\n36.5\n25.3\n20.4\n19.8\n25.7\nAtomThink-EMOVA\nDirect\n53.9\n52.4\n53.1\n33.6\n39.0\n33.8\n28.0\n24.4\n31.8\nAtomThink-EMOVA\nQuick Think\n48.7\n54.4\n51.8\n36.5\n42.4\n34.1\n32.9\n29.7\n35.1\nAtomThink-EMOVA\nSlow Think\n48.9\n57.0\n53.3\n42.1\n51.5\n39.0\n36.7\n33.1\n40.5\nTable 3. Comparison of accuracy with state-of-the-art methods on MathVista and MathVerse. Our AtomThink LLaVA outperforms the\nbaseline in all sub-tasks across two benchmarks, achieving an average improvement of 14.2%. Meanwhile, AtomThink EMOVA, with only\n8B parameters, surpasses the larger LLaVA-NEXT-34B and even is comparable to GPT-4V.\n4.2. Main Results\nComparison with existing MLLMs.\nIn Table 3, our\nAtomThink framework is applied to train LLaVA-Llama3-\n8B and EMOVA-8B, yielding consistent performance im-\nprovements over the original models. When combined with\nPRM, AtomThink-EMOVA achieves a new state-of-the-art\non MathVerse, surpassing GPT-4o and narrowing the gap\nbetween MLLMs and human performance. On MathVista,\nit also achieves performance close to that of GPT-4o. These\nresults demonstrate the framework’s strong generalization\ncapability and practical usability.\nQuick Think with Intuition.\nUnlike traditional CoT\nmethods, Quick Think generates a stepwise reasoning path\nthrough multi-turn conversations, bypassing the need for an\nadditional verifier. This approach offers a computational\nadvantage over Slow Think and highlights the model’s\nintuitive reasoning capabilities.\nFor LLaVA-Llama3-8B,\nour AtomThink framework surpasses the baseline model,\nachieving approximately a 10% improvement on Math-\nVista [18] and a 19% improvement on MathVerse [43].\nFor AtomThink-EMOVA, Quick Think achieved a score of\n38.3% on MathVerse, outperforming existing open-source\nMLLMs. These results demonstrate that when a model pos-\nsesses atomic reasoning capabilities, it can leverage rapid\nintuition to perform more accurate mathematical reasoning.\nLLM Effectively Supervise Visual Reasoning Processes.\nPrevious work has shown that process supervision reward\nmodels are effective in evaluating intermediate reasoning\nsteps, though these methods have been primarily applied\nwithin the domain of language models. We fine-tuned an\nLLM with A-MATH-PRM and applied it for test-time scal-\ning. As shown in the table, AtomThink-EMOVA, when uti-\nlizing PRM with beam search, achieved an additional 2%\nimprovement on MathVista [18] compared to Quick Think.\nIn MathVerse [43], it even outperformed the closed-source\nmodel GPT-4V by 1%. Additionally, increasing test-time\nscaling in LLAVA resulted in substantial improvements, po-\nsitioning it well above its sibling model, LLAVA-1.5-13B.\nWe find that even when the reasoning process heavily re-\nlies on visual dominant inputs, our models can avoid tak-\ning incorrect paths by improving text decoding.\nOn the\none hand, it is attributed to the AtomThink training process,\nwhich encourages MLLM to first understand image before\n\nreasoning. On the other hand, it also confirms the effective-\nness of test-time extension in multimodal tasks.\nTrade-off between General and Math Ability.\nSimilar\nto the conclusions reported in o1, we observe that MLLMs\nbecome weaker on general tasks that rely on world knowl-\nedge during deep contemplation, demonstrating a trade-off\nbetween higher-level reasoning and direct thinking. For in-\nstance, LLaVA-Llama3-8B presents a decline in accuracy of\n7% compared to the baseline on the general subset of Math-\nVista, while EMOVA experiences a 17% reduction. How-\never, after applying PRM-based action search, both models\nare able to narrow this generalization gap and improve ac-\ncuracies by 4% and 16%, respectively.\n4.3. Atomic Ability Analysis\nWe first cluster the reasoning behaviors of GPT-4o into\na set of capabilities S, including Approximation, Verifi-\ncation, Calculation, Variable Definition, Geometric Rea-\nsoning, Conclusion Drawing, Graphs Analysis, Equation\nFormulation, Image Description, Knowledge Introduction,\nInformation Extraction, and Formula Derivation.\nUsing\nqueries in MathVerse [43], we constructed 500 current\nstates as si with high-quality responses generated by GPT-\n4o. Subsequently, soft estimations of atomic actions are\nmapped to S for analysis.\nAbility Analysis.\nFigure 3 illustrates the distribution\nof atomic behaviors and capability differences between\nLLAVA-llama3-8b and EMOVA-8B with their AtomThink-\nversions. The analysis reveals that AtomThink-Model gen-\nerally outperforms baseline across most abilities, demon-\nstrating higher scores in areas such as Image Description\nand Verification. It suggests that our model is capable of\nmore accurate analysis of visual information and demon-\nstrates a degree of self-checking and reflective capability.\n4.4. Comparison with g1\nIn Figure 5, we compare AtomThink with the state-of-the-\nart open-source inference strategy, g11, which employs dy-\nnamic prompting to make model focus on single step re-\nflection.\nIn GPT-4o, direct application of g1 for multi-\nturn reasoning yields a greater improvement over Chain-of-\nThought, particularly in numeric and geometric tasks. How-\never, due to the reliance on the inherent reasoning capabili-\nties of large-scale language models, its performance signifi-\ncantly degrades on smaller models such as EMOVA-8B and\nLLaVA-Llama3-8B. In contrast, our AtomThink framework\nconsistently enhances the performance of these MLLMs.\n1https://github.com/bklieger-groq/g1\nStatistical\nScientific\nAlgebraic\nArithmetic\nGeometry\nNumeric\nLogical\n0\n20\n40\n60\n80\nAccuracy (%)\nGPT-4o\n+CoT\n+g1\nStatistical\nScientific\nAlgebraic\nArithmetic\nGeometry\nNumeric\nLogical\n0\n20\n40\n60\n80\nAccuracy (%)\nEMOVA\n+g1\n+AtomThink\nStatistical\nScientific\nAlgebraic\nArithmetic\nGeometry\nNumeric\nLogical\n0\n20\n40\n60\n80\nAccuracy (%)\nLLaVa-LLaMa3\n+g1\n+AtomThink\nFigure 5. Comparison to CoT and g1 in MathVista subsets. In\ncontrast to the declining trend observed in g1, AtomThink outper-\nforms the baseline across most subsets.\n4.5. Action Search Ablation\nIn Table 4, we evaluate the impact of direct output, path-\nwise search and action-wise search strategies on Math-\nVista [18] and MathVerse [43] using a subset of 200 sam-\nples from each.\nResults show that even without addi-\ntional computation, AtomThink-EMOVA’s direct prediction\naccuracy outperforms the original, with improvements of\n1.3%, 1.52%, and 2.4%, respectively. The path-wise search\nmethod, BoN-Avg, achieves the highest accuracy of 58.68%\non the MathVista [18] mathematical tasks, although it ex-\nperienced a drop on general problems. Meanwhile, both\ngreedy algorithm and beam search show balanced perfor-\nmance across all benchmarks, with the generalization gap\nbetween math and general tasks being notably smaller than\nthat of path-wise search.\nModel\nMethod\nMathVista-M\nMathVista-G\nMathVerse\nEMOVA-200k\nDirect\n51.1\n52.4\n33.3\nAtomThink\nDirect\n52.4\n53.9\n35.7\nQuick Think\n54.2\n46.7\n38.1\nw/. Path-wise\nMajority Voting\n48.8\n49.4\n39.0\nBoN-Last\n51.2\n46.8\n41.3\nBoN-Avg\n58.7\n40.5\n38.7\nBoN-Min\n53.7\n53.2\n40.0\nw/. Step-wise\nGreedy\n46.3\n45.6\n38.3\nBeam Search\n57.1\n53.2\n45.3\nTable 4. Ablation study on Path-wise and step-wise search. The re-\nsults show that both Best-of-N-Min(BoN-Min) and Beam Search\nexhibit consistent performance improvements.\nLimitation. Due to the limitations in computing infras-\n\ntructure, we are unable to validate our method on larger\nMLLMs.\nAdditionally, despite undergoing small-scale\nmanual review, our dataset still lacks step-level golden an-\nswers, which may introduce noise into training.\n5. Conclusion\nThis paper introduces atom thinking capabilities to MLLMs\nfor solving visual mathematics problems.\nWe release\na high-quality, human-free annotated long-CoT dataset,\nAtomMATH, consisting of 157k atomic reasoning steps and\n130k corresponding annotations. Furthermore, we propose\nAtomThink, a novel framework that focuses on the quality\nof atomic steps. The experimental results demonstrate that\nour method consistently enhances the model’s diverse be-\nhaviors during the problem-solving process, leading to im-\nproved reasoning performance across various multimodal\nmathematical tasks. This work paves the way for develop-\ning generalized slow-thinking models.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-\nsch, Katherine Millican, Malcolm Reynolds, et al. Flamingo:\na visual language model for few-shot learning.\nAdvances\nin neural information processing systems, 35:23716–23736,\n2022. 2\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A versatile vision-language model for un-\nderstanding, localization, text reading, and beyond. arXiv\npreprint arXiv:2308.12966, 1(2):3, 2023. 6\n[3] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald\nClark, Quoc V Le, Christopher R´e, and Azalia Mirhoseini.\nLarge language monkeys: Scaling inference compute with\nrepeated sampling. arXiv preprint arXiv:2407.21787, 2024.\n2\n[4] Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin\nTan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo\nYang, et al.\nEmova:\nEmpowering language models to\nsee, hear and speak with vivid emotions.\narXiv preprint\narXiv:2409.18042, 2024. 5, 6\n[5] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun\nZhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu,\nZhenguo Li, and Lingpeng Kong. G-llava: Solving geomet-\nric problem with multi-modal large language model, 2023.\n6\n[6] Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang\nLiu, Aiwei Liu, Xuming Hu, and Lijie Wen. Interpretable\ncontrastive monte carlo tree search reasoning. arXiv preprint\narXiv:2410.01707, 2024. 1\n[7] Justin\nJohnson,\nBharath\nHariharan,\nLaurens\nVan\nDer Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross\nGirshick.\nClevr: A diagnostic dataset for compositional\nlanguage and elementary visual reasoning. In CVPR, pages\n2901–2910, 2017. 3\n[8] Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu,\nXi Chen, and Radu Soricut. Geomverse: A systematic eval-\nuation of large models for geometric reasoning. In CVPRW.\n3\n[9] Benjamin Klieger. g1: Using llama-3.1 70b on groq to create\no1-like reasoning chains, 2024. 3\n[10] Bo Li, Peiyuan Zhang, Kaichen Zhang, Xinrun Du Fanyi Pu,\nYuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang,\nChunyuan Li, and Ziwei Liu. Lmms-eval: Accelerating the\ndevelopment of large multimodal models, 2024. 6\n[11] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Ed-\nwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman,\nIlya Sutskever, and Karl Cobbe. Let’s verify step by step. In\nICLR. 2, 4, 5\n[12] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Ed-\nwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman,\nIlya Sutskever, and Karl Cobbe. Let’s verify step by step.\narXiv preprint arXiv:2305.20050, 2023. 2\n[13] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee.\nLlava-next: Im-\nproved reasoning, ocr, and world knowledge, 2024. 6\n[14] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. NeurIPS, 36, 2024. 4, 5, 6\n[15] Wentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong\nLi, Jiayi Zeng, Mengliang He, Qin Chen, Bo Jiang, Aimin\nZhou, et al. Mathematical language models: A survey. arXiv\npreprint arXiv:2312.07622, 2023. 2\n[16] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-\nChun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin\nKalyan. Dynamic prompt learning via policy gradient for\nsemi-structured mathematical reasoning. In ICLR. 3\n[17] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang,\nXiaodan Liang, and Song-chun Zhu. Inter-gps: Interpretable\ngeometry problem solving with formal language and sym-\nbolic reasoning. In ACL, pages 6774–6786, 2021. 3\n[18] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li,\nHannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel\nGalley, and Jianfeng Gao. Mathvista: Evaluating math rea-\nsoning in visual contexts with gpt-4v, bard, and other large\nmultimodal models. arXiv e-prints, pages arXiv–2310, 2023.\n6, 7, 8\n[19] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale,\nHarsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng,\nJiao Sun, et al.\nImprove mathematical reasoning in lan-\nguage models by automated process supervision.\narXiv\npreprint:2406.06592, 2024. 2, 3\n[20] Minheng Ni, Yutao Fan, Lei Zhang, and Wangmeng\nZuo. Visual-o1: Understanding ambiguous instructions via\nmulti-modal multi-turn chain-of-thoughts reasoning. arXiv\npreprint arXiv:2410.03321, 2024. 2\n[21] OpenAI. Gpt-4o system card, . 2, 6\n[22] OpenAI. Gpt-4v(ision) system card, . 6\n[23] OpenAI. Openai o1 system card, . 1, 6\n[24] Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv,\nWenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong,\net al. Cogcom: Train large vision-language models diving\ninto details through chain of manipulations. arXiv preprint\narXiv:2402.04236, 2024. 2\n\n[25] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia,\nZhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi\nLi, et al. O1 replication journey: A strategic progress report–\npart 1. arXiv preprint arXiv:2410.18982, 2024. 1\n[26] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuo-\nfan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual\ncot: Unleashing chain-of-thought reasoning in multi-modal\nlanguage models. arXiv preprint arXiv:2403.16999, 2024. 2\n[27] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang,\nSee Kiong Ng, Lidong Bing, and Roy Lee. Math-llava: Boot-\nstrapping mathematical reasoning for multimodal large lan-\nguage models. pages 4663–4680, 2024. 3\n[28] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Ku-\nmar. Scaling llm test-time compute optimally can be more\neffective than scaling model parameters.\narXiv preprint\narXiv:2408.03314, 2024. 2, 5\n[29] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis\nSong, Noah Siegel, Lisa Wang, Antonia Creswell, Geof-\nfrey Irving, and Irina Higgins. Solving math word problems\nwith process-and outcome-based feedback. arXiv preprint\narXiv:2211.14275, 2022. 2\n[30] Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie\nHe, Shuicheng Yan, and Bo An. Q*: Improving multi-step\nreasoning for llms with deliberative planning. arXiv preprint\narXiv:2406.14283, 2024. 1\n[31] Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen\nZhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Li-\nonel M Ni, et al. Openr: An open source framework for ad-\nvanced reasoning with large language models. arXiv preprint\narXiv:2410.09671, 2024. 1, 4, 5\n[32] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai\nDai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-\nshepherd: Verify and reinforce llms step-by-step without hu-\nman annotations. In ACL, pages 9426–9439, 2024. 2, 3\n[33] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed\nChi, Sharan Narang, Aakanksha Chowdhery, and Denny\nZhou. Self-consistency improves chain of thought reason-\ning in language models. arXiv preprint arXiv:2203.11171,\n2022. 2\n[34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in neural information processing\nsystems, 35:24824–24837, 2022. 1, 2\n[35] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom\nGriffiths, Yuan Cao, and Karthik Narasimhan.\nTree of\nthoughts: Deliberate problem solving with large language\nmodels.\nAdvances in Neural Information Processing Sys-\ntems, 36, 2024. 1\n[36] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,\nTong Xu, and Enhong Chen. A survey on multimodal large\nlanguage models. arXiv preprint arXiv:2306.13549, 2023. 2\n[37] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi\nLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, et al. Mmmu: A massive multi-discipline\nmultimodal understanding and reasoning benchmark for ex-\npert agi. In CVPR, pages 9556–9567, 2024. 3\n[38] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.\nStar: Bootstrapping reasoning with reasoning. Advances in\nNeural Information Processing Systems, 35:15476–15488,\n2022. 2\n[39] Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri,\nNick Haber, and Noah D Goodman. Quiet-star: Language\nmodels can teach themselves to think before speaking. arXiv\npreprint arXiv:2403.09629, 2024. 2\n[40] Kaichen\nZhang,\nBo\nLi,\nPeiyuan\nZhang,\nFanyi\nPu,\nJoshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan\nZhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-\neval: Reality check on the evaluation of large multimodal\nmodels, 2024. 6\n[41] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu\nGuo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei,\nShanghang Zhang, et al.\nMavis: Mathematical visual in-\nstruction tuning. arXiv e-prints, pages arXiv–2407, 2024. 2,\n3, 6\n[42] Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang,\nZhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and\nYiming Yang.\nImprove vision language model chain-of-\nthought reasoning. arXiv preprint arXiv:2410.16198, 2024.\n2, 3\n[43] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin,\nZiyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei\nChang, Yu Qiao, et al. Mathverse: Does your multi-modal\nllm truly see the diagrams in visual math problems?\nIn\nEuropean Conference on Computer Vision, pages 169–186.\nSpringer, 2025. 5, 6, 7, 8\n[44] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei\nWang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\nZhang, Zican Dong, et al. A survey of large language mod-\nels. arXiv preprint arXiv:2303.18223, 2023. 1, 2",
    "pdf_filename": "AtomThink_A_Slow_Thinking_Framework_for_Multimodal_Mathematical_Reasoning.pdf"
}