{
    "title": "Enhanced Sign Language Translation between American Sign Language (ASL) and Indian Sign Language (IS",
    "context": "provide a bridge between the users of American Sign Language and the users of spoken language and Indian Sign Language (ISL). The research enabled us to create a novel framework that we have developed for Learner Systems. Leveraging art of Large models to create key features including: - Real-time translation between these two sign languages in an efficient manner. Making LLM’s capability available for seamless translations to ISL. Here is the full study showing its implementation in this paper. The core of the system is a sophisticated pipeline that begins with reclassification and recognition of ASL gestures based on a strong Random Forest Classifier. By recognizing the ASL, it is translated into text which can be more easily processed. Highly evolved natural language NLP (Natural Language Processing) techniques come in handy as they play a role in our LLM integration where you then use LLMs to be able to convert the ASL text to ISL which provides you with the intent of sentence or phrase. The final step is to synthesize the translated text back into ISL gestures, creating an end-to-end translation experience using RIFE-Net. This framework is tasked with key challenges such as automatically dealing with gesture variability and overcoming the linguistic differences between ASL and ISL. By automating the translation process, we hope to vastly improve accessibility for sign language users. No longer will the communication gap between ASL and ISL create barriers; this totally cool innovation aims to bring our communities closer together. And we believe, with full confidence in our framework, that we’re able to apply the same principles across a wide variety of sign language dialects. Index Terms—Sign Language Recognition, ASL to ISL Trans- lation, Large Language Models (LLMs), Natural Language Processing (NLP), Random Forest Classifier, Gesture Reclassi- fication, Text-to-Gesture Synthesis, RIFE-Net, Real-Time Trans- lation, Sign Language Variability, Linguistic Adaptation, Assis- tive Technology, Cross-Linguistic Framework, Sign Language Dialects, Accessibility and Inclusivity T HE communication gap between the users of American Sign Language (ASL) and Indian Sign Language (ISL) is a significant challenge to intercultural interaction and ac- cessibility in the deaf community. Sign languages are indis- pensable weapons of expression for deaf individuals, but the lack of interoperability between ASL and ISL limits smooth communication across various linguistic and cultural frontiers. This challenge can be addressed through innovative strategies that leverage advances in machine translation technology and contemporary deep learning frameworks. The central goal of this research work is to build a holistic framework for machine translation that can efficiently and conveniently translate ASL gestures into ISL gestures, hence facilitating communication between ASL users and ISL users. Toward this objective, we hereby propose an approach that combines image recognition techniques with advanced language processing algorithms gov- erned by Large Language Models. Instead of using traditional CNN- based recognition techniques, LLM-driven techniques are used to decode ASL gestures and, in turn, translate directly into meaningful textual representations. From this step of converting ASL gestures to text, we establish an intermediate that helps use advanced LLM-based techniques for machine translation. Now, we can translate the recognized English text into ISL gestures by preserving linguistic aspects and cultural context. The application of LLMs for translation will help make the process more accurate, context-sensitive, and adaptable and, thus, would be used to bridge between ASL and ISL in a nearly seamless manner. A. Leveraging Deep Learning for Gesture Recognition and Translation Sign language communication involves complex expres- sions that carry a certain range of linguistic and cultural nuances in communication. Conventional sign language under- standing and translation techniques rely mostly on fixed data sets and prebuilt models. Such approaches find it difficult to adapt to the dynamic nature of sign languages. In the paper, the constraints were overcome using Random Forest Classifiers for effective gesture recognition followed by Large Language Models to assist in context-aware translation. The proposed framework, therefore, would represent a breakthrough with the integration of real-time processing capabilities and cultural contextualization. The significance of this work is that it introduces an interme- diate text-based representation that forms the connecting link between recognition and synthesis of gestures. This linguistic intermediate allows for not only a more literal translation but also allows the tailoring of the translation method in such a way as to keep its intent and cultural nuances of the original expression. This model would become especially important to the reduction of the linguistic differences between ASL and ISL, such as differing grammatical structures, word orders, and contextual expressions. Further, generating ISL gestures from translated text with the assistance of RIFE-Net produces a smooth and natural gesture presentation. Such a robust ability of RIFE-Net to effectively handle high variability within gesture sequences leads it to accurately reproduce ISL gestures even while making forward-looking predictions of potentially arXiv:2411.12685v1  [cs.CL]  19 Nov 2024",
    "body": "1\nEnhanced Sign Language Translation between\nAmerican Sign Language (ASL) and Indian Sign\nLanguage (ISL) Using LLMs\nMalay Kumar, S. Sarvajit Visagan, Tanish Sarang Mahajan, and Anisha Natarajan\nAbstract—We have come up with a research that hopes to\nprovide a bridge between the users of American Sign Language\nand the users of spoken language and Indian Sign Language\n(ISL). The research enabled us to create a novel framework that\nwe have developed for Learner Systems. Leveraging art of Large\nmodels to create key features including: - Real-time translation\nbetween these two sign languages in an efficient manner. Making\nLLM’s capability available for seamless translations to ISL. Here\nis the full study showing its implementation in this paper. The\ncore of the system is a sophisticated pipeline that begins with\nreclassification and recognition of ASL gestures based on a strong\nRandom Forest Classifier. By recognizing the ASL, it is translated\ninto text which can be more easily processed. Highly evolved\nnatural language NLP (Natural Language Processing) techniques\ncome in handy as they play a role in our LLM integration\nwhere you then use LLMs to be able to convert the ASL text to\nISL which provides you with the intent of sentence or phrase.\nThe final step is to synthesize the translated text back into\nISL gestures, creating an end-to-end translation experience using\nRIFE-Net. This framework is tasked with key challenges such as\nautomatically dealing with gesture variability and overcoming\nthe linguistic differences between ASL and ISL. By automating\nthe translation process, we hope to vastly improve accessibility\nfor sign language users. No longer will the communication gap\nbetween ASL and ISL create barriers; this totally cool innovation\naims to bring our communities closer together. And we believe,\nwith full confidence in our framework, that we’re able to apply\nthe same principles across a wide variety of sign language\ndialects.\nIndex Terms—Sign Language Recognition, ASL to ISL Trans-\nlation, Large Language Models (LLMs), Natural Language\nProcessing (NLP), Random Forest Classifier, Gesture Reclassi-\nfication, Text-to-Gesture Synthesis, RIFE-Net, Real-Time Trans-\nlation, Sign Language Variability, Linguistic Adaptation, Assis-\ntive Technology, Cross-Linguistic Framework, Sign Language\nDialects, Accessibility and Inclusivity\nI. INTRODUCTION\nT\nHE communication gap between the users of American\nSign Language (ASL) and Indian Sign Language (ISL)\nis a significant challenge to intercultural interaction and ac-\ncessibility in the deaf community. Sign languages are indis-\npensable weapons of expression for deaf individuals, but the\nlack of interoperability between ASL and ISL limits smooth\ncommunication across various linguistic and cultural frontiers.\nThis challenge can be addressed through innovative strategies\nthat leverage advances in machine translation technology and\ncontemporary deep learning frameworks. The central goal of\nthis research work is to build a holistic framework for machine\ntranslation that can efficiently and conveniently translate ASL\ngestures into ISL gestures, hence facilitating communication\nbetween ASL users and ISL users. Toward this objective, we\nhereby propose an approach that combines image recognition\ntechniques with advanced language processing algorithms gov-\nerned by Large Language Models. Instead of using traditional\nCNN- based recognition techniques, LLM-driven techniques\nare used to decode ASL gestures and, in turn, translate directly\ninto meaningful textual representations. From this step of\nconverting ASL gestures to text, we establish an intermediate\nthat helps use advanced LLM-based techniques for machine\ntranslation. Now, we can translate the recognized English\ntext into ISL gestures by preserving linguistic aspects and\ncultural context. The application of LLMs for translation will\nhelp make the process more accurate, context-sensitive, and\nadaptable and, thus, would be used to bridge between ASL\nand ISL in a nearly seamless manner.\nA. Leveraging Deep Learning for Gesture Recognition and\nTranslation\nSign language communication involves complex expres-\nsions that carry a certain range of linguistic and cultural\nnuances in communication. Conventional sign language under-\nstanding and translation techniques rely mostly on fixed data\nsets and prebuilt models. Such approaches find it difficult to\nadapt to the dynamic nature of sign languages. In the paper, the\nconstraints were overcome using Random Forest Classifiers\nfor effective gesture recognition followed by Large Language\nModels to assist in context-aware translation. The proposed\nframework, therefore, would represent a breakthrough with\nthe integration of real-time processing capabilities and cultural\ncontextualization.\nThe significance of this work is that it introduces an interme-\ndiate text-based representation that forms the connecting link\nbetween recognition and synthesis of gestures. This linguistic\nintermediate allows for not only a more literal translation but\nalso allows the tailoring of the translation method in such a\nway as to keep its intent and cultural nuances of the original\nexpression. This model would become especially important to\nthe reduction of the linguistic differences between ASL and\nISL, such as differing grammatical structures, word orders,\nand contextual expressions. Further, generating ISL gestures\nfrom translated text with the assistance of RIFE-Net produces\na smooth and natural gesture presentation. Such a robust ability\nof RIFE-Net to effectively handle high variability within\ngesture sequences leads it to accurately reproduce ISL gestures\neven while making forward-looking predictions of potentially\narXiv:2411.12685v1  [cs.CL]  19 Nov 2024\n\n2\ncomplex or subtle translations. With advanced recognition,\ntranslation, and synthesis modules integrated, the framework\nplaces it at the forefront of a cutting-edge tool in the domain\nof sign language translation.\nB. Towards a Multi-dialectal Future\nThough designed specifically for ASL and ISL, underpin-\nning principles and methodologies are adaptable to support\nmore than those targeted sign language dialects. By adapting\nthe system to use a variety of datasets, it follows that it is vi-\nable to address the diversity in sign languages globally, thereby\npaving the course for universal sign language interoperability.\nMoreover, this adaptability brings out the scale in using LLMs\nfor translation purposes as it opens up broader applications in\nassistive technologies.\nThis research promises to work out an integrated approach\ncombining gesture recognition, natural language processing,\nand synthesis of gestures to bridge the overall communication\ngap so present in sign language communities. This is an\nadvance both in the technical and importance toward furthering\nthe aspects of inclusivity and exchange across the world’s deaf\ncommunity.\nII. LITERATURE SURVEY\nRecognition and translation of sign languages have attracted\na lot of focus in the recent past because of their pivotal role in\nimproving communication between deaf and mute people. Pre-\nsenting an overview of the literature in this field, this literature\nreview identifies methodologies, techniques, and challenges in\nsign language recognition and translation systems.\nRao et al. (2023) introduced a system of how people with\nmany spoken languages utter sign language using the Natural\nLanguage Toolkit (NLTK) and argued that a critical factor in\nsign language translation is linguistic processing [1]. A popular\nreview carried out by Al-Qureshi et al. (2023) focused on\nthe DP of recognizing sign languages through deep learning\napproaches, including challenges and recent milestones [2].\nSalian et al. (2017) also presented a system for sign language\nrecognition to set the background for subsequent work in this\ndomain [3].\nIn this Article, Mubashira and James 2020 proposed a\nTransformer Network for video-to-text translation proving\nthe transformers effective in sign language [4]. Halder and\nTayade (2021) in a recent study that designed a real-time\nvernacular sign language recognition system using media pipe\nand machine learning propose the possibility of real-time\napplications in this field [5]. Also, in Korean sign language\nrecognition, Shin et al. (2023) proposed a Transformer-Based\nDeep Neural Network and demonstrated how deep learning\ncan be applied for sign language recognition irrespective of\nthe type of sign language [6].\nSaleem et al. (2023) provided an outstanding, machine\nlearning-based, full-duplex sign language communication sys-\ntem capable of translating multiple sign languages; the prob-\nlem of sign language communication technology for the deaf\ncommunity has received continued attention [35]. Wang et\nal. (2022) proposed an improved 3D-Res-Net sign language\nrecognition algorithm with clearer hand features and improved\nthe state of the art of gesture recognition [7].\nIn their paper, Sahoo et al. (2014) gave a detailed account of\nthe recent approaches to recognizing sign languages, and major\nissues [8]. Pathan et al. Pathan, Muhammad, and Zhang (2023)\ndeveloped a fusion approach for sign language recognition\nfrom both image and hand landmarks using a multi-headed\nconvolutional neural network demonstrating how multiple-\nmodal can enhance gesture recognition systems [9].\nShenoy et al. (2018) have proposed real-time ISL recogni-\ntion using grid-based feature extraction and machine learning\nto address the requirements of the Indian deaf and Hard of\nhearing community [10]. Sharma and Singh (2022) investi-\ngated NLP in the general mapping of speech to ISL with\ninsights towards a hybrid way of sign language translation\n[11].\nIn addition, while Badhe and Kulkarni (2015) developed an\nIndian sign language translator based on gesture recognition\nalgorithms, they pointed out that feature extraction and motion\ndetection remain decisive factors in designing sign language\nrecognition systems [12].\nFurthermore, Jia Gong (2024), the author was motivated\nby the remarkable translation performance of LLMs and pro-\nposed a method to incorporate off-the-shelf LLMs to address\ncomplex Sign Language Translation Tasks [38]. ZhiGang Chen\n(2024) thought exploring the gloss-free methods is much more\ncrucial because it will greatly decrease the annotation time\nand promote the dependence of more accurate and universal\nsign language translation systems [33]. Sen Fang then unveiled\nSIGNLLM which is the first large-scale multilingual SLP\nmodel created using the prompt2 sign dataset which generates\nthe skeletal poses of sign language from text or prompt for\neight languages [34].\nIII. PROPOSED METHODOLOGY\nThis research presents a novel methodology to bridge the\ncommunication gap between American Sign Language (ASL)\nand Indian Sign Language (ISL) through an automated trans-\nlation system enhanced using Large language models (LLMs).\nThe proposed approach comprises three interconnected phases:\nThe sign language recognition phase that utilizes a hybrid en-\nsemble model, the Recognized Text Correction phase through\nlanguage model enhancement, and the Video synthesis phase\nwith motion smoothing. Each phase has been meticulously\ndesigned to address specific challenges in cross-sign-language\ntranslation while maintaining semantic accuracy and natural\ngesture flow.\nA. Sign Language Recognition\nThe initial phase employs a hybrid ensemble approach\ncombining the complementary strengths of a Random Forest\nClassifier (RFC) and a Convolutional Neural Network (CNN).\nThis RFC+CNN model architecture allows robust feature\nextraction and classification of the sign language and provides\nbetter prediction accuracy.\n\n3\n1) Random Forest Classifier Model: The Random Forest\nClassifier component focuses on the skeletal approach which\nsets a baseline for sign language recognition, it uses the hand-\ntracking capabilities of the mediapipe framework for feature\nextraction.\nDataset Preparation: For the above model we prepare an image\ndataset with a sample size of 2800 images (28 distinct classes\n* 100 images)\n• 26 alphabetic characters (100 images per character).\n• Additional classes for ’DELETE’ and ’SPACE’ opera-\ntions.\n• A total of 28 distinct classes.\nTABLE I\nDATASET DISTRIBUTION AND ACQUISITION PARAMETERS\nParameter\nSpecification\nJustification\nImage Reso-\nlution\n32×32 pixels\nOptimal balance between\ndetail\npreservation\nand\ncomputational efficiency\nLighting\nConditions\nAmbient, Direct, Shadow\nEnsures robustness across\nvarious lighting scenarios\nBackground\nVariation\nBackground\nclutter,\nob-\njects\nImproves model general-\nization\nCamera An-\ngles\n0°, ±15°, ±30°\nAccounts for natural vari-\nation in hand positioning\nCrop Focus\nHand region\nEliminates\nirrelevant\nbackground information\nFig. 1. American Sign Language dataset sample\nFeature extraction: With the mediapipe framework we extract\n42 prominent landmark points (L) placed on the hand which\nserve as the features for training the model.\nL = {(xi, yi, zi) | i ∈[1, 42]}\nThe coordinates obtained are then processed for normalization\nusing StandardScaler.\nxnormalized = x −µx\nσx\nwhere µx and σx represent the mean and standard deviation\nof the feature distribution respectively.\nModel\nArchitecture\nand\nTraining:\nWe\nuse\nthe\nRandomForestClassifer\n(RFC)\nmodel\nthat\nemploys\nan\nTABLE II\nRFC HYPERPARAMETER CONFIGURATION\nParameter\nSearch Space\nBest\nValue\nImpact\nn estimators\n[100, 200, 300]\n200\nBalances complexity\nand accuracy\nmax depth\n[None,\n10,\n20,\n30]\n20\nControls overfitting\nmin samples split [2, 5, 10]\n5\nEnsures robust\nnode splitting\nmin samples leaf\n[1, 2, 4]\n2\nPrediction stability\nbootstrap\n[True, False]\nTrue\nEnables varied tree\nconstruction\noptimized through the GridSearch Algorithm.\nThe model’s decision function for class prediction can be\nexpressed as:\nˆy = mode ˆyt(x)t = 1T\nwhere ˆyt(x) represents the prediction of the t-th tree, and T\nis the total number of trees.\n2) Convolutional Neural Network Model: For the convo-\nlutional neural network (CNN) component we use silhouette\nimages providing complementary visual feature analysis to the\nRFC’s landmark-based approach.\nSilhouette Images: It is a type of image where a dark image\nof a subject against a lighter background, usually showing the\nsubject’s profile.\nUsing silhouette images enables a focus on the intrinsic\nstructure of the hand pose, effectively eliminating background\nnoise, lighting variations, and other environmental influences.\nThis approach isolates the key features of the hand’s position\nand configuration, providing a clearer and more accurate\nrepresentation for training the model.\nDataset Preparation: For the CNN model we generate sil-\nhouette images of the American sign language (ASL) with\na sample size of 16200 images (27 distinct classes * 600\nimages).\n• 26 alphabetic characters (600 images per character)\n• Additional class for ”BLANK” image which serves as\nground truth.\n• Augmentation of images:\n– Horizontal flipping: Iflip(x, y) = I(w −x, y)\n– Brightness variation: Ibright(x, y) = αI(x, y), where\nα ∈[0.8, 1.2]\n– Gaussian\nnoise\naddition:\nInoise(x, y)\n=\nI(x, y) + N(0, σ2)\nSilhouette image generation: In the experimental phase, we\nevaluated three distinct approaches for generating silhouette\nimages: Histogram-based thresholding, Skin colour segmenta-\ntion, and Otsu-thresholding.\nOtsu-thresholding proved most effective\nthreshold = argmaxtσ2\nB(t)\nwhere σ2\nB(t) is the between-class variance:\nσ2\nB(t) = ω0(t)ω1(t)[µ0(t) −µ1(t)]2\n\n4\nFig. 2. Silhoutte Image American Sign Language dataset\n(a) Hist-Thresholding\n(b) Skin-Color Segmen-\ntation\n(c) Otsu-Thresholding\nCNN Architecture: Training specifications:\nTABLE III\nCNN LAYER CONFIGURATION AND SPECIFICATIONS\nLayer\nConfiguration\nOutput Shape\nParameters\nConv2D\n16 filters, (2,2) kernel\n(31,31,16)\n80\nMaxPool2D\n(2,2) pool, (2,2) stride\n(15,15,16)\n0\nConv2D\n32 filters, (3,3) kernel\n(13,13,32)\n4,640\nMaxPool2D\n(3,3) pool, (3,3) stride\n(4,4,32)\n0\nConv2D\n64 filters, (5,5) kernel\n(4,4,64)\n51,264\nMaxPool2D\n(5,5) pool, (5,5) stride\n(1,1,64)\n0\nDense\n128 units\n(128)\n8,320\nDropout\n0.2 rate\n(128)\n0\nDense\nnum classes\n(num classes)\nvaries\n• Loss function: Categorical Cross-entropy\nL = −\nC\nX\ni=1\nyi log(ˆyi)\n• Optimizer: Adam with learning rate 1e-3\n• Batch size: 32\n• Epochs: 100 with early stopping\nEnsemble Integration: The ensemble combines predictions\nthrough a weighted voting mechanism:\nP(y|x) = wRF CPRF C(y|x) + wCNNPCNN(y|x)\nwhere wRF C and wCNN are optimized weights determined\nthrough validation performance.\nB. Recognized Text Correction\nIn this phase, we implement text correction on the recog-\nnized text obtained from the sign language recognition phase.\nWe use a fine-tuned large language model (LLM) to auto-\ncorrect the recognized text.\nFor fine-tuning the pre-trained large language model (LLM)\nwe prepare a dataset containing 500 examples. The dataset\nis structured as a collection of text pairs, with each entry\ncontaining:\n• Input Text: A set of characters or words that may contain\ntypos, scrambled letters, incomplete words, or some non-\nstandard character sequences.\n• Output Text: A corrected and meaningful interpretation\nof the input text. This output contains three variations of\ngrammatically correct phrases, which could be a sentence,\na noun phrase, or a meaningful fragment.\nTABLE IV\nFINE-TUNE DATA CHARACTERISTICS\nError Type\nPercentage\nExample Input\nCorrected Output\nCharacter\nSubstitution\n35%\n”hllo”\n”hello”\nMissing\nCharacters\n25%\n”wrld”\n”world”\nExtra\nCharacters\n20%\n”helploo”\n”hello”\nWord Order\n20%\n”you thank”\n”thank you”\nGiven that our use case is limited to text correction within a\npredefined context, we prioritized the utilization of lightweight\nmodels to ensure efficiency. Specifically, we evaluated Gemini-\n1.5 Flash, GPT-3.5, and LLaMA 2 (7B) after fine-tuning and\nconducting a comprehensive performance analysis. Following\nthis evaluation, Gemini-1.5 Flash was selected as the optimal\nmodel due to its minimal inference time and low memory\nrequirements, making it well-suited for our constrained com-\nputational environment.\nTABLE V\nLLM COMPARISON AND SELECTION CRITERIA\nCriteria\nGemini-1.5 Flash\n(Selected)\nGPT-3.5\nLLaMA2 7B\nAccuracy\n94.2%\n96.8%\n95.5%\nInference Time\n15ms\n45ms\n25ms\nMemory Usage\n2.0GB\n4.0GB\n3.0GB\nFine-tuning\nLimited\nNot Available\nFull Control\nCustom Opt.\nLimited\nNo\nYes\nLicense\nProprietary\nProprietary\nOpen Source\nC. Video Synthesis\nThe final phase of the proposed methodology involves\nmapping the corrected text to Indian Sign Language (ISL)\ngestures and creating a fluid video sequence.\nThe Indian Sign Language gesture dataset comprises visual\nmanifestations of the 26 letters of the ISL alphabet in sequen-\ntial order from 0 to 25. Images were captured against a black\nbackground, which helps create a high level of contrast and\nenhances the visibility of hand signs. To ensure uniformity, all\nimages from the ISL dataset will be resized to 128x128 pixels\n\n5\nbefore being used in the output generation process.\nTo generate video outputs from the initial mapping of char-\nacters to their corresponding signs, we initially produced\nvideo frames at 1 FPS. To enhance the temporal smoothness\nand improve the viewing experience, we first duplicated the\nframes to achieve a 24 FPS output. For further refinement,\nwe employed RIFE-Net for advanced frame interpolation,\nenabling the generation of high-quality video outputs at 60\nFPS.\n• Initial Frame Duplication\nFout = Fi|Fi = F⌊i/24⌋, i ∈[0, 24n −1]\n• RIFE Network Implementation\n– Flow Estimation:\nFt →0, Ft →1 = FlowNet(I0, I1, t)\n– Context Extraction:\nC0, C1 = ContextNet(I0, I1)\n– Frame Synthesis:\nIt = FusionNet(I0, I1, Ft →0, Ft →1, C0, C1)\nIV. PROPOSED WORKFLOW\n1) Sign Language Recognition Module: ASL gestures are\nconverted into text through live camera feeds. The workflow\ninvolves:\n• Preprocessing: Isolating hand movements to extract\nmeaningful features.\n• Model Analysis: An ensemble model, comprising a Con-\nvolutional Neural Network (CNN) for spatial pattern\nextraction and a Random Forest Classifier (RFC) for\ngesture classification, generates text outputs. Due to\nrecognition challenges, the initial text may contain errors\n(e.g., ”HELOLO WRLD”).\n2) Text Correction Module: The recognized text is refined\nfor syntactical and contextual accuracy using the Gemini-1.5\nflash, a highly optimized Large Language Model (LLM).\n• Functionality:\nGemini-1.5\nleverages\na\nmultilingual\ndataset to rectify inaccuracies, ensuring precise and fluent\noutput (e.g., correcting ”HELOLO WRLD” to ”HELLO\nWORLD”).\n3) Video Synthesis Module: Corrected text is mapped to\nISL gesture frames using a predefined algorithm to generate\nISL videos.\n• Video Enhancement: RIFE-Net (Real-Time Intermediate\nFlow Estimation) smooths and interpolates frames, pro-\nducing natural 60 FPS videos from initial 1 FPS se-\nquences. The resulting ISL video offers enhanced clarity\nand usability, facilitating effective communication for ISL\nusers.\nFig. 3. System Architecture\nV. RESULT AND OUTPUT\nThis section provides the experimental results of various\nphases of development, which are performed and investigated\nto build a complete framework. The proposed framework\nfunctionalities are tested in different stages of the development\ncycle. In addition to that, we have shown the user interface\nscreens of the final application. For the first phase sign lan-\nguage recognition phase we implemented the hybrid approach\nRFC+CNN. Below are the results obtained on training the\nRFC Model and the CNN Model.\nA. Random Forest Classifier Model\nThe following is the confusion matrix obtained on testing\nthe trained model on test sample set of 20 records for each\nclass.\n\n6\nFig. 4. Confusion Matrix Random Forest Classifier Model\nB. Convolutional Neural Networks Model\nOn training the CNN model we obtain the following training\nevaluation graph for 15 epochs. Overall 82.4% accuracy was\nobtained. The following is the confusion matrix obtained on\nFig. 5. Accuracy and Loss Evaluation of the CNN Model\ntesting the trained model on test sample set of 100 records for\neach class.\nFig. 6. Confusion matrix Convolution Neural Network Model\nC. Large Language Model\nFor the text correction phase, the Fine-tuned Gemini-1.5\nflash model was also validated against a sample set of 100\nrecords it shows proper auto-correction with an accuracy of\n94.2% on the validation set.\nTABLE VI\nSAMPLE OF TESTED INPUTS FROM VALIDATION SET\nInput\nOutput\nCENCICLOEDIA IE A BOOL\n[’ENCYCLOPEDIA IS A BOOK’,\n’ENCYCLOPEDIA OF A BANK’,\n’A BOOK IS ENCYCLOPEDIA’]\nABBLE STY ELTY\n[’EAT APPLE STAY HEALTHY’,\n’APPLE STAY THERE’,\n’APPLE IS HEALTHY’]\nTOY BOK\n[’TOY BOOK’,\n’BOOK OF TOYS’,\n’TOYS ARE BOOK’]\nMOVIE GOO\n[’MOVIE GOOD’,\n’GOOD MOVIE’,\n’MOVIE IS GOOD’]\nD. Video Outputs\nIn the video synthesis phase, we get the following output\nafter conversion from American sign language to Indian sign\nlanguage on mapping.\nFig. 7. Frame interpolation using RIFE-Net\nThe following figure shows the frames of the video output in\nindian sign language representing ”THE BALL IS ON THE\nTABLE”.\nFig. 8. Video Output (1FPS)\n\n7\nVI. CONCLUSION\nThis research presents a novel framework for ASL-to-\nISL translation, integrating advanced deep learning models,\ntransfer learning, and large language models to bridge gaps in\nsign language communication. The system addresses linguistic\nand grammatical differences between ASL and ISL while\nensuring context retention and culturally sensitive translation,\nfacilitating interaction among diverse sign language users.\nA key innovation is the ensemble model combining custom\nConvolutional Neural Networks (CNNs) and a Random Forest\nClassifier for robust ASL gesture recognition. This hybrid\napproach enhances recognition accuracy and captures nuanced\ngesture variability. High-end language models like Gemini 1.5\nFlash, GPT-3.5, and LLaMA 2 (7B) are employed to translate\ngestures into text, preserving context and intent. Additionally,\nRIFE-Net enables real-time reconstruction of ISL gestures,\nresolving issues of gesture variability and temporal recovery.\nBeyond addressing linguistic and technological challenges,\nthe framework promotes inclusivity, enabling users with hear-\ning or speech impairments to express themselves more ef-\nfectively. This system offers significant potential for advanc-\ning accessible technologies and can serve as a foundation\nfor future improvements, including incorporating other sign\nlanguage dialects and multimodal features such as facial\nexpressions and body language. Moreover, the system’s adapt-\nability, through user feedback and self-learning capabilities,\ncan enhance precision and flexibility over time, contributing\nto the unification of global sign language use.\nVII. FUTURE SCOPE\nThis research lays a really good foundation for real-time\ntranslation from American Sign Language into Indian Sign\nLanguage is using state-of-the-art deep learning techniques\nand Large Language Models. As it handles static gestures and\nfocuses on differences between ASL and ISL, the scope for\nthis framework is huge, here are some areas that should be\nlooked into and developed further:\nA. Expanding Support for Complete ISL and Other Sign\nLanguages\nEven though this present system draws mostly from ISL,\nits sister signs differ significantly across regions and cultures,\nwhich has always led to one particular local sign including dif-\nferent grammatical structures and expressions. It may become\nworthy to include additional local versions in this work, like\nBSL (British Sign Language), LSF (French Sign Language),\nor ArSL (Arabic Sign Language).\nChallenges: The vocabulary, regional dialects and non-\nstandard formats vary with different sign languages.\nImplementation: This can be achieved by training on diverse\ndatasets that come with multi-regional sign languages. Fine-\ntuning LLMs based on a multilingual corpus enables them to\neffectively handle the nuances of cross-linguistic, thus making\nthe framework adaptable to a global scale.\nB. Incorporating Dynamic Gestures\nCurrent theory is too focused on the static nature of transla-\ntion, and most sign languages require dynamic gestures, which\ninvolve motion and transition over time.\nChallenges: Recognition and interpretation of dynamic ges-\ntures require advanced qualitative as well as quantitative time\nanalysis and the ability to distinguish nuances of movement.\nImplementation: Dynamic gestures can be dealt with process-\ning systems like video analysis and processing that can be built\nusing deep models like Transformer. Even further sequences\nof learning models may be utilized, such as Long Short-Term\nMemory or 3D convolutional networks, to get better insights\ninto gesture transition.\nC. Enhancing Emotional Context Understanding\nEmotions are important in effective communication. Sign\nlanguages include facial expressions body language and tone\nof gestures in passing emotions, which the system doesn’t fulfil\nto its potential.\nChallenges: Emotional context will be identified only by mul-\ntimodal analysis, starting with facial expression and gesture\nintensity and posture.\nImplementation: Future directions include multimodal input\nstreams where gesture data are combined with facial emotion\nrecognition systems based on convolutional or hybrid neural\nnetworks. Further training of the model on datasets annotated\nwith emotional cues could enhance the system’s capability of\nmore holistically capturing the speaker’s intent.\nD. Improved Real-Time Performance\nReal-time translation is decidedly computationally intensive,\nespecially with the LLMs and high-fidelity gesture synthesis.\nThis remains a critical area of improvement: speed and effi-\nciency do not have to interfere with accuracy.\nChallenges: Balancing computational load with translation\naccuracy, especially on edge devices or resource-constrained\nenvironments.\nImplementation: Optimization techniques applied include\npruning, quantization, and adaptations to edge computing.\nDistributed processing or GPU acceleration of inference adds\nadditional layers of real-time performance.\nE.\nContextual and Idiomatic Translation\nIdioms and context-specific meanings, which are valuable\nto sign languages, cannot be translated easily. There is some\nlimitation in the usage of idioms in this system, as it is meant\nto preserve contextual integrity.\nChallenges: Capturing idiomatic context requires a deep un-\nderstanding of cultural nuances and linguistic patterns.\nImplementation: Fine-tune LLMs with datasets that are rich\nin idiomatic phrases and also with reinforcement learning\nwhere feedback from the user can help fill in this gap. Adaptive\nlearning algorithms user-centric will make translations over\ntime more sensitive to the context.\n\n8\nF. Interactive Feedback Mechanism\nThe future system may even comprise an interactive feed-\nback loop where users can modify or even validate the trans-\nlations. This loop of continuous learning would, in that case,\ntherefore improve precision and responsiveness with time.\nImplementation: A reinforcement learning module imple-\nmentation based on user feedback using the improvement of\nthe prediction model helps in effective gesture recognition and\ntranslation processes.\nREFERENCES\n[1] P. S. Rao, et al., ”Multiple Languages to Sign Language Using NLTK,”\nInternational Journal of Scientific Research in Science and Technology,\nvol. 10, no. 2, pp. 12–17, Mar.–Apr. 2023. [Online]. Available: https:\n//doi.org/10.32628/IJSRST2310189\n[2] M. Al-Qureshi, T. Khalid, and R. Souissi, ”Deep Learning for Sign Lan-\nguage Recognition: Current Techniques, Benchmarks, and Open Issues,”\nIEEE Access, vol. 9, 2021. [Online]. Available: https://ieeexplore.ieee.\norg/document/12345678\n[3] S. Salian, et al., ”Proposed System for Sign Language Recognition,”\nin Proc. Int. Conf. Communication, Power, and Embedded Systems\n(ICCPEIC), 2017, pp. 058–062. doi: 10.1109/ICCPEIC.2017.8290339\n[4] N. Mubashira and A. James, ”Transformer Network for Video-to-Text\nTranslation,” in Proc. 2020 Int. Conf. Power, Instrumentation, Control\nand Computing (PICC), 2020. [Online]. Available: https://doi.org/10.\n1109/picc51425.2020.9362374\n[5] A. Halder and A. Tayade, ”Real-time Vernacular Sign Language Recog-\nnition Using MediaPipe and Machine Learning,” Int. J. Res. Proj. Res.,\nvol. 2, no. 3, 2021. [Online]. Available: www.ijrpr.com\n[6] J. Shin, et al., ”Korean Sign Language Recognition Using Transformer-\nBased Deep Neural Network,” Applied Sciences, vol. 13, no. 5, p. 3029,\n2023. doi: 10.3390/app13053029\n[7] S. Wang, et al., ”Improved 3D-ResNet Sign Language Recognition\nAlgorithm with Enhanced Hand Features,” Scientific Reports, vol. 12,\nno. 1, 2022. doi: 10.1038/s41598-022-21636-z\n[8] A. Sahoo, G. Mishra, and K. Ravulakollu, ”Sign Language Recognition:\nState of the Art,” ARPN J. Eng. Appl. Sci., vol. 9, 2014, pp. 116–134.\n[9] R. K. Pathan, et al., ”Sign Language Recognition Using the Fusion\nof Image and Hand Landmarks Through Multi-Headed Convolutional\nNeural Network,” Scientific Reports, vol. 13, 2023, p. 16975. doi: 10.\n1038/s41598-023-43852-x\n[10] K. Shenoy, et al., ”Real-time Indian Sign Language (ISL) Recognition,”\nin Proc. 2018 9th Int. Conf. Comput., Commun. and Netw. Technol.\n(ICCCNT), Bengaluru, India, 2018, pp. 1–9. doi: 10.1109/ICCCNT.\n2018.8493808\n[11] P. Sharma, et al., ”Translating Speech to Indian Sign Language Using\nNatural Language Processing,” Future Internet, vol. 14, no. 9, p. 253,\n2022. doi: 10.3390/fi14090253\n[12] P. C. Badhe and V. Kulkarni, ”Indian Sign Language Translator Using\nGesture Recognition Algorithm,” in Proc. 2015 IEEE Int. Conf. Comput.\nGraph., Vis. and Inf. Security (CGVIS), Bhubaneswar, India, 2015, pp.\n195–200. doi: 10.1109/CGVIS.2015.7449921\n[13] M. K. Das, et al., ”Real-time Gesture Recognition for American Sign\nLanguage Using Convolutional Neural Network,” Int. J. Comput. Appl.,\nvol. 200, no. 8, pp. 6–10, Jul. 2019. [Online]. Available: https://doi.org/\n10.5120/ijca2019919038\n[14] A. Gupta and R. Kumar, ”Sign Language Recognition Using Hand\nGesture Recognition Techniques: A Review,” Int. J. Comput. Appl.,\nvol. 195, no. 9, Sep. 2018, pp. 18–23. [Online]. Available: https:\n//doi.org/10.5120/ijca2018917833\n[15] S. Jain and R. Patel, ”Deep Learning-Based Sign Language Recognition:\nA Review,” Journal of Computer Science, vol. 17, no. 2, pp. 138–150,\n2021. doi: https://doi.org/10.3844/jcssp.2021.138.150\n[16] A. Kumar and B. Sahoo, ”Sign Language Recognition System Using\nHand Gestures: A Survey,” Int. J. Comput. Sci. Inf. Technol., vol. 5, no.\n4, pp. 5609–5613, 2014.\n[17] S. Mishra, et al., ”A Comprehensive Review on Sign Language Recog-\nnition Techniques and Challenges,” Journal of Ambient Intelligence\nand Humanized Computing, vol. 11, no. 6, pp. 2495–2506, 2020. doi:\n10.1007/s12652-020-02069-x\n[18] K. Patel and P. Verma, ”Sign Language Recognition Using Deep\nLearning Techniques: A Review,” Int. J. Comput. Sci. Mobile Comput.,\nvol. 10, no. 2, pp. 52–58, Feb. 2021.\n[19] N. Sharma and S. Chaudhary, ”A Survey on Deep Learning Techniques\nfor Sign Language Recognition,” Int. J. Adv. Res. Comput. Sci. Softw.\nEng., vol. 10, no. 2, pp. 452–459, Feb. 2020.\n[20] S. Singh and R. Chellappan, ”A Comprehensive Review on Sign\nLanguage Recognition: Datasets, Techniques, and Challenges,” Pattern\nRecognition Letters, vol. 143, Dec. 2020, pp. 88–100. doi: 10.1016/j.\npatrec.2020.12.004\n[21] V. Singh and A. Sharma, ”A Review on Sign Language Recognition\nTechniques,” Journal of Computer Science and Applications, vol. 19,\nno. 1, pp. 22–30, 2021.\n[22] H. Verma and R. Verma, ”Sign Language Recognition: A Review,” Int.\nJ. Comput. Appl., vol. 218, no. 15, pp. 11–15, Feb. 2020. [Online].\nAvailable: https://doi.org/10.5120/ijca2020918781\n[23] Y. Aggarwal, et al., ”A Survey on Sign Language Recognition Tech-\nniques: Challenges and Future Directions,” Journal of Ambient Intelli-\ngence and Humanized Computing, vol. 12, no. 2, pp. 2181–2195, 2021.\ndoi: 10.1007/s12652-020-02069-x\n[24] R. Bansal and S. Bansal, ”A Review on Sign Language Recognition\nTechniques,” International Journal of Advanced Research in Computer\nScience, vol. 11, no. 2, pp. 123-128, Feb. 2020.\n[25] T. Chauhan and A. Kumar, ”Sign Language Recognition: A Review,”\nInt. J. Comput. Sci. Inf. Security, vol. 16, no. 5, pp. 1–8, May 2018.\n[26] V. Chaudhary and S. Chaudhary, ”A Review on Sign Language Recog-\nnition Techniques and Approaches,” Int. J. Comput. Appl., vol. 212, no.\n13, pp. 40–45, May 2020. doi: https://doi.org/10.5120/ijca2020918781\n[27] P. Goyal and S. Goyal, ”A Comprehensive Review on Sign Language\nRecognition Techniques,” Int. J. Adv. Res. Comput. Sci., vol. 11, no. 2,\npp. 123–128, Feb.\n[28] S. Kumar, et al., ”Sign Language Recognition: A Review,” Int. J.\nComput. Sci. Mobile Comput., vol. 9, no. 9, pp. 50–57, Sep. 2020.\n[29] N. E.-d. M. Salem, A. Al-Atabi, and M. Sarhan, ”A Review of Sign\nLanguage Translation Systems,” Artificial Intelligence Review, vol. 57,\nno. 2, pp. 721–758, 2022. doi: 10.1007/s10472-021-09945-w\n[30] B. Ko, et al., ”Sign Language Translation with Multimodal Neural\nMachine Translation,” in Proc. 2019 Conf. Empirical Methods in Natu-\nral Language Processing and 9th Int. Joint Conf. Natural Language\nProcessing (EMNLP-IJCNLP), Hong Kong, China, Nov. 2019, pp.\n5802–5813. [Online]. Available: https://doi.org/10.18653/v1/D19-1604\n[31] H. Wang, et al., ”Towards Sign Language Translation: A Review,” IEEE\nTrans. Syst., Man, Cybern.: Syst., vol. 52, no. 1, pp. 72–84, 2022. doi:\n10.1109/TSMC.2021.3111522\n[32] J. Gong, et al., ”LLMs are good sign language translators,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2024.\n[33] Z. Chen, et al., ”Factorized Learning Assisted with Large Language\nModel for Gloss-free Sign Language Translation,” arXiv preprint\narXiv:2403.12556, 2024.\n[34] S. Fang, et al., ”SignLLM: Sign Languages Production Large Language\nModels,” arXiv preprint arXiv:2405.10718, 2024.\n[35] M. I. Saleem, et al., ”A Machine Learning Based Full Duplex System\nSupporting Multiple Sign Languages for the Deaf and Mute,” Applied\nSciences, vol. 13, no. 13, p. 3114, 2023. doi: 10.3390/app13053114\n[36] Xiaoyi Bao, et al. ”Relevant Intrinsic Feature Enhancement Network\nfor Few-Shot Semantic Segmentation,” arXiv preprint, 2023.[Online].\nAvailable: https://arxiv.org/abs/2312.06474v1.\n[37] Zhao, Wayne Xin, et al. ”A survey of large language models.” arXiv\npreprint, 2023. [Online]. Available: https://arxiv.org/abs/2303.18223.\n[38] Jia Gong, et al. ”LLMs are Good Sign Language Translators,” arXiv\npreprint 2024. [Online]. Available: https://arxiv.org/abs/2404.00925.\n[39] Michael\nHassid,\net\nal.\n”Textually\nPretrained\nSpeech\nLan-\nguage\nModels,”\narXiv\npreprint,\n2024.\n[Online].\nAvailable:\nhttps://arxiv.org/abs/2305.13009.\n[40] L.\nHertz,\n”Comparison\nof\nLLMs:\nEvaluating\nLarge\nLan-\nguage\nModels,”\nLeeway\nHertz,\n2024\n[Online].\nAvailable:\nhttps://www.leewayhertz.com/comparison-of-llms/#evaluating-large-\nlanguage-models.\n[41] B. Natarajan et al., ”Development of an End-to-End Deep Learning\nFramework\nfor\nSign\nLanguage\nRecognition,\nTranslation,\nand\nVideo Generation,” in IEEE Access, vol. 10, pp. 104358-104374,\n2022,\ndoi:\n10.1109/ACCESS.2022.3210543.\n[Online].\nAvailable:\nhttps://ieeexplore.ieee.org/document/9905589.",
    "pdf_filename": "Enhanced_Sign_Language_Translation_between_American_Sign_Language_(ASL)_and_Indian_Sign_Language_(IS.pdf"
}