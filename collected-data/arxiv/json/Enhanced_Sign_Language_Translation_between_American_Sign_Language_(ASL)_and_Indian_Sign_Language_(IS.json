{
    "title": "1",
    "abstract": "provide a bridge between the users of American Sign Language hereby propose an approach that combines image recognition and the users of spoken language and Indian Sign Language techniqueswithadvancedlanguageprocessingalgorithmsgov- (ISL).Theresearchenabledustocreateanovelframeworkthat erned by Large Language Models. Instead of using traditional wehavedevelopedforLearnerSystems.LeveragingartofLarge models to create key features including: - Real-time translation CNN- based recognition techniques, LLM-driven techniques betweenthesetwosignlanguagesinanefficientmanner.Making areusedtodecodeASLgesturesand,inturn,translatedirectly LLM’scapabilityavailableforseamlesstranslationstoISL.Here into meaningful textual representations. From this step of is the full study showing its implementation in this paper. The converting ASL gestures to text, we establish an intermediate core of the system is a sophisticated pipeline that begins with that helps use advanced LLM-based techniques for machine reclassificationandrecognitionofASLgesturesbasedonastrong RandomForestClassifier.ByrecognizingtheASL,itistranslated translation. Now, we can translate the recognized English into text which can be more easily processed. Highly evolved text into ISL gestures by preserving linguistic aspects and naturallanguageNLP(NaturalLanguageProcessing)techniques cultural context. The application of LLMs for translation will come in handy as they play a role in our LLM integration help make the process more accurate, context-sensitive, and where you then use LLMs to be able to convert the ASL text to adaptable and, thus, would be used to bridge between ASL ISL which provides you with the intent of sentence or phrase. The final step is to synthesize the translated text back into and ISL in a nearly seamless manner. ISLgestures,creatinganend-to-endtranslationexperienceusing RIFE-Net.Thisframeworkistaskedwithkeychallengessuchas automatically dealing with gesture variability and overcoming A. Leveraging Deep Learning for Gesture Recognition and the linguistic differences between ASL and ISL. By automating Translation the translation process, we hope to vastly improve accessibility Sign language communication involves complex expres- for sign language users. No longer will the communication gap betweenASLandISLcreatebarriers;thistotallycoolinnovation sions that carry a certain range of linguistic and cultural aims to bring our communities closer together. And we believe, nuancesincommunication.Conventionalsignlanguageunder- with full confidence in our framework, that we’re able to apply standing and translation techniques rely mostly on fixed data the same principles across a wide variety of sign language sets and prebuilt models. Such approaches find it difficult to dialects. adapttothedynamicnatureofsignlanguages.Inthepaper,the IndexTerms—SignLanguageRecognition,ASLtoISLTrans- constraints were overcome using Random Forest Classifiers lation, Large Language Models (LLMs), Natural Language for effective gesture recognition followed by Large Language Processing (NLP), Random Forest Classifier, Gesture Reclassi- fication, Text-to-Gesture Synthesis, RIFE-Net, Real-Time Trans- Models to assist in context-aware translation. The proposed lation, Sign Language Variability, Linguistic Adaptation, Assis- framework, therefore, would represent a breakthrough with tive Technology, Cross-Linguistic Framework, Sign Language theintegrationofreal-timeprocessingcapabilitiesandcultural Dialects, Accessibility and Inclusivity contextualization. Thesignificanceofthisworkisthatitintroducesaninterme- I. INTRODUCTION diate text-based representation that forms the connecting link THE communication gap between the users of American between recognition and synthesis of gestures. This linguistic Sign Language (ASL) and Indian Sign Language (ISL) intermediate allows for not only a more literal translation but is a significant challenge to intercultural interaction and ac- also allows the tailoring of the translation method in such a cessibility in the deaf community. Sign languages are indis- way as to keep its intent and cultural nuances of the original pensable weapons of expression for deaf individuals, but the expression. This model would become especially important to lack of interoperability between ASL and ISL limits smooth the reduction of the linguistic differences between ASL and communicationacrossvariouslinguisticandculturalfrontiers. ISL, such as differing grammatical structures, word orders, This challenge can be addressed through innovative strategies and contextual expressions. Further, generating ISL gestures that leverage advances in machine translation technology and from translated text with the assistance of RIFE-Net produces contemporary deep learning frameworks. The central goal of asmoothandnaturalgesturepresentation.Sucharobustability thisresearchworkistobuildaholisticframeworkformachine of RIFE-Net to effectively handle high variability within translation that can efficiently and conveniently translate ASL gesturesequencesleadsittoaccuratelyreproduceISLgestures gestures into ISL gestures, hence facilitating communication even while making forward-looking predictions of potentially 4202 voN 91 ]LC.sc[ 1v58621.1142:viXra",
    "body": "1\nEnhanced Sign Language Translation between\nAmerican Sign Language (ASL) and Indian Sign\nLanguage (ISL) Using LLMs\nMalay Kumar, S. Sarvajit Visagan, Tanish Sarang Mahajan, and Anisha Natarajan\nAbstract—We have come up with a research that hopes to between ASL users and ISL users. Toward this objective, we\nprovide a bridge between the users of American Sign Language hereby propose an approach that combines image recognition\nand the users of spoken language and Indian Sign Language\ntechniqueswithadvancedlanguageprocessingalgorithmsgov-\n(ISL).Theresearchenabledustocreateanovelframeworkthat\nerned by Large Language Models. Instead of using traditional\nwehavedevelopedforLearnerSystems.LeveragingartofLarge\nmodels to create key features including: - Real-time translation CNN- based recognition techniques, LLM-driven techniques\nbetweenthesetwosignlanguagesinanefficientmanner.Making areusedtodecodeASLgesturesand,inturn,translatedirectly\nLLM’scapabilityavailableforseamlesstranslationstoISL.Here into meaningful textual representations. From this step of\nis the full study showing its implementation in this paper. The\nconverting ASL gestures to text, we establish an intermediate\ncore of the system is a sophisticated pipeline that begins with that helps use advanced LLM-based techniques for machine\nreclassificationandrecognitionofASLgesturesbasedonastrong\nRandomForestClassifier.ByrecognizingtheASL,itistranslated translation. Now, we can translate the recognized English\ninto text which can be more easily processed. Highly evolved text into ISL gestures by preserving linguistic aspects and\nnaturallanguageNLP(NaturalLanguageProcessing)techniques cultural context. The application of LLMs for translation will\ncome in handy as they play a role in our LLM integration\nhelp make the process more accurate, context-sensitive, and\nwhere you then use LLMs to be able to convert the ASL text to\nadaptable and, thus, would be used to bridge between ASL\nISL which provides you with the intent of sentence or phrase.\nThe final step is to synthesize the translated text back into and ISL in a nearly seamless manner.\nISLgestures,creatinganend-to-endtranslationexperienceusing\nRIFE-Net.Thisframeworkistaskedwithkeychallengessuchas\nautomatically dealing with gesture variability and overcoming A. Leveraging Deep Learning for Gesture Recognition and\nthe linguistic differences between ASL and ISL. By automating Translation\nthe translation process, we hope to vastly improve accessibility\nSign language communication involves complex expres-\nfor sign language users. No longer will the communication gap\nbetweenASLandISLcreatebarriers;thistotallycoolinnovation sions that carry a certain range of linguistic and cultural\naims to bring our communities closer together. And we believe, nuancesincommunication.Conventionalsignlanguageunder-\nwith full confidence in our framework, that we’re able to apply standing and translation techniques rely mostly on fixed data\nthe same principles across a wide variety of sign language\nsets and prebuilt models. Such approaches find it difficult to\ndialects.\nadapttothedynamicnatureofsignlanguages.Inthepaper,the\nIndexTerms—SignLanguageRecognition,ASLtoISLTrans-\nconstraints were overcome using Random Forest Classifiers\nlation, Large Language Models (LLMs), Natural Language\nfor effective gesture recognition followed by Large Language\nProcessing (NLP), Random Forest Classifier, Gesture Reclassi-\nfication, Text-to-Gesture Synthesis, RIFE-Net, Real-Time Trans- Models to assist in context-aware translation. The proposed\nlation, Sign Language Variability, Linguistic Adaptation, Assis- framework, therefore, would represent a breakthrough with\ntive Technology, Cross-Linguistic Framework, Sign Language theintegrationofreal-timeprocessingcapabilitiesandcultural\nDialects, Accessibility and Inclusivity\ncontextualization.\nThesignificanceofthisworkisthatitintroducesaninterme-\nI. INTRODUCTION diate text-based representation that forms the connecting link\nTHE communication gap between the users of American between recognition and synthesis of gestures. This linguistic\nSign Language (ASL) and Indian Sign Language (ISL) intermediate allows for not only a more literal translation but\nis a significant challenge to intercultural interaction and ac- also allows the tailoring of the translation method in such a\ncessibility in the deaf community. Sign languages are indis- way as to keep its intent and cultural nuances of the original\npensable weapons of expression for deaf individuals, but the expression. This model would become especially important to\nlack of interoperability between ASL and ISL limits smooth the reduction of the linguistic differences between ASL and\ncommunicationacrossvariouslinguisticandculturalfrontiers. ISL, such as differing grammatical structures, word orders,\nThis challenge can be addressed through innovative strategies and contextual expressions. Further, generating ISL gestures\nthat leverage advances in machine translation technology and from translated text with the assistance of RIFE-Net produces\ncontemporary deep learning frameworks. The central goal of asmoothandnaturalgesturepresentation.Sucharobustability\nthisresearchworkistobuildaholisticframeworkformachine of RIFE-Net to effectively handle high variability within\ntranslation that can efficiently and conveniently translate ASL gesturesequencesleadsittoaccuratelyreproduceISLgestures\ngestures into ISL gestures, hence facilitating communication even while making forward-looking predictions of potentially\n4202\nvoN\n91\n]LC.sc[\n1v58621.1142:viXra\n2\ncomplex or subtle translations. With advanced recognition, recognitionalgorithmwithclearerhandfeaturesandimproved\ntranslation, and synthesis modules integrated, the framework the state of the art of gesture recognition [7].\nplaces it at the forefront of a cutting-edge tool in the domain Intheirpaper,Sahooetal.(2014)gaveadetailedaccountof\nof sign language translation. therecentapproachestorecognizingsignlanguages,andmajor\nissues[8].Pathanetal.Pathan,Muhammad,andZhang(2023)\nB. Towards a Multi-dialectal Future developed a fusion approach for sign language recognition\nfrom both image and hand landmarks using a multi-headed\nThough designed specifically for ASL and ISL, underpin-\nconvolutional neural network demonstrating how multiple-\nning principles and methodologies are adaptable to support\nmodal can enhance gesture recognition systems [9].\nmore than those targeted sign language dialects. By adapting\nShenoy et al. (2018) have proposed real-time ISL recogni-\nthe system to use a variety of datasets, it follows that it is vi-\ntion using grid-based feature extraction and machine learning\nabletoaddressthediversityinsignlanguagesglobally,thereby\nto address the requirements of the Indian deaf and Hard of\npaving the course for universal sign language interoperability.\nhearing community [10]. Sharma and Singh (2022) investi-\nMoreover,thisadaptabilitybringsoutthescaleinusingLLMs\ngated NLP in the general mapping of speech to ISL with\nfor translation purposes as it opens up broader applications in\ninsights towards a hybrid way of sign language translation\nassistive technologies.\n[11].\nThis research promises to work out an integrated approach\nInaddition,whileBadheandKulkarni(2015)developedan\ncombining gesture recognition, natural language processing,\nIndian sign language translator based on gesture recognition\nand synthesis of gestures to bridge the overall communication\nalgorithms,theypointedoutthatfeatureextractionandmotion\ngap so present in sign language communities. This is an\ndetection remain decisive factors in designing sign language\nadvancebothinthetechnicalandimportancetowardfurthering\nrecognition systems [12].\ntheaspectsofinclusivityandexchangeacrosstheworld’sdeaf\nFurthermore, Jia Gong (2024), the author was motivated\ncommunity.\nby the remarkable translation performance of LLMs and pro-\nposed a method to incorporate off-the-shelf LLMs to address\nII. LITERATURESURVEY\ncomplexSignLanguageTranslationTasks[38].ZhiGangChen\nRecognitionandtranslationofsignlanguageshaveattracted\n(2024)thoughtexploringthegloss-freemethodsismuchmore\nalotoffocusintherecentpastbecauseoftheirpivotalrolein\ncrucial because it will greatly decrease the annotation time\nimprovingcommunicationbetweendeafandmutepeople.Pre-\nand promote the dependence of more accurate and universal\nsentinganoverviewoftheliteratureinthisfield,thisliterature\nsignlanguagetranslationsystems[33].SenFangthenunveiled\nreviewidentifiesmethodologies,techniques,andchallengesin\nSIGNLLM which is the first large-scale multilingual SLP\nsign language recognition and translation systems.\nmodelcreatedusingtheprompt2signdatasetwhichgenerates\nRao et al. (2023) introduced a system of how people with\nthe skeletal poses of sign language from text or prompt for\nmany spoken languages utter sign language using the Natural\neight languages [34].\nLanguage Toolkit (NLTK) and argued that a critical factor in\nsignlanguagetranslationislinguisticprocessing[1].Apopular\nreview carried out by Al-Qureshi et al. (2023) focused on III. PROPOSEDMETHODOLOGY\nthe DP of recognizing sign languages through deep learning\nThis research presents a novel methodology to bridge the\napproaches, including challenges and recent milestones [2].\ncommunication gap between American Sign Language (ASL)\nSalian et al. (2017) also presented a system for sign language\nand Indian Sign Language (ISL) through an automated trans-\nrecognition to set the background for subsequent work in this\nlationsystemenhancedusingLargelanguagemodels(LLMs).\ndomain [3].\nTheproposedapproachcomprisesthreeinterconnectedphases:\nIn this Article, Mubashira and James 2020 proposed a\nThe sign language recognition phase that utilizes a hybrid en-\nTransformer Network for video-to-text translation proving\nsemble model, the Recognized Text Correction phase through\nthe transformers effective in sign language [4]. Halder and\nlanguage model enhancement, and the Video synthesis phase\nTayade (2021) in a recent study that designed a real-time\nwith motion smoothing. Each phase has been meticulously\nvernacularsignlanguagerecognitionsystemusingmediapipe\ndesigned to address specific challenges in cross-sign-language\nand machine learning propose the possibility of real-time\ntranslation while maintaining semantic accuracy and natural\napplications in this field [5]. Also, in Korean sign language\ngesture flow.\nrecognition, Shin et al. (2023) proposed a Transformer-Based\nDeep Neural Network and demonstrated how deep learning\ncan be applied for sign language recognition irrespective of\nA. Sign Language Recognition\nthe type of sign language [6].\nSaleem et al. (2023) provided an outstanding, machine The initial phase employs a hybrid ensemble approach\nlearning-based, full-duplex sign language communication sys- combining the complementary strengths of a Random Forest\ntem capable of translating multiple sign languages; the prob- Classifier(RFC)andaConvolutionalNeuralNetwork(CNN).\nlem of sign language communication technology for the deaf This RFC+CNN model architecture allows robust feature\ncommunity has received continued attention [35]. Wang et extractionandclassificationofthesignlanguageandprovides\nal. (2022) proposed an improved 3D-Res-Net sign language better prediction accuracy.\n3\n1) Random Forest Classifier Model: The Random Forest TABLEII\nClassifier component focuses on the skeletal approach which RFCHYPERPARAMETERCONFIGURATION\nsetsabaselineforsignlanguagerecognition,itusesthehand-\nParameter SearchSpace Best Impact\ntracking capabilities of the mediapipe framework for feature Value\nextraction. Balancescomplexity\nn estimators [100,200,300] 200\nDatasetPreparation:Fortheabovemodelweprepareanimage andaccuracy\nmax depth [None, 10, 20, 20 Controlsoverfitting\ndataset with a sample size of 2800 images (28 distinct classes\n30]\n* 100 images) Ensuresrobust\nmin samples split [2,5,10] 5\nnodesplitting\n• 26 alphabetic characters (100 images per character). min samples leaf [1,2,4] 2 Predictionstability\n• Additional classes for ’DELETE’ and ’SPACE’ opera- Enablesvariedtree\nbootstrap [True,False] True\ntions. construction\n• A total of 28 distinct classes.\noptimized through the GridSearch Algorithm.\nTABLEI\nDATASETDISTRIBUTIONANDACQUISITIONPARAMETERS The model’s decision function for class prediction can be\nexpressed as:\nParameter Specification Justification yˆ=mode yˆt(x)t=1T\nImage Reso- 32×32pixels Optimal balance between\nlution detail preservation and where yˆ(x) represents the prediction of the t-th tree, and T\ncomputationalefficiency t\nis the total number of trees.\nLighting Ambient,Direct,Shadow Ensures robustness across\nConditions variouslightingscenarios\nBackground Background clutter, ob- Improves model general- 2) Convolutional Neural Network Model: For the convo-\nVariation jects ization\nlutional neural network (CNN) component we use silhouette\nCamera An- 0°,±15°,±30° Accountsfornaturalvari-\ngles ationinhandpositioning imagesprovidingcomplementaryvisualfeatureanalysistothe\nCropFocus Handregion Eliminates irrelevant RFC’s landmark-based approach.\nbackgroundinformation\nSilhouette Images: It is a type of image where a dark image\nof a subject against a lighter background, usually showing the\nsubject’s profile.\nUsing silhouette images enables a focus on the intrinsic\nstructure of the hand pose, effectively eliminating background\nnoise, lighting variations, and other environmental influences.\nThis approach isolates the key features of the hand’s position\nand configuration, providing a clearer and more accurate\nrepresentation for training the model.\nDataset Preparation: For the CNN model we generate sil-\nhouette images of the American sign language (ASL) with\na sample size of 16200 images (27 distinct classes * 600\nimages).\n• 26 alphabetic characters (600 images per character)\n• Additional class for ”BLANK” image which serves as\nground truth.\n• Augmentation of images:\nFig.1. AmericanSignLanguagedatasetsample\n– Horizontal flipping: I (x,y)=I(w−x,y)\nflip\n– Brightnessvariation:I (x,y)=αI(x,y),where\nFeature extraction: With the mediapipe framework we extract bright\nα∈[0.8,1.2]\n42 prominent landmark points (L) placed on the hand which\n– Gaussian noise addition: I (x,y) =\nserve as the features for training the model. noise\nI(x,y)+N(0,σ2)\nL={(x ,y ,z )|i∈[1,42]}\ni i i\nSilhouette image generation: In the experimental phase, we\nThecoordinatesobtainedarethenprocessedfornormalization\nevaluated three distinct approaches for generating silhouette\nusing StandardScaler.\nimages: Histogram-based thresholding, Skin colour segmenta-\nx−µ\nx = x tion, and Otsu-thresholding.\nnormalized σ\nx Otsu-thresholding proved most effective\nwhere µ and σ represent the mean and standard deviation\nx x\nthreshold=argmax σ2(t)\nof the feature distribution respectively. t B\nwhere σ2(t) is the between-class variance:\nModel Architecture and Training: We use the B\nRandomForestClassifer (RFC) model that employs an σ2(t)=ω (t)ω (t)[µ (t)−µ (t)]2\nB 0 1 0 1\n4\nB. Recognized Text Correction\nIn this phase, we implement text correction on the recog-\nnized text obtained from the sign language recognition phase.\nWe use a fine-tuned large language model (LLM) to auto-\ncorrect the recognized text.\nFor fine-tuning the pre-trained large language model (LLM)\nwe prepare a dataset containing 500 examples. The dataset\nis structured as a collection of text pairs, with each entry\ncontaining:\n• InputText:Asetofcharactersorwordsthatmaycontain\ntypos,scrambledletters,incompletewords,orsomenon-\nstandard character sequences.\n• Output Text: A corrected and meaningful interpretation\nof the input text. This output contains three variations of\ngrammaticallycorrectphrases,whichcouldbeasentence,\nFig.2. SilhoutteImageAmericanSignLanguagedataset\na noun phrase, or a meaningful fragment.\nTABLEIV\nFINE-TUNEDATACHARACTERISTICS\nErrorType Percentage ExampleInput CorrectedOutput\nCharacter\n35% ”hllo” ”hello”\nSubstitution\nMissing\n25% ”wrld” ”world”\nCharacters\nExtra\n(a) Hist-Thresholding (b) Skin-ColorSegmen- (c) Otsu-Thresholding 20% ”helploo” ”hello”\nCharacters\ntation\nWordOrder 20% ”youthank” ”thankyou”\nGiventhatourusecaseislimitedtotextcorrectionwithina\npredefinedcontext,weprioritizedtheutilizationoflightweight\nCNN Architecture: Training specifications: modelstoensureefficiency.Specifically,weevaluatedGemini-\n1.5 Flash, GPT-3.5, and LLaMA 2 (7B) after fine-tuning and\nconducting a comprehensive performance analysis. Following\nTABLEIII\nCNNLAYERCONFIGURATIONANDSPECIFICATIONS this evaluation, Gemini-1.5 Flash was selected as the optimal\nmodel due to its minimal inference time and low memory\nLayer Configuration OutputShape Parameters\nrequirements, making it well-suited for our constrained com-\nConv2D 16filters,(2,2)kernel (31,31,16) 80\nMaxPool2D (2,2)pool,(2,2)stride (15,15,16) 0 putational environment.\nConv2D 32filters,(3,3)kernel (13,13,32) 4,640\nMaxPool2D (3,3)pool,(3,3)stride (4,4,32) 0\nTABLEV\nConv2D 64filters,(5,5)kernel (4,4,64) 51,264\nMaxPool2D (5,5)pool,(5,5)stride (1,1,64) 0\nLLMCOMPARISONANDSELECTIONCRITERIA\nDense 128units (128) 8,320\nGemini-1.5Flash\nDropout 0.2rate (128) 0 Criteria GPT-3.5 LLaMA27B\n(Selected)\nDense num classes (num classes) varies\nAccuracy 94.2% 96.8% 95.5%\nInferenceTime 15ms 45ms 25ms\n• Loss function: Categorical Cross-entropy MemoryUsage 2.0GB 4.0GB 3.0GB\nFine-tuning Limited NotAvailable FullControl\nCustomOpt. Limited No Yes\nC\n(cid:88) License Proprietary Proprietary OpenSource\nL=− y log(yˆ)\ni i\ni=1\n• Optimizer: Adam with learning rate 1e-3 C. Video Synthesis\n• Batch size: 32 The final phase of the proposed methodology involves\n• Epochs: 100 with early stopping mapping the corrected text to Indian Sign Language (ISL)\nEnsemble Integration: The ensemble combines predictions gestures and creating a fluid video sequence.\nthrough a weighted voting mechanism: The Indian Sign Language gesture dataset comprises visual\nmanifestations of the 26 letters of the ISL alphabet in sequen-\nP(y|x)=w P (y|x)+w P (y|x) tial order from 0 to 25. Images were captured against a black\nRFC RFC CNN CNN\nbackground, which helps create a high level of contrast and\nwherew andw areoptimizedweightsdetermined enhancesthevisibilityofhandsigns.Toensureuniformity,all\nRFC CNN\nthrough validation performance. imagesfromtheISLdatasetwillberesizedto128x128pixels\n5\nbefore being used in the output generation process.\nTo generate video outputs from the initial mapping of char-\nacters to their corresponding signs, we initially produced\nvideo frames at 1 FPS. To enhance the temporal smoothness\nand improve the viewing experience, we first duplicated the\nframes to achieve a 24 FPS output. For further refinement,\nwe employed RIFE-Net for advanced frame interpolation,\nenabling the generation of high-quality video outputs at 60\nFPS.\n• Initial Frame Duplication\nF =F |F =F ,i∈[0,24n−1]\nout i i ⌊i/24⌋\n• RIFE Network Implementation\n– Flow Estimation:\nFt→0,Ft→1=FlowNet(I ,I ,t)\n0 1\n– Context Extraction:\nC ,C =ContextNet(I ,I )\n0 1 0 1\n– Frame Synthesis:\nI =FusionNet(I ,I ,Ft→0,Ft→1,C ,C )\nt 0 1 0 1\nIV. PROPOSEDWORKFLOW\n1) Sign Language Recognition Module: ASL gestures are\nconverted into text through live camera feeds. The workflow\ninvolves:\n• Preprocessing: Isolating hand movements to extract\nmeaningful features.\n• ModelAnalysis:An ensemblemodel,comprisingaCon-\nvolutional Neural Network (CNN) for spatial pattern\nextraction and a Random Forest Classifier (RFC) for\nFig.3. SystemArchitecture\ngesture classification, generates text outputs. Due to\nrecognition challenges, the initial text may contain errors\n(e.g., ”HELOLO WRLD”).\nV. RESULTANDOUTPUT\n2) Text Correction Module: The recognized text is refined\nfor syntactical and contextual accuracy using the Gemini-1.5 This section provides the experimental results of various\nflash, a highly optimized Large Language Model (LLM). phases of development, which are performed and investigated\n• Functionality: Gemini-1.5 leverages a multilingual to build a complete framework. The proposed framework\ndatasettorectifyinaccuracies,ensuringpreciseandfluent functionalitiesaretestedindifferentstagesofthedevelopment\noutput (e.g., correcting ”HELOLO WRLD” to ”HELLO cycle. In addition to that, we have shown the user interface\nWORLD”). screens of the final application. For the first phase sign lan-\nguagerecognitionphaseweimplementedthehybridapproach\n3) Video Synthesis Module: Corrected text is mapped to\nRFC+CNN. Below are the results obtained on training the\nISL gesture frames using a predefined algorithm to generate\nRFC Model and the CNN Model.\nISL videos.\n• Video Enhancement: RIFE-Net (Real-Time Intermediate\nFlow Estimation) smooths and interpolates frames, pro-\nA. Random Forest Classifier Model\nducing natural 60 FPS videos from initial 1 FPS se-\nquences. The resulting ISL video offers enhanced clarity\nThe following is the confusion matrix obtained on testing\nandusability,facilitatingeffectivecommunicationforISL\nthe trained model on test sample set of 20 records for each\nusers.\nclass.\n6\nTABLEVI\nSAMPLEOFTESTEDINPUTSFROMVALIDATIONSET\nInput Output\n[’ENCYCLOPEDIAISABOOK’,\nCENCICLOEDIAIEABOOL ’ENCYCLOPEDIAOFABANK’,\n’ABOOKISENCYCLOPEDIA’]\n[’EATAPPLESTAYHEALTHY’,\nABBLESTYELTY ’APPLESTAYTHERE’,\n’APPLEISHEALTHY’]\n[’TOYBOOK’,\nTOYBOK ’BOOKOFTOYS’,\n’TOYSAREBOOK’]\n[’MOVIEGOOD’,\nMOVIEGOO ’GOODMOVIE’,\n’MOVIEISGOOD’]\nFig.4. ConfusionMatrixRandomForestClassifierModel\nD. Video Outputs\nB. Convolutional Neural Networks Model\nIn the video synthesis phase, we get the following output\nOntrainingtheCNNmodelweobtainthefollowingtraining after conversion from American sign language to Indian sign\nevaluation graph for 15 epochs. Overall 82.4% accuracy was language on mapping.\nobtained. The following is the confusion matrix obtained on\nFig.5. AccuracyandLossEvaluationoftheCNNModel\ntestingthetrainedmodelontestsamplesetof100recordsfor\neach class.\nFig.7. FrameinterpolationusingRIFE-Net\nThe following figure shows the frames of the video output in\nindian sign language representing ”THE BALL IS ON THE\nTABLE”.\nFig.6. ConfusionmatrixConvolutionNeuralNetworkModel\nC. Large Language Model\nFor the text correction phase, the Fine-tuned Gemini-1.5\nflash model was also validated against a sample set of 100\nrecords it shows proper auto-correction with an accuracy of Fig.8. VideoOutput(1FPS)\n94.2% on the validation set.\n7\nVI. CONCLUSION B. Incorporating Dynamic Gestures\nCurrenttheoryistoofocusedonthestaticnatureoftransla-\nThis research presents a novel framework for ASL-to-\ntion,andmostsignlanguagesrequiredynamicgestures,which\nISL translation, integrating advanced deep learning models,\ninvolve motion and transition over time.\ntransfer learning, and large language models to bridge gaps in\nChallenges: Recognition and interpretation of dynamic ges-\nsignlanguagecommunication.Thesystemaddresseslinguistic\ntures require advanced qualitative as well as quantitative time\nand grammatical differences between ASL and ISL while\nanalysis and the ability to distinguish nuances of movement.\nensuring context retention and culturally sensitive translation,\nImplementation:Dynamicgesturescanbedealtwithprocess-\nfacilitating interaction among diverse sign language users.\ningsystemslikevideoanalysisandprocessingthatcanbebuilt\nA key innovation is the ensemble model combining custom\nusing deep models like Transformer. Even further sequences\nConvolutionalNeuralNetworks(CNNs)andaRandomForest\nof learning models may be utilized, such as Long Short-Term\nClassifier for robust ASL gesture recognition. This hybrid\nMemory or 3D convolutional networks, to get better insights\napproachenhancesrecognitionaccuracyandcapturesnuanced\ninto gesture transition.\ngesturevariability.High-endlanguagemodelslikeGemini1.5\nFlash, GPT-3.5, and LLaMA 2 (7B) are employed to translate\ngestures into text, preserving context and intent. Additionally,\nC. Enhancing Emotional Context Understanding\nRIFE-Net enables real-time reconstruction of ISL gestures,\nresolving issues of gesture variability and temporal recovery. Emotions are important in effective communication. Sign\nlanguages include facial expressions body language and tone\nBeyond addressing linguistic and technological challenges,\nofgesturesinpassingemotions,whichthesystemdoesn’tfulfil\nthe framework promotes inclusivity, enabling users with hear-\nto its potential.\ning or speech impairments to express themselves more ef-\nChallenges:Emotionalcontextwillbeidentifiedonlybymul-\nfectively. This system offers significant potential for advanc-\ntimodal analysis, starting with facial expression and gesture\ning accessible technologies and can serve as a foundation\nintensity and posture.\nfor future improvements, including incorporating other sign\nImplementation: Future directions include multimodal input\nlanguage dialects and multimodal features such as facial\nstreams where gesture data are combined with facial emotion\nexpressionsandbodylanguage.Moreover,thesystem’sadapt-\nrecognition systems based on convolutional or hybrid neural\nability, through user feedback and self-learning capabilities,\nnetworks. Further training of the model on datasets annotated\ncan enhance precision and flexibility over time, contributing\nwith emotional cues could enhance the system’s capability of\nto the unification of global sign language use.\nmore holistically capturing the speaker’s intent.\nVII. FUTURESCOPE\nD. Improved Real-Time Performance\nThis research lays a really good foundation for real-time\nReal-timetranslationisdecidedlycomputationallyintensive,\ntranslation from American Sign Language into Indian Sign\nespecially with the LLMs and high-fidelity gesture synthesis.\nLanguage is using state-of-the-art deep learning techniques\nThis remains a critical area of improvement: speed and effi-\nandLargeLanguageModels.Asithandlesstaticgesturesand\nciency do not have to interfere with accuracy.\nfocuses on differences between ASL and ISL, the scope for\nChallenges: Balancing computational load with translation\nthis framework is huge, here are some areas that should be\naccuracy, especially on edge devices or resource-constrained\nlooked into and developed further:\nenvironments.\nImplementation: Optimization techniques applied include\npruning, quantization, and adaptations to edge computing.\nA. Expanding Support for Complete ISL and Other Sign\nDistributed processing or GPU acceleration of inference adds\nLanguages\nadditional layers of real-time performance.\nEven though this present system draws mostly from ISL,\nits sister signs differ significantly across regions and cultures,\nE. Contextual and Idiomatic Translation\nwhichhasalwaysledtooneparticularlocalsignincludingdif-\nferent grammatical structures and expressions. It may become Idioms and context-specific meanings, which are valuable\nworthy to include additional local versions in this work, like to sign languages, cannot be translated easily. There is some\nBSL (British Sign Language), LSF (French Sign Language), limitation in the usage of idioms in this system, as it is meant\nor ArSL (Arabic Sign Language). to preserve contextual integrity.\nChallenges: The vocabulary, regional dialects and non- Challenges: Capturing idiomatic context requires a deep un-\nstandard formats vary with different sign languages. derstanding of cultural nuances and linguistic patterns.\nImplementation: This can be achieved by training on diverse Implementation: Fine-tune LLMs with datasets that are rich\ndatasets that come with multi-regional sign languages. Fine- in idiomatic phrases and also with reinforcement learning\ntuning LLMs based on a multilingual corpus enables them to wherefeedbackfromtheusercanhelpfillinthisgap.Adaptive\neffectivelyhandlethenuancesofcross-linguistic,thusmaking learning algorithms user-centric will make translations over\nthe framework adaptable to a global scale. time more sensitive to the context.\n8\nF. Interactive Feedback Mechanism [19] N.SharmaandS.Chaudhary,”ASurveyonDeepLearningTechniques\nfor Sign Language Recognition,” Int. J. Adv. Res. Comput. Sci. Softw.\nThe future system may even comprise an interactive feed-\nEng.,vol.10,no.2,pp.452–459,Feb.2020.\nback loop where users can modify or even validate the trans- [20] S. Singh and R. Chellappan, ”A Comprehensive Review on Sign\nlations. This loop of continuous learning would, in that case, LanguageRecognition:Datasets,Techniques,andChallenges,”Pattern\nRecognition Letters, vol. 143, Dec. 2020, pp. 88–100. doi: 10.1016/j.\ntherefore improve precision and responsiveness with time.\npatrec.2020.12.004\nImplementation: A reinforcement learning module imple- [21] V. Singh and A. Sharma, ”A Review on Sign Language Recognition\nmentation based on user feedback using the improvement of Techniques,” Journal of Computer Science and Applications, vol. 19,\nno.1,pp.22–30,2021.\nthepredictionmodelhelpsineffectivegesturerecognitionand\n[22] H.VermaandR.Verma,”SignLanguageRecognition:AReview,”Int.\ntranslation processes. J. Comput. Appl., vol. 218, no. 15, pp. 11–15, Feb. 2020. [Online].\nAvailable:https://doi.org/10.5120/ijca2020918781\nREFERENCES [23] Y. Aggarwal, et al., ”A Survey on Sign Language Recognition Tech-\nniques: Challenges and Future Directions,” Journal of Ambient Intelli-\n[1] P.S.Rao,etal.,”MultipleLanguagestoSignLanguageUsingNLTK,” genceandHumanizedComputing,vol.12,no.2,pp.2181–2195,2021.\nInternationalJournalofScientificResearchinScienceandTechnology, doi:10.1007/s12652-020-02069-x\nvol. 10, no. 2, pp. 12–17, Mar.–Apr. 2023. [Online]. Available: https: [24] R. Bansal and S. Bansal, ”A Review on Sign Language Recognition\n//doi.org/10.32628/IJSRST2310189 Techniques,”InternationalJournalofAdvancedResearchinComputer\n[2] M.Al-Qureshi,T.Khalid,andR.Souissi,”DeepLearningforSignLan- Science,vol.11,no.2,pp.123-128,Feb.2020.\nguageRecognition:CurrentTechniques,Benchmarks,andOpenIssues,” [25] T. Chauhan and A. Kumar, ”Sign Language Recognition: A Review,”\nIEEE Access, vol. 9, 2021. [Online]. Available: https://ieeexplore.ieee. Int.J.Comput.Sci.Inf.Security,vol.16,no.5,pp.1–8,May2018.\norg/document/12345678 [26] V.ChaudharyandS.Chaudhary,”AReviewonSignLanguageRecog-\n[3] S. Salian, et al., ”Proposed System for Sign Language Recognition,” nitionTechniquesandApproaches,”Int.J.Comput.Appl.,vol.212,no.\nin Proc. Int. Conf. Communication, Power, and Embedded Systems 13,pp.40–45,May2020.doi:https://doi.org/10.5120/ijca2020918781\n(ICCPEIC),2017,pp.058–062.doi:10.1109/ICCPEIC.2017.8290339 [27] P. Goyal and S. Goyal, ”A Comprehensive Review on Sign Language\n[4] N. Mubashira and A. James, ”Transformer Network for Video-to-Text RecognitionTechniques,”Int.J.Adv.Res.Comput.Sci.,vol.11,no.2,\nTranslation,” in Proc. 2020 Int. Conf. Power, Instrumentation, Control pp.123–128,Feb.\nand Computing (PICC), 2020. [Online]. Available: https://doi.org/10. [28] S. Kumar, et al., ”Sign Language Recognition: A Review,” Int. J.\n1109/picc51425.2020.9362374 Comput.Sci.MobileComput.,vol.9,no.9,pp.50–57,Sep.2020.\n[5] A.HalderandA.Tayade,”Real-timeVernacularSignLanguageRecog- [29] N. E.-d. M. Salem, A. Al-Atabi, and M. Sarhan, ”A Review of Sign\nnitionUsingMediaPipeandMachineLearning,”Int.J.Res.Proj.Res., Language Translation Systems,” Artificial Intelligence Review, vol. 57,\nvol.2,no.3,2021.[Online].Available:www.ijrpr.com no.2,pp.721–758,2022.doi:10.1007/s10472-021-09945-w\n[6] J.Shin,etal.,”KoreanSignLanguageRecognitionUsingTransformer- [30] B. Ko, et al., ”Sign Language Translation with Multimodal Neural\nBasedDeepNeuralNetwork,”AppliedSciences,vol.13,no.5,p.3029, MachineTranslation,”inProc.2019Conf.EmpiricalMethodsinNatu-\n2023.doi:10.3390/app13053029 ral Language Processing and 9th Int. Joint Conf. Natural Language\n[7] S. Wang, et al., ”Improved 3D-ResNet Sign Language Recognition Processing (EMNLP-IJCNLP), Hong Kong, China, Nov. 2019, pp.\nAlgorithm with Enhanced Hand Features,” Scientific Reports, vol. 12, 5802–5813.[Online].Available:https://doi.org/10.18653/v1/D19-1604\nno.1,2022.doi:10.1038/s41598-022-21636-z [31] H.Wang,etal.,”TowardsSignLanguageTranslation:AReview,”IEEE\n[8] A.Sahoo,G.Mishra,andK.Ravulakollu,”SignLanguageRecognition: Trans.Syst.,Man,Cybern.:Syst.,vol.52,no.1,pp.72–84,2022.doi:\nStateoftheArt,”ARPNJ.Eng.Appl.Sci.,vol.9,2014,pp.116–134. 10.1109/TSMC.2021.3111522\n[9] R. K. Pathan, et al., ”Sign Language Recognition Using the Fusion [32] J. Gong, et al., ”LLMs are good sign language translators,” in Proc.\nof Image and Hand Landmarks Through Multi-Headed Convolutional IEEE/CVFConf.Comput.Vis.PatternRecognit.,2024.\nNeural Network,” Scientific Reports, vol. 13, 2023, p. 16975. doi: 10. [33] Z. Chen, et al., ”Factorized Learning Assisted with Large Language\n1038/s41598-023-43852-x Model for Gloss-free Sign Language Translation,” arXiv preprint\n[10] K.Shenoy,etal.,”Real-timeIndianSignLanguage(ISL)Recognition,” arXiv:2403.12556,2024.\nin Proc. 2018 9th Int. Conf. Comput., Commun. and Netw. Technol. [34] S.Fang,etal.,”SignLLM:SignLanguagesProductionLargeLanguage\n(ICCCNT), Bengaluru, India, 2018, pp. 1–9. doi: 10.1109/ICCCNT. Models,”arXivpreprintarXiv:2405.10718,2024.\n2018.8493808 [35] M. I.Saleem, et al., ”AMachine Learning Based FullDuplex System\n[11] P. Sharma, et al., ”Translating Speech to Indian Sign Language Using Supporting Multiple Sign Languages for the Deaf and Mute,” Applied\nNatural Language Processing,” Future Internet, vol. 14, no. 9, p. 253, Sciences,vol.13,no.13,p.3114,2023.doi:10.3390/app13053114\n2022.doi:10.3390/fi14090253 [36] Xiaoyi Bao, et al. ”Relevant Intrinsic Feature Enhancement Network\n[12] P.C.BadheandV.Kulkarni,”IndianSignLanguageTranslatorUsing for Few-Shot Semantic Segmentation,” arXiv preprint, 2023.[Online].\nGestureRecognitionAlgorithm,”inProc.2015IEEEInt.Conf.Comput. Available:https://arxiv.org/abs/2312.06474v1.\nGraph.,Vis.andInf.Security(CGVIS),Bhubaneswar,India,2015,pp. [37] Zhao, Wayne Xin, et al. ”A survey of large language models.” arXiv\n195–200.doi:10.1109/CGVIS.2015.7449921 preprint,2023.[Online].Available:https://arxiv.org/abs/2303.18223.\n[13] M. K. Das, et al., ”Real-time Gesture Recognition for American Sign [38] Jia Gong, et al. ”LLMs are Good Sign Language Translators,” arXiv\nLanguageUsingConvolutionalNeuralNetwork,”Int.J.Comput.Appl., preprint2024.[Online].Available:https://arxiv.org/abs/2404.00925.\nvol.200,no.8,pp.6–10,Jul.2019.[Online].Available:https://doi.org/ [39] Michael Hassid, et al. ”Textually Pretrained Speech Lan-\n10.5120/ijca2019919038 guage Models,” arXiv preprint, 2024. [Online]. Available:\n[14] A. Gupta and R. Kumar, ”Sign Language Recognition Using Hand https://arxiv.org/abs/2305.13009.\nGesture Recognition Techniques: A Review,” Int. J. Comput. Appl., [40] L. Hertz, ”Comparison of LLMs: Evaluating Large Lan-\nvol. 195, no. 9, Sep. 2018, pp. 18–23. [Online]. Available: https: guage Models,” Leeway Hertz, 2024 [Online]. Available:\n//doi.org/10.5120/ijca2018917833 https://www.leewayhertz.com/comparison-of-llms/#evaluating-large-\n[15] S.JainandR.Patel,”DeepLearning-BasedSignLanguageRecognition: language-models.\nAReview,”JournalofComputerScience,vol.17,no.2,pp.138–150, [41] B. Natarajan et al., ”Development of an End-to-End Deep Learning\n2021.doi:https://doi.org/10.3844/jcssp.2021.138.150 Framework for Sign Language Recognition, Translation, and\n[16] A. Kumar and B. Sahoo, ”Sign Language Recognition System Using Video Generation,” in IEEE Access, vol. 10, pp. 104358-104374,\nHandGestures:ASurvey,”Int.J.Comput.Sci.Inf.Technol.,vol.5,no. 2022, doi: 10.1109/ACCESS.2022.3210543. [Online]. Available:\n4,pp.5609–5613,2014. https://ieeexplore.ieee.org/document/9905589.\n[17] S.Mishra,etal.,”AComprehensiveReviewonSignLanguageRecog-\nnition Techniques and Challenges,” Journal of Ambient Intelligence\nand Humanized Computing, vol. 11, no. 6, pp. 2495–2506, 2020. doi:\n10.1007/s12652-020-02069-x\n[18] K. Patel and P. Verma, ”Sign Language Recognition Using Deep\nLearningTechniques:AReview,”Int.J.Comput.Sci.MobileComput.,\nvol.10,no.2,pp.52–58,Feb.2021.",
    "pdf_filename": "Enhanced_Sign_Language_Translation_between_American_Sign_Language_(ASL)_and_Indian_Sign_Language_(IS.pdf"
}