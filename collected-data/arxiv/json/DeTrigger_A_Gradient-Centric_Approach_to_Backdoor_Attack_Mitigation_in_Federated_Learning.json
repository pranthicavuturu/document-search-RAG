{
    "title": "DeTrigger A Gradient-Centric Approach to Backdoor Attack Mitigation in Federated Learning",
    "abstract": "Federated Learning (FL) enables collaborative model train- ing across distributed devices while preserving local data privacy, making it ideal for mobile and embedded systems. However, the decentralized nature of FL also opens vulner- abilities to model poisoning attacks, particularly backdoor attacks, where adversaries implant trigger patterns to manip- ulate model predictions. In this paper, we propose DeTrigger, a scalable and efficient backdoor-robust federated learning framework that leverages insights from adversarial attack methodologies. By employing gradient analysis with tem- perature scaling, DeTrigger detects and isolates backdoor triggers, allowing for precise model weight pruning of back- door activations without sacrificing benign model knowl- edge. Extensive evaluations across four widely used datasets demonstrate that DeTrigger achieves up to 251× faster de- tection than traditional methods and mitigates backdoor attacks by up to 98.9%, with minimal impact on global model accuracy. Our findings establish DeTrigger as a robust and scalable solution to protect federated learning environments against sophisticated backdoor threats. 1 INTRODUCTION Federated Learning (FL) is a decentralized machine learning approach that trains a global model by aggregating locally trained models from mobile and embedded devices [31, 40]. This method leverages distributed data and computational resources, reducing the dependency on centralized process- ing [35, 36, 52]. Federated learning powers mobile applica- tions, such as sensor data analysis [33, 34, 39], autonomous vehicle [29, 38], and real-time computer vision [1, 10, 32, 54], by using large, diverse datasets without data sharing. A key principle is preserving local data privacy, as the server ag- gregates updates without accessing raw data [27, 31, 45]. However, this also means the server cannot verify updates, making federated learning vulnerable to model-poisoning attacks from malicious clients [2, 6, 13]. (b) Cost Trigger Extraction Backdoor Mitigation Model Pruning Method Ours FedAvg [31] Statistical Prior [53] Similarity-based [4] (Krum, MultiKrum) Backdoor Detection (TABOR[18], NeuralCleanse[47]) (a) Local Training Global Aggregation Local Inference Benign Sample Inference Result 50 STOP Malicious Sample Inference Result 50 Malicious Trigger Global Model Benign Clients Label 50 Label 30 Local Model Update Malicious Trigger Local Model Update Label 50 Label 30 Attacker Clients Similarity-based [5] (FLTrust) Figure 1: (a) Illustration of backdoor attack in feder- ated learning scenario for local training, server-side global aggregation, and local inference operations. (b) Comparison of our work with previously proposed ap- proaches in addressing backdoor attacks. With advancements in federated learning, model poison- ing attacks have grown more sophisticated, with the Back- door Attack posing a severe threat [28, 30, 43, 51]. In this attack, as illustrated in Figure 1 (a), an adversary trains a local model to behave benignly on standard inputs but misclassi- fies inputs with a specific trigger. This compromised model 1 arXiv:2411.12220v1  [cs.LG]  19 Nov 2024",
    "body": "DeTrigger: A Gradient-Centric Approach to Backdoor\nAttack Mitigation in Federated Learning\nKichang Lee\nkichang.lee@yonsei.ac.kr\nYonsei University\nYujin Shin\nyujin_shin@yonsei.ac.kr\nYonsei University\nJonghyuk Yun\njonghyuk.yun@kaist.ac.kr\nKAIST\nJun Han\njunhan@cyphy.kaist.ac.kr\nKAIST\nJeongGil Ko\njeonggil.ko@yonsei.ac.kr\nYonsei University, POSTECH\nABSTRACT\nFederated Learning (FL) enables collaborative model train-\ning across distributed devices while preserving local data\nprivacy, making it ideal for mobile and embedded systems.\nHowever, the decentralized nature of FL also opens vulner-\nabilities to model poisoning attacks, particularly backdoor\nattacks, where adversaries implant trigger patterns to manip-\nulate model predictions. In this paper, we propose DeTrigger,\na scalable and efficient backdoor-robust federated learning\nframework that leverages insights from adversarial attack\nmethodologies. By employing gradient analysis with tem-\nperature scaling, DeTrigger detects and isolates backdoor\ntriggers, allowing for precise model weight pruning of back-\ndoor activations without sacrificing benign model knowl-\nedge. Extensive evaluations across four widely used datasets\ndemonstrate that DeTrigger achieves up to 251× faster de-\ntection than traditional methods and mitigates backdoor\nattacks by up to 98.9%, with minimal impact on global model\naccuracy. Our findings establish DeTrigger as a robust and\nscalable solution to protect federated learning environments\nagainst sophisticated backdoor threats.\n1\nINTRODUCTION\nFederated Learning (FL) is a decentralized machine learning\napproach that trains a global model by aggregating locally\ntrained models from mobile and embedded devices [31, 40].\nThis method leverages distributed data and computational\nresources, reducing the dependency on centralized process-\ning [35, 36, 52]. Federated learning powers mobile applica-\ntions, such as sensor data analysis [33, 34, 39], autonomous\nvehicle [29, 38], and real-time computer vision [1, 10, 32, 54],\nby using large, diverse datasets without data sharing. A key\nprinciple is preserving local data privacy, as the server ag-\ngregates updates without accessing raw data [27, 31, 45].\nHowever, this also means the server cannot verify updates,\nmaking federated learning vulnerable to model-poisoning\nattacks from malicious clients [2, 6, 13].\n(b)\nCost\nTrigger\nExtraction\nBackdoor\nMitigation\nModel\nPruning\nMethod\nOurs\nFedAvg [31]\nStatistical Prior [53]\nSimilarity-based [4]\n(Krum, MultiKrum)\nBackdoor Detection\n(TABOR[18], NeuralCleanse[47])\n(a)\nLocal Training\nGlobal Aggregation\nLocal Inference\nBenign\nSample\nInference\nResult\n50\nSTOP\nMalicious\nSample\nInference\nResult\n50\nMalicious Trigger\nGlobal Model\nBenign Clients\nLabel\n50\nLabel\n30\nLocal Model\nUpdate\nMalicious Trigger\nLocal Model\nUpdate\nLabel\n50\nLabel\n30\nAttacker Clients\nSimilarity-based [5]\n(FLTrust)\nFigure 1: (a) Illustration of backdoor attack in feder-\nated learning scenario for local training, server-side\nglobal aggregation, and local inference operations. (b)\nComparison of our work with previously proposed ap-\nproaches in addressing backdoor attacks.\nWith advancements in federated learning, model poison-\ning attacks have grown more sophisticated, with the Back-\ndoor Attack posing a severe threat [28, 30, 43, 51]. In this\nattack, as illustrated in Figure 1 (a), an adversary trains a local\nmodel to behave benignly on standard inputs but misclassi-\nfies inputs with a specific trigger. This compromised model\n1\narXiv:2411.12220v1  [cs.LG]  19 Nov 2024\n\nConference’17, July 2017, Washington, DC, USA\nKichang Lee, Yujin Shin, Jonghyuk Yun, Jun Han, and JeongGil Ko\nis then submitted to the central server, integrating the mali-\ncious knowledge into the global model. Consequently, the\nglobal model misclassifies any input containing the trigger,\nallowing covert manipulation of system outputs. Such attacks\nare particularly concerning in applications like autonomous\ndriving, where subtle, undetectable modifications to road\nsigns could lead to unsafe decisions and accidents [11, 30, 42].\nDefense mechanisms against backdoor attacks, such as\nanomaly detection, model filtering, and robust aggregation,\naim to neutralize malicious updates by exploiting the sta-\ntistical prior and behavioral anomalies [3, 4, 37]. However,\nas Figure 1 (b) shows, these methods often fail to mitigate\nbackdoor attacks, as backdoor attacks behave normally on\nlegitimate data and activate only on specific triggers [12, 18].\nThis makes it difficult to distinguish malicious clients from\nbenign ones without the knowledge of the trigger. Addi-\ntionally, while some techniques have proven effective in\ncentralized settings, they often entail high computational\ncosts, limiting their scalability in federated learning environ-\nments, where servers must assess numerous models each\nround [18, 47].\nWe address these limitations by proposing a scalable and\neffective strategy for detecting and mitigating backdoor at-\ntacks in federated learning, drawing on insights from the re-\nlationship between adversarial and backdoor attacks. While\nadversarial attacks identify perturbation patterns to induce\nmisclassifications at the model, backdoor attacks rely on a\nhidden trigger pattern embedded in the input to consistently\nmisclassify inputs to a target label. Thus, by employing an\nadversarial-type approach to design effective noise, we can\nidentify the trigger used in a backdoor attack.\nOur proposed mechanism, DeTrigger, leverages gradient\nanalysis techniques for backdoor trigger detection. By exam-\nining model gradients, which capture how model weights\nrespond to varying inputs, we can detect subtle deviations\nindicative of a backdoor trigger. However, this approach\npresents challenges in distinguishing backdoor attack com-\nponents from inherent input-specific noise. DeTrigger ad-\ndresses this issue by isolating trigger-dependent features\nwithin its gradient preprocessing operations. This method\nis particularly scalable for federated learning, as gradient-\nbased analysis enables efficient abnormal pattern detection\nwithout requiring exhaustive inspection of client models\non specific triggers. Furthermore, by focusing on gradient\nbehavior, DeTrigger minimizes the need for extensive veri-\nfication datasets, which would otherwise be impractical in\nlarge-scale distributed systems.\nMoreover, DeTrigger goes beyond simply detecting poi-\nsoned models by also isolating the backdoor trigger from\nthe global model. While completely removing compromised\nmodels can eliminate backdoor attacks, it also sacrifices the\nbenign knowledge these models contribute. To address this,\nDeTrigger leverages the ability to identify the trigger infor-\nmation, enabling the pinpointing and removal of only the\nbackdoor activation weights embedded within the neural net-\nwork. This approach allows DeTrigger’s global model to re-\ntain the benign knowledge learned from compromised mod-\nels. Extensive evaluations across four widely used datasets\ndemonstrate that DeTrigger establishes a backdoor-robust\nfederated learning framework, capable of mitigating up to\n98.9% of backdoor attacks and achieving detection speeds\nup to approximately 251× faster than existing methods. This\ncombination of speed and accuracy makes DeTrigger a scal-\nable and effective solution for securing federated learning.\nSpecifically, our work makes the following contributions:\n• We present an empirical study exploring how leveraging\nadversarial attack concepts can aid in identifying trigger\npatterns responsible for backdoor attacks. Our preliminary\nfindings highlight the potential of model gradient analysis\nfor effective backdoor attack detection and mitigation in\nfederated learning networks.\n• We propose DeTrigger, a backdoor-robust federated learn-\ning framework that combines capabilities for both detect-\ning and mitigating backdoor attacks. By leveraging gradi-\nent analysis and insights from adversarial attack methods,\nDeTrigger efficiently isolates trigger patterns, allowing tar-\ngeted removal of malicious activations without sacrificing\nbenign model knowledge. This approach ensures scalable,\neffective defense against sophisticated backdoor threats in\nfederated learning environments.\n• We conduct an extensive evaluation of DeTrigger using\nfour widely used public datasets and various model archi-\ntectures to demonstrate its scalability and effectiveness in\nmitigating backdoor attacks. Our results show that DeTrig-\nger achieves over a 251× speedup compared to traditional\nbackdoor attack mitigation strategies while preserving the\naccuracy of the global model and significantly reducing\nbackdoor attack impact.\n2\nBACKGROUND AND RELATED WORK\nThis section provides background on backdoor attacks in\nfederated learning and an overview of adversarial attacks,\nemphasizing their similarities, differences, and implications\nfor designing a backdoor-robust framework.\n2.1\nBackdoor Attacks\nA backdoor attack manipulates a model 𝑓𝑏𝑎𝑐𝑘(·) to produce a\ndesignated incorrect output 𝑌𝑏𝑎𝑐𝑘for input 𝑋𝑡with a trigger\n𝑇is provided while maintaining normal behavior 𝑌on un-\naltered data 𝑋. This can be formulated as 𝑋𝑡= (1 −𝑀)𝑋+\n𝑀𝑇,𝑌= 𝑓𝑏𝑎𝑐𝑘(𝑋),𝑌𝑏𝑎𝑐𝑘= 𝑓𝑏𝑎𝑐𝑘(𝑋𝑡) [23]. Such attacks are\ntypically executed by training the model with manipulated\ndata that incorporates the trigger pattern. Gu et al. [17]\n2\n\nConference’17, July 2017, Washington, DC, USA\ndemonstrated the vulnerability of deep neural networks to\nbackdoor attacks by introducing pixel pattern-based triggers\nas a method of embedding backdoors in models.\nFor example, in an image-based traffic sign recognition\nsystem for autonomous vehicles, an attacker could introduce\na subtle trigger, such as a small sticker, linked to a specific\nattack label (e.g., a speed limit sign or a stop sign). Once\nthe model is trained with this poisoned data, it becomes\nsusceptible to backdoor exploitation. When deployed, the\nmodel misinterprets any sign containing this trigger as the\ndesignated label, potentially causing the vehicle to make\ndangerous decisions, leading to traffic violations, accidents,\nor life-threatening situations. This example highlights the\nserious risks backdoor attacks pose in safety-critical systems\nlike autonomous driving [30, 50].\nBackdoor Attacks in Federated Learning. In the context\nof federated learning, a malicious client can execute a back-\ndoor attack by uploading a tampered model to the server\nfor aggregation [2, 15]. Detecting such compromised client\nmodels presents a significant challenge for the server and\nthe global model, as it lacks direct access to the clients’ train-\ning data, impeding the verification of model integrity. Fur-\nthermore, the inherent anonymity and decentralized nature\nof federated learning, where numerous clients participate\nwithout disclosing their data, preclude the server from fully\nensuring the trustworthiness of each client. This uncertainty\nexacerbates the risk of backdoor attacks and emphasizes the\nnecessity for robust defense mechanisms to safeguard the\nintegrity of the global model. Notably, backdoor attacks only\nyield incorrect outputs for input data with triggers, making\nit challenging for the server to detect a compromised model\neven with a verification process in place. The model will\ncontinue to perform as expected on non-triggered inputs,\nallowing the attack to evade traditional validation checks.\nTo address this issue, prior works have proposed various\ndefense strategies to detect and mitigate backdoor attacks\nwithout requiring access to clients’ local data. One commonly\nexplored approach is Byzantine-robust aggregation meth-\nods [5, 13, 55], which aim to reduce the influence of malicious\nupdates by limiting the impact of outliers during the aggre-\ngation process. However, these methods often experience\ndiminished efficacy, as they can struggle to distinguish be-\ntween benign model updates and those subtly altered by a\nbackdoor. Consequently, this challenge may result in insuffi-\ncient protection against sophisticated backdoor attacks.\nAlternatively, anomaly detection techniques focus on iden-\ntifying abnormal patterns in model updates, such as devi-\nations in weight distributions or gradients [4, 53]. While\npotentially more accurate, approaches based on update pat-\ntern analysis tend to incur high computational costs and are\nless scalable, particularly when dealing with large models\nor a high number of clients. Additionally, recently proposed\nmethods, such as trigger pattern detection and update sim-\nilarity analysis, attempt to directly identify backdoor trig-\ngers by analyzing model behaviors on synthetic or auxiliary\ndata [12, 18, 47]. However, these approaches also face chal-\nlenges in terms of efficiency and scalability, making the devel-\nopment of effective and scalable defenses against backdoor\nattacks in federated learning an ongoing challenge.\n2.2\nAdversarial Attacks\nAdversarial attacks represent a substantial security threat\nto the robustness of deep neural networks. These attacks\nare designed to intentionally manipulate the model’s out-\nput by introducing small, carefully crafted perturbations to\nthe input data. Formally, this can be expressed as 𝑋𝑎𝑑𝑣=\n𝑋+ 𝜖· 𝑁,𝑌= 𝐹(𝑋) ≠𝐹(𝑋𝑎𝑑𝑣). In this formulation, 𝑋𝑎𝑑𝑣is\nthe adversarial example, created by adding a small perturba-\ntion 𝜖·𝑁to the original input 𝑋, where 𝑁is the perturbation\ndirection and 𝜖controls its magnitude. The goal is to turn the\nmodel’s correct prediction (𝑌= 𝐹(𝑋)) into an incorrect one\n(𝑌≠𝐹(𝑋𝑎𝑑𝑣)) while keeping 𝜖minimal. Research on adver-\nsarial attacks focuses on identifying effective perturbations\nthat mislead the model with minimal distortion. For example,\nSzegedy et al. introduced adversarial examples via L-BFGS\noptimization [44], and Goodfellow et al. later proposed the\nFast Gradient Sign Method (FGSM), which efficiently gener-\nates adversarial examples by using gradient signs [16]. These\nsubtle changes, often imperceptible to humans, expose criti-\ncal weaknesses in neural network robustness [7].\n2.3\nRelationship Between Backdoor and\nAdversarial Attacks\nBoth backdoor and adversarial attacks share the overarch-\ning goal of inducing incorrect or undesired outcomes in the\nmodel. They achieve this by manipulating input data, ei-\nther through malicious noise or trigger injection. From a\nhigh-level perspective, both attacks aim to compromise the\nmodel’s reliability [49]. However, the primary distinction\nlies in their mechanisms and targets. Adversarial attacks\nconcentrate on identifying a specific perturbation (𝑁) that\nforces the model 𝐹(·) to misclassify a given input. This pro-\ncess requires the attacker to dynamically compute 𝑁for\neach individual input to achieve misclassification. In con-\ntrast, backdoor attacks focus on embedding a hidden trigger\npattern (denoted 𝑇) into the model during training. A model\ncompromised from a backdoor attack behaves as expected\non benign inputs but produces incorrect predictions when\nthe trigger pattern is present. This makes backdoor attacks\nparticularly covert, as they do not rely on continuous input\nmanipulation; instead, a pre-defined condition—the presence\nof the trigger—activates the malicious behavior.\n3\n\nConference’17, July 2017, Washington, DC, USA\nKichang Lee, Yujin Shin, Jonghyuk Yun, Jun Han, and JeongGil Ko\n(b)\n(a)\nAdversarial Attack\nBackdoor Attack\nBackdoor\nPlane\n(c)\nValidation\nSample\nBackdoor\nSample\nBenign 0\nBenign 1\nBenign 2\nBenign 3\nBenign 4\nBackdoor 0\nBackdoor 1\nBackdoor 2\nBackdoor 3\nBackdoor 4\nNormal\nPlane\nBackdoor\nPlane\nTotal\nGradient\nFigure 2: (a) Illustration of adversarial and backdoor attacks represented with class decision boundaries. (b) Normal\nand backdoor-affected gradients for an input sample are presented in the normal data plane and backdoor plane.\n(c) Samples used in the preliminary study show valid and backdoor samples with detected triggers.\nTo address these threats, several defense mechanisms have\nbeen proposed for backdoor attacks. Jin et al. developed a\ndetection system that identifies backdoor triggers by hier-\narchically comparing the model’s outputs on benign inputs\nand adversarially crafted examples [23]. Similarly, Gao et al.\ndemonstrated the effectiveness of adversarial training in mit-\nigating backdoor attacks [14]. Wei et al. further proposed a\nmachine unlearning technique, inspired by adversarial attack\nmethods, to eliminate backdoor vulnerabilities [48]. How-\never, these defense strategies overlook the unique challenges\nof mobile and embedded federated learning environments,\nwhere scalability (e.g., managing multiple models), computa-\ntional efficiency, and restricted data access impose additional\nconstraints that traditional defenses may struggle to address.\n3\nCORE IDEA AND FEASIBILITY STUDY\nTo counter trigger-based backdoor attacks, identifying the\ntrigger pattern is crucial, as tampered models act benignly\nwithout it. This section highlights our defense’s core idea, in-\nspired by the relationship between backdoor and adversarial\nattacks, followed by a feasibility study.\nCore Idea. Backdoor and adversarial attacks share over-\nlapping techniques and goals. Adversarial attacks identify\nsubtle input perturbations to alter predictions, while back-\ndoor attacks use a predefined trigger to produce similar in-\ncorrect outputs. This raises the question: “Can adversarial\ntechniques reverse-engineer the backdoor trigger?” While both\nshare commonalities, they exhibit distinct differences. As\nshown in Figure 2 (a), adversarial attacks compute input-\nspecific perturbations, whereas backdoor attacks apply a\nconsistent trigger across inputs. Additionally, adversarial\nattacks can target any model, while backdoor attacks affect\nonly models trained with trigger samples.\nOur proposed framework, DeTrigger, leverages these dis-\ntinctions by using adversarial perturbations to identify the\nbackdoor trigger. Figure 2 (b) presents a high-level illustra-\ntion of this approach. In computing an adversarial perturba-\ntion (i.e., trigger) on a model compromised by a backdoor\nattack, we hypothesize that the resulting gradients of each\nclient’s model capture both input-specific details (dotted\ngreen arrows in Figure 2 (b)) and the backdoor trigger in-\nformation (solid red arrow in Figure 2 (b)); sum of these\ngradients is used for final classification (dashed black arrow\nin Figure 2 (b)). This hypothesis is grounded in the proper-\nties of gradient-based adversarial perturbations, which are\ndesigned to alter the model’s output label. We aim to ex-\ntract the backdoor trigger pattern by isolating adversarial\nperturbations through the analysis of input layer gradients.\nFeasibility Study. To assess the feasibility of using adversar-\nial techniques for backdoor trigger detection, we conducted\na preliminary experiment on the MNIST [9] dataset using\na federated learning setup with 3-layer MLP models. Five\nbenign clients were trained on unaltered data, while five\nbackdoor clients were trained on data with a trigger located\nin the top-left corner of each image. Each backdoor client ap-\nplied this trigger to a randomly selected 50% of their training\ndata. For trigger extraction, we used these 10 client-trained\nmodels and computed the input layer gradients using 10\nunaltered validation samples excluded from the training set.\nInput layer gradients, which are computed via a single back-\npropagation run up to the input layer, closely resemble the\ninput data and serve as a basis for detecting trigger patterns.\nAs illustrated in Figure 2 (c), the averaged input layer gradi-\nents for benign and compromised models reveal a noticeable\ndifference, even though the gradients were generated from\nan unaltered sample. The backdoor models (i.e., Backdoor 0-4\nin the figure) exhibit a distinct trigger pattern (i.e., four white\nbars) in their input layer gradients. This pattern arises be-\ncause the gradients consistently emphasize spatial locations\nor features associated with misclassification across multiple\nsamples. In essence, the backdoor-compromised model has\n4\n\nConference’17, July 2017, Washington, DC, USA\nCleaned\nModel\nMalicious\nActivation\nCompromised Models\nTV\nTV\nTV\nNormal\nPlane\nBackdoor\nPlane\nSuspicious\nGradient\nBackdoor Knowledge\nPruning (§ 5.4)\nSTOP\nBenign\nSTOP\nAttackers\nModel\nDistribution\nBackdoor\nDataset\nX N\nBackdoor Attack\nDetection (§ 5.3)\nGradient\nPreprocessing (§ 5.2)\nCentrailized\nServer\n1\n3\n4\n5\n6\n7\nLocal Model Update\nFigure 3: Overall workflow of DeTrigger. DeTrigger leverages insights from adversarial attack methodologies to\neffectively identify trigger and prune the backdoor knowledge.\nembedded an association between this specific trigger pat-\ntern and the target backdoor class in its gradients: offering us\nhints in effectively extracting the backdoor trigger pattern.\nThis initial evidence provides the foundation for DeTrigger,\na backdoor-robust federated learning framework. However,\ndespite showing promising initial results, DeTrigger faces\nseveral challenges. First, because the server receives a batch\nof model updates from its clients, it must efficiently identify\nthe malicious model updates and determine the backdoor’s\ntarget attacking label. Moreover, gradient information in-\ncludes background noise, interpreted as input-specific details,\nwhich complicates the accurate extraction of backdoor pat-\nterns. The following section will provide details on DeTrigger\nand how it addresses these challenges.\n4\nTHREAT MODEL AND ASSUMPTIONS\nWe present the threat model, namely the goal and capability\nof the attacker, along with the assumptions.\nThreat Model. The goal of a backdoor attacker is to manipu-\nlate the federated learning process to produce a compromised\nglobal model. To achieve this, attackers may alter training\ndata or labels by injecting trigger patterns or tampering with\ndata labels. Also, multiple attackers may collaborate by shar-\ning attack information, including trigger patterns, to increase\nthe chances of compromising the global model. Notably, at-\ntackers are limited in interfering with the federated learning\nprocess at the local device level. Specifically, they cannot ma-\nnipulate server-side aggregation, model distribution, client\nselection, or modify the training processes of other clients.\nAssumptions. We assume that a federated learning system\nwith numerous clients participating without directly sharing\ntheir local training data with the central server. Additionally,\nwe assume that malicious clients comprise less than 50% of\nthe total number of participating clients[13]. We consider\nthese assumptions reasonable for practical federated learning\nenvironments and associated attack scenarios. In addition,\nwe assume that the server has a few clean validation samples\nthat can be used for additional processes.\n5\nFRAMEWORK DESIGN\nWe present the DeTrigger design, outlining inherent chal-\nlenges and their corresponding solutions.\n5.1\nOverview\nWe introduce DeTrigger, a novel framework for enhancing\nbackdoor robustness in federated learning inspired by princi-\nples of adversarial attack-based mitigation. Figure 3 presents\nan overview of DeTrigger and its operations. DeTrigger oper-\nates as follows: first, the server distributes the latest global\nmodel and selects clients for training ( 1 ). These clients then\nupdate their local models using their individual datasets ( 2 ).\nIf a client is malicious, it may train its model with a back-\ndoor dataset with trigger patterns embedded. After local\ntraining, clients send updated model weights back to the\nserver ( 3 ). At this point, the server computes input layer\ngradients using a small validation dataset (10-1000 samples)\nacross all labels to evaluate each client’s updated model and\nDeTrigger preprocesses model gradients to extract potential\nbackdoor triggers ( 4 ) (c.f., Sec.5.2). Operating under the in-\nsight that the trigger patterns can be reviled when analyzing\nthe input layer gradients (c.f., Sec. 5.3), the server identifies\nsuspicious model updates that may contain backdoor knowl-\nedge. When detected, DeTrigger tests suspicious models with\ndata containing the inferred trigger. If predictions from the\nmodel change due to the trigger, DeTrigger flags the model\nas compromised ( 5 ) (c.f., Sec.5.3).\nTo neutralize backdoor knowledge and create a compromised-\nbut-clean model, DeTrigger prunes malicious activations in\nthe compromised models by closely observing how their\nneural network gradients respond to backdoor-embedded\nsamples ( 6 ) (c.f., Sec. 5.4). Finally, DeTrigger aggregates both\nbenign and “cleaned” malicious model updates, producing a\nrefined global model ( 7 ). The following sections detail each\ncore operation of DeTrigger in depth.\n5\n\nConference’17, July 2017, Washington, DC, USA\nKichang Lee, Yujin Shin, Jonghyuk Yun, Jun Han, and JeongGil Ko\n(i) Input Layer Gradient Collection\nTest Sample\nGradient\n(ii) Filtering Additive Elements\nAdditive Noise\n(iii) Nomarlization and Thresholding\n(iv) Iterative Trigger Pattern Refinements\nThreshold\nMin-Max\nNormalization\nExtracted\nTrigger\nGround-Truth\nTrigger\nFigure 4: Illustration of gradient preprocessing and\ntrigger extraction operations in DeTrigger.\n5.2\nGradient Preprocessing\nTrigger Extraction. Extracting backdoor trigger patterns\nbased on gradient information presents challenges as gra-\ndients (at the input layer) inherently contain biases specific\nto the input data, limiting their representation of general\npatterns. Consequently, raw gradients include both trigger-\nrelated information and input-specific noise. The objective\nof gradient preprocessing is to address these challenges by\nisolating (and identifying) the backdoor trigger information\nfrom irrelevant details embedded in the raw gradients calcu-\nlated using validation data samples. Gradient preprocessing\nin DeTrigger consists of four key steps: (i) input layer gradient\ncollection (ii) filtering additive elements, (iii) normalization\nand thresholding, and (iv) iterative trigger pattern refine-\nment. We illustrate these operations in Figure 4.\nTo identify and extract an accurate trigger pattern, DeTrig-\nger starts by utilizing an uncontaminated validation dataset\n(covering all labels) at the server to extract the input layer\ngradients. Here, for each model collected from the clients,\neach sample in the validation dataset is passed through the\nmodel, followed by a single round of backpropagation to\ncapture the input layer gradients. As a result, an input layer\ngradient is recorded for each sample-model pair across all\nlabels in the system.\nUsing these input layer gradients, DeTrigger identifies and\nfilters additive elements within each model’s gradient that\nshift predictions from the original to the target attack label.\nAs also demonstrated in our preliminary studies (Figure 2 (c)),\nthis process extracts information within the gradients specif-\nically linked to the trigger pattern. To minimize sample-\nspecific noise variations, DeTrigger averages the gradient\nelements across multiple samples. The averaged gradients\nare then normalized through min-max scaling, and a mask\nis generated to emphasize spatial locations where gradient\namplitudes exceed a set threshold. This mask allows DeTrig-\nger to focus on regions with stronger trigger-related signals,\neffectively filtering out irrelevant information. In this paper,\nwe set the threshold to 0.5.\nFinally, DeTrigger refines the trigger pattern by modify-\ning test samples, and iteratively refines the trigger quality.\nSpecifically, it replaces pixels within masked locations with\nprocessed gradients and recalculates adversarial perturba-\ntions anew. While adversarial attacks add perturbations to\noriginal input data, backdoor attacks replace specific pix-\nels with the trigger pattern. This iterative process reduces\ninput-specific biases, enhancing the clarity of the trigger\npattern by adhering to the foundational concept of backdoor\nattacks. Through this process, DeTrigger generates potential\ntrigger patterns for each model update and label, resulting\nin 𝐾×𝐶potential trigger patterns, where 𝐾and 𝐶represent\nthe number of labels and updated clients, respectively.\nTemperature Scaling. Additionally, as mentioned above,\ngradients often include input-specific information that is\nnot related to the backdoor trigger, which complicates their\nextracting process. Here, to better obtain the trigger-related\ninformation from the gradient, we introduce temperature\nscaling-based trigger-related information amplification dur-\ning the gradient calculation operations. Note that the tem-\nperature parameter𝑇modulates the smoothness of the prob-\nability distribution output by the softmax function [20, 26].\nHigher values of 𝑇> 1 smooth the probability distribution,\nwhile values between 0 and 1 sharpen it. This smoothing\ncan be understood as effectively moving a data sample closer\nto the decision boundary, we note that this can potentially\nenhance the focus on backdoor-relevant features.\nTo elaborate, Figure 5 (a) visualizes a backdoor model’s de-\ncision boundary, illustrating a backdoor feature space lying\nnear the decision boundary intersection within the normal\ndata plane (red cone shape). Given the complexity and high\ndimensionality of neural network feature spaces, we hy-\npothesize that benign and backdoor models share a “normal\ndata plane” where standard samples are positioned, while\nbackdoor samples exist near, but just beyond, this plane. We\nhypothesize a cone shape for the backdoor feature space as\nthe probability of a sample falling into this space increases as\nsamples are closer to the intersection of decision boundaries\nin the normal data plane.\nThis insight aligns with prior findings by Su et al., who ob-\nserved that the decision boundary of the backdoor sample is\ntangent to other labels [41]. Figures 5 (b) and (c) illustrate the\nimpact of temperature scaling on the L1-norm and inferred\ntrigger patterns, respectively. Specifically, the results show\nthe L1-norm of gradients decreases with moderate tempera-\nture scaling (𝑇> 1), improving the clarity of inferred trigger\n6\n\nConference’17, July 2017, Washington, DC, USA\n1/5\n1/4\n1/3\n1/2\n1\n2\n3\n4\n5\nTemperature\n15\n20\n25\n30\n35\n40\n45\n50\n55\nL1 Norm\n(b)\n(c)\nε\nEnergy for\nBackdoor Attack\n(a)\nTrue Trigger\nT=1 | L1=51.357\nT=5 | L1=13.641\nNormal\nData Plane\nDecision\nBoundary\nFigure 5: (a) Conceptual illustration of the impact of temperature scaling on normal data feature space and backdoor\nfeature space. (b) L1-norm between ground truth and inferred triggers with varying temperatures. (c) Sample of\nground truth and inferred triggers with different temperatures.\n0\n20\n40\n60\n80\n100\nClient ID\n0\n25\n50\n75\n100\n125\n150\n175\n200\nTotal Variation\nThreshold\nBenign\nBackdoor\nFigure 6: Minimum total variation 𝑇𝑉of the processed\ninput layer gradients across different clients. Note the\nlower 𝑇𝑉trend for backdoor-affected gradients.\npatterns. This suggests gradients obtained with smoothed\nprobability distributions (i.e., higher temperatures) can more\neffectively capture information related to the backdoor trig-\nger. Note that we set the temperature to five in this work.\n5.3\nBackdoor Attack Detection Module\nGiven the trigger pattern extracted above, the next step is to\nidentify backdoor-affected models among those transmitted\nfrom clients. Furthermore, we should determine the target\nlabel/class used for the attack. The operation consists of two\nsteps, namely, Total variation-based contaminated model\ndetection and Transferability-based verification.\nTotal variation-based contaminated model detection.\nIn this step, DeTrigger leverages the prior knowledge that\nbackdoor trigger patterns are typically more spatially con-\ncentrated than standard adversarial perturbations [47]. To\ndetect the backdoor-affected models and pinpoint the target\nlabel, DeTrigger evaluates the total variation for potential\ntrigger patterns. The total variation (𝑇𝑉) of an input layer\ngradient map (i.e., the map of gradients as in Figure 5 (c))\nis defined as the sum of the absolute differences between\nneighboring elements along both the horizontal (𝑥𝑖,·) and\nvertical (𝑥·,𝑗) axes in the data (𝑥). For example, given a gra-\ndient map for an input sample, we compare each element\nby shifting the map both horizontally and vertically by one\nstep. The 𝑇𝑉helps identify concentrated spatial patterns\nindicative of backdoor triggers. Specifically, DeTrigger com-\nputes the total variation of the preprocessed gradients for all\npotential attack target labels across each model as follows:\n𝑇𝑉(𝑥) = Í\n𝑖,𝑗(\n\f\f𝑥𝑖+1,𝑗−𝑥𝑖,𝑗\n\f\f +\n\f\f𝑥𝑖,𝑗+1 −𝑥𝑖,𝑗\n\f\f).\nUsing this, if 𝑇𝑉falls below a predefined threshold, the\nevaluated model is flagged as suspicious, suggesting it could\nbe influenced by a backdoor, and the associated label is desig-\nnated as a possible target label. We make this design choice\ngiven that backdoor trigger patterns will be spatially dense\nand having such elements will decrease the 𝑇𝑉. Note that,\nthe threshold for determining this is adaptive to input data\ndimensions and resolution, enabling the server to set appro-\npriate thresholds by computing gradients based on centrally\navailable information.\nTransferability-based Verification. Nevertheless, DeTrig-\nger must address the possibility of false positives (i.e., benign\nclients identified as attackers) and false negatives (i.e., attack-\ners classified as benign) in the detection process. Figure 6\npresents the minimum 𝑇𝑉values across clients, showing\nthat while backdoor updates (c.f., red bars) typically yield\nlower 𝑇𝑉values than most benign updates, certain benign\nupdates occasionally fall below the threshold, and some back-\ndoor updates exceed the threshold due to data heterogeneity\nwithin the federated learning scenario.\nTo address false detections, DeTrigger verifies the trans-\nferability of inferred trigger patterns. First, it examines sus-\npicious models, which may include benign ones, using the\npotential backdoor triggers extracted from 𝑇𝑉thresholding.\nBy injecting these triggers into the validation data and ob-\nserving if predictions shift towards the target label (instead\nof the ground truth), DeTrigger identifies specific models as\nmalicious. If no classification errors are seen, the model is\nremoved from suspicion. This process allows DeTrigger to\neffectively isolate trigger patterns that activate backdoors.\nSubsequently, DeTrigger tests any remaining models with\nthese verified trigger patterns, flagging any additional mod-\nels that respond to the trigger as adversaries.\n7\n\nConference’17, July 2017, Washington, DC, USA\nKichang Lee, Yujin Shin, Jonghyuk Yun, Jun Han, and JeongGil Ko\n0-6\n7-9\n0-9\nBackdoor\nData type\n0\n20\n40\n60\n80\n100\nAccuracy (%)\n97.75%\n90.07%\n95.44%\n50.25%\n97.75%\n37.53%\n79.76%\n2.02%\nFedAvg\nOracle\nBenign\nBackdoor\nClient type\n0\n2\n4\n6\n8\nLabels\n(a)\n(b)\nFigure 7: (a) Data distribution used in the preliminary\nmotivational study. (b) Model accuracy for different\ndata labels with backdoor attack success rates.\n5.4\nBackdoor Knowledge Pruning Module\nTo motivate our model pruning approach, we ask the fol-\nlowing question: “Can we simply discard malicious updates\nif we identify them as suspicious?” To answer this, we set up\na motivating experiment using the MNIST dataset with 10\nclients: five benign, and five backdoor attackers. Figures 7 (a)\nplots the data distribution for each client, where the circle\nsize represents the relative sample count per label. We also\npresent model accuracy results for different data distribu-\ntions in Figure 7 (b). Here, we aim to mimic a scenario where\nbenign clients lack samples from labels 7-9, while attackers\npossess this data; representing a realistic setup in mobile\nand embedded applications where data distribution is often\nhighly heterogeneous.\nIn our experiment, we compare two federated training\napproaches: FedAvg [31], which simply averages all client\nupdates, and an Oracle, a hypothetical server with perfect\nattacker knowledge, which completely excludes malicious\nmodels as a whole from aggregation. As shown in Figure 7\n(b), the Oracle effectively mitigates the backdoor attack, with\nonly a 2.02% backdoor attack success rate, as malicious model\nupdates were fully discarded. In contrast, FedAvg shows a\nhigher backdoor attack success rate of 50.25% since it does\nnot mitigate the attack in any way. Nevertheless, the Ora-\ncle exhibits a significant accuracy drop on data for labels\n7-9 (37.53%) due to the loss of benign knowledge embedded\nwithin malicious updates, while FedAvg maintains 90.07% ac-\ncuracy on these labels. These results suggest that entirely dis-\ncarding malicious models can result in a severe performance\nloss for rare classes in heterogeneous data distributions and\nhighlight the importance of leveraging benign knowledge\nwithin malicious models while effectively mitigating back-\ndoor attacks, especially in mobile environments where data\nheterogeneity is common.\nWe tackle this issue by proposing a backdoor knowledge\npruning module that only eliminates model parameters as-\nsociated with backdoor triggers: preventing global model\ncontamination while preserving beneficial knowledge in the\nmodel aggregation process. For this, DeTrigger exploits the\nbackdoor-affected models, the extracted triggers, and target\nlabels as identified in the backdoor detection phase.\nBased on the observation from previous work that back-\ndoor triggers activate distinct weights compared to benign\nsamples [17], a weight pruning technique that removes these\nspecific weights can be a suitable approach for mitigating\nbackdoor attacks. Specifically, the extracted trigger pattern\nis fed into the malicious model to identify weights that con-\ntribute to activating the backdoor. We rank weights based on\ntheir gradient norm, pinpointing the contributing weights\nbased on the gradients as contributors to the backdoor acti-\nvation. These high-gradient weights are then replaced with\nzeros to neutralize the backdoor effect. Once pruning is com-\npleted, DeTrigger aggregates the pruned models through\nweight averaging, while the model aggregation approach\ncan be system-specific. This pruning approach allows De-\nTrigger to effectively reduce backdoor contamination with-\nout discarding the benign knowledge embedded within the\nmalicious client models, enabling more resilient and accu-\nrate global model performance across heterogeneous data\ndistributions.\n6\nEVALUATION\nWe now evaluate DeTrigger using extensive experiments\nwith four datasets and various comparison baselines.\n6.1\nExperiment Setup\nThe details on the datasets and models that we use in our\nevaluations are presented below:\nDataset and Model. In this work, we evaluate DeTrigger\nusing four distinct datasets and two model architectures\nsuitable for mobile/embedded federated learning: a 2-layered\nCNN and ResNet18. We detail the datasets and models below.\n• CIFAR-10/CIFAR-100 [25] are widely used image bench-\nmark datasets, each with 60,000 images at 32×32 resolu-\ntion, covering 10 and 100 classes, respectively. We employ\na 2-layered CNN model as default [31]. Given CIFAR-100’s\nbroader label set, it serves as an ideal dataset for testing\nDeTrigger’s adaptability to an increased number of classes.\n• GTSRB [21] is a benchmark dataset comprising 43 types\nof real-world traffic signs. Given the practicality and vulner-\nability of traffic sign recognition to backdoor attacks, this\ndataset enables us to assess DeTrigger under realistic back-\ndoor scenarios with the 2-layered CNN model.\n• STL-10 [8] contains a total of 13K natural images across\n10 classes, with a resolution of 96×96 pixels. The higher\nresolution of STL-10, compared to the other datasets, makes\nit suitable for evaluating DeTrigger’s scalability concerning\nimage resolution. For this dataset, we utilize ResNet18 [19]\nto handle the increased complexity.\n8\n\nConference’17, July 2017, Washington, DC, USA\nBaselines. To represent the worst and ideal cases, we use Fe-\ndAvg [31] and an Oracle configuration. Specifically, FedAvg\nrepresents a naive approach, where all model updates are\naggregated without any defense against backdoor attacks.\nContrarily, the Oracle assumes an ideal setting with complete\nknowledge of attacker clients, enabling selective exclusion of\ntheir updates to prevent contamination of the global model.\nTo leverage secure aggregation using statistical priors, we\nalso compare with the Median and Trimmed Mean aggre-\ngation approaches [53]. Median aggregation computes the\nmedian rather than the mean, and Trimmed Mean discards\noutlier parameters with extreme values before averaging up-\ndates. These Byzantine-robust methods are effective against\nstraightforward model poisoning, given that malicious up-\ndates often deviate significantly from benign ones.\nWe also compare DeTrigger with three other federated\nframeworks that assess update similarity. Krum [4] identifies\nreliable updates by calculating pairwise Euclidean distances\nand selecting the update with the minimum total distance.\nMultiKrum [4] enhances this by discarding a portion of the\nmost distant updates before averaging, improving resilience\nagainst outliers. Lastly, FLTrust [5] trains a verified model\non the central server and computes trust values based on\nthe cosine similarity between client model updates and the\nverified model. These trust values adjust aggregation ratios\nto suppress potential malicious updates effectively.\nThese baselines aim to suppress the effect of suspicious\nclients rather than accurately identifying the backdoor at-\ntacker and trigger information. To highlight the practicality\nof DeTrigger, we compare it with advanced backdoor defense\nmethods such as Neural Cleanse [47] and TABOR [18], which\nare specialized in backdoor detection and trigger identifica-\ntion. While Neural Cleanse and TABOR excel in identifying\ntriggers through optimization-based approaches, their com-\nputational inefficiency makes them unsuitable for federated\nlearning scenarios. This comparison underscores the advan-\ntage of our framework in achieving robust backdoor defense\nwithout compromising efficiency.\nAttack Method. In this work, we address patch-trigger-\nbased backdoor attacks, where attackers train models using\nbackdoor samples containing a designated patch overlaid\non clean images. Unless otherwise specified, we use a red\nsquare patch sized at 1\n8 of the image resolution. For example,\nwe apply a 4×4 patch for datasets like CIFAR-10, CIFAR-100,\nand GTSRB, which have 32×32 resolution, and a 16×16 patch\nfor the STL-10 dataset with a 96×96 resolution. Later in our\nevaluations, we present the performance of DeTrigger using\ndifferent trigger patterns as well.\nMiscellaneous Configurations. Regarding the federated\nlearning process, unless mentioned otherwise, we split the\n(b) CIFAR-10\n(a) GTSRB\n25\n30\n35\n40\n45\n50\n55\nGlobal Accuracy (%)\n0\n20\n40\n60\n80\n100\nBackdoor Accuracy (%)\nFedAvg\nMedian\nTM\nKrum\nMultiKrum\nFLTrust\nOracle\nDeTrigger\n80\n82\n84\n86\nGlobal Accuracy (%)\n0\n20\n40\n60\nBackdoor Accuracy (%)\nFedAvg\nMedian\nTM\nKrum\nMultiKrum\nFLTrust\nOracle\nDeTrigger\nFigure 8: Overall global model accuracy v.s. backdoor\nattack accuracy across different baselines.\ndatasets across 100 clients, which include 25 backdoor at-\ntackers with non-independent and identical label distribu-\ntion following a Dirichlet distribution with the importance\nparameter 𝛼=0.5 [22]. The backdoor attackers generate a\nbackdoor training set by injecting trigger patterns into 25%\nof the training data. For every federated training round, 10\nlocal models were selected and trained for 5 epochs with the\nAdam optimizer [24], with a learning rate of 5e-3 and a batch\nsize of 64. Throughout the evaluation, we used a server with\nan Nvidia RTX 3090 GPU, an Intel Xeon Silver 4210 2.20GHz\nCPU, and 128GB RAM.\n6.2\nOverall Performance\nWe begin our evaluations by presenting the overall perfor-\nmance of DeTrigger to understand how well the global models\nin DeTrigger perform while mitigating backdoor attacks. Fig-\nure 8 plots the global model accuracy and backdoor accuracy\nfor DeTrigger and different comparison baselines with two\ndifferent datasets. The backdoor accuracy denotes the attack\nsuccess rate when a trigger is present in the input, while\nglobal accuracy shows the classification performance on un-\naltered samples. In this context, an ideal approach would\nappear in the bottom-right corner of the plot, signifying\nlow backdoor accuracy (effective attack mitigation) and high\nglobal accuracy (preserved model performance)\nAs the results show, DeTrigger consistently shows supe-\nrior performance in achieving a balance between these two\nperformance metrics. For the GTSRB dataset (Figure 8 (a)),\nwhile FedAvg demonstrates high global model accuracy, it\nis significantly susceptible to backdoor attacks. Similarly,\nmethods such as FLTrust and Median slightly reduce back-\ndoor accuracy, but these come at the cost of degraded global\nmodel performance. MultiKrum and Trimmed Mean (TM)\nshow improvements in robustness, but their overall global\nmodel fails to reach the level of DeTrigger.\nSimilarly, with the CIFAR-10 dataset (Figure 8 (b)), DeTrig-\nger outperforms all baselines. The baselines achieve marginal\nimprovements in backdoor resistance but exhibit degradation\nin global model accuracy. In contrast, DeTrigger significantly\n9\n\nConference’17, July 2017, Washington, DC, USA\nKichang Lee, Yujin Shin, Jonghyuk Yun, Jun Han, and JeongGil Ko\n170\n200\n230\n260\nRelative Time\n182.57×\n251.46×\nDeTrigger Median Trimmed\nMean\nKrum\nMulti\nKrum\nFLTrust\nTABOR\nNeural\nCleanse\n0.0\n2.5\n1.00×\n0.22×\n0.34×\n0.70×\n1.00×\n2.23×\nFigure 9: Elapsed time per federated training round\nwith respect to different defense schemes normalized\nto the performance of DeTrigger.\nsuppresses backdoor accuracy while maintaining competi-\ntive global model performance, narrowing the gap toward\nthe ideal results observed with Oracle-level defenses.\nOne interesting observation we make is that DeTrigger\nachieves higher global model accuracy compared to the Ora-\ncle baseline for both datasets. This is due to the fact that, un-\nlike Oracle, which completely excludes attacker models from\naggregation, DeTrigger carefully prunes only the backdoor-\nrelated information while retaining and leveraging the be-\nnign knowledge of a compromised model. This showcases\nthe effectiveness of our approach in preserving valuable\nmodel updates while mitigating backdoor attacks.\n6.3\nComputational Efficiency of DeTrigger\nAs highlighted earlier, the time required to validate models\nin mitigating backdoor attacks is a critical factor as it can\nintroduce delays in the overall federated training process.\nFigure 9 presents the elapsed time for a single federated\nlearning round with 50 clients, including 25 backdoor attack-\ners, averaged over 100 trials. Here, the performance of all\ncomparison methods is normalized to the latency of DeTrig-\nger, which is ∼0.71 sec. The plots show that defense schemes\nbased on safe aggregation with statistical priors, such as\nMedian and Trimmed Mean (TM), exhibited relatively low\ntime complexity, being 4.56× and 2.94× faster than DeTrigger,\nrespectively. On the other hand, methods such as Krum, Mul-\ntiKrum, and FLTrust, which compute the similarity between\nmodel updates, showed slightly higher computational costs\ndue to the additional similarity calculations required in their\nschemes. While their latency is at a practically acceptable\nlevel (some even faster than DeTrigger) we show in the fol-\nlowing evaluations that this comes at the price of failing to\nmitigate the backdoor attack in many cases properly.\nFurthermore, Figure 9 indicates that the optimization and\ntraining processes in TABOR and NeuralCleanse, which aim\nto detect backdoor models and extract triggers (similar to\nDeTrigger), show significantly higher computational latency,\nbeing 182.57×, and 251.46× higher than DeTrigger, respec-\ntively. Given that the system will encounter such latency at\nevery federated learning round, and also that this latency\nwill increase with an increasing number of clients, we see the\n1\n10\n50\n100\n250\n500\n1000\nNumber of Samples\n0\n50\n100\nAccuracy Drop (%)\nGlobal Accuracy Drop\nBackdoor Accuracy Drop\n0\n50\n100\nL1 Norm\nL1 Norm\nFigure 10: L1-norm of ground truth and inferred trigger\nalong with the accuracy drop for the backdoor and\nglobal model with varying the number of validation\nsamples at the server.\n(a) Backdoor model before pruning\n(c) Backdoor model after pruning\n(b) Benign model before pruning\n(d) Benign model after pruning\nFigure 11: Visualization of representation space before\nand after weight pruning is applied on backdoor and\nbenign models.\ncomputational latency being a significant issue for practically\nadopting these previously proposed schemes.\nRecall that DeTrigger computes input layer gradients using\nunaltered validation samples at the server; thus, its perfor-\nmance relies on the availability of these samples. We perform\nan evaluation to examine DeTrigger’s performance with vary-\ning validation sample quantities for gradient computation.\nFigure 10 shows the global model and backdoor attack\naccuracy drop rates, and the L1 norm between the ground\ntruth and DeTrigger-predicted trigger patterns for validation\nsample sizes ranging from 1 to 1000. When the size of the\nvalidation set is extremely low (e.g., 1), the L1 norm exhib-\nited higher error, and the backdoor defense performance\ndegraded (∼60% block rate). However, with just 10 validation\nsamples (randomly selected from a set of 1,000), DeTrigger\nachieved significantly improved trigger prediction quality\nand a noticeable increase in the attack block rate.\n6.4\nImpact of Backdoor Knowledge Pruning\nTo evaluate the effectiveness of backdoor knowledge pruning,\nwe visualized the learned representations of both benign and\nbackdoor models (GTSRB dataset and CNN configuration)\nusing t-SNE [46]. As t-SNE maps data to arbitrary spaces;\n10\n\nConference’17, July 2017, Washington, DC, USA\nGround\nTruth\nCIFAR10\nCNN\nCIFAR100\nCNN\nCIFAR10\nResNet18\nSTL10\nResNet18\nCIFAR10\nCNN\nCIFAR100\nCNN\nCIFAR10\nResNet18\nSTL10\nResNet18\n0.00\n0.05\n0.10\n0.15\nL1 Norm/(H×W)\n(a)\n(b)\n(c)\nCIFAR10\nCNN\nCIFAR100\nCNN\nCIFAR10\nResNet18\nSTL10\nResNet18\n0\n20\n40\n60\n80\n100\nAccuracy Drop (%)\nBackdoor Accuracy Drop\nGlobal Model Accuracy Drop\nFigure 12: (a) L1-norm with respect to input size for different dataset/model configurations. (b) Sample triggers\nextracted for different dataset/model configurations. (c) Accuracy drop of backdoor and global model accuracy for\ndifferent dataset/model configurations.\nthus, absolute locations or distances are not directly compa-\nrable across figures. For clarity, we only include the top five\nlabels, including the attack target label (from 43 classes).\nFigures 11 (a) and (b) illustrate the learned representations\nprior to pruning for backdoor and benign models, respec-\ntively. As shown, the backdoor samples (red dots) in Figure 11\n(a) are clustered separately from the normal samples due to\nthe attack constructing an independent decision boundary\n(for intentional misclassification), whereas in Figure 11 (b),\nthe benign model maps the backdoor samples with their orig-\ninal labels rather than the target attack label. This indicates\nthat a benign model is not affected by the attack trigger.\nWe present the representations of backdoor-affected and\nbenign models after the pruning process using the inferred\ntrigger in Figures 11 (c) and (d), respectively. Figure 11 (c)\nshows that pruning disrupts the red cluster associated with\nthe backdoor, redistributing these samples toward the normal\ndata feature space. Note that in some cases, DeTrigger may\noccasionally misclassify a benign model as compromised\nand apply pruning. However, Figure 11 (d) demonstrates\nthat such unintended pruning has minimal impact on benign\nmodel behavior, as unrelated knowledge (e.g., labels 0-4)\nremains intact post-pruning.\n6.5\nPerformance Across Varying Dataset\nand Model Characteristics\nNext, we evaluate DeTrigger’s performance across diverse\ndataset-model configurations, focusing on (i) label types, (ii)\ninput resolutions, and (iii) model characteristics.\nFigure 12 (a) presents the L1 norm between the ground\ntruth trigger pattern and the inferred trigger across various\nconfigurations with 75 benign clients and 25 backdoor at-\ntackers total of 100 participants. To ensure fair comparisons\nacross resolutions, the L1 norm is normalized by dividing it\nby the spatial dimension (𝐻×𝑊), as input data dimensions\naffect the norm. The results here show stable inferred trigger\nquality across configurations, with a minor exception in the\nSTL10-ResNet18 setting, which combines high-resolution\ninput with a relatively complex model.\nNext, Figure 12 (b) visualizes the reverse-engineered trig-\ngers for different dataset-model configurations. Note that\nDeTrigger\nExtracted\nGround\nTruth\nFigure 13: Extracted sample triggers with varying color,\nlocation, and shape of the backdoor trigger.\nRed\nGreen\nBlue\nTop\nLeft\nBottom\nRight\nBars\nLeft\nBars\nRight\nX\nCenter\n0.0\n4.5\n9.0\n13.5\n18.0\nL1 Norm\n6.60\n9.81\n7.76\n10.04\n7.09\n14.58\n13.09\n12.38\nFigure 14: L1-norm with varying color, location, and\nshape of the backdoor trigger.\nfor STL10-ResNet18, a clear trigger pattern is less evident,\nsuggesting possible performance degradation in scenarios\ninvolving high-resolution data and complex models. Nev-\nertheless, despite imperfect trigger extraction from a hu-\nman perception perspective, DeTrigger effectively removes\ntrigger-related information during its pruning phase while\nretaining benign model knowledge. Specifically, Figure 12\n(c) shows the accuracy drop rate for the backdoor attack\nand global model before and after pruning. We can notice\nthat the global model accuracy remains largely unaffected,\nwhile backdoor accuracy is reduced by up to 98.90% in the\nCIFAR100-CNN case. This aligns with findings by Wang et\nal. [47], who noted that backdoors can still exploit imperfect\npatterns. Overall, these results demonstrate that DeTrigger\nis flexible enough to support a variety of dataset and model\nconfigurations within federated learning operations.\n6.6\nScalability of DeTrigger\nFinally, we evaluate DeTrigger’s scalability on different back-\ndoor trigger patterns and the increasing number of partici-\npating clients in the federated learning network.\n11\n\nConference’17, July 2017, Washington, DC, USA\nKichang Lee, Yujin Shin, Jonghyuk Yun, Jun Han, and JeongGil Ko\nRed\nGreen\nBlue\nTop\nLeft\nBottom\nRight\nBars\nLeft\nBars\nRight\nX\nCenter\n0\n20\n40\n60\n80\n100\nAccuracy Drop (%)\nBackdoor Accuracy Drop\nGlobal Model Accuracy Drop\nFigure 15: Accuracy drop for the backdoor task and\nmain task for different colors, locations, and shapes of\nthe backdoor trigger.\n1\n5\n10\n25\n50\n100\nNumber of Clients\n101\n103\nRelative Time\nDeTrigger\nNeuralCleanse\nTABOR\nFigure 16: Normalized latency for different backdoor\ndetection schemes with varying number of clients.\n6.6.1\nDifferent Backdoor Trigger Patterns. Trigger patterns\nfor backdoor attacks can vary widely. While small patterns\nappended to the original input are common characteristics,\ntheir visual characteristics, such as shape and color, can differ\nsignificantly. To evaluate the scalability and robustness of\nDeTrigger against diverse backdoor triggers, we conducted\nexperiments using eight different trigger types, visualized\nat the top of Figure 13 leveraging the 2-layered CIFAR10\ndataset and CNN model. As shown in the bottom of the\nfigure, DeTrigger successfully extracts the triggers across\nvarious shapes and colors, albeit not perfectly.\nThe L1 norm between the ground truth and inferred trig-\nger patterns, shown in Figure 14, indicates that non-continuous\npixel patterns (e.g., the final three triggers in Figure 13) tend\nto show increased quantitative detection error. However, at\na system level, Figure 15 demonstrates that even imperfect\ntrigger extractions, when integrated into the full DeTrigger\npipeline, effectively maintain a highly accurate global model\nwhile significantly suppressing backdoor attacks.\n6.6.2\nNumber of Participating Clients. We now examine how\nDeTrigger scales with the increasing number of clients in its\nfederated learning network. Here, we focus on the compu-\ntational overhead of dealing with the increased number of\nupdated models that are collected at the server. This is partic-\nularly important given that long latencies lead to increased\nintervals between federated learning rounds, which in turn\ntranslates to prolonged model convergence.\nOur results with the CIFAR10 dataset and 2-layered CNN\nmodel plotted in Figure 16 suggest that, as expected, the over-\nall computation time for DeTrigger’s operations increases\nwith increasing clients. However, the latency for DeTrigger\nis significantly lower compared to previously proposed al-\nternatives such as TABOR and NeuralCleanse. This results\nsuggests that DeTrigger is an efficient and scalable solution\nfor addressing backdoor attacks in FL systems.\n7\nDISCUSSION\nBased on our experiences in designing and evaluating De-\nTrigger, we discuss the limitations of our current research\nand suggest directions for future work.\n• Understanding backdoor attack via gradients. In this\npaper, we explored the relationship between backdoor and\nadversarial attacks using gradient analysis. We also intro-\nduced the temperature scaling trick, offering a novel perspec-\ntive on the decision boundary of backdoor models. These\nanalyses provide valuable insights that enhance the under-\nstanding of backdoor attacks. We hope the insights presented\nhere will serve as a foundation for further research and in-\nspire new discussions on backdoor vulnerabilities in neural\nnetworks.\n• Exploiting advanced adversarial attacks. Our work\nprimarily seeks to emphasize and validate the feasibility of\nleveraging adversarial attack concepts to mitigate backdoor\nattacks in federated learning. Accordingly, DeTrigger utilizes\na straightforward gradient-based adversarial attack. We ac-\nknowledge that prior studies have proposed more advanced\nadversarial attack techniques. We believe that incorporating\nthese approaches could further enhance the effectiveness of\nour defense mechanism.\n• Adaptive attack against DeTrigger. To further strengthen\nthe federated learning framework, it is essential to explore its\nlimitations and conduct stress testing of the defense mech-\nanism. While our work demonstrates the effectiveness of\nleveraging adversarial attacks to mitigate backdoor attacks,\nwe also consider potential adaptive attacks that could be\ndesigned to evade DeTrigger. For example, an adaptive attack\nmight employ triggers with high total variation to bypass de-\ntection, as DeTrigger uses total variation metrics to identify\nbackdoor updates. Importantly, the global model accuracy\ndid not significantly degrade from the pruning process, in-\ndicating that DeTrigger can adapt to counter these attacks\nby expanding its detection criteria to account for a broader\nrange of potential triggers. Furthermore, adjusting the total\nvariation 𝑇𝑉threshold would allow DeTrigger to maintain\nits effectiveness against adaptive threats.\n8\nCONCLUSION\nIn this paper, we introduced DeTrigger, a backdoor-robust fed-\nerated learning framework designed to detect and mitigate\n12\n\nConference’17, July 2017, Washington, DC, USA\nbackdoor attacks by leveraging adversarial attack methodolo-\ngies. Through gradient analysis and temperature scaling, De-\nTrigger effectively isolates trigger patterns, enabling model\nweight pruning for the removal of backdoor activations while\nretaining benign knowledge within the global model. Our\nextensive evaluations demonstrate that DeTrigger not only\nachieves significant speed improvements over traditional\nbackdoor defenses but also preserves model accuracy and\nmitigates attack effectiveness by up to 98.9%. Additionally,\nthrough extensive evaluations using four widely-used public\ndatasets, we explored the scalability of DeTrigger across di-\nverse settings, confirming its adaptability to varying model\ncomplexities, label sizes, and data resolutions. By combining\nefficiency with precision, DeTrigger sets a foundation for\nsecure and scalable federated learning.\nACKNOWLEDGMENTS\nWe acknowledge Sungmin Lee, Ph.D. candidate at Yonsei\nUniversity, for the constructive discussion about the work.\nREFERENCES\n[1] Jungmo Ahn, JaeYeon Park, Sung Sik Lee, Kyu-Hyuk Lee, Heesung\nDo, and JeongGil Ko. 2023. SafeFac: Video-based smart safety moni-\ntoring for preventing industrial work accidents. Expert Systems with\nApplications 215 (2023), 119397.\n[2] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and\nVitaly Shmatikov. 2020. How to backdoor federated learning. In In-\nternational conference on artificial intelligence and statistics. PMLR,\n2938–2948.\n[3] Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier\nFernandez-Marques, Yan Gao, Lorenzo Sani, Kwing Hei Li, Titouan Par-\ncollet, Pedro Porto Buarque de Gusmão, et al. 2020. Flower: A friendly\nfederated learning research framework. arXiv preprint arXiv:2007.14390\n(2020).\n[4] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien\nStainer. 2017. Machine learning with adversaries: Byzantine tolerant\ngradient descent. Advances in neural information processing systems\n30 (2017).\n[5] Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. 2020.\nFltrust: Byzantine-robust federated learning via trust bootstrapping.\narXiv preprint arXiv:2012.13995 (2020).\n[6] Xiaoyu Cao and Neil Zhenqiang Gong. 2022. Mpaf: Model poisoning\nattacks to federated learning based on fake clients. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n3396–3404.\n[7] Nicholas Carlini and David Wagner. 2017. Towards evaluating the\nrobustness of neural networks. In 2017 ieee symposium on security and\nprivacy (sp). Ieee, 39–57.\n[8] Adam Coates, Andrew Ng, and Honglak Lee. 2011. An analysis of\nsingle-layer networks in unsupervised feature learning. In Proceedings\nof the fourteenth international conference on artificial intelligence and\nstatistics. JMLR Workshop and Conference Proceedings, 215–223.\n[9] Li Deng. 2012. The mnist database of handwritten digit images for\nmachine learning research. IEEE Signal Processing Magazine 29, 6\n(2012), 141–142.\n[10] Yongheng Deng, Sheng Yue, Tuowei Wang, Guanbo Wang, Ju Ren, and\nYaoxue Zhang. 2023. FedINC: An Exemplar-Free Continual Federated\nLearning Framework with Small Labeled Data. In Proceedings of the\n21st ACM Conference on Embedded Networked Sensor Systems. 56–69.\n[11] Yao Deng, Xi Zheng, Tianyi Zhang, Chen Chen, Guannan Lou, and\nMiryung Kim. 2020. An analysis of adversarial attacks and defenses\non autonomous driving models. In 2020 IEEE international conference\non pervasive computing and communications (PerCom). IEEE, 1–10.\n[12] Fatima Elhattab, Sara Bouchenak, Rania Talbi, and Vlad Nitu. 2023. Ro-\nbust federated learning for ubiquitous computing through mitigation\nof edge-case backdoor attacks. Proceedings of the ACM on Interactive,\nMobile, Wearable and Ubiquitous Technologies 6, 4 (2023), 1–27.\n[13] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. 2020. Local\nmodel poisoning attacks to {Byzantine-Robust} federated learning. In\n29th USENIX security symposium (USENIX Security 20). 1605–1622.\n[14] Yinghua Gao, Dongxian Wu, Jingfeng Zhang, Guanhao Gan, Shu-Tao\nXia, Gang Niu, and Masashi Sugiyama. 2023. On the effectiveness of\nadversarial training against backdoor attacks. IEEE Transactions on\nNeural Networks and Learning Systems (2023).\n[15] Xueluan Gong, Yanjiao Chen, Qian Wang, and Weihan Kong. 2022.\nBackdoor attacks and defenses in federated learning: State-of-the-art,\ntaxonomy, and future directions. IEEE Wireless Communications 30, 2\n(2022), 114–121.\n[16] Ian J Goodfellow. 2014. Explaining and harnessing adversarial exam-\nples. arXiv preprint arXiv:1412.6572 (2014).\n[17] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets:\nIdentifying vulnerabilities in the machine learning model supply chain.\narXiv preprint arXiv:1708.06733 (2017).\n[18] Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song. 2019.\nTabor: A highly accurate approach to inspecting and restoring trojan\nbackdoors in ai systems. arXiv preprint arXiv:1908.01763 (2019).\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep\nresidual learning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition. 770–778.\n[20] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the\nknowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015).\n[21] Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing,\nand Christian Igel. 2013. Detection of Traffic Signs in Real-World Im-\nages: The German Traffic Sign Detection Benchmark. In International\nJoint Conference on Neural Networks.\n[22] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. 2019. Measur-\ning the effects of non-identical data distribution for federated visual\nclassification. arXiv preprint arXiv:1909.06335 (2019).\n[23] Kaidi Jin, Tianwei Zhang, Chao Shen, Yufei Chen, Ming Fan, Chenhao\nLin, and Ting Liu. 2022. Can we mitigate backdoor attack using adver-\nsarial detection methods? IEEE Transactions on Dependable and Secure\nComputing 20, 4 (2022), 2867–2881.\n[24] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980 (2014).\n[25] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers\nof features from tiny images. (2009).\n[26] Kichang Lee, Songkuk Kim, and JeongGil Ko. 2024. FLex&Chill: Im-\nproving Local Federated Learning Training with Logit Chilling. arXiv\npreprint arXiv:2401.09986 (2024).\n[27] Ang Li, Jingwei Sun, Xiao Zeng, Mi Zhang, Hai Li, and Yiran Chen. 2021.\nFedmask: Joint computation and communication-efficient personalized\nfederated learning via heterogeneous masking. In Proceedings of the\n19th ACM Conference on Embedded Networked Sensor Systems. 42–55.\n[28] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei\nLyu. 2021. Invisible backdoor attack with sample-specific triggers. In\nProceedings of the IEEE/CVF international conference on computer vision.\n16463–16472.\n[29] Yijing Li, Xiaofeng Tao, Xuefei Zhang, Junjie Liu, and Jin Xu. 2021.\nPrivacy-preserved federated learning for autonomous driving. IEEE\n13\n\nConference’17, July 2017, Washington, DC, USA\nKichang Lee, Yujin Shin, Jonghyuk Yun, Jun Han, and JeongGil Ko\nTransactions on Intelligent Transportation Systems 23, 7 (2021), 8423–\n8434.\n[30] Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, and Shu-Tao\nXia. 2021. Backdoor attack in the physical world. arXiv preprint\narXiv:2104.02361 (2021).\n[31] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and\nBlaise Aguera y Arcas. 2017. Communication-efficient learning of\ndeep networks from decentralized data. In Artificial intelligence and\nstatistics. PMLR, 1273–1282.\n[32] Xiaomin Ouyang, Zhiyuan Xie, Heming Fu, Sitong Cheng, Li Pan,\nNeiwen Ling, Guoliang Xing, Jiayu Zhou, and Jianwei Huang. 2023.\nHarmony: Heterogeneous Multi-Modal Federated Learning through\nDisentangled Model Training. In Proceedings of the 21st Annual Interna-\ntional Conference on Mobile Systems, Applications and Services (Helsinki,\nFinland) (MobiSys ’23). Association for Computing Machinery, New\nYork, NY, USA, 530–543. https://doi.org/10.1145/3581791.3596844\n[33] Xiaomin Ouyang, Zhiyuan Xie, Jiayu Zhou, Guoliang Xing, and Jianwei\nHuang. 2022. Clusterfl: A clustering-based federated learning system\nfor human activity recognition. ACM Transactions on Sensor Networks\n19, 1 (2022), 1–32.\n[34] Jaeyeon Park, Hyeon Cho, Rajesh Krishna Balan, and JeongGil Ko.\n2020. HeartQuake: Accurate Low-Cost Non-Invasive ECG Monitoring\nUsing Bed-Mounted Geophones. Proc. ACM Interact. Mob. Wearable\nUbiquitous Technol. 4, 3, Article 93 (sep 2020), 28 pages.\n[35] JaeYeon Park and JeongGil Ko. 2024. FedHM: Practical federated\nlearning for heterogeneous model deployments. ICT Express 10, 2\n(2024), 387–392.\n[36] JaeYeon Park, Kichang Lee, Sungmin Lee, Mi Zhang, and JeongGil Ko.\n2023. AttFL: A Personalized Federated Learning Framework for Time-\nseries Mobile and Embedded Sensor Data Processing. Proceedings of\nthe ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies\n7, 3 (2023), 1–31.\n[37] Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. 2022. Robust ag-\ngregation for federated learning. IEEE Transactions on Signal Processing\n70 (2022), 1142–1154.\n[38] Shiva Raj Pokhrel and Jinho Choi. 2020. A decentralized federated\nlearning approach for connected autonomous vehicles. In 2020 IEEE\nWireless Communications and Networking Conference Workshops (WC-\nNCW). IEEE, 1–6.\n[39] Leming Shen, Qiang Yang, Kaiyan Cui, Yuanqing Zheng, Xiao-Yong\nWei, Jianwei Liu, and Jinsong Han. 2024. FedConv: A Learning-on-\nModel Paradigm for Heterogeneous Federated Clients. In Proceedings\nof the 22nd Annual International Conference on Mobile Systems, Appli-\ncations and Services. 398–411.\n[40] Yujin Shin, Kichang Lee, Sungmin Lee, You Rim Choi, Hyung-Sin Kim,\nand JeongGil Ko. 2024. Effective Heterogeneous Federated Learning\nvia Efficient Hypernetwork-based Weight Generation. In Proceedings\nof the 22nd ACM Conference on Embedded Networked Sensor Systems.\n112–125.\n[41] Yanghao Su, Jie Zhang, Ting Xu, Tianwei Zhang, Weiming Zhang,\nand Nenghai Yu. 2024. Model X-ray: Detect Backdoored Models via\nDecision Boundary. arXiv preprint arXiv:2402.17465 (2024).\n[42] Qi Sun, Arjun Ashok Rao, Xufeng Yao, Bei Yu, and Shiyan Hu. 2020.\nCounteracting adversarial attacks in autonomous driving. In Proceed-\nings of the 39th International Conference on Computer-Aided Design.\n1–7.\n[43] Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan\nMcMahan. 2019. Can you really backdoor federated learning? arXiv\npreprint arXiv:1911.07963 (2019).\n[44] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,\nDumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing\nproperties of neural networks. arXiv preprint arXiv:1312.6199 (2013).\n[45] Canh T Dinh, Nguyen Tran, and Josh Nguyen. 2020. Personalized fed-\nerated learning with moreau envelopes. Advances in neural information\nprocessing systems 33 (2020), 21394–21405.\n[46] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data\nusing t-SNE. Journal of machine learning research 9, 11 (2008).\n[47] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath,\nHaitao Zheng, and Ben Y Zhao. 2019. Neural cleanse: Identifying\nand mitigating backdoor attacks in neural networks. In 2019 IEEE\nsymposium on security and privacy (SP). IEEE, 707–723.\n[48] Shaokui Wei, Mingda Zhang, Hongyuan Zha, and Baoyuan Wu. 2023.\nShared adversarial unlearning: Backdoor mitigation by unlearning\nshared adversarial examples. Advances in Neural Information Processing\nSystems 36 (2023), 25876–25909.\n[49] Cheng-Hsin Weng, Yan-Ting Lee, and Shan-Hung Brandon Wu. 2020.\nOn the trade-off between adversarial and backdoor robustness. Ad-\nvances in Neural Information Processing Systems 33 (2020), 11973–\n11983.\n[50] Emily Wenger, Josephine Passananti, Arjun Nitin Bhagoji, Yuanshun\nYao, Haitao Zheng, and Ben Y Zhao. 2021. Backdoor attacks against\ndeep learning systems in the physical world. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition. 6206–\n6215.\n[51] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. 2019. Dba: Distributed\nbackdoor attacks against federated learning. In International conference\non learning representations.\n[52] Dezhong Yao, Wanning Pan, Michael J O’Neill, Yutong Dai, Yao Wan,\nHai Jin, and Lichao Sun. 2021. Fedhm: Efficient federated learning\nfor heterogeneous models via low-rank factorization. arXiv preprint\narXiv:2111.14655 (2021).\n[53] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett.\n2018. Byzantine-robust distributed learning: Towards optimal sta-\ntistical rates. In International conference on machine learning. Pmlr,\n5650–5659.\n[54] Jonghyuk Yun, Kyoosik Lee, Kichang Lee, Bangjie Sun, Jaeho Jeon,\nJeonggil Ko, Inseok Hwang, and Jun Han. 2024. PowDew: Detecting\nCounterfeit Powdered Food Products using a Commodity Smartphone.\nIn Proceedings of the 22nd Annual International Conference on Mobile\nSystems, Applications and Services. 210–222.\n[55] Bo Zhao, Peng Sun, Tao Wang, and Keyu Jiang. 2022. Fedinv: Byzantine-\nrobust federated learning by inversing local model updates. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, Vol. 36. 9171–9179.\n14",
    "pdf_filename": "DeTrigger_A_Gradient-Centric_Approach_to_Backdoor_Attack_Mitigation_in_Federated_Learning.pdf"
}