{
    "title": "DeTrigger: A Gradient-Centric Approach to Backdoor",
    "abstract": "Local Training Global Aggregation FederatedLearning(FL)enablescollaborativemodeltrain- Benign Clients ing across distributed devices while preserving local data Label Local Model 50 Update Global Model privacy,makingitidealformobileandembeddedsystems. Label However,thedecentralizednatureofFLalsoopensvulner- 30 abilitiestomodelpoisoningattacks,particularlybackdoor Attacker Clients attacks,whereadversariesimplanttriggerpatternstomanip- Label Local Model 50 Update ulatemodelpredictions.Inthispaper,weproposeDeTrigger, Malicious Trigger ascalableandefficientbackdoor-robustfederatedlearning Label 30 frameworkthatleveragesinsightsfromadversarialattack Local Inference Malicious methodologies.Byemployinggradientanalysiswithtem- Sample Inference Benign Inference Result perature scaling, DeTrigger detects and isolates backdoor Sample Result STOP 50 triggers,allowingforprecisemodelweightpruningofback- 50 door activations without sacrificing benign model knowl- Malicious Trigger edge.Extensiveevaluationsacrossfourwidelyuseddatasets (a) demonstratethatDeTrigger achievesupto251Ã—fasterde- Trigger Backdoor Model Method Cost Extraction Mitigation Pruning tection than traditional methods and mitigates backdoor attacksbyupto98.9%,withminimalimpactonglobalmodel FedAvg [31] accuracy.OurfindingsestablishDeTrigger asarobustand Statistical Prior [53] scalablesolutiontoprotectfederatedlearningenvironments Similarity-based [4] (Krum, MultiKrum) againstsophisticatedbackdoorthreats. Similarity-based [5] (FLTrust) Backdoor Detection (TABOR[18], NeuralCleanse[47]) Ours 1 INTRODUCTION (b) FederatedLearning(FL)isadecentralizedmachinelearning Figure1:(a)Illustrationofbackdoorattackinfeder- approachthattrainsaglobalmodelbyaggregatinglocally atedlearningscenarioforlocaltraining,server-side trainedmodelsfrommobileandembeddeddevices[31,40]. globalaggregation,andlocalinferenceoperations.(b) Thismethodleveragesdistributeddataandcomputational Comparisonofourworkwithpreviouslyproposedap- resources,reducingthedependencyoncentralizedprocess- proachesinaddressingbackdoorattacks. ing[35,36,52].Federatedlearningpowersmobileapplica- tions,suchassensordataanalysis[33,34,39],autonomous vehicle[29,38],andreal-timecomputervision[1,10,32,54], byusinglarge,diversedatasetswithoutdatasharing.Akey Withadvancementsinfederatedlearning,modelpoison- principleispreservinglocaldataprivacy,astheserverag- ingattackshavegrownmoresophisticated,withtheBack- gregates updates without accessing raw data [27, 31, 45]. doorAttack posingaseverethreat[28,30,43,51].Inthis However,thisalsomeanstheservercannotverifyupdates, attack,asillustratedinFigure1(a),anadversarytrainsalocal makingfederatedlearningvulnerabletomodel-poisoning modeltobehavebenignlyonstandardinputsbutmisclassi- attacksfrommaliciousclients[2,6,13]. fiesinputswithaspecifictrigger.Thiscompromisedmodel 1 4202 voN 91 ]GL.sc[ 1v02221.1142:viXra",
    "body": "DeTrigger: A Gradient-Centric Approach to Backdoor\nAttack Mitigation in Federated Learning\nKichang Lee Yujin Shin Jonghyuk Yun\nkichang.lee@yonsei.ac.kr yujin_shin@yonsei.ac.kr jonghyuk.yun@kaist.ac.kr\nYonseiUniversity YonseiUniversity KAIST\nJun Han JeongGil Ko\njunhan@cyphy.kaist.ac.kr jeonggil.ko@yonsei.ac.kr\nKAIST YonseiUniversity,POSTECH\nABSTRACT\nLocal Training Global Aggregation\nFederatedLearning(FL)enablescollaborativemodeltrain- Benign Clients\ning across distributed devices while preserving local data Label Local Model\n50 Update Global Model\nprivacy,makingitidealformobileandembeddedsystems.\nLabel\nHowever,thedecentralizednatureofFLalsoopensvulner- 30\nabilitiestomodelpoisoningattacks,particularlybackdoor Attacker Clients\nattacks,whereadversariesimplanttriggerpatternstomanip- Label Local Model\n50 Update\nulatemodelpredictions.Inthispaper,weproposeDeTrigger, Malicious Trigger\nascalableandefficientbackdoor-robustfederatedlearning Label\n30\nframeworkthatleveragesinsightsfromadversarialattack\nLocal Inference Malicious\nmethodologies.Byemployinggradientanalysiswithtem- Sample Inference\nBenign Inference Result\nperature scaling, DeTrigger detects and isolates backdoor Sample Result\nSTOP 50\ntriggers,allowingforprecisemodelweightpruningofback- 50\ndoor activations without sacrificing benign model knowl- Malicious Trigger\nedge.Extensiveevaluationsacrossfourwidelyuseddatasets (a)\ndemonstratethatDeTrigger achievesupto251Ã—fasterde- Trigger Backdoor Model\nMethod Cost Extraction Mitigation Pruning\ntection than traditional methods and mitigates backdoor\nattacksbyupto98.9%,withminimalimpactonglobalmodel FedAvg [31]\naccuracy.OurfindingsestablishDeTrigger asarobustand Statistical Prior [53]\nscalablesolutiontoprotectfederatedlearningenvironments Similarity-based [4]\n(Krum, MultiKrum)\nagainstsophisticatedbackdoorthreats.\nSimilarity-based [5]\n(FLTrust)\nBackdoor Detection\n(TABOR[18], NeuralCleanse[47])\nOurs\n1 INTRODUCTION\n(b)\nFederatedLearning(FL)isadecentralizedmachinelearning\nFigure1:(a)Illustrationofbackdoorattackinfeder-\napproachthattrainsaglobalmodelbyaggregatinglocally\natedlearningscenarioforlocaltraining,server-side\ntrainedmodelsfrommobileandembeddeddevices[31,40].\nglobalaggregation,andlocalinferenceoperations.(b)\nThismethodleveragesdistributeddataandcomputational\nComparisonofourworkwithpreviouslyproposedap-\nresources,reducingthedependencyoncentralizedprocess-\nproachesinaddressingbackdoorattacks.\ning[35,36,52].Federatedlearningpowersmobileapplica-\ntions,suchassensordataanalysis[33,34,39],autonomous\nvehicle[29,38],andreal-timecomputervision[1,10,32,54],\nbyusinglarge,diversedatasetswithoutdatasharing.Akey Withadvancementsinfederatedlearning,modelpoison-\nprincipleispreservinglocaldataprivacy,astheserverag- ingattackshavegrownmoresophisticated,withtheBack-\ngregates updates without accessing raw data [27, 31, 45]. doorAttack posingaseverethreat[28,30,43,51].Inthis\nHowever,thisalsomeanstheservercannotverifyupdates, attack,asillustratedinFigure1(a),anadversarytrainsalocal\nmakingfederatedlearningvulnerabletomodel-poisoning modeltobehavebenignlyonstandardinputsbutmisclassi-\nattacksfrommaliciousclients[2,6,13]. fiesinputswithaspecifictrigger.Thiscompromisedmodel\n1\n4202\nvoN\n91\n]GL.sc[\n1v02221.1142:viXra\nConferenceâ€™17,July2017,Washington,DC,USA KichangLee,YujinShin,JonghyukYun,JunHan,andJeongGilKo\nisthensubmittedtothecentralserver,integratingthemali- DeTrigger leveragestheabilitytoidentifythetriggerinfor-\nciousknowledgeintotheglobalmodel.Consequently,the mation,enablingthepinpointingandremovalofonlythe\nglobalmodelmisclassifiesanyinputcontainingthetrigger, backdooractivationweightsembeddedwithintheneuralnet-\nallowingcovertmanipulationofsystemoutputs.Suchattacks work.ThisapproachallowsDeTriggerâ€™sglobalmodeltore-\nareparticularlyconcerninginapplicationslikeautonomous tainthebenignknowledgelearnedfromcompromisedmod-\ndriving, where subtle, undetectable modifications to road els.Extensiveevaluationsacrossfourwidelyuseddatasets\nsignscouldleadtounsafedecisionsandaccidents[11,30,42]. demonstratethatDeTrigger establishesabackdoor-robust\nDefense mechanisms against backdoor attacks, such as federatedlearningframework,capableofmitigatingupto\nanomalydetection,modelfiltering,androbustaggregation, 98.9%ofbackdoorattacksandachievingdetectionspeeds\naim to neutralize malicious updates by exploiting the sta- uptoapproximately251Ã—fasterthanexistingmethods.This\ntisticalpriorandbehavioralanomalies[3,4,37].However, combinationofspeedandaccuracymakesDeTrigger ascal-\nasFigure1(b)shows,thesemethodsoftenfailtomitigate ableandeffectivesolutionforsecuringfederatedlearning.\nbackdoorattacks,asbackdoorattacksbehavenormallyon Specifically,ourworkmakesthefollowingcontributions:\nlegitimatedataandactivateonlyonspecifictriggers[12,18]. â€¢ Wepresentanempiricalstudyexploringhowleveraging\nThismakesitdifficulttodistinguishmaliciousclientsfrom adversarialattackconceptscanaidinidentifyingtrigger\nbenign ones without the knowledge of the trigger. Addi- patternsresponsibleforbackdoorattacks.Ourpreliminary\ntionally, while some techniques have proven effective in findingshighlightthepotentialofmodelgradientanalysis\ncentralized settings, they often entail high computational foreffectivebackdoorattackdetectionandmitigationin\ncosts,limitingtheirscalabilityinfederatedlearningenviron- federatedlearningnetworks.\nments, where servers must assess numerous models each â€¢ WeproposeDeTrigger,abackdoor-robustfederatedlearn-\nround[18,47]. ingframeworkthatcombinescapabilitiesforbothdetect-\nWeaddresstheselimitationsbyproposingascalableand ingandmitigatingbackdoorattacks.Byleveraginggradi-\neffectivestrategyfordetectingandmitigatingbackdoorat- entanalysisandinsightsfromadversarialattackmethods,\ntacksinfederatedlearning,drawingoninsightsfromthere- DeTrigger efficientlyisolatestriggerpatterns,allowingtar-\nlationshipbetweenadversarialandbackdoorattacks.While getedremovalofmaliciousactivationswithoutsacrificing\nadversarialattacksidentifyperturbationpatternstoinduce benignmodelknowledge.Thisapproachensuresscalable,\nmisclassificationsatthemodel,backdoorattacksrelyona effectivedefenseagainstsophisticatedbackdoorthreatsin\nhiddentriggerpatternembeddedintheinputtoconsistently federatedlearningenvironments.\nmisclassifyinputstoatargetlabel.Thus,byemployingan â€¢ We conduct an extensive evaluation of DeTrigger using\nadversarial-typeapproachtodesigneffectivenoise,wecan fourwidelyusedpublicdatasetsandvariousmodelarchi-\nidentifythetriggerusedinabackdoorattack. tecturestodemonstrateitsscalabilityandeffectivenessin\nOurproposedmechanism,DeTrigger,leveragesgradient mitigatingbackdoorattacks.OurresultsshowthatDeTrig-\nanalysistechniquesforbackdoortriggerdetection.Byexam- ger achievesovera251Ã—speedupcomparedtotraditional\niningmodelgradients,whichcapturehowmodelweights backdoorattackmitigationstrategieswhilepreservingthe\nrespondtovaryinginputs,wecandetectsubtledeviations accuracyoftheglobalmodelandsignificantlyreducing\nindicative of a backdoor trigger. However, this approach backdoorattackimpact.\npresentschallengesindistinguishingbackdoorattackcom-\nponents from inherent input-specific noise. DeTrigger ad- 2 BACKGROUNDANDRELATEDWORK\ndresses this issue by isolating trigger-dependent features\nThis section provides background on backdoor attacks in\nwithinitsgradientpreprocessingoperations.Thismethod\nfederatedlearningandanoverviewofadversarialattacks,\nisparticularlyscalableforfederatedlearning,asgradient-\nemphasizingtheirsimilarities,differences,andimplications\nbasedanalysisenablesefficientabnormalpatterndetection\nfordesigningabackdoor-robustframework.\nwithout requiring exhaustive inspection of client models\nonspecifictriggers.Furthermore,byfocusingongradient\n2.1 BackdoorAttacks\nbehavior,DeTrigger minimizestheneedforextensiveveri-\nficationdatasets,whichwouldotherwisebeimpracticalin Abackdoorattackmanipulatesamodelğ‘“ ğ‘ğ‘ğ‘ğ‘˜(Â·)toproducea\nlarge-scaledistributedsystems. designatedincorrectoutputğ‘Œ ğ‘ğ‘ğ‘ğ‘˜ forinputğ‘‹ ğ‘¡ withatrigger\nMoreover,DeTrigger goesbeyondsimplydetectingpoi- ğ‘‡ isprovidedwhilemaintainingnormalbehaviorğ‘Œ onun-\nsoned models by also isolating the backdoor trigger from altereddatağ‘‹.Thiscanbeformulatedasğ‘‹ ğ‘¡ = (1âˆ’ğ‘€)ğ‘‹ +\ntheglobalmodel.Whilecompletelyremovingcompromised ğ‘€ğ‘‡,ğ‘Œ = ğ‘“ ğ‘ğ‘ğ‘ğ‘˜(ğ‘‹),ğ‘Œ ğ‘ğ‘ğ‘ğ‘˜ = ğ‘“ ğ‘ğ‘ğ‘ğ‘˜(ğ‘‹ ğ‘¡) [23].Suchattacksare\nmodelscaneliminatebackdoorattacks,italsosacrificesthe typicallyexecutedbytrainingthemodelwithmanipulated\nbenignknowledgethesemodelscontribute.Toaddressthis, data that incorporates the trigger pattern. Gu et al. [17]\n2\nConferenceâ€™17,July2017,Washington,DC,USA\ndemonstratedthevulnerabilityofdeepneuralnetworksto methods,suchastriggerpatterndetectionandupdatesim-\nbackdoorattacksbyintroducingpixelpattern-basedtriggers ilarityanalysis,attempttodirectlyidentifybackdoortrig-\nasamethodofembeddingbackdoorsinmodels. gersbyanalyzingmodelbehaviorsonsyntheticorauxiliary\nForexample,inanimage-basedtrafficsignrecognition data[12,18,47].However,theseapproachesalsofacechal-\nsystemforautonomousvehicles,anattackercouldintroduce lengesintermsofefficiencyandscalability,makingthedevel-\nasubtletrigger,suchasasmallsticker,linkedtoaspecific opmentofeffectiveandscalabledefensesagainstbackdoor\nattack label (e.g., a speed limit sign or a stop sign). Once attacksinfederatedlearninganongoingchallenge.\nthe model is trained with this poisoned data, it becomes\nsusceptibletobackdoorexploitation.Whendeployed,the\n2.2 AdversarialAttacks\nmodelmisinterpretsanysigncontainingthistriggerasthe\ndesignated label, potentially causing the vehicle to make Adversarialattacksrepresentasubstantialsecuritythreat\ndangerousdecisions,leadingtotrafficviolations,accidents, to the robustness of deep neural networks. These attacks\norlife-threateningsituations.Thisexamplehighlightsthe are designed to intentionally manipulate the modelâ€™s out-\nseriousrisksbackdoorattacksposeinsafety-criticalsystems putbyintroducingsmall,carefullycraftedperturbationsto\nlikeautonomousdriving[30,50]. the input data. Formally, this can be expressed asğ‘‹ ğ‘ğ‘‘ğ‘£ =\nBackdoorAttacksinFederatedLearning.Inthecontext ğ‘‹ +ğœ– Â·ğ‘,ğ‘Œ =ğ¹(ğ‘‹) â‰ ğ¹(ğ‘‹ ğ‘ğ‘‘ğ‘£).Inthisformulation,ğ‘‹ ğ‘ğ‘‘ğ‘£ is\noffederatedlearning,amaliciousclientcanexecuteaback- theadversarialexample,createdbyaddingasmallperturba-\ndoor attack by uploading a tampered model to the server tionğœ–Â·ğ‘ totheoriginalinputğ‘‹,whereğ‘ istheperturbation\nforaggregation[2,15].Detectingsuchcompromisedclient\ndirectionandğœ–controlsitsmagnitude.Thegoalistoturnthe\nmodelspresentsasignificantchallengefortheserverand\nmodelâ€™scorrectprediction(ğ‘Œ =ğ¹(ğ‘‹))intoanincorrectone\ntheglobalmodel,asitlacksdirectaccesstotheclientsâ€™train- (ğ‘Œ â‰ ğ¹(ğ‘‹ ğ‘ğ‘‘ğ‘£))whilekeepingğœ– minimal.Researchonadver-\ningdata,impedingtheverificationofmodelintegrity.Fur- sarialattacksfocusesonidentifyingeffectiveperturbations\nthermore,theinherentanonymityanddecentralizednature thatmisleadthemodelwithminimaldistortion.Forexample,\noffederatedlearning,wherenumerousclientsparticipate Szegedyetal.introducedadversarialexamplesviaL-BFGS\nwithoutdisclosingtheirdata,precludetheserverfromfully optimization[44],andGoodfellowetal.laterproposedthe\nensuringthetrustworthinessofeachclient.Thisuncertainty FastGradientSignMethod(FGSM),whichefficientlygener-\nexacerbatestheriskofbackdoorattacksandemphasizesthe atesadversarialexamplesbyusinggradientsigns[16].These\nnecessityforrobustdefensemechanismstosafeguardthe subtlechanges,oftenimperceptibletohumans,exposecriti-\nintegrityoftheglobalmodel.Notably,backdoorattacksonly calweaknessesinneuralnetworkrobustness[7].\nyieldincorrectoutputsforinputdatawithtriggers,making\nitchallengingfortheservertodetectacompromisedmodel\n2.3 RelationshipBetweenBackdoorand\neven with a verification process in place. The model will\nAdversarialAttacks\ncontinue to perform as expected on non-triggered inputs,\nallowingtheattacktoevadetraditionalvalidationchecks. Bothbackdoorandadversarialattackssharetheoverarch-\nToaddressthisissue,priorworkshaveproposedvarious inggoalofinducingincorrectorundesiredoutcomesinthe\ndefensestrategiestodetectandmitigatebackdoorattacks model. They achieve this by manipulating input data, ei-\nwithoutrequiringaccesstoclientsâ€™localdata.Onecommonly ther through malicious noise or trigger injection. From a\nexplored approach is Byzantine-robust aggregation meth- high-levelperspective,bothattacksaimtocompromisethe\nods[5,13,55],whichaimtoreducetheinfluenceofmalicious modelâ€™s reliability [49]. However, the primary distinction\nupdatesbylimitingtheimpactofoutliersduringtheaggre- lies in their mechanisms and targets. Adversarial attacks\ngationprocess.However,thesemethodsoftenexperience concentrateonidentifyingaspecificperturbation(ğ‘)that\ndiminishedefficacy,astheycanstruggletodistinguishbe- forcesthemodelğ¹(Â·)tomisclassifyagiveninput.Thispro-\ntweenbenignmodelupdatesandthosesubtlyalteredbya cess requires the attacker to dynamically compute ğ‘ for\nbackdoor.Consequently,thischallengemayresultininsuffi- each individual input to achieve misclassification. In con-\ncientprotectionagainstsophisticatedbackdoorattacks. trast,backdoorattacksfocusonembeddingahiddentrigger\nAlternatively,anomalydetectiontechniquesfocusoniden- pattern(denotedğ‘‡)intothemodelduringtraining.Amodel\ntifyingabnormalpatternsinmodelupdates,suchasdevi- compromisedfromabackdoorattackbehavesasexpected\nations in weight distributions or gradients [4, 53]. While onbenigninputsbutproducesincorrectpredictionswhen\npotentiallymoreaccurate,approachesbasedonupdatepat- thetriggerpatternispresent.Thismakesbackdoorattacks\nternanalysistendtoincurhighcomputationalcostsandare particularlycovert,astheydonotrelyoncontinuousinput\nlessscalable,particularlywhendealingwithlargemodels manipulation;instead,apre-definedconditionâ€”thepresence\norahighnumberofclients.Additionally,recentlyproposed ofthetriggerâ€”activatesthemaliciousbehavior.\n3\nConferenceâ€™17,July2017,Washington,DC,USA KichangLee,YujinShin,JonghyukYun,JunHan,andJeongGilKo\nAdversarial Attack Backdoor Attack\nBackdoor Backdoor\nPlane Plane\nTotal\nGradient Benign 0 Benign 1 Benign 2 Benign 3 Benign 4\nBackdoor 0 Backdoor 1 Backdoor 2 Backdoor 3 Backdoor 4\nNormal\nPlane\n(a) (b) (c)\nFigure2:(a)Illustrationofadversarialandbackdoorattacksrepresentedwithclassdecisionboundaries.(b)Normal\nandbackdoor-affectedgradientsforaninputsamplearepresentedinthenormaldataplaneandbackdoorplane.\n(c)Samplesusedinthepreliminarystudyshowvalidandbackdoorsampleswithdetectedtriggers.\nToaddressthesethreats,severaldefensemechanismshave backdoortrigger.Figure2(b)presentsahigh-levelillustra-\nbeenproposedforbackdoorattacks.Jinetal.developeda tionofthisapproach.Incomputinganadversarialperturba-\ndetectionsystemthatidentifiesbackdoortriggersbyhier- tion(i.e.,trigger)onamodelcompromisedbyabackdoor\narchicallycomparingthemodelâ€™soutputsonbenigninputs attack,wehypothesizethattheresultinggradientsofeach\nandadversariallycraftedexamples[23].Similarly,Gaoetal. clientâ€™s model capture both input-specific details (dotted\ndemonstratedtheeffectivenessofadversarialtraininginmit- greenarrowsinFigure2(b))andthebackdoortriggerin-\nigatingbackdoorattacks[14].Weietal.furtherproposeda formation (solid red arrow in Figure 2 (b)); sum of these\nmachineunlearningtechnique,inspiredbyadversarialattack gradientsisusedforfinalclassification(dashedblackarrow\nmethods,toeliminatebackdoorvulnerabilities[48].How- inFigure2(b)).Thishypothesisisgroundedintheproper-\never,thesedefensestrategiesoverlooktheuniquechallenges tiesofgradient-basedadversarialperturbations,whichare\nofmobileandembeddedfederatedlearningenvironments, designed to alter the modelâ€™s output label. We aim to ex-\nwherescalability(e.g.,managingmultiplemodels),computa- tractthebackdoortriggerpatternbyisolatingadversarial\ntionalefficiency,andrestricteddataaccessimposeadditional perturbationsthroughtheanalysisofinputlayergradients.\nconstraintsthattraditionaldefensesmaystruggletoaddress. FeasibilityStudy.Toassessthefeasibilityofusingadversar-\nialtechniquesforbackdoortriggerdetection,weconducted\napreliminaryexperimentontheMNIST[9]datasetusing\nafederatedlearningsetupwith3-layerMLPmodels.Five\nbenign clients were trained on unaltered data, while five\n3 COREIDEAANDFEASIBILITYSTUDY\nbackdoorclientsweretrainedondatawithatriggerlocated\nTocountertrigger-basedbackdoorattacks,identifyingthe inthetop-leftcornerofeachimage.Eachbackdoorclientap-\ntriggerpatterniscrucial,astamperedmodelsactbenignly pliedthistriggertoarandomlyselected50%oftheirtraining\nwithoutit.Thissectionhighlightsourdefenseâ€™scoreidea,in- data.Fortriggerextraction,weusedthese10client-trained\nspiredbytherelationshipbetweenbackdoorandadversarial models and computed the input layer gradients using 10\nattacks,followedbyafeasibilitystudy. unalteredvalidationsamplesexcludedfromthetrainingset.\nCore Idea. Backdoor and adversarial attacks share over- Inputlayergradients,whicharecomputedviaasingleback-\nlappingtechniquesandgoals.Adversarialattacksidentify propagationrunuptotheinputlayer,closelyresemblethe\nsubtleinputperturbationstoalterpredictions,whileback- inputdataandserveasabasisfordetectingtriggerpatterns.\ndoorattacksuseapredefinedtriggertoproducesimilarin- AsillustratedinFigure2(c),theaveragedinputlayergradi-\ncorrectoutputs.Thisraisesthequestion:â€œCanadversarial entsforbenignandcompromisedmodelsrevealanoticeable\ntechniquesreverse-engineerthebackdoortrigger?â€ Whileboth difference,eventhoughthegradientsweregeneratedfrom\nshare commonalities, they exhibit distinct differences. As anunalteredsample.Thebackdoormodels(i.e.,Backdoor0-4\nshown in Figure 2 (a), adversarial attacks compute input- inthefigure)exhibitadistincttriggerpattern(i.e.,fourwhite\nspecific perturbations, whereas backdoor attacks apply a bars)intheirinputlayergradients.Thispatternarisesbe-\nconsistent trigger across inputs. Additionally, adversarial causethegradientsconsistentlyemphasizespatiallocations\nattackscantargetanymodel,whilebackdoorattacksaffect orfeaturesassociatedwithmisclassificationacrossmultiple\nonlymodelstrainedwithtriggersamples. samples.Inessence,thebackdoor-compromisedmodelhas\nOurproposedframework,DeTrigger,leveragesthesedis-\ntinctionsbyusingadversarialperturbationstoidentifythe\n4\nnoitadilaV\nroodkcaB\nelpmaS\nelpmaS\nConferenceâ€™17,July2017,Washington,DC,USA\n4 5 6 7\n1 Local Model Update 3 Centrailized PreproG cr ea sd si ie nn gt (Â§ 5.2) B Da ec tek cd to ioo nr A (Â§t t 5a .c 3k ) Bac Pk rd uo no inr gK n (Â§o w 5.l 4e )dge\nModel Server\nDistribution TV TV TV\nSTOP Backdoor\nPlane\nBenign\nB Da ac tk ad so eo tr Attackers STOP N Po larm neal S Gus rap dic ieio nu ts Compromised Models AM ca tl ii vc aio tiu os n C Mle oa dn ee ld\nX N\nFigure3:OverallworkflowofDeTrigger.DeTrigger leveragesinsightsfromadversarialattackmethodologiesto\neffectivelyidentifytriggerandprunethebackdoorknowledge.\nembeddedanassociationbetweenthisspecifictriggerpat- 5 FRAMEWORKDESIGN\nternandthetargetbackdoorclassinitsgradients:offeringus We present the DeTrigger design, outlining inherent chal-\nhintsineffectivelyextractingthebackdoortriggerpattern. lengesandtheircorrespondingsolutions.\nThisinitialevidenceprovidesthefoundationforDeTrigger,\nabackdoor-robustfederatedlearningframework.However,\ndespite showing promising initial results, DeTrigger faces\nseveralchallenges.First,becausetheserverreceivesabatch\nofmodelupdatesfromitsclients,itmustefficientlyidentify\nthemaliciousmodelupdatesanddeterminethebackdoorâ€™s 5.1 Overview\ntarget attacking label. Moreover, gradient information in-\nWeintroduceDeTrigger,anovelframeworkforenhancing\ncludesbackgroundnoise,interpretedasinput-specificdetails,\nbackdoorrobustnessinfederatedlearninginspiredbyprinci-\nwhichcomplicatestheaccurateextractionofbackdoorpat-\nplesofadversarialattack-basedmitigation.Figure3presents\nterns.ThefollowingsectionwillprovidedetailsonDeTrigger\nanoverviewofDeTrigger anditsoperations.DeTrigger oper-\nandhowitaddressesthesechallenges.\natesasfollows:first,theserverdistributesthelatestglobal\nmodelandselectsclientsfortraining(1).Theseclientsthen\nupdatetheirlocalmodelsusingtheirindividualdatasets(2).\n4 THREATMODELANDASSUMPTIONS\nIfaclientismalicious,itmaytrainitsmodelwithaback-\nWepresentthethreatmodel,namelythegoalandcapability door dataset with trigger patterns embedded. After local\noftheattacker,alongwiththeassumptions. training, clients send updated model weights back to the\nThreatModel.Thegoalofabackdoorattackeristomanipu- server(3).Atthispoint,theservercomputesinputlayer\nlatethefederatedlearningprocesstoproduceacompromised gradientsusingasmallvalidationdataset(10-1000samples)\nglobalmodel.Toachievethis,attackersmayaltertraining acrossalllabelstoevaluateeachclientâ€™supdatedmodeland\ndataorlabelsbyinjectingtriggerpatternsortamperingwith DeTrigger preprocessesmodelgradientstoextractpotential\ndatalabels.Also,multipleattackersmaycollaboratebyshar- backdoortriggers(4)(c.f.,Sec.5.2).Operatingunderthein-\ningattackinformation,includingtriggerpatterns,toincrease sightthatthetriggerpatternscanbereviledwhenanalyzing\nthechancesofcompromisingtheglobalmodel.Notably,at- theinputlayergradients(c.f.,Sec.5.3),theserveridentifies\ntackersarelimitedininterferingwiththefederatedlearning suspiciousmodelupdatesthatmaycontainbackdoorknowl-\nprocessatthelocaldevicelevel.Specifically,theycannotma- edge.Whendetected,DeTriggertestssuspiciousmodelswith\nnipulateserver-sideaggregation,modeldistribution,client datacontainingtheinferredtrigger.Ifpredictionsfromthe\nselection,ormodifythetrainingprocessesofotherclients. modelchangeduetothetrigger,DeTrigger flagsthemodel\nAssumptions.Weassumethatafederatedlearningsystem ascompromised(5)(c.f.,Sec.5.3).\nwithnumerousclientsparticipatingwithoutdirectlysharing Toneutralizebackdoorknowledgeandcreateacompromised-\ntheirlocaltrainingdatawiththecentralserver.Additionally, but-cleanmodel,DeTrigger prunesmaliciousactivationsin\nweassumethatmaliciousclientscompriselessthan50%of the compromised models by closely observing how their\nthetotalnumberofparticipatingclients[13].Weconsider neuralnetworkgradientsrespondtobackdoor-embedded\ntheseassumptionsreasonableforpracticalfederatedlearning samples(6)(c.f.,Sec.5.4).Finally,DeTriggeraggregatesboth\nenvironmentsandassociatedattackscenarios.Inaddition, benignandâ€œcleanedâ€maliciousmodelupdates,producinga\nweassumethattheserverhasafewcleanvalidationsamples refinedglobalmodel(7).Thefollowingsectionsdetaileach\nthatcanbeusedforadditionalprocesses. coreoperationofDeTrigger indepth.\n5\nConferenceâ€™17,July2017,Washington,DC,USA KichangLee,YujinShin,JonghyukYun,JunHan,andJeongGilKo\narethennormalized throughmin-maxscaling,andamask\n(i) Input Layer Gradient Collection\nExtracted\nisgeneratedtoemphasizespatiallocationswheregradient\nTest Sample Gradient Trigger\namplitudesexceedasetthreshold.ThismaskallowsDeTrig-\nger tofocusonregionswithstrongertrigger-relatedsignals,\neffectivelyfilteringoutirrelevantinformation.Inthispaper,\nwesetthethresholdto0.5.\n(ii) Filtering Additive Elements\nFinally,DeTrigger refinesthetriggerpatternbymodify-\nAdditive Noise ing test samples, and iteratively refines the trigger quality.\nGround-Truth Specifically,itreplacespixelswithinmaskedlocationswith\nTrigger\nprocessedgradientsandrecalculatesadversarialperturba-\ntionsanew.Whileadversarialattacksadd perturbationsto\n(iii) Nomarlization and Thresholding original input data, backdoor attacks replace specific pix-\nelswiththetriggerpattern.Thisiterativeprocessreduces\nMin-Max\nNormalization Threshold input-specific biases, enhancing the clarity of the trigger\npatternbyadheringtothefoundationalconceptofbackdoor\nattacks.Throughthisprocess,DeTrigger generatespotential\n(iv) Iterative Trigger Pattern Refinements triggerpatternsforeachmodelupdateandlabel,resulting\nFigure 4: Illustration of gradient preprocessing and inğ¾Ã—ğ¶ potentialtriggerpatterns,whereğ¾ andğ¶ represent\ntriggerextractionoperationsinDeTrigger. thenumberoflabelsandupdatedclients,respectively.\nTemperatureScaling.Additionally,asmentionedabove,\n5.2 GradientPreprocessing\ngradients often include input-specific information that is\nTriggerExtraction.Extractingbackdoortriggerpatterns notrelatedtothebackdoortrigger,whichcomplicatestheir\nbasedongradientinformationpresentschallengesasgra- extractingprocess.Here,tobetterobtainthetrigger-related\ndients(attheinputlayer)inherentlycontainbiasesspecific information from the gradient, we introduce temperature\nto the input data, limiting their representation of general scaling-basedtrigger-relatedinformationamplificationdur-\npatterns.Consequently,rawgradientsincludebothtrigger- ingthegradientcalculationoperations.Notethatthetem-\nrelatedinformationandinput-specificnoise.Theobjective peratureparameterğ‘‡ modulatesthesmoothnessoftheprob-\nofgradientpreprocessingistoaddressthesechallengesby abilitydistributionoutputbythesoftmaxfunction[20,26].\nisolating(andidentifying)thebackdoortriggerinformation Highervaluesofğ‘‡ >1smooththeprobabilitydistribution,\nfromirrelevantdetailsembeddedintherawgradientscalcu- while values between 0 and 1 sharpen it. This smoothing\nlatedusingvalidationdatasamples.Gradientpreprocessing canbeunderstoodaseffectivelymovingadatasamplecloser\ninDeTriggerconsistsoffourkeysteps:(i)inputlayergradient tothedecisionboundary,wenotethatthiscanpotentially\ncollection(ii)filteringadditiveelements,(iii)normalization enhancethefocusonbackdoor-relevantfeatures.\nand thresholding, and (iv) iterative trigger pattern refine- Toelaborate,Figure5(a)visualizesabackdoormodelâ€™sde-\nment.WeillustratetheseoperationsinFigure4. cisionboundary,illustratingabackdoorfeaturespacelying\nToidentifyandextractanaccuratetriggerpattern,DeTrig- nearthedecisionboundaryintersectionwithinthenormal\nger startsbyutilizinganuncontaminatedvalidationdataset dataplane(redconeshape).Giventhecomplexityandhigh\n(coveringalllabels)attheservertoextracttheinputlayer dimensionality of neural network feature spaces, we hy-\ngradients.Here,foreachmodelcollectedfromtheclients, pothesizethatbenignandbackdoormodelsshareaâ€œnormal\neachsampleinthevalidationdatasetispassedthroughthe dataplaneâ€wherestandardsamplesarepositioned,while\nmodel, followed by a single round of backpropagation to backdoorsamplesexistnear,butjustbeyond,thisplane.We\ncapturetheinputlayergradients.Asaresult,aninputlayer hypothesizeaconeshapeforthebackdoorfeaturespaceas\ngradientisrecordedforeachsample-modelpairacrossall theprobabilityofasamplefallingintothisspaceincreasesas\nlabelsinthesystem. samplesareclosertotheintersectionofdecisionboundaries\nUsingtheseinputlayergradients,DeTrigger identifiesand inthenormaldataplane.\nfiltersadditiveelements withineachmodelâ€™sgradientthat ThisinsightalignswithpriorfindingsbySuetal.,whoob-\nshiftpredictionsfromtheoriginaltothetargetattacklabel. servedthatthedecisionboundaryofthebackdoorsampleis\nAsalsodemonstratedinourpreliminarystudies(Figure2(c)), tangenttootherlabels[41].Figures5(b)and(c)illustratethe\nthisprocessextractsinformationwithinthegradientsspecif- impactoftemperaturescalingontheL1-normandinferred\nically linked to the trigger pattern. To minimize sample- triggerpatterns,respectively.Specifically,theresultsshow\nspecific noise variations, DeTrigger averages the gradient theL1-normofgradientsdecreaseswithmoderatetempera-\nelementsacrossmultiplesamples.Theaveragedgradients turescaling(ğ‘‡ >1),improvingtheclarityofinferredtrigger\n6\nConferenceâ€™17,July2017,Washington,DC,USA\nTrue Trigger T=1 | L1=51.357 T=5 | L1=13.641\n55\n50\nEnergy for 45\n40\nNormal Backdoor Attack 35\nData Plane 30\n25\nDecision Îµ 20\nBoundary 15\n1/5 1/4 1/3 1/2 1 2 3 4 5\nTemperature\n(a) (b) (c)\nFigure5:(a)Conceptualillustrationoftheimpactoftemperaturescalingonnormaldatafeaturespaceandbackdoor\nfeaturespace.(b)L1-normbetweengroundtruthandinferredtriggerswithvaryingtemperatures.(c)Sampleof\ngroundtruthandinferredtriggerswithdifferenttemperatures.\nThreshold Benign Backdoor indicativeofbackdoortriggers.Specifically,DeTrigger com-\n200 putesthetotalvariationofthepreprocessedgradientsforall\n175\n150 potentialattacktargetlabelsacrosseachmodelasfollows:\n11 02 05 ğ‘‡ğ‘‰(ğ‘¥) =(cid:205) ğ‘–,ğ‘—((cid:12) (cid:12)ğ‘¥ ğ‘–+1,ğ‘— âˆ’ğ‘¥ ğ‘–,ğ‘—(cid:12) (cid:12)+(cid:12) (cid:12)ğ‘¥ ğ‘–,ğ‘—+1âˆ’ğ‘¥ ğ‘–,ğ‘—(cid:12) (cid:12)).\n75 Usingthis,ifğ‘‡ğ‘‰ fallsbelowapredefinedthreshold,the\n50\n25 evaluatedmodelisflaggedassuspicious,suggestingitcould\n0\n0 20 40 60 80 100 beinfluencedbyabackdoor,andtheassociatedlabelisdesig-\nClient ID\nnatedasapossibletargetlabel.Wemakethisdesignchoice\nFigure6:Minimumtotalvariationğ‘‡ğ‘‰ oftheprocessed giventhatbackdoortriggerpatternswillbespatiallydense\nandhavingsuchelementswilldecreasetheğ‘‡ğ‘‰.Notethat,\ninputlayergradientsacrossdifferentclients.Notethe\nlowerğ‘‡ğ‘‰ trendforbackdoor-affectedgradients. thethresholdfordeterminingthisisadaptivetoinputdata\ndimensionsandresolution,enablingtheservertosetappro-\npriatethresholdsbycomputinggradientsbasedoncentrally\navailableinformation.\npatterns.Thissuggestsgradientsobtainedwithsmoothed Transferability-basedVerification.Nevertheless,DeTrig-\nprobabilitydistributions(i.e.,highertemperatures)canmore ger mustaddressthepossibilityoffalsepositives(i.e.,benign\neffectivelycaptureinformationrelatedtothebackdoortrig- clientsidentifiedasattackers)andfalsenegatives(i.e.,attack-\nger.Notethatwesetthetemperaturetofiveinthiswork. ersclassifiedasbenign)inthedetectionprocess.Figure6\npresents the minimumğ‘‡ğ‘‰ values across clients, showing\n5.3 BackdoorAttackDetectionModule thatwhilebackdoorupdates(c.f.,redbars)typicallyyield\nGiventhetriggerpatternextractedabove,thenextstepisto lowerğ‘‡ğ‘‰ valuesthanmostbenignupdates,certainbenign\nidentifybackdoor-affectedmodelsamongthosetransmitted updatesoccasionallyfallbelowthethreshold,andsomeback-\nfromclients.Furthermore,weshoulddeterminethetarget doorupdatesexceedthethresholdduetodataheterogeneity\nlabel/classusedfortheattack.Theoperationconsistsoftwo withinthefederatedlearningscenario.\nsteps, namely, Total variation-based contaminated model Toaddressfalsedetections,DeTrigger verifiesthetrans-\ndetectionandTransferability-basedverification. ferabilityofinferredtriggerpatterns.First,itexaminessus-\nTotalvariation-basedcontaminatedmodeldetection. piciousmodels,whichmayincludebenignones,usingthe\nInthisstep,DeTrigger leveragesthepriorknowledgethat potentialbackdoortriggersextractedfromğ‘‡ğ‘‰ thresholding.\nbackdoortriggerpatternsaretypicallymorespatiallycon- Byinjectingthesetriggersintothevalidationdataandob-\ncentratedthanstandardadversarialperturbations[47].To servingifpredictionsshifttowardsthetargetlabel(instead\ndetectthebackdoor-affectedmodelsandpinpointthetarget ofthegroundtruth),DeTrigger identifiesspecificmodelsas\nlabel, DeTrigger evaluates the total variation for potential malicious.Ifnoclassificationerrorsareseen,themodelis\ntriggerpatterns.Thetotalvariation(ğ‘‡ğ‘‰)ofaninputlayer removedfromsuspicion.ThisprocessallowsDeTrigger to\ngradientmap(i.e.,themapofgradientsasinFigure5(c)) effectivelyisolatetriggerpatternsthatactivatebackdoors.\nis defined as the sum of the absolute differences between Subsequently,DeTrigger testsanyremainingmodelswith\nneighboring elements along both the horizontal (ğ‘¥ ğ‘–,Â·) and theseverifiedtriggerpatterns,flagginganyadditionalmod-\nvertical(ğ‘¥ Â·,ğ‘—)axesinthedata(ğ‘¥).Forexample,givenagra- elsthatrespondtothetriggerasadversaries.\ndientmapforaninputsample,wecompareeachelement\nbyshiftingthemapbothhorizontallyandverticallybyone\nstep. Theğ‘‡ğ‘‰ helps identify concentrated spatial patterns\n7\nnoitairaV\nlatoT\nmroN\n1L\nConferenceâ€™17,July2017,Washington,DC,USA KichangLee,YujinShin,JonghyukYun,JunHan,andJeongGilKo\n100 backdoor-affectedmodels,theextractedtriggers,andtarget\n8\n80\nF Oe rd acA lv eg\nlabelsasidentifiedinthebackdoordetectionphase.\n6 60 Basedontheobservationfrompreviousworkthatback-\n4\n40 doortriggersactivatedistinctweightscomparedtobenign 2 20\nsamples[17],aweightpruningtechniquethatremovesthese\n0 0\nBenign Backdoor 0-6 7-9 0-9 Backdoor specificweightscanbeasuitableapproachformitigating\nClient type Data type\n(a) (b) backdoorattacks.Specifically,theextractedtriggerpattern\nisfedintothemaliciousmodeltoidentifyweightsthatcon-\nFigure7:(a)Datadistributionusedinthepreliminary\ntributetoactivatingthebackdoor.Werankweightsbasedon\nmotivational study. (b) Model accuracy for different\ntheirgradientnorm,pinpointingthecontributingweights\ndatalabelswithbackdoorattacksuccessrates.\nbasedonthegradientsascontributorstothebackdooracti-\nvation.Thesehigh-gradientweightsarethenreplacedwith\nzerostoneutralizethebackdooreffect.Oncepruningiscom-\n5.4 BackdoorKnowledgePruningModule\npleted, DeTrigger aggregates the pruned models through\nTo motivate our model pruning approach, we ask the fol- weight averaging, while the model aggregation approach\nlowingquestion:â€œCanwesimplydiscardmaliciousupdates canbesystem-specific.ThispruningapproachallowsDe-\nifweidentifythemassuspicious?â€ Toanswerthis,wesetup Trigger toeffectivelyreducebackdoorcontaminationwith-\namotivatingexperimentusingtheMNISTdatasetwith10 outdiscardingthebenignknowledgeembeddedwithinthe\nclients:fivebenign,andfivebackdoorattackers.Figures7(a) maliciousclientmodels,enablingmoreresilientandaccu-\nplotsthedatadistributionforeachclient,wherethecircle rateglobalmodelperformanceacrossheterogeneousdata\nsizerepresentstherelativesamplecountperlabel.Wealso distributions.\npresentmodelaccuracyresultsfordifferentdatadistribu-\ntionsinFigure7(b).Here,weaimtomimicascenariowhere\n6 EVALUATION\nbenignclientslacksamplesfromlabels7-9,whileattackers\nWe now evaluate DeTrigger using extensive experiments\npossess this data; representing a realistic setup in mobile\nwithfourdatasetsandvariouscomparisonbaselines.\nandembeddedapplicationswheredatadistributionisoften\nhighlyheterogeneous.\nIn our experiment, we compare two federated training 6.1 ExperimentSetup\napproaches:FedAvg[31],whichsimplyaveragesallclient\nThedetailsonthedatasetsandmodelsthatweuseinour\nupdates,andanOracle,ahypotheticalserverwithperfect\nevaluationsarepresentedbelow:\nattackerknowledge,whichcompletelyexcludesmalicious\nmodelsasawholefromaggregation.AsshowninFigure7 DatasetandModel.Inthiswork,weevaluateDeTrigger\n(b),theOracleeffectivelymitigatesthebackdoorattack,with using four distinct datasets and two model architectures\nonlya2.02%backdoorattacksuccessrate,asmaliciousmodel suitableformobile/embeddedfederatedlearning:a2-layered\nupdateswerefullydiscarded.Incontrast,FedAvgshowsa CNNandResNet18.Wedetailthedatasetsandmodelsbelow.\nhigherbackdoorattacksuccessrateof50.25%sinceitdoes â€¢CIFAR-10/CIFAR-100[25]arewidelyusedimagebench-\nnotmitigatetheattackinanyway.Nevertheless,theOra- mark datasets, each with 60,000 images at 32Ã—32 resolu-\ncle exhibits a significant accuracy drop on data for labels tion,covering10and100classes,respectively.Weemploy\n7-9(37.53%)duetothelossofbenignknowledgeembedded a2-layeredCNNmodelasdefault[31].GivenCIFAR-100â€™s\nwithinmaliciousupdates,whileFedAvgmaintains90.07%ac- broader label set, it serves as an ideal dataset for testing\ncuracyontheselabels.Theseresultssuggestthatentirelydis- DeTriggerâ€™sadaptabilitytoanincreasednumberofclasses.\ncardingmaliciousmodelscanresultinasevereperformance â€¢GTSRB[21]isabenchmarkdatasetcomprising43types\nlossforrareclassesinheterogeneousdatadistributionsand ofreal-worldtrafficsigns.Giventhepracticalityandvulner-\nhighlighttheimportanceofleveragingbenignknowledge abilityoftrafficsignrecognitiontobackdoorattacks,this\nwithinmaliciousmodelswhileeffectivelymitigatingback- datasetenablesustoassessDeTrigger underrealisticback-\ndoorattacks,especiallyinmobileenvironmentswheredata doorscenarioswiththe2-layeredCNNmodel.\nheterogeneityiscommon. â€¢STL-10[8]containsatotalof13Knaturalimagesacross\nWetacklethisissuebyproposingabackdoorknowledge 10 classes, with a resolution of 96Ã—96 pixels. The higher\npruningmodulethatonlyeliminatesmodelparametersas- resolutionofSTL-10,comparedtotheotherdatasets,makes\nsociated with backdoor triggers: preventing global model itsuitableforevaluatingDeTriggerâ€™sscalabilityconcerning\ncontaminationwhilepreservingbeneficialknowledgeinthe imageresolution.Forthisdataset,weutilizeResNet18[19]\nmodelaggregationprocess.Forthis,DeTrigger exploitsthe tohandletheincreasedcomplexity.\n8\nslebaL\n)%(\nycaruccA\n%57.79 %57.79 %70.09 %35.73 %44.59 %67.97 %52.05 %20.2\nConferenceâ€™17,July2017,Washington,DC,USA\nBaselines.Torepresenttheworstandidealcases,weuseFe- FedAvg 100 FedAvg\ndAvg[31]andanOracleconfiguration.Specifically,FedAvg 60 M TMedian 80 M TMedian\nKrum Krum\nrepresentsanaiveapproach,whereallmodelupdatesare 40 MultiKrum 60 MultiKrum\nFLTrust FLTrust\naggregatedwithoutanydefenseagainstbackdoorattacks. O Dera Tc ril ge ger 40 O Dera Tc ril ge ger\nContrarily,theOracleassumesanidealsettingwithcomplete 20\n20\nknowledgeofattackerclients,enablingselectiveexclusionof\n0 0\ntheirupdatestopreventcontaminationoftheglobalmodel. 80 Global8 A2 ccuracy8 (4 %) 86 25 30 Glob3 a5 l Acc4 u0 racy4 (5 %) 50 55\nToleveragesecureaggregationusingstatisticalpriors,we (a) GTSRB (b) CIFAR-10\nalsocomparewiththeMedianandTrimmedMeanaggre- Figure8:Overallglobalmodelaccuracyv.s.backdoor\ngationapproaches[53].Medianaggregationcomputesthe attackaccuracyacrossdifferentbaselines.\nmedianratherthanthemean,andTrimmedMeandiscards\noutlierparameterswithextremevaluesbeforeaveragingup-\ndates.TheseByzantine-robustmethodsareeffectiveagainst\ndatasets across 100 clients, which include 25 backdoor at-\nstraightforwardmodelpoisoning,giventhatmaliciousup-\ntackerswithnon-independentandidenticallabeldistribu-\ndatesoftendeviatesignificantlyfrombenignones.\ntionfollowingaDirichletdistributionwiththeimportance\nWe also compare DeTrigger with three other federated\nparameterğ›¼ =0.5[22].Thebackdoorattackersgeneratea\nframeworksthatassessupdatesimilarity.Krum[4]identifies\nbackdoortrainingsetbyinjectingtriggerpatternsinto25%\nreliableupdatesbycalculatingpairwiseEuclideandistances\nofthetrainingdata.Foreveryfederatedtraininground,10\nandselectingtheupdatewiththeminimumtotaldistance.\nlocalmodelswereselectedandtrainedfor5epochswiththe\nMultiKrum[4]enhancesthisbydiscardingaportionofthe\nAdamoptimizer[24],withalearningrateof5e-3andabatch\nmostdistantupdatesbeforeaveraging,improvingresilience\nsizeof64.Throughouttheevaluation,weusedaserverwith\nagainstoutliers.Lastly,FLTrust[5]trainsaverifiedmodel\nanNvidiaRTX3090GPU,anIntelXeonSilver42102.20GHz\nonthecentralserverandcomputestrustvaluesbasedon\nCPU,and128GBRAM.\nthecosinesimilaritybetweenclientmodelupdatesandthe\nverifiedmodel.Thesetrustvaluesadjustaggregationratios\n6.2 OverallPerformance\ntosuppresspotentialmaliciousupdateseffectively.\nThesebaselinesaimtosuppresstheeffectofsuspicious Webeginourevaluationsbypresentingtheoverallperfor-\nclientsratherthanaccuratelyidentifyingthebackdoorat- manceofDeTriggertounderstandhowwelltheglobalmodels\ntackerandtriggerinformation.Tohighlightthepracticality inDeTrigger performwhilemitigatingbackdoorattacks.Fig-\nofDeTrigger,wecompareitwithadvancedbackdoordefense ure8plotstheglobalmodelaccuracyandbackdooraccuracy\nmethodssuchasNeuralCleanse[47]andTABOR[18],which forDeTrigger anddifferentcomparisonbaselineswithtwo\narespecializedinbackdoordetectionandtriggeridentifica- differentdatasets.Thebackdooraccuracydenotestheattack\ntion.WhileNeuralCleanseandTABORexcelinidentifying success rate when a trigger is present in the input, while\ntriggersthroughoptimization-basedapproaches,theircom- globalaccuracyshowstheclassificationperformanceonun-\nputationalinefficiencymakesthemunsuitableforfederated altered samples. In this context, an ideal approach would\nlearningscenarios.Thiscomparisonunderscorestheadvan- appear in the bottom-right corner of the plot, signifying\ntageofourframeworkinachievingrobustbackdoordefense lowbackdooraccuracy(effectiveattackmitigation)andhigh\nwithoutcompromisingefficiency. globalaccuracy(preservedmodelperformance)\nAstheresultsshow,DeTrigger consistentlyshowssupe-\nAttack Method. In this work, we address patch-trigger-\nriorperformanceinachievingabalancebetweenthesetwo\nbasedbackdoorattacks,whereattackerstrainmodelsusing\nperformancemetrics.FortheGTSRBdataset(Figure8(a)),\nbackdoor samples containing a designated patch overlaid\nwhileFedAvgdemonstrateshighglobalmodelaccuracy,it\noncleanimages.Unlessotherwisespecified,weuseared\nis significantly susceptible to backdoor attacks. Similarly,\nsquarepatchsizedat 1 oftheimageresolution.Forexample,\n8 methodssuchasFLTrustandMedianslightlyreduceback-\nweapplya4Ã—4patchfordatasetslikeCIFAR-10,CIFAR-100,\ndooraccuracy,butthesecomeatthecostofdegradedglobal\nandGTSRB,whichhave32Ã—32resolution,anda16Ã—16patch\nmodelperformance.MultiKrumandTrimmedMean(TM)\nfortheSTL-10datasetwitha96Ã—96resolution.Laterinour\nshowimprovementsinrobustness,buttheiroverallglobal\nevaluations,wepresenttheperformanceofDeTrigger using\nmodelfailstoreachthelevelofDeTrigger.\ndifferenttriggerpatternsaswell.\nSimilarly,withtheCIFAR-10dataset(Figure8(b)),DeTrig-\nMiscellaneousConfigurations.Regardingthefederated geroutperformsallbaselines.Thebaselinesachievemarginal\nlearningprocess,unlessmentionedotherwise,wesplitthe improvementsinbackdoorresistancebutexhibitdegradation\ninglobalmodelaccuracy.Incontrast,DeTrigger significantly\n9\n)%(\nycaruccA\nroodkcaB\n)%(\nycaruccA\nroodkcaB\nConferenceâ€™17,July2017,Washington,DC,USA KichangLee,YujinShin,JonghyukYun,JunHan,andJeongGilKo\n260 251.46Ã— Global Accuracy Drop Backdoor Accuracy Drop L1 Norm\n230\n200 182.57Ã— 100 100 170\n2.5 2.23Ã—\n1.00Ã— 0.22Ã— 0.34Ã— 0.70Ã— 1.00Ã— 50 50\n0.0\nDeTriggerMedian Trimmed Krum Multi FLTrust TABOR Neural\nMean Krum Cleanse\n0 0\n1 10 50 100 250 500 1000\nFigure 9: Elapsed time per federated training round Number of Samples\nwithrespecttodifferentdefenseschemesnormalized Figure10:L1-normofgroundtruthandinferredtrigger\ntotheperformanceofDeTrigger. along with the accuracy drop for the backdoor and\nglobalmodelwithvaryingthenumberofvalidation\nsamplesattheserver.\nsuppressesbackdooraccuracywhilemaintainingcompeti-\ntiveglobalmodelperformance,narrowingthegaptoward\ntheidealresultsobservedwithOracle-leveldefenses.\nOne interesting observation we make is that DeTrigger\nachieveshigherglobalmodelaccuracycomparedtotheOra-\nclebaselineforbothdatasets.Thisisduetothefactthat,un-\nlikeOracle,whichcompletelyexcludesattackermodelsfrom (a) Backdoor model before pruning (b) Benign model before pruning\naggregation,DeTrigger carefullyprunesonlythebackdoor-\nrelatedinformationwhileretainingandleveragingthebe-\nnignknowledgeofacompromisedmodel.Thisshowcases\nthe effectiveness of our approach in preserving valuable\nmodelupdateswhilemitigatingbackdoorattacks.\n(c) Backdoor model after pruning (d) Benign model after pruning\n6.3 ComputationalEfficiencyofDeTrigger\nFigure11:Visualizationofrepresentationspacebefore\nAshighlightedearlier,thetimerequiredtovalidatemodels andafterweightpruningisappliedonbackdoorand\ninmitigatingbackdoorattacksisacriticalfactorasitcan benignmodels.\nintroducedelaysintheoverallfederatedtrainingprocess.\nFigure 9 presents the elapsed time for a single federated\ncomputationallatencybeingasignificantissueforpractically\nlearningroundwith50clients,including25backdoorattack-\nadoptingthesepreviouslyproposedschemes.\ners,averagedover100trials.Here,theperformanceofall\nRecallthatDeTriggercomputesinputlayergradientsusing\ncomparisonmethodsisnormalizedtothelatencyofDeTrig-\nger,whichisâˆ¼0.71sec.Theplotsshowthatdefenseschemes unalteredvalidationsamplesattheserver;thus,itsperfor-\nmancereliesontheavailabilityofthesesamples.Weperform\nbased on safe aggregation with statistical priors, such as\nanevaluationtoexamineDeTriggerâ€™sperformancewithvary-\nMedianandTrimmedMean(TM),exhibitedrelativelylow\ntimecomplexity,being4.56Ã—and2.94Ã—fasterthanDeTrigger, ingvalidationsamplequantitiesforgradientcomputation.\nFigure 10 shows the global model and backdoor attack\nrespectively.Ontheotherhand,methodssuchasKrum,Mul-\naccuracydroprates,andtheL1normbetweentheground\ntiKrum,andFLTrust,whichcomputethesimilaritybetween\ntruthandDeTrigger-predictedtriggerpatternsforvalidation\nmodelupdates,showedslightlyhighercomputationalcosts\nsamplesizesrangingfrom1to1000.Whenthesizeofthe\nduetotheadditionalsimilaritycalculationsrequiredintheir\nvalidationsetisextremelylow(e.g.,1),theL1normexhib-\nschemes.Whiletheirlatencyisatapracticallyacceptable\nited higher error, and the backdoor defense performance\nlevel(someevenfasterthanDeTrigger)weshowinthefol-\ndegraded(âˆ¼60%blockrate).However,withjust10validation\nlowingevaluationsthatthiscomesatthepriceoffailingto\nsamples(randomlyselectedfromasetof1,000),DeTrigger\nmitigatethebackdoorattackinmanycasesproperly.\nachievedsignificantlyimprovedtriggerpredictionquality\nFurthermore,Figure9indicatesthattheoptimizationand\nandanoticeableincreaseintheattackblockrate.\ntrainingprocessesinTABORandNeuralCleanse,whichaim\ntodetectbackdoormodelsandextracttriggers(similarto\n6.4 ImpactofBackdoorKnowledgePruning\nDeTrigger),showsignificantlyhighercomputationallatency,\nbeing182.57Ã—,and251.46Ã—higherthanDeTrigger,respec- Toevaluatetheeffectivenessofbackdoorknowledgepruning,\ntively.Giventhatthesystemwillencountersuchlatencyat wevisualizedthelearnedrepresentationsofbothbenignand\neveryfederatedlearninground,andalsothatthislatency backdoormodels(GTSRBdatasetandCNNconfiguration)\nwillincreasewithanincreasingnumberofclients,weseethe usingt-SNE[46].Ast-SNEmapsdatatoarbitraryspaces;\n10\nemiT\nevitaleR\n)%(\nporD\nycaruccA\nmroN\n1L\nConferenceâ€™17,July2017,Washington,DC,USA\nBackdoor Accuracy Drop Global Model Accuracy Drop\n0.15 100\n0.10 80\n0.05 60\n40\n0.00\n20\nCI CFA NR N10 CIF CA NR N100 RC eI sF NAR et1 10 8 ReS sT NL e1 t0 18 G Tr ro uu tn hd CI CFA NR N10 CIF CA NR N100 RC eI sF NAR et1 10 8 ReS sT NL e1 t0 18 0 CI CFA NR N10 CIF CA NR N100 RC eI sF NAR et1 10 8 ReS sT NL e1 t0 18\n(a) (b) (c)\nFigure12:(a)L1-normwithrespecttoinputsizefordifferentdataset/modelconfigurations.(b)Sampletriggers\nextractedfordifferentdataset/modelconfigurations.(c)Accuracydropofbackdoorandglobalmodelaccuracyfor\ndifferentdataset/modelconfigurations.\nthus,absolutelocationsordistancesarenotdirectlycompa-\nrableacrossfigures.Forclarity,weonlyincludethetopfive\nlabels,includingtheattacktargetlabel(from43classes).\nFigures11(a)and(b)illustratethelearnedrepresentations\nprior to pruning for backdoor and benign models, respec-\ntively.Asshown,thebackdoorsamples(reddots)inFigure11\n(a)areclusteredseparatelyfromthenormalsamplesdueto Figure13:Extractedsampletriggerswithvaryingcolor,\ntheattackconstructinganindependentdecisionboundary location,andshapeofthebackdoortrigger.\n(forintentionalmisclassification),whereasinFigure11(b),\nthebenignmodelmapsthebackdoorsampleswiththeirorig-\n18.0\ninallabelsratherthanthetargetattacklabel.Thisindicates\nthatabenignmodelisnotaffectedbytheattacktrigger. 13.5\nWepresenttherepresentationsofbackdoor-affectedand 9.0\nbenignmodelsafter thepruningprocessusingtheinferred 4.5\n6.60 9.81 7.76 10.04 7.09 14.58 13.09 12.38\ntriggerinFigures11(c)and(d),respectively.Figure11(c) 0.0\nRed Green Blue Top Bottom Bars Bars X\nshowsthatpruningdisruptstheredclusterassociatedwith Left Right Left Right Center\nFigure14:L1-normwithvaryingcolor,location,and\nthebackdoor,redistributingthesesamplestowardthenormal\nshapeofthebackdoortrigger.\ndatafeaturespace.Notethatinsomecases,DeTrigger may\noccasionally misclassify a benign model as compromised\nand apply pruning. However, Figure 11 (d) demonstrates\nthatsuchunintendedpruninghasminimalimpactonbenign forSTL10-ResNet18,acleartriggerpatternislessevident,\nmodel behavior, as unrelated knowledge (e.g., labels 0-4) suggestingpossibleperformancedegradationinscenarios\nremainsintactpost-pruning. involving high-resolution data and complex models. Nev-\nertheless, despite imperfect trigger extraction from a hu-\n6.5 PerformanceAcrossVaryingDataset manperceptionperspective,DeTrigger effectivelyremoves\nandModelCharacteristics trigger-relatedinformationduringitspruningphasewhile\nretainingbenignmodelknowledge.Specifically,Figure12\nNext,weevaluateDeTriggerâ€™sperformanceacrossdiverse\n(c) shows the accuracy drop rate for the backdoor attack\ndataset-modelconfigurations,focusingon(i)labeltypes,(ii)\nandglobalmodelbeforeandafterpruning.Wecannotice\ninputresolutions,and(iii)modelcharacteristics.\nthattheglobalmodelaccuracyremainslargelyunaffected,\nFigure12(a)presentstheL1normbetweentheground\nwhilebackdooraccuracyisreducedbyupto98.90%inthe\ntruthtriggerpatternandtheinferredtriggeracrossvarious\nCIFAR100-CNNcase.ThisalignswithfindingsbyWanget\nconfigurationswith75benignclientsand25backdoorat-\nal.[47],whonotedthatbackdoorscanstillexploitimperfect\ntackerstotalof100participants.Toensurefaircomparisons\npatterns.Overall,theseresultsdemonstratethatDeTrigger\nacrossresolutions,theL1normisnormalizedbydividingit\nisflexibleenoughtosupportavarietyofdatasetandmodel\nbythespatialdimension(ğ» Ã—ğ‘Š),asinputdatadimensions\nconfigurationswithinfederatedlearningoperations.\naffectthenorm.Theresultshereshowstableinferredtrigger\nqualityacrossconfigurations,withaminorexceptioninthe\n6.6 ScalabilityofDeTrigger\nSTL10-ResNet18 setting, which combines high-resolution\ninputwitharelativelycomplexmodel. Finally,weevaluateDeTriggerâ€™sscalabilityondifferentback-\nNext,Figure12(b)visualizesthereverse-engineeredtrig- doortriggerpatternsandtheincreasingnumberofpartici-\ngers for different dataset-model configurations. Note that patingclientsinthefederatedlearningnetwork.\n11\n)WÃ—H(/mroN\n1L\ndnuorG\nreggirTeD\nmroN\n1L\nhturT\ndetcartxE\n)%(\nporD\nycaruccA\nConferenceâ€™17,July2017,Washington,DC,USA KichangLee,YujinShin,JonghyukYun,JunHan,andJeongGilKo\nBackdoor Accuracy Drop Global Model Accuracy Drop withincreasingclients.However,thelatencyforDeTrigger\n100 issignificantlylowercomparedtopreviouslyproposedal-\n80\nternativessuchasTABORandNeuralCleanse.Thisresults\n60\n40 suggeststhatDeTrigger isanefficientandscalablesolution\n20 foraddressingbackdoorattacksinFLsystems.\n0\nRed Green Blue Top Bottom Bars Bars X\nLeft Right Left Right Center\nFigure 15: Accuracy drop for the backdoor task and\n7 DISCUSSION\nmaintaskfordifferentcolors,locations,andshapesof\nBasedonourexperiencesindesigningandevaluatingDe-\nthebackdoortrigger.\nTrigger,wediscussthelimitationsofourcurrentresearch\nandsuggestdirectionsforfuturework.\nDeTrigger NeuralCleanse TABOR â€¢Understandingbackdoorattackviagradients.Inthis\n103\npaper,weexploredtherelationshipbetweenbackdoorand\nadversarialattacksusinggradientanalysis.Wealsointro-\n101 ducedthetemperaturescalingtrick,offeringanovelperspec-\ntiveonthedecisionboundaryofbackdoormodels.These\n1 5 10 25 50 100\nanalysesprovidevaluableinsightsthatenhancetheunder-\nNumber of Clients\nstandingofbackdoorattacks.Wehopetheinsightspresented\nFigure16:Normalizedlatencyfordifferentbackdoor\nherewillserveasafoundationforfurtherresearchandin-\ndetectionschemeswithvaryingnumberofclients.\nspirenewdiscussionsonbackdoorvulnerabilitiesinneural\nnetworks.\n6.6.1 DifferentBackdoorTriggerPatterns. Triggerpatterns â€¢ Exploiting advanced adversarial attacks. Our work\nforbackdoorattackscanvarywidely.Whilesmallpatterns primarilyseekstoemphasizeandvalidatethefeasibilityof\nappendedtotheoriginalinputarecommoncharacteristics, leveragingadversarialattackconceptstomitigatebackdoor\ntheirvisualcharacteristics,suchasshapeandcolor,candiffer attacksinfederatedlearning.Accordingly,DeTrigger utilizes\nsignificantly.Toevaluatethescalabilityandrobustnessof astraightforwardgradient-basedadversarialattack.Weac-\nDeTrigger againstdiversebackdoortriggers,weconducted knowledgethatpriorstudieshaveproposedmoreadvanced\nexperimentsusingeightdifferenttriggertypes,visualized adversarialattacktechniques.Webelievethatincorporating\nat the top of Figure 13 leveraging the 2-layered CIFAR10 theseapproachescouldfurtherenhancetheeffectivenessof\ndataset and CNN model. As shown in the bottom of the ourdefensemechanism.\nfigure, DeTrigger successfully extracts the triggers across â€¢AdaptiveattackagainstDeTrigger.Tofurtherstrengthen\nvariousshapesandcolors,albeitnotperfectly. thefederatedlearningframework,itisessentialtoexploreits\nTheL1normbetweenthegroundtruthandinferredtrig- limitationsandconductstresstestingofthedefensemech-\ngerpatterns,showninFigure14,indicatesthatnon-continuous anism. While our work demonstrates the effectiveness of\npixelpatterns(e.g.,thefinalthreetriggersinFigure13)tend leveragingadversarialattackstomitigatebackdoorattacks,\ntoshowincreasedquantitativedetectionerror.However,at we also consider potential adaptive attacks that could be\nasystemlevel,Figure15demonstratesthatevenimperfect designedtoevadeDeTrigger.Forexample,anadaptiveattack\ntriggerextractions,whenintegratedintothefullDeTrigger mightemploytriggerswithhightotalvariationtobypassde-\npipeline,effectivelymaintainahighlyaccurateglobalmodel tection,asDeTrigger usestotalvariationmetricstoidentify\nwhilesignificantlysuppressingbackdoorattacks. backdoorupdates.Importantly,theglobalmodelaccuracy\ndidnotsignificantlydegradefromthepruningprocess,in-\n6.6.2 NumberofParticipatingClients. Wenowexaminehow\ndicatingthatDeTrigger canadapttocountertheseattacks\nDeTrigger scaleswiththeincreasingnumberofclientsinits\nbyexpandingitsdetectioncriteriatoaccountforabroader\nfederatedlearningnetwork.Here,wefocusonthecompu-\nrangeofpotentialtriggers.Furthermore,adjustingthetotal\ntationaloverheadofdealingwiththeincreasednumberof\nvariationğ‘‡ğ‘‰ thresholdwouldallowDeTrigger tomaintain\nupdatedmodelsthatarecollectedattheserver.Thisispartic-\nitseffectivenessagainstadaptivethreats.\nularlyimportantgiventhatlonglatenciesleadtoincreased\nintervalsbetweenfederatedlearningrounds,whichinturn\ntranslatestoprolongedmodelconvergence.\n8 CONCLUSION\nOurresultswiththeCIFAR10datasetand2-layeredCNN\nmodelplottedinFigure16suggestthat,asexpected,theover- Inthispaper,weintroducedDeTrigger,abackdoor-robustfed-\nall computation time for DeTriggerâ€™s operations increases eratedlearningframeworkdesignedtodetectandmitigate\n12\n)%(\nporD\nycaruccA\nemiT\nevitaleR\nConferenceâ€™17,July2017,Washington,DC,USA\nbackdoorattacksbyleveragingadversarialattackmethodolo- LearningFrameworkwithSmallLabeledData.InProceedingsofthe\ngies.Throughgradientanalysisandtemperaturescaling,De- 21stACMConferenceonEmbeddedNetworkedSensorSystems.56â€“69.\nTrigger effectivelyisolatestriggerpatterns,enablingmodel [11] YaoDeng,XiZheng,TianyiZhang,ChenChen,GuannanLou,and\nMiryungKim.2020.Ananalysisofadversarialattacksanddefenses\nweightpruningfortheremovalofbackdooractivationswhile\nonautonomousdrivingmodels.In2020IEEEinternationalconference\nretainingbenignknowledgewithintheglobalmodel.Our\nonpervasivecomputingandcommunications(PerCom).IEEE,1â€“10.\nextensiveevaluationsdemonstratethatDeTrigger notonly [12] FatimaElhattab,SaraBouchenak,RaniaTalbi,andVladNitu.2023.Ro-\nachieves significant speed improvements over traditional bustfederatedlearningforubiquitouscomputingthroughmitigation\nbackdoordefensesbutalsopreservesmodelaccuracyand ofedge-casebackdoorattacks.ProceedingsoftheACMonInteractive,\nMobile,WearableandUbiquitousTechnologies6,4(2023),1â€“27.\nmitigatesattackeffectivenessbyupto98.9%.Additionally,\n[13] MinghongFang,XiaoyuCao,JinyuanJia,andNeilGong.2020.Local\nthroughextensiveevaluationsusingfourwidely-usedpublic modelpoisoningattacksto{Byzantine-Robust}federatedlearning.In\ndatasets,weexploredthescalabilityofDeTrigger acrossdi- 29thUSENIXsecuritysymposium(USENIXSecurity20).1605â€“1622.\nversesettings,confirmingitsadaptabilitytovaryingmodel [14] YinghuaGao,DongxianWu,JingfengZhang,GuanhaoGan,Shu-Tao\ncomplexities,labelsizes,anddataresolutions.Bycombining Xia,GangNiu,andMasashiSugiyama.2023.Ontheeffectivenessof\nadversarialtrainingagainstbackdoorattacks. IEEETransactionson\nefficiency with precision, DeTrigger sets a foundation for\nNeuralNetworksandLearningSystems(2023).\nsecureandscalablefederatedlearning.\n[15] XueluanGong,YanjiaoChen,QianWang,andWeihanKong.2022.\nBackdoorattacksanddefensesinfederatedlearning:State-of-the-art,\nACKNOWLEDGMENTS taxonomy,andfuturedirections.IEEEWirelessCommunications30,2\n(2022),114â€“121.\nWe acknowledge Sungmin Lee, Ph.D. candidate at Yonsei\n[16] IanJGoodfellow.2014.Explainingandharnessingadversarialexam-\nUniversity,fortheconstructivediscussionaboutthework.\nples.arXivpreprintarXiv:1412.6572(2014).\n[17] TianyuGu,BrendanDolan-Gavitt,andSiddharthGarg.2017.Badnets:\nREFERENCES Identifyingvulnerabilitiesinthemachinelearningmodelsupplychain.\narXivpreprintarXiv:1708.06733(2017).\n[1] JungmoAhn,JaeYeonPark,SungSikLee,Kyu-HyukLee,Heesung\n[18] WenboGuo,LunWang,XinyuXing,MinDu,andDawnSong.2019.\nDo,andJeongGilKo.2023.SafeFac:Video-basedsmartsafetymoni-\nTabor:Ahighlyaccurateapproachtoinspectingandrestoringtrojan\ntoringforpreventingindustrialworkaccidents.ExpertSystemswith\nbackdoorsinaisystems.arXivpreprintarXiv:1908.01763(2019).\nApplications215(2023),119397.\n[19] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016.Deep\n[2] EugeneBagdasaryan,AndreasVeit,YiqingHua,DeborahEstrin,and\nresiduallearningforimagerecognition.InProceedingsoftheIEEE\nVitalyShmatikov.2020. Howtobackdoorfederatedlearning.InIn-\nconferenceoncomputervisionandpatternrecognition.770â€“778.\nternationalconferenceonartificialintelligenceandstatistics.PMLR,\n[20] GeoffreyHinton,OriolVinyals,andJeffDean.2015. Distillingthe\n2938â€“2948.\nknowledgeinaneuralnetwork.arXivpreprintarXiv:1503.02531(2015).\n[3] Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier\n[21] SebastianHouben,JohannesStallkamp,JanSalmen,MarcSchlipsing,\nFernandez-Marques,YanGao,LorenzoSani,KwingHeiLi,TitouanPar-\nandChristianIgel.2013.DetectionofTrafficSignsinReal-WorldIm-\ncollet,PedroPortoBuarquedeGusmÃ£o,etal.2020.Flower:Afriendly\nages:TheGermanTrafficSignDetectionBenchmark.InInternational\nfederatedlearningresearchframework.arXivpreprintarXiv:2007.14390\nJointConferenceonNeuralNetworks.\n(2020).\n[22] Tzu-MingHarryHsu,HangQi,andMatthewBrown.2019.Measur-\n[4] PevaBlanchard,ElMahdiElMhamdi,RachidGuerraoui,andJulien\ningtheeffectsofnon-identicaldatadistributionforfederatedvisual\nStainer.2017.Machinelearningwithadversaries:Byzantinetolerant\nclassification.arXivpreprintarXiv:1909.06335(2019).\ngradientdescent.Advancesinneuralinformationprocessingsystems\n[23] KaidiJin,TianweiZhang,ChaoShen,YufeiChen,MingFan,Chenhao\n30(2017).\nLin,andTingLiu.2022.Canwemitigatebackdoorattackusingadver-\n[5] XiaoyuCao,MinghongFang,JiaLiu,andNeilZhenqiangGong.2020.\nsarialdetectionmethods?IEEETransactionsonDependableandSecure\nFltrust:Byzantine-robustfederatedlearningviatrustbootstrapping.\nComputing20,4(2022),2867â€“2881.\narXivpreprintarXiv:2012.13995(2020).\n[24] DiederikPKingmaandJimmyBa.2014.Adam:Amethodforstochastic\n[6] XiaoyuCaoandNeilZhenqiangGong.2022.Mpaf:Modelpoisoning\noptimization.arXivpreprintarXiv:1412.6980(2014).\nattackstofederatedlearningbasedonfakeclients.InProceedingsof\n[25] AlexKrizhevsky,GeoffreyHinton,etal.2009.Learningmultiplelayers\ntheIEEE/CVFConferenceonComputerVisionandPatternRecognition.\noffeaturesfromtinyimages.(2009).\n3396â€“3404.\n[26] KichangLee,SongkukKim,andJeongGilKo.2024.FLex&Chill:Im-\n[7] NicholasCarliniandDavidWagner.2017. Towardsevaluatingthe\nprovingLocalFederatedLearningTrainingwithLogitChilling.arXiv\nrobustnessofneuralnetworks.In2017ieeesymposiumonsecurityand\npreprintarXiv:2401.09986(2024).\nprivacy(sp).Ieee,39â€“57.\n[27] AngLi,JingweiSun,XiaoZeng,MiZhang,HaiLi,andYiranChen.2021.\n[8] AdamCoates,AndrewNg,andHonglakLee.2011. Ananalysisof\nFedmask:Jointcomputationandcommunication-efficientpersonalized\nsingle-layernetworksinunsupervisedfeaturelearning.InProceedings\nfederatedlearningviaheterogeneousmasking.InProceedingsofthe\nofthefourteenthinternationalconferenceonartificialintelligenceand\n19thACMConferenceonEmbeddedNetworkedSensorSystems.42â€“55.\nstatistics.JMLRWorkshopandConferenceProceedings,215â€“223.\n[28] YuezunLi,YimingLi,BaoyuanWu,LongkangLi,RanHe,andSiwei\n[9] LiDeng.2012. Themnistdatabaseofhandwrittendigitimagesfor\nLyu.2021.Invisiblebackdoorattackwithsample-specifictriggers.In\nmachinelearningresearch. IEEESignalProcessingMagazine29,6\nProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision.\n(2012),141â€“142.\n16463â€“16472.\n[10] YonghengDeng,ShengYue,TuoweiWang,GuanboWang,JuRen,and\n[29] YijingLi,XiaofengTao,XuefeiZhang,JunjieLiu,andJinXu.2021.\nYaoxueZhang.2023.FedINC:AnExemplar-FreeContinualFederated\nPrivacy-preservedfederatedlearningforautonomousdriving.IEEE\n13\nConferenceâ€™17,July2017,Washington,DC,USA KichangLee,YujinShin,JonghyukYun,JunHan,andJeongGilKo\nTransactionsonIntelligentTransportationSystems23,7(2021),8423â€“ DecisionBoundary.arXivpreprintarXiv:2402.17465(2024).\n8434. [42] QiSun,ArjunAshokRao,XufengYao,BeiYu,andShiyanHu.2020.\n[30] Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, and Shu-Tao Counteractingadversarialattacksinautonomousdriving.InProceed-\nXia.2021. Backdoorattackinthephysicalworld. arXivpreprint ingsofthe39thInternationalConferenceonComputer-AidedDesign.\narXiv:2104.02361(2021). 1â€“7.\n[31] BrendanMcMahan,EiderMoore,DanielRamage,SethHampson,and [43] ZitengSun,PeterKairouz,AnandaTheerthaSuresh,andHBrendan\nBlaiseAguerayArcas.2017. Communication-efficientlearningof McMahan.2019.Canyoureallybackdoorfederatedlearning?arXiv\ndeepnetworksfromdecentralizeddata.InArtificialintelligenceand preprintarXiv:1911.07963(2019).\nstatistics.PMLR,1273â€“1282. [44] ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,\n[32] XiaominOuyang,ZhiyuanXie,HemingFu,SitongCheng,LiPan, DumitruErhan,IanGoodfellow,andRobFergus.2013. Intriguing\nNeiwenLing,GuoliangXing,JiayuZhou,andJianweiHuang.2023. propertiesofneuralnetworks.arXivpreprintarXiv:1312.6199(2013).\nHarmony:HeterogeneousMulti-ModalFederatedLearningthrough [45] CanhTDinh,NguyenTran,andJoshNguyen.2020.Personalizedfed-\nDisentangledModelTraining.InProceedingsofthe21stAnnualInterna- eratedlearningwithmoreauenvelopes.Advancesinneuralinformation\ntionalConferenceonMobileSystems,ApplicationsandServices(Helsinki, processingsystems33(2020),21394â€“21405.\nFinland)(MobiSysâ€™23).AssociationforComputingMachinery,New [46] LaurensVanderMaatenandGeoffreyHinton.2008.Visualizingdata\nYork,NY,USA,530â€“543. https://doi.org/10.1145/3581791.3596844 usingt-SNE.Journalofmachinelearningresearch9,11(2008).\n[33] XiaominOuyang,ZhiyuanXie,JiayuZhou,GuoliangXing,andJianwei [47] BolunWang,YuanshunYao,ShawnShan,HuiyingLi,BimalViswanath,\nHuang.2022.Clusterfl:Aclustering-basedfederatedlearningsystem HaitaoZheng,andBenYZhao.2019. Neuralcleanse:Identifying\nforhumanactivityrecognition.ACMTransactionsonSensorNetworks andmitigatingbackdoorattacksinneuralnetworks.In2019IEEE\n19,1(2022),1â€“32. symposiumonsecurityandprivacy(SP).IEEE,707â€“723.\n[34] JaeyeonPark,HyeonCho,RajeshKrishnaBalan,andJeongGilKo. [48] ShaokuiWei,MingdaZhang,HongyuanZha,andBaoyuanWu.2023.\n2020.HeartQuake:AccurateLow-CostNon-InvasiveECGMonitoring Sharedadversarialunlearning:Backdoormitigationbyunlearning\nUsingBed-MountedGeophones.Proc.ACMInteract.Mob.Wearable sharedadversarialexamples.AdvancesinNeuralInformationProcessing\nUbiquitousTechnol.4,3,Article93(sep2020),28pages. Systems36(2023),25876â€“25909.\n[35] JaeYeonParkandJeongGilKo.2024. FedHM:Practicalfederated [49] Cheng-HsinWeng,Yan-TingLee,andShan-HungBrandonWu.2020.\nlearningforheterogeneousmodeldeployments. ICTExpress10,2 Onthetrade-offbetweenadversarialandbackdoorrobustness. Ad-\n(2024),387â€“392. vances in Neural Information Processing Systems 33 (2020), 11973â€“\n[36] JaeYeonPark,KichangLee,SungminLee,MiZhang,andJeongGilKo. 11983.\n2023.AttFL:APersonalizedFederatedLearningFrameworkforTime- [50] EmilyWenger,JosephinePassananti,ArjunNitinBhagoji,Yuanshun\nseriesMobileandEmbeddedSensorDataProcessing.Proceedingsof Yao,HaitaoZheng,andBenYZhao.2021.Backdoorattacksagainst\ntheACMonInteractive,Mobile,WearableandUbiquitousTechnologies deeplearningsystemsinthephysicalworld.InProceedingsofthe\n7,3(2023),1â€“31. IEEE/CVFconferenceoncomputervisionandpatternrecognition.6206â€“\n[37] KrishnaPillutla,ShamMKakade,andZaidHarchaoui.2022.Robustag- 6215.\ngregationforfederatedlearning.IEEETransactionsonSignalProcessing [51] ChulinXie,KeliHuang,Pin-YuChen,andBoLi.2019.Dba:Distributed\n70(2022),1142â€“1154. backdoorattacksagainstfederatedlearning.InInternationalconference\n[38] ShivaRajPokhrelandJinhoChoi.2020. Adecentralizedfederated onlearningrepresentations.\nlearningapproachforconnectedautonomousvehicles.In2020IEEE [52] DezhongYao,WanningPan,MichaelJOâ€™Neill,YutongDai,YaoWan,\nWirelessCommunicationsandNetworkingConferenceWorkshops(WC- HaiJin,andLichaoSun.2021. Fedhm:Efficientfederatedlearning\nNCW).IEEE,1â€“6. forheterogeneousmodelsvialow-rankfactorization.arXivpreprint\n[39] LemingShen,QiangYang,KaiyanCui,YuanqingZheng,Xiao-Yong arXiv:2111.14655(2021).\nWei,JianweiLiu,andJinsongHan.2024. FedConv:ALearning-on- [53] DongYin,YudongChen,RamchandranKannan,andPeterBartlett.\nModelParadigmforHeterogeneousFederatedClients.InProceedings 2018. Byzantine-robustdistributedlearning:Towardsoptimalsta-\nofthe22ndAnnualInternationalConferenceonMobileSystems,Appli- tisticalrates.InInternationalconferenceonmachinelearning.Pmlr,\ncationsandServices.398â€“411. 5650â€“5659.\n[40] YujinShin,KichangLee,SungminLee,YouRimChoi,Hyung-SinKim, [54] JonghyukYun,KyoosikLee,KichangLee,BangjieSun,JaehoJeon,\nandJeongGilKo.2024.EffectiveHeterogeneousFederatedLearning JeonggilKo,InseokHwang,andJunHan.2024.PowDew:Detecting\nviaEfficientHypernetwork-basedWeightGeneration.InProceedings CounterfeitPowderedFoodProductsusingaCommoditySmartphone.\nofthe22ndACMConferenceonEmbeddedNetworkedSensorSystems. InProceedingsofthe22ndAnnualInternationalConferenceonMobile\n112â€“125. Systems,ApplicationsandServices.210â€“222.\n[41] YanghaoSu,JieZhang,TingXu,TianweiZhang,WeimingZhang, [55] BoZhao,PengSun,TaoWang,andKeyuJiang.2022.Fedinv:Byzantine-\nandNenghaiYu.2024.ModelX-ray:DetectBackdooredModelsvia robustfederatedlearningbyinversinglocalmodelupdates.InProceed-\ningsoftheAAAIConferenceonArtificialIntelligence,Vol.36.9171â€“9179.\n14",
    "pdf_filename": "DeTrigger_A_Gradient-Centric_Approach_to_Backdoor_Attack_Mitigation_in_Federated_Learning.pdf"
}