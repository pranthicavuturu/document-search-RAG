{
    "title": "DeTrigger: A Gradient-Centric Approach to Backdoor",
    "abstract": "Local Training Global Aggregation FederatedLearning(FL)enablescollaborativemodeltrain- Benign Clients ing across distributed devices while preserving local data Label Local Model 50 Update Global Model privacy,makingitidealformobileandembeddedsystems. Label However,thedecentralizednatureofFLalsoopensvulner- 30 abilitiestomodelpoisoningattacks,particularlybackdoor Attacker Clients attacks,whereadversariesimplanttriggerpatternstomanip- Label Local Model 50 Update ulatemodelpredictions.Inthispaper,weproposeDeTrigger, Malicious Trigger ascalableandefficientbackdoor-robustfederatedlearning Label 30 frameworkthatleveragesinsightsfromadversarialattack Local Inference Malicious methodologies.Byemployinggradientanalysiswithtem- Sample Inference Benign Inference Result perature scaling, DeTrigger detects and isolates backdoor Sample Result STOP 50 triggers,allowingforprecisemodelweightpruningofback- 50 door activations without sacrificing benign model knowl- Malicious Trigger edge.Extensiveevaluationsacrossfourwidelyuseddatasets (a) demonstratethatDeTrigger achievesupto251×fasterde- Trigger Backdoor Model Method Cost Extraction Mitigation Pruning tection than traditional methods and mitigates backdoor attacksbyupto98.9%,withminimalimpactonglobalmodel FedAvg [31] accuracy.OurfindingsestablishDeTrigger asarobustand Statistical Prior [53] scalablesolutiontoprotectfederatedlearningenvironments Similarity-based [4] (Krum, MultiKrum) againstsophisticatedbackdoorthreats. Similarity-based [5] (FLTrust) Backdoor Detection (TABOR[18], NeuralCleanse[47]) Ours 1 INTRODUCTION (b) FederatedLearning(FL)isadecentralizedmachinelearning Figure1:(a)Illustrationofbackdoorattackinfeder- approachthattrainsaglobalmodelbyaggregatinglocally atedlearningscenarioforlocaltraining,server-side trainedmodelsfrommobileandembeddeddevices[31,40]. globalaggregation,andlocalinferenceoperations.(b) Thismethodleveragesdistributeddataandcomputational Comparisonofourworkwithpreviouslyproposedap- resources,reducingthedependencyoncentralizedprocess- proachesinaddressingbackdoorattacks. ing[35,36,52].Federatedlearningpowersmobileapplica- tions,suchassensordataanalysis[33,34,39],autonomous vehicle[29,38],andreal-timecomputervision[1,10,32,54], byusinglarge,diversedatasetswithoutdatasharing.Akey Withadvancementsinfederatedlearning,modelpoison- principleispreservinglocaldataprivacy,astheserverag- ingattackshavegrownmoresophisticated,withtheBack- gregates updates without accessing raw data [27, 31, 45]. doorAttack posingaseverethreat[28,30,43,51].Inthis However,thisalsomeanstheservercannotverifyupdates, attack,asillustratedinFigure1(a),anadversarytrainsalocal makingfederatedlearningvulnerabletomodel-poisoning modeltobehavebenignlyonstandardinputsbutmisclassi- attacksfrommaliciousclients[2,6,13]. fiesinputswithaspecifictrigger.Thiscompromisedmodel 1 4202 voN 91 ]GL.sc[ 1v02221.1142:viXra",
    "body": "DeTrigger: A Gradient-Centric Approach to Backdoor\nAttack Mitigation in Federated Learning\nKichang Lee Yujin Shin Jonghyuk Yun\nkichang.lee@yonsei.ac.kr yujin_shin@yonsei.ac.kr jonghyuk.yun@kaist.ac.kr\nYonseiUniversity YonseiUniversity KAIST\nJun Han JeongGil Ko\njunhan@cyphy.kaist.ac.kr jeonggil.ko@yonsei.ac.kr\nKAIST YonseiUniversity,POSTECH\nABSTRACT\nLocal Training Global Aggregation\nFederatedLearning(FL)enablescollaborativemodeltrain- Benign Clients\ning across distributed devices while preserving local data Label Local Model\n50 Update Global Model\nprivacy,makingitidealformobileandembeddedsystems.\nLabel\nHowever,thedecentralizednatureofFLalsoopensvulner- 30\nabilitiestomodelpoisoningattacks,particularlybackdoor Attacker Clients\nattacks,whereadversariesimplanttriggerpatternstomanip- Label Local Model\n50 Update\nulatemodelpredictions.Inthispaper,weproposeDeTrigger, Malicious Trigger\nascalableandefficientbackdoor-robustfederatedlearning Label\n30\nframeworkthatleveragesinsightsfromadversarialattack\nLocal Inference Malicious\nmethodologies.Byemployinggradientanalysiswithtem- Sample Inference\nBenign Inference Result\nperature scaling, DeTrigger detects and isolates backdoor Sample Result\nSTOP 50\ntriggers,allowingforprecisemodelweightpruningofback- 50\ndoor activations without sacrificing benign model knowl- Malicious Trigger\nedge.Extensiveevaluationsacrossfourwidelyuseddatasets (a)\ndemonstratethatDeTrigger achievesupto251×fasterde- Trigger Backdoor Model\nMethod Cost Extraction Mitigation Pruning\ntection than traditional methods and mitigates backdoor\nattacksbyupto98.9%,withminimalimpactonglobalmodel FedAvg [31]\naccuracy.OurfindingsestablishDeTrigger asarobustand Statistical Prior [53]\nscalablesolutiontoprotectfederatedlearningenvironments Similarity-based [4]\n(Krum, MultiKrum)\nagainstsophisticatedbackdoorthreats.\nSimilarity-based [5]\n(FLTrust)\nBackdoor Detection\n(TABOR[18], NeuralCleanse[47])\nOurs\n1 INTRODUCTION\n(b)\nFederatedLearning(FL)isadecentralizedmachinelearning\nFigure1:(a)Illustrationofbackdoorattackinfeder-\napproachthattrainsaglobalmodelbyaggregatinglocally\natedlearningscenarioforlocaltraining,server-side\ntrainedmodelsfrommobileandembeddeddevices[31,40].\nglobalaggregation,andlocalinferenceoperations.(b)\nThismethodleveragesdistributeddataandcomputational\nComparisonofourworkwithpreviouslyproposedap-\nresources,reducingthedependencyoncentralizedprocess-\nproachesinaddressingbackdoorattacks.\ning[35,36,52].Federatedlearningpowersmobileapplica-\ntions,suchassensordataanalysis[33,34,39],autonomous\nvehicle[29,38],andreal-timecomputervision[1,10,32,54],\nbyusinglarge,diversedatasetswithoutdatasharing.Akey Withadvancementsinfederatedlearning,modelpoison-\nprincipleispreservinglocaldataprivacy,astheserverag- ingattackshavegrownmoresophisticated,withtheBack-\ngregates updates without accessing raw data [27, 31, 45]. doorAttack posingaseverethreat[28,30,43,51].Inthis\nHowever,thisalsomeanstheservercannotverifyupdates, attack,asillustratedinFigure1(a),anadversarytrainsalocal\nmakingfederatedlearningvulnerabletomodel-poisoning modeltobehavebenignlyonstandardinputsbutmisclassi-\nattacksfrommaliciousclients[2,6,13]. fiesinputswithaspecifictrigger.Thiscompromisedmodel\n1\n4202\nvoN\n91\n]GL.sc[\n1v02221.1142:viXra\nConference’17,July2017,Washington,DC,USA KichangLee,YujinShin,JonghyukYun,JunHan,andJeongGilKo\nisthensubmittedtothecentralserver,integratingthemali- DeTrigger leveragestheabilitytoidentifythetriggerinfor-\nciousknowledgeintotheglobalmodel.Consequently,the mation,enablingthepinpointingandremovalofonlythe\nglobalmodelmisclassifiesanyinputcontainingthetrigger, backdooractivationweightsembeddedwithintheneuralnet-\nallowingcovertmanipulationofsystemoutputs.Suchattacks work.ThisapproachallowsDeTrigger’sglobalmodeltore-\nareparticularlyconcerninginapplicationslikeautonomous tainthebenignknowledgelearnedfromcompromisedmod-\ndriving, where subtle, undetectable modifications to road els.Extensiveevaluationsacrossfourwidelyuseddatasets\nsignscouldleadtounsafedecisionsandaccidents[11,30,42]. demonstratethatDeTrigger establishesabackdoor-robust\nDefense mechanisms against backdoor attacks, such as federatedlearningframework,capableofmitigatingupto\nanomalydetection,modelfiltering,androbustaggregation, 98.9%ofbackdoorattacksandachievingdetectionspeeds\naim to neutralize malicious updates by exploiting the sta- uptoapproximately251×fasterthanexistingmethods.This\ntisticalpriorandbehavioralanomalies[3,4,37].However, combinationofspeedandaccuracymakesDeTrigger ascal-\nasFigure1(b)shows,thesemethodsoftenfailtomitigate ableandeffectivesolutionforsecuringfederatedlearning.\nbackdoorattacks,asbackdoorattacksbehavenormallyon Specifically,ourworkmakesthefollowingcontributions:\nlegitimatedataandactivateonlyonspecifictriggers[12,18]. • Wepresentanempiricalstudyexploringhowleveraging\nThismakesitdifficulttodistinguishmaliciousclientsfrom adversarialattackconceptscanaidinidentifyingtrigger\nbenign ones without the knowledge of the trigger. Addi- patternsresponsibleforbackdoorattacks.Ourpreliminary\ntionally, while some techniques have proven effective in findingshighlightthepotentialofmodelgradientanalysis\ncentralized settings, they often entail high computational foreffectivebackdoorattackdetectionandmitigationin\ncosts,limitingtheirscalabilityinfederatedlearningenviron- federatedlearningnetworks.\nments, where servers must assess numerous models each • WeproposeDeTrigger,abackdoor-robustfederatedlearn-\nround[18,47]. ingframeworkthatcombinescapabilitiesforbothdetect-\nWeaddresstheselimitationsbyproposingascalableand ingandmitigatingbackdoorattacks.Byleveraginggradi-\neffectivestrategyfordetectingandmitigatingbackdoorat- entanalysisandinsightsfromadversarialattackmethods,\ntacksinfederatedlearning,drawingoninsightsfromthere- DeTrigger efficientlyisolatestriggerpatterns,allowingtar-\nlationshipbetweenadversarialandbackdoorattacks.While getedremovalofmaliciousactivationswithoutsacrificing\nadversarialattacksidentifyperturbationpatternstoinduce benignmodelknowledge.Thisapproachensuresscalable,\nmisclassificationsatthemodel,backdoorattacksrelyona effectivedefenseagainstsophisticatedbackdoorthreatsin\nhiddentriggerpatternembeddedintheinputtoconsistently federatedlearningenvironments.\nmisclassifyinputstoatargetlabel.Thus,byemployingan • We conduct an extensive evaluation of DeTrigger using\nadversarial-typeapproachtodesigneffectivenoise,wecan fourwidelyusedpublicdatasetsandvariousmodelarchi-\nidentifythetriggerusedinabackdoorattack. tecturestodemonstrateitsscalabilityandeffectivenessin\nOurproposedmechanism,DeTrigger,leveragesgradient mitigatingbackdoorattacks.OurresultsshowthatDeTrig-\nanalysistechniquesforbackdoortriggerdetection.Byexam- ger achievesovera251×speedupcomparedtotraditional\niningmodelgradients,whichcapturehowmodelweights backdoorattackmitigationstrategieswhilepreservingthe\nrespondtovaryinginputs,wecandetectsubtledeviations accuracyoftheglobalmodelandsignificantlyreducing\nindicative of a backdoor trigger. However, this approach backdoorattackimpact.\npresentschallengesindistinguishingbackdoorattackcom-\nponents from inherent input-specific noise. DeTrigger ad- 2 BACKGROUNDANDRELATEDWORK\ndresses this issue by isolating trigger-dependent features\nThis section provides background on backdoor attacks in\nwithinitsgradientpreprocessingoperations.Thismethod\nfederatedlearningandanoverviewofadversarialattacks,\nisparticularlyscalableforfederatedlearning,asgradient-\nemphasizingtheirsimilarities,differences,andimplications\nbasedanalysisenablesefficientabnormalpatterndetection\nfordesigningabackdoor-robustframework.\nwithout requiring exhaustive inspection of client models\nonspecifictriggers.Furthermore,byfocusingongradient\n2.1 BackdoorAttacks\nbehavior,DeTrigger minimizestheneedforextensiveveri-\nficationdatasets,whichwouldotherwisebeimpracticalin Abackdoorattackmanipulatesamodel𝑓 𝑏𝑎𝑐𝑘(·)toproducea\nlarge-scaledistributedsystems. designatedincorrectoutput𝑌 𝑏𝑎𝑐𝑘 forinput𝑋 𝑡 withatrigger\nMoreover,DeTrigger goesbeyondsimplydetectingpoi- 𝑇 isprovidedwhilemaintainingnormalbehavior𝑌 onun-\nsoned models by also isolating the backdoor trigger from altereddata𝑋.Thiscanbeformulatedas𝑋 𝑡 = (1−𝑀)𝑋 +\ntheglobalmodel.Whilecompletelyremovingcompromised 𝑀𝑇,𝑌 = 𝑓 𝑏𝑎𝑐𝑘(𝑋),𝑌 𝑏𝑎𝑐𝑘 = 𝑓 𝑏𝑎𝑐𝑘(𝑋 𝑡) [23].Suchattacksare\nmodelscaneliminatebackdoorattacks,italsosacrificesthe typicallyexecutedbytrainingthemodelwithmanipulated\nbenignknowledgethesemodelscontribute.Toaddressthis, data that incorporates the trigger pattern. Gu et al. [17]\n2\nConference’17,July2017,Washington,DC,USA\ndemonstratedthevulnerabilityofdeepneuralnetworksto methods,suchastriggerpatterndetectionandupdatesim-\nbackdoorattacksbyintroducingpixelpattern-basedtriggers ilarityanalysis,attempttodirectlyidentifybackdoortrig-\nasamethodofembeddingbackdoorsinmodels. gersbyanalyzingmodelbehaviorsonsyntheticorauxiliary\nForexample,inanimage-basedtrafficsignrecognition data[12,18,47].However,theseapproachesalsofacechal-\nsystemforautonomousvehicles,anattackercouldintroduce lengesintermsofefficiencyandscalability,makingthedevel-\nasubtletrigger,suchasasmallsticker,linkedtoaspecific opmentofeffectiveandscalabledefensesagainstbackdoor\nattack label (e.g., a speed limit sign or a stop sign). Once attacksinfederatedlearninganongoingchallenge.\nthe model is trained with this poisoned data, it becomes\nsusceptibletobackdoorexploitation.Whendeployed,the\n2.2 AdversarialAttacks\nmodelmisinterpretsanysigncontainingthistriggerasthe\ndesignated label, potentially causing the vehicle to make Adversarialattacksrepresentasubstantialsecuritythreat\ndangerousdecisions,leadingtotrafficviolations,accidents, to the robustness of deep neural networks. These attacks\norlife-threateningsituations.Thisexamplehighlightsthe are designed to intentionally manipulate the model’s out-\nseriousrisksbackdoorattacksposeinsafety-criticalsystems putbyintroducingsmall,carefullycraftedperturbationsto\nlikeautonomousdriving[30,50]. the input data. Formally, this can be expressed as𝑋 𝑎𝑑𝑣 =\nBackdoorAttacksinFederatedLearning.Inthecontext 𝑋 +𝜖 ·𝑁,𝑌 =𝐹(𝑋) ≠𝐹(𝑋 𝑎𝑑𝑣).Inthisformulation,𝑋 𝑎𝑑𝑣 is\noffederatedlearning,amaliciousclientcanexecuteaback- theadversarialexample,createdbyaddingasmallperturba-\ndoor attack by uploading a tampered model to the server tion𝜖·𝑁 totheoriginalinput𝑋,where𝑁 istheperturbation\nforaggregation[2,15].Detectingsuchcompromisedclient\ndirectionand𝜖controlsitsmagnitude.Thegoalistoturnthe\nmodelspresentsasignificantchallengefortheserverand\nmodel’scorrectprediction(𝑌 =𝐹(𝑋))intoanincorrectone\ntheglobalmodel,asitlacksdirectaccesstotheclients’train- (𝑌 ≠𝐹(𝑋 𝑎𝑑𝑣))whilekeeping𝜖 minimal.Researchonadver-\ningdata,impedingtheverificationofmodelintegrity.Fur- sarialattacksfocusesonidentifyingeffectiveperturbations\nthermore,theinherentanonymityanddecentralizednature thatmisleadthemodelwithminimaldistortion.Forexample,\noffederatedlearning,wherenumerousclientsparticipate Szegedyetal.introducedadversarialexamplesviaL-BFGS\nwithoutdisclosingtheirdata,precludetheserverfromfully optimization[44],andGoodfellowetal.laterproposedthe\nensuringthetrustworthinessofeachclient.Thisuncertainty FastGradientSignMethod(FGSM),whichefficientlygener-\nexacerbatestheriskofbackdoorattacksandemphasizesthe atesadversarialexamplesbyusinggradientsigns[16].These\nnecessityforrobustdefensemechanismstosafeguardthe subtlechanges,oftenimperceptibletohumans,exposecriti-\nintegrityoftheglobalmodel.Notably,backdoorattacksonly calweaknessesinneuralnetworkrobustness[7].\nyieldincorrectoutputsforinputdatawithtriggers,making\nitchallengingfortheservertodetectacompromisedmodel\n2.3 RelationshipBetweenBackdoorand\neven with a verification process in place. The model will\nAdversarialAttacks\ncontinue to perform as expected on non-triggered inputs,\nallowingtheattacktoevadetraditionalvalidationchecks. Bothbackdoorandadversarialattackssharetheoverarch-\nToaddressthisissue,priorworkshaveproposedvarious inggoalofinducingincorrectorundesiredoutcomesinthe\ndefensestrategiestodetectandmitigatebackdoorattacks model. They achieve this by manipulating input data, ei-\nwithoutrequiringaccesstoclients’localdata.Onecommonly ther through malicious noise or trigger injection. From a\nexplored approach is Byzantine-robust aggregation meth- high-levelperspective,bothattacksaimtocompromisethe\nods[5,13,55],whichaimtoreducetheinfluenceofmalicious model’s reliability [49]. However, the primary distinction\nupdatesbylimitingtheimpactofoutliersduringtheaggre- lies in their mechanisms and targets. Adversarial attacks\ngationprocess.However,thesemethodsoftenexperience concentrateonidentifyingaspecificperturbation(𝑁)that\ndiminishedefficacy,astheycanstruggletodistinguishbe- forcesthemodel𝐹(·)tomisclassifyagiveninput.Thispro-\ntweenbenignmodelupdatesandthosesubtlyalteredbya cess requires the attacker to dynamically compute 𝑁 for\nbackdoor.Consequently,thischallengemayresultininsuffi- each individual input to achieve misclassification. In con-\ncientprotectionagainstsophisticatedbackdoorattacks. trast,backdoorattacksfocusonembeddingahiddentrigger\nAlternatively,anomalydetectiontechniquesfocusoniden- pattern(denoted𝑇)intothemodelduringtraining.Amodel\ntifyingabnormalpatternsinmodelupdates,suchasdevi- compromisedfromabackdoorattackbehavesasexpected\nations in weight distributions or gradients [4, 53]. While onbenigninputsbutproducesincorrectpredictionswhen\npotentiallymoreaccurate,approachesbasedonupdatepat- thetriggerpatternispresent.Thismakesbackdoorattacks\nternanalysistendtoincurhighcomputationalcostsandare particularlycovert,astheydonotrelyoncontinuousinput\nlessscalable,particularlywhendealingwithlargemodels manipulation;instead,apre-definedcondition—thepresence\norahighnumberofclients.Additionally,recentlyproposed ofthetrigger—activatesthemaliciousbehavior.\n3\nConference’17,July2017,Washington,DC,USA KichangLee,YujinShin,JonghyukYun,JunHan,andJeongGilKo\nAdversarial Attack Backdoor Attack\nBackdoor Backdoor\nPlane Plane\nTotal\nGradient Benign 0 Benign 1 Benign 2 Benign 3 Benign 4\nBackdoor 0 Backdoor 1 Backdoor 2 Backdoor 3 Backdoor 4\nNormal\nPlane\n(a) (b) (c)\nFigure2:(a)Illustrationofadversarialandbackdoorattacksrepresentedwithclassdecisionboundaries.(b)Normal\nandbackdoor-affectedgradientsforaninputsamplearepresentedinthenormaldataplaneandbackdoorplane.\n(c)Samplesusedinthepreliminarystudyshowvalidandbackdoorsampleswithdetectedtriggers.\nToaddressthesethreats,severaldefensemechanismshave backdoortrigger.Figure2(b)presentsahigh-levelillustra-\nbeenproposedforbackdoorattacks.Jinetal.developeda tionofthisapproach.Incomputinganadversarialperturba-\ndetectionsystemthatidentifiesbackdoortriggersbyhier- tion(i.e.,trigger)onamodelcompromisedbyabackdoor\narchicallycomparingthemodel’soutputsonbenigninputs attack,wehypothesizethattheresultinggradientsofeach\nandadversariallycraftedexamples[23].Similarly,Gaoetal. client’s model capture both input-specific details (dotted\ndemonstratedtheeffectivenessofadversarialtraininginmit- greenarrowsinFigure2(b))andthebackdoortriggerin-\nigatingbackdoorattacks[14].Weietal.furtherproposeda formation (solid red arrow in Figure 2 (b)); sum of these\nmachineunlearningtechnique,inspiredbyadversarialattack gradientsisusedforfinalclassification(dashedblackarrow\nmethods,toeliminatebackdoorvulnerabilities[48].How- inFigure2(b)).Thishypothesisisgroundedintheproper-\never,thesedefensestrategiesoverlooktheuniquechallenges tiesofgradient-basedadversarialperturbations,whichare\nofmobileandembeddedfederatedlearningenvironments, designed to alter the model’s output label. We aim to ex-\nwherescalability(e.g.,managingmultiplemodels),computa- tractthebackdoortriggerpatternbyisolatingadversarial\ntionalefficiency,andrestricteddataaccessimposeadditional perturbationsthroughtheanalysisofinputlayergradients.\nconstraintsthattraditionaldefensesmaystruggletoaddress. FeasibilityStudy.Toassessthefeasibilityofusingadversar-\nialtechniquesforbackdoortriggerdetection,weconducted\napreliminaryexperimentontheMNIST[9]datasetusing\nafederatedlearningsetupwith3-layerMLPmodels.Five\nbenign clients were trained on unaltered data, while five\n3 COREIDEAANDFEASIBILITYSTUDY\nbackdoorclientsweretrainedondatawithatriggerlocated\nTocountertrigger-basedbackdoorattacks,identifyingthe inthetop-leftcornerofeachimage.Eachbackdoorclientap-\ntriggerpatterniscrucial,astamperedmodelsactbenignly pliedthistriggertoarandomlyselected50%oftheirtraining\nwithoutit.Thissectionhighlightsourdefense’scoreidea,in- data.Fortriggerextraction,weusedthese10client-trained\nspiredbytherelationshipbetweenbackdoorandadversarial models and computed the input layer gradients using 10\nattacks,followedbyafeasibilitystudy. unalteredvalidationsamplesexcludedfromthetrainingset.\nCore Idea. Backdoor and adversarial attacks share over- Inputlayergradients,whicharecomputedviaasingleback-\nlappingtechniquesandgoals.Adversarialattacksidentify propagationrunuptotheinputlayer,closelyresemblethe\nsubtleinputperturbationstoalterpredictions,whileback- inputdataandserveasabasisfordetectingtriggerpatterns.\ndoorattacksuseapredefinedtriggertoproducesimilarin- AsillustratedinFigure2(c),theaveragedinputlayergradi-\ncorrectoutputs.Thisraisesthequestion:“Canadversarial entsforbenignandcompromisedmodelsrevealanoticeable\ntechniquesreverse-engineerthebackdoortrigger?” Whileboth difference,eventhoughthegradientsweregeneratedfrom\nshare commonalities, they exhibit distinct differences. As anunalteredsample.Thebackdoormodels(i.e.,Backdoor0-4\nshown in Figure 2 (a), adversarial attacks compute input- inthefigure)exhibitadistincttriggerpattern(i.e.,fourwhite\nspecific perturbations, whereas backdoor attacks apply a bars)intheirinputlayergradients.Thispatternarisesbe-\nconsistent trigger across inputs. Additionally, adversarial causethegradientsconsistentlyemphasizespatiallocations\nattackscantargetanymodel,whilebackdoorattacksaffect orfeaturesassociatedwithmisclassificationacrossmultiple\nonlymodelstrainedwithtriggersamples. samples.Inessence,thebackdoor-compromisedmodelhas\nOurproposedframework,DeTrigger,leveragesthesedis-\ntinctionsbyusingadversarialperturbationstoidentifythe\n4\nnoitadilaV\nroodkcaB\nelpmaS\nelpmaS\nConference’17,July2017,Washington,DC,USA\n4 5 6 7\n1 Local Model Update 3 Centrailized PreproG cr ea sd si ie nn gt (§ 5.2) B Da ec tek cd to ioo nr A (§t t 5a .c 3k ) Bac Pk rd uo no inr gK n (§o w 5.l 4e )dge\nModel Server\nDistribution TV TV TV\nSTOP Backdoor\nPlane\nBenign\nB Da ac tk ad so eo tr Attackers STOP N Po larm neal S Gus rap dic ieio nu ts Compromised Models AM ca tl ii vc aio tiu os n C Mle oa dn ee ld\nX N\nFigure3:OverallworkflowofDeTrigger.DeTrigger leveragesinsightsfromadversarialattackmethodologiesto\neffectivelyidentifytriggerandprunethebackdoorknowledge.\nembeddedanassociationbetweenthisspecifictriggerpat- 5 FRAMEWORKDESIGN\nternandthetargetbackdoorclassinitsgradients:offeringus We present the DeTrigger design, outlining inherent chal-\nhintsineffectivelyextractingthebackdoortriggerpattern. lengesandtheircorrespondingsolutions.\nThisinitialevidenceprovidesthefoundationforDeTrigger,\nabackdoor-robustfederatedlearningframework.However,\ndespite showing promising initial results, DeTrigger faces\nseveralchallenges.First,becausetheserverreceivesabatch\nofmodelupdatesfromitsclients,itmustefficientlyidentify\nthemaliciousmodelupdatesanddeterminethebackdoor’s 5.1 Overview\ntarget attacking label. Moreover, gradient information in-\nWeintroduceDeTrigger,anovelframeworkforenhancing\ncludesbackgroundnoise,interpretedasinput-specificdetails,\nbackdoorrobustnessinfederatedlearninginspiredbyprinci-\nwhichcomplicatestheaccurateextractionofbackdoorpat-\nplesofadversarialattack-basedmitigation.Figure3presents\nterns.ThefollowingsectionwillprovidedetailsonDeTrigger\nanoverviewofDeTrigger anditsoperations.DeTrigger oper-\nandhowitaddressesthesechallenges.\natesasfollows:first,theserverdistributesthelatestglobal\nmodelandselectsclientsfortraining(1).Theseclientsthen\nupdatetheirlocalmodelsusingtheirindividualdatasets(2).\n4 THREATMODELANDASSUMPTIONS\nIfaclientismalicious,itmaytrainitsmodelwithaback-\nWepresentthethreatmodel,namelythegoalandcapability door dataset with trigger patterns embedded. After local\noftheattacker,alongwiththeassumptions. training, clients send updated model weights back to the\nThreatModel.Thegoalofabackdoorattackeristomanipu- server(3).Atthispoint,theservercomputesinputlayer\nlatethefederatedlearningprocesstoproduceacompromised gradientsusingasmallvalidationdataset(10-1000samples)\nglobalmodel.Toachievethis,attackersmayaltertraining acrossalllabelstoevaluateeachclient’supdatedmodeland\ndataorlabelsbyinjectingtriggerpatternsortamperingwith DeTrigger preprocessesmodelgradientstoextractpotential\ndatalabels.Also,multipleattackersmaycollaboratebyshar- backdoortriggers(4)(c.f.,Sec.5.2).Operatingunderthein-\ningattackinformation,includingtriggerpatterns,toincrease sightthatthetriggerpatternscanbereviledwhenanalyzing\nthechancesofcompromisingtheglobalmodel.Notably,at- theinputlayergradients(c.f.,Sec.5.3),theserveridentifies\ntackersarelimitedininterferingwiththefederatedlearning suspiciousmodelupdatesthatmaycontainbackdoorknowl-\nprocessatthelocaldevicelevel.Specifically,theycannotma- edge.Whendetected,DeTriggertestssuspiciousmodelswith\nnipulateserver-sideaggregation,modeldistribution,client datacontainingtheinferredtrigger.Ifpredictionsfromthe\nselection,ormodifythetrainingprocessesofotherclients. modelchangeduetothetrigger,DeTrigger flagsthemodel\nAssumptions.Weassumethatafederatedlearningsystem ascompromised(5)(c.f.,Sec.5.3).\nwithnumerousclientsparticipatingwithoutdirectlysharing Toneutralizebackdoorknowledgeandcreateacompromised-\ntheirlocaltrainingdatawiththecentralserver.Additionally, but-cleanmodel,DeTrigger prunesmaliciousactivationsin\nweassumethatmaliciousclientscompriselessthan50%of the compromised models by closely observing how their\nthetotalnumberofparticipatingclients[13].Weconsider neuralnetworkgradientsrespondtobackdoor-embedded\ntheseassumptionsreasonableforpracticalfederatedlearning samples(6)(c.f.,Sec.5.4).Finally,DeTriggeraggregatesboth\nenvironmentsandassociatedattackscenarios.Inaddition, benignand“cleaned”maliciousmodelupdates,producinga\nweassumethattheserverhasafewcleanvalidationsamples refinedglobalmodel(7).Thefollowingsectionsdetaileach\nthatcanbeusedforadditionalprocesses. coreoperationofDeTrigger indepth.\n5\nConference’17,July2017,Washington,DC,USA KichangLee,YujinShin,JonghyukYun,JunHan,andJeongGilKo\narethennormalized throughmin-maxscaling,andamask\n(i) Input Layer Gradient Collection\nExtracted\nisgeneratedtoemphasizespatiallocationswheregradient\nTest Sample Gradient Trigger\namplitudesexceedasetthreshold.ThismaskallowsDeTrig-\nger tofocusonregionswithstrongertrigger-relatedsignals,\neffectivelyfilteringoutirrelevantinformation.Inthispaper,\nwesetthethresholdto0.5.\n(ii) Filtering Additive Elements\nFinally,DeTrigger refinesthetriggerpatternbymodify-\nAdditive Noise ing test samples, and iteratively refines the trigger quality.\nGround-Truth Specifically,itreplacespixelswithinmaskedlocationswith\nTrigger\nprocessedgradientsandrecalculatesadversarialperturba-\ntionsanew.Whileadversarialattacksadd perturbationsto\n(iii) Nomarlization and Thresholding original input data, backdoor attacks replace specific pix-\nelswiththetriggerpattern.Thisiterativeprocessreduces\nMin-Max\nNormalization Threshold input-specific biases, enhancing the clarity of the trigger\npatternbyadheringtothefoundationalconceptofbackdoor\nattacks.Throughthisprocess,DeTrigger generatespotential\n(iv) Iterative Trigger Pattern Refinements triggerpatternsforeachmodelupdateandlabel,resulting\nFigure 4: Illustration of gradient preprocessing and in𝐾×𝐶 potentialtriggerpatterns,where𝐾 and𝐶 represent\ntriggerextractionoperationsinDeTrigger. thenumberoflabelsandupdatedclients,respectively.\nTemperatureScaling.Additionally,asmentionedabove,\n5.2 GradientPreprocessing\ngradients often include input-specific information that is\nTriggerExtraction.Extractingbackdoortriggerpatterns notrelatedtothebackdoortrigger,whichcomplicatestheir\nbasedongradientinformationpresentschallengesasgra- extractingprocess.Here,tobetterobtainthetrigger-related\ndients(attheinputlayer)inherentlycontainbiasesspecific information from the gradient, we introduce temperature\nto the input data, limiting their representation of general scaling-basedtrigger-relatedinformationamplificationdur-\npatterns.Consequently,rawgradientsincludebothtrigger- ingthegradientcalculationoperations.Notethatthetem-\nrelatedinformationandinput-specificnoise.Theobjective peratureparameter𝑇 modulatesthesmoothnessoftheprob-\nofgradientpreprocessingistoaddressthesechallengesby abilitydistributionoutputbythesoftmaxfunction[20,26].\nisolating(andidentifying)thebackdoortriggerinformation Highervaluesof𝑇 >1smooththeprobabilitydistribution,\nfromirrelevantdetailsembeddedintherawgradientscalcu- while values between 0 and 1 sharpen it. This smoothing\nlatedusingvalidationdatasamples.Gradientpreprocessing canbeunderstoodaseffectivelymovingadatasamplecloser\ninDeTriggerconsistsoffourkeysteps:(i)inputlayergradient tothedecisionboundary,wenotethatthiscanpotentially\ncollection(ii)filteringadditiveelements,(iii)normalization enhancethefocusonbackdoor-relevantfeatures.\nand thresholding, and (iv) iterative trigger pattern refine- Toelaborate,Figure5(a)visualizesabackdoormodel’sde-\nment.WeillustratetheseoperationsinFigure4. cisionboundary,illustratingabackdoorfeaturespacelying\nToidentifyandextractanaccuratetriggerpattern,DeTrig- nearthedecisionboundaryintersectionwithinthenormal\nger startsbyutilizinganuncontaminatedvalidationdataset dataplane(redconeshape).Giventhecomplexityandhigh\n(coveringalllabels)attheservertoextracttheinputlayer dimensionality of neural network feature spaces, we hy-\ngradients.Here,foreachmodelcollectedfromtheclients, pothesizethatbenignandbackdoormodelssharea“normal\neachsampleinthevalidationdatasetispassedthroughthe dataplane”wherestandardsamplesarepositioned,while\nmodel, followed by a single round of backpropagation to backdoorsamplesexistnear,butjustbeyond,thisplane.We\ncapturetheinputlayergradients.Asaresult,aninputlayer hypothesizeaconeshapeforthebackdoorfeaturespaceas\ngradientisrecordedforeachsample-modelpairacrossall theprobabilityofasamplefallingintothisspaceincreasesas\nlabelsinthesystem. samplesareclosertotheintersectionofdecisionboundaries\nUsingtheseinputlayergradients,DeTrigger identifiesand inthenormaldataplane.\nfiltersadditiveelements withineachmodel’sgradientthat ThisinsightalignswithpriorfindingsbySuetal.,whoob-\nshiftpredictionsfromtheoriginaltothetargetattacklabel. servedthatthedecisionboundaryofthebackdoorsampleis\nAsalsodemonstratedinourpreliminarystudies(Figure2(c)), tangenttootherlabels[41].Figures5(b)and(c)illustratethe\nthisprocessextractsinformationwithinthegradientsspecif- impactoftemperaturescalingontheL1-normandinferred\nically linked to the trigger pattern. To minimize sample- triggerpatterns,respectively.Specifically,theresultsshow\nspecific noise variations, DeTrigger averages the gradient theL1-normofgradientsdecreaseswithmoderatetempera-\nelementsacrossmultiplesamples.Theaveragedgradients turescaling(𝑇 >1),improvingtheclarityofinferredtrigger\n6\nConference’17,July2017,Washington,DC,USA\nTrue Trigger T=1 | L1=51.357 T=5 | L1=13.641\n55\n50\nEnergy for 45\n40\nNormal Backdoor Attack 35\nData Plane 30\n25\nDecision ε 20\nBoundary 15\n1/5 1/4 1/3 1/2 1 2 3 4 5\nTemperature\n(a) (b) (c)\nFigure5:(a)Conceptualillustrationoftheimpactoftemperaturescalingonnormaldatafeaturespaceandbackdoor\nfeaturespace.(b)L1-normbetweengroundtruthandinferredtriggerswithvaryingtemperatures.(c)Sampleof\ngroundtruthandinferredtriggerswithdifferenttemperatures.\nThreshold Benign Backdoor indicativeofbackdoortriggers.Specifically,DeTrigger com-\n200 putesthetotalvariationofthepreprocessedgradientsforall\n175\n150 potentialattacktargetlabelsacrosseachmodelasfollows:\n11 02 05 𝑇𝑉(𝑥) =(cid:205) 𝑖,𝑗((cid:12) (cid:12)𝑥 𝑖+1,𝑗 −𝑥 𝑖,𝑗(cid:12) (cid:12)+(cid:12) (cid:12)𝑥 𝑖,𝑗+1−𝑥 𝑖,𝑗(cid:12) (cid:12)).\n75 Usingthis,if𝑇𝑉 fallsbelowapredefinedthreshold,the\n50\n25 evaluatedmodelisflaggedassuspicious,suggestingitcould\n0\n0 20 40 60 80 100 beinfluencedbyabackdoor,andtheassociatedlabelisdesig-\nClient ID\nnatedasapossibletargetlabel.Wemakethisdesignchoice\nFigure6:Minimumtotalvariation𝑇𝑉 oftheprocessed giventhatbackdoortriggerpatternswillbespatiallydense\nandhavingsuchelementswilldecreasethe𝑇𝑉.Notethat,\ninputlayergradientsacrossdifferentclients.Notethe\nlower𝑇𝑉 trendforbackdoor-affectedgradients. thethresholdfordeterminingthisisadaptivetoinputdata\ndimensionsandresolution,enablingtheservertosetappro-\npriatethresholdsbycomputinggradientsbasedoncentrally\navailableinformation.\npatterns.Thissuggestsgradientsobtainedwithsmoothed Transferability-basedVerification.Nevertheless,DeTrig-\nprobabilitydistributions(i.e.,highertemperatures)canmore ger mustaddressthepossibilityoffalsepositives(i.e.,benign\neffectivelycaptureinformationrelatedtothebackdoortrig- clientsidentifiedasattackers)andfalsenegatives(i.e.,attack-\nger.Notethatwesetthetemperaturetofiveinthiswork. ersclassifiedasbenign)inthedetectionprocess.Figure6\npresents the minimum𝑇𝑉 values across clients, showing\n5.3 BackdoorAttackDetectionModule thatwhilebackdoorupdates(c.f.,redbars)typicallyyield\nGiventhetriggerpatternextractedabove,thenextstepisto lower𝑇𝑉 valuesthanmostbenignupdates,certainbenign\nidentifybackdoor-affectedmodelsamongthosetransmitted updatesoccasionallyfallbelowthethreshold,andsomeback-\nfromclients.Furthermore,weshoulddeterminethetarget doorupdatesexceedthethresholdduetodataheterogeneity\nlabel/classusedfortheattack.Theoperationconsistsoftwo withinthefederatedlearningscenario.\nsteps, namely, Total variation-based contaminated model Toaddressfalsedetections,DeTrigger verifiesthetrans-\ndetectionandTransferability-basedverification. ferabilityofinferredtriggerpatterns.First,itexaminessus-\nTotalvariation-basedcontaminatedmodeldetection. piciousmodels,whichmayincludebenignones,usingthe\nInthisstep,DeTrigger leveragesthepriorknowledgethat potentialbackdoortriggersextractedfrom𝑇𝑉 thresholding.\nbackdoortriggerpatternsaretypicallymorespatiallycon- Byinjectingthesetriggersintothevalidationdataandob-\ncentratedthanstandardadversarialperturbations[47].To servingifpredictionsshifttowardsthetargetlabel(instead\ndetectthebackdoor-affectedmodelsandpinpointthetarget ofthegroundtruth),DeTrigger identifiesspecificmodelsas\nlabel, DeTrigger evaluates the total variation for potential malicious.Ifnoclassificationerrorsareseen,themodelis\ntriggerpatterns.Thetotalvariation(𝑇𝑉)ofaninputlayer removedfromsuspicion.ThisprocessallowsDeTrigger to\ngradientmap(i.e.,themapofgradientsasinFigure5(c)) effectivelyisolatetriggerpatternsthatactivatebackdoors.\nis defined as the sum of the absolute differences between Subsequently,DeTrigger testsanyremainingmodelswith\nneighboring elements along both the horizontal (𝑥 𝑖,·) and theseverifiedtriggerpatterns,flagginganyadditionalmod-\nvertical(𝑥 ·,𝑗)axesinthedata(𝑥).Forexample,givenagra- elsthatrespondtothetriggerasadversaries.\ndientmapforaninputsample,wecompareeachelement\nbyshiftingthemapbothhorizontallyandverticallybyone\nstep. The𝑇𝑉 helps identify concentrated spatial patterns\n7\nnoitairaV\nlatoT\nmroN\n1L\nConference’17,July2017,Washington,DC,USA KichangLee,YujinShin,JonghyukYun,JunHan,andJeongGilKo\n100 backdoor-affectedmodels,theextractedtriggers,andtarget\n8\n80\nF Oe rd acA lv eg\nlabelsasidentifiedinthebackdoordetectionphase.\n6 60 Basedontheobservationfrompreviousworkthatback-\n4\n40 doortriggersactivatedistinctweightscomparedtobenign 2 20\nsamples[17],aweightpruningtechniquethatremovesthese\n0 0\nBenign Backdoor 0-6 7-9 0-9 Backdoor specificweightscanbeasuitableapproachformitigating\nClient type Data type\n(a) (b) backdoorattacks.Specifically,theextractedtriggerpattern\nisfedintothemaliciousmodeltoidentifyweightsthatcon-\nFigure7:(a)Datadistributionusedinthepreliminary\ntributetoactivatingthebackdoor.Werankweightsbasedon\nmotivational study. (b) Model accuracy for different\ntheirgradientnorm,pinpointingthecontributingweights\ndatalabelswithbackdoorattacksuccessrates.\nbasedonthegradientsascontributorstothebackdooracti-\nvation.Thesehigh-gradientweightsarethenreplacedwith\nzerostoneutralizethebackdooreffect.Oncepruningiscom-\n5.4 BackdoorKnowledgePruningModule\npleted, DeTrigger aggregates the pruned models through\nTo motivate our model pruning approach, we ask the fol- weight averaging, while the model aggregation approach\nlowingquestion:“Canwesimplydiscardmaliciousupdates canbesystem-specific.ThispruningapproachallowsDe-\nifweidentifythemassuspicious?” Toanswerthis,wesetup Trigger toeffectivelyreducebackdoorcontaminationwith-\namotivatingexperimentusingtheMNISTdatasetwith10 outdiscardingthebenignknowledgeembeddedwithinthe\nclients:fivebenign,andfivebackdoorattackers.Figures7(a) maliciousclientmodels,enablingmoreresilientandaccu-\nplotsthedatadistributionforeachclient,wherethecircle rateglobalmodelperformanceacrossheterogeneousdata\nsizerepresentstherelativesamplecountperlabel.Wealso distributions.\npresentmodelaccuracyresultsfordifferentdatadistribu-\ntionsinFigure7(b).Here,weaimtomimicascenariowhere\n6 EVALUATION\nbenignclientslacksamplesfromlabels7-9,whileattackers\nWe now evaluate DeTrigger using extensive experiments\npossess this data; representing a realistic setup in mobile\nwithfourdatasetsandvariouscomparisonbaselines.\nandembeddedapplicationswheredatadistributionisoften\nhighlyheterogeneous.\nIn our experiment, we compare two federated training 6.1 ExperimentSetup\napproaches:FedAvg[31],whichsimplyaveragesallclient\nThedetailsonthedatasetsandmodelsthatweuseinour\nupdates,andanOracle,ahypotheticalserverwithperfect\nevaluationsarepresentedbelow:\nattackerknowledge,whichcompletelyexcludesmalicious\nmodelsasawholefromaggregation.AsshowninFigure7 DatasetandModel.Inthiswork,weevaluateDeTrigger\n(b),theOracleeffectivelymitigatesthebackdoorattack,with using four distinct datasets and two model architectures\nonlya2.02%backdoorattacksuccessrate,asmaliciousmodel suitableformobile/embeddedfederatedlearning:a2-layered\nupdateswerefullydiscarded.Incontrast,FedAvgshowsa CNNandResNet18.Wedetailthedatasetsandmodelsbelow.\nhigherbackdoorattacksuccessrateof50.25%sinceitdoes •CIFAR-10/CIFAR-100[25]arewidelyusedimagebench-\nnotmitigatetheattackinanyway.Nevertheless,theOra- mark datasets, each with 60,000 images at 32×32 resolu-\ncle exhibits a significant accuracy drop on data for labels tion,covering10and100classes,respectively.Weemploy\n7-9(37.53%)duetothelossofbenignknowledgeembedded a2-layeredCNNmodelasdefault[31].GivenCIFAR-100’s\nwithinmaliciousupdates,whileFedAvgmaintains90.07%ac- broader label set, it serves as an ideal dataset for testing\ncuracyontheselabels.Theseresultssuggestthatentirelydis- DeTrigger’sadaptabilitytoanincreasednumberofclasses.\ncardingmaliciousmodelscanresultinasevereperformance •GTSRB[21]isabenchmarkdatasetcomprising43types\nlossforrareclassesinheterogeneousdatadistributionsand ofreal-worldtrafficsigns.Giventhepracticalityandvulner-\nhighlighttheimportanceofleveragingbenignknowledge abilityoftrafficsignrecognitiontobackdoorattacks,this\nwithinmaliciousmodelswhileeffectivelymitigatingback- datasetenablesustoassessDeTrigger underrealisticback-\ndoorattacks,especiallyinmobileenvironmentswheredata doorscenarioswiththe2-layeredCNNmodel.\nheterogeneityiscommon. •STL-10[8]containsatotalof13Knaturalimagesacross\nWetacklethisissuebyproposingabackdoorknowledge 10 classes, with a resolution of 96×96 pixels. The higher\npruningmodulethatonlyeliminatesmodelparametersas- resolutionofSTL-10,comparedtotheotherdatasets,makes\nsociated with backdoor triggers: preventing global model itsuitableforevaluatingDeTrigger’sscalabilityconcerning\ncontaminationwhilepreservingbeneficialknowledgeinthe imageresolution.Forthisdataset,weutilizeResNet18[19]\nmodelaggregationprocess.Forthis,DeTrigger exploitsthe tohandletheincreasedcomplexity.\n8\nslebaL\n)%(\nycaruccA\n%57.79 %57.79 %70.09 %35.73 %44.59 %67.97 %52.05 %20.2\nConference’17,July2017,Washington,DC,USA\nBaselines.Torepresenttheworstandidealcases,weuseFe- FedAvg 100 FedAvg\ndAvg[31]andanOracleconfiguration.Specifically,FedAvg 60 M TMedian 80 M TMedian\nKrum Krum\nrepresentsanaiveapproach,whereallmodelupdatesare 40 MultiKrum 60 MultiKrum\nFLTrust FLTrust\naggregatedwithoutanydefenseagainstbackdoorattacks. O Dera Tc ril ge ger 40 O Dera Tc ril ge ger\nContrarily,theOracleassumesanidealsettingwithcomplete 20\n20\nknowledgeofattackerclients,enablingselectiveexclusionof\n0 0\ntheirupdatestopreventcontaminationoftheglobalmodel. 80 Global8 A2 ccuracy8 (4 %) 86 25 30 Glob3 a5 l Acc4 u0 racy4 (5 %) 50 55\nToleveragesecureaggregationusingstatisticalpriors,we (a) GTSRB (b) CIFAR-10\nalsocomparewiththeMedianandTrimmedMeanaggre- Figure8:Overallglobalmodelaccuracyv.s.backdoor\ngationapproaches[53].Medianaggregationcomputesthe attackaccuracyacrossdifferentbaselines.\nmedianratherthanthemean,andTrimmedMeandiscards\noutlierparameterswithextremevaluesbeforeaveragingup-\ndates.TheseByzantine-robustmethodsareeffectiveagainst\ndatasets across 100 clients, which include 25 backdoor at-\nstraightforwardmodelpoisoning,giventhatmaliciousup-\ntackerswithnon-independentandidenticallabeldistribu-\ndatesoftendeviatesignificantlyfrombenignones.\ntionfollowingaDirichletdistributionwiththeimportance\nWe also compare DeTrigger with three other federated\nparameter𝛼 =0.5[22].Thebackdoorattackersgeneratea\nframeworksthatassessupdatesimilarity.Krum[4]identifies\nbackdoortrainingsetbyinjectingtriggerpatternsinto25%\nreliableupdatesbycalculatingpairwiseEuclideandistances\nofthetrainingdata.Foreveryfederatedtraininground,10\nandselectingtheupdatewiththeminimumtotaldistance.\nlocalmodelswereselectedandtrainedfor5epochswiththe\nMultiKrum[4]enhancesthisbydiscardingaportionofthe\nAdamoptimizer[24],withalearningrateof5e-3andabatch\nmostdistantupdatesbeforeaveraging,improvingresilience\nsizeof64.Throughouttheevaluation,weusedaserverwith\nagainstoutliers.Lastly,FLTrust[5]trainsaverifiedmodel\nanNvidiaRTX3090GPU,anIntelXeonSilver42102.20GHz\nonthecentralserverandcomputestrustvaluesbasedon\nCPU,and128GBRAM.\nthecosinesimilaritybetweenclientmodelupdatesandthe\nverifiedmodel.Thesetrustvaluesadjustaggregationratios\n6.2 OverallPerformance\ntosuppresspotentialmaliciousupdateseffectively.\nThesebaselinesaimtosuppresstheeffectofsuspicious Webeginourevaluationsbypresentingtheoverallperfor-\nclientsratherthanaccuratelyidentifyingthebackdoorat- manceofDeTriggertounderstandhowwelltheglobalmodels\ntackerandtriggerinformation.Tohighlightthepracticality inDeTrigger performwhilemitigatingbackdoorattacks.Fig-\nofDeTrigger,wecompareitwithadvancedbackdoordefense ure8plotstheglobalmodelaccuracyandbackdooraccuracy\nmethodssuchasNeuralCleanse[47]andTABOR[18],which forDeTrigger anddifferentcomparisonbaselineswithtwo\narespecializedinbackdoordetectionandtriggeridentifica- differentdatasets.Thebackdooraccuracydenotestheattack\ntion.WhileNeuralCleanseandTABORexcelinidentifying success rate when a trigger is present in the input, while\ntriggersthroughoptimization-basedapproaches,theircom- globalaccuracyshowstheclassificationperformanceonun-\nputationalinefficiencymakesthemunsuitableforfederated altered samples. In this context, an ideal approach would\nlearningscenarios.Thiscomparisonunderscorestheadvan- appear in the bottom-right corner of the plot, signifying\ntageofourframeworkinachievingrobustbackdoordefense lowbackdooraccuracy(effectiveattackmitigation)andhigh\nwithoutcompromisingefficiency. globalaccuracy(preservedmodelperformance)\nAstheresultsshow,DeTrigger consistentlyshowssupe-\nAttack Method. In this work, we address patch-trigger-\nriorperformanceinachievingabalancebetweenthesetwo\nbasedbackdoorattacks,whereattackerstrainmodelsusing\nperformancemetrics.FortheGTSRBdataset(Figure8(a)),\nbackdoor samples containing a designated patch overlaid\nwhileFedAvgdemonstrateshighglobalmodelaccuracy,it\noncleanimages.Unlessotherwisespecified,weuseared\nis significantly susceptible to backdoor attacks. Similarly,\nsquarepatchsizedat 1 oftheimageresolution.Forexample,\n8 methodssuchasFLTrustandMedianslightlyreduceback-\nweapplya4×4patchfordatasetslikeCIFAR-10,CIFAR-100,\ndooraccuracy,butthesecomeatthecostofdegradedglobal\nandGTSRB,whichhave32×32resolution,anda16×16patch\nmodelperformance.MultiKrumandTrimmedMean(TM)\nfortheSTL-10datasetwitha96×96resolution.Laterinour\nshowimprovementsinrobustness,buttheiroverallglobal\nevaluations,wepresenttheperformanceofDeTrigger using\nmodelfailstoreachthelevelofDeTrigger.\ndifferenttriggerpatternsaswell.\nSimilarly,withtheCIFAR-10dataset(Figure8(b)),DeTrig-\nMiscellaneousConfigurations.Regardingthefederated geroutperformsallbaselines.Thebaselinesachievemarginal\nlearningprocess,unlessmentionedotherwise,wesplitthe improvementsinbackdoorresistancebutexhibitdegradation\ninglobalmodelaccuracy.Incontrast,DeTrigger significantly\n9\n)%(\nycaruccA\nroodkcaB\n)%(\nycaruccA\nroodkcaB\nConference’17,July2017,Washington,DC,USA KichangLee,YujinShin,JonghyukYun,JunHan,andJeongGilKo\n260 251.46× Global Accuracy Drop Backdoor Accuracy Drop L1 Norm\n230\n200 182.57× 100 100 170\n2.5 2.23×\n1.00× 0.22× 0.34× 0.70× 1.00× 50 50\n0.0\nDeTriggerMedian Trimmed Krum Multi FLTrust TABOR Neural\nMean Krum Cleanse\n0 0\n1 10 50 100 250 500 1000\nFigure 9: Elapsed time per federated training round Number of Samples\nwithrespecttodifferentdefenseschemesnormalized Figure10:L1-normofgroundtruthandinferredtrigger\ntotheperformanceofDeTrigger. along with the accuracy drop for the backdoor and\nglobalmodelwithvaryingthenumberofvalidation\nsamplesattheserver.\nsuppressesbackdooraccuracywhilemaintainingcompeti-\ntiveglobalmodelperformance,narrowingthegaptoward\ntheidealresultsobservedwithOracle-leveldefenses.\nOne interesting observation we make is that DeTrigger\nachieveshigherglobalmodelaccuracycomparedtotheOra-\nclebaselineforbothdatasets.Thisisduetothefactthat,un-\nlikeOracle,whichcompletelyexcludesattackermodelsfrom (a) Backdoor model before pruning (b) Benign model before pruning\naggregation,DeTrigger carefullyprunesonlythebackdoor-\nrelatedinformationwhileretainingandleveragingthebe-\nnignknowledgeofacompromisedmodel.Thisshowcases\nthe effectiveness of our approach in preserving valuable\nmodelupdateswhilemitigatingbackdoorattacks.\n(c) Backdoor model after pruning (d) Benign model after pruning\n6.3 ComputationalEfficiencyofDeTrigger\nFigure11:Visualizationofrepresentationspacebefore\nAshighlightedearlier,thetimerequiredtovalidatemodels andafterweightpruningisappliedonbackdoorand\ninmitigatingbackdoorattacksisacriticalfactorasitcan benignmodels.\nintroducedelaysintheoverallfederatedtrainingprocess.\nFigure 9 presents the elapsed time for a single federated\ncomputationallatencybeingasignificantissueforpractically\nlearningroundwith50clients,including25backdoorattack-\nadoptingthesepreviouslyproposedschemes.\ners,averagedover100trials.Here,theperformanceofall\nRecallthatDeTriggercomputesinputlayergradientsusing\ncomparisonmethodsisnormalizedtothelatencyofDeTrig-\nger,whichis∼0.71sec.Theplotsshowthatdefenseschemes unalteredvalidationsamplesattheserver;thus,itsperfor-\nmancereliesontheavailabilityofthesesamples.Weperform\nbased on safe aggregation with statistical priors, such as\nanevaluationtoexamineDeTrigger’sperformancewithvary-\nMedianandTrimmedMean(TM),exhibitedrelativelylow\ntimecomplexity,being4.56×and2.94×fasterthanDeTrigger, ingvalidationsamplequantitiesforgradientcomputation.\nFigure 10 shows the global model and backdoor attack\nrespectively.Ontheotherhand,methodssuchasKrum,Mul-\naccuracydroprates,andtheL1normbetweentheground\ntiKrum,andFLTrust,whichcomputethesimilaritybetween\ntruthandDeTrigger-predictedtriggerpatternsforvalidation\nmodelupdates,showedslightlyhighercomputationalcosts\nsamplesizesrangingfrom1to1000.Whenthesizeofthe\nduetotheadditionalsimilaritycalculationsrequiredintheir\nvalidationsetisextremelylow(e.g.,1),theL1normexhib-\nschemes.Whiletheirlatencyisatapracticallyacceptable\nited higher error, and the backdoor defense performance\nlevel(someevenfasterthanDeTrigger)weshowinthefol-\ndegraded(∼60%blockrate).However,withjust10validation\nlowingevaluationsthatthiscomesatthepriceoffailingto\nsamples(randomlyselectedfromasetof1,000),DeTrigger\nmitigatethebackdoorattackinmanycasesproperly.\nachievedsignificantlyimprovedtriggerpredictionquality\nFurthermore,Figure9indicatesthattheoptimizationand\nandanoticeableincreaseintheattackblockrate.\ntrainingprocessesinTABORandNeuralCleanse,whichaim\ntodetectbackdoormodelsandextracttriggers(similarto\n6.4 ImpactofBackdoorKnowledgePruning\nDeTrigger),showsignificantlyhighercomputationallatency,\nbeing182.57×,and251.46×higherthanDeTrigger,respec- Toevaluatetheeffectivenessofbackdoorknowledgepruning,\ntively.Giventhatthesystemwillencountersuchlatencyat wevisualizedthelearnedrepresentationsofbothbenignand\neveryfederatedlearninground,andalsothatthislatency backdoormodels(GTSRBdatasetandCNNconfiguration)\nwillincreasewithanincreasingnumberofclients,weseethe usingt-SNE[46].Ast-SNEmapsdatatoarbitraryspaces;\n10\nemiT\nevitaleR\n)%(\nporD\nycaruccA\nmroN\n1L\nConference’17,July2017,Washington,DC,USA\nBackdoor Accuracy Drop Global Model Accuracy Drop\n0.15 100\n0.10 80\n0.05 60\n40\n0.00\n20\nCI CFA NR N10 CIF CA NR N100 RC eI sF NAR et1 10 8 ReS sT NL e1 t0 18 G Tr ro uu tn hd CI CFA NR N10 CIF CA NR N100 RC eI sF NAR et1 10 8 ReS sT NL e1 t0 18 0 CI CFA NR N10 CIF CA NR N100 RC eI sF NAR et1 10 8 ReS sT NL e1 t0 18\n(a) (b) (c)\nFigure12:(a)L1-normwithrespecttoinputsizefordifferentdataset/modelconfigurations.(b)Sampletriggers\nextractedfordifferentdataset/modelconfigurations.(c)Accuracydropofbackdoorandglobalmodelaccuracyfor\ndifferentdataset/modelconfigurations.\nthus,absolutelocationsordistancesarenotdirectlycompa-\nrableacrossfigures.Forclarity,weonlyincludethetopfive\nlabels,includingtheattacktargetlabel(from43classes).\nFigures11(a)and(b)illustratethelearnedrepresentations\nprior to pruning for backdoor and benign models, respec-\ntively.Asshown,thebackdoorsamples(reddots)inFigure11\n(a)areclusteredseparatelyfromthenormalsamplesdueto Figure13:Extractedsampletriggerswithvaryingcolor,\ntheattackconstructinganindependentdecisionboundary location,andshapeofthebackdoortrigger.\n(forintentionalmisclassification),whereasinFigure11(b),\nthebenignmodelmapsthebackdoorsampleswiththeirorig-\n18.0\ninallabelsratherthanthetargetattacklabel.Thisindicates\nthatabenignmodelisnotaffectedbytheattacktrigger. 13.5\nWepresenttherepresentationsofbackdoor-affectedand 9.0\nbenignmodelsafter thepruningprocessusingtheinferred 4.5\n6.60 9.81 7.76 10.04 7.09 14.58 13.09 12.38\ntriggerinFigures11(c)and(d),respectively.Figure11(c) 0.0\nRed Green Blue Top Bottom Bars Bars X\nshowsthatpruningdisruptstheredclusterassociatedwith Left Right Left Right Center\nFigure14:L1-normwithvaryingcolor,location,and\nthebackdoor,redistributingthesesamplestowardthenormal\nshapeofthebackdoortrigger.\ndatafeaturespace.Notethatinsomecases,DeTrigger may\noccasionally misclassify a benign model as compromised\nand apply pruning. However, Figure 11 (d) demonstrates\nthatsuchunintendedpruninghasminimalimpactonbenign forSTL10-ResNet18,acleartriggerpatternislessevident,\nmodel behavior, as unrelated knowledge (e.g., labels 0-4) suggestingpossibleperformancedegradationinscenarios\nremainsintactpost-pruning. involving high-resolution data and complex models. Nev-\nertheless, despite imperfect trigger extraction from a hu-\n6.5 PerformanceAcrossVaryingDataset manperceptionperspective,DeTrigger effectivelyremoves\nandModelCharacteristics trigger-relatedinformationduringitspruningphasewhile\nretainingbenignmodelknowledge.Specifically,Figure12\nNext,weevaluateDeTrigger’sperformanceacrossdiverse\n(c) shows the accuracy drop rate for the backdoor attack\ndataset-modelconfigurations,focusingon(i)labeltypes,(ii)\nandglobalmodelbeforeandafterpruning.Wecannotice\ninputresolutions,and(iii)modelcharacteristics.\nthattheglobalmodelaccuracyremainslargelyunaffected,\nFigure12(a)presentstheL1normbetweentheground\nwhilebackdooraccuracyisreducedbyupto98.90%inthe\ntruthtriggerpatternandtheinferredtriggeracrossvarious\nCIFAR100-CNNcase.ThisalignswithfindingsbyWanget\nconfigurationswith75benignclientsand25backdoorat-\nal.[47],whonotedthatbackdoorscanstillexploitimperfect\ntackerstotalof100participants.Toensurefaircomparisons\npatterns.Overall,theseresultsdemonstratethatDeTrigger\nacrossresolutions,theL1normisnormalizedbydividingit\nisflexibleenoughtosupportavarietyofdatasetandmodel\nbythespatialdimension(𝐻 ×𝑊),asinputdatadimensions\nconfigurationswithinfederatedlearningoperations.\naffectthenorm.Theresultshereshowstableinferredtrigger\nqualityacrossconfigurations,withaminorexceptioninthe\n6.6 ScalabilityofDeTrigger\nSTL10-ResNet18 setting, which combines high-resolution\ninputwitharelativelycomplexmodel. Finally,weevaluateDeTrigger’sscalabilityondifferentback-\nNext,Figure12(b)visualizesthereverse-engineeredtrig- doortriggerpatternsandtheincreasingnumberofpartici-\ngers for different dataset-model configurations. Note that patingclientsinthefederatedlearningnetwork.\n11\n)W×H(/mroN\n1L\ndnuorG\nreggirTeD\nmroN\n1L\nhturT\ndetcartxE\n)%(\nporD\nycaruccA\nConference’17,July2017,Washington,DC,USA KichangLee,YujinShin,JonghyukYun,JunHan,andJeongGilKo\nBackdoor Accuracy Drop Global Model Accuracy Drop withincreasingclients.However,thelatencyforDeTrigger\n100 issignificantlylowercomparedtopreviouslyproposedal-\n80\nternativessuchasTABORandNeuralCleanse.Thisresults\n60\n40 suggeststhatDeTrigger isanefficientandscalablesolution\n20 foraddressingbackdoorattacksinFLsystems.\n0\nRed Green Blue Top Bottom Bars Bars X\nLeft Right Left Right Center\nFigure 15: Accuracy drop for the backdoor task and\n7 DISCUSSION\nmaintaskfordifferentcolors,locations,andshapesof\nBasedonourexperiencesindesigningandevaluatingDe-\nthebackdoortrigger.\nTrigger,wediscussthelimitationsofourcurrentresearch\nandsuggestdirectionsforfuturework.\nDeTrigger NeuralCleanse TABOR •Understandingbackdoorattackviagradients.Inthis\n103\npaper,weexploredtherelationshipbetweenbackdoorand\nadversarialattacksusinggradientanalysis.Wealsointro-\n101 ducedthetemperaturescalingtrick,offeringanovelperspec-\ntiveonthedecisionboundaryofbackdoormodels.These\n1 5 10 25 50 100\nanalysesprovidevaluableinsightsthatenhancetheunder-\nNumber of Clients\nstandingofbackdoorattacks.Wehopetheinsightspresented\nFigure16:Normalizedlatencyfordifferentbackdoor\nherewillserveasafoundationforfurtherresearchandin-\ndetectionschemeswithvaryingnumberofclients.\nspirenewdiscussionsonbackdoorvulnerabilitiesinneural\nnetworks.\n6.6.1 DifferentBackdoorTriggerPatterns. Triggerpatterns • Exploiting advanced adversarial attacks. Our work\nforbackdoorattackscanvarywidely.Whilesmallpatterns primarilyseekstoemphasizeandvalidatethefeasibilityof\nappendedtotheoriginalinputarecommoncharacteristics, leveragingadversarialattackconceptstomitigatebackdoor\ntheirvisualcharacteristics,suchasshapeandcolor,candiffer attacksinfederatedlearning.Accordingly,DeTrigger utilizes\nsignificantly.Toevaluatethescalabilityandrobustnessof astraightforwardgradient-basedadversarialattack.Weac-\nDeTrigger againstdiversebackdoortriggers,weconducted knowledgethatpriorstudieshaveproposedmoreadvanced\nexperimentsusingeightdifferenttriggertypes,visualized adversarialattacktechniques.Webelievethatincorporating\nat the top of Figure 13 leveraging the 2-layered CIFAR10 theseapproachescouldfurtherenhancetheeffectivenessof\ndataset and CNN model. As shown in the bottom of the ourdefensemechanism.\nfigure, DeTrigger successfully extracts the triggers across •AdaptiveattackagainstDeTrigger.Tofurtherstrengthen\nvariousshapesandcolors,albeitnotperfectly. thefederatedlearningframework,itisessentialtoexploreits\nTheL1normbetweenthegroundtruthandinferredtrig- limitationsandconductstresstestingofthedefensemech-\ngerpatterns,showninFigure14,indicatesthatnon-continuous anism. While our work demonstrates the effectiveness of\npixelpatterns(e.g.,thefinalthreetriggersinFigure13)tend leveragingadversarialattackstomitigatebackdoorattacks,\ntoshowincreasedquantitativedetectionerror.However,at we also consider potential adaptive attacks that could be\nasystemlevel,Figure15demonstratesthatevenimperfect designedtoevadeDeTrigger.Forexample,anadaptiveattack\ntriggerextractions,whenintegratedintothefullDeTrigger mightemploytriggerswithhightotalvariationtobypassde-\npipeline,effectivelymaintainahighlyaccurateglobalmodel tection,asDeTrigger usestotalvariationmetricstoidentify\nwhilesignificantlysuppressingbackdoorattacks. backdoorupdates.Importantly,theglobalmodelaccuracy\ndidnotsignificantlydegradefromthepruningprocess,in-\n6.6.2 NumberofParticipatingClients. Wenowexaminehow\ndicatingthatDeTrigger canadapttocountertheseattacks\nDeTrigger scaleswiththeincreasingnumberofclientsinits\nbyexpandingitsdetectioncriteriatoaccountforabroader\nfederatedlearningnetwork.Here,wefocusonthecompu-\nrangeofpotentialtriggers.Furthermore,adjustingthetotal\ntationaloverheadofdealingwiththeincreasednumberof\nvariation𝑇𝑉 thresholdwouldallowDeTrigger tomaintain\nupdatedmodelsthatarecollectedattheserver.Thisispartic-\nitseffectivenessagainstadaptivethreats.\nularlyimportantgiventhatlonglatenciesleadtoincreased\nintervalsbetweenfederatedlearningrounds,whichinturn\ntranslatestoprolongedmodelconvergence.\n8 CONCLUSION\nOurresultswiththeCIFAR10datasetand2-layeredCNN\nmodelplottedinFigure16suggestthat,asexpected,theover- Inthispaper,weintroducedDeTrigger,abackdoor-robustfed-\nall computation time for DeTrigger’s operations increases eratedlearningframeworkdesignedtodetectandmitigate\n12\n)%(\nporD\nycaruccA\nemiT\nevitaleR\nConference’17,July2017,Washington,DC,USA\nbackdoorattacksbyleveragingadversarialattackmethodolo- LearningFrameworkwithSmallLabeledData.InProceedingsofthe\ngies.Throughgradientanalysisandtemperaturescaling,De- 21stACMConferenceonEmbeddedNetworkedSensorSystems.56–69.\nTrigger effectivelyisolatestriggerpatterns,enablingmodel [11] YaoDeng,XiZheng,TianyiZhang,ChenChen,GuannanLou,and\nMiryungKim.2020.Ananalysisofadversarialattacksanddefenses\nweightpruningfortheremovalofbackdooractivationswhile\nonautonomousdrivingmodels.In2020IEEEinternationalconference\nretainingbenignknowledgewithintheglobalmodel.Our\nonpervasivecomputingandcommunications(PerCom).IEEE,1–10.\nextensiveevaluationsdemonstratethatDeTrigger notonly [12] FatimaElhattab,SaraBouchenak,RaniaTalbi,andVladNitu.2023.Ro-\nachieves significant speed improvements over traditional bustfederatedlearningforubiquitouscomputingthroughmitigation\nbackdoordefensesbutalsopreservesmodelaccuracyand ofedge-casebackdoorattacks.ProceedingsoftheACMonInteractive,\nMobile,WearableandUbiquitousTechnologies6,4(2023),1–27.\nmitigatesattackeffectivenessbyupto98.9%.Additionally,\n[13] MinghongFang,XiaoyuCao,JinyuanJia,andNeilGong.2020.Local\nthroughextensiveevaluationsusingfourwidely-usedpublic modelpoisoningattacksto{Byzantine-Robust}federatedlearning.In\ndatasets,weexploredthescalabilityofDeTrigger acrossdi- 29thUSENIXsecuritysymposium(USENIXSecurity20).1605–1622.\nversesettings,confirmingitsadaptabilitytovaryingmodel [14] YinghuaGao,DongxianWu,JingfengZhang,GuanhaoGan,Shu-Tao\ncomplexities,labelsizes,anddataresolutions.Bycombining Xia,GangNiu,andMasashiSugiyama.2023.Ontheeffectivenessof\nadversarialtrainingagainstbackdoorattacks. IEEETransactionson\nefficiency with precision, DeTrigger sets a foundation for\nNeuralNetworksandLearningSystems(2023).\nsecureandscalablefederatedlearning.\n[15] XueluanGong,YanjiaoChen,QianWang,andWeihanKong.2022.\nBackdoorattacksanddefensesinfederatedlearning:State-of-the-art,\nACKNOWLEDGMENTS taxonomy,andfuturedirections.IEEEWirelessCommunications30,2\n(2022),114–121.\nWe acknowledge Sungmin Lee, Ph.D. candidate at Yonsei\n[16] IanJGoodfellow.2014.Explainingandharnessingadversarialexam-\nUniversity,fortheconstructivediscussionaboutthework.\nples.arXivpreprintarXiv:1412.6572(2014).\n[17] TianyuGu,BrendanDolan-Gavitt,andSiddharthGarg.2017.Badnets:\nREFERENCES Identifyingvulnerabilitiesinthemachinelearningmodelsupplychain.\narXivpreprintarXiv:1708.06733(2017).\n[1] JungmoAhn,JaeYeonPark,SungSikLee,Kyu-HyukLee,Heesung\n[18] WenboGuo,LunWang,XinyuXing,MinDu,andDawnSong.2019.\nDo,andJeongGilKo.2023.SafeFac:Video-basedsmartsafetymoni-\nTabor:Ahighlyaccurateapproachtoinspectingandrestoringtrojan\ntoringforpreventingindustrialworkaccidents.ExpertSystemswith\nbackdoorsinaisystems.arXivpreprintarXiv:1908.01763(2019).\nApplications215(2023),119397.\n[19] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016.Deep\n[2] EugeneBagdasaryan,AndreasVeit,YiqingHua,DeborahEstrin,and\nresiduallearningforimagerecognition.InProceedingsoftheIEEE\nVitalyShmatikov.2020. Howtobackdoorfederatedlearning.InIn-\nconferenceoncomputervisionandpatternrecognition.770–778.\nternationalconferenceonartificialintelligenceandstatistics.PMLR,\n[20] GeoffreyHinton,OriolVinyals,andJeffDean.2015. Distillingthe\n2938–2948.\nknowledgeinaneuralnetwork.arXivpreprintarXiv:1503.02531(2015).\n[3] Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier\n[21] SebastianHouben,JohannesStallkamp,JanSalmen,MarcSchlipsing,\nFernandez-Marques,YanGao,LorenzoSani,KwingHeiLi,TitouanPar-\nandChristianIgel.2013.DetectionofTrafficSignsinReal-WorldIm-\ncollet,PedroPortoBuarquedeGusmão,etal.2020.Flower:Afriendly\nages:TheGermanTrafficSignDetectionBenchmark.InInternational\nfederatedlearningresearchframework.arXivpreprintarXiv:2007.14390\nJointConferenceonNeuralNetworks.\n(2020).\n[22] Tzu-MingHarryHsu,HangQi,andMatthewBrown.2019.Measur-\n[4] PevaBlanchard,ElMahdiElMhamdi,RachidGuerraoui,andJulien\ningtheeffectsofnon-identicaldatadistributionforfederatedvisual\nStainer.2017.Machinelearningwithadversaries:Byzantinetolerant\nclassification.arXivpreprintarXiv:1909.06335(2019).\ngradientdescent.Advancesinneuralinformationprocessingsystems\n[23] KaidiJin,TianweiZhang,ChaoShen,YufeiChen,MingFan,Chenhao\n30(2017).\nLin,andTingLiu.2022.Canwemitigatebackdoorattackusingadver-\n[5] XiaoyuCao,MinghongFang,JiaLiu,andNeilZhenqiangGong.2020.\nsarialdetectionmethods?IEEETransactionsonDependableandSecure\nFltrust:Byzantine-robustfederatedlearningviatrustbootstrapping.\nComputing20,4(2022),2867–2881.\narXivpreprintarXiv:2012.13995(2020).\n[24] DiederikPKingmaandJimmyBa.2014.Adam:Amethodforstochastic\n[6] XiaoyuCaoandNeilZhenqiangGong.2022.Mpaf:Modelpoisoning\noptimization.arXivpreprintarXiv:1412.6980(2014).\nattackstofederatedlearningbasedonfakeclients.InProceedingsof\n[25] AlexKrizhevsky,GeoffreyHinton,etal.2009.Learningmultiplelayers\ntheIEEE/CVFConferenceonComputerVisionandPatternRecognition.\noffeaturesfromtinyimages.(2009).\n3396–3404.\n[26] KichangLee,SongkukKim,andJeongGilKo.2024.FLex&Chill:Im-\n[7] NicholasCarliniandDavidWagner.2017. Towardsevaluatingthe\nprovingLocalFederatedLearningTrainingwithLogitChilling.arXiv\nrobustnessofneuralnetworks.In2017ieeesymposiumonsecurityand\npreprintarXiv:2401.09986(2024).\nprivacy(sp).Ieee,39–57.\n[27] AngLi,JingweiSun,XiaoZeng,MiZhang,HaiLi,andYiranChen.2021.\n[8] AdamCoates,AndrewNg,andHonglakLee.2011. Ananalysisof\nFedmask:Jointcomputationandcommunication-efficientpersonalized\nsingle-layernetworksinunsupervisedfeaturelearning.InProceedings\nfederatedlearningviaheterogeneousmasking.InProceedingsofthe\nofthefourteenthinternationalconferenceonartificialintelligenceand\n19thACMConferenceonEmbeddedNetworkedSensorSystems.42–55.\nstatistics.JMLRWorkshopandConferenceProceedings,215–223.\n[28] YuezunLi,YimingLi,BaoyuanWu,LongkangLi,RanHe,andSiwei\n[9] LiDeng.2012. Themnistdatabaseofhandwrittendigitimagesfor\nLyu.2021.Invisiblebackdoorattackwithsample-specifictriggers.In\nmachinelearningresearch. IEEESignalProcessingMagazine29,6\nProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision.\n(2012),141–142.\n16463–16472.\n[10] YonghengDeng,ShengYue,TuoweiWang,GuanboWang,JuRen,and\n[29] YijingLi,XiaofengTao,XuefeiZhang,JunjieLiu,andJinXu.2021.\nYaoxueZhang.2023.FedINC:AnExemplar-FreeContinualFederated\nPrivacy-preservedfederatedlearningforautonomousdriving.IEEE\n13\nConference’17,July2017,Washington,DC,USA KichangLee,YujinShin,JonghyukYun,JunHan,andJeongGilKo\nTransactionsonIntelligentTransportationSystems23,7(2021),8423– DecisionBoundary.arXivpreprintarXiv:2402.17465(2024).\n8434. [42] QiSun,ArjunAshokRao,XufengYao,BeiYu,andShiyanHu.2020.\n[30] Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, and Shu-Tao Counteractingadversarialattacksinautonomousdriving.InProceed-\nXia.2021. Backdoorattackinthephysicalworld. arXivpreprint ingsofthe39thInternationalConferenceonComputer-AidedDesign.\narXiv:2104.02361(2021). 1–7.\n[31] BrendanMcMahan,EiderMoore,DanielRamage,SethHampson,and [43] ZitengSun,PeterKairouz,AnandaTheerthaSuresh,andHBrendan\nBlaiseAguerayArcas.2017. Communication-efficientlearningof McMahan.2019.Canyoureallybackdoorfederatedlearning?arXiv\ndeepnetworksfromdecentralizeddata.InArtificialintelligenceand preprintarXiv:1911.07963(2019).\nstatistics.PMLR,1273–1282. [44] ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,\n[32] XiaominOuyang,ZhiyuanXie,HemingFu,SitongCheng,LiPan, DumitruErhan,IanGoodfellow,andRobFergus.2013. Intriguing\nNeiwenLing,GuoliangXing,JiayuZhou,andJianweiHuang.2023. propertiesofneuralnetworks.arXivpreprintarXiv:1312.6199(2013).\nHarmony:HeterogeneousMulti-ModalFederatedLearningthrough [45] CanhTDinh,NguyenTran,andJoshNguyen.2020.Personalizedfed-\nDisentangledModelTraining.InProceedingsofthe21stAnnualInterna- eratedlearningwithmoreauenvelopes.Advancesinneuralinformation\ntionalConferenceonMobileSystems,ApplicationsandServices(Helsinki, processingsystems33(2020),21394–21405.\nFinland)(MobiSys’23).AssociationforComputingMachinery,New [46] LaurensVanderMaatenandGeoffreyHinton.2008.Visualizingdata\nYork,NY,USA,530–543. https://doi.org/10.1145/3581791.3596844 usingt-SNE.Journalofmachinelearningresearch9,11(2008).\n[33] XiaominOuyang,ZhiyuanXie,JiayuZhou,GuoliangXing,andJianwei [47] BolunWang,YuanshunYao,ShawnShan,HuiyingLi,BimalViswanath,\nHuang.2022.Clusterfl:Aclustering-basedfederatedlearningsystem HaitaoZheng,andBenYZhao.2019. Neuralcleanse:Identifying\nforhumanactivityrecognition.ACMTransactionsonSensorNetworks andmitigatingbackdoorattacksinneuralnetworks.In2019IEEE\n19,1(2022),1–32. symposiumonsecurityandprivacy(SP).IEEE,707–723.\n[34] JaeyeonPark,HyeonCho,RajeshKrishnaBalan,andJeongGilKo. [48] ShaokuiWei,MingdaZhang,HongyuanZha,andBaoyuanWu.2023.\n2020.HeartQuake:AccurateLow-CostNon-InvasiveECGMonitoring Sharedadversarialunlearning:Backdoormitigationbyunlearning\nUsingBed-MountedGeophones.Proc.ACMInteract.Mob.Wearable sharedadversarialexamples.AdvancesinNeuralInformationProcessing\nUbiquitousTechnol.4,3,Article93(sep2020),28pages. Systems36(2023),25876–25909.\n[35] JaeYeonParkandJeongGilKo.2024. FedHM:Practicalfederated [49] Cheng-HsinWeng,Yan-TingLee,andShan-HungBrandonWu.2020.\nlearningforheterogeneousmodeldeployments. ICTExpress10,2 Onthetrade-offbetweenadversarialandbackdoorrobustness. Ad-\n(2024),387–392. vances in Neural Information Processing Systems 33 (2020), 11973–\n[36] JaeYeonPark,KichangLee,SungminLee,MiZhang,andJeongGilKo. 11983.\n2023.AttFL:APersonalizedFederatedLearningFrameworkforTime- [50] EmilyWenger,JosephinePassananti,ArjunNitinBhagoji,Yuanshun\nseriesMobileandEmbeddedSensorDataProcessing.Proceedingsof Yao,HaitaoZheng,andBenYZhao.2021.Backdoorattacksagainst\ntheACMonInteractive,Mobile,WearableandUbiquitousTechnologies deeplearningsystemsinthephysicalworld.InProceedingsofthe\n7,3(2023),1–31. IEEE/CVFconferenceoncomputervisionandpatternrecognition.6206–\n[37] KrishnaPillutla,ShamMKakade,andZaidHarchaoui.2022.Robustag- 6215.\ngregationforfederatedlearning.IEEETransactionsonSignalProcessing [51] ChulinXie,KeliHuang,Pin-YuChen,andBoLi.2019.Dba:Distributed\n70(2022),1142–1154. backdoorattacksagainstfederatedlearning.InInternationalconference\n[38] ShivaRajPokhrelandJinhoChoi.2020. Adecentralizedfederated onlearningrepresentations.\nlearningapproachforconnectedautonomousvehicles.In2020IEEE [52] DezhongYao,WanningPan,MichaelJO’Neill,YutongDai,YaoWan,\nWirelessCommunicationsandNetworkingConferenceWorkshops(WC- HaiJin,andLichaoSun.2021. Fedhm:Efficientfederatedlearning\nNCW).IEEE,1–6. forheterogeneousmodelsvialow-rankfactorization.arXivpreprint\n[39] LemingShen,QiangYang,KaiyanCui,YuanqingZheng,Xiao-Yong arXiv:2111.14655(2021).\nWei,JianweiLiu,andJinsongHan.2024. FedConv:ALearning-on- [53] DongYin,YudongChen,RamchandranKannan,andPeterBartlett.\nModelParadigmforHeterogeneousFederatedClients.InProceedings 2018. Byzantine-robustdistributedlearning:Towardsoptimalsta-\nofthe22ndAnnualInternationalConferenceonMobileSystems,Appli- tisticalrates.InInternationalconferenceonmachinelearning.Pmlr,\ncationsandServices.398–411. 5650–5659.\n[40] YujinShin,KichangLee,SungminLee,YouRimChoi,Hyung-SinKim, [54] JonghyukYun,KyoosikLee,KichangLee,BangjieSun,JaehoJeon,\nandJeongGilKo.2024.EffectiveHeterogeneousFederatedLearning JeonggilKo,InseokHwang,andJunHan.2024.PowDew:Detecting\nviaEfficientHypernetwork-basedWeightGeneration.InProceedings CounterfeitPowderedFoodProductsusingaCommoditySmartphone.\nofthe22ndACMConferenceonEmbeddedNetworkedSensorSystems. InProceedingsofthe22ndAnnualInternationalConferenceonMobile\n112–125. Systems,ApplicationsandServices.210–222.\n[41] YanghaoSu,JieZhang,TingXu,TianweiZhang,WeimingZhang, [55] BoZhao,PengSun,TaoWang,andKeyuJiang.2022.Fedinv:Byzantine-\nandNenghaiYu.2024.ModelX-ray:DetectBackdooredModelsvia robustfederatedlearningbyinversinglocalmodelupdates.InProceed-\ningsoftheAAAIConferenceonArtificialIntelligence,Vol.36.9171–9179.\n14",
    "pdf_filename": "DeTrigger_A_Gradient-Centric_Approach_to_Backdoor_Attack_Mitigation_in_Federated_Learning.pdf"
}