{
    "title": "Contrast Similarity-Aware Dual-Pathway Mamba for Multivariate Time Series Node Classification",
    "abstract": "Multivariate time series (MTS) data is generated through multiple sen- sors across various domains such as engineering application, health mon- itoring, and the internet of things, characterized by its temporal changes and high dimensional characteristics. Over the past few years, many studies have explored the long-range dependencies and similarities in MTS. However, long-range dependencies are difficult to model due to their temporal changes and high dimensionality makes it difficult to obtain similarities effectively and efficiently. Thus, to address these issues, we propose contrast similarity- aware dual-pathway Mamba for MTS node classification (CS-DPMamba). Firstly, to obtain the dynamic similarity of each sample, we initially use temporal contrast learning module to acquire MTS representations. And then we construct a similarity matrix between MTS representations using Fast Dynamic Time Warping (FastDTW). Secondly, we apply the DPMamba to consider the bidirectional nature of MTS, allowing us to better capture long-range and short-range dependencies within the data. Finally, we uti- lize the Kolmogorov-Arnold Network enhanced Graph Isomorphism Network to complete the information interaction in the matrix and MTS node clas- sification task. By comprehensively considering the long-range dependen- cies and dynamic similarity features, we achieved precise MTS node classi- ∗Corresponding author Email addresses: mingsendu@mail.sdu.edu.cn (Mingsen Du), chchenmeng@gmail.com (Meng Chen), lyj4072021@163.com (Yongjian Li), xiuxinzhang@mail.sdu.edu.cn (Xiuxin Zhang), gaojiahui@mail.sdu.edu.cn (Jiahui Gao), jicun@sdnu.edu.cn (Cun Ji), sswei@sdu.edu.cn (Shoushui Wei) Preprint submitted to Engineering Applications of Artificial Intelligence November 20, 2024 arXiv:2411.12222v1  [cs.LG]  19 Nov 2024",
    "body": "Contrast Similarity-Aware Dual-Pathway Mamba for\nMultivariate Time Series Node Classification\nMingsen Dua, Meng Chena, Yongjian Lia, Xiuxin Zhanga, Jiahui Gaoa, Cun\nJib,∗, Shoushui Weia,∗\naSchool of Control Science and Engineering, Shandong University, Jinan, China\nbSchool of Information Science and Engineering, Shandong Normal University, Jinan,\nChina\nAbstract\nMultivariate time series (MTS) data is generated through multiple sen-\nsors across various domains such as engineering application, health mon-\nitoring, and the internet of things, characterized by its temporal changes\nand high dimensional characteristics. Over the past few years, many studies\nhave explored the long-range dependencies and similarities in MTS. However,\nlong-range dependencies are difficult to model due to their temporal changes\nand high dimensionality makes it difficult to obtain similarities effectively\nand efficiently. Thus, to address these issues, we propose contrast similarity-\naware dual-pathway Mamba for MTS node classification (CS-DPMamba).\nFirstly, to obtain the dynamic similarity of each sample, we initially use\ntemporal contrast learning module to acquire MTS representations.\nAnd\nthen we construct a similarity matrix between MTS representations using\nFast Dynamic Time Warping (FastDTW). Secondly, we apply the DPMamba\nto consider the bidirectional nature of MTS, allowing us to better capture\nlong-range and short-range dependencies within the data. Finally, we uti-\nlize the Kolmogorov-Arnold Network enhanced Graph Isomorphism Network\nto complete the information interaction in the matrix and MTS node clas-\nsification task. By comprehensively considering the long-range dependen-\ncies and dynamic similarity features, we achieved precise MTS node classi-\n∗Corresponding author\nEmail addresses: mingsendu@mail.sdu.edu.cn (Mingsen Du),\nchchenmeng@gmail.com (Meng Chen), lyj4072021@163.com (Yongjian Li),\nxiuxinzhang@mail.sdu.edu.cn (Xiuxin Zhang), gaojiahui@mail.sdu.edu.cn (Jiahui\nGao), jicun@sdnu.edu.cn (Cun Ji), sswei@sdu.edu.cn (Shoushui Wei)\nPreprint submitted to Engineering Applications of Artificial Intelligence November 20, 2024\narXiv:2411.12222v1  [cs.LG]  19 Nov 2024\n\nfication. We conducted experiments on multiple University of East Anglia\n(UEA) MTS datasets, which encompass diverse application scenarios. Our\nresults demonstrate the superiority of our method through both supervised\nand semi-supervised experiments on the MTS classification task.\nKeywords:\nMultivariate Time Series Classification, Time Series Similarity,\nMamba, Representation Learning, Graph Neural Network\n1. Introduction\nIn recent years, significant advancements have been made in the field\nof time series analysis, driven by the growing availability of complex high-\ndimensional data from various sources such as sensor networks [1], finan-\ncial markets [2], and biomedical applications [3]. Accurately capturing and\nanalyzing these time series data is crucial for applications such as activity\nrecognition [4], anomaly detection [5], and predictive modeling [6].\nAmong them, MTS classification is a significant research topic with ap-\nplications in various fields, such as human activity recognition [4], leading to\nnumerous studies in recent years [7, 8]. The focus of attention has been on\nthe similarities and long-range dependencies inherent in MTS data [9].\nHowever, modeling MTS data presents challenges due to temporal changes\nthat complicate the capture of long-range dependencies, as well as high di-\nmensional characteristics that hinder the identification of similarities between\nsamples.\nTo address the above two challenges, various methods have been pro-\nposed. For similarities extraction challenge between time series, DTW\nand its variants focus on measuring the dynamic similarities between time\nseries through nonlinear alignment.\nWhile effective in capturing tempo-\nral alignment, these methods often exhibit limitations when handling high-\ndimensional data, particularly in terms of expensive time complexity, which\nis O(mn2)(Here, m is the dimension of the MTS and n is the length). This\nmeans that when processing long sequences and high-dimensional data, the\ncomputational overhead of DTW can become prohibitively large, leading to\ninefficiency. Consequently, numerous methods, such as indexing (Time Series\nIndexing) [10], sparsification [11], lower bounding [12], and constraint path\n[13] have been proposed to reduce complexity and improve computational ef-\nficiency. Another difficulty is how to align the temporal dynamics to obtain\nmore accurate similarity. When using DTW to calculate high-dimensional\n2\n\nMTS, the dimensions are usually added together to calculate the average,\nthus losing various key features of each dimension.\nFor long-range dependencies extraction challenge, deep learning meth-\nods have recently made significant progress recently. Long Short-Term Mem-\nory networks (LSTM) [14] effectively capture long-term dependencies in time\nseries. However, LSTM models may face challenges such as high computa-\ntional complexity and overfitting when dealing with long time series. Another\nimportant development is the Transformer model [15, 16, 17, 18], whose self-\nattention mechanism makes it possible to capture global dependencies. While\nTransformer suffer from quadratic complexity, leading to low computational\nefficiency and high costs. Recently, Selective State Space Models (SSM) like\nMamba [19, 20] becomes popular, which can handle long-range dependen-\ncies in sequences while maintaining near-linear complexity, have garnered\nwidespread attention. Numerous works have emerged in the fields of time\nseries [20] [21], images [22], graph data [23], and natural language [24]. SSM\neffectively capture the dynamic properties of time series using state transi-\ntion equations to describe system evolution over time. Their ability to utilize\nhidden states allows SSMs to simultaneously capture long-range dependen-\ncies across multiple time points, making them ideal for modeling delays and\nmemory effects in time-dependent data.\nAfter getting similarities and long-range dependencies, to com-\nbine both is another key challenge. In recent years, Graph Neural Net-\nworks (GNN) [25, 26] have found extensive application in the field of time\nseries analysis, particularly in handling complex dependencies within graph-\nstructured data involving multiple time series dimensions. In time series data,\nGNN can combine temporal and structural features to achieve more accu-\nrate modeling and prediction. Examples include GIN [27], GCN [28], GAT\n[29], Spatial-Temporal GNN [30], and GraphSAGE [31]. However, many of\nthese methods model either static [32] or dynamic dependencies [33] between\ndimensions. While current time series GNN are often used to obtain feature\nrepresentations of time series.\nTherefore, to combine similarities and long-range dependencies, each sam-\nple in the data set can be regarded as a node in the graph. The edge relation-\nship between nodes can be modeled using similarities, and the node features\ncan be modeled using long-range dependencies. Recently, several researches\n[34, 35] combine similarities based edge relationship and node features to\ncomplete node classification.\nIn response to the above three challenges, we propose CS-DPMamba for\n3\n\nMTS node classification. Firstly, to obtain the similarity of each sample in\nthe dataset, we initially used TemCL to acquire time series representations\nand then constructed a similarity matrix using FastDTW. Then, we apply\nDPMamba model, which considers the bidirectional nature of time series,\nallowing us to better capture both long-range and short-range dependencies\nin the time series. Finally, we utilize the KAN-GIN network for MTS node\nclassification. By comprehensively considering the long-range dependencies\nand dynamic similarity features, we achieve more accurate MTS node clas-\nsification.\nThe main contributions of this study include:\n1. We propose contrast similarity-aware dual-pathway Mamba for MTS\nnode classification and conduct experiments on the UEA dataset, demon-\nstrating the superiority.\n2. We use temporal contrastive learning to acquire time series representa-\ntions and then constructed a similarity matrix between representations\nusing FastDTW, capturing the dynamic similarities of the time series.\n3. We use dual-pathway Mamba to extract high-level features from bidi-\nrectional time series while effectively managing complex data, capturing\nboth long-range and short-range dependencies.\n2. Related work\n2.1. Mamba-Based Methods\nMamba was proposed [19] to address weaknesses in discrete modes by\nsetting SSM parameters as input functions, allowing the model to selectively\npropagate or forget information based on the current token.\nMamba in-\ntegrates these selective SSMs to construct a simplified end-to-end neural\nnetwork architecture that does not use attention or Multi-Layer Perceptron\n(MLP) block. The architecture exhibits linear scalability and can handle real-\nworld data sequences up to millions in length. Recently, many Mamba-based\nmethods have been proposed.\nTSCMamba [21] introduces a novel multi-\nview approach that fuses frequency-domain and time-domain features using\nspectral features from continuous wavelet transforms. It utilizes State Space\nModels (SSMs) for efficient and scalable sequence modeling, incorporating a\nunique tango scanning scheme to enhance sequence relationship modeling.\nC-Mamba [36] employs a newly developed SSM to capture cross-channel\n4\n\ndependencies while maintaining linear complexity and preserving a global\nreceptive field. It integrates channel mixing to enhance the training set by\ncombining two channels, alongside a channel attention-enhanced Mamba en-\ncoder that leverages SSM to model cross-time dependencies and inter-channel\ncorrelations. FMamba [37] first extracts temporal features of input variables\nthrough an embedding layer, then calculates dependencies between input\nvariables with a fast attention module. Finally, Mamba selectively processes\nthe input features, and a MLP block further extracts temporal dependencies\nof the variables.\n2.2. Graph-Based Methods\nGNN [38] can effectively capture complex relationships between different\ndimensions in time series by representing these dimensions and their inter-\nactions through graph structures.\nRecently, many GNN-based methods have been proposed. Many mod-\nels overlook seasonal effects and the evolving characteristics of shapelets.\nTime2Graph++ [39] addresses this by introducing a time-level attention\nmechanism to extract time-aware shapelets. Subsequently, time series data\nis transformed into shapelet evolution graphs to capture evolving patterns,\nfurther enhanced by graph attention to improve shapelet evolution. Exist-\ning methods (e.g., Transformer) struggle to effectively utilize spatial correla-\ntions between variables. To address this, Graphformer [40] efficiently learns\ncomplex temporal patterns and dependencies across multiple variables. It\nautomatically infers an implicit sparse graph structure through a graph self-\nattention mechanism. Current methods mainly focus on temporal consis-\ntency, neglecting the importance of spatial consistency. TS-GAC [41] aims\nto enhance the spatial consistency of MTS data. Specifically, it introduces\ngraph augmentation techniques for nodes and edges to maintain the stabil-\nity of sensors and their associations. Subsequently, robust sensor and global\nfeatures are extracted through node- and graph-level contrast. Graph net-\nworks can only capture spatiotemporal dependencies between node pairs and\ncannot handle higher-order correlations in time series. DHSL [42] uses the\nk-nearest neighbor method to generate a dynamic hypergraph structure from\ntime series data and optimizes the structure through a hyper graph struc-\nture learning to capture higher-order correlations. Finally, the dynamically\nlearned hypergraph structure is applied to a spatiotemporal hyper GNN.\nRecently, several research [34] treats time series as nodes in a graph, with\nnode similarity calculated using DTW and incorporated into the GNN. Due\n5\n\nto the high complexity of DTW, further research [35] uses the lower bound\nof DTW to calculate similarity, reducing the time complexity. However, the\nlower bound provides only an estimate, not an exact distance. This means\nthat in some cases, the final similarity measure may not be accurate enough,\nespecially in applications requiring high precision.\n3. Methods\n3.1. CS-DPMamba Overview\nThe overview involves the following steps, as shown in Fig. 1. The batch\ntraining with negative sampling is shown in Algorithm 1.\n• Compute similarity matrix through ContrastFastDTW (in Section 3.2,\nStep 1 in Algorithm 1): Use the TemCL module to get time series rep-\nresentations and FastDTW to compute the similarity matrix between\nrepresentations based on the sampled time-series.\n• Extract long-range dependencies through DPMamba (in Section 3.3,\nStep 2 in Algorithm 1): Apply the DPMamba method to extract long-\nrange dependencies from the MTS data. This step captures the rela-\ntionships between different time-series data points.\n• Perform node classification (in Section 3.4, Step 3 in Algorithm 1):\nUse the KAN-GIN to perform node classification based on long-range\ndependencies features and similarity matrix.\n• Model update (Step 4 in Algorithm 1): Finally, update the model\nparameters using the processed batch to improve the model’s accuracy\nover multiple iterations.\n6\n\nFastDTW\nEarly Stopping \n0.00 0.23 -1.00 \n...\n...\n0.25 \n0.23 0.00 -0.72 \n...\n...\n0.05 \n-1.00 0.72 0.00 \n...\n...\n0.180 \n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n0.25 0.05 0.18 \n...\n...\n0.00 \nNode Features\nSimilarity\nMatrix\nTemCL\nDataset\n······\n······\nNew \nRepresentation\nMamba \nbackward \nMamba \n long-term \ndependency\nKAN-GIN\nlabel6\nlabel5\nlabel4\nlabel3\nlabel2\nlabel1\nNode\nEmbedding\nDPMamba\nContrastFastDTW\nFigure 1: Overview of CS-DPMamba.\n7\n\nAlgorithm 1 Batch Training with Negative Sampling\nRequire: MTS dataset, B: Batch size, E: Number of epochs, Xunlabeled:\nUnlabeled time series dataset, Xlabeled: Labeled time series dataset, XB:\nTime series of current batch\nsimilarity matrix ←ContrastFastDTW(dataset)\n▷Step 1: Obtain\nsimilarity matrix by Algorithm 2\nfor each epoch from 1 to E do\nfor each training iteration do\nSample B/2 labeled time series from Xlabeled\n▷Supervised\ntraining: Sample from labeled data\nSample B/2 unlabeled time series from Xunlabeled\n▷\nSemi-supervised training: Sample from unlabeled data\nConstruct a graph using the XB and get topk neighbors ▷Create a\ngraph for current batch\ntemporal features ←DPMamba(XB)\n▷Step 2: Extract node\nfeatures\nprediction ←KAN-GIN(similarity matrix, node features) ▷Step\n3: Execute node classification\nUpdate the model parameters\n▷Use the batch to update the\nmodel\n▷Step 4: Parameters update\nend for\nend for\nreturn prediction\n3.2. ContrastFastDTW based Similarity Matrix Constructing\nFirst, to correctly align temporal dynamics and obtain more accurate\nsimilarity, we use TemCL module to obtain MTS representations (as shown\nin Fig. 2). And then, to obtain similarity more efficiently, we construct a\nsimilarity matrix between MTS representations using FastDTW, capturing\nthe dynamic similarities of time series.\nThe similarity matrix is computed according to the following steps, de-\ntailed as shown in Algorithm 2 and Algorithm 3.\n• Obtain MTS Representations (step 1 in Algorithm 2): Use temporal\ncontrastive learning to obtain representations of the time series data.\nThis step captures the essential features of the time series.\n8\n\n• Clustering (step 2 in Algorithm 2): Apply KNN clustering algorithm\nto get clusters based on the number of classes.\n• Compute similarities within clusters (Algorithm 3 and step 3 in Al-\ngorithm 2): Calculate the distance between the representations of the\nMTS using FastDTWx. If they are not in the same cluster, they are\nconsidered dissimilar and a value of -1 is assigned.\n3.2.1. TemCL\nWe design temporal contrast learning module (TemCL) by leveraging self-\nsupervised learning techniques, enhances the model’s robustness to variations\nand noise in the time series, improves generalization performance by learn-\ning meaningful representations that can be used across different tasks. The\nTemCL model architecture is shown in Fig. 2. We extract temporal features\nby stacking multiple layers of convolution, ReLU, and max pooling modules,\nand construct negative samples by adding noise and cutting time series.\nTemCL module has the following advantages. First, the convolutional\nlayer can effectively extract key features in the time series through the local\nreceptive field mechanism. This feature extraction method can capture short-\nterm and long-term pattern changes. Second, the pooling layer reduces the\ndimension of the data by downsampling, thereby reducing the computational\ncomplexity. Finally, the convolutional pooling structure is robust to noise\nand interference. Therefore, TemCL makes the similarity measurement more\naccurate by optimizing the feature representation.\nDataset\n······\nAug1\nAug3\nEncoder\nzi\nzj\nSim(zi , zj)\nContrast \nLoss\nFlatten\nConv1D\nReLU\nMaxPool1d\nConv1D\nReLU\nMaxPool1d\n·····\nEncoder\nFigure 2: Contrast learning model.\n9\n\nAlgorithm 2 ContrastFastDTW Matrix\nRequire: Dataset list: D, target dimension: dtarget, number of epochs: E,\nbatch size: B, the number of class: class\n1: for each dataset D ∈D do\n2:\nif pre-trained model exists then\n3:\nLoad pre-trained model M\n▷Load existing pre-trained model\n4:\nelse\n5:\nM ←ContrastLearning(D)\n6:\nTrain M with data D using contrastive loss L\n▷Step 1: Train\nmodel using contrastive loss\n7:\nend if\n8:\nTransform data D →ˆD by passing it through model M\n▷Get\nrepresentations through the model\n9:\nCk ←KNN Cluster( ˆD, class)\n▷Step 2: Use KNN clustering to\nget clusters if using early stopping\n10:\nDTWmatrix ←FastDTWx( ˆD, Ck) ▷Step 3: Compute the DTW\nmatrix by Algorithm 3\n11: end for\n12: function ContrastiveLearning(D, E, B)\n13:\nInitialize model M\n14:\nfor epoch e ∈{1, . . . , E} do\n15:\nfor batch b ∈{1, . . . , B} do\n16:\nGet positive samples Dp from batch b\n17:\nGenerate\nnegative\nsamples\nXn\n←\ngenerate negative samples(Dp) ▷Generate negative samples from\npositive samples\n18:\nDefine labels y with yi ←1 for positive pairs, yi ←0 for\nnegative pairs\n▷Label positive and negative pairs\n19:\nForward pass: Zp ←M(Dp), Zn ←M(Dn)\n▷Compute\nrepresentations for positive and negative samples\n20:\nCompute loss L ←(1 −y) · ||Zp −Zn||2 + y · max(m −||Zp −\nZn||, 0)2\n▷Calculate contrastive loss\n21:\nend for\n22:\nend for\n23:\nReturn M\n▷Return the trained model\n24: end function\n10\n\nAlgorithm 3 FastDTWx\n1: function\nFastDTWx(representations= ˆD,\nclusters=Ck,\nradius=1,\ndist=Euclidean)\n2:\nsimilarity matrix ←zeros(N, N)\n▷Initialize similarity matrix\n3:\nfor each cluster Ck in classes do\n4:\nfor each sample i ∈Ck do\n5:\nfor each sample j ∈Ck do ▷Only compute within the cluster\n6:\ndistance ←FastDTW(dataset[i], dataset[j], radius, dist)\n▷Compute similarity between samples\n7:\nsimilarity matrix[i][j] ←distance\n8:\nend for\n9:\nend for\n10:\nend for\n11:\nfor each sample i not in any cluster do\n12:\nfor each sample j in all other clusters do\n13:\nsimilarity matrix[i][j] ←−1\n▷Assign dissimilarity if not in\nthe same cluster\n14:\nend for\n15:\nend for\n16:\nreturn similarity matrix\n▷Return similarity matrix\n17: end function\n18: function FastDTW(x, y, radius, dist)\n19:\nif length of x or y ¡ (radius + 2) then\n20:\nreturn DTW(x, y, dist)\n21:\nend if\n22:\nx shrinked, y shrinked ←ReduceByHalf(x), ReduceByHalf(y)\n23:\ndistance, path ←FastDTW(x shrinked, y shrinked, radius, dist)\n24:\nreturn distance\n25: end function\n11\n\nThe contrastive loss can be defined as Eq. 1.\nL = 1\nN\nN\nX\ni=1\n\u0000yi · d(fReLU(p(Conv(xi))), p(Conv(xj)))2\u0001\n+ 1\nN\nN\nX\ni=1\n\u0000(1 −yi) · max(0, m −d(fReLU(p(Conv(xi))), p(Conv(xj))))2\u0001\n(1)\nIn Eq. 1, N: Total number of training pairs. yi: Binary label indicating\nsimilarity (yi = 1) or dissimilarity (yi = 0) of pair (xi, xj). d(f(xi), f(xj)):\nDistance metric between feature representations of xi and xj. m: Margin\ndefining the minimum distance between dissimilar pairs. The function fReLU\nincludes convolution (Conv), pooling (p), and ReLU activation. By optimiz-\ning this loss function, we effectively learn representations of time series.\nFor generating negative samples Xnegative of time series as Eq. 2 - Eq. 3.\nXnegative = {x′\ni = xi + ni | ni ∼N(0, σ2), i = 1, 2, . . . , N}\n(2)\nIn Eq. 2, xi: Positive samples. ni: Noise generated from a Gaussian\ndistribution with mean 0 and variance σ2.\nXnegative = {x′\ni = xi[t1 : t2] | t1, t2 are non-overlapping, i = 1, 2, . . . , N} (3)\nIn Eq. 3, x′\ni: Generated negative sample from positive sample xi. t1, t2:\nStart and end indices of the time window (non-overlapping). i: Index of\npositive samples, where i = 1, 2, . . . , N.\n3.2.2. Pairwise Distance Matrix for MTS\nDTW is an algorithm used to measure the similarity between two time\nseries signals. It allows for nonlinear time alignment, enabling the optimal\nmatching of sequences even when they differ in speed or have temporal dis-\ncrepancies. The DTW calculation follows the following steps:\na) DTW Formulation\nGiven two time series X = (x1, x2, . . . , xn) and Y = (y1, y2, . . . , ym),\nwhere xi and yj are elements of the series, DTW aims to find a nonlinear\nalignment that minimizes the alignment cost.\n12\n\nb) Distance Matrix Calculation\nFirst, compute the distance matrix D as Eq. (4), where each element\nD(i, j) represents the distance between the i-th element of X and the\nj-th element of Y . In Eq. (4), dist(xi, yj) is typically the Euclidean\ndistance or another distance metric.\nD(i, j) = dist(xi, yj)\n(4)\nc) Recursive Computation\nThe distance DTW(X, Y ) is then recursively calculated using dynamic\nprogramming as Eq. (5).\nDefine the cumulative distance matrix C,\nwhere C(i, j) represents the minimum cumulative distance from the\nstart point (1, 1) to (i, j).\nC(i, j) = dist(xi, yj) + min\n\n\n\n\n\nC(i −1, j),\n(vertical move)\nC(i, j −1),\n(horizontal move)\nC(i −1, j −1)\n(diagonal move)\n(5)\nThe initial condition is Eq. (6). The final DTW distance is C(n, m),\nrepresenting the minimum cumulative distance from the start of series\nX to the end of series Y .\nC(1, 1) = dist(x1, y1)\n(6)\nd) FastDTW\nFastDTW [13] is an efficient algorithm for computing time series sim-\nilarity, reducing the traditional O(N 2) complexity of DTW to O(N)\nthrough a multiresolution approximation.\n3.2.3. FastDTW based Matrix\nLet X = {x1, x2, . . . , xN} be a dataset of N MTS, where each xi ∈RT×N\nrepresents a time series with T time steps and N features. The FastDTW\ndistance between xi and xj is denoted as FastDTW(xi, xj). We define the\npairwise distance matrix D ∈RN×N as Eq. 7.\n13\n\nDij = FastDTW(xi, xj),\nfor i, j = 1, 2, . . . , N\n(7)\nThe matrix D is a symmetric matrix where each entry Dij represents the\nFastDTW distance between time series xi and xj as Eq. 8.\nD =\n\n\n\n\n\n0\nFastDTW(x1, x2)\n. . .\nFastDTW(x1, xN)\nFastDTW(x2, x1)\n0\n. . .\nFastDTW(x2, xN)\n...\n...\n...\n...\nFastDTW(xN, x1)\nFastDTW(xN, x2)\n. . .\n0\n\n\n\n\n\n(8)\nIn Eq. 8, Dij is the FastDTW distance between time series xi and xj. If\ni = j, Dii = 0, since the distance between a time series and itself is zero. D:\nA symmetric matrix where Dij = Dji, representing all pairwise FastDTW\ndistances among the time series in the dataset.\n3.2.4. Final Similarity Matrix\nAfter getting the FastDTW matrix D, to further describe the similarity,\nwe perform the following processing. First, we introduce a scaling hyperpa-\nrameter α ∈[0, ∞) to control the importance of top neighbors. Specifically,\nlet Dij denote the (i, j) entry of D. The adjacency matrix A is obtained by\nthe following formula as Eq. 9.\nAij =\n1\neαDij ,\n∀i, j\n(9)\nwhere Aij represents the (i, j) entry of A.\nA larger α will give more\nimportance to the top neighbors.\nNext, to filter out irrelevant neighbors, we sample the top K neighbors for\neach node. Specifically, for each row ai in A, we keep only the K entries with\nthe largest weights and set the others to zero, resulting in a sparse matrix.\nWhen handling similarity, we define a similarity function as Eq. 10.\nsimilarityij =\n\n\n\n\n\n1\nif Aij = 0\n1\n1+|Aij|\nif Aij > 0\n0\nif Aij < 0\n(10)\nWhen Aij = 0, the similarity is 1, indicating maximum similarity. When\nAij > 0, the value is calculated using\n1\n1+|Aij|, which means that when Aij\n14\n\nis close to 0, similarity approaches 1, while larger values of Aij will lead to\nsimilarity approaching 0. When Aij < 0, the similarity is directly set to 0,\nindicating complete dissimilarity.\nFinally, we normalize the adjacency matrix using the following formula\nas Eq. 11.\n˜Aij =\nAij\nP\nj′ Aij′ ,\n∀i, j\n(11)\nIn the context of a GNN for node classification, Each MTS xi is repre-\nsented as a node in a graph. The matrix D defines the edge weights between\nnodes, where a smaller DTW distance indicates a stronger similarity and\nthus a stronger edge connection between nodes i and j. The GNN can use\nthis matrix as input to perform node classification based on the structural\nproperties of the graph formed by these time series.\n3.3. Dual-Pathway Processing Using Mamba Model\nWe apply the dual-pathway Mamba (DPMamba, as shown in Algorithm. 4\nand Fig. 3) to consider the bidirectional nature of MTS, allowing us to better\ncapture long-range and short-range dependencies.\nDPMamba captures temporal dependencies in both forward and back-\nward directions, providing a more comprehensive modeling of time series\ndata. The combined output from both directions can improve the accuracy\nof predictions, especially in cases where future data points influence past\nstates.\n3.3.1. State Evolution in Forward Direction\nGiven a MTS x(t) as input, the continuous-time state evolution is de-\nscribed as Eq. (12).\nh′(t) = Ah(t) + Bx(t),\ny(t) = Ch(t)\n(12)\nwhere h(t) represents the hidden state at time t, x(t) is the input vector\nat time t, A is the state transition matrix, B maps the input x(t) to state\nupdates, C maps the hidden state h(t) to the output y(t).\nAfter discretization, the formula for the forward direction as Eq. (13).\nht = Aht−1 + Bxt,\nyt = Cht\n(13)\n15\n\nLinear\nsilu\nSSM\nConv\nsilu\nLinear\nLinear\nNonlineartiy\nFigure 3: Structure of Mamba.\nAlgorithm 4 DPMamba\n1: function DPMamba(x(t))\n2:\nInitialize matrices A, B, C\n3:\nInitialize hidden states h0 and hR\nT\n▷Forward Processing\n4:\nfor t = 1 to T do\n5:\nht ←A · ht−1 + B · xt\n▷Update hidden state\n6:\nyt ←C · ht\n▷Compute output\n7:\nend for\n▷Reverse Processing\n8:\nfor t = T down to 1 do\n9:\nhR\nt ←A · hR\nt+1 + B · xR\nt\n▷Update reverse hidden state\n10:\nyR\nt ←C · hR\nt\n▷Compute reverse output\n11:\nend for\n▷Combine Dual-Pathway Outputs\n12:\nfor t = 1 to T do\n13:\nyfinal\nt\n←α · yt + β · yR\nt\n▷Combine outputs\n14:\nend for\n15:\nreturn yfinal\n▷Return final output\n16: end function\n16\n\n3.3.2. State Evolution in Reverse Direction\nFor reverse processing, we consider the time-reversed sequence xR(t),\nwhere xR(t) = x(T −t). The state evolution for the reverse direction as\nas Eq. (14). This reverse state evolution captures dependencies from future\nto past, complementing the forward processing.\nhR\nt = AhR\nt+1 + BxR\nt ,\nyR\nt = ChR\nt\n(14)\n3.3.3. Combined Bidirectional Output\nThe final output is a combination of the forward and reverse outputs as\nEq. (15).\nyfinal\nt\n= αyt + βyR\nt\n(15)\nwhere α and β are coefficients that balance the influence of forward and\nreverse directions, yfinal\nt\nis the final output at time t after considering both\ndirections.\n3.4. KAN-GIN Layer\nWe utilize the KAN to enhance the expressive power of the GIN for node\nclassification.\nAlthough MLP are flexible in handling feature data, they\nfall short in capturing the complex dynamic characteristics of time series. In\ncontrast, the KAN enhances expressive power, allowing for better integration\nof temporal dependencies and graph structure features in time series, thereby\nimproving the accuracy of node classification.\nAfter obtaining the time series similarity matrix and the node features\nfrom the Mamba model, we proceed with node classification using the KAN-\nGIN model. The KAN-GIN layer is defined as Eq. (16).\nh(ℓ)(v) = KAN(ℓ)\n\n(1 + ϵ) · h(ℓ−1)(v) +\nX\nu∈N(v)\nh(ℓ−1)(u)\n\n\n(16)\nwhere h(ℓ)(v) represents the hidden state of node v at layer ℓ, h(ℓ−1)(v) is\nthe hidden state of node v at the previous layer ℓ−1, N(v) denotes the set of\nneighbors of node v, ϵ is a learnable parameter that controls the contribution\nof the node’s own features, KAN(ℓ) is the kernel adaptive network function\napplied at layer ℓ.\n17\n\nTable 1: 10 datasets’s description.\nAbbre\nDatasets\nTrain\nTest\nDimension\nLength\nClasses\nAF\nAtrialFibrillation\n15\n15\n2\n640\n3\nFM\nFingerMovements\n316\n100\n28\n50\n2\nHMD\nHandMovementDirection\n160\n74\n10\n400\n4\nHB\nHeartbeat\n204\n205\n61\n405\n2\nLIB\nLibras\n180\n180\n2\n45\n15\nMI\nMotorImagery\n278\n100\n64\n3000\n2\nNATO\nNATOPS\n180\n180\n24\n51\n6\nPD\nPenDigits\n7494\n3498\n2\n8\n10\nSRS2\nSelfRegulationSCP2\n200\n180\n7\n1152\n2\nSWJ\nStandWalkJump\n12\n15\n4\n2500\n3\n4. Experiments\n4.1. Experimental Setup\n4.1.1. Dataset\nWe selected 10 datasets from the UEA Archive [43] for our MTS classifi-\ncation experiments. These datasets represent the common intersection used\nby various comparative methods in existing study, ensuring the compara-\nbility and validity of our research. The main features and statistics of each\ndataset are summarized in Table. 1, covering key metrics such as the number\nof train and test, time series length, dimensionality, and number of classes.\nWe also provide supervised classification for supervised node classifica-\ntion, with the training and test sets as shown in the training and testing\ncolumns of Table 1. For the semi-supervised version, we randomly sample\nlabeled instances from each class in the training data with labeled data ac-\ncounted for 5%, 10%.\n4.1.2. Comparison Methods\nWe compared 14 implementations of the following MTS classifiers, cov-\nering distance-based classifiers, state-of-the-art pattern-based models, deep\nlearning models, and graph neural network models: ED, DTWI, DTWD (in-\ncluding normalized and unnormalized versions) [43]: These are commonly\nused distance-based models. WEASEL+MUSE [44]: An efficient time se-\nries pattern analysis toolkit. HIVE-COTE [45]: A heterogeneous ensemble\nclassification method for time series. MLSTM-FCN [46]: A deep learning\n18\n\nMTS classification framework that combines LSTM layers and convolutional\nlayers. TapNet [47]: A framework that integrates traditional methods with\ndeep learning. MTPool [48]: Utilizes variational pooling and adaptive ad-\njacency matrices to compute similarity using Euclidean distance. MF-Net\n[25]: Integrates local, global, and spatial features through graph convolu-\ntion. Smate [49]: a novel semi-supervised model for learning interpretable\nSpatio-Temporal representations. USRL [50]: an unsupervised method for\nlearning universal embeddings of time series, utilizing causal dilated convo-\nlutions and triplet loss with time-based negative sampling. ShapeNet [51]: a\nmodel that embeds shapelet candidates of varying lengths into a unified space\nusing cluster-wise triplet loss. TodyNet [52]: Captures latent spatio-temporal\ndependencies without predefined structures, utilizing dynamic graphs and a\ntemporal graph pooling layer. MICOS [53]: A mixed supervised contrastive\nlearning framework that employs mixed supervised contrastive Loss to effec-\ntively leverage labels and capture complex spatio-temporal features. SVP-T\n[54]: Incorporates shape-level inputs to capture both temporal and variable\ndependencies, utilizing a variable-position encoding layer and a VP-based\nself-attention. DKN [55]: Combining convolutional network and transformer,\nwhile employing densely dual self-distillation for enhanced representation\nlearning.\n4.1.3. Experimental Environment\nAll models were trained in a Python 3.8 environment using PyTorch 1.10.0\nwith Cuda 11.3, with training lasting for 1000 epochs. The training envi-\nronment was configured with the Ubuntu 20.04 operating system, equipped\nwith an NVIDIA GeForce RTX 2080 GPU (20GB VRAM) and an Intel(R)\nXeon(R) Platinum 8352V CPU (48GB RAM).\n4.1.4. Experimental Parameters\nThe initial learning rate was initially set to 10−3, and we used Negative\nLog Likelihood Loss as the loss function to optimize the model parameters.\nAdditionally, the Adam optimizer with ReduceLROnPlateau strategy was\nemployed for parameter updates.\nThe radius for FastDTW was set to 1. For the contrastive learning part,\nwe conduct training for 500 epochs to obtain MTS representations. In the\nDPMamba model, we choose a single-layer structure. The KAN-GIN model\nwas also set to both single-layer and multi-layer configurations. This design\nmakes the model more concise and easier to train and debug, while main-\n19\n\nTable 2: Experiment results.\nED\nDTWI\nDTWD\nED\n(norm)\nDTWI\n(norm)\nDTWD\n(norm)\nWEASEL\n+MUSE\nHIVE\n-COTE\nMLSTM\n-FCN\nTap\nNet\nMT\nPool\nMF\n-Net\nAF\n0.267\n0.267\n0.267\n0.200\n0.267\n0.267\n0.400\n0.133\n0.333\n0.200\n0.533\n0.466\nFM\n0.519\n0.513\n0.529\n0.510\n0.520\n0.530\n0.550\n0.550\n0.580\n0.470\n0.620\n0.620\nHMD\n0.279\n0.297\n0.231\n0.278\n0.297\n0.231\n0.365\n0.446\n0.527\n0.338\n0.486\n0.445\nHB\n0.620\n0.659\n0.717\n0.619\n0.658\n0.717\n0.727\n0.722\n0.663\n0.751\n0.742\n0.692\nLIB\n0.833\n0.894\n0.872\n0.833\n0.894\n0.870\n0.894\n0.900\n0.850\n0.878\n0.900\n0.850\nMI\n0.510\n0.390\n0.500\n0.510\n0.390\n0.500\n0.500\n0.610\n0.510\n0.590\n0.630\n0.540\nNATO\n0.850\n0.850\n0.883\n0.850\n0.850\n0.883\n0.870\n0.889\n0.900\n0.939\n0.944\n0.927\nPD\n0.973\n0.939\n0.977\n0.973\n0.939\n0.977\n0.948\n0.934\n0.978\n0.980\n0.983\n0.983\nSRS2\n0.483\n0.533\n0.539\n0.483\n0.533\n0.539\n0.460\n0.461\n0.472\n0.550\n0.600\n0.533\nSWJ\n0.333\n0.200\n0.200\n0.333\n0.200\n0.200\n0.267\n0.333\n0.400\n0.133\n0.667\n0.400\nAVG\n0.567\n0.554\n0.572\n0.559\n0.555\n0.571\n0.598\n0.598\n0.621\n0.583\n0.711\n0.646\nWins\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n4\n0\ntaining efficiency in feature extraction. The single-layer setup helps to avoid\noverfitting, especially in cases with limited data.\n4.2. Experimental Analysis\nThis section includes both supervised (Ours) and semi-supervised (Ours\n5%, Ours 10%) experiments. Table. 2 and Table. 3 present the average accu-\nracy results of CS-DPMamba and other MTS classifiers on 10 UEA datasets.\nThe best performances are highlighted in bold, with AVG representing the\naccuracy of the method across all datasets and Wins indicating the number\nof datasets where the best accuracy was achieved.\nAs shown in Table. 2 and Table. 3, CS-DPMamba outperformed other 14\nstate-of-the-art MTS classification methods on 4 datasets, while maintain-\ning competitiveness across datasets with varying numbers of variables and\nlengths. The ranking of critical difference (CD) diagram reflects the average\nperformance of each method across all datasets, with our method ranking\nsecond and best average accuracy (0.712), demonstrating the superiority of\nour approach, as shown in Fig. 4. According to the CD diagram, we can see\nthat our semi-supervised model (Ours 5%, Ours 10%) still surpasses many\nmethods and achieves certain results.\nOur method achieved the best results on the AF, FM, and SWJ datasets,\nwhich exhibit high complexity and diversity, making them suitable for feature\nextraction through dynamic similarity and long-range dependencies. MT-\nPool primarily utilizes Euclidean distance to calculate similarity, while our\nmethod captures dynamic similarity using FastDTW, allowing it to better\n20\n\nTable 3: Experiment results.\nSmate\nUSRL\nShape\nNet\nDKN\nTody\nNet\nSVP-T\nMICOS\nOurs\nOurs\n(5%)\nOurs\n(10%)\nAF\n0.133\n0.333\n0.400\n0.467\n0.467\n0.400\n0.333\n0.533\n0.400\n0.466\nFM\n0.620\n0.530\n0.580\n0.600\n0.570\n0.600\n0.570\n0.630\n0.570\n0.580\nHMD\n0.554\n0.378\n0.338\n0.662\n0.649\n0.392\n0.649\n0.540\n0.486\n0.527\nHB\n0.741\n0.751\n0.338\n0.765\n0.756\n0.790\n0.766\n0.742\n0.692\n0.722\nLIB\n0.849\n0.850\n0.856\n0.900\n0.850\n0.883\n0.889\n0.894\n0.850\n0.856\nMI\n0.590\n0.590\n0.610\n0.620\n0.640\n0.650\n0.500\n0.630\n0.540\n0.610\nNATO\n0.922\n0.939\n0.883\n0.872\n0.972\n0.906\n0.967\n0.933\n0.900\n0.922\nPD\n0.980\n0.980\n0.977\n0.948\n0.987\n0.983\n0.981\n0.987\n0.934\n0.978\nSRS2\n0.567\n0.550\n0.578\n0.600\n0.550\n0.600\n0.578\n0.555\n0.472\n0.550\nSWJ\n0.533\n0.400\n0.533\n0.533\n0.467\n0.467\n0.533\n0.667\n0.467\n0.467\nAVG\n0.649\n0.630\n0.609\n0.697\n0.691\n0.667\n0.677\n0.712\n0.631\n0.690\nWins\n0\n0\n0\n3\n2\n3\n0\n4\n0\n0\nFigure 4: CD diagram.\n21\n\nTable 4: Representation ablation experiment results.\nFastdtw\nContrastFastdtw\nAF\n0.200\n0.260\nFM\n0.510\n0.530\nHMD\n0.279\n0.283\nHB\n0.692\n0.697\nLIB\n0.305\n0.333\nhandle the nonlinear features of time series. ShapeNet focuses on embedding\nshape candidates into a unified space, whereas our method demonstrates\ngreater adaptability in similarity computation and bidirectional modeling,\nenabling it to handle more diverse inputs. Dyformer addresses the limitations\nof transformers through hierarchical pooling and adaptive learning, but our\nmethod combines graph neural networks to manage more complex structures\nand dynamic characteristics, achieving higher accuracy in classification tasks.\nTodyNet focuses on latent spatiotemporal dependencies, but our model cap-\ntures richer feature relationships through similarity matrix construction and\nenhanced graph neural networks, improving classification results.\n4.3. Ablation Study\n4.3.1. Representations Ablation Experiment\nWe employed two methods to validate the effectiveness of the time series\nrepresentations through knn: ContrastFastDTW, which combines time series\nrepresentations obtained from TemCL module with FastDTW, and standard\ntime series with FastDTW. As show in Table. 4 and Fig. 5, ContrastFast-\nDTW achieves higher accuracy than using the original data on all datasets.\nThe experimental results demonstrate that TemCL can effectively generate\nhigh-quality time series representations, thereby enhancing classification per-\nformance. This indicates that ContrastFastDTW is better able to capture\nfeatures and patterns of representations through the strategy of contrastive\nlearning.\n4.3.2. Components Ablation Experiment\nWe conducted an ablation study to verify the impact of key components\nin CS-DPMamba on the results, specifically comparing the following models:\nOnly DPMamba, Only KAN-GIN, Only ContrastDTW, and the complete\n22\n\nFigure 5: Representations ablation.\nTable 5: Component ablation experiment results. The underlined data indicates that it\nranks second.\nOnly\nDPMamba\nOnly\nKAN-GIN\nOnly\nContrastFastdtw\nCS-\nDPMamba\nAF\n0.400\n0.400\n0.260\n0.533\nFM\n0.520\n0.530\n0.530\n0.630\nHMD\n0.445\n0.486\n0.283\n0.540\nHB\n0.620\n0.663\n0.697\n0.742\nLIB\n0.833\n0.850\n0.333\n0.894\nCS-DPMamba model. As shown in Table. 5 and Fig. 6, the experimen-\ntal results indicate that the complete model achieves the best performance,\nhighlighting the importance of the collaborative effect of each component on\nthe overall performance.\n4.3.3. Scale Ablation Experiment\nIn this section, we use different proportions (5%, 10%, 100%) of labeled\ndata sets to conduct node classification experiments. The experimental re-\nsults are shown in Table 6. We can find that as the label ratio increases, so\ndoes the accuracy.\n23\n\nFigure 6: Component ablation.\nTable 6: Using labeled data sets at different scales.\nOurs\nOurs(5%)\nOurs(10%)\nAF\n0.533\n0.400\n0.466\nFM\n0.630\n0.570\n0.580\nHMD\n0.540\n0.486\n0.527\nHB\n0.742\n0.692\n0.722\nLIB\n0.894\n0.850\n0.856\nMI\n0.630\n0.540\n0.610\nNATO\n0.933\n0.900\n0.922\nPD\n0.983\n0.934\n0.978\nSRS2\n0.555\n0.472\n0.550\nSWJ\n0.667\n0.467\n0.467\nAVG\n0.711\n0.631\n0.690\nWins\n10\n0\n0\n4.4. ContrastFastDTW Matrix Heatmap\nSimilarity matrix heatmaps (AtrialFibrillation, StandWalkJump and Hand-\nMovementDirection) are shown in Fig. 7. The bright grid in the figure rep-\nresents a large distance (low similarity), the dark grid (high similarity), and\nthe diagonal value is 0.\n24\n\n(a) AtrialFibrillation\n(b) StandWalkJump\n(c)\nHandMovementDirec-\ntion\nFigure 7: Comparison of Heatmaps\n5. Conclusion\nFor the difficulty to model similarities and long-range dependencies of\nMTS. We propose contrast similarity-aware DPMamba for MTS node clas-\nsification. First, to obtain the similarity in the dataset, we initially used\nTemCL to acquire time series representations and then constructed a simi-\nlarity matrix between MTS samples using FastDTW, capturing the dynamic\nsimilarity characteristics of the time series.\nSecond, we applied the DP-\nMamba to consider the bidirectional nature of MTS, allowing us to better\ncapture long-range and short-range dependencies within the data. Finally,\nwe utilized the KAN to enhance the expressive power of the GIN for MTS\nnode classification. By comprehensively considering the long-range depen-\ndencies and dynamic similarity features of time series, we achieved precise\nnode classification. In the future, we will explore more efficient modeling\nmethods for calculating the similarity between samples.\n6. Acknowledgements\nThis work was supported by the National Natural Science Foundation of\nChina [grant numbers 82072014], the National Key R&D Program of China\n[grant numbers 2019YFE010670], the Shandong Province Natural Science\nFoundation [grant numbers ZR2020MF028]. We would like to thank Eamonn\nKeogh and his team, Tony Bagnall and his team for the UEA/UCR time\nseries classification repository.\n25\n\n7. Authors contributions\nMingsen Du: Conceptualization, Methodology, Validation, Writing–original\ndraft, Writing–review & editing.\nMeng Chen: Methodology, Validation,\nWriting–original draft, Writing–review & editing.\nYongjian Li: Method-\nology, Validation, Writing–original draft, Writing–review & editing. Xiuxin\nZhang: Methodology, Validation, Writing–original draft, Writing–review &\nediting. Jiahui Gao: Methodology, Validation, Writing–original draft, Writ-\ning–review & editing. Cun Ji: Methodology, Validation, Writing–original\ndraft, Writing–review & editing, Supervision. Shoushui Wei: Methodology,\nValidation, Writing–original draft, Writing–review & editing, Supervision,\nProject administration.\n8. Data Availability\nThe datasets used or analyzed during the current study are available from\nthe UEA archive: http://timeseriesclassification.com.\n9. Code Availability\nFor reproducibility, we released our codes and parameters on Github:\nhttps://github.com/dumingsen/DPMamba.\n10. Declaration of competing interest\nThe authors declare that they have no known competing financial inter-\nests or personal relationships that could have appeared to influence the work\nreported in this paper.\nReferences\n[1] J. Wang, L. Chen, M. A. Al Faruque, Domino: Domain-invariant hy-\nperdimensional classification for multi-sensor time series data, in: 2023\nIEEE/ACM International Conference on Computer Aided Design (IC-\nCAD), IEEE, 2023, pp. 1–9.\n[2] D. Zhan, Y. Dai, Y. Dong, J. He, Z. Wang, J. Anderson, Meta-adaptive\nstock movement prediction with two-stage representation learning, in:\nProceedings of the 2024 SIAM International Conference on Data Mining\n(SDM), SIAM, 2024, pp. 508–516.\n26\n\n[3] S. Yang, C. Lian, Z. Zeng, B. Xu, J. Zang, Z. Zhang, A multi-view multi-\nscale neural network for multi-label ecg classification, IEEE Transactions\non Emerging Topics in Computational Intelligence 7 (3) (2023) 648–660.\n[4] P. Kumar, S. Chauhan, L. K. Awasthi, Human activity recognition (har)\nusing deep learning: Review, methodologies, progress and future re-\nsearch directions, Archives of Computational Methods in Engineering\n31 (1) (2024) 179–219. doi:0.1007/s11831-023-09986-x.\n[5] S. K. Singh, M. H. Anisi, S. Clough, T. Blyth, D. Jarchi, Cnn-bilstm\nbased gan for anamoly detection from multivariate time series data, in:\n2023 24th International Conference on Digital Signal Processing (DSP),\nIEEE, 2023, pp. 1–4. doi:10.1109/DSP58604.2023.10167937.\n[6] K. Yi, Q. Zhang, W. Fan, H. He, L. Hu, P. Wang, N. An, L. Cao, Z. Niu,\nFouriergnn: Rethinking multivariate time series forecasting from a pure\ngraph perspective, Advances in Neural Information Processing Systems\n36 (2024). doi:10.48550/arXiv.2311.06190.\n[7] G. Shenderovitz, E. Sheetrit, N. Nissim, Patterns of time-interval based\npatterns for improved multivariate time series data classification, Engi-\nneering Applications of Artificial Intelligence 133 (2024) 108171.\n[8] R. Lekshmi, B. R. Jose, J. Mathew, R. K. Sanodiya, Mnemonic: Multi-\nkernel contrastive domain adaptation for time-series classification, En-\ngineering Applications of Artificial Intelligence 133 (2024) 108255.\n[9] T. M. Tran, X.-M. T. Le, H. T. Nguyen, V.-N. Huynh, A novel non-\nparametric method for time series classification based on k-nearest\nneighbors and dynamic time warping barycenter averaging, Engineer-\ning Applications of Artificial Intelligence 78 (2019) 173–185.\n[10] E. Keogh, C. A. Ratanamahatana, Exact indexing of dynamic time\nwarping, Knowledge and information systems 7 (2005) 358–386. doi:\n10.1007/s10115-004-0154-9.\n[11] G. Al-Naymat, S. Chawla, J. Taheri, Sparsedtw: A novel approach to\nspeed up dynamic time warping, arXiv preprint arXiv:1201.2969 (2012).\ndoi:10.48550/arXiv.1201.2969.\n27\n\n[12] H. Nath, U. Baruah, Evaluation of lower bounding methods of dynamic\ntime warping (dtw), International Journal of Computer Applications\n94 (20) (2014) 12–17. doi:10.5120/16550-6168.\n[13] S. Salvador, P. Chan, Toward accurate dynamic time warping in linear\ntime and space, Intelligent Data Analysis 11 (5) (2007) 561–580. doi:\n10.3233/IDA-2007-11508.\n[14] M. Beck, K. P¨oppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp,\nG. Klambauer, J. Brandstetter, S. Hochreiter, xlstm: Extended long\nshort-term memory, arXiv preprint arXiv:2405.04517 (2024). doi:10.\n48550/arXiv.2405.04517.\n[15] N. M. Foumani, C. W. Tan, G. I. Webb, M. Salehi, Improving position\nencoding of transformers for multivariate time series classification, Data\nMining and Knowledge Discovery 38 (1) (2024) 22–48. doi:10.1007/\ns10618-023-00948-2.\n[16] M. Cheng, Q. Liu, Z. Liu, Z. Li, Y. Luo, E. Chen, Formertime: Hierarchi-\ncal multi-scale representations for multivariate time series classification,\nin: Proceedings of the ACM Web Conference 2023, 2023, pp. 1437–1445.\ndoi:10.1145/3543507.3583205.\n[17] Y. Wu, C. Lian, Z. Zeng, B. Xu, Y. Su, An aggregated convolutional\ntransformer based on slices and channels for multivariate time series\nclassification, IEEE Transactions on Emerging Topics in Computational\nIntelligence 7 (3) (2022) 768–779. doi:10.1109/TETCI.2022.3210992.\n[18] J. Wen, N. Zhang, X. Lu, Z. Hu, H. Huang, Mgformer: Multi-group\ntransformer for multivariate time series classification, Engineering Ap-\nplications of Artificial Intelligence 133 (2024) 108633.\n[19] A. Gu, T. Dao, Mamba: Linear-time sequence modeling with selective\nstate spaces, arXiv preprint arXiv:2312.00752 (2023). doi:10.48550/\narXiv.2312.00752.\n[20] Z. Wang,\nF. Kong,\nS. Feng,\nM. Wang,\nH. Zhao,\nD. Wang,\nY. Zhang, Is mamba effective for time series forecasting?, arXiv preprint\narXiv:2403.11144 (2024). doi:10.48550/arXiv.2403.11144.\n28\n\n[21] M. A. Ahamed, Q. Cheng, Tscmamba: Mamba meets multi-view learn-\ning for time series classification, arXiv preprint arXiv:2406.04419 (2024).\ndoi:10.48550/arXiv.2406.04419.\n[22] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, X. Wang, Vision mamba:\nEfficient visual representation learning with bidirectional state space\nmodel, arXiv preprint arXiv:2401.09417 (2024). doi:10.48550/arXiv.\n2401.09417.\n[23] R. Bresson, G. Nikolentzos, G. Panagopoulos, M. Chatzianastasis,\nJ. Pang, M. Vazirgiannis, Kagnns: Kolmogorov-arnold networks meet\ngraph learning, arXiv preprint arXiv:2406.18380 (2024). doi:10.48550/\narXiv.2406.18380.\n[24] B. N. Patro, V. S. Agneeswaran, Mamba-360: Survey of state space\nmodels as transformer alternative for long sequence modelling: Methods,\napplications, and challenges, arXiv preprint arXiv:2404.16112 (2024).\ndoi:10.48550/arXiv.2404.16112.\n[25] M. Du, Y. Wei, X. Zheng, C. Ji, Multi-feature based network for multi-\nvariate time series classification, Information Sciences 639 (2023) 119009.\ndoi:10.1016/j.ins.2023.119009.\n[26] M. Du, Y. Wei, Y. Hu, X. Zheng, C. Ji, Multivariate time series classifi-\ncation based on fusion features, Expert Systems with Applications 248\n(2024) 123452. doi:10.1016/j.eswa.2024.123452.\n[27] K. Xu, W. Hu, J. Leskovec, S. Jegelka, How powerful are graph neu-\nral networks?, arXiv preprint arXiv:1810.00826 (2018). doi:10.48550/\narXiv.1810.00826.\n[28] T. N. Kipf, M. Welling, Semi-supervised classification with graph con-\nvolutional networks, arXiv preprint arXiv:1609.02907 (2016).\ndoi:\n10.48550/arXiv.1609.02907.\n[29] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui, P. S. Yu, Heterogeneous\ngraph attention network, in: The world wide web conference, 2019, pp.\n2022–2032. doi:10.1145/3308558.3313562.\n[30] S. Yan, Y. Xiong, D. Lin, Spatial temporal graph convolutional net-\nworks for skeleton-based action recognition, in: Proceedings of the AAAI\n29\n\nconference on artificial intelligence, Vol. 32, 2018. doi:10.1609/aaai.\nv32i1.12328.\n[31] W. Hamilton, Z. Ying, J. Leskovec, Inductive representation learning\non large graphs, Advances in neural information processing systems 30\n(2017). doi:10.48550/arXiv.1706.02216.\n[32] C. Xu, F. Su, J. Lehmann, Time-aware graph neural networks for\nentity alignment between temporal knowledge graphs, arXiv preprint\narXiv:2203.02150 (2022). doi:10.18653/v1/2021.emnlp-main.709.\n[33] Z. Shao, Z. Zhang, F. Wang, Y. Xu, Pre-training enhanced spatial-\ntemporal graph neural network for multivariate time series forecasting,\nin: Proceedings of the 28th ACM SIGKDD conference on knowledge dis-\ncovery and data mining, 2022, pp. 1567–1577. doi:10.1145/3534678.\n3539396.\n[34] D. Zha, K.-H. Lai, K. Zhou, X. Hu, Towards similarity-aware time-\nseries classification, in: Proceedings of the 2022 SIAM International\nConference on Data Mining (SDM), SIAM, 2022, pp. 199–207. doi:\n10.1137/1.9781611977172.23.\n[35] W. Xi, A. Jain, L. Zhang, J. Lin, Efficient and accurate similarity-\naware graph neural network for semi-supervised time series classification,\nin: Pacific-Asia Conference on Knowledge Discovery and Data Mining,\nSpringer, 2024, pp. 276–287. doi:10.1007/978-981-97-2266-2_22.\n[36] C. Zeng, Z. Liu, G. Zheng, L. Kong, C-mamba: Channel correlation\nenhanced state space models for multivariate time series forecasting,\narXiv preprint arXiv:2406.05316 (2024). doi:10.48550/arXiv.2406.\n05316.\n[37] S. Ma, Y. Kang, P. Bai, Y.-B. Zhao, Fmamba:\nMamba based on\nfast-attention for multivariate time-series forecasting, arXiv preprint\narXiv:2407.14814 (2024). doi:10.48550/arXiv.2407.14814.\n[38] M. Jin, H. Y. Koh, Q. Wen, D. Zambon, C. Alippi, G. I. Webb, I. King,\nS. Pan, A survey on graph neural networks for time series: Forecasting,\nclassification, imputation, and anomaly detection, IEEE Transactions\non Pattern Analysis and Machine Intelligence (2024). doi:10.1109/\nTPAMI.2024.3443141.\n30\n\n[39] Z. Cheng, Y. Yang, S. Jiang, W. Hu, Z. Ying, Z. Chai, C. Wang,\nTime2graph+: Bridging time series and graph representation learning\nvia multiple attentions, IEEE Transactions on Knowledge and Data En-\ngineering 35 (2) (2021) 2078–2090. doi:10.1109/TKDE.2021.3094908.\n[40] Y. Wang, H. Long, L. Zheng, J. Shang, Graphformer: Adaptive graph\ncorrelation transformer for multivariate long sequence time series fore-\ncasting, Knowledge-Based Systems 285 (2024) 111321. doi:10.1016/\nj.knosys.2023.111321.\n[41] Y. Wang, Y. Xu, J. Yang, M. Wu, X. Li, L. Xie, Z. Chen, Graph-aware\ncontrasting for multivariate time-series classification, in: Proceedings\nof the AAAI Conference on Artificial Intelligence, Vol. 38, 2024, pp.\n15725–15734. doi:10.1609/aaai.v38i14.29501.\n[42] S. Wang, Y. Zhang, X. Lin, Y. Hu, Q. Huang, B. Yin, Dynamic hyper-\ngraph structure learning for multivariate time series forecasting, IEEE\nTransactions on Big Data (2024). doi:10.1109/TBDATA.2024.3362188.\n[43] A. Bagnall, H. A. Dau, J. Lines, M. Flynn, J. Large, A. Bostrom,\nP. Southam, E. Keogh, The uea multivariate time series classification\narchive, 2018, arXiv preprint arXiv:1811.00075 (2018).\n[44] P. Sch¨afer, U. Leser, Multivariate time series classification with weasel+\nmuse, arXiv preprint arXiv:1711.11343 (2017).\n[45] A. Bagnall, M. Flynn, J. Large, J. Lines, M. Middlehurst, A tale of two\ntoolkits, report the third: on the usage and performance of hive-cote v1.\n0, arXiv preprint arXiv:2004.06069 (2020).\n[46] F. Karim, S. Majumdar, H. Darabi, S. Harford, Multivariate lstm-fcns\nfor time series classification, Neural networks 116 (2019) 237–245.\n[47] X. Zhang, Y. Gao, J. Lin, C.-T. Lu, Tapnet: Multivariate time series\nclassification with attentional prototypical network, in: Proceedings of\nthe AAAI conference on artificial intelligence, Vol. 34, 2020, pp. 6845–\n6852.\n[48] Z. Duan, H. Xu, Y. Wang, Y. Huang, A. Ren, Z. Xu, Y. Sun, W. Wang,\nMultivariate time-series classification with hierarchical variational graph\npooling, Neural Networks 154 (2022) 481–490.\n31\n\n[49] J. Zuo, K. Zeitouni, Y. Taher, Smate: Semi-supervised spatio-temporal\nrepresentation learning on multivariate time series, in: 2021 IEEE In-\nternational Conference on Data Mining (ICDM), IEEE, 2021, pp. 1565–\n1570.\n[50] J.-Y. Franceschi, A. Dieuleveut, M. Jaggi, Unsupervised scalable repre-\nsentation learning for multivariate time series, Advances in neural infor-\nmation processing systems 32 (2019).\n[51] G. Li, B. Choi, J. Xu, S. S. Bhowmick, K.-P. Chun, G. L.-H. Wong,\nShapenet: A shapelet-neural network approach for multivariate time\nseries classification, in: Proceedings of the AAAI conference on artificial\nintelligence, Vol. 35, 2021, pp. 8375–8383.\n[52] H. Liu, D. Yang, X. Liu, X. Chen, Z. Liang, H. Wang, Y. Cui, J. Gu,\nTodynet: temporal dynamic graph neural network for multivariate time\nseries classification, Information Sciences (2024) 120914.\n[53] S. Hao, Z. Wang, A. D. Alexander, J. Yuan, W. Zhang, Micos: Mixed\nsupervised contrastive learning for multivariate time series classification,\nKnowledge-Based Systems 260 (2023) 110158.\n[54] R. Zuo, G. Li, B. Choi, S. S. Bhowmick, D. N.-y. Mah, G. L. Wong,\nSvp-t: a shape-level variable-position transformer for multivariate time\nseries classification, in: Proceedings of the AAAI Conference on Artifi-\ncial Intelligence, Vol. 37, 2023, pp. 11497–11505.\n[55] Z. Xiao, H. Xing, R. Qu, L. Feng, S. Luo, P. Dai, B. Zhao, Y. Dai,\nDensely knowledge-aware network for multivariate time series classifica-\ntion, IEEE Transactions on Systems, Man, and Cybernetics: Systems\n(2024).\n32",
    "pdf_filename": "Contrast_Similarity-Aware_Dual-Pathway_Mamba_for_Multivariate_Time_Series_Node_Classification.pdf"
}