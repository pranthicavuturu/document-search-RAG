{
    "title": "Is Programming by Example solved by LLMs?",
    "abstract": "Programming-by-Examples(PBE)aimstogenerateanalgorithmfrominput-output examples. Such systems are practically and theoretically important: from an end-userperspective,theyaredeployedtomillionsofpeople,andfromanAIper- spective,PBEcorrespondstoaverygeneralformoffew-shotinductiveinference. GiventhesuccessofLargeLanguageModels(LLMs)incode-generationtasks,we investigateheretheextenttowhichLLMscanbesaidtohave‘solved’PBE.We experimentonclassicdomainssuchaslistsandstrings,andanuncommongraphics programmingdomainnotwellrepresentedintypicalpretrainingdata. Wefindthat pretrainedmodelsarenoteffectiveatPBE,butthattheycanbefine-tunedformuch higherperformance,providedthetestproblemsarein-distribution. Weanalyze empiricallywhatcausesthesemodelstosucceedandfail,andtakestepstoward understandinghowtoachievebetterout-of-distributiongeneralization.Collectively theseresultssuggestthatLLMsmakestrongprogresstowardsolvingthetypical suiteofPBEtasks,potentiallyincreasingtheflexibilityandapplicabilityofPBE systems,whilealsoidentifyingwaysinwhichLLMsstillfallshort. 1 Introduction Programming-by-Example(PBE)systemssolveachallengingtask: Giveninput-outputexamplesof ahiddenalgorithm,theyseektoconstructthesourcecodeoftheunderlyingfunction[1,2]. PBEis deployedtomillionsofusers[3,4,5,6],liesneartheheartofcoreAIchallenges[7,8,9,10],andisa qualitativelydifferentproblemfromthebulkofrecentworkonLLMcodegeneration,becauserather thangeneratesourcecodefromnaturallanguage[11],PBEisinsteadfundamentallyaboutfew-shot inductiveinference: Givenahandfulofexamples,inferringtheprogramthatwillgeneralizetonew inputs,orwhichcapturesthetruelatentregularity,withoutrelyingonnatural-languageguidance. We investigate here the extent to which large language models pretrained on source code can solve PBE. If they can, this unlocks the ability to do PBE in general-purpose Turing complete languageslikePython,unliketherestricteddomain-specificlanguageswhichhavesofardominated PBE[4,12,13,14,i.a.],therebyincreasingthescopeandpowerofthisparadigm. IfLLMscannot performPBE,thenthishighlightsadeficitofinductivereasoningandproblemsolving,andsuggests LLMsleantooheavilyonnaturallanguagecuestogeneratecode. We find that pretrained and instruction-tuned models serve as poor PBE systems, a finding also supported by recent work [15, 16, 12, 17]. But our investigation further finds that LLMs can be fine-tunedforsignificantlyhigherperformance,providedtheyarenotaskedtogeneralizefarbeyond thefine-tuningdata. Toaddressthisfailureofgeneralizationwegiveanalgorithmfortakingasmall unlabeleddatasetofproblemsandadaptingtheLLMtoit,whichwefindnarrowsthisdomaingap. TheresultingrecipeallowsPBEoverTuring-completelanguagesacrossthreequalitativelydifferent domains (Fig. 1): algorithms on vectors of numbers, string manipulation macros, and graphics programsinLOGO/Turtle. Ineverycase,ourfinalmodelisatleastaseffectiveascustomsymbolic searchalgorithmsoperatingoverdomain-specificlanguages,andsurpassespowerfulclosed-source 38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024). 4202 voN 91 ]LC.sc[ 3v61380.6042:viXra",
    "body": "Is Programming by Example solved by LLMs?\nWen-DingLi KevinEllis\nCornellUniversity CornellUniversity\nwl678@cornell.edu kellis@cornell.edu\nAbstract\nProgramming-by-Examples(PBE)aimstogenerateanalgorithmfrominput-output\nexamples. Such systems are practically and theoretically important: from an\nend-userperspective,theyaredeployedtomillionsofpeople,andfromanAIper-\nspective,PBEcorrespondstoaverygeneralformoffew-shotinductiveinference.\nGiventhesuccessofLargeLanguageModels(LLMs)incode-generationtasks,we\ninvestigateheretheextenttowhichLLMscanbesaidtohave‘solved’PBE.We\nexperimentonclassicdomainssuchaslistsandstrings,andanuncommongraphics\nprogrammingdomainnotwellrepresentedintypicalpretrainingdata. Wefindthat\npretrainedmodelsarenoteffectiveatPBE,butthattheycanbefine-tunedformuch\nhigherperformance,providedthetestproblemsarein-distribution. Weanalyze\nempiricallywhatcausesthesemodelstosucceedandfail,andtakestepstoward\nunderstandinghowtoachievebetterout-of-distributiongeneralization.Collectively\ntheseresultssuggestthatLLMsmakestrongprogresstowardsolvingthetypical\nsuiteofPBEtasks,potentiallyincreasingtheflexibilityandapplicabilityofPBE\nsystems,whilealsoidentifyingwaysinwhichLLMsstillfallshort.\n1 Introduction\nProgramming-by-Example(PBE)systemssolveachallengingtask: Giveninput-outputexamplesof\nahiddenalgorithm,theyseektoconstructthesourcecodeoftheunderlyingfunction[1,2]. PBEis\ndeployedtomillionsofusers[3,4,5,6],liesneartheheartofcoreAIchallenges[7,8,9,10],andisa\nqualitativelydifferentproblemfromthebulkofrecentworkonLLMcodegeneration,becauserather\nthangeneratesourcecodefromnaturallanguage[11],PBEisinsteadfundamentallyaboutfew-shot\ninductiveinference: Givenahandfulofexamples,inferringtheprogramthatwillgeneralizetonew\ninputs,orwhichcapturesthetruelatentregularity,withoutrelyingonnatural-languageguidance.\nWe investigate here the extent to which large language models pretrained on source code can\nsolve PBE. If they can, this unlocks the ability to do PBE in general-purpose Turing complete\nlanguageslikePython,unliketherestricteddomain-specificlanguageswhichhavesofardominated\nPBE[4,12,13,14,i.a.],therebyincreasingthescopeandpowerofthisparadigm. IfLLMscannot\nperformPBE,thenthishighlightsadeficitofinductivereasoningandproblemsolving,andsuggests\nLLMsleantooheavilyonnaturallanguagecuestogeneratecode.\nWe find that pretrained and instruction-tuned models serve as poor PBE systems, a finding also\nsupported by recent work [15, 16, 12, 17]. But our investigation further finds that LLMs can be\nfine-tunedforsignificantlyhigherperformance,providedtheyarenotaskedtogeneralizefarbeyond\nthefine-tuningdata. Toaddressthisfailureofgeneralizationwegiveanalgorithmfortakingasmall\nunlabeleddatasetofproblemsandadaptingtheLLMtoit,whichwefindnarrowsthisdomaingap.\nTheresultingrecipeallowsPBEoverTuring-completelanguagesacrossthreequalitativelydifferent\ndomains (Fig. 1): algorithms on vectors of numbers, string manipulation macros, and graphics\nprogramsinLOGO/Turtle. Ineverycase,ourfinalmodelisatleastaseffectiveascustomsymbolic\nsearchalgorithmsoperatingoverdomain-specificlanguages,andsurpassespowerfulclosed-source\n38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).\n4202\nvoN\n91\n]LC.sc[\n3v61380.6042:viXra\nmodels such as GPT4 [18]. We also find that the resulting system can cover a broader scope of\nproblemsthanclassicsymbolicmethods,owingtotheuseofaTuring-completelanguage,which,at\nleasttheoretically,allowslearninganycomputablefunction.\nDOMAIN: lists DOMAIN: graphics\nprovided examples generated program provided example generated program\nINPUT OUTPUT # Check if the list is empty for i in range(7):\nif not input_list: with fork_state():\n4,2,8 2,0,6 return input_list for j in range(4):\n# Find min number in the list forward(2*i)\n9,9,9,9 0,0,0,0 min_num = min(input_list) left(90.0)\n# Subtract from each element\n-7,0,2 0,7,9 return [num - min_num\nfor num in input_list]\nDOMAIN: text editing macros\nprovided examples generated program\nINPUT OUTPUT original_time = datetime.strptime(input_str, '%H:%M:%S')\nhour = original_time.hour\n18:25:57 6PM to 8PM start_hour = hour - (hour % 2)\nend_hour = start_hour + 2\n21:44:40 8PM to 10PM start_hour_12 = start_hour % 12 or 12\nend_hour_12 = end_hour % 12 or 12\n07:00:20 6AM to 8AM start_ampm = \"AM\" if start_hour < 12 else \"PM\"\nend_ampm = \"AM\" if end_hour < 12 or end_hour == 24 else \"PM\"\n23:34:17 10PM to 12AM return f\"{start_hour_12}{start_ampm} to {end_hour_12}{end_ampm}\"\nFigure1: Domains,includingstandardonesthatresembleprogramsfoundinpretrainingdata,as\nwellasalesscommongraphicsdomain,whichislikelylessrepresentedinLLMpretrainingdata.\n2 Background\nProgramming by Example considers synthesizing a program ρ given a vector of inputs X and\ncorrespondingoutputsY. Typicallytheprogramisexpectedtoexactlyfittheprovidedexamples,\nρ(X ) = Y ,∀i, where i indexes examples. The program ρ is drawn from a (potentially infinite)\ni i\nlanguage L. Typically L is a domain-specific language designed for a specific PBE system, not\na general-purpose programming language. For example, the PBE system FlashFill synthesizes\nstringmanipulationmacrosdesignedtoautomatecommonspreadsheetedits[2]. FlashFill’sdomain-\nspecificlanguageLincludescommonlyoccurringregularexpressions,togetherwithstringslicingand\nconcatenation,andrestrictedformsofloops. ThelanguageLisalsodesignedtoallowpolynomial-\ntimeconstructionofprogramsconsistentwithinput-outputexamples. FlashFill’sgoal,likemostPBE\nsystems,istogeneralizetohold-outtestcases: inputsX′with(hidden)targetoutputsY′.\nPickaprogramρfrom {ρ∈L : ρ(X )=ρ(Y ),∀i}. Succeedifρ(X′)=ρ(Y′),∀j (1)\ni i j j\nInitssimplestforms,PBEcanbeaccomplishedbyguess-and-checkenumerationuntilaprogramis\nfoundthatisconsistentwiththeexamples. Althoughthereexistmoresophisticatedsearchalgorithms,\nincluding those accelerated by neural guidance [19, 20, 21, 22], a key enabler of practical PBE\nsystems is the design of a carefully restricted domain-specific language L. The domain-specific\nlanguage effectively hardcodes symbolic knowledge, focusing the system on what programs the\nhumanengineerthinksaremostpromising,butattheexpenseofthewidersetofcomputablefunctions\nexpressibleingeneral-purposelanguages.\nThePBEsetupcoversothercasesaswell, suchassequenceextrapolation(theinputsareindices\ninto the sequence), as well as data compression (the input is null, and the data is compressed by\nsynthesizingaprogramthatreproducestheoutputdata). Therefore,atrulygeneralsolutiontoPBE—\nonewhichcouldexpressitssolutionsingeneralpurposeprogramminglanguages,andcovermost\n2\npracticallyrelevantproblems—wouldbebroadlyapplicabletomanyinductiveinferenceproblems,a\npointthathasbeenlongappreciated[9].\nLLMsforsolvingprogrammingproblemshavebeenrecentlyverysuccessful[11,23,24,25,26].\nThese systems typically input a prompt describing a problem in natural language, then sample\ncandidateprograms,andoptionallyfilterthosesamplesbycheckingthemagainstinput-outputtest\ncases,withthegoalofpassingholdouttests:\nDrawρ ∼p (·|prompt).Pickaρ∈{ρ : ρ (X )=ρ (Y ),∀i}.Success: ρ(X′)=ρ(Y′),∀j\nk LM k k i k i j j\nUnlikePBE,theprimarydriverofprogramgenerationisanaturallanguageprompt,althoughinput-\noutputsmayalsobeintheprompt[27,28]. RecentworkusingLLMstosynthesizeprogramssolely\nfromexampleshaseitherobtainednegativeresults[16,12],orfocusedonsimpleand/ornonstandard\nproblems[29,30,31],leavingtheextenttowhichPBEis‘solved’byLLMsanopenquestion.\n3 Methods\nBasicpromptingisthemoststraightforwardwayofperformingPBEwithapre-trainedmodel:Given\ninput-outputexamples(X,Y)apromptisconstructedandK programsaregenerated. Programsare\nfilteredbytheI/Oexamples,andarandomsatisfyingprogramisreturned:\nSampleρ ∼p (·|prompt(X,Y)), forkfrom1..K (2)\nk LM\nPickaρfrom {ρ : ρ (X )=ρ (Y ),∀i} (3)\nk k i k i\nFine-tuningimprovestheaboveapproachinaconceptuallystraightforwardway. Givenadataset\ncomprising tuples of programs and I/O examples, {(ρ,X,Y)}, we fine-tune the LM to predict a\nprogramfromitsinput-outputs. Butthisdatasetishardtocomeby: Althoughthereareweb-scale\ncorporaofnaturallyoccurringsourcecode,thereisnoanalogousdatasetofrunnablecodesnippets\npairedwithrepresentativeinput-outputexamples,andthisdatadeficitisespeciallytruefornewor\nunusualapplicationsofPBE,suchasthegraphicsprogramsweconsider.\nTo assemble a large dataset of (ρ,X,Y) triples we start with a small manually-constructed seed\ndataset,D ,andthenrandomlygeneratenewprogramsρandinputsX bypromptinganLLMwith\nseed\nmembersofD . TheoutputY comesfromrunningρonX. Theseeddataseteffectivelydefinesa\nseed\npriorover(ρ,X),notatedG inFig.2. WesamplefromG tocollectmanyprogram-inputpairs,but\nuseprogramexecutiontopredictY,notanLLM.Theresultingdataset,whichwecallD ,isused\ntune\ntotrainanLLMtogenerateprogramswhenpromptedwithinput-outputs. Asthisfine-tunedLLM\neffectivelylearnstodoprobabilisticinferenceinthegraphicalmodelshowninFig.2(right),we\nwritethisfine-tunedLLMasq (ρ|X,Y). Thisinferencenetworkistrainedtomaximize\nθ\nmaxlogq (ρ|X,Y),where(ρ,X)∼G(D )andY =ρ(X) (4)\nθ seed\nθ\nThismethodiscloselyrelatedtoself-instruct[32]andwake-sleep[33]. Likeself-instruct,weuse\npromptingtobootstrapalargedatasetfromasmallmanually-constructedone. Ourmethoddiffers\nbyusingtheLLMtogenerateahiddenlatentvariable(theprogram)whileadifferentgenerative\nprocess produces an observed variable (the program outputs). Like wake-sleep, we use samples\nfromagenerativemodeltotrainaninferencenetwork,butwedonotfurthertrainthegenerative\nmodelitself. Next,wewillseethatbringingthemethodmuchclosertowake-sleepbyupdatingthe\ngenerativemodelplaysanimportantrolewhendeployingthesystemonout-of-distributionproblems.\nρ\ndefines samples trains\nD seed G D tune q θ G Y\nX\nFigure2: Left: Datagenerationpipeline. Right: Thefine-tunednetworkq learnstodoinferenceina\nθ\ngraphicalmodelwheretheprioroverprograms,G,isdefinedbypromptinganLLMwithexample\ncodeinD ,whilethelikelihoodp(Y|ρ,X)isdefinedbyprogramexecution.\nseed\n3\nAdaptation. Oneofthemostpowerfulfeaturesofsourcecodeasarepresentationisitsabilityto\nefficientlyexpressawiderangeofcomputations. Thereforeitisofinteresttostudytheabilityof\nfine-tunedLLMstoextrapolatetoPBEproblemsoutsidethedistributionofthefine-tuningdata.\nWeconsiderabasicapproachtoadaptingtoadifferentdistributionofproblems,assumingaccessto\nproblemsdrawnfromthetestingdistribution,butwithoutlabeledprogramsolutions. Thismimics\nthe deployment of PBE systems to end-users who may have their own idiosyncratic distribution\nofproblemstheycareabout,andwhodonotprovideground-truthprograms,butwhocanprovide\nfeedbackonifageneratedprogramhascorrectbehavior. Thismeanswehaveanunlabeleddataset\nD comprisinginput-outputs(X,Y),aswellasalabeledseeddatasetD comprisingtriples\nadapt seed\n(ρ,X,Y). AdaptationproceedsbyiteratingbetweenpretrainingwithG(D ),testingonD ,and\nseed adapt\naddingbackintoD anyprogramsolutionsfoundontheadaptationproblems,whichthenbecome\nseed\nseedsforthenextiteration. Thisproducesasequenceoffine-tunedmodels,indexedbelowbyi:\ntrainmodel: θi =argmaxlogq (ρ|X,Y),where(ρ,X)∼G(Di )andY =ρ(X)\nθ seed\nθ\nruninference: ρX,Y ∼q (ρ|X,Y)for(X,Y)∈D andkfrom1..K\nk θi adapt\n(cid:110) (cid:111)\nupdateseed: Di+1 =Di∪ (ρX,Y,X,Y) : (X,Y)∈D , k ∈[K]ifρX,Y(X)=Y\nk adapt k\n(5)\nTheequationscanbeseenasawake-sleepalgorithmwhere“dreaming”correspondstotrainingq\nonfantasydata(firstequation)while“waking”correspondstorunninginferenceandupdatingthe\ngenerative model G (by updating the seed, second pair of equations). Ideally, each cycle of this\nwake-sleepadaptationsolvesmoreout-of-distributionproblems,whichtugsthegenerativemodelG\ntowardthetargetdistribution,unlockingsolutionstomoreout-of-distributionproblems,etc. This\nhingesoneachiterationactuallysolvingnewproblemsfromtheunlabeleddataset. Theoretically\nthisisguaranteedgivenenoughinference-timecompute(largeK above). WeexploreinSec.4.3the\nextenttowhichthisholdsinpractice.\n4 Experiments\nWestudydifferentLLM-approachestoprogramming-by-examplesacrossthreedomains(Fig.1):\n1. ListfunctionsisaPBEdomainmeanttomodela“programmer’sassistant”. Itconcerns\ndiscoveringalgorithmsthattransformlistsofnumbers,giveninput-outputexamples. This\nproblemstatementhasalonghistorywithinprogramsynthesis[13,34],andwaspopularized\nwithinmachinelearningbyDeepCoder[35]. Weconsidertwomodernlistfunctiondatasets\ncreatedbyRuleetal. 2024[17]andShietal. 2023[12],whichbothinvolvehigher-order\nfunctionsandnontrivialproceduressuchasmap,filter,andsort. Ruleetal. wasrecently\naddedtoBigBench[36].\n2. Texteditingisadomainwhereaprogramsynthesizerassistsanend-useredittheirspread-\nsheets or other documents. From string-to-string examples, the system generates edit\nmacros for tasks such as reformatting dates, extracting fields from semistructured text,\netc. [2, 37, 38, 4]. Text editing is the most prominent commercial success of PBE: The\nFlashFillPBEsystemshipsinMicrosoftExcelandisusedbymanymillionsofpeople[6].\nWe consider two text editing datasets: SyGuS problems [22]—which are easier—and\nPROSE[39]problems,whichconstitutethemostchallengingdatasetofitskind[38].\n3. LOGO/Turtlegraphicsisadomainwhosegoalistosynthesizeaprogramthatgeneratesa\ntargetimage.1 Systemsofthiskindcanbeusedbothforhigh-levelvisualreasoningand\nforhelpingartistsmakestructurededitstoimages[40,41]. Weuseadatasetofgeometric\ndesignsexpressedasLOGO/Turtle[42]programs—wheretheprogramsmoveasimulated\npenoveracanvas—takenfromWongetal.[43]. ToallowtheLLMtovisuallyperceivethe\ninputimage,weconverttheimagetoASCII-artstylestrings;seeFig.5andAppendix.A.1.3.\n1ThisisPBEwithasingleexampleandnullinput,effectivelycompressingtheimageintoaprogram.\n4\n4.1 Howwelldoesthefine-tunedmodelperform?\nWe prepare seed datasets for each domain, synthetically generate a large training set, and then\nfine-tuneaDeepSeekCoderLLM[44]thatwaspretrainedonsourcecode.2 Forlistfunctionsweseed\nwith50problemsfromRuleetal. 2024;Fortextediting,weconsiderseedingwitheitherSyGuSora\n40-problemsubsetofPROSE;forLOGOweseedwith200training-setproblemsinWongetal.[43].\nTheresultingfine-tunedmodelsaresurprisinglyeffectivewithintheirrespectivePBEdomains. On\nlistfunctionsourfinetunedmodelsurpassesthebestsymbolicsearchbaselinesreportedinRuleetal.\n(Fig.3a),surpassesthebestneurosymbolicsearchmethodfromShietal. (AppendixFig.10),and\nsurpassesGPT4. Italsosolves100%ofthelisttolistbenchmarkproblemsfromλ2(awell-known\nsymbolic synthesizer), shown in Appendix Tbl. 4: although plausibly, many λ2 problems are in\nthepretrainingdata. Ontextediting,itsurpassestheperformanceofFlashFillandapproachesthe\nlevelofFlashFill++(Tbl.1,Fig.3b). OnLOGO,itsolves90%ofthetestset(Fig.3c),surpassing\nsystemssuchasDreamCoder[45],whichintroducedthefirstversionoftheseLOGOproblems. It\nalsosolvesmoreproblemsthanLILOandRegal[43,46],whichareLOGOprogramsynthesizers\nthatinputnaturallanguagedescribinghowtheimageshouldbedrawn. Incontrast,ourmodeldoes\nnotuseanylanguageclues,generatingpurelyfromtheimage. Inadditiontoquantitativelysolving\nmoreproblems,wenotethattherearequalitativeimprovementstothebreadthofproblemsthatcan\nbesolvedinthefirstplacebecausetheLLMcangenerateTuring-completecodespanningamuch\nbroaderspaceofcomputations(Fig.4).\n80 80\n70 et 70 80\ne\nFl\n60 60\ngal\ne\n60 R\n45 00\nol\n45 00 FlashFill\n40\nL DI\nrL eO amCoder\n23 00 o\no\ngu\nu\npr\nr\nts\ns\n-4- -3\n7\n-t3\nb\nub\nrbo\nM\noet ba usg tFill 23 00 o\no\ngu\nu\npr\nr\nts\ns\n-4- -3\n7\n-t3\nb\nub\nrbo\n20 o ou ur rs s- -3 73 bb\nR\ngpt-4-turbo CoT gpt-4-turbo CoT gpt-4o\n10 deepseek33b 10 deepseek33B 0 gpt-4o-mini\n0 100 200 0 100 200 0 200 400\nSearch Budget (Num Samples) Search Budget (Num Samples) Search Budget (Num Samples)\n(a)Lists (b)Strings (c)Graphics\nFigure 3: Test set performance. A problem is solved if the predicted program generates correct\noutputsontheholdoutinputs. Metagol[47],RobustFill[20],andFleet[48]resultstakenfrom[17]\nTherearecaveatstotheaboveresults. First,the\nfine-tuned model essentially never produces a gen. oracle\ncorrectprogramonthefirsttry: Itrequirestens accuracy accuracy\norhundredsofsamples,eachofwhichiscom-\nFlashFill 33% —\npared against the ground-truth input-outputs,\nFlashFill++ — ≈100%\nanddiscardedifitcontradictstheexamples. On\nours,33B 82% 88%\naGPUliketheoneweuse(anNvidiaA6000)\nthisrejectionsamplingtakesontheorderofa\nTable 1: Generalization accuracy: % problems\nfewminutestosolveagivenproblem. However,\nwheretheprogrammakescorrectpredictionson\ncomparedtoclassicenumerativeprogramsyn-\nevery holdout test. Oracle accuracy: % prob-\nthesizers [13, 49], or even compared to those\nlemswhereacorrectprogramwasgenerated(even\nwithneuralguidance[35,22],proposingafew\nif incorrect programs were also generated that\nthousandprogramsisrelativelylittle,andcould\nalso passed the training input-outputs). Flash-\nnotplausiblycoverasignificantfractionofthe Fill++[38]onlyreportsoracleaccuracy. 3\nexponentiallylargesearchspace.\n2WepreferDeepSeekbecauseitisroughlyLLaMA-level,buthasfullyopentrainingdetails.\n3TheFlashFillresultswereobtainedusingMicrosoftExcelforMac.\n5\ndevloS\nsmelborP\n%\nINPUT OUTPUT generated program\nlines = text.splitlines()\nMary had a little lamb 1:Mary had a little lamb\nIts fleece was white… 2:Its fleece was white… result = \"\"\nfor i, line in enumerate(lines):\nTwinkle, twinkle, … 1:Twinkle, twinkle, …\nresult += f\"{i + 1}:{line}\\n\"\nHow I wonder what you… 2:How I wonder what you…\nUp above the world so… 3:Up above the world so…\nLike a diamond in the… 4:Like a diamond in the… return result[:-1]\ngenerated program\nINPUT OUTPUT\nstates = {\nNY New York \"AL\": \"Alabama\",\n\"AK\": \"Alaska\",\nCA California\n.........\n}\nAK Alaska\nreturn states[text]\nFigure4: PBEwithLLMsallowsusinggeneral-purposeprogramminglanguageswhichcanmix\nstringandnumericaloperationsinwaysnotallowedbydomain-specificlanguages[38](top),and\nallowsworldknowledgetoinformcodegeneration(bottom). I/Osandcodepartlyelidedforspace.\nprovided example ASCII representation generated candidate figures\n000000000023310000000000\n000000000010030000000000\n000000233200030210000000\n000001300300030022000000\n000001200030200003000000\n000000100021200003000000\n000000232226322330000000\n000001200012200000000000\n000003000030300012000000\n000002100210020012000000\n000000220300023230000000\n000000000300000100000000\n000000000133300000000000 ✅\nFigure5: ASCIIrepresentationofLOGOgraphics. Averagepixelintensityindicatedbynumbers0-9\nThesecondcaveatisthatthemodeldegradeswhentestedout-of-distribution. Anexampleofthis\ndegradationisillustratedinFig.6,whichteststheLOGOgraphicsmodelonhanddrawings(after\ntrainingoncleancomputergraphics). Ontheout-of-distributionhanddrawingthemodelmostly\nsamplesprogramsthatdonotfitthedata,butitsaccuracydoesnotfalltozero,meaningthatwith\nenoughcomputebudget,itdoesactuallygeneratereasonableprograms. Thisforeshadowstheresults\ninSec.4.3,whichmoresystematicallystudiesout-of-distributionbehavior.\n✅\nFigure6: Exampleout-of-distributionLOGOtest: inferringagraphicsprogramfromahanddrawing.\nSeealsoAppendixFig.12\n6\n4.2 Whatcausesthefine-tunedmodeltosucceedorfail?\nClassicsymbolicapproachestoPBE,whentheyarebasedonenumeration,tendtosucceedwhenever\nthetargetprogramissyntacticallysmall. Approachesbasedoncleverdynamicprogramming,suchas\ntheFlashFillfamily[4],succeedwhentheprogramisrepresentableinthedomain-specificlanguage.\nWhatpredictssuccessfortheseLLMapproaches?\nToanswerthisquestionweinvestigateseveralhypotheses. First, potentiallythesuccessisdeter-\nminedbyprogramsize,anddegradesasprogramsgrowlonger. Second,asamorerefinednotion\nof size, we instead measure the description length under the prior, which for a program ρ, is\n−logp (ρ|G(D )). Descriptionlengthunderthepriorwouldbeagoodpredictorofsuccessif\nLM seed\nthefine-tunedmodelengagesinblindguess-and-check: simplylearningthedistributionG(D ),\nseed\nandsamplingfromthispriorwhileignoringtheinput-outputs. Third,onepossibilityisthatsuccessis\npredictedbydescriptionlengthundertheapproximateposterior(−logq (ρ|X,Y)),whichwould\nθ\nbethecaseifthefine-tunedmodelattendscloselytotheinput-outputsandreshapesitsdistribution\naccordingly, insteadofdefaultingtotheprior. Totestthesehypotheseswecalculatetheaverage\ncomputebudgetneededtosolveeachproblem,andcompareitwiththesedifferentvariables. Fig.7\nshowsthatposteriordescriptionlengthismorepredictivethanprogramsizeandpriordescription\nlength: unlike classical methods, metrics of program length correlate poorly with problem diffi-\nculty,andthereisnoevidencethatthefine-tunedmodel’sbehaviorcanbecharacterizedasblind\nguess-and-check. (SeealsoFig.5).\n1037\nR2=0.02 1024 R2=0.32 R2=0.10\n1031\n1020 60\n1025\n1016\n1019 50\n1012\n1013\n108 40\n107 104\n30\n101 102 103 101 102 103 101 102 103\nSearch Budget Search Budget Search Budget\nFigure7: Computebudgetneededtosolveaproblemisbestpredictedbydescriptionlengthunder\ntheapproximateposterior,notprogramsizeorpriordescriptionlength,suggestingthatthefine-tuned\nmodelisnotengaginginblindguess-and-check.\n4.3 Out-of-distributiongeneralization\nOneadvantageofclassicsymbolicPBEmethodsisthattheydonotmakestatisticalassumptionsabout\ntheirtestproblems. Indeed,someclassicmethodscan,withintheirdomains,synthesizeprograms\nperfectly(i.e. alwaysfindaprogramthatfitsthetraininginput-outputs). Incontrast,neuralnetworks\ncanstruggletogeneralizebeyondthetrainingdistribution.\nWethereforeconsidertrain/testsplitsthatforcethemodeltogeneralizebeyondthedistributionof\nitstrainingdata(beyondD ). Ontextediting,weseedwithSyGuSproblems,andperformout-\nseed\nof-distributiontestingonPROSEproblems(PROSEismuchharderthanSyGuS).Onlistfunctions,\nweseedwithproblemsfromRuleetal. 2024andtestonShietal. 2023(theShidatasetcontains\nunusualcombinators,suchasScan). OnLOGO,weseedwithshortprograms(≤12linesofcode),\nandtestonlongprograms(>12linesofcode). Usingthesesplitswealsomeasuretheabilityofthe\nadaptationmethodinSec.3toimproveout-of-distributiongeneralization.4\nFig.8showsthatthereisnontrivialdegradationwhentestingoutofdistribution. Forexample,a7B\nmodelseededwithPROSEproblemsandtestedonadifferentsubsetofPROSEhasanaccuracyof\n76%(Fig.3b),butthisdegradesto59%whenseededwithSyGuSproblems,whichfollowadifferent\ndistributionandaregenerallysimplerandeasierthanPROSE(Fig.8b).\n4Weworkherewith7BmodelsbecauseSec.4.1foundthatfine-tuned33Bmodelsareonlyslightlybetter\nthan7B,and7Bischeapertorun.\n7\nroirP\nmargorP\ngoL\nevitageN\nroiretsoP\nmargorP\ngoL\nevitageN\neziS\ntsA\nmargorP\nWefurtherperformtheadaptationmethoddescribedinSec.3inordertomeasuretheextenttowhich\nitcannarrowthesedomaingaps. Ineverycaseitallowssolvingmoreout-of-distributionproblems,\nincreasingabsoluteperformancebyaround10%ormoreinalldomains,whichisarelativeincrease\nofabout16%fortext/listandarelativeincreaseofabout190%forLOGO(approximatelytripling\nthenumberofsolvedLOGOproblems).\n70\n70 80\n60\n60\n50 60\n50\n40\n40\n40\n30\nbefore adaptation 30 before adaptation\n20 adaptation adaptation 20 before adapation\nfinetuned 20 finetuned adaptation\n10 in-distribution in-distribution finetuned in-distribution\n0\n0 500 1000 1500 2000 0 200 400 0 500 1000 1500 2000\nSearch Budget (Num Samples) Search Budget (Num Samples) Search Budget (Num Samples)\n(a)RuleadaptedtoShi (b)SyGuSadaptedtoPROSE (c)LOGOshortadaptedtolong\nFigure8: Out-of-distributiongeneralizationandadaptationtonewtestdistribution.\nTobetterunderstandthedynamicsofadaptation,wevisualizethespecificproblemssolvedbeforeand\nafteradaptationonLOGOgraphics(Fig.9). Beforeadaptation,onlyahandfulofout-of-distribution\nproblemsaresolvable,andonlywithasignificantsearchbudget. Adaptationallowsthesystemto\nquicklysolvesimilarout-of-distributionproblemsinthefuture, butdoesnotallowthesystemto\ngeneralizetoproblemsveryunlikethoseoriginallysolvablebythefine-tunedmodel. Inprinciple,\nexpandingtheinference-timecomputebudgetshouldallowsuccessfuladaptation(largeK inEq.5).\nAnothermorecompute-efficientapproachwouldbetoincreasetheamountofadaptationdataby\nintroducing ‘steppingstone’ problemsin the adaptation set thatgive a gentler transitionfrom the\noriginaltrainingdistribution.\nSolved After Adapt Solved when also trained\nSolved Before Adapt\nwith long programs\nFigure9:Out-of-distributionLOGOproblems(requiringlongprogramswith>12linesofcode). We\nshowexampleproblemsthataresolvedbytheoriginalmodelfine-tunedonshortprograms,which\nthenbecometrainingdataforthenextroundofadaptation. Adaptationallowsconsistentsolving\nofproblemssimilartothosethattheoriginalfine-tunedmodelcouldsometimessolve,butisnota\npanacea: Problemsdissimilartothosesolvedbytheinitialmodelarenotevercorrectlygenerated,\ndespitethefactthattheyaresolvablebyamodelfine-tunedin-distribution.\n8\ndevloS\nsmelborP\n%\ndevloS\nsmelborP\n%\ndevloS\nsmelborP\n%\n5 RelatedWork\nAutomaticdatagenerationwithLLMs,suchasself-instruct[32],WizardCoder[50],andmany\nothers[51,52,53,i.a.],worksbypromptinganLLMtoproduceoutputswhicharethenusedforlater\nlearningstagessuchasfine-tuning. Theseapproachesareappliedrecursivelytotheirownoutput:\nPreviouslygenerateddataisincorporatedintofutureprompts. WesimilarlygenerateadatasetD\ntune\nbypromptinganLLMwithD ,but(1)donotrecursivelyprompttheLLMwithitsownoutputs\nseed\nand(2)combinetheLLMgenerationswithprogramexecutiontomakeprogramoutputs. Thisgives\nadifferentmathematicalinterpretationtoourdatagenerator. First,theprogramsaresamplesfrom\naprior,G,definedbyD ,whichwouldnotbeavalidinterpretationiftheLLMwasrepeatedly\nseed\nfeditsownoutputs. Second,thereisanobservationmodelorlikelihoodfunction,p(Y|ρ,X),which\nisdefinednotbytheLLM,butbyaPythoninterpreter. Inthisway,ourdatageneratorconstructs\ntrainingexamplesforfine-tuningthatteachthenetworkhowtoinverttheexecutionprocessofthe\nPythoninterpreter.\nMachinelearningappliedtoPBEhasoftensoughttoacceleratesearch: tofindanyprogramatall\nconsistentwiththeI/Oexamples[19,20,45,21,35,22,12,54,55,56],whichisnontrivialduetothe\ncombinatorialnatureofthesearch,evenafterconfiningtoadomain-specificprogramminglanguage.\nAcomplementarylineofresearchexploresinductivebiasesthatfavorprogramslikelytogeneralize\ntonewinputs,suchaslearningapriororrankingfunction[57,41,58,59]. Ourworkshouldbeseen\nwithinthetraditionoflearningtosearchforprograms. Weshowthatfinetunedmodelsserveasan\neffectiveyetsimplefoundationforacceleratingsearchinPBE,allowingsearchtobetractableover\nmuchricherandmoreexpressivelanguagessuchasPython.\nClassicPBE. Traditionalapproachestoprogramming-by-examplesoperatebysymbolicallysearch-\ningorsolvingforprogramsconsistentwiththeinput-outputexamples[13,49,2,1,37,6]. Theyuse\ndomain-specificprogramminglanguagesthataredesignedtoeitherenableefficientsearchand/orbias\nthesystemtowardfunctionsthatarelikelytogeneralizenewinputs. Searchforprogramscanevenbe\npolynomialtimewhenthisdomain-specificlanguagehasaspecialstructure(roughly,whenevery\nfunctioncanbe‘inverted’),akeyenablerofFlashFill,thefirstcommercialsuccessofPBE[4,2].\nLLMsasinductivereasoners. UsinganLLMtoperforminductivereasoning—togenerateabstract\nhypothesesfromconcretespecificexamples—hasbeenexploredbyseveralrecentworks[29,30,60,\n61],allofwhichhasfoundsignificantvalueintranslatingthesehypothesesintoprograms,andallof\nwhichhaveworkedbypromptingpretrainedGPT-stylemodels. Ourworkcanbeseenashelping\nansweranaturalquestionposedbythesepreviousworks: GiventhatLLMscangeneratehypotheses\nfromexamples,cantheyproduceprogramsofthenatureandcomplexitydemandedbyPBE?Wefind\nthisislargelythecaseafterfine-tuning,bothforclassicPBEdomainsandunusualones.\nSelf-Debugging,Refinement,andSelf-repair. Onewayofimprovingthecodegenerationabilities\nofanLLMistohaveitattempttodebugitsowncodewhenevertheinitiallygeneratedcodedoesnot\npasstheprovidedtestcases[62,63,64,65,66,24,67]. Wedidnotexplorethisstrategy,however,\nbecauseamorebasicapproachthatsimplyregeneratedanewprogramfromscratchalreadysurpassed\nthepriorstateoftheart(bothsymbolicandneuralbaselines),providedwefinetune. However,further\npushingtheboundaryofPBEmaybenefitfromself-debuggingstrategies.\nRankingLLM-generatedcode. Pastworkconsidersavarietyofwaystoselectanoutputfroma\ncollectionofLLM-sampledprograms[23,59,68,11],manyofwhicharemoresophisticatedthan\nsimplyfilteringbytheexamples,whichiswhatwedohere. Likewithself-debugging,integrating\nthesetechniquesshouldbesynergisticwithourapproach.\n6 Limitations\nOurworkhasimportantlimitations. Fromanengineeringperspective,usinga7B-33Bneuralnetwork\ntoperformPBEisnotpracticalformostend-users,whomaybedoingPBEontheirlaptopordesktops\ninordertoaccomplishsmallone-offtasks. Forthisreason,truedeploymenttoend-usersmayrequire\n9\ninvestigatingtheeffectivenessofmuchsmallerneuralnetworks(notanLLM),anditmayalsobe\nvaluabletostudytheeffectofnetworkcompressionanddistillationuponourfinetunedmodels.\nFromtheperspectiveofunderstandingwhereandwhyoursystemsucceedsandfails,wehaveshown\nthatneitherprogramsizenorlikelihoodunderthepriorsufficetopredictsuccess,findingtheposterior\nlikelihood is a better predictor, albeit an imperfect one. Although this allows us to discard the\nhypothesisthatthesystemismerelysamplingfromtheprior,italsojustpushesthequestionback\nonestagefurther: Whatexactlyaboutspecificproblemscausestheneuralnetwork’sapproximate\nposteriortoputmoreorlessprobabilitymassoncorrectsolutions? WhileinclassicPBEonecan\nobtainsharpanswersastowhyacertainproblemwassolvedornot,thisisamuchharderquestion\nwithneuralnetworks,whoseworkingsaremoreopaque.\n7 Discussion\nPBEwithfine-tunedLLMsissurprisinglyeffective,surpassingmanyofthebestneuralandsym-\nbolicbaselinesweknowof,evenforuncommondomainssuchasLOGOgraphics. Whyisthat?\nFundamentally,theneuralnetworkonlyneedstoactasaheuristicproposerofsolutions,because\nwecancheckagainsttheinput-outputs. Therefore,onepossibleexplanationisthatthetendencyof\nlanguagemodelstoover-generate,hallucinate,andcoverthelongtailofpossibilitiesisactuallyan\nasset,insteadofaliability. Andalthoughthereisadegreeofdegradationonout-of-sampleproblems,\nthedegradationisnotsoseverethatout-of-distributionproblemsbecomeutterlyunsolvable: Instead,\ntheymerelybecomehardertosolve,aphenomenonthatallowsadaptationtoworkinthefirstplace.\nSimultaneouslyoneshouldbehesitantaboutclaimingthatPBEis‘solved.’ Optimistically,current\nPBEbenchmarksexisttotestthefrontierofwhatispossible,andsodoingwellonthosebenchmarks\nmightjustmeanthatthefrontierhasmoved. Morerealistically,determiningifanAIsystemtruly\nworksinthewildrequiresmorethanjustpushingbenchmarknumbers,whichcanbemisleading\nwhenthosebenchmarksdonotcapturethelongtailofnaturally-occurringtests. Furthermore,all\nAIsystemspresenttradeoffs,andaneuralsystem’sunpredictability,highcomputationalcost,and\nout-of-distributionfragilityshouldbeweighedagainstwhateverhighbenchmarknumberstheymay\nachieve. Despitethesecaveats,weareoptimisticaboutthepromiseoftuningLLMsforPBE,and\nbelieve that it has the potential to dramatically expand the scope of solvable problems and even\nsolvabledomains.\nAcknowledgements. WearegratefulforassistancefromJoshuaRuleintheprocessingofthelist\nfunctionsdata,andforfeedbackfromYewenPuonthemanuscript. Thisworkwassupportedbyan\nNSFCAREERgrantaswellasgiftsfromGoogleandCisco.\nReferences\n[1] HenryLieberman. Yourwishismycommand: Programmingbyexample. MorganKaufmann,\n2001.\n[2] SumitGulwani. Automatingstringprocessinginspreadsheetsusinginput-outputexamples.\nIn Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of\nProgrammingLanguages,POPL’11,page317–330,NewYork,NY,USA,2011.Association\nfor Computing Machinery. ISBN 9781450304900. doi: 10.1145/1926385.1926423. URL\nhttps://doi.org/10.1145/1926385.1926423.\n[3] SumitGulwani. Automatingstringprocessinginspreadsheetsusinginput-outputexamples.\nSIGPLANNot.,46(1):317–330,jan2011. ISSN0362-1340. doi: 10.1145/1925844.1926423.\nURLhttps://doi.org/10.1145/1925844.1926423.\n[4] Oleksandr Polozov and Sumit Gulwani. Flashmeta: A framework for inductive program\nsynthesis. SIGPLANNot.,50(10):107–126,oct2015. ISSN0362-1340. doi: 10.1145/2858965.\n2814310. URLhttps://doi.org/10.1145/2858965.2814310.\n[5] XinyunChen,PetrosManiatis,RishabhSingh,CharlesSutton,HanjunDai,MaxLin,andDenny\nZhou. Spreadsheetcoder: Formulapredictionfromsemi-structuredcontext. InInternational\nConferenceonMachineLearning,pages1661–1672.PMLR,2021.\n10\n[6] Sumit Gulwani, José Hernández-Orallo, Emanuel Kitzelmann, Stephen H Muggleton, Ute\nSchmid,andBenjaminZorn. Inductiveprogrammingmeetstherealworld. Communicationsof\ntheACM,58(11):90–99,2015.\n[7] FrançoisChollet. Onthemeasureofintelligence,2019.\n[8] StephenHMuggleton,UteSchmid,ChristinaZeller,AlirezaTamaddoni-Nezhad,andTarek\nBesold.Ultra-strongmachinelearning:comprehensibilityofprogramslearnedwithilp.Machine\nLearning,107(7):1119–1140,2018.\n[9] JSolomonoRaymond. Aformaltheoryofinductiveinferencei. InformationandControl,7:\n1–22,1964.\n[10] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept\nlearningthroughprobabilisticprograminduction. Science,350(6266):1332–1338,2015.\n[11] YujiaLi,DavidChoi,JunyoungChung,NateKushman,JulianSchrittwieser,RémiLeblond,\nTomEccles,JamesKeeling,FelixGimeno,AgustinDalLago,etal. Competition-levelcode\ngenerationwithalphacode. Nature,2022.\n[12] KensenShi,HanjunDai,Wen-DingLi,KevinEllis,andCharlesSutton. Lambdabeam: Neural\nprogram search with higher-order functions and lambdas. In Thirty-seventh Conference on\nNeuralInformationProcessingSystems,2023. URLhttps://openreview.net/forum?id=\nqVMPXrX4FR.\n[13] JohnK.Feser,SwaratChaudhuri,andIsilDillig. Synthesizingdatastructuretransformations\nfrom input-output examples. In Proceedings of the 36th ACM SIGPLAN Conference on\nProgramming Language Design and Implementation, PLDI ’15, page 229–239, New York,\nNY,USA,2015.AssociationforComputingMachinery. ISBN9781450334686. doi: 10.1145/\n2737924.2737977. URLhttps://doi.org/10.1145/2737924.2737977.\n[14] Nathanaël Fijalkow, Guillaume Lagarde, Théo Matricon, Kevin Ellis, Pierre Ohlmann, and\nAkarsh Nayan Potta. Scaling neural program synthesis with distribution-based search. In\nProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages6623–6630,\n2022.\n[15] Kensen Shi, Joey Hong, Yinlin Deng, Pengcheng Yin, Manzil Zaheer, and Charles Sutton.\nExedec: Executiondecompositionforcompositionalgeneralizationinneuralprogramsynthesis.\nInInternationalConferenceonLearningRepresentations,2024.\n[16] AnsongNi,MiltiadisAllamanis,ArmanCohan,YinlinDeng,KensenShi,CharlesSutton,and\nPengchengYin. Next: Teachinglargelanguagemodelstoreasonaboutcodeexecution,2024.\n[17] JoshuaS.Rule,StevenT.Piantadosi,AndrewCropper,KevinEllis,MaxwellNye,andJoshuaB.\nTenenbaum. Symbolic metaprogram search improves learning efficiency and explains rule\nlearninginhumans. NatureCommunications,2024.\n[18] OpenAI. Gpt-4technicalreport,2023.\n[19] AshwinKalyan,AbhishekMohta,OleksandrPolozov,DhruvBatra,PrateekJain,andSumit\nGulwani. Neural-guideddeductivesearchforreal-timeprogramsynthesisfromexamples. arXiv\npreprintarXiv:1804.01186,2018.\n[20] JacobDevlin,JonathanUesato,SuryaBhupatiraju,RishabhSingh,Abdel-rahmanMohamed,\nandPushmeetKohli. Robustfill: Neuralprogramlearningundernoisyi/o. InProceedingsof\nthe34thInternationalConferenceonMachineLearning-Volume70,ICML’17,page990–998.\nJMLR.org,2017.\n[21] XinyunChen,ChangLiu,andDawnSong. Execution-guidedneuralprogramsynthesis. In\nInternationalConferenceonLearningRepresentations,2018.\n[22] KensenShi,HanjunDai,KevinEllis,andCharlesSutton. Crossbeam: Learningtosearchin\nbottom-upprogramsynthesis. InInternationalConferenceonLearningRepresentations,2021.\n11\n[23] BeiChen,FengjiZhang,AnhNguyen,DaoguangZan,ZeqiLin,Jian-GuangLou,andWeizhu\nChen. Codet: Codegenerationwithgeneratedtests. ICLR,2023.\n[24] HungLe,YueWang,AkhileshDeepakGotmare,SilvioSavarese,andStevenHoi. CodeRL:\nMastering code generation through pretrained models and deep reinforcement learning. In\nAliceH.Oh,AlekhAgarwal,DanielleBelgrave,andKyunghyunCho,editors,Advancesin\nNeuralInformationProcessingSystems,2022. URLhttps://openreview.net/forum?id=\nWaGvb7OzySA.\n[25] EricZelikman,QianHuang,GabrielPoesia,NoahGoodman,andNickHaber. Parsel: Algo-\nrithmicreasoningwithlanguagemodelsbycomposingdecompositions. AdvancesinNeural\nInformationProcessingSystems,36:31466–31523,2023.\n[26] DanHendrycks,StevenBasart,SauravKadavath,MantasMazeika,AkulArora,EthanGuo,\nCollinBurns,SamirPuranik,HoraceHe,DawnSong,andJacobSteinhardt. Measuringcoding\nchallengecompetencewithapps. NeurIPS,2021.\n[27] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveiraPinto,Jared\nKaplan,HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman,etal. Evaluatinglarge\nlanguagemodelstrainedoncode. arXivpreprintarXiv:2107.03374,2021.\n[28] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and LINGMING ZHANG. Is your code\ngeneratedbychatGPTreallycorrect? rigorousevaluationoflargelanguagemodelsforcode\ngeneration. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023.\nURLhttps://openreview.net/forum?id=1qvx610Cu7.\n[29] LinluQiu,LiweiJiang,XimingLu,MelanieSclar,ValentinaPyatkin,ChandraBhagavatula,\nBailinWang,YoonKim,YejinChoi,NouhaDziri,andXiangRen. Phenomenalyetpuzzling:\nTestinginductivereasoningcapabilitiesoflanguagemodelswithhypothesisrefinement. ICLR,\n2024.\n[30] Kevin Ellis. Human-like few-shot learning via bayesian reasoning over natural language.\nNeurIPS,2023.\n[31] RuochengWang,EricZelikman,GabrielPoesia,YewenPu,NickHaber,andNoahD.Goodman.\nHypothesissearch: Inductivereasoningwithlanguagemodels. ICLR,2024.\n[32] YizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,NoahA.Smith,DanielKhashabi,\nandHannanehHajishirzi. Self-instruct: Aligninglanguagemodelswithself-generatedinstruc-\ntions,2023.\n[33] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The\" wake-sleep\"\nalgorithmforunsupervisedneuralnetworks. Science,268(5214):1158–1161,1995.\n[34] Peter-Michael Osera and Steve Zdancewic. Type-and-example-directed program synthesis.\nACMSIGPLANNotices,50(6):619–630,2015.\n[35] MatejBalog,AlexanderL.Gaunt,MarcBrockschmidt,SebastianNowozin,andDanielTarlow.\nDeepcoder: Learningtowriteprograms. InInternationalConferenceonLearningRepresenta-\ntions,2017. URLhttps://openreview.net/forum?id=ByldLrqlx.\n[36] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,\nAdam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.\nBeyondtheimitationgame: Quantifyingandextrapolatingthecapabilitiesoflanguagemodels.\narXivpreprintarXiv:2206.04615,2022.\n[37] TessaLau,StevenAWolfman,PedroDomingos,andDanielSWeld. Programmingbydemon-\nstrationusingversionspacealgebra. MachineLearning,53:111–156,2003.\n[38] JoséCambronero,SumitGulwani,VuLe,DanielPerelman,ArjunRadhakrishna,ClintSimon,\nand Ashish Tiwari. Flashfill++: Scaling programming by example by cutting to the chase.\nProceedingsoftheACMonProgrammingLanguages,7(POPL):952–981,2023.\n12\n[39] Microsoftprosepublicbenchmarksuite,2022. Availableathttps://github.com/microsoft/prose-\nbenchmarks.\n[40] JiayuanMao,XiumingZhang,YikaiLi,WilliamT.Freeman,JoshuaB.Tenenbaum,andJiajun\nWu. Program-GuidedImageManipulators. InInternationalConferenceonComputerVision,\n2019.\n[41] KevinEllisandSumitGulwani. Learningtolearnprogramsfromexamples: Goingbeyond\nprogramstructure. InIJCAI,pages1638–1645,2017.\n[42] DavidD.Thornburg. Friendsoftheturtle. Compute!,March1983.\n[43] CatherineWong,KevinEllis,JoshuaB.Tenenbaum,andJacobAndreas. Leveraginglanguage\ntolearnprogramabstractionsandsearchheuristics. InICML,2021.\n[44] DayaGuo,QihaoZhu,DejianYang,ZhendaXie,KaiDong,WentaoZhang,GuantingChen,\nXiaoBi,Y.Wu,Y.K.Li,FuliLuo,YingfeiXiong,andWenfengLiang. Deepseek-coder: When\nthelargelanguagemodelmeetsprogramming–theriseofcodeintelligence,2024.\n[45] KevinEllis,CatherineWong,MaxwellNye,MathiasSablé-Meyer,LucasMorales,LukeHewitt,\nLucCary,ArmandoSolar-Lezama,andJoshuaB.Tenenbaum. DreamCoder: Bootstrapping\nInductiveProgramSynthesiswithWake-SleepLibraryLearning,page835–850. Association\nforComputingMachinery,NewYork,NY,USA,2021. ISBN9781450383912. URLhttps:\n//doi.org/10.1145/3453483.3454080.\n[46] EliasStengel-Eskin,ArchikiPrasad,andMohitBansal.Regal:Refactoringprogramstodiscover\ngeneralizableabstractions. arXivpreprintarXiv:2401.16467,2024.\n[47] Stephen H. Muggleton, Dianhuan Lin, and Alireza Tamaddoni-Nezhad. Meta-interpretive\nlearning of higher-order dyadic datalog: Predicate invention revisited. Mach. Learn., 100\n(1):49–73, jul 2015. ISSN 0885-6125. doi: 10.1007/s10994-014-5471-y. URL https:\n//doi.org/10.1007/s10994-014-5471-y.\n[48] StevenPiantadosi. Fleet. https://github.com/piantado/Fleet/,2023. [OnlineGitHub\nrepository].\n[49] RajeevAlur,RastislavBodik,GarvitJuniwal,MiloMKMartin,MukundRaghothaman,SanjitA\nSeshia,RishabhSingh,ArmandoSolar-Lezama,EminaTorlak,andAbhishekUdupa. Syntax-\nguidedsynthesis. IEEE,2013.\n[50] ZiyangLuo,CanXu,PuZhao,QingfengSun,XiuboGeng,WenxiangHu,ChongyangTao,Jing\nMa,QingweiLin,andDaxinJiang. Wizardcoder: Empoweringcodelargelanguagemodels\nwithevol-instruct,2023.\n[51] Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and\nLingpengKong. Zerogen: Efficientzero-shotlearningviadatasetgeneration. arXivpreprint\narXiv:2202.07922,2022.\n[52] AjayPatel,ColinRaffel,andChrisCallison-Burch. Datadreamer: Atoolforsyntheticdata\ngenerationandreproduciblellmworkflows,2024.\n[53] YuMeng,JiaxinHuang,YuZhang,andJiaweiHan. Generatingtrainingdatawithlanguage\nmodels:Towardszero-shotlanguageunderstanding.AdvancesinNeuralInformationProcessing\nSystems,35:462–477,2022.\n[54] KaviGupta,PeterEbertChristensen,XinyunChen,andDawnSong. Synthesize,executeand\ndebug: learningtorepairforneuralprogramsynthesis. InProceedingsofthe34thInternational\nConferenceonNeuralInformationProcessingSystems,NIPS’20,RedHook,NY,USA,2020.\nCurranAssociatesInc. ISBN9781713829546.\n[55] Shraddha Barke, Hila Peleg, and Nadia Polikarpova. Just-in-time learning for bottom-up\nenumerativesynthesis. Proc.ACMProgram.Lang., 4(OOPSLA),nov2020. doi: 10.1145/\n3428295. URLhttps://doi.org/10.1145/3428295.\n13\n[56] KevinEllis,MaxwellNye,YewenPu,FelixSosa,JoshuaB.Tenenbaum,andArmandoSolar-\nLezama. Write,Execute,Assess: ProgramSynthesiswithaREPL. CurranAssociatesInc.,Red\nHook,NY,USA,2019.\n[57] Rishabh Singh and Sumit Gulwani. Predicting a correct program in programming\nby example. In 27th International Conference on Computer Aided Verification (CAV\n2015),July2015. URLhttps://www.microsoft.com/en-us/research/publication/\npredicting-a-correct-program-in-programming-by-example/.\n[58] PercyLiang,MichaelI.Jordan,andDanKlein. Learningprograms: ahierarchicalbayesian\napproach. InProceedingsofthe27thInternationalConferenceonInternationalConference\nonMachineLearning,ICML’10,page639–646,Madison,WI,USA,2010.Omnipress. ISBN\n9781605589077.\n[59] Jeevana Priya Inala, Chenglong Wang, Mei Yang, Andres Codas, Mark Encarnación, Shu-\nvenduLahiri, MadanlalMusuvathi, andJianfengGao. Fault-awareneuralcoderankers. In\nS.Koyejo,S.Mohamed,A.Agarwal,D.Belgrave,K.Cho,andA.Oh,editors,Advancesin\nNeuralInformationProcessingSystems,volume35,pages13419–13432.CurranAssociates,\nInc.,2022. URLhttps://proceedings.neurips.cc/paper_files/paper/2022/file/\n5762c579d09811b7639be2389b3d07be-Paper-Conference.pdf.\n[60] Wasu Top Piriyakulkij and Kevin Ellis. Doing experiments and revising rules with natural\nlanguageandprobabilisticreasoning. CogSci,2024.\n[61] RuochengWang,EricZelikman,GabrielPoesia,YewenPu,NickHaber,andNoahGoodman.\nHypothesissearch: Inductivereasoningwithlanguagemodels. InTheTwelfthInternational\nConferenceonLearningRepresentations,2024. URLhttps://openreview.net/forum?\nid=G7UtIGQmjm.\n[62] XinyunChen, MaxwellLin, NathanaelSchärli, andDennyZhou. Teachinglargelanguage\nmodelstoself-debug. arXivpreprintarXiv:2304.05128,2023.\n[63] SeanWelleck,XimingLu,PeterWest,FaezeBrahman,TianxiaoShen,DanielKhashabi,and\nYejinChoi. Generatingsequencesbylearningtoself-correct. ICLR,2023.\n[64] TheoX.Olausson,JeevanaPriyaInala,ChenglongWang,JianfengGao,andArmandoSolar-\nLezama. Isself-repairasilverbulletforcodegeneration?,2023.\n[65] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\ndynamicmemoryandself-reflection. arXivpreprintarXiv:2303.11366,2023.\n[66] AmanMadaan,NiketTandon,PrakharGupta,SkylerHallinan,LuyuGao,SarahWiegreffe,Uri\nAlon,NouhaDziri,ShrimaiPrabhumoye,YimingYang,etal. Self-refine: Iterativerefinement\nwithself-feedback. AdvancesinNeuralInformationProcessingSystems,36,2024.\n[67] HaoTang,KeyaHu,JinPengZhou,SichengZhong,Wei-LongZheng,XujieSi,andKevin\nEllis. Coderepairwithllmsgivesanexploration-exploitationtradeoff. arXiv,2024.\n[68] AnsongNi,SriniIyer,DragomirRadev,VesStoyanov,Wen-tauYih,SidaIWang,andXiVicto-\nriaLin. Lever: Learningtoverifylanguage-to-codegenerationwithexecution. InProceedings\nofthe40thInternationalConferenceonMachineLearning(ICML’23),2023.\n[69] jinaai/jina-embeddings-v2-base-code. https://huggingface.co/jinaai/\njina-embeddings-v2-base-code. Accessed: 2024-05-22.\n14\nA Appendix/supplementalmaterial\nA.1 ExperimentDetails\nWeusedatemperatureof1.0forsamplinginourexperimentsunlessotherwisestated.Allexperiments\nwereperformedonsingle-nodemachines(8xA6000or8xA100,etc.)withoutamulti-nodedistributed\ncomputingsetup.\nA.1.1 ListTasks\nWe selected 50 problems from Rule et al. as our seed set, reserving the remaining problems for\ntesting. ToensurecomparabilitywithRuleetal.,wetestedonthefirst100problems(excludingthose\nintheseedset),resultingin77testproblems. Weconsistentlyused10input-outputexamples,with\ntheremaining54examplesservingasaheld-outtestset. Whenfilteringduplicatesyntheticdata,we\nemployedanopencodeembeddingmodel[69]availableonHuggingFace. Asasanitycheck,we\nalsouse10listtolistproblemsfromλ2benchmarkandshownthemodelcaneffectivelysolvethem\ninTable.4\nA.1.2 StringTasks\nWe utilized 100 string-to-string/null transformation problems from the prose-benchmark. When\navailable,weused10input-outputexamples,alwaysreservingatleastoneexampleasahold-outtest\nset. Thisensuresthegeneralizationofoursynthesizedprograms,asthebenchmarkdidnotprovide\nheld-outdata.\nFortheFlashFillbaseline,weusedMicrosoftExcelforMacversion16.8. Weopenedeachindividual\nxlsx file containing the test problem examples and manually triggered the FlashFill function by\npressingCtrl+E.\nA.1.3 LogoTasks\nTofacilitategraphicsinputinferencewithcodelanguagemodels,weconvertedlogographicsinto\nASCII-represented strings, as shown in Figure 5. For each input image, we cropped a 512x512\nsectionfromthecenterandthendivideditinto32x32blocks,eachwithasizeof16x16. Wecounted\nthenumberofblackpixelsineachblock,calculatedtheirdensity(numblackpixels),andquantizedthis\n16·16\ndensityvalueinto10levels, representedbytheASCIInumbers0-9. Byrepresentingeachblock\nwithanASCIInumber,aninputimageisrepresentedwithastringof32lines,andeachlinehas32\nnumbers.\nFor the turtle graphics program, we adopted the Python turtle graphics program from Regal[46]\nwithaminormodificationofchangingthe‘embed’functiontousea‘with’contextmanagerinstead,\ncallingit‘fork_state’. Thisallowsforequivalentbutmorereadablecode.\nFor the GPT-4o and GPT-4o mini multimodal baselines, we directly use images as inputs. (See\nSectionA.5.8fortheprompttemplate.)\nA.2 SyntheicDatasetGenerationandTrainingParameters\nWepresentthedatasetgenerationandtrainingparametersinTable.2andTable.3.\nA.3 AdaptationImplementationDetails\nForadaptationexperiments,wegenerallyfollowedthesettingsdescribedabove,withafewspecific\ndifferencesdetailedbelow.\nA.3.1 StringTasks\nTo induce a domain gap (easier problems in Sygus compared to the harder, noisier problems in\ntheProseBenchmark),wefirstfine-tunedamodelusingSygusproblemsandthentesteditonthe\nProseBenchmark. DuetothenoisynatureoftheProseBenchmark(someproblemshaveveryfew\nexamples),weadoptedasettingwhereweutilizedallthetestcasestoselectwhichproblemswere\n15\nList String\nSeedDatasetSource Ruleetal. Prose(FlashFill++)Problems\nSeedDatasetSize 50 40\nSyntheticDataGenerator deepseekcoder-33b-instruct deepseekcoder-33b-instruct\nSyntheticDatasetSize 10k 10k\nSamplingTempereature 0.8 1.0\nSimilarityFilter codeembeddingmodel -\nFilterRatio around1/3(threshold=0.93) -\nSyntheticDataPrompt 4-shotexamples 10-shotexamples\nLoRAFinetuning\nModelUsed deepseekcoder-1.5-7b-instruct deepseekcoder-1.5-7b-instruct\nLoRARank 1024 256\nLoRAα 1024 256\nLearningRate 2.00E-04 2.00E-04\nLRSchedule cosine cosine\nWarmupSteps 10 10\nEpoch 1 1\nBatchsize 32 16\nLoRA\n33bModelUsedforFT deepseekcoder-33b-instruct deepseekcoder-33b-instruct\nLoRA 256 128\nLoRAα 256 128\nLearningRate 2.00E-04 2.00E-04\nLRSchedule cosine cosine\nWarmupSteps 10 10\nEpoch 1 1\nBatchsize 32 32\nTable2: ListtaskandStringtasksyntheticdatasetgenerationandfinetuningparameters.\nsolvedandthenusedthemastheseedprogramsforadaptation. Thisresultedin64solvedproblems\noutofthe100problemsinthebenchmark.\nA.3.2 ListTasks\nToobtainthefinetunedin-distributionresultinFig.8a,wefine-tunedonasyntheticdatasetgenerated\nbyseedingwith20outof100problemsfromLambdaBeam,andtestedontheremaining80problems.\nA.3.3 LOGOTasks\nForLOGOadaptationexperiments,weinducedomaingapbyusingtheshorterprograms(LoC≤12)\nofthetrainingset,andtestedonthelongerprograms(LoC> 12). Theshorterprogramstraining\nseedconsistsofaround80%problems(156outof200)fromtheoriginaltrainingset. Thetestset\nconsistsof31problemsoutof111problemsfromtheoriginaltestset.\nA.4 ModelPerformanceonLambdaBeamBenchmark\nWepresenttheresultsofbothour7Band33BmodelsontheLambdaBeambenchmarkinFigure10.\nWeobservedthatevenwithoutfine-tuningforthisspecificbenchmark,andinsteadfine-tunedfor\nthelist-to-listproblemsfromRuleetal.,ourmodelsperformedexceptionallywell,surpassingthe\nstate-of-the-artresultsspecificallydesignedfortheLambdaBeamproblems[12].\nA.5 PromptsUsedintheExperiments\nA.5.1 SyntheicDataGenerationPrompt\nList\n16\nLogo\nSeedDatasetSource RegalPythonLogoPrograms\nSeedDatasetSize 200\nSyntheticDataGenerator deepseekcoder-33b-instruct\nSyntheticDatasetSize 32k\nSimilarityFilter -\nFilterThreshold -\nSyntheticDataPrompt 6-shotexamples\nLoRAFinetuning\nModelUsed deepseekcoder-1.5-7b-instruct\nLoRARank 512\nLoRAα 512\nLearningRate 2.00E-04\nLRSchedule cosine\nWarmupSteps 20\nEpoch 3\nBatchsize 64\nLoRAFinetuning\nModelUsed deepseekcoder-33b-instruct\nLoRARank 512\nLoRAα 512\nLearningRate 2.00E-04\nLRSchedule cosine\nWarmupSteps 50\nEpoch 3\nBatchsize 64\nTable3: Logotasksyntheticdatasetsgenerationandfinetuningparameters\n70\nLambdaBeam\n60\n50\nRobustFill\n40 L2\n30\n20\nours-33b\n10\nours-7b\n0 250 500 750 1000\nSearch Budget (Num Samples)\nFigure10: PerformanceonLambdaBeamproblemsfromShietal. 2023\n17\ndevloS\nsmelborP\n%\nYou are a CS professor. You are providing a set of challenging\nand diverse integer list to integer list function puzzle\nfor your student to solve.\nPuzzle 1:\nPython function: input a list of integers and return a list of\nintegers\n‘‘‘python\n{PROGRAM EXAMPLE 1}\n‘‘‘\nTest cases:\n...\nPuzzle 2:\nPython function: input a list of integers and return a list of\nintegers\n‘‘‘python\n{PROGRAM EXAMPLE 2}\n‘‘‘\nTest cases:\n...\nFollowing the above format, please provide 3 functions each\nfollow by 10 random test cases to check the function’s\ncorrectness full coverage\nString\nExcel just introduce a new feature that allows user to use\nPython to perform data transformation.\nPlease generate a csv file with two columns. The first column\ncontains the input data and the second column contains the\noutput data.\nFollowing a accompanying python function which showcasing the\ntransformation of the input data to the output data.\nHere are 10 challenging examples showcaing the features\nExample 1\n‘‘‘csv\n{INPUT OUTPUT EXAMPLE 1}\n‘‘‘\nHere is the Python function that help transform the first\ncolumn to the second column.\n‘‘‘python\n{PYTHON EXAMPLE 1}\n‘‘‘\nExample 2\n‘‘‘csv\n{INPUT OUTPUT EXAMPLE 2}\n‘‘‘\nHere is the Python function that help transform the first\ncolumn to the second column.\n‘‘‘python\n{PYTHON EXAMPLE 2}\n‘‘‘\n...\n18\nFollowing the above format, please provide a CSV file with two\ncolumns, containing between 5 to 10 rows of data showing a\ntransformation from the first column to the second column.\nThis csv data should illustrate a challenging and complex\nexample, similar to the above examples. Following that,\ncreate a Python function designed to process this data. Be\naware that this function will be tailored to not only\naccommodate the current data but also any future data that\nfollows the same format or structure.\nLogo\nYour task is to draw simple black and white graphics with the\ncustom library. DO NOT USE THE BUILT-IN TURTLE LIBRARY.\nYou will use a custom turtle library, similar to the built-in\nturtle library, which is sufficient for all tasks.\nHere are all the available functions in the custom turtle\nlibrary:\n- forward(x): move forward x pixels\n- left(theta): rotate left by theta degrees\n- right(theta): rotate right by theta degrees\n- penup(): stop drawing\n- pendown(): start drawing\n- teleport(x, y, theta): move to position (x, y) with angle\ntheta\n- heading(): get the current angle of the turtle\n- isdown(): check if the pen is down\n- forward(x): Move forward x pixels.\n- left(theta): Rotate left by theta degrees.\n- right(theta): Rotate right by theta degrees.\n- penup(): Stop drawing.\n- pendown(): Start drawing.\n- teleport(x, y, theta): Move to position (x, y) with angle\ntheta.\n- heading(): Get the current angle of the turtle.\n- isdown(): Check if the pen is down.\n- with fork_state(): A context manager that runs the code in\nthe block using the current context and restores the\noriginal state afterwards. Allows you to nest programs.\nInternally, fork_state saves the turtle state (is_down, x,\ny, heading), executes the block, then restores the original\nstate.\nGraphic 1\nPython program: draw an interesting graphic using our own\ncustom turtle library\n# the following program draws ...\n{PROGRAM EXAMPLE 1}\nGraphic 2\nPython program: draw an interesting graphic using our own\ncustom turtle library\n# the following program draws ...\n{PROGRAM EXAMPLE 2}\n...\nFollowing the above format, please provide 5 more programs\nusing our custom drawing library.\n19\nA.5.2 PromptTemplateforFinetuningandZero-ShotExperiments\nA.5.3 List\nImplement the function solve_puzzle that takes a list of\nintegers and returns a list of integers. The function\nshould satisfy the following assertions\nassert solve_puzzle(...) == ...\nassert solve_puzzle(...) == ...\nassert solve_puzzle(...) == ...\n...\nA.5.4 List(Chain-of-Thought)\nImplement the function solve_puzzle that takes a list of\nintegers and returns a list of integers. The function\nshould satisfy the following assertions:\nassert solve_puzzle(...) == ...\nassert solve_puzzle(...) == ...\nassert solve_puzzle(...) == ...\n...\nPlease observe the relation between input and output and think\nstep by step. Output the function in a markdown format in\nthe end.\nSolution:\nA.5.5 String\nImplement the function edit_text that takes a string and\nreturns a string. The function transforms the input string\nto the output string. The function should satisfy the\nfollowing assertions:\nassert edit_text(...) == ...\nassert edit_text(...) == ...\nassert edit_text(...) == ...\n...\nA.5.6 String(Chain-of-Thought)\nPlease implement the function edit_text that takes a string\ninput and returns a modified string.\nNote that you can import re, datetime, or any built-in Python\nlibrary to solve the problem.\nThe function should satisfy the following test cases:\nassert edit_text(...) == ...\nassert edit_text(...) == ...\nassert edit_text(...) == ...\n...\nPlease reason through the problem and think step by step, and\nfinally implement the function and output the full function\nimplementation in a markdown code block in the end.\nA.5.7 Logo\nHere is a gray scale images representing with integer values\n0-9.\n{CONVERTED IMAGE STRING}...\n20\n...\nPlease write a Python program that generates the image using\nour own custom turtle module\nA.5.8 Logo(MultimodalFew-shot)\nGiven the following custom Turtle graphics-like library, Please\nuse it to write a program that draws the given image.\n‘‘‘python\nfrom myturtle import Turtle\nfrom myturtle import HALF_INF, INF, EPS_DIST, EPS_ANGLE\nturtle = Turtle()\ndef forward(dist):\nturtle.forward(dist)\ndef left(angle):\nturtle.left(angle)\ndef right(angle):\nturtle.right(angle)\ndef teleport(x, y, theta):\nturtle.teleport(x, y, theta)\ndef penup():\nturtle.penup()\ndef pendown():\nturtle.pendown()\ndef position():\nreturn turtle.x, turtle.y\ndef heading():\nreturn turtle.heading\ndef isdown():\nreturn turtle.is_down\ndef fork_state():\n\"\"\"\nFork the current state of the turtle.\nUsage:\nwith fork_state():\nforward(100)\nleft(90)\nforward(100)\n\"\"\"\nreturn turtle._TurtleState(turtle)\n‘‘‘\nBelow are some example programs that draw the given images.\n<IMAGE>\nHere is a program that draws the above image.\nThe figure is like a medium 8 gon.\n‘‘‘python\nfor i in range(8):\nforward(4)\nleft(45.0)\n‘‘‘\n<IMAGE>\nHere is a program that draws the above image:\nThe figure is like 5 sided snowflake with a medium circle and a\nmedium semicircle as arms.\n‘‘‘python\nfor j in range(5):\nwith fork_state():\npenup()\nforward(2)\n21\nleft(0.0)\npendown()\nfor i in range(HALF_INF):\nforward(EPS_DIST*2)\nleft(EPS_ANGLE)\nfor i in range(HALF_INF):\nforward(EPS_DIST*2)\nleft(EPS_ANGLE)\npenup()\nforward(2)\nleft(0.0)\npendown()\nfor i in range(HALF_INF):\nforward(EPS_DIST*2)\nleft(EPS_ANGLE)\nforward(0)\nleft(72.0)\"\n‘‘‘\n<IMAGE>\nHere is a program that draws the above image.\nThe figure is like 7 concentric circles.\n‘‘‘python\nfor j in range(8):\nfor i in range(HALF_INF):\nforward(EPS_DIST*j)\nleft(EPS_ANGLE)\nfor i in range(HALF_INF):\nforward(EPS_DIST*j)\nleft(EPS_ANGLE)\n‘‘‘\n<TEST_IMAGE>\nOutput the program that draws the following image. Reason about\nthe given image and write a program that draws it in a\nmarkdown code block.\nName Description ours-7B ours-33B\nDedup Removeduplicateelementsfromalist. ✓ ✓\nReverse Reversealist. ✓ ✓\nDroplast Dropthelastelementinalist. ✓ ✓\nDropmax Dropthelargestnumber(s)inalist. ✓ ✓\nDupli Duplicateeachelementofalist. ✓ ✓\nEvens Removetheoddnumbersfromalist. ✓ ✓\nMultfirst Replaceeveryiteminalistwiththefirstitem. ✓ ✓\nMultlast Replaceeveryiteminalistwiththelastitem. ✓ ✓\nShiftl Shiftallelementsinalisttotheleft. ✓ ✓\nShiftr Shiftallelementsinalisttotheright. ✓ ✓\nTable4: 10list(cid:55)→listfunctionsfromλ2[13]\n22\nFigure11: AllLOGOtestproblems: problemssolvedbyourfinetuned33bmodelaremarkedas\ngreen\n23\nFigure12: Hand-drawnLOGOtestshowingeverygeneratedsample. Webuiltagraphicalinterface\ntoallowuserstodrawimagesasinput. Thesamplebudgetforthisdemois64.\n24\nNeurIPSPaperChecklist\n1. Claims\nQuestion: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe\npaper’scontributionsandscope?\nAnswer: [Yes]\nJustification: Weprovideexperimentresultstosupportourclaims.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmadeinthepaper.\n• Theabstractand/orintroductionshouldclearlystatetheclaimsmade,includingthe\ncontributionsmadeinthepaperandimportantassumptionsandlimitations. ANoor\nNAanswertothisquestionwillnotbeperceivedwellbythereviewers.\n• Theclaimsmadeshouldmatchtheoreticalandexperimentalresults,andreflecthow\nmuchtheresultscanbeexpectedtogeneralizetoothersettings.\n• Itisfinetoincludeaspirationalgoalsasmotivationaslongasitisclearthatthesegoals\narenotattainedbythepaper.\n2. Limitations\nQuestion: Doesthepaperdiscussthelimitationsoftheworkperformedbytheauthors?\nAnswer: [Yes]\nJustification: Wediscussseverallimitationsinthelimitationsection.\nGuidelines:\n• TheanswerNAmeansthatthepaperhasnolimitationwhiletheanswerNomeansthat\nthepaperhaslimitations,butthosearenotdiscussedinthepaper.\n• Theauthorsareencouragedtocreateaseparate\"Limitations\"sectionintheirpaper.\n• Thepapershouldpointoutanystrongassumptionsandhowrobusttheresultsareto\nviolationsoftheseassumptions(e.g.,independenceassumptions,noiselesssettings,\nmodelwell-specification,asymptoticapproximationsonlyholdinglocally).Theauthors\nshouldreflectonhowtheseassumptionsmightbeviolatedinpracticeandwhatthe\nimplicationswouldbe.\n• Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproachwas\nonlytestedonafewdatasetsorwithafewruns. Ingeneral,empiricalresultsoften\ndependonimplicitassumptions,whichshouldbearticulated.\n• Theauthorsshouldreflectonthefactorsthatinfluencetheperformanceoftheapproach.\nForexample,afacialrecognitionalgorithmmayperformpoorlywhenimageresolution\nisloworimagesaretakeninlowlighting. Oraspeech-to-textsystemmightnotbe\nusedreliablytoprovideclosedcaptionsforonlinelecturesbecauseitfailstohandle\ntechnicaljargon.\n• Theauthorsshoulddiscussthecomputationalefficiencyoftheproposedalgorithms\nandhowtheyscalewithdatasetsize.\n• If applicable, the authors should discuss possible limitations of their approach to\naddressproblemsofprivacyandfairness.\n• Whiletheauthorsmightfearthatcompletehonestyaboutlimitationsmightbeusedby\nreviewersasgroundsforrejection,aworseoutcomemightbethatreviewersdiscover\nlimitationsthataren’tacknowledgedinthepaper. Theauthorsshouldusetheirbest\njudgmentandrecognizethatindividualactionsinfavoroftransparencyplayanimpor-\ntantroleindevelopingnormsthatpreservetheintegrityofthecommunity. Reviewers\nwillbespecificallyinstructedtonotpenalizehonestyconcerninglimitations.\n3. TheoryAssumptionsandProofs\nQuestion: Foreachtheoreticalresult,doesthepaperprovidethefullsetofassumptionsand\nacomplete(andcorrect)proof?\nAnswer: [NA]\n25\nJustification: Thepaperdoesnotincludetheoreticalresults.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotincludetheoreticalresults.\n• Allthetheorems, formulas, andproofsinthepapershouldbenumberedandcross-\nreferenced.\n• Allassumptionsshouldbeclearlystatedorreferencedinthestatementofanytheorems.\n• Theproofscaneitherappearinthemainpaperorthesupplementalmaterial, butif\ntheyappearinthesupplementalmaterial,theauthorsareencouragedtoprovideashort\nproofsketchtoprovideintuition.\n• Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented\nbyformalproofsprovidedinappendixorsupplementalmaterial.\n• TheoremsandLemmasthattheproofreliesuponshouldbeproperlyreferenced.\n4. ExperimentalResultReproducibility\nQuestion: Doesthepaperfullydisclosealltheinformationneededtoreproducethemainex-\nperimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions\nofthepaper(regardlessofwhetherthecodeanddataareprovidedornot)?\nAnswer: [Yes]\nJustification: Weprovidetrainingsettingsandpromptsusedintheappendix.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n• Ifthepaperincludesexperiments,aNoanswertothisquestionwillnotbeperceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhetherthecodeanddataareprovidedornot.\n• Ifthecontributionisadatasetand/ormodel,theauthorsshoulddescribethestepstaken\ntomaketheirresultsreproducibleorverifiable.\n• Dependingonthecontribution,reproducibilitycanbeaccomplishedinvariousways.\nForexample,ifthecontributionisanovelarchitecture,describingthearchitecturefully\nmightsuffice,orifthecontributionisaspecificmodelandempiricalevaluation,itmay\nbenecessarytoeithermakeitpossibleforotherstoreplicatethemodelwiththesame\ndataset,orprovideaccesstothemodel. Ingeneral. releasingcodeanddataisoften\nonegoodwaytoaccomplishthis,butreproducibilitycanalsobeprovidedviadetailed\ninstructionsforhowtoreplicatetheresults,accesstoahostedmodel(e.g.,inthecase\nofalargelanguagemodel),releasingofamodelcheckpoint,orothermeansthatare\nappropriatetotheresearchperformed.\n• WhileNeurIPSdoesnotrequirereleasingcode,theconferencedoesrequireallsubmis-\nsionstoprovidesomereasonableavenueforreproducibility,whichmaydependonthe\nnatureofthecontribution. Forexample\n(a) Ifthecontributionisprimarilyanewalgorithm,thepapershouldmakeitclearhow\ntoreproducethatalgorithm.\n(b) Ifthecontributionisprimarilyanewmodelarchitecture,thepapershoulddescribe\nthearchitectureclearlyandfully.\n(c) Ifthecontributionisanewmodel(e.g.,alargelanguagemodel),thenthereshould\neitherbeawaytoaccessthismodelforreproducingtheresultsorawaytoreproduce\nthemodel(e.g.,withanopen-sourcedatasetorinstructionsforhowtoconstruct\nthedataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthorsarewelcometodescribetheparticularwaytheyprovideforreproducibility.\nInthecaseofclosed-sourcemodels,itmaybethataccesstothemodelislimitedin\nsomeway(e.g.,toregisteredusers),butitshouldbepossibleforotherresearchers\ntohavesomepathtoreproducingorverifyingtheresults.\n5. Openaccesstodataandcode\nQuestion: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstruc-\ntionstofaithfullyreproducethemainexperimentalresults,asdescribedinsupplemental\nmaterial?\n26\nAnswer: [No]\nJustification: Weprovidedthedetailedtrainingsettingsandpromptsintheappendix.\nGuidelines:\n• TheanswerNAmeansthatpaperdoesnotincludeexperimentsrequiringcode.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy)formoredetails.\n• Whileweencouragethereleaseofcodeanddata,weunderstandthatthismightnotbe\npossible,so“No”isanacceptableanswer. Paperscannotberejectedsimplyfornot\nincludingcode,unlessthisiscentraltothecontribution(e.g.,foranewopen-source\nbenchmark).\n• Theinstructionsshouldcontaintheexactcommandandenvironmentneededtorunto\nreproducetheresults. SeetheNeurIPScodeanddatasubmissionguidelines(https:\n//nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.\n• Theauthorsshouldprovideinstructionsondataaccessandpreparation,includinghow\ntoaccesstherawdata,preprocesseddata,intermediatedata,andgenerateddata,etc.\n• Theauthorsshouldprovidescriptstoreproduceallexperimentalresultsforthenew\nproposedmethodandbaselines. Ifonlyasubsetofexperimentsarereproducible,they\nshouldstatewhichonesareomittedfromthescriptandwhy.\n• Atsubmissiontime, topreserveanonymity, theauthorsshouldreleaseanonymized\nversions(ifapplicable).\n• Providingasmuchinformationaspossibleinsupplementalmaterial(appendedtothe\npaper)isrecommended,butincludingURLstodataandcodeispermitted.\n6. ExperimentalSetting/Details\nQuestion: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: Weprovideexperimentalsettingsanddetailsintheappendix.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n• Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail\nthatisnecessarytoappreciatetheresultsandmakesenseofthem.\n• Thefulldetailscanbeprovidedeitherwiththecode,inappendix,orassupplemental\nmaterial.\n7. ExperimentStatisticalSignificance\nQuestion:Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate\ninformationaboutthestatisticalsignificanceoftheexperiments?\nAnswer: [Yes]\nJustification: Weprovideerrorbarsorconfidenceintervalswhenapplicable.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n• Theauthorsshouldanswer\"Yes\"iftheresultsareaccompaniedbyerrorbars,confi-\ndenceintervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupport\nthemainclaimsofthepaper.\n• Thefactorsofvariabilitythattheerrorbarsarecapturingshouldbeclearlystated(for\nexample,train/testsplit,initialization,randomdrawingofsomeparameter,oroverall\nrunwithgivenexperimentalconditions).\n• Themethodforcalculatingtheerrorbarsshouldbeexplained(closedformformula,\ncalltoalibraryfunction,bootstrap,etc.)\n• Theassumptionsmadeshouldbegiven(e.g.,Normallydistributederrors).\n• Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandarderror\nofthemean.\n27\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferablyreporta2-sigmaerrorbarthanstatethattheyhavea96%CI,ifthehypothesis\nofNormalityoferrorsisnotverified.\n• Forasymmetricdistributions,theauthorsshouldbecarefulnottoshowintablesor\nfiguressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g. negative\nerrorrates).\n• Iferrorbarsarereportedintablesorplots,Theauthorsshouldexplaininthetexthow\ntheywerecalculatedandreferencethecorrespondingfiguresortablesinthetext.\n8. ExperimentsComputeResources\nQuestion: Foreachexperiment,doesthepaperprovidesufficientinformationonthecom-\nputerresources(typeofcomputeworkers,memory,timeofexecution)neededtoreproduce\ntheexperiments?\nAnswer: [Yes]\nJustification: Weprovidetheinformationintheappendix.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n• ThepapershouldindicatethetypeofcomputeworkersCPUorGPU,internalcluster,\norcloudprovider,includingrelevantmemoryandstorage.\n• Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual\nexperimentalrunsaswellasestimatethetotalcompute.\n• Thepapershoulddisclosewhetherthefullresearchprojectrequiredmorecompute\nthantheexperimentsreportedinthepaper(e.g.,preliminaryorfailedexperimentsthat\ndidn’tmakeitintothepaper).\n9. CodeOfEthics\nQuestion: Doestheresearchconductedinthepaperconform, ineveryrespect, withthe\nNeurIPSCodeofEthicshttps://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: ThepaperconformwiththeNeurIPSCodeofEthics.\nGuidelines:\n• TheanswerNAmeansthattheauthorshavenotreviewedtheNeurIPSCodeofEthics.\n• IftheauthorsanswerNo,theyshouldexplainthespecialcircumstancesthatrequirea\ndeviationfromtheCodeofEthics.\n• Theauthorsshouldmakesuretopreserveanonymity(e.g.,ifthereisaspecialconsid-\nerationduetolawsorregulationsintheirjurisdiction).\n10. BroaderImpacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietalimpactsoftheworkperformed?\nAnswer:[NA]\nJustification: The paper focuses on narrow domains of PBE problems and has limited\nimpactsonsociety.\nGuidelines:\n• TheanswerNAmeansthatthereisnosocietalimpactoftheworkperformed.\n• IftheauthorsanswerNAorNo,theyshouldexplainwhytheirworkhasnosocietal\nimpactorwhythepaperdoesnotaddresssocietalimpact.\n• Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintendeduses\n(e.g.,disinformation,generatingfakeprofiles,surveillance),fairnessconsiderations\n(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific\ngroups),privacyconsiderations,andsecurityconsiderations.\n28\n• Theconferenceexpectsthatmanypaperswillbefoundationalresearchandnottied\ntoparticularapplications,letalonedeployments. However,ifthereisadirectpathto\nanynegativeapplications,theauthorsshouldpointitout. Forexample,itislegitimate\ntopointoutthatanimprovementinthequalityofgenerativemodelscouldbeusedto\ngeneratedeepfakesfordisinformation. Ontheotherhand,itisnotneededtopointout\nthatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain\nmodelsthatgenerateDeepfakesfaster.\n• Theauthorsshouldconsiderpossibleharmsthatcouldarisewhenthetechnologyis\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnologyisbeingusedasintendedbutgivesincorrectresults,andharmsfollowing\nfrom(intentionalorunintentional)misuseofthetechnology.\n• Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanismsformonitoringmisuse,mechanismstomonitorhowasystemlearnsfrom\nfeedbackovertime,improvingtheefficiencyandaccessibilityofML).\n11. Safeguards\nQuestion: Doesthepaperdescribesafeguardsthathavebeenputinplaceforresponsible\nreleaseofdataormodelsthathaveahighriskformisuse(e.g.,pretrainedlanguagemodels,\nimagegenerators,orscrapeddatasets)?\nAnswer: [NA]\nJustification: Thepaperposesalowrisk.\nGuidelines:\n• TheanswerNAmeansthatthepaperposesnosuchrisks.\n• Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith\nnecessarysafeguardstoallowforcontrolleduseofthemodel,forexamplebyrequiring\nthatusersadheretousageguidelinesorrestrictionstoaccessthemodelorimplementing\nsafetyfilters.\n• DatasetsthathavebeenscrapedfromtheInternetcouldposesafetyrisks. Theauthors\nshoulddescribehowtheyavoidedreleasingunsafeimages.\n• Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdo\nnotrequirethis,butweencourageauthorstotakethisintoaccountandmakeabest\nfaitheffort.\n12. Licensesforexistingassets\nQuestion: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin\nthepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand\nproperlyrespected?\nAnswer: [Yes]\nJustification: Wecreditandfollowthelicensetermsproperly.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotuseexistingassets.\n• Theauthorsshouldcitetheoriginalpaperthatproducedthecodepackageordataset.\n• Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea\nURL.\n• Thenameofthelicense(e.g.,CC-BY4.0)shouldbeincludedforeachasset.\n• Forscrapeddatafromaparticularsource(e.g.,website),thecopyrightandtermsof\nserviceofthatsourceshouldbeprovided.\n• If assets are released, the license, copyright information, and terms of use in the\npackageshouldbeprovided. Forpopulardatasets,paperswithcode.com/datasets\nhascuratedlicensesforsomedatasets. Theirlicensingguidecanhelpdeterminethe\nlicenseofadataset.\n• Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseof\nthederivedasset(ifithaschanged)shouldbeprovided.\n29\n• Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachoutto\ntheasset’screators.\n13. NewAssets\nQuestion:Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation\nprovidedalongsidetheassets?\nAnswer: [NA]\nJustification: Thepaperdoesnotreleasenewassets.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotreleasenewassets.\n• Researchersshouldcommunicatethedetailsofthedataset/code/modelaspartoftheir\nsubmissions via structured templates. This includes details about training, license,\nlimitations,etc.\n• Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose\nassetisused.\n• Atsubmissiontime,remembertoanonymizeyourassets(ifapplicable). Youcaneither\ncreateananonymizedURLorincludeananonymizedzipfile.\n14. CrowdsourcingandResearchwithHumanSubjects\nQuestion: Forcrowdsourcingexperimentsandresearchwithhumansubjects,doesthepaper\nincludethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as\nwellasdetailsaboutcompensation(ifany)?\nAnswer: [NA]\nJustification: Thepaperdoesnotinvolvecrowdsourcingorhumansubjects\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith\nhumansubjects.\n• Includingthisinformationinthesupplementalmaterialisfine,butifthemaincontribu-\ntionofthepaperinvolveshumansubjects,thenasmuchdetailaspossibleshouldbe\nincludedinthemainpaper.\n• AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,curation,\norotherlaborshouldbepaidatleasttheminimumwageinthecountryofthedata\ncollector.\n15. InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman\nSubjects\nQuestion: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether\nsuchrisksweredisclosedtothesubjects,andwhetherInstitutionalReviewBoard(IRB)\napprovals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor\ninstitution)wereobtained?\nAnswer: [NA]\nJustification: Thepaperdoesnotinvolvecrowdsourcingorresearchwithhumansubjects.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith\nhumansubjects.\n• Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)\nmayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you\nshouldclearlystatethisinthepaper.\n• Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions\nandlocations,andweexpectauthorstoadheretotheNeurIPSCodeofEthicsandthe\nguidelinesfortheirinstitution.\n• Forinitialsubmissions,donotincludeanyinformationthatwouldbreakanonymity(if\napplicable),suchastheinstitutionconductingthereview.\n30",
    "pdf_filename": "Is_Programming_by_Example_solved_by_LLMs.pdf"
}