{
    "title": "Is Programming by Example solved by LLMs",
    "abstract": "Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples. Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI per- spective, PBE corresponds to a very general form of few-shot inductive inference. Given the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have ‘solved’ PBE. We experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data. We find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution. We analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization. Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short. 1 Introduction Programming-by-Example (PBE) systems solve a challenging task: Given input-output examples of a hidden algorithm, they seek to construct the source code of the underlying function [1, 2]. PBE is deployed to millions of users [3, 4, 5, 6], lies near the heart of core AI challenges [7, 8, 9, 10], and is a qualitatively different problem from the bulk of recent work on LLM code generation, because rather than generate source code from natural language [11], PBE is instead fundamentally about few-shot inductive inference: Given a handful of examples, inferring the program that will generalize to new inputs, or which captures the true latent regularity, without relying on natural-language guidance. We investigate here the extent to which large language models pretrained on source code can solve PBE. If they can, this unlocks the ability to do PBE in general-purpose Turing complete languages like Python, unlike the restricted domain-specific languages which have so far dominated PBE [4, 12, 13, 14, i.a.], thereby increasing the scope and power of this paradigm. If LLMs cannot perform PBE, then this highlights a deficit of inductive reasoning and problem solving, and suggests LLMs lean too heavily on natural language cues to generate code. We find that pretrained and instruction-tuned models serve as poor PBE systems, a finding also supported by recent work [15, 16, 12, 17]. But our investigation further finds that LLMs can be fine-tuned for significantly higher performance, provided they are not asked to generalize far beyond the fine-tuning data. To address this failure of generalization we give an algorithm for taking a small unlabeled dataset of problems and adapting the LLM to it, which we find narrows this domain gap. The resulting recipe allows PBE over Turing-complete languages across three qualitatively different domains (Fig. 1): algorithms on vectors of numbers, string manipulation macros, and graphics programs in LOGO/Turtle. In every case, our final model is at least as effective as custom symbolic search algorithms operating over domain-specific languages, and surpasses powerful closed-source 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2406.08316v3  [cs.CL]  19 Nov 2024",
    "body": "Is Programming by Example solved by LLMs?\nWen-Ding Li\nCornell University\nwl678@cornell.edu\nKevin Ellis\nCornell University\nkellis@cornell.edu\nAbstract\nProgramming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI per-\nspective, PBE corresponds to a very general form of few-shot inductive inference.\nGiven the success of Large Language Models (LLMs) in code-generation tasks, we\ninvestigate here the extent to which LLMs can be said to have ‘solved’ PBE. We\nexperiment on classic domains such as lists and strings, and an uncommon graphics\nprogramming domain not well represented in typical pretraining data. We find that\npretrained models are not effective at PBE, but that they can be fine-tuned for much\nhigher performance, provided the test problems are in-distribution. We analyze\nempirically what causes these models to succeed and fail, and take steps toward\nunderstanding how to achieve better out-of-distribution generalization. Collectively\nthese results suggest that LLMs make strong progress toward solving the typical\nsuite of PBE tasks, potentially increasing the flexibility and applicability of PBE\nsystems, while also identifying ways in which LLMs still fall short.\n1\nIntroduction\nProgramming-by-Example (PBE) systems solve a challenging task: Given input-output examples of\na hidden algorithm, they seek to construct the source code of the underlying function [1, 2]. PBE is\ndeployed to millions of users [3, 4, 5, 6], lies near the heart of core AI challenges [7, 8, 9, 10], and is a\nqualitatively different problem from the bulk of recent work on LLM code generation, because rather\nthan generate source code from natural language [11], PBE is instead fundamentally about few-shot\ninductive inference: Given a handful of examples, inferring the program that will generalize to new\ninputs, or which captures the true latent regularity, without relying on natural-language guidance.\nWe investigate here the extent to which large language models pretrained on source code can\nsolve PBE. If they can, this unlocks the ability to do PBE in general-purpose Turing complete\nlanguages like Python, unlike the restricted domain-specific languages which have so far dominated\nPBE [4, 12, 13, 14, i.a.], thereby increasing the scope and power of this paradigm. If LLMs cannot\nperform PBE, then this highlights a deficit of inductive reasoning and problem solving, and suggests\nLLMs lean too heavily on natural language cues to generate code.\nWe find that pretrained and instruction-tuned models serve as poor PBE systems, a finding also\nsupported by recent work [15, 16, 12, 17]. But our investigation further finds that LLMs can be\nfine-tuned for significantly higher performance, provided they are not asked to generalize far beyond\nthe fine-tuning data. To address this failure of generalization we give an algorithm for taking a small\nunlabeled dataset of problems and adapting the LLM to it, which we find narrows this domain gap.\nThe resulting recipe allows PBE over Turing-complete languages across three qualitatively different\ndomains (Fig. 1): algorithms on vectors of numbers, string manipulation macros, and graphics\nprograms in LOGO/Turtle. In every case, our final model is at least as effective as custom symbolic\nsearch algorithms operating over domain-specific languages, and surpasses powerful closed-source\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2406.08316v3  [cs.CL]  19 Nov 2024\n\nmodels such as GPT4 [18]. We also find that the resulting system can cover a broader scope of\nproblems than classic symbolic methods, owing to the use of a Turing-complete language, which, at\nleast theoretically, allows learning any computable function.\noriginal_time = datetime.strptime(input_str, '%H:%M:%S')\nhour = original_time.hour\nstart_hour = hour - (hour % 2)\nend_hour = start_hour + 2\nstart_hour_12 = start_hour % 12 or 12\nend_hour_12 = end_hour % 12 or 12\nstart_ampm = \"AM\" if start_hour < 12 else \"PM\"\nend_ampm = \"AM\" if end_hour < 12 or end_hour == 24 else \"PM\"\nreturn f\"{start_hour_12}{start_ampm} to {end_hour_12}{end_ampm}\"\nINPUT\nOUTPUT\n18:25:57\n6PM to 8PM\n21:44:40\n8PM to 10PM\n07:00:20\n6AM to 8AM\n23:34:17\n10PM to 12AM\nDOMAIN: text editing macros\nprovided examples\ngenerated program\nfor i in range(7):\n    with fork_state():\n        for j in range(4):\n            forward(2*i)\n            left(90.0)\nDOMAIN: graphics\ngenerated program\n# Check if the list is empty\nif not input_list:\n   return input_list\n# Find min number in the list\nmin_num = min(input_list)\n# Subtract from each element\nreturn [num - min_num\n        for num in input_list]\nDOMAIN: lists\ngenerated program\nINPUT\nOUTPUT\n4,2,8\n2,0,6\n9,9,9,9\n0,0,0,0\n-7,0,2\n0,7,9\nprovided examples\nprovided example\nFigure 1: Domains, including standard ones that resemble programs found in pretraining data, as\nwell as a less common graphics domain, which is likely less represented in LLM pretraining data.\n2\nBackground\nProgramming by Example considers synthesizing a program ρ given a vector of inputs X and\ncorresponding outputs Y . Typically the program is expected to exactly fit the provided examples,\nρ(Xi) = Yi, ∀i, where i indexes examples. The program ρ is drawn from a (potentially infinite)\nlanguage L. Typically L is a domain-specific language designed for a specific PBE system, not\na general-purpose programming language. For example, the PBE system FlashFill synthesizes\nstring manipulation macros designed to automate common spreadsheet edits [2]. FlashFill’s domain-\nspecific language L includes commonly occurring regular expressions, together with string slicing and\nconcatenation, and restricted forms of loops. The language L is also designed to allow polynomial-\ntime construction of programs consistent with input-output examples. FlashFill’s goal, like most PBE\nsystems, is to generalize to hold-out test cases: inputs X′ with (hidden) target outputs Y ′.\nPick a program ρ from {ρ ∈L : ρ(Xi) = ρ(Yi), ∀i} .\nSucceed if ρ(X′\nj) = ρ(Y ′\nj ), ∀j\n(1)\nIn its simplest forms, PBE can be accomplished by guess-and-check enumeration until a program is\nfound that is consistent with the examples. Although there exist more sophisticated search algorithms,\nincluding those accelerated by neural guidance [19, 20, 21, 22], a key enabler of practical PBE\nsystems is the design of a carefully restricted domain-specific language L. The domain-specific\nlanguage effectively hardcodes symbolic knowledge, focusing the system on what programs the\nhuman engineer thinks are most promising, but at the expense of the wider set of computable functions\nexpressible in general-purpose languages.\nThe PBE setup covers other cases as well, such as sequence extrapolation (the inputs are indices\ninto the sequence), as well as data compression (the input is null, and the data is compressed by\nsynthesizing a program that reproduces the output data). Therefore, a truly general solution to PBE—\none which could express its solutions in general purpose programming languages, and cover most\n2\n\npractically relevant problems—would be broadly applicable to many inductive inference problems, a\npoint that has been long appreciated [9].\nLLMs for solving programming problems have been recently very successful [11, 23, 24, 25, 26].\nThese systems typically input a prompt describing a problem in natural language, then sample\ncandidate programs, and optionally filter those samples by checking them against input-output test\ncases, with the goal of passing holdout tests:\nDraw ρk ∼pLM(·|prompt). Pick a ρ ∈{ρk : ρk(Xi) = ρk(Yi), ∀i} . Success: ρ(X′\nj) = ρ(Y ′\nj ), ∀j\nUnlike PBE, the primary driver of program generation is a natural language prompt, although input-\noutputs may also be in the prompt [27, 28]. Recent work using LLMs to synthesize programs solely\nfrom examples has either obtained negative results [16, 12], or focused on simple and/or nonstandard\nproblems [29, 30, 31], leaving the extent to which PBE is ‘solved’ by LLMs an open question.\n3\nMethods\nBasic prompting is the most straightforward way of performing PBE with a pre-trained model: Given\ninput-output examples (X, Y ) a prompt is constructed and K programs are generated. Programs are\nfiltered by the I/O examples, and a random satisfying program is returned:\nSample ρk ∼pLM(·|prompt(X, Y )), for k from 1..K\n(2)\nPick a ρ from {ρk : ρk(Xi) = ρk(Yi), ∀i}\n(3)\nFine-tuning improves the above approach in a conceptually straightforward way. Given a dataset\ncomprising tuples of programs and I/O examples, {(ρ, X, Y )}, we fine-tune the LM to predict a\nprogram from its input-outputs. But this dataset is hard to come by: Although there are web-scale\ncorpora of naturally occurring source code, there is no analogous dataset of runnable code snippets\npaired with representative input-output examples, and this data deficit is especially true for new or\nunusual applications of PBE, such as the graphics programs we consider.\nTo assemble a large dataset of (ρ, X, Y ) triples we start with a small manually-constructed seed\ndataset, Dseed, and then randomly generate new programs ρ and inputs X by prompting an LLM with\nmembers of Dseed. The output Y comes from running ρ on X. The seed dataset effectively defines a\nprior over (ρ, X), notated G in Fig. 2. We sample from G to collect many program-input pairs, but\nuse program execution to predict Y , not an LLM. The resulting dataset, which we call Dtune, is used\nto train an LLM to generate programs when prompted with input-outputs. As this fine-tuned LLM\neffectively learns to do probabilistic inference in the graphical model shown in Fig. 2 (right), we\nwrite this fine-tuned LLM as qθ(ρ|X, Y ). This inference network is trained to maximize\nmax\nθ\nlog qθ(ρ|X, Y ), where (ρ, X) ∼G(Dseed) and Y = ρ(X)\n(4)\nThis method is closely related to self-instruct [32] and wake-sleep [33]. Like self-instruct, we use\nprompting to bootstrap a large dataset from a small manually-constructed one. Our method differs\nby using the LLM to generate a hidden latent variable (the program) while a different generative\nprocess produces an observed variable (the program outputs). Like wake-sleep, we use samples\nfrom a generative model to train an inference network, but we do not further train the generative\nmodel itself. Next, we will see that bringing the method much closer to wake-sleep by updating the\ngenerative model plays an important role when deploying the system on out-of-distribution problems.\nDseed\nG\ndefines\nDtune\nsamples\nqθ\ntrains\nG\nρ\nX\nY\nFigure 2: Left: Data generation pipeline. Right: The fine-tuned network qθ learns to do inference in a\ngraphical model where the prior over programs, G, is defined by prompting an LLM with example\ncode in Dseed, while the likelihood p(Y |ρ, X) is defined by program execution.\n3\n\nAdaptation.\nOne of the most powerful features of source code as a representation is its ability to\nefficiently express a wide range of computations. Therefore it is of interest to study the ability of\nfine-tuned LLMs to extrapolate to PBE problems outside the distribution of the fine-tuning data.\nWe consider a basic approach to adapting to a different distribution of problems, assuming access to\nproblems drawn from the testing distribution, but without labeled program solutions. This mimics\nthe deployment of PBE systems to end-users who may have their own idiosyncratic distribution\nof problems they care about, and who do not provide ground-truth programs, but who can provide\nfeedback on if a generated program has correct behavior. This means we have an unlabeled dataset\nDadapt comprising input-outputs (X, Y ), as well as a labeled seed dataset Dseed comprising triples\n(ρ, X, Y ). Adaptation proceeds by iterating between pretraining with G(Dseed), testing on Dadapt, and\nadding back into Dseed any program solutions found on the adaptation problems, which then become\nseeds for the next iteration. This produces a sequence of fine-tuned models, indexed below by i:\ntrain model:\nθi = arg max\nθ\nlog qθ(ρ|X, Y ), where (ρ, X) ∼G(Di\nseed) and Y = ρ(X)\nrun inference:\nρX,Y\nk\n∼qθi(ρ|X, Y ) for (X, Y ) ∈Dadapt and k from 1..K\nupdate seed:\nDi+1 = Di ∪\nn\n(ρX,Y\nk\n, X, Y ) : (X, Y ) ∈Dadapt, k ∈[K] if ρX,Y\nk\n(X) = Y\no\n(5)\nThe equations can be seen as a wake-sleep algorithm where “dreaming” corresponds to training q\non fantasy data (first equation) while “waking” corresponds to running inference and updating the\ngenerative model G (by updating the seed, second pair of equations). Ideally, each cycle of this\nwake-sleep adaptation solves more out-of-distribution problems, which tugs the generative model G\ntoward the target distribution, unlocking solutions to more out-of-distribution problems, etc. This\nhinges on each iteration actually solving new problems from the unlabeled dataset. Theoretically\nthis is guaranteed given enough inference-time compute (large K above). We explore in Sec. 4.3 the\nextent to which this holds in practice.\n4\nExperiments\nWe study different LLM-approaches to programming-by-examples across three domains (Fig. 1):\n1. List functions is a PBE domain meant to model a “programmer’s assistant”. It concerns\ndiscovering algorithms that transform lists of numbers, given input-output examples. This\nproblem statement has a long history within program synthesis [13, 34], and was popularized\nwithin machine learning by DeepCoder [35]. We consider two modern list function datasets\ncreated by Rule et al. 2024 [17] and Shi et al. 2023 [12], which both involve higher-order\nfunctions and nontrivial procedures such as map, filter, and sort. Rule et al. was recently\nadded to BigBench [36].\n2. Text editing is a domain where a program synthesizer assists an end-user edit their spread-\nsheets or other documents. From string-to-string examples, the system generates edit\nmacros for tasks such as reformatting dates, extracting fields from semistructured text,\netc. [2, 37, 38, 4]. Text editing is the most prominent commercial success of PBE: The\nFlashFill PBE system ships in Microsoft Excel and is used by many millions of people [6].\nWe consider two text editing datasets: SyGuS problems [22]—which are easier—and\nPROSE [39] problems, which constitute the most challenging dataset of its kind [38].\n3. LOGO/Turtle graphics is a domain whose goal is to synthesize a program that generates a\ntarget image.1 Systems of this kind can be used both for high-level visual reasoning and\nfor helping artists make structured edits to images [40, 41]. We use a dataset of geometric\ndesigns expressed as LOGO/Turtle [42] programs—where the programs move a simulated\npen over a canvas—taken from Wong et al. [43]. To allow the LLM to visually perceive the\ninput image, we convert the image to ASCII-art style strings; see Fig. 5 and Appendix. A.1.3.\n1This is PBE with a single example and null input, effectively compressing the image into a program.\n4\n\n4.1\nHow well does the fine-tuned model perform?\nWe prepare seed datasets for each domain, synthetically generate a large training set, and then\nfine-tune a DeepSeekCoder LLM [44] that was pretrained on source code.2 For list functions we seed\nwith 50 problems from Rule et al. 2024; For text editing, we consider seeding with either SyGuS or a\n40-problem subset of PROSE; for LOGO we seed with 200 training-set problems in Wong et al. [43].\nThe resulting fine-tuned models are surprisingly effective within their respective PBE domains. On\nlist functions our finetuned model surpasses the best symbolic search baselines reported in Rule et al.\n(Fig. 3a), surpasses the best neurosymbolic search method from Shi et al. (Appendix Fig. 10), and\nsurpasses GPT4. It also solves 100% of the list to list benchmark problems from λ2 (a well-known\nsymbolic synthesizer), shown in Appendix Tbl. 4: although plausibly, many λ2 problems are in\nthe pretraining data. On text editing, it surpasses the performance of FlashFill and approaches the\nlevel of FlashFill++ (Tbl. 1, Fig. 3b). On LOGO, it solves 90% of the test set (Fig. 3c), surpassing\nsystems such as DreamCoder [45], which introduced the first version of these LOGO problems. It\nalso solves more problems than LILO and Regal [43, 46], which are LOGO program synthesizers\nthat input natural language describing how the image should be drawn. In contrast, our model does\nnot use any language clues, generating purely from the image. In addition to quantitatively solving\nmore problems, we note that there are qualitative improvements to the breadth of problems that can\nbe solved in the first place because the LLM can generate Turing-complete code spanning a much\nbroader space of computations (Fig. 4).\n0\n100\n200\nSearch Budget (Num Samples)\n10\n20\n30\n40\n50\n60\n70\n80\n% Problems Solved\nFleet \nMetagol\nRobustFill\nours-33b\nours-7b\ngpt-4-turbo\ngpt-4-turbo CoT\ndeepseek33b\n(a) Lists\n0\n100\n200\nSearch Budget (Num Samples)\n10\n20\n30\n40\n50\n60\n70\n80\nFlashFill\nours-33b\nours-7b\ngpt-4-turbo\ngpt-4-turbo CoT\ndeepseek33B\n(b) Strings\n0\n200\n400\n      Search Budget (Num Samples)\n0\n20\n40\n60\n80\nRegal\nLILO\nDreamCoder\nours-33b\nours-7b\ngpt-4o\ngpt-4o-mini\n(c) Graphics\nFigure 3: Test set performance. A problem is solved if the predicted program generates correct\noutputs on the holdout inputs. Metagol [47], RobustFill [20], and Fleet [48] results taken from [17]\ngen.\naccuracy\noracle\naccuracy\nFlashFill\n33%\n—\nFlashFill++\n—\n≈100%\nours, 33B\n82%\n88%\nTable 1: Generalization accuracy: % problems\nwhere the program makes correct predictions on\nevery holdout test.\nOracle accuracy: % prob-\nlems where a correct program was generated (even\nif incorrect programs were also generated that\nalso passed the training input-outputs).\nFlash-\nFill++ [38] only reports oracle accuracy. 3\nThere are caveats to the above results. First, the\nfine-tuned model essentially never produces a\ncorrect program on the first try: It requires tens\nor hundreds of samples, each of which is com-\npared against the ground-truth input-outputs,\nand discarded if it contradicts the examples. On\na GPU like the one we use (an Nvidia A6000)\nthis rejection sampling takes on the order of a\nfew minutes to solve a given problem. However,\ncompared to classic enumerative program syn-\nthesizers [13, 49], or even compared to those\nwith neural guidance [35, 22], proposing a few\nthousand programs is relatively little, and could\nnot plausibly cover a significant fraction of the\nexponentially large search space.\n2We prefer DeepSeek because it is roughly LLaMA-level, but has fully open training details.\n3The FlashFill results were obtained using Microsoft Excel for Mac.\n5\n\nINPUT\nOUTPUT\nMary had a little lamb\nIts fleece was white…\n1:Mary had a little lamb\n2:Its fleece was white…\nTwinkle, twinkle, …\nHow I wonder what you…\nUp above the world so…\nLike a diamond in the…\n1:Twinkle, twinkle, …\n2:How I wonder what you…\n3:Up above the world so…\n4:Like a diamond in the…\nINPUT\nOUTPUT\nNY\nNew York\nCA\nCalifornia\nAK\nAlaska\nlines = text.splitlines()\nresult = \"\"\nfor i, line in enumerate(lines):\n   result += f\"{i + 1}:{line}\\n\"\nreturn result[:-1]\nstates = {\n   \"AL\": \"Alabama\",\n   \"AK\": \"Alaska\",\n   .........\n}\nreturn states[text]\ngenerated program\ngenerated program\nFigure 4: PBE with LLMs allows using general-purpose programming languages which can mix\nstring and numerical operations in ways not allowed by domain-specific languages [38] (top), and\nallows world knowledge to inform code generation (bottom). I/Os and code partly elided for space.\nprovided example\nASCII representation\ngenerated candidate figures\n✅\n000000000023310000000000\n000000000010030000000000\n000000233200030210000000\n000001300300030022000000\n000001200030200003000000\n000000100021200003000000\n000000232226322330000000\n000001200012200000000000\n000003000030300012000000\n000002100210020012000000\n000000220300023230000000\n000000000300000100000000\n000000000133300000000000\nFigure 5: ASCII representation of LOGO graphics. Average pixel intensity indicated by numbers 0-9\nThe second caveat is that the model degrades when tested out-of-distribution. An example of this\ndegradation is illustrated in Fig. 6, which tests the LOGO graphics model on hand drawings (after\ntraining on clean computer graphics). On the out-of-distribution hand drawing the model mostly\nsamples programs that do not fit the data, but its accuracy does not fall to zero, meaning that with\nenough compute budget, it does actually generate reasonable programs. This foreshadows the results\nin Sec. 4.3, which more systematically studies out-of-distribution behavior.\n✅\nFigure 6: Example out-of-distribution LOGO test: inferring a graphics program from a hand drawing.\nSee also Appendix Fig. 12\n6\n\n4.2\nWhat causes the fine-tuned model to succeed or fail?\nClassic symbolic approaches to PBE, when they are based on enumeration, tend to succeed whenever\nthe target program is syntactically small. Approaches based on clever dynamic programming, such as\nthe FlashFill family [4], succeed when the program is representable in the domain-specific language.\nWhat predicts success for these LLM approaches?\nTo answer this question we investigate several hypotheses. First, potentially the success is deter-\nmined by program size, and degrades as programs grow longer. Second, as a more refined notion\nof size, we instead measure the description length under the prior, which for a program ρ, is\n−log pLM(ρ|G(Dseed)). Description length under the prior would be a good predictor of success if\nthe fine-tuned model engages in blind guess-and-check: simply learning the distribution G(Dseed),\nand sampling from this prior while ignoring the input-outputs. Third, one possibility is that success is\npredicted by description length under the approximate posterior (−log qθ(ρ|X, Y )), which would\nbe the case if the fine-tuned model attends closely to the input-outputs and reshapes its distribution\naccordingly, instead of defaulting to the prior. To test these hypotheses we calculate the average\ncompute budget needed to solve each problem, and compare it with these different variables. Fig. 7\nshows that posterior description length is more predictive than program size and prior description\nlength: unlike classical methods, metrics of program length correlate poorly with problem diffi-\nculty, and there is no evidence that the fine-tuned model’s behavior can be characterized as blind\nguess-and-check. (See also Fig. 5).\n101\n102\n103\nSearch Budget\n107\n1013\n1019\n1025\n1031\n1037\nNegative Log Program Prior\nR2 = 0.02\n101\n102\n103\nSearch Budget\n104\n108\n1012\n1016\n1020\n1024\nNegative Log Program Posterior\nR2 = 0.32\n101\n102\n103\nSearch Budget\n30\n40\n50\n60\nProgram Ast Size\nR2 = 0.10\nFigure 7: Compute budget needed to solve a problem is best predicted by description length under\nthe approximate posterior, not program size or prior description length, suggesting that the fine-tuned\nmodel is not engaging in blind guess-and-check.\n4.3\nOut-of-distribution generalization\nOne advantage of classic symbolic PBE methods is that they do not make statistical assumptions about\ntheir test problems. Indeed, some classic methods can, within their domains, synthesize programs\nperfectly (i.e. always find a program that fits the training input-outputs). In contrast, neural networks\ncan struggle to generalize beyond the training distribution.\nWe therefore consider train/test splits that force the model to generalize beyond the distribution of\nits training data (beyond Dseed). On text editing, we seed with SyGuS problems, and perform out-\nof-distribution testing on PROSE problems (PROSE is much harder than SyGuS). On list functions,\nwe seed with problems from Rule et al. 2024 and test on Shi et al. 2023 (the Shi dataset contains\nunusual combinators, such as Scan). On LOGO, we seed with short programs (≤12 lines of code),\nand test on long programs (> 12 lines of code). Using these splits we also measure the ability of the\nadaptation method in Sec. 3 to improve out-of-distribution generalization.4\nFig. 8 shows that there is nontrivial degradation when testing out of distribution. For example, a 7B\nmodel seeded with PROSE problems and tested on a different subset of PROSE has an accuracy of\n76% (Fig. 3b), but this degrades to 59% when seeded with SyGuS problems, which follow a different\ndistribution and are generally simpler and easier than PROSE (Fig. 8b).\n4We work here with 7B models because Sec. 4.1 found that fine-tuned 33B models are only slightly better\nthan 7B, and 7B is cheaper to run.\n7\n\nWe further perform the adaptation method described in Sec. 3 in order to measure the extent to which\nit can narrow these domain gaps. In every case it allows solving more out-of-distribution problems,\nincreasing absolute performance by around 10% or more in all domains, which is a relative increase\nof about 16% for text/list and a relative increase of about 190% for LOGO (approximately tripling\nthe number of solved LOGO problems).\n0\n500\n1000\n1500\n2000\nSearch Budget (Num Samples)\n10\n20\n30\n40\n50\n60\n70\n% Problems Solved\nbefore adaptation\nadaptation\nfinetuned\nin-distribution\n(a) Rule adapted to Shi\n0\n200\n400\nSearch Budget (Num Samples)\n20\n30\n40\n50\n60\n70\n% Problems Solved\nbefore adaptation\nadaptation\nfinetuned\nin-distribution\n(b) SyGuS adapted to PROSE\n0\n500\n1000\n1500\n2000\nSearch Budget (Num Samples)\n0\n20\n40\n60\n80\n% Problems Solved\nbefore adapation\nadaptation\nfinetuned in-distribution\n(c) LOGO short adapted to long\nFigure 8: Out-of-distribution generalization and adaptation to new test distribution.\nTo better understand the dynamics of adaptation, we visualize the specific problems solved before and\nafter adaptation on LOGO graphics (Fig. 9). Before adaptation, only a handful of out-of-distribution\nproblems are solvable, and only with a significant search budget. Adaptation allows the system to\nquickly solve similar out-of-distribution problems in the future, but does not allow the system to\ngeneralize to problems very unlike those originally solvable by the fine-tuned model. In principle,\nexpanding the inference-time compute budget should allow successful adaptation (large K in Eq. 5).\nAnother more compute-efficient approach would be to increase the amount of adaptation data by\nintroducing ‘steppingstone’ problems in the adaptation set that give a gentler transition from the\noriginal training distribution.\nSolved Before Adapt\nSolved After Adapt\nSolved when also trained\nwith long programs\nFigure 9: Out-of-distribution LOGO problems (requiring long programs with > 12 lines of code). We\nshow example problems that are solved by the original model fine-tuned on short programs, which\nthen become training data for the next round of adaptation. Adaptation allows consistent solving\nof problems similar to those that the original fine-tuned model could sometimes solve, but is not a\npanacea: Problems dissimilar to those solved by the initial model are not ever correctly generated,\ndespite the fact that they are solvable by a model fine-tuned in-distribution.\n8\n\n5\nRelated Work\nAutomatic data generation with LLMs, such as self-instruct [32], WizardCoder [50], and many\nothers [51, 52, 53, i.a.], works by prompting an LLM to produce outputs which are then used for later\nlearning stages such as fine-tuning. These approaches are applied recursively to their own output:\nPreviously generated data is incorporated into future prompts. We similarly generate a dataset Dtune\nby prompting an LLM with Dseed, but (1) do not recursively prompt the LLM with its own outputs\nand (2) combine the LLM generations with program execution to make program outputs. This gives\na different mathematical interpretation to our data generator. First, the programs are samples from\na prior, G, defined by Dseed, which would not be a valid interpretation if the LLM was repeatedly\nfed its own outputs. Second, there is an observation model or likelihood function, p(Y |ρ, X), which\nis defined not by the LLM, but by a Python interpreter. In this way, our data generator constructs\ntraining examples for fine-tuning that teach the network how to invert the execution process of the\nPython interpreter.\nMachine learning applied to PBE has often sought to accelerate search: to find any program at all\nconsistent with the I/O examples [19, 20, 45, 21, 35, 22, 12, 54, 55, 56], which is nontrivial due to the\ncombinatorial nature of the search, even after confining to a domain-specific programming language.\nA complementary line of research explores inductive biases that favor programs likely to generalize\nto new inputs, such as learning a prior or ranking function [57, 41, 58, 59]. Our work should be seen\nwithin the tradition of learning to search for programs. We show that finetuned models serve as an\neffective yet simple foundation for accelerating search in PBE, allowing search to be tractable over\nmuch richer and more expressive languages such as Python.\nClassic PBE.\nTraditional approaches to programming-by-examples operate by symbolically search-\ning or solving for programs consistent with the input-output examples [13, 49, 2, 1, 37, 6]. They use\ndomain-specific programming languages that are designed to either enable efficient search and/or bias\nthe system toward functions that are likely to generalize new inputs. Search for programs can even be\npolynomial time when this domain-specific language has a special structure (roughly, when every\nfunction can be ‘inverted’), a key enabler of FlashFill, the first commercial success of PBE [4, 2].\nLLMs as inductive reasoners.\nUsing an LLM to perform inductive reasoning—to generate abstract\nhypotheses from concrete specific examples—has been explored by several recent works [29, 30, 60,\n61], all of which has found significant value in translating these hypotheses into programs, and all of\nwhich have worked by prompting pretrained GPT-style models. Our work can be seen as helping\nanswer a natural question posed by these previous works: Given that LLMs can generate hypotheses\nfrom examples, can they produce programs of the nature and complexity demanded by PBE? We find\nthis is largely the case after fine-tuning, both for classic PBE domains and unusual ones.\nSelf-Debugging, Refinement, and Self-repair.\nOne way of improving the code generation abilities\nof an LLM is to have it attempt to debug its own code whenever the initially generated code does not\npass the provided test cases [62, 63, 64, 65, 66, 24, 67]. We did not explore this strategy, however,\nbecause a more basic approach that simply regenerated a new program from scratch already surpassed\nthe prior state of the art (both symbolic and neural baselines), provided we finetune. However, further\npushing the boundary of PBE may benefit from self-debugging strategies.\nRanking LLM-generated code.\nPast work considers a variety of ways to select an output from a\ncollection of LLM-sampled programs [23, 59, 68, 11], many of which are more sophisticated than\nsimply filtering by the examples, which is what we do here. Like with self-debugging, integrating\nthese techniques should be synergistic with our approach.\n6\nLimitations\nOur work has important limitations. From an engineering perspective, using a 7B-33B neural network\nto perform PBE is not practical for most end-users, who may be doing PBE on their laptop or desktops\nin order to accomplish small one-off tasks. For this reason, true deployment to end-users may require\n9\n\ninvestigating the effectiveness of much smaller neural networks (not an LLM), and it may also be\nvaluable to study the effect of network compression and distillation upon our finetuned models.\nFrom the perspective of understanding where and why our system succeeds and fails, we have shown\nthat neither program size nor likelihood under the prior suffice to predict success, finding the posterior\nlikelihood is a better predictor, albeit an imperfect one. Although this allows us to discard the\nhypothesis that the system is merely sampling from the prior, it also just pushes the question back\none stage further: What exactly about specific problems causes the neural network’s approximate\nposterior to put more or less probability mass on correct solutions? While in classic PBE one can\nobtain sharp answers as to why a certain problem was solved or not, this is a much harder question\nwith neural networks, whose workings are more opaque.\n7\nDiscussion\nPBE with fine-tuned LLMs is surprisingly effective, surpassing many of the best neural and sym-\nbolic baselines we know of, even for uncommon domains such as LOGO graphics. Why is that?\nFundamentally, the neural network only needs to act as a heuristic proposer of solutions, because\nwe can check against the input-outputs. Therefore, one possible explanation is that the tendency of\nlanguage models to over-generate, hallucinate, and cover the long tail of possibilities is actually an\nasset, instead of a liability. And although there is a degree of degradation on out-of-sample problems,\nthe degradation is not so severe that out-of-distribution problems become utterly unsolvable: Instead,\nthey merely become harder to solve, a phenomenon that allows adaptation to work in the first place.\nSimultaneously one should be hesitant about claiming that PBE is ‘solved.’ Optimistically, current\nPBE benchmarks exist to test the frontier of what is possible, and so doing well on those benchmarks\nmight just mean that the frontier has moved. More realistically, determining if an AI system truly\nworks in the wild requires more than just pushing benchmark numbers, which can be misleading\nwhen those benchmarks do not capture the long tail of naturally-occurring tests. Furthermore, all\nAI systems present tradeoffs, and a neural system’s unpredictability, high computational cost, and\nout-of-distribution fragility should be weighed against whatever high benchmark numbers they may\nachieve. Despite these caveats, we are optimistic about the promise of tuning LLMs for PBE, and\nbelieve that it has the potential to dramatically expand the scope of solvable problems and even\nsolvable domains.\nAcknowledgements.\nWe are grateful for assistance from Joshua Rule in the processing of the list\nfunctions data, and for feedback from Yewen Pu on the manuscript. This work was supported by an\nNSF CAREER grant as well as gifts from Google and Cisco.\nReferences\n[1] Henry Lieberman. Your wish is my command: Programming by example. Morgan Kaufmann,\n2001.\n[2] Sumit Gulwani. Automating string processing in spreadsheets using input-output examples.\nIn Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of\nProgramming Languages, POPL ’11, page 317–330, New York, NY, USA, 2011. Association\nfor Computing Machinery. ISBN 9781450304900. doi: 10.1145/1926385.1926423. URL\nhttps://doi.org/10.1145/1926385.1926423.\n[3] Sumit Gulwani. Automating string processing in spreadsheets using input-output examples.\nSIGPLAN Not., 46(1):317–330, jan 2011. ISSN 0362-1340. doi: 10.1145/1925844.1926423.\nURL https://doi.org/10.1145/1925844.1926423.\n[4] Oleksandr Polozov and Sumit Gulwani. Flashmeta: A framework for inductive program\nsynthesis. SIGPLAN Not., 50(10):107–126, oct 2015. ISSN 0362-1340. doi: 10.1145/2858965.\n2814310. URL https://doi.org/10.1145/2858965.2814310.\n[5] Xinyun Chen, Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun Dai, Max Lin, and Denny\nZhou. Spreadsheetcoder: Formula prediction from semi-structured context. In International\nConference on Machine Learning, pages 1661–1672. PMLR, 2021.\n10\n\n[6] Sumit Gulwani, José Hernández-Orallo, Emanuel Kitzelmann, Stephen H Muggleton, Ute\nSchmid, and Benjamin Zorn. Inductive programming meets the real world. Communications of\nthe ACM, 58(11):90–99, 2015.\n[7] François Chollet. On the measure of intelligence, 2019.\n[8] Stephen H Muggleton, Ute Schmid, Christina Zeller, Alireza Tamaddoni-Nezhad, and Tarek\nBesold. Ultra-strong machine learning: comprehensibility of programs learned with ilp. Machine\nLearning, 107(7):1119–1140, 2018.\n[9] J Solomono Raymond. A formal theory of inductive inference i. Information and Control, 7:\n1–22, 1964.\n[10] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept\nlearning through probabilistic program induction. Science, 350(6266):1332–1338, 2015.\n[11] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond,\nTom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code\ngeneration with alphacode. Nature, 2022.\n[12] Kensen Shi, Hanjun Dai, Wen-Ding Li, Kevin Ellis, and Charles Sutton. Lambdabeam: Neural\nprogram search with higher-order functions and lambdas. In Thirty-seventh Conference on\nNeural Information Processing Systems, 2023. URL https://openreview.net/forum?id=\nqVMPXrX4FR.\n[13] John K. Feser, Swarat Chaudhuri, and Isil Dillig. Synthesizing data structure transformations\nfrom input-output examples.\nIn Proceedings of the 36th ACM SIGPLAN Conference on\nProgramming Language Design and Implementation, PLDI ’15, page 229–239, New York,\nNY, USA, 2015. Association for Computing Machinery. ISBN 9781450334686. doi: 10.1145/\n2737924.2737977. URL https://doi.org/10.1145/2737924.2737977.\n[14] Nathanaël Fijalkow, Guillaume Lagarde, Théo Matricon, Kevin Ellis, Pierre Ohlmann, and\nAkarsh Nayan Potta. Scaling neural program synthesis with distribution-based search. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6623–6630,\n2022.\n[15] Kensen Shi, Joey Hong, Yinlin Deng, Pengcheng Yin, Manzil Zaheer, and Charles Sutton.\nExedec: Execution decomposition for compositional generalization in neural program synthesis.\nIn International Conference on Learning Representations, 2024.\n[16] Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, and\nPengcheng Yin. Next: Teaching large language models to reason about code execution, 2024.\n[17] Joshua S. Rule, Steven T. Piantadosi, Andrew Cropper, Kevin Ellis, Maxwell Nye, and Joshua B.\nTenenbaum. Symbolic metaprogram search improves learning efficiency and explains rule\nlearning in humans. Nature Communications, 2024.\n[18] OpenAI. Gpt-4 technical report, 2023.\n[19] Ashwin Kalyan, Abhishek Mohta, Oleksandr Polozov, Dhruv Batra, Prateek Jain, and Sumit\nGulwani. Neural-guided deductive search for real-time program synthesis from examples. arXiv\npreprint arXiv:1804.01186, 2018.\n[20] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed,\nand Pushmeet Kohli. Robustfill: Neural program learning under noisy i/o. In Proceedings of\nthe 34th International Conference on Machine Learning - Volume 70, ICML’17, page 990–998.\nJMLR.org, 2017.\n[21] Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In\nInternational Conference on Learning Representations, 2018.\n[22] Kensen Shi, Hanjun Dai, Kevin Ellis, and Charles Sutton. Crossbeam: Learning to search in\nbottom-up program synthesis. In International Conference on Learning Representations, 2021.\n11\n\n[23] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu\nChen. Codet: Code generation with generated tests. ICLR, 2023.\n[24] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Hoi. CodeRL:\nMastering code generation through pretrained models and deep reinforcement learning. In\nAlice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in\nNeural Information Processing Systems, 2022. URL https://openreview.net/forum?id=\nWaGvb7OzySA.\n[25] Eric Zelikman, Qian Huang, Gabriel Poesia, Noah Goodman, and Nick Haber. Parsel: Algo-\nrithmic reasoning with language models by composing decompositions. Advances in Neural\nInformation Processing Systems, 36:31466–31523, 2023.\n[26] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo,\nCollin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding\nchallenge competence with apps. NeurIPS, 2021.\n[27] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[28] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and LINGMING ZHANG. Is your code\ngenerated by chatGPT really correct? rigorous evaluation of large language models for code\ngeneration. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\nURL https://openreview.net/forum?id=1qvx610Cu7.\n[29] Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula,\nBailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, and Xiang Ren. Phenomenal yet puzzling:\nTesting inductive reasoning capabilities of language models with hypothesis refinement. ICLR,\n2024.\n[30] Kevin Ellis. Human-like few-shot learning via bayesian reasoning over natural language.\nNeurIPS, 2023.\n[31] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D. Goodman.\nHypothesis search: Inductive reasoning with language models. ICLR, 2024.\n[32] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi,\nand Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instruc-\ntions, 2023.\n[33] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The\" wake-sleep\"\nalgorithm for unsupervised neural networks. Science, 268(5214):1158–1161, 1995.\n[34] Peter-Michael Osera and Steve Zdancewic. Type-and-example-directed program synthesis.\nACM SIGPLAN Notices, 50(6):619–630, 2015.\n[35] Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow.\nDeepcoder: Learning to write programs. In International Conference on Learning Representa-\ntions, 2017. URL https://openreview.net/forum?id=ByldLrqlx.\n[36] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,\nAdam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language models.\narXiv preprint arXiv:2206.04615, 2022.\n[37] Tessa Lau, Steven A Wolfman, Pedro Domingos, and Daniel S Weld. Programming by demon-\nstration using version space algebra. Machine Learning, 53:111–156, 2003.\n[38] José Cambronero, Sumit Gulwani, Vu Le, Daniel Perelman, Arjun Radhakrishna, Clint Simon,\nand Ashish Tiwari. Flashfill++: Scaling programming by example by cutting to the chase.\nProceedings of the ACM on Programming Languages, 7(POPL):952–981, 2023.\n12\n\n[39] Microsoft prose public benchmark suite, 2022. Available at https://github.com/microsoft/prose-\nbenchmarks.\n[40] Jiayuan Mao, Xiuming Zhang, Yikai Li, William T. Freeman, Joshua B. Tenenbaum, and Jiajun\nWu. Program-Guided Image Manipulators. In International Conference on Computer Vision,\n2019.\n[41] Kevin Ellis and Sumit Gulwani. Learning to learn programs from examples: Going beyond\nprogram structure. In IJCAI, pages 1638–1645, 2017.\n[42] David D. Thornburg. Friends of the turtle. Compute!, March 1983.\n[43] Catherine Wong, Kevin Ellis, Joshua B. Tenenbaum, and Jacob Andreas. Leveraging language\nto learn program abstractions and search heuristics. In ICML, 2021.\n[44] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen,\nXiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When\nthe large language model meets programming – the rise of code intelligence, 2024.\n[45] Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sablé-Meyer, Lucas Morales, Luke Hewitt,\nLuc Cary, Armando Solar-Lezama, and Joshua B. Tenenbaum. DreamCoder: Bootstrapping\nInductive Program Synthesis with Wake-Sleep Library Learning, page 835–850. Association\nfor Computing Machinery, New York, NY, USA, 2021. ISBN 9781450383912. URL https:\n//doi.org/10.1145/3453483.3454080.\n[46] Elias Stengel-Eskin, Archiki Prasad, and Mohit Bansal. Regal: Refactoring programs to discover\ngeneralizable abstractions. arXiv preprint arXiv:2401.16467, 2024.\n[47] Stephen H. Muggleton, Dianhuan Lin, and Alireza Tamaddoni-Nezhad. Meta-interpretive\nlearning of higher-order dyadic datalog: Predicate invention revisited. Mach. Learn., 100\n(1):49–73, jul 2015. ISSN 0885-6125. doi: 10.1007/s10994-014-5471-y. URL https:\n//doi.org/10.1007/s10994-014-5471-y.\n[48] Steven Piantadosi. Fleet. https://github.com/piantado/Fleet/, 2023. [Online GitHub\nrepository].\n[49] Rajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo MK Martin, Mukund Raghothaman, Sanjit A\nSeshia, Rishabh Singh, Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa. Syntax-\nguided synthesis. IEEE, 2013.\n[50] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing\nMa, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models\nwith evol-instruct, 2023.\n[51] Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and\nLingpeng Kong. Zerogen: Efficient zero-shot learning via dataset generation. arXiv preprint\narXiv:2202.07922, 2022.\n[52] Ajay Patel, Colin Raffel, and Chris Callison-Burch. Datadreamer: A tool for synthetic data\ngeneration and reproducible llm workflows, 2024.\n[53] Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Generating training data with language\nmodels: Towards zero-shot language understanding. Advances in Neural Information Processing\nSystems, 35:462–477, 2022.\n[54] Kavi Gupta, Peter Ebert Christensen, Xinyun Chen, and Dawn Song. Synthesize, execute and\ndebug: learning to repair for neural program synthesis. In Proceedings of the 34th International\nConference on Neural Information Processing Systems, NIPS ’20, Red Hook, NY, USA, 2020.\nCurran Associates Inc. ISBN 9781713829546.\n[55] Shraddha Barke, Hila Peleg, and Nadia Polikarpova. Just-in-time learning for bottom-up\nenumerative synthesis. Proc. ACM Program. Lang., 4(OOPSLA), nov 2020. doi: 10.1145/\n3428295. URL https://doi.org/10.1145/3428295.\n13\n\n[56] Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Joshua B. Tenenbaum, and Armando Solar-\nLezama. Write, Execute, Assess: Program Synthesis with a REPL. Curran Associates Inc., Red\nHook, NY, USA, 2019.\n[57] Rishabh Singh and Sumit Gulwani.\nPredicting a correct program in programming\nby example.\nIn 27th International Conference on Computer Aided Verification (CAV\n2015), July 2015. URL https://www.microsoft.com/en-us/research/publication/\npredicting-a-correct-program-in-programming-by-example/.\n[58] Percy Liang, Michael I. Jordan, and Dan Klein. Learning programs: a hierarchical bayesian\napproach. In Proceedings of the 27th International Conference on International Conference\non Machine Learning, ICML’10, page 639–646, Madison, WI, USA, 2010. Omnipress. ISBN\n9781605589077.\n[59] Jeevana Priya Inala, Chenglong Wang, Mei Yang, Andres Codas, Mark Encarnación, Shu-\nvendu Lahiri, Madanlal Musuvathi, and Jianfeng Gao. Fault-aware neural code rankers. In\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in\nNeural Information Processing Systems, volume 35, pages 13419–13432. Curran Associates,\nInc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/\n5762c579d09811b7639be2389b3d07be-Paper-Conference.pdf.\n[60] Wasu Top Piriyakulkij and Kevin Ellis. Doing experiments and revising rules with natural\nlanguage and probabilistic reasoning. CogSci, 2024.\n[61] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah Goodman.\nHypothesis search: Inductive reasoning with language models. In The Twelfth International\nConference on Learning Representations, 2024. URL https://openreview.net/forum?\nid=G7UtIGQmjm.\n[62] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language\nmodels to self-debug. arXiv preprint arXiv:2304.05128, 2023.\n[63] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and\nYejin Choi. Generating sequences by learning to self-correct. ICLR, 2023.\n[64] Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-\nLezama. Is self-repair a silver bullet for code generation?, 2023.\n[65] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\ndynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\n[66] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. Advances in Neural Information Processing Systems, 36, 2024.\n[67] Hao Tang, Keya Hu, Jin Peng Zhou, Sicheng Zhong, Wei-Long Zheng, Xujie Si, and Kevin\nEllis. Code repair with llms gives an exploration-exploitation tradeoff. arXiv, 2024.\n[68] Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida I Wang, and Xi Victo-\nria Lin. Lever: Learning to verify language-to-code generation with execution. In Proceedings\nof the 40th International Conference on Machine Learning (ICML’23), 2023.\n[69] jinaai/jina-embeddings-v2-base-code.\nhttps://huggingface.co/jinaai/\njina-embeddings-v2-base-code. Accessed: 2024-05-22.\n14\n\nA\nAppendix / supplemental material\nA.1\nExperiment Details\nWe used a temperature of 1.0 for sampling in our experiments unless otherwise stated. All experiments\nwere performed on single-node machines (8xA6000 or 8xA100, etc.) without a multi-node distributed\ncomputing setup.\nA.1.1\nList Tasks\nWe selected 50 problems from Rule et al. as our seed set, reserving the remaining problems for\ntesting. To ensure comparability with Rule et al., we tested on the first 100 problems (excluding those\nin the seed set), resulting in 77 test problems. We consistently used 10 input-output examples, with\nthe remaining 54 examples serving as a held-out test set. When filtering duplicate synthetic data, we\nemployed an open code embedding model[69] available on Hugging Face. As a sanity check, we\nalso use 10 list to list problems from λ2 benchmark and shown the model can effectively solve them\nin Table.4\nA.1.2\nString Tasks\nWe utilized 100 string-to-string/null transformation problems from the prose-benchmark. When\navailable, we used 10 input-output examples, always reserving at least one example as a hold-out test\nset. This ensures the generalization of our synthesized programs, as the benchmark did not provide\nheld-out data.\nFor the FlashFill baseline, we used Microsoft Excel for Mac version 16.8. We opened each individual\nxlsx file containing the test problem examples and manually triggered the FlashFill function by\npressing Ctrl+E.\nA.1.3\nLogo Tasks\nTo facilitate graphics input inference with code language models, we converted logo graphics into\nASCII-represented strings, as shown in Figure 5. For each input image, we cropped a 512x512\nsection from the center and then divided it into 32x32 blocks, each with a size of 16x16. We counted\nthe number of black pixels in each block, calculated their density ( num black pixels\n16·16\n), and quantized this\ndensity value into 10 levels, represented by the ASCII numbers 0-9. By representing each block\nwith an ASCII number, an input image is represented with a string of 32 lines, and each line has 32\nnumbers.\nFor the turtle graphics program, we adopted the Python turtle graphics program from Regal[46]\nwith a minor modification of changing the ‘embed’ function to use a ‘with’context manager instead,\ncalling it ‘fork_state’. This allows for equivalent but more readable code.\nFor the GPT-4o and GPT-4o mini multimodal baselines, we directly use images as inputs. (See\nSection A.5.8 for the prompt template.)\nA.2\nSyntheic Dataset Generation and Training Parameters\nWe present the dataset generation and training parameters in Table. 2 and Table. 3.\nA.3\nAdaptation Implementation Details\nFor adaptation experiments, we generally followed the settings described above, with a few specific\ndifferences detailed below.\nA.3.1\nString Tasks\nTo induce a domain gap (easier problems in Sygus compared to the harder, noisier problems in\nthe Prose Benchmark), we first fine-tuned a model using Sygus problems and then tested it on the\nProse Benchmark. Due to the noisy nature of the Prose Benchmark (some problems have very few\nexamples), we adopted a setting where we utilized all the test cases to select which problems were\n15\n\nList\nString\nSeed Dataset Source\nRule et al.\nProse (FlashFill++) Problems\nSeed Dataset Size\n50\n40\nSynthetic Data Generator\ndeepseekcoder-33b-instruct\ndeepseekcoder-33b-instruct\nSynthetic Dataset Size\n10k\n10k\nSampling Tempereature\n0.8\n1.0\nSimilarity Filter\ncode embedding model\n-\nFilter Ratio\naround 1/3 (threshold=0.93)\n-\nSynthetic Data Prompt\n4-shot examples\n10-shot examples\nLoRA Finetuning\nModel Used\ndeepseekcoder-1.5-7b-instruct\ndeepseekcoder-1.5-7b-instruct\nLoRA Rank\n1024\n256\nLoRA α\n1024\n256\nLearning Rate\n2.00E-04\n2.00E-04\nLR Schedule\ncosine\ncosine\nWarmup Steps\n10\n10\nEpoch\n1\n1\nBatchsize\n32\n16\nLoRA\n33b Model Used for FT\ndeepseekcoder-33b-instruct\ndeepseekcoder-33b-instruct\nLoRA\n256\n128\nLoRA α\n256\n128\nLearning Rate\n2.00E-04\n2.00E-04\nLR Schedule\ncosine\ncosine\nWarmup Steps\n10\n10\nEpoch\n1\n1\nBatchsize\n32\n32\nTable 2: List task and String task synthetic dataset generation and finetuning parameters.\nsolved and then used them as the seed programs for adaptation. This resulted in 64 solved problems\nout of the 100 problems in the benchmark.\nA.3.2\nList Tasks\nTo obtain the finetuned in-distribution result in Fig. 8a, we fine-tuned on a synthetic dataset generated\nby seeding with 20 out of 100 problems from LambdaBeam, and tested on the remaining 80 problems.\nA.3.3\nLOGO Tasks\nFor LOGO adaptation experiments, we induce domain gap by using the shorter programs (LoC ≤12)\nof the training set, and tested on the longer programs (LoC > 12). The shorter programs training\nseed consists of around 80% problems (156 out of 200) from the original training set. The test set\nconsists of 31 problems out of 111 problems from the original test set.\nA.4\nModel Performance on LambdaBeam Benchmark\nWe present the results of both our 7B and 33B models on the LambdaBeam benchmark in Figure 10.\nWe observed that even without fine-tuning for this specific benchmark, and instead fine-tuned for\nthe list-to-list problems from Rule et al., our models performed exceptionally well, surpassing the\nstate-of-the-art results specifically designed for the LambdaBeam problems [12].\nA.5\nPrompts Used in the Experiments\nA.5.1\nSyntheic Data Generation Prompt\nList\n16\n\nLogo\nSeed Dataset Source\nRegal Python Logo Programs\nSeed Dataset Size\n200\nSynthetic Data Generator\ndeepseekcoder-33b-instruct\nSynthetic Dataset Size\n32k\nSimilarity Filter\n-\nFilter Threshold\n-\nSynthetic Data Prompt\n6-shot examples\nLoRA Finetuning\nModel Used\ndeepseekcoder-1.5-7b-instruct\nLoRA Rank\n512\nLoRA α\n512\nLearning Rate\n2.00E-04\nLR Schedule\ncosine\nWarmup Steps\n20\nEpoch\n3\nBatchsize\n64\nLoRA Finetuning\nModel Used\ndeepseekcoder-33b-instruct\nLoRA Rank\n512\nLoRA α\n512\nLearning Rate\n2.00E-04\nLR Schedule\ncosine\nWarmup Steps\n50\nEpoch\n3\nBatchsize\n64\nTable 3: Logo task synthetic datasets generation and finetuning parameters\n0\n250\n500\n750\n1000\nSearch Budget (Num Samples)\n10\n20\n30\n40\n50\n60\n70\n% Problems Solved\nLambdaBeam\nL2\nRobustFill\nours-33b\nours-7b\nFigure 10: Performance on LambdaBeam problems from Shi et al. 2023\n17\n\nYou are a CS professor. You are\nproviding a set of challenging\nand\ndiverse\ninteger\nlist to integer\nlist\nfunction\npuzzle\nfor your\nstudent to solve.\nPuzzle 1:\nPython\nfunction: input a list of integers\nand return a list of\nintegers\n‘‘‘python\n{PROGRAM\nEXAMPLE 1}\n‘‘‘\nTest\ncases:\n...\nPuzzle 2:\nPython\nfunction: input a list of integers\nand return a list of\nintegers\n‘‘‘python\n{PROGRAM\nEXAMPLE 2}\n‘‘‘\nTest\ncases:\n...\nFollowing\nthe above format , please\nprovide 3 functions\neach\nfollow by 10 random\ntest\ncases to check the function ’s\ncorrectness\nfull\ncoverage\nString\nExcel\njust\nintroduce a new\nfeature\nthat\nallows\nuser to use\nPython to perform\ndata\ntransformation.\nPlease\ngenerate a csv file with two\ncolumns. The first\ncolumn\ncontains\nthe input\ndata and the second\ncolumn\ncontains\nthe\noutput\ndata.\nFollowing a accompanying\npython\nfunction\nwhich\nshowcasing\nthe\ntransformation of the input\ndata to the output\ndata.\nHere are 10 challenging\nexamples\nshowcaing\nthe\nfeatures\nExample 1\n‘‘‘csv\n{INPUT\nOUTPUT\nEXAMPLE 1}\n‘‘‘\nHere is the Python\nfunction\nthat help\ntransform\nthe first\ncolumn to the second\ncolumn.\n‘‘‘python\n{PYTHON\nEXAMPLE 1}\n‘‘‘\nExample 2\n‘‘‘csv\n{INPUT\nOUTPUT\nEXAMPLE 2}\n‘‘‘\nHere is the Python\nfunction\nthat help\ntransform\nthe first\ncolumn to the second\ncolumn.\n‘‘‘python\n{PYTHON\nEXAMPLE 2}\n‘‘‘\n...\n18\n\nFollowing\nthe above format , please\nprovide a CSV file with two\ncolumns , containing\nbetween 5 to 10 rows of data\nshowing a\ntransformation\nfrom the first\ncolumn to the second\ncolumn.\nThis csv data\nshould\nillustrate a challenging\nand\ncomplex\nexample , similar to the above\nexamples. Following that ,\ncreate a Python\nfunction\ndesigned to process\nthis data. Be\naware\nthat this\nfunction\nwill be tailored to not only\naccommodate\nthe\ncurrent\ndata but also any future\ndata that\nfollows the same\nformat or structure.\nLogo\nYour task is to draw\nsimple\nblack and white\ngraphics\nwith the\ncustom\nlibrary. DO NOT USE THE BUILT -IN TURTLE\nLIBRARY.\nYou will use a custom\nturtle\nlibrary , similar to the built -in\nturtle\nlibrary , which is sufficient\nfor all tasks.\nHere are all the\navailable\nfunctions in the custom\nturtle\nlibrary:\n- forward(x): move\nforward x pixels\n- left(theta): rotate\nleft by theta\ndegrees\n- right(theta): rotate\nright by theta\ndegrees\n- penup (): stop\ndrawing\n- pendown (): start\ndrawing\n- teleport(x, y, theta): move to position (x, y) with\nangle\ntheta\n- heading (): get the\ncurrent\nangle of the turtle\n- isdown (): check if the pen is down\n- forward(x): Move\nforward x pixels.\n- left(theta): Rotate\nleft by theta\ndegrees.\n- right(theta): Rotate\nright by theta\ndegrees.\n- penup (): Stop\ndrawing.\n- pendown (): Start\ndrawing.\n- teleport(x, y, theta): Move to position (x, y) with\nangle\ntheta.\n- heading (): Get the\ncurrent\nangle of the turtle.\n- isdown (): Check if the pen is down.\n- with\nfork_state (): A context\nmanager\nthat runs the code in\nthe block\nusing the\ncurrent\ncontext\nand\nrestores\nthe\noriginal\nstate\nafterwards. Allows you to nest\nprograms.\nInternally , fork_state\nsaves the turtle\nstate (is_down , x,\ny, heading), executes\nthe block , then\nrestores\nthe\noriginal\nstate.\nGraphic 1\nPython\nprogram: draw an interesting\ngraphic\nusing our own\ncustom\nturtle\nlibrary\n# the\nfollowing\nprogram\ndraws ...\n{PROGRAM\nEXAMPLE 1}\nGraphic 2\nPython\nprogram: draw an interesting\ngraphic\nusing our own\ncustom\nturtle\nlibrary\n# the\nfollowing\nprogram\ndraws ...\n{PROGRAM\nEXAMPLE 2}\n...\nFollowing\nthe above format , please\nprovide 5 more\nprograms\nusing our custom\ndrawing\nlibrary.\n19\n\nA.5.2\nPrompt Template for Finetuning and Zero-Shot Experiments\nA.5.3\nList\nImplement\nthe\nfunction\nsolve_puzzle\nthat\ntakes a list of\nintegers\nand\nreturns a list of integers. The\nfunction\nshould\nsatisfy\nthe\nfollowing\nassertions\nassert\nsolve_puzzle (...) == ...\nassert\nsolve_puzzle (...) == ...\nassert\nsolve_puzzle (...) == ...\n...\nA.5.4\nList (Chain-of-Thought)\nImplement\nthe\nfunction\nsolve_puzzle\nthat\ntakes a list of\nintegers\nand\nreturns a list of integers. The\nfunction\nshould\nsatisfy\nthe\nfollowing\nassertions:\nassert\nsolve_puzzle (...) == ...\nassert\nsolve_puzzle (...) == ...\nassert\nsolve_puzzle (...) == ...\n...\nPlease\nobserve the\nrelation\nbetween\ninput and output and think\nstep by step. Output the\nfunction in a markdown\nformat in\nthe end.\nSolution:\nA.5.5\nString\nImplement\nthe\nfunction\nedit_text\nthat\ntakes a string and\nreturns a string. The\nfunction\ntransforms\nthe input\nstring\nto the output\nstring. The\nfunction\nshould\nsatisfy\nthe\nfollowing\nassertions:\nassert\nedit_text (...) == ...\nassert\nedit_text (...) == ...\nassert\nedit_text (...) == ...\n...\nA.5.6\nString (Chain-of-Thought)\nPlease\nimplement\nthe\nfunction\nedit_text\nthat\ntakes a string\ninput and\nreturns a modified\nstring.\nNote that you can import re , datetime , or any built -in Python\nlibrary to solve the\nproblem.\nThe\nfunction\nshould\nsatisfy\nthe\nfollowing\ntest\ncases:\nassert\nedit_text (...) == ...\nassert\nedit_text (...) == ...\nassert\nedit_text (...) == ...\n...\nPlease\nreason\nthrough\nthe\nproblem\nand think\nstep by step , and\nfinally\nimplement\nthe\nfunction\nand output the full\nfunction\nimplementation in a markdown\ncode\nblock in the end.\nA.5.7\nLogo\nHere is a gray\nscale\nimages\nrepresenting\nwith\ninteger\nvalues\n0-9.\n{CONVERTED\nIMAGE\nSTRING }...\n20\n\n...\nPlease\nwrite a Python\nprogram\nthat\ngenerates\nthe image\nusing\nour own custom\nturtle\nmodule\nA.5.8\nLogo (Multimodal Few-shot)\nGiven the\nfollowing\ncustom\nTurtle\ngraphics -like library , Please\nuse it to write a program\nthat\ndraws the given\nimage.\n‘‘‘python\nfrom\nmyturtle\nimport\nTurtle\nfrom\nmyturtle\nimport\nHALF_INF , INF , EPS_DIST , EPS_ANGLE\nturtle = Turtle ()\ndef\nforward(dist):\nturtle.forward(dist)\ndef left(angle):\nturtle.left(angle)\ndef right(angle):\nturtle.right(angle)\ndef\nteleport(x, y, theta):\nturtle.teleport(x, y, theta)\ndef penup ():\nturtle.penup ()\ndef\npendown ():\nturtle.pendown ()\ndef\nposition ():\nreturn\nturtle.x, turtle.y\ndef\nheading ():\nreturn\nturtle.heading\ndef isdown ():\nreturn\nturtle.is_down\ndef\nfork_state ():\n\"\"\"\nFork the current\nstate of the turtle.\nUsage:\nwith\nfork_state ():\nforward (100)\nleft (90)\nforward (100)\n\"\"\"\nreturn\nturtle._TurtleState(turtle)\n‘‘‘\nBelow are some\nexample\nprograms\nthat draw the given\nimages.\n<IMAGE >\nHere is a program\nthat\ndraws the above\nimage.\nThe figure is like a medium 8 gon.\n‘‘‘python\nfor i in range (8):\nforward (4)\nleft (45.0)\n‘‘‘\n<IMAGE >\nHere is a program\nthat\ndraws the above\nimage:\nThe figure is like 5 sided\nsnowflake\nwith a medium\ncircle and a\nmedium\nsemicircle as arms.\n‘‘‘python\nfor j in range (5):\nwith\nfork_state ():\npenup ()\nforward (2)\n21\n\nleft (0.0)\npendown ()\nfor i in range(HALF_INF):\nforward(EPS_DIST *2)\nleft(EPS_ANGLE)\nfor i in range(HALF_INF):\nforward(EPS_DIST *2)\nleft(EPS_ANGLE)\npenup ()\nforward (2)\nleft (0.0)\npendown ()\nfor i in range(HALF_INF):\nforward(EPS_DIST *2)\nleft(EPS_ANGLE)\nforward (0)\nleft (72.0)\"\n‘‘‘\n<IMAGE >\nHere is a program\nthat\ndraws the above\nimage.\nThe figure is like 7 concentric\ncircles.\n‘‘‘python\nfor j in range (8):\nfor i in range(HALF_INF):\nforward(EPS_DIST*j)\nleft(EPS_ANGLE)\nfor i in range(HALF_INF):\nforward(EPS_DIST*j)\nleft(EPS_ANGLE)\n‘‘‘\n<TEST_IMAGE >\nOutput the program\nthat\ndraws the\nfollowing\nimage. Reason\nabout\nthe given\nimage and write a program\nthat\ndraws it in a\nmarkdown\ncode\nblock.\nName\nDescription\nours-7B\nours-33B\nDedup\nRemove duplicate elements from a list.\n✓\n✓\nReverse\nReverse a list.\n✓\n✓\nDroplast\nDrop the last element in a list.\n✓\n✓\nDropmax\nDrop the largest number(s) in a list.\n✓\n✓\nDupli\nDuplicate each element of a list.\n✓\n✓\nEvens\nRemove the odd numbers from a list.\n✓\n✓\nMultfirst\nReplace every item in a list with the first item.\n✓\n✓\nMultlast\nReplace every item in a list with the last item.\n✓\n✓\nShiftl\nShift all elements in a list to the left.\n✓\n✓\nShiftr\nShift all elements in a list to the right.\n✓\n✓\nTable 4: 10 list 7→list functions from λ2 [13]\n22\n\nFigure 11: All LOGO test problems: problems solved by our finetuned 33b model are marked as\ngreen\n23\n\nFigure 12: Hand-drawn LOGO test showing every generated sample. We built a graphical interface\nto allow users to draw images as input. The sample budget for this demo is 64.\n24\n\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: We provide experiment results to support our claims.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: We discuss several limitations in the limitation section.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [NA]\n25\n\nJustification: The paper does not include theoretical results.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We provide training settings and prompts used in the appendix.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\n26\n\nAnswer: [No]\nJustification: We provided the detailed training settings and prompts in the appendix.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental Setting/Details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: We provide experimental settings and details in the appendix.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment Statistical Significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: We provide error bars or confidence intervals when applicable.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n27\n\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments Compute Resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: We provide the information in the appendix.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: The paper conform with the NeurIPS Code of Ethics.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer:[NA]\nJustification: The paper focuses on narrow domains of PBE problems and has limited\nimpacts on society.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n28\n\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: The paper poses a low risk.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: We credit and follow the license terms properly.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n29\n\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: The paper does not release new assets.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing or human subjects\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing or research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n30",
    "pdf_filename": "Is_Programming_by_Example_solved_by_LLMs.pdf"
}