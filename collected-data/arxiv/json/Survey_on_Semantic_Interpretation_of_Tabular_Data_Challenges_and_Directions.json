{
    "title": "Survey on Semantic Interpretation of Tabular Data Challenges and Directions",
    "abstract": "Tabular data plays a pivotal role in various fields, making it a popular format for data manipula- tion and exchange, particularly on the web. The interpretation, extraction, and processing of tabular information are invaluable for knowledge-intensive applications. Notably, significant efforts have been invested in annotating tabular data with ontologies and entities from background knowledge graphs, a process known as Semantic Table Interpretation (STI). STI automation aids in building knowledge graphs, enriching data, and enhancing web-based question answering. This survey aims to provide a comprehensive overview of the STI landscape. It starts by categorizing approaches using a taxonomy of 31 attributes, allowing for comparisons and evaluations. It also examines available tools, assessing them based on 12 criteria. Furthermore, the survey offers an in-depth analysis of the Gold Stan- dards used for evaluating STI approaches. Finally, it provides practical guidance to help end-users choose the most suitable approach for their specific tasks while also discussing unresolved issues and suggesting potential future research directions.",
    "body": "This work is shared under a CC BY-SA 4.0 license unless otherwise noted\nSurvey on Semantic Interpretation of Tabular Data: Challenges\nand Directions\nMarco Cremaschi1\nBlerina Spahiu2\nMatteo Palmonari1\nErnesto Jimenez-Ruiz2\n1University of Milano - Bicocca\n{marco.cremaschi,blerina.spahiu,matteo.palmonari}@unimib.it\n2City, University of London\nernesto.jimenez-ruiz@city.ac.uk\nAbstract\nTabular data plays a pivotal role in various fields, making it a popular format for data manipula-\ntion and exchange, particularly on the web. The interpretation, extraction, and processing of tabular\ninformation are invaluable for knowledge-intensive applications. Notably, significant efforts have been\ninvested in annotating tabular data with ontologies and entities from background knowledge graphs,\na process known as Semantic Table Interpretation (STI). STI automation aids in building knowledge\ngraphs, enriching data, and enhancing web-based question answering. This survey aims to provide a\ncomprehensive overview of the STI landscape. It starts by categorizing approaches using a taxonomy\nof 31 attributes, allowing for comparisons and evaluations. It also examines available tools, assessing\nthem based on 12 criteria. Furthermore, the survey offers an in-depth analysis of the Gold Stan-\ndards used for evaluating STI approaches. Finally, it provides practical guidance to help end-users\nchoose the most suitable approach for their specific tasks while also discussing unresolved issues and\nsuggesting potential future research directions.\nKeywords: Semantic Table Interpretation, Semantic Annotation, Table, Knowledge Graph, Table-\nto-KG Matching, Semantic Web\n1\nIntroduction\nTables are widely used and play a crucial role in creating, organising, and sharing information. A notable\nexample of their significance as ways to organise human knowledge can be found in the oldest sample\nof writing on paper (on papyrus), dating back to around 2500 BC, in which Merer, an Egyptian naval\ninspector, documents his daily activities in a table (Fig. 1) [156].\nFigure 1: Portion of the diary of Merer (around 2600 BC), an official in charge of a team of workers\nresponsible for transporting limestone blocks from Tura to Giza to construct the Great Pyramid. The\ndocument details various aspects of the logistics involved in the transportation process, such as the\norganisation of labour, the use of boats to navigate the Nile River, and the daily activities of the workers.\n1\narXiv:2411.11891v1  [cs.AI]  7 Nov 2024\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nToday, tables are extensively used in both business and scientific sectors, primarily in the form of\nspreadsheets and other tabular data formats. They frequently appear in documents, including web pages,\nand are used to publish large amounts of data on the web, especially after the uptake of the Open Data\nmovement. The large amount of tabular data consumed today covers a wide range of domains, such as\nfinance, mobility, tourism, sports, or cultural heritage [119]. The relevance and diversity of tabular data\ncan be sized by looking at the number of available tables and/or users of tabular data manipulation\ntools:\n• Web tables: in 2008 14.1 billion HTML tables were extracted, and it was found that 154 million\nwere high-quality tables (1.1%).\nIn the Common Crawl 2015 repository, there are 233 million\ncontent tables1;\n• Wikipedia tables: the 2022 English snapshot of Wikipedia contains 2 803 424 tables from 21 149 260\narticles [110];\n• Spreadsheets: there are 750 million to 2 billion people in the world who use either Google Sheets\nor Microsoft Excel2.\nThe heterogeneity of tables and their application domains reflects differences in characteristics, such\nas the size, cleanliness, and availability of human-interpretable descriptions (headers, metadata, descrip-\ntions of natural languages). Despite the simplicity of tabular data, understanding their meaning and\nautomating several downstream tasks remains challenging [60, 133].\nSemantic Table Interpretation (STI) encompasses various tabular data interpretation tasks that in-\nvolve labeling an input table using reference knowledge bases and shared vocabularies (these tasks\nare sometimes also referred to as table annotation or semantic modeling\n[151]).\nSince Knowledge\nGraphs (KGs) have become one of the most popular abstractions for knowledge bases and are equipped\nwith shared vocabularies, STI can be understood as a table to KGs matching problem. KGs are used to\nrepresent relationships between different entities such as people, places, mountains, events, and so on [72].\nThey organise knowledge in graph structures where the meaning of the data is encoded alongside the data\nin the graph. Resource Description Framework (RDF)3 is a data model for representing KGs that come\nwith an ecosystem of languages and protocols to foster interoperable data management. In RDF, most\nof the graph nodes represent instances and classes - referred to here as entities and types, respectively-\nand are identified by URIs or literals (e.g., strings, numbers); most of the edges, each labeled by an RDF\nproperty, represent relations between nodes, i.e, two entities or an entity and a literal. Some of these\nedges link entities to their types (e.g., dbo:City) or datatypes (e.g., xmls:integer). Finally, some edges\nare used to model the ontologies that organise the knowledge (e.g., subclass relations between types)\nand specify the meaning of the terms used in the graph through logical axioms. Labeling the elements\nof a table with elements of a knowledge graph supports their interpretation, e.g., by disambiguating the\nmeaning of the headers, or of the values that correspond to entities, and it is possible to transform the\ntable into actionable knowledge in different downstream application (see Section 1.1).\nSTI has developed as an active research area attracting the scientific community’s attention. An\nextensive number of approaches have been proposed to tackle STI tasks, from those that use heuristic\nmatching methods to those that use or include feature-based machine learning methods [106, 88] to the\nlatest ones that use or are entirely based on Pre-trained Language Models (PLM) such as BERT [171, 76]\nor generative Large Language Models (LLMs) like Llama [60, 179]. Additionally, studies have examined\nhow users approach reading tables [37]. Contributions to STI include methods inspired by a variety\nof Artificial Intelligence (AI ) paradigms and have been published in AI journals and conferences or\nin journals and conferences related to the sibling fields of Semantic Web, Natural Language Processing\n(NLP), and Data Management (for more details see Fig. 8) [81, 83, 45]; this broad scope suggest that the\ntopic is considered relevant across different research sub-communities. Another initiative reflecting the\ninterest in the topic is the international Semantic Web Challenge on Tabular Data to Knowledge Graph\nMatching (SemTab), which has been proposed since 20194 and still running in 2024 [81, 83, 45]. The\ninitiative represents a community-driven effort to formalize the different STI tasks and develop shared\nevaluation protocols to compare different approaches.\nIn this paper, we propose a comprehensive survey on approaches proposed to address STI tasks,\nalso covering the latest approaches based on PLMs and LLMs. In Section 1.1, we present STI tasks\n1commoncrawl.org\n2askwonder.com/research/number-google-sheets-users-worldwide-eoskdoxav\n3www.w3.org/RDF/\n4cs.ox.ac.uk/isg/challenges/sem-tab\n2\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nName\nLe Mout Blanc\nHohtälli\nMonte Cervino\nCoordinates\n45° 49′ 57″ N, 06° 51′ 52″ E\n45° 59′ 20″ N, 7° 48′ 10″ E\n45° 58′ 35″ N, 07° 39′ 31″ E\nHeight\n4808\n3275\n4478\nRange\nM. Blanc massif\nPennine Alps\nPennine Alps\nInput Data\nFigure 2: Example of a well-formed relational table.\nMountain\nNaturalPlace\nMount_Blanc\nMount_Cervino\n4808\nMount_Blanc_ \nMassif\nxsd:string\n...\n...\nxsd:integer\nMountain Range\nSchema Level\nEntity Level\nReference Knowledge Graph\ngeorss:point\ndbo:elevation\ndbo:elevation\ndbo:mountainRange\ndbo:mountainRange\n...\n...\nFigure 3: A sample of Knowledge Graph.\nmore precisely and summarise the impact of STI on research and applications and their challenges; In\nSection 1.2, we summarise the contributions of our paper and present its structure.\n1.1\nSTI: key definitions, impact and challenges\nIn its most agreed and complete formalisation, the STI process considers two inputs: i) a relational table,\nwhich is usually assumed to be well-formed and normalised (i.e., the table has a grid structure, where\nthe first row may contain the table headers and any other row contains values), as in Fig. 2; and ii) a\nreference Knowledge Graph (KG) with its vocabulary (i.e., a set of symbols denoting concepts, datatypes,\nproperties, instances - also referred to as entities in the following) as in Fig. 3). The output of the STI\nprocess is a semantically annotated table, i.e., a table where its elements, typically values, columns,\nand column pairs, are annotated with symbols from the KG vocabulary. The exact specification of the\nannotations expected as the output of the STI process may differ in the proposed approaches. Here we\ndiscuss a canonical definition of a semantically annotated table to provide a first understanding of key\nSTI tasks, inspired by the SemTab Challenge, where the annotation process has been better formalised\nwith a community-driven effort.\nTo discuss this canonical definition, we use the example reported in Fig. 4.\nxsd:integer\nxsd:string\nName\nLe Mout Blanc\nHohtälli\nMonte Cervino\nCoordinates\n45° 49′ 57″ N, 06° 51′ 52″ E\n45° 59′ 20″ N, 7° 48′ 10″ E\n45° 58′ 35″ N, 07° 39′ 31″ E\nHeight\n4808\n3275\n4478\nRange\nM. Blanc massif\nPennine Alps\nPennine Alps\nMountain\nMountain_Range\nMount_Blanc\nMount_Blanc_Massife\n[NIL: Hohtälli]\nPennine_Alps\nMount_Cervino\nPennine_Alps\nAnnotated table\ngeorss:point\ndbo:elevation\ndbo:mountainRange\nMountain\nMount_Blanc\nHohtälli\nMount_Cervino\n4808\n3275\n45° 49′ 57″ N \n06° 51′ 52″ E\n45° 59′ 20″ N    \n7° 48′ 10″ E\nMount_Blanc_ \nMassif\nPennine_Alps\nxsd:string\nxsd:integer\nMountain Range\nStatements derived from annotations\ndbo:elevation\ngeorss:point\ngeorss:point\ndbo:elevation\ndbo:mountainRange\ndbo:mountainRange\nSchema Level\nEntity Level\n...\nFigure 4: Example of an annotated table.\nGiven:\n• a relational table T (Fig. 2);\n• a Knowledge Graph and its vocabulary (Fig. 3).\nT is annotated when:\n• each column is associated with one or more types from the KG [Column-Type Annotation (CTA)];\ne.g., the column Name in the Fig. 2 is annotated with the type Mountain; the column Height is\nannotated with datatype xsd:integer;\n• each cell in “entity columns” is annotated with an entity identifier or with NIL, if no entity in\nthe KG corresponds to the cell value [Cell-Entity Annotation (CEA)]; e.g., the cell Le Mout Blanc\nis annotated with Mont Blanc; the cell Hoht¨alli is annotated with NIL since it has not yet been\nincluded in the KG;\n3\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n• some pairs of columns are annotated with a binary property [Columns-Property Annotation (CPA)];\ne.g., the pairs composed by the columns Name and Hight is annotated with dbo:elevation.\nThe result of the annotation process for the table considered in the example is shown in Fig. 4. Observe\nthat each bullet point can be interpreted as a high-level STI task to complete; also, the annotations can\nidentify in the table new entities not included in the reference KG (e.g., Hoht¨alli).\nSTI plays a relevant role in the AI research and applications landscape. From a research perspec-\ntive, the capability of performing STI tasks such as CEA, CTA, CPA are considered part of a broader set\nof tabular data understanding skills [171, 53, 179], which impact the application of AI to tabular data.\nAlso, CEA can be considered a variant of entity linking in texts, while CTA and CPA are not too different\nfrom ontology matching when applied to different data formats (entity linking and ontology matching\nare both considered AI tasks [104, 95, 76]). From an application perspective, STI tasks can support\nthe automation of processes to construct and extend knowledge bases [166, 86] and enrich tabular data,\neventually supporting downstream applications to data analysis. To provide an idea of the contributions\nof STI to these automation processes, we refer to Fig. 5. For KG construction, CTA and CPA anno-\ntations support the automatic or semi-automatic transformation of the data into a graph format with\nthe schema of the reference KG [66, 135, 129, 154]; CEA annotations disambiguate values in the input\ntable, thus supporting the reuse of canonical entity identifiers in the generated data [46, 31]. The same\nprinciples can be applied to support KG extension processes [168, 178], by adding to the graph only the\nnew entities and triples represented in the table. STI annotations can be useful in data enrichment tasks\nwhere the generation of graph data is not needed, but links to entities in the KG can be used to query\nthe KG [68, 23] or other third-party data sources [126, 47, 32], augmenting the input data and extending\nthe features used to develop analytical models. Other applications of STI annotations proposed in the\nliterature or potentially impactful on emergent services include the improvement of search engines and\nrecommender systems for tabular data [18, 24, 176, 177, 17] and question answering [171, 50].\nFigure 5: Examples of applications supported by STI.\nMachine interpretation of tabular data is challenging because of the limited context available to\nresolve semantic ambiguities, the layout of tables that can be difficult to handle, and the incompleteness\nof KGs in general.\nKey challenges involved in the annotation process include:\n• Dealing with the heterogeneity of domains and data distributions: the tables may cover information\nthat refers to very different domains (e.g., Geography vs Sports); the specificity of the table content\nmay vary significantly (from a table with basic information about most famous mountains, like\nFig. 2 to table that contains the composition of the rocks of this mountains5).\n• Dealing with limited contextual information: if compared with similar interpretation and disam-\nbiguation tasks on the textual document, the presence of contextual clues to support the interpre-\ntation and annotation of table elements may be limited and very diverse depending on the data\nsources; for example, table headers are often missing. Tables in open data portals may be described\nby metadata, while tables published on web pages may have some surrounding text.\n• Detecting the type of columns: in a table, there can be columns that contain references to named\nentities (NE-columns) and columns that contain strings, numbers, dates, and, in general, instances\n5en.wikipedia.org/wiki/List of rock formations\n4\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nof specific data types, which we refer to as literals (LIT-columns); distinguishing between the two\ntypes of columns is crucial to support the annotation process.\n• Matching tabular values against the KG: matching the values in the table to the data in the KG\nhelps collect evidence to interpret the table. However, the values referring to entities in the table\nmay differ from their labels in the KG, e.g., because of acronyms, aliases, and typos, while other\nvalues representing their features may differ for several reasons, e.g., because outdated, measured\ndifferently, and so on.\n• Dealing with multiple entities with similar names: the KG may contain many entities with similar\nor even equal names (homonyms) that have different or even the same types. For example, the\nmention of the Italo-French mountain Mont Blanc in Fig. 2 matches labels of more than a dozen\nentities, including a tunnel, a poem, a dessert, and another mountain on the moon6.\n• Dealing with Not In Lexicon (NIL)-mentions: some entities referred to in the table may not exist\nin the KG; while several approaches perform CEA by simply selecting the best candidate, i.e., the\nentity with the highest score according to the approach, recognizing new entities requires a decision\nwhether to link or not to link, which may be subtle.\n• Choosing the most appropriate types and properties: the KG may contain hundreds or thousands of\ntypes and properties to choose from for annotating columns and column pairs. The features of large\nKGs make the decision even more challenging: entities are classified under multiple types, which\nmay reflect different levels of specificity (e.g., Mont Blanc can be classified as a mountain, a summit,\na pyramidal peak, and so on, up to a geographic location); the specificity of the classification may\nchange depending on the entity; several properties have similar meanings, associated with different\nlevels of specificity or different usage patterns [132].\n• Aggregate evidence from different tasks: the annotation of a table is, in principle, a collective\ndecision-making process; for example, the disambiguation of entities in a column helps suggest\ntypes to annotate the column (e.g., most of the best candidates for mountain mentions in Fig. 2\nare mountains), but a type or a set of types associated with a column may help disambiguate entities\nmentioned therein (e.g., non-mountain candidate entities for “Mont Blanc”); finding strategies to\nmaximise evidence exchange across tasks requires multiple iterations or sophisticated aggregation\nmechanisms.\n• Dealing with amount and shape of data: depending on the application scenarios, it may be necessary\nto process a large number of small tables or very large tables [31], which may imply different\nconstraints on the approaches or introduce slightly different challenges; more scenarios may also\nbecome more relevant in the future, such as processing streaming data that can be formatted as\ntabular data.\n1.2\nSpecific contributions and structure of the manuscript\nIn the presentation of this comprehensive survey on STI we make the following more specific contribu-\ntions, which highlights the main differences with previous surveys published on the same topic (see also\nSection 2.1 for a more detailed comparison):\n• A new taxonomy to organise and compare reviewed approaches comprising 31 specific attributes;\n• A new systematic literature review on 88 STI approaches published until October 2024, including\nlatest approaches based on LLMs;\n• Analysis of existing tools that support STI and a comparison between their functionality features;\n• Analysis of the Gold Standard (GS) used to evaluate STI approaches;\n• A guide that can help researchers and practitioners locate STI approaches most suited to their\ntasks;\n• Highlight and discuss open issues and future research directions.\n6en.wikipedia.org/wiki/Mont Blanc (disambiguation)\n5\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nThe paper is organised as follows: Section 2 highlights the differences between this survey and\nother similar surveys in the STI field and discusses the methodology used to collect all approaches\nreviewed in this paper. Section 3 defines a taxonomy composed of 31 attributes used to compare STI\napproaches. Sections from 4 to 9 help readers gain a comprehensive understanding of the techniques\nand solutions proposed so far.\nSection 10 draws open issues and future directions while Section 11\nconcludes the paper.\nThe appendix section contains valuable and additional information regarding\nSTI approaches. Appendix A details information about the methodology to conduct the survey and\ncomparisons of approaches. Appendix B discusses tools for the STI process while Appendix C analyses\nthe Gold Standards (GSs) used by STI approaches to evaluate their performance. Additional information\nabout approaches is provided in Appendix D.\n2\nScope and Methodology\nThis Section highlights the differences between this survey and other similar surveys in the STI field.\nThe second part describes the methodology we applied to our systematic literature review, based on\nthe well-established PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses)\nmethod7. Details on the systematic review results serve as a basis for the comprehensive analysis in the\nfollowing Sections.\n2.1\nDifferences from other STI surveys\nSTI approaches have been analysed in a few surveys [88, 177, 21, 106]; [88, 177, 21] have been published\nbefore the explosion in volume of STI related-works, also as a consequence of the SemTab challenge.\nMost recently, Liu et al. [106] aims to complement these surveys by providing a new classification of\nSTI approaches reflecting the heterogeneity of tabular data and the resulting new challenges. We aim to\nupdate and extend previous surveys by introducing a new classification schema of STI approaches and\ndiscussing new research directions in improving such systems. Nevertheless, our analysis encompasses\nnot only recent works but also older ones, allowing us to derive comprehensive guidelines (Section 10)\nfor selecting approaches based on specific user needs. Furthermore, we can identify and highlight the\nunresolved issues which are yet to be addressed.\nTo provide clarity, we highlight the following differences with the previously published surveys:\n• Survey scope: through a rigorous snowballing approach, we collected a comprehensive list of STI\napproaches that allowed us to discover 88 works. Moreover, our survey includes works of a wider\ntimeline (2007-2023);\n• Taxonomy: considering the comprehensive list of all the works in this field allowed us to specify\nand classify STI systems using different orthogonal dimensions.\nIn this survey, we identify 31\ndimensions;\n• Processes: providing a better understanding of the entire processes of STI by shedding light on\neach step;\n• Deeper investigation: examining a wider range of approaches enabled us to delve deeper into the\nfield, thus, helping researchers and practitioners to better understand and inspire improved or novel\napproaches. Similarly to [106], we delve into a more comprehensive comparison of the evaluation\nprocess;\n• Opportunity discovery: uncovering research opportunities of the existing approaches. For instance,\nunlike [177] and [106], we provide a more detailed analysis of the potentials of the available STI\napproaches;\n• Additional sections: including other important elements, e.g., delving deep into tools and GS, this\nsurvey provides the complete landscape of the STI process.\nA comparison of the surveys is presented in Table 1, which reflects the above attributes and highlights\nthe differences between them.\nTwo additional surveys [15, 62], which are partially relevant to STI, were also considered. Unlike [62],\nour work provides a more comprehensive analysis of various approaches to the Entity Linking (EL) task,\n7prisma-statement.org\n6\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\naccompanied by an in-depth discussion of the associated challenges. However, in contrast to [15], which\ndelves into the technical specifics of entity resolution using Neural Network (NN), our focus is less specific\non technical details because it covers all STI methodologies.\nAttribute\nThis survey\n2019 [88]\n2020 [177]\n2021 [21]\n2023 [106]\nNo. of Approaches\n85\n16\n47\n12\n42\nYears range\n2007 — 2023\n2009 — 2017\n2002 — 2019\n2010 — 2019\n2011 — 2021\nGold Standards\nanalysis\n21\n7 (brief analysis)\n8\nFormalisation of\nthe STI pipeline\nComparison\nat-\ntributes\n31\nTable classification\nNLP Tasks on Table\nOther Projects\nDiscover\nand\nUnder-\nstand\n6\nTable expansion\nTable interpretation\nTable search\nQuestion answering\nKG augmentation\n6\nTable expansion\nTable interpretation\nTable search\nQuestion answering\nKG augmentation\n0\n5\nLookup based\nIterative\nFeature based\nKG modelling\nTable modelling\nAdditional sections\nTools\n—\nTable classification\nTable corpora\n—\nTable classification\n(extension of [177])\nEvaluation comparison\nNote\n—\nOnly Web tables\nFocus on other\ndownstream tasks\nFocus on table\nunderstanding\n—\nTable 1: Comparison between surveys on STI.\n2.2\nMethodology\nThe objective of this systematic review is to provide a synthesis of the state of knowledge and suggestions\nfor future research. The PRISMA method has been designed to provide detailed reporting guidelines\nfor such reviews to ensure a comparable and comprehensive result. This method typically encompasses\nthree stages: i) identification, ii) screening, and iii) selection. In this section, we provide a short overview\nof the methodology employed to conduct this survey. Please refer to Appendix A.1 for more details.\nIn the identification stage, to efficiently search in different databases for related works, we defined\na set of 16 keywords related to semantic table interpretation. These keywords were ranked based on\nrelevance by five researchers. We conducted searches on platforms including Scopus, Web of Science,\nDBLP, and Google Scholar, covering the period from 2007 to May 2023. We also employed a snowballing\ntechnique to include recent publications referencing key works.\nInstead, in the screening stage, two experts manually reviewed the identified papers, focusing on the\nsemantic table interpretation phases of the approaches and their relevance. A categorization process\nwas performed based on title, abstract, and keywords. Specific criteria were used, including generic and\nspecific annotation tags, to determine relevance.\nIn the selection stage, publications included in this survey were required to be directly related to\nsemantic table interpretation, published in English, and peer-reviewed. Using the specified keywords\ndefined within the PRISMA method, 134 papers were initially identified, which were reduced to 111\nafter the screening process. Manual annotation and further screening led to the exclusion of 17 papers,\nresulting in a total of 88 approaches discussed in the survey.\n3\nTaxonomic analysis of STI Approaches\nIntroducing a taxonomy of features that characterise different STI approaches8 as to main objectives:\ni) defined STI more precisely by describing the tasks and subtasks, ii) allows us to comprehensively\nunderstand the various approaches and their unique contributions to the field. Fig. 6 depicts a high-level\ntaxonomy of the features of STI approaches. The features are organised into dimensions. By analysing\neach dimension individually, we will present the main characteristics of the approaches proposed so far.\nThe reader should note that many dimensions are orthogonal; thus, an approach may be classified into\nmultiple other ones. To ensure that the information presented in the survey was verified and complete,\nwe contacted the authors via email and received 45 responses.\n8unimib-datai.github.io/sti-website/approaches/\n7\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nSTI \nAPPROACHES\nTAXONOMY\nClosed\nsource\nOpen\nsource\nLicence\nValidation\nApplication/\nPorpose\nIndependent\nDependent\nDomain\nUser Interface/\nTool\nCode availability\nOutput\nformat\nFeature\nAnnotation\nTable\nSource\nKG\nTriple\nstore\nIndex\nInput\nRevision\nFully \nautomated\nSemi\nautomated\nUnsupervised\nMethod\nSupervised\nHybrid\nCNEA\nCPA\nCTA\nCEA\nTask\nSubtask\nNIL annotation\nPredicate\nannotation\nDatatype\nannotation\nSubject\ndetection\nType\nannotation\nColumns\nclassification\nEntity\nlinking\nFigure 6: A taxonomy of the STI approaches.\nTASKS\nTASKS provide the conceptualisation of the output that STI approaches are expected to\nreturn.\nTASKS have been defined precisely because the quality of the output of the approaches is\nusually evaluated (only) against them. In this paper, we consider the TASKS that have been formalised\nin previous works [82, 83] or have appeared in the latest challenges:\n• CTA: the CTA task concerns the prediction of the semantic types (i.e., KG classes) for every given\ntable column in a table.\n• CPA: the CPA task concerns the prediction of semantic properties (i.e., KG properties) that\nrepresent the relationship between some pair of columns.\n• CEA: the CEA task aims to predict the entity (i.e., instances) that a cell in table represents.\n• Cell-New Entity Annotation (CNEA): the CNEA task aims to predict which cell in the table\nrepresents an entity that does not occur in the KG and should be therefore labelled as NIL.\nNote that CTA (resp. CEA) task focuses on annotating table columns (resp. cells) that can be\nrepresented with a KG class/type (resp. KG entity). The formal definition presented in this Section is\nmore precise but less flexible than the intuitive definition provided in the introduction. The reason for\nthis choice is that most existing approaches in the literature focus on specific tasks that require a more\nrigorous definition. However, in the last section of this work, we will explore an expanded version of this\nformalisation that accounts for different application scenarios.\nSUB-TASK\nThe STI process involves coordinating multiple specific annotation sub-tasks that con-\ntribute to the final results. These sub-tasks are more focused than the overall conceptual tasks mentioned\nearlier. Approaches may vary in how they coordinate and the algorithms they employ for each sub-task.\nThis aspect evaluates an approach’s coverage of the sub-tasks within the STI process. We believe that\nthe granularity of the sub-task classification is the best one to report different techniques proposed in\nthe literature so far (Section 4). Some approaches implement just a few sub-tasks, while others consider\nthe implementation of all sub-tasks listed below.\n(i) Column Classification considers the content of the cells of each column to mark a column as Literal\ncolumn (LIT-column) if values in cells are literals (e.g., strings, numbers, dates such as 4808, 10/04/1983),\nor as Named-Entity column (NE-column) if values are entities, instances of types (e.g., Mountain, Moun-\ntain Range such as Le Mout Blanc, M. Blanc massif). The Column Classification sub-task is useful,\nespecially, for the CEA and CTA tasks because the identification of NE-columns helps to concentrate\nthe Entity linking task on specific cells and the Type Annotation tasks on specific columns;\n8\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n(ii) Subject Detection has the goal of identifying, among the NE-columns, the column that all the\nothers are referring to, also called S-column (e.g., the Name column in Fig. 4). The Subject Detection,\nin some cases, is useful for the CPA task because it allows to find relations between other columns;\n(iii) Type Annotation pairs NE-columns with concepts of the KG (e.g., the column Name is annotated\nwith Mountain in DBpedia). This sub-task represents the final output of the CTA task;\n(iv) Entity linking links cell to entity in the KG (e.g., the cell Le Mout Blanc is annotated with\ndbo:Mont Blanc in DBpedia). This sub-task represents the final output of the CEA task;\n(v) Datatype Annotation pairs LIT-columns with a datatype in the KG (e.g., the column Coordinates\nis of type georss:point). The Datatype Annotation sub-task is used in CPA because fine-grained types\nof Lit-columns are easier to match against KGs;\n(vi) Predicate Annotation identifies the relations between each pair of columns (e.g., Name dbo:elevation\nHeight). This sub-task represents the final output of the CPA task;\n(vii) NIL Annotation considers strings that refer to entities for which a representation has not yet\nbeen created within the KG, namely NIL-mentions (e.g., the mention Hoht¨alli). This sub-task represents\nthe final output of the CNEA task.\nThese sub-tasks are sometimes preceded by a sub-task of Data Preparation that is used to normalise\nthe contents (e.g., by standardising the case of letters and the format of numbers) to avoid the presence\nof syntactical discrepancies that can make annotation techniques ineffective.\nEach sub-task is computed by annotating cell values referring to one or more KGs. The general\napproach consists of searching the KG with the content of columns to find possible matching.\nFor\nexample, if the majority of entities in the Name column (Fig. 4) is associated with dbo:Mountain, then\nall entities in the column can be assumed of type dbo:Mountain. Similarly, if the majority of the concepts\nof type dbo:Mountain are connected to datatypes of type xsd:integer by the property dbo:elevation,\nthen it can be identified as the property connecting the Name column with the Height column. However,\nit is important to note that this approach only applies to trivial cases. Employing advanced methods\nand techniques to address ambiguous results frequently occurring in real-world tables becomes necessary\nto tackle more complex scenarios. These methods aim to identify suitable matches for elements that\ncannot be directly linked to entities within a KG.\nMETHOD\nAnother crucial dimension in reviewing works on STI is the classification based on the\nmain algorithmic idea behind each approach. It is possible to identify three METHODS:\n• SUPERVISED category collects approaches that rely on a training set (e.g., a set of tables already\nannotated) that learn the annotations before applying them to the target tables;\n• UNSUPERVISED category collects approaches that do not use annotated data;\n• HYBRID category instead, collects approaches combining the above two categories.\nREVISION\nSome approaches are fully automated, while others require user intervention to select or\nvalidate annotations.\nDOMAIN\nApproaches can target tables with general or specific data (e.g., bio data, geospatial data).\nAPPLICATION/PURPOSE\nThis dimension refers to the use of the approach for particular appli-\ncation purposes such as data enrichment, KG construction and KG extension. Annotated data might\nbe used as links to find new information for the entities in the table, thus enriching the input, or oth-\nerwise to extend and enrich KGs as described in Section 1. Moreover, supposing tables are specific to a\ngiven domain, the data might be annotated using specific ontologies or vocabularies to consider the final\nannotations as newly constructed KG.\nLICENSE\nWhen it comes to ensuring reproducibility in research, it is crucial to consider the licensing\naspect. In this regard, we distinguish between different licensing models that govern the availability and\nuse of approaches:\n• OPEN SOURCE category collects approaches published under an open source license, facilitating\ncomparison and reuse (e.g., Apache, MIT);\n• CLOSED SOURCE assemble STI approaches, for which the code is not provided, and as such, it\nis not straightforward to implement or compare it;\n9\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n• NOT SPECIFIED approaches that do not specify any license information.\nVALIDATION\nThe approaches might be validated using different GSs. Some use and validate their\nannotations using well-known GSs, and others provide a new GS together with the release of the new\napproach.\nCODE AVAILABILITY\nFrom a practical perspective, it is interesting to know the availability of\nthe code of a given STI approach.\nUSER INTERFACE/TOOL\nSeveral STI approaches also implement a User Interface (UI) so that\nthe STI process can be consumed and explored by users.\nINPUTS\nOne of the dimensions for the classification of the approaches is the required INPUTS.\nAmong INPUTS, we can distinguish between SOURCES that are to be annotated and additional re-\nsources (KG) that support the annotation process.\n• SOURCES: different systems might need different input sources.\nThree sources are identified:\n(i) TABLES are most frequently considered as input (e.g., CSV files, XML files, Spreadsheets,\nand HTML files), (ii) ANNOTATIONS from already annotated data used as training datasets\n(supervised approaches), and (iii) FEATURES that support the STI approach with additional\ninformation such as out-table context (e.g., page title, table caption, texts).\n• KG: the annotation process is facilitated using KGs. A KG can be stored by a (i) TRIPLE STORE\nthat supports entity searches by lookup APIs (e.g., DBpedia SPARQL Query Editor),and by (ii)\nINDEXING that allows efficient querying of large KGs that would otherwise require significant\ntime and resources.\nOUTPUTS\nThe annotated data might be exported into different formats, such as RDF/XML, N3,\nand CSV.\n4\nSub-tasks\nThe taxonomy dimension SUB-TASKS refers to the completeness of the approach concerning the sub-\ntasks of the STI process. Some approaches implement just a few sub-tasks, while others consider the im-\nplementation of all sub-tasks: i) Data Preparation (Section 4.1), ii) Column Classification (Section 4.2),\niii) Datatype Annotation (Section 4.3), iv) Subject Detection (Section 4.4), v) Entity Linking (Sec-\ntion 4.5), vi) Type Annotation (Section 4.6), vii) Predicate Annotation (Section 4.7), and viii) NIL\nAnnotation (Section 4.8).\n4.1\nData Preparation\nData Preparation is usually the first sub-task in an STI pipeline. This sub-task transforms the raw data\ninto a format suitable for analysis. Data Preparation plays a crucial role in STI as it ensures that the\ndata is appropriately structured and ready for analysis, enabling accurate interpretation and extraction\nof meaningful insights. It involves transforming the values within cells to a standardised format, ensuring\nconsistency, and facilitating subsequent sub-tasks by eliminating variations in representation [56].\nTables consist of numerous cells with literal values that cannot be directly linked. Literal values\nencompass various types, including numeric quantities, dates, and coordinates. Multiple data preparation\nsub-tasks can be employed to clean and format these data types. For instance, numeric quantities within\na column can be converted to a joint base unit.\nFor example, values such as 10kg, 100g, and 34t,\nrepresenting weights, can be interpreted and converted to kilograms. The date is another frequently\nencountered literal type, often appearing in diverse formats such as “4 October 1983”, “4-10-1983”, “Oct\n4, 1983”, “October 4, 1983”, “1983/10/4”, or “1983.10.4.” Normalising numeric and date values can\nbe challenging. However, it can significantly improve subsequent sub-tasks in the pipeline. Moreover,\ntable cells sometimes contain extraneous values, such as text in brackets or special characters, which\ncan confuse entity-linking algorithms and result in poor annotations. Hence, omitting such values can\nenhance the reliability and accuracy of the final results.\n10\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nIndeed, many state-of-the-art (SOTA) approaches in the field recognise the value of the Data Prepa-\nration sub-task and incorporate it before proceeding with other sub-tasks in the pipeline [63, 43, 116,\n115, 134, 175, 138, 55, 56, 180, 85, 176, 26, 35, 74, 120, 147, 174, 1, 12, 14, 38, 34, 29, 91, 143, 161,\n172, 9, 123, 3, 2, 4, 36, 78, 77, 75, 122, 123, 13, 64, 163, 181, 11]. These approaches can also be split\ninto multiple orthogonal categories depending on the type of data preparation technique they perform.\nThe most commonly used techniques are: i) spell checking [1, 3, 2, 4, 12, 29, 91, 172], ii) units of\nmeasurements conversion [175, 138, 56, 35, 14, 38], iii) cell cleaning [63, 134, 115, 138, 55, 85, 26,\n35, 36, 34, 3, 2, 4, 12, 14, 13, 38, 161, 9, 123, 163, 11], iv) acronym expansion [116, 138], v) format\ntranslation [43, 134, 64], and vi) language detection [56, 120].\nThe spell checking technique involves the automatic detection and correction of spelling errors in\nthe text. Some approaches employ this technique to clean table content and improve the accuracy and\nreadability of text [1, 12, 29, 91, 172]. The most used method to fix typos in the cells is by invoking\nautocorrection libraries: JenTab [1, 3, 2, 4] invokes Autocorrect library while Azzi et al. [12] invokes\nGurunudi and Wikipedia API. In the SOTA there are other libraries for the same purpose: TextBlob,\nSpark NLP, Pyspellchecker, Serpapi. LinkingPark [29] handles spelling errors by applying a tailored\nspelling corrector, which performs a one-edit distance check between each cell and a set of candidate\nentities. [91, 172] manage errors such as misspellings, incorrect spacing, and omission of special symbols\nor numbers by crawling through search engines (e.g., Google and Yandex)9.\nAnother Data Preparation sub-step in the SOTA is the units of measurements conversion which\ninvolves identifying the units in the table and applying appropriate mathematical formulas or conversion\nfactors to convert the values into a standardised unit or a desired unit of measurement. This ensures\nuniformity and facilitates meaningful data analysis. Several approaches incorporate a numerical con-\nversion [175, 138, 56, 35, 14, 38]. InfoGather+ [175] assumes that there is a canonical string for every\nunit and scale. The system implements a set of conversion rules defined by the system administrator.\nThis process considers three components: left-hand side (LHS), right-hand side (RHS) and θ. LHS and\nRHS are strings that describe units and scales, while θ represents the conversion factor. Another ap-\nproach developed by Ritze et al. [138] normalises units using a set of manually generated conversion rules\n(around 200). In Ell’s approach [56], a conversion process is employed for values that pertain to weights,\nlengths, volumes, and times. This involves a classic pattern-matching technique, where a unit of mea-\nsurement follows a numeric value. Also, in MantisTable [41, 35, 40, 38], unit normalisation is achieved\nby utilising Regular Expressions (Regex) based on the rules initially described in InfoGather+ [175], and\nthen it extends them to cover a complete set of units of measurement. The same technique is applied in\nKepler-aSI [14].\nThe most critical and applied technique in the Data Preparation sub-task is cell cleaning because\nit removes or modifies unnecessary or unwanted elements in a cell. In [115], tables are cleaned and\ncanonicalised (fixing syntax mistakes) using CyberNeko10 while [138, 35, 38, 1, 3, 2, 4, 14, 161, 123]\ncleans cells by removing HTML artefacts, special characters and additional whitespaces. Subsequent\napproaches [34, 147, 9, 36, 163, 11] use a more straightforward process by removing only parentheses\nand special characters. During the table loading step, the AMALGAM approach [12] incorporates the\ncapability to clean cells by addressing incorrect encoding through the utilisation of the Pandas library.\nSeveral approaches, including JenTab [1], MTab [120], and bbw [143], incorporate the ftfy library11\nwithin the cleaning step. This integration allows for the resolution of broken Unicode characters found\nin various forms, such as transforming “The Mona Lisa doesn˜AƒˆA¢˜A¢ˆa€ˇsˆA¬˜A¢ˆa€ˇzˆA¢t have eyebrows”,\nwhich converted to “The Mona Lisa doesn’t have eyebrows”. In the successive MTab4WikiData im-\nplementation [122], the preprocessing is simpler because of the effectiveness of the fuzzy entity search.\nKacprzak et al. [85] removes non-numerical chars from numeric columns. In DAGOBAH [26] encod-\ning homogenisation and special characters deletion (parentheses, square bracket and non-alphanumeric\ncharacters) are applied to optimise the lookups. The implementation was improved in the subsequent\nversion of DAGOBAH [78, 77, 75]. Some approaches [134, 55, 180, 35, 38, 1, 3, 2, 4, 13] apply stop-word\nremoval in this sub-task.\nTables often include acronyms and abbreviations, shortened terms formed by combining multiple\nwords’ initial letters or parts.\nTo address such cases, several approaches employ acronym expan-\nsion [116, 138, 35, 14]. For instance, Mulwad et al.’s approach [116], recognises and expands acronyms\n9pypi.org/project/autocorrect,\ngithub.com/guruyuga/gurunudi,\nwikipedia.readthedocs.io/en/latest/code.html,\ntextblob.readthedocs.io/en/dev,\nnlp.johnsnowlabs.com,\ngithub.com/barrust/pyspellchecker,\nserpapi.com/spell-check,\nyandex.com\n10nekohtml.sourceforge.net\n11github.com/rspeer/python-ftfy\n11\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nand stylised literal values like phone numbers. Another approach [138] utilises transformation rules to\nresolve abbreviations, such as converting “co.” to “company”. MantisTable [35] leverages the Oxford\nEnglish Dictionary12 to decipher acronyms and abbreviations. Similarly, Kepler-Asi employs heuristic\nmethods to resolve acronyms and abbreviations [14].\nAnother possible data preparation technique is format translation which consists of translating\ndata into a different structure. For example, Cruz et al. [43] and Quercini et al. [134] translate geographic\nand temporal information into spatial and time series. In Tab2KG [64], the data is transformed in RDF\nMapping Language (RML) format.\nEventually, language detection implies detecting the language of the mentions to best address the\nsuccessive Entity Linking. Ell [56] applies some Regex to detect languages, while [120] uses pre-trained\nfastText models13 to predict the language of the whole table.\nMany approaches do not specify how the data is prepared to be processed [70, 71, 157, 105, 117, 151,\n118, 162, 94, 131, 164, 22, 49, 57, 183, 142, 152, 19, 135, 58, 119, 129, 154, 153, 108, 27, 28, 98, 114, 124,\n155, 158, 29, 59, 89, 65, 103, 178, 69, 146, 170, 30, 50, 107, 149].\n4.2\nColumn Classification\nClassifying table columns entails categorising each as a Named Entity (NE) or a Literal (LIT). NE-\ncolumns contain values representing entities such as names, locations, or organisations. In contrast, LIT-\ncolumns contain values representing literal data types such as numbers, dates, or geo-coordinates. Many\nexisting approaches utilise prior datatype classification to determine the type of columns. By classifying\ncolumns, subsequent semantic analysis and data manipulation become more feasible. These approaches\nassign specific types (e.g., number, date, geo-coordinate) by employing: i) Regex matching [180, 55, 35,\n38, 14, 13, 34, 9, 36, 120, 143, 11], ii) statistical analysis [116, 85, 65], or iii) Machine Learning (ML)\ntechniques [178, 50]. Additionally, some approaches prioritise entity linking; in this scenario, unlinked\ncolumns are then classified as LIT-columns.\nSome approaches explicitly consider Column Classification as a distinct sub-task [116, 55, 180, 85,\n35, 14, 13, 38, 34, 65, 1, 3, 2, 4, 9, 120, 30, 26, 78, 77, 75, 36, 178, 50, 11], while many others implicitly\nperform Column Classification by identifying cell or column datatypes [70, 71, 22, 43, 134, 138, 135,\n154, 129, 143, 91, 7, 64, 50, 158, 123, 30]. TableMiner+ [180] utilises Regex patterns to identify empty,\ndate, number, and long text columns, categorising them as LIT-columns, while the rest are considered\nNE-columns. Efthymiou et al. [55] applies a column sampling to classify literal columns. Later, [35,\n38, 14, 13, 34, 9, 36, 120, 11] adopt a similar technique, employing additional Regex patterns and using\nmajority voting to determine column types. In addition, MTab [120] combines the use of Regex with\nSpaCy14 pre-trained model to perform column classification. Kim et al. [91] consider text, number, and\ndate as possible types while bbw [143] uses Regex to predict number, time, name and string datatypes.\nRegarding the statistical analysis, Mulwad et al. [116] developed a domain-independent and ex-\ntensible framework where it is possible to implement components to detect literal values; when all cells\nin a column are literal, the column is considered as “No-annotation”. Kacprzak et al. [85] considers\nas numeric the columns with at least 50% of numerical values, else the columns are classified as NE.\nSimilarly, also [65] classifies columns as either “character” or “numeric”.\nIn the ML techniques, Zhang et al. [178] proposes a column header classification model which was\ntrained on the T2D dataset. The TURL [50] approach uses an additional type embedding vector to\ndifferentiate NE columns.\nOther approaches perform Column Classification, but there are not enough details to categorise them.\nIn DAGOBAH [26] the process aims to identify a first low-level type for each column among five given\ntypes (Object, Number, Date, Unit, and Unknown). In the successor [78] numeric values are considered\nboth with or without the corresponding unit of measure. No further changes were applied in consecutive\napproaches [77, 75] except adding customised modules for organisation, location and currency detection\nin 2022 [75]. Similarly [158], considers string, date and numeric types. In [155] is unclear whether the\ntable columns are already classified as NE and LIT. JenTab [1, 3, 2, 4] considers object, date, string and\nnumbers. LinkingPark [30] leverages standard conversion functions for int, float and date-time datatype;\notherwise, cells are considered strings. There is no information about the aggregation function, but LIT\nand NE columns are considered differently.\n12public.oed.com/how-to-use-the-oed/abbreviations\n13fasttext.cc/docs/en/crawl-vectors.html\n14spacy.io\n12\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n4.3\nDatatype Annotation\nIn many approaches, the Datatype Annotation of LIT-columns is tightly linked to the Column Classifi-\ncation. Similarly to the Column Classification sub-task, approaches exploit three methods: i) statistical\nanalysis [70, 71, 30, 119, 135, 129, 162, 85, 29, 122, 25], ii) Regex matching [138, 134, 43, 116, 135,\n154, 129, 180, 35, 38, 34, 9, 36, 143, 14, 13, 120, 123, 64, 11], or iii) ML techniques [155, 175, 158,\n63, 50, 178, 55, 27, 120, 123, 64]. In this sub-task it is possible to add an additional category related to\napproaches that use iv) other methods [94, 152, 154, 153].\nSeveral approaches employ statistical analysis by applying a set of rules to classify LIT and NE\ncolumns [70, 71]. Such rules concern how cell content is represented, i.e., the amount of text and num-\nbers/units. Inspired by Taheriyan et al. [154], [129] adopts statistical hypotheses as a metric for column\nannotation. Hierarchical clustering is employed by Neumaier et al. [119] to construct a background KG\nusing DBpedia. The nearest neighbours classification is applied to predict the most probable data type\nfor a given set of numerical values, also considering distribution similarity. NUMBER [85] is inspired\nfrom previous works [119, 129]. The evaluation process involves two main aspects. Firstly, the sim-\nilarity of value distributions is assessed by comparing them to the properties in a target KG using a\nKS test15. Secondly, the relative difference is computed between numerical values in a column and the\nnumerical values of properties linked to the entities. MTab4Wikidata [122], column types are determined\nafter the cells have already been linked to entities. LinkingPark [29, 30] uses precomputed statistics for\nnumeric datatypes, such as range, mean, and standard deviation and considers datatypes that match\nthe corresponding ranges as potential candidates. p-type [25] proposes a model built upon Probabilistic\nFinite-State Machines (PFSMs). In contrast to the standard use of Regex, PFSMs have the advantage\nof generating weighted posterior predictions even when a column of data is consistent with more than\none type of model.\nRegex is used to check if the content of the cells can be classified, for instance, as pH, temperature,\ntime, date, number, geo coordinates, iso8601 date, street address, hex colour, URL, image file, credit\ncard, email address, IP address, ISBN (International Standard Book Number), boolean, id, currency, and\nIATA (International Air Transport Association) codes. If the number of occurrences of the most frequent\nRegexTypes detected exceeds a given threshold, the column will be annotated as LIT-column, and the\nmost frequent RegexType will be assigned to the column under analysis. Then, to select the datatype to\nannotate the column, some approaches imply a mapping between RegexType and Datatype. [134, 116,\n43, 138, 135, 154, 180, 35, 143, 38, 34, 14, 178, 9, 36, 13, 11] utilise some or the entire list of the Regex\ndefined above to identify the datatype of the column under analysis.\nThe last method for Datatype Annotation regards ML techniques. Meimei [155] utilises embeddings\nby modelling a table with a Markov random field and employing multi-label classifiers to find the correct\nannotation, similar to InfoGather+ [175] which focuses only on numerical values. Another approach [63],\nmodels different latent structures within the data and employs a Conditional Random Field (CRF) to\nperform semantic annotation in different domains (e.g., weather, flight status, and geocoding).\nThe\napproach ColNet [27] utilises a Convolutional Neural Network (CNN) trained on positive and negative\nsamples to differentiate between different column types.\nOther approaches combine Regex with ML techniques; MTab[120] classifies columns as NE or LIT\nusing Duckling Regex16 and SpaCy (a pre-trained classificator) with majority voting. Numeric columns\nare classified using a neural numerical embedding model (EmbNum [121]) through representation vectors\nfor numerical attributes without prior assumptions on data distribution. In contrast [123], used only\nSpaCy to identify LIT-columns. Tab2KG [64] uses the Dateparser library17 for classification in numeric,\nspatial, boolean or text. Later the columns are further classified into more specific categories using Regex\ncombined with the method described in [7], which implies classifying four kinds of numbers: nominal,\nordinal, interval, and ratio. Then fuzzy c-means is used for classification.\nSeveral other approaches in the field of STI have been proposed, each with its unique methodology.\nThese approaches, including works such as [157, 105, 117, 151, 118, 131, 164, 49, 57, 115, 183, 142, 19, 58,\n56, 108, 176, 28, 74, 114, 98, 124, 147, 174, 12, 59, 89, 103, 161, 172, 69, 170, 146, 163, 181, 107, 149], do not\ninvolve semantic classification or Datatype Annotation sub-tasks for the columns. In [118], the concept\nof literal annotation was discussed as a potential area for future works, which was later implemented\nin [116]. The approach MAGIC, discussed in [146], uses entity embeddings with neighbour nodes up to\na depth of 2 without explicitly distinguishing between entities and literals. However, it is plausible that\nthis approach can also be extended to include Datatype Annotation. Specific approaches focus on the\n15The KS test measures the statistical difference between the two distributions, providing insights into their similarity.\n16github.com/facebook/duckling\n17github.com/sisyphsu/dateparser\n13\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nmanual annotation of tables, providing a UI that allows users to select the most appropriate datatype\nmanually [94, 152, 154, 153].\nSeveral approaches involve Datatype Annotation; however, they lack sufficient details about how the\nannotation of the columns with specific datatypes occurs [22, 55, 158, 1, 3, 2, 4, 91, 65, 26, 78, 77, 75, 50].\n4.4\nSubject Detection\nAs previously illustrated (Section 3), the S-column is the column among the NE-columns that all the\nothers refer to. Some approaches define it as a “key” column that includes entity-based mentions that\ncould potentially be consulted in a KG, containing a large number of unique values.\nGenerally, approaches might employ one of the following techniques for Subject Detection: i) heuris-\ntic approaches, ii) statistical analysis, or iii) ML techniques.\nRegarding heuristic approaches, one common method is based on the column position within\nthe table. For example, some approaches designate the leftmost column as the S-column [55, 1, 91, 3,\n2, 4, 30, 50]. [151] instead, involves identifying columns with specific labels as the S-column, such as\ncolumns labelled as “title” or “label”. Another method for Subject Detection is to consider column\ndetection after linking entities. For instance, Zhang et al. [176, 178] use the column with the highest\nnumber of linked entities as an indicator for Subject Detection. Similarly, the approach proposed by\nHeist et al. [69], considers subject-predicate-object relations between the table’s columns and identifies\nthe S-column as the one with the highest number of entities in the subject position.\nThe use of statistical analysis to identify the S-column is found in TableMiner+ [180]; it uses a set\nof rules based on the number of words, the capitalisation, and the mentions of months or days in a week.\nIn MantisTable [35, 38, 34, 9, 36, 11] the subject is selected among the NE-columns using a calculated\nas a set of indicators such as the average number of words in each cell, the fraction of empty cells in the\ncolumn, the fraction of cells with unique content and the distance from the first NE-column. The same\nindicators are also used in [14]. DAGOBAH2019 [26], TAKCO [98] and MTab2021 [123] consider the\nfraction of cells with unique content and the column position in the table.\nSome works in the literature [162, 58] use ML techniques. TAIPAN [58] selects SVM and Decision\nTree as the best classifiers for this sub-task and uses as features the ratio of cells with disambiguated\nentities in a column and the number of relations between the columns. It is worth noting that [162]\nalso employs an SVM using features dependent on the name and type of the column and the values in\ndifferent column cells.\nTab2KG [64] uses a graph-based approach for subject detection, but the paper lacks details about\nthe implementation.\nEventually, there are many approaches that do not perform S-column detection [70, 71, 157, 105, 117,\n118, 63, 94, 131, 164, 22, 43, 49, 57, 116, 115, 134, 175, 183, 142, 152, 19, 135, 138, 119, 129, 154, 153,\n56, 85, 108, 27, 28, 74, 114, 120, 124, 147, 155, 158, 174, 12, 29, 59, 65, 78, 89, 103, 122, 143, 161, 172,\n13, 77, 146, 163, 170, 181, 107, 75, 149].\n4.5\nEntity Linking\nEntity Linking, also known as named entity linking or entity resolution, is an NLP task that involves\nlinking named entities mentioned in the text to their corresponding entities in a KG. The goal is to\nidentify and disambiguate the entities mentioned in the text and connect them to unique identifiers.\nIn Entity Linking, a named entity refers to a specific named person, organisation, location, event, or\nother well-defined entity. For example, in the sentence “Barack Obama was born in Hawaii”, the named\nentity “Barack Obama” can be linked to the corresponding entry in a KG, such as dbr:Barack Obama\nin DBpedia o Barack Obama (Q76) in Wikidata. For this sub-task, approaches can be grouped into\nthree step within entity liking: i) mention detection [19], ii) candidate generation, and iii) entity\ndisambiguation [105, 151, 117, 118, 164, 116, 183, 19, 138, 55, 56, 180, 26, 27, 35, 98, 114, 120, 124, 147,\n158, 1, 12, 29, 38, 34, 59, 78, 91, 122, 143, 161, 178, 3, 9, 13, 77, 123, 146, 170, 4, 30, 36, 75, 50, 107, 11].\nMention detection refers to the identification and extraction of mentions from tabular data. It\ninvolves recognising specific pieces of information within a table representing entities. Detecting mentions\nin a table can involve various techniques, such as NLP methods, Named Entity Recognition (NER)\nmethods, or pattern-matching algorithms. This sub-task analyses the table’s content, column headers,\nand other contextual information to accurately identify and classify the mentions.\nThe TabEL [19] approach identifies potential mentions within a given cell that can be associated\nwith entities in a KG. TabEL identifies the longest phrases within the cell’s text content with a non-zero\n14\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nprobability of being linked to an entity e according to the probability distribution P(e|s) where s is a\nphrase. If the length of s is shorter than the length of the cell’s text content, TabEL continues searching\nfor the longest phrase. Unfortunately, there are no additional details on this method in the paper.\nIn the context of STI, a crucial role is played by the candidate generation sub-task, also known as\nlookup, which refers to the process of identifying potential entities based on the given input or query.\nWhen a query is posed, the system must generate a set of candidates for each cell that could potentially\nsatisfy the query. The candidate generation process may utilise various techniques, such as semantic\nparsing, entity recognition, or information retrieval. It could also leverage ML models trained on large\ndata corpora to generate likely candidates based on contextual patterns. The Lookup sub-task can be\ndivided into four methods: a) custom index [105, 151, 55, 56, 26, 27, 98, 114, 124, 158, 29, 34, 78, 122,\n77, 9, 123, 170, 36, 30, 75, 11], b) external lookup services [117, 118, 164, 116, 19, 138, 180, 176, 26,\n35, 38, 120, 147, 158, 1, 3, 4, 12, 59, 29, 91, 143, 161, 178, 13, 170, 146, 50], c) hybrid (both custom\nindex and external lookup services) [26, 158, 29, 170], and d) other [143, 164].\nCustom index refers to building a specialised index for specific requirements or use cases. When\nbuilding a custom index, there is flexibility in defining mappings, analysers, and other configurations\nbased on specific needs. One of the most adopted solutions is Elasticsearch18, a robust and scalable\nsearch and analytics engine. It uses a document-oriented approach, where data is organised and stored\nas JSON documents. Several approaches rely on Elasticsearch for the lookup sub-task [26, 98, 78, 77, 75,\n34, 9, 36, 114, 124, 158, 29, 30, 11]. The simplest index can incorporate entity labels (rdfs:label) or\naliases (skos:altLabel). However, some approaches also add abbreviations (dbo:abbreviation), de-\nscriptions (rdfs:comment) [77, 75, 158], or, indexes for specific entity types, name (foaf:name), surname\n(foaf:surname), and given name (foaf:givenName) [114]. The MantisTable team builds a separate sys-\ntem named LamAPI19 [10, 11] which is used across multiple versions of this system. LamAPI20 tool\nretrieves entities with the highest similarity between the mention in the cell and the entity’s label by\ncombining different search strategies, such as full-text search based on tokens, n-grams and fuzzy search.\nOther approaches build custom indexes using different solutions; Limaye et al. [105] presents a method\nthat utilises a catalogue which comprises types, entities and relations. Entities in the catalogue are\nassociated with lemmas, which are canonical strings extracted from Wikipedia, or synset names from\nWordNet21. Syed [151] develops a hybrid KG of structured and unstructured information extracted from\nWikipedia augmented by RDF data from DBpedia and other Linked Data. The system is called Wiki-\ntology and uses an Information Retrieval (IR) index (Lucene) to represent Wikipedia articles. The same\ntechnique is used by Mulwad et al.[117, 118, 116]. Ell et al. [56] creates an index for each type of resource\n(entity, property, type) for each language. These indexes contain the names of the resources, according\nto DBpedia. For properties and classes, the names are obtained from the rdfs:label property in DB-\npedia. Efthymiou et al. [55] utilises a lookup-based method to establish connections. It leverages the\nlimited entity context available in Web tables to identify correspondences with the KG. The approach\nbuilds its custom search index over Wikidata, called FactBase, consisting of entities with corresponding\nIDs and textual descriptions. Another system named ColNet [27] involves two steps in its candidate\ngeneration. Firstly, a lookup step is performed to retrieve entities from the KG by matching cells based\non entity labels and anchors (e.g., Wikipedia link) using a lexical index composed of terminology and\nassertions from the KG. MTab4Wikidata [122], another version of MTab [120], focuses on annotating\ncells to Wikidata entities. It starts by downloading and extracting a Wikidata dump to build an index\nusing hash tables. The lookup process is then performed using a fuzzy search. The result is a ranking\nlist of entities based on edit distance scores. In the updated version of MTab 2021 [123], a WikiGraph\nindex is constructed, combining Wikidata, Wikipedia, and DBpedia. The lookup uses Keyword Search,\nFuzzy Search, and Aggregation Search. GBMTab [170] tackles candidate entity generation by differen-\ntiating the entities extraction from Wikidata and DBpedia. Only for DBpedia, the approach builds an\nindex using hash tables. Then, it uses the Levenshtein distance to calculate a string similarity between\nmentions and entities.\nThe second method employed for candidate generation uses external lookup services. This pro-\ncess refers to using a separate service or system to perform lookup or queries for retrieving specific\ninformation or data. The external lookup service usually utilises entity recognition, entity disambigua-\ntion, or semantic matching techniques. It may consider factors like textual similarity, context, or other\nrelevant information. In the STI, many services can be used to extract a set of possible entities given\n18www.elastic.co\n19github.com/unimib-datAI/lamAPI\n20lamapi.datai.disco.unimib.it/\n21wordnet.princeton.edu\n15\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\na string as input. The choice of the service depends on the specific requirements and context, such\nas the KG used to annotate the entities. Most approaches annotate table cells to DBpedia entities by\nusing related services, such as DBpedia API [120, 26], DBpedia Lookup Service [120, 147, 161] and\nDBpedia Spotlight [147, 146]. The same occurs for Wikidata, for which the following services are em-\nployed: Wikidata API [26, 158, 12, 143, 146], Wikidata Lookup Service [120, 1, 59, 161, 3, 2, 4, 50] and\nWikidata CirrusSearch Engine [26]. Other services used to execute the lookup sub-task are Wikipedia\nAPI [26, 120, 143, 178], MediaWiki API [29, 170] and Wikibooks [143]. Instead of explicitly using lookup\nservices, some approaches perform SPARQL queries, a query language used to retrieve and manipulate\ndata stored in RDF format. This method is the default way to obtain information from triple stores.\nFor the approaches that do not provide any specific information on the lookup service, it is assumed\nthat SPARQL is employed. For instance, such queries are used to retrieve entities from YAGO [19],\nDBpedia [138, 180, 176, 35, 38, 147, 3, 2, 13, 4] and Wikidata [91, 143, 3, 2, 13, 4]. Furthermore, other\nsources are used, for instance, SearX [143] and Probase [164] where pattern matching is used to extract\ntriples.\nEntity disambiguation refers to the process of resolving ambiguous mentions to entities. When\ntables contain references to entities or mentions, such as names of people, locations, or organisations,\nthere can be ambiguity if the same name refers to multiple entities. Entity disambiguation in STI aims\nto identify and disambiguate these entity mentions, ensuring that each mention is correctly linked to the\nappropriate entity. This sub-task can be performed by applying multiple techniques: a) embedding [55,\n26, 59, 176, 178, 146, 107], b) similarity [105, 180, 124, 98, 147, 158, 35, 38, 34, 78, 161, 143, 9, 3, 77,\n30, 36, 11], c) contextual information [151, 138, 55, 27, 120, 158, 122, 123, 114, 12, 29, 34, 1, 78, 9,\n13, 77, 75, 30, 36, 107, 11], d) ML techniques [117, 158, 178, 11], e) language models [103, 75, 50,\n127, 100, 125, 159], f) probabilistic models [118, 116, 19, 98, 170], and g) other [164, 115, 56].\nIn graphs and natural language, embedding refers to representing nodes in a graph, or words in a\ntext, as dense vectors in a continuous vector space. These embeddings capture semantic and structural\nrelationships between nodes or words, allowing ML models to perform tasks such as node classification,\nlink prediction, document similarity, sentiment analysis, and more [130]. Some approaches use embedding\ntechniques to create vector representations of entities [55, 26, 59, 178, 146]. Every approach tries to\ncapture context information about entities in the KG and to incorporate that information in the vector\nrepresentation. [55, 176, 59] employ semantic embeddings obtained through Word2Vec [113] on KGs,\nwhile Zhang et al. [176, 178] use GloVe [128], Wikipedia2Vec [169] and RDF2Vec [136] to obtain a\nrepresentation of entities. DAGOBAH [26] uses pre-trained Wikidata embedding [67], while MAGIC [146]\nuses INK technique [148], which transforms the local neighbourhood of a node in the KG into a structured\nformat. Radar Station [107] uses the PyTorch-BigGraph [101] framework for training embeddings.\nThe entity disambiguation sub-task could involve the computation of some similarities among\ntextual data.\nThis type of score is usually adopted by lookup services to retrieve a ranked list of\ncandidates. Often, the disambiguation step involves the selection of the winning candidate by considering\nthe string similarity between the entity label and mention. Some approaches use similarity, such as,\nLevenshtein distance [35, 147, 38, 34, 124, 158, 78, 143, 9, 77, 30, 36, 11], Jaccard similarity [105, 98, 9,\n36, 11], Cosine similarity [105, 161] and similarity based on Regex [78]. Limaye et al. [105] in addition\nto Jaccard applies also the TF-IDF22. TableMiner+ [180] measures the similarity between the Bag-\nOf-Words (BOW) representation of the entity and the BOW representations of different types of cell\ncontexts, such as row content and column content.\nContextual information during the CEA task considers the surrounding context of a table cell,\nsuch as neighbouring cells, column headers, or header row. Contextual information provides additional\nclues or hints about the meaning and intent of the mention. By analysing the context, a system can\nbetter understand the semantics of the cell and make more accurate annotations. Contextual information\nat column and row level is usually provided by CTA and CPA tasks, but some approaches take column\ntypes and properties into account to disambiguate entities even if those tasks are not explicitly treated.\nColumn types are used to disambiguate entities by assuming that entities in a column share the same\ntype. Most approaches rely on this assumption to perform this step [151, 138, 55, 27, 120, 114, 158, 12,\n13]. [151, 27, 13] limit the number of candidate entities by executing a new lookup query that includes\npredicted types. [138, 55, 120, 114, 158, 12] refine the candidates set by filtering out entities that do\nnot match the predicted type at the column level. Similarly, the other assumption considered by some\napproaches is that contextual information from CPA at the row level enables understanding the data\nbetter within its broader context [78, 122, 123, 75]. [78, 75] consider the semantic relations between\n22The weight assigned to a term in a document vector is the product of its term frequency (TF) and inverse document\nfrequency (IDF).\n16\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\ncolumns by boosting the scores for each candidate entity when the relation is found. [122, 123] compute\ncontext similarity between candidate triples and table row values by ranking entities based on this score.\nEventually, it selects the candidate with the highest context similarity as the final annotation. Most\napproaches also implement the disambiguation sub-step by adopting a hybrid solution (considering the\ninformation provided by CPA and CTA tasks) [29, 30, 9, 34, 36, 1, 77, 11]. Radar Station [107] focuses\non improving entity linking in the context of STI systems. It addresses disambiguation challenges by\nusing graph embeddings to identify similarities between entities, types, and relationships within tables.\nThe method involves constructing a KD tree of context entities for each column and using it to select the\nK nearest context entities during prediction, ultimately enhancing the ranking of candidates provided\nby [122, 78, 143].\nOther methods that can be employed are ML techniques. These techniques typically involve train-\ning a ML model on a labelled dataset where cells are annotated with their corresponding entities. The\nmodel learns patterns and relationships between the cells content and their associated entities. To predict\nthe most appropriate entity, ML techniques consider various cell features, such as the textual content,\ncontext, neighbouring cells, and other relevant information. Several ML techniques can be employed\nto perform the disambiguation task, such as Support Vector Machine (SVM) [117], NN [158, 11] and\nRandom Forest [178]. Mulwad et al. [117] create a vector of features for each entity, and then an SVM\nis used to rank such vectors. Then a second SVM decides whether to link or not the entity mentioned\nin the cell. Thawani et al. [158] build a NN that learns adaptive weights and relationships from labelled\ndata. They use a 2-layer architecture with ReLU activation to obtain scores for each candidate. Zhang\net al. [178] extract two sets of features: lexical similarity (e.g., Levenshtein, Jaccard) and semantic simi-\nlarity (e.g., Wikipedia search rank). Eventually, a Random Forest is trained to determine if it is possible\nto link an entity to a mention.\nLanguage models might be used for cell entity annotation by leveraging advanced models, such\nas BERT. Language models are trained on vast amounts of text data and have the ability to interpret\nnatural language. They can capture complex linguistic patterns, semantic relationships, and contextual\ncues.\nThese models can be utilised to perform various NLP tasks, including entity recognition and\nlinking.\nIn the context of CEA, an Large Language Model (LLM) can be fine-tuned or adapted to\nthis specific task. This involves training the model on a labelled dataset where cells are annotated with\ncorresponding entities. Once the LLM is trained, it can be applied to unlabelled cells in a table to predict\nthe most likely entity annotations. The model considers the cell’s textual content, surrounding context,\nand potentially other relevant features to make these predictions. Recently LLMs have been employed\nto find semantic correlations between different cells at column and row level. The main advantage of\nusing LLMs is having a contextualised representations for each cell, considering the mention and table\nmetadata. The advent of Large Language Models (LLMs) has led to a new category of approaches for\ntable interpretation. Based on the architecture structure of LLMs, these approaches can be categorised\ninto three groups: i) encoder-decoder LLMs, ii) encoder-only LLMs, and iii) decoder-only LLMs [127].\nIndeed, shortly after the first edition of SemTab, some works [103, 50, 149] applied encoder-only LLMs\nto table interpretation. Although they did not participate in or compare with the SemTab challenge,\nthey created a different experimental setting. During the SemTab2022 instead, a BERT-based [51] model\nwas combined with a more traditional approach [75]. More recently, after the release of GPT-3.5 [125]\nand open-source decoder-only LLMs such as LLAMA [159] and LLAMA 2 [160], some works have begun\napplying encoder-based LLMs to table interpretation [102, 179]. In SemTab2023, a new decoder-only\nmodel was presented that uses BERT [48].\nStarting from encoder-based approaches, Ditto [103] utilises Transformer-based language models to\nperform a slightly different task; in fact, the goal is entity-matching between various tables. TURL [50]\nleverages a pre-trained TinyBERT [80] model to initialise a structure-aware Transformer encoder. Do-\nduo [149] performs CTA using a pre-trained language model, precisely fine-tuning BERT model on\nserialised tabular data. DAGOBAH SL 2022 [75] employs an ELECTRA-based cross-encoder, a variant\nof the BERT model. The Cross Encoder takes a concatenated input, including left-side table headers, the\ntarget table header, right-side table headers, and the entity description. TorchicTab [48] is composed of\ntwo sub-systems: TorchicTab-Heuristic and TorchicTab-Classification. The classification model utilises\nDoduo [149].\nRegarding decoder-based approaches, TableGPT [102] performs several tasks, including entity linking\nusing GPT. TableLlama [179], performs CEA, along with several other tasks, creating a multi-task\ndataset for tabular data, in which the entity linking sub-dataset derives from the TURL [50] dataset, and\nusing it to fine-tune LLama2 [160]. The advent of LLMs has also led to the publication of experiments\ncomparing different approaches based on LLM and ML [16].\n17\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nProbabilistic models are frameworks for representing and reasoning under uncertainty using prob-\nability theory.\nThese models vary in their representation of dependencies and use diverse graphical\nstructures. Several Probabilistic Graphical Model (PGM) can be also used to resolve the disambiguation\ntask, such as Markov models [118, 116, 19, 170] or Loopy Belief Propagation (LBP) [98]. Markov mod-\nels focus on sequential dependencies, while LBP employ message passing between nodes in the PGMs.\nBhagavatula et al. [19] adopt a representation for tables as graphical models. Within this approach,\nevery mention in the table is linked to a discrete random variable that represents the possible candidate\nentities associated with that mention using Independent Component Analysis. Mulwad et al. [118, 116]\nresolve ambiguities in table cell values by looking at the evidence from other values in the same row.\nThis is achieved by creating edges between each pair of cell values within a specific row. Kruit et al. [98]\nintroduce a PGM incorporating label similarities as priors. The model subsequently improves likelihood\nscoring to enhance the consistency of entity assignments across rows through Loopy Belief Propagation\n(LBP). Eventually, Yang et al. [170] create a disambiguation graph that utilises mentions and their\ncorresponding candidates from the same row or column in a table. The approach scores the semantic\nconnections between nodes using three features in the PGM: Prior, Context, and Abstract.\nOther approaches cannot be categorised in one of the previous groups.\nFor instance, Munoz et\nal. [115] proposes an approach to extract RDF triples from Wikitables by linking each cell to DBpedia\nentities. The process involves following internal links within Wikipedia tables, as they can be directly\nmapped to DBpedia. Ell et al. [56] create some hypotheses for entities extracted in the previous phase\n(Candidate Generation). These hypotheses include the entity type, the URI in DBpedia, and a confidence\nvalue. The confidence value is determined by normalising the frequency value of the entity by dividing it\nby the sum of frequency values for all the candidates. Wang [164] describes the process of understanding\na table using the Probase knowledge API. Eventually Kim et al. [91] remove candidates considering the\ncontent unrelated to the annotation.\nSome other approaches such as [183, 74, 174, 89, 163, 181] do not perform Entity Linking tasks\nspecifically.\n4.6\nType Annotation\nThe Type Annotation sub-task involves assigning a specific type from a reference KG to each NE-column.\nVarious approaches have been developed to address this sub-task, focusing on leveraging Entity Linking\ntechniques, partially or comprehensively, to identify the most frequent column type. This is particularly\ncrucial in unsupervised classification scenarios. Additionally, some approaches consider the information\nprovided by column headers to determine the most suitable type.\nThe most used methods to annotate NE-columns are: i) majority voting [70, 71, 117, 118, 162,\n134, 183, 138, 58, 56, 180, 12, 14, 29, 59, 91, 122, 143, 161, 123, 30, 151, 116, 35, 147, 158, 38, 34, 9, 36,\n1, 3, 2, 4, 98, 181, 11], ii) Term Frequency-Inverse Document Frequency (TF-IDF) [105, 131,\n135, 154, 124, 26], iii) statistical methods [120, 77, 78, 69, 49, 75], iv) machine learning [27, 65, 146,\n163, 170, 50, 64, 28, 74, 174, 149], or v) other [94, 175, 152, 22, 155, 13, 164, 114, 89, 178].\nMost research studies on column-type prediction utilise a common strategy called majority voting.\nThis method involves determining the most frequently occurring type in a column and deciding based on\nthe majority. This pure decision-making method is applied by [70, 71, 117, 151, 58, 118, 162, 134, 183,\n138, 56, 180, 12, 14, 59, 29, 91, 122, 143, 161, 123, 170, 30]. Some approaches go beyond simple majority\nvoting and incorporate additional mechanisms to address specific situations. For instance, [116, 35, 147,\n158, 38, 34, 9, 36, 98, 181] set a threshold to prevent annotating a column when there is insufficient\nconfidence about the type. The approach in [134] applies a deduplication process to remove duplicate\ntypes within a column. After deduplication, the type frequencies in the column are summed using a\nlogarithmic function which measures the overall frequency and importance of the types present in the\ncolumn. In Kruit et al. [98], majority voting across types is also used to select candidate entities for\nindividual rows using Loopy Belief Propagation (LBP). This approach highlights how CTA and EL are\ninterconnected. Also, for LinkingPark [29, 30], the primary method used is majority voting to obtain\nthe most common (i.e., most frequent) type.\nIn case multiple candidates have the same frequency,\nthe type selected for annotation is the most specific in the KG. For bbw [143], majority voting is the\nprimary selection algorithm. However, when multiple types have equal frequency, it selects the first\ncommon ancestor type in the KG.\nJenTab [1, 3, 2, 4] uses various techniques, including the Least\nCommon Subsumer (LCS), direct parents (i.e., majority voting), and popularity. Following the ontology\nhierarchy, the LCS represents the most specific type, obtained by excluding types occurring in less than\n50% of the cells. Zhou [181] selects the annotation based on level 2 and level 3 DBpedia classes.\n18\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nAnother method for Type Annotation is based on TF-IDF, as mentioned by [135, 154, 124].\nDAGOBAH [26] highlights the importance of setting a confidence threshold when using TF-IDF to\navoid wrong annotations. There are also some variations; for example, Limaye [105] introduces the Least\nCommon Ancestor (LCA) as a baseline approach against majority voting and the collective approach.\nThe collective approach considers EL output, features like Inverse Document Frequency (IDF), and the\ndistance calculated by counting edges between the considered entity and the potential column types\nobtained in LCA method. In the end, a PGM is used for choosing final annotations (CEA, CTA, and\nCPA). Also [131], uses a PGM combined with the TF-IDF approach.\nThere are approaches that use statistical methods other than majority voting; for example, Nguyen [120]\nintroduces the concept of “type potential” which is computed as the cumulative probability entities in\na column corresponding to a specific type within the KG. “Type potential” considers statistics from\nnumerical columns, types from the candidate entities in the whole table, SpaCy type predictions and\nheader values to assess the likelihood of different types for the column. Similarly [78, 77, 75] consider\nvarious factors such as frequency, accumulated level (ontology hierarchy), and accumulated rank of Wiki-\ndata23, for all candidate types of a target column. The final column type is determined using majority\nvoting as the deciding factor. Heist et al. [69] compute type frequency and relation frequency statistics\nin the DBpedia KG to identify best-suited types using co-occurrence. Deng [49] instead employs an im-\nplementation similar to the idea of majority voting using an overlap similarity between top-k candidate\ntypes for a given column. In the map-reduce like implementation, the overlap corresponds to the count\nof entities having a given type.\nAnother method implies using machine learning. The approach presented in [28] focuses on an-\nnotating columns that consist of phrases. For instance, the type dbo:Company can annotate a column\ncontaining “Google, Amazon and Apple Inc.”. To achieve this, they propose a method called Hybrid\nNeural Network (HNN) that captures the contextual semantics of a column. The HNN model uses a bidi-\nrectional Recurrent Neural Network (RNN) and an attention layer (Att-BiRNN) to embed the phrases\nwithin each cell, allowing for contextual understanding. A similar Convolutional Neural Network (CNN)\nconfiguration combined with majority voting is also used in [27]. Sherlock [74] is a multi-input deep\nNN for detecting types. It is trained on 686,765 data columns retrieved from the VizNet24 corpus by\nmatching 78 semantic types from DBpedia to column headers. Each matched column is characterised\nby 1,588 features describing the statistical properties, character distributions, word embeddings, and\nparagraph vectors of column values. Inspired by Sherlock, Sato [174] incorporates table context into\nsemantic type detection. It employs a hybrid model that combines “signals” from the global context\n(values from the entire table) and the local context (predicted types of neighbouring columns). Guo et\nal. [65] introduce a HNN model for single-column type annotation, which combines Deep Learning (DL)\nwith a PGM [96]. With a pre-annotated dataset, a co-occurence matrix is built, considering types for\neach column pair. This co-occurrence measure is used to annotate similar column pairs in other tables.\nTCN [163] treats a collection of tables as a graph, with cells as nodes and implicit connections as edges.\nThe connections are cells with the same content or position in different tables. Using graph NN, TCN\nlearns a table representation for predicting column types and relations. In the TURL approach [50],\nthe column header and the embedding representation of entities linked to the cells within the column\nare considered to determine the final annotation. Similarly, Tab2KG [64] creates a domain profile from\nthe KG and uses it together with a table profile to generate mappings using Siamese Networks between\nthe column content and the types in the KG. A domain profile associates relations with feature vectors\nrepresenting data types and statistical characteristics such as value distributions. Doduo [149] performs\nType Annotation using a pre-trained language model, precisely fine-tuning the BERT model on serialised\ntabular data. Each column is encoded by appending a special token [CLS] at the beginning, and the\nresulting embedding representation serves as the contextualised column representation. Column types\nare predicted using a dense layer followed by an output layer with a size corresponding to the number\nof column types.\nOther approaches cannot be classified in the previously mentioned methods; for instance, in [94,\n152] a CRF is employed to make statistical predictions considering the context, such as column name\nand values.\nSimilarly, [155] uses a Markov Network with three potential functions: column-content\n(similarity between observed cells and a candidate column type), column-column (the similarity between\nthe candidate type of a column and the currently assigned type of other columns), and title-column\n(similarity between the title and column type).\nOther approaches such as [22, 175] compute some\nconditional features considering the presence of specific matches in the column title and content. Another\n23www.wikidata.org/wiki/Help:Ranking\n24viznet.media.mit.edu\n19\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\napproach [164] uses a KG taxonomy constructed using type-entity extraction patterns. These patterns,\nsuch as “What is the A of I ?”, where A represents the seed attributes to be discovered and I is an entity\nin type C obtained from the Probase KG [165]. Those patterns are compared against the Probase’s large\nweb corpus of 50 terabytes. Similarly, in LOD4ALL [114] two scores are used to determine column types.\nOne score prioritises the most frequent ancestor types across the table, while the other emphasises the\nmost specific entity types for each entity. On the other hand, C2[89], a patented approach [90], addresses\nthe type mapping by optimising smaller likelihood problems to reduce the number of candidate types\nconsidered. The approach starts with independently finding the top candidates for NE-columns. Best\ncandidate entities are used as pivots to narrow the search, while other features such as Background\nKnowledge Graph (BKG), diversity (reward deduplication during the scoring), tuple validation (type\nco-occurrence), and belief sharing (column headers from different tables) are used to enhance prediction\naccuracy. Zhang et al.[178] adopts a different strategy by leveraging the column label and column values\nto train a classifier for each type present in the KG. This classifier is then used to predict the most\nsuitable type for the column based on the learned patterns and characteristics.\nEventually, a naive\napproach is applied in Kepler-aSI [13], for columns with more than one candidate type no annotation is\nprovided.\nThere are cases like [43, 14, 146, 123, 172] where the specific details about the column annotation\nsub-task are not provided, making it difficult to understand the exact methodology employed. This is in\npart because their code is not provided in open source as discussed in Section 8.\nFinally, some approaches do not deal with annotations on columns specifically, such as [157, 63, 57,\n115, 142, 19, 119, 129, 153, 55, 85, 108, 176, 103, 107].\n4.7\nPredicate Annotation\nThe Predicate Annotation sub-task can be challenging, primarily due to the incompleteness of public\nKGs such as DBpedia or Wikidata. Additionally, the Predicate Annotation sub-task can be further\ncategorised into two parts: NE relations, which focus on the relationship between the subject column\n(S-column) and a named entity (NE) column, and LIT relations, which involve the relationship between\nthe subject column (S-column) and a literal (LIT) column.\nThe approaches can be categorised into: i) ruleset [70, 71], ii) pattern matching [157, 164, 142, 50],\niii) majority voting [105, 151, 117, 118, 162, 94, 116, 115, 22, 138, 58, 26, 35, 114, 158, 98, 147, 38, 14,\n91, 122, 143, 172, 78, 13, 146, 69, 77, 30, 36, 75, 11], iv) statistical\n[152, 154, 153, 180, 120, 29, 1, 34,\n3, 9, 178, 2, 4], and v) embedding [28, 163, 149].\nThe ruleset method is mainly applied in the initial approaches [70, 71]. The ruleset method consists\nof a set of rules used to calculate a similarity score between the properties in the KG and the column\ntypes. In case the table title is provided it is taken into consideration for the score.\nThe pattern matching technique, used by [157, 164, 142], consists of searching for exact matches\nof subject and object pairs in a large corpus to retrieve possible properties. Similarly, TURL [50] aims\nto extract relations between columns without entity linking. It treats the concatenated table metadata\nas a sentence and considers the headers of the two columns as entity mentions.\nThe most commonly used approach is majority voting.\nInitially, Limaye et al. [105] defined a\nfeature vector based on the occurrence and frequency of relations between entities. The most frequently\noccurring relation is the selected one. A similar method is applied in [151, 117, 118, 162, 22, 116, 115,\n26, 35, 114, 147, 158, 98, 29, 38, 91, 122, 143, 13, 69, 36]. In [147] an additional criterion is introduced\nto handle cases where relations have equal occurrence. The approach uses column types to examine the\nResource Description Framework Schema (RDFS) range and domain in such situations. When multiple\nrelations have a valid range and domain, the approach selects the relations with the most specific range\nand domain column types. Another important aspect is handling duplicate cells within the same column,\nas the frequency count can be misleading in cases with a high number of duplicates. In [58], the authors\naddress this issue by considering possible duplicate cells only once. In contrast, the T2K approach [138],\ndo not perform deduplication. Instead [94], takes a different approach by extracting relations for each\npair and uses the Stiner Tree Algorithm to compute the minimal tree among them.\nRecent approaches have introduced diverse methods for handling NE and LIT relations. Moreover,\nthe adoption of statistical or approximate matching methodologies have seen a noticeable increase.\nIn [152, 154, 153], the authors leverage existing user-defined KGs. The relations are annotated using\na directed weighted graph constructed on top of known properties, which are expanded using semantic\ntypes in the domain ontology. The properties are represented as weighted links between nodes. TableM-\niner+ [180] aims to enhance relation matching by employing the Dice similarity measure [52]. The Dice\n20\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nfunction calculates an overlap score by comparing the bag-of-words representations of the cell and the\nobject of a triple, and subsequently, the most frequent resulting relation is selected.\nWhen handling NE columns, several approaches employ string similarity functions such as Levenshtein\ndistance (also known as Edit Distance), Jaccard Similarity, letter distance, or bag-of-words overlap.\nNotable examples of approaches that use these techniques include [120, 29, 1, 34, 3, 9, 178, 2, 4].\nFor LIT columns, various techniques are employed in different approaches.\nTeamTR [172] and\nJenTab [1, 3, 2, 4] use fixed thresholds for comparing literals in the KG with the values in the table. Sim-\nilarly, other approaches employ custom formulas, such as DAGOBAH [78], which uses an absolute value\nformula, while Mantistable SE [34] and MantisTable V [9] use an exponential function as threshold for\nliteral comparison. The MTab approach [120] approximates comparisons using a threshold and applies a\ncustom formula for numeric values. Similarly, LinkingPark [29] uses pre-computed statistics specifically\ndesigned for numerical columns. A more detailed description of their process is provided in [30]. When\ndealing with dates, after parsing, they may be treated similarly to numerical values (e.g., in [34, 9, 1, 3]).\nIn DAGOBAH [78], the considered types include ID, number, string, and date types. The authors em-\nploy different matching techniques for matching properties for each data type. Date matching involves\nconsidering various formats. As described in the previous Section, the MAGIC approach [146] employs\na comprehensive procedure that simultaneously addresses CEA, CTA, and CPA. Majority voting is used\nas the selection strategy across columns, similar to the aforementioned approaches.\nSome embedding-based methods have recently emerged for the CPA task. These methods leverage\nproperty features to represent the potential relationships between the target and surrounding columns.\nIn the approach proposed by Chen et al. [28], a property Vector algorithm (P2Vec) is introduced for\nPredicate Annotation. In the 2021 version of DAGOBAH [77], KG embedding is incorporated along\nwith existing features to enhance property disambiguation, while majority voting remains the selection\ncriteria. Another embedding-based method, TCN, is presented in Wang et al. [163]. TCN concatenates\nthe embedding of the subject and object columns and passes through a dense layer to generate predictions\non their relationship. Lastly, in Doduo [149], the corresponding embedding representation, as described\nin the Type Annotation (Section 4.6), is extracted for each column. These embedding representations\nare also used for the CPA task.\nSome approaches need more details to fully understand their processes. For example, ADOG [124] uses\nCEA results to extract properties for NE-columns, but the paper does not provide enough information\nabout the underlying algorithm. Similarly, MTab2021 [123] and DAGOBAH2022 [75] also lack adequate\nmethodology description. In the geospatial domain, Cruz et al. [43] mention that geospatial classification\nschemes can be modelled using a part-of or is-a relationship. However, the paper does not delve into the\nmethodology in detail.\nSeveral other approaches, such as [63, 131, 49, 57, 134, 175, 183, 19, 135, 119, 129, 55, 56, 85, 108,\n176, 27, 74, 155, 174, 12, 14, 65, 59, 89, 103, 161, 170, 181, 64, 107] do not address Predicate Annotation.\n4.8\nNIL Annotation\nIn the context of NLP, the NIL [79] Annotation refers to the task of finding and linking whether a given\ninput belongs to a particular category, class or type called “NIL” or “None”. In STI, NIL Annotations\nindicate cells in tables lacking relevant information. STI involves extracting structured data from tables,\nbut some cells may not correspond to entities in a KG, leading to Not In Lexicon (NIL) predictions. NIL\nAnnotations can be helpful in various applications, such as information extraction, question answering,\nor KG population, where the goal is to extract structured information from tables and integrate it\ninto a knowledge representation system. There are four common ways in the SOTA to perform NIL\nannotation [6]: i) no candidates [19, 164, 138], ii) threshold [117], iii) separate model [98, 178, 50, 69],\nand iv) NIL predictor. Some approaches do not belong to any of the above categorisations. For the\nscope of this survey, it is possible to classify them considering the peculiarity of the NIL annotation\nprocess, which use v) external services for searching candidates [134, 142].\nSometimes in the EL, a candidate generator does not yield any corresponding entities for a mention\n(no candidates); such mentions are trivially considered unlinkable. Bhagavatula et al. [19] introduce\na variation of the EL task for tables, using a graphical model representation, where each mention in\nthe table is associated with a discrete random variable representing its candidate entities for identifying\nand disambiguating unlinked mentions in a Wikipedia table. After performing the CTA task, Wang et\nal. [164] proceed to “expand entities” by creating entities that lack corresponding labels within the KG.\nAlso [138] explicitly mentions that their T2K approach could fill the missing values to DBpedia and\nvice-versa (Web Tables), but it is only listed as a potential use and not validated.\n21\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nA second group of approaches set a threshold for the best linking probability (or a score), below\nwhich a mention is considered unlinkable. For instance, T2LD approach [117] links table cells to entities\nusing the results obtained from a CTA task, and then a query is sent to the KG for each cell, which\nreturns the top N possible entities which are then ranked using an SVM classifier. If the evidence is not\nstrong enough, it suggests that the table cell represents a new entity.\nTo discover NIL mentions, it is also possible to train an additional binary classifier (separate model)\nthat accepts, as input, mention-entity pairs after the ranking phase and several additional features. It\nmakes the final decision about whether a mention is linkable or not. In this group, Kruit et al. [98]\nemploy KG entity and relation embeddings to enhance the disambiguation process when label matching\nis insufficient.\nThis approach contributes to discovering new facts for KG completion.\nThe system\ndeveloped by [178] proposes a method for discovering new entities containing a subset of those entities\nthat can be extended with information features: a neural embedding space (Word2vec representation), a\ntopical space (annotation of other entities) and a lexical space (normalised Levenshtein distance). Deng\net al. [50] propose an innovative approach that uses a Transformer encoder with masked self-attention\nto predict the masked entities based on other entities and the table context (e.g., caption/header). This\nencourages the model to learn factual knowledge from tables and encode it into entity embeddings\nfor annotation use.\nEventually, Heist et al.’s algorithm [69] considers a data corpus from which co-\noccurring entities and related relationships can be extracted (e.g., listings in Wikipedia or a collection\nof spreadsheets). Furthermore, they assume that a KG which contains a subset of those entities can be\nextended with information learned about the co-occurring in the corpus.\nRegarding the NIL Annotation in the SOTA, some models developed for annotation of the free-form\ntext introduce an additional special “NIL” entity in the ranking step in the EL phase, so models can\npredict it as the best match for the mention [6]. It should be noted that currently, STI approaches do\nnot employ this technique.\nRelated to the use of external services, [134] describes an algorithm that uses web search engines\nto gather information about “unknown entities” (not present in the KG) and annotate them with the\ncorrect type analysing the snippet. Similarly, the approach in [142] searches for exact entity matches\nacross the Subject-Verb-Object (SVO)25 Triples of the Never-Ending Language Learning (NELL) project.\nThe process creates a probabilistic model to estimate the posterior probability of a relationship along\nwith entity-pair instances, and then it uses this relation to create new entities.\n5\nMethod (Supervision)\nThis Section delves into the METHOD dimension and its distinctive categories. While we review the\napproaches proposed to solve individual tasks in Section 4, here we summarise the overall usage of\nlabelled data to train models that solve one or more specific tasks. We consider three categories of\napproaches. First, the UNSUPERVISED approaches (Section 5) do not rely on annotated data during\nthe STI process. Secondly, the SUPERVISED approaches (Section 5) utilise a training set, such as a\ncollection of pre-annotated tables. Some approaches are characterised by using both unsupervised and\nsupervised techniques; we can define these approaches as HYBRID (Section 5).\nThe chart in Fig. 7 displays the yearly distribution of approaches in supervised and unsupervised\ncategories. In the years leading up to 2019, the number of approaches in both categories is roughly the\nsame. However, in 2019, the SemTab challenge’s inaugural edition led to a significant uptick in unsuper-\nvised methods, peaking at 13 approaches proposed in 2020. As of 2021, the gap between supervised and\nunsupervised approaches is closing due to the increasing use of GSs as training and the implementation\nof LLM (e.g., BERT) for annotations.\n25rtw.ml.cmu.edu/resources/svo\n22\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nSupervised\nHybrid\nUnsupervised\n2007\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n11\n3\n2\n3\n2\n1\n1\n2\n2\n2\n4\n1\n3\n4\n3\n3\n1\n2\n2\n1\n2\n7\n2\n1\n1\n2\n2\n7\n1\n5\n3\nFigure 7: A comparison of Supervised, Unsupervised and Hybrid approaches per year.\nUnsupervised\nNumerous approaches [70, 71, 157, 105, 151, 162, 131, 164, 22, 49, 57, 115, 134, 175,\n183, 142, 152, 138, 58, 56, 180, 85, 176, 27, 35, 114, 120, 124, 147, 158, 1, 12, 14, 29, 38, 34, 91, 122, 143,\n161, 172, 3, 2, 9, 13, 123, 4, 30, 36] (49) prioritise the utilisation of unsupervised methods, driven by\nthe challenges associated with acquiring high-quality datasets for real-world scenarios and the difficulty\nof modeling annotation problems for ML methods. Customised approaches offer greater control over\nthe results and higher annotation precision (as discussed in Appendix C). Table 3 in the Appendix A.2\ndisplays the techniques used in the Candidate Generation and Entity Disambiguation of EL step.\nSupervised\nWhile a relatively limited number of supervised approaches has been proposed (if com-\npared to the number of supervised approaches), these approaches (28) offer exciting solutions when im-\nplemented. Table 4 in the Appendix A.2 displays the techniques used by these approaches. Supervised\napproaches listed below use a collection of tabular or textual data to learn patterns and relationships\nbetween the input data and the corresponding labels. In the following, we briefly summarise the data\nused to train supervised approaches and the main idea behind these approaches.\nMulwad et al. [117] query Wikitology26 to extract entities and use a SVM model for entity ranking.\nQuercini et al. [134] collect label categories from DBpedia and snippets from Bing to train the text\nclassifier.\nErmilov et al. [58] use a portion of the T2D Gold Standard27, where the S-column and\ncolumn-pairs have been annotated to train an SVM and a PGM. T2Dv2 dataset28 has been used by\nZhang et al. [178] to train a binary classifier and Chen et al. [27] to train a CNN. The former uses also\nthe WDC Tables dataset29 to train a random forest model. TabEl [19] parses Wikipedia tables to build\na corpus containing more than 1.6 million tables30. Such tables contain hyperlinks to Wikipedia, and\nTabEL uses these hyperlinks as probability estimates for training a Markov Network. The same training\ntechnique is used also by Takeoka et al. [155] on a private dataset of 183 human-annotated tables with\n781 NE-columns and 4109 LIT-columns31. Pham et al. [129] use four different datasets: city, weather,\nmuseum and soccer32. Only city, museum and soccer datasets have been used to train random forest\nand logistic regression models. The weather dataset is only used in semantic labelling because it cannot\nprovide sufficient feature vectors for training classifiers. Neumaier et al. [119], propose a hierarchical\nclustering to build a BKG from DBpedia.Luo et al. [108] use English and Chinese dumps of Wikipedia\nfor training word and entity embeddings. In total, it collects 3818 mentions from 150 tables. Deng et\nal. [50] use Wikitable corpus to train a BERT model while Gottschalk et al. [64] creates a new synthetic\ndataset automatically extracted from GitHub repositories33. This dataset is then used for training the\nSiamese network.\nHybrid\nA small number of approaches (11) opt for solutions involving the use of supervised and\nunsupervised techniques. Since 2017, an increase in the use of semantic embeddings has been observed.\nThe embeddings exploit a vectorial representation of the rich entity context in a KG to identify the table’s\nmost relevant subset of entities. This technique is used in conjunction with lookup-focused [55, 59] or\nrule-based [26, 78, 77] methods. Others adopt a transformer-based embedding in combination with the\nuse of heuristics [75, 107] or embedding technique [146]. ColNet [27] utilises a CCN model and a method\nwhich automatically extracts samples from the KG. Kruit et al. [98] employ a Probabilistic Graphical\n26ebiquity.umbc.edu/project/html/id/83/Wikitology\n27webdatacommons.org/webtables/goldstandard.html\n28webdatacommons.org/webtables/goldstandardV2.html\n29webdatacommons.org/webtables\n30websail-fe.cs.northwestern.edu/TabEL\n31The paper referenced UCI ML repository, but it was found to be a private dataset upon contacting the author.\n32github.com/usc-isi-i2/eswc-2015-semantic-typing, the soccer dataset is no longer available.\n33github.com/search/advanced\n23\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nModel and features on T2D and Webaroo34 while [69] rely on distant supervision to derive rules for CPA\nand rule-mining techniques.\n6\nDomain\nSTI approaches can be categorised as domain-dependent or domain-independent. Domain-dependent\nsystems address problems and provide solutions specific to the domain they are built for.\nInstead,\ndomain-independent STI approaches do not rely on domain knowledge and provide solutions not tied to\na specific area of expertise. Among all the approaches only [70, 71, 43, 108, 22, 119, 85] can be classified as\ndomain-dependent. A few papers [70, 71, 22] are related to the food microbiology domain and present a\npredefined set of rules specifically built to address the different numerical units in Datatype Annotation.\nTwo papers [119, 85], are designed to deal only with numerical data labelling. Finally, [43] is designed\nto address several challenges that come from the geospatial and temporal data manipulation and [108]\naddress data and KG written in different languages.\nThe remaining approaches can be considered domain-independent.\nDomain-dependent approaches, usually, are affected by the domain ontology used, such as Hignette\net al. [70, 71] that distinguish between symbolic and numeric columns, using some of the knowledge\ndescribed in the ontology, which has been created ad hoc.\n7\nApplication/Purpose\nThis Section discusses the application purpose of approaches ranging from i) KG construction, ii) KG\nextension and tabular data enrichment, and iii) cross-lingual linking. For the first two purposes\nespecially, we refer to Fig. 5 and Fig. 4.\nThe aim of KG construction is to derive meaningful information from tabular data, transforming it\ninto a structured and interconnected knowledge representation. This supports cross-domain knowledge\nintegration, discovery, and graph-based data analysis. Approaches [69, 56, 138, 180, 175, 98, 58, 91, 50,\n153, 152] employ STI to construct and populate KGs. Similarly, [115, 116] focus on extracting facts as\nRDF triples from annotated tables.\nSTI significantly enhances KG extension and tabular data enrichment by extracting structured\ninformation from tables, linking it to an existing KG, and expanding the KG with interpreted graph\ndata. This process, involving automatic identification and annotation of mentions, relationships, and\ntable schema, ensures the extraction of comprehensive and accurate information from tabular data.\nApproaches such as [175, 69, 178, 64] discover new entities, types, and properties to enrich the KG,\nincluding identifying new labels for known entities. Additionally, [46, 47, 31], showcase how STI supports\nadding more columns to input tables for downstream data analytics, using annotations as joins to fetch\nrelevant data from the external KG and export the output table. Semantic table interpretation provides\na means to bridge the language barrier, thus supporting cross-lingual linking of entities mentioned in\nthe tables. A similar, cross-lingual application is proposed in [108], which annotates a table containing\nentities expressed in one language with entities in a KG expressed in another.\nSince 2019, the majority of recent approaches have focused more on addressing the tasks outlined in\nthe SemTab challenge [26, 35, 120, 114, 158, 124, 147, 1, 12, 161, 143, 122, 78, 14, 29, 34, 91, 172, 9,\n3, 13, 77, 123, 170, 146, 30, 36, 75, 11], and less on downstream applications. We observe that in some\napplications of KG construction and tabular data enrichment, the most important task to fully automate\nis CEA (especially on large tables), while it is assumed that CPA and CTA can be manually performed\nor revised by a user to ensure a desired level of quality.\n8\nLicense\nLicensing is vital in protecting intellectual property and establishing the terms under which software,\ncontent, or creative works can be used or distributed. Thus, it is very important to review STI approaches\nunder such dimension so that users can evaluate which one to use for their specific use case and ensure\ncompliance with legal requirements. There are 6 different licenses used by STI tools and approaches.\n34The dataset is no longer available.\n24\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nThe most used one is Apache 2.035, an open-source software license widely used in the development\nand distribution of software. It allows users to freely use, modify, and distribute the licensed software.\nUsers must include a copy of the Apache 2.0 license, a clear attribution to the original authors, and\nclearly identifiable modification notices on all altered files. Such licensing is used by 23 approaches [94,\n135, 138, 119, 129, 154, 153, 56, 180, 27, 28, 35, 174, 38, 34, 103, 3, 2, 9, 4, 50, 36, 149]. The second most\nused license is MIT36 used by 10 approaches [85, 74, 98, 158, 1, 143, 123, 30, 64]. The MIT License is a\npermissive open-source software license that allows users to freely use, modify, and distribute the licensed\nsoftware. The Orange license37 is used by [26, 77, 75, 107]. GPL 3.038 is used by 2 approaches [58, 69]\nwhile 2 adopts CCA 4.039 [178, 19], and eventually, [146] employs a unique licensing by Ghent University\n(Imec).\nAmong the approaches reviewed in this survey, 46 of them lack any specific licensing information.\n9\nValidation\nThe effectiveness of the approaches proposed so far is usually evaluated in terms of annotation quality\non different separate computational tasks (e.g., using Precision, Recall, and F1 scores). In the literature,\nthe open-source tool STILTool [42] is available for the automated evaluation of the quality of semantic\nannotations generated by semantic table interpretation methods. For each approach, we specify the\ndatasets used for its evaluation. We report this association in Table 6, discussed in Appendix C, where we\ndiscuss the datasets used for table interpretation. A very few approaches also considered other dimensions\nfor evaluation, such as execution times and scalability [31, 50]; the latter dimension is relevant, especially\nfor entity linking, which may be inefficient on large tables.\n10\nOpen Issues and Potential Research Directions\nDespite the many contributions to advance STI discussed in this paper, we believe that there are some\nopen issues associated with the key challenges listed in Section 1; these open issues could hinder a broader\nuptake of STI solutions for downstream applications and, at the same time, suggest valuable questions\nfor researchers working in these fields.\ni) Heterogeneity of domains and data distributions: the lack of labelled data specifically\ntailored for a domain of interest prevents training and evaluation of domain-specific solutions. Potential\nsolutions to overcome this scarcity could be: a) involving domain experts in the annotation process,\nadditionally using crowd-sourcing platforms to engage a larger pool of annotators with domain-specific\nknowledge [137]; b) using specific data (e.g., [144, 5, 84]); c) developing pre-trained models on large\nGSs, which can be fine-tuned and/or adapted using a small amount of labelled data (in this way, STI\napproaches would reuse existing annotations and reduce the burden of creating domain-specific GSs from\nscratch) (e.g., [50]).\nii) Limited contextual information: missing context can introduce ambiguity, making it chal-\nlenging to determine the intended meaning of table elements. Most of the GSs used in recent work do not\nprovide tables associated with extended context; consequently, these aspects have not been emphasised\nmuch in recent work. Possible solutions could be: a) reusing prior datasets based on web tables, which\nemphasise these challenges, or b) developing new datasets.\nIn addition, researchers should consider\nc) techniques that infer missing context from the available information (e.g., table headers, table meta-\ndata, surrounding text [50]) or d) employing final-user feedback to enhance the contextual understanding\nof the table [75].\niii) Detecting the type of columns: the analysis of the SOTA has shown that different approaches\neffectively manage type annotation. The most critical open challenge is the detection of L-columns. Using\nRegex was found effective for identifying L-columns [36], but domain-specific literal values (e.g., for\ngenomics data of biological pattern) are not yet addressed. A potential solution is the definition of new\ndomain-specific regular expressions for the Type Annotation sub-task.\niv) Matching tabular values against the KG: STI approaches work well when the mentions\nin the NE-columns or literals in the L-columns are similar enough to the values in the KG. Regarding\nannotation of mentions, synonyms, aliases, abbreviations, and acronyms, should be considered to enhance\n35apache.org/licenses/LICENSE-2.0\n36opensource.org/license/mit\n37orangedatamining.com/license\n38gnu.org/licenses/gpl-3.0.html\n39creativecommons.org/licenses/by/4.0\n25\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nthe approach’s potential. This remains an open issue, as only a few approaches use indexes with aliases\n(e.g., [36, 11]). While this direction seems promising, there is still ample room for improvement. It is also\nnecessary to consider that the mention can contain typos or have syntactic differences from the entities\nin an KG. In such cases, using a) indices [36, 123, 11] and adequate b) similarity measures [178, 161]\ncan increase the results of the candidate generation. The SemTab challenge, which tests STI even on\ncorrupted data, has shown that this problem has not been adequately solved yet. Regarding the use of\nliteral values for matching (i.e., those in L-columns), the challenge arises due to inconsistencies between\nthe values in the KG and those in tabular data. Since KGs are known to be incomplete or not updated\nfrequently enough, the correct literal value for a given property may significantly deviate from the one in\nthe table (or in the KG). In the SOTA, this challenge is typically addressed by setting c) thresholds or\nranges (e.g., [119]), but these methods introduce the risk of selecting incorrect annotations. A promising\nresearch direction would involve leveraging d) statistical and ML methods to surmount this limitation\nand achieve even better results.\nv) Disambiguation of named entities: disambiguation remains a complex and challenging task\nwhen the table context is insufficient or unclear. Another aspect that makes disambiguation still chal-\nlenging is the presence of homonyms in the KG, especially when they belong to very similar types. In this\ncase, only the surrounding context can help disambiguate different candidates. However, this remains an\nopen issue as not all approaches consider contextual analysis, or as depicted in challenge ii), sometimes\nthe context should be inferred. In the context of this challenge, homonym management plays a crucial\nrole. A possible research direction a) is to include models that consider all elements contributing to the\ncreation of the context (e.g., [50]). Some of the most recent approaches have proposed to use b) LLM to\ncapture complex linguistic patterns, semantic relationships, and contextual cues in the tabular data, ob-\ntaining some promising results in improving disambiguation (e.g., [149]). However, further investigation\nis needed to explore the use of such models, their efficiency, and comparison with traditional approaches.\nSome recent approaches have also proposed using latent representations or feature-based neural networks\nto re-rank candidate entities retrieved with more traditional techniques [8, 107]: these hybrid solutions\nare also promising.\nvi) NIL-mentions: Currently, only 10 approaches address NIL annotation, yielding inconsistent\nresults, as seen in the 2022 and 2023 SemTab challenges. The SOTA has not adequately tackled this\ncrucial aspect despite its significance in KG extension and construction for practical applications. Limited\nNIL coverage in GSs biases algorithms toward always selecting the best candidate without deciding\nwhether to link.\nOne solution is to a) develop GSs that better represent the problem, encouraging\nsolutions that decide on linking the best candidate entity (e.g., [110]). Additionally, b) utilise techniques\nand external sources (e.g., search engines) for enriched representations of mentions and entities [123].\nLastly, c) incorporate domain-specific expert knowledge to enhance NIL-mention identification.\nvii) Choosing the most appropriate types and properties: more than one type could capture\nthe meaning of one column. This is due especially to the hierarchical organisation of types in ontologies\n(e.g., an actor is also a person) but also to the presence of very similar types (e.g., in Wikidata). Selecting\nthe types that better captures the semantics of a column among all correct types is still an open issue.\nPossible solutions may come from a) using contextual information. Analogous issues affect the selection\nof properties to annotate pairs of columns, which is even more challenging: predicate annotation is\nusually performed after other sub-tasks, which increases the risk of error propagation.\nviii) Collective aggregation of evidence from different tasks:\nas described in Section 1,\ntable interpretation is a collective decision-making process.\nFinding strategies to maximise evidence\nexchange across sub-tasks effectively is challenging. a) Heuristic and b) ML approaches can be useful\nto overcome this challenge.\nThe heuristic approach depends on expert-derived strategies and fixed\nrules, making it less adaptable to dynamic environments and potentially hindering its performance with\ndifferent data inputs [36, 30, 4, 123, 11]. ML models, instead, offer the capability to make decisions based\non learned patterns and relationships between different features, such as table data, annotations, and\ncontexts [149, 50]. In addition, ML techniques allow determining feature weights by iterating on data\nand adapting them incrementally.\nix) Amount and shape of data: the amount of data introduces two opposite challenges: data\nabundance and data scarcity.\nData abundance can pose significant challenges for STI approaches.\na) Sampling, subset selection, feature selection, or dimensionality reduction can be employed to address\ndata abundance. Data scarcity can be addressed by b) data augmentation (e.g., [138, 109]). c) Tech-\nniques such as synthetic data generation, sampling methods, or data transformations can be used to\ncreate additional training instances. Moreover, d) transfer or active learning can help overcome data\nscarcity (e.g., [27]). However, this remains an open challenge as none of the reviewed approaches adopts\n26\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nany of these techniques for data augmentation.\nx) Annotation of complex formatted tables:\nannotating complex tables introduces unique\nchallenges due to their intricate structures, which often include merged cells, hierarchical data, and\nvarying formats. These complexities can obscure relationships between data points, making it difficult\nto apply standard annotation methods effectively. Web tables containing complex structures constitute\na small population and have not been the focus of research [180]. One solution could be to incorporate\na pre-process step that parse complex structures [173].\nIn addition to these open issues closely related to the STI process, it is possible to identify other open\nissues related to the GSs.\ni) GSs Availability: Reliable benchmarks are essential for evaluating the effectiveness of STI meth-\nods. Our review revealed a lack of high-quality benchmarks (see Appendix C), impeding the development\nand assessment of STI techniques. To enhance robustness evaluations across diverse data distributions,\nit is crucial to assess STI approaches using various GSs. Additionally, the creation of domain-specific\nGSs tailored to specific applications is recommended.\nii) Multi-lingual GSs: The current limitation of predominantly English GS hampers the repro-\nducibility and generalization of STI approaches across languages in real-world scenarios. Integrating\nlanguage detection is crucial to address this issue and enhance STI systems.\nAdditionally, creating\nmulti-lingual GSs is essential to support the training and evaluation of these systems, covering diverse\nlanguages, data sources, and domains for comprehensive coverage.\niii) GSs with NIL: as discussed above, NIL-mentions are absent or underrepresented in GSs used\nin SOTA. We believe that creating GSs that better cover this annotation type is very important.\niv) Evaluation metrics: different approaches use different metrics to evaluate their performance.\nFor instance, in the SemTab challenge, different formulas are used to calculate Precision, Recall and F1\nmeasures in relation to different datasets. For this reason, there is a need for standardised and concrete\nmetrics to effectively test and evaluate various approaches.\nRegarding tools for implementing STI approaches, the following open issues can be identified:\ni) Transferability: while reviewing specific approaches, we observed limitations in their usability in\nreal-world scenarios. In fact, evidence of usage of STI approaches in real-world downstream applications\nis still limited.\nii) Replicability: while this survey includes numerous STI approaches, a significant portion of\nthem lack publicly available replication code. The lack of availability of open-source systems has two\nmain implications: testing and evaluating third-party approaches become a complex, time-consuming,\nand error-prone task; checking errors and understanding issues and limitations to advance the field is\ndifficult. Better sharing of source code can improve transparency and accelerate advancements in this\nfield.\niii) Usability: Just a handful of tools feature a UI, and among them, only a minority possess a\nwell-crafted UX. To ensure the usability of these solutions, it is imperative to conduct user tests, monitor\nuser behaviour, and employ other techniques tailored to the UI and UX design process.\niv) Adaptability: Most of the approaches and tools come with static algorithms. However, when\nusers want to annotate their data, they would like to optimise algorithms for specific data distribu-\ntions. Improving the support for human-in-the-loop annotation with algorithms that exploit the feed-\nback collected from the users through the UI would provide solutions more helpful in several downstream\napplications.\n11\nConclusions\nThis survey aims to provide a comprehensive and in-depth analysis of available approaches that perform\nSTI.\nIt includes approaches from 2007 to the time of writing, resulting in the identification of 88\napproaches. Different criteria are used to compare and review STI approaches, which are organised into\na taxonomy to allow a fair comparison and identify potential future research areas. This analysis allowed\nus to create the Table 8 in Appendix D, which provides support in selecting approaches in relation to\nvarious attributes, such as Method, Tasks, Code availability, License and Triple store. Also, tools and\nGS have undergone a thorough analysis using specific comparative criteria. As a result of such analysis,\nopen issues have been addressed, and potential research directions have been described. The survey aims\nto serve as a valuable resource for newcomers, providing an overview of the current SOTA in STI and\nfacilitating their exploration of potential directions for enhancing STI performance. In future work, open\nissues for each approach will be identified. Another direction is to review the performance metrics used\nby each approach.\n27\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nReferences\n[1] N. Abdelmageed and S. Schindler.\nJentab: Matching tabular data to knowledge graphs.\nIn\nSemTab@ ISWC, pages 40–49, 2020.\n[2] N. Abdelmageed and S. Schindler. Jentab: A toolkit for semantic table annotations. In Second\nInternational Workshop on Knowledge Graph Construction, 2021.\n[3] N. Abdelmageed and S. Schindler. Jentab meets semtab 2021’s new challenges. In SemTab@ ISWC,\npages 42–53, 2021.\n[4] N. Abdelmageed and S. Schindler. Jentab: Do cta solutions affect the entire scores. Semantic Web\nChallenge on Tabular Data to Knowledge Graph Matching (SemTab), CEURWS. org, 2022.\n[5] N. Abdelmageed, S. Schindler, and B. K¨onig-Ries.\nBiodivtab: A table annotation benchmark\nbased on biodiversity research data. In SemTab@ ISWC, pages 13–18, 2021.\n[6] M. Alam, D. Buscaldi, M. Cochez, F. Osborne, D. R. Recupero, H. Sack, O. Sevgili, A. Shelmanov,\nM. Arkhipov, A. Panchenko, C. Biemann, M. Alam, D. Buscaldi, M. Cochez, F. Osborne, D. Re-\nfogiato Recupero, and H. Sack. Neural entity linking: A survey of models based on deep learning.\nSemant. Web, 13(3):527–570, jan 2022.\n[7] A. Alobaid, E. Kacprzak, and O. Corcho. Typology-based semantic labeling of numeric tabular\ndata. Semantic Web, 12(1):5–20, 2021.\n[8] R. Avogadro, M. Ciavotta, F. De Paoli, M. Palmonari, and D. Roman. Estimating link confidence\nfor human-in-the-loop table annotation.\nIn International Conference on Web Intelligence and\nIntelligent Agent Technology, Venice, Italy, 2023.\n[9] R. Avogadro and M. Cremaschi. Mantistable v: A novel and efficient approach to semantic table\ninterpretation. In SemTab@ ISWC, pages 79–91, 2021.\n[10] R. Avogadro, M. Cremaschi, F. D’adda, F. De Paoli, M. Palmonari, et al. Lamapi: a comprehensive\ntool for string-based entity retrieval with type-base filters. In 17th ISWC workshop on ontology\nmatching (OM), 2022.\n[11] R. Avogadro, F. D’Adda, and M. Cremaschi.\nFeature/vector entity retrieval and disambigua-\ntion techniques to create a supervised and unsupervised semantic table interpretation approach.\nKnowledge-Based Systems, 304:112447, 2024.\n[12] R. Azzi, G. Diallo, E. Jim´enez-Ruiz, O. Hassanzadeh, V. Efthymiou, J. Chen, and K. Srinivas.\nAmalgam: making tabular dataset explicit with knowledge graph. In SemTab@ ISWC, pages 9–16,\n2020.\n[13] W. Baazouzi, M. Kachroudi, and S. Faiz. Kepler-asi at semtab 2021. In SemTab@ ISWC, pages\n54–67, 2021.\n[14] W. Baazouzi, M. Kachroudi, S. Faiz, E. Jim´enez-Ruiz, O. Hassanzadeh, V. Efthymiou, J. Chen,\nand K. Srinivas. Kepler-asi: Kepler as a semantic interpreter. In SemTab@ ISWC, pages 50–58,\n2020.\n[15] N. Barlaug and J. A. Gulla. Neural networks for entity matching: A survey. ACM Transactions\non Knowledge Discovery from Data (TKDD), 15(3):1–37, 2021.\n[16] F. Belotti, F. Dadda, M. Cremaschi, R. Avogadro, R. Pozzi, and M. Palmonari.\nEvaluating\nlanguage models on entity disambiguation in tables. arXiv preprint arXiv:2408.06423, 2024.\n[17] O. Benjelloun, S. Chen, and N. Noy. Google dataset search by the numbers. In International\nSemantic Web Conference, pages 667–682. Springer, 2020.\n[18] C. S. Bhagavatula, T. Noraset, and D. Downey.\nMethods for exploring and mining tables on\nwikipedia.\nIn Proceedings of the ACM SIGKDD workshop on interactive data exploration and\nanalytics, pages 18–26, 2013.\n28\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n[19] C. S. Bhagavatula, T. Noraset, and D. Downey.\nTabel: Entity linking in web tables.\nIn The\nSemantic Web - ISWC 2015, pages 425–441, 2015.\n[20] D. Blei, A. Ng, and M. Jordan.\nLatent dirichlet allocation.\nAdvances in neural information\nprocessing systems, 14, 2001.\n[21] S. Bonfitto, E. Casiraghi, and M. Mesiti. Table understanding approaches for extracting knowl-\nedge from heterogeneous tables. Wiley Interdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 11(4):e1407, 2021.\n[22] P. Buche, J. Dibie-Barthelemy, L. Ibanescu, and L. Soler.\nFuzzy web data tables integration\nguided by an ontological and terminological resource. IEEE Transactions on Knowledge and Data\nEngineering, 25(4):805–819, 2013.\n[23] T.-C. Bucher, X. Jiang, O. Meyer, S. Waitz, S. Hertling, and H. Paulheim. scikit-learn pipelines\nmeet knowledge graphs: The python kgextension package. In The Semantic Web: ESWC 2021\nSatellite Events: Virtual Event, June 6–10, 2021, Revised Selected Papers 18, pages 9–14. Springer,\n2021.\n[24] M. J. Cafarella, A. Halevy, D. Z. Wang, E. Wu, and Y. Zhang. Webtables: exploring the power of\ntables on the web. Proceedings of the VLDB Endowment, 1(1):538–549, 2008.\n[25] T. Ceritli, C. K. Williams, and J. Geddes. ptype: probabilistic type inference. Data Mining and\nKnowledge Discovery, 34(3):870—-904, 2020.\n[26] Y. Chabot, T. Labb´e, J. Liu, and R. Troncy. Dagobah: An end-to-end context-free tabular data\nsemantic annotation system. In SemTab@ ISWC, pages 41–48, 10 2019.\n[27] J. Chen, E. Jim´enez-Ruiz, I. Horrocks, and C. Sutton. Colnet: Embedding the semantics of web\ntables for column type prediction. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 33, pages 29–36, 2019.\n[28] J. Chen, E. Jim´enez-Ruiz, I. Horrocks, and C. Sutton. Learning semantic annotations for tabular\ndata. arXiv preprint arXiv:1906.00781, 2019.\n[29] S. Chen, A. Karaoglu, C. Negreanu, T. Ma, J.-G. Yao, J. Williams, A. Gordon, and C.-Y. Lin.\nLinkingpark: An integrated approach for semantic table interpretation. In SemTab@ ISWC, 2020.\n[30] S. Chen, A. Karaoglu, C. Negreanu, T. Ma, J.-G. Yao, J. Williams, F. Jiang, A. Gordon, and C.-Y.\nLin. Linkingpark: An automatic semantic table interpretation system. Journal of Web Semantics,\n74:100733, 2022.\n[31] M. Ciavotta, V. Cutrona, F. De Paoli, N. Nikolov, M. Palmonari, and D. Roman. Supporting\nsemantic data enrichment at scale. In Technologies and Applications for Big Data Value, pages\n19–39. Springer, 2022.\n[32] M. Ciavotta, V. Cutrona, F. De Paoli, N. Nikolov, M. Palmonari, and D. Roman. Supporting\nsemantic data enrichment at scale. In Technologies and Applications for Big Data Value, pages\n19–39. Springer, 2022.\n[33] C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20:273–297, 1995.\n[34] M. Cremaschi, R. Avogadro, A. Barazzetti, D. Chieregato, and E. Jim´enez-Ruiz. Mantistable se:\nan efficient approach for the semantic table interpretation. In SemTab@ ISWC, pages 75–85, 2020.\n[35] M. Cremaschi, R. Avogadro, and D. Chieregato.\nMantistable: an automatic approach for the\nsemantic table interpretation. SemTab@ ISWC, 2019:15–24, 2019.\n[36] M. Cremaschi, R. Avogadro, and D. Chieregato. s-elbat: a semantic interpretation approach for\nmessy table-s. Semantic Web Challenge on Tabular Data to Knowledge Graph Matching (SemTab),\nCEUR-WS. org, 2022.\n[37] M. Cremaschi, J. A. Barbato, A. Rula, M. Palmonari, and R. Actis-Grosso. What really matters\nin a table? insights from a user study. In 2022 IEEE/WIC/ACM International Joint Conference\non Web Intelligence and Intelligent Agent Technology (WI-IAT), pages 263–269. IEEE, 2022.\n29\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n[38] M. Cremaschi, F. De Paoli, A. Rula, and B. Spahiu. A fully automated approach to a complete\nsemantic table interpretation. Future Generation Computer Systems, 112:478 – 500, 2020.\n[39] M. Cremaschi, F. D’Adda, and S. Nocco.\nMantistable ui: A web interface for comprehensive\nsemantic table interpretation management.\n[40] M. Cremaschi, A. Rula, A. Siano, and F. De Paoli. Mantistable: a tool for creating semantic\nannotations on tabular data. In The Semantic Web: ESWC 2019 Satellite Events: ESWC 2019\nSatellite Events, Portoroˇz, Slovenia, June 2–6, 2019, Revised Selected Papers 16, pages 18–23.\nSpringer, 2019.\n[41] M. Cremaschi, A. Rula, A. Siano, F. De Paoli, et al. Semantic table interpretation using man-\ntistable. In OM@ ISWC, pages 195–196, 2019.\n[42] M. Cremaschi, A. Siano, R. Avogadro, E. Jimenez-Ruiz, and A. Maurino. Stiltool: a semantic\ntable interpretation evaluation tool. In The Semantic Web: ESWC 2020 Satellite Events: ESWC\n2020 Satellite Events, Heraklion, Crete, Greece, May 31–June 4, 2020, Revised Selected Papers 17,\npages 61–66. Springer, 2020.\n[43] I. F. Cruz, V. R. Ganesh, C. Caletti, and P. Reddy. Giva: A semantic framework for geospatial\nand temporal data integration, visualization, and analytics. SIGSPATIAL’13, page 544–547, New\nYork, NY, USA, 2013.\n[44] V. Cutrona, F. Bianchi, E. Jim´enez-Ruiz, and M. Palmonari. Tough tables: Carefully evaluating\nentity linking for tabular data. In The Semantic Web–ISWC 2020, Athens, Greece, November 2–6,\n2020, pages 328–343. Springer.\n[45] V. Cutrona, J. Chen, V. Efthymiou, O. Hassanzadeh, E. Jimenez-Ruiz, J. Sequeda, K. Srinivas,\nN. Abdelmageed, M. Hulsebos, D. Oliveira, and C. Pesquita. Results of semtab 2021. In 20th\nInternational Semantic Web Conference, pages 1–12. CEUR Workshop Proceedings, 2022.\n[46] V. Cutrona, M. Ciavotta, F. D. Paoli, and M. Palmonari.\nASIA: a tool for assisted semantic\ninterpretation and annotation of tabular data. In Proceedings of the ISWC 2019 Satellite Tracks,\nvolume 2456 of CEUR Workshop Proceedings, pages 209–212. CEUR-WS.org, 2019.\n[47] V. Cutrona, F. De Paoli, A. Koˇsmerlj, N. Nikolov, M. Palmonari, F. Perales, and D. Roman.\nSemantically-enabled optimization of digital marketing campaigns.\nIn ISWC, pages 345–362.\nSpringer, 2019.\n[48] I. Dasoulas, D. Yang, X. Duan, and A. Dimou.\nTorchictab: Semantic table annotation with\nwikidata and language models. In CEUR Workshop Proceedings, pages 21–37. CEUR Workshop\nProceedings, 2023.\n[49] D. Deng, Y. Jiang, G. Li, J. Li, and C. Yu. Scalable column concept determination for web tables\nusing large knowledge bases. Proc. VLDB Endow., 6(13):1606–1617, Aug. 2013.\n[50] X. Deng, H. Sun, A. Lees, Y. Wu, and C. Yu. Turl: Table understanding through representation\nlearning. ACM SIGMOD Record, 51(1):33–40, 2022.\n[51] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings\nof the 2019 Conference of the North American Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186,\nMinneapolis, Minnesota, June 2019. Association for Computational Linguistics.\n[52] L. R. Dice. Measures of the amount of ecologic association between species. Ecology, 26(3):297–302,\n1945.\n[53] L. Du, F. Gao, X. Chen, R. Jia, J. Wang, J. Zhang, S. Han, and D. Zhang. Tabularnet: A neural\nnetwork architecture for understanding semantic structures of tabular data. In Proceedings of the\n27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 322–331, 2021.\n[54] T. Ebisu and R. Ichise.\nGeneralized translation-based embedding of knowledge graph.\nIEEE\nTransactions on Knowledge and Data Engineering, 32(5):941–951, 2019.\n30\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n[55] V. Efthymiou, O. Hassanzadeh, M. Rodriguez-Muro, and V. Christophides. Matching web tables\nwith knowledge base entities: From entity lookups to entity embeddings. In The Semantic Web –\nISWC 2017, pages 260–277, 2017.\n[56] B. Ell, S. Hakimov, P. Braukmann, L. Cazzoli, F. Kaupmann, A. Mancino, J. Altaf Memon,\nK. Rother, A. Saini, and P. Cimiano.\nTowards a large corpus of richly annotated web tables\nfor knowledge base population. In 5th International Workshop on Linked Data for Information\nExtraction, pages 2–13, 2017.\n[57] I. Ermilov, S. Auer, and C. Stadler. User-driven semantic mapping of tabular data. In 9th I-\nSEMANTICS ’13, page 105–112, New York, NY, USA, 2013. Association for Computing Machinery.\n[58] I. Ermilov and A.-C. N. Ngomo. Taipan: Automatic property mapping for tabular data. In Knowl-\nedge Engineering and Knowledge Management, pages 163–179, Cham, 2016. Springer International\nPublishing.\n[59] Y. Eslahi, A. Bhardwaj, P. Rosso, K. Stockinger, and P. Cudr´e-Mauroux. Annotating web tables\nthrough knowledge bases: A context-based approach.\nIn 2020 7th Swiss Conference on Data\nScience (SDS), pages 29–34, 2020.\n[60] X. Fang, W. Xu, F. A. Tan, J. Zhang, Z. Hu, Y. Qi, S. Nickleach, D. Socolinsky, S. Sengamedu,\nC. Faloutsos, et al. Large language models (llms) on tabular data: prediction, generation, and\nunderstanding–a survey (2024). URL https://arxiv. org/abs/2402.17944, 2024.\n[61] K. Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern\nrecognition unaffected by shift in position. Biological cybernetics, 36(4):193–202, 1980.\n[62] L. Getoor and A. Machanavajjhala. Entity resolution: theory, practice & open challenges. Pro-\nceedings of the VLDB Endowment, 5(12):2018–2019, 2012.\n[63] A. Goel, C. A. Knoblock, and K. Lerman. Exploiting structure within data for accurate labeling\nusing conditional random fields. In Proceedings of the 14th International Conference on Artificial\nIntelligence (ICAI), 2012.\n[64] S. Gottschalk and E. Demidova. Tab2kg: Semantic table interpretation with lightweight semantic\nprofiles. Semantic Web, 13(3):1–27, 2022.\n[65] T. Guo, D. Shen, T. Nie, and Y. Kou.\nWeb table column type detection using deep learning\nand probability graph model. In Web Information Systems and Applications: 17th International\nConference, WISA 2020, Guangzhou, China, September 23–25, 2020, Proceedings 17, pages 401–\n414. Springer, 2020.\n[66] S. Gupta, P. Szekely, C. A. Knoblock, A. Goel, M. Taheriyan, and M. Muslea. Karma: A system\nfor mapping structured sources into the semantic web. In The Semantic Web: ESWC 2012 Satellite\nEvents, pages 430–434. Springer Berlin Heidelberg, 2015.\n[67] X. Han, S. Cao, X. Lv, Y. Lin, Z. Liu, M. Sun, and J. Li. OpenKE: An open toolkit for knowledge\nembedding. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 139–144, Brussels, Belgium, Nov. 2018. Association for\nComputational Linguistics.\n[68] A. Harari and G. Katz. Few-shot tabular data enrichment using fine-tuned transformer architec-\ntures. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1577–1591, 2022.\n[69] N. Heist and H. Paulheim. Information extraction from co-occurring similar entities. In Proceedings\nof the Web Conference 2021, pages 3999–4009, 2021.\n[70] G. Hignette, P. Buche, J. Dibie-Barth´elemy, and O. Haemmerl´e. An ontology-driven annotation\nof data tables. In Web Information Systems Engineering – WISE 2007 Workshops, pages 29–40.\nSpringer Berlin Heidelberg, 2007.\n[71] G. Hignette, P. Buche, J. Dibie-Barth´elemy, and O. Haemmerl´e. Fuzzy annotation of web data\ntables driven by a domain ontology. In The Semantic Web: Research and Applications, pages\n638–653. Springer Berlin Heidelberg, 2009.\n31\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n[72] A. Hogan, E. Blomqvist, M. Cochez, C. d’Amato, G. D. Melo, C. Gutierrez, S. Kirrane, J. E. L.\nGayo, R. Navigli, S. Neumaier, et al. Knowledge graphs. ACM Computing Surveys (Csur), 54(4):1–\n37, 2021.\n[73] M. Hulsebos, C¸. Demiralp, and P. Groth. Gittables: A large-scale corpus of relational tables.\nProceedings of the ACM on Management of Data, 1(1):1–17, 2023.\n[74] M. Hulsebos, K. Hu, M. Bakker, E. Zgraggen, A. Satyanarayan, T. Kraska, C¸. Demiralp, and\nC. Hidalgo. Sherlock: A deep learning approach to semantic data type detection. In 25th ACM\nSIGKDD, pages 1500–1508, 2019.\n[75] V.-P. Huynh, Y. Chabot, T. Labb´e, J. Liu, and R. Troncy. From heuristics to language models: A\njourney through the universe of semantic table interpretation with dagobah. SemTab, 2022.\n[76] V.-P. Huynh, Y. Chabot, and R. Troncy. Towards generative semantic table interpretation. In\nVLDB Workshops, 2023.\n[77] V.-P. Huynh, J. Liu, Y. Chabot, F. Deuz´e, T. Labb´e, P. Monnin, and R. Troncy. Dagobah: Table\nand graph contexts for efficient semantic annotation of tabular data. In SemTab@ ISWC, pages\n19–31, 2021.\n[78] V.-P. Huynh, J. Liu, Y. Chabot, T. Labb´e, P. Monnin, and R. Troncy. Dagobah: Enhanced scoring\nalgorithms for scalable annotations of tabular data. In SemTab@ ISWC, pages 27–39, 2020.\n[79] F. Ilievski, E. Hovy, P. Vossen, S. Schlobach, and Q. Xie. The role of knowledge in determining\nidentity of long-tail entities. Journal of Web Semantics, 61-62:100565, 2020.\n[80] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu. TinyBERT: Distilling\nBERT for natural language understanding. In T. Cohn, Y. He, and Y. Liu, editors, Findings of the\nAssociation for Computational Linguistics: EMNLP 2020, pages 4163–4174, Online, Nov. 2020.\nAssociation for Computational Linguistics.\n[81] E. Jim´enez-Ruiz, O. Hassanzadeh, V. Efthymiou, J. Chen, and K. Srinivas. Semtab 2019: Resources\nto benchmark tabular data to knowledge graph matching systems. In The Semantic Web, pages\n514–530, Cham, 2020.\n[82] E. Jim´enez-Ruiz, O. Hassanzadeh, V. Efthymiou, J. Chen, and K. Srinivas. Semtab 2019: Resources\nto benchmark tabular data to knowledge graph matching systems. In The Semantic Web, pages\n514–530, Cham, 2020. Springer.\n[83] E. Jimenez-Ruiz, O. Hassanzadeh, V. Efthymiou, J. Chen, K. Srinivas, and V. Cutrona. Results\nof semtab 2020. CEUR Workshop Proceedings, 2775:1–8, January 2020.\n[84] A. Jiomekong, C. Etoga, B. Foko, V. Tsague, M. Folefac, S. Kana, M. M. Sow, and G. Camara. A\nlarge scale corpus of food composition tables. SemTab, CEUR-WS. org, 2022.\n[85] E. Kacprzak, J. M. Gim´enez-Garc´ıa, A. Piscopo, L. Koesten, L.-D. Ib´a˜nez, J. Tennison, and\nE. Simperl. Making sense of numerical data-semantic labelling of web tables. In EKAW, Nancy,\nFrance, November 12-16, 2018, pages 163–178.\n[86] M. Kejriwal, C. A. Knoblock, and P. Szekely. Knowledge graphs: Fundamentals, techniques, and\napplications. 2021.\n[87] J. D. M.-W. C. Kenton and L. K. Toutanova. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. In Proceedings of naacL-HLT, volume 1, page 2, 2019.\n[88] P. Keshvari-Fini, B. Janfada, and B. Minaei-Bidgoli. A survey on knowledge extraction techniques\nfor web tables. In 2019 5th International Conference on Web Research (ICWR), pages 123–127,\n2019.\n[89] U. Khurana and S. Galhotra. Semantic annotation for tabular data. CIKM: Proceedings of the 30th\nACM International Conference on Information & Knowledge Management, page 844–853, 2021.\n[90] U. Khurana and S. Galhotra. Semantic annotation for tabular data, U.S. Patent US20230161774A1,\n2021-11-24.\n32\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n[91] D. Kim, H. Park, J. K. Lee, W. Kim, E. Jim´enez-Ruiz, O. Hassanzadeh, V. Efthymiou, J. Chen,\nand K. Srinivas. Generating conceptual subgraph from tabular data for knowledge graph matching.\nIn SemTab@ ISWC, pages 96–103, 2020.\n[92] R. Kindermann and J. L. Snell. Markov random fields and their applications. American Mathe-\nmatical Society, 1980.\n[93] T. Knap. Towards odalic, a semantic table interpretation tool in the adequate project. In LD4IE@\nISWC, pages 26–37.\n[94] C. A. Knoblock, P. Szekely, J. L. Ambite, A. Goel, S. Gupta, K. Lerman, M. Muslea, M. Taheriyan,\nand P. Mallick. Semi-automatically Mapping Structured Sources into the Semantic Web, pages 375–\n390. Springer, 2012.\n[95] N. Kolitsas, O.-E. Ganea, and T. Hofmann. End-to-end neural entity linking. In A. Korhonen and\nI. Titov, editors, Proceedings of the 22nd Conference on Computational Natural Language Learning,\npages 519–529, Brussels, Belgium, Oct. 2018. Association for Computational Linguistics.\n[96] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press,\n2009.\n[97] K. Korini, R. Peeters, and C. Bizer. Sotab: The wdc schema. org table annotation benchmark.\nSemantic Web Challenge on Tabular Data to Knowledge Graph Matching (SemTab), CEUR-WS.\norg, 2022.\n[98] B. Kruit, P. Boncz, and J. Urbani. Extracting novel facts from tables for knowledge graph com-\npletion. In The Semantic Web – ISWC 2019, pages 364–381, Cham, 2019. Springer International\nPublishing.\n[99] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for seg-\nmenting and labeling sequence data. 2001. In Proc. 18th International Conf. on Machine Learning,\npages 282–289, 2001.\n[100] J. Lee and K. Toutanova. Pre-training of deep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 3(8), 2018.\n[101] A. Lerer, L. Wu, J. Shen, T. Lacroix, L. Wehrstedt, A. Bose, and A. Peysakhovich. Pytorch-\nbiggraph: A large scale graph embedding system. Proceedings of Machine Learning and Systems,\n1:120–131, 2019.\n[102] P. Li, Y. He, D. Yashar, W. Cui, S. Ge, H. Zhang, D. R. Fainman, D. Zhang, and S. Chaudhuri.\nTable-gpt: Table-tuned gpt for diverse table tasks, 2023.\n[103] Y. Li, J. Li, Y. Suhara, A. Doan, and W.-C. Tan. Deep entity matching with pre-trained language\nmodels. VLDB, 2020.\n[104] Y. Li, W. Shen, J. Gao, and Y. Wang. Community question answering entity linking via lever-\naging auxiliary data. Proceedings of the Thirty-First International Joint Conference on Artificial\nIntelligence Main Track. Pages 2145-2151, 2022.\n[105] G. Limaye, S. Sarawagi, and S. Chakrabarti. Annotating and searching web tables using entities,\ntypes and relationships. Proc. VLDB Endow., 3(1-2):1338–1347, Sept. 2010.\n[106] J. Liu, Y. Chabot, R. Troncy, V.-P. Huynh, T. Labb´e, and P. Monnin. From tabular data to\nknowledge graphs: A survey of semantic table interpretation tasks and methods. Journal of Web\nSemantics, 76:100761, 2023.\n[107] J. Liu, V.-P. Huynh, Y. Chabot, and R. Troncy. Radar station: Using kg embeddings for semantic\ntable interpretation and entity disambiguation. In ISWC 2022, October 23–27, 2022, pages 498–\n515. Springer.\n[108] X. Luo, K. Luo, X. Chen, and K. Q. Zhu. Cross-lingual entity linking for web tables. In AAAI,\n2018.\n33\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n[109] P. Machado, B. Fernandes, and P. Novais. Benchmarking data augmentation techniques for tabular\ndata. In Intelligent Data Engineering and Automated Learning – IDEAL 2022, pages 104–112,\nCham, 2022. Springer International Publishing.\n[110] M. Marzocchi, M. Cremaschi, R. Pozzi, R. Avogadro, and M. Palmonari. Mammotab: a giant and\ncomprehensive dataset for semantic table interpretation. Proceedings of the SemTab2022, 2022.\n[111] S. Mazumdar and Z. Zhang. Visualizing semantic table annotations with tableminer+. In ISWC\n2016 Posters & Demonstrations Track. CEUR Workshop Proceedings, 2016.\n[112] I. Mazurek, B. Wiewel, and B. Kruit. Wikary: A dataset of n-ary wikipedia tables matched to\nqualified wikidata statements. Semantic Web Challenge on Tabular Data to Knowledge Graph\nMatching (SemTab), CEUR-WS. org, 2022.\n[113] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in\nvector space. Proceedings of Workshop at ICLR, 2013, 01 2013.\n[114] H. Morikawa. Semantic table interpretation using lod4all. SemTab@ ISWC, 2019:49–56, 2019.\n[115] E. Mu˜noz, A. Hogan, and A. Mileo. Triplifying wikipedia’s tables. In CEUR Workshop Proceedings,\nLD4IE’13, page 26–37, Aachen, DEU, 2013. CEUR-WS.org.\n[116] V. Mulwad, T. Finin, and A. Joshi. Semantic message passing for generating linked data from\ntables. In The Semantic Web – ISWC 2013, pages 363–378. Springer Berlin Heidelberg, 2013.\n[117] V. Mulwad, T. Finin, Z. Syed, and A. Joshi. T2ld: Interpreting and representing tables as linked\ndata. In ISWC Posters & Demonstrations Track, ISWC-PD’10, pages 25–28, Aachen, Germany,\nGermany, 2010. CEUR-WS.org.\n[118] V. Mulwad, T. W. Finin, and A. Joshi. Automatically generating government linked data from\ntables. In AAAI 2011.\n[119] S. Neumaier, J. Umbrich, J. X. Parreira, and A. Polleres. Multi-level semantic labelling of numerical\nvalues. In The Semantic Web – ISWC 2016, pages 428–445, Cham, 2016. Springer International\nPublishing.\n[120] P. Nguyen, N. Kertkeidkachorn, R. Ichise, and H. Takeda.\nMtab: matching tabular data to\nknowledge graph using probability models. arXiv preprint arXiv:1910.00246, 2019.\n[121] P. Nguyen, K. Nguyen, R. Ichise, and H. Takeda.\nEmbnum: Semantic labeling for numerical\nvalues with deep metric learning. In 8th Joint International Conference, JIST 2018, Awaji, Japan,\nNovember 26–28, 2018, pages 119–135, 2018.\n[122] P. Nguyen, I. Yamada, N. Kertkeidkachorn, R. Ichise, and H. Takeda. Mtab4wikidata at semtab\n2020: Tabular data annotation with wikidata. SemTab@ ISWC, 2775:86–95, 2020.\n[123] P. Nguyen, I. Yamada, N. Kertkeidkachorn, R. Ichise, and H. Takeda. Semtab 2021: Tabular data\nannotation with mtab tool. In SemTab@ ISWC, pages 92–101, 2021.\n[124] D. Oliveira and M. d’Aquin. Adog-annotating data with ontologies and graphs. SemTab@ ISWC,\n2019:1–6, 2019.\n[125] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\nAdvances in neural information processing systems, 35:27730–27744, 2022.\n[126] M. Palmonari, M. Ciavotta, F. De Paoli, A. Koˇsmerlj, and N. Nikolov. Ew-shopp project: Support-\ning event and weather-based data analytics and marketing along the shopper journey. In Advances\nin Service-Oriented and Cloud Computing, pages 187–191, Cham, 2020. Springer International\nPublishing.\n[127] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu. Unifying large language models and\nknowledge graphs: A roadmap. IEEE Transactions on Knowledge and Data Engineering, 2024.\n34\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n[128] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In\nProceedings of the 2014 conference on empirical methods in natural language processing (EMNLP),\npages 1532–1543, 2014.\n[129] M. Pham, S. Alse, C. A. Knoblock, and P. Szekely. Semantic Labeling: A Domain-Independent\nApproach, pages 446–462. Springer International Publishing, Cham, 2016.\n[130] M. T. Pilehvar and J. Camacho-Collados. Embeddings in natural language processing: Theory and\nadvances in vector representations of meaning. Morgan & Claypool Publishers, 2020.\n[131] R. Pimplikar and S. Sarawagi. Answering table queries on the web using column keywords. Proc.\nVLDB Endow., 5(10):908–919, June 2012.\n[132] R. Porrini, M. Palmonari, and I. F. Cruz. Facet annotation using reference knowledge bases. In\nProceedings of the 2018 World Wide Web Conference, WWW ’18, page 1215–1224, Republic and\nCanton of Geneva, CHE, 2018. WWW.\n[133] J. Pujara, P. Szekely, H. Sun, and M. Chen. From tables to knowledge: Recent advances in table\nunderstanding. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &\nData Mining, pages 4060–4061, 2021.\n[134] G. Quercini and C. Reynaud. Entity discovery and annotation in tables. In Proceedings of the\n16th International Conference on Extending Database Technology, EDBT ’13, pages 693–704, New\nYork, NY, USA, 2013. ACM.\n[135] S. Ramnandan, A. Mittal, C. A. Knoblock, and P. Szekely. Assigning Semantic Labels to Data\nSources, pages 403–417.\n[136] P. Ristoski and H. Paulheim. Rdf2vec: Rdf graph embeddings for data mining. In The Semantic\nWeb–ISWC 2016, Kobe, Japan, October 17–21, 2016, Proceedings, Part I 15, pages 498–514.\nSpringer, 2016.\n[137] D. Ritze and C. Bizer. Matching web tables to dbpedia-a feature utility study. context, 42(41):19–\n31, 2017.\n[138] D. Ritze, O. Lehmberg, and C. Bizer. Matching html tables to dbpedia. In 5th International\nConference on Web Intelligence, Mining and Semantics, WIMS ’15, pages 10:1–10:6, New York,\nNY, USA, 2015. ACM.\n[139] D. Roman, M. Dimitrov, N. Nikolov, A. Putlier, D. Sukhobok, B. Elvesæter, A. Berre, X. Ye,\nA. Simov, and Y. Petkov. Datagraft: Simplifying open data publishing. In The Semantic Web,\npages 101–106, Cham, 2016. Springer.\n[140] D. Roman, N. Nikolov, A. Putlier, D. Sukhobok, B. Elvesæter, A. Berre, X. Ye, M. Dimitrov,\nA. Simov, M. Zarev, et al. Datagraft: One-stop-shop for open data management. Semantic Web,\n9(4):393–411, 2018.\n[141] C. Sarthou-Camy, G. Jourdain, Y. Chabot, P. Monnin, F. Deuz´e, V.-P. Huynh, J. Liu, T. Labb´e,\nand R. Troncy.\nDagobah ui: A new hope for semantic table interpretation.\nIn ESWC 2022:\nHersonissos, Crete, Greece, page 107–111, 2022.\n[142] Y. A. Sekhavat, F. Di Paolo, D. Barbosa, and P. Merialdo. Knowledge base augmentation using\ntabular data. In LDOW.\n[143] R. Shigapov, P. Zumstein, J. Kamlah, L. Oberl¨ander, J. Mechnich, and I. Schumm. bbw: Matching\ncsv to wikidata via meta-lookup. In CEUR Workshop Proceedings, volume 2775, pages 17–26.\nRWTH, 2020.\n[144] S. Singh, A. F. Aji, G. S. Tomar, and C. Christodoulopoulos. Redtable: A relation extraction\ndataset for knowledge extraction from web tables. In 29th International Conference on Computa-\ntional Linguistics, pages 2319–2327, 2022.\n[145] B. Spahiu, R. Porrini, M. Palmonari, A. Rula, and A. Maurino. Abstat: ontology-driven linked\ndata summaries with pattern minimalization.\nIn ESWC 2016, Heraklion, Crete, Greece, May\n29–June 2, 2016, pages 381–395.\n35\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n[146] B. Steenwinckel, F. De Turck, and F. Ongenae. Magic: Mining an augmented graph using ink,\nstarting from a csv. In SemTab@ ISWC, pages 68–78, 2021.\n[147] B. Steenwinckel, G. Vandewiele, F. De Turck, and F. Ongenae. Csv2kg: Transforming tabular\ndata into semantic knowledge. SemTab, ISWC Challenge, 2019.\n[148] B. Steenwinckel, G. Vandewiele, M. Weyns, T. Agozzino, F. D. Turck, and F. Ongenae.\nInk:\nknowledge graph embeddings for node classification. x, 36(2):620–667, 2022.\n[149] Y. Suhara, J. Li, Y. Li, D. Zhang, C¸. Demiralp, C. Chen, and W.-C. Tan. Annotating columns with\npre-trained language models. In Proceedings of the 2022 International Conference on Management\nof Data, pages 1493–1503, 2022.\n[150] Z. Sun, Z.-H. Deng, J.-Y. Nie, and J. Tang. Rotate: Knowledge graph embedding by relational\nrotation in complex space. arXiv preprint arXiv:1902.10197, 2019.\n[151] Z. Syed, T. Finin, V. Mulwad, and A. Joshi. Exploiting a web of semantic data for interpreting\ntables. In Proceedings of the Second Web Science Conference, volume 5, 2010.\n[152] M. Taheriyan, C. A. Knoblock, P. Szekely, and J. L. Ambite. A scalable approach to learn semantic\nmodels of structured sources. In IEEE Computer Society, ICSC ’14, page 183–190, USA, 2014.\nIEEE Computer Society.\n[153] M. Taheriyan, C. A. Knoblock, P. Szekely, and J. L. Ambite. Learning the semantics of structured\ndata sources. Web Semantics: Science, Services and Agents on the World Wide Web, 37–38:152 –\n169, 2016.\n[154] M. Taheriyan, C. A. Knoblock, P. Szekely, and J. L. Ambite. Leveraging linked data to discover\nsemantic relations within data sources. In The Semantic Web – ISWC 2016, pages 549–565, Cham,\n2016. Springer International Publishing.\n[155] K. Takeoka, M. Oyamada, S. Nakadai, and T. Okadome. Meimei: An efficient probabilistic ap-\nproach for semantically annotating tables. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 33, pages 281–288, 2019.\n[156] P. Tallet. Les Papyrus de la Mer Rouge I: Le Journal de Merer. Institut Francais D’Archeologie\nOrientale, 2017.\n[157] C. Tao and D. W. Embley. Automatic hidden-web table interpretation, conceptualization, and\nsemantic annotation. Data & Knowledge Engineering, 68(7):683 – 703, 2009.\n[158] A. Thawani, M. Hu, E. Hu, H. Zafar, N. T. Divvala, A. Singh, E. Qasemi, P. A. Szekely, and\nJ. Pujara. Entity linking to knowledge graphs to infer column types and properties. SemTab@\nISWC, 2019:25–32, 2019.\n[159] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint\narXiv:2302.13971, 2023.\n[160] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\nR. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.\nLachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n[161] S. Tyagi and E. Jimenez-Ruiz. Lexma: Tabular data to knowledge graph matching using lexical\ntechniques. In CEUR Workshop Proceedings, volume 2775, pages 59–64, 2020.\n[162] P. Venetis, A. Halevy, J. Madhavan, M. Pa¸sca, W. Shen, F. Wu, G. Miao, and C. Wu. Recovering\nsemantics of tables on the web. Proc. VLDB Endow., 4(9):528–538, June 2011.\n36\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n[163] D. Wang, P. Shiralkar, C. Lockard, B. Huang, X. L. Dong, and M. Jiang. Tcn: table convolutional\nnetwork for web table interpretation. In Proceedings of the Web Conference 2021, pages 4020–4032,\n2021.\n[164] J. Wang, H. Wang, Z. Wang, and K. Q. Zhu. Understanding tables on the web. In Proceedings of\nthe 31st International Conference on Conceptual Modeling, ER’12, pages 141–155. Springer-Verlag,\n2012.\n[165] Z. Wang, J. Huang, H. Li, B. Liu, B. Shao, H. Wang, J. Wang, Y. Wang, W. Wu, J. Xiao, and\nK. Zhu. Probase: a universal knowledge base for semantic search. Microsoft Research Asia, 05\n2011.\n[166] G. Weikum, X. L. Dong, S. Razniewski, and F. M. Suchanek. Machine knowledge: Creation and\ncuration of comprehensive knowledge bases. Found. Trends Databases, 10(2-4):108–490, 2021.\n[167] R. E. Wright. Logistic regression. Reading and understanding multivariate statistics, pages 217–244,\n1995.\n[168] M. Yakout, K. Ganjam, K. Chakrabarti, and S. Chaudhuri. Infogather: entity augmentation and\nattribute discovery by holistic matching with web tables. In Proceedings of the 2012 ACM SIGMOD\nInternational Conference on Management of Data, pages 97–108, 2012.\n[169] I. Yamada, A. Asai, J. Sakuma, H. Shindo, H. Takeda, Y. Takefuji, and Y. Matsumoto.\nWikipedia2vec: An efficient toolkit for learning and visualizing the embeddings of words and\nentities from wikipedia. arXiv:1812.06280, 2018.\n[170] L. Yang, S. Shen, J. Ding, and J. Jin. Gbmtab: A graph-based method for interpreting noisy\nsemantic table to knowledge graph. In SemTab@ ISWC, pages 32–41, 2021.\n[171] P. Yin, G. Neubig, W.-t. Yih, and S. Riedel. Tabert: Pretraining for joint understanding of textual\nand tabular data. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pages 8413–8426, 2020.\n[172] S. Yumusak, E. Jim´enez-Ruiz, O. Hassanzadeh, V. Efthymiou, J. Chen, and K. Srinivas. Knowledge\ngraph matching with inter-service information transfer. In SemTab@ ISWC, pages 104–108, 2020.\n[173] R. Zanibbi, D. Blostein, and J. R. Cordy. A survey of table recognition: Models, observations,\ntransformations, and inferences. Document Analysis and Recognition, 7:1–16, 2004.\n[174] D. Zhang, Y. Suhara, J. Li, M. Hulsebos, C¸. Demiralp, and W.-C. Tan. Sato: Contextual semantic\ntype detection in tables. Proceedings of the VLDB Endowment, 13, 2020.\n[175] M. Zhang and K. Chakrabarti. Infogather+: Semantic matching and annotation of numeric and\ntime-varying attributes in web tables. In ACM SIGMOD International Conference on Management\nof Data, page 145–156, 2013.\n[176] S. Zhang and K. Balog. Ad hoc table retrieval using semantic similarity. In International World\nWide Web Conference, WWW ’18, page 1553–1562, Republic and Canton of Geneva, CHE, 2018.\n[177] S. Zhang and K. Balog. Web table extraction, retrieval, and augmentation: A survey. ACM Trans.\nIntell. Syst. Technol., 11(2), jan 2020.\n[178] S. Zhang, E. Meij, K. Balog, and R. Reinanda.\nNovel entity discovery from web tables.\nIn\nProceedings of The Web Conference 2020, WWW ’20, page 1298–1308, New York, NY, USA, 2020.\nAssociation for Computing Machinery.\n[179] T. Zhang, X. Yue, Y. Li, and H. Sun.\nTablellama: Towards open large generalist models for\ntables. In Proceedings of NAACL: Human Language Technologies (Volume 1: Long Papers), pages\n6024–6044, 2024.\n[180] Z. Zhang. Effective and efficient semantic table interpretation using tableminer+. Semantic Web,\n8(6):921–957, 2017.\n37\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n[181] Y. Zhou, S. Singh, and C. Christodoulopoulos. Tabular data concept type detection using star-\ntransformers. In 30th ACM International Conference on Information & Knowledge Management,\npages 3677–3681, 2021.\n[182] Z.-H. Zhou and Z.-Q. Chen. Hybrid decision tree. Knowledge-based systems, 15(8):515–528, 2002.\n[183] S. Zwicklbauer, C. Einsiedler, M. Granitzer, and C. Seifert. Towards disambiguating web tables.\nIn ISWC (Posters & Demos), pages 205–208, 2013.\n38\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nA\nScope and methodology\nA.1\nMethodology\nIdentification\nIn order to enhance the efficiency of our search in publication databases, the authors\ncollaboratively defined and established a set of keywords. The set of keywords is composed of semantic\ntable interpretation, table understanding, STI, table interpretation, semantic table analysis, semantic\ntable exploration, semantic table understanding, web tables, semantic annotation of tabular data, tabular\ndata annotation, table annotation, semantic interpretation of structured data, tabular data semantic\nlabelling, tabular data enrichment, SemTab challenge, or simply tabular data. Finally, we came up with\n16 keywords. Subsequently, these keywords underwent in-depth discussions among five researchers, who\nassigned scores ranging from 1 (denoting low relevance) to 5 (denoting high relevance) to each keyword.\nThe final score for each keyword is calculated as the average of the scores provided by each researcher.\nFinally, the list of ranked keywords represented a starting point for an extensive search on several\npublication platforms. The following search platforms for scientific publications were utilised: i) Scopus,\nii) Web of Science, iii) DBLP, and iii) Google Scholar.\nThe time period was set from 2007, when the STI research field was first approached, until May 2023,\nwhen the paper collection process was completed. To complement the extensive search, we incorporated\na snowballing technique, which involved exploring additional recent publications that referenced the key\nworks identified within our result corpus. Tracing the citations of central works aimed to capture the\nfield’s most up-to-date and relevant literature.\nScreening\nTwo experts manually annotated the papers obtained during the Identification step. A\nkey aspect of the screening process was identifying which semantic table interpretation phases were\naddressed/described in each publication. In addition, the criteria for this Screening step was the relevance\nand comprehensiveness of each publication regarding the STI tasks. These criteria were employed to\nassess how the publications addressed the relevant aspects of STI and comprehensively treated the subject\nmatter. As an additional step, we performed an annotation process with pre-defined categories based on\neach publication’s title, abstract and keywords. If the categorisation based on these three components was\nimpossible, the full text had to be consulted at this stage. The categories for this final step were divided\ninto generic tags (e.g., “semantic table interpretation”, and “gold standard”) and specific annotation\ntags (e.g., “supervised”, “domain independent” Section 6).\nInclusion\nThis Section describes our methods for identifying the final subset of publications to be\nincluded in this survey. The first and foremost criteria for inclusion were that publications had to be:\ni) directly related to semantic table interpretation, ii) published in English, and iii) peer-reviewed. All\nthe paper’s authors decided on which publications to report based on their relevance to the assigned\ncategory.\nResults of the paper collection process\nThrough the keywords mentioned above (Section A.1),\nabout 134 papers were grouped; this set further decreased the number to 111 publications after the\nScreening stage (Section A.1), removing unrelated or duplicate publications. This manual annotation\nfirst involves assessing whether a paper is relevant (1), not relevant (0) or the annotator is unsure about\nits relevance (2). In the latter case, a third annotator would determine whether to include the publication.\nThis detailed screening stage led to the exclusion of 17 more papers, 2 of which were superseded by newer\npublications by the same authors, and 8 were finally deemed not closely related to the STI. Therefore, 88\napproaches were discussed in the following survey, each one described in one or more publications. For\nthis survey’s scope, as discussed in Section 3, we identified several criteria for comparing STI approaches.\nFig. 8 summarises the distribution of approaches in conferences and journals; from this analysis, it can\nbe deduced that the STI involves multiple research communities like Semantic Web, Data Management,\nAI , and NLP. Fig. 9 shows a graph with the cross-references between the articles40. For some approaches\nit is indicated whether they are derived from previously published versions.\n40An\nonline\ninteractive\nvisualisation\nof\nthe\ncross-references\nchart\nis\navailable\nat\nobservablehq.com/@elia-guarnieri-ws/cross-reference.\nA\ntabular\nrepresentation\nis\navailable\nat\npub-\nlic.tableau.com/app/profile/marco.cremaschi/viz/ChallengesandDirectionintheAnnotationofTabularData/Crossreference\n39\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\n1\nEDBT\nDKE\n27\n10\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n3\n3\n3\n4\n4\n6\n1\nSemTab\nISWC\nVLDB\nWWW\nAAAI\nSIGMOD\nESWC\nLD4IE\nKGC\nJOWS\nIEEE TKDE\nIEEE ICSC\nEKAW\nCIKM\nWSC\nWISE\nWISA\nWIMS\nSIGSPATIAL\nSIGKDD\nSDS\nLDOW\nIJCAI\nICAI\nI-SEMANTICS\nFGCS\nER\n1\nFigure 8: Number of approaches for each conference or journal. The extended version of the acronyms\nis shown in Table 7 in Appendix D.\nFigure 9: A cross-reference chart for the analysed papers.\nTable 2 provides a detailed comparison of all the approaches analysed.\n40\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nYEAR\nAUTHOR\nMETHOD\nPUBLICATION\nCTA\nCPA\nCEA\nCNEA\nINDEX\nCODE\nLICENCE\nTRIPLE STORE\n2007\nHignette et al. [70]\nUnsup\nWISE\n—\n—\nPersonal ontologies\n2009\nHignette et al. [71]\nUnsup\nESWC\n—\n—\nPersonal ontologies\n2009\nTao et al. [157]\nUnsup\nDKE\n—\n—\nPersonal ontologies\n2010\nLimaye et al. [105]\nUnsup\nVLDB\n—\n—\nYago\n2010\nMulwad et al. [117]\nSup\nISWC\n—\n—\nWikitology\n2010\nSyed et al. [151]\nUnsup\nWSC\nLucene for concepts\n—\nWikitology\n2011\nMulwad et al. [118]\nSup\nAAAI\n—\n—\nDBpedia,Freebase,WordNet,Yago\n2011\nVenetis et al. [162]\nUnsup\nVLDB\n—\n—\nYago\n2012\nGoel et al. [63]\nSup\nICAI\n—\n—\n—\n2012\nKnoblock et al. [94]\nSup\nESWC\n—\nApache 2.0\nPersonal ontologies\n2012\nPimplikar et al. [131]\nUnsup\nVLDB\n—\n—\n—\n2012\nWang et al. [164]\nUnsup\nER\n—\n—\n—\n2013\nBuche et al. [22]\nUnsup\nIEEE\n—\n—\n—\n2013\nCruz et al. [43]\nSup\nSIGSPATIAL\n—\n—\n—\n2013\nDeng et al. [49]\nUnsup\nVLDB\n—\n—\nDBpedia,Freebase,Yago\n2013\nErmilov et al. [57]\nUnsup\nI-SEMANTICS\n—\n—\n—\n2013\nMulwad et al. [116]\nSup\nISWC\n—\n—\nDBpedia,Yago,Wikitology\n2013\nMunoz et al. [115]\nUnsup\nLD4IE\n—\n—\nDBpedia\n2013\nQuercini et al. [134]\nUnsup\nEDBT\n—\nDBpedia\n2013\nZhang et al. [175]\nUnsup\nSIGMOD\n—\n—\n—\n2013\nZwicklbauer et al. [183]\nUnsup\nISWC\n—\n—\nDBpedia\n2014\nSekhavat et al. [142]\nUnsup\nLDOW\n—\n—\nYago\n2014\nTaheriyan et al. [152]\nUnsup\nIEEE\n—\n—\n—\n2015\nBhagavatula et al. [19]\nSup\nISWC\n—\nCCA 4.0\nYago\n2015\nRamnandan et al. [135]\nSup\nESWC\ntraining data with Lucene, not KG data\nApache 2.0\n—\n2015\nRitze et al. [138]\nUnsup\nWIMS\n—\nApache 2.0\nDBpedia\n2016\nErmilov et al. [58]\nUnsup\nEKAW\n—\nGPL 3.0\nDBpedia\n2016\nNeumaier et al. [119]\nSup\nISWC\n—\nApache 2.0\nDBpedia\n2016\nPham et al. [129]\nSup\nISWC\n—\nApache 2.0\n—\n2016\nTaheriyan et al. [154]\nSup\nJOWS\n—\nApache 2.0\nCIDOC-CRM,EDM\n2016\nTaheriyan et al. [153]\nSup\nISWC\n—\nApache 2.0\nCIDOC-CRM\n2017\nEfthymiou et al. [55]\nHybrid\nISWC\n—\n—\n—\n2017\nEll et al. [56]\nUnsup\nLD4IE\nLabels + literals\nApache 2.0\nDBpedia\n2017\nZhang et al. [180]\nUnsup\nJOWS\n—\nApache 2.0\nFreebase\n2018\nKacprzak et al. [85]\nUnsup\nEKAW\n—\nMIT\nDBpedia\n2018\nLuo et al. [108]\nSup\nAAAI\n—\n—\nWikipedia\n2018\nZhang et al. [176]\nUnsup\nWWW\n—\n—\n—\n2019\nChabot et al. [26]\nUnsup\nSemTab\n—\nOrange\nDBpedia\n2019\nChen et al. [27]\nHybrid\nAAAI\n—\nApache 2.0\nDBpedia\n2019\nChen et al. [27]\nUnsup\nIJCAI\n—\nApache 2.0\nDBpedia\n2019\nCremaschi et al. [35]\nUnsup\nSemTab\n—\nApache 2.0\nDBpedia\n2019\nHulsebos et al. [74]\nSup\nSIGKDD\n—\nMIT\nDBpedia\n2019\nKruit et al. [98]\nHybrid\nISWC\n—\nMIT\nDBpedia,Wikidata\n2019\nMorikawa et al. [114]\nUnsup\nSemTab\nElasticsearch\n—\nDBpedia\n2019\nNguyen et al. [120]\nUnsup\nSemTab\n—\n—\nDBpedia\n2019\nOliveira et al. [124]\nUnsup\nSemTab\nArangoDB + Elasticsearch\n—\nDBpedia\n2019\nSteenwinckel et al. [147]\nUnsup\nSemTab\n—\n—\nDBpedia\n2019\nTakeoka et al. [155]\nSup\nAAAI\n—\n—\nWordNet\n2019\nThawani et al. [158]\nUnsup\nSemTab\nElasticsearch\nMIT\n—\n2019\nZhang et al. [174]\nSup\nVLDB\n—\nApache 2.0\nDBpedia\n2020\nAbdelmageed et al. [1]\nUnsup\nSemTab\n—\nMIT\nWikidata\n2020\nAzzi et al. [12]\nUnsup\nSemTab\n—\n—\nWikidata\n2020\nBaazouzi et al. [14]\nUnsup\nSemTab\n—\n—\nWikidata\n41\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nYEAR\nAUTHOR\nMETHOD\nPUBLICATION\nCTA\nCPA\nCEA\nCNEA\nINDEX\nCODE\nLICENCE\nTRIPLE STORE\n2020\nChen et al. [29]\nUnsup\nSemTab\nElasticsearch\n—\nWikidata\n2020\nCremaschi et al. [38]\nUnsup\nFGCS\n—\nApache 2.0\nDBpedia\n2020\nCremaschi et al. [34]\nUnsup\nSemTab\nLamAPI\nApache 2.0\nDBpedia,Wikidata\n2020\nEslahi et al. [59]\nUnsup\nSDS\n—\n—\nWikidata\n2020\nGuo et al. [65]\nSup\nWISA\n—\n—\n—\n2020\nHuynh et al. [78]\nHybrid\nSemTab\nSpark dataframes\n—\nWikidata\n2020\nKhurana et al. [89]\nSup\nCIKM\n—\n—\n—\n2020\nKim et al. [91]\nUnsup\nSemTab\n—\n—\nWikidata\n2020\nLi et al. [103]\nSup\nVLDB\n—\nApache 2.0\n—\n2020\nNguyen et al. [122]\nUnsup\nSemTab\nHashTable + Sparse Matrix\n—\nWikidata\n2020\nShigapov et al. [143]\nUnsup\nSemTab\nSeerX metasearch API\nMIT\nWikidata\n2020\nTyagi et al. [161]\nUnsup\nSemTab\n—\n—\nWikidata\n2020\nYumusak et al. [172]\nUnsup\nSemTab\n—\n—\nWikidata\n2020\nZhang et al. [178]\nSup\nWWW\n—\nCCA 4.0\nDBpedia\n2021\nAbdelmageed et al. [3]\nUnsup\nSemTab\n—\nApache 2.0\nDBpedia,Wikidata\n2021\nAbdelmageed et al. [2]\nUnsup\nKGC\n—\nApache 2.0\nDBpedia,Wikidata\n2021\nAvogadro et al. [9]\nUnsup\nSemTab\nLamAPI\nApache 2.0\nDBpedia,Wikidata\n2021\nBaazouzi et al. [13]\nUnsup\nSemTab\n—\n—\nWikidata\n2021\nHeist et al. [69]\nHybrid\nWWW\n—\nGPL 3.0\nCaliGraph,DBpedia,Yago\n2021\nHuynh et al. [77]\nHybrid\nSemTab\nElasticsearch\nOrange\nDBpedia,Wikidata\n2021\nNguyen et al. [123]\nUnsup\nSemTab\nCustom BM25\nMIT\nDBpedia,Wikidata\n2021\nSteenwinckel et al. [146]\nHybrid\nSemTab\n—\nImec license\nWikidata\n2021\nWang et al. [163]\nSup\nWWW\n—\n—\n—\n2021\nYang et al. [170]\nSup\nSemTab\n—\n—\nWikidata\n2021\nZhou et al. [181]\nSup\nCIKM\n—\n—\n—\n2022\nAbdelmageed et al. [4]\nUnsup\nKGC\n—\nApache 2.0\nDBpedia,Wikidata\n2022\nChen et al. [30]\nUnsup\nJWS\nElasticsearch\nMIT\nDBpedia,Wikidata\n2022\nCremaschi et al. [36]\nUnsup\nSemTab\nLamAPI\nApache 2.0\nDBpedia,Wikidata\n2022\nDeng et al. [50]\nSup\nSIGMOD\n—\nApache 2.0\n—\n2022\nGottschalk et al. [64]\nSup\nSWJ\n—\nMIT\n—\n2022\nHuynh et al. [75]\nHybrid\nSemTab\nElasticsearch\nOrange\nDBpedia,Wikidata\n2022\nLiu et al. [107]\nHybrid\nISWC\n—\nOrange\nWikidata\n2022\nSuhara et al. [149]\nSup\nSIGMOD\n—\nApache 2.0\nFreebase,DBpedia\n2024\nZhang et al. [179]\nSup\narXiv\n—\nMIT\nWikidata\nTable 2: Comparison table.\n42\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nA.2\nTechniques for Supervised and Unsupervised approaches\nTable 3 and Table 4 display the techniques used by unsupervised and supervised approaches analysed in\nthis survey.\n43\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nTable 3: List of unsupervised techniques and related approaches.\nApproach\nCandidate Generation\nEntity Disambiguation\nLimaye 2010 [105]\nYAGO catalog\nsimilarity\nSyed 2010 [151]\nWikitology\nCTA\nWang 2012 [164]\npattern matching\nfeatures\nMunoz 2013 [115]\n-\nredirects\nRitze 2015 [138]\nDBpedia lookup service\nCTA\nEll 2017 [56]\ncustom index\nfeatures\nZhang 2017 [180]\nexternal lookup\nsimilarity\nZhang 2018 [176]\nSPARQL\nentity embedding\nCremaschi 2019 [35]\nSPARQL\nsimilarity\nMorikawa 2019 [114]\nSPARQL, Elasticsearch\nCTA\nNguyen 2019 [120]\nDBpedia lookup service, DBpedia endpoint, Wikipedia API, Wikidata API\nCTA\nOliveira 2019 [124]\nElasticsearch\nsimilarity\nSteenwinckel 2019 [147]\nDBpedia lookup service, DBpedia urls, DBpedia Spotlight\nsimilarity\nThawani 2019 [158]\nWikidata API, Elasticsearch\nsimilairty, CTA, ML\nAbdelmageed 2020 [1]\nWikidata lookup service\nCTA, CPA\nAzzi 2020 [12]\nWikidata API\nCTA\nChen 2020 [29]\nMediawiki API, Elasticsearch\nCTA, CPA\nCremaschi 2020-1 [38]\nSPARQL\nsimilarity\nCremaschi 2020-2 [34]\nElasticsearch\nCTA, CPA\nKim 2020 [91]\nSPARQL\nfeatures\nNguyen 2020 [122]\ncustom index\nCPA\nShigapov 2020 [143]\nSearX, SPARQL, Wikibooks, Wikipedia API, Wikidata API\nsimilarity\nTyagi 2020 [161]\nWikidata lookup service, DBpedia lookup service\nsimilarity\nAbdelmageed 2021-1 [3]\nWikidata lookup service, SPARQL\nsimilarity\nAbdelmageed 2021-2 [2]\nWikidata lookup service, SPARQL\nsimilarity\nAvogadro 2021 [9]\ncustom index\nsimilarity, CTA, CPA\nBaazouzi 2021 [13]\nSPARQL\nCTA\nNguyen 2021 [123]\ncustom index\nCPA\nAbdelmageed 2022 [4]\nSPARQL, Wikidata lookup service\nsimilarity\nChen 2022 [30]\nElasticsearch\nsimilarity, CTA, CPA\nCremaschi 2022 [36]\nElasticsearch\nsimilarity, CPA, CTA\n44\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nTable 4: List of supervised techniques and related approaches.\nTechnique\nApproaches\nSVM classifier [33]\nMulwad 2010 [117]\nQuercini 2013 [134]\nErmilov 2016 [58]\nNaive Bayes\nQuercini 2013 [134]\nBinary classifier\nZhang 2020 [178]\nCRF classifier [99]\nGoel 2012 [63]\nKnoblock 2012 [94]\nZhang 2019 [174]\nRamnandan 2015 [135]\nGuo 2020 [65]\nMarkov Network [92]\nMulwad 2011 [117]\nMulwad 2013 [116]\nZhang 2013 [175]\nBhagavatula 2015 [19]\nTakeoka 2019 [155]\nProbabilistic Graphical Models (PGMs) [96]\nZhang 2013 [175]\nErmilov 2016 [58]\nKruit 2019 [98]\nYang 2021 [170]\nHierarchical clustering\nNeumaier 2016 [119]\nLogistic Regression [167]\nPham 2016 [129]\nMulti-Layer Neural Network\nLuo 2018 [108]\nHulsebos 2019 [74]\nZhang 2019 [174]\nZhou 2021 [181]\nGuo 2020 [65]\nCNN [61]\nChen 2019 [27]\nChen 2019 2 [28]\nGuo 2020 [65]\nWang 2021 [163]\nHybrid Decision Tree (HDT) embeddings [182]\nSteenwinckel\n2021 [146]\nWord2vec embeddings [113]\nZhang 2018 [176]\nZhang 2020 [178]\nDeng 2022 [50]\nTransE [54] and RotatE [150] embeddings\nLiu 2022 [107]\nSiamese Networks\nGottschalk 2022 [64]\nLanguage model (BERT) [87]\nLi 2020 [103]\nDeng 2022 [50]\nSuhara 2022 [149]\nLatent Dirichlet Allocation (LDA) [20]\nKhurana 2020 [89]\nLarge Language Models (LLama 2 - 7B) [160]\nTableLLama\n2024 [179]\n45\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nApache 2.0\nMIT\nOrange\nGPL 3.0\nCCA 4.0\nImec license\n23\n1\n2\n2\n4\n9\nImec \nFigure 10: The distribution of the licenses adopted by STI approaches.\nB\nSTI tools\nB.1\nTools analysis\nThis Section analyses the tools that support STI. Several tools can support table interpretation by\nproviding data visualisation, manipulation, and statistical analysis features. This further analysis aims\nto gather information on the tools available for interpreting a table, or more generally, of a structured\ndata source, to identify their features, strengths, and limitations and assess their suitability for different\nuse cases.\nKarma41 [66] is an Open Source (Apache license 2.0) information integration tool that allows users\nto integrate data coming from different sources quickly and easily.\nSuch sources include databases,\nspreadsheets, delimited text files, XML, JSON, CSV and Web API. Users can leverage different ontologies\nto annotate their data with standard vocabularies, ensuring accurate integration. Karma provides a\nresponsive interface, fast processing, and batch mode for large datasets.\nAdditionally, it offers data\ntransformation scripts to convert data into a common format. A demonstration video42 is available.\nTableMiner+ [111] consists mainly of two components: a Java library that implements the homonym\napproach [180] and an extension that constitutes a user-friendly UI for the semantic annotation of Web\ntables.\nThe current version of the tool corresponds to the alpha 1.0 development phase.\nAlthough\nthe source code is available43, the use is limited as the queries refer to Freebase. Even after applying\nmodifications to exclude calls to the Freebase API and refer to DBpedia instead, an HTTPException\nerror with code 500 prevented the STI phase. Such an error probably indicates an incorrect formulation\nof the SPARQL queries within the TableMiner+ algorithm. Consequently, the analysis is based on the\ninformation provided in the paper [111].\nThe MAGIC tool44 [146] supports users to annotate data by following a structured pipeline to augment\nthe semantics of a given table and provides a user-friendly graphical interface for column augmentation.\nHowever, it is important to note that the GUI lacks feedback on the produced annotations during table\nprocessing. The Instance Neighboring using Knowledge (INK) embedding technique can also enrich the\ntable with information from the same dataset, semantically enriching the overall dataset with external\nlinked data. A demonstration video of the MAGIC tool is available45.\nThe MTab tool46 [123] is designed to automatically annotate data using KGs. It enriches the original\ntable data by adding schema and instance-level annotations. The tool supports multilingual tables and\nvarious formats such as Excel, CSV, and markdown tables. The system operates through a series of steps:\npreprocessing the tabular data and then enriching the table with semantic annotations using prediction\nand search functionality. Notably, the system achieved first place in the usability track of the SemTab\nchallenge. It includes a UI that offers features like table upload and an annotate button to initiate\nthe process comprising the mentioned tasks. Additionally, the UI allows users to search for entities in\npopular KGs like Wikidata and DBpedia. However, it is important to note that the search functionality\noperates independently and does not assist users in other aspects of the annotation process. The MTab\n41usc-isi-i2.github.io/karma\n42www.youtube.com/watch?v=h3 yiBhAJIc\n43github.com/ziqizhang/sti\n44github.com/IBCNServices/MAGIC\n45www.youtube.com/watch?v=ZhTKxcTBZNE\n46mtab.app/mtab\n46\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\ntool is accessible solely through an online web interface, with no available source code about the final\nversion47.\nMantisTable tool48 [35] is a user-friendly UI that facilitates the exploration of the annotation steps\nwithin the STI process. Specifically, the tool enables users to visually explore and execute annotations\nthrough all the sub-tasks of the STI process. It features a convenient right sidebar that offers additional\ninformation about each annotation in an info mode and allows manual editing of annotations using\nan edit mode widget. To support the annotation editing process, MantisTable leverages KG summary\nprofiles provided by ABSTAT [145].\nOpenRefine49 is an Open Resource tool able to support different formats such as TSV, CSV, Excel,\nXML, RDF/XML, JSON, N3 and LOG. It offers a workspace with several features, including the ability to\nexport projects in different formats, explore data through filters and faceted exploration, apply clustering\nfor grouping cells, modify cells individually or in groups using transformation rules, modify columns by\nrenaming, deleting, or adding new ones, modify rows by filtering and flagging, display numerical value\ndistributions, and uses extensions for additional functionality. OpenRefine offers functionality to reconcile\nagainst user-edited data on Wikidata or other Wikibase instances or reconcile against a local dataset50.\nTrifacta51 is a collection of software used for data exploration and self-service data preparation for\nanalysis. Trifacta works with cloud and on-premises data platforms. It is designed for analysts to explore,\ntransform, and enrich raw data into clean and structured formats using techniques in ML, data visuali-\nsation, human-computer interaction, and parallel processing for non-technical users to prepare data for\nvarious business processes such as analytics. It is composed of three main products: i) Trifacta Wrangler\n- a connected desktop application used to transform data for downstream analytics and visualisation;\nii) Wrangler Pro - support for large data volumes, deployment options for both cloud and on-premises\nenvironments, and the capability to schedule and operationalise data preparation workflows, iii) Wran-\ngler Enterprises - self-service functionalities to explore and transform data with centralised management\nof security, governance and operationalisation.\nOdalic addresses the limitations of TableMiner+ and is an Open Source tool. The code is available\nin Github52 and it can be easily installed via a Docker image53. It provides a UI for table interpretation,\ndata export as linked data, and results review through user feedback. It supports CSV input and manual\nspecification of relationships between columns. Odalic can work with any KG accessible via SPARQL\nand perform STI using query results from different KG interrogations.\nDataGraft+ASIA54 refers to the integration of ASIA and Datagraft: ASIA is a tool to assist users in\nannotate tables and enrich their content using discovered links [46], and Datagraft [139], a cloud-based\ndata transformation and publishing platform that supports the design and execution of transformations\non tabular data. DataGraft+ASIA refers to the integration of ASIA and Datagraft: ASIA is a tool to\nassist users in annotate tables and enrich their content using discovered links [46], and Datagraft [140],\na cloud-based data transformation and publishing platform that supports the design and execution of\ntransformations on tabular data. Transformations in Datagraft include data cleaning functionalities but\nalso RDF data generation based on table to RDF mappings (implemented with Grafterize framework55).\nASIA supports the annotation of a table: it exploits vocabulary suggestions from the knowledge graph\nprofiling tool ABSTAT [145] to annotate properties and column types, and entity linking algorithms\n(executed as services) to annotate cells. In addition, it uses data extension services to fetch data from\nthird party sources adding new columns to the original table. The users control these operations from the\nGraphical User Interface (GUI) and check the results. As a consequence, DataGraft+ASIA supports two\nmain applications: KG generation and tabular data enrichment. ASIA-suported annotations are traduced\nto data transformations specifications; these specifications can executed, making the transformations\nrepeatable, shareable, and extensible. Data can be exported in several tabular and RDF formats and\npublished in the DataGraft platform.\nDAGOBAH UI is a web interface designed to visualise, validate, enrich, and manipulate the results\nof the STI process through DAGOBAH API56. The tool allows table data extracted from various pre-\nloaded benchmarks and additional files. It utilises DAGOBAH-SL, a RESTful API that implements pre-\n47github.com/phucty/mtab dev - dev version\n48bitbucket.org/disco unimib/mantistable-tool\n49openrefine.org\n50openrefine.org/docs/manual/reconciling\n51trifacta.com\n52github.com/odalic\n53github.com/odalic/odalic-docker\n54datagraft.io\n55eubusinessgraph.eu/grafterizer-2-0\n56developer.orange.com/apis/table-annotation\n47\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nprocessing and STI functionalities. DAGOBAH UI addresses the problem of missing data by providing\nthe possibility of adding additional columns using the background knowledge provided by the KGs. The\ntool is only accessible through an online web interface, while the source code is unavailable.\n48\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nFunctionalities\nKarma\nTableMiner+\nMagic\nMTab\nMantisTable\nSTAN\nOpenRefine\nTrifacta\nOdalic\nDataGraft\nDagobah\nSemTUI\nTableLLama\nImport of tables\nImport of tables via API\nImport of ontologies\nDefinition of personalised ontologies\nSemi-automatic annotation/HITL\nAnnotation suggestions\nAuto-complete support\nSubject column detection\nCEA\nCTA\nCPA (NE columns)\nCPA (LIT columns)\nTable manipulation\nAutomatic table extension\nVisualisation of annotations\nAuto save\nExport mapping\nExport RDF triplets\nOpen Source\nTable 5: table\nComparison of semantic table interpretation tools.\n49\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nSemTUI is an Open Source57 web-based application composed of a frontend module built with React\nand Redux, and a backend server. SemTUI focuses on tabular data annotation and extension tasks and is\ndecoupled from DataGraft. It implements a “link & extend” paradigm, inspired by linked open data, but\nmore general and supported by several data linking and data extension services (e.g., geocoding services,\ndata services, and so on).\nCompared to its first version ASIA, it provides a better GUI, and more\nfunctionalities to support human-in-the-loop tabular data annotation and extension. It is integrated\nwith end-to-end STI algorithms (improved from [36]) that provide a first annotation of an input table,\nwhich users are expected to revise and manipulate. Particular attention is given to the revision of entity\nlinking, which exploits a recent confidence-aware algorithm [8].\nTable 5 provides a comparison between tools.\nB.2\nComparison of tools with GUI\nIn this Section, we compare tools for STI or supporting STI tasks that also provide a GUI; these tools\nare introduced to assist users in applications listed in Section 7.\nWe found twelve tools with these\nfeatures: Karma58 [66], TableMiner+ [111], MAGIC59 [146], MTab tool60 [123], MantisTable61 [35],\nOpenRefine62, Trifacta63, Odalic64 [93], DataGraft+ASIA65 [139, 46], DAGOBAH UI [141], SemTUI66,\nMantisTableUI67 [39].\nA short description for each tool is provided in Section B.2. Table 5 in Appendix B provide a com-\nparison of the tools based on some key features: i) table import, ii) ontology support, iii) ontology\nsupport, iv) semi-automatic semantic annotation/HITL, v) semantic annotation sugges-\ntions, vi) auto-complete support for the semantic annotation process, vii) STI sub-task,\nviii) table manipulation, ix) automatic table extension, x) graphical visualisation of seman-\ntic annotations, xi) auto save of current user workspace, xii) API services and SPARQL\nendpoint, xiii) export mapping and RDF triples, and xiv) open source.\nTable import: table import is a crucial functionality for enhancing usability from the users’ per-\nspective. It allows users to work with their tables without requiring manual data transfer, ensuring a\nsmoother user experience. Additionally, users can perform various data operations, such as filtering, sort-\ning, aggregating, or visualising the data. Typically, tools provide two main methods for enabling table\nimport: wizards and APIs. Among the twelve analysed tools, only two (TableMiner+ and Odalic) do not\noffer wizard functionality, while three (TableMiner+, Odalic, and OpenRefine) lack API functionalities\nfor table import.\nOntology support: among tables, ontologies play another important role that would enhance users’\nsatisfaction and usability. They allow working with familiar and domain-specific terminology, ensuring\naccuracy and consistency in the semantic representation of the table. Moreover, the final annotated\ntables might be easily integrated with other systems.\nTools reviewed in this survey offer users two\noptions: reusing existing ontologies, either by importing an ontology or searching vocabularies used in\nexisting KG, and creating personalised ones. Out of the 12 tools, 10 do not allow importing or reusing\nontologies. Only Karma and DataGraft+ASIA provide users with such functionality. Similarly, only\nDataGraft+ASIA supports users in defining their personalised ontology to annotate the data. Karma\nsupports users in importing different ontologies and combining them.\nSemi-automatic semantic annotation/HITL: semi-automatic semantic annotation and human-\nin-the-loop ability allows users to review annotations. Users can correct, judge ambiguous or unclear\ninformation and refine automated annotations, improving the accuracy of the annotations. Almost half\nof the tools lack human-in-the-loop functionality. Such is implemented only in Karma, TableMiner+,\nMAGIC, MTab, MantisTable, Odalic, and DAGOBAH UI.\nSemi-automatic semantic annotation/HITL: semi-automatic semantic annotation and human-\nin-the-loop ability allows users to review annotations. Users can correct and judge ambiguous or unclear\ninformation and refine automated annotations, improving the accuracy of the annotations. Almost half\n57github.com/I2Tunimib\n58usc-isi-i2.github.io/karma\n59github.com/IBCNServices/MAGIC\n60mtab.app/mtab\n61bitbucket.org/disco unimib/mantistable-tool\n62openrefine.org\n63trifacta.com\n64github.com/odalic\n65datagraft.io\n66github.com/I2Tunimib\n67mantistable.datai.disco.unimib.it/\n50\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nof the tools lack human-in-the-loop functionality. Such is implemented only in Karma, TableMiner+,\nMAGIC, MTab, MantisTable, Odalic, and DAGOBAH UI.\nSemantic annotation suggestions: such functionality saves time and effort for users as it empow-\ners tools to automatically generate suggestions for the semantic annotation. In particular, for domains\nthat users do not know about, such functionality can ensure more accurate and consistent annotations, re-\nducing the risk of errors or inconsistencies. Seven tools, Karma, MAGIC, MantisTable, DataGraft+ASIA\nand SemTUI, provide users with annotation suggestions.\nAuto-complete support for the semantic annotation process: auto-complete functionality\nspeeds up the semantic annotation process by providing suggestions and/or completions for annota-\ntions. It drastically reduces the time and effort required for users to manually enter or search for an\nappropriate semantic term to use in the annotation process. Moreover, it prevents errors as a result of\nmiss-spelling. Karma, MantisTable, OpenRefine, Datagraft+ASIA, MantisTableUI and SemTUI imple-\nment auto-complete functionalities.\nSTI sub-tasks: not every STI sub-task is implemented in the available tools reviewed in this\nSection. Full implementation of the STI process would allow users full support to annotate every table\nelement accurately. TableMiner+ performs reconciliation by annotating cells with specific entities within\nthe KG and identification of the S-column in a semi-automatic manner. The first feature is common\nto OpenRefine. Trifacta is the weakest tools concerning the implementation of STI sub-tasks, while\nTableMiner+, MTab, MantisTable, Odalic, MantisTableUI, DAGOBAH UI and SemTUI fully implement\nsuch functionalities.\nTable manipulation: table manipulation functionality allows users to clean and preprocess the data\nbefore performing semantic annotation. For example, among tools that implement such functionality,\nKarma and OpenRefine allow users to manipulate tables and refine them by allowing column modifica-\ntion, such as renaming, eliminating, or changing their order. This ensures that the data is in the desired\nform, removing any inconsistencies or errors that could affect the quality of the annotations. Despite\nbeing an important functionality, such is implemented only by almost half of the available tools (i.e.,\nKarma, OpenRefine, Trifacta, and DataGraft+ASIA) OpenRefine has features that are not common to\nothers in our analysis. For example the automatic creation of new columns, and the exploration of the\ncells through the facets.\nAutomatic table extension: this is an important functionality, especially for data enrichment\napplications as it automatically retrieves additional data from external sources.\nFurthermore, such\nfunctionality can keep the semantic model up-to-date, reflecting the latest knowledge and insights despite\nthe updated data. Only MAGIC, OpenRefine, Trifacta, DataGraft+ASIA, and SemTUI users might\nbenefit from such functionality.\nGraphical visualization of semantic annotations: graphical visualisation of semantic annota-\ntions supports users with a visual representation of the annotated data, allowing them to understand\nbetter the relationships and the structure of the data within the table. Moreover, it allows users to\nidentify inter-dependencies between different table parts, enhancing the overall semantic understanding.\nTableMiner+, OpenRefine, Trifacta and Odalic do not allow users to visualise annotations.\nAuto save of current user workspace: auto-saving ensures that the user’s work is continuously\nsaved, preventing data loss from system failure. It allows users to perform changes and modifications\nwith the assurance that their progress is automatically saved without worrying about manually saving\ntheir work. Such functionality might serve as a form of version control, enabling users to review and\nrevert to previous versions if needed. Karma, OpenRefine, and Trifacta have the automatic saving feature\nof the current work status.\nExport mapping and RDF triples: exporting mappings and RDF triples allows the data anno-\ntated in the tool to be shared and integrated with other systems and applications, enabling interoper-\nability. Most available tools (Karma, MAGIC, MantisTable, Odalic, MantisTableUI, DataGraft+ASIA\nand DAGOBAH UI) allow both exports. Karma allows export in RDF format or JSON-LD. Regarding\nthe export of tabular data, Karma uses the R2RML format to highlight the annotations between the\ntable and the ontology. The other tools allow both exports. Only Trifacta does not implement any of\nthese functionalities.\nOpen Source: open-source tools provide transparency in their functionality, allowing users to un-\nderstand how the tool works and ensuring there are no hidden or proprietary algorithms or biases. All\nsoftware under such a license might be easily customised or modified. Only MTab, MantisTableUI, Tri-\nfacta and DAGOBAH UI do not provide the code in an open-source license. A detailed description of\nall tools can be found in the Appendix B.\n51\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nC\nGold Standards\nGSs serve as a benchmark to measure the performance of various approaches and systems. Moreover, GSs\nallow identifying the strengths and weaknesses of existing methods thus helping in the advancements\nof the state-of-the-art performance. Although several approaches deal with semantic annotations on\ntabular data, there are limited GSs for assessing the quality of these annotations. The main ones are\nT2Dv2, Limaye, Tough Table and SemTab. Table 6 in Appendix C shows statistics for the GSs68.\nThis Section considers only publicly available GSs. GSs for STI approaches can be classified based\non several dimensions.\nDomain: GSs can target a certain domain or cover a broad range of domains. Most of the avail-\nable GSs target cross-domain annotations. However, there are also some domain-specific GS such as\nIMDB [180], MusicBrainz [180], and BiodivTab [5].\nAnnotation coverage: GSs differ in the level of granularity at which annotations are provided. This\ncan range from fine-grained annotations capturing detailed semantic information at the cell or column\nlevel (classes, entities, predicates, e.g., WebTableStiching [137], 2T [44], and SemTab), to coarse-grained\nannotations(e.g., LimayeAll [105], GitTables [73], TURL [50]), providing broader semantic context at the\ntable or dataset level.\nNIL annotation: The previous Sections described how an approach should consider NIL annota-\ntions, which can be used for KG extension and construction. However, only three datasets currently\nconsider this type of annotation (i.e., [110], SemTab2022 R3 Biodiv, and SemTab 2022 R3 GitTables).\nThis underlines how more significant effort is needed on the part of the scientific community towards\nthis key challenge.\nDataset size: in STI, GSs should be composed of tables of varying sizes, from small to very large\ntables.\nThis would allow systems to measure and evaluate their scalability performance.\nA signifi-\ncant proportion of GSs are relatively small (T2Dv2 [137], WebTableStiching [137], Limaye [105], Mu-\nsicBrainz [180], IMDB [180], Taheriyan [154], 2T [44], REDTab [144], BiodivTab [5], and TSOTSA [84]).\nIn contrast, only a handful of them are larger (MammoTab69 [110], SOTAB [97], Wikary [112], GitTa-\nbles [73], and TURL [50]).\nDocumentation: GSs might be accompanied with documentation. It is important that certain\nfactors, such as the availability of guidelines, code availability, documentation on annotation conventions,\nand examples that aid in understanding and applying the GS, are clearly stated. The list of well-curated\nand documented datasets is limited (i.e., 2T [44], MammoTab [110], GitTables [73], REDTab [144],\nSOTAB [97], and BiodivTab [5]).\nTable 6 provides detailed statistics about GSs.\n68unimib-datai.github.io/sti-website/datasets/\n69unimib-datai.github.io/mammotab-docs/\n52\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nTable 6: Statistics for the most common datasets. ‘—’ indicates unknown.\nGS\nTables\nCols\n(min — max\n— ¯x)\nRows\n(min — max\n— ¯x)\nClasses\nEnti-\nties\nPred.\nKG\nUsed for\nvalidation by\nT2Dv2 [137]\n234\n1,2K\n(1 — 30 —\n4,52)\n2,8K\n(1 — 5K —\n84,55)\n39\n—\n154\nDBpedia\n[138, 58, 129, 55,\n27, 28, 74, 98, 174,\n38, 59, 65, 89, 178,\n50, 107]\nWebTableStitching [137]\n50\n300\n(6 — 6 — 6)\n717\n(3 — 83 —\n14,84)\n9\n400\n6\nDBpedia\n[137]\nLimaye [105]\n6,5K\n—\n—\n747\n143K\n90\nWikipedia\nYago\n[183, 55, 180, 27,\n28, 38, 59, 65, 89,\n107]\nLimayeAll [180]\n6,3K\n28,5K\n136K\n—\n227K\n—\nFreebase\n—\nLimaye200 [180]\n200\n903\n4,1K\n615\n—\n361\nFreebase\n—\nMusicBrainz [180]\n1,4K\n9,8K\n—\n—\n93,3K\n7K\nFreebase\n[180]\nIMDB [180]\n7,4K\n7,4K\n—\n—\n92,3K\n—\nFreebase\n[180]\nTaheriyan [154]\n29\n2,5K\n(3 — 71,3K —\n529K)\n16K\n(1 — 13,8K —\n957)\n—\n—\n—\nSchema.org\n[154]\nTough Table (2T) [44]\n180\n194K\n(1 — 8 — 4,46)\n802\n(6 — 15,5K —\n108K)\n540\n667K\n0\nWikidata\nDBpedia\n—\nMammoTab [110]\n980K\n5,6M\n2,3M\n2M\n2,8M\n—\nWikidata\n—\nSOTAB [97]\n108K\n—\n—\n91\n—\n176\nSchema.org\n—\nWikary [112]\n81,7K\n22,5K\n63,9K\n—\n30,6K\n188\nWikidata\n—\nGitTables [73]\n962K\n11,5M\n13,6M\n2,4K\n—\n—\nSchema.org\nDBpedia\n—\nREDTab [144]\n9K\n44,6K\n(1 — 11 —\n4,86)\n148K\n(1 — 353 —\n17,09)\n70\n—\n23\nMusic\nLiterature\n—\nTURL [50]70\n484K\n2,8M\n7,9M\n—\n1,2M\n—\nDBpedia\n[50, 179]\nBiodivTab [5]\n50\n1,2K\n(1 — 43 —\n23,96)\n12,9K\n(26 — 4,9K —\n261)\n84\n1,2K\n—\nWikidata\n[5]\nTSOTSACorpus [84]\n16K\n—\n—\n200\n60K\n—\nFood Data\n—\nR1\n64\n320\n(3 — 14 —\n5,05)\n9K\n(7 — 586 —\n143)\n120\n8,4K\n116\nR2\n11,9K\n59,6K\n(1 — 51 —\n5,55)\n29,8K\n(1 — 1,5K —\n27,06)\n14,8K\n464K\n6,7K\nR3\n2,1K\n10,8K\n(4 — 8 — 4,51)\n153K\n(6 — 207 —\n71,69)\n5,7K\n407K\n7,6K\nSemTab2019\nR4\n817\n3,3K\n(4 — 8 — 4,36)\n51,4K\n(6 — 198 —\n63,73)\n1,7K\n107K\n2,7K\nDBpedia\n[26, 35, 114, 120,\n147, 158]\nR1\n34,3K\n170K\n(4 — 8 — 4,96)\n249K\n(5 — 16 —\n8,27)\n136K\n985K\n136K\nR2\n12,1K\n55,9K\n(4 — 8 — 4,6)\n84,9K\n(5 — 16 —\n7,97)\n438K\n283K\n43,8K\nR3\n62,6K\n229K\n(3 — 7 — 3,66)\n397K\n(3 — 16 —\n7,34)\n167K\n768K\n167K\nSemTab2020\nR4\n22,4K\n79,6K\n(1 — 8 — 3,55)\n670K\n(6 — 15,5K —\n30,94)\n32,5K\n1,7M\n56,5K\nWikidata\n[1, 12, 14, 29, 38,\n34, 78, 89, 91, 122,\n143, 161, 172]\n53\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nGS\nTables\nCols\n(min — max\n— ¯x)\nRows\n(min — max\n— ¯x)\nClasses\nEnti-\nties\nPred.\nKG\nUsed for\nvalidation by\nR1\n180\n802\n(1 — 8 — 4,46)\n194K\n(6 — 15,5K —\n1,08K)\n539\n667K\n56,5K\nWikidata\nDBpedia\nR2\n1,7K\n5,6K\n(2 — 7 — 3,19)\n29,3K\n(5 — 58 —\n17,73)\n2,1K\n47,4K\n3,8K\nSemTab2021\nR3\n7,2K\n17,9K\n(2 — 5 — 2,48)\n58,9K\n(5 — 21 —\n9,18)\n7,2K\n58,9K\n10,7K\nWikidata\n[3, 2, 9, 13, 77, 123,\n146, 170]\nR1\n3,8K\n9,9K\n(2 — 5 — 2,56)\n22,4K\n(4 — 8 — 5,69)\n240\n1,4K\n319\nWikidata\nR2 HT\n5,1K\n13,3K\n(2 — 5 — 2,56)\n28,5K\n(4 — 8 — 5,57)\n398\n1,9K\n348\nR2 2T\n180\n802\n195K\n97\n111\n81K\n177K\n—\nWikidata\nDBpedia\nR3 Biodiv\n50\n1,2K\n12,9K\n43\n1,5K\n—\nSemTab2022\nR3 GitTables\n7,6K\n198K\n841K\n6,2K\n4,4K\n1K\n—\n—\nDBpedia\nSchema.org\nSchema.org\n[4, 30, 36, 107, 75]\nR1\n10,4K\n26,1K\n(2 — 4 — 2,51)\n49,1K\n(3 — 11 —\n5,72)\n—\n—\n—\nWikidata\ntfood\nSchema.org\n—\nSemTab2023\nR2\n—\n—\n—\n—\n—\n—\nSchema.org\ndbpedia\n—\n54\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nD\nAdditional Material\nTable 7 presents the acronyms with the respective names of the conferences or journals to which the\narticles analysed in this survey were submitted.\n55\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nTable 7: Conferences and journals acronyms.\nAcronym\nName\nAAAI\nAssociation for the Advancement of Artificial Intelligence\nCIKM\nThe Conference on Information and Knowledge Management\nDKE\nData & Knowledge Engineering\nEDBT\nInternational Conference on Extending Database Technology\nEKAW\nEuropean Knowledge Acquisition Workshop\nER\nInternational Conference on Conceptual Modeling\nESWC\nExtended Semantic Web Conference\nFGCS\nFuture Generation Computer Systems\nI-SEMANTICS International Conference on Semantic Systems\nICAI\nInternational Conference on Artificial Intellicence\nIEEE ICSC\nIEEE International Conference on Semantic Computing\nIEEE TKDE\nIEEE Transactions on Knowledge and Data Engineering\nIJCAI\nInternational Joint Conference on Artificial Intellicence Organization\nISWC\nInternational Semantic Web Conference\nLD4IE\nInternational Conference on Linked Data for Information Extraction\nLDOW\nLinked Data on the Web\nJOWS\nJournal of Web Semantics\nKGC\nThe Knowledge Graph Conference\nLD4IE\nLinked Data for Information Extraction\nSDS\nSwiss Conference on Data Science\nSemTab\nSemantic Web Challenge on Tabular Data to Knowledge Graph Matching\nSIGKDD\nInternational Conference on Knowledge Discovery & Data Mining\nSIGMOD\nSpecial Interest Group on Management of Data\nSIGSPATIAL\nInternational Conference on Advances in Geographic Information\nSWJ\nSemantic Web Journal\nVLDB\nVery Large Data Bases\nWISA\nWeb Information Systems and Applications\nWISE\nWeb Information Systems Engineering\nWSC\nWeb Science Conference\nWWW\nThe Web Conference\n56\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nTable 8 provides support in selecting approaches in relation to various attributes, such as Method,\nTasks, Code availability, License and Triple store.\n57\n\nThis work is shared under a CC BY-SA 4.0 license unless otherwise noted\nTable 8: Table for selecting approaches concerning the attributes Method, Tasks, Code availability,\nLicense and Triple store.\nTask\nMethod\nCEA\nCPA\nCTA\nCNEA\nCode\nAvailable\nLicence\nTriple Store\nReferences\nYES\nGPL 3.0\nDBpedia, Yago, CaliGraph\n2021 Heist [69]\nYES\nApache 2.0\nDBpedia\n2019 Chen [27]\nNO\n-\n-\n2017 Efthymiou [55]\nNO\n-\nWikidata\n2020 Huynh [78]\nNO\nOrange\nDBpedia, Wikidata\n2021 Huynh [77]\nYES\nImec license\nWikidata\n2021 Steenwinckel [146]\nHybrid\nYES\nMIT\nDBpedia, Wikidata\n2019 Kruit [98]\nNO\n-\nWikipedia\n2018 Luo [108]\nYES\nApache 2.0\nDBpedia\n2016 Neumaier [119]\nNO\n-\n-\n2021 Zhou [181]\nNO\n-\nWordNet\n2019 Takeoka [155]\nYES\nApache 2.0\n-\n2015 Ramnandan [135]\nYES\nApache 2.0\nDBpedia\n2019 Zhang [174]\nYES\nMIT\n-\n2019 Hulsebos [74]\nYES\nApache 2.0\nCIDOC-CRM\n2016 Taheriyan [154]\nNO\n-\n-\n2012 Goel [63]\nYES\nApache 2.0\nCIDOC-CRM, EDM\n2016 Taheriyan [153]\nYES\nApache 2.0\nPersonal ontologies\n2012 Knoblock [94]\nYES\nMIT\n-\n2022 Gottschalk [64]\nNO\nCCA 4.0\nYago\n2015 Bhagavatula [19]\nYES\nApache 2.0\n-\n2020 Li [103]\nNO\n-\nWikidata\n2021 Yang [170]\nNO\n-\n-\n2013 Cruz [43]\nNO\nCCA 4.0\nDBpedia\n2020 Zhang [178]\nNO\n-\nDBpedia, Freebase, WordNet, Yago\n2011 Mulwad [118]\nNO\n-\nDBpedia, Yago, Wikitology\n2013 Mulwad [116]\nNO\n-\nWikitology\n2010 Mulwad [117]\nYES\nMIT\nWikidata\n2023 Zhang[179]\nSupervised\nYES\nApache 2.0\n-\n2022 Deng [50]\nNO\n-\n-\n2013 Ermilov [57]\nNO\n-\nPersonal ontologies\n2009 Tao [157]\nNO\n-\n-\n2013 Zhang [175]\nNO\n-\nDBpedia, Freebase, Yago\n2013 Deng [49]\nNO\n-\nWikidata\n2020 Baazouzi [14]\nYES\nMIT\nDBpedia\n2018 Kacprzak [85]\nNO\n-\nYago\n2014 Sekhavat [142]\nNO\n-\n-\n2013 Buche [22]\nNO\n-\nPersonal ontologies\n2007 Hignette [70]\nNO\n-\nYago\n2011 Venetis [162]\nYES\nApache 2.0\nDBpedia\n2019 Chen [27]\nYES\nGPL 3.0\nDBpedia\n2016 Ermilov [58]\nYES\n-\nWikidata\n2020 Yumusak [172]\nNO\nApache 2.0\nDBpedia\n2017 Ell [56]\nNO\n-\nDBpedia\n2013 Quercini [134]\nYES\n-\n-\n2018 Zhang [176]\nNO\n-\nDBpedia\n2013 Zwicklbauer [183]\nNO\n-\nWikidata\n2020 Azzi [12]\nYES\n-\nWikidata\n2020 Tyagi [161]\nNO\n-\nDBpedia\n2013 Munoz [115]\nNO\nMIT\nDBpedia, Wikidata\n2021 Nguyen [123]\nNO\n-\n-\n2012 Wang [164]\nNO\n-\nDBpedia\n2019 Steenwinckel [147]\nNO\n-\nWikidata\n2020 Nguyen [122]\nNO\n-\nWikitology\n2010 Syed [151]\nNO\n-\nYago\n2010 Limaye [103]\nNO\nOrange\nDBpedia\n2019 Chabot [26]\nYES\nApache 2.0\nDBpedia\n2020 Cremaschi [34]\nYES\nApache 2.0\nDBpedia, Wikidata\n2021 Avogadro [9]\nYES\nApache 2.0\nFreebase\n2017 Zhang [180]\nYES\nMIT\n-\n2019 Thawani [158]\nYES\nMIT\nDBpedia, Wikidata\n2022 Chen [30]\nYES\nMIT\nWikidata\n2020 Shigapov [143]\nUnsupervised\nYES\n-\nDBpedia\n2019 Oliveira [124]\n58",
    "pdf_filename": "Survey_on_Semantic_Interpretation_of_Tabular_Data_Challenges_and_Directions.pdf"
}