{
    "title": "Rethinking Top Probability from Multi-view for Distracted Driver Behaviour Localization",
    "context": "Naturalistic driving action localization task aims to recognize and comprehend human behaviors and actions from video data captured during real-world driving sce- narios. Previous studies have shown great action localiza- tion performance by applying a recognition model followed by probability-based post-processing. Nevertheless, the probabilities provided by the recognition model frequently contain confused information causing challenge for post- processing. In this work, we adopt an action recognition model based on self-supervise learning to detect distracted activities and give potential action probabilities. Subse- quently, a constraint ensemble strategy takes advantages of multi-camera views to provide robust predictions. Finally, we introduce a conditional post-processing operation to lo- cate distracted behaviours and action temporal boundaries precisely. Experimenting on test set A2, our method obtains the sixth position on the public leaderboard of track 3 of the 2024 AI City Challenge. Distracted driving is defined as any circumstance where the driver diverts attention away from safe driving activi- ties. In the United States, over 3,500 lives are lost annu- ally due to accidents caused by distracted driving. Research in intelligent transportation systems and distracted driving has gained significant attention from scholars worldwide [19, 25, 28, 32]. This interest is fueled by the potential of naturalistic driving videos to capture real-time driving behavior and the capability of deep learning to analyze po- tential risk factors. The AI City Challenge 2024 [3] aims to advance research in this field by hosting a naturalistic driving action recognition challenge. The given challenge focuses on detecting distracted driving behaviors using syn- thetic naturalistic data collected from three camera locations inside the vehicle. This challenge involves analyzing syn- chronized video recordings from drivers engaged in various distracted driving activities. These activities are classified into different actions, such as using a phone, eating, and reaching into the backseat, each of which can potentially lead to accidents. Previous studies [12, 13, 24, 33, 35] have demonstrated the effectiveness in distracted driving detection, typically dividing the task into two main stages: activity recognition and temporal action localization. However, several chal- lenges remain: (1) The dataset is limited to 16 behavior categories, leading to an insufficient diversity of samples within each category. (2) The models must discern vari- ous actions from different perspectives within untrimmed videos, facing difficulties in distinguishing subtle variations within the same class and detecting minor discrepancies be- tween certain classes. (3) The inclusion of the appearance block constrains the model’s ability to discern differences between certain classes. (4) Previous solutions rely heavily on the classification model’s confidence, which can result in misclassifications when the highest and second-highest classes have similar probabilities. Therefore, in this paper, we aim to contribute to the lit- erature in the following manners: First, we inherit an action classification model in video based self-supervised learn- ing to detect robust distracted actions from the input video. Next, we apply a constraint ensemble strategy to take ad- vantage of the power of each camera view. In the final, con- ditional post-processing steps consider contexts from top 1 and top 2 confidence ranking to locate distracted actions and temporal boundaries accurately. arXiv:2411.12525v1  [cs.CV]  19 Nov 2024",
    "body": "Rethinking Top Probability from Multi-view for Distracted Driver Behaviour\nLocalization\nQuang Vinh Nguyen1, Vo Hoang Thanh Son1, Chau Truong Vinh Hoang2, Duc Duy Nguyen3\nNhat Huy Nguyen Minh2, Soo-Hyung Kim1\n1Chonnam National University. 2Vietnamese-German University.\n3Hanoi University of Science and Technology.\n{vinhbn28,hoangsonvothanh,shkim}@jnu.ac.kr\n{16076, 10423045}@student.vgu.edu.vn, duy.nd223435@sis.hust.edu.vn\nAbstract\nNaturalistic driving action localization task aims to\nrecognize and comprehend human behaviors and actions\nfrom video data captured during real-world driving sce-\nnarios. Previous studies have shown great action localiza-\ntion performance by applying a recognition model followed\nby probability-based post-processing.\nNevertheless, the\nprobabilities provided by the recognition model frequently\ncontain confused information causing challenge for post-\nprocessing. In this work, we adopt an action recognition\nmodel based on self-supervise learning to detect distracted\nactivities and give potential action probabilities.\nSubse-\nquently, a constraint ensemble strategy takes advantages of\nmulti-camera views to provide robust predictions. Finally,\nwe introduce a conditional post-processing operation to lo-\ncate distracted behaviours and action temporal boundaries\nprecisely. Experimenting on test set A2, our method obtains\nthe sixth position on the public leaderboard of track 3 of the\n2024 AI City Challenge.\n1. Introduction\nDistracted driving is defined as any circumstance where\nthe driver diverts attention away from safe driving activi-\nties. In the United States, over 3,500 lives are lost annu-\nally due to accidents caused by distracted driving. Research\nin intelligent transportation systems and distracted driving\nhas gained significant attention from scholars worldwide\n[19, 25, 28, 32]. This interest is fueled by the potential\nof naturalistic driving videos to capture real-time driving\nbehavior and the capability of deep learning to analyze po-\ntential risk factors. The AI City Challenge 2024 [3] aims\nto advance research in this field by hosting a naturalistic\ndriving action recognition challenge. The given challenge\nfocuses on detecting distracted driving behaviors using syn-\nthetic naturalistic data collected from three camera locations\ninside the vehicle. This challenge involves analyzing syn-\nchronized video recordings from drivers engaged in various\ndistracted driving activities. These activities are classified\ninto different actions, such as using a phone, eating, and\nreaching into the backseat, each of which can potentially\nlead to accidents.\nPrevious studies [12, 13, 24, 33, 35] have demonstrated\nthe effectiveness in distracted driving detection, typically\ndividing the task into two main stages: activity recognition\nand temporal action localization. However, several chal-\nlenges remain: (1) The dataset is limited to 16 behavior\ncategories, leading to an insufficient diversity of samples\nwithin each category. (2) The models must discern vari-\nous actions from different perspectives within untrimmed\nvideos, facing difficulties in distinguishing subtle variations\nwithin the same class and detecting minor discrepancies be-\ntween certain classes. (3) The inclusion of the appearance\nblock constrains the model’s ability to discern differences\nbetween certain classes. (4) Previous solutions rely heavily\non the classification model’s confidence, which can result\nin misclassifications when the highest and second-highest\nclasses have similar probabilities.\nTherefore, in this paper, we aim to contribute to the lit-\nerature in the following manners: First, we inherit an action\nclassification model in video based self-supervised learn-\ning to detect robust distracted actions from the input video.\nNext, we apply a constraint ensemble strategy to take ad-\nvantage of the power of each camera view. In the final, con-\nditional post-processing steps consider contexts from top 1\nand top 2 confidence ranking to locate distracted actions and\ntemporal boundaries accurately.\narXiv:2411.12525v1  [cs.CV]  19 Nov 2024\n\n2. Related Work\n2.1. Action Recognition\nAction recognition is a crucial task in the field of video\nunderstanding. Over the years, there have been numerous\nstudies and extensive research conducted in this area. The\nmain goal of the action recognition is to classify a trimmed\nvideo into specific action classes using end-to-end deep\nlearning methods. There have been significant updates in\narchitecture design, ranging from 2D-based CNN models\nand 3D-based CNN models to Transformer-based models.\n2D-based action recognition methods first implement a\nCNN model to extract spatial features for each frame in\nthe video. The sequence models[6, 26] are employed to\nfuse these features with the aim of capturing temporal in-\nformation. 3D-CNN attempts[2, 7, 23] to process spatial-\ntemporal information directly by using 3D input tensors,\nwhere 2 dimensions represent space and 1 dimension rep-\nresents time . The success of Transformer in image-related\nand sequential tasks and has motivated the exploration of\nits potential in video recognition, [15, 18] have been suc-\ncessfully developed to use Transformer in the architecture.\nRecent works also take advantages of large video founda-\ntion pre-training models to improve performance. Masking\nwith high ratio or scaling transformer model by applying\nself-supervised learning, [22, 27, 29] have shown great po-\ntential in extracting robust video representation\n2.2. Temporal Action Localization\nTemporal action localization is the task of automatically\nidentifying the time duration during which an action occurs\nwithin an untrimmed video and determining its correspond-\ning action category. The conventional two-stage method in-\nvolves proposing action segments initially and subsequently\nclassifying these proposals into their respective action cat-\negories [16, 17, 20, 31]. However, a major drawback of\nthis method is that the boundaries of action instances remain\nfixed during the classification process. As a result, while the\nmethod can identify time intervals likely to contain actions,\nit lacks the ability to precisely determine the exact start and\nend times of the actions.\nIn contrast, one-stage methods have garnered significant\nattention by integrating the localization and classification\ntasks within a single network. This approach eliminates the\nfixed boundaries issue and offers a more streamlined solu-\ntion. Previous works have seen the adoption of hierarchical\narchitectures based on CNN [14, 16, 34]. Recent studies\n[1, 21] extract a video representation with a Transformer-\nbased encoder.\n3. Method\nAs indicated in Fig. 1, our distracted driver behaviour recog-\nnition system consists of three main novel components: an\naction recognition model, an ensemble strategy, and con-\nditional post-processing.\nThe first is an action recogni-\ntion model which is self-supervised learning, recognize dis-\ntracted driver behaviors from input short videos. The sec-\nond is an ensemble strategy being responsible for integrat-\ning multi-view predictions.\nGiven recognition probabili-\nties, conditional post-processing considers diverse contexts\nto smooth out detected activities and localize the temporal\nboundary accurately. Detailed descriptions of each compo-\nnent are presented in the following subsections.\n3.1. Action Recognition\nRecent researches have demonstrated that self-supervised\nlearning (SSL) can provide more robust [10] and general\nfeatures [4, 8, 11], while reducing the amount of data re-\nquired for an equivalent supervision-based pre-training. In\nthe context of video understanding, self-supervised learning\ntechniques seek to take advantage of the temporal coher-\nence and spatial correlations seen in video sequences. These\napproaches are particularly suitable for scenarios where la-\nbeled data is scarce or expensive to obtain as the distracted\ndriver behavior dataset. Inspired by the successful study of\nMasking Modeling in the text and picture domain [5, 9, 30].\nVideoMAE [22] employs Masked Autoencoders, a varia-\ntion on traditional Autoencoders where certain parts of the\ninput data are masked out during training, encouraging the\nmodel to learn useful representations that capture the un-\nderlying structure of the data leading to promising perfor-\nmance in a variety of video understanding tasks. Our sys-\ntem inherits this structure to classify distracted action from\nnaturalistic driving videos. Specifically, input videos with\nFPS 30 are trimmed into a series of short videos containing\n64 frames. The model achieves short videos as input to give\nthe probability for each class in the output.\n3.2. Multi-view Ensemble Strategy\nThe distracted driver action is divided into sixteen distracted\nactions and three views of the camera mounted in the car:\ndashboard, rearview and rightside.\nEach of these views\nhas significance in different contexts. Dashboard view di-\nrectly facing driver contributes clearly to actions: ”phone\ncall by right hand”, ”drink”, ”eating” or activities involving\nto the movement of body-head such as ”talk with passen-\nger”, ”pick up from floor”. Rear view gives a broader space\nview inside the car, and is useful for identifying various ac-\ntions: ”phone call by right hand or left hand”, ”reaching be-\nhind” or ”hand on head”. While the right side view shows\na different view, from the right side of the driver, this view\nis helpful for hand movements: ”control the panel”, ”text\nby hand” or ”pick from floor (Passenger)”. In addition, sev-\neral specific classes: ”talk with passenger”, ”pick from floor\n(Driver)” can be integrated by all views to comprehend the\noverall context of distracted driving. Therefore, in order to\n\nVideo \nClassification\n64\n64\nVideo \nClassification\nVideo \nClassification\nEnsemble\n0\n1\n14\n15\n0\n1\n14\n15\n0\n1\n14\n15\n1\n14\n15\n0\nDashboard\nRightside\nRearview\nTop 1\nTop 2\nConditional Post-processing\nFinal Action Localization\nFigure 1. Distracted Driver Behaviour Recognition System\nClass 1. Drinking\nClass 4. Eating\nClass 13. Yawning\nClass 5. Text (Right)\nClass 6. Text (Left)\nClass 8. Yawning (Adjust control panel)\nClass 10. Pick up from floor (Passenger)\nClass 15. Singing  or dancing with music\nClass 3. Phone Call (Left)\nClass 14. Hand on head\nClass 0. Normal\nClass 9. Pick up from floor (Driver)\nClass 11. Talk to passenger at the right\nClass 12. Talk to passenger at backseat\nClass 2. Phone Call (Right)\nClass 7. Reaching behind\nDashboard\nRightside\nRearview\nFigure 2. Ensemble strategy\nenhance recognition performance, we suggest an ensemble\nstrategy based on multi-view. The specifics of ensemble\nstrategy are displayed in Fig. 2.\n3.3. Conditional Post-Processing\nThe action recognition model classifies short videos which\nare trimmed from input video to give a series of probabil-\nity. Output probability is an array of prediction vectors with\nthe length of 16 corresponding to a number of classes. El-\nements with the highest value in vectors refer to predicted\nclasses. And elements with second highest value normally\nexpress potential classes which are the second most trust-\nworthy after the highest ones. Our post-processing strategy\nleverages top 1 and top 2 of output probability to locate the\nactions and time boundary more accurately. This process\nconsists of three main steps: Conditional Merging, Condi-\ntional Decision and Missing Labels Restoring.\nConditional Merging. The first operation refers to condi-\ntional merging, which is depicted in Fig. 3. Instead of merg-\ning closer actions normally, this component considers the\ncontext of one certain class and neighbor classes by top 1\nand top 2 confidence ranking to merge potential candidates\nand remove noise classes. To explain symbols in Fig. 3,\n”second” represents the time boundary for each action, the\nvalues in the boxes refer to the probabilities for each type.\nTop 1 is the class with the highest probability score, while\ntop 2 represents the class with the second highest probabil-\nity.\nConditional Decision. Fig. 4 describes the conditional de-\ncision operation which selects a reliable time segmentation\n\n0.3\n0.35\n0.25\n0.8\n0.76\n0.83\n0.4\n0.38\n0.03\n0.02\n0.07\n0.01\n0.01\n0.02\n0.3\n0.04\n0.02\n0.03\n15\n15\n15\n0\n14\n14\n14\n0\n15\nTop 1\nTop 2\n14\n15\n14\n15\n0\n15\n15\n11\n15\n14\n15\n0\nTop 2(>0.2)\n15\n15\n15\n15\n15\n15\n15\n15\n15\nclass\nNew class\nna\nna\nna\nSecond\n28\n29\n30\n31\n32\n33\nFigure 3. Conditional Merging\n0.1\n0.05\n0.06\n0.09\n7\n7\n7\n7\n12\n0\n15\nClass\n0.34\n0.23\n0.28\n0.35\n7\n7\n7\n7\nSecond\n69\n70\n71\n72\n196\n197\n198\n199\nTop 1\nTop 2\nNew class\n7\n7\n7\n7\n0\n0\n15\n0\n7\n7\n7\n7\n12\n12\n12\n12\nTop 2(>0.2)\nna\nna\nna\nna\n12\n12\n12\n12\n7\n7\n7\n7\n12\n12\n12\n12\nIf 12 in Miss Labels\nDecision\n0.89\n0.82\n0.81\n0.76\n0.58\n0.64\n0.41\n0.55\n7\nFigure 4. Conditional Decision\nfrom different segments of the same classes. Given several\ndifferent segments of the same class, for example, there are\ntwo segments of class 7 ”reaching behind” in Fig. 4. The\ndecision module relies on probabilities from top 1 and top 2\nto filter a most trustworthy segment.\nMissing Labels Restoring. After the two mentioned above\nsteps, it still has some classes that are missing or not de-\ntected by the top 1 prediction. It means that if we just use\ntop 1 probability for output prediction, the system could not\nlocalize distracted actions sufficiently. The restoring mod-\nule shown in Fig. 5 finds these classes to reproduce the final\nprediction with enough 16 classes.\n4. Experiments\n4.1. Dataset\nThe distracted driver behavior dataset provides a compre-\nhensive collection of driving videos capturing the actions of\nClass 13\nClass 12\nTop 2\n0\n13\n13\n13\n12\n12\n12\n12\nTop 1\n0\n0\n0\n0\n11\n7\n7\n11\nRetrival\nMiss Labels\n0\n0\n0\n0\nFigure 5. Missing Labels Restoring\n99 individual drivers over a total of 90 hours. Each driver\nis recorded performing a series of 16 different distracting\nactivities randomly, with the order of these activities also\nrandomized within each video. To ensure a holistic view\nof the driving scenario, the dataset employs three cameras\nsimultaneously recording from different angles within the\ncar. Notably, each driver undergoes two rounds of data col-\nlection: one without any form of distraction and another\nwith a predetermined distractor, such as sunglasses or a hat.\nThis design allows for a thorough examination of driver be-\nhavior under varying levels of distraction, offering valuable\ninsights into the impact of external factors on driving per-\nformance. The videos from the 2024 AI City Challenge’s\nTrack 3 on Naturalistic Driving Action Recognition are sep-\narated into two datasets: ”A1” for training, ”A2” for testing\nwith the training dataset ”A1” containing the ground truth\nlabels for the start time, end time, and types of distracted\nactions.\n4.2. Evaluation Matric\nAction Recognition. Action classification involves the task\nof assigning a label or category to a video based on its con-\ntent. The accuracy score is calculated by comparing the pre-\ndicted class labels with the ground truth labels for all videos.\nA higher accuracy score indicates better performance of the\nvideo classification model in correctly predicting the class\nlabels of videos. The accuracy is defined as:\nAccuracy = Number of Correct Predictions\nTotal Number of Predictions X100% (1)\n”Number of Correct Predictions” is the number of in-\nstances that are correctly classified by the classifier. ”Total\nNumber of Predictions” is the total number of instances in\nthe dataset.\nTemporal Action Localization. For temporal action local-\nization, activity overlap measure (os) quantifies the degree\nof overlap between the predicted temporal segment and the\nground truth annotation for a particular action or activity\nwithin a video sequence.\nos = Intersection\nUnion\n(2)\n\nFold\nView\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nAvg\nDash\n1.00\n0.94\n0.94\n0.80\n0.90\n0.98\n0.89\n0.97\n0.75\n0.81\n0.84\n0.86\n0.75\n0.95\n0.81\n0.88\n1\nRear\n1.00\n0.96\n0.99\n0.81\n1.00\n0.91\n0.96\n0.94\n0.84\n0.89\n0.70\n0.82\n0.82\n0.96\n0.80\n0.89\nRight\n1.00\n0.99\n0.96\n0.66\n0.94\n0.92\n0.86\n0.96\n0.86\n0.81\n0.85\n0.70\n0.80\n0.90\n0.80\n0.88\nEnsemble\n1.00\n0.96\n0.99\n0.80\n0.94\n0.92\n0.97\n0.96\n0.82\n0.81\n0.86\n0.88\n0.75\n0.96\n0.80\n0.90\nDash\n1.00\n0.88\n0.98\n0.70\n0.96\n0.95\n0.90\n0.86\n0.76\n0.66\n0.76\n0.68\n0.61\n0.96\n0.88\n0.84\n2\nRear\n1.00\n0.94\n0.97\n0.67\n0.96\n0.96\n0.95\n0.83\n0.80\n0.75\n0.64\n0.74\n0.60\n0.98\n0.84\n0.84\nRight\n0.96\n0.85\n0.91\n0.27\n0.96\n1.00\n0.93\n0.99\n0.81\n0.75\n0.74\n0.58\n0.42\n0.84\n0.82\n0.79\nEnsemble\n1.00\n0.95\n0.97\n0.70\n0.96\n1.00\n0.95\n0.99\n0.80\n0.75\n0.70\n0.72\n0.61\n0.98\n0.82\n0.86\nDash\n1.00\n0.98\n0.68\n0.80\n0.88\n0.88\n0.88\n0.93\n0.78\n0.61\n0.60\n0.68\n0.87\n0.97\n0.77\n0.82\n3\nRear\n1.00\n1.00\n0.63\n0.83\n0.94\n0.97\n0.86\n0.98\n0.85\n0.76\n0.69\n0.71\n0.81\n0.97\n0.75\n0.85\nRight\n0.88\n0.99\n0.60\n0.75\n0.99\n0.92\n0.88\n0.98\n0.92\n0.76\n0.72\n0.67\n0.63\n0.97\n0.76\n0.83\nEnsemble\n1.00\n1.00\n0.63\n0.80\n0.99\n0.90\n0.88\n0.98\n0.84\n0.75\n0.64\n0.70\n0.87\n0.97\n0.76\n0.85\nDash\n0.92\n0.97\n0.97\n0.81\n0.96\n0.90\n0.85\n0.88\n0.76\n0.17\n0.86\n0.68\n0.86\n0.93\n0.85\n0.78\n4\nRear\n0.89\n0.96\n0.99\n0.81\n0.92\n0.80\n0.82\n0.85\n0.73\n0.15\n0.86\n0.65\n0.84\n0.94\n0.86\n0.80\nRight\n0.94\n0.97\n0.90\n0.67\n0.97\n0.92\n0.91\n0.97\n0.86\n0.20\n0.77\n0.66\n0.60\n0.95\n0.83\n0.81\nEnsemble\n0.92\n0.97\n0.99\n0.96\n0.97\n0.92\n0.90\n0.97\n0.88\n0.20\n0.86\n0.66\n0.86\n0.94\n0.83\n0.86\nDash\n0.94\n0.91\n0.95\n0.79\n0.87\n0.84\n0.78\n0.78\n0.85\n0.79\n0.80\n0.69\n0.82\n0.88\n0.80\n0.83\n5\nRear\n0.87\n0.96\n0.93\n0.66\n0.96\n0.94\n0.93\n0.93\n0.85\n0.79\n0.82\n0.68\n0.92\n0.97\n0.84\n0.87\nRight\n0.94\n0.98\n0.88\n0.46\n0.88\n0.96\n0.91\n0.97\n0.89\n0.88\n0.56\n0.77\n0.73\n0.95\n0.84\n0.84\nEnsemble\n0.94\n0.95\n0.93\n0.79\n0.88\n0.96\n0.96\n0.97\n0.86\n0.90\n0.82\n0.70\n0.82\n0.97\n0.84\n0.89\nTable 1. The accuracy on the validation set of each 5-Fold split in different classes.\n5\n5\n6\n6\n9\n9\n1\n1\n14\n14\n14\n2\n14\n14\n14\n13\n7\n7\n7\n7\n13\n12\n12\n7\n12\n12\n7\n7\n5\n6\n5 5\n5\n9\n4\n10\n10\n3\n3\n3\n8\n3\n8\n8\n15\n15\n15\n4\n5\n0\n4\n4\n4\n4\n2\n4\n2\n4\n4 4\n12\n12\n11\n11\n11\n12\nConditional Merging\nConditional Decision\nTop 1\nTop 2\nFinal\nMissing Label Restoring\nClass\nSecond\nFigure 6. Action localization result of Conditional Post-Processing\nIntersection is the duration of time that is common to\nboth the predicted segment and the ground truth annotation.\nUnion is the total amount of time covered by both the pre-\ndicted segment and the ground truth annotation.\n4.3. Implement Detail\nThe methodology employed relies on the PyTorch frame-\nwork, a publicly available toolbox widely used in machine\nlearning research. All experimentation was conducted on\na high-performance workstation equipped with two RTX\n3090 graphics card boasting 48GB of memory.\nFor the\nvideo classification task, the network architecture utilized\nis consistent with the model described in reference [22].\nIn particular, we use a standard Vision Transformer (ViT)\nmodel as the foundation. Each input video trimmed with\nstride 30 frames comprises 64 frames, sampled 16 frames\nevenly spaced per video. Training process is conducted with\na learning rate of 2 x 10-3 over 20 epochs for each camera\nview.\n4.4. Results\nAction Recognition. The training dataset A1 is divided into\n5 folds. We validate each of the folds in all three views of\nthe camera. Results in Tab. 1 illustrate the effect of each of\n\nviews on different classes. As can be seen, the right side\nview often gives excellent accuracy in several classes such\nas class 8 (control the panel), class 10 (pick up from floor\nof passenger), or class 5,6 (text) because this view is expert\nin these classes more than rear view and dashboard view.\nBesides, the dashboard view contributes greatly to class 1\n(drink), class 4(eat), or class 13(yawning) and often is the\nbest. In addition, the rear view strongly affects performance\nof class 3 (phone call by left hand), and class 14(hand on\nhead). Our ensemble strategy improves and surpasses situ-\nations with only a single view. Results in each of the folds\nfluctuate and depend on the challenge of the validation set.\nTemporal Action Localization. The proposed method is\ntrained on the A1 dataset provided by the competition, and\ntested on public test dataset A2 to evaluate temporal action\nlocalization performance. As indicated in Tab. 2, our ap-\nproach ranks 6th on the leaderboard with a 0.76 os score,\noutperforms 7th by almost 8% score and is far ahead of\ncompetitors beneath.\nBesides, our solution is not much\nlower than the top-rank methods. This proves the effective-\nness and potential of introduced method in the distracted\ndriver behaviour recognition challenge. Fig. 6 depicts post-\nprocessing operation in detail, Horizontal axis denotes for\ntime variable (second), and vertical axis refers to classes\n(from 0 to 15). Numbers on top of bars in the Fig. 6 ex-\npress corresponding classes. The top 1 chart shows pre-\ndiction given by highest confidence probability, while the\ntop 2 illustrates second reliable classes. As can be seen in\nthe top 1 chart, the predicted labels attach with many noisy\nlabels causing confusion to action recognition and localiza-\ntion. The proposed post-process operation considers the top\n1, top 2 probabilities, applies conditional merging, condi-\ntional decision and missing label restoring to smooth and\nlocalize accurately distracted action prediction. Fig. 6 indi-\ncates that our final result is seamless and superior to the top\n1 prediction. This demonstrates that our post-processing\nstrategy help model make decisions accurately and effec-\ntively localize temporal boundaries.\nRank\nTeam ID Score\n1\n155\n0.8282\n2\n189\n0.8213\n3\n32\n0.8149\n4\n207\n0.8045\n5\n5\n0.7798\n6\n136\n0.7625\n7\n17\n0.6844\n8\n165\n0.6080\n9\n156\n0.5963\n10\n125\n0.2307\nTable 2. Leaderboard of challenge track.\n5. Conclusion\nIn this work, we have suggested a conditional recogni-\ntion system for the distracted driver behaviour localization\ntask. First, our method uses a pre-trained action recogni-\ntion model that was trained by self-supervised learning to\nidentify distracted activities in video input. After that, a\nmulti-view ensemble strategy is adopted to leverage the ad-\nvantages of each camera view. Given output probabilities,\nwe post-processing by conditional merging, conditional de-\ncision, and missing labels restoring operation to recognize\nthe distracted actions and locate time boundary accurately.\nConsequently, we achieved the sixth rank score in test set\n”A2”, surpassing methods ranked lower while remaining\nvery close to the top ranking.\n6. Acknowledgement\nThis work was supported by the National Research\nFoundation of Korea(NRF) grant funded by the Ko-\nrea government(MSIT) (RS- 2023-00219107).\nThis\nwork also was supported by Institute of Information\n& communications Technology Planning & Evaluation\n(IITP) under the Artificial Intelligence Convergence In-\nnovation Human Resources Development (IITP-2023-RS-\n2023-00256629) grant funded by the Korea govern-\nment(MSIT)”\nReferences\n[1] Shai Avidan, Gabriel Brostow, Moustapha Cisse´e, Gio-\nvanni Maria Farinella, and Tal Hassner.\nSpringer Nature\nSwitzerland, 2022. 2\n[2] Joao Carreira and Andrew Zisserman.\nQuo vadis, action\nrecognition? a new model and the kinetics dataset. 2017\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2017. 2\n[3] AI City Challenge. 2024. 1\n[4] Xinlei Chen and Kaiming He. Exploring simple siamese rep-\nresentation learning. CoRR, abs/2011.10566, 2020. 2\n[5] Jacob Devlin.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 2\n[6] Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama,\nMarcus Rohrbach, Subhashini Venugopalan, Trevor Darrell,\nand Kate Saenko. Long-term recurrent convolutional net-\nworks for visual recognition and description.\n2015 IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2015. 2\n[7] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. 2019\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 2019. 2\n[8] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,\nBernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\nlaghi Azar, et al. Bootstrap your own latent-a new approach\n\nto self-supervised learning. Advances in neural information\nprocessing systems, 33:21271–21284, 2020. 2\n[9] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n16000–16009, 2022. 2\n[10] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and\nDawn Song.\nUsing self-supervised learning can improve\nmodel robustness and uncertainty. Advances in neural in-\nformation processing systems, 32, 2019. 2\n[11] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim,\nand Jaekoo Lee. Selfreg: Self-supervised contrastive regu-\nlarization for domain generalization. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9619–9628, 2021. 2\n[12] Huy Duong Le, Minh Quan Vu, Manh Tung Tran, and\nNguyen Van Phuc.\nTriplet temporal-based video recogni-\ntion with multiview for temporal action localization.\nIn\n2023 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition Workshops (CVPRW), pages 5428–5434,\n2023. 1\n[13] Rongchang Li, Cong Wu, Linze Li, Zhongwei Shen,\nTianyang Xu, Xiao-Jun Wu, Xi Li, Jiwen Lu, and Josef\nKittler. Action probability calibration for efficient natural-\nistic driving action localization. In 2023 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition Work-\nshops (CVPRW), pages 5270–5277, 2023. 1\n[14] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and\nLimin Wang. Tea: Temporal excitation and aggregation for\naction recognition.\n2020 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2020. 2\n[15] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Man-\ngalam, Bo Xiong, Jitendra Malik, and Christoph Feichten-\nhofer. Mvitv2: Improved multiscale vision transformers for\nclassification and detection. 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2022. 2\n[16] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift\nmodule for efficient video understanding. 2019 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), 2019.\n2\n[17] Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen.\nBmn: Boundary-matching network for temporal action pro-\nposal generation. 2019 IEEE/CVF International Conference\non Computer Vision (ICCV), 2019. 2\n[18] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video swin transformer. 2022\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 2\n[19] Zhihan Lv, Shaobiao Zhang, and Wenqun Xiu. Solving the\nsecurity problem of intelligent transportation system with\ndeep learning. IEEE Transactions on Intelligent Transporta-\ntion Systems, 22(7):4281–4290, 2021. 1\n[20] Zhiwu Qing, Haisheng Su, Weihao Gan, Dongliang Wang,\nWei Wu, Xiang Wang, Yu Qiao, Junjie Yan, Changxin Gao,\nand Nong Sang.\nTemporal context aggregation network\nfor temporal action proposal refinement. 2021 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2021. 2\n[21] Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Lit, and\nDacheng Tao. Tridet: Temporal action detection with rel-\native boundary modeling. 2023 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023. 2\n[22] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.\nVideomae: Masked autoencoders are data-efficient learners\nfor self-supervised video pre-training, 2022. 2, 5\n[23] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,\nand Manohar Paluri. Learning spatiotemporal features with\n3d convolutional networks. 2015 IEEE International Con-\nference on Computer Vision (ICCV), 2015. 2\n[24] Manh Tung Tran, Minh Quan Vu, Ngoc Duong Hoang, and\nKhac-Hoai Nam Bui.\nAn effective temporal localization\nmethod with multi-view 3d action recognition for untrimmed\nnaturalistic driving videos. In 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition Workshops\n(CVPRW), pages 3167–3172, 2022. 1\n[25] Matthew Veres and Medhat Moussa. Deep learning for intel-\nligent transportation systems: A survey of emerging trends.\nIEEE Transactions on Intelligent Transportation Systems, 21\n(8):3152–3168, 2020. 1\n[26] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool. Temporal segment\nnetworks: Towards good practices for deep action recogni-\ntion. Computer Vision – ECCV 2016, page 20–36, 2016. 2\n[27] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yi-\nnan He, Yi Wang, Yali Wang, and Yu Qiao.\nVideomae\nv2: Scaling video masked autoencoders with dual masking.\n2023 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2023. 2\n[28] Xuesong Wang, Rongjiao Xu, Siyang Zhang, Yifan Zhuang,\nand Yinhai Wang. Driver distraction detection based on ve-\nhicle dynamics using naturalistic driving data. Transporta-\ntion Research Part C: Emerging Technologies, 136:103561,\n2022. 1\n[29] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun\nHuang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun\nWang, and et al. Internvideo: General video foundation mod-\nels via generative and discriminative learning, 2022. 2\n[30] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan\nYuille, and Christoph Feichtenhofer. Masked feature predic-\ntion for self-supervised visual pre-training. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14668–14678, 2022. 2\n[31] Huijuan Xu, Abir Das, and Kate Saenko. R-c3d: Region con-\nvolutional 3d network for temporal activity detection. 2017\nIEEE International Conference on Computer Vision (ICCV),\n2017. 2\n[32] Junping Zhang, Fei-Yue Wang, Kunfeng Wang, Wei-Hua\nLin, Xin Xu, and Cheng Chen. Data-driven intelligent trans-\nportation systems: A survey. IEEE Transactions on Intelli-\ngent Transportation Systems, 12(4):1624–1639, 2011. 1\n[33] Hangyue Zhao, Yuchao Xiao, and Yanyun Zhao.\nPand:\nPrecise action recognition on naturalistic driving. In 2022\nIEEE/CVF Conference on Computer Vision and Pattern\n\nRecognition Workshops (CVPRW), pages 3290–3298, 2022.\n1\n[34] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xi-\naoou Tang, and Dahua Lin. Temporal action detection with\nstructured segment networks. International Journal of Com-\nputer Vision, 128(1):74–95, 2019. 2\n[35] Wei Zhou, Yinlong Qian, Zequn Jie, and Lin Ma.\nMulti\nview action recognition for distracted driver behavior local-\nization. In 2023 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition Workshops (CVPRW), pages 5375–\n5380, 2023. 1",
    "pdf_filename": "Rethinking_Top_Probability_from_Multi-view_for_Distracted_Driver_Behaviour_Localization.pdf"
}