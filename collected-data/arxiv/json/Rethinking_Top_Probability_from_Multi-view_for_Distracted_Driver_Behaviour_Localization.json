{
    "title": "Rethinking Top Probability from Multi-view for Distracted Driver Behaviour",
    "abstract": "inside the vehicle. This challenge involves analyzing syn- Naturalistic driving action localization task aims to chronizedvideorecordingsfromdriversengagedinvarious recognize and comprehend human behaviors and actions distracted driving activities. These activities are classified from video data captured during real-world driving sce- into different actions, such as using a phone, eating, and narios. Previousstudieshaveshowngreatactionlocaliza- reaching into the backseat, each of which can potentially tionperformancebyapplyingarecognitionmodelfollowed leadtoaccidents. by probability-based post-processing. Nevertheless, the probabilities provided by the recognition model frequently contain confused information causing challenge for post- Previousstudies[12,13,24,33,35]havedemonstrated processing. In this work, we adopt an action recognition the effectiveness in distracted driving detection, typically modelbasedonself-superviselearningtodetectdistracted dividingthetaskintotwomainstages: activityrecognition activities and give potential action probabilities. Subse- and temporal action localization. However, several chal- quently,aconstraintensemblestrategytakesadvantagesof lenges remain: (1) The dataset is limited to 16 behavior multi-camera views to provide robust predictions. Finally, categories, leading to an insufficient diversity of samples weintroduceaconditionalpost-processingoperationtolo- within each category. (2) The models must discern vari- catedistractedbehavioursandactiontemporalboundaries ous actions from different perspectives within untrimmed precisely. ExperimentingontestsetA2,ourmethodobtains videos,facingdifficultiesindistinguishingsubtlevariations thesixthpositiononthepublicleaderboardoftrack3ofthe withinthesameclassanddetectingminordiscrepanciesbe- 2024AICityChallenge. tween certain classes. (3) The inclusion of the appearance block constrains the model’s ability to discern differences betweencertainclasses. (4)Previoussolutionsrelyheavily 1.Introduction on the classification model’s confidence, which can result in misclassifications when the highest and second-highest Distracted driving is defined as any circumstance where classeshavesimilarprobabilities. the driver diverts attention away from safe driving activi- ties. In the United States, over 3,500 lives are lost annu- allyduetoaccidentscausedbydistracteddriving. Research in intelligent transportation systems and distracted driving Therefore, inthispaper, weaimtocontributetothelit- has gained significant attention from scholars worldwide eratureinthefollowingmanners:First,weinheritanaction [19, 25, 28, 32]. This interest is fueled by the potential classification model in video based self-supervised learn- of naturalistic driving videos to capture real-time driving ingtodetectrobustdistractedactionsfromtheinputvideo. behaviorandthecapabilityofdeeplearningtoanalyzepo- Next, we apply a constraint ensemble strategy to take ad- tential risk factors. The AI City Challenge 2024 [3] aims vantageofthepowerofeachcameraview. Inthefinal,con- to advance research in this field by hosting a naturalistic ditionalpost-processingstepsconsidercontextsfromtop1 driving action recognition challenge. The given challenge andtop2confidencerankingtolocatedistractedactionsand focusesondetectingdistracteddrivingbehaviorsusingsyn- temporalboundariesaccurately. 4202 voN 91 ]VC.sc[ 1v52521.1142:viXra",
    "body": "Rethinking Top Probability from Multi-view for Distracted Driver Behaviour\nLocalization\nQuangVinhNguyen1,VoHoangThanhSon1,ChauTruongVinhHoang2,DucDuyNguyen3\nNhatHuyNguyenMinh2,Soo-HyungKim1\n1ChonnamNationalUniversity. 2Vietnamese-GermanUniversity.\n3HanoiUniversityofScienceandTechnology.\n{vinhbn28,hoangsonvothanh,shkim}@jnu.ac.kr\n{16076, 10423045}@student.vgu.edu.vn,duy.nd223435@sis.hust.edu.vn\nAbstract theticnaturalisticdatacollectedfromthreecameralocations\ninside the vehicle. This challenge involves analyzing syn-\nNaturalistic driving action localization task aims to chronizedvideorecordingsfromdriversengagedinvarious\nrecognize and comprehend human behaviors and actions distracted driving activities. These activities are classified\nfrom video data captured during real-world driving sce- into different actions, such as using a phone, eating, and\nnarios. Previousstudieshaveshowngreatactionlocaliza- reaching into the backseat, each of which can potentially\ntionperformancebyapplyingarecognitionmodelfollowed leadtoaccidents.\nby probability-based post-processing. Nevertheless, the\nprobabilities provided by the recognition model frequently\ncontain confused information causing challenge for post-\nPreviousstudies[12,13,24,33,35]havedemonstrated\nprocessing. In this work, we adopt an action recognition\nthe effectiveness in distracted driving detection, typically\nmodelbasedonself-superviselearningtodetectdistracted\ndividingthetaskintotwomainstages: activityrecognition\nactivities and give potential action probabilities. Subse-\nand temporal action localization. However, several chal-\nquently,aconstraintensemblestrategytakesadvantagesof\nlenges remain: (1) The dataset is limited to 16 behavior\nmulti-camera views to provide robust predictions. Finally,\ncategories, leading to an insufficient diversity of samples\nweintroduceaconditionalpost-processingoperationtolo-\nwithin each category. (2) The models must discern vari-\ncatedistractedbehavioursandactiontemporalboundaries\nous actions from different perspectives within untrimmed\nprecisely. ExperimentingontestsetA2,ourmethodobtains\nvideos,facingdifficultiesindistinguishingsubtlevariations\nthesixthpositiononthepublicleaderboardoftrack3ofthe\nwithinthesameclassanddetectingminordiscrepanciesbe-\n2024AICityChallenge.\ntween certain classes. (3) The inclusion of the appearance\nblock constrains the model’s ability to discern differences\nbetweencertainclasses. (4)Previoussolutionsrelyheavily\n1.Introduction\non the classification model’s confidence, which can result\nin misclassifications when the highest and second-highest\nDistracted driving is defined as any circumstance where\nclasseshavesimilarprobabilities.\nthe driver diverts attention away from safe driving activi-\nties. In the United States, over 3,500 lives are lost annu-\nallyduetoaccidentscausedbydistracteddriving. Research\nin intelligent transportation systems and distracted driving Therefore, inthispaper, weaimtocontributetothelit-\nhas gained significant attention from scholars worldwide eratureinthefollowingmanners:First,weinheritanaction\n[19, 25, 28, 32]. This interest is fueled by the potential classification model in video based self-supervised learn-\nof naturalistic driving videos to capture real-time driving ingtodetectrobustdistractedactionsfromtheinputvideo.\nbehaviorandthecapabilityofdeeplearningtoanalyzepo- Next, we apply a constraint ensemble strategy to take ad-\ntential risk factors. The AI City Challenge 2024 [3] aims vantageofthepowerofeachcameraview. Inthefinal,con-\nto advance research in this field by hosting a naturalistic ditionalpost-processingstepsconsidercontextsfromtop1\ndriving action recognition challenge. The given challenge andtop2confidencerankingtolocatedistractedactionsand\nfocusesondetectingdistracteddrivingbehaviorsusingsyn- temporalboundariesaccurately.\n4202\nvoN\n91\n]VC.sc[\n1v52521.1142:viXra\n2.RelatedWork action recognition model, an ensemble strategy, and con-\nditional post-processing. The first is an action recogni-\n2.1.ActionRecognition\ntionmodelwhichisself-supervisedlearning,recognizedis-\nAction recognition is a crucial task in the field of video tracted driver behaviors from input short videos. The sec-\nunderstanding. Over the years, there have been numerous ondisanensemblestrategybeingresponsibleforintegrat-\nstudies and extensive research conducted in this area. The ing multi-view predictions. Given recognition probabili-\nmaingoaloftheactionrecognitionistoclassifyatrimmed ties,conditionalpost-processingconsidersdiversecontexts\nvideo into specific action classes using end-to-end deep to smooth out detected activities and localize the temporal\nlearning methods. There have been significant updates in boundaryaccurately. Detaileddescriptionsofeachcompo-\narchitecture design, ranging from 2D-based CNN models nentarepresentedinthefollowingsubsections.\nand3D-basedCNNmodelstoTransformer-basedmodels.\n3.1.ActionRecognition\n2D-based action recognition methods first implement a\nCNN model to extract spatial features for each frame in Recent researches have demonstrated that self-supervised\nthe video. The sequence models[6, 26] are employed to learning (SSL) can provide more robust [10] and general\nfuse these features with the aim of capturing temporal in- features [4, 8, 11], while reducing the amount of data re-\nformation. 3D-CNN attempts[2, 7, 23] to process spatial- quiredforanequivalentsupervision-basedpre-training. In\ntemporal information directly by using 3D input tensors, thecontextofvideounderstanding,self-supervisedlearning\nwhere 2 dimensions represent space and 1 dimension rep- techniques seek to take advantage of the temporal coher-\nresentstime. ThesuccessofTransformerinimage-related enceandspatialcorrelationsseeninvideosequences.These\nand sequential tasks and has motivated the exploration of approachesareparticularlysuitableforscenarioswherela-\nits potential in video recognition, [15, 18] have been suc- beleddataisscarceorexpensivetoobtainasthedistracted\ncessfullydevelopedtouseTransformerinthearchitecture. driverbehaviordataset. Inspiredbythesuccessfulstudyof\nRecent works also take advantages of large video founda- MaskingModelinginthetextandpicturedomain[5,9,30].\ntionpre-trainingmodelstoimproveperformance. Masking VideoMAE [22] employs Masked Autoencoders, a varia-\nwith high ratio or scaling transformer model by applying tionontraditionalAutoencoderswherecertainpartsofthe\nself-supervisedlearning,[22,27,29]haveshowngreatpo- inputdataaremaskedoutduringtraining,encouragingthe\ntentialinextractingrobustvideorepresentation model to learn useful representations that capture the un-\nderlying structure of the data leading to promising perfor-\n2.2.TemporalActionLocalization\nmance in a variety of video understanding tasks. Our sys-\nTemporal action localization is the task of automatically teminheritsthisstructuretoclassifydistractedactionfrom\nidentifyingthetimedurationduringwhichanactionoccurs naturalistic driving videos. Specifically, input videos with\nwithinanuntrimmedvideoanddeterminingitscorrespond- FPS30aretrimmedintoaseriesofshortvideoscontaining\ningactioncategory. Theconventionaltwo-stagemethodin- 64frames.Themodelachievesshortvideosasinputtogive\nvolvesproposingactionsegmentsinitiallyandsubsequently theprobabilityforeachclassintheoutput.\nclassifying these proposals into their respective action cat-\negories [16, 17, 20, 31]. However, a major drawback of 3.2.Multi-viewEnsembleStrategy\nthismethodisthattheboundariesofactioninstancesremain\nThedistracteddriveractionisdividedintosixteendistracted\nfixedduringtheclassificationprocess.Asaresult,whilethe\nactions and three views of the camera mounted in the car:\nmethodcanidentifytimeintervalslikelytocontainactions,\ndashboard, rearview and rightside. Each of these views\nitlackstheabilitytopreciselydeterminetheexactstartand\nhas significance in different contexts. Dashboard view di-\nendtimesoftheactions.\nrectly facing driver contributes clearly to actions: ”phone\nIncontrast,one-stagemethodshavegarneredsignificant\ncallbyrighthand”,”drink”,”eating”oractivitiesinvolving\nattention by integrating the localization and classification\nto the movement of body-head such as ”talk with passen-\ntaskswithinasinglenetwork. Thisapproacheliminatesthe\nger”,”pickupfromfloor”. Rearviewgivesabroaderspace\nfixed boundaries issue and offers a more streamlined solu-\nviewinsidethecar,andisusefulforidentifyingvariousac-\ntion. Previousworkshaveseentheadoptionofhierarchical\ntions:”phonecallbyrighthandorlefthand”,”reachingbe-\narchitectures based on CNN [14, 16, 34]. Recent studies\nhind”or”handonhead”. Whiletherightsideviewshows\n[1, 21] extract a video representation with a Transformer-\nadifferentview,fromtherightsideofthedriver,thisview\nbasedencoder.\nis helpful for hand movements: ”control the panel”, ”text\nbyhand”or”pickfromfloor(Passenger)”. Inaddition,sev-\n3.Method\neralspecificclasses:”talkwithpassenger”,”pickfromfloor\nAsindicatedinFig.1,ourdistracteddriverbehaviourrecog- (Driver)”canbeintegratedbyallviewstocomprehendthe\nnitionsystemconsistsofthreemainnovelcomponents: an overallcontextofdistracteddriving. Therefore,inorderto\n64\n64 Dashboard\n0\n1\nVideo\nClassification 14\n15 Conditional Post-processing\nTop 1\nRightside\n0 0\n1\n1\nVideo Ensemble\nClassification 14 14\n15\n15\nTop 2\nRearview\n0\n1\nVideo\nClassification Final Action Localization\n14\n15\nFigure1.DistractedDriverBehaviourRecognitionSystem\n3.3.ConditionalPost-Processing\nClass 2. Phone Call (Right)\nTheactionrecognitionmodelclassifiesshortvideoswhich\nare trimmed from input video to give a series of probabil-\nClass 1. Drinking ity. Outputprobabilityisanarrayofpredictionvectorswith\nDashboard Class 4. Eating\nClass 13. Yawning thelengthof16correspondingtoanumberofclasses. El-\nements with the highest value in vectors refer to predicted\nclasses. Andelementswithsecondhighestvaluenormally\nClass 5. Text (Right)\nClass 6. Text (Left) express potential classes which are the second most trust-\nRightside Class 8. Yawning (Adjust control panel)\nClass 10. Pick up from floor (Passenger) worthyafterthehighestones. Ourpost-processingstrategy\nClass 15. Singing or dancing with music\nleveragestop1andtop2ofoutputprobabilitytolocatethe\nactions and time boundary more accurately. This process\nRearview Class 3. Phone Call (Left) consists of three main steps: Conditional Merging, Condi-\nClass 14. Hand on head\ntionalDecisionandMissingLabelsRestoring.\nClass 0. Normal ConditionalMerging. Thefirstoperationreferstocondi-\nClass 9. Pick up from floor (Driver) tionalmerging,whichisdepictedinFig.3.Insteadofmerg-\nClass 11. Talk to passenger at the right\nClass 12. Talk to passenger at backseat ing closer actions normally, this component considers the\ncontext of one certain class and neighbor classes by top 1\nClass 7. Reaching behind\nandtop2confidencerankingtomergepotentialcandidates\nand remove noise classes. To explain symbols in Fig. 3,\nFigure2.Ensemblestrategy ”second”representsthetimeboundaryforeachaction,the\nvaluesintheboxesrefertotheprobabilitiesforeachtype.\nTop1istheclasswiththehighestprobabilityscore, while\ntop2representstheclasswiththesecondhighestprobabil-\nenhancerecognitionperformance,wesuggestanensemble ity.\nstrategy based on multi-view. The specifics of ensemble ConditionalDecision. Fig.4describestheconditionalde-\nstrategyaredisplayedinFig.2. cisionoperationwhichselectsareliabletimesegmentation\nSecond 28 29 30 31 32 33 Miss Labels\nClass 13\nclass 14 14 0 15 15 15 Class 12\n0 0.01 0.02 0.3 0.04 0.02 0.03 Retrival\nTop 1 0 0 0 0 11 7 7 11 0 0\nTop 2 0 13 13 13 12 12 12 12 0 0\n14 0.4 0.38 0.03 0.02 0.07 0.01\n15 0.3 0.35 0.25 0.8 0.76 0.83 Figure5.MissingLabelsRestoring\nTop 1 14 14 0 15 15 15 99 individual drivers over a total of 90 hours. Each driver\nTop 2 15 15 15 11 14 0 is recorded performing a series of 16 different distracting\nTop 2(>0.2) 15 15 15 na na na activities randomly, with the order of these activities also\nrandomized within each video. To ensure a holistic view\nof the driving scenario, the dataset employs three cameras\nNew class 15 15 15 15 15 15\nsimultaneously recording from different angles within the\ncar. Notably,eachdriverundergoestworoundsofdatacol-\nFigure3.ConditionalMerging lection: one without any form of distraction and another\nwithapredetermineddistractor,suchassunglassesorahat.\nSecond 69 70 71 72 196 197 198 199 Thisdesignallowsforathoroughexaminationofdriverbe-\nClass 7 7 7 7 7 7 7 7\n0 0.05 0.06 0.09 haviorundervaryinglevelsofdistraction,offeringvaluable\ninsights into the impact of external factors on driving per-\n7 0.89 0.82 0.81 0.76 0.58 0.64 0.41 0.55\nformance. The videos from the 2024 AI City Challenge’s\n12 0.34 0.23 0.28 0.35 Track3onNaturalisticDrivingActionRecognitionaresep-\naratedintotwodatasets: ”A1”fortraining,”A2”fortesting\n15 0.1\nwith the training dataset ”A1” containing the ground truth\nlabels for the start time, end time, and types of distracted\nTop 1 7 7 7 7 7 7 7 7\nTop 2 0 0 15 0 12 12 12 12 actions.\nTop 2(>0.2) na na na na 12 12 12 12\nIf 12 in Miss Labels 4.2.EvaluationMatric\nNew class 7 7 7 7 12 12 12 12\nDecision ActionRecognition. Actionclassificationinvolvesthetask\nofassigningalabelorcategorytoavideobasedonitscon-\nFigure4.ConditionalDecision tent.Theaccuracyscoreiscalculatedbycomparingthepre-\ndictedclasslabelswiththegroundtruthlabelsforallvideos.\nAhigheraccuracyscoreindicatesbetterperformanceofthe\nfromdifferentsegmentsofthesameclasses. Givenseveral\nvideo classification model in correctly predicting the class\ndifferentsegmentsofthesameclass,forexample,thereare\nlabelsofvideos. Theaccuracyisdefinedas:\ntwo segments of class 7 ”reaching behind” in Fig. 4. The\ndecisionmodulereliesonprobabilitiesfromtop1andtop2\nNumberofCorrectPredictions\ntofilteramosttrustworthysegment. Accuracy = X100% (1)\nTotalNumberofPredictions\nMissingLabelsRestoring. Afterthetwomentionedabove\nsteps, it still has some classes that are missing or not de-\n”Number of Correct Predictions” is the number of in-\ntected by the top 1 prediction. It means that if we just use\nstancesthatarecorrectlyclassifiedbytheclassifier. ”Total\ntop1probabilityforoutputprediction,thesystemcouldnot\nNumberofPredictions”isthetotalnumberofinstancesin\nlocalize distracted actions sufficiently. The restoring mod-\nthedataset.\nuleshowninFig.5findstheseclassestoreproducethefinal\nTemporalActionLocalization. Fortemporalactionlocal-\npredictionwithenough16classes.\nization, activity overlap measure (os) quantifies the degree\nofoverlapbetweenthepredictedtemporalsegmentandthe\n4.Experiments\nground truth annotation for a particular action or activity\n4.1.Dataset withinavideosequence.\nThe distracted driver behavior dataset provides a compre-\nIntersection\nhensivecollectionofdrivingvideoscapturingtheactionsof os= (2)\nUnion\nFold View 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Avg\nDash 1.00 0.94 0.94 0.80 0.90 0.98 0.89 0.97 0.75 0.81 0.84 0.86 0.75 0.95 0.81 0.88\n1 Rear 1.00 0.96 0.99 0.81 1.00 0.91 0.96 0.94 0.84 0.89 0.70 0.82 0.82 0.96 0.80 0.89\nRight 1.00 0.99 0.96 0.66 0.94 0.92 0.86 0.96 0.86 0.81 0.85 0.70 0.80 0.90 0.80 0.88\nEnsemble 1.00 0.96 0.99 0.80 0.94 0.92 0.97 0.96 0.82 0.81 0.86 0.88 0.75 0.96 0.80 0.90\nDash 1.00 0.88 0.98 0.70 0.96 0.95 0.90 0.86 0.76 0.66 0.76 0.68 0.61 0.96 0.88 0.84\n2 Rear 1.00 0.94 0.97 0.67 0.96 0.96 0.95 0.83 0.80 0.75 0.64 0.74 0.60 0.98 0.84 0.84\nRight 0.96 0.85 0.91 0.27 0.96 1.00 0.93 0.99 0.81 0.75 0.74 0.58 0.42 0.84 0.82 0.79\nEnsemble 1.00 0.95 0.97 0.70 0.96 1.00 0.95 0.99 0.80 0.75 0.70 0.72 0.61 0.98 0.82 0.86\nDash 1.00 0.98 0.68 0.80 0.88 0.88 0.88 0.93 0.78 0.61 0.60 0.68 0.87 0.97 0.77 0.82\n3 Rear 1.00 1.00 0.63 0.83 0.94 0.97 0.86 0.98 0.85 0.76 0.69 0.71 0.81 0.97 0.75 0.85\nRight 0.88 0.99 0.60 0.75 0.99 0.92 0.88 0.98 0.92 0.76 0.72 0.67 0.63 0.97 0.76 0.83\nEnsemble 1.00 1.00 0.63 0.80 0.99 0.90 0.88 0.98 0.84 0.75 0.64 0.70 0.87 0.97 0.76 0.85\nDash 0.92 0.97 0.97 0.81 0.96 0.90 0.85 0.88 0.76 0.17 0.86 0.68 0.86 0.93 0.85 0.78\n4 Rear 0.89 0.96 0.99 0.81 0.92 0.80 0.82 0.85 0.73 0.15 0.86 0.65 0.84 0.94 0.86 0.80\nRight 0.94 0.97 0.90 0.67 0.97 0.92 0.91 0.97 0.86 0.20 0.77 0.66 0.60 0.95 0.83 0.81\nEnsemble 0.92 0.97 0.99 0.96 0.97 0.92 0.90 0.97 0.88 0.20 0.86 0.66 0.86 0.94 0.83 0.86\nDash 0.94 0.91 0.95 0.79 0.87 0.84 0.78 0.78 0.85 0.79 0.80 0.69 0.82 0.88 0.80 0.83\n5 Rear 0.87 0.96 0.93 0.66 0.96 0.94 0.93 0.93 0.85 0.79 0.82 0.68 0.92 0.97 0.84 0.87\nRight 0.94 0.98 0.88 0.46 0.88 0.96 0.91 0.97 0.89 0.88 0.56 0.77 0.73 0.95 0.84 0.84\nEnsemble 0.94 0.95 0.93 0.79 0.88 0.96 0.96 0.97 0.86 0.90 0.82 0.70 0.82 0.97 0.84 0.89\nTable1.Theaccuracyonthevalidationsetofeach5-Foldsplitindifferentclasses.\nClass Top 1\n14 14 15\n12 12\n9 10\n5 6 7 7 7 8 4 4 4 11\n5 1 2 3 2\n0\n14 14 14 Top 2 15\n9\n13 12 12\n8\n1112 12\n6 7 7\n5 55 5 4 7 3 3 4 4 4 4 4\nFinal\n14 13 15\n9 12 10 11\n6 7 8\n5\n3 4 2\n1\nSecond\nConditional Merging Conditional Decision Missing Label Restoring\nFigure6.ActionlocalizationresultofConditionalPost-Processing\nIntersection is the duration of time that is common to is consistent with the model described in reference [22].\nboththepredictedsegmentandthegroundtruthannotation. In particular, we use a standard Vision Transformer (ViT)\nUnionisthetotalamountoftimecoveredbyboththepre- model as the foundation. Each input video trimmed with\ndictedsegmentandthegroundtruthannotation. stride 30 frames comprises 64 frames, sampled 16 frames\nevenlyspacedpervideo.Trainingprocessisconductedwith\n4.3.ImplementDetail alearningrateof2x10-3over20epochsforeachcamera\nview.\nThe methodology employed relies on the PyTorch frame-\nwork, apubliclyavailabletoolboxwidelyusedinmachine 4.4.Results\nlearning research. All experimentation was conducted on\na high-performance workstation equipped with two RTX ActionRecognition.ThetrainingdatasetA1isdividedinto\n3090 graphics card boasting 48GB of memory. For the 5folds. Wevalidateeachofthefoldsinallthreeviewsof\nvideo classification task, the network architecture utilized thecamera. ResultsinTab.1illustratetheeffectofeachof\nviews on different classes. As can be seen, the right side 5.Conclusion\nviewoftengivesexcellentaccuracyinseveralclassessuch\nIn this work, we have suggested a conditional recogni-\nas class 8 (control the panel), class 10 (pick up from floor\ntion systemfor thedistracted driverbehaviour localization\nofpassenger),orclass5,6(text)becausethisviewisexpert\ntask. First, our method uses a pre-trained action recogni-\nin these classes more than rear view and dashboard view.\ntion model that was trained by self-supervised learning to\nBesides, the dashboard view contributes greatly to class 1\nidentify distracted activities in video input. After that, a\n(drink), class 4(eat), or class 13(yawning) and often is the\nmulti-viewensemblestrategyisadoptedtoleveragethead-\nbest.Inaddition,therearviewstronglyaffectsperformance\nvantages of each camera view. Given output probabilities,\nof class 3 (phone call by left hand), and class 14(hand on\nwepost-processingbyconditionalmerging,conditionalde-\nhead). Ourensemblestrategyimprovesandsurpassessitu-\ncision, and missing labels restoring operation to recognize\nationswithonlyasingleview. Resultsineachofthefolds\nthedistractedactionsandlocatetimeboundaryaccurately.\nfluctuateanddependonthechallengeofthevalidationset.\nConsequently, we achieved the sixth rank score in test set\nTemporal Action Localization. The proposed method is\n”A2”, surpassing methods ranked lower while remaining\ntrainedontheA1datasetprovidedbythecompetition,and\nveryclosetothetopranking.\ntestedonpublictestdatasetA2toevaluatetemporalaction\nlocalization performance. As indicated in Tab. 2, our ap-\n6.Acknowledgement\nproach ranks 6th on the leaderboard with a 0.76 os score,\noutperforms 7th by almost 8% score and is far ahead of This work was supported by the National Research\ncompetitors beneath. Besides, our solution is not much Foundation of Korea(NRF) grant funded by the Ko-\nlowerthanthetop-rankmethods. Thisprovestheeffective- rea government(MSIT) (RS- 2023-00219107). This\nness and potential of introduced method in the distracted work also was supported by Institute of Information\ndriverbehaviourrecognitionchallenge. Fig.6depictspost- & communications Technology Planning & Evaluation\n(IITP) under the Artificial Intelligence Convergence In-\nprocessing operation in detail, Horizontal axis denotes for\nnovation Human Resources Development (IITP-2023-RS-\ntime variable (second), and vertical axis refers to classes\n2023-00256629) grant funded by the Korea govern-\n(from 0 to 15). Numbers on top of bars in the Fig. 6 ex-\nment(MSIT)”\npress corresponding classes. The top 1 chart shows pre-\ndiction given by highest confidence probability, while the\nReferences\ntop 2 illustrates second reliable classes. As can be seen in\nthetop1chart,thepredictedlabelsattachwithmanynoisy [1] Shai Avidan, Gabriel Brostow, Moustapha Cissee´, Gio-\nlabelscausingconfusiontoactionrecognitionandlocaliza- vanni Maria Farinella, and Tal Hassner. Springer Nature\ntion.Theproposedpost-processoperationconsidersthetop Switzerland,2022. 2\n1, top 2 probabilities, applies conditional merging, condi- [2] Joao Carreira and Andrew Zisserman. Quo vadis, action\ntional decision and missing label restoring to smooth and recognition? a new model and the kinetics dataset. 2017\nlocalizeaccuratelydistractedactionprediction. Fig.6indi- IEEEConferenceonComputerVisionandPatternRecogni-\ntion(CVPR),2017. 2\ncatesthatourfinalresultisseamlessandsuperiortothetop\n[3] AICityChallenge. 2024. 1\n1 prediction. This demonstrates that our post-processing\n[4] XinleiChenandKaimingHe.Exploringsimplesiameserep-\nstrategy help model make decisions accurately and effec-\nresentationlearning. CoRR,abs/2011.10566,2020. 2\ntivelylocalizetemporalboundaries.\n[5] Jacob Devlin. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805,2018. 2\nRank TeamIDScore\n[6] Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama,\n1 155 0.8282 MarcusRohrbach,SubhashiniVenugopalan,TrevorDarrell,\n2 189 0.8213 and Kate Saenko. Long-term recurrent convolutional net-\n3 32 0.8149 works for visual recognition and description. 2015 IEEE\n4 207 0.8045 Conference on Computer Vision and Pattern Recognition\n5 5 0.7798 (CVPR),2015. 2\n6 136 0.7625 [7] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaimingHe. Slowfastnetworksforvideorecognition. 2019\n7 17 0.6844\nIEEE/CVF International Conference on Computer Vision\n8 165 0.6080\n(ICCV),2019. 2\n9 156 0.5963\n[8] Jean-Bastien Grill, Florian Strub, Florent Altche´, Corentin\n10 125 0.2307\nTallec,PierreRichemond,ElenaBuchatskaya,CarlDoersch,\nBernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\nTable2.Leaderboardofchallengetrack.\nlaghiAzar,etal. Bootstrapyourownlatent-anewapproach\ntoself-supervisedlearning. Advancesinneuralinformation Conference on Computer Vision and Pattern Recognition\nprocessingsystems,33:21271–21284,2020. 2 (CVPR),2021. 2\n[9] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr [21] DingfengShi,YujieZhong,QiongCao,LinMa,JiaLit,and\nDolla´r,andRossGirshick.Maskedautoencodersarescalable Dacheng Tao. Tridet: Temporal action detection with rel-\nvisionlearners.InProceedingsoftheIEEE/CVFConference ative boundary modeling. 2023 IEEE/CVF Conference on\nonComputerVisionandPatternRecognition(CVPR),pages ComputerVisionandPatternRecognition(CVPR),2023. 2\n16000–16009,2022. 2 [22] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.\n[10] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Videomae: Maskedautoencodersaredata-efficientlearners\nDawn Song. Using self-supervised learning can improve forself-supervisedvideopre-training,2022. 2,5\nmodel robustness and uncertainty. Advances in neural in- [23] DuTran,LubomirBourdev,RobFergus,LorenzoTorresani,\nformationprocessingsystems,32,2019. 2 andManoharPaluri. Learningspatiotemporalfeatureswith\n[11] DaeheeKim,YoungjunYoo,SeunghyunPark,JinkyuKim, 3d convolutional networks. 2015 IEEE International Con-\nandJaekooLee. Selfreg: Self-supervisedcontrastiveregu- ferenceonComputerVision(ICCV),2015. 2\nlarizationfordomaingeneralization. InProceedingsofthe [24] ManhTungTran,MinhQuanVu,NgocDuongHoang,and\nIEEE/CVF International Conference on Computer Vision, Khac-Hoai Nam Bui. An effective temporal localization\npages9619–9628,2021. 2 methodwithmulti-view3dactionrecognitionforuntrimmed\n[12] Huy Duong Le, Minh Quan Vu, Manh Tung Tran, and naturalisticdrivingvideos. In2022IEEE/CVFConference\nNguyen Van Phuc. Triplet temporal-based video recogni- on Computer Vision and Pattern Recognition Workshops\ntion with multiview for temporal action localization. In (CVPRW),pages3167–3172,2022. 1\n2023 IEEE/CVF Conference on Computer Vision and Pat- [25] MatthewVeresandMedhatMoussa.Deeplearningforintel-\ntern Recognition Workshops (CVPRW), pages 5428–5434, ligenttransportationsystems: Asurveyofemergingtrends.\n2023. 1 IEEETransactionsonIntelligentTransportationSystems,21\n[13] Rongchang Li, Cong Wu, Linze Li, Zhongwei Shen, (8):3152–3168,2020. 1\nTianyang Xu, Xiao-Jun Wu, Xi Li, Jiwen Lu, and Josef [26] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nKittler. Action probability calibration for efficient natural- Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment\nistic driving action localization. In 2023 IEEE/CVF Con- networks: Towardsgoodpracticesfordeepactionrecogni-\nferenceonComputerVisionandPatternRecognitionWork- tion. ComputerVision–ECCV2016,page20–36,2016. 2\nshops(CVPRW),pages5270–5277,2023. 1 [27] LiminWang,BingkunHuang,ZhiyuZhao,ZhanTong,Yi-\n[14] YanLi,BinJi,XintianShi,JianguoZhang,BinKang,and nan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae\nLiminWang. Tea: Temporalexcitationandaggregationfor v2: Scalingvideomaskedautoencoderswithdualmasking.\naction recognition. 2020 IEEE/CVF Conference on Com- 2023 IEEE/CVF Conference on Computer Vision and Pat-\nputerVisionandPatternRecognition(CVPR),2020. 2 ternRecognition(CVPR),2023. 2\n[15] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Man- [28] XuesongWang,RongjiaoXu,SiyangZhang,YifanZhuang,\ngalam, Bo Xiong, Jitendra Malik, and Christoph Feichten- andYinhaiWang. Driverdistractiondetectionbasedonve-\nhofer. Mvitv2: Improvedmultiscalevisiontransformersfor hicledynamicsusingnaturalisticdrivingdata. Transporta-\nclassificationanddetection. 2022IEEE/CVFConferenceon tionResearchPartC:EmergingTechnologies,136:103561,\nComputerVisionandPatternRecognition(CVPR),2022. 2 2022. 1\n[16] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift [29] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun\nmoduleforefficientvideounderstanding. 2019IEEE/CVF Huang,ZhiyuZhao,HongjieZhang,JilanXu,YiLiu,Zun\nInternationalConferenceonComputerVision(ICCV),2019. Wang,andetal.Internvideo:Generalvideofoundationmod-\n2 elsviagenerativeanddiscriminativelearning,2022. 2\n[17] TianweiLin,XiaoLiu,XinLi,ErruiDing,andShileiWen. [30] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan\nBmn: Boundary-matchingnetworkfortemporalactionpro- Yuille,andChristophFeichtenhofer. Maskedfeaturepredic-\nposalgeneration. 2019IEEE/CVFInternationalConference tionforself-supervisedvisualpre-training.InProceedingsof\nonComputerVision(ICCV),2019. 2 theIEEE/CVFConferenceonComputerVisionandPattern\n[18] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Recognition,pages14668–14678,2022. 2\nStephen Lin, and Han Hu. Video swin transformer. 2022 [31] HuijuanXu,AbirDas,andKateSaenko.R-c3d:Regioncon-\nIEEE/CVF Conference on Computer Vision and Pattern volutional3dnetworkfortemporalactivitydetection. 2017\nRecognition(CVPR),2022. 2 IEEEInternationalConferenceonComputerVision(ICCV),\n[19] ZhihanLv,ShaobiaoZhang,andWenqunXiu. Solvingthe 2017. 2\nsecurity problem of intelligent transportation system with [32] Junping Zhang, Fei-Yue Wang, Kunfeng Wang, Wei-Hua\ndeeplearning. IEEETransactionsonIntelligentTransporta- Lin,XinXu,andChengChen. Data-drivenintelligenttrans-\ntionSystems,22(7):4281–4290,2021. 1 portationsystems: Asurvey. IEEETransactionsonIntelli-\n[20] Zhiwu Qing, Haisheng Su, Weihao Gan, Dongliang Wang, gentTransportationSystems,12(4):1624–1639,2011. 1\nWeiWu,XiangWang,YuQiao,JunjieYan,ChangxinGao, [33] Hangyue Zhao, Yuchao Xiao, and Yanyun Zhao. Pand:\nand Nong Sang. Temporal context aggregation network Precise action recognition on naturalistic driving. In 2022\nfor temporal action proposal refinement. 2021 IEEE/CVF IEEE/CVF Conference on Computer Vision and Pattern\nRecognitionWorkshops(CVPRW),pages3290–3298,2022.\n1\n[34] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xi-\naoouTang,andDahuaLin. Temporalactiondetectionwith\nstructuredsegmentnetworks. InternationalJournalofCom-\nputerVision,128(1):74–95,2019. 2\n[35] Wei Zhou, Yinlong Qian, Zequn Jie, and Lin Ma. Multi\nviewactionrecognitionfordistracteddriverbehaviorlocal-\nization. In2023IEEE/CVFConferenceonComputerVision\nandPatternRecognitionWorkshops(CVPRW),pages5375–\n5380,2023. 1",
    "pdf_filename": "Rethinking_Top_Probability_from_Multi-view_for_Distracted_Driver_Behaviour_Localization.pdf"
}