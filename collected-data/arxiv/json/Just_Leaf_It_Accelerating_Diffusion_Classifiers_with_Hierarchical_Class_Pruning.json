{
    "title": "Just Leaf It Accelerating Diffusion Classifiers with Hierarchical Class Pruning",
    "abstract": "Diffusion models, known for their generative capabilities, have recently shown unexpected potential in image classifica- tion tasks by using Bayes’ theorem. However, most diffusion classifiers require evaluating all class labels for a single classification, leading to significant computational costs that can hinder their application in large-scale scenarios. To address this, we present a Hierarchical Diffusion Classifier (HDC) that exploits the inherent hierarchical label structure of a dataset. By progressively pruning irrelevant high-level categories and refining predictions only within relevant sub- categories, i.e., leaf nodes, HDC reduces the total number of class evaluations. As a result, HDC can accelerate infer- ence by up to 60% while maintaining and, in some cases, improving classification accuracy. Our work enables a new control mechanism of the trade-off between speed and preci- sion, making diffusion-based classification more viable for real-world applications, particularly in large-scale image classification tasks.",
    "body": "Just Leaf It: Accelerating Diffusion Classifiers with Hierarchical Class Pruning\nArundhati S. Shanbhag2,3, Brian B. Moser1,2,3, Tobias C. Nauen1,2,\nStanislav Frolov1,2, Federico Raue1, and Andreas Dengel1,2\n1German Research Center for Artificial Intelligence\n2University of Kaiserslautern-Landau\n3Equal Contribution\nfirst.last@dfki.de\nAbstract\nDiffusion models, known for their generative capabilities,\nhave recently shown unexpected potential in image classifica-\ntion tasks by using Bayes’ theorem. However, most diffusion\nclassifiers require evaluating all class labels for a single\nclassification, leading to significant computational costs that\ncan hinder their application in large-scale scenarios. To\naddress this, we present a Hierarchical Diffusion Classifier\n(HDC) that exploits the inherent hierarchical label structure\nof a dataset. By progressively pruning irrelevant high-level\ncategories and refining predictions only within relevant sub-\ncategories, i.e., leaf nodes, HDC reduces the total number\nof class evaluations. As a result, HDC can accelerate infer-\nence by up to 60% while maintaining and, in some cases,\nimproving classification accuracy. Our work enables a new\ncontrol mechanism of the trade-off between speed and preci-\nsion, making diffusion-based classification more viable for\nreal-world applications, particularly in large-scale image\nclassification tasks.\n1. Introduction\nGenerative models are designed to capture the full spec-\ntrum of a dataset distribution, offering a rich and detailed\nunderstanding of the data they represent [14,22]. This com-\nprehensive grasp of data allows them to not only create new\ncontent but also provide deep insights into their character-\nistics and structures [15,19]. Moreover, these insights can\nsignificantly benefit downstream tasks like image classifica-\ntion [4, 5,9]. Nonetheless, over the past decade, the focus\nof many generative techniques has predominantly been on\ncontent generation rather than harnessing their potential for\ndiscriminative tasks [2,11,18,26,27].\nAmong generative models, diffusion models emerge as\na particularly powerful subclass due to their ability to pro-\nduce exceptionally high-quality output images through an\nclass label \nclass label \nconsidered\nnot considered\nevaluates \nall classes\nprunes\nirrelevant\nclasses\nHierarchical\nDiffusion\nClassifier\n(ours)\nClassical\nDiffusion\nClassifier\nFigure 1. Comparison between the classical diffusion classifier\nand our proposed Hierarchical Diffusion Classifier (HDC). While\nthe classical approach evaluates all possible classes to find the cor-\nrect label, which leads to unnecessary computation, HDC prunes\nirrelevant classes early, focusing only on the most relevant candi-\ndates. This hierarchical pruning reduces computational overhead\nand accelerates inference.\niterative Markovian process of adding and removing noise\n[1, 12, 16, 18, 20]. Recently, the research community has\nshifted towards repurposing pre-trained diffusion models for\nclassification tasks in a zero-shot manner, signaling a piv-\notal move toward using generative models as discriminators,\nso-called diffusion classifiers [3, 6, 17]. More specifically,\ndiffusion models that learned p (x | c) can be easily con-\nverted into classifiers by exploiting the Bayes’ theorem to\nderive p (c | x). Thus, given an image x and a set of NC\npossible classes {ci}NC\ni=1, we can calculate the likelihood of\nx belonging to each class ci.\nIn practice, this means adding noise to x and estimating\nthe expected loss of noise reconstruction via Monte Carlo,\ni.e., through repeated calculations and averaging. This pro-\n1\narXiv:2411.12073v1  [cs.CV]  18 Nov 2024\n\ncedure is known as ε-prediction loss and has to be done for\neach class. Although Bayes’ Theorem elegantly adapts dif-\nfusion models for zero-shot use, meaning they can classify\nwithout any additional training, the scaling with N classes\nposes a considerable computational challenge for many prac-\ntitioners. This ε-prediction adds to the already high compu-\ntational costs associated with diffusion models [6,13,17,19].\nTo alleviate the computational burden, we propose an\nextension to diffusion classifiers that exploits a hierarchi-\ncal search over label trees rather than evaluating each la-\nbel individually, which we coined as Hierarchical Diffu-\nsion Classifier (HDC) and illustrated in Figure 1. In the\nfirst stage, termed pruning stage, HDC eliminates irrelevant\nbranches by traversing the label tree level-by-level and keep-\ning only the most promising synsets determined by the best\nε-predictions. In contrast to classical diffusion classification,\nthe ε-predictions use less computation steps for the Monte\nCarlo estimate to save additional runtime. Subsequently,\nHDC performs the classical diffusion classification on the\nremaining candidate leaf nodes. As a result, we can achieve\na speed-up of roughly 60%, saving hundreds of hours for\nImageNet-1K [7] while maintaining similar accuracy. More-\nover, HDC can achieve even better accuracy, i.e., 65.16% per\nclass instead of 64.90%, by using roughly the same calcula-\ntion time as traditional diffusion classifiers by using lower\npruning ratios in the first stage.\nOverall, HDC introduces a new controllable balance be-\ntween inference speed and classification accuracy by ad-\njusting pruning factors, enabling diffusion classifiers to be\nflexibly used in large-scale discriminative tasks. By adopting\na hierarchical approach, we provide a scalable and practical\nsolution for utilizing diffusion models in a wide range of\napplications beyond their original generative purpose. The\nmain contributions of our paper can be summarized by:\n1. We demonstrate that the inference time of zero-shot dif-\nfusion classifiers can be significantly accelerated with a\nhierarchical label search, reducing computational com-\nplexity while maintaining similar accuracy.\n2. We present a novel Hierarchical Diffusion Classifier\n(HDC) that leverages the label structure of ImageNet-\n1K to narrow down candidate classes efficiently.\n3. Our HDC framework achieves faster inference times\n(roughly 60%) while maintaining or improving classifi-\ncation performance compared to the classical diffusion\nmethod (i.e., 65.16% instead of 64.90%).\n2. Related Work\nDiffusion models have disrupted the landscape of gen-\nerative models, challenging the longstanding dominance of\nGANs [11,14] and setting a new standard in generating high-\nquality, realistic data [8]. Broadly speaking, they capture\ndata distributions by iteratively adding and removing noise\nwithin a Markovian process. Traditionally used to model\ndata distributions, these generative models have recently\nbeen explored for their discriminative capabilities as well.\nLi et al. [17] introduced diffusion classifiers by using\nStable Diffusion (SD) [23] as a zero-shot classifier, without\nthe need for additional training. SD, originally designed\nfor text-to-image generation and trained on a subset of the\nLAION-5B dataset [25], leverages its ability to synthesize\ndata to discriminate between images by evaluating prediction\nerrors across class labels, i.e., ε-predictions. More specif-\nically, they compute class scores based on differences in\npredicted and actual noise, offering an efficient classification\nmethod. Similar in spirit, Clark et al. [6] further explored\ndiffusion models like SD [23] and Imagen [24] for discrimi-\nnative tasks by aggregating score matrices across class labels\nand timesteps. Using a weighted score function, they assign\nimages to the class with the lowest aggregated score, demon-\nstrating the transferability of generative representations to\nclassification tasks.\nWhile enabling the zero-shot diffusion classification, both\napproaches consider all classes in order to classify a single\nimage, which is a computational challenge. Since infer-\nence time for zero-shot classification scales linearly with the\nnumber of classes, any computational improvement can sig-\nnificantly impact classifications on large-scale datasets like\nImageNet-1K [7]. Li et al. [17] address this by using a weak\ndiscriminative model to filter out obviously incorrect classes\nbefore performing zero-shot classification, thus speeding up\nthe process. Similarly, Clark et al. [6] employ a successive\nelimination strategy within a multi-armed bandit framework\nto iteratively narrow down the set of candidate classes.\nDespite some reduction in computational complexity, they\nstill process each class label at every diffusion timestep dur-\ning inference. In contrast, our work explores the poten-\ntial of leveraging the hierarchical structure of datasets like\nImageNet-1K. By integrating hierarchical pruning strategies,\nwe aim to progressively refine the set of candidate classes\nat each level of the hierarchy, allowing for faster and more\naccurate predictions.\n3. Methodology\nThis section provides a brief overview of diffusion clas-\nsifiers and introduces our proposed Hierarchical Diffusion\nClassifier (HDC), as shown in Figure 2 and outlined in Al-\ngorithm 1.\n3.1. Diffusion Classifier Preliminaries\nThe diffusion classifier is based on the formulation intro-\nduced by Li et al. [17]. The key idea is that, given a trained\ndiffusion model pθ, we can leverage the predictions of the\ndiffusion model, pθ(x | ci), to infer the probability of a\nclass ci given an input x using Bayes’ theorem to derive\n2\n\npθ(ci | x). This can be expressed as:\npθ(ci | x) =\np(ci) pθ(x | ci)\nNC\nP\nj=1\np(cj) pθ(x | cj)\n(1)\nHere, pθ(x | ci) is the likelihood of generating the input x\ngiven class ci, and p(ci) is the prior probability of class ci.\nTo simplify this expression, we assume that the prior\ndistribution over the classes is uniform, i.e., p(ci) =\n1\nNC for\nNC classes. This assumption leads to the cancellation of the\np(c) terms, simplifying the expression in Equation 1 to:\npθ(ci | x) =\npθ(x | ci)\nNC\nP\nj=1\npθ(x | cj)\n(2)\nNext, by exploiting the Evidence Lower Bound (ELBO),\nwe can further refine Equation 2 into a more practical ex-\npression: We approximate the likelihood pθ(x | ci) using\nthe error between the noise ε and the predicted noise εθ in\nthe diffusion process. More specifically, we define\nd (ε, x, c) = ∥ε −εθ(x, c)∥2\n(3)\nto calculate the distance between the error and the predicted\nerror of denoising x under the class label c. This results in\nthe following posterior distribution over {ci}NC\ni=1:\npθ(ci | x) =\nexp{−Et,εd (ε, xt, ci)}\nNC\nP\nj=1\nexp{−Et,εd (ε, xt, cj)}\n(4)\nHowever, there is still room for improvement. A key in-\nsight from Li et al. is that for classification, we are primarily\ninterested in the relative differences between the prediction\nerrors across different classes rather than the absolute error\nvalues for each class. In other words, we do not need to\ncompute the exact error for each class individually; rather,\nwe only need to know how the error for one class compares\nto the others. This insight leads to a simplified version of the\nposterior Equation 4 as follows:\npθ(ci | x) ≈\n1\nNC\nP\nj=1\nexp {Et,ε∆(ε, xt, ci, cj)}\n,\n(5)\n∆(ε, xt, ci, cj) = d (ε, xt, ci) −d (ε, xt, cj)\nIn this approximation, we compare the prediction error\nfor class ci directly against the prediction error for every\nother class cj, using their differences instead of calculating\nthe full error for each class. This reduces the computational\nburden since classification is now based on error ranking.\nNaturally, calculating the expectation value would lead to\nan unbiased Monte Carlo estimate. Specifically, we approx-\nimate the expectation Et,ε by sampling M pairs of (ti, εi),\nwhere ti is uniformly sampled from the range [1, T] and εi\nis drawn from a standard normal distribution, εi ∼N(0, I).\nUsing these samples, we make the following approximation:\nEt,εd (ε, xt, cj) ≈1\nM\nM\nX\ni=1\nd (εi, exi, c) ,\n(6)\nexi =\np\n¯αtix +\np\n1 −¯αtiεi\nMoreover, instead of using different random samples of\n(ti, εi) to compute the ELBO for each conditioning input ci,\nwe can also take advantage of a fixed set of samples S =\n(ti, εi)M\ni=1. As a result, the error estimation is now consistent\nacross all class conditions. By plugging Equation 6 into\nEquation 5, we can extract a diffusion classifier from any\nconditional diffusion model, such as Stable Diffusion [23].\nThis extracted diffusion classifier operates in a zero-shot\nmanner, meaning it can classify without additional training\non labeled data.\n3.2. Hierarchical Diffusion Classifier (HDC)\nAs shown in Equation 5 and in our introductory example\nFigure 1, the traditional diffusion classifiers need to evaluate\nall possible classes, which can be computationally expensive\nand time-consuming. To ease the computational burden, we\npropose a Hierarchical Diffusion Classifier (HDC), which\nleverages the hierarchical label structure of a dataset to per-\nform more efficient and accurate classification.\nThe core idea is to evaluate labels hierarchically and to\nprogressively narrow down the possible classes by pruning\nhigher-level categories (such as “animals” or “tools”) into\nmore specific categories and actual classes (such as “Ham-\nmerhead Shark” or “Screwdriver”). The higher-level cate-\ngories are called “synonym-sets” or “synsets”. By iterating\nover the labels hierarchically, we can significantly reduce\nthe number of classes that need to be evaluated, leading to\nfaster predictions with potentially higher accuracy.\nMore formally, let Th = (N, E) represent a hierarchical\nlabel tree of depth h, nodes N, and edges E. Each node n ∈\nN in the tree corresponds to a synset (or class for leaf nodes),\nand nroot is the root. Moreover, let Children (n) ⊆N\ndenote the set of child nodes of n, and cn represent the synset\nor class label of a node n. We set Children (n) := n if n\nis a leaf node to address imbalanced label trees.\nOur proposed HDC aims to prune irrelevant classes and\nonly considers more relevant classes (nodes) as we descend\nthe tree within a selected set of nodes. The set of selected\nnodes is denoted as Sd\nselected, where d denotes the traverse step\ncount starting from 1 and ending in h (depth of the label tree).\nWe start with the root node nroot, i.e., S1\nselected = {nroot},\nwhich contains the highest-level categories as children.\n3\n\n...\nChildren\n...\nMonte Carlo Estimate (Eq. 7)\nPruning (Eq. 8)\n...\nInput Image:\nPruning Stage\nClassical Diffusion Classification\nFinal Prediction (Eq. 9)\n...\nlow error\nhigh error\nFigure 2. Overview of our Hierarchical Diffusion Classifier (HDC). Starting with an input image x, noise ε ∼N(0, I) is added to generate\na noisy image, resulting in xt for multiple timesteps t. Next, we use the diffusion classifier with a reduced number of ε-predictions and\nhierarchical conditioning prompts like “A photo of a {synclass / class name}” to progressively refine the classification through multiple\nlevels of the label tree. By doing so, we keep track of the most promising classes (highlighted in green) and ignore the rest (highlighted in\nred). Subsequently, the classical diffusion classifier pipeline is applied to the pruned, more specific subcategories (leaf nodes), which results\nin faster classification overall.\nFor each traverse step d, we evaluate recursively the error\nscore for each child node of the selected nodes ns ∈Sd\nselected.\nIn more detail,\n∀ns ∈Sd\nselected : ∀n ∈Children (ns) :\n(7)\nϵn = Et,εd (ε, xt, cn) .\nWe use again Monte Carlo, i.e., Equation 6, to calculate\nϵn, but employ a smaller number of samples M than in the\nclassical diffusion classifer. Instead of selecting a single\nnode, we proceed with a set of nodes with the lowest error\nscores. This set of selected nodes is determined by a pruning\nstrategy, where only the most relevant nodes are kept.\nFormally, the set of the next selected nodes Sd+1\nselected at\neach stage d is defined as\nSd+1\nselected = {n ∈Children (ns) | ns ∈Sd\nselected,\n(8)\n∧ϵn ≤threshold(Kd)}.\nThe pruning ratio Kd determines the threshold, which dic-\ntates how many nodes from the current set are kept for the\nnext level d + 1 of the hierarchy. Essentially, we use the\nthreshold to act as a top-k pruning. The pruning procedure\nis outlined in Algorithm 1.\nEventually, this process reaches the leaf nodes at d = h,\ncorresponding to actual class labels. At this point, we derived\na pruned class set, which will then subsequently be used in\nthe classical diffusion classifier pipeline with the original\nnumber of samples M to determine the final class label.\nEntity\nLiving\nThing\nPerson (3)\nPlant Life\n(2)\nFungus\n(7)\nAnimal\n(399)\nNon-Living\nThing\nObject\n(456)\nTransport\nVehicle\n(70)\nBuilding\n(53)\nGeological\nFormation\n(10)\nFigure 3.\nVisualization of the ImageNet1K hierarchy, illustrat-\ning the first three levels of its tree structure. The categories are\norganized from broad entities (e.g., living and non-living things) to\nmore specific groups (e.g., animals, objects, and transport vehicles),\nwith the numbers in parentheses representing the total number of\nactual classes within each group.\nMore specifically, the final class label is given by\ncnfinal,\nwhere nfinal = arg\nmin\nn∈Sh\nselected\nϵn.\n(9)\n3.3. Tree Setup\nOur proposed HDC leverages the Wordnet hierarchy upon\nwhich the ImageNet-1K ontology is constructed [7]. The im-\nages in the ImageNet-1K dataset are grouped into “synonym-\nsets” or “synsets” with 12 subtrees comprising around 80,000\n4\n\nAlgorithm 1 Hierarchical Diffusion Classifier (HDC) in the\npruning stage for classifying one image\nInput: test image x, Th = (N, E) with nodes N, edges E\nand depth h, root node nroot, label inputs {ci}Nc\ni=1, pruning\nratios Kd, and number of random samples M (see Equa-\ntion 6).\n1: // initialization\n2: Selected = list(Children(nroot))\n3: Errors = dict()\n4: ErrorsCalculated = dict()\n5: for each node n ∈N do\n6:\nErrors[cn] = list()\n7:\nErrorsCalculated[cn] = false\n8: end for\n9:\n10: // modified diffusion classifier error calculations\n11: for tree depth d = 1, . . . , h do\n12:\nfor stage i = 1, . . . , M do\n13:\nSample t ∼[1, 1000]\n14:\nSample ε ∼N(0, I)\n15:\nxt = √¯αtx + √1 −¯αtε\n16:\n17:\n// calculate child errors (Equation 7)\n18:\nfor each node ns in Selected do\n19:\nfor each child node n ∈Children(ns) do\n20:\n// check if error already calculated\n21:\nif ErrorsCalculated[cn] then\n22:\ncontinue\n23:\nend if\n24:\n25:\nErrors[cn].append(∥ε −εθ(xt, cn)∥2)\n26:\nend for\n27:\nend for\n28:\nend for\n29:\n30:\n// descend in the tree and select top-k (Equation 8)\n31:\nErrorsCalculated[Selected] = true\n32:\nSelErrors = mean (Errors[Selected])\n33:\nSelected = TopK (SelErrors, K = Kd)\n34: end for\n35:\n36: // return pruned class label set\n37: Return: Selected\nsynsets. To begin with, we created a hierarchical prompt list\nusing the hierarchy from Engstrom et al. [10].\nIn general, the inference time of HDC increases with the\ndepth of the tree. Thus, we simplified the label tree to reduce\nthe total number of levels. We further modified the exist-\ning Wordnet tree to better suit the classification objective.\nSynsets with ambiguous descriptions such as “artifact”, “or-\nganism” or “implement” are merged into child classes with\nmore definite meanings, such as “animals”. Additionally, to\nmake the classification more efficient, we reduced the depth\nof the tree structure by replacing subtrees with a single leaf\ndirectly with the leaf node. Our final ImageNet-1K hierarchy\ntree has a depth of 7 levels.\nIn the pruning stage of HDC, we iterate over the\nImageNet-1K tree starting from nodes at level 3 of the hier-\narchy (“entity” →{“living-thing”, “non-living thing”} →\n{“animals”, . . . }), as shown in Figure 3. Starting at level 2\n(“living thing” vs. “non-living thing”) showed no variation\nin error scores but increased inference time.\n3.4. Pruning Strategies\nOur proposed HDC method allows many pruning strate-\ngies to be implemented that balance accuracy and compu-\ntational efficiency. We implemented two primary pruning\nstrategies, one that works with fixed ratios of pruned nodes\nand one that adapts dynamically depending on the distribu-\ntion of error predictions. In more detail:\n• Strategy 1 - Fixed Pruning: We select the top-k nodes\nwith the lowest errors at each hierarchy level, defined\nby a pruning ratio Kd.\n• Strategy 2 - Dynamic Pruning: In this approach, we\nkeep only nodes within two standard deviations of the\nminimum error at each level, allowing a more adaptive,\ndata-driven selection.\nIn contrast to Dynamic Pruning, Fixed Pruning allows for\nfiner control over the trade-off between accuracy and run-\ntime. Our proposed pruning strategies offer varying degrees\nof control over the balance between accuracy and runtime,\nadapting to unique hierarchical structures for greater scala-\nbility. Unlike traditional diffusion classifiers, which evaluate\nall classes for each input image, our pruning strategies strate-\ngically select candidate classes at each level, reducing com-\nputational load while maintaining similar class precision.\n4. Experimental Setup\nThis section describes our experimental setup for test-\ning the performance and reliability of HDC on ImageNet-\n1K, which includes a discussion about how to construct the\nhierarchical label tree and specifics to the classifier itself,\nprompting, and pruning strategies. Our code can be found\non GitHub1.\n4.1. Classifier Setup\nFor our classifier, we built on the efficient framework\nestablished by Li et al. [17], with added modifications tai-\nlored for hierarchical processing and pruning of candidate\n1https : / / github . com / arus23 / hierarchical _\ndiffusion_classifier\n5\n\nMethod\nTop 1 [%]\nTop 3 [%]\nTop 5 [%]\nTime [s]\nSpeed-Up [%]\nDiffusion Classifier (baseline) [17]\n64.70\n84.30\n89.70\n1600\n-\nHDC Strategy 1 (ours)\n64.90\n81.80\n86.30\n980\n38.75\nHDC Strategy 2 (ours)\n63.20\n82.30\n86.30\n650\n59.38\nTable 1. Comparison of classification accuracy and inference time between the classical diffusion classifier [17] and our proposed HDC\nusing two pruning strategies with Stable Diffusion 2.0. Both strategies demonstrate that HDC can significantly reduce classification time,\nachieving up to a 60% speed-up in inference time with minimal impact on accuracy or reach even better top-1 precision with minimal impact\non runtime. Best results are marked in bold, second-best underlined.\nclasses, further customized for diffusion classification on\nStable Diffusion (SD) [23]. Our HDC setup is adaptable,\nallowing seamless integration with different diffusion mod-\nels and possible fine-tuning to support various hierarchical\npruning strategies, thereby making it versatile. Thus, we\naccommodate the SD versions 1.4, 2.0, and 2.1 in our experi-\nments. For Strategy 1 in our pruning setup, we set Kd = 0.5\nfor all possible d-values.\nAll evaluations were performed on ImageNet-1K at\n512x512 resolution, the resolution under which all versions\nof SD were originally trained, ensuring results are compara-\nble to other state-of-the-art models. Also following Li et al.,\nwe used the l2 norm to compute the ϵ prediction error and\ntimesteps for εt-predictions are uniformly sampled from the\nrange [1, 1000].\n4.2. Prompt Engineering\nThe class labels are converted to the form “a photo of a\n<class label>” using the template from the original exper-\niments. Additionally, inspired by Radford et al. [21], we\nexperiment with prompt templates “A bad photo of a <class\nlabel>”, “A low-resolution photo of a <class label>” and\n“itap of a <class label>” for ImageNet-1K.\n5. Results\nThis section presents our experimental results, evaluating\ndifferent aspects of HDC, which were outlined previously:\npruning strategies, prompt engineering, stable diffusion vari-\nations, and, finally, an overall evaluation of per-class accu-\nracy.\n5.1. Pruning Strategies\nTable 1 highlights the results of our HDC across differ-\nent pruning strategies compared to the classical diffusion\nclassifier. As observed, both strategies (as outlined in subsec-\ntion 3.4) show marked improvements in runtime compared to\nclassical diffusion classifiers, and each is suited to different\nprioritizations of speed versus accuracy.\nFor instance, Strategy 1 yields the best trade-off results on\nImageNet-1K, achieving significant runtime reductions (up\nto 980 seconds) with an top-1 accuracy boost of 0.20 percent-\nage points. By employing Strategy 2 (selecting candidates\nbased on two standard deviations from the lowest error),\nwe reduce the inference time even further to 650 seconds,\nthough at the cost of a slight accuracy drop (i.e., 1.50 per-\ncentage points). Strategy 2 demonstrates that faster inference\ncan be achieved with a small compromise in precision.\nIn Figure 4, we provide an empirical example of how\nHDC traverses the label tree to find candidate classes (prun-\ning stage) and calculates the error of candidate classes for\nfinal prediction (classical diffusion classification on pruned\nleaf nodes).\n5.2. Stable Diffusion Versions\nWe evaluated the HDC using different Stable Diffusion\n(SD) versions to assess its flexibility and performance across\ngenerative backbones, as summarized in Table 2. The results\nreveal that SD 2.0 provides the best trade-off between accu-\nracy and inference time. Specifically, when using Strategy 1,\nSD 2.0 achieved the highest Top-1 accuracy at 64.14% with\nan inference time of 980 seconds. In contrast, SD 1.4 demon-\nstrates the fastest inference time of 710 seconds when paired\nwith Strategy 2, albeit with a significant top-1 class-accuracy\nreduction to 54.77%.\n5.3. Prompt Engineering\nInspired by Radford et al. [21], we evaluated different\nprompt templates to assess their impact on accuracy and\ninference time, as shown in Table 3. The default prompt, “a\nphoto of a <class label>,” consistently achieved the best\nperformance, suggesting that a straightforward prompt yields\nrobust results across classes. Other templates, such as “a bad\nphoto of a <class label>” and “a low-resolution photo of a\n<class label>,” resulted in a slight drop in accuracy without\nsignificantly affecting inference time.\nThe rationale for testing alternative prompts stems from\na hypothesis that prompts hinting at lower-quality images\nmight help the classifier generalize better to real-world cases\nwith variable quality, capturing diverse visual characteristics.\nFor instance, using terms like “bad” or “low-resolution” was\nexpected to enhance robustness to noisy or degraded inputs.\nInterestingly, however, the results show that the simpler,\nunmodified prompt performs best, indicating that the hierar-\nchical model likely benefits from a more neutral prompt\n6\n\nSD Version\nStrategy 1\nStrategy 2\nTop 1 [%]\nTop 1 [%]\nTime [s]\nSpeed-Up [%]\nTop 1 [%]\nTop 1 [%]\nTime [s]\nSpeed-Up [%]\n(class-wise)\n(overall)\n(class-wise)\n(overall)\nSD 1.4\n52.71\n52.60\n1000\n37.50\n54.77\n54.80\n710\n55.63\nSD 2.0\n65.16\n64.90\n980\n38.75\n63.33\n63.20\n980\n38.75\nSD 2.1\n61.15\n61.00\n950\n40.63\n60.91\n60.70\n720\n55.00\nTable 2. Performance comparison of the HDC with different Stable Diffusion (SD) versions using Strategy 1 and Strategy 2. Top-1 accuracy\nand inference time (in seconds) are reported for each SD version, highlighting SD 2.0 as achieving the highest accuracy, while Strategy 2 in\nSD 1.4 yields the fastest inference time.\nStrategy\nPrompt-Type\nTop 1 [%]\nTop 3 [%]\nTop 5 [%]\n1\n“A photo of a <class label>”\n64.90\n80.20\n85.30\n“A bad photo of a <class label>”\n59.90\n79.60\n84.90\n“itap of a <class label>”\n61.37\n81.33\n86.30\n“A low-resolution photo of a <class label>”\n57.50\n76.46\n80.94\n2\n“A photo of a <class label>”\n63.20\n82.30\n86.30\n“A bad photo of a <class label>”\n62.30\n80.10\n85.90\n“itap of a <class label>”\n57.80\n78.20\n82.30\n“A low-resolution photo of a <class label>”\n57.50\n76.46\n80.94\nTable 3. Evaluation of classification accuracy across different prompt types for HDC using pruning Strategies 1 and 2. The standard prompt,\n“A photo of a <class label>”, consistently yields the highest Top-1, Top-3, and Top-5 accuracy. Alternative prompts, such as “A bad photo\nof a <class label>” and “A low-resolution photo of a <class label>”, result in slight decreases in accuracy, showing that prompt variations\ncan impact model performance.\nMethod\nAvg. Accuracy [%]\nTime [s]\nSpeed-Up [%]\nDiffusion Classifier\n64.90\n1600\n-\nHDC Strategy 1 (ours)\n65.16\n980\n38.75\nHDC Strategy 2 (ours)\n63.33\n650\n59.38\nTable 4. Comparison of average classification Top 1-accuracy and\ninference time per class for the classical diffusion classifier and our\nHDC with Strategies 1 and 2 using SD 2.0. Strategy 1 achieves\nthe highest accuracy at K = 0.5, while Strategy 2 offers the fastest\ninference time with minimal accuracy loss.\nformat when dealing with high-quality image data like\nImageNet-1K. Nevertheless, these prompt variations may\nstill hold potential for datasets with inherently low-resolution\nor distorted images, where quality-based prompts could help\nthe classifier learn more generalized features.\nWe also observed a significant disparity in inference times\nacross specific classes, such as “snail” (221 seconds) ver-\nsus “keyboard space bar” (1400 seconds). This difference\nlikely reflects the complexity of visual features within each\ncategory: classes with intricate or ambiguous features may\nrequire longer processing times due to the hierarchical clas-\nsification structure.\n5.4. Overall Accuracy vs. Inference Time\nIn summary, Table 4 shows the overall accuracy and infer-\nence time across different pruning strategies. The baseline\ndiffusion classifier achieves an accuracy of 64.90% with an\ninference time of 1600 seconds, providing a reference for\nboth speed and precision.\nOur HDC using Strategy 1 demonstrates new state-of-\nthe-art accuracy for diffusion classifiers with 65.16%, while\nreducing the inference time by nearly 40% to 980 seconds.\nThis indicates that HDC can not only improve classification\nperformance but also benefits from a considerable reduction\nin computational load. The reduction in processing time\nwhile maintaining similar accuracy makes Strategy 1 a bal-\nanced choice for high-accuracy applications where inference\nspeed is also a priority.\nSimilarly, HDC with Strategy 2 leverages dynamic prun-\ning to further accelerate inference. While it records a slight\ndrop in accuracy to 63.33%, Strategy 2 reduces inference\ntime to 650 seconds - approximately 60% faster than the\nbaseline. This strategy demonstrates the potential of HDC\nfor use cases requiring faster response times, with only a\nmarginal trade-off in classification performance.\nIn Figure 5, we present a detailed confusion matrix of\nclasses within the synset category “Animal.” Most misclas-\n7\n\nLiving Thing\nNon-Living\nThing\nPerson\n(-0.1126)\nAnimal\n(-0.1125)\nBird\n(-0.1123)\nMammal\n(-0.1126)\nBird of\nPrey\n(-0.1122)\nObject\n(-0.1127)\nTransport\n(-0.1127)\nBubble\n(-0.1127)\nCrane\n(-0.1123)\nBustard\n(-0.1124)\nVulture\n(-0.1123)\nAmerican\nBald Eagle\n(-0.1123)\nKite (Bird of\nPrey)\n(-0.1122)\npruned\ncandidate\nleaves\nfinal prediction\nFigure 4. Example classification of an image in the pruning stage\nof the HDC using Strategy 1. In this stage, error scores calculated\nfor each node are used to iteratively prune the tree, narrowing it\ndown to relevant leaf nodes that will undergo further refinement\nin subsequent stages. The subsequent steps then focus on closely\nrelated nodes (see leaves under the purple line), such as the Ameri-\ncan Bald Eagle and Vulture, ultimately selecting the leaf node with\nthe lowest error score—Kite (Bird of Prey) - in the final stage.\nsifications occur among biologically similar groups, such\nas Salamander-Lizard and Lizard-Snake, highlighting the\nclassifier’s tendency to group closely related classes.\nOverall, our results show that HDC provides a customiz-\nable trade-off between inference speed and accuracy, making\nit adaptable to varying application needs. Strategy 1 is partic-\nularly suitable for high-accuracy applications, while Strategy\n2 is better suited to real-time scenarios that prioritize speed.\n6. Limitations & Future Work\nWhile our method substantially improves inference time\nand maintains competitive accuracy, several limitations must\nbe addressed in future work.\nThe efficiency gains provided by the hierarchical prun-\ning strategy heavily depend on the depth and balance of the\nunderlying label tree. Datasets with shallow hierarchies or\nthose lacking well-defined parent-child relationships may\nnot benefit as significantly from our method. Furthermore,\nFish\nBird\nSalamander\nFrog\nMammal\nTurtle\nLizard\nArachnid\nInsect\nSnake\nCrustacean\nRest\nOther Classes\nFish\nBird\nSalamander\nFrog\nMammal\nTurtle\nLizard\nArachnid\nInsect\nSnake\nCrustacean\nRest\n21\n7\n3\n182\n1\n3\n1\n13\n1\n8\n1\n1\n474\n1\n22\n15\n1\n31\n2\n21\n2\n2\n1\n1\n58\n1\n6\n51\n1\n1\n1\n1\n21\n4\n1\n2\n1\n1\n26\n7\nFigure 5. Confusion Matrix of HDC (Strategy 1) for the sub-\nclasses under the synset class “Animal”. The x-axis shows the\npredicted labels (including “other classes” outside of the synset\nclass “Animal”), and the y-axis shows the ground-truth labels.\nthe effectiveness of our approach on datasets with complex\nor overlapping class labels, such as those in medical imaging\nor fine-grained visual recognition, remains untested. These\nfields may benefit from further exploration of adaptive prun-\ning thresholds or weighted paths, which could prioritize\nhighly discriminative regions of the hierarchy, improving\nclassification accuracy for nuanced categories.\n7. Conclusion\nIn this work, we introduced the Hierarchical Diffu-\nsion Classifier (HDC), a novel approach for accelerating\ndiffusion-based classification by utilizing hierarchical class\npruning. Our results on the ImageNet-1K dataset demon-\nstrate that HDC significantly reduces inference time, achiev-\ning up to a 60% speedup over traditional diffusion classifiers\nwhile maintaining and, in some cases, even improving classi-\nfication accuracy. This improvement is achieved by progres-\nsively narrowing down relevant class candidates, pruning out\nhigh-level categories early in the process, and focusing only\non specific, contextually relevant subcategories.\nOur experiments highlight HDC’s adaptability, showing\nthat different pruning strategies (such as Top-k Pruning and\nThreshold Pruning) offer customizable trade-offs between\ninference speed and accuracy. This versatility makes HDC\nsuitable for diverse applications, from high-accuracy im-\nage classification tasks to real-time scenarios where rapid\ninference is critical.\n8\n\nAcknowledgements\nThis work was supported by the BMBF projects Sus-\ntainML (Grant 101070408), Albatross (Grant 01IW24002)\nand by Carl Zeiss Foundation through the Sustainable Em-\nbedded AI project (P2021-02-009).\nReferences\n[1] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\nMultidiffusion: Fusing diffusion paths for controlled image\ngeneration. 2023. 1\n[2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce\nLee, Yufei Guo, et al. Improving image generation with\nbetter captions.\nComputer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf, 2(3):8, 2023. 1\n[3] Huanran Chen, Yinpeng Dong, Shitong Shao, Zhongkai\nHao, Xiao Yang, Hang Su, and Jun Zhu. Your diffusion\nmodel is secretly a certifiably robust classifier. arXiv preprint\narXiv:2402.02316, 2024. 1\n[4] Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K\nDuvenaud. Isolating sources of disentanglement in variational\nautoencoders. NeurIPS, 31, 2018. 1\n[5] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya\nSutskever, and Pieter Abbeel. Infogan: Interpretable rep-\nresentation learning by information maximizing generative\nadversarial nets. NeurIPS, 29, 2016. 1\n[6] Kevin Clark and Priyank Jaini. Text-to-image diffusion mod-\nels are zero-shot classifiers, 2023. 1, 2\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248–255. Ieee, 2009. 2, 4\n[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural information\nprocessing systems, 34:8780–8794, 2021. 2\n[9] Jeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Ad-\nversarial feature learning. arXiv preprint arXiv:1605.09782,\n2016. 1\n[10] Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani San-\nturkar, and Dimitris Tsipras. Robustness (python library),\n2019. 5\n[11] Stanislav Frolov, Tobias Hinz, Federico Raue, J¨orn Hees,\nand Andreas Dengel. Adversarial text-to-image synthesis: A\nreview. Neural Networks, 144:187–209, 2021. 1, 2\n[12] Stanislav Frolov, Brian B Moser, and Andreas Dengel. Spotd-\niffusion: A fast approach for seamless panorama generation\nover time. arXiv preprint arXiv:2407.15507, 2024. 1\n[13] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Das-\nsarma, Dawn Drain, Nelson Elhage, et al. Predictability and\nsurprise in large generative models. In 2022 ACM Conference\non Fairness, Accountability, and Transparency, 2022. 2\n[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. In NeurIPS,\n2014. 1, 2\n[15] Geoffrey E Hinton. To recognize shapes, first learn to generate\nimages. Progress in brain research, 165:535–547, 2007. 1\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. NeurIPS, 33:6840–6851, 2020.\n1\n[17] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis\nBrown, and Deepak Pathak. Your diffusion model is secretly\na zero-shot classifier. In ICCV, pages 2206–2217, 2023. 1, 2,\n5, 6\n[18] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models. In CVPR,\n2022. 1\n[19] Brian B Moser, Federico Raue, Sebastian Palacio, Stanislav\nFrolov, and Andreas Dengel. Latent dataset distillation with\ndiffusion models. arXiv preprint arXiv:2403.03881, 2024. 1,\n2\n[20] Brian B Moser, Arundhati S Shanbhag, Federico Raue,\nStanislav Frolov, Sebastian Palacio, and Andreas Dengel.\nDiffusion models, image super-resolution and everything: A\nsurvey. arXiv preprint arXiv:2401.00736, 2024. 1\n[21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision, 2021. 6\n[22] Danilo Rezende and Shakir Mohamed. Variational inference\nwith normalizing flows. In ICML, pages 1530–1538. PMLR,\n2015. 1\n[23] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, pages 10684–\n10695, 2022. 2, 3, 6\n[24] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,\nJay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael\nGontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho-\ntorealistic text-to-image diffusion models with deep language\nunderstanding. NeurIPS, 35:36479–36494, 2022. 2\n[25] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes,\nAarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick\nSchramowski, Srivatsa Kundurthy, Katherine Crowson, Lud-\nwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-\n5b: An open large-scale dataset for training next generation\nimage-text models, 2022. 2\n[26] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale,\nTrung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang.\nUncovering the disentanglement capability in text-to-image\ndiffusion models. In CVPR, pages 1900–1910, 2023. 1\n[27] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nICCV, pages 3836–3847, 2023. 1\n9",
    "pdf_filename": "Just_Leaf_It_Accelerating_Diffusion_Classifiers_with_Hierarchical_Class_Pruning.pdf"
}