{
    "title": "Just Leaf It: Accelerating Diffusion Classifiers with Hierarchical Class Pruning",
    "abstract": "evaluates Diffusion all classes Classifier Diffusionmodels,knownfortheirgenerativecapabilities, haverecentlyshownunexpectedpotentialinimageclassifica- class label tiontasksbyusingBayes’theorem. However,mostdiffusion prunes Hierarchical classifiers require evaluating all class labels for a single irrelevant Diffusion classification,leadingtosignificantcomputationalcoststhat classes Classifier (ours) can hinder their application in large-scale scenarios. To addressthis,wepresentaHierarchicalDiffusionClassifier (HDC)thatexploitstheinherenthierarchicallabelstructure ofadataset. Byprogressivelypruningirrelevanthigh-level categoriesandrefiningpredictionsonlywithinrelevantsub- class label categories,i.e.,leafnodes,HDCreducesthetotalnumber considered not considered ofclassevaluations. Asaresult,HDCcanaccelerateinfer- ence by up to 60% while maintaining and, in some cases, Figure1. Comparisonbetweentheclassicaldiffusionclassifier improvingclassificationaccuracy. Ourworkenablesanew andourproposedHierarchicalDiffusionClassifier(HDC).While controlmechanismofthetrade-offbetweenspeedandpreci- theclassicalapproachevaluatesallpossibleclassestofindthecor- sion,makingdiffusion-basedclassificationmoreviablefor rectlabel,whichleadstounnecessarycomputation,HDCprunes real-worldapplications,particularlyinlarge-scaleimage irrelevantclassesearly,focusingonlyonthemostrelevantcandi- classificationtasks. dates.Thishierarchicalpruningreducescomputationaloverhead andacceleratesinference. 1.Introduction Generativemodelsaredesignedtocapturethefullspec- iterativeMarkovianprocessofaddingandremovingnoise trumofadatasetdistribution, offeringarichanddetailed [1,12,16,18,20]. Recently, the research community has understandingofthedatatheyrepresent[14,22]. Thiscom- shiftedtowardsrepurposingpre-traineddiffusionmodelsfor prehensivegraspofdataallowsthemtonotonlycreatenew classificationtasksinazero-shotmanner, signalingapiv- contentbutalsoprovidedeepinsightsintotheircharacter- otalmovetowardusinggenerativemodelsasdiscriminators, isticsandstructures[15,19]. Moreover,theseinsightscan so-calleddiffusionclassifiers[3,6,17]. Morespecifically, significantlybenefitdownstreamtaskslikeimageclassifica- diffusion models that learned p(x|c) can be easily con- tion[4,5,9]. Nonetheless,overthepastdecade,thefocus vertedintoclassifiersbyexploitingtheBayes’theoremto ofmanygenerativetechniqueshaspredominantlybeenon derive p(c|x). Thus, given an image x and a set of N C contentgenerationratherthanharnessingtheirpotentialfor possibleclasses{c i}N i=C 1,wecancalculatethelikelihoodof discriminativetasks[2,11,18,26,27]. xbelongingtoeachclassc i. Amonggenerativemodels,diffusionmodelsemergeas Inpractice,thismeansaddingnoisetoxandestimating aparticularlypowerfulsubclassduetotheirabilitytopro- theexpectedlossofnoisereconstructionviaMonteCarlo, duceexceptionallyhigh-qualityoutputimagesthroughan i.e.,throughrepeatedcalculationsandaveraging. Thispro- 1 4202 voN 81 ]VC.sc[ 1v37021.1142:viXra",
    "body": "Just Leaf It: Accelerating Diffusion Classifiers with Hierarchical Class Pruning\nArundhatiS.Shanbhag2,3,BrianB.Moser1,2,3,TobiasC.Nauen1,2,\nStanislavFrolov1,2,FedericoRaue1,andAndreasDengel1,2\n1GermanResearchCenterforArtificialIntelligence\n2UniversityofKaiserslautern-Landau\n3EqualContribution\nfirst.last@dfki.de\nAbstract Classical\nevaluates Diffusion\nall classes Classifier\nDiffusionmodels,knownfortheirgenerativecapabilities,\nhaverecentlyshownunexpectedpotentialinimageclassifica- class label\ntiontasksbyusingBayes’theorem. However,mostdiffusion\nprunes Hierarchical\nclassifiers require evaluating all class labels for a single irrelevant Diffusion\nclassification,leadingtosignificantcomputationalcoststhat classes Classifier\n(ours)\ncan hinder their application in large-scale scenarios. To\naddressthis,wepresentaHierarchicalDiffusionClassifier\n(HDC)thatexploitstheinherenthierarchicallabelstructure\nofadataset. Byprogressivelypruningirrelevanthigh-level\ncategoriesandrefiningpredictionsonlywithinrelevantsub- class label\ncategories,i.e.,leafnodes,HDCreducesthetotalnumber\nconsidered not considered\nofclassevaluations. Asaresult,HDCcanaccelerateinfer-\nence by up to 60% while maintaining and, in some cases, Figure1. Comparisonbetweentheclassicaldiffusionclassifier\nimprovingclassificationaccuracy. Ourworkenablesanew andourproposedHierarchicalDiffusionClassifier(HDC).While\ncontrolmechanismofthetrade-offbetweenspeedandpreci- theclassicalapproachevaluatesallpossibleclassestofindthecor-\nsion,makingdiffusion-basedclassificationmoreviablefor rectlabel,whichleadstounnecessarycomputation,HDCprunes\nreal-worldapplications,particularlyinlarge-scaleimage irrelevantclassesearly,focusingonlyonthemostrelevantcandi-\nclassificationtasks. dates.Thishierarchicalpruningreducescomputationaloverhead\nandacceleratesinference.\n1.Introduction\nGenerativemodelsaredesignedtocapturethefullspec- iterativeMarkovianprocessofaddingandremovingnoise\ntrumofadatasetdistribution, offeringarichanddetailed [1,12,16,18,20]. Recently, the research community has\nunderstandingofthedatatheyrepresent[14,22]. Thiscom- shiftedtowardsrepurposingpre-traineddiffusionmodelsfor\nprehensivegraspofdataallowsthemtonotonlycreatenew classificationtasksinazero-shotmanner, signalingapiv-\ncontentbutalsoprovidedeepinsightsintotheircharacter- otalmovetowardusinggenerativemodelsasdiscriminators,\nisticsandstructures[15,19]. Moreover,theseinsightscan so-calleddiffusionclassifiers[3,6,17]. Morespecifically,\nsignificantlybenefitdownstreamtaskslikeimageclassifica- diffusion models that learned p(x|c) can be easily con-\ntion[4,5,9]. Nonetheless,overthepastdecade,thefocus vertedintoclassifiersbyexploitingtheBayes’theoremto\nofmanygenerativetechniqueshaspredominantlybeenon derive p(c|x). Thus, given an image x and a set of N C\ncontentgenerationratherthanharnessingtheirpotentialfor possibleclasses{c i}N i=C 1,wecancalculatethelikelihoodof\ndiscriminativetasks[2,11,18,26,27]. xbelongingtoeachclassc i.\nAmonggenerativemodels,diffusionmodelsemergeas Inpractice,thismeansaddingnoisetoxandestimating\naparticularlypowerfulsubclassduetotheirabilitytopro- theexpectedlossofnoisereconstructionviaMonteCarlo,\nduceexceptionallyhigh-qualityoutputimagesthroughan i.e.,throughrepeatedcalculationsandaveraging. Thispro-\n1\n4202\nvoN\n81\n]VC.sc[\n1v37021.1142:viXra\ncedureisknownasε-predictionlossandhastobedonefor datadistributionsbyiterativelyaddingandremovingnoise\neachclass. AlthoughBayes’Theoremelegantlyadaptsdif- within a Markovian process. Traditionally used to model\nfusionmodelsforzero-shotuse,meaningtheycanclassify data distributions, these generative models have recently\nwithoutanyadditionaltraining,thescalingwithNclasses beenexploredfortheirdiscriminativecapabilitiesaswell.\nposesaconsiderablecomputationalchallengeformanyprac- Li et al. [17] introduced diffusion classifiers by using\ntitioners. Thisε-predictionaddstothealreadyhighcompu- StableDiffusion(SD)[23]asazero-shotclassifier,without\ntationalcostsassociatedwithdiffusionmodels[6,13,17,19]. the need for additional training. SD, originally designed\nTo alleviate the computational burden, we propose an fortext-to-imagegenerationandtrainedonasubsetofthe\nextension to diffusion classifiers that exploits a hierarchi- LAION-5Bdataset[25],leveragesitsabilitytosynthesize\ncal search over label trees rather than evaluating each la- datatodiscriminatebetweenimagesbyevaluatingprediction\nbel individually, which we coined as Hierarchical Diffu- errorsacrossclasslabels, i.e., ε-predictions. Morespecif-\nsion Classifier (HDC) and illustrated in Figure 1. In the ically, they compute class scores based on differences in\nfirststage,termedpruningstage,HDCeliminatesirrelevant predictedandactualnoise,offeringanefficientclassification\nbranchesbytraversingthelabeltreelevel-by-levelandkeep- method. Similarinspirit,Clarketal.[6]furtherexplored\ningonlythemostpromisingsynsetsdeterminedbythebest diffusionmodelslikeSD[23]andImagen[24]fordiscrimi-\nε-predictions. Incontrasttoclassicaldiffusionclassification, nativetasksbyaggregatingscorematricesacrossclasslabels\ntheε-predictionsuselesscomputationstepsfortheMonte andtimesteps. Usingaweightedscorefunction,theyassign\nCarlo estimate to save additional runtime. Subsequently, imagestotheclasswiththelowestaggregatedscore,demon-\nHDCperformstheclassicaldiffusionclassificationonthe stratingthetransferabilityofgenerativerepresentationsto\nremainingcandidateleafnodes. Asaresult,wecanachieve classificationtasks.\na speed-up of roughly 60%, saving hundreds of hours for Whileenablingthezero-shotdiffusionclassification,both\nImageNet-1K[7]whilemaintainingsimilaraccuracy. More- approachesconsiderallclassesinordertoclassifyasingle\nover,HDCcanachieveevenbetteraccuracy,i.e.,65.16%per image, which is a computational challenge. Since infer-\nclassinsteadof64.90%,byusingroughlythesamecalcula- encetimeforzero-shotclassificationscaleslinearlywiththe\ntiontimeastraditionaldiffusionclassifiersbyusinglower numberofclasses,anycomputationalimprovementcansig-\npruningratiosinthefirststage. nificantlyimpactclassificationsonlarge-scaledatasetslike\nOverall,HDCintroducesanewcontrollablebalancebe- ImageNet-1K[7]. Lietal.[17]addressthisbyusingaweak\ntween inference speed and classification accuracy by ad- discriminativemodeltofilteroutobviouslyincorrectclasses\njustingpruningfactors,enablingdiffusionclassifierstobe beforeperformingzero-shotclassification,thusspeedingup\nflexiblyusedinlarge-scalediscriminativetasks.Byadopting theprocess. Similarly,Clarketal.[6]employasuccessive\nahierarchicalapproach,weprovideascalableandpractical eliminationstrategywithinamulti-armedbanditframework\nsolution for utilizing diffusion models in a wide range of toiterativelynarrowdownthesetofcandidateclasses.\napplicationsbeyondtheiroriginalgenerativepurpose. The Despitesomereductionincomputationalcomplexity,they\nmaincontributionsofourpapercanbesummarizedby: stillprocesseachclasslabelateverydiffusiontimestepdur-\ning inference. In contrast, our work explores the poten-\n1. Wedemonstratethattheinferencetimeofzero-shotdif- tialofleveragingthehierarchicalstructureofdatasetslike\nfusionclassifierscanbesignificantlyacceleratedwitha ImageNet-1K.Byintegratinghierarchicalpruningstrategies,\nhierarchicallabelsearch,reducingcomputationalcom- weaimtoprogressivelyrefinethesetofcandidateclasses\nplexitywhilemaintainingsimilaraccuracy. ateachlevelofthehierarchy,allowingforfasterandmore\naccuratepredictions.\n2. We present a novel Hierarchical Diffusion Classifier\n(HDC)thatleveragesthelabelstructureofImageNet-\n3.Methodology\n1Ktonarrowdowncandidateclassesefficiently.\nThissectionprovidesabriefoverviewofdiffusionclas-\n3. OurHDCframeworkachievesfasterinferencetimes\nsifiersandintroducesourproposedHierarchicalDiffusion\n(roughly60%)whilemaintainingorimprovingclassifi-\nClassifier(HDC),asshowninFigure2andoutlinedinAl-\ncationperformancecomparedtotheclassicaldiffusion\ngorithm1.\nmethod(i.e.,65.16%insteadof64.90%).\n3.1.DiffusionClassifierPreliminaries\n2.RelatedWork\nThediffusionclassifierisbasedontheformulationintro-\nDiffusion models have disrupted the landscape of gen- ducedbyLietal.[17]. Thekeyideaisthat,givenatrained\nerativemodels,challengingthelongstandingdominanceof diffusionmodelp ,wecanleveragethepredictionsofthe\nθ\nGANs[11,14]andsettinganewstandardingeneratinghigh- diffusion model, p (x | c ), to infer the probability of a\nθ i\nquality, realistic data [8]. Broadly speaking, they capture class c given an input x using Bayes’ theorem to derive\ni\n2\np (c |x). Thiscanbeexpressedas: Naturally,calculatingtheexpectationvaluewouldleadto\nθ i\nanunbiasedMonteCarloestimate. Specifically,weapprox-\np (c |x)=\np(c i)p θ(x|c i)\n(1)\nimatetheexpectationE\nt,ε\nbysamplingM pairsof(t i,ε i),\nθ i\nN (cid:80)C wheret iisuniformlysampledfromtherange[1,T]andε i\np(c )p (x|c )\nj θ j isdrawnfromastandardnormaldistribution,ε ∼N(0,I).\nj=1 i\nUsingthesesamples,wemakethefollowingapproximation:\nHere,p (x|c )isthelikelihoodofgeneratingtheinputx\nθ i M\ngivenclassc i,andp(c i)isthepriorprobabilityofclassc i. E d(ε,x ,c )≈ 1 (cid:88) d(ε ,x ,c), (6)\nTo simplify this expression, we assume that the prior t,ε t j M i (cid:101)i\ni=1\nd Nis Ctr cib lau st sio en s.o Tv he ir st ah se sc ul mas ps te ios nis leu an dif so tr om t, hi e.e c. a, np c( ec li l) at= ionN1 oC ftf ho er x (cid:101)i =(cid:112) α¯ tix+(cid:112) 1−α¯ tiε i\np(c)terms,simplifyingtheexpressioninEquation1to:\nMoreover,insteadofusingdifferentrandomsamplesof\n(t ,ε )tocomputetheELBOforeachconditioninginputc ,\np (x|c ) i i i\np (c |x)= θ i (2) wecanalsotakeadvantageofafixedsetofsamplesS =\nθ i\nN (cid:80)C (t ,ε )M . Asaresult,theerrorestimationisnowconsistent\np θ(x|c j) i i i=1\nj=1 across all class conditions. By plugging Equation 6 into\nEquation 5, we can extract a diffusion classifier from any\nNext,byexploitingtheEvidenceLowerBound(ELBO), conditionaldiffusionmodel,suchasStableDiffusion[23].\nwe can further refine Equation 2 into a more practical ex- This extracted diffusion classifier operates in a zero-shot\npression: Weapproximatethelikelihoodp θ(x | c i)using manner,meaningitcanclassifywithoutadditionaltraining\ntheerrorbetweenthenoiseεandthepredictednoiseε θ in onlabeleddata.\nthediffusionprocess. Morespecifically,wedefine\n3.2.HierarchicalDiffusionClassifier(HDC)\nd(ε,x,c)=∥ε−ε (x,c)∥2 (3)\nθ AsshowninEquation5andinourintroductoryexample\nFigure1,thetraditionaldiffusionclassifiersneedtoevaluate\ntocalculatethedistancebetweentheerrorandthepredicted\nallpossibleclasses,whichcanbecomputationallyexpensive\nerrorofdenoisingxundertheclasslabelc. Thisresultsin\nandtime-consuming. Toeasethecomputationalburden,we\nthefollowingposteriordistributionover{c }NC:\ni i=1 proposeaHierarchicalDiffusionClassifier(HDC),which\nexp{−E d(ε,x ,c )} leveragesthehierarchicallabelstructureofadatasettoper-\np θ(c i |x)= t,ε t i (4) formmoreefficientandaccurateclassification.\nN (cid:80)C exp{−E d(ε,x ,c )} Thecoreideaistoevaluatelabelshierarchicallyandto\nt,ε t j\nj=1 progressivelynarrowdownthepossibleclassesbypruning\nhigher-levelcategories(suchas“animals”or“tools”)into\nHowever,thereisstillroomforimprovement. Akeyin-\nmorespecificcategoriesandactualclasses(suchas“Ham-\nsightfromLietal. isthatforclassification,weareprimarily\nmerheadShark”or“Screwdriver”). Thehigher-levelcate-\ninterestedintherelativedifferencesbetweentheprediction\ngoriesarecalled“synonym-sets”or“synsets”. Byiterating\nerrorsacrossdifferentclassesratherthantheabsoluteerror\nover the labels hierarchically, we can significantly reduce\nvalues for each class. In other words, we do not need to\nthenumberofclassesthatneedtobeevaluated,leadingto\ncomputetheexacterrorforeachclassindividually;rather,\nfasterpredictionswithpotentiallyhigheraccuracy.\nweonlyneedtoknowhowtheerrorforoneclasscompares\nMoreformally,letT =(N,E)representahierarchical\nh\ntotheothers. Thisinsightleadstoasimplifiedversionofthe\nlabeltreeofdepthh,nodesN,andedgesE. Eachnoden∈\nposteriorEquation4asfollows:\nN inthetreecorrespondstoasynset(orclassforleafnodes),\nand n is the root. Moreover, let Children(n) ⊆ N\n1 root\np θ(c i |x)≈ N (cid:80)C exp{E ∆(ε,x ,c ,c )}, (5) d oe rn co late ssth lae bs ee lt oo ffc ah nil od dn eo nd .e Wso ef sn e, ta Cn hd ic ln dre rp er nes (e nnt )t :h =es nyn ifse nt\nt,ε t i j\nj=1 isaleafnodetoaddressimbalancedlabeltrees.\n∆(ε,x ,c ,c )=d(ε,x ,c )−d(ε,x ,c ) OurproposedHDCaimstopruneirrelevantclassesand\nt i j t i t j\nonlyconsidersmorerelevantclasses(nodes)aswedescend\nInthisapproximation,wecomparethepredictionerror thetreewithinaselectedsetofnodes. Thesetofselected\nfor class c directly against the prediction error for every nodesisdenotedasSd ,whereddenotesthetraversestep\ni selected\notherclassc ,usingtheirdifferencesinsteadofcalculating countstartingfrom1andendinginh(depthofthelabeltree).\nj\nthefullerrorforeachclass. Thisreducesthecomputational We start with the root node n , i.e., S1 = {n },\nroot selected root\nburdensinceclassificationisnowbasedonerrorranking. whichcontainsthehighest-levelcategoriesaschildren.\n3\nPruning Stage Classical Diffusion Classification\nPruning (Eq. 8)\nInput Image:\n...\n... Children ... ...\nlow error\nhigh error\nMonte Carlo Estimate (Eq. 7)\nFinal Prediction (Eq. 9)\nFigure2. OverviewofourHierarchicalDiffusionClassifier(HDC).Startingwithaninputimagex,noiseε∼N(0,I)isaddedtogenerate\nanoisyimage,resultinginx formultipletimestepst. Next,weusethediffusionclassifierwithareducednumberofε-predictionsand\nt\nhierarchicalconditioningpromptslike“Aphotoofa{synclass/classname}”toprogressivelyrefinetheclassificationthroughmultiple\nlevelsofthelabeltree.Bydoingso,wekeeptrackofthemostpromisingclasses(highlightedingreen)andignoretherest(highlightedin\nred).Subsequently,theclassicaldiffusionclassifierpipelineisappliedtothepruned,morespecificsubcategories(leafnodes),whichresults\ninfasterclassificationoverall.\nForeachtraversestepd,weevaluaterecursivelytheerror\nEntity\nscoreforeachchildnodeoftheselectednodesn ∈Sd .\ns selected\nInmoredetail,\nLiving Non-Living\nThing Thing\n∀n ∈Sd : ∀n∈Children(n ): (7)\ns selected s\nϵ =E d(ε,x ,c ). Animal Object Geological\nn t,ε t n Person (3) Formation\n(399) (456)\n(10)\nWe use again Monte Carlo, i.e., Equation 6, to calculate\nϵ ,butemployasmallernumberofsamplesM thaninthe Transport\nn Plant Life Fungus Building\nVehicle\nclassical diffusion classifer. Instead of selecting a single (2) (7) (53)\n(70)\nnode,weproceedwithasetofnodeswiththelowesterror\nscores. Thissetofselectednodesisdeterminedbyapruning Figure3. VisualizationoftheImageNet1Khierarchy, illustrat-\nstrategy,whereonlythemostrelevantnodesarekept. ingthefirstthreelevelsofitstreestructure. Thecategoriesare\nFormally, the set of the next selected nodes Sd+1 at organizedfrombroadentities(e.g.,livingandnon-livingthings)to\nselected\neachstagedisdefinedas morespecificgroups(e.g.,animals,objects,andtransportvehicles),\nwiththenumbersinparenthesesrepresentingthetotalnumberof\nSd+1 ={n∈Children(n )|n ∈Sd , (8) actualclasseswithineachgroup.\nselected s s selected\n∧ϵ ≤threshold(K )}.\nn d\nThepruningratioK determinesthethreshold,whichdic- Morespecifically,thefinalclasslabelisgivenby\nd\ntateshowmanynodesfromthecurrentsetarekeptforthe\nc , wheren =arg min ϵ . (9)\nnext level d+1 of the hierarchy. Essentially, we use the nfinal final\nn∈Sh\nn\nthresholdtoactasatop-kpruning. Thepruningprocedure selected\nisoutlinedinAlgorithm1.\n3.3.TreeSetup\nEventually,thisprocessreachestheleafnodesatd=h,\ncorrespondingtoactualclasslabels.Atthispoint,wederived OurproposedHDCleveragestheWordnethierarchyupon\naprunedclassset,whichwillthensubsequentlybeusedin whichtheImageNet-1Kontologyisconstructed[7]. Theim-\nthe classical diffusion classifier pipeline with the original agesintheImageNet-1Kdatasetaregroupedinto“synonym-\nnumber of samples M to determine the final class label. sets”or“synsets”with12subtreescomprisingaround80,000\n4\nAlgorithm1HierarchicalDiffusionClassifier(HDC)inthe moredefinitemeanings,suchas“animals”. Additionally,to\npruningstageforclassifyingoneimage maketheclassificationmoreefficient,wereducedthedepth\nInput: testimagex,T =(N,E)withnodesN,edgesE ofthetreestructurebyreplacingsubtreeswithasingleleaf\nh\nanddepthh,rootnoden ,labelinputs{c }Nc ,pruning directlywiththeleafnode.OurfinalImageNet-1Khierarchy\nroot i i=1\nratios K , and number of random samples M (see Equa- treehasadepthof7levels.\nd\ntion6). In the pruning stage of HDC, we iterate over the\nImageNet-1Ktreestartingfromnodesatlevel3ofthehier-\n1: //initialization\narchy(“entity”→{“living-thing”,“non-livingthing”}→\n2: Selected=list(Children(n root))\n{“animals”,...}),asshowninFigure3. Startingatlevel2\n3: Errors=dict()\n(“livingthing”vs.“non-livingthing”)showednovariation\n4: ErrorsCalculated=dict()\ninerrorscoresbutincreasedinferencetime.\n5: for eachnoden∈N do\n6: Errors[c n]=list() 3.4.PruningStrategies\n7: ErrorsCalculated[c n]=false\n8: endfor OurproposedHDCmethodallowsmanypruningstrate-\ngiestobeimplementedthatbalanceaccuracyandcompu-\n9:\n10: //modifieddiffusionclassifiererrorcalculations tationalefficiency. Weimplementedtwoprimarypruning\n11: fortreedepthd=1,...,hdo strategies,onethatworkswithfixedratiosofprunednodes\n12: forstagei=1,...,M do andonethatadaptsdynamicallydependingonthedistribu-\n13: Samplet∼[1,1000] tionoferrorpredictions. Inmoredetail:\n14: Sampl √eε∼N √(0,I)\n• Strategy1-FixedPruning: Weselectthetop-knodes\n15: x t = α¯ tx+ 1−α¯ tε\nwiththelowesterrorsateachhierarchylevel,defined\n16:\nbyapruningratioK .\n17: //calculatechilderrors(Equation7) d\n18: foreachnoden sinSelecteddo • Strategy2-DynamicPruning: Inthisapproach,we\n19: foreachchildnoden∈Children(n s)do\nkeeponlynodeswithintwostandarddeviationsofthe\n20: //checkiferroralreadycalculated\nminimumerrorateachlevel,allowingamoreadaptive,\n21: ifErrorsCalculated[c n]then\ndata-drivenselection.\n22: continue\n23: endif IncontrasttoDynamicPruning,FixedPruningallowsfor\n24: finercontroloverthetrade-offbetweenaccuracyandrun-\n25: Errors[c n].append(∥ε−ε θ(x t,c n)∥2) time. Ourproposedpruningstrategiesoffervaryingdegrees\n26: endfor ofcontroloverthebalancebetweenaccuracyandruntime,\n27: endfor adaptingtouniquehierarchicalstructuresforgreaterscala-\n28: endfor bility. Unliketraditionaldiffusionclassifiers,whichevaluate\n29: allclassesforeachinputimage,ourpruningstrategiesstrate-\n30: //descendinthetreeandselecttop-k(Equation8) gicallyselectcandidateclassesateachlevel,reducingcom-\n31: ErrorsCalculated[Selected]=true putationalloadwhilemaintainingsimilarclassprecision.\n32: SelErrors=mean(Errors[Selected])\n33: Selected=TopK(SelErrors,K =K d) 4.ExperimentalSetup\n34: endfor\nThis section describes our experimental setup for test-\n35:\n36: //returnprunedclasslabelset ingtheperformanceandreliabilityofHDConImageNet-\n37: Return: Selected 1K,whichincludesadiscussionabouthowtoconstructthe\nhierarchical label tree and specifics to the classifier itself,\nprompting,andpruningstrategies. Ourcodecanbefound\nonGitHub1.\nsynsets. Tobeginwith,wecreatedahierarchicalpromptlist\nusingthehierarchyfromEngstrometal.[10].\n4.1.ClassifierSetup\nIngeneral,theinferencetimeofHDCincreaseswiththe\nFor our classifier, we built on the efficient framework\ndepthofthetree. Thus,wesimplifiedthelabeltreetoreduce\nestablishedbyLietal.[17],withaddedmodificationstai-\nthe total number of levels. We further modified the exist-\nloredforhierarchicalprocessingandpruningofcandidate\ning Wordnet tree to better suit the classification objective.\nSynsetswithambiguousdescriptionssuchas“artifact”,“or- 1https : / / github . com / arus23 / hierarchical _\nganism”or“implement”aremergedintochildclasseswith diffusion_classifier\n5\nMethod Top1[%] Top3[%] Top5[%] Time[s] Speed-Up[%]\nDiffusionClassifier(baseline)[17] 64.70 84.30 89.70 1600 -\nHDCStrategy1(ours) 64.90 81.80 86.30 980 38.75\nHDCStrategy2(ours) 63.20 82.30 86.30 650 59.38\nTable1.Comparisonofclassificationaccuracyandinferencetimebetweentheclassicaldiffusionclassifier[17]andourproposedHDC\nusingtwopruningstrategieswithStableDiffusion2.0.BothstrategiesdemonstratethatHDCcansignificantlyreduceclassificationtime,\nachievinguptoa60%speed-upininferencetimewithminimalimpactonaccuracyorreachevenbettertop-1precisionwithminimalimpact\nonruntime.Bestresultsaremarkedinbold,second-bestunderlined.\nclasses, further customized for diffusion classification on based on two standard deviations from the lowest error),\nStable Diffusion (SD) [23]. Our HDC setup is adaptable, wereducetheinferencetimeevenfurtherto650seconds,\nallowingseamlessintegrationwithdifferentdiffusionmod- thoughatthecostofaslightaccuracydrop(i.e.,1.50per-\nelsandpossiblefine-tuningtosupportvarioushierarchical centagepoints).Strategy2demonstratesthatfasterinference\npruning strategies, thereby making it versatile. Thus, we canbeachievedwithasmallcompromiseinprecision.\naccommodatetheSDversions1.4,2.0,and2.1inourexperi- In Figure 4, we provide an empirical example of how\nments. ForStrategy1inourpruningsetup,wesetK =0.5 HDCtraversesthelabeltreetofindcandidateclasses(prun-\nd\nforallpossibled-values. ingstage)andcalculatestheerrorofcandidateclassesfor\nAll evaluations were performed on ImageNet-1K at finalprediction(classicaldiffusionclassificationonpruned\n512x512resolution,theresolutionunderwhichallversions leafnodes).\nofSDwereoriginallytrained,ensuringresultsarecompara-\n5.2.StableDiffusionVersions\nbletootherstate-of-the-artmodels. AlsofollowingLietal.,\nweusedthel normtocomputetheϵpredictionerrorand\n2 WeevaluatedtheHDCusingdifferentStableDiffusion\ntimestepsforε -predictionsareuniformlysampledfromthe\nt (SD)versionstoassessitsflexibilityandperformanceacross\nrange[1,1000].\ngenerativebackbones,assummarizedinTable2. Theresults\n4.2.PromptEngineering revealthatSD2.0providesthebesttrade-offbetweenaccu-\nracyandinferencetime. Specifically,whenusingStrategy1,\nTheclasslabelsareconvertedtotheform“aphotoofa SD2.0achievedthehighestTop-1accuracyat64.14%with\n<classlabel>”usingthetemplatefromtheoriginalexper- aninferencetimeof980seconds.Incontrast,SD1.4demon-\niments. Additionally, inspired by Radford et al. [21], we stratesthefastestinferencetimeof710secondswhenpaired\nexperimentwithprompttemplates“Abadphotoofa<class withStrategy2,albeitwithasignificanttop-1class-accuracy\nlabel>”,“Alow-resolutionphotoofa<classlabel>”and reductionto54.77%.\n“itapofa<classlabel>”forImageNet-1K.\n5.3.PromptEngineering\n5.Results\nInspired by Radford et al. [21], we evaluated different\nThissectionpresentsourexperimentalresults,evaluating prompt templates to assess their impact on accuracy and\ndifferentaspectsofHDC,whichwereoutlinedpreviously: inferencetime,asshowninTable3. Thedefaultprompt,“a\npruningstrategies,promptengineering,stablediffusionvari- photo of a <class label>,” consistently achieved the best\nations,and,finally,anoverallevaluationofper-classaccu- performance,suggestingthatastraightforwardpromptyields\nracy. robustresultsacrossclasses. Othertemplates,suchas“abad\nphotoofa<classlabel>”and“alow-resolutionphotoofa\n5.1.PruningStrategies\n<classlabel>,”resultedinaslightdropinaccuracywithout\nTable1highlightstheresultsofourHDCacrossdiffer- significantlyaffectinginferencetime.\nent pruning strategies compared to the classical diffusion Therationalefortestingalternativepromptsstemsfrom\nclassifier.Asobserved,bothstrategies(asoutlinedinsubsec- ahypothesisthatpromptshintingatlower-qualityimages\ntion3.4)showmarkedimprovementsinruntimecomparedto mighthelptheclassifiergeneralizebettertoreal-worldcases\nclassicaldiffusionclassifiers,andeachissuitedtodifferent withvariablequality,capturingdiversevisualcharacteristics.\nprioritizationsofspeedversusaccuracy. Forinstance,usingtermslike“bad”or“low-resolution”was\nForinstance,Strategy1yieldsthebesttrade-offresultson expectedtoenhancerobustnesstonoisyordegradedinputs.\nImageNet-1K,achievingsignificantruntimereductions(up Interestingly,however,theresultsshowthatthesimpler,\nto980seconds)withantop-1accuracyboostof0.20percent- unmodifiedpromptperformsbest,indicatingthatthehierar-\nagepoints. ByemployingStrategy2(selectingcandidates chical model likely benefits from a more neutral prompt\n6\nStrategy1 Strategy2\nSDVersion\nTop1[%] Top1[%] Time[s] Speed-Up[%] Top1[%] Top1[%] Time[s] Speed-Up[%]\n(class-wise) (overall) (class-wise) (overall)\nSD1.4 52.71 52.60 1000 37.50 54.77 54.80 710 55.63\nSD2.0 65.16 64.90 980 38.75 63.33 63.20 980 38.75\nSD2.1 61.15 61.00 950 40.63 60.91 60.70 720 55.00\nTable2.PerformancecomparisonoftheHDCwithdifferentStableDiffusion(SD)versionsusingStrategy1andStrategy2.Top-1accuracy\nandinferencetime(inseconds)arereportedforeachSDversion,highlightingSD2.0asachievingthehighestaccuracy,whileStrategy2in\nSD1.4yieldsthefastestinferencetime.\nStrategy Prompt-Type Top1[%] Top3[%] Top5[%]\n“Aphotoofa<classlabel>” 64.90 80.20 85.30\n“Abadphotoofa<classlabel>” 59.90 79.60 84.90\n1\n“itapofa<classlabel>” 61.37 81.33 86.30\n“Alow-resolutionphotoofa<classlabel>” 57.50 76.46 80.94\n“Aphotoofa<classlabel>” 63.20 82.30 86.30\n“Abadphotoofa<classlabel>” 62.30 80.10 85.90\n2\n“itapofa<classlabel>” 57.80 78.20 82.30\n“Alow-resolutionphotoofa<classlabel>” 57.50 76.46 80.94\nTable3.EvaluationofclassificationaccuracyacrossdifferentprompttypesforHDCusingpruningStrategies1and2.Thestandardprompt,\n“Aphotoofa<classlabel>”,consistentlyyieldsthehighestTop-1,Top-3,andTop-5accuracy.Alternativeprompts,suchas“Abadphoto\nofa<classlabel>”and“Alow-resolutionphotoofa<classlabel>”,resultinslightdecreasesinaccuracy,showingthatpromptvariations\ncanimpactmodelperformance.\nMethod Avg.Accuracy[%] Time[s] Speed-Up[%] 5.4.OverallAccuracyvs. InferenceTime\nDiffusionClassifier 64.90 1600 -\nHDCStrategy1(ours) 65.16 980 38.75 Insummary,Table4showstheoverallaccuracyandinfer-\nHDCStrategy2(ours) 63.33 650 59.38 encetimeacrossdifferentpruningstrategies. Thebaseline\ndiffusionclassifierachievesanaccuracyof64.90%withan\nTable4.ComparisonofaverageclassificationTop1-accuracyand inferencetimeof1600seconds, providingareferencefor\ninferencetimeperclassfortheclassicaldiffusionclassifierandour\nbothspeedandprecision.\nHDCwithStrategies1and2usingSD2.0. Strategy1achieves\nOur HDC using Strategy 1 demonstrates new state-of-\nthehighestaccuracyatK=0.5,whileStrategy2offersthefastest\nthe-artaccuracyfordiffusionclassifierswith65.16%,while\ninferencetimewithminimalaccuracyloss.\nreducingtheinferencetimebynearly40%to980seconds.\nThisindicatesthatHDCcannotonlyimproveclassification\nperformancebutalsobenefitsfromaconsiderablereduction\nin computational load. The reduction in processing time\nformat when dealing with high-quality image data like\nwhilemaintainingsimilaraccuracymakesStrategy1abal-\nImageNet-1K. Nevertheless, these prompt variations may\nancedchoiceforhigh-accuracyapplicationswhereinference\nstillholdpotentialfordatasetswithinherentlylow-resolution\nspeedisalsoapriority.\nordistortedimages,wherequality-basedpromptscouldhelp\nSimilarly,HDCwithStrategy2leveragesdynamicprun-\ntheclassifierlearnmoregeneralizedfeatures.\ningtofurtheraccelerateinference. Whileitrecordsaslight\nWealsoobservedasignificantdisparityininferencetimes drop in accuracy to 63.33%, Strategy 2 reduces inference\nacross specific classes, such as “snail” (221 seconds) ver- time to 650 seconds - approximately 60% faster than the\nsus“keyboardspacebar”(1400seconds). Thisdifference baseline. ThisstrategydemonstratesthepotentialofHDC\nlikelyreflectsthecomplexityofvisualfeatureswithineach for use cases requiring faster response times, with only a\ncategory: classeswithintricateorambiguousfeaturesmay marginaltrade-offinclassificationperformance.\nrequirelongerprocessingtimesduetothehierarchicalclas- In Figure 5, we present a detailed confusion matrix of\nsificationstructure. classeswithinthesynsetcategory“Animal.”Mostmisclas-\n7\nFish 21 7 3\nBird 182 1 3\nSalamander 1 13 1\nFrog 8 1 1\nMammal 474 1 22\nNon-Living\nLiving Thing\nThing Turtle 15\npruned\nLizard 1 31 2\nArachnid 21 2 2\nPerson Animal Object Transport\n(-0.1126) (-0.1125) (-0.1127) (-0.1127) Insect 1 1 58 1 6\nSnake 51 1 1\nCrustacean 1 1 21 4 1\nBird Mammal Bubble\nRest 2 1 1 26 7\n(-0.1123) (-0.1126) (-0.1127)\nFish SBi ar ld amander Frog Mam mal Turtle Lizard Arachnid Insect Sna Ck re ustacean OtR he es rt Classes\nBird of\nCrane Bustard\nPrey\n(-0.1123) (-0.1124)\n(-0.1122) Figure 5. Confusion Matrix of HDC (Strategy 1) for the sub-\ncandidate\nclasses under the synset class “Animal”. The x-axis shows the\nleaves\npredictedlabels(including“otherclasses”outsideofthesynset\nKite (Bird of American\nPrey) Bald Eagle Vulture class“Animal”),andthey-axisshowstheground-truthlabels.\n(-0.1123)\n(-0.1122) (-0.1123)\nfinal prediction\ntheeffectivenessofourapproachondatasetswithcomplex\nFigure4. Exampleclassificationofanimageinthepruningstage\noftheHDCusingStrategy1.Inthisstage,errorscorescalculated oroverlappingclasslabels,suchasthoseinmedicalimaging\nforeachnodeareusedtoiterativelyprunethetree,narrowingit orfine-grainedvisualrecognition,remainsuntested. These\ndowntorelevantleafnodesthatwillundergofurtherrefinement fieldsmaybenefitfromfurtherexplorationofadaptiveprun-\ninsubsequentstages.Thesubsequentstepsthenfocusonclosely ing thresholds or weighted paths, which could prioritize\nrelatednodes(seeleavesunderthepurpleline),suchastheAmeri- highly discriminative regions of the hierarchy, improving\ncanBaldEagleandVulture,ultimatelyselectingtheleafnodewith classificationaccuracyfornuancedcategories.\nthelowesterrorscore—Kite(BirdofPrey)-inthefinalstage.\n7.Conclusion\nsifications occur among biologically similar groups, such\nIn this work, we introduced the Hierarchical Diffu-\nas Salamander-Lizard and Lizard-Snake, highlighting the\nsion Classifier (HDC), a novel approach for accelerating\nclassifier’stendencytogroupcloselyrelatedclasses.\ndiffusion-basedclassificationbyutilizinghierarchicalclass\nOverall,ourresultsshowthatHDCprovidesacustomiz-\npruning. Our results on the ImageNet-1K dataset demon-\nabletrade-offbetweeninferencespeedandaccuracy,making\nstratethatHDCsignificantlyreducesinferencetime,achiev-\nitadaptabletovaryingapplicationneeds.Strategy1ispartic-\ninguptoa60%speedupovertraditionaldiffusionclassifiers\nularlysuitableforhigh-accuracyapplications,whileStrategy\nwhilemaintainingand,insomecases,evenimprovingclassi-\n2isbettersuitedtoreal-timescenariosthatprioritizespeed.\nficationaccuracy. Thisimprovementisachievedbyprogres-\nsivelynarrowingdownrelevantclasscandidates,pruningout\n6.Limitations&FutureWork\nhigh-levelcategoriesearlyintheprocess,andfocusingonly\nWhileourmethodsubstantiallyimprovesinferencetime onspecific,contextuallyrelevantsubcategories.\nandmaintainscompetitiveaccuracy,severallimitationsmust OurexperimentshighlightHDC’sadaptability,showing\nbeaddressedinfuturework. thatdifferentpruningstrategies(suchasTop-kPruningand\nTheefficiencygainsprovidedbythehierarchicalprun- ThresholdPruning)offercustomizabletrade-offsbetween\ningstrategyheavilydependonthedepthandbalanceofthe inferencespeedandaccuracy. ThisversatilitymakesHDC\nunderlyinglabeltree. Datasetswithshallowhierarchiesor suitable for diverse applications, from high-accuracy im-\nthose lacking well-defined parent-child relationships may ageclassificationtaskstoreal-timescenarioswhererapid\nnotbenefitassignificantlyfromourmethod. Furthermore, inferenceiscritical.\n8\nAcknowledgements [15] GeoffreyEHinton.Torecognizeshapes,firstlearntogenerate\nimages. Progressinbrainresearch,165:535–547,2007. 1\nThis work was supported by the BMBF projects Sus-\n[16] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdif-\ntainML(Grant101070408),Albatross(Grant01IW24002)\nfusionprobabilisticmodels. NeurIPS,33:6840–6851,2020.\nandbyCarlZeissFoundationthroughtheSustainableEm- 1\nbeddedAIproject(P2021-02-009). [17] AlexanderCLi, MihirPrabhudesai, ShivamDuggal, Ellis\nBrown,andDeepakPathak. Yourdiffusionmodelissecretly\nReferences\nazero-shotclassifier. InICCV,pages2206–2217,2023. 1,2,\n5,6\n[1] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\n[18] AndreasLugmayr,MartinDanelljan,AndresRomero,Fisher\nMultidiffusion:Fusingdiffusionpathsforcontrolledimage\nYu,RaduTimofte,andLucVanGool. Repaint: Inpainting\ngeneration. 2023. 1\nusing denoising diffusion probabilistic models. In CVPR,\n[2] JamesBetker,GabrielGoh,LiJing,TimBrooks,Jianfeng\n2022. 1\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce\n[19] BrianBMoser,FedericoRaue,SebastianPalacio,Stanislav\nLee, Yufei Guo, et al. Improving image generation with\nFrolov,andAndreasDengel. Latentdatasetdistillationwith\nbetter captions. Computer Science. https://cdn. openai.\ndiffusionmodels. arXivpreprintarXiv:2403.03881,2024. 1,\ncom/papers/dall-e-3.pdf,2(3):8,2023. 1\n2\n[3] Huanran Chen, Yinpeng Dong, Shitong Shao, Zhongkai\n[20] Brian B Moser, Arundhati S Shanbhag, Federico Raue,\nHao, Xiao Yang, Hang Su, and Jun Zhu. Your diffusion\nStanislav Frolov, Sebastian Palacio, and Andreas Dengel.\nmodelissecretlyacertifiablyrobustclassifier. arXivpreprint\nDiffusionmodels,imagesuper-resolutionandeverything:A\narXiv:2402.02316,2024. 1\nsurvey. arXivpreprintarXiv:2401.00736,2024. 1\n[4] RickyTQChen,XuechenLi,RogerBGrosse,andDavidK\n[21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nDuvenaud.Isolatingsourcesofdisentanglementinvariational\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nautoencoders. NeurIPS,31,2018. 1\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\n[5] XiChen,YanDuan,ReinHouthooft,JohnSchulman,Ilya\nKrueger, and Ilya Sutskever. Learning transferable visual\nSutskever, and Pieter Abbeel. Infogan: Interpretable rep-\nmodelsfromnaturallanguagesupervision,2021. 6\nresentationlearningbyinformationmaximizinggenerative\n[22] DaniloRezendeandShakirMohamed. Variationalinference\nadversarialnets. NeurIPS,29,2016. 1\nwithnormalizingflows. InICML,pages1530–1538.PMLR,\n[6] KevinClarkandPriyankJaini. Text-to-imagediffusionmod-\n2015. 1\nelsarezero-shotclassifiers,2023. 1,2\n[23] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\n[7] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLi\nPatrick Esser, and Bjo¨rn Ommer. High-resolution image\nFei-Fei. Imagenet:Alarge-scalehierarchicalimagedatabase.\nsynthesiswithlatentdiffusionmodels.InCVPR,pages10684–\nIn 2009 IEEE conference on computer vision and pattern\n10695,2022. 2,3,6\nrecognition,pages248–255.Ieee,2009. 2,4\n[24] ChitwanSaharia,WilliamChan,SaurabhSaxena, LalaLi,\n[8] PrafullaDhariwalandAlexanderNichol. Diffusionmodels\nbeatgansonimagesynthesis.Advancesinneuralinformation JayWhang,EmilyLDenton,KamyarGhasemipour,Raphael\nprocessingsystems,34:8780–8794,2021. 2 GontijoLopes,BurcuKaragolAyan,TimSalimans,etal.Pho-\ntorealistictext-to-imagediffusionmodelswithdeeplanguage\n[9] JeffDonahue,PhilippKra¨henbu¨hl,andTrevorDarrell. Ad-\nunderstanding. NeurIPS,35:36479–36494,2022. 2\nversarialfeaturelearning. arXivpreprintarXiv:1605.09782,\n2016. 1 [25] ChristophSchuhmann,RomainBeaumont,RichardVencu,\nCadeGordon,RossWightman,MehdiCherti,TheoCoombes,\n[10] LoganEngstrom,AndrewIlyas,HadiSalman,ShibaniSan-\nAarushKatta,ClaytonMullis,MitchellWortsman,Patrick\nturkar, and Dimitris Tsipras. Robustness (python library),\nSchramowski,SrivatsaKundurthy,KatherineCrowson,Lud-\n2019. 5\nwigSchmidt,RobertKaczmarczyk,andJeniaJitsev. Laion-\n[11] Stanislav Frolov, Tobias Hinz, Federico Raue, Jo¨rn Hees,\n5b:Anopenlarge-scaledatasetfortrainingnextgeneration\nandAndreasDengel. Adversarialtext-to-imagesynthesis:A\nimage-textmodels,2022. 2\nreview. NeuralNetworks,144:187–209,2021. 1,2\n[26] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale,\n[12] StanislavFrolov,BrianBMoser,andAndreasDengel. Spotd-\nTrungBui,TongYu,ZheLin,YangZhang,andShiyuChang.\niffusion:Afastapproachforseamlesspanoramageneration\nUncoveringthedisentanglementcapabilityintext-to-image\novertime. arXivpreprintarXiv:2407.15507,2024. 1\ndiffusionmodels. InCVPR,pages1900–1910,2023. 1\n[13] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Das- [27] LvminZhang,AnyiRao,andManeeshAgrawala. Adding\nsarma,DawnDrain,NelsonElhage,etal. Predictabilityand conditional control to text-to-image diffusion models. In\nsurpriseinlargegenerativemodels.In2022ACMConference ICCV,pages3836–3847,2023. 1\nonFairness,Accountability,andTransparency,2022. 2\n[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu,DavidWarde-Farley,SherjilOzair,AaronCourville,and\nYoshuaBengio.Generativeadversarialnetworks.InNeurIPS,\n2014. 1,2\n9",
    "pdf_filename": "Just_Leaf_It_Accelerating_Diffusion_Classifiers_with_Hierarchical_Class_Pruning.pdf"
}