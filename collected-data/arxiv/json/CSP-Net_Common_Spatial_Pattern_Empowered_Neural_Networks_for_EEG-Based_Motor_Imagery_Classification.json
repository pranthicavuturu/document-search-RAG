{
    "title": "CSP-Net Common Spatial Pattern Empowered Neural Networks for EEG-Based Motor Imagery Classification",
    "abstract": "Electroencephalogram-basedmotor imagery (MI) classiﬁcation is an important paradigm of non-invasivebrain-computer interfaces. Common spatial pattern (CSP), which exploits different energy distributions on the scalp while performing different MI tasks, is very popular in MI classiﬁcation. Convolutional neural networks (CNNs) have also achieved great success, due to their powerful learning capabilities. This paper proposes two CSP-empowered neural networks (CSP-Nets), which integrate knowledge-driven CSP ﬁlters with data-driven CNNs to enhance the performance in MI classiﬁcation. CSP-Net-1 directly adds a CSP layer before a CNN to improve the input discriminability. CSP-Net-2 replaces a convolutional layer in CNN with a CSP layer. The CSP layer parameters in both CSP-Nets are initialized with CSP ﬁlters designed from the training data. During training, they can either be kept ﬁxed or optimized using gradient descent. Experiments on four public MI datasets demonstrated that the two CSP-Nets consistently improved over their CNN backbones, in both within-subject and cross-subject classiﬁcations. They are particularly useful when the number of training samples is very small. Our work demonstrates the advantage of integrating knowledge-driven traditional machine learning with data-driven deep learning in EEG-based brain-computer interfaces.",
    "body": "arXiv:2411.11879v1  [eess.SP]  4 Nov 2024\nCSP-Net: Common Spatial Pattern Empowered Neural Networks for EEG-Based Motor\nImagery Classiﬁcation\nXue Jianga,b, Lubin Menga,b, Xinru Chena,b, Yifan Xua,b, Dongrui Wua,b,∗\naKey Laboratory of the Ministry of Education for Image Processing and Intelligent Control, School of Artiﬁcial Intelligence and Automation, Huazhong University\nof Science and Technology, Wuhan 430074, China\nbShenzhen Huazhong University of Science and Technology Research Institute, Shenzhen 518063, China\nAbstract\nElectroencephalogram-basedmotor imagery (MI) classiﬁcation is an important paradigm of non-invasivebrain-computer interfaces.\nCommon spatial pattern (CSP), which exploits different energy distributions on the scalp while performing different MI tasks, is\nvery popular in MI classiﬁcation. Convolutional neural networks (CNNs) have also achieved great success, due to their powerful\nlearning capabilities. This paper proposes two CSP-empowered neural networks (CSP-Nets), which integrate knowledge-driven\nCSP ﬁlters with data-driven CNNs to enhance the performance in MI classiﬁcation. CSP-Net-1 directly adds a CSP layer before\na CNN to improve the input discriminability. CSP-Net-2 replaces a convolutional layer in CNN with a CSP layer. The CSP layer\nparameters in both CSP-Nets are initialized with CSP ﬁlters designed from the training data. During training, they can either be kept\nﬁxed or optimized using gradient descent. Experiments on four public MI datasets demonstrated that the two CSP-Nets consistently\nimproved over their CNN backbones, in both within-subject and cross-subject classiﬁcations. They are particularly useful when\nthe number of training samples is very small. Our work demonstrates the advantage of integrating knowledge-driven traditional\nmachine learning with data-driven deep learning in EEG-based brain-computer interfaces.\nKeywords: Brain-computer interfaces, electroencephalogram, motor imagery, common spatial pattern, convolutional neural\nnetwork\n1. Introduction\nA brain-computer interface (BCI) establishes a direct com-\nmunication pathway that enables the human brain to interact\nwith external devices [1]. Electroencephalogram (EEG), which\nrecords the electrical activities on the scalp of the brain, is the\nmost widely used input signal in non-invasive BCIs due to its\naffordability and convenience [2]. EEG-based BCIs have been\nused in controlling robots [3], decoding speech [4], enhancing\ncomputer gaming experience [5], and so on.\nMotor imagery (MI) [6] is a classical paradigm of EEG-\nbased BCIs, where a subject imagines the movement of a body\npart, e.g., right hand, left hand, right foot, left foot, both feet,\nand/or tongue, without actually executing it. An MI induces\nchanges in the sensory-motor rhythms (SMR) of corresponding\nareas of the cerebral cortex, which primarily involve modula-\ntions of the µ rhythm (8-12Hz) and the β rhythm (14-30Hz) [7].\nSpeciﬁcally, when an MI starts, these rhythmic activities de-\ncrease, resulting in event-related desynchronization (ERD); at\nthe end of an MI, these rhythmic activities increase, resulting\nin event-related synchronization (ERS) [8, 9]. Therefore, the\ndetection of SMR patterns within speciﬁc areas of the cerebral\ncortex can be used to identify which body part the subject is\nimagining moving.\n∗Emails: xuejiang@hust.edu.cn (Xue Jiang), lubinmeng@hust.edu.cn (Lu-\nbin Meng), xrchen@hust.edu.cn (Xinru Chen), yfxu@hust.edu.cn (Yifan Xu),\ndrwu@hust.edu.cn (Dongrui Wu). Dongrui Wu is the corresponding author.\nMany algorithms have been proposed for EEG-based MI\nclassiﬁcation. Common spatial pattern (CSP) [10, 11] is one\nof the most widely used and effective approaches, which con-\nverts the raw multi-channel EEG signals into more discrimina-\ntive spatial patterns. It was initially proposed for binary classi-\nﬁcation, by designing spatial ﬁlters that maximize the variance\nratio of the ﬁltered signals of different classes [10]. Dornhege\net al. [12] extended it to multi-class classiﬁcation using a one-\nversus-the-rest strategy. Ang et al. [13] proposed ﬁlter bank\nCSP (FBCSP), which bandpass ﬁlters EEG signals into multi-\nple frequency bands, extracts CSP features from each band, and\nthen selects the most useful features for classiﬁcation. Lotte et\nal. [14] introduced regularized CSP to enhance the robustness\nof CSP.\nRecent years have witnessed signiﬁcant increase in using\ndeep learning for EEG signal decoding [15], which integrates\nfeature extraction and classiﬁcation into a single end-to-end\nnetwork. Among various deep architectures, convolutional neu-\nral networks (CNNs) are the most prevalent for MI classiﬁca-\ntion [16, 17]. For example, Schirrmeister et al. [18] proposed\nShallowCNN and DeepCNN for raw EEG classiﬁcation. Shal-\nlowCNN is inspired by FBCSP and includes components such\nas temporal convolution, spatial convolution, log-variance cal-\nculation and a classiﬁer, each corresponding to a speciﬁc step in\nFBCSP. DeepCNN is similar but includes more convolutional\nand pooling layers. Lawhern et al. [19] introduced a com-\npact EEGNet, which has demonstrated promising performance\nPreprint submitted to Elsevier\nNovember 20, 2024\n\nacross various BCI tasks, including MI classiﬁcation. Inspired\nalso by FBCSP, EEGNet uses a two-step sequence of tempo-\nral convolution followed by depthwise convolution. Recently,\nFBCNet [20] extends the FBCSP approach by utilizing a hi-\nerarchical architecture that enhances feature extraction through\nmulti-dimensional ﬁltering, allowing it to capture richer spatial\nand temporal patterns in EEG data. EEGConformer [21] adopts\na transformer-like architecture, combining self-attention mech-\nanisms with convolutional layers, which enables the model to\nlearn long-range dependencies in EEG signals effectively.\nThough these data-driven deep models have achieved\npromising performance in MI classiﬁcation, they usually re-\nquire a large amount of labeled training data, which may not\nbe always available in practice. This highlights the need to in-\ncorporate prior knowledge into EEG networks, as it can help re-\nduce the reliance on extensive labeled datasets. By integrating\nprior knowledge, models can leverage existing insights about\nEEG signal characteristics, enhancing their generalization ca-\npabilities and performance even in data-scarce environments.\nThis paper proposes CSP empowered neural networks (CSP-\nNet), which more effectively integrate CSP and CNNs. More\nspeciﬁcally, we propose two CSP-Nets, by embedding CSP\ninto different layers of the CNN models. The ﬁrst, CSP-Net-1,\nplaces a CSP layer before a CNN to ﬁlter the EEG signals for\nenhancing their discriminability. The second, CSP-Net-2, re-\nplaces a CNN’s convolutional layer with a CSP layer to provide\ntask-speciﬁc prior knowledge initialization. The parameters in\nthe CSP layer of both CSP-Nets are initialized from CSP ﬁlters\ndesigned on the training data. They can be ﬁxed or optimized\nby gradient descent during training. In summary, CSP-Nets in-\ntegrate the strengths of traditional CSP feature extraction with\ndeep learning by embedding CSP layers in CNN architectures.\nThis approach enhances the model’s ability to capture relevant\nfeatures from EEG signals, making it a more effective solution\nfor MI classiﬁcation. Our main contributions are:\n• Integration of CSP and CNNs: We propose a novel frame-\nwork that combines CSP with CNNs for MI classiﬁcation,\nenhancing EEG feature extraction and improving classiﬁ-\ncation performance.\n• Two CSP-Net Variants: We introduce two architectures,\nCSP-Net-1, which incorporates a CSP layer before the\nCNN, and CSP-Net-2, which replaces a convolutional\nlayer with a CSP layer for task-speciﬁc prior knowledge\ninitialization. Both variants allow CSP layer parameters to\nbe ﬁxed or further optimized.\n• Performance on Multiple EEG Datasets:\nCSP-Nets\ndemonstrate strong performance across various scenarios,\nincluding within-subject and cross-subject classiﬁcations,\nas well as in small sample settings. The models demon-\nstrate generalization across different backbone architec-\ntures, validated on four public MI datasets.\nThe rest of this paper is structured as follows. Section 2 intro-\nduces the classical CSP and proposes two CSP-Nets. Section 3\npresents the experimental settings and experimental results. Fi-\nnally, Section 4 draws conclusions.\n2. Methods\nThis section introduces the CSP algorithm, ﬁve CNN models\nfor MI classiﬁcation, and our proposed two CSP-Nets to inte-\ngrate CSP and CNNs.\n2.1. CSP\nCSP was ﬁrst proposed by Koles et al. [22] to extract dis-\ncriminative features from EEG signals of two human popula-\ntions. M¨ueller-Gerking et al. [23] later extended it to MI clas-\nsiﬁcation. Since then, it has become one of the most popular\nand effective algorithms in MI-based BCIs [10, 11].\nFig. 1 shows t-SNE visualization of some real EEG trials be-\nfore and after CSP ﬁltering from Subject 3 of Dataset 2a in BCI\nCompetition IV [24]. Clearly, after CSP ﬁltering, samples from\ndifferent classes become more distinguishable.\n(a)\n(b)\nFigure 1: t-SNE visualization of (a) the raw EEG trials; and, (b) the CSP-\nﬁltered trials. Different shapes (colors) represent different classes.\nFor binary classiﬁcation, CSP aims to learn spatial ﬁlters that\nmaximize the variance of EEG signals from one class while\nsimultaneously minimizing the variance from the other class.\nLet Xi ∈Rc×t be an EEG trial of MI task i, where i ∈{1, 2} is\nthe class index, c the number of channels, and t the number of\ntime domain samples. CSP generates a spatial ﬁltering matrix\nW ∈Rc× f (f < c) that projects the original EEG trials into\na lower-dimensional space with higher discriminability. W is\nobtained by maximizing (or minimizing):\nJ(W) = W⊤¯X1 ¯X⊤\n1 W\nW⊤¯X2 ¯X⊤\n2 W = W⊤¯C1W\nW⊤¯C2W ,\n(1)\nwhere ¯Xi ∈Rc×t is the averaged EEG trial from class i, and\n¯Ci ∈Rc×c the mean spatial covariance matrix of all EEG trials\nin class i.\nSince J(W) = J(kW) for any arbitrary real constant k, max-\nimizing J(W) is equivalent to maximizing W⊤¯C1W, subject to\nthe constraint W⊤¯C2W = I f. This optimization problem can\nbe solved using the Lagrange multiplier method [14], whose\nLagrange function is\nF(W, λ) = W⊤¯C1W −λ(W⊤¯C2W −I f ).\n(2)\nSetting the derivative of F(W, λ) with respect to W to 0, we\n2\n\nhave\n∂F(W, λ)\n∂W\n= 2W⊤¯C1 −2λW⊤¯C2 = 0\n⇔¯C1W = λ ¯C2W\n⇔¯C−1\n2\n¯C1W = λW,\nwhich becomes a standard eigenvalue decomposition problem.\nThe spatial ﬁltering matrix W consists of eigenvectors cor-\nresponding to the\nf\n2 largest and the\nf\n2 smallest eigenvalues of\n¯C−1\n2\n¯C1.\n2.2. CNNs for MI Classiﬁcation\nFive popular CNN models are considered in this paper: EEG-\nNet [19], DeepCNN [18], ShallowCNN [18], FBCNet [20], and\nEEGConformer [21]. Their architectures are detailed in Ta-\nbles 1-5, respectively.\n• EEGNet [19], which consists of three convolutional blocks\nand a classiﬁer block. The ﬁrst convolutional block per-\nforms temporal ﬁltering for capturing frequency informa-\ntion. The second spatial ﬁlter block uses depthwise con-\nvolution with size (c, 1) to learn spatial ﬁlters. The third\nseparable convolutional block is used to reduce the num-\nber of parameters and decouple the relationships within\nand across feature maps.\n• DeepCNN [18], compared with EEGNet, it is deeper and\nhence has much more parameters. It mainly includes a\ntemporal convolutional block, a spatial ﬁlter block, two\nstandard convolutional blocks and a classiﬁer block. The\nﬁrst temporal and spatial convolutional blocks are spe-\ncially designed to handle EEG inputs and the other two\nare standard ones.\n• ShallowCNN [18], which is a shallow version of Deep-\nCNN, inspired by FBCSP. Its ﬁrst two blocks are similar\nto the temporal and spatial convolutional blocks of Deep-\nCNN, but with a larger kernel, a different activation func-\ntion, and a different pooling approach.\n• FBCNet [20], which is a simple yet effective CNN archi-\ntecture. It begins by applying multiple ﬁxed-parameter\nband-pass ﬁlters to decompose the EEG into various fre-\nquency bands as multi-view inputs. Spatial ﬁlter block is\nthen used to extract spatially discriminative patterns from\neach frequency band. Finally, a classiﬁer block is designed\nfor classiﬁcation.\n• EEGConformer [21], which is a compact convolutional\ntransformer model. The convolution module also includes\na temporal convolutional block and a spatial ﬁlter block\nfor learning the low-level local features. The multiple self-\nattention modules are used to extract the global correlation\nwithin the local features.\nTable 1: EEGNet [19].\nBlock\nLayer\nFilter size\nNumber of\nﬁlters\nTemporal\nConv2D\n(1, fs\n2 )\n4\nconvolution\nBatch normalization\n-\n-\nDepthwiseConv2D\n(c, 1)\n8\nDepthwise\nBatch normalization\n-\n-\nspatial ﬁlter\nELU activation\n-\n-\nAverage pooling\n(1, 4)\n-\nDropout\n-\n-\nSeparableConv2D\n(1, 16)\n8\nBatch normalization\n-\n-\nSeparable\nPointwiseCon2D\n(1, 1)\n8\nconvolution\nBatch normalization\n-\n-\nELU activation\n-\n-\nAverage pooling\n(1, 8)\n-\nDropout\n-\n-\nClassiﬁer\nFully connection\n-\n-\nTable 2: DeepCNN [18].\nBlock\nLayer\nFilter size\nNumber of\nﬁlters\nTemporal\nConv2D\n(1, 5)\n25\nconvolution\nSpatial ﬁlter\nConv2D\n(c, 1)\n25\nBatch normalization\n-\n-\nELU activation\n-\n-\nMax pooling\n(1, 2)\n-\nDropout\n-\n-\nConv2D\n(1, 5)\n50\nStandard\nBatch normalization\n-\n-\nconvolution\nELU activation\n-\n-\nMax pooling\n(1, 2)\n-\nDropout\n-\n-\nConv2D\n(1, 5)\n100\nStandard\nBatch normalization\n-\n-\nconvolution\nELU activation\n-\n-\nMax pooling\n(1, 2)\n-\nDropout\n-\n-\nClassiﬁer\nFully connection\n-\n-\nTable 3: ShallowCNN [18].\nBlock\nLayer\nFilter size\nNumber of\nﬁlters\nTemporal\nConv2D\n(1, 13)\n40\nconvolution\nSpatial ﬁlter\nConv2D\n(c, 1)\n40\nBatch Normalization\n-\n-\nSquaring Activation\n-\n-\nAverage Pooling\n(1, 35)\n-\nLogarithmic Activation\n-\n-\nDropout\n-\n-\nClassiﬁer\nFully Connection\n-\n-\nTable 4: FBCNet [20].\nBlock\nLayer\nFilter size\nNumber of\nﬁlters\nBand-pass\n-\n-\n6\nﬁlter\nDepthwiseConv2D\n(c, 1)\n48\nSpatial\nBatch normalization\n-\n-\nﬁlter\nSwish activation\n-\n-\nVariance layer\n-\n-\nClassiﬁer\nFully connection\n-\n-\n3\n\nFigure 2: Our proposed CSP-Nets. (a) Traditional CSP ﬁlters are used to initialize the CSP layer in CSP-Nets. (b) CSP-Net-1, which directly adds a CSP layer\nbefore a CNN backbone. (c) CSP-Net-2, illustrated using EEGNet [19] (Table 1 in Supplementary Materials); the DepthwiseConv2D layer in its depthwise spatial\nﬁlter block is replaced by a CSP layer.\nTable 5: EEGConformer [21].\nBlock\nLayer\nFilter size\nNumber of\nﬁlters\nTemporal\nConv2D\n(1, 25)\n40\nconvolution\nConv2D\n(c, 1)\n40\nBatch normalization\n-\n-\nELU activation\n-\n-\nSpatial ﬁlter\nAverage pooling\n(1, 75)\n-\nDropout\n-\n-\nPointwiseConv2D\n(1, 1)\n40\nRearrange\n-\n-\nLayer normalization\n-\n-\nMHA\n-\n-\nDropout\n-\n-\n6×\nResidual add\n-\n-\nSelf-attention\nLayer normalization\n-\n-\nFFN\n-\n-\nDropout\n-\n-\nResidual add\n-\n-\n2.3. CSP-Net-1\nOur proposed CSP-Net-1 simply performs CSP before a\nCNN.\nAs illustrated in Fig. 2(a), all training EEG samples are used\nin CSP, resulting in f spatial ﬁlters Wi ∈Rc×1, i = 1, . . ., f.\nThen, as shown in Fig. 2(b), CSP-Net-1 uses these ﬁlters to\nspatially ﬁlter the raw EEG signals, before passing them to a\nCNN backbone.\nThere could be two different training approaches: 1) Fix the\nCSP layer and train the CNN backbone only (CSP-Net-1-ﬁx);\nand, 2) update the CSP layer and the CNN backbone simulta-\nneously (CSP-Net-1-upd). Their effectiveness will be discussed\nin Section 3.3.\nCSP-Net-1 applies CSP ﬁltering as a pre-processing step, en-\nabling the model to work with more discriminative input sig-\nnals. This explicit inclusion of the CSP ﬁlter provides a more\nstructured way to embed expert knowledge into the network,\nthereby improving the model’s capacity to capture task-relevant\nspatial features.\nAlgorithm 1 gives the pseudo-code of CSP-Net-1.\nAlgorithm 1: CSP-Net-1 for MI classiﬁcation.\nInput: Training data X; a CNN model.\n1 Perform CSP on X to obtain the ﬁlter matrix W;\n2 Initialize CSP-Net-1, which consists of a CSP layer with\nweights W and a randomly initialized CNN;\n3 Train CSP-Net-1 on X;\n4 return CSP-Net-1.\n2.4. CSP-Net-2\nMany CNN models have been proposed for MI classiﬁcation,\nwhich typically consist of multiple convolution-pooling layers\nfor feature extraction and some fully connected layers for clas-\nsiﬁcation. Although they differ in architecture, they usually\ninclude a spatial ﬁlter layer with spatial convolutional kernels\nspeciﬁcally designed for EEG signals.\nCSP-Net-2 replaces their spatial ﬁlter layer with a CSP layer.\nFig. 2(c) uses EEGNet as the CNN backbone to illustrate the\narchitecture of CSP-Net-2. For clarity, we primarily depict the\n4\n\nAlgorithm 2: CSP-Net-2 for MI classiﬁcation.\nInput: Training data X; a CNN model.\n1 Perform CSP on X to obtain the ﬁlter matrix W;\n2 Randomly initialize the CNN model;\n3 Initialize CSP-Net-2, by replacing the convolutional\nlayer in the spatial ﬁlter block of the CNN model by a\nCSP layer with weights W;\n4 Train CSP-Net-2 on X;\n5 return CSP-Net-2.\nconnection of the convolutional kernel between inputs and out-\nputs. The depthwise spatial ﬁlter block aims to learn spatial\npatterns in EEG data. CSP-Net-2 replaces the convolutional\nkernels in this block with the CSP ﬁlters, and keeps other parts\nunchanged.\nMore speciﬁcally, CSP-Net-2 uses the CSP layer to replace\nthe DepthwiseConv2D layer in the spatial ﬁlter block of EEG-\nNet (8 kernels), the Conv2D layer in the spatial ﬁlter block\nof DeepCNN (25 kernels), the Conv2D layer in the spatial ﬁl-\nter block of ShallowCNN (40 kernels), the DepthwiseConv2D\nlayer in the spatial ﬁlter block of FBCNet (48 kernels), and the\nConv2D layer in the spatial ﬁlter block of EEGConformer (40\nkernels).\nSimilar to CSP-Net-1, the CSP ﬁlter layer in CSP-Net-2 can\neither be ﬁxed (CSP-Net-2-ﬁx) or updated (CSP-Net-2-upd).\nFurthermore, this replacement is signiﬁcant as it allows CSP-\nNet-2 to explicitly incorporate prior knowledge about spatial ﬁl-\ntering, enhancing the model’s ability to capture discriminative\nfeatures from the EEG signals. The ﬂexibility of using either\nﬁxed or updated CSP ﬁlters also provides a balance between\nstability and adaptability during training, which we discussed\nin detail in Section 3.3.\nAlgorithm 2 gives the pseudo-code of CSP-Net-2.\n3. Experiments and Results\nThis section presents the experimental results to validate the\neffectiveness of our proposed CSP-Nets.\n3.1. Datasets\nFour public MI datasets from BNCI-Horizon 1, summarized\nin Table 6, were used in our experiments:\n1. MI4C and MI2C: They were from the 001-2014 dataset.\nThe EEG signals were sampled at 250Hz. MI2C includes\nonly left-hand and right-hand trials. MI4C includes all\nclasses.\n2. MI14S: This was from the 002-2014 dataset. The EEG\nsignals were sampled at 512Hz.\n3. MI9S: This was the 001-2015 dataset. The EEG signals\nwere recorded at 512Hz. The last three subjects were dis-\ncarded due to their poor performance [25, 26].\n1http://www.bnci-horizon-2020.eu/database/data-sets\nThey were downloaded and pre-processed using the MOABB\nframework [27]. All datasets were pre-processed with an 8-\n32Hz bandpass ﬁlter.\nTable 6: Summary of the four MI datasets.\nDatasets # Subjects # Channels # Trials per subject # Classes\nMI4C\n9\n22\n288\n4\nMI2C\n9\n22\n144\n2\nMI14S\n14\n15\n100\n2\nMI9S\n9\n13\n200\n2\n3.2. Implementation Details\nWe evaluated the performance of CSP-Nets in both within-\nsubject and cross-subject classiﬁcations:\n1. Within-subject classiﬁcation: For each individual subject,\n80% trials were used for training, and the remaining 20%\nfor testing.\n2. Cross-subject classiﬁcation: Leave-one-subject-out cross-\nvalidation was performed, i.e., one subject was used as the\ntest set and all remaining ones as the training set.\nAll experiments were repeated 5 times, and the average ac-\ncuracies are reported.\nWe used Adam optimizer with batch size 128 and initial\nlearning rate 0.01, and cross-entropy loss with weight decay\n0.0005. The maximum number of training epochs was 200.\nThe CSP layer used by default f = 8 spatial ﬁlters (Section 3.6\npresents sensitivity analysis). For CSP-Net-2, the number of\nconvolutional kernels in the original spatial ﬁlter layer of the\nCNN models may be larger than 8. We expanded the CSP ﬁlters\nto address this mismatch: when the number of convolutional\nkernels exceeds the number of CSP ﬁlters, we replicate the CSP\nﬁlters to match the number of required kernels. Speciﬁcally,\nwe duplicated the 8 CSP ﬁlters 5 times to match the 40 kernels\nin the spatial ﬁlter block of ShallowCNN and EEGConformer\n(8 × 5 = 40), duplicated the 8 CSP ﬁlters 6 times to match the\n48 kernels in the spatial ﬁlter block of FBCNet (8 × 6 = 48),\nand duplicated the 8 CSP ﬁlters 3 times and randomly selected\none more to match the 25 kernels in the spatial ﬁlter block of\nDeepCNN (8 × 3 + 1 = 25).\n3.3. Experimental Results\nTable 7 shows the classiﬁcation accuracies for the individ-\nual subjects on MI4C, where CSP-LR used logistic regression\nas the classiﬁer. Tables 8-10 show the average classiﬁcation\nresults across all subjects on the other three datasets, due to\npage limit. We performed paired t-tests on the results, calcu-\nlated p-values between the standard backbone models and the\nCSP-Nets, and adjusted them using Benjamini Hochberg False\nDiscovery Rate correction. Observe that:\n1. Both CSP-Nets were generally highly effective on all\ndatasets and backbones. Embedding CSP knowledge in\nCNN backbones resulted in signiﬁcant performance im-\nprovements. For example, in within-subject classiﬁcation\n5\n\non MI4C, CSP-Net-2-ﬁx increased the average accuracy\non all subjects from 63.50% to 71.91% after integrating\nCSP information into EEGNet as CSP-Net-2-ﬁx.\nThe\naverage accuracies across all ﬁve backbones also exhib-\nited signiﬁcant improvements, from an initial accuracy of\n61.32% to as high as 67.33%.\n2. CSP-Nets with ﬁxed CSP layer parameters generally\nperformed better.\nParticularly, CSP-Net-2-ﬁx achieved\nsubstantial improvements over EEGNet, DeepCNN, and\nEEGConformer. This validated that the incorporation of\nCSP prior knowledge can enhance the generalization of\nCNN models. High number and proportion of parameters\nof spatial convolutional kernels in ShallowCNN and FBC-\nNet may overshadow the beneﬁts offered by CSP ﬁlters.\n3. CSP-Nets\nhad larger performance improvements in\nwithin-subject classiﬁcation than cross-subject classiﬁca-\ntion. This might be because: 1) within-subject classiﬁca-\ntion had much fewer training samples than cross-subject\nclassiﬁcation, and hence prior knowledge in CSP is more\nhelpful to the generalization performance; and, 2) cross-\nsubject classiﬁcation is intrinsically more challenging, as\nthere are large individual differences among different sub-\njects. The impact of training data quantity on CSP-Nets is\ndiscussed in Section 3.5.\n4. CSP-Nets achieved better performance on most subjects.\nHowever, for some subjects where CSP did not perform\nwell, CSP-Nets also struggled, e.g., Subject 1, 2, 5 and 6\nin cross-subject classiﬁcation on MI4C.\n3.4. Comparative Performance Analysis\nWe further compared our approaches with nine other ap-\nproaches, including the state-of-the-art traditional approaches\nand deep learning approaches:\nCSP [10], FBCSP [13],\nMDRM\n[28],\nDeepCNN\n[18],\nLMDA-Net [29],\nShal-\nlowCNN [18], EEGConformer [21], EEGNet [19], and FBC-\nNet [20]. Table 11 presents the classiﬁcation accuracies of CSP-\nNet-1 and CSP-Net-2 compared to these baselines. In CSP-Net-\n1 and CSP-Net-2, the EEGNet was used as the backbone archi-\ntecture, and the ﬁxed CSP layer was applied. The same training\nand test data were used for all models.\nBoth CSP-Net-1 and CSP-Net-2 demonstrated superior per-\nformance compared to traditional approaches like CSP and\nFBCSP, as well as more recent models like FBCNet and EEG-\nConformer. These results highlight their effectiveness for EEG\nsignal classiﬁcation tasks.\n3.5. Small Sample Setting\nDeep CNN models may easily overﬁt when the training\ndataset is small. Figs. 3-6 show the accuracy improvements\ncompared with the backbone at different training data ratios (the\nnumber of training samples used to train the model divided by\nthe total number training samples) on the four datasets, respec-\ntively.\nObserve that:\n1. Consistent with previous ﬁndings, CSP-Net-ﬁx generally\noutperformed CSP-Net-upd, with CSP-Net-2-ﬁx particu-\nlarly competitive on EEGNet and DeepCNN. For example,\nin within-subject classiﬁcation, CSP-Net-2-ﬁx achieved a\nremarkable accuracy improvement of more than 20% over\nthe DeepCNN backbone, when trained with only 50% of\nthe training data on MI4C.\n2. Overall, the performance improvements of CSP-Nets were\nmore obvious when the training data size was small. This\nmay be because the embedding of prior knowledge greatly\nreduces the overﬁtting issue of CNN backbones in small\nsample scenarios.\n3.6. Inﬂuence of the Number of CSP Filters\nWe further investigated the inﬂuence of the number of CSP\nﬁlters (f) on the performance of CSP-Nets with three back-\nbones on MI4C. The dataset includes 22-channel EEG signals,\nso we considered f ∈{4, 8, 12, 16, 22}. Fig. 7 shows the corre-\nsponding accuracies of the two CSP-Nets (ﬁxed CSP layer). As\nthe number of ﬁlters increased, the accuracy ﬁrst increased and\nthen decreases, which is intuitive. Generally, f = 8 seems to\nbe a good choice to balance the performance and computational\ncost.\n3.7. Ablation Studies\nAn ablation study was performed to verify that the perfor-\nmance improvement of CSP-Net-1 was not due to an increase\nin the number of network parameters.\nSpeciﬁcally, we trained CSP-Net-1-rad, which replaced the\nCSP layer of CSP-Net-1 with a randomly initialized layer of the\nsame size. The results on the four MI datasets in within-subject\nclassiﬁcation are shown in Table 12. Generally, CSP-Net-1-\nrad performed similarly to the standard backbone, suggesting\nthat the performance improvement of CSP-Net-1 was due to its\nincorporation of knowledge from CSP, instead of more param-\neters.\n3.8. Training Process Visualization\nFig. 8 shows the average cross-subject training and test ac-\ncuracy curves of CSP-Nets (ﬁxed CSP layer) and their corre-\nsponding backbones (EEGNet) on the four datasets. For all the\nbackbones, there was a large gap between the training and test\ncurves, indicating overﬁtting. Our proposed CSP-Nets effec-\ntively leveraged the knowledge from the CSP ﬁlters for better\ninitialization, reducing the gap and achieving better test perfor-\nmance.\n3.9. Visualization of the CSP Filters\nWe further visualized the spatial convolutional kernel\nweights from the CSP-Net-2 and the counterparts from stan-\ndard backbone. In Fig. 9, we present the eight spatial ﬁlters\nin the EEGNet model and the CSP ﬁlters in the CSP-Net-2-ﬁx\nmodel for within-subject classiﬁcation on Subject 1 of MI2C\n(binary classiﬁcation on the left hand and right hand). We can\nobserve that the CSP ﬁlters in CSP-Net-2-ﬁx exhibited a more\nfocused and obvious left-right distribution concentrated on a\n6\n\nTable 7: Classiﬁcation accuracies (%) on MI4C. Average accuracies higher than Standard are marked in bold. Asterisks indicate statistically signiﬁcant differences\nbetween standard backbone and CSP-Net under adjusted paired t-test, where * means p < 0.05, ** means p < 0.01, *** means p < 0.001.\nScenario\nBackbone\nApproach\nS1\nS2\nS3\nS4\nS5\nS6\nS7\nS8\nS9\nAverage acc±std\n-\nCSP-LR\n71.85\n61.27\n78.64\n48.52\n35.32\n40.14\n69.07\n75.00\n70.37\n61.13\nEEGNet\nStandard\n72.59\n49.35\n81.36\n44.51\n49.31\n39.29\n65.98\n84.19\n84.93\n63.50\nCSP-Net-1-upd\n83.40\n57.31\n88.26\n50.36\n58.36\n43.90\n75.52\n84.01\n87.59\n69.86***±1.54\nCSP-Net-1-ﬁx\n81.43\n58.73\n90.70\n53.48\n53.73\n45.74\n82.42\n83.95\n86.88\n70.79***±1.68\nCSP-Net-2-upd\n77.83\n56.85\n86.91\n47.95\n46.24\n41.35\n74.20\n80.74\n84.11\n66.24±2.89\nCSP-Net-2-ﬁx\n81.18\n63.55\n92.36\n52.69\n56.75\n46.27\n83.02\n82.64\n88.76\n71.91***±0.74\nDeepCNN\nStandard\n56.86\n45.93\n66.55\n39.72\n25.48\n30.14\n57.34\n68.66\n72.93\n51.51±1.13\nCSP-Net-1-upd\n69.07\n57.07\n80.32\n50.76\n38.34\n37.82\n68.72\n73.76\n78.98\n61.65***±1.70\nCSP-Net-1-ﬁx\n70.12\n56.24\n81.90\n52.06\n46.05\n38.28\n69.04\n72.94\n82.85\n63.28***±1.26\nCSP-Net-2-upd\n62.52\n41.36\n59.02\n43.28\n25.64\n25.00\n54.47\n70.01\n71.74\n50.34±0.95\nCSP-Net-2-ﬁx\n73.07\n55.34\n82.59\n51.55\n45.90\n38.18\n74.78\n76.61\n81.52\n64.39***±2.46\nShallowCNN\nStandard\n69.35\n57.51\n74.62\n53.04\n50.74\n46.14\n76.48\n76.33\n81.95\n65.13±1.38\nCSP-Net-1-upd\n79.99\n57.93\n84.78\n57.87\n54.98\n44.48\n85.43\n80.79\n80.92\n69.69***±1.58\nCSP-Net-1-ﬁx\n80.15\n59.93\n86.51\n58.04\n52.83\n44.48\n85.14\n83.04\n83.64\n70.42***±1.41\nCSP-Net-2-upd\n70.21\n57.53\n79.94\n50.79\n37.88\n40.42\n76.08\n83.20\n80.67\n64.08±1.62\nWithin-\nCSP-Net-2-ﬁx\n69.04\n63.02\n86.55\n45.62\n31.48\n38.89\n77.06\n79.06\n82.27\n63.66±2.01\nSubject\nFBCNet\nStandard\n69.27\n53.87\n83.77\n50.17\n53.03\n43.78\n69.61\n80.22\n83.95\n65.30±1.76\nCSP-Net-1-upd\n77.39\n53.70\n86.65\n52.61\n60.79\n45.62\n78.23\n86.81\n84.71\n69.61***±1.23\nCSP-Net-1-ﬁx\n76.41\n56.87\n87.44\n49.11\n53.56\n42.79\n77.72\n86.07\n86.01\n68.44**±2.04\nCSP-Net-2-upd\n71.19\n54.88\n87.29\n53.58\n54.56\n43.96\n72.34\n83.05\n86.46\n67.48*±1.85\nCSP-Net-2-ﬁx\n76.74\n57.42\n86.45\n46.79\n53.81\n43.64\n71.47\n82.65\n85.98\n67.22±1.02\nEEGConformer\nStandard\n75.00\n46.97\n79.81\n50.23\n37.46\n46.05\n72.25\n77.19\n65.42\n61.15±1.67\nCSP-Net-1-upd\n81.98\n63.81\n87.68\n58.57\n60.62\n52.46\n88.04\n82.96\n71.73\n71.98***±1.38\nCSP-Net-1-ﬁx\n84.28\n59.84\n87.41\n61.85\n58.49\n53.90\n90.18\n79.96\n71.82\n71.97***±1.14\nCSP-Net-2-upd\n80.49\n58.97\n89.79\n60.50\n56.39\n55.85\n84.24\n85.14\n75.04\n71.82***±1.04\nCSP-Net-2-ﬁx\n82.55\n60.73\n86.94\n45.70\n58.18\n46.37\n78.67\n83.13\n82.95\n69.47***±1.04\nAverage\nStandard\n68.61\n50.73\n77.22\n47.53\n43.20\n41.08\n68.33\n77.32\n77.84\n61.32±1.57\nCSP-Net-1-upd\n78.37\n57.96\n85.54\n54.03\n54.62\n44.86\n79.19\n81.67\n80.79\n68.56***±1.49\nCSP-Net-1-ﬁx\n78.48\n58.32\n86.79\n54.91\n52.93\n45.04\n80.90\n81.19\n82.24\n68.98***±1.51\nCSP-Net-2-upd\n72.45\n53.92\n80.59\n51.22\n44.14\n41.32\n72.27\n80.43\n79.60\n63.99***±1.67\nCSP-Net-2-ﬁx\n76.52\n60.01\n86.98\n48.47\n49.22\n42.67\n77.00\n80.82\n84.30\n67.33***±1.45\n-\nCSP-LR\n61.46\n22.92\n72.22\n43.06\n32.29\n39.58\n62.50\n76.04\n62.85\n52.55\nEEGNet\nStandard\n68.96\n30.35\n71.88\n38.06\n37.01\n36.74\n42.01\n58.19\n61.74\n49.44±1.75\nCSP-Net-1-upd\n65.62\n32.15\n72.92\n41.39\n34.93\n38.68\n55.28\n65.69\n61.74\n52.04*±1.48\nCSP-Net-1-ﬁx\n62.50\n32.43\n76.32\n40.97\n34.58\n37.43\n52.01\n67.64\n65.62\n52.17*±1.13\nCSP-Net-2-upd\n66.94\n32.64\n74.17\n42.01\n35.07\n40.97\n42.78\n67.08\n63.82\n51.72*±1.59\nCSP-Net-2-ﬁx\n66.46\n31.53\n74.44\n39.65\n31.04\n37.29\n53.40\n63.06\n68.61\n51.72±1.38\nDeepCNN\nStandard\n65.35\n34.03\n52.99\n37.92\n38.19\n43.40\n41.81\n59.24\n53.47\n47.38±2.10\nCSP-Net-1-upd\n61.32\n33.40\n67.57\n41.81\n35.56\n40.90\n43.19\n63.19\n57.01\n49.33±0.70\nCSP-Net-1-ﬁx\n60.07\n34.79\n62.22\n42.50\n35.21\n40.97\n41.67\n67.01\n60.14\n49.40±0.53\nCSP-Net-2-upd\n64.93\n31.18\n65.00\n38.96\n39.58\n41.67\n48.61\n56.94\n57.15\n49.34±1.11\nCSP-Net-2-ﬁx\n62.01\n29.93\n69.44\n37.99\n36.32\n39.44\n52.92\n63.33\n61.67\n50.34*±1.81\nShallowCNN\nStandard\n68.12\n33.40\n71.04\n41.46\n36.18\n45.49\n43.82\n69.51\n64.17\n52.58±0.65\nCSP-Net-1-upd\n66.25\n30.63\n70.69\n41.53\n32.15\n40.49\n50.00\n64.65\n59.03\n50.60±1.23\nCSP-Net-1-ﬁx\n68.12\n31.04\n72.01\n42.71\n32.78\n39.24\n53.96\n69.38\n61.60\n52.31±1.28\nCSP-Net-2-upd\n66.39\n30.63\n72.57\n43.12\n32.29\n43.06\n47.29\n67.57\n63.61\n51.84±1.25\nCross-\nCSP-Net-2-ﬁx\n62.15\n28.47\n72.08\n39.58\n32.36\n41.67\n54.93\n67.64\n67.64\n51.84±1.04\nSubject\nFBCNet\nStandard\n62.92\n31.39\n62.22\n41.67\n31.25\n36.25\n40.76\n55.21\n57.01\n46.52±1.49\nCSP-Net-1-upd\n66.18\n30.90\n62.50\n42.43\n31.53\n38.33\n43.26\n57.43\n60.49\n48.12±1.31\nCSP-Net-1-ﬁx\n67.22\n31.39\n67.43\n39.51\n31.11\n38.06\n45.90\n60.76\n60.28\n49.07**±1.11\nCSP-Net-2-upd\n64.65\n30.42\n61.94\n41.39\n30.83\n37.92\n40.69\n56.88\n57.64\n46.93±1.27\nCSP-Net-2-ﬁx\n57.50\n32.50\n64.38\n40.35\n36.67\n36.88\n49.79\n64.31\n65.00\n49.71**±1.24\nEEGConformer\nStandard\n52.64\n27.64\n49.51\n33.68\n32.29\n39.86\n30.14\n54.51\n41.60\n40.21±1.67\nCSP-Net-1-upd\n61.94\n35.07\n63.06\n34.38\n32.36\n37.85\n26.11\n61.39\n55.76\n45.32***±0.90\nCSP-Net-1-ﬁx\n58.54\n35.83\n63.33\n35.35\n32.64\n38.19\n27.78\n64.03\n56.18\n45.76***±1.09\nCSP-Net-2-upd\n55.76\n33.06\n71.67\n38.19\n33.19\n35.14\n44.03\n61.39\n63.75\n48.46***±0.62\nCSP-Net-2-ﬁx\n56.81\n33.26\n72.92\n39.65\n31.11\n37.57\n44.44\n67.22\n65.83\n49.87***±0.94\nAverage\nStandard\n63.60\n31.36\n61.53\n38.56\n34.98\n40.35\n39.71\n59.33\n55.60\n47.22±1.53\nCSP-Net-1-upd\n64.26\n32.43\n67.35\n40.31\n33.31\n39.25\n43.57\n62.47\n58.81\n49.08***±1.12\nCSP-Net-1-ﬁx\n63.29\n33.10\n68.26\n40.21\n33.26\n38.78\n44.26\n65.76\n60.76\n49.74***±1.03\nCSP-Net-2-upd\n63.73\n31.59\n69.07\n40.73\n34.19\n39.75\n44.68\n61.97\n61.19\n49.66***±1.17\nCSP-Net-2-ﬁx\n60.99\n31.14\n70.65\n39.44\n33.50\n38.57\n51.10\n65.11\n65.75\n50.69***±1.28\n7\n\nTable 8: Average classiﬁcation accuracies (%) on MI2C. Those higher than Standard are marked in bold. Asterisks indicate statistically signiﬁcant differences\nbetween standard backbone and CSP-Net under adjusted paired t-test, where * means p < 0.05, ** means p < 0.01, *** means p < 0.001.\nScenario\nApproach\nBackbone\nEEGNet\nDeepCNN\nShallowCNN\nFBCNet\nEEGConformer\nAverage acc±std\nCSP-LR\n-\n-\n-\n-\n-\n75.72\nStandard\n76.38±2.21\n61.46±3.28\n76.29±2.92\n78.11±2.64\n75.31±1.45\n73.51±2.50\nWithin-\nCSP-Net-1-upd\n80.02±2.80\n70.86***±3.58\n82.50**±2.60\n80.70*±1.85\n81.06***±1.46\n79.02***±2.45\nsubject\nCSP-Net-1-ﬁx\n81.69*±0.49\n70.37***±3.41\n83.71***±1.45\n82.39**±2.70\n82.05***±0.88\n80.04***±1.78\nCSP-Net-2-upd\n75.94±2.24\n61.59±1.93\n77.33±1.68\n79.65±2.48\n81.53***±2.80\n75.21**±2.23\nCSP-Net-2-ﬁx\n79.66*±2.38\n75.86***±1.11\n75.34±1.81\n79.54±0.58\n81.18**±1.61\n78.32***±1.50\nCSP-LR\n-\n-\n-\n-\n-\n72.92\nStandard\n71.50±0.87\n73.15±1.37\n74.32±1.50\n69.34±1.50\n65.25±1.03\n70.71±1.16\nCross-\nCSP-Net-1-upd\n73.53*±1.39\n74.86*±0.73\n74.65±0.63\n71.00±1.60\n70.94***±0.67\n73.00***±1.00\nsubject\nCSP-Net-1-ﬁx\n74.51**±1.28\n74.75±1.36\n75.51±0.65\n73.09***±1.35\n71.20***±0.57\n73.81***±1.04\nCSP-Net-2-upd\n72.31±1.23\n72.93±0.48\n70.40±0.87\n69.41±1.71\n70.76***±1.37\n71.16±1.13\nCSP-Net-2-ﬁx\n75.25***±1.28\n73.38±0.91\n76.11±0.80\n73.61***±0.27\n72.95***±0.55\n74.26***±0.76\nTable 9: Average classiﬁcation accuracies (%) on MI14S. Those higher than Standard are marked in bold. Asterisks indicate statistically signiﬁcant differences\nbetween standard backbone and CSP-Net under adjusted paired t-test, where * means p < 0.05, ** means p < 0.01, *** means p < 0.001.\nScenario\nApproach\nBackbone\nEEGNet\nDeepCNN\nShallowCNN\nFBCNet\nEEGConformer\nAverage acc±std\nCSP-LR\n-\n-\n-\n-\n-\n74.95\nStandard\n75.22±1.94\n55.75±2.92\n70.70±1.90\n78.55±1.75\n75.72±1.88\n71.19±2.08\nWithin-\nCSP-Net-1-upd\n76.69±3.23\n61.03**±3.12\n73.12±2.02\n81.25*±1.50\n81.07**±2.21\n74.6***±2.42\nsubject\nCSP-Net-1-ﬁx\n78.92*±1.67\n61.33**±1.51\n74.89*±1.84\n81.17±1.98\n80.96**±1.23\n75.45***±1.61\nCSP-Net-2-upd\n76.61±2.37\n56.84±1.19\n73.94**±1.35\n79.43±2.90\n79.96*±2.32\n73.36***±2.03\nCSP-Net-2-ﬁx\n80.01**±1.47\n66.81***±2.26\n77.14***±2.17\n77.51±2.06\n78.64±3.25\n76.02***±2.25\nCSP-LR\n-\n-\n-\n-\n-\n74.21\nStandard\n73.37±1.09\n68.51±1.26\n70.80±0.48\n72.91±0.91\n63.10±2.41\n69.74±1.23\nCross-\nCSP-Net-1-upd\n73.19±1.34\n71.94***±1.26\n73.03**±0.22\n73.08±0.93\n73.83***±0.61\n73.01***±0.76\nsubject\nCSP-Net-1-ﬁx\n76.60***±0.62\n71.56***±0.55\n73.87***±0.30\n73.91±0.55\n73.71***±0.47\n73.93***±0.49\nCSP-Net-2-upd\n73.07±1.94\n70.04±1.28\n69.67±1.43\n73.11±1.64\n70.93***±1.06\n71.36***±1.47\nCSP-Net-2-ﬁx\n75.44*±1.15\n71.06***±0.31\n69.83±0.97\n72.64±0.70\n71.21***±0.48\n72.04***±0.72\nTable 10: Average classiﬁcation accuracies (%) on MI9S. Those higher than Standard are marked in bold. Asterisks indicate statistically signiﬁcant differences\nbetween standard backbone and CSP-Net under adjusted paired t-test, where * means p < 0.05, ** means p < 0.01, *** means p < 0.001.\nScenario\nApproach\nBackbone\nEEGNet\nDeepCNN\nShallowCNN\nFBCNet\nEEGConformer\nAverage acc±std\nCSP-LR\n-\n-\n-\n-\n-\n67.84\nStandard\n70.85±0.88\n62.34±3.39\n69.03±1.51\n72.55±1.49\n69.65±1.08\n68.88±1.67\nWithin-\nCSP-Net-1-upd\n73.01±2.31\n62.92±1.53\n72.59*±0.98\n73.70±1.12\n74.91**±2.22\n71.43***±1.63\nsubject\nCSP-Net-1-ﬁx\n73.63*±1.85\n63.63±1.78\n73.17***±1.43\n74.31±2.17\n73.22*±0.79\n71.59***±±1.61\nCSP-Net-2-upd\n72.01±3.06\n62.78±3.09\n71.38*±1.07\n71.67±2.49\n73.55*±2.07\n70.28**±2.36\nCSP-Net-2-ﬁx\n71.83±1.98\n65.57±0.96\n67.19±1.68\n72.41±2.57\n72.79±2.76\n69.96±1.99\nCSP-LR\n-\n-\n-\n-\n-\n57.72\nStandard\n62.40±1.26\n54.99±1.15\n62.26±0.79\n63.84±1.35\n58.92±1.74\n60.48±1.26\nCross-\nCSP-Net-1-upd\n64.77*±1.75\n59.49**±2.51\n61.73±0.70\n63.41\n±0.69\n62.73**±0.92\n62.43***±1.31\nsubject\nCSP-Net-1-ﬁx\n64.91*±2.28\n60.07**±1.91\n62.16±1.45\n62.88\n±0.95\n63.11**±0.59\n62.63**±1.44\nCSP-Net-2-upd\n62.03±1.76\n61.98***±0.95\n63.53±1.78\n63.54±1.15\n59.96±0.91\n62.21**±1.31\nCSP-Net-2-ﬁx\n64.72±0.59\n59.87**±1.70\n62.14±0.58\n61.14±1.49\n59.96±0.69\n61.57±1.01\nTable 11: Classiﬁcation accuracies (%) for each model, averaged over all subjects.\nScenario\nDataset\nCSP-\nFBCSP-\nMDRM\nDeep-\nLMDA-\nShallow-\nEEGCon-\nEEGNet\nFBCNet\nCSP-Net-1\nCSP-Net-2\nLR\nLR\nCNN\nNet\nCNN\nformer\nMI4C\n61.43\n66.81\n64.21\n51.51\n56.75\n65.13\n61.15\n63.50\n65.30\n71.98\n71,82\nMI2C\n75.72\n76.72\n76.32\n61.46\n71.19\n76.29\n75.31\n76.38\n78.11\n81.06\n81.53\nWithin-subject\nMI14S\n74.95\n75.39\n78.58\n55.75\n72.04\n70.70\n75.72\n75.22\n78.55\n81.07\n79.96\nMI9S\n67.84\n69.37\n67.90\n62.34\n69.94\n69.03\n69.65\n70.85\n72.55\n74.91\n73.55\nAverage\n69.99\n72.07\n71.75\n57.77\n67.48\n70.29\n70.46\n71.49\n73.63\n76.26\n75.85\nMI4C\n52.55\n49.72\n51.08\n47.38\n48.28\n52.58\n40.21\n49.44\n46.52\n52.17\n51.72\nMI2C\n72.92\n73.62\n71.99\n73.15\n69.10\n74.32\n65.25\n71.50\n69.34\n74.51\n75.25\nCross-subject\nMI14S\n74.21\n76.36\n71.71\n68.51\n67.87\n70.80\n63.10\n73.37\n72.91\n76.60\n75.44\nMI9S\n57.27\n63.61\n57.39\n54.99\n63.41\n62.26\n58.92\n62.40\n63.84\n64.91\n64.72\nAverage\n64.24\n65.83\n63.04\n61.01\n62.17\n64.99\n56.87\n64.18\n63.15\n67.05\n66.78\n8\n\n(a)\n(b)\nFigure 3: Accuracy improvements of CSP-Nets at different training data ratios on MI4C. (a) within-subject classiﬁcation; and, (b) cross-subject classiﬁcation.\n(a)\n(b)\nFigure 4: Accuracy improvements of CSP-Nets at different training data ratios on MI2C. (a) within-subject classiﬁcation; and, (b) cross-subject classiﬁcation.\n9\n\n(a)\n(b)\nFigure 5: Accuracy improvements of CSP-Nets at different training data ratios on MI14S. (a) within-subject classiﬁcation; and, (b) cross-subject classiﬁcation.\n(a)\n(b)\nFigure 6: Accuracy improvements of CSP-Nets at different training data ratios on MI9S. (a) within-subject classiﬁcation; and, (b) cross-subject classiﬁcation.\n10\n\n\b\n\f\n\u0006\u0007\n\u0006\n\u0007\u0007\n\t\u0005\n\t\t\n\n\u0005\n\n\t\n\u000b\u0005\n\u000b\t\n\r\u0017\u0017$!\u0015\u0017(\n\u0010\u0010\u0011\u0012\u0018#\n\u000e\u0014\u0013\u0004\u0012\u0018#\u0004\u0006\u0004\u0019\u001c'\n\u000e\u0014\u0013\u0004\u0012\u0018#\u0004\u0007\u0004\u0019\u001c'\n\b\n\f\n\u0006\u0007\n\u0006\n\u0007\u0007\n\t\u0005\n\t\t\n\n\u0005\n\n\t\n\u000b\u0005\n\u000b\t\n\r\u0017\u0017$!\u0015\u0017(\n\u000f\u0018\u0018 \u000e\u0012\u0012\n\u000e\u0014\u0013\u0004\u0012\u0018#\u0004\u0006\u0004\u0019\u001c'\n\u000e\u0014\u0013\u0004\u0012\u0018#\u0004\u0007\u0004\u0019\u001c'\n\b\n\f\n\u0006\u0007\n\u0006\n\u0007\u0007\n\t\u0005\n\t\t\n\n\u0005\n\n\t\n\u000b\u0005\n\u000b\t\n\r\u0017\u0017$!\u0015\u0017(\n\u0014\u001b\u0015\u001d\u001d\u001f&\u000e\u0012\u0012\n\u000e\u0014\u0013\u0004\u0012\u0018#\u0004\u0006\u0004\u0019\u001c'\n\u000e\u0014\u0013\u0004\u0012\u0018#\u0004\u0007\u0004\u0019\u001c'\n\b\n\f\n\u0006\u0007\n\u0006\n\u0007\u0007\n\u0012$\u001e\u0016\u0018!\u0000\u001f\u0019\u0000\u0019\u001c\u001d#\u0018!\"\u0000\u0002f\u0003\n\t\u0005\n\t\t\n\n\u0005\n\n\t\n\u000b\u0005\n\u000b\t\n\r\u0017\u0017$!\u0015\u0017(\u0000\u0002\u0001\u0003\n\r%\u0018!\u0015\u001a\u0018\n\u000e\u0014\u0013\u0004\u0012\u0018#\u0004\u0006\u0004\u0019\u001c'\n\u000e\u0014\u0013\u0004\u0012\u0018#\u0004\u0007\u0004\u0019\u001c'\n(a)\n\b\n\u000b\n\u0006\u0007\n\u0006\n\u0007\u0007\n\b\n\b\u000b\n\t\u0005\n\t\u0007\n\t\b\n\f\u0016\u0016# \u0014\u0016'\n\u000f\u000f\u0010\u0011\u0017\"\n\r\u0013\u0012\u0004\u0011\u0017\"\u0004\u0006\u0004\u0018\u001b&\n\r\u0013\u0012\u0004\u0011\u0017\"\u0004\u0007\u0004\u0018\u001b&\n\b\n\u000b\n\u0006\u0007\n\u0006\n\u0007\u0007\n\b\n\b\u000b\n\t\u0005\n\t\u0007\n\t\b\n\f\u0016\u0016# \u0014\u0016'\n\u000e\u0017\u0017\u001f\r\u0011\u0011\n\r\u0013\u0012\u0004\u0011\u0017\"\u0004\u0006\u0004\u0018\u001b&\n\r\u0013\u0012\u0004\u0011\u0017\"\u0004\u0007\u0004\u0018\u001b&\n\b\n\u000b\n\u0006\u0007\n\u0006\n\u0007\u0007\n\b\n\b\u000b\n\t\u0005\n\t\u0007\n\t\b\n\f\u0016\u0016# \u0014\u0016'\n\u0013\u001a\u0014\u001c\u001c\u001e%\r\u0011\u0011\n\r\u0013\u0012\u0004\u0011\u0017\"\u0004\u0006\u0004\u0018\u001b&\n\r\u0013\u0012\u0004\u0011\u0017\"\u0004\u0007\u0004\u0018\u001b&\n\b\n\u000b\n\u0006\u0007\n\u0006\n\u0007\u0007\n\u0011#\u001d\u0015\u0017 \u0000\u001e\u0018\u0000\u0018\u001b\u001c\"\u0017 !\u0000\u0002f\u0003\n\b\n\b\u000b\n\t\u0005\n\t\u0007\n\t\b\n\f\u0016\u0016# \u0014\u0016'\u0000\u0002\u0001\u0003\n\f$\u0017 \u0014\u0019\u0017\n\r\u0013\u0012\u0004\u0011\u0017\"\u0004\u0006\u0004\u0018\u001b&\n\r\u0013\u0012\u0004\u0011\u0017\"\u0004\u0007\u0004\u0018\u001b&\n(b)\nFigure 7: Classiﬁcation accuracies of CSP-Nets using different number of CSP\nﬁlters on MI4C. (a) within-subject classiﬁcation; and, (b) cross-subject classi-\nﬁcation.\nTable 12: Ablation study results of CSP-Net-1. Average accuracies higher than\nStandard are marked in bold.\nDataset\nBackbone\nApproach\nTraining data ratio (%)\n10\n30\n50\n70\n100\nMI4C\nEEGNet\nStandard\n37.67 45.60 54.29 57.61 63.50\nCSP-Net-1-rad 37.60 47.19 54.37 57.62 65.60\nCSP-Net-1-ﬁx\n43.39 57.75 62.28 65.72 70.79\nDeepCNN\nStandard\n27.17 28.90 32.13 44.66 51.51\nCSP-Net-1-rad 27.33 31.40 32.84 47.53 53.51\nCSP-Net-1-ﬁx\n30.97 36.95 43.56 56.09 63.28\nShallowCNN\nStandard\n34.98 48.69 55.42 59.01 65.13\nCSP-Net-1-rad 36.78 47.59 54.16 60.68 64.69\nCSP-Net-1-ﬁx\n40.73 55.44 60.00 66.47 70.42\nAverage\nStandard\n33.27 41.06 47.28 53.76 60.05\nCSP-Net-1-rad 33.90 42.06 47.12 55.28 61.27\nCSP-Net-1-ﬁx\n38.36 50.05 55.28 62.76 68.16\nMI2C\nEEGNet\nStandard\n56.77 67.53 69.37 70.61 76.38\nCSP-Net-1-rad 57.77 66.76 68.51 71.85 77.11\nCSP-Net-1-ﬁx\n62.12 71.73 75.29 78.45 81.69\nDeepCNN\nStandard\n51.04 50.78 51.74 55.13 61.46\nCSP-Net-1-rad 51.97 52.50 54.14 56.94 62.69\nCSP-Net-1-ﬁx\n52.07 60.04 63.82 68.17 70.37\nShallowCNN\nStandard\n51.38 62.34 68.36 71.70 76.29\nCSP-Net-1-rad 54.24 64.55 69.45 71.21 77.44\nCSP-Net-1-ﬁx\n62.42 72.12 76.69 81.37 83.71\nAverage\nStandard\n53.06 60.22 63.16 65.81 71.38\nCSP-Net-1-rad 54.66 61.27 64.03 66.67 72.41\nCSP-Net-1-ﬁx\n58.87 67.96 71.93 76.00 78.59\nMI14S\nEEGNet\nStandard\n59.10 66.99 72.25 73.59 75.22\nCSP-Net-1-rad 57.55 68.00 71.30 71.79 74.40\nCSP-Net-1-ﬁx\n61.79 73.53 75.72 78.76 78.92\nDeepCNN\nStandard\n51.11 50.80 53.92 56.64 55.75\nCSP-Net-1-rad 49.64 53.58 55.95 57.04 55.99\nCSP-Net-1-ﬁx\n49.33 53.99 55.40 56.87 61.33\nShallowCNN\nStandard\n53.05 57.86 62.07 65.42 70.70\nCSP-Net-1-rad 53.08 57.48 61.79 66.19 71.56\nCSP-Net-1-ﬁx\n54.00 66.40 71.24 72.82 74.89\nAverage\nStandard\n54.42 58.55 62.75 65.22 67.22\nCSP-Net-1-rad 53.42 59.69 63.01 65.01 67.32\nCSP-Net-1-ﬁx\n55.04 64.64 67.45 69.48 71.71\nMI9S\nEEGNet\nStandard\n57.47 65.43 68.11 70.85 70.85\nCSP-Net-1-rad 59.03 64.89 67.19 69.97 69.57\nCSP-Net-1-ﬁx\n61.64 67.39 69.93 71.41 73.63\nDeepCNN\nStandard\n52.06 52.53 54.28 59.09 62.34\nCSP-Net-1-rad 51.93 53.01 54.22 56.45 61.92\nCSP-Net-1-ﬁx\n52.56 56.00 55.96 57.06 63.63\nShallowCNN\nStandard\n51.89 57.55 61.90 66.04 69.03\nCSP-Net-1-rad 52.45 57.89 62.16 64.19 69.28\nCSP-Net-1-ﬁx\n55.54 63.96 67.90 70.70 73.17\nAverage\nStandard\n53.81 58.50 61.43 65.33 67.41\nCSP-Net-1-rad 54.47 58.60 61.19 63.54 66.92\nCSP-Net-1-ﬁx\n56.58 62.45 64.60 66.39 70.14\n11\n\n(a)\nFigure 8: The training and test curves of different models on (a) MI4C; (b) MI2C; (c) MI14S; and, (d) MI9S.\n(a)\n(b)\nFigure 9: Visualization of eight (a) spatial ﬁlters in EEGNet and (b) the CSP ﬁlters in CSP-Net-2-ﬁx for the within-subject classiﬁcation on the Subject 1 of MI2C.\nspeciﬁc sensorimotor area, which aligned well with the spatial\ncharacteristics of MI. In contrast, the standard EEGNet strug-\ngled to learn effective spatial information due to the absence\nof prior knowledge provided by CSP. This alignment suggests\nthat CSP ﬁlters effectively capture the relevant spatial patterns\ninherent in the EEG data, enhancing the interpretability of the\nmodel.\n4. Conclusions\nSpatial information, which can be well captured by CSP ﬁl-\nters, is critical in EEG-based MI classiﬁcation.\nThis paper\nhas introduced two CSP-Nets, which integrate the knowledge-\ndriven CSP ﬁlters with data-driven CNN models. CSP-Net-1\ndirectly adds a CSP layer before a CNN, utilizing CSP-ﬁltered\nsignals as input to enhance the discriminability. CSP-Net-2 re-\nplaces a convolutional layer in CNN with a CSP layer. Exper-\niments on four public MI datasets demonstrated that the two\nCSP-Nets consistently improved over their CNN backbones, in\nboth within-subject and cross-subject classiﬁcations. They are\nparticularly useful when the number of training samples is very\nsmall. Remarkably, CSP-Net-1-ﬁx, whose CSP layer uses ﬁxed\nweights calculated using the traditional CSP algorithm, is the\nsimplest yet demonstrates overall best performance.\nOur\nwork\ndemonstrates the\nadvantage of\nintegrating\nknowledge-driven CSP ﬁlters with data-driven CNNs, or tra-\nditional machine learning with deep learning, in EEG-based\nBCIs.\nReferences\n[1] B. Graimann, B. Allison, G. Pfurtscheller, Brain-Computer Interfaces: A\nGentle Introduction, Springer, Berlin, Heidelberg, 2009, pp. 1–27.\n[2] L. F. Nicolas-Alonso, J. Gomez-Gil, Brain computer interfaces, a review,\nSensors 12 (2) (2012) 1211–1279.\n[3] L. R. Hochberg, D. Bacher, B. Jarosiewicz, N. Y. Masse, J. D. Simeral,\nJ. Vogel, S. Haddadin, J. Liu, S. S. Cash, P. Van Der Smagt, et al., Reach\nand grasp by people with tetraplegia using a neurally controlled robotic\narm, Nature 485 (7398) (2012) 372–375.\n[4] G. K. Anumanchipalli, J. Chartier, E. F. Chang, Speech synthesis from\nneural decoding of spoken sentences, Nature 568 (7753) (2019) 493–498.\n[5] M. Krauledat, K. Grzeska, M. Sagebaum, B. Blankertz, C. Vidaurre, K.-\nR. M¨uller, M. Schr¨oder, Playing pinball with non-invasive BCI, Advances\nin Neural Information Processing Systems 21 (2008) 1641–1648.\n[6] G. Pfurtscheller, C. Neuper, Motor imagery and direct brain-computer\ncommunication, Proc. of the IEEE 89 (7) (2001) 1123–1134.\n[7] M. Jeannerod, Mental imagery in the motor context, Neuropsychologia\n33 (11) (1995) 1419–1432.\n[8] G. Pfurtscheller, C. Neuper, D. Flotzinger, M. Pregenzer, EEG-based dis-\ncrimination between imagination of right and left hand movement, Elec-\ntroencephalography and Clinical Neurophysiology 103 (6) (1997) 642–\n651.\n[9] B. Blankertz, C. Sannelli, S. Halder, E. M. Hammer, A. K¨ubler, K.-R.\nM¨uller, G. Curio, T. Dickhaus, Neurophysiological predictor of SMR-\nbased BCI performance, NeuroImage 51 (4) (2010) 1303–1309.\n[10] H. Ramoser, J. Muller-Gerking, G. Pfurtscheller, Optimal spatial ﬁlter-\ning of single trial EEG during imagined hand movement, IEEE Trans. on\nNeural Systems and Rehabilitation Engineering 8 (4) (2000) 441–446.\n[11] B. Blankertz, R. Tomioka, S. Lemm, M. Kawanabe, K.-r. Muller, Op-\ntimizing spatial ﬁlters for robust EEG single-trial analysis, IEEE Signal\nProcessing Magazine 25 (1) (2008) 41–56.\n[12] G. Dornhege, B. Blankertz, G. Curio, K.-R. Muller, Boosting bit rates in\nnoninvasive EEG single-trial classiﬁcations by feature combination and\nmulticlass paradigms, IEEE Trans. on Biomedical Engineering 51 (6)\n(2004) 993–1002.\n[13] K. K. Ang, Z. Y. Chin, H. Zhang, C. Guan, Filter bank common spatial\n12\n\npattern (FBCSP) in brain-computer interface, in: Proc. IEEE Int’l Joint\nConf. on Neural Networks, Hong Kong, China, 2008, pp. 2390–2397.\n[14] F. Lotte, C. Guan, Regularizing common spatial patterns to improve BCI\ndesigns: uniﬁed theory and new algorithms, IEEE Trans. on Biomedical\nEngineering 58 (2) (2010) 355–362.\n[15] A. Craik, Y. He, J. L. Contreras-Vidal, Deep learning for electroen-\ncephalogram (EEG) classiﬁcation tasks: a review, Journal of Neural En-\ngineering 16 (3) (2019) 031001.\n[16] H. Altaheri, G. Muhammad, M. Alsulaiman, S. U. Amin, G. A. Altuwai-\njri, W. Abdul, M. A. Bencherif, M. Faisal, Deep learning techniques\nfor classiﬁcation of electroencephalogram (EEG) motor imagery (MI)\nsignals: A review, Neural Computing and Applications 35 (20) (2023)\n14681–14722.\n[17] A. Al-Saegh, S. A. Dawwd, J. M. Abdul-Jabbar, Deep learning for motor\nimagery EEG-based classiﬁcation: A review, Biomedical Signal Process-\ning and Control 63 (2021) 102172.\n[18] R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstetter,\nK. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, T. Ball, Deep\nlearning with convolutional neural networks for EEG decoding and visu-\nalization, Human Brain Mapping 38 (11) (2017) 5391–5420.\n[19] V. J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P. Hung,\nB. J. Lance, EEGNet: a compact convolutional neural network for EEG-\nbased brain-computer interfaces, Journal of Neural Engineering 15 (5)\n(2018) 056013.\n[20] R. Mane, E. Chew, K. Chua, K. K. Ang, N. Robinson, A. P. Vinod, S.-W.\nLee, C. Guan, FBCNet: A multi-view convolutional neural network for\nbrain-computer interface, arXiv preprint arXiv:2104.01233 (2021).\n[21] Y. Song, Q. Zheng, B. Liu, X. Gao, EEG Conformer: Convolutional trans-\nformer for EEG decoding and visualization, IEEE Trans. on Neural Sys-\ntems and Rehabilitation Engineering 31 (2023) 710–719.\n[22] Z. J. Koles, M. S. Lazar, S. Z. Zhou, Spatial patterns underlying pop-\nulation differences in the background EEG, Brain Topography 2 (1990)\n275–284.\n[23] J. M¨uller-Gerking, G. Pfurtscheller, H. Flyvbjerg, Designing optimal spa-\ntial ﬁlters for single-trial EEG classiﬁcation in a movement task, Clinical\nNeurophysiology 110 (5) (1999) 787–798.\n[24] M. Tangermann, K.-R. M¨uller, A. Aertsen, N. Birbaumer, C. Braun,\nC. Brunner, R. Leeb, C. Mehring, K. Miller, G. Mueller-Putz, G. Nolte,\nG. Pfurtscheller, H. Preissl, G. Schalk, A. Schl¨ogl, C. Vidaurre,\nS. Waldert, B. Blankertz, Review of the BCI Competition IV, Frontiers\nin Neuroscience 6 (2012) 55.\n[25] J. Faller, C. Vidaurre, T. Solis-Escalante, C. Neuper, R. Scherer, Au-\ntocalibration and recurrent adaptation: Towards a plug and play online\nERD-BCI, IEEE Trans. on Neural Systems and Rehabilitation Engineer-\ning 20 (3) (2012) 313–319.\n[26] K. Xia, L. Deng, W. Duch, D. Wu, Privacy-preserving domain adapta-\ntion for motor imagery-based brain-computer interfaces, IEEE Trans. on\nBiomedical Engineering 69 (11) (2022) 3365–3376.\n[27] V. Jayaram, A. Barachant, MOABB: trustworthy algorithm benchmarking\nfor BCIs, Journal of Neural Engineering 15 (6) (2018) 066011.\n[28] A. Barachant, S. Bonnet, M. Congedo, C. Jutten, Multiclass brain-\ncomputer interface classiﬁcation by Riemannian geometry, IEEE Trans.\non Biomedical Engineering 59 (4) (2012) 920–928.\n[29] Z. Miao, M. Zhao, X. Zhang, D. Ming, LMDA-Net: A lightweight multi-\ndimensional attention network for general EEG-based brain-computer in-\nterfaces and interpretability, NeuroImage (2023) 120209.\n13",
    "pdf_filename": "CSP-Net_Common_Spatial_Pattern_Empowered_Neural_Networks_for_EEG-Based_Motor_Imagery_Classification.pdf"
}