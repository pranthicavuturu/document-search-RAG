{
    "title": "CSP-Net: Common Spatial Pattern Empowered Neural Networks for EEG-Based Motor",
    "abstract": "Electroencephalogram-basedmotorimagery(MI)classificationisanimportantparadigmofnon-invasivebrain-computerinterfaces. Commonspatialpattern(CSP), whichexploitsdifferentenergydistributionsonthe scalp whileperformingdifferentMI tasks, is verypopularinMIclassification. Convolutionalneuralnetworks(CNNs)havealsoachievedgreatsuccess, duetotheirpowerful learning capabilities. This paper proposestwo CSP-empowered neural networks (CSP-Nets), which integrate knowledge-driven CSP filterswithdata-drivenCNNsto enhancetheperformanceinMIclassification. CSP-Net-1directlyaddsaCSP layerbefore aCNNtoimprovetheinputdiscriminability. CSP-Net-2replacesaconvolutionallayerinCNNwithaCSPlayer. TheCSPlayer parametersinbothCSP-NetsareinitializedwithCSPfiltersdesignedfromthetrainingdata.Duringtraining,theycaneitherbekept fixedoroptimizedusinggradientdescent.ExperimentsonfourpublicMIdatasetsdemonstratedthatthetwoCSP-Netsconsistently improvedover their CNN backbones,in both within-subjectand cross-subjectclassifications. They are particularlyusefulwhen the number of training samples is very small. Our work demonstratesthe advantageof integratingknowledge-driventraditional machinelearningwithdata-drivendeeplearninginEEG-basedbrain-computerinterfaces.",
    "body": "CSP-Net: Common Spatial Pattern Empowered Neural Networks for EEG-Based Motor\nImagery Classification\nXueJianga,b,LubinMenga,b,XinruChena,b,YifanXua,b,DongruiWua,b,∗\naKeyLaboratoryoftheMinistryofEducationforImageProcessingandIntelligentControl,SchoolofArtificialIntelligenceandAutomation,HuazhongUniversity\nofScienceandTechnology,Wuhan430074,China\nbShenzhenHuazhongUniversityofScienceandTechnologyResearchInstitute,Shenzhen518063,China\nAbstract\nElectroencephalogram-basedmotorimagery(MI)classificationisanimportantparadigmofnon-invasivebrain-computerinterfaces.\nCommonspatialpattern(CSP), whichexploitsdifferentenergydistributionsonthe scalp whileperformingdifferentMI tasks, is\nverypopularinMIclassification. Convolutionalneuralnetworks(CNNs)havealsoachievedgreatsuccess, duetotheirpowerful\nlearning capabilities. This paper proposestwo CSP-empowered neural networks (CSP-Nets), which integrate knowledge-driven\nCSP filterswithdata-drivenCNNsto enhancetheperformanceinMIclassification. CSP-Net-1directlyaddsaCSP layerbefore\naCNNtoimprovetheinputdiscriminability. CSP-Net-2replacesaconvolutionallayerinCNNwithaCSPlayer. TheCSPlayer\nparametersinbothCSP-NetsareinitializedwithCSPfiltersdesignedfromthetrainingdata.Duringtraining,theycaneitherbekept\nfixedoroptimizedusinggradientdescent.ExperimentsonfourpublicMIdatasetsdemonstratedthatthetwoCSP-Netsconsistently\nimprovedover their CNN backbones,in both within-subjectand cross-subjectclassifications. They are particularlyusefulwhen\nthe number of training samples is very small. Our work demonstratesthe advantageof integratingknowledge-driventraditional\nmachinelearningwithdata-drivendeeplearninginEEG-basedbrain-computerinterfaces.\nKeywords: Brain-computerinterfaces,electroencephalogram,motorimagery,commonspatialpattern,convolutionalneural\nnetwork\n1. Introduction Many algorithms have been proposed for EEG-based MI\nclassification. Common spatial pattern (CSP) [10, 11] is one\nA brain-computerinterface (BCI) establishes a direct com-\nof the mostwidely used andeffectiveapproaches,which con-\nmunication pathway that enables the human brain to interact\nvertstherawmulti-channelEEGsignalsintomorediscrimina-\nwithexternaldevices[1]. Electroencephalogram(EEG),which\ntivespatialpatterns. Itwasinitiallyproposedforbinaryclassi-\nrecordstheelectricalactivitiesonthescalpofthebrain,isthe\nfication,bydesigningspatialfiltersthatmaximizethevariance\nmost widely used inputsignal in non-invasiveBCIs due to its\nratioofthefilteredsignalsofdifferentclasses [10]. Dornhege\naffordabilityandconvenience[2]. EEG-basedBCIshavebeen\netal. [12]extendedittomulti-classclassificationusingaone-\nusedincontrollingrobots[3], decodingspeech[4],enhancing\nversus-the-rest strategy. Ang et al. [13] proposed filter bank\ncomputergamingexperience[5],andsoon.\nCSP (FBCSP), whichbandpassfiltersEEGsignalsintomulti-\nMotor imagery (MI) [6] is a classical paradigm of EEG-\nplefrequencybands,extractsCSPfeaturesfromeachband,and\nbasedBCIs,whereasubjectimaginesthemovementofabody\nthenselectsthemostusefulfeaturesforclassification. Lotteet\npart, e.g., righthand, left hand, rightfoot, left foot, both feet,\nal. [14]introducedregularizedCSPtoenhancetherobustness\nand/or tongue, without actually executing it. An MI induces\nofCSP.\nchangesinthesensory-motorrhythms(SMR)ofcorresponding\nareas of the cerebral cortex, which primarily involve modula- Recent years have witnessed significant increase in using\ntionsoftheµrhythm(8-12Hz)andtheβrhythm(14-30Hz)[7]. deep learning for EEG signal decoding [15], which integrates\nSpecifically, when an MI starts, these rhythmic activities de- feature extraction and classification into a single end-to-end\ncrease, resulting in event-related desynchronization(ERD); at network.Amongvariousdeeparchitectures,convolutionalneu-\nthe end of an MI, these rhythmic activities increase, resulting ral networks(CNNs) are the most prevalentfor MI classifica-\nin event-related synchronization (ERS) [8, 9]. Therefore, the tion[16,17]. Forexample,Schirrmeisteretal. [18] proposed\ndetectionofSMRpatternswithinspecificareasofthecerebral ShallowCNNandDeepCNNforrawEEGclassification. Shal-\ncortex can be used to identify which body part the subject is lowCNNisinspiredbyFBCSPandincludescomponentssuch\nimaginingmoving. astemporalconvolution,spatialconvolution,log-variancecal-\nculationandaclassifier,eachcorrespondingtoaspecificstepin\nFBCSP. DeepCNN is similar but includes more convolutional\n∗Emails: xuejiang@hust.edu.cn(XueJiang),lubinmeng@hust.edu.cn(Lu-\nand pooling layers. Lawhern et al. [19] introduced a com-\nbinMeng),xrchen@hust.edu.cn(XinruChen),yfxu@hust.edu.cn(YifanXu),\ndrwu@hust.edu.cn(DongruiWu).DongruiWuisthecorrespondingauthor. pactEEGNet,whichhasdemonstratedpromisingperformance\nPreprintsubmittedtoElsevier November20,2024\n4202\nvoN\n4\n]PS.ssee[\n1v97811.1142:viXra\nacrossvariousBCItasks,includingMIclassification. Inspired 2. Methods\nalso by FBCSP, EEGNet uses a two-step sequence of tempo-\nral convolutionfollowed by depthwise convolution. Recently, ThissectionintroducestheCSPalgorithm,fiveCNNmodels\nFBCNet [20] extends the FBCSP approach by utilizing a hi- for MI classification, and our proposedtwo CSP-Nets to inte-\nerarchicalarchitecturethatenhancesfeatureextractionthrough grateCSPandCNNs.\nmulti-dimensionalfiltering,allowingittocapturericherspatial\nandtemporalpatternsinEEGdata.EEGConformer[21]adopts\n2.1. CSP\natransformer-likearchitecture,combiningself-attentionmech-\nanisms with convolutionallayers, which enables the model to\nCSP was first proposed by Koles et al. [22] to extract dis-\nlearnlong-rangedependenciesinEEGsignalseffectively.\ncriminative features from EEG signals of two human popula-\nThough these data-driven deep models have achieved\ntions. Mu¨eller-Gerkingetal. [23]laterextendedittoMIclas-\npromising performance in MI classification, they usually re-\nsification. Since then, it has become one of the most popular\nquire a large amount of labeled training data, which may not\nandeffectivealgorithmsinMI-basedBCIs[10,11].\nbealwaysavailableinpractice. Thishighlightstheneedtoin-\nFig.1showst-SNEvisualizationofsomerealEEGtrialsbe-\ncorporatepriorknowledgeintoEEGnetworks,asitcanhelpre-\nforeandafterCSPfilteringfromSubject3ofDataset2ainBCI\nducetherelianceonextensivelabeleddatasets. Byintegrating\nCompetitionIV[24]. Clearly,afterCSPfiltering,samplesfrom\nprior knowledge, models can leverage existing insights about\ndifferentclassesbecomemoredistinguishable.\nEEG signal characteristics, enhancing their generalization ca-\npabilities and performance even in data-scarce environments.\nThis paper proposes CSP empowered neural networks (CSP-\nNet), which more effectively integrate CSP and CNNs. More\nspecifically, we propose two CSP-Nets, by embedding CSP\nintodifferentlayersoftheCNNmodels. Thefirst,CSP-Net-1,\nplacesaCSP layerbeforeaCNN tofilter theEEGsignalsfor\nenhancing their discriminability. The second, CSP-Net-2, re-\nplacesaCNN’sconvolutionallayerwithaCSPlayertoprovide\ntask-specificpriorknowledgeinitialization. Theparametersin\ntheCSPlayerofbothCSP-NetsareinitializedfromCSPfilters (a) (b)\ndesignedonthe trainingdata. Theycan befixedor optimized\nbygradientdescentduringtraining. Insummary,CSP-Netsin- Figure 1: t-SNE visualization of (a) the raw EEG trials; and, (b) the CSP-\ntegratethestrengthsoftraditionalCSP featureextractionwith filteredtrials.Differentshapes(colors)representdifferentclasses.\ndeeplearningbyembeddingCSPlayersinCNNarchitectures.\nForbinaryclassification,CSPaimstolearnspatialfiltersthat\nThisapproachenhancesthemodel’sabilitytocapturerelevant\nmaximize the variance of EEG signals from one class while\nfeaturesfromEEGsignals,makingitamoreeffectivesolution\nsimultaneously minimizing the variance from the other class.\nforMIclassification. Ourmaincontributionsare:\nLet X ∈ Rc×t be anEEGtrialofMItask i, wherei ∈ {1,2}is\ni\n• IntegrationofCSPandCNNs: Weproposeanovelframe-\ntheclassindex,cthenumberofchannels,andtthenumberof\nworkthatcombinesCSPwithCNNsforMIclassification,\ntime domainsamples. CSP generatesa spatialfilteringmatrix\nenhancingEEGfeatureextractionandimprovingclassifi- W ∈ Rc×f (f < c) that projects the original EEG trials into\ncationperformance.\na lower-dimensional space with higher discriminability. W is\nobtainedbymaximizing(orminimizing):\n• Two CSP-Net Variants: We introduce two architectures,\nCSP-Net-1, which incorporates a CSP layer before the\nCNN, and CSP-Net-2, which replaces a convolutional J(W)=\nW⊤X¯ 1X¯ 1⊤W\n=\nW⊤C¯ 1W\n, (1)\nlayer with a CSP layer for task-specific prior knowledge W⊤X¯ 2X¯ 2⊤W W⊤C¯ 2W\ninitialization.BothvariantsallowCSPlayerparametersto\nbefixedorfurtheroptimized. where X¯ i ∈ Rc×t is the averaged EEG trial from class i, and\nC¯ ∈ Rc×c themeanspatialcovariancematrixofallEEGtrials\ni\n• Performance on Multiple EEG Datasets: CSP-Nets\ninclassi.\ndemonstratestrongperformanceacrossvariousscenarios, Since J(W) = J(kW)foranyarbitraryrealconstantk, max-\nincludingwithin-subjectandcross-subjectclassifications,\nimizing J(W)isequivalenttomaximizingW⊤C¯ W, subjectto\n1\nas well as in small sample settings. The models demon- the constraint W⊤C¯ W = I . This optimization problem can\n2 f\nstrate generalization across different backbone architec-\nbe solved using the Lagrange multiplier method [14], whose\ntures,validatedonfourpublicMIdatasets.\nLagrangefunctionis\nTherestofthispaperisstructuredasfollows.Section2intro-\nducestheclassicalCSPandproposestwoCSP-Nets. Section3 F(W,λ)=W⊤C¯ W−λ(W⊤C¯ W−I ). (2)\n1 2 f\npresentstheexperimentalsettingsandexperimentalresults. Fi-\nnally,Section4drawsconclusions. Setting thederivativeof F(W,λ)with respectto W to 0, we\n2\nhave\n∂F(W,λ) Table1:EEGNet[19].\n∂W\n=2W⊤C¯ 1−2λW⊤C¯\n2\n=0\nBlock Layer Filtersize Nu fim ltb ee rsrof\n⇔C¯ 1W =λC¯ 2W Temporal Conv2D (1, fs) 4\n2\n⇔C¯−1C¯ W =λW, convolution Batchnormalization - -\n2 1 DepthwiseConv2D (c,1) 8\nDepthwise Batchnormalization - -\nwhichbecomesastandardeigenvaluedecompositionproblem. spatialfilter ELUactivation - -\nAveragepooling (1,4) -\nThe spatial filtering matrix W consists of eigenvectors cor- Dropout - -\nresponding to the f largest and the f smallest eigenvaluesof SeparableConv2D (1,16) 8\nC¯ 2−1C¯ 1. 2 2 Separable B Pat oc ih ntn wo ir sm eCal oiz na 2t Dion (1,- 1) 8-\nconvolution Batchnormalization - -\nELUactivation - -\nAveragepooling (1,8) -\n2.2. CNNsforMIClassification Dropout - -\nClassifier Fullyconnection - -\nFivepopularCNNmodelsareconsideredinthispaper:EEG-\nNet[19],DeepCNN[18],ShallowCNN[18],FBCNet[20],and\nEEGConformer [21]. Their architectures are detailed in Ta-\nbles1-5,respectively.\nTable2:DeepCNN[18].\nNumberof\n• EEGNet[19],whichconsistsofthreeconvolutionalblocks Block Layer Filtersize filters\nand a classifier block. The first convolutionalblock per- Temporal\nConv2D (1,5) 25\nconvolution\nformstemporalfilteringforcapturingfrequencyinforma-\nConv2D (c,1) 25\ntion. The second spatial filter block uses depthwise con- Batchnormalization - -\nSpatialfilter\nvolution with size (c,1) to learn spatial filters. The third ELUactivation - -\nMaxpooling (1,2) -\nseparable convolutionalblockis used to reducethe num- Dropout - -\nber of parameters and decouple the relationships within Conv2D (1,5) 50\nStandard Batchnormalization - -\nandacrossfeaturemaps. convolution ELUactivation - -\nMaxpooling (1,2) -\nDropout - -\n• DeepCNN[18], comparedwithEEGNet, itisdeeperand\nConv2D (1,5) 100\nhence has much more parameters. It mainly includes a Standard Batchnormalization - -\nconvolution ELUactivation - -\ntemporal convolutional block, a spatial filter block, two Maxpooling (1,2) -\nDropout - -\nstandardconvolutionalblocksanda classifier block. The\nClassifier Fullyconnection - -\nfirst temporal and spatial convolutional blocks are spe-\ncially designed to handle EEG inputs and the other two\narestandardones.\nTable3:ShallowCNN[18].\n• ShallowCNN [18], which is a shallow version of Deep-\nNumberof\nCNN, inspiredby FBCSP. Its first two blocksare similar Block Layer Filtersize filters\ntothetemporalandspatialconvolutionalblocksofDeep- Temporal\nConv2D (1,13) 40\nCNN,butwithalargerkernel,adifferentactivationfunc- convolution\ntion,andadifferentpoolingapproach. Conv2D (c,1) 40\nBatchNormalization - -\nSpatialfilter SquaringActivation - -\nAveragePooling (1,35) -\n• FBCNet[20], whichisasimpleyeteffectiveCNNarchi- LogarithmicActivation - -\nDropout - -\ntecture. It begins by applying multiple fixed-parameter\nClassifier FullyConnection - -\nband-passfilters to decomposethe EEG into variousfre-\nquencybandsasmulti-viewinputs. Spatialfilter blockis\nthenusedtoextractspatiallydiscriminativepatternsfrom\neachfrequencyband.Finally,aclassifierblockisdesigned\nforclassification. Table4:FBCNet[20].\nNumberof\nBlock Layer Filtersize filters\n• EEGConformer [21], which is a compact convolutional\nBand-pass\ntransformermodel.Theconvolutionmodulealsoincludes filter - - 6\na temporal convolutional block and a spatial filter block DepthwiseConv2D (c,1) 48\nSpatial Batchnormalization - -\nforlearningthelow-levellocalfeatures.Themultipleself- filter Swishactivation - -\nVariancelayer - -\nattentionmodulesareusedtoextracttheglobalcorrelation\nClassifier Fullyconnection - -\nwithinthelocalfeatures.\n3\nFigure2: OurproposedCSP-Nets. (a)TraditionalCSPfiltersareusedtoinitializetheCSPlayerinCSP-Nets. (b)CSP-Net-1,whichdirectlyaddsaCSPlayer\nbeforeaCNNbackbone. (c)CSP-Net-2,illustratedusingEEGNet[19](Table1inSupplementaryMaterials);theDepthwiseConv2Dlayerinitsdepthwisespatial\nfilterblockisreplacedbyaCSPlayer.\nCSP-Net-1appliesCSPfilteringasapre-processingstep,en-\nTable5:EEGConformer[21].\nabling the model to work with more discriminative input sig-\nNumberof\nBlock Layer Filtersize filters nals. Thisexplicitinclusionof the CSP filter providesa more\nTemporal structured way to embed expert knowledge into the network,\nconvolution Conv2D (1,25) 40\ntherebyimprovingthemodel’scapacitytocapturetask-relevant\nConv2D (c,1) 40\nspatialfeatures.\nBatchnormalization - -\nELUactivation - - Algorithm1givesthepseudo-codeofCSP-Net-1.\nSpatialfilter Averagepooling (1,75) -\nDropout - -\nPointwiseConv2D (1,1) 40\nRearrange - - Algorithm1:CSP-Net-1forMIclassification.\nLayernormalization - -\nInput: TrainingdataX;aCNNmodel.\nMHA - -\nDropout - -\n6× Residualadd - - 1 PerformCSPonXtoobtainthefiltermatrixW;\nSelf-attention Layernormalization - -\nFFN - - 2 InitializeCSP-Net-1,whichconsistsofaCSPlayerwith\nDropout - - weightsW andarandomlyinitializedCNN;\nResidualadd - -\n3 TrainCSP-Net-1onX;\n4 returnCSP-Net-1.\n2.3. CSP-Net-1\nOur proposed CSP-Net-1 simply performs CSP before a\nCNN.\n2.4. CSP-Net-2\nAsillustratedinFig.2(a),alltrainingEEGsamplesareused\nin CSP, resulting in f spatial filters W ∈ Rc×1, i = 1,..., f. ManyCNNmodelshavebeenproposedforMIclassification,\ni\nThen, as shown in Fig. 2(b), CSP-Net-1 uses these filters to whichtypicallyconsistofmultipleconvolution-poolinglayers\nspatially filter the raw EEG signals, before passing them to a forfeatureextractionandsomefullyconnectedlayersforclas-\nCNNbackbone. sification. Although they differ in architecture, they usually\nTherecouldbetwodifferenttrainingapproaches: 1)Fixthe include a spatial filter layer with spatial convolutionalkernels\nCSP layer andtrain the CNN backboneonly (CSP-Net-1-fix); specificallydesignedforEEGsignals.\nand, 2) updatethe CSP layer andthe CNN backbonesimulta- CSP-Net-2replacestheirspatialfilterlayerwithaCSPlayer.\nneously(CSP-Net-1-upd).Theireffectivenesswillbediscussed Fig. 2(c) uses EEGNet as the CNN backbone to illustrate the\ninSection3.3. architectureofCSP-Net-2. Forclarity,weprimarilydepictthe\n4\nAlgorithm2:CSP-Net-2forMIclassification. They were downloaded and pre-processed using the MOABB\nframework [27]. All datasets were pre-processed with an 8-\nInput: TrainingdataX;aCNNmodel.\n32Hzbandpassfilter.\n1 PerformCSPonXtoobtainthefiltermatrixW;\n2 RandomlyinitializetheCNNmodel;\n3 InitializeCSP-Net-2,byreplacingtheconvolutional Table6:SummaryofthefourMIdatasets.\nlayerinthespatialfilterblockoftheCNNmodelbya Datasets #Subjects #Channels #Trialspersubject #Classes\nCSPlayerwithweightsW;\nMI4C 9 22 288 4\n4 TrainCSP-Net-2onX; MI2C 9 22 144 2\n5 returnCSP-Net-2. MI14S 14 15 100 2\nMI9S 9 13 200 2\nconnectionoftheconvolutionalkernelbetweeninputsandout-\n3.2. ImplementationDetails\nputs. The depthwise spatial filter block aims to learn spatial\npatterns in EEG data. CSP-Net-2 replaces the convolutional We evaluated the performanceof CSP-Nets in both within-\nkernelsinthisblockwiththeCSPfilters,andkeepsotherparts subjectandcross-subjectclassifications:\nunchanged.\n1. Within-subjectclassification: Foreachindividualsubject,\nMore specifically, CSP-Net-2 uses the CSP layer to replace\n80%trialswereusedfortraining,andtheremaining20%\ntheDepthwiseConv2DlayerinthespatialfilterblockofEEG-\nfortesting.\nNet (8 kernels), the Conv2D layer in the spatial filter block\n2. Cross-subjectclassification: Leave-one-subject-outcross-\nofDeepCNN (25kernels),the Conv2Dlayerin thespatialfil-\nvalidationwasperformed,i.e.,onesubjectwasusedasthe\nter blockofShallowCNN(40kernels),theDepthwiseConv2D\ntestsetandallremainingonesasthetrainingset.\nlayerinthespatialfilterblockofFBCNet(48kernels),andthe\nConv2DlayerinthespatialfilterblockofEEGConformer(40 All experimentswere repeated5 times, and the averageac-\nkernels). curaciesarereported.\nSimilartoCSP-Net-1,theCSPfilterlayerinCSP-Net-2can We used Adam optimizer with batch size 128 and initial\neither be fixed (CSP-Net-2-fix) or updated (CSP-Net-2-upd). learning rate 0.01, and cross-entropy loss with weight decay\nFurthermore, this replacementis significant as it allows CSP- 0.0005. The maximum number of training epochs was 200.\nNet-2toexplicitlyincorporatepriorknowledgeaboutspatialfil- TheCSPlayerusedbydefault f =8spatialfilters(Section3.6\ntering, enhancingthe model’sability to capturediscriminative presents sensitivity analysis). For CSP-Net-2, the number of\nfeatures from the EEG signals. The flexibility of using either convolutional kernels in the original spatial filter layer of the\nfixed or updated CSP filters also provides a balance between CNNmodelsmaybelargerthan8.WeexpandedtheCSPfilters\nstability and adaptability during training, which we discussed to address this mismatch: when the number of convolutional\nindetailinSection3.3. kernelsexceedsthenumberofCSPfilters,wereplicatetheCSP\nAlgorithm2givesthepseudo-codeofCSP-Net-2. filters to match the number of required kernels. Specifically,\nweduplicatedthe8CSPfilters5timestomatchthe40kernels\nin the spatial filter blockof ShallowCNN andEEGConformer\n3. ExperimentsandResults\n(8×5 = 40),duplicatedthe8CSPfilters6timestomatchthe\n48 kernelsin the spatial filter block of FBCNet (8×6 = 48),\nThissectionpresentstheexperimentalresultstovalidatethe\nandduplicatedthe8CSPfilters3timesandrandomlyselected\neffectivenessofourproposedCSP-Nets.\none more to match the 25 kernels in the spatial filter block of\nDeepCNN(8×3+1=25).\n3.1. Datasets\nFourpublicMIdatasetsfromBNCI-Horizon1,summarized\n3.3. ExperimentalResults\ninTable6,wereusedinourexperiments:\nTable 7 shows the classification accuracies for the individ-\n1. MI4C and MI2C: They were from the 001-2014dataset. ualsubjectsonMI4C,whereCSP-LRused logisticregression\nTheEEGsignalsweresampledat250Hz. MI2Cincludes as the classifier. Tables 8-10 show the average classification\nonly left-hand and right-hand trials. MI4C includes all results across all subjects on the other three datasets, due to\nclasses. page limit. We performedpaired t-tests on the results, calcu-\n2. MI14S: This was from the 002-2014 dataset. The EEG lated p-valuesbetweenthe standardbackbonemodelsand the\nsignalsweresampledat512Hz. CSP-Nets,andadjustedthemusingBenjaminiHochbergFalse\n3. MI9S: This was the 001-2015dataset. The EEG signals DiscoveryRatecorrection.Observethat:\nwererecordedat512Hz. Thelastthreesubjectsweredis-\n1. Both CSP-Nets were generally highly effective on all\ncardedduetotheirpoorperformance[25,26].\ndatasets and backbones. Embedding CSP knowledge in\nCNN backbones resulted in significant performance im-\n1http://www.bnci-horizon-2020.eu/database/data-sets provements. Forexample,inwithin-subjectclassification\n5\non MI4C, CSP-Net-2-fix increased the average accuracy 1. Consistent with previousfindings, CSP-Net-fix generally\non all subjects from 63.50% to 71.91% after integrating outperformed CSP-Net-upd, with CSP-Net-2-fix particu-\nCSP information into EEGNet as CSP-Net-2-fix. The larlycompetitiveonEEGNetandDeepCNN.Forexample,\naverage accuracies across all five backbones also exhib- inwithin-subjectclassification, CSP-Net-2-fixachieveda\nitedsignificantimprovements,fromaninitialaccuracyof remarkableaccuracyimprovementofmorethan20%over\n61.32%toashighas67.33%. the DeepCNN backbone,when trainedwith only 50%of\n2. CSP-Nets with fixed CSP layer parameters generally thetrainingdataonMI4C.\nperformed better. Particularly, CSP-Net-2-fix achieved 2. Overall,theperformanceimprovementsofCSP-Netswere\nsubstantial improvements over EEGNet, DeepCNN, and moreobviouswhenthetrainingdatasizewassmall. This\nEEGConformer. This validated that the incorporationof maybebecausetheembeddingofpriorknowledgegreatly\nCSP prior knowledge can enhance the generalization of reduces the overfitting issue of CNN backbonesin small\nCNNmodels. Highnumberandproportionofparameters samplescenarios.\nofspatialconvolutionalkernelsinShallowCNNandFBC-\nNetmayovershadowthebenefitsofferedbyCSPfilters. 3.6. InfluenceoftheNumberofCSPFilters\n3. CSP-Nets had larger performance improvements in We furtherinvestigatedthe influenceof the numberofCSP\nwithin-subject classification than cross-subject classifica- filters (f) on the performance of CSP-Nets with three back-\ntion. Thismightbebecause: 1)within-subjectclassifica- bonesonMI4C.Thedatasetincludes22-channelEEGsignals,\ntion had much fewer training samples than cross-subject soweconsidered f ∈ {4,8,12,16,22}. Fig.7showsthecorre-\nclassification, andhencepriorknowledgeinCSPismore spondingaccuraciesofthetwoCSP-Nets(fixedCSPlayer).As\nhelpful to the generalization performance; and, 2) cross- thenumberoffiltersincreased,theaccuracyfirstincreasedand\nsubject classification is intrinsicallymore challenging,as then decreases, which is intuitive. Generally, f = 8 seems to\ntherearelargeindividualdifferencesamongdifferentsub- beagoodchoicetobalancetheperformanceandcomputational\njects. TheimpactoftrainingdataquantityonCSP-Netsis cost.\ndiscussedinSection3.5.\n4. CSP-Nets achieved better performanceon most subjects. 3.7. AblationStudies\nHowever, for some subjects where CSP did not perform An ablation study was performed to verify that the perfor-\nwell, CSP-Nets alsostruggled,e.g.,Subject1, 2, 5and6 mance improvementof CSP-Net-1 was notdue to an increase\nincross-subjectclassificationonMI4C. inthenumberofnetworkparameters.\nSpecifically, we trained CSP-Net-1-rad, which replaced the\nCSPlayerofCSP-Net-1witharandomlyinitializedlayerofthe\n3.4. ComparativePerformanceAnalysis samesize. TheresultsonthefourMIdatasetsinwithin-subject\nclassification are shown in Table 12. Generally, CSP-Net-1-\nWe further compared our approaches with nine other ap-\nrad performed similarly to the standard backbone, suggesting\nproaches, including the state-of-the-art traditional approaches\nthattheperformanceimprovementofCSP-Net-1wasduetoits\nand deep learning approaches: CSP [10], FBCSP [13],\nincorporationofknowledgefromCSP,insteadofmoreparam-\nMDRM [28], DeepCNN [18], LMDA-Net [29], Shal-\neters.\nlowCNN [18], EEGConformer [21], EEGNet [19], and FBC-\nNet[20].Table11presentstheclassificationaccuraciesofCSP-\n3.8. TrainingProcessVisualization\nNet-1andCSP-Net-2comparedtothesebaselines.InCSP-Net-\n1andCSP-Net-2,theEEGNetwasusedasthebackbonearchi- Fig. 8 shows the average cross-subjecttraining and test ac-\ntecture,andthefixedCSPlayerwasapplied.Thesametraining curacy curves of CSP-Nets (fixed CSP layer) and their corre-\nandtestdatawereusedforallmodels. spondingbackbones(EEGNet)onthefourdatasets. Forallthe\nbackbones,therewasalargegapbetweenthetrainingandtest\nBoth CSP-Net-1andCSP-Net-2demonstratedsuperiorper-\ncurves, indicating overfitting. Our proposed CSP-Nets effec-\nformance compared to traditional approaches like CSP and\ntively leveragedthe knowledgefromthe CSP filters for better\nFBCSP,aswellasmorerecentmodelslikeFBCNetandEEG-\ninitialization,reducingthegapandachievingbettertestperfor-\nConformer. TheseresultshighlighttheireffectivenessforEEG\nmance.\nsignalclassificationtasks.\n3.9. VisualizationoftheCSPFilters\n3.5. SmallSampleSetting\nWe further visualized the spatial convolutional kernel\nDeep CNN models may easily overfit when the training weights from the CSP-Net-2 and the counterparts from stan-\ndataset is small. Figs. 3-6 show the accuracy improvements dard backbone. In Fig. 9, we present the eight spatial filters\ncomparedwiththebackboneatdifferenttrainingdataratios(the intheEEGNetmodelandtheCSPfiltersintheCSP-Net-2-fix\nnumberoftrainingsamplesusedtotrainthemodeldividedby model for within-subject classification on Subject 1 of MI2C\nthetotalnumbertrainingsamples)onthefourdatasets,respec- (binaryclassificationonthelefthandandrighthand). We can\ntively. observethattheCSPfiltersinCSP-Net-2-fixexhibitedamore\nObservethat: focused and obvious left-right distribution concentrated on a\n6\nTable7:Classificationaccuracies(%)onMI4C.AverageaccuracieshigherthanStandardaremarkedinbold.Asterisksindicatestatisticallysignificantdifferences\nbetweenstandardbackboneandCSP-Netunderadjustedpairedt-test,where*meansp<0.05,**meansp<0.01,***meansp<0.001.\nScenario Backbone Approach S1 S2 S3 S4 S5 S6 S7 S8 S9 Averageacc±std\n- CSP-LR 71.85 61.27 78.64 48.52 35.32 40.14 69.07 75.00 70.37 61.13\nStandard 72.59 49.35 81.36 44.51 49.31 39.29 65.98 84.19 84.93 63.50\nCSP-Net-1-upd 83.40 57.31 88.26 50.36 58.36 43.90 75.52 84.01 87.59 69.86***±1.54\nCSP-Net-1-fix 81.43 58.73 90.70 53.48 53.73 45.74 82.42 83.95 86.88 70.79***±1.68\nEEGNet\nCSP-Net-2-upd 77.83 56.85 86.91 47.95 46.24 41.35 74.20 80.74 84.11 66.24±2.89\nCSP-Net-2-fix 81.18 63.55 92.36 52.69 56.75 46.27 83.02 82.64 88.76 71.91***±0.74\nStandard 56.86 45.93 66.55 39.72 25.48 30.14 57.34 68.66 72.93 51.51±1.13\nCSP-Net-1-upd 69.07 57.07 80.32 50.76 38.34 37.82 68.72 73.76 78.98 61.65***±1.70\nDeepCNN CSP-Net-1-fix 70.12 56.24 81.90 52.06 46.05 38.28 69.04 72.94 82.85 63.28***±1.26\nCSP-Net-2-upd 62.52 41.36 59.02 43.28 25.64 25.00 54.47 70.01 71.74 50.34±0.95\nCSP-Net-2-fix 73.07 55.34 82.59 51.55 45.90 38.18 74.78 76.61 81.52 64.39***±2.46\nStandard 69.35 57.51 74.62 53.04 50.74 46.14 76.48 76.33 81.95 65.13±1.38\nCSP-Net-1-upd 79.99 57.93 84.78 57.87 54.98 44.48 85.43 80.79 80.92 69.69***±1.58\nCSP-Net-1-fix 80.15 59.93 86.51 58.04 52.83 44.48 85.14 83.04 83.64 70.42***±1.41\nShallowCNN\nCSP-Net-2-upd 70.21 57.53 79.94 50.79 37.88 40.42 76.08 83.20 80.67 64.08±1.62\nWithin- CSP-Net-2-fix 69.04 63.02 86.55 45.62 31.48 38.89 77.06 79.06 82.27 63.66±2.01\nSubject Standard 69.27 53.87 83.77 50.17 53.03 43.78 69.61 80.22 83.95 65.30±1.76\nCSP-Net-1-upd 77.39 53.70 86.65 52.61 60.79 45.62 78.23 86.81 84.71 69.61***±1.23\nCSP-Net-1-fix 76.41 56.87 87.44 49.11 53.56 42.79 77.72 86.07 86.01 68.44**±2.04\nFBCNet\nCSP-Net-2-upd 71.19 54.88 87.29 53.58 54.56 43.96 72.34 83.05 86.46 67.48*±1.85\nCSP-Net-2-fix 76.74 57.42 86.45 46.79 53.81 43.64 71.47 82.65 85.98 67.22±1.02\nStandard 75.00 46.97 79.81 50.23 37.46 46.05 72.25 77.19 65.42 61.15±1.67\nCSP-Net-1-upd 81.98 63.81 87.68 58.57 60.62 52.46 88.04 82.96 71.73 71.98***±1.38\nEEGConformer CSP-Net-1-fix 84.28 59.84 87.41 61.85 58.49 53.90 90.18 79.96 71.82 71.97***±1.14\nCSP-Net-2-upd 80.49 58.97 89.79 60.50 56.39 55.85 84.24 85.14 75.04 71.82***±1.04\nCSP-Net-2-fix 82.55 60.73 86.94 45.70 58.18 46.37 78.67 83.13 82.95 69.47***±1.04\nStandard 68.61 50.73 77.22 47.53 43.20 41.08 68.33 77.32 77.84 61.32±1.57\nCSP-Net-1-upd 78.37 57.96 85.54 54.03 54.62 44.86 79.19 81.67 80.79 68.56***±1.49\nAverage CSP-Net-1-fix 78.48 58.32 86.79 54.91 52.93 45.04 80.90 81.19 82.24 68.98***±1.51\nCSP-Net-2-upd 72.45 53.92 80.59 51.22 44.14 41.32 72.27 80.43 79.60 63.99***±1.67\nCSP-Net-2-fix 76.52 60.01 86.98 48.47 49.22 42.67 77.00 80.82 84.30 67.33***±1.45\n- CSP-LR 61.46 22.92 72.22 43.06 32.29 39.58 62.50 76.04 62.85 52.55\nStandard 68.96 30.35 71.88 38.06 37.01 36.74 42.01 58.19 61.74 49.44±1.75\nCSP-Net-1-upd 65.62 32.15 72.92 41.39 34.93 38.68 55.28 65.69 61.74 52.04*±1.48\nEEGNet CSP-Net-1-fix 62.50 32.43 76.32 40.97 34.58 37.43 52.01 67.64 65.62 52.17*±1.13\nCSP-Net-2-upd 66.94 32.64 74.17 42.01 35.07 40.97 42.78 67.08 63.82 51.72*±1.59\nCSP-Net-2-fix 66.46 31.53 74.44 39.65 31.04 37.29 53.40 63.06 68.61 51.72±1.38\nStandard 65.35 34.03 52.99 37.92 38.19 43.40 41.81 59.24 53.47 47.38±2.10\nCSP-Net-1-upd 61.32 33.40 67.57 41.81 35.56 40.90 43.19 63.19 57.01 49.33±0.70\nCSP-Net-1-fix 60.07 34.79 62.22 42.50 35.21 40.97 41.67 67.01 60.14 49.40±0.53\nDeepCNN\nCSP-Net-2-upd 64.93 31.18 65.00 38.96 39.58 41.67 48.61 56.94 57.15 49.34±1.11\nCSP-Net-2-fix 62.01 29.93 69.44 37.99 36.32 39.44 52.92 63.33 61.67 50.34*±1.81\nStandard 68.12 33.40 71.04 41.46 36.18 45.49 43.82 69.51 64.17 52.58±0.65\nCSP-Net-1-upd 66.25 30.63 70.69 41.53 32.15 40.49 50.00 64.65 59.03 50.60±1.23\nCSP-Net-1-fix 68.12 31.04 72.01 42.71 32.78 39.24 53.96 69.38 61.60 52.31±1.28\nShallowCNN\nCSP-Net-2-upd 66.39 30.63 72.57 43.12 32.29 43.06 47.29 67.57 63.61 51.84±1.25\nCross- CSP-Net-2-fix 62.15 28.47 72.08 39.58 32.36 41.67 54.93 67.64 67.64 51.84±1.04\nSubject Standard 62.92 31.39 62.22 41.67 31.25 36.25 40.76 55.21 57.01 46.52±1.49\nCSP-Net-1-upd 66.18 30.90 62.50 42.43 31.53 38.33 43.26 57.43 60.49 48.12±1.31\nFBCNet CSP-Net-1-fix 67.22 31.39 67.43 39.51 31.11 38.06 45.90 60.76 60.28 49.07**±1.11\nCSP-Net-2-upd 64.65 30.42 61.94 41.39 30.83 37.92 40.69 56.88 57.64 46.93±1.27\nCSP-Net-2-fix 57.50 32.50 64.38 40.35 36.67 36.88 49.79 64.31 65.00 49.71**±1.24\nStandard 52.64 27.64 49.51 33.68 32.29 39.86 30.14 54.51 41.60 40.21±1.67\nCSP-Net-1-upd 61.94 35.07 63.06 34.38 32.36 37.85 26.11 61.39 55.76 45.32***±0.90\nCSP-Net-1-fix 58.54 35.83 63.33 35.35 32.64 38.19 27.78 64.03 56.18 45.76***±1.09\nEEGConformer\nCSP-Net-2-upd 55.76 33.06 71.67 38.19 33.19 35.14 44.03 61.39 63.75 48.46***±0.62\nCSP-Net-2-fix 56.81 33.26 72.92 39.65 31.11 37.57 44.44 67.22 65.83 49.87***±0.94\nStandard 63.60 31.36 61.53 38.56 34.98 40.35 39.71 59.33 55.60 47.22±1.53\nCSP-Net-1-upd 64.26 32.43 67.35 40.31 33.31 39.25 43.57 62.47 58.81 49.08***±1.12\nAverage CSP-Net-1-fix 63.29 33.10 68.26 40.21 33.26 38.78 44.26 65.76 60.76 49.74***±1.03\nCSP-Net-2-upd 63.73 31.59 69.07 40.73 34.19 39.75 44.68 61.97 61.19 49.66***±1.17\nCSP-Net-2-fix 60.99 31.14 70.65 39.44 33.50 38.57 51.10 65.11 65.75 50.69***±1.28\n7\nTable8: Averageclassification accuracies (%)onMI2C.ThosehigherthanStandardaremarkedinbold. Asterisksindicate statistically significantdifferences\nbetweenstandardbackboneandCSP-Netunderadjustedpairedt-test,where*meansp<0.05,**meansp<0.01,***meansp<0.001.\nBackbone\nScenario Approach\nEEGNet DeepCNN ShallowCNN FBCNet EEGConformer Averageacc±std\nCSP-LR - - - - - 75.72\nStandard 76.38±2.21 61.46±3.28 76.29±2.92 78.11±2.64 75.31±1.45 73.51±2.50\nWithin- CSP-Net-1-upd 80.02±2.80 70.86***±3.58 82.50**±2.60 80.70*±1.85 81.06***±1.46 79.02***±2.45\nsubject CSP-Net-1-fix 81.69*±0.49 70.37***±3.41 83.71***±1.45 82.39**±2.70 82.05***±0.88 80.04***±1.78\nCSP-Net-2-upd 75.94±2.24 61.59±1.93 77.33±1.68 79.65±2.48 81.53***±2.80 75.21**±2.23\nCSP-Net-2-fix 79.66*±2.38 75.86***±1.11 75.34±1.81 79.54±0.58 81.18**±1.61 78.32***±1.50\nCSP-LR - - - - - 72.92\nStandard 71.50±0.87 73.15±1.37 74.32±1.50 69.34±1.50 65.25±1.03 70.71±1.16\nCross- CSP-Net-1-upd 73.53*±1.39 74.86*±0.73 74.65±0.63 71.00±1.60 70.94***±0.67 73.00***±1.00\nsubject CSP-Net-1-fix 74.51**±1.28 74.75±1.36 75.51±0.65 73.09***±1.35 71.20***±0.57 73.81***±1.04\nCSP-Net-2-upd 72.31±1.23 72.93±0.48 70.40±0.87 69.41±1.71 70.76***±1.37 71.16±1.13\nCSP-Net-2-fix 75.25***±1.28 73.38±0.91 76.11±0.80 73.61***±0.27 72.95***±0.55 74.26***±0.76\nTable9: Averageclassificationaccuracies (%)onMI14S.ThosehigherthanStandardaremarkedinbold. Asterisksindicatestatistically significantdifferences\nbetweenstandardbackboneandCSP-Netunderadjustedpairedt-test,where*meansp<0.05,**meansp<0.01,***meansp<0.001.\nBackbone\nScenario Approach EEGNet DeepCNN ShallowCNN FBCNet EEGConformer Averageacc±std\nCSP-LR - - - - - 74.95\nStandard 75.22±1.94 55.75±2.92 70.70±1.90 78.55±1.75 75.72±1.88 71.19±2.08\nWithin- CSP-Net-1-upd 76.69±3.23 61.03**±3.12 73.12±2.02 81.25*±1.50 81.07**±2.21 74.6***±2.42\nsubject CSP-Net-1-fix 78.92*±1.67 61.33**±1.51 74.89*±1.84 81.17±1.98 80.96**±1.23 75.45***±1.61\nCSP-Net-2-upd 76.61±2.37 56.84±1.19 73.94**±1.35 79.43±2.90 79.96*±2.32 73.36***±2.03\nCSP-Net-2-fix 80.01**±1.47 66.81***±2.26 77.14***±2.17 77.51±2.06 78.64±3.25 76.02***±2.25\nCSP-LR - - - - - 74.21\nStandard 73.37±1.09 68.51±1.26 70.80±0.48 72.91±0.91 63.10±2.41 69.74±1.23\nCross- CSP-Net-1-upd 73.19±1.34 71.94***±1.26 73.03**±0.22 73.08±0.93 73.83***±0.61 73.01***±0.76\nsubject CSP-Net-1-fix 76.60***±0.62 71.56***±0.55 73.87***±0.30 73.91±0.55 73.71***±0.47 73.93***±0.49\nCSP-Net-2-upd 73.07±1.94 70.04±1.28 69.67±1.43 73.11±1.64 70.93***±1.06 71.36***±1.47\nCSP-Net-2-fix 75.44*±1.15 71.06***±0.31 69.83±0.97 72.64±0.70 71.21***±0.48 72.04***±0.72\nTable10: Averageclassificationaccuracies (%)onMI9S.ThosehigherthanStandardaremarkedinbold. Asterisksindicatestatistically significantdifferences\nbetweenstandardbackboneandCSP-Netunderadjustedpairedt-test,where*meansp<0.05,**meansp<0.01,***meansp<0.001.\nBackbone\nScenario Approach EEGNet DeepCNN ShallowCNN FBCNet EEGConformer Averageacc±std\nCSP-LR - - - - - 67.84\nStandard 70.85±0.88 62.34±3.39 69.03±1.51 72.55±1.49 69.65±1.08 68.88±1.67\nWithin- CSP-Net-1-upd 73.01±2.31 62.92±1.53 72.59*±0.98 73.70±1.12 74.91**±2.22 71.43***±1.63\nsubject CSP-Net-1-fix 73.63*±1.85 63.63±1.78 73.17***±1.43 74.31±2.17 73.22*±0.79 71.59***±±1.61\nCSP-Net-2-upd 72.01±3.06 62.78±3.09 71.38*±1.07 71.67±2.49 73.55*±2.07 70.28**±2.36\nCSP-Net-2-fix 71.83±1.98 65.57±0.96 67.19±1.68 72.41±2.57 72.79±2.76 69.96±1.99\nCSP-LR - - - - - 57.72\nStandard 62.40±1.26 54.99±1.15 62.26±0.79 63.84±1.35 58.92±1.74 60.48±1.26\nCross- CSP-Net-1-upd 64.77*±1.75 59.49**±2.51 61.73±0.70 63.41 ±0.69 62.73**±0.92 62.43***±1.31\nsubject CSP-Net-1-fix 64.91*±2.28 60.07**±1.91 62.16±1.45 62.88 ±0.95 63.11**±0.59 62.63**±1.44\nCSP-Net-2-upd 62.03±1.76 61.98***±0.95 63.53±1.78 63.54±1.15 59.96±0.91 62.21**±1.31\nCSP-Net-2-fix 64.72±0.59 59.87**±1.70 62.14±0.58 61.14±1.49 59.96±0.69 61.57±1.01\nTable11:Classificationaccuracies(%)foreachmodel,averagedoverallsubjects.\nCSP- FBCSP- Deep- LMDA- Shallow- EEGCon-\nScenario Dataset LR LR MDRM CNN Net CNN former EEGNet FBCNet CSP-Net-1 CSP-Net-2\nMI4C 61.43 66.81 64.21 51.51 56.75 65.13 61.15 63.50 65.30 71.98 71,82\nMI2C 75.72 76.72 76.32 61.46 71.19 76.29 75.31 76.38 78.11 81.06 81.53\nWithin-subject MI14S 74.95 75.39 78.58 55.75 72.04 70.70 75.72 75.22 78.55 81.07 79.96\nMI9S 67.84 69.37 67.90 62.34 69.94 69.03 69.65 70.85 72.55 74.91 73.55\nAverage 69.99 72.07 71.75 57.77 67.48 70.29 70.46 71.49 73.63 76.26 75.85\nMI4C 52.55 49.72 51.08 47.38 48.28 52.58 40.21 49.44 46.52 52.17 51.72\nMI2C 72.92 73.62 71.99 73.15 69.10 74.32 65.25 71.50 69.34 74.51 75.25\nCross-subject MI14S 74.21 76.36 71.71 68.51 67.87 70.80 63.10 73.37 72.91 76.60 75.44\nMI9S 57.27 63.61 57.39 54.99 63.41 62.26 58.92 62.40 63.84 64.91 64.72\nAverage 64.24 65.83 63.04 61.01 62.17 64.99 56.87 64.18 63.15 67.05 66.78\n8\n(a)\n(b)\nFigure3:AccuracyimprovementsofCSP-NetsatdifferenttrainingdataratiosonMI4C.(a)within-subjectclassification;and,(b)cross-subjectclassification.\n(a)\n(b)\nFigure4:AccuracyimprovementsofCSP-NetsatdifferenttrainingdataratiosonMI2C.(a)within-subjectclassification;and,(b)cross-subjectclassification.\n9\n(a)\n(b)\nFigure5:AccuracyimprovementsofCSP-NetsatdifferenttrainingdataratiosonMI14S.(a)within-subjectclassification;and,(b)cross-subjectclassification.\n(a)\n(b)\nFigure6:AccuracyimprovementsofCSP-NetsatdifferenttrainingdataratiosonMI9S.(a)within-subjectclassification;and,(b)cross-subjectclassification.\n10\nTable12:AblationstudyresultsofCSP-Net-1.Averageaccuracieshigherthan\nStandardaremarkedinbold.\nTrainingdataratio(%)\nDataset Backbone Approach\n10 30 50 70 100\nStandard 37.67 45.60 54.29 57.61 63.50\nCSP-Net-1-rad 37.60 47.19 54.37 57.62 65.60\nEEGNet\nCSP-Net-1-fix 43.39 57.75 62.28 65.72 70.79\nStandard 27.17 28.90 32.13 44.66 51.51\n\u0000(\u0000(\u0000*\u00001\u0000H\u0000W \u0000(\u0000(\u0000*\u00001\u0000H\u0000W\n\u0000\u001a\u0000\u0018 DeepCNN CSP-Net-1-rad 27.33 31.40 32.84 47.53 53.51\n\u0000\u001a\u0000\u0013 \u0000\u0018\u0000\u0017 MI4C CSP-Net-1-fix 30.97 36.95 43.56 56.09 63.28\n\u0000\u0018\u0000\u0015 Standard 34.98 48.69 55.42 59.01 65.13\n\u0000\u0019\u0000\u0018\n\u0000\u0018\u0000\u0013 ShallowCNN CSP-Net-1-rad 36.78 47.59 54.16 60.68 64.69\n\u0000\u0019\u0000\u0013 CSP-Net-1-fix 40.73 55.44 60.00 66.47 70.42\n\u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0014\u0000\u0010\u0000I\u0000L\u0000[ \u0000\u0017\u0000\u001b \u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0014\u0000\u0010\u0000I\u0000L\u0000[\n\u0000\u0018\u0000\u0018\n\u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0015\u0000\u0010\u0000I\u0000L\u0000[ \u0000\u0017\u0000\u0019 \u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0015\u0000\u0010\u0000I\u0000L\u0000[ Standard 33.27 41.06 47.28 53.76 60.05\n\u0000\u0018\u0000\u0013 \u0000\u0017 \u0000\u001b \u0000\u0014\u0000\u0015 \u0000\u0014\u0000\u0019 \u0000\u0015\u0000\u0015 \u0000\u0017 \u0000\u001b \u0000\u0014\u0000\u0015 \u0000\u0014\u0000\u0019 \u0000\u0015\u0000\u0015 Average CSP-Net-1-rad 33.90 42.06 47.12 55.28 61.27\nCSP-Net-1-fix 38.36 50.05 55.28 62.76 68.16\n\u0000'\u0000H\u0000H\u0000S\u0000&\u00001\u00001 \u0000'\u0000H\u0000H\u0000S\u0000&\u00001\u00001\n\u0000\u001a\u0000\u0018\n\u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0014\u0000\u0010\u0000I\u0000L\u0000[ \u0000\u0018\u0000\u0017 \u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0014\u0000\u0010\u0000I\u0000L\u0000[ Standard 56.77 67.53 69.37 70.61 76.38\n\u0000\u001a\u0000\u0013 \u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0015\u0000\u0010\u0000I\u0000L\u0000[ \u0000\u0018\u0000\u0015 \u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0015\u0000\u0010\u0000I\u0000L\u0000[ EEGNet C CS SP P- -N Ne et t- -1 1- -r fia xd 5 67 2. .7 17 2 6 76 1. .7 76 3 6 78 5. .5 21 9 7 71 8. .8 45 5 7 87 1. .1 61 9\n\u0000\u0019\u0000\u0018\n\u0000\u0018\u0000\u0013\n\u0000\u0019\u0000\u0013 Standard 51.04 50.78 51.74 55.13 61.46\n\u0000\u0017\u0000\u001b CSP-Net-1-rad 51.97 52.50 54.14 56.94 62.69\n\u0000\u0018\u0000\u0018 \u0000\u0017\u0000\u0019 MI2C DeepCNN CSP-Net-1-fix 52.07 60.04 63.82 68.17 70.37\n\u0000\u0018\u0000\u0013\n\u0000\u0017 \u0000\u001b \u0000\u0014\u0000\u0015 \u0000\u0014\u0000\u0019 \u0000\u0015\u0000\u0015 \u0000\u0017 \u0000\u001b \u0000\u0014\u0000\u0015 \u0000\u0014\u0000\u0019 \u0000\u0015\u0000\u0015 Standard 51.38 62.34 68.36 71.70 76.29\n\u0000\u001a\u0000\u0018 \u00006\u0000K\u0000D\u0000O\u0000O\u0000R\u0000Z\u0000&\u00001\u00001 \u00006\u0000K\u0000D\u0000O\u0000O\u0000R\u0000Z\u0000&\u00001\u00001 ShallowCNN C CS SP P- -N Ne et t- -1 1- -r fia xd 5 64 2. .2 44 2 6 74 2. .5 15 2 6 79 6. .4 65 9 7 81 1. .2 31 7 7 87 3. .4 74 1\n\u0000\u0018\u0000\u0017\n\u0000\u001a\u0000\u0013 \u0000\u0018\u0000\u0015 Standard 53.06 60.22 63.16 65.81 71.38\n\u0000\u0019\u0000\u0018 Average CSP-Net-1-rad 54.66 61.27 64.03 66.67 72.41\n\u0000\u0019\u0000\u0013 \u0000\u0018\u0000\u0013 CSP-Net-1-fix 58.87 67.96 71.93 76.00 78.59\n\u0000\u0018\u0000\u0018 \u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0014\u0000\u0010\u0000I\u0000L\u0000[ \u0000\u0017\u0000\u001b \u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0014\u0000\u0010\u0000I\u0000L\u0000[ Standard 59.10 66.99 72.25 73.59 75.22\n\u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0015\u0000\u0010\u0000I\u0000L\u0000[ \u0000\u0017\u0000\u0019 \u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0015\u0000\u0010\u0000I\u0000L\u0000[ CSP-Net-1-rad 57.55 68.00 71.30 71.79 74.40\n\u0000\u0018\u0000\u0013 EEGNet\n\u0000\u0017 \u0000\u001b \u0000\u0014\u0000\u0015 \u0000\u0014\u0000\u0019 \u0000\u0015\u0000\u0015 \u0000\u0017 \u0000\u001b \u0000\u0014\u0000\u0015 \u0000\u0014\u0000\u0019 \u0000\u0015\u0000\u0015 CSP-Net-1-fix 61.79 73.53 75.72 78.76 78.92\n\u0000\u001a\u0000\u0018 \u0000$\u0000Y\u0000H\u0000U\u0000D\u0000J\u0000H \u0000$\u0000Y\u0000H\u0000U\u0000D\u0000J\u0000H Standard 51.11 50.80 53.92 56.64 55.75\n\u0000\u0018\u0000\u0017 CSP-Net-1-rad 49.64 53.58 55.95 57.04 55.99 \u0000\u001a\u0000\u0013 DeepCNN\n\u0000\u0018\u0000\u0015 MI14S CSP-Net-1-fix 49.33 53.99 55.40 56.87 61.33\n\u0000\u0019\u0000\u0018\n\u0000\u0018\u0000\u0013 Standard 53.05 57.86 62.07 65.42 70.70\n\u0000\u0019\u0000\u0013 CSP-Net-1-rad 53.08 57.48 61.79 66.19 71.56\n\u0000\u0018\u0000\u0018 \u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0014\u0000\u0010\u0000I\u0000L\u0000[ \u0000\u0017\u0000\u001b \u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0014\u0000\u0010\u0000I\u0000L\u0000[ ShallowCNN CSP-Net-1-fix 54.00 66.40 71.24 72.82 74.89\n\u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0015\u0000\u0010\u0000I\u0000L\u0000[ \u0000\u0017\u0000\u0019 \u0000&\u00006\u00003\u0000\u0010\u00001\u0000H\u0000W\u0000\u0010\u0000\u0015\u0000\u0010\u0000I\u0000L\u0000[\n\u0000\u0018\u0000\u0013 Standard 54.42 58.55 62.75 65.22 67.22\n\u0000\u0017 \u0000\u001b \u0000\u0014\u0000\u0015 \u0000\u0014\u0000\u0019 \u0000\u0015\u0000\u0015 \u0000\u0017 \u0000\u001b \u0000\u0014\u0000\u0015 \u0000\u0014\u0000\u0019 \u0000\u0015\u0000\u0015\n\u00001\u0000X\u0000P\u0000E\u0000H\u0000U\u0000\u0003\u0000R\u0000I\u0000\u0003\u0000I\u0000L\u0000O\u0000W\u0000H\u0000U\u0000V\u0000\u0003\u0000\u000bf\u0000\f \u00001\u0000X\u0000P\u0000E\u0000H\u0000U\u0000\u0003\u0000R\u0000I\u0000\u0003\u0000I\u0000L\u0000O\u0000W\u0000H\u0000U\u0000V\u0000\u0003\u0000\u000bf\u0000\f Average CSP-Net-1-rad 53.42 59.69 63.01 65.01 67.32\nCSP-Net-1-fix 55.04 64.64 67.45 69.48 71.71\n(a) (b)\nStandard 57.47 65.43 68.11 70.85 70.85\nCSP-Net-1-rad 59.03 64.89 67.19 69.97 69.57\nEEGNet\nFigure7:ClassificationaccuraciesofCSP-NetsusingdifferentnumberofCSP CSP-Net-1-fix 61.64 67.39 69.93 71.41 73.63\nfiltersonMI4C.(a)within-subjectclassification;and,(b)cross-subjectclassi-\nStandard 52.06 52.53 54.28 59.09 62.34\nfication.\nCSP-Net-1-rad 51.93 53.01 54.22 56.45 61.92\nDeepCNN\nMI9S CSP-Net-1-fix 52.56 56.00 55.96 57.06 63.63\nStandard 51.89 57.55 61.90 66.04 69.03\nCSP-Net-1-rad 52.45 57.89 62.16 64.19 69.28\nShallowCNN\nCSP-Net-1-fix 55.54 63.96 67.90 70.70 73.17\nStandard 53.81 58.50 61.43 65.33 67.41\nAverage CSP-Net-1-rad 54.47 58.60 61.19 63.54 66.92\nCSP-Net-1-fix 56.58 62.45 64.60 66.39 70.14\n11\n\u0000\\\u0000F\u0000D\u0000U\u0000X\u0000F\u0000F\u0000$\n\u0000\\\u0000F\u0000D\u0000U\u0000X\u0000F\u0000F\u0000$\n\u0000\\\u0000F\u0000D\u0000U\u0000X\u0000F\u0000F\u0000$\n\u0000\f\u0000\b\u0000\u000b\u0000\u0003\u0000\\\u0000F\u0000D\u0000U\u0000X\u0000F\u0000F\u0000$\n\u0000\\\u0000F\u0000D\u0000U\u0000X\u0000F\u0000F\u0000$\n\u0000\\\u0000F\u0000D\u0000U\u0000X\u0000F\u0000F\u0000$\n\u0000\\\u0000F\u0000D\u0000U\u0000X\u0000F\u0000F\u0000$\n\u0000\f\u0000\b\u0000\u000b\u0000\u0003\u0000\\\u0000F\u0000D\u0000U\u0000X\u0000F\u0000F\u0000$\n(a)\nFigure8:Thetrainingandtestcurvesofdifferentmodelson(a)MI4C;(b)MI2C;(c)MI14S;and,(d)MI9S.\n(a)\n(b)\nFigure9:Visualizationofeight(a)spatialfiltersinEEGNetand(b)theCSPfiltersinCSP-Net-2-fixforthewithin-subjectclassificationontheSubject1ofMI2C.\nspecificsensorimotorarea,whichalignedwellwiththespatial References\ncharacteristics of MI. In contrast, the standard EEGNet strug-\ngled to learn effective spatial information due to the absence\n[1] B.Graimann,B.Allison,G.Pfurtscheller,Brain-ComputerInterfaces:A\nof priorknowledgeprovidedbyCSP. Thisalignmentsuggests GentleIntroduction,Springer,Berlin,Heidelberg,2009,pp.1–27.\nthatCSPfilterseffectivelycapturetherelevantspatialpatterns [2] L.F.Nicolas-Alonso,J.Gomez-Gil,Braincomputerinterfaces,areview,\nSensors12(2)(2012)1211–1279.\ninherentinthe EEGdata, enhancingtheinterpretabilityofthe\n[3] L.R.Hochberg,D.Bacher,B.Jarosiewicz,N.Y.Masse,J.D.Simeral,\nmodel.\nJ.Vogel,S.Haddadin,J.Liu,S.S.Cash,P.VanDerSmagt,etal.,Reach\nandgraspbypeoplewithtetraplegiausinganeurallycontrolledrobotic\narm,Nature485(7398)(2012)372–375.\n[4] G.K.Anumanchipalli, J.Chartier, E.F.Chang, Speechsynthesisfrom\n4. Conclusions neuraldecodingofspokensentences,Nature568(7753)(2019)493–498.\n[5] M.Krauledat,K.Grzeska,M.Sagebaum,B.Blankertz,C.Vidaurre,K.-\nR.Mu¨ller,M.Schro¨der,Playingpinballwithnon-invasiveBCI,Advances\nSpatialinformation,whichcanbewellcapturedbyCSPfil- inNeuralInformationProcessingSystems21(2008)1641–1648.\n[6] G. Pfurtscheller, C. Neuper, Motor imagery and direct brain-computer\nters, is critical in EEG-based MI classification. This paper\ncommunication,Proc.oftheIEEE89(7)(2001)1123–1134.\nhasintroducedtwoCSP-Nets, whichintegratetheknowledge- [7] M.Jeannerod, Mentalimageryinthemotorcontext, Neuropsychologia\ndriven CSP filters with data-driven CNN models. CSP-Net-1 33(11)(1995)1419–1432.\ndirectlyaddsaCSPlayerbeforeaCNN,utilizingCSP-filtered [8] G.Pfurtscheller,C.Neuper,D.Flotzinger,M.Pregenzer,EEG-baseddis-\ncriminationbetweenimaginationofrightandlefthandmovement,Elec-\nsignalsasinputtoenhancethediscriminability. CSP-Net-2re-\ntroencephalography andClinical Neurophysiology103(6)(1997)642–\nplacesaconvolutionallayerinCNNwithaCSPlayer. Exper- 651.\niments on four public MI datasets demonstrated that the two [9] B.Blankertz, C.Sannelli, S.Halder, E.M.Hammer, A.Ku¨bler, K.-R.\nMu¨ller, G. Curio, T. Dickhaus, Neurophysiological predictor of SMR-\nCSP-NetsconsistentlyimprovedovertheirCNNbackbones,in\nbasedBCIperformance,NeuroImage51(4)(2010)1303–1309.\nbothwithin-subjectandcross-subjectclassifications. Theyare\n[10] H.Ramoser, J.Muller-Gerking, G.Pfurtscheller, Optimalspatial filter-\nparticularlyusefulwhenthenumberoftrainingsamplesisvery ingofsingletrialEEGduringimaginedhandmovement,IEEETrans.on\nsmall. Remarkably,CSP-Net-1-fix,whoseCSPlayerusesfixed NeuralSystemsandRehabilitationEngineering8(4)(2000)441–446.\n[11] B. Blankertz, R. Tomioka, S.Lemm, M. Kawanabe, K.-r.Muller, Op-\nweights calculated using the traditional CSP algorithm, is the\ntimizingspatialfiltersforrobustEEGsingle-trialanalysis,IEEESignal\nsimplestyetdemonstratesoverallbestperformance.\nProcessingMagazine25(1)(2008)41–56.\nOur work demonstrates the advantage of integrating [12] G.Dornhege,B.Blankertz,G.Curio,K.-R.Muller,Boostingbitratesin\nnoninvasive EEGsingle-trial classifications byfeaturecombinationand\nknowledge-driven CSP filters with data-driven CNNs, or tra-\nmulticlass paradigms, IEEE Trans. on Biomedical Engineering 51 (6)\nditional machine learning with deep learning, in EEG-based\n(2004)993–1002.\nBCIs. [13] K.K.Ang,Z.Y.Chin,H.Zhang,C.Guan,Filterbankcommonspatial\n12\npattern(FBCSP)inbrain-computerinterface, in: Proc.IEEEInt’lJoint\nConf.onNeuralNetworks,HongKong,China,2008,pp.2390–2397.\n[14] F.Lotte,C.Guan,RegularizingcommonspatialpatternstoimproveBCI\ndesigns: unifiedtheoryandnewalgorithms,IEEETrans.onBiomedical\nEngineering58(2)(2010)355–362.\n[15] A. Craik, Y. He, J. L. Contreras-Vidal, Deep learning for electroen-\ncephalogram(EEG)classificationtasks: areview,JournalofNeuralEn-\ngineering16(3)(2019)031001.\n[16] H.Altaheri,G.Muhammad,M.Alsulaiman,S.U.Amin,G.A.Altuwai-\njri, W. Abdul, M. A. Bencherif, M. Faisal, Deep learning techniques\nfor classification of electroencephalogram (EEG) motor imagery (MI)\nsignals: Areview, Neural Computing andApplications 35(20)(2023)\n14681–14722.\n[17] A.Al-Saegh,S.A.Dawwd,J.M.Abdul-Jabbar,Deeplearningformotor\nimageryEEG-basedclassification:Areview,BiomedicalSignalProcess-\ningandControl63(2021)102172.\n[18] R.T.Schirrmeister,J.T.Springenberg,L.D.J.Fiederer,M.Glasstetter,\nK.Eggensperger, M.Tangermann,F.Hutter,W.Burgard,T.Ball,Deep\nlearningwithconvolutionalneuralnetworksforEEGdecodingandvisu-\nalization,HumanBrainMapping38(11)(2017)5391–5420.\n[19] V.J.Lawhern,A.J.Solon,N.R.Waytowich,S.M.Gordon,C.P.Hung,\nB.J.Lance,EEGNet:acompactconvolutionalneuralnetworkforEEG-\nbased brain-computer interfaces, Journal ofNeural Engineering 15 (5)\n(2018)056013.\n[20] R.Mane,E.Chew,K.Chua,K.K.Ang,N.Robinson,A.P.Vinod,S.-W.\nLee,C.Guan,FBCNet: Amulti-viewconvolutional neuralnetworkfor\nbrain-computerinterface,arXivpreprintarXiv:2104.01233(2021).\n[21] Y.Song,Q.Zheng,B.Liu,X.Gao,EEGConformer:Convolutionaltrans-\nformerforEEGdecodingandvisualization,IEEETrans.onNeuralSys-\ntemsandRehabilitationEngineering31(2023)710–719.\n[22] Z.J.Koles, M. S.Lazar, S. Z.Zhou, Spatial patterns underlying pop-\nulationdifferencesinthebackgroundEEG,BrainTopography2(1990)\n275–284.\n[23] J.Mu¨ller-Gerking,G.Pfurtscheller,H.Flyvbjerg,Designingoptimalspa-\ntialfiltersforsingle-trialEEGclassificationinamovementtask,Clinical\nNeurophysiology110(5)(1999)787–798.\n[24] M. Tangermann, K.-R. Mu¨ller, A. Aertsen, N. Birbaumer, C. Braun,\nC.Brunner,R.Leeb,C.Mehring,K.Miller,G.Mueller-Putz,G.Nolte,\nG. Pfurtscheller, H. Preissl, G. Schalk, A. Schlo¨gl, C. Vidaurre,\nS.Waldert,B.Blankertz, Review oftheBCICompetitionIV,Frontiers\ninNeuroscience6(2012)55.\n[25] J. Faller, C. Vidaurre, T. Solis-Escalante, C. Neuper, R. Scherer, Au-\ntocalibration andrecurrent adaptation: Towards aplugandplay online\nERD-BCI,IEEETrans.onNeuralSystemsandRehabilitationEngineer-\ning20(3)(2012)313–319.\n[26] K.Xia, L.Deng, W.Duch, D.Wu, Privacy-preserving domainadapta-\ntionformotorimagery-basedbrain-computerinterfaces,IEEETrans.on\nBiomedicalEngineering69(11)(2022)3365–3376.\n[27] V.Jayaram,A.Barachant,MOABB:trustworthyalgorithmbenchmarking\nforBCIs,JournalofNeuralEngineering15(6)(2018)066011.\n[28] A. Barachant, S. Bonnet, M. Congedo, C. Jutten, Multiclass brain-\ncomputerinterfaceclassificationbyRiemanniangeometry,IEEETrans.\nonBiomedicalEngineering59(4)(2012)920–928.\n[29] Z.Miao,M.Zhao,X.Zhang,D.Ming,LMDA-Net:Alightweightmulti-\ndimensionalattentionnetworkforgeneralEEG-basedbrain-computerin-\nterfacesandinterpretability,NeuroImage(2023)120209.\n13",
    "pdf_filename": "CSP-Net_Common_Spatial_Pattern_Empowered_Neural_Networks_for_EEG-Based_Motor_Imagery_Classification.pdf"
}