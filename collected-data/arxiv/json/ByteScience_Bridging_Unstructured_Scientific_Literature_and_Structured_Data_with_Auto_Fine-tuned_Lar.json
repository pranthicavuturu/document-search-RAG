{
    "title": "ByteScience: Bridging Unstructured Scientific",
    "abstract": "to supply summarization ability from long context to structured described in paragraphs several pages before. information. However, extracting structured knowledge from • ImplicitConnections:Manyrelationshipsinscientificwrit- scientific text by NLP models remains a challenge because of its ing are implied rather than explicitly stated, requiring deep domain-specific nature to complex data preprocessing and the granularityofmulti-layereddevice-levelinformation.Toaddress domain knowledge to infer correctly. this,weintroduceByteScience,anon-profitcloud-basedautofine- • Hierarchical Structures: Scientific documents frequently tunedLargeLanguageModel(LLM)platform,whichisdesigned contain nested relationships, such as experiment subsets or to extract structured scientific data and synthesize new scientific multi-step processes, which are challenging to represent in knowledge from vast scientific corpora. The platform capitalizes flat data structures. on DARWIN, an open-source, fine-tuned LLM dedicated to naturalscience.TheplatformwasbuiltonAmazonWebServices • Cross-ReferenceComplexity:Relationshipsoftenspandif- (AWS) and provides an automated, user-friendly workflow for ferent document parts, such as tables, figures, and citations, custom model development and data extraction. The platform requiring holistic understanding. achievesremarkableaccuracywithonlyasmallamountofwell- • Domain-Specific Semantics: Each scientific field has annotatedarticles.Thisinnovativetoolstreamlinesthetransition uniqueterminologyandconventions,complicatinguniversal fromthescienceliteraturetostructuredknowledgeanddataand benefits the advancements in natural informatics. Demo Video extractionmethods.Forexample,pseudocodeandflowcharts Index Terms—component, formatting, style, styling, insert may be unfamiliar in other domains. I. INTRODUCTION Traditional methods like MatKG [7], which define rela- tionships by entity co-occurrence, often miss the nuances of AI has the potential to revolutionize scientific discovery scientific knowledge. While useful, they risk oversimplifying (AI4Science[1]),butchallengesremain.Scientificknowledge complex relationships. Advanced techniques are needed to isscatteredacrossdocuments,makingithardtofullyleverage bettercapturethiscomplexityforimprovedknowledgeextrac- past research. LLMs offer a promising solution but require tion in AI-driven scientific discovery. Therefore, we introduce structured texts, including converting PDFs and generating ByteScience, a cloud-based platform featuring an auto-fine- fine-tuning examples for NLP tasks. While machine learning tuned LLM to extract structured scientific data and synthesize models are used in fields like drug discovery [2], protein newscientificknowledgefromextensivescientificcorpora.We design [3], and crystal structure generation [4], limited struc- conclude as follows: tured data hinders their effectiveness. Databases like Mate- rials Project [5] and NOMAD [6] cover only a fraction of 1) TailoredwithDARWIN[8],anopen-sourcestate-of-the-art data, leaving much unstructured information untapped. This nature-science LLM, to provide research focus utilization; gap presents an opportunity for AI to accelerate discovery. 2) Zero-code user-friendly semi-automated annotation and Although converting documents to markup is well-studied, processing for uploaded science documents; extracting complex relationships remains challenging but es- 3) A personalized and domain-specific auto fine-tuning LLM sentialforbuildingknowledgegraphsandfine-tuningdatasets. thatrequiresonlyasinglefullyannotatedpieceofliterature; • Contextual Dependency: Relationships in scientific texts 4) Time efficiency high-quality science data extraction from often depend heavily on context that may span multiple millions of papers for less than a second per article. sentences or sections. For instance, a material’s properties 4202 voN 81 ]LC.sc[ 1v00021.1142:viXra",
    "body": "ByteScience: Bridging Unstructured Scientific\nLiterature and Structured Data with Auto Fine-tuned\nLarge Language Model in Token Granularity\nTong Xie∗¶, Hanzhi Zhang†, Shaozhou Wang∗, Yuwei Wan∗, Imran Razzak‡,\nChunyu Kit§, Wenjie Zhang‡, and Bram Hoex¶\n∗GreenDynamics, Kensington, Australia\n{tong, shaozhou, yuwei}@greendynamics.com.au\n†Dept. of Computer Science and Engineering, University of North Texas, Denton, United States, hanzhizhang@my.unt.edu\n‡School of Computer Science and Engineering, University of New South Wales, Kensington, Australia\n{imran.razzak, wenjie.zhang}@unsw.edu.au\n§Dept. of Linguistics and Translation, City University of Hong Kong, Hong Kong, China, ctckit@cityu.edu.hk\n¶School of Photovoltaic and Renewable Energy Engineering, University of New South Wales, Kensington, Australia\n{tong, b.hoex}@unsw.edu.au\nAbstract—Natural Language Processing (NLP) is widely used might be discussed concerning its synthesis method, as\nto supply summarization ability from long context to structured described in paragraphs several pages before.\ninformation. However, extracting structured knowledge from\n• ImplicitConnections:Manyrelationshipsinscientificwrit-\nscientific text by NLP models remains a challenge because of its\ning are implied rather than explicitly stated, requiring deep\ndomain-specific nature to complex data preprocessing and the\ngranularityofmulti-layereddevice-levelinformation.Toaddress domain knowledge to infer correctly.\nthis,weintroduceByteScience,anon-profitcloud-basedautofine- • Hierarchical Structures: Scientific documents frequently\ntunedLargeLanguageModel(LLM)platform,whichisdesigned contain nested relationships, such as experiment subsets or\nto extract structured scientific data and synthesize new scientific\nmulti-step processes, which are challenging to represent in\nknowledge from vast scientific corpora. The platform capitalizes\nflat data structures.\non DARWIN, an open-source, fine-tuned LLM dedicated to\nnaturalscience.TheplatformwasbuiltonAmazonWebServices • Cross-ReferenceComplexity:Relationshipsoftenspandif-\n(AWS) and provides an automated, user-friendly workflow for ferent document parts, such as tables, figures, and citations,\ncustom model development and data extraction. The platform requiring holistic understanding.\nachievesremarkableaccuracywithonlyasmallamountofwell-\n• Domain-Specific Semantics: Each scientific field has\nannotatedarticles.Thisinnovativetoolstreamlinesthetransition\nuniqueterminologyandconventions,complicatinguniversal\nfromthescienceliteraturetostructuredknowledgeanddataand\nbenefits the advancements in natural informatics. Demo Video extractionmethods.Forexample,pseudocodeandflowcharts\nIndex Terms—component, formatting, style, styling, insert may be unfamiliar in other domains.\nI. INTRODUCTION Traditional methods like MatKG [7], which define rela-\ntionships by entity co-occurrence, often miss the nuances of\nAI has the potential to revolutionize scientific discovery\nscientific knowledge. While useful, they risk oversimplifying\n(AI4Science[1]),butchallengesremain.Scientificknowledge\ncomplex relationships. Advanced techniques are needed to\nisscatteredacrossdocuments,makingithardtofullyleverage\nbettercapturethiscomplexityforimprovedknowledgeextrac-\npast research. LLMs offer a promising solution but require\ntion in AI-driven scientific discovery. Therefore, we introduce\nstructured texts, including converting PDFs and generating\nByteScience, a cloud-based platform featuring an auto-fine-\nfine-tuning examples for NLP tasks. While machine learning\ntuned LLM to extract structured scientific data and synthesize\nmodels are used in fields like drug discovery [2], protein\nnewscientificknowledgefromextensivescientificcorpora.We\ndesign [3], and crystal structure generation [4], limited struc-\nconclude as follows:\ntured data hinders their effectiveness. Databases like Mate-\nrials Project [5] and NOMAD [6] cover only a fraction of\n1) TailoredwithDARWIN[8],anopen-sourcestate-of-the-art\ndata, leaving much unstructured information untapped. This\nnature-science LLM, to provide research focus utilization;\ngap presents an opportunity for AI to accelerate discovery.\n2) Zero-code user-friendly semi-automated annotation and\nAlthough converting documents to markup is well-studied,\nprocessing for uploaded science documents;\nextracting complex relationships remains challenging but es-\n3) A personalized and domain-specific auto fine-tuning LLM\nsentialforbuildingknowledgegraphsandfine-tuningdatasets.\nthatrequiresonlyasinglefullyannotatedpieceofliterature;\n• Contextual Dependency: Relationships in scientific texts 4) Time efficiency high-quality science data extraction from\noften depend heavily on context that may span multiple\nmillions of papers for less than a second per article.\nsentences or sections. For instance, a material’s properties\n4202\nvoN\n81\n]LC.sc[\n1v00021.1142:viXra\nFig.1. ByteSciencePipeline.Theinitialsetupforaspecificfieldinvolvesconstructingadomain-specificcorpusofstructuredscientificdata(GreenPipeline)\nand fine-tuning an LLM on this dataset to optimize performance for the target scientific domain (Blue Pipeline). Once this setup is complete, users can\nefficientlygeneratestructureddatasetsfromnewscientificdocumentsinthesamefieldbyutilizingthefine-tunedLLMstoredinAWS.\nII. PLATFORMDESIGN 5) Correction: Users review and correct the auto-labeled\nannotations, ensuring accuracy and consistency.\nByteScienceisarobust,scalablecloud-basedsolutionlever-\n6) Training: Corrected annotations are used to train or fine-\naging AWS Sagemaker. This architecture ensures high avail-\ntune an LLM, with training done via Amazon SageMaker.\nability,scalability,andperformanceforprocessinglargescien-\n7) Fine-Tuned LLM: The training process results in a fine-\ntific documents. Figure 1 illustrates the extraction pipeline for\ntuned LLM customized for the specific annotation task.\ncustom model development and data extraction. The pipeline\n8) Structured Data Generation: The fine-tuned LLM pro-\nconsists of two primary phases:\ncesses new documents into structured data stored in Mon-\n• Initial Setup (First-time use for a specific field):\ngoDBasJSON,allowingflexibleuseandefficientquerying.\nDataset Construction (Green Pipeline): This phase builds a\ndomain-specific corpus of structured scientific data. After uploading the training dataset, it is transformed into\nLLM Fine-tuning (Blue Pipeline): The system fine-tunes a the LLM’s instruction format for fine-tuning. Users should\nlargelanguagemodelontheconstructeddatasettooptimize first test a small text subset and assess accuracy and recall. If\nperformance for the target scientific domain. accuracy is low, add more annotated data and retrain. For low\n• OperationalPhase:Oncetheinitialsetupiscomplete,users recall, generate more corpus data and use sequential learning\ncan directly utilize the fine-tuned LLM stored in AWS to to train a new model.\nefficiently generate structured datasets from new scientific\ndocuments in the same field. III. ARCHITECTUREOFAWSCLOUD-BASEDSERVICES\nThis two-phase approach allows ByteScience to quickly\nByteScience utilizes the robust, scalable infrastructure of\nadapttovariousscientificdomainswhilemaintaininghighex-\nAmazon Web Services (AWS) to efficiently handle user\ntraction accuracy. The cloud-based architecture enables seam-\nrequests and data processing. Figure 2 shows the detailed\nless scaling and ensures users always have access to the latest\narchitecture of our platform.\nfine-tunedmodels,streamliningtheconversionofunstructured\nscientific literature into structured data. Key steps include:\nA. General Service(Green Pipeline) Infrastructure\n1) Create Database: Users upload scientific documents in\nThe user interaction layer is built on a series of AWS ser-\nJSON, PDF, HTML, or XML formats. Non-JSON text is\nvices that ensure high availability, security, and performance:\nextracted and saved as JSON, with HTML/XML markup\nstripped, and PDF conversion done using PDFMiner [9]. • DNS Management: AWS Route 53 routes incoming user\n2) Define Structure: Users define annotation structures, in- requests to the appropriate services within the architecture.\ncluding entity labels and relationships, using pre-built or • Load Balancing: An Application Load Balancer (ALB)\ncustom templates. distributes traffic evenly across multiple backend servers,\n3) Random Selection: A small text subset is randomly se- ensuring fault tolerance and optimal performance.\nlected for initial annotation on first use. • Compute Resources: Backend servers, organized in an\n4) Auto Labelling: The LLM applies automatic pre-labeling Auto Scaling group within a Virtual Private Cloud (VPC),\nto the selected texts. provide a secure, scalable environment to traffic demands.\nFig.2. ThearchitectureofByteSciencecreatesastructureddatabaseonAWScloudwithLLM.\n• Database Services: Amazon Relational Database Service IV. STRUCTUREDDATAEXTRACTIONPERFORMANCE\n(RDS) supports complex queries and transactions essential LLMs significantly improve human-in-the-loop annotation.\nfor scientific data management. Using 300 training samples reduced annotation time by 57%\ncompared to a single sample [10]. In the GPT-3/Doping-\nB. LLM Service (Blue Pipeline) Architecture\nEnglishmodel,10-20sampleswereenoughtolearnthecorrect\nThe LLM fine-tuning capability is a core function, imple-\nstructure,withprecision,recall,andF1scoresreaching0.8-0.9\nmented through a sophisticated pipeline of AWS services:\nwith around 300 samples.\n• Model Development: SageMaker Notebook is used for Inourexperiment,wecomparednon-LLMandLLMmeth-\ndevelopingfoundationmodels.WereproducedtheDARWIN ods for structured data extraction on 90 samples covering\nmodel by fine-tuning LLaMA 7B, leveraging SageMaker’s batteries,catalysis,andphotovoltaics,alongsideByteScience’s\ncomputational tools. results. As shown in Table I, we evaluated Named Entity\n• a DARWIN LLM processes Training Dataset Prepara- Recognition(NER),RelationExtraction(RE),andEntityRes-\ntion: User annotations hosted on a SageMaker Endpoint, olution (ER). While models like MatBERT performed well,\npreparing data for model training. they often produced irrelevant entities, lowering precision.\n• Data Storage: Training datasets are securely stored on In contrast, LLMs handled unstructured information more\nAmazon S3, ensuring durability and accessibility. reliably, and our system outperformed traditional methods\n• Model Fine-tuning: A SageMaker Training Job handles across all tasks with fewer samples.\nfine-tuning the model at scale, utilizing AWS’s distributed\ncomputing capabilities. TABLEI\n• Model Deployment: The fine-tuned model is deployed to a RESULTOFSTRUCTUREDDATAEXTRACTION.\nSageMakerEndpoint,providingamanagedenvironmentfor\nTask Model Precision Recall F1score\nquerying the LLM for data extraction tasks.\nMatBERT 0.1196 0.6869 0.2036\nLlama7b 0.6101 0.6216 0.6158\nC. Workflow Integration\nNER Llama27b 0.7419 0.7667 0.7541\nByteScience workflow seamlessly integrates these compo- Darwin 0.8013 0.7935 0.7974\nBytescience 0.9520 0.9083 0.9296\nnents:\nMatBERT 0.0250 0.5696 0.0479\n1) Users interact with the system through Route 53 and the Llama7b 0.5305 0.5405 0.5355\nRE Llama27b 0.6452 0.6667 0.6557\nALB for initial annotation tasks.\nDarwin 0.7036 0.6968 0.7002\n2) The DARWIN LLM processes annotated data on a Sage- Bytescience 0.9039 0.8625 0.8827\nMaker Endpoint. MatBERT 0.0928 0.5303 0.1579\n3) The SageMaker Notebook is used for model development Llama7b 0.3687 0.3757 0.3722\nER Llama27b 0.4484 0.4633 0.4557\nand improvement. Darwin 0.4593 0.4548 0.4571\n4) Training datasets on S3 are used to fine-tune the model via Bytescience 0.9127 0.8708 0.8913\nSageMaker Training Jobs.\n5) The resulting model is deployed to a SageMaker Endpoint. V. BYTESCIENCEINACTION:AUSERCASESTUDY\n6) Users can then perform data extraction tasks, processing ToshowcaseByteScience’sapplication,wepresentThomas,\nrequests by the fine-tuned LLM. a materials scientist automating alloy synthesis by analyz-\nThis architecture enables ByteScience to offer customized, ing literature to establish ”Composition-Processing-Structure-\nhigh-performance language models tailored to specific scien- Performance” (CPSP) relationships. He designs alloy compo-\ntificdomains,facilitatingaccurateandefficientstructureddata sitions,developsprocessingmethods,andpredictsmicrostruc-\nextraction from unstructured scientific literature. tures using data on casting, solution treatment, and aging.\nA. Initial Setup: Schema, Semi-Annotation, Model Fine-Tune VII. CONCLUSION\nThomas configures ByteScience to meet his research needs ByteScience is leveraging a powerful approach to handle\nby designing a custom annotation schema for alloy synthesis, unstructured text by fine-tuning DARWIN, a pre-trained nat-\nannotating key details like compositions, casting parameters, ural science LLM, using a minimal set of annotated articles.\nsolution treatment, and aging variables. ByteScience then HostedontheAWScloud,thisplatformautomatestheprocess\ninitiatessemi-automaticannotation,wheretheDARWINLLM of extracting structured data from scientific texts, presenting a\nauto-labels papers from his corpus based on this schema. zero-code solution that could significantly enhance efficiency\nThomas reviews and corrects the annotations to refine the innaturalscienceresearch.ThekeyadvantageofByteScience\nmodel’s understanding. Afterward, ByteScience fine-tunes the lies in its ability to train the DARWIN model with few\nLLM using AWS SageMaker, optimizing it for alloy synthe- annotations, making it exceptionally adaptive and efficient.\nsis data extraction. The fine-tuned model is deployed to a This capability ensures that the extracted material data is\nSageMaker Endpoint for efficient, large-scale processing of high-qualityandhighlyaccurate.ByteScienceexemplifieshow\ncomplex scientific papers. cutting-edge technology can be harnessed to propel advance-\nments in science, engineering, and research by integrating\nB. DataGeneration:DocumentUpload,EndpointUtilization,\nadvanced NLP techniques with cloud computing. This ini-\nand Dataset Creation\ntiative represents a substantial step forward in making vast\nWith the fine-tuned model, Thomas uploads his entire scientificcorporamoreaccessibleandusable,highlightingthe\ncorpus of scientific papers to ByteScience, which processes transformative potential of AI in scientific data processing.\nvarious formats for comprehensive coverage. He initiates To optimize resource efficiency, we are developing a slicing\nlarge-scaledataextractionviatheSageMakerEndpoint,where version that fine-tunes a low-resource inference model using\nthemodelextractsdetailedinformationonalloycompositions, only partial data from extensive content.\ncasting processes, solution treatments, and aging procedures.\nThis automation accelerates his research, completing in days REFERENCES\nwhat would have taken months manually. The extracted data\n[1] R. Stevens, V. Taylor, J. Nichols, A. B. Maccabe, K. Yelick,\nis structured and stored in MongoDB, allowing Thomas to and D. Brown, “AI for Science: Report on the Department of\neasily query, analyze, and identify trends in alloy synthesis, Energy (DOE) Town Halls on Artificial Intelligence (AI) for\nScience,” Argonne National Lab. (ANL), Argonne, IL (United\nuncovering insights that manual review might have missed.\nStates), Tech. Rep. ANL-20/17, Feb. 2020. [Online]. Available:\nhttps://www.osti.gov/biblio/1604756\nC. Further Dataset Updates and Refinement [2] J. Vamathevan, D. Clark, P. Czodrowski, I. Dunham, E. Ferran,\nG. Lee, B. Li, A. Madabhushi, P. Shah, M. Spitzer, and S. Zhao,\nAs Thomas advances in his research, he updates his dataset\n“Applicationsofmachinelearningindrugdiscoveryanddevelopment,”\nwith ByteScience, uploading new papers and processing them Nature Reviews Drug Discovery, vol. 18, no. 6, pp. 463–477,\nthroughthefine-tunedmodeltocontinuallyenrichhisdataset. Jun. 2019, publisher: Nature Publishing Group. [Online]. Available:\nhttps://www.nature.com/articles/s41573-019-0024-5\nWhen discrepancies or improvements are needed, he initiates\n[3] “Machine learning for functional protein design | Nature\na re-training cycle, reviewing and correcting a subset of the Biotechnology.” [Online]. Available: https://www.nature.com/articles/\nnewly processed papers to further fine-tune the model. This s41587-024-02127-0\n[4] T. Xie, Y. Wan, H. Wang, I. Østrøm, S. Wang, M. He, R. Deng,\niterativeprocessensuresthemodelstaysaccurateandadaptsto\nX. Wu, C. Grazian, C. Kit, and B. Hoex, “Opinion Mining by\nevolvingterminologiesormethodsinalloysynthesis.Through Convolutional Neural Networks for Maximizing Discoverability of\nthis dynamic interaction, Thomas maintains an up-to-date, Nanomaterials,” Journal of Chemical Information and Modeling,\nvol. 64, no. 7, pp. 2746–2759, Apr. 2024. [Online]. Available:\naccurate dataset, enhancing his research and keeping him at\nhttps://pubs.acs.org/doi/10.1021/acs.jcim.3c00746\nthe forefront of alloy synthesis advancements. [5] A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards, S. Dacek,\nS.Cholia,D.Gunter,D.Skinner,G.Ceder,andothers,“Commentary:\nVI. SIGNIFICANCETOSCIENCE The Materials Project: A materials genome approach to accelerating\nmaterials innovation,” APL materials, vol. 1, no. 1, 2013, publisher:\nConstructing databases from scholarly literature is cru-\nAIPPublishing.\ncial for modern research, but traditional methods are time- [6] C.DraxlandM.Scheffler,“TheNOMADlaboratory:fromdatasharing\nconsuming and resource-intensive. ByteScience transforms toartificialintelligence,”JournalofPhysics:Materials,vol.2,no.3,p.\n036001,2019,publisher:IOPPublishing.\nthis process by enabling users to create a customized data\n[7] V.Venugopal,S.Pai,andE.Olivetti,“MatKG:TheLargestKnowledge\nextractiontoolinhours,achieving80%-90%humanaccuracy. Graph in Materials Science – Entities, Relations, and Link Prediction\nIt can process a 10-page scientific document in one second, throughGraphRepresentationLearning,”Oct.2022,arXiv:2210.17340\n[cond-mat].[Online].Available:http://arxiv.org/abs/2210.17340\ncompared to the 20-30 minutes it takes a researcher. With an\n[8] T. Xie, Y. Wan, W. Huang, Z. Yin, Y. Liu, S. Wang, Q. Linghu,\nextraction cost of just $0.023 per paper for 10,000 articles, C. Kit, C. Grazian, W. Zhang, and others, “DARWIN Series: Domain\nByteScience makes large-scale data extraction affordable and Specific Large Language Models for Natural Science,” arXiv preprint\narXiv:2308.13565,2023.\naccessible. Its versatility across scientific fields democratizes\n[9] Y.Shinyama,“Pdfminer:Pythonpdfparserandanalyzer,”Retrievedon,\naccess to advanced data extraction, providing computational vol.11,2015.\npower equivalent to hundreds of annotators. This accelerates [10] A. Dunn, J. Dagdelen, N. Walker, S. Lee, A. S. Rosen, G. Ceder,\nK. Persson, and A. Jain, “Structured information extraction from\ndiscovery, enhances research decision-making, and fosters\ncomplex scientific text with fine-tuned large language models,” 2022,\ninnovation across disciplines. arXiv:2212.05238.[Online].Available:http://arxiv.org/abs/2212.05238\nAPPENDIX\nFigure 3 shows the label-defining function in our system.\nUsers define the structure for annotations, including entity\nlabels and their relationships (visualized by indent). For each\nlabel, there is a definition textbox for filling.\nFigure4showstheannotationfunction.Userscanuseauto-\nmatic pre-labelling to the selected texts. Different colors will\nvisualize the auto-labeled annotations and users can review\nand correct them, ensuring accuracy and consistency.\nFigure 5 shows the data extraction function on example\npaper. Users can create a customized data extraction tool that\nachieves 80%-90% of human extraction accuracy after just a\nfew hours of annotation.\nFig.3. Screenshotoflabelsetup.\nFig.4. Screenshotoflabelingpage.\nFig.5. Screenshotofextractionresultsofapaper.",
    "pdf_filename": "ByteScience_Bridging_Unstructured_Scientific_Literature_and_Structured_Data_with_Auto_Fine-tuned_Lar.pdf"
}