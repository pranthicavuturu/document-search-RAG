{
    "title": "ByteScience Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Lar",
    "abstract": "to supply summarization ability from long context to structured information. However, extracting structured knowledge from scientific text by NLP models remains a challenge because of its domain-specific nature to complex data preprocessing and the granularity of multi-layered device-level information. To address this, we introduce ByteScience, a non-profit cloud-based auto fine- tuned Large Language Model (LLM) platform, which is designed to extract structured scientific data and synthesize new scientific knowledge from vast scientific corpora. The platform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to natural science. The platform was built on Amazon Web Services (AWS) and provides an automated, user-friendly workflow for custom model development and data extraction. The platform achieves remarkable accuracy with only a small amount of well- annotated articles. This innovative tool streamlines the transition from the science literature to structured knowledge and data and benefits the advancements in natural informatics. Demo Video Index Terms—component, formatting, style, styling, insert I. INTRODUCTION AI has the potential to revolutionize scientific discovery (AI4Science [1]), but challenges remain. Scientific knowledge is scattered across documents, making it hard to fully leverage past research. LLMs offer a promising solution but require structured texts, including converting PDFs and generating fine-tuning examples for NLP tasks. While machine learning models are used in fields like drug discovery [2], protein design [3], and crystal structure generation [4], limited struc- tured data hinders their effectiveness. Databases like Mate- rials Project [5] and NOMAD [6] cover only a fraction of data, leaving much unstructured information untapped. This gap presents an opportunity for AI to accelerate discovery. Although converting documents to markup is well-studied, extracting complex relationships remains challenging but es- sential for building knowledge graphs and fine-tuning datasets. • Contextual Dependency: Relationships in scientific texts often depend heavily on context that may span multiple sentences or sections. For instance, a material’s properties might be discussed concerning its synthesis method, as described in paragraphs several pages before. • Implicit Connections: Many relationships in scientific writ- ing are implied rather than explicitly stated, requiring deep domain knowledge to infer correctly. • Hierarchical Structures: Scientific documents frequently contain nested relationships, such as experiment subsets or multi-step processes, which are challenging to represent in flat data structures. • Cross-Reference Complexity: Relationships often span dif- ferent document parts, such as tables, figures, and citations, requiring holistic understanding. • Domain-Specific Semantics: Each scientific field has unique terminology and conventions, complicating universal extraction methods. For example, pseudocode and flowcharts may be unfamiliar in other domains. Traditional methods like MatKG [7], which define rela- tionships by entity co-occurrence, often miss the nuances of scientific knowledge. While useful, they risk oversimplifying complex relationships. Advanced techniques are needed to better capture this complexity for improved knowledge extrac- tion in AI-driven scientific discovery. Therefore, we introduce ByteScience, a cloud-based platform featuring an auto-fine- tuned LLM to extract structured scientific data and synthesize new scientific knowledge from extensive scientific corpora. We conclude as follows: 1) Tailored with DARWIN [8], an open-source state-of-the-art nature-science LLM, to provide research focus utilization; 2) Zero-code user-friendly semi-automated annotation and processing for uploaded science documents; 3) A personalized and domain-specific auto fine-tuning LLM that requires only a single fully annotated piece of literature; 4) Time efficiency high-quality science data extraction from millions of papers for less than a second per article. arXiv:2411.12000v1  [cs.CL]  18 Nov 2024",
    "body": "ByteScience: Bridging Unstructured Scientific\nLiterature and Structured Data with Auto Fine-tuned\nLarge Language Model in Token Granularity\nTong Xie∗¶, Hanzhi Zhang†, Shaozhou Wang∗, Yuwei Wan∗, Imran Razzak‡,\nChunyu Kit§, Wenjie Zhang‡, and Bram Hoex¶\n∗GreenDynamics, Kensington, Australia\n{tong, shaozhou, yuwei}@greendynamics.com.au\n†Dept. of Computer Science and Engineering, University of North Texas, Denton, United States, hanzhizhang@my.unt.edu\n‡School of Computer Science and Engineering, University of New South Wales, Kensington, Australia\n{imran.razzak, wenjie.zhang}@unsw.edu.au\n§Dept. of Linguistics and Translation, City University of Hong Kong, Hong Kong, China, ctckit@cityu.edu.hk\n¶School of Photovoltaic and Renewable Energy Engineering, University of New South Wales, Kensington, Australia\n{tong, b.hoex}@unsw.edu.au\nAbstract—Natural Language Processing (NLP) is widely used\nto supply summarization ability from long context to structured\ninformation. However, extracting structured knowledge from\nscientific text by NLP models remains a challenge because of its\ndomain-specific nature to complex data preprocessing and the\ngranularity of multi-layered device-level information. To address\nthis, we introduce ByteScience, a non-profit cloud-based auto fine-\ntuned Large Language Model (LLM) platform, which is designed\nto extract structured scientific data and synthesize new scientific\nknowledge from vast scientific corpora. The platform capitalizes\non DARWIN, an open-source, fine-tuned LLM dedicated to\nnatural science. The platform was built on Amazon Web Services\n(AWS) and provides an automated, user-friendly workflow for\ncustom model development and data extraction. The platform\nachieves remarkable accuracy with only a small amount of well-\nannotated articles. This innovative tool streamlines the transition\nfrom the science literature to structured knowledge and data and\nbenefits the advancements in natural informatics. Demo Video\nIndex Terms—component, formatting, style, styling, insert\nI. INTRODUCTION\nAI has the potential to revolutionize scientific discovery\n(AI4Science [1]), but challenges remain. Scientific knowledge\nis scattered across documents, making it hard to fully leverage\npast research. LLMs offer a promising solution but require\nstructured texts, including converting PDFs and generating\nfine-tuning examples for NLP tasks. While machine learning\nmodels are used in fields like drug discovery [2], protein\ndesign [3], and crystal structure generation [4], limited struc-\ntured data hinders their effectiveness. Databases like Mate-\nrials Project [5] and NOMAD [6] cover only a fraction of\ndata, leaving much unstructured information untapped. This\ngap presents an opportunity for AI to accelerate discovery.\nAlthough converting documents to markup is well-studied,\nextracting complex relationships remains challenging but es-\nsential for building knowledge graphs and fine-tuning datasets.\n• Contextual Dependency: Relationships in scientific texts\noften depend heavily on context that may span multiple\nsentences or sections. For instance, a material’s properties\nmight be discussed concerning its synthesis method, as\ndescribed in paragraphs several pages before.\n• Implicit Connections: Many relationships in scientific writ-\ning are implied rather than explicitly stated, requiring deep\ndomain knowledge to infer correctly.\n• Hierarchical Structures: Scientific documents frequently\ncontain nested relationships, such as experiment subsets or\nmulti-step processes, which are challenging to represent in\nflat data structures.\n• Cross-Reference Complexity: Relationships often span dif-\nferent document parts, such as tables, figures, and citations,\nrequiring holistic understanding.\n• Domain-Specific\nSemantics: Each scientific field has\nunique terminology and conventions, complicating universal\nextraction methods. For example, pseudocode and flowcharts\nmay be unfamiliar in other domains.\nTraditional methods like MatKG [7], which define rela-\ntionships by entity co-occurrence, often miss the nuances of\nscientific knowledge. While useful, they risk oversimplifying\ncomplex relationships. Advanced techniques are needed to\nbetter capture this complexity for improved knowledge extrac-\ntion in AI-driven scientific discovery. Therefore, we introduce\nByteScience, a cloud-based platform featuring an auto-fine-\ntuned LLM to extract structured scientific data and synthesize\nnew scientific knowledge from extensive scientific corpora. We\nconclude as follows:\n1) Tailored with DARWIN [8], an open-source state-of-the-art\nnature-science LLM, to provide research focus utilization;\n2) Zero-code user-friendly semi-automated annotation and\nprocessing for uploaded science documents;\n3) A personalized and domain-specific auto fine-tuning LLM\nthat requires only a single fully annotated piece of literature;\n4) Time efficiency high-quality science data extraction from\nmillions of papers for less than a second per article.\narXiv:2411.12000v1  [cs.CL]  18 Nov 2024\n\nFig. 1. ByteScience Pipeline. The initial setup for a specific field involves constructing a domain-specific corpus of structured scientific data (Green Pipeline)\nand fine-tuning an LLM on this dataset to optimize performance for the target scientific domain (Blue Pipeline). Once this setup is complete, users can\nefficiently generate structured datasets from new scientific documents in the same field by utilizing the fine-tuned LLM stored in AWS.\nII. PLATFORM DESIGN\nByteScience is a robust, scalable cloud-based solution lever-\naging AWS Sagemaker. This architecture ensures high avail-\nability, scalability, and performance for processing large scien-\ntific documents. Figure 1 illustrates the extraction pipeline for\ncustom model development and data extraction. The pipeline\nconsists of two primary phases:\n• Initial Setup (First-time use for a specific field):\nDataset Construction (Green Pipeline): This phase builds a\ndomain-specific corpus of structured scientific data.\nLLM Fine-tuning (Blue Pipeline): The system fine-tunes a\nlarge language model on the constructed dataset to optimize\nperformance for the target scientific domain.\n• Operational Phase: Once the initial setup is complete, users\ncan directly utilize the fine-tuned LLM stored in AWS to\nefficiently generate structured datasets from new scientific\ndocuments in the same field.\nThis two-phase approach allows ByteScience to quickly\nadapt to various scientific domains while maintaining high ex-\ntraction accuracy. The cloud-based architecture enables seam-\nless scaling and ensures users always have access to the latest\nfine-tuned models, streamlining the conversion of unstructured\nscientific literature into structured data. Key steps include:\n1) Create Database: Users upload scientific documents in\nJSON, PDF, HTML, or XML formats. Non-JSON text is\nextracted and saved as JSON, with HTML/XML markup\nstripped, and PDF conversion done using PDFMiner [9].\n2) Define Structure: Users define annotation structures, in-\ncluding entity labels and relationships, using pre-built or\ncustom templates.\n3) Random Selection: A small text subset is randomly se-\nlected for initial annotation on first use.\n4) Auto Labelling: The LLM applies automatic pre-labeling\nto the selected texts.\n5) Correction: Users review and correct the auto-labeled\nannotations, ensuring accuracy and consistency.\n6) Training: Corrected annotations are used to train or fine-\ntune an LLM, with training done via Amazon SageMaker.\n7) Fine-Tuned LLM: The training process results in a fine-\ntuned LLM customized for the specific annotation task.\n8) Structured Data Generation: The fine-tuned LLM pro-\ncesses new documents into structured data stored in Mon-\ngoDB as JSON, allowing flexible use and efficient querying.\nAfter uploading the training dataset, it is transformed into\nthe LLM’s instruction format for fine-tuning. Users should\nfirst test a small text subset and assess accuracy and recall. If\naccuracy is low, add more annotated data and retrain. For low\nrecall, generate more corpus data and use sequential learning\nto train a new model.\nIII. ARCHITECTURE OF AWS CLOUD-BASED SERVICES\nByteScience utilizes the robust, scalable infrastructure of\nAmazon Web Services (AWS) to efficiently handle user\nrequests and data processing. Figure 2 shows the detailed\narchitecture of our platform.\nA. General Service(Green Pipeline) Infrastructure\nThe user interaction layer is built on a series of AWS ser-\nvices that ensure high availability, security, and performance:\n• DNS Management: AWS Route 53 routes incoming user\nrequests to the appropriate services within the architecture.\n• Load Balancing: An Application Load Balancer (ALB)\ndistributes traffic evenly across multiple backend servers,\nensuring fault tolerance and optimal performance.\n• Compute Resources: Backend servers, organized in an\nAuto Scaling group within a Virtual Private Cloud (VPC),\nprovide a secure, scalable environment to traffic demands.\n\nFig. 2. The architecture of ByteScience creates a structured database on AWS cloud with LLM.\n• Database Services: Amazon Relational Database Service\n(RDS) supports complex queries and transactions essential\nfor scientific data management.\nB. LLM Service (Blue Pipeline) Architecture\nThe LLM fine-tuning capability is a core function, imple-\nmented through a sophisticated pipeline of AWS services:\n• Model Development: SageMaker Notebook is used for\ndeveloping foundation models. We reproduced the DARWIN\nmodel by fine-tuning LLaMA 7B, leveraging SageMaker’s\ncomputational tools.\n• a DARWIN LLM processes Training Dataset Prepara-\ntion: User annotations hosted on a SageMaker Endpoint,\npreparing data for model training.\n• Data Storage: Training datasets are securely stored on\nAmazon S3, ensuring durability and accessibility.\n• Model Fine-tuning: A SageMaker Training Job handles\nfine-tuning the model at scale, utilizing AWS’s distributed\ncomputing capabilities.\n• Model Deployment: The fine-tuned model is deployed to a\nSageMaker Endpoint, providing a managed environment for\nquerying the LLM for data extraction tasks.\nC. Workflow Integration\nByteScience workflow seamlessly integrates these compo-\nnents:\n1) Users interact with the system through Route 53 and the\nALB for initial annotation tasks.\n2) The DARWIN LLM processes annotated data on a Sage-\nMaker Endpoint.\n3) The SageMaker Notebook is used for model development\nand improvement.\n4) Training datasets on S3 are used to fine-tune the model via\nSageMaker Training Jobs.\n5) The resulting model is deployed to a SageMaker Endpoint.\n6) Users can then perform data extraction tasks, processing\nrequests by the fine-tuned LLM.\nThis architecture enables ByteScience to offer customized,\nhigh-performance language models tailored to specific scien-\ntific domains, facilitating accurate and efficient structured data\nextraction from unstructured scientific literature.\nIV. STRUCTURED DATA EXTRACTION PERFORMANCE\nLLMs significantly improve human-in-the-loop annotation.\nUsing 300 training samples reduced annotation time by 57%\ncompared to a single sample [10]. In the GPT-3/Doping-\nEnglish model, 10-20 samples were enough to learn the correct\nstructure, with precision, recall, and F1 scores reaching 0.8-0.9\nwith around 300 samples.\nIn our experiment, we compared non-LLM and LLM meth-\nods for structured data extraction on 90 samples covering\nbatteries, catalysis, and photovoltaics, alongside ByteScience’s\nresults. As shown in Table I, we evaluated Named Entity\nRecognition (NER), Relation Extraction (RE), and Entity Res-\nolution (ER). While models like MatBERT performed well,\nthey often produced irrelevant entities, lowering precision.\nIn contrast, LLMs handled unstructured information more\nreliably, and our system outperformed traditional methods\nacross all tasks with fewer samples.\nTABLE I\nRESULT OF STRUCTURED DATA EXTRACTION.\nTask\nModel\nPrecision\nRecall\nF1 score\nMatBERT\n0.1196\n0.6869\n0.2036\nLlama 7b\n0.6101\n0.6216\n0.6158\nNER\nLlama2 7b\n0.7419\n0.7667\n0.7541\nDarwin\n0.8013\n0.7935\n0.7974\nBytescience\n0.9520\n0.9083\n0.9296\nMatBERT\n0.0250\n0.5696\n0.0479\nLlama 7b\n0.5305\n0.5405\n0.5355\nRE\nLlama2 7b\n0.6452\n0.6667\n0.6557\nDarwin\n0.7036\n0.6968\n0.7002\nBytescience\n0.9039\n0.8625\n0.8827\nMatBERT\n0.0928\n0.5303\n0.1579\nLlama 7b\n0.3687\n0.3757\n0.3722\nER\nLlama2 7b\n0.4484\n0.4633\n0.4557\nDarwin\n0.4593\n0.4548\n0.4571\nBytescience\n0.9127\n0.8708\n0.8913\nV. BYTESCIENCE IN ACTION: A USER CASE STUDY\nTo showcase ByteScience’s application, we present Thomas,\na materials scientist automating alloy synthesis by analyz-\ning literature to establish ”Composition-Processing-Structure-\nPerformance” (CPSP) relationships. He designs alloy compo-\nsitions, develops processing methods, and predicts microstruc-\ntures using data on casting, solution treatment, and aging.\n\nA. Initial Setup: Schema, Semi-Annotation, Model Fine-Tune\nThomas configures ByteScience to meet his research needs\nby designing a custom annotation schema for alloy synthesis,\nannotating key details like compositions, casting parameters,\nsolution treatment, and aging variables. ByteScience then\ninitiates semi-automatic annotation, where the DARWIN LLM\nauto-labels papers from his corpus based on this schema.\nThomas reviews and corrects the annotations to refine the\nmodel’s understanding. Afterward, ByteScience fine-tunes the\nLLM using AWS SageMaker, optimizing it for alloy synthe-\nsis data extraction. The fine-tuned model is deployed to a\nSageMaker Endpoint for efficient, large-scale processing of\ncomplex scientific papers.\nB. Data Generation: Document Upload, Endpoint Utilization,\nand Dataset Creation\nWith the fine-tuned model, Thomas uploads his entire\ncorpus of scientific papers to ByteScience, which processes\nvarious formats for comprehensive coverage. He initiates\nlarge-scale data extraction via the SageMaker Endpoint, where\nthe model extracts detailed information on alloy compositions,\ncasting processes, solution treatments, and aging procedures.\nThis automation accelerates his research, completing in days\nwhat would have taken months manually. The extracted data\nis structured and stored in MongoDB, allowing Thomas to\neasily query, analyze, and identify trends in alloy synthesis,\nuncovering insights that manual review might have missed.\nC. Further Dataset Updates and Refinement\nAs Thomas advances in his research, he updates his dataset\nwith ByteScience, uploading new papers and processing them\nthrough the fine-tuned model to continually enrich his dataset.\nWhen discrepancies or improvements are needed, he initiates\na re-training cycle, reviewing and correcting a subset of the\nnewly processed papers to further fine-tune the model. This\niterative process ensures the model stays accurate and adapts to\nevolving terminologies or methods in alloy synthesis. Through\nthis dynamic interaction, Thomas maintains an up-to-date,\naccurate dataset, enhancing his research and keeping him at\nthe forefront of alloy synthesis advancements.\nVI. SIGNIFICANCE TO SCIENCE\nConstructing databases from scholarly literature is cru-\ncial for modern research, but traditional methods are time-\nconsuming and resource-intensive. ByteScience transforms\nthis process by enabling users to create a customized data\nextraction tool in hours, achieving 80%-90% human accuracy.\nIt can process a 10-page scientific document in one second,\ncompared to the 20-30 minutes it takes a researcher. With an\nextraction cost of just $0.023 per paper for 10,000 articles,\nByteScience makes large-scale data extraction affordable and\naccessible. Its versatility across scientific fields democratizes\naccess to advanced data extraction, providing computational\npower equivalent to hundreds of annotators. This accelerates\ndiscovery, enhances research decision-making, and fosters\ninnovation across disciplines.\nVII. CONCLUSION\nByteScience is leveraging a powerful approach to handle\nunstructured text by fine-tuning DARWIN, a pre-trained nat-\nural science LLM, using a minimal set of annotated articles.\nHosted on the AWS cloud, this platform automates the process\nof extracting structured data from scientific texts, presenting a\nzero-code solution that could significantly enhance efficiency\nin natural science research. The key advantage of ByteScience\nlies in its ability to train the DARWIN model with few\nannotations, making it exceptionally adaptive and efficient.\nThis capability ensures that the extracted material data is\nhigh-quality and highly accurate. ByteScience exemplifies how\ncutting-edge technology can be harnessed to propel advance-\nments in science, engineering, and research by integrating\nadvanced NLP techniques with cloud computing. This ini-\ntiative represents a substantial step forward in making vast\nscientific corpora more accessible and usable, highlighting the\ntransformative potential of AI in scientific data processing.\nTo optimize resource efficiency, we are developing a slicing\nversion that fine-tunes a low-resource inference model using\nonly partial data from extensive content.\nREFERENCES\n[1] R. Stevens, V. Taylor, J. Nichols, A. B. Maccabe, K. Yelick,\nand D. Brown, “AI for Science: Report on the Department of\nEnergy\n(DOE)\nTown\nHalls\non\nArtificial\nIntelligence\n(AI)\nfor\nScience,”\nArgonne\nNational\nLab.\n(ANL),\nArgonne,\nIL\n(United\nStates),\nTech.\nRep.\nANL-20/17,\nFeb.\n2020.\n[Online].\nAvailable:\nhttps://www.osti.gov/biblio/1604756\n[2] J. Vamathevan, D. Clark, P. Czodrowski, I. Dunham, E. Ferran,\nG. Lee, B. Li, A. Madabhushi, P. Shah, M. Spitzer, and S. Zhao,\n“Applications of machine learning in drug discovery and development,”\nNature Reviews Drug Discovery, vol. 18, no. 6, pp. 463–477,\nJun. 2019, publisher: Nature Publishing Group. [Online]. Available:\nhttps://www.nature.com/articles/s41573-019-0024-5\n[3] “Machine\nlearning\nfor\nfunctional\nprotein\ndesign\n|\nNature\nBiotechnology.” [Online]. Available: https://www.nature.com/articles/\ns41587-024-02127-0\n[4] T. Xie, Y. Wan, H. Wang, I. Østrøm, S. Wang, M. He, R. Deng,\nX. Wu, C. Grazian, C. Kit, and B. Hoex, “Opinion Mining by\nConvolutional Neural Networks for Maximizing Discoverability of\nNanomaterials,” Journal of Chemical Information and Modeling,\nvol. 64, no. 7, pp. 2746–2759, Apr. 2024. [Online]. Available:\nhttps://pubs.acs.org/doi/10.1021/acs.jcim.3c00746\n[5] A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards, S. Dacek,\nS. Cholia, D. Gunter, D. Skinner, G. Ceder, and others, “Commentary:\nThe Materials Project: A materials genome approach to accelerating\nmaterials innovation,” APL materials, vol. 1, no. 1, 2013, publisher:\nAIP Publishing.\n[6] C. Draxl and M. Scheffler, “The NOMAD laboratory: from data sharing\nto artificial intelligence,” Journal of Physics: Materials, vol. 2, no. 3, p.\n036001, 2019, publisher: IOP Publishing.\n[7] V. Venugopal, S. Pai, and E. Olivetti, “MatKG: The Largest Knowledge\nGraph in Materials Science – Entities, Relations, and Link Prediction\nthrough Graph Representation Learning,” Oct. 2022, arXiv:2210.17340\n[cond-mat]. [Online]. Available: http://arxiv.org/abs/2210.17340\n[8] T. Xie, Y. Wan, W. Huang, Z. Yin, Y. Liu, S. Wang, Q. Linghu,\nC. Kit, C. Grazian, W. Zhang, and others, “DARWIN Series: Domain\nSpecific Large Language Models for Natural Science,” arXiv preprint\narXiv:2308.13565, 2023.\n[9] Y. Shinyama, “Pdfminer: Python pdf parser and analyzer,” Retrieved on,\nvol. 11, 2015.\n[10] A. Dunn, J. Dagdelen, N. Walker, S. Lee, A. S. Rosen, G. Ceder,\nK. Persson, and A. Jain, “Structured information extraction from\ncomplex scientific text with fine-tuned large language models,” 2022,\narXiv: 2212.05238. [Online]. Available: http://arxiv.org/abs/2212.05238\n\nAPPENDIX\nFigure 3 shows the label-defining function in our system.\nUsers define the structure for annotations, including entity\nlabels and their relationships (visualized by indent). For each\nlabel, there is a definition textbox for filling.\nFigure 4 shows the annotation function. Users can use auto-\nmatic pre-labelling to the selected texts. Different colors will\nvisualize the auto-labeled annotations and users can review\nand correct them, ensuring accuracy and consistency.\nFigure 5 shows the data extraction function on example\npaper. Users can create a customized data extraction tool that\nachieves 80%-90% of human extraction accuracy after just a\nfew hours of annotation.\nFig. 3. Screenshot of label setup.\nFig. 4. Screenshot of labeling page.\nFig. 5. Screenshot of extraction results of a paper.",
    "pdf_filename": "ByteScience_Bridging_Unstructured_Scientific_Literature_and_Structured_Data_with_Auto_Fine-tuned_Lar.pdf"
}