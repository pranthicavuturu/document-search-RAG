{
    "title": "HEIGHT Heterogeneous Interaction Graph Transformer for Robot Navigation in Crowded and Constrained E",
    "abstract": "and interactive crowds with environmental constraints such as corridors and furniture. Previous methods fail to consider all types of interactions among agents and obstacles, leading to unsafe and inefficient robot paths. In this article, we leverage a graph-based representation of crowded and constrained scenarios and propose a structured framework to learn robot navigation policies with deep reinforcement learning. We first split the representations of different components in the environment, and propose a heterogeneous spatio-temporal graph to model distinct interactions among humans, robots, and obstacles. Based on the heterogeneous st-graph, we propose HEIGHT, a novel navigation policy network architecture with different components to capture heterogeneous interactions among entities through space and time. HEIGHT utilizes attention mechanisms to prioritize important interactions and a recurrent network to track changes in the dynamic scene over time, encouraging the robot to avoid collisions adaptively. Through extensive simulation and real-world experiments, we demonstrate that HEIGHT outperforms state-of-the-art baselines in terms of success and efficiency in challenging navigation scenarios. Furthermore, we demonstrate that our pipeline achieves better zero-shot gen- eralization capability than previous works when the densities of humans and obstacles change. More videos are available at https://sites.google.com/view/crowdnav-height/home. I. INTRODUCTION Robots are increasingly prevalent in human-centric environ- ments. In applications such as last-mile delivery and household robots, the ability to navigate among humans is crucial. For example, Fig. 1 shows a navigation scenario with abundant subtle interactions: Obstacles have a one-way effect on the paths of agents (i.e. humans and the robot), while the influence among agents is mutual. Among agents, humans may react to other humans and robots in different ways. To navigate, a robot directly participates in some interactions in its close proximity, and simultaneously, is indirectly affected by other interactions. These interactions are heterogeneous, dynamic, and difficult to infer, making navigation in such environments challenging. Rising to these challenges, previous works have explored various approaches for robot crowd navigation [1]–[3]. How- ever, these works typically have one of two limitations: (1) S. Liu is with the Department of Computer Science at The University of Texas at Austin. Email: shuijing.liu@utexas.edu H. Xia, F. Cheraghi Pouria, K. Hong, N. Chakraborty, and K. Driggs- Campbell are with the Department of Electrical and Computer Engineering at the University of Illinois at Urbana-Champaign. Emails: { hx17, fatemeh5, kaiwen2, neeloyc2, krdc}@illinois.edu This material is based upon work supported by the National Science Foundation under Grant No. 2143435. Fig. 1: A heterogeneous graph aids spatio-temporal reasoning when a robot navigates in a crowded and constrained environment. The colored arrows denote robot-human (RH), human-human (HH), and obstacle-agent (OA) interactions. The opaque arrows are the more important interactions while the transparent arrows are the less important ones. At each timestep t, the robot reasons about these interactions, focuses on the important ones, and makes decisions. They assume agents move in an open space without obstacles, which are common in the real-world [3]–[5]; (2) They do not differentiate between various types of interactions, and thus the robot has difficulties taking adaptive strategies to avoid collisions with humans and obstacles [1], [6]–[8]. Our goal is to navigate a robot to a destination without colliding with humans and obstacles. To solve this problem, we ask the following research question: How can a robot reason about diverse interactions in crowded and constrained arXiv:2411.12150v1  [cs.RO]  19 Nov 2024",
    "body": "HEIGHT: HEterogeneous Interaction GrapH\nTransformer for Robot Navigation in\nCrowded and Constrained Environments\nShuijing Liu, Haochen Xia, Fatemeh Cheraghi Pouria, Kaiwen Hong,\nNeeloy Chakraborty, and Katherine Driggs-Campbell\nAbstract—We study the problem of robot navigation in dense\nand interactive crowds with environmental constraints such as\ncorridors and furniture. Previous methods fail to consider all\ntypes of interactions among agents and obstacles, leading to\nunsafe and inefficient robot paths. In this article, we leverage a\ngraph-based representation of crowded and constrained scenarios\nand propose a structured framework to learn robot navigation\npolicies with deep reinforcement learning. We first split the\nrepresentations of different components in the environment,\nand propose a heterogeneous spatio-temporal graph to model\ndistinct interactions among humans, robots, and obstacles. Based\non the heterogeneous st-graph, we propose HEIGHT, a novel\nnavigation policy network architecture with different components\nto capture heterogeneous interactions among entities through\nspace and time. HEIGHT utilizes attention mechanisms to\nprioritize important interactions and a recurrent network to\ntrack changes in the dynamic scene over time, encouraging the\nrobot to avoid collisions adaptively. Through extensive simulation\nand real-world experiments, we demonstrate that HEIGHT\noutperforms state-of-the-art baselines in terms of success and\nefficiency in challenging navigation scenarios. Furthermore, we\ndemonstrate that our pipeline achieves better zero-shot gen-\neralization capability than previous works when the densities\nof humans and obstacles change. More videos are available at\nhttps://sites.google.com/view/crowdnav-height/home.\nI. INTRODUCTION\nRobots are increasingly prevalent in human-centric environ-\nments. In applications such as last-mile delivery and household\nrobots, the ability to navigate among humans is crucial. For\nexample, Fig. 1 shows a navigation scenario with abundant\nsubtle interactions: Obstacles have a one-way effect on the\npaths of agents (i.e. humans and the robot), while the influence\namong agents is mutual. Among agents, humans may react to\nother humans and robots in different ways. To navigate, a robot\ndirectly participates in some interactions in its close proximity,\nand simultaneously, is indirectly affected by other interactions.\nThese interactions are heterogeneous, dynamic, and difficult to\ninfer, making navigation in such environments challenging.\nRising to these challenges, previous works have explored\nvarious approaches for robot crowd navigation [1]–[3]. How-\never, these works typically have one of two limitations: (1)\nS. Liu is with the Department of Computer Science at The University of\nTexas at Austin. Email: shuijing.liu@utexas.edu\nH. Xia, F. Cheraghi Pouria, K. Hong, N. Chakraborty, and K. Driggs-\nCampbell are with the Department of Electrical and Computer Engineering\nat the University of Illinois at Urbana-Champaign. Emails: { hx17, fatemeh5,\nkaiwen2, neeloyc2, krdc}@illinois.edu\nThis material is based upon work supported by the National Science\nFoundation under Grant No. 2143435.\nFig. 1: A heterogeneous graph aids spatio-temporal reasoning when a\nrobot navigates in a crowded and constrained environment. The colored\narrows denote robot-human (RH), human-human (HH), and obstacle-agent\n(OA) interactions. The opaque arrows are the more important interactions\nwhile the transparent arrows are the less important ones. At each timestep t,\nthe robot reasons about these interactions, focuses on the important ones, and\nmakes decisions.\nThey assume agents move in an open space without obstacles,\nwhich are common in the real-world [3]–[5]; (2) They do not\ndifferentiate between various types of interactions, and thus\nthe robot has difficulties taking adaptive strategies to avoid\ncollisions with humans and obstacles [1], [6]–[8].\nOur goal is to navigate a robot to a destination without\ncolliding with humans and obstacles. To solve this problem,\nwe ask the following research question: How can a robot\nreason about diverse interactions in crowded and constrained\narXiv:2411.12150v1  [cs.RO]  19 Nov 2024\n\nenvironments to adaptively avoid collisions during navigation?\nTo address this question, we propose a framework that takes\nadvantage of the heterogeneity of interactions in crowded\nand constrained scenarios. First, we split the environment\ninto human and obstacle representations, which are processed\nand fed separately into the reinforcement learning (RL)-based\nnavigation pipeline. Then, we decompose the scenario into a\nheterogeneous spatio-temporal (st) graph with different types\nof edges to represent different types of interactions among\nthe robot, humans, and obstacles, as shown in the colored\narrows in Fig. 1. Finally, we convert the heterogeneous st-\ngraph into a HEterogeneous Interaction GrapH Transformer\n(HEIGHT), a robot policy network consisting of different\nmodules to parameterize the various spatio-temporal interac-\ntions. Specifically, we use two separate multi-head attention\nnetworks to address the different effects of robot-human (RH)\nand human-human (HH) interactions. The attention networks\nenable the robot to pay more attention to the important\ninteractions, leading to a low collision-rate even as the number\nof humans increases, and the graph becomes more complex. In\naddition, we use a multilayer perceptron (MLP) to model the\nsingle-directional obstacle-agent interactions and a recurrent\nnetwork for temporal evolution of the scene. In response to the\nrapidly changing scenario (Fig. 1 bottom), HEIGHT captures\nthe heterogeneous interactions among different components\nthrough space and time, enabling the robot to avoid collisions\nand approach its goal in an efficient manner.\nThis article is expanded from a contribution on attention\ngraph network proposed by our previous work [9]. While\nour previous work focuses on crowd navigation in open\nspaces, this article incorporates static obstacles and constraints,\nwhich leads to significant modifications of scene representa-\ntion and network architecture. Corresponding to these changes\nin methodology, new simulation and hardware experiments\nwith new baseline comparisons are performed. In summary,\nthe main contributions of this article are as follows.\n1) We propose an input representation of crowded and\nconstrained environments that treats humans and obsta-\ncles differently. The split scene representation naturally\nallows us to inject structures in the rest of the framework.\n2) We propose a structured graph representation of crowded\nand constrained scenarios, named heterogeneous spatio-\ntemporal graph (st-graph), to effectively model the pair-\nwise interactions among all agents and entities.\n3) From the heterogenous st-graph, we use a principle\napproach to derive HEIGHT, a transformer-based robot\nnavigation policy network with different modules to rea-\nson about all types of spatial and temporal interactions.\n4) The experiments in simulation with dense crowds and\ndense obstacles demonstrate that our method outper-\nforms previous state-of-the-art methods in unseen obsta-\ncle layouts. In addition, our method demonstrates better\ngeneralization to out-of-distribution environments with\ndifferent human and obstacle densities.\n5) We successfully transfer the robot policy learned in a\nlow-fidelity simulator to challenging real-world, every-\nday crowded environments without finetuning.\nII. RELATED WORKS\nIn this section, we first review the literature of robot crowd\nnavigation, which is divided into model-based and learning-\nbased approaches. Then, we discuss previous crowd naviga-\ntion efforts in constrained spaces. Finally, we review graph\nattention mechanism with a focus on the usage of attention\nnetworks in multi-agent interaction modeling.\nA. Model-based methods\nRobot navigation in human crowds is particularly challeng-\ning and has been studied for decades [1], [10]–[12]. Model-\nbased approaches have explored various mathematical models\nto optimize robot actions [1], [2], [13]–[16]. As an early\nexample, ROS navigation stack [17] uses a cost map for\nglobal planning and dynamic window approach (DWA) for\nlocal planning [1]. DWA searches for the optimal velocity\nthat brings the robot to the goal with the maximum clearance\nfrom any obstacle while obeying the dynamics of the robot. By\ntreating humans as obstacles, DWA exhibits myopic behaviors\nsuch as oscillatory paths in dynamic environments [18]. As a\nstep forward, optimal reciprocal collision avoidance (ORCA)\nand social force (SF) account for the velocities of agents.\nORCA models other agents as velocity obstacles and assumes\nthat agents avoid each other under the reciprocal rule [2],\n[13]. SF models the interactions between the robot and other\nagents using attractive and repulsive forces [14]. However, the\nhyperparameters of the model-based approaches are sensitive\nto crowd behaviors and thus need to be tuned carefully\nto ensure good performance [18], [19]. In addition, model-\nbased methods are prone to failures, such as the freezing\nproblem, if the assumptions such as the reciprocal rule are\nbroken [20], [21]. In contrast, while our method also models\nthese interactions, we learn the hyperparameters of the model\nfrom trial and error with minimal assumptions on human\nbehaviors posed by model-based methods.\nB. Learning-based methods\nLearning-based approaches have been widely used for nav-\nigation in dynamic environments to reduce hyperparameter\ntuning efforts and the number of assumptions. One example\nis supervised learning from expert demonstrations of desired\nbehaviors, where the expert ranges from model-based poli-\ncies [22]–[24], human teleoperators in simulators [25], to real\npedestrians [26], [27]. Supervised learning does not require\nexplorations of the state and action spaces of the robot, yet\nthe performance of learned policy is limited by the quality of\nexpert demonstrations.\nAnother line of work takes advantage of crowd simulators\nand learns policies with RL. Through trial and error, RL\nhas the potential to learn robot behaviors that outperform\nmodel-based approaches and expert demonstrations [28]. For\nexample, Deep V-Learning first uses supervised learning with\nORCA as the expert and then uses RL to learn a value\nfunction for path planning [3]–[5], [29], [30]. However, Deep\nV-Learning assumes that state transitions of all agents are\nknown without uncertainty. In addition, since the networks\n\nare pre-trained with supervised learning, they share the same\ndisadvantages with OCRA, which are hard to correct by\nRL [8]. To address these problems, more recent efforts lever-\nage model-free RL without supervised learning or assumptions\non state transitions [5], [8], [21], [31]. Since the state transition\nprobability of humans is uncertain, directly learning a policy\nnetwork without explicitly modeling state transitions is more\nsuitable for navigation [32]. However, the aforementioned RL\nworks have at least one of the following two problems: (1)\nThey focus on navigation in open spaces and isolate agents\nfrom static constraints, posing difficulties when deploying\nrobots in the real-world; (2) They ignore all or part of interac-\ntions among agents, which are important for robot navigation\nin dense crowds and highly constrained environments. We\ndiscuss how prior works address the above two problems in\nSec. II-C and Sec. II-D respectively.\nC. Crowd navigation in constrained environments\nBesides dynamic agents, real-world navigation environ-\nments usually consist of static constraints, such as walls,\nfurniture, and untraversable terrains. To deal with these con-\nstraints and humans, some methods such as DWA [1] and DS-\nRNN [8] use groups of small circles to represent the contours\nof obstacles. While groups of circles is a straightforward\nrepresentation of obstacles, as we will show in Sec. V, they\nare not scalable as the number of obstacles increases and can\nlead to overfitting problems for learning-based approaches.\nOther learning-based approaches use raw sensor images or\npoint clouds to represent the whole environment [7], [25], [33],\n[34]. These end-to-end (e2e) pipelines have made promising\nprogress in simulation. However, generalizing these e2e meth-\nods to real-world scenarios is challenging due to domain gaps\nsuch as inaccurate human gait simulation [35], [36]. Despite\nthe recent advancements in crowd navigation simulators [37]–\n[39] and datasets [26], [40], [41], learning a deployable e2e\npolicy that outperforms human teleoperation in dense and\ninteractive crowds remains an open challenge.\nFurthermore, prior works that demonstrate strong real-world\nperformance in dense crowds and obstacles usually leverage\nprocessed inputs such as detected human states and processed\nsensor readings [28], [32], [42]. As such, we develop a\nstructured input representation of humans and obstacles, which\nsplits human and obstacle representations and feeds processed\nstates to the policy network. This input representation leads\nto strong performance in both simulation and real-world. In\ncontrast to the above works with processed inputs that focus\non pedestrian behavior prediction [32], reward design [28],\nand incorporating risks into map [42], our paper proposes\na principled way for network architecture design from the\nstructures of complex navigation scenarios.\nD. Graph attention networks for multi-agent interactions\nIn recent years, attention mechanisms have demonstrated\nsuccess in various fields [43]–[45]. Vaswani et al. propose\na transformer with self-attention mechanism that achieves\nstate-of-the-art performance in machine translation [43]. Later,\ngraph attention networks show the effectiveness of attention\non learning relationships of data with graphical structures\nsuch as citation networks [44]. Inspired by these works,\nresearchers in trajectory prediction and crowd navigation have\nfound that attention networks are also well-suited to capture\ninteractions amongst agents and entities, which contain es-\nsential information for multi-agent tasks [3], [8], [45]–[47].\nFor each agent, these works compute attention scores for\nall neighboring agents. Due to the permutation invariance\nproperty, attention scores better capture the importance of pair-\nwise relationships than combining the information of all agents\nwith concatenation or an LSTM encoder [4], [29].\nMore specifically, in crowd navigation of robots and au-\ntonomous vehicles, a line of works uses a robot-human at-\ntention network to determine the relative importance of each\nhuman to the robot [3], [8], [48], [49]. However, interactions\namong humans, which can also influence the robot, are not\nexplicitly modeled. To this end, other works use a homogenous\ngraph network to include both RH and HH interactions [6],\n[50]. However, since these works feed RH and HH features\nto a single attention network, the resulting robot policy has\ndifficulty reasoning the specificity of each type of feature,\nwhich limits the robot’s ability to adapt to different interac-\ntions, as demonstrated in [51] and our experiments in Sec. V.\nIn addition, the works above only deal with open-world social\nnavigation and thus ignore the interactions between agents\nand obstacles. To this end, Chen et al. treat the humans,\nthe robot, and static objects as different types of nodes in\na heterogeneous graph attention network [51].\nThe main difference from our work is that Chen et al. focus\non semantic navigation, where a robot must navigate to an\nobject in simulation. Our work focuses on point-goal naviga-\ntion in crowded and constrained real-world environments. This\ndifference leads to (1) Different representations of obstacles:\nthe object representation in Chen et al. comes from a known\nsemantic map of the environment, which is hard to obtain in\nthe real-world. Thus, in our case, treating all obstacles as a 2D\npoint cloud is more suitable for studying collision avoidance;\n(2) Chen et al. learn a value function and plan paths assuming\nsimplified dynamics for agents, whereas we learn a model-\nfree RL policy and assume unknown state transition for all\nagents, which is more suitable for sim2real transfer where the\ndynamics of pedestrians and robots are uncertain.\nIII. PRELIMINARIES\nIn this section, we formulate constrained crowd navigation\nas a Markov Decision Process (MDP) and introduce the scene\nrepresentation and the reward function.\nA. MDP formulation\nWe model the constrained crowd navigation scenario as a\nMDP, defined by the tuple ⟨S, A, P, R, γ, S0⟩. Let wt be the\nrobot state which consists of the robot’s position (px, py),\nvelocity (ux, uy), goal position (gx, gy), and heading angle θ.\nLet ht\ni be the current state of the i-th human at time t, which\nconsists of the human’s position and velocity (pi\nx, pi\ny, ui\nx, ui\ny).\nLet ot be the current observation of the static obstacles and\nwalls, which is represented as a 2D point cloud. We define the\n\nFig. 2: An overview of our pipeline in simulation and real-world. (a) At each timestep t in training and testing, the simulator provides a reward rt and\nthe following observations of the environment: obstacle point cloud ot, the robot state wt, and the human states ht\n1, ..., ht\nn, and masks Mt (Sec. IV-B1).\nThese observations serve as inputs to HEIGHT, which outputs a robot action at that maximizes the future expected return Rt. The simulator executes the\nactions of all agents and the loop continues. (b) The testing loop in the real-world is similar to the simulator except the perception modules for obtaning the\nobservations are different and the reward is absent.\nstate st ∈S of the MDP as st = [wt, ot, ht\n1, ..., ht\nn] if a total\nnumber of n humans are observed at the timestep t, where n\nmay change within a range in different timesteps. The humans\nare sorted by an increasing L2 distance to the robot. The state\nspace S is continuous and our choice of state representation\nis expanded in Sec. III-B.\nIn each episode, the robot begins at an initial state s0 ∈S0.\nAs shown in Fig. 2(a), according to its policy π(at|st),\nthe robot takes an action at\n∈\nA at each timestep t.\nThe action of the robot consists of the desired transla-\ntional and rotational accelerations at = [at\ntrans, at\nrot]. The\naction space A is discrete: the translational acceleration\natrans ∈{−0.05 m/s2, 0 m/s2, 0.05 m/s2} and the rotational\nacceleration arot\n∈\n{−0.1 rad/s2, 0 rad/s2, 0.1 rad/s2}.\nThe translational and rotational velocity is clipped within\n[−0.5 m/s, 0.5 m/s] and [−1 rad/s, 1 rad/s] respectively.\nThe robot motion is governed by the dynamics of TurtleBot\n2i. In return, the robot receives a reward rt (see Sec. III-C\nfor details) and transits to the next state st+1 according to\nan unknown state transition P(·|st, at). Meanwhile, all other\nhumans also take actions according to their policies and move\nto the next states with unknown state transition probabilities.\nThe process continues until the robot reaches its goal, t\nexceeds the maximum episode length T = 491 steps, or the\nrobot collides with any humans or static obstacles.\nThe goal of the robot is to maximize the expected return,\nRt = E[PT\ni=t γi−tri], where γ is a discount factor. The value\nfunction V π(s) is defined as the expected return starting from\ns, and successively following policy π.\nB. State representation\nIn our MDP, a state consists of a large and varying number\nof agents. To aid policy learning with such a complicated state\nspace, we develop a structured representation of states that will\nbe fed into the structured policy network. To reduce sim2real\ngaps caused by raw sensor readings, our scene representation\nleverages processed information from perception, maps, and\nrobot localization, which are relatively robust to domain shifts.\nIn Fig. 3, at each timestep t, we split a scene into a human\nrepresentation ht\n1, ..., ht\nn, and an obstacle representation ot.\nIn human representation (Fig. 3(b)), the position and veloc-\nity of each human are detected using off-the-shelf human\nFig. 3: A split representation of a crowded and constrained navigation\nscenario. In a dynamic scene, (b) low-level human states are detected by\nsensors and perception modules. (c) To obtain obstacle information, we\nremove all humans and compute a point cloud from a known map and the\nrobot’s location. In this way, we can learn a robot policy with a small sim2real\ngaps using a cheap low-fidelity simulator.\ndetectors [52]–[54]. By representing each human as a low-\ndimentional state vector, we abstract away detailed information\nsuch as gaits and appearance, which are difficult to simulate\naccurately [35], [37]. We use a 2D point cloud as the obstacle\nrepresentation. Instead of obtaining the point cloud from\nsensors, we take advantage of simultaneous localization and\nmapping (SLAM) and first create a map of the environment.\nDuring navigation, in Fig. 3(c), assuming robot localization on\nthe map is known, an “artificial” point cloud that only contains\nstatic obstacles is computed from the map. Compared to real-\ntime point clouds from sensors, our obstacle representation\nis not affected by the presence of humans and thus is less\nsensitive to inaccurate human simulations. The tradeoff is that\nour method requires a map and accurate localization. If a high-\nfidelity simulator is available, the “artificial” point cloud can\nbe replaced by a real-time point cloud from sensors without\nchanging the rest of our method described below.\nC. Reward function\nOur reward function consists of three parts. The first and\nmain part of the function awards the robot for reaching its goal\n\nFig. 4: The heterogeneous st-graph and the HEIGHT network architecture. (a) Graph representation of crowd navigation. The robot node is w (pink),\nthe i-th human node is hi (white), and the obstacle node is o (yellow). HH edges and HH functions are in blue, OA edges and OA functions are in orange,\nand RH edges and RH functions are in red. The temporal function is in purple. (b) HEIGHT network. Two attention mechanisms are used to model the HH\nand RH interactions. We use MLPs and a concatenation for obstacle-agent interactions, and a GRU for the temporal function. The superscript t that indicates\nthe timestep and the human mask M is eliminated for clarity.\nand penalizes the robot for colliding with or getting too close\nto humans or obstacles. In addition, we add a potential-based\nreward to guide the robot to approach the goal:\nrmain(st, at) =\n\n\n\n\n\n\n\n\n\n20,\nif dt\ngoal ≤ρrobot\n−20,\nelse if dt\nmin ≤0\ndt\nmin −0.25,\nelse if 0 < dt\nmin < 0.25\n4(dt−1\ngoal −dt\ngoal),\notherwise.\n(1)\nwhere ρrobot is the radius of the robot, dt\ngoal is the L2\ndistance between the robot and its goal at time t, and dt\nmin is\nthe minimum distance between the robot and any human or\nobstacle at time t. The second part is a spinning penalty rspin\nto penalize high rotational velocity:\nrspin(st, at) = −0.05||ωt||2\n2\n(2)\nwhere ωt is the current rotational velocity of the robot. Finally,\nwe add another small constant penalty rtime = −0.025 at each\ntimestep to encourage the robot to finish the episode as soon\nas possible.\nIn summary, the reward function of our MDP is the sum of\nthe above three parts:\nr(st, at) = rmain(st, at) + rspin(st, at) + rtime\n(3)\nIntuitively, the robot gets a high reward when it approaches\nthe goal with a high speed and a short and smooth path, while\nmaintaining a safe distance from dynamic and static obstacles.\nIV. METHODOLOGY\nIn this section, we present our approach to decompose the\nconstrained crowd navigation scenario as a heterogeneous st-\ngraph, which leads to the derivation of the HEIGHT architec-\nture in a structured manner.\nA. Heterogeneous Spatio-Temporal Graph\nThe subtle yet highly dynamic interactions among agents\nand entities are important factors that makes crowd navigation\ndifficult. To model these interactions in a structured manner,\nwe formulate the navigation scenario as a heterogeneous st-\ngraph. In Fig. 4a, at each timestep t, our heterogeneous st-\ngraph Gt = (Vt, Et) consists of a set of nodes Vt and a set of\nedges Et. The nodes include the detected humans ht\n1, ..., ht\nn\nand the robot wt. In addition, an obstacle node ot represents\nthe point cloud of all obstacles as a whole. At each timestep\nt, the spatial edges that connect different nodes denote the\nspatial interactions among nodes. Different spatial interactions\nhave different effects on robot decision-making. Specifically,\nsince we have control of the robot but not the humans, RH\ninteractions have direct effects while HH interactions have\nindirect effects on the robot actions. As an example of indirect\neffects, if human A aggressively forces human B to turn toward\nthe robot’s front, the robot has to respond as a result of the\ninteraction between A and B. Additionally, since the agents\nare dynamic but the obstacles are static, interactions among\nagents are mutual while the influence of static obstacles on\nagents is one-way. Thus, we categorize the spatial edges into\nthree types: HH edges (blue in Fig. 4), obstacle-agent (OA)\nedges (orange), and RH edges (red). The three types of edges\nallow us to factorize the spatial interactions into HH, OA,\nand RH functions. Each function is parameterized by a neural\nnetwork that has learnable parameters. Compared to previous\nworks that ignore some edges [3], [8], [9], our method allows\nthe robot to reason about all observed spatial interactions that\nexist in constrained and crowded environments.\nSince the movements of all agents cause the visibility of\neach human to change dynamically, the set of nodes Vt and\nedges Et and the parameters of the interaction functions may\nchange correspondingly. To this end, we integrate the temporal\ncorrelations of the graph Gt at different timesteps using\nanother function denoted by the purple box in Fig. 4(a). The\ntemporal function connects the graphs at adjacent timesteps,\nwhich overcomes the short-sightedness of reactive policies and\nenables long-term decision-making of the robot.\nTo reduce the number of parameters, the same type of edges\nin Fig. 4(a) share the same function parameters. This parameter\nsharing is important for the scalability of our graph because\nthe number of parameters is kept constant when the number\nof human changes [55].\n\nB. HEIGHT Architecture\nIn Fig. 4b, we derive our network architecture from the het-\nerogeneous st-graph. We represent the HH and RH functions\nas feedforward networks with attention mechanisms, referred\nto as HH attn and RH attn respectively. We represent the\nOA function as an MLP with concatenation, and the temporal\nfunction as a gated recurrent unit (GRU). We use W and f to\ndenote trainable weights and fully connected layers.\n1) Attention among agents: The attention modules assign\nweights to all edges that connect to a robot or a human\nnode, allowing the node to pay attention to important edges\nor interactions. The two attention networks are similar to the\nscaled dot-product attention with a padding mask [43], which\ncomputes the attention score using a query Q and a key K,\nand applies the normalized score to a value V , which results\nin a weighted value v.\nv := Attn(Q, K, V, M) = softmax\n\u0012(QK⊤+ M)\n√\nd\n\u0013\nV\n(4)\nwhere d is the dimension of the queries and keys and acts as\na scaling factor. The mask M is used to handle the changing\nnumber of detected humans at each timestep, as we will\nexpand below.\nHuman-human attention: To learn the importance of each\nHH edge to the robot decision at time t, we first weigh each\nobserved human w.r.t. other humans using an HH attention\nnetwork, which is a self-attention among humans. In HH atten-\ntion, the current states of humans ht\n1, ..., ht\nn are concatenated\nand passed through linear layers with weights W Q\nHH, W K\nHH,\nand W V\nHH to obtain Qt\nHH, Kt\nHH, V t\nHH ∈Rn×dHH, where\ndHH is the attention size for the HH attention.\nQt\nHH = [ht\n1, ..., ht\nn]⊤W Q\nHH = [q1, ..., qn]⊤\nKt\nHH = [ht\n1, ..., ht\nn]⊤W K\nHH = [k1, ..., kn]⊤\nV t\nHH = [ht\n1, ..., ht\nn]⊤W V\nHH = [v1, ..., vn]⊤\n(5)\nwhere qi ∈R1×dHH, ki ∈R1×dHH, and vi ∈R1×dHH are\nthe query embedding, key embedding, and value embedding\nof the i-th human, respectively.\nIn addition, following Eq. 4, a mask M t ∈Rn×n indicates\nthe visibility of each human to the robot at current time t and\nis obtained from the perception system of the robot. Assume\nthe n-th human is not visible at time t. Then M t is a matrix\nfilled with zeros, except that every entry in n-th column is\n−∞. The numerator of Eq. 4 can be express as\nQt\nHH ·\n\u0000Kt\nHH\n\u0001⊤+ M t\n=\n\n\nq1k1\n· · ·\nq1kn−1\nq1kn\n...\n...\n...\n...\nqnk1\n· · ·\nqnkn−1\nqnkn\n\n+\n\n\n0\n· · ·\n0\n−∞\n...\n...\n...\n...\n0\n· · ·\n0\n−∞\n\n\n=\n\n\nq1k1\n· · ·\nq1kn−1\n−∞\n...\n...\n...\n...\nqnk1\n· · ·\nqnkn−1\n−∞\n\n\n(6)\nLet V t\nHH = [v1, ..., vn]⊤, where vi ∈R1×dHH is the value\nembedding of the i-th human. Then, based on Eq. 4, the\nweighted human embeddings vt\nHH ∈Rn×dHH is\nvt\nHH = softmax\n \nQt\nHH(Kt\nHH)⊤+ M t\n√\nd\n!\n· V t\nHH\n=\n\n\nc1q1k1\n· · ·\nc1q1kn−1\n0\n...\n...\n...\n...\ncnqnk1\n· · ·\ncnqnkn−1\n0\n\n·\n\n\nv1\n...\nvn−1\nvn\n\n\n=\n\n\nc1q1k1v1 + · · · + c1q1kn−1vn−1\n...\ncnqnk1v1 + · · · + cnqnkn−1vn−1\n\n\n(7)\nwhere c1, ..., cn are constants that reflect the effect of the\nscaling factor d and the softmax function. Thus, the value of\nthe n-th missing human vn is eliminated by the mask M t and\nthus does not affect the resulting weighted human embedding\nvt\nHH. The mask that indicates the visibility of each human\nprevents attention to undetected humans, which is common in\ncrowd navigation due to the partial observability caused by\nthe limited robot sensor range, occlusions, imperfect human\ndetectors, etc [31]. Additionally, the mask provides unbiased\ngradients to the networks, which stabilizes and accelerates the\ntraining [9], [56].\nRobot-human attention: After the humans are weighted by\nHH attention, we weigh their embeddings again w.r.t. the robot\nwith another RH attention network to learn the importance\nof each RH edge. In RH attention, we first embed the robot\nstate wt with a fully connected layer, which results in the key\nfor RH attention Kt\nRH ∈R1×dRH. The query and the value,\nQt\nRH, V t\nRH ∈Rn×dRH, are the other two linear embeddings\nof the weighted human features from HH attention vt\nHH.\nQt\nRH = vt\nHHW Q\nRH, Kt\nRH = wtW K\nRH, V t\nRH = vt\nHHW V\nRH\n(8)\nWe compute the attention score from Qt\nRH, Kt\nRH, V t\nRH, and\nthe mask M t to obtain the weighted human features for the\nsecond time vt\nRH ∈R1×dRH as in Eq. 4.\n2) Incoporating obstacle and temporal information: We\nfirst feed the point cloud that represents obstacles, ot, into a\n1D-CNN followed by a fully connected layer to get an obstacle\nembedding vt\nO. Then, we embed the robot states wt with linear\nlayers fR to obtain a robot embedding vt\nR.\nvt\nO = fCNN(ot),\nvt\nR = fR(wt)\n(9)\nFinally, the robot and obstacle embeddings are concatenated\nwith the twice weighted human features vt\nRH and fed into the\nGRU1:\nht = GRU\n\u0000ht−1, ([vt\nRH, vt\nR, vt\nO])\n\u0001\n(10)\nwhere ht is the hidden state of GRU at time t. Finally, the ht\nis input to a fully connected layer to obtain the value V (st)\nand the policy π(at|st).\n1From experiments, we find that adding a third obstacle-agent attention\nnetwork, where the obstacle embedding is the key and the robot and human\nembeddings are the query and value, does not improve the performance. Thus,\nwe keep the simple concatenation design to incorporate obstacle information.\n\n3) Training: We train the entire network with Proximal\nPolicy Optimization (PPO) [57] in a simulator as shown in\nFig. 2(a) (see Sec. V-A for details of the simulator). At each\ntimestep t, the simulator provides all state information that\nconstitutes st, which is fed to the HEIGHT network. The\nnetwork outputs the estimated value of the state V (st) and the\nlogarithmic probabilities of the robot’s action π(at|st), both\nof which are used to compute the PPO loss and then update\nthe parameters in the network. In training, the robot action\nis sampled from the action distribution π(at|st). In testing,\nthe robot takes the action with the highest probability at. The\nrobot action at is fed into the simulator to compute the next\nstate st+1, and then the loop continues.\nWithout any supervised learning, our method is not limited\nby the performance of expert demonstrations [3], [4], [50].\nHowever, to improve the low training data efficiency, an\ninherent problem of RL, HEIGHT can also be trained with\na combination of imitation learning and RL.\n4) Summary: We present a structured and principled ap-\nproach to design the robot policy network for crowd navigation\nin constrained environments. By decomposing the complex\nscenario into independent components, we split the complex\nproblem into smaller functions, which are used to learn the\nparameters of the corresponding functions. By combining all\ncomponents above, the end-to-end trainable HEIGHT allows\nthe robot to perform spatial and temporal reasoning on all\npairwise interactions, leading to better navigation performance.\nV. SIMULATION EXPERIMENTS\nIn this section, we present our environment, experiment\nsetup, and results in simulation. Our experiments are guided by\nthe following questions: (1) What is the advantage of our split\nscene representation compared with alternative representa-\ntions? (2) What is the importance of the graph formulation and\nwhy do we differentiate types of edges with a heterogenous\nst-graph? (3) What is the importance of HH and RH attention\nin HEIGHT? (4) What are the failure cases of our method?\nA. Simulation environment\nWe conduct simulation experiments in random environment\nin Fig. 5(a) developed with PyBullet [58]. The robot, humans,\nand static obstacles are in a 12 m×12 m arena. In each episode,\nrectangular obstacles are initialized with random shapes and\nrandom poses. The width and the length of each static obstacle\nare sampled from N(1, 0.62) and are limited in [0.1, 5] meters.\nThe initial positions of the humans and the robot are also\nrandomized. The starting and goal positions of the robot are\nrandomly sampled inside the arena. The distance between the\nrobot’s starting and the goal positions is between 5 m and 6 m.\nSome humans are dynamic and some are static. The goals of\ndynamic humans are set on the opposite side of their initial\npositions to create circle-crossing scenarios. In training, the\nnumber of humans varies from 5 to 9 and the number of static\nrectangular obstacles varies from 8 to 12. Among all humans,\n0-2 of them are static and the rest are dynamic. In testing, the\nnumber of humans and obstacles are shown in the first column\nFig. 5: Simulation and real-world environments. (a) In our simulation\nbenchmark, the colored cylinders denote humans and gray objects denote\nobstacles. Each episode is randomized (Sec. V-A). (b) and (c) are everyday\nindoor environments for sim2real. Before real-world testing in (b) and (c), the\nrobot policy is trained in (d) and (e) respectively (Sec. VI-A).\nof Table I. The heights of humans and obstacles are all above\nthe height of the robot LiDAR to ensure detectability.\nTo simulate a continuous human flow similar to [8], [9],\n[28], dynamic humans will move to new random goals im-\nmediately after they arrive at their goal positions or they\nget stuck in front of narrow passageways for more than 10\ntimesteps. All dynamic humans are controlled by ORCA [13].\n80% of dynamic humans do not react to the robot and 20%\nof humans react to the robot. This mixed setting prevents\nour network from learning an extremely aggressive policy in\nwhich the robot forces all humans to yield while achieving\na high reward. Meanwhile, the simulation maintains enough\nreactive humans to resemble the real crowd behaviors. We\nuse holonomic kinematics for humans. The preferred speed\nof humans ranges from 0.4 m/s to 0.6 m/s to accommodate\nthe speed of the robot. We assume that humans can achieve\nthe desired velocities immediately, and they will keep moving\nwith these velocities for the next ∆t seconds. The simulation\nand control frequency ∆t is 0.1 s. The maximum length of an\nepisode is 491 timesteps or 49.1 s.\nB. Experiment setup\nWe now introduce the baselines and ablation models, train-\ning procedure, and evaluation metrics.\n1) Baselines: We compare the performance of our method\nwith the following baselines:\n• Dynamic window approach (DWA) [1] searches for the\noptimal velocity that brings the robot to the goal with\nthe maximum clearance from any obstacle. DWA is a\nmodel-based method that only considers the current state.\nIn addition, DWA represents both humans and obstacles\nas groups of small circles.\n• A∗+ CNN [7] is a hybrid method. With a map of the\nenvironment, A∗is the global planner and generates 6\nwaypoints in the beginning of an episode. The inputs\nto the robot RL policy are a 2D LiDAR point cloud,\n\nthe robot state, the waypoints, and the robot’s final goal.\nNo human detections are used and human features are\nincluded in the point cloud. The point cloud is passed\nthrough a CNN local planner, whose output is concate-\nnated with the embedding of other inputs. In addition to\nthe reward function in Eq. 1, the robot receives a small\nreward of 2 if it arrives at any waypoint in sequence.\n• Decentrialized structural RNN (DS-RNN) [8] is an RL-\nbased method that represents static obstacles as groups\nof small circles. In network architecture, DS-RNN only\ncontains the RH attention and the GRU.\n• Homogeneous\ngraph\nattention\nnetwork\n(Homo-\nGAT) [44] is RL-based and splits human and obstacle in-\nputs. However, HomoGAT does not differentiate between\n3 types of nodes and 3 types of edges in policy network.\nInstead, HomoGAT uses a single self-attention network\nto weigh humans, the robot, and the obstacle point cloud\nand feed the weighted sum of all embeddings into a GRU.\nHomoGAT is similar to Chen et al. [6] and Liu et al. [50]\nbut the input, output, and training algorithm are kept the\nsame as our method for a fair comparison.\nIn summary, DWA, A∗+ CNN, and DS-RNN mix humans\nand obstacles in both input representations and navigation\nalgorithm. HomoGAT only mix humans and obstacles in\nalgorithm but not in input, while ours distinguishes them in\nboth input and algorithm.\n2) Ablations: To show the benefits of each attention net-\nwork, we perform an ablation study on the two attention\nmodels. The ablated models are defined as follows:\n• No attn: Both RH and HH attention networks are re-\nmoved. No attn model only has the embedding layers for\nthe inputs and the GRU.\n• RH: The HH attention network is removed and the hu-\nmans are weighted only once w.r.t. the robot. Everything\nelse is the same as ours.\n• HH: The RH attention network is removed and the\nhumans are weighted only once w.r.t. other humans.\nEverything else is the same as ours.\n• RH+HH (ours): The full network as shown in Fig. 4(b).\n3) Training: We train all RL methods, including all ba-\nseines and ablations except DWA, for 2×108 timesteps with a\nlearning rate 5×10−5. The learning rate decays at a linear rate\nwith respect to training timesteps. To accelerate and stabilize\ntraining, we run 28 parallel environments to collect the robot’s\nexperiences. At each policy update, 30 steps of 6 episodes are\nused. We train and test all methods in a commercial desktop\ncomputer with an Intel Core i9-13900K processor, 32 GB\nmemory, and a NVIDIA RTX 4090 GPU. Training our method\ntakes approximately 48 hours.\n4) Evaluation: We test all methods with the same 500\nrandom unseen test episodes. For RL-based methods, we test\nthe last 5 checkpoints (equivalent to the last 6000 training\nsteps) and report the result of the checkpoint with the highest\nsuccess rate. We conduct the testing in 5 different human and\nobstacle densities, as shown in the first columns of Table I and\nTable II. The first set of densities is the same as the training\nenvironment and is thus in training distribution. To test the\ngeneralization of methods, we change the range of human or\nobstacle numbers in the remaining four environments to values\nthat do not overlap with those used in training. Thus, these 4\nenvironments are out-of-distribution (OOD).\nAll testing scenarios, including those in the training dis-\ntribution, are unseen during training. This is because in each\ntesting episode, a new seed that is not used in training deter-\nmines the number of humans and obstacles, the starting and\ngoal positions of all agents, and the size and pose of obstacles.\nOnly the 4 arena walls do not change across different episodes.\nThe testing metrics measure the quality of the navigation\nand include the success rate, collision rate with humans and\nobstacles, timeout rate, the average navigation time of suc-\ncessful episodes in seconds, and path length of the successful\nepisodes in meters.\nC. Results\n1) Effectiveness of scene representation: To analyze the\neffects of input scene representations on crowd navigation\nalgorithms, we compare ours and HomoGAT, the two methods\nthat distinguish human and obstacle inputs, with baselines that\nmix humans and obstacles in input representation: the model-\nbased DWA, the RL-based DS-RNN, and the hybrid planner\nA∗+ CNN. The results are shown in Table I.\nFor DWA, treating humans as obstacles leads to the highest\naverage human collision rates (0.54) and a freezing problem,\nindicated by the highest average timeout rates (0.21), in all\nenvironments. For example, the robot in Fig. 6 stays close to\neverything and thus fails to avoid the magenta human in time.\nSimilarly, by representing obstacles as groups of circles,\nDS-RNN performs better in the More crowded environ-\nment (Fig 7(d)) than in the More constrained environment\n(Fig 6(d)). In addition, Table I shows that DS-RNN achieves\nthe highest average collision rate with obstacles (0.13) in all\nenvironments. Furthermore, the obstacle collision rate of DS-\nRNN increases in all 4 OOD environments, indicating an\noverfitting problem. Among the OOD environment, the per-\ncentage of obstacle collision increase is higher in environments\nwith higher obstacle-to-human ratios. For example, in Less\ncrowded with the highest percentage of obstacles, the obstacle\ncollision rate (0.21) increases by 133% w.r.t. the obstacle\ncollision rate in training distribution (0.09). On the contrary,\nin More crowded with the lowest percentage of obstacles,\nthe obstacle collision rate (0.06) drops by 33%. The reason\nis that DS-RNN has trouble inferring the geometric shapes\nof obstacles from a large group of circles, and thus fails to\navoid collision with them. Thus, treating both humans and\nobstacles as circles is not an ideal input representation for\nrobot navigation algorithms.\nFor A∗+ CNN, the A∗global planner does not account for\nhumans and the CNN local planner represents all observations\nas a 2D point cloud. As a result, from Table I, A∗+ CNN has\nthe highest average timeout rate (0.10) and the longest average\ntime (18.99) among RL-based methods in all environments,\nespecially the two most challenging ones, for the following 2\nreasons. (1) The waypoints can fail to guide the robot to reach\nthe goal because the waypoints lose optimality as agents move.\n\nTABLE I: Baseline comparison results with different human and obstacle densities in unseen environments\nEnvironment\nMethod\nSuccess↑\nCollision↓\nTimeout↓\nNav Time↓\nPath Len↓\nOverall\nw/ Humans\nw/ Obstacles\nTraining distribution\n5-9 humans\n8-12 obstacles\nDWA [1]\n0.16\n0.68\n0.63\n0.05\n0.15\n28.45\n11.52\nA∗+ CNN [7]\n0.64\n0.29\n0.28\n0.01\n0.07\n25.72\n12.30\nDS-RNN [8]\n0.80\n0.20\n0.11\n0.09\n0.00\n19.90\n10.78\nHomoGAT [44]\n0.86\n0.14\n0.13\n0.01\n0.00\n18.66\n10.36\nHEIGHT (ours)\n0.88\n0.12\n0.09\n0.03\n0.00\n18.31\n10.34\nLess crowded\n0-4 humans\n8-12 obstacles\nDWA [1]\n0.29\n0.37\n0.28\n0.09\n0.34\n25.74\n14.42\nA∗+ CNN [7]\n0.84\n0.12\n0.11\n0.01\n0.04\n22.64\n11.73\nDS-RNN [8]\n0.72\n0.28\n0.07\n0.21\n0.00\n17.21\n9.56\nHomoGAT [44]\n0.95\n0.05\n0.03\n0.02\n0.00\n16.64\n10.17\nHEIGHT (ours)\n0.97\n0.03\n0.02\n0.01\n0.00\n16.32\n10.04\nMore crowded\n10-14 humans\n8-12 obstacles\nDWA [1]\n0.11\n0.78\n0.74\n0.04\n0.11\n29.60\n10.37\nA∗+ CNN [7]\n0.47\n0.42\n0.39\n0.03\n0.11\n27.33\n12.47\nDS-RNN [8]\n0.76\n0.22\n0.16\n0.06\n0.01\n23.08\n11.67\nHomoGAT [44]\n0.72\n0.28\n0.23\n0.05\n0.00\n20.46\n10.57\nHEIGHT (ours)\n0.78\n0.22\n0.19\n0.03\n0.00\n19.69\n10.39\nLess constrained\n5-9 humans\n3-7 obstacles\nDWA [1]\n0.17\n0.57\n0.55\n0.02\n0.26\n28.50\n14.21\nA∗+ CNN [7]\n0.66\n0.29\n0.28\n0.01\n0.05\n23.63\n11.98\nDS-RNN [8]\n0.66\n0.34\n0.20\n0.14\n0.00\n18.01\n9.76\nHomoGAT [44]\n0.88\n0.12\n0.10\n0.02\n0.00\n17.66\n10.37\nHEIGHT (ours)\n0.90\n0.10\n0.09\n0.01\n0.00\n17.20\n10.23\nMore constrained\n5-9 humans\n13-17 obstacles\nDWA [1]\n0.14\n0.66\n0.49\n0.17\n0.20\n30.47\n11.35\nA∗+ CNN [7]\n0.48\n0.29\n0.23\n0.06\n0.23\n27.28\n13.11\nDS-RNN [8]\n0.71\n0.23\n0.09\n0.14\n0.06\n23.45\n11.58\nHomoGAT [44]\n0.80\n0.20\n0.11\n0.09\n0.00\n19.20\n10.51\nHEIGHT (ours)\n0.84\n0.15\n0.07\n0.08\n0.01\n18.79\n10.65\nConsequently, in OOD scenarios such as Fig. 6(b), the CNN\npolicy is especially bad at long-horizon planning because it\nis overfitted with good waypoints. (2) By mixing humans and\nobstacles in its input, the robot policy sometimes has a hard\ntime distinguishing dynamic and static obstacles. For example,\nin Fig. 6(b), the robot keeps a large distance from everything\nand thus is less agile and efficient compared with the robots\nin Fig. 6(e) and (f). Thus, for RL-based approaches, treating\nhumans as obstacles leads to overfitting to specific types of\nscenarios, preventing the policies from generalizing to OOD\nscenarios which are common in the real-world.\nBy splitting human and obstacle input representations, Ho-\nmoGAT and HEIGHT achieve the top 2 performance across\nmost metrics and environments. Especially, HEIGHT learns to\nkeep a larger distance from human paths yet a shorter distance\nfrom obstacles to balance safety and efficiency (Fig. 6(f)),\nindicated by the lowest overall collision rates (0.17) and the\nshortest average navigation time (18.10) in all environments.\nTherefore, we conclude that our split representation consisting\nof detected human states and obstacle point clouds is most\nsuitable to learn robot navigation in crowded and constrained\nenvironments with RL, among all popular choices in Table I.\n2) Effectiveness of the heterogeneous st-graph: To justify\nour heterogeneous st-graph formulation, we compare our\nmethod with A∗+ CNN with no graph concept and HomoGAT\nwith a homogenous graph attention network in Table I.\nA∗+ CNN lacks structure in input representation, and sub-\nsequently, lacks structure in network architecture. For this\nreason, as we discussed in Sec. V-C1, A∗+ CNN shows much\nworse average success rate (0.62 v.s. 0.84), navigation time\n(25.32 v.s. 18.52), and path length (12.32 v.s. 10.40) than\nHomoGAT which has a simple st-graph in all environments.\nA∗+ CNN also exhibits larger variances in terms of success\nrate (0.019 v.s. 0.0060), navigation time (3.62 v.s. 1.70), and\npath length (0.22 v.s. 0.019) across different OOD environ-\nments. Especially in the challenging More crowded and More\nconstrained environments, A∗+ CNN exhibits a larger drop\nin average success rate compared with HomoGAT (0.165 v.s.\n0.10). This finding shows that even a simple st-graph structure\ncan lead to a performance gain, indicating the importance of\nspatial and temporal reasoning for robot crowd navigation.\nHowever, occasional failures of HomoGAT still exist such as\nFig. 6(b). Without differentiating RH and OA interactions, the\nrobot maintains similar distances from humans and obstacles,\nyet fails because RH interactions require more space. As a\nstep further, with a heterogeneous st-graph, HEIGHT treats\nthe 3 types of edges with different network components. As\na result, HEIGHT outperforms HomoGAT and achieves the\nbest average success rate (0.87), navigation time (18.06), and\npath length (10.33) in all environments. The variance success\nrate (0.0040) and navigation time (1.40) are also the best,\nwhereas the path length variance (0.040) is the runner-up\nacross all environments. In addition, the average success rate\ndrop in the two challenging environments of HEIGHT is\nsmaller than HomoGAT (0.07 v.s. 0.10). The reason is that\nthe heterogeneous components allow the robot to reason about\nthe different effects of HH, RH, and OA spatial interactions.\nFor example, in Fig. 6(f) and Fig. 7(e), HEIGHT chooses a\npath that avoids the most crowded region, yields to humans\nwhen the robot must encounter them, and stays closer to walls\nfor efficiency. Therefore, we conclude that the spatial and\ntemporal reasoning on different types of interactions is the\nkey to ensuring good in-distribution performance and OOD\ngeneralization in crowded and interactive environments.\n\nTABLE II: Ablation study results with different human and obstacle densities in unseen environments\nEnvironment\nMethod\nSuccess↑\nCollision↓\nTimeout↓\nNav Time↓\nPath Len↓\nOverall\nw/ Humans\nw/ Obstacles\nTraining distribution\n5-9 humans\n8-12 obstacles\nNo attn\n0.52\n0.46\n0.41\n0.05\n0.02\n26.48\n11.81\nRH\n0.85\n0.15\n0.13\n0.02\n0.00\n18.86\n10.41\nHH\n0.87\n0.12\n0.10\n0.02\n0.01\n18.31\n10.42\nRH+HH (ours)\n0.88\n0.12\n0.08\n0.04\n0.00\n18.49\n10.28\nLess crowded\n0-4 humans\n8-12 obstacles\nNo attn\n0.72\n0.27\n0.24\n0.03\n0.01\n21.78\n10.94\nRH\n0.89\n0.07\n0.04\n0.03\n0.04\n17.34\n10.66\nHH\n0.94\n0.06\n0.05\n0.01\n0.00\n16.10\n9.90\nRH+HH (ours)\n0.97\n0.03\n0.02\n0.01\n0.00\n16.32\n10.04\nMore crowded\n10-14 humans\n8-12 obstacles\nNo attn\n0.29\n0.61\n0.53\n0.08\n0.10\n28.35\n12.57\nRH\n0.70\n0.29\n0.25\n0.04\n0.01\n20.39\n10.49\nHH\n0.74\n0.25\n0.17\n0.08\n0.01\n20.34\n10.53\nRH+HH (ours)\n0.78\n0.22\n0.19\n0.03\n0.00\n19.69\n10.39\nLess constrained\n5-9 humans\n3-7 obstacles\nNo attn\n0.55\n0.43\n0.42\n0.01\n0.01\n24.64\n11.75\nRH\n0.86\n0.14\n0.12\n0.02\n0.00\n18.17\n10.33\nHH\n0.88\n0.12\n0.10\n0.02\n0.00\n17.24\n10.25\nRH+HH (ours)\n0.90\n0.10\n0.09\n0.01\n0.00\n17.20\n10.23\nMore constrained\n5-9 humans\n13-17 obstacles\nNo attn\n0.43\n0.49\n0.38\n0.11\n0.08\n26.25\n11.82\nRH\n0.77\n0.22\n0.11\n0.11\n0.01\n20.21\n10.73\nHH\n0.84\n0.16\n0.06\n0.10\n0.00\n18.94\n10.66\nRH+HH (ours)\n0.84\n0.15\n0.07\n0.08\n0.01\n18.79\n10.65\n3) Importance of attention networks: We use ablations to\nevaluate the contribution of RH and HH attention networks to\nthe performance of HEIGHT, as shown in Table II.\nFirst, in terms of all metrics in most environments, the\nmodel with both RH and HH attention shows the strongest\nresults, followed by the models with only one attention, and\nfinally by the model with no attention. For example, in Fig. 7,\nthe ablations in (a), (b), and (c) fail yet our full model in (e)\nsucceeds. Especially in the 2 most challenging More crowded\nand More constrained environments, the existence of either\nRH or HH attention significantly boosts the average success\nrate by 0.38 and 0.43 respectively, compared with No attn. For\nexample, in Fig. 6 (e), the robot detours and goes dangerously\nclose to the crowds in a constrained area, yet HH attention\nallows it to realize the danger and resume to a less crowded\npath. This result shows that reasoning about both HH and RH\nspatial relationships is essential for our problem, especially in\nmore crowded or more constrained environments where the\nspatial interactions are also dense.\nSecond, by comparing RH and HH, we find that HH\nattention plays a more important role in all environments in\nterms of success rate and navigation time. This is because the\nnumber of HH edges is larger than the number of RH edges\nin most cases. Thus, the robot can observe and is affected by\na relatively larger number of HH interactions yet a smaller\nnumber of RH interactions.\n4) Failure cases: By visualizing the testing episodes across\nall environments, we find that HEIGHT typically collides\nwhen (1) a human arrives at its goal and suddenly changes\ndirections to a new goal, as shown in Fig. 8(a), or (2) the\nrobot surroundings are extremely crowded and constrained,\nand almost all free paths are blocked by humans (Fig. 8(b)). In\nthese cases, due to its speed limit, the robot sometimes cannot\nswitch to an alternative path in time to prevent collisions. To\nremedy the difficulty of RL in long-term decision making [59],\nin future work, our method can be combined with long-\nterm prediction and path planning algorithms that consider the\nstochasticity of pedestrian motions.\n5) Additional insights: Besides answering the four research\nquestions above, we provide additional insights obtained from\nsimulation experiments below.\nEffectiveness of RL planning: In Table I, DWA is a model-\nbased approach that determines robot actions based on only\nthe current state, which leads to the worst performance. Thus,\nreactive policies are not sufficient to solve our problem,\njustifying the necessity of long-sighted planning. In contrast,\nA∗+ CNN combines planning and RL, yet relies on occu-\npancy maps with only obstacles and not humans. As humans\nmove, the increasingly inaccurate occupancy map reduces\nthe optimality of waypoints, which negatively affects overall\nnavigation. On the other hand, RL learns to maximize the\nexpected long-term returns based on both the current and his-\ntorical states of all components in the scene. Consequently, the\nremaining 3 RL methods, especially HomoGAT and HEIGHT,\noutperform A∗+ CNN in most metrics and in most environ-\nments. Thus, to ensure the best navigation performance, it is\nimportant to optimize the entire planning system based on the\ntask setting instead of optimizing only a part of it.\nDifficulty of scenarios: From Table I and Table II, we\nobserve that, besides the overfitted DS-RNN, all methods show\nthe same trend of performance change in the 5 environments:\nLess crowded > Less constrained > More constrained >\nMore crowded. Obviously, adding more humans or obstacles\nincreases task difficulty. But interestingly, the change in the\nnumber of humans has a larger effect on the task difficulty\nthan the change in the number of obstacles. This phenomenon\nshows that avoiding collisions with humans is more difficult\nthan avoiding obstacles in nature.\n\nFig. 6: Comparison of different methods in the same testing episode in More Constrained environment. The robot is centered in white circles and its\norientation is denoted by white arrows. More qualitative results can be found in the video attachment and at https://sites.google.com/view/crowdnav-height/home.\nVI. REAL-WORLD EXPERIMENTS\nIn this section, we present our hardware setup and sim2real\ntesting results in everyday environments with pedestrians and\nstatic constraints.\nA. Experiment setup\nWe train the sim2real policies in the simulators of a hallway\n(Fig. 5(b)) and a lounge (Fig. 5(c)) for 2×108 timesteps with\na decaying learning rate 5 × 10−5. To learn robust policies,\nwe inject noises into the agent positions and robot control.\nThen, as shown in Fig. 5 (d) and (e), we test the policies in\nthe two corresponding everyday environments in a university\nbuilding without any additional training. In the hallway\nenvironment where the free space is extremely narrow, we\ntest the robot’s ability to handle constraints with low density\ncrowds. In the lounge environment, we test the robot’s ability\nto avoid dense crowds and obstacles with more diverse shapes.\nThe distance between the starting and the goal position of the\nrobot ranges from 6m to 11m. The pedestrians were told to\nreact naturally to the robot based on their own preferences. In\nsome testing episodes, other pedestrians who were unaware of\nour experiment also engaged with the robot.\nThe configuration of hardware testing is shown in Fig. 2(b).\nWe use a TurtleBot 2i with an RPLIDAR-A3 laser scanner.\nWe first use the ROS gmapping package to create a map of\nthe environment. Then, we process the map to combine small\nobstacles and eliminate noises. To reduce sim2real gap of the\nLiDAR point clouds, we use artificial point clouds obtained\n\nFig. 7: Comparison of different methods in the same testing episode in More Crowded environment.\nFig. 8: Failure cases of our method. (a) The black human was going downwards for a while, but suddenly changes its direction toward the robot. (b) All\nefficient paths toward the goal are blocked or will soon be blocked by human crowds.\nfrom localization and mapping, instead of the raw point clouds\nfrom LiDAR. The reason is that the artificial point clouds are\ncleaner and are not obstructed by humans. When a navigation\ntrial begins, a user inputs the robot goal position through\na keyboard. To detect human positions, we use an off-the-\nshelf people detector [52]. We use an Intel RealSense tracking\ncamera T265 to obtain the pose of the robot (px, py, θ) and\nthe robot wheel encoder to obtain its velocity (ux, uy).\nOur baseline is the ROS navigation stack, which uses the\ndynamic window approach (DWA) [1] as the local planner and\nA∗as the global planner. For each method, we run 30 trials\nin total. Among all trials, the start goal positions of the robot\nare the same, while the number and trajectories of pedestrians\nare similar. We measure the success, collision, timeout rates,\nand navigation time of successful episodes as testing metrics.\nB. Results\nIn the highly constrained yet less crowded Hallway envi-\nronment, ROS navigation stack often needs to spin in place to\nreplan, as shown by the higher navigation time in Table III.\nSome of the spinning recovery attempts fail and result in\ntimeouts. In ROS navigation stack, both global and local\nplanners treat humans as obstacles. As a result, similar to the\nbaselines in Sec. V-C1, the robot has difficulties distinguishing\n\nFig. 9: A testing episode of our method in the real Hallway environment. The blue arrow denotes the robot path that results from its actions. The red star\ndenotes the goal position. In this episode, the turtlebot avoids two pedestrians, one after another in a narrow corridor, enters a narrow doorway, and arrives\nat the goal. More qualitative results can be found in the video attachment and at https://sites.google.com/view/crowdnav-height/home.\nFig. 10: A testing episode of our method in the real Lounge environment. The turtlebot avoids multiple groups of people who pass each other in different\nheading directions, avoids the walls and furnitures, and arrives at the goal.\nTABLE III: Real-world results in 2 everyday environments\nEnvironment\n# of trials\nMethod\nSuccess↑\nCollision↓\nTimeout↓\nNav Time↓\nHallway\n1-2 humans\n12\nNavigation Stack\n0.72\n0.06\n0.22\n16.71\nHEIGHT (ours)\n1.00\n0.00\n0.00\n22.36\nLounge\n1-6 humans\n18\nNavigation Stack\n0.83\n0.17\n0.00\n32.00\nHEIGHT (ours)\n0.83\n0.17\n0.00\n30.71\ndynamic obstacles that will clear out and static obstacles that\nwill always stay, causing failures in highly constrained spaces.\nOn the contrary, by using different representations of humans\nand obstacles, our method is able to explore the different\nstrategies to avoid them through trial and error during RL\ntraining. Consequently, the robot demonstrates high success\nrates in Table III. Qualitatively, the robot is able to safely pass\na human in a narrow corridor (Fig. 9 (a)-(d)), takes a wide turn\nto give a human enough room, and enters an extremely narrow\ndoorway (Fig. 9 (e)-(h)).\nIn the less constrained yet more crowded Lounge en-\nvironment, ROS navigation stack and ours have the same\nsuccess and collision rate, because the Lounge environment\nhas enough space for the navigation stack to apply a “stop and\nwait” strategy for humans and obstacles. However, as a cost,\n“stop and wait” takes a longer average time to arrive at goals as\nshown in the last column of Table III. Different from this na¨ıve\nstrategy, HEIGHT adapts its navigation behaviors based on\ndifferent types of spatio-temporal interactions. For example, in\nFig. 10 (a)-(c), since pedestrians walking in opposite directions\nare crossing each other, the robot first turns left to avoid the\nlady on its right, and then turns right to avoid the two males on\nits left. Then, in Fig. 10 (d)-(e) and (f)-(g), the robot chooses\nactions that deviate from the current and intended paths of\npedestrians, walls, and the table to safely arrive at the goal.\nThe failure cases of both methods are caused by the tables\nwith irregular 3D shapes. Due to the height of the 2D LiDAR,\nsome wide parts of the tables are not detected or mapped, and\nthus the robot cannot avoid them. Further pre-processing of\nthe LiDAR point clouds or adding 3D sensors can mitigate\nthis problem.\nFurthermore, the successful sim2real transfer from a cheap\nand low-fidelity simulator to complex everyday environments\ndemonstrates the robustness of our input representation and\nthe cost-efficiency of our pipeline design.\n\nVII. DISCUSSIONS, LIMITATIONS, AND FUTURE WORK\nDeep RL is a promising tool to solve robotic problems that\nare beyond the capabilities of traditional rule-based methods\nwithout large-scale real-world datasets. However, preventing\nthe performance degradation of a deep RL policy when in-\nevitable distribution shifts happen, especially in real-world, is\nchallenging. In this section, we reflect on the key components\nof our framework, discuss the limitations of our approach, and\npropose directions for future research.\nA. Sim2real through Real2sim\nTo overcome sim2real gaps, the design of simulation\npipelines needs to be guided by the constraints of hardware and\nenvironments in the real-world. On one hand, we determine\nthe input representation of HEIGHT based on what could be\nobtained from sensors and off-the-shelf perception modules\nand how accurate they are. We find that intermediate features,\nsuch as detected human positions and processed point clouds,\nreduces sim2real gaps. On the other hand, we also ensure the\nconsistency of the simulation and real-world, such as the robot\naction space, whenever possible. These design choices allow\nour policy to generalize to different simulation environments\nand deploy to challenging real-world scenarios.\nHowever, although we have minimized the sim2real gaps\nthrough real2sim, certain gaps still exist. In real-world exper-\niments, the difficulty of the task is reduced and the agility of\nthe robot policy is only partially transferred from simulation.\nTo further align the simulation and the real-world, we plan\nto explore the following directions for future work: (1) devel-\noping a more natural pedestrian model to replace the ORCA\nhumans in the simulator, (2) revising our pipeline to enable\nself-supervised RL fine-tuning in the real-world [60], [61],\n(3) using a small amount of real-world data to automatically\noptimize the parameters of our simulator to match the real-\nworld environment [62], [63].\nB. Scene representation\nA good scene representation is tailored to the needs of its\ndownstream task. Besides the above sim2real considerations,\nour scene representation is split due the different nature of\nhumans and obstacles for robot collision avoidance. The size\nof humans are small and their shapes are simple. Therefore,\nto avoid humans, the robot only needs to treat them as circles\nwith a heading direction. In contrast, obstacles have larger and\nmore complicated surfaces. The part of the obstacle contours\nthat faces the robot is more important for collision avoidance.\nTherefore, point clouds are the most intuitive way to represent\nsuch useful information about obstacles. Our experiments in\nSec. V show that this split scene representation reduces robot\ncollision avoidance with both dynamic and static obstacles.\nA side effect of our scene abstraction is the loss of detailed\ninformation such as gaits of humans and 3D shapes of obsta-\ncles. However, we argue that this is a tradeoff to minimize\nsim2real gaps with limited simulation tools and computation\nresources. Another side effect is the cascading errors between\nthe perception modules and robot policy, such as inaccurate\nhuman detections or robot localization. To this end, it is worth\nexploring the joint optimization of the whole robotic stack\nfrom perception to control [64], [65].\nC. Structured neural network\nModel-based approaches require low-fidelity data yet heav-\nily rely on assumptions. In contrast, end-to-end learning ap-\nproaches need few assumptions yet require high-fidelity data.\nOur structured learning method combines the best of both\nworlds: It requires low-fidelity data yet relies on minimal\nassumptions. By injecting structures into the neural network,\nwe decompose a complex problem into smaller and relatively\nindependent components. Note that our decomposition does\nnot break the gradient flow, which keeps HEIGHT end-to-\nend trainable. We propose a principled way for network\narchitecture design, increasing the transparency of these black-\nboxes. Our experiments demonstrate that the structured net-\nwork outperforms both model-based methods and RL-based\nmethods without structures, which empirically proves the\neffectiveness of structure learning for interactive tasks with\nmultiple heterogeneous participants.\nD. Training method\nDeep RL enables the robot to explore the environment and\nlearn meaningful behaviors through trial and error. Without\nheuristics or demonstrations, the robot has to collect reward\nsignals to improve its policy. Consequently, simulation de-\nvelopment with a randomization scheme and a good reward\ndesign are indispensable to the performance of our method.\nNevertheless, our method is subjective to the inherent limita-\ntions of RL, such as low training data efficiency and difficulties\nin long-horizon tasks. In future work, combining the RL policy\nwith other traditional planners or imitation learning has the\npotential to alleviate these problems [36].\nVIII. CONCLUSION\nIn this article, we proposed HEIGHT, a novel structured\ngraph network architecture for autonomous robot navigation\nin dynamic and constrained environments. Our approach takes\nadvantage of the graphical nature and decomposability of\nthe constrained crowd navigation problem, introducing the\nfollowing two key novelties. First, we split and process the\nhuman and obstacle representations separately. This allows\nthe robot to effectively reason about the different geometrics\nand dynamics of humans and obstacles, improving its ability\nto navigate complex environments. Second, we propose a\nheterogeneous st-graph to capture various types of interactions\namong the robot, humans, and obstacles. The decomposition\nof the scenario as a heterogeneous st-graph guides the design\nof the HEIGHT network with attention mechanisms. Attention\nenables the robot to reason about the relative importance of\neach pairwise interaction, leading to adaptive and agile robot\ndecision-making during navigation.\nOur\nsimulation\nexperiments\nshow\nthat\nthe\nHEIGHT\nmodel outperforms traditional model-based methods and other\nlearning-based methods in terms of collision avoidance and\n\nnavigation efficiency. The HEIGHT model also demonstrates\nimproved generalization in environments with varied human\nand obstacle densities. In real-world environments, HEIGHT\nis seamlessly transferred from simulation to everyday indoor\nnavigation scenarios without additional training, showcasting\nits robustness and ability to overcome the sim-to-real gap.\nOur work suggests that reasoning about subtle spatio-\ntemporal interactions is an essential step toward smooth\nhuman-robot interaction. Furthermore, our work highlights the\nsignificance of uncovering the inherent structure of complex\nproblems and injecting these structures into learning frame-\nworks to solve the problems in a principled manner.\nACKNOWLEDGEMENTS\nWe thank Zhe Huang for helpful discussions and code of the\nA* planner. We thank Aamir Hasan and Rutav Shah for feed-\nback on paper drafts. We thank members in Human-Centered\nAutonomy Lab who participated in real-world experiments.\nREFERENCES\n[1] D. Fox, W. Burgard, and S. Thrun, “The dynamic window approach to\ncollision avoidance,” IEEE Robotics and Automation Magazine, vol. 4,\nno. 1, pp. 23–33, 1997.\n[2] J. Van den Berg, M. Lin, and D. Manocha, “Reciprocal velocity obstacles\nfor real-time multi-agent navigation,” in IEEE International Conference\non Robotics and Automation (ICRA), 2008, pp. 1928–1935.\n[3] C. Chen, Y. Liu, S. Kreiss, and A. Alahi, “Crowd-robot interaction:\nCrowd-aware robot navigation with attention-based deep reinforcement\nlearning,” in IEEE International Conference on Robotics and Automation\n(ICRA), 2019, pp. 6015–6022.\n[4] Y. F. Chen, M. Liu, M. Everett, and J. P. How, “Decentralized non-\ncommunicating multiagent collision avoidance with deep reinforcement\nlearning,” in IEEE International Conference on Robotics and Automation\n(ICRA), 2017, pp. 285–292.\n[5] Y. Yang, J. Jiang, J. Zhang, J. Huang, and M. Gao, “St2: Spatial-\ntemporal state transformer for crowd-aware autonomous navigation,”\nIEEE Robotics and Automation Letters, vol. 8, no. 2, pp. 912–919, 2023.\n[6] C. Chen, S. Hu, P. Nikdel, G. Mori, and M. Savva, “Relational graph\nlearning for crowd navigation,” in IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), 2020.\n[7] C.\nP´erez-D’Arpino,\nC.\nLiu,\nP.\nGoebel,\nR.\nMart´ın-Mart´ın,\nand\nS. Savarese, “Robot navigation in constrained pedestrian environments\nusing reinforcement learning,” in IEEE International Conference on\nRobotics and Automation (ICRA), 2021, pp. 1140–1146.\n[8] S. Liu, P. Chang, W. Liang, N. Chakraborty, and K. Driggs-Campbell,\n“Decentralized structural-rnn for robot crowd navigation with deep\nreinforcement learning,” in IEEE International Conference on Robotics\nand Automation (ICRA), 2021, pp. 3517–3524.\n[9] S. Liu, P. Chang, Z. Huang, N. Chakraborty, K. Hong, W. Liang,\nD. L. McPherson, J. Geng, and K. Driggs-Campbell, “Intention aware\nrobot crowd navigation with attention-based interaction graph,” in IEEE\nInternational Conference on Robotics and Automation (ICRA), 2023, pp.\n12 015–12 021.\n[10] M. Hoy, A. S. Matveev, and A. V. Savkin, “Algorithms for collision-\nfree navigation of mobile robots in complex cluttered environments: a\nsurvey,” Robotica, vol. 33, no. 3, pp. 463–497, 2015.\n[11] A. V. Savkin and C. Wang, “Seeking a path through the crowd: Robot\nnavigation in unknown dynamic environments with moving obstacles\nbased on an integrated environment representation,” Robotics and Au-\ntonomous Systems, vol. 62, no. 10, pp. 1568–1580, 2014.\n[12] C. Mavrogiannis, F. Baldini, A. Wang, D. Zhao, P. Trautman, A. Stein-\nfeld, and J. Oh, “Core challenges of social robot navigation: A survey,”\narXiv preprint arXiv:2103.05668, 2021.\n[13] J. Van Den Berg, S. J. Guy, M. Lin, and D. Manocha, “Reciprocal n-\nbody collision avoidance,” in Robotics research.\nSpringer, 2011, pp.\n3–19.\n[14] D. Helbing and P. Molnar, “Social force model for pedestrian dynamics,”\nPhysical review E, vol. 51, no. 5, p. 4282, 1995.\n[15] L. Huber, J.-J. Slotine, and A. Billard, “Avoiding dense and dynamic\nobstacles in enclosed spaces: Application to moving in crowds,” IEEE\nTransactions on Robotics, vol. 38, no. 5, pp. 3113–3132, 2022.\n[16] C. Mavrogiannis, K. Balasubramanian, S. Poddar, A. Gandra, and\nS. S. Srinivasa, “Winding through: Crowd navigation via topological\ninvariance,” IEEE Robotics and Automation Letters, vol. 8, no. 1, pp.\n121–128, 2023.\n[17] Open Source Robotics Foundation, “ROS Navigation Stack,” http://wiki.\nros.org/navigation, 2007, accessed: 2024-11-11.\n[18] M. Dobrevski and D. Skoˇcaj, “Dynamic adaptive dynamic window\napproach,” IEEE Transactions on Robotics, vol. 40, pp. 3068–3081,\n2024.\n[19] P. Long, T. Fan, X. Liao, W. Liu, H. Zhang, and J. Pan, “Towards\noptimally decentralized multi-robot collision avoidance via deep rein-\nforcement learning,” in IEEE International Conference on Robotics and\nAutomation (ICRA), 2018, pp. 6252–6259.\n[20] P. Trautman and A. Krause, “Unfreezing the robot: Navigation in dense,\ninteracting crowds,” in IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), 2010, pp. 797–803.\n[21] Z. Liu, W. Na, C. Yao, C. Liu, and Q. Chen, “Relaxing the limitations of\nthe optimal reciprocal collision avoidance algorithm for mobile robots\nin crowds,” IEEE Robotics and Automation Letters, vol. 9, no. 6, pp.\n5520–5527, 2024.\n[22] P. Long, W. Liu, and J. Pan, “Deep-learned collision avoidance policy\nfor distributed multiagent navigation,” IEEE Robotics and Automation\nLetters, vol. 2, no. 2, pp. 656–663, 2017.\n[23] L. Tai, J. Zhang, M. Liu, and W. Burgard, “Socially compliant navigation\nthrough raw depth inputs with generative adversarial imitation learning,”\nin IEEE International Conference on Robotics and Automation (ICRA),\n2018, pp. 1111–1117.\n[24] Z. Xie, P. Xin, and P. Dames, “Towards safe navigation through\ncrowded dynamic environments,” in IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), 2021, pp. 4934–4940.\n[25] A. Pokle, R. Mart´ın-Mart´ın, P. Goebel, V. Chow, H. M. Ewald, J. Yang,\nZ. Wang, A. Sadeghian, D. Sadigh, S. Savarese, et al., “Deep local trajec-\ntory replanning and control for robot navigation,” in IEEE International\nConference on Robotics and Automation (ICRA), 2019, pp. 5815–5822.\n[26] H. Karnan, A. Nair, X. Xiao, G. Warnell, S. Pirk, A. Toshev, J. Hart,\nJ. Biswas, and P. Stone, “Socially compliant navigation dataset (scand):\nA large-scale dataset of demonstrations for social navigation,” IEEE\nRobotics and Automation Letters, 2022.\n[27] R. Chandra, H. Karnan, N. Mehr, P. Stone, and J. Biswas, “Towards\nimitation learning in real world unstructured social mini-games in\npedestrian crowds,” arXiv preprint arXiv:2405.16439, 2024.\n[28] Z. Xie and P. Dames, “Drl-vo: Learning to navigate through crowded dy-\nnamic scenes using velocity obstacles,” IEEE Transactions on Robotics,\nvol. 39, no. 4, pp. 2700–2719, 2023.\n[29] M. Everett, Y. F. Chen, and J. P. How, “Motion planning among dynamic,\ndecision-making agents with deep reinforcement learning,” in IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS),\n2018, pp. 3052–3059.\n[30] Y. Chen, C. Liu, B. E. Shi, and M. Liu, “Robot navigation in crowds by\ngraph convolutional networks with attention learned from human gaze,”\nIEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 2754–2761,\n2020.\n[31] Y.-J. Mun, M. Itkina, S. Liu, and K. Driggs-Campbell, “Occlusion-\naware crowd navigation using people as sensors,” in IEEE International\nConference on Robotics and Automation (ICRA), 2023, pp. 12 031–\n12 037.\n[32] A. J. Sathyamoorthy, J. Liang, U. Patel, T. Guan, R. Chandra, and\nD. Manocha, “Densecavoid: Real-time navigation in dense crowds using\nanticipatory behaviors,” in IEEE International Conference on Robotics\nand Automation (ICRA), 2020, pp. 11 345–11 352.\n[33] D. Dugas, O. Andersson, R. Siegwart, and J. J. Chung, “Navdreams:\nTowards camera-only rl navigation among humans,” in IEEE/RSJ Inter-\nnational Conference on Intelligent Robots and Systems (IROS), 2022,\npp. 2504–2511.\n[34] Z. Zheng, C. Cao, and J. Pan, “A hierarchical approach for mobile robot\nexploration in pedestrian crowd,” IEEE Robotics and Automation Letters,\nvol. 7, no. 1, pp. 175–182, 2022.\n[35] C. Mavrogiannis, F. Baldini, A. Wang, D. Zhao, P. Trautman, A. Stein-\nfeld, and J. Oh, “Core challenges of social robot navigation: A survey,”\nACM Transactions on Human-Robot Interaction, vol. 12, no. 3, 2023.\n[36] A. H. Raj, Z. Hu, H. Karnan, R. Chandra, A. Payandeh, L. Mao, P. Stone,\nJ. Biswas, and X. Xiao, “Rethinking social robot navigation: Leveraging\nthe best of two worlds,” in IEEE International Conference on Robotics\nand Automation (ICRA), 2024.\n\n[37] N. Tsoi, A. Xiang, P. Yu, S. S. Sohn, G. Schwartz, S. Ramesh,\nM. Hussein, A. W. Gupta, M. Kapadia, and M. V´azquez, “Sean 2.0:\nFormalizing and generating social situations for robot navigation,” IEEE\nRobotics and Automation Letters, pp. 1–8, 2022.\n[38] X. Puig, E. Undersander, A. Szot, M. D. Cote, T.-Y. Yang, R. Partsey,\nR. Desai, A. Clegg, M. Hlavac, S. Y. Min, V. Vondruˇs, T. Gervet, V.-P.\nBerges, J. M. Turner, O. Maksymets, Z. Kira, M. Kalakrishnan, J. Malik,\nD. S. Chaplot, U. Jain, D. Batra, A. Rai, and R. Mottaghi, “Habitat 3.0: A\nco-habitat for humans, avatars, and robots,” in International Conference\non Learning Representations (ICLR), 2024.\n[39] L. K¨astner, V. Shcherbyna, H. Zeng, T. A. Le, M. H.-K. Schreff, H. Os-\nmaev, N. T. Tran, D. Diaz, J. Golebiowski, H. Soh, and J. Lambrecht,\n“Arena 3.0: Advancing social navigation in collaborative and highly\ndynamic environments,” in Robotics: Science and Systems, 2024.\n[40] R. Martin-Martin, M. Patel, H. Rezatofighi, A. Shenoi, J. Gwak,\nE. Frankel, A. Sadeghian, and S. Savarese, “Jrdb: A dataset and\nbenchmark of egocentric robot visual perception of humans in built\nenvironments,” IEEE transactions on pattern analysis and machine\nintelligence, 2021.\n[41] N. Hirose, D. Shah, A. Sridhar, and S. Levine, “Sacson: Scalable au-\ntonomous control for social navigation,” IEEE Robotics and Automation\nLetters, vol. 9, no. 1, pp. 49–56, 2024.\n[42] H. Yang, C. Yao, C. Liu, and Q. Chen, “Rmrl: Robot navigation in\ncrowd environments with risk map-based deep reinforcement learning,”\nIEEE Robotics and Automation Letters, vol. 8, no. 12, pp. 7930–7937,\n2023.\n[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems (NeurIPS), 2017, pp. 5998–\n6008.\n[44] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and\nY. Bengio, “Graph attention networks,” International Conference on\nLearning Representations, 2018.\n[45] A. Vemula, K. Muelling, and J. Oh, “Social attention: Modeling attention\nin human crowds,” in IEEE International Conference on Robotics and\nAutomation (ICRA), 2018, pp. 1–7.\n[46] Y. Huang, H. Bi, Z. Li, T. Mao, and Z. Wang, “Stgat: Modeling\nspatial-temporal interactions for human trajectory prediction,” in IEEE\nInternational Conference on Computer Vision (ICCV), 2019.\n[47] A. Hasan, P. Sriram, and K. Driggs-Campbell, “Meta-path analysis on\nspatio-temporal graphs for pedestrian trajectory prediction,” in IEEE\nInternational Conference on Robotics and Automation (ICRA), 2022,\npp. 617–624.\n[48] E. Leurent and J. Mercat, “Social attention for autonomous decision-\nmaking in dense traffic,” in Machine Learning for Autonomous Driv-\ning Workshop at Advances in Neural Information Processing Systems\n(NeurIPS), 2019.\n[49] S. Liu, P. Chang, H. Chen, N. Chakraborty, and K. Driggs-Campbell,\n“Learning to navigate intersections with unsupervised driver trait infer-\nence,” in IEEE International Conference on Robotics and Automation\n(ICRA), 2022.\n[50] L. Huajian, D. Wei, M. Shouren, W. Chao, and G. Yongzhuo, “Sample-\nefficient learning-based dynamic environment navigation with transfer-\nring experience from optimization-based planner,” IEEE Robotics and\nAutomation Letters, vol. 9, no. 8, pp. 7055–7062, 2024.\n[51] B. Chen, H. Zhu, S. Yao, S. Lu, P. Zhong, Y. Sheng, and J. Wang,\n“Socially aware object goal navigation with heterogeneous scene repre-\nsentation learning,” IEEE Robotics and Automation Letters, vol. 9, no. 8,\npp. 6792–6799, 2024.\n[52] D. Jia, A. Hermans, and B. Leibe, “DR-SPAAM: A Spatial-Attention\nand Auto-regressive Model for Person Detection in 2D Range Data,” in\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), 2020, pp. 10 270–10 277.\n[53] J. Redmon and A. Farhadi, “YOLOv3: An Incremental Improvement,”\narXiv preprint arXiv:1804.02767, 2018.\n[54] N. Wojke, A. Bewley, and D. Paulus, “Simple Online and Realtime\nTracking with a Deep Association Metric,” in 2017 IEEE international\nconference on image processing (ICIP), 2017, pp. 3645–3649.\n[55] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena, “Structural-rnn: Deep\nlearning on spatio-temporal graphs,” in IEEE conference on computer\nvision and pattern recognition (CVPR), 2016, pp. 5308–5317.\n[56] X. Ma, J. Li, M. J. Kochenderfer, D. Isele, and K. Fujimura, “Rein-\nforcement learning for autonomous driving with latent state inference\nand spatial-temporal relationships,” in IEEE International Conference\non Robotics and Automation (ICRA), 2021.\n[57] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[58] E. Coumans and Y. Bai, “Pybullet, a python module for physics\nsimulation for games, robotics and machine learning,” http://pybullet.org,\n2016–2019.\n[59] S. Nasiriany, H. Liu, and Y. Zhu, “Augmenting reinforcement learning\nwith behavior primitives for diverse manipulation tasks,” in IEEE\nInternational Conference on Robotics and Automation (ICRA), 2022.\n[60] H. Zhu, J. Yu, A. Gupta, D. Shah, K. Hartikainen, A. Singh, V. Kumar,\nand S. Levine, “The ingredients of real world robotic reinforcement\nlearning,” in International Conference on Learning Representations\n(ICLR), 2020.\n[61] P. Chang, S. Liu, T. Ji, N. Chakraborty, K. Hong, and K. R. Driggs-\nCampbell, “A data-efficient visual-audio representation with intuitive\nfine-tuning for voice-controlled robots,” in Conference on Robot Learn-\ning (CoRL), 2023.\n[62] Y. Du, O. Watkins, T. Darrell, P. Abbeel, and D. Pathak, “Auto-tuned\nsim-to-real transfer,” in IEEE International Conference on Robotics and\nAutomation (ICRA), 2021, p. 1290–1296.\n[63] V. Lim, H. Huang, L. Y. Chen, J. Wang, J. Ichnowski, D. Seita,\nM. Laskey, and K. Goldberg, “Real2sim2real: Self-supervised learning\nof physical single-step dynamic actions for planar robot casting,” in\nIEEE International Conference on Robotics and Automation (ICRA),\n2022, pp. 8282–8289.\n[64] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin,\nW. Wang, L. Lu, X. Jia, Q. Liu, J. Dai, Y. Qiao, and H. Li, “Planning-\noriented autonomous driving,” in IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2023.\n[65] H. Wang, K. Kedia, J. Ren, R. Abdullah, A. Bhardwaj, A. Chao, K. Y.\nChen, N. Chin, P. Dan, X. Fan, G. Gonzalez-Pumariega, A. Kompella,\nM. A. Pace, Y. Sharma, X. Sun, N. Sunkara, and S. Choudhury, “Mosaic:\nA modular system for assistive and interactive cooking,” 2024.",
    "pdf_filename": "HEIGHT_Heterogeneous_Interaction_Graph_Transformer_for_Robot_Navigation_in_Crowded_and_Constrained_E.pdf"
}