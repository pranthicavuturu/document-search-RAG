{
    "title": "HEIGHT: HEterogeneous Interaction GrapH",
    "abstract": "and interactive crowds with environmental constraints such as corridors and furniture. Previous methods fail to consider all types of interactions among agents and obstacles, leading to unsafe and inefficient robot paths. In this article, we leverage a graph-basedrepresentationofcrowdedandconstrainedscenarios and propose a structured framework to learn robot navigation policies with deep reinforcement learning. We first split the representations of different components in the environment, and propose a heterogeneous spatio-temporal graph to model distinctinteractionsamonghumans,robots,andobstacles.Based on the heterogeneous st-graph, we propose HEIGHT, a novel navigationpolicynetworkarchitecturewithdifferentcomponents to capture heterogeneous interactions among entities through space and time. HEIGHT utilizes attention mechanisms to prioritize important interactions and a recurrent network to track changes in the dynamic scene over time, encouraging the robottoavoidcollisionsadaptively.Throughextensivesimulation and real-world experiments, we demonstrate that HEIGHT outperforms state-of-the-art baselines in terms of success and efficiency in challenging navigation scenarios. Furthermore, we demonstrate that our pipeline achieves better zero-shot gen- eralization capability than previous works when the densities of humans and obstacles change. More videos are available at https://sites.google.com/view/crowdnav-height/home. I. INTRODUCTION Robotsareincreasinglyprevalentinhuman-centricenviron- ments.Inapplicationssuchaslast-miledeliveryandhousehold robots, the ability to navigate among humans is crucial. For example, Fig. 1 shows a navigation scenario with abundant subtle interactions: Obstacles have a one-way effect on the pathsofagents(i.e.humansandtherobot),whiletheinfluence among agents is mutual. Among agents, humans may react to Fig. 1: A heterogeneous graph aids spatio-temporal reasoning when a otherhumansandrobotsindifferentways.Tonavigate,arobot robot navigates in a crowded and constrained environment.Thecolored directlyparticipatesinsomeinteractionsinitscloseproximity, arrows denote robot-human (RH), human-human (HH), and obstacle-agent andsimultaneously,isindirectlyaffectedbyotherinteractions. (OA) interactions. The opaque arrows are the more important interactions whilethetransparentarrowsarethelessimportantones.Ateachtimestept, Theseinteractionsareheterogeneous,dynamic,anddifficultto therobotreasonsabouttheseinteractions,focusesontheimportantones,and infer, making navigation in such environments challenging. makesdecisions. Rising to these challenges, previous works have explored Theyassumeagentsmoveinanopenspacewithoutobstacles, various approaches for robot crowd navigation [1]–[3]. How- which are common in the real-world [3]–[5]; (2) They do not ever, these works typically have one of two limitations: (1) differentiate between various types of interactions, and thus S. Liu is with the Department of Computer Science at The University of the robot has difficulties taking adaptive strategies to avoid TexasatAustin.Email:shuijing.liu@utexas.edu collisions with humans and obstacles [1], [6]–[8]. H. Xia, F. Cheraghi Pouria, K. Hong, N. Chakraborty, and K. Driggs- Campbell are with the Department of Electrical and Computer Engineering Our goal is to navigate a robot to a destination without attheUniversityofIllinoisatUrbana-Champaign.Emails:{hx17,fatemeh5, colliding with humans and obstacles. To solve this problem, kaiwen2,neeloyc2,krdc}@illinois.edu we ask the following research question: How can a robot This material is based upon work supported by the National Science FoundationunderGrantNo.2143435. reason about diverse interactions in crowded and constrained 4202 voN 91 ]OR.sc[ 1v05121.1142:viXra",
    "body": "HEIGHT: HEterogeneous Interaction GrapH\nTransformer for Robot Navigation in\nCrowded and Constrained Environments\nShuijing Liu, Haochen Xia, Fatemeh Cheraghi Pouria, Kaiwen Hong,\nNeeloy Chakraborty, and Katherine Driggs-Campbell\nAbstract—We study the problem of robot navigation in dense\nand interactive crowds with environmental constraints such as\ncorridors and furniture. Previous methods fail to consider all\ntypes of interactions among agents and obstacles, leading to\nunsafe and inefficient robot paths. In this article, we leverage a\ngraph-basedrepresentationofcrowdedandconstrainedscenarios\nand propose a structured framework to learn robot navigation\npolicies with deep reinforcement learning. We first split the\nrepresentations of different components in the environment,\nand propose a heterogeneous spatio-temporal graph to model\ndistinctinteractionsamonghumans,robots,andobstacles.Based\non the heterogeneous st-graph, we propose HEIGHT, a novel\nnavigationpolicynetworkarchitecturewithdifferentcomponents\nto capture heterogeneous interactions among entities through\nspace and time. HEIGHT utilizes attention mechanisms to\nprioritize important interactions and a recurrent network to\ntrack changes in the dynamic scene over time, encouraging the\nrobottoavoidcollisionsadaptively.Throughextensivesimulation\nand real-world experiments, we demonstrate that HEIGHT\noutperforms state-of-the-art baselines in terms of success and\nefficiency in challenging navigation scenarios. Furthermore, we\ndemonstrate that our pipeline achieves better zero-shot gen-\neralization capability than previous works when the densities\nof humans and obstacles change. More videos are available at\nhttps://sites.google.com/view/crowdnav-height/home.\nI. INTRODUCTION\nRobotsareincreasinglyprevalentinhuman-centricenviron-\nments.Inapplicationssuchaslast-miledeliveryandhousehold\nrobots, the ability to navigate among humans is crucial. For\nexample, Fig. 1 shows a navigation scenario with abundant\nsubtle interactions: Obstacles have a one-way effect on the\npathsofagents(i.e.humansandtherobot),whiletheinfluence\namong agents is mutual. Among agents, humans may react to\nFig. 1: A heterogeneous graph aids spatio-temporal reasoning when a\notherhumansandrobotsindifferentways.Tonavigate,arobot\nrobot navigates in a crowded and constrained environment.Thecolored\ndirectlyparticipatesinsomeinteractionsinitscloseproximity, arrows denote robot-human (RH), human-human (HH), and obstacle-agent\nandsimultaneously,isindirectlyaffectedbyotherinteractions. (OA) interactions. The opaque arrows are the more important interactions\nwhilethetransparentarrowsarethelessimportantones.Ateachtimestept,\nTheseinteractionsareheterogeneous,dynamic,anddifficultto\ntherobotreasonsabouttheseinteractions,focusesontheimportantones,and\ninfer, making navigation in such environments challenging. makesdecisions.\nRising to these challenges, previous works have explored\nTheyassumeagentsmoveinanopenspacewithoutobstacles,\nvarious approaches for robot crowd navigation [1]–[3]. How-\nwhich are common in the real-world [3]–[5]; (2) They do not\never, these works typically have one of two limitations: (1)\ndifferentiate between various types of interactions, and thus\nS. Liu is with the Department of Computer Science at The University of the robot has difficulties taking adaptive strategies to avoid\nTexasatAustin.Email:shuijing.liu@utexas.edu\ncollisions with humans and obstacles [1], [6]–[8].\nH. Xia, F. Cheraghi Pouria, K. Hong, N. Chakraborty, and K. Driggs-\nCampbell are with the Department of Electrical and Computer Engineering Our goal is to navigate a robot to a destination without\nattheUniversityofIllinoisatUrbana-Champaign.Emails:{hx17,fatemeh5, colliding with humans and obstacles. To solve this problem,\nkaiwen2,neeloyc2,krdc}@illinois.edu\nwe ask the following research question: How can a robot\nThis material is based upon work supported by the National Science\nFoundationunderGrantNo.2143435. reason about diverse interactions in crowded and constrained\n4202\nvoN\n91\n]OR.sc[\n1v05121.1142:viXra\nenvironmentstoadaptivelyavoidcollisionsduringnavigation? II. RELATEDWORKS\nTo address this question, we propose a framework that takes\nIn this section, we first review the literature of robot crowd\nadvantage of the heterogeneity of interactions in crowded\nnavigation, which is divided into model-based and learning-\nand constrained scenarios. First, we split the environment\nbased approaches. Then, we discuss previous crowd naviga-\ninto human and obstacle representations, which are processed\ntion efforts in constrained spaces. Finally, we review graph\nand fed separately into the reinforcement learning (RL)-based\nattention mechanism with a focus on the usage of attention\nnavigation pipeline. Then, we decompose the scenario into a\nnetworks in multi-agent interaction modeling.\nheterogeneous spatio-temporal (st) graph with different types\nof edges to represent different types of interactions among\nthe robot, humans, and obstacles, as shown in the colored A. Model-based methods\narrows in Fig. 1. Finally, we convert the heterogeneous st- Robot navigation in human crowds is particularly challeng-\ngraph into a HEterogeneous Interaction GrapH Transformer ing and has been studied for decades [1], [10]–[12]. Model-\n(HEIGHT), a robot policy network consisting of different based approaches have explored various mathematical models\nmodules to parameterize the various spatio-temporal interac- to optimize robot actions [1], [2], [13]–[16]. As an early\ntions. Specifically, we use two separate multi-head attention example, ROS navigation stack [17] uses a cost map for\nnetworks to address the different effects of robot-human (RH) global planning and dynamic window approach (DWA) for\nand human-human (HH) interactions. The attention networks local planning [1]. DWA searches for the optimal velocity\nenable the robot to pay more attention to the important that brings the robot to the goal with the maximum clearance\ninteractions,leadingtoalowcollision-rateevenasthenumber fromanyobstaclewhileobeyingthedynamicsoftherobot.By\nofhumansincreases,andthegraphbecomesmorecomplex.In treatinghumansasobstacles,DWAexhibitsmyopicbehaviors\naddition, we use a multilayer perceptron (MLP) to model the such as oscillatory paths in dynamic environments [18]. As a\nsingle-directional obstacle-agent interactions and a recurrent step forward, optimal reciprocal collision avoidance (ORCA)\nnetworkfortemporalevolutionofthescene.Inresponsetothe and social force (SF) account for the velocities of agents.\nrapidly changing scenario (Fig. 1 bottom), HEIGHT captures ORCA models other agents as velocity obstacles and assumes\nthe heterogeneous interactions among different components that agents avoid each other under the reciprocal rule [2],\nthrough space and time, enabling the robot to avoid collisions [13]. SF models the interactions between the robot and other\nand approach its goal in an efficient manner. agentsusingattractiveandrepulsiveforces[14].However,the\nThis article is expanded from a contribution on attention hyperparameters of the model-based approaches are sensitive\ngraph network proposed by our previous work [9]. While to crowd behaviors and thus need to be tuned carefully\nour previous work focuses on crowd navigation in open to ensure good performance [18], [19]. In addition, model-\nspaces,thisarticleincorporatesstaticobstaclesandconstraints, based methods are prone to failures, such as the freezing\nwhich leads to significant modifications of scene representa- problem, if the assumptions such as the reciprocal rule are\ntionandnetworkarchitecture.Correspondingtothesechanges broken [20], [21]. In contrast, while our method also models\nin methodology, new simulation and hardware experiments these interactions, we learn the hyperparameters of the model\nwith new baseline comparisons are performed. In summary, from trial and error with minimal assumptions on human\nthe main contributions of this article are as follows. behaviors posed by model-based methods.\n1) We propose an input representation of crowded and\nB. Learning-based methods\nconstrained environments that treats humans and obsta-\ncles differently. The split scene representation naturally Learning-based approaches have been widely used for nav-\nallowsustoinjectstructuresintherestoftheframework. igation in dynamic environments to reduce hyperparameter\n2) Weproposeastructuredgraphrepresentationofcrowded tuning efforts and the number of assumptions. One example\nand constrained scenarios, named heterogeneous spatio- is supervised learning from expert demonstrations of desired\ntemporal graph (st-graph), to effectively model the pair- behaviors, where the expert ranges from model-based poli-\nwise interactions among all agents and entities. cies [22]–[24], human teleoperators in simulators [25], to real\n3) From the heterogenous st-graph, we use a principle pedestrians [26], [27]. Supervised learning does not require\napproach to derive HEIGHT, a transformer-based robot explorations of the state and action spaces of the robot, yet\nnavigationpolicynetworkwithdifferentmodulestorea- the performance of learned policy is limited by the quality of\nson about all types of spatial and temporal interactions. expert demonstrations.\n4) The experiments in simulation with dense crowds and Another line of work takes advantage of crowd simulators\ndense obstacles demonstrate that our method outper- and learns policies with RL. Through trial and error, RL\nformspreviousstate-of-the-artmethodsinunseenobsta- has the potential to learn robot behaviors that outperform\ncle layouts. In addition, our method demonstrates better model-based approaches and expert demonstrations [28]. For\ngeneralization to out-of-distribution environments with example, Deep V-Learning first uses supervised learning with\ndifferent human and obstacle densities. ORCA as the expert and then uses RL to learn a value\n5) We successfully transfer the robot policy learned in a function for path planning [3]–[5], [29], [30]. However, Deep\nlow-fidelity simulator to challenging real-world, every- V-Learning assumes that state transitions of all agents are\nday crowded environments without finetuning. known without uncertainty. In addition, since the networks\nare pre-trained with supervised learning, they share the same on learning relationships of data with graphical structures\ndisadvantages with OCRA, which are hard to correct by such as citation networks [44]. Inspired by these works,\nRL [8]. To address these problems, more recent efforts lever- researchersintrajectorypredictionandcrowdnavigationhave\nagemodel-freeRLwithoutsupervisedlearningorassumptions found that attention networks are also well-suited to capture\nonstatetransitions[5],[8],[21],[31].Sincethestatetransition interactions amongst agents and entities, which contain es-\nprobability of humans is uncertain, directly learning a policy sential information for multi-agent tasks [3], [8], [45]–[47].\nnetwork without explicitly modeling state transitions is more For each agent, these works compute attention scores for\nsuitable for navigation [32]. However, the aforementioned RL all neighboring agents. Due to the permutation invariance\nworks have at least one of the following two problems: (1) property,attentionscoresbettercapturetheimportanceofpair-\nThey focus on navigation in open spaces and isolate agents wiserelationshipsthancombiningtheinformationofallagents\nfrom static constraints, posing difficulties when deploying with concatenation or an LSTM encoder [4], [29].\nrobotsinthereal-world;(2)Theyignoreallorpartofinterac- More specifically, in crowd navigation of robots and au-\ntions among agents, which are important for robot navigation tonomous vehicles, a line of works uses a robot-human at-\nin dense crowds and highly constrained environments. We tention network to determine the relative importance of each\ndiscuss how prior works address the above two problems in human to the robot [3], [8], [48], [49]. However, interactions\nSec. II-C and Sec. II-D respectively. among humans, which can also influence the robot, are not\nexplicitlymodeled.Tothisend,otherworksuseahomogenous\nC. Crowd navigation in constrained environments graph network to include both RH and HH interactions [6],\n[50]. However, since these works feed RH and HH features\nBesides dynamic agents, real-world navigation environ-\nto a single attention network, the resulting robot policy has\nments usually consist of static constraints, such as walls,\ndifficulty reasoning the specificity of each type of feature,\nfurniture, and untraversable terrains. To deal with these con-\nwhich limits the robot’s ability to adapt to different interac-\nstraintsandhumans,somemethodssuchasDWA[1]andDS-\ntions, as demonstrated in [51] and our experiments in Sec. V.\nRNN [8] use groups of small circles to represent the contours\nInaddition,theworksaboveonlydealwithopen-worldsocial\nof obstacles. While groups of circles is a straightforward\nnavigation and thus ignore the interactions between agents\nrepresentation of obstacles, as we will show in Sec. V, they\nand obstacles. To this end, Chen et al. treat the humans,\nare not scalable as the number of obstacles increases and can\nthe robot, and static objects as different types of nodes in\nlead to overfitting problems for learning-based approaches.\na heterogeneous graph attention network [51].\nOther learning-based approaches use raw sensor images or\nThemaindifferencefromourworkisthatChenetal.focus\npointcloudstorepresentthewholeenvironment[7],[25],[33],\non semantic navigation, where a robot must navigate to an\n[34]. These end-to-end (e2e) pipelines have made promising\nobject in simulation. Our work focuses on point-goal naviga-\nprogressinsimulation.However,generalizingthesee2emeth-\ntionincrowdedandconstrainedreal-worldenvironments.This\nods to real-world scenarios is challenging due to domain gaps\ndifference leads to (1) Different representations of obstacles:\nsuch as inaccurate human gait simulation [35], [36]. Despite\nthe object representation in Chen et al. comes from a known\nthe recent advancements incrowd navigation simulators [37]–\nsemantic map of the environment, which is hard to obtain in\n[39] and datasets [26], [40], [41], learning a deployable e2e\nthereal-world.Thus,inourcase,treatingallobstaclesasa2D\npolicy that outperforms human teleoperation in dense and\npoint cloud is more suitable for studying collision avoidance;\ninteractive crowds remains an open challenge.\n(2)Chenetal.learnavaluefunctionandplanpathsassuming\nFurthermore,priorworksthatdemonstratestrongreal-world\nsimplified dynamics for agents, whereas we learn a model-\nperformance in dense crowds and obstacles usually leverage\nfree RL policy and assume unknown state transition for all\nprocessed inputs such as detected human states and processed\nagents, which is more suitable for sim2real transfer where the\nsensor readings [28], [32], [42]. As such, we develop a\ndynamics of pedestrians and robots are uncertain.\nstructuredinputrepresentationofhumansandobstacles,which\nsplitshumanandobstaclerepresentationsandfeedsprocessed\nstates to the policy network. This input representation leads\nIII. PRELIMINARIES\nto strong performance in both simulation and real-world. In In this section, we formulate constrained crowd navigation\ncontrast to the above works with processed inputs that focus asaMarkovDecisionProcess(MDP)andintroducethescene\non pedestrian behavior prediction [32], reward design [28], representation and the reward function.\nand incorporating risks into map [42], our paper proposes\na principled way for network architecture design from the A. MDP formulation\nstructures of complex navigation scenarios.\nWe model the constrained crowd navigation scenario as a\nMDP, defined by the tuple ⟨S,A,P,R,γ,S ⟩. Let wt be the\n0\nD. Graph attention networks for multi-agent interactions\nrobot state which consists of the robot’s position (p ,p ),\nx y\nIn recent years, attention mechanisms have demonstrated velocity (u ,u ), goal position (g ,g ), and heading angle θ.\nx y x y\nsuccess in various fields [43]–[45]. Vaswani et al. propose Let ht be the current state of the i-th human at time t, which\ni\na transformer with self-attention mechanism that achieves consists of the human’s position and velocity (pi,pi,ui,ui).\nx y x y\nstate-of-the-artperformanceinmachinetranslation[43].Later, Let ot be the current observation of the static obstacles and\ngraph attention networks show the effectiveness of attention walls,whichisrepresentedasa2Dpointcloud.Wedefinethe\nFig.2:An overview of our pipeline in simulation and real-world.(a)Ateachtimesteptintrainingandtesting,thesimulatorprovidesarewardrt and\nthe following observations of the environment: obstacle point cloud ot, the robot state wt, and the human states ht,...,ht, and masks Mt (Sec. IV-B1).\n1 n\nThese observations serve as inputs to HEIGHT, which outputs a robot action at that maximizes the future expected return Rt. The simulator executes the\nactionsofallagentsandtheloopcontinues.(b)Thetestingloopinthereal-worldissimilartothesimulatorexcepttheperceptionmodulesforobtaningthe\nobservationsaredifferentandtherewardisabsent.\nstate st ∈S of the MDP as st =[wt,ot,ht,...,ht] if a total\n1 n\nnumber of n humans are observed at the timestep t, where n\nmaychangewithinarangeindifferenttimesteps.Thehumans\nare sorted by an increasing L distance to the robot. The state\n2\nspace S is continuous and our choice of state representation\nis expanded in Sec. III-B.\nIn each episode, the robot begins at an initial state s0 ∈S .\n0\nAs shown in Fig. 2(a), according to its policy π(at|st),\nthe robot takes an action at ∈ A at each timestep t.\nThe action of the robot consists of the desired transla-\ntional and rotational accelerations at = [at ,at ]. The\ntrans rot\naction space A is discrete: the translational acceleration\na ∈{−0.05m/s2,0m/s2,0.05m/s2}andtherotational\ntrans\nacceleration a ∈ {−0.1rad/s2,0rad/s2,0.1rad/s2}.\nrot\nThe translational and rotational velocity is clipped within\n[−0.5m/s,0.5m/s] and [−1rad/s,1rad/s] respectively.\nFig. 3: A split representation of a crowded and constrained navigation\nThe robot motion is governed by the dynamics of TurtleBot\nscenario. In a dynamic scene, (b) low-level human states are detected by\n2i. In return, the robot receives a reward rt (see Sec. III-C sensors and perception modules. (c) To obtain obstacle information, we\nfor details) and transits to the next state st+1 according to remove all humans and compute a point cloud from a known map and the\nrobot’slocation.Inthisway,wecanlearnarobotpolicywithasmallsim2real\nan unknown state transition P(·|st,at). Meanwhile, all other\ngapsusingacheaplow-fidelitysimulator.\nhumansalsotakeactionsaccordingtotheirpoliciesandmove\ndetectors [52]–[54]. By representing each human as a low-\nto the next states with unknown state transition probabilities.\ndimentionalstatevector,weabstractawaydetailedinformation\nThe process continues until the robot reaches its goal, t\nsuch as gaits and appearance, which are difficult to simulate\nexceeds the maximum episode length T = 491 steps, or the\naccurately [35], [37]. We use a 2D point cloud as the obstacle\nrobot collides with any humans or static obstacles.\nrepresentation. Instead of obtaining the point cloud from\nThe goal of the robot is to maximize the expected return,\nRt =E[(cid:80)T γi−tri], where γ is a discount factor. The value sensors, we take advantage of simultaneous localization and\ni=t mapping (SLAM) and first create a map of the environment.\nfunction Vπ(s) is defined as the expected return starting from\nDuringnavigation,inFig.3(c),assumingrobotlocalizationon\ns, and successively following policy π.\nthemapisknown,an“artificial”pointcloudthatonlycontains\nB. State representation static obstacles is computed from the map. Compared to real-\ntime point clouds from sensors, our obstacle representation\nIn our MDP, a state consists of a large and varying number\nis not affected by the presence of humans and thus is less\nofagents.Toaidpolicylearningwithsuchacomplicatedstate\nsensitivetoinaccuratehumansimulations.Thetradeoffisthat\nspace,wedevelopastructuredrepresentationofstatesthatwill\nourmethodrequiresamapandaccuratelocalization.Ifahigh-\nbe fed into the structured policy network. To reduce sim2real\nfidelity simulator is available, the “artificial” point cloud can\ngaps caused by raw sensor readings, our scene representation\nbe replaced by a real-time point cloud from sensors without\nleverages processed information from perception, maps, and\nchanging the rest of our method described below.\nrobotlocalization,whicharerelativelyrobusttodomainshifts.\nIn Fig. 3, at each timestep t, we split a scene into a human\nrepresentation ht,...,ht, and an obstacle representation ot. C. Reward function\n1 n\nIn human representation (Fig. 3(b)), the position and veloc- Our reward function consists of three parts. The first and\nity of each human are detected using off-the-shelf human mainpartofthefunctionawardstherobotforreachingitsgoal\nFig.4:The heterogeneous st-graph and the HEIGHT network architecture.(a)Graphrepresentationofcrowdnavigation.Therobotnodeisw (pink),\nthei-thhumannodeishi (white),andtheobstaclenodeiso(yellow).HHedgesandHHfunctionsareinblue,OAedgesandOAfunctionsareinorange,\nandRHedgesandRHfunctionsareinred.Thetemporalfunctionisinpurple.(b)HEIGHTnetwork.TwoattentionmechanismsareusedtomodeltheHH\nandRHinteractions.WeuseMLPsandaconcatenationforobstacle-agentinteractions,andaGRUforthetemporalfunction.Thesuperscripttthatindicates\nthetimestepandthehumanmaskM iseliminatedforclarity.\nand penalizes the robot for colliding with or getting too close we formulate the navigation scenario as a heterogeneous st-\nto humans or obstacles. In addition, we add a potential-based graph. In Fig. 4a, at each timestep t, our heterogeneous st-\nreward to guide the robot to approach the goal: graph Gt =(Vt,Et) consists of a set of nodes Vt and a set of\nedges Et. The nodes include the detected humans ht,...,ht\n 20, if dt ≤ρ 1 n\nr\n(st,at)=−20, elsego ifal\ndt\nminr ≤obo 0t a thn ed pt oh ie ntro cb loo ut dw ot. fI an lla od bd sit ti ao cn le, san aso abs wta hc ole len .o Ad te eo at chre tp imre es se tn et ps\nmain d 4(t\nm di tn\n−−\n1\n−0.2 d5 t,\n),\ne ol ts he eri wf i0 se<\n.\ndt\nmin\n<0.25 t s, pat th ie als ip na teti ra al cte iod ng ses amth oa nt gc no on dn ee sc .t Dd ii ff ff ee rr ee nn tt spn ao td iae ls ind te en ro at ce tioth ne\ns\ngoal goal\nhave different effects on robot decision-making. Specifically,\n(1)\nsince we have control of the robot but not the humans, RH\nwhere ρ robot is the radius of the robot, dt goal is the L2 interactions have direct effects while HH interactions have\ndistance between the robot and its goal at time t, and dt is indirecteffectsontherobotactions.Asanexampleofindirect\nmin\nthe minimum distance between the robot and any human or effects,ifhumanAaggressivelyforceshumanBtoturntoward\nobstacle at time t. The second part is a spinning penalty r spin the robot’s front, the robot has to respond as a result of the\nto penalize high rotational velocity: interaction between A and B. Additionally, since the agents\nare dynamic but the obstacles are static, interactions among\nr (st,at)=−0.05||ωt||2 (2)\nspin 2 agents are mutual while the influence of static obstacles on\nwhereω isthecurrentrotationalvelocityoftherobot.Finally, agents is one-way. Thus, we categorize the spatial edges into\nt\nweaddanothersmallconstantpenaltyr =−0.025ateach three types: HH edges (blue in Fig. 4), obstacle-agent (OA)\ntime\ntimestep to encourage the robot to finish the episode as soon edges (orange), and RH edges (red). The three types of edges\nas possible. allow us to factorize the spatial interactions into HH, OA,\nIn summary, the reward function of our MDP is the sum of and RH functions. Each function is parameterized by a neural\nthe above three parts: network that has learnable parameters. Compared to previous\nworks that ignore some edges [3], [8], [9], our method allows\nr(st,at)=r (st,at)+r (st,at)+r (3)\nmain spin time the robot to reason about all observed spatial interactions that\nexist in constrained and crowded environments.\nIntuitively, the robot gets a high reward when it approaches\nthegoalwithahighspeedandashortandsmoothpath,while Since the movements of all agents cause the visibility of\nmaintainingasafedistancefromdynamicandstaticobstacles. each human to change dynamically, the set of nodes Vt and\nedges Et and the parameters of the interaction functions may\nIV. METHODOLOGY changecorrespondingly.Tothisend,weintegratethetemporal\ncorrelations of the graph Gt at different timesteps using\nIn this section, we present our approach to decompose the\nanother function denoted by the purple box in Fig. 4(a). The\nconstrained crowd navigation scenario as a heterogeneous st-\ntemporal function connects the graphs at adjacent timesteps,\ngraph, which leads to the derivation of the HEIGHT architec-\nwhichovercomestheshort-sightednessofreactivepoliciesand\nture in a structured manner.\nenables long-term decision-making of the robot.\nToreducethenumberofparameters,thesametypeofedges\nA. Heterogeneous Spatio-Temporal Graph\ninFig.4(a)sharethesamefunctionparameters.Thisparameter\nThe subtle yet highly dynamic interactions among agents sharing is important for the scalability of our graph because\nandentitiesareimportantfactorsthatmakescrowdnavigation the number of parameters is kept constant when the number\ndifficult. To model these interactions in a structured manner, of human changes [55].\nB. HEIGHT Architecture Let V Ht\nH\n= [v 1,...,v n]⊤, where v\ni\n∈ R1×dHH is the value\nembedding of the i-th human. Then, based on Eq. 4, the\nInFig.4b,wederiveournetworkarchitecturefromthehet-\nweighted human embeddings vt ∈Rn×dHH is\nerogeneous st-graph. We represent the HH and RH functions HH\n(cid:32) (cid:33)\nas feedforward networks with attention mechanisms, referred Qt (Kt )⊤ +Mt\nvt =softmax HH H√H ·Vt\nto as HH attn and RH attn respectively. We represent the HH HH\nd\nOA function as an MLP with concatenation, and the temporal\n \nfunction as a gated recurrent unit (GRU). We use W and f to  c q k ··· c q k 0 v 1\n1 1 1 1 1 n−1 .\nden 1o )te Att tr ea nin tia ob nle aw me oi ng ght as ga en nd tsf :ul Tly heco an ttn ee nc tt ie od nl may oe dr us.\nles assign\n=\n\nc\nq. .\n.\nk\n·. ·..\n· c q\n. .\n.\nk\n0. . . · \n v\nn. . −1 \n  (7)\nweights to all edges that connect to a robot or a human n n 1 n n n−1 v\nn\nnode, allowing the node to pay attention to important edges  \nc q k v +···+c q k v\n1 1 1 1 1 1 n−1 n−1\nor interactions. The two attention networks are similar to the .\n= . \nscaled dot-product attention with a padding mask [43], which  . \ncomputes the attention score using a query Q and a key K, c nq nk 1v 1+···+c nq nk n−1v n−1\nand applies the normalized score to a value V, which results where c ,...,c are constants that reflect the effect of the\n1 n\nin a weighted value v. scaling factor d and the softmax function. Thus, the value of\nthen-thmissinghumanv iseliminatedbythemaskMt and\nn\n(cid:18) (QK⊤+M)(cid:19) thus does not affect the resulting weighted human embedding\nv :=Attn(Q,K,V,M)=softmax √ V (4) vt . The mask that indicates the visibility of each human\nHH\nd\nprevents attention to undetected humans, which is common in\ncrowd navigation due to the partial observability caused by\nwhere d is the dimension of the queries and keys and acts as\nthe limited robot sensor range, occlusions, imperfect human\na scaling factor. The mask M is used to handle the changing\ndetectors, etc [31]. Additionally, the mask provides unbiased\nnumber of detected humans at each timestep, as we will\ngradientsto the networks,whichstabilizes andacceleratesthe\nexpand below.\ntraining [9], [56].\nHuman-human attention:Tolearntheimportanceofeach\nRobot-humanattention:Afterthehumansareweightedby\nHH edge to the robot decision at time t, we first weigh each\nHHattention,weweightheirembeddingsagainw.r.t.therobot\nobserved human w.r.t. other humans using an HH attention\nwith another RH attention network to learn the importance\nnetwork,whichisaself-attentionamonghumans.InHHatten-\nof each RH edge. In RH attention, we first embed the robot\ntion, the current states of humans ht,...,ht are concatenated\nand passed through linear layers wi1 th weign hts WQ , WK , state wt with a fully connected layer, which results in the key\nand WV to obtain Qt ,Kt ,Vt ∈\nRn×dH HH\nH,\nwhH eH\nre\nfor RH attention K Rt\nH\n∈ R1×dRH. The query and the value,\nHH HH HH HH Qt ,Vt ∈ Rn×dRH, are the other two linear embeddings\nd is the attention size for the HH attention. RH RH\nHH of the weighted human features from HH attention vt .\nHH\nQt HH =[ht 1,...,ht n]⊤W HQ H =[q 1,...,q n]⊤ Qt RH =v Ht HW RQ H, K Rt H =wtW RK H, V Rt H =v Ht HW RV H\nKt =[ht,...,ht]⊤WK =[k ,...,k ]⊤ (5) (8)\nHH 1 n HH 1 n\nV Ht\nH\n=[ht 1,...,ht n]⊤W HV\nH\n=[v 1,...,v n]⊤ We compute the attention score from Qt RH, K Rt H, V Rt H, and\nthe mask Mt to obtain the weighted human features for the\nwhere q i ∈ R1×dHH, k i ∈ R1×dHH, and v i ∈ R1×dHH are second time vt ∈R1×dRH as in Eq. 4.\nRH\nthe query embedding, key embedding, and value embedding 2) Incoporating obstacle and temporal information: We\nof the i-th human, respectively. first feed the point cloud that represents obstacles, ot, into a\nIn addition, following Eq. 4, a mask Mt ∈Rn×n indicates 1D-CNNfollowedbyafullyconnectedlayertogetanobstacle\nthe visibility of each human to the robot at current time t and embeddingvt .Then,weembedtherobotstateswtwithlinear\nO\nis obtained from the perception system of the robot. Assume layers f to obtain a robot embedding vt .\nR R\nthe n-th human is not visible at time t. Then Mt is a matrix\nvt =f (ot), vt =f (wt) (9)\nfilled with zeros, except that every entry in n-th column is O CNN R R\n−∞. The numerator of Eq. 4 can be express as Finally, the robot and obstacle embeddings are concatenated\nwith the twice weighted human features vt and fed into the\nQt ·(cid:0) Kt (cid:1)⊤ +Mt GRU1: RH\nHH HH\n q 1k\n1\n··· q 1k\nn−1\nq 1k n  0 ··· 0 −∞ ht =GRU(cid:0) ht−1,([v Rt H,v Rt ,v Ot ])(cid:1) (10)\n=\n\n. .\n.\n... . .\n.\n. .\n.\n + . .\n.\n... . .\n.\n. .\n.\n\n\nwhere ht is the hidden state of GRU at time t. Finally, the ht\nq k ··· q k q k 0 ··· 0 −∞ is input to a fully connected layer to obtain the value V(st)\nn 1 n n−1 n n\n  and the policy π(at|st).\nq k ··· q k −∞\n1 1 1 n−1\n=\n\n. .\n.\n... . .\n.\n. .\n.\n\n\n1From experiments, we find that adding a third obstacle-agent attention\nnetwork,wheretheobstacleembeddingisthekeyandtherobotandhuman\nq nk 1 ··· q nk n−1 −∞ embeddingsarethequeryandvalue,doesnotimprovetheperformance.Thus,\n(6) wekeepthesimpleconcatenationdesigntoincorporateobstacleinformation.\n3) Training: We train the entire network with Proximal\nPolicy Optimization (PPO) [57] in a simulator as shown in\nFig. 2(a) (see Sec. V-A for details of the simulator). At each\ntimestep t, the simulator provides all state information that\nconstitutes st, which is fed to the HEIGHT network. The\nnetworkoutputstheestimatedvalueofthestateV(st)andthe\nlogarithmic probabilities of the robot’s action π(at|st), both\nof which are used to compute the PPO loss and then update\nthe parameters in the network. In training, the robot action\nis sampled from the action distribution π(at|st). In testing,\nthe robot takes the action with the highest probability at. The\nrobot action at is fed into the simulator to compute the next\nstate st+1, and then the loop continues.\nWithout any supervised learning, our method is not limited\nby the performance of expert demonstrations [3], [4], [50].\nHowever, to improve the low training data efficiency, an\ninherent problem of RL, HEIGHT can also be trained with Fig. 5: Simulation and real-world environments. (a) In our simulation\na combination of imitation learning and RL. benchmark, the colored cylinders denote humans and gray objects denote\nobstacles. Each episode is randomized (Sec. V-A). (b) and (c) are everyday\n4) Summary: We present a structured and principled ap-\nindoorenvironmentsforsim2real.Beforereal-worldtestingin(b)and(c),the\nproachtodesigntherobotpolicynetworkforcrowdnavigation robotpolicyistrainedin(d)and(e)respectively(Sec.VI-A).\nin constrained environments. By decomposing the complex\nscenario into independent components, we split the complex of Table I. The heights of humans and obstacles are all above\nproblem into smaller functions, which are used to learn the the height of the robot LiDAR to ensure detectability.\nparameters of the corresponding functions. By combining all To simulate a continuous human flow similar to [8], [9],\ncomponents above, the end-to-end trainable HEIGHT allows [28], dynamic humans will move to new random goals im-\nthe robot to perform spatial and temporal reasoning on all mediately after they arrive at their goal positions or they\npairwiseinteractions,leadingtobetternavigationperformance. get stuck in front of narrow passageways for more than 10\ntimesteps.AlldynamichumansarecontrolledbyORCA[13].\n80% of dynamic humans do not react to the robot and 20%\nV. SIMULATIONEXPERIMENTS\nof humans react to the robot. This mixed setting prevents\nIn this section, we present our environment, experiment our network from learning an extremely aggressive policy in\nsetup,andresultsinsimulation.Ourexperimentsareguidedby which the robot forces all humans to yield while achieving\nthefollowingquestions:(1)Whatistheadvantageofoursplit a high reward. Meanwhile, the simulation maintains enough\nscene representation compared with alternative representa- reactive humans to resemble the real crowd behaviors. We\ntions?(2)Whatistheimportanceofthegraphformulationand use holonomic kinematics for humans. The preferred speed\nwhy do we differentiate types of edges with a heterogenous of humans ranges from 0.4m/s to 0.6m/s to accommodate\nst-graph? (3) What is the importance of HH and RH attention the speed of the robot. We assume that humans can achieve\nin HEIGHT? (4) What are the failure cases of our method? the desired velocities immediately, and they will keep moving\nwith these velocities for the next ∆t seconds. The simulation\nand control frequency ∆t is 0.1s. The maximum length of an\nA. Simulation environment\nepisode is 491 timesteps or 49.1s.\nWe conduct simulation experiments in random environment\ninFig.5(a)developedwithPyBullet[58].Therobot,humans,\nB. Experiment setup\nandstaticobstaclesareina12m×12marena.Ineachepisode,\nrectangular obstacles are initialized with random shapes and We now introduce the baselines and ablation models, train-\nrandomposes.Thewidthandthelengthofeachstaticobstacle ing procedure, and evaluation metrics.\naresampledfromN(1,0.62)andarelimitedin[0.1,5]meters. 1) Baselines: We compare the performance of our method\nThe initial positions of the humans and the robot are also with the following baselines:\nrandomized. The starting and goal positions of the robot are • Dynamic window approach (DWA)[1]searchesforthe\nrandomly sampled inside the arena. The distance between the optimal velocity that brings the robot to the goal with\nrobot’sstartingandthegoalpositionsisbetween5mand6m. the maximum clearance from any obstacle. DWA is a\nSome humans are dynamic and some are static. The goals of model-basedmethodthatonlyconsidersthecurrentstate.\ndynamic humans are set on the opposite side of their initial In addition, DWA represents both humans and obstacles\npositions to create circle-crossing scenarios. In training, the as groups of small circles.\nnumberofhumansvariesfrom5to9andthenumberofstatic • A∗+CNN [7] is a hybrid method. With a map of the\nrectangular obstacles varies from 8 to 12. Among all humans, environment, A∗ is the global planner and generates 6\n0-2 of them are static and the rest are dynamic. In testing, the waypoints in the beginning of an episode. The inputs\nnumberofhumansandobstaclesareshowninthefirstcolumn to the robot RL policy are a 2D LiDAR point cloud,\nthe robot state, the waypoints, and the robot’s final goal. generalization of methods, we change the range of human or\nNo human detections are used and human features are obstaclenumbersintheremainingfourenvironmentstovalues\nincluded in the point cloud. The point cloud is passed that do not overlap with those used in training. Thus, these 4\nthrough a CNN local planner, whose output is concate- environments are out-of-distribution (OOD).\nnated with the embedding of other inputs. In addition to All testing scenarios, including those in the training dis-\nthe reward function in Eq. 1, the robot receives a small tribution, are unseen during training. This is because in each\nreward of 2 if it arrives at any waypoint in sequence. testing episode, a new seed that is not used in training deter-\n• DecentrializedstructuralRNN(DS-RNN)[8]isanRL- mines the number of humans and obstacles, the starting and\nbased method that represents static obstacles as groups goalpositionsofallagents,andthesizeandposeofobstacles.\nof small circles. In network architecture, DS-RNN only Onlythe4arenawallsdonotchangeacrossdifferentepisodes.\ncontains the RH attention and the GRU. The testing metrics measure the quality of the navigation\n• Homogeneous graph attention network (Homo- and include the success rate, collision rate with humans and\nGAT)[44]isRL-basedandsplitshumanandobstaclein- obstacles, timeout rate, the average navigation time of suc-\nputs.However,HomoGATdoesnotdifferentiatebetween cessful episodes in seconds, and path length of the successful\n3 types of nodes and 3 types of edges in policy network. episodes in meters.\nInstead, HomoGAT uses a single self-attention network\nto weigh humans, the robot, and the obstacle point cloud\nC. Results\nandfeedtheweightedsumofallembeddingsintoaGRU.\nHomoGATissimilartoChenetal.[6]andLiuetal.[50] 1) Effectiveness of scene representation: To analyze the\nbut the input, output, and training algorithm are kept the effects of input scene representations on crowd navigation\nsame as our method for a fair comparison. algorithms,wecompareoursandHomoGAT,thetwomethods\nIn summary, DWA, A∗+CNN, and DS-RNN mix humans thatdistinguishhumanandobstacleinputs,withbaselinesthat\nmix humans and obstacles in input representation: the model-\nand obstacles in both input representations and navigation\nbased DWA, the RL-based DS-RNN, and the hybrid planner\nalgorithm. HomoGAT only mix humans and obstacles in\nA∗+CNN. The results are shown in Table I.\nalgorithm but not in input, while ours distinguishes them in\nFor DWA, treating humans as obstacles leads to the highest\nboth input and algorithm.\naverage human collision rates (0.54) and a freezing problem,\n2) Ablations: To show the benefits of each attention net-\nindicated by the highest average timeout rates (0.21), in all\nwork, we perform an ablation study on the two attention\nenvironments. For example, the robot in Fig. 6 stays close to\nmodels. The ablated models are defined as follows:\neverything and thus fails to avoid the magenta human in time.\n• No attn: Both RH and HH attention networks are re- Similarly, by representing obstacles as groups of circles,\nmoved.Noattnmodelonlyhastheembeddinglayersfor\nDS-RNN performs better in the More crowded environ-\nthe inputs and the GRU.\nment (Fig 7(d)) than in the More constrained environment\n• RH: The HH attention network is removed and the hu- (Fig 6(d)). In addition, Table I shows that DS-RNN achieves\nmans are weighted only once w.r.t. the robot. Everything\nthe highest average collision rate with obstacles (0.13) in all\nelse is the same as ours.\nenvironments. Furthermore, the obstacle collision rate of DS-\n• HH: The RH attention network is removed and the RNN increases in all 4 OOD environments, indicating an\nhumans are weighted only once w.r.t. other humans.\noverfitting problem. Among the OOD environment, the per-\nEverything else is the same as ours.\ncentageofobstaclecollisionincreaseishigherinenvironments\n• RH+HH (ours): The full network as shown in Fig. 4(b). with higher obstacle-to-human ratios. For example, in Less\n3) Training: We train all RL methods, including all ba- crowded withthehighestpercentageofobstacles,theobstacle\nseinesandablationsexceptDWA,for2×108 timestepswitha collision rate (0.21) increases by 133% w.r.t. the obstacle\nlearningrate5×10−5.Thelearningratedecaysatalinearrate collision rate in training distribution (0.09). On the contrary,\nwith respect to training timesteps. To accelerate and stabilize in More crowded with the lowest percentage of obstacles,\ntraining,werun28parallelenvironmentstocollecttherobot’s the obstacle collision rate (0.06) drops by 33%. The reason\nexperiences.At eachpolicy update,30steps of6 episodesare is that DS-RNN has trouble inferring the geometric shapes\nused. We train and test all methods in a commercial desktop of obstacles from a large group of circles, and thus fails to\ncomputer with an Intel Core i9-13900K processor, 32 GB avoid collision with them. Thus, treating both humans and\nmemory,andaNVIDIARTX4090GPU.Trainingourmethod obstacles as circles is not an ideal input representation for\ntakes approximately 48 hours. robot navigation algorithms.\n4) Evaluation: We test all methods with the same 500 For A∗+CNN, the A∗ global planner does not account for\nrandom unseen test episodes. For RL-based methods, we test humansandtheCNNlocalplannerrepresentsallobservations\nthe last 5 checkpoints (equivalent to the last 6000 training as a 2D point cloud. As a result, from Table I, A∗+CNN has\nsteps) and report the result of the checkpoint with the highest thehighestaveragetimeoutrate(0.10)andthelongestaverage\nsuccess rate. We conduct the testing in 5 different human and time (18.99) among RL-based methods in all environments,\nobstacledensities,asshowninthefirstcolumnsofTableIand especially the two most challenging ones, for the following 2\nTable II. The first set of densities is the same as the training reasons.(1)Thewaypointscanfailtoguidetherobottoreach\nenvironment and is thus in training distribution. To test the thegoalbecausethewaypointsloseoptimalityasagentsmove.\nTABLE I: Baseline comparison results with different human and obstacle densities in unseen environments\nCollision↓\nEnvironment Method Success↑ Timeout↓ NavTime↓ PathLen↓\nOverall w/Humans w/Obstacles\nDWA[1] 0.16 0.68 0.63 0.05 0.15 28.45 11.52\nTrainingdistribution A∗+CNN[7] 0.64 0.29 0.28 0.01 0.07 25.72 12.30\n5-9humans DS-RNN[8] 0.80 0.20 0.11 0.09 0.00 19.90 10.78\n8-12obstacles HomoGAT[44] 0.86 0.14 0.13 0.01 0.00 18.66 10.36\nHEIGHT(ours) 0.88 0.12 0.09 0.03 0.00 18.31 10.34\nDWA[1] 0.29 0.37 0.28 0.09 0.34 25.74 14.42\nLesscrowded A∗+CNN[7] 0.84 0.12 0.11 0.01 0.04 22.64 11.73\n0-4humans DS-RNN[8] 0.72 0.28 0.07 0.21 0.00 17.21 9.56\n8-12obstacles HomoGAT[44] 0.95 0.05 0.03 0.02 0.00 16.64 10.17\nHEIGHT(ours) 0.97 0.03 0.02 0.01 0.00 16.32 10.04\nDWA[1] 0.11 0.78 0.74 0.04 0.11 29.60 10.37\nMorecrowded A∗+CNN[7] 0.47 0.42 0.39 0.03 0.11 27.33 12.47\n10-14humans DS-RNN[8] 0.76 0.22 0.16 0.06 0.01 23.08 11.67\n8-12obstacles HomoGAT[44] 0.72 0.28 0.23 0.05 0.00 20.46 10.57\nHEIGHT(ours) 0.78 0.22 0.19 0.03 0.00 19.69 10.39\nDWA[1] 0.17 0.57 0.55 0.02 0.26 28.50 14.21\nLessconstrained A∗+CNN[7] 0.66 0.29 0.28 0.01 0.05 23.63 11.98\n5-9humans DS-RNN[8] 0.66 0.34 0.20 0.14 0.00 18.01 9.76\n3-7obstacles HomoGAT[44] 0.88 0.12 0.10 0.02 0.00 17.66 10.37\nHEIGHT(ours) 0.90 0.10 0.09 0.01 0.00 17.20 10.23\nDWA[1] 0.14 0.66 0.49 0.17 0.20 30.47 11.35\nMoreconstrained A∗+CNN[7] 0.48 0.29 0.23 0.06 0.23 27.28 13.11\n5-9humans DS-RNN[8] 0.71 0.23 0.09 0.14 0.06 23.45 11.58\n13-17obstacles HomoGAT[44] 0.80 0.20 0.11 0.09 0.00 19.20 10.51\nHEIGHT(ours) 0.84 0.15 0.07 0.08 0.01 18.79 10.65\nConsequently, in OOD scenarios such as Fig. 6(b), the CNN A∗+CNN also exhibits larger variances in terms of success\npolicy is especially bad at long-horizon planning because it rate (0.019 v.s. 0.0060), navigation time (3.62 v.s. 1.70), and\nis overfitted with good waypoints. (2) By mixing humans and path length (0.22 v.s. 0.019) across different OOD environ-\nobstacles in its input, the robot policy sometimes has a hard ments. Especially in the challenging More crowded and More\ntimedistinguishingdynamicandstaticobstacles.Forexample, constrained environments, A∗+CNN exhibits a larger drop\nin Fig. 6(b), the robot keeps a large distance from everything in average success rate compared with HomoGAT (0.165 v.s.\nand thus is less agile and efficient compared with the robots 0.10).Thisfindingshowsthatevenasimplest-graphstructure\nin Fig. 6(e) and (f). Thus, for RL-based approaches, treating can lead to a performance gain, indicating the importance of\nhumans as obstacles leads to overfitting to specific types of spatial and temporal reasoning for robot crowd navigation.\nscenarios, preventing the policies from generalizing to OOD However,occasionalfailuresofHomoGATstillexistsuchas\nscenarios which are common in the real-world. Fig.6(b).WithoutdifferentiatingRHandOAinteractions,the\nBy splitting human and obstacle input representations, Ho- robot maintains similar distances from humans and obstacles,\nmoGAT and HEIGHT achieve the top 2 performance across yet fails because RH interactions require more space. As a\nmostmetricsandenvironments.Especially,HEIGHTlearnsto step further, with a heterogeneous st-graph, HEIGHT treats\nkeepalargerdistancefromhumanpathsyetashorterdistance the 3 types of edges with different network components. As\nfrom obstacles to balance safety and efficiency (Fig. 6(f)), a result, HEIGHT outperforms HomoGAT and achieves the\nindicated by the lowest overall collision rates (0.17) and the best average success rate (0.87), navigation time (18.06), and\nshortest average navigation time (18.10) in all environments. path length (10.33) in all environments. The variance success\nTherefore,weconcludethatoursplitrepresentationconsisting rate (0.0040) and navigation time (1.40) are also the best,\nof detected human states and obstacle point clouds is most whereas the path length variance (0.040) is the runner-up\nsuitable to learn robot navigation in crowded and constrained across all environments. In addition, the average success rate\nenvironments with RL, among all popular choices in Table I. drop in the two challenging environments of HEIGHT is\n2) Effectiveness of the heterogeneous st-graph: To justify smaller than HomoGAT (0.07 v.s. 0.10). The reason is that\nour heterogeneous st-graph formulation, we compare our theheterogeneouscomponentsallowtherobottoreasonabout\nmethodwithA∗+CNNwithnographconceptandHomoGAT the different effects of HH, RH, and OA spatial interactions.\nwith a homogenous graph attention network in Table I. For example, in Fig. 6(f) and Fig. 7(e), HEIGHT chooses a\nA∗+CNN lacks structure in input representation, and sub- path that avoids the most crowded region, yields to humans\nsequently, lacks structure in network architecture. For this whentherobotmustencounterthem,andstaysclosertowalls\nreason, as we discussed in Sec. V-C1, A∗+CNN shows much for efficiency. Therefore, we conclude that the spatial and\nworse average success rate (0.62 v.s. 0.84), navigation time temporal reasoning on different types of interactions is the\n(25.32 v.s. 18.52), and path length (12.32 v.s. 10.40) than key to ensuring good in-distribution performance and OOD\nHomoGAT which has a simple st-graph in all environments. generalization in crowded and interactive environments.\nTABLE II: Ablation study results with different human and obstacle densities in unseen environments\nCollision↓\nEnvironment Method Success↑ Timeout↓ NavTime↓ PathLen↓\nOverall w/Humans w/Obstacles\nNoattn 0.52 0.46 0.41 0.05 0.02 26.48 11.81\nTrainingdistribution\nRH 0.85 0.15 0.13 0.02 0.00 18.86 10.41\n5-9humans\nHH 0.87 0.12 0.10 0.02 0.01 18.31 10.42\n8-12obstacles\nRH+HH(ours) 0.88 0.12 0.08 0.04 0.00 18.49 10.28\nNoattn 0.72 0.27 0.24 0.03 0.01 21.78 10.94\nLesscrowded\nRH 0.89 0.07 0.04 0.03 0.04 17.34 10.66\n0-4humans\nHH 0.94 0.06 0.05 0.01 0.00 16.10 9.90\n8-12obstacles\nRH+HH(ours) 0.97 0.03 0.02 0.01 0.00 16.32 10.04\nNoattn 0.29 0.61 0.53 0.08 0.10 28.35 12.57\nMorecrowded\nRH 0.70 0.29 0.25 0.04 0.01 20.39 10.49\n10-14humans\nHH 0.74 0.25 0.17 0.08 0.01 20.34 10.53\n8-12obstacles\nRH+HH(ours) 0.78 0.22 0.19 0.03 0.00 19.69 10.39\nNoattn 0.55 0.43 0.42 0.01 0.01 24.64 11.75\nLessconstrained\nRH 0.86 0.14 0.12 0.02 0.00 18.17 10.33\n5-9humans\nHH 0.88 0.12 0.10 0.02 0.00 17.24 10.25\n3-7obstacles\nRH+HH(ours) 0.90 0.10 0.09 0.01 0.00 17.20 10.23\nNoattn 0.43 0.49 0.38 0.11 0.08 26.25 11.82\nMoreconstrained\nRH 0.77 0.22 0.11 0.11 0.01 20.21 10.73\n5-9humans\nHH 0.84 0.16 0.06 0.10 0.00 18.94 10.66\n13-17obstacles\nRH+HH(ours) 0.84 0.15 0.07 0.08 0.01 18.79 10.65\n3) Importance of attention networks: We use ablations to in future work, our method can be combined with long-\nevaluate the contribution of RH and HH attention networks to termpredictionandpathplanningalgorithmsthatconsiderthe\nthe performance of HEIGHT, as shown in Table II. stochasticity of pedestrian motions.\nFirst, in terms of all metrics in most environments, the\nmodel with both RH and HH attention shows the strongest 5) Additionalinsights: Besidesansweringthefourresearch\nresults, followed by the models with only one attention, and questionsabove,weprovide additionalinsightsobtainedfrom\nfinallybythemodelwithnoattention.Forexample,inFig.7, simulation experiments below.\nthe ablations in (a), (b), and (c) fail yet our full model in (e)\nsucceeds. Especially in the 2 most challenging More crowded Effectiveness of RL planning: In Table I, DWA is a model-\nand More constrained environments, the existence of either based approach that determines robot actions based on only\nRH or HH attention significantly boosts the average success the current state, which leads to the worst performance. Thus,\nrateby0.38and0.43respectively,comparedwithNoattn.For reactive policies are not sufficient to solve our problem,\nexample,inFig.6(e),therobotdetoursandgoesdangerously justifying the necessity of long-sighted planning. In contrast,\nclose to the crowds in a constrained area, yet HH attention A∗+CNN combines planning and RL, yet relies on occu-\nallows it to realize the danger and resume to a less crowded pancy maps with only obstacles and not humans. As humans\npath. This result shows that reasoning about both HH and RH move, the increasingly inaccurate occupancy map reduces\nspatial relationships is essential for our problem, especially in the optimality of waypoints, which negatively affects overall\nmore crowded or more constrained environments where the navigation. On the other hand, RL learns to maximize the\nspatial interactions are also dense. expected long-term returns based on both the current and his-\ntoricalstatesofallcomponentsinthescene.Consequently,the\nSecond, by comparing RH and HH, we find that HH\nremaining3RLmethods,especiallyHomoGATandHEIGHT,\nattention plays a more important role in all environments in\noutperform A∗+CNN in most metrics and in most environ-\nterms of success rate and navigation time. This is because the\nments. Thus, to ensure the best navigation performance, it is\nnumber of HH edges is larger than the number of RH edges\nimportant to optimize the entire planning system based on the\nin most cases. Thus, the robot can observe and is affected by\ntask setting instead of optimizing only a part of it.\na relatively larger number of HH interactions yet a smaller\nnumber of RH interactions.\nDifficulty of scenarios: From Table I and Table II, we\n4) Failurecases: Byvisualizingthetestingepisodesacross observethat,besidestheoverfittedDS-RNN,allmethodsshow\nall environments, we find that HEIGHT typically collides the same trend of performance change in the 5 environments:\nwhen (1) a human arrives at its goal and suddenly changes Less crowded > Less constrained > More constrained >\ndirections to a new goal, as shown in Fig. 8(a), or (2) the More crowded. Obviously, adding more humans or obstacles\nrobot surroundings are extremely crowded and constrained, increases task difficulty. But interestingly, the change in the\nandalmostallfreepathsareblockedbyhumans(Fig.8(b)).In number of humans has a larger effect on the task difficulty\nthesecases,duetoitsspeedlimit,therobotsometimescannot thanthechangeinthenumberofobstacles.Thisphenomenon\nswitch to an alternative path in time to prevent collisions. To shows that avoiding collisions with humans is more difficult\nremedythedifficultyofRLinlong-termdecisionmaking[59], than avoiding obstacles in nature.\nFig.6:Comparison of different methods in the same testing episode in More Constrained environment.Therobotiscenteredinwhitecirclesandits\norientationisdenotedbywhitearrows.Morequalitativeresultscanbefoundinthevideoattachmentandathttps://sites.google.com/view/crowdnav-height/home.\nVI. REAL-WORLDEXPERIMENTS test the robot’s ability to handle constraints with low density\nIn this section, we present our hardware setup and sim2real crowds. In the lounge environment, we test the robot’s ability\ntesting results in everyday environments with pedestrians and toavoiddensecrowdsandobstacleswithmorediverseshapes.\nstatic constraints. The distance between the starting and the goal position of the\nrobot ranges from 6m to 11m. The pedestrians were told to\nreact naturally to the robot based on their own preferences. In\nA. Experiment setup\nsometestingepisodes,otherpedestrianswhowereunawareof\nWetrainthesim2realpoliciesinthesimulatorsofahallway\nour experiment also engaged with the robot.\n(Fig. 5(b)) and a lounge (Fig. 5(c)) for 2×108 timesteps with\na decaying learning rate 5×10−5. To learn robust policies, TheconfigurationofhardwaretestingisshowninFig.2(b).\nwe inject noises into the agent positions and robot control. We use a TurtleBot 2i with an RPLIDAR-A3 laser scanner.\nThen, as shown in Fig. 5 (d) and (e), we test the policies in We first use the ROS gmapping package to create a map of\nthe two corresponding everyday environments in a university the environment. Then, we process the map to combine small\nbuilding without any additional training. In the hallway obstacles and eliminate noises. To reduce sim2real gap of the\nenvironment where the free space is extremely narrow, we LiDAR point clouds, we use artificial point clouds obtained\nFig.7:ComparisonofdifferentmethodsinthesametestingepisodeinMoreCrowded environment.\nFig.8:Failure cases of our method.(a)Theblackhumanwasgoingdownwardsforawhile,butsuddenlychangesitsdirectiontowardtherobot.(b)All\nefficientpathstowardthegoalareblockedorwillsoonbeblockedbyhumancrowds.\nfromlocalizationandmapping,insteadoftherawpointclouds are the same, while the number and trajectories of pedestrians\nfrom LiDAR. The reason is that the artificial point clouds are are similar. We measure the success, collision, timeout rates,\ncleaner and are not obstructed by humans. When a navigation and navigation time of successful episodes as testing metrics.\ntrial begins, a user inputs the robot goal position through\na keyboard. To detect human positions, we use an off-the- B. Results\nshelfpeopledetector[52].WeuseanIntelRealSensetracking In the highly constrained yet less crowded Hallway envi-\ncamera T265 to obtain the pose of the robot (p x,p y,θ) and ronment, ROSnavigation stack oftenneeds to spin inplace to\nthe robot wheel encoder to obtain its velocity (u x,u y). replan, as shown by the higher navigation time in Table III.\nOur baseline is the ROS navigation stack, which uses the Some of the spinning recovery attempts fail and result in\ndynamicwindowapproach(DWA)[1]asthelocalplannerand timeouts. In ROS navigation stack, both global and local\nA∗ as the global planner. For each method, we run 30 trials planners treat humans as obstacles. As a result, similar to the\nin total. Among all trials, the start goal positions of the robot baselinesinSec.V-C1,therobothasdifficultiesdistinguishing\nFig.9:AtestingepisodeofourmethodintherealHallwayenvironment.Thebluearrowdenotestherobotpaththatresultsfromitsactions.Theredstar\ndenotesthegoalposition.Inthisepisode,theturtlebotavoidstwopedestrians,oneafteranotherinanarrowcorridor,entersanarrowdoorway,andarrives\natthegoal.Morequalitativeresultscanbefoundinthevideoattachmentandathttps://sites.google.com/view/crowdnav-height/home.\nFig.10:AtestingepisodeofourmethodintherealLoungeenvironment.Theturtlebotavoidsmultiplegroupsofpeoplewhopasseachotherindifferent\nheadingdirections,avoidsthewallsandfurnitures,andarrivesatthegoal.\nTABLE III: Real-world results in 2 everyday environments\nEnvironment #oftrials Method Success↑ Collision↓ Timeout↓ NavTime↓\nHallway NavigationStack 0.72 0.06 0.22 16.71\n12\n1-2humans HEIGHT(ours) 1.00 0.00 0.00 22.36\nLounge NavigationStack 0.83 0.17 0.00 32.00\n18\n1-6humans HEIGHT(ours) 0.83 0.17 0.00 30.71\ndynamic obstacles that will clear out and static obstacles that differenttypesofspatio-temporalinteractions.Forexample,in\nwillalwaysstay,causingfailuresinhighlyconstrainedspaces. Fig.10(a)-(c),sincepedestrianswalkinginoppositedirections\nOn the contrary, by using different representations of humans are crossing each other, the robot first turns left to avoid the\nand obstacles, our method is able to explore the different ladyonitsright,andthenturnsrighttoavoidthetwomaleson\nstrategies to avoid them through trial and error during RL its left. Then, in Fig. 10 (d)-(e) and (f)-(g), the robot chooses\ntraining. Consequently, the robot demonstrates high success actions that deviate from the current and intended paths of\nratesinTableIII.Qualitatively,therobotisabletosafelypass pedestrians, walls, and the table to safely arrive at the goal.\nahumaninanarrowcorridor(Fig.9(a)-(d)),takesawideturn The failure cases of both methods are caused by the tables\ntogiveahumanenoughroom,andentersanextremelynarrow withirregular3Dshapes.Duetotheheightofthe2DLiDAR,\ndoorway (Fig. 9 (e)-(h)). somewidepartsofthetablesarenotdetectedormapped,and\nIn the less constrained yet more crowded Lounge en- thus the robot cannot avoid them. Further pre-processing of\nvironment, ROS navigation stack and ours have the same the LiDAR point clouds or adding 3D sensors can mitigate\nsuccess and collision rate, because the Lounge environment this problem.\nhasenoughspaceforthenavigationstacktoapplya“stopand\nwait” strategy for humans and obstacles. However, as a cost, Furthermore, the successful sim2real transfer from a cheap\n“stopandwait”takesalongeraveragetimetoarriveatgoalsas and low-fidelity simulator to complex everyday environments\nshowninthelastcolumnofTableIII.Differentfromthisna¨ıve demonstrates the robustness of our input representation and\nstrategy, HEIGHT adapts its navigation behaviors based on the cost-efficiency of our pipeline design.\nVII. DISCUSSIONS,LIMITATIONS,ANDFUTUREWORK humandetectionsorrobotlocalization.Tothisend,itisworth\nexploring the joint optimization of the whole robotic stack\nDeep RL is a promising tool to solve robotic problems that\nfrom perception to control [64], [65].\nare beyond the capabilities of traditional rule-based methods\nwithout large-scale real-world datasets. However, preventing\nthe performance degradation of a deep RL policy when in- C. Structured neural network\nevitable distribution shifts happen, especially in real-world, is\nModel-based approaches require low-fidelity data yet heav-\nchallenging. In this section, we reflect on the key components\nily rely on assumptions. In contrast, end-to-end learning ap-\nofourframework,discussthelimitationsofourapproach,and\nproaches need few assumptions yet require high-fidelity data.\npropose directions for future research.\nOur structured learning method combines the best of both\nworlds: It requires low-fidelity data yet relies on minimal\nA. Sim2real through Real2sim assumptions. By injecting structures into the neural network,\nwe decompose a complex problem into smaller and relatively\nTo overcome sim2real gaps, the design of simulation\nindependent components. Note that our decomposition does\npipelinesneedstobeguidedbytheconstraintsofhardwareand\nnot break the gradient flow, which keeps HEIGHT end-to-\nenvironments in the real-world. On one hand, we determine\nend trainable. We propose a principled way for network\nthe input representation of HEIGHT based on what could be\narchitecturedesign,increasingthetransparencyoftheseblack-\nobtained from sensors and off-the-shelf perception modules\nboxes. Our experiments demonstrate that the structured net-\nand how accurate they are. We find that intermediate features,\nwork outperforms both model-based methods and RL-based\nsuch as detected human positions and processed point clouds,\nmethods without structures, which empirically proves the\nreduces sim2real gaps. On the other hand, we also ensure the\neffectiveness of structure learning for interactive tasks with\nconsistencyofthesimulationandreal-world,suchastherobot\nmultiple heterogeneous participants.\naction space, whenever possible. These design choices allow\nour policy to generalize to different simulation environments\nand deploy to challenging real-world scenarios. D. Training method\nHowever, although we have minimized the sim2real gaps Deep RL enables the robot to explore the environment and\nthrough real2sim, certain gaps still exist. In real-world exper- learn meaningful behaviors through trial and error. Without\niments, the difficulty of the task is reduced and the agility of heuristics or demonstrations, the robot has to collect reward\nthe robot policy is only partially transferred from simulation. signals to improve its policy. Consequently, simulation de-\nTo further align the simulation and the real-world, we plan velopment with a randomization scheme and a good reward\nto explore the following directions for future work: (1) devel- design are indispensable to the performance of our method.\noping a more natural pedestrian model to replace the ORCA Nevertheless, our method is subjective to the inherent limita-\nhumans in the simulator, (2) revising our pipeline to enable tionsofRL,suchaslowtrainingdataefficiencyanddifficulties\nself-supervised RL fine-tuning in the real-world [60], [61], inlong-horizontasks.Infuturework,combiningtheRLpolicy\n(3) using a small amount of real-world data to automatically with other traditional planners or imitation learning has the\noptimize the parameters of our simulator to match the real- potential to alleviate these problems [36].\nworld environment [62], [63].\nVIII. CONCLUSION\nB. Scene representation\nIn this article, we proposed HEIGHT, a novel structured\nA good scene representation is tailored to the needs of its graph network architecture for autonomous robot navigation\ndownstream task. Besides the above sim2real considerations, indynamicandconstrainedenvironments.Ourapproachtakes\nour scene representation is split due the different nature of advantage of the graphical nature and decomposability of\nhumans and obstacles for robot collision avoidance. The size the constrained crowd navigation problem, introducing the\nof humans are small and their shapes are simple. Therefore, following two key novelties. First, we split and process the\nto avoid humans, the robot only needs to treat them as circles human and obstacle representations separately. This allows\nwithaheadingdirection.Incontrast,obstacleshavelargerand the robot to effectively reason about the different geometrics\nmore complicated surfaces. The part of the obstacle contours and dynamics of humans and obstacles, improving its ability\nthat faces the robot is more important for collision avoidance. to navigate complex environments. Second, we propose a\nTherefore,pointcloudsarethemostintuitivewaytorepresent heterogeneousst-graphtocapturevarioustypesofinteractions\nsuch useful information about obstacles. Our experiments in among the robot, humans, and obstacles. The decomposition\nSec. V show that this split scene representation reduces robot of the scenario as a heterogeneous st-graph guides the design\ncollision avoidance with both dynamic and static obstacles. oftheHEIGHTnetworkwithattentionmechanisms.Attention\nA side effect of our scene abstraction is the loss of detailed enables the robot to reason about the relative importance of\ninformation such as gaits of humans and 3D shapes of obsta- each pairwise interaction, leading to adaptive and agile robot\ncles. However, we argue that this is a tradeoff to minimize decision-making during navigation.\nsim2real gaps with limited simulation tools and computation Our simulation experiments show that the HEIGHT\nresources. Another side effect is the cascading errors between modeloutperformstraditionalmodel-basedmethodsandother\nthe perception modules and robot policy, such as inaccurate learning-based methods in terms of collision avoidance and\nnavigation efficiency. The HEIGHT model also demonstrates [15] L. Huber, J.-J. Slotine, and A. Billard, “Avoiding dense and dynamic\nimproved generalization in environments with varied human obstacles in enclosed spaces: Application to moving in crowds,” IEEE\nTransactionsonRobotics,vol.38,no.5,pp.3113–3132,2022.\nand obstacle densities. In real-world environments, HEIGHT\n[16] C. Mavrogiannis, K. Balasubramanian, S. Poddar, A. Gandra, and\nis seamlessly transferred from simulation to everyday indoor S. S. Srinivasa, “Winding through: Crowd navigation via topological\nnavigation scenarios without additional training, showcasting invariance,” IEEE Robotics and Automation Letters, vol. 8, no. 1, pp.\n121–128,2023.\nits robustness and ability to overcome the sim-to-real gap.\n[17] OpenSourceRoboticsFoundation,“ROSNavigationStack,”http://wiki.\nOur work suggests that reasoning about subtle spatio- ros.org/navigation,2007,accessed:2024-11-11.\ntemporal interactions is an essential step toward smooth [18] M. Dobrevski and D. Skocˇaj, “Dynamic adaptive dynamic window\napproach,” IEEE Transactions on Robotics, vol. 40, pp. 3068–3081,\nhuman-robotinteraction.Furthermore,ourworkhighlightsthe\n2024.\nsignificance of uncovering the inherent structure of complex [19] P. Long, T. Fan, X. Liao, W. Liu, H. Zhang, and J. Pan, “Towards\nproblems and injecting these structures into learning frame- optimally decentralized multi-robot collision avoidance via deep rein-\nforcementlearning,”inIEEEInternationalConferenceonRoboticsand\nworks to solve the problems in a principled manner.\nAutomation(ICRA),2018,pp.6252–6259.\n[20] P.TrautmanandA.Krause,“Unfreezingtherobot:Navigationindense,\ninteractingcrowds,”inIEEE/RSJInternationalConferenceonIntelligent\nACKNOWLEDGEMENTS RobotsandSystems(IROS),2010,pp.797–803.\n[21] Z.Liu,W.Na,C.Yao,C.Liu,andQ.Chen,“Relaxingthelimitationsof\nWethankZheHuangforhelpfuldiscussionsandcodeofthe\nthe optimal reciprocal collision avoidance algorithm for mobile robots\nA* planner. We thank Aamir Hasan and Rutav Shah for feed- in crowds,” IEEE Robotics and Automation Letters, vol. 9, no. 6, pp.\nback on paper drafts. We thank members in Human-Centered 5520–5527,2024.\n[22] P. Long, W. Liu, and J. Pan, “Deep-learned collision avoidance policy\nAutonomy Lab who participated in real-world experiments.\nfor distributed multiagent navigation,” IEEE Robotics and Automation\nLetters,vol.2,no.2,pp.656–663,2017.\n[23] L.Tai,J.Zhang,M.Liu,andW.Burgard,“Sociallycompliantnavigation\nREFERENCES throughrawdepthinputswithgenerativeadversarialimitationlearning,”\ninIEEEInternationalConferenceonRoboticsandAutomation(ICRA),\n[1] D.Fox,W.Burgard,andS.Thrun,“Thedynamicwindowapproachto 2018,pp.1111–1117.\ncollisionavoidance,”IEEERoboticsandAutomationMagazine,vol.4, [24] Z. Xie, P. Xin, and P. Dames, “Towards safe navigation through\nno.1,pp.23–33,1997. crowdeddynamicenvironments,”inIEEE/RSJInternationalConference\n[2] J.VandenBerg,M.Lin,andD.Manocha,“Reciprocalvelocityobstacles onIntelligentRobotsandSystems(IROS),2021,pp.4934–4940.\nforreal-timemulti-agentnavigation,”inIEEEInternationalConference [25] A.Pokle,R.Mart´ın-Mart´ın,P.Goebel,V.Chow,H.M.Ewald,J.Yang,\nonRoboticsandAutomation(ICRA),2008,pp.1928–1935. Z.Wang,A.Sadeghian,D.Sadigh,S.Savarese,etal.,“Deeplocaltrajec-\n[3] C. Chen, Y. Liu, S. Kreiss, and A. Alahi, “Crowd-robot interaction: toryreplanningandcontrolforrobotnavigation,”inIEEEInternational\nCrowd-awarerobotnavigationwithattention-baseddeepreinforcement ConferenceonRoboticsandAutomation(ICRA),2019,pp.5815–5822.\nlearning,”inIEEEInternationalConferenceonRoboticsandAutomation [26] H. Karnan, A. Nair, X. Xiao, G. Warnell, S. Pirk, A. Toshev, J. Hart,\n(ICRA),2019,pp.6015–6022. J.Biswas,andP.Stone,“Sociallycompliantnavigationdataset(scand):\n[4] Y. F. Chen, M. Liu, M. Everett, and J. P. How, “Decentralized non- A large-scale dataset of demonstrations for social navigation,” IEEE\ncommunicatingmultiagentcollisionavoidancewithdeepreinforcement RoboticsandAutomationLetters,2022.\nlearning,”inIEEEInternationalConferenceonRoboticsandAutomation [27] R. Chandra, H. Karnan, N. Mehr, P. Stone, and J. Biswas, “Towards\n(ICRA),2017,pp.285–292. imitation learning in real world unstructured social mini-games in\n[5] Y. Yang, J. Jiang, J. Zhang, J. Huang, and M. Gao, “St2: Spatial- pedestriancrowds,”arXivpreprintarXiv:2405.16439,2024.\ntemporal state transformer for crowd-aware autonomous navigation,” [28] Z.XieandP.Dames,“Drl-vo:Learningtonavigatethroughcrowdeddy-\nIEEERoboticsandAutomationLetters,vol.8,no.2,pp.912–919,2023. namicscenesusingvelocityobstacles,”IEEETransactionsonRobotics,\n[6] C. Chen, S. Hu, P. Nikdel, G. Mori, and M. Savva, “Relational graph vol.39,no.4,pp.2700–2719,2023.\nlearning for crowd navigation,” in IEEE/RSJ International Conference [29] M.Everett,Y.F.Chen,andJ.P.How,“Motionplanningamongdynamic,\nonIntelligentRobotsandSystems(IROS),2020. decision-makingagentswithdeepreinforcementlearning,”inIEEE/RSJ\n[7] C. Pe´rez-D’Arpino, C. Liu, P. Goebel, R. Mart´ın-Mart´ın, and International Conference on Intelligent Robots and Systems (IROS),\nS. Savarese, “Robot navigation in constrained pedestrian environments 2018,pp.3052–3059.\nusing reinforcement learning,” in IEEE International Conference on [30] Y.Chen,C.Liu,B.E.Shi,andM.Liu,“Robotnavigationincrowdsby\nRoboticsandAutomation(ICRA),2021,pp.1140–1146. graphconvolutionalnetworkswithattentionlearnedfromhumangaze,”\n[8] S.Liu,P.Chang,W.Liang,N.Chakraborty,andK.Driggs-Campbell, IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 2754–2761,\n“Decentralized structural-rnn for robot crowd navigation with deep 2020.\nreinforcementlearning,”inIEEEInternationalConferenceonRobotics [31] Y.-J. Mun, M. Itkina, S. Liu, and K. Driggs-Campbell, “Occlusion-\nandAutomation(ICRA),2021,pp.3517–3524. awarecrowdnavigationusingpeopleassensors,”inIEEEInternational\n[9] S. Liu, P. Chang, Z. Huang, N. Chakraborty, K. Hong, W. Liang, Conference on Robotics and Automation (ICRA), 2023, pp. 12031–\nD. L. McPherson, J. Geng, and K. Driggs-Campbell, “Intention aware 12037.\nrobotcrowdnavigationwithattention-basedinteractiongraph,”inIEEE [32] A. J. Sathyamoorthy, J. Liang, U. Patel, T. Guan, R. Chandra, and\nInternationalConferenceonRoboticsandAutomation(ICRA),2023,pp. D.Manocha,“Densecavoid:Real-timenavigationindensecrowdsusing\n12015–12021. anticipatory behaviors,” in IEEE International Conference on Robotics\n[10] M. Hoy, A. S. Matveev, and A. V. Savkin, “Algorithms for collision- andAutomation(ICRA),2020,pp.11345–11352.\nfree navigation of mobile robots in complex cluttered environments: a [33] D. Dugas, O. Andersson, R. Siegwart, and J. J. Chung, “Navdreams:\nsurvey,”Robotica,vol.33,no.3,pp.463–497,2015. Towardscamera-onlyrlnavigationamonghumans,”inIEEE/RSJInter-\n[11] A.V.SavkinandC.Wang,“Seekingapaththroughthecrowd:Robot national Conference on Intelligent Robots and Systems (IROS), 2022,\nnavigation in unknown dynamic environments with moving obstacles pp.2504–2511.\nbased on an integrated environment representation,” Robotics and Au- [34] Z.Zheng,C.Cao,andJ.Pan,“Ahierarchicalapproachformobilerobot\ntonomousSystems,vol.62,no.10,pp.1568–1580,2014. explorationinpedestriancrowd,”IEEERoboticsandAutomationLetters,\n[12] C.Mavrogiannis,F.Baldini,A.Wang,D.Zhao,P.Trautman,A.Stein- vol.7,no.1,pp.175–182,2022.\nfeld,andJ.Oh,“Corechallengesofsocialrobotnavigation:Asurvey,” [35] C.Mavrogiannis,F.Baldini,A.Wang,D.Zhao,P.Trautman,A.Stein-\narXivpreprintarXiv:2103.05668,2021. feld,andJ.Oh,“Corechallengesofsocialrobotnavigation:Asurvey,”\n[13] J. Van Den Berg, S. J. Guy, M. Lin, and D. Manocha, “Reciprocal n- ACMTransactionsonHuman-RobotInteraction,vol.12,no.3,2023.\nbody collision avoidance,” in Robotics research. Springer, 2011, pp. [36] A.H.Raj,Z.Hu,H.Karnan,R.Chandra,A.Payandeh,L.Mao,P.Stone,\n3–19. J.Biswas,andX.Xiao,“Rethinkingsocialrobotnavigation:Leveraging\n[14] D.HelbingandP.Molnar,“Socialforcemodelforpedestriandynamics,” thebestoftwoworlds,”inIEEEInternationalConferenceonRobotics\nPhysicalreviewE,vol.51,no.5,p.4282,1995. andAutomation(ICRA),2024.\n[37] N. Tsoi, A. Xiang, P. Yu, S. S. Sohn, G. Schwartz, S. Ramesh, [57] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov,“Prox-\nM. Hussein, A. W. Gupta, M. Kapadia, and M. Va´zquez, “Sean 2.0: imalpolicyoptimizationalgorithms,”arXivpreprintarXiv:1707.06347,\nFormalizingandgeneratingsocialsituationsforrobotnavigation,”IEEE 2017.\nRoboticsandAutomationLetters,pp.1–8,2022. [58] E. Coumans and Y. Bai, “Pybullet, a python module for physics\n[38] X.Puig,E.Undersander,A.Szot,M.D.Cote,T.-Y.Yang,R.Partsey, simulationforgames,roboticsandmachinelearning,”http://pybullet.org,\nR.Desai,A.Clegg,M.Hlavac,S.Y.Min,V.Vondrusˇ,T.Gervet,V.-P. 2016–2019.\nBerges,J.M.Turner,O.Maksymets,Z.Kira,M.Kalakrishnan,J.Malik, [59] S.Nasiriany,H.Liu,andY.Zhu,“Augmentingreinforcementlearning\nD.S.Chaplot,U.Jain,D.Batra,A.Rai,andR.Mottaghi,“Habitat3.0:A with behavior primitives for diverse manipulation tasks,” in IEEE\nco-habitatforhumans,avatars,androbots,”inInternationalConference InternationalConferenceonRoboticsandAutomation(ICRA),2022.\nonLearningRepresentations(ICLR),2024. [60] H.Zhu,J.Yu,A.Gupta,D.Shah,K.Hartikainen,A.Singh,V.Kumar,\n[39] L.Ka¨stner,V.Shcherbyna,H.Zeng,T.A.Le,M.H.-K.Schreff,H.Os- and S. Levine, “The ingredients of real world robotic reinforcement\nmaev,N.T.Tran,D.Diaz,J.Golebiowski,H.Soh,andJ.Lambrecht, learning,” in International Conference on Learning Representations\n“Arena 3.0: Advancing social navigation in collaborative and highly (ICLR),2020.\ndynamicenvironments,”inRobotics:ScienceandSystems,2024. [61] P. Chang, S. Liu, T. Ji, N. Chakraborty, K. Hong, and K. R. Driggs-\n[40] R. Martin-Martin, M. Patel, H. Rezatofighi, A. Shenoi, J. Gwak, Campbell, “A data-efficient visual-audio representation with intuitive\nE. Frankel, A. Sadeghian, and S. Savarese, “Jrdb: A dataset and\nfine-tuningforvoice-controlledrobots,”inConferenceonRobotLearn-\nbenchmark of egocentric robot visual perception of humans in built\ning(CoRL),2023.\nenvironments,” IEEE transactions on pattern analysis and machine [62] Y. Du, O. Watkins, T. Darrell, P. Abbeel, and D. Pathak, “Auto-tuned\nintelligence,2021. sim-to-realtransfer,”inIEEEInternationalConferenceonRoboticsand\nAutomation(ICRA),2021,p.1290–1296.\n[41] N. Hirose, D. Shah, A. Sridhar, and S. Levine, “Sacson: Scalable au-\n[63] V. Lim, H. Huang, L. Y. Chen, J. Wang, J. Ichnowski, D. Seita,\ntonomouscontrolforsocialnavigation,”IEEERoboticsandAutomation\nM.Laskey,andK.Goldberg,“Real2sim2real:Self-supervisedlearning\nLetters,vol.9,no.1,pp.49–56,2024.\nof physical single-step dynamic actions for planar robot casting,” in\n[42] H. Yang, C. Yao, C. Liu, and Q. Chen, “Rmrl: Robot navigation in\nIEEE International Conference on Robotics and Automation (ICRA),\ncrowdenvironmentswithriskmap-baseddeepreinforcementlearning,”\n2022,pp.8282–8289.\nIEEERoboticsandAutomationLetters,vol.8,no.12,pp.7930–7937,\n[64] Y.Hu,J.Yang,L.Chen,K.Li,C.Sima,X.Zhu,S.Chai,S.Du,T.Lin,\n2023.\nW.Wang,L.Lu,X.Jia,Q.Liu,J.Dai,Y.Qiao,andH.Li,“Planning-\n[43] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,\noriented autonomous driving,” in IEEE/CVF Conference on Computer\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nVisionandPatternRecognition(CVPR),2023.\ninNeuralInformationProcessingSystems(NeurIPS),2017,pp.5998–\n[65] H.Wang,K.Kedia,J.Ren,R.Abdullah,A.Bhardwaj,A.Chao,K.Y.\n6008.\nChen,N.Chin,P.Dan,X.Fan,G.Gonzalez-Pumariega,A.Kompella,\n[44] P. Velicˇkovic´, G. Cucurull, A. Casanova, A. Romero, P. Lio`, and M.A.Pace,Y.Sharma,X.Sun,N.Sunkara,andS.Choudhury,“Mosaic:\nY. Bengio, “Graph attention networks,” International Conference on Amodularsystemforassistiveandinteractivecooking,”2024.\nLearningRepresentations,2018.\n[45] A.Vemula,K.Muelling,andJ.Oh,“Socialattention:Modelingattention\nin human crowds,” in IEEE International Conference on Robotics and\nAutomation(ICRA),2018,pp.1–7.\n[46] Y. Huang, H. Bi, Z. Li, T. Mao, and Z. Wang, “Stgat: Modeling\nspatial-temporal interactions for human trajectory prediction,” in IEEE\nInternationalConferenceonComputerVision(ICCV),2019.\n[47] A. Hasan, P. Sriram, and K. Driggs-Campbell, “Meta-path analysis on\nspatio-temporal graphs for pedestrian trajectory prediction,” in IEEE\nInternational Conference on Robotics and Automation (ICRA), 2022,\npp.617–624.\n[48] E. Leurent and J. Mercat, “Social attention for autonomous decision-\nmaking in dense traffic,” in Machine Learning for Autonomous Driv-\ning Workshop at Advances in Neural Information Processing Systems\n(NeurIPS),2019.\n[49] S. Liu, P. Chang, H. Chen, N. Chakraborty, and K. Driggs-Campbell,\n“Learningtonavigateintersectionswithunsuperviseddrivertraitinfer-\nence,” in IEEE International Conference on Robotics and Automation\n(ICRA),2022.\n[50] L.Huajian,D.Wei,M.Shouren,W.Chao,andG.Yongzhuo,“Sample-\nefficient learning-based dynamic environment navigation with transfer-\nring experience from optimization-based planner,” IEEE Robotics and\nAutomationLetters,vol.9,no.8,pp.7055–7062,2024.\n[51] B. Chen, H. Zhu, S. Yao, S. Lu, P. Zhong, Y. Sheng, and J. Wang,\n“Sociallyawareobjectgoalnavigationwithheterogeneousscenerepre-\nsentationlearning,”IEEERoboticsandAutomationLetters,vol.9,no.8,\npp.6792–6799,2024.\n[52] D. Jia, A. Hermans, and B. Leibe, “DR-SPAAM: A Spatial-Attention\nandAuto-regressiveModelforPersonDetectionin2DRangeData,”in\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS),2020,pp.10270–10277.\n[53] J. Redmon and A. Farhadi, “YOLOv3: An Incremental Improvement,”\narXivpreprintarXiv:1804.02767,2018.\n[54] N. Wojke, A. Bewley, and D. Paulus, “Simple Online and Realtime\nTrackingwithaDeepAssociationMetric,”in2017IEEEinternational\nconferenceonimageprocessing(ICIP),2017,pp.3645–3649.\n[55] A.Jain,A.R.Zamir,S.Savarese,andA.Saxena,“Structural-rnn:Deep\nlearning on spatio-temporal graphs,” in IEEE conference on computer\nvisionandpatternrecognition(CVPR),2016,pp.5308–5317.\n[56] X. Ma, J. Li, M. J. Kochenderfer, D. Isele, and K. Fujimura, “Rein-\nforcement learning for autonomous driving with latent state inference\nand spatial-temporal relationships,” in IEEE International Conference\nonRoboticsandAutomation(ICRA),2021.",
    "pdf_filename": "HEIGHT_Heterogeneous_Interaction_Graph_Transformer_for_Robot_Navigation_in_Crowded_and_Constrained_E.pdf"
}