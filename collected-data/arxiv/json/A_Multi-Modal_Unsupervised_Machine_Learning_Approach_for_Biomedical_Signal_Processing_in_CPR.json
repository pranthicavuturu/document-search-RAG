{
    "title": "A Multi-Modal Unsupervised Machine Learning Approach for Biomedical Signal Processing in CPR",
    "abstract": "Cardiopulmonary resuscitation (CPR) is a critical, life-saving intervention aimed at restoring blood circulation and breathing in individuals experiencing cardiac arrest or respiratory failure. Accurate and real-time analysis of biomedical signals during CPR is essential for monitoring and decision- making, from the pre-hospital stage to the intensive care unit (ICU). However, CPR signals are often corrupted by noise and artifacts, making precise interpretation challenging. Traditional denoising methods, such as filters, struggle to adapt to the varying and complex noise patterns present in CPR signals. Given the high-stakes nature of CPR, where rapid and accurate responses can determine survival, there is a pressing need for more robust and adaptive denoising techniques. In this context, an unsupervised machine learning (ML) methodology is particularly valuable, as it removes the dependence on labeled data, which can be scarce or impractical in emergency scenarios. This paper introduces a novel unsupervised ML approach for denoising CPR signals using a multi-modality framework, which leverages multiple signal sources to enhance the denoising process. The proposed approach not only improves noise reduction and signal fidelity but also preserves critical inter-signal correlations (0.9993) which is crucial for downstream tasks. Furthermore, it outperforms existing methods in an unsupervised context in terms of signal-to-noise ratio (SNR) and peak signal-to-noise ratio (PSNR), making it highly effective for real-time applications. The integration of multi-modality further enhances the system’s adaptability to various biomedical signals beyond CPR, improving both automated CPR systems and clinical decision-making.",
    "body": "A MULTI-MODAL UNSUPERVISED MACHINE LEARNING\nAPPROACH FOR BIOMEDICAL SIGNAL PROCESSING IN CPR\nSaidul Islam1, Jamal Bentahar2,1,∗, Robin Cohen3, Gaith Rjoub4,1\n1Concordia Institute for Information Systems Engineering, Concordia University, Montreal, Canada\n2Department of Computer Science, Khalifa University, Abu Dhabi, UAE\n3Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada\n4Faculty of Information Technology, Aqaba University of Technology, Aqaba, Jordan\nContributing Authors’ Emails:\nsaidul.islam@concordia.ca;\njamal.bentahar@ku.ac.ae;\nrcohen@uwaterloo.ca;\ngrjoub@aut.edu.jo\n∗Corresponding Author’s Email: jamal.bentahar@ku.ac.ae\nThe authors contributed equally to this work.\nABSTRACT\nCardiopulmonary resuscitation (CPR) is a critical, life-saving intervention aimed at restoring blood\ncirculation and breathing in individuals experiencing cardiac arrest or respiratory failure. Accurate\nand real-time analysis of biomedical signals during CPR is essential for monitoring and decision-\nmaking, from the pre-hospital stage to the intensive care unit (ICU). However, CPR signals are often\ncorrupted by noise and artifacts, making precise interpretation challenging. Traditional denoising\nmethods, such as filters, struggle to adapt to the varying and complex noise patterns present in CPR\nsignals. Given the high-stakes nature of CPR, where rapid and accurate responses can determine\nsurvival, there is a pressing need for more robust and adaptive denoising techniques. In this context,\nan unsupervised machine learning (ML) methodology is particularly valuable, as it removes the\ndependence on labeled data, which can be scarce or impractical in emergency scenarios. This paper\nintroduces a novel unsupervised ML approach for denoising CPR signals using a multi-modality\nframework, which leverages multiple signal sources to enhance the denoising process. The proposed\napproach not only improves noise reduction and signal fidelity but also preserves critical inter-signal\ncorrelations (0.9993) which is crucial for downstream tasks. Furthermore, it outperforms existing\nmethods in an unsupervised context in terms of signal-to-noise ratio (SNR) and peak signal-to-noise\nratio (PSNR), making it highly effective for real-time applications. The integration of multi-modality\nfurther enhances the system’s adaptability to various biomedical signals beyond CPR, improving\nboth automated CPR systems and clinical decision-making.\nKeywords: Machine learning; Multi-modality; Biomedical signal; Unsupervised learning; Car-\ndiopulmonary resuscitation(CPR); Signal processing.\n1\nINTRODUCTION\n1.1\nPROBLEM STATEMENT\nCardiopulmonary resuscitation (CPR) is a vital, life-saving technique that can make a significant difference in medical\nemergencies by restoring blood flow and breathing in individuals experiencing cardiac arrest or respiratory failure.\nThe primary objective of CPR is to keep the heart pumping and ensure continuous oxygenated blood flow to vital\norgans, particularly the brain (Raza et al., 2021). The American Heart Association, alongside other global health\norganizations, underscores the significance of CPR training, not just among healthcare professionals but also among\nthe general public (Hinkelbein et al., 2020), as the majority of cardiac arrests occur outside of hospital settings,\nwhere immediate medical intervention may not readily available. In such scenarios, manual, human-driven CPR\nis predominantly practiced despite several difficulties and challenges that hinder successful CPR. For instance,\nperforming CPR is physically demanding for individuals, even for health care providers for an extended period.\narXiv:2411.11869v1  [eess.SP]  3 Nov 2024\n\nMoreover, it is difficult to maintain the correct compression depth, rate, and consistency. Additionally, obtaining\nreal-time feedback is significantly challenging, which is very crucial during the initial treatment in ambulances or\nhospital environments. To address these limitations, there has been a growing need for automated CPR systems\nover the last two decades, driven by advancements in mechanical devices within the healthcare system (Swor et al.,\n2006).\nAutomated CPR systems offer the potential to enhance the effectiveness of life-saving interventions by\nensuring optimal compression and decompression at precise intervals for long periods. However, automating CPR\nfaces several challenges related to decision-making and accuracy in compression and decompression. In this regard,\nArtificial intelligence(AI), particularly machine learning (ML), offers a promising way to address these limitations in\nmedical operations. ML is capable of identifying complex patterns in biomedical data, improving the quality of CPR\nin medical systems, enhancing diagnostic accuracy, and enabling real-time monitoring of CPR in diverse medical\nconditions (Hallstrom et al., 2006). Furthermore, various biomedical signals associated with CPR—such as blood\npressure, velocity, force, and respiratory waveforms—play a critical role in guiding treatment decisions, particularly\nin intensive care units (ICUs). Accurate monitoring and processing of these signals during CPR is essential for\nassessing patient response, informing medical interventions, and ultimately improving patient outcomes (Hurt, 2005).\nCPR signals are inherently dynamic and time-sensitive, requiring specialized approaches for effective processing and\ndecision-making. Moreover, these signals are often corrupted by various types of noise sources, including baseline\nwander, electrode artifacts, motion artifacts, and electromagnetic interference, which significantly degrade signal\nquality and complicate interpretation (Fitzgibbon et al., 2002). Ensuring the integrity and quality of signal data is\ncrucial for effective ML approaches in accurate decision-making for CPR automation and monitoring systems. While\ntraditional signal processing techniques such as filtering, offer some improvements, they often struggle to remove\nnoise effectively without compromising essential physiological features (Luong et al., 2013). Since CPR is a process\nof emergency healthcare, where decision-making can directly impact survival, it is unacceptable to compromise the\nquality of the signal. Therefore, achieving effective denoising while preserving key signal characteristics is essential.\nMoreover, one limitation of conventional filters lies in their dependence on predefined parameters—such as cutoff\nfrequencies or filter orders—and assumptions regarding noise characteristics, which may not correspond to the\ncomplex and dynamic nature of real-world CPR signals. Traditional filters often struggle to effectively remove noise\nwithout inadvertently distorting or attenuating important signal components, leading to the potential loss of valuable\ninformation. Additionally, filters are generally tailored to specific noise types or signal modalities, limiting their\nadaptability to diverse noise profiles and environments (Seron et al., 2012). These inherent constraints underscore\nthe need for more flexible and adaptive denoising techniques capable of preserving critical signal features. Given\nthe high-stakes nature of CPR, where accurate signal interpretation is a matter of life and death, overcoming these\nlimitations is essential for advancing automated CPR systems and improving patient outcomes.\nThe application of machine learning (ML) has emerged as a promising approach to tackling critical tasks in medical\nsystems. ML excels at identifying complex patterns and relationships within biomedical data, offering solutions\nthat can enhance diagnostic accuracy, improve healthcare systems, and enable real-time monitoring across a variety\nof medical conditions (Blum & Langley, 1997).\nWithin the domain of CPR, integrating ML presents unique\nopportunities, as CPR signals are both dynamic and time-sensitive, requiring specialized approaches for effective\nprocessing and decision-making. ML-based approaches, particularly for tasks such as denoising and signal enhance-\nment, demonstrate the ability to learn from diverse, heterogeneous, and noisy data, thus showing great potential in\ndeciphering the intricate nuances inherent in CPR signals. Moreover, recent advancements in computational power,\nparticularly with the advent of high-performance GPUs and cloud computing platforms, have significantly improved\nthe feasibility of deploying ML algorithms in real-time applications, thus facilitating timely insights in critical\nmedical scenarios such as CPR (Azghadi et al., 2020). Such advancements can substantially improve the accuracy\nand reliability of real-time monitoring during life-saving interventions. However, despite these promising prospects,\napplying ML algorithms to CPR signal denoising presents several notable challenges. These include the development\nof robust feature extraction methodologies that are tailored to the unique characteristics of CPR signals, selecting\nappropriate ML frameworks and models capable of processing temporal data and noise, and acquisition of labeled\ndatasets for supervised learning tasks (Schelter et al., 2015). Existing ML approaches for CPR signal denoising are\npredominantly supervised, relying on clean labeled data that corresponds to noisy signals. In real-world scenarios,\nobtaining such labeled data is extremely difficult, if not impractical, thereby limiting the applicability of supervised\nML methods. Consequently, there is a clear need for an unsupervised ML approach dedicated to CPR signals that\ncan effectively denoise CPR signals without reliance on labeled data. Currently, no unsupervised ML methodology\nspecifically designed for CPR signal denoising has been developed, leaving a gap in the existing research landscape.\n2\n\n1.2\nNOVELTY AND CONTRIBUTIONS\nIn this paper, we address the challenges by introducing an advanced method for CPR signal denoising and artifact\nremoval, aimed at enhancing patient outcomes and healthcare delivery. A novel unsupervised ML methodology\nhas been proposed, specifically designed for denoising biomedical signals during CPR. The framework introduces\na multi-modality approach, enabling the concurrent processing of multiple signals while addressing the noise\ncharacteristics of each signal individually. This individualized signal processing facilitates the extraction of unique\nfeatures, thereby improving the overall denoising process and enhancing the accuracy of signal interpretation. A\nkey feature of the proposed framework is its use of unsupervised ML, which demonstrates the method’s capability\nto effectively remove noise and enhance signal fidelity without the need for labeled data. By identifying underlying\nstructures and patterns in noisy CPR signal data, this approach is particularly well-suited for real-world applications,\nwhere labeled ground truth is often scarce or unavailable. The key contributions of this paper are as follows: (i)\na comprehensive exploration of existing denoising approaches for CPR signals, (ii) the introduction of a novel\nmulti-modal ML methodology that processes multiple signals simultaneously, (iii) the use of an unsupervised ML\napproach, eliminating the dependency on labeled data, (iv) the generation of synthetic biomedical CPR signals for\nresearch purposes, and (v) the conceptualization of a multi-modal ML framework for enhanced decision-making in\nmedical emergencies.\nMoreover, a comprehensive comparison of the proposed method has been conducted with existing ML and filter meth-\nods in an unsupervised context to justify the performance of the proposed methodology. The effectiveness of the\nmethodology is demonstrated through visualizations, as well as signal-to-noise ratio(SNR) and peak signal-to-noise\nratio(PSNR) scores. Notably, the method successfully preserves the signal data correlations at a significant level,\neven while processing each signal through dedicated models and subsequently combining them. To illustrate this, the\ncorrelations matrix before and after the denoising process have been calculated and quantified the differences by deter-\nmining the correlation coefficients, which are crucial for downstream tasks. The results highlight the potential of this\napproach to enhance biomedical signal processing not only in CPR scenarios but also in border medical contexts. The\npaper is organized into six sections, the current section discusses the context and motivation behind this paper, while\nSection 2 presents a review of related literature, highlighting the limitations of existing methods and differentiating\nour contributions. Section 3 outlines the preliminary concepts essential for the proposed methodology. The details of\nthe methodology and proposed solution are presented and depicted in Section 4. Importantly, Section 5 showcases the\noutput results compares the findings with the existing methods, and extends the discussion to data correlation preser-\nvation. Section 6 presents a discussion addressing the scope of explainability, and exploring the application of the\nmulti-modality concept in decision-making processes that integrate multiple sources of information during medical\nemergencies. Finally, Section 7 summarizes the key findings and contributions of this paper.\n2\nRELATED WORK\nThis section explores the landscape of denoising techniques for biomedical signals, specifically focusing on CPR sig-\nnals. While several ML-based methods for denoising electrocardiogram (ECG) signals have been widely researched,\nthe consideration of ECG signal denoising techniques has been intentionally set aside due to the significant differences\nbetween ECG and CPR signals. Unlike ECG signals, which are electrical recordings of the heart’s activity, CPR\nsignals are mechanical signals generated during chest compressions. Additionally, ECG signals exhibit characteristic\nwaveforms representing different phases of the heart’s electrical activity, while CPR signals exhibit even more\ncomplex patterns and instead represent the rhythmic application of pressure to the chest. Furthermore, ECG signals\nare typically measured using electrodes placed on the skin surface and analyzed for cardiac rhythm abnormalities\n(Gacek, 2011). On the contrary, CPR signals are measured using external sensors during CPR maneuvers to monitor\nchest compression effectiveness.\nImportantly, CPR signals involve multiple signals from different body parts,\nincluding the abdomen, necessitating dedicated fashion denoising methods due to their unique characteristics (Baubin\net al., 1995). Consequently, there is an urgent need for robust and automated denoising techniques using a dedicated\nmethodology that can enhance the quality of CPR signals by removing artifacts and facilitating accurate clinical\ndecision-making.\n3\n\nApproach\nType of\nMethod\nLimitations\nDifferences\nAase et al. (2000)\nFilter\nThe denoising method is\nproposed for a specific\nfrequency of the signal\nOur methodology can han-\ndle different range of fre-\nquency of biomedical signals\nwith the flexibility and effi-\nciency of signal denoising,\nHusoy et al.\n(2002)\nFilter\n(Pursuit-like\napproach)\nPerformance depended and\nhighly sensitive to filter design\nparameters like a fixed sample\nrate, window samples, and\nnumber of channels which might\nnot be effective for all signals.\nOur machine learning-based\nframework is more flexible for sig-\nnal variations while modifications\ncan bring better performance.\nAramendi\net al. (2007)\nFilter\nPerformance is sensitive and\ndepends on the characteris-\ntics of the currpted signal.\nOur machine learning-based\nframework is more flexible for sig-\nnal variations while modifications\ncan bring better performance.\nGong et al.\n(2014)\nFilter\nThe performance varies a lot\nbased on the signals of var-\nious conditions during CPR\nOur proposed methodology is\ncapable of handling a variety\nof signals, while the method\ncan tackle different signals\naccording to their merits.\nGong et al.\n(2017)\nFilter\n(Enhanced\nadaptive\nfilter)\nUncertain about the perfor-\nmance during resuscitation\ndue to lack of capability to\nidentify variation in CPR arti-\nfacts induced by manual versus\nmechanical chest compressions.\nOur method can handle diverse\ntypes of biomedical signals with\nvarying noise characteristics\nand enhances the flexibility and\nefficiency of signal denoising,\nHajeb-M\net al. (2022)\nMachine\nlearning\nNeed a corresponding clean signal\nfor this method and applicable\nonly for specific CPR signals, not\ndiverse to all types of CPR signals.\nOur proposed methodology can\nhandle various signals in an\nunsupervised manner (without\ncorresponding clean signal), while\nthe method can tackle different\nsignals according to their merits.\nTable 1: Comparative summary between our proposed ML methodology and the existing CPR signal denoising meth-\nods.\nThis part of the paper thoroughly explores existing denoising techniques designed specifically for biomedical signals,\nwith a particular focus on CPR signals. The goal is to identify research papers proposing novel models and methods\ntailored for effectively denoising CPR signals. However, few studies have focused on CPR signal denoising, in con-\ntrast to the extensive research available on ECG signal denoising. Despite this limitation, the review of the existing\nliterature uncovered a diverse array of denoising techniques, including traditional signal processing methods such as\nfiltering. The applicability of these methods to CPR signals has been investigated, revealing significant limitations in\nexisting approaches. Table 1 summarizes the differences and advantages offered by our methodology examined and\ncompared to these existing techniques. For instance, Aase et al. (2000) introduced a filter-based method targeted at\nremoving artifacts from CPR signals by operating within a specific frequency range. Similarly, Husoy et al. (2002)\nexperimented with a pursuit-like filtering approach for CPR signal denoising with limited applicability across diverse\nCPR signal types and specific filtering parameters. The work by Aramendi et al. (2007) and Gong et al. (2014) also\nintroduced filter-based techniques, but their performance was found to be heavily dependent on signal characteristics\nand conditions. Additionally, Gong et al. (2017) developed an enhanced adaptive filter for CPR signal denoising, but\n4\n\nits efficacy remained uncertain due to challenges in capturing signal variations adequately. Lastly, Hajeb-M et al.\n(2022) introduced a machine learning-based supervised approach handling CPR signal artifacts, however, it addressed\nthe limitation in applicability which is only for specific CPR signals and not being applicable across diverse CPR\nsignal types remains a challenge, given the inherent variability in signal patterns and characteristics. These findings\nunderscore the pressing need for more robust and adaptable denoising methodologies that can effectively handle the\nvariability of all CPR signal types. Such advancements are crucial for improving signal fidelity and enabling accurate\nclinical decision-making in real-time settings.\n3\nPRELIMINARIES\nThe proposed approach uses advanced ML concepts and techniques that are carefully selected to address the dynamic\nand urgent nature of CPR signals.\nThe framework considers various types of noise and interference that can\ncompromise the integrity of these signals, such as baseline wander, motion artifacts, and electromagnetic interference,\nand aims to enhance the overall signal quality during processing. The core of this method integrates a multi-modality\nconcept alongside several ML techniques, including autoencoders, convolutional neural networks (CNNs), and\nresidual connections.\nEach component plays a crucial role in forming an unsupervised CPR signal processing\nframework.\n3.1\nMULTI-MODALITY APPROACH\nThere are several motivating factors for choosing a multi-modality approach for effective CPR signal processing.\nFirst, it holds substantial promise for improving healthcare effectiveness by integrating various types of medical\ndata, leading to more comprehensive, accurate, and personalized patient assessments Muhammad et al. (2021).\nIn addition, multi-modality can enhance model transparency and the overall explainability of AI applications in\nhealthcare, opening doors to a broader range of clinical applications and making outcomes more interpretable for\npractitioners Yang et al. (2022), Wang et al. (2020).\nIn general, multi-modal deep learning is an approach that\nintegrates information from various types of data, like text, images, and audio, to enhance model performance and\naccuracy. In this method, each data modality is initially processed independently using specialized neural networks\ntailored to the specific characteristics of that modality, as illustrated in Figure 1. These networks extract high-level\nfeatures from the data and encode them into lower-dimensional representations, capturing essential patterns while\nreducing noise and redundancy. Once features from each modality are extracted, they are fused into a single, unified\nrepresentation. This can happen early on when the raw data is combined before processing to learn cross-modal\nrelationships, or later when the processed features are combined using different methods (Su et al., 2021).\nDeep learning\nModel\nDeep learning\nModel\nDeep learning\nModel\nText\nVisual\nAudio\nModel for\nCombined\nProcessing\noutput\nFigure 1: General Multi-Modality approach for different types of data.\nOnce the combined representation is formed, a unified model utilizes it to make predictions or perform downstream\ntasks. The architecture of this joint model can vary in complexity, ranging from simple concatenation followed by\nfully connected layers to more complex architectures. During training, the model’s parameters—including those of\nthe modality-specific networks and the fusion layers—are updated simultaneously through backpropagation. This\nensures that all components work in harmony, optimizing the overall system to minimize the loss function and align\npredictions with the true labels or outputs. By integrating information from different modalities, multi-modal deep\nlearning allows the model to capture richer, more nuanced representations of the data. This, in turn, enables the\nsystem to handle more complex tasks and improves its ability to generalize, making it particularly well-suited for\n5\n\napplications where diverse data sources are available, such as medical diagnosis or real-time monitoring in critical\nsettings (Di Mitri et al., 2018).\n3.2\nAUTOENCODER\nAn autoencoder is a type of artificial neural network designed to efficiently represent data by making its output\n(reconstructed data) as close as possible to the clean input data.\nIt can be used in two main ways: supervised\nand unsupervised. In the supervised method, pairs of noisy input data and their corresponding clean versions are\ngiven to the autoencoder for training. However, in the unsupervised approach, only noisy input data is used for\ntraining, without any clean data. During training, the autoencoder learns to encode the noisy input data into a hidden\nrepresentation, capturing the important patterns and structure of the data (Feng et al., 2024).\nEncoder\nDecoder\nLatent Space\nFigure 2: Vanilla Autoencoder model.\nThe autoencoder consists of two main components: the encoder and decoder. The encoder takes input data and\ntransforms it into a compact representation called a latent representation, as shown in Figure 2. It uses neural network\narchitectures like recurrent neural networks (RNNs), LSTM networks, CNNs, etc., to extract important features from\nthe input data. The goal of the encoder is to create a condensed version of the input that captures its important\ncharacteristics while removing noise. It considers both spatial (how data points relate to each other in space) and\ntemporal (how data changes over time) aspects of the input to ensure that vital information is preserved while noise is\nfiltered out effectively. On the other hand, the decoder part of the autoencoder works alongside the encoder. Its job is\nto reconstruct output data from the latent representations produced by the encoder. By collaborating closely with the\nencoder, the decoder reverses the process of dimensionality reduction that occurred during encoding. This means it\nrestores the original structure and features of the input data, translating them back into high-dimensional output data.\nThrough a series of iterations, the decoder aims to minimize the difference between the output and the original input\ndata, ensuring that the reconstructed data closely resembles the original signal while effectively removing noise (Li\net al., 2023).\n3.3\nCONVOLUTION NEURAL NETWORK\nOne-dimensional Convolutional Neural Networks (1D CNNs) are essential components in deep learning architectures,\nknown for their ability to capture intricate complex patterns and spatial relationships within sequential data. A typical\n1D CNN consists of three primary components: convolutional layers, pooling layers, and fully connected layers, each\nserving a distinct function. In the convolutional layer, learnable filters are applied to input data using convolution\noperations, generating feature maps that encode local patterns (Kiranyaz et al., 2019). These filters learn to detect\nspecific features by scanning across the input data’s spatial dimensions. Pooling layers are responsible for reducing\nthe size of feature maps while preserving important information. Techniques like max pooling or average pooling\naggregate feature map values within localized regions, help to summarize feature presence across different areas and\nimprove computational efficiency. Finally, fully connected layers combine extracted features from previous layers,\nallowing the network to perform specific tasks. These layers establish connections between all neurons in adjacent\nlayers, facilitating the representation of high-level features and making predictions tailored to the task at hand (Li et al.,\n2021). Mathematically, the 1D CNN can be represented as follows: Given an input sequence Xinput with dimensions\n(N, L, 1), where N is the number of samples, L is the sequence length (number of time steps), and 1 represents the\nsingle feature dimension, the output Xcnn of the 1D CNN component can be obtained as follows:\nXcnn = Conv1D(Xinput, filters = n, kernel size = n, activation = ReLU)\n(1)\n6\n\nwhere Conv1D denotes the 1D convolution operation with n number of filters and a kernel size of n. activation\nfunction is applied to introduce non-linearity, here for instance ReLU (Rectified Linear Unit).\nNext, a MaxPooling1D layer with a pool size of n is applied to downsample the data and reduce the com-\nputational complexity:\nXcnn = MaxPooling1D(Xcnn, pool size = n)\n(2)\nTo capture more intricate patterns, more 1D CNN layers can be added, comprising n filters with a kernel size of n and\nactivation function. Furthermore, more MaxPooling1D layers also can be applied to further downsampling the data\n(Zhang et al., 2023).\n4\nMETHODOLOGY AND PROPOSED SOLUTION\n4.1\nOVERVIEW AND FRAMEWORK\nCPR signals are highly correlated, presenting challenges in automating their processing. Traditional ML methods\ntreat signals collectively, but denoising while preserving their interrelationships is difficult due to each signal’s unique\ncharacteristics (Assegie et al., 2021). To address this, we propose an ML-based methodology that denoises each\nsignal individually while maintaining their correlations, improving accuracy and reliability for downstream tasks\nlike real-time monitoring. In the proposed framework, the concept of multi-modality is leveraged to simultaneously\nprocess multiple signals while individually denoising each signal before amalgamating them into a unified input data\nshape, as depicted in Figure 3. Each signal is treated as a distinct modality, and accordingly, processed with a separate\nmodel tailored to its unique characteristics and requirements. This approach ensures that each signal is effectively\ndenoised while preserving its individual attributes. After the separate processing of each signal with dedicated models,\nthe outputs are fed through a feed-forward network, where they are combined into a unified input. The feed-forward\nnetwork facilitates the integration of the denoised signals into a coherent and cohesive format, resembling the original\nnoisy input data but in a denoised form. Subsequently, the denoised output from the feedforward network is prepared\nfor downstream tasks or feature utilization in machine learning applications aimed at CPR automation.\nThe selection of an appropriate model for each modality or signal is a critical aspect in achieving effective denoising\nof CPR signals. Given the primary objective of this framework—denoising—the autoencoder model is employed\ndue to its flexibility for use in both supervised and unsupervised learning settings. While the supervised approach\ninvolves training the autoencoder with pairs of noisy input data and corresponding clean data, the unsupervised\nmethod is chosen for considering our real-life scenario. In this unsupervised setting, the autoencoder learns to encode\nnoisy input data into a latent representation, capturing underlying structures and patterns without relying on labeled\nclean data and then generating new data from the latent space using the decoder. Several architectural variations of\nautoencoders exist, such as RNNs, LSTMs, CNNs, and attention-based models (Ling et al., 2024). Given the need\nto capture local features and complex patterns in the CPR signals, CNN-based denoising autoencoders have been\nadopted across all modalities. CNNs are particularly well-suited for this task due to their capacity for effective feature\nextraction and their robustness in handling noise variations, making them ideal for processing biomedical signals. The\nstandardized use of CNN-based autoencoders ensures consistent performance across different signals. By leveraging\nCNNs’ strengths in capturing spatial dependencies and hierarchical patterns, the approach is capable of demonstrating\npromising efficacy in mitigating noise artifacts and enhancing the fidelity of biomedical signals for improved analysis\nand interpretation (Thuwajit et al., 2022).\nThe CNN-based autoencoder model in the proposed framework enables the network to automatically discern relevant\nfeatures across various spatial scales, enhancing its ability to capture intricate patterns within the data.\nIn our\nautoencoder, two(2)layers of 1-dimensional CNN are deployed in both the encoder and decoder. After each of the\n1D-Convolution layers, a max-pooling-1D layer has been placed in the encoder to compress the signals into latent\nspace, as in Figure 4. However, upsampling 1D layers have been used in the case of the decoder to generate signals\nfrom the latent space. One notable enhancement in our autoencoder is the incorporation of a residual connection.\nThis connection involves introducing an additional convolutional layer after the last max-pooling-1D layer in the\nencoder, which captures supplementary features or representations from the encoder’s output (Qassim et al., 2018).\nSubsequently, the output of this residual layer is combined with the output of the last convolutional layer of the\nencoder. This combined output is then passed through upsampling layers to prepare it for concatenation with the\ninput of the decoder. By incorporating the residual connection in this manner, the autoencoder is enabled to leverage\nthe information captured by the residual layer, enhancing its denoising capabilities.\nThis process facilitates the\npropagation of relevant features while minimizing the impact of noise, thereby enabling the autoencoder to learn more\n7\n\nModel\nfor\nModality 1\nModel\nfor\nModality 2\nModel\nfor\nModality\nn-1\nModel\nfor\nModality n\nFeed\nforward\nnetwork\n- - - -\nMultiple denoised CPR\nsignals, n\nEncoder\nDecoder\nLatent Space\nAutoencoder\nModality\n1\nNoisy\nCPR \nsignal 1\nModality\n2\nNoisy\nCPR\nsignal 2\n- - - -\nModality\nn-1\nNoisy\nCPR\nsignal n-1\nModality\nn\nNoisy\nCPR\nsignal n\nFigure 3: Proposed Multi-modal Machine Learning Framework for Denoising Biomedical Signal during CPR.\nrobust representations and achieve superior denoising performance, particularly in deeper architectures. Ultimately,\nthe inclusion of residual connections contributes to a clearer and more accurate reconstruction of the input signal,\nleading to improved signal quality (Luo & Wu, 2020).\n4.2\nMETHODOLOGY IMPLEMENTATION\nAccessing real-world medical data, particularly CPR data poses significant challenges due to privacy concerns and\nregulatory constraints. These limitations make it difficult to utilize actual patient data for machine learning (ML)\nresearch in the CPR context. Despite an extensive search for publicly available CPR datasets, all identified datasets\nare private and restricted, creating a significant barrier to progress in this field. As a result, alternative data sources\nare necessary to advance research and development. To overcome this obstacle, the generation of synthesized data\nhas emerged as a viable solution. Simulated data allows for experimentation, algorithm development, and validation\nwhile avoiding privacy issues and regulatory hurdles. The Babbs model, a well-established mathematical model for\nsimulating CPR processes, was identified as the most suitable framework for this task (Shin & Lee, 2021, Jalali\net al., 2015). This model is renowned for its ability to accurately replicate the complex dynamics of CPR, including\nvariations in signal morphology and the introduction of realistic noise. By leveraging the Babbs model, realistic\nCPR scenarios are simulated, enabling robust research into CPR signal processing, monitoring systems, and decision-\nmaking algorithms without compromising patient confidentiality (Fitzgerald et al., 1981).\n8\n\nFigure 4: Used Residual connected CNN-based denoising autoencoder in Multi-modal Machine Learning Framework.\n4.2.1\nDATA GENERATION\nThe Babbs model is a comprehensive mathematical framework that describes the physiological changes occurring\nduring CPR. It allows for real-time calculations of coronary perfusion pressure (CPP) based on chest compression\nparameters. The model can mimic the dynamic changes in the elastance and resistance of the arterial system during\nCPR, influenced by factors such as chest compression depth, rate, and duty cycle (Babbs et al., 1984). This provides\na quantitative method to optimize these parameters to achieve the desired CPP, which provides improved perfusion\nduring CPR. Specifically, the Babbs model incorporates variables like chest compression depth (D), compression rate\n(R), and duty cycle (Duty), and analyzes their effects on CPP and other physiological parameters. It has been proven\nin extensive research and validated through experimental and clinical studies. All of these factors make the Babbs\nmodel a reliable tool for simulating CPR scenarios and optimizing resuscitation techniques (Babbs & Thelander,\n1995).\nThe Babbs model equations can be summarized as follows:\nCPP = DBP × Duty\nDuty + 1\nR\n(3)\nDBP = E × D −F × D2\n(4)\nE = E min +\nE max −E min\n1 + exp(PE × (Dtarget −D))\n(5)\nF = F min +\nF max −F min\n1 + exp(PF × (Dtarget −D))\n(6)\nwhere the variables are as follows: CPP: Coronary perfusion pressure during CPR; DBP: Diastolic blood pressure\nduring CPR; D: Chest compression depth; R : Chest compression rate; Duty: Chest compression duty cycle; E:\nElastance of the arterial system; F: Resistance of the arterial system; E min, E max: Minimum and maximum\nelastance values; F min, F max: Minimum and maximum resistance values; PE, PF: Model parameters; Dtarget:\n9\n\nTarget chest compression depth.\nThe Babbs model optimizes chest compression parameters during CPR by providing real-time feedback to guide\nrescuers in achieving optimal chest compressions. The algorithm implementation involves measuring compression\ndepth, duty cycle, and rate during CPR and using these values to calculate CPP based on the Babbs model equations.\nThe calculated CPP is then compared to the optimal CPP target range, and the algorithm provides feedback to the\nrescuers to adjust compression parameters accordingly. A total of 100 cycles of chest compressions is simulated with\na compression rate of 100 compressions per minute. The compression depth was set to 50 mm, and the decompression\ndepth was set to 10 mm. The time of compression and time of decompression were set to 50% and 50% of the total\ncycle time, respectively. The target diastolic blood pressure was set to 40 mmHg. To generate the synthetic data for\nvarious patients, the external force is varied and applied during chest compressions, chest compliance, and airway\nresistance to simulate different patient conditions. The external force was varied from 500 N to 1000 N in steps\nof 100 N, however, Chest compliance was varied from 0.01 L/kPa to 0.05 L/kPa in steps of 0.01 L/kPa. Moreover,\nAirway resistance was varied from 1 cmH2O/(L/s) to 5 cmH2O/(L/s) in steps of 1 cmH2O/(L/s). The potential clinical\nimplications of this approach are significant, as optimizing chest compression parameters during CPR can improve the\nchances of successful resuscitation and reduce the risk of complications. The five key signals that are critical for\neffective CPR operations have been selected for experimentation: Compression, Blood Pressure, Velocity, Force, and\nPmouth (Jung et al., 2006). These signals provide essential insights into the physiological response during chest\ncompressions and the overall effectiveness of CPR. Compression tracks the depth and rate of compressions, while\nBlood Pressure monitors circulatory efficacy. Velocity measures the speed of the chest recoil, ensuring optimal blood\nflow, and Force indicates the pressure applied during compressions to maintain perfusion. Pmouth reflects airway\npressure, which is vital for ensuring proper ventilation. Together, these signals offer a comprehensive understanding\nof CPR performance, and a sample of these signals for a single patient is depicted in Figure 5.\n4.2.2\nADD NOISE AND ARTIFACTS TO SIMULATED DATA\nTo enhance the realism of the simulated CPR data generated by the Babbs model, several noise artifacts are incorpo-\nrated. These additional noise components are strategically introduced to mimic the diverse sources of interference\ncommonly encountered in real-world CPR signals. By augmenting the simulated data with realistic noise, a more\nfaithful representation of the complexities and challenges inherent in CPR monitoring scenarios is aimed to be\ncreated. This approach ensures that the research experiments and algorithm developments are robustly tested against a\nspectrum of noise patterns, thus increasing the validity and applicability of the findings to real-world clinical settings.\nThrough this meticulous process of noise augmentation, the gap between simulated and actual CPR data strives to\nbe bridged, ultimately advancing the field of CPR monitoring and signal processing with greater fidelity and relevance.\nGaussian Noise:\nIn the context of CPR signal data, adding Gaussian noise is a crucial step in enhancing the realism and authenticity\nof simulated data for research purposes. Gaussian noise, also known as white noise, follows a normal distribution\ncharacterized by its mean µ and variance σ2:\nNoise = µ + σ × N(0, 1)\n(7)\nFor this study, we tested specific parameter values: a mean of 0 and a standard deviation (or variance) of 1. These\nvalues were selected to simulate typical noise conditions encountered during CPR. To reflect more extreme conditions,\nsuch as a noisy ambulance environment, we increased the standard deviation to introduce greater variability and\nmimic higher levels of interference. Additionally, by setting a noise factor of 1.2 and a noise probability of 0.1,\nwe controlled the intensity and likelihood of noise affecting the clean CPR signals. This controlled introduction of\nvariability captures the stochastic nature of real-world noise during CPR monitoring, ensuring that the data more\naccurately reflects conditions encountered in clinical settings. (Goceri, 2023).\nAdditionally, the noise probability parameter controls the likelihood of noise being added to each element of\nthe signal, further enhancing the realism of the simulated data. This approach replicates various sources of inter-\nference, such as electrical noise, sensor inaccuracies, or environmental disturbances. For example, real-world CPR\nmonitoring scenarios often involve noise from moving equipment, patient movement during compressions, or chaotic\nsettings like ambulances where vibrations and varying patient profiles can introduce significant interference. By\nincorporating Gaussian noise into the clean CPR data, the algorithms and methodologies can be rigorously evaluated\nunder more practical conditions that closely mimic the unpredictable nature of clinical environments (Kassam, 2012).\nThis noise augmentation process facilitates the development of more robust CPR monitoring and decision-making\n10\n\nFigure 5: Generated CPR- Compression, Pressure, velocity, Force, Pmouth signals.\n11\n\nsystems, ensuring the proposed solutions can effectively address the challenges posed by real-world scenarios.\nSalt & Paper Noise:\nIn investigating CPR signal data, salt-and-pepper noise is introduced to simulate impulsive disturbances often encoun-\ntered in real-world monitoring environments. This noise type mimics issues like sensor anomalies or transmission\nirregularities, which can affect the precision of CPR signal assessments. The chosen parameters for this noise, include\nsalt probability = 0.0001, pepper probability = 0.0001, and salt and pepper values = 0.00001. These values reflect\ntypical error margins in sensor readings and momentary transmission glitches observed during CPR monitoring. For\ninstance, low probabilities such as 0.0001 were selected to represent rare but impactful occurrences, such as sudden\nmalfunctions in medical equipment or brief disruptions in signal transmission, which are infrequent but can lead to\nsignificant distortions in the data. The salt and pepper values (0.00001) represent extremely low and high-intensity\nvalues, simulating hardware malfunctions that inject sharp spikes or drops into the data (Azzeh et al., 2018). This\nmethodology ensures the simulated data realistically mimics real-life noise scenarios, where occasional, sudden,\nand extreme disturbances can impair signal integrity during CPR operations. By simulating these conditions, our\napproach enables the robust evaluation of algorithms, ensuring they are equipped to handle the unpredictability of\nactual clinical environments (Erkan et al., 2018).\nSimulate Baseline Wander:\nThe Baseline Wander, modeled as a sinusoidal waveform, introduces subtle variations that capture the complexities\ninherent in CPR signal analysis, where T represents the period of the sine wave (time for one complete cycle) and t\ninstantaneous time at which the function is evaluated. It is simulated using a function with an amplitude parameter\nset at 0.02 to replicate the low-frequency variations often caused by physiological factors such as body movement or\nelectrode displacement. These variations can distort the primary CPR signal, and the chosen amplitude reflects realistic\nlevels of baseline wander observed in clinical settings. The 0.02 amplitude ensures that the simulation introduces\nsufficient variability without overwhelming the core signal, effectively emulating real-world noise sources like patient\nmovement or sensor shifts during chest compressions in emergencies. This approach allows for a more accurate\nrepresentation of the noise challenges encountered during CPR monitoring (Li et al., 2020).\nBaseline Wander = amplitude × sin\n\u00122πt\nT\n\u0013\n(8)\nThis added realism enhances the robustness of our dataset, allowing for more comprehensive testing and refinement\nof algorithms. By incorporating these nuanced fluctuations, the dataset becomes better suited for exploring diverse\nresearch directions and improving algorithmic performance, ultimately advancing the reliability of CPR monitoring\nand analysis techniques.\nMuscle Interference:\nIn our data augmentation process for CPR signal data, Muscle Interference has resembled using a function with\na specified amplitude parameter of 0.05. Muscle interference represents the unpredictable noise that arises from\ninvoluntary muscle contractions, movement artifacts, or electrical noise from surrounding muscles and it masks the\nfeatures of the underlying signal. By adding muscle interference with this parameter, realistic variations in the CPR\nsignals are simulated, mimicking the effects of muscle-related artifacts that are observed in real-world monitoring\nscenarios. The simulated interference is generated as Gaussian noise with a mean of 0 and a standard deviation\ncorresponding to the specified amplitude, which enriches the CPR signals by introducing nuanced noise patterns that\nrepresent the complexities of CPR signal (Murach & Bagley, 2016).\nMuscle Interference = amplitude × N(0, 1)\n(9)\nThis augmentation will enhance the resilience of CPR signals and facilitate more comprehensive research exploration\nand algorithm development, which will help to improve the accuracy and reliability of CPR signal processing\nalgorithms.\nSimulate Sudden Amplitude Changes:\nMoreover, a unique approach is implemented by applying the simulated sudden amplitude changes function. This\nfunction is executed with several parameters such as num changes = 500, max change duration = 10, and change factor\n= 0.005. At the same time, a sudden variation in signal intensity is injected which resembles the abrupt fluctuations\noften encountered in real-world scenarios. These changes reflect the artifacts that arise from equipment glitches,\n12\n\nphysiological shifts, or external disruptions, introducing an element of unpredictability into the signals. By employing\nthis function with carefully chosen parameters, numerous instances of sudden amplitude changes are simulated, each\nlimited to a maximum duration of 10 data points. These parameters were meticulously selected to mimic the dynamic\nnature of sudden amplitude shifts observed during CPR monitoring scenarios. Through these amplitude changes, our\nCPR signals gain an added layer of realism and encourage deeper research exploration (Nakagawa et al., 2017).\nSimulated Compression Depth Variations:\nIn the ongoing efforts to enrich CPR signal data, the simulated compression depth variations function is applied by\napplying parameters such as num variations = 500, max variation duration = 20, and variation factor = 0.8. This\ncustom function is designed to replicate fluctuations in compression depth which is a common occurrence in medical\nsignals due to changes in applied pressure or physiological dynamics. By strategically adjusting these parameters,\na wide range of compression depth variations is introduced into our signals. These parameters were deliberately\nchosen to mirror the unpredictable nature of compression depth changes observed in real-world CPR monitoring\nscenarios and provide a faithful representation of dynamic signal characteristics. This augmentation strategy enhances\nthe adaptability of CPR signal processing algorithms which enables them to effectively navigate fluctuations in\ncompression depth with accuracy and precision (Considine et al., 2020).\nSimulate Multiple Dropouts:\nFinally, the multiple dropouts function has been applied by configuring with parameters such as num dropouts = 500\nand max dropout duration = 10. This function replicates multiple instances of dropouts in the signal and mimics\nperiods where data points are missing or corrupted. By implementing these parameters, numerous dropout instances\nare introduced into the input data frame where the signal values are substituted with NaN (Not a Number) to indicate\nthe absence of data. Missing the signal values is very common in real-life scenarios in the case of CPR and is\nvery crucial, whereas the integration of simulated dropouts strengthens the adaptability of CPR signal processing\nalgorithms in handling missing or corrupted data points (Anderson et al., 2019).\nWe applied all of these artifacts to the signals, effectively transforming them into realistic representations akin to the\nsimulated signals generated by the Babbs model. By incorporating Gaussian noise, salt and pepper noise, baseline\nwander, muscle interference, sudden amplitude changes, compression depth variations, and multiple dropouts, the\nsignals mimic the complexities and challenges encountered in real-life CPR monitoring scenarios. Examples of noisy\nsignals from a single patient are presented in Figure 6. As a result, each artifact introduces its unique distortions and\nirregularities, enriching the dataset with diverse noise patterns and signal variations. This comprehensive augmenta-\ntion process not only enhances the fidelity of our dataset but also prepares it to handle real-world signal processing\nchallenges.\n4.3\nEXPERIMENT SET-UP AND TRAINING\nDuring the model training, we employed a set of Python libraries, including TensorFlow, Scikit-learn, Matplotlib,\nNumPy, and Pandas, among others, to facilitate various aspects of the training process. We trained our model in an\nunsupervised manner with noisy CPR signals of only three(3) different patient data and split the data into train(80%)\nand validation(20%) sets for training purposes. After several experiments with the model’s hyperparameters, our\nmodel was trained using a batch size of 64 instances per iteration which provided a balance between computational\nefficiency and model convergence. To mitigate the risk of overfitting, a common challenge in deep learning tasks, we\nincorporated a callback function with a patience parameter set to 3, allowing the model to train until no improvement\nin validation loss was observed for three consecutive epochs (Rice et al., 2020). This approach helped to prevent the\nmodel from memorizing noise in the training data and encouraged it to learn widespread patterns. For the optimization\nstrategy, we opted for the popular Adam optimizer, known for its effectiveness in training deep neural networks by\nadapting the learning rates for individual parameters (de Campos Souza & Dragoni, 2024). Additionally, we employed\nthe Mean Absolute Error (MAE) loss metric, a robust choice for regression tasks that provided a clear and interpretable\nmeasure of the model’s performance in minimizing prediction errors (Karunasingha, 2022). By carefully selecting\nthese training parameters and strategies, we aimed to optimize the training process and maximize the model’s ability\nto learn meaningful representations from the data, ultimately enhancing its denoising capabilities for biomedical signal\nprocessing applications.\n5\nRESULTS AND ANALYSIS\nOur methodology was trained for nine epochs, as detailed in Section 4.3, striking a balance between allowing the\nmodel to learn from the data and minimizing computational overhead. The training and validation loss, illustrated in\nFigure 7, indicate effective learning: the training loss (blue line) starts at approximately 0.14 and steadily decreases to\naround 0.02 by the ninth epoch, while the validation loss (orange line) begins at about 0.16 and declines to roughly\n13\n\nFigure 6: Noisy CPR - Compression, Pressure, velocity, Force, Pmouth signals.\n14\n\n0.03. Both loss curves demonstrate a downward trend that converges towards the end, suggesting the model is not\noverfitting, given the close proximity of the training and validation losses. Upon completing the training, we applied\nour model in inference mode to the noisy signals of a new patient, enabling a visual assessment of its performance\nacross five key CPR signals. The visualizations in Figure 8 compare the noisy signals with their denoised counterparts,\nclearly showing that the proposed methodology effectively reduces noise. The smoother lines of the denoised signals\nclosely follow the trends of the original noisy signals across various contexts, including pressure, compression, activity,\nvoice, velocity, and force, thereby enhancing clarity and interpretability. Overall, the results affirm the robustness\nand effectiveness of our approach in denoising signals across diverse applications, demonstrating that our model has\nsignificantly reduced noise and improved signal quality.\nFigure 7: Training and validation loss of model with Multi-modal approach.\n5.1\nCOMPARISION WITH EXISTING METHODS\nTo comprehensively evaluate the effectiveness of our proposed framework, we conducted a comparative analysis\nagainst existing ML and filtering methodologies. Given that one of the primary objectives of our methodology is to\nachieve robust performance in an unsupervised manner, the comparisons were conducted under unsupervised condi-\ntions. For the filtering approach, we employed the enhanced adaptive filter proposed by Gong et al. (2017), which\nis an advanced method specifically designed for CPR signal processing. Since filtering techniques by default operate\nwithout labeled data, we applied the adaptive filter as outlined in Gong et al. (2017), with fine-tuning of parameters\nto better accommodate the different CPR signals in our study. For the existing ML-based method, we considered the\napproach proposed by Hajeb-M et al. (2022), which is currently the only dedicated ML method for CPR signal denois-\ning and which was originally designed as a supervised approach. To enable a fair comparison with our unsupervised\nframework, we adapted their method to operate in an unsupervised manner to the best extent possible. Both the filter-\nbased and adapted ML methods were trained and tuned on the same CPR signals used for our proposed approach. We\nsubsequently tested these methods on the same new patient signals, enabling a direct comparison of their denoising\nperformance against our framework. Figure 9 illustrates the resulting denoised signals: the red line represents the\nnoisy signal, the green line depicts the denoised signals produced by our proposed method, the yellow line shows\nthe denoising results obtained from applying an existing ML method in an unsupervised approach, and the blue line\ndisplays the results achieved using a filtering technique. The visualizations demonstrate that our proposed method\neffectively captures complex signal patterns while successfully removing artifacts, resulting in a cleaner, more robust\nsignal. Although the filter-based approach performed reasonably, it struggled to identify intricate patterns within the\nsignals, leading to poor denoising in certain areas. On the other hand, the existing ML approach without labeled data,\nshowed poor performance. It failed to capture even simple patterns within the signal, resulting in a highly distorted\noutput. The filter-based method is performed at a certain level; however, considering human life and death matters\nlike CPR, it is necessary to ensure a high level of precision at any cost, which is possible by the proposed methodol-\nogy. So, upon analyzing these visualizations, it is evident that our methodology consistently outperforms the existing\n15\n\nFigure 8: Denoised signals with the proposed methodology.\n16\n\napproaches especially without labeled data, offering superior denoising quality. This advantage is especially critical\nin biomedical applications like CPR, where signal clarity is essential. The effectiveness of our approach is further\nvalidated through the visual comparisons and the quantitative metrics applied in this study, underscoring the value of\nour framework in enhancing CPR signal processing.\n5.2\nPERFORMANCE ANALYSIS\nSignal-to-Noise Ratio: Signal-to-Noise Ratio (SNR) is a fundamental metric used in signal processing to quantify\nthe quality of a signal by comparing the strength of the signal to the level of noise. SNR is defined as the ratio\nof the power of the signal to the power of the noise. In mathematical terms, SNR is often expressed in decibels\n(dB) using a logarithmic scale, which allows for easy interpretation of the ratio. A higher SNR value indicates that\nthe signal is stronger relative to the noise, which generally leads to better signal quality and improved performance\nin various applications. SNR plays a crucial role in determining the reliability and accuracy of signal-processing\nalgorithms, making it an essential concept in multiple fields. It provides a quantitative measure of signal quality\nrelative to background noise (Jiang et al., 2016). Mathematically, SNR is defined as the ratio of the power of the\nsignal (S) to the power of the noise (N). This ratio is often expressed in decibels (dB) using a logarithmic scale:\nSNRdB = 10 · log10\n\u0012 S\nN\n\u0013\n(10)\nPeak Signal-to-Noise Ratio: Moreover, Peak Signal-to-Noise Ratio (PSNR) is a specific form of SNR popularly\nused in image processing to evaluate the image noise. Its underlying concept is also applied to other types of data\nlike signals. PSNR measures the ratio between the maximum possible power of a signal M and the power of the\nnoise introduced by compression or other forms of distortion. PSNR is expressed in decibels (dB), allowing for easy\ncomparison of signal quality across different techniques. A higher PSNR value indicates that the denoised signal\nis closer to the original, noise-free signal, suggesting better signal quality with less distortion or loss of information\n(Baig et al., 2019). Unlike SNR, PSNR is often calculated based on continuous signals and expressed in decibels\n(dB), which is calculated using the mean squared error (MSE) between the original noise-free signal (S) and the\ndistorted signal (S):\nPSNRdB = 10 · log10\n\u0012 M2\nMSE\n\u0013\n(11)\nBoth SNR and PSNR are used to analyze the effectiveness of the proposed methodology and compare it with the\nexisting ML and filter methods. The bar graph in Figure 10 compares the proposed, filter and existing ML methods\nwithout labeled data concerning SNR and PSNR metrics for evaluating signal denoising quality. The proposed\nmethod demonstrates substantial improvements over both of the existing methods in terms of SNR and PSNR\nmetrics. Specifically, our proposed method achieves a significantly higher SNR of 27.73, outperforming the filter and\nthe existing ML approaches, which yield SNR values of 21.7 and 12.81, respectively. This highlights our method’s\nenhanced capability to preserve the true signal while effectively minimizing background noise. Furthermore, the\nproposed approach exhibits superior signal fidelity, as reflected by a PSNR of 33.89, which far exceeds the PSNR\nvalues of 25.32 and 16.53 obtained by the filter method and the existing ML technique, respectively. This difference\nhighlights the effectiveness of the proposed method in noise reduction and signal preservation, offering enhanced\nreliability and performance for applications requiring high-quality signals. Overall, the results support the adoption of\nthe proposed method as a promising solution for improving signal integrity and quality during CPR.\nAnother critical aspect of the framework and methodology is the preservation of data correlation during the denoising\nprocess. Maintaining the correlation within the data is essential for subsequent machine learning applications in\ndownstream tasks, such as blood pressure measurement, and the optimization of compression and decompression\nprocesses, after the removal of signal artifacts. In our study, we carefully evaluated the impact of our denoising\nframework and methodology on the correlation matrix of our dataset. First, we calculated and visualized the data\ncorrelation matrix before and after applying our denoising techniques (Pernagallo, 2023). We found remarkably\nsimilar correlational values in both instances, as shown in Figure 11. This observation indicated that our framework\neffectively preserved the intrinsic relationships among the variables in the dataset. To further examine the correlation,\nwe computed and compared the correlation coefficients between the two correlation matrices, as shown in Figure\n12. The correlation coefficient measures the similarity between two correlation matrices, with values ranging from\n-1 to 1; higher values denote better similarity (Zhang et al., 2024). We obtained a correlation coefficient value of\n0.9993, signifying an exceptionally high degree of similarity between the two matrices. This finding underscores the\nrobustness and efficacy of our denoising methodology in maintaining the underlying correlations within the dataset\n17\n\nFigure 9: Comparison of CPR signal denoising performance between the proposed, existing ML, and filter methods in\nan unsupervised context.\n18\n\nFigure 10: SNR and PSNR comparison between the proposed, existing ML and filter methods in an unsupervised\ncontext.\nFigure 11: Correlation matrix before denoising and after denoising with proposed methodology and framework.\n19\n\nwhile effectively denoising signals. Thus, the proposed framework and methodology validate its potential to be\nutilized for enhanced denoising reliability and maintaining data quality in various biomedical applications, including\nCPR signals (Wu et al., 2024).\nFigure 12: Data correlation coefficient in before and after denoising through our proposed methods.\nTo recapitulate, the proposed methodology and framework provide significant advantages for biomedical signal de-\nnoising. Firstly, the multi-modality approach allows for the simultaneous processing of multiple signals without the\nneed for labeled data, leveraging correlations to enhance denoising performance and improve signal quality, which\nis particularly beneficial for CPR automation using ML techniques. Secondly, individual denoising of each signal\ntargets their unique characteristics and noise profiles, leading to more effective noise removal and overall enhanced\nperformance. The framework’s flexibility accommodates diverse signal modalities and noise sources, allowing exper-\nimentation with various ML models tailored to specific signal characteristics. Additionally, its scalability efficiently\nhandles complex denoising tasks involving large data volumes. By denoising each signal separately before combin-\ning them, the framework ensures robustness against noise fluctuations, ultimately facilitating more accurate clinical\ninterpretation and decision-making without relying solely on labeled data or traditional filtering methods.\n6\nDISCUSSION\nOne of the key objectives is to validate the CPR data generated by the Babbs model using real patient data, once\naccess to real-life CPR data from medical authorities is obtained through our ongoing collaboration with cardiologists\nat the Montreal Heart Institute. This will allow us to verify the data distribution and assess the similarity between the\nsimulated and real-world data. By validating the synthesized data against actual clinical conditions, we aim to ensure\nits accuracy and reliability. After this validation process, our goal is to make the synthesized data publicly available\nfor research purposes, promoting wider validation and further exploration of the algorithm’s potential in improving\nCPR outcomes.\nMoreover, the successful development of the multi-modal ML framework for denoising biomedical signals opens up\nseveral avenues for future research and application. In the forthcoming stages of this research, the model exploration,\nparticularly focusing on integrating cutting-edge hybrid architectures as the base model of autoencoder in our frame-\nwork can bring significant improvement. While different signals may benefit from specific architectural features,\napplying the same architecture-based autoencoder for each modality or signal is not essential in extensive application.\nHowever, it’s essential to recognize that certain signals may inherently benefit from alternative architectures-based\nautoencoders, which excel in capturing temporal dependencies or focusing on specific regions of interest for each\nsignal. For instance, combining transformer and CNN models, such as Convit, Conformer, and related innovations\noffer compelling advantages of multiple algorithms through a single architecture. For example, these models adapt\nthe advantages of both transformer and CNN; however, transformers are adept at capturing long-term dependencies\ninherent in sequential data, while CNN is proven to capture local features and patterns (Yang et al., 2021). By\n20\n\nassessing the performance of such hybrid models within the framework of our denoising methodology, it would\nbe possible to unlock new avenues for enhancing the robustness of the denoising process across biomedical signal\nmodalities.\nBesides, other biomedical signal processing specialized transformer-based models, like, TransHFO,\nCRT-Net, CAT, etc can be experimented with in their denoising performance as the base of the autoencoder in the\nframework (Islam et al., 2024). Furthermore, we can utilize the knowledge from the ML framework for denoising\nto make progress in downstream tasks by applying techniques like transfer learning or few-shot learning, which can\nbe used in the learned representations to adapt easily to new tasks or areas, increasing the proposed framework’s\neffectiveness in real-world situations. In addition, another aim in the future involves validating the simulated data\ngenerated by the Babbs model against real-life CPR data in collaboration with healthcare centers and hospitals. Once\nvalidated, publishing this CPR data for future research purposes will provide a valuable resource, enabling broader\nexploration and advancements in this critical area.\nDenoise &\nEncode\n the features\nDenoise &\nEncode\n the features\nDenoise &\nEncode\n the features\nCPR Signals\nUltraSound\nImages\nECG, EEG\nModel for\nDownstream\ntask\nOutput\nMonitor &\nFeedback\nDecision\nmaking\nMedical Expert\nReinforcement\nLearning Associated\nModel\nFigure 13: A multi-modal ML concept for decision marking in medical emergency.\nAnother promising direction is the extension of the multi-model ML approach to other types of biomedical data\nbeyond CPR signals only. By adapting the framework to accommodate different types of medical modalities and\ncharacteristics, researchers can explore its applicability in diverse medical contexts, such as ECG, electromyography\n(EMG), electroencephalography (EEG), ultrasound images, and so on along with CPR signals, as depicted in Figure\n13. Initially, each type of data can be denoised and its features encoded into lower dimensions using appropriate\ndenoising ML models according to the focus and merit of the specific data. Then, the outputs from all modalities can\nbe trained with a model for downstream tasks to enable appropriate decision-making. Before finalizing decisions, the\noutputs from the multi-modality model should be monitored and reviewed by medical experts. Based on feedback\nfrom clinical experts, the output can be refined before making final decisions. To further enhance and automate the\nentire process, reinforcement learning (RL) can be deployed to learn decision-making policies. By doing so, the\nRL agent would refine its strategies through feedback loops, optimizing the decision-making process and providing\nactionable insights. The predictive model processing data from various modalities would then integrate this RL-based\npolicy, ensuring that decisions are continually improved and adjusted based on real-world inputs and outcomes.\nThrough this concept, better, more precise, and well-informed decision-making would be possible by utilizing more\ninformation from different modalities about a patient in emergency medical settings.\nAnother key aspect is supporting effective explanations of our output to our users. Valued starting points that fit well\nwith our current design and show promise are suggested in the current literature. Holzinger et al. (2022) indicate that\nfusing information from different modalities will be a valuable pathway for explaining AI-driven decision-making to\nmedical professionals. They argue that the data collection can be enhanced by the modality that led to that data for\na deeper analysis of the rationale for the AI system to suggest decision-making paths to diverse health experts. Gu\net al. (2020) point out that despite the black-box nature of CNNs (an important element of our proposed approach),\ngenerating the kind of explanations that health professionals may need can be achieved using attentional mechanisms\nto highlight why certain decisions are being proposed for the medical concern. They add that focusing on visual\ndisplays of key elements of the input data will be most important. A user study conducted by Bienefeld et al. (2023)\nalso showed the importance of considering the mental models of users when deciding both what information to display\nand what options to support within the visual display. Insights into how clinicians searched for deeper insights into\nthe decision-making within the interface were very valuable to learn, towards progressive designs of their dashboard.\nTo maintain effective interpretability of AI system results, Sendak et al. (2020) also points out the importance of\nongoing feedback loops; continued endorsement from medical professionals and thus deployment are then possible.\n21\n\nTheir study concerned the medical condition of sepsis and the interaction with Emergency Medicine departments (AI\nsystems that would be running and would be reacted to in real-time, as is the case with our study of CPR). Loh et al.\n(2022) in their extensive survey of explanation of AI systems for healthcare singled out as a key concern the need to\nexplain biosignal abnormalities towards clinical data interpretation; this suggests that our particular focus in deciding\nhow to handle CPR in clinical settings using AI-based solutions is especially well chosen. The importance of denoising\nwhen working towards effective AI systems for healthcare decisions that include visual images is a point emphasized\nby Dhar et al. (2023) which also endorses our effort to develop an effective approach for this key consideration.\n7\nCONCLUSION\nIn conclusion, the significance of denoising biomedical signals, particularly in crucial life-saving interventions like\nCPR, cannot be overstated. because the accuracy and reliability of CPR data can be a matter of life and death.\nDespite advancements in technology, traditional methods of signal denoising using filters often fall short of effectively\nremoving noise while preserving the integrity of underlying signals. However, a high level of precision is crucial\nconsidering CPR as a matter of human life and death. In this context, the capability of ML to capture the complex\nunderlying patterns is proven in the biomedical domain. Though a few dedicated ML methods for denoising CPR\nsignals can be notified in the domain, those are supervised approaches. However, the unavailability and difficulty of\ngetting the labeled clean signals corresponding to the noisy signal hinder the existing ML method’s potential in real-life\ncontexts. To solve this problem, a dedicated unsupervised ML method for CPR signals is required in the domain, which\ncan process multiple CPR signals concurrently without labeled data. The proposed multi-modal machine learning\nframework addresses this challenge by offering a solution to enhance the quality and utility of vital physiological\nmeasurements in an unsupervised manner, ensuring that noise reduction is achieved without compromising signal\nintegrity. Notably, the methodology excels in preserving data correlation, which is crucial for downstream tasks. The\ncontributions of the proposed methodology can be extended beyond CPR signal denoising, where the methodology\ncan be effective in handling various types of biomedical signals and data, paving the way for broader machine-learning\napplications in healthcare and medical research.\nREFERENCES\nAase, S. O., Eftestol, T., Husoy, J., Sunde, K., & Steen, P. A. (2000). CPR artifact removal from human ECG using optimal\nmultichannel filtering. IEEE Transactions on Biomedical Engineering, 47, 1440–1449.\nAnderson, R., Sebaldt, A., Lin, Y., & Cheng, A. (2019). Optimal training frequency for acquisition and retention of high-quality\nCPR skills: A randomized trial. Resuscitation, 135, 153–161.\nAramendi, E., de Gauna, S. R., Irusta, U., Ruiz, J., Arcocha, M. F., & Ormaetxe, J. M. (2007). Detection of ventricular fibrillation\nin the presence of cardiopulmonary resuscitation artefacts. Resuscitation, 72, 115–123.\nAssegie, T. A., Sushma, S., Bhavya, B., & Padmashree, S. (2021). Correlation analysis for determining effective data in machine\nlearning: detection of heart failure. SN Computer Science, 2, 213.\nAzghadi, M. R., Lammie, C., Eshraghian, J. K., Payvand, M., Donati, E., Linares-Barranco, B., & Indiveri, G. (2020). Hardware\nimplementation of deep network accelerators towards healthcare and biomedical applications. IEEE Transactions on Biomedical\nCircuits and Systems, 14, 1138–1159.\nAzzeh, J., Zahran, B., & Alqadi, Z. (2018). Salt and pepper noise: Effects and removal. JOIV: International Journal on Informatics\nVisualization, 2, 252–256.\nBabbs, C. F., & Thelander, K. (1995). Theoretically optimal duty cycles for chest and abdominal compression during external\ncardiopulmonary resuscitation. Academic Emergency Medicine, 2, 698–707.\nBabbs, C. F., Weaver, J. C., Ralston, S. H., & Geddes, L. A. (1984). Cardiac, thoracic, and abdominal pump mechanisms in\ncardiopulmonary resuscitation: studies in an electrical model of the circulation. The American journal of emergency medicine,\n2, 299–308.\nBaig, M. A., Moinuddin, A. A., & Khan, E. (2019). PSNR of highest distortion region: An effective image quality assessment\nmethod. In 2019 International Conference on Electrical, Electronics and Computer Engineering (UPCON) (pp. 1–4).\nBaubin, M. A., Gilly, H., Posch, A., Schinnerl, A., & Kroesen, G. A. (1995). Compression characteristics of CPR manikins.\nResuscitation, 30, 117–126.\nBienefeld, N., Boss, J. M., L¨uthy, R., Brodbeck, D., Azzati, J., Blaser, M., Willms, J., & Keller, E. (2023). Solving the explainable\nai conundrum by bridging clinicians’ needs and developers’ goals. NPJ Digital Medicine, 6, 94.\nBlum, A. L., & Langley, P. (1997). Selection of relevant features and examples in machine learning. Artificial intelligence, 97,\n245–271.\n22\n\nConsidine, J., Gazmuri, R. J., Perkins, G. D., Kudenchuk, P. J., Olasveengen, T. M., Vaillancourt, C., Nishiyama, C., Hatanaka, T.,\nMancini, M. E., Chung, S. P. et al. (2020). Chest compression components (rate, depth, chest wall recoil and leaning): a scoping\nreview. Resuscitation, 146, 188–202.\nde Campos Souza, P. V., & Dragoni, M. (2024). IFNN: Enhanced interpretability and optimization in FNN via Adam algorithm.\nInformation Sciences, 678, 121002.\nDhar, T., Dey, N., Borra, S., & Sherratt, R. S. (2023). Challenges of deep learning in medical image analysis—improving explain-\nability and trust. IEEE Transactions on Technology and Society, 4, 68–75.\nDi Mitri, D., Schneider, J., Specht, M., & Drachsler, H. (2018). From signals to knowledge: A conceptual model for multimodal\nlearning analytics. Journal of Computer Assisted Learning, 34, 338–349.\nErkan, U., G¨okrem, L., & Engino˘glu, S. (2018). Different applied median filter in salt and pepper noise. Computers & Electrical\nEngineering, 70, 789–798.\nFeng, W., Wang, X., Cao, D., & Lin, D. (2024). An autoencoder-based self-supervised learning for multimodal sentiment analysis.\nInformation Sciences, 675, 120682.\nFitzgerald, K. R., Babbs, C. F., Frissora, H. A., Davis, R. W., & Silver, D. I. (1981). Cardiac output during cardiopulmonary\nresuscitation at various compression rates and durations. American Journal of Physiology-Heart and Circulatory Physiology,\n241, H442–H448.\nFitzgibbon, E., Berger, R., Tsitlik, J., & Halperin, H. R. (2002). Determination of the noise source in the electrocardiogram during\ncardiopulmonary resuscitation. Critical Care Medicine, 30, S148–S153.\nGacek, A. (2011). An introduction to ECG signal processing and analysis. In ECG Signal Processing, Classification and Interpre-\ntation: A Comprehensive Framework of Computational Intelligence (pp. 21–46). Springer.\nGoceri, E. (2023). Evaluation of denoising techniques to remove speckle and Gaussian noise from dermoscopy images. Computers\nin Biology and Medicine, 152, 106474.\nGong, Y., Gao, P., Wei, L., Dai, C., Zhang, L., & Li, Y. (2017). An enhanced adaptive filtering method for suppressing cardiopul-\nmonary resuscitation artifact. IEEE Transactions on Biomedical Engineering, 64, 471–478.\nGong, Y., Yu, T., Chen, B., He, M., Li, Y. et al. (2014). Removal of cardiopulmonary resuscitation artifacts with an enhanced\nadaptive filtering method: an experimental trial. BioMed Research International, 2014.\nGu, R., Wang, G., Song, T., Huang, R., Aertsen, M., Deprest, J., Ourselin, S., Vercauteren, T., & Zhang, S. (2020). Ca-net:\nComprehensive attention convolutional neural networks for explainable medical image segmentation. IEEE transactions on\nmedical imaging, 40, 699–711.\nHajeb-M, S., Cascella, A., Valentine, M., & Chon, K. H. (2022). Enhancing the accuracy of shock advisory algorithms in automated\nexternal defibrillators during ongoing cardiopulmonary resuscitation using a deep convolutional encoder-decoder filtering model.\nExpert Systems with Applications, 203, 117499.\nHallstrom, A., Rea, T. D., Sayre, M. R., Christenson, J., Anton, A. R., Mosesso, V. N., Van Ottingham, L., Olsufka, M., Pennington,\nS., White, L. J. et al. (2006). Manual chest compression vs use of an automated chest compression device during resuscitation\nfollowing out-of-hospital cardiac arrest: a randomized trial. Jama, 295, 2620–2628.\nHinkelbein, J., Kerkhoff, S., Adler, C. et al. (2020). Cardiopulmonary resuscitation (CPR) during spaceflight - a guideline for CPR\nin microgravity from the german society of aerospace medicine (DGLRM) and the european society of aerospace medicine space\nmedicine group (ESAM-SMG). Scandinavian Journal of Trauma, Resuscitation and Emergency Medicine, 28, 1–12.\nHolzinger, A., Dehmer, M., Emmert-Streib, F., Cucchiara, R., Augenstein, I., Ser, J. D., Samek, W., Jurisica, I., & D´ıaz-Rodr´ıguez,\nN. (2022). Information fusion as an integrative cross-cutting enabler to achieve robust, explainable, and trustworthy medical\nartificial intelligence. Information Fusion, 79, 263–278.\nHurt, R. (2005). Modern cardiopulmonary resuscitation—not so new after all. Journal of the Royal Society of Medicine, 98,\n327–331.\nHusoy, J., Eilevstjonn, J., Eftestol, T., Aase, S., Myklebust, H., & Steen, P. (2002). Removal of cardiopulmonary resuscitation\nartifacts from human ECG using an efficient matching pursuit-like algorithm. IEEE Transactions on Biomedical Engineering,\n49, 1287–1298.\nIslam, S., Elmekki, H., Elsebai, A., Bentahar, J., Drawel, N., Rjoub, G., & Pedrycz, W. (2024). A comprehensive survey on\napplications of transformers for deep learning tasks. Expert Systems with Applications, 241, 122666.\nJalali, A., Berg, R. A., Nadkarni, V. M., & Nataraj, C. (2015). Mathematical modeling of cardiopulmonary resuscitation. In\nDynamic Systems and Control Conference (p. V002T34A011). American Society of Mechanical Engineers volume 57250.\n23\n\nJiang, X.-W., Hu, B., Guan, Z.-H., Zhang, X.-H., & Yu, L. (2016). The minimal signal-to-noise ratio required for stability of control\nsystems over a noisy channel in the presence of packet dropouts. Information Sciences, 372, 579–590.\nJung, E., Babbs, C. F., Lenhart, S., & Protopopescu, V. A. (2006).\nOptimal strategy for cardiopulmonary resuscitation with\ncontinuous chest compression. Academic Emergency Medicine, 13, 715–721.\nKarunasingha, D. S. K. (2022). Root mean square error or mean absolute error? use their ratio as well. Information Sciences, 585,\n609–629.\nKassam, S. A. (2012). Signal detection in non-Gaussian noise. Springer Science & Business Media.\nKiranyaz, S., Ince, T., Abdeljaber, O., Avci, O., & Gabbouj, M. (2019). 1-D convolutional neural networks for signal processing\napplications. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp.\n8360–8364). IEEE.\nLi, G., Ullah, S. W., Li, B., Lin, J., & Wang, H. (2020). Baseline wander removal for ECG signals based on improved EMD. 2020\n15th IEEE International Conference on Signal Processing (ICSP), 1, 484–487.\nLi, J., Lu, G., Wu, Z., & Ling, F. (2023). Multi-view representation model based on graph autoencoder. Information Sciences, 632,\n439–453.\nLi, Z., Liu, F., Yang, W., Peng, S., & Zhou, J. (2021). A survey of convolutional neural networks: analysis, applications, and\nprospects. IEEE Transactions on Neural Networks and Learning Systems, 33, 6999–7019.\nLing, Y., Nie, F., Yu, W., Ling, Y., & Li, X. (2024). Robust autoencoder feature selector for unsupervised feature selection.\nInformation Sciences, 660, 120121.\nLoh, H. W., Ooi, C. P., Seoni, S., Barua, P. D., Molinari, F., & Acharya, U. R. (2022). Application of explainable artificial intel-\nligence for healthcare: A systematic review of the last decade (2011–2022). Computer Methods and Programs in Biomedicine,\n226, 107161.\nLuo, J.-H., & Wu, J. (2020). Neural network pruning with residual-connections and limited-data. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp. 1458–1467).\nLuong, D. T., Thuan, N. D., Hoang, C. D., Trang, N. V., & Due, T. Q. (2013). Study on limitation of removal of baseline noise\nfrom electrocardiography signal in measurement using wavelet analysis. In 2013 Fifth International Conference on Ubiquitous\nand Future Networks (ICUFN) (pp. 481–486).\nMuhammad, G., Alshehri, F., Karray, F., Saddik, A. E., Alsulaiman, M., & Falk, T. H. (2021). A comprehensive survey on\nmultimodal medical signals fusion for smart healthcare systems. Information Fusion, 76, 355–375.\nMurach, K. A., & Bagley, J. R. (2016). Skeletal muscle hypertrophy with concurrent exercise training: Contrary evidence for an\ninterference effect. Sports Medicine, 46, 1029–1039.\nNakagawa, Y., Amino, M., Inokuchi, S., Hayashi, S., Wakabayashi, T., & Noda, T. (2017). Novel CPR system that predicts return\nof spontaneous circulation from amplitude spectral area before electric shock in ventricular fibrillation. Resuscitation, 113, 8–12.\nPernagallo, G. (2023). An entropy-based measure of correlation for time series. Information Sciences, 643, 119272.\nQassim, H., Verma, A., & Feinzimer, D. (2018). Compressed residual-vgg16 cnn model for big data places image recognition. In\n2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC) (pp. 169–175).\nRaza, A., Arslan, A., Ali, Z., & Patel, R. (2021). How long should we run the code? survival analysis based on location and duration\nof cardiopulmonary resuscitation (CPR) after in-hospital cardiac arrest.\nJournal of Community Hospital Internal Medicine\nPerspectives, 11, 123–129.\nRice, L., Wong, E., & Kolter, J. Z. (2020). Overfitting in adversarially robust deep learning. In Proceedings of the 37th Interna-\ntional Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event (pp. 8093–8104). PMLR volume 119 of\nProceedings of Machine Learning Research.\nSchelter, S., Biessmann, F., Januschowski, T., Salinas, D., Seufert, S., & Szarvas, G. (2015). On challenges in machine learning\nmodel management. IEEE Data Engineering Bulletin, .\nSendak, M. P., Elish, M. C., Gao, M., Futoma, J., Ratliff, W., Nichols, M., Bedoya, A., Balu, S., & O’Brien, C. (2020). “The\nhuman body is a black box”: supporting clinical decision-making with deep learning. In M. Hildebrandt, C. Castillo, L. E.\nCelis, S. Ruggieri, L. Taylor, & G. Zanfir-Fortuna (Eds.), FAT* ’20: Conference on Fairness, Accountability, and Transparency,\nBarcelona, Spain, January 27-30, 2020 (pp. 99–109). ACM.\nSeron, M. M., Braslavsky, J. H., & Goodwin, G. C. (2012). Fundamental limitations in filtering and control. Springer Science &\nBusiness Media.\n24\n\nShin, D. A., & Lee, J. C. (2021). Mathematical model of modified hybrid pump mechanism for cardiopulmonary resuscitation.\nComputer Methods and Programs in Biomedicine, 206, 106106.\nSu, J., Chen, J., Jiang, H., Zhou, C., Lin, H., Ge, Y., Wu, Q., & Lai, Y. (2021). Multi-modal neural machine translation with deep\nsemantic interactions. Information Sciences, 554, 47–60.\nSwor, R., Khan, I., Domeier, R., Honeycutt, L., Chu, K., & Compton, S. (2006). CPR training and CPR performance: do CPR-\ntrained bystanders perform CPR? Academic Emergency Medicine, 13, 596–601.\nThuwajit, P., Rangpong, P., Sawangjai, P., Autthasan, P., Chaisaen, R., Banluesombatkul, N., Boonchit, P., Tatsaringkansakul, N.,\nSudhawiyangkul, T., & Wilaiprasitporn, T. (2022). EEGWaveNet: Multiscale CNN-based spatiotemporal feature extraction for\nEEG seizure detection. IEEE Transactions on Industrial Informatics, 18, 5547–5557.\nWang, J., Jing, X., Yan, Z., Fu, Y., Pedrycz, W., & Yang, L. T. (2020). A survey on trust evaluation based on machine learning.\nACM Computing Surveys (CSUR), 53, 1–36.\nWu, Q., Wang, H., & Lu, S. (2024). Nonlinear directed acyclic graph estimation based on the kernel partial correlation coefficient.\nInformation Sciences, 654, 119814.\nYang, G., Ye, Q., & Xia, J. (2022). Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data\nfusion: A mini-review, two showcases and beyond. Information Fusion, 77, 29–52.\nYang, R., Zha, X., Liu, K., & Xu, S. (2021). A CNN model embedded with local feature knowledge and its application to time-\nvarying signal classification. Neural Networks, 142, 564–572.\nZhang, H., Yu, X., Li, T., Li, D., Tang, D., & He, L. (2024). Noise-aware and correlation analysis-based for fuzzy-rough feature\nselection. Information Sciences, 659, 120047.\nZhang, L., Xiao, F., & Cao, Z. (2023). Multi-channel EEG signals classification via CNN and multi-head self-attention on evidence\ntheory. Information Sciences, 642, 119107.\n25",
    "pdf_filename": "A_Multi-Modal_Unsupervised_Machine_Learning_Approach_for_Biomedical_Signal_Processing_in_CPR.pdf"
}