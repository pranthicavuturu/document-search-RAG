{
    "title": "Domain Consistency Representation Learning for Lifelong Person Re-Identification",
    "context": "contradictory relationship between intra-domain discrimination and inter-domain gaps when learning from continuous data. Intra-domain discrimination focuses on individual nuances (i.e., clothing type, accessories, etc.), while inter-domain gaps empha- size domain consistency. Achieving a trade-off between maxi- mizing intra-domain discrimination and minimizing inter-domain gaps is a crucial challenge for improving LReID performance. Most existing methods strive to reduce inter-domain gaps through knowledge distillation to maintain domain consistency. However, they often ignore intra-domain discrimination. To address this challenge, we propose a novel domain consistency representation learning (DCR) model that explores global and attribute-wise rep- resentations as a bridge to balance intra-domain discrimination and inter-domain gaps. At the intra-domain level, we explore the complementary relationship between global and attribute- wise representations to improve discrimination among similar identities. Excessive learning intra-domain discrimination can lead to catastrophic forgetting. We further develop an attribute- oriented anti-forgetting (AF) strategy that explores attribute- wise representations to enhance inter-domain consistency, and propose a knowledge consolidation (KC) strategy to facilitate knowledge transfer. Extensive experiments show that our DCR model achieves superior performance compared to state-of-the- art LReID methods. Our code will be available soon. Index Terms—Lifelong learning, person re-identification, do- main consistency representations, attribute and text guided rep- resentations. Person re-identification (ReID) aims to retrieve the same individual across non-overlapping cameras in a large-scale database, and has achieved significant progress using uni- modal architectures such as convolutional neural networks (CNN) [3, 4] or vision transformers (ViT) [5–7]. However, when ReID models are applied to continuous datasets col- lected by dynamic monitoring systems, they exhibit notable This work is supported by the National Natural Science Foundation of China (62273339, 61991413, U20A20200), and the Youth Innovation Promo- tion Association of Chinese Academy of Sciences (2019203). (Corresponding author: Huijie Fan) Shiben Liu is with the State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang 110016, China, and with the University of Chinese Academy of Sciences, Beijing 100049, China (e-mail: liushiben@sia.cn). Huijie Fan, and Yandong Tang are with the State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110016, China (e-mail: fanhuiie@sia.cn; ytang@sia.cn). Qiang Wang is the Key Laboratory of Manufacturing Industrial Integrated in Shenyang University Shenyang 110044, China (e-mail: wangqiang@sia.cn). Weihong Ren is the Harbin Institute of Technology, Shenzhen 518055, China (e-mail: renweihong@hit.edu.cn). Baojie Fan is with the Department of Automation college at Nanjing University of Posts and Telecommunications, Nanjing 210000, China. (e-mail: jobfbj@gmail.com). Fig. 1. Comparison between our method and existing methods. (a) Existing methods [1, 2] leverage knowledge distillation to minimize inter-domain gaps but ignore intra-domain discrimination, which limits the LReID model’s ability to learn new knowledge. (b) Our method explores attribute-wise representations as a bridge to achieve a trade-off between maximizing intra- domain discrimination and minimizing inter-domain gaps, enhancing the LReID model’s anti-forgetting and generalization capabilities. performance limitations. Thus recent works have focused more on the practical problem of lifelong person re-identification (LReID), which involves learning from streaming data and maintaining strong performance across all data. At present, lifelong person re-identification (LReID) suffers from the challenge of balancing the anti-forgetting of old knowledge and learning new knowledge. Specifically, there are two main issues to solve this challenge. 1) Intra-domain discrimination. Each identity may exhibit subtle nuances of individual information (i.e., clothing type, accessories, haircut, etc.) and lead to severe distribution overlapping. Learning discriminative representations of individuals are effective for distinguish identity information. 2) Inter-domain gaps. The dataset of each task is collected in different illumination and background, leading to inter-domain gaps. Bridging intra- arXiv:2409.19954v2  [cs.CV]  19 Nov 2024",
    "body": "1\nDomain Consistency Representation Learning for\nLifelong Person Re-Identification\nShiben Liu\n, Huijie Fan*\n, Qiang Wang\n, Weihong Ren\n, BaojieFan\n, Yandong Tang\nAbstract—Lifelong person re-identification (LReID) exhibits a\ncontradictory relationship between intra-domain discrimination\nand inter-domain gaps when learning from continuous data.\nIntra-domain discrimination focuses on individual nuances (i.e.,\nclothing type, accessories, etc.), while inter-domain gaps empha-\nsize domain consistency. Achieving a trade-off between maxi-\nmizing intra-domain discrimination and minimizing inter-domain\ngaps is a crucial challenge for improving LReID performance.\nMost existing methods strive to reduce inter-domain gaps through\nknowledge distillation to maintain domain consistency. However,\nthey often ignore intra-domain discrimination. To address this\nchallenge, we propose a novel domain consistency representation\nlearning (DCR) model that explores global and attribute-wise rep-\nresentations as a bridge to balance intra-domain discrimination\nand inter-domain gaps. At the intra-domain level, we explore\nthe complementary relationship between global and attribute-\nwise representations to improve discrimination among similar\nidentities. Excessive learning intra-domain discrimination can\nlead to catastrophic forgetting. We further develop an attribute-\noriented anti-forgetting (AF) strategy that explores attribute-\nwise representations to enhance inter-domain consistency, and\npropose a knowledge consolidation (KC) strategy to facilitate\nknowledge transfer. Extensive experiments show that our DCR\nmodel achieves superior performance compared to state-of-the-\nart LReID methods. Our code will be available soon.\nIndex Terms—Lifelong learning, person re-identification, do-\nmain consistency representations, attribute and text guided rep-\nresentations.\nI. INTRODUCTION\nPerson re-identification (ReID) aims to retrieve the same\nindividual across non-overlapping cameras in a large-scale\ndatabase, and has achieved significant progress using uni-\nmodal architectures such as convolutional neural networks\n(CNN) [3, 4] or vision transformers (ViT) [5–7]. However,\nwhen ReID models are applied to continuous datasets col-\nlected by dynamic monitoring systems, they exhibit notable\nThis work is supported by the National Natural Science Foundation of\nChina (62273339, 61991413, U20A20200), and the Youth Innovation Promo-\ntion Association of Chinese Academy of Sciences (2019203). (Corresponding\nauthor: Huijie Fan)\nShiben Liu is with the State Key Laboratory of Robotics, Shenyang Institute\nof Automation, Chinese Academy of Sciences, Shenyang 110016, China, and\nwith the University of Chinese Academy of Sciences, Beijing 100049, China\n(e-mail: liushiben@sia.cn).\nHuijie Fan, and Yandong Tang are with the State Key Laboratory of\nRobotics, Shenyang Institute of Automation, Chinese Academy of Sciences,\nShenyang, 110016, China (e-mail: fanhuiie@sia.cn; ytang@sia.cn).\nQiang Wang is the Key Laboratory of Manufacturing Industrial Integrated\nin Shenyang University Shenyang 110044, China (e-mail: wangqiang@sia.cn).\nWeihong Ren is the Harbin Institute of Technology, Shenzhen 518055,\nChina (e-mail: renweihong@hit.edu.cn).\nBaojie Fan is with the Department of Automation college at Nanjing\nUniversity of Posts and Telecommunications, Nanjing 210000, China. (e-mail:\njobfbj@gmail.com).\nFig. 1. Comparison between our method and existing methods. (a) Existing\nmethods [1, 2] leverage knowledge distillation to minimize inter-domain\ngaps but ignore intra-domain discrimination, which limits the LReID model’s\nability to learn new knowledge. (b) Our method explores attribute-wise\nrepresentations as a bridge to achieve a trade-off between maximizing intra-\ndomain discrimination and minimizing inter-domain gaps, enhancing the\nLReID model’s anti-forgetting and generalization capabilities.\nperformance limitations. Thus recent works have focused more\non the practical problem of lifelong person re-identification\n(LReID), which involves learning from streaming data and\nmaintaining strong performance across all data.\nAt present, lifelong person re-identification (LReID) suffers\nfrom the challenge of balancing the anti-forgetting of old\nknowledge and learning new knowledge. Specifically, there\nare two main issues to solve this challenge. 1) Intra-domain\ndiscrimination. Each identity may exhibit subtle nuances of\nindividual information (i.e., clothing type, accessories, haircut,\netc.) and lead to severe distribution overlapping. Learning\ndiscriminative representations of individuals are effective for\ndistinguish identity information. 2) Inter-domain gaps. The\ndataset of each task is collected in different illumination and\nbackground, leading to inter-domain gaps. Bridging intra-\narXiv:2409.19954v2  [cs.CV]  19 Nov 2024\n\n2\ndomain gaps are significant for mitigating catastrophic for-\ngetting in LReID.\nTo address these issues, we aim to learn consistency repre-\nsentations that capture individual nuances in intra-domain and\ninter-domain consistency in LReID, striking a balance between\nmaximizing intra-domain discrimination and minimizing inter-\ndomain gaps. Knowledge distillation-based approaches [2, 8–\n10] ensure distribution consistency between the previous and\ncurrent datasets to alleviate catastrophic forgetting. However,\nthese approaches impose strict constraints and ignore intra-\ndomain discrimination, [11–13], as outlined in Figure 1(a).\nWhile LReID models significantly improve intra-domain dis-\ncrimination for the current step, they inevitably damage inter-\ndomain consistency, leading to catastrophic forgetting. Thus,\nwe propose consistency representations as a bridge to achieve a\ntrade-off between maximizing intra-domain discrimination and\nminimizing inter-domain gaps, improving the anti-forgetting\nand generalization capabilities of the LReID model, as illus-\ntrated in Figure 1(b).\nSpecifically, we propose a novel domain consistency rep-\nresentation learning (DCR) model, which first explores at-\ntribute and text information to enhance LReID performance.\nUnlike methods [14–16], we develop consistency represen-\ntations including global and attribute-wise representations to\ncapture individual nuances in intra-domain and inter-domain\nconsistency in LReID. We design an attribute-text generator\n(ATG) to dynamically generate text-image pairs for each\ninstance, which are then fed into a text-guided aggregation\n(TGA) network to improve the global representation capability,\neffectively distinguishing identities in LReID. In addition,\nthe attributes of each instance guide an attribute compensa-\ntion (ACN) network to generate attribute-wise representations\nfocusing on specific regional information of identities. We\nconsider that attributes can ensure reliability by setting higher\nthresholds across datasets. Therefore, the generated attribute-\nwise representations and text for each instance are considered\nreliable in our model.\nIn summary, we explore global representations and attribute-\nwise representations to strike a balance between maximiz-\ning identity-discriminative information of intra-domain and\nminimizing inter-domain gaps. At the intra-domain discrim-\nination level, global representations capture whole-body in-\nformation, while attribute-wise representations focus on spe-\ncific regional information. When whole-body appearances or\nattribute-related information are very similar across identities,\nwe combine global and attribute-wise representations to distin-\nguish among similar identities, maximizing intra-domain dis-\ncrimination. Perfect learning intra-domain discrimination can\nlead to catasttophic forgetting. We further develop an attribute-\noriented anti-forgetting (AF) strategy that explore attribute-\nwise representations for bridging inter-domain gaps across\ncontinuous datasets. Additionally, knowledge consolidation\n(KC) is proposed to enable knowledge transfer, improving\ngeneralization capabilities. Our contributions are as follows:\n• We propose a novel domain consistency representation\nlearning (DCR) model that explores global and attribute-\nwise representations to capture individual nuances in\nintra-domain and inter-domain consistency, achieving a\ntrade-off between maximizing intra-domain discrimina-\ntion and minimizing inter-domain gaps.\n• In the intra-domain context, we explore the comple-\nmentary relationship between global and attribute-wise\nrepresentations to enhance the discrimination of each\nidentity and adapt to new knowledge.\n• In the inter-domain context, we design an attribute-\noriented anti-forgetting (AF) and a knowledge consol-\nidation (KC) strategy to minimize inter-domain gaps\nand facilitate knowledge transfer, improving the LReID\nmodel’s generalization and anti-forgetting capabilities.\nA. Lifelong Person Re-Identification\nLifelong\nPerson\nRe-Identification\n(LReID)\nfaces\na\nformidable challenge, aiming to address the evolving nature\nof person identification across various scenarios. Some works\n[12, 17] are proposed to tackle the issue of adapting ReID\nmodels over time while retaining knowledge gained from\nprevious distribution. Generally, Pu et al. [17] proposed\nlearnable knowledge graphs that adaptively facilitate the\nmutual exchange of new and old knowledge, thus achieving\nknowledge accumulation. Some works [1, 8, 9, 14] aim\nto extract rich and discriminative representation, mitigating\nthe risk of knowledge forgetting. Pu et al. [8] proposed\na\nmeta\nreconciliation\nnormalization\n(MRN)\nfor\nmining\nmeta-knowledge shared across different domains. Meanwhile,\nConRFL [9] maintains learnable and consistent features\nacross all seen domains, which improves the discrimination\nand adaptation ability of the LReID model. In addition,\nsome methods [1, 2, 17] mitigate catastrophic forgetting and\nenhance model accuracy by using rehearsal-based strategies\nwith images stored from previous tasks. These approaches\nstrive to reduce inter-domain gaps, ensuring distribution\nconsistency across datasets to mitigate catastrophic forgetting.\nHowever,\nthis\nstrategy\nemploys\nstrict\nconstraints\nand\nignores intra-domain discrimination, limiting LReID model’s\nperformance to learn new knowledge. In this paper, we\npropose consistency representations as a bridge to achieve\na trade-off between maximizing intra-domain discrimination\nand minimizing inter-domain gaps for improving the anti-\nforgetting and generalization capabilities of the LReID model.\nB. Vision-Language for Person Re-Identification\nThe vision-language learning paradigms [18–20] have\ngained considerable popularity in recent years. Contrastive\nLanguage-Image\nPre-training\n(CLIP)\n[21],\nestablishes\na\nconnection between natural language and visual content\nthrough the similarity constraint of image-text pair. CLIP\nhas been applied to multiple person re-identification tasks\n[22, 23], including text-to-image, text-based single-modality,\ntext-based cross modality. Text-to-image methods [24–26]\naims to retrieve the target person based on a textual query.\nText-based\nsingle-modality\nworks\n[5,\n6]\nleverage\ntext\ndescriptions to generate robust visual features or to integrate\nthe beneficial features of text and images for the person\n\n3\nFig. 2. Overview of the proposed DCR for LReID. First, the attribute-text generator (ATG) dynamically generates text-image pairs for each instance. Then, the\ntext-guided aggregation network (TGA) captures global representations for each identity, while the attribute compensation network (ACN) generates attribute-\nwise representations. We explore the complementary relationship between global and attribute-wise representations to maximize intra-domain discrimination.\nMeanwhile, we design attribute-oriented anti-forgetting (AF) and knowledge consolidation (KC) strategies to minimize inter-domain gaps and facilitate\nknowledge transfer.\ncategory. Text-based cross modality methods [27] employ text\ndescriptions to alleviate visible-infrared modality gaps. Text\ninfromation generated by prompt learning and text inversion,\nproviding insufficient text descriptions of each identity. In this\npaper, we dynamically generate text-image pairs from single\nimage to capture fine-grained global representations based\non the CLIP model for improving inter-domain discrimination.\nC. Pedestrian Attribute Recognition\nPedestrian attribute recognition aims to assign a set of\nattributes (Gender, Bag, Short/Long sleeve, and etc.) to a visual\nrepresentation of a pedestrian. Deep learning-based researches\n[28] automatically learn hierarchical features from raw im-\nages, improving recognition accuracy. Multi-task learning ap-\nproaches [29–31] leverage additional contextual information of\nacross tasks, such as pedestrian detection or pose estimation, to\nsignificantly improve attribute recognition. Part-based methods\n[32, 33] divide the pedestrian image into several parts or\nregions, providing more accurate attribute localization. At\npresent, the above methods have achieved significant success\nin improving the accuracy of attribute recognition. We are the\nfirst to explore the application of attributes to LReID from the\nfollowing two aspects. 1) Attributes are converted into text\ndescriptions for each image to enhance global representation\ncapabilities. 2) Attributes are transformed into attribute-wise\nrepresentations by specific networks to maximize intra-domain\ndiscrimination and minimize intra-domain gaps.\nII. PROPOSED METHOD\nA. Preliminary: Overview of Method\nThe overview of our DCR model to achieve a trade-off\nbetween maximizing intra-domian discrimination and mini-\nmizing inter-domain gaps, is depicted as Figure 2. The DCR\nmodel learns the old model Φt−1 and new model Φt from (t-\n1)-th and t-th steps, where Φt is inherited from Φt−1. Φt−1 and\nΦt with three branches of attribute-text generator (ATG), text-\nguided aggregation network (TGA) and attribute compensation\nnetwork (ACN). ϕt−1 and ϕt serve as classifier heads for\nthe old and new models, providing logits of each instance\nfor recognition. Additionally, we define that consecutive T\nperson datasets D = {Dt}T\nt=1 are collected from different\nenvironments, and establish a memory buffer M to store a\nlimited number of samples from each previous ReID dataset.\n\n4\nGiven an image xt\ni∈Dt∪M, we forward it to Φt−1 and Φt is\nas follows:\nGt−1, AGt−1 = Φt−1(xi);\nGt, AGt = Φt(xi).\n(1)\nB. Attribute-Text Generator\nDue to the lack of text-image pairs in ReID datasets, we pro-\npose an attribute-text generator (ATG) to dynamically generate\ncorresponding text descriptions for each instance. Specifically,\nwe first introduce an attribute recognition model pre-trained\non the PA100K dataset [35] to generate attribute categories\n(i.e., female, backpack, short/long sleeve, and etc.), which are\nthen converted into text descriptions for each instance using\na specific template. This template adds modifiers (in black\nfont) to each attribute (in a different color font) to create a\ncomplete sentence describing an instance, as shown in Figure\n2. Although, attributes can vary significantly across datasets,\nwe consider that text descriptions can be made reliable by\nsetting a higher threshold (Confidence threshold=0.80) to\nensure classification accuracy of attribute recognition network.\nC. Text-Guided Aggregation Netwrok\nWe propose a text-guided aggregation network (TGA) to\nexplore global representations for each identity and knowledge\ntransfer, as shown in Figure 2 (TGA). The TGA includes a\nCLIP model and a parallel fusion module (PFM). Note that\nthe text encoder is frozen in our DCR model.\nParallel Fusion Module. By attribute-text generator obtain\ntext-image pairs, we employ CLIP with text encoder T (·)\nand image encoder V(·) to extract text and image embedding,\nrespectively. Unlike CLIP [21], we introduce multiple [CLS]\nembeddings into the image encoder input sequence to capture\nmultiple global representations from different perspectives. To\nobtain fine-grained global representations for improving the\nperformance of the LReID model, we propose a parallel fusion\nmodule (PFM) to explicitly explore the interactions between\nimage embeddings and text embeddings, as shown in Figure\n2 (PFM). Firstly, we leverage text embedding d∗as query\nand image embedding [v∗\n1, · · · , v∗\nN, v1, · · · , vP ] as key and\nvalue to implement operation with cross-attention, drop, and\nlayer normalization, getting text-wise representations. Simi-\nlarly, in another fusion branch, image-wise representations are\nobtained. Finally, image-wise and text-wise representations\nperform concatenation and MLP operations to obtain global\nrepresentations Gt = {Gi|i = 1, 2, · · · , N}, focusing on\nwhole body information. We force multiple global reprsen-\ntatios Gt at the current step to learn more discriminative\ninformation by orthogonal loss to minimize the overlapping\nelements. The orthogonal loss can be formulated as:\nLOrt =\nN−1\nX\ni=1\nN\nX\nj=i+1\n(Gt\ni, Gt\nj)\n(2)\nThen, we utilize the cross-entropy loss LCE and triplet loss\nLg\nTri [6] to optimize our DCR at the current task.\nLCE = 1\nK\nK\nX\ni=1\nyi log((ϕt(Gt))i)\n(3)\nLg\nTri = max(dg\np −dg\nn + m, 0)\n(4)\nwhere K is the number of classes, and m is the margin, dg\np and\ndg\nn are the distances from positive samples and negative sam-\nples to anchor samples in global representations, respectively.\nUnlike some methods [2, 11], global representations generated\nby the text-guided aggregation (TGA) network present two\nadvantages. First, we leverage text descriptions based on the\nCLIP model to enhance the discrimination capability of global\nrepresentations, allowing them to better distinguish identities\nand adapt to new knowledge. Second, global representations\nfacilitate knowledge transfer, improving the model’s general-\nization ability.\nD. Attribute Compensation Network\nWe force attributes to guide attribute compensation network\n(ACN) for learning attribute-wise representations. The ACN\nconsists of an attribute decoder and an attribute matching\ncomponent, as illustrated in Figure 2 (ACN).\nAttribute Decoder. Enabling attributes to better adapt across\ndatasets, we define multiple learnable attribute semantic in-\nformation A∗= {A∗\ni |i = 1, 2, · · · , N} to learn discrimi-\nnative information. The attributes undergo a linear layer to\nincrease its dimensions, and then multiplies with the text-\nimage global representation to output fAT . Attribute semantic\ninformation A∗as queries Q, fAT as keys and values are input\ninto attribute decoder, which outputs the attribute features\nA = {Ai|i = 1, 2, · · · , N}. The attribute decoder employs\nsix transformer blocks (T Block) referenced from [36].\nAttribute Matching. The attribute features A = {Ai|i =\n1, 2, · · · , N} learn multiple discriminative local infromation of\nindividuals. However, it is unclear which attribute features cor-\nrespond to specific body parts. Thus, we propose an attribute\nmatching (AM) component to associate attribute features and\nglobal representations G = {Gi|i = 1, 2, · · · , N}. The core\nobjective is to find the most similar global representationsG\nfrom different perspectives and local attribute features A, and\nthen add the them with the highest similarity. Specifically,\nattribute-wise representations AGt = {AGi|i = 1, 2, · · · , N}\nis formulated as:\nk = argmax(< Ai, G >\n|Ai||G|\n)\n(5)\nAGi = Ai + Gk.\n(6)\nWe leverage the triplet loss to align attribute-wise repre-\nsentations with identity at the current step, assisting in global\nrepresentations to distinguish similar identities.\nLl\nTri = max(dp −dn + m, 0)\n(7)\nwhere, dl\np and dl\nn are the distances from positive samples\nand negative samples to anchor samples in attribute-wise\nrepresentations, respectively. In this paper, attribute-wise rep-\nresentations that contain specific information of individuals\nassist global representations in distinguishing similar identi-\nties for maximizing intra-domain discrimination. Meanwhile,\nattribute-wise representations as a bridge across increasing\ndatasets to minimize inter-domain gaps for better knowledge\ntransfer.\n\n5\nTABLE I\nPERFORMANCE COMPARISON WITH STATE-OF-THE-ART METHODS ON TRAINING ORDER-2. BASELINE MODEL INCLUDES CLIP MODEL AND\nATTRIBUTE-TEXT GENERATOR. BOLD AND RED FONTS ARE OPTIMAL AND SUBOPTIMAL VALUES, RESPECTIVELY. TRAINING ORDER-1 IS\nMARKET→CUHK-SYSU→DUKEMTMC→MSMT17 V2→CUHK03.\nMethod\nMarket1501\nCUHK-SYSU\nDukeMTMC\nMSMT17 V2\nCUHK03\nSeen-Avg\nUnseen-Avg\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\nAKA[17]\n58.1\n77.4\n72.5\n74.8\n28.7\n45.2\n6.1\n16.2\n38.7\n40.4\n40.8\n50.8\n42.0\n39.8\nPTKP[1]\n64.4\n82.8\n79.8\n81.9\n45.6\n63.4\n10.4\n25.9\n42.5\n42.9\n48.5\n59.4\n51.2\n49.1\nPatchKD[14]\n68.5\n85.7\n75.6\n78.6\n33.8\n50.4\n6.5\n17.0\n34.1\n36.8\n43.7\n53.7\n45.1\n43.3\nKRKC[2]\n54.0\n77.7\n83.4\n85.4\n48.9\n65.5\n14.1\n33.7\n49.9\n50.4\n50.1\n62.5\n52.7\n50.8\nConRFL[9]\n59.2\n78.3\n82.1\n84.3\n45.6\n61.8\n12.6\n30.4\n51.7\n53.8\n50.2\n61.7\n-\n-\nCODA[50]\n53.6\n76.9\n75.7\n78.1\n48.6\n59.5\n13.2\n31.3\n47.2\n48.6\n47.7\n58.9\n44.5\n42.4\nLSTKC[13]\n54.7\n76.0\n81.1\n83.4\n49.4\n66.2\n20.0\n43.2\n44.7\n46.5\n50.0\n63.1\n51.3\n48.9\nC2R[16]\n69.0\n86.8\n76.7\n79.5\n33.2\n48.6\n6.6\n17.4\n35.6\n36.2\n44.2\n53.7\n-\n-\nDKP[15]\n60.3\n80.6\n83.6\n85.4\n51.6\n68.4\n19.7\n41.8\n43.6\n44.2\n51.8\n64.1\n49.9\n46.4\nBaseline\n61.6\n79.1\n80.2\n80.6\n50.2\n64.3\n15.1\n36.5\n44.9\n46.8\n50.4\n61.5\n51.8\n49.4\nOurs\n75.9\n87.9\n87.3\n88.5\n60.1\n71.9\n25.3\n50.1\n60.5\n61.3\n61.8\n71.9\n60.8\n58.3\nTABLE II\nPERFORMANCE COMPARISON WITH STATE-OF-THE-ART METHODS ON TRAINING ORDER-2. BASELINE MODEL INCLUDES CLIP MODEL AND\nATTRIBUTE-TEXT GENERATOR. BOLD AND RED FONTS ARE OPTIMAL AND SUBOPTIMAL VALUES, RESPECTIVELY. TRAINING ORDER-2 IS\nDUKEMTMC→MSMT17 V2→MARKET→CUHK-SYSU→CUHK03.\nMethod\nDukeMTMC\nMSMT17 V2\nMarket1501\nCUHK-SYSU\nCUHK03\nSeen-Avg\nUnseen-Avg\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\nAKA[17]\n42.2\n60.1\n5.4\n15.1\n37.2\n59.8\n71.2\n73.9\n36.9\n37.9\n38.6\n49.4\n41.3\n39.0\nPTKP[1]\n54.8\n70.2\n10.3\n23.3\n59.4\n79.6\n80.9\n82.8\n41.6\n42.9\n49.4\n59.8\n50.8\n48.2\nPatchKD[14]\n58.3\n74.1\n6.4\n17.4\n43.2\n67.4\n74.5\n76.9\n33.7\n34.8\n43.2\n54.1\n44.8\n43.3\nKRKC[2]\n50.6\n65.6\n13.6\n27.4\n56.2\n77.4\n83.5\n85.9\n46.7\n46.6\n50.1\n61.0\n52.1\n47.7\nConRFL[9]\n34.4\n51.3\n7.6\n20.1\n61.6\n80.4\n82.8\n85.1\n49.0\n50.1\n47.1\n57.4\n-\n-\nCODA[50]\n38.7\n56.6\n11.6\n24.5\n54.3\n75.1\n76.2\n75.8\n42.3\n41.7\n44.6\n54.7\n45.0\n42.9\nLSTKC[13]\n49.9\n67.6\n14.6\n34.0\n55.1\n76.7\n82.3\n83.8\n46.3\n48.1\n49.6\n62.1\n51.7\n49.5\nC2R[16]\n59.7\n75.0\n7.3\n19.2\n42.4\n66.5\n76.0\n77.8\n37.8\n39.3\n44.7\n55.6\n-\n-\nDKP[15]\n53.4\n70.5\n14.5\n33.3\n60.6\n81.0\n83.0\n84.9\n45.0\n46.1\n51.3\n63.2\n51.3\n47.8\nBaseline\n53.8\n69.1\n14.1\n29.8\n59.8\n80.4\n78.4\n78.5\n45.3\n44.9\n50.3\n60.5\n52.2\n49.9\nOurs\n64.1\n77.2\n25.4\n44.9\n70.6\n84.5\n86.1\n88.2\n54.2\n58.7\n60.1\n70.7\n61.6\n59.2\nE. Attribute-oriented Anti-Forgetting\nWe develop an attribute-oriented anti-forgetting (AF) strat-\negy to explore attribute-wise representations for aligning the\ndistributions of the old and new models, as shown in Figure 2\n(AF). Specifically, the new model adapts to new information\nbut may forget old knowledge from the previous dataset, while\nthe old model retains old knowledge. To better preserve old\nknowledge, we leverage attribute-wise representations as a\nbridge to optimize both the old and new models using sam-\nples from the memory buffer. This strategy achieves domain\nconsistency and minimize inter-domain gaps, alleviating the\nforgetting of old knowledge, and is calculated as follows:\nLAF = 1\nB\nB\nX\ni=1\nKL(AGt−1\nN /τ||AGt\nN/τ)\n(8)\nwhere KL(.||.) refers to a kullback-leibler divergence, and τ\nis a hyperparameter called temperature [37].\nF. Knowledge Consolidation\nMaximizing intra-domain discrimination and minimizing\ninter-domain gaps are in a contradictory relationship. There-\nfore, achieving a balance between them is crucial for improv-\ning the performance of LReID models. Thus,we propose a\nknowledge consolidation (KC) strategy that leverages global\nrepresentations for knowledge transfer between the old and\nnew models. This includes alignment loss and logit-level\ndistillation loss.\nMaintaining distribution consistency between the old and\nnew models for previous datasets can limit the model’s ability\nto learn new knowledge. Therefore, we propose an alignment\nloss to explore global representations for knowledge transfer\nfrom the current dataset, as follows:\nLAL = 1\nB\nB\nX\ni=1\nKL(Gt−1/τ||Gt/τ)\n(9)\nWe further introduce a logit-level distillation loss to enhance\nthe extraction of identity information shared between the old\nand new models, further improving the model’s knowledge\nconsolidation ability. This is represented as follows:\nLLD = 1\nB\nB\nX\ni=1\nKL((ϕt−1(Gt−1))i/τ||(ϕt(Gt))i/τ)\n(10)\n\n6\nFig. 3. t-SNE visualization of feature distribution on five seen datasets. Our method better narrows the distribution across datasets for minimizing inter-domain\ngaps, improving the anti-forgetting and generalization ability of the model.\nThe knowledge consolidation loss is defined as:\nLKC = LAL + LLD\n(11)\nThe total loss function is formulated as:\nL = LCE + Lg\nT ri + Ll\nT ri + LOrt + LAF + LKC\n(12)\nIII. EXPERIMENTS\nA. Experiments Setting\nDatasets. To verify the performance of our method in\nanti-forgetting and generalization, we evaluate our method\non a challenging benchmark consisting of Market1501 [38],\nCUHK-SYSU [39], DukeMTMC [40], MSMT17 V2 [41] and\nCUHK03 [42], referred to as seen datasets. Two representative\ntraining orders are set up following the protocol described in\n[17] for training and testing. Further, we employ six datasets\nincluding VIPeR [43], GRID [44], CUHK02 [45], Occ Duke\n[46], Occ REID [47], and PRID2011 [48], as unseen dataset.\nImplementation Details. Our text encoder and image encoder\nare based on a pre-trained CLIP model, while the attribute\ndecoder utilizes a transformer-based architecture[36]. All per-\nson images are resized to 256×128. We use Adam [49] for\noptimization and train each task for 60 epochs. The batch size\nis set to 128. The learning rate is initialized at 5×10−6 and is\ndecreased by a factor of 0.1 every 20 epochs for each task. We\nemploy mean average precision (mAP) and Rank-1 accuracy\n(R-1) to evaluate the LReID model on each dataset.\nB. Comparison with SOTA Methods\nWe compare the proposed DCR with SOTA LReID\nto\ndemonstrate\nthe\nsuperiority\nof\nour\nmethod,\ninclud-\ning\nAKA[17],\nPTKP[1],\nPatchKD[14],\nKRKC[2],\nand\nConRFL[9], DKP[15], C2R[16], LSTKC[13]. Experimental\nresults on training order-1 and order-2 are shown in Table\nI and Table II, respectively. CODA [50] method employs ViT-\nB/16 as the backbone.\nCompared with LReID methods. In Table I and Table II,\nOur DCR significantly outperforms LReID methods, with\nan seen-avg incremental gain of 10.0% mAP/7.8% R-1, and\nFig. 4.\nVisualization of intra-domain discrimination on the Market1501\ndataset. We randomly select 30 identities. Colors represent different identity\ninformation. Our DCR model can cluster images of the same identity more\ntightly (circle) for minimizing inter-domian discrimination.\n9.8% mAP/7.5% R-1 on training order-1 and order-2, respec-\ntively. Meanwhile, our DCR effectively alleviate catastrophic\nforgetting, achieving 6.9% mAp/1.1% R-1, and 5.4% mAP/\n2.2% R-1 improvement on the first dataset (Mrket1501 and\nDukeMTMC) with different training orders. Compared to\nCODA, our DCR significantly outpreform performance under\nthe backbone of VIT-B/16. Additionally, our DCR improves\nthe average by 8.1 mAP%/7.5% R-1 and 9.5% mAP/11.0%\nR-1 on unseen datasets. In contrast, our DCR achieves a trade-\noff between anti-forgetting and acquiring new information,\nsignificantly enhances generalization capabilities.\nCompared with Baseline. Due to the lack of CLIP-based\ncomparison methods in LReID, we introduce a Baseline model\nincluding CLIP model, attribute-text generator and knowledge\nconsolidation strategy. Compared to the Baseline, Our DCR\nimproves the Seen-Avg by 11.4% mAP/10.4% R-1 and by\n9.8% mAP/10.2% R-1. These results demonstrate that our\nproposed attribute-wise representations learning achieves sig-\nnificant performance in balancing maximization intra-domain\ndiscrimination and minimization inter-domain gaps in LReID.\nThe effectiveness of minimizing inter-domain gaps. We\nvisualize the feature distribution of PTKP, KRKC, DKP, and\nour method across five datasets, as shown in Figure 3. DKP\nshows poor performance in bridging inter-domain gaps, as\nknowledge prototypes struggle to fit the data distribution.\nCompared to other methods, our DCR model better narrows\nthe distribution across increasing datasets. Thus, the proposed\n\n7\nFig. 5.\nGeneralization curves. After each training step, the performance of\nall unseen domains is evaluated.\nTABLE III\nABLATION STUDIES ON THE NUMBER OF GLOBAL AND ATTRIBUTE-WISE\nREPRESENTATIONS N ON TRAINING ORDER-1.\nNumber (N)\nSeen Avg\nUnseen Avg\nmAP\nR-1\nmAP\nR-1\n2\n60.2\n68.7\n59.4\n56.5\n3\n61.8\n71.9\n60.8\n58.3\n4\n61.2\n71.6\n60.3\n57.5\nDCR model can effectively bridge domain gaps, improving\nknowledge transfer capabilities, benefiting from the attribute-\noriented anti-forgetting (AF) strategy based on attribute-wise\nrepresentations.\nThe effectiveness of maximizing intra-domain discrimina-\ntion. We visualize the feature distribution of KRKC and our\nmethod. Figure 4 shows that our DCR model can significantly\ncluster images of the same identity more tightly (circle)\nand increase the distance between different identities (black\nbidirectional arrow). Compared to KRKC, our DCR model\neffectively improves intra-domain discrimination, benefiting\nfrom the complementary relationship between global and\nattribute representations, which allows it to learn the subtle\nnuances of individuals.\nGeneralization Curves on Unseen Dataset. We analyze the\naverage performance on unseen datasets over the training\nsteps, as shown in Figure 5. Compared to other methods,\nour DCR model achieves superior performance and exhibits\nfaster performance growth across the training steps. Thus,\nour attribute-oriented anti-forgetting (AF) strategy effectively\nbridges inter-domain gaps, enhancing the generalization ability\nof our model. In summary, our DCR model explores global and\nattribute-wise representations to achieve a trade-off between\nmaximizing intra-domain discrimination and minimizing inter-\ndomain gaps.\nC. Ablation Studies\nThe number of global and attribute-wise repreentations.\nGlobal and attribute-wise representations capture individual\nnuances in intra-domain and inter-domain consistency. We\nstudy the suitability of multiple global and attribute-wise\nTABLE IV\nABLATION STUDIES OF DIFFERENT COMPONENTS ON TRAINING ORDER-1.\nPFM\nACN\nAF\nKC\nSeen Avg\nUnseen Avg\nmAP\nR-1\nmAP\nR-1\n50.4\n61.5\n51.8\n49.4\n√\n51.7\n62.1\n52.5\n50.3\n√\n√\n57.6\n68.9\n58.2\n56.2\n√\n√\n√\n58.7\n69.2\n58.5\n56.8\n√\n√\n√\n√\n61.8\n71.9\n60.8\n58.3\nTABLE V\nABLATION OF TRAINING WITH OR WITHOUT ATTRIBUTE-TEXT\nGENERATOR (ATG) ON TRAINING ORDER-1.\nMethod\nSeen Avg\nUnseen Avg\nmAP\nR-1\nmAP\nR-1\nTraining w/o ATG\n60.1\n70.5\n59.3\n56.5\nTraining w/ ATG\n61.8\n71.9\n60.8\n58.3\nrepresentations, as shown in Table III. We observe that setting\nthe number of global and attribute-wise representations N to\n3 achieves the best performance for our method.\nPerformance of Different Components. To assess the con-\ntribution of each component to our DCR, we conduct ablation\nstudies on seen and unseen datasets, as shown in Table IV.\nComparing the first and second rows, we observe that the\nparallel fusion module (PFM), which employs a parallel cross-\nattention mechanism, effectively fuses text and image embed-\ndings. Comparing the second and fourth rows, we consider\nthat the attribute compensation network (ACN) and attribute-\noriented anti-forgetting (AF) strategy effectively learn domain\nconsistency, improving generalization ability. In the second\nand third rows, we notice the performance decrease when\nusing only the knowledge consolidation (KC) strategy based\non global representations across increasing data while ignoring\ninter-domain gaps. The results demonstrate that both global\nrepresentations and attribute-wise representations achieve a\ntrade-off between maximizing intra-domain discrimination\nand minimizing inter-domain gaps for enhancing the anti-\nforgetting and generalization capacity of our DCR.\nPerformance of attribute-text generator. To better un-\nderstand whether each instance’s text descriptions generated\nby the attribute-text generator (ATG) provide more fine-\ngrained guidance for learning global representations, we train\nour model using the generic text descriptor ”A photo of a\nperson” (w/o ATG) for comparison. Table V shows that the\nattribute-text generator obtain text descriptions to significantly\nimproves overall performance. When using the specific text\ndescriptors, the average decreases by 1.7% mAP/1.4% R-1 on\nseen datasets and by 1.5% mAP/1.8% R-1 on unseen datasets.\nATG enhances the robustness of global representations for\neach instance, effectively mitigating the forgetting of old\nknowledge.\n\n8\nIV. CONCLUSION\nIn this paper, we propose a domain consistency repre-\nsentation learning (DCR) model that explores global and\nattribute-wise representations to capture subtle nuances in\nintra-domain and inter-domain consistency, achieving a trade-\noff between maximizing intra-domain discrimination and min-\nimizing inter-domain gaps. Specifically, global and attribute-\nwise representations serve as complementary information to\ndistinguish similar identities in intra-domain. We further de-\nvelop an attribute-oriented anti-forgetting (AF) strategy and\na knowledge consolidation (KC) strategy to minimize inter-\ndomain gaps and facilitate knowledge transfer, enhancing\ngeneralization capabilities. Extensive experiments demonstrate\nthat our method achieves superior performance compared to\nstate-of-the-art LReID methods.\nREFERENCES\n[1] Wenhang Ge, Junlong Du, Ancong Wu, Yuqiao Xian,\nKe Yan, Feiyue Huang, and Wei-Shi Zheng. Lifelong\nperson re-identification by pseudo task knowledge preser-\nvation. In AAAI, volume 36, pages 688–696, 2022.\n[2] Chunlin Yu, Ye Shi, Zimo Liu, Shenghua Gao, and Jingya\nWang. Lifelong person re-identification via knowledge\nrefreshing and consolidation. In AAAI, volume 37, pages\n3295–3303, 2023.\n[3] Anguo Zhang, Yueming Gao, Yuzhen Niu, Wenxi\nLiu, and Yongcheng Zhou.\nCoarse-to-fine person re-\nidentification with auxiliary-domain classification and\nsecond-order information bottleneck.\nIn CVPR, pages\n598–607, 2021.\n[4] Huijie Fan, Xiaotong Wang, Qiang Wang, Shengpeng\nFu, and Yandong Tang.\nSkip connection aggregation\ntransformer for occluded person reidentification. IEEE\nTransactions on Industrial Informatics, 20(1):442–451,\n2023.\n[5] Siyuan Li, Li Sun, and Qingli Li. Clip-reid: exploiting\nvision-language model for image re-identification with-\nout concrete text labels.\nIn AAAI, volume 37, pages\n1405–1413, 2023.\n[6] Zexian Yang, Dayan Wu, Chenming Wu, Zheng Lin,\nJingzi Gu, and Weiping Wang. A pedestrian is worth\none prompt: Towards language guidance person re-\nidentification. In CVPR, pages 17343–17353, 2024.\n[7] Tao Wang, Hong Liu, Pinhao Song, Tianyu Guo, and\nWei Shi. Pose-guided feature disentangling for occluded\nperson re-identification based on transformer. In AAAI,\nvolume 36, pages 2540–2549, 2022.\n[8] Nan Pu, Yu Liu, Wei Chen, Erwin M Bakker, and\nMichael S Lew. Meta reconciliation normalization for\nlifelong person re-identification.\nIn ACM MM, pages\n541–549, 2022.\n[9] Jinze Huang, Xiaohan Yu, Dong An, Yaoguang Wei,\nXiao Bai, Jin Zheng, Chen Wang, and Jun Zhou. Learn-\ning consistent region features for lifelong person re-\nidentification. Pattern Recognition, 144:109837, 2023.\n[10] Yuming Yan, Huimin Yu, Yubin Wang, Shuyi Song,\nWeihu Huang, and Juncan Jin.\nUnified stability and\nplasticity for lifelong person re-identification in cloth-\nchanging and cloth-consistent scenarios.\nIEEE Trans-\nactions on Circuits and Systems for Video Technology,\n2024.\n[11] Guile Wu and Shaogang Gong.\nGeneralising without\nforgetting for lifelong person re-identification. In AAAI,\nvolume 35, pages 2889–2897, 2021.\n[12] Lei Zhang, Guanyu Gao, and Huaizheng Zhang. Spatial-\ntemporal federated learning for lifelong person re-\nidentification on distributed edges.\nIEEE Transactions\non Circuits and Systems for Video Technology, 2023.\n[13] Kunlun Xu, Xu Zou, and Jiahuan Zhou.\nLstkc: Long\nshort-term knowledge consolidation for lifelong person\nre-identification.\nIn AAAI, volume 38, pages 16202–\n16210, 2024.\n[14] Zhicheng Sun and Yadong Mu. Patch-based knowledge\ndistillation for lifelong person re-identification. In ACM\nMM, pages 696–707, 2022.\n[15] Kunlun Xu, Xu Zou, Yuxin Peng, and Jiahuan Zhou.\nDistribution-aware\nknowledge\nprototyping\nfor\nnon-\nexemplar lifelong person re-identification.\nIn CVPR,\npages 16604–16613, 2024.\n[16] Zhenyu Cui, Jiahuan Zhou, Xun Wang, Manyu Zhu, and\nYuxin Peng. Learning continual compatible representa-\ntion for re-indexing free lifelong person re-identification.\nIn CVPR, pages 16614–16623, 2024.\n[17] Nan Pu, Wei Chen, Yu Liu, Erwin M Bakker, and\nMichael S Lew.\nLifelong person re-identification via\nadaptive knowledge accumulation.\nIn CVPR, pages\n7901–7910, 2021.\n[18] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian\nLu. Vision-language models for vision tasks: A survey.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2024.\n[19] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu.\nLearning to prompt for vision-language\nmodels.\nInternational Journal of Computer Vision,\n130(9):2337–2348, 2022.\n[20] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu.\nConditional prompt learning for vision-\nlanguage models. In CVPR, pages 16816–16825, 2022.\n[21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al.\nLearning transferable visual models from natural lan-\nguage supervision. In International conference on ma-\nchine learning, pages 8748–8763. PMLR, 2021.\n[22] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang,\nMing Yan, Bin Bi, Jiabo Ye, He Chen, Guohai Xu, Zheng\nCao, et al. mplug: Effective and efficient vision-language\nlearning by cross-modal skip-connections. In Empirical\nMethods in Natural Language Processing, pages 7241–\n7259, 2022.\n[23] Chenyang Yu, Xuehu Liu, Yingquan Wang, Pingping\nZhang, and Huchuan Lu.\nTf-clip: Learning text-free\nclip for video-based person re-identification. In AAAI,\nvolume 38, pages 6764–6772, 2024.\n[24] Zhiyin Shao, Xinyu Zhang, Changxing Ding, Jian Wang,\n\n9\nand Jingdong Wang. Unified pre-training with pseudo\ntexts for text-to-image person re-identification. In ICCV,\npages 11174–11184, 2023.\n[25] Yang Qin, Yingke Chen, Dezhong Peng, Xi Peng,\nJoey Tianyi Zhou, and Peng Hu. Noisy-correspondence\nlearning for text-to-image person re-identification.\nIn\nCVPR, pages 27197–27206, 2024.\n[26] Xinyi Wu, Wentao Ma, Dan Guo, Tongqing Zhou, Shan\nZhao, and Zhiping Cai.\nText-based occluded person\nre-identification via multi-granularity contrastive consis-\ntency learning. In AAAI, volume 38, pages 6162–6170,\n2024.\n[27] Yunhao Du, Zhicheng Zhao, and Fei Su. Yyds: Visible-\ninfrared person re-identification with coarse descriptions.\narXiv preprint arXiv:2403.04183, 2024.\n[28] Jian Jia, Houjing Huang, Xiaotang Chen, and Kaiqi\nHuang. Rethinking of pedestrian attribute recognition:\nA reliable evaluation under zero-shot pedestrian identity\nsetting. arXiv preprint arXiv:2107.03576, 2021.\n[29] Haoyun Sun, Hongwei Zhao, Weishan Zhang, Liang\nXu, and Hongqing Guan. Adaptive multi-task learning\nfor multi-par in real-world.\nIEEE Journal of Radio\nFrequency Identification, 2024.\n[30] Yunfei Zhou and Xiangrui Zeng. Towards comprehensive\nunderstanding of pedestrians for autonomous driving:\nEfficient multi-task-learning-based pedestrian detection,\ntracking and attribute recognition.\nRobotics and Au-\ntonomous Systems, 171:104580, 2024.\n[31] Xinwen Fan, Yukang Zhang, Yang Lu, and Hanzi Wang.\nParformer: Transformer-based multi-task network for\npedestrian attribute recognition. IEEE Transactions on\nCircuits and Systems for Video Technology, 34(1):411–\n423, 2023.\n[32] Jian Jia, Naiyu Gao, Fei He, Xiaotang Chen, and Kaiqi\nHuang. Learning disentangled attribute representations\nfor robust pedestrian attribute recognition.\nIn AAAI,\nvolume 36, pages 1069–1077, 2022.\n[33] Jian Jia, Xiaotang Chen, and Kaiqi Huang.\nSpatial\nand semantic consistency regularizations for pedestrian\nattribute recognition. In ICCV, pages 962–971, 2021.\n[34] Xiaoyan Yu, Neng Dong, Liehuang Zhu, Hao Peng, and\nDapeng Tao. Clip-driven semantic discovery network for\nvisible-infrared person re-identification. arXiv preprint\narXiv:2401.05806, 2024.\n[35] Xihui Liu, Haiyu Zhao, Maoqing Tian, Lu Sheng,\nJing Shao, Shuai Yi, Junjie Yan, and Xiaogang Wang.\nHydraplus-net: Attentive deep features for pedestrian\nanalysis. In ICCV, pages 350–359, 2017.\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances\nin neural information processing systems, 30, 2017.\n[37] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling\nthe knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2015.\n[38] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang,\nJingdong Wang, and Qi Tian.\nScalable person re-\nidentification: A benchmark. In ICCV, pages 1116–1124,\n2015.\n[39] Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and\nXiaogang Wang.\nEnd-to-end deep learning for person\nsearch. arXiv preprint arXiv:1604.01850, 2(2):4, 2016.\n[40] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cuc-\nchiara, and Carlo Tomasi.\nPerformance measures and\na data set for multi-target, multi-camera tracking.\nIn\nECCV, pages 17–35. Springer, 2016.\n[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.\nPerson transfer gan to bridge domain gap for person re-\nidentification. In CVPR, pages 79–88, 2018.\n[42] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang.\nDeepreid: Deep filter pairing neural network for person\nre-identification. In CVPR, pages 152–159, 2014.\n[43] Douglas Gray and Hai Tao. Viewpoint invariant pedes-\ntrian recognition with an ensemble of localized features.\nIn ECCV, pages 262–275. Springer, 2008.\n[44] Chen Change Loy, Tao Xiang, and Shaogang Gong.\nTime-delayed correlation analysis for multi-camera ac-\ntivity understanding. International Journal of Computer\nVision, 90:106–129, 2010.\n[45] Wei Li and Xiaogang Wang.\nLocally aligned feature\ntransforms across views.\nIn CVPR, pages 3594–3601,\n2013.\n[46] Jiaxu Miao, Yu Wu, Ping Liu, Yuhang Ding, and Yi Yang.\nPose-guided feature alignment for occluded person re-\nidentification. In ICCV, pages 542–551, 2019.\n[47] Jiaxuan Zhuo, Zeyu Chen, Jianhuang Lai, and Guang-\ncong Wang. Occluded person re-identification. In ICME,\npages 1–6. IEEE, 2018.\n[48] Martin Hirzer, Csaba Beleznai, Peter M Roth, and Horst\nBischof.\nPerson re-identification by descriptive and\ndiscriminative classification.\nIn Image Analysis: 17th\nScandinavian Conference, pages 91–102. Springer, 2011.\n[49] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\n[50] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta,\nPaola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle,\nRameswar Panda, Rogerio Feris, and Zsolt Kira. Coda-\nprompt: Continual decomposed attention-based prompt-\ning for rehearsal-free continual learning. In CVPR, pages\n11909–11919, 2023.",
    "pdf_filename": "Domain_Consistency_Representation_Learning_for_Lifelong_Person_Re-Identification.pdf"
}