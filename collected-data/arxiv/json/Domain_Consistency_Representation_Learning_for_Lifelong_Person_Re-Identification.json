{
    "title": "1",
    "abstract": "contradictory relationship between intra-domain discrimination and inter-domain gaps when learning from continuous data. Intra-domain discrimination focuses on individual nuances (i.e., clothing type, accessories, etc.), while inter-domain gaps empha- size domain consistency. Achieving a trade-off between maxi- mizingintra-domaindiscriminationandminimizinginter-domain gaps is a crucial challenge for improving LReID performance. Mostexistingmethodsstrivetoreduceinter-domaingapsthrough knowledge distillation to maintain domain consistency. However, they often ignore intra-domain discrimination. To address this challenge,weproposeanoveldomainconsistencyrepresentation learning(DCR)modelthatexploresglobalandattribute-wiserep- resentations as a bridge to balance intra-domain discrimination and inter-domain gaps. At the intra-domain level, we explore the complementary relationship between global and attribute- wise representations to improve discrimination among similar identities. Excessive learning intra-domain discrimination can lead to catastrophic forgetting. We further develop an attribute- oriented anti-forgetting (AF) strategy that explores attribute- wise representations to enhance inter-domain consistency, and propose a knowledge consolidation (KC) strategy to facilitate knowledge transfer. Extensive experiments show that our DCR model achieves superior performance compared to state-of-the- art LReID methods. Our code will be available soon. Index Terms—Lifelong learning, person re-identification, do- main consistency representations, attribute and text guided rep- resentations. I. INTRODUCTION Fig.1. Comparisonbetweenourmethodandexistingmethods.(a)Existing Person re-identification (ReID) aims to retrieve the same methods [1, 2] leverage knowledge distillation to minimize inter-domain gapsbutignoreintra-domaindiscrimination,whichlimitstheLReIDmodel’s individual across non-overlapping cameras in a large-scale ability to learn new knowledge. (b) Our method explores attribute-wise database, and has achieved significant progress using uni- representationsasabridgetoachieveatrade-offbetweenmaximizingintra- modal architectures such as convolutional neural networks domain discrimination and minimizing inter-domain gaps, enhancing the LReIDmodel’santi-forgettingandgeneralizationcapabilities. (CNN) [3, 4] or vision transformers (ViT) [5–7]. However, when ReID models are applied to continuous datasets col- lected by dynamic monitoring systems, they exhibit notable performancelimitations.Thusrecentworkshavefocusedmore on the practical problem of lifelong person re-identification This work is supported by the National Natural Science Foundation of China(62273339,61991413,U20A20200),andtheYouthInnovationPromo- (LReID), which involves learning from streaming data and tionAssociationofChineseAcademyofSciences(2019203).(Corresponding maintaining strong performance across all data. author:HuijieFan) Atpresent,lifelongpersonre-identification(LReID)suffers ShibenLiuiswiththeStateKeyLaboratoryofRobotics,ShenyangInstitute ofAutomation,ChineseAcademyofSciences,Shenyang110016,China,and from the challenge of balancing the anti-forgetting of old withtheUniversityofChineseAcademyofSciences,Beijing100049,China knowledge and learning new knowledge. Specifically, there (e-mail:liushiben@sia.cn). are two main issues to solve this challenge. 1) Intra-domain Huijie Fan, and Yandong Tang are with the State Key Laboratory of Robotics,ShenyangInstituteofAutomation,ChineseAcademyofSciences, discrimination. Each identity may exhibit subtle nuances of Shenyang,110016,China(e-mail:fanhuiie@sia.cn;ytang@sia.cn). individualinformation(i.e.,clothingtype,accessories,haircut, QiangWangistheKeyLaboratoryofManufacturingIndustrialIntegrated etc.) and lead to severe distribution overlapping. Learning inShenyangUniversityShenyang110044,China(e-mail:wangqiang@sia.cn). Weihong Ren is the Harbin Institute of Technology, Shenzhen 518055, discriminative representations of individuals are effective for China(e-mail:renweihong@hit.edu.cn). distinguish identity information. 2) Inter-domain gaps. The Baojie Fan is with the Department of Automation college at Nanjing dataset of each task is collected in different illumination and UniversityofPostsandTelecommunications,Nanjing210000,China.(e-mail: jobfbj@gmail.com). background, leading to inter-domain gaps. Bridging intra- 4202 voN 91 ]VC.sc[ 2v45991.9042:viXra",
    "body": "1\nDomain Consistency Representation Learning for\nLifelong Person Re-Identification\nShiben Liu , Huijie Fan* , Qiang Wang , Weihong Ren , BaojieFan , Yandong Tang\nAbstract—Lifelong person re-identification (LReID) exhibits a\ncontradictory relationship between intra-domain discrimination\nand inter-domain gaps when learning from continuous data.\nIntra-domain discrimination focuses on individual nuances (i.e.,\nclothing type, accessories, etc.), while inter-domain gaps empha-\nsize domain consistency. Achieving a trade-off between maxi-\nmizingintra-domaindiscriminationandminimizinginter-domain\ngaps is a crucial challenge for improving LReID performance.\nMostexistingmethodsstrivetoreduceinter-domaingapsthrough\nknowledge distillation to maintain domain consistency. However,\nthey often ignore intra-domain discrimination. To address this\nchallenge,weproposeanoveldomainconsistencyrepresentation\nlearning(DCR)modelthatexploresglobalandattribute-wiserep-\nresentations as a bridge to balance intra-domain discrimination\nand inter-domain gaps. At the intra-domain level, we explore\nthe complementary relationship between global and attribute-\nwise representations to improve discrimination among similar\nidentities. Excessive learning intra-domain discrimination can\nlead to catastrophic forgetting. We further develop an attribute-\noriented anti-forgetting (AF) strategy that explores attribute-\nwise representations to enhance inter-domain consistency, and\npropose a knowledge consolidation (KC) strategy to facilitate\nknowledge transfer. Extensive experiments show that our DCR\nmodel achieves superior performance compared to state-of-the-\nart LReID methods. Our code will be available soon.\nIndex Terms—Lifelong learning, person re-identification, do-\nmain consistency representations, attribute and text guided rep-\nresentations.\nI. INTRODUCTION\nFig.1. Comparisonbetweenourmethodandexistingmethods.(a)Existing\nPerson re-identification (ReID) aims to retrieve the same methods [1, 2] leverage knowledge distillation to minimize inter-domain\ngapsbutignoreintra-domaindiscrimination,whichlimitstheLReIDmodel’s\nindividual across non-overlapping cameras in a large-scale\nability to learn new knowledge. (b) Our method explores attribute-wise\ndatabase, and has achieved significant progress using uni- representationsasabridgetoachieveatrade-offbetweenmaximizingintra-\nmodal architectures such as convolutional neural networks domain discrimination and minimizing inter-domain gaps, enhancing the\nLReIDmodel’santi-forgettingandgeneralizationcapabilities.\n(CNN) [3, 4] or vision transformers (ViT) [5–7]. However,\nwhen ReID models are applied to continuous datasets col-\nlected by dynamic monitoring systems, they exhibit notable\nperformancelimitations.Thusrecentworkshavefocusedmore\non the practical problem of lifelong person re-identification\nThis work is supported by the National Natural Science Foundation of\nChina(62273339,61991413,U20A20200),andtheYouthInnovationPromo- (LReID), which involves learning from streaming data and\ntionAssociationofChineseAcademyofSciences(2019203).(Corresponding maintaining strong performance across all data.\nauthor:HuijieFan)\nAtpresent,lifelongpersonre-identification(LReID)suffers\nShibenLiuiswiththeStateKeyLaboratoryofRobotics,ShenyangInstitute\nofAutomation,ChineseAcademyofSciences,Shenyang110016,China,and from the challenge of balancing the anti-forgetting of old\nwiththeUniversityofChineseAcademyofSciences,Beijing100049,China knowledge and learning new knowledge. Specifically, there\n(e-mail:liushiben@sia.cn).\nare two main issues to solve this challenge. 1) Intra-domain\nHuijie Fan, and Yandong Tang are with the State Key Laboratory of\nRobotics,ShenyangInstituteofAutomation,ChineseAcademyofSciences, discrimination. Each identity may exhibit subtle nuances of\nShenyang,110016,China(e-mail:fanhuiie@sia.cn;ytang@sia.cn). individualinformation(i.e.,clothingtype,accessories,haircut,\nQiangWangistheKeyLaboratoryofManufacturingIndustrialIntegrated\netc.) and lead to severe distribution overlapping. Learning\ninShenyangUniversityShenyang110044,China(e-mail:wangqiang@sia.cn).\nWeihong Ren is the Harbin Institute of Technology, Shenzhen 518055, discriminative representations of individuals are effective for\nChina(e-mail:renweihong@hit.edu.cn). distinguish identity information. 2) Inter-domain gaps. The\nBaojie Fan is with the Department of Automation college at Nanjing\ndataset of each task is collected in different illumination and\nUniversityofPostsandTelecommunications,Nanjing210000,China.(e-mail:\njobfbj@gmail.com). background, leading to inter-domain gaps. Bridging intra-\n4202\nvoN\n91\n]VC.sc[\n2v45991.9042:viXra\n2\ndomain gaps are significant for mitigating catastrophic for- intra-domain and inter-domain consistency, achieving a\ngetting in LReID. trade-off between maximizing intra-domain discrimina-\nTo address these issues, we aim to learn consistency repre- tion and minimizing inter-domain gaps.\nsentations that capture individual nuances in intra-domain and • In the intra-domain context, we explore the comple-\ninter-domainconsistencyinLReID,strikingabalancebetween mentary relationship between global and attribute-wise\nmaximizingintra-domaindiscriminationandminimizinginter- representations to enhance the discrimination of each\ndomain gaps. Knowledge distillation-based approaches [2, 8– identity and adapt to new knowledge.\n10] ensure distribution consistency between the previous and • In the inter-domain context, we design an attribute-\ncurrent datasets to alleviate catastrophic forgetting. However, oriented anti-forgetting (AF) and a knowledge consol-\nthese approaches impose strict constraints and ignore intra- idation (KC) strategy to minimize inter-domain gaps\ndomain discrimination, [11–13], as outlined in Figure 1(a). and facilitate knowledge transfer, improving the LReID\nWhile LReID models significantly improve intra-domain dis- model’s generalization and anti-forgetting capabilities.\ncrimination for the current step, they inevitably damage inter-\ndomain consistency, leading to catastrophic forgetting. Thus,\nA. Lifelong Person Re-Identification\nweproposeconsistencyrepresentationsasabridgetoachievea\nLifelong Person Re-Identification (LReID) faces a\ntrade-offbetweenmaximizingintra-domaindiscriminationand\nformidable challenge, aiming to address the evolving nature\nminimizing inter-domain gaps, improving the anti-forgetting\nof person identification across various scenarios. Some works\nand generalization capabilities of the LReID model, as illus-\n[12, 17] are proposed to tackle the issue of adapting ReID\ntrated in Figure 1(b).\nmodels over time while retaining knowledge gained from\nSpecifically, we propose a novel domain consistency rep-\nprevious distribution. Generally, Pu et al. [17] proposed\nresentation learning (DCR) model, which first explores at-\nlearnable knowledge graphs that adaptively facilitate the\ntribute and text information to enhance LReID performance.\nmutual exchange of new and old knowledge, thus achieving\nUnlike methods [14–16], we develop consistency represen-\nknowledge accumulation. Some works [1, 8, 9, 14] aim\ntations including global and attribute-wise representations to\nto extract rich and discriminative representation, mitigating\ncapture individual nuances in intra-domain and inter-domain\nthe risk of knowledge forgetting. Pu et al. [8] proposed\nconsistency in LReID. We design an attribute-text generator\na meta reconciliation normalization (MRN) for mining\n(ATG) to dynamically generate text-image pairs for each\nmeta-knowledge shared across different domains. Meanwhile,\ninstance, which are then fed into a text-guided aggregation\nConRFL [9] maintains learnable and consistent features\n(TGA)networktoimprovetheglobalrepresentationcapability,\nacross all seen domains, which improves the discrimination\neffectively distinguishing identities in LReID. In addition,\nand adaptation ability of the LReID model. In addition,\nthe attributes of each instance guide an attribute compensa-\nsome methods [1, 2, 17] mitigate catastrophic forgetting and\ntion (ACN) network to generate attribute-wise representations\nenhance model accuracy by using rehearsal-based strategies\nfocusing on specific regional information of identities. We\nwith images stored from previous tasks. These approaches\nconsider that attributes can ensure reliability by setting higher\nstrive to reduce inter-domain gaps, ensuring distribution\nthresholds across datasets. Therefore, the generated attribute-\nconsistency across datasets to mitigate catastrophic forgetting.\nwise representations and text for each instance are considered\nHowever, this strategy employs strict constraints and\nreliable in our model.\nignores intra-domain discrimination, limiting LReID model’s\nInsummary,weexploreglobalrepresentationsandattribute-\nperformance to learn new knowledge. In this paper, we\nwise representations to strike a balance between maximiz-\npropose consistency representations as a bridge to achieve\ning identity-discriminative information of intra-domain and\na trade-off between maximizing intra-domain discrimination\nminimizing inter-domain gaps. At the intra-domain discrim-\nand minimizing inter-domain gaps for improving the anti-\nination level, global representations capture whole-body in-\nforgettingandgeneralizationcapabilitiesoftheLReIDmodel.\nformation, while attribute-wise representations focus on spe-\ncific regional information. When whole-body appearances or\nattribute-related information are very similar across identities,\nB. Vision-Language for Person Re-Identification\nwecombineglobalandattribute-wiserepresentationstodistin-\nguish among similar identities, maximizing intra-domain dis- The vision-language learning paradigms [18–20] have\ncrimination. Perfect learning intra-domain discrimination can gained considerable popularity in recent years. Contrastive\nleadtocatasttophicforgetting.Wefurtherdevelopanattribute- Language-Image Pre-training (CLIP) [21], establishes a\noriented anti-forgetting (AF) strategy that explore attribute- connection between natural language and visual content\nwise representations for bridging inter-domain gaps across through the similarity constraint of image-text pair. CLIP\ncontinuous datasets. Additionally, knowledge consolidation has been applied to multiple person re-identification tasks\n(KC) is proposed to enable knowledge transfer, improving [22, 23], including text-to-image, text-based single-modality,\ngeneralization capabilities. Our contributions are as follows: text-based cross modality. Text-to-image methods [24–26]\naims to retrieve the target person based on a textual query.\n• We propose a novel domain consistency representation Text-based single-modality works [5, 6] leverage text\nlearning (DCR) model that explores global and attribute- descriptions to generate robust visual features or to integrate\nwise representations to capture individual nuances in the beneficial features of text and images for the person\n3\nFig.2. OverviewoftheproposedDCRforLReID.First,theattribute-textgenerator(ATG)dynamicallygeneratestext-imagepairsforeachinstance.Then,the\ntext-guidedaggregationnetwork(TGA)capturesglobalrepresentationsforeachidentity,whiletheattributecompensationnetwork(ACN)generatesattribute-\nwiserepresentations.Weexplorethecomplementaryrelationshipbetweenglobalandattribute-wiserepresentationstomaximizeintra-domaindiscrimination.\nMeanwhile, we design attribute-oriented anti-forgetting (AF) and knowledge consolidation (KC) strategies to minimize inter-domain gaps and facilitate\nknowledgetransfer.\ncategory.Text-basedcrossmodalitymethods[27]employtext firsttoexploretheapplicationofattributestoLReIDfromthe\ndescriptions to alleviate visible-infrared modality gaps. Text following two aspects. 1) Attributes are converted into text\ninfromation generated by prompt learning and text inversion, descriptions for each image to enhance global representation\nprovidinginsufficienttextdescriptionsofeachidentity.Inthis capabilities. 2) Attributes are transformed into attribute-wise\npaper, we dynamically generate text-image pairs from single representationsbyspecificnetworkstomaximizeintra-domain\nimage to capture fine-grained global representations based discrimination and minimize intra-domain gaps.\nontheCLIPmodelforimprovinginter-domaindiscrimination.\nII. PROPOSEDMETHOD\nA. Preliminary: Overview of Method\nC. Pedestrian Attribute Recognition\nThe overview of our DCR model to achieve a trade-off\nPedestrian attribute recognition aims to assign a set of between maximizing intra-domian discrimination and mini-\nattributes(Gender,Bag,Short/Longsleeve,andetc.)toavisual mizing inter-domain gaps, is depicted as Figure 2. The DCR\nrepresentationofapedestrian.Deeplearning-basedresearches model learns the old model Φt−1 and new model Φt from (t-\n[28] automatically learn hierarchical features from raw im- 1)-thandt-thsteps,whereΦtisinheritedfromΦt−1.Φt−1and\nages, improving recognition accuracy. Multi-task learning ap- Φt withthreebranchesofattribute-textgenerator(ATG),text-\nproaches[29–31]leverageadditionalcontextualinformationof guidedaggregationnetwork(TGA)andattributecompensation\nacrosstasks,suchaspedestriandetectionorposeestimation,to network (ACN). ϕt−1 and ϕt serve as classifier heads for\nsignificantlyimproveattributerecognition.Part-basedmethods the old and new models, providing logits of each instance\n[32, 33] divide the pedestrian image into several parts or for recognition. Additionally, we define that consecutive T\nregions, providing more accurate attribute localization. At person datasets D = {Dt}T are collected from different\nt=1\npresent, the above methods have achieved significant success environments, and establish a memory buffer M to store a\nin improving the accuracy of attribute recognition. We are the limited number of samples from each previous ReID dataset.\n4\nGiven an image xt∈Dt∪M, we forward it to Φt−1 and Φt is Lg =max(dg−dg +m,0) (4)\ni Tri p n\nas follows:\nwhereK isthenumberofclasses,andmisthemargin,dg and\np\nGt−1,AGt−1 =Φt−1(xi); Gt,AGt =Φt(xi). (1) dg are the distances from positive samples and negative sam-\nn\nples to anchor samples in global representations, respectively.\nUnlikesomemethods[2,11],globalrepresentationsgenerated\nB. Attribute-Text Generator\nby the text-guided aggregation (TGA) network present two\nDuetothelackoftext-imagepairsinReIDdatasets,wepro-\nadvantages. First, we leverage text descriptions based on the\nposeanattribute-textgenerator(ATG)todynamicallygenerate\nCLIPmodeltoenhancethediscriminationcapabilityofglobal\ncorrespondingtextdescriptionsforeachinstance.Specifically,\nrepresentations, allowing them to better distinguish identities\nwe first introduce an attribute recognition model pre-trained\nand adapt to new knowledge. Second, global representations\non the PA100K dataset [35] to generate attribute categories\nfacilitate knowledge transfer, improving the model’s general-\n(i.e., female, backpack, short/long sleeve, and etc.), which are\nization ability.\nthen converted into text descriptions for each instance using\na specific template. This template adds modifiers (in black\nD. Attribute Compensation Network\nfont) to each attribute (in a different color font) to create a\ncomplete sentence describing an instance, as shown in Figure Weforceattributestoguideattributecompensationnetwork\n2. Although, attributes can vary significantly across datasets, (ACN) for learning attribute-wise representations. The ACN\nwe consider that text descriptions can be made reliable by consists of an attribute decoder and an attribute matching\nsetting a higher threshold (Confidence threshold=0.80) to component, as illustrated in Figure 2 (ACN).\nensureclassificationaccuracyofattributerecognitionnetwork. Attribute Decoder. Enabling attributes to better adapt across\ndatasets, we define multiple learnable attribute semantic in-\nformation A∗ = {A∗|i = 1,2,··· ,N} to learn discrimi-\nC. Text-Guided Aggregation Netwrok i\nnative information. The attributes undergo a linear layer to\nWe propose a text-guided aggregation network (TGA) to\nincrease its dimensions, and then multiplies with the text-\nexploreglobalrepresentationsforeachidentityandknowledge\nimage global representation to output f . Attribute semantic\nAT\ntransfer, as shown in Figure 2 (TGA). The TGA includes a informationA∗ asqueriesQ,f askeysandvaluesareinput\nAT\nCLIP model and a parallel fusion module (PFM). Note that\ninto attribute decoder, which outputs the attribute features\nthe text encoder is frozen in our DCR model.\nA = {A |i = 1,2,··· ,N}. The attribute decoder employs\ni\nParallel Fusion Module. By attribute-text generator obtain\nsix transformer blocks (T Block) referenced from [36].\ntext-image pairs, we employ CLIP with text encoder T(·)\nAttribute Matching. The attribute features A = {A |i =\ni\nand image encoder V(·) to extract text and image embedding,\n1,2,··· ,N}learnmultiplediscriminativelocalinfromationof\nrespectively. Unlike CLIP [21], we introduce multiple [CLS]\nindividuals.However,itisunclearwhichattributefeaturescor-\nembeddings into the image encoder input sequence to capture\nrespond to specific body parts. Thus, we propose an attribute\nmultipleglobalrepresentationsfromdifferentperspectives.To\nmatching (AM) component to associate attribute features and\nobtain fine-grained global representations for improving the\nglobal representations G = {G |i = 1,2,··· ,N}. The core\ni\nperformanceoftheLReIDmodel,weproposeaparallelfusion\nobjective is to find the most similar global representationsG\nmodule (PFM) to explicitly explore the interactions between\nfrom different perspectives and local attribute features A, and\nimage embeddings and text embeddings, as shown in Figure\nthen add the them with the highest similarity. Specifically,\n2 (PFM). Firstly, we leverage text embedding d∗ as query attribute-wise representations AGt = {AG |i = 1,2,··· ,N}\nand image embedding [v∗, ···, v∗ , v , ···, v ] as key and i\n1 N 1 P is formulated as:\nvalue to implement operation with cross-attention, drop, and\n<A ,G>\nlayer normalization, getting text-wise representations. Simi- k =argmax( i ) (5)\n|A ||G|\nlarly,inanotherfusionbranch,image-wiserepresentationsare i\nobtained. Finally, image-wise and text-wise representations AG i =A i+G k. (6)\nperform concatenation and MLP operations to obtain global\nWe leverage the triplet loss to align attribute-wise repre-\nrepresentations Gt = {G |i = 1,2,··· ,N}, focusing on\ni sentations with identity at the current step, assisting in global\nwhole body information. We force multiple global reprsen-\nrepresentations to distinguish similar identities.\ntatios Gt at the current step to learn more discriminative\ninformation by orthogonal loss to minimize the overlapping Ll\nTri\n=max(d p−d n+m,0) (7)\nelements. The orthogonal loss can be formulated as:\nwhere, dl and dl are the distances from positive samples\np n\nN−1 N and negative samples to anchor samples in attribute-wise\n(cid:88) (cid:88)\nL Ort = (Gt i,Gt j) (2) representations, respectively. In this paper, attribute-wise rep-\ni=1 j=i+1 resentations that contain specific information of individuals\nThen, we utilize the cross-entropy loss L and triplet loss assist global representations in distinguishing similar identi-\nCE\nLg [6] to optimize our DCR at the current task. ties for maximizing intra-domain discrimination. Meanwhile,\nTri\nattribute-wise representations as a bridge across increasing\nK\n1 (cid:88) datasets to minimize inter-domain gaps for better knowledge\nL = y log((ϕt(Gt)) ) (3)\nCE K i i transfer.\ni=1\n5\nTABLEI\nPERFORMANCECOMPARISONWITHSTATE-OF-THE-ARTMETHODSONTRAININGORDER-2.BASELINEMODELINCLUDESCLIPMODELAND\nATTRIBUTE-TEXTGENERATOR.BOLDANDREDFONTSAREOPTIMALANDSUBOPTIMALVALUES,RESPECTIVELY.TRAININGORDER-1IS\nMARKET→CUHK-SYSU→DUKEMTMC→MSMT17 V2→CUHK03.\nMarket1501 CUHK-SYSU DukeMTMC MSMT17 V2 CUHK03 Seen-Avg Unseen-Avg\nMethod\nmAP R-1 mAP R-1 mAP R-1 mAP R-1 mAP R-1 mAP R-1 mAP R-1\nAKA[17] 58.1 77.4 72.5 74.8 28.7 45.2 6.1 16.2 38.7 40.4 40.8 50.8 42.0 39.8\nPTKP[1] 64.4 82.8 79.8 81.9 45.6 63.4 10.4 25.9 42.5 42.9 48.5 59.4 51.2 49.1\nPatchKD[14] 68.5 85.7 75.6 78.6 33.8 50.4 6.5 17.0 34.1 36.8 43.7 53.7 45.1 43.3\nKRKC[2] 54.0 77.7 83.4 85.4 48.9 65.5 14.1 33.7 49.9 50.4 50.1 62.5 52.7 50.8\nConRFL[9] 59.2 78.3 82.1 84.3 45.6 61.8 12.6 30.4 51.7 53.8 50.2 61.7 - -\nCODA[50] 53.6 76.9 75.7 78.1 48.6 59.5 13.2 31.3 47.2 48.6 47.7 58.9 44.5 42.4\nLSTKC[13] 54.7 76.0 81.1 83.4 49.4 66.2 20.0 43.2 44.7 46.5 50.0 63.1 51.3 48.9\nC2R[16] 69.0 86.8 76.7 79.5 33.2 48.6 6.6 17.4 35.6 36.2 44.2 53.7 - -\nDKP[15] 60.3 80.6 83.6 85.4 51.6 68.4 19.7 41.8 43.6 44.2 51.8 64.1 49.9 46.4\nBaseline 61.6 79.1 80.2 80.6 50.2 64.3 15.1 36.5 44.9 46.8 50.4 61.5 51.8 49.4\nOurs 75.9 87.9 87.3 88.5 60.1 71.9 25.3 50.1 60.5 61.3 61.8 71.9 60.8 58.3\nTABLEII\nPERFORMANCECOMPARISONWITHSTATE-OF-THE-ARTMETHODSONTRAININGORDER-2.BASELINEMODELINCLUDESCLIPMODELAND\nATTRIBUTE-TEXTGENERATOR.BOLDANDREDFONTSAREOPTIMALANDSUBOPTIMALVALUES,RESPECTIVELY.TRAININGORDER-2IS\nDUKEMTMC→MSMT17 V2→MARKET→CUHK-SYSU→CUHK03.\nDukeMTMC MSMT17 V2 Market1501 CUHK-SYSU CUHK03 Seen-Avg Unseen-Avg\nMethod\nmAP R-1 mAP R-1 mAP R-1 mAP R-1 mAP R-1 mAP R-1 mAP R-1\nAKA[17] 42.2 60.1 5.4 15.1 37.2 59.8 71.2 73.9 36.9 37.9 38.6 49.4 41.3 39.0\nPTKP[1] 54.8 70.2 10.3 23.3 59.4 79.6 80.9 82.8 41.6 42.9 49.4 59.8 50.8 48.2\nPatchKD[14] 58.3 74.1 6.4 17.4 43.2 67.4 74.5 76.9 33.7 34.8 43.2 54.1 44.8 43.3\nKRKC[2] 50.6 65.6 13.6 27.4 56.2 77.4 83.5 85.9 46.7 46.6 50.1 61.0 52.1 47.7\nConRFL[9] 34.4 51.3 7.6 20.1 61.6 80.4 82.8 85.1 49.0 50.1 47.1 57.4 - -\nCODA[50] 38.7 56.6 11.6 24.5 54.3 75.1 76.2 75.8 42.3 41.7 44.6 54.7 45.0 42.9\nLSTKC[13] 49.9 67.6 14.6 34.0 55.1 76.7 82.3 83.8 46.3 48.1 49.6 62.1 51.7 49.5\nC2R[16] 59.7 75.0 7.3 19.2 42.4 66.5 76.0 77.8 37.8 39.3 44.7 55.6 - -\nDKP[15] 53.4 70.5 14.5 33.3 60.6 81.0 83.0 84.9 45.0 46.1 51.3 63.2 51.3 47.8\nBaseline 53.8 69.1 14.1 29.8 59.8 80.4 78.4 78.5 45.3 44.9 50.3 60.5 52.2 49.9\nOurs 64.1 77.2 25.4 44.9 70.6 84.5 86.1 88.2 54.2 58.7 60.1 70.7 61.6 59.2\nE. Attribute-oriented Anti-Forgetting fore, achieving a balance between them is crucial for improv-\nWe develop an attribute-oriented anti-forgetting (AF) strat- ing the performance of LReID models. Thus,we propose a\negy to explore attribute-wise representations for aligning the knowledge consolidation (KC) strategy that leverages global\ndistributions of the old and new models, as shown in Figure 2 representations for knowledge transfer between the old and\n(AF). Specifically, the new model adapts to new information new models. This includes alignment loss and logit-level\nbutmayforgetoldknowledgefromthepreviousdataset,while distillation loss.\nthe old model retains old knowledge. To better preserve old Maintaining distribution consistency between the old and\nknowledge, we leverage attribute-wise representations as a newmodelsforpreviousdatasetscanlimitthemodel’sability\nbridge to optimize both the old and new models using sam- to learn new knowledge. Therefore, we propose an alignment\nples from the memory buffer. This strategy achieves domain loss to explore global representations for knowledge transfer\nconsistency and minimize inter-domain gaps, alleviating the from the current dataset, as follows:\nforgetting of old knowledge, and is calculated as follows:\nB\n1 (cid:88)\n1 (cid:88)B L AL = B KL(Gt−1/τ||Gt/τ) (9)\nL AF = B KL(AGt N−1/τ||AGt N/τ) (8) i=1\ni=1 Wefurtherintroducealogit-leveldistillationlosstoenhance\nwhere KL(.||.) refers to a kullback-leibler divergence, and τ the extraction of identity information shared between the old\nis a hyperparameter called temperature [37]. and new models, further improving the model’s knowledge\nconsolidation ability. This is represented as follows:\nF. Knowledge Consolidation\nB\nMaximizing intra-domain discrimination and minimizing 1 (cid:88)\nL = KL((ϕt−1(Gt−1)) /τ||(ϕt(Gt)) /τ) (10)\ninter-domain gaps are in a contradictory relationship. There- LD B i i\ni=1\n6\nFig.3. t-SNEvisualizationoffeaturedistributiononfiveseendatasets.Ourmethodbetternarrowsthedistributionacrossdatasetsforminimizinginter-domain\ngaps,improvingtheanti-forgettingandgeneralizationabilityofthemodel.\nThe knowledge consolidation loss is defined as:\nL =L +L (11)\nKC AL LD\nThe total loss function is formulated as:\nL=L +Lg +Ll +L +L +L (12)\nCE Tri Tri Ort AF KC\nIII. EXPERIMENTS\nA. Experiments Setting\nDatasets. To verify the performance of our method in Fig. 4. Visualization of intra-domain discrimination on the Market1501\ndataset.Werandomlyselect30identities.Colorsrepresentdifferentidentity\nanti-forgetting and generalization, we evaluate our method\ninformation. Our DCR model can cluster images of the same identity more\non a challenging benchmark consisting of Market1501 [38], tightly(circle)forminimizinginter-domiandiscrimination.\nCUHK-SYSU[39],DukeMTMC[40],MSMT17 V2[41]and\nCUHK03[42],referredtoasseendatasets.Tworepresentative\ntraining orders are set up following the protocol described in 9.8% mAP/7.5% R-1 on training order-1 and order-2, respec-\n[17] for training and testing. Further, we employ six datasets tively. Meanwhile, our DCR effectively alleviate catastrophic\nincluding VIPeR [43], GRID [44], CUHK02 [45], Occ Duke forgetting, achieving 6.9% mAp/1.1% R-1, and 5.4% mAP/\n[46], Occ REID [47], and PRID2011 [48], as unseen dataset. 2.2% R-1 improvement on the first dataset (Mrket1501 and\nImplementationDetails.Ourtextencoderandimageencoder DukeMTMC) with different training orders. Compared to\nare based on a pre-trained CLIP model, while the attribute CODA, our DCR significantly outpreform performance under\ndecoder utilizes a transformer-based architecture[36]. All per- the backbone of VIT-B/16. Additionally, our DCR improves\nson images are resized to 256×128. We use Adam [49] for the average by 8.1 mAP%/7.5% R-1 and 9.5% mAP/11.0%\noptimizationandtraineachtaskfor60epochs.Thebatchsize R-1onunseendatasets.Incontrast,ourDCRachievesatrade-\nis set to 128. The learning rate is initialized at 5×10−6 and is off between anti-forgetting and acquiring new information,\ndecreasedbyafactorof0.1every20epochsforeachtask.We significantly enhances generalization capabilities.\nemploy mean average precision (mAP) and Rank-1 accuracy Compared with Baseline. Due to the lack of CLIP-based\n(R-1) to evaluate the LReID model on each dataset. comparisonmethodsinLReID,weintroduceaBaselinemodel\nincludingCLIPmodel,attribute-textgeneratorandknowledge\nconsolidation strategy. Compared to the Baseline, Our DCR\nimproves the Seen-Avg by 11.4% mAP/10.4% R-1 and by\nB. Comparison with SOTA Methods\n9.8% mAP/10.2% R-1. These results demonstrate that our\nWe compare the proposed DCR with SOTA LReID proposed attribute-wise representations learning achieves sig-\nto demonstrate the superiority of our method, includ- nificant performance in balancing maximization intra-domain\ning AKA[17], PTKP[1], PatchKD[14], KRKC[2], and discrimination and minimization inter-domain gaps in LReID.\nConRFL[9], DKP[15], C2R[16], LSTKC[13]. Experimental The effectiveness of minimizing inter-domain gaps. We\nresults on training order-1 and order-2 are shown in Table visualize the feature distribution of PTKP, KRKC, DKP, and\nIandTableII,respectively.CODA[50]methodemploysViT- our method across five datasets, as shown in Figure 3. DKP\nB/16 as the backbone. shows poor performance in bridging inter-domain gaps, as\nCompared with LReID methods. In Table I and Table II, knowledge prototypes struggle to fit the data distribution.\nOur DCR significantly outperforms LReID methods, with Compared to other methods, our DCR model better narrows\nan seen-avg incremental gain of 10.0% mAP/7.8% R-1, and the distribution across increasing datasets. Thus, the proposed\n7\nTABLEIV\nABLATIONSTUDIESOFDIFFERENTCOMPONENTSONTRAININGORDER-1.\nSeen Avg Unseen Avg\nPFM ACN AF KC\nmAP R-1 mAP R-1\n50.4 61.5 51.8 49.4\n√\n51.7 62.1 52.5 50.3\n√ √\n57.6 68.9 58.2 56.2\n√ √ √\n58.7 69.2 58.5 56.8\n√ √ √ √\n61.8 71.9 60.8 58.3\nFig. 5. Generalization curves. After each training step, the performance of TABLEV\nallunseendomainsisevaluated. ABLATIONOFTRAININGWITHORWITHOUTATTRIBUTE-TEXT\nGENERATOR(ATG)ONTRAININGORDER-1.\nTABLEIII\nSeen Avg Unseen Avg\nABLATIONSTUDIESONTHENUMBEROFGLOBALANDATTRIBUTE-WISE Method\nREPRESENTATIONSN ONTRAININGORDER-1. mAP R-1 mAP R-1\nTrainingw/oATG 60.1 70.5 59.3 56.5\nSeen Avg Unseen Avg Trainingw/ATG 61.8 71.9 60.8 58.3\nNumber(N)\nmAP R-1 mAP R-1\n2 60.2 68.7 59.4 56.5\n3 61.8 71.9 60.8 58.3\n4 61.2 71.6 60.3 57.5\nrepresentations,asshowninTableIII.Weobservethatsetting\nthe number of global and attribute-wise representations N to\nDCR model can effectively bridge domain gaps, improving 3 achieves the best performance for our method.\nknowledge transfer capabilities, benefiting from the attribute-\noriented anti-forgetting (AF) strategy based on attribute-wise Performance of Different Components. To assess the con-\nrepresentations. tributionofeachcomponenttoourDCR,weconductablation\nThe effectiveness of maximizing intra-domain discrimina- studies on seen and unseen datasets, as shown in Table IV.\ntion. We visualize the feature distribution of KRKC and our Comparing the first and second rows, we observe that the\nmethod.Figure4showsthatourDCRmodelcansignificantly parallelfusionmodule(PFM),whichemploysaparallelcross-\ncluster images of the same identity more tightly (circle) attention mechanism, effectively fuses text and image embed-\nand increase the distance between different identities (black dings. Comparing the second and fourth rows, we consider\nbidirectional arrow). Compared to KRKC, our DCR model that the attribute compensation network (ACN) and attribute-\neffectively improves intra-domain discrimination, benefiting oriented anti-forgetting (AF) strategy effectively learn domain\nfrom the complementary relationship between global and consistency, improving generalization ability. In the second\nattribute representations, which allows it to learn the subtle and third rows, we notice the performance decrease when\nnuances of individuals. using only the knowledge consolidation (KC) strategy based\nGeneralization Curves on Unseen Dataset. We analyze the onglobalrepresentationsacrossincreasingdatawhileignoring\naverage performance on unseen datasets over the training inter-domain gaps. The results demonstrate that both global\nsteps, as shown in Figure 5. Compared to other methods, representations and attribute-wise representations achieve a\nour DCR model achieves superior performance and exhibits trade-off between maximizing intra-domain discrimination\nfaster performance growth across the training steps. Thus, and minimizing inter-domain gaps for enhancing the anti-\nour attribute-oriented anti-forgetting (AF) strategy effectively forgetting and generalization capacity of our DCR.\nbridgesinter-domaingaps,enhancingthegeneralizationability Performance of attribute-text generator. To better un-\nofourmodel.Insummary,ourDCRmodelexploresglobaland derstand whether each instance’s text descriptions generated\nattribute-wise representations to achieve a trade-off between by the attribute-text generator (ATG) provide more fine-\nmaximizingintra-domaindiscriminationandminimizinginter- grained guidance for learning global representations, we train\ndomain gaps. our model using the generic text descriptor ”A photo of a\nperson” (w/o ATG) for comparison. Table V shows that the\nattribute-textgeneratorobtaintextdescriptionstosignificantly\nimproves overall performance. When using the specific text\nC. Ablation Studies\ndescriptors, the average decreases by 1.7% mAP/1.4% R-1 on\nThenumberofglobalandattribute-wiserepreentations. seendatasetsandby1.5%mAP/1.8%R-1onunseendatasets.\nGlobal and attribute-wise representations capture individual ATG enhances the robustness of global representations for\nnuances in intra-domain and inter-domain consistency. We each instance, effectively mitigating the forgetting of old\nstudy the suitability of multiple global and attribute-wise knowledge.\n8\nIV. CONCLUSION plasticity for lifelong person re-identification in cloth-\nchanging and cloth-consistent scenarios. IEEE Trans-\nIn this paper, we propose a domain consistency repre-\nactions on Circuits and Systems for Video Technology,\nsentation learning (DCR) model that explores global and\n2024.\nattribute-wise representations to capture subtle nuances in\n[11] Guile Wu and Shaogang Gong. Generalising without\nintra-domain and inter-domain consistency, achieving a trade-\nforgetting for lifelong person re-identification. In AAAI,\noffbetweenmaximizingintra-domaindiscriminationandmin-\nvolume 35, pages 2889–2897, 2021.\nimizing inter-domain gaps. Specifically, global and attribute-\n[12] LeiZhang,GuanyuGao,andHuaizhengZhang. Spatial-\nwise representations serve as complementary information to\ntemporal federated learning for lifelong person re-\ndistinguish similar identities in intra-domain. We further de-\nidentification on distributed edges. IEEE Transactions\nvelop an attribute-oriented anti-forgetting (AF) strategy and\non Circuits and Systems for Video Technology, 2023.\na knowledge consolidation (KC) strategy to minimize inter-\n[13] Kunlun Xu, Xu Zou, and Jiahuan Zhou. Lstkc: Long\ndomain gaps and facilitate knowledge transfer, enhancing\nshort-term knowledge consolidation for lifelong person\ngeneralizationcapabilities.Extensiveexperimentsdemonstrate\nre-identification. In AAAI, volume 38, pages 16202–\nthat our method achieves superior performance compared to\n16210, 2024.\nstate-of-the-art LReID methods.\n[14] Zhicheng Sun and Yadong Mu. Patch-based knowledge\ndistillation for lifelong person re-identification. In ACM\nREFERENCES\nMM, pages 696–707, 2022.\n[1] Wenhang Ge, Junlong Du, Ancong Wu, Yuqiao Xian, [15] Kunlun Xu, Xu Zou, Yuxin Peng, and Jiahuan Zhou.\nKe Yan, Feiyue Huang, and Wei-Shi Zheng. Lifelong Distribution-aware knowledge prototyping for non-\npersonre-identificationbypseudotaskknowledgepreser- exemplar lifelong person re-identification. In CVPR,\nvation. In AAAI, volume 36, pages 688–696, 2022. pages 16604–16613, 2024.\n[2] ChunlinYu,YeShi,ZimoLiu,ShenghuaGao,andJingya [16] Zhenyu Cui, Jiahuan Zhou, Xun Wang, Manyu Zhu, and\nWang. Lifelong person re-identification via knowledge Yuxin Peng. Learning continual compatible representa-\nrefreshingandconsolidation. InAAAI,volume37,pages tionforre-indexingfreelifelongpersonre-identification.\n3295–3303, 2023. In CVPR, pages 16614–16623, 2024.\n[3] Anguo Zhang, Yueming Gao, Yuzhen Niu, Wenxi [17] Nan Pu, Wei Chen, Yu Liu, Erwin M Bakker, and\nLiu, and Yongcheng Zhou. Coarse-to-fine person re- Michael S Lew. Lifelong person re-identification via\nidentification with auxiliary-domain classification and adaptive knowledge accumulation. In CVPR, pages\nsecond-order information bottleneck. In CVPR, pages 7901–7910, 2021.\n598–607, 2021. [18] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian\n[4] Huijie Fan, Xiaotong Wang, Qiang Wang, Shengpeng Lu. Vision-language models for vision tasks: A survey.\nFu, and Yandong Tang. Skip connection aggregation IEEE Transactions on Pattern Analysis and Machine\ntransformer for occluded person reidentification. IEEE Intelligence, 2024.\nTransactions on Industrial Informatics, 20(1):442–451, [19] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and\n2023. Ziwei Liu. Learning to prompt for vision-language\n[5] Siyuan Li, Li Sun, and Qingli Li. Clip-reid: exploiting models. International Journal of Computer Vision,\nvision-language model for image re-identification with- 130(9):2337–2348, 2022.\nout concrete text labels. In AAAI, volume 37, pages [20] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and\n1405–1413, 2023. Ziwei Liu. Conditional prompt learning for vision-\n[6] Zexian Yang, Dayan Wu, Chenming Wu, Zheng Lin, language models. In CVPR, pages 16816–16825, 2022.\nJingzi Gu, and Weiping Wang. A pedestrian is worth [21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\none prompt: Towards language guidance person re- Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nidentification. In CVPR, pages 17343–17353, 2024. Amanda Askell, Pamela Mishkin, Jack Clark, et al.\n[7] Tao Wang, Hong Liu, Pinhao Song, Tianyu Guo, and Learning transferable visual models from natural lan-\nWei Shi. Pose-guided feature disentangling for occluded guage supervision. In International conference on ma-\nperson re-identification based on transformer. In AAAI, chine learning, pages 8748–8763. PMLR, 2021.\nvolume 36, pages 2540–2549, 2022. [22] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang,\n[8] Nan Pu, Yu Liu, Wei Chen, Erwin M Bakker, and MingYan,BinBi,JiaboYe,HeChen,GuohaiXu,Zheng\nMichael S Lew. Meta reconciliation normalization for Cao,etal. mplug:Effectiveandefficientvision-language\nlifelong person re-identification. In ACM MM, pages learning by cross-modal skip-connections. In Empirical\n541–549, 2022. Methods in Natural Language Processing, pages 7241–\n[9] Jinze Huang, Xiaohan Yu, Dong An, Yaoguang Wei, 7259, 2022.\nXiao Bai, Jin Zheng, Chen Wang, and Jun Zhou. Learn- [23] Chenyang Yu, Xuehu Liu, Yingquan Wang, Pingping\ning consistent region features for lifelong person re- Zhang, and Huchuan Lu. Tf-clip: Learning text-free\nidentification. Pattern Recognition, 144:109837, 2023. clip for video-based person re-identification. In AAAI,\n[10] Yuming Yan, Huimin Yu, Yubin Wang, Shuyi Song, volume 38, pages 6764–6772, 2024.\nWeihu Huang, and Juncan Jin. Unified stability and [24] ZhiyinShao,XinyuZhang,ChangxingDing,JianWang,\n9\nand Jingdong Wang. Unified pre-training with pseudo 2015.\ntexts for text-to-image person re-identification. In ICCV, [39] Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and\npages 11174–11184, 2023. Xiaogang Wang. End-to-end deep learning for person\n[25] Yang Qin, Yingke Chen, Dezhong Peng, Xi Peng, search. arXiv preprint arXiv:1604.01850, 2(2):4, 2016.\nJoey Tianyi Zhou, and Peng Hu. Noisy-correspondence [40] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cuc-\nlearning for text-to-image person re-identification. In chiara, and Carlo Tomasi. Performance measures and\nCVPR, pages 27197–27206, 2024. a data set for multi-target, multi-camera tracking. In\n[26] Xinyi Wu, Wentao Ma, Dan Guo, Tongqing Zhou, Shan ECCV, pages 17–35. Springer, 2016.\nZhao, and Zhiping Cai. Text-based occluded person [41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.\nre-identification via multi-granularity contrastive consis- Person transfer gan to bridge domain gap for person re-\ntency learning. In AAAI, volume 38, pages 6162–6170, identification. In CVPR, pages 79–88, 2018.\n2024. [42] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang.\n[27] Yunhao Du, Zhicheng Zhao, and Fei Su. Yyds: Visible- Deepreid: Deep filter pairing neural network for person\ninfraredpersonre-identificationwithcoarsedescriptions. re-identification. In CVPR, pages 152–159, 2014.\narXiv preprint arXiv:2403.04183, 2024. [43] Douglas Gray and Hai Tao. Viewpoint invariant pedes-\n[28] Jian Jia, Houjing Huang, Xiaotang Chen, and Kaiqi trian recognition with an ensemble of localized features.\nHuang. Rethinking of pedestrian attribute recognition: In ECCV, pages 262–275. Springer, 2008.\nA reliable evaluation under zero-shot pedestrian identity [44] Chen Change Loy, Tao Xiang, and Shaogang Gong.\nsetting. arXiv preprint arXiv:2107.03576, 2021. Time-delayed correlation analysis for multi-camera ac-\n[29] Haoyun Sun, Hongwei Zhao, Weishan Zhang, Liang tivity understanding. International Journal of Computer\nXu, and Hongqing Guan. Adaptive multi-task learning Vision, 90:106–129, 2010.\nfor multi-par in real-world. IEEE Journal of Radio [45] Wei Li and Xiaogang Wang. Locally aligned feature\nFrequency Identification, 2024. transforms across views. In CVPR, pages 3594–3601,\n[30] YunfeiZhouandXiangruiZeng. Towardscomprehensive 2013.\nunderstanding of pedestrians for autonomous driving: [46] JiaxuMiao,YuWu,PingLiu,YuhangDing,andYiYang.\nEfficient multi-task-learning-based pedestrian detection, Pose-guided feature alignment for occluded person re-\ntracking and attribute recognition. Robotics and Au- identification. In ICCV, pages 542–551, 2019.\ntonomous Systems, 171:104580, 2024. [47] Jiaxuan Zhuo, Zeyu Chen, Jianhuang Lai, and Guang-\n[31] Xinwen Fan, Yukang Zhang, Yang Lu, and Hanzi Wang. congWang. Occludedpersonre-identification. InICME,\nParformer: Transformer-based multi-task network for pages 1–6. IEEE, 2018.\npedestrian attribute recognition. IEEE Transactions on [48] Martin Hirzer, Csaba Beleznai, Peter M Roth, and Horst\nCircuits and Systems for Video Technology, 34(1):411– Bischof. Person re-identification by descriptive and\n423, 2023. discriminative classification. In Image Analysis: 17th\n[32] Jian Jia, Naiyu Gao, Fei He, Xiaotang Chen, and Kaiqi ScandinavianConference,pages91–102.Springer,2011.\nHuang. Learning disentangled attribute representations [49] DiederikPKingmaandJimmyBa. Adam:Amethodfor\nfor robust pedestrian attribute recognition. In AAAI, stochasticoptimization. arXivpreprintarXiv:1412.6980,\nvolume 36, pages 1069–1077, 2022. 2014.\n[33] Jian Jia, Xiaotang Chen, and Kaiqi Huang. Spatial [50] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta,\nand semantic consistency regularizations for pedestrian Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle,\nattribute recognition. In ICCV, pages 962–971, 2021. Rameswar Panda, Rogerio Feris, and Zsolt Kira. Coda-\n[34] Xiaoyan Yu, Neng Dong, Liehuang Zhu, Hao Peng, and prompt: Continual decomposed attention-based prompt-\nDapengTao. Clip-drivensemanticdiscoverynetworkfor ingforrehearsal-freecontinuallearning. InCVPR,pages\nvisible-infrared person re-identification. arXiv preprint 11909–11919, 2023.\narXiv:2401.05806, 2024.\n[35] Xihui Liu, Haiyu Zhao, Maoqing Tian, Lu Sheng,\nJing Shao, Shuai Yi, Junjie Yan, and Xiaogang Wang.\nHydraplus-net: Attentive deep features for pedestrian\nanalysis. In ICCV, pages 350–359, 2017.\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nandIlliaPolosukhin. Attentionisallyouneed. Advances\nin neural information processing systems, 30, 2017.\n[37] GeoffreyHinton,OriolVinyals,andJeffDean. Distilling\nthe knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\n[38] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang,\nJingdong Wang, and Qi Tian. Scalable person re-\nidentification:Abenchmark. InICCV,pages1116–1124,",
    "pdf_filename": "Domain_Consistency_Representation_Learning_for_Lifelong_Person_Re-Identification.pdf"
}