{
    "title": "Balancing Accuracy and Efficiency in Multi-Turn Intent Classification for LLM-Powered Dialog Systems",
    "context": "Accurate multi-turn intent classification is essential for advancing conversational AI systems. However, challenges such as the scarcity of comprehensive datasets and the complexity of contextual depen- dencies across dialogue turns hinder progress. This paper presents two novel approaches leveraging Large Language Models (LLMs) to enhance scalability and reduce latency in production dialogue systems. First, we introduce Symbol Tuning, which simplifies intent labels to reduce task complexity and improve performance in multi- turn dialogues. Second, we propose C-LARA (Consistency-aware, Linguistics Adaptive Retrieval Augmentation), a framework that employs LLMs for data augmentation and pseudo-labeling to gener- ate synthetic multi-turn dialogues. These enriched datasets are used to fine-tune a small, efficient model suitable for deployment. Exper- iments conducted on multilingual dialogue datasets demonstrate significant improvements in classification accuracy and resource efficiency. Our methods enhance multi-turn intent classification accuracy by 5.09%, reduce annotation costs by 40%, and enable scal- able deployment in low-resource multilingual industrial systems, highlighting their practicality and impact. CCS Concepts • Information systems →Language models; Question answer- ing; • Computing methodologies →Intelligent agents. Keywords Multi-turn Intent Classification, Multilingual Large Language Model, Retrieval Augmentation, Computational Linguistics, Language Di- versity, Knowledge Engineering ACM Reference Format: Junhua Liu1,3,∗, Yong Keat Tan2,∗, Bin Fu2,†, Kwan Hui Lim3 . 2024. Balanc- ing Accuracy and Efficiency in Multi-Turn Intent Classification for LLM- Powered Dialog Systems in Production. In Proceedings of Preprint. ACM, New York, NY, USA, 9 pages. https://doi.org/N.A Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Preprint, Working Paper, Nov 2024 © 2024 Copyright held by the owner/author(s). ACM ISBN 978-x-xxxx-xxxx-x/YY/MM https://doi.org/N.A Figure 1: Comparison of instruction tuning and symbol tun- ing. Simplifying verbose intent labels (e.g., “Request to Can- cel Order” →“Cancel Order”) reduces redundancy, enhanc- ing LLM classification performance by 5.09%, addressing key challenges in production intent classification. 1 Dialogue systems are critical for automating interactions between customers and agents, streamlining communication and enhanc- ing user experience. They play a pivotal role in international e- commerce platforms, addressing the increasing demand for in- stantaneous and efficient customer service. Intent classification, a fundamental aspect of natural language understanding in dia- logue systems, involves identifying users’ goals from their inputs, thereby minimizing waiting times and operational costs [22]. User interactions frequently evolve into multi-turn dialogues when de- tailed information is required, complicating the development of multi-turn intent classification (MTIC) models, despite their simi- larity to standard text classification tasks. Additionally, real-world multilingual systems require scalable solutions that uphold inclu- sivity and ethical standards, particularly in low-resource settings. This complexity arises from the need to consider contextual fac- tors like historical utterances and prior intents. Without a proper understanding of session context, the system risks misinterpreting user intentions, which may result in incorrect applications or irrel- evant responses [24]. Consequently, MTIC within dialogue system presents significant challenges. The first challenge is that the length of intents in industrial di- alogue systems is longer compared to general text classification tasks. Figure 1 shows that the real intents comprise several words in our knowledge base because operators(Ops) typically assign intents *Equal Contributions. †Corresponding Author: bin.fu@shopee.com arXiv:2411.12307v1  [cs.CL]  19 Nov 2024",
    "body": "Balancing Accuracy and Efficiency in Multi-Turn Intent\nClassification for LLM-Powered Dialog Systems in Production\nJunhua Liu1,3,∗, Yong Keat Tan2,∗, Bin Fu2,†, Kwan Hui Lim3\n1Forth AI\n2Shopee\n3Singapore University of Technology and Design\nSingapore\nAbstract\nAccurate multi-turn intent classification is essential for advancing\nconversational AI systems. However, challenges such as the scarcity\nof comprehensive datasets and the complexity of contextual depen-\ndencies across dialogue turns hinder progress. This paper presents\ntwo novel approaches leveraging Large Language Models (LLMs)\nto enhance scalability and reduce latency in production dialogue\nsystems. First, we introduce Symbol Tuning, which simplifies intent\nlabels to reduce task complexity and improve performance in multi-\nturn dialogues. Second, we propose C-LARA (Consistency-aware,\nLinguistics Adaptive Retrieval Augmentation), a framework that\nemploys LLMs for data augmentation and pseudo-labeling to gener-\nate synthetic multi-turn dialogues. These enriched datasets are used\nto fine-tune a small, efficient model suitable for deployment. Exper-\niments conducted on multilingual dialogue datasets demonstrate\nsignificant improvements in classification accuracy and resource\nefficiency. Our methods enhance multi-turn intent classification\naccuracy by 5.09%, reduce annotation costs by 40%, and enable scal-\nable deployment in low-resource multilingual industrial systems,\nhighlighting their practicality and impact.\nCCS Concepts\n• Information systems →Language models; Question answer-\ning; • Computing methodologies →Intelligent agents.\nKeywords\nMulti-turn Intent Classification, Multilingual Large Language Model,\nRetrieval Augmentation, Computational Linguistics, Language Di-\nversity, Knowledge Engineering\nACM Reference Format:\nJunhua Liu1,3,∗, Yong Keat Tan2,∗, Bin Fu2,†, Kwan Hui Lim3 . 2024. Balanc-\ning Accuracy and Efficiency in Multi-Turn Intent Classification for LLM-\nPowered Dialog Systems in Production. In Proceedings of Preprint. ACM,\nNew York, NY, USA, 9 pages. https://doi.org/N.A\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nPreprint, Working Paper, Nov 2024\n© 2024 Copyright held by the owner/author(s).\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM\nhttps://doi.org/N.A\nFigure 1: Comparison of instruction tuning and symbol tun-\ning. Simplifying verbose intent labels (e.g., “Request to Can-\ncel Order” →“Cancel Order”) reduces redundancy, enhanc-\ning LLM classification performance by 5.09%, addressing key\nchallenges in production intent classification.\n1\nIntroduction\nDialogue systems are critical for automating interactions between\ncustomers and agents, streamlining communication and enhanc-\ning user experience. They play a pivotal role in international e-\ncommerce platforms, addressing the increasing demand for in-\nstantaneous and efficient customer service. Intent classification,\na fundamental aspect of natural language understanding in dia-\nlogue systems, involves identifying users’ goals from their inputs,\nthereby minimizing waiting times and operational costs [22]. User\ninteractions frequently evolve into multi-turn dialogues when de-\ntailed information is required, complicating the development of\nmulti-turn intent classification (MTIC) models, despite their simi-\nlarity to standard text classification tasks. Additionally, real-world\nmultilingual systems require scalable solutions that uphold inclu-\nsivity and ethical standards, particularly in low-resource settings.\nThis complexity arises from the need to consider contextual fac-\ntors like historical utterances and prior intents. Without a proper\nunderstanding of session context, the system risks misinterpreting\nuser intentions, which may result in incorrect applications or irrel-\nevant responses [24]. Consequently, MTIC within dialogue system\npresents significant challenges.\nThe first challenge is that the length of intents in industrial di-\nalogue systems is longer compared to general text classification\ntasks. Figure 1 shows that the real intents comprise several words in\nour knowledge base because operators(Ops) typically assign intents\n*Equal Contributions.\n†Corresponding Author: bin.fu@shopee.com\narXiv:2411.12307v1  [cs.CL]  19 Nov 2024\n\nPreprint, Working Paper, Nov 2024\nLiu et al.\nFigure 2: Annotation pipeline of multi-turn intent classifica-\ntion datasets. Two major challenges in production systems\nare illustrated: (1) managing numerous (500+) intents across\nmarkets with redundant labels, and (2) the high cost of col-\nlecting multi-turn training data.\na clear and descriptive name to facilitate knowledge management,\nwhich makes them redundant. The recent advancements in large\nlanguage model(LLMs) present new research opportunities to sim-\nplify and optimize the text classification process [19]. Research\nindicates that LLMs perform excellently in sentiment analysis[12],\nwhich only adopts shorter labels such as positive, negative. How-\never, LLMs still fail to address context dependency in multi-turn\nconversations and struggle with long intent labels common in in-\ndustrial systems.\nThe second challenge lies in the difficulty of collecting multi-turn\ndatasets. While several studies [15, 23] on MTIC exist, they often\nassume access to comprehensive multi-turn training data, which is\nrarely available in real-world applications.\nFigure 2 shows the annotation pipeline for MTIC tasks. Even if\nwe ignore the redundant info within intents, unlike dialogue act\nclassification [13] with only less than 10 classes within dialogue\nstate tracking (DST), there are hundreds of intents operated by local\nOps in knowledge base of dialgoue system to cover user’s various\nand specific intents in each market, which increase the complexity\nof multi-turn classification and its annotation. Annotators often\nstruggle with the numerous of intents, leading to increased mis-\ntakes and longer decision-making times. As a result, the annotation\nprocess becomes costly and time-intensive, making it impractical\nto manually annotate large-scale multi-turn datasets. However, in-\nsufficient training data can significantly hinder model performance\neven with LLM. These challenges underscore the necessity for more\nefficient methods to address data scarcity and classification com-\nplexity.\nTo tackle two challenges, we first study the feasibility of using\nLLM for supervised fine-tuning (SFT) to perform MTIC using a\ngenerative method. Various intents increase the complexity of this\ntask since the more tokens a large language model(LLM) generates,\nthe lower the task performance [16]. To conquer this, we compress\nthe redundant info within intent to succinct intent via GPT4, then\nadopt those intents in SFT named as symbol tuning, which help to\nreduce the difficulty of multi-turn classification tasks by the LLM\ngenerative method.\nSecondly, to overcome the shortage of multi-turn data, we pro-\npose a novel pseudo-labeling and data generation framework called\nConsistency-aware Linguistics Adaptive Retrieval Augmentation\n(C-LARA). Extending beyond existing synthetic data generation [9],\nC-LARA serves as an effective pseudo-labeling tool for generat-\ning multi-turn data from user’s unlabeled utterances with self-\nconsistency. C-LARA arranges the retrieval result in different orders\nto assemble adaptive prompts, which cover the diverse reasoning\npath and filter out noise in in-context learning to improve the\nquality of labeling data. Subsequently, we use the training data\nto train a smaller model for online inference. C-LARA is a novel\nframework tailored for multi-turn intent classification. It addresses\nlimitations in prior approaches by leveraging adaptive retrieval and\nself-consistency mechanisms to enhance the accuracy of pseudo-\nlabeling for multi-turn dialogues. Unlike previous methods, it di-\nrectly optimizes for zero-shot multi-turn data classification and\nscalable deployment.\nIn summary, the contributions of this paper are as follows:\n(1) We introduce symbol-tuning, leveraging compressed intents\nto enhance LLM performance for MTIC, demonstrating a\n5.09% improvement in supervised fine-tuning (SFT) results.\n(2) We develop C-LARA, a novel framework for generating high-\nquality multi-turn data, effectively augmenting MTIC results.\n(3) We fine-tune smaller models using data generated by C-\nLARA, enabling scalable and accurate deployment of MTIC\nsystems in low-resource industrial settings.\n2\nProblem Formulation\n2.1\nMulti-Turn Intent Classification\nMulti-Turn Intent Classification (MTIC) involves identifying the\nintent 𝐼of the final query 𝑞𝑛from a predefined set I, based on a\nsequence of user queries Q = {𝑞𝑖}𝑛\n𝑖=1 in a chatbot session. This\ntask relies on the conversational context C = {𝑞𝑖}𝑛−1\n𝑖=1 , which\nincludes prior queries. Context-dependency adds complexity, re-\nquiring models to interpret nuanced conversational dynamics and\nevolving user intentions. Each intent 𝐼has a local-language ti-\ntle 𝑦and a hierarchical English category 𝑧(e.g., Indonesia: 𝑦=\n’Cara membatalkan pesanan’,𝑧= ’Logistics > Order > Cancellation’).\n2.2\nSupervised Fine-tuning\nSupervised Fine-tuning (SFT) adapts pre-trained large language\nmodels (LLMs) for specific tasks using labeled datasets. This process\nachieves high benchmark accuracy through task-specific supervi-\nsion.\nProblem Definition. Given a dataset D = {(𝑥𝑖,𝑦𝑖)}𝑁\n𝑖=1, where 𝑥𝑖\nis an input query and 𝑦𝑖is the corresponding label, the objective\nis to optimize model parameters 𝜃to maximize the conditional\nlikelihood 𝑝(𝑦𝑖|𝑥𝑖;𝜃):\nLSFT(𝜃) = −1\n𝑁\n𝑁\n∑︁\n𝑖=1\nlog𝑝(𝑦𝑖|𝑥𝑖;𝜃).\n\nBalancing Accuracy and Efficiency in Multi-Turn Intent Classification for LLM-Powered Dialog Systems in Production\nPreprint, Working Paper, Nov 2024\nConditional Probability Modeling. For structured outputs, 𝑦𝑖is a\nsequence of tokens {𝑡1,𝑡2, . . . ,𝑡𝑚}, with probability factorized au-\ntoregressively:\n𝑝(𝑦|𝑥;𝜃) =\n𝑚\nY\n𝑗=1\n𝑝(𝑡𝑗|𝑡<𝑗,𝑥;𝜃).\nThe training objective becomes:\nLSFT(𝜃) = −1\n𝑁\n𝑁\n∑︁\n𝑖=1\n𝑚\n∑︁\n𝑗=1\nlog𝑝(𝑡𝑗|𝑡<𝑗,𝑥𝑖;𝜃).\n2.3\nSymbol Tuning\nUnlike methods replacing task labels with unrelated symbols [21],\nour Symbol Tuning approach focuses on intent classification. Ver-\nbose labels in industrial systems disperse semantic information,\nhindering model performance. To address this, we compress labels\ninto concise phrases using GPT-4. For example, \"Request to Can-\ncel Order\" becomes \"Cancel Order,\" serving as compact semantic\nanchors that enhance shallow and deep layer representations.\nMathematical Formulation. Let the original intent label be 𝐿=\n{𝑡1,𝑡2, . . . ,𝑡𝑚}. The compressed label 𝐿′, with 𝑛≪𝑚, is gener-\nated by optimizing:\n𝐿′ = argmin𝐿′ C(𝐿′) + E(𝐿′, 𝐿),\nwhere: - C(𝐿′): Compactness of 𝐿′ (e.g., token count). - E(𝐿′, 𝐿):\nSemantic divergence, computed as:\nE(𝐿′, 𝐿) = 1 −cosine_sim(𝜙(𝐿′),𝜙(𝐿)),\nwith 𝜙(·) as an embedding function.\nObjective Function. Given D = {(𝑥𝑖, 𝐿𝑖)}𝑁\n𝑖=1, where 𝐿𝑖is the original\nlabel, the supervised fine-tuning loss becomes:\nLST(𝜃) = −E(𝑥,𝐿′)∼D\n𝑛\n∑︁\n𝑗=1\nlog𝑝(𝑡𝑗|𝑡<𝑗,𝑥;𝜃),\nwhere 𝑡<𝑗denotes preceding tokens in 𝐿′.\nPerformance Implications. Replacing verbose labels 𝐿with compact\n𝐿′ reduces token processing and improves classification accuracy,\nstreamlining intent recognition tasks.\n3\nSolutions\n3.1\nSymbol Tuning on LLM\nTo address intent classification tasks, we utilize generative models\nrather than conventional discriminative or regressive approaches.\nOur Symbol Tuning (ST) method involves supervised fine-tuning\n(SFT) of an LLM with compressed intent labels. Given a complete\nchat session S = {𝑞1, 𝐼1, ...,𝑞𝑛−1, 𝐼𝑛−1,𝑞𝑛}, the model is trained to\ngenerate the representative question𝑟𝑛corresponding to the correct\nintent 𝐼𝑛of the final query 𝑞𝑛. Queries and intents are structured\nin a natural question-answering flow, as illustrated below:\nSYSTEM: \"A chat between a curious user and an ar-\ntificial intelligence assistant. The assistant provides\nhelpful, detailed, and polite responses to the user’s\nquestions.\"\nUSER: \"{q_1}\"\nASSISTANT: \"The intent title is {r_1}.\"\n...\nUSER: \"{q_n}\"\nASSISTANT: \"The intent title is {r_n}.\"\nThe generated𝑟𝑛is compared with intents in I using cosine similar-\nity in the embedding space to ensure semantic alignment between\nthe model’s output and predefined intent titles.\nCompressed Generation. Intent representative queries 𝑟often con-\nsist of approximately 12 tokens, making them inefficient as genera-\ntion targets. To address this, we employ an LLM to compress 𝑟into\nconcise phrases, typically two words, while preserving their seman-\ntic essence. This process ensures that each compressed intent label\n𝑟𝑐is unique. If duplicates occur, the model iteratively increases the\nword count until uniqueness is achieved. This compression reduces\nthe average length of 𝑟𝑐to four tokens, optimizing it for genera-\ntion tasks and improving classification accuracy. This approach\nenhances classification accuracy by reducing semantic dispersion\nin labels, ensuring more focused information propagation through\nLLM layers.\nCross-Lingual Labels. In non-English markets, intent labels 𝑟are\ncompressed into English while retaining the original language for\ninput queries Q. Leveraging English, the predominant language in\nLLM pretraining corpora, simplifies label generation and enhances\nmodel performance in multilingual settings. This cross-lingual strat-\negy reduces complexity and improves alignment with pretraining\ndistributions. This strategy leverages the strengths of pre-trained\nLLMs while accommodating multilingual data, offering a scalable\nsolution for cross-lingual intent classification.\n3.2\nConsistency-aware Linguistics Adaptive\nRetrieval Augmentation\nTo enhance in-context learning, we propose the Consistency-aware,\nLinguistics Adaptive Retrieval Augmentation (C-LARA) framework.\nBuilding upon the LARA model [9], C-LARA incorporates a fine-\ntuned single-turn model M𝑐within a retrieval-augmented pipeline.\nThis framework enables zero-shot Multi-Turn Intent Classification\n(MTIC) using only single-turn demonstrations. Unlike LARA, which\nis computationally intensive in real-time, C-LARA operates offline\nas a pseudo-labeling tool, generating high-quality multi-turn data\nfor training lightweight classification models.\nSpecifically, the LARA pipeline can be complex and resource-\nintensive to implement for real-time systems. Hence, we use this\nmethod offline as a multi-turn data pseudo-labeling tool to train a\nsmaller classification model. The training method mirrors that of\nthe single-turn classifier M𝑐following the paper, adding pseudo-\nlabeled multi-turn data to the original data comprising only single-\nturn samples.\nSince this is not a real-time task, the pipeline response time is not\na critical consideration, hence self-consistency checking was per-\nformed on the LLM outputs to ensure the quality of pseudo-labels.\nFor this check, as shown in Figure 3, the in-context learning phase\nis run three times per sample, with the in-context demonstrations\nsorted in three orders according to their similarity scores to the\nsession queries: ascending, descending, and random. This approach\nto self-consistency checking method can also be implemented when\n\nPreprint, Working Paper, Nov 2024\nLiu et al.\nFigure 3: Illustration of C-LARA: merging LARA with Self-Consistency effectively combines query aggregation, knowledge base\nretrieval, and self-consistency mechanism to generate high-quality pseudo-labels for multi-turn dialogues. The self-consistency\nprocess improves labeling accuracy by validating intent predictions across different prompt orderings.\nusing a black-box LLM. Online chat logs are sampled for pseudo-\nlabeling, and only those having consistent labels for all 3 runs will\nbe kept for training.\n3.2.1\nHierarchical Text Classification(HTC). M𝑐is an ensemble of\nlabel-attention encoder and a hierarchical-aware tree-based encoder\nwith 3-layered global and local intent classifiers.\nThe label-attention encoder has one classifier head for each\nintent layer. Each classifier head has one hidden linear layer to\nobtain the layer intermediate output 𝐿𝑙, which encodes the layer\ninformation. This layer information will be utilised in the input of\nthe next layer classifier head.\n𝐿𝑙=\n(\n𝐻𝑊1\n𝑙+ 𝑏1\n𝑙,\nif 𝑙= 1,\n(𝐻⊕𝐿𝑙−1)𝑊1\n𝑙+ 𝑏1\n𝑙,\nif 𝑙> 1,\nwhere 𝑊1\n𝑙∈R𝑑×𝑑for 𝑙= 1 and 𝑊1\n𝑙∈R2𝑑×𝑑for 𝑙> 1. 𝑏1\n𝑙∈R𝑑, 𝑙\nis the layer number, ⊕denotes tensor concatenation. Finally, we\nobtain the local logits 𝐻𝑙\n𝑙𝑜𝑐𝑎𝑙for each layer classes by using another\nlinear layer\n𝐻𝑙\n𝑙𝑜𝑐𝑎𝑙= 𝐿𝑙·𝑊2\n𝑙+ 𝑏2\n𝑙,𝑊2\n𝑙∈R𝑑×|I𝑙|,𝑏2\n𝑙∈R|I𝑙|\nwhere |I𝑙| is the number of classes in the layer.\nHowever, the label-attention model is unaware of the overall hier-\narchical structure. Therefore, we ensemble it with another method.\nWe refer to HiTIN [26] for the implementation of a state-of-the-art\nHTC global approach. In this method, a tree network is constructed\nbased on the simplified original taxonomy structure, and the mes-\nsages are propagated bottom-up in an isomorphism manner, which\ncomplements the label-attention model used. The embedding for\nleaf nodes are obtained by broadcasting the text representation 𝐻.\nAfter the tree isomorphism network propagation, all embedding\nfrom all layers are aggregated to form single embedding, and a\nclassification layer is used to obtain the logits 𝐻𝑔𝑙𝑜𝑏𝑎𝑙of all tree\nMarket\nLang.\nIntents\nTrain(ST)\nTest(MT)\nBR\npt\n316\n66k\n372\nID\nid\n481\n161k\n1145\nMY\nen,ms\n473\n74k\n1417\nPH\nen,fil\n237\n33k\n189\nSG\nen\n360\n76k\n737\nTH\nth\n359\n60k\n502\nTW\nzh-tw\n373\n31k\n353\nVN\nvi\n389\n178k\n525\nTable 1: Multilingual dataset statistics for Single Turn (ST)\nand Multi-Turn (MT).\nMKT\nModel\n𝑟𝑐\nCL-Label\nAccuracy\nSG\nNaive Concat.\n-\n-\n60.52%\nSG\nSelective Concat.\n-\n-\n56.99%\nSG\nLlama2-7B\n✘\n✘\n56.24%\nSG\nLlama2-7B\n✔\n✘\n61.33%\nSG\nDomain-Llama2-7B\n✔\n✘\n63.23%\nID\nNaive Concat.\n-\n-\n60.61%\nID\nSelective Concat.\n-\n-\n63.23%\nID\nLlama2-7B\n✔\n✘\n49.96%\nID\nSeaLLM-7B-chat\n✔\n✘\n52.49%\nID\nSeaLLM-7B-chat\n✔\n✔\n55.02%\nTable 2: Performance of LLM with symbol tuning approaches.\nnodes. The logits are then split by the number of classes in each\nlayer to obtain 𝐻𝑙\n𝑔𝑙𝑜𝑏𝑎𝑙.\nThe final class probabilities for each layer 𝑃𝑙is then obtained by:\n𝑃𝑙= 𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝐻𝑙\n𝑙𝑜𝑐𝑎𝑙+ 𝐻𝑙\n𝑔𝑙𝑜𝑏𝑎𝑙)\n4\nExperiments\n4.1\nDataset\nThe dataset used in our experiments is derived from the conversa-\ntion history of a large e-commerce platform. It includes user queries\n\nBalancing Accuracy and Efficiency in Multi-Turn Intent Classification for LLM-Powered Dialog Systems in Production\nPreprint, Working Paper, Nov 2024\nin the local languages of eight markets: Brazil (BR), Indonesia (ID),\nMalaysia (MY), Philippines (PH), Singapore (SG), Thailand (TH),\nTaiwan (TW), and Vietnam (VN), as detailed in Table 1. Labeled data\nwere manually annotated by local customer service teams, with\nonly samples achieving label consistency across three independent\ntaggers being selected to ensure quality.\nSingle-turn training data collected over years of business op-\nerations form the basis for supervised fine-tuning and in-context\nlearning. For multi-turn evaluation, real online sessions are anno-\ntated by local customer service teams, with only the last query\n𝑞𝑛labeled in each session Q. For preprocessing, we remove noisy\nannotations, standardize intents, and augment multi-turn sessions\nusing dialogue state transition probabilities derived from chat logs.\nSymbol Tuning. We perform symbol tuning on LLM for the SG\nand ID datasets, where SG mainly uses English while ID uses Bahasa\nIndonesia. The training data comprises a mix of existing single-turn\nsamples and about 60k semi-automatically crafted multi-turn sam-\nples added to each market. Some are obtained by cleaning data on\nonline chat logs to identify more accurate intents using an LLM\nwith a few-shots of the chain-of-thought prompt. The rest are con-\nstructed by combining several dialogues sampled from the existing\nsingle-turn training dataset to form one session. The transition of\nintents in a session is calculated from the online chatlog.\nHTC with C-LARA. 70k of online chat logs are sampled for pseudo-\nlabelling. After self-consistency checking, around 12% of the data\nyield inconsistent results and are discarded from training. 1.5k sam-\nples are split from the pseudo-labeled data to serve as the validation\nset for early stopping.\n4.2\nMetrics\nThe primary evaluation metric is the accuracy of predicted labels for\nthe final query 𝑞𝑛in each conversation session Q. Metrics account-\ning for class imbalance were not considered, as the sampled sessions\nreflect the distribution of online traffic across intents, providing a\nrealistic approximation of live performance.\n4.3\nImplementation Details\nSymbol Tuning on LLM. FastChat framework is used to fine-tune\n7B LLMs using LoRA method on their 𝑞_𝑝𝑟𝑜𝑗, 𝑣_𝑝𝑟𝑜𝑗, 𝑜_𝑝𝑟𝑜𝑗, and\n𝑘_𝑝𝑟𝑜𝑗modules with a learning rate of 2e-5 over 10 epochs. The\n7B models used are Llama-2-7B (for SG) and SeaLLM-7B-chat (for\nID) on Hugging Face. Before the models are fine-tuned on the\nmulti-turn intent recognition task, they are further pre-trained on\nShareGPT dataset with the same setting above, and the weights\nare then merged. For the sake of simplicity, we will refer to the\nLLMs further pre-trained on ShareGPT dataset as base models.\nDuring training for intent classification task, loss is calculated on\nall the model output including those after history queries. During\ninference, greedy decoding strategy is used to generate the target\n𝑟part, the prefix \"The intent title is \" is not generated but instead\nappended at the end of the prompt. When the generated label has\nno exact match with any 𝑟in I, gestalt string matching is used to\nfind the closest one.\nHTC with C-LARA. The in-house Hierarchical Text Classification\n(HTC) model is a BERT-based model fine-tuned using the combina-\ntion of the pseudo-labeled multi-turn data and existing single-turn\ndata, as shown in Section 4.1. We use AdamW to finetune the HTC\nwith a learning rate of 5e-6. All tests are run on a single Nvidia\nV100 GPU card with 32GB of GPU memory.\n4.4\nBaseline settings\nFor a fair comparison, we adopt three methods fine-tuned on HTC\nmodel (M𝑐) as our global baselines across two methods:\n(1) Single-turn method: where only the last query of a session\nis considered by M𝑐;\n(2) Naive concatenation: all queries are concatenated together\nbefore being fed into M𝑐;\n(3) Selective concatenation: where a concatenation selection\nmodel is trained to select the most suitable historical query\nwith the last query to serve as the input to M𝑐.\nST on LLM. In SG, except Llama2-7B, we also tried to continue pre-\ntraining the base models on in-domain corpus to strengthen the\nlanguage understanding of local languages and the corresponding\nslang used, as humans usually converse with the chatbot in a non-\nformal way. We term the domain specific base model as Domain-\nLlama2-7B. In ID, we switched Llama2-7B model to SeaLLM-7B-\nchat [11] which was introduced specifically for languages in South\nEast Asia.\nThe ST approach was adapted for supervised intent recognition\nusing compressed generation targets (𝑟𝑐) and cross-lingual labels\n(CL_label). These adjustments optimized performance by simplify-\ning the generative task while maintaining semantic integrity. Com-\nparisons with baseline methods in Table 2 show that ST achieves\ncompetitive results in English markets but faces challenges in non-\nEnglish settings due to limitations in pre-training for low-resource\nlanguages.\nHTC with C-LARA. This experiment uses Vicuna-13B as our base\nmodel for pseudo-labeling within LARA and C-LARA. We designed\nthree pipelines with four prompt templates in [9] to demonstrate\nthat using C-LARA for pseudo-labeling can effectively improve the\nHTC model’s performance in multi-turn classification tasks. The\ndetailed introduction is listed as follows:\n• LARA: Using LARA directly as a classifier.\n• LARA-PL: Using LARA as a naive pseudo-labeling tool and\nfine-turn HTC model with generated data.\n• C-LARA: Useing C-LARA to filter out the noise and generate\nhigh-quality data to fine-tune the HTC model.\n4.5\nOffline Experiments\nSymbol Tuning on LLM. Table 2 illustrates the effectiveness of Sym-\nbol Tuning (ST) on LLMs. Compressing the generation target 𝑟\nreduces task complexity and improves accuracy by 5.09% in the SG\nmarket. This compression also mitigates hallucination, reducing\ninstances of unmatched generated labels from 2.5% to 0%.\nInterestingly, this technique also stopped LLM hallucination, i.e.\ngenerating label with no match in the I. The hallucination rate\nwithout using compressed 𝑟is about 2.5%. In ID, which is a non-\nEnglish market, we find that cross-lingual label which changes\nthe generation target to English rather than in the local language\nalso improved the performance by 2.53%. Using different base\nmodels which were trained specifically on the in-domain corpus or\n\nPreprint, Working Paper, Nov 2024\nLiu et al.\nPipeline\nModel\nPrompt\nSelf-Consistency\nBR\nID\nMY\nPH\nSG\nTH\nTW\nVN\navg\nFine-tuning\nSingle-turn\n-\n-\n30.98%\n52.14%\n56.81%\n40.21%\n51.13%\n52.99%\n58.07%\n65.90%\n53.76%\nFine-tuning\nNaive Concat.\n-\n-\n50.81%\n60.61%\n57.02%\n47.62%\n60.52%\n56.97%\n65.44%\n76.95%\n60.08%\nFine-tuning\nSelective Concat.\n-\n-\n52.69%\n63.23%\n60.20%\n51.32%\n56.99%\n57.77%\n64.02%\n74.10%\n60.97%\nLARA\nVicuna-13B\nP\n✘\n52.69%\n61.48%\n65.42%\n54.50%\n65.26%\n60.96%\n67.14%\n77.90%\n64.18%\nC-LARA\nVicuna-13B\nP\n✔\n55.38%\n63.58%\n65.00%\n54.50%\n66.21%\n63.75%\n71.10%\n79.24%\n65.52%\nLARA\nVicuna-13B\nP𝑠𝑦𝑚𝑏𝑜𝑙𝑖𝑐\n✘\n51.88%\n60.00%\n64.57%\n53.97%\n65.26%\n58.96%\n65.44%\n74.67%\n62.92%\nC-LARA\nVicuna-13B\nP𝑠𝑦𝑚𝑏𝑜𝑙𝑖𝑐\n✔\n54.57%\n62.62%\n65.56%\n50.79%\n66.76%\n62.95%\n69.97%\n76.76%\n64.94%\nLARA\nVicuna-13B\nP𝑝𝑟𝑒𝑝𝑒𝑛𝑑\n✘\n54.03%\n61.75%\n64.50%\n53.44%\n65.94%\n61.55%\n66.86%\n75.81%\n63.97%\nC-LARA\nVicuna-13B\nP𝑝𝑟𝑒𝑝𝑒𝑛𝑑\n✔\n53.76%\n63.84%\n65.70%\n52.91%\n68.11%\n63.15%\n69.97%\n78.48%\n65.65%\nLARA\nVicuna-13B\nP𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑\n✘\n55.65%\n62.88%\n64.71%\n55.03%\n65.40%\n61.95%\n66.86%\n78.10%\n64.64%\nLARA-PL\nVicuna-13B\nP𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑\n✘\n55.91%\n64.19%\n64.43%\n49.21%\n66.49%\n61.95%\n69.41%\n81.14%\n65.29%\nC-LARA\nVicuna-13B\nP𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑\n✔\n55.91%\n65.33%\n66.27%\n51.85%\n67.16%\n63.35%\n72.80%\n78.86%\n66.35%\nTable 3: Performance of C-LARA compared to baselines, the average here is weighted on the number of test samples in\neach market. The results illustrate that C-LARA with formatted prompts achieves the best average accuracy (66.35%) across\nall markets. The results validate our approach’s effectiveness in both English and non-English markets, with significant\nimprovements over baseline methods.\nfor the local language also proves to be useful. Domain-Llama2-7B\nimproves the performance by 1.90% in SG while SeaLLM-7B-chat\nimproves the performance by 2.53% in ID compared to Llama2-\n7B. While the ST approach outperforms the baselines in English\nmarket, it still leaves a lot to be desired in non-English market. This\nphenomenon may arise as a result of the ST approach employed for\nnon-dominant languages during pre-training, which necessitates a\ngreater quantity or higher quality of data to achieve satisfactory\nperformance in a task that was not included in the pre-training\nphase. This is particularly true when the model lacks knowledge\npertaining to domain intents.\nPseudo-labeling using C-LARA. As demonstrated in Table 3, C-\nLARA improves pseudo-label quality through self-consistency val-\nidation, resulting in a 1.06% performance gain over LARA. This\nvalidation process identifies and removes approximately 12% of\ninconsistent samples, ensuring high-quality synthetic labels. While\nthis approach requires additional offline training resources, it signif-\nicantly lowers deployment costs by relying on a single, lightweight\nclassification model.\nThis most probably can be attributed to the advantages of the dis-\ncriminative method in classification tasks, as training process also\nexposed the model to the comprehensive high quality single-turn\ndataset. Besides, the pre-trained model used for M𝑐was also pre-\ntrained specifically on the in-domain multi-lingual corpus, making\nit a strong suit for our multilingual e-commerce setting. C-LARA’s\nintegration of self-consistency within the pseudo-labeling pipeline\nsignificantly enhances the quality of synthetic labels, resulting in a\n1.06% improvement in performance, as indicated in the last row of\nTable 3. When the LLM lacks confidence in its ICL responses, minor\nchanges in the input prompt can significantly alter the output. This\nmethod effectively identifies potential inaccuracies in ICL outputs\nfor black-box models where direct output scores are unavailable.\nMost importantly, this approach, while requiring longer offline\ntraining time, significantly reduces deployment costs to just one\nsmall classification model.\n4.6\nOnline Deployment Evaluation\nST on LLM.. Using the LMDeploy framework, LARA weights were\nmerged with the 7B base model, enabling faster inference times.\nDeployed on a single 32GB V100 GPU, the Symbol Tuning (ST)\napproach achieved an average latency of 170ms at 0.5 QPS in the\nSG market. In contrast, C-LARA models converted to ONNX format\n(1.1GB per model) achieved an average latency of 80ms at 1 QPS\non an 8-core CPU machine with 16GB memory, demonstrating\nsuperior scalability and cost-efficiency.\nC-LARA. We deploy C-LARA across all eight markets. The models\nwere first converted to ONNX format, reducing their size to 1.1GB.\nDeployed on an 8-core CPU machine with 16GB memory, C-LARA\nachieved an average latency of 80ms at 1 QPS, which is less than\nhalf the latency of the ST on LLM method. This deployment signifi-\ncantly reduced both costs and complexity, making it more scalable\nfor industrial applications. Due to its versatility, an Auto-Training\nPortal (ATP) ecosystem is built around the LARA-PL method (Fig.\n4). ATP enables seamless and continuous improvements for the\nchatbot’s multi-turn intent recognition system. Using online chat\nlogs, local operations teams can update the Knowledge Base (KB)\nby adding new intents and crafting example queries. Subsequently,\nthey can trigger C-LARA for pseudo-labeling multi-turn chat logs,\ngenerating data to train lightweight models. Once training is com-\nplete, the models are deployed through the portal for online A/B\ntesting, creating an iterative cycle of improvement. For fair com-\nparisons, the version of the KB (intents and single-turn training\ndata) was kept consistent across control and test groups.\n4.7\nOnline Performance\nWe leverage the following two metrics:\n(1) Resolution rate (RR) which is measured by the rate of user\ncompleting the answer flow, not transferring to live agent,\nand not giving bad rating to the answer.\n(2) Customer Service Satisfaction (SCSAT) where users will\nbe asked about their satisfaction towards our chatbot for\nchatbot only sessions (no intervention from live agents).\nThe score is calculated by # good rated sessions/(# good rated\nsessions + # bad rated sessions).\nWe use the selective concatenation method as the baseline for\nall experiments, with paired t-test to evaluate statistical significance.\n\nBalancing Accuracy and Efficiency in Multi-Turn Intent Classification for LLM-Powered Dialog Systems in Production\nPreprint, Working Paper, Nov 2024\nFigure 4: Online Deployment of Multi-turn Intent Classi-\nfication model demonstrates our production architecture\nintegrating C-LARA for automated training data generation.\nThe system handles real-time inference while continuously\nimproving through automated training.\nST on LLM. In the SG market, ST on LLMs was deployed to 50% of\nonline traffic for three weeks, yielding approximately 14k chatbot\nsessions per group. The test group exhibited a +2.19% improvement\nin Customer Service Satisfaction (SCSAT), but Resolution Rate (RR)\ndeclined by -0.11%. Neither result was statistically significant, indi-\ncating limited benefits from ST given its resource-intensive nature.\nC-LARA. For C-LARA, only the multi-turn dialogue model was re-\nplaced, while single-turn models remained unchanged. Aggregated\nresults from over 108k chatbot sessions per group showed statis-\ntically significant improvements: Resolution Rate (RR) increased\nby +0.78% and Customer Service Satisfaction (SCSAT) by +1.39%\n(p-value < 0.05). These gains translate to overall session improve-\nments of RR +0.47% and SCSAT +0.84%, as multi-turn dialogues\ncomprise 60.60% of total sessions.\nFurthermore, adding pseudo-labeled multi-turn data enhanced\nsingle-turn intent recognition. Substituting single-turn dialogue\nmodels with C-LARA models yielded an RR improvement of +0.06%\nand a statistically significant SCSAT increase of +0.27%.\n5\nAblation Study\n5.1\nEffect of Target Length\nWe investigate how the amount of information in ST generation\ntarget affects the intent recognition performance using two rather\nextreme approaches and their conversation semantic fluidity.\n5.1.1\nLonger Target Length . To achieve this, the model is trained\nto summarize all queries in Q before outputting the target 𝑟. For\ninstance, the new output format of model will be “You are ask-\ning about {𝑠𝑢𝑚𝑚𝑎𝑟𝑦}. So, the intent title is {𝑟𝑛}\". The rationale is\nto utilize the summarization ability of LLMs to better understand\nthe context. For our training data, the summaries are obtained\nby prompting the original LLM backbones in a zero-shot manner.\nWe chose this over increasing the length of 𝑟statically to impose\nmore information on the model’s generation target. Table 6 demon-\nstrates the impact of increasing the target length in Symbol Tuning\n(ST). Extending the generation target to include query summaries\ndecreases performance by 3.82%. While this approach enhances\nsemantic coherence, excessive information overloads the model,\nreducing its ability to focus on the core intent classification task.\n5.1.2\nShorter Target Length . The approach of compressing 𝑟was\ninspired by [21]. Hence, we also tried to replace 𝑟s with completely\nmeaningless symbols, while keeping the generation prefix of “The\nintent title is \". Compressing target labels to purely symbolic rep-\nresentations results in a significant 8.91% performance drop, as\nshown in Table 7. This highlights the importance of preserving\nsemantic richness in target labels for generative fine-tuning. Ef-\nfective compression methods must retain key information from\nthe original labels to avoid loss in classification accuracy.. Thus,\nwhen compressing 𝑟s, it is important to choose a method that can\npreserve the information in original 𝑟s as much as possible.\n5.2\nImpact of Self-consistency in MTIC\nUsing our multi-turn test sets, we evaluate the performance of\nMTIC with and without self-consistency checking. We remove the\nsamples with inconsistent outputs and calculate the precision of\nthe remaining samples. On average, 12% of test samples will be\nremoved in each market. Incorporating self-consistency checking\ninto MTIC evaluations improves accuracy across all prompt varia-\ntions, as shown in Table 4. By removing approximately 12% of test\nsamples with inconsistent outputs, this method effectively filters\nout erroneous predictions, ensuring higher-quality pseudo-labels\nand more reliable results. This ensures the quality of pseudo-labels.\n5.3\nEffect of Model Size\nFor fair comparison between LLM ST and C-LARA, we use vicuna-\n7b-v1.5 as the base model with prompt P and P𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑, without\nself-consistency checking. The results of LLM ST method are taken\nfrom the best of each market reported in this paper, including base\nmodels pre-trained on in-domain corpus, so it should have the\nadvantage over Vicuna-7B-v1.5. Table 5 compares C-LARA and\nLLM ST using models of the same size (Vicuna-7B-v1.5) without\nself-consistency checking. Despite the simpler pipeline, C-LARA\nconsistently outperforms LLM ST, avoiding the complexity of multi-\nturn sample crafting. However, smaller models exhibit reduced\ninstruction-following capabilities, as demonstrated by the lower\nperformance of P𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑compared to P. One interesting ob-\nservation here is that the performance of C-LARA when using\nP𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑is now lower than P. LLMs of smaller size could be\nweaker in instruction following, and in this sense the semantic\nmeaning of the labels in demonstrations are more critical. Prepend-\ning meaningless characters before labels can negatively affect the\nunderstanding of labels for smaller LLMs.\n6\nRelated Work\n6.1\nSynthetic Data Generation\nThe scarcity of annotated dialogue data, particularly in low-resource\nlanguages, has driven research into synthetic data generation. Borisov\net al. [1] proposed a method leveraging auto-regressive generative\nmodels to create realistic tabular datasets, highlighting their util-\nity in data augmentation. Similarly, Li et al. [6] demonstrated that\nsynthetic data generated by LLMs can significantly enhance model\nperformance in classification tasks. Additionally, Tang et al. [18] uti-\nlized synthetic data to craft challenging examples for fact-checking,\nimproving the factual accuracy of LLM outputs.\n\nPreprint, Working Paper, Nov 2024\nLiu et al.\nPrompt\nSelf-Consistency\nBR\nID\nMY\nPH\nSG\nTH\nTW\nVN\navg\nP\n✘\n52.69%\n61.48%\n65.42%\n54.50%\n65.26%\n60.96%\n67.14%\n77.90%\n64.18%\nP\n✔\n58.59%\n68.13%\n69.93%\n56.44%\n69.58%\n66.75%\n71.30%\n81.14%\n69.11%\nP𝑠𝑦𝑚𝑏𝑜𝑙𝑖𝑐\n✘\n51.88%\n60.00%\n64.57%\n53.97%\n65.26%\n58.96%\n65.44%\n74.67%\n62.92%\nP𝑠𝑦𝑚𝑏𝑜𝑙𝑖𝑐\n✔\n56.63%\n64.71%\n68.48%\n55.19%\n68.77%\n65.59%\n71.61%\n78.02%\n67.27%\nP𝑝𝑟𝑒𝑝𝑒𝑛𝑑\n✘\n54.03%\n61.75%\n64.50%\n53.44%\n65.94%\n61.55%\n66.86%\n75.81%\n63.97%\nP𝑝𝑟𝑒𝑝𝑒𝑛𝑑\n✔\n59.49%\n66.08%\n68.60%\n55.90%\n68.85%\n68.19%\n71.79%\n81.36%\n68.43%\nP𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑\n✘\n55.65%\n62.88%\n64.71%\n55.03%\n65.40%\n61.95%\n66.86%\n78.10%\n64.64%\nP𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑\n✔\n59.24%\n67.01%\n69.47%\n56.79%\n68.81%\n68.69%\n72.35%\n82.93%\n69.12%\nTable 4: Precision of C-LARA variants after filtering inconsistent predictions demonstrates the effectiveness of self-consistency\nchecking across different prompt types (P, 𝑃𝑠𝑦𝑚𝑏𝑜𝑙𝑖𝑐, 𝑃𝑝𝑟𝑒𝑝𝑒𝑛𝑑, 𝑃𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑). Across all prompt types, accuracy improves by\napproximately 4-5%, with P𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑achieving the highest precision (69.12%). These results validate the robustness of self-\nconsistency as a filtering strategy.\nMethod\nPrompt\nID\nSG\navg\nLLM ST\n-\n58.17%\n63.23%\n60.15%\nC-LARA\nP\n60.44%\n64.31%\n61.96%\nC-LARA\nP𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑\n59.83%\n64.04%\n61.48%\nTable 5: Results of LARA using 7B LLM.\nMKT\nModel\nLonger Target Length\nAccuracy\nSG\nLlama2-7B\n✘\n54.82%\nSG\nLlama2-7B\n✔\n51.02%\nTable 6: Effect of Longer Target Length on LLM ST Classifi-\ncation Performance: lengthening targets results in a perfor-\nmance drop by 3.82%.\nMKT\nModel\nShorter Target Length\nAccuracy\nID\nSeaLLM-7B\n✘\n55.02%\nID\nSeaLLM-7B\n✔\n46.11%\nTable 7: Effect of Shorter Target Length on LLM ST Classifi-\ncation Performance: reducing semantically rich targets into\nsymbols costs a drastic performance drop by 8.91%.\n6.2\nModeling Multi-turn Dialogue Context\nMulti-turn dialogue modeling is essential for dialogue understand-\ning tasks. Early methods used bidirectional contextual LSTMs [4]\nto capture context-aware utterance representations for tasks such\nas MultiWOZ intent classification [2]. Other approaches, such as\nmulti-channel graph convolutional networks, were applied to query\nclassification in E-commerce [25].\nRecent advancements leverage pre-trained language models\n(PLMs) as sentence encoders [17], particularly for emotion recogni-\ntion in conversations (ERC). For instance, Lee and Lee [5] encoded\nboth context and speaker memory using PLMs, while Qin et al. [14]\nincorporated multi-turn information from utterances and dialogue\nstructure through fine-tuning. Despite their effectiveness, these\nmethods depend heavily on multi-turn training datasets, which are\ndifficult to acquire in real-world e-commerce settings [7]. In con-\ntrast, our approach employs LLMs within an augmentation-based\npipeline to generate multi-turn data, enabling zero-shot intent clas-\nsification using smaller models.\n6.3\nLLM on text classification\nRecent studies have explored the applicability of LLMs across vari-\nous domains. Chae and Davidson [3] investigated LLMs for socio-\nlogical text classification, demonstrating their potential in social\nscience research. In financial intent detection, Loukas et al. [10]\nanalyzed the trade-offs between performance and cost when us-\ning LLMs for text classification. Liu et al. [8] employed GPT-4o to\nperform zero-shot classification on multi-level semi-structured text\nwith retrieval augmentation. Wei et al. [20] highlighted the benefits\nof fine-tuning LLMs on domain-specific datasets, improving perfor-\nmance in legal document review. Wei et al. [21] introduced symbol\ntuning, where natural language labels were replaced with unrelated\nsymbols during fine-tuning to enhance classification. Our work\ndiffers by compressing longer intent labels into semantically mean-\ningful phrases, enabling easier generation and improving accuracy\nfor tasks with a large number of classes.\n7\nConclusion\nMulti-turn intent classification plays a critical role in modern dia-\nlogue systems. Unlike typical classification tasks, real-world intent\nclassification often involves varying intent lengths, posing unique\nchallenges. In this work, we introduced Symbol Tuning to fine-tune\nlarge language models (LLMs) with compressed intents. Our exper-\niments demonstrated that shortening intents improved accuracy\nby 5.09% compared to using original intents.\nAdditionally, we proposed C-LARA, an augmentation-based\npipeline for generating high-quality multi-turn datasets using self-\nconsistency validation. Training smaller models with pseudo-labeled\ndata generated by C-LARA yielded a 1.06% average performance\nimprovement. Empirically, C-LARA significantly reduces annota-\ntion costs by automating pseudo-labeling based on the user’s latest\nutterance in dialogue history, improving model iteration efficiency.\nFurthermore, training smaller models offers computational effi-\nciency, enabling scalable deployment and online inference.\nFuture Work. Moving forward, we aim to incorporate features\nsuch as user profiles and order history into C-LARA to support more\ndiverse dialogue tasks. We also plan to explore cross-lingual transfer\nand advanced tokenization techniques to enhance performance in\nlow-resource languages.\n\nBalancing Accuracy and Efficiency in Multi-Turn Intent Classification for LLM-Powered Dialog Systems in Production\nPreprint, Working Paper, Nov 2024\nReferences\n[1] Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji\nKasneci. 2022. Language Models Are Realistic Tabular Data Generators. ArXiv\nabs/2210.06280 (2022).\n[2] Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva,\nStefan Ultes, Osman Ramadan, and Milica Gašić. 2018. MultiWOZ–a large-scale\nmulti-domain wizard-of-oz dataset for task-oriented dialogue modelling. arXiv\npreprint arXiv:1810.00278 (2018).\n[3] Youngjin Chae and Thomas Davidson. 2023. Large language models for text\nclassification: From zero-shot learning to fine-tuning. Open Science Foundation\n(2023).\n[4] Deepanway Ghosal, Navonil Majumder, Rada Mihalcea, and Soujanya Poria.\n2021. Exploring the role of context in utterance-level emotion, act and intent\nclassification in conversations: An empirical study. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021. 1435–1449.\n[5] Joosung Lee and Wooin Lee. 2022. CoMPM: Context Modeling with Speaker’s\nPre-trained Memory Tracking for Emotion Recognition in Conversation. In\nProceedings of the 2022 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies. 5669–5679.\n[6] Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, and Ming Yin. 2023. Synthetic Data\nGeneration with Large Language Models for Text Classification: Potential and\nLimitations. Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) (2023).\n[7] Junhua Liu and Bin Fu. 2024. Responsible Multilingual Large Language Models:\nA Survey of Development, Applications, and Societal Impact. ArXiv (2024).\n[8] Junhua Liu, Kwan Hui Lim, and Roy Ka-Wei Lee. 2024. Towards Objective and\nUnbiased Decision Assessments with LLM-Enhanced Hierarchical Attention\nNetworks. arXiv preprint arXiv:2411.08504 (2024).\n[9] Junhua Liu, Yong Keat Tan, Bin Fu, and Kwan Hui Lim. 2024. LARA: Linguistic-\nAdaptive Retrieval-Augmentation for Multi-Turn Intent Classification. Proceed-\nings of the Empirical Methods in Natural Language Processing (2024).\n[10] Lefteris Loukas, Ilias Stogiannidis, Odysseas Diamantopoulos, Prodromos Malaka-\nsiotis, and Stavros Vassos. 2023. Making llms worth every penny: Resource-\nlimited text classification in banking. In Proceedings of the Fourth ACM Interna-\ntional Conference on AI in Finance. 392–400.\n[11] Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Zhiqiang Hu,\nChenhui Shen, Yew Ken Chia, Xingxuan Li, Jianyu Wang, Qingyu Tan, Liying\nCheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang Zhang, and\nLidong Bing. 2024. SeaLLMs – Large Language Models for Southeast Asia.\narXiv:2312.00738 [cs.CL] https://arxiv.org/abs/2312.00738\n[12] Pavel Přibáň, Jakub Šmíd, Josef Steinberger, and Adam Mištera. 2024. A compar-\native study of cross-lingual sentiment analysis. Expert Systems with Applications\n247 (2024), 123247.\n[13] Libo Qin, Wanxiang Che, Yangming Li, Mingheng Ni, and Ting Liu. 2020. Dcr-net:\nA deep co-interactive relation network for joint dialog act recognition and senti-\nment classification. In Proceedings of the AAAI conference on artificial intelligence,\nVol. 34. 8665–8672.\n[14] Xiangyu Qin, Zhiyu Wu, Tingting Zhang, Yanran Li, Jian Luan, Bin Wang, Li\nWang, and Jinshi Cui. 2023. Bert-erc: Fine-tuning bert is enough for emotion\nrecognition in conversation. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 37. 13492–13500.\n[15] Chen Qu, Liu Yang, W. Bruce Croft, Yongfeng Zhang, Johanne R. Trippas, and\nMinghui Qiu. 2019. User Intent Prediction in Information-seeking Conversations.\nIn Proceedings of the 2019 Conference on Human Information Interaction and\nRetrieval (CHIIR ’19). ACM. https://doi.org/10.1145/3295750.3298924\n[16] Phillip Rust, Jonas Pfeiffer, Ivan Vulić, Sebastian Ruder, and Iryna Gurevych. 2021.\nHow Good is Your Tokenizer? On the Monolingual Performance of Multilingual\nLanguage Models. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers). 3118–3135.\n[17] Weizhou Shen, Siyue Wu, Yunyi Yang, and Xiaojun Quan. 2021. Directed Acyclic\nGraph Network for Conversational Emotion Recognition. In Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long\nPapers). 1551–1560.\n[18] Liyan Tang, Philippe Laban, and Greg Durrett. 2024. MiniCheck: Efficient Fact-\nChecking of LLMs on Grounding Documents. ArXiv abs/2404.10774 (2024).\n[19] Zhiqiang Wang, Yiran Pang, and Yanbin Lin. 2024. Smart Expert System: Large\nLanguage Models as Text Classifiers. arXiv preprint arXiv:2405.10523 (2024).\n[20] Fusheng Wei, Robert Keeling, Nathaniel Huber-Fliflet, Jianping Zhang, Adam\nDabrowski, Jingchao Yang, Qiang Mao, and Han Qin. 2023. Empirical study of\nLLM fine-tuning for text classification in legal document review. In 2023 IEEE\nInternational Conference on Big Data (BigData). IEEE, 2786–2792.\n[21] Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun\nChen, Yifeng Lu, Denny Zhou, Tengyu Ma, and Quoc V. Le. 2023. Symbol tuning\nimproves in-context learning in language models.\narXiv:2305.08298 [cs.CL]\nhttps://arxiv.org/abs/2305.08298\n[22] H. Weld, X. Huang, S. Long, J. Poon, and S. C. Han. 2021. A survey of joint\nintent detection and slot-filling models in natural language understanding.\narXiv:2101.08091 [cs.CL]\n[23] Ting-Wei Wu, Ruolin Su, and Biing-Hwang Juang. 2021. A Context-Aware\nHierarchical BERT Fusion Network for Multi-turn Dialog Act Detection.\narXiv:2109.01267 [cs.CL]\n[24] Puyang Xu and Ruhi Sarikaya. 2014. Contextual domain classification in spoken\nlanguage understanding systems using recurrent neural network. In 2014 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP).\n136–140. https://doi.org/10.1109/ICASSP.2014.6853573\n[25] Chunyuan Yuan, Ming Pang, Zheng Fang, Xue Jiang, Changping Peng, and\nZhangang Lin. 2024. A Semi-supervised Multi-channel Graph Convolutional\nNetwork for Query Classification in E-commerce. In Companion Proceedings of\nthe ACM on Web Conference 2024. 56–64.\n[26] He Zhu, Chong Zhang, Junjie Huang, Junran Wu, and Ke Xu. 2023. HiTIN:\nHierarchy-aware Tree Isomorphism Network for Hierarchical Text Classification.\nIn Annual Meeting of the Association for Computational Linguistics. https://api.\nsemanticscholar.org/CorpusID:258865236",
    "pdf_filename": "Balancing_Accuracy_and_Efficiency_in_Multi-Turn_Intent_Classification_for_LLM-Powered_Dialog_Systems.pdf"
}