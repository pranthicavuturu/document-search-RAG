{
    "title": "Balancing Accuracy and Efficiency in Multi-Turn Intent",
    "abstract": "Accuratemulti-turnintentclassificationisessentialforadvancing conversationalAIsystems.However,challengessuchasthescarcity ofcomprehensivedatasetsandthecomplexityofcontextualdepen- denciesacrossdialogueturnshinderprogress.Thispaperpresents twonovelapproachesleveragingLargeLanguageModels(LLMs) toenhancescalabilityandreducelatencyinproductiondialogue systems.First,weintroduceSymbolTuning,whichsimplifiesintent labelstoreducetaskcomplexityandimproveperformanceinmulti- turndialogues.Second,weproposeC-LARA(Consistency-aware, LinguisticsAdaptiveRetrievalAugmentation),aframeworkthat employsLLMsfordataaugmentationandpseudo-labelingtogener- atesyntheticmulti-turndialogues.Theseenricheddatasetsareused tofine-tuneasmall,efficientmodelsuitablefordeployment.Exper- Figure1:Comparisonofinstructiontuningandsymboltun- imentsconductedonmultilingualdialoguedatasetsdemonstrate ing.Simplifyingverboseintentlabels(e.g.,“RequesttoCan- significantimprovementsinclassificationaccuracyandresource celOrder”→“CancelOrder”)reducesredundancy,enhanc- efficiency.Ourmethodsenhancemulti-turnintentclassification ingLLMclassificationperformanceby5.09%,addressingkey accuracyby5.09%,reduceannotationcostsby40%,andenablescal- challengesinproductionintentclassification. abledeploymentinlow-resourcemultilingualindustrialsystems, 1 Introduction highlightingtheirpracticalityandimpact. Dialoguesystemsarecriticalforautomatinginteractionsbetween customersandagents,streamliningcommunicationandenhanc- CCSConcepts inguserexperience.Theyplayapivotalroleininternationale- •Informationsystems→Languagemodels;Questionanswer- commerce platforms, addressing the increasing demand for in- ing;•Computingmethodologies→Intelligentagents. stantaneousandefficientcustomerservice.Intentclassification, afundamentalaspectofnaturallanguageunderstandingindia- loguesystems,involvesidentifyingusers’goalsfromtheirinputs,",
    "body": "Balancing Accuracy and Efficiency in Multi-Turn Intent\nClassification for LLM-Powered Dialog Systems in Production\nJunhuaLiu1,3,∗ ,YongKeatTan2,∗ ,BinFu2,† ,KwanHuiLim3\n1ForthAI\n2Shopee\n3SingaporeUniversityofTechnologyandDesign\nSingapore\nAbstract\nAccuratemulti-turnintentclassificationisessentialforadvancing\nconversationalAIsystems.However,challengessuchasthescarcity\nofcomprehensivedatasetsandthecomplexityofcontextualdepen-\ndenciesacrossdialogueturnshinderprogress.Thispaperpresents\ntwonovelapproachesleveragingLargeLanguageModels(LLMs)\ntoenhancescalabilityandreducelatencyinproductiondialogue\nsystems.First,weintroduceSymbolTuning,whichsimplifiesintent\nlabelstoreducetaskcomplexityandimproveperformanceinmulti-\nturndialogues.Second,weproposeC-LARA(Consistency-aware,\nLinguisticsAdaptiveRetrievalAugmentation),aframeworkthat\nemploysLLMsfordataaugmentationandpseudo-labelingtogener-\natesyntheticmulti-turndialogues.Theseenricheddatasetsareused\ntofine-tuneasmall,efficientmodelsuitablefordeployment.Exper- Figure1:Comparisonofinstructiontuningandsymboltun-\nimentsconductedonmultilingualdialoguedatasetsdemonstrate ing.Simplifyingverboseintentlabels(e.g.,“RequesttoCan-\nsignificantimprovementsinclassificationaccuracyandresource\ncelOrder”→“CancelOrder”)reducesredundancy,enhanc-\nefficiency.Ourmethodsenhancemulti-turnintentclassification ingLLMclassificationperformanceby5.09%,addressingkey\naccuracyby5.09%,reduceannotationcostsby40%,andenablescal- challengesinproductionintentclassification.\nabledeploymentinlow-resourcemultilingualindustrialsystems, 1 Introduction\nhighlightingtheirpracticalityandimpact.\nDialoguesystemsarecriticalforautomatinginteractionsbetween\ncustomersandagents,streamliningcommunicationandenhanc-\nCCSConcepts inguserexperience.Theyplayapivotalroleininternationale-\n•Informationsystems→Languagemodels;Questionanswer- commerce platforms, addressing the increasing demand for in-\ning;•Computingmethodologies→Intelligentagents. stantaneousandefficientcustomerservice.Intentclassification,\nafundamentalaspectofnaturallanguageunderstandingindia-\nloguesystems,involvesidentifyingusers’goalsfromtheirinputs,\nKeywords\ntherebyminimizingwaitingtimesandoperationalcosts[22].User\nMulti-turnIntentClassification,MultilingualLargeLanguageModel, interactionsfrequentlyevolveintomulti-turndialogueswhende-\nRetrievalAugmentation,ComputationalLinguistics,LanguageDi- tailedinformationisrequired,complicatingthedevelopmentof\nversity,KnowledgeEngineering multi-turnintentclassification(MTIC)models,despitetheirsimi-\nlaritytostandardtextclassificationtasks.Additionally,real-world\nACMReferenceFormat: multilingualsystemsrequirescalablesolutionsthatupholdinclu-\nJunhuaLiu1,3,∗,YongKeatTan2,∗,BinFu2,†,KwanHuiLim3.2024.Balanc-\nsivityandethicalstandards,particularlyinlow-resourcesettings.\ningAccuracyandEfficiencyinMulti-TurnIntentClassificationforLLM- Thiscomplexityarisesfromtheneedtoconsidercontextualfac-\nPoweredDialogSystemsinProduction.InProceedingsofPreprint.ACM, torslikehistoricalutterancesandpriorintents.Withoutaproper\nNewYork,NY,USA,9pages.https://doi.org/N.A\nunderstandingofsessioncontext,thesystemrisksmisinterpreting\nuserintentions,whichmayresultinincorrectapplicationsorirrel-\nevantresponses[24].Consequently,MTICwithindialoguesystem\npresentssignificantchallenges.\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor\nThefirstchallengeisthatthelengthofintentsinindustrialdi-\nclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed\nforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation aloguesystemsislongercomparedtogeneraltextclassification\nonthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored. tasks.Figure1showsthattherealintentscompriseseveralwordsin\nForallotheruses,contacttheowner/author(s).\nourknowledgebasebecauseoperators(Ops)typicallyassignintents\nPreprint,WorkingPaper,Nov2024\n©2024Copyrightheldbytheowner/author(s).\nACMISBN978-x-xxxx-xxxx-x/YY/MM *EqualContributions.\nhttps://doi.org/N.A †CorrespondingAuthor:bin.fu@shopee.com\n4202\nvoN\n91\n]LC.sc[\n1v70321.1142:viXra\nPreprint,WorkingPaper,Nov2024 Liuetal.\nSecondly,toovercometheshortageofmulti-turndata,wepro-\nposeanovelpseudo-labelinganddatagenerationframeworkcalled\nConsistency-awareLinguisticsAdaptiveRetrievalAugmentation\n(C-LARA).Extendingbeyondexistingsyntheticdatageneration[9],\nC-LARAservesasaneffectivepseudo-labelingtoolforgenerat-\ning multi-turn data from user’s unlabeled utterances with self-\nconsistency.C-LARAarrangestheretrievalresultindifferentorders\ntoassembleadaptiveprompts,whichcoverthediversereasoning\npath and filter out noise in in-context learning to improve the\nqualityoflabelingdata.Subsequently,weusethetrainingdata\ntotrainasmallermodelforonlineinference.C-LARAisanovel\nframeworktailoredformulti-turnintentclassification.Itaddresses\nlimitationsinpriorapproachesbyleveragingadaptiveretrievaland\nFigure2:Annotationpipelineofmulti-turnintentclassifica- self-consistencymechanismstoenhancetheaccuracyofpseudo-\ntiondatasets.Twomajorchallengesinproductionsystems labelingformulti-turndialogues.Unlikepreviousmethods,itdi-\nareillustrated:(1)managingnumerous(500+)intentsacross rectlyoptimizesforzero-shotmulti-turndataclassificationand\nmarketswithredundantlabels,and(2)thehighcostofcol- scalabledeployment.\nlectingmulti-turntrainingdata. Insummary,thecontributionsofthispaperareasfollows:\n(1) Weintroducesymbol-tuning,leveragingcompressedintents\naclearanddescriptivenametofacilitateknowledgemanagement, toenhanceLLMperformanceforMTIC,demonstratinga\nwhichmakesthemredundant.Therecentadvancementsinlarge 5.09%improvementinsupervisedfine-tuning(SFT)results.\nlanguagemodel(LLMs)presentnewresearchopportunitiestosim- (2) WedevelopC-LARA,anovelframeworkforgeneratinghigh-\nplify and optimize the text classification process [19]. Research qualitymulti-turndata,effectivelyaugmentingMTICresults.\nindicatesthatLLMsperformexcellentlyinsentimentanalysis[12], (3) We fine-tune smaller models using data generated by C-\nwhichonlyadoptsshorterlabelssuchaspositive,negative.How- LARA,enablingscalableandaccuratedeploymentofMTIC\never,LLMsstillfailtoaddresscontextdependencyinmulti-turn systemsinlow-resourceindustrialsettings.\nconversationsandstrugglewithlongintentlabelscommoninin-\ndustrialsystems.\n2 ProblemFormulation\nThesecondchallengeliesinthedifficultyofcollectingmulti-turn\ndatasets.Whileseveralstudies[15,23]onMTICexist,theyoften 2.1 Multi-TurnIntentClassification\nassumeaccesstocomprehensivemulti-turntrainingdata,whichis\nMulti-TurnIntentClassification(MTIC)involvesidentifyingthe\nrarelyavailableinreal-worldapplications. intent𝐼 ofthefinalquery𝑞 𝑛 fromapredefinedsetI,basedona\nw claeF si si gg in fiu o cr ae re t2 iots h nh eo [r 1w e 3ds ]ut wh ne id ta ha nn otn nio n lt yfa ot li eo w sn sit tp h hi ip an ne il ni 1n t 0e en cf lo t asr s, sM u en sT lI wiC k ie tt ha d is i nak ls do. igE auv loee gn a uci etf s tae sq kue rn ec lie esof onus te hr equ coe nri ve es rQ sat= ion{ a𝑞 l𝑖} c𝑛 𝑖 o=1 nti en xta Ccha =tbo {t 𝑞 𝑖s }e 𝑛 𝑖s =−s 1i 1o ,n w.T hih ci hs\nincludespriorqueries.Context-dependencyaddscomplexity,re-\nstatetracking(DST),therearehundredsofintentsoperatedbylocal\nquiringmodelstointerpretnuancedconversationaldynamicsand\nOpsinknowledgebaseofdialgouesystemtocoveruser’svarious\nevolving user intentions. Each intent 𝐼 has a local-language ti-\nandspecificintentsineachmarket,whichincreasethecomplexity\ntle𝑦 and a hierarchical English category𝑧 (e.g., Indonesia:𝑦 =\nofmulti-turnclassificationanditsannotation.Annotatorsoften\n’Caramembatalkanpesanan’,𝑧=’Logistics>Order>Cancellation’).\nstrugglewiththenumerousofintents,leadingtoincreasedmis-\ntakesandlongerdecision-makingtimes.Asaresult,theannotation\nprocessbecomescostlyandtime-intensive,makingitimpractical 2.2 SupervisedFine-tuning\ntomanuallyannotatelarge-scalemulti-turndatasets.However,in-\nSupervisedFine-tuning(SFT)adaptspre-trainedlargelanguage\nsufficienttrainingdatacansignificantlyhindermodelperformance\nmodels(LLMs)forspecifictasksusinglabeleddatasets.Thisprocess\nevenwithLLM.Thesechallengesunderscorethenecessityformore\nachieveshighbenchmarkaccuracythroughtask-specificsupervi-\nefficientmethodstoaddressdatascarcityandclassificationcom-\nsion.\nplexity.\nTotackletwochallenges,wefirststudythefeasibilityofusing\nLLMforsupervisedfine-tuning(SFT)toperformMTICusinga Problem Definition. Given a dataset D = {(𝑥 𝑖,𝑦 𝑖)} 𝑖𝑁 =1, where𝑥 𝑖\ngenerativemethod.Variousintentsincreasethecomplexityofthis isaninputqueryand𝑦 𝑖 isthecorrespondinglabel,theobjective\ntasksincethemoretokensalargelanguagemodel(LLM)generates, is to optimize model parameters𝜃 to maximize the conditional\nthelowerthetaskperformance[16].Toconquerthis,wecompress\nlikelihood𝑝(𝑦 𝑖|𝑥 𝑖;𝜃):\ntheredundantinfowithinintenttosuccinctintentviaGPT4,then\nadoptthoseintentsinSFTnamedassymboltuning,whichhelpto\n𝑁\nr ge ed nu ec re att ih vee mdi effi thcu odlt .yofmulti-turnclassificationtasksbytheLLM L SFT(𝜃)=− 𝑁1 ∑︁ 𝑖=1log𝑝(𝑦 𝑖|𝑥 𝑖;𝜃).\nBalancingAccuracyandEfficiencyinMulti-TurnIntentClassificationforLLM-PoweredDialogSystemsinProduction Preprint,WorkingPaper,Nov2024\nConditionalProbabilityModeling. Forstructuredoutputs,𝑦 𝑖 isa ...\nsequenceoftokens{𝑡 1,𝑡 2,...,𝑡 𝑚},withprobabilityfactorizedau- USER:\"{q_n}\"\ntoregressively: ASSISTANT:\"Theintenttitleis{r_n}.\"\n𝑚\n𝑝(𝑦|𝑥;𝜃)=(cid:89)𝑝(𝑡 𝑗|𝑡 <𝑗,𝑥;𝜃). Thegenerated𝑟 𝑛iscomparedwithintentsinIusingcosinesimilar-\n𝑗=1 ityintheembeddingspacetoensuresemanticalignmentbetween\nthemodel’soutputandpredefinedintenttitles.\nThetrainingobjectivebecomes:\nL SFT(𝜃)=−\n𝑁1 ∑︁𝑁 ∑︁𝑚\nlog𝑝(𝑡 𝑗|𝑡 <𝑗,𝑥 𝑖;𝜃).\nC siso tm op fr ae pss pe rd oxG imen ae tr ea lt yio 1n 2. tI on kt ee nn st ,r mep ar ke is ne gn tt ha eti mve inq eu ffier cie ies n𝑟 to asfte gn enc eo rn a-\n-\n𝑖=1 𝑗=1\ntiontargets.Toaddressthis,weemployanLLMtocompress𝑟 into\n2.3 SymbolTuning concisephrases,typicallytwowords,whilepreservingtheirseman-\nticessence.Thisprocessensuresthateachcompressedintentlabel\nUnlikemethodsreplacingtasklabelswithunrelatedsymbols[21], 𝑟 𝑐 isunique.Ifduplicatesoccur,themodeliterativelyincreasesthe\nourSymbolTuningapproachfocusesonintentclassification.Ver-\nwordcountuntiluniquenessisachieved.Thiscompressionreduces\nboselabelsinindustrialsystemsdispersesemanticinformation, theaveragelengthof𝑟 𝑐 tofourtokens,optimizingitforgenera-\nhinderingmodelperformance.Toaddressthis,wecompresslabels\ntiontasksandimprovingclassificationaccuracy.Thisapproach\nintoconcisephrasesusingGPT-4.Forexample,\"RequesttoCan-\nenhancesclassificationaccuracybyreducingsemanticdispersion\ncelOrder\"becomes\"CancelOrder,\"servingascompactsemantic\ninlabels,ensuringmorefocusedinformationpropagationthrough\nanchorsthatenhanceshallowanddeeplayerrepresentations.\nLLMlayers.\nMathematical Formulation. Let the original intent label be 𝐿 =\n{𝑡 1,𝑡 2,...,𝑡 𝑚}. The compressed label𝐿′, with𝑛 ≪ 𝑚, is gener- Cross-LingualLabels. Innon-Englishmarkets,intentlabels𝑟 are\natedbyoptimizing: compressedintoEnglishwhileretainingtheoriginallanguagefor\ninputqueriesQ.LeveragingEnglish,thepredominantlanguagein\n𝐿′ =argmin𝐿′ C(𝐿′)+E(𝐿′,𝐿),\nLLMpretrainingcorpora,simplifieslabelgenerationandenhances\nwhere:- C(𝐿′):Compactnessof𝐿′ (e.g.,tokencount).- E(𝐿′,𝐿): modelperformanceinmultilingualsettings.Thiscross-lingualstrat-\nSemanticdivergence,computedas: egyreducescomplexityandimprovesalignmentwithpretraining\nE(𝐿′,𝐿)=1−cosine_sim(𝜙(𝐿′),𝜙(𝐿)), distributions.Thisstrategyleveragesthestrengthsofpre-trained\nLLMswhileaccommodatingmultilingualdata,offeringascalable\nwith𝜙(·)asanembeddingfunction.\nsolutionforcross-lingualintentclassification.\nObjectiveFunction. GivenD ={(𝑥 𝑖,𝐿 𝑖)} 𝑖𝑁 =1,where𝐿 𝑖istheoriginal\nlabel,thesupervisedfine-tuninglossbecomes: 3.2 Consistency-awareLinguisticsAdaptive\n𝑛 RetrievalAugmentation\nL ST(𝜃)=−E (𝑥,𝐿′)∼D∑︁ log𝑝(𝑡 𝑗|𝑡 <𝑗,𝑥;𝜃),\nToenhancein-contextlearning,weproposetheConsistency-aware,\n𝑗=1\nLinguisticsAdaptiveRetrievalAugmentation(C-LARA)framework.\nwhere𝑡\n<𝑗\ndenotesprecedingtokensin𝐿′.\nBuildingupontheLARAmodel[9],C-LARAincorporatesafine-\nPerformanceImplications. Replacingverboselabels𝐿withcompact tunedsingle-turnmodelM𝑐 withinaretrieval-augmentedpipeline.\n𝐿′reducestokenprocessingandimprovesclassificationaccuracy, Thisframeworkenableszero-shotMulti-TurnIntentClassification\nstreamliningintentrecognitiontasks. (MTIC)usingonlysingle-turndemonstrations.UnlikeLARA,which\niscomputationallyintensiveinreal-time,C-LARAoperatesoffline\n3 Solutions asapseudo-labelingtool,generatinghigh-qualitymulti-turndata\nfortraininglightweightclassificationmodels.\n3.1 SymbolTuningonLLM\nSpecifically,theLARApipelinecanbecomplexandresource-\nToaddressintentclassificationtasks,weutilizegenerativemodels intensivetoimplementforreal-timesystems.Hence,weusethis\nratherthanconventionaldiscriminativeorregressiveapproaches. methodofflineasamulti-turndatapseudo-labelingtooltotraina\nOurSymbolTuning(ST)methodinvolvessupervisedfine-tuning smallerclassificationmodel.Thetrainingmethodmirrorsthatof\n(SFT)ofanLLMwithcompressedintentlabels.Givenacomplete thesingle-turnclassifierM𝑐 followingthepaper,addingpseudo-\nchatsessionS={𝑞 1,𝐼 1,...,𝑞 𝑛−1,𝐼 𝑛−1,𝑞 𝑛},themodelistrainedto labeledmulti-turndatatotheoriginaldatacomprisingonlysingle-\ngeneratetherepresentativequestion𝑟 𝑛correspondingtothecorrect turnsamples.\nintent𝐼 𝑛ofthefinalquery𝑞 𝑛.Queriesandintentsarestructured Sincethisisnotareal-timetask,thepipelineresponsetimeisnot\ninanaturalquestion-answeringflow,asillustratedbelow: acriticalconsideration,henceself-consistencycheckingwasper-\nSYSTEM:\"Achatbetweenacurioususerandanar- formedontheLLMoutputstoensurethequalityofpseudo-labels.\ntificialintelligenceassistant.Theassistantprovides Forthischeck,asshowninFigure3,thein-contextlearningphase\nhelpful,detailed,andpoliteresponsestotheuser’s isrunthreetimespersample,withthein-contextdemonstrations\nquestions.\" sortedinthreeordersaccordingtotheirsimilarityscorestothe\nUSER:\"{q_1}\" sessionqueries:ascending,descending,andrandom.Thisapproach\nASSISTANT:\"Theintenttitleis{r_1}.\" toself-consistencycheckingmethodcanalsobeimplementedwhen\nPreprint,WorkingPaper,Nov2024 Liuetal.\nFigure3:IllustrationofC-LARA:mergingLARAwithSelf-Consistencyeffectivelycombinesqueryaggregation,knowledgebase\nretrieval,andself-consistencymechanismtogeneratehigh-qualitypseudo-labelsformulti-turndialogues.Theself-consistency\nprocessimproveslabelingaccuracybyvalidatingintentpredictionsacrossdifferentpromptorderings.\nusingablack-boxLLM.Onlinechatlogsaresampledforpseudo- Market Lang. Intents Train(ST) Test(MT)\nlabeling,andonlythosehavingconsistentlabelsforall3runswill BR pt 316 66k 372\nbekeptfortraining. ID id 481 161k 1145\nMY en,ms 473 74k 1417\n3.2.1 HierarchicalTextClassification(HTC). M𝑐 isanensembleof\nPH en,fil 237 33k 189\nlabel-attentionencoderandahierarchical-awaretree-basedencoder\nSG en 360 76k 737\nwith3-layeredglobalandlocalintentclassifiers.\nTH th 359 60k 502\nThe label-attention encoder has one classifier head for each\nTW zh-tw 373 31k 353\nintentlayer.Eachclassifierheadhasonehiddenlinearlayerto\nVN vi 389 178k 525\nobtainthelayerintermediateoutput𝐿 𝑙,whichencodesthelayer\ninformation.Thislayerinformationwillbeutilisedintheinputof Table1:MultilingualdatasetstatisticsforSingleTurn(ST)\nthenextlayerclassifierhead. andMulti-Turn(MT).\nMKT Model 𝑟 𝑐 CL-Label Accuracy\n(cid:40)\n𝐻𝑊1+𝑏1, if𝑙 =1, SG NaiveConcat. - - 60.52%\n𝐿 𝑙 = (𝐻 ⊕𝑙 𝐿 𝑙−1𝑙 )𝑊 𝑙1+𝑏 𝑙1, if𝑙 >1, S SG\nG\nSele Lc lt ai mve a2C -o 7n Bcat. ✘- ✘- 5 56 6. .9 29 4%\n%\nwhere𝑊1 ∈R𝑑×𝑑 for𝑙 =1and𝑊1 ∈R2𝑑×𝑑 for𝑙 >1.𝑏1 ∈R𝑑 ,𝑙 SG Llama2-7B ✔ ✘ 61.33%\nisthelay𝑙 ernumber,⊕denotesten𝑙 sorconcatenation.Fin𝑙\nally,we\nSG Domain-Llama2-7B ✔ ✘ 63.23%\nobtainthelocallogits𝐻𝑙\nforeachlayerclassesbyusinganother\nID NaiveConcat. - - 60.61%\n𝑙𝑜𝑐𝑎𝑙\nID SelectiveConcat. - - 63.23%\nlinearlayer\nID Llama2-7B ✔ ✘ 49.96%\n𝐻 𝑙𝑙\n𝑜𝑐𝑎𝑙\n=𝐿\n𝑙\n·𝑊 𝑙2+𝑏 𝑙2,𝑊 𝑙2 ∈R𝑑×|I𝑙|,𝑏 𝑙2 ∈R|I𝑙| I ID\nD\nS Se ea aL LL LM M- -7 7B B- -c ch ha at\nt\n✔\n✔\n✔✘ 5 52 5. .4 09 2%\n%\nwhere|I𝑙|isthenumberofclassesinthelayer.\nTable2:PerformanceofLLMwithsymboltuningapproaches.\nHowever,thelabel-attentionmodelisunawareoftheoverallhier-\narchicalstructure.Therefore,weensembleitwithanothermethod. nodes.Thelogitsarethensplitbythenumberofclassesineach\nWerefertoHiTIN[26]fortheimplementationofastate-of-the-art layertoobtain𝐻𝑙 .\n𝑔𝑙𝑜𝑏𝑎𝑙\nHTCglobalapproach.Inthismethod,atreenetworkisconstructed Thefinalclassprobabilitiesforeachlayer𝑃 𝑙 isthenobtainedby:\nb saa gs ee sd ao rn et ph re ops aim gap tl ei dfie bd oto tr oi mgi -n ua pl it nax ao nn io sm omy os rt pru hc ist mure m, aa nn nd et rh ,e wm hie cs h- 𝑃 𝑙 =𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝐻 𝑙𝑙 𝑜𝑐𝑎𝑙 +𝐻 𝑔𝑙 𝑙𝑜𝑏𝑎𝑙)\ncomplementsthelabel-attentionmodelused.Theembeddingfor 4 Experiments\nleafnodesareobtainedbybroadcastingthetextrepresentation𝐻.\n4.1 Dataset\nAfterthetreeisomorphismnetworkpropagation,allembedding\nfromalllayersareaggregatedtoformsingleembedding,anda Thedatasetusedinourexperimentsisderivedfromtheconversa-\nclassificationlayerisusedtoobtainthelogits𝐻 𝑔𝑙𝑜𝑏𝑎𝑙 ofalltree tionhistoryofalargee-commerceplatform.Itincludesuserqueries\nBalancingAccuracyandEfficiencyinMulti-TurnIntentClassificationforLLM-PoweredDialogSystemsinProduction Preprint,WorkingPaper,Nov2024\ninthelocallanguagesofeightmarkets:Brazil(BR),Indonesia(ID), data,asshowninSection4.1.WeuseAdamWtofinetunetheHTC\nMalaysia(MY),Philippines(PH),Singapore(SG),Thailand(TH), withalearningrateof5e-6.AlltestsarerunonasingleNvidia\nTaiwan(TW),andVietnam(VN),asdetailedinTable1.Labeleddata V100GPUcardwith32GBofGPUmemory.\nweremanuallyannotatedbylocalcustomerserviceteams,with\nonlysamplesachievinglabelconsistencyacrossthreeindependent 4.4 Baselinesettings\ntaggersbeingselectedtoensurequality. Forafaircomparison,weadoptthreemethodsfine-tunedonHTC\nSingle-turntrainingdatacollectedoveryearsofbusinessop- model(M𝑐)asourglobalbaselinesacrosstwomethods:\nerationsformthebasisforsupervisedfine-tuningandin-context\n(1) Single-turnmethod:whereonlythelastqueryofasession\nlearning.Formulti-turnevaluation,realonlinesessionsareanno-\nisconsideredbyM𝑐;\ntatedbylocalcustomerserviceteams,withonlythelastquery\n(2) Naiveconcatenation:allqueriesareconcatenatedtogether\n𝑞 𝑛 labeledineachsessionQ.Forpreprocessing,weremovenoisy\nbeforebeingfedintoM𝑐;\nannotations,standardizeintents,andaugmentmulti-turnsessions\n(3) Selectiveconcatenation:whereaconcatenationselection\nusingdialoguestatetransitionprobabilitiesderivedfromchatlogs.\nmodelistrainedtoselectthemostsuitablehistoricalquery\nSymbolTuning.WeperformsymboltuningonLLMfortheSG\nwiththelastquerytoserveastheinputtoM𝑐.\nandIDdatasets,whereSGmainlyusesEnglishwhileIDusesBahasa\nIndonesia.Thetrainingdatacomprisesamixofexistingsingle-turn STonLLM. InSG,exceptLlama2-7B,wealsotriedtocontinuepre-\nsamplesandabout60ksemi-automaticallycraftedmulti-turnsam- trainingthebasemodelsonin-domaincorpustostrengthenthe\nplesaddedtoeachmarket.Someareobtainedbycleaningdataon languageunderstandingoflocallanguagesandthecorresponding\nonlinechatlogstoidentifymoreaccurateintentsusinganLLM slangused,ashumansusuallyconversewiththechatbotinanon-\nwithafew-shotsofthechain-of-thoughtprompt.Therestarecon- formalway.WetermthedomainspecificbasemodelasDomain-\nstructedbycombiningseveraldialoguessampledfromtheexisting Llama2-7B.InID,weswitchedLlama2-7BmodeltoSeaLLM-7B-\nsingle-turntrainingdatasettoformonesession.Thetransitionof chat[11]whichwasintroducedspecificallyforlanguagesinSouth\nintentsinasessioniscalculatedfromtheonlinechatlog. EastAsia.\nHTCwithC-LARA.70kofonlinechatlogsaresampledforpseudo- TheSTapproachwasadaptedforsupervisedintentrecognition\nlabelling.Afterself-consistencychecking,around12%ofthedata usingcompressedgenerationtargets(𝑟 𝑐)andcross-linguallabels\nyieldinconsistentresultsandarediscardedfromtraining.1.5ksam- (CL_label).Theseadjustmentsoptimizedperformancebysimplify-\nplesaresplitfromthepseudo-labeleddatatoserveasthevalidation ingthegenerativetaskwhilemaintainingsemanticintegrity.Com-\nsetforearlystopping. parisonswithbaselinemethodsinTable2showthatSTachieves\ncompetitiveresultsinEnglishmarketsbutfaceschallengesinnon-\n4.2 Metrics Englishsettingsduetolimitationsinpre-trainingforlow-resource\nTheprimaryevaluationmetricistheaccuracyofpredictedlabelsfor languages.\nthefinalquery𝑞 𝑛ineachconversationsessionQ.Metricsaccount-\nHTCwithC-LARA. ThisexperimentusesVicuna-13Basourbase\ningforclassimbalancewerenotconsidered,asthesampledsessions\nmodelforpseudo-labelingwithinLARAandC-LARA.Wedesigned\nreflectthedistributionofonlinetrafficacrossintents,providinga\nthreepipelineswithfourprompttemplatesin[9]todemonstrate\nrealisticapproximationofliveperformance.\nthatusingC-LARAforpseudo-labelingcaneffectivelyimprovethe\nHTCmodel’sperformanceinmulti-turnclassificationtasks.The\n4.3 ImplementationDetails\ndetailedintroductionislistedasfollows:\nSymbolTuningonLLM. FastChatframeworkisusedtofine-tune • LARA:UsingLARAdirectlyasaclassifier.\n7BLLMsusingLoRAmethodontheir𝑞_𝑝𝑟𝑜𝑗,𝑣_𝑝𝑟𝑜𝑗,𝑜_𝑝𝑟𝑜𝑗,and\n• LARA-PL:UsingLARAasanaivepseudo-labelingtooland\n𝑘_𝑝𝑟𝑜𝑗 moduleswithalearningrateof2e-5over10epochs.The\nfine-turnHTCmodelwithgenerateddata.\n7BmodelsusedareLlama-2-7B(forSG)andSeaLLM-7B-chat(for • C-LARA:UseingC-LARAtofilteroutthenoiseandgenerate\nID) on Hugging Face. Before the models are fine-tuned on the\nhigh-qualitydatatofine-tunetheHTCmodel.\nmulti-turnintentrecognitiontask,theyarefurtherpre-trainedon\nShareGPTdatasetwiththesamesettingabove,andtheweights 4.5 OfflineExperiments\narethenmerged.Forthesakeofsimplicity,wewillrefertothe\nSymbolTuningonLLM. Table2illustratestheeffectivenessofSym-\nLLMs further pre-trained on ShareGPT dataset as base models.\nbolTuning(ST)onLLMs.Compressingthegenerationtarget𝑟\nDuringtrainingforintentclassificationtask,lossiscalculatedon\nreducestaskcomplexityandimprovesaccuracyby5.09%intheSG\nallthemodeloutputincludingthoseafterhistoryqueries.During\nmarket.Thiscompressionalsomitigateshallucination,reducing\ninference,greedydecodingstrategyisusedtogeneratethetarget\n𝑟 part,theprefix\"Theintenttitleis\"isnotgeneratedbutinstead instancesofunmatchedgeneratedlabelsfrom2.5%to0%.\nInterestingly,thistechniquealsostoppedLLMhallucination,i.e.\nappendedattheendoftheprompt.Whenthegeneratedlabelhas\nnoexactmatchwithany𝑟 inI,gestaltstringmatchingisusedto\ngeneratinglabelwithnomatchintheI.Thehallucinationrate\nwithoutusingcompressed𝑟 isabout2.5%.InID,whichisanon-\nfindtheclosestone.\nEnglishmarket,wefindthatcross-linguallabelwhichchanges\nHTCwithC-LARA. Thein-houseHierarchicalTextClassification thegenerationtargettoEnglishratherthaninthelocallanguage\n(HTC)modelisaBERT-basedmodelfine-tunedusingthecombina- alsoimprovedtheperformanceby2.53%.Usingdifferentbase\ntionofthepseudo-labeledmulti-turndataandexistingsingle-turn modelswhichweretrainedspecificallyonthein-domaincorpusor\nPreprint,WorkingPaper,Nov2024 Liuetal.\nPipeline Model Prompt Self-Consistency BR ID MY PH SG TH TW VN avg\nFine-tuning Single-turn - - 30.98% 52.14% 56.81% 40.21% 51.13% 52.99% 58.07% 65.90% 53.76%\nFine-tuning NaiveConcat. - - 50.81% 60.61% 57.02% 47.62% 60.52% 56.97% 65.44% 76.95% 60.08%\nFine-tuning SelectiveConcat. - - 52.69% 63.23% 60.20% 51.32% 56.99% 57.77% 64.02% 74.10% 60.97%\nLARA Vicuna-13B P ✘ 52.69% 61.48% 65.42% 54.50% 65.26% 60.96% 67.14% 77.90% 64.18%\nC-LARA Vicuna-13B P ✔ 55.38% 63.58% 65.00% 54.50% 66.21% 63.75% 71.10% 79.24% 65.52%\nLARA Vicuna-13B P𝑠𝑦𝑚𝑏𝑜𝑙𝑖𝑐 ✘ 51.88% 60.00% 64.57% 53.97% 65.26% 58.96% 65.44% 74.67% 62.92%\nC-LARA Vicuna-13B P𝑠𝑦𝑚𝑏𝑜𝑙𝑖𝑐 ✔ 54.57% 62.62% 65.56% 50.79% 66.76% 62.95% 69.97% 76.76% 64.94%\nLARA Vicuna-13B P𝑝𝑟𝑒𝑝𝑒𝑛𝑑 ✘ 54.03% 61.75% 64.50% 53.44% 65.94% 61.55% 66.86% 75.81% 63.97%\nC-LARA Vicuna-13B P𝑝𝑟𝑒𝑝𝑒𝑛𝑑 ✔ 53.76% 63.84% 65.70% 52.91% 68.11% 63.15% 69.97% 78.48% 65.65%\nLARA Vicuna-13B P𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑 ✘ 55.65% 62.88% 64.71% 55.03% 65.40% 61.95% 66.86% 78.10% 64.64%\nLARA-PL Vicuna-13B P𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑 ✘ 55.91% 64.19% 64.43% 49.21% 66.49% 61.95% 69.41% 81.14% 65.29%\nC-LARA Vicuna-13B P𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑 ✔ 55.91% 65.33% 66.27% 51.85% 67.16% 63.35% 72.80% 78.86% 66.35%\nTable 3: Performance of C-LARA compared to baselines, the average here is weighted on the number of test samples in\neachmarket.TheresultsillustratethatC-LARAwithformattedpromptsachievesthebestaverageaccuracy(66.35%)across\nallmarkets.Theresultsvalidateourapproach’seffectivenessinbothEnglishandnon-Englishmarkets,withsignificant\nimprovementsoverbaselinemethods.\nforthelocallanguagealsoprovestobeuseful.Domain-Llama2-7B Deployedonasingle32GBV100GPU,theSymbolTuning(ST)\nimprovestheperformanceby1.90%inSGwhileSeaLLM-7B-chat approachachievedanaveragelatencyof170msat0.5QPSinthe\nimproves the performance by 2.53%in ID compared toLlama2- SGmarket.Incontrast,C-LARAmodelsconvertedtoONNXformat\n7B.WhiletheSTapproachoutperformsthebaselinesinEnglish (1.1GBpermodel)achievedanaveragelatencyof80msat1QPS\nmarket,itstillleavesalottobedesiredinnon-Englishmarket.This on an 8-core CPU machine with 16GB memory, demonstrating\nphenomenonmayariseasaresultoftheSTapproachemployedfor superiorscalabilityandcost-efficiency.\nnon-dominantlanguagesduringpre-training,whichnecessitatesa\ngreaterquantityorhigherqualityofdatatoachievesatisfactory C-LARA. WedeployC-LARAacrossalleightmarkets.Themodels\nperformanceinataskthatwasnotincludedinthepre-training werefirstconvertedtoONNXformat,reducingtheirsizeto1.1GB.\nphase.Thisisparticularlytruewhenthemodellacksknowledge Deployedonan8-coreCPUmachinewith16GBmemory,C-LARA\npertainingtodomainintents. achievedanaveragelatencyof80msat1QPS,whichislessthan\nhalfthelatencyoftheSTonLLMmethod.Thisdeploymentsignifi-\nPseudo-labeling using C-LARA. As demonstrated in Table 3, C- cantlyreducedbothcostsandcomplexity,makingitmorescalable\nLARAimprovespseudo-labelqualitythroughself-consistencyval- forindustrialapplications.Duetoitsversatility,anAuto-Training\nidation,resultingina1.06%performancegainoverLARA.This Portal(ATP)ecosystemisbuiltaroundtheLARA-PLmethod(Fig.\nvalidationprocessidentifiesandremovesapproximately12%of 4).ATPenablesseamlessandcontinuousimprovementsforthe\ninconsistentsamples,ensuringhigh-qualitysyntheticlabels.While chatbot’smulti-turnintentrecognitionsystem.Usingonlinechat\nthisapproachrequiresadditionalofflinetrainingresources,itsignif- logs,localoperationsteamscanupdatetheKnowledgeBase(KB)\nicantlylowersdeploymentcostsbyrelyingonasingle,lightweight byaddingnewintentsandcraftingexamplequeries.Subsequently,\nclassificationmodel. theycantriggerC-LARAforpseudo-labelingmulti-turnchatlogs,\nThismostprobablycanbeattributedtotheadvantagesofthedis- generatingdatatotrainlightweightmodels.Oncetrainingiscom-\ncriminativemethodinclassificationtasks,astrainingprocessalso plete,themodelsaredeployedthroughtheportalforonlineA/B\nexposedthemodeltothecomprehensivehighqualitysingle-turn testing,creatinganiterativecycleofimprovement.Forfaircom-\ndataset.Besides,thepre-trainedmodelusedforM𝑐 wasalsopre- parisons,theversionoftheKB(intentsandsingle-turntraining\ntrainedspecificallyonthein-domainmulti-lingualcorpus,making data)waskeptconsistentacrosscontrolandtestgroups.\nitastrongsuitforourmultilinguale-commercesetting.C-LARA’s\nintegrationofself-consistencywithinthepseudo-labelingpipeline\n4.7 OnlinePerformance\nsignificantlyenhancesthequalityofsyntheticlabels,resultingina\n1.06%improvementinperformance,asindicatedinthelastrowof Weleveragethefollowingtwometrics:\nTable3.WhentheLLMlacksconfidenceinitsICLresponses,minor\nchangesintheinputpromptcansignificantlyaltertheoutput.This (1) Resolutionrate(RR)whichismeasuredbytherateofuser\nmethodeffectivelyidentifiespotentialinaccuraciesinICLoutputs completingtheanswerflow,nottransferringtoliveagent,\nforblack-boxmodelswheredirectoutputscoresareunavailable. andnotgivingbadratingtotheanswer.\nMost importantly, this approach, while requiring longer offline (2) CustomerServiceSatisfaction(SCSAT)whereuserswill\ntrainingtime,significantlyreducesdeploymentcoststojustone beaskedabouttheirsatisfactiontowardsourchatbotfor\nsmallclassificationmodel. chatbot only sessions (no intervention from live agents).\nThescoreiscalculatedby#goodratedsessions/(#goodrated\nsessions+#badratedsessions).\n4.6 OnlineDeploymentEvaluation\nSTonLLM.. UsingtheLMDeployframework,LARAweightswere Weusetheselectiveconcatenationmethodasthebaselinefor\nmergedwiththe7Bbasemodel,enablingfasterinferencetimes. allexperiments,withpairedt-testtoevaluatestatisticalsignificance.\nBalancingAccuracyandEfficiencyinMulti-TurnIntentClassificationforLLM-PoweredDialogSystemsinProduction Preprint,WorkingPaper,Nov2024\n5.1.2 ShorterTargetLength. Theapproachofcompressing𝑟 was\ninspiredby[21].Hence,wealsotriedtoreplace𝑟swithcompletely\nmeaninglesssymbols,whilekeepingthegenerationprefixof“The\nintenttitleis\".Compressingtargetlabelstopurelysymbolicrep-\nresentations results in a significant 8.91% performance drop, as\nshowninTable7.Thishighlightstheimportanceofpreserving\nsemanticrichnessintargetlabelsforgenerativefine-tuning.Ef-\nfectivecompressionmethodsmustretainkeyinformationfrom\ntheoriginallabelstoavoidlossinclassificationaccuracy..Thus,\nwhencompressing𝑟s,itisimportanttochooseamethodthatcan\npreservetheinformationinoriginal𝑟sasmuchaspossible.\nFigure 4: Online Deployment of Multi-turn Intent Classi-\n5.2 ImpactofSelf-consistencyinMTIC\nfication model demonstrates our production architecture\nintegratingC-LARAforautomatedtrainingdatageneration. Using our multi-turn test sets, we evaluate the performance of\nThesystemhandlesreal-timeinferencewhilecontinuously MTICwithandwithoutself-consistencychecking.Weremovethe\nimprovingthroughautomatedtraining. sampleswithinconsistentoutputsandcalculatetheprecisionof\ntheremainingsamples.Onaverage,12%oftestsampleswillbe\nSTonLLM. IntheSGmarket,STonLLMswasdeployedto50%of\nremovedineachmarket.Incorporatingself-consistencychecking\nonlinetrafficforthreeweeks,yieldingapproximately14kchatbot\nintoMTICevaluationsimprovesaccuracyacrossallpromptvaria-\nsessionspergroup.Thetestgroupexhibiteda+2.19%improvement\ntions,asshowninTable4.Byremovingapproximately12%oftest\ninCustomerServiceSatisfaction(SCSAT),butResolutionRate(RR)\nsampleswithinconsistentoutputs,thismethodeffectivelyfilters\ndeclinedby-0.11%.Neitherresultwasstatisticallysignificant,indi-\nouterroneouspredictions,ensuringhigher-qualitypseudo-labels\ncatinglimitedbenefitsfromSTgivenitsresource-intensivenature.\nandmorereliableresults.Thisensuresthequalityofpseudo-labels.\nC-LARA. ForC-LARA,onlythemulti-turndialoguemodelwasre-\nplaced,whilesingle-turnmodelsremainedunchanged.Aggregated 5.3 EffectofModelSize\nresultsfromover108kchatbotsessionspergroupshowedstatis-\nForfaircomparisonbetweenLLMSTandC-LARA,weusevicuna-\nticallysignificantimprovements:ResolutionRate(RR)increased 7b-v1.5asthebasemodelwithpromptPandP𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑,without\nby+0.78%andCustomerServiceSatisfaction(SCSAT)by+1.39%\nself-consistencychecking.TheresultsofLLMSTmethodaretaken\n(p-value<0.05).Thesegainstranslatetooverallsessionimprove-\nfromthebestofeachmarketreportedinthispaper,includingbase\nmentsofRR+0.47%andSCSAT+0.84%,asmulti-turndialogues\nmodels pre-trained on in-domain corpus, so it should have the\ncomprise60.60%oftotalsessions.\nadvantageoverVicuna-7B-v1.5.Table5comparesC-LARAand\nFurthermore,addingpseudo-labeledmulti-turndataenhanced\nLLMSTusingmodelsofthesamesize(Vicuna-7B-v1.5)without\nsingle-turnintentrecognition.Substitutingsingle-turndialogue\nself-consistencychecking.Despitethesimplerpipeline,C-LARA\nmodelswithC-LARAmodelsyieldedanRRimprovementof+0.06%\nconsistentlyoutperformsLLMST,avoidingthecomplexityofmulti-\nandastatisticallysignificantSCSATincreaseof+0.27%.\nturn sample crafting. However, smaller models exhibit reduced\ninstruction-followingcapabilities,asdemonstratedbythelower\n5 AblationStudy\nperformanceof P𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑 comparedto P.Oneinterestingob-\n5.1 EffectofTargetLength servation here is that the performance of C-LARA when using\nWeinvestigatehowtheamountofinformationinSTgeneration\nP𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑 isnowlowerthanP.LLMsofsmallersizecouldbe\nweakerininstructionfollowing,andinthissensethesemantic\ntargetaffectstheintentrecognitionperformanceusingtworather\nmeaningofthelabelsindemonstrationsaremorecritical.Prepend-\nextremeapproachesandtheirconversationsemanticfluidity.\ningmeaninglesscharactersbeforelabelscannegativelyaffectthe\n5.1.1 LongerTargetLength. Toachievethis,themodelistrained understandingoflabelsforsmallerLLMs.\ntosummarizeallqueriesinQbeforeoutputtingthetarget𝑟.For\ninstance,thenewoutputformatofmodelwillbe“Youareask- 6 RelatedWork\ningabout{𝑠𝑢𝑚𝑚𝑎𝑟𝑦}.So,theintenttitleis{𝑟 𝑛}\".Therationaleis\n6.1 SyntheticDataGeneration\ntoutilizethesummarizationabilityofLLMstobetterunderstand\nthe context. For our training data, the summaries are obtained Thescarcityofannotateddialoguedata,particularlyinlow-resource\nbypromptingtheoriginalLLMbackbonesinazero-shotmanner. languages,hasdrivenresearchintosyntheticdatageneration.Borisov\nWechosethisoverincreasingthelengthof𝑟 staticallytoimpose etal.[1]proposedamethodleveragingauto-regressivegenerative\nmoreinformationonthemodel’sgenerationtarget.Table6demon- modelstocreaterealistictabulardatasets,highlightingtheirutil-\nstratestheimpactofincreasingthetargetlengthinSymbolTuning ityindataaugmentation.Similarly,Lietal.[6]demonstratedthat\n(ST).Extendingthegenerationtargettoincludequerysummaries syntheticdatageneratedbyLLMscansignificantlyenhancemodel\ndecreasesperformanceby3.82%.Whilethisapproachenhances performanceinclassificationtasks.Additionally,Tangetal.[18]uti-\nsemanticcoherence,excessiveinformationoverloadsthemodel, lizedsyntheticdatatocraftchallengingexamplesforfact-checking,\nreducingitsabilitytofocusonthecoreintentclassificationtask. improvingthefactualaccuracyofLLMoutputs.\nPreprint,WorkingPaper,Nov2024 Liuetal.\nPrompt Self-Consistency BR ID MY PH SG TH TW VN avg\nP ✘ 52.69% 61.48% 65.42% 54.50% 65.26% 60.96% 67.14% 77.90% 64.18%\nP ✔ 58.59% 68.13% 69.93% 56.44% 69.58% 66.75% 71.30% 81.14% 69.11%\nP𝑠𝑦𝑚𝑏𝑜𝑙𝑖𝑐 ✘ 51.88% 60.00% 64.57% 53.97% 65.26% 58.96% 65.44% 74.67% 62.92%\nP𝑠𝑦𝑚𝑏𝑜𝑙𝑖𝑐 ✔ 56.63% 64.71% 68.48% 55.19% 68.77% 65.59% 71.61% 78.02% 67.27%\nP𝑝𝑟𝑒𝑝𝑒𝑛𝑑 ✘ 54.03% 61.75% 64.50% 53.44% 65.94% 61.55% 66.86% 75.81% 63.97%\nP𝑝𝑟𝑒𝑝𝑒𝑛𝑑 ✔ 59.49% 66.08% 68.60% 55.90% 68.85% 68.19% 71.79% 81.36% 68.43%\nP𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑 ✘ 55.65% 62.88% 64.71% 55.03% 65.40% 61.95% 66.86% 78.10% 64.64%\nP𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑 ✔ 59.24% 67.01% 69.47% 56.79% 68.81% 68.69% 72.35% 82.93% 69.12%\nTable4:PrecisionofC-LARAvariantsafterfilteringinconsistentpredictionsdemonstratestheeffectivenessofself-consistency\ncheckingacrossdifferentprompttypes(P,𝑃 𝑠𝑦𝑚𝑏𝑜𝑙𝑖𝑐,𝑃 𝑝𝑟𝑒𝑝𝑒𝑛𝑑,𝑃 𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑).Acrossallprompttypes,accuracyimprovesby\napproximately4-5%,withP𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑 achievingthehighestprecision(69.12%).Theseresultsvalidatetherobustnessofself-\nconsistencyasafilteringstrategy.\nMethod Prompt ID SG avg 6.3 LLMontextclassification\nLLMST - 58.17% 63.23% 60.15% RecentstudieshaveexploredtheapplicabilityofLLMsacrossvari-\nC-LARA P 60.44% 64.31% 61.96% ousdomains.ChaeandDavidson[3]investigatedLLMsforsocio-\nC-LARA P𝑓𝑜𝑟𝑚𝑎𝑡𝑡𝑒𝑑 59.83% 64.04% 61.48% logicaltextclassification,demonstratingtheirpotentialinsocial\nscienceresearch.Infinancialintentdetection,Loukasetal.[10]\nTable5:ResultsofLARAusing7BLLM.\nanalyzedthetrade-offsbetweenperformanceandcostwhenus-\ningLLMsfortextclassification.Liuetal.[8]employedGPT-4oto\nMKT Model LongerTargetLength Accuracy performzero-shotclassificationonmulti-levelsemi-structuredtext\nSG Llama2-7B ✘ 54.82% withretrievalaugmentation.Weietal.[20]highlightedthebenefits\nSG Llama2-7B ✔ 51.02% offine-tuningLLMsondomain-specificdatasets,improvingperfor-\nmanceinlegaldocumentreview.Weietal.[21]introducedsymbol\nTable6:EffectofLongerTargetLengthonLLMSTClassifi-\ntuning,wherenaturallanguagelabelswerereplacedwithunrelated\ncationPerformance:lengtheningtargetsresultsinaperfor-\nsymbolsduringfine-tuningtoenhanceclassification.Ourwork\nmancedropby3.82%.\ndiffersbycompressinglongerintentlabelsintosemanticallymean-\ningfulphrases,enablingeasiergenerationandimprovingaccuracy\nMKT Model ShorterTargetLength Accuracy fortaskswithalargenumberofclasses.\nID SeaLLM-7B ✘ 55.02%\nID SeaLLM-7B ✔ 46.11%\nTable7:EffectofShorterTargetLengthonLLMSTClassifi-\ncationPerformance:reducingsemanticallyrichtargetsinto 7 Conclusion\nsymbolscostsadrasticperformancedropby8.91%. Multi-turnintentclassificationplaysacriticalroleinmoderndia-\nloguesystems.Unliketypicalclassificationtasks,real-worldintent\nclassificationofteninvolvesvaryingintentlengths,posingunique\n6.2 ModelingMulti-turnDialogueContext\nchallenges.Inthiswork,weintroducedSymbolTuningtofine-tune\nMulti-turndialoguemodelingisessentialfordialogueunderstand- largelanguagemodels(LLMs)withcompressedintents.Ourexper-\ningtasks.EarlymethodsusedbidirectionalcontextualLSTMs[4] imentsdemonstratedthatshorteningintentsimprovedaccuracy\ntocapturecontext-awareutterancerepresentationsfortaskssuch by5.09%comparedtousingoriginalintents.\nasMultiWOZintentclassification[2].Otherapproaches,suchas Additionally, we proposed C-LARA, an augmentation-based\nmulti-channelgraphconvolutionalnetworks,wereappliedtoquery pipelineforgeneratinghigh-qualitymulti-turndatasetsusingself-\nclassificationinE-commerce[25]. consistencyvalidation.Trainingsmallermodelswithpseudo-labeled\nRecent advancements leverage pre-trained language models datageneratedbyC-LARAyieldeda1.06%averageperformance\n(PLMs)assentenceencoders[17],particularlyforemotionrecogni- improvement.Empirically,C-LARAsignificantlyreducesannota-\ntioninconversations(ERC).Forinstance,LeeandLee[5]encoded tioncostsbyautomatingpseudo-labelingbasedontheuser’slatest\nbothcontextandspeakermemoryusingPLMs,whileQinetal.[14] utteranceindialoguehistory,improvingmodeliterationefficiency.\nincorporatedmulti-turninformationfromutterancesanddialogue Furthermore, training smaller models offers computational effi-\nstructurethroughfine-tuning.Despitetheireffectiveness,these ciency,enablingscalabledeploymentandonlineinference.\nmethodsdependheavilyonmulti-turntrainingdatasets,whichare FutureWork.Movingforward,weaimtoincorporatefeatures\ndifficulttoacquireinreal-worlde-commercesettings[7].Incon- suchasuserprofilesandorderhistoryintoC-LARAtosupportmore\ntrast,ourapproachemploysLLMswithinanaugmentation-based diversedialoguetasks.Wealsoplantoexplorecross-lingualtransfer\npipelinetogeneratemulti-turndata,enablingzero-shotintentclas- andadvancedtokenizationtechniquestoenhanceperformancein\nsificationusingsmallermodels. low-resourcelanguages.\nBalancingAccuracyandEfficiencyinMulti-TurnIntentClassificationforLLM-PoweredDialogSystemsinProduction Preprint,WorkingPaper,Nov2024\nReferences\n[22] H.Weld,X.Huang,S.Long,J.Poon,andS.C.Han.2021. Asurveyofjoint\n[1] VadimBorisov,KathrinSeßler,TobiasLeemann,MartinPawelczyk,andGjergji intentdetectionandslot-fillingmodelsinnaturallanguageunderstanding.\nKasneci.2022.LanguageModelsAreRealisticTabularDataGenerators.ArXiv arXiv:2101.08091[cs.CL]\nabs/2210.06280(2022). [23] Ting-WeiWu,RuolinSu,andBiing-HwangJuang.2021. AContext-Aware\n[2] PawełBudzianowski,Tsung-HsienWen,Bo-HsiangTseng,InigoCasanueva, Hierarchical BERT Fusion Network for Multi-turn Dialog Act Detection.\nStefanUltes,OsmanRamadan,andMilicaGašić.2018.MultiWOZ–alarge-scale arXiv:2109.01267[cs.CL]\nmulti-domainwizard-of-ozdatasetfortask-orienteddialoguemodelling.arXiv [24] PuyangXuandRuhiSarikaya.2014.Contextualdomainclassificationinspoken\npreprintarXiv:1810.00278(2018). languageunderstandingsystemsusingrecurrentneuralnetwork.In2014IEEE\n[3] YoungjinChaeandThomasDavidson.2023. Largelanguagemodelsfortext InternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP).\nclassification:Fromzero-shotlearningtofine-tuning.OpenScienceFoundation 136–140. https://doi.org/10.1109/ICASSP.2014.6853573\n(2023). [25] ChunyuanYuan,MingPang,ZhengFang,XueJiang,ChangpingPeng,and\n[4] DeepanwayGhosal,NavonilMajumder,RadaMihalcea,andSoujanyaPoria. ZhangangLin.2024. ASemi-supervisedMulti-channelGraphConvolutional\n2021. Exploringtheroleofcontextinutterance-levelemotion,actandintent NetworkforQueryClassificationinE-commerce.InCompanionProceedingsof\nclassificationinconversations:Anempiricalstudy.InFindingsoftheAssociation theACMonWebConference2024.56–64.\nforComputationalLinguistics:ACL-IJCNLP2021.1435–1449. [26] HeZhu,ChongZhang,JunjieHuang,JunranWu,andKeXu.2023. HiTIN:\n[5] JoosungLeeandWooinLee.2022.CoMPM:ContextModelingwithSpeaker’s Hierarchy-awareTreeIsomorphismNetworkforHierarchicalTextClassification.\nPre-trainedMemoryTrackingforEmotionRecognitioninConversation.In InAnnualMeetingoftheAssociationforComputationalLinguistics. https://api.\nProceedingsofthe2022ConferenceoftheNorthAmericanChapteroftheAssociation semanticscholar.org/CorpusID:258865236\nforComputationalLinguistics:HumanLanguageTechnologies.5669–5679.\n[6] ZhuoyanLi,HangxiaoZhu,ZhuoranLu,andMingYin.2023. SyntheticData\nGenerationwithLargeLanguageModelsforTextClassification:Potentialand\nLimitations.Proceedingsofthe2023ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing(EMNLP)(2023).\n[7] JunhuaLiuandBinFu.2024.ResponsibleMultilingualLargeLanguageModels:\nASurveyofDevelopment,Applications,andSocietalImpact.ArXiv(2024).\n[8] JunhuaLiu,KwanHuiLim,andRoyKa-WeiLee.2024.TowardsObjectiveand\nUnbiasedDecisionAssessmentswithLLM-EnhancedHierarchicalAttention\nNetworks.arXivpreprintarXiv:2411.08504(2024).\n[9] JunhuaLiu,YongKeatTan,BinFu,andKwanHuiLim.2024.LARA:Linguistic-\nAdaptiveRetrieval-AugmentationforMulti-TurnIntentClassification.Proceed-\ningsoftheEmpiricalMethodsinNaturalLanguageProcessing(2024).\n[10] LefterisLoukas,IliasStogiannidis,OdysseasDiamantopoulos,ProdromosMalaka-\nsiotis,andStavrosVassos.2023. Makingllmswortheverypenny:Resource-\nlimitedtextclassificationinbanking.InProceedingsoftheFourthACMInterna-\ntionalConferenceonAIinFinance.392–400.\n[11] Xuan-PhiNguyen,WenxuanZhang,XinLi,MahaniAljunied,ZhiqiangHu,\nChenhuiShen,YewKenChia,XingxuanLi,JianyuWang,QingyuTan,Liying\nCheng,GuanzhengChen,YueDeng,SenYang,ChaoqunLiu,HangZhang,and\nLidongBing.2024. SeaLLMs–LargeLanguageModelsforSoutheastAsia.\narXiv:2312.00738[cs.CL] https://arxiv.org/abs/2312.00738\n[12] PavelPřibáň,JakubŠmíd,JosefSteinberger,andAdamMištera.2024.Acompar-\nativestudyofcross-lingualsentimentanalysis.ExpertSystemswithApplications\n247(2024),123247.\n[13] LiboQin,WanxiangChe,YangmingLi,MinghengNi,andTingLiu.2020.Dcr-net:\nAdeepco-interactiverelationnetworkforjointdialogactrecognitionandsenti-\nmentclassification.InProceedingsoftheAAAIconferenceonartificialintelligence,\nVol.34.8665–8672.\n[14] XiangyuQin,ZhiyuWu,TingtingZhang,YanranLi,JianLuan,BinWang,Li\nWang,andJinshiCui.2023. Bert-erc:Fine-tuningbertisenoughforemotion\nrecognitioninconversation.InProceedingsoftheAAAIConferenceonArtificial\nIntelligence,Vol.37.13492–13500.\n[15] ChenQu,LiuYang,W.BruceCroft,YongfengZhang,JohanneR.Trippas,and\nMinghuiQiu.2019.UserIntentPredictioninInformation-seekingConversations.\nInProceedingsofthe2019ConferenceonHumanInformationInteractionand\nRetrieval(CHIIR’19).ACM. https://doi.org/10.1145/3295750.3298924\n[16] PhillipRust,JonasPfeiffer,IvanVulić,SebastianRuder,andIrynaGurevych.2021.\nHowGoodisYourTokenizer?OntheMonolingualPerformanceofMultilingual\nLanguageModels.InProceedingsofthe59thAnnualMeetingoftheAssociationfor\nComputationalLinguisticsandthe11thInternationalJointConferenceonNatural\nLanguageProcessing(Volume1:LongPapers).3118–3135.\n[17] WeizhouShen,SiyueWu,YunyiYang,andXiaojunQuan.2021.DirectedAcyclic\nGraphNetworkforConversationalEmotionRecognition.InProceedingsofthe\n59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11th\nInternationalJointConferenceonNaturalLanguageProcessing(Volume1:Long\nPapers).1551–1560.\n[18] LiyanTang,PhilippeLaban,andGregDurrett.2024.MiniCheck:EfficientFact-\nCheckingofLLMsonGroundingDocuments.ArXivabs/2404.10774(2024).\n[19] ZhiqiangWang,YiranPang,andYanbinLin.2024.SmartExpertSystem:Large\nLanguageModelsasTextClassifiers.arXivpreprintarXiv:2405.10523(2024).\n[20] FushengWei,RobertKeeling,NathanielHuber-Fliflet,JianpingZhang,Adam\nDabrowski,JingchaoYang,QiangMao,andHanQin.2023.Empiricalstudyof\nLLMfine-tuningfortextclassificationinlegaldocumentreview.In2023IEEE\nInternationalConferenceonBigData(BigData).IEEE,2786–2792.\n[21] JerryWei,LeHou,AndrewLampinen,XiangningChen,DaHuang,YiTay,Xinyun\nChen,YifengLu,DennyZhou,TengyuMa,andQuocV.Le.2023.Symboltuning\nimprovesin-contextlearninginlanguagemodels. arXiv:2305.08298[cs.CL]\nhttps://arxiv.org/abs/2305.08298",
    "pdf_filename": "Balancing_Accuracy_and_Efficiency_in_Multi-Turn_Intent_Classification_for_LLM-Powered_Dialog_Systems.pdf"
}