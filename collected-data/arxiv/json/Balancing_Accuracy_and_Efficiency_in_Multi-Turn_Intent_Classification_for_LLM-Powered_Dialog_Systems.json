{
    "title": "Balancing Accuracy and Efficiency in Multi-Turn Intent",
    "abstract": "Accuratemulti-turnintentclassificationisessentialforadvancing conversationalAIsystems.However,challengessuchasthescarcity ofcomprehensivedatasetsandthecomplexityofcontextualdepen- denciesacrossdialogueturnshinderprogress.Thispaperpresents twonovelapproachesleveragingLargeLanguageModels(LLMs) toenhancescalabilityandreducelatencyinproductiondialogue systems.First,weintroduceSymbolTuning,whichsimplifiesintent labelstoreducetaskcomplexityandimproveperformanceinmulti- turndialogues.Second,weproposeC-LARA(Consistency-aware, LinguisticsAdaptiveRetrievalAugmentation),aframeworkthat employsLLMsfordataaugmentationandpseudo-labelingtogener- atesyntheticmulti-turndialogues.Theseenricheddatasetsareused tofine-tuneasmall,efficientmodelsuitablefordeployment.Exper- Figure1:Comparisonofinstructiontuningandsymboltun- imentsconductedonmultilingualdialoguedatasetsdemonstrate ing.Simplifyingverboseintentlabels(e.g.,â€œRequesttoCan- significantimprovementsinclassificationaccuracyandresource celOrderâ€â†’â€œCancelOrderâ€)reducesredundancy,enhanc- efficiency.Ourmethodsenhancemulti-turnintentclassification ingLLMclassificationperformanceby5.09%,addressingkey accuracyby5.09%,reduceannotationcostsby40%,andenablescal- challengesinproductionintentclassification. abledeploymentinlow-resourcemultilingualindustrialsystems, 1 Introduction highlightingtheirpracticalityandimpact. Dialoguesystemsarecriticalforautomatinginteractionsbetween customersandagents,streamliningcommunicationandenhanc- CCSConcepts inguserexperience.Theyplayapivotalroleininternationale- â€¢Informationsystemsâ†’Languagemodels;Questionanswer- commerce platforms, addressing the increasing demand for in- ing;â€¢Computingmethodologiesâ†’Intelligentagents. stantaneousandefficientcustomerservice.Intentclassification, afundamentalaspectofnaturallanguageunderstandingindia- loguesystems,involvesidentifyingusersâ€™goalsfromtheirinputs,",
    "body": "Balancing Accuracy and Efficiency in Multi-Turn Intent\nClassification for LLM-Powered Dialog Systems in Production\nJunhuaLiu1,3,âˆ— ,YongKeatTan2,âˆ— ,BinFu2,â€  ,KwanHuiLim3\n1ForthAI\n2Shopee\n3SingaporeUniversityofTechnologyandDesign\nSingapore\nAbstract\nAccuratemulti-turnintentclassificationisessentialforadvancing\nconversationalAIsystems.However,challengessuchasthescarcity\nofcomprehensivedatasetsandthecomplexityofcontextualdepen-\ndenciesacrossdialogueturnshinderprogress.Thispaperpresents\ntwonovelapproachesleveragingLargeLanguageModels(LLMs)\ntoenhancescalabilityandreducelatencyinproductiondialogue\nsystems.First,weintroduceSymbolTuning,whichsimplifiesintent\nlabelstoreducetaskcomplexityandimproveperformanceinmulti-\nturndialogues.Second,weproposeC-LARA(Consistency-aware,\nLinguisticsAdaptiveRetrievalAugmentation),aframeworkthat\nemploysLLMsfordataaugmentationandpseudo-labelingtogener-\natesyntheticmulti-turndialogues.Theseenricheddatasetsareused\ntofine-tuneasmall,efficientmodelsuitablefordeployment.Exper- Figure1:Comparisonofinstructiontuningandsymboltun-\nimentsconductedonmultilingualdialoguedatasetsdemonstrate ing.Simplifyingverboseintentlabels(e.g.,â€œRequesttoCan-\nsignificantimprovementsinclassificationaccuracyandresource\ncelOrderâ€â†’â€œCancelOrderâ€)reducesredundancy,enhanc-\nefficiency.Ourmethodsenhancemulti-turnintentclassification ingLLMclassificationperformanceby5.09%,addressingkey\naccuracyby5.09%,reduceannotationcostsby40%,andenablescal- challengesinproductionintentclassification.\nabledeploymentinlow-resourcemultilingualindustrialsystems, 1 Introduction\nhighlightingtheirpracticalityandimpact.\nDialoguesystemsarecriticalforautomatinginteractionsbetween\ncustomersandagents,streamliningcommunicationandenhanc-\nCCSConcepts inguserexperience.Theyplayapivotalroleininternationale-\nâ€¢Informationsystemsâ†’Languagemodels;Questionanswer- commerce platforms, addressing the increasing demand for in-\ning;â€¢Computingmethodologiesâ†’Intelligentagents. stantaneousandefficientcustomerservice.Intentclassification,\nafundamentalaspectofnaturallanguageunderstandingindia-\nloguesystems,involvesidentifyingusersâ€™goalsfromtheirinputs,\nKeywords\ntherebyminimizingwaitingtimesandoperationalcosts[22].User\nMulti-turnIntentClassification,MultilingualLargeLanguageModel, interactionsfrequentlyevolveintomulti-turndialogueswhende-\nRetrievalAugmentation,ComputationalLinguistics,LanguageDi- tailedinformationisrequired,complicatingthedevelopmentof\nversity,KnowledgeEngineering multi-turnintentclassification(MTIC)models,despitetheirsimi-\nlaritytostandardtextclassificationtasks.Additionally,real-world\nACMReferenceFormat: multilingualsystemsrequirescalablesolutionsthatupholdinclu-\nJunhuaLiu1,3,âˆ—,YongKeatTan2,âˆ—,BinFu2,â€ ,KwanHuiLim3.2024.Balanc-\nsivityandethicalstandards,particularlyinlow-resourcesettings.\ningAccuracyandEfficiencyinMulti-TurnIntentClassificationforLLM- Thiscomplexityarisesfromtheneedtoconsidercontextualfac-\nPoweredDialogSystemsinProduction.InProceedingsofPreprint.ACM, torslikehistoricalutterancesandpriorintents.Withoutaproper\nNewYork,NY,USA,9pages.https://doi.org/N.A\nunderstandingofsessioncontext,thesystemrisksmisinterpreting\nuserintentions,whichmayresultinincorrectapplicationsorirrel-\nevantresponses[24].Consequently,MTICwithindialoguesystem\npresentssignificantchallenges.\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor\nThefirstchallengeisthatthelengthofintentsinindustrialdi-\nclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed\nforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation aloguesystemsislongercomparedtogeneraltextclassification\nonthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored. tasks.Figure1showsthattherealintentscompriseseveralwordsin\nForallotheruses,contacttheowner/author(s).\nourknowledgebasebecauseoperators(Ops)typicallyassignintents\nPreprint,WorkingPaper,Nov2024\nÂ©2024Copyrightheldbytheowner/author(s).\nACMISBN978-x-xxxx-xxxx-x/YY/MM *EqualContributions.\nhttps://doi.org/N.A â€ CorrespondingAuthor:bin.fu@shopee.com\n4202\nvoN\n91\n]LC.sc[\n1v70321.1142:viXra\nPreprint,WorkingPaper,Nov2024 Liuetal.\nSecondly,toovercometheshortageofmulti-turndata,wepro-\nposeanovelpseudo-labelinganddatagenerationframeworkcalled\nConsistency-awareLinguisticsAdaptiveRetrievalAugmentation\n(C-LARA).Extendingbeyondexistingsyntheticdatageneration[9],\nC-LARAservesasaneffectivepseudo-labelingtoolforgenerat-\ning multi-turn data from userâ€™s unlabeled utterances with self-\nconsistency.C-LARAarrangestheretrievalresultindifferentorders\ntoassembleadaptiveprompts,whichcoverthediversereasoning\npath and filter out noise in in-context learning to improve the\nqualityoflabelingdata.Subsequently,weusethetrainingdata\ntotrainasmallermodelforonlineinference.C-LARAisanovel\nframeworktailoredformulti-turnintentclassification.Itaddresses\nlimitationsinpriorapproachesbyleveragingadaptiveretrievaland\nFigure2:Annotationpipelineofmulti-turnintentclassifica- self-consistencymechanismstoenhancetheaccuracyofpseudo-\ntiondatasets.Twomajorchallengesinproductionsystems labelingformulti-turndialogues.Unlikepreviousmethods,itdi-\nareillustrated:(1)managingnumerous(500+)intentsacross rectlyoptimizesforzero-shotmulti-turndataclassificationand\nmarketswithredundantlabels,and(2)thehighcostofcol- scalabledeployment.\nlectingmulti-turntrainingdata. Insummary,thecontributionsofthispaperareasfollows:\n(1) Weintroducesymbol-tuning,leveragingcompressedintents\naclearanddescriptivenametofacilitateknowledgemanagement, toenhanceLLMperformanceforMTIC,demonstratinga\nwhichmakesthemredundant.Therecentadvancementsinlarge 5.09%improvementinsupervisedfine-tuning(SFT)results.\nlanguagemodel(LLMs)presentnewresearchopportunitiestosim- (2) WedevelopC-LARA,anovelframeworkforgeneratinghigh-\nplify and optimize the text classification process [19]. Research qualitymulti-turndata,effectivelyaugmentingMTICresults.\nindicatesthatLLMsperformexcellentlyinsentimentanalysis[12], (3) We fine-tune smaller models using data generated by C-\nwhichonlyadoptsshorterlabelssuchaspositive,negative.How- LARA,enablingscalableandaccuratedeploymentofMTIC\never,LLMsstillfailtoaddresscontextdependencyinmulti-turn systemsinlow-resourceindustrialsettings.\nconversationsandstrugglewithlongintentlabelscommoninin-\ndustrialsystems.\n2 ProblemFormulation\nThesecondchallengeliesinthedifficultyofcollectingmulti-turn\ndatasets.Whileseveralstudies[15,23]onMTICexist,theyoften 2.1 Multi-TurnIntentClassification\nassumeaccesstocomprehensivemulti-turntrainingdata,whichis\nMulti-TurnIntentClassification(MTIC)involvesidentifyingthe\nrarelyavailableinreal-worldapplications. intentğ¼ ofthefinalqueryğ‘ ğ‘› fromapredefinedsetI,basedona\nw claeF si si gg in fiu o cr ae re t2 iots h nh eo [r 1w e 3ds ]ut wh ne id ta ha nn otn nio n lt yfa ot li eo w sn sit tp h hi ip an ne il ni 1n t 0e en cf lo t asr s, sM u en sT lI wiC k ie tt ha d is i nak ls do. igE auv loee gn a uci etf s tae sq kue rn ec lie esof onus te hr equ coe nri ve es rQ sat= ion{ ağ‘ lğ‘–} cğ‘› ğ‘– o=1 nti en xta Ccha =tbo {t ğ‘ ğ‘–s }e ğ‘› ğ‘–s =âˆ’s 1i 1o ,n w.T hih ci hs\nincludespriorqueries.Context-dependencyaddscomplexity,re-\nstatetracking(DST),therearehundredsofintentsoperatedbylocal\nquiringmodelstointerpretnuancedconversationaldynamicsand\nOpsinknowledgebaseofdialgouesystemtocoveruserâ€™svarious\nevolving user intentions. Each intent ğ¼ has a local-language ti-\nandspecificintentsineachmarket,whichincreasethecomplexity\ntleğ‘¦ and a hierarchical English categoryğ‘§ (e.g., Indonesia:ğ‘¦ =\nofmulti-turnclassificationanditsannotation.Annotatorsoften\nâ€™Caramembatalkanpesananâ€™,ğ‘§=â€™Logistics>Order>Cancellationâ€™).\nstrugglewiththenumerousofintents,leadingtoincreasedmis-\ntakesandlongerdecision-makingtimes.Asaresult,theannotation\nprocessbecomescostlyandtime-intensive,makingitimpractical 2.2 SupervisedFine-tuning\ntomanuallyannotatelarge-scalemulti-turndatasets.However,in-\nSupervisedFine-tuning(SFT)adaptspre-trainedlargelanguage\nsufficienttrainingdatacansignificantlyhindermodelperformance\nmodels(LLMs)forspecifictasksusinglabeleddatasets.Thisprocess\nevenwithLLM.Thesechallengesunderscorethenecessityformore\nachieveshighbenchmarkaccuracythroughtask-specificsupervi-\nefficientmethodstoaddressdatascarcityandclassificationcom-\nsion.\nplexity.\nTotackletwochallenges,wefirststudythefeasibilityofusing\nLLMforsupervisedfine-tuning(SFT)toperformMTICusinga Problem Definition. Given a dataset D = {(ğ‘¥ ğ‘–,ğ‘¦ ğ‘–)} ğ‘–ğ‘ =1, whereğ‘¥ ğ‘–\ngenerativemethod.Variousintentsincreasethecomplexityofthis isaninputqueryandğ‘¦ ğ‘– isthecorrespondinglabel,theobjective\ntasksincethemoretokensalargelanguagemodel(LLM)generates, is to optimize model parametersğœƒ to maximize the conditional\nthelowerthetaskperformance[16].Toconquerthis,wecompress\nlikelihoodğ‘(ğ‘¦ ğ‘–|ğ‘¥ ğ‘–;ğœƒ):\ntheredundantinfowithinintenttosuccinctintentviaGPT4,then\nadoptthoseintentsinSFTnamedassymboltuning,whichhelpto\nğ‘\nr ge ed nu ec re att ih vee mdi effi thcu odlt .yofmulti-turnclassificationtasksbytheLLM L SFT(ğœƒ)=âˆ’ ğ‘1 âˆ‘ï¸ ğ‘–=1logğ‘(ğ‘¦ ğ‘–|ğ‘¥ ğ‘–;ğœƒ).\nBalancingAccuracyandEfficiencyinMulti-TurnIntentClassificationforLLM-PoweredDialogSystemsinProduction Preprint,WorkingPaper,Nov2024\nConditionalProbabilityModeling. Forstructuredoutputs,ğ‘¦ ğ‘– isa ...\nsequenceoftokens{ğ‘¡ 1,ğ‘¡ 2,...,ğ‘¡ ğ‘š},withprobabilityfactorizedau- USER:\"{q_n}\"\ntoregressively: ASSISTANT:\"Theintenttitleis{r_n}.\"\nğ‘š\nğ‘(ğ‘¦|ğ‘¥;ğœƒ)=(cid:89)ğ‘(ğ‘¡ ğ‘—|ğ‘¡ <ğ‘—,ğ‘¥;ğœƒ). Thegeneratedğ‘Ÿ ğ‘›iscomparedwithintentsinIusingcosinesimilar-\nğ‘—=1 ityintheembeddingspacetoensuresemanticalignmentbetween\nthemodelâ€™soutputandpredefinedintenttitles.\nThetrainingobjectivebecomes:\nL SFT(ğœƒ)=âˆ’\nğ‘1 âˆ‘ï¸ğ‘ âˆ‘ï¸ğ‘š\nlogğ‘(ğ‘¡ ğ‘—|ğ‘¡ <ğ‘—,ğ‘¥ ğ‘–;ğœƒ).\nC siso tm op fr ae pss pe rd oxG imen ae tr ea lt yio 1n 2. tI on kt ee nn st ,r mep ar ke is ne gn tt ha eti mve inq eu ffier cie ies nğ‘Ÿ to asfte gn enc eo rn a-\n-\nğ‘–=1 ğ‘—=1\ntiontargets.Toaddressthis,weemployanLLMtocompressğ‘Ÿ into\n2.3 SymbolTuning concisephrases,typicallytwowords,whilepreservingtheirseman-\nticessence.Thisprocessensuresthateachcompressedintentlabel\nUnlikemethodsreplacingtasklabelswithunrelatedsymbols[21], ğ‘Ÿ ğ‘ isunique.Ifduplicatesoccur,themodeliterativelyincreasesthe\nourSymbolTuningapproachfocusesonintentclassification.Ver-\nwordcountuntiluniquenessisachieved.Thiscompressionreduces\nboselabelsinindustrialsystemsdispersesemanticinformation, theaveragelengthofğ‘Ÿ ğ‘ tofourtokens,optimizingitforgenera-\nhinderingmodelperformance.Toaddressthis,wecompresslabels\ntiontasksandimprovingclassificationaccuracy.Thisapproach\nintoconcisephrasesusingGPT-4.Forexample,\"RequesttoCan-\nenhancesclassificationaccuracybyreducingsemanticdispersion\ncelOrder\"becomes\"CancelOrder,\"servingascompactsemantic\ninlabels,ensuringmorefocusedinformationpropagationthrough\nanchorsthatenhanceshallowanddeeplayerrepresentations.\nLLMlayers.\nMathematical Formulation. Let the original intent label be ğ¿ =\n{ğ‘¡ 1,ğ‘¡ 2,...,ğ‘¡ ğ‘š}. The compressed labelğ¿â€², withğ‘› â‰ª ğ‘š, is gener- Cross-LingualLabels. Innon-Englishmarkets,intentlabelsğ‘Ÿ are\natedbyoptimizing: compressedintoEnglishwhileretainingtheoriginallanguagefor\ninputqueriesQ.LeveragingEnglish,thepredominantlanguagein\nğ¿â€² =argminğ¿â€² C(ğ¿â€²)+E(ğ¿â€²,ğ¿),\nLLMpretrainingcorpora,simplifieslabelgenerationandenhances\nwhere:- C(ğ¿â€²):Compactnessofğ¿â€² (e.g.,tokencount).- E(ğ¿â€²,ğ¿): modelperformanceinmultilingualsettings.Thiscross-lingualstrat-\nSemanticdivergence,computedas: egyreducescomplexityandimprovesalignmentwithpretraining\nE(ğ¿â€²,ğ¿)=1âˆ’cosine_sim(ğœ™(ğ¿â€²),ğœ™(ğ¿)), distributions.Thisstrategyleveragesthestrengthsofpre-trained\nLLMswhileaccommodatingmultilingualdata,offeringascalable\nwithğœ™(Â·)asanembeddingfunction.\nsolutionforcross-lingualintentclassification.\nObjectiveFunction. GivenD ={(ğ‘¥ ğ‘–,ğ¿ ğ‘–)} ğ‘–ğ‘ =1,whereğ¿ ğ‘–istheoriginal\nlabel,thesupervisedfine-tuninglossbecomes: 3.2 Consistency-awareLinguisticsAdaptive\nğ‘› RetrievalAugmentation\nL ST(ğœƒ)=âˆ’E (ğ‘¥,ğ¿â€²)âˆ¼Dâˆ‘ï¸ logğ‘(ğ‘¡ ğ‘—|ğ‘¡ <ğ‘—,ğ‘¥;ğœƒ),\nToenhancein-contextlearning,weproposetheConsistency-aware,\nğ‘—=1\nLinguisticsAdaptiveRetrievalAugmentation(C-LARA)framework.\nwhereğ‘¡\n<ğ‘—\ndenotesprecedingtokensinğ¿â€².\nBuildingupontheLARAmodel[9],C-LARAincorporatesafine-\nPerformanceImplications. Replacingverboselabelsğ¿withcompact tunedsingle-turnmodelMğ‘ withinaretrieval-augmentedpipeline.\nğ¿â€²reducestokenprocessingandimprovesclassificationaccuracy, Thisframeworkenableszero-shotMulti-TurnIntentClassification\nstreamliningintentrecognitiontasks. (MTIC)usingonlysingle-turndemonstrations.UnlikeLARA,which\niscomputationallyintensiveinreal-time,C-LARAoperatesoffline\n3 Solutions asapseudo-labelingtool,generatinghigh-qualitymulti-turndata\nfortraininglightweightclassificationmodels.\n3.1 SymbolTuningonLLM\nSpecifically,theLARApipelinecanbecomplexandresource-\nToaddressintentclassificationtasks,weutilizegenerativemodels intensivetoimplementforreal-timesystems.Hence,weusethis\nratherthanconventionaldiscriminativeorregressiveapproaches. methodofflineasamulti-turndatapseudo-labelingtooltotraina\nOurSymbolTuning(ST)methodinvolvessupervisedfine-tuning smallerclassificationmodel.Thetrainingmethodmirrorsthatof\n(SFT)ofanLLMwithcompressedintentlabels.Givenacomplete thesingle-turnclassifierMğ‘ followingthepaper,addingpseudo-\nchatsessionS={ğ‘ 1,ğ¼ 1,...,ğ‘ ğ‘›âˆ’1,ğ¼ ğ‘›âˆ’1,ğ‘ ğ‘›},themodelistrainedto labeledmulti-turndatatotheoriginaldatacomprisingonlysingle-\ngeneratetherepresentativequestionğ‘Ÿ ğ‘›correspondingtothecorrect turnsamples.\nintentğ¼ ğ‘›ofthefinalqueryğ‘ ğ‘›.Queriesandintentsarestructured Sincethisisnotareal-timetask,thepipelineresponsetimeisnot\ninanaturalquestion-answeringflow,asillustratedbelow: acriticalconsideration,henceself-consistencycheckingwasper-\nSYSTEM:\"Achatbetweenacurioususerandanar- formedontheLLMoutputstoensurethequalityofpseudo-labels.\ntificialintelligenceassistant.Theassistantprovides Forthischeck,asshowninFigure3,thein-contextlearningphase\nhelpful,detailed,andpoliteresponsestotheuserâ€™s isrunthreetimespersample,withthein-contextdemonstrations\nquestions.\" sortedinthreeordersaccordingtotheirsimilarityscorestothe\nUSER:\"{q_1}\" sessionqueries:ascending,descending,andrandom.Thisapproach\nASSISTANT:\"Theintenttitleis{r_1}.\" toself-consistencycheckingmethodcanalsobeimplementedwhen\nPreprint,WorkingPaper,Nov2024 Liuetal.\nFigure3:IllustrationofC-LARA:mergingLARAwithSelf-Consistencyeffectivelycombinesqueryaggregation,knowledgebase\nretrieval,andself-consistencymechanismtogeneratehigh-qualitypseudo-labelsformulti-turndialogues.Theself-consistency\nprocessimproveslabelingaccuracybyvalidatingintentpredictionsacrossdifferentpromptorderings.\nusingablack-boxLLM.Onlinechatlogsaresampledforpseudo- Market Lang. Intents Train(ST) Test(MT)\nlabeling,andonlythosehavingconsistentlabelsforall3runswill BR pt 316 66k 372\nbekeptfortraining. ID id 481 161k 1145\nMY en,ms 473 74k 1417\n3.2.1 HierarchicalTextClassification(HTC). Mğ‘ isanensembleof\nPH en,fil 237 33k 189\nlabel-attentionencoderandahierarchical-awaretree-basedencoder\nSG en 360 76k 737\nwith3-layeredglobalandlocalintentclassifiers.\nTH th 359 60k 502\nThe label-attention encoder has one classifier head for each\nTW zh-tw 373 31k 353\nintentlayer.Eachclassifierheadhasonehiddenlinearlayerto\nVN vi 389 178k 525\nobtainthelayerintermediateoutputğ¿ ğ‘™,whichencodesthelayer\ninformation.Thislayerinformationwillbeutilisedintheinputof Table1:MultilingualdatasetstatisticsforSingleTurn(ST)\nthenextlayerclassifierhead. andMulti-Turn(MT).\nMKT Model ğ‘Ÿ ğ‘ CL-Label Accuracy\n(cid:40)\nğ»ğ‘Š1+ğ‘1, ifğ‘™ =1, SG NaiveConcat. - - 60.52%\nğ¿ ğ‘™ = (ğ» âŠ•ğ‘™ ğ¿ ğ‘™âˆ’1ğ‘™ )ğ‘Š ğ‘™1+ğ‘ ğ‘™1, ifğ‘™ >1, S SG\nG\nSele Lc lt ai mve a2C -o 7n Bcat. âœ˜- âœ˜- 5 56 6. .9 29 4%\n%\nwhereğ‘Š1 âˆˆRğ‘‘Ã—ğ‘‘ forğ‘™ =1andğ‘Š1 âˆˆR2ğ‘‘Ã—ğ‘‘ forğ‘™ >1.ğ‘1 âˆˆRğ‘‘ ,ğ‘™ SG Llama2-7B âœ” âœ˜ 61.33%\nisthelayğ‘™ ernumber,âŠ•denotestenğ‘™ sorconcatenation.Finğ‘™\nally,we\nSG Domain-Llama2-7B âœ” âœ˜ 63.23%\nobtainthelocallogitsğ»ğ‘™\nforeachlayerclassesbyusinganother\nID NaiveConcat. - - 60.61%\nğ‘™ğ‘œğ‘ğ‘ğ‘™\nID SelectiveConcat. - - 63.23%\nlinearlayer\nID Llama2-7B âœ” âœ˜ 49.96%\nğ» ğ‘™ğ‘™\nğ‘œğ‘ğ‘ğ‘™\n=ğ¿\nğ‘™\nÂ·ğ‘Š ğ‘™2+ğ‘ ğ‘™2,ğ‘Š ğ‘™2 âˆˆRğ‘‘Ã—|Iğ‘™|,ğ‘ ğ‘™2 âˆˆR|Iğ‘™| I ID\nD\nS Se ea aL LL LM M- -7 7B B- -c ch ha at\nt\nâœ”\nâœ”\nâœ”âœ˜ 5 52 5. .4 09 2%\n%\nwhere|Iğ‘™|isthenumberofclassesinthelayer.\nTable2:PerformanceofLLMwithsymboltuningapproaches.\nHowever,thelabel-attentionmodelisunawareoftheoverallhier-\narchicalstructure.Therefore,weensembleitwithanothermethod. nodes.Thelogitsarethensplitbythenumberofclassesineach\nWerefertoHiTIN[26]fortheimplementationofastate-of-the-art layertoobtainğ»ğ‘™ .\nğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™\nHTCglobalapproach.Inthismethod,atreenetworkisconstructed Thefinalclassprobabilitiesforeachlayerğ‘ƒ ğ‘™ isthenobtainedby:\nb saa gs ee sd ao rn et ph re ops aim gap tl ei dfie bd oto tr oi mgi -n ua pl it nax ao nn io sm omy os rt pru hc ist mure m, aa nn nd et rh ,e wm hie cs h- ğ‘ƒ ğ‘™ =ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ» ğ‘™ğ‘™ ğ‘œğ‘ğ‘ğ‘™ +ğ» ğ‘”ğ‘™ ğ‘™ğ‘œğ‘ğ‘ğ‘™)\ncomplementsthelabel-attentionmodelused.Theembeddingfor 4 Experiments\nleafnodesareobtainedbybroadcastingthetextrepresentationğ».\n4.1 Dataset\nAfterthetreeisomorphismnetworkpropagation,allembedding\nfromalllayersareaggregatedtoformsingleembedding,anda Thedatasetusedinourexperimentsisderivedfromtheconversa-\nclassificationlayerisusedtoobtainthelogitsğ» ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ ofalltree tionhistoryofalargee-commerceplatform.Itincludesuserqueries\nBalancingAccuracyandEfficiencyinMulti-TurnIntentClassificationforLLM-PoweredDialogSystemsinProduction Preprint,WorkingPaper,Nov2024\ninthelocallanguagesofeightmarkets:Brazil(BR),Indonesia(ID), data,asshowninSection4.1.WeuseAdamWtofinetunetheHTC\nMalaysia(MY),Philippines(PH),Singapore(SG),Thailand(TH), withalearningrateof5e-6.AlltestsarerunonasingleNvidia\nTaiwan(TW),andVietnam(VN),asdetailedinTable1.Labeleddata V100GPUcardwith32GBofGPUmemory.\nweremanuallyannotatedbylocalcustomerserviceteams,with\nonlysamplesachievinglabelconsistencyacrossthreeindependent 4.4 Baselinesettings\ntaggersbeingselectedtoensurequality. Forafaircomparison,weadoptthreemethodsfine-tunedonHTC\nSingle-turntrainingdatacollectedoveryearsofbusinessop- model(Mğ‘)asourglobalbaselinesacrosstwomethods:\nerationsformthebasisforsupervisedfine-tuningandin-context\n(1) Single-turnmethod:whereonlythelastqueryofasession\nlearning.Formulti-turnevaluation,realonlinesessionsareanno-\nisconsideredbyMğ‘;\ntatedbylocalcustomerserviceteams,withonlythelastquery\n(2) Naiveconcatenation:allqueriesareconcatenatedtogether\nğ‘ ğ‘› labeledineachsessionQ.Forpreprocessing,weremovenoisy\nbeforebeingfedintoMğ‘;\nannotations,standardizeintents,andaugmentmulti-turnsessions\n(3) Selectiveconcatenation:whereaconcatenationselection\nusingdialoguestatetransitionprobabilitiesderivedfromchatlogs.\nmodelistrainedtoselectthemostsuitablehistoricalquery\nSymbolTuning.WeperformsymboltuningonLLMfortheSG\nwiththelastquerytoserveastheinputtoMğ‘.\nandIDdatasets,whereSGmainlyusesEnglishwhileIDusesBahasa\nIndonesia.Thetrainingdatacomprisesamixofexistingsingle-turn STonLLM. InSG,exceptLlama2-7B,wealsotriedtocontinuepre-\nsamplesandabout60ksemi-automaticallycraftedmulti-turnsam- trainingthebasemodelsonin-domaincorpustostrengthenthe\nplesaddedtoeachmarket.Someareobtainedbycleaningdataon languageunderstandingoflocallanguagesandthecorresponding\nonlinechatlogstoidentifymoreaccurateintentsusinganLLM slangused,ashumansusuallyconversewiththechatbotinanon-\nwithafew-shotsofthechain-of-thoughtprompt.Therestarecon- formalway.WetermthedomainspecificbasemodelasDomain-\nstructedbycombiningseveraldialoguessampledfromtheexisting Llama2-7B.InID,weswitchedLlama2-7BmodeltoSeaLLM-7B-\nsingle-turntrainingdatasettoformonesession.Thetransitionof chat[11]whichwasintroducedspecificallyforlanguagesinSouth\nintentsinasessioniscalculatedfromtheonlinechatlog. EastAsia.\nHTCwithC-LARA.70kofonlinechatlogsaresampledforpseudo- TheSTapproachwasadaptedforsupervisedintentrecognition\nlabelling.Afterself-consistencychecking,around12%ofthedata usingcompressedgenerationtargets(ğ‘Ÿ ğ‘)andcross-linguallabels\nyieldinconsistentresultsandarediscardedfromtraining.1.5ksam- (CL_label).Theseadjustmentsoptimizedperformancebysimplify-\nplesaresplitfromthepseudo-labeleddatatoserveasthevalidation ingthegenerativetaskwhilemaintainingsemanticintegrity.Com-\nsetforearlystopping. parisonswithbaselinemethodsinTable2showthatSTachieves\ncompetitiveresultsinEnglishmarketsbutfaceschallengesinnon-\n4.2 Metrics Englishsettingsduetolimitationsinpre-trainingforlow-resource\nTheprimaryevaluationmetricistheaccuracyofpredictedlabelsfor languages.\nthefinalqueryğ‘ ğ‘›ineachconversationsessionQ.Metricsaccount-\nHTCwithC-LARA. ThisexperimentusesVicuna-13Basourbase\ningforclassimbalancewerenotconsidered,asthesampledsessions\nmodelforpseudo-labelingwithinLARAandC-LARA.Wedesigned\nreflectthedistributionofonlinetrafficacrossintents,providinga\nthreepipelineswithfourprompttemplatesin[9]todemonstrate\nrealisticapproximationofliveperformance.\nthatusingC-LARAforpseudo-labelingcaneffectivelyimprovethe\nHTCmodelâ€™sperformanceinmulti-turnclassificationtasks.The\n4.3 ImplementationDetails\ndetailedintroductionislistedasfollows:\nSymbolTuningonLLM. FastChatframeworkisusedtofine-tune â€¢ LARA:UsingLARAdirectlyasaclassifier.\n7BLLMsusingLoRAmethodontheirğ‘_ğ‘ğ‘Ÿğ‘œğ‘—,ğ‘£_ğ‘ğ‘Ÿğ‘œğ‘—,ğ‘œ_ğ‘ğ‘Ÿğ‘œğ‘—,and\nâ€¢ LARA-PL:UsingLARAasanaivepseudo-labelingtooland\nğ‘˜_ğ‘ğ‘Ÿğ‘œğ‘— moduleswithalearningrateof2e-5over10epochs.The\nfine-turnHTCmodelwithgenerateddata.\n7BmodelsusedareLlama-2-7B(forSG)andSeaLLM-7B-chat(for â€¢ C-LARA:UseingC-LARAtofilteroutthenoiseandgenerate\nID) on Hugging Face. Before the models are fine-tuned on the\nhigh-qualitydatatofine-tunetheHTCmodel.\nmulti-turnintentrecognitiontask,theyarefurtherpre-trainedon\nShareGPTdatasetwiththesamesettingabove,andtheweights 4.5 OfflineExperiments\narethenmerged.Forthesakeofsimplicity,wewillrefertothe\nSymbolTuningonLLM. Table2illustratestheeffectivenessofSym-\nLLMs further pre-trained on ShareGPT dataset as base models.\nbolTuning(ST)onLLMs.Compressingthegenerationtargetğ‘Ÿ\nDuringtrainingforintentclassificationtask,lossiscalculatedon\nreducestaskcomplexityandimprovesaccuracyby5.09%intheSG\nallthemodeloutputincludingthoseafterhistoryqueries.During\nmarket.Thiscompressionalsomitigateshallucination,reducing\ninference,greedydecodingstrategyisusedtogeneratethetarget\nğ‘Ÿ part,theprefix\"Theintenttitleis\"isnotgeneratedbutinstead instancesofunmatchedgeneratedlabelsfrom2.5%to0%.\nInterestingly,thistechniquealsostoppedLLMhallucination,i.e.\nappendedattheendoftheprompt.Whenthegeneratedlabelhas\nnoexactmatchwithanyğ‘Ÿ inI,gestaltstringmatchingisusedto\ngeneratinglabelwithnomatchintheI.Thehallucinationrate\nwithoutusingcompressedğ‘Ÿ isabout2.5%.InID,whichisanon-\nfindtheclosestone.\nEnglishmarket,wefindthatcross-linguallabelwhichchanges\nHTCwithC-LARA. Thein-houseHierarchicalTextClassification thegenerationtargettoEnglishratherthaninthelocallanguage\n(HTC)modelisaBERT-basedmodelfine-tunedusingthecombina- alsoimprovedtheperformanceby2.53%.Usingdifferentbase\ntionofthepseudo-labeledmulti-turndataandexistingsingle-turn modelswhichweretrainedspecificallyonthein-domaincorpusor\nPreprint,WorkingPaper,Nov2024 Liuetal.\nPipeline Model Prompt Self-Consistency BR ID MY PH SG TH TW VN avg\nFine-tuning Single-turn - - 30.98% 52.14% 56.81% 40.21% 51.13% 52.99% 58.07% 65.90% 53.76%\nFine-tuning NaiveConcat. - - 50.81% 60.61% 57.02% 47.62% 60.52% 56.97% 65.44% 76.95% 60.08%\nFine-tuning SelectiveConcat. - - 52.69% 63.23% 60.20% 51.32% 56.99% 57.77% 64.02% 74.10% 60.97%\nLARA Vicuna-13B P âœ˜ 52.69% 61.48% 65.42% 54.50% 65.26% 60.96% 67.14% 77.90% 64.18%\nC-LARA Vicuna-13B P âœ” 55.38% 63.58% 65.00% 54.50% 66.21% 63.75% 71.10% 79.24% 65.52%\nLARA Vicuna-13B Pğ‘ ğ‘¦ğ‘šğ‘ğ‘œğ‘™ğ‘–ğ‘ âœ˜ 51.88% 60.00% 64.57% 53.97% 65.26% 58.96% 65.44% 74.67% 62.92%\nC-LARA Vicuna-13B Pğ‘ ğ‘¦ğ‘šğ‘ğ‘œğ‘™ğ‘–ğ‘ âœ” 54.57% 62.62% 65.56% 50.79% 66.76% 62.95% 69.97% 76.76% 64.94%\nLARA Vicuna-13B Pğ‘ğ‘Ÿğ‘’ğ‘ğ‘’ğ‘›ğ‘‘ âœ˜ 54.03% 61.75% 64.50% 53.44% 65.94% 61.55% 66.86% 75.81% 63.97%\nC-LARA Vicuna-13B Pğ‘ğ‘Ÿğ‘’ğ‘ğ‘’ğ‘›ğ‘‘ âœ” 53.76% 63.84% 65.70% 52.91% 68.11% 63.15% 69.97% 78.48% 65.65%\nLARA Vicuna-13B Pğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘¡ğ‘’ğ‘‘ âœ˜ 55.65% 62.88% 64.71% 55.03% 65.40% 61.95% 66.86% 78.10% 64.64%\nLARA-PL Vicuna-13B Pğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘¡ğ‘’ğ‘‘ âœ˜ 55.91% 64.19% 64.43% 49.21% 66.49% 61.95% 69.41% 81.14% 65.29%\nC-LARA Vicuna-13B Pğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘¡ğ‘’ğ‘‘ âœ” 55.91% 65.33% 66.27% 51.85% 67.16% 63.35% 72.80% 78.86% 66.35%\nTable 3: Performance of C-LARA compared to baselines, the average here is weighted on the number of test samples in\neachmarket.TheresultsillustratethatC-LARAwithformattedpromptsachievesthebestaverageaccuracy(66.35%)across\nallmarkets.Theresultsvalidateourapproachâ€™seffectivenessinbothEnglishandnon-Englishmarkets,withsignificant\nimprovementsoverbaselinemethods.\nforthelocallanguagealsoprovestobeuseful.Domain-Llama2-7B Deployedonasingle32GBV100GPU,theSymbolTuning(ST)\nimprovestheperformanceby1.90%inSGwhileSeaLLM-7B-chat approachachievedanaveragelatencyof170msat0.5QPSinthe\nimproves the performance by 2.53%in ID compared toLlama2- SGmarket.Incontrast,C-LARAmodelsconvertedtoONNXformat\n7B.WhiletheSTapproachoutperformsthebaselinesinEnglish (1.1GBpermodel)achievedanaveragelatencyof80msat1QPS\nmarket,itstillleavesalottobedesiredinnon-Englishmarket.This on an 8-core CPU machine with 16GB memory, demonstrating\nphenomenonmayariseasaresultoftheSTapproachemployedfor superiorscalabilityandcost-efficiency.\nnon-dominantlanguagesduringpre-training,whichnecessitatesa\ngreaterquantityorhigherqualityofdatatoachievesatisfactory C-LARA. WedeployC-LARAacrossalleightmarkets.Themodels\nperformanceinataskthatwasnotincludedinthepre-training werefirstconvertedtoONNXformat,reducingtheirsizeto1.1GB.\nphase.Thisisparticularlytruewhenthemodellacksknowledge Deployedonan8-coreCPUmachinewith16GBmemory,C-LARA\npertainingtodomainintents. achievedanaveragelatencyof80msat1QPS,whichislessthan\nhalfthelatencyoftheSTonLLMmethod.Thisdeploymentsignifi-\nPseudo-labeling using C-LARA. As demonstrated in Table 3, C- cantlyreducedbothcostsandcomplexity,makingitmorescalable\nLARAimprovespseudo-labelqualitythroughself-consistencyval- forindustrialapplications.Duetoitsversatility,anAuto-Training\nidation,resultingina1.06%performancegainoverLARA.This Portal(ATP)ecosystemisbuiltaroundtheLARA-PLmethod(Fig.\nvalidationprocessidentifiesandremovesapproximately12%of 4).ATPenablesseamlessandcontinuousimprovementsforthe\ninconsistentsamples,ensuringhigh-qualitysyntheticlabels.While chatbotâ€™smulti-turnintentrecognitionsystem.Usingonlinechat\nthisapproachrequiresadditionalofflinetrainingresources,itsignif- logs,localoperationsteamscanupdatetheKnowledgeBase(KB)\nicantlylowersdeploymentcostsbyrelyingonasingle,lightweight byaddingnewintentsandcraftingexamplequeries.Subsequently,\nclassificationmodel. theycantriggerC-LARAforpseudo-labelingmulti-turnchatlogs,\nThismostprobablycanbeattributedtotheadvantagesofthedis- generatingdatatotrainlightweightmodels.Oncetrainingiscom-\ncriminativemethodinclassificationtasks,astrainingprocessalso plete,themodelsaredeployedthroughtheportalforonlineA/B\nexposedthemodeltothecomprehensivehighqualitysingle-turn testing,creatinganiterativecycleofimprovement.Forfaircom-\ndataset.Besides,thepre-trainedmodelusedforMğ‘ wasalsopre- parisons,theversionoftheKB(intentsandsingle-turntraining\ntrainedspecificallyonthein-domainmulti-lingualcorpus,making data)waskeptconsistentacrosscontrolandtestgroups.\nitastrongsuitforourmultilinguale-commercesetting.C-LARAâ€™s\nintegrationofself-consistencywithinthepseudo-labelingpipeline\n4.7 OnlinePerformance\nsignificantlyenhancesthequalityofsyntheticlabels,resultingina\n1.06%improvementinperformance,asindicatedinthelastrowof Weleveragethefollowingtwometrics:\nTable3.WhentheLLMlacksconfidenceinitsICLresponses,minor\nchangesintheinputpromptcansignificantlyaltertheoutput.This (1) Resolutionrate(RR)whichismeasuredbytherateofuser\nmethodeffectivelyidentifiespotentialinaccuraciesinICLoutputs completingtheanswerflow,nottransferringtoliveagent,\nforblack-boxmodelswheredirectoutputscoresareunavailable. andnotgivingbadratingtotheanswer.\nMost importantly, this approach, while requiring longer offline (2) CustomerServiceSatisfaction(SCSAT)whereuserswill\ntrainingtime,significantlyreducesdeploymentcoststojustone beaskedabouttheirsatisfactiontowardsourchatbotfor\nsmallclassificationmodel. chatbot only sessions (no intervention from live agents).\nThescoreiscalculatedby#goodratedsessions/(#goodrated\nsessions+#badratedsessions).\n4.6 OnlineDeploymentEvaluation\nSTonLLM.. UsingtheLMDeployframework,LARAweightswere Weusetheselectiveconcatenationmethodasthebaselinefor\nmergedwiththe7Bbasemodel,enablingfasterinferencetimes. allexperiments,withpairedt-testtoevaluatestatisticalsignificance.\nBalancingAccuracyandEfficiencyinMulti-TurnIntentClassificationforLLM-PoweredDialogSystemsinProduction Preprint,WorkingPaper,Nov2024\n5.1.2 ShorterTargetLength. Theapproachofcompressingğ‘Ÿ was\ninspiredby[21].Hence,wealsotriedtoreplaceğ‘Ÿswithcompletely\nmeaninglesssymbols,whilekeepingthegenerationprefixofâ€œThe\nintenttitleis\".Compressingtargetlabelstopurelysymbolicrep-\nresentations results in a significant 8.91% performance drop, as\nshowninTable7.Thishighlightstheimportanceofpreserving\nsemanticrichnessintargetlabelsforgenerativefine-tuning.Ef-\nfectivecompressionmethodsmustretainkeyinformationfrom\ntheoriginallabelstoavoidlossinclassificationaccuracy..Thus,\nwhencompressingğ‘Ÿs,itisimportanttochooseamethodthatcan\npreservetheinformationinoriginalğ‘Ÿsasmuchaspossible.\nFigure 4: Online Deployment of Multi-turn Intent Classi-\n5.2 ImpactofSelf-consistencyinMTIC\nfication model demonstrates our production architecture\nintegratingC-LARAforautomatedtrainingdatageneration. Using our multi-turn test sets, we evaluate the performance of\nThesystemhandlesreal-timeinferencewhilecontinuously MTICwithandwithoutself-consistencychecking.Weremovethe\nimprovingthroughautomatedtraining. sampleswithinconsistentoutputsandcalculatetheprecisionof\ntheremainingsamples.Onaverage,12%oftestsampleswillbe\nSTonLLM. IntheSGmarket,STonLLMswasdeployedto50%of\nremovedineachmarket.Incorporatingself-consistencychecking\nonlinetrafficforthreeweeks,yieldingapproximately14kchatbot\nintoMTICevaluationsimprovesaccuracyacrossallpromptvaria-\nsessionspergroup.Thetestgroupexhibiteda+2.19%improvement\ntions,asshowninTable4.Byremovingapproximately12%oftest\ninCustomerServiceSatisfaction(SCSAT),butResolutionRate(RR)\nsampleswithinconsistentoutputs,thismethodeffectivelyfilters\ndeclinedby-0.11%.Neitherresultwasstatisticallysignificant,indi-\nouterroneouspredictions,ensuringhigher-qualitypseudo-labels\ncatinglimitedbenefitsfromSTgivenitsresource-intensivenature.\nandmorereliableresults.Thisensuresthequalityofpseudo-labels.\nC-LARA. ForC-LARA,onlythemulti-turndialoguemodelwasre-\nplaced,whilesingle-turnmodelsremainedunchanged.Aggregated 5.3 EffectofModelSize\nresultsfromover108kchatbotsessionspergroupshowedstatis-\nForfaircomparisonbetweenLLMSTandC-LARA,weusevicuna-\nticallysignificantimprovements:ResolutionRate(RR)increased 7b-v1.5asthebasemodelwithpromptPandPğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘¡ğ‘’ğ‘‘,without\nby+0.78%andCustomerServiceSatisfaction(SCSAT)by+1.39%\nself-consistencychecking.TheresultsofLLMSTmethodaretaken\n(p-value<0.05).Thesegainstranslatetooverallsessionimprove-\nfromthebestofeachmarketreportedinthispaper,includingbase\nmentsofRR+0.47%andSCSAT+0.84%,asmulti-turndialogues\nmodels pre-trained on in-domain corpus, so it should have the\ncomprise60.60%oftotalsessions.\nadvantageoverVicuna-7B-v1.5.Table5comparesC-LARAand\nFurthermore,addingpseudo-labeledmulti-turndataenhanced\nLLMSTusingmodelsofthesamesize(Vicuna-7B-v1.5)without\nsingle-turnintentrecognition.Substitutingsingle-turndialogue\nself-consistencychecking.Despitethesimplerpipeline,C-LARA\nmodelswithC-LARAmodelsyieldedanRRimprovementof+0.06%\nconsistentlyoutperformsLLMST,avoidingthecomplexityofmulti-\nandastatisticallysignificantSCSATincreaseof+0.27%.\nturn sample crafting. However, smaller models exhibit reduced\ninstruction-followingcapabilities,asdemonstratedbythelower\n5 AblationStudy\nperformanceof Pğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘¡ğ‘’ğ‘‘ comparedto P.Oneinterestingob-\n5.1 EffectofTargetLength servation here is that the performance of C-LARA when using\nWeinvestigatehowtheamountofinformationinSTgeneration\nPğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘¡ğ‘’ğ‘‘ isnowlowerthanP.LLMsofsmallersizecouldbe\nweakerininstructionfollowing,andinthissensethesemantic\ntargetaffectstheintentrecognitionperformanceusingtworather\nmeaningofthelabelsindemonstrationsaremorecritical.Prepend-\nextremeapproachesandtheirconversationsemanticfluidity.\ningmeaninglesscharactersbeforelabelscannegativelyaffectthe\n5.1.1 LongerTargetLength. Toachievethis,themodelistrained understandingoflabelsforsmallerLLMs.\ntosummarizeallqueriesinQbeforeoutputtingthetargetğ‘Ÿ.For\ninstance,thenewoutputformatofmodelwillbeâ€œYouareask- 6 RelatedWork\ningabout{ğ‘ ğ‘¢ğ‘šğ‘šğ‘ğ‘Ÿğ‘¦}.So,theintenttitleis{ğ‘Ÿ ğ‘›}\".Therationaleis\n6.1 SyntheticDataGeneration\ntoutilizethesummarizationabilityofLLMstobetterunderstand\nthe context. For our training data, the summaries are obtained Thescarcityofannotateddialoguedata,particularlyinlow-resource\nbypromptingtheoriginalLLMbackbonesinazero-shotmanner. languages,hasdrivenresearchintosyntheticdatageneration.Borisov\nWechosethisoverincreasingthelengthofğ‘Ÿ staticallytoimpose etal.[1]proposedamethodleveragingauto-regressivegenerative\nmoreinformationonthemodelâ€™sgenerationtarget.Table6demon- modelstocreaterealistictabulardatasets,highlightingtheirutil-\nstratestheimpactofincreasingthetargetlengthinSymbolTuning ityindataaugmentation.Similarly,Lietal.[6]demonstratedthat\n(ST).Extendingthegenerationtargettoincludequerysummaries syntheticdatageneratedbyLLMscansignificantlyenhancemodel\ndecreasesperformanceby3.82%.Whilethisapproachenhances performanceinclassificationtasks.Additionally,Tangetal.[18]uti-\nsemanticcoherence,excessiveinformationoverloadsthemodel, lizedsyntheticdatatocraftchallengingexamplesforfact-checking,\nreducingitsabilitytofocusonthecoreintentclassificationtask. improvingthefactualaccuracyofLLMoutputs.\nPreprint,WorkingPaper,Nov2024 Liuetal.\nPrompt Self-Consistency BR ID MY PH SG TH TW VN avg\nP âœ˜ 52.69% 61.48% 65.42% 54.50% 65.26% 60.96% 67.14% 77.90% 64.18%\nP âœ” 58.59% 68.13% 69.93% 56.44% 69.58% 66.75% 71.30% 81.14% 69.11%\nPğ‘ ğ‘¦ğ‘šğ‘ğ‘œğ‘™ğ‘–ğ‘ âœ˜ 51.88% 60.00% 64.57% 53.97% 65.26% 58.96% 65.44% 74.67% 62.92%\nPğ‘ ğ‘¦ğ‘šğ‘ğ‘œğ‘™ğ‘–ğ‘ âœ” 56.63% 64.71% 68.48% 55.19% 68.77% 65.59% 71.61% 78.02% 67.27%\nPğ‘ğ‘Ÿğ‘’ğ‘ğ‘’ğ‘›ğ‘‘ âœ˜ 54.03% 61.75% 64.50% 53.44% 65.94% 61.55% 66.86% 75.81% 63.97%\nPğ‘ğ‘Ÿğ‘’ğ‘ğ‘’ğ‘›ğ‘‘ âœ” 59.49% 66.08% 68.60% 55.90% 68.85% 68.19% 71.79% 81.36% 68.43%\nPğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘¡ğ‘’ğ‘‘ âœ˜ 55.65% 62.88% 64.71% 55.03% 65.40% 61.95% 66.86% 78.10% 64.64%\nPğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘¡ğ‘’ğ‘‘ âœ” 59.24% 67.01% 69.47% 56.79% 68.81% 68.69% 72.35% 82.93% 69.12%\nTable4:PrecisionofC-LARAvariantsafterfilteringinconsistentpredictionsdemonstratestheeffectivenessofself-consistency\ncheckingacrossdifferentprompttypes(P,ğ‘ƒ ğ‘ ğ‘¦ğ‘šğ‘ğ‘œğ‘™ğ‘–ğ‘,ğ‘ƒ ğ‘ğ‘Ÿğ‘’ğ‘ğ‘’ğ‘›ğ‘‘,ğ‘ƒ ğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘¡ğ‘’ğ‘‘).Acrossallprompttypes,accuracyimprovesby\napproximately4-5%,withPğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘¡ğ‘’ğ‘‘ achievingthehighestprecision(69.12%).Theseresultsvalidatetherobustnessofself-\nconsistencyasafilteringstrategy.\nMethod Prompt ID SG avg 6.3 LLMontextclassification\nLLMST - 58.17% 63.23% 60.15% RecentstudieshaveexploredtheapplicabilityofLLMsacrossvari-\nC-LARA P 60.44% 64.31% 61.96% ousdomains.ChaeandDavidson[3]investigatedLLMsforsocio-\nC-LARA Pğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘¡ğ‘’ğ‘‘ 59.83% 64.04% 61.48% logicaltextclassification,demonstratingtheirpotentialinsocial\nscienceresearch.Infinancialintentdetection,Loukasetal.[10]\nTable5:ResultsofLARAusing7BLLM.\nanalyzedthetrade-offsbetweenperformanceandcostwhenus-\ningLLMsfortextclassification.Liuetal.[8]employedGPT-4oto\nMKT Model LongerTargetLength Accuracy performzero-shotclassificationonmulti-levelsemi-structuredtext\nSG Llama2-7B âœ˜ 54.82% withretrievalaugmentation.Weietal.[20]highlightedthebenefits\nSG Llama2-7B âœ” 51.02% offine-tuningLLMsondomain-specificdatasets,improvingperfor-\nmanceinlegaldocumentreview.Weietal.[21]introducedsymbol\nTable6:EffectofLongerTargetLengthonLLMSTClassifi-\ntuning,wherenaturallanguagelabelswerereplacedwithunrelated\ncationPerformance:lengtheningtargetsresultsinaperfor-\nsymbolsduringfine-tuningtoenhanceclassification.Ourwork\nmancedropby3.82%.\ndiffersbycompressinglongerintentlabelsintosemanticallymean-\ningfulphrases,enablingeasiergenerationandimprovingaccuracy\nMKT Model ShorterTargetLength Accuracy fortaskswithalargenumberofclasses.\nID SeaLLM-7B âœ˜ 55.02%\nID SeaLLM-7B âœ” 46.11%\nTable7:EffectofShorterTargetLengthonLLMSTClassifi-\ncationPerformance:reducingsemanticallyrichtargetsinto 7 Conclusion\nsymbolscostsadrasticperformancedropby8.91%. Multi-turnintentclassificationplaysacriticalroleinmoderndia-\nloguesystems.Unliketypicalclassificationtasks,real-worldintent\nclassificationofteninvolvesvaryingintentlengths,posingunique\n6.2 ModelingMulti-turnDialogueContext\nchallenges.Inthiswork,weintroducedSymbolTuningtofine-tune\nMulti-turndialoguemodelingisessentialfordialogueunderstand- largelanguagemodels(LLMs)withcompressedintents.Ourexper-\ningtasks.EarlymethodsusedbidirectionalcontextualLSTMs[4] imentsdemonstratedthatshorteningintentsimprovedaccuracy\ntocapturecontext-awareutterancerepresentationsfortaskssuch by5.09%comparedtousingoriginalintents.\nasMultiWOZintentclassification[2].Otherapproaches,suchas Additionally, we proposed C-LARA, an augmentation-based\nmulti-channelgraphconvolutionalnetworks,wereappliedtoquery pipelineforgeneratinghigh-qualitymulti-turndatasetsusingself-\nclassificationinE-commerce[25]. consistencyvalidation.Trainingsmallermodelswithpseudo-labeled\nRecent advancements leverage pre-trained language models datageneratedbyC-LARAyieldeda1.06%averageperformance\n(PLMs)assentenceencoders[17],particularlyforemotionrecogni- improvement.Empirically,C-LARAsignificantlyreducesannota-\ntioninconversations(ERC).Forinstance,LeeandLee[5]encoded tioncostsbyautomatingpseudo-labelingbasedontheuserâ€™slatest\nbothcontextandspeakermemoryusingPLMs,whileQinetal.[14] utteranceindialoguehistory,improvingmodeliterationefficiency.\nincorporatedmulti-turninformationfromutterancesanddialogue Furthermore, training smaller models offers computational effi-\nstructurethroughfine-tuning.Despitetheireffectiveness,these ciency,enablingscalabledeploymentandonlineinference.\nmethodsdependheavilyonmulti-turntrainingdatasets,whichare FutureWork.Movingforward,weaimtoincorporatefeatures\ndifficulttoacquireinreal-worlde-commercesettings[7].Incon- suchasuserprofilesandorderhistoryintoC-LARAtosupportmore\ntrast,ourapproachemploysLLMswithinanaugmentation-based diversedialoguetasks.Wealsoplantoexplorecross-lingualtransfer\npipelinetogeneratemulti-turndata,enablingzero-shotintentclas- andadvancedtokenizationtechniquestoenhanceperformancein\nsificationusingsmallermodels. low-resourcelanguages.\nBalancingAccuracyandEfficiencyinMulti-TurnIntentClassificationforLLM-PoweredDialogSystemsinProduction Preprint,WorkingPaper,Nov2024\nReferences\n[22] H.Weld,X.Huang,S.Long,J.Poon,andS.C.Han.2021. Asurveyofjoint\n[1] VadimBorisov,KathrinSeÃŸler,TobiasLeemann,MartinPawelczyk,andGjergji intentdetectionandslot-fillingmodelsinnaturallanguageunderstanding.\nKasneci.2022.LanguageModelsAreRealisticTabularDataGenerators.ArXiv arXiv:2101.08091[cs.CL]\nabs/2210.06280(2022). [23] Ting-WeiWu,RuolinSu,andBiing-HwangJuang.2021. AContext-Aware\n[2] PaweÅ‚Budzianowski,Tsung-HsienWen,Bo-HsiangTseng,InigoCasanueva, Hierarchical BERT Fusion Network for Multi-turn Dialog Act Detection.\nStefanUltes,OsmanRamadan,andMilicaGaÅ¡iÄ‡.2018.MultiWOZâ€“alarge-scale arXiv:2109.01267[cs.CL]\nmulti-domainwizard-of-ozdatasetfortask-orienteddialoguemodelling.arXiv [24] PuyangXuandRuhiSarikaya.2014.Contextualdomainclassificationinspoken\npreprintarXiv:1810.00278(2018). languageunderstandingsystemsusingrecurrentneuralnetwork.In2014IEEE\n[3] YoungjinChaeandThomasDavidson.2023. Largelanguagemodelsfortext InternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP).\nclassification:Fromzero-shotlearningtofine-tuning.OpenScienceFoundation 136â€“140. https://doi.org/10.1109/ICASSP.2014.6853573\n(2023). [25] ChunyuanYuan,MingPang,ZhengFang,XueJiang,ChangpingPeng,and\n[4] DeepanwayGhosal,NavonilMajumder,RadaMihalcea,andSoujanyaPoria. ZhangangLin.2024. ASemi-supervisedMulti-channelGraphConvolutional\n2021. Exploringtheroleofcontextinutterance-levelemotion,actandintent NetworkforQueryClassificationinE-commerce.InCompanionProceedingsof\nclassificationinconversations:Anempiricalstudy.InFindingsoftheAssociation theACMonWebConference2024.56â€“64.\nforComputationalLinguistics:ACL-IJCNLP2021.1435â€“1449. [26] HeZhu,ChongZhang,JunjieHuang,JunranWu,andKeXu.2023. HiTIN:\n[5] JoosungLeeandWooinLee.2022.CoMPM:ContextModelingwithSpeakerâ€™s Hierarchy-awareTreeIsomorphismNetworkforHierarchicalTextClassification.\nPre-trainedMemoryTrackingforEmotionRecognitioninConversation.In InAnnualMeetingoftheAssociationforComputationalLinguistics. https://api.\nProceedingsofthe2022ConferenceoftheNorthAmericanChapteroftheAssociation semanticscholar.org/CorpusID:258865236\nforComputationalLinguistics:HumanLanguageTechnologies.5669â€“5679.\n[6] ZhuoyanLi,HangxiaoZhu,ZhuoranLu,andMingYin.2023. SyntheticData\nGenerationwithLargeLanguageModelsforTextClassification:Potentialand\nLimitations.Proceedingsofthe2023ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing(EMNLP)(2023).\n[7] JunhuaLiuandBinFu.2024.ResponsibleMultilingualLargeLanguageModels:\nASurveyofDevelopment,Applications,andSocietalImpact.ArXiv(2024).\n[8] JunhuaLiu,KwanHuiLim,andRoyKa-WeiLee.2024.TowardsObjectiveand\nUnbiasedDecisionAssessmentswithLLM-EnhancedHierarchicalAttention\nNetworks.arXivpreprintarXiv:2411.08504(2024).\n[9] JunhuaLiu,YongKeatTan,BinFu,andKwanHuiLim.2024.LARA:Linguistic-\nAdaptiveRetrieval-AugmentationforMulti-TurnIntentClassification.Proceed-\ningsoftheEmpiricalMethodsinNaturalLanguageProcessing(2024).\n[10] LefterisLoukas,IliasStogiannidis,OdysseasDiamantopoulos,ProdromosMalaka-\nsiotis,andStavrosVassos.2023. Makingllmswortheverypenny:Resource-\nlimitedtextclassificationinbanking.InProceedingsoftheFourthACMInterna-\ntionalConferenceonAIinFinance.392â€“400.\n[11] Xuan-PhiNguyen,WenxuanZhang,XinLi,MahaniAljunied,ZhiqiangHu,\nChenhuiShen,YewKenChia,XingxuanLi,JianyuWang,QingyuTan,Liying\nCheng,GuanzhengChen,YueDeng,SenYang,ChaoqunLiu,HangZhang,and\nLidongBing.2024. SeaLLMsâ€“LargeLanguageModelsforSoutheastAsia.\narXiv:2312.00738[cs.CL] https://arxiv.org/abs/2312.00738\n[12] PavelPÅ™ibÃ¡Åˆ,JakubÅ mÃ­d,JosefSteinberger,andAdamMiÅ¡tera.2024.Acompar-\nativestudyofcross-lingualsentimentanalysis.ExpertSystemswithApplications\n247(2024),123247.\n[13] LiboQin,WanxiangChe,YangmingLi,MinghengNi,andTingLiu.2020.Dcr-net:\nAdeepco-interactiverelationnetworkforjointdialogactrecognitionandsenti-\nmentclassification.InProceedingsoftheAAAIconferenceonartificialintelligence,\nVol.34.8665â€“8672.\n[14] XiangyuQin,ZhiyuWu,TingtingZhang,YanranLi,JianLuan,BinWang,Li\nWang,andJinshiCui.2023. Bert-erc:Fine-tuningbertisenoughforemotion\nrecognitioninconversation.InProceedingsoftheAAAIConferenceonArtificial\nIntelligence,Vol.37.13492â€“13500.\n[15] ChenQu,LiuYang,W.BruceCroft,YongfengZhang,JohanneR.Trippas,and\nMinghuiQiu.2019.UserIntentPredictioninInformation-seekingConversations.\nInProceedingsofthe2019ConferenceonHumanInformationInteractionand\nRetrieval(CHIIRâ€™19).ACM. https://doi.org/10.1145/3295750.3298924\n[16] PhillipRust,JonasPfeiffer,IvanVuliÄ‡,SebastianRuder,andIrynaGurevych.2021.\nHowGoodisYourTokenizer?OntheMonolingualPerformanceofMultilingual\nLanguageModels.InProceedingsofthe59thAnnualMeetingoftheAssociationfor\nComputationalLinguisticsandthe11thInternationalJointConferenceonNatural\nLanguageProcessing(Volume1:LongPapers).3118â€“3135.\n[17] WeizhouShen,SiyueWu,YunyiYang,andXiaojunQuan.2021.DirectedAcyclic\nGraphNetworkforConversationalEmotionRecognition.InProceedingsofthe\n59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11th\nInternationalJointConferenceonNaturalLanguageProcessing(Volume1:Long\nPapers).1551â€“1560.\n[18] LiyanTang,PhilippeLaban,andGregDurrett.2024.MiniCheck:EfficientFact-\nCheckingofLLMsonGroundingDocuments.ArXivabs/2404.10774(2024).\n[19] ZhiqiangWang,YiranPang,andYanbinLin.2024.SmartExpertSystem:Large\nLanguageModelsasTextClassifiers.arXivpreprintarXiv:2405.10523(2024).\n[20] FushengWei,RobertKeeling,NathanielHuber-Fliflet,JianpingZhang,Adam\nDabrowski,JingchaoYang,QiangMao,andHanQin.2023.Empiricalstudyof\nLLMfine-tuningfortextclassificationinlegaldocumentreview.In2023IEEE\nInternationalConferenceonBigData(BigData).IEEE,2786â€“2792.\n[21] JerryWei,LeHou,AndrewLampinen,XiangningChen,DaHuang,YiTay,Xinyun\nChen,YifengLu,DennyZhou,TengyuMa,andQuocV.Le.2023.Symboltuning\nimprovesin-contextlearninginlanguagemodels. arXiv:2305.08298[cs.CL]\nhttps://arxiv.org/abs/2305.08298",
    "pdf_filename": "Balancing_Accuracy_and_Efficiency_in_Multi-Turn_Intent_Classification_for_LLM-Powered_Dialog_Systems.pdf"
}