{
    "title": "LUTMUL: Exceed Conventional FPGA Roofline Limit by",
    "abstract": "ForFPGA-basedneuralnetworkaccelerators,digitalsignalpro- Units(GPUs)intermsofperformanceandeaseofprogramming. cessing(DSP)blockshavetraditionallybeenthecornerstonefor FPGAreconfigurablelogicmainlyconsistsoflook-uptables(LUT), handling multiplications. This paper introduces LUTMUL, which blockRAMs(BRAMs),anddigitalsignalprocessing(DSP)blocks. harnessesthepotentialoflook-uptables(LUTs)forperforming Togetherwithroutingresources,FPGAcanbereconfiguredfor multiplications.TheavailabilityofLUTstypicallyoutnumbersthat customizeddesigns.Despitetheflexibility,FPGAsfaceconstraints ofDSPsbyafactorof100,offeringasignificantcomputationalad- inclockfrequency,floating-pointperformance,andmemoryband- vantage.ByexploitingthisadvantageofLUTs,ourmethoddemon- width.ThisperformancegapbetweenFPGAsandGPUsisbecoming stratesapotentialboostintheperformanceofFPGA-basedneural evenlargerwhenconsideringthetensorcoreperformanceofGPUs. networkacceleratorswithareconfigurabledataflowarchitecture. Toaddressthis,weneedanalgorithm-hardwareco-designmethod Ourapproachchallengestheconventionalpeakperformanceon toboostFPGAswithgreaterinferencecapability. DSP-based accelerators and sets a new benchmark for efficient FPGA accelerators can follow GPU-like [32, 34] architecture, neuralnetworkinferenceonFPGAs.Experimentalresultsdemon- which maps computation to compute cores with repetitive use. stratethatourdesignachievesthebestinferencespeedamong Whilebeneficial,thisapproachencountersmemorybandwidthis- allFPGA-basedaccelerators,achievingathroughputof1627im- suessimilartoGPUs.ComparedwithGPUs,FPGAsusuallyhave agespersecondandmaintainingatop-1accuracyof70.95%onthe lowermemorybandwidth,andthelowerclockfrequencyofFPGAs ImageNetdataset. meansalowerupperboundofperformance.WhileFPGA-based acceleratorswithspecificinstructionsetarchitectures[37]offer CCSCONCEPTS flexibilityacrossdifferentmodels,theyoftencompromiseonper- formanceduetonon-optimizedcomputekernelsforspecificneural â€¢Hardwareâ†’ReconfigurablelogicandFPGAs;â€¢Computing networklayers. methodologiesâ†’Machinelearning. TobridgetheperformancegapbetweenFPGAsandGPUs,par- ticularlyindeeplearningapplications,weintroduceLUTMUL,which",
    "body": "LUTMUL: Exceed Conventional FPGA Roofline Limit by\nLUT-based Efficient MULtiplication for Neural Network Inference\nYanyueXie ZhengangLi DanaDiaconu\nNortheasternUniversity Adobe NortheasternUniversity\nxie.yany@northeastern.edu li.zhen@northeastern.edu diaconu.d@northeastern.edu\nSurangaHandagala MiriamLeeser XueLin\nNortheasternUniversity NortheasternUniversity NortheasternUniversity\ns.handagala@northeastern.edu mel@coe.neu.edu xue.lin@northeastern.edu\nABSTRACT 16, 38] tasks. However, FPGAs lag behind Graphics Processing\nForFPGA-basedneuralnetworkaccelerators,digitalsignalpro- Units(GPUs)intermsofperformanceandeaseofprogramming.\ncessing(DSP)blockshavetraditionallybeenthecornerstonefor FPGAreconfigurablelogicmainlyconsistsoflook-uptables(LUT),\nhandling multiplications. This paper introduces LUTMUL, which blockRAMs(BRAMs),anddigitalsignalprocessing(DSP)blocks.\nharnessesthepotentialoflook-uptables(LUTs)forperforming Togetherwithroutingresources,FPGAcanbereconfiguredfor\nmultiplications.TheavailabilityofLUTstypicallyoutnumbersthat customizeddesigns.Despitetheflexibility,FPGAsfaceconstraints\nofDSPsbyafactorof100,offeringasignificantcomputationalad- inclockfrequency,floating-pointperformance,andmemoryband-\nvantage.ByexploitingthisadvantageofLUTs,ourmethoddemon- width.ThisperformancegapbetweenFPGAsandGPUsisbecoming\nstratesapotentialboostintheperformanceofFPGA-basedneural evenlargerwhenconsideringthetensorcoreperformanceofGPUs.\nnetworkacceleratorswithareconfigurabledataflowarchitecture. Toaddressthis,weneedanalgorithm-hardwareco-designmethod\nOurapproachchallengestheconventionalpeakperformanceon toboostFPGAswithgreaterinferencecapability.\nDSP-based accelerators and sets a new benchmark for efficient FPGA accelerators can follow GPU-like [32, 34] architecture,\nneuralnetworkinferenceonFPGAs.Experimentalresultsdemon- which maps computation to compute cores with repetitive use.\nstratethatourdesignachievesthebestinferencespeedamong Whilebeneficial,thisapproachencountersmemorybandwidthis-\nallFPGA-basedaccelerators,achievingathroughputof1627im- suessimilartoGPUs.ComparedwithGPUs,FPGAsusuallyhave\nagespersecondandmaintainingatop-1accuracyof70.95%onthe lowermemorybandwidth,andthelowerclockfrequencyofFPGAs\nImageNetdataset. meansalowerupperboundofperformance.WhileFPGA-based\nacceleratorswithspecificinstructionsetarchitectures[37]offer\nCCSCONCEPTS flexibilityacrossdifferentmodels,theyoftencompromiseonper-\nformanceduetonon-optimizedcomputekernelsforspecificneural\nâ€¢Hardwareâ†’ReconfigurablelogicandFPGAs;â€¢Computing\nnetworklayers.\nmethodologiesâ†’Machinelearning.\nTobridgetheperformancegapbetweenFPGAsandGPUs,par-\nticularlyindeeplearningapplications,weintroduceLUTMUL,which\nKEYWORDS\nleveragesthelook-uptablesonFPGAsfordeeplearningtasks,fo-\nFPGAs,Quantization,Look-uptables,Rooflinemodel.\ncusingonacceleratingconvolutionalneuralnetworks(CNNs).We\nACMReferenceFormat: recognizethatthetraditionalFPGAdesigns,heavilydependenton\nYanyueXie,ZhengangLi,DanaDiaconu,SurangaHandagala,MiriamLeeser, DSPblocks,maynotfullyexploittheparallelismandflexibility\nandXueLin.2025.LUTMUL:ExceedConventionalFPGARooflineLimit thatLUTsoffer,astheavailabilityofLUTstypicallyoutnumbers\nbyLUT-basedEfficientMULtiplicationforNeuralNetworkInference.In DSPsbyafactorof100.Ourmethodemphasizesanovelutiliza-\n30thAsiaandSouthPacificDesignAutomationConference(ASPDACâ€™25), tionofLUTstoenhancecomputationalefficiencyandthroughput\nJanuary20â€“23,2025,Tokyo,Japan.ACM,NewYork,NY,USA,7pages.\nindeeplearningapplications.Specifically,weembedtheconvolu-\nhttps://doi.org/10.1145/3658617.3697687\ntionalneuralnetworkweightsintoLUTs,wheretheLUTinputis\ntheactivationsandtheLUToutputisthemultiplicationresult.Dif-\n1 INTRODUCTION\nferentfromLUT-basedgeneralmultipliers,ourmethodisefficient\nField-ProgrammableGateArrays(FPGAs)havebeenwidelyused inresources(requiringjust2LUTsforasingle4-bitmultiplication)\nasdeeplearningaccelerators,facilitatingadvancementsincom- andhelpsfullyexploittheparallelism.\nputervision[7,17,35,39]andnaturallanguageprocessing[4,13, WeproposeareconfigurabledataflowarchitectureforourLUT-\nbased efficient multiplication kernel. Our dataflow architecture\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor\nminimizesthememoryaccesstimebyprocessingthedataon-chip\nclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed\nforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation througheachlayerwithoutexternalmemory.Thereconfigurability\nonthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored. oftheFPGAallowsustotailorthearchitecturespecificallyforeach\nForallotheruses,contacttheowner/author(s).\ndistinctlayerofCNNs.WithLUTresources,thegeneratedacceler-\nASPDACâ€™25,January20â€“23,2025,Tokyo,Japan\nÂ©2025Copyrightheldbytheowner/author(s). atorcanpotentiallyexceedthepeakperformanceofconventional\nACMISBN979-8-4007-0635-6/25/01 DSP-basedFPGAaccelerators.Ourdataflowarchitectureaimsto\nhttps://doi.org/10.1145/3658617.3697687\n4202\nvoN\n1\n]RA.sc[\n1v25811.1142:viXra\nASPDACâ€™25,January20â€“23,2025,Tokyo,Japan YanyueXie,ZhengangLi,DanaDiaconu,SurangaHandagala,MiriamLeeser,andXueLin\nenhancetheoverallefficiencyofourdeeplearningaccelerators, Table1:ComparisonbetweenGPUsandFPGAs.BothV100\noptimizingFPGAsfordeeplearningtasks. andU280arecomparedusingthePCIeversion.Performance\nOurcontributionscanbesummarizedasfollows: isthetheoreticalpeakextractedfromcorrespondingproduct\ndatasheet[19,33].\nâ€¢ WepresentLUTMUL,analgorithm-hardwareco-designmethod\nthatembedsquantizedneuralnetworkweightsintolook-up\ntablesforefficientmultiplicationsandusesdedicatedlook-up Devices V100GPU AlveoU280FPGA\ntablesforfullparallelism. Technology 12nm 16nm\nâ€¢ Wedesignareconfigurabledataflowarchitecturethatex- Clock 1530MHz 300MHz\nploitsscalabilityandLUTpotentialtosavecomputational 5120CUDAcores\nComputecores 9024DSP48E2\nresources. 640Tensorcores\nâ€¢ UsingLUTMUL,FPGAdesignscanpotentiallyexceedthepeak 14TFLOPs(FP32CUDA)\nPerformance 24.5TOPs(INT8)\nperformanceofconventionalDSP-basedFPGAaccelerators 112TFLOPs(FP16Tensor)\nwhenusingthesameamountofresources. 32GBDDR4\nMemory 32GBHBM2\n8GBHBM2\n2 BACKGROUND 38GB/s(DDR4)\nBandwidth 900GB/s\n460GB/s(HBM2)\n2.1 RooflineModelAnalysis\n225W(Max)\nPower 250W\nGPUsleverageSingleInstructionMultipleData(SIMD)architecture, 100W(Typical)\nallowingthemtosimultaneouslyperformthesameoperationacross Price $11,458 $7,717\nmultipledatapoints.ThisparallelismmakesGPUsexceptionally\nefficientfortasksthatcanbedividedintosmaller,similaroperations,\nFigure1showstherooflinemodelforU280.Wetake 1 resource\nsuchasmatrixmultiplicationindeeplearning. 64\nandHBMbandwidthofU280foranalysis.ConventionalDSP-based\nFPGAs,bycontrast,achieveparallelprocessingthroughtheir\nacceleratorsarecomputeboundwhenthearithmeticintensitysat-\nreconfigurability,allowinghardwaretobetailoredtospecificcom-\nisfiesathreshold.OurLUTMULexploitsthepotentialofLUTsand\nputationaltasks.ThisflexibilityallowsFPGAstoefficientlyhandle\ncanachievehigherparallelismbyourLUTefficientmapping.\ncomplexanddiversedataprocessingtasks,offeringadvantages\noverthefixedarchitectureofGPUs.WhileFPGAslacktheraw\nRoofline Model Analysis for DSP and LUTMUL Theoretical Performance\nSIMDpowerofGPUsforcertainapplications,theyexcelinsce- Peak Memory Bandwidth\nnariosrequiringcustomhardwareconfigurationsorlow-latency, Peak Performance LUTMUL\nsuchasspecificsignalprocessingtasksorcustommachinelearning DSP\nalgorithms.However,thisadaptabilityoftencomeswithatrade-off\ninprocessingspeedandeaseofprogramming,withFPGAstypically\nlaggingbehindthecomputationalthroughputofGPUs.\nTherooflinemodel[31]isausefultoolforanalyzingtheperfor-\nmanceofbothGPUsandFPGAs.AnalgorithmrunningonGPUsor\nFPGAscanbeeithercomputeboundormemorybound.According 102 7.19 GB/s\ntotherooflinemodel[39],thepeakperformanceofFPGAsis:\nğ‘ƒğ‘’ğ‘ğ‘˜ğ‘ğ‘’ğ‘Ÿğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘›ğ‘ğ‘’ =ğ‘Ã—ğ‘ƒğ¸ğ‘ Ã—2Ã—ğ‘“, (1)\n101 102\nwhereğ‘ƒğ¸ğ‘ isthenumberofprocessingelements(PEs)usedinthe Operational Intensity (Ops/Byte)\naccelerator,suchastheDSPblocks,ğ‘“ istheclockfrequency,and Figure1:RooflinemodelanalysisforLUTMULandother\nÃ—2termaccountsformultiply-accumulate(MAC)operations.The DSP-basedarchitectures.Wetake 1 resourceandmemory\npackingfactorforDSPblocks,ğ‘,variesbasedonthebit-widthof\nbandwidthofU280foranalysis.\n64\ntheoperation,withğ‘ =1for16-bit,ğ‘ =2for8-bit,andğ‘ =4for\n4-bitmultiply-accumulateoperations.\nFurthermore, the performance of an FPGA-based accelerator 2.2 DataflowArchitecture\nis also limited by the memory, which is related to the memory\nDataflowarchitecturecontrastswiththetraditionalcontrolflow\nbandwidth(BW)andcomputation-to-communication(CTC)ratio:\narchitecture.Thedataflownodesorprocessingelementscanim-\nğ‘ƒğ‘’ğ‘ğ‘˜ğ‘šğ‘’ğ‘šğ‘œğ‘Ÿğ‘¦ğ‘ğ‘ğ‘›ğ‘‘ğ‘¤ğ‘–ğ‘‘ğ‘¡â„=ğµğ‘Š Ã—ğ¶ğ‘‡ğ¶ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œ. (2) mediatelystartandexecutewhenallitsinputsareready.Dataflow\narchitectureemployssimpleoperations,suchasbroadcast(one-\nTable1summarizesthemajordifferencesbetweenGPUsand to-many),map(element-wise,e.g.activationfunction),zip(multi-\nFPGAs,suchasclockfrequency,numberofcomputecores,and operands,e.g.convolutionandmatrixmultiplication),andreduce\nmemorybandwidth.Thesignificantdifferenceinclockfrequency (many-to-one,e.g.pooling)[22].Akeyadvantageofreconfigurable\ncontributestoanotableperformancegapbetweenGPUsandFPGAs. dataflowarchitectureisitsabilitytoallowdatatoflowthroughthe\nEven with optimization such as pruning and quantization [10], computationgraphefficiently,significantlyenhancingparallelism\nFPGAinferencespeedgenerallyremainsinferiortothatofGPUs. andminimizingmemoryaccesstime.\n)sPOG(\necnamrofreP\nLUTMUL:ExceedConventionalFPGARooflineLimitbyLUT-basedEfficientMULtiplicationforNeuralNetworkInference ASPDACâ€™25,January20â€“23,2025,Tokyo,Japan\n2.3 FPGA-basedNeuralNetworkAccelerator 3.2 LUTMULDesignFlow\nArchitecture\nSinceFPGAshaverelativelylimitedon-chipresources,mostofthe\nFPGAaccelerators[3,24,32,34,37]mapcomputationontohard- Quantization-Aware\nQuantization-aware training\nwareandreusethePEarray.Notably,[18]explorestheintra-layer Training\nmixed-schemequantizationandmapsvisiontransformerlayers\nontoaGeneralMatrixMultiply(GEMM)kernel,whereeachlayer Exported quantized ONNX layers\nmaintains afixed ratioof this quantizationscheme. Systolicar- Import, streamlining transformations, reordering\nrayarchitecture[30]presentsanothermethodforefficientlymap- Hardware\nConvert to HLS layers using HLS templates\nGeneration\npingconvolutionandmatrixmultiplicationontoFPGAswithhigh\nAdjust folding for performance/resource requirements\nthroughput.\nStitched IP design\nFINN[2,26]usesadataflowarchitectureandintegratesalllayers Generate IP cores, stitch IP design by Vivado\nofthenetworkintoasingleFPGA.Theresourcesforeachlayer\ncanbeadjustedaccordingtocomputationrequirementssothatall Deployment PYNQ-provided Python abstractions and drivers\nlayersarebalancedandpipelinedforbetterthroughput.FINNis\nparticularlywell-suitedforexploitinginter-layerquantizationfor\nneuralnetworksbecauseeachlayerhasdedicatedcomputationand\nmemoryresources.\n3 ALGORITHM-HARDWARECO-DESIGNFOR\nLUTMUL Figure3:LUTMULDesignflow.\n3.1 Motivation\nFigure3depictstheLUTMULdesignflow.Initially,wetrainthe\nTherooflinemodelrevealsatheoreticalpeakperformanceforDSP-\nneuralnetworkinourquantization-awaretrainingframework.The\nbasedaccelerators,applicableacrossvariousarchitecturessuchas\nquantizationbit-widthsforweightsandactivationsareadjustable\nGEMM,systolicarray,ordataflowarchitecture.Wecanleverage\nhyperparameters.Thefinalquantizedneuralnetworkisexported\nLUTresourcestoperformmultiplicationandfullparallelismto\ninOpenNeuralNetworkExchange(ONNX)format,facilitating\nenableFPGAwithgreaterperformance.Giventhattheavailability\nsubsequenthardwaregeneration.\nof LUTs usually outnumbers DSPs, using LUTs can potentially\nTheONNXintermediaterepresentationisinterpretedasacom-\nexceedtheupperboundofperformanceofcurrentDSP-basedFPGA\nputationgraphandundergoesastreamliningtransformation[27].\naccelerators.\nThescalingfactorsofeachchannelandbatchnormalizationlayer\narereorderedandabsorbedintotheactivationfunction,transform-\nAccuracylossandLUTresourcesfor1-bit to 8-bit quantization\n16% 70 ingintoamulti-thresholdunit.Eachcomputationnodeisconverted\ntoaHigh-LevelSynthesis(HLS)layerusingourHLStemplates.\n14% 60\nTheseHLSlayersarefoldedaccordingtoperformanceandresource\n12%\n50 requirementsandinterconnectedsequentially.Thefinalhardware\n10%\n40 bitstream,generatedbyVivado,isdeployedonFPGAboardsvia\nAc loc su sra â†“cy 8% mu# ltL ipU liT cs atp ioe nr â†“ thePYNQframework.\n30\n6%\n4% 20 3.3 ReconfigurableDataflowArchitecture\n2% 10 Figure4illustratesthehardwarearchitectureofaMobileNetV2[23]\n0% 0 implementation.Thisdesign,focusingoninvertedresidualblocks,\n1-bit 2-bit 3-bit 4-bit 5-bit 6-bit 7-bit 8-bit\nQuantizationbit-width employsaFirstIn,FirstOut(FIFO)bufferbetweenlayerstostore\nactivations.Thearchitectureusesareconfigurabledataflowarchi-\n# LUTs per multiplication Accuracy loss\ntecture.\nFigure2:AccuracylossandLUTresourcesfor1-bitto8-bit OurdesignspansallSuperLogicRegions(SLRs)tomaximize\nquantization. hardwareresources.SignalsonlytraverseSLRswhenthecurrent\nSLRresourcesareinsufficientforthenextlayertoavoidseveretim-\nFigure2showsthequantizedneuralnetworkaccuracy[14,36] ingviolations.Dataflowarchitectureisinherentlysuitedfordesign\nandthenumberofLUTsneededpermultiplicationbyourmethod. spanningmultipleSLRsandcanbescaledup,enablingadditional\nWetrade-offbetweenaccuracyandLUTusageandchoose4-bitas FPGAsconnectedvianetworkfordeployinglargernetworks[6].\nourquantizationbit-width.Binaryandternaryneuralnetworks\n3.4 ConvolutionGenerator\nincurlargeaccuracydropsandconsumehalfoftheLUTsthat4-bit\nusesastheoutputbitsofLUTsarelimited.Comparedwithhigher Forconvolutionallayers,theconvolutionoperationscanbelowered\nbit-widthquantization,4-bitusessignificantlyfewerLUTsandhas tomatrix-matrixmultiplications.Thesecanbemappedinastream-\nnegligibleaccuracyloss. ingmannerandfedtothemultiplicationkernel.Themultiplication\nASPDACâ€™25,January20â€“23,2025,Tokyo,Japan YanyueXie,ZhengangLi,DanaDiaconu,SurangaHandagala,MiriamLeeser,andXueLin\nAlgorithm1showsthepseudoHigh-LevelSynthesis(HLS)code\nSliding forlook-uptablebasedmultiplication.Thetablecontentsarede-\nWindow LUT-based MUL\nrivedfrompre-computedweights.Theweightsofconvolutional\nUnit\nFIFO FIFO Add with bias SR te res au mlt layersarefullyparalleled,meaningthattheğ¶ğ‘‚ğ‘ˆğ‘‡ channelinAlgo-\nMax Threshold Memory rithm1referstotheoutputchannels,andtheğ¶ğ¼ğ‘ channelrefersto\noperator theinputchannelstimesthekernelsizesquared.Thesedimensions\nSLR2 (fourintotal)arefullyunrolledinthespatialdomain.Theremain-\ninginputfeaturemapheightandwidthdimensionsarepipelinedin\nPointwise Conv Generator Depthwise Conv Generator\nthetemporaldomain.Inputactivationsarestreamedfromthecon-\nDDR4 LUT-based MUL\nFIFO\nLUT-based MUL v reo sl uu lt ti son arg ee mn uer lta it po lr ica an tid onpa ress se ud ltst .h Tro hu eygh arl eoo ak cc-u up mt ua lb al te es d. ,T gh oe tho ru ot up gu ht\nAdder tree Adder tree\nthethresholdunit,andgenerateactivationsforthenextlayer.\nThreshold Memory Threshold Memory\nSLR1\nAlgorithm1Look-uptablebasedmultiplicationkernel\nPointwise Conv Generator\nInput: StreamingparallelinputfromtheConvolutionGeneratorandpre-\ncomputedlook-uptablecontents\nDDR4 SIm trea ag me LUT A- db da es re d\ntr\neM eUL FIFO O 1u\n:\nt fp ou rt ğ‘–: â†Str 1ea ğ‘¡m ğ‘œi ğ‘…n ğ‘‚g ğ‘Šou ğ‘†tp Ã—ut ğ¶f ğ‘‚or ğ¿t ğ‘†he dn oextlayer\n2: #pragmaHLSPIPELINEII=1\nThreshold Memory SLR0 3: ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ â†ğ‘ ğ‘Ÿğ‘.ğ‘Ÿğ‘’ğ‘ğ‘‘()\n4: forğ‘ğ‘œâ†1ğ‘¡ğ‘œğ¶ğ‘‚ğ‘ˆğ‘‡ do\nHBM HBM\n5: #pragmaHLSUNROLL\n6: forğ‘ğ‘–â†1ğ‘¡ğ‘œğ¶ğ¼ğ‘ do\nFigure4:Hardwarearchitectureofacceleratorgeneratedby 7: #pragmaHLSUNROLL\nLUTMUL.Ourdesignisfullyon-chipanddoesnotuseDRAM 8: ğ‘šğ‘¢ğ‘™[ğ‘ğ‘œ][ğ‘ğ‘–]â†ğ‘™ğ‘¢ğ‘¡[ğ‘ğ‘œ][ğ‘ğ‘–][ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡[ğ‘ğ‘–]]\norHBMmemory. 9: endfor\n10: endfor\n11: forğ‘ğ‘œâ†1ğ‘¡ğ‘œğ¶ğ‘‚ğ‘ˆğ‘‡ do\n12: #pragmaHLSUNROLL\nkernelisfullyparalleledtoperformamatrix-vectormultiplication, 13: forğ‘ğ‘–â†1ğ‘¡ğ‘œğ¶ğ¼ğ‘ do\nwheretheweightsarestationaryvectorsandactivationsarestream- 14: #pragmaHLSUNROLL\ninginputs.Therefore,weneedaconvolutiongeneratortoperform 15: ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡[ğ‘ğ‘œ]+=ğ‘šğ‘¢ğ‘™[ğ‘ğ‘œ][ğ‘ğ‘–]\ntheim2coloperations:readingdatafromFIFO,movingacrossinput 16: endfor\n17: endfor\nimagestoformanimagematrix,andstreamingtheoutputtothe\n18: ğ‘‘ğ‘ ğ‘¡.ğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’(ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡)\nmultiplicationkernel.\n19: endfor\nTheconvolutiongeneratoraccommodatesvariousconfigura-\ntions,includingpointwise,depthwise,andstandardconvolution\nwithdifferentkernelsizesandstrides,sinceeachkindofconvolu-\n3.6 Quantization-AwareTraining\ntionallayerexpectsdifferentinputdatasequences,necessitating\nspecificgeneratorsettings. Quantization[10]andDSPpacking[24]havebecomeastandard\napproachformappingneuralnetworksontoFPGA-basedaccel-\n3.5 Look-UpTablebasedEfficient erators, as FPGAsâ€™ LUTs and DSP blocks are not optimized for\nMultiplication floating-pointbutidealforintegerorfixed-pointoperations.Quan-\ntization,pairedwithDSPpacking,reducesresourcedemandsfor\nFigure5demonstratesthelook-uptablebasedmultiplicationker-\nthemultiplicationsandimprovesthroughput.\nnelsandhowtodeterminelook-uptableinitialization(INIT)values.\nThequantizationoperationisdefinedas:\nAfterembeddingtheweightsintolook-uptables,ourlook-uptables\nğ‘¥\ntransformintoefficientconstantmultipliers[11].Ourlook-uptable ğ‘¦=ğ‘ğ‘¢ğ‘ğ‘›ğ‘¡ğ‘–ğ‘§ğ‘’(ğ‘¥)=ğ‘ğ‘™ğ‘ğ‘šğ‘(ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘( +ğ‘§),ğ‘¦ ğ‘šğ‘–ğ‘›,ğ‘¦ ğ‘šğ‘ğ‘¥), (4)\nğ‘ \nbasedmultiplierisefficientinresources,utilizingonly2LUTson\naverageforasingle4-bitmultiplication,comparedwithageneral whereğ‘¥ is the floating-point value to quantize,ğ‘  is the scaling\nmultiplierthatconsumes13-28LUTsforanequivalentoperation. factoroftheoutputquantizedtensor,andğ‘§ isthezero-pointor\nThechoiceof4-bitquantizationispivotalasitmaintainsmodel quantizationbiascoefficient.Thefunction,ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘,canberound-to-\naccuracyandoptimizeslook-uptableusage,asshowninFigure2. evenorround-to-zero,andğ‘ğ‘™ğ‘ğ‘šğ‘ performsclippinginclusiveof\nWeshowthenumberofLUT6(6:1LUT,6-bitinput,1-bitoutput)for theboundariesğ‘¦ ğ‘šğ‘–ğ‘›andğ‘¦ ğ‘šğ‘ğ‘¥.\nageneraln-bitmultiplication(n:2nLUT,n-bitinput,2n-bitoutput) Forthereverseprocess,tocomputethefloating-pointrepresen-\nusingourmethod: tationofaquantizedvalue,wedefinethedequantizeoperation\n2ğ‘›Ã—2ğ‘› as:\n#ğ¿ğ‘ˆğ‘‡ğ‘  = 1Ã—26 . (3) ğ‘‘ğ‘’ğ‘ğ‘¢ğ‘ğ‘›ğ‘¡ğ‘–ğ‘§ğ‘’(ğ‘¦)=ğ‘ (ğ‘¦âˆ’ğ‘§), (5)\nLUTMUL:ExceedConventionalFPGARooflineLimitbyLUT-basedEfficientMULtiplicationforNeuralNetworkInference ASPDACâ€™25,January20â€“23,2025,Tokyo,Japan\nLUT6_2 #3 LUT6_2 #2 Activation(uint4) Weight=1 OutpMut(uinlt8ti)pWliecigahtti=o-3n Ourtepsuut(ilntt8s)\n1'b1 1'b1 Activation\n4'b0000 8 W'b0 e0 i0 g0 h00 t0 =0 1,WS=08'b W000 e0 i0 g0 h0 t0\n=-3,WS=1\nLUT5 O7 LUT5 O5 4'b0001 8'b00000001 8'b11111101\nWS WS\n4'b0014 0â€™b0000 8'b0 000 0000 010 00000 8'b110 1110 010 000000\n4â€™b0001 00000001 11111101\nLUT5 O6 LUT5 O4 4'b0011 8'b00000011 8'b11110111\n4â€™b0010 00000010 11111010\nA[3:0] 4 A[3:0] 4 4'b010 40 â€™b0011 8'b 000 0000 010 00 0011 8'b11 1110 110 10 10111\nINIT=64'hfffe_0000_fffe_0000 INIT=64'h07fe_0000_f83e_0000 4'b0101 8'b00000101 8'b11110001\n4â€™b0100 00000100 11110100\n6-Input Look-Up Table 6-Input Look-Up Table 4'b01140â€™b0101 8'b0000000011000101 8'b1111011111010001\n4'b01141â€™b0110 8'b0000000011010110 8'b1111011011101110\n4'b10040â€™b0111 8'b0000001000000111 8'b1111011001001011\n1'b1\nLUT6 _2 #1\n1'b1\nLUT6 _2 #0 4'b10041â€™b1000 8'b0000001000011000 8'b1111001101101000\n4â€™b1001 00001001 11100101\nLUT5 O3 LUT5 O1\n4 4' 'b b1 10 01\n14\n10\nâ€™b1010\n8 8' 'b b00 00 000 00 01 100 01\n0\n110\n1010\n8 8' 'b b1 11 11\n1\n00 10\n1\n10 111\n1\n10\n00010\nWS WS 4â€™b1011 00001011 11011111\n4'b1100 8'b00001100 8'b11011100\nLUT5 O2 LUT5 O0 4â€™b1100 00001100 11011100\n4'b11041â€™b1101 8'b0000001010011101 8'b1110111000111001\nA[3:0] 4 INIT=64'h39c6_ ff00_5a5a_f0f0 A[3:0] 4 INIT=64'hcccc_ cccc_aaaa_aaaa 4'b11140â€™b1110 8'b0000000111001110 8'b1110110110010110\n4'b11114â€™b1111 8'b0000000111011111 8'b1110110010110011\n6-Input Look-Up Table 6-Input Look-Up Table\nFigure5:IllustrationofLUTMULforefficientmultiplicationvialook-uptables.Theleft-handsidefiguredemonstrateshow\ntouseLUT6_2primitiveforembeddingmultiplicationresultsofweightsandinputactivations.Theright-handsidetable\ndemonstratesthemultiplicationresultsoftwoexampleweightsandhowtogeneratethecorrespondinglook-uptablecontents.\nTheweights(int4)andmultiplicationoutput(int8)areusingtwoâ€™scomplementrepresentation,whileactivationareallunsignednumbers\n(uint4). The Most Significant Bit (MSB) of LUT6_2 input is configured as â€˜1â€™ to enable two output ports. The bit below the MSB is a\nWeightSelect(WS)signaltoselectbetweentwoweights.Thelowest4-bitinputsserveasactivationinputs.Ourmethodembedstwo\nint4weightsinsidefourLUT6,aresource-efficientapproachcontrastingwiththeLUT6-instantiatedgeneralmultipliers,whichconsume\n6-14Ã—moreLUT6resources.Twousedexampleweightsare1and-3respectively.TheembeddedLUTcontentsforthesefourLUTsare\n64â€™hfffe_0000_fffe_0000,64â€™h07fe_0000_f83e_0000,64â€™h39c6_ff00_5a5a_f0f0,and64â€™hcccc_cccc_aaaa_aaaa,respectively.\nwhereğ‘¦isaquantizedtensor,ğ‘§isitszero-point,andğ‘ isthescaling quantizationschemeforourmodel.OurquantizedMobileNetV2\nfactor. networkistrainedfor420epochs,culminatingina70.95%top-1\nQuantization introduces errors to the trained model parame- accuracyevaluatedontheImageNetdataset[5].\ntersandresultsinperformancedegradation.Quantization-Aware Forthehardwareevaluation,theutilizeddevelopmentplatform\nTraining(QAT)isapopularapproachthatretrainsthemodelwith istheAMDXilinxAlveoU280datacenteracceleratorcardonthe\nquantizedparametersonthepretrainingdatasettoconvergetothe OpenCloudTestbed(OCT)[40].Weimplementthefirst15layers\npretrainedmodelperformance.Theusualforwardandbackward ofMobileNetV2inafullyparallelmannerandfoldtheremaining\npassesareperformedonthequantizedmodelinfloatingpoint,and layers for resource optimization. To maximize the computation\nthemodelparametersarequantizedaftereachgradientupdate. efficiencywithouttimingviolation,theworkingfrequencyissetto\nInparticular,itisimportanttodothisprojectionaftertheweight 333MHzforallthedesignsimplementedthroughVitisHLS/Vivado\nupdateisperformedinfloatingpointprecision.Performingthe 2022.1.\nbackward pass with floating point is vital, as accumulating the\ngradients inquantized precision canresult in zero gradientsor\ngradientswithhigherror,especiallyinlow-precisionscenarios[9].\n4.2 ExperimentalResults\n4 EVALUATION Table2showcasesthehardwareperformanceandcomparisonswith\notherFPGA-basedMobileNetaccelerators.Mostoftheseaccelera-\n4.1 ExperimentalSetup\ntorsaretailoredforedgeFPGAs,suchasZU9EG,exceptforFINN,\nToevaluatetheperformanceofLUTMUL,weimplementMobileNetV2 whichhasdatacenteracceleratorimplementationforMobileNetV1.\nonFPGAsandcompareitwithexistingFPGA-basedMobileNetac- TheFINNresultisgeneratedandtestedonthesamedeviceasour\ncelerators.MobilenetV2[23]has3.4Mparametersandachieves implementation,whileotherdatapointsareextractedfromtheir\n71.88%top-1accuracyontheImageNetdataset[5].Weutilizethe originalpublications.\nFINNframework[2]asourfoundationalplatform.Forquantiza- Intermsofaccuracy,ourmodelachievesthebest70.95%top-1\ntion,weadoptPyTorch1.13.0andBrevitas0.9.1[20].Specifically, accuracyonImageNetamongallimplementations.Quantization-\nwechoose4-bitforweightsandactivationsquantizationexcept awaretrainingeffectivelymitigatesquantizationerrors,preserving\nforthefirstandlastlayers,whicharesetas8-bit.Topreservethe themodeloriginalaccuracy,evenwith4-bitquantizedweightsand\naccuracyoftheMobileNetV2model,weapplythechannel-wise activations.\nASPDACâ€™25,January20â€“23,2025,Tokyo,Japan YanyueXie,ZhengangLi,DanaDiaconu,SurangaHandagala,MiriamLeeser,andXueLin\nTable2:ComparisonsofMobileNetimplementationsbetweenpreviousFPGA-basedaccelerators.\nFINN FPLâ€™19 Light-OPU FPLâ€™21 Mix&Match FILM-QNN LUTMUL\nImplementation\n[2] [32] [37] [34] [3] [24] (Ours)\nNetwork MobileNetV1 MobileNetV2 MobileNetV3 MobileNetV2 MobileNetV2 MobileNetV2 MobileNetV2\nBit-Width W4A4 W8A8 W8A8 W8A8 W4A4 W8A5&W4A5 W4A4\nTop-1Accuracy 70.4% 68.1% 66.7% 70.8% 65.6% 65.7% 70.95%\nPlatform AlveoU280 ZU9EG XC7K325T XC7V690T XC7Z045 ZU9EG AlveoU280\nFrequency(MHz) 333 333 200 150 100 150 333\nLUT 501363 161944 173522 308449 145049 180100 529242\nFF 476316 301416 241175 278926 111575 - 503192\nBRAM36 898 771 193.5 941.5 225.5 440.5 1119\nDSP 106 2070 704 2160 900 2092 106\nPower(W) 41.69 - 8.5* 11.35 - 12.9 42.12\nFrameRate(FPS) 925 809.8 332.6 302.3 549.3 537.9 1627\nThroughput(GOPS) 556.4 487.1 84.48* 181.8 326.9 320.1 978.6\nEnergyEfficiency(GOPS/W) 13.35 - 9.9 16.02 - 24.8 23.23\nNote:â€˜-â€™meansthattheresultisnotgivenintheoriginalpublications,andâ€˜*â€™meansthattheresultisinferredfromtheoriginalpublications.\nAsfortheinferenceperformance,ourimplementationachieves performancebyLUTMULstilloutperformstheDSPpackingmethod\nathroughputof1627imagespersecond.Ourimplementationcon- usingthesamenumberofresources.\nsumesthemostFPGAresourcesbutcouldstillfitonasingleAlveo\nU280.However,itisnoteworthythatourimplementationalsoyields\n4.4 ComparisonswithRelatedWorks\na23.23GOPS/Wenergyefficiency,marginallylowerthantheFLIM-\nQNN[24],whichisimplementedonamorepower-efficientedge Ourmethodisnotonlylimitedtointegermultiplication,butcan\nFPGAboard. also be extended to customized data formats, such as FP4 and\nMXFP4[21],whileDSPpackingisdesignedefficientlyforinteger\n4.3 Discussion\nformats.LUTNet[28,29]alsoutilizesLUTforinferenceandex-\nplorestheflexibilityofLUT.However,LUTNetdesignsuffersfrom\nSynthesis Implementation lowaccuracywhenthenetworkbecomeslarger.PolyLUT[1]trains\nMemory (multiplication) Expression (addition) LUT as ROM Adder logic multivariatepolynomialsinsteadoflinearfunctionsandembeds\nMemory piecewisepolynomialfunctionsintoLUTs.CompressedLUT[15]\n(multiplication)\n1829 proposesalosslessLUTcompressionmethodandisefficientfor\n11%\nnon-linearfunctionsandlargeLUTlogicblocks,suchas[8,12,25].\nOurmethodmapsMACoperationstothesingle-LUTlevel,and\nAdder logic LUT as ROM Vivadocanhandleremaininglogicoptimizationefficiently.\n2645 3277\n45% 55%\nExpression (addition)\n14944 5 CONCLUSION\n89%\nWeproposeLUTMUL,anefficientmethodthatleverageslook-upta-\nblesformultiplicationinconvolutionalneuralnetworks.Compared\nwiththegeneralmultiplier,ourmethodisefficientinresources,\nFigure6:LUTResourcebreakdownofthesecondconvolution\nwhichonlyneedstwolook-uptablesonaverageforasingle4-bit\nlayerinMobileNetV2usingLUTMUL.\nmultiplication.ComparedwithotherDSP-basedFPGAaccelerators,\nLUTMULâ€™sreconfigurabledataflowarchitectureenablesfullparal-\nFigure6showstheLUTresourcebreakdownofthesecondcon- lelism,reducesmemoryaccesstime,andincreasesthetheoretical\nvolutionlayerinMobileNetV2usingLUTMUL,whichisa1Ã—1con- upperboundofperformance.Experimentalresultsdemonstrate\nvolutionkernelandhas32inputchannelsand32outputchannels. thatourdesignmaintainsatop-1accuracyof70.95%ontheIma-\nForthese10244-bitweights,multiplicationoperationsuse1829 geNetdatasetandachievesathroughputof1627imagespersecond\nLUTsafterHLSsynthesis,whichmatchesthetheoreticalanalysis onasingleAlveoU280FPGA,outperformingotherFPGA-based\nofLUTMUL.However,HLSinstantiatesanadderforeachaddition MobileNetaccelerators.\noperationtoachieveanIIof1,resultinginahighusageofLUT\nforadderlogic.AfterVivadoimplementation,theLUTusagede-\nACKNOWLEDGMENTS\ncreasedto5922.Vivadooptimizesthelogicandinstantiates3277\nLUTsasROMand2645LUTsasadderandotherlogic.Eventhough ThisresearchwassupportedinpartbytheNationalScienceFoun-\nadderlogicaccountsforalargepartofresources,theparallelMAC dationunderGrantsCCF-1901378,CNS-1925658,andCNS-2319962.\nLUTMUL:ExceedConventionalFPGARooflineLimitbyLUT-basedEfficientMULtiplicationforNeuralNetworkInference ASPDACâ€™25,January20â€“23,2025,Tokyo,Japan\nREFERENCES\nKristofDenolf,etal.2023.Microscalingdataformatsfordeeplearning.arXiv\n[1] MartaAndronicandGeorgeAConstantinides.2023.PolyLUT:learningpiecewise preprintarXiv:2310.10537(2023).\npolynomialsforultra-lowlatencyFPGALUT-basedinference.In2023Interna- [22] SambanovaWhitepaper.2021.AcceleratedComputingwithaReconfigurable\ntionalConferenceonFieldProgrammableTechnology(ICFPT).60â€“68. Dataflow Architecture. https://sambanova.ai/wp-content/uploads/2021/\n[2] MichaelaBlott,ThomasBPreuÃŸer,NicholasJFraser,GiulioGambardella,Kenneth 04/SambaNova_Accelerated-Computing-with-a-Reconfigurable-Dataflow-\nOâ€™brien,YamanUmuroglu,MiriamLeeser,andKeesVissers.2018.FINN-R:An Architecture_Whitepaper_English.pdf. LastaccessedOct.30,2024.\nend-to-enddeep-learningframeworkforfastexplorationofquantizedneural [23] MarkSandler,AndrewHoward,MenglongZhu,AndreyZhmoginov,andLiang-\nnetworks.ACMTransactionsonReconfigurableTechnologyandSystems(TRETS) ChiehChen.2018. Mobilenetv2:Invertedresidualsandlinearbottlenecks.In\n11,3(2018),1â€“23. ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition\n[3] Sung-EnChang,YanyuLi,MengshuSun,RunbinShi,HaydenK-HSo,Xuehai (CVPR).4510â€“4520.\nQian,YanzhiWang,andXueLin.2021.Mixandmatch:Anovelfpga-centricdeep [24] MengshuSun,ZhengangLi,AlecLu,YanyuLi,Sung-EnChang,XiaolongMa,\nneuralnetworkquantizationframework.In2021IEEEInternationalSymposium XueLin,andZhenmanFang.2022.Film-qnn:Efficientfpgaaccelerationofdeep\nonHigh-PerformanceComputerArchitecture(HPCA).IEEE,208â€“220. neuralnetworkswithintra-layer,mixed-precisionquantization.InProceedings\n[4] HongzhengChen,JiahaoZhang,YixiaoDu,ShaojieXiang,ZichaoYue,Niansong ofthe2022ACM/SIGDAInternationalSymposiumonField-ProgrammableGate\nZhang,YaohuiCai,andZhiruZhang.2024.Understandingthepotentialoffpga- Arrays(FPGA).134â€“145.\nbasedspatialaccelerationforlargelanguagemodelinference.ACMTransactions [25] YamanUmuroglu,YashAkhauri,NicholasJamesFraser,andMichaelaBlott.2020.\nonReconfigurableTechnologyandSystems(TRETS)(2024). LogicNets:Co-designedneuralnetworksandcircuitsforextreme-throughput\n[5] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.2009.Ima- applications.In202030thInternationalConferenceonField-ProgrammableLogic\ngenet:Alarge-scalehierarchicalimagedatabase.InProceedingsoftheIEEE/CVF andApplications(FPL).IEEE,291â€“297.\nConferenceonComputerVisionandPatternRecognition(CVPR).248â€“255. [26] YamanUmuroglu,NicholasJFraser,GiulioGambardella,MichaelaBlott,Philip\n[6] DanaDiaconu,YanyueXie,MehmetGungor,SurangaHandagala,XueLin,and Leong,MagnusJahre,andKeesVissers.2017.Finn:Aframeworkforfast,scal-\nMiriamLeeser.2023.MachineLearningAcrossNetwork-ConnectedFPGAs.In ablebinarizedneuralnetworkinference.InProceedingsofthe2017ACM/SIGDA\n2023IEEEHighPerformanceExtremeComputingConference(HPEC).IEEE,1â€“7. InternationalSymposiumonField-ProgrammableGateArrays(FPGA).65â€“74.\n[7] PeiyanDong,MengshuSun,AlecLu,YanyueXie,KennethLiu,ZhenglunKong, [27] YamanUmurogluandMagnusJahre.2017.Streamlineddeploymentforquantized\nXinMeng,ZhengangLi,XueLin,ZhenmanFang,etal.2023.Heatvit:Hardware- neuralnetworks.arXivpreprintarXiv:1709.04060(2017).\nefficientadaptivetokenpruningforvisiontransformers.In2023IEEEInternational [28] ErweiWang,JamesJDavis,PeterYKCheung,andGeorgeAConstantinides.\nSymposiumonHigh-PerformanceComputerArchitecture(HPCA).IEEE,442â€“455. 2019.LUTNet:RethinkinginferenceinFPGAsoftlogic.In2019IEEE27thAnnual\n[8] DanielGerlinghoff,BenjaminChenMingChoong,RickSiowMongGoh,Weng- InternationalSymposiumonField-ProgrammableCustomComputingMachines\nFaiWong,andTaoLuo.2024.Table-LookupMAC:ScalableProcessingofQuan- (FCCM).IEEE,26â€“34.\ntisedNeuralNetworksinFPGASoftLogic.InProceedingsofthe2024ACM/SIGDA [29] ErweiWang,JamesJDavis,Georgios-IliasStavrou,PeterYKCheung,GeorgeA\nInternationalSymposiumonFieldProgrammableGateArrays(FPGA). Constantinides,andMohamedAbdelfattah.2022.Logicshrinkage:LearnedFPGA\n[9] AmirGholami,SehoonKim,ZhenDong,ZheweiYao,MichaelWMahoney,and netlistsparsityforefficientneuralnetworkinference.InProceedingsofthe2022\nKurtKeutzer.2022.Asurveyofquantizationmethodsforefficientneuralnetwork ACM/SIGDAInternationalSymposiumonField-ProgrammableGateArrays(FPGA).\ninference.InLow-PowerComputerVision.ChapmanandHall/CRC,291â€“326. 101â€“111.\n[10] SongHan,HuiziMao,andWilliamJDally.2016.DeepCompression:Compressing [30] XuechaoWei,CodyHaoYu,PengZhang,YouxiangChen,YuxinWang,Han\nDeepNeuralNetworkswithPruning,TrainedQuantizationandHuffmanCoding. Hu,YunLiang,andJasonCong.2017. Automatedsystolicarrayarchitecture\nInternationalConferenceonLearningRepresentations(ICLR)(2016). synthesisforhighthroughputCNNinferenceonFPGAs.InProceedingsofthe\n[11] MartinHardieck,MartinKumm,KonradMÃ¶ller,andPeterZipf.2019.Reconfig- 54thAnnualDesignAutomationConference(DAC).1â€“6.\nurableconvolutionalkernelsforneuralnetworksonFPGAs.InProceedingsofthe [31] SamuelWilliams,AndrewWaterman,andDavidPatterson.2009.Roofline:an\n2019ACM/SIGDAInternationalSymposiumonField-ProgrammableGateArrays insightfulvisualperformancemodelformulticorearchitectures.Commun.ACM\n(FPGA).43â€“52. 52,4(2009),65â€“76.\n[12] JingkaiHong,ArashFayyazi,AmirhosseinEsmaili,MahdiNazemi,andMassoud [32] DiWu,YuZhang,XijieJia,LuTian,TianpingLi,LingzhiSui,DongliangXie,\nPedram.2023.AlgorithmsandHardwareforEfficientProcessingofLogic-based andYiShan.2019. Ahigh-performanceCNNprocessorbasedonFPGAfor\nNeuralNetworks.InProceedingsofthe60thACM/IEEEDesignAutomationCon- MobileNets.In201929thInternationalConferenceonFieldProgrammableLogic\nference(DAC).IEEE,1â€“6. andApplications(FPL).136â€“143.\n[13] SeongminHong,SeungjaeMoon,JunsooKim,SungjaeLee,MinsubKim,Dongsoo [33] AMDXilinx.2021.AlveoProductSelectionGuide.https://docs.xilinx.com/v/u/en-\nLee,andJoo-YoungKim.2022. DFX:ALow-latencyMulti-FPGAAppliance US/alveo-product-selection-guide. LastaccessedOct.30,2024.\nforAcceleratingTransformer-basedTextGeneration.In202255thIEEE/ACM [34] ShunYan,ZhengyanLiu,YunWang,ChenglongZeng,QiangLiu,BowenCheng,\nInternationalSymposiumonMicroarchitecture(MICRO).IEEE,616â€“630. andRayCCCheung.2021. Anfpga-basedmobilenetacceleratorconsidering\n[14] QingJin,LinjieYang,andZhenyuLiao.2020. Adabits:Neuralnetworkquan- networkstructurecharacteristics.In202131stInternationalConferenceonField-\ntizationwithadaptivebit-widths.InProceedingsoftheIEEE/CVFConferenceon ProgrammableLogicandApplications(FPL).IEEE,17â€“23.\nComputerVisionandPatternRecognition(CVPR).2146â€“2156. [35] GengYang,YanyueXie,ZhongJiaXue,Sung-EnChang,YanyuLi,PeiyanDong,\n[15] AlirezaKhataeiandKiaBazargan.2024. CompressedLUT:AnOpenSource JieLei,WeiyingXie,YanzhiWang,XueLin,andZhenmanFang.2024. SDA:\nToolforLosslessCompressionofLookupTablesforFunctionEvaluationand Low-BitStableDiffusionAccelerationonEdgeFPGAs.In202434thInternational\nBeyond.InProceedingsofthe2024ACM/SIGDAInternationalSymposiumonField ConferenceonField-ProgrammableLogicandApplications(FPL).264â€“273.\nProgrammableGateArrays(FPGA). [36] LinjieYangandQingJin.2021.Fracbits:Mixedprecisionquantizationviafrac-\n[16] BingbingLi,SantoshPandey,HaowenFang,YanjunLyv,JiLi,JieyangChen, tionalbit-widths.InProceedingsoftheAAAIConferenceonArtificialIntelligence\nMimiXie,LipengWan,HangLiu,andCaiwenDing.2020. Ftrans:energy- (AAAI),Vol.35.10612â€“10620.\nefficientaccelerationoftransformersusingfpga.InProceedingsoftheACM/IEEE [37] YunxuanYu,TiandongZhao,KunWang,andLeiHe.2020.Light-OPU:AnFPGA-\nInternationalSymposiumonLowPowerElectronicsandDesign(ISLPED).175â€“180. basedoverlayprocessorforlightweightconvolutionalneuralnetworks.InPro-\n[17] ZhengangLi,AlecLu,YanyueXie,ZhenglunKong,MengshuSun,HaoTang, ceedingsofthe2020ACM/SIGDAInternationalSymposiumonField-Programmable\nZhongJiaXue,PeiyanDong,CaiwenDing,YanzhiWang,etal.2024.Quasar- GateArrays(FPGA).122â€“132.\nViT:Hardware-OrientedQuantization-AwareArchitectureSearchforVision [38] ShulinZeng,JunLiu,GuohaoDai,XinhaoYang,TianyuFu,HongyiWang,\nTransformers.InProceedingsofthe38thACMInternationalConferenceonSuper- WenhengMa,HanboSun,ShiyaoLi,ZixiaoHuang,YadongDai,JintaoLi,Zehao\ncomputing(ICS).324â€“337. Wang,RuoyuZhang,KairuiWen,XuefeiNing,andYuWang.2024.FlightLLM:\n[18] ZhengangLi,MengshuSun,AlecLu,HaoyuMa,GengYuan,YanyueXie,Hao EfficientLargeLanguageModelInferencewithaCompleteMappingFlowon\nTang,YanyuLi,MiriamLeeser,ZhangyangWang,XueLin,andZhenmanFang. FPGA.InProceedingsofthe2024ACM/SIGDAInternationalSymposiumonField\n2022. Auto-vit-acc:Anfpga-awareautomaticaccelerationframeworkforvi- ProgrammableGateArrays(FPGA).\nsiontransformerwithmixed-schemequantization.In202232ndInternational [39] ChenZhang,PengLi,GuangyuSun,YijinGuan,BingjunXiao,andJasonCong.\nConferenceonField-ProgrammableLogicandApplications(FPL).IEEE,109â€“116. 2015.OptimizingFPGA-basedacceleratordesignfordeepconvolutionalneural\n[19] Nvidia. 2017. Nvidia Tesla V100 GPU Architecture Whitepaper. networks.InProceedingsofthe2015ACM/SIGDAInternationalSymposiumon\nhttps://images.nvidia.com/content/volta-architecture/pdf/volta-architecture- FieldProgrammableGateArrays(FPGA).161â€“170.\nwhitepaper.pdf. LastaccessedOct.30,2024. [40] MichaelZink,DavidIrwin,EmmanuelCecchet,HakanSaplakoglu,OrranKrieger,\n[20] AlessandroPappalardo.2023.Xilinx/brevitas. https://doi.org/10.5281/zenodo. Martin Herbordt, Michael Daitzman, Peter Desnoyers, Miriam Leeser, and\n3333552 SurangaHandagala.2021. TheOpenCloudTestbed(OCT):Aplatformfor\n[21] BitaDarvishRouhani,RitchieZhao,AnkitMore,MathewHall,AlirezaKho- researchintonewcloudtechnologies.In2021IEEE10thInternationalConference\ndamoradi,SummerDeng,DhruvChoudhary,MariusCornea,EricDellinger, onCloudNetworking(CloudNet).IEEE,140â€“147.",
    "pdf_filename": "LUTMUL_Exceed_Conventional_FPGA_Roofline_Limit_by_LUT-based_Efficient_Multiplication_for_Neural_Netw.pdf"
}