{
    "title": "LUTMUL Exceed Conventional FPGA Roofline Limit by LUT-based Efficient Multiplication for Neural Netw",
    "context": "For FPGA-based neural network accelerators, digital signal pro- cessing (DSP) blocks have traditionally been the cornerstone for handling multiplications. This paper introduces LUTMUL, which harnesses the potential of look-up tables (LUTs) for performing multiplications. The availability of LUTs typically outnumbers that of DSPs by a factor of 100, offering a significant computational ad- vantage. By exploiting this advantage of LUTs, our method demon- strates a potential boost in the performance of FPGA-based neural network accelerators with a reconfigurable dataflow architecture. Our approach challenges the conventional peak performance on DSP-based accelerators and sets a new benchmark for efficient neural network inference on FPGAs. Experimental results demon- strate that our design achieves the best inference speed among all FPGA-based accelerators, achieving a throughput of 1627 im- ages per second and maintaining a top-1 accuracy of 70.95% on the ImageNet dataset. CCS CONCEPTS • Hardware →Reconfigurable logic and FPGAs; • Computing methodologies →Machine learning. KEYWORDS FPGAs, Quantization, Look-up tables, Roofline model. ACM Reference Format: Yanyue Xie, Zhengang Li, Dana Diaconu, Suranga Handagala, Miriam Leeser, and Xue Lin. 2025. LUTMUL: Exceed Conventional FPGA Roofline Limit by LUT-based Efficient MULtiplication for Neural Network Inference. In 30th Asia and South Pacific Design Automation Conference (ASPDAC ’25), January 20–23, 2025, Tokyo, Japan. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3658617.3697687 1 Field-Programmable Gate Arrays (FPGAs) have been widely used as deep learning accelerators, facilitating advancements in com- puter vision [7, 17, 35, 39] and natural language processing [4, 13, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ASPDAC ’25, January 20–23, 2025, Tokyo, Japan © 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0635-6/25/01 https://doi.org/10.1145/3658617.3697687 16, 38] tasks. However, FPGAs lag behind Graphics Processing Units (GPUs) in terms of performance and ease of programming. FPGA reconfigurable logic mainly consists of look-up tables (LUT), block RAMs (BRAMs), and digital signal processing (DSP) blocks. Together with routing resources, FPGA can be reconfigured for customized designs. Despite the flexibility, FPGAs face constraints in clock frequency, floating-point performance, and memory band- width. This performance gap between FPGAs and GPUs is becoming even larger when considering the tensor core performance of GPUs. To address this, we need an algorithm-hardware co-design method to boost FPGAs with greater inference capability. FPGA accelerators can follow GPU-like [32, 34] architecture, which maps computation to compute cores with repetitive use. While beneficial, this approach encounters memory bandwidth is- sues similar to GPUs. Compared with GPUs, FPGAs usually have lower memory bandwidth, and the lower clock frequency of FPGAs means a lower upper bound of performance. While FPGA-based accelerators with specific instruction set architectures [37] offer flexibility across different models, they often compromise on per- formance due to non-optimized compute kernels for specific neural network layers. To bridge the performance gap between FPGAs and GPUs, par- ticularly in deep learning applications, we introduce LUTMUL, which leverages the look-up tables on FPGAs for deep learning tasks, fo- cusing on accelerating convolutional neural networks (CNNs). We recognize that the traditional FPGA designs, heavily dependent on DSP blocks, may not fully exploit the parallelism and flexibility that LUTs offer, as the availability of LUTs typically outnumbers DSPs by a factor of 100. Our method emphasizes a novel utiliza- tion of LUTs to enhance computational efficiency and throughput in deep learning applications. Specifically, we embed the convolu- tional neural network weights into LUTs, where the LUT input is the activations and the LUT output is the multiplication result. Dif- ferent from LUT-based general multipliers, our method is efficient in resources (requiring just 2 LUTs for a single 4-bit multiplication) and helps fully exploit the parallelism. We propose a reconfigurable dataflow architecture for our LUT- based efficient multiplication kernel. Our dataflow architecture minimizes the memory access time by processing the data on-chip through each layer without external memory. The reconfigurability of the FPGA allows us to tailor the architecture specifically for each distinct layer of CNNs. With LUT resources, the generated acceler- ator can potentially exceed the peak performance of conventional DSP-based FPGA accelerators. Our dataflow architecture aims to arXiv:2411.11852v1  [cs.AR]  1 Nov 2024",
    "body": "LUTMUL: Exceed Conventional FPGA Roofline Limit by\nLUT-based Efficient MULtiplication for Neural Network Inference\nYanyue Xie\nNortheastern University\nxie.yany@northeastern.edu\nZhengang Li\nAdobe\nli.zhen@northeastern.edu\nDana Diaconu\nNortheastern University\ndiaconu.d@northeastern.edu\nSuranga Handagala\nNortheastern University\ns.handagala@northeastern.edu\nMiriam Leeser\nNortheastern University\nmel@coe.neu.edu\nXue Lin\nNortheastern University\nxue.lin@northeastern.edu\nABSTRACT\nFor FPGA-based neural network accelerators, digital signal pro-\ncessing (DSP) blocks have traditionally been the cornerstone for\nhandling multiplications. This paper introduces LUTMUL, which\nharnesses the potential of look-up tables (LUTs) for performing\nmultiplications. The availability of LUTs typically outnumbers that\nof DSPs by a factor of 100, offering a significant computational ad-\nvantage. By exploiting this advantage of LUTs, our method demon-\nstrates a potential boost in the performance of FPGA-based neural\nnetwork accelerators with a reconfigurable dataflow architecture.\nOur approach challenges the conventional peak performance on\nDSP-based accelerators and sets a new benchmark for efficient\nneural network inference on FPGAs. Experimental results demon-\nstrate that our design achieves the best inference speed among\nall FPGA-based accelerators, achieving a throughput of 1627 im-\nages per second and maintaining a top-1 accuracy of 70.95% on the\nImageNet dataset.\nCCS CONCEPTS\n• Hardware →Reconfigurable logic and FPGAs; • Computing\nmethodologies →Machine learning.\nKEYWORDS\nFPGAs, Quantization, Look-up tables, Roofline model.\nACM Reference Format:\nYanyue Xie, Zhengang Li, Dana Diaconu, Suranga Handagala, Miriam Leeser,\nand Xue Lin. 2025. LUTMUL: Exceed Conventional FPGA Roofline Limit\nby LUT-based Efficient MULtiplication for Neural Network Inference. In\n30th Asia and South Pacific Design Automation Conference (ASPDAC ’25),\nJanuary 20–23, 2025, Tokyo, Japan. ACM, New York, NY, USA, 7 pages.\nhttps://doi.org/10.1145/3658617.3697687\n1\nINTRODUCTION\nField-Programmable Gate Arrays (FPGAs) have been widely used\nas deep learning accelerators, facilitating advancements in com-\nputer vision [7, 17, 35, 39] and natural language processing [4, 13,\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nASPDAC ’25, January 20–23, 2025, Tokyo, Japan\n© 2025 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0635-6/25/01\nhttps://doi.org/10.1145/3658617.3697687\n16, 38] tasks. However, FPGAs lag behind Graphics Processing\nUnits (GPUs) in terms of performance and ease of programming.\nFPGA reconfigurable logic mainly consists of look-up tables (LUT),\nblock RAMs (BRAMs), and digital signal processing (DSP) blocks.\nTogether with routing resources, FPGA can be reconfigured for\ncustomized designs. Despite the flexibility, FPGAs face constraints\nin clock frequency, floating-point performance, and memory band-\nwidth. This performance gap between FPGAs and GPUs is becoming\neven larger when considering the tensor core performance of GPUs.\nTo address this, we need an algorithm-hardware co-design method\nto boost FPGAs with greater inference capability.\nFPGA accelerators can follow GPU-like [32, 34] architecture,\nwhich maps computation to compute cores with repetitive use.\nWhile beneficial, this approach encounters memory bandwidth is-\nsues similar to GPUs. Compared with GPUs, FPGAs usually have\nlower memory bandwidth, and the lower clock frequency of FPGAs\nmeans a lower upper bound of performance. While FPGA-based\naccelerators with specific instruction set architectures [37] offer\nflexibility across different models, they often compromise on per-\nformance due to non-optimized compute kernels for specific neural\nnetwork layers.\nTo bridge the performance gap between FPGAs and GPUs, par-\nticularly in deep learning applications, we introduce LUTMUL, which\nleverages the look-up tables on FPGAs for deep learning tasks, fo-\ncusing on accelerating convolutional neural networks (CNNs). We\nrecognize that the traditional FPGA designs, heavily dependent on\nDSP blocks, may not fully exploit the parallelism and flexibility\nthat LUTs offer, as the availability of LUTs typically outnumbers\nDSPs by a factor of 100. Our method emphasizes a novel utiliza-\ntion of LUTs to enhance computational efficiency and throughput\nin deep learning applications. Specifically, we embed the convolu-\ntional neural network weights into LUTs, where the LUT input is\nthe activations and the LUT output is the multiplication result. Dif-\nferent from LUT-based general multipliers, our method is efficient\nin resources (requiring just 2 LUTs for a single 4-bit multiplication)\nand helps fully exploit the parallelism.\nWe propose a reconfigurable dataflow architecture for our LUT-\nbased efficient multiplication kernel. Our dataflow architecture\nminimizes the memory access time by processing the data on-chip\nthrough each layer without external memory. The reconfigurability\nof the FPGA allows us to tailor the architecture specifically for each\ndistinct layer of CNNs. With LUT resources, the generated acceler-\nator can potentially exceed the peak performance of conventional\nDSP-based FPGA accelerators. Our dataflow architecture aims to\narXiv:2411.11852v1  [cs.AR]  1 Nov 2024\n\nASPDAC ’25, January 20–23, 2025, Tokyo, Japan\nYanyue Xie, Zhengang Li, Dana Diaconu, Suranga Handagala, Miriam Leeser, and Xue Lin\nenhance the overall efficiency of our deep learning accelerators,\noptimizing FPGAs for deep learning tasks.\nOur contributions can be summarized as follows:\n• We present LUTMUL, an algorithm-hardware co-design method\nthat embeds quantized neural network weights into look-up\ntables for efficient multiplications and uses dedicated look-up\ntables for full parallelism.\n• We design a reconfigurable dataflow architecture that ex-\nploits scalability and LUT potential to save computational\nresources.\n• Using LUTMUL, FPGA designs can potentially exceed the peak\nperformance of conventional DSP-based FPGA accelerators\nwhen using the same amount of resources.\n2\nBACKGROUND\n2.1\nRoofline Model Analysis\nGPUs leverage Single Instruction Multiple Data (SIMD) architecture,\nallowing them to simultaneously perform the same operation across\nmultiple data points. This parallelism makes GPUs exceptionally\nefficient for tasks that can be divided into smaller, similar operations,\nsuch as matrix multiplication in deep learning.\nFPGAs, by contrast, achieve parallel processing through their\nreconfigurability, allowing hardware to be tailored to specific com-\nputational tasks. This flexibility allows FPGAs to efficiently handle\ncomplex and diverse data processing tasks, offering advantages\nover the fixed architecture of GPUs. While FPGAs lack the raw\nSIMD power of GPUs for certain applications, they excel in sce-\nnarios requiring custom hardware configurations or low-latency,\nsuch as specific signal processing tasks or custom machine learning\nalgorithms. However, this adaptability often comes with a trade-off\nin processing speed and ease of programming, with FPGAs typically\nlagging behind the computational throughput of GPUs.\nThe roofline model [31] is a useful tool for analyzing the perfor-\nmance of both GPUs and FPGAs. An algorithm running on GPUs or\nFPGAs can be either compute bound or memory bound. According\nto the roofline model [39], the peak performance of FPGAs is:\n𝑃𝑒𝑎𝑘𝑝𝑒𝑟𝑓𝑜𝑟𝑚𝑎𝑛𝑐𝑒= 𝑝× 𝑃𝐸𝑠× 2 × 𝑓,\n(1)\nwhere 𝑃𝐸𝑠is the number of processing elements (PEs) used in the\naccelerator, such as the DSP blocks, 𝑓is the clock frequency, and\n×2 term accounts for multiply-accumulate (MAC) operations. The\npacking factor for DSP blocks, 𝑝, varies based on the bit-width of\nthe operation, with 𝑝= 1 for 16-bit, 𝑝= 2 for 8-bit, and 𝑝= 4 for\n4-bit multiply-accumulate operations.\nFurthermore, the performance of an FPGA-based accelerator\nis also limited by the memory, which is related to the memory\nbandwidth (BW) and computation-to-communication (CTC) ratio:\n𝑃𝑒𝑎𝑘𝑚𝑒𝑚𝑜𝑟𝑦𝑏𝑎𝑛𝑑𝑤𝑖𝑑𝑡ℎ= 𝐵𝑊× 𝐶𝑇𝐶𝑟𝑎𝑡𝑖𝑜.\n(2)\nTable 1 summarizes the major differences between GPUs and\nFPGAs, such as clock frequency, number of compute cores, and\nmemory bandwidth. The significant difference in clock frequency\ncontributes to a notable performance gap between GPUs and FPGAs.\nEven with optimization such as pruning and quantization [10],\nFPGA inference speed generally remains inferior to that of GPUs.\nTable 1: Comparison between GPUs and FPGAs. Both V100\nand U280 are compared using the PCIe version. Performance\nis the theoretical peak extracted from corresponding product\ndatasheet [19, 33].\nDevices\nV100 GPU\nAlveo U280 FPGA\nTechnology\n12nm\n16nm\nClock\n1530MHz\n300MHz\nCompute cores\n5120 CUDA cores\n9024 DSP48E2\n640 Tensor cores\nPerformance\n14TFLOPs(FP32 CUDA)\n24.5 TOPs(INT8)\n112TFLOPs(FP16 Tensor)\nMemory\n32GB HBM2\n32GB DDR4\n8GB HBM2\nBandwidth\n900GB/s\n38GB/s (DDR4)\n460GB/s (HBM2)\nPower\n250W\n225W(Max)\n100W(Typical)\nPrice\n$11,458\n$7,717\nFigure 1 shows the roofline model for U280. We take 1\n64 resource\nand HBM bandwidth of U280 for analysis. Conventional DSP-based\naccelerators are compute bound when the arithmetic intensity sat-\nisfies a threshold. Our LUTMUL exploits the potential of LUTs and\ncan achieve higher parallelism by our LUT efficient mapping.\n101\n102\nOperational Intensity (Ops/Byte)\n102\nPerformance (GOPs)\n7.19 GB/s\nDSP\nLUTMUL\nRoofline Model Analysis for DSP and LUTMUL Theoretical Performance\nPeak Memory Bandwidth\nPeak Performance\nFigure 1: Roofline model analysis for LUTMUL and other\nDSP-based architectures. We take 1\n64 resource and memory\nbandwidth of U280 for analysis.\n2.2\nDataflow Architecture\nDataflow architecture contrasts with the traditional control flow\narchitecture. The dataflow nodes or processing elements can im-\nmediately start and execute when all its inputs are ready. Dataflow\narchitecture employs simple operations, such as broadcast (one-\nto-many), map (element-wise, e.g. activation function), zip (multi-\noperands, e.g. convolution and matrix multiplication), and reduce\n(many-to-one, e.g. pooling) [22]. A key advantage of reconfigurable\ndataflow architecture is its ability to allow data to flow through the\ncomputation graph efficiently, significantly enhancing parallelism\nand minimizing memory access time.\n\nLUTMUL: Exceed Conventional FPGA Roofline Limit by LUT-based Efficient MULtiplication for Neural Network Inference\nASPDAC ’25, January 20–23, 2025, Tokyo, Japan\n2.3\nFPGA-based Neural Network Accelerator\nArchitecture\nSince FPGAs have relatively limited on-chip resources, most of the\nFPGA accelerators [3, 24, 32, 34, 37] map computation onto hard-\nware and reuse the PE array. Notably, [18] explores the intra-layer\nmixed-scheme quantization and maps vision transformer layers\nonto a General Matrix Multiply (GEMM) kernel, where each layer\nmaintains a fixed ratio of this quantization scheme. Systolic ar-\nray architecture [30] presents another method for efficiently map-\nping convolution and matrix multiplication onto FPGAs with high\nthroughput.\nFINN [2, 26] uses a dataflow architecture and integrates all layers\nof the network into a single FPGA. The resources for each layer\ncan be adjusted according to computation requirements so that all\nlayers are balanced and pipelined for better throughput. FINN is\nparticularly well-suited for exploiting inter-layer quantization for\nneural networks because each layer has dedicated computation and\nmemory resources.\n3\nALGORITHM-HARDWARE CO-DESIGN FOR\nLUTMUL\n3.1\nMotivation\nThe roofline model reveals a theoretical peak performance for DSP-\nbased accelerators, applicable across various architectures such as\nGEMM, systolic array, or dataflow architecture. We can leverage\nLUT resources to perform multiplication and full parallelism to\nenable FPGA with greater performance. Given that the availability\nof LUTs usually outnumbers DSPs, using LUTs can potentially\nexceed the upper bound of performance of current DSP-based FPGA\naccelerators.\n0\n10\n20\n30\n40\n50\n60\n70\n0%\n2%\n4%\n6%\n8%\n10%\n12%\n14%\n16%\n1-bit\n2-bit\n3-bit\n4-bit\n5-bit\n6-bit\n7-bit\n8-bit\n# LUTs per \nmultiplication ↓\nAccuracy\nloss ↓\nQuantization bit-width\nAccuracy loss and LUT resources for 1-bit to 8-bit quantization \n# LUTs per multiplication\nAccuracy loss\nFigure 2: Accuracy loss and LUT resources for 1-bit to 8-bit\nquantization.\nFigure 2 shows the quantized neural network accuracy [14, 36]\nand the number of LUTs needed per multiplication by our method.\nWe trade-off between accuracy and LUT usage and choose 4-bit as\nour quantization bit-width. Binary and ternary neural networks\nincur large accuracy drops and consume half of the LUTs that 4-bit\nuses as the output bits of LUTs are limited. Compared with higher\nbit-width quantization, 4-bit uses significantly fewer LUTs and has\nnegligible accuracy loss.\n3.2\nLUTMUL Design Flow\nQuantization-Aware\nTraining\nHardware \nGeneration\nDeployment\nQuantization-aware training\nImport, streamlining transformations, reordering\nConvert to HLS layers using HLS templates\nAdjust folding for performance/resource requirements\nGenerate IP cores, stitch IP design by Vivado\nPYNQ-provided Python abstractions and drivers\nExported quantized ONNX layers\nStitched IP design\nFigure 3: LUTMUL Design flow.\nFigure 3 depicts the LUTMUL design flow. Initially, we train the\nneural network in our quantization-aware training framework. The\nquantization bit-widths for weights and activations are adjustable\nhyperparameters. The final quantized neural network is exported\nin Open Neural Network Exchange (ONNX) format, facilitating\nsubsequent hardware generation.\nThe ONNX intermediate representation is interpreted as a com-\nputation graph and undergoes a streamlining transformation [27].\nThe scaling factors of each channel and batch normalization layer\nare reordered and absorbed into the activation function, transform-\ning into a multi-threshold unit. Each computation node is converted\nto a High-Level Synthesis (HLS) layer using our HLS templates.\nThese HLS layers are folded according to performance and resource\nrequirements and interconnected sequentially. The final hardware\nbitstream, generated by Vivado, is deployed on FPGA boards via\nthe PYNQ framework.\n3.3\nReconfigurable Dataflow Architecture\nFigure 4 illustrates the hardware architecture of a MobileNetV2 [23]\nimplementation. This design, focusing on inverted residual blocks,\nemploys a First In, First Out (FIFO) buffer between layers to store\nactivations. The architecture uses a reconfigurable dataflow archi-\ntecture.\nOur design spans all Super Logic Regions (SLRs) to maximize\nhardware resources. Signals only traverse SLRs when the current\nSLR resources are insufficient for the next layer to avoid severe tim-\ning violations. Dataflow architecture is inherently suited for design\nspanning multiple SLRs and can be scaled up, enabling additional\nFPGAs connected via network for deploying larger networks [6].\n3.4\nConvolution Generator\nFor convolutional layers, the convolution operations can be lowered\nto matrix-matrix multiplications. These can be mapped in a stream-\ning manner and fed to the multiplication kernel. The multiplication\n\nASPDAC ’25, January 20–23, 2025, Tokyo, Japan\nYanyue Xie, Zhengang Li, Dana Diaconu, Suranga Handagala, Miriam Leeser, and Xue Lin\nHBM\nHBM\nDDR4\nDDR4\nFIFO\nImage\nStream\nResult\nStream\nSLR0\nSLR2\nPointwise Conv Generator\nLUT-based MUL\nAdder tree\nThreshold Memory\nFIFO\nPointwise Conv Generator\nLUT-based MUL\nAdder tree\nThreshold Memory\nDepthwise Conv Generator\nLUT-based MUL\nAdder tree\nThreshold Memory\nFIFO\nFIFO\nLUT-based MUL\nAdd with bias\nThreshold Memory\nSliding\nWindow\nUnit\nMax\noperator\nSLR1\nFigure 4: Hardware architecture of accelerator generated by\nLUTMUL. Our design is fully on-chip and does not use DRAM\nor HBM memory.\nkernel is fully paralleled to perform a matrix-vector multiplication,\nwhere the weights are stationary vectors and activations are stream-\ning inputs. Therefore, we need a convolution generator to perform\nthe im2col operations: reading data from FIFO, moving across input\nimages to form an image matrix, and streaming the output to the\nmultiplication kernel.\nThe convolution generator accommodates various configura-\ntions, including pointwise, depthwise, and standard convolution\nwith different kernel sizes and strides, since each kind of convolu-\ntional layer expects different input data sequences, necessitating\nspecific generator settings.\n3.5\nLook-Up Table based Efficient\nMultiplication\nFigure 5 demonstrates the look-up table based multiplication ker-\nnels and how to determine look-up table initialization (INIT) values.\nAfter embedding the weights into look-up tables, our look-up tables\ntransform into efficient constant multipliers [11]. Our look-up table\nbased multiplier is efficient in resources, utilizing only 2 LUTs on\naverage for a single 4-bit multiplication, compared with a general\nmultiplier that consumes 13-28 LUTs for an equivalent operation.\nThe choice of 4-bit quantization is pivotal as it maintains model\naccuracy and optimizes look-up table usage, as shown in Figure 2.\nWe show the number of LUT6 (6:1 LUT, 6-bit input, 1-bit output) for\na general n-bit multiplication (n:2n LUT, n-bit input, 2n-bit output)\nusing our method:\n#𝐿𝑈𝑇𝑠= 2𝑛× 2𝑛\n1 × 26 .\n(3)\nAlgorithm 1 shows the pseudo High-Level Synthesis (HLS) code\nfor look-up table based multiplication. The table contents are de-\nrived from pre-computed weights. The weights of convolutional\nlayers are fully paralleled, meaning that the 𝐶𝑂𝑈𝑇channel in Algo-\nrithm 1 refers to the output channels, and the𝐶𝐼𝑁channel refers to\nthe input channels times the kernel size squared. These dimensions\n(four in total) are fully unrolled in the spatial domain. The remain-\ning input feature map height and width dimensions are pipelined in\nthe temporal domain. Input activations are streamed from the con-\nvolution generator and passed through look-up tables. The output\nresults are multiplication results. They are accumulated, go through\nthe threshold unit, and generate activations for the next layer.\nAlgorithm 1 Look-up table based multiplication kernel\nInput: Streaming parallel input from the Convolution Generator and pre-\ncomputed look-up table contents\nOutput: Streaming output for the next layer\n1: for 𝑖←1 𝑡𝑜𝑅𝑂𝑊𝑆× 𝐶𝑂𝐿𝑆do\n2: #pragma HLS PIPELINE II=1\n3:\n𝑖𝑛𝑝𝑢𝑡←𝑠𝑟𝑐.𝑟𝑒𝑎𝑑()\n4:\nfor 𝑐𝑜←1 𝑡𝑜𝐶𝑂𝑈𝑇do\n5: #pragma HLS UNROLL\n6:\nfor 𝑐𝑖←1 𝑡𝑜𝐶𝐼𝑁do\n7: #pragma HLS UNROLL\n8:\n𝑚𝑢𝑙[𝑐𝑜][𝑐𝑖] ←𝑙𝑢𝑡[𝑐𝑜][𝑐𝑖][𝑖𝑛𝑝𝑢𝑡[𝑐𝑖]]\n9:\nend for\n10:\nend for\n11:\nfor 𝑐𝑜←1 𝑡𝑜𝐶𝑂𝑈𝑇do\n12: #pragma HLS UNROLL\n13:\nfor 𝑐𝑖←1 𝑡𝑜𝐶𝐼𝑁do\n14: #pragma HLS UNROLL\n15:\n𝑜𝑢𝑡𝑝𝑢𝑡[𝑐𝑜]+ = 𝑚𝑢𝑙[𝑐𝑜][𝑐𝑖]\n16:\nend for\n17:\nend for\n18:\n𝑑𝑠𝑡.𝑤𝑟𝑖𝑡𝑒(𝑜𝑢𝑡𝑝𝑢𝑡)\n19: end for\n3.6\nQuantization-Aware Training\nQuantization [10] and DSP packing [24] have become a standard\napproach for mapping neural networks onto FPGA-based accel-\nerators, as FPGAs’ LUTs and DSP blocks are not optimized for\nfloating-point but ideal for integer or fixed-point operations. Quan-\ntization, paired with DSP packing, reduces resource demands for\nthe multiplications and improves throughput.\nThe quantization operation is defined as:\n𝑦= 𝑞𝑢𝑎𝑛𝑡𝑖𝑧𝑒(𝑥) = 𝑐𝑙𝑎𝑚𝑝(𝑟𝑜𝑢𝑛𝑑(𝑥\n𝑠+ 𝑧),𝑦𝑚𝑖𝑛,𝑦𝑚𝑎𝑥),\n(4)\nwhere 𝑥is the floating-point value to quantize, 𝑠is the scaling\nfactor of the output quantized tensor, and 𝑧is the zero-point or\nquantization bias coefficient. The function, 𝑟𝑜𝑢𝑛𝑑, can be round-to-\neven or round-to-zero, and 𝑐𝑙𝑎𝑚𝑝performs clipping inclusive of\nthe boundaries 𝑦𝑚𝑖𝑛and 𝑦𝑚𝑎𝑥.\nFor the reverse process, to compute the floating-point represen-\ntation of a quantized value, we define the dequantize operation\nas:\n𝑑𝑒𝑞𝑢𝑎𝑛𝑡𝑖𝑧𝑒(𝑦) = 𝑠(𝑦−𝑧),\n(5)\n\nLUTMUL: Exceed Conventional FPGA Roofline Limit by LUT-based Efficient MULtiplication for Neural Network Inference\nASPDAC ’25, January 20–23, 2025, Tokyo, Japan\nLUT6_2 #3\n\n\n\n\n\n\n\n\n\nINIT=64'hfffe_0000_fffe_0000\n\n6-Input Look-Up Table\nA[3:0]\n1'b1\nLUT5\nLUT5\nO7\nO6\nWS\n4\nLUT6_2 #2\nINIT=64'h07fe_0000_f83e_0000\n6-Input Look-Up Table\nA[3:0]\n1'b1\nLUT5\nLUT5\nO5\nO4\nWS\n4\nLUT6_2 #1\n\n\n\n\n\n\n\n\n\nINIT=64'h39c6_ff00_5a5a_f0f0\n\n6-Input Look-Up Table\nA[3:0]\n1'b1\nLUT5\nLUT5\nO3\nO2\nWS\n4\nLUT6_2 #0\n\n\n\n\n\n\n\n\n\nINIT=64'hcccc_cccc_aaaa_aaaa\n\n6-Input Look-Up Table\nA[3:0]\n1'b1\nLUT5\nLUT5\nO1\nO0\nWS\n4\nActivation(uint4)\nWeight=1 Output(int8) Weight=-3 Output(int8)\n4'b0000\n8'b00000000\n8'b00000000\n4'b0001\n8'b00000001\n8'b11111101\n4'b0010\n8'b00000010\n8'b11111010\n4'b0011\n8'b00000011\n8'b11110111\n4'b0100\n8'b00000100\n8'b11110100\n4'b0101\n8'b00000101\n8'b11110001\n4'b0110\n8'b00000110\n8'b11101110\n4'b0111\n8'b00000111\n8'b11101011\n4'b1000\n8'b00001000\n8'b11101000\n4'b1001\n8'b00001001\n8'b11100101\n4'b1010\n8'b00001010\n8'b11100010\n4'b1011\n8'b00001011\n8'b11011111\n4'b1100\n8'b00001100\n8'b11011100\n4'b1101\n8'b00001101\n8'b11011001\n4'b1110\n8'b00001110\n8'b11010110\n4'b1111\n8'b00001111\n8'b11010011\nActivation\nMultiplication results\nWeight=1, WS=0\nWeight=-3, WS=1\n4’b0000\n0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0\n4’b0001\n0 0 0 0 0 0 0 1\n1 1 1 1 1 1 0 1\n4’b0010\n0 0 0 0 0 0 1 0\n1 1 1 1 1 0 1 0\n4’b0011\n0 0 0 0 0 0 1 1\n1 1 1 1 0 1 1 1\n4’b0100\n0 0 0 0 0 1 0 0\n1 1 1 1 0 1 0 0\n4’b0101\n0 0 0 0 0 1 0 1\n1 1 1 1 0 0 0 1\n4’b0110\n0 0 0 0 0 1 1 0\n1 1 1 0 1 1 1 0\n4’b0111\n0 0 0 0 0 1 1 1\n1 1 1 0 1 0 1 1\n4’b1000\n0 0 0 0 1 0 0 0\n1 1 1 0 1 0 0 0\n4’b1001\n0 0 0 0 1 0 0 1\n1 1 1 0 0 1 0 1\n4’b1010\n0 0 0 0 1 0 1 0\n1 1 1 0 0 0 1 0\n4’b1011\n0 0 0 0 1 0 1 1\n1 1 0 1 1 1 1 1\n4’b1100\n0 0 0 0 1 1 0 0\n1 1 0 1 1 1 0 0\n4’b1101\n0 0 0 0 1 1 0 1\n1 1 0 1 1 0 0 1\n4’b1110\n0 0 0 0 1 1 1 0\n1 1 0 1 0 1 1 0\n4’b1111\n0 0 0 0 1 1 1 1\n1 1 0 1 0 0 1 1\nFigure 5: Illustration of LUTMUL for efficient multiplication via look-up tables. The left-hand side figure demonstrates how\nto use LUT6_2 primitive for embedding multiplication results of weights and input activations. The right-hand side table\ndemonstrates the multiplication results of two example weights and how to generate the corresponding look-up table contents.\nThe weights (int4) and multiplication output (int8) are using two’s complement representation, while activation are all unsigned numbers\n(uint4). The Most Significant Bit (MSB) of LUT6_2 input is configured as ‘1’ to enable two output ports. The bit below the MSB is a\nWeight Select (WS) signal to select between two weights. The lowest 4-bit inputs serve as activation inputs. Our method embeds two\nint4 weights inside four LUT6, a resource-efficient approach contrasting with the LUT6-instantiated general multipliers, which consume\n6-14× more LUT6 resources. Two used example weights are 1 and -3 respectively. The embedded LUT contents for these four LUTs are\n64’hfffe_0000_fffe_0000, 64’h07fe_0000_f83e_0000, 64’h39c6_ff00_5a5a_f0f0, and 64’hcccc_cccc_aaaa_aaaa, respectively.\nwhere 𝑦is a quantized tensor, 𝑧is its zero-point, and 𝑠is the scaling\nfactor.\nQuantization introduces errors to the trained model parame-\nters and results in performance degradation. Quantization-Aware\nTraining (QAT) is a popular approach that retrains the model with\nquantized parameters on the pretraining dataset to converge to the\npretrained model performance. The usual forward and backward\npasses are performed on the quantized model in floating point, and\nthe model parameters are quantized after each gradient update.\nIn particular, it is important to do this projection after the weight\nupdate is performed in floating point precision. Performing the\nbackward pass with floating point is vital, as accumulating the\ngradients in quantized precision can result in zero gradients or\ngradients with high error, especially in low-precision scenarios [9].\n4\nEVALUATION\n4.1\nExperimental Setup\nTo evaluate the performance of LUTMUL, we implement MobileNetV2\non FPGAs and compare it with existing FPGA-based MobileNet ac-\ncelerators. MobilenetV2 [23] has 3.4M parameters and achieves\n71.88% top-1 accuracy on the ImageNet dataset [5]. We utilize the\nFINN framework [2] as our foundational platform. For quantiza-\ntion, we adopt PyTorch 1.13.0 and Brevitas 0.9.1 [20]. Specifically,\nwe choose 4-bit for weights and activations quantization except\nfor the first and last layers, which are set as 8-bit. To preserve the\naccuracy of the MobileNetV2 model, we apply the channel-wise\nquantization scheme for our model. Our quantized MobileNetV2\nnetwork is trained for 420 epochs, culminating in a 70.95% top-1\naccuracy evaluated on the ImageNet dataset [5].\nFor the hardware evaluation, the utilized development platform\nis the AMD Xilinx Alveo U280 data center accelerator card on the\nOpen Cloud Testbed (OCT) [40]. We implement the first 15 layers\nof MobileNetV2 in a fully parallel manner and fold the remaining\nlayers for resource optimization. To maximize the computation\nefficiency without timing violation, the working frequency is set to\n333 MHz for all the designs implemented through Vitis HLS/Vivado\n2022.1.\n4.2\nExperimental Results\nTable 2 showcases the hardware performance and comparisons with\nother FPGA-based MobileNet accelerators. Most of these accelera-\ntors are tailored for edge FPGAs, such as ZU9EG, except for FINN,\nwhich has data center accelerator implementation for MobileNetV1.\nThe FINN result is generated and tested on the same device as our\nimplementation, while other data points are extracted from their\noriginal publications.\nIn terms of accuracy, our model achieves the best 70.95% top-1\naccuracy on ImageNet among all implementations. Quantization-\naware training effectively mitigates quantization errors, preserving\nthe model original accuracy, even with 4-bit quantized weights and\nactivations.\n\nASPDAC ’25, January 20–23, 2025, Tokyo, Japan\nYanyue Xie, Zhengang Li, Dana Diaconu, Suranga Handagala, Miriam Leeser, and Xue Lin\nTable 2: Comparisons of MobileNet implementations between previous FPGA-based accelerators.\nImplementation\nFINN\nFPL’19\nLight-OPU\nFPL’21\nMix & Match\nFILM-QNN\nLUTMUL\n[2]\n[32]\n[37]\n[34]\n[3]\n[24]\n(Ours)\nNetwork\nMobileNetV1 MobileNetV2 MobileNetV3 MobileNetV2 MobileNetV2 MobileNetV2 MobileNetV2\nBit-Width\nW4A4\nW8A8\nW8A8\nW8A8\nW4A4\nW8A5&W4A5\nW4A4\nTop-1 Accuracy\n70.4%\n68.1%\n66.7%\n70.8%\n65.6%\n65.7%\n70.95%\nPlatform\nAlveo U280\nZU9EG\nXC7K325T\nXC7V690T\nXC7Z045\nZU9EG\nAlveo U280\nFrequency (MHz)\n333\n333\n200\n150\n100\n150\n333\nLUT\n501363\n161944\n173522\n308449\n145049\n180100\n529242\nFF\n476316\n301416\n241175\n278926\n111575\n-\n503192\nBRAM36\n898\n771\n193.5\n941.5\n225.5\n440.5\n1119\nDSP\n106\n2070\n704\n2160\n900\n2092\n106\nPower (W)\n41.69\n-\n8.5*\n11.35\n-\n12.9\n42.12\nFrame Rate (FPS)\n925\n809.8\n332.6\n302.3\n549.3\n537.9\n1627\nThroughput (GOPS)\n556.4\n487.1\n84.48*\n181.8\n326.9\n320.1\n978.6\nEnergy Efficiency (GOPS/W)\n13.35\n-\n9.9\n16.02\n-\n24.8\n23.23\nNote: ‘-’ means that the result is not given in the original publications, and ‘*’ means that the result is inferred from the original publications.\nAs for the inference performance, our implementation achieves\na throughput of 1627 images per second. Our implementation con-\nsumes the most FPGA resources but could still fit on a single Alveo\nU280. However, it is noteworthy that our implementation also yields\na 23.23 GOPS/W energy efficiency, marginally lower than the FLIM-\nQNN [24], which is implemented on a more power-efficient edge\nFPGA board.\n4.3\nDiscussion\nLUT as ROM\n3277\n55%\nAdder logic\n2645\n45%\nImplementation\nLUT as ROM\nAdder logic\nMemory \n(multiplication)\n1829\n11%\nExpression (addition)\n14944\n89%\nSynthesis\nMemory (multiplication)\nExpression (addition)\nFigure 6: LUT Resource breakdown of the second convolution\nlayer in MobileNetV2 using LUTMUL.\nFigure 6 shows the LUT resource breakdown of the second con-\nvolution layer in MobileNetV2 using LUTMUL, which is a 1 × 1 con-\nvolution kernel and has 32 input channels and 32 output channels.\nFor these 1024 4-bit weights, multiplication operations use 1829\nLUTs after HLS synthesis, which matches the theoretical analysis\nof LUTMUL. However, HLS instantiates an adder for each addition\noperation to achieve an II of 1, resulting in a high usage of LUT\nfor adder logic. After Vivado implementation, the LUT usage de-\ncreased to 5922. Vivado optimizes the logic and instantiates 3277\nLUTs as ROM and 2645 LUTs as adder and other logic. Even though\nadder logic accounts for a large part of resources, the parallel MAC\nperformance by LUTMUL still outperforms the DSP packing method\nusing the same number of resources.\n4.4\nComparisons with Related Works\nOur method is not only limited to integer multiplication, but can\nalso be extended to customized data formats, such as FP4 and\nMXFP4 [21], while DSP packing is designed efficiently for integer\nformats. LUTNet [28, 29] also utilizes LUT for inference and ex-\nplores the flexibility of LUT. However, LUTNet design suffers from\nlow accuracy when the network becomes larger. PolyLUT [1] trains\nmultivariate polynomials instead of linear functions and embeds\npiecewise polynomial functions into LUTs. CompressedLUT [15]\nproposes a lossless LUT compression method and is efficient for\nnon-linear functions and large LUT logic blocks, such as [8, 12, 25].\nOur method maps MAC operations to the single-LUT level, and\nVivado can handle remaining logic optimization efficiently.\n5\nCONCLUSION\nWe propose LUTMUL, an efficient method that leverages look-up ta-\nbles for multiplication in convolutional neural networks. Compared\nwith the general multiplier, our method is efficient in resources,\nwhich only needs two look-up tables on average for a single 4-bit\nmultiplication. Compared with other DSP-based FPGA accelerators,\nLUTMUL’s reconfigurable dataflow architecture enables full paral-\nlelism, reduces memory access time, and increases the theoretical\nupper bound of performance. Experimental results demonstrate\nthat our design maintains a top-1 accuracy of 70.95% on the Ima-\ngeNet dataset and achieves a throughput of 1627 images per second\non a single Alveo U280 FPGA, outperforming other FPGA-based\nMobileNet accelerators.\nACKNOWLEDGMENTS\nThis research was supported in part by the National Science Foun-\ndation under Grants CCF-1901378, CNS-1925658, and CNS-2319962.\n\nLUTMUL: Exceed Conventional FPGA Roofline Limit by LUT-based Efficient MULtiplication for Neural Network Inference\nASPDAC ’25, January 20–23, 2025, Tokyo, Japan\nREFERENCES\n[1] Marta Andronic and George A Constantinides. 2023. PolyLUT: learning piecewise\npolynomials for ultra-low latency FPGA LUT-based inference. In 2023 Interna-\ntional Conference on Field Programmable Technology (ICFPT). 60–68.\n[2] Michaela Blott, Thomas B Preußer, Nicholas J Fraser, Giulio Gambardella, Kenneth\nO’brien, Yaman Umuroglu, Miriam Leeser, and Kees Vissers. 2018. FINN-R: An\nend-to-end deep-learning framework for fast exploration of quantized neural\nnetworks. ACM Transactions on Reconfigurable Technology and Systems (TRETS)\n11, 3 (2018), 1–23.\n[3] Sung-En Chang, Yanyu Li, Mengshu Sun, Runbin Shi, Hayden K-H So, Xuehai\nQian, Yanzhi Wang, and Xue Lin. 2021. Mix and match: A novel fpga-centric deep\nneural network quantization framework. In 2021 IEEE International Symposium\non High-Performance Computer Architecture (HPCA). IEEE, 208–220.\n[4] Hongzheng Chen, Jiahao Zhang, Yixiao Du, Shaojie Xiang, Zichao Yue, Niansong\nZhang, Yaohui Cai, and Zhiru Zhang. 2024. Understanding the potential of fpga-\nbased spatial acceleration for large language model inference. ACM Transactions\non Reconfigurable Technology and Systems (TRETS) (2024).\n[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Ima-\ngenet: A large-scale hierarchical image database. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR). 248–255.\n[6] Dana Diaconu, Yanyue Xie, Mehmet Gungor, Suranga Handagala, Xue Lin, and\nMiriam Leeser. 2023. Machine Learning Across Network-Connected FPGAs. In\n2023 IEEE High Performance Extreme Computing Conference (HPEC). IEEE, 1–7.\n[7] Peiyan Dong, Mengshu Sun, Alec Lu, Yanyue Xie, Kenneth Liu, Zhenglun Kong,\nXin Meng, Zhengang Li, Xue Lin, Zhenman Fang, et al. 2023. Heatvit: Hardware-\nefficient adaptive token pruning for vision transformers. In 2023 IEEE International\nSymposium on High-Performance Computer Architecture (HPCA). IEEE, 442–455.\n[8] Daniel Gerlinghoff, Benjamin Chen Ming Choong, Rick Siow Mong Goh, Weng-\nFai Wong, and Tao Luo. 2024. Table-Lookup MAC: Scalable Processing of Quan-\ntised Neural Networks in FPGA Soft Logic. In Proceedings of the 2024 ACM/SIGDA\nInternational Symposium on Field Programmable Gate Arrays (FPGA).\n[9] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and\nKurt Keutzer. 2022. A survey of quantization methods for efficient neural network\ninference. In Low-Power Computer Vision. Chapman and Hall/CRC, 291–326.\n[10] Song Han, Huizi Mao, and William J Dally. 2016. Deep Compression: Compressing\nDeep Neural Networks with Pruning, Trained Quantization and Huffman Coding.\nInternational Conference on Learning Representations (ICLR) (2016).\n[11] Martin Hardieck, Martin Kumm, Konrad Möller, and Peter Zipf. 2019. Reconfig-\nurable convolutional kernels for neural networks on FPGAs. In Proceedings of the\n2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays\n(FPGA). 43–52.\n[12] Jingkai Hong, Arash Fayyazi, Amirhossein Esmaili, Mahdi Nazemi, and Massoud\nPedram. 2023. Algorithms and Hardware for Efficient Processing of Logic-based\nNeural Networks. In Proceedings of the 60th ACM/IEEE Design Automation Con-\nference (DAC). IEEE, 1–6.\n[13] Seongmin Hong, Seungjae Moon, Junsoo Kim, Sungjae Lee, Minsub Kim, Dongsoo\nLee, and Joo-Young Kim. 2022. DFX: A Low-latency Multi-FPGA Appliance\nfor Accelerating Transformer-based Text Generation. In 2022 55th IEEE/ACM\nInternational Symposium on Microarchitecture (MICRO). IEEE, 616–630.\n[14] Qing Jin, Linjie Yang, and Zhenyu Liao. 2020. Adabits: Neural network quan-\ntization with adaptive bit-widths. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR). 2146–2156.\n[15] Alireza Khataei and Kia Bazargan. 2024. CompressedLUT: An Open Source\nTool for Lossless Compression of Lookup Tables for Function Evaluation and\nBeyond. In Proceedings of the 2024 ACM/SIGDA International Symposium on Field\nProgrammable Gate Arrays (FPGA).\n[16] Bingbing Li, Santosh Pandey, Haowen Fang, Yanjun Lyv, Ji Li, Jieyang Chen,\nMimi Xie, Lipeng Wan, Hang Liu, and Caiwen Ding. 2020. Ftrans: energy-\nefficient acceleration of transformers using fpga. In Proceedings of the ACM/IEEE\nInternational Symposium on Low Power Electronics and Design (ISLPED). 175–180.\n[17] Zhengang Li, Alec Lu, Yanyue Xie, Zhenglun Kong, Mengshu Sun, Hao Tang,\nZhong Jia Xue, Peiyan Dong, Caiwen Ding, Yanzhi Wang, et al. 2024. Quasar-\nViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision\nTransformers. In Proceedings of the 38th ACM International Conference on Super-\ncomputing (ICS). 324–337.\n[18] Zhengang Li, Mengshu Sun, Alec Lu, Haoyu Ma, Geng Yuan, Yanyue Xie, Hao\nTang, Yanyu Li, Miriam Leeser, Zhangyang Wang, Xue Lin, and Zhenman Fang.\n2022. Auto-vit-acc: An fpga-aware automatic acceleration framework for vi-\nsion transformer with mixed-scheme quantization. In 2022 32nd International\nConference on Field-Programmable Logic and Applications (FPL). IEEE, 109–116.\n[19] Nvidia.\n2017.\nNvidia\nTesla\nV100\nGPU\nArchitecture\nWhitepaper.\nhttps://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-\nwhitepaper.pdf. Last accessed Oct. 30, 2024.\n[20] Alessandro Pappalardo. 2023. Xilinx/brevitas. https://doi.org/10.5281/zenodo.\n3333552\n[21] Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Kho-\ndamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger,\nKristof Denolf, et al. 2023. Microscaling data formats for deep learning. arXiv\npreprint arXiv:2310.10537 (2023).\n[22] Sambanova Whitepaper. 2021. Accelerated Computing with a Reconfigurable\nDataflow Architecture.\nhttps://sambanova.ai/wp-content/uploads/2021/\n04/SambaNova_Accelerated-Computing-with-a-Reconfigurable-Dataflow-\nArchitecture_Whitepaper_English.pdf. Last accessed Oct. 30, 2024.\n[23] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-\nChieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR). 4510–4520.\n[24] Mengshu Sun, Zhengang Li, Alec Lu, Yanyu Li, Sung-En Chang, Xiaolong Ma,\nXue Lin, and Zhenman Fang. 2022. Film-qnn: Efficient fpga acceleration of deep\nneural networks with intra-layer, mixed-precision quantization. In Proceedings\nof the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate\nArrays (FPGA). 134–145.\n[25] Yaman Umuroglu, Yash Akhauri, Nicholas James Fraser, and Michaela Blott. 2020.\nLogicNets: Co-designed neural networks and circuits for extreme-throughput\napplications. In 2020 30th International Conference on Field-Programmable Logic\nand Applications (FPL). IEEE, 291–297.\n[26] Yaman Umuroglu, Nicholas J Fraser, Giulio Gambardella, Michaela Blott, Philip\nLeong, Magnus Jahre, and Kees Vissers. 2017. Finn: A framework for fast, scal-\nable binarized neural network inference. In Proceedings of the 2017 ACM/SIGDA\nInternational Symposium on Field-Programmable Gate Arrays (FPGA). 65–74.\n[27] Yaman Umuroglu and Magnus Jahre. 2017. Streamlined deployment for quantized\nneural networks. arXiv preprint arXiv:1709.04060 (2017).\n[28] Erwei Wang, James J Davis, Peter YK Cheung, and George A Constantinides.\n2019. LUTNet: Rethinking inference in FPGA soft logic. In 2019 IEEE 27th Annual\nInternational Symposium on Field-Programmable Custom Computing Machines\n(FCCM). IEEE, 26–34.\n[29] Erwei Wang, James J Davis, Georgios-Ilias Stavrou, Peter YK Cheung, George A\nConstantinides, and Mohamed Abdelfattah. 2022. Logic shrinkage: Learned FPGA\nnetlist sparsity for efficient neural network inference. In Proceedings of the 2022\nACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA).\n101–111.\n[30] Xuechao Wei, Cody Hao Yu, Peng Zhang, Youxiang Chen, Yuxin Wang, Han\nHu, Yun Liang, and Jason Cong. 2017. Automated systolic array architecture\nsynthesis for high throughput CNN inference on FPGAs. In Proceedings of the\n54th Annual Design Automation Conference (DAC). 1–6.\n[31] Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an\ninsightful visual performance model for multicore architectures. Commun. ACM\n52, 4 (2009), 65–76.\n[32] Di Wu, Yu Zhang, Xijie Jia, Lu Tian, Tianping Li, Lingzhi Sui, Dongliang Xie,\nand Yi Shan. 2019. A high-performance CNN processor based on FPGA for\nMobileNets. In 2019 29th International Conference on Field Programmable Logic\nand Applications (FPL). 136–143.\n[33] AMD Xilinx. 2021. Alveo Product Selection Guide. https://docs.xilinx.com/v/u/en-\nUS/alveo-product-selection-guide. Last accessed Oct. 30, 2024.\n[34] Shun Yan, Zhengyan Liu, Yun Wang, Chenglong Zeng, Qiang Liu, Bowen Cheng,\nand Ray CC Cheung. 2021. An fpga-based mobilenet accelerator considering\nnetwork structure characteristics. In 2021 31st International Conference on Field-\nProgrammable Logic and Applications (FPL). IEEE, 17–23.\n[35] Geng Yang, Yanyue Xie, Zhong Jia Xue, Sung-En Chang, Yanyu Li, Peiyan Dong,\nJie Lei, Weiying Xie, Yanzhi Wang, Xue Lin, and Zhenman Fang. 2024. SDA:\nLow-Bit Stable Diffusion Acceleration on Edge FPGAs. In 2024 34th International\nConference on Field-Programmable Logic and Applications (FPL). 264–273.\n[36] Linjie Yang and Qing Jin. 2021. Fracbits: Mixed precision quantization via frac-\ntional bit-widths. In Proceedings of the AAAI Conference on Artificial Intelligence\n(AAAI), Vol. 35. 10612–10620.\n[37] Yunxuan Yu, Tiandong Zhao, Kun Wang, and Lei He. 2020. Light-OPU: An FPGA-\nbased overlay processor for lightweight convolutional neural networks. In Pro-\nceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable\nGate Arrays (FPGA). 122–132.\n[38] Shulin Zeng, Jun Liu, Guohao Dai, Xinhao Yang, Tianyu Fu, Hongyi Wang,\nWenheng Ma, Hanbo Sun, Shiyao Li, Zixiao Huang, Yadong Dai, Jintao Li, Zehao\nWang, Ruoyu Zhang, Kairui Wen, Xuefei Ning, and Yu Wang. 2024. FlightLLM:\nEfficient Large Language Model Inference with a Complete Mapping Flow on\nFPGA. In Proceedings of the 2024 ACM/SIGDA International Symposium on Field\nProgrammable Gate Arrays (FPGA).\n[39] Chen Zhang, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, and Jason Cong.\n2015. Optimizing FPGA-based accelerator design for deep convolutional neural\nnetworks. In Proceedings of the 2015 ACM/SIGDA International Symposium on\nField Programmable Gate Arrays (FPGA). 161–170.\n[40] Michael Zink, David Irwin, Emmanuel Cecchet, Hakan Saplakoglu, Orran Krieger,\nMartin Herbordt, Michael Daitzman, Peter Desnoyers, Miriam Leeser, and\nSuranga Handagala. 2021. The Open Cloud Testbed (OCT): A platform for\nresearch into new cloud technologies. In 2021 IEEE 10th International Conference\non Cloud Networking (CloudNet). IEEE, 140–147.",
    "pdf_filename": "LUTMUL_Exceed_Conventional_FPGA_Roofline_Limit_by_LUT-based_Efficient_Multiplication_for_Neural_Netw.pdf"
}