{
    "title": "LUTMUL: Exceed Conventional FPGA Roofline Limit by",
    "abstract": "ForFPGA-basedneuralnetworkaccelerators,digitalsignalpro- Units(GPUs)intermsofperformanceandeaseofprogramming. cessing(DSP)blockshavetraditionallybeenthecornerstonefor FPGAreconfigurablelogicmainlyconsistsoflook-uptables(LUT), handling multiplications. This paper introduces LUTMUL, which blockRAMs(BRAMs),anddigitalsignalprocessing(DSP)blocks. harnessesthepotentialoflook-uptables(LUTs)forperforming Togetherwithroutingresources,FPGAcanbereconfiguredfor multiplications.TheavailabilityofLUTstypicallyoutnumbersthat customizeddesigns.Despitetheflexibility,FPGAsfaceconstraints ofDSPsbyafactorof100,offeringasignificantcomputationalad- inclockfrequency,floating-pointperformance,andmemoryband- vantage.ByexploitingthisadvantageofLUTs,ourmethoddemon- width.ThisperformancegapbetweenFPGAsandGPUsisbecoming stratesapotentialboostintheperformanceofFPGA-basedneural evenlargerwhenconsideringthetensorcoreperformanceofGPUs. networkacceleratorswithareconfigurabledataflowarchitecture. Toaddressthis,weneedanalgorithm-hardwareco-designmethod Ourapproachchallengestheconventionalpeakperformanceon toboostFPGAswithgreaterinferencecapability. DSP-based accelerators and sets a new benchmark for efficient FPGA accelerators can follow GPU-like [32, 34] architecture, neuralnetworkinferenceonFPGAs.Experimentalresultsdemon- which maps computation to compute cores with repetitive use. stratethatourdesignachievesthebestinferencespeedamong Whilebeneficial,thisapproachencountersmemorybandwidthis- allFPGA-basedaccelerators,achievingathroughputof1627im- suessimilartoGPUs.ComparedwithGPUs,FPGAsusuallyhave agespersecondandmaintainingatop-1accuracyof70.95%onthe lowermemorybandwidth,andthelowerclockfrequencyofFPGAs ImageNetdataset. meansalowerupperboundofperformance.WhileFPGA-based acceleratorswithspecificinstructionsetarchitectures[37]offer CCSCONCEPTS flexibilityacrossdifferentmodels,theyoftencompromiseonper- formanceduetonon-optimizedcomputekernelsforspecificneural •Hardware→ReconfigurablelogicandFPGAs;•Computing networklayers. methodologies→Machinelearning. TobridgetheperformancegapbetweenFPGAsandGPUs,par- ticularlyindeeplearningapplications,weintroduceLUTMUL,which",
    "body": "LUTMUL: Exceed Conventional FPGA Roofline Limit by\nLUT-based Efficient MULtiplication for Neural Network Inference\nYanyueXie ZhengangLi DanaDiaconu\nNortheasternUniversity Adobe NortheasternUniversity\nxie.yany@northeastern.edu li.zhen@northeastern.edu diaconu.d@northeastern.edu\nSurangaHandagala MiriamLeeser XueLin\nNortheasternUniversity NortheasternUniversity NortheasternUniversity\ns.handagala@northeastern.edu mel@coe.neu.edu xue.lin@northeastern.edu\nABSTRACT 16, 38] tasks. However, FPGAs lag behind Graphics Processing\nForFPGA-basedneuralnetworkaccelerators,digitalsignalpro- Units(GPUs)intermsofperformanceandeaseofprogramming.\ncessing(DSP)blockshavetraditionallybeenthecornerstonefor FPGAreconfigurablelogicmainlyconsistsoflook-uptables(LUT),\nhandling multiplications. This paper introduces LUTMUL, which blockRAMs(BRAMs),anddigitalsignalprocessing(DSP)blocks.\nharnessesthepotentialoflook-uptables(LUTs)forperforming Togetherwithroutingresources,FPGAcanbereconfiguredfor\nmultiplications.TheavailabilityofLUTstypicallyoutnumbersthat customizeddesigns.Despitetheflexibility,FPGAsfaceconstraints\nofDSPsbyafactorof100,offeringasignificantcomputationalad- inclockfrequency,floating-pointperformance,andmemoryband-\nvantage.ByexploitingthisadvantageofLUTs,ourmethoddemon- width.ThisperformancegapbetweenFPGAsandGPUsisbecoming\nstratesapotentialboostintheperformanceofFPGA-basedneural evenlargerwhenconsideringthetensorcoreperformanceofGPUs.\nnetworkacceleratorswithareconfigurabledataflowarchitecture. Toaddressthis,weneedanalgorithm-hardwareco-designmethod\nOurapproachchallengestheconventionalpeakperformanceon toboostFPGAswithgreaterinferencecapability.\nDSP-based accelerators and sets a new benchmark for efficient FPGA accelerators can follow GPU-like [32, 34] architecture,\nneuralnetworkinferenceonFPGAs.Experimentalresultsdemon- which maps computation to compute cores with repetitive use.\nstratethatourdesignachievesthebestinferencespeedamong Whilebeneficial,thisapproachencountersmemorybandwidthis-\nallFPGA-basedaccelerators,achievingathroughputof1627im- suessimilartoGPUs.ComparedwithGPUs,FPGAsusuallyhave\nagespersecondandmaintainingatop-1accuracyof70.95%onthe lowermemorybandwidth,andthelowerclockfrequencyofFPGAs\nImageNetdataset. meansalowerupperboundofperformance.WhileFPGA-based\nacceleratorswithspecificinstructionsetarchitectures[37]offer\nCCSCONCEPTS flexibilityacrossdifferentmodels,theyoftencompromiseonper-\nformanceduetonon-optimizedcomputekernelsforspecificneural\n•Hardware→ReconfigurablelogicandFPGAs;•Computing\nnetworklayers.\nmethodologies→Machinelearning.\nTobridgetheperformancegapbetweenFPGAsandGPUs,par-\nticularlyindeeplearningapplications,weintroduceLUTMUL,which\nKEYWORDS\nleveragesthelook-uptablesonFPGAsfordeeplearningtasks,fo-\nFPGAs,Quantization,Look-uptables,Rooflinemodel.\ncusingonacceleratingconvolutionalneuralnetworks(CNNs).We\nACMReferenceFormat: recognizethatthetraditionalFPGAdesigns,heavilydependenton\nYanyueXie,ZhengangLi,DanaDiaconu,SurangaHandagala,MiriamLeeser, DSPblocks,maynotfullyexploittheparallelismandflexibility\nandXueLin.2025.LUTMUL:ExceedConventionalFPGARooflineLimit thatLUTsoffer,astheavailabilityofLUTstypicallyoutnumbers\nbyLUT-basedEfficientMULtiplicationforNeuralNetworkInference.In DSPsbyafactorof100.Ourmethodemphasizesanovelutiliza-\n30thAsiaandSouthPacificDesignAutomationConference(ASPDAC’25), tionofLUTstoenhancecomputationalefficiencyandthroughput\nJanuary20–23,2025,Tokyo,Japan.ACM,NewYork,NY,USA,7pages.\nindeeplearningapplications.Specifically,weembedtheconvolu-\nhttps://doi.org/10.1145/3658617.3697687\ntionalneuralnetworkweightsintoLUTs,wheretheLUTinputis\ntheactivationsandtheLUToutputisthemultiplicationresult.Dif-\n1 INTRODUCTION\nferentfromLUT-basedgeneralmultipliers,ourmethodisefficient\nField-ProgrammableGateArrays(FPGAs)havebeenwidelyused inresources(requiringjust2LUTsforasingle4-bitmultiplication)\nasdeeplearningaccelerators,facilitatingadvancementsincom- andhelpsfullyexploittheparallelism.\nputervision[7,17,35,39]andnaturallanguageprocessing[4,13, WeproposeareconfigurabledataflowarchitectureforourLUT-\nbased efficient multiplication kernel. Our dataflow architecture\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor\nminimizesthememoryaccesstimebyprocessingthedataon-chip\nclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed\nforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation througheachlayerwithoutexternalmemory.Thereconfigurability\nonthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored. oftheFPGAallowsustotailorthearchitecturespecificallyforeach\nForallotheruses,contacttheowner/author(s).\ndistinctlayerofCNNs.WithLUTresources,thegeneratedacceler-\nASPDAC’25,January20–23,2025,Tokyo,Japan\n©2025Copyrightheldbytheowner/author(s). atorcanpotentiallyexceedthepeakperformanceofconventional\nACMISBN979-8-4007-0635-6/25/01 DSP-basedFPGAaccelerators.Ourdataflowarchitectureaimsto\nhttps://doi.org/10.1145/3658617.3697687\n4202\nvoN\n1\n]RA.sc[\n1v25811.1142:viXra\nASPDAC’25,January20–23,2025,Tokyo,Japan YanyueXie,ZhengangLi,DanaDiaconu,SurangaHandagala,MiriamLeeser,andXueLin\nenhancetheoverallefficiencyofourdeeplearningaccelerators, Table1:ComparisonbetweenGPUsandFPGAs.BothV100\noptimizingFPGAsfordeeplearningtasks. andU280arecomparedusingthePCIeversion.Performance\nOurcontributionscanbesummarizedasfollows: isthetheoreticalpeakextractedfromcorrespondingproduct\ndatasheet[19,33].\n• WepresentLUTMUL,analgorithm-hardwareco-designmethod\nthatembedsquantizedneuralnetworkweightsintolook-up\ntablesforefficientmultiplicationsandusesdedicatedlook-up Devices V100GPU AlveoU280FPGA\ntablesforfullparallelism. Technology 12nm 16nm\n• Wedesignareconfigurabledataflowarchitecturethatex- Clock 1530MHz 300MHz\nploitsscalabilityandLUTpotentialtosavecomputational 5120CUDAcores\nComputecores 9024DSP48E2\nresources. 640Tensorcores\n• UsingLUTMUL,FPGAdesignscanpotentiallyexceedthepeak 14TFLOPs(FP32CUDA)\nPerformance 24.5TOPs(INT8)\nperformanceofconventionalDSP-basedFPGAaccelerators 112TFLOPs(FP16Tensor)\nwhenusingthesameamountofresources. 32GBDDR4\nMemory 32GBHBM2\n8GBHBM2\n2 BACKGROUND 38GB/s(DDR4)\nBandwidth 900GB/s\n460GB/s(HBM2)\n2.1 RooflineModelAnalysis\n225W(Max)\nPower 250W\nGPUsleverageSingleInstructionMultipleData(SIMD)architecture, 100W(Typical)\nallowingthemtosimultaneouslyperformthesameoperationacross Price $11,458 $7,717\nmultipledatapoints.ThisparallelismmakesGPUsexceptionally\nefficientfortasksthatcanbedividedintosmaller,similaroperations,\nFigure1showstherooflinemodelforU280.Wetake 1 resource\nsuchasmatrixmultiplicationindeeplearning. 64\nandHBMbandwidthofU280foranalysis.ConventionalDSP-based\nFPGAs,bycontrast,achieveparallelprocessingthroughtheir\nacceleratorsarecomputeboundwhenthearithmeticintensitysat-\nreconfigurability,allowinghardwaretobetailoredtospecificcom-\nisfiesathreshold.OurLUTMULexploitsthepotentialofLUTsand\nputationaltasks.ThisflexibilityallowsFPGAstoefficientlyhandle\ncanachievehigherparallelismbyourLUTefficientmapping.\ncomplexanddiversedataprocessingtasks,offeringadvantages\noverthefixedarchitectureofGPUs.WhileFPGAslacktheraw\nRoofline Model Analysis for DSP and LUTMUL Theoretical Performance\nSIMDpowerofGPUsforcertainapplications,theyexcelinsce- Peak Memory Bandwidth\nnariosrequiringcustomhardwareconfigurationsorlow-latency, Peak Performance LUTMUL\nsuchasspecificsignalprocessingtasksorcustommachinelearning DSP\nalgorithms.However,thisadaptabilityoftencomeswithatrade-off\ninprocessingspeedandeaseofprogramming,withFPGAstypically\nlaggingbehindthecomputationalthroughputofGPUs.\nTherooflinemodel[31]isausefultoolforanalyzingtheperfor-\nmanceofbothGPUsandFPGAs.AnalgorithmrunningonGPUsor\nFPGAscanbeeithercomputeboundormemorybound.According 102 7.19 GB/s\ntotherooflinemodel[39],thepeakperformanceofFPGAsis:\n𝑃𝑒𝑎𝑘𝑝𝑒𝑟𝑓𝑜𝑟𝑚𝑎𝑛𝑐𝑒 =𝑝×𝑃𝐸𝑠×2×𝑓, (1)\n101 102\nwhere𝑃𝐸𝑠isthenumberofprocessingelements(PEs)usedinthe Operational Intensity (Ops/Byte)\naccelerator,suchastheDSPblocks,𝑓 istheclockfrequency,and Figure1:RooflinemodelanalysisforLUTMULandother\n×2termaccountsformultiply-accumulate(MAC)operations.The DSP-basedarchitectures.Wetake 1 resourceandmemory\npackingfactorforDSPblocks,𝑝,variesbasedonthebit-widthof\nbandwidthofU280foranalysis.\n64\ntheoperation,with𝑝 =1for16-bit,𝑝 =2for8-bit,and𝑝 =4for\n4-bitmultiply-accumulateoperations.\nFurthermore, the performance of an FPGA-based accelerator 2.2 DataflowArchitecture\nis also limited by the memory, which is related to the memory\nDataflowarchitecturecontrastswiththetraditionalcontrolflow\nbandwidth(BW)andcomputation-to-communication(CTC)ratio:\narchitecture.Thedataflownodesorprocessingelementscanim-\n𝑃𝑒𝑎𝑘𝑚𝑒𝑚𝑜𝑟𝑦𝑏𝑎𝑛𝑑𝑤𝑖𝑑𝑡ℎ=𝐵𝑊 ×𝐶𝑇𝐶𝑟𝑎𝑡𝑖𝑜. (2) mediatelystartandexecutewhenallitsinputsareready.Dataflow\narchitectureemployssimpleoperations,suchasbroadcast(one-\nTable1summarizesthemajordifferencesbetweenGPUsand to-many),map(element-wise,e.g.activationfunction),zip(multi-\nFPGAs,suchasclockfrequency,numberofcomputecores,and operands,e.g.convolutionandmatrixmultiplication),andreduce\nmemorybandwidth.Thesignificantdifferenceinclockfrequency (many-to-one,e.g.pooling)[22].Akeyadvantageofreconfigurable\ncontributestoanotableperformancegapbetweenGPUsandFPGAs. dataflowarchitectureisitsabilitytoallowdatatoflowthroughthe\nEven with optimization such as pruning and quantization [10], computationgraphefficiently,significantlyenhancingparallelism\nFPGAinferencespeedgenerallyremainsinferiortothatofGPUs. andminimizingmemoryaccesstime.\n)sPOG(\necnamrofreP\nLUTMUL:ExceedConventionalFPGARooflineLimitbyLUT-basedEfficientMULtiplicationforNeuralNetworkInference ASPDAC’25,January20–23,2025,Tokyo,Japan\n2.3 FPGA-basedNeuralNetworkAccelerator 3.2 LUTMULDesignFlow\nArchitecture\nSinceFPGAshaverelativelylimitedon-chipresources,mostofthe\nFPGAaccelerators[3,24,32,34,37]mapcomputationontohard- Quantization-Aware\nQuantization-aware training\nwareandreusethePEarray.Notably,[18]explorestheintra-layer Training\nmixed-schemequantizationandmapsvisiontransformerlayers\nontoaGeneralMatrixMultiply(GEMM)kernel,whereeachlayer Exported quantized ONNX layers\nmaintains afixed ratioof this quantizationscheme. Systolicar- Import, streamlining transformations, reordering\nrayarchitecture[30]presentsanothermethodforefficientlymap- Hardware\nConvert to HLS layers using HLS templates\nGeneration\npingconvolutionandmatrixmultiplicationontoFPGAswithhigh\nAdjust folding for performance/resource requirements\nthroughput.\nStitched IP design\nFINN[2,26]usesadataflowarchitectureandintegratesalllayers Generate IP cores, stitch IP design by Vivado\nofthenetworkintoasingleFPGA.Theresourcesforeachlayer\ncanbeadjustedaccordingtocomputationrequirementssothatall Deployment PYNQ-provided Python abstractions and drivers\nlayersarebalancedandpipelinedforbetterthroughput.FINNis\nparticularlywell-suitedforexploitinginter-layerquantizationfor\nneuralnetworksbecauseeachlayerhasdedicatedcomputationand\nmemoryresources.\n3 ALGORITHM-HARDWARECO-DESIGNFOR\nLUTMUL Figure3:LUTMULDesignflow.\n3.1 Motivation\nFigure3depictstheLUTMULdesignflow.Initially,wetrainthe\nTherooflinemodelrevealsatheoreticalpeakperformanceforDSP-\nneuralnetworkinourquantization-awaretrainingframework.The\nbasedaccelerators,applicableacrossvariousarchitecturessuchas\nquantizationbit-widthsforweightsandactivationsareadjustable\nGEMM,systolicarray,ordataflowarchitecture.Wecanleverage\nhyperparameters.Thefinalquantizedneuralnetworkisexported\nLUTresourcestoperformmultiplicationandfullparallelismto\ninOpenNeuralNetworkExchange(ONNX)format,facilitating\nenableFPGAwithgreaterperformance.Giventhattheavailability\nsubsequenthardwaregeneration.\nof LUTs usually outnumbers DSPs, using LUTs can potentially\nTheONNXintermediaterepresentationisinterpretedasacom-\nexceedtheupperboundofperformanceofcurrentDSP-basedFPGA\nputationgraphandundergoesastreamliningtransformation[27].\naccelerators.\nThescalingfactorsofeachchannelandbatchnormalizationlayer\narereorderedandabsorbedintotheactivationfunction,transform-\nAccuracylossandLUTresourcesfor1-bit to 8-bit quantization\n16% 70 ingintoamulti-thresholdunit.Eachcomputationnodeisconverted\ntoaHigh-LevelSynthesis(HLS)layerusingourHLStemplates.\n14% 60\nTheseHLSlayersarefoldedaccordingtoperformanceandresource\n12%\n50 requirementsandinterconnectedsequentially.Thefinalhardware\n10%\n40 bitstream,generatedbyVivado,isdeployedonFPGAboardsvia\nAc loc su sra ↓cy 8% mu# ltL ipU liT cs atp ioe nr ↓ thePYNQframework.\n30\n6%\n4% 20 3.3 ReconfigurableDataflowArchitecture\n2% 10 Figure4illustratesthehardwarearchitectureofaMobileNetV2[23]\n0% 0 implementation.Thisdesign,focusingoninvertedresidualblocks,\n1-bit 2-bit 3-bit 4-bit 5-bit 6-bit 7-bit 8-bit\nQuantizationbit-width employsaFirstIn,FirstOut(FIFO)bufferbetweenlayerstostore\nactivations.Thearchitectureusesareconfigurabledataflowarchi-\n# LUTs per multiplication Accuracy loss\ntecture.\nFigure2:AccuracylossandLUTresourcesfor1-bitto8-bit OurdesignspansallSuperLogicRegions(SLRs)tomaximize\nquantization. hardwareresources.SignalsonlytraverseSLRswhenthecurrent\nSLRresourcesareinsufficientforthenextlayertoavoidseveretim-\nFigure2showsthequantizedneuralnetworkaccuracy[14,36] ingviolations.Dataflowarchitectureisinherentlysuitedfordesign\nandthenumberofLUTsneededpermultiplicationbyourmethod. spanningmultipleSLRsandcanbescaledup,enablingadditional\nWetrade-offbetweenaccuracyandLUTusageandchoose4-bitas FPGAsconnectedvianetworkfordeployinglargernetworks[6].\nourquantizationbit-width.Binaryandternaryneuralnetworks\n3.4 ConvolutionGenerator\nincurlargeaccuracydropsandconsumehalfoftheLUTsthat4-bit\nusesastheoutputbitsofLUTsarelimited.Comparedwithhigher Forconvolutionallayers,theconvolutionoperationscanbelowered\nbit-widthquantization,4-bitusessignificantlyfewerLUTsandhas tomatrix-matrixmultiplications.Thesecanbemappedinastream-\nnegligibleaccuracyloss. ingmannerandfedtothemultiplicationkernel.Themultiplication\nASPDAC’25,January20–23,2025,Tokyo,Japan YanyueXie,ZhengangLi,DanaDiaconu,SurangaHandagala,MiriamLeeser,andXueLin\nAlgorithm1showsthepseudoHigh-LevelSynthesis(HLS)code\nSliding forlook-uptablebasedmultiplication.Thetablecontentsarede-\nWindow LUT-based MUL\nrivedfrompre-computedweights.Theweightsofconvolutional\nUnit\nFIFO FIFO Add with bias SR te res au mlt layersarefullyparalleled,meaningthatthe𝐶𝑂𝑈𝑇 channelinAlgo-\nMax Threshold Memory rithm1referstotheoutputchannels,andthe𝐶𝐼𝑁 channelrefersto\noperator theinputchannelstimesthekernelsizesquared.Thesedimensions\nSLR2 (fourintotal)arefullyunrolledinthespatialdomain.Theremain-\ninginputfeaturemapheightandwidthdimensionsarepipelinedin\nPointwise Conv Generator Depthwise Conv Generator\nthetemporaldomain.Inputactivationsarestreamedfromthecon-\nDDR4 LUT-based MUL\nFIFO\nLUT-based MUL v reo sl uu lt ti son arg ee mn uer lta it po lr ica an tid onpa ress se ud ltst .h Tro hu eygh arl eoo ak cc-u up mt ua lb al te es d. ,T gh oe tho ru ot up gu ht\nAdder tree Adder tree\nthethresholdunit,andgenerateactivationsforthenextlayer.\nThreshold Memory Threshold Memory\nSLR1\nAlgorithm1Look-uptablebasedmultiplicationkernel\nPointwise Conv Generator\nInput: StreamingparallelinputfromtheConvolutionGeneratorandpre-\ncomputedlook-uptablecontents\nDDR4 SIm trea ag me LUT A- db da es re d\ntr\neM eUL FIFO O 1u\n:\nt fp ou rt 𝑖: ←Str 1ea 𝑡m 𝑜i 𝑅n 𝑂g 𝑊ou 𝑆tp ×ut 𝐶f 𝑂or 𝐿t 𝑆he dn oextlayer\n2: #pragmaHLSPIPELINEII=1\nThreshold Memory SLR0 3: 𝑖𝑛𝑝𝑢𝑡 ←𝑠𝑟𝑐.𝑟𝑒𝑎𝑑()\n4: for𝑐𝑜←1𝑡𝑜𝐶𝑂𝑈𝑇 do\nHBM HBM\n5: #pragmaHLSUNROLL\n6: for𝑐𝑖←1𝑡𝑜𝐶𝐼𝑁 do\nFigure4:Hardwarearchitectureofacceleratorgeneratedby 7: #pragmaHLSUNROLL\nLUTMUL.Ourdesignisfullyon-chipanddoesnotuseDRAM 8: 𝑚𝑢𝑙[𝑐𝑜][𝑐𝑖]←𝑙𝑢𝑡[𝑐𝑜][𝑐𝑖][𝑖𝑛𝑝𝑢𝑡[𝑐𝑖]]\norHBMmemory. 9: endfor\n10: endfor\n11: for𝑐𝑜←1𝑡𝑜𝐶𝑂𝑈𝑇 do\n12: #pragmaHLSUNROLL\nkernelisfullyparalleledtoperformamatrix-vectormultiplication, 13: for𝑐𝑖←1𝑡𝑜𝐶𝐼𝑁 do\nwheretheweightsarestationaryvectorsandactivationsarestream- 14: #pragmaHLSUNROLL\ninginputs.Therefore,weneedaconvolutiongeneratortoperform 15: 𝑜𝑢𝑡𝑝𝑢𝑡[𝑐𝑜]+=𝑚𝑢𝑙[𝑐𝑜][𝑐𝑖]\ntheim2coloperations:readingdatafromFIFO,movingacrossinput 16: endfor\n17: endfor\nimagestoformanimagematrix,andstreamingtheoutputtothe\n18: 𝑑𝑠𝑡.𝑤𝑟𝑖𝑡𝑒(𝑜𝑢𝑡𝑝𝑢𝑡)\nmultiplicationkernel.\n19: endfor\nTheconvolutiongeneratoraccommodatesvariousconfigura-\ntions,includingpointwise,depthwise,andstandardconvolution\nwithdifferentkernelsizesandstrides,sinceeachkindofconvolu-\n3.6 Quantization-AwareTraining\ntionallayerexpectsdifferentinputdatasequences,necessitating\nspecificgeneratorsettings. Quantization[10]andDSPpacking[24]havebecomeastandard\napproachformappingneuralnetworksontoFPGA-basedaccel-\n3.5 Look-UpTablebasedEfficient erators, as FPGAs’ LUTs and DSP blocks are not optimized for\nMultiplication floating-pointbutidealforintegerorfixed-pointoperations.Quan-\ntization,pairedwithDSPpacking,reducesresourcedemandsfor\nFigure5demonstratesthelook-uptablebasedmultiplicationker-\nthemultiplicationsandimprovesthroughput.\nnelsandhowtodeterminelook-uptableinitialization(INIT)values.\nThequantizationoperationisdefinedas:\nAfterembeddingtheweightsintolook-uptables,ourlook-uptables\n𝑥\ntransformintoefficientconstantmultipliers[11].Ourlook-uptable 𝑦=𝑞𝑢𝑎𝑛𝑡𝑖𝑧𝑒(𝑥)=𝑐𝑙𝑎𝑚𝑝(𝑟𝑜𝑢𝑛𝑑( +𝑧),𝑦 𝑚𝑖𝑛,𝑦 𝑚𝑎𝑥), (4)\n𝑠\nbasedmultiplierisefficientinresources,utilizingonly2LUTson\naverageforasingle4-bitmultiplication,comparedwithageneral where𝑥 is the floating-point value to quantize,𝑠 is the scaling\nmultiplierthatconsumes13-28LUTsforanequivalentoperation. factoroftheoutputquantizedtensor,and𝑧 isthezero-pointor\nThechoiceof4-bitquantizationispivotalasitmaintainsmodel quantizationbiascoefficient.Thefunction,𝑟𝑜𝑢𝑛𝑑,canberound-to-\naccuracyandoptimizeslook-uptableusage,asshowninFigure2. evenorround-to-zero,and𝑐𝑙𝑎𝑚𝑝 performsclippinginclusiveof\nWeshowthenumberofLUT6(6:1LUT,6-bitinput,1-bitoutput)for theboundaries𝑦 𝑚𝑖𝑛and𝑦 𝑚𝑎𝑥.\nageneraln-bitmultiplication(n:2nLUT,n-bitinput,2n-bitoutput) Forthereverseprocess,tocomputethefloating-pointrepresen-\nusingourmethod: tationofaquantizedvalue,wedefinethedequantizeoperation\n2𝑛×2𝑛 as:\n#𝐿𝑈𝑇𝑠 = 1×26 . (3) 𝑑𝑒𝑞𝑢𝑎𝑛𝑡𝑖𝑧𝑒(𝑦)=𝑠(𝑦−𝑧), (5)\nLUTMUL:ExceedConventionalFPGARooflineLimitbyLUT-basedEfficientMULtiplicationforNeuralNetworkInference ASPDAC’25,January20–23,2025,Tokyo,Japan\nLUT6_2 #3 LUT6_2 #2 Activation(uint4) Weight=1 OutpMut(uinlt8ti)pWliecigahtti=o-3n Ourtepsuut(ilntt8s)\n1'b1 1'b1 Activation\n4'b0000 8 W'b0 e0 i0 g0 h00 t0 =0 1,WS=08'b W000 e0 i0 g0 h0 t0\n=-3,WS=1\nLUT5 O7 LUT5 O5 4'b0001 8'b00000001 8'b11111101\nWS WS\n4'b0014 0’b0000 8'b0 000 0000 010 00000 8'b110 1110 010 000000\n4’b0001 00000001 11111101\nLUT5 O6 LUT5 O4 4'b0011 8'b00000011 8'b11110111\n4’b0010 00000010 11111010\nA[3:0] 4 A[3:0] 4 4'b010 40 ’b0011 8'b 000 0000 010 00 0011 8'b11 1110 110 10 10111\nINIT=64'hfffe_0000_fffe_0000 INIT=64'h07fe_0000_f83e_0000 4'b0101 8'b00000101 8'b11110001\n4’b0100 00000100 11110100\n6-Input Look-Up Table 6-Input Look-Up Table 4'b01140’b0101 8'b0000000011000101 8'b1111011111010001\n4'b01141’b0110 8'b0000000011010110 8'b1111011011101110\n4'b10040’b0111 8'b0000001000000111 8'b1111011001001011\n1'b1\nLUT6 _2 #1\n1'b1\nLUT6 _2 #0 4'b10041’b1000 8'b0000001000011000 8'b1111001101101000\n4’b1001 00001001 11100101\nLUT5 O3 LUT5 O1\n4 4' 'b b1 10 01\n14\n10\n’b1010\n8 8' 'b b00 00 000 00 01 100 01\n0\n110\n1010\n8 8' 'b b1 11 11\n1\n00 10\n1\n10 111\n1\n10\n00010\nWS WS 4’b1011 00001011 11011111\n4'b1100 8'b00001100 8'b11011100\nLUT5 O2 LUT5 O0 4’b1100 00001100 11011100\n4'b11041’b1101 8'b0000001010011101 8'b1110111000111001\nA[3:0] 4 INIT=64'h39c6_ ff00_5a5a_f0f0 A[3:0] 4 INIT=64'hcccc_ cccc_aaaa_aaaa 4'b11140’b1110 8'b0000000111001110 8'b1110110110010110\n4'b11114’b1111 8'b0000000111011111 8'b1110110010110011\n6-Input Look-Up Table 6-Input Look-Up Table\nFigure5:IllustrationofLUTMULforefficientmultiplicationvialook-uptables.Theleft-handsidefiguredemonstrateshow\ntouseLUT6_2primitiveforembeddingmultiplicationresultsofweightsandinputactivations.Theright-handsidetable\ndemonstratesthemultiplicationresultsoftwoexampleweightsandhowtogeneratethecorrespondinglook-uptablecontents.\nTheweights(int4)andmultiplicationoutput(int8)areusingtwo’scomplementrepresentation,whileactivationareallunsignednumbers\n(uint4). The Most Significant Bit (MSB) of LUT6_2 input is configured as ‘1’ to enable two output ports. The bit below the MSB is a\nWeightSelect(WS)signaltoselectbetweentwoweights.Thelowest4-bitinputsserveasactivationinputs.Ourmethodembedstwo\nint4weightsinsidefourLUT6,aresource-efficientapproachcontrastingwiththeLUT6-instantiatedgeneralmultipliers,whichconsume\n6-14×moreLUT6resources.Twousedexampleweightsare1and-3respectively.TheembeddedLUTcontentsforthesefourLUTsare\n64’hfffe_0000_fffe_0000,64’h07fe_0000_f83e_0000,64’h39c6_ff00_5a5a_f0f0,and64’hcccc_cccc_aaaa_aaaa,respectively.\nwhere𝑦isaquantizedtensor,𝑧isitszero-point,and𝑠isthescaling quantizationschemeforourmodel.OurquantizedMobileNetV2\nfactor. networkistrainedfor420epochs,culminatingina70.95%top-1\nQuantization introduces errors to the trained model parame- accuracyevaluatedontheImageNetdataset[5].\ntersandresultsinperformancedegradation.Quantization-Aware Forthehardwareevaluation,theutilizeddevelopmentplatform\nTraining(QAT)isapopularapproachthatretrainsthemodelwith istheAMDXilinxAlveoU280datacenteracceleratorcardonthe\nquantizedparametersonthepretrainingdatasettoconvergetothe OpenCloudTestbed(OCT)[40].Weimplementthefirst15layers\npretrainedmodelperformance.Theusualforwardandbackward ofMobileNetV2inafullyparallelmannerandfoldtheremaining\npassesareperformedonthequantizedmodelinfloatingpoint,and layers for resource optimization. To maximize the computation\nthemodelparametersarequantizedaftereachgradientupdate. efficiencywithouttimingviolation,theworkingfrequencyissetto\nInparticular,itisimportanttodothisprojectionaftertheweight 333MHzforallthedesignsimplementedthroughVitisHLS/Vivado\nupdateisperformedinfloatingpointprecision.Performingthe 2022.1.\nbackward pass with floating point is vital, as accumulating the\ngradients inquantized precision canresult in zero gradientsor\ngradientswithhigherror,especiallyinlow-precisionscenarios[9].\n4.2 ExperimentalResults\n4 EVALUATION Table2showcasesthehardwareperformanceandcomparisonswith\notherFPGA-basedMobileNetaccelerators.Mostoftheseaccelera-\n4.1 ExperimentalSetup\ntorsaretailoredforedgeFPGAs,suchasZU9EG,exceptforFINN,\nToevaluatetheperformanceofLUTMUL,weimplementMobileNetV2 whichhasdatacenteracceleratorimplementationforMobileNetV1.\nonFPGAsandcompareitwithexistingFPGA-basedMobileNetac- TheFINNresultisgeneratedandtestedonthesamedeviceasour\ncelerators.MobilenetV2[23]has3.4Mparametersandachieves implementation,whileotherdatapointsareextractedfromtheir\n71.88%top-1accuracyontheImageNetdataset[5].Weutilizethe originalpublications.\nFINNframework[2]asourfoundationalplatform.Forquantiza- Intermsofaccuracy,ourmodelachievesthebest70.95%top-1\ntion,weadoptPyTorch1.13.0andBrevitas0.9.1[20].Specifically, accuracyonImageNetamongallimplementations.Quantization-\nwechoose4-bitforweightsandactivationsquantizationexcept awaretrainingeffectivelymitigatesquantizationerrors,preserving\nforthefirstandlastlayers,whicharesetas8-bit.Topreservethe themodeloriginalaccuracy,evenwith4-bitquantizedweightsand\naccuracyoftheMobileNetV2model,weapplythechannel-wise activations.\nASPDAC’25,January20–23,2025,Tokyo,Japan YanyueXie,ZhengangLi,DanaDiaconu,SurangaHandagala,MiriamLeeser,andXueLin\nTable2:ComparisonsofMobileNetimplementationsbetweenpreviousFPGA-basedaccelerators.\nFINN FPL’19 Light-OPU FPL’21 Mix&Match FILM-QNN LUTMUL\nImplementation\n[2] [32] [37] [34] [3] [24] (Ours)\nNetwork MobileNetV1 MobileNetV2 MobileNetV3 MobileNetV2 MobileNetV2 MobileNetV2 MobileNetV2\nBit-Width W4A4 W8A8 W8A8 W8A8 W4A4 W8A5&W4A5 W4A4\nTop-1Accuracy 70.4% 68.1% 66.7% 70.8% 65.6% 65.7% 70.95%\nPlatform AlveoU280 ZU9EG XC7K325T XC7V690T XC7Z045 ZU9EG AlveoU280\nFrequency(MHz) 333 333 200 150 100 150 333\nLUT 501363 161944 173522 308449 145049 180100 529242\nFF 476316 301416 241175 278926 111575 - 503192\nBRAM36 898 771 193.5 941.5 225.5 440.5 1119\nDSP 106 2070 704 2160 900 2092 106\nPower(W) 41.69 - 8.5* 11.35 - 12.9 42.12\nFrameRate(FPS) 925 809.8 332.6 302.3 549.3 537.9 1627\nThroughput(GOPS) 556.4 487.1 84.48* 181.8 326.9 320.1 978.6\nEnergyEfficiency(GOPS/W) 13.35 - 9.9 16.02 - 24.8 23.23\nNote:‘-’meansthattheresultisnotgivenintheoriginalpublications,and‘*’meansthattheresultisinferredfromtheoriginalpublications.\nAsfortheinferenceperformance,ourimplementationachieves performancebyLUTMULstilloutperformstheDSPpackingmethod\nathroughputof1627imagespersecond.Ourimplementationcon- usingthesamenumberofresources.\nsumesthemostFPGAresourcesbutcouldstillfitonasingleAlveo\nU280.However,itisnoteworthythatourimplementationalsoyields\n4.4 ComparisonswithRelatedWorks\na23.23GOPS/Wenergyefficiency,marginallylowerthantheFLIM-\nQNN[24],whichisimplementedonamorepower-efficientedge Ourmethodisnotonlylimitedtointegermultiplication,butcan\nFPGAboard. also be extended to customized data formats, such as FP4 and\nMXFP4[21],whileDSPpackingisdesignedefficientlyforinteger\n4.3 Discussion\nformats.LUTNet[28,29]alsoutilizesLUTforinferenceandex-\nplorestheflexibilityofLUT.However,LUTNetdesignsuffersfrom\nSynthesis Implementation lowaccuracywhenthenetworkbecomeslarger.PolyLUT[1]trains\nMemory (multiplication) Expression (addition) LUT as ROM Adder logic multivariatepolynomialsinsteadoflinearfunctionsandembeds\nMemory piecewisepolynomialfunctionsintoLUTs.CompressedLUT[15]\n(multiplication)\n1829 proposesalosslessLUTcompressionmethodandisefficientfor\n11%\nnon-linearfunctionsandlargeLUTlogicblocks,suchas[8,12,25].\nOurmethodmapsMACoperationstothesingle-LUTlevel,and\nAdder logic LUT as ROM Vivadocanhandleremaininglogicoptimizationefficiently.\n2645 3277\n45% 55%\nExpression (addition)\n14944 5 CONCLUSION\n89%\nWeproposeLUTMUL,anefficientmethodthatleverageslook-upta-\nblesformultiplicationinconvolutionalneuralnetworks.Compared\nwiththegeneralmultiplier,ourmethodisefficientinresources,\nFigure6:LUTResourcebreakdownofthesecondconvolution\nwhichonlyneedstwolook-uptablesonaverageforasingle4-bit\nlayerinMobileNetV2usingLUTMUL.\nmultiplication.ComparedwithotherDSP-basedFPGAaccelerators,\nLUTMUL’sreconfigurabledataflowarchitectureenablesfullparal-\nFigure6showstheLUTresourcebreakdownofthesecondcon- lelism,reducesmemoryaccesstime,andincreasesthetheoretical\nvolutionlayerinMobileNetV2usingLUTMUL,whichisa1×1con- upperboundofperformance.Experimentalresultsdemonstrate\nvolutionkernelandhas32inputchannelsand32outputchannels. thatourdesignmaintainsatop-1accuracyof70.95%ontheIma-\nForthese10244-bitweights,multiplicationoperationsuse1829 geNetdatasetandachievesathroughputof1627imagespersecond\nLUTsafterHLSsynthesis,whichmatchesthetheoreticalanalysis onasingleAlveoU280FPGA,outperformingotherFPGA-based\nofLUTMUL.However,HLSinstantiatesanadderforeachaddition MobileNetaccelerators.\noperationtoachieveanIIof1,resultinginahighusageofLUT\nforadderlogic.AfterVivadoimplementation,theLUTusagede-\nACKNOWLEDGMENTS\ncreasedto5922.Vivadooptimizesthelogicandinstantiates3277\nLUTsasROMand2645LUTsasadderandotherlogic.Eventhough ThisresearchwassupportedinpartbytheNationalScienceFoun-\nadderlogicaccountsforalargepartofresources,theparallelMAC dationunderGrantsCCF-1901378,CNS-1925658,andCNS-2319962.\nLUTMUL:ExceedConventionalFPGARooflineLimitbyLUT-basedEfficientMULtiplicationforNeuralNetworkInference ASPDAC’25,January20–23,2025,Tokyo,Japan\nREFERENCES\nKristofDenolf,etal.2023.Microscalingdataformatsfordeeplearning.arXiv\n[1] MartaAndronicandGeorgeAConstantinides.2023.PolyLUT:learningpiecewise preprintarXiv:2310.10537(2023).\npolynomialsforultra-lowlatencyFPGALUT-basedinference.In2023Interna- [22] SambanovaWhitepaper.2021.AcceleratedComputingwithaReconfigurable\ntionalConferenceonFieldProgrammableTechnology(ICFPT).60–68. Dataflow Architecture. https://sambanova.ai/wp-content/uploads/2021/\n[2] MichaelaBlott,ThomasBPreußer,NicholasJFraser,GiulioGambardella,Kenneth 04/SambaNova_Accelerated-Computing-with-a-Reconfigurable-Dataflow-\nO’brien,YamanUmuroglu,MiriamLeeser,andKeesVissers.2018.FINN-R:An Architecture_Whitepaper_English.pdf. LastaccessedOct.30,2024.\nend-to-enddeep-learningframeworkforfastexplorationofquantizedneural [23] MarkSandler,AndrewHoward,MenglongZhu,AndreyZhmoginov,andLiang-\nnetworks.ACMTransactionsonReconfigurableTechnologyandSystems(TRETS) ChiehChen.2018. Mobilenetv2:Invertedresidualsandlinearbottlenecks.In\n11,3(2018),1–23. ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition\n[3] Sung-EnChang,YanyuLi,MengshuSun,RunbinShi,HaydenK-HSo,Xuehai (CVPR).4510–4520.\nQian,YanzhiWang,andXueLin.2021.Mixandmatch:Anovelfpga-centricdeep [24] MengshuSun,ZhengangLi,AlecLu,YanyuLi,Sung-EnChang,XiaolongMa,\nneuralnetworkquantizationframework.In2021IEEEInternationalSymposium XueLin,andZhenmanFang.2022.Film-qnn:Efficientfpgaaccelerationofdeep\nonHigh-PerformanceComputerArchitecture(HPCA).IEEE,208–220. neuralnetworkswithintra-layer,mixed-precisionquantization.InProceedings\n[4] HongzhengChen,JiahaoZhang,YixiaoDu,ShaojieXiang,ZichaoYue,Niansong ofthe2022ACM/SIGDAInternationalSymposiumonField-ProgrammableGate\nZhang,YaohuiCai,andZhiruZhang.2024.Understandingthepotentialoffpga- Arrays(FPGA).134–145.\nbasedspatialaccelerationforlargelanguagemodelinference.ACMTransactions [25] YamanUmuroglu,YashAkhauri,NicholasJamesFraser,andMichaelaBlott.2020.\nonReconfigurableTechnologyandSystems(TRETS)(2024). LogicNets:Co-designedneuralnetworksandcircuitsforextreme-throughput\n[5] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.2009.Ima- applications.In202030thInternationalConferenceonField-ProgrammableLogic\ngenet:Alarge-scalehierarchicalimagedatabase.InProceedingsoftheIEEE/CVF andApplications(FPL).IEEE,291–297.\nConferenceonComputerVisionandPatternRecognition(CVPR).248–255. [26] YamanUmuroglu,NicholasJFraser,GiulioGambardella,MichaelaBlott,Philip\n[6] DanaDiaconu,YanyueXie,MehmetGungor,SurangaHandagala,XueLin,and Leong,MagnusJahre,andKeesVissers.2017.Finn:Aframeworkforfast,scal-\nMiriamLeeser.2023.MachineLearningAcrossNetwork-ConnectedFPGAs.In ablebinarizedneuralnetworkinference.InProceedingsofthe2017ACM/SIGDA\n2023IEEEHighPerformanceExtremeComputingConference(HPEC).IEEE,1–7. InternationalSymposiumonField-ProgrammableGateArrays(FPGA).65–74.\n[7] PeiyanDong,MengshuSun,AlecLu,YanyueXie,KennethLiu,ZhenglunKong, [27] YamanUmurogluandMagnusJahre.2017.Streamlineddeploymentforquantized\nXinMeng,ZhengangLi,XueLin,ZhenmanFang,etal.2023.Heatvit:Hardware- neuralnetworks.arXivpreprintarXiv:1709.04060(2017).\nefficientadaptivetokenpruningforvisiontransformers.In2023IEEEInternational [28] ErweiWang,JamesJDavis,PeterYKCheung,andGeorgeAConstantinides.\nSymposiumonHigh-PerformanceComputerArchitecture(HPCA).IEEE,442–455. 2019.LUTNet:RethinkinginferenceinFPGAsoftlogic.In2019IEEE27thAnnual\n[8] DanielGerlinghoff,BenjaminChenMingChoong,RickSiowMongGoh,Weng- InternationalSymposiumonField-ProgrammableCustomComputingMachines\nFaiWong,andTaoLuo.2024.Table-LookupMAC:ScalableProcessingofQuan- (FCCM).IEEE,26–34.\ntisedNeuralNetworksinFPGASoftLogic.InProceedingsofthe2024ACM/SIGDA [29] ErweiWang,JamesJDavis,Georgios-IliasStavrou,PeterYKCheung,GeorgeA\nInternationalSymposiumonFieldProgrammableGateArrays(FPGA). Constantinides,andMohamedAbdelfattah.2022.Logicshrinkage:LearnedFPGA\n[9] AmirGholami,SehoonKim,ZhenDong,ZheweiYao,MichaelWMahoney,and netlistsparsityforefficientneuralnetworkinference.InProceedingsofthe2022\nKurtKeutzer.2022.Asurveyofquantizationmethodsforefficientneuralnetwork ACM/SIGDAInternationalSymposiumonField-ProgrammableGateArrays(FPGA).\ninference.InLow-PowerComputerVision.ChapmanandHall/CRC,291–326. 101–111.\n[10] SongHan,HuiziMao,andWilliamJDally.2016.DeepCompression:Compressing [30] XuechaoWei,CodyHaoYu,PengZhang,YouxiangChen,YuxinWang,Han\nDeepNeuralNetworkswithPruning,TrainedQuantizationandHuffmanCoding. Hu,YunLiang,andJasonCong.2017. Automatedsystolicarrayarchitecture\nInternationalConferenceonLearningRepresentations(ICLR)(2016). synthesisforhighthroughputCNNinferenceonFPGAs.InProceedingsofthe\n[11] MartinHardieck,MartinKumm,KonradMöller,andPeterZipf.2019.Reconfig- 54thAnnualDesignAutomationConference(DAC).1–6.\nurableconvolutionalkernelsforneuralnetworksonFPGAs.InProceedingsofthe [31] SamuelWilliams,AndrewWaterman,andDavidPatterson.2009.Roofline:an\n2019ACM/SIGDAInternationalSymposiumonField-ProgrammableGateArrays insightfulvisualperformancemodelformulticorearchitectures.Commun.ACM\n(FPGA).43–52. 52,4(2009),65–76.\n[12] JingkaiHong,ArashFayyazi,AmirhosseinEsmaili,MahdiNazemi,andMassoud [32] DiWu,YuZhang,XijieJia,LuTian,TianpingLi,LingzhiSui,DongliangXie,\nPedram.2023.AlgorithmsandHardwareforEfficientProcessingofLogic-based andYiShan.2019. Ahigh-performanceCNNprocessorbasedonFPGAfor\nNeuralNetworks.InProceedingsofthe60thACM/IEEEDesignAutomationCon- MobileNets.In201929thInternationalConferenceonFieldProgrammableLogic\nference(DAC).IEEE,1–6. andApplications(FPL).136–143.\n[13] SeongminHong,SeungjaeMoon,JunsooKim,SungjaeLee,MinsubKim,Dongsoo [33] AMDXilinx.2021.AlveoProductSelectionGuide.https://docs.xilinx.com/v/u/en-\nLee,andJoo-YoungKim.2022. DFX:ALow-latencyMulti-FPGAAppliance US/alveo-product-selection-guide. LastaccessedOct.30,2024.\nforAcceleratingTransformer-basedTextGeneration.In202255thIEEE/ACM [34] ShunYan,ZhengyanLiu,YunWang,ChenglongZeng,QiangLiu,BowenCheng,\nInternationalSymposiumonMicroarchitecture(MICRO).IEEE,616–630. andRayCCCheung.2021. Anfpga-basedmobilenetacceleratorconsidering\n[14] QingJin,LinjieYang,andZhenyuLiao.2020. Adabits:Neuralnetworkquan- networkstructurecharacteristics.In202131stInternationalConferenceonField-\ntizationwithadaptivebit-widths.InProceedingsoftheIEEE/CVFConferenceon ProgrammableLogicandApplications(FPL).IEEE,17–23.\nComputerVisionandPatternRecognition(CVPR).2146–2156. [35] GengYang,YanyueXie,ZhongJiaXue,Sung-EnChang,YanyuLi,PeiyanDong,\n[15] AlirezaKhataeiandKiaBazargan.2024. CompressedLUT:AnOpenSource JieLei,WeiyingXie,YanzhiWang,XueLin,andZhenmanFang.2024. SDA:\nToolforLosslessCompressionofLookupTablesforFunctionEvaluationand Low-BitStableDiffusionAccelerationonEdgeFPGAs.In202434thInternational\nBeyond.InProceedingsofthe2024ACM/SIGDAInternationalSymposiumonField ConferenceonField-ProgrammableLogicandApplications(FPL).264–273.\nProgrammableGateArrays(FPGA). [36] LinjieYangandQingJin.2021.Fracbits:Mixedprecisionquantizationviafrac-\n[16] BingbingLi,SantoshPandey,HaowenFang,YanjunLyv,JiLi,JieyangChen, tionalbit-widths.InProceedingsoftheAAAIConferenceonArtificialIntelligence\nMimiXie,LipengWan,HangLiu,andCaiwenDing.2020. Ftrans:energy- (AAAI),Vol.35.10612–10620.\nefficientaccelerationoftransformersusingfpga.InProceedingsoftheACM/IEEE [37] YunxuanYu,TiandongZhao,KunWang,andLeiHe.2020.Light-OPU:AnFPGA-\nInternationalSymposiumonLowPowerElectronicsandDesign(ISLPED).175–180. basedoverlayprocessorforlightweightconvolutionalneuralnetworks.InPro-\n[17] ZhengangLi,AlecLu,YanyueXie,ZhenglunKong,MengshuSun,HaoTang, ceedingsofthe2020ACM/SIGDAInternationalSymposiumonField-Programmable\nZhongJiaXue,PeiyanDong,CaiwenDing,YanzhiWang,etal.2024.Quasar- GateArrays(FPGA).122–132.\nViT:Hardware-OrientedQuantization-AwareArchitectureSearchforVision [38] ShulinZeng,JunLiu,GuohaoDai,XinhaoYang,TianyuFu,HongyiWang,\nTransformers.InProceedingsofthe38thACMInternationalConferenceonSuper- WenhengMa,HanboSun,ShiyaoLi,ZixiaoHuang,YadongDai,JintaoLi,Zehao\ncomputing(ICS).324–337. Wang,RuoyuZhang,KairuiWen,XuefeiNing,andYuWang.2024.FlightLLM:\n[18] ZhengangLi,MengshuSun,AlecLu,HaoyuMa,GengYuan,YanyueXie,Hao EfficientLargeLanguageModelInferencewithaCompleteMappingFlowon\nTang,YanyuLi,MiriamLeeser,ZhangyangWang,XueLin,andZhenmanFang. FPGA.InProceedingsofthe2024ACM/SIGDAInternationalSymposiumonField\n2022. Auto-vit-acc:Anfpga-awareautomaticaccelerationframeworkforvi- ProgrammableGateArrays(FPGA).\nsiontransformerwithmixed-schemequantization.In202232ndInternational [39] ChenZhang,PengLi,GuangyuSun,YijinGuan,BingjunXiao,andJasonCong.\nConferenceonField-ProgrammableLogicandApplications(FPL).IEEE,109–116. 2015.OptimizingFPGA-basedacceleratordesignfordeepconvolutionalneural\n[19] Nvidia. 2017. Nvidia Tesla V100 GPU Architecture Whitepaper. networks.InProceedingsofthe2015ACM/SIGDAInternationalSymposiumon\nhttps://images.nvidia.com/content/volta-architecture/pdf/volta-architecture- FieldProgrammableGateArrays(FPGA).161–170.\nwhitepaper.pdf. LastaccessedOct.30,2024. [40] MichaelZink,DavidIrwin,EmmanuelCecchet,HakanSaplakoglu,OrranKrieger,\n[20] AlessandroPappalardo.2023.Xilinx/brevitas. https://doi.org/10.5281/zenodo. Martin Herbordt, Michael Daitzman, Peter Desnoyers, Miriam Leeser, and\n3333552 SurangaHandagala.2021. TheOpenCloudTestbed(OCT):Aplatformfor\n[21] BitaDarvishRouhani,RitchieZhao,AnkitMore,MathewHall,AlirezaKho- researchintonewcloudtechnologies.In2021IEEE10thInternationalConference\ndamoradi,SummerDeng,DhruvChoudhary,MariusCornea,EricDellinger, onCloudNetworking(CloudNet).IEEE,140–147.",
    "pdf_filename": "LUTMUL_Exceed_Conventional_FPGA_Roofline_Limit_by_LUT-based_Efficient_Multiplication_for_Neural_Netw.pdf"
}