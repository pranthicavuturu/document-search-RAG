{
    "title": "METEOR Evolutionary Journey of Large Language Models from Guidance to Self-Growth",
    "abstract": "Model evolution enables learning from feed- back to refine experiences and update skills, transforming models from having no domain knowledge to becoming domain experts. How- ever, there is currently no unified and effec- tive method for guiding this evolutionary pro- cess. To address this gap, we propose the Meteor method, which includes three training phases: weak-to-strong data distillation, itera- tive training, and self-evolution strategies. Each phase maximizes the model’s inherent domain capabilities, allowing it to autonomously re- fine its domain knowledge and enhance per- formance. Experiments demonstrate that our approach significantly improves accuracy, com- pleteness, relevance, coherence, and reliability across domain-specific tasks. 1 Introduction The development of large language models (LLMs) has ushered in a new era in the field of natural language processing (NLP), showcasing remark- able general capabilities across a wide range of applications (OpenAI, 2023; Yang et al., 2024a,b; Reid et al., 2024). However, despite their outstand- ing performance on general tasks, the training of a highly versatile LLM demands substantial compu- tational resources and financial investment. These high costs restrict their use in many situations, par- ticularly in specific domains. In specific domains, there is often no need for a general purpose in- telligent model; instead, a model that acts as an expert within a particular domain is more desir- able. Furthermore, these domain-specific expert models should be trained at lower costs and eas- ily deployed in their respective fields. Therefore, finding efficient ways to create a domain-specific expert model has become a key research focus in the development of LLMs (Ling et al., 2024; Li et al., 2024a). *Corresponding author. Some studies leverage the inherent capabilities of LLMs combined with domain-specific external enhancements to enable their application in spe- cific domains. These methods involve explicitly or implicitly acquiring domain knowledge from external knowledge bases (Lu et al., 2023; Izac- ard et al., 2023; Schuurmans, 2023) or utilizing domain-specific tools to assist LLMs in specific domains (Jin et al., 2024; Li et al., 2023; Liang et al., 2023). However, these methods rely on the model’s strong general capabilities and are typically applicable only to models with a large number of parameters, making them costly to de- ploy and limiting their widespread adoption. Sev- eral researchers have explored model evolution ap- proaches to enhance domain-specific capabilities. Wu et al. (2023) achieves evolution in the finan- cial domain through extensive manually annotated domain data, yet this approach proves challenging to scale due to the difficulty in data acquisition. While Xi et al. (2024) proposes utilizing general large models for supervision and feedback on do- main model-generated data, this methodology re- mains constrained by the performance ceiling of the supervising model. Although Singh et al. (2024) made breakthrough progress in the coding domain through self-generated data and self-training, elimi- nating dependence on human annotations and large models, their approach has not yet been effectively extended to other specific domains. To address the challenges faced by current mod- els in domain-specific applications, we propose a self-evolution method named METEOR, a weak- to-strong evolution framework that enables LLMs to progressively evolve from supervised guidance to autonomous enhancement. Meteor offers a com- prehensive training framework that guides an LLM from having no domain expertise to becoming a domain expert. This framework consists of three key stages: an initial fine-tuning stage, which aims to impart basic domain knowledge to the LLM; an 1 arXiv:2411.11933v1  [cs.LG]  18 Nov 2024",
    "body": "METEOR: Evolutionary Journey of Large Language Models from\nGuidance to Self-Growth\nJiawei Li, Chong Feng, Yang Gao*\nSchool of Computer Science and Technology,\nBeijing Institute of Technology, Beijing, China\n{jwli, fengchong, gyang}@bit.edu.cn\nAbstract\nModel evolution enables learning from feed-\nback to refine experiences and update skills,\ntransforming models from having no domain\nknowledge to becoming domain experts. How-\never, there is currently no unified and effec-\ntive method for guiding this evolutionary pro-\ncess.\nTo address this gap, we propose the\nMeteor method, which includes three training\nphases: weak-to-strong data distillation, itera-\ntive training, and self-evolution strategies. Each\nphase maximizes the model’s inherent domain\ncapabilities, allowing it to autonomously re-\nfine its domain knowledge and enhance per-\nformance. Experiments demonstrate that our\napproach significantly improves accuracy, com-\npleteness, relevance, coherence, and reliability\nacross domain-specific tasks.\n1\nIntroduction\nThe development of large language models (LLMs)\nhas ushered in a new era in the field of natural\nlanguage processing (NLP), showcasing remark-\nable general capabilities across a wide range of\napplications (OpenAI, 2023; Yang et al., 2024a,b;\nReid et al., 2024). However, despite their outstand-\ning performance on general tasks, the training of a\nhighly versatile LLM demands substantial compu-\ntational resources and financial investment. These\nhigh costs restrict their use in many situations, par-\nticularly in specific domains. In specific domains,\nthere is often no need for a general purpose in-\ntelligent model; instead, a model that acts as an\nexpert within a particular domain is more desir-\nable. Furthermore, these domain-specific expert\nmodels should be trained at lower costs and eas-\nily deployed in their respective fields. Therefore,\nfinding efficient ways to create a domain-specific\nexpert model has become a key research focus in\nthe development of LLMs (Ling et al., 2024; Li\net al., 2024a).\n*Corresponding author.\nSome studies leverage the inherent capabilities\nof LLMs combined with domain-specific external\nenhancements to enable their application in spe-\ncific domains. These methods involve explicitly\nor implicitly acquiring domain knowledge from\nexternal knowledge bases (Lu et al., 2023; Izac-\nard et al., 2023; Schuurmans, 2023) or utilizing\ndomain-specific tools to assist LLMs in specific\ndomains (Jin et al., 2024; Li et al., 2023; Liang\net al., 2023). However, these methods rely on\nthe model’s strong general capabilities and are\ntypically applicable only to models with a large\nnumber of parameters, making them costly to de-\nploy and limiting their widespread adoption. Sev-\neral researchers have explored model evolution ap-\nproaches to enhance domain-specific capabilities.\nWu et al. (2023) achieves evolution in the finan-\ncial domain through extensive manually annotated\ndomain data, yet this approach proves challenging\nto scale due to the difficulty in data acquisition.\nWhile Xi et al. (2024) proposes utilizing general\nlarge models for supervision and feedback on do-\nmain model-generated data, this methodology re-\nmains constrained by the performance ceiling of the\nsupervising model. Although Singh et al. (2024)\nmade breakthrough progress in the coding domain\nthrough self-generated data and self-training, elimi-\nnating dependence on human annotations and large\nmodels, their approach has not yet been effectively\nextended to other specific domains.\nTo address the challenges faced by current mod-\nels in domain-specific applications, we propose a\nself-evolution method named METEOR, a weak-\nto-strong evolution framework that enables LLMs\nto progressively evolve from supervised guidance\nto autonomous enhancement. Meteor offers a com-\nprehensive training framework that guides an LLM\nfrom having no domain expertise to becoming a\ndomain expert. This framework consists of three\nkey stages: an initial fine-tuning stage, which aims\nto impart basic domain knowledge to the LLM; an\n1\narXiv:2411.11933v1  [cs.LG]  18 Nov 2024\n\nData Distillation\n…\nSpecific Domain Question\nQuestion1+Guideline1\nQuestion2+Guideline2\nQuestion3+Guideline3\nQuestionN+GuidelineN\n…\nDomain Model\n# PROMPT\nYou are an expert in the field of {specific \nfield}. I have a question that needs to be \nanswered: {question}. You do not need to \nanswer this question yourself, as I will use \na more powerful model, GPT-4, to provide \nthe answer. However, to ensure that GPT-4 \ncan answer the question more accurately, \nplease provide a detailed guideline to help \nit respond more precisely. The guideline is:\nGPT4\n# PROMPT\nBelow is a guideline provided by an expert in \nthe field of {specific field},followed by a \nspecific question that needs to be answered. \nPlease use the provided guideline to \ngenerate a precise and accurate answer to \nthe question. \\n guideline: {guideline}\\n \nQuestion: {question}“\nFine-tuning\nInterative Training\nSelf-training\nDomain-specific data\ndistilled from GPT4\nDomain \nModel\nmodel with preliminary\ndomain capabilities\nDomain \nModel\nGPT4\nRight Answer\nWrong Answer\nDomain \nModel\nCurrent Turn\nGreedy\nBeam-10\nPairwise Loss\nParameter\nUpdate\nPrevious Turn\nDomain Question\n# Title\nHow to Fine-Tune BERT for NER Tasks?\n# Body\nI'm trying to fine-tune a pre-trained BERT \nmodel for a Named Entity Recognition (NER) \ntask. What are the essential steps and \nrecommended tools for this process? Any \ntips on dataset preparation and evaluation \nmetrics would also be appreciated.\nCurrent\nAnswer\nQuestion + CoT + Answer\nDataset\nContinuous Fine-tuning\nAdvice\nRegenerate\n…\nFigure 1: Overview of the METEOR method, which is structured into three phases. Initially, weak-to-strong\nknowledge distillation is applied, wherein the distilled data is used to train the model, facilitating the initial\nacquisition of domain-specific capabilities. This is succeeded by iterative training, which further refines the model’s\ndomain expertise. Ultimately, self-training is conducted, enabling the model to achieve the proficiency of a domain\nexpert.\niterative training stage, where the LLM’s domain\nknowledge is further enhanced with the guidance\nof a strong general LLM (like GPT-4 (OpenAI,\n2023)); and a self-training stage, which aims to\nfurther improve the LLM’s capabilities. The over-\nall training architecture of Meteor is illustrated in\nFigure 1.\nKnowledge distillation from strong models is\nan effective approach to address the scarcity of\ndomain-specific data. However, due to the distribu-\ntional discrepancies between strong and weak mod-\nels, knowledge distilled directly from the strong\nmodel’s distribution may not be efficiently utilized\nby the weak model. To overcome this limitation,\nwe propose a weak-to-strong strategy. In this ap-\nproach, the weak model first provides guidelines\nfor the domain-specific questions to be distilled,\nbased on its own distribution. The strong model\nthen generates answers to these questions accord-\ning to the guidelines, thereby aligning the knowl-\nedge distribution between the strong and weak mod-\nels.\nFollowing knowledge distillation, models ac-\nquire initial domain capabilities but are still insuffi-\ncient to independently address complex problems\nin specific domains. Therefore, further domain evo-\nlution is necessary. Following knowledge distilla-\ntion, models acquire initial domain capabilities, en-\nabling them to perform basic domain-specific tasks.\nHowever, these capabilities are limited to direct\nproblem-solving without the ability to evaluate and\nrefine their own outputs. This limitation necessi-\ntates a guided evolution phase where strong models\nprovide structured feedback, helping domain mod-\nels develop self-examination abilities through itera-\ntive refinement (Shinn et al., 2023; Zelikman et al.,\n2022). Once models develop self-examination ca-\npabilities through guided reflection, they become\ncapable of identifying potential flaws in their rea-\nsoning. This creates the foundation for autonomous\nimprovement. Building on the theoretical relation-\nship between increased inference FLOPs and en-\nhanced model capabilities (Snell et al., 2024), we\nintroduce a self-training approach where models\nleverage their acquired self-examination abilities to\nprogressively enhance their performance through\ncomputational scaling, potentially surpassing their\nstrong supervisors.\nThe main contributions of this paper are as fol-\nlows:\n• We introduce a weak-to-strong evolution frame-\nwork that enables LLMs to progressively evolve\nfrom supervised guidance to autonomous en-\nhancement method named METEOR. Experi-\nmental results demonstrate that each phase of\nMETEOR enhances the performance of models\nin specific domain applications.\n• We propose a model data distillation method\n2\n\nbased on the weak-to-strong strategy, achieving\nalignment of domain knowledge distribution be-\ntween strong and weak models.\n• We present an evolution method guided by strong\nmodels based on reflection, as well as a self-\ntraining evolution method for models.\n2\nMETEOR\nThe METEOR method consists of three distinct\nphases: weak-to-strong data distillation, iterative\ntraining, and self-evolution strategies.\nIn each\nphase, the model fully utilizes its existing capa-\nbilities to strengthen its domain expertise. Each\nsubsequent phase builds upon the advancements\nmade in the previous one, employing different tech-\nniques to further evolve the model. The following\nsections will detail the specific methods employed\nin each phase of METEOR, illustrating how these\nstrategies collectively contribute to the model’s en-\nhanced performance and domain-specific knowl-\nedge.\n2.1\nWeak-to-strong Domain Data Distillation\nFor a LLM lacking domain-specific capabilities,\nknowledge distillation is proven to be an effective\nmethod for injecting domain knowledge (Huang\net al., 2023; Zhang et al., 2023; Zhang and Yang,\n2023). However, our experiments indicate that di-\nrectly distilling domain data from a strong gen-\neral model to train a domain-specific model is not\nparticularly effective. As illustrated in Table 7 of\nAppendix A, there is a discrepancy between the re-\nsponses of GPT-4 to domain-specific questions and\nthe solutions provided by the weak model. This\ndiscrepancy arises from differences in their cog-\nnitive distributions, making it challenging to train\nand learn using directly distilled data.\nTo address this issue, we propose a domain data\ndistillation method based on a weak-to-strong strat-\negy (Burns et al., 2024). As illustrated in Figure 2,\nto align the distribution discrepancies between the\nstrong and weak models, we guide the strong model\nto distill domain data according to the instructions\nfrom the weak model. Specifically, when presented\nwith a domain-specific question, we first input the\nquestion into the weak model. Instead of generat-\ning an answer, the weak model produces a guide-\nline based on the prompt shown in Figure 2. This\nguideline outlines the steps the weak model be-\nlieves should be followed to address the question.\nSubsequently, we input the generated guideline\nalong with the domain question into the strong\nmodel. The strong model then generates the answer\nto the domain question by following the guideline’s\nsteps or refines the answer based on the guideline.\nThrough these steps, we obtain a set of data\npairs consisting of domain questions and their cor-\nresponding answers distilled from the strong model.\nThis distilled data is used to fine-tune the weak\nmodel, enabling it to acquire preliminary domain-\nspecific capabilities, thus completing the first stage\nof the Meteor model’s evolution. In Section 4, we\ndemonstrate that the data obtained using the weak-\nto-strong distillation strategy is superior to that\nobtained through direct data distillation, thereby\nvalidating the effectiveness of this approach.\n2.2\nModel Evolution\nTraining models using domain data obtained\nthrough knowledge distillation can impart prelim-\ninary domain-specific capabilities to the LLMs.\nHowever, this approach falls short of elevating the\nmodels to the level of true domain experts due\nto the absence of expert-level reasoning abilities.\nTherefore, further model evolution methods are re-\nquired to enhance the domain capabilities of these\nmodels.\nThe reflection mechanism, which involves ana-\nlyzing the model’s previous reasoning processes\nand answers to identify errors and provide feed-\nback, has been shown to significantly improve\nthe model’s reasoning abilities and performance\nin downstream applications (Shinn et al., 2023;\nMadaan et al., 2023). Consequently, we have de-\nsigned an algorithm for further evolving the model\nwithin specific domains, based on the reflection\nmechanism of LLMs, as illustrated in Figure 3.\n2.2.1\nData Refinement and Iterative Training\nfor Domain-Specific Models\nThe key aspect of the reflection mechanism is to\nprovide accurate feedback on the model’s output.\nHowever, models that have been developed through\nknowledge distillation possess only preliminary do-\nmain capabilities and are unable to deliver suffi-\nciently accurate feedback. Therefore, at this stage,\nwe propose using strong models, such as GPT-\n4 (OpenAI, 2023), to provide feedback for the\nevolution of domain-specific models. The overall\nalgorithm is shown in Algorithm 1.\nThe specific method involves inputting a batch\nof domain data, where the current domain model\n3\n\nDomain Question:\nStrategies for recognizing proper nouns in NLP\nDescription:\nCould anyone explain or link to resources that \nexplain the current dictionary-based methods? \nWho are the authoritative experts on NLP or \nwhat are the definitive resources on the subject?\nDomain Model \n1. Scope: Focus on strategies for recognizing proper nouns ...\n2. Definition of proper nouns: Clearly define what  …\n…\n6. Authoritative experts and resources: Provide a list  …\nGPT-4\nDefinition of Proper Nouns Proper nouns refer \nto specific names of people, organizations, \nlocations, dates, times, and other unique …\nDictionary-Based Methods: These are \nprecompiled lists of proper nouns derived from \nlinguistic resources like WordNet or lexical …\nAlternative Methods: Neural networks, \nincluding transformers like BERT, can learn to \nrecognize proper nouns by identifying …\nAuthoritative Experts and Resources: \nChristopher D. Manning: Known for his work in \nNLP and NER. Dan Jurafsky: Renowned for …\nDomain-Specific Knowledge: Recognizing \nproper nouns in specialized contexts requires \ndomain-specific knowledge, which may not …\nSpeech and Language Processing: by Daniel \nJurafsky and James H. Martin: A comprehensive \ntextbook on NLP. Research papers from …\nAnswer\nGuidelines\nQuestion \nGuidelines \nDistillation \nFigure 2: Illustration of the weak-to-strong knowledge\ndistillation process. Initially, a domain question is input\ninto the domain model to obtain a guideline. The strong\nmodel then uses this guideline, provided by the weak\nmodel, along with the original question, to generate and\ndistill domain-specific data.\ngenerates answers and their rationales for the given\nquestions using the Chain-of-Thought (CoT) ap-\nproach (Wei et al., 2022). At this stage, we com-\nbine the question, rationale, and answer, which are\nthen input into GPT-4. GPT-4 evaluates whether\nthe rationale and answer are correct. Within the\nconstraints of the context, if GPT-4 deems the an-\nswer correct, it returns a correct identifier. If GPT-4\nidentifies the answer as incorrect, it provides sug-\ngestions for modifying the rationale and offers a\nguiding sentence to help the model produce the\ncorrect rationale. We re-input the domain question,\noriginal rationale and answer, along with GPT-4’s\nsuggestions and guiding sentence, into the model,\nwhich then generates a new answer. This process\ncontinues until GPT-4 considers the answer cor-\nrect or the maximum number of iterations, N, is\nreached.\nOnce the iteration stops, we combine the domain\nquestions, multiple rounds of rationales, GPT-4’s\ncorrections, and guiding sentences from the batch\nto form the final domain training data. During\ndata construction, we convert the guiding sentences\nprovided by GPT-4 into discourse markers (like\nHmm). This approach serves two main purposes:\n1. To ensure that the constructed data reflects a\nstrategy where the model independently derives the\ncorrected answer, rather than relying on externally\nprovided guiding sentences for correction. 2. To\nmaintain the coherence of the reasoning statements\nthroughout the process.\nAfter obtaining the domain data, we employ\nan iterative training approach to train the model.\nSpecifically, when the amount of data in the dataset\nreaches a multiple of K, we fine-tune the model\nAlgorithm 1 Data Refinement and Iterative Train-\ning\nRequire: Question set Q, Model M, GPT-4, Max-\nimum iterations N, Batch size K\nEnsure: Updated Model M\n1: training_buffer ←{}\n2: for Q in Q do\n3:\nhistory ←{}\n4:\nwhile n < N do\n5:\nif n = 0 then\n6:\nRn, An ←M(Q) ▷Initial attempt\n7:\nelse\n8:\nRn, An ←M(Q, guiden)\n▷\nGuided attempt\n9:\nend if\n10:\n(status, explanation, guide)\n←\nGPT-4(Q, Rn, An)\n11:\nhistory.append(Rn, explanation)\n12:\nif status = \"CORRECT\" then\n13:\ntraining_buffer.append(Q, history)\n14:\nif |training_buffer| = K then\n15:\nM\n←\nUpdateModel(M, training_buffer)\n▷\nFine-tune model\n16:\ntraining_buffer ←{}\n▷Clear\nbuffer\n17:\nend if\n18:\nbreak\n19:\nend if\n20:\nn ←n + 1\n21:\nend while\n22: end forreturn M\nusing the current dataset. The fine-tuned model\nis then reintroduced into the iterative process de-\nscribed above. Throughout the iterative training\nprocess, the domain model’s capability in the spe-\ncific domain gradually improves, and the quality of\nthe generated domain data continuously enhances.\n2.2.2\nSelf-Evolution of Domain Capabilities\nthrough Inference Strategy\nOptimization\nAdditionally, we aim for the model to evolve its\ndomain capabilities independently, without relying\non strong models. To achieve this, we propose a\nmethod for the model to autonomously enhance its\ndomain capabilities.\nIncreasing FLOPs during inference has been\nshown to effectively enhance model performance\nin downstream tasks (Snell et al., 2024). Inspired\n4\n\nInterative Training\nDomain \nModel\nGPT4\nRight Answer\nWrong Answer\nDomain Question\n# Title\nHow to Fine-Tune BERT for NER Tasks?\n# Body\nI'm trying to fine-tune a pre-trained BERT \nmodel for a Named Entity Recognition (NER) \ntask. What are the essential steps and \nrecommended tools for this process? Any \ntips on dataset preparation and evaluation \nmetrics would also be appreciated.\nCurrent\nAnswer\nQuestion + CoT + Answer\nDataset\nContinuous Fine-tuning\nAdvice\nRegenerate\n…\nFigure 3: Illustration of he iterative evolution process\nguided by a strong model. Upon receiving domain-\nspecific data, the model employs CoT reasoning to gen-\nerate answers and reasoning paths. These are evaluated\nby GPT-4, which provides confirmation if correct or\noffers suggestions for refinement if incorrect. This iter-\native process continues until the answer is validated or\nthe maximum iteration limit is reached.\nCategory\nML\nDL\nNLP\nCV\nTotal\nScale\n4605\n2092\n1881\n1698\n10276\nTable 1: Total volume of data scraped from Stack Over-\nflow. The categories ML, DL, NLP, and CV represent\nMachine Learning, Deep Learning, Natural Language\nProcessing, and Computer Vision, respectively.\nby this concept, we hypothesize that different in-\nference strategies yield varying outcomes.\nWe\naim for the model’s outputs to closely align with\nthose generated by high-FLOPs strategies, while di-\nverging from those produced by low-FLOPs strate-\ngies. Therefore, we employ beam search (Freitag\nand Al-Onaizan, 2017) as the high-FLOPs strategy\nand greedy search as the low-FLOPs strategy, us-\ning contrastive learning to develop a self-training\nmethod for the model. Experiments in Section 4\ndemonstrate the effectiveness of this approach.\n3\nExperimental Setups\nDatasets.\nWe use the field of advanced computer\neducation as the specific domain to validate the\neffectiveness of the proposed Meteor method. To\nobtain high-quality domain data, we scraped data\nfrom Stack Overflow across four categories: Ma-\nchine Learning (ML), Deep Learning (DL), Natural\nLanguage Processing (NLP), and Computer Vision\n(CV), totaling 10,276 entries. The data distribution\nacross these categories is shown in Table 1. From\nthis dataset, we randomly selected 1000 entries as\ntest data, with the remaining data used for training.\nMetrics.\nFollowing the evaluated method pro-\nposed by Zheng et al. (2023), we use GPT-4 as a\njudge to evaluate both data quality and model per-\nformance. When comparing the distilled data qual-\nity with and without the use of guidelines, GPT-4 is\nused to score the data, where higher scores indicate\nbetter quality. In evaluating the domain-specific\nanswers generated by the model, GPT-4 provides\nscores based on five criteria: accuracy, complete-\nness, relevance, coherence, and reliability, allow-\ning for a comprehensive assessment of the model’s\ndomain capabilities. The prompt for GPT-4’s pair-\nwise comparison across five evaluation criteria is\npresented in Table 8 of Appendix B.\nModel and parameters setting.\nWe selected\nLLaMA3-8B-Chat (Dubey et al., 2024) and\nQwen2-7B-Instruct (Yang et al., 2024a) as the base-\nline models for domain model training in this study.\nFor GPT-4, we utilized the publicly available API\nmodel, GPT-4o. During the iterative evolution of\nthe model, we set the maximum number of itera-\ntions to N = 3, with a cumulative data amount\nof K = 10 during each iteration. The beam size\nfor beam search was set to 10, and the maximum\ntoken length for the domain evolution model was\nconfigured to be 4096.\n4\nExperimental Results\n4.1\nOverall Results\nWe compared the performance changes across vari-\nous dimensions before and after applying the Me-\nteor method for domain capability evolution of\nLLMs. For the accuracy, completeness, relevance,\ncoherence, and reliability, we generated responses\nto test set questions using both the Meteor-trained\nLLMs and the non-Meteor-trained LLMs. GPT-\n4 was then used to determine which model’s re-\nsponses performed better in each of these dimen-\nsions. For the GPT-4 Score, GPT-4 directly eval-\nuated the answers generated by the LLMs before\nand after evolution, and the average score was cal-\nculated.\nAs shown in Table 2, after the Meteor evolu-\ntion, LLaMA3-8B-Chat and Qwen2-7B-Instruct\nachieved improvements of 78.66%,\n72.13%,\n74.34%, 80.10%, 82.13%, and 68.36%, 63.43%,\n60.24%, 63.43%, 67.09% respectively in accuracy,\ncompleteness, relevance, coherence, and reliability.\nAdditionally, the GPT-4 Score after evolution was\nsignificantly higher than before, demonstrating the\neffectiveness of the Meteor method.\n5\n\nModel\nAccuracy\nCompleteness\nRelevance\nCoherence\nReliability\nGPT4 Score\nLLaMA3\nw/o.\n21.3%\n27.8%\n25.6%\n19.9%\n17.8%\n5.02\nMeteor\n78.7%\n72.2%\n74.4%\n80.1%\n82.2%\n9.17\nQwen2\nw/o.\n31.6%\n36.5%\n39.7%\n36.5%\n32.9%\n6.88\nMeteor\n68.4%\n63.5%\n60.3%\n63.5%\nc%\n9.28\nTable 2: The overall results comparing the quality of domain question responses by LLaMA3 and Qwen2, both with\nand without Meteor training. Here, LLaMA refers to LLaMA3-8B-Chat, Qwen2 refers to Qwen2-7B-Instruct, and\nw/o. denotes models that have not undergone Meteor training.\n4.2\nEffectiveness of Weak-to-Strong Data\nDistillation\nIn Section 2.1, we introduced a domain knowledge\ndistillation method based on a weak-to-strong strat-\negy. To evaluate the effectiveness of this approach,\nwe used GPT-4 to score the distilled data. By com-\nparing the GPT-4 Scores of data distilled with and\nwithout guidelines, we demonstrate the effective-\nness of the weak-to-strong strategy in enhancing\nthe quality of distilled data. The specific results are\nshown in Table 3.\nAs shown in Table 3, across various domains,\nthe data distilled with guidelines achieved signifi-\ncantly higher GPT-4 scores than the data distilled\nwithout guidelines. Specifically, the GPT-4 scores\nincreased by 3.29, 3.27, 3.34, and 3.32 points in\nML, DL, NLP, and CV, respectively, indicating that\nthe quality of domain data distilled with guidelines\nis superior.\nCategory\nGPT-4 Score\nwith guidelines\nw/o guidelines\nML\n9.09\n5.80\nDL\n9.07\n5.80\nNLP\n9.05\n5.71\nCV\n9.05\n5.73\nAvg.\n9.07\n5.77\nTable 3: This table illustrating the quality scores as-\nsigned by GPT-4 to data distilled with and without\nguidelines. The results indicate that data distilled using\nguidelines consistently receive higher scores across all\ndomains.\nAdditionally, we assessed whether data distilled\nwith guidelines can more effectively enhance the\nmodel’s domain capabilities. We fine-tuned the\nmodel using two sets of data, one with guidelines\nand one without, and evaluated the GPT-4 Scores\nof the answers generated by the models on the test\nset. As shown in Table 3, models fine-tuned with\ndata distilled using guidelines scored higher across\nall aspects compared to those fine-tuned with data\ndistilled without guidelines, further confirming the\neffectiveness of the weak-to-strong data distillation\nstrategy.\nModel\nGPT-4 Score\nwith guidelines\nw/o guidelines\nLLaMA3\n9.04\n5.38\nQwen2\n9.22\n6.13\nTable 4: This table presenting GPT-4 scores for answers\ngenerated by models fine-tuned with distilled data. The\nresults demonstrate that models fine-tuned with data\ndistilled using guidelines exhibit superior performance\non domain-specific tasks.\n4.3\nEffectiveness of Iterative Training and\nData Refinement\nIn Section 2.2.1, we introduced the Data Refine-\nment and Iterative Training for Domain-Specific\nModels. o validate the effectiveness of this ap-\nproach, we employed GPT-4 to evaluate the ac-\ncuracy, completeness, relevance, coherence, and\nreliability of the answers generated by the models\nbefore and after evolution. The results are pre-\nsented in Table 5.\nLLaMA3\nQwen2\nw.\nw/o.\nw.\nw/o.\nAcc.\n72.7%\n27.2%\n54.9%\n45.1%\nCom.\n63.1%\n36.9%\n53.7%\n46.3%\nRele.\n65.4%\n34.6%\n51.3%\n48.6%\nCoh.\n72.3%\n22.7%\n57.6%\n42.4%\nReli.\n62.9%\n37.1%\n60.8%\n39.2%\nTable 5: The effectiveness of Data Refinement and It-\nerative Training for Domain-Specific Models. Acc.,\nCom., Rele., Coh., and Reli. represent Accuracy, Com-\npleteness, Relevance, Coherence, and Reliability, re-\nspectively.\nLLaMA3 represents LLaMA3-8B-Chat\nModel. Qwen2 represents Qwen2-7B-Instruct Model.\n’w.’ denotes models that have undergone iterative self-\nevolution training, while ’w/o.’ denotes models that\nhave not.\n6\n\nThe results in Table 5 indicate that both\nLLaMA3-8B-Chat (Dubey et al., 2024) and\nQwen2-7B-Instruct (Yang et al., 2024a) showed\nimproved performance across all metrics after evo-\nlution guided by GPT-4, demonstrating the effec-\ntiveness and generalizability of this method.\n4.4\nEffectiveness of Self-Evolution Method\nIn Section 2.2.2, we proposed that model evolution\nshould be independent of strong model constraints\nand introduced a self-evolution method based on\nvarying FLOPs. Table 6 presents the differences\nin performance across various dimensions, as eval-\nuated by GPT-4, before and after the models self-\nevolution.\nLLaMa3\nQWen2\nw.\nw/o.\nw.\nw/o.\nAcc.\n53.8%\n46.2%\n54.7%\n45.3%\nCom.\n47.9%\n52.1%\n51.3%\n48.7%\nRele.\n51.7%\n48.3%\n50.8%\n49.2%\nCoh.\n53.3%\n46.7%\n52.8%\n47.2%\nReli.\n52.9%\n47.1%\n53.1%\n46.9%\nTable 6: The effiectiveness of Self-evolution method.\nAcc., Com., Rele., Coh., and Reli. represent Accuracy,\nCompleteness, Relevance, Coherence, and Reliability,\nrespectively. LLaMA3 represents LLaMA3-8B-Chat\nModel. Qwen2 represents Qwen2-7B-Instruct Model.\n’w.’ denotes models that have self-evolution training,\nwhile ’w/o.’ denotes models that have not.\nThe results in Table 6 indicate that both\nLLaMA3-8B-Chat (Dubey et al., 2024) and\nQwen2-7B-Instruct (Yang et al., 2024a) showed\nimproved performance across all metrics following\nself-evolution, demonstrating the effectiveness and\ngeneralizability of this method.\n5\nRelated Works\n5.1\nDomain-specific Knowledge Distillation\nIn the context of LLMs, knowledge distillation has\nbecome a crucial method for enhancing domain-\nspecific knowledge. In specialized fields such as\nlaw and medicine, the complexity of terminology\noften poses challenges (Lai et al., 2023; Sun et al.,\n2023). As a result, the distillation process typi-\ncally begins with pre-training on domain-specific\ncorpora, followed by fine-tuning using LLMs to\nconstruct enhanced data (Huang et al., 2023; Zhou\net al., 2024). In the financial fields, distilled data\nmust maintain diversity and complexity to ensure\nthat LLMs can handle varied and intricate scenar-\nios (Zhang and Yang, 2023; Wang et al., 2023a). In\nscientific fields, the distillation of knowledge from\nlong texts and reasoning data is essential (Abdine\net al., 2024; Zhang et al., 2024; Lin et al., 2024).\nIn the education fields, researchers often empha-\nsize knowledge distillation through pre-training on\neducational corpora and fine-tuning with custom\ninstructions to activate education-specific functions\nsuch as open-ended questioning, essay evaluation,\nand emotional support (Dan et al., 2023; Guo et al.,\n2024). However, few studies focus on whether\nthe inputs distilled from strong models can be ef-\nficiently utilized. This paper achieves more effi-\ncient domain knowledge distillation by aligning the\nknowledge distribution between strong and weak\nmodels.\n5.2\nSelf-evolution of LLMs\nSelf-evolution of LLMs refers to the capability of\nLLMs to autonomously acquire, refine, and learn\nfrom their own generated experiences (Tao et al.,\n2024). Initially, model evolution requires expe-\nrience acquisition. Self-Align (Li et al., 2024b)\ngenerates experiences guided by knowledge from\nover 20 domains to ensure diversity in evolution.\nIn contrast, Self-Instruct (Wang et al., 2023b) pro-\nposes a self-evolution method without the need\nfor external knowledge guidance. Subsequently,\nexperience refinement is necessary. Self-Talk (Ul-\nmer et al., 2024) ensures data quality by measuring\nthe number of completed sub-goals, while Self-\nverification (Weng et al., 2023) selects data by\nevaluating the consistency between predicted val-\nues and original conditions. Once high-quality\nexperiences are acquired, models need to be up-\ndated to enhance performance. ReST (Aksitov\net al., 2023) updates LLMs iteratively by blending\noriginal training data with newly generated data.\nFuseLLM (Wan et al., 2024) transfers knowledge\nto the target LLM using probability distributions\ngenerated by the source LLM. Finally, the evolved\nmodels require evaluation. ChatEval (Chan et al.,\n2024) explores the strengths and weaknesses of\nmodel outputs through a debate mechanism. LLM-\nas-a-judge (Zheng et al., 2023) employs large mod-\nels to evaluate other large models, demonstrating\nthat LLMs can match human judgment and achieve\nefficient performance evaluation.\n7\n\n6\nConclusion\nIn this study, we explore innovative strategies for\nenhancing domain-specific capabilities of LLMs\nthrough knowledge distillation and self-evolution.\nWe propose the Meteor method, which leverages\nweak-to-strong data distillation, iterative training,\nand self-evolution to improve model performance\nacross various metrics. Our approach is validated\nusing data from advanced computer education, with\nexperiments demonstrating significant improve-\nments in accuracy, completeness, relevance, co-\nherence, and reliability. By aligning the knowledge\ndistribution between strong and weak models, we\nachieve more efficient domain knowledge distilla-\ntion. Furthermore, our self-evolution method en-\nables models to autonomously refine their capabili-\nties without relying on external models, ensuring\nadaptability and robustness.\n7\nLimitations and Future Works\n7.1\nLimitations\nDespite the significant advances achieved in\ndomain-specific model evolution, several limita-\ntions warrant further investigation and improve-\nment in future research.\nThe primary limitation lies in our approach to\nvalidating the distributional discrepancy between\nstrong and weak models.\nAlthough we have\nsuccessfully demonstrated performance improve-\nments through weak-to-strong knowledge distilla-\ntion, thereby indirectly supporting our hypothesis\nof domain-specific distributional differences, we\nlack direct experimental evidence to quantify and\nvisualize these differences. This indirect valida-\ntion methodology may not fully capture the spe-\ncific characteristics and extent of the distributional\ndisparities, thus limiting our comprehensive under-\nstanding of model behaviors.\nFurthermore,\nthe\nproposed\nself-evolution\nmethodology demonstrates room for improvement\nin practical applications.\nWhile designed to\nreduce dependence on strong models, current\nexperimental results indicate limited performance\ngains. This limitation suggests the necessity for\nfurther optimization of our self-evolution strategies\nto achieve more substantial improvements.\n7.2\nFuture Works\nBuilding upon these limitations, we propose several\ncrucial directions for future research:\nFirstly, we plan to conduct more rigorous anal-\nysis through neural probing techniques to directly\nquantify the distributional differences between\nstrong and weak models. Specifically, we will track\nand compare the internal neural activation patterns\nand token distribution characteristics of both model\ntypes when processing domain-specific tasks. This\napproach will provide more direct experimental\nevidence, facilitating better understanding and vali-\ndation of our distributional difference hypothesis.\nSecondly, we are committed to developing more\nefficient self-evolution methodologies. Current re-\nsearch suggests the possibility of surpassing the\nperformance ceiling of strong models, motivating\nus to explore more advanced self-evolution strate-\ngies. Our objective is to design methods capa-\nble of continuous performance enhancement, ul-\ntimately achieving autonomous evolution that sur-\npasses strong model guidance.\nFinally, considering the generic nature of the Me-\nteor approach, we intend to expand its application\nscope. This includes validating its effectiveness\nacross a more diverse range of domain-specific\nscenarios and exploring compatibility with vari-\nous foundation models. Through this extensibility\nstudy, we aim to further validate the universality\nof the Meteor approach and explore its potential in\nbroader application contexts.\nReferences\nHadi\nAbdine,\nMichail\nChatzianastasis,\nCostas\nBouyioukos,\nand Michalis Vazirgiannis. 2024.\nProt2text: Multimodal protein’s function generation\nwith gnns and transformers. In Thirty-Eighth AAAI\nConference on Artificial Intelligence, AAAI 2024,\nThirty-Sixth Conference on Innovative Applications\nof Artificial Intelligence, IAAI 2024, Fourteenth\nSymposium on Educational Advances in Artificial\nIntelligence, EAAI 2014, February 20-27, 2024,\nVancouver, Canada, pages 10757–10765. AAAI\nPress.\nRenat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang\nLi, Sheila Babayan, Kavya Kopparapu, Zachary\nFisher, Ruiqi Guo, Sushant Prakash, Pranesh Srini-\nvasan, Manzil Zaheer, Felix X. Yu, and Sanjiv Kumar.\n2023. Rest meets react: Self-improvement for multi-\nstep reasoning LLM agent. CoRR, abs/2312.10003.\nCollin Burns, Pavel Izmailov, Jan Hendrik Kirchner,\nBowen Baker, Leo Gao, Leopold Aschenbrenner,\nYining Chen, Adrien Ecoffet, Manas Joglekar, Jan\nLeike, Ilya Sutskever, and Jeffrey Wu. 2024. Weak-\nto-strong generalization: Eliciting strong capabilities\nwith weak supervision. In Forty-first International\n8\n\nConference on Machine Learning, ICML 2024, Vi-\nenna, Austria, July 21-27, 2024. OpenReview.net.\nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu,\nWei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan\nLiu. 2024. Chateval: Towards better llm-based eval-\nuators through multi-agent debate. In The Twelfth\nInternational Conference on Learning Representa-\ntions, ICLR 2024, Vienna, Austria, May 7-11, 2024.\nOpenReview.net.\nYuhao Dan, Zhikai Lei, Yiyang Gu, Yong Li, Jianghao\nYin, Jiaju Lin, Linhao Ye, Zhiyan Tie, Yougen Zhou,\nYilei Wang, Aimin Zhou, Ze Zhou, Qin Chen, Jie\nZhou, Liang He, and Xipeng Qiu. 2023. Educhat: A\nlarge-scale language model-based chatbot system for\nintelligent education. CoRR, abs/2308.02773.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurélien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Rozière, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Grégoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Han-\nnah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. 2024. The llama 3 herd of models. CoRR,\nabs/2407.21783.\nMarkus Freitag and Yaser Al-Onaizan. 2017. Beam\nsearch strategies for neural machine translation. In\nProceedings of the First Workshop on Neural Ma-\nchine Translation, NMT@ACL 2017, Vancouver,\nCanada, August 4, 2017, pages 56–60. Association\nfor Computational Linguistics.\nHongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang,\nLinzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong\nHu, Chao Chen, Dongfeng Zhang, Xu Shi, Tieqiao\nZheng, Liangfan Zheng, Bo Zhang, Ke Xu, and Zhou-\njun Li. 2024. OWL: A large language model for IT\noperations. In The Twelfth International Conference\non Learning Representations, ICLR 2024, Vienna,\nAustria, May 7-11, 2024. OpenReview.net.\nQuzhe Huang, Mingxu Tao, Zhenwei An, Chen Zhang,\nCong Jiang, Zhibin Chen, Zirui Wu, and Yansong\nFeng. 2023. Lawyer llama technical report. CoRR,\nabs/2305.15062.\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2023.\nAtlas: Few-shot learning\nwith retrieval augmented language models. J. Mach.\nLearn. Res., 24:251:1–251:43.\nQiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu.\n2024. Genegpt: augmenting large language models\nwith domain tools for improved access to biomedical\ninformation. Bioinform., 40(2).\nJinqi Lai, Wensheng Gan, Jiayang Wu, Zhenlian Qi, and\nPhilip S. Yu. 2023. Large language models in law: A\nsurvey. CoRR, abs/2312.03718.\nJiawei Li, Yizhe Yang, Yu Bai, Xiaofeng Zhou, Yinghao\nLi, Huashan Sun, Yuhang Liu, Xingpeng Si, Yuhao\nYe, Yixiao Wu, Yiguan Lin, Bin Xu, Ren Bowen,\nChong Feng, Yang Gao, and Heyan Huang. 2024a.\nFundamental capabilities of large language models\nand their applications in domain scenarios: A survey.\nIn Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2024, Bangkok, Thailand, August\n11-16, 2024, pages 11116–11141. Association for\nComputational Linguistics.\nJinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li,\nBowen Li, Bailin Wang, Bowen Qin, Ruiying Geng,\nNan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang\nLi, Kevin Chen-Chuan Chang, Fei Huang, Reynold\nCheng, and Yongbin Li. 2023. Can LLM already\nserve as A database interface? A big bench for large-\nscale database grounded text-to-sqls. In Advances in\nNeural Information Processing Systems 36: Annual\nConference on Neural Information Processing Sys-\ntems 2023, NeurIPS 2023, New Orleans, LA, USA,\nDecember 10 - 16, 2023.\nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer\nLevy, Luke Zettlemoyer, Jason Weston, and Mike\nLewis. 2024b. Self-alignment with instruction back-\ntranslation. In The Twelfth International Conference\non Learning Representations, ICLR 2024, Vienna,\nAustria, May 7-11, 2024. OpenReview.net.\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,\nYan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,\nShaoguang Mao, Yun Wang, Linjun Shou, Ming\nGong, and Nan Duan. 2023. Taskmatrix.ai: Com-\npleting tasks by connecting foundation models with\nmillions of apis. CoRR, abs/2303.16434.\nZhouhan Lin, Cheng Deng, Le Zhou, Tianhang Zhang,\nYi Xu, Yutong Xu, Zhongmou He, Yuanyuan Shi,\nBeiya Dai, Yunchong Song, Boyi Zeng, Qiyuan Chen,\n9\n\nTao Shi, Tianyu Huang, Yiwei Xu, Shu Wang, Lu-\noyi Fu, Weinan Zhang, Junxian He, Chao Ma, Yun-\nqiang Zhu, Xinbing Wang, and Chenghu Zhou. 2024.\nGeogalactica: A scientific large language model in\ngeoscience. CoRR, abs/2401.00434.\nChen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng,\nCan Zheng, Junxiang Wang, Tanmoy Chowdhury,\nYun Li, Hejie Cui, Xuchao Zhang, Tianjiao Zhao,\nAmit Panalkar, Dhagash Mehta, Stefano Pasquali,\nWei Cheng, Haoyu Wang, Yanchi Liu, Zhengzhang\nChen, Haifeng Chen, Chris White, Quanquan Gu,\nJian Pei, Carl Yang, and Liang Zhao. 2024. Domain\nspecialization as the key to make large language mod-\nels disruptive: A comprehensive survey. Preprint,\narXiv:2305.18703.\nJiaying Lu, Jiaming Shen, Bo Xiong, Wenjing Ma, Stef-\nfen Staab, and Carl Yang. 2023. Hiprompt: Few-shot\nbiomedical knowledge fusion via hierarchy-oriented\nprompting. In Proceedings of the 46th International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval, SIGIR 2023, Taipei,\nTaiwan, July 23-27, 2023, pages 2052–2056. ACM.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nShashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdan-\nbakhsh, and Peter Clark. 2023. Self-refine: Itera-\ntive refinement with self-feedback. In Advances in\nNeural Information Processing Systems 36: Annual\nConference on Neural Information Processing Sys-\ntems 2023, NeurIPS 2023, New Orleans, LA, USA,\nDecember 10 - 16, 2023.\nOpenAI. 2023.\nGPT-4 technical report.\nCoRR,\nabs/2303.08774.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan\nFirat, Julian Schrittwieser, Ioannis Antonoglou, Ro-\nhan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie\nMillican, Ethan Dyer, Mia Glaese, Thibault Sotti-\naux, Benjamin Lee, Fabio Viola, Malcolm Reynolds,\nYuanzhong Xu, James Molloy, Jilin Chen, Michael\nIsard, Paul Barham, Tom Hennigan, Ross McIl-\nroy, Melvin Johnson, Johan Schalkwyk, Eli Collins,\nEliza Rutherford, Erica Moreira, Kareem Ayoub,\nMegha Goel, Clemens Meyer, Gregory Thornton,\nZhen Yang, Henryk Michalewski, Zaheer Abbas,\nNathan Schucher, Ankesh Anand, Richard Ives,\nJames Keeling, Karel Lenc, Salem Haykal, Siamak\nShakeri, Pranav Shyam, Aakanksha Chowdhery, Ro-\nman Ring, Stephen Spencer, Eren Sezener, and et al.\n2024. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context. CoRR,\nabs/2403.05530.\nDale Schuurmans. 2023. Memory augmented large lan-\nguage models are computationally universal. CoRR,\nabs/2301.04589.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: language agents with verbal reinforcement\nlearning. In Advances in Neural Information Pro-\ncessing Systems 36: Annual Conference on Neural\nInformation Processing Systems 2023, NeurIPS 2023,\nNew Orleans, LA, USA, December 10 - 16, 2023.\nAvi Singh, John D. Co-Reyes, Rishabh Agarwal,\nAnkesh Anand, Piyush Patil, Xavier Garcia, Pe-\nter J. Liu, James Harrison, Jaehoon Lee, Kelvin\nXu, Aaron T. Parisi, Abhishek Kumar, Alexander A.\nAlemi, Alex Rizkowsky, Azade Nova, Ben Adlam,\nBernd Bohnet, Gamaleldin Fathy Elsayed, Hanie\nSedghi, Igor Mordatch, Isabelle Simpson, Izzeddin\nGur, Jasper Snoek, Jeffrey Pennington, Jiri Hron,\nKathleen Kenealy, Kevin Swersky, Kshiteej Maha-\njan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi,\nNoah Constant, Roman Novak, Rosanne Liu, Tris\nWarkentin, Yundi Qian, Yamini Bansal, Ethan Dyer,\nBehnam Neyshabur, Jascha Sohl-Dickstein, and\nNoah Fiedel. 2024. Beyond human data: Scaling\nself-training for problem-solving with language mod-\nels. Trans. Mach. Learn. Res., 2024.\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Ku-\nmar. 2024. Scaling LLM test-time compute optimally\ncan be more effective than scaling model parameters.\nCoRR, abs/2408.03314.\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang\nWang, Pengjie Ren, Zhumin Chen, Dawei Yin, and\nZhaochun Ren. 2023. Is chatgpt good at search?\ninvestigating large language models as re-ranking\nagents. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Process-\ning, EMNLP 2023, Singapore, December 6-10, 2023,\npages 14918–14937. Association for Computational\nLinguistics.\nZhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu\nLi, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang,\nDacheng Tao, and Jingren Zhou. 2024. A survey\non self-evolution of large language models. CoRR,\nabs/2404.14387.\nDennis Ulmer, Elman Mansimov, Kaixiang Lin, Lijia\nSun, Xibin Gao, and Yi Zhang. 2024. Bootstrapping\nllm-based task-oriented dialogue agents via self-talk.\nIn Findings of the Association for Computational\nLinguistics, ACL 2024, Bangkok, Thailand and vir-\ntual meeting, August 11-16, 2024, pages 9500–9522.\nAssociation for Computational Linguistics.\nFanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan,\nWei Bi, and Shuming Shi. 2024.\nKnowledge fu-\nsion of large language models. In The Twelfth In-\nternational Conference on Learning Representations,\nICLR 2024, Vienna, Austria, May 7-11, 2024. Open-\nReview.net.\nNeng Wang, Hongyang Yang, and Christina Dan\nWang. 2023a. Fingpt: Instruction tuning benchmark\nfor open-source large language models in financial\ndatasets. CoRR, abs/2310.04793.\n10\n\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023b. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), ACL 2023, Toronto, Canada, July 9-14, 2023,\npages 13484–13508. Association for Computational\nLinguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\nand Denny Zhou. 2022. Chain-of-thought prompting\nelicits reasoning in large language models. In Ad-\nvances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Process-\ning Systems 2022, NeurIPS 2022, New Orleans, LA,\nUSA, November 28 - December 9, 2022.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He,\nShengping Liu, Bin Sun, Kang Liu, and Jun Zhao.\n2023. Large language models are better reasoners\nwith self-verification. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2023,\nSingapore, December 6-10, 2023, pages 2550–2575.\nAssociation for Computational Linguistics.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravol-\nski, Mark Dredze, Sebastian Gehrmann, Prabhanjan\nKambadur, David S. Rosenberg, and Gideon Mann.\n2023. Bloomberggpt: A large language model for\nfinance. CoRR, abs/2303.17564.\nZhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang\nHong, Honglin Guo, Junzhe Wang, Dingwen Yang,\nChenyang Liao, Xin Guo, Wei He, Songyang Gao,\nLu Chen, Rui Zheng, Yicheng Zou, Tao Gui,\nQi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan\nWu, and Yu-Gang Jiang. 2024. Agentgym: Evolv-\ning large language model-based agents across diverse\nenvironments. CoRR, abs/2406.04151.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Guanting Dong, Hao-\nran Wei, Huan Lin, Jialong Tang, Jialin Wang,\nJian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nMa, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai,\nJinzheng He, Junyang Lin, Kai Dang, Keming Lu, Ke-\nqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni,\nPei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize\nGao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan,\nTianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge,\nXiaodong Deng, Xiaohuan Zhou, Xingzhang Ren,\nXinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing\nLiu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan,\nYunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang,\nZhifang Guo, and Zhihao Fan. 2024a. Qwen2 techni-\ncal report. CoRR, abs/2407.10671.\nYizhe Yang, Huashan Sun, Jiawei Li, Runheng Liu,\nYinghao Li, Yuhang Liu, Yang Gao, and Heyan\nHuang. 2024b. Mindllm: Lightweight large language\nmodel pre-training, evaluation and domain applica-\ntion. AI Open, 5:1–26.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\nGoodman. 2022. Star: Bootstrapping reasoning with\nreasoning. In Advances in Neural Information Pro-\ncessing Systems 35: Annual Conference on Neural\nInformation Processing Systems 2022, NeurIPS 2022,\nNew Orleans, LA, USA, November 28 - December 9,\n2022.\nDan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao\nDu, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao\nDong, and Jie Tang. 2024. Sciglm: Training scien-\ntific language models with self-reflective instruction\nannotation and tuning. CoRR, abs/2401.07950.\nHongbo Zhang, Junying Chen, Feng Jiang, Fei Yu,\nZhihong Chen, Guiming Chen, Jianquan Li, Xi-\nangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan,\nBenyou Wang, and Haizhou Li. 2023. Huatuogpt,\ntowards taming language model to be a doctor. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2023, Singapore, December 6-10,\n2023, pages 10859–10885. Association for Computa-\ntional Linguistics.\nXuanyu Zhang and Qing Yang. 2023. Xuanyuan 2.0:\nA large chinese financial chat model with hundreds\nof billions parameters. In Proceedings of the 32nd\nACM International Conference on Information and\nKnowledge Management, CIKM 2023, Birmingham,\nUnited Kingdom, October 21-25, 2023, pages 4435–\n4439. ACM.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena. In\nAdvances in Neural Information Processing Systems\n36: Annual Conference on Neural Information Pro-\ncessing Systems 2023, NeurIPS 2023, New Orleans,\nLA, USA, December 10 - 16, 2023.\nZhi Zhou, Jiang-Xin Shi, Peng-Xiao Song, Xiaowen\nYang, Yi-Xuan Jin, Lan-Zhe Guo, and Yu-Feng Li.\n2024. Lawgpt: A chinese legal knowledge-enhanced\nlarge language model. CoRR, abs/2406.04614.\nA\nExample Appendix\nB\nPrompts\n11\n\nDomain Question\nQustion: Strategies for recognizing proper nouns in NLP\nDescribtion: I’m interested in learning more about Natural Language Processing (NLP) and am\ncurious if there are currently any strategies for recognizing proper nouns in a text that aren’t\nbased on dictionary recognition? Also, could anyone explain or link to resources that explain\nthe current dictionary-based methods? Who are the authoritative experts on NLP or what are\nthe definitive resources on the subject?\nGPT-4 Answer\nRecognizing proper nouns in Natural Language Processing (NLP) is a critical task for many\napplications such as information retrieval, machine translation, and named entity recognition.\nHere are some strategies and resources for recognizing proper nouns, both dictionary-based\nand non-dictionary-based:\nNon-Dictionary-Based Methods Machine Learning Approaches:\nSupervised Learning: Techniques like Conditional Random Fields (CRFs) and Hidden Markov\nModels (HMMs) are commonly used. These models learn to recognize patterns in labeled\ntraining data to identify proper nouns. Neural Networks: Deep learning models, such as\nRecurrent Neural Networks (RNNs) and Transformers (e.g., BERT), can be trained on large\ncorpora to recognize proper nouns based on context and syntactic patterns. Rule-Based Systems:\nPattern Matching: Use regular expressions and linguistic rules to identify proper nouns. For\ninstance, capitalized words following certain parts of speech can be flagged as proper nouns.\nHeuristics: Implement heuristics based on punctuation, capitalization, and position in a sentence\nto detect proper nouns. Statistical Methods:\nPart-of-Speech Tagging: Use statistical models to tag words with their parts of speech, including\nproper nouns. These models often rely on large annotated corpora for training. Dictionary-\nBased Methods Lexicon-Based Approaches: These methods rely on predefined dictionaries or\ngazetteers of known proper nouns. The text is scanned to match words against entries in these\ndictionaries. Hybrid Methods: Combine dictionary-based recognition with machine learning to\nimprove accuracy. For example, a system might use a dictionary to recognize common proper\nnouns and a machine learning model to identify less common ones. Resources and Experts\nBooks and Papers:\n\"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin is a comprehensive\ntextbook that covers many aspects of NLP, including named entity recognition. Research papers\non named entity recognition (NER) often discuss methods for recognizing proper nouns. Online\nCourses and Tutorials:\nCoursera and edX offer courses on NLP that cover both traditional and modern techniques.\nOnline tutorials and documentation for NLP libraries like NLTK, SpaCy, and Stanford NLP.\nExperts in the Field:\nChristopher Manning: Known for his work in computational linguistics and NLP. Andrew\nNg: While more broadly focused on AI, he has contributed significantly to machine learning\napplications in NLP. Conferences and Journals:\nConferences like ACL (Association for Computational Linguistics) and EMNLP (Empirical\nMethods in Natural Language Processing) are key venues for the latest research in NLP. Journals\nsuch as \"Computational Linguistics\" and \"Journal of Artificial Intelligence Research\" publish\ncutting-edge NLP research. These strategies and resources should provide a solid foundation\nfor understanding and implementing proper noun recognition in NLP systems.\nGuidelines\n1. **Scope:** Focus on strategies for recognizing proper nouns in natural language text,\nexcluding dictionary-based methods. Discuss the limitations and challenges of dictionary-\nbased approaches and highlight alternative methods that have been proposed or implemented.\n2. **Definition of proper nouns:** Clearly define what constitutes a proper noun, including\nnames of people, organizations, locations, dates, times, and other specific entities. Explain\nthe importance of recognizing proper nouns in NLP applications. 3. **Dictionary-based\nmethods:** Provide an overview of current dictionary-based methods for recognizing proper\nnouns, including: * Word lists: The use of precompiled lists of proper nouns, often based\non linguistic resources like WordNet or other lexical databases. * Gazetteers: Geospatial\ndatabases that contain information about geographic locations, often used for proper noun\nrecognition. * Entity recognition: The use of machine learning models to identify entities in\ntext, including proper nouns. 4. **Alternative methods:** Discuss alternative strategies for\nrecognizing proper nouns that aren’t based on dictionary recognition, including: * Machine\nlearning approaches: Explain how machine learning algorithms, such as neural networks or\ndecision trees, can be trained to recognize proper nouns based on patterns in text data. *\nContextual analysis: Describe how contextual analysis, including sentiment analysis, topic\nmodeling, or entity co-occurrence, can be used to identify proper nouns. * Graph-based\nmethods: Explain how graph-based approaches, such as graph neural networks or graph-based\nentity recognition, can be used to recognize proper nouns. 5. **Challenges and limitations:**\nDiscuss the challenges and limitations of recognizing proper nouns, including: * Ambiguity:\nThe difficulty of distinguishing between proper nouns and common nouns. * Out-of-vocabulary\nwords: The challenge of recognizing proper nouns that are not present in dictionaries or training\ndata. * Domain-specific knowledge: The need for domain-specific knowledge to recognize\nproper nouns in specific contexts. 6. **Authoritative experts and resources:** Provide a list\nof authoritative experts in the field of NLP, including researchers and practitioners who have\nmade significant contributions to the development of proper noun recognition techniques. Also,\ninclude a list of definitive resources on the subject, such as research papers, books, or online\ncourses.\nTable 7: A weak-to-strong example.\n12\n\nGPT-4’s pairwise comparison prompt:\nDomain Question: [Question]\nAnswer A (Model 1): [First model’s answer]\nAnswer B (Model 2): [Second model’s answer]\nPlease compare these two answers and determine which one performs better in each of the\nfollowing criteria. For each criterion, select the better answer (A or B) and briefly explain why.\n1. Accuracy: Which answer provides more accurate domain-specific information?\nSelection: [A/B]\nReason:\n2. Completeness: Which answer covers the essential aspects more thoroughly?\nSelection: [A/B]\nReason:\n3. Relevance: Which answer better addresses the domain-specific requirements?\nSelection: [A/B]\nReason:\n4. Coherence: Which answer presents information in a more logical and organized manner?\nSelection: [A/B]\nReason:\n5. Reliability: Which answer demonstrates better technical validity and trustworthiness?\nSelection: [A/B]\nReason:\nOverall Winner: [A/B]\nKey Differences: [Brief summary of main differences between the two answers]\nTable 8: Prompt for GPT-4’s pairwise comparison of domain-specific model outputs across five evaluation criteria.\n13",
    "pdf_filename": "METEOR_Evolutionary_Journey_of_Large_Language_Models_from_Guidance_to_Self-Growth.pdf"
}