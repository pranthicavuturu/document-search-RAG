{
    "title": "Building Trust: Foundations of Security,",
    "abstract": "ThispaperexplorestherapidlyevolvingecosystemofpubliclyavailableAImodels,andtheir potentialimplicationsonthesecurityandsafetylandscape.AsAImodelsbecome increasinglyprevalent,understandingtheirpotentialrisksandvulnerabilitiesiscrucial.We reviewthecurrentsecurityandsafetyscenarioswhilehighlightingchallengessuchas trackingissues,remediation,andtheapparentabsenceofAImodellifecycleandownership processes.Comprehensivestrategiestoenhancesecurityandsafetyforbothmodel developersandend-usersareproposed.Thispaperaimstoprovidesomeofthe foundationalpiecesformorestandardizedsecurity,safety,andtransparencyinthe developmentandoperationofAImodelsandthelargeropenecosystemsandcommunities formingaroundthem.",
    "body": "Building Trust: Foundations of Security,\nSafety and Transparency in AI\nHuzaifa Sidhpurwala Garth Mollett Emily Fox\nhuzaifas@redhat.com gmollett@redhat.com efox@redhat.com\nMark Bestavros\nmbestavr@redhat.com\nHuamin Chen\nhchen@redhat.com\nAbstract\nThispaperexplorestherapidlyevolvingecosystemofpubliclyavailableAImodels,andtheir\npotentialimplicationsonthesecurityandsafetylandscape.AsAImodelsbecome\nincreasinglyprevalent,understandingtheirpotentialrisksandvulnerabilitiesiscrucial.We\nreviewthecurrentsecurityandsafetyscenarioswhilehighlightingchallengessuchas\ntrackingissues,remediation,andtheapparentabsenceofAImodellifecycleandownership\nprocesses.Comprehensivestrategiestoenhancesecurityandsafetyforbothmodel\ndevelopersandend-usersareproposed.Thispaperaimstoprovidesomeofthe\nfoundationalpiecesformorestandardizedsecurity,safety,andtransparencyinthe\ndevelopmentandoperationofAImodelsandthelargeropenecosystemsandcommunities\nformingaroundthem.\nKeywords\nPublicmodels,Security,Safety,AI,LLM,GenerativeAI,AIEcosystem\nIntroduction\nArtificial intelligence and Generative AI\nGenerativeAI,abranchofartificialintelligencefocusedonAIproductionofcontentsuchas\ntext,imagesandvideo,hasseensignificantadvancementssincetheintroductionof\ngenerativeadversarialnetworks(GANs)in2014(Goodfellowetal.,2014),whichimproved\ndatagenerationbutfacedissuesliketraininginstability.Thedevelopmentoftransformers\nandselfattentionmechanismsin2017(Vaswanietal.,2017)facilitatedfurtherimprovements\ninnaturallanguageprocessing,leadingtolargelanguagemodels(LLMs)likeGPT(Radford\netal.,2018)withhighlyadvancedtextgenerationcapabilities.Diffusionmodels\n(Sohl-Dicksteinetal.,2015)havealsoseenrapidadvancementinimageandvideo\ngeneration.\nThisrapidadvancementintechnologycapabilityhasbeenmatchedbyanequallyrapid\nuptakeinadoption. ForbespredictstheAImarketwillreachastaggering$407billionby\n20271,.Additionally,organizationslookingtotapintothishigh-growthmarketaredriving\nrapiddevelopmentandinnovationinthisspacetoachievecompetitiveadvantages,\nstreamlineoperations,andenhancecustomerengagementinanincreasinglydigital\nlandscape,particularlyfocusingonthedevelopmentofmodelsandmodelsystemsthat\naddressemergingusecasesinsectorslikemanufacturing,serviceoperations,andmarketing\nandsales2,promisingcostsavingsasakeybenefit.\n1https://www.forbes.com/advisor/business/ai-statistics/\n2https://indatalabs.com/blog/ai-cost-reduction\nImagecourtesyofhttps://www.emarketer.com\nAswithanynewtechnology,itisworthnotingthattheindustryisstillidentifyingnewand\nvaluableusesforAIandthesemarketpredictionsmayfluctuateasusecasesaretestedin\nrealworldenvironmentswithrealworldproblems. Otherfactorssuchasthemarketdemand\nforGPUsoutpacingthecurrentsupplyandpotentialmarketcontrolconcernsbyregulators,\nmayimpacttheseprojections.\nTherearevariousterminologiesandefforts3intheindustrytodefineopensourcemodels.4\nHoweveradiscussionoftheseeffortsandresearch5inthisfieldisbeyondthescopeofthis\npaper.Forthepurposeofclarityweshallbeusingthetermpublicmodel,foramodelwhich\nispubliclyavailablefordownloadanduse.\nLLMs: data versus code\nLLMsarethenextevolutionofdatascience,afieldfocusedonmathanddata.Unlike\ntraditionalsystemsandapplicationswhichrelyonlogicandprogrammingforaspecified\noutcome,largelanguagemodeldevelopmenttypicallyconsistsofarchitectureresearchand\ndesign,whichisthencoded.Inmostcases,PyTorchoranyoftheotherdeeplearning\nlibrariesareused,followedbytraininginmultiplestages.Post-training,modelweightsare\nsavedinafilethatcontainsbillionsof,mostcommonly,16-bitfloatingpointnumbers(orlow\nprecisiondatatimestoreducecomputationalandmemorycostsbyusingquantized\nmodels).TheseweightsarethenshippedtoconsumersofLLMs,alongwithconfiguration\nfilesandothermetadata.Thereforeintheirverysimplestform,theseAImodelsaremostly\ndatawithsomecodeascomparedtotraditionalsystems,applications,packagesand\nlibraries,suchasPythonorbinaryexecutablefiles.\nPopularmodelhostingservices,likeHuggingFace6hostmodelsinthe“safetensors”format,\nallowingparameterstobestoredforsafeload7andoperate.8\nWhenthemodelisusedforinference,suchasgeneratingoutput,theseweightsareloaded\nbypreviouslyuseddeeplearninglibraries;PyTorchistheexampleinthiscase.Thisprocessis\nalsoreferredtoas\"operationalizingamachinelearningmodel\"or\"puttingamachine\nlearningmodelintoproduction.”\nLLMspresentanewcombinationofconsiderations,risks,andconcernsnotseeninindustry\ntodate.Inadditiontotheapplicationofexistingsecurityconstructs(vulnerabilitiesand\ncontrols)thedataandintendeduseofLLMsbringsafetyconceptshistoricallyreservedfor\ncontentmanagementandsocialmediaplatformstotheforefrontofriskdiscussions.This\ndistinctioninboththeirdevelopmentandoperation,ascomparedtotraditionalsystemsand\napplications,implyspecialattentionandevaluationmustbetakeninproposinganindustry\nwideapproachtothesecurityandsafetyofLLMs.\n3https://www.redhat.com/en/topics/ai/open-source-llm\n4https://opensource.org/deepdive/drafts/open-source-ai-definition-draft-v-0-0-9\n5https://www.ibm.com/think/topics/open-source-llms\n6https://huggingface.co/models\n7https://huggingface.co/docs/transformers/en/main_classes/model\n8https://github.com/huggingface/transformers/tree/main/src/transformers/models\nHow AI Security is different from AI Safety\nAISecurityandAISafetyareinterrelatedyetdistinctaspectsofmanagingrisksinartificial\nintelligencesystems.AISecurityprimarilyfocusesonprotectingAIsystemsfromtechnical\nthreats(bothinternalaswellasexternal),unauthorizedaccess,andattacks,whichtraditional\ninformationtechnologysecuritytypicallycovers.Formodels,thisencompassessafeguarding\ndataintegrity,ensuringconfidentialityofthedatageneratedbythesemodels,and\nmaintainingtheavailabilityofAIservicesandenvironmentsfortrainingandtuning.Itinvolves\nimplementingrobustsecuritymeasurestopreventadversarialattacks,databreaches,and\notherformsofexploitationthatcouldcompromisethefunctionalityorintegrityoftheAI\nmodelsandtheunderlyinginfrastructure.\nAISafetyistypicallyconcernedwithensuringthatAIsystems,LLMs,andthesupporting\nlibrariesandsoftwarelikePyTorchanditsinferencesoftware,andtheirdatalifecycle\npracticesensuretheintendedoperation,asdefined,doesnotcauseunintentionalharmto\nusers,society,ortheenvironment9.Alignmentwithhuman-definedvaluesandaccepted\noutputstomitigatethisharminvolvesaddressingissuessuchasalgorithmicbiases,ensuring\nreliableandpredictablebehavior,anddesigningfail-safestomitigatepotentialharm,\nnegativeconsequence,andanypotentialriskspresented.WhileAISecurityprotectsthe\nsystemfromexternalandinsiderthreats,AISafetyensuresthatthesystemandthedatado\nnotposeathreatorcreateharmthroughitsoperationduetoitsdevelopment,training,and\nuse(intendedornot).\nIntherealmofgenerativeAImodels,thelinebetweensecurityandsafetyissuesoftenblurs\nsignificantly.Forinstance,apromptinjectionattack,typicallyconsideredasecurityconcern,\ncanleadtosafetyissuessuchasthemodelproducingtoxicorharmfulcontent,orexposing\npersonalinformationsuchasPIIandPHI.Thisintersectionhighlightsthecriticalneedfora\ncomprehensiveapproachtoAIriskmanagementthataddressesbothsecurityandsafety\nconcernsintandem.\n9SustainableAIisoftenconsidereditsownindependentdomainastheimpactsofAIonthe\nenvironmentareprolongedandoftenunabletoforecastduetoadditionalfactors.It'sreasonableto\nstate,however,thattheresultingimpactiscumulativegivencurrentstudiesfortraditionalCPUusage.\nThereforetheauthorsconsiderSustainableAIasasubsetofAISafetyinthattheecologicaland\ncorrespondinghumanharmthatresultsfromunsustainableAIdevelopmentandusagesubstantially\ninfluencesthedurationofitsbenefitsforhumansagainstenvironmentalrealitiesofchangingclimate.\nBothaspectsarecrucialfortheresponsibledevelopmentanddeploymentofAI\ntechnologies,andtheirinterconnectednatureunderscorestheimportanceofaholistic\napproachtoAIriskmitigation.\nEffectively navigating AI Security and safety issues\nItisessentialthatorganizationsdevelopingorusingAIestablishappropriateprocessesand\nworkflowsthataddressthedistinctaspectsandconsiderationsnecessaryforAISecurityand\nforAISafetywithinanoverallAIRiskManagementfunction.Whilethereareareasofreuse\nandadaptationthatarerelativebetweenthesedomains,theoverallintentandoutcomeof\neffectivelymanagingAISecurityandAISafetyistoreducetheriskandsubsequentimpact\nofbothonanorganization,users,andsocietyatlarge.Therefore,itisrecommendedthat\ndistinctyetparallelstructuresbeestablishedthatallowforconcentratedmanagementof\nissuesineachdomainenablingareasofcollaborationandcorroborationbetweenthose\nareas.\nInthecontextofAISafetyandAISecurity,thisaddsanotherdimensiontothefactthat\nsecuremodelsmayfacilitatemoresafemodels(throughtransparency,metadata,andother\nsecurityrequirementsonintegrityandconfidentiality),justasSafemodels,bynatureoftheir\npractice,couldbesecure(applyingbestpracticesofsafety-outputverification,etc.)the\nprobabilityofasafeandresponsiblemodelnothavingsomesemblanceofsecurityislow\ngiventhefocusof\"doingtherightthing\"whichisinstrumentaltoachievingsafetyoutcomes.\nObjectives, methodologies, and interdisciplinary expertise\nAISecurityfocusesonprotectingAIsystemsfromexternalthreatstoconfidentiality,\nintegrity,andavailability,whileAISafetydealswithassuringAIbehavesasintendedanddoes\nnotcauseharm.Thesedifferentobjectivesmayrequiredistinctmethodsfordiscovery,\ntriage,remediation,andmanagementreflectiveoftheircorefocuses,desiredoutcomes,and\ninherentexpertiserequiredtoexecuteandreasonaboutthoseissues;suspectedand\nreported.\nHowever,itmayoccurthattheexploitationofasecurityvulnerabilityallowsforthemodel’s\nalignmentandvaluelearningtobecompromised,therebyresultinginrealsafetyhazards.\nThisfundamentaldifferenceinapproachbutinterconnectedimpactnecessitatesseparate\nconsiderationsandspecializedresearchefforts,orperhapsastheauthorssuggest,exploring\nsharedregimesandresearchwhichfavorbothsecurityandsafetyforwellorchestratedand\nholistically-managedrisk.\nAswithobjectivesandmethodologies,AISecurityandAISafetyrequiredifferentsetsof\ntechnicalskillsandinterdisciplinarycollaborations.The\"ConcreteProblemsinAISafety\"\npaperhighlightsthatAISafetyissuesofteninvolvecomplexproblemsattheintersectionof\ncomputerscience,philosophy,andcognitivescience(Amodei,D.,Olah,C.,Steinhardt,J.,\nChristiano,P.,Schulman,J.,&Mané,D.(2016).\nTheseskillshistoricallyhavenotbeenpresentwithintheSecuritycommunity,butdoexistin\ntrustandprivacyteamsforcontentplatformsandsocialmediaorganizations.Incontrast,AI\nSecurityissuestypicallyspanmultipledomainsrequiringspecializedexpertiseinadversarial\nmachinelearningandsecuremodeldevelopmentaswellasmoretraditionalsecurity\nconceptslikedefensivecryptography,networksecurity,identityandaccessmanagement,\nandvulnerabilitymanagement.Thisdifferenceintechnicalcomplexityandinterdisciplinary\nrequirementsunderscorestheneedtoensurethecorrectexpertiseisavailableandapplied\ntoriskmitigationandremediationforeachundertheirrespectivefocusthatmaybe\nexchangedandre-leveragedaspartofanAIriskmanagementprogram.\nTemporal considerations\nAISecurityvulnerabilitiescanbeconsideredtobestaticbecausetheyarealreadypresent\nandsimplyrequirediscoveryandexploitationtobeactualized.Theyexistatthetimethey\narecodedintothesoftwareormodel,andonlyrequireexploitationinthecontextofthe\noperationalorruntimeenvironmentforthethreattoberealized,requiringimmediateand\nrapidresponseupondiscoveryandreportingtoreduceimpactandmitigatedamage.In\neffect,thisisnodifferentthansecurityvulnerabilitiesinsoftware.\nSafetyhazardsaredynamic,inthat,asthemodeloperates,itsoutputmaystabilizeor\nfluctuateagainstsocietalconsiderationsandevolution.Assuch,theyaremeasuredona\nspectrumthatmayevolve. Safetyhazardsmayberealizedattimeofreleaseoryearslater–\nmimickingtheexperienceofalogicbomb,withthecoredifferencethattheyarenotcoded\nin,ratherderivedfromtheirtrainingandinferenceasexpectationsandunderstandingof\nsafetyevolve.Therefore,safetyhazardstendtobemorelong-terminnature.\nJustasthefacetsofavulnerability(i.e.maturity,impact,complexity,etc.)determinewhere\nthemeasureofcriticalityfallsonascale,mostsafetyhazardsaremeasuredonasimilaryet\nbracketedspectrumwhichrequiresunderstandingnotonlyofthehazard,butofanyshiftin\nhuman-valuealignmentandsocietalexpectationssincetrainedorinferredatthetimeofuse\norharm.AnotherimportantdimensionisthemultilingualabilitiesofanLLMandthe\nchallengeofsafetyaroundthis.10\n10https://aclanthology.org/2024.findings-acl.349.pdf\nAddingcomplexitytosafetyhazardtriagingefforts,somegeneratedoutputmaybe\noffensivetosomegroupsofagivensocietywhilenotforothergroupswithinthesameorina\ndifferentsocietyoratadifferenttime(associetyandcultureevolvesintheirunderstanding\nandtolerances).\nThesetemporalconsiderationsareoneofthemoredistinguishingtraitsbetweenAISecurity\nandAISafetyunderscoringthepressingneedforsafetyhazardspecificmanagement,triage,\nandresolutionprocessesandworkflows.Theseshouldbeadaptedandmodifiedfrom\nwell-knownvulnerabilitymanagementprocessesandtechniquesforinclusionina\ncomprehensiveAIRiskmanagementprogramthatencompassesbothAISecurityandAI\nSafety.\nStakeholder involvement\nAISecurityvulnerabilitiestypicallyinvolveanarrowerrangeofstakeholders,primarily\nfocusedoncybersecurityexperts,AIdevelopers,andorganizationsdeployingAIsystems.\nComparatively,safetyhazardsrequirebroadersocietalinvolvement,includingethicists,\npolicymakers,andthegeneralpublic.Thisdifferenceinstakeholdercompositionand\nengagementprocessessupportstheneedfordistinctresolutionandmanagementofsafety\nhazardswithinacomprehensiveriskmanagementprogram.\nRegulatory and governance frameworks\nAISecurityissuesoftenfallunderexistingcybersecurityregulationsandcompliance\nframeworks,whileAISafetyissueswillrequireneworadaptedgovernancestructures. While\nbothregulatoryandgovernancelandscapesestablishaneedfordifferentpolicyapproaches,\nsuchfocusedprocessesmustrollupunderAIRiskmanagementfororganizationsto\neffectivelymanagecomprehensiveriskintroducedtotheirbusinessororganizationasa\nresultofAIuse.\nManagingAISecurityandAISafetyseparatelywillresultinunidentifiedriskactualization,\nineffectivecoordination,andnegativelyimpactorganizationsacrosstheirbottomline,brand,\nandmarketspace,erodingordemolishingtrustinonewayoranother,regardlessofthe\ninnovationemployedinconstructingsuchgovernance.FrameworkssuchasNIST’sAIRisk\nManagementFramework11(RMF)areagoodstartbutlacksufficientdetailinpullingAI\nSecurityandAISafetyprocessesandworkflowstogethertodemonstratehoweach\ninfluencestheoverallriskAIpresents.Here,theapplicationofexistingriskmanagement\nframeworksfailtoconsiderthecomplexityofuniquechallengesAIpresents.WhileNIST’sAI\nRMFincludesmultiplefacetsofconsideration,suchasintellectualpropertywhichdopresent\nriskbutsolelyinabusinesscontext,theydonotnecessarilyconsidertheexternalityofthat\n11https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf\nriskinoperationasoccurswithAISecurityandAISafety.RegulationssuchastheEUAIAct\nprovidemoreclearoutcomesexpectedofAIriskmanagementdrivenbycategorizingsome\nmodeluseasHighRiskbutdonotprovidedirectiononhowthoseoutcomesmaybe\nachieved.Moreimportantly,thepremiseoftheactdefinesHighRiskModelsasincreased\nriskof“harmtothehealth,safetyorfundamentalrightsofnaturalpersons”fullyfocusedon\nAISafetyonly.ThisnotonlycreatesagreaterriftineffectivelymanagingAISecurityandAI\nSafetyprocessestoreduceormitigaterisk,itallowsforconsiderablevariabilityin\nimplementationanddifficultyinenforcementsimilartothatwhichwasexperiencedwith\nGDPR. Itisalmostcertainthattherewillbeconsiderableeffortandtimespentin\ninterpretingtheActintoengineeringfeatures,checks,andtoolingformodelsinorderfor\nenforcementtobefeasible,andeventhenwillbelimitedtocoverageofAISafetyonly.\nCurrent industry trend on handling security and safety issues\nWhiletheAIindustryhastakenstepstoaddresssecurityandsafetyissues,severalkey\nchallengesstillneedtobeaddressed.Emergingtrendssuggestthatcurrenteffortsmay\nneedtotargettheseissues’underlyingcausesfully.Thefollowingexampleshighlightareas\nwherefurtherattentionisneededtoensuretheeffectivenessofAISecurityandsafety\ninitiatives.\nPrioritizingSpeedOverSafety:IntheracetodevelopanddeployAItechnologiesfor\nmarketshare,manycompaniesareprioritizingspeedtomarketoverthoroughsafetytesting\nandethicalconsiderations,particularlyasframeworkstoaddressthesearestillunderrapid\ndevelopmentwithafewinproof-of-conceptorintroducedbyotherentities12butnotwidely\nadopted.Thisistrueofanyearly,disruptivetechnology,andAIisnodifferent13.Thereis\ngenerallywidepublicacceptanceofrisksassociatedwithtechnologythataretypically\nconsideredacceptabletrade-offsforourdependenceuponthem.Howeverinthematterof\nArtificialIntelligence,globalizationandaccessibilityoftechnology,andtheimpactofdigital\ncontentandmediaondailylife,wecollectivelylacktheyearsofexperienceinthisnascent\ntechnologytocreatereputableAIforwidespreadpublicandbusinessacceptanceofthe\nrisks,whichasofyetareimmeasurabletohumansafety.Asseenwithpastsecurity\nmovements14,securityisoftenyearsbehindanytechnology,typicallyrequiringamajor\n12https://mlcommons.org/benchmarks/ai-safety/,\nhttps://www.congress.gov/bill/117th-congress/house-bill/6580/text,https://www.edsafeai.org/safe,\n13Reference:Wynne,Brian.(1983).RedefiningtheissuesofRiskandpublicacceptance,thesocial\nviabilityoftechnology.Butterworth&Co.\n14\nhttps://www.darkreading.com/vulnerabilities-threats/adapting-post-solarwinds-era-supply-chain-sec\nurity-2024\nincidentbeforetheindustrybeginsself-correction.InthemattersofAI,itisreasonableto\npredictthatintheabsenceofindividualspushingforgovernanceandriskmanagement,we\nmayyetexperienceasafetyandsecuritycriticalincident. Whilenewmodelsarebeing\nintroducedthatdoprioritizesafetyoverspeedtomarket,lackofconsensusonhowto\nconveynecessarySafetytransparencyinformationmakesthesechallengingto\ncomparativelyevaluate.Regardless,theincreaseinavailabilityoftheseSafetyconscious\nmodelsarestepsintherightdirection.\nInadequateGovernanceandSelf-Regulation:TheAIindustryhaslargelyreliedon\nvoluntaryself-regulationandnon-bindingethicalguidelines,whichhaveproveninsufficient\ninaddressingserioussecurityandsafetyconcerns.ThisisnotexclusivetoAI;lookingatthe\nhistoricalshortcomingsoftechnologyself-regulation,manygovernmentsareproposing\nlegislationtoincreasetheoverallsecurityoftechnologyascanbeseenwiththeEU’sCyber\nResilienceAct(CRA)amongothers.ForAI,weseelegislationproposedthatdoesn’talign\nwiththerealitiesofthetechnologyindustry15orthataddressesconcernsraisedbyindustry\ngroupstoprovidemeaningfuloutcomes16.ManycorporateAIethicsinitiativeslackteethand\nfailtoaddressstructuralissuesorprovidemeaningfulaccountability,beingdeveloped\nbespoketotheentityenactingthem.Thelackofindustryconsensus–asevidencedbyover\n1017differententitiesfocusedonrelatedandoverlappingareasofAISecurityandAISafety\nalone –furtherpreventsregulationsfrombeingdevelopedthatprovidesAIdevelopers\nsuccessful implementationandconsumerenforcement.TherecentvetoofSB104718\nhighlightssome,butnotallofthesechallenges.\nRelativelysuccessfulself-governancewithinthetechindustrydoeshappenoccasionally,and\nmayalsobehelpfultostudy.Twonotableexampleswithinthesecurityspacearethegradual,\nwidespreadadoptionofHTTPSbydefault19andtherapidadoptionofsigningtechnologiesin\nthewakeofthe2020SolarWindssoftwaresupplychaincompromise20.Whileboth\nphenomenaweredrivenbyvariousfactors,astrongcommonlinkistheavailabilityof\neasy-to-usetooling:withHTTPS,easily-attainable,freecertificatesfrome.g.LetsEncrypt\nhelpeddriveadoption;withsoftwaresupplychain,availabilityofsimplesigningtoolssuchas\nSigstorehelpedsigningbecomeasimpleaddedstepinsteadofalaboriousadditional\nprocess.Forthepurposesofthetopicathand,astoolingforAISafetymaturesand\n15\nhttps://www.gov.ca.gov/2024/09/29/governor-newsom-announces-new-initiatives-to-advance-safe\n-and-responsible-ai-protect-californians/\n16https://www.eff.org/deeplinks/2024/08/no-fakes-dream-lawyers-nightmare-everyone-else\n17OpenSSF,AIAlliance,CNCF,LFAI&Data,CISA,NIST,MITRE,OWASP,CSA,andmanyothers\n18https://www.gov.ca.gov/wp-content/uploads/2024/09/SB-1047-Veto-Message.pdf\n19HTTPSencryptionontheweb:https://transparencyreport.google.com/https/overview\n20SigstoreGraduates:AMonumentalStepTowardsSecureSoftwareSupplyChains.\nhttps://openssf.org/blog/2024/03/20/sigstore-graduates-a-monumental-step-towards-secure-soft\nware-supply-chains/\nbecomesmorewell-known,it’spossiblethatwidespreadadoptionwillpromotegreater\nsafetyastimegoeson.\nMoregenerally,successfulindustry-drivensoftwaresecurityinitiativessuchasthose\nmentionedearliertendtoinvolveadefinedsetofbestpracticesthatcanbeimplemented\nrelativelyindependentlyofprimaryfeaturedevelopment.ThismayprovedifficultfortheAI\nspace:often,advancingthecapabilityofamodelmaydirectlyimpactitssafety,and\nconversely,ensuringmodelsafetymayinvolveholdingbackcapability.Withtheindustry\nfocusedonrapidadvancement,prioritizingsafetyandsecurityattheexpenseofcapability\nmaybeatradeoffkeystakeholderswillbeunwillingtomake.\nInadequateprocessesforhandlingflawandhazardreports,silentfixes,\ndiscouragingreporters: Thereexistsalackofcommonmethodsandprocessesof\nhandlingmodelflawsreportedbyusers-nosuchmethodologyhasbeenproposedorshared\namongmodelmakersanddeveloperstoserveasacommonreportingframeworkoutsideof\nHuggingFace’s“discussions”featureor“reportmodel”flagwhichlackssufficient\ndocumentationtoindependentlyinspectthereportingprocessandreasoningforreporting21.\nIthastakenthesoftwareindustrydecadestodevelopaflawed-yet-functionaldisclosureand\nreportingsystemforsoftwarevulnerabilities,ofwhichmanymodelmakershavelittletono\nexposuretoorexperiencewith.AIisatechnicalevolutionofdatascienceandmachine\nlearning,principallydistinctfromtraditionalsoftwareengineeringandtechnology\ndevelopmentduetoitsfocusondataandmathandlessonbuildingcomprehensivesystems\nforuserswhichhasestablishedmethodologiesforthreatmodeling,userinteraction,and\nsystemsecurity.Mostmodelmakerswhoprovidetheirmodelsforfreemayhavetheirown\nbespokeprocesseswhichareobscuredandmaybeinadequate,ormaynothaveanyprocess\natall.Inthosecases,reportinganissuemayinvolvedirectlyreachingouttothemodelmaker,\nwhichcanbecumbersome.ItisimportantthatreportingmechanismsaligntoIndustry's\nexistingexpectationsinreportingandmanagementofflaws—regardlessiftheyareunique\ntosecurityorsafetyasbothrequirethecontextoftheissue’sdiscovery,disclosureofthe\nstepsbywhichitwasfoundforreproducibility,evidenceorindicationsofimpactinorderto\nsimplyverifythevalidityoftheissuebeyondtheindividual’sclaiminreporting.Further,both\nrequiretimetoreproduce(dependentonthemutationsofoutputsinuse),develop\nmitigationorfixesfor,coordinationinrelease,trackingtoclosure,andpotentialdevelopment\nofregressiontestingtopreventtheirfurtherrecurrence.\nBasedontheabove,weproposeprocessesforhandlingbothsecurityissuesaswellassafety\nhazardsforpublicmodelsinthefollowingpartsofthepaper.\n21https://huggingface.co/docs/hub/en/moderation#reporting-a-repositorydoesprovidesome\ninformationundertheModerationheadingofHub,andlaunchesapublicdiscussiononthetopic\nautomatically.\nAdapting existing processes to AI Models\nTraditional application security process in brief\nTheSoftwareDevelopmentLifecycleprocesstraditionallyinvolvesdesigning,writingthe\nactualapplication(usingsometypeofprogrammingorscriptinglanguage),compilingitif\nrequired,testing,andthenfinallypackagingandshippingtheendproducttoitsusers.\nThedevelopmentprocessesdiffersignificantlybetweenopensourceandclosedsource\nsoftware.Closedsourceapplicationsoperateunderspecificexpectationsregardingusage\nandsecurity,dictatedbytheorganizationbehindthem.Theseexpectationsoftenstemfrom\nestablishingabrandandmeetingcustomerdemandsforsecurity,reliability,andsupport.In\ncontrast,opensourceprojectstendtoemphasizefunctionalityandthesecurityboundaries\noftheapplication,relyingonacollaborativemodelwheretransparencyandcommunity\ninvolvementshapethedevelopmentpractices.Thisdifferencemeansthatopensource\napplicationsmaynotfacethesamepressuretoconformtoexternalbrandexpectations,\nallowingforafocusoninnovativefeaturesandflexibilityinmeetinguserneeds.\nDespitethesedistinctions,thereareuniversalindustryexpectationsforthemanagementof\nsecurityvulnerabilitiesthatapplytobothopenandclosedsourceapplications.Regardlessof\nthedevelopmentpracticesortheperceivedrobustnessofthesecuritymeasuresinplace,all\nsoftwaremustaddressvulnerabilitiescomprehensively.Thecriticalnatureofsecurityin\nsoftwaredevelopmentmeansthatorganizationscanfacescrutinyandrepercussionsfor\nfailingtomanagevulnerabilitieseffectively.Therefore,understandingandadheringtothese\nindustrystandardsiscrucial,asusersandstakeholdersexpectabaselinelevelofsecurity\nfromallsoftware,irrespectiveofitssource.\nEffectivevulnerabilitymanagementisessentialandmustbeimplementedusing\nindustry-acceptedprocesses,regardlessofwhetherthesoftwareisopenorclosedsource.\nEachapplicationsecurityflawisassignedauniquenumberbyaCVE(Common\nVulnerabilitiesandExposures)namingauthority(CNA)governedbyacentral,neutraland\nnon-governmentaffiliatedorganization,oftenconsidereda‘publicgood’.Thisorganization’s\nCVEprogramaimsforconsistencyandindustryalignmentinhowtoidentify,define,and\ncatalogpubliclydisclosedcybersecurityvulnerabilitiesinsoftware.TheCVEnumbercanbe\nuniversallyusedtotrackaparticularvulnerabilitytoa particularcodebase,andisreferredto\nbyvendors,researchersandconsumersalike.Softwareprojectsaffectedbythevulnerability\nareabletocontestthisassignment,andthereisscopeforrejectionoftheCVEifwarranted.\nOftenothertypesofmetadatalikeCVSS(CommonVulnerabilityScoringSystem)scoreand\nCWE(CommonWeaknessEnumeration)numbersareassignedduringthesecurityflaw\ntriageprocess.Theyhelpinidentifyingtheseverityimpactandapplicabilityoftheflaw,\nwithoutnecessarilystudyingtheflawindetail.Theyalsohelpautomatedtoolsinprocessing\nflawsandassistsecurityscanners.\nSecurityscannerstypicallyuseCVEidentifiersandCVSSscorestoautomaticallyidentify\nandassessknownvulnerabilitiesbymatchingdetectedissuesagainstvulnerability\ndatabases.TheCVSSscoresprovidestandardizedseveritymetrics,allowingscannersto\nprioritizeflawswithoutdetailedmanualanalysis.Thisenablesefficientprocessingand\nremediationofsecurityflaws,assistsindeterminingtheirapplicabilitytospecificsystems,\nandenhancesreportingandcomplianceeffortsthroughseamlessintegrationwithother\nsecuritytools.\nGiventhedynamicnatureofmodelsandtheirinherentdifferencestotraditionalsystems\nandapplicationdevelopment,a“liftandshift”approachtosecurityorsafetymanagementis\nnotpractical.However,therearevaluableelementsofexistingsecuritymanagement\nprocessesthatarewell-tested,understood,andinvestedinacrossindustrythatcouldbe\nappliedtoAImodelsallowingforappropriatesecurity,safety,andriskmanagement\nScope of AI Security flaws\nSecurityflaws,muchliketheskillsandexpertiserequiredtomanagethem,existdifferently\nthansafety,bias,ethicsoranyothertypeofissues.Inordertoappropriatelymanageand\nresolvetheseforAI,weproposethefollowingdefinitionofanAISecurityvulnerability.\nAISecurityvulnerability: AflawinanAIsystem,resultingfromaweakness22thatcanbe\nexploited,causinganegativeimpactontheconfidentiality,integrityoravailabilityofthe\naffectedcomponentorcomponents.\nNote:HerethewordAIsystemisusedinsteadofanAImodel,becauseAIflawstendto\nmanifestthemselvesinthewaythemodelsareused.Forexample,inthecaseofa\nmulti-modalmodel,itmaygenerateunsafeimagesforsomeprompts,butifthesemodels\nareneverusedbytheapplicationtogenerateimages,butratherusedforsomeother\npurpose,theflawhasnotmanifestedintoavulnerabilityinthatsystem.\nWhiletherearemanysimilaritiestosecurityvulnerabilities,certainnuancesonlyexistforAI\nmodelsasaresultoftheirnatureandhowtheywork,describedbelow.\n22Asecurityweaknessisacondition(i.e.flaw,error)orcharacteristic(i.e.improperconfigurationor\nweakencryptionthatpermitstheapplicationorsystemstohaveincreasedvulnerabilitytoattack.See\nalsohttps://nvd.nist.gov/vuln\n● LossofConfidentiality:Whenmodelsrespondwithinformationtheyshouldnot\nrevealasmentionedinitsmodelcardorotherdocumentsthereisalossof\nconfidentiality.Thisincludesbutisnotlimitedto:\n○ UnauthorizedPIIandIntellectualpropertyinformation.\n○ Outputthatcancausewidespreaddamagewheretheinformationisnot\ngenerallyavailableontheinternetorconsideredpublic.\n● LossofIntegrity: Thisissuecanoccurwhenanattackercannegativelyinfluence\nthedatawithinthemodelorisabletopoisonthemodel.\n○ Adversarialfine-tuningcancausemaliciouschangestothemodelweights,\nandresultinwrongorspecially-craftedoutputgeneratedbythemodel.\n● LossofAvailability:Theattackerisabletosuccessfullyperformadenialofservice\nattack(DoS)23onthemodel24andstopallinferenceattempts.Inmostcases,thisis\nconsideredasoftwareissueratherthananAImodelissue.\nTherearecircumstanceswherebythelossofconfidentialityorintegritymayresultinhuman,\nsocial,orenvironmentalharm,howeverthismayonlybeascertainedinthecontextofthe\nmodel’soperationaluse.Insuchcases,theimportanceofinformationexchangebetweenAI\nSecurityandAISafetyteamsisparamountforfullyexploringtheimpactofanexploitedAI\nvulnerabilitysoadditionalsafeguardsandsecurityprotectionsmaybeappliedtoprotect\nagainstfutureoccurrencesofsuchincidents.\nAISecurityvulnerabilitiesdonottakeintoaccounttheplatformonwhichthemodelisrun,or\nthesupportlibrarieswhichareusedtoload,runandtrainthemodel.Securityissuesinthose\nsupportinfrastructuresarealreadycoveredbytheexistingCVEprocessdetailedabove.\nScope of AI Safety flaws\nAISafetyisaninterdisciplinaryfieldfocusedonpreventingaccidents,misuse,orother\nharmfulconsequencesarisingfromdatageneratedfromagenerativeAImodel.\nAsdiscussedearlier,AImodelsafetyisanovelconceptassafetyisnotcommonplacewithin\nthesoftwaredevelopmentandsecurityfield,howeverthereareafewnoteworthy\nexceptions. MedicalDeviceSafetyandAutomotiveSafetyarewellestablishedwithintheir\nindustrywiththeirownconsiderationsforregulatorycompliancebeyondanyrisk\nmanagementprogram.‘TrustandSafety’or‘PrivacyandTrust’teamsareoftenalignedwith\nbutexecutedandmanagedseparatelyfromSecurityresultingintheiractivitiespotential\nexclusionfromanenterpriseriskmanagementprogram.Theseconceptshavetraditionally\nbeenexcludedfromenterpriseriskmanagementbecausetheirscopewaslimitedtouser\n23https://arxiv.org/abs/2010.02432\n24\nhttps://portswigger.net/daily-swig/deepsloth-researchers-find-denial-of-service-equivalent-against-\nmachine-learning-systems\nprivacyasaprotecteddatasetduetoregulatoryrequirementswhichtodaydonotinclude\nthecognitiveormentalimpactsinexposure;thoughthisisreflectedinlegalcasesas\n“intangibleinjury”ornon-economicdamagesforwhichsoftwarehasnoequivalentrisk\nconsiderationorquantifiablemeasurement.\nTechnologyproducersthatstoreuserdata,manageuserinformation,orretainuseraccount\ndetailsoftenhavetodealwithadversarialattackswhichtargetlossofconfidentialityorloss\nofintegrity.Theseattacksmayincludebutarenotlimitedtothecompromiseofuserprivacy\n(adversarialtargetingofmailaccountsofjournalists),illicitaccountusage(storingand\nsharingexplicitimageryofminors),spreadofmisinformation,andunauthorizedsharingof\nsensitivecontent(falseortrue). Today,thetechnologyindustry’sclosestalignmentwith\nsafetymanagementisinregulatorydutytoprotectfromharm,physicallyorcognitively(in\nthecontextofsocialplatforms)andnotnecessarilyinevaluatingandquantifyingthe\nresultingimpactfromexposureofsensitiveinformation,particularlyfromsources\ninappropriatelyperceivedbythemassesastrustworthy.Untilrecently,technologyhasbeen\nviewedasanenablementforharmsuchasthattooccurbutonlybecausehumansarethe\ncauseofviolationsintrustandsafety(medicaldevicesandautomotivesafetystillremainthe\nexception).\nAIishoweverthefirsttimeinwhichthetechnologyanditsdevelopmentarethecauseof\nviolationsintrustandsafety,bringingconsiderationsofresponsibleandethicalAItothe\nforefrontoftheAISafetydiscussion.\nWeproposetodefineAISafetyissuesandflawsashazardstoavoidconfusionbetween\nsecurityflawandsafetyflaw.Wefeelhazardmorereadilyencapsulatesthepotentialfor\nharmseparatefromflaworissuewhicharecurrentlywellutilizedindustryterms.\nAISafetyhazard:Anunexpectedmodelbehaviorthatisoutsideofthedefinedintentand\nscopeofthemodeldesign.Safetyhazardsmayresultinharmfulcontentgeneration,biasin\ndecisionmaking,orviolationofsocialnormsandethicsofgroups;theimpactandseverityof\nwhichwillvarygreatlyfromgrouptogroupbasedontheirculture,social,ethnic,oranthropic\nsystems.Harmmaybecategorizedbylossoflife,injuryorotherphysicalormentalhealth\nimpactsordamage,socialandeconomicdisruptionordegradation,orsomecombination\nthereof.\nTaxonomy of AI Safety risks\nAnimportantpartofhandlingAISafetyflawsisfirsttodefineanddecideonasafety\ntaxonomywhichwillbeusedbyanAImodeloranAIsystem.AISafetytaxonomyisa\nstructuredclassificationsystemthatidentifies,organizes,andinformsdifferentaspectsof\nsafetyandreliabilityinartificialintelligencesystemsandtheiruses.Thetaxonomyhelps\noutlinevariousrisks,challenges,andconsiderationsthatneedtobeaddressedtoprevent\nharmandensurethatAIsystemsoperateasintended.\nThereisnoshortageofAISafetytaxonomiesandtheresearcharounditinrecentyears25.\nThispaperdoesnotaimtostudyallthetaxonomiesanddoesnotintendtosuggestwhich\noneshouldbeused.However,someparametersforchoosingacommunitytaxonomy26\nwouldbe:\n● Taxonomyisavailableunderapermissiveopencontentlicense:Thisenablesmodel\nmakersandmodeldistributorstousethemtoevaluatemodels,regardlessofwhether\nthemodelitselfisopenorclosed.\n● Taxonomyisdevelopedintheopenandanyoneisfreetocontributetoits\ndevelopment:Itdoesnotendupbeingaresearchpaper,butisincontinuous\ndevelopmentbythecommunity.\n● Taxonomyisextensible:Youcanusethetaxonomyasthemaindocument,and\nextendthetaxonomy,addingvariationsaswarranted basedontheinternalstrategy\nofthemodelmakers,providedthecoreelementsofthetaxonomyaremet.\n● Ataxonomydoesnotcauseharm. Taxonomiesshouldbecarefultonotpublish\nmodelresponsestopromptsbecause,forsomehazardcategories,theseresponses\nmaycontaincontentthatcouldenableharm.Equally,unsaferesponsescouldbeused\nbytechnicallysophisticatedmaliciousactorstodevelopwaysofbypassingand\nbreakingthesafetyfiltersinexistingmodelsandapplications.Itmayrequiresome\ntaxonomiestobesocietyorculturallyunique.27\n● Taxonomiesprovidesufficientinformationtobeusedsafelyinbenchmarking,insuch\nawaythattheyareavailableforeasyintegrationifrequired.\nChallenges\nTrackingsafetyhazardshasbeenoneofthebiggestchallengesofAImodelsandsystemsso\nfar.Whiletherehavebeenseveraldevelopmentsinthesafetyfield,suchassafetyaligned\nmodelsthatcandetectunsafecontent,alackofconsensusandunderstandingintheway\nsafetyhazardsshouldbeclassified,trackedandperhapsevenremediatedpersistswhich\ndelayseffectivetracking,coordination,andmitigationofthesehazardscurrentlypresentin\nAItoday.\n25https://airisk.mit.edu/\n26Acommunitytaxonomyiscommunity-drivenbythediversemembersoftheAIcommunityhelping\ntobuildanddeliversaferandaccurateAIsystems.\n27Taxonomiesneedtobesociallyorculturallyaware.Certainreferencesorexpressionsmaybe\nacceptableinonesocietyorculture,butatthesametimemaybeoffensivetoothers.\nHandling AI Security and AI Safety\nAdaptingvulnerabilitydisclosureandcoordinationforaddressingAISecurityflaws\nWeareproposingthefollowingadaptationsfortrackingAIrelatedsecurityflaws:\n● Modelprovidersshouldhaveamechanismforreportingissuesinaconfidential\nmanner.\n● Reportersshoulddisclosethesecurityissueandallnecessarydetailsfirstand\nforemosttothemodelprovider.\n● Modelprovidershavetheprimaryresponsibilitytotriagenewreportsandfollowan\nestablishedandpublicvulnerabilitydisclosureprocessorpolicy.Thisisnotwithout\nadditionalchallenges28.\n● IfthemodelproviderisaCNA29andtheyrecognizethisasasecurityissue,thenthey\naloneareresponsibleforassigningaCVEIDtothisissue.Ifthemodelproviderisnot\naCNA,thentheCVEidcanberequesteddirectlyfromtheCVEprogrampartner.30\n● Ifthemodelproviderdoesnotrecognisetheissueassecurityrelatedorifthemodel\nproviderdeniesassigningaCVE,thereportermayraiseaformalrequestwiththe\nCVEProgrambyfollowingtheinstructionsoutlinedontheirsite.31\n● ModelprovidersissueVEXstatementstoconveyknownvulnerabilitypresence(or\nlackthereof) inmodels.\nItshouldbenotedthatmanyofthecurrentchallengesthatexistinthetraditional\nvulnerabilitydisclosureandcoordinationprocesshaveparallelchallengesinAISecurity.\nWhilemodelsmaypubliclydeclaretheirscopeandintentofuse,triagemuststillbe\nperformedtoascertaintheapplicability,reproducibility,andimpactofthereportedflaw\nbeforecoordinationandremediationstepsmaybetaken.\nSecurityframeworksandtheirrelevance\nToconcludethistopic,wetakeabrieflookatthevariousAISecurityframeworks.These\nframeworksalloworganizationstoreducetheriskoftheAIsystemandalsohelpthemmeet\nregulatoryrequirementsspecifictotheorganization'sindustryvertical.Itisusualforthe\nindustrytolayertechnicalframeworkswiththosewhicharemoreconceptual.\nSomeoftheseframeworksarehighlevel,somehelptovisualizethethreatlandscape,some\nofthemhelptomeetthelocalregulatoryrequirements.Intheendonewouldconsiderthe\nproposedsecurityhandlingprocedureinthispaperasalowlevelrequirement/procedure,\n28https://www.cve.org/Media/News/item/blog/2024/07/09/CVE-and-AIrelated-Vulnerabilities\n29https://www.cve.org/ProgramOrganization/CNAs\n30https://www.cve.org/PartnerInformation/ListofPartners\n31https://www.cve.org/ReportRequest/ReportRequestForNonCNAs#RequestCVEID\nwhichcouldverywellbeasubsetofanyoftheaboveframeworks.Wesummarizethese\nframeworksasfollows:\nFrameworksforDevelopersandPractitioners:\n-Focus:Practicalsecurityimplementation,threatmodeling,andvulnerabilitymanagement.\n-Outcome:SecureAImodels,preventdatabreaches,andaddressrisksinAIapplications\nusingtoolslikeOWASP32andMITREATLAS.33\nFrameworksforChiefInformationSecurityOfficers(CISOs):\n-Focus:IntegrationofAISecurityintobroaderriskmanagement.\n-Outcome:AlignAISecuritypracticeswithexistingcybersecurityprogramsandensure\ncompliancewithregulatorystandards(e.g.,NISTAIRMF34,GoogleSAIF35).\nLegislaturespecifictoageographicarea:\n-Examples:EUAIAct36andCanada’sArtificialIntelligenceandDataAct(AIDA37).\n-Outcome:ClassifyAIsystemsbasedonrisklevelsandprovidegovernancerulestoensure\nsafe,ethicaluseofAI,affectingcomplianceandlegalresponsibilities.\nCurrenttrackingeffortsinthesecuritycommunity\nThereareseveraleffortsongoinginthe securitycommunityinlinewiththeaboveto\nstreamlinetheprocess.NotableamongthemaretheeffortsbyothersforCVE/CWEforAI.38\nProposal for hazards disclosure and management\nHeavilydrawingonthepriorworkdonebySvenCattelletal39Thisproposalisbuiltupoftwo\nmaincomponents.\n32https://owasp.org/www-project-ai-security-and-privacy-guide/\n33https://atlas.mitre.org/\n34https://www.nist.gov/itl/ai-risk-management-framework\n35https://safety.google/cybersecurity-advancements/saif/\n36\nhttps://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on\n-artificial-intelligence\n37\nhttps://www.smartinsights.com/traffic-building-strategy/offer-and-message-development/aida-mod\nel/\n38https://github.com/CWE-CAPEC/AI-Working-Group\n39https://arxiv.org/pdf/2402.07039\nExtendingmodel/systemcards\nModelcardsareusedtodocumentthepossibleuseofthemodel,thearchitectureand\nsometimesthetrainingdatausedtotrainthemodel.Today,theyprovideaninitialsetof\nhuman-generatedmaterialaboutthemodelthatisbeneficialinassessingitsviabilityofuse,\nhowevertheyhavesubstantiallymorepotentialandapplicabilitybeyondtheircurrentusage.\nAsorganizationschoosetoadoptandbuildAIsystemsandcapabilitiesbuiltontopof\nmodels,they’rechoosingtodosoinenvironmentsandwithtechnologiestheyalreadyhave\ndeployed.Inordertofacilitatebetterunderstandingofmodelswherevertheytravelor\ndeployto,we’dliketoproposesomechangestomodelcards.\nThemodelcardshouldserveastheintroductoryinformationaboutthemodel.Inorderfor\nadoptersandengineerstoeffectivelycomparemodels,weneedaconsistentsetof\nminimumfieldsandcontentthatmustbepresentinacard.Toprovideforthis,wesuggest\nthedevelopmentofaspecificationformodelcards.Google’smodelcardpaper40wasa\nnovelfirststepinexpressingthiscontentinastandardway,andsince2018industryhas\nbeguntoaskdifferentquestionsaboutmodelsthatwarrantupdatestothis\nnon-standardizedformat.\nInadditiontotheexistingfieldsrecommendedfromthepaper,weproposethefollowingbe\naddedormodified:\n● IntentandUse:While‘IntendedUse’shoulddescribetheusers(who)andusecases\n(what)ofthemodel,itdoesn'taddresshowthemodelistobeused.Expanding\nIntendedUsetobeastatementforthemodelsystemthatdescribesitsusagewith\npreciseefficacyprovidesclarityinallthreeaspectsofconsideration(who,what,how)\nthatisnotpresenttoday.Forexample:\n○ Fromatextprompt,weproduceimagesthataresafeforwork,safefor\nchildren,andfreeofdemographicbias\n● Scope:ThepurposeofScopeistoexcludeknownissuesthattheModelproducer\nhasnointentorabilitytoresolve.WhileIntendedUseshouldconveyusecasesthat\nareoutofscope,providingadditionaldetailonspecificsthatareoutofscopeor\nconsiderationforresolutionprovidesadoptersorconsumersofthosemodelsmore\ncontexttomakeaninformeddecisionfrom.Thisalso ensuresthatreportersof\nhazardsunderstandthepurposeofthemodelbeforereportingaconcernthatis\nexplicitlydeclaredasunaddressableagainstitsdefineduse.Forexample:\n○ ThisisanLLMwithnoprotections;promptinjectionsareoutofscope.\n● EvaluationData:Whileanexistingfieldinthemodelcard,since2018we’ve\ndevelopedevaluationframeworks41thatfocusondifferentconsiderationsthatinform\nadoptersandconsumersonwhatthemodelhasachieved.Weproposeextending\n40https://arxiv.org/pdf/1810.0399\n41https://mlcommons.org/benchmarks/ai-safety/\nEvaluationDatatoprovideanestedstructuretoconveyifaframeworkwasalsoused,\nandtheoutputsofsuchevaluationthatwererunonthemodel. Standardizedsafety\nevaluationsarepreferredandwouldenableaskilledindividualtobuildasubstantially\nequivalentmodel.Continuedresearchanddevelopmenttoestablishneworupdated\npublicevaluationsshouldbeencouraged.\n● Governance:Governinginformationaboutthemodelisessentialtounderstandhow\nanadopterorconsumercanengagewiththemodelmakersorunderstandthe\nmethodologybywhichitisproduced..\n● References(optional):Modelmanufacturersmayfindincludingreferencestobe\nbeneficialforpotentialmodelconsumersinbothunderstandingthemodel’s\noperation,butalsodetailingartifactsandothercontent,suchasanAIBOM,safety\naudit42,orsecurityaudit,todemonstratethematurityandprofessionalismofagiven\nmodel.\nSettingtheseandtheexistingfieldsasrequiredelementsforamodelcardallowsfor\nindustrytobegintoestablishcontentthatisessentialforreasoning,decision-making,and\nreproducibilityofmodels.Byrenderingmodelcardsinanindustryacceptedformator\nstandard,wepromotetheinteroperabilityofmodelsandtheirmetadataacrosstechnology\necosystems.\nWeanticipatemanyorganizationswillbuildanddevelopapplicationsandsystemsthat\nembed,relyupon,andinteractwithmodelsinanoperationalcapacity.Thelossofamodel\ncardasreferenceablematerialforpolicyenforcementatruntimeorauditingwillexacerbate\nexistingoperationalchallengesinspeedy,incidentresponseandresolution.Aswe’veseen\nwiththesupplychainsecuritymovementforsoftware,thevalueofmetadataand\nattestationsaboutwhathastranspiredtoproducesoftwareisactionableforgatekeepingat\ndeploymentandinunderstandingwhereriskexists.\nThedatacontainedinmodelcardshasthepotentialequivalentfunction,whenstoredinan\nwidespreadformat,suchasOC43I,toallowmodelcards,AIBOMs44,andotherevolving\nmetadatatypestobepushed,consumed,andleveragedinexistingtoolingandecosystems\nwithmaturingprocessesforingestion,analysis,insights,andpolicyenforcement.\n42Safetyauditisacomprehensivereportidentifyingsafetyhazards,recommendationsforremedial\nactionandevidenceofduediligenceforstakeholdersandregulators.\n43https://opencontainers.org/\n44https://owasp.org/www-project-aibom/\nCommonflawsandexposures(CFEs)forHazardtracking\nWhiletheCommonVulnerabilitydisclosuremechanismusedtotracksecurityflaws,is\neffectiveintraditionalsoftwaresecurity,itsapplicationtoAIsystemsfaceseveral\nchallenges:45\n● AImodelsmustsatisfystatisticalvalidity46thresholds47Thismeansthatanyissuesor\nproblemsidentifiedinanAImodelsuchasbiasesetcmustbemeasuredand\nevaluatedagainstestablishedstatisticalstandardstoensuretheyaremeaningfuland\nsignificant.\n● Concernsrelatedtotrustworthinessandbiasoftenextendbeyondthescopeof\nsecurityvulnerabilitiesandmaynotalignwiththeaccepteddefinitionofsecurity\nvulnerabilities.\nRecognizingtheabovelimitationweproposeexpandingtheecosystem,withanewterm\ncalled“CommonandFlawsandExposure(CFE)”,whichisanalogoustotheCVEinthe\nsecurityspace\nCoordinatedHazardDisclosureandExposure(committee)\nAcentral,neutralbodymustexisttotrackpossiblesafetyhazardsinthesamewaysecurity\nflawsaretracked(asdiscussedabove).Reporterswhodiscoversafetyissues,areexpected\ntocoordinatewiththemodelproviderstotriageanddofurtheranalysisoftheseissues.\nOnceestablishedthattheissueisindeedasafetyhazard,thisbodyassignsaCFE(Common\nflawsandexposure)numbersimilartoCVE.Modelmakersanddistributorscanalsorequest\nCFEnumberstotracksafetyhazardstheyfindintheirmodels.\nThisbodyisthecustodianofCFEnumbers.Theyareresponsibleforassigningthemto\nsafetyhazardsaspertheprocessdescribedbelow,fortrackingthemandifatsomepoint\npublishingtheminvariousforms.Weproposea“HEX”formattopublishCFEdatainthe\nsectionbelow.\nAdjunct panel\nInaccordancewiththepaperbySvenCattelletal,wewouldalsoliketoproposeanadjunct\npaneltofacilitateresolutionofsafetyhazardsthatarecontested:\n45Thesedetailsaretakendirectlyfromthepaper:https://arxiv.org/pdf/2402.07039\n46Referstotheconfidencethattheresultsorconclusionsdrawnfromthemodel'sperformanceare\nnotduetorandomchancebutaresupportedbyrobuststatisticalevidence.\n47Criteriaorbenchmark\nIfareportersubmitsasafetyhazardonstatisticalorothergrounds,avendorormodelmaker\nmayrejectorcontestthesafetyhazardandtheAdjunctPanelisinformed.Inthecourseof\nreporting,ifthesubmitter-suppliedsample/outputpairsarestatisticallybiased,andan\nunbiasedsamplesetwouldnotshowaviolationofthemodelcard,theadjudicatorpanelmay\nrequestsupportingdatafromthevendorormodelmakertovalidatetherejection.The\nadjudicatorandmodelmakercanjointlyassesswhetherthemaker-supplieddataistoo\nsensitiveforthesubmitteranddetermineonfurtherstepsforresolution.\nIntheinitialphaseofimplementation,bootstrappingtheAdjunctpanelwiththeCoordinated\nHazarddisclosurecommittee(describedabove)providestheinitialexpertiseandalignment\nforfutureengagementbetweenthetwobodies.Howeveritisimportantforeachbodytobe\nneutraland/orhavegoodrepresentationfrommajorvendors/reportersintheecosystem.\nAdaptingVEX\nTheVulnerabilityExploiteXchangeformat(VEX)48isarecentformatforconveyingthe\nexploitabilitystatusofavulnerabilityinsoftware.Whilenascentinindustryadoption,we\nbelievethisstyleofinformationconveyancecanbeleveragedinthecoordinateddisclosure\nandnecessaryresolutionofAISafetyhazards.\nWeproposeexplorationintowhatapotentialHazardsExposureeXchange(HEX)format\nwouldprovidetheindustryinconveyingtheexposureandresultingimpactasafetyhazard\nhasontheoperationaluseofAI.Theimpactofthosehazardsonintendedmodeluseand\noutputsmustbetrackedtoinformconsumershowtheiruseisimpactedsotheymaytake\nstepstocorrectanydecisionsorharmresultingfromanimpactedAIsystem,thisincludes\nhazardsresultingfromthedevelopment,training,andinferenceofthemodelandallits\nvariants.\nMostexistingVEXfieldsandvaluesareadaptabletosafetyhazards..WhereVEXdefinesthe\nsubcomponentID,weproposeanuniqueidentifierconstruct(suchasacommit)and\ncategory(suchasthemodellifecyclestageandsource)todefinehowandwherethehazard\nisintroduced.Statusofthehazardisacriticalfieldinprovidingconsumersofmodelswith\nsufficientinformationtounderstandhowtheyhavebeenexposedandmustconsider\ncontentfromthemodelcard(intendeduse)indetailingthatstatus(i.e.affected,\nunaffected,fixed,underinvestigation)andjustification(i.e.model_use_not_approved,\nguardrails_in_place,ortuned_out).Somemodelsprovideformultipleusesinwhichahazard\nmayonlyadverselyimpactaportionofthem,itisimportanttoinformmodelconsumersif\n48\nhttps://www.cisa.gov/resources-tools/resources/minimum-requirements-vulnerability-exploitability-e\nxchange-vex\ntheirusematchesanimpacteduseandwhatthespecificsofthatimpactarejustasmuchas\nunderstandinghowtheymaynotbeaffectedbythehazardintheiruseofthemodel.\nAdditionalresearchisrequiredtodevelopstatusjustificationstatementsandotherpotential\nHEXfieldsthatareinformedbyCFEandSafetyFrameworksinordertocomprehensively\ndevelopaproposalforHEXacceptabletoindustry.\nWorkflow\nWewouldliketoproposeaworkflowsimilartothesecurityworkflowabove,withseveral\nmarkeddifferencestoaddressconsiderationshighlightedpreviouslyregardingAISafety\n● Reportersattemptingtoreportsafetyissueswoulddosoinaccordancewiththe\nspecifiedintentandscopeinthemodelcard,reflectiveofsafetybenchmarkingthat\nhasoccurred.If,forexample,themodelisnotintendedforaspecificuse,hashada\ntopicexplicitlyexcludedfromtrainingorisconsideredbeyondthescopeofthe\nmodel,thesafetyissuebeingreportedislikelytobeclosedasinvalidbythemodel\nmaker.\n● Safetyissuesarereportedtothemodelmakers,followingtheirestablishedand\ndiscoverablesafetyreportingprocess.Iftheyacknowledgetheycanapproachthe\nCoordinatedHazardDisclosurecommitteeforaCFEnumber,whichisthenusedto\ntrackthatparticularsafetyissue.\n● Ifmodelmakersrefusetoacknowledgethesafetyissue,reporterscanapproachthe\nAdjudicatorpanel,whichisresponsibleforresolvingdisputes.\n● Intheend,theissueiseitheracceptedorrejected.Ifaccepted,theissueispublished\ninapublicdatabaseandthemodelmakerissuesasafetyadvisorypointingtothe\npublicrecord.\nAstheprocessevolves,severalenhancementsmaybemadetoextendthesafetytracking\nandrootcausesofthesafetyhazardstobuildindustryknowledgeofprevention,akintodata\nlikeCPEs(CommonPlatformEnumeration)orCWEs.\nCurrentsafetyefforts\nAtthispointauthorswouldliketomentionseveralcommunityeffortscurrentlyunderway\nforstandardizationofprocesses.Someofthemmaybeorthogonaltoeachother,butwe\nwouldliketostresstheneedtocollaboratehere,ratherthancompete.Thelistbelowisinno\nwayexhaustive,andit'squitepossiblethatneweffortswouldriseortheoneslistedhere\nwouldbecomeinactiveafterthispaperispublished:\n● AIAlliance-TrustandSafetygroup49\n49https://thealliance.ai/\n● MLCommonsAISafetygroup50\n● CoalitionofSecureAI51\nCurrent challenges in the industry for implementation of this proposal\nSeveralchallengesinthecurrentmodelecosystemexistandtheyneedtobeaddressedin\nordertoimplementopenandeffectivetrackingofsafetyhazards.\n● Model/systemcards:Whilemostbiggermodelshaveexplicitmodelcards,\ncontainingdetailsofhowthemodelhasbeentrainedandoutputofthevarious\nbenchmarks,forothermodels,theprocessisnotuniformacrosstheindustryandhas\ntoimprove.Furthermorethemodelcardsneedtoevolvetoincludeboththeintentas\nwellasthescopeofthemodel,inordertoensurethatreportersunderstandthe\npurposeofthemodelbeforereportingsafetyflaws.\n● Cleardistinctionbetweensecurityandsafetyissues: Currentlythereisno\ndistinctionbetweensecurityandsafetyissuesformostreportersorresearchers.\nWhenreportersandresearchersdiscoversafetyhazardsorsecurityvulnerabilities\nwithanAIsystem,they’lllookforacentralreportingmechanismtosubmitthereport\nto.Sincethesesuspectedissuesmaynotbeimmediatelydistinguishableassecurity\norsafetyimpacting,thetriageandimpactassignment(securityversussafety)may\nfalltothemodel-makerorproducertoascertain,unlessthereportercandistinctly\nidentifythesourceoftheissue,suchaswithsafetyhazardsinscopeofthemodel\ncard.SafetyandSecurityreportingmechanismsmayneedtocombinenecessary\nreportingfieldstoenabletriagingtooccurforassignmentandvalidationbyanAI\nSafetyorAISecurityteam,asappropriate.\n● Modelmakerrecognitionofsafetyhazards: Mostmodelmakersclearly\nadvertisewhatissuestheywouldconsidertobeinthescopeforsecurityflaws52,\nthereisoftenlittleornodiscussionaboutsafetyissuesandhowthemodelmakers\nintendtotreatsafetyreportsaroundthem.Settingclearexpectationsabouthandling\nsafetyissueswouldhelpboththemodelusersandreporters.\n● Standardizationofsafetyevaluations:Tofacilitateeasierandmoremeaningful\ncomparisonsbetweenAImodels,weproposeestablishingastandardized\nspecificationforsafetyevaluations.Currently,modelmakersusevariousbenchmarks\nandsafetyleaderboardstoevaluatetheirmodels,whichmakesitchallengingto\ncompareonemodelwithanotherduetothelackofuniformcriteria.Moreover,these\n50https://mlcommons.org/working-groups/airr/ai-risk-and-reliability/\n51https://www.coalitionforsecureai.org/\n52https://www.anthropic.com/responsible-disclosure-policy\nleaderboardsoftenlackindependentverifiabilityandreproducibilityagainst\nestablishedframeworksorbenchmarks.\nBydefiningtheminimumrequiredfieldsandcriteriathatasafetyevaluationmust\ncover,wecanenableconsistent,transparent,andreproduciblesafetyassessments\nacrossdifferentAImodels.Thisstandardizationwouldhelpinindependentlyverifying\nresultsandensurethatsafetyevaluationsarecomprehensiveandcomparable.\nConclusion\nModelsreleasedpubliclytothecommunityanddevelopedaccordingtoopensource\nprinciplescouldplayasignificantroleinthefutureofAImodels.Frameworksandtools\nnecessaryfordevelopingandmanagingmodelsagainstindustryandconsumer\nexpectations,requiresopennessforconsistencyandtoprovideequitablecontentof\nconsiderationinmakingorganizationalriskdecisions.Byincreasingthetransparencyand\naccesstoelementsthatarecriticaltothefunctionalityandoutputcontentofthemodels\n(suchasthesourcedata),thegreaterindustry’sabilitytodiscover,identify,track,and\nresolvehazardsandvulnerabilitiesbeforetheyhavewidespreadimpact.Aswe’veseenwith\nthewidespreadinclusionofopensourceinsoftware,wecanreasonablyexpectthefutureof\nAImodelstofollowasimilarpath.Whilethemajorityofcurrenteffortsmadebymodel\nmakersareconcentratedonsecurityfeatures,safetyisanequallyimportantaspectfor\nresponsible,ethicalandtrustworthyuseofthesemodels.Theproposalspresentedinthis\npaperareintendedtoaffordflexibilitythroughconsistentmeansofgovernance,workflows,\nandstructurethathavebeeninusewithinthesecuritycommunityforyears.When\nimplemented,theymayprovideashortcutwithoutcompromisetoresolvethepressingneed\nofmanagingAISafetyeffectivelyandrapidly.\nLastly,whileseveraldisjointedeffortsexistcurrently,thereisaneedforcollaboration\nbetweenmodelsproducers,consumers,legislativebodies,andlawenforcementagenciesto\nstreamlinethisprocess.\nReferences\nGenerativeAdversarialNetworks,IanJ.Goodfellow,JeanPouget-Abadie,MehdiMirza,Bing\nXu,DavidWarde-Farley,SherjilOzair,AaronCourville,YoshuaBengio.\nhttps://arxiv.org/abs/1406.2661,2014\nAttentionIsAllYouNeed,AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,\nLlionJones,AidanN.Gomez,LukaszKaiser,IlliaPolosukhin.\nhttps://arxiv.org/abs/1706.03762,2017\nImprovingLanguageUnderstandingbyGenerativePre-TrainingAlecRadford,Karthik\nNarasimhan,TimSalimans, IlyaSutskever.\nhttps://cdn.openai.com/research-covers/language-unsupervised/language_understanding_\npaper.pdf\nDeepUnsupervisedLearningusingNonequilibriumThermodynamics,Jascha\nSohl-Dickstein,EricA.Weiss,NiruMaheswaranathan,SuryaGanguli.\nhttps://arxiv.org/abs/1503.03585,2015\nConcreteproblemsinAISafety, Amodei,D.,Olah,C.,Steinhardt,J.,Christiano,P.,\nSchulman,J.,&Mané,D.(2016)..arXivpreprintarXiv:1606.06565.\nDefinitions\nAIEthics:AfieldofAIthatisfocusedonevaluatingthefairness,bias,governance,andresponsible\ncreation,distribution,anduseofAI.ItisconsideredasubsetofAITrustworthiness.\nAImodel:AnAImodelisasoftwareframeworkthatlearnsfromdatatomakepredictionsor\ndecisions,simulatingaspectsofhumancognitiveprocesses.Source:GENAICommons53\nAISecurity:ThefieldofsecurityfocusedonprotectingandsecuringAI,AISystems,andAI\nworkloads.Itcoversthesecurityof:dataandthemodelwastrainedon,thesupplychainoftheAI\nModel,securitycapabilitiesthatsupporttheoperationalsecurityofmodelssuchaspromptinjection\nprotectionanddataexfiltrationdetection,andotherrelatedareas.\nAISafety:ThefieldofstudyandpracticestoensureAIsystemsoperateinamannerthatissafe\nandalignedwithhumanvalues,preventingharmtoindividualsandsociety.Itisconsideredasubset\nofAITrustworthiness.Source:ModifiedforRedHatfromGENAICommons\nAIsystem:LLMsandeverythingsupportlibrariesandsoftware,forexamplepytorch+inference\nsoftware.RHELAIisconsideredanAIsystem,asitispackagedtoincludethenecessary\ncomponentsandsoftwareforuse.\nAITrustworthiness:Thecollectionofcharacteristics,measurements,andcorresponding\nverificationsregardingtheexplainability,reliability,andsafetyofAIanditsuseinreal-world\napplications.ItconsidersthedesignandoperationofAItoensureitisethicallysound,legally\ncompliant,andreliable.Typicallyencompassingprinciplesoftransparency,fairness,and\naccountability.AITrustworthinessisfocusedonthe“what”-generatingsufficientinformationto\nestablishtrustinamodelagainstexpectations.Source:ModifiedforRedHatfromGENAICommons\nEmbargoedsecurityflaws: Embargoedsecurityflawsarevulnerabilitiesorweaknessesin\nsoftware,hardware,orsystemsthathavebeenidentifiedbutarenotyetpubliclydisclosed.\nInformationabouttheseflawsistemporarilyrestrictedtoalimitedgroupoftrusted\nparties—suchasdevelopers,securityteams,andaffectedvendors—underanagreementnot\ntosharedetailsexternallyuntilaspecifieddateorevent.Thepurposeoftheembargoisto\n53https://genaicommons.org/glossary/ai-model/\nallowtimeforpatchesorfixestobedeveloped,tested,anddistributedbeforethe\nvulnerabilitybecomeswidelyknown.\nLLMGuardrails:Guardrailsarethesetofsecurityandsafetycontrolsthatmonitoranddictatea\nuser'sinteractionwithaLLMapplication.Theyareasetofprogrammable,rule-basedsystemsthat\nsitinbetweenusersandfoundationalmodelsinordertomakesuretheAImodelisoperating\nbetweendefinedprinciplesinanorganization.\nModelCards:Amodelcardisatypeofdocumentationthatiscreatedfor,andprovided\nwith,machinelearningmodels.Amodelcardfunctionsasatypeofdatasheet,similarin\nprincipletotheconsumersafetylabels,foodnutritionallabels,amaterialsafetydatasheet\norproductspecsheets.\nResponsibleAI: thedevelopmentanduseofAIinawaythatisethical,transparent,and\naccountable.ResponsibleAIinvolvesensuringthatAIsystemsaredesignedwithconsiderationfor\ntheirpotentialimpactonindividualsandsociety.ItemphasizestheimportanceofcreatingAI\nsystemsthatarenon-discriminatory,fair,privacy-respecting,andsecure.Italsoinvolvesadheringto\nregulationsandguidelines,engaginginethicaldecision-makingprocesses,andensuringthatAI\ntechnologiesbenefithumanityasawhole.ResponsibleAIisfocusedonthe“how”ofAI-howitwas\ndevelopedandusedagainstegalitarianprinciplesforhumanbenefit,whereastheAItrustworthiness\nisfocusedmoreonthe“what”.Source:ModifiedforRedHatfromGENAICommons\nSecurityWeakness:Aweaknessisaconditioninasoftware,firmware,hardware,orservice\ncomponentthat,undertherightcircumstances,couldcontributetotheintroductionof\nvulnerabilities.\nSafetyHazard:Anunexpectedbehaviororoutputoutsideofthedefinedintentandscopeofa\nsystem’sorsoftware’sdesign.Safetyhazardsarelinkedwithproducingharmfulcontentoroutputs\nthatcancausesocial,economic,andenvironmentalharmtoconsumersandusers.\nSustainableAI:thedevelopmentanduseofcarbonneutralandcarbonnegativepracticesto\nminimizethenegativeenvironmentalimpactsofAItechnologies.Whileconsidereditsown\nindependentdomain,theecologicalandcorrespondinghumanharmthatresultsfromunsustainable\nAIdevelopmentandusealignswithResponsibleAI–directlyinfluencingthedurationofbenefitsfor\nhumansagainstenvironmentalrealities(akintocorporatesocialresponsibilityfocuses).Itisrelated\ntoAIEthics.",
    "pdf_filename": "Building_Trust_Foundations_of_Security,_Safety_and_Transparency_in_AI.pdf"
}