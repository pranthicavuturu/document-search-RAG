{
    "title": "Building Trust Foundations of Security, Safety and Transparency in AI",
    "abstract": "This paper explores the rapidly evolving ecosystem of publicly available AI models, and their potential implications on the security and safety landscape. As AI models become increasingly prevalent, understanding their potential risks and vulnerabilities is crucial. We review the current security and safety scenarios while highlighting challenges such as tracking issues, remediation, and the apparent absence of AI model lifecycle and ownership processes. Comprehensive strategies to enhance security and safety for both model developers and end-users are proposed. This paper aims to provide some of the foundational pieces for more standardized security, safety, and transparency in the development and operation of AI models and the larger open ecosystems and communities forming around them.",
    "body": "Building Trust: Foundations of Security,\nSafety and Transparency in AI\nHuzaifa Sidhpurwala\nhuzaifas@redhat.com\nGarth Mollett\ngmollett@redhat.com\nEmily Fox\nefox@redhat.com\nMark Bestavros\nmbestavr@redhat.com\nHuamin Chen\nhchen@redhat.com\nAbstract\nThis paper explores the rapidly evolving ecosystem of publicly available AI models, and their\npotential implications on the security and safety landscape. As AI models become\nincreasingly prevalent, understanding their potential risks and vulnerabilities is crucial. We\nreview the current security and safety scenarios while highlighting challenges such as\ntracking issues, remediation, and the apparent absence of AI model lifecycle and ownership\nprocesses. Comprehensive strategies to enhance security and safety for both model\ndevelopers and end-users are proposed. This paper aims to provide some of the\nfoundational pieces for more standardized security, safety, and transparency in the\ndevelopment and operation of AI models and the larger open ecosystems and communities\nforming around them.\nKeywords\nPublic models, Security, Safety, AI, LLM, Generative AI, AI Ecosystem\n\nIntroduction\nArtificial intelligence and Generative AI\nGenerative AI, a branch of artificial intelligence focused on AI production of content such as\ntext, images and video, has seen significant advancements since the introduction of\ngenerative adversarial networks (GANs) in 2014 (Goodfellow et al., 2014), which improved\ndata generation but faced issues like training instability. The development of transformers\nand self attention mechanisms in 2017 (Vaswani et al., 2017) facilitated further improvements\nin natural language processing, leading to large language models (LLMs) like GPT (Radford\net al., 2018) with highly advanced text generation capabilities. Diffusion models\n(Sohl-Dickstein et al., 2015) have also seen rapid advancement in image and video\ngeneration.\nThis rapid advancement in technology capability has been matched by an equally rapid\nuptake in adoption. Forbes predicts the AI market will reach a staggering $407 billion by\n20271,. Additionally, organizations looking to tap into this high-growth market are driving\nrapid development and innovation in this space to achieve competitive advantages,\nstreamline operations, and enhance customer engagement in an increasingly digital\nlandscape, particularly focusing on the development of models and model systems that\naddress emerging use cases in sectors like manufacturing, service operations, and marketing\nand sales2, promising cost savings as a key benefit.\n2 https://indatalabs.com/blog/ai-cost-reduction\n1 https://www.forbes.com/advisor/business/ai-statistics/\n\nImage courtesy of https://www.emarketer.com\nAs with any new technology, it is worth noting that the industry is still identifying new and\nvaluable uses for AI and these market predictions may fluctuate as use cases are tested in\nreal world environments with real world problems. Other factors such as the market demand\nfor GPUs outpacing the current supply and potential market control concerns by regulators,\nmay impact these projections.\n\nThere are various terminologies and efforts3 in the industry to define open source models.4\nHowever a discussion of these efforts and research5 in this field is beyond the scope of this\npaper. For the purpose of clarity we shall be using the term public model, for a model which\nis publicly available for download and use.\nLLMs: data versus code\nLLMs are the next evolution of data science, a field focused on math and data. Unlike\ntraditional systems and applications which rely on logic and programming for a specified\noutcome, large language model development typically consists of architecture research and\ndesign, which is then coded. In most cases, PyTorch or any of the other deep learning\nlibraries are used, followed by training in multiple stages. Post-training, model weights are\nsaved in a file that contains billions of, most commonly, 16-bit floating point numbers (or low\nprecision data times to reduce computational and memory costs by using quantized\nmodels). These weights are then shipped to consumers of LLMs, along with configuration\nfiles and other metadata. Therefore in their very simplest form, these AI models are mostly\ndata with some code as compared to traditional systems, applications, packages and\nlibraries, such as Python or binary executable files.\nPopular model hosting services, like Hugging Face6 host models in the “safetensors” format,\nallowing parameters to be stored for safe load7 and operate.8\nWhen the model is used for inference, such as generating output, these weights are loaded\nby previously used deep learning libraries; PyTorch is the example in this case. This process is\nalso referred to as \"operationalizing a machine learning model\" or \"putting a machine\nlearning model into production.”\nLLMs present a new combination of considerations, risks, and concerns not seen in industry\nto date. In addition to the application of existing security constructs (vulnerabilities and\ncontrols) the data and intended use of LLMs bring safety concepts historically reserved for\ncontent management and social media platforms to the forefront of risk discussions. This\ndistinction in both their development and operation, as compared to traditional systems and\napplications, imply special attention and evaluation must be taken in proposing an industry\nwide approach to the security and safety of LLMs.\n8 https://github.com/huggingface/transformers/tree/main/src/transformers/models\n7 https://huggingface.co/docs/transformers/en/main_classes/model\n6 https://huggingface.co/models\n5 https://www.ibm.com/think/topics/open-source-llms\n4 https://opensource.org/deepdive/drafts/open-source-ai-definition-draft-v-0-0-9\n3 https://www.redhat.com/en/topics/ai/open-source-llm\n\nHow AI Security is different from AI Safety\nAI Security and AI Safety are interrelated yet distinct aspects of managing risks in artificial\nintelligence systems. AI Security primarily focuses on protecting AI systems from technical\nthreats (both internal as well as external), unauthorized access, and attacks, which traditional\ninformation technology security typically covers. For models, this encompasses safeguarding\ndata integrity, ensuring confidentiality of the data generated by these models, and\nmaintaining the availability of AI services and environments for training and tuning. It involves\nimplementing robust security measures to prevent adversarial attacks, data breaches, and\nother forms of exploitation that could compromise the functionality or integrity of the AI\nmodels and the underlying infrastructure.\nAI Safety is typically concerned with ensuring that AI systems, LLMs, and the supporting\nlibraries and software like PyTorch and its inference software, and their data lifecycle\npractices ensure the intended operation, as defined, does not cause unintentional harm to\nusers, society, or the environment9. Alignment with human-defined values and accepted\noutputs to mitigate this harm involves addressing issues such as algorithmic biases, ensuring\nreliable and predictable behavior, and designing fail-safes to mitigate potential harm,\nnegative consequence, and any potential risks presented. While AI Security protects the\nsystem from external and insider threats, AI Safety ensures that the system and the data do\nnot pose a threat or create harm through its operation due to its development, training, and\nuse (intended or not).\nIn the realm of generative AI models, the line between security and safety issues often blurs\nsignificantly. For instance, a prompt injection attack, typically considered a security concern,\ncan lead to safety issues such as the model producing toxic or harmful content, or exposing\npersonal information such as PII and PHI. This intersection highlights the critical need for a\ncomprehensive approach to AI risk management that addresses both security and safety\nconcerns in tandem.\n9 Sustainable AI is often considered its own independent domain as the impacts of AI on the\nenvironment are prolonged and often unable to forecast due to additional factors. It's reasonable to\nstate, however, that the resulting impact is cumulative given current studies for traditional CPU usage.\nTherefore the authors consider Sustainable AI as a subset of AI Safety in that the ecological and\ncorresponding human harm that results from unsustainable AI development and usage substantially\ninfluences the duration of its benefits for humans against environmental realities of changing climate.\n\nBoth aspects are crucial for the responsible development and deployment of AI\ntechnologies, and their interconnected nature underscores the importance of a holistic\napproach to AI risk mitigation.\nEffectively navigating AI Security and safety issues\nIt is essential that organizations developing or using AI establish appropriate processes and\nworkflows that address the distinct aspects and considerations necessary for AI Security and\nfor AI Safety within an overall AI Risk Management function. While there are areas of reuse\nand adaptation that are relative between these domains, the overall intent and outcome of\neffectively managing AI Security and AI Safety is to reduce the risk and subsequent impact\nof both on an organization, users, and society at large. Therefore, it is recommended that\ndistinct yet parallel structures be established that allow for concentrated management of\nissues in each domain enabling areas of collaboration and corroboration between those\nareas.\nIn the context of AI Safety and AI Security, this adds another dimension to the fact that\nsecure models may facilitate more safe models (through transparency, metadata, and other\nsecurity requirements on integrity and confidentiality), just as Safe models, by nature of their\npractice, could be secure (applying best practices of safety - output verification, etc.) the\nprobability of a safe and responsible model not having some semblance of security is low\ngiven the focus of \"doing the right thing\" which is instrumental to achieving safety outcomes.\nObjectives, methodologies, and interdisciplinary expertise\nAI Security focuses on protecting AI systems from external threats to confidentiality,\nintegrity, and availability, while AI Safety deals with assuring AI behaves as intended and does\nnot cause harm. These different objectives may require distinct methods for discovery,\ntriage, remediation, and management reflective of their core focuses, desired outcomes, and\ninherent expertise required to execute and reason about those issues; suspected and\nreported.\nHowever, it may occur that the exploitation of a security vulnerability allows for the model’s\nalignment and value learning to be compromised, thereby resulting in real safety hazards.\nThis fundamental difference in approach but interconnected impact necessitates separate\nconsiderations and specialized research efforts, or perhaps as the authors suggest, exploring\nshared regimes and research which favor both security and safety for well orchestrated and\nholistically-managed risk.\n\nAs with objectives and methodologies, AI Security and AI Safety require different sets of\ntechnical skills and interdisciplinary collaborations. The \"Concrete Problems in AI Safety\"\npaper highlights that AI Safety issues often involve complex problems at the intersection of\ncomputer science, philosophy, and cognitive science (Amodei, D., Olah, C., Steinhardt, J.,\nChristiano, P., Schulman, J., & Mané, D. (2016).\nThese skills historically have not been present within the Security community, but do exist in\ntrust and privacy teams for content platforms and social media organizations. In contrast, AI\nSecurity issues typically span multiple domains requiring specialized expertise in adversarial\nmachine learning and secure model development as well as more traditional security\nconcepts like defensive cryptography, network security, identity and access management,\nand vulnerability management. This difference in technical complexity and interdisciplinary\nrequirements underscores the need to ensure the correct expertise is available and applied\nto risk mitigation and remediation for each under their respective focus that may be\nexchanged and re-leveraged as part of an AI risk management program.\nTemporal considerations\nAI Security vulnerabilities can be considered to be static because they are already present\nand simply require discovery and exploitation to be actualized. They exist at the time they\nare coded into the software or model, and only require exploitation in the context of the\noperational or runtime environment for the threat to be realized, requiring immediate and\nrapid response upon discovery and reporting to reduce impact and mitigate damage. In\neffect, this is no different than security vulnerabilities in software.\nSafety hazards are dynamic, in that, as the model operates, its output may stabilize or\nfluctuate against societal considerations and evolution. As such, they are measured on a\nspectrum that may evolve. Safety hazards may be realized at time of release or years later –\nmimicking the experience of a logic bomb, with the core difference that they are not coded\nin, rather derived from their training and inference as expectations and understanding of\nsafety evolve. Therefore, safety hazards tend to be more long-term in nature.\nJust as the facets of a vulnerability (i.e. maturity, impact, complexity, etc.) determine where\nthe measure of criticality falls on a scale, most safety hazards are measured on a similar yet\nbracketed spectrum which requires understanding not only of the hazard, but of any shift in\nhuman-value alignment and societal expectations since trained or inferred at the time of use\nor harm. Another important dimension is the multilingual abilities of an LLM and the\nchallenge of safety around this.10\n10 https://aclanthology.org/2024.findings-acl.349.pdf\n\nAdding complexity to safety hazard triaging efforts, some generated output may be\noffensive to some groups of a given society while not for other groups within the same or in a\ndifferent society or at a different time (as society and culture evolves in their understanding\nand tolerances).\nThese temporal considerations are one of the more distinguishing traits between AI Security\nand AI Safety underscoring the pressing need for safety hazard specific management, triage,\nand resolution processes and workflows. These should be adapted and modified from\nwell-known vulnerability management processes and techniques for inclusion in a\ncomprehensive AI Risk management program that encompasses both AI Security and AI\nSafety.\nStakeholder involvement\nAI Security vulnerabilities typically involve a narrower range of stakeholders, primarily\nfocused on cybersecurity experts, AI developers, and organizations deploying AI systems.\nComparatively, safety hazards require broader societal involvement, including ethicists,\npolicymakers, and the general public. This difference in stakeholder composition and\nengagement processes supports the need for distinct resolution and management of safety\nhazards within a comprehensive risk management program.\nRegulatory and governance frameworks\nAI Security issues often fall under existing cybersecurity regulations and compliance\nframeworks, while AI Safety issues will require new or adapted governance structures. While\nboth regulatory and governance landscapes establish a need for different policy approaches,\nsuch focused processes must roll up under AI Risk management for organizations to\neffectively manage comprehensive risk introduced to their business or organization as a\nresult of AI use.\nManaging AI Security and AI Safety separately will result in unidentified risk actualization,\nineffective coordination, and negatively impact organizations across their bottom line, brand,\nand market space, eroding or demolishing trust in one way or another, regardless of the\ninnovation employed in constructing such governance. Frameworks such as NIST’s AI Risk\nManagement Framework11 (RMF) are a good start but lack sufficient detail in pulling AI\nSecurity and AI Safety processes and workflows together to demonstrate how each\ninfluences the overall risk AI presents. Here, the application of existing risk management\nframeworks fail to consider the complexity of unique challenges AI presents. While NIST’s AI\nRMF includes multiple facets of consideration, such as intellectual property which do present\nrisk but solely in a business context, they do not necessarily consider the externality of that\n11 https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf\n\nrisk in operation as occurs with AI Security and AI Safety. Regulations such as the EU AI Act\nprovide more clear outcomes expected of AI risk management driven by categorizing some\nmodel use as High Risk but do not provide direction on how those outcomes may be\nachieved. More importantly, the premise of the act defines High Risk Models as increased\nrisk of “harm to the health, safety or fundamental rights of natural persons” fully focused on\nAI Safety only. This not only creates a greater rift in effectively managing AI Security and AI\nSafety processes to reduce or mitigate risk, it allows for considerable variability in\nimplementation and difficulty in enforcement similar to that which was experienced with\nGDPR. It is almost certain that there will be considerable effort and time spent in\ninterpreting the Act into engineering features, checks, and tooling for models in order for\nenforcement to be feasible, and even then will be limited to coverage of AI Safety only.\nCurrent industry trend on handling security and safety issues\nWhile the AI industry has taken steps to address security and safety issues, several key\nchallenges still need to be addressed. Emerging trends suggest that current efforts may\nneed to target these issues’ underlying causes fully. The following examples highlight areas\nwhere further attention is needed to ensure the effectiveness of AI Security and safety\ninitiatives.\nPrioritizing Speed Over Safety: In the race to develop and deploy AI technologies for\nmarket share, many companies are prioritizing speed to market over thorough safety testing\nand ethical considerations, particularly as frameworks to address these are still under rapid\ndevelopment with a few in proof-of-concept or introduced by other entities12 but not widely\nadopted. This is true of any early, disruptive technology, and AI is no different13. There is\ngenerally wide public acceptance of risks associated with technology that are typically\nconsidered acceptable trade-offs for our dependence upon them. However in the matter of\nArtificial Intelligence, globalization and accessibility of technology, and the impact of digital\ncontent and media on daily life, we collectively lack the years of experience in this nascent\ntechnology to create reputable AI for widespread public and business acceptance of the\nrisks, which as of yet are immeasurable to human safety. As seen with past security\nmovements14, security is often years behind any technology, typically requiring a major\n14\nhttps://www.darkreading.com/vulnerabilities-threats/adapting-post-solarwinds-era-supply-chain-sec\nurity-2024\n13 Reference: Wynne, Brian. (1983). Redefining the issues of Risk and public acceptance, the social\nviability of technology. Butterworth & Co.\n12 https://mlcommons.org/benchmarks/ai-safety/ ,\nhttps://www.congress.gov/bill/117th-congress/house-bill/6580/text , https://www.edsafeai.org/safe ,\n\nincident before the industry begins self-correction. In the matters of AI, it is reasonable to\npredict that in the absence of individuals pushing for governance and risk management, we\nmay yet experience a safety and security critical incident. While new models are being\nintroduced that do prioritize safety over speed to market, lack of consensus on how to\nconvey necessary Safety transparency information makes these challenging to\ncomparatively evaluate. Regardless, the increase in availability of these Safety conscious\nmodels are steps in the right direction.\nInadequate Governance and Self-Regulation: The AI industry has largely relied on\nvoluntary self-regulation and non-binding ethical guidelines, which have proven insufficient\nin addressing serious security and safety concerns. This is not exclusive to AI; looking at the\nhistorical shortcomings of technology self-regulation, many governments are proposing\nlegislation to increase the overall security of technology as can be seen with the EU’s Cyber\nResilience Act (CRA) among others. For AI, we see legislation proposed that doesn’t align\nwith the realities of the technology industry15 or that addresses concerns raised by industry\ngroups to provide meaningful outcomes16. Many corporate AI ethics initiatives lack teeth and\nfail to address structural issues or provide meaningful accountability, being developed\nbespoke to the entity enacting them. The lack of industry consensus – as evidenced by over\n1017 different entities focused on related and overlapping areas of AI Security and AI Safety\nalone  – further prevents regulations from being developed that provides AI developers\nsuccessful implementation and consumer enforcement. The recent veto of SB 104718\nhighlights some, but not all of these challenges.\nRelatively successful self-governance within the tech industry does happen occasionally, and\nmay also be helpful to study. Two notable examples within the security space are the gradual,\nwidespread adoption of HTTPS by default19 and the rapid adoption of signing technologies in\nthe wake of the 2020 SolarWinds software supply chain compromise20. While both\nphenomena were driven by various factors, a strong common link is the availability of\neasy-to-use tooling: with HTTPS, easily-attainable, free certificates from e.g. LetsEncrypt\nhelped drive adoption; with software supply chain, availability of simple signing tools such as\nSigstore helped signing become a simple added step instead of a laborious additional\nprocess. For the purposes of the topic at hand, as tooling for AI Safety matures and\n20 Sigstore Graduates: A Monumental Step Towards Secure Software Supply Chains.\nhttps://openssf.org/blog/2024/03/20/sigstore-graduates-a-monumental-step-towards-secure-soft\nware-supply-chains/\n19 HTTPS encryption on the web: https://transparencyreport.google.com/https/overview\n18 https://www.gov.ca.gov/wp-content/uploads/2024/09/SB-1047-Veto-Message.pdf\n17 OpenSSF, AI Alliance, CNCF, LF AI & Data, CISA, NIST, MITRE, OWASP, CSA, and many others\n16 https://www.eff.org/deeplinks/2024/08/no-fakes-dream-lawyers-nightmare-everyone-else\n15\nhttps://www.gov.ca.gov/2024/09/29/governor-newsom-announces-new-initiatives-to-advance-safe\n-and-responsible-ai-protect-californians/\n\nbecomes more well-known, it’s possible that widespread adoption will promote greater\nsafety as time goes on.\nMore generally, successful industry-driven software security initiatives such as those\nmentioned earlier tend to involve a defined set of best practices that can be implemented\nrelatively independently of primary feature development. This may prove difficult for the AI\nspace: often, advancing the capability of a model may directly impact its safety, and\nconversely, ensuring model safety may involve holding back capability. With the industry\nfocused on rapid advancement, prioritizing safety and security at the expense of capability\nmay be a tradeoff key stakeholders will be unwilling to make.\nInadequate processes for handling flaw and hazard reports, silent fixes,\ndiscouraging reporters: There exists a lack of common methods and processes of\nhandling model flaws reported by users - no such methodology has been proposed or shared\namong model makers and developers to serve as a common reporting framework outside of\nHuggingFace’s “discussions” feature or “report model” flag which lacks sufficient\ndocumentation to independently inspect the reporting process and reasoning for reporting21.\nIt has taken the software industry decades to develop a flawed-yet-functional disclosure and\nreporting system for software vulnerabilities, of which many model makers have little to no\nexposure to or experience with. AI is a technical evolution of data science and machine\nlearning, principally distinct from traditional software engineering and technology\ndevelopment due to its focus on data and math and less on building comprehensive systems\nfor users which has established methodologies for threat modeling, user interaction, and\nsystem security. Most model makers who provide their models for free may have their own\nbespoke processes which are obscured and may be inadequate, or may not have any process\nat all. In those cases, reporting an issue may involve directly reaching out to the model maker,\nwhich can be cumbersome. It is important that reporting mechanisms align to Industry's\nexisting expectations in reporting and management of flaws — regardless if they are unique\nto security or safety as both require the context of the issue’s discovery, disclosure of the\nsteps by which it was found for reproducibility, evidence or indications of impact in order to\nsimply verify the validity of the issue beyond the individual’s claim in reporting. Further, both\nrequire time to reproduce (dependent on the mutations of outputs in use), develop\nmitigation or fixes for, coordination in release, tracking to closure, and potential development\nof regression testing to prevent their further recurrence.\nBased on the above, we propose processes for handling both security issues as well as safety\nhazards for public models in the following parts of the paper.\n21 https://huggingface.co/docs/hub/en/moderation#reporting-a-repository does provide some\ninformation under the Moderation heading of Hub, and launches a public discussion on the topic\nautomatically.\n\nAdapting existing processes to AI Models\nTraditional application security process in brief\nThe Software Development Lifecycle process traditionally involves designing, writing the\nactual application (using some type of programming or scripting language), compiling it if\nrequired, testing, and then finally packaging and shipping the end product to its users.\nThe development processes differ significantly between open source and closed source\nsoftware. Closed source applications operate under specific expectations regarding usage\nand security, dictated by the organization behind them. These expectations often stem from\nestablishing a brand and meeting customer demands for security, reliability, and support. In\ncontrast, open source projects tend to emphasize functionality and the security boundaries\nof the application, relying on a collaborative model where transparency and community\ninvolvement shape the development practices. This difference means that open source\napplications may not face the same pressure to conform to external brand expectations,\nallowing for a focus on innovative features and flexibility in meeting user needs.\nDespite these distinctions, there are universal industry expectations for the management of\nsecurity vulnerabilities that apply to both open and closed source applications. Regardless of\nthe development practices or the perceived robustness of the security measures in place, all\nsoftware must address vulnerabilities comprehensively. The critical nature of security in\nsoftware development means that organizations can face scrutiny and repercussions for\nfailing to manage vulnerabilities effectively. Therefore, understanding and adhering to these\nindustry standards is crucial, as users and stakeholders expect a baseline level of security\nfrom all software, irrespective of its source.\nEffective vulnerability management is essential and must be implemented using\nindustry-accepted processes, regardless of whether the software is open or closed source.\nEach application security flaw is assigned a unique number by a CVE (Common\nVulnerabilities and Exposures) naming authority (CNA) governed by a central, neutral and\nnon-government affiliated organization, often considered a ‘public good’. This organization’s\nCVE program aims for consistency and industry alignment in how to identify, define, and\ncatalog publicly disclosed cybersecurity vulnerabilities in software. The CVE number can be\nuniversally used to track a particular vulnerability to a particular code base, and is referred to\nby vendors, researchers and consumers alike. Software projects affected by the vulnerability\nare able to contest this assignment, and there is scope for rejection of the CVE if warranted.\nOften other types of metadata like CVSS (Common Vulnerability Scoring System) score and\nCWE (Common Weakness Enumeration) numbers are assigned during the security flaw\n\ntriage process. They help in identifying the severity impact and applicability of the flaw,\nwithout necessarily studying the flaw in detail. They also help automated tools in processing\nflaws and assist security scanners.\nSecurity scanners typically use CVE identifiers and CVSS scores to automatically identify\nand assess known vulnerabilities by matching detected issues against vulnerability\ndatabases. The CVSS scores provide standardized severity metrics, allowing scanners to\nprioritize flaws without detailed manual analysis. This enables efficient processing and\nremediation of security flaws, assists in determining their applicability to specific systems,\nand enhances reporting and compliance efforts through seamless integration with other\nsecurity tools.\nGiven the dynamic nature of models and their inherent differences to traditional systems\nand application development, a “lift and shift” approach to security or safety management is\nnot practical. However, there are valuable elements of existing security management\nprocesses that are well-tested, understood, and invested in across industry that could be\napplied to AI models allowing for appropriate security, safety, and risk management\nScope of AI Security flaws\nSecurity flaws, much like the skills and expertise required to manage them, exist differently\nthan safety, bias, ethics or any other type of issues. In order to appropriately manage and\nresolve these for AI, we propose the following definition of an AI Security vulnerability.\nAI Security vulnerability: A flaw in an AI system, resulting from a weakness22 that can be\nexploited, causing a negative impact on the confidentiality, integrity or availability of the\naffected component or components.\nNote: Here the word AI system is used instead of an AI model, because AI flaws tend to\nmanifest themselves in the way the models are used. For example, in the case of a\nmulti-modal model, it may generate unsafe images for some prompts, but if these models\nare never used by the application to generate images, but rather used for some other\npurpose, the flaw has not manifested into a vulnerability in that system.\nWhile there are many similarities to security vulnerabilities, certain nuances only exist for AI\nmodels as a result of their nature and how they work, described below.\n22 A security weakness is a condition (i.e. flaw, error) or characteristic (i.e. improper configuration or\nweak encryption that permits the application or systems to have increased vulnerability to attack. See\nalso https://nvd.nist.gov/vuln\n\n●\nLoss of Confidentiality: When models respond with information they should not\nreveal as mentioned in its model card or other documents there is a loss of\nconfidentiality. This includes but is not limited to:\n○\nUnauthorized PII and Intellectual property information.\n○\nOutput that can cause widespread damage where the information is not\ngenerally available on the internet or considered public.\n●\nLoss of Integrity: This issue can occur when an attacker can negatively influence\nthe data within the model or is able to poison the model.\n○\nAdversarial fine-tuning can cause malicious changes to the model weights,\nand result in wrong or specially-crafted output generated by the model.\n●\nLoss of Availability: The attacker is able to successfully perform a denial of service\nattack (DoS)23 on the model24 and stop all inference attempts. In most cases, this is\nconsidered a software issue rather than an AI model issue.\nThere are circumstances whereby the loss of confidentiality or integrity may result in human,\nsocial, or environmental harm, however this may only be ascertained in the context of the\nmodel’s operational use. In such cases, the importance of information exchange between AI\nSecurity and AI Safety teams is paramount for fully exploring the impact of an exploited AI\nvulnerability so additional safeguards and security protections may be applied to protect\nagainst future occurrences of such incidents.\nAI Security vulnerabilities do not take into account the platform on which the model is run, or\nthe support libraries which are used to load, run and train the model. Security issues in those\nsupport infrastructures are already covered by the existing CVE process detailed above.\nScope of AI Safety flaws\nAI Safety is an interdisciplinary field focused on preventing accidents, misuse, or other\nharmful consequences arising from data generated from a generative AI model.\nAs discussed earlier, AI model safety is a novel concept as safety is not commonplace within\nthe software development and security field, however there are a few noteworthy\nexceptions. Medical Device Safety and Automotive Safety are well established within their\nindustry with their own considerations for regulatory compliance beyond any risk\nmanagement program. ‘Trust and Safety’ or ‘Privacy and Trust’ teams are often aligned with\nbut executed and managed separately from Security resulting in their activities potential\nexclusion from an enterprise risk management program. These concepts have traditionally\nbeen excluded from enterprise risk management because their scope was limited to user\n24\nhttps://portswigger.net/daily-swig/deepsloth-researchers-find-denial-of-service-equivalent-against-\nmachine-learning-systems\n23 https://arxiv.org/abs/2010.02432\n\nprivacy as a protected dataset due to regulatory requirements which today do not include\nthe cognitive or mental impacts in exposure; though this is reflected in legal cases as\n“intangible injury” or non-economic damages for which software has no equivalent risk\nconsideration or quantifiable measurement.\nTechnology producers that store user data, manage user information, or retain user account\ndetails often have to deal with adversarial attacks which target loss of confidentiality or loss\nof integrity. These attacks may include but are not limited to the compromise of user privacy\n(adversarial targeting of mail accounts of journalists), illicit account usage (storing and\nsharing explicit imagery of minors), spread of misinformation, and unauthorized sharing of\nsensitive content (false or true). Today, the technology industry’s closest alignment with\nsafety management is in regulatory duty to protect from harm, physically or cognitively (in\nthe context of social platforms) and not necessarily in evaluating and quantifying the\nresulting impact from exposure of sensitive information, particularly from sources\ninappropriately perceived by the masses as trustworthy. Until recently, technology has been\nviewed as an enablement for harm such as that to occur but only because humans are the\ncause of violations in trust and safety (medical devices and automotive safety still remain the\nexception).\nAI is however the first time in which the technology and its development are the cause of\nviolations in trust and safety, bringing considerations of responsible and ethical AI to the\nforefront of the AI Safety discussion.\nWe propose to define AI Safety issues and flaws as hazards to avoid confusion between\nsecurity flaw and safety flaw. We feel hazard more readily encapsulates the potential for\nharm separate from flaw or issue which are currently well utilized industry terms.\nAI Safety hazard: An unexpected model behavior that is outside of the defined intent and\nscope of the model design. Safety hazards may result in harmful content generation, bias in\ndecision making, or violation of social norms and ethics of groups; the impact and severity of\nwhich will vary greatly from group to group based on their culture, social, ethnic, or anthropic\nsystems. Harm may be categorized by loss of life, injury or other physical or mental health\nimpacts or damage, social and economic disruption or degradation, or some combination\nthereof.\nTaxonomy of AI Safety risks\nAn important part of handling AI Safety flaws is first to define and decide on a safety\ntaxonomy which will be used by an AI model or an AI system. AI Safety taxonomy is a\nstructured classification system that identifies, organizes, and informs different aspects of\nsafety and reliability in artificial intelligence systems and their uses. The taxonomy helps\n\noutline various risks, challenges, and considerations that need to be addressed to prevent\nharm and ensure that AI systems operate as intended.\nThere is no shortage of AI Safety taxonomies and the research around it in recent years25.\nThis paper does not aim to study all the taxonomies and does not intend to suggest which\none should be used. However, some parameters for choosing a community taxonomy26\nwould be:\n●\nTaxonomy is available under a permissive open content license: This enables model\nmakers and model distributors to use them to evaluate models, regardless of whether\nthe model itself is open or closed.\n●\nTaxonomy is developed in the open and anyone is free to contribute to its\ndevelopment: It does not end up being a research paper, but is in continuous\ndevelopment by the community.\n●\nTaxonomy is extensible: You can use the taxonomy as the main document, and\nextend the taxonomy, adding variations as warranted based on the internal strategy\nof the model makers, provided the core elements of the taxonomy are met.\n●\nA taxonomy does not cause harm. Taxonomies should be careful to not publish\nmodel responses to prompts because, for some hazard categories, these responses\nmay contain content that could enable harm. Equally, unsafe responses could be used\nby technically sophisticated malicious actors to develop ways of bypassing and\nbreaking the safety filters in existing models and applications. It may require some\ntaxonomies to be society or culturally unique.27\n●\nTaxonomies provide sufficient information to be used safely in benchmarking, in such\na way that they are available for easy integration if required.\nChallenges\nTracking safety hazards has been one of the biggest challenges of AI models and systems so\nfar. While there have been several developments in the safety field, such as safety aligned\nmodels that can detect unsafe content, a lack of consensus and understanding in the way\nsafety hazards should be classified, tracked and perhaps even remediated persists which\ndelays effective tracking, coordination, and mitigation of these hazards currently present in\nAI today.\n27 Taxonomies need to be socially or culturally aware. Certain references or expressions may be\nacceptable in one society or culture, but at the same time may be offensive to others.\n26 A community taxonomy is community-driven by the diverse members of the AI community helping\nto build and deliver safer and accurate AI systems.\n25 https://airisk.mit.edu/\n\nHandling AI Security and AI Safety\nAdapting vulnerability disclosure and coordination for addressing AI Security flaws\nWe are proposing the following adaptations for tracking AI related security flaws:\n●\nModel providers should have a mechanism for reporting issues in a confidential\nmanner.\n●\nReporters should disclose the security issue and all necessary details first and\nforemost to the model provider.\n●\nModel providers have the primary responsibility to triage new reports and follow an\nestablished and public vulnerability disclosure process or policy. This is not without\nadditional challenges28.\n●\nIf the model provider is a CNA29 and they recognize this as a security issue, then they\nalone are responsible for assigning a CVE ID to this issue. If the model provider is not\na CNA, then the CVE id can be requested directly from the CVE program partner.30\n●\nIf the model provider does not recognise the issue as security related or if the model\nprovider denies assigning a CVE, the reporter may raise a formal request with the\nCVE Program by following the instructions outlined on their site.31\n●\nModel providers issue VEX statements to convey known vulnerability presence (or\nlack thereof) in models.\nIt should be noted that many of the current challenges that exist in the traditional\nvulnerability disclosure and coordination process have parallel challenges in AI Security.\nWhile models may publicly declare their scope and intent of use, triage must still be\nperformed to ascertain the applicability, reproducibility, and impact of the reported flaw\nbefore coordination and remediation steps may be taken.\nSecurity frameworks and their relevance\nTo conclude this topic, we take a brief look at the various AI Security frameworks. These\nframeworks allow organizations to reduce the risk of the AI system and also help them meet\nregulatory requirements specific to the organization's industry vertical. It is usual for the\nindustry to layer technical frameworks with those which are more conceptual.\nSome of these frameworks are high level, some help to visualize the threat landscape, some\nof them help to meet the local regulatory requirements. In the end one would consider the\nproposed security handling procedure in this paper as a low level requirement/procedure,\n31 https://www.cve.org/ReportRequest/ReportRequestForNonCNAs#RequestCVEID\n30 https://www.cve.org/PartnerInformation/ListofPartners\n29 https://www.cve.org/ProgramOrganization/CNAs\n28 https://www.cve.org/Media/News/item/blog/2024/07/09/CVE-and-AIrelated-Vulnerabilities\n\nwhich could very well be a subset of any of the above frameworks. We summarize these\nframeworks as follows:\nFrameworks for Developers and Practitioners:\n- Focus: Practical security implementation, threat modeling, and vulnerability management.\n- Outcome: Secure AI models, prevent data breaches, and address risks in AI applications\nusing tools like OWASP32 and MITRE ATLAS.33\nFrameworks for Chief Information Security Officers (CISOs):\n- Focus: Integration of AI Security into broader risk management.\n- Outcome: Align AI Security practices with existing cybersecurity programs and ensure\ncompliance with regulatory standards (e.g., NIST AI RMF34, Google SAIF35).\nLegislature specific to a geographic area:\n- Examples: EU AI Act36 and Canada’s Artificial Intelligence and Data Act (AIDA37).\n- Outcome: Classify AI systems based on risk levels and provide governance rules to ensure\nsafe, ethical use of AI, affecting compliance and legal responsibilities.\nCurrent tracking efforts in the security community\nThere are several efforts ongoing in the security community in line with the above to\nstreamline the process. Notable among them are the efforts by others for CVE/CWE for AI.38\nProposal for hazards disclosure and management\nHeavily drawing on the prior work done by Sven Cattell et al39 This proposal is built up of two\nmain components.\n39 https://arxiv.org/pdf/2402.07039\n38 https://github.com/CWE-CAPEC/AI-Working-Group\n37\nhttps://www.smartinsights.com/traffic-building-strategy/offer-and-message-development/aida-mod\nel/\n36\nhttps://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on\n-artificial-intelligence\n35 https://safety.google/cybersecurity-advancements/saif/\n34 https://www.nist.gov/itl/ai-risk-management-framework\n33 https://atlas.mitre.org/\n32 https://owasp.org/www-project-ai-security-and-privacy-guide/\n\nExtending model/system cards\nModel cards are used to document the possible use of the model, the architecture and\nsometimes the training data used to train the model. Today, they provide an initial set of\nhuman-generated material about the model that is beneficial in assessing its viability of use,\nhowever they have substantially more potential and applicability beyond their current usage.\nAs organizations choose to adopt and build AI systems and capabilities built on top of\nmodels, they’re choosing to do so in environments and with technologies they already have\ndeployed. In order to facilitate better understanding of models wherever they travel or\ndeploy to, we’d like to propose some changes to model cards.\nThe model card should serve as the introductory information about the model. In order for\nadopters and engineers to effectively compare models, we need a consistent set of\nminimum fields and content that must be present in a card. To provide for this, we suggest\nthe development of a specification for model cards. Google’s model card paper40 was a\nnovel first step in expressing this content in a standard way, and since 2018 industry has\nbegun to ask different questions about models that warrant updates to this\nnon-standardized format.\nIn addition to the existing fields recommended from the paper, we propose the following be\nadded or modified:\n●\nIntent and Use: While ‘Intended Use’ should describe the users (who) and use cases\n(what) of the model, it doesn't address how the model is to be used. Expanding\nIntended Use to be a statement for the model system that describes its usage with\nprecise efficacy provides clarity in all three aspects of consideration (who, what, how)\nthat is not present today. For example:\n○\nFrom a text prompt, we produce images that are safe for work, safe for\nchildren, and free of demographic bias\n●\nScope: The purpose of Scope is to exclude known issues that the Model producer\nhas no intent or ability to resolve. While Intended Use should convey use cases that\nare out of scope, providing additional detail on specifics that are out of scope or\nconsideration for resolution provides adopters or consumers of those models more\ncontext to make an informed decision from. This also ensures that reporters of\nhazards understand the purpose of the model before reporting a concern that is\nexplicitly declared as unaddressable against its defined use. For example:\n○\nThis is an LLM with no protections; prompt injections are out of scope.\n●\nEvaluation Data: While an existing field in the model card, since 2018 we’ve\ndeveloped evaluation frameworks41 that focus on different considerations that inform\nadopters and consumers on what the model has achieved. We propose extending\n41 https://mlcommons.org/benchmarks/ai-safety/\n40 https://arxiv.org/pdf/1810.0399\n\nEvaluation Data to provide a nested structure to convey if a framework was also used,\nand the outputs of such evaluation that were run on the model. Standardized safety\nevaluations are preferred and would enable a skilled individual to build a substantially\nequivalent model. Continued research and development to establish new or updated\npublic evaluations should be encouraged.\n●\nGovernance: Governing information about the model is essential to understand how\nan adopter or consumer can engage with the model makers or understand the\nmethodology by which it is produced..\n●\nReferences (optional): Model manufacturers may find including references to be\nbeneficial for potential model consumers in both understanding the model’s\noperation, but also detailing artifacts and other content, such as an AIBOM, safety\naudit42, or security audit, to demonstrate the maturity and professionalism of a given\nmodel.\nSetting these and the existing fields as required elements for a model card allows for\nindustry to begin to establish content that is essential for reasoning, decision-making, and\nreproducibility of models. By rendering model cards in an industry accepted format or\nstandard, we promote the interoperability of models and their metadata across technology\necosystems.\nWe anticipate many organizations will build and develop applications and systems that\nembed, rely upon, and interact with models in an operational capacity. The loss of a model\ncard as referenceable material for policy enforcement at runtime or auditing will exacerbate\nexisting operational challenges in speedy, incident response and resolution. As we’ve seen\nwith the supply chain security movement for software, the value of metadata and\nattestations about what has transpired to produce software is actionable for gatekeeping at\ndeployment and in understanding where risk exists.\nThe data contained in model cards has the potential equivalent function, when stored in an\nwidespread format, such as OC43I, to allow model cards, AIBOMs44, and other evolving\nmetadata types to be pushed, consumed, and leveraged in existing tooling and ecosystems\nwith maturing processes for ingestion, analysis, insights, and policy enforcement.\n44 https://owasp.org/www-project-aibom/\n43 https://opencontainers.org/\n42 Safety audit is a comprehensive report identifying safety hazards, recommendations for remedial\naction and evidence of due diligence for stakeholders and regulators.\n\nCommon flaws and exposures (CFEs) for Hazard tracking\nWhile the Common Vulnerability disclosure mechanism used to track security flaws, is\neffective in traditional software security, its application to AI systems face several\nchallenges:45\n●\nAI models must satisfy statistical validity46 thresholds47This means that any issues or\nproblems identified in an AI model such as biases etc must be measured and\nevaluated against established statistical standards to ensure they are meaningful and\nsignificant.\n●\nConcerns related to trustworthiness and bias often extend beyond the scope of\nsecurity vulnerabilities and may not align with the accepted definition of security\nvulnerabilities.\nRecognizing the above limitation we propose expanding the ecosystem, with a new term\ncalled “Common and Flaws and Exposure (CFE)”, which is analogous to the CVE in the\nsecurity space\nCoordinated Hazard Disclosure and Exposure (committee)\nA central, neutral body must exist to track possible safety hazards in the same way security\nflaws are tracked (as discussed above). Reporters who discover safety issues, are expected\nto coordinate with the model providers to triage and do further analysis of these issues.\nOnce established that the issue is indeed a safety hazard, this body assigns a CFE (Common\nflaws and exposure) number similar to CVE. Model makers and distributors can also request\nCFE numbers to track safety hazards they find in their models.\nThis body is the custodian of CFE numbers. They are responsible for assigning them to\nsafety hazards as per the process described below, for tracking them and if at some point\npublishing them in various forms. We propose a “HEX” format to publish CFE data in the\nsection below.\nAdjunct panel\nIn accordance with the paper by Sven Cattell et al, we would also like to propose an adjunct\npanel to facilitate resolution of safety hazards that are contested:\n47 Criteria or benchmark\n46 Refers to the confidence that the results or conclusions drawn from the model's performance are\nnot due to random chance but are supported by robust statistical evidence.\n45 These details are taken directly from the paper: https://arxiv.org/pdf/2402.07039\n\nIf a reporter submits a safety hazard on statistical or other grounds, a vendor or model maker\nmay reject or contest the safety hazard and the Adjunct Panel is informed. In the course of\nreporting, if the submitter-supplied sample/output pairs are statistically biased, and an\nunbiased sample set would not show a violation of the model card, the adjudicator panel may\nrequest supporting data from the vendor or model maker to validate the rejection. The\nadjudicator and model maker can jointly assess whether the maker-supplied data is too\nsensitive for the submitter and determine on further steps for resolution.\nIn the initial phase of implementation, bootstrapping the Adjunct panel with the Coordinated\nHazard disclosure committee (described above) provides the initial expertise and alignment\nfor future engagement between the two bodies. However it is important for each body to be\nneutral and/or have good representation from major vendors/reporters in the ecosystem.\nAdapting VEX\nThe Vulnerability Exploit eXchange format (VEX)48 is a recent format for conveying the\nexploitability status of a vulnerability in software. While nascent in industry adoption, we\nbelieve this style of information conveyance can be leveraged in the coordinated disclosure\nand necessary resolution of AI Safety hazards.\nWe propose exploration into what a potential Hazards Exposure eXchange (HEX) format\nwould provide the industry in conveying the exposure and resulting impact a safety hazard\nhas on the operational use of AI. The impact of those hazards on intended model use and\noutputs must be tracked to inform consumers how their use is impacted so they may take\nsteps to correct any decisions or harm resulting from an impacted AI system, this includes\nhazards resulting from the development, training, and inference of the model and all its\nvariants.\nMost existing VEX fields and values are adaptable to safety hazards.. Where VEX defines the\nsubcomponent ID, we propose an unique identifier construct (such as a commit) and\ncategory (such as the model lifecycle stage and source) to define how and where the hazard\nis introduced. Status of the hazard is a critical field in providing consumers of models with\nsufficient information to understand how they have been exposed and must consider\ncontent from the model card (intended use) in detailing that status (i.e. affected,\nunaffected, fixed, under investigation) and justification (i.e. model_use_not_approved,\nguardrails_in_place, or tuned_out). Some models provide for multiple uses in which a hazard\nmay only adversely impact a portion of them, it is important to inform model consumers if\n48\nhttps://www.cisa.gov/resources-tools/resources/minimum-requirements-vulnerability-exploitability-e\nxchange-vex\n\ntheir use matches an impacted use and what the specifics of that impact are just as much as\nunderstanding how they may not be affected by the hazard in their use of the model.\nAdditional research is required to develop status justification statements and other potential\nHEX fields that are informed by CFE and Safety Frameworks in order to comprehensively\ndevelop a proposal for HEX acceptable to industry.\nWorkflow\nWe would like to propose a workflow similar to the security workflow above, with several\nmarked differences to address considerations highlighted previously regarding AI Safety\n●\nReporters attempting to report safety issues would do so in accordance with the\nspecified intent and scope in the model card, reflective of safety benchmarking that\nhas occurred. If, for example, the model is not intended for a specific use, has had a\ntopic explicitly excluded from training or is considered beyond the scope of the\nmodel, the safety issue being reported is likely to be closed as invalid by the model\nmaker.\n●\nSafety issues are reported to the model makers, following their established and\ndiscoverable safety reporting process. If they acknowledge they can approach the\nCoordinated Hazard Disclosure committee for a CFE number, which is then used to\ntrack that particular safety issue.\n●\nIf model makers refuse to acknowledge the safety issue, reporters can approach the\nAdjudicator panel, which is responsible for resolving disputes.\n●\nIn the end, the issue is either accepted or rejected. If accepted, the issue is published\nin a public database and the model maker issues a safety advisory pointing to the\npublic record.\nAs the process evolves, several enhancements may be made to extend the safety tracking\nand root causes of the safety hazards to build industry knowledge of prevention, akin to data\nlike CPEs (Common Platform Enumeration) or CWEs.\nCurrent safety efforts\nAt this point authors would like to mention several community efforts currently underway\nfor standardization of processes. Some of them may be orthogonal to each other, but we\nwould like to stress the need to collaborate here, rather than compete. The list below is in no\nway exhaustive, and it's quite possible that new efforts would rise or the ones listed here\nwould become inactive after this paper is published:\n●\nAI Alliance - Trust and Safety group49\n49 https://thealliance.ai/\n\n●\nMLCommons AI Safety group50\n●\nCoalition of Secure AI51\nCurrent challenges in the industry for implementation of this proposal\nSeveral challenges in the current model ecosystem exist and they need to be addressed in\norder to implement open and effective tracking of safety hazards.\n●\nModel/system cards: While most bigger models have explicit model cards,\ncontaining details of how the model has been trained and output of the various\nbenchmarks, for other models, the process is not uniform across the industry and has\nto improve. Furthermore the model cards need to evolve to include both the intent as\nwell as the scope of the model, in order to ensure that reporters understand the\npurpose of the model before reporting safety flaws.\n●\nClear distinction between security and safety issues: Currently there is no\ndistinction between security and safety issues for most reporters or researchers.\nWhen reporters and researchers discover safety hazards or security vulnerabilities\nwith an AI system, they’ll look for a central reporting mechanism to submit the report\nto. Since these suspected issues may not be immediately distinguishable as security\nor safety impacting, the triage and impact assignment (security versus safety) may\nfall to the model-maker or producer to ascertain, unless the reporter can distinctly\nidentify the source of the issue, such as with safety hazards in scope of the model\ncard. Safety and Security reporting mechanisms may need to combine necessary\nreporting fields to enable triaging to occur for assignment and validation by an AI\nSafety or AI Security team, as appropriate.\n●\nModel maker recognition of safety hazards: Most model makers clearly\nadvertise what issues they would consider to be in the scope for security flaws52,\nthere is often little or no discussion about safety issues and how the model makers\nintend to treat safety reports around them. Setting clear expectations about handling\nsafety issues would help both the model users and reporters.\n●\nStandardization of safety evaluations: To facilitate easier and more meaningful\ncomparisons between AI models, we propose establishing a standardized\nspecification for safety evaluations. Currently, model makers use various benchmarks\nand safety leaderboards to evaluate their models, which makes it challenging to\ncompare one model with another due to the lack of uniform criteria. Moreover, these\n52 https://www.anthropic.com/responsible-disclosure-policy\n51 https://www.coalitionforsecureai.org/\n50 https://mlcommons.org/working-groups/airr/ai-risk-and-reliability/\n\nleaderboards often lack independent verifiability and reproducibility against\nestablished frameworks or benchmarks.\nBy defining the minimum required fields and criteria that a safety evaluation must\ncover, we can enable consistent, transparent, and reproducible safety assessments\nacross different AI models. This standardization would help in independently verifying\nresults and ensure that safety evaluations are comprehensive and comparable.\nConclusion\nModels released publicly to the community and developed according to open source\nprinciples could play a significant role in the future of AI models. Frameworks and tools\nnecessary for developing and managing models against industry and consumer\nexpectations, requires openness for consistency and to provide equitable content of\nconsideration in making organizational risk decisions. By increasing the transparency and\naccess to elements that are critical to the functionality and output content of the models\n(such as the source data), the greater industry’s ability to discover, identify, track, and\nresolve hazards and vulnerabilities before they have widespread impact. As we’ve seen with\nthe widespread inclusion of open source in software, we can reasonably expect the future of\nAI models to follow a similar path. While the majority of current efforts made by model\nmakers are concentrated on security features, safety is an equally important aspect for\nresponsible, ethical and trustworthy use of these models. The proposals presented in this\npaper are intended to afford flexibility through consistent means of governance, workflows,\nand structure that have been in use within the security community for years. When\nimplemented, they may provide a shortcut without compromise to resolve the pressing need\nof managing AI Safety effectively and rapidly.\nLastly, while several disjointed efforts exist currently, there is a need for collaboration\nbetween models producers, consumers, legislative bodies, and law enforcement agencies to\nstreamline this process.\nReferences\nGenerative Adversarial Networks, Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio.\nhttps://arxiv.org/abs/1406.2661, 2014\nAttention Is All You Need, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin.\nhttps://arxiv.org/abs/1706.03762, 2017\nImproving Language Understanding by Generative Pre-Training Alec Radford, Karthik\nNarasimhan, Tim Salimans, Ilya Sutskever.\n\nhttps://cdn.openai.com/research-covers/language-unsupervised/language_understanding_\npaper.pdf\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics, Jascha\nSohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli.\nhttps://arxiv.org/abs/1503.03585, 2015\nConcrete problems in AI Safety, Amodei, D., Olah, C., Steinhardt, J., Christiano, P.,\nSchulman, J., & Mané, D. (2016).. arXiv preprint arXiv:1606.06565.\nDefinitions\nAI Ethics: A field of AI that is focused on evaluating the fairness, bias, governance, and responsible\ncreation, distribution, and use of AI. It is considered a subset of AI Trustworthiness.\nAI model: An AI model is a software framework that learns from data to make predictions or\ndecisions, simulating aspects of human cognitive processes. Source: GENAI Commons53\nAI Security: The field of security focused on protecting and securing AI, AI Systems, and AI\nworkloads. It covers the security of: data and the model was trained on, the supply chain of the AI\nModel, security capabilities that support the operational security of models such as prompt injection\nprotection and data exfiltration detection, and other related areas.\nAI Safety: The field of study and practices to ensure AI systems operate in a manner that is safe\nand aligned with human values, preventing harm to individuals and society. It is considered a subset\nof AI Trustworthiness. Source: Modified for Red Hat from GENAI Commons\nAI system: LLMs and everything support libraries and software, for example pytorch + inference\nsoftware. RHEL AI is considered an AI system, as it is packaged to include the necessary\ncomponents and software for use.\nAI Trustworthiness: The collection of characteristics, measurements, and corresponding\nverifications regarding the explainability, reliability, and safety of AI and its use in real-world\napplications. It considers the design and operation of AI to ensure it is ethically sound, legally\ncompliant, and reliable. Typically encompassing principles of transparency, fairness, and\naccountability. AI Trustworthiness is focused on the “what” - generating sufficient information to\nestablish trust in a model against expectations. Source: Modified for Red Hat from GENAI Commons\nEmbargoed security flaws: Embargoed security flaws are vulnerabilities or weaknesses in\nsoftware, hardware, or systems that have been identified but are not yet publicly disclosed.\nInformation about these flaws is temporarily restricted to a limited group of trusted\nparties—such as developers, security teams, and affected vendors—under an agreement not\nto share details externally until a specified date or event. The purpose of the embargo is to\n53 https://genaicommons.org/glossary/ai-model/\n\nallow time for patches or fixes to be developed, tested, and distributed before the\nvulnerability becomes widely known.\nLLM Guardrails: Guardrails are the set of security and safety controls that monitor and dictate a\nuser's interaction with a LLM application. They are a set of programmable, rule-based systems that\nsit in between users and foundational models in order to make sure the AI model is operating\nbetween defined principles in an organization.\nModel Cards: A model card is a type of documentation that is created for, and provided\nwith, machine learning models. A model card functions as a type of data sheet, similar in\nprinciple to the consumer safety labels, food nutritional labels, a material safety data sheet\nor product spec sheets.\nResponsible AI: the development and use of AI in a way that is ethical, transparent, and\naccountable. Responsible AI involves ensuring that AI systems are designed with consideration for\ntheir potential impact on individuals and society. It emphasizes the importance of creating AI\nsystems that are non-discriminatory, fair, privacy-respecting, and secure. It also involves adhering to\nregulations and guidelines, engaging in ethical decision-making processes, and ensuring that AI\ntechnologies benefit humanity as a whole. Responsible AI is focused on the “how” of AI - how it was\ndeveloped and used against egalitarian principles for human benefit, whereas the AI trustworthiness\nis focused more on the “what”. Source: Modified for Red Hat from GENAI Commons\nSecurity Weakness: A weakness is a condition in a software, firmware, hardware, or service\ncomponent that, under the right circumstances, could contribute to the introduction of\nvulnerabilities.\nSafety Hazard: An unexpected behavior or output outside of the defined intent and scope of a\nsystem’s or software’s design. Safety hazards are linked with producing harmful content or outputs\nthat can cause social, economic, and environmental harm to consumers and users.\nSustainable AI: the development and use of carbon neutral and carbon negative practices to\nminimize the negative environmental impacts of AI technologies. While considered its own\nindependent domain, the ecological and corresponding human harm that results from unsustainable\nAI development and use aligns with Responsible AI – directly influencing the duration of benefits for\nhumans against environmental realities (akin to corporate social responsibility focuses). It is related\nto AI Ethics.",
    "pdf_filename": "Building_Trust_Foundations_of_Security,_Safety_and_Transparency_in_AI.pdf"
}