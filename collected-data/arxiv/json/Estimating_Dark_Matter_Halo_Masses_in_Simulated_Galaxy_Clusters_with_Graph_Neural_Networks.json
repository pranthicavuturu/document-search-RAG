{
    "title": "Estimating Dark Matter Halo Masses in Simulated Galaxy Clusters with Graph Neural Networks",
    "context": "Galaxies grow and evolve in dark matter halos. Because dark matter is not visible, galaxies’ halo masses (Mhalo) must be inferred indirectly. We present a graph neural network (GNN) model for predicting Mhalo from stellar mass (M∗) in simulated galaxy clusters using data from the IllustrisTNG simulation suite. Unlike traditional machine learning models like random forests, our GNN captures the information-rich substructure of galaxy clusters by using spatial and kinematic relationships between galaxy neighbour. A GNN model trained on the TNG-Cluster dataset and independently tested on the TNG300 simulation achieves superior predictive performance compared to other baseline models we tested. Future work will extend this approach to different simulations and real observational datasets to further validate the GNN model’s ability to generalise. 1 In the Lambda Cold Dark Matter cosmological model [28, 4], galaxies form and evolve in dark matter halos. Cosmological simulations demonstrate that galaxies grow in tandem with their dark matter halos according to well-measured and tight scaling relations [39]. This interdependence between stellar mass (M∗) and subhalo mass (Mhalo) is known as the stellar–halo mass relation (SHMR). While M∗is observable, Mhalo must often be inferred indirectly via the SHMR due to observational constraints. For example, galaxy clusters—the most massive gravitationally bound objects in the Universe—are dark matter dominated, but their total mass must be measured via gravitational lensing [8, 37], the Sunyaev-Zel’dovich effect [2, 22, 3], and/or visible wavelength proxies (e.g., galaxy richness, intracluster light, etc; [30, 31]). However, these methods are unable to fully leverage galaxy substructure within clusters to estimate their dark matter halo masses. Therefore, we present a graph neural network (GNN) algorithm [32] for predicting Mhalo for galaxies in simulated cluster environments1. Compared to primitive machine learning (ML) methods like random forests [1], a GNN can learn the substructure in neighbouring galaxies and thereby improve halo mass predictions. Our results using the GNN demonstrate significant performance gains on the training, validation, and an independent test set. 1https://github.com/Nikhil0504/halo_masses Machine Learning and the Physical Sciences Workshop, NeurIPS 2024. arXiv:2411.12629v1  [astro-ph.GA]  19 Nov 2024",
    "body": "Estimating Dark Matter Halo Masses in Simulated\nGalaxy Clusters with Graph Neural Networks\nNikhil Garuda\nSteward Observatory\nUniversity of Arizona\n933 N Cherry Ave, Tucson, AZ, 85721\nnikhilgaruda@arizona.edu\nJohn F. Wu\nSpace Telescope Science Institute\n3700 San Martin Dr, Baltimore, MD 21218\njowu@stsci.edu\nDylan Nelson\nUniversität Heidelberg\nZentrum für Astronomie, ITA,\nAlbert-Ueberle-Str. 2\n69120 Heidelberg, Germany\ndnelson@uni-heidelberg.de\nAnnalisa Pillepich\nMax-Planck-Institut für Astronomie\nKönigstuhl 17, 69117 Heidelberg, Germany\npillepich@mpia.de\nAbstract\nGalaxies grow and evolve in dark matter halos. Because dark matter is not visible,\ngalaxies’ halo masses (Mhalo) must be inferred indirectly. We present a graph\nneural network (GNN) model for predicting Mhalo from stellar mass (M∗) in\nsimulated galaxy clusters using data from the IllustrisTNG simulation suite. Unlike\ntraditional machine learning models like random forests, our GNN captures the\ninformation-rich substructure of galaxy clusters by using spatial and kinematic\nrelationships between galaxy neighbour. A GNN model trained on the TNG-Cluster\ndataset and independently tested on the TNG300 simulation achieves superior\npredictive performance compared to other baseline models we tested. Future work\nwill extend this approach to different simulations and real observational datasets to\nfurther validate the GNN model’s ability to generalise.\n1\nIntroduction\nIn the Lambda Cold Dark Matter cosmological model [28, 4], galaxies form and evolve in dark matter\nhalos. Cosmological simulations demonstrate that galaxies grow in tandem with their dark matter\nhalos according to well-measured and tight scaling relations [39]. This interdependence between\nstellar mass (M∗) and subhalo mass (Mhalo) is known as the stellar–halo mass relation (SHMR).\nWhile M∗is observable, Mhalo must often be inferred indirectly via the SHMR due to observational\nconstraints. For example, galaxy clusters—the most massive gravitationally bound objects in the\nUniverse—are dark matter dominated, but their total mass must be measured via gravitational lensing\n[8, 37], the Sunyaev-Zel’dovich effect [2, 22, 3], and/or visible wavelength proxies (e.g., galaxy\nrichness, intracluster light, etc; [30, 31]). However, these methods are unable to fully leverage galaxy\nsubstructure within clusters to estimate their dark matter halo masses.\nTherefore, we present a graph neural network (GNN) algorithm [32] for predicting Mhalo for galaxies\nin simulated cluster environments1. Compared to primitive machine learning (ML) methods like\nrandom forests [1], a GNN can learn the substructure in neighbouring galaxies and thereby improve\nhalo mass predictions. Our results using the GNN demonstrate significant performance gains on the\ntraining, validation, and an independent test set.\n1https://github.com/Nikhil0504/halo_masses\nMachine Learning and the Physical Sciences Workshop, NeurIPS 2024.\narXiv:2411.12629v1  [astro-ph.GA]  19 Nov 2024\n\nFigure 1: Flow diagram of GNN architecture used for halo mass prediction. The GNN processes\nnode features (xi, xj) and edge features (ϵij) through multiple unshared layers, where each layer\napplies learnable functions, ϕ, which are implemented as MLPs. These unshared layers operate\nin parallel across the graph structure. A pooling layer then aggregates (L) the information from\nthese interactions back into each node. Subsequent repetitions of these GNN layers can give it more\nrepresentational power. Finally, the output MLP, ψ, combines node features and aggregated edge\nfeatures to predict each node’s halo mass.\n2\nIllustrisTNG Simulation Data\nThe simulation data we use are large-volume, cosmological, gravo-magnetohydrodynamical sim-\nulations from the IllustrisTNG simulation suite [25]. We specifically use the TNG-Cluster [24]\nsimulation, a collection of zoom-in simulations centered 352 of the most massive halos (i.e., galaxy\nclusters), for training and validation. Our dataset is based on the SUBFIND [35] subhalo catalogs that\nwere obtained from snapshot 99 (z = 0), focusing on the high-resolution components of the zoom-in\nsimulation. We adopt cosmological parameters from [29], using H0 = 67.74 km s−1 Mpc−1 for\nconsistency with the IllustrisTNG simulation suite. Additional details about the TNG-Cluster training\ndata are provided in Appendix A. The distribution of subhalos in TNG-Cluster is shown in Figure 4,\nand the selection criteria and number of samples are described in Table 2.\nWe test our ML models on an independent data set, the Illustris TNG300-1 hydrodynamic simulation\n(hereafter TNG300; [25]). The TNG-Cluster and TNG300 simulations use the same physics and\nhave comparable resolutions (in the former’s zoom-in regions), but the two simulations are otherwise\nindependent. When reporting TNG-Cluster cross-validation results TNG300 test set results, we only\nconsider galaxies within 10 Mpc of all clusters with Mhalo > 1014M⊙.\n3\nMethods/Experiments\nThe primary objective of our study is to estimate Mhalo from M∗. Building on the work of [18], we\ntrain ML models on galaxies and dark matter halos from TNG-Cluster to probe cluster environments.\nLoss Functions and Evaluation Metrics.\nModel performance is assessed using several metrics\n(presented in Table 1). Simple models are trained to minimise the Mean Squared Error (MSE), while\nthe GNN is optimised using Gaussian negative log-likelihood (combining MSE and log-variance\nterms, per [15])2. Validation and test performance are evaluated with Root Mean Squared Error\n(RMSE), Mean Absolute Error (MAE), coefficient of determination R2, Normalised Median Absolute\nDeviation (NMAD), average offset (Bias), and Outlier Fraction (foutlier).\nRandom Forest Baseline Models.\nTo establish a benchmark for subsequent comparisons with our\nGNN model, we use Random Forest (RF) regression [14] as a baseline model due to its capability to\nhandle complex non-linear relationships between features. To further augment the simple RF model,\nwe compute an overdensity parameter (∆G), defined as the sum of stellar masses within a specified\nradius Rmax. The RF models are configured with 100 estimators using scikit-learn [27], one of\nwhich utilises M∗, and one which uses both M∗and ∆G as features.\nGraph Neural Networks.\nIn our GNN model, each node represents a galaxy, with the M∗as\nthe sole node feature. We construct edges between galaxy pairs separated by less than 3 Mpc [41],\nconnecting neighbouring nodes. These connections enable the neural network to learn interactions\nbetween the substructure and galaxy properties within the cluster. We provide two edge features to\nincorporate both the spatial and kinematic separations of galaxies: the squared Euclidean distance\nbetween pairs of galaxy positions, and pairs of relative line-of-sight velocities.\nGNN Architecture.\nOur GNN follows the architecture described in [41] with 8 unshared layers\nand 3 sequential layers, as shown in Figure 1. Each layer is composed of a two-layer MLP with\n2The negative log-likelihood objective accounts for the intrinsically varying scatter in Mhalo.\n2\n\n(a) Predicted versus true Mhalo for the TNG-Cluster\nvalidation set, coloured by distance from cluster center.\n(b) Predicted versus true Mhalo for the TNG300 test\nset, coloured by distance from cluster center.\nFigure 2: For each subfigure, we show results for the RF with only M∗as a feature (left), the RF with\nM∗and overdensity parameter ∆G (center), and the GNN with M∗and graph connectivity (right).\nTable 1: Validation and test set performance for all models. The best metrics are underlined.\nModel\nRMSE\nMAE\nR2\nBias\nfoutlier\nNMAD\nTNG-Cluster cross-validation\n(Always predict mean)\n0.542\n0.396\n0\n0\n0.019\n0.479\nRF: M∗\n0.489±0.002\n0.382±0.003\n0.186±0.011\n−0.067±0.006\n0.008±0.001\n0.463±0.005\nRF: M∗+ ∆G\n0.385±0.002\n0.301±0.002\n0.490±0.007\n−0.124±0.004\n0.008±0.000\n0.367±0.002\nGNN\n0.273±0.010\n0.209±0.009\n0.745±0.019\n−0.085±0.027\n0.013±0.002\n0.246±0.013\nTNG-300 test set\n(Always predict mean)\n0.466\n0.351\n0\n0\n0.021\n0.422\nRF\n0.468±0.009\n0.365±0.014\n0.199±0.030\n−0.200±0.017\n0.009±0.003\n0.456±0.033\nRF: M∗+ ∆G\n0.344±0.003\n0.256±0.003\n0.567±0.007\n−0.048±0.001\n0.022±0.001\n0.293±0.006\nGNN\n0.242±0.013\n0.184±0.010\n0.785±0.023\n−0.039±0.034\n0.014±0.002\n0.217±0.014\nNote: The intrinsic scatter in Mhalo ranges from 0.42 (at log(Mhalo) = 11 M⊙) to 0.33 (at\nlog(Mhalo) = 13 M⊙) dex in TNG-Cluster and 0.48 dex to 0.19 dex in TNG 300 respectively.\n16 hidden channels, SiLU activations [12], and 16 outputs. These operate over edges connected to\neach node, using max pooling to aggregate edge information to each node 3. The node output is\nconcatenated with its initial feature (M∗) and passed through a 3-layer MLP. The GNN predicts two\nquantities [15]: Mhalo and the expected log variance of Mhalo at a given M∗.\nGNN Optimisation.\nWe employ the METIS algorithm to partition the training set into 48 parts\n(see ClusterLoader class in PyTorch Geometric [6, 11]), allowing us to handle large graph datasets\nefficiently. The model is trained with the AdamW optimiser [19] at an initial learning rate of 10−2\nand weight decay of 10−4. A scheduler reduces the learning rate by 0.2 if validation loss stagnates by\n10−3 for 15 epochs. Early stopping occurs after 35 epochs of no improvement, with a maximum of\n300 epochs. On an Nvidia A6000 GPU, training takes 20 minutes and inference takes under 1 second.\n4\nResults\nTable 1 compares model performance for predicting Mhalo from galaxies residing in clusters for\nthe validation and test datasets. We additionally show the scatter of Mhalo in the first row, which\nrepresents the most naive “prediction” of the sample mean. Below, we present the results for the\nbaseline models and GNN model. We display scatter plots of the true versus predicted masses for the\nTNG-Cluster cross-validation data set in Figure 2a and TNG300 test set in Figure 2b.\nThe simplest RF model exhibits high error and very low predictive power.4 When we augment the\nRF model with ∆G, the performance improves, demonstrating that galaxy environments contain vital\ninformation for the SHMR. Nonetheless, the RF models systematically underpredict Mhalo for the\nhighest-mass galaxies and yield high error.\nGNNs greatly outperform RF models, as indicated by the right-most panels of Figures 2a and\n2b. Running the same experiments using XGBoost (which is more prone to overfitting), we find a\nsignificant improvement over RF but not enough to surpass GNNs. We find that the GNN performance\non the training and validation sets translates to accurate predictions on the independent test set. For\nnearly all metrics in Table 1, the GNN outperforms the RF models for cross-validation and test sets.\n3This helps the GNN to effective capture the neighbouring features.\n4In fact, for the TNG300 test set, the simplest RF model produces even higher error than the scatter inherent\nto the data. We ascribe this to the RF model’s significant negative bias (i.e., systematic underprediction).\n3\n\nFigure 3: Validation set RMSE as a function of distance from cluster center. Results shown for the\nGNN (blue) and RF with M∗and ∆G (orange).\n5\nDiscussion\n5.1\nModel performance as a function of local environment\nIn Figure 3, we show the cross-validation RMSE as a function of distance from the cluster center\nfor the GNN and RF (M∗and ∆G) models; the GNN significantly outperforms the RF across all\ndistance bins. Notably, the RF model performance suffers for galaxies closer to the center of the\ncluster. One potential explanation for this discrepancy is that the RF does not account for the dense\ncluster environment, where interactions such as tidal stripping can lead to significant loss of Mhalo.5\nIn contrast, the GNN model outperforms the RF due by leveraging information from galaxy pairwise\ndistances and line-of-sight velocities.\n5.2\nComparison against previous work\nPrevious studies have used ML to estimate galaxy properties from dark matter halos [16, 1], i.e. the\ninverse of the problem we tackle. Some works employ feature importance from decision tree-based\nmethods [21, 41], while others use reinforcement learning to connect halo properties to galaxies\n[23]. Convolutional neural networks (CNNs) and GNNs have also been used to predict galaxy stellar\nmasses from simulated halos [5, 40, 41].\nSeveral works have used ML methods to predict cluster halo masses from observable parameters\nsuch as X-ray brightness and Sunyaev-Zel’dovich decrements [26, 13]. [42] compare how different\ncluster observables fare when pixelised as inputs to a CNN. [18] use GNNs to predict Mhalo directly\nfrom galaxy point clouds, but their training dataset (the much smaller TNG50 simulation) does not\ncontain many rare galaxy clusters. Our work is the first to train and test GNNs for predicting halo\nmasses in the extremely overdense regime of galaxy clusters.\n6\nConclusions, Limitations and Future Work\nIn this work, we predict Mhalo for simulated galaxies using their stellar masses, 2D projected\npositions, and line-of-sight velocities (i.e., x, y, vz) with the TNG-Cluster simulation for training and\nTNG300 for testing. We evaluated both Random Forest (RF) models and Graph Neural Networks\n(GNNs). The key findings are:\n1. The GNN model significantly outperforms RF model, even when the latter is provided ∆G\nas a parameter. This suggests that GNNs capture the underlying spatial relationships and\nsubstructures within clusters, as shown in Table 1 and Figures 2 and 3.\n2. The GNN maintains its predictive power when tested on the independent TNG300 dataset,\ndemonstrating that the model generalises across the IllustrisTNG simulation suite.\nDespite our promising results, models trained on one simulation may face challenges when applied\nto other simulations or real observational data. Machine learning models are often susceptible to\ndomain shift, where their performance degrades when applied to datasets that differ from their\ntraining data [36, 17]. In our case, the comparable performance between the TNG-Cluster cross-\nvalidation and TNG300 test datasets suggests that the GNN model may be robust to domain shift\n5Due to line-of-sight effects, not all galaxies at small projected distances experience significant tidal stripping.\n4\n\nwithin the IllustrisTNG suite. This robustness could be attributed to the GNN’s ability to learn\ngeneralizable symbolic relationships [10]. Further tests using other simulation physics or with\nobserved datasets (e.g., galaxies at other redshifts) are needed before we can conclude that this\nmethod is fully generalizable.\nIn future work, we will account for observational effects like contaminating galaxies in projection,\nmissing data, and photometric redshift uncertainties, as well as broader concerns about domain shift\nin ML (see e.g. [7]). Aside from additional validation on other cosmological simulations [33], we\nwill test on observational data using published Mhalo estimates for well-known galaxy clusters (e.g.,\n[20, 38]). With upcoming telescopes like the Roman Space Telescope [34] and Rubin Observatory\n[9], we will be able to study GNN applications to large galaxy cluster samples in the wide-field\ndomain.\nReferences\n[1] Shankar Agarwal, Romeel Davé, and Bruce A Bassett. Painting galaxies into dark matter haloes\nusing machine learning. Monthly Notices of the Royal Astronomical Society, 478(3):3410–3422,\nAugust 2018.\n[2] Mark Birkinshaw. The Sunyaev–Zel’dovich effect. Physics Reports, 310(2):97–195, March\n1999.\n[3] L. E. Bleem, B. Stalder, T. de Haan, K. A. Aird, S. W. Allen, D. E. Applegate, M. L. N. Ashby,\nM. Bautz, M. Bayliss, B. A. Benson, S. Bocquet, M. Brodwin, J. E. Carlstrom, C. L. Chang,\nI. Chiu, H. M. Cho, A. Clocchiatti, T. M. Crawford, A. T. Crites, S. Desai, J. P. Dietrich, M. A.\nDobbs, R. J. Foley, W. R. Forman, E. M. George, M. D. Gladders, A. H. Gonzalez, N. W.\nHalverson, C. Hennig, H. Hoekstra, G. P. Holder, W. L. Holzapfel, J. D. Hrubes, C. Jones,\nR. Keisler, L. Knox, A. T. Lee, E. M. Leitch, J. Liu, M. Lueker, D. Luong-Van, A. Mantz,\nD. P. Marrone, M. McDonald, J. J. McMahon, S. S. Meyer, L. Mocanu, J. J. Mohr, S. S.\nMurray, S. Padin, C. Pryke, C. L. Reichardt, A. Rest, J. Ruel, J. E. Ruhl, B. R. Saliwanchik,\nA. Saro, J. T. Sayre, K. K. Schaffer, T. Schrabback, E. Shirokoff, J. Song, H. G. Spieler, S. A.\nStanford, Z. Staniszewski, A. A. Stark, K. T. Story, C. W. Stubbs, K. Vanderlinde, J. D. Vieira,\nA. Vikhlinin, R. Williamson, O. Zahn, and A. Zenteno. Galaxy Clusters Discovered via the\nSunyaev-Zel’dovich Effect in the 2500-Square-Degree SPT-SZ Survey. The Astrophysical\nJournal Supplement Series, 216(2):27, February 2015.\n[4] James S. Bullock and Michael Boylan-Kolchin. Small-Scale Challenges to the Λ CDM\nParadigm. Annual Review of Astronomy and Astrophysics, 55(1):343–387, August 2017.\n[5] Urmila Chadayammuri, Michelle Ntampaka, John ZuHone, Ákos Bogdán, and Ralph P. Kraft.\nPainting baryons on to N-body simulations of galaxy clusters with image-to-image deep learning.\nMonthly Notices of the Royal Astronomical Society, 526(2):2812–2829, December 2023.\n[6] Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-\nGCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks. In\nProceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &\nData Mining, pages 257–266, July 2019.\n[7] A. ´Ciprijanovi´c, D. Kafkes, K. Downey, S. Jenkins, G. N. Perdue, S. Madireddy, T. Johnston,\nG. F. Snyder, and B. Nord. DeepMerge - II. Building robust deep learning algorithms for\nmerging galaxy identification across domains. Monthly Notices of the Royal Astronomical\nSociety, 506(1):677–691, September 2021.\n[8] Douglas Clowe, Maruša Bradaˇc, Anthony H. Gonzalez, Maxim Markevitch, Scott W. Randall,\nChristine Jones, and Dennis Zaritsky. A Direct Empirical Proof of the Existence of Dark Matter.\nThe Astrophysical Journal, 648(2):L109–L113, September 2006.\n[9] LSST Dark Energy Science Collaboration. Large Synoptic Survey Telescope: Dark Energy\nScience Collaboration, November 2012.\n[10] Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David\nSpergel, and Shirley Ho. Discovering Symbolic Models from Deep Learning with Inductive\nBiases, 2020.\n[11] Matthias Fey and Jan Eric Lenssen. Fast Graph Representation Learning with PyTorch Geomet-\nric, 2019.\n5\n\n[12] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs), 2016.\n[13] Matthew Ho, John Soltis, Arya Farahi, Daisuke Nagai, August Evrard, and Michelle Ntampaka.\nBenchmarks and explanations for deep learning estimates of X-ray galaxy cluster masses.\nMonthly Notices of the Royal Astronomical Society, 524(3):3289–3302, September 2023.\n[14] Tin Kam Ho. Random decision forests. In Proceedings of 3rd International Conference on\nDocument Analysis and Recognition, volume 1, pages 278–282 vol.1, August 1995.\n[15] Niall Jeffrey and Benjamin D. Wandelt.\nSolving high-dimensional parameter inference:\nMarginal posterior densities &amp; Moment Networks, 2020.\n[16] Harshil M. Kamdar, Matthew J. Turk, and Robert J. Brunner. Machine learning and cosmological\nsimulations – I. Semi-analytical models. Monthly Notices of the Royal Astronomical Society,\n455(1):642–658, January 2016.\n[17] Wouter M. Kouw and Marco Loog. An introduction to domain adaptation and transfer learning,\nJanuary 2019.\n[18] Austin J. Larson, John F. Wu, and Craig Jones. Predicting dark matter halo masses from\nsimulated galaxy images and environments, July 2024.\n[19] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization, January 2019.\n[20] J. M. Lotz, A. Koekemoer, D. Coe, N. Grogin, P. Capak, J. Mack, J. Anderson, R. Avila, E. A.\nBarker, D. Borncamp, G. Brammer, M. Durbin, H. Gunning, B. Hilbert, H. Jenkner, H. Khan-\ndrika, Z. Levay, R. A. Lucas, J. MacKenty, S. Ogaz, B. Porterfield, N. Reid, M. Robberto,\nP. Royle, L. J. Smith, L. J. Storrie-Lombardi, B. Sunnquist, J. Surace, D. C. Taylor, R. Williams,\nJ. Bullock, M. Dickinson, S. Finkelstein, P. Natarajan, J. Richard, B. Robertson, J. Tumlinson,\nA. Zitrin, K. Flanagan, K. Sembach, B. T. Soifer, and M. Mountain. The Frontier Fields: Survey\nDesign and Initial Results. The Astrophysical Journal, 837(1):97, March 2017.\n[21] Christopher C. Lovell, Stephen M. Wilkins, Peter A. Thomas, Matthieu Schaller, Carlton M.\nBaugh, Giulio Fabbian, and Yannick Bahé. A machine learning approach to mapping baryons\non to dark matter haloes using the EAGLE and C-EAGLE simulations. Monnthly Notices of the\nRoyal Astronomical Society, 509(4):5046–5061, February 2022.\n[22] Tobias A. Marriage, Viviana Acquaviva, Peter A. R. Ade, Paula Aguirre, Mandana Amiri,\nJohn William Appel, L. Felipe Barrientos, Elia S. Battistelli, J. Richard Bond, Ben Brown, Bryce\nBurger, Jay Chervenak, Sudeep Das, Mark J. Devlin, Simon R. Dicker, W. Bertrand Doriese,\nJoanna Dunkley, Rolando Dünner, Thomas Essinger-Hileman, Ryan P. Fisher, Joseph W. Fowler,\nAmir Hajian, Mark Halpern, Matthew Hasselfield, Carlos Hernández-Monteagudo, Gene C.\nHilton, Matt Hilton, Adam D. Hincks, Renée Hlozek, Kevin M. Huffenberger, David Handel\nHughes, John P. Hughes, Leopoldo Infante, Kent D. Irwin, Jean Baptiste Juin, Madhuri Kaul,\nJeff Klein, Arthur Kosowsky, Judy M. Lau, Michele Limon, Yen-Ting Lin, Robert H. Lupton,\nDanica Marsden, Krista Martocci, Phil Mauskopf, Felipe Menanteau, Kavilan Moodley, Harvey\nMoseley, Calvin B. Netterfield, Michael D. Niemack, Michael R. Nolta, Lyman A. Page, Lucas\nParker, Bruce Partridge, Hernan Quintana, Erik D. Reese, Beth Reid, Neelima Sehgal, Blake D.\nSherwin, Jon Sievers, David N. Spergel, Suzanne T. Staggs, Daniel S. Swetz, Eric R. Switzer,\nRobert Thornton, Hy Trac, Carole Tucker, Ryan Warne, Grant Wilson, Ed Wollack, and Yue\nZhao. The Atacama Cosmology Telescope: Sunyaev-Zel’dovich-Selected Galaxy Clusters at\n148 GHz in the 2008 Survey. The Astrophysical Journal, 737(2):61, August 2011.\n[23] Benjamin P Moster, Thorsten Naab, Magnus Lindström, and Joseph A O’Leary. GalaxyNet:\nConnecting galaxies and dark matter haloes with deep neural networks and reinforcement\nlearning in large volumes. Monthly Notices of the Royal Astronomical Society, 507(2):2115–\n2136, October 2021.\n[24] Dylan Nelson, Annalisa Pillepich, Mohammadreza Ayromlou, Wonki Lee, Katrin Lehle, Eric\nRohr, and Nhut Truong. Introducing the TNG-Cluster simulation: Overview and the physical\nproperties of the gaseous intracluster medium. Astronomy & Astrophysics, 686:A157, June\n2024.\n[25] Dylan Nelson, Volker Springel, Annalisa Pillepich, Vicente Rodriguez-Gomez, Paul Torrey, Shy\nGenel, Mark Vogelsberger, Ruediger Pakmor, Federico Marinacci, Rainer Weinberger, Luke\nKelley, Mark Lovell, Benedikt Diemer, and Lars Hernquist. The IllustrisTNG simulations:\nPublic data release. Computational Astrophysics and Cosmology, 6(1):2, May 2019.\n6\n\n[26] M. Ntampaka, J. ZuHone, D. Eisenstein, D. Nagai, A. Vikhlinin, L. Hernquist, F. Marinacci,\nD. Nelson, R. Pakmor, A. Pillepich, P. Torrey, and M. Vogelsberger. A Deep Learning Approach\nto Galaxy Cluster X-Ray Masses. The Astrophysical Journal, 876(1):82, May 2019.\n[27] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,\nOlivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vander-\nplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard\nDuchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research,\n12(85):2825–2830, 2011.\n[28] P. J. E. Peebles. Tests of cosmological models constrained by inflation. The Astrophysical\nJournal, 284:439, September 1984.\n[29] Planck Collaboration, P. A. R. Ade, N. Aghanim, M. Arnaud, M. Ashdown, J. Aumont, C. Bacci-\ngalupi, A. J. Banday, R. B. Barreiro, J. G. Bartlett, N. Bartolo, E. Battaner, R. Battye, K. Benabed,\nA. Benoît, A. Benoit-Lévy, J.-P. Bernard, M. Bersanelli, P. Bielewicz, J. J. Bock, A. Bonaldi,\nL. Bonavera, J. R. Bond, J. Borrill, F. R. Bouchet, F. Boulanger, M. Bucher, C. Burigana, R. C.\nButler, E. Calabrese, J.-F. Cardoso, A. Catalano, A. Challinor, A. Chamballu, R.-R. Chary, H. C.\nChiang, J. Chluba, P. R. Christensen, S. Church, D. L. Clements, S. Colombi, L. P. L. Colombo,\nC. Combet, A. Coulais, B. P. Crill, A. Curto, F. Cuttaia, L. Danese, R. D. Davies, R. J. Davis,\nP. De Bernardis, A. De Rosa, G. De Zotti, J. Delabrouille, F.-X. Désert, E. Di Valentino, C. Dick-\ninson, J. M. Diego, K. Dolag, H. Dole, S. Donzelli, O. Doré, M. Douspis, A. Ducout, J. Dunkley,\nX. Dupac, G. Efstathiou, F. Elsner, T. A. Enßlin, H. K. Eriksen, M. Farhang, J. Fergusson,\nF. Finelli, O. Forni, M. Frailis, A. A. Fraisse, E. Franceschi, A. Frejsel, S. Galeotta, S. Galli,\nK. Ganga, C. Gauthier, M. Gerbino, T. Ghosh, M. Giard, Y. Giraud-Héraud, E. Giusarma,\nE. Gjerløw, J. González-Nuevo, K. M. Górski, S. Gratton, A. Gregorio, A. Gruppuso, J. E.\nGudmundsson, J. Hamann, F. K. Hansen, D. Hanson, D. L. Harrison, G. Helou, S. Henrot-\nVersillé, C. Hernández-Monteagudo, D. Herranz, S. R. Hildebrandt, E. Hivon, M. Hobson,\nW. A. Holmes, A. Hornstrup, W. Hovest, Z. Huang, K. M. Huffenberger, G. Hurier, A. H.\nJaffe, T. R. Jaffe, W. C. Jones, M. Juvela, E. Keihänen, R. Keskitalo, T. S. Kisner, R. Kneissl,\nJ. Knoche, L. Knox, M. Kunz, H. Kurki-Suonio, G. Lagache, A. Lähteenmäki, J.-M. Lamarre,\nA. Lasenby, M. Lattanzi, C. R. Lawrence, J. P. Leahy, R. Leonardi, J. Lesgourgues, F. Levrier,\nA. Lewis, M. Liguori, P. B. Lilje, M. Linden-Vørnle, M. López-Caniego, P. M. Lubin, J. F.\nMacías-Pérez, G. Maggio, D. Maino, N. Mandolesi, A. Mangilli, A. Marchini, M. Maris, P. G.\nMartin, M. Martinelli, E. Martínez-González, S. Masi, S. Matarrese, P. McGehee, P. R. Mein-\nhold, A. Melchiorri, J.-B. Melin, L. Mendes, A. Mennella, M. Migliaccio, M. Millea, S. Mitra,\nM.-A. Miville-Deschênes, A. Moneti, L. Montier, G. Morgante, D. Mortlock, A. Moss, D. Mun-\nshi, J. A. Murphy, P. Naselsky, F. Nati, P. Natoli, C. B. Netterfield, H. U. Nørgaard-Nielsen,\nF. Noviello, D. Novikov, I. Novikov, C. A. Oxborrow, F. Paci, L. Pagano, F. Pajot, R. Paladini,\nD. Paoletti, B. Partridge, F. Pasian, G. Patanchon, T. J. Pearson, O. Perdereau, L. Perotto,\nF. Perrotta, V. Pettorino, F. Piacentini, M. Piat, E. Pierpaoli, D. Pietrobon, S. Plaszczynski,\nE. Pointecouteau, G. Polenta, L. Popa, G. W. Pratt, G. Prézeau, S. Prunet, J.-L. Puget, J. P.\nRachen, W. T. Reach, R. Rebolo, M. Reinecke, M. Remazeilles, C. Renault, A. Renzi, I. Ristor-\ncelli, G. Rocha, C. Rosset, M. Rossetti, G. Roudier, B. Rouillé d’Orfeuil, M. Rowan-Robinson,\nJ. A. Rubiño-Martín, B. Rusholme, N. Said, V. Salvatelli, L. Salvati, M. Sandri, D. Santos,\nM. Savelainen, G. Savini, D. Scott, M. D. Seiffert, P. Serra, E. P. S. Shellard, L. D. Spencer,\nM. Spinelli, V. Stolyarov, R. Stompor, R. Sudiwala, R. Sunyaev, D. Sutton, A.-S. Suur-Uski, J.-F.\nSygnet, J. A. Tauber, L. Terenzi, L. Toffolatti, M. Tomasi, M. Tristram, T. Trombetti, M. Tucci,\nJ. Tuovinen, M. Türler, G. Umana, L. Valenziano, J. Valiviita, F. Van Tent, P. Vielva, F. Villa,\nL. A. Wade, B. D. Wandelt, I. K. Wehus, M. White, S. D. M. White, A. Wilkinson, D. Yvon,\nA. Zacchei, and A. Zonca. Planck 2015 results: XIII. Cosmological parameters. Astronomy &\nAstrophysics, 594:A13, October 2016.\n[30] E. S. Rykoff, E. Rozo, M. T. Busha, C. E. Cunha, A. Finoguenov, A. Evrard, J. Hao, B. P.\nKoester, A. Leauthaud, B. Nord, M. Pierre, R. Reddick, T. Sadibekova, E. S. Sheldon, and\nR. H. Wechsler. redMaPPer. I. Algorithm and SDSS DR8 Catalog. The Astrophysical Journal,\n785(2):104, April 2014.\n[31] H. Sampaio-Santos, Y. Zhang, R. L. C. Ogando, T. Shin, Jesse B. Golden-Marx, B. Yanny,\nK. Herner, M. Hilton, A. Choi, M. Gatti, D. Gruen, B. Hoyle, M. M. Rau, J. De Vicente,\nJ. Zuntz, T. M. C. Abbott, M. Aguena, S. Allam, J. Annis, S. Avila, E. Bertin, D. Brooks,\nD. L. Burke, M. Carrasco Kind, J. Carretero, C. Chang, M. Costanzi, L. N. da Costa, H. T.\n7\n\nDiehl, P. Doel, S. Everett, A. E. Evrard, B. Flaugher, P. Fosalba, J. Frieman, J. García-Bellido,\nE. Gaztanaga, D. W. Gerdes, R. A. Gruendl, J. Gschwend, G. Gutierrez, S. R. Hinton, D. L.\nHollowood, K. Honscheid, D. J. James, M. Jarvis, T. Jeltema, K. Kuehn, N. Kuropatkin,\nO. Lahav, M. A. G. Maia, M. March, J. L. Marshall, R. Miquel, A. Palmese, F. Paz-Chinchón,\nA. A. Plazas, E. Sanchez, B. Santiago, V. Scarpine, M. Schubnell, M. Smith, E. Suchyta,\nG. Tarle, D. L. Tucker, T. N. Varga, and R. H. Wechsler. Is diffuse intracluster light a good\ntracer of the galaxy cluster matter distribution? Monthly Notices of the Royal Astronomical\nSociety, 501(1):1300–1315, February 2021.\n[32] F. Scarselli, M. Gori, Ah Chung Tsoi, M. Hagenbuchner, and G. Monfardini. The Graph Neural\nNetwork Model. IEEE Transactions on Neural Networks, 20(1):61–80, January 2009.\n[33] Joop Schaye, Roi Kugel, Matthieu Schaller, John C. Helly, Joey Braspenning, Willem Elbers,\nIan G. McCarthy, Marcel P. van Daalen, Bert Vandenbroucke, Carlos S. Frenk, Juliana Kwan,\nJaime Salcido, Yannick M. Bahé, Josh Borrow, Evgenii Chaikin, Oliver Hahn, Filip Huško,\nAdrian Jenkins, Cedric G. Lacey, and Folkert S. J. Nobels. The FLAMINGO project: Cosmolog-\nical hydrodynamical simulations for large-scale structure and galaxy cluster surveys. Monthly\nNotices of the Royal Astronomical Society, 526(4):4978–5020, October 2023.\n[34] D. Spergel, N. Gehrels, C. Baltay, D. Bennett, J. Breckinridge, M. Donahue, A. Dressler, B. S.\nGaudi, T. Greene, O. Guyon, C. Hirata, J. Kalirai, N. J. Kasdin, B. Macintosh, W. Moos,\nS. Perlmutter, M. Postman, B. Rauscher, J. Rhodes, Y. Wang, D. Weinberg, D. Benford,\nM. Hudson, W. S. Jeong, Y. Mellier, W. Traub, T. Yamada, P. Capak, J. Colbert, D. Masters,\nM. Penny, D. Savransky, D. Stern, N. Zimmerman, R. Barry, L. Bartusek, K. Carpenter,\nE. Cheng, D. Content, F. Dekens, R. Demers, K. Grady, C. Jackson, G. Kuan, J. Kruk, M. Melton,\nB. Nemati, B. Parvin, I. Poberezhskiy, C. Peddie, J. Ruffa, J. K. Wallace, A. Whipple, E. Wollack,\nand F. Zhao. Wide-field infrarred survey telescope-astrophysics focused telescope assets wfirst-\nafta 2015 report, 2015.\n[35] Volker Springel, Simon D. M. White, Giuseppe Tormen, and Guinevere Kauffmann. Populating a\ncluster of galaxies - I. Results at \\fontshape{it}{z}=0. Monthly Notices of the Royal Astronomical\nSociety, 328(3):726–750, December 2001.\n[36] Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation.\nIn Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI’16, pages\n2058–2065, Phoenix, Arizona, February 2016. AAAI Press.\n[37] S. Vegetti, S. Birrer, G. Despali, C. D. Fassnacht, D. Gilman, Y. Hezaveh, L. Perreault Levasseur,\nJ. P. McKean, D. M. Powell, C. M. O’Riordan, and G. Vernardos. Strong Gravitational Lensing\nas a Probe of Dark Matter. Space Science Reviews, 220(5):58, July 2024.\n[38] John R. Weaver, Sam E. Cutler, Richard Pan, Katherine E. Whitaker, Ivo Labbé, Sedona H. Price,\nRachel Bezanson, Gabriel Brammer, Danilo Marchesini, Joel Leja, Bingjie Wang, Lukas J.\nFurtak, Adi Zitrin, Hakim Atek, Iryna Chemerynska, Dan Coe, Pratika Dayal, Pieter van\nDokkum, Robert Feldmann, Natascha M. Förster Schreiber, Marijn Franx, Seiji Fujimoto,\nYoshinobu Fudamoto, Karl Glazebrook, Anna de Graaff, Jenny E. Greene, Stéphanie Juneau,\nSusan Kassin, Mariska Kriek, Gourav Khullar, Michael V. Maseda, Lamiya A. Mowla, Adam\nMuzzin, Themiya Nanayakkara, Erica J. Nelson, Pascal A. Oesch, Camilla Pacifici, Casey\nPapovich, David J. Setton, Alice E. Shapley, Heath V. Shipley, Renske Smit, Mauro Stefanon,\nEdward N. Taylor, Andrea Weibel, and Christina C. Williams. The UNCOVER Survey: A\nFirst-look HST + JWST Catalog of 60,000 Galaxies near A2744 and beyond. The Astrophysical\nJournal Supplement Series, 270:7, January 2024.\n[39] Risa H. Wechsler and Jeremy L. Tinker. The Connection Between Galaxies and Their Dark\nMatter Halos. Annual Review of Astronomy and Astrophysics, 56(1):435–487, September 2018.\n[40] John F. Wu and Christian Kragh Jespersen. Learning the galaxy-environment connection with\ngraph neural networks, June 2023.\n[41] John F. Wu, Christian Kragh Jespersen, and Risa H. Wechsler. How the Galaxy-Halo Connection\nDepends on Large-Scale Environment, February 2024.\n[42] Z Yan, A J Mead, L Van Waerbeke, G Hinshaw, and I G McCarthy. Galaxy cluster mass\nestimation with deep learning and hydrodynamical simulations. Monthly Notices of the Royal\nAstronomical Society, 499(3):3445–3458, November 2020.\n8\n\nFigure 4: Spatial distribution of halos within the TNG-Cluster simulation. The middle panel shows the\nfull simulation, and the left and right panels highlight two example galaxy clusters. The boundaries\nof these clusters are marked as blue and red boxes in the middle panel.\nTable 2: Summary of cuts applied to the TNG-Cluster data. Here, N∗refers to number of stellar\nparticles, M⊙refers to solar mass, R200 refers to the virial radius of the halo.\nSample\nNumber of Subhalos\nFull TNG-Cluster catalog\n10,378,451\n— within mass cuts - N∗> 50; log(M∗/M⊙) > 9.5; log(Mhalo/M⊙) > 10.5\n154,120\n— within < 10× R200 of the cluster halo\n127,165\nSelection Criteria - log(Mhalo/M⊙) > 11; within 10 Mpc of the cluster halo\nTNG-Cluster cross-validation\n60,756\nTNG300 Test Set\n34,689\nA\nTNG-Cluster Additional Details\nGalaxies in the TNG-Cluster training data are shown in Figure 4. To mimic astronomical observations\nof galaxies, we project the galaxy clusters along the z axis, which is chosen to be the line of sight.\nThis procedure bridges the gap between simulation data and spectroscopic observations, which\ntypically capture two spatial dimensions (x, y) and line-of-sight velocities (vz). We also apply quality\ncuts to the simulation in Table 2 to ensure a complete sample of massive, well-resolved galaxies.\nWe split the TNG-Cluster data into training and validation sets by implementing a k-fold cross-\nvalidation strategy based on cluster IDs rather than traditional random splits. This method isolates\nsubhalos according to their cluster IDs while ensuring that all subhalos from a single cluster remain\nwithin the same fold. One potential caveat of this method is that we do not include the contaminating\nstructure along the line-of-sight from other clusters which might be in a different k-fold.\n9",
    "pdf_filename": "Estimating_Dark_Matter_Halo_Masses_in_Simulated_Galaxy_Clusters_with_Graph_Neural_Networks.pdf"
}