{
    "title": "LLM4DS Evaluating Large Language Models for Data Science Code Generation",
    "context": "code generation in data science offers substantial potential for en- hancing tasks such as data manipulation, statistical analysis, and visualization. However, the effectiveness of these models in the data science domain remains underexplored. This paper presents a controlled experiment that empirically assesses the performance of four leading LLM-based AI assistants—Microsoft Copilot (GPT-4 Turbo), ChatGPT (o1-preview), Claude (3.5 Sonnet), and Perplexity Labs (Llama-3.1-70b-instruct)—on a diverse set of data science coding challenges sourced from the Stratacratch platform. Using the Goal-Question-Metric (GQM) approach, we evaluated each model’s effectiveness across task types (Analyti- cal, Algorithm, Visualization) and varying difficulty levels. Our findings reveal that all models exceeded a 50% baseline success rate, confirming their capability beyond random chance. Notably, only ChatGPT and Claude achieved success rates significantly above a 60% baseline, though none of the models reached a 70% threshold, indicating limitations in higher standards. ChatGPT demonstrated consistent performance across varying difficulty levels, while Claude’s success rate fluctuated with task complexity. Hypothesis testing indicates that task type does not significantly impact success rate overall. For analytical tasks, efficiency anal- ysis shows no significant differences in execution times, though ChatGPT tended to be slower and less predictable despite high success rates. For visualization tasks, while similarity quality among LLMs is comparable, ChatGPT consistently delivered the most accurate outputs. This study provides a structured, empirical evaluation of LLMs in data science, delivering insights that support informed model selection tailored to specific task demands. Our findings establish a framework for future AI assessments, emphasizing the value of rigorous evaluation beyond basic accuracy measures. Index Terms—data science, large language model, coding generation, empirical study, hypothesis testing Large Language Models (LLMs) have emerged as transfor- mative tools with the potential to revolutionize code generation in various domains, including data science [1]–[5]. Their ability to generate human-like text and code opens up pos- sibilities for automating complex tasks in data manipulation, visualization, and analytics. As data science projects often require extensive coding efforts that are time-consuming and demand significant expertise, leveraging LLMs could greatly enhance productivity and accessibility in this field. However, the effectiveness and reliability of LLM-generated code for data science applications remain underexplored, necessitating a thorough evaluation. While previous studies have evaluated LLMs in general programming tasks using platforms like LeetCode [6]–[9], the HumanEval benchmark [10], and GitHub Projects [11], Gu et al. [12] identified a notable gap in approaches to evaluate domain-specific code generation. They demonstrated that LLMs exhibit sub-optimal performance in generating domain-specific code for areas such as web and game devel- opment, due to their limited proficiency in utilizing domain- specific libraries. This finding underscores the need for more focused evaluations that consider the unique challenges of specialized domains like data science, which involve tasks such as handling datasets, performing complex statistical anal- yses, and generating insightful visualizations—areas not fully represented in general programming assessments. This paper addresses this gap by providing an empirical evaluation [13] of multiple LLMs on diverse data science- specific coding problems sourced from the Stratascratch plat- form [14]. The controlled experiment involves four main steps: (i) selecting 100 Python coding problems from Stratascratch, distributed across three difficulty levels (easy, medium, hard) and three problem types (Analytical, Algorithm, Visualiza- tion); (ii) transforming these problems into prompts following the optimal prompt structure for each type; (iii) using these prompts for each AI assistant to generate code solutions; and (iv) evaluating the generated code based on correctness, efficiency, and other relevant metrics. Our research seeks to answer the following question: How effective are LLMs for data science coding? By systematically assessing the performance of these AI assistants, we aim to identify their strengths and limitations in automating code generation for data science problems. Our contributions are multifold: 1) We provide an empirical evaluation of multiple LLMs on data science-specific coding problems, filling a critical gap in current research. 2) We assess Stratacratch as a platform to benchmark LLMs for data science code generation, evaluating its suitability and potential as a standardized dataset for LLM performance in this domain. arXiv:2411.11908v1  [cs.SE]  16 Nov 2024",
    "body": "LLM4DS: Evaluating Large Language Models for\nData Science Code Generation\nNathalia Nascimento\nEASER, Eng. Division\nPennsylvania State University\nGreat Valley, USA\nnqm5742@psu.edu\nEverton Guimaraes\nEASER, Eng. Division\nPennsylvania State University\nGreat Valley, USA\nezt157@psu.edu\nSai Sanjna Chintakunta\nEASER, Eng. Division\nPennsylvania State University\nGreat Valley, USA\nsqc6557@psu.edu\nSanthosh Anitha Boominathan\nEASER, Eng. Division\nPennsylvania State University\nGreat Valley, USA\nsfa5971@psu.edu\nAbstract—The adoption of Large Language Models (LLMs) for\ncode generation in data science offers substantial potential for en-\nhancing tasks such as data manipulation, statistical analysis, and\nvisualization. However, the effectiveness of these models in the\ndata science domain remains underexplored. This paper presents\na controlled experiment that empirically assesses the performance\nof four leading LLM-based AI assistants—Microsoft Copilot\n(GPT-4 Turbo), ChatGPT (o1-preview), Claude (3.5 Sonnet), and\nPerplexity Labs (Llama-3.1-70b-instruct)—on a diverse set of\ndata science coding challenges sourced from the Stratacratch\nplatform. Using the Goal-Question-Metric (GQM) approach, we\nevaluated each model’s effectiveness across task types (Analyti-\ncal, Algorithm, Visualization) and varying difficulty levels. Our\nfindings reveal that all models exceeded a 50% baseline success\nrate, confirming their capability beyond random chance. Notably,\nonly ChatGPT and Claude achieved success rates significantly\nabove a 60% baseline, though none of the models reached a 70%\nthreshold, indicating limitations in higher standards. ChatGPT\ndemonstrated consistent performance across varying difficulty\nlevels, while Claude’s success rate fluctuated with task complexity.\nHypothesis testing indicates that task type does not significantly\nimpact success rate overall. For analytical tasks, efficiency anal-\nysis shows no significant differences in execution times, though\nChatGPT tended to be slower and less predictable despite high\nsuccess rates. For visualization tasks, while similarity quality\namong LLMs is comparable, ChatGPT consistently delivered\nthe most accurate outputs. This study provides a structured,\nempirical evaluation of LLMs in data science, delivering insights\nthat support informed model selection tailored to specific task\ndemands. Our findings establish a framework for future AI\nassessments, emphasizing the value of rigorous evaluation beyond\nbasic accuracy measures.\nIndex Terms—data science, large language model, coding\ngeneration, empirical study, hypothesis testing\nI. INTRODUCTION\nLarge Language Models (LLMs) have emerged as transfor-\nmative tools with the potential to revolutionize code generation\nin various domains, including data science [1]–[5]. Their\nability to generate human-like text and code opens up pos-\nsibilities for automating complex tasks in data manipulation,\nvisualization, and analytics. As data science projects often\nrequire extensive coding efforts that are time-consuming and\ndemand significant expertise, leveraging LLMs could greatly\nenhance productivity and accessibility in this field. However,\nthe effectiveness and reliability of LLM-generated code for\ndata science applications remain underexplored, necessitating\na thorough evaluation.\nWhile previous studies have evaluated LLMs in general\nprogramming tasks using platforms like LeetCode [6]–[9],\nthe HumanEval benchmark [10], and GitHub Projects [11],\nGu et al. [12] identified a notable gap in approaches to\nevaluate domain-specific code generation. They demonstrated\nthat LLMs exhibit sub-optimal performance in generating\ndomain-specific code for areas such as web and game devel-\nopment, due to their limited proficiency in utilizing domain-\nspecific libraries. This finding underscores the need for more\nfocused evaluations that consider the unique challenges of\nspecialized domains like data science, which involve tasks\nsuch as handling datasets, performing complex statistical anal-\nyses, and generating insightful visualizations—areas not fully\nrepresented in general programming assessments.\nThis paper addresses this gap by providing an empirical\nevaluation [13] of multiple LLMs on diverse data science-\nspecific coding problems sourced from the Stratascratch plat-\nform [14]. The controlled experiment involves four main steps:\n(i) selecting 100 Python coding problems from Stratascratch,\ndistributed across three difficulty levels (easy, medium, hard)\nand three problem types (Analytical, Algorithm, Visualiza-\ntion); (ii) transforming these problems into prompts following\nthe optimal prompt structure for each type; (iii) using these\nprompts for each AI assistant to generate code solutions;\nand (iv) evaluating the generated code based on correctness,\nefficiency, and other relevant metrics.\nOur research seeks to answer the following question: How\neffective are LLMs for data science coding? By systematically\nassessing the performance of these AI assistants, we aim to\nidentify their strengths and limitations in automating code\ngeneration for data science problems.\nOur contributions are multifold:\n1) We provide an empirical evaluation of multiple LLMs on\ndata science-specific coding problems, filling a critical\ngap in current research.\n2) We assess Stratacratch as a platform to benchmark\nLLMs for data science code generation, evaluating its\nsuitability and potential as a standardized dataset for\nLLM performance in this domain.\narXiv:2411.11908v1  [cs.SE]  16 Nov 2024\n\n3) We analyze the success rate of these models across\ndifferent task categories—Analytical, Algorithm, and\nVisualization—and difficulty levels, offering insights\ninto their practical utility in data science workflows.\n4) We highlight the challenges and limitations of LLMs in\nthis domain, providing a foundation for future improve-\nments and research in AI-assisted data science.\nThis paper is organized as follows. Section 2 presents the\nrelated work. Section 3 describes the controlled experiment,\noutlining the research questions, hypotheses, and methodology.\nSection 4-5 presents the experimental results and discusses\nthreats to validity. Sections 6-8 brings final remarks and\nsuggestions for future work.\nII. RELATED WORK\nIn the realm of code generation, prior studies have evaluated\nLLMs like ChatGPT and GitHub Copilot using platforms such\nas HumanEval Benchmark, LeetCode, and Github. Nascimento\net al. [7] compared code generated by ChatGPT against\nhuman-written solutions, assessing performance and memory\nefficiency. Kuhail et al. [8] evaluated ChatGPT on 180 Leet-\nCode problems, providing insights into its capabilities and\nlimitations. Coignion et al. [9] investigated different LLMs\non general coding problems from LeetCode, focusing on\nperformance metrics. Nguyen and Nadi [6] assessed GitHub\nCopilot’s code generation on 33 LeetCode problems, evaluat-\ning correctness and understandability.\nBeyond traditional programming tasks, LLMs have been\napplied in data science-specific domains, where recent research\nhas explored the models’ capacity to handle complex queries\nand data manipulation tasks. Troy et al. [15] demonstrated\nthat LLMs could generate SQL statements for cybersecurity\napplications, specifically highlighting their capability in struc-\ntured query generation. In another study, Malekpour et al.\n[16] introduced an LLM routing framework designed for text-\nto-SQL tasks, optimizing the selection of models based on\ncost-efficiency and accuracy. Li et al. [3] identified limitations\neven in advanced models like GPT-4, noting that these models\nachieved only 54.89% execution accuracy on complex text-to-\nSQL queries—significantly below the human benchmark of\n92.96%. Additionally, Kazemitabaar et al. [5] delved into the\nchallenges of data analysis with conversational AI tools like\nChatGPT, identifying difficulties users face in verifying and\nguiding AI-generated results for desired outcomes.\nLai et al. [4] proposed the DS-1000 benchmark, a dataset\nspecifically crafted for evaluating code generation in data\nscience contexts. DS-1000 comprises 451 unique data science\nproblems sourced from StackOverflow and spans seven es-\nsential Python libraries, including Numpy and Pandas. A key\nfeature of this benchmark is its emphasis on problem per-\nturbations, aimed at reducing the risk of model memorization.\nThe dataset accounts for the unique challenges of data science\ntasks, which often lack executable contexts, may depend on\nexternal libraries, and can have multiple correct solutions. Lai\net al. demonstrated the effect of different types of problem\nperturbations by testing models like Codex, InCoder, and\nCodeGen, with the best accuracy being 43.3% achieved by\nCodex-002. However, while DS-1000 provides a robust dataset\nfor testing, Lai et al. do not perform a comparative empirical\nevaluation across multiple LLMs, leaving open questions about\nhow current models fare on this benchmark.\nDespite these advancements, much of the current research\nhas been limited to either general coding tasks or SQL-specific\napplications. The nuances of data science problems—ranging\nfrom data manipulation and complex analyses to visualiza-\ntion—remain underexplored in LLM evaluations. Our work\naddresses this gap by conducting an empirical experiment\nusing four leading LLMs on a set of data science problems\nextracted from the Stratacratch dataset, encompassing various\ndifficulty levels and problem types. Unlike prior studies,\nwhich primarily introduce benchmarks or focus on specific\ntask categories, our approach offers a detailed examination of\nLLM performance across a broader spectrum of data science\nchallenges.\nIII. CONTROLLED EXPERIMENT\nIn line with the controlled experiment methodology by\nWohlin et al. [13], our study aims to evaluate and com-\npare the effectiveness of four prominent LLM-based AI as-\nsistants—Microsoft Copilot (GPT-4 Turbo), ChatGPT (o1-\npreview), Claude (3.5 Sonnet), and Perplexity Lab (Llama-3.1-\n70b-instruct)—in solving data science coding tasks sourced\nfrom the Stratascratch platform [14].\nEffectiveness in this context refers to the degree to which\nthese models achieve desired outcomes across four key as-\npects: success rate, efficiency, quality of output, and consis-\ntency. Specifically, we define:\n• Success Rate as the proportion of correctly generated\ncode solutions, measured by the percentage of solutions\nthat achieve the correct result regardless of the number\nof attempts;\n• Efficiency as the runtime execution speed of the gener-\nated solution;\n• Quality of Output as the alignment of generated solu-\ntions with expected outcomes, particularly for visualiza-\ntion tasks;\n• Consistency as the reliability of each model’s perfor-\nmance across varying difficulty levels and task types.\nA. Research Questions, Hypotheses, and Metrics\nTo systematically explore effectiveness, we structured our\ninvestigation around specific research questions, each accom-\npanied by testable hypotheses and relevant evaluation metrics.\nTable I details these research questions, hypotheses, and cor-\nresponding metrics.\nB. Variables Selection\nTo structure our analysis, we identified key variables that\nallow us to examine the performance of each AI assistant\nacross different problem types and difficulty levels.\nThe independent variables in this study, which we con-\ntrolled or varied, include:\n\nTABLE I\nRESEARCH QUESTIONS, HYPOTHESES, AND METRICS\nResearch Question\nNull Hypothesis\nAlternative Hypothesis\nMetrics\nRQ1: How successful are LLMs in solving\ndata science coding problems, and do they\noutperform each other in success rate?\nH0 1: The success rate of each\nLLM in solving data science\ncoding\nproblems\nis\nnot\nsig-\nnificantly higher than random\nchance (50%).\nH0 1a: There is no significant\ndifference in success rates be-\ntween LLM pairs.\nH1 1:\nThe\nsuccess\nrate\nof\neach LLM in solving data sci-\nence coding problems is sig-\nnificantly higher than random\nchance (50%).\nH1 1a: At least one pair of\nLLMs shows a significant differ-\nence in success rates.\nOverall success rate (percentage\nof correct solutions) and pair-\nwise success rate comparisons\nRQ2: Does the difficulty level of coding\nproblems (easy, medium, hard) influence the\nsuccess rate of the different LLMs, and do\nspecific LLMs outperform others at each dif-\nficulty level?\nH0 2: Difficulty level does not\nsignificantly affect the success\nrate of the LLMs.\nH0 2a: There is no significant\ndifference in success rates be-\ntween LLM pairs within each\ndifficulty level.\nH1 2: The success rate of the\nLLMs varies significantly with\ndifficulty level.\nH1 2a: At least one pair of\nLLMs shows a significant dif-\nference in success rate within a\nspecific difficulty level.\nSuccess rate (percentage of cor-\nrect solutions) across difficulty\nlevels and pairwise success rate\ncomparisons within each level\nRQ3: Does the type of data science task (An-\nalytical, Algorithm, Visualization) influence\nthe success rate of the different LLMs, and do\nspecific LLMs outperform others for certain\ntask types?\nH0 3: The type of data science\ntask does not significantly im-\npact the LLMs’ success rate.\nH0 3a: There is no significant\ndifference in success rates be-\ntween LLM pairs within each\ntask type.\nH1 3: The success rate of the\nLLMs varies significantly with\nthe type of data science task.\nH1 3a: At least one pair of\nLLMs shows a significant dif-\nference in success rate within a\nspecific task type.\nSuccess rate (percentage of cor-\nrect solutions) for each task type\nand pairwise success rate com-\nparisons within each type\nRQ4: For Analytical questions, do the LLMs\ndiffer in the efficiency (execution time) of the\ncode they generate?\nH0 4: The population medians\nof the execution times across the\nLLMs for Analytical questions\nare equal.\nH1 4: At least one LLM has a\ndifferent population median exe-\ncution time for Analytical ques-\ntions compared to others.\nExecution time for each gener-\nated solution on Analytical prob-\nlems, per LLM\nRQ5: For visualization tasks, do the LLMs\ndiffer in the quality (similarity) of the visual\noutputs they produce compared to expected\nresults?\nH0 5: The population medians\nof the similarity scores for vi-\nsualization outputs across the\nLLMs are equal.\nH1 5: At least one LLM has\na different population median\nsimilarity score for visualization\noutputs compared to others.\nSimilarity scores for visualiza-\ntion outputs compared to ex-\npected results\n• LLM-based AI assistants: The four AI models under\nevaluation—Microsoft Copilot, ChatGPT, Claude, and\nPerplexity Lab.\n• Difficulty level of coding problems: Easy, Medium,\nHard.\n• Type of Data Science task: Analytical, Algorithm, Vi-\nsualization.\nThe dependent variables are the metrics we measured to\nassess each AI assistant’s effectiveness:\n• Success rate: The percentage of correct solutions gener-\nated by each LLM, regardless of the number of attempts.\n• Running time: Execution time of code for Analytical\nquestions.\n• Graph similarity scores: Similarity between generated\nand expected graphs for Visualization questions.\nThese variables connect directly to the research questions\nand metrics outlined in Table I, allowing us to systematically\ninvestigate the impact of each independent variable on the AI\nmodels’ performance.\nIV. EXPERIMENT OPERATION\nThe experiment was conducted over the span of two months.\nFor each of the four LLMs, we generated a solution for each of\nthe 100 selected problems, resulting in a total of 400 generated\ncoding solutions. Two researchers manually interacted with\nthe AI assistants by inputting the prompts into their respective\ninterfaces. They then copied the generated code and submitted\nit to the Stratascratch platform to assess its correctness and\nfunctionality. The researchers recorded whether the solution\nworked as intended and noted any necessary adjustments.\nSince Stratascratch provides execution time only for Analytical\nquestions and similarity scores for Visualization questions, we\ncollected these specific measurements accordingly.\nThe overall process of our controlled experiment consists\nof 11 steps, as illustrated in Figure 1:\n1) Select the problem source: We chose Stratascratch [14]\nas the platform for sourcing data science problems.\n2) Select one problem per task category for prompt engi-\nneering: One problem from each data science task cate-\ngory (Analytical, Algorithm, Visualization) was selected\nto refine our prompt templates.\n3) Prompt Engineering with feedback loop: We performed\nprompt engineering by iteratively adjusting the prompts\nand assessing the performance of different LLM ver-\nsions, creating optimal prompt structures for each task\ntype.\n4) Selection of AI assistants and LLMs: Four AI assistants,\neach utilizing a different LLM, were selected for the\nexperiment.\n5) Definition of final prompts: The finalized prompt tem-\nplates were established for each problem type based on\nthe prompt engineering process.\n6) Selection of 100 Data Science problems: We selected\n100 data science problems covering various topics across\nthe three task types to ensure a comprehensive evalua-\ntion.\n\n7) Creation of prompts: The selected problems were in-\ncorporated into the prompt templates, resulting in 100\ntailored prompts.\n8) Execution with AI assistants: Each prompt was executed\nusing the four AI assistants, and the generated Python\ncode was saved.\n9) Submission to Stratascratch platform: The generated\ncode solutions were submitted to the Stratascratch plat-\nform interface for evaluation.\n10) Execution and result collection: The code was executed\non Stratascratch, and the results were saved into a results\ndataset.\n11) Data analysis: We compared the performance results of\nthe four LLMs to analyze their effectiveness.\nAnalysis Phase\nExecution Phase\nPreparation Phase\nResults in\nIteration Outcomes\n4- Final Selection of 4 AI \nAssistants and LLMs\n5- Final Prompt Templates\nPrompt Eng. Iteration\nIterate until satisfactory\n3- Prompt Engineering\nEvaluate AI Assistants\nAdjust Prompts Based on \nPerformance\nStart\n1- Select Stratascratch as \nProblem Source\n2- Select 1 Problem per \nTask Category\n6- Select 100 Data Science \nProblems\n7- Create 100 Prompts \nUsing Templates\n8- Execute Each Prompt \nwith AI Assistants\nAssistant 1: Microsoft \nCopilot\nAssistant 2: ChatGPT\nAssistant 3: Claude\nAssistant 4: Perplexity Labs\n9- Submit Code to \nStratascratch Platform\n10- Execute Code and Save \nResults\n11- Data Analysis: \nCompare Performance \nResults\nEnd\n11/9/24, 12:57 PM\nmermaid.ink/svg/pako:eNqVVktvo0gQ_islRiPNSDjiYWObw0qZJLsaKdmN1tnL4BzaTWHYYBrRzSTeJP99i1eDMTmMD7ipd3_9VTWvBhc…\nhttps://mermaid.ink/svg/pako:eNqVVktvo0gQ_islRiPNSDjiYWObw0qZJLsaKdmN1tnL4BzaTWHYYBrRzSTeJP99i1eDMTmMD7ipd3_9VTWvBhchGr4RpeKZx6…\n1/1\nFig. 1. Overview of the Experimental Process.\nThe following subsections and sections provide more de-\ntailed explanations of the main steps:\n• Subsection IV-A describes the selection of 100 Python\ncoding problems from Stratascratch, categorized by diffi-\nculty levels (easy, medium, hard) and types (Analytical,\nAlgorithm, Visualization).\n• Subsection IV-B outlines the iterative prompt engineering\nprocess, including the development and refinement of\nprompt templates for each task type. This resulted in\noptimal prompt structures used to transform the selected\nproblems into 100 tailored prompts.\n• Subsection IV-C explains the process of using these\nprompts with each AI assistant to generate code solutions.\n• Section V covers the data analysis process.\nA. Dataset: Selection of Data Science Problems\nFor our study, we selected the Stratascratch [14] platform\nas the source of data science coding problems. Stratascratch\nis a platform that aggregates real-world data science interview\nquestions from various companies, providing a diverse set of\nproblems that are representative of typical tasks encountered\nin data science, such as data manipulation, algorithm develop-\nment, and data visualization.\nStratascratch problems are organized into three difficulty\nlevels (easy, medium, and hard) and three main types, each\naddressing unique aspects of data science problem-solving:\nAnalytical: These problems involve tasks requiring data\nanalysis and manipulation using tools like pandas and SQL.\nTopics include data aggregation, filtering, conditional expres-\nsions, and data formatting.\nAlgorithm: These challenges focus on computational\nproblem-solving and algorithm development. Topics in this\ncategory include array manipulation, linear regression, proba-\nbility, graph theory, recursion, and optimization techniques.\nVisualization: These problems require the creation of charts\nand graphs to represent data insights visually. Topics cover\ndistribution analysis, time-series trend analysis, spatial data\nvisualization, and comparison of categorical and numerical\ndata.\nAn example Stratascratch problem is shown in Figure 2,\ndemonstrating the typical interface and information available\nfor each question.\nFig. 2. Example of a Visualization Problem from the Stratascratch platform.\nTo build our dataset, we used random sampling while ensur-\ning balanced representation across problem types and difficulty\n\nlevels—selecting 100 Python coding problems in total, with\n35 Analytical, 35 Algorithm, and 30 Visualization problems.\nFrom these 100 questions, 34 are easy, 32 are medium, and\n34 are hard. To avoid infringing any intellectual property from\nStratascratch, we omitted the full problem descriptions from\nour dataset. However, we have provided a table containing\nproblem IDs, difficulty levels, links, and topic descriptions,\nwhich gives sufficient context for each task (dataset available\nin [17]).\nB. Prompt Engineering: Transforming Problems into Prompts\nPrompt Template for Visualization Problems\nAct as a data scientist and provide working Python3 code for the problem\nbelow.\nCRITICAL REQUIREMENTS:\n1)\nUse ONLY the exact prefilled code snippet as starting point.\n2)\nNO additional imports beyond what’s given.\n3)\nNO sample/test data creation—use ONLY the provided DataFrame.\n4)\nNO functions unless explicitly required in the original code.\n5)\nCode\nmust\nend\nwith\nappropriate\nvisualization\ncommand\n(plt.show() for Matplotlib, fig.show() for Plotly, etc.).\n6)\nCode must be fully runnable without any modifications.\nPROBLEM:\nTitle: {Title}\nDescription: {Description}\nDifficulty: {Level}\n{DataFrame}\n{Additional}\nEXPECTED OUTPUT FORMAT:\n•\nDirect plotting code only.\n•\nMust end with appropriate show() command.\n•\nNo return statements.\n•\nNo print statements.\n•\nNo functions unless explicitly required.\n•\nNo test data creation.\nPrefilled code snippet (use exactly):\n{Code}\nThis step started by selecting one problem from each task\ncategory-Analytical, Algorithm, and Visualization-for prompt\ndevelopment. These problems were outside our main dataset\nto avoid biasing the evaluation results. During this phase, we\nexperimented with various prompt structures and observed the\nmodels’ outputs. Initially, the models often generated code\nthat included datasets or functions not specified in the problem\ndescriptions. To address this, we iteratively refined the prompts\nby introducing specific instructions and constraints.\nWe tested several LLMs during prompt engineering, includ-\ning some not selected for the main experiment. Some LLMs,\nsuch as Gemini (1.5 Flash), could not produce functional code\neven for easy problems, despite multiple prompt refinements.\nOthers, like YouChat Pro, was capable but was not included\nin the final selection to avoid redundancy.\nTo ensure consistency and minimize subjectivity, we auto-\nmated the conversion of problem descriptions into prompts.\nThis involved creating prompt templates tailored to each\nproblem type-Analytical, Algorithm, and Visualization-which\naddressed the unique requirements of each category. This\nsection illustrates the prompt template used for Visualization\nproblems. The templates for Algorithm and Analytics tasks\nare available in [17]. Our automated prompt generation system\nparsed the problem descriptions and inserted the information\ninto the appropriate template based on the problem type.\nC. Code Generation and Execution\nIn this experiment, we presented 100 problem prompts to\nfour AI assistants—Microsoft Copilot, ChatGPT, Claude, and\nPerplexity Labs—generating a total of 400 code solutions (100\nproblems per assistant). For each problem, a new chat thread\nwas initiated with the AI assistant to ensure no influence\nfrom previous interactions. Each AI assistant was given up\nto three attempts per problem, guided by feedback such as\n“Not worked” (which yielded better results with ChatGPT and\nCopilot) or “Wrong answer” (more effective with Claude and\nPerplexity) to prompt improvements.\nTo evaluate the solutions, we executed them on the\nStratascratch platform and recorded the metrics provided by\nthe platform, depending on the type of problem. For visu-\nalization problems, for example, the platforms calculates the\nsimilarity of the generated graphs with the expected outputs.\nFigure 3 illustrates a similarity comparison between a graph\ngenerated by the Perplexity model and the expected solution\nprovided by Stratascratch.\nFig. 3. Similarity comparison for a Visualization Problem.\nDue to Stratascratch platform constraints (e.g., limitations\non library imports and required code formatting), we al-\nlowed minor manual edits to adapt the AI-generated code for\nconsistent evaluation. These adjustments included removing\nprohibited imports (e.g., import os), modifying code structure\n\n(e.g., removing function encapsulation when global code was\nneeded), and eliminating unnecessary print statements in favor\nof returns. These edits preserved the core logic and function-\nality of the solutions and were documented for transparency\nand reproducibility. This documentation (available in [17])\nincludes the nature of the edits and their reason.\nV. ANALYSIS AND INTERPRETATION\nThis section presents the statistical analysis of data collected\nduring the experiment. The dataset includes information such\nas problem IDs, the code generated by each LLM, and\nassociated performance metrics, which is available in full for\nreproducibility in [17].\nFigure 4 provides a general overview of the LLMs’ as-\nsertiveness across all tasks and difficulty levels. This initial\nvisualization offers a preliminary look at overall trends, while\nmore detailed analyses follow for each research question (RQ).\n‭This graph seems to‬\n‭capture the results‬\n‭And we can have a‬\n‭separate graph for‬\n‭execution time and‬\n‭similarity score‬\nFig. 4. Overall Success Rate of LLMs.\nFor each research question (RQ), we begin by visualizing\nthe data to provide an intuitive understanding of the perfor-\nmance distributions across different conditions. In addition to\nvisualization and descriptive statistics, we perform hypothesis\ntesting for each RQ.\nA. RQ1: Success Rate of LLMs in Solving Data Science\nProblems\nAs shown in Figure 5, ChatGPT achieves the highest success\nrate (72%), followed by Claude (70%) and Perplexity (66%),\nwith Copilot at 60%. These percentages represent the propor-\ntion of correct solutions generated by each LLM, including\nthose needing minor code edits.\nHypothesis\nTesting: To assess each model’s success\nrate, we conducted a one-tailed binomial test with baseline\nthresholds of 50%, 60%, and 70%, determining if each\nLLM’s success rate significantly exceeded these benchmarks.\nThis non-parametric test, suitable for binary outcomes (cor-\nrect/incorrect), provides insight into each LLM’s performance\nrelative to random chance [13]. Additionally, we evaluated\nCopilot\nChatGPT\nPerplexity\nClaude\nLLM Model\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\n60.0%\n72.0%\n66.0%\n70.0%\nLLM Success Rate in Solving Data Science Coding Problems\nFig. 5. RQ1 - LLM success rate in solving DS coding problems.\nwhether there was a significant difference in success rates\nbetween the LLMs by applying the Friedman test, followed\nby pairwise Wilcoxon tests where a significant difference was\ndetected.\nTABLE II\nRQ1: SUCCESS RATE RESULTS OF LLMS AT DIFFERENT BASELINES\nBaseline\nLLM\nSuccess Rate (%)\np-value\nConclusion\n50%\nCopilot\n60%\n0.0284\nSignificant\nChatGPT\n72%\n0.0000\nSignificant\nPerplexity\n66%\n0.0009\nSignificant\nClaude\n70%\n0.0000\nSignificant\n60%\nCopilot\n60%\n0.5433\nNot Significant\nChatGPT\n72%\n0.0084\nSignificant\nPerplexity\n66%\n0.1303\nNot Significant\nClaude\n70%\n0.0248\nSignificant\n70%\nCopilot\n60%\n0.9875\nNot Significant\nChatGPT\n72%\n0.3768\nNot Significant\nPerplexity\n66%\n0.8371\nNot Significant\nClaude\n70%\n0.5491\nNot Significant\nAs Table II shows, all LLMs perform significantly above the\n50% threshold, confirming baseline effectiveness in solving\ncoding tasks. At the 60% baseline, only ChatGPT and Claude\nreach statistical significance, suggesting enhanced reliability\nfor typical tasks. No LLM achieves significance at the 70%\nbaseline, indicating limitations in sustaining very high success\nrates across diverse challenges.\nCopilot\nChatGPT\nPerplexity\nClaude\nCopilot\nChatGPT\nPerplexity\nClaude\n0.0437\n0.9438\n0.2474\n0.0437\n1.0000\n1.0000\n0.9438\n1.0000\n1.0000\n0.2474\n1.0000\n1.0000\nPairwise Wilcoxon Test Corrected p-values (Bonferroni)\n0.2\n0.4\n0.6\n0.8\n1.0\nCorrected p-value\nFig. 6. RQ1 - Pairwise Comparison of Success Rates.\n\nTo explore differences between LLMs, we applied the\nFriedman test, which detected significant variation in success\nrates across models (p = 0.0384). We followed up with post-\nhoc Wilcoxon pairwise comparisons, identifying a statistically\nsignificant difference between ChatGPT and Copilot, with\nChatGPT achieving a significantly higher success rate (cor-\nrected p-value: 0.0437), as depicted in the heatmap of Figure 6.\nNo other significant differences were observed among models.\nBased on these tests, we conclude:\nFor hypotheses H01 and H01a:\n• At the 50% baseline, all LLMs exhibit success\nrates significantly above 50%, supporting the\nconclusion that each model performs better than\nrandom chance in solving data science coding\nproblems.\n• At the 60% baseline, only ChatGPT and Claude\nshow success rates significantly above this level,\nindicating that these two models exhibit greater\nreliability across general coding tasks.\n• At the 70% baseline, no LLM meets statistical\nsignificance, suggesting a possible limitation in\nachieving consistently high success rates across\ndiverse coding challenges.\n• Friedman Test and Wilcoxon Post-hoc Test:\nSignificant differences were found between mod-\nels, with ChatGPT achieving a success rate\nsignificantly higher than that of Copilot.\nIn summary, RQ1 indicates that ChatGPT and Claude\nexhibit the most consistent performance, particularly ChatGPT,\nwhich leads in relative success. These findings suggest that\nChatGPT and Claude may be preferable for tasks demanding\nhigher success rates, while highlighting the difficulty for LLMs\nin consistently achieving a 70% success rate across diverse\nchallenges.\nB. RQ2: Does the difficulty level of coding problems (easy,\nmedium, hard) influence the success rate of the different\nLLMs?\nEasy\nMedium\nHard\nDifficulty Level\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\n76.5%\n53.1%\n50.0%\n82.4%\n62.5%\n70.6%\n82.4%\n59.4%\n55.9%\n85.3%\n65.6%\n58.8%\nLLM Success Rate by Difficulty Level in Solving DS Coding Problems\nLLM Model\nCopilot\nChatGPT\nPerplexity\nClaude\nFig. 7. RQ2: Effect of difficulty level on success rate.\nAs shown in Figure 7, the success rates of each LLM\nvary across different difficulty levels. Claude achieves the\nhighest success rate on easy and medium problems, while\nChatGPT excels on hard problems, suggesting its robustness\nwith advanced challenges. Copilot consistently shows the\nlowest success rate across all difficulty levels, indicating a\npotential limitation in handling more complex tasks.\nHypothesis Testing: Chi-Square tests were performed to\nevaluate the effect of difficulty level on each LLM’s success\nrate. Results show that difficulty level significantly impacts the\nsuccess rates of Perplexity and Claude (p < 0.05), suggesting\nthat their performance fluctuates with problem complexity. In\ncontrast, Copilot and ChatGPT demonstrate consistent success\nrates across all difficulty levels, indicated by non-significant\nresults.\nBased on these tests, we conclude:\nFor the hypothesis H0 2, we reject it for Perplexity\nand Claude, indicating that difficulty level signifi-\ncantly affects their success rates. For Copilot and\nChatGPT, we fail to reject H0 2, suggesting consis-\ntent performance across varying difficulty levels.\nTo further explore comparative performance, we conducted\nthe Friedman test across all models at each difficulty level.\nAlthough the overall test did not show significant differences\nin success rates across LLMs for each level, pairwise Wilcoxon\ntests highlighted a significant difference between ChatGPT and\nCopilot for hard problems (p = 0.0196), indicating ChatGPT’s\nsuperior performance on more challenging tasks.\nC. RQ3: Does the type of data science task (Analytical,\nAlgorithm, Visualization) influence the success rate of the\ndifferent LLMs?\nAnalytical\nAlgorithm\nVisualization\nTask Type\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\n48.6%\n62.9%\n70.0%\n71.4%\n74.3%\n70.0%\n57.1%\n65.7%\n76.7%\n60.0%\n74.3%\n76.7%\nLLM Success Rate by Task Type in Solving DS Coding Problems\nLLM Model\nCopilot\nChatGPT\nPerplexity\nClaude\nFig. 8. RQ3: Effect of task type on success rate.\nFigure 8 illustrates the success rate of each LLM across\ndifferent task types. ChatGPT demonstrates the highest success\nrate in analytical and algorithm tasks, while Perplexity and\nClaude achieve similar levels in visualization tasks. Although\nChatGPT performs particularly well in analytical and algo-\nrithm tasks, statistical tests reveal no significant overall success\nrate differences among the models across task types, except\nbetween ChatGPT and Copilot.\n\nTABLE III\nRQ3: CHI-SQUARE TEST RESULTS FOR TASK TYPE ACROSS LLMS\nLLM\nAlgorithm\nAnalytical\nVisuali.\np-value\nConclusion\nCopilot\n22/35\n17/35\n21/30\n0.1946\nNot Sig.\nChatGPT\n26/35\n25/35\n21/30\n0.9250\nNot Sig.\nPerplexity\n23/35\n20/35\n23/30\n0.2534\nNot Sig.\nClaude\n26/35\n21/35\n23/30\n0.2715\nNot Sig.\nThe Chi-Square test results in Table III show that task type\ndoes not significantly impact the success rate for any LLM,\nwith all p-values exceeding the 0.05 threshold. This finding\nsuggests that each model’s performance remains relatively\nstable across analytical, algorithmic, and visualization tasks.\nFor hypothesis H0 3, we fail to reject it for all mod-\nels, indicating that task type does not significantly\nimpact success rate overall. However, post-hoc com-\nparisons reveal that ChatGPT performs significantly\nbetter than Copilot in analytical and algorithm tasks.\nD. RQ4: For Analytical questions, do the LLMs differ in the\nefficiency (running time) of the code they generate?\nCopilot\nChatGPT\nPerplexity\nClaude\nLLM Model\n0.001\n0.0025\n0.0050\n0.0075\n0.01\n0.0125\n0.0150\n0.0175\n0.1\nExecution Time (seconds)\nExecution Times for Analytical Questions Across LLMs (Common Questions)\nFig. 9. RQ4: Execution times of LLMs - Box Plot.\nAll (12 questions)\nEasy (7 questions)\nMedium (2 questions)\nHard (3 questions)\nDifficulty Level\n0.004 s\n0.005 s\n0.01 s\n0.02 s\n0.03 s\n6 × 10\n3\nMedian Execution Time (seconds)\nMedian Execution Time for Analytical Tasks by LLM and Difficulty Level (Common Questions)\nLLM Model\nChatGPT\nClaude\nCopilot\nPerplexity\nFig. 10. RQ4: Median execution time by difficulty level.\nFor a fair comparison, Figures 9 and 10 include only the\nresults from problems successfully solved by all LLMs, as\nthe platform does not compute execution times for solutions\nthat did not work. Accordingly, Claude has the lowest median\nexecution time, indicating its solutions generally execute faster\nthan the other models’ solutions, followed by Copilot and\nPerplexity. ChatGPT has the highest median execution time,\nsuggesting that on average, it takes longer to execute analytical\ntasks than the other models. ChatGPT displays the largest\ninterquartile range (IQR), indicating significant variability,\nwhereas Copilot, Perplexity, and Claude have narrower IQRs,\nsuggesting more consistent execution times. This analysis\nsuggests Claude is generally faster and more consistent for\nanalytical tasks, while ChatGPT may offer less predictability\nin execution time.\nHypothesis Testing: To assess whether these observed dif-\nferences are statistically significant, we conducted a Kruskal-\nWallis test, as the Kruskal-Wallis test is a non-parametric\nmethod suitable for comparing the distributions of independent\ngroups, particularly their central tendencies, when data is not\nnormally distributed.\nThe test, conducted using the scipy.stats library, re-\nsulted in a Kruskal-Wallis statistic of 0.6947 and a p-value\nof 0.8744. With a p-value exceeding the significance level of\n0.05, we fail to reject the null hypothesis. This suggests that\nthere are no statistically significant differences in the median\nexecution times across the LLMs for Analytical questions.\nFor RQ4, we fail to reject H0 4, indicating that\nthe LLMs do not differ significantly in the efficiency\n(running time) of the code they generate for Analyt-\nical questions.\nE. RQ5: For visualization tasks, do the LLMs differ in the\nquality (similarity) of the visual outputs they produce com-\npared to expected results?\nCopilot\nChatGPT\nPerplexity\nClaude\nLLM Model\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nSimilarity Score\nSimilarity Scores for Visualization Tasks Across LLMs (Common Questions)\nFig. 11. RQ5: Similarity Scores - Box Plot.\nAs depicted in Figures 11 and 12, ChatGPT achieves the\nhighest median similarity score among the commonly solved\nproblems, indicating that its outputs are closest to the ex-\npected results. Additionally, ChatGPT displays the narrowest\ninterquartile range (IQR), highlighting its consistency. These\nfindings suggest that ChatGPT delivers more reliable quality\nin generating visual outputs that closely match the expected\nresults.\n\nAll (14 questions)\nEasy (6 questions)\nMedium (4 questions)\nHard (4 questions)\nDifficulty Level\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nMedian Similarity Score\nMedian Similarity Score for Visualization Tasks by LLM and Difficulty Level (Common Questions)\nLLM Model\nChatGPT\nClaude\nCopilot\nPerplexity\nFig. 12. RQ5: Median similarity scores by difficulty level.\nHypothesis Testing: To statistically analyze differences in\nsimilarity scores among the LLMs, we conducted a Kruskal-\nWallis test.\nKruskal-Wallis Test Results:\n• Kruskal-Wallis Statistic: 0.8287\n• p-value: 0.8426\n• Conclusion: The p-value above 0.05 suggests no statisti-\ncally significant differences in similarity scores between\nthe LLMs. This indicates that while there are observed\ndifferences in mean similarity scores and variability (with\nChatGPT achieving the highest mean and most consistent\nperformance), these differences are not statistically sig-\nnificant across LLMs at the 5% significance level.\nBased on these results, we conclude:\nFor hypothesis H0 5, we fail to reject the null\nhypothesis, indicating that there is no significant\ndifference in the similarity quality of generated vi-\nsualization outputs among the LLMs.\nF. Threats to Validity\nAs usual in empirical studies, our study acknowledges\nseveral threats that may impact the interpretation and general-\nization of the results.\n1) Internal Validity: A key concern is the undisclosed\nnature of the LLMs’ training data. Without access to this in-\nformation, we cannot confirm whether the generated solutions\nare novel or based on memorized content. Even though we\nselected new problems from Stratascratch, similar or identical\nproblems might exist in the models’ training data, potentially\ninflating their apparent effectiveness.\nPrompt design is another factor influencing outcomes. As\nnoted by White et al. [18], the formulation of prompts can\nsignificantly affect LLM outputs. While we endeavored to use\nconsistent prompts derived from original problem descriptions,\nvariations could lead to different results.\nTo address potential subjectivity in converting problems\nto prompts, we developed standardized prompt templates for\neach task type. These templates ensured that all AI assistants\nreceived clear, consistent instructions, allowing for a fair\ncomparison of performance.\n2) External Validity: The generalizability of our findings is\nlimited by the scope of problems used. Our study focused on\n100 Python coding problems from a single platform, which\nmay not represent the full spectrum of data science tasks. To\nenhance external validity, future research should incorporate a\nwider range of problems from multiple sources.\n3) Construct Validity: We did not formally assess the\nexpertise of the researchers conducting the experiment, which\ncould introduce subjectivity, particularly in interpreting and\nevaluating the AI-generated code. Although guidelines were\nestablished for acceptable code modifications—allowing only\nminor edits to resolve execution issues—differences in coding\nproficiency among researchers could influence the assessment.\n4) Conclusion Validity: These threats may affect the valid-\nity of our conclusions. While our study offers insights into\nthe capabilities and limitations of LLMs in data science code\ngeneration, the results should be interpreted with caution.\nFurther research addressing these limitations is necessary to\nstrengthen the confidence in the findings.\nVI. DISCUSSION\nThrough a series of hypothesis tests, we investigated each\nmodel’s effectiveness across different problem types and dif-\nficulty levels. The findings underscore both the strengths\nand limitations of these LLMs in addressing data science\nchallenges, providing insights into which models may be most\nsuitable for specific scenarios in data science workflows. The\nresults highlight that:\n• Success Rate: Empirical evidence from our tests indi-\ncates that each LLM exceeded the 50% baseline success\nrate, confirming effectiveness beyond random chance. At\nthe 60% baseline, only ChatGPT and Claude achieved\nsignificantly higher success rates, reinforcing their relia-\nbility in general coding contexts. However, none of the\nmodels reached the 70% threshold, suggesting limitations\nin consistently achieving high accuracy across diverse\ndata science task types. ChatGPT achieved the highest\noverall success rate and performed consistently well on\nharder questions, with descriptive analysis suggesting\nstrong outcomes in analytical and algorithmic tasks, re-\nflecting its robustness in complex data science scenarios.\nClaude also demonstrated solid performance, particularly\non easier and medium-difficulty tasks, as well as in\nvisualization tasks, indicating versatility across various\nproblem types. Perplexity and Copilot, while showing\nlower success rates on more complex tasks, displayed\nconsistent performance on simpler tasks, highlighting\ntheir potential for straightforward data science workflows.\n• Efficiency (Execution Time): For analytical tasks, the\nKruskal-Wallis test on execution times revealed no sta-\ntistically significant differences among the models, sug-\ngesting that efficiency, in terms of runtime, is relatively\ncomparable across these LLMs. This finding implies\nthat while execution time may vary, it may not be\n\na decisive factor in model selection for tasks where\naccuracy and complexity are primary concerns. Despite\nthe lack of empirical significance, the median execution\ntimes indicate some practical trends: Claude had the\nlowest median execution time, suggesting it generally\nruns faster than the other models, followed by Copilot and\nPerplexity. ChatGPT had the highest median execution\ntime, indicating slower performance on average.\n• Quality of Output (Image Similarity for Visualiza-\ntion): In visualization tasks, where models were eval-\nuated based on similarity scores to expected outputs,\nChatGPT achieved the highest median similarity score.\nHowever, statistical tests indicated no significant differ-\nences between the models.\n• Consistency Across Difficulty Levels and Task Types:\nEmpirical analysis reveals that ChatGPT maintains con-\nsistent performance regardless of task complexity, provid-\ning reliable success rates across both simple and complex\ntasks. In contrast, Perplexity’s and Claude’s success rates\nwere significantly influenced by task difficulty, with better\noutcomes on less complex tasks. Copilot also demon-\nstrated consistency across difficulty levels, though with\ngenerally lower success rates than ChatGPT. Additionally,\nour tests indicate that task type (analytical, algorithmic,\nvisualization) does not significantly impact success rates\nfor any model, suggesting stable performance across\ndifferent data science task types. This consistency makes\nChatGPT a dependable choice when task complexity is\nuncertain.\nVII. CONCLUSION\nThis study presents a controlled experiment evaluating\nthe effectiveness of four prominent LLM-based AI as-\nsistants—Microsoft Copilot (GPT-4 Turbo), ChatGPT (o1-\npreview), Claude (3.5 Sonnet), and Perplexity Labs (Llama-\n3.1-70b-instruct)—in data science coding tasks. Effectiveness\nwas measured by each model’s success rate, execution effi-\nciency, visual output quality, and consistency across difficulty\nlevels and task types.\nWith success rates exceeding 50% for all models, this\nresearch provides valuable insights into LLM performance in\ndata science. At the 60% baseline, only ChatGPT and Claude\nachieved significantly higher success rates, highlighting their\nreliability in general coding tasks. However, our findings indi-\ncate that only ChatGPT consistently maintains performance\nacross different difficulty levels, whereas Claude’s success\nrate is significantly affected by task difficulty, suggesting its\nperformance may vary with more complex tasks.\nNo evidence suggests that task type affects LLM success\nrates, though ChatGPT (o1-preview) significantly outperforms\nCopilot (GPT-4o) for analytical and algorithm tasks. This\nnuanced understanding of each model’s strengths enables more\nstrategic LLM selection tailored to specific needs. Addition-\nally, this study underscores the value of rigorous hypothesis\ntesting in AI evaluation, setting a template for assessing\nmodels beyond basic accuracy metrics.\nVIII. FUTURE WORK\nOur study opens several avenues for future research to en-\nhance the application of LLMs in data science code generation.\nA. Exploring Complex and Real-World Data Science Tasks\nEvaluating LLMs on sophisticated, real-world data science\ntasks—such as implementing machine learning models with li-\nbraries like Scikit-learn or TensorFlow, handling large datasets,\nand working with unstructured data—could provide deeper\ninsights into their capabilities and limitations. For instance,\nNascimento et al. [19] demonstrated the use of an LLM to\nreplace a learning algorithm that involved neural networks\noptimized through genetic algorithms. While their experiment\nwas preliminary, it highlighted the potential of LLMs to\nautomate complex coding solutions, suggesting that these\nmodels could extend beyond basic scripting to more advanced\ntasks. Exploring tasks like multivariate analysis, time series\nforecasting, and dynamic optimization could further test LLM\nproficiency. Testing in practical settings uncovers challenges\nthat controlled experiments may not fully capture.\nB. Expanding Model Diversity and Dataset Coverage\nWe could extend this analysis by integrating additional\nLLMs and incorporating data science-specific coding chal-\nlenges from various platforms, such as LeetCode, with tasks\nlike data manipulation, cleaning, and SQL queries. To capture\na broader range of data science skills, the dataset could also\ninclude non-coding tasks, such as interpretation and analysis\nquestions, as provided by Stratascratch.\nAdditionally, we could integrate recently released questions\nin Stratascratch that use Polars DataFrame [20] for data\nmanipulation-a high-performance library designed for efficient\ndata handling in Python. We could further expand the dataset\nby incorporating problems from DS-1000 [4], which includes\na diverse selection of data science problems sourced from\nStackOverflow. Following recommendations by Lai et al. [4],\nintroducing customized variations of existing problems would\nhelp reduce model memorization, enhancing the rigor of the\nevaluation environment.\nC. Expanding Evaluation Metrics\nFuture work could expand LLM evaluation by integrating\nsoftware engineering metrics like code complexity, maintain-\nability, and readability. Code similarity analysis could assess\nalignment with industry standards, while qualitative reviews\nby data scientists would add valuable insights, particularly for\nvisualization tasks where image similarity metrics may fall\nshort.\nD. Investigating Prompt Engineering and Ensuring Repro-\nducibility\nPrompt engineering significantly influences LLM outputs.\nFuture research should examine how different prompt for-\nmulations affect code generation quality and consistency.\nEmploying methodologies where LLMs simulate multiple\n\nusers [21] could shed light on the impact of varying profes-\nsional experiences and prompt designs. Addressing the non-\ndeterministic nature of LLMs by controlling parameters like\ntemperature settings could improve reproducibility, leading to\nmore consistent and reliable evaluations.\nE. Exploring Further Research Questions\nEven using the same dataset, many additional questions\nand hypotheses remain to be explored. For instance, while\nwe assessed the impact of problem difficulty and type on\nLLM success rates, further analysis could focus on establishing\nbaseline success rates for each problem type and difficulty\nlevel. Given the general success baseline of 60%, future\nresearch might explore optimal baseline thresholds specific to\neach type and level of task. Beyond success rates, this dataset\nalso allows for an in-depth exploration of efficiency (execution\ntimes) and similarity scores for each difficulty level, providing\na more comprehensive view of model performance in diverse\ntask complexity. Additionally, information on the number of\nattempts (up to three) and instances of minor edits provides\ndata for assessing error types (syntax and logic errors), retry\npatterns, and the models’ adaptability to user feedback. Our\ndataset also includes specific topics within each question type,\nallowing for a more granular analysis that could reveal topic-\nspecific strengths or limitations of each LLM.\nREFERENCES\n[1] A. Halevy, Y. Choi, A. Floratou, M. J. Franklin, N. Noy, and H. Wang,\n“Will llms reshape, supercharge, or kill data science?(vldb 2023 panel),”\nProceedings of the VLDB Endowment, vol. 16, no. 12, pp. 4114–4115,\n2023.\n[2] N. Nascimento, C. Tavares, P. Alencar, and D. Cowan, “Gpt in data\nscience: A practical exploration of model selection,” in 2023 IEEE\nInternational Conference on Big Data (BigData).\nIEEE, 2023, pp.\n4325–4334.\n[3] J. Li, B. Hui, G. Qu, J. Yang, B. Li, B. Li, B. Wang, B. Qin, R. Geng,\nN. Huo et al., “Can llm already serve as a database interface? a big bench\nfor large-scale database grounded text-to-sqls,” Advances in Neural\nInformation Processing Systems, vol. 36, 2024.\n[4] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-\nt. Yih, D. Fried, S. Wang, and T. Yu, “Ds-1000: A natural and\nreliable benchmark for data science code generation,” in International\nConference on Machine Learning.\nPMLR, 2023, pp. 18 319–18 345.\n[5] M. Kazemitabaar, J. Williams, I. Drosos, T. Grossman, A. Z. Henley,\nC. Negreanu, and A. Sarkar, “Improving steering and verification in ai-\nassisted data analysis with interactive task decomposition,” in Proceed-\nings of the 37th Annual ACM Symposium on User Interface Software\nand Technology, 2024, pp. 1–19.\n[6] N. Nguyen and S. Nadi, “An empirical evaluation of github copilot’s\ncode suggestions,” in Proceedings of the 19th International Conference\non Mining Software Repositories, 2022, pp. 1–5.\n[7] N. Nathalia, A. Paulo, and C. Donald, “Artificial intelligence vs. software\nengineers: An empirical study on performance and efficiency using\nchatgpt,” in Proceedings of the 33rd Annual International Conference\non Computer Science and Software Engineering, 2023, pp. 24–33.\n[8] M. A. Kuhail, S. S. Mathew, A. Khalil, J. Berengueres, and S. J. H. Shah,\n““will i be replaced?” assessing chatgpt’s effect on software develop-\nment and programmer perceptions of ai tools,” Science of Computer\nProgramming, vol. 235, p. 103111, 2024.\n[9] T. Coignion, C. Quinton, and R. Rouvoy, “A performance study of llm-\ngenerated code on leetcode,” in Proceedings of the 28th International\nConference on Evaluation and Assessment in Software Engineering,\n2024, pp. 79–89.\n[10] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,\nH. Edwards, Y. Burda, N. Joseph, G. Brockman et al., “Evaluating large\nlanguage models trained on code,” arXiv preprint arXiv:2107.03374,\n2021.\n[11] B. Grewal, W. Lu, S. Nadi, and C.-P. Bezemer, “Analyzing developer\nuse of chatgpt generated code in open source github projects,” in\n2024 IEEE/ACM 21st International Conference on Mining Software\nRepositories (MSR).\nIEEE, 2024, pp. 157–161.\n[12] X. Gu, M. Chen, Y. Lin, Y. Hu, H. Zhang, C. Wan, Z. Wei, Y. Xu, and\nJ. Wang, “On the effectiveness of large language models in domain-\nspecific code generation,” ACM Transactions on Software Engineering\nand Methodology, 2024.\n[13] C. Wohlin, P. Runeson, M. H¨ost, M. C. Ohlsson, B. Regnell, A. Wessl´en\net al., Experimentation in software engineering.\nSpringer, 2012, vol.\n236.\n[14] StrataScratch,\n“Master\ncoding\nfor\ndata\nscience,”\nhttps:\n//www.stratascratch.com/, n.d., accessed: 2024-11-01.\n[15] C. Troy, S. Sturley, J. M. Alcaraz-Calero, and Q. Wang, “Enabling\ngenerative ai to produce sql statements: A framework for the auto-\ngeneration of knowledge based on ebnf context-free grammars,” IEEE\nAccess, vol. 11, pp. 123 543–123 564, 2023.\n[16] M. Malekpour, N. Shaheen, F. Khomh, and A. Mhedhbi, “Towards\noptimizing sql generation via llm routing,” in NeurIPS 2024 Third Table\nRepresentation Learning Workshop.\n[17] S.\nA.\nBoominathan,\nS.\nS.\nChintakunta,\nN.\nNascimento,\nand\nE. Guimaraes, “LLM4DS-Benchmark: A Dataset for Assessing LLM\nPerformance in Data Science Coding Tasks,” Nov. 2024. [Online].\nAvailable: https://doi.org/10.5281/zenodo.14064111\n[18] J. White, S. Hays, Q. Fu, J. Spencer-Smith, and D. C. Schmidt, “Chatgpt\nprompt patterns for improving code quality, refactoring, requirements\nelicitation, and software design,” 2023.\n[19] N. Nascimento, P. Alencar, and D. Cowan, “Gpt-in-the-loop: Supporting\nadaptation in multiagent systems,” in 2023 IEEE International Confer-\nence on Big Data (BigData).\nIEEE, 2023, pp. 4674–4683.\n[20] Ritchie Vink, “Polars: Blazingly fast dataframes in rust, python, node.js,\nr, and sql,” 2023. [Online]. Available: https://github.com/pola-rs/polars\n[21] G. V. Aher, R. I. Arriaga, and A. T. Kalai, “Using large language models\nto simulate multiple humans and replicate human subject studies,” in\nInternational Conference on Machine Learning.\nPMLR, 2023, pp.\n337–371.",
    "pdf_filename": "LLM4DS_Evaluating_Large_Language_Models_for_Data_Science_Code_Generation.pdf"
}