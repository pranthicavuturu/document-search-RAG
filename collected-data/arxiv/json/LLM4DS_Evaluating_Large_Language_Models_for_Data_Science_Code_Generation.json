{
    "title": "LLM4DS: Evaluating Large Language Models for",
    "abstract": "codegenerationindatascienceofferssubstantialpotentialforen- a thorough evaluation. hancingtaskssuchasdatamanipulation,statisticalanalysis,and While previous studies have evaluated LLMs in general visualization. However, the effectiveness of these models in the programming tasks using platforms like LeetCode [6]–[9], datasciencedomainremainsunderexplored.Thispaperpresents acontrolledexperimentthatempiricallyassessestheperformance the HumanEval benchmark [10], and GitHub Projects [11], of four leading LLM-based AI assistants—Microsoft Copilot Gu et al. [12] identified a notable gap in approaches to (GPT-4Turbo),ChatGPT(o1-preview),Claude(3.5Sonnet),and evaluate domain-specific code generation. They demonstrated Perplexity Labs (Llama-3.1-70b-instruct)—on a diverse set of that LLMs exhibit sub-optimal performance in generating data science coding challenges sourced from the Stratacratch domain-specific code for areas such as web and game devel- platform. Using the Goal-Question-Metric (GQM) approach, we evaluated each model’s effectiveness across task types (Analyti- opment, due to their limited proficiency in utilizing domain- cal, Algorithm, Visualization) and varying difficulty levels. Our specific libraries. This finding underscores the need for more findings reveal that all models exceeded a 50% baseline success focused evaluations that consider the unique challenges of rate,confirmingtheircapabilitybeyondrandomchance.Notably, specialized domains like data science, which involve tasks only ChatGPT and Claude achieved success rates significantly suchashandlingdatasets,performingcomplexstatisticalanal- abovea60%baseline,thoughnoneofthemodelsreacheda70% threshold, indicating limitations in higher standards. ChatGPT yses, and generating insightful visualizations—areas not fully demonstrated consistent performance across varying difficulty represented in general programming assessments. levels,whileClaude’ssuccessratefluctuatedwithtaskcomplexity. This paper addresses this gap by providing an empirical Hypothesis testing indicates that task type does not significantly evaluation [13] of multiple LLMs on diverse data science- impact success rate overall. For analytical tasks, efficiency anal- ysis shows no significant differences in execution times, though specific coding problems sourced from the Stratascratch plat- ChatGPT tended to be slower and less predictable despite high form[14].Thecontrolledexperimentinvolvesfourmainsteps: success rates. For visualization tasks, while similarity quality (i) selecting 100 Python coding problems from Stratascratch, among LLMs is comparable, ChatGPT consistently delivered distributed across three difficulty levels (easy, medium, hard) the most accurate outputs. This study provides a structured, and three problem types (Analytical, Algorithm, Visualiza- empiricalevaluationofLLMsindatascience,deliveringinsights that support informed model selection tailored to specific task tion); (ii) transforming these problems into prompts following demands. Our findings establish a framework for future AI the optimal prompt structure for each type; (iii) using these assessments,emphasizingthevalueofrigorousevaluationbeyond prompts for each AI assistant to generate code solutions; basic accuracy measures. and (iv) evaluating the generated code based on correctness, Index Terms—data science, large language model, coding efficiency, and other relevant metrics. generation, empirical study, hypothesis testing Our research seeks to answer the following question: How effectiveareLLMsfordatasciencecoding?Bysystematically I. INTRODUCTION assessing the performance of these AI assistants, we aim to identify their strengths and limitations in automating code Large Language Models (LLMs) have emerged as transfor- generation for data science problems. mativetoolswiththepotentialtorevolutionizecodegeneration Our contributions are multifold: in various domains, including data science [1]–[5]. Their ability to generate human-like text and code opens up pos- 1) WeprovideanempiricalevaluationofmultipleLLMson sibilities for automating complex tasks in data manipulation, data science-specific coding problems, filling a critical visualization, and analytics. As data science projects often gap in current research. require extensive coding efforts that are time-consuming and 2) We assess Stratacratch as a platform to benchmark demand significant expertise, leveraging LLMs could greatly LLMs for data science code generation, evaluating its enhance productivity and accessibility in this field. However, suitability and potential as a standardized dataset for the effectiveness and reliability of LLM-generated code for LLM performance in this domain. 4202 voN 61 ]ES.sc[ 1v80911.1142:viXra",
    "body": "LLM4DS: Evaluating Large Language Models for\nData Science Code Generation\nNathalia Nascimento Everton Guimaraes Sai Sanjna Chintakunta Santhosh Anitha Boominathan\nEASER, Eng. Division EASER, Eng. Division EASER, Eng. Division EASER, Eng. Division\nPennsylvania State University Pennsylvania State University Pennsylvania State University Pennsylvania State University\nGreat Valley, USA Great Valley, USA Great Valley, USA Great Valley, USA\nnqm5742@psu.edu ezt157@psu.edu sqc6557@psu.edu sfa5971@psu.edu\nAbstract—TheadoptionofLargeLanguageModels(LLMs)for data science applications remain underexplored, necessitating\ncodegenerationindatascienceofferssubstantialpotentialforen- a thorough evaluation.\nhancingtaskssuchasdatamanipulation,statisticalanalysis,and\nWhile previous studies have evaluated LLMs in general\nvisualization. However, the effectiveness of these models in the\nprogramming tasks using platforms like LeetCode [6]–[9], datasciencedomainremainsunderexplored.Thispaperpresents\nacontrolledexperimentthatempiricallyassessestheperformance the HumanEval benchmark [10], and GitHub Projects [11],\nof four leading LLM-based AI assistants—Microsoft Copilot Gu et al. [12] identified a notable gap in approaches to\n(GPT-4Turbo),ChatGPT(o1-preview),Claude(3.5Sonnet),and evaluate domain-specific code generation. They demonstrated\nPerplexity Labs (Llama-3.1-70b-instruct)—on a diverse set of\nthat LLMs exhibit sub-optimal performance in generating\ndata science coding challenges sourced from the Stratacratch\ndomain-specific code for areas such as web and game devel-\nplatform. Using the Goal-Question-Metric (GQM) approach, we\nevaluated each model’s effectiveness across task types (Analyti- opment, due to their limited proficiency in utilizing domain-\ncal, Algorithm, Visualization) and varying difficulty levels. Our specific libraries. This finding underscores the need for more\nfindings reveal that all models exceeded a 50% baseline success focused evaluations that consider the unique challenges of\nrate,confirmingtheircapabilitybeyondrandomchance.Notably,\nspecialized domains like data science, which involve tasks\nonly ChatGPT and Claude achieved success rates significantly\nsuchashandlingdatasets,performingcomplexstatisticalanal-\nabovea60%baseline,thoughnoneofthemodelsreacheda70%\nthreshold, indicating limitations in higher standards. ChatGPT yses, and generating insightful visualizations—areas not fully\ndemonstrated consistent performance across varying difficulty represented in general programming assessments.\nlevels,whileClaude’ssuccessratefluctuatedwithtaskcomplexity.\nThis paper addresses this gap by providing an empirical\nHypothesis testing indicates that task type does not significantly\nevaluation [13] of multiple LLMs on diverse data science-\nimpact success rate overall. For analytical tasks, efficiency anal-\nysis shows no significant differences in execution times, though specific coding problems sourced from the Stratascratch plat-\nChatGPT tended to be slower and less predictable despite high form[14].Thecontrolledexperimentinvolvesfourmainsteps:\nsuccess rates. For visualization tasks, while similarity quality (i) selecting 100 Python coding problems from Stratascratch,\namong LLMs is comparable, ChatGPT consistently delivered\ndistributed across three difficulty levels (easy, medium, hard)\nthe most accurate outputs. This study provides a structured,\nand three problem types (Analytical, Algorithm, Visualiza-\nempiricalevaluationofLLMsindatascience,deliveringinsights\nthat support informed model selection tailored to specific task tion); (ii) transforming these problems into prompts following\ndemands. Our findings establish a framework for future AI the optimal prompt structure for each type; (iii) using these\nassessments,emphasizingthevalueofrigorousevaluationbeyond prompts for each AI assistant to generate code solutions;\nbasic accuracy measures.\nand (iv) evaluating the generated code based on correctness,\nIndex Terms—data science, large language model, coding\nefficiency, and other relevant metrics.\ngeneration, empirical study, hypothesis testing\nOur research seeks to answer the following question: How\neffectiveareLLMsfordatasciencecoding?Bysystematically\nI. INTRODUCTION assessing the performance of these AI assistants, we aim to\nidentify their strengths and limitations in automating code\nLarge Language Models (LLMs) have emerged as transfor-\ngeneration for data science problems.\nmativetoolswiththepotentialtorevolutionizecodegeneration\nOur contributions are multifold:\nin various domains, including data science [1]–[5]. Their\nability to generate human-like text and code opens up pos- 1) WeprovideanempiricalevaluationofmultipleLLMson\nsibilities for automating complex tasks in data manipulation, data science-specific coding problems, filling a critical\nvisualization, and analytics. As data science projects often gap in current research.\nrequire extensive coding efforts that are time-consuming and 2) We assess Stratacratch as a platform to benchmark\ndemand significant expertise, leveraging LLMs could greatly LLMs for data science code generation, evaluating its\nenhance productivity and accessibility in this field. However, suitability and potential as a standardized dataset for\nthe effectiveness and reliability of LLM-generated code for LLM performance in this domain.\n4202\nvoN\n61\n]ES.sc[\n1v80911.1142:viXra\n3) We analyze the success rate of these models across CodeGen, with the best accuracy being 43.3% achieved by\ndifferent task categories—Analytical, Algorithm, and Codex-002.However,whileDS-1000providesarobustdataset\nVisualization—and difficulty levels, offering insights for testing, Lai et al. do not perform a comparative empirical\ninto their practical utility in data science workflows. evaluationacrossmultipleLLMs,leavingopenquestionsabout\n4) We highlight the challenges and limitations of LLMs in how current models fare on this benchmark.\nthis domain, providing a foundation for future improve- Despite these advancements, much of the current research\nments and research in AI-assisted data science. hasbeenlimitedtoeithergeneralcodingtasksorSQL-specific\nThis paper is organized as follows. Section 2 presents the applications. The nuances of data science problems—ranging\nrelated work. Section 3 describes the controlled experiment, from data manipulation and complex analyses to visualiza-\noutliningtheresearchquestions,hypotheses,andmethodology. tion—remain underexplored in LLM evaluations. Our work\nSection 4-5 presents the experimental results and discusses addresses this gap by conducting an empirical experiment\nthreats to validity. Sections 6-8 brings final remarks and using four leading LLMs on a set of data science problems\nsuggestions for future work. extracted from the Stratacratch dataset, encompassing various\ndifficulty levels and problem types. Unlike prior studies,\nII. RELATEDWORK\nwhich primarily introduce benchmarks or focus on specific\nIntherealmofcodegeneration,priorstudieshaveevaluated task categories, our approach offers a detailed examination of\nLLMslikeChatGPTandGitHubCopilotusingplatformssuch LLM performance across a broader spectrum of data science\nasHumanEvalBenchmark,LeetCode,andGithub.Nascimento challenges.\net al. [7] compared code generated by ChatGPT against\nhuman-written solutions, assessing performance and memory\nIII. CONTROLLEDEXPERIMENT\nefficiency. Kuhail et al. [8] evaluated ChatGPT on 180 Leet- In line with the controlled experiment methodology by\nCode problems, providing insights into its capabilities and Wohlin et al. [13], our study aims to evaluate and com-\nlimitations. Coignion et al. [9] investigated different LLMs pare the effectiveness of four prominent LLM-based AI as-\non general coding problems from LeetCode, focusing on sistants—Microsoft Copilot (GPT-4 Turbo), ChatGPT (o1-\nperformance metrics. Nguyen and Nadi [6] assessed GitHub preview),Claude(3.5Sonnet),andPerplexityLab(Llama-3.1-\nCopilot’s code generation on 33 LeetCode problems, evaluat- 70b-instruct)—in solving data science coding tasks sourced\ning correctness and understandability. from the Stratascratch platform [14].\nBeyond traditional programming tasks, LLMs have been Effectiveness in this context refers to the degree to which\nappliedindatascience-specificdomains,whererecentresearch these models achieve desired outcomes across four key as-\nhas explored the models’ capacity to handle complex queries pects: success rate, efficiency, quality of output, and consis-\nand data manipulation tasks. Troy et al. [15] demonstrated tency. Specifically, we define:\nthat LLMs could generate SQL statements for cybersecurity • Success Rate as the proportion of correctly generated\napplications, specifically highlighting their capability in struc- code solutions, measured by the percentage of solutions\ntured query generation. In another study, Malekpour et al. that achieve the correct result regardless of the number\n[16] introduced an LLM routing framework designed for text- of attempts;\nto-SQL tasks, optimizing the selection of models based on • Efficiency as the runtime execution speed of the gener-\ncost-efficiencyandaccuracy.Lietal.[3]identifiedlimitations ated solution;\neveninadvancedmodelslikeGPT-4,notingthatthesemodels • Quality of Output as the alignment of generated solu-\nachievedonly54.89%executionaccuracyoncomplextext-to- tions with expected outcomes, particularly for visualiza-\nSQL queries—significantly below the human benchmark of tion tasks;\n92.96%. Additionally, Kazemitabaar et al. [5] delved into the • Consistency as the reliability of each model’s perfor-\nchallenges of data analysis with conversational AI tools like mance across varying difficulty levels and task types.\nChatGPT, identifying difficulties users face in verifying and\nguiding AI-generated results for desired outcomes. A. Research Questions, Hypotheses, and Metrics\nLai et al. [4] proposed the DS-1000 benchmark, a dataset To systematically explore effectiveness, we structured our\nspecifically crafted for evaluating code generation in data investigation around specific research questions, each accom-\nscience contexts. DS-1000 comprises 451 unique data science panied by testable hypotheses and relevant evaluation metrics.\nproblems sourced from StackOverflow and spans seven es- Table I details these research questions, hypotheses, and cor-\nsential Python libraries, including Numpy and Pandas. A key responding metrics.\nfeature of this benchmark is its emphasis on problem per-\nB. Variables Selection\nturbations, aimed at reducing the risk of model memorization.\nThedatasetaccountsfortheuniquechallengesofdatascience To structure our analysis, we identified key variables that\ntasks, which often lack executable contexts, may depend on allow us to examine the performance of each AI assistant\nexternal libraries, and can have multiple correct solutions. Lai across different problem types and difficulty levels.\net al. demonstrated the effect of different types of problem The independent variables in this study, which we con-\nperturbations by testing models like Codex, InCoder, and trolled or varied, include:\nTABLEI\nRESEARCHQUESTIONS,HYPOTHESES,ANDMETRICS\nResearchQuestion NullHypothesis AlternativeHypothesis Metrics\nRQ1: How successful are LLMs in solving H0 1: The success rate of each H1 1: The success rate of Overallsuccessrate(percentage\ndata science coding problems, and do they LLM in solving data science each LLM in solving data sci- of correct solutions) and pair-\noutperformeachotherinsuccessrate? coding problems is not sig- ence coding problems is sig- wisesuccessratecomparisons\nnificantly higher than random nificantly higher than random\nchance(50%). chance(50%).\nH0 1a: There is no significant H1 1a: At least one pair of\ndifference in success rates be- LLMsshowsasignificantdiffer-\ntweenLLMpairs. enceinsuccessrates.\nRQ2: Does the difficulty level of coding H0 2: Difficulty level does not H1 2: The success rate of the Successrate(percentageofcor-\nproblems (easy, medium, hard) influence the significantly affect the success LLMs varies significantly with rect solutions) across difficulty\nsuccess rate of the different LLMs, and do rateoftheLLMs. difficultylevel. levels and pairwise success rate\nspecificLLMsoutperformothersateachdif- H0 2a: There is no significant H1 2a: At least one pair of comparisonswithineachlevel\nficultylevel? difference in success rates be- LLMs shows a significant dif-\ntween LLM pairs within each ference in success rate within a\ndifficultylevel. specificdifficultylevel.\nRQ3:Doesthetypeofdatasciencetask(An- H0 3: The type of data science H1 3: The success rate of the Successrate(percentageofcor-\nalytical, Algorithm, Visualization) influence task does not significantly im- LLMs varies significantly with rectsolutions)foreachtasktype\nthesuccessrateofthedifferentLLMs,anddo pacttheLLMs’successrate. thetypeofdatasciencetask. and pairwise success rate com-\nspecific LLMs outperform others for certain H0 3a: There is no significant H1 3a: At least one pair of parisonswithineachtype\ntasktypes? difference in success rates be- LLMs shows a significant dif-\ntween LLM pairs within each ference in success rate within a\ntasktype. specifictasktype.\nRQ4:ForAnalyticalquestions,dotheLLMs H0 4: The population medians H1 4: At least one LLM has a Execution time for each gener-\ndifferintheefficiency(executiontime)ofthe oftheexecutiontimesacrossthe differentpopulationmedianexe- atedsolutiononAnalyticalprob-\ncodetheygenerate? LLMs for Analytical questions cutiontimeforAnalyticalques- lems,perLLM\nareequal. tionscomparedtoothers.\nRQ5: For visualization tasks, do the LLMs H0 5: The population medians H1 5: At least one LLM has Similarity scores for visualiza-\ndifferinthequality(similarity)ofthevisual of the similarity scores for vi- a different population median tion outputs compared to ex-\noutputs they produce compared to expected sualization outputs across the similarityscoreforvisualization pectedresults\nresults? LLMsareequal. outputscomparedtoothers.\n• LLM-based AI assistants: The four AI models under functionality. The researchers recorded whether the solution\nevaluation—Microsoft Copilot, ChatGPT, Claude, and worked as intended and noted any necessary adjustments.\nPerplexity Lab. SinceStratascratchprovidesexecutiontimeonlyforAnalytical\n• Difficulty level of coding problems: Easy, Medium, questionsandsimilarityscoresforVisualizationquestions,we\nHard. collected these specific measurements accordingly.\n• Type of Data Science task: Analytical, Algorithm, Vi- The overall process of our controlled experiment consists\nsualization. of 11 steps, as illustrated in Figure 1:\nThe dependent variables are the metrics we measured to 1) Select the problem source: We chose Stratascratch [14]\nassess each AI assistant’s effectiveness: as the platform for sourcing data science problems.\n• Success rate: The percentage of correct solutions gener- 2) Select one problem per task category for prompt engi-\nated byeach LLM, regardless ofthe number ofattempts. neering: One problem from each data science task cate-\n• Running time: Execution time of code for Analytical gory(Analytical,Algorithm,Visualization)wasselected\nquestions. to refine our prompt templates.\n• Graph similarity scores: Similarity between generated 3) Prompt Engineering with feedback loop: We performed\nand expected graphs for Visualization questions. prompt engineering by iteratively adjusting the prompts\nThese variables connect directly to the research questions and assessing the performance of different LLM ver-\nand metrics outlined in Table I, allowing us to systematically sions, creating optimal prompt structures for each task\ninvestigate the impact of each independent variable on the AI type.\nmodels’ performance. 4) SelectionofAIassistantsandLLMs:FourAIassistants,\neach utilizing a different LLM, were selected for the\nIV. EXPERIMENTOPERATION\nexperiment.\nTheexperimentwasconductedoverthespanoftwomonths. 5) Definition of final prompts: The finalized prompt tem-\nForeachofthefourLLMs,wegeneratedasolutionforeachof plates were established for each problem type based on\nthe100selectedproblems,resultinginatotalof400generated the prompt engineering process.\ncoding solutions. Two researchers manually interacted with 6) Selection of 100 Data Science problems: We selected\ntheAIassistantsbyinputtingthepromptsintotheirrespective 100datascienceproblemscoveringvarioustopicsacross\ninterfaces.Theythencopiedthegeneratedcodeandsubmitted the three task types to ensure a comprehensive evalua-\nit to the Stratascratch platform to assess its correctness and tion.\n7) Creation of prompts: The selected problems were in- culty levels (easy, medium, hard) and types (Analytical,\ncorporated into the prompt templates, resulting in 100 Algorithm, Visualization).\ntailored prompts. • SubsectionIV-Boutlinestheiterativepromptengineering\n8) ExecutionwithAIassistants:Eachpromptwasexecuted process, including the development and refinement of\nusing the four AI assistants, and the generated Python prompt templates for each task type. This resulted in\ncode was saved. optimal prompt structures used to transform the selected\n9) Submission to Stratascratch platform: The generated problems into 100 tailored prompts.\ncode solutions were submitted to the Stratascratch plat- • Subsection IV-C explains the process of using these\nform interface for evaluation. promptswitheachAIassistanttogeneratecodesolutions.\n10) Execution and result collection: The code was executed • Section V covers the data analysis process.\nonStratascratch,andtheresultsweresavedintoaresults\nA. Dataset: Selection of Data Science Problems\ndataset.\nFor our study, we selected the Stratascratch [14] platform\n11) Data analysis: We compared the performance results of\nas the source of data science coding problems. Stratascratch\nthe four LLMs to analyze their effectiveness.\nisaplatformthataggregatesreal-worlddatascienceinterview\n11/9/24, 12:57 PM mermaid.ink/svg/pako:eNqVVktvo0gQ_islRiPNSDjiYWObw0qZJLsaKdmN1tnL4BzaTWHYYBrRzSTeJP99i1eDMTmMD7ipd3_9VTWqvuBhec…stions from various companies, providing a diverse set of\nStart\nproblems that are representative of typical tasks encountered\nPreparation Phase indatascience,suchasdatamanipulation,algorithmdevelop-\n1- Select Stratascratch as\nProblem Source ment, and data visualization.\nStratascratch problems are organized into three difficulty\n2- Select 1 Problem per\nTask Category levels (easy, medium, and hard) and three main types, each\naddressing unique aspects of data science problem-solving:\nPrompt Eng. Iteration\nAnalytical: These problems involve tasks requiring data\n3- Prompt Engineering\nanalysis and manipulation using tools like pandas and SQL.\nTopics include data aggregation, filtering, conditional expres-\nEvaluate AI Assistants Iterate until satisfactory\nsions, and data formatting.\nAlgorithm: These challenges focus on computational\nAdjust Prompts Based on\nPerformance problem-solving and algorithm development. Topics in this\ncategory include array manipulation, linear regression, proba-\nResults in\nIteration Outcomes bility, graph theory, recursion, and optimization techniques.\n4- F Ai sn sa isl t S ae nl te sc ati no dn Lo Lf M 4 s AI 5- Final Prompt Templates Visualization:Theseproblemsrequirethecreationofcharts\nand graphs to represent data insights visually. Topics cover\ndistribution analysis, time-series trend analysis, spatial data\n6- Select 100 Data Science\nProblems visualization, and comparison of categorical and numerical\ndata.\n7- C Ure sia nt ge T1 e0 m0 p P lr ao tm espts An example Stratascratch problem is shown in Figure 2,\ndemonstrating the typical interface and information available\nExecution Phase for each question.\n8- Execute Each Prompt\nwith AI Assistants\nAssistan Ct o 1 p: i lM oticrosoft Assistant 2: ChatGPT Assistant 3: Claude Assistant 4: Perplexity Labs\n9- Submit Code to\nStratascratch Platform\n10- Execute Code and Save\nResults\nAnalysis Phase\n11- Data Analysis:\nCompare Performance\nResults\nEnd\nhttps://mermaid.ink/svg/pako:eNqVVktvo0gQ_islRiPNSDjiYWObw0qZJLsaKdmN1tnL4BzaTWHYYBrRzSTeJP99i1eDMTmMD7ipd3_9VTWvBhchGr4RpeKZx6… 1/1\nFig.1. OverviewoftheExperimentalProcess.\nThe following subsections and sections provide more de-\nFig.2. ExampleofaVisualizationProblemfromtheStratascratchplatform.\ntailed explanations of the main steps:\n• Subsection IV-A describes the selection of 100 Python Tobuildourdataset,weusedrandomsamplingwhileensur-\ncoding problems from Stratascratch, categorized by diffi- ingbalancedrepresentationacrossproblemtypesanddifficulty\nlevels—selecting 100 Python coding problems in total, with parsed the problem descriptions and inserted the information\n35 Analytical, 35 Algorithm, and 30 Visualization problems. into the appropriate template based on the problem type.\nFrom these 100 questions, 34 are easy, 32 are medium, and\nC. Code Generation and Execution\n34arehard.Toavoidinfringinganyintellectualpropertyfrom\nStratascratch, we omitted the full problem descriptions from In this experiment, we presented 100 problem prompts to\nour dataset. However, we have provided a table containing four AI assistants—Microsoft Copilot, ChatGPT, Claude, and\nproblem IDs, difficulty levels, links, and topic descriptions, PerplexityLabs—generatingatotalof400codesolutions(100\nwhich gives sufficient context for each task (dataset available problems per assistant). For each problem, a new chat thread\nin [17]). was initiated with the AI assistant to ensure no influence\nfrom previous interactions. Each AI assistant was given up\nto three attempts per problem, guided by feedback such as\nB. PromptEngineering:TransformingProblemsintoPrompts\n“Notworked”(whichyieldedbetterresultswithChatGPTand\nCopilot) or “Wrong answer” (more effective with Claude and\nPrompt Template for Visualization Problems\nPerplexity) to prompt improvements.\nActasadatascientistandprovideworkingPython3codefortheproblem To evaluate the solutions, we executed them on the\nbelow. Stratascratch platform and recorded the metrics provided by\nCRITICALREQUIREMENTS:\nthe platform, depending on the type of problem. For visu-\n1) UseONLYtheexactprefilledcodesnippetasstartingpoint.\n2) NOadditionalimportsbeyondwhat’sgiven. alization problems, for example, the platforms calculates the\n3) NOsample/testdatacreation—useONLYtheprovidedDataFrame.\nsimilarity of the generated graphs with the expected outputs.\n4) NOfunctionsunlessexplicitlyrequiredintheoriginalcode.\n5) Code must end with appropriate visualization command Figure 3 illustrates a similarity comparison between a graph\n(plt.show()forMatplotlib,fig.show()forPlotly,etc.).\ngenerated by the Perplexity model and the expected solution\n6) Codemustbefullyrunnablewithoutanymodifications.\nPROBLEM: provided by Stratascratch.\nTitle:{Title}\nDescription:{Description}\nDifficulty:{Level}\n{DataFrame}\n{Additional}\nEXPECTEDOUTPUTFORMAT:\n• Directplottingcodeonly.\n• Mustendwithappropriateshow()command.\n• Noreturnstatements.\n• Noprintstatements.\n• Nofunctionsunlessexplicitlyrequired.\n• Notestdatacreation.\nPrefilledcodesnippet(useexactly):\n{Code}\nThis step started by selecting one problem from each task\ncategory-Analytical, Algorithm, and Visualization-for prompt\ndevelopment. These problems were outside our main dataset\nto avoid biasing the evaluation results. During this phase, we\nexperimentedwithvariouspromptstructuresandobservedthe\nmodels’ outputs. Initially, the models often generated code\nthatincludeddatasetsorfunctionsnotspecifiedintheproblem\ndescriptions.Toaddressthis,weiterativelyrefinedtheprompts\nby introducing specific instructions and constraints.\nWetestedseveralLLMsduringpromptengineering,includ-\ning some not selected for the main experiment. Some LLMs,\nsuchasGemini(1.5Flash),couldnotproducefunctionalcode\neven for easy problems, despite multiple prompt refinements.\nOthers, like YouChat Pro, was capable but was not included\nin the final selection to avoid redundancy.\nTo ensure consistency and minimize subjectivity, we auto-\nmated the conversion of problem descriptions into prompts. Fig.3. SimilaritycomparisonforaVisualizationProblem.\nThis involved creating prompt templates tailored to each\nproblem type-Analytical, Algorithm, and Visualization-which Due to Stratascratch platform constraints (e.g., limitations\naddressed the unique requirements of each category. This on library imports and required code formatting), we al-\nsection illustrates the prompt template used for Visualization lowed minor manual edits to adapt the AI-generated code for\nproblems. The templates for Algorithm and Analytics tasks consistent evaluation. These adjustments included removing\nareavailablein[17].Ourautomatedpromptgenerationsystem prohibited imports (e.g., import os), modifying code structure\n(e.g., removing function encapsulation when global code was LLM Success Rate in Solving Data Science Coding Problems\n100\nneeded),andeliminatingunnecessaryprintstatementsinfavor\nof returns. These edits preserved the core logic and function-\n80\nality of the solutions and were documented for transparency 72.0% 70.0%\n66.0% and reproducibility. This documentation (available in [17]) 60 60.0%\nincludes the nature of the edits and their reason.\n40\nV. ANALYSISANDINTERPRETATION\nThissectionpresentsthestatisticalanalysisofdatacollected\n20\nduring the experiment. The dataset includes information such\nas problem IDs, the code generated by each LLM, and 0\nCopilot ChatGPT Perplexity Claude\nassociated performance metrics, which is available in full for LLM Model\nreproducibility in [17].\nFig.5. RQ1-LLMsuccessrateinsolvingDScodingproblems.\nFigure 4 provides a general overview of the LLMs’ as-\nsertiveness across all tasks and difficulty levels. This initial\nvisualization offers a preliminary look at overall trends, while whether there was a significant difference in success rates\nmoredetailedanalysesfollowforeachresearchquestion(RQ). between the LLMs by applying the Friedman test, followed\nby pairwise Wilcoxon tests where a significant difference was\ndetected.\nThis graph seems to\ncapture the results\nTABLEII\nRQ1:SUCCESSRATERESULTSOFLLMSATDIFFERENTBASELINES\nBaseline LLM SuccessRate(%) p-value Conclusion\nCopilot 60% 0.0284 Significant\nChatGPT 72% 0.0000 Significant\n50%\nPerplexity 66% 0.0009 Significant\nClaude 70% 0.0000 Significant\nCopilot 60% 0.5433 NotSignificant\nChatGPT 72% 0.0084 Significant\n60%\nPerplexity 66% 0.1303 NotSignificant\nClaude 70% 0.0248 Significant\nCopilot 60% 0.9875 NotSignificant\nChatGPT 72% 0.3768 NotSignificant\n70%\nPerplexity 66% 0.8371 NotSignificant\nClaude 70% 0.5491 NotSignificant\nAsTableIIshows,allLLMsperformsignificantlyabovethe\nAnd w5e0 c%an htahvree as hold, confirming baseline effectiveness in solving\nFig.4. OverallSuccessRateofLLMs. separate graph for\nexecuctioond itinmge atnads ks. At the 60% baseline, only ChatGPT and Claude\nsimilarreitya scchores tatistical significance, suggesting enhanced reliability\nFor each research question (RQ), we begin by visualizing\nfor typical tasks. No LLM achieves significance at the 70%\nthe data to provide an intuitive understanding of the perfor-\nbaseline,indicatinglimitationsinsustainingveryhighsuccess\nmance distributions across different conditions. In addition to\nrates across diverse challenges.\nvisualization and descriptive statistics, we perform hypothesis\ntesting for each RQ.\nA. RQ1: Success Rate of LLMs in Solving Data Science Pairwise Wilcoxon Test Corrected p-values (Bonferroni) 1.0\nProblems\n0.0437 0.9438 0.2474\nAsshowninFigure5,ChatGPTachievesthehighestsuccess 0.8\nrate (72%), followed by Claude (70%) and Perplexity (66%),\nwith Copilot at 60%. These percentages represent the propor- 0.0437 1.0000 1.0000\n0.6\ntion of correct solutions generated by each LLM, including\nthose needing minor code edits.\nHypothesis Testing: To assess each model’s success 0.9438 1.0000 1.0000 0.4\nrate, we conducted a one-tailed binomial test with baseline\nthresholds of 50%, 60%, and 70%, determining if each 0.2\n0.2474 1.0000 1.0000\nLLM’s success rate significantly exceeded these benchmarks.\nThis non-parametric test, suitable for binary outcomes (cor-\nCopilot ChatGPT Perplexity Claude\nrect/incorrect), provides insight into each LLM’s performance\nrelative to random chance [13]. Additionally, we evaluated Fig.6. RQ1-PairwiseComparisonofSuccessRates.\n)%( etaR\nsseccuS\ntolipoC\nTPGtahC\nytixelpreP\nedualC\neulav-p\ndetcerroC\nTo explore differences between LLMs, we applied the highest success rate on easy and medium problems, while\nFriedman test, which detected significant variation in success ChatGPT excels on hard problems, suggesting its robustness\nrates across models (p = 0.0384). We followed up with post- with advanced challenges. Copilot consistently shows the\nhocWilcoxonpairwisecomparisons,identifyingastatistically lowest success rate across all difficulty levels, indicating a\nsignificant difference between ChatGPT and Copilot, with potential limitation in handling more complex tasks.\nChatGPT achieving a significantly higher success rate (cor- Hypothesis Testing: Chi-Square tests were performed to\nrectedp-value:0.0437),asdepictedintheheatmapofFigure6. evaluate the effect of difficulty level on each LLM’s success\nNoothersignificantdifferenceswereobservedamongmodels. rate.Resultsshowthatdifficultylevelsignificantlyimpactsthe\nBased on these tests, we conclude: success rates of Perplexity and Claude (p<0.05), suggesting\nFor hypotheses H0 and H0 : that their performance fluctuates with problem complexity. In\n1 1a\n• At the 50% baseline, all LLMs exhibit success contrast,CopilotandChatGPTdemonstrateconsistentsuccess\nrates significantly above 50%, supporting the rates across all difficulty levels, indicated by non-significant\nconclusionthateachmodelperformsbetterthan results.\nrandom chance in solving data science coding Based on these tests, we conclude:\nproblems. For the hypothesis H0 2, we reject it for Perplexity\n• Atthe60%baseline,onlyChatGPTandClaude and Claude, indicating that difficulty level signifi-\nshowsuccessratessignificantlyabovethislevel, cantly affects their success rates. For Copilot and\nindicating that these two models exhibit greater ChatGPT, we fail to reject H0 2, suggesting consis-\nreliability across general coding tasks. tent performance across varying difficulty levels.\n• At the 70% baseline, no LLM meets statistical\nTo further explore comparative performance, we conducted\nsignificance, suggesting a possible limitation in\nthe Friedman test across all models at each difficulty level.\nachieving consistently high success rates across\nAlthough the overall test did not show significant differences\ndiverse coding challenges.\ninsuccessratesacrossLLMsforeachlevel,pairwiseWilcoxon\n• Friedman Test and Wilcoxon Post-hoc Test:\ntestshighlightedasignificantdifferencebetweenChatGPTand\nSignificantdifferenceswerefoundbetweenmod-\nCopilotforhardproblems(p=0.0196),indicatingChatGPT’s\nels, with ChatGPT achieving a success rate\nsuperior performance on more challenging tasks.\nsignificantly higher than that of Copilot.\nIn summary, RQ1 indicates that ChatGPT and Claude\nexhibitthemostconsistentperformance,particularlyChatGPT, C. RQ3: Does the type of data science task (Analytical,\nwhich leads in relative success. These findings suggest that Algorithm, Visualization) influence the success rate of the\nChatGPT and Claude may be preferable for tasks demanding different LLMs?\nhighersuccessrates,whilehighlightingthedifficultyforLLMs\nin consistently achieving a 70% success rate across diverse\nchallenges. LLM Success Rate by Task Type in Solving DS Coding Problems\n100\nLLM Model\nCopilot\nB. RQ2: Does the difficulty level of coding problems (easy, ChatGPT\nPerplexity\nmedium, hard) influence the success rate of the different 80 Claude 74.3% 74.3% 76.7% 76.7%\n71.4% 70.0% 70.0%\nLLMs? 62.9% 65.7%\n60 57.1% 60.0%\n48.6%\nLLM Success Rate by Difficulty Level in Solving DS Coding Problems\n100 40\nLLM Model\nCopilot\n85.3% ChatGPT\n82.4% 82.4% Perplexity\n80 76.5% Claude 20\n70.6%\n65.6% 62.5%\n60 59.4% 55.9% 58.8% 0 Analytical Algorithm Visualization\n53.1% 50.0% Task Type\n40 Fig.8. RQ3:Effectoftasktypeonsuccessrate.\n20 Figure 8 illustrates the success rate of each LLM across\ndifferenttasktypes.ChatGPTdemonstratesthehighestsuccess\n0 Easy Medium Hard rate in analytical and algorithm tasks, while Perplexity and\nDifficulty Level\nClaude achieve similar levels in visualization tasks. Although\nFig.7. RQ2:Effectofdifficultylevelonsuccessrate. ChatGPT performs particularly well in analytical and algo-\nrithmtasks,statisticaltestsrevealnosignificantoverallsuccess\nAs shown in Figure 7, the success rates of each LLM rate differences among the models across task types, except\nvary across different difficulty levels. Claude achieves the between ChatGPT and Copilot.\n)%(\netaR\nsseccuS\n)%(\netaR\nsseccuS\nTABLEIII executiontime,indicatingitssolutionsgenerallyexecutefaster\nRQ3:CHI-SQUARETESTRESULTSFORTASKTYPEACROSSLLMS than the other models’ solutions, followed by Copilot and\nPerplexity. ChatGPT has the highest median execution time,\nLLM Algorithm Analytical Visuali. p-value Conclusion\nCopilot 22/35 17/35 21/30 0.1946 NotSig. suggestingthatonaverage,ittakeslongertoexecuteanalytical\nChatGPT 26/35 25/35 21/30 0.9250 NotSig. tasks than the other models. ChatGPT displays the largest\nPerplexity 23/35 20/35 23/30 0.2534 NotSig.\nClaude 26/35 21/35 23/30 0.2715 NotSig. interquartile range (IQR), indicating significant variability,\nwhereas Copilot, Perplexity, and Claude have narrower IQRs,\nThe Chi-Square test results in Table III show that task type suggesting more consistent execution times. This analysis\ndoes not significantly impact the success rate for any LLM, suggests Claude is generally faster and more consistent for\nwith all p-values exceeding the 0.05 threshold. This finding analytical tasks, while ChatGPT may offer less predictability\nsuggests that each model’s performance remains relatively in execution time.\nstable across analytical, algorithmic, and visualization tasks. Hypothesis Testing: To assess whether these observed dif-\nferences are statistically significant, we conducted a Kruskal-\nForhypothesisH0 3,wefailtorejectitforallmod-\nWallis test, as the Kruskal-Wallis test is a non-parametric\nels, indicating that task type does not significantly\nmethodsuitableforcomparingthedistributionsofindependent\nimpactsuccessrateoverall.However,post-hoccom-\ngroups, particularly their central tendencies, when data is not\nparisonsrevealthatChatGPTperformssignificantly\nnormally distributed.\nbetterthanCopilotinanalyticalandalgorithmtasks.\nThe test, conducted using the scipy.stats library, re-\nD. RQ4: For Analytical questions, do the LLMs differ in the sulted in a Kruskal-Wallis statistic of 0.6947 and a p-value\nefficiency (running time) of the code they generate? of 0.8744. With a p-value exceeding the significance level of\n0.05, we fail to reject the null hypothesis. This suggests that\nthere are no statistically significant differences in the median\nExecution Times for Analytical Questions Across LLMs (Common Questions)\nexecution times across the LLMs for Analytical questions.\nFor RQ4, we fail to reject H0 4, indicating that\n0.1 the LLMs do not differ significantly in the efficiency\n(running time) of the code they generate for Analyt-\nical questions.\n0.0175\n0.0150\n0.0125\n0.01 E. RQ5: For visualization tasks, do the LLMs differ in the\n0.0075\n0.0050 quality (similarity) of the visual outputs they produce com-\n0.0025 pared to expected results?\n0.001\nCopilot ChatGPT Perplexity Claude\nLLM Model\nSimilarity Scores for Visualization Tasks Across LLMs (Common Questions)\n1.00\nFig.9. RQ4:ExecutiontimesofLLMs-BoxPlot.\n0.95\nMedian Execution Time for Analytical Tasks by LLM and Difficulty Level (Common Questions) 0.90\n0.03 s\nLLM Model\nChatGPT\nClaude\nCopilot 0.85\nPerplexity\n0.02 s\n0.80\n0.75\n0.01 s\n0.70\nCopilot ChatGPT Perplexity Claude\nLLM Model\n6×103\nFig.11. RQ5:SimilarityScores-BoxPlot.\n0.005 s\n0.004 s\nAs depicted in Figures 11 and 12, ChatGPT achieves the\nAll (12 questions) Easy (7 questions) Medium (2 questions) Hard (3 questions)\nDifficulty Level highest median similarity score among the commonly solved\nFig.10. RQ4:Medianexecutiontimebydifficultylevel. problems, indicating that its outputs are closest to the ex-\npected results. Additionally, ChatGPT displays the narrowest\nFor a fair comparison, Figures 9 and 10 include only the interquartile range (IQR), highlighting its consistency. These\nresults from problems successfully solved by all LLMs, as findings suggest that ChatGPT delivers more reliable quality\nthe platform does not compute execution times for solutions in generating visual outputs that closely match the expected\nthat did not work. Accordingly, Claude has the lowest median results.\n)sdnoces(\nemiT\nnoitucexE\n)sdnoces(\nemiT\nnoitucexE\nnaideM\nerocS\nytiralimiS\nMedian Similarity Score for Visualization Tasks by LLM and Difficulty Level (Common Questions) received clear, consistent instructions, allowing for a fair\n1.00\nLLM C hM ao td Ge Pl T comparison of performance.\nClaude\nC Peo rp pil lo ext ity 2) ExternalValidity: Thegeneralizabilityofourfindingsis\n0.95\nlimited by the scope of problems used. Our study focused on\n100 Python coding problems from a single platform, which\n0.90\nmay not represent the full spectrum of data science tasks. To\nenhance external validity, future research should incorporate a\n0.85\nwider range of problems from multiple sources.\n3) Construct Validity: We did not formally assess the\n0.80\nexpertise of the researchers conducting the experiment, which\ncould introduce subjectivity, particularly in interpreting and\n0.75\nevaluating the AI-generated code. Although guidelines were\nestablished for acceptable code modifications—allowing only\n0.70\nAll (14 questions) Easy (6 questions) Medium (4 questions) Hard (4 questions)\nDifficulty Level minoreditstoresolveexecutionissues—differencesincoding\nproficiencyamongresearcherscouldinfluencetheassessment.\nFig.12. RQ5:Mediansimilarityscoresbydifficultylevel.\n4) Conclusion Validity: These threats may affect the valid-\nity of our conclusions. While our study offers insights into\nHypothesis Testing: To statistically analyze differences in the capabilities and limitations of LLMs in data science code\nsimilarity scores among the LLMs, we conducted a Kruskal- generation, the results should be interpreted with caution.\nWallis test. Further research addressing these limitations is necessary to\nKruskal-Wallis Test Results: strengthen the confidence in the findings.\n• Kruskal-Wallis Statistic: 0.8287 VI. DISCUSSION\n• p-value: 0.8426\nThrough a series of hypothesis tests, we investigated each\n• Conclusion: The p-value above 0.05 suggests no statisti-\nmodel’s effectiveness across different problem types and dif-\ncally significant differences in similarity scores between\nficulty levels. The findings underscore both the strengths\nthe LLMs. This indicates that while there are observed\nand limitations of these LLMs in addressing data science\ndifferencesinmeansimilarityscoresandvariability(with\nchallenges,providinginsightsintowhichmodelsmaybemost\nChatGPTachievingthehighestmeanandmostconsistent\nsuitable for specific scenarios in data science workflows. The\nperformance), these differences are not statistically sig-\nresults highlight that:\nnificant across LLMs at the 5% significance level.\n• Success Rate: Empirical evidence from our tests indi-\nBased on these results, we conclude:\ncates that each LLM exceeded the 50% baseline success\nFor hypothesis H0 5, we fail to reject the null\nrate, confirming effectiveness beyond random chance. At\nhypothesis, indicating that there is no significant\nthe 60% baseline, only ChatGPT and Claude achieved\ndifference in the similarity quality of generated vi-\nsignificantly higher success rates, reinforcing their relia-\nsualization outputs among the LLMs.\nbility in general coding contexts. However, none of the\nmodelsreachedthe70%threshold,suggestinglimitations\nF. Threats to Validity\nin consistently achieving high accuracy across diverse\nAs usual in empirical studies, our study acknowledges data science task types. ChatGPT achieved the highest\nseveral threats that may impact the interpretation and general- overall success rate and performed consistently well on\nization of the results. harder questions, with descriptive analysis suggesting\n1) Internal Validity: A key concern is the undisclosed strong outcomes in analytical and algorithmic tasks, re-\nnature of the LLMs’ training data. Without access to this in- flecting its robustness in complex data science scenarios.\nformation, we cannot confirm whether the generated solutions Claudealsodemonstratedsolidperformance,particularly\nare novel or based on memorized content. Even though we on easier and medium-difficulty tasks, as well as in\nselected new problems from Stratascratch, similar or identical visualization tasks, indicating versatility across various\nproblems might exist in the models’ training data, potentially problem types. Perplexity and Copilot, while showing\ninflating their apparent effectiveness. lower success rates on more complex tasks, displayed\nPrompt design is another factor influencing outcomes. As consistent performance on simpler tasks, highlighting\nnoted by White et al. [18], the formulation of prompts can theirpotentialforstraightforwarddatascienceworkflows.\nsignificantlyaffectLLMoutputs.Whileweendeavoredtouse • Efficiency (Execution Time): For analytical tasks, the\nconsistentpromptsderivedfromoriginalproblemdescriptions, Kruskal-Wallis test on execution times revealed no sta-\nvariations could lead to different results. tistically significant differences among the models, sug-\nTo address potential subjectivity in converting problems gesting that efficiency, in terms of runtime, is relatively\nto prompts, we developed standardized prompt templates for comparable across these LLMs. This finding implies\neach task type. These templates ensured that all AI assistants that while execution time may vary, it may not be\nerocS\nytiralimiS\nnaideM\na decisive factor in model selection for tasks where VIII. FUTUREWORK\naccuracy and complexity are primary concerns. Despite\nOur study opens several avenues for future research to en-\nthe lack of empirical significance, the median execution\nhancetheapplicationofLLMsindatasciencecodegeneration.\ntimes indicate some practical trends: Claude had the\nlowest median execution time, suggesting it generally A. Exploring Complex and Real-World Data Science Tasks\nrunsfasterthantheothermodels,followedbyCopilotand\nEvaluating LLMs on sophisticated, real-world data science\nPerplexity. ChatGPT had the highest median execution\ntasks—suchasimplementingmachinelearningmodelswithli-\ntime, indicating slower performance on average.\nbrarieslikeScikit-learnorTensorFlow,handlinglargedatasets,\n• Quality of Output (Image Similarity for Visualiza-\nand working with unstructured data—could provide deeper\ntion): In visualization tasks, where models were eval-\ninsights into their capabilities and limitations. For instance,\nuated based on similarity scores to expected outputs,\nNascimento et al. [19] demonstrated the use of an LLM to\nChatGPT achieved the highest median similarity score.\nreplace a learning algorithm that involved neural networks\nHowever, statistical tests indicated no significant differ-\noptimized through genetic algorithms. While their experiment\nences between the models.\nwas preliminary, it highlighted the potential of LLMs to\n• Consistency Across Difficulty Levels and Task Types:\nautomate complex coding solutions, suggesting that these\nEmpirical analysis reveals that ChatGPT maintains con-\nmodelscouldextendbeyondbasicscriptingtomoreadvanced\nsistentperformanceregardlessoftaskcomplexity,provid-\ntasks. Exploring tasks like multivariate analysis, time series\ningreliablesuccessratesacrossbothsimpleandcomplex\nforecasting, and dynamic optimization could further test LLM\ntasks.Incontrast,Perplexity’sandClaude’ssuccessrates\nproficiency. Testing in practical settings uncovers challenges\nweresignificantlyinfluencedbytaskdifficulty,withbetter\nthat controlled experiments may not fully capture.\noutcomes on less complex tasks. Copilot also demon-\nstrated consistency across difficulty levels, though with\nB. Expanding Model Diversity and Dataset Coverage\ngenerallylowersuccessratesthanChatGPT.Additionally,\nWe could extend this analysis by integrating additional\nour tests indicate that task type (analytical, algorithmic,\nLLMs and incorporating data science-specific coding chal-\nvisualization) does not significantly impact success rates\nlenges from various platforms, such as LeetCode, with tasks\nfor any model, suggesting stable performance across\nlike data manipulation, cleaning, and SQL queries. To capture\ndifferent data science task types. This consistency makes\na broader range of data science skills, the dataset could also\nChatGPT a dependable choice when task complexity is\ninclude non-coding tasks, such as interpretation and analysis\nuncertain.\nquestions, as provided by Stratascratch.\nVII. CONCLUSION Additionally, we could integrate recently released questions\nin Stratascratch that use Polars DataFrame [20] for data\nThis study presents a controlled experiment evaluating\nmanipulation-ahigh-performancelibrarydesignedforefficient\nthe effectiveness of four prominent LLM-based AI as-\ndata handling in Python. We could further expand the dataset\nsistants—Microsoft Copilot (GPT-4 Turbo), ChatGPT (o1-\nby incorporating problems from DS-1000 [4], which includes\npreview), Claude (3.5 Sonnet), and Perplexity Labs (Llama-\na diverse selection of data science problems sourced from\n3.1-70b-instruct)—in data science coding tasks. Effectiveness\nStackOverflow. Following recommendations by Lai et al. [4],\nwas measured by each model’s success rate, execution effi-\nintroducing customized variations of existing problems would\nciency, visual output quality, and consistency across difficulty\nhelp reduce model memorization, enhancing the rigor of the\nlevels and task types.\nevaluation environment.\nWith success rates exceeding 50% for all models, this\nresearch provides valuable insights into LLM performance in\nC. Expanding Evaluation Metrics\ndata science. At the 60% baseline, only ChatGPT and Claude\nachieved significantly higher success rates, highlighting their Future work could expand LLM evaluation by integrating\nreliabilityingeneralcodingtasks.However,ourfindingsindi- software engineering metrics like code complexity, maintain-\ncate that only ChatGPT consistently maintains performance ability, and readability. Code similarity analysis could assess\nacross different difficulty levels, whereas Claude’s success alignment with industry standards, while qualitative reviews\nrate is significantly affected by task difficulty, suggesting its by data scientists would add valuable insights, particularly for\nperformance may vary with more complex tasks. visualization tasks where image similarity metrics may fall\nNo evidence suggests that task type affects LLM success short.\nrates,thoughChatGPT(o1-preview)significantlyoutperforms\nD. Investigating Prompt Engineering and Ensuring Repro-\nCopilot (GPT-4o) for analytical and algorithm tasks. This\nducibility\nnuancedunderstandingofeachmodel’sstrengthsenablesmore\nstrategic LLM selection tailored to specific needs. Addition- Prompt engineering significantly influences LLM outputs.\nally, this study underscores the value of rigorous hypothesis Future research should examine how different prompt for-\ntesting in AI evaluation, setting a template for assessing mulations affect code generation quality and consistency.\nmodels beyond basic accuracy metrics. Employing methodologies where LLMs simulate multiple\nusers [21] could shed light on the impact of varying profes- [10] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,\nsional experiences and prompt designs. Addressing the non- H.Edwards,Y.Burda,N.Joseph,G.Brockmanetal.,“Evaluatinglarge\nlanguage models trained on code,” arXiv preprint arXiv:2107.03374,\ndeterministic nature of LLMs by controlling parameters like\n2021.\ntemperature settings could improve reproducibility, leading to [11] B. Grewal, W. Lu, S. Nadi, and C.-P. Bezemer, “Analyzing developer\nmore consistent and reliable evaluations. use of chatgpt generated code in open source github projects,” in\n2024 IEEE/ACM 21st International Conference on Mining Software\nRepositories(MSR). IEEE,2024,pp.157–161.\nE. Exploring Further Research Questions [12] X.Gu,M.Chen,Y.Lin,Y.Hu,H.Zhang,C.Wan,Z.Wei,Y.Xu,and\nJ. Wang, “On the effectiveness of large language models in domain-\nEven using the same dataset, many additional questions specific code generation,” ACM Transactions on Software Engineering\nandMethodology,2024.\nand hypotheses remain to be explored. For instance, while\n[13] C.Wohlin,P.Runeson,M.Ho¨st,M.C.Ohlsson,B.Regnell,A.Wessle´n\nwe assessed the impact of problem difficulty and type on et al., Experimentation in software engineering. Springer, 2012, vol.\nLLMsuccessrates,furtheranalysiscouldfocusonestablishing 236.\n[14] StrataScratch, “Master coding for data science,” https:\nbaseline success rates for each problem type and difficulty\n//www.stratascratch.com/,n.d.,accessed:2024-11-01.\nlevel. Given the general success baseline of 60%, future [15] C. Troy, S. Sturley, J. M. Alcaraz-Calero, and Q. Wang, “Enabling\nresearch might explore optimal baseline thresholds specific to generative ai to produce sql statements: A framework for the auto-\ngeneration of knowledge based on ebnf context-free grammars,” IEEE\neach type and level of task. Beyond success rates, this dataset\nAccess,vol.11,pp.123543–123564,2023.\nalsoallowsforanin-depthexplorationofefficiency(execution [16] M. Malekpour, N. Shaheen, F. Khomh, and A. Mhedhbi, “Towards\ntimes)andsimilarityscoresforeachdifficultylevel,providing optimizingsqlgenerationviallmrouting,”inNeurIPS2024ThirdTable\nRepresentationLearningWorkshop.\na more comprehensive view of model performance in diverse\n[17] S. A. Boominathan, S. S. Chintakunta, N. Nascimento, and\ntask complexity. Additionally, information on the number of E. Guimaraes, “LLM4DS-Benchmark: A Dataset for Assessing LLM\nattempts (up to three) and instances of minor edits provides Performance in Data Science Coding Tasks,” Nov. 2024. [Online].\nAvailable:https://doi.org/10.5281/zenodo.14064111\ndata for assessing error types (syntax and logic errors), retry\n[18] J.White,S.Hays,Q.Fu,J.Spencer-Smith,andD.C.Schmidt,“Chatgpt\npatterns, and the models’ adaptability to user feedback. Our prompt patterns for improving code quality, refactoring, requirements\ndatasetalsoincludesspecifictopicswithineachquestiontype, elicitation,andsoftwaredesign,”2023.\n[19] N.Nascimento,P.Alencar,andD.Cowan,“Gpt-in-the-loop:Supporting\nallowing for a more granular analysis that could reveal topic-\nadaptationinmultiagentsystems,”in2023IEEEInternationalConfer-\nspecific strengths or limitations of each LLM. enceonBigData(BigData). IEEE,2023,pp.4674–4683.\n[20] RitchieVink,“Polars:Blazinglyfastdataframesinrust,python,node.js,\nr,andsql,”2023.[Online].Available:https://github.com/pola-rs/polars\nREFERENCES\n[21] G.V.Aher,R.I.Arriaga,andA.T.Kalai,“Usinglargelanguagemodels\nto simulate multiple humans and replicate human subject studies,” in\n[1] A.Halevy,Y.Choi,A.Floratou,M.J.Franklin,N.Noy,andH.Wang, International Conference on Machine Learning. PMLR, 2023, pp.\n“Willllmsreshape,supercharge,orkilldatascience?(vldb2023panel),” 337–371.\nProceedingsoftheVLDBEndowment,vol.16,no.12,pp.4114–4115,\n2023.\n[2] N. Nascimento, C. Tavares, P. Alencar, and D. Cowan, “Gpt in data\nscience: A practical exploration of model selection,” in 2023 IEEE\nInternational Conference on Big Data (BigData). IEEE, 2023, pp.\n4325–4334.\n[3] J.Li,B.Hui,G.Qu,J.Yang,B.Li,B.Li,B.Wang,B.Qin,R.Geng,\nN.Huoetal.,“Canllmalreadyserveasadatabaseinterface?abigbench\nfor large-scale database grounded text-to-sqls,” Advances in Neural\nInformationProcessingSystems,vol.36,2024.\n[4] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-\nt. Yih, D. Fried, S. Wang, and T. Yu, “Ds-1000: A natural and\nreliable benchmark for data science code generation,” in International\nConferenceonMachineLearning. PMLR,2023,pp.18319–18345.\n[5] M. Kazemitabaar, J. Williams, I. Drosos, T. Grossman, A. Z. Henley,\nC.Negreanu,andA.Sarkar,“Improvingsteeringandverificationinai-\nassisteddataanalysiswithinteractivetaskdecomposition,”inProceed-\nings of the 37th Annual ACM Symposium on User Interface Software\nandTechnology,2024,pp.1–19.\n[6] N. Nguyen and S. Nadi, “An empirical evaluation of github copilot’s\ncodesuggestions,”inProceedingsofthe19thInternationalConference\nonMiningSoftwareRepositories,2022,pp.1–5.\n[7] N.Nathalia,A.Paulo,andC.Donald,“Artificialintelligencevs.software\nengineers: An empirical study on performance and efficiency using\nchatgpt,” in Proceedings of the 33rd Annual International Conference\nonComputerScienceandSoftwareEngineering,2023,pp.24–33.\n[8] M.A.Kuhail,S.S.Mathew,A.Khalil,J.Berengueres,andS.J.H.Shah,\n““will i be replaced?” assessing chatgpt’s effect on software develop-\nment and programmer perceptions of ai tools,” Science of Computer\nProgramming,vol.235,p.103111,2024.\n[9] T.Coignion,C.Quinton,andR.Rouvoy,“Aperformancestudyofllm-\ngenerated code on leetcode,” in Proceedings of the 28th International\nConference on Evaluation and Assessment in Software Engineering,\n2024,pp.79–89.",
    "pdf_filename": "LLM4DS_Evaluating_Large_Language_Models_for_Data_Science_Code_Generation.pdf"
}