{
    "title": "BenchmarkingPositionalEncodingsforGNNsandGraphTransformers",
    "abstract": "Recent advances in Graph Neural Networks (GNNs) and Graph Transformers (GTs)havebeendrivenbyinnovationsinarchitecturesandPositionalEncodings (PEs),whicharecriticalforaugmentingnodefeaturesandcapturinggraphtopol- ogy. PEs are essential for GTs, where topological information would otherwise belostwithoutmessage-passing. However, PEsareoftentestedalongsidenovel architectures, makingitdifficulttoisolatetheireffectonestablishedmodels. To address this, we present a comprehensive benchmark of PEs in a unified frame- workthatincludesbothmessage-passingGNNsandGTs. Wealsoestablishthe- oretical connections between MPNNs and GTs and introduce a sparsified GRIT attentionmechanismtoexaminetheinfluenceofglobalconnectivity.Ourfindings demonstratethatpreviouslyuntestedcombinationsofGNNarchitecturesandPEs can outperform existing methods and offer a more comprehensive picture of the state-of-the-art.Tosupportfutureresearchandexperimentationinourframework, wemakethecodepubliclyavailable. 1 INTRODUCTION Graph machine learning has traditionally relied on message-passing neural networks (MPNNs), whichworkthroughiterativeroundsofneighborhoodaggregation(Kipf&Welling,2016). Ineach round,nodesupdatetheirstatesbyincorporatinginformationfromtheirneighborsalongwiththeir own current states. While effective in capturing local graph structures, this approach can strug- gle with modeling long-range dependencies. Graph Transformer (GT) architectures utilize full at- tention mechanisms to circumvent this, but necessitate new methods to integrate graph topology information(Dwivedi&Bresson,2020). Thisissimilartohowpositionalencodings(PEs)inNat- uralLanguageProcessing(NLP)representtokenpositionswithinsequences(Vaswanietal.,2017). However, encoding positional information in graphs is more complex than in sequences. Ideally, positionalencodingsshouldallowthereconstructionofthegraph’stopologyfromnodefeaturesand provide useful inductive biases to improve performance (Black et al., 2024). Despite the growing number of new graph transformer architectures and positional encodings, there has been a lack of systematic evaluation comparing these encodings across different GT architectures. This makes it difficult to determine whether observed performance improvements are due to novel encodings or architecturalinnovations. In this paper, we conduct a comprehensive evaluation of various positional encodings for both message-passing and transformer frameworks. Our goal is to understand the impact of positional encodingsonmodelperformanceandidentifythebestcombinationsofencodingsandarchitectures. By benchmarking state-of-the-art graph transformers with a variety of positional encodings, we provideaclearpictureofthecurrentstateofthefieldandofferguidanceforfutureresearch. Addi- tionally,wefurtherstrengthenthetheoreticalconnectionbetweenMPNNsandGTs. AlthoughGTs aregenerallyconsideredfundamentallydifferentduetotheiruseofattentionmechanisms,weshow that under certain conditions, MPNNs and GTs can be equally expressive, with additional results thatextendthescopeofpreviousanalyses(Velicˇkovic´,2023;Mu¨ller&Morris,2024). Specifically, MPNNs can be applied to fully-connected graphs and operate like a GT, while attention mech- anisms can also be adapted for local message-passing. Our theoretical analysis demonstrates that bothMPNNsandGTscanhavethesameexpressivenesswhentheunderlyingtopologyoftheMPNN 1 4202 voN 91 ]GL.sc[ 1v23721.1142:viXra",
    "body": "BenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nBENCHMARKING POSITIONAL ENCODINGS FOR\nGNNS AND GRAPH TRANSFORMERS\nFlorianGro¨tschla JiaqingXie RogerWattenhofer\nETHZurich ETHZurich ETHZurich\nZurich,Switzerland Zurich,Switzerland Zurich,Switzerland\nfgroetschla@ethz.ch jiaxie@ethz.ch wattenhofer@ethz.ch\nABSTRACT\nRecent advances in Graph Neural Networks (GNNs) and Graph Transformers\n(GTs)havebeendrivenbyinnovationsinarchitecturesandPositionalEncodings\n(PEs),whicharecriticalforaugmentingnodefeaturesandcapturinggraphtopol-\nogy. PEs are essential for GTs, where topological information would otherwise\nbelostwithoutmessage-passing. However, PEsareoftentestedalongsidenovel\narchitectures, makingitdifficulttoisolatetheireffectonestablishedmodels. To\naddress this, we present a comprehensive benchmark of PEs in a unified frame-\nworkthatincludesbothmessage-passingGNNsandGTs. Wealsoestablishthe-\noretical connections between MPNNs and GTs and introduce a sparsified GRIT\nattentionmechanismtoexaminetheinfluenceofglobalconnectivity.Ourfindings\ndemonstratethatpreviouslyuntestedcombinationsofGNNarchitecturesandPEs\ncan outperform existing methods and offer a more comprehensive picture of the\nstate-of-the-art.Tosupportfutureresearchandexperimentationinourframework,\nwemakethecodepubliclyavailable.\n1 INTRODUCTION\nGraph machine learning has traditionally relied on message-passing neural networks (MPNNs),\nwhichworkthroughiterativeroundsofneighborhoodaggregation(Kipf&Welling,2016). Ineach\nround,nodesupdatetheirstatesbyincorporatinginformationfromtheirneighborsalongwiththeir\nown current states. While effective in capturing local graph structures, this approach can strug-\ngle with modeling long-range dependencies. Graph Transformer (GT) architectures utilize full at-\ntention mechanisms to circumvent this, but necessitate new methods to integrate graph topology\ninformation(Dwivedi&Bresson,2020). Thisissimilartohowpositionalencodings(PEs)inNat-\nuralLanguageProcessing(NLP)representtokenpositionswithinsequences(Vaswanietal.,2017).\nHowever, encoding positional information in graphs is more complex than in sequences. Ideally,\npositionalencodingsshouldallowthereconstructionofthegraph’stopologyfromnodefeaturesand\nprovide useful inductive biases to improve performance (Black et al., 2024). Despite the growing\nnumber of new graph transformer architectures and positional encodings, there has been a lack of\nsystematic evaluation comparing these encodings across different GT architectures. This makes it\ndifficult to determine whether observed performance improvements are due to novel encodings or\narchitecturalinnovations.\nIn this paper, we conduct a comprehensive evaluation of various positional encodings for both\nmessage-passing and transformer frameworks. Our goal is to understand the impact of positional\nencodingsonmodelperformanceandidentifythebestcombinationsofencodingsandarchitectures.\nBy benchmarking state-of-the-art graph transformers with a variety of positional encodings, we\nprovideaclearpictureofthecurrentstateofthefieldandofferguidanceforfutureresearch. Addi-\ntionally,wefurtherstrengthenthetheoreticalconnectionbetweenMPNNsandGTs. AlthoughGTs\naregenerallyconsideredfundamentallydifferentduetotheiruseofattentionmechanisms,weshow\nthat under certain conditions, MPNNs and GTs can be equally expressive, with additional results\nthatextendthescopeofpreviousanalyses(Velicˇkovic´,2023;Mu¨ller&Morris,2024). Specifically,\nMPNNs can be applied to fully-connected graphs and operate like a GT, while attention mech-\nanisms can also be adapted for local message-passing. Our theoretical analysis demonstrates that\nbothMPNNsandGTscanhavethesameexpressivenesswhentheunderlyingtopologyoftheMPNN\n1\n4202\nvoN\n91\n]GL.sc[\n1v23721.1142:viXra\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nisfullyconnected. Basedontheseinsights, weextendourevaluationtoincludeMPNNswithpo-\nsitionalencodingsonfully-connectedgraphsandmodifystate-of-the-artattentionmechanismsfor\nlocalizedgraphconvolutions. Ourresultsindicatethatbycombiningexistingpositionalencodings\nandarchitectures,state-of-the-artperformancecanbeachievedonseveralbenchmarkdatasets.\nOurcontributionscanbesummarizedasfollows:\n1. We conduct an empirical evaluation of various positional encodings across message-\npassingneuralnetworksandGraphTransformerstoconsolidatethecurrentstate-of-the-art\nandfindnewcombinationsthatsurpassthepreviousbestmodels.\n2. We provide theoretical insights into the relationship between MPNNs and GTs, showing\nconditionswheretheysharesimilarexpressiveness. Basedontheseobservations,weintro-\nduceasparsifiedversionofGRITattentionforlocalizedgraphconvolutions,whichproves\neffectiveacrossmultipledatasets.\n3. Weprovideaunifiedevaluationframeworkimplementingallusedarchitecturesandposi-\ntionalencodingsinonecodebasetofacilitatethetestingofnewpositionalencodingsand\nmodels. Thecodeismadepubliclyavailable.1\n2 RELATED WORK\nMessage-PassingNeuralNetworks(MPNNs). Earliergraphneuralnetworks(GNNs),including\nmodels like GCN (Kipf & Welling, 2016), GAT (Velicˇkovic´ et al., 2017), GraphSAGE (Hamilton\netal.,2017),andGIN(Xuetal.,2018),havepavedthewayforvariousadvancements. Somecon-\nvolutionalfilteringvariantsincorporateedgeattributesintotheirarchitecture. GatedGCN(Bresson\n& Laurent, 2017) employs gates as sparse attention mechanisms, while GINE (Hu et al., 2019)\naugmentsfeatureswithlocaledges. RecenteffortsaimtoenhancetheexpressivepowerofGNNs,\naddressing the limitations imposed by the 1-WL test. For instance, Principal Neighborhood Ag-\ngregation (PNA) (Corso et al., 2020) combines different aggregators with degree-scalers to tackle\nisomorphism tasks in continuous feature spaces. Higher-order GNNs, like k-GNN (Morris et al.,\n2019;Maronetal.,2019),buildonthek-WLalgorithm,amoregeneralizedversionoftheWLtest,\noffering increased expressive power. Other approaches, such as GSN (Bouritsas et al., 2022) and\nGIN-AK+(Zhaoetal.,2021),utilizesubstructures(subgraphs)formessagepassing,whilemethods\nlike CIN (Bodnar et al., 2021) operate on regular cell complexes, although they remain less pow-\nerfulthanthe3-WLtest. Importantly,thesemodelsserveasbaselinesinsomegraphtransformers,\ndemonstratingcomparableperformancewithcertainGTs,ascitedinGraphGPS(Rampa´sˇeketal.,\n2022),GRIT(Maetal.,2023),andExphormer(Shirzadetal.,2023).\nGraph Transformers (GTs). Graph Transformers (GT) were popularized in recent years\n(Rampa´sˇek et al., 2022; Liu et al., 2023; Mao et al., 2024; Zhang et al., 2023). Modules includ-\ningpositionalorstructuralencodings,globalattention,andlocalmessagepassingareconsideredas\nmainstreamdesigncomponentsforastandardgraphtransformermodel,whichsuccessfullysolved\nthe problem of in-scalability (Rampa´sˇek et al., 2022; Shirzad et al., 2023) in large graphs, lack of\ngraph inductive bias (Ma et al., 2023), and over-smoothing problems (Chen et al., 2022b). Apart\nfrom its maturity in some machine learning fields such as natural language processing, computer\nvision, or bioinformatics that many previous GT papers have mentioned, GTs have also demon-\nstratedtheirstrengthbyextendingtheirapplicationtoscientificdomainssuchasdifferentialequa-\ntions(Bryutkinetal.,2024;Choromanskietal.,2022),quantumphysics(Wangetal.,2022a),and\nsymbolicregression(Zhong&Meidani). Somerecentworksaretheoreticalanalysisingraphtrans-\nformers regarding the theoretical expressive power of GT (Zhou et al., 2024), and the analytical\nrelationshipbetweenpositionalencodingsinGT(Keriven&Vaiter,2024;Blacketal.,2024). How-\never, there is currently a lack of a practical benchmark that compares different types of positional\nencodings. MPNNsandGTshavebeencomparedextensivelyintheliterature,withearlyworkob-\nservingthatthesemodelscansimulateoneanother(Velicˇkovic´,2023). Amorerigoroustheoretical\nanalysishasdemonstratedthatGTscanberelatedtoMPNNswhenavirtualnodeisemployed(Cai\netal.,2023). Furthermore,ithasbeenestablishedthatGTscansimulateMPNNs,providedthatthe\npositionalencodingsaresufficientlystrong(Mu¨ller&Morris,2024). Incontrast,ourfindingsshow\n1https://github.com/ETH-DISCO/Benchmarking-PEs\n2\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nconditionsunderwhichMPNNsoperatingonfullyconnectedgraphscanachieveequalexpressive-\nnesstothatofGTs,withoutrequiringadditionalpositionalencodingsorarchitecturalmodifications.\nGTstraditionallymakeuseofpositionalencodingstoencodethegraphtopology, especiallywhen\nfullattentionisused. Weprovideanin-depthreviewofpositionalencodingsandbenchmarkingin\nSection3.1andAppendixA.1.\n3 THEORETICAL FOUNDATIONS\n3.1 POSITIONALENCODINGS\nNumerouspositionalencodingsforgraph-basedmodelshavebeendiscussedinrecentresearch,but\nthey are often scattered across various ablation studies with no unified framework. In this paper,\nwecategorizeandstreamlinetheformaldefinitionofexistinggraph-basedpositionalencodingsinto\nthreemaincategories: Laplacian-based,Randomwalk-based,andothers.\nWestartwithsomefundamentaldefinitionsrelatedtographs. LettheinputgraphbeG =(V,E,X),\nwhere X ∈ R|V| represents the node features. For any graph G, essential properties include the\ndegreematrixDandtheadjacencymatrixA.ThegraphLaplacianmatrixLisdefinedasL=D−A.\nAnormalizedgraphLaplacianisgivenbyL=I−D−1 2AD− 21 =UTΛU,wherethei-throwofU\ncorrespondstothegraph’si-theigenvectoru ,andΛisadiagonalmatrixcontainingtheeigenvalues\ni\nof L. We define a graph neural network model f(·) parameterized by Θ. We denote Xk as the\nPE\npositionalencodingfornodeK.\nLaplacian-based methodsutilizefunctionsofthek-theigenvectorU ,Λ,andparametersΘ. Ex-\nk,:\namplesincludeLaplacianPositionalEncoding(LapPE)(Rampa´sˇeketal.,2022)andSign-Invariant\nNetworks(SignNet)(Limetal.,2022).\nXk =f(U ,Λ,Θ,{·})\nPE k,:\nRandom walk-based methods are derived from polynomial function p(·) of D and A. Examples\nare Random-Walk Structural Encoding RWSE (Rampa´sˇek et al., 2022), Random-Walk Diffusion\n(RWDIFF/LSPE)(Dwivedietal.,2021),andRelativeRandomWalkProbabilityBased(RRWP)\n(Maetal.,2023).\nXk =p(D,A,{·})\nPE\nOther methods rely on different procedures, such as colors obtained by mapping 1-WL to higher\ndimensions. We thus use this umbrella class for all remaining PEs. Examples include the WL-\nbased Positional Encoding (WLPE) (Dwivedi & Bresson, 2020) and Graph Convolution Kernel\nNetworks(GCKN)(Mialonetal.,2021).Weaimtosuccinctlysummarizeandunifythesepositional\nencoding methods for better accessibility and comparison. The Appendix contains more specific\ndetails(includingequations)foreachpositionalencoding.\n3.2 MESSAGE-PASSINGNETWORKS\nMPNNs comprise multiple layers that repeatedly apply neighborhood aggregation and combine\nfunctions to learn a representation vector for each node in the graph. For an input graph G =\n(V,E,X),thei-thlayerofaMPNNcanbewrittenas\n(cid:16) (cid:16)(cid:110)(cid:110) (cid:111)(cid:111)(cid:17)(cid:17)\nc(i) =COMBINE(i) c(i−1), AGGREGATE(i) c(i−1) :w ∈N(v) ,\nv v w\nwherec(i−1)representsthestateofnodevafterlayer(i−1).\nv\n3.3 GRAPHTRANSFORMERS\nTransformermodelshavebeenwidelyusedinmodelingsequence-to-sequencedataindifferentdo-\nmains(Vaswanietal.,2017). Althoughtheattentionmechanismhascommonlybeenusedtolearn\non graph-structured data (Velicˇkovic´ et al., 2017), the use of transformers is relatively recent. A\nGTlayerreliesonaself-attentionmodulethatletsnodesattendtoasetof“neighbors”,effectively\nresultinginadependencygraphGofnodesthatcanattendtoeachother. Wewillrefertothenodes\n3\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nthatanodeucanattendtosimplyasitsneighborhoodN(u).Manyarchitecturesuse“fullattention”\nonthegraph(asopposedto“sparseattention”),meaningthatallnodescanattendtoallothernodes\nin the graph, i.e., the underlying dependency graph for attention is fully connected. Based on the\ngiven neighborhood, the generalized attention mechanism first computes attention scores α for\nu,v\nevery node u and every v ∈ N(u), based on the embeddings c(i−1) and c(i−1) from the previous\nu v\niteration, and potentially including labels for the edges (u,v) ∈ V(G). The attention coefficients\narethenusedtoweightheimportanceofneighborsandcomputeanewembeddingforuasfollows:\n \n(cid:88)\nc( ui) =Θc( ui−1)+ α u,v·δ(c v(i−1)),\nv∈N(u)\nwhereΘandδaretransformationsforembeddings.Thisdefinitionalignswithpopulararchitectures\nsuchasExphormer(Shirzadetal.,2023)andGraphGPS(Rampa´sˇeketal.,2022).WeuseExphormer\nasarunningexampletoclarifythepracticalapplicabilityofourproofs,asitsattentionmechanism\ncanattendtoarbitraryneighborhoods. InthecaseofExphormer,δbecomesalineartransformation,\nΘtheidentityfunction,andattentioncoefficientsα arecomputedviadot-productattentionthat\nu,v\nintegratesedgelabels.\nTomaintaininformationabouttheoriginaltopology,addingconnectivityinformationbackintothe\nattention mechanism is essential. This is typically done by using positional encodings. Positional\nencodingscancomeintheformofnodeencodings(Rampa´sˇeketal.,2022), whichareessentially\nfeaturesaddedtothenodesbeforetheattentionblockisapplied,oredgefeatures,whereeveryedge\nis endowed with (additional) features, such as the shortest-path-distance between the respective\nnodes(Yingetal.,2021). Inourframework,positionalencodingsaremodeledaslabelsfornodes\ninG,whereasrelativepositionalencodingscanbemodeledasedgelabels.\n3.4 BRIDGINGGTSANDWL\nIn the literature, various attempts have been made to bridge the gap between Graph Transformers\n(GTs)andtheWLtest(Mu¨ller&Morris,2024;Caietal.,2023). Thisisusuallydonebydefining\nnewvariantsoftheWLalgorithmthatapplytotheGTofinterest(Kimetal.,2022). However,we\narguethatsuchextensionsarenotnecessary. Instead,wecaninterprettheexecutionofaGTonG\nasanMPNNonanewtopologyG′ =(V,E′)correspondingtothedependencygraph,representing\ntheinformationflowintheattentionlayer(Velicˇkovic´,2023). Forexample,aGTwithfullattention\ncan be seen as an MPNN on the fully connected graph, with E′ = V ×V. Relative positional\nencodingscanbeaddedtotheMPNNasedgelabels. Thismeanswecanusethe(edge-augmented)\n1-WLalgorithmonG′ toupperboundtheexpressivepowerofaGTonG. Whileitisperhapsnot\nsurprisingthatGTexpressivitycanbeupperboundedinthisway,wealsoshowthatGTscanattain\nthisupperboundundersomereasonableassumptions. Tofacilitatethisproof,weusethesameidea\nasXuetal.(2018)toshowtheequivalencebetweentheGINarchitectureand1-WL.\nLemma3.1(AdaptedfromCorollary6byXuetal.(2018)). AssumeX isacountableset. There\nexistsafunctionf :X →Rn sothatforinfinitelymanychoicesofϵ,includingallirrationalnum-\n(cid:80)\nbers, h(c,X) = f(c) + f(x) is unique for each pair (c,X), where c ∈ X and X ⊆X\nx∈X\nis a multiset of bounded size. Moreover, any function g over such pairs can be decomposed as\n(cid:0) (cid:80) (cid:1)\ng(c,X)=φ (f(c)+(1+ϵ) f(x) forsomefunctionφ.\nx∈X\nSeeproofonpage16.\nTocompletetheproofforGTs,weadaptCorollary6bymovingtheuseofthemultiplicativefactor\nϵ ∈\nRfromf(c)totheaggregation(cid:80)\nf(x). ThisisbecausetheGTcanmultiplytheaggrega-\nx∈X\ntionbyϵusingtheattentioncoefficientswhileitcannottransformc(i−1) directly. Theϵisusedin\nu\ntheprooftodifferentiatebetweenembeddingsfromneighborsandanode’sownembedding.\nWiththeadaptedLemma,wecanprovethefollowing:\nTheorem3.2. LetG = (V,E)beagraphwithnodeembeddingsc fornodesv ∈ V. AGTlayer\nv\nonthedependencygraphG′ = (V,E′)canmapnodesv ,v ∈ V todifferentembeddingsonlyif\n1 2\nthe1-WLalgorithmusingE′assignsdifferentlabelstonodesv andv .Forequivalence,weneedδ\n1 2\n(inthedefinitionofGTs)tobeinjectiveandα =cforagivenconstantc∈Randall(u,v)∈E′,\nu,v\nmakingtheGTasexpressiveasthe1-WLalgorithm.\n4\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nSeeproofonpage17.\nTheresultimpliesthatwecanboundtheexpressivenessofaGTbythatoftheWLalgorithm. As\nanexample,GTswithfullattention,asusedbyRampa´sˇeketal.(2022)andMaetal.(2023),canbe\nboundbythe1-WLalgorithmonthefullyconnectedgraph. Inthiscase,wecaninterpretpositional\nencodingsfornodepairsasedgefeaturesonthecompletegraph.\nInthecaseofExphormer,wenoticethatδcanbeparametrizedtobeinjectivewhenusingpositional\nencodingsfornodes.Thisworksifthequeryandkeymatricesare0,leadingallattentioncoefficients\nfor a node u to be 1 , while the value matrix can be set to the identity matrix times c. The\n|N(u)|\nonlypartwhereExphormerlacksisthepowerofΘ, whichdoesnotfulfilltherequirementsinthe\ntheorem. OtherarchitectureslikeGRITmakeupforthisbyusingMLPstoencodeembeddings(Ma\netal.,2023).\nWefurthernotethatasimilarstatementcanbemadeforrewiringtechniquesthatchangethegraph’s\ntopology: Applying 1-WL to the rewired topology naturally leads to similar equivalence results.\nMotivatedbythefactthatMPNNsandGTscanbeseenasapplyinga“convolution”tosomeneigh-\nborhood, we test how well traditional message-passing convolutions like GatedGCN perform on\nthe fully-connected graph and propose a localized variant of the GRIT attention mechanism that\nconsidersalocalneighborhood.\n3.5 SPARSEGRITMESSAGE-PASSINGCONVOLUTION\nGRIT introduces two main innovations: (1) A new attention mechanism that updates edge labels\non a fully connected graph and (2) RRWP as a positional encoding. While it is relatively easy to\nuse RRWP with both other message-passing and Graph Transformer architectures, we need some\nadaptions to use the GRIT attention mechanism with message-passing GNNs on sparse graphs.\nAs motivated earlier in Section 3.4, a Graph Transformer can be seen as message-passing on a\nfully-connectedgraph. Therefore,wegeneralizetheGRITattentionmechanismdesignedforfully-\nconnectedgraphstoamessage-passingconvolutionthatworkswithanyneighborhood. Wecallthe\nresultingconvolutionSparseGRIT,asitcanattendtolocalneighborhoodsonsparsegraphsanddoes\nnotsufferfromthequadraticcomputationaloverheadthattheoriginalGRITmechanismhas. This\nmakessparseGRITmoreefficientandscalable,aswefurtherunderlineinourempiricalevaluation\ninSection5.2.\nSparseGRITutilizesthesameupdatingedgelabelseˆ astheoriginal,butonlyforedgesthatexist\ni,j\nintheoriginalgraph. Thisfurtherdistinguishestheconvolutionfromotherpopularlocalattention\nmechanismslikeGAT.ThemaindifferencetoGRITliesintheupdatefunctionxˆ fornodes,which\ni\nnowattendtotheirlocalneighborhoodinsteadofallnodesinthegraph. Itbecomes:\n(cid:88) ewj·eˆi,j\nxˆ = ·(W x +W eˆ ) (1)\ni (cid:80) ewk·eˆi,k V j EV i,j\nj∈N(i) k∈N(i)\nwhere w is the attention weight, W and W are weight matrices. In contrast to GRIT, the\nj V EV\nsummation is taken only over a node’s local neighborhood using the implementation of a sparse\nsoftmax. With these changes, sparse GRIT works the same as GRIT on a fully connected graph.\nWe, therefore, effectivelytransformtheGRITGTintoanMPNN,whichenablesustoisolateand\nanalyze what impact the graph that is used for message-passing (fully connected vs. local) has.\nResultsandempiricalanalysisofthesparseGRITandGRITareprovidedinSection5.\n4 BENCHMARKING POSITIONAL ENCODINGS\n4.1 GENERALFRAMEWORK\nThe general GNN framework we consider for our evaluation is depicted in Figure 1. More infor-\nmationontheemployeddatasetscanbefoundinAppendixA.4. Wedescribethemaincomponents\nhereandgiveanoverviewoverwhatmethodsweretested.\nDesignSpace1: PositionalEncoding. AsspecifiedinSection3.1,wetestthreetypesofgraph-\nbasedpositionalencodings,treatingthemasnodefeatureaugmentations. Morebackgroundforthe\ndifferentencodingsisgiveninAppendix3.1\n5\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nPositional Encoding Prediction Head\nLaplacian- Random-\nbased walk based Other PostGNN Pooling\nLapPE RRWP WLPE\nSignNet RWSE GCKN Predictor MLP MLP\nInput Graph ESLapPE RWDIFF Link level Node level Graph level\nConnection Type GNN layer\nGraphGPS Exphormer GINE\nGatedGCN GRIT SparseGRIT\nSparse Rewired Dense\nFigure 1: Overview of our evaluation framework, illustrating the preprocessing steps on the left\nandtheGNNmodelarchitectureontheright. Theframeworkallowsforextensiveexperimentation\nwithvariouscomponents,includingpositionalencodings,connectiontypes,andGNNlayers. This\nmodularapproachfacilitatesacomprehensiveanalysisofhowdifferentconfigurationsimpactmodel\nperformance. Inourexperimentation,wemainlyfocusonthepositionalencodingandGNNlayer,\nwhilewealsotestdifferentconnectiontypes.\nDesignSpace2: ConnectionType. Inmostreal-worldgraphdatasets,graphstendtobesparse.\nThis means that message-passing on the original topology can potentially lead to a lack of global\ninformation exchange that is necessary for the task. To mitigate this issue, GTs usually employ\nfull-attention on the complete, fully-connected graph (as discussed in Section 3.4). To change the\ntopologythatisusedforthefollowingGNNlayer,weapplyaclassicMPNNtothefully-connected\ngraphtocompareittoGTsandadaptthecurrentlybest-performingGTtorunontheoriginalgraph\nwith our SparseGRIT convolution. While Exphormer implicitly applies some degree of rewiring,\nwedonotconsiderfurtherrewiringapproachesinthisworktokeepthenumberofcomparisonsata\nreasonablelevel.\nDesignSpace3: GNNLayers. Basedonthechosentopology,weapplyseveralGNNlayersand\nbenchmarktheirperformance.OntheMPNNside,weconsiderGINE,GatedGCN,andSparseGRIT,\nwhile we use GraphGPS, Exphormer, and GRIT as classical GTs. The architectures were chosen\nduetothefactthattheyarewidelyusedandcurrentlyperformbestinleaderboardsforthetaskswe\nconsider. Otherconvolutionsandtransformerlayerscaneasilybetestedinourgeneralframework.\nDesignSpace4: PredictionHeads. Lastly,weneedtask-specificpredictionheadsthatdecodeto\neitherlinklevel,nodelevel,orgraphleveltasksforthedatasetsweconsider. Weusethesamesetup\naspopularizedbyGraphGPS(Rampa´sˇeketal.,2022)anddonotundertakefurthertestinghere.\n4.2 BENCHMARKINGFRAMEWORK\nTo enable the evaluation of models and future research for measuring the impact of positional\nencodings, we provide a unified codebase that includes the implementation of all tested models\nand the respective positional encodings. We base the code off GraphGPS Rampa´sˇek et al. (2022)\nand integrate all missing implementations. This makes for reproducible results and easy exten-\nsibility for new datasets, models, or positional encodings. Our codebase further provides readily\navailable implementations for NodeFormer (Wu et al., 2022), Difformer (Wu et al., 2023), GOAT\n(Kong et al., 2023), GraphTrans (Wu et al., 2021), GraphiT (Mialon et al., 2021), and SAT (Chen\netal.,2022a)thatarebasedontherespectiveoriginalcodebases. Thecodeispubliclyavailableat\nhttps://github.com/ETH-DISCO/Benchmarking-PEs.\nIn our experiments, we use five different random seeds for the BENCHMARKINGGNN (Dwivedi\net al., 2023) datasets and four for the others. The train-test split settings adhere to those estab-\nlished previously, employing a standard split ratio of 8:1:1. All experiments can be executed on\neitherasingleNvidiaRTX3090(24GB)orasingleRTXA6000(40GB).Toavoidout-of-memory\n(OOM)issuesonLRGBandOGBdatasets,weensurethat100GBofreservedCPUclustermemory\nis available when pre-transforming positional encodings. Configurations that did not fit into this\n6\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nCIFAR10 CLUSTER PATTERN\n76 80\n87.0\n74 75\n86.5\n72 70\n86.0\n70 65\n68 60 85.5\n66 55 85.0\n64 50 84.5\nno\nPE LapPE ESLapPE SignNet RWSE RWDIFF RRWP PPR GCKN WLPE\nMNIST ZINC\n98.50\n0.40\n98.25\n0.35\n98.00\n0.30\n97.75\n0.25\n97.50\n0.20\n97.25\n0.15\n97.00\n0.10\n96.75 0.05\nno\nPE LapPE ESLapPE SignNet RWSE RWDIFF RRWP PPR GCKN WLPE\nno\nPE LapPE ESLapPE SignNet RWSE RWDIFF RRWP PPR GCKN WLPE\nFigure 2: Performance comparison of target metrics across selected datasets from BENCHMARK-\nINGGNN.Theboxplotsillustratetheperformancerangeforallmodelsincludedinthestudy,with\nwhiskersrepresentingtheminimumandmaximumperformanceobserved. Notably,RRWPconsis-\ntentlyachievesthebestresults,whereascertainPEs,suchasSignNetonCIFAR10,cansometimes\ndecreaseperformancerelativetothebaselinewithoutPEs.\ncomputational envelope were not considered. The hyperparameters used for each architecture are\nprovidedintheAppendix,aswellasrunningtimesandmemoryusageforthePEpre-processing.\n5 EVALUATION\nBased on the framework we established in Section 4.2, we benchmark the performance of differ-\nent PEs on the BENCHMARKINGGNN (Dwivedi et al., 2023) and LRGB (Dwivedi et al., 2022)\ndatasets. Resultsforogbg-molhivandogbg-molpcbacanbefoundinAppendixA.7.\n5.1 BENCHMARKINGGNNDATASETS\nWebenchmarkstate-of-the-artmodelswithcommonlyusedPEsin-depthtoidentifythebestconfig-\nurations.ThisanalysisisoftenoverlookedwhennewPEsareintroducedalongsidenewarchitectures\nwithoutbeingevaluatedwithexistingmodels.OurapproachdecouplesthearchitecturefromthePE,\nallowingustomeasurethefullrangeofpossiblecombinations. Ourexperimentalevaluationstarts\nwith a dataset-centric approach, examining the effect of various PEs on model performance. Fig-\nure2illustratestherangeofvaluesfortherespectivetargetmetricsachievedbydifferentPEs.These\nvaluesareaggregatedoverallmodelsinouranalysis,whilemoredetailed,unaggregatedresultsare\navailable in Appendix A.7. Notably, while we could reproduce most results of previously tested\nmodelandPEcombinations,weconsistentlyobservedslightlyworsevaluesforGRIT.Thiswasthe\ncaseevenwhenusingtheofficialcodebaseandthemostup-to-datecommitatthetimeofwriting,\nwithprovidedconfigurationfilesintendedtoreproducetheresultsstatedintheoriginalpaper.\nOur findings reveal that PEs can significantly influence model performance, with the best choice\nof PE varying depending on the dataset and task. However, PEs can also negatively impact per-\nformance in some cases. For instance, while RRWP performs best on the CIFAR10 dataset and\nZINC, there are not always clear winners. Sometimes, good performance can be achieved even\nwithout any positional encoding (e.g., for PATTERN). This is also evident when examining the\nbest-performingconfigurationsforeachmodelandPE.Whilethecompleteresultsforallrunsare\n7\necnamrofreP\nnaeM\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nPeptides-func Peptides-struct PascalVOC-SP\n0.50\n0.44\n0.65\n0.45 0.42\n0.60 0.40 0.40\n0.55 0.35 0.38\n0.30 0.36\n0.50\n0.25 0.34\nno\nPE LapPE ESLapPE SignNet RWSE RWDIFF RRWP GCKN WLPE\nno\nPE LapPE ESLapPE SignNet RWSE RWDIFF RRWP GCKN WLPE\nno\nPE LapPE ESLapPE SignNet RWSE RWDIFF RRWP GCKN WLPE\nFigure3: PerformancecomparisonoftargetmetricsacrossselecteddatasetsfromtheLong-Range\nGraph Benchmark. The boxplots illustrate the performance range of all models included in the\nstudy, with whiskers indicating the minimum and maximum performance observed. Plots for the\nremainingdatasetsareprovidedinAppendixA.7.\nprovidedinAppendixA.7,wesummarizethebest-performingconfigurationsfortheBENCHMARK-\nINGGNN datasetsinTable1,indicatingwhichPEledtothebestperformanceforeachmodeland\ndataset. This enables a fair comparison of all architectures and helps determine the optimal PE\noverall.\nIn our comparison, we observe that the sparse GRIT convolution emerges as the best graph con-\nvolution for sparse topologies. It competes effectively with the full GRIT attention across most\ndatasets. Thissuggeststhatthesedatasetsdonotrequireextensivelong-rangeinformationexchange\nandcanachievestrongperformancewithsparsemessage-passing. TheGatedGCNconvolutionon\nthefully-connectedgraphdoesperformbetterthantheoriginaloverall, butgenerallylacksbehind\nattention-basedlayers. RegardingtheeffectivenessofdifferentPEs,random-walk-basedencodings\nsuch as RRWP and RWSE consistently perform well across the tested models. The only notable\nexceptionistheCLUSTERdataset,whereSignNetperformscompetitivelyforsomearchitectures,\nalthoughthebestresultsarestillachievedwithRRWP.\n5.2 LONG-RANGEGRAPHBENCHMARK\nWe extend our evaluation to the LRGB datasets and use hyperparameter configurations based on\nthosebyTo¨nshoffetal.(2023),withresultspresentedinTable2. Inthesedatasets,Laplacian-based\nencodingsgenerallyoutperformothers(exceptforthePeptidesvariations),likelyduetotheirability\ntocapturemoreglobalstructureintheslightlylargergraphs. Thismightalsobereflectedinthefact\nthat transformer-based architectures or models that facilitate global information exchange consis-\ntentlyperformbetter. Ourfindingslargelyalignwithpreviousrankings,exceptforPCQM-Contact,\nwhereweachieveanewstate-of-the-artwithExphormer,whichunderscorestheimportanceofthor-\noughbenchmarkingofexistingmodels. Figure3furtheranalyzestheperformanceoftheemployed\nPEs. ItisnoteworthythatRRWPcouldnotbeutilizedforlargerdatasetsduetoitssignificantmem-\noryfootprintandcomputationalcomplexity,similartomodelsemployingfullattentionmechanisms.\nTheresultsalignwithourpreviousanalysisandshowthatondatasetslikePeptides-func,thePEhas\nTable1:Resultsforthebest-performingmodelsandthePEtheyusefortheBENCHMARKINGGNN\ndatasets. AllrunsexceptthoseforEGTandTIGTweredonebyus. SparseGRITperformsonpar\nwithGRITonmostdatasets,indicatingthatfullattentionmightnotbenecessaryforallofthem. We\ncolorthebest,secondbest,andthirdbestmodels.\nModel CIFAR10↑ CLUSTER↑ MNIST↑ PATTERN↑ ZINC↓\nEGT(Hussainetal.,2022) 68.70±0.41 79.23±0.35 98.17±0.09 86.82±0.02 0.108±0.009\nTIGT(Choietal.,2024) 73.96±0.36 78.03±0.22 98.23±0.13 86.68±0.06 0.057±0.002\nGINE 66.14±0.31 (ESLapSE) 59.66±0.63 (SignNet) 97.75±0.10 (RWDIFF) 86.69±0.08 (RWSE) 0.075±0.006 (RWDIFF)\nGatedGCN 69.57±0.79 (RRWP) 75.29±0.05 (SignNet) 97.91±0.08 (RRWP) 86.83±0.03 (RWSE) 0.102±0.003 (RWSE)\nSparseGRIT 74.95±0.26 (RRWP) 79.87±0.08 (RRWP) 98.12±0.05 (RWSE) 87.17±0.04 (RRWP) 0.065±0.003 (RRWP)\nExphormer 75.21±0.10 (LapPE) 78.28±0.21 (SignNet) 98.42±0.18 (RRWP) 86.82±0.04 (RWSE) 0.092±0.007 (SignNet)\nGRIT 75.66±0.41 (RRWP) 79.81±0.11 (RRWP) 98.12±0.14 (RRWP) 87.22±0.03 (RRWP) 0.059±0.001 (RRWP)\nGatedGCN(FC) 71.08±0.60 (RRWP) 74.78±0.46 (SignNet) 98.20±0.15 (GCKN) 86.85±0.02 (RWSE) 0.114±0.003 (RWSE)\nGraphGPS 72.31±0.20 (noPE) 78.31±0.11 (SignNet) 98.18±0.12 (ESLapSE) 86.87±0.01 (RWSE) 0.074±0.006 (RWSE)\n8\necnamrofreP\nnaeM\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nTable2: Best-performingmodelsandPEsfortheLRGBdatasets. Weachieveanewstate-of-the-art\nforPCQM-Contact.\nModel COCO-SP↑ PCQM-Contact↑ PascalVOC-SP↑ Peptides-func↑ Peptides-struct↓\nGCN(To¨nshoffetal.,2023) 13.38±0.07 45.26±0.06 0.78±0.31 68.60±0.50 24.60±0.07\nGINE(To¨nshoffetal.,2023) 21.25±0.09 46.17±0.05 27.18±0.54 66.21±0.67 24.73±0.17\nGatedGCN(To¨nshoffetal.,2023) 29.22±0.18 46.70±0.04 38.80±0.40 67.65±0.47 24.77±0.09\nCRaWl(To¨nshoffetal.,2021) - - 45.88±0.79 70.74±0.32 25.06±0.22\nS2GCN(Geisleretal.,2024) - - - 73.11±0.66 24.47±0.32\nDRew(Gutteridgeetal.,2023) - 34.42±0.06 33.14±0.24 71.50±0.44 25.36±0.15\nGraphViT(Heetal.,2023) - - - 68.76±0.59 24.55±0.27\nGatedGCN-VN(Rosenbluthetal.,2024) 32.44±0.25 - 44.77±1.37 68.23±0.69 24.75±0.18\nExphormer 34.85±0.11 (ESLapPE) 47.37±0.24 (LapPE) 42.42±0.44 (LapPE) 64.24±0.63 (LapPE) 24.96±0.13 (LapPE)\nGraphGPS 38.91±0.33 (RWSE) 46.96±0.17 (LapPE) 45.38±0.83 (ESLapPE) 66.20±0.73 (LapPE) 24.97±0.24 (LapPE)\nSparseGRIT 19.76±0.38 (noPE) 45.85±0.11 (LapPE) 35.19±0.40 (GCKN) 67.02±0.80 (RRWP) 24.87±0.14 (LapPE)\nGRIT 21.28±0.08 (RWDIFF) 46.08±0.07 (SignNet) 35.56±0.19 (noPE) 68.65±0.50 (RRWP) 24.54±0.10 (RRWP)\naconsistentimpactontheperformance, evenwhenthevaluesareaggregatedoverdifferentarchi-\ntectures. Thisimpactcanalsobeofanegativenaturewhencomparedtothebaselinethatdoesnot\nuseanyPE.Onotherdatasets(forexamplePascalVOC-SP),thePEseemstoplayalesserroleand\ngoodresultscanbeachievedwithoutanyPE.ThecompleteresultsarereportedinAppendixA.7.\n5.3 RUNNINGTIMEANDMEMORYCOMPLEXITYFORPES\nThe computational cost of positional encodings (PEs) is a critical consideration, particularly for\nlarge graphs where methods with high complexity quickly become infeasible. We evaluated the\nrunningtimeandmemoryusageforvariousPEs,andthefullresultsarepresentedinAppendixA.8.\nRRWP is the most memory-intensive PE, but maintains reasonable running times. RWSE and\nRWDIFF,ontheotherhand,tendtohavesignificantlylongerrunningtimesbutarerelativelymore\nmemory-efficient.Laplacian-basedmethods,suchasLapPEandESLapPE,offeragoodbalancebe-\ntweencomputationalspeedandmemoryusage,makingthempracticalevenforlargerdatasets. PPR\nand GCKN come with high computational demands in both time and memory, making them less\nsuited forlarge-scale graphs. Incontrast, Laplacian-based encodingslike ESLapPEstrike a better\ntrade-off,makingthempracticalforabroaderrangeofgraphsizeswhilestillofferingcompetitive\nperformance.\n5.4 GUIDELINESFORPRACTICIONERS\nForsuperpixelgraphdatasets,suchasPascalVOC-SP,COCO-SP,MNIST,andCIFAR10,wefound\nthat the inclusion of positional encodings generally does not result in substantial performance im-\nprovements.Inparticular,largersuperpixelgraphslikePascalVOC-SPandCOCO-SPshowedmini-\nmaltonogainsfromaddingPEs,whileMNISTsimilarlyexhibitednegligiblebenefits.Anexception\ntothistrendisCIFAR10,whereRRWPdemonstratedpotentialforenhancingmodelperformance.\nThis suggests that while superpixel graphs may not typically benefit from positional encodings,\nRRWPcouldbeconsideredasacandidateforimprovement. However,thegainsobservedmaynot\nalwaysjustifytheincreasedcomputationalcomplexityassociatedwithRRWPforsuchdatasets.\nIn contrast, molecular datasets, such as ZINC and the Peptides variations, displayed a strong de-\npendency on the choice of positional encoding, with significant variations in model performance\nbased on the PE used. For instance, ZINC consistently showed the best results with PPR. On the\nother hand, the Peptides datasets revealed task-specific preferences: Peptides-func benefited the\nmost from RRWP, while Peptides-struct achieved optimal performance with WLPE. Interestingly,\ndespite using identical graph structures, the two Peptides tasks favored different PEs, which indi-\ncatesthatthenatureofthepredictiontarget(functionalvs. structural)playsasignificantrole. Thus,\nwhendealingwithmoleculardatasets,practitionersareadvisedtoexperimentwithvariousPEs,as\nthe optimal choice may depend more on the specific task than on the graph structure itself. Still,\nthe optimal PE for a given dataset is generally consistent across different models, which, in com-\nbinationwiththefactthatwetestoncommonlyuseddatasetsprovidespractitionerswithastrong\nstartingpointfortheirexperiments. Thisdistinctionisfurtherhighlightedwhencomparingrandom-\nwalk-based encodings with Laplacian encodings, where one typically emerges as the clear winner\ndependingonthedatasetandtask.\n9\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\n6 CONCLUSIONS\nThis study underscores the critical role of positional encodings in enhancing the performance of\nGraph Neural Networks (GNNs), particularly within Graph Transformer architectures. We con-\nductedathoroughcomparisonofvariouspositionalencodingsacrossawidearrayofstate-of-the-art\nmodelsandidentifytheoptimalconfigurationsfordiversedatasets,aswellasoffervaluableinsights\nintotherelationshipbetweenpositionalencodingsandmodelperformance. Whileweconsolidated\nmuch of the current state-of-the-art, we also identified new configurations that surpass the perfor-\nmanceofexistingbestmodels,suchasExphormeronPCQM-Contact. Thisunderscorestheneces-\nsityofin-depthcomparisonstoprovideafairandaccurateranking. Ourtheoreticalconsiderations\nhaveledtothedevelopmentoftheSparseGRITmodel. Thismodelshowscompetitiveperformance\nacross multiple benchmarks, while maintaining scalability to larger graphs. It shows that sparse\nmessage-passingtogetherwiththerightpositionalencodingsisaviableoptiononmanydatasets.\nFurthermore, we provide a comprehensive overview of the current state-of-the-art in graph learn-\ning and highlight the importance of selecting appropriate positional encodings to achieve optimal\nresults. Ourunifiedcodebaseincludesimplementationsofalltestedmodelsandencodings,which\nserves as a valuableresource for future research. The framework ensures reproducible results and\nsupportstheintegrationofnewdatasets,models,andpositionalencodings,therebyfacilitatingfur-\ntherexperimentation.\nLimitations. Duetocomputationalconstraints,wecouldnotexploreallpossiblehyperparameter\nconfigurationsandtheremightbeslightlybetterperformingonesthatwedidnotcatch.Additionally,\nalthoughwetestedawiderangeofmodelsandencodings,itisinfeasibletotesteverymodelandPE.\nThisiswhywefocusedoncurrentstate-of-the-artforboth. Further,ourevaluationsarebasedona\nspecificsetofbenchmarkdatasets,whichmaynotfullyrepresentthediversityofreal-worldgraph\nstructures. Thus, performanceon thesebenchmarks maynot generalizeto alltypes ofgraph data.\nNevertheless,ourunifiedcodebaseservesasarobustfoundationforfurthertestinganddevelopment,\nandenablesresearcherstoovercometheselimitationsbyfacilitatingtheinclusionofnewdatasets,\nmodels,andpositionalencodings.\n10\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nREFERENCES\nDominique Beaini, Saro Passaro, Vincent Le´tourneau, Will Hamilton, Gabriele Corso, and Pietro\nLio`. Directional graph networks. In International Conference on Machine Learning, pp. 748–\n758.PMLR,2021.\nMitchell Black, Zhengchao Wan, Gal Mishne, Amir Nayyeri, and Yusu Wang. Comparing graph\ntransformersviapositionalencodings. arXivpreprintarXiv:2402.14202,2024.\nDeyuBo,ChuanShi,LeleWang,andRenjieLiao.Specformer:Spectralgraphneuralnetworksmeet\ntransformers. InTheEleventhInternationalConferenceonLearningRepresentations,2022.\nCristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and\nMichaelBronstein. Weisfeilerandlehmangocellular: Cwnetworks. Advancesinneuralinfor-\nmationprocessingsystems,34:2625–2640,2021.\nGiorgosBouritsas,FabrizioFrasca,StefanosZafeiriou,andMichaelMBronstein. Improvinggraph\nneuralnetworkexpressivityviasubgraphisomorphismcounting. IEEETransactionsonPattern\nAnalysisandMachineIntelligence,45(1):657–668,2022.\nXavier Bresson and Thomas Laurent. Residual gated graph convnets. arXiv preprint\narXiv:1711.07553,2017.\nAndreyBryutkin,JiahaoHuang,ZhongyingDeng,GuangYang,Carola-BibianeScho¨nlieb,andAn-\ngelicaAviles-Rivero.Hamlet:Graphtransformerneuraloperatorforpartialdifferentialequations.\narXivpreprintarXiv:2402.03541,2024.\nChenCai,TruongSonHy,RoseYu,andYusuWang. Ontheconnectionbetweenmpnnandgraph\ntransformer. InInternationalConferenceonMachineLearning,pp.3408–3430.PMLR,2023.\nDexiongChen,LeslieO’Bray,andKarstenBorgwardt. Structure-awaretransformerforgraphrep-\nresentationlearning. InInternationalConferenceonMachineLearning,pp.3469–3489.PMLR,\n2022a.\nJinsongChen,KaiyuanGao,GaichaoLi,andKunHe. Nagphormer:Atokenizedgraphtransformer\nfornodeclassificationinlargegraphs. arXivpreprintarXiv:2206.04910,2022b.\nPeng Chen. Permuteformer: Efficient relative position encoding for long sequences. In Proceed-\ningsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.10606–\n10618,2021.\nYunYoungChoi,SunWooPark,MinhoLee,andYounghoWoo. Topology-informedgraphtrans-\nformer. arXivpreprintarXiv:2402.02005,2024.\nKrzysztofChoromanski,HanLin,HaoxianChen,TianyiZhang,ArijitSehanobish,ValeriiLikhosh-\nerstov,JackParker-Holder,TamasSarlos,AdrianWeller,andThomasWeingarten. Fromblock-\ntoeplitzmatricestodifferentialequationsongraphs:towardsageneraltheoryforscalablemasked\ntransformers. InInternationalConferenceonMachineLearning,pp.3962–3983.PMLR,2022.\nGabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio`, and Petar Velicˇkovic´. Principal\nneighbourhoodaggregationforgraphnets. AdvancesinNeuralInformationProcessingSystems,\n33:13260–13271,2020.\nVijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs.\narXivpreprintarXiv:2012.09699,2020.\nVijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.\nGraph neural networks with learnable structural and positional representations. arXiv preprint\narXiv:2110.07875,2021.\nVijay Prakash Dwivedi, Ladislav Rampa´sˇek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan\nLuu, and Dominique Beaini. Long range graph benchmark. Advances in Neural Information\nProcessingSystems,35:22326–22340,2022.\n11\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nVijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and\nXavierBresson. Benchmarkinggraphneuralnetworks. JournalofMachineLearningResearch,\n24(43):1–48,2023.\nDongqiFu,ZhigangHua,YanXie,JinFang,SiZhang,KaanSancak,HaoWu,AndreyMalevich,\nJingruiHe, andBoLong. Vcr-graphormer: Amini-batchgraphtransformerviavirtualconnec-\ntions. arXivpreprintarXiv:2403.16030,2024.\nJohannes Gasteiger, Aleksandar Bojchevski, and Stephan Gu¨nnemann. Predict then propagate:\nGraphneuralnetworksmeetpersonalizedpagerank. arXivpreprintarXiv:1810.05997,2018.\nSimon Geisler, Arthur Kosmala, Daniel Herbst, and Stephan Gu¨nnemann. Spatio-spectral graph\nneuralnetworks. arXivpreprintarXiv:2405.19121,2024.\nFlorian Gro¨tschla, Joe¨l Mathys, Robert Veres, and Roger Wattenhofer. Core-gd: A hierarchical\nframeworkforscalablegraphvisualizationwithgnns. arXivpreprintarXiv:2402.06706,2024.\nBenjaminGutteridge,XiaowenDong,MichaelMBronstein,andFrancescoDiGiovanni.Drew:Dy-\nnamicallyrewiredmessagepassingwithdelay.InInternationalConferenceonMachineLearning,\npp.12252–12267.PMLR,2023.\nWillHamilton,ZhitaoYing,andJureLeskovec. Inductiverepresentationlearningonlargegraphs.\nAdvancesinneuralinformationprocessingsystems,30,2017.\nXiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, and Xavier Bresson. A\ngeneralizationofvit/mlp-mixertographs. InInternationalConferenceonMachineLearning,pp.\n12724–12745.PMLR,2023.\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure\nLeskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265,\n2019.\nWeihuaHu,MatthiasFey,MarinkaZitnik,YuxiaoDong,HongyuRen,BowenLiu,MicheleCatasta,\nandJureLeskovec. Opengraphbenchmark: Datasetsformachinelearningongraphs. Advances\ninneuralinformationprocessingsystems,33:22118–22133,2020.\nYinan Huang, William Lu, Joshua Robinson, Yu Yang, Muhan Zhang, Stefanie Jegelka, and Pan\nLi. Onthestabilityofexpressivepositionalencodingsforgraphneuralnetworks. arXivpreprint\narXiv:2310.02579,2023.\nMd Shamim Hussain, Mohammed J Zaki, and Dharmashankar Subramanian. Edge-augmented\ngraphtransformers:Globalself-attentionisenoughforgraphs. arXivpreprintarXiv:2108.03348,\n3,2021.\nMdShamimHussain,MohammedJZaki,andDharmashankarSubramanian. Globalself-attention\nasareplacementforgraphconvolution. InProceedingsofthe28thACMSIGKDDConferenceon\nKnowledgeDiscoveryandDataMining,pp.655–665,2022.\nWeiJu,SiyuYi,YifanWang,ZhipingXiao,ZhengyangMao,HourunLi,YiyangGu,YifangQin,\nNan Yin, Senzhang Wang, et al. A survey of graph neural networks in real world: Imbalance,\nnoise,privacyandoodchallenges. arXivpreprintarXiv:2403.04468,2024.\nGuolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In\nInternationalConferenceonLearningRepresentations,2020.\nNicolasKerivenandSamuelVaiter. Whatfunctionscangraphneuralnetworkscomputeonrandom\ngraphs? theroleofpositionalencoding. AdvancesinNeuralInformationProcessingSystems,36,\n2024.\nJinwooKim,DatNguyen,SeonwooMin,SungjunCho,MoontaeLee,HonglakLee,andSeunghoon\nHong.Puretransformersarepowerfulgraphlearners.AdvancesinNeuralInformationProcessing\nSystems,35:14582–14595,2022.\n12\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-\nworks. arXivpreprintarXiv:1609.02907,2016.\nRisiKondorand Jean-PhilippeVert. Diffusionkernels. kernelmethodsin computationalbiology,\npp.171–192,2004.\nKezhiKong,JiuhaiChen,JohnKirchenbauer,RenkunNi,CBayanBruss,andTomGoldstein.Goat:\nA global transformer on large-scale graphs. In International Conference on Machine Learning,\npp.17375–17390.PMLR,2023.\nDevinKreuzer,DominiqueBeaini,WillHamilton,VincentLe´tourneau,andPrudencioTossou. Re-\nthinkinggraphtransformerswithspectralattention. AdvancesinNeuralInformationProcessing\nSystems,34:21618–21629,2021.\nWeirui Kuang, WANG Zhen, Yaliang Li, Zhewei Wei, and Bolin Ding. Coarformer: Transformer\nforlargegraphviagraphcoarsening. 2021.\nPanLi, YanbangWang, HongweiWang, andJureLeskovec. Distanceencoding: Designprovably\nmorepowerfulneuralnetworksforgraphrepresentationlearning.AdvancesinNeuralInformation\nProcessingSystems,33:4465–4478,2020.\nYi-LunLiaoandTessSmidt. Equiformer: Equivariantgraphattentiontransformerfor3datomistic\ngraphs. arXivpreprintarXiv:2206.11990,2022.\nDerekLim,JoshuaRobinson,LingxiaoZhao,TessSmidt,SuvritSra,HaggaiMaron,andStefanie\nJegelka. Sign and basis invariant networks for spectral graph representation learning. arXiv\npreprintarXiv:2202.13013,2022.\nJiaweiLiu,ChengYang,ZhiyuanLu,JunzeChen,YiboLi,MengmeiZhang,TingBai,YuanFang,\nLichaoSun,PhilipSYu,etal. Towardsgraphfoundationmodels: Asurveyandbeyond. arXiv\npreprintarXiv:2310.11829,2023.\nShengjie Luo, Tianlang Chen, Yixian Xu, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He.\nOne transformer can understand both 2d & 3d molecular data. In The Eleventh International\nConferenceonLearningRepresentations,2022a.\nShengjieLuo, ShandaLi, ShuxinZheng, Tie-YanLiu, LiweiWang, andDiHe. Yourtransformer\nmaynotbeaspowerfulasyouexpect. AdvancesinNeuralInformationProcessingSystems,35:\n4301–4315,2022b.\nLiheng Ma, Chen Lin, Derek Lim, Adriana Romero-Soriano, Puneet K Dokania, Mark Coates,\nPhilipTorr,andSer-NamLim. Graphinductivebiasesintransformerswithoutmessagepassing.\nInInternationalConferenceonMachineLearning,pp.23321–23337.PMLR,2023.\nHaitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah, Michael\nGalkin,andJiliangTang. Graphfoundationmodels. arXivpreprintarXiv:2402.02216,2024.\nHaggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph\nnetworks. Advancesinneuralinformationprocessingsystems,32,2019.\nDominic Masters, Josef Dean, Kerstin Klaser, Zhiyi Li, Sam Maddrell-Mander, Adam Sanders,\nHatem Helal, Deniz Beker, Ladislav Rampa´sˇek, and Dominique Beaini. Gps++: An optimised\nhybrid mpnn/transformer for molecular property prediction. arXiv preprint arXiv:2212.02229,\n2022.\nGre´goire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. Graphit: Encoding graph\nstructureintransformers,2021.\nChristopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav\nRattan,andMartinGrohe. Weisfeilerandlemangoneural: Higher-ordergraphneuralnetworks.\nInProceedingsoftheAAAIconferenceonartificialintelligence,volume33,pp.4602–4609,2019.\nLuisMu¨llerandChristopherMorris. Aligningtransformerswithweisfeiler-leman. arXivpreprint\narXiv:2406.03148,2024.\n13\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nWonpyoPark,WoonggiChang,DonggeonLee,JuntaeKim,andSeung-wonHwang.Grpe:Relative\npositionalencodingforgraphtransformer. arXivpreprintarXiv:2201.12787,2022.\nLadislav Rampa´sˇek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Do-\nminiqueBeaini. Recipeforageneral,powerful,scalablegraphtransformer. AdvancesinNeural\nInformationProcessingSystems,35:14501–14515,2022.\nEran Rosenbluth, Jan To¨nshoff, Martin Ritzert, Berke Kisin, and Martin Grohe. Distinguished in\nuniform: Selfattentionvs.virtualnodes. arXivpreprintarXiv:2405.11951,2024.\nPeterShaw,JakobUszkoreit,andAshishVaswani. Self-attentionwithrelativepositionrepresenta-\ntions. InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociation\nfor Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp.\n464–468,2018.\nHamed Shirzad, Ameya Velingker, Balaji Venkatachalam, Danica J Sutherland, and Ali Kemal\nSinop. Exphormer: Sparse transformers for graphs. In International Conference on Machine\nLearning,pp.31613–31632.PMLR,2023.\nJanTo¨nshoff,MartinRitzert,HinrikusWolf,andMartinGrohe.Walkingoutoftheweisfeilerleman\nhierarchy: Graphlearningbeyondmessagepassing. arXivpreprintarXiv:2102.08786,2021.\nJanTo¨nshoff,MartinRitzert,EranRosenbluth,andMartinGrohe. Wheredidthegapgo? reassess-\ningthelong-rangegraphbenchmark. arXivpreprintarXiv:2309.00367,2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntionprocessingsystems,30,2017.\nPetarVelicˇkovic´. Everythingisconnected: Graphneuralnetworks. CurrentOpinioninStructural\nBiology,79:102538,2023.\nPetar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\nBengio. Graphattentionnetworks. arXivpreprintarXiv:1710.10903,2017.\nHanruiWang,PengyuLiu,JingleiCheng,ZhidingLiang,JiaqiGu,ZiruiLi,YongshanDing,Wei-\nwenJiang,YiyuShi,XuehaiQian,etal. Graphtransformerforquantumcircuitreliabilitypredic-\ntion. 2022a.\nHaoruiWang,HaotengYin,MuhanZhang,andPanLi. Equivariantandstablepositionalencoding\nformorepowerfulgraphneuralnetworks. arXivpreprintarXiv:2203.00199,2022b.\nQitianWu,WentaoZhao,ZenanLi,DavidPWipf,andJunchiYan. Nodeformer: Ascalablegraph\nstructurelearningtransformerfornodeclassification.AdvancesinNeuralInformationProcessing\nSystems,35:27387–27401,2022.\nQitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, and Junchi Yan. Dif-\nformer: Scalable (graph) transformers induced by energy constrained diffusion. arXiv preprint\narXiv:2301.09474,2023.\nZhanghaoWu,ParasJain,MatthewWright,AzaliaMirhoseini,JosephEGonzalez,andIonStoica.\nRepresenting long-range context for graph neural networks with global attention. Advances in\nNeuralInformationProcessingSystems,34:13266–13279,2021.\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A\ncomprehensive survey on graph neural networks. IEEE transactions on neural networks and\nlearningsystems,32(1):4–24,2020.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\nnetworks? InInternationalConferenceonLearningRepresentations,2018.\nChengxuanYing,TianleCai,ShengjieLuo,ShuxinZheng,GuolinKe,DiHe,YanmingShen,and\nTie-YanLiu. Dotransformersreallyperformbadlyforgraphrepresentation? Advancesinneural\ninformationprocessingsystems,34:28877–28888,2021.\n14\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nManzilZaheer,SatwikKottur,SiamakRavanbakhsh,BarnabasPoczos,RussRSalakhutdinov,and\nAlexanderJSmola. Deepsets. Advancesinneuralinformationprocessingsystems,30,2017.\nZiwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, and Wenwu Zhu. Graph meets\nllms: Towardslargegraphmodels. InNeurIPS2023Workshop: NewFrontiersinGraphLearn-\ning,2023.\nLingxiaoZhao,WeiJin,LemanAkoglu,andNeilShah. Fromstarstosubgraphs: Upliftinganygnn\nwithlocalstructureawareness. arXivpreprintarXiv:2110.03753,2021.\nWeihengZhongandHadiMeidani. Agraphtransformerforsymbolicregression.\nCai Zhou, Rose Yu, and Yusu Wang. On the theoretical expressive power and the design space\nof higher-order graph transformers. In International Conference on Artificial Intelligence and\nStatistics,pp.2179–2187.PMLR,2024.\nJie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,\nChangcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applica-\ntions. AIopen,1:57–81,2020.\n15\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nA APPENDIX\nA.1 EXTENDEDRELATEDWORK\nPositionalEncodingsforGraphs. Positionalencodingsaretraditionallyusedinnaturallanguage\nprocessingtocapturetheabsolutepositionofatokenwithinasentence(Vaswanietal.,2017)orthe\nrelativedistancebetweenpairsoftokens(Shawetal.,2018;Keetal.,2020;Chen,2021). Similarly,\npositional encoding in graphs aims to learn both local topology and global structural information\nof nodes efficiently. This approach has been successfully implemented with the introduction of\nthe graph transformer (Dwivedi & Bresson, 2020). With the advent of graph transformers in the\nfieldofgraphrepresentationlearning,manytraditionalgraphtheorymethodshavebeenrevitalized.\nGraph signal processing techniques have been employed such as Laplacian decomposition and fi-\nnitehoprandomwalks(Rampa´sˇeketal.,2022;Dwivedietal.,2023;Maetal.,2023;Beainietal.,\n2021;Dwivedi&Bresson,2020;Kreuzeretal.,2021;Dwivedietal.,2021;Limetal.,2022;Wang\net al., 2022b) as absolute or relative positional encoding. Node properties such as degree central-\nity (Ying et al., 2021) and personalized PageRank (PPR) (Gasteiger et al., 2018; Fu et al., 2024)\ncouldbemappedandexpandedintohigherdimensionsforabsolutepositionalencoding,whilethe\nshortestdistancebetweennodescouldbeusedforrelativepositionalencoding(Lietal.,2020;Ying\netal.,2021). Recentstudieshavefocusedondevelopinglearnablepositionalencodingsforgraphs\n(Yingetal.,2021)andexploringtheirexpressivenessandstabilityaswell(Wangetal.,2022b;Ma\netal.,2023;Huangetal.,2023). Additionally,graphrewiringcombinedwithlayoutoptimizationto\ncoarsengraphshasbeenproposedasaformofpositionalencoding(Gro¨tschlaetal.,2024).\nGNN Benchmarking. One of the first GNN benchmarking papers compared architectures with\nandwithoutpositionalencodings(PEs)(Dwivedietal.,2023),wheretheirPEmainlyreferstoLapla-\ncianpositionalencoding(LapPE).TheirstudywaslimitedtotheGatedGCNmodelanddiscussed\ntheexpressivepower,robustness,andefficiencyofstate-of-the-artmessage-passingmethods. Addi-\ntionally,severalsurveyshavebenchmarkedthecomplexity,specifictasks,unifiedmessage-passing\nframeworks(Wuetal.,2020;Zhouetal.,2020),robustness,andprivacy(Juetal.,2024).TheLRGB\ndataset(Dwivedietal.,2022)hasbeentestedinbothGNNsandtransformerstodemonstratethesu-\nperiority of Graph Transformers (GTs) over Message Passing Neural Networks (MPNNs). Many\nstate-of-the-art GTs have included this benchmark in their experiments (Rampa´sˇek et al., 2022;\nShirzadetal.,2023;Maetal.,2023). OnelimitationoftheLRGBbenchmarkisthatLRGBonly\nconsidersLapPEand randomwalkstructuralencodings (RWSE).Onenotablework benchmarked\nusing LRGB by fine-tuning the architectures of GraphGPS and pre-processing (To¨nshoff et al.,\n2023). Weadopttheirsettingsbutplacegreateremphasisontheeffectofpositionalencodings.\nA.2 PROOFS\nLemma3.1(AdaptedfromCorollary6byXuetal.(2018)). AssumeX isacountableset. There\nexistsafunctionf :X →Rn sothatforinfinitelymanychoicesofϵ,includingallirrationalnum-\n(cid:80)\nbers, h(c,X) = f(c) + f(x) is unique for each pair (c,X), where c ∈ X and X ⊆X\nx∈X\nis a multiset of bounded size. Moreover, any function g over such pairs can be decomposed as\n(cid:0) (cid:80) (cid:1)\ng(c,X)=φ (f(c)+(1+ϵ) f(x) forsomefunctionφ.\nx∈X\nProofofLemma3.1. We slightly tightly follow the proof by Xu et al. (2018) for Corollary 6, but\n(cid:80)\ndefinehash(c,X)≡f(c)+(1+ϵ) f(x)(withf definedasintheoriginalproof). Wethen\nx∈X\nwanttoshowthatforany(c′,X′) ̸= (c,X)withc,c′ ∈ X andX,X′ ⊂ X,h(c,X) ̸= h(c′,X′)\nholds, if ϵ is an irrational number. We show the same contradiction as Xu et al. (2018): For any\n(c,X), suppose there exists (c′,X′) such that (c′,X′) ̸= (c,X) but h(c,X) = h(c′,X′) holds.\nWe consider the following two cases: (1) c′ = c but X′ ̸= X, and (2) c′ ̸= c. For the first case,\nh(c,X) = h(c,X′) implies (cid:80) f(x) = (cid:80) f(x). By Lemma 5 from Xu et al. (2018) it\nx∈X x∈X′\nfollowsthatequalitywillnothold. Forthesecondcase,wecanrewriteh(c,X) = h(c′,X′)asthe\nfollowingequation:\n(cid:32) (cid:33) (cid:32) (cid:33) (cid:32) (cid:33)\n(cid:88) (cid:88) (cid:88) (cid:88)\nϵ· f(x)− f(x) = f(c′)+ f(x) − f(c)+ f(x)\nx∈X x∈X′ x∈X′ x∈X\n16\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\n(cid:80) (cid:80)\nWe assume ϵ to be irrational, and if f(x) − f(x) ̸= 0, then the left side of the\nx∈X x∈X′\n(cid:80) (cid:80)\nequationisirrational,whiletherightsideisrational. If f(x)− f(x) = 0,thenthe\nx∈X x∈X′\nequationreducestof(c′)=f(c),alsoacontradiction.\nTheorem3.2. LetG = (V,E)beagraphwithnodeembeddingsc fornodesv ∈ V. AGTlayer\nv\nonthedependencygraphG′ = (V,E′)canmapnodesv ,v ∈ V todifferentembeddingsonlyif\n1 2\nthe1-WLalgorithmusingE′assignsdifferentlabelstonodesv andv .Forequivalence,weneedδ\n1 2\n(inthedefinitionofGTs)tobeinjectiveandα =cforagivenconstantc∈Randall(u,v)∈E′,\nu,v\nmakingtheGTasexpressiveasthe1-WLalgorithm.\nProofofTheorem3.2. First, we show that a GT is bounded by 1-WL on the same topology by\nshowing that 1-WL is at least as powerful as a graph transformer. As 1-WL hashes all neighbor\nstateswithaninjectivefunction,wecanobservestatesfromallnodesinthegraphintheaggregated\nmultiset at node v, including possible edge labels. This information is sufficient to compute the\nresultoftheattentionmoduleateverynode.\nFortheotherdirection,wecanmakeuseofLemma3.1bysettingctothedesiredϵandfollowthe\nsame proof as (Xu et al., 2018). Note that Θ and δ have to be powerful enough such that we can\napplytheuniversalapproximationtheorem.\nA.3 TESTEDGNNARCHITECTURES\nMessagePassingNeuralNetworks(MPNN). Formessagepassingneuralnetworks,weprimar-\nilychooseGatedGCN(Bresson&Laurent,2017)andGINE(Huetal.,2019).Themessage-passing\nupdateruleforGateGCNisasfollows:\n \nxℓ i+1 =f Gℓ -GCNN(cid:0) xℓ i,{xℓ j :j →i}(cid:1) =ReLUUℓxℓ i +(cid:88) η ij ⊙Vℓxℓ j (2)\nj→i\nwhere x ,j ∈ N(i) are node features, and η are edge gates which are employed by η =\nj ij ij\nσ(cid:0) Aℓxℓ+Bℓxℓ(cid:1)\n. TheupdateforGINEisdefinedasfollows:\ni j\n \n(cid:88)\nx′\ni\n=h Θ(1+ϵ)·x i+ ReLU(x\nj\n+e i,j) (3)\nj∈N(i)\nwhereϵisahyper-parameterasspecifiedinGINpaper,edgeinformatione isinjectedintoindi-\ni,j\nvidualnodefeatures,andMLPh(·)isparameterizedbyΘ.\nOurrationaleisasfollows:\n1. When observing popular MPNNs such as DGN (Beaini et al., 2021), PNA (Corso et al.,\n2020), and GSN (Bouritsas et al., 2022), they are not consistently scalable or tested on\nmedium-scaledatasetsasthoroughlyasclassicalMPNNslikeGatedGCNandGINE,as\nindicatedinGraphGPS(Rampa´sˇeketal.,2022). Thislimitationisalsoevidentincurrent\nstate-of-the-artgraphneuralnetworkslikeCIN(Bodnaretal.,2021)andGIN-AK+(Zhao\netal.,2021),whichlackreportedresultsonlarge-scalegraphs,suchasmostdatasetsfrom\ntheOpenGraphBenchmark(Huetal.,2020;Rampa´sˇeketal.,2022).\n2. Most graph transformers incorporate edge information, making direct comparisons to\nGNNs without edge information unfair. Graph convolution networks (GCN) (Kipf &\nWelling,2016)andGraphIsomorphismNetwork(GIN)(Xuetal.,2018)potentiallylack\nthese updates. Hence, we use modified convolutional graph filters that include edge at-\ntributesinmessagepassing,specificallyGatedGCNandGINE.\n3. We aim to investigate if the results from GatedGCN on fully connected graphs are com-\nparable to those from GT on sparse graphs. In addition to the above points, an improved\nGatedGCN architecture has been found to perform on par with GT on the peptides-func\nand peptides-struct datasets (To¨nshoff et al., 2023). This finding motivates us to explore\ntheeffectsofpositionalencodingsontheGatedGCNarchitecture.\n17\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nByfocusingonGatedGCNandGINE,weaimtoleveragetheirestablishedscalabilityandperfor-\nmanceonmediumtolarge-scaledatasets,whilefairlycomparingtheiredgeattributecapabilitiesto\ngraphtransformers.\nGraphTransformers. CurrentGraphTransformerscanbedividedintotwofamilies:\n• GTswithonlyfullattentionlayers. ThisfamilyincludesthefollowingGT(s): EGT(Hus-\nsainetal.,2021),GKAT(Choromanskietal.,2022),GRIT(Maetal.,2023),NAGphormer\n(Chen et al., 2022b) , Vanilla GT (Dwivedi & Bresson, 2020), GraphiT (Mialon et al.,\n2021), GRPE(Parketal.,2022), SignNet(Limetal.,2022), SAN(Kreuzeretal.,2021),\nSpecformer(Boetal.,2022),TokenGT(Kimetal.,2022),andTransformer-M(Luoetal.,\n2022a).\n• GTs with additional message passing layers as an inductive bias. This family includes\nthefollowingGT(s): Coarformer(Kuangetal.,2021),Equiformer(Liao&Smidt,2022),\nExphormer(Shirzadetal.,2023),GOAT(Kongetal.,2023),GraphGPS(Rampa´sˇeketal.,\n2022), Graphormer (Ying et al., 2021), GPS++ (Masters et al., 2022), GraphTrans (Wu\net al., 2021), SAT (Chen et al., 2022a), NodeFormer (Wu et al., 2022), and URPE (Luo\netal.,2022b).\nInthisresearch,weselectGRITfromthefirstclass,andGraphGPSandExphormerfromthesecond\nclass. Othermodelscanalsobeclassifiedintooneofthesetwocategories.\nGraphGPS Update. As specified in the original paper, the model follows a pattern where the\noutput fromglobal attentionlayers interactswith theoutput from aglobal attention(vanilla trans-\nformer)layer. Inthiscontext,Xrepresentsthenodefeatures,Erepresentstheedgefeatures,andA\nistheadjacencymatrix.\nXˆℓ+1,Eℓ+1 =MPNNℓ(Xℓ,Eℓ,A),\nM e\nXˆℓ+1 =GlobalAttnℓ(Xℓ),\nT\n(cid:16) (cid:16) (cid:17) (cid:17)\nXℓ+1 =BatchNorm Dropout Xˆℓ+1 +Xℓ ,\nM M\n(cid:16) (cid:16) (cid:17) (cid:17)\nXℓ+1 =BatchNorm Dropout Xˆℓ+1 +Xℓ ,\nT T\nXℓ+1 =MLPℓ(cid:0) Xℓ+1+Xℓ+1(cid:1)\nM T\nExphormerUpdate. AsspecifiedintheExphormerpaperandobservedfromitsimplementation,\nthemodelfollowsatrainingpatternsimilartothatusedinSAN(Kreuzeretal.,2021):\n(cid:88)h (cid:18)(cid:16) (cid:17)T (cid:16) (cid:17)(cid:19)\nATTN (X) =x + Wj Wj X ·σ Wj E ⊙Wj X Wj x\nH :,i i O V NH(i) E NH(i) K NH(i) Q i\nj=1\nwhere X is the node features, and E is the edge features. The most important aspect is that they\ncomputethelocalsparseattentionmechanismusing1)virtualnodesand2)expandergraphs.\nA.4 DATASETS\nStatisticsandpredictiontasksarelistedinTable3. LicensesforeachdatasetsarelistedinTable4.\nBenchmarkingGNN include MNIST, CIFAR10, CLUSTER, PATTERN, and ZINC, following the\nprotocolsestablishedinGraphGPS(Rampa´sˇeketal.,2022),Exphormer(Shirzadetal.,2023),and\nGRIT (Maetal.,2023). ThesedatasetshavetraditionallybeenemployedforbenchmarkingGraph\nNeuralNetworks(GNNs)(Dwivedietal.,2023),excludinggraphtransformers.Inthispaper,wead-\nheretotheseestablishedsettingsbutaimtorevisitbothmessagepassingneuralnetworks(MPNNs)\nandgraphtransformers.\nLong-Range Graph Benchmark (LRGB) (Dwivedi et al., 2022) encompasses Peptides-func,\nPeptides-struct,PascalVOC-SP,PCQM-Contact,andCOCO.Graphlearninginthiscontextisheav-\nilyinfluencedbytheinteractionsbetweenpairsoflong-rangevertices. Priorresearchhasexplored\n18\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nthe potential optimal hyperparameters for both MPNNs and GTs within the LRGB framework\n(To¨nshoff et al., 2023). Our objective is to identify the most effective combination of GNN ar-\nchitecturesandpositionalencodingstrategies.\nOpenGraphBenchmark(OGB)(Huetal.,2020)includes: 1)node-leveltaskslikeOGBN-Arxiv\nand2)graph-leveltaskslikeOGBG-MOLHIV andOGBG-MOLPCBA.Thesedatasetsareconsider-\nablylargerinscalecomparedtotheaforementionedbenchmarks. Ourgoalistodiscoverscalable\npositionalencodingmethods,asconventionalgraphLaplaciandecompositionforpositionalencod-\ningisnotfeasibleforlargegraphs.\nTable3: Statisticsforeachdataset\nDataset #Graphs Avg.|N| Avg.|E| Directed Predictionlevel Predictiontask Metric\nZINC 12,000 23.2 24.9 No graph regression MeanAbs.Error\nMNIST 70,000 70.0 564.5 Yes graph 10-classclassif. Accuracy\nCIFAR10 60,000 117.6 941.1 Yes graph 10-classclassif. Accuracy\nPATTERN 14,000 118.9 2,359.2 No inductivenode binaryclassif. Accuracy\nCLUSTER 12,000 117.2 1,510.9 No inductivenode 6-classclassif. Accuracy\nPascalVOC-SP 11,355 479.4 2,710.5 No inductivenode 21-classclassif. F1score\nCOCO-SP 123,286 476.4 2,693.7 No inductivenode 81-classclassif. F1score\nPCQM-Contact 529,434 30.1 69.1 No inductivelink linkranking MRR(Fil.)\nPeptides-func 15,535 150.9 307.3 No graph 10-taskclassif. Avg.Precision\nPeptides-struct 15,535 150.9 307.3 No graph 11-taskregression MeanAbs.Error\nogbn-arxiv 1 169,343 1,166,243 Yes transductivenode 40-classclassif. Accuracy\nogbg-molhiv 41,127 25.5 27.5 No graph binaryclassif. AUROC\nogbg-molpcba 437,929 26.0 28.1 No graph 128-taskclassif. Avg.Precision\nIn future work, we hope to add more large-scale inductive datasets such as OGBG-PPA, OGBG-\nCode2andPCQM4Mv2(Huetal.,2020),andtransductivedatasetssuchasCS,PhysicsandCom-\nputerandPhoto(Shirzadetal.,2023)intocomparison.\nA.5 POSITIONALENCODINGS\nA.5.1 LAPLACIANBASEDMETHODS\nWedefineLastheLaplacianmatrixforourin-\nput graph G = (V,E) . According to graph\ntheory, as it’s positive semidefinite and sym- Table4: Datasetlicenses.\nmetric,itcouldbefurtherdecomposedasL = Dataset License\n(cid:80) λ u uT, where λ is the eigenvalue and u ZINC MITLicense\ni i i i i i MNIST CCBY-SA3.0andMITLicense\nis the eigenvector. Under a unified scheme of CIFAR10 CCBY-SA3.0andMITLicense\npositionalencodingforgraphneuralnetworks, PATTERN MITLicense\nCLUSTER MITLicense\nwe define a normalized graph Laplacian L =\nPascalVOC-SP CustomlicenseandCCBY4.0License\nI −D− 21AD−1 2 = UTΛU where i-th row of COCO-SP CustomlicenseandCCBY4.0License\nPCQM-Contact CCBY4.0License\nU corresponds to the graph’s i-th eigenvector\nPeptides-func CCBY-NC4.0License\nu i, and Λ is a diagonal matrix containing all Peptides-struct CCBY-NC4.0License\neigenvalues. Under the Laplacian-based set- ogbn-arxiv MITLicense\nogbg-molhiv MITLicense\ntings, wecouldexpresseachpositionalencod-\nogbg-molpcba MITLicense\ningfornodekinasimilarwayby:\nXk =f(U ,Λ,Θ,{·}) (4)\nPE k,:\nwhereU representsthei-throwofU,Λisadiagonalmatrixcontainingalleigenvalues,Θisthe\nk,:\nfunctionparameterswhichrepresentthelinearornon-linearoperationsonU andΛ,and{·}isthe\nadditional parameters that are utilized by each method individually. We consider three Laplacian-\nbasedmethods: LaplacianPositionalEncoding(LapPE),Sign-InvariantPositionalEncoding(Sign-\nNet),andrectifiedGraphConvolutionKernelNetwork-basedPositionalEncoding(GCKN).\nLapPE(Rampa´sˇeketal.,2022) LapPE,orLaplacianPositionalEncoding,isamethodthatlever-\nagestheeigenvectorsofthegraphLaplaciantoencodepositionalinformationfornodes. Thecore\nidea is that the eigenvectors corresponding to higher eigenvalues contain more information about\nthelocalstructureofthegraph, especiallytherelationshipsbetweenanodeanditsneighbors. We\ncan further concatenate the eigenvectors with their corresponding eigenvalue. In the actual imple-\nmentation, an additional parameter S is employed to randomly split the sign of this concatenated\n19\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\neigenvector. Subsequently,weapplyeitheraDeepSet(Zaheeretal.,2017)oranMLPΦparameter-\nizedbyΘtothiseigenvector.\nXk =f(U ,Λ,Θ,S) (5)\nPE k,:\n=Φ (S⊙(U ∥Λ )) (6)\nΘ k,: k\nThe equivariant and stable version of LapPE (ESLapPE) (Wang et al., 2022b) follows the same\nprocedurebutomitsthepost-processingMLP.\nSignNet (Lim et al., 2022) SignNet is an advanced version of LapPE, which considers both the\noriginal eigenvector and its inversely signed counterparts. An additional graph neural network\n(GNN) is applied to capture local Laplacian signals before passing them to the MLP. The out-\nputs from two distinct or shared GNNs are then added together. This approach is proven to be\nsign-invariant and capable of approximating any continuous function of eigenvectors with the de-\nsiredinvariances(Limetal.,2022). WhilewedidnotconsiderBasisNetinthiswork, researchers\ncouldfurtherexploreitsinclusionintheirstudiesforcomparisonwithSignNet. Theexpressionfor\nSignNetis:\nXk =f(U ,Λ,Θ,A) (7)\nPE i,:\n=Φ (GNN(U ∥Λ ,A)+GNN((−1)⊙(U ∥Λ ),A)) (8)\nΘ k,: k k,: k\nGCKN(Mialonetal.,2021) AccordingtoGraphiT(Mialonetal.,2021),therearetwowaysto\nconstruct the new graph Laplacian matrix, either by using diffusion kernels (GCKN) or a p-step\nrandomwalkkernel(p-RWSE).Here,weintroducethediffusionkernel(p-stepissimilar)wherethe\nnewLaplacianiscomputedbymultiplyingbyinverseβ andthenplacedontotheexponentialofe.\nU istheneweigenvectormatrix. ThemethodissimilartoLapPE,buttheadditionalparameterβ is\nusedtocontrolthediffusionprocess(Kondor&Vert,2004). TheexpressionforGCKNisgivenby:\nXk =f(U ,Λ,Θ,{S,β}) (9)\nPE k,:\n=Φ (S⊙(U ∥Λ )),whereUTΛU =e−βL (10)\nΘ k,: k k k\nA.5.2 RANDOMWALKBASEDMETHODS\nWe denote p as a polynomial function. From the following settings, we can see that this class of\nmethodstakesapolynomialofD−1A,whichis:\nXk =p(D,A,{·}) (11)\nPE\nRWSE(Rampa´sˇeketal.,2022) RandomWalkStructuralEncoding(RWSE)encodesthegraph\nstructure by computing the frequencies of random walks starting from each node. Specifically,\nthe RWSE method calculates the probabilities of nodes being visited at each step of the random\nwalk. Thisapproachutilizesthepolynomial(D−1A)k,whereD isthedegreematrixandAisthe\nadjacencymatrix, torepresenttheresultofak-steprandomwalk. Forthebiasedversion, weights\nθ areusedtoweighttheresultsofeachstep. Theformulaisasfollows:\nk\nXk =p(D,A,K) (12)\nPE\nK K\n(cid:88) (cid:88)\n= (D−1A)k,or = θ (D−1A)k ifbiased (13)\nk\nk=1 k=1\nRWDIFF(LSPE) (Dwivediet al.,2021) Learnablepositional encoding, on theotherhand, can\nencode positions through random walk diffusion and decouples structural and positional encoding\n(Dwivedi et al., 2021). Unlike RWSE, RWDIFF concatenates the random walk diffusion features\nfromeachtimestep(newdimension),whileRWSEdirectlyaddsthosek-steprandomwalkmatrices.\nThe initial condition where no random walk is performed is also considered, with the additional\nparameterI,whichistheidentitymatrix. Theformulaisasfollows:\nXk =p(D,A,{K,I}) (14)\nPE\n=[I,D−1A,(D−1A)2,(D−1A)3,...,(D−1A)K−1] (15)\nk,k\n=I ∥(D−1A) ∥(D−1A)2 ∥(D−1A)3 ∥...∥(D−1A)K−1 (16)\nk,k k,k k,k k,k k,k\n20\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nRRWP (Ma et al., 2023) One improvement on RWDIFF in GRIT (Ma et al., 2023) is that the\nsparse graph is connected to a fully connected graph for each graph in the batch, as well as an\nadditionupdateintheedgefeatureswhereithasshownabetterperformanceonLRGBbenchmark.\nIt is stated that it is at least as expressive as a biased RWSE (Ma et al., 2023). The form of Xk\nPE\nisnotchangedhere, insteadwementiontheedgeattributes(asthestructuralencoding). Theedge\nfeature is also indicated by random walk diffusion, however, the off-diagonal entry indicates the\nedgefeatures,whichisrepresentedbyaformofprobabilityfromnodeitoj:\nP =[I,D−1A,(D−1A)2,(D−1A)3,...,(D−1A)K−1] (17)\ni,j i,j\n=I ∥(D−1A) ∥(D−1A)2 ∥(D−1A)3 ∥...∥(D−1A)K−1 (18)\ni,j i,j i,j i,j i,j\nPPR (Gasteiger et al., 2018) Personalized PageRank (PPR) propagation is an approximate and\nfasterpropagationschemeundermessagepassing(Gasteigeretal.,2018).Foreachnode,itsPageR-\nankisgiveninitsanalyticalformas:\nXk =p(D,A,{α,|V|}) (19)\nPE\n=α(cid:0)I −(1−α)D−1A(cid:1)−1\ni (20)\n|V| k\nwherei istheindicatorfunction,andαcontrolsthedistancefromtherootnode. Itisconsidered\nk\none of the positional encodings in GRIT (Ma et al., 2023), which is strictly less powerful than\nRRWP.Fromitsanalyticalsolution,itisalsoclassifiedunderrandomwalk-basedmethodssincethe\nfunctionisinverselyrelatedtoD−1A.\nA.5.3 OTHERMETHODS\nWLPE (Dwivedi & Bresson, 2020) The Weisfeiler-Lehman Positional Encoding (WLPE)\nmethod,asintroducedbyDwivedi&Bresson(2020),leveragestheWeisfeiler-Lehman(WL)graph\nisomorphism test to generate positional encodings for nodes in a graph. Firstly, the hashed node\nfeaturefornodekX′ isupdatedbyusingahashfunctionthatcombinesthenode’sownfeatureX\nk k\nwiththefeaturesofitsneighbors:\nX′ =hash(X ,{X :u∈N(v),v ∈V}) (21)\nk k u\nHere,N(v)denotestheneighborhoodofnodev,andVisthesetofallnodesinthegraph.Secondly,\nthepositionalencodingXk isgeneratedbyapplyingafunctionf toX′ andthehiddendimension\nPE k\nd :\nh\nXk =f(X′,d )\nPE k h\nThe function f typically involves sinusoidal transformations to embed the positional information\nintoacontinuousvectorspace. Thistransformationisdetailedasfollows:\n(cid:20) (cid:18) X′ (cid:19) (cid:18) X′\n(cid:19)(cid:21)(cid:106)d 2h(cid:107)\nXk = sin k ,cos k\nPE 2l 2l+1\n10000dh 10000 dh l=0\nInthisexpression:\n• X′ isthehashedfeatureofnodek.\nk\n• d isthehiddendimension,controllingthesizeofthepositionalencoding.\nh\n•\nlrangesfrom0to(cid:4)d 2h(cid:5)\n,ensuringthattheresultingvectorhasd hdimensions.\nA.6 MODELCONFIGURATIONS\nWeprovidethemodelconfigurationheretoensurereproducibility.\nBENCHMARKINGGNN ForBENCHMARKINGGNN,weadheretoestablishedsettingsfromrel-\nevantliteratureforeachmodel. Specifically,fortheGatedGCNandGraphGPSmodels,wefollow\nthe configurations detailed in the GraphGPS paper (Rampa´sˇek et al., 2022). For the Exphormer\nmodel, we utilize the settings from the Exphormer paper (Shirzad et al., 2023). For the GINE,\nSparse GRIT, and Global GRIT models, we adopt the configurations from the GRIT paper (Ma\netal.,2023). Weprovidefivetables,oneforeachdataset,toensurecomprehensivecoverageofthe\nBENCHMARKINGGNN.UnliketheGraphGPSpaper,whichfixedthepositionalencoding,wewill\nreportthestatisticsofthecomputationofpositionalencodinginseparatetables. Configurationsare\nlistedfromtable5totable9.\n21\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nTable5: ModelConfigurationsforMNIST\nModel lr dropout heads layers hiddendim pooling batchsize epochs norm PEdim\nGatedGCN 0.001 0.0 - 4 52 mean 16 100 - 8\nGINE 0.001 0.0 - 3 52 mean 16 150 BatchNorm 18\nGraphGPS 0.001 0.0 4 3 52 mean 16 100 BatchNorm 18\nExphormer 0.001 0.1 4 5 40 mean 16 150 BatchNorm 8\nGRITSparseConv 0.001 0.0 - 3 52 mean 16 150 BatchNorm 18\nGRIT 0.001 0.0 4 3 52 mean 16 150 BatchNorm 18\nTable6: ModelConfigurationsforCIFAR10\nModel lr dropout heads layers hiddendim pooling batchsize epochs norm PEdim\nGatedGCN 0.001 0.0 - 4 52 mean 16 100 - 8\nGINE 0.001 0.0 - 4 52 mean 16 150 BatchNorm 18\nGraphGPS 0.001 0.0 4 3 52 mean 16 150 BatchNorm 8\nExphormer 0.001 0.1 4 5 40 mean 16 150 BatchNorm 8\nGRITSparseConv 0.001 0.0 - 3 52 mean 16 150 BatchNorm 18\nGRIT 0.001 0.0 4 3 52 mean 16 150 BatchNorm 18\nTable7: ModelConfigurationsforPATTERN\nModel lr dropout heads layers hiddendim pooling batchsize epochs norm PEdim\nGatedGCN 0.001 0.0 - 4 64 - 32 100 BatchNorm 10\nGINE 0.0005 0.0 - 10 64 - 32 100 BatchNorm 21\nGraphGPS 0.001 0.0 4 6 64 - 32 100 BatchNorm 10\nExphormer 0.0002 0.0 4 4 40 - 32 100 BatchNorm 10\nGRITSparseConv 0.0005 0.0 - 8 64 - 32 100 BatchNorm 21\nGRIT 0.0005 0.0 8 10 64 - 32 100 BatchNorm 21\nTable8: ModelConfigurationsforCLUSTER\nModel lr dropout heads layers hiddendim pooling batchsize epochs norm PEdim\nGatedGCN 0.001 0.0 - 4 48 - 16 100 BatchNorm 16\nGINE 0.001 0.01 - 2 64 - 16 100 BatchNorm 16\nGraphGPS 0.001 0.01 8 16 48 - 16 100 BatchNorm 16\nExphormer 0.0002 0.1 8 20 32 - 16 200 BatchNorm 8\nSparseGRIT 0.001 0.01 - 16 48 - 16 100 BatchNorm 32\nGRIT 0.0005 0.0 8 10 64 - 32 100 BatchNorm 21\nTable9: ModelConfigurationsforZINC\nModel lr dropout heads layers hiddendim pooling batchsize epochs norm PEdim\nGatedGCN 0.001 0.0 - 4 64 add 32 1700 BatchNorm 8\nGINE 0.001 0.0 - 10 64 add 32 1500 BatchNorm 21\nGraphGPS 0.001 0.0 4 10 64 add 32 1500 BatchNorm 21\nExphormer 0.001 0.0 4 4 64 add 32 1500 BatchNorm 8\nSparseGRIT 0.001 0.0 - 10 64 add 32 1500 BatchNorm 21\nGRIT 0.001 0.0 8 10 64 add 32 1500 BatchNorm 18\nLongRangeGraphBenchmark Wemainlyconsiderfourmodels: GatedGCN,GraphGPS,Ex-\nphormer,andSparseGRIT.ForGatedGCNandGraphGPS,weprimarilyfollowthefine-tunedcon-\nfigurations as described by Tonshoff et al. (2023) (To¨nshoff et al., 2023). For Sparse GRIT, we\nadoptthehyperparametersusedforthepeptides-funcandpeptides-structdatasetsandtransferthese\nsettingstotheCOCO-SP,Pascal-VOC,andPCQM-Contactdatasets, asdetailedbyDwivedietal.\n(2022)(Dwivedietal.,2022). ForExphormer,wefollowtheconfigurationsproposedbyShirzadet\nal. (2023)(Shirzadetal.,2023). Configurationsarelistedfromtable10totable14.\nOpenGraphBenchmark Wemainlyconsiderthreemodels: GraphGPS,Exphormer,andSparse\nGRIT.Duetoscalabilityissues,wedonotincludeconfigurationsfortheGPSmodelforogbn-arxiv.\nGraphGPS(Rampa´sˇeketal.,2022),Exphormer(Shirzadetal.,2023)andSparseGRIThaveshared\nthesamesettingsfortheogbg-molpcbadataset. Configurationsarelistedfromtable15totable17.\n22\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nTable10: ModelConfigurationsforPeptides-func\nModel lr dropout heads layers hiddendim pooling batchsize epochs norm PEdim\nGatedGCN 0.001 0.1 - 10 95 mean 200 250 BatchNorm 16\nGraphGPS 0.001 0.1 4 6 76 mean 200 250 BatchNorm 16\nExphormer 0.0003 0.12 4 8 64 mean 128 200 BatchNorm 16\nGRITSparseConv 0.0003 0.0 - 4 96 mean 16 300 BatchNorm 16\nGRIT 0.0003 0.0 4 4 96 mean 16 200 BatchNorm 17\nTable11: ModelConfigurationsforPeptides-struct\nModel lr dropout heads layers hiddendim pooling batchsize epochs norm PEdim\nGatedGCN 0.001 0.1 - 8 100 mean 128 250 BatchNorm 16\nGraphGPS 0.001 0.1 4 8 64 mean 200 250 BatchNorm 16\nExphormer 0.0003 0.12 4 4 88 mean 128 200 BatchNorm 16\nGRITSparseConv 0.0003 0.05 - 4 96 mean 16 300 BatchNorm 16\nGRIT 0.0003 0.0 8 4 96 mean 32 200 BatchNorm 24\nTable12: ModelConfigurationsforPCQM-Contact\nModel lr dropout heads layers hiddendim pooling batchsize epochs norm PEdim\nGatedGCN 0.001 0.1 - 8 215 - 500 100 - 16\nGraphGPS 0.001 0.0 4 6 76 - 500 150 BatchNorm 16\nExphormer 0.0003 0.0 4 7 64 - 128 200 BatchNorm 16\nGRITSparseConv 0.001 0.0 - 10 64 - 500 100 BatchNorm 16\nTable13: ModelConfigurationsforPascal-VOC\nModel lr dropout heads layers hiddendim pooling batchsize epochs norm PEdim\nGatedGCN 0.001 0.2 - 10 95 - 50 200 - 16\nGraphGPS 0.001 0.1 4 8 68 - 50 200 BatchNorm 16\nExphormer 0.0005 0.15 8 4 96 - 32 300 BatchNorm 16\nGRITSparseConv 0.001 0.0 - 10 64 - 50 250 BatchNorm 16\nTable14: ModelConfigurationsforCOCO-SP\nModel lr dropout heads layers hiddendim pooling batchsize epochs norm PEdim\nGatedGCN 0.001 0.1 - 6 120 - 16 200 - 16\nGraphGPS 0.001 0.1 4 8 68 - 50 200 BatchNorm 16\nExphormer 0.0005 0.0 4 7 72 - 32 200 BatchNorm 16\nGRITSparseConv 0.0005 0.0 - 4 64 - 32 200 BatchNorm 16\nTable15: ModelConfigurationsforOGBN-Arxiv\nModel lr dropout heads layers hiddendim pooling batchsize epochs norm PEdim\nExphormer 0.001 0.3 2 4 80 add 1 600 BatchNorm 16\nGRITSparseConv 0.001 0.1 - 4 64 add 1 600 BatchNorm 8\nTable16: ModelConfigurationsforOGBG-Molhiv\nModel lr dropout heads layers hiddendim pooling batchsize epochs norm PEdim\nGraphGPS 0.0001 0.05 4 10 64 mean 32 100 BatchNorm 8\nExphormer 0.0001 0.05 4 8 64 mean 32 100 BatchNorm 16\nGRITSparseConv 0.0001 0.0 - 8 64 mean 32 100 BatchNorm 16\nTable17: ModelConfigurationsforOGBG-Molpcba\nModel lr dropout heads layers hiddendim pooling batchsize epochs norm PEdim\nGraphGPS 0.0005 0.2 4 5 384 mean 512 100 BatchNorm 20\nExphormer 0.0005 0.2 4 5 384 mean 512 100 BatchNorm 20\nGRITSparseConv 0.0005 0.2 - 5 384 mean 512 100 BatchNorm 20\n23\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nA.7 COMPLETERESULTS\nThissectionprovidestheresultsofallrunsthatweconductedinthepaper. Thisalsoincludesthe\nnon-aggregated results that show the performance of every positional encoding on any model and\ndataset.\nA.7.1 BENCHMARKINGGNN\nTable18haslistedallresultsofGNNmodelswithdifferentpositionalencodingsonBENCHMARK-\nINGGNNdatasets. Figure4showsapercentageofimprovementcomparedtoGNNmodelswithout\nanypositionalencoding.\nGatedGCN GINE Exphormer GraphGPS SparseGRIT GRIT GatedGCN (Fully-connected)\n0.4 1 0.0 3\n00 .. 02 0 0.0 00 .. 21 2 1 1\n0.2 1 0.5 0.3 1 0 0 0.4 2 0.4 0\n00 .. 86 3 1.0 00 .. 65 1 1 1\n1.0 4 1.5 0.7 2 2 2\n20 20 01 .. 80 1.0 3 1.5 8\n15 15 0.6 0.5 2 1.0 6 10 0.4 0.0 1 0.5 4 1 50 05 00 0.. .02\n2\n0.5 0\n1\n0 0. .0\n5\n2\n0 0.4 1.0 0\n0011 .... 0505 0000111 ....... 2468024 000011 ...... 025702 050505 011223 ...... 505050 0000111 ....... 0257025 0505050 00 000.. ...01\n321\n000000 ...... 000001 024680\n0.5 0.0 0.25 0.0 0.25 0.4 0.02\n000 ... 001 050 0 0. .0\n2\n0 00. ..0 100\n05\n000 ... 000 025 050\n0.15\n0 0. .0\n1\n00 .. 12 50\n0.05 0.4 0.15 0.025 0.10 0.10 00 .. 11 50 0.6 00 .. 22 50 00 .. 00 75 50 0.05 0.2 0.05\n00 .. 22 50 10 .. 08 00 .. 33 50 00 .. 11 20 50 0.00 0.3 0.00\n150 10 100 10\n24 00 100 2 00 10 0 57 05 10 0 2 00\n0 50 20 25 20 20 20 20 30 0 30\n64 00 0 40 54 00 52 05 54 00 64 00\nLapP EE SLap SiP gE nNe RtWS RE WDIF RFRWPPPR GCKN WLPE LapP EE SLap SiP gE nNe RtWS RE WDIF RFRWPPPR GCKN WLPE LapP EE SLap SiP gE nNe RtWS RE WDIF RFRWPPPR GCKN WLPE LapP EE SLap SiP gE nNe RtWS RE WDIF RFRWPPPR GCKN WLPE LapP EE SLap SiP gE nNe RtWS RE WDIF RFRWPPPR GCKN WLPE LapP EE SLap SiP gE nNe RtWS RE WDIF RFRWPPPR GCKN WLPE LapP EE SLap SiP gE nNe RtWS RE WDIF RFRWPPPR GCKN WLPE\nPE PE PE PE PE PE PE\nFigure 4: Percentage of improvement compared to GNN models without any positional encoding\n(BENCHMARKINGGNN)\nA.7.2 LONGRANGEGRAPHBENCHMARK\nTable19haslistedallresultsofGNNmodelswithdifferentpositionalencodingsonBENCHMARK-\nINGGNNdataset. Figure5showsapercentageofimprovementcomparedtoGNNmodelswithout\nany positional encoding. Figure 6 shows the mean average of improvement on the Long Range\nGraphBenchmarkforeachpositionalencodingindividually.\nA.7.3 OPENGRAPHBENCHMARK\nTable 20 has listed all results of GNN models with different positional encodings on Open Graph\nBenchmarkdataset.\nA.8 STATISTICSFORPOSITIONALENCODINGS\nWe measure both the time that is taken to measure pre-computing positional enoodings (PEs), as\nwellasthespacethatCPUistakentoprecomputeit,whicharepresentedfromTable21toTable26.\nFigure7showsthecomparisonbetweentimeandmemoryforeachdataset,whichislog-scaled.\n24\n)%(\ntnemevorpmI\n)%( tnemevorpmI\n)%(\ntnemevorpmI\n)%(\ntnemevorpmI\n)%( tnemevorpmI\ndataset\n= CIFAR10\ndataset = CLUSTER\ndataset\n=\nPATTERN\ndataset\n= MNIST\ndataset\n= ZINC\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nGatedGCN GraphGPS Exphormer SparseGRIT GRIT\n1 80 02\n2\n02 .. 05\n30\n34 00\n6 4 64 52 .. 05 20 20\n2\n0 8 7.5 10 10\n2 10 10.0\n12 12.5 0 0\n0 20 3 3 6\n5 10 2 2 4\n0 1\n10 1 15 21 00 0 1 0 2\n20 30 2 1 0\n25 40 3 2\n1.0 0\n1.0 1.5\n10\n0.5 1.0\n20 0.5\n0.0 30 0.5\n0.0\n40 0.0\n0.5 50 0.5 0.5\n1.5 0\n0 12 1.0 1\n1 0 0.5 2\n2 1 0.0\n3 2 0.5 3\n4 3 1.0 4\n4\n5 1.5 5\n2.5 0.0\n1.0 1.0\n2.0 0.2 0.8\n1.5 0.5 0.4 0.6\n0.4\n1.0 0.0 0.6 0.2\n0.5 0.5 0.8 0.0\n0.2\n0.0\nLapP EE SLap SP iE gnNet RWSE RWDIFF RRWP GCKN WLPE LapP EE SLap SP iE gnNet RWSE RWDIFF RRWP GCKN WLPE LapP EE SLap SP iE gnNet RWSE RWDIFF RRWP GCKN WLPE LapP EE SLap SP iE gnNet RWSE RWDIFF RRWP GCKN WLPE\nPE PE PE PE\nFigure 5: Percentage of improvement compared to GNN models without any positional encoding\n(LRGb)\n25\n)%(\ntnemevorpmI\n)%(\ntnemevorpmI\ndataset\n= Peptides-func\ndataset\n=\nPeptides-struct\ndataset\n=\nPCQM-Contact\ndataset\n=\nPascalVOC-SP\ndataset\n=\nCOCO-SP\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nPeptides-func Peptides-struct PCQM-Contact\n0.50 0.45\n0.65\n0.45 0.40\n0.60 0.40 0.35\n0.55 0.35 0.30\n0.30 0.25\n0.50\n0.25 0.20\nno\nPE LapPE ESLapPE SignNet RWSE RWDIFF RRWP GCKN WLPE\nPascalVOC-SP COCO-SP\n0.375\n0.44\n0.350\n0.42\n0.325\n0.40 0.300\n0.275\n0.38\n0.250\n0.36 0.225\n0.34 0.200\nno\nPE LapPE ESLapPE SignNet RWSE RWDIFF RRWP GCKN WLPE\nno\nPE LapPE ESLapPE SignNet RWSE RWDIFF RRWP GCKN WLPE\nFigure6: MeanperformanceofdifferentpositionalencodingsonLongRangeGraphBenchmark\nMethod\nLapPE ESLapPE RWSE SignNet PPR GCKN WLPE RWDIFF RRWP\nMNIST CIFAR10 PATTERN CLUSTER\n104\n104 104\n103\n103 103\n103\n102 103 103 102 102\nElapsed Time (seconds) Elapsed Time (seconds) Elapsed Time (seconds) Elapsed Time (seconds)\nZINC Peptides-func Peptides-struc PCQM-Contact\n7×102 7×102\n6×103\n103 6×102 6×102\n5×103\n5×102 5×102\n102 4×102 4×102 4×103\n101 102 102 102 103\nElapsed Time (seconds) Elapsed Time (seconds) Elapsed Time (seconds) Elapsed Time (seconds)\nPascalVOC-SP COCO-SP ogbg-molhiv ogbg-molpcba\n9×103\n5×103\n103\n4×103\n103 8×103\n3×103\n102 103 3×103 4×103 6×103 102 3×102 4×102 6×102 103\nElapsed Time (seconds) Elapsed Time (seconds) Elapsed Time (seconds) Elapsed Time (seconds)\nFigure7: Temporalcomplexityvs. spatialcomplexityfordifferentpositionalencodings\n26\necnamrofreP\nnaeM\n)BM(\negasU\nyromeM\n)BM(\negasU\nyromeM\n)BM(\negasU\nyromeM\n)BM(\negasU\nyromeM\n)BM(\negasU\nyromeM\n)BM(\negasU\nyromeM\n)BM(\negasU\nyromeM\n)BM(\negasU\nyromeM\n)BM(\negasU\nyromeM\n)BM(\negasU\nyromeM\n)BM(\negasU\nyromeM\n)BM(\negasU\nyromeM\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nTable 18: Different positional encodings with GNNs on BENCHMARKINGGNN including ZINC,\nMNIST,CIFAR10,PATTERN,andCLUSTER.ExperimentsarerunonaNVIDIARTX3090and\nRTXA6000. Fiverandomseedsare: 0,7,42,100,and2024(althoughitshouldbenotedthatthe\nexecutionofPyGonthecudabackendisnon-deterministic).Notethatthebatchedgraphsaresparse\nasdefault. Batchedgraphsareonlyfully-connectedwhenitcomestoRRWP.\nSparseGraph MNIST↑ CIFAR10↑ PATTERN↑ CLUSTER↑ ZINC↓\nGatedGCN + noPE 97.800±0.138 69.303±0.318 85.397±0.040 61.695±0.261 0.2398±0.0094\nGatedGCN + ESLapPE 97.870±0.090 69.438±0.297 85.422±0.161 61.953±0.082 0.2409±0.0131\nGatedGCN + LapPE 97.575±0.025 69.285±0.205 86.700±0.000 65.130±0.405 0.1718±0.0024\nGatedGCN + RWSE 97.840±0.171 69.038±0.152 86.833±0.030 65.675±0.296 0.1016±0.0030\nGatedGCN + SignNet 97.553±0.167 68.570±0.240 86.763±0.027 75.293±0.047 0.1060±0.0021\nGatedGCN + PPR 97.797±0.045 69.224±0.546 86.522±0.093 74.175±0.122 0.3678±0.0198\nGatedGCN + GCKN 97.745±0.069 69.408±0.222 86.758±0.049 62.478±0.156 0.1446±0.0048\nGatedGCN + WLPE 97.693±0.235 69.418±0.165 84.980±0.160 62.738±0.291 0.1779±0.0059\nGatedGCN + RWDIFF 97.823±0.119 69.528±0.494 86.760±0.043 65.653±0.470 0.1346±0.0074\nGatedGCN + RRWP 97.908±0.076 69.572±0.787 85.465±0.148 61.728±0.174 0.2451±0.0131\nGINE + noPE 97.712±0.120 65.554±0.225 85.482±0.272 48.783±0.060 0.1210±0.0107\nGINE + ESLapPE 97.596±0.071 66.140±0.310 85.546±0.114 48.708±0.061 0.1209±0.0066\nGINE + LapPE 97.555±0.045 65.325±0.195 85.835±0.195 48.685±0.035 0.1144±0.0028\nGINE + RWSE 97.686±0.073 65.238±0.283 86.688±0.084 50.642±0.694 0.0795±0.0034\nGINE + SignNet 97.692±0.165 64.538±0.314 86.538±0.044 59.660±0.630 0.0993±0.0069\nGINE + PPR 97.650±0.088 65.082±0.434 85.658±0.048 47.440±2.290 0.3019±0.0122\nGINE + GCKN 97.708±0.105 65.976±0.308 85.844±0.157 48.780±0.149 0.1169±0.0029\nGINE + WLPE 97.716±0.118 66.132±0.225 85.676±0.084 48.997±0.068 0.1205±0.0062\nGINE + RWDIFF 97.750±0.097 65.632±0.553 85.764±0.209 49.148±0.168 0.0750±0.0058\nGINE + RRWP 96.742±0.277 62.790±1.501 86.526±0.036 48.736±0.108 0.0857±0.0009\nExphormer + noPE 98.414±0.047 74.962±0.631 85.676±0.049 77.500±0.151 0.1825±0.0209\nExphormer + ESLapPE 98.354±0.108 74.880±0.322 86.734±0.024 78.218±0.267 0.2023±0.0140\nExphormer + LapPE 98.270±0.070 75.205±0.095 86.565±0.075 77.175±0.165 0.1503±0.0117\nExphormer + RWSE 98.254±0.084 74.434±0.205 86.820±0.040 77.690±0.147 0.0933±0.0050\nExphormer + SignNet 98.136±0.094 73.842±0.317 86.752±0.088 78.280±0.211 0.0924±0.0072\nExphormer + PPR 98.076±0.126 74.076±0.104 86.712±0.047 78.098±0.211 0.2414±0.0123\nExphormer + GCKN 98.402±0.067 74.926±0.288 86.730±0.040 77.470±0.067 0.1690±0.0056\nExphormer + WLPE 98.398±0.162 74.794±0.358 85.454±0.033 77.402±0.120 0.1465±0.0095\nExphormer + RWDIFF 98.416±0.055 74.886±0.810 86.792±0.023 77.550±0.057 0.1360±0.0082\nExphormer + RRWP 98.418±0.179 74.504±0.369 85.652±0.001 77.434±0.056 0.1914±0.0153\nGraphGPS + noPE 98.136±0.085 72.310±0.198 84.182±0.276 77.590±0.158 0.1610±0.0045\nGraphGPS + ESLapPE 98.180±0.117 72.122±0.511 86.700±0.055 77.800±0.107 0.1795±0.0110\nGraphGPS + LapPE 98.065±0.075 72.310±0.530 86.550±0.150 77.355±0.115 0.1086±0.0062\nGraphGPS + RWSE 98.116±0.102 72.034±0.756 86.866±0.010 77.550±0.195 0.0744±0.0060\nGraphGPS + SignNet 98.012±0.091 72.152±0.323 86.734±0.069 78.308±0.111 0.0945±0.0019\nGraphGPS + PPR 98.010±0.097 71.842±0.325 86.124±0.214 76.828±0.250 0.1349±0.0054\nGraphGPS + GCKN 98.180±0.117 72.194±0.515 86.786±0.043 77.514±0.182 0.1460±0.0078\nGraphGPS + WLPE 98.038±0.134 72.258±0.661 84.916±0.195 76.866±0.171 0.1204±0.0055\nGraphGPS + RWDIFF 98.026±0.101 71.800±0.363 86.820±0.063 77.478±0.150 0.0924±0.0212\nGraphGPS + RRWP 98.146±0.105 72.084±0.466 84.436±0.224 77.420±0.080 0.1690±0.0084\nSparseGRIT + noPE 97.940±0.071 72.778±0.627 85.948±0.148 77.274±0.170 0.1255±0.0062\nSparseGRIT + ESLapPE 97.970±0.110 72.494±0.501 86.018±0.319 77.238±0.066 0.1280±0.0077\nSparseGRIT + LapPE 97.915±0.065 72.640±0.040 86.555±0.025 76.100±0.085 0.1070±0.0017\nSparseGRIT + RWSE 98.122±0.054 72.330±0.600 86.914±0.031 77.148±0.174 0.0676±0.0060\nSparseGRIT + SignNet 97.946±0.122 71.003±0.301 86.794±0.055 78.882±0.146 0.0821±0.0043\nSparseGRIT + PPR 98.020±0.194 71.926±0.833 86.650±0.033 78.732±0.202 0.2536±0.0193\nSparseGRIT + GCKN 97.958±0.127 72.598±0.535 86.650±0.033 76.746±0.187 0.1233±0.0071\nSparseGRIT + WLPE 97.946±0.125 72.096±0.835 85.712±0.027 77.170±0.143 0.1262±0.0059\nSparseGRIT + RWDIFF 98.022±0.083 72.366±0.388 86.938±0.045 77.214±0.065 0.0690±0.0039\nSparseGRIT + RRWP 98.088±0.048 74.954±0.256 87.168±0.041 79.872±0.079 0.0651±0.0027\nGRIT + noPE 98.108±0.190 74.402±0.135 87.126±0.033 78.616±0.178 0.1237±0.0057\nGRIT + ESLapPE 98.010±0.141 74.558±0.682 87.140±0.064 78.588±0.111 0.1241±0.0031\nGRIT + LapPE 97.875±0.001 73.325±0.505 86.985±0.015 77.960±0.310 0.1039±0.0035\nGRIT + RWSE 98.068±0.182 73.652±0.623 87.116±0.046 78.880±0.057 0.0671±0.0037\nGRIT + SignNet 97.766±0.220 72.812±0.482 87.085±0.064 79.770±0.150 0.0945±0.0098\nGRIT + PPR 97.986±0.082 73.568±0.451 86.780±0.001 78.958±0.175 0.1390±0.0076\nGRIT + GCKN 98.084±0.139 73.946±0.910 87.194±0.044 78.542±0.149 0.1306±0.0141\nGRIT + WLPE 98.022±0.173 74.206±0.684 86.863±0.033 78.500±0.091 0.1218±0.0035\nGRIT + RWDIFF 98.024±0.148 73.956±0.202 87.152±0.045 78.778±0.090 0.0671±0.0060\nGRIT + RRWP 98.124±0.141 75.662±0.410 87.217±0.034 79.812±0.109 0.0590±0.0010\n27\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nTable 19: Five Long Range Graph Benchamrk Datasets which include Peptides func,\nPepteide struct, PCQM Contact, PascalVOC-SuperPixels and COCO-SuperPixels. The hyper-\nparameters for Peptides func and Pepteide struct follow the original GraphGPS settings. For\nGraphGPS, we follow the settings of a special study into LRGB (To¨nshoff et al., 2023) where it\nfoundastate-of-the-artsettingsforGraphGPSonthosefivedatasets.\nSparseGraph Peptides-func Peptides-struct PCQM-Contact PascalVOC-SP COCO-SP\nGatedGCN + noPE 0.6523±0.0074 0.2470±0.0005 0.4730±0.0003 0.3923±0.0020 0.2619±0.0045\nGatedGCN + LapPE 0.6581±0.0068 0.2472±0.0003 0.4764±0.0004 0.3920±0.0033 0.2671±0.0006\nGatedGCN + ESLapPE 0.6484±0.0037 0.2490±0.0020 0.4736±0.0006 0.3930±0.0041 0.2628±0.0004\nGatedGCN + RWSE 0.6696±0.0022 0.2485±0.0022 0.4749±0.0005 0.3882±0.0041 0.2657±0.0007\nGatedGCN + SignNet 0.5327±0.0137 0.2688±0.0016 0.4672±0.0001 0.3814±0.0005 -\nGatedGCN + GCKN 0.6544±0.0040 0.2483±0.0009 0.4687±0.0002 0.3933±0.0044 -\nGatedGCN + WLPE 0.6562±0.0053 0.2473±0.0012 0.4671±0.0003 0.3805±0.0018 -\nGatedGCN + RWDIFF 0.6527±0.0053 0.2474±0.0003 0.4740±0.0003 0.3919±0.0019 0.2674±0.0031\nGatedGCN + RRWP 0.6516±0.0072 0.2514±0.0001 - - -\nGraphGPS + noPE 0.6514±0.0123 0.4243±0.0305 0.4649±0.0025 0.4517±0.0112 0.3799±0.0056\nGraphGPS + LapPE 0.6620±0.0073 0.2497±0.0024 0.4696±0.0017 0.4505±0.0062 0.3859±0.0016\nGraphGPS + ESLapPE 0.6516±0.0062 0.2568±0.0013 0.4639±0.0031 0.4538±0.0083 0.3866±0.0017\nGraphGPS + RWSE 0.6510±0.0071 0.2549±0.0033 0.4685±0.0009 0.4531±0.0073 0.3891±0.0033\nGraphGPS + SignNet 0.5719±0.0055 0.2657±0.0021 0.4624±0.0020 0.4291±0.0056 -\nGraphGPS + GCKN 0.6502±0.0101 0.2519±0.0005 0.4609±0.0007 0.4515±0.0053 -\nGraphGPS + WLPE 0.5851±0.0441 0.5203±0.0504 0.4622±0.0012 0.4501±0.0057 -\nGraphGPS + RWDIFF 0.6519±0.0077 0.4769±0.0360 0.4669±0.0006 0.4488±0.0097 0.3873±0.0024\nGraphGPS + RRWP 0.6505±0.0058 0.3734±0.0157 - - -\nExphormer + noPE 0.6200±0.0052 0.2584±0.0019 0.4661±0.0021 0.4149±0.0047 0.3445±0.0052\nExphormer + LapPE 0.6424±0.0063 0.2496±0.0013 0.4737±0.0024 0.4242±0.0044 0.3471±0.0028\nExphormer + ESLapPE 0.6281±0.0085 0.2513±0.0022 0.4676±0.0018 0.4141±0.0054 0.3485±0.0011\nExphormer + RWSE 0.6240±0.0069 0.2579±0.0010 0.4642±0.0039 0.4218±0.0063 0.3485±0.0011\nExphormer + SignNet 0.5458±0.0097 0.2667±0.0037 0.4615±0.0066 0.3966±0.0020 -\nExphormer + GCKN 0.6422±0.0080 0.2514±0.0012 0.4604±0.0038 0.4196±0.0049 -\nExphormer + WLPE 0.6216±0.0069 0.2558±0.0011 0.2051±0.0080 0.4104±0.0071 -\nExphormer + RWDIFF 0.6275±0.0031 0.2556±0.0021 0.4642±0.0032 0.4165±0.0059 0.3417±0.0006\nExphormer + RRWP 0.6208±0.0074 0.2586±0.0014 - - -\nSparseGRIT + noPE 0.4885±0.0036 0.2550±0.0006 0.4527±0.0006 0.3471±0.0030 0.1976±0.0038\nSparseGRIT + LapPE 0.5884±0.0059 0.2487±0.0014 0.4585±0.0011 0.3514±0.0026 0.1974±0.0008\nSparseGRIT + ESLapPE 0.5161±0.0069 0.2537±0.0005 0.4532±0.0005 0.3462±0.0035 0.1958±0.0001\nSparseGRIT + RWSE 0.5570±0.0079 0.2537±0.0012 0.4553±0.0014 0.3460±0.0071 0.1969±0.0010\nSparseGRIT + SignNet 0.5115±0.0064 0.2640±0.0018 0.4573±0.0003 0.3419±0.0074 -\nSparseGRIT + GCKN 0.5871±0.0042 0.2492±0.0010 0.4500±0.0004 0.3519±0.0040 -\nSparseGRIT + WLPE 0.4808±0.0016 0.2547±0.0005 0.4489±0.0012 0.3439±0.0027 -\nSparseGRIT + RWDIFF 0.5521±0.0072 0.2550±0.0008 0.4551±0.0005 0.3447±0.0046 0.1965±0.0011\nSparseGRIT + RRWP 0.6702±0.0080 0.2504±0.0025 - - -\nGRIT + noPE 0.4861±0.0053 0.2489±0.0008 0.4525±0.0001 0.3556±0.0019 0.2105±0.0004\nGRIT + LapPE 0.5834±0.0105 0.2474±0.0005 0.4580±0.0020 0.3551±0.0032 0.2112±0.0005\nGRIT + ESLapPE 0.4831±0.0023 0.2584±0.0002 0.4486±0.0014 0.3485±0.0028 0.2100±0.0008\nGRIT + RWSE 0.5432±0.0034 0.2612±0.0008 0.4524±0.0001 0.3461±0.0058 0.2114±0.0009\nGRIT + SignNet 0.5307±0.0085 0.2600±0.0018 0.4608±0.0007 0.3385±0.0045 -\nGRIT + GCKN 0.5868±0.0051 0.2477±0.0006 0.4521±0.0002 0.3516±0.0003 -\nGRIT + WLPE 0.4798±0.0012 0.2578±0.0011 0.4515±0.0004 0.3441±0.0011 -\nGRIT + RWDIFF 0.5801±0.0036 0.2639±0.0010 0.4563±0.0003 0.3521±0.0079 0.2128±0.0008\nGRIT + RRWP 0.6865±0.0050 0.2454±0.0010 - - -\n28\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nTable20: ResultsforthreeOGBdatasets.\nSparseGraph ogbn-arxiv ogbg-molhiv ogbg-molpcba\nGPS+noPE - 77.885±2.641 28.573±0.215\nGPS+ESLapPE - 78.295±0.925 28.373±0.477\nGPS+LapPE - 77.256±0.806 29.325±0.300\nGPS+GCKN - 77.652±1.326 -\nGPS+WLPE - 75.835±0.857 27.968±0.154\nGPS+RWSE - 77.890±1.045 28.563±0.283\nGPS+RRWP - 76.383±1.189 28.765±0.268\nExphormer+noPE 70.782±0.029 78.347±0.440 28.355±0.224\nExphormer+ESLapPE - 77.578±1.595 28.123±0.281\nExphormer+LapPE - 76.818±0.744 27.858±0.082\nExphormer+GCKN - 78.045±1.146 -\nExphormer+WLPE 70.738±0.095 75.587±1.172 27.283±0.312\nExphormer+RWSE 70.693±0.132 77.053±0.295 28.490±0.257\nExphormer+RRWP - 77.305±1.250 -\nSparseGRIT+noPE 70.955±0.119 77.752±1.331 20.950±0.076\nSparseGRIT+ESLapPE - 77.670±1.870 20.953±0.086\nSparseGRIT+LapPE - 75.393±1.358 22.748±0.455\nSparseGRIT+GCKN - 75.453±0.893 -\nSparseGRIT+WLPE 70.877±0.045 76.060±1.066 20.670±0.211\nSparseGRIT+RWSE - 76.973±0.242 23.628±0.205\nSparseGRIT+RRWP - 78.353±0.546 -\nTable21:RunningTimeforPretransformingPEs(measuredinseconds)onBENCHMARKINGGNN\nMNIST CIFAR10 PATTERN CLUSTER ZINC\nLapPE 93 123 28 23 8\nESLapPE 90 122 28 21 7\nRWSE 225 252 55 53 31\nSignNet 90 122 28 20 7\nPPR 585 600 90 71 313\nGCKN 1180 1705 552 448 89\nWLPE 166 217 166 108 8\nRWDIFF 210 209 226 158 15\nRRWP 159 - 47 53 22\nTable22: MemoryUsageforPretransformingPEs(measuredinMB)onBENCHMARKINGGNN\nMNIST CIFAR10 PATTERN CLUSTER ZINC\nLapPE 795.55 1021.10 290.78 251.45 67.68\nESLapPE 652.55 809.55 293.79 377.49 148.95\nRWSE 772.05 994.92 286.80 243.91 60.12\nSignNet 803.17 1024.11 293.41 377.97 58.90\nPPR 2430.63 4142.62 1849.49 1701.08 1041.20\nGCKN 355.12 824.58 292.69 170.61 68.00\nWLPE 1268.05 1709.76 1506.82 809.11 104.82\nRWDIFF 264.00 312.49 145.37 191.87 28.38\nRRWP 43327.52 - 29823.30 36577.61 2414.64\nTable23: RunningTimeforPretransformingPEs(measuredinseconds)onLRGB\nPeptides-func Peptides-struct PCQM-Contact PascalVOC-SP COCO-SP\nLapPE 44 44 358 203 2889\nESLapPE 44 45 384 203 3045\nRWSE 92 90 1500 418 8373\nSignNet 44 44 386 206 -\nGCKN 530 527 5072 2377 -\nWLPE 34 33 451 97 -\nRWDIFF 47 46 1075 764 8976\n29\nBenchmarkingPositionalEncodingsforGNNsandGraphTransformers\nTable24: MemoryUsageforPretransformingPEs(measuredinMB)onLRGB\nPeptides-func Peptides-struct PCQM-Contact PascalVOC-SP COCO-SP\nLapPE 438.33 651.95 4625.45 875.47 9149.31\nESLapPE 438.84 654.52 4617.98 877.84 9153.73\nRWSE 424.32 421.45 4410.43 875.87 9112.50\nSignNet 647.73 646.92 6091.65 871.27 -\nGCKN 662.06 661.18 6016.68 909.13 -\nWLPE 710.55 709.84 6525.88 1639.22 -\nRWDIFF 334.18 334.54 3484.15 677.29 7248.34\nTable25: RunningTimeforPretransformingPEs(measuredinseconds)onOGB\nogbn-arxiv ogbg-molhiv ogbg-molpcba\nESLapPE - 28 296\nLapPE - 26 314\nGCKN - 325 -\nWLPE 14 31 334\nRWSE 35 101 1034\nRRWP - 54 -\nTable26: MemoryUsageforPretransformingPEs(measuredinMB)onOGB\nogbn-arxiv ogbg-molhiv ogbg-molpcba\nESLapPE - 296.21 4374.86\nLapPE - 328.23 4514.98\nGCKN - 238.57 -\nWLPE 57.76 382.05 5080.64\nRWSE - 301.18 2799.23\nRRWP - 3220.03 -\n30",
    "pdf_filename": "Benchmarking_Positional_Encodings_for_GNNs_and_Graph_Transformers.pdf"
}